{"hands_on_practices": [{"introduction": "We begin our exploration by examining the simplest possible scenario: conditioning on the trivial $\\sigma$-algebra, $\\mathcal{G} = \\{\\emptyset, \\Omega\\}$. This represents a situation where we have no specific information about the outcome beyond the fact that it occurred. This exercise [@problem_id:1411086] demonstrates how the sophisticated Radon-Nikodym derivative definition elegantly reduces to the familiar concept of unconditional probability, grounding the abstract theory in intuitive understanding.", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space. Let $A$ be an event in the sigma-algebra $\\mathcal{F}$ such that its probability is $P(A)$. Consider the trivial sub-sigma-algebra $\\mathcal{G} = \\{\\emptyset, \\Omega\\}$.\n\nIn advanced probability theory, the conditional probability of $A$ given $\\mathcal{G}$, denoted as the random variable $P(A|\\mathcal{G})$, is defined as the unique (up to almost sure equality) $\\mathcal{G}$-measurable random variable that satisfies the following integral condition for every set $G \\in \\mathcal{G}$:\n$$\n\\int_{G} P(A|\\mathcal{G}) \\, dP = P(A \\cap G)\n$$\nThis is equivalent to saying that $P(A|\\mathcal{G})$ is the Radon-Nikodym derivative of the measure $Q_A(G) = P(A \\cap G)$ with respect to the measure $P$ restricted to $\\mathcal{G}$.\n\nA random variable $X: \\Omega \\to \\mathbb{R}$ is said to be $\\mathcal{G}$-measurable if the pre-image $X^{-1}(B)$ is in $\\mathcal{G}$ for every Borel set $B \\subseteq \\mathbb{R}$. For the specific case of the trivial algebra $\\mathcal{G} = \\{\\emptyset, \\Omega\\}$, this implies that the random variable must be a constant almost everywhere.\n\nGiven this framework, determine the explicit expression for the random variable $P(A|\\mathcal{G})$.", "solution": "We use the definition: $P(A \\mid \\mathcal{G})$ is a $\\mathcal{G}$-measurable random variable such that for every $G \\in \\mathcal{G}$,\n$$\n\\int_{G} P(A \\mid \\mathcal{G}) \\, dP = P(A \\cap G).\n$$\nSince $\\mathcal{G} = \\{\\emptyset, \\Omega\\}$ is the trivial sigma-algebra, any $\\mathcal{G}$-measurable random variable must be almost surely constant. Hence there exists a constant $c \\in \\mathbb{R}$ such that\n$$\nP(A \\mid \\mathcal{G}) = c \\quad \\text{almost surely}.\n$$\nImposing the defining condition for $G = \\Omega$ gives\n$$\n\\int_{\\Omega} P(A \\mid \\mathcal{G}) \\, dP = \\int_{\\Omega} c \\, dP = c \\, P(\\Omega) = P(A \\cap \\Omega) = P(A).\n$$\nBy the probability axiom $P(\\Omega) = 1$, we obtain\n$$\nc = P(A).\n$$\nFor $G = \\emptyset$, the condition reads\n$$\n\\int_{\\emptyset} P(A \\mid \\mathcal{G}) \\, dP = 0 = P(A \\cap \\emptyset),\n$$\nwhich is automatically satisfied and gives no further constraint. Therefore,\n$$\nP(A \\mid \\mathcal{G}) = P(A) \\quad \\text{almost surely},\n$$\ni.e., the conditional probability is the constant random variable equal to $P(A)$.", "answer": "$$\\boxed{P(A)}$$", "id": "1411086"}, {"introduction": "As a natural counterpart to conditioning on no information, we now consider the opposite extreme: conditioning on the full $\\sigma$-algebra $\\mathcal{F}$. This represents a state of complete information, where the precise outcome $\\omega$ is known. This practice [@problem_id:1411063] will reveal how conditional probability behaves when all uncertainty is removed, connecting it to the deterministic indicator function.", "problem": "In a probability space $(\\Omega, \\mathcal{F}, P)$, the conditional probability of an event $A \\in \\mathcal{F}$ given a sub-$\\sigma$-algebra $\\mathcal{G} \\subseteq \\mathcal{F}$ is defined as a $\\mathcal{G}$-measurable random variable, denoted $P(A|\\mathcal{G})$, which satisfies the property:\n$$ \\int_B P(A|\\mathcal{G}) \\, dP = P(A \\cap B) \\quad \\text{for all } B \\in \\mathcal{G}. $$\nThe existence and almost-sure uniqueness of such a random variable are guaranteed by the Radon-Nikodym theorem.\n\nConsider the probability space where the sample space is the unit interval, $\\Omega = [0, 1]$. The $\\sigma$-algebra $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0, 1]$, and the probability measure $P$ is the Lebesgue measure on $[0, 1]$.\n\nLet the event $A$ be the subset of $\\Omega$ defined as $A = [0, 1/4] \\cup [3/4, 1]$.\n\nDetermine the conditional probability of $A$ given the full $\\sigma$-algebra $\\mathcal{F}$, denoted as the random variable $X(\\omega) = P(A|\\mathcal{F})(\\omega)$. Which of the following expressions represents $X(\\omega)$?\n\nA. $X(\\omega) = 1/2$\n\nB. $X(\\omega) = \\omega$\n\nC. $X(\\omega) = 1_A(\\omega)$, where $1_A(\\omega)$ is the indicator function of the set $A$.\n\nD. $X(\\omega) = 1 - 1_A(\\omega)$\n\nE. $X(\\omega) = 4\\omega \\cdot 1_{[0, 1/4]}(\\omega) + (4-4\\omega) \\cdot 1_{[3/4, 1]}(\\omega)$", "solution": "By definition, for a sub-$\\sigma$-algebra $\\mathcal{G}$, a version of $P(A \\mid \\mathcal{G})$ is a $\\mathcal{G}$-measurable random variable $X$ such that for all $B \\in \\mathcal{G}$,\n$$\n\\int_{B} X \\, dP = P(A \\cap B).\n$$\nSet $\\mathcal{G} = \\mathcal{F}$. Consider $X = 1_{A}$. Then $1_{A}$ is $\\mathcal{F}$-measurable and, for any $B \\in \\mathcal{F}$,\n$$\n\\int_{B} 1_{A} \\, dP = \\int 1_{B} 1_{A} \\, dP = P(B \\cap A).\n$$\nThus $1_{A}$ satisfies the defining property of $P(A \\mid \\mathcal{F})$. By the Radon-Nikodym theorem, the version is unique almost surely, so\n$$\nP(A \\mid \\mathcal{F}) = 1_{A} \\quad \\text{a.s.}\n$$\nTherefore, as a random variable, $X(\\omega) = 1_{A}(\\omega)$.\n\nTo see that the other options do not satisfy the defining property for all $B \\in \\mathcal{F}$: \n- If $X(\\omega) = \\frac{1}{2}$, then $\\int_{B} X \\, dP = \\frac{1}{2} P(B)$, which equals $P(A \\cap B)$ for all $B$ only if $P(A \\cap B) = \\frac{1}{2} P(B)$ for all $B$, which is false (take $B \\subseteq A$).\n- If $X(\\omega) = \\omega$, then $\\int_{B} \\omega \\, dP$ cannot equal $P(A \\cap B)$ for arbitrary $B$ (e.g., $B = A$ would give $\\int_{A} \\omega \\, dP \\neq P(A)$).\n- If $X(\\omega) = 1 - 1_{A}(\\omega)$, then $\\int_{B} X \\, dP = P(B) - P(A \\cap B) \\neq P(A \\cap B)$ unless $P(B) = 2 P(A \\cap B)$ for all $B$, which is false.\n- If $X(\\omega)$ is the piecewise linear function in option E, it is not equal to $1_{A}$ on sets of positive measure (e.g., on $[0, 1/4]$ it takes values in $(0,1]$ rather than $1$), so it fails the integral identity for suitable $B$.\n\nHence the correct expression is $X(\\omega) = 1_{A}(\\omega)$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1411063"}, {"introduction": "Real-world information often arrives in stages, and our probabilistic models must account for this flow of knowledge. This problem [@problem_id:1411087] models this process using a sequence of coin flips and a filtration of nested $\\sigma$-algebras, representing accumulating information. By explicitly calculating the conditional probabilities at each stage, you will verify the fundamental tower property, which ensures that our expectations are consistent as we refine our knowledge over time.", "problem": "Consider a probability space $(\\Omega, \\mathcal{F}, P)$ corresponding to three successive independent flips of a fair coin. Let $H$ denote the outcome 'heads' and $T$ denote 'tails'. The sample space is $\\Omega = \\{H, T\\}^3 = \\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\}$, $\\mathcal{F}$ is the power set of $\\Omega$, and $P$ is the uniform probability measure on $\\Omega$, assigning a probability of $1/8$ to each elementary outcome.\n\nLet $X_i: \\Omega \\to \\{H, T\\}$ be the random variable representing the outcome of the $i$-th flip for $i \\in \\{1, 2, 3\\}$. We define two sigma-algebras: $\\mathcal{G}_1 = \\sigma(X_1)$, representing the information from the first flip, and $\\mathcal{G}_2 = \\sigma(X_1, X_2)$, representing the information from the first two flips.\n\nLet $A$ be the event that all three flips result in heads, i.e., $A = \\{HHH\\}$. In measure-theoretic probability, the conditional probability of $A$ given a sigma-algebra $\\mathcal{G} \\subseteq \\mathcal{F}$, denoted $P(A|\\mathcal{G})$, is defined as the conditional expectation $E[\\mathbf{1}_A|\\mathcal{G}]$, where $\\mathbf{1}_A$ is the indicator function for the event $A$. The result is a $\\mathcal{G}$-measurable random variable on $\\Omega$.\n\nLet $Y_1$ be the random variable defined as $P(A|\\mathcal{G}_1)$. Let $Z$ be the random variable defined as $P(A|\\mathcal{G}_2)$. We then define a third random variable, $Y_2 = E[Z|\\mathcal{G}_1]$.\n\nBased on a full calculation of the random variables $Y_1$ and $Y_2$ on the sample space $\\Omega$, which of the following statements correctly describes the relationship between them?\n\nA. $Y_1(\\omega) = Y_2(\\omega)$ for all $\\omega \\in \\Omega$.\n\nB. $Y_1(\\omega) = 2 Y_2(\\omega)$ for all $\\omega \\in \\Omega$.\n\nC. $Y_1(\\omega) = [Y_2(\\omega)]^2$ for all $\\omega \\in \\Omega$.\n\nD. There exists at least one $\\omega \\in \\Omega$ for which $Y_1(\\omega) \\neq Y_2(\\omega)$.\n\nE. $Y_1$ is constant on $\\Omega$, while $Y_2$ is not.", "solution": "We work on the finite probability space with uniform measure and independent flips. Let $A=\\{HHH\\}$. Define $\\mathcal{G}_{1}=\\sigma(X_{1})$ and $\\mathcal{G}_{2}=\\sigma(X_{1},X_{2})$. The atoms of $\\mathcal{G}_{1}$ are\n$$\nH_{1}=\\{\\omega\\in\\Omega:X_{1}(\\omega)=H\\}=\\{HHH,HHT,HTH,HTT\\},\\quad\nT_{1}=\\{\\omega\\in\\Omega:X_{1}(\\omega)=T\\}=\\{THH,THT,TTH,TTT\\}.\n$$\nThe atoms of $\\mathcal{G}_{2}$ are\n$$\nHH=\\{HHH,HHT\\},\\quad HT=\\{HTH,HTT\\},\\quad TH=\\{THH,THT\\},\\quad TT=\\{TTH,TTT\\}.\n$$\n\nCompute $Y_{1}=P(A\\mid\\mathcal{G}_{1})=E[\\mathbf{1}_{A}\\mid\\mathcal{G}_{1}]$. Since $Y_{1}$ is constant on each atom of $\\mathcal{G}_{1}$,\n- For $\\omega\\in H_{1}$,\n$$\nY_{1}(\\omega)=P(A\\mid X_{1}=H)=P(X_{2}=H,X_{3}=H\\mid X_{1}=H)=P(X_{2}=H)P(X_{3}=H)=\\frac{1}{4},\n$$\nby independence of flips.\n- For $\\omega\\in T_{1}$,\n$$\nY_{1}(\\omega)=P(A\\mid X_{1}=T)=0,\n$$\nsince $A$ requires $X_{1}=H$.\n\nThus\n$$\nY_{1}(\\omega)=\\begin{cases}\n\\frac{1}{4}, & \\omega\\in H_{1},\\\\\n0, & \\omega\\in T_{1}.\n\\end{cases}\n$$\n\nCompute $Z=P(A\\mid\\mathcal{G}_{2})=E[\\mathbf{1}_{A}\\mid\\mathcal{G}_{2}]$. Since $Z$ is constant on each atom of $\\mathcal{G}_{2}$,\n- For $\\omega\\in HH$,\n$$\nZ(\\omega)=P(A\\mid X_{1}=H,X_{2}=H)=P(X_{3}=H)=\\frac{1}{2}.\n$$\n- For $\\omega\\in HT\\cup TH\\cup TT$,\n$$\nZ(\\omega)=P(A\\mid \\text{first two flips are not both }H)=0.\n$$\nHence\n$$\nZ(\\omega)=\\begin{cases}\n\\frac{1}{2}, & \\omega\\in HH,\\\\\n0, & \\omega\\in HT\\cup TH\\cup TT.\n\\end{cases}\n$$\n\nNow compute $Y_{2}=E[Z\\mid\\mathcal{G}_{1}]$. Again $Y_{2}$ is constant on $H_{1}$ and $T_{1}$:\n- For $\\omega\\in H_{1}$, condition on $X_{1}=H$. Then $X_{2}$ is independent and takes $H$ or $T$ with probability $\\frac{1}{2}$. Therefore\n$$\nY_{2}(\\omega)=E[Z\\mid X_{1}=H]=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot 0=\\frac{1}{4}.\n$$\n- For $\\omega\\in T_{1}$, if $X_{1}=T$ then the first two flips are not both $H$, so $Z=0$ always, hence\n$$\nY_{2}(\\omega)=E[Z\\mid X_{1}=T]=0.\n$$\n\nThus\n$$\nY_{2}(\\omega)=\\begin{cases}\n\\frac{1}{4}, & \\omega\\in H_{1},\\\\\n0, & \\omega\\in T_{1},\n\\end{cases}\n$$\nwhich coincides pointwise with $Y_{1}(\\omega)$ on all $\\omega\\in\\Omega$.\n\nEquivalently, by the tower property, since $\\mathcal{G}_{1}\\subseteq\\mathcal{G}_{2}$,\n$$\nY_{2}=E[Z\\mid\\mathcal{G}_{1}]=E[E[\\mathbf{1}_{A}\\mid\\mathcal{G}_{2}]\\mid\\mathcal{G}_{1}]=E[\\mathbf{1}_{A}\\mid\\mathcal{G}_{1}]=Y_{1},\n$$\nand in this finite setting the equality holds for all $\\omega\\in\\Omega$.\n\nTherefore the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1411087"}]}