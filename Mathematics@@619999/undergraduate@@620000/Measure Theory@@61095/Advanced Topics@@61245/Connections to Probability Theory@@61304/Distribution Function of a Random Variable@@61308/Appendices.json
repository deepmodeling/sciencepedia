{"hands_on_practices": [{"introduction": "Understanding how a function transforms a random variable is a cornerstone of probability theory. This first exercise provides a clear, foundational practice in the discrete case. By first determining the probability mass function (PMF) of a random variable $X$ and then deriving the PMF and expected value for a new variable $Y=X^2$, you will solidify your grasp of the fundamental mechanics of transforming random variables. [@problem_id:5113]", "problem": "A discrete random variable $X$ has a set of possible outcomes (support) given by $S_X = \\{-2, -1, 1, 2\\}$. Its probability mass function (PMF), $P_X(k)$, is defined as:\n$$\nP_X(k) = P(X=k) = \\frac{|k|}{C}\n$$\nfor any $k \\in S_X$, where $C$ is a normalization constant.\n\nLet a new random variable $Y$ be defined by the function $Y = X^2$.\n\nYour task is to derive the expected value of the random variable $Y$, denoted as $E[Y]$.", "solution": "The problem asks for the expected value of the random variable $Y$, which is defined as $Y=X^2$. To find $E[Y]$, we first need to fully characterize the probability distributions of both $X$ and $Y$.\n\n**Step 1: Determine the normalization constant $C$ for the PMF of $X$.**\n\nThe sum of probabilities over the entire support of a random variable must equal 1. This is a fundamental axiom of probability theory.\n$$\n\\sum_{k \\in S_X} P_X(k) = 1\n$$\nThe support of $X$ is $S_X = \\{-2, -1, 1, 2\\}$. We can write out the sum:\n$$\nP_X(-2) + P_X(-1) + P_X(1) + P_X(2) = 1\n$$\nUsing the given formula for the PMF, $P_X(k) = \\frac{|k|}{C}$:\n$$\n\\frac{|-2|}{C} + \\frac{|-1|}{C} + \\frac{|1|}{C} + \\frac{|2|}{C} = 1\n$$\n$$\n\\frac{2}{C} + \\frac{1}{C} + \\frac{1}{C} + \\frac{2}{C} = 1\n$$\n$$\n\\frac{2+1+1+2}{C} = 1\n$$\n$$\n\\frac{6}{C} = 1 \\implies C = 6\n$$\nSo, the normalized PMF of $X$ is $P_X(k) = \\frac{|k|}{6}$ for $k \\in \\{-2, -1, 1, 2\\}$. Specifically:\n$P_X(-2) = \\frac{2}{6} = \\frac{1}{3}$\n$P_X(-1) = \\frac{1}{6}$\n$P_X(1) = \\frac{1}{6}$\n$P_X(2) = \\frac{2}{6} = \\frac{1}{3}$\n\n**Step 2: Determine the support and PMF of $Y$.**\n\nThe random variable $Y$ is defined as $Y = X^2$. We find the possible values of $Y$ (its support, $S_Y$) by applying this function to each value in the support of $X$.\n- If $X = -2$, then $Y = (-2)^2 = 4$.\n- If $X = -1$, then $Y = (-1)^2 = 1$.\n- If $X = 1$, then $Y = (1)^2 = 1$.\n- If $X = 2$, then $Y = (2)^2 = 4$.\n\nThe set of unique possible values for $Y$ is $S_Y = \\{1, 4\\}$.\n\nNow, we find the PMF of $Y$, denoted $P_Y(y)$, for each value $y \\in S_Y$. The probability $P_Y(y)$ is the sum of the probabilities of all outcomes of $X$ that map to the value $y$.\n\nFor $y=1$:\nThe event $Y=1$ occurs if $X^2=1$, which means $X=-1$ or $X=1$. Since these are mutually exclusive events for $X$:\n$$\nP_Y(1) = P(Y=1) = P(X=-1 \\text{ or } X=1) = P_X(-1) + P_X(1)\n$$\n$$\nP_Y(1) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}\n$$\n\nFor $y=4$:\nThe event $Y=4$ occurs if $X^2=4$, which means $X=-2$ or $X=2$.\n$$\nP_Y(4) = P(Y=4) = P(X=-2 \\text{ or } X=2) = P_X(-2) + P_X(2)\n$$\n$$\nP_Y(4) = \\frac{2}{6} + \\frac{2}{6} = \\frac{4}{6} = \\frac{2}{3}\n$$\nThe PMF of $Y$ is thus: $P_Y(1) = 1/3$ and $P_Y(4) = 2/3$. (Check: $\\frac{1}{3} + \\frac{2}{3} = 1$).\n\n**Step 3: Calculate the expected value of $Y$.**\n\nThe expected value of a discrete random variable $Y$ is given by the formula:\n$$\nE[Y] = \\sum_{y \\in S_Y} y \\cdot P_Y(y)\n$$\nUsing the support $S_Y = \\{1, 4\\}$ and the derived PMF for $Y$:\n$$\nE[Y] = (1) \\cdot P_Y(1) + (4) \\cdot P_Y(4)\n$$\nSubstituting the probabilities we found:\n$$\nE[Y] = (1) \\cdot \\left(\\frac{1}{3}\\right) + (4) \\cdot \\left(\\frac{2}{3}\\right)\n$$\n$$\nE[Y] = \\frac{1}{3} + \\frac{8}{3}\n$$\n$$\nE[Y] = \\frac{9}{3} = 3\n$$", "answer": "$$\\boxed{3}$$", "id": "5113"}, {"introduction": "Many real-world systems transform continuous signals in ways that produce outputs with both continuous and discrete characteristics. This practice explores such a scenario, modeled after a half-wave rectifier, where a continuous input voltage results in a mixed random variable for the output voltage. By deriving the Cumulative Distribution Function (CDF), you will directly engage with how point masses (jump discontinuities) and continuous segments coexist in a single distribution. [@problem_id:1356795]", "problem": "A signal processing unit receives an input voltage $X$ which is a random variable uniformly distributed on the interval $[-1, 1]$ volts. The unit contains a simple half-wave rectifier circuit that transforms the input voltage $X$ into an output voltage $Y$ according to the rule:\n$$\nY = \\begin{cases} X & \\text{if } X > 0 \\\\ 0 & \\text{if } X \\le 0 \\end{cases}\n$$\nDetermine the Cumulative Distribution Function (CDF) of the output voltage $Y$, which we denote as $F_Y(y)$. Your answer should be a piecewise function of $y$.", "solution": "Let $X$ be uniformly distributed on $[-1,1]$, so its probability density function is\n$$\nf_{X}(x) = \\begin{cases}\n\\frac{1}{2}, & -1 \\le x \\le 1, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe transformation is $Y = \\max(X,0)$, so $Y$ takes values in $[0,1]$, with a point mass at $0$ corresponding to all $X \\le 0$.\n\nThe cumulative distribution function of $Y$ is $F_{Y}(y) = \\Pr(Y \\le y)$. We evaluate it by cases on $y$:\n1) For $y < 0$, since $Y \\ge 0$ almost surely, $\\Pr(Y \\le y) = 0$. Thus $F_{Y}(y) = 0$.\n\n2) For $0 \\le y < 1$, decompose\n$$\nF_{Y}(y) = \\Pr(Y \\le y) = \\Pr(Y = 0) + \\Pr(0 < Y \\le y).\n$$\nHere $\\Pr(Y=0) = \\Pr(X \\le 0) = \\frac{1}{2}$ by the uniform distribution of $X$. For $0<Y\\le y$, since $Y=X$ when $X>0$, we have\n$$\n\\Pr(0<Y\\le y) = \\Pr(0 < X \\le y) = \\int_{0}^{y} \\frac{1}{2}\\,dx = \\frac{y}{2}.\n$$\nTherefore,\n$$\nF_{Y}(y) = \\frac{1}{2} + \\frac{y}{2} = \\frac{1+y}{2}, \\quad 0 \\le y < 1.\n$$\n\n3) For $y \\ge 1$, since $Y \\le 1$ always, $\\Pr(Y \\le y) = 1$. Thus $F_{Y}(y) = 1$.\n\nCombining these cases and noting that at $y=0$ the formula $\\frac{1+y}{2}$ yields $\\frac{1}{2}$, which matches $\\Pr(Y \\le 0) = \\Pr(Y=0) = \\frac{1}{2}$, the CDF is\n$$\nF_{Y}(y) = \\begin{cases}\n0, & y < 0, \\\\\n\\frac{1+y}{2}, & 0 \\le y < 1, \\\\\n1, & y \\ge 1.\n\\end{cases}\n$$", "answer": "$$\\boxed{\\begin{cases}\n0, & y<0,\\\\[4pt]\n\\frac{1+y}{2}, & 0 \\le y < 1,\\\\[4pt]\n1, & y \\ge 1\n\\end{cases}}$$", "id": "1356795"}, {"introduction": "The ultimate goal of studying distributions is often to answer practical questions about expectations and averages. This problem moves into an applied context, blending a discrete random variable (bus arrival time) with a continuous one (student arrival time) to model a common waiting time scenario. This exercise showcases the power of conditioning and the law of total expectation to solve complex problems, demonstrating why understanding the distributions of random variables is essential for real-world analysis. [@problem_id:1416757]", "problem": "A student plans to take a university shuttle bus. According to the schedule, the bus is supposed to arrive at a specific stop at 10:00 AM. However, due to variable traffic conditions, the bus arrives at exactly 10:00 AM with a probability of $1/2$, or it arrives exactly 10 minutes late, at 10:10 AM, also with a probability of $1/2$. There are no other possibilities for the bus's arrival time.\n\nThe student's arrival time at the bus stop is a continuous random variable. Specifically, the student arrives at a time uniformly distributed in the 10-minute interval between 9:55 AM and 10:05 AM. The student's arrival time is independent of the bus's arrival time.\n\nThe student's waiting time is defined as the difference between the bus's actual arrival time and the student's arrival time. If the student arrives after the bus has already departed, their waiting time is zero.\n\nCalculate the student's expected waiting time. Express your answer in minutes as an exact fraction in simplest form.", "solution": "Let time be measured in minutes relative to 10:00 AM. Let $B$ be the bus arrival time and $S$ be the student arrival time. Then $B \\in \\{0, 10\\}$ with $\\Pr(B=0)=\\Pr(B=10)=\\frac{1}{2}$, and $S \\sim \\text{Uniform}([-5,5])$, independent of $B$. The waiting time is $W=\\max(0, B-S)$.\n\nBy the law of total expectation,\n$$\n\\mathbb{E}[W]=\\mathbb{E}\\big[\\mathbb{E}[W \\mid B]\\big]=\\frac{1}{2}\\,\\mathbb{E}[(0-S)_{+}]+\\frac{1}{2}\\,\\mathbb{E}[(10-S)_{+}].\n$$\nThe density of $S$ is $f_{S}(s)=\\frac{1}{10}$ on $[-5,5]$.\n\nFor $B=0$, $(0-S)_{+}=-S$ when $s<0$ and $0$ otherwise, so\n$$\n\\mathbb{E}[(0-S)_{+}]=\\int_{-5}^{0}(-s)\\,\\frac{1}{10}\\,ds=\\frac{1}{10}\\left[-\\frac{s^{2}}{2}\\right]_{-5}^{0}=\\frac{1}{10}\\left(0-\\left(-\\frac{25}{2}\\right)\\right)=\\frac{5}{4}.\n$$\nFor $B=10$, since $s \\leq 5$ on the support of $S$, we have $(10-S)_{+}=10-S$ for all $s \\in [-5,5]$, hence\n$$\n\\mathbb{E}[(10-S)_{+}]=\\int_{-5}^{5}(10-s)\\,\\frac{1}{10}\\,ds=\\frac{1}{10}\\left([10s]_{-5}^{5}-\\left[\\frac{s^{2}}{2}\\right]_{-5}^{5}\\right)=\\frac{1}{10}\\left(100-\\left(\\frac{25}{2}-\\frac{25}{2}\\right)\\right)=10.\n$$\nTherefore,\n$$\n\\mathbb{E}[W]=\\frac{1}{2}\\cdot\\frac{5}{4}+\\frac{1}{2}\\cdot 10=\\frac{5}{8}+5=\\frac{45}{8}.\n$$\nThis is the expected waiting time in minutes.", "answer": "$$\\boxed{\\frac{45}{8}}$$", "id": "1416757"}]}