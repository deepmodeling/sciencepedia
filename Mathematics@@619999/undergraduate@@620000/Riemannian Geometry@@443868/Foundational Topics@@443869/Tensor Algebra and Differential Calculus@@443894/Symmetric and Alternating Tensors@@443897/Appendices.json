{"hands_on_practices": [{"introduction": "Tensors are often introduced through their components in a basis, such as a matrix. This first exercise provides a foundational link between this component representation and the tensor's action as a multilinear map. By starting with a symmetric matrix for a bilinear form, you will derive its explicit formula in coordinates, reinforcing the meaning of bilinearity and seeing directly how matrix symmetry translates into the symmetry of the function [@problem_id:3064500].", "problem": "Let $V=\\mathbb{R}^{2}$ with the standard basis $\\{e_{1},e_{2}\\}$. Consider the bilinear form $B:V\\times V\\to \\mathbb{R}$ whose matrix relative to this basis is the symmetric matrix\n$$\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}.\n$$\nLet $u=(a,b)$ and $v=(c,d)$ denote arbitrary vectors in $V$ written in the standard coordinates, so that $u=a\\,e_{1}+b\\,e_{2}$ and $v=c\\,e_{1}+d\\,e_{2}$. Using only the foundational definitions that a bilinear form is linear in each argument and that the matrix entries $B_{ij}$ record the values $B(e_{i},e_{j})$ relative to a basis, compute an explicit expression for $B(u,v)$ in terms of $a$, $b$, $c$, and $d$. Then, starting from these definitions, verify in coordinates that $B(u,v)=B(v,u)$.\n\nProvide your final answer as the single closed-form expression for $B(u,v)$ in terms of $a$, $b$, $c$, and $d$ (no units and no numeric rounding are required).", "solution": "The problem is well-posed and mathematically sound, containing all necessary information for a unique solution. We proceed with the derivation.\n\nLet $V = \\mathbb{R}^2$ be a real vector space with the standard basis $\\{e_1, e_2\\}$. We are given two vectors $u, v \\in V$ with coordinate representations $u = a e_1 + b e_2$ and $v = c e_1 + d e_2$ for some scalars $a, b, c, d \\in \\mathbb{R}$.\n\nWe are also given a bilinear form $B: V \\times V \\to \\mathbb{R}$. The matrix representation of $B$ with respect to the basis $\\{e_1, e_2\\}$ is given as:\n$$\n[B] = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nBy the foundational definition of the matrix of a bilinear form, the entries $B_{ij}$ are the values of the form on the basis vectors: $B_{ij} = B(e_i, e_j)$. Therefore, we have:\n$B(e_1, e_1) = 2$\n$B(e_1, e_2) = 1$\n$B(e_2, e_1) = 1$\n$B(e_2, e_2) = 3$\n\nThe problem requires the computation of $B(u, v)$ using only the definition that $B$ is linear in each of its arguments.\n\nFirst, we express $B(u, v)$ using the coordinate representations of $u$ and $v$:\n$$\nB(u, v) = B(a e_1 + b e_2, c e_1 + d e_2)\n$$\nUsing the linearity of $B$ in the first argument, we can expand this expression:\n$$\nB(a e_1 + b e_2, c e_1 + d e_2) = B(a e_1, c e_1 + d e_2) + B(b e_2, c e_1 + d e_2)\n$$\nApplying the scalar multiplication property of linearity in the first argument, we get:\n$$\n= a B(e_1, c e_1 + d e_2) + b B(e_2, c e_1 + d e_2)\n$$\nNext, we apply the linearity of $B$ in the second argument to each of the two terms:\n$$\na B(e_1, c e_1 + d e_2) = a \\left( B(e_1, c e_1) + B(e_1, d e_2) \\right) = a \\left( c B(e_1, e_1) + d B(e_1, e_2) \\right)\n$$\n$$\nb B(e_2, c e_1 + d e_2) = b \\left( B(e_2, c e_1) + B(e_2, d e_2) \\right) = b \\left( c B(e_2, e_1) + d B(e_2, e_2) \\right)\n$$\nCombining these expanded terms, we obtain the full expansion of $B(u, v)$:\n$$\nB(u, v) = a c B(e_1, e_1) + a d B(e_1, e_2) + b c B(e_2, e_1) + b d B(e_2, e_2)\n$$\nNow, we substitute the given values for $B(e_i, e_j)$ from the matrix $[B]$:\n$$\nB(u, v) = a c (2) + a d (1) + b c (1) + b d (3)\n$$\nSimplifying this expression yields the explicit formula for $B(u, v)$ in terms of the coordinates $a, b, c, d$:\n$$\nB(u, v) = 2ac + ad + bc + 3bd\n$$\nThis completes the first part of the problem.\n\nFor the second part, we must verify that $B(u, v) = B(v, u)$ using the same foundational definitions. We start by computing $B(v, u)$. The vectors are $v = c e_1 + d e_2$ and $u = a e_1 + b e_2$.\n$$\nB(v, u) = B(c e_1 + d e_2, a e_1 + b e_2)\n$$\nFollowing the same procedure of applying bilinearity, we expand the expression:\n$$\nB(v, u) = c B(e_1, a e_1 + b e_2) + d B(e_2, a e_1 + b e_2)\n$$\n$$\n= c \\left( a B(e_1, e_1) + b B(e_1, e_2) \\right) + d \\left( a B(e_2, e_1) + b B(e_2, e_2) \\right)\n$$\n$$\nB(v, u) = c a B(e_1, e_1) + c b B(e_1, e_2) + d a B(e_2, e_1) + d b B(e_2, e_2)\n$$\nNow, we substitute the values for $B(e_i, e_j)$:\n$$\nB(v, u) = c a (2) + c b (1) + d a (1) + d b (3)\n$$\n$$\nB(v, u) = 2ca + cb + da + 3db\n$$\nSince multiplication of real numbers is commutative ($ac = ca$, $ad = da$, $bc = cb$, $bd = db$), we can rearrange the terms to match the expression for $B(u, v)$:\n$$\nB(v, u) = 2ac + bc + ad + 3bd = 2ac + ad + bc + 3bd\n$$\nComparing this with our result for $B(u, v)$, we see that:\n$$\nB(u, v) = 2ac + ad + bc + 3bd = B(v, u)\n$$\nThe verification is complete. The symmetry of the bilinear form, $B(u, v) = B(v, u)$, is a direct consequence of the symmetry of its matrix representation with respect to the chosen basis, i.e., $B(e_i, e_j) = B_{ij} = B_{ji} = B(e_j, e_i)$. In this specific case, the fact that $B(e_1, e_2) = 1$ and $B(e_2, e_1) = 1$ ensures the symmetry.\nThe final required answer is the explicit expression for $B(u, v)$.", "answer": "$$\n\\boxed{2ac + ad + bc + 3bd}\n$$", "id": "3064500"}, {"introduction": "A fundamental property of tensors is that they can be decomposed into parts with specific symmetries. This practice explores the essential decomposition of a general $(0,2)$-tensor into its unique symmetric and alternating components. This exercise not only provides computational practice with this decomposition but also deepens understanding of why symmetry and alternation are intrinsic, geometric properties that persist under a change of basis [@problem_id:3066950].", "problem": "Let $(V,g)$ be a $3$-dimensional real inner product space modeling a tangent space of a Riemannian manifold at a point, with metric components $g_{ij}=\\delta_{ij}$ in an orthonormal basis $\\{e_{1},e_{2},e_{3}\\}$. Consider a $(0,2)$-tensor $B$ with components in this basis given by the matrix\n$$\n\\big(B_{ij}\\big)=\\begin{pmatrix}\n2 & 1 & -1\\\\\n3 & 0 & 4\\\\\n1 & -4 & 5\n\\end{pmatrix}.\n$$\nPerform the following tasks using only foundational definitions of symmetry/alternation for tensors and the definition of how tensor components transform under a change of basis.\n\n1) Compute the symmetric and alternating parts $B_{(ij)}$ and $B_{[ij]}$ of $B$ in the given orthonormal basis.\n\n2) Let $\\{e'_{1},e'_{2},e'_{3}\\}$ be another orthonormal basis obtained by rotating $\\{e_{1},e_{2},e_{3}\\}$ about $e_{3}$ by angle $\\theta=\\pi/3$. Denote the associated rotation matrix by\n$$\nR=\\begin{pmatrix}\n\\cos(\\pi/3) & -\\sin(\\pi/3) & 0\\\\\n\\sin(\\pi/3) & \\cos(\\pi/3) & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nUsing the definition of a $(0,2)$-tensor and the transformation of basis vectors, derive how the components of $B_{(ij)}$ and $B_{[ij]}$ transform from the unprimed to the primed basis, and explain why symmetry and alternation are preserved under this change of basis.\n\n3) With respect to $g$, define the squared norm of the alternating part by\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2}:=g^{ik}g^{jl}B_{[ij]}B_{[kl]}.\n$$\nCompute this quantity using the components found in part (1). Give your final answer as a single exact integer with no units.", "solution": "The problem is assessed to be valid as it is mathematically well-defined, self-contained, and grounded in the standard principles of tensor algebra on an inner product space. All provided information is consistent and sufficient to arrive at a unique solution.\n\nThe problem is addressed in three parts as requested.\n\n1) Computation of symmetric and alternating parts of $B$.\n\nA $(0,2)$-tensor $B$ with components $B_{ij}$ can be decomposed into its symmetric part, denoted $B_{(ij)}$, and its alternating (or anti-symmetric) part, denoted $B_{[ij]}$. The definitions for these components are:\n$$\nB_{(ij)} = \\frac{1}{2}(B_{ij} + B_{ji})\n$$\n$$\nB_{[ij]} = \\frac{1}{2}(B_{ij} - B_{ji})\n$$\nThe given components of $B$ in the orthonormal basis $\\{e_1, e_2, e_3\\}$ form the matrix:\n$$\n(B_{ij}) = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix}\n$$\nThe transpose of this matrix gives the components $B_{ji}$:\n$$\n(B_{ji}) = \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix}\n$$\nNow, we compute the component matrices for the symmetric and alternating parts.\n\nFor the symmetric part $B_{(ij)}$:\n$$\n(B_{(ij)}) = \\frac{1}{2} \\left[ \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix} + \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2+2 & 1+3 & -1+1 \\\\\n3+1 & 0+0 & 4-4 \\\\\n1-1 & -4+4 & 5+5\n\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}\n4 & 4 & 0 \\\\\n4 & 0 & 0 \\\\\n0 & 0 & 10\n\\end{pmatrix}\n$$\n$$\n(B_{(ij)}) = \\begin{pmatrix}\n2 & 2 & 0 \\\\\n2 & 0 & 0 \\\\\n0 & 0 & 5\n\\end{pmatrix}\n$$\nFor the alternating part $B_{[ij]}$:\n$$\n(B_{[ij]}) = \\frac{1}{2} \\left[ \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix} - \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2-2 & 1-3 & -1-1 \\\\\n3-1 & 0-0 & 4-(-4) \\\\\n1-(-1) & -4-4 & 5-5\n\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}\n0 & -2 & -2 \\\\\n2 & 0 & 8 \\\\\n2 & -8 & 0\n\\end{pmatrix}\n$$\n$$\n(B_{[ij]}) = \\begin{pmatrix}\n0 & -1 & -1 \\\\\n1 & 0 & 4 \\\\\n1 & -4 & 0\n\\end{pmatrix}\n$$\n\n2) Transformation of components and preservation of symmetry/alternation.\n\nLet $\\{e_i\\}$ be the original orthonormal basis and $\\{e'_j\\}$ be the new orthonormal basis. The transformation relating them is given by the rotation matrix $R$. The new basis vectors are expressed in terms of the old basis vectors as:\n$$\ne'_j = \\sum_{i=1}^3 R^i_{\\;j} e_i\n$$\nwhere $R^i_{\\;j}$ are the elements of the matrix $R$. The components of a $(0,2)$-tensor $B$ transform according to the rule $B'_{kl} = \\sum_{i,j} B_{ij} (R^{-1})^i_{\\;k} (R^{-1})^j_{\\;l}$. For an orthonormal transformation, the matrix $R$ is orthogonal, so $R^{-1} = R^T$. Let's denote the matrix of components of $B$ as $\\mathbf{B}$ and the matrix of its transformed components as $\\mathbf{B}'$. The transformation rule is $\\mathbf{B}' = (R^{-1})^T \\mathbf{B} R^{-1} = (R^T)^T \\mathbf{B} R^T = R \\mathbf{B} R^T$. However, the standard transformation for components of a covariant tensor is $B'_{kl} = \\sum_{i,j} B_{ij} \\Lambda^i_k \\Lambda^j_l$ where $e'_k = \\Lambda^i_k e_i$. Here, the matrix $R$ is this $\\Lambda$. So the rule is $\\mathbf{B}' = R^T \\mathbf{B} R$.\n\nLet $S$ be a symmetric tensor, so its component matrix $\\mathbf{S}$ satisfies $\\mathbf{S}^T = \\mathbf{S}$. The transformed component matrix is $\\mathbf{S}' = R^T \\mathbf{S} R$. We check its transpose:\n$$\n(\\mathbf{S}')^T = (R^T \\mathbf{S} R)^T = R^T \\mathbf{S}^T (R^T)^T = R^T \\mathbf{S} R = \\mathbf{S}'\n$$\nThus, the transformed tensor is also symmetric.\n\nLet $A$ be an alternating tensor, so its component matrix $\\mathbf{A}$ satisfies $\\mathbf{A}^T = -\\mathbf{A}$. The transformed component matrix is $\\mathbf{A}' = R^T \\mathbf{A} R$. We check its transpose:\n$$\n(\\mathbf{A}')^T = (R^T \\mathbf{A} R)^T = R^T \\mathbf{A}^T (R^T)^T = R^T (-\\mathbf{A}) R = -(R^T \\mathbf{A} R) = -\\mathbf{A}'\n$$\nThus, the transformed tensor is also alternating. The properties of being symmetric or alternating are geometric (coordinate-independent). Since $B=B_s + B_a$, applying the transformation gives $B' = (B_s)' + (B_a)'$. We have just shown that $(B_s)'$ is symmetric and $(B_a)'$ is alternating. By the uniqueness of this decomposition, $(B_s)'$ must be the symmetric part of $B'$, and $(B_a)'$ must be the alternating part of $B'$.\n\n3) Computation of the squared norm of the alternating part.\n\nThe squared norm of the alternating part $B_{[ij]}$ is defined as:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} := g^{ik}g^{jl}B_{[ij]}B_{[kl]}\n$$\nThe problem states that we are in an orthonormal basis $\\{e_i\\}$, in which the metric components are $g_{ij} = \\delta_{ij}$. The matrix of components of the contravariant metric tensor, $g^{ij}$, is the inverse of the matrix of $g_{ij}$. Since the matrix $(g_{ij})$ is the identity matrix $I$, its inverse is also the identity matrix. Thus, $g^{ij} = \\delta^{ij}$, where $\\delta^{ij}$ are the components of the identity matrix (numerically same as $\\delta_{ij}$).\n\nSubstituting $g^{ik} = \\delta^{ik}$ and $g^{jl} = \\delta^{jl}$ into the norm definition:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\delta^{ik}\\delta^{jl}B_{[ij]}B_{[kl]}\n$$\nThe summation is over all four indices $i,j,k,l$. The Kronecker deltas simplify the sum. The term $\\delta^{ik}$ is non-zero (equal to $1$) only when $k=i$. Summing over $k$:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\sum_{i,j,l} \\delta^{jl}B_{[ij]}B_{[il]}\n$$\nSimilarly, the term $\\delta^{jl}$ is non-zero (equal to $1$) only when $l=j$. Summing over $l$:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\sum_{i,j} B_{[ij]}B_{[ij]} = \\sum_{i=1}^3 \\sum_{j=1}^3 (B_{[ij]})^2\n$$\nThis is the sum of the squares of all the components of the matrix for $B_{[ij]}$. From part (1), we have:\n$$\n(B_{[ij]}) = \\begin{pmatrix}\n0 & -1 & -1 \\\\\n1 & 0 & 4 \\\\\n1 & -4 & 0\n\\end{pmatrix}\n$$\nThe sum of the squares of these components is:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = (0)^2 + (-1)^2 + (-1)^2 + (1)^2 + (0)^2 + (4)^2 + (1)^2 + (-4)^2 + (0)^2\n$$\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = 0 + 1 + 1 + 1 + 0 + 16 + 1 + 16 + 0\n$$\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = 36\n$$\nThe squared norm is a scalar invariant, meaning its value does not depend on the choice of orthonormal basis. The calculation in the primed basis would yield the same result.", "answer": "$$\\boxed{36}$$", "id": "3066950"}, {"introduction": "Our exploration culminates with a look at higher-rank tensors, where symmetry properties can be more nuanced. This problem presents a rank-3 tensor that exhibits symmetry in only some of its arguments, a common occurrence in physics and geometry. You will investigate how this partial symmetry has profound consequences for the tensor's fully alternating part and practice computing the fully symmetric projection, synthesizing many of the core concepts of this topic [@problem_id:3066954].", "problem": "Let $(V,\\langle\\cdot,\\cdot\\rangle)$ be the real inner-product space $V=\\mathbb{R}^{2}$ with the standard Euclidean inner product, and let $\\{e_{1},e_{2}\\}$ be the standard basis with dual basis $\\{e^{1},e^{2}\\}$. Consider the covariant $3$-tensor $T \\in T^{3}(V^{*})$ defined by\n$$\nT \\;=\\; e^{1}\\otimes e^{2}\\otimes e^{1} \\;+\\; e^{2}\\otimes e^{1}\\otimes e^{1}.\n$$\nUsing only the fundamental definitions of the action of the symmetric group on tensor slots, the alternating and symmetric projections onto $\\Lambda^{3}(V^{*})$ and $S^{3}(V^{*})$, and the inner product on $T^{3}(V^{*})$ induced from $\\langle\\cdot,\\cdot\\rangle$, do the following:\n- Justify that $T$ is symmetric in its first two arguments and explain why its alternation must vanish.\n- Compute explicitly the alternating projection $\\operatorname{Alt}(T)$ and the symmetric projection $\\operatorname{Sym}(T)$.\n- Compute the squared norm $\\| \\operatorname{Sym}(T)\\|^{2}$ with respect to the induced inner product on $T^{3}(V^{*})$ determined by $\\langle\\cdot,\\cdot\\rangle$ and the orthonormal dual basis $\\{e^{1},e^{2}\\}$.\n\nProvide your final answer as the single real number equal to $\\| \\operatorname{Sym}(T)\\|^{2}$. No rounding is required.", "solution": "This problem involves analyzing the symmetry properties of a specific rank-3 tensor. We will address each part of the problem sequentially.\n\n**1. Symmetry and Vanishing Alternation**\n\nThe action of a permutation $\\sigma \\in S_k$ on a simple tensor $f^1 \\otimes \\dots \\otimes f^k$ is defined by permuting its slots (factors): $\\sigma \\cdot (f^1 \\otimes \\dots \\otimes f^k) = f^{\\sigma^{-1}(1)} \\otimes \\dots \\otimes f^{\\sigma^{-1}(k)}$. This action extends linearly to all tensors in $T^k(V^*)$.\n\nThe given tensor is $T = T_1 + T_2$, where $T_1 = e^1 \\otimes e^2 \\otimes e^1$ and $T_2 = e^2 \\otimes e^1 \\otimes e^1$.\nTo check for symmetry in the first two arguments, we apply the transposition $\\tau = (12) \\in S_3$. Since $\\tau^{-1} = (12)$, its action is:\n$$\n\\tau \\cdot T = \\tau \\cdot (e^1 \\otimes e^2 \\otimes e^1) + \\tau \\cdot (e^2 \\otimes e^1 \\otimes e^1)\n$$\n$$\n= (e^2 \\otimes e^1 \\otimes e^1) + (e^1 \\otimes e^2 \\otimes e^1) = T_2 + T_1 = T\n$$\nSince $\\tau \\cdot T = T$, the tensor $T$ is symmetric in its first two arguments.\n\nThere are two main reasons why its full alternation, $\\operatorname{Alt}(T)$, must vanish:\n1.  **Dimensionality Argument:** The space of alternating $k$-tensors on an $n$-dimensional vector space $V^*$ is denoted $\\Lambda^k(V^*)$. Its dimension is $\\binom{n}{k}$. Here, $n=\\dim(V^*)=2$ and $k=3$. Thus, the dimension of the space of alternating 3-tensors is $\\dim \\Lambda^3(V^*) = \\binom{2}{3} = 0$. The only element in a zero-dimensional vector space is the zero vector. Since $\\operatorname{Alt}(T)$ is an alternating 3-tensor by definition, it must be zero.\n2.  **Symmetry Argument:** The alternation operator has the property that for any $\\sigma \\in S_k$, $\\operatorname{Alt}(\\sigma \\cdot U) = \\operatorname{sgn}(\\sigma) \\operatorname{Alt}(U)$. We showed that $T$ is symmetric with respect to the transposition $\\tau=(12)$, meaning $\\tau \\cdot T = T$. The sign of a transposition is $-1$. Therefore:\n    $$\n    \\operatorname{Alt}(T) = \\operatorname{Alt}(\\tau \\cdot T) = \\operatorname{sgn}(\\tau) \\operatorname{Alt}(T) = - \\operatorname{Alt}(T)\n    $$\n    This implies $2\\operatorname{Alt}(T) = 0$, so $\\operatorname{Alt}(T) = 0$.\n\n**2. Explicit Computation of Projections**\n\nThe alternating projection is $\\operatorname{Alt}(T) = \\frac{1}{3!} \\sum_{\\sigma \\in S_3} \\operatorname{sgn}(\\sigma) (\\sigma \\cdot T)$. As proven above, the result must be 0.\n\nThe symmetric projection is $\\operatorname{Sym}(T) = \\frac{1}{3!} \\sum_{\\sigma \\in S_3} \\sigma \\cdot T$. We need to compute the action of all 6 permutations in $S_3$ on $T$. Let $T_3 = e^1 \\otimes e^1 \\otimes e^2$.\n- $e \\cdot T = T_1 + T_2$\n- $(12) \\cdot T = T_2 + T_1$\n- $(13) \\cdot T = (e^1 \\otimes e^2 \\otimes e^1) + (e^1 \\otimes e^1 \\otimes e^2) = T_1 + T_3$\n- $(23) \\cdot T = (e^1 \\otimes e^1 \\otimes e^2) + (e^2 \\otimes e^1 \\otimes e^1) = T_3 + T_2$\n- $(123) \\cdot T = (e^2 \\otimes e^1 \\otimes e^1) + (e^1 \\otimes e^1 \\otimes e^2) = T_2 + T_3$\n- $(132) \\cdot T = (e^1 \\otimes e^1 \\otimes e^2) + (e^1 \\otimes e^2 \\otimes e^1) = T_3 + T_1$\n\nSumming these results:\n$$\n\\sum_{\\sigma \\in S_3} \\sigma \\cdot T = (T_1+T_2) + (T_2+T_1) + (T_1+T_3) + (T_3+T_2) + (T_2+T_3) + (T_3+T_1)\n$$\n$$\n= 4T_1 + 4T_2 + 4T_3 = 4(T_1 + T_2 + T_3)\n$$\nThe symmetric projection is this sum divided by $3! = 6$:\n$$\n\\operatorname{Sym}(T) = \\frac{4}{6}(T_1 + T_2 + T_3) = \\frac{2}{3}(e^1 \\otimes e^2 \\otimes e^1 + e^2 \\otimes e^1 \\otimes e^1 + e^1 \\otimes e^1 \\otimes e^2)\n$$\n\n**3. Computation of the Squared Norm**\n\nThe standard inner product on $V = \\mathbb{R}^2$ makes the basis $\\{e_1, e_2\\}$ orthonormal. The induced inner product on the dual space $V^*$ makes the dual basis $\\{e^1, e^2\\}$ orthonormal, i.e., $\\langle e^i, e^j \\rangle = \\delta^{ij}$.\nThis inner product extends to the tensor space $T^3(V^*)$ such that the basis of simple tensors $\\{e^{i_1} \\otimes e^{i_2} \\otimes e^{i_3}\\}$ is orthonormal.\n\nThe tensor $\\operatorname{Sym}(T)$ is a linear combination of three distinct basis tensors: $T_1 = e^1 \\otimes e^2 \\otimes e^1$, $T_2 = e^2 \\otimes e^1 \\otimes e^1$, and $T_3 = e^1 \\otimes e^1 \\otimes e^2$. Since these are distinct elements of an orthonormal basis, they are mutually orthogonal.\n\nWe compute the squared norm:\n$$\n\\| \\operatorname{Sym}(T)\\|^{2} = \\left\\langle \\frac{2}{3}(T_1+T_2+T_3), \\frac{2}{3}(T_1+T_2+T_3) \\right\\rangle\n$$\n$$\n= \\left(\\frac{2}{3}\\right)^2 \\langle T_1+T_2+T_3, T_1+T_2+T_3 \\rangle\n$$\nUsing orthogonality ($\\langle T_i, T_j \\rangle = 0$ for $i \\neq j$) and orthonormality ($\\langle T_i, T_i \\rangle = \\|T_i\\|^2 = 1$):\n$$\n= \\frac{4}{9} \\left( \\langle T_1, T_1 \\rangle + \\langle T_2, T_2 \\rangle + \\langle T_3, T_3 \\rangle \\right)\n$$\n$$\n= \\frac{4}{9} (1 + 1 + 1) = \\frac{4}{9}(3) = \\frac{4}{3}\n$$\nThe squared norm is $\\frac{4}{3}$.", "answer": "$$\n\\boxed{\\frac{4}{3}}\n$$", "id": "3066954"}]}