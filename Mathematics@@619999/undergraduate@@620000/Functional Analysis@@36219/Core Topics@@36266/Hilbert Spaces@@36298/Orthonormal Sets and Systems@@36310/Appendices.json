{"hands_on_practices": [{"introduction": "The Gram-Schmidt process is a cornerstone algorithm for building an orthogonal basis from any set of linearly independent vectors. This exercise invites you to explore what happens when this condition is not met by applying the process to a linearly dependent set in $\\mathbb{R}^3$. This practice is not merely computational; it offers a crucial insight into how the algorithm geometrically identifies and algebraically nullifies redundancy within a set of vectors. [@problem_id:1874299]", "problem": "Consider the set of three vectors $S = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$ in the Euclidean space $\\mathbb{R}^3$, where the vectors are defined as $\\mathbf{v}_1 = (1, 1, 0)$, $\\mathbf{v}_2 = (1, 0, 1)$, and $\\mathbf{v}_3 = (0, 1, -1)$. Apply the Gram-Schmidt orthogonalization process to this set of vectors, taken in the given order, to generate an orthogonal set $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\}$. What is the final vector, $\\mathbf{u}_3$, in the resulting orthogonal set? Express your answer as a row vector.", "solution": "We use the standard Euclidean inner product. The Gram-Schmidt process constructs an orthogonal set $\\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\mathbf{u}_{3}\\}$ from $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\mathbf{v}_{3}\\}$ in the given order by\n$$\n\\mathbf{u}_{1}=\\mathbf{v}_{1},\\quad\n\\mathbf{u}_{2}=\\mathbf{v}_{2}-\\frac{\\mathbf{v}_{2}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}\\mathbf{u}_{1},\\quad\n\\mathbf{u}_{3}=\\mathbf{v}_{3}-\\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}\\mathbf{u}_{1}-\\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{2}}{\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}}\\mathbf{u}_{2}.\n$$\nGiven $\\mathbf{v}_{1}=(1,1,0)$, we have\n$$\n\\mathbf{u}_{1}=\\mathbf{v}_{1}=(1,1,0),\\quad \\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}=1^{2}+1^{2}+0^{2}=2.\n$$\nNext,\n$$\n\\mathbf{v}_{2}\\cdot\\mathbf{u}_{1}=(1,0,1)\\cdot(1,1,0)=1,\\quad\n\\frac{\\mathbf{v}_{2}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}=\\frac{1}{2},\n$$\nso\n$$\n\\mathbf{u}_{2}=\\mathbf{v}_{2}-\\frac{1}{2}\\mathbf{u}_{1}=(1,0,1)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)=\\left(\\frac{1}{2},-\\frac{1}{2},1\\right).\n$$\nCompute the quantities needed for $\\mathbf{u}_{3}$:\n$$\n\\mathbf{v}_{3}\\cdot\\mathbf{u}_{1}=(0,1,-1)\\cdot(1,1,0)=1,\\quad \\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}=\\frac{1}{2},\n$$\n$$\n\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}=\\left(\\frac{1}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+1^{2}=\\frac{1}{4}+\\frac{1}{4}+1=\\frac{3}{2},\n$$\n$$\n\\mathbf{v}_{3}\\cdot\\mathbf{u}_{2}=(0,1,-1)\\cdot\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)=0\\cdot\\frac{1}{2}+1\\cdot\\left(-\\frac{1}{2}\\right)+(-1)\\cdot 1=-\\frac{3}{2},\n$$\n$$\n\\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{2}}{\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}}=\\frac{-\\frac{3}{2}}{\\frac{3}{2}}=-1.\n$$\nTherefore,\n$$\n\\mathbf{u}_{3}=\\mathbf{v}_{3}-\\frac{1}{2}\\mathbf{u}_{1}-(-1)\\mathbf{u}_{2}\n=\\mathbf{v}_{3}-\\frac{1}{2}\\mathbf{u}_{1}+\\mathbf{u}_{2}.\n$$\nSubstituting the vectors,\n$$\n\\mathbf{u}_{3}=(0,1,-1)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)+\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)\n=\\left(0,0,0\\right).\n$$\nThus the final vector in the orthogonal set is the zero vector, reflecting the linear dependence $\\mathbf{v}_{3}=\\mathbf{v}_{1}-\\mathbf{v}_{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}0 & 0 & 0\\end{pmatrix}}$$", "id": "1874299"}, {"introduction": "The power of functional analysis lies in extending geometric intuitions from finite-dimensional spaces like $\\mathbb{R}^n$ to infinite-dimensional function spaces. Here, we apply the Gram-Schmidt process not to vectors, but to a set of simple monomial functions to construct orthogonal polynomials. This practice illustrates a powerful method for generating specialized bases, which are fundamental in areas ranging from quantum mechanics to numerical approximation theory. [@problem_id:1874280]", "problem": "Consider the vector space of real-valued polynomials over the domain $\\mathbb{R}$. This space is equipped with a weighted inner product defined as:\n$$\n\\langle f, g \\rangle = \\int_{-\\infty}^{\\infty} f(x)g(x)e^{-x^2} dx\n$$\nfor any two polynomials $f(x)$ and $g(x)$ in the space.\n\nYour task is to apply the Gram-Schmidt orthogonalization process to the set of monomials $\\{1, x, x^2\\}$. Let this initial set be denoted by $\\{v_0, v_1, v_2\\}$ where $v_0(x) = 1$, $v_1(x) = x$, and $v_2(x) = x^2$.\n\nFind the third orthogonal polynomial, $u_2(x)$, in the resulting set $\\{u_0, u_1, u_2\\}$. The polynomial $u_2(x)$ must be normalized such that it is monic, meaning its leading coefficient (the coefficient of the highest power of $x$) is 1.\n\nFor your calculations, you may use the following standard Gaussian integral identities without proof:\n- For any non-negative integer $n$, $\\int_{-\\infty}^{\\infty} x^{2n+1} e^{-x^2} dx = 0$.\n- $\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$.\n- $\\int_{-\\infty}^{\\infty} x^2 e^{-x^2} dx = \\frac{\\sqrt{\\pi}}{2}$.\n- $\\int_{-\\infty}^{\\infty} x^4 e^{-x^2} dx = \\frac{3\\sqrt{\\pi}}{4}$.\n\nYour final answer should be the explicit expression for the polynomial $u_2(x)$.", "solution": "We work in the inner product space with $\\langle f,g\\rangle=\\int_{-\\infty}^{\\infty} f(x)g(x)e^{-x^{2}}\\,dx$. Apply Gram-Schmidt to $v_{0}(x)=1$, $v_{1}(x)=x$, $v_{2}(x)=x^{2}$.\n\nFirst, set $u_{0}=v_{0}=1$. Next,\n$$\nu_{1}=v_{1}-\\frac{\\langle v_{1},u_{0}\\rangle}{\\langle u_{0},u_{0}\\rangle}u_{0}.\n$$\nCompute $\\langle v_{1},u_{0}\\rangle=\\int_{-\\infty}^{\\infty} x\\cdot 1\\cdot e^{-x^{2}}\\,dx=0$ by oddness, and $\\langle u_{0},u_{0}\\rangle=\\int_{-\\infty}^{\\infty} e^{-x^{2}}\\,dx=\\sqrt{\\pi}$. Hence $u_{1}=x$.\n\nFor the third, \n$$\nu_{2}=v_{2}-\\frac{\\langle v_{2},u_{0}\\rangle}{\\langle u_{0},u_{0}\\rangle}u_{0}-\\frac{\\langle v_{2},u_{1}\\rangle}{\\langle u_{1},u_{1}\\rangle}u_{1}.\n$$\nCompute the needed inner products:\n$$\n\\langle v_{2},u_{0}\\rangle=\\int_{-\\infty}^{\\infty} x^{2}\\cdot 1\\cdot e^{-x^{2}}\\,dx=\\frac{\\sqrt{\\pi}}{2},\\quad \\langle u_{0},u_{0}\\rangle=\\sqrt{\\pi},\n$$\n$$\n\\langle v_{2},u_{1}\\rangle=\\int_{-\\infty}^{\\infty} x^{2}\\cdot x\\cdot e^{-x^{2}}\\,dx=\\int_{-\\infty}^{\\infty} x^{3}e^{-x^{2}}\\,dx=0,\n$$\n$$\n\\langle u_{1},u_{1}\\rangle=\\int_{-\\infty}^{\\infty} x^{2}e^{-x^{2}}\\,dx=\\frac{\\sqrt{\\pi}}{2}.\n$$\nTherefore,\n$$\nu_{2}(x)=x^{2}-\\frac{\\frac{\\sqrt{\\pi}}{2}}{\\sqrt{\\pi}}\\cdot 1-0\\cdot x=x^{2}-\\frac{1}{2}.\n$$\nThis polynomial is orthogonal to $u_{0}$ and $u_{1}$ under the given inner product and is monic since its leading coefficient is $1$.", "answer": "$$\\boxed{x^{2}-\\frac{1}{2}}$$", "id": "1874280"}, {"introduction": "Once we have an orthogonal system, we can use it to simplify complex problems, most notably by finding the best approximation of a vector within a given subspace. This exercise asks you to perform such an approximation, known as an orthogonal projection, in the context of the function space $L^2[0, 1]$. By projecting the function $f(x) = e^x$ onto the simple subspace of constant functions, you will gain hands-on experience with a core concept that underpins Fourier analysis and signal processing. [@problem_id:1874314]", "problem": "Consider the real inner product space $L^2[0, 1]$, which consists of all real-valued square-integrable functions on the interval $[0, 1]$. The inner product for any two functions $f$ and $g$ in this space is defined as:\n$$\n\\langle f, g \\rangle = \\int_0^1 f(x) g(x) \\, dx\n$$\nLet $W$ be the one-dimensional subspace of $L^2[0, 1]$ spanned by the function $u(x) = 1$. This subspace represents the set of all constant functions on $[0, 1]$.\n\nDetermine the orthogonal projection of the function $f(x) = e^x$ onto the subspace $W$. Your answer should be the expression that defines the resulting projected function.", "solution": "We project $f$ onto the one-dimensional subspace $W=\\operatorname{span}\\{u\\}$ with $u(x)=1$ using the orthogonal projection formula onto a span of a single vector:\n$$\nP_{W}f=\\frac{\\langle f,u\\rangle}{\\langle u,u\\rangle}\\,u.\n$$\nHere $f(x)=e^x$ and $u(x)=1$. Compute the inner products:\n$$\n\\langle f,u\\rangle=\\int_{0}^{1}e^x\\cdot 1\\,dx=\\int_{0}^{1}e^x\\,dx=\\left[e^x\\right]_{0}^{1}=e^1-e^0=e-1,\n$$\n$$\n\\langle u,u\\rangle=\\int_{0}^{1}1\\cdot 1\\,dx=\\int_{0}^{1}1\\,dx=\\left[x\\right]_{0}^{1}=1.\n$$\nTherefore the projection coefficient is\n$$\n\\frac{\\langle f,u\\rangle}{\\langle u,u\\rangle}=e-1,\n$$\nand the projected function is\n$$\nP_{W}f(x)=\\left(e-1\\right)u(x)=e-1.\n$$\nThus the orthogonal projection is the constant function equal to $e-1$ on $[0,1]$.", "answer": "$$\\boxed{e-1}$$", "id": "1874314"}]}