{"hands_on_practices": [{"introduction": "The power of Fourier analysis stems from a geometric idea: representing a function as a sum of orthogonal (perpendicular) components. Before tackling infinite series, let's practice this core concept by constructing orthogonal functions ourselves. This exercise [@problem_id:1863402] guides you through a single step of the Gram-Schmidt process in the function space $L^2[-1, 1]$, providing concrete intuition for how projections are used to build an orthogonal basis.", "problem": "Consider the vector space of real-valued, square-integrable functions on the interval $[-1, 1]$. We can define an inner product on this space for any two functions $f(x)$ and $g(x)$ as:\n$$\n\\langle f, g \\rangle = \\int_{-1}^{1} f(x)g(x) \\, dx\n$$\nTwo functions are said to be orthogonal if their inner product is zero.\n\nLet's consider two specific functions within this space: $f_1(x) = 1$ and $f_2(x) = x^2$. We want to construct a new polynomial function, $p(x)$, which is a linear combination of $f_1(x)$ and $f_2(x)$. Specifically, $p(x)$ has the form\n$$\np(x) = f_2(x) - c \\cdot f_1(x)\n$$\nwhere $c$ is a real constant.\n\nDetermine the analytic expression for the function $p(x)$ such that it is orthogonal to the function $f_1(x)$ with respect to the given inner product.", "solution": "We require $p(x)$ to be orthogonal to $f_{1}(x)=1$ under the inner product $\\langle f,g\\rangle=\\int_{-1}^{1}f(x)g(x)\\,dx$. With $p(x)=x^{2}-c$, the orthogonality condition is\n$$\n\\langle p,f_{1}\\rangle=\\int_{-1}^{1}(x^{2}-c)\\cdot 1\\,dx=0.\n$$\nUsing linearity of the integral and the fact that $c$ is a constant,\n$$\n\\int_{-1}^{1}(x^{2}-c)\\,dx=\\int_{-1}^{1}x^{2}\\,dx-c\\int_{-1}^{1}1\\,dx.\n$$\nEvaluate each integral explicitly:\n$$\n\\int_{-1}^{1}x^{2}\\,dx=\\left[\\frac{x^{3}}{3}\\right]_{-1}^{1}=\\frac{1}{3}-\\left(-\\frac{1}{3}\\right)=\\frac{2}{3},\n$$\n$$\n\\int_{-1}^{1}1\\,dx=\\left[x\\right]_{-1}^{1}=1-(-1)=2.\n$$\nTherefore,\n$$\n\\frac{2}{3}-c\\cdot 2=0 \\quad\\Longrightarrow\\quad 2c=\\frac{2}{3}\\quad\\Longrightarrow\\quad c=\\frac{1}{3}.\n$$\nSubstituting back gives\n$$\np(x)=x^{2}-\\frac{1}{3}.\n$$", "answer": "$$\\boxed{x^{2}-\\frac{1}{3}}$$", "id": "1863402"}, {"introduction": "With a solid understanding of orthogonality, we can now move to the central task of Fourier analysis: finding the representation of a function in a given basis. This practice [@problem_id:1863398] asks you to compute the very first approximation of the function $f(x)=x$ using its Fourier series. Calculating these initial coefficients is a fundamental skill that mirrors finding the components of a vector along coordinate axes, revealing how a function 'projects' onto the standard trigonometric basis.", "problem": "Consider the function $f(x) = x$ defined on the interval $[-\\pi, \\pi]$. In the context of the Hilbert space $L^2([-\\pi, \\pi])$ with the standard inner product, we can represent this function by its Fourier series. The $N$-th partial sum of the Fourier series for a function $f(x)$ is given by\n$$S_N(x) = \\frac{a_0}{2} + \\sum_{n=1}^{N} (a_n \\cos(nx) + b_n \\sin(nx))$$\nwhere $a_n$ and $b_n$ are the Fourier coefficients.\n\nDetermine the first partial sum, $S_1(x)$, for the function $f(x) = x$. Express your answer as a function of $x$.", "solution": "We work in $L^{2}([-\\pi,\\pi])$ with the standard inner product and the standard real Fourier series on $[-\\pi,\\pi]$. For a function $f$, the Fourier coefficients are\n$$\na_{0}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\,dx,\\quad\na_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\cos(nx)\\,dx,\\quad\nb_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\sin(nx)\\,dx.\n$$\nFor $f(x)=x$, note that $f$ is odd. Since $\\cos(nx)$ is even and $\\sin(nx)$ is odd, we have:\n- $a_{0}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}x\\,dx=0$ because the integrand is odd.\n- $a_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}x\\cos(nx)\\,dx=0$ because $x\\cos(nx)$ is odd.\n- $b_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}x\\sin(nx)\\,dx$ may be nonzero because $x\\sin(nx)$ is even.\n\nCompute $b_{n}$:\n$$\nb_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}x\\sin(nx)\\,dx=\\frac{2}{\\pi}\\int_{0}^{\\pi}x\\sin(nx)\\,dx.\n$$\nUse integration by parts with $u=x$, $dv=\\sin(nx)\\,dx$, so $du=dx$ and $v=-\\cos(nx)/n$. Then\n$$\n\\int x\\sin(nx)\\,dx=-\\frac{x\\cos(nx)}{n}+\\frac{\\sin(nx)}{n^{2}}.\n$$\nEvaluate from $0$ to $\\pi$:\n$$\n\\int_{0}^{\\pi}x\\sin(nx)\\,dx=\\left[-\\frac{x\\cos(nx)}{n}+\\frac{\\sin(nx)}{n^{2}}\\right]_{0}^{\\pi}\n=-\\frac{\\pi\\cos(n\\pi)}{n}+\\frac{\\sin(n\\pi)}{n^{2}}-0.\n$$\nSince $\\sin(n\\pi)=0$ and $\\cos(n\\pi)=(-1)^{n}$, we get\n$$\n\\int_{0}^{\\pi}x\\sin(nx)\\,dx=-\\frac{\\pi(-1)^{n}}{n}.\n$$\nTherefore,\n$$\nb_{n}=\\frac{2}{\\pi}\\left(-\\frac{\\pi(-1)^{n}}{n}\\right)=\\frac{2(-1)^{n+1}}{n}.\n$$\nThe first partial sum is\n$$\nS_{1}(x)=\\frac{a_{0}}{2}+a_{1}\\cos(x)+b_{1}\\sin(x)=0+0+2\\sin(x)=2\\sin(x).\n$$\nThus, $S_{1}(x)=2\\sin(x)$.", "answer": "$$\\boxed{2\\sin(x)}$$", "id": "1863398"}, {"introduction": "We've seen how to decompose a function into its Fourier components, but what guarantees that this process captures the entire function? This thought experiment [@problem_id:1863421] addresses this by exploring a crucial property of the Fourier basis: its completeness. By considering a function that is orthogonal to every single basis element, you will uncover the powerful implication that such a function must be the zero function, solidifying why Fourier series can represent such a vast class of functions.", "problem": "In the Hilbert space $H = L^2[0, 1]$ of complex-valued, square-integrable functions on the interval $[0, 1]$, the inner product between two functions $g(x)$ and $h(x)$ is defined as:\n$$\n\\langle g, h \\rangle = \\int_{0}^{1} g(x) \\overline{h(x)} \\,dx\n$$\nwhere $\\overline{h(x)}$ denotes the complex conjugate of $h(x)$. It is a fundamental result of Fourier analysis that the set of functions $\\{e_n(x) = \\exp(i 2 \\pi n x) \\mid n \\in \\mathbb{Z}\\}$ forms a complete orthonormal basis for this space.\n\nSuppose a particular function $f(x) \\in L^2[0, 1]$ is found to satisfy the condition\n$$\n\\int_{0}^{1} f(x) \\exp(-i 2 \\pi n x) \\,dx = 0\n$$\nfor every integer $n$ (i.e., $n \\in \\{\\dots, -2, -1, 0, 1, 2, \\dots\\}$).\n\nWhich of the following statements about the function $f(x)$ is correct?\n\nA. $f(x)$ must be the constant function $f(x) = 1$.\n\nB. $f(x)$ must be the zero function, i.e., $f(x) = 0$ almost everywhere on $[0,1]$.\n\nC. $f(x)$ must be a real-valued function.\n\nD. The information is insufficient to uniquely determine the nature of $f(x)$.\n\nE. $f(x)$ must be the constant function $f(x) = i$, where $i^2 = -1$.", "solution": "We work in the Hilbert space $H=L^{2}[0,1]$ with inner product $\\langle g,h\\rangle=\\int_{0}^{1}g(x)\\overline{h(x)}\\,dx$. The given orthonormal system is $e_{n}(x)=\\exp(i2\\pi n x)$ for $n\\in\\mathbb{Z}$, which is complete in $H$.\n\nThe hypothesis states that for every $n\\in\\mathbb{Z}$,\n$$\n\\int_{0}^{1} f(x)\\exp(-i2\\pi n x)\\,dx=0,\n$$\nwhich is exactly\n$$\n\\langle f,e_{n}\\rangle=0\\quad\\text{for all }n\\in\\mathbb{Z}.\n$$\nThus all Fourier coefficients $c_{n}:=\\langle f,e_{n}\\rangle$ of $f$ with respect to the complete orthonormal basis $\\{e_{n}\\}$ vanish.\n\nBy Parsevalâ€™s identity for a complete orthonormal basis,\n$$\n\\|f\\|_{L^{2}}^{2}=\\sum_{n\\in\\mathbb{Z}}|\\langle f,e_{n}\\rangle|^{2}=\\sum_{n\\in\\mathbb{Z}}|c_{n}|^{2}.\n$$\nSince $c_{n}=0$ for all $n$, we obtain\n$$\n\\|f\\|_{L^{2}}^{2}=\\sum_{n\\in\\mathbb{Z}}0=0.\n$$\nIn a Hilbert space, $\\|f\\|_{L^{2}}=0$ implies $f=0$ almost everywhere. Therefore $f$ must be the zero function almost everywhere on $[0,1]$.\n\nConsequently, among the options, only statement B is correct. Statements A and E are false because those constant functions have nonzero Fourier coefficients at $n=0$. Statement C is too weak and not implied independently of B, while D is false because the completeness uniquely determines $f$ to be zero almost everywhere under the given condition.", "answer": "$$\\boxed{B}$$", "id": "1863421"}]}