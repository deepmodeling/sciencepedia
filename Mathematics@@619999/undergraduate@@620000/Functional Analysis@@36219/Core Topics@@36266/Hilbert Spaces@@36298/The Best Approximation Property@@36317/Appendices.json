{"hands_on_practices": [{"introduction": "The fundamental idea behind best approximation in an inner product space is geometric: it is an orthogonal projection of a vector onto a subspace. This first exercise provides a clear and focused entry point by simplifying the problem to a projection onto a one-dimensional subspace. By finding the best approximation of the function $f(t) = t^2$ from the space of linear functions passing through the origin, you will apply the core principle of orthogonality in its most direct form [@problem_id:1886631].", "problem": "Consider the vector space of real-valued functions that are square-integrable on the interval $[0, 1]$, denoted as $L^2[0,1]$. This space is equipped with an inner product defined for any two functions $f$ and $g$ as $\\langle f, g \\rangle = \\int_0^1 f(t)g(t) \\, dt$. The distance between two functions $f$ and $g$ in this space is given by the norm of their difference, $\\|f-g\\| = \\sqrt{\\langle f-g, f-g \\rangle}$.\n\nLet $W$ be the subspace of $L^2[0,1]$ consisting of all linear polynomials that pass through the origin. That is, any polynomial $p(t)$ in $W$ has the form $p(t)=at+b$ and must satisfy the condition $p(0) = 0$.\n\nYour task is to find the specific function $p^*(t)$ within the subspace $W$ that serves as the best approximation to the function $f(t) = t^2$. The best approximation is defined as the function $p^*(t) \\in W$ that minimizes the distance $\\|f(t) - p(t)\\|$ for all possible choices of $p(t)$ in $W$.\n\nExpress your answer for $p^*(t)$ as a polynomial in the variable $t$.", "solution": "We work in the Hilbert space $L^{2}[0,1]$ with inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(t)g(t) \\, dt$. The subspace $W$ consists of all linear polynomials through the origin, so $W = \\{p(t) = a t : a \\in \\mathbb{R}\\} = \\operatorname{span}\\{t\\}$.\n\nIn an inner product space, the best approximation $p^{*} \\in W$ to $f$ is the orthogonal projection of $f$ onto $W$. Therefore, the residual $f - p^{*}$ is orthogonal to $W$. Writing $p^{*}(t) = a t$, the orthogonality condition is\n$$\n\\langle f - a t, t \\rangle = 0.\n$$\nHere $f(t) = t^{2}$, hence\n$$\n\\langle t^{2} - a t, t \\rangle = \\int_{0}^{1} \\left(t^{2} - a t\\right) t \\, dt = \\int_{0}^{1} t^{3} \\, dt - a \\int_{0}^{1} t^{2} \\, dt = 0.\n$$\nEvaluating the integrals,\n$$\n\\int_{0}^{1} t^{3} \\, dt = \\left[\\frac{t^{4}}{4}\\right]_{0}^{1} = \\frac{1}{4}, \\qquad \\int_{0}^{1} t^{2} \\, dt = \\left[\\frac{t^{3}}{3}\\right]_{0}^{1} = \\frac{1}{3}.\n$$\nThus,\n$$\n\\frac{1}{4} - a \\cdot \\frac{1}{3} = 0 \\quad \\Rightarrow \\quad a = \\frac{\\frac{1}{4}}{\\frac{1}{3}} = \\frac{3}{4}.\n$$\nTherefore, the best approximation is\n$$\np^{*}(t) = \\frac{3}{4}\\, t.\n$$", "answer": "$$\\boxed{\\frac{3}{4} t}$$", "id": "1886631"}, {"introduction": "Building on the foundational concept of projection, we now advance to a more general and practical scenario. In this practice, you will find the best approximation of $f(t) = t^2$ from the two-dimensional subspace of all linear polynomials [@problem_id:1886655]. This requires extending the orthogonality condition to a system of \"normal equations,\" a powerful and essential technique for determining projections in higher-dimensional subspaces.", "problem": "Let $L^2[0, 1]$ denote the inner product space of real-valued, square-integrable functions on the interval $[0, 1]$. The inner product for any two functions $f(t)$ and $g(t)$ in this space is defined as:\n$$\n\\langle f, g \\rangle = \\int_{0}^{1} f(t)g(t) \\, dt\n$$\nThis inner product induces a norm, $\\|f\\| = \\sqrt{\\langle f, f \\rangle}$, which can be interpreted as a measure of the \"size\" or \"length\" of a function.\n\nConsider the specific function $f(t) = t^2$. We wish to find its best approximation within the subspace $M$ of all linear polynomials. A function $g(t)$ belongs to $M$ if it can be written in the form $g(t) = c_0 + c_1 t$, where $c_0$ and $c_1$ are real constants.\n\nThe best approximation is defined as the function $g_0 \\in M$ that minimizes the distance to $f(t)$, i.e., it minimizes the value of $\\|f - g\\|$ over all possible choices of $g \\in M$.\n\nCalculate this minimum distance. Express your answer as a single closed-form analytic expression.", "solution": "We seek the best approximation of $f(t)=t^{2}$ in the subspace $M=\\{a+bt: a,b\\in\\mathbb{R}\\}$ with respect to the inner product $\\langle f,g\\rangle=\\int_{0}^{1} f(t)g(t)\\,dt$. The orthogonal projection $g_{0}(t)=a+bt$ is characterized by the normal equations\n$$\n\\langle f-g_{0},1\\rangle=0,\\qquad \\langle f-g_{0},t\\rangle=0.\n$$\nThese give\n$$\n\\int_{0}^{1}\\big(t^{2}-a-bt\\big)\\,dt=0,\\qquad \\int_{0}^{1}\\big(t^{2}-a-bt\\big)t\\,dt=0,\n$$\nthat is,\n$$\n\\frac{1}{3}-a-\\frac{b}{2}=0,\\qquad \\frac{1}{4}-\\frac{a}{2}-\\frac{b}{3}=0.\n$$\nFrom the first equation, $a=\\frac{1}{3}-\\frac{b}{2}$. Substituting into the second,\n$$\n\\frac{1}{4}-\\frac{1}{2}\\left(\\frac{1}{3}-\\frac{b}{2}\\right)-\\frac{b}{3}=0\n\\;\\Rightarrow\\;\n\\frac{1}{4}-\\frac{1}{6}+\\frac{b}{4}-\\frac{b}{3}=0\n\\;\\Rightarrow\\;\n\\frac{1}{12}-\\frac{b}{12}=0\n\\;\\Rightarrow\\;\nb=1,\n$$\nand hence $a=\\frac{1}{3}-\\frac{1}{2}=-\\frac{1}{6}$. Therefore $g_{0}(t)=t-\\frac{1}{6}$ and the residual is\n$$\nr(t)=f(t)-g_{0}(t)=t^{2}-t+\\frac{1}{6}.\n$$\nThe minimal distance is $\\|r\\|=\\left(\\int_{0}^{1} r(t)^{2}\\,dt\\right)^{1/2}$. Expand\n$$\n\\left(t^{2}-t+\\frac{1}{6}\\right)^{2}\n=t^{4}+(-t)^{2}+\\left(\\frac{1}{6}\\right)^{2}+2(t^{2})(-t)+2(t^{2})\\left(\\frac{1}{6}\\right)+2(-t)\\left(\\frac{1}{6}\\right)\n=t^{4}-2t^{3}+\\frac{4}{3}t^{2}-\\frac{1}{3}t+\\frac{1}{36}.\n$$\nIntegrating term by term from $0$ to $1$,\n$$\n\\int_{0}^{1}t^{4}\\,dt=\\frac{1}{5},\\quad\n\\int_{0}^{1}(-2t^{3})\\,dt=-\\frac{1}{2},\\quad\n\\int_{0}^{1}\\frac{4}{3}t^{2}\\,dt=\\frac{4}{9},\\quad\n\\int_{0}^{1}-\\frac{1}{3}t\\,dt=-\\frac{1}{6},\\quad\n\\int_{0}^{1}\\frac{1}{36}\\,dt=\\frac{1}{36}.\n$$\nThus\n$$\n\\int_{0}^{1}\\left(t^{2}-t+\\frac{1}{6}\\right)^{2}\\,dt\n=\\frac{1}{5}-\\frac{1}{2}+\\frac{4}{9}-\\frac{1}{6}+\\frac{1}{36}\n=\\frac{1}{180}.\n$$\nTherefore the minimum distance is\n$$\n\\|f-g_{0}\\|=\\sqrt{\\frac{1}{180}}=\\frac{1}{6\\sqrt{5}}.\n$$", "answer": "$$\\boxed{\\frac{1}{6\\sqrt{5}}}$$", "id": "1886655"}, {"introduction": "Our exploration so far has been within the familiar geometry of the $L^2$ norm, where approximation is synonymous with minimizing squared error. This practice challenges you to look beyond a single metric by comparing the best constant approximation of a function in both the $L^2$ and $L^1$ norms [@problem_id:1886646]. By working through this problem, you will discover that the notion of the \"best\" approximation is not universal but is fundamentally tied to the norm used to measure the error, providing a deeper insight into the structure of function spaces.", "problem": "In the study of approximation theory, a central problem is to find the \"best\" simple approximation to a more complex function. The notion of \"best\" depends on the norm used to measure the error.\n\nConsider the vector space $C[0,1]$ of continuous real-valued functions on the interval $[0,1]$. We wish to approximate the function $f(x) = x^2$ using a constant function $g(x) = c$.\n\nLet $c_1$ be the unique real constant that provides the best approximation to $f(x)$ in the $L^1$ sense. This means $c_1$ is the value of $c$ that minimizes the $L^1$ error, defined as:\n$$E_1(c) = \\int_{0}^{1} |f(x) - c| \\, dx$$\n\nLet $c_2$ be the unique real constant that provides the best approximation to $f(x)$ in the $L^2$ sense. This means $c_2$ is the value of $c$ that minimizes the $L^2$ error, defined as the square of the $L^2$ norm of the difference:\n$$E_2(c) = \\int_{0}^{1} (f(x) - c)^2 \\, dx$$\n\nDetermine the exact value of the ratio $\\frac{c_1}{c_2}$. Express your answer as a fraction in simplest form.", "solution": "We seek constants $c_{1}$ and $c_{2}$ minimizing the $L^{1}$ and $L^{2}$ errors for approximating $f(x)=x^{2}$ on $[0,1]$ by the constant function $g(x)=c$.\n\nFor the $L^{2}$ case, define\n$$\nE_{2}(c)=\\int_{0}^{1}(x^{2}-c)^{2}\\,dx=\\int_{0}^{1}\\left(x^{4}-2cx^{2}+c^{2}\\right)\\,dx.\n$$\nEvaluating the integrals gives\n$$\nE_{2}(c)=\\int_{0}^{1}x^{4}\\,dx-2c\\int_{0}^{1}x^{2}\\,dx+c^{2}\\int_{0}^{1}1\\,dx=\\frac{1}{5}-\\frac{2}{3}c+c^{2}.\n$$\nDifferentiate with respect to $c$ and set to zero:\n$$\n\\frac{dE_{2}}{dc}=-\\frac{2}{3}+2c=0 \\quad\\Longrightarrow\\quad c_{2}=\\frac{1}{3}.\n$$\nSince $E_{2}$ is a strictly convex quadratic in $c$, this critical point is the unique minimizer.\n\nFor the $L^{1}$ case, define\n$$\nE_{1}(c)=\\int_{0}^{1}|x^{2}-c|\\,dx.\n$$\nFor any $c$ at which $x^{2}\\neq c$ almost everywhere, the derivative is the integral of the pointwise derivative of $|x^{2}-c|$ with respect to $c$, namely\n$$\n\\frac{dE_{1}}{dc}=\\int_{0}^{1}\\operatorname{sgn}(c-x^{2})\\,dx=\\lambda\\{x\\in[0,1]:x^{2}<c\\}-\\lambda\\{x\\in[0,1]:x^{2}>c\\},\n$$\nwhere $\\lambda$ denotes Lebesgue measure. Since the total measure is $1$, this simplifies to\n$$\n\\frac{dE_{1}}{dc}=2\\,\\lambda\\{x\\in[0,1]:x^{2}<c\\}-1.\n$$\nOn $[0,1]$, the condition $x^{2}<c$ is equivalent to $x<\\sqrt{c}$ for $c\\in[0,1]$, so\n$$\n\\frac{dE_{1}}{dc}=\n\\begin{cases}\n-1, & c<0,\\\\\n2\\sqrt{c}-1, & 0<c<1,\\\\\n1, & c>1.\n\\end{cases}\n$$\nThus $E_{1}$ is strictly decreasing for $c<0$, strictly increasing for $c>1$, and in $(0,1)$ its critical point satisfies\n$$\n2\\sqrt{c}-1=0 \\quad\\Longrightarrow\\quad \\sqrt{c}=\\frac{1}{2}\\quad\\Longrightarrow\\quad c_{1}=\\frac{1}{4}.\n$$\nConvexity of $E_{1}$ in $c$ (as an integral of convex functions) ensures this critical point is the unique minimizer.\n\nTherefore,\n$$\n\\frac{c_{1}}{c_{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{3}}=\\frac{3}{4}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1886646"}]}