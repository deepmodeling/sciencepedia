{"hands_on_practices": [{"introduction": "Understanding the structure of a vector space is the first step toward applying powerful techniques like orthogonal decomposition. This exercise challenges you to identify the orthogonal complement of a fundamental subspace—the skew-symmetric matrices—within the space of all $2 \\times 2$ matrices, $M_{2}(\\mathbb{R})$. Mastering this concept is key to understanding how any matrix can be uniquely split into two orthogonal components, a cornerstone of the direct sum decomposition $V = W \\oplus W^\\perp$.", "problem": "Let $M_{2}(\\mathbb{R})$ be the vector space of all $2 \\times 2$ matrices with real entries. This space is equipped with the Frobenius inner product, defined for any two matrices $A, B \\in M_{2}(\\mathbb{R})$ as $\\langle A, B \\rangle_F = \\mathrm{tr}(A^T B)$, where $\\mathrm{tr}(X)$ denotes the trace of a matrix $X$ and $X^T$ denotes its transpose.\n\nConsider the subspace $W \\subset M_{2}(\\mathbb{R})$ consisting of all skew-symmetric matrices, that is, matrices $S$ such that $S^T = -S$.\n\nThe orthogonal complement of $W$, denoted by $W^\\perp$, is the set of all matrices $A \\in M_{2}(\\mathbb{R})$ such that $\\langle A, S \\rangle_F = 0$ for every matrix $S \\in W$. Which of the following correctly describes the subspace $W^\\perp$?\n\nA. The subspace of all symmetric matrices (i.e., matrices $A$ satisfying $A^T = A$).\n\nB. The subspace of all matrices with zero trace (i.e., matrices $A$ satisfying $\\mathrm{tr}(A) = 0$).\n\nC. The subspace of all diagonal matrices.\n\nD. The subspace consisting only of the zero matrix.\n\nE. The subspace of all scalar matrices (i.e., matrices of the form $kI$ for some scalar $k \\in \\mathbb{R}$ and $I$ being the identity matrix).", "solution": "To determine the orthogonal complement $W^\\perp$ of the subspace $W$ of skew-symmetric matrices, we must identify all matrices $A \\in M_{2}(\\mathbb{R})$ that are orthogonal to every matrix $S \\in W$ with respect to the Frobenius inner product.\n\nFirst, let's characterize a general skew-symmetric matrix $S \\in W$. A matrix $S = \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{21} & s_{22} \\end{pmatrix}$ is skew-symmetric if $S^T = -S$.\n$$\n\\begin{pmatrix} s_{11} & s_{21} \\\\ s_{12} & s_{22} \\end{pmatrix} = -\\begin{pmatrix} s_{11} & s_{12} \\\\ s_{21} & s_{22} \\end{pmatrix} = \\begin{pmatrix} -s_{11} & -s_{12} \\\\ -s_{21} & -s_{22} \\end{pmatrix}\n$$\nEquating the corresponding entries gives:\n$s_{11} = -s_{11} \\implies 2s_{11} = 0 \\implies s_{11} = 0$\n$s_{22} = -s_{22} \\implies 2s_{22} = 0 \\implies s_{22} = 0$\n$s_{21} = -s_{12}$\nSo, any skew-symmetric matrix $S \\in W$ must be of the form $S = \\begin{pmatrix} 0 & s \\\\ -s & 0 \\end{pmatrix}$ for some real number $s$.\n\nNext, let's consider a general matrix $A \\in M_{2}(\\mathbb{R})$, which can be written as $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$.\nBy definition, $A \\in W^\\perp$ if and only if $\\langle A, S \\rangle_F = 0$ for all $S \\in W$. Using the definition of the Frobenius inner product, this condition is $\\mathrm{tr}(A^T S) = 0$.\n\nLet's compute the product $A^T S$:\n$$\nA^T S = \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix} \\begin{pmatrix} 0 & s \\\\ -s & 0 \\end{pmatrix} = \\begin{pmatrix} (a)(0) + (c)(-s) & (a)(s) + (c)(0) \\\\ (b)(0) + (d)(-s) & (b)(s) + (d)(0) \\end{pmatrix} = \\begin{pmatrix} -cs & as \\\\ -ds & bs \\end{pmatrix}\n$$\n\nNow, we compute the trace of this product matrix:\n$$\n\\mathrm{tr}(A^T S) = -cs + bs = s(b - c)\n$$\n\nThe condition for $A$ to be in $W^\\perp$ is that $\\mathrm{tr}(A^T S) = 0$ for all possible choices of $S \\in W$. This means that $s(b - c) = 0$ must hold for any real number $s$. If we choose any non-zero value for $s$ (e.g., $s=1$), the equation implies that we must have $b - c = 0$, which means $b = c$.\n\nTherefore, any matrix $A \\in W^\\perp$ must have the form:\n$$\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}\n$$\nwhere $a, b, d$ can be any real numbers. This is precisely the definition of a symmetric matrix, since for such a matrix,\n$$\nA^T = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}^T = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix} = A\n$$\nThus, the orthogonal complement $W^\\perp$ is the subspace of all $2 \\times 2$ symmetric matrices.\n\nComparing this result with the given options:\nA. The subspace of all symmetric matrices ($A^T = A$). This matches our finding.\nB. The subspace of all matrices with zero trace ($\\mathrm{tr}(A) = 0$). This would require $a+d=0$, which is not a necessary condition.\nC. The subspace of all diagonal matrices. This would require $b=c=0$, which is a subset of symmetric matrices but not the entire space.\nD. The subspace consisting only of the zero matrix. This is also a subset but not the entire space.\nE. The subspace of all scalar matrices. This would require $b=c=0$ and $a=d$, which is a smaller subset of symmetric matrices.\n\nThe correct description for $W^\\perp$ is the entire subspace of symmetric matrices.", "answer": "$$\\boxed{A}$$", "id": "1858250"}, {"introduction": "Having established the relationship between symmetric and skew-symmetric matrices ([@problem_id:1858250]), we now move from theory to application. This practice provides a concrete opportunity to compute the orthogonal projection of a specific matrix onto the subspace of symmetric matrices. By working through this calculation, you will gain hands-on experience with the projection formula and see firsthand how an arbitrary element of a space is decomposed into its constituent parts.", "problem": "Consider the vector space $V = M_{2 \\times 2}(\\mathbb{R})$ of all real $2 \\times 2$ matrices. This space is equipped with the Frobenius inner product, defined for any two matrices $A, B \\in V$ as $\\langle A, B \\rangle = \\mathrm{tr}(A^T B)$, where $\\mathrm{tr}(\\cdot)$ denotes the trace of a matrix.\n\nLet $W$ be the subspace of $V$ consisting of all symmetric matrices (i.e., matrices $S$ such that $S^T = S$). By the projection theorem, any matrix $M \\in V$ can be uniquely written as a sum $M = M_W + M_{W^\\perp}$, where $M_W$ is the orthogonal projection of $M$ onto the subspace $W$, and $M_{W^\\perp}$ is the component in the orthogonal complement of $W$.\n\nGiven the matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$, determine its orthogonal projection $A_W$ onto the subspace $W$.", "solution": "The problem asks for the orthogonal projection of the matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$ onto the subspace $W$ of symmetric $2 \\times 2$ matrices, within the inner product space $V = M_{2 \\times 2}(\\mathbb{R})$ with the Frobenius inner product $\\langle A, B \\rangle = \\mathrm{tr}(A^T B)$.\n\nThe orthogonal projection $A_W$ of $A$ onto $W$ is given by the formula $A_W = \\sum_{i=1}^{k} \\langle A, u_i \\rangle u_i$, where $\\{u_1, u_2, \\dots, u_k\\}$ is an orthonormal basis for the subspace $W$.\n\nFirst, we need to find an orthonormal basis for $W$. A general symmetric $2 \\times 2$ matrix has the form $S = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$, where $a, b, c \\in \\mathbb{R}$. We can write this as a linear combination of basis matrices:\n$S = a \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + c \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + b \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\nSo, a basis for $W$ is given by the set $\\{B_1, B_2, B_3\\}$, where\n$B_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$, $B_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $B_3 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n\nNext, we check if this basis is orthogonal by computing the inner products between distinct basis vectors.\n$\\langle B_1, B_2 \\rangle = \\mathrm{tr}(B_1^T B_2) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\\right) = 0$.\n$\\langle B_1, B_3 \\rangle = \\mathrm{tr}(B_1^T B_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\right) = 0$.\n$\\langle B_2, B_3 \\rangle = \\mathrm{tr}(B_2^T B_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\\right) = 0$.\nThe basis is indeed orthogonal. Now we find the norm of each basis vector to normalize them. The squared norm is $\\|B\\|^2 = \\langle B, B \\rangle$.\n$\\|B_1\\|^2 = \\langle B_1, B_1 \\rangle = \\mathrm{tr}(B_1^T B_1) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}^2\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\\right) = 1$. So, $\\|B_1\\|=1$.\n$\\|B_2\\|^2 = \\langle B_2, B_2 \\rangle = \\mathrm{tr}(B_2^T B_2) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}^2\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = 1$. So, $\\|B_2\\|=1$.\n$\\|B_3\\|^2 = \\langle B_3, B_3 \\rangle = \\mathrm{tr}(B_3^T B_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}^2\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = 2$. So, $\\|B_3\\|=\\sqrt{2}$.\n\nAn orthonormal basis $\\{u_1, u_2, u_3\\}$ for $W$ is therefore:\n$u_1 = \\frac{B_1}{\\|B_1\\|} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n$u_2 = \\frac{B_2}{\\|B_2\\|} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n$u_3 = \\frac{B_3}{\\|B_3\\|} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n\nNow we compute the inner products of $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$ with these orthonormal basis vectors.\n$\\langle A, u_1 \\rangle = \\mathrm{tr}(A^T u_1) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 0 \\\\ 2 & 0 \\end{pmatrix}\\right) = 1$.\n$\\langle A, u_2 \\rangle = \\mathrm{tr}(A^T u_2) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0 & 3 \\\\ 0 & 4 \\end{pmatrix}\\right) = 4$.\n$\\langle A, u_3 \\rangle = \\mathrm{tr}(A^T u_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\right) = \\frac{1}{\\sqrt{2}}\\mathrm{tr}\\left(\\begin{pmatrix} 3 & 1 \\\\ 4 & 2 \\end{pmatrix}\\right) = \\frac{3+2}{\\sqrt{2}} = \\frac{5}{\\sqrt{2}}$.\n\nFinally, we construct the projection $A_W$:\n$A_W = \\langle A, u_1 \\rangle u_1 + \\langle A, u_2 \\rangle u_2 + \\langle A, u_3 \\rangle u_3$\n$A_W = (1) \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + (4) \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\left(\\frac{5}{\\sqrt{2}}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\right)$\n$A_W = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\frac{5}{2}\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$\n$A_W = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 0 & \\frac{5}{2} \\\\ \\frac{5}{2} & 0 \\end{pmatrix}$\n$A_W = \\begin{pmatrix} 1 & \\frac{5}{2} \\\\ \\frac{5}{2} & 4 \\end{pmatrix}$.\n\nThis is the component of $A$ that lies in the subspace of symmetric matrices.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 1 & \\frac{5}{2} \\\\ \\frac{5}{2} & 4 \\end{pmatrix}\n}\n$$", "id": "1858280"}, {"introduction": "The power of the projection theorem extends far beyond matrix algebra into the realm of function spaces, which is central to functional analysis. This problem asks you to apply the same principles of orthogonal projection to the space of polynomials, $P_2([0,1])$, equipped with an integral-based inner product. Solving this will solidify your understanding of how these geometric concepts generalize, allowing you to find the \"closest\" function in a subspace to a given function, a technique with wide applications in approximation theory and signal processing.", "problem": "Consider the vector space $V = P_2([0,1])$ consisting of all polynomials of degree at most 2 with real coefficients, defined on the interval $[0,1]$. This space is equipped with the inner product given by $\\langle f, g \\rangle = \\int_0^1 f(x)g(x)dx$. Let $U$ be the one-dimensional subspace of $V$ spanned by the polynomial $u(x) = x$. According to the projection theorem, any polynomial $p(x) \\in V$ can be uniquely decomposed as $p(x) = p_U(x) + p_{U^\\perp}(x)$, where $p_U(x) \\in U$ and $p_{U^\\perp}(x)$ is in the orthogonal complement of $U$, denoted $U^\\perp$.\n\nGiven the polynomial $p(x) = 6x^2 - 5x + 1$, determine its component $p_U(x)$ that lies in the subspace $U$. Your final answer should be a polynomial in the variable $x$.", "solution": "We seek the orthogonal projection of $p(x) = 6x^{2} - 5x + 1$ onto the one-dimensional subspace $U = \\operatorname{span}\\{u\\}$ with $u(x) = x$ under the inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(x)g(x)\\,dx$. By the projection theorem, the component in $U$ is\n$$\np_{U}(x) = \\frac{\\langle p, u \\rangle}{\\langle u, u \\rangle}\\,u(x).\n$$\nCompute the denominator:\n$$\n\\langle u, u \\rangle = \\int_{0}^{1} x \\cdot x \\, dx = \\int_{0}^{1} x^{2}\\,dx = \\left.\\frac{x^{3}}{3}\\right|_{0}^{1} = \\frac{1}{3}.\n$$\nCompute the numerator:\n$$\n\\langle p, u \\rangle = \\int_{0}^{1} (6x^{2} - 5x + 1)\\,x\\,dx = \\int_{0}^{1} \\left(6x^{3} - 5x^{2} + x\\right)\\,dx.\n$$\nEvaluate term by term:\n$$\n\\int_{0}^{1} 6x^{3}\\,dx = \\left.6\\cdot \\frac{x^{4}}{4}\\right|_{0}^{1} = \\frac{3}{2}, \\quad \\int_{0}^{1} (-5x^{2})\\,dx = \\left.-5\\cdot \\frac{x^{3}}{3}\\right|_{0}^{1} = -\\frac{5}{3}, \\quad \\int_{0}^{1} x\\,dx = \\left.\\frac{x^{2}}{2}\\right|_{0}^{1} = \\frac{1}{2}.\n$$\nThus,\n$$\n\\langle p, u \\rangle = \\frac{3}{2} - \\frac{5}{3} + \\frac{1}{2} = 2 - \\frac{5}{3} = \\frac{1}{3}.\n$$\nTherefore, the projection coefficient is\n$$\n\\frac{\\langle p, u \\rangle}{\\langle u, u \\rangle} = \\frac{\\frac{1}{3}}{\\frac{1}{3}} = 1,\n$$\nand hence\n$$\np_{U}(x) = 1 \\cdot x = x.\n$$", "answer": "$$\\boxed{x}$$", "id": "1858276"}]}