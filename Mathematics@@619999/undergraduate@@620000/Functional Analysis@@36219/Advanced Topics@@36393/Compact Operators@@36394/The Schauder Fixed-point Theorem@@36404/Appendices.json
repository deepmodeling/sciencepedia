{"hands_on_practices": [{"introduction": "This first practice serves as a concrete starting point for understanding fixed-point problems. We will analyze an integral operator whose simple structure allows us to deduce the functional form of any potential fixed point. This exercise demonstrates how, in certain cases, a fixed-point equation can be solved directly through substitution and basic calculus, providing a tangible example before we explore more abstract methods. [@problem_id:1900365]", "problem": "Consider the space of continuous real-valued functions on the interval $[0, 1]$, denoted by $C([0,1])$. An integral operator $T$ is defined to act on any function $f \\in C([0,1])$ as follows:\n$$\n(Tf)(x) = \\frac{15}{4} \\int_0^1 (1-x^2) y^2 (f(y))^3 dy\n$$\nA function $f$ is a fixed point of the operator $T$ if it satisfies the equation $Tf = f$. This operator is known to have three distinct fixed points. One is the trivial fixed point, $f_0(x) = 0$ for all $x \\in [0,1]$. Another fixed point, let's call it $f_p(x)$, is non-negative for all $x \\in [0,1]$ and is not identically zero.\n\nDetermine the maximum value attained by this non-negative, non-trivial fixed point $f_p(x)$ on the interval $[0,1]$. Present your answer as a closed-form analytic expression.", "solution": "For any $f \\in C([0,1])$, the operator is\n$$\n(Tf)(x) = \\frac{15}{4}\\int_{0}^{1} (1 - x^{2})\\, y^{2} (f(y))^{3} \\, dy.\n$$\nThe $x$-dependence factors out as\n$$\n(Tf)(x) = (1 - x^{2}) \\cdot \\frac{15}{4} \\int_{0}^{1} y^{2} (f(y))^{3} \\, dy.\n$$\nHence any fixed point $f$ satisfying $Tf = f$ must be of the form\n$$\nf(x) = A(1 - x^{2}),\n$$\nfor some constant $A \\in \\mathbb{R}$. Substituting $f(x) = A(1 - x^{2})$ into the definition of $T$ gives\n$$\n(Tf)(x) = (1 - x^{2}) \\cdot \\frac{15}{4} \\int_{0}^{1} y^{2} \\left(A(1 - y^{2})\\right)^{3} dy\n= (1 - x^{2}) \\cdot \\frac{15}{4} A^{3} \\int_{0}^{1} y^{2} (1 - y^{2})^{3} dy.\n$$\nThe fixed point condition $Tf = f$ becomes\n$$\nA(1 - x^{2}) = (1 - x^{2}) \\cdot \\frac{15}{4} A^{3} \\int_{0}^{1} y^{2} (1 - y^{2})^{3} dy.\n$$\nFor $x \\in [0,1)$ we cancel $(1 - x^{2})$ and obtain\n$$\nA = \\frac{15}{4} A^{3} I, \\quad \\text{where } I = \\int_{0}^{1} y^{2} (1 - y^{2})^{3} dy.\n$$\nThus either $A = 0$ or $A^{2} = \\frac{4}{15 I}$. Compute $I$ by expansion:\n$$\n(1 - y^{2})^{3} = 1 - 3y^{2} + 3y^{4} - y^{6},\n$$\nso\n$$\nI = \\int_{0}^{1} \\left(y^{2} - 3y^{4} + 3y^{6} - y^{8}\\right) dy\n= \\left[\\frac{y^{3}}{3} - 3\\frac{y^{5}}{5} + 3\\frac{y^{7}}{7} - \\frac{y^{9}}{9}\\right]_{0}^{1}\n= \\frac{1}{3} - \\frac{3}{5} + \\frac{3}{7} - \\frac{1}{9}.\n$$\nEvaluating,\n$$\nI = \\frac{1}{3} - \\frac{3}{5} + \\frac{3}{7} - \\frac{1}{9} = \\frac{16}{315}.\n$$\nHence\n$$\nA^{2} = \\frac{4}{15 \\cdot \\frac{16}{315}} = \\frac{4}{\\frac{240}{315}} = 4 \\cdot \\frac{315}{240} = \\frac{21}{4}.\n$$\nTherefore the three fixed points are $A=0$ and $A=\\pm \\frac{\\sqrt{21}}{2}$. The non-negative, non-trivial fixed point is\n$$\nf_{p}(x) = \\frac{\\sqrt{21}}{2} (1 - x^{2}).\n$$\nOn $[0,1]$, the function $1 - x^{2}$ attains its maximum value $1$ at $x=0$, so the maximum of $f_{p}$ is\n$$\n\\max_{x \\in [0,1]} f_{p}(x) = \\frac{\\sqrt{21}}{2}.\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{21}}{2}}$$", "id": "1900365"}, {"introduction": "Building upon direct methods, this problem introduces a more sophisticated technique for tackling integral equations. You will learn how to convert a fixed-point problem for an integral operator into a more familiar ordinary differential equation (ODE). This powerful strategy showcases the interplay between different areas of analysis and is a common tool in the study of functional equations. [@problem_id:1900343]", "problem": "Consider the operator $T$ acting on the space $C([0,1])$ of continuous real-valued functions on the interval $[0,1]$. For a function $f \\in C([0,1])$, the operator is defined as:\n$$ (Tf)(x) = \\min\\left\\{1, x + \\int_0^x [f(t)]^2 dt\\right\\} $$\nfor all $x \\in [0,1]$.\n\nThis operator possesses a unique fixed point, denoted by $f_*(x)$, within the space $C([0,1])$. A fixed point is a function that satisfies the equation $T(f_*) = f_*$.\n\nYour task is to determine the value of the definite integral of this fixed-point function over its domain. Calculate the value of $\\int_0^1 f_*(x) dx$. Your answer should be a closed-form analytic expression.", "solution": "We seek a fixed point $f_{*} \\in C([0,1])$ satisfying\n$$\nf_{*}(x)=\\min\\left\\{1,\\, x+\\int_{0}^{x} f_{*}(t)^{2}\\, dt\\right\\}\\quad \\text{for all } x\\in[0,1].\n$$\nDefine $g(x)=x+\\int_{0}^{x} f_{*}(t)^{2}\\, dt$. Since $f_{*}$ is continuous, $g$ is continuously differentiable with\n$$\ng'(x)=1+f_{*}(x)^{2}.\n$$\nAt $x=0$, we have $f_{*}(0)=\\min\\{1,0\\}=0$. On any subinterval where $f_{*}(x)<1$, the minimum is attained by $g(x)$, so $f_{*}(x)=g(x)$ there and hence $f_{*}$ satisfies the differential equation\n$$\nf_{*}'(x)=1+f_{*}(x)^{2},\\qquad f_{*}(0)=0.\n$$\nThis separable ODE yields\n$$\n\\int_{0}^{f_{*}(x)} \\frac{du}{1+u^{2}}=\\int_{0}^{x} ds \\quad \\Longrightarrow \\quad \\arctan(f_{*}(x))=x,\n$$\nso\n$$\nf_{*}(x)=\\tan(x)\n$$\nas long as $f_{*}(x)<1$. The first point where this reaches $1$ is determined by $\\tan(a)=1$, i.e.,\n$$\na=\\arctan(1)=\\frac{\\pi}{4}.\n$$\nWe check consistency at $x=a$. On $[0,a]$, $f_{*}(t)=\\tan t$, so\n$$\n\\int_{0}^{a} f_{*}(t)^{2}\\, dt=\\int_{0}^{a} \\tan^{2} t\\, dt=\\int_{0}^{a} (\\sec^{2} t-1)\\, dt=[\\tan t]_{0}^{a}-a=\\tan a-a=1-\\frac{\\pi}{4}.\n$$\nHence\n$$\ng(a)=a+\\int_{0}^{a} f_{*}(t)^{2}\\, dt=\\frac{\\pi}{4}+\\left(1-\\frac{\\pi}{4}\\right)=1,\n$$\nso $f_{*}(a)=\\min\\{1,1\\}=1$, agreeing with $\\tan(\\pi/4)=1$.\n\nFor $x\\geq a$, set $f_{*}(x)=1$ and verify the fixed-point relation. Using the piecewise form,\n$$\n\\int_{0}^{x} f_{*}(t)^{2}\\, dt=\\int_{0}^{a} \\tan^{2} t\\, dt+\\int_{a}^{x} 1\\, dt=\\left(1-\\frac{\\pi}{4}\\right)+(x-a)=x+1-2a,\n$$\nso\n$$\ng(x)=x+\\int_{0}^{x} f_{*}(t)^{2}\\, dt=2x+1-2a.\n$$\nFor $x\\geq a$, we have $g(x)\\geq 1$, hence $f_{*}(x)=\\min\\{1,g(x)\\}=1$, confirming self-consistency. Therefore,\n$$\nf_{*}(x)=\\begin{cases}\n\\tan x, & 0\\leq x\\leq \\frac{\\pi}{4},\\\\\n1, & \\frac{\\pi}{4}\\leq x\\leq 1.\n\\end{cases}\n$$\n\nNow compute the required integral:\n$$\n\\int_{0}^{1} f_{*}(x)\\, dx=\\int_{0}^{\\pi/4} \\tan x\\, dx+\\int_{\\pi/4}^{1} 1\\, dx.\n$$\nUsing $\\int \\tan x\\, dx=-\\ln(\\cos x)$,\n$$\n\\int_{0}^{\\pi/4} \\tan x\\, dx=\\left[-\\ln(\\cos x)\\right]_{0}^{\\pi/4}=-\\ln\\!\\left(\\cos\\!\\left(\\frac{\\pi}{4}\\right)\\right)+\\ln(\\cos 0)=-\\ln\\!\\left(\\frac{\\sqrt{2}}{2}\\right)=\\frac{1}{2}\\ln 2,\n$$\nand\n$$\n\\int_{\\pi/4}^{1} 1\\, dx=1-\\frac{\\pi}{4}.\n$$\nTherefore,\n$$\n\\int_{0}^{1} f_{*}(x)\\, dx=\\frac{1}{2}\\ln 2+1-\\frac{\\pi}{4}.\n$$", "answer": "$$\\boxed{1-\\frac{\\pi}{4}+\\frac{1}{2}\\ln 2}$$", "id": "1900343"}, {"introduction": "Our final exercise shifts the focus from finding an explicit solution to proving its existence, which lies at the heart of the Schauder theorem's utility. We will transform a complex system of equations for polynomial coefficients into a fixed-point problem in a finite-dimensional space. By applying Brouwer's Fixed-Point Theorem, the finite-dimensional foundation of Schauder's theorem, we can guarantee a solution exists without ever needing to calculate it. [@problem_id:1900335]", "problem": "For any given non-negative integer $N$, let $P_N$ be the vector space of all polynomials with real coefficients of degree at most $N$. A polynomial $p(x) \\in P_N$ can be uniquely represented by its vector of coefficients $c = (c_0, c_1, \\dots, c_N)$, where $p(x) = \\sum_{k=0}^{N} c_k x^k$.\n\nConsider the problem of finding a polynomial $p(x)$ in $P_N$ whose coefficients $c_k$ are determined by the following system of $N+1$ integral equations:\n$$c_k = \\int_0^1 \\sin(p(t)) \\, t^k \\, dt \\quad \\text{for } k = 0, 1, \\dots, N$$\nwhere $\\sin(p(t))$ denotes the sine of the value of the polynomial $p(x)$ evaluated at $x=t$.\n\nWhich of the following statements about the existence of such a polynomial is correct?\n\nA. Such a polynomial exists for any non-negative integer $N$.\n\nB. Such a polynomial exists only if $N$ is an even number.\n\nC. Such a polynomial exists only if $N$ is an odd number.\n\nD. Such a polynomial exists only for $N=0$ and $N=1$.\n\nE. No such polynomial exists for any non-negative integer $N$.", "solution": "We recast the problem as a fixed point problem in a finite-dimensional space. For any $c=(c_{0},\\dots,c_{N})\\in \\mathbb{R}^{N+1}$, define the polynomial\n$$\np_{c}(t)=\\sum_{k=0}^{N} c_{k} t^{k}.\n$$\nDefine a map $F:\\mathbb{R}^{N+1}\\to \\mathbb{R}^{N+1}$ by its components\n$$\nF_{k}(c)=\\int_{0}^{1} \\sin\\!\\big(p_{c}(t)\\big)\\, t^{k}\\, dt,\\quad k=0,1,\\dots,N.\n$$\nA solution to the given system is precisely a fixed point $c$ of $F$, i.e., $c=F(c)$.\n\nFirst, $F$ is continuous. Indeed, if $c^{(m)}\\to c$ in $\\mathbb{R}^{N+1}$, then for all $t\\in[0,1]$,\n$$\n|p_{c^{(m)}}(t)-p_{c}(t)|=\\left|\\sum_{k=0}^{N}\\big(c^{(m)}_{k}-c_{k}\\big) t^{k}\\right|\\leq \\sum_{k=0}^{N}\\big|c^{(m)}_{k}-c_{k}\\big| \\to 0,\n$$\nuniformly in $t$. Using $|\\sin u-\\sin v|\\leq |u-v|$ for all $u,v\\in\\mathbb{R}$, we get uniform convergence $\\sin(p_{c^{(m)}}(t))\\to \\sin(p_{c}(t))$ on $[0,1]$. Therefore, for each $k$,\n$$\nF_{k}(c^{(m)})=\\int_{0}^{1} \\sin\\!\\big(p_{c^{(m)}}(t)\\big)\\, t^{k}\\, dt \\to \\int_{0}^{1} \\sin\\!\\big(p_{c}(t)\\big)\\, t^{k}\\, dt=F_{k}(c),\n$$\nso $F$ is continuous.\n\nSecond, the image of $F$ is uniformly bounded, independently of $c$. For each $k$,\n$$\n|F_{k}(c)|=\\left|\\int_{0}^{1} \\sin\\!\\big(p_{c}(t)\\big)\\, t^{k}\\, dt\\right|\\leq \\int_{0}^{1} |\\sin\\!\\big(p_{c}(t)\\big)|\\, t^{k}\\, dt \\leq \\int_{0}^{1} t^{k}\\, dt=\\frac{1}{k+1}.\n$$\nDefine the box\n$$\nK=\\prod_{k=0}^{N}\\left[-\\frac{1}{k+1},\\, \\frac{1}{k+1}\\right]\\subset \\mathbb{R}^{N+1}.\n$$\nThis set $K$ is nonempty, compact, and convex, and for every $c\\in \\mathbb{R}^{N+1}$ we have $F(c)\\in K$. Hence the restriction $G=F|_{K}:K\\to K$ is a continuous self-map of $K$.\n\nBy Brouwerâ€™s fixed point theorem, there exists $c^{*}\\in K$ such that $G(c^{*})=c^{*}$, i.e., $F(c^{*})=c^{*}$. Letting $p^{*}(x)=\\sum_{k=0}^{N} c^{*}_{k} x^{k}$, we obtain\n$$\nc^{*}_{k}=\\int_{0}^{1} \\sin\\!\\big(p^{*}(t)\\big)\\, t^{k}\\, dt,\\quad k=0,1,\\dots,N,\n$$\nwhich is exactly the required system. Therefore, such a polynomial exists for every non-negative integer $N$.\n\nThus, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1900335"}]}