{"hands_on_practices": [{"introduction": "Understanding a new concept often begins with applying it to a familiar structure. This exercise invites you to explore the definition of a finite-rank operator within the concrete setting of the Hilbert space $\\ell^2$. By examining a diagonal operator, which acts like an infinite-dimensional diagonal matrix, you will determine the precise condition on its defining sequence $(\\lambda_n)$ that constrains its range to be finite-dimensional [@problem_id:1849791]. This practice not only solidifies the definition of rank but also helps build intuition for the distinction between finite-rank operators and the broader class of compact operators.", "problem": "Let $\\ell^2$ be the Hilbert space of all sequences $x = (x_n)_{n=1}^\\infty$ of complex numbers such that $\\sum_{n=1}^\\infty |x_n|^2 < \\infty$. Consider a linear operator $D: \\ell^2 \\to \\ell^2$, known as a diagonal operator, which is defined by the action $D(x) = (\\lambda_n x_n)_{n=1}^\\infty$ for a given bounded sequence of complex scalars $(\\lambda_n)_{n=1}^\\infty$.\n\nA linear operator $T$ is said to be of finite rank if its range, $\\text{ran}(T)$, is a finite-dimensional vector subspace.\n\nWhich one of the following conditions on the sequence $(\\lambda_n)$ is both necessary and sufficient for the operator $D$ to be a finite-rank operator?\n\nA. The sequence $(\\lambda_n)$ converges to zero, i.e., $\\lim_{n \\to \\infty} \\lambda_n = 0$.\n\nB. The sequence $(\\lambda_n)$ is absolutely summable, i.e., $\\sum_{n=1}^\\infty |\\lambda_n| < \\infty$.\n\nC. The set of indices for which $\\lambda_n$ is non-zero is finite, i.e., the set $\\{ n \\in \\mathbb{N} \\mid \\lambda_n \\neq 0 \\}$ has a finite number of elements.\n\nD. The sequence $(\\lambda_n)$ is constant, i.e., $\\lambda_n = c$ for all $n \\in \\mathbb{N}$ and for some constant $c \\in \\mathbb{C}$.\n\nE. The supremum of the absolute values of the terms is less than one, i.e., $\\sup_{n \\in \\mathbb{N}} |\\lambda_n| < 1$.", "solution": "Let $\\ell^{2}$ be the Hilbert space with canonical orthonormal basis $\\{e_{n}\\}_{n=1}^{\\infty}$, where $e_{n}$ has $1$ in the $n$-th coordinate and $0$ elsewhere. The diagonal operator $D$ defined by $D(x)=(\\lambda_{n}x_{n})_{n=1}^{\\infty}$ satisfies\n$$\nD(e_{n})=\\lambda_{n}e_{n}\\quad\\text{for each }n\\in\\mathbb{N}.\n$$\nRecall that $D$ is finite rank if and only if $\\dim(\\operatorname{ran}D)<\\infty$.\n\nSufficiency of C: Suppose the set $S=\\{n\\in\\mathbb{N}:\\lambda_{n}\\neq 0\\}$ is finite, say $S=\\{n_{1},\\dots,n_{m}\\}$. For any $x=\\sum_{n=1}^{\\infty}x_{n}e_{n}\\in\\ell^{2}$,\n$$\nD(x)=\\sum_{n=1}^{\\infty}\\lambda_{n}x_{n}e_{n}=\\sum_{j=1}^{m}\\lambda_{n_{j}}x_{n_{j}}e_{n_{j}}.\n$$\nHence $\\operatorname{ran}D\\subseteq\\operatorname{span}\\{e_{n_{1}},\\dots,e_{n_{m}}\\}$, which is $m$-dimensional. Therefore $D$ is finite rank.\n\nNecessity of C: Suppose instead that $S=\\{n\\in\\mathbb{N}:\\lambda_{n}\\neq 0\\}$ is infinite. Then for each $n\\in S$, $D(e_{n})=\\lambda_{n}e_{n}\\neq 0$. Consider any finite linear combination\n$$\n\\sum_{j=1}^{k}c_{j}D(e_{n_{j}})=\\sum_{j=1}^{k}c_{j}\\lambda_{n_{j}}e_{n_{j}}=0.\n$$\nBy linear independence of $\\{e_{n}\\}$ and the fact that each $\\lambda_{n_{j}}\\neq 0$, it follows that $c_{j}=0$ for all $j$. Therefore $\\{D(e_{n}):n\\in S\\}$ is an infinite linearly independent subset of $\\operatorname{ran}D$, which implies $\\dim(\\operatorname{ran}D)=\\infty$. Hence $D$ is not finite rank. Thus $D$ is finite rank if and only if only finitely many $\\lambda_{n}$ are nonzero.\n\nVerification against alternatives:\n- A: $\\lim_{n\\to\\infty}\\lambda_{n}=0$ is not sufficient; for example $\\lambda_{n}=1/n$ gives infinite rank. It is implied by C but does not imply C.\n- B: $\\sum_{n=1}^{\\infty}|\\lambda_{n}|<\\infty$ is not sufficient; for example $\\lambda_{n}=2^{-n}$ yields infinite rank. It is implied by C but does not imply C.\n- D: If $\\lambda_{n}=c\\neq 0$, then $\\operatorname{ran}D=\\ell^{2}$ is infinite dimensional; only the trivial case $c=0$ is finite rank, so this condition is neither necessary nor sufficient.\n- E: $\\sup_{n}|\\lambda_{n}|<1$ only asserts boundedness and does not control rank; it is neither necessary nor sufficient.\n\nTherefore, the necessary and sufficient condition is that only finitely many $\\lambda_{n}$ are nonzero, i.e., option C.", "answer": "$$\\boxed{C}$$", "id": "1849791"}, {"introduction": "Finite-rank operators are constructed from simpler, fundamental building blocks known as rank-one operators. This exercise focuses on the canonical rank-one operator, defined by $T(x) = \\langle x, u \\rangle v$, which maps every vector to a multiple of a single vector $v$. Your task is to compute the Hilbert-adjoint $T^*$, a core skill in operator theory that reveals deep structural properties [@problem_id:1849827]. This calculation is an essential workout in manipulating the inner product and understanding how the adjoint operation acts on these elementary operators.", "problem": "Let $H$ be a complex Hilbert space with an inner product $\\langle \\cdot, \\cdot \\rangle$, which is defined to be linear in its first argument and conjugate-linear in its second argument. Let $u$ and $v$ be two fixed non-zero vectors in $H$. Consider the linear operator $T: H \\to H$ defined for any vector $x \\in H$ by the action:\n$$T(x) = \\langle x, u \\rangle v$$\nThe Hilbert-adjoint of $T$, denoted as $T^*$, is the unique operator on $H$ that satisfies the relation $\\langle T(x), y \\rangle = \\langle x, T^*(y) \\rangle$ for all vectors $x, y \\in H$.\n\nWhich of the following expressions correctly defines the action of the Hilbert-adjoint operator $T^*$ on an arbitrary vector $y \\in H$?\n\nA. $T^*(y) = \\langle y, v \\rangle u$\n\nB. $T^*(y) = \\langle y, u \\rangle v$\n\nC. $T^*(y) = \\langle u, y \\rangle v$\n\nD. $T^*(y) = \\langle v, y \\rangle u$\n\nE. $T^*(y) = \\langle u, x \\rangle v$", "solution": "The Hilbert-adjoint operator $T^*$ of a linear operator $T$ on a Hilbert space $H$ is defined by the property $\\langle T(x), y \\rangle = \\langle x, T^*(y) \\rangle$ for all $x, y \\in H$. Our goal is to find the explicit form of $T^*(y)$.\n\nWe begin by evaluating the left-hand side of the defining equation using the given form of $T(x) = \\langle x, u \\rangle v$.\n$$\n\\langle T(x), y \\rangle = \\langle \\left( \\langle x, u \\rangle v \\right), y \\rangle\n$$\nThe term $\\langle x, u \\rangle$ is a scalar (a complex number). Since the inner product is linear in its first argument, we can pull this scalar factor out of the inner product without modification.\n$$\n\\langle T(x), y \\rangle = \\langle x, u \\rangle \\langle v, y \\rangle\n$$\nNow we want to manipulate this expression into the form $\\langle x, \\dots \\rangle$ to identify $T^*(y)$. To do this, we can use the conjugate-symmetry property of the inner product, which states that $\\langle a, b \\rangle = \\overline{\\langle b, a \\rangle}$ for any vectors $a,b \\in H$. We apply this to the term $\\langle v, y \\rangle$.\n$$\n\\langle v, y \\rangle = \\overline{\\langle y, v \\rangle}\n$$\nSubstituting this back into our expression gives:\n$$\n\\langle T(x), y \\rangle = \\langle x, u \\rangle \\overline{\\langle y, v \\rangle}\n$$\nThe term $\\overline{\\langle y, v \\rangle}$ is also a scalar. We can bring it inside the first inner product, $\\langle x, u \\rangle$. Since the inner product is conjugate-linear in its second argument, a scalar $c$ enters the second argument as its complex conjugate $\\bar{c}$. Therefore, to bring the scalar $S = \\overline{\\langle y, v \\rangle}$ into the second argument of $\\langle x, u \\rangle$, we must insert its conjugate, $\\bar{S} = \\overline{\\overline{\\langle y, v \\rangle}} = \\langle y, v \\rangle$. This logic is slightly confusing. Let's use the property more directly.\n\nAn easier way is to use the conjugate-linearity in the second slot directly. For any scalar $c$ and vectors $a, b \\in H$, we have $\\langle a, c b \\rangle = \\bar{c} \\langle a, b \\rangle$. We wish to write $\\langle x, u \\rangle \\overline{\\langle y, v \\rangle}$ in the form $\\langle x, \\text{something} \\rangle$. We can rewrite the expression as:\n$$\n\\langle T(x), y \\rangle = \\overline{\\langle y, v \\rangle} \\langle x, u \\rangle\n$$\nLet the scalar be $c = \\langle y, v \\rangle$. Our expression is $\\bar{c} \\langle x, u \\rangle$. Using the property $\\bar{c} \\langle x, u \\rangle = \\langle x, c u \\rangle$, we can write:\n$$\n\\langle T(x), y \\rangle = \\langle x, (\\langle y, v \\rangle) u \\rangle\n$$\nWe have now successfully transformed the left-hand side into the required format. By comparing this result with the definition of the adjoint,\n$$\n\\langle T(x), y \\rangle = \\langle x, T^*(y) \\rangle\n$$\nwe can directly identify the action of $T^*$ on $y$. For the equality $\\langle x, (\\langle y, v \\rangle) u \\rangle = \\langle x, T^*(y) \\rangle$ to hold for all vectors $x \\in H$, the second arguments of the inner products must be equal. Therefore,\n$$\nT^*(y) = \\langle y, v \\rangle u\n$$\nThis matches option A.", "answer": "$$\\boxed{A}$$", "id": "1849827"}, {"introduction": "The theory of finite-rank operators finds powerful applications in the study of integral equations. This practice demonstrates this connection by examining an integral operator whose kernel is \"separable\" or \"degenerate\"â€”meaning it is a finite sum of products of functions of its variables. While the operator acts on the infinite-dimensional space $L^2([0,1])$, you will discover that its structure is fundamentally finite-dimensional [@problem_id:1849822]. This problem beautifully illustrates how a question about an operator on a function space can be reduced to a question about the rank of a finite matrix.", "problem": "Consider the Hilbert space $H = L^2([0,1])$ of square-integrable complex-valued functions on the interval $[0,1]$, equipped with the standard inner product $\\langle f, g \\rangle = \\int_0^1 f(t) \\overline{g(t)} dt$. Let $T: H \\to H$ be an integral operator defined by\n$$ (Tx)(t) = \\int_0^1 K(t,s) x(s) \\, ds $$\nwhere the kernel $K(t,s)$ is a polynomial in the variables $t$ and $s$ given by\n$$ K(t,s) = \\sum_{i=0}^{N} \\sum_{j=0}^{M} c_{ij} t^i s^j $$\nHere, $N$ and $M$ are non-negative integers, and the coefficients $c_{ij}$ are real constants. These coefficients form an $(N+1) \\times (M+1)$ matrix $C$ where the entry in the $i$-th row and $j$-th column is $c_{ij}$ (with indices $i$ from $0$ to $N$ and $j$ from $0$ to $M$). The rank of the operator $T$ is defined as the dimension of its range, $\\mathrm{dim}(\\mathrm{ran}(T))$.\n\nWhich of the following statements correctly describes the rank of the operator $T$?\n\nA. The rank of $T$ is always $(N+1)(M+1)$.\n\nB. The rank of $T$ is always $\\min(N+1, M+1)$.\n\nC. The rank of $T$ is equal to the rank of the matrix $C$.\n\nD. The rank of $T$ is equal to $N+M+2$.\n\nE. The rank of $T$ could be infinite if the matrix $C$ is chosen appropriately.", "solution": "Let $H=L^{2}([0,1])$ and let $T:H\\to H$ be given by\n$$\n(Tx)(t)=\\int_{0}^{1}K(t,s)x(s)\\,ds,\\quad K(t,s)=\\sum_{i=0}^{N}\\sum_{j=0}^{M}c_{ij}t^{i}s^{j}.\n$$\nFor any $x\\in H$, write the moments\n$$\nm_{j}(x)=\\int_{0}^{1}s^{j}x(s)\\,ds,\\quad j=0,\\dots,M.\n$$\nThen\n$$\n(Tx)(t)=\\sum_{i=0}^{N}\\sum_{j=0}^{M}c_{ij}t^{i}m_{j}(x)=\\sum_{i=0}^{N}\\Big(\\sum_{j=0}^{M}c_{ij}m_{j}(x)\\Big)t^{i}.\n$$\nHence $\\mathrm{ran}(T)\\subseteq\\mathrm{span}\\{1,t,\\dots,t^{N}\\}$, so $T$ has finite rank and, in particular, cannot have infinite rank.\n\nDefine linear maps\n$$\n\\Phi:H\\to\\mathbb{C}^{M+1},\\quad \\Phi(x)=(m_{0}(x),\\dots,m_{M}(x)),\n$$\n$$\nU:\\mathbb{C}^{N+1}\\to H,\\quad U(b_{0},\\dots,b_{N})=\\sum_{i=0}^{N}b_{i}t^{i},\n$$\nand view the coefficient matrix $C=(c_{ij})$ as a linear map $C:\\mathbb{C}^{M+1}\\to\\mathbb{C}^{N+1}$. Then\n$$\nT=U\\circ C\\circ\\Phi.\n$$\nWe claim that $\\Phi$ is surjective. Restrict $\\Phi$ to the finite-dimensional subspace $P_{M}=\\mathrm{span}\\{1,s,\\dots,s^{M}\\}\\subset H$. Relative to the basis $\\{s^{k}\\}_{k=0}^{M}$ of $P_{M}$ and the standard basis of $\\mathbb{C}^{M+1}$, the matrix of $\\Phi|_{P_{M}}$ is the Gram (Hilbert) matrix\n$$\nG_{jk}=\\int_{0}^{1}s^{j+k}\\,ds,\\quad j,k=0,\\dots,M.\n$$\nSince $\\{s^{k}\\}_{k=0}^{M}$ are linearly independent in $H$, this Gram matrix is positive definite and thus invertible. Therefore $\\Phi|_{P_{M}}:P_{M}\\to\\mathbb{C}^{M+1}$ is an isomorphism, which implies $\\Phi(H)=\\mathbb{C}^{M+1}$.\n\nIt follows that\n$$\n\\mathrm{Im}(C\\circ\\Phi)=\\mathrm{Im}(C),\n$$\nand hence\n$$\n\\mathrm{ran}(T)=U(\\mathrm{Im}(C)).\n$$\nThe map $U$ is injective: if $U(b)=0$ in $H$, then the polynomial $\\sum_{i=0}^{N}b_{i}t^{i}$ vanishes almost everywhere on $[0,1]$, hence is identically zero, so all $b_{i}=0$. Therefore $U$ preserves dimensions of subspaces of $\\mathbb{C}^{N+1}$, and we obtain\n$$\n\\mathrm{dim}(\\mathrm{ran}(T))=\\mathrm{dim}(\\mathrm{Im}(C))=\\mathrm{rank}(C).\n$$\nThus the rank of $T$ is equal to the rank of the matrix $C$. Consequently, among the given options, only statement C is correct; in particular, the rank is finite and bounded by $\\min(N+1,M+1)$, so none of A, B, D, or E holds in general.", "answer": "$$\\boxed{C}$$", "id": "1849822"}]}