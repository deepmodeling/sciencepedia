{"hands_on_practices": [{"introduction": "This first practice grounds the abstract concept of an operator's graph in the familiar territory of Euclidean space. By treating a simple rotation operator on $\\mathbb{R}^2$ and viewing its graph as a subspace of $\\mathbb{R}^4$, you will gain concrete geometric intuition for how an operator's action is encoded in its graph [@problem_id:1892191]. This exercise reinforces fundamental linear algebra skills and provides a visual foundation for more abstract concepts to come.", "problem": "Consider the vector space $\\mathbb{R}^2$ equipped with the standard Euclidean inner product. Let $R_\\theta: \\mathbb{R}^2 \\to \\mathbb{R}^2$ be the linear operator that rotates any vector $\\vec{v} = (x_1, x_2)$ counter-clockwise by a fixed angle $\\theta$. The action of this operator is given by $R_\\theta(x_1, x_2) = (x_1\\cos\\theta - x_2\\sin\\theta, x_1\\sin\\theta + x_2\\cos\\theta)$.\n\nThe graph of this operator, denoted $G(R_\\theta)$, is the set of all points of the form $(\\vec{v}, R_\\theta(\\vec{v}))$ in the product space $\\mathbb{R}^2 \\times \\mathbb{R}^2$. By identifying $\\mathbb{R}^2 \\times \\mathbb{R}^2$ with $\\mathbb{R}^4$, the graph $G(R_\\theta)$ can be viewed as a 2-dimensional linear subspace of $\\mathbb{R}^4$.\n\nFind the $2 \\times 4$ matrix whose rows form an orthonormal basis for the subspace $G(R_\\theta) \\subset \\mathbb{R}^4$. To ensure a unique answer, the two basis vectors (rows of the matrix), $\\vec{u}_1$ and $\\vec{u}_2$, must be ordered such that the first non-zero component of $\\vec{u}_1$ appears at a lower index than the first non-zero component of $\\vec{u}_2$. Furthermore, the first non-zero component of each basis vector must be positive.", "solution": "The problem asks for an orthonormal basis for the graph of the rotation operator $R_\\theta: \\mathbb{R}^2 \\to \\mathbb{R}^2$. The graph, $G(R_\\theta)$, is a subset of $\\mathbb{R}^4$.\n\nStep 1: Characterize the vectors in the graph $G(R_\\theta)$.\nAn element of the graph $G(R_\\theta)$ is a vector in $\\mathbb{R}^4$ of the form $(\\vec{v}, R_\\theta(\\vec{v}))$, where $\\vec{v} = (x_1, x_2) \\in \\mathbb{R}^2$. Let the coordinates in $\\mathbb{R}^4$ be $(z_1, z_2, z_3, z_4)$.\nWe identify $(z_1, z_2)$ with the input vector $\\vec{v}=(x_1, x_2)$ and $(z_3, z_4)$ with the output vector $R_\\theta(\\vec{v})$.\nSo, a general vector $\\vec{z} \\in G(R_\\theta)$ can be written as:\n$$\n\\vec{z} = (x_1, x_2, x_1\\cos\\theta - x_2\\sin\\theta, x_1\\sin\\theta + x_2\\cos\\theta)\n$$\nwhere $x_1$ and $x_2$ are arbitrary real numbers.\n\nStep 2: Find a basis for the subspace $G(R_\\theta)$.\nThe expression for a general vector $\\vec{z}$ in the graph shows that it is a linear combination of two vectors, with coefficients being the free parameters $x_1$ and $x_2$. We can separate the terms containing $x_1$ and $x_2$:\n$$\n\\vec{z} = x_1 (1, 0, \\cos\\theta, \\sin\\theta) + x_2 (0, 1, -\\sin\\theta, \\cos\\theta)\n$$\nThis means that the subspace $G(R_\\theta)$ is spanned by the two vectors:\n$$\n\\vec{b}_1 = (1, 0, \\cos\\theta, \\sin\\theta)\n$$\n$$\n\\vec{b}_2 = (0, 1, -\\sin\\theta, \\cos\\theta)\n$$\nThese two vectors are linearly independent. To see this, consider a linear combination $c_1\\vec{b}_1 + c_2\\vec{b}_2 = \\vec{0}$. This gives the vector equation $(c_1, c_2, \\dots) = (0, 0, 0, 0)$, which immediately implies $c_1=0$ and $c_2=0$. Therefore, $\\{\\vec{b}_1, \\vec{b}_2\\}$ forms a basis for $G(R_\\theta)$.\n\nStep 3: Orthonormalize the basis.\nWe will use the Gram-Schmidt process to find an orthonormal basis $\\{\\vec{u}_1, \\vec{u}_2\\}$ from the basis $\\{\\vec{b}_1, \\vec{b}_2\\}$. The inner product is the standard dot product in $\\mathbb{R}^4$.\n\nFirst, let's check if the basis vectors $\\vec{b}_1$ and $\\vec{b}_2$ are already orthogonal:\n$$\n\\vec{b}_1 \\cdot \\vec{b}_2 = (1)(0) + (0)(1) + (\\cos\\theta)(-\\sin\\theta) + (\\sin\\theta)(\\cos\\theta) = 0 + 0 - \\sin\\theta\\cos\\theta + \\sin\\theta\\cos\\theta = 0\n$$\nSince their dot product is zero, the vectors are orthogonal. The Gram-Schmidt process simplifies to just normalizing each vector.\n\nNext, we calculate the norm (magnitude) of each basis vector:\n$$\n\\|\\vec{b}_1\\|^2 = 1^2 + 0^2 + (\\cos\\theta)^2 + (\\sin\\theta)^2 = 1 + (\\cos^2\\theta + \\sin^2\\theta) = 1 + 1 = 2\n$$\nSo, $\\|\\vec{b}_1\\| = \\sqrt{2}$.\n\n$$\n\\|\\vec{b}_2\\|^2 = 0^2 + 1^2 + (-\\sin\\theta)^2 + (\\cos\\theta)^2 = 1 + (\\sin^2\\theta + \\cos^2\\theta) = 1 + 1 = 2\n$$\nSo, $\\|\\vec{b}_2\\| = \\sqrt{2}$.\n\nNow, we normalize the vectors to obtain an orthonormal basis $\\{\\vec{u}'_1, \\vec{u}'_2\\}$:\n$$\n\\vec{u}'_1 = \\frac{\\vec{b}_1}{\\|\\vec{b}_1\\|} = \\frac{1}{\\sqrt{2}}(1, 0, \\cos\\theta, \\sin\\theta)\n$$\n$$\n\\vec{u}'_2 = \\frac{\\vec{b}_2}{\\|\\vec{b}_2\\|} = \\frac{1}{\\sqrt{2}}(0, 1, -\\sin\\theta, \\cos\\theta)\n$$\n\nStep 4: Apply the uniqueness conditions.\nThe problem requires an ordered basis $\\{\\vec{u}_1, \\vec{u}_2\\}$ satisfying specific conditions.\n1. Ordering: The first non-zero component of $\\vec{u}_1$ must be at a lower index than that of $\\vec{u}_2$.\n   - For $\\vec{u}'_1$, the first non-zero component is the 1st component ($1/\\sqrt{2}$).\n   - For $\\vec{u}'_2$, the first non-zero component is the 2nd component ($1/\\sqrt{2}$).\n   Since $1  2$, the ordering is $\\vec{u}_1 = \\vec{u}'_1$ and $\\vec{u}_2 = \\vec{u}'_2$.\n\n2. Sign: The first non-zero component of each vector must be positive.\n   - For $\\vec{u}_1$, the first non-zero component is $1/\\sqrt{2}$, which is positive.\n   - For $\\vec{u}_2$, the first non-zero component is $1/\\sqrt{2}$, which is positive.\nBoth conditions are met by our chosen vectors without any modification.\n\nStep 5: Construct the final matrix.\nThe final answer is a $2 \\times 4$ matrix whose rows are the orthonormal basis vectors $\\vec{u}_1$ and $\\vec{u}_2$.\nRow 1: $\\vec{u}_1 = \\left(\\frac{1}{\\sqrt{2}}, 0, \\frac{\\cos\\theta}{\\sqrt{2}}, \\frac{\\sin\\theta}{\\sqrt{2}}\\right)$\nRow 2: $\\vec{u}_2 = \\left(0, \\frac{1}{\\sqrt{2}}, -\\frac{\\sin\\theta}{\\sqrt{2}}, \\frac{\\cos\\theta}{\\sqrt{2}}\\right)$\n\nThe matrix is:\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  0  \\frac{\\cos{\\theta}}{\\sqrt{2}}  \\frac{\\sin{\\theta}}{\\sqrt{2}} \\\\\n0  \\frac{1}{\\sqrt{2}}  -\\frac{\\sin{\\theta}}{\\sqrt{2}}  \\frac{\\cos{\\theta}}{\\sqrt{2}}\n\\end{pmatrix}\n$$\nWe can factor out the common term $\\frac{1}{\\sqrt{2}}$:\n$$\n\\frac{1}{\\sqrt{2}} \\begin{pmatrix}\n1  0  \\cos{\\theta}  \\sin{\\theta} \\\\\n0  1  -\\sin{\\theta}  \\cos{\\theta}\n\\end{pmatrix}\n$$\nThis is the final expression for the matrix.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  0  \\cos{\\theta}  \\sin{\\theta} \\\\ 0  1  -\\sin{\\theta}  \\cos{\\theta} \\end{pmatrix}}$$", "id": "1892191"}, {"introduction": "A key insight in functional analysis is the deep connection between an operator's algebraic properties and its graph's geometric structure. This practice explores the most fundamental of these connections: an operator is linear if and only if its graph is a vector subspace. By testing several operators defined on the space of continuous functions, you will develop a practical understanding of this crucial equivalence and sharpen your ability to identify linear transformations [@problem_id:1892214].", "problem": "Let $X = C([0, 1])$ be the vector space of all real-valued continuous functions on the closed interval $[0, 1]$. The product space $X \\times X$ is also a vector space with addition and scalar multiplication defined component-wise. The graph of an operator $T: X \\to X$ is the set $G_T = \\{ (f, T(f)) \\mid f \\in X \\}$, which is a subset of $X \\times X$.\n\nConsider the following operators mapping $X$ to $X$. For each operator, the definition is given for a function $f \\in X$ and for all $t \\in [0, 1]$.\n\nA. $T_A(f)(t) = (f(t))^2$\n\nB. $T_B(f)(t) = f(t) + 1$\n\nC. $T_C(f)(t) = \\int_0^t f(s) ds$\n\nD. $T_D(f)(t) = f(0)t$\n\nE. $T_E(f)(t) = |f(t)|$\n\nWhich of the above operators have a graph that is a vector subspace of $X \\times X$? Identify all correct options.", "solution": "We work in the real vector space $X=C([0,1])$, and $X \\times X$ with component-wise operations. For an operator $T:X \\to X$, its graph is $G_{T}=\\{(f,T(f)) : f \\in X\\} \\subset X \\times X$.\n\nKey criterion: $G_{T}$ is a vector subspace of $X \\times X$ if and only if $T$ is linear.\n\nProof of the criterion:\n(1) Suppose $G_{T}$ is a subspace. Then the zero vector $(0,0)$ belongs to $G_{T}$, so there exists $f_{0} \\in X$ such that $(f_{0},T(f_{0}))=(0,0)$. Hence $f_{0}=0$ and $T(0)=0$. Next, for any $f,g \\in X$, since $(f,T(f))$ and $(g,T(g))$ are in $G_{T}$ and $G_{T}$ is closed under addition, we have\n$$(f,T(f))+(g,T(g))=(f+g,\\,T(f)+T(g)) \\in G_{T}.$$\nThus there exists $h \\in X$ with $(h,T(h))=(f+g,\\,T(f)+T(g))$, forcing $h=f+g$ and\n$$T(f+g)=T(f)+T(g).$$\nSimilarly, for any $\\alpha \\in \\mathbb{R}$, since $G_{T}$ is closed under scalar multiplication,\n$$\\alpha\\,(f,T(f))=(\\alpha f,\\,\\alpha T(f)) \\in G_{T},$$\nso there exists $k \\in X$ with $(k,T(k))=(\\alpha f,\\,\\alpha T(f))$, hence $k=\\alpha f$ and\n$$T(\\alpha f)=\\alpha T(f).$$\nTherefore $T$ is linear.\n\n(2) Conversely, if $T$ is linear, then for any $(f,T(f)),(g,T(g)) \\in G_{T}$ and any scalars $\\alpha,\\beta \\in \\mathbb{R}$,\n$$\\alpha\\,(f,T(f))+\\beta\\,(g,T(g))=(\\alpha f+\\beta g,\\,\\alpha T(f)+\\beta T(g))=(\\alpha f+\\beta g,\\,T(\\alpha f+\\beta g)) \\in G_{T},$$\nso $G_{T}$ is a subspace.\n\nTherefore, for each operator below, $G_{T}$ is a subspace if and only if $T$ is linear. We test linearity for each given operator.\n\nA. $T_A(f)(t) = (f(t))^2$. This operator is not linear. For additivity: $(T_A(f+g))(t) = (f(t)+g(t))^2 = (f(t))^2 + 2f(t)g(t) + (g(t))^2$. This is not equal to $(T_A f)(t) + (T_A g)(t) = (f(t))^2 + (g(t))^2$ in general. Hence $T_A$ is not linear, so $G_{T_A}$ is not a subspace.\n\nB. $T_B(f)(t) = f(t) + 1$. This operator is not linear because it does not map the zero function to the zero function. For the zero function $0 \\in X$, $(T_B(0))(t) = 0(t)+1 = 1 \\neq 0$. Hence $G_{T_B}$ is not a subspace.\n\nC. $T_C(f)(t) = \\int_0^t f(s) ds$. From the linearity of the integral, for any scalars $\\alpha, \\beta \\in \\mathbb{R}$ and functions $f,g \\in X$: $$(T_C(\\alpha f + \\beta g))(t) = \\int_0^t (\\alpha f(s) + \\beta g(s)) ds = \\alpha \\int_0^t f(s) ds + \\beta \\int_0^t g(s) ds = \\alpha (T_C f)(t) + \\beta (T_C g)(t).$$ This shows that $T_C$ is linear. Therefore, $G_{T_C}$ is a subspace.\n\nD. $T_D(f)(t) = f(0)t$. For any scalars $\\alpha, \\beta \\in \\mathbb{R}$ and functions $f,g \\in X$: $$(T_D(\\alpha f + \\beta g))(t) = (\\alpha f + \\beta g)(0) \\cdot t = (\\alpha f(0) + \\beta g(0))t = \\alpha f(0)t + \\beta g(0)t = \\alpha (T_D f)(t) + \\beta (T_D g)(t).$$ This shows that $T_D$ is linear. Therefore, $G_{T_D}$ is a subspace.\n\nE. $T_E(f)(t) = |f(t)|$. This operator is not linear. It fails homogeneity for negative scalars. For $\\alpha=-1$: $(T_E(-f))(t) = |-f(t)| = |f(t)| = (T_E f)(t)$. This is not equal to $-(T_E f)(t)$ unless $f$ is the zero function. Hence $T_E$ is not linear, so $G_{T_E}$ is not a subspace.\n\nTherefore, the operators with graphs that are vector subspaces of $X \\times X$ are $T_C$ and $T_D$.", "answer": "$$\\boxed{CD}$$", "id": "1892214"}, {"introduction": "Beyond being a subspace, the topological properties of an operator's graph are critical, leading to the celebrated Closed Graph Theorem. This exercise introduces the concept of a closable operator by challenging you to construct a counterexample: a linear operator that is *not* closable [@problem_id:1892213]. By finding a sequence in the domain that converges to zero while its image converges to a non-zero vector, you will directly demonstrate what it means for a graph to fail to be closed, highlighting why this property is so important in analysis.", "problem": "Let $\\ell^2$ be the Hilbert space of all square-summable sequences of complex numbers, $x = (x_k)_{k=1}^\\infty$ where $x_k \\in \\mathbb{C}$, with the inner product $\\langle x, y \\rangle = \\sum_{k=1}^\\infty x_k \\overline{y_k}$ and the induced norm $\\|x\\|_{\\ell^2} = (\\sum_{k=1}^\\infty |x_k|^2)^{1/2}$. Let $c_{00}$ be the dense subspace of $\\ell^2$ consisting of all sequences that have only a finite number of non-zero terms.\n\nConsider a linear operator $T: D(T) \\to \\ell^2$ with domain $D(T) = c_{00}$. The action of the operator on any $x = (x_k)_{k=1}^\\infty \\in c_{00}$ is defined by\n$$T(x) = \\left(\\sum_{k=1}^\\infty x_k\\right) u$$\nwhere $u$ is a fixed, non-zero vector in $\\ell^2$.\n\nThe graph of $T$, denoted $G(T)$, is the subspace of the product space $\\ell^2 \\times \\ell^2$ defined by $G(T) = \\{(x, T(x)) \\mid x \\in D(T)\\}$. An operator is said to be not closable if the closure of its graph, $\\overline{G(T)}$, contains an element of the form $(0, y)$ for some non-zero $y \\in \\ell^2$.\n\nDetermine the specific element $y \\in \\ell^2$ such that $(0, y)$ belongs to $\\overline{G(T)}$. Express your answer in terms of the vector $u$.", "solution": "We use the definition of closability via the closure of the graph. An element $(0,y)$ lies in $\\overline{G(T)}$ if there exists a sequence $\\{x^{(n)}\\} \\subset D(T)=c_{00}$ such that $x^{(n)} \\to 0$ in $\\ell^2$ and $T(x^{(n)}) \\to y$ in $\\ell^2$.\n\nDefine, for each $n \\in \\mathbb{N}$,\n$$\nx^{(n)} := \\left(\\underbrace{\\tfrac{1}{n},\\tfrac{1}{n},\\ldots,\\tfrac{1}{n}}_{n\\ \\text{terms}},0,0,\\ldots\\right).\n$$\nThen $x^{(n)} \\in c_{00}$ for every $n$, and its $\\ell^2$ norm satisfies\n$$\n\\|x^{(n)}\\|_{\\ell^2}^{2}=\\sum_{k=1}^{\\infty}|x^{(n)}_{k}|^{2}=\\sum_{k=1}^{n}\\left|\\tfrac{1}{n}\\right|^{2}=n\\cdot \\tfrac{1}{n^{2}}=\\tfrac{1}{n}\\to 0,\n$$\nhence $x^{(n)} \\to 0$ in $\\ell^2$.\n\nSince $x^{(n)}$ has only finitely many nonzero entries, the sum $\\sum_{k=1}^{\\infty}x^{(n)}_{k}$ is well-defined and equals\n$$\n\\sum_{k=1}^{\\infty}x^{(n)}_{k}=\\sum_{k=1}^{n}\\tfrac{1}{n}=1.\n$$\nBy the definition of $T$,\n$$\nT(x^{(n)})=\\left(\\sum_{k=1}^{\\infty}x^{(n)}_{k}\\right)u=1\\cdot u=u.\n$$\nTherefore $(x^{(n)},T(x^{(n)}))=(x^{(n)},u)\\in G(T)$ for all $n$, and as $n\\to\\infty$,\n$$\n(x^{(n)},T(x^{(n)}))\\to (0,u)\\quad \\text{in } \\ell^2\\times \\ell^2.\n$$\nHence $(0,u)\\in \\overline{G(T)}$. By the given criterion, this shows $T$ is not closable, and the required element is $y=u$.", "answer": "$$\\boxed{u}$$", "id": "1892213"}]}