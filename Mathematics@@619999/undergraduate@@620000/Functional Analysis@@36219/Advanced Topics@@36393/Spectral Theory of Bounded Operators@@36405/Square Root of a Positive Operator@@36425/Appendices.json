{"hands_on_practices": [{"introduction": "We begin our hands-on exploration by grounding the abstract concept of an operator's square root in the familiar context of matrix algebra. This first practice provides a direct, computable example involving a $2 \\times 2$ positive definite matrix. By working through this problem, you will not only calculate a specific square root but also see how fundamental matrix properties can be leveraged to find it [@problem_id:1882666].", "problem": "In operator theory, a bounded linear operator $A$ on a Hilbert space is called positive (or more specifically, positive semi-definite) if $\\langle Ax, x \\rangle \\ge 0$ for all vectors $x$ in the space. If, in addition, $\\langle Ax, x \\rangle = 0$ only for $x=0$, the operator is called positive definite. For a positive definite operator $A$, there exists a unique positive definite operator $B$ such that $B^2 = A$. This operator $B$ is called the (unique positive definite) square root of $A$ and is denoted by $A^{1/2}$.\n\nConsider the real vector space $\\mathbb{R}^2$ equipped with the standard dot product. A linear operator is represented by a matrix. A real symmetric matrix $M$ is positive definite if $x^T M x > 0$ for all non-zero vectors $x \\in \\mathbb{R}^2$.\n\nLet the operator $A$ be represented by the following $2 \\times 2$ real symmetric matrix:\n$$\nA = \\begin{pmatrix} \\frac{36}{5} & -\\frac{12}{5} \\\\ -\\frac{12}{5} & \\frac{29}{5} \\end{pmatrix}\n$$\nDetermine the unique positive definite square root, $B$, of the matrix $A$. Present your answer as a $2 \\times 2$ matrix.", "solution": "We work in $\\mathbb{R}^{2}$ with the standard dot product. For a $2 \\times 2$ real symmetric positive definite matrix $A$, we seek the unique positive definite square root $B$ satisfying $B^2=A$. By the Cayley–Hamilton theorem for $2 \\times 2$ matrices,\n$$\nA^{2} - (\\operatorname{tr}A)\\,A + (\\det A)\\,I = 0,\n$$\nso $A^{2} = (\\operatorname{tr}A)\\,A - (\\det A)\\,I$. Assume $B$ is of the form $B = xA + yI$ for scalars $x,y$. Then\n$B^2 = (xA + yI)^2 = x^2A^2 + 2xy A + y^2I.$\nUsing the Cayley–Hamilton relation to eliminate $A^2$,\n$B^2 = x^2\\big((\\operatorname{tr}A)\\,A - (\\det A)\\,I\\big) + 2xy A + y^2I = \\big(x^2\\operatorname{tr}A + 2xy\\big)A + \\big(-x^2\\det A + y^2\\big)I.$\nFor $B^2=A$, we require the system\n$$\nx^{2}\\operatorname{tr}A + 2xy = 1,\\qquad -x^{2}\\det A + y^{2} = 0.\n$$\nFrom the second equation, $y = \\pm x\\sqrt{\\det A}$. To obtain the positive definite square root, we take the positive sign, yielding $y = x\\sqrt{\\det A}$. Substituting into the first equation gives\n$$\nx^{2}\\big(\\operatorname{tr}A + 2\\sqrt{\\det A}\\big) = 1,\n$$\nhence\n$$\nx = \\frac{1}{\\sqrt{\\operatorname{tr}A + 2\\sqrt{\\det A}}},\\qquad y = \\frac{\\sqrt{\\det A}}{\\sqrt{\\operatorname{tr}A + 2\\sqrt{\\det A}}}.\n$$\nTherefore,\n$$\nB = \\frac{A + \\sqrt{\\det A}\\,I}{\\sqrt{\\operatorname{tr}A + 2\\sqrt{\\det A}}}.\n$$\nFor the given\n$$\nA = \\begin{pmatrix} \\frac{36}{5} & -\\frac{12}{5} \\\\ -\\frac{12}{5} & \\frac{29}{5} \\end{pmatrix},\n$$\ncompute\n$$\n\\operatorname{tr}A = \\frac{36}{5} + \\frac{29}{5} = \\frac{65}{5} = 13,\\qquad\n\\det A = \\frac{36}{5}\\cdot\\frac{29}{5} - \\left(-\\frac{12}{5}\\right)^{2} = \\frac{1044 - 144}{25} = \\frac{900}{25} = 36.\n$$\nThus $\\sqrt{\\det A} = 6$ and $\\sqrt{\\operatorname{tr}A + 2\\sqrt{\\det A}} = \\sqrt{13 + 12} = \\sqrt{25} = 5$. Hence\n$$\nB = \\frac{A + 6I}{5}\n= \\frac{1}{5}\\begin{pmatrix} \\frac{36}{5} + 6 & -\\frac{12}{5} \\\\ -\\frac{12}{5} & \\frac{29}{5} + 6 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{66}{25} & -\\frac{12}{25} \\\\ -\\frac{12}{25} & \\frac{59}{25} \\end{pmatrix}.\n$$\nA direct verification confirms $B^2 = A$. Writing $B = \\frac{1}{25}\\begin{pmatrix} 66 & -12 \\\\ -12 & 59 \\end{pmatrix}$, we compute\n$$\n\\begin{pmatrix} 66 & -12 \\\\ -12 & 59 \\end{pmatrix}^{2}\n= \\begin{pmatrix} 4500 & -1500 \\\\ -1500 & 3625 \\end{pmatrix},\n$$\nso\n$$\nB^{2} = \\frac{1}{625}\\begin{pmatrix} 4500 & -1500 \\\\ -1500 & 3625 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{36}{5} & -\\frac{12}{5} \\\\ -\\frac{12}{5} & \\frac{29}{5} \\end{pmatrix}\n= A.\n$$\nTherefore, the unique positive definite square root of $A$ is\n$$\n\\begin{pmatrix} \\frac{66}{25} & -\\frac{12}{25} \\\\ -\\frac{12}{25} & \\frac{59}{25} \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{66}{25} & -\\frac{12}{25} \\\\ -\\frac{12}{25} & \\frac{59}{25} \\end{pmatrix}}$$", "id": "1882666"}, {"introduction": "Having mastered the finite-dimensional case, we now take a crucial step into the realm of infinite-dimensional Hilbert spaces. This exercise introduces multiplication operators, one of the most fundamental and intuitive classes of operators in functional analysis. You will discover how the concept of a square root translates elegantly to this setting, providing a powerful illustration of the spectral theorem's application [@problem_id:1882701].", "problem": "Let $H = L^2([0,1])$ be the complex Hilbert space of square-integrable functions on the interval $[0,1]$. Consider the linear operator $T: H \\to H$ defined by its action on any function $g \\in H$ as:\n$$ (Tg)(x) = (x^4 + 2x^2 + 1) g(x) $$\nfor all $x \\in [0,1]$. This operator is an example of a multiplication operator.\n\nGiven that $T$ is a positive operator, there exists a unique positive operator $S$ on $H$ such that $S^2 = T$. This operator $S$ is called the square root of $T$.\n\nDetermine the explicit action of the operator $S$ on an arbitrary function $g \\in H$. Your answer should be an expression for $(Sg)(x)$.", "solution": "We consider $H = L^{2}([0,1])$ and the multiplication operator $T$ defined by\n$$\n(Tg)(x) = m(x)\\,g(x), \\quad m(x) = x^{4} + 2x^{2} + 1,\n$$\nfor all $g \\in H$ and $x \\in [0,1]$. First, note that\n$$\nm(x) = x^{4} + 2x^{2} + 1 = (x^{2} + 1)^{2},\n$$\nso $m(x) \\ge 0$ for all $x \\in [0,1]$, and indeed $m(x) \\ge 1$.\n\nPositivity of $T$ follows from the definition: for any $g \\in H$,\n$$\n\\langle Tg, g \\rangle = \\int_{0}^{1} m(x)\\,|g(x)|^{2}\\,dx \\geq 0,\n$$\nsince $m(x) \\ge 0$ almost everywhere.\n\nBy the spectral theorem/functional calculus for bounded self-adjoint multiplication operators, the unique positive square root $S$ of $T$ is also a multiplication operator given by multiplication by the pointwise square root of $m$, namely\n$s(x) = m(x)^{1/2}.$\nBecause $m(x) = (x^{2}+1)^{2}$ and $x^2+1 \\ge 1$ on $[0,1]$, we have\n$$\ns(x) = \\sqrt{(x^{2}+1)^{2}} = |x^{2}+1| = x^{2}+1.\n$$\nTherefore, for any $g \\in H$,\n$$\n(Sg)(x) = s(x)\\,g(x) = (x^{2}+1)\\,g(x).\n$$\nA direct verification shows $S^2 = T$, since\n$$\n(S^{2}g)(x) = s(x)^{2}\\,g(x) = (x^{2}+1)^{2}\\,g(x) = m(x)\\,g(x) = (Tg)(x).\n$$\nThis $S$ is positive and unique by the spectral theorem.", "answer": "$$\\boxed{(x^{2}+1)\\,g(x)}$$", "id": "1882701"}, {"introduction": "Our final practice demonstrates the practical power of operator theory by applying it to numerical analysis. Computing the square root of a large matrix is a common task in science and engineering, often tackled with iterative methods like the Newton-Raphson algorithm. This problem invites you to analyze the convergence of this method, revealing how the error shrinks at each step and showcasing why this approach is so effective [@problem_id:1882698].", "problem": "In the analysis of iterative algorithms for matrix computations, it's essential to characterize the propagation of error. Consider an iterative method for finding the square root of a positive definite matrix, a task relevant in fields like statistical analysis (e.g., Mahalanobis distance) and control systems.\n\nLet $A$ be a positive definite $n \\times n$ real matrix. The unique positive definite square root of $A$ is denoted by $S$, such that $S^2 = A$. The sequence of approximations to $S$ is generated by the Newton-Raphson iteration:\n$$X_{k+1} = \\frac{1}{2}(X_k + A X_k^{-1})$$\nfor $k = 0, 1, 2, \\dots$. The initial guess, $X_0$, is a positive definite matrix that commutes with $A$ (i.e., $X_0 A = A X_0$).\n\nTo analyze the convergence, we define the error matrix at step $k$ as $E_k = X_k - S$, and the relative error matrix at step $k$ as $R_k = S^{-1}E_k$.\n\nFind an expression for the relative error matrix at step $k+1$, $R_{k+1}$, purely in terms of the identity matrix $I$ and the relative error matrix from the previous step, $R_k$.", "solution": "We start from the Newton-Raphson iteration for the matrix square root:\n$$\nX_{k+1}=\\frac{1}{2}\\left(X_{k}+A X_{k}^{-1}\\right),\n$$\nwhere $A=S^2$, with $S$ the unique positive definite square root of $A$. Define the error $E_{k}=X_{k}-S$ and the relative error $R_{k}=S^{-1}E_{k}$. Then\n$$\nX_{k}=S+E_{k}=S(I+R_{k}).\n$$\nBecause $X_{0}$ commutes with $A$ and $S$ is an analytic function of $A$, it follows by induction that each $X_{k}$ commutes with $A$, hence with $S$. Consequently, $E_{k}$ and $R_{k}$ commute with $S$, and $(I+R_{k})$ commutes with $S$. Therefore,\n$$\nX_{k}^{-1}=(S(I+R_{k}))^{-1}=(I+R_{k})^{-1}S^{-1},\n$$\nand using $A=S^2$,\n$$\nA X_{k}^{-1}=S^{2}(I+R_{k})^{-1}S^{-1}=S(I+R_{k})^{-1},\n$$\nwhere commutativity was used to move $S$ through $(I+R_{k})^{-1}$. Substituting into the iteration gives\n$$\nX_{k+1}=\\frac{1}{2}\\left(S(I+R_{k})+S(I+R_{k})^{-1}\\right)=S\\cdot\\frac{1}{2}\\left((I+R_{k})+(I+R_{k})^{-1}\\right).\n$$\nHence,\n$$\nE_{k+1}=X_{k+1}-S=S\\left[\\frac{1}{2}\\left((I+R_{k})+(I+R_{k})^{-1}\\right)-I\\right],\n$$\nand dividing by $S$ on the left yields the relative error update\n$$\nR_{k+1}=\\frac{1}{2}\\left((I+R_{k})+(I+R_{k})^{-1}\\right)-I.\n$$\nLetting $Y=I+R_k$, we use the identity $Y+Y^{-1}-2I=(Y-I)^2 Y^{-1}$ to obtain\n$$\nR_{k+1}=\\frac{1}{2}(Y+Y^{-1}-2I)=\\frac{1}{2}(Y-I)^{2}Y^{-1}=\\frac{1}{2}R_{k}^{2}(I+R_{k})^{-1}.\n$$\nThis expresses $R_{k+1}$ purely in terms of $I$ and $R_{k}$.", "answer": "$$\\boxed{\\frac{1}{2}\\,R_{k}^{2}\\,(I+R_{k})^{-1}}$$", "id": "1882698"}]}