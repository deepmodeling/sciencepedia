{"hands_on_practices": [{"introduction": "The power of functional calculus lies in its ability to define $f(T)$ for a vast class of functions $f$ and operators $T$. This practice serves as a fundamental starting point by guiding you through a direct computation. By applying the definition of functional calculus to a simple diagonal operator, you will see how the abstract theory translates into a concrete and manageable matrix calculation, solidifying your understanding of how $f(T)$ acts on the operator's eigenspaces. [@problem_id:1863653]", "problem": "Consider the Hilbert space $\\mathcal{H} = \\mathbb{C}^3$, the vector space of 3-tuples of complex numbers, equipped with the standard inner product $\\langle u, v \\rangle = \\sum_{k=1}^3 u_k \\overline{v_k}$. Let $\\{e_1, e_2, e_3\\}$ denote the standard orthonormal basis of $\\mathcal{H}$, where $e_1=(1,0,0)$, $e_2=(0,1,0)$, and $e_3=(0,0,1)$.\n\nA bounded linear operator $T: \\mathcal{H} \\to \\mathcal{H}$ is defined by its action on these basis vectors:\n$T(e_1) = 2 e_1$\n$T(e_2) = (\\ln 3) e_2$\n$T(e_3) = -e_3$\n\nCompute the matrix representation of the operator $\\exp(T)$ with respect to the standard basis $\\{e_1, e_2, e_3\\}$. Present your answer as a $3 \\times 3$ matrix.", "solution": "We are given a bounded linear operator $T$ on $\\mathcal{H}=\\mathbb{C}^{3}$ defined on the orthonormal basis $\\{e_{1},e_{2},e_{3}\\}$ by $T(e_{1})=2e_{1}$, $T(e_{2})=(\\ln 3)e_{2}$, and $T(e_{3})=-e_{3}$. Thus $T$ is diagonal with respect to this basis, with eigenvalues $\\lambda_{1}=2$, $\\lambda_{2}=\\ln 3$, and $\\lambda_{3}=-1$.\n\nBy definition of the operator exponential via its power series, we have\n$$\n\\exp(T)=\\sum_{n=0}^{\\infty}\\frac{T^{n}}{n!}.\n$$\nFor each basis vector $e_{i}$ and each $n\\in\\mathbb{N}$, the diagonal action implies\n$$\nT^{n}e_{i}=\\lambda_{i}^{n}e_{i}.\n$$\nTherefore,\n$$\n\\exp(T)e_{i}=\\sum_{n=0}^{\\infty}\\frac{T^{n}}{n!}e_{i}=\\sum_{n=0}^{\\infty}\\frac{\\lambda_{i}^{n}}{n!}e_{i}=\\exp(\\lambda_{i})e_{i}.\n$$\nSubstituting the eigenvalues, we obtain\n$$\n\\exp(T)e_{1}=\\exp(2)e_{1},\\quad \\exp(T)e_{2}=\\exp(\\ln 3)e_{2}=3e_{2},\\quad \\exp(T)e_{3}=\\exp(-1)e_{3}.\n$$\nHence, the matrix of $\\exp(T)$ in the standard basis is diagonal with diagonal entries $\\exp(2)$, $3$, and $\\exp(-1)$:\n$$\n\\begin{pmatrix}\n\\exp(2) & 0 & 0\\\\\n0 & 3 & 0\\\\\n0 & 0 & \\exp(-1)\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\exp(2)&0&0\\\\0&3&0\\\\0&0&\\exp(-1)\\end{pmatrix}}$$", "id": "1863653"}, {"introduction": "Beyond simple computation, a deeper understanding of functional calculus comes from seeing how properties of the function $f$ translate into properties of the operator $f(T)$. This exercise explores the connection between the algebraic properties of $f$ and the geometric nature of $f(T)$. You will determine the precise condition a function must satisfy for $f(T)$ to become an orthogonal projection, one of the most important types of operators in Hilbert space theory. [@problem_id:1863688]", "problem": "In the mathematical framework of functional analysis, the continuous functional calculus is a powerful tool for defining functions of operators. This problem explores the properties of functions that transform a self-adjoint operator into a fundamental geometric object: a projection.\n\nLet $H$ be a Hilbert space and let $T: H \\to H$ be a compact, self-adjoint operator. The spectrum of $T$, denoted $\\sigma(T)$, is a compact subset of the real line. For any real-valued continuous function $f$ defined on the spectrum of $T$, i.e., $f \\in C(\\sigma(T), \\mathbb{R})$, we can define a new operator $f(T)$ using the continuous functional calculus. Specifically, if $T$ has the spectral representation $T = \\sum_{k} \\lambda_k P_k$, where $\\{\\lambda_k\\}$ are the distinct eigenvalues of $T$ and $P_k$ is the orthogonal projection onto the eigenspace corresponding to $\\lambda_k$, then the operator $f(T)$ is defined as $f(T) = \\sum_{k} f(\\lambda_k) P_k$.\n\nAn operator $P$ on $H$ is called an orthogonal projection if it is both self-adjoint (i.e., $P = P^*$, where $P^*$ is the adjoint of $P$) and idempotent (i.e., $P^2 = P$).\n\nFor a given compact, self-adjoint operator $T$, which of the following options states the necessary and sufficient condition on the function $f \\in C(\\sigma(T), \\mathbb{R})$ for the operator $f(T)$ to be an orthogonal projection?\n\nA. $f$ must be a constant function on $\\sigma(T)$, taking a value of either 0 or 1.\n\nB. The range of $f$ on the set $\\sigma(T)$ must be a subset of $\\{0, 1\\}$.\n\nC. $f$ must be a polynomial function with real coefficients.\n\nD. $f$ must be the identity function on $\\sigma(T)$, i.e., $f(\\lambda) = \\lambda$ for all $\\lambda \\in \\sigma(T)$.\n\nE. The function $f$ must be an even function, i.e., $f(-\\lambda) = f(\\lambda)$ for all $\\lambda$ such that both $\\lambda$ and $-\\lambda$ are in $\\sigma(T)$.", "solution": "We use the continuous functional calculus for compact self-adjoint operators and the spectral theorem. Since $T$ is compact and self-adjoint, it has a spectral decomposition\n$$\nT=\\sum_{k} \\lambda_{k} P_{k},\n$$\nwhere each $\\lambda_{k} \\in \\mathbb{R}$ is a (nonzero) eigenvalue and $P_{k}$ is the orthogonal projection onto the corresponding eigenspace; additionally, $0 \\in \\sigma(T)$ is the only possible accumulation point. For any real-valued continuous $f$ on $\\sigma(T)$, the functional calculus defines\n$$\nf(T)=\\sum_{k} f(\\lambda_{k}) P_{k} + f(0)P_{\\{0\\}},\n$$\nwhere $P_{\\{0\\}}$ is the spectral projection onto $\\ker T$ (this term is implicitly included if $0 \\in \\sigma(T)$). Equivalently, we may write succinctly\n$$\nf(T)=\\sum_{\\lambda \\in \\sigma(T)} f(\\lambda) E_{\\{\\lambda\\}},\n$$\nwhere $E_{\\{\\lambda\\}}$ are the spectral projections.\n\nFirst, self-adjointness. Since $f$ is real-valued on $\\sigma(T)$ and $E_{\\{\\lambda\\}}$ are orthogonal projections (hence self-adjoint), we have\n$$\nf(T)^{*}=\\left(\\sum_{\\lambda \\in \\sigma(T)} f(\\lambda) E_{\\{\\lambda\\}}\\right)^{*}=\\sum_{\\lambda \\in \\sigma(T)} f(\\lambda) E_{\\{\\lambda\\}}=f(T).\n$$\nThus $f(T)$ is automatically self-adjoint for real-valued $f$.\n\nSecond, idempotency. Compute\n$$\n\\left(f(T)\\right)^{2}\n=\\left(\\sum_{\\lambda} f(\\lambda) E_{\\{\\lambda\\}}\\right)\\left(\\sum_{\\mu} f(\\mu) E_{\\{\\mu\\}}\\right)\n=\\sum_{\\lambda,\\mu} f(\\lambda) f(\\mu) E_{\\{\\lambda\\}} E_{\\{\\mu\\}}.\n$$\nSince the spectral projections are mutually orthogonal, $E_{\\{\\lambda\\}} E_{\\{\\mu\\}}=0$ for $\\lambda \\neq \\mu$ and $E_{\\{\\lambda\\}}^{2}=E_{\\{\\lambda\\}}$. Therefore,\n$$\n\\left(f(T)\\right)^{2}=\\sum_{\\lambda} f(\\lambda)^{2} E_{\\{\\lambda\\}}.\n$$\nThus the idempotency condition $\\left(f(T)\\right)^{2}=f(T)$ is equivalent to\n$$\n\\sum_{\\lambda} \\left(f(\\lambda)^{2}-f(\\lambda)\\right) E_{\\{\\lambda\\}}=0,\n$$\nwhich holds if and only if\n$$\nf(\\lambda)^{2}-f(\\lambda)=0 \\quad \\text{for all } \\lambda \\in \\sigma(T).\n$$\nFor real numbers, the equation $x^{2}-x=0$ holds if and only if $x \\in \\{0,1\\}$. Hence the necessary and sufficient condition is\n$$\nf(\\sigma(T)) \\subseteq \\{0,1\\}.\n$$\n\nEquivalently, in the language of functional calculus, letting $h = f^{2}-f \\in C(\\sigma(T),\\mathbb{R})$, we have $h(T)=0$ if and only if $h$ vanishes on $\\sigma(T)$. Thus $f(T)$ is a projection if and only if $f^{2}=f$ on $\\sigma(T)$, i.e., $f$ attains only the values $0$ and $1$ on $\\sigma(T)$.\n\nSufficiency is immediate: if $f(\\sigma(T)) \\subseteq \\{0,1\\}$, then\n$$\nf(T)=\\sum_{\\lambda} f(\\lambda) E_{\\{\\lambda\\}}\n$$\nhas coefficients in $\\{0,1\\}$, is self-adjoint, and satisfies\n$$\n\\left(f(T)\\right)^{2}=\\sum_{\\lambda} f(\\lambda)^{2} E_{\\{\\lambda\\}}=\\sum_{\\lambda} f(\\lambda) E_{\\{\\lambda\\}}=f(T).\n$$\n\nTherefore, the correct necessary and sufficient condition among the options is that the range of $f$ on $\\sigma(T)$ is contained in $\\{0,1\\}$, which is option B. Options A, C, D, and E are not necessary nor sufficient in general.", "answer": "$$\\boxed{B}$$", "id": "1863688"}, {"introduction": "A common pitfall in operator theory is assuming that all familiar properties of real numbers generalize directly to self-adjoint operators. This practice explores a famous and important instance where this intuition fails, specifically concerning operator inequalities. By working through a concrete counterexample with $2 \\times 2$ matrices, you will investigate whether the inequality $S \\le T$ implies that $\\exp(S) \\le \\exp(T)$, a result that highlights the profound consequences of non-commutativity. [@problem_id:1863692]", "problem": "In the study of operator theory, a foundational topic is the extension of functions of real variables to functions of self-adjoint operators. For a self-adjoint matrix $A$, which is a matrix equal to its conjugate transpose ($A=A^\\dagger$), we can define the matrix exponential $e^A$ through its Taylor series, $e^A = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!}$.\n\nA key concept is the notion of operator ordering. A self-adjoint matrix $H$ is said to be positive semi-definite, denoted $H \\ge 0$, if all of its eigenvalues are non-negative. This defines a partial order on the set of self-adjoint matrices: we say $S \\le T$ if the matrix $T-S$ is positive semi-definite.\n\nFor real numbers $s$ and $t$, it is a basic property that if $s \\le t$, then $e^s \\le e^t$. This problem asks you to investigate whether this property holds for matrices.\n\nConsider the following two $2 \\times 2$ real symmetric matrices:\n$$\nS = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad T = \\begin{pmatrix} 4 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix}\n$$\nIt can be verified that $S \\le T$ according to the definition above. To investigate whether this implies $e^S \\le e^T$, we analyze the matrix $M = e^T - e^S$. For $M$ to be positive semi-definite, it is a necessary (but not sufficient) condition that all of its diagonal entries are non-negative.\n\nCalculate the value of the bottom-right diagonal entry of the matrix $M$, which is denoted as $M_{22}$. Round your final answer to three significant figures.", "solution": "The problem asks us to compute the $(2,2)$ entry of the matrix $M = e^T - e^S$ for the given matrices $S$ and $T$.\n\nFirst, let's briefly verify the given condition that $S \\le T$. This means we need to check if the matrix $D = T - S$ is positive semi-definite.\n$$\nD = T - S = \\begin{pmatrix} 4 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix} - \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & -1 \\\\ -1 & \\frac{1}{4} \\end{pmatrix}\n$$\nA $2 \\times 2$ real symmetric matrix is positive semi-definite if and only if its trace is non-negative and its determinant is non-negative.\nThe trace of $D$ is $\\text{Tr}(D) = 4 + \\frac{1}{4} = \\frac{17}{4} \\ge 0$.\nThe determinant of $D$ is $\\det(D) = (4)\\left(\\frac{1}{4}\\right) - (-1)^2 = 1 - 1 = 0 \\ge 0$.\nSince both conditions are met, the eigenvalues of $D$ are non-negative ($\\lambda_1 = 0, \\lambda_2 = 17/4$), and thus $D$ is positive semi-definite. This confirms that $S \\le T$.\n\nNow, we need to compute the matrix $M = e^T - e^S$. This involves computing the matrix exponentials $e^T$ and $e^S$.\n\nThe matrix $T$ is a diagonal matrix. The exponential of a diagonal matrix is found by exponentiating its diagonal entries:\n$$\ne^T = \\begin{pmatrix} e^4 & 0 \\\\ 0 & e^{1/4} \\end{pmatrix}\n$$\n\nThe matrix $S$ is not diagonal. To compute its exponential, we can diagonalize it. The eigenvalues $\\lambda$ of $S$ are given by the characteristic equation:\n$$\n\\det(S - \\lambda I) = \\det\\begin{pmatrix} -\\lambda & 1 \\\\ 1 & -\\lambda \\end{pmatrix} = \\lambda^2 - 1 = 0\n$$\nThe eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = -1$.\nA more direct way to compute the exponential of $S = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$ is to use the property that $S^2 = I$, where $I$ is the identity matrix.\nUsing the Taylor series definition of the matrix exponential:\n$$\ne^S = \\sum_{k=0}^{\\infty} \\frac{S^k}{k!} = I + S + \\frac{S^2}{2!} + \\frac{S^3}{3!} + \\frac{S^4}{4!} + \\dots\n$$\nSince $S^2 = I$, we have $S^3 = S$, $S^4 = I$, and in general $S^{2n} = I$ and $S^{2n+1} = S$ for $n \\ge 0$.\nWe can group the even and odd powers:\n$$\ne^S = \\left( \\frac{S^0}{0!} + \\frac{S^2}{2!} + \\frac{S^4}{4!} + \\dots \\right) + \\left( \\frac{S^1}{1!} + \\frac{S^3}{3!} + \\frac{S^5}{5!} + \\dots \\right)\n$$\n$$\ne^S = I \\left( 1 + \\frac{1}{2!} + \\frac{1}{4!} + \\dots \\right) + S \\left( 1 + \\frac{1}{3!} + \\frac{1}{5!} + \\dots \\right)\n$$\nThe series in the parentheses are the definitions of the hyperbolic cosine and sine functions evaluated at 1:\n$\\cosh(1) = \\sum_{n=0}^{\\infty} \\frac{1}{(2n)!}$ and $\\sinh(1) = \\sum_{n=0}^{\\infty} \\frac{1}{(2n+1)!}$.\nTherefore, $e^S = I \\cosh(1) + S \\sinh(1)$.\nSubstituting the matrices $I$ and $S$:\n$$\ne^S = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\cosh(1) + \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\sinh(1) = \\begin{pmatrix} \\cosh(1) & \\sinh(1) \\\\ \\sinh(1) & \\cosh(1) \\end{pmatrix}\n$$\n\nNow we can compute the matrix $M = e^T - e^S$:\n$$\nM = \\begin{pmatrix} e^4 & 0 \\\\ 0 & e^{1/4} \\end{pmatrix} - \\begin{pmatrix} \\cosh(1) & \\sinh(1) \\\\ \\sinh(1) & \\cosh(1) \\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix} e^4 - \\cosh(1) & -\\sinh(1) \\\\ -\\sinh(1) & e^{1/4} - \\cosh(1) \\end{pmatrix}\n$$\nThe problem asks for the bottom-right diagonal entry, $M_{22}$.\n$$\nM_{22} = e^{1/4} - \\cosh(1)\n$$\nTo find the numerical value, we use the approximations for $e$ and standard function values:\n$e^{1/4} = e^{0.25} \\approx 1.284025$\n$\\cosh(1) = \\frac{e^1 + e^{-1}}{2} \\approx \\frac{2.718282 + 0.367879}{2} \\approx \\frac{3.086161}{2} \\approx 1.543081$\nSo,\n$$\nM_{22} \\approx 1.284025 - 1.543081 = -0.259056\n$$\nRounding this value to three significant figures, we get $-0.259$.\n\nThe fact that $M_{22}$ is negative is sufficient to prove that the matrix $M$ is not positive semi-definite. For a matrix $M$ to be positive semi-definite, the condition $\\mathbf{x}^\\dagger M \\mathbf{x} \\ge 0$ must hold for any vector $\\mathbf{x}$. If we choose the vector $\\mathbf{x} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, then $\\mathbf{x}^\\dagger M \\mathbf{x} = M_{22}$. Since $M_{22} < 0$, the condition is violated. Thus, $e^S \\le e^T$ is false, providing a counterexample to the initial proposition.", "answer": "$$\\boxed{-0.259}$$", "id": "1863692"}]}