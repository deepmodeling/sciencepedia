{"hands_on_practices": [{"introduction": "This first exercise provides a concrete entry point into working with martingales. You are given a stochastic process that is explicitly defined as a martingale, and your task is to calculate a conditional probability related to its future behavior. This practice will help you build intuition for how the abstract properties of a martingale translate into tangible probabilistic statements about the underlying random variables [@problem_id:1359203].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\ldots$, drawn from a uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is a fixed parameter. Let $M_n = \\max\\{X_1, X_2, \\ldots, X_n\\}$ be the running maximum of the sequence.\n\nA stochastic process $Y_n$, for $n \\ge 1$, is constructed using a known constant $c$ such that $0 < c < 1$. The process is defined as:\n$$ Y_n = \\left(\\frac{1}{c}\\right)^n \\cdot I(M_n \\le c\\theta) $$\nwhere $I(A)$ is the indicator function that equals 1 if event $A$ is true, and 0 otherwise. This process, $Y_n$, is a martingale with respect to the natural filtration $\\mathcal{F}_n = \\sigma(X_1, \\ldots, X_n)$.\n\nSuppose that at time $n=3$, the process is observed to be non-zero, i.e., $Y_3 > 0$. Calculate the conditional probability that the process will have a value of zero at or before time $n=5$. Express your answer as a function of $c$.", "solution": "The problem asks for the conditional probability $P(Y_5 = 0 | Y_3 > 0)$.\n\nFirst, let's translate the events involving the process $Y_n$ into events involving the running maximum $M_n$.\nThe definition of the process is $Y_n = (1/c)^n \\cdot I(M_n \\le c\\theta)$. Since $c \\in (0,1)$, the term $(1/c)^n$ is always positive.\nTherefore, the value of $Y_n$ is zero if and only if the indicator function is zero.\n- The event $\\{Y_3 > 0\\}$ is equivalent to the event $\\{I(M_3 \\le c\\theta) = 1\\}$, which simplifies to $\\{M_3 \\le c\\theta\\}$.\n- The event $\\{Y_5 = 0\\}$ is equivalent to the event $\\{I(M_5 \\le c\\theta) = 0\\}$, which simplifies to $\\{M_5 > c\\theta\\}$. The problem asks for the probability that the process becomes zero *at or before* time $n=5$. This means we are interested in the event that the process is zero at time $n=4$ or at time $n=5$ (if it wasn't already zero at $n=4$). Let's analyze the event. The process $Y_n$ can only transition from a non-zero value to zero. Once it is zero, it stays zero, because if $M_k > c\\theta$ for some $k$, then for all $n \\ge k$, $M_n = \\max(M_k, X_{k+1}, \\dots, X_n) \\ge M_k > c\\theta$, which implies $Y_n=0$. Therefore, the event \"the process will become zero at or before time $n=5$\" is simply $\\{Y_5=0\\}$, which is $\\{M_5 > c\\theta\\}$.\n\nSo, the quantity we need to compute is $P(M_5 > c\\theta | M_3 \\le c\\theta)$.\n\nUsing the rule for conditional probability, it is often easier to compute the probability of the complement event:\n$$ P(M_5 > c\\theta | M_3 \\le c\\theta) = 1 - P(M_5 \\le c\\theta | M_3 \\le c\\theta) $$\n\nNow, let's evaluate the conditional probability $P(M_5 \\le c\\theta | M_3 \\le c\\theta)$. By definition:\n$$ P(M_5 \\le c\\theta | M_3 \\le c\\theta) = \\frac{P(\\{M_5 \\le c\\theta\\} \\cap \\{M_3 \\le c\\theta\\})}{P(M_3 \\le c\\theta)} $$\n\nLet's analyze the event in the numerator, $\\{M_5 \\le c\\theta\\} \\cap \\{M_3 \\le c\\theta\\}$.\nThe condition $M_5 \\le c\\theta$ means that $\\max\\{X_1, X_2, X_3, X_4, X_5\\} \\le c\\theta$. This implies that $X_i \\le c\\theta$ for all $i \\in \\{1, 2, 3, 4, 5\\}$.\nIf all of the first five variables are less than or equal to $c\\theta$, then it is necessarily true that the maximum of the first three, $M_3 = \\max\\{X_1, X_2, X_3\\}$, is also less than or equal to $c\\theta$.\nIn other words, the event $\\{M_5 \\le c\\theta\\}$ is a subset of the event $\\{M_3 \\le c\\theta\\}$, i.e., $\\{M_5 \\le c\\theta\\} \\subseteq \\{M_3 \\le c\\theta\\}$.\nTherefore, their intersection is just the smaller event: $\\{M_5 \\le c\\theta\\} \\cap \\{M_3 \\le c\\theta\\} = \\{M_5 \\le c\\theta\\}$.\n\nSo the conditional probability simplifies to:\n$$ P(M_5 \\le c\\theta | M_3 \\le c\\theta) = \\frac{P(M_5 \\le c\\theta)}{P(M_3 \\le c\\theta)} $$\n\nNext, we need to calculate the probability $P(M_n \\le c\\theta)$ for a general $n$.\nThe event $M_n \\le c\\theta$ is equivalent to the event that all $n$ random variables $X_1, \\ldots, X_n$ are less than or equal to $c\\theta$.\n$$ P(M_n \\le c\\theta) = P(X_1 \\le c\\theta, X_2 \\le c\\theta, \\ldots, X_n \\le c\\theta) $$\nSince the random variables $X_i$ are independent and identically distributed, this is:\n$$ P(M_n \\le c\\theta) = \\left(P(X_1 \\le c\\theta)\\right)^n $$\nFor a single random variable $X_1 \\sim U[0, \\theta]$, the probability is:\n$$ P(X_1 \\le c\\theta) = \\frac{\\text{length of } [0, c\\theta]}{\\text{length of } [0, \\theta]} = \\frac{c\\theta}{\\theta} = c $$\nSubstituting this back, we get:\n$$ P(M_n \\le c\\theta) = c^n $$\n\nNow we can calculate the conditional probability:\n$$ P(M_5 \\le c\\theta | M_3 \\le c\\theta) = \\frac{P(M_5 \\le c\\theta)}{P(M_3 \\le c\\theta)} = \\frac{c^5}{c^3} = c^2 $$\n\nFinally, we can find the desired probability:\n$$ P(M_5 > c\\theta | M_3 \\le c\\theta) = 1 - P(M_5 \\le c\\theta | M_3 \\le c\\theta) = 1 - c^2 $$\nThis is the conditional probability that the process will be zero at or before time $n=5$, given it was non-zero at time $n=3$.", "answer": "$$\\boxed{1-c^{2}}$$", "id": "1359203"}, {"introduction": "We now move to a classic application of martingale theory: the study of branching processes. Hypothetical scenarios like the growth of a bacterial colony, the propagation of a family name, or a nuclear chain reaction can be modeled using these processes. By properly scaling the population size, we can construct a non-negative martingale whose convergence, guaranteed by the Doob Martingale Convergence Theorem, is directly linked to the probability of the population's ultimate survival or extinction [@problem_id:1359221].", "problem": "A single, genetically engineered bacterium is placed in a nutrient-rich petri dish. At the end of each second, this bacterium, and any of its descendants, will either perish with probability $q$ or divide into two identical bacteria with probability $p$. For this process, a bacterium cannot remain as a single entity; it must either perish or divide, so $p+q=1$. Each bacterium in the colony behaves independently and according to these same probabilities. The colony is considered to have survived if its population does not go to zero in the long run. Assume that the probability of splitting, $p$, is strictly greater than the probability of perishing, $q$.\n\nDetermine the long-term probability that the initial bacterium gives rise to a colony that survives indefinitely. Your answer should be a closed-form expression in terms of $p$ and $q$.", "solution": "Model the colony as a Galtonâ€“Watson branching process with offspring distribution: each bacterium has either 0 offspring with probability $q$ (perishes) or 2 offspring with probability $p$ (splits), with $p+q=1$. Let $s$ denote the extinction probability starting from a single bacterium. By the branching property and independence, extinction occurs either immediately (with probability $q$) or, if the initial bacterium splits (with probability $p$), if both descendant lineages go extinct independently, which has probability $s^{2}$. Thus $s$ satisfies\n$$\ns = q + p s^{2}.\n$$\nRewriting gives the quadratic\n$$\np s^{2} - s + q = 0.\n$$\nSolving,\n$$\ns = \\frac{1 \\pm \\sqrt{1 - 4 p q}}{2 p}.\n$$\nUsing $p+q=1$, compute the discriminant:\n$$\n1 - 4 p q = 1 - 4 p (1 - p) = (1 - 2 p)^{2}.\n$$\nHence\n$$\ns = \\frac{1 \\pm |1 - 2 p|}{2 p}.\n$$\nGiven $p>q$ and $p+q=1$, we have $p>\\frac{1}{2}$, so $|1 - 2 p| = 2 p - 1$. The two roots are\n$$\ns_{1} = 1, \\qquad s_{2} = \\frac{1 - p}{p} = \\frac{q}{p}.\n$$\nThe extinction probability is the smallest root in $[0,1]$, namely $s = \\frac{q}{p}$. Therefore, the survival probability is\n$$\n1 - s = 1 - \\frac{q}{p} = \\frac{p - q}{p},\n$$\nwhich is positive since $p>q$.", "answer": "$$\\boxed{\\frac{p - q}{p}}$$", "id": "1359221"}, {"introduction": "This final practice problem challenges you to explore the nuances of martingale convergence. The Doob Martingale Convergence Theorem provides powerful guarantees for almost sure convergence, but this does not automatically imply convergence in other senses, such as in mean square ($L^2$). This exercise demonstrates a famous counterexample: a martingale that converges almost surely to a finite limit, yet whose variance explodes, preventing $L^2$ convergence [@problem_id:1317116]. Working through this will solidify your understanding of the different modes of convergence and the precise conditions under which they hold.", "problem": "Let $\\{X_k\\}_{k=1}^\\infty$ be a sequence of independent Rademacher random variables defined on a probability space $(\\Omega, \\mathcal{F}, P)$. A Rademacher random variable $X_k$ takes the value $1$ with probability $1/2$ and the value $-1$ with probability $1/2$. Let $\\mathcal{F}_n = \\sigma(X_1, X_2, \\dots, X_n)$ for $n \\ge 1$ be the natural filtration generated by the sequence.\n\nConsider the stochastic process $\\{M_n\\}_{n=1}^\\infty$ defined by\n$$M_n = \\prod_{k=1}^{n} \\left(1 + \\frac{X_k}{\\sqrt{k}}\\right)$$\nIt can be shown that $\\{M_n, \\mathcal{F}_n\\}_{n=1}^\\infty$ is a martingale.\n\nWhich of the following statements about the convergence of the martingale $M_n$ as $n \\to \\infty$ is correct?\n\nA. $M_n$ converges in $L^2$ but not almost surely.\n\nB. $M_n$ converges almost surely but not in $L^2$.\n\nC. $M_n$ converges both almost surely and in $L^2$.\n\nD. $M_n$ converges neither almost surely nor in $L^2$.\n\nE. The convergence behavior of $M_n$ cannot be determined from the given information.", "solution": "We first verify key properties of the process. For each $k \\geq 1$, the factor $1 + \\frac{X_{k}}{\\sqrt{k}}$ is almost surely nonnegative: for $k=1$ it equals either $0$ or $2$, and for $k \\geq 2$ it lies in $\\left(1 - \\frac{1}{\\sqrt{k}}, 1 + \\frac{1}{\\sqrt{k}}\\right) \\subset (0, \\infty)$. Hence $M_{n} \\geq 0$ almost surely for all $n$. Moreover, since the $X_{k}$ are independent, centered Rademacher variables and $\\mathcal{F}_{n} = \\sigma(X_{1},\\dots,X_{n})$, we have\n$$\nE\\!\\left[M_{n} \\mid \\mathcal{F}_{n-1}\\right]\n= M_{n-1}\\,E\\!\\left[1 + \\frac{X_{n}}{\\sqrt{n}} \\mid \\mathcal{F}_{n-1}\\right]\n= M_{n-1}\\,\\left(1 + \\frac{E[X_{n}]}{\\sqrt{n}}\\right)\n= M_{n-1}.\n$$\nThus $\\{M_{n},\\mathcal{F}_{n}\\}$ is a nonnegative martingale with $E[M_{n}]=E[M_{1}]=1$ for all $n$.\n\nBy Doobâ€™s martingale convergence theorem for nonnegative supermartingales (applied here to a nonnegative martingale), $M_{n}$ converges almost surely to a finite limit $M_{\\infty}$.\n\nWe now show that $M_{n}$ does not converge in $L^{2}$. Using independence across $k$,\n$$\nE[M_{n}^{2}] = E\\!\\left[\\prod_{k=1}^{n}\\left(1 + \\frac{X_{k}}{\\sqrt{k}}\\right)^{2}\\right]\n= \\prod_{k=1}^{n}E\\!\\left[\\left(1 + \\frac{X_{k}}{\\sqrt{k}}\\right)^{2}\\right].\n$$\nFor each $k$,\n$$\n\\left(1 + \\frac{X_{k}}{\\sqrt{k}}\\right)^{2} = 1 + \\frac{2X_{k}}{\\sqrt{k}} + \\frac{X_{k}^{2}}{k},\n$$\nso using $E[X_{k}]=0$ and $X_{k}^{2}=1$ almost surely,\n$$\nE\\!\\left[\\left(1 + \\frac{X_{k}}{\\sqrt{k}}\\right)^{2}\\right] = 1 + \\frac{1}{k} = \\frac{k+1}{k}.\n$$\nTherefore,\n$$\nE[M_{n}^{2}] = \\prod_{k=1}^{n}\\frac{k+1}{k} = n+1,\n$$\nwhich diverges to $+\\infty$ as $n \\to \\infty$. If $M_{n}$ converged in $L^{2}$, then $\\{E[M_{n}^{2}]\\}$ would be bounded and converge to $E[M_{\\infty}^{2}]$, a contradiction. Hence $M_{n}$ does not converge in $L^{2}$.\n\nCombining these facts, $M_{n}$ converges almost surely but not in $L^{2}$. The correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1317116"}]}