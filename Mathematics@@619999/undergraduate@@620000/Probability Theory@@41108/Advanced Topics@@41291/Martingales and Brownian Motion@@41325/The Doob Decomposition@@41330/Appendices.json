{"hands_on_practices": [{"introduction": "Let's begin with a foundational example that clearly illustrates the core principle of the Doob decomposition. In this problem, we analyze a process built from the cumulative sum of independent random variables, each with a known, time-dependent mean. This exercise [@problem_id:1397477] provides a straightforward application of the decomposition formula, allowing you to practice separating a clear, deterministic trend from the underlying random noise.", "problem": "Let $\\{Y_k\\}_{k=1}^\\infty$ be a sequence of independent random variables. The mean of each random variable is dependent on its index, given by $E[Y_k] = \\frac{1}{k}$ for $k \\ge 1$. Furthermore, assume that $E[|Y_k|] < \\infty$ for all $k$.\n\nConsider the stochastic process $X_n$ defined by the cumulative sum $X_n = \\sum_{k=1}^n Y_k$ for $n \\ge 1$, with the initial condition $X_0 = 0$. Let $\\{\\mathcal{F}_n\\}_{n=0}^\\infty$ be the natural filtration generated by the sequence $\\{Y_k\\}$, where $\\mathcal{F}_0 = \\{\\emptyset, \\Omega\\}$ is the trivial sigma-algebra and $\\mathcal{F}_n = \\sigma(Y_1, \\dots, Y_n)$ for $n \\ge 1$.\n\nAccording to the Doob Decomposition Theorem, any adapted process $X_n$ with finite expectation can be uniquely expressed as the sum of a martingale $M_n$ and a predictable process $A_n$, such that $X_n = M_n + A_n$.\n\nFind the explicit expressions for the martingale component $M_n$ and the predictable process component $A_n$ for $n \\geq 1$, subject to the standard initial conditions $M_0 = 0$ and $A_0 = 0$.\n\nPresent your answer as a row matrix containing two expressions: the first for $M_n$ and the second for $A_n$.", "solution": "We first recall the discrete-time Doob decomposition for an adapted, integrable process. For an adapted process $\\{X_n\\}_{n \\geq 0}$ with $E[|X_n|] < \\infty$, the Doob decomposition writes $X_n = M_n + A_n$, where $M_n$ is a martingale with $M_0 = 0$ and $A_n$ is a predictable process with $A_0 = 0$. In discrete time, the predictable component is given by\n$$\nA_n = \\sum_{k=1}^{n} E[X_k - X_{k-1} \\mid \\mathcal{F}_{k-1}],\n$$\nand the martingale component is $M_n = X_n - A_n$.\n\nIn our case, $X_n = \\sum_{k=1}^{n} Y_k$ with $X_0 = 0$. Therefore, the one-step increment is\n$$\nX_n - X_{n-1} = Y_n.\n$$\nThus, the predictable increment is\n$$\nE[X_n - X_{n-1} \\mid \\mathcal{F}_{n-1}] = E[Y_n \\mid \\mathcal{F}_{n-1}].\n$$\nSince $Y_n$ is independent of $\\mathcal{F}_{n-1} = \\sigma(Y_1,\\dots,Y_{n-1})$ and $E[|Y_n|] < \\infty$, we have\n$$\nE[Y_n \\mid \\mathcal{F}_{n-1}] = E[Y_n] = \\frac{1}{n}.\n$$\nTherefore, the predictable process is\n$$\nA_n = \\sum_{k=1}^{n} E[Y_k \\mid \\mathcal{F}_{k-1}] = \\sum_{k=1}^{n} \\frac{1}{k}.\n$$\nConsequently, the martingale component is\n$$\nM_n = X_n - A_n = \\sum_{k=1}^{n} Y_k - \\sum_{k=1}^{n} \\frac{1}{k} = \\sum_{k=1}^{n} \\left(Y_k - \\frac{1}{k}\\right).\n$$\nBy construction, $A_n$ is predictable since $A_n - A_{n-1} = \\frac{1}{n}$ is $\\mathcal{F}_{n-1}$-measurable, $A_0 = 0$, and $M_n$ is a martingale since\n$$\nE[M_n \\mid \\mathcal{F}_{n-1}] = E\\left[\\sum_{k=1}^{n} \\left(Y_k - \\frac{1}{k}\\right) \\Biggm| \\mathcal{F}_{n-1}\\right]\n= \\sum_{k=1}^{n-1} \\left(Y_k - \\frac{1}{k}\\right) + E\\left[Y_n - \\frac{1}{n} \\mid \\mathcal{F}_{n-1}\\right]\n= M_{n-1}.\n$$\nThus the Doob decomposition is uniquely identified as above.", "answer": "$$\\boxed{\\begin{pmatrix}\\sum_{k=1}^{n}\\left(Y_{k}-\\frac{1}{k}\\right) & \\sum_{k=1}^{n}\\frac{1}{k}\\end{pmatrix}}$$", "id": "1397477"}, {"introduction": "Now, let's explore a more subtle case where the predictable trend is not immediately obvious, by examining the absolute value of a simple symmetric random walk. This process is a submartingale, meaning it has a tendency to drift upwards. This practice [@problem_id:1397444] demonstrates the power of the Doob decomposition to uncover that hidden drift; you will discover that the predictable increase occurs only when the walk returns to its starting point, a concept related to the process's \"local time.\"", "problem": "Let $(Y_i)_{i \\ge 1}$ be a sequence of independent and identically distributed (i.i.d.) random variables, where the outcome of each variable is governed by the probability distribution $P(Y_i = 1) = P(Y_i = -1) = 1/2$. A simple symmetric random walk (SSRW), denoted by $(S_n)_{n \\ge 0}$, is defined by $S_0 = 0$ and $S_n = \\sum_{i=1}^n Y_i$ for $n \\ge 1$.\n\nThe natural filtration generated by this process is $(\\mathcal{F}_n)_{n \\ge 0}$, where $\\mathcal{F}_0 = \\{\\emptyset, \\Omega\\}$ (the trivial sigma-algebra) and $\\mathcal{F}_n = \\sigma(Y_1, \\dots, Y_n)$ for $n \\ge 1$.\n\nConsider the stochastic process $(X_n)_{n \\ge 0}$ defined by $X_n = |S_n|$. According to the Doob Decomposition Theorem, any process $X_n$ that is adapted to the filtration $(\\mathcal{F}_n)_{n \\ge 0}$ can be uniquely written as the sum $X_n = M_n + A_n$, where $(M_n)_{n \\ge 0}$ is a martingale with respect to $(\\mathcal{F}_n)_{n \\ge 0}$, and $(A_n)_{n \\ge 0}$ is a predictable process with $A_0 = 0$. A process $(A_n)_{n \\ge 1}$ is called predictable if for each $n \\ge 1$, the random variable $A_n$ is measurable with respect to the sigma-algebra $\\mathcal{F}_{n-1}$.\n\nYour task is to find the explicit expressions for the martingale component $M_n$ and the predictable component $A_n$ for the process $X_n = |S_n|$.\n\nUse the indicator function notation $\\mathbf{1}_{E}$, which equals 1 if the event $E$ occurs and 0 otherwise. Present your final answer as a $1 \\times 2$ matrix, with the first entry being the expression for $M_n$ and the second entry being the expression for $A_n$.", "solution": "The Doob Decomposition Theorem states that any adapted process $X_n$ can be uniquely written as $X_n = M_n + A_n$, where $M_n$ is a martingale and $A_n$ is a predictable process. The components are given by the formulas:\n$A_0 = 0$ and for $n \\ge 1$,\n$$A_n = \\sum_{k=1}^{n} \\left( E[X_k | \\mathcal{F}_{k-1}] - X_{k-1} \\right)$$\nThe martingale part is then given by $M_n = X_n - A_n$.\n\nOur process is $X_n = |S_n|$. We start by calculating the conditional expectation term $E[X_k | \\mathcal{F}_{k-1}]$.\n$$E[X_k | \\mathcal{F}_{k-1}] = E[|S_k| | \\mathcal{F}_{k-1}]$$\nWe know that $S_k = S_{k-1} + Y_k$. Since $S_{k-1}$ is $\\mathcal{F}_{k-1}$-measurable, we can treat its value as known given $\\mathcal{F}_{k-1}$. The random variable $Y_k$ is independent of $\\mathcal{F}_{k-1}$.\n$$E[|S_k| | \\mathcal{F}_{k-1}] = E[|S_{k-1} + Y_k| | \\mathcal{F}_{k-1}]$$\nBy the definition of expectation and the distribution of $Y_k$:\n$$E[|S_{k-1} + Y_k| | \\mathcal{F}_{k-1}] = \\frac{1}{2} |S_{k-1} + 1| + \\frac{1}{2} |S_{k-1} - 1|$$\nWe now analyze this expression by considering two cases for the value of $S_{k-1}$. Note that $S_n$ can only take integer values.\n\nCase 1: $S_{k-1} \\neq 0$.\nSince $S_{k-1}$ is a non-zero integer, $|S_{k-1}| \\ge 1$.\nIf $S_{k-1} \\ge 1$, then $S_{k-1}+1 > 0$ and $S_{k-1}-1 \\ge 0$. So, $|S_{k-1}+1| = S_{k-1}+1$ and $|S_{k-1}-1| = S_{k-1}-1$.\nThe conditional expectation is:\n$$\\frac{1}{2} (S_{k-1}+1) + \\frac{1}{2} (S_{k-1}-1) = \\frac{1}{2} (2S_{k-1}) = S_{k-1} = |S_{k-1}| = X_{k-1}$$\nIf $S_{k-1} \\le -1$, then $S_{k-1}+1 \\le 0$ and $S_{k-1}-1 < 0$. So, $|S_{k-1}+1| = -(S_{k-1}+1)$ and $|S_{k-1}-1| = -(S_{k-1}-1)$.\nThe conditional expectation is:\n$$\\frac{1}{2} (-(S_{k-1}+1)) + \\frac{1}{2} (-(S_{k-1}-1)) = \\frac{1}{2} (-S_{k-1}-1 -S_{k-1}+1) = \\frac{1}{2} (-2S_{k-1}) = -S_{k-1} = |S_{k-1}| = X_{k-1}$$\nIn both sub-cases where $S_{k-1} \\neq 0$, we have $E[X_k | \\mathcal{F}_{k-1}] = |S_{k-1}| = X_{k-1}$.\n\nCase 2: $S_{k-1} = 0$.\nThe conditional expectation becomes:\n$$\\frac{1}{2} |0 + 1| + \\frac{1}{2} |0 - 1| = \\frac{1}{2}(1) + \\frac{1}{2}(1) = 1$$\nAlso, note that if $S_{k-1} = 0$, then $X_{k-1} = |S_{k-1}| = 0$. So in this case, $E[X_k | \\mathcal{F}_{k-1}] = 1 = X_{k-1} + 1$.\n\nWe can combine both cases using the indicator function $\\mathbf{1}_{\\{E\\}}$, which is 1 if event $E$ is true, and 0 otherwise.\n$$E[X_k | \\mathcal{F}_{k-1}] = \\begin{cases} X_{k-1} & \\text{if } S_{k-1} \\neq 0 \\\\ X_{k-1} + 1 & \\text{if } S_{k-1} = 0 \\end{cases}$$\nThis can be written compactly as:\n$$E[X_k | \\mathcal{F}_{k-1}] = X_{k-1} + \\mathbf{1}_{\\{S_{k-1}=0\\}}$$\nNow we can compute the predictable process $A_n$:\n$$A_n = \\sum_{k=1}^{n} \\left( E[X_k | \\mathcal{F}_{k-1}] - X_{k-1} \\right)$$\nSubstituting our result for the conditional expectation:\n$$A_n = \\sum_{k=1}^{n} \\left( (X_{k-1} + \\mathbf{1}_{\\{S_{k-1}=0\\}}) - X_{k-1} \\right) = \\sum_{k=1}^{n} \\mathbf{1}_{\\{S_{k-1}=0\\}}$$\nChanging the index of summation from $k$ to $j = k-1$:\n$$A_n = \\sum_{j=0}^{n-1} \\mathbf{1}_{\\{S_j=0\\}}$$\nThis process $A_n$ counts the number of times the random walk has been at the origin up to time $n-1$. It is a predictable process as $A_n$ depends on the path of the walk only up to time $n-1$, and is therefore $\\mathcal{F}_{n-1}$-measurable. Since $S_0=0$ is given, the sum always includes the term for $j=0$, so $A_n \\ge 1$ for $n \\ge 1$.\n\nFinally, we find the martingale component $M_n$:\n$$M_n = X_n - A_n = |S_n| - \\sum_{k=0}^{n-1} \\mathbf{1}_{\\{S_k=0\\}}$$\nSo, the Doob decomposition of $X_n = |S_n|$ is $X_n = M_n + A_n$ with:\n$M_n = |S_n| - \\sum_{k=0}^{n-1} \\mathbf{1}_{\\{S_k=0\\}}$\n$A_n = \\sum_{k=0}^{n-1} \\mathbf{1}_{\\{S_k=0\\}}$", "answer": "$$\\boxed{\\begin{pmatrix} |S_n| - \\sum_{k=0}^{n-1} \\mathbf{1}_{\\{S_k=0\\}} & \\sum_{k=0}^{n-1} \\mathbf{1}_{\\{S_k=0\\}} \\end{pmatrix}}$$", "id": "1397444"}, {"introduction": "To showcase the versatility of the Doob decomposition, our final practice moves from random walks to a different class of models: Markov chains. We consider a simple system that switches between 'failed' and 'operational' states, a common scenario in reliability engineering and queuing theory. This exercise [@problem_id:1298497] will guide you in determining how the process's tendency to change state can be captured in a predictable component derived from its transition probabilities.", "problem": "Consider a system whose state at any discrete time step $n \\ge 0$ is described by a random variable $X_n$. The system can be in one of two states: 'failed', represented by state 0, or 'operational', represented by state 1. The sequence of states $(X_n)_{n \\ge 0}$ forms a time-homogeneous Markov chain on the state space $S = \\{0, 1\\}$.\n\nThe transition probabilities are defined as follows:\n- The probability of transitioning from the 'failed' state to the 'operational' state in one time step is $P(X_{n+1}=1 | X_n=0) = \\alpha$.\n- The probability of transitioning from the 'operational' state to the 'failed' state in one time step is $P(X_{n+1}=0 | X_n=1) = \\beta$.\nHere, $\\alpha$ and $\\beta$ are constants such that $0 < \\alpha < 1$ and $0 < \\beta < 1$.\n\nLet $\\mathcal{F}_n = \\sigma(X_0, X_1, \\dots, X_n)$ be the natural filtration generated by the process. The process $X_n$ admits a Doob decomposition of the form $X_n = M_n + A_n$, where $(M_n)_{n \\ge 0}$ is a martingale with respect to the filtration $(\\mathcal{F}_n)_{n \\ge 0}$, and $(A_n)_{n \\ge 0}$ is a predictable process with $A_0 = 0$.\n\nFind the explicit expressions for the martingale $M_n$ and the predictable process $A_n$ for $n \\ge 1$. Your answer should be a pair of expressions $(M_n, A_n)$ in terms of $\\alpha$, $\\beta$, and the past values of the process $(X_k)_{k=0, \\dots, n}$.", "solution": "The Doob decomposition of an adapted process $X_n$ is given by $X_n = M_n + A_n$, where $M_n$ is a martingale and $A_n$ is a predictable process with $A_0=0$. The predictable process $A_n$ is defined by the recurrence relation $A_n - A_{n-1} = E[X_n | \\mathcal{F}_{n-1}] - X_{n-1}$ for $n \\ge 1$. The martingale $M_n$ is then given by $M_n = X_n - A_n$.\n\nFirst, we need to compute the conditional expectation $E[X_n | \\mathcal{F}_{n-1}]$. Due to the Markov property of the process $(X_n)$, this expectation depends only on the state at time $n-1$.\n$$E[X_n | \\mathcal{F}_{n-1}] = E[X_n | X_{n-1}]$$\nThe random variable $X_n$ can take values in $\\{0, 1\\}$, so its expectation is given by:\n$$E[X_n | X_{n-1}] = 1 \\cdot P(X_n=1 | X_{n-1}) + 0 \\cdot P(X_n=0 | X_{n-1}) = P(X_n=1 | X_{n-1})$$\nWe evaluate this conditional probability for the two possible values of $X_{n-1}$:\n- If $X_{n-1} = 0$, then $P(X_n=1 | X_{n-1}=0) = \\alpha$.\n- If $X_{n-1} = 1$, then $P(X_n=1 | X_{n-1}=1) = 1 - P(X_n=0 | X_{n-1}=1) = 1-\\beta$.\n\nWe can write a single expression for $E[X_n | X_{n-1}]$ that is valid for both cases. Since $X_{n-1}$ is either 0 or 1, we can use it as an indicator variable:\n$$E[X_n | X_{n-1}] = \\alpha (1-X_{n-1}) + (1-\\beta) X_{n-1} = \\alpha + X_{n-1} - \\alpha X_{n-1} - \\beta X_{n-1} = \\alpha + (1-\\alpha-\\beta)X_{n-1}$$\n\nNow we can find the increment of the predictable process, $\\Delta A_n = A_n - A_{n-1}$:\n$$\\Delta A_n = E[X_n | \\mathcal{F}_{n-1}] - X_{n-1} = (\\alpha + (1-\\alpha-\\beta)X_{n-1}) - X_{n-1}$$\n$$\\Delta A_n = \\alpha + X_{n-1} - \\alpha X_{n-1} - \\beta X_{n-1} - X_{n-1} = \\alpha - (\\alpha+\\beta)X_{n-1}$$\nThe process $A_n$ is obtained by summing these increments from $k=1$ to $n$, given that $A_0=0$:\n$$A_n = \\sum_{k=1}^{n} \\Delta A_k = \\sum_{k=1}^{n} (\\alpha - (\\alpha+\\beta)X_{k-1})$$\n$$A_n = n\\alpha - (\\alpha+\\beta)\\sum_{k=1}^{n} X_{k-1}$$\nBy shifting the index of summation, we get the final expression for $A_n$:\n$$A_n = n\\alpha - (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k$$\nThis is the explicit expression for the predictable part of the decomposition for $n \\ge 1$.\n\nNext, we find the martingale part, $M_n$. By definition, $M_n = X_n - A_n$.\nSubstituting the expression for $A_n$ we just found:\n$$M_n = X_n - \\left( n\\alpha - (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k \\right)$$\n$$M_n = X_n - n\\alpha + (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k$$\nThis is the explicit expression for the martingale part of the decomposition for $n \\ge 1$.\n\nThe pair of expressions for the Doob decomposition $(M_n, A_n)$ is:\n$$M_n = X_n - n\\alpha + (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k$$\n$$A_n = n\\alpha - (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k$$", "answer": "$$\\boxed{\\begin{pmatrix}X_n - n\\alpha + (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k & n\\alpha - (\\alpha+\\beta)\\sum_{k=0}^{n-1} X_k\\end{pmatrix}}$$", "id": "1298497"}]}