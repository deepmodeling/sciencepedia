{"hands_on_practices": [{"introduction": "We begin with a classic model of population growth, the Galton-Watson process. By normalizing the population size by its expected exponential growth, we create a non-negative martingale, a key structure in stochastic analysis. This practice [@problem_id:1359392] will guide you in using Doob's maximal inequality to find a sharp upper bound on the probability of the population far exceeding its expected size, offering a fundamental insight into managing random fluctuations.", "problem": "Consider a simplified model for the proliferation of a self-replicating nanomachine. The process begins with a single machine, which we denote as generation zero. Let $Z_k$ represent the number of machines in generation $k$. The process starts with $Z_0 = 1$.\n\nEach machine from generation $k-1$ independently creates a random number of \"offspring\" machines for generation $k$. The number of offspring produced by any single machine is a random variable drawn from a common distribution with a mean of $\\mu = 4.0$. This type of stochastic process is known as a Galton-Watson process.\n\nTo study the growth of the population relative to its expectation, we define a normalized population size $X_k = Z_k / \\mu^k$ for $k \\ge 0$. You are given that the sequence of random variables $\\{X_k\\}_{k \\ge 0}$ forms a martingale with respect to the information available up to generation $k$.\n\nAn operational failure is declared if the normalized population size at any generation up to the 20th generation (i.e., for any integer $k$ from 0 to 20) reaches or surpasses a critical threshold of $C=5.0$.\n\nCalculate the tightest possible upper bound for the probability of such a failure, based only on the information provided. Provide your answer as a decimal.", "solution": "We model the number of machines by a Galton-Watson process with offspring mean $\\mu=4.0$, starting from $Z_{0}=1$. Define the normalized process $X_{k}=Z_{k}/\\mu^{k}$ for $k\\geq 0$. By the given information, $\\{X_{k}\\}_{k\\geq 0}$ is a martingale with respect to the natural filtration, and since $Z_{k}\\geq 0$ almost surely and $\\mu^{k}>0$, it follows that $X_{k}\\geq 0$ almost surely for all $k$.\n\nLet the failure event be\n$$\nA=\\left\\{\\max_{0\\leq k\\leq 20} X_{k}\\geq C\\right\\},\n$$\nwith $C=5.0$. Because $\\{X_{k}\\}$ is a nonnegative martingale, it is in particular a nonnegative supermartingale. Doob’s maximal inequality for nonnegative supermartingales states that for any $a>0$ and any integer $n\\geq 0$,\n$$\n\\mathbb{P}\\!\\left(\\max_{0\\leq k\\leq n} X_{k}\\geq a\\right)\\leq \\frac{\\mathbb{E}[X_{0}]}{a}.\n$$\nApplying this with $a=C$ and $n=20$ yields\n$$\n\\mathbb{P}(A)\\leq \\frac{\\mathbb{E}[X_{0}]}{C}.\n$$\nSince $X_{0}=Z_{0}/\\mu^{0}=1$, we have $\\mathbb{E}[X_{0}]=1$, so\n$$\n\\mathbb{P}(A)\\leq \\frac{1}{C}=\\frac{1}{5.0}=0.2.\n$$\nThis bound is the tightest possible upper bound based solely on the provided information (nonnegativity and martingale property), as Doob’s maximal inequality is sharp in general for nonnegative martingales.", "answer": "$$\\boxed{0.2}$$", "id": "1359392"}, {"introduction": "Next, we explore the connection between random walks and martingales, a cornerstone of modern probability theory. For a particle diffusing between two absorbing barriers, the probability of it reaching a specific end-point is a harmonic function of its position. This practice [@problem_id:1359400] reveals that evaluating this probability along the particle's random path generates a martingale, allowing us to use Doob's inequality to bound the chance of this \"escape probability\" becoming unusually high.", "problem": "A simplified one-dimensional model for the diffusion of a charge carrier is considered. The carrier is constrained to move along a chain of atomic sites, labeled by the integers $\\{0, 1, 2, \\dots, N\\}$. The carrier starts at site $X_0 = k$, where $0 < k < N$. At each discrete time step, if the carrier is at an interior site $i \\in \\{1, 2, \\dots, N-1\\}$, it jumps to either site $i-1$ or site $i+1$ with equal probability. The sites $0$ and $N$ act as \"traps\"; if the carrier reaches either of these sites, it is immediately absorbed and its motion stops.\n\nLet $h(i)$ denote the probability that a carrier starting at site $i$ is eventually absorbed at site $N$ rather than at site $0$. This function depends only on $i$ and $N$. Now, consider the stochastic process $H_t = h(X_t)$, which represents the evolving probability of eventual absorption at $N$ based on the carrier's current position $X_t$.\n\nFor a system with $N=50$ and a carrier starting at $X_0 = k=10$, find a rigorous and non-trivial upper bound for the probability that the value of $H_t$ reaches or exceeds a threshold of $\\alpha=0.8$ at any time step $t$ within the first $n$ steps of its random walk (i.e., for $0 \\le t \\le n$). Note that the bound you derive might be independent of the total number of steps $n$.\n\nExpress your answer as a decimal rounded to three significant figures.", "solution": "We consider the simple symmetric random walk on the sites $\\{0,1,\\dots,N\\}$ with absorption at $0$ and $N$. The probability $h(i)$ of eventual absorption at $N$ starting from $i$ satisfies the harmonicity condition on interior sites,\n$$\nh(i)=\\frac{h(i-1)+h(i+1)}{2}\\quad\\text{for }i\\in\\{1,\\dots,N-1\\},\n$$\nwith boundary conditions $h(0)=0$ and $h(N)=1$. The general solution of the linear second-difference equation is affine, $h(i)=A i+B$. Applying the boundary conditions gives $B=0$ and $A=\\frac{1}{N}$, hence\n$$\nh(i)=\\frac{i}{N}.\n$$\nDefine $H_{t}=h(X_{t})=\\frac{X_{t}}{N}$. Then $H_{t}$ is a bounded martingale with respect to the natural filtration, because for interior sites\n$$\n\\mathbb{E}[H_{t+1}\\mid\\mathcal{F}_{t}]=\\frac{h(X_{t}-1)+h(X_{t}+1)}{2}=h(X_{t})=H_{t},\n$$\nand at absorbing sites the process is constant. In particular, $\\mathbb{E}[H_{t}]=\\mathbb{E}[H_{0}]=h(k)=\\frac{k}{N}$ for all $t$.\n\nWe are given $N=50$ and $k=10$, so\n$$\nH_{0}=h(10)=\\frac{10}{50}=\\frac{1}{5}.\n$$\nWe want an upper bound on\n$$\n\\mathbb{P}\\!\\left(\\sup_{0\\leq t\\leq n}H_{t}\\geq \\alpha\\right)\\quad\\text{with }\\alpha=0.8=\\frac{4}{5}.\n$$\nApplying Doob’s maximal inequality for nonnegative submartingales (which applies to the nonnegative martingale $H_{t}$) yields\n$$\n\\mathbb{P}\\!\\left(\\sup_{0\\leq t\\leq n}H_{t}\\geq \\frac{4}{5}\\right)\\leq \\frac{\\mathbb{E}[H_{n}]}{4/5}=\\frac{H_{0}}{4/5}=\\frac{(1/5)}{(4/5)}=\\frac{1}{4}=0.25,\n$$\na bound independent of $n$.\n\nAn equivalent route is to note that $H_{t}\\geq \\frac{4}{5}$ if and only if $X_{t}\\geq 40$. Therefore\n$$\n\\left\\{\\sup_{0\\leq t\\leq n}H_{t}\\geq \\frac{4}{5}\\right\\}\\subseteq\\{\\tau_{40}<\\tau_{0}\\},\n$$\nwhere $\\tau_{j}$ is the hitting time of site $j$. Using the optional stopping theorem for the bounded martingale $H_{t}$ at $T=\\tau_{40}\\wedge\\tau_{0}$ gives\n$$\n\\mathbb{E}[H_{T}]=H_{0}=\\frac{1}{5},\\quad H_{T}=\\frac{40}{50}\\mathbf{1}_{\\{\\tau_{40}<\\tau_{0}\\}}+0\\cdot\\mathbf{1}_{\\{\\tau_{0}<\\tau_{40}\\}}=\\frac{4}{5}\\mathbf{1}_{\\{\\tau_{40}<\\tau_{0}\\}},\n$$\nhence\n$$\n\\mathbb{P}(\\tau_{40}<\\tau_{0})=\\frac{H_{0}}{4/5}=\\frac{1}{4}.\n$$\nTherefore, for any $n$,\n$$\n\\mathbb{P}\\!\\left(\\sup_{0\\leq t\\leq n}H_{t}\\geq \\frac{4}{5}\\right)\\leq \\mathbb{P}(\\tau_{40}<\\tau_{0})=\\frac{1}{4}=0.25.\n$$\nExpressed as a decimal rounded to three significant figures, the rigorous non-trivial upper bound is $0.250$.", "answer": "$$\\boxed{0.250}$$", "id": "1359400"}, {"introduction": "Our final practice ventures into the realm of statistics and data science, showcasing one of the most powerful applications of martingale theory: sequential hypothesis testing. The likelihood ratio, which measures the evidence for one hypothesis over another, forms a non-negative martingale under the null hypothesis. This exercise [@problem_id:1359388] applies Doob's maximal inequality to bound the probability of incorrectly rejecting the null hypothesis, a fundamental concept in designing efficient and reliable statistical tests.", "problem": "A data scientist is analyzing a stream of binary data, $X_1, X_2, \\dots$, presumed to be a sequence of independent and identically distributed Bernoulli trials. There are two competing hypotheses about the underlying data-generating process. The null hypothesis, $H_0$, states that the probability of observing a '1' is $p_0 = 0.4$. An alternative hypothesis, $H_1$, suggests a different generative model where the probability of observing a '1' is $p_1 = 0.6$.\n\nTo decide between these hypotheses, the scientist employs a sequential testing procedure. After each observation $X_n$, they compute the likelihood ratio $L_n$, which is the ratio of the probability of the observed sequence $\\{X_1, \\dots, X_n\\}$ under $H_1$ to its probability under $H_0$. The test is designed to stop and accept the alternative hypothesis $H_1$ if the likelihood ratio $L_n$ ever reaches or exceeds a pre-defined threshold $A=12.5$.\n\nAssuming the null hypothesis $H_0$ is actually true, determine the tightest possible upper bound on the probability that the test incorrectly stops and accepts $H_1$ at any point during the observation of the first $N=500$ data points. Express your answer as a decimal.", "solution": "Let $X_{1},X_{2},\\dots$ be i.i.d. Bernoulli with parameter $p_{0}$ under $H_{0}$. The likelihood ratio after $n$ observations for testing $H_{0}:p=p_{0}$ versus $H_{1}:p=p_{1}$ is\n$$\nL_{n}=\\prod_{i=1}^{n}\\frac{\\mathbb{P}_{1}(X_{i})}{\\mathbb{P}_{0}(X_{i})}\n=\\prod_{i=1}^{n}\\left(\\frac{p_{1}}{p_{0}}\\right)^{X_{i}}\\left(\\frac{1-p_{1}}{1-p_{0}}\\right)^{1-X_{i}}.\n$$\nUnder $H_{0}$, $\\{L_{n}\\}$ is a nonnegative martingale with respect to the filtration generated by the data, since\n$$\n\\mathbb{E}_{0}\\!\\left[L_{n+1}\\mid\\mathcal{F}_{n}\\right]\n=L_{n}\\,\\mathbb{E}_{0}\\!\\left[\\left(\\frac{p_{1}}{p_{0}}\\right)^{X_{n+1}}\\left(\\frac{1-p_{1}}{1-p_{0}}\\right)^{1-X_{n+1}}\\middle|\\mathcal{F}_{n}\\right]\n=L_{n}\\left(p_{0}\\frac{p_{1}}{p_{0}}+(1-p_{0})\\frac{1-p_{1}}{1-p_{0}}\\right)=L_{n},\n$$\nand hence $\\mathbb{E}_{0}[L_{n}]=\\mathbb{E}_{0}[L_{0}]=1$ for all $n$.\n\nDefine the error event within the first $N$ observations as $\\{\\max_{1\\leq n\\leq N}L_{n}\\geq A\\}$. By Ville’s inequality (equivalently, Doob’s maximal inequality for nonnegative martingales),\n$$\n\\mathbb{P}_{0}\\!\\left(\\max_{1\\leq n\\leq N}L_{n}\\geq A\\right)\\leq \\frac{\\mathbb{E}_{0}[L_{N}]}{A}=\\frac{1}{A}.\n$$\nWith the given threshold $A=12.5$, this yields the tightest general upper bound\n$$\n\\mathbb{P}_{0}\\!\\left(\\text{accept }H_{1}\\text{ at some }n\\leq N\\right)\\leq \\frac{1}{12.5}=0.08.\n$$\nThis bound is tight in the sense that, without additional assumptions beyond the martingale property of the likelihood ratio and the threshold $A$, no uniformly smaller upper bound is guaranteed.", "answer": "$$\\boxed{0.08}$$", "id": "1359388"}]}