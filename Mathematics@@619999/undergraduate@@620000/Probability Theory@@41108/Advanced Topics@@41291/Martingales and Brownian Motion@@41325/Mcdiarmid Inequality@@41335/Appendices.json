{"hands_on_practices": [{"introduction": "Understanding the structure of random graphs is fundamental in fields from network theory to epidemiology. This first practice problem provides a clear and direct application of McDiarmid's inequality to count 'source' vertices—nodes with no incoming connections—in a random directed graph [@problem_id:1372535]. By treating the existence of each potential edge as an independent random variable, you will practice the core skill of calculating the 'bounded difference' and see how even complex global properties can exhibit strong concentration around their average value.", "problem": "Consider a random directed graph, denoted as $D(n, p)$, constructed on a set of $n$ vertices labeled $\\{1, 2, \\dots, n\\}$. For every ordered pair of distinct vertices $(i, j)$ where $i \\neq j$, a directed edge from $i$ to $j$ is included in the graph with a probability $p \\in (0, 1)$, independently of all other edges.\n\nA vertex is defined as a \"source vertex\" if its in-degree is zero, meaning there are no directed edges pointing towards it. Let $X$ be the random variable representing the total number of source vertices in a randomly generated instance of $D(n, p)$.\n\nYour task is to derive a general upper bound for the probability that the number of source vertices, $X$, deviates from its expected value, $\\mathbb{E}[X]$, by at least a positive amount $t$. The final expression for this probability bound must be a closed-form analytic expression in terms of the number of vertices, $n$, and the deviation, $t$.", "solution": "Let the directed edges be represented by independent Bernoulli random variables $\\{Y_{ij}\\}_{i \\neq j}$ where $Y_{ij}=1$ if the edge $i \\to j$ is present and $Y_{ij}=0$ otherwise, each with parameter $p$. There are $m=n(n-1)$ such variables. For each vertex $v \\in \\{1,\\dots,n\\}$, define the indicator\n$$\nI_{v}=\\mathbf{1}\\{\\text{$v$ has in-degree }0\\}=\\prod_{u \\neq v}\\mathbf{1}\\{Y_{uv}=0\\}.\n$$\nThe total number of source vertices is\n$$\nX=\\sum_{v=1}^{n} I_{v}.\n$$\nBy independence of incoming edges to a fixed $v$, we have\n$$\n\\mathbb{E}[I_{v}]=\\prod_{u \\neq v}\\mathbb{P}(Y_{uv}=0)=(1-p)^{n-1},\n$$\nso linearity of expectation gives\n$$\n\\mathbb{E}[X]=\\sum_{v=1}^{n}\\mathbb{E}[I_{v}]=n(1-p)^{n-1}.\n$$\n\nWe now derive a concentration bound for $X$ around $\\mathbb{E}[X]$ using McDiarmid's bounded differences inequality. View $X$ as a function $f$ of the independent variables $\\{Y_{ij}\\}_{i \\neq j}$:\n$$\nX=f\\big(\\{Y_{ij}\\}_{i \\neq j}\\big).\n$$\nIf two edge configurations differ only in a single coordinate $Y_{ij}$, then only the in-degree of vertex $j$ can change, and hence only $I_{j}$ can change. Therefore the value of $X$ can change by at most $1$:\n$$\n|f(\\dots,Y_{ij},\\dots)-f(\\dots,Y_{ij}',\\dots)| \\leq 1 \\quad \\text{for all } i \\neq j.\n$$\nThus each coordinate has Lipschitz constant $c_{ij}=1$, and\n$$\n\\sum_{i \\neq j} c_{ij}^{2}=\\sum_{i \\neq j} 1^{2}=n(n-1)=m.\n$$\nBy McDiarmid's inequality, for all $t>0$,\n$$\n\\mathbb{P}\\big(|X-\\mathbb{E}[X]|\\geq t\\big) \\leq 2 \\exp\\!\\left(-\\frac{2t^{2}}{\\sum_{i \\neq j} c_{ij}^{2}}\\right)\n= 2 \\exp\\!\\left(-\\frac{2t^{2}}{n(n-1)}\\right).\n$$\nThis yields a closed-form upper bound in terms of $n$ and $t$, independent of $p$.", "answer": "$$\\boxed{2\\,\\exp\\!\\left(-\\frac{2t^{2}}{n(n-1)}\\right)}$$", "id": "1372535"}, {"introduction": "Moving from graph structures to data analysis, this exercise explores the concentration of 'singleton' items in a random data stream—a common task in areas like network traffic monitoring or database analysis [@problem_id:1372557]. This problem requires a more nuanced argument to bound the effect of changing a single data point, as it can potentially alter the status of two different analytical categories. It's an excellent opportunity to refine your ability to determine the bounded difference constant in situations where the effect of a change is not immediately obvious.", "problem": "A university's network security system monitors incoming data packets to detect unusual activity. In a simplified model, each of the $n$ packets arriving in a specific time window is assigned a \"source hash,\" which is an integer value. Each source hash is generated independently and is uniformly distributed over the set of $m$ possible integer values, $\\{1, 2, \\ldots, m\\}$.\n\nAn analyst is interested in identifying \"singleton sources,\" defined as sources from which exactly one packet originates during the time window. Let the random variable $U$ represent the number of such singleton sources observed. While the exact expected number of singletons, $E[U]$, can be complex to compute, it is crucial to establish a strong guarantee about how much $U$ is likely to deviate from its expectation.\n\nUsing an appropriate concentration inequality, find a non-trivial upper bound for the probability $P(|U - E[U]| \\ge \\epsilon n)$, where $\\epsilon$ is a small positive constant representing the fractional deviation from the mean. Your answer should be a closed-form analytic expression in terms of $n$ and $\\epsilon$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent random variables taking values in $\\{1,2,\\ldots,m\\}$ uniformly. Let $N_{j}=\\sum_{i=1}^{n}\\mathbf{1}\\{X_{i}=j\\}$ be the count of packets from source $j$, and define\n$$\nU=\\sum_{j=1}^{m}\\mathbf{1}\\{N_{j}=1\\}.\n$$\nThus $U=f(X_{1},\\ldots,X_{n})$ is a deterministic function of the independent inputs.\n\nWe apply McDiarmid’s bounded differences inequality. For this, we bound the effect of changing a single coordinate. Fix $k\\in\\{1,\\ldots,n\\}$, and consider changing $X_{k}$ from value $a$ to value $b$. Only the counts $N_{a}$ and $N_{b}$ can change (by at most one each), so only the indicators $\\mathbf{1}\\{N_{a}=1\\}$ and $\\mathbf{1}\\{N_{b}=1\\}$ can be affected. Consequently, the total change in $U$ satisfies\n$$\n|f(x_{1},\\ldots,x_{k},\\ldots,x_{n})-f(x_{1},\\ldots,x_{k}',\\ldots,x_{n})|\\leq 2,\n$$\nfor all choices of $x_{1},\\ldots,x_{n}$ and $x_{k}'$. Hence the bounded differences constants can be taken as $c_{i}=2$ for all $i$, and\n$$\n\\sum_{i=1}^{n}c_{i}^{2}=4n.\n$$\n\nMcDiarmid’s inequality states that for all $t>0$,\n$$\n\\mathbb{P}\\big(f(X_{1},\\ldots,X_{n})-\\mathbb{E}[f(X_{1},\\ldots,X_{n})]\\geq t\\big)\\leq \\exp\\!\\left(-\\frac{2t^{2}}{\\sum_{i=1}^{n}c_{i}^{2}}\\right),\n$$\nand the same bound holds for the lower tail. Applying this with $f=U$ and $t=\\epsilon n$, we obtain the two-sided deviation bound\n$$\n\\mathbb{P}\\big(|U-\\mathbb{E}[U]|\\geq \\epsilon n\\big)\\leq 2\\exp\\!\\left(-\\frac{2(\\epsilon n)^{2}}{4n}\\right)=2\\exp\\!\\left(-\\frac{\\epsilon^{2}n}{2}\\right).\n$$\n\nThis gives a closed-form, exponentially decaying upper bound in terms of $n$ and $\\epsilon$, independent of $m$.", "answer": "$$\\boxed{2\\exp\\!\\left(-\\frac{1}{2}\\,\\epsilon^{2} n\\right)}$$", "id": "1372557"}, {"introduction": "McDiarmid's inequality is not limited to counting problems or discrete variables; its power extends to complex optimization problems in continuous spaces. This practice problem tackles the famous Traveling Salesperson Problem (TSP) for points randomly placed in a square, a classic scenario in stochastic optimization [@problem_id:1372547]. You will employ a clever geometric argument using the triangle inequality to bound the change in the optimal tour length, demonstrating how this powerful tool can provide stability guarantees for even NP-hard problems with random inputs.", "problem": "In the field of stochastic optimization, we are interested in the stability of solutions to problems with random inputs. Consider the Traveling Salesperson Problem (TSP) for a set of $n$ points, $X_1, X_2, \\dots, X_n$, chosen independently and uniformly at random from the unit square $[0, 1]^2$. Let $L_n = f(X_1, \\dots, X_n)$ be the length of the shortest possible tour that visits every point and returns to the origin. The value of $L_n$ is a random variable, and we wish to understand how much it can deviate from its expected value, $E[L_n]$.\n\nMcDiarmid's inequality provides a powerful tool for this. It states that if a function $f(x_1, \\dots, x_n)$ of independent random variables satisfies the bounded differences property, such that for any $i \\in \\{1, \\dots, n\\}$ and any values $x_1, \\dots, x_n, x'_i$ in the domain,\n$$ |f(x_1, \\dots, x_i, \\dots, x_n) - f(x_1, \\dots, x'_i, \\dots, x_n)| \\le c_i $$\nthen for any $t > 0$, the following concentration inequality holds:\n$$ P(|f(X_1, \\dots, X_n) - E[f(X_1, \\dots, X_n)]| \\ge t) \\le 2\\exp\\left(-\\frac{2t^2}{\\sum_{i=1}^n c_i^2}\\right) $$\n\nYour task is to apply this inequality to the TSP length $L_n$. Determine an upper bound for the probability $P(|L_n - E[L_n]| \\ge t)$. Your final answer should be an analytical expression in terms of $n$ and $t$.", "solution": "Let $L_{n}=f(X_{1},\\dots,X_{n})$ be the length of the optimal Euclidean TSP tour through the set $\\{X_{1},\\dots,X_{n}\\}$ and the fixed origin in $[0,1]^{2}$. The random variables $X_{1},\\dots,X_{n}$ are independent, satisfying the independence assumption of McDiarmid's inequality.\n\nTo verify the bounded differences property, fix $i\\in\\{1,\\dots,n\\}$ and configurations $(x_{1},\\dots,x_{n})$ and $(x_{1},\\dots,x'_{i},\\dots,x_{n})$. Let $L$ be the optimal tour length for $(x_{1},\\dots,x_{n})$, and let $T$ be an optimal tour achieving length $L$. In $T$, the point $x_{i}$ has two tour neighbors, say $A$ and $B$ (each is either one of the other $x_{j}$ or the origin). Consider the tour $T'$ for the modified instance $(x_{1},\\dots,x'_{i},\\dots,x_{n})$ that preserves all adjacencies of $T$ except replacing the edges $(A,x_{i})$ and $(x_{i},B)$ by $(A,x'_{i})$ and $(x'_{i},B)$. Then the change in length satisfies\n$$\n\\text{len}(T')-\\text{len}(T)\n=\\big(d(A,x'_{i})-d(A,x_{i})\\big)+\\big(d(B,x'_{i})-d(B,x_{i})\\big),\n$$\nwhere $d(\\cdot,\\cdot)$ is the Euclidean distance. By the reverse triangle inequality, for any points $a,p,q$,\n$$\n\\big|d(a,p)-d(a,q)\\big|\\le d(p,q).\n$$\nApplying this twice yields\n$$\n\\text{len}(T')-\\text{len}(T)\\le d(x_{i},x'_{i})+d(x_{i},x'_{i})=2\\,d(x_{i},x'_{i}).\n$$\nSince the optimal length $L'$ for $(x_{1},\\dots,x'_{i},\\dots,x_{n})$ is at most $\\text{len}(T')$, we get $L'-L\\le 2\\,d(x_{i},x'_{i})$. By symmetry (interchanging the roles of $x_{i}$ and $x'_{i}$), we also obtain $L-L'\\le 2\\,d(x_{i},x'_{i})$. Therefore,\n$$\n\\big|f(x_{1},\\dots,x_{i},\\dots,x_{n})-f(x_{1},\\dots,x'_{i},\\dots,x_{n})\\big|\\le 2\\,d(x_{i},x'_{i}).\n$$\nBecause all points lie in $[0,1]^{2}$, the maximum possible distance is $d(x_{i},x'_{i})\\le \\sqrt{2}$, hence we can take\n$$\nc_{i}=2\\sqrt{2}\\quad\\text{for all }i\\in\\{1,\\dots,n\\}.\n$$\nThus $\\sum_{i=1}^{n}c_{i}^{2}=n\\,(2\\sqrt{2})^{2}=8n$. McDiarmid's inequality then gives, for any $t>0$,\n$$\nP\\big(\\,|L_{n}-E[L_{n}]|\\ge t\\,\\big)\\le 2\\exp\\!\\left(-\\frac{2t^{2}}{\\sum_{i=1}^{n}c_{i}^{2}}\\right)\n=2\\exp\\!\\left(-\\frac{2t^{2}}{8n}\\right)\n=2\\exp\\!\\left(-\\frac{t^{2}}{4n}\\right).\n$$\nThis is the desired upper bound expressed in terms of $n$ and $t$.", "answer": "$$\\boxed{2\\exp\\!\\left(-\\frac{t^{2}}{4n}\\right)}$$", "id": "1372547"}]}