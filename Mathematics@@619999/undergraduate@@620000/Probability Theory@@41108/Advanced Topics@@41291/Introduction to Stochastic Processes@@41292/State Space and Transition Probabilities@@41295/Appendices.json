{"hands_on_practices": [{"introduction": "Understanding the evolution of a system over time is at the heart of studying stochastic processes. This first practice provides a gentle introduction to the computational core of Markov chains. We will explore a hypothetical scenario of an algorithm generating a musical sequence, which serves as a simple and intuitive model for a system transitioning between a finite number of states. By calculating the probability of a specific note appearing at a certain position, you will practice applying the fundamental rules of transition probabilities and the law of total probability. [@problem_id:1356270]", "problem": "An experimental music composition algorithm generates a sequence of notes. The set of possible notes is limited to a simple triad: {C, E, G}. The selection of each subsequent note in the sequence is a probabilistic process that depends only on the immediately preceding note.\n\nThe process is initiated with the first note of the composition, which is always C.\n\nThe probabilities governing the transition from one note to the next are defined as follows:\n- If the current note is C, the next note will be C with a probability of 0.2, E with a probability of 0.3, and G with a probability of 0.5.\n- If the current note is E, the next note will be C with a probability of 0.4, E with a probability of 0.1, and G with a probability of 0.5.\n- If the current note is G, the next note will be C with a probability of 0.6, E with a probability of 0.2, and G with a probability of 0.2.\n\nCalculate the exact probability that the third note in the sequence is G. Express your answer as a decimal.", "solution": "We model the notes as a Markov chain with states $\\{C,E,G\\}$ and initial state $C$ with probability $1$. Let the second and third notes be denoted by $N_{2}$ and $N_{3}$, respectively. By the law of total probability and the Markov property,\n$$\n\\Pr(N_{3}=G)=\\Pr(N_{2}=C)\\Pr(G\\mid C)+\\Pr(N_{2}=E)\\Pr(G\\mid E)+\\Pr(N_{2}=G)\\Pr(G\\mid G).\n$$\nFrom the transition probabilities starting at $C$, we have\n$$\n\\Pr(N_{2}=C)=0.2,\\quad \\Pr(N_{2}=E)=0.3,\\quad \\Pr(N_{2}=G)=0.5.\n$$\nFrom the transition probabilities to $G$, we have\n$$\n\\Pr(G\\mid C)=0.5,\\quad \\Pr(G\\mid E)=0.5,\\quad \\Pr(G\\mid G)=0.2.\n$$\nSubstituting,\n$$\n\\Pr(N_{3}=G)=0.2\\cdot 0.5+0.3\\cdot 0.5+0.5\\cdot 0.2=0.1+0.15+0.1=0.35.\n$$\nThus, the exact probability that the third note is $G$ is $0.35$.", "answer": "$$\\boxed{0.35}$$", "id": "1356270"}, {"introduction": "After learning how to perform calculations for a system known to be a Markov chain, a crucial next step is developing the skill to identify whether a process is Markovian in the first place. This conceptual problem challenges you to analyze a hypothetical data processing protocol and determine if it meets the essential \"memoryless\" requirement of a Markov chain. Understanding when and why a process violates the Markov property is as important as knowing how to work with one, as it highlights the critical role of the state definition in accurately modeling a system. [@problem_id:1342472]", "problem": "Consider a simplified model for an adaptive data processing protocol called the Novelty Detection Protocol (NDP). The NDP processes a sequence of data items $I_1, I_2, I_3, \\dots$ that are drawn independently and with uniform probability from a finite set of possible items $\\mathcal{S}$, where the total number of distinct items is $M$ with $M > 1$.\n\nThe NDP operates in one of two states: \"ADAPT\" or \"OPERATE\". Let the stochastic process $\\{X_n\\}_{n \\ge 1}$ represent the sequence of states of the protocol, where $X_n$ is the state just before processing the $n$-th item, $I_n$. The process evolves according to the following rules:\n\n1.  **Initial State**: The protocol always starts in the ADAPT state, so $X_1 = \\text{ADAPT}$.\n2.  **Transition from ADAPT**: If the protocol is in the ADAPT state at step $n$ (i.e., $X_n = \\text{ADAPT}$), it performs an internal update and then unconditionally transitions to the OPERATE state for the next step. Thus, if $X_n = \\text{ADAPT}$, then $X_{n+1} = \\text{OPERATE}$ with certainty.\n3.  **Transition from OPERATE**: If the protocol is in the OPERATE state at step $n$ (i.e., $X_n = \\text{OPERATE}$), it examines the incoming item $I_n$.\n    *   If the item $I_n$ has been seen at any point in the past history of items (i.e., $I_n \\in \\{I_1, I_2, \\dots, I_{n-1}\\}$), the protocol remains in the OPERATE state. Thus, $X_{n+1} = \\text{OPERATE}$.\n    *   If the item $I_n$ is novel, meaning it has never appeared in the history $\\{I_1, I_2, \\dots, I_{n-1}\\}$, the protocol transitions to the ADAPT state to incorporate this new information. Thus, $X_{n+1} = \\text{ADAPT}$.\n\nBased on this definition, is the stochastic process $\\{X_n\\}_{n \\ge 1}$ a Markov chain?\n\nA. Yes, because the transition from any state only depends on the current state and the next random item, not the past states.\n\nB. Yes, because the state space is finite, and the transition probabilities are well-defined.\n\nC. No, because the transition probability from the OPERATE state depends on the number of unique items seen in the entire past history, which is not fully determined by the current state being OPERATE.\n\nD. No, because the transition probability from the ADAPT state depends on the past history.\n\nE. It is impossible to determine without knowing the specific size $M$ of the set $\\mathcal{S}$.", "solution": "We analyze whether the process $\\{X_{n}\\}_{n\\ge 1}$ with state space $\\{\\text{ADAPT}, \\text{OPERATE}\\}$ satisfies the Markov property. The Markov property requires that, for every $n$ and every history,\n$$\n\\Pr\\left(X_{n+1}=x \\mid X_{n}=x_{n}, X_{n-1}=x_{n-1}, \\dots, X_{1}=x_{1}\\right)=\\Pr\\left(X_{n+1}=x \\mid X_{n}=x_{n}\\right).\n$$\n\nFrom the protocol rules:\n- If $X_{n}=\\text{ADAPT}$, then deterministically $X_{n+1}=\\text{OPERATE}$. Thus,\n$$\n\\Pr\\left(X_{n+1}=\\text{OPERATE} \\mid X_{n}=\\text{ADAPT}, \\text{history}\\right)=1,\n$$\nwhich depends only on $X_{n}$ and not on the past.\n\n- If $X_{n}=\\text{OPERATE}$, then the next state depends on whether $I_{n}$ is novel relative to $\\{I_{1},\\dots,I_{n-1}\\}$. Let\n$$\nU_{n-1}=\\left|\\{I_{1},I_{2},\\dots,I_{n-1}\\}\\right|\n$$\ndenote the number of distinct items seen up to step $n-1$. Since $I_{n}$ is drawn uniformly and independently from $\\mathcal{S}$ with $|\\mathcal{S}|=M$, the probability that $I_{n}$ is novel given the past is\n$$\n\\Pr(\\text{$I_{n}$ novel} \\mid I_{1},\\dots,I_{n-1})=\\frac{M-U_{n-1}}{M},\n$$\nprovided $U_{n-1}\\le M$. Therefore, when $X_{n}=\\text{OPERATE}$,\n$$\n\\Pr\\left(X_{n+1}=\\text{ADAPT} \\mid X_{n}=\\text{OPERATE}, I_{1},\\dots,I_{n-1}\\right)=\\frac{M-U_{n-1}}{M},\n$$\nand\n$$\n\\Pr\\left(X_{n+1}=\\text{OPERATE} \\mid X_{n}=\\text{OPERATE}, I_{1},\\dots,I_{n-1}\\right)=\\frac{U_{n-1}}{M}.\n$$\n\nCrucially, $U_{n-1}$ is not determined by the single state value $X_{n}=\\text{OPERATE}$. Two different past histories can both yield $X_{n}=\\text{OPERATE}$ but have different $U_{n-1}$, leading to different transition probabilities. For example, if the past consists of repeated occurrences of one item, then $U_{n-1}=1$ and\n$$\n\\Pr\\left(X_{n+1}=\\text{ADAPT} \\mid X_{n}=\\text{OPERATE}, \\text{history}\\right)=\\frac{M-1}{M}.\n$$\nIf instead the past contains all distinct items, then $U_{n-1}=\\min\\{n-1,M\\}$ and\n$$\n\\Pr\\left(X_{n+1}=\\text{ADAPT} \\mid X_{n}=\\text{OPERATE}, \\text{history}\\right)=\\frac{M-\\min\\{n-1,M\\}}{M},\n$$\nwhich differs from $\\frac{M-1}{M}$ whenever $\\min\\{n-1,M\\}\\ne 1$.\n\nHence,\n$$\n\\Pr\\left(X_{n+1}=\\cdot \\mid X_{n}=\\text{OPERATE}, X_{n-1},\\dots,X_{1}\\right)\n$$\nis not determined solely by $X_{n}$ but also by the past through $U_{n-1}$. Therefore, $\\{X_{n}\\}$ is not a Markov chain. Among the given options, this corresponds to the statement that the transition probability from OPERATE depends on the number of unique items in the entire past, which is not captured by the current state.\n\nThe transition from ADAPT does not depend on history, so the reason in option D is incorrect. The finiteness of the state space in option B does not guarantee the Markov property, and option A incorrectly treats dependence on the next item as sufficient without encoding past novelty in the state. The size $M$ in option E does not affect the qualitative conclusion.", "answer": "$$\\boxed{C}$$", "id": "1342472"}, {"introduction": "Building a model often involves transforming or simplifying other well-understood processes. This exercise delves deeper into the structural properties of Markov chains by asking under what conditions a new process, derived from an existing Markov chain, will also be Markovian. It explores concepts such as state augmentation, bijective transformations, and time reversal, which are powerful techniques in the stochastic modeler's toolkit. This practice will strengthen your theoretical understanding and your ability to reason about the preservation of the Markov property. [@problem_id:1342456]", "problem": "Let $\\{X_n\\}_{n \\ge 0}$ be a time-homogeneous Markov chain with a discrete state space $S_X$ and transition probabilities $P_{ij} = P(X_{n+1}=j | X_n=i)$. Based on this chain, four new stochastic processes are constructed. Your task is to determine which of these new processes are also necessarily Markov processes under the conditions given for each case.\n\nI. A process $\\{Y_n\\}$ is defined as $Y_n = X_n^2$. For this specific case, the base process $\\{X_n\\}$ is defined on the state space $S_X = \\{-1, 1, 2\\}$. Its transition probabilities are such that moving from state 1 to state 2 is possible ($P_{1,2} > 0$), but moving from state -1 to state 2 is impossible ($P_{-1,2} = 0$).\n\nII. A process $\\{Z_n\\}$ is defined for $n \\ge 1$ by taking pairs of consecutive states from the original chain, $Z_n = (X_n, X_{n-1})$. The state space for $\\{Z_n\\}$ is $S_X \\times S_X$.\n\nIII. A process $\\{U_n\\}$ is defined via a bijective (one-to-one and onto) function $f: S_X \\to S_U$, where $U_n = f(X_n)$.\n\nIV. A process $\\{V_n\\}$ is defined by reversing time. It is assumed that the original chain $\\{X_n\\}$ is stationary, possessing a stationary distribution $\\pi = (\\pi_i)_{i \\in S_X}$ with all $\\pi_i > 0$. The time-reversed process is defined as $V_n = X_{-n}$ for $n \\in \\mathbb{Z}$.\n\nWhich of the processes described above are guaranteed to be Markov processes under the specified conditions?\n\nA. I and II only\n\nB. II and III only\n\nC. I, II, and III only\n\nD. II, III, and IV only\n\nE. I, III, and IV only", "solution": "We analyze each construction and test the Markov property, using explicit conditional probabilities and the lumpability criterion when appropriate.\n\nI. Define $Y_{n}=X_{n}^{2}$ with $S_{X}=\\{-1,1,2\\}$ and transitions satisfying $P_{1,2}>0$ and $P_{-1,2}=0$. The image state space is $S_{Y}=\\{1,4\\}$ with the preimage classes $f^{-1}(1)=\\{-1,1\\}$ and $f^{-1}(4)=\\{2\\}$. A function of a Markov chain is itself Markov if and only if the partition induced by the function is strongly lumpable, i.e., for any $y\\in S_{Y}$, any $x,x'\\in f^{-1}(y)$, and any $y'\\in S_{Y}$,\n$$\n\\sum_{j\\in f^{-1}(y')}P_{xj}=\\sum_{j\\in f^{-1}(y')}P_{x'j}.\n$$\nHere, take $y=1$, so $x=1$, $x'=-1$, and take $y'=4$, so $f^{-1}(4)=\\{2\\}$. Then\n$$\n\\sum_{j\\in f^{-1}(4)}P_{1j}=P_{1,2}>0,\\qquad \\sum_{j\\in f^{-1}(4)}P_{-1,j}=P_{-1,2}=0,\n$$\nwhich are not equal. Therefore the lumpability condition fails, and $\\{Y_{n}\\}$ is not necessarily Markov under the given assumptions.\n\nII. Define $Z_{n}=(X_{n},X_{n-1})$ on $S_{X}\\times S_{X}$. For any $(i,j)\\in S_{X}\\times S_{X}$,\n$$\n\\mathbb{P}\\big(Z_{n+1}=(k,i)\\mid Z_{n}=(i,j), Z_{n-1}, Z_{n-2},\\dots\\big)=\\mathbb{P}(X_{n+1}=k\\mid X_{n}=i)=P_{ik},\n$$\nwhich depends only on the current state $(i,j)$ through $i$, and not on earlier history. Hence $\\{Z_{n}\\}$ is a first-order Markov chain on $S_{X}\\times S_{X}$.\n\nIII. Let $f:S_{X}\\to S_{U}$ be bijective and define $U_{n}=f(X_{n})$. Since $f$ is invertible, for $u,u'\\in S_{U}$,\n$$\n\\mathbb{P}(U_{n+1}=u'\\mid U_{n}=u)=\\mathbb{P}\\big(X_{n+1}=f^{-1}(u')\\mid X_{n}=f^{-1}(u)\\big)=P_{f^{-1}(u),\\,f^{-1}(u')},\n$$\nwhich depends only on the current state $u$. Therefore $\\{U_{n}\\}$ is Markov.\n\nIV. Assume $\\{X_{n}\\}_{n\\in\\mathbb{Z}}$ is stationary with stationary distribution $\\pi=(\\pi_{i})_{i\\in S_{X}}$ satisfying $\\pi_{i}>0$ for all $i$, and define $V_{n}=X_{-n}$. For $i,j\\in S_{X}$, the transition probabilities of the time-reversed chain are\n$$\nP^{*}_{ij}=\\mathbb{P}(V_{n+1}=j\\mid V_{n}=i)=\\frac{\\pi_{j}P_{ji}}{\\pi_{i}},\n$$\nwhich are well-defined because $\\pi_{i}>0$. This shows that $\\{V_{n}\\}$ is Markov (the standard time-reversal of a stationary Markov chain).\n\nCombining the conclusions: I is not necessarily Markov, while II, III, and IV are Markov under the stated conditions. The correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1342456"}]}