{"hands_on_practices": [{"introduction": "Let's begin with a foundational exercise that captures the essence of a compound Poisson process. We will model the one-dimensional movement of a subatomic particle subject to random, instantaneous collisions. This practice [@problem_id:1317627] will guide you through calculating the mean and variance of the particle's total displacement, which are the two most important characteristics of the process. By mastering this, you will learn how to connect the properties of individual random \"jumps\" to the aggregate behavior of the system over time.", "problem": "A subatomic particle moves in one dimension. Its motion is characterized by a series of instantaneous displacements caused by collisions. The number of collisions in a time interval of length $t$, denoted by $N(t)$, follows a Poisson process with rate $\\lambda$. Each collision causes a displacement $Y_i$ for $i=1, 2, \\dots$. The displacements $\\{Y_i\\}$ are independent and identically distributed random variables, and they are also independent of the process $N(t)$.\n\nThe distribution of each displacement $Y_i$ is as follows:\n- A displacement of $+\\delta$ with probability $p$.\n- A displacement of $-2\\delta$ with probability $q$.\n- A displacement of $0$ with probability $1-p-q$.\n\nHere, $\\delta$ is a positive constant representing a fundamental length scale, and $p$ and $q$ are probabilities. Let $X(t) = \\sum_{i=1}^{N(t)} Y_i$ be the net displacement of the particle after time $t$. If no collisions occur, the displacement is zero.\n\nDetermine the mean, $E[X(t)]$, and the variance, $\\text{Var}(X(t))$, of the particle's net displacement after time $t$. The final answer should be a pair of expressions, one for the mean and one for the variance, in terms of $\\lambda, t, \\delta, p,$ and $q$.", "solution": "Let $N(t)$ be Poisson with parameter $\\lambda t$, independent of the i.i.d. displacements $\\{Y_{i}\\}$. Define $X(t)=\\sum_{i=1}^{N(t)} Y_{i}$, with the convention $X(t)=0$ if $N(t)=0$.\n\nFirst compute the first and second moments of a single displacement $Y$. By the given distribution,\n$$\nE[Y]=\\delta \\cdot p+(-2\\delta)\\cdot q+0\\cdot(1-p-q)=\\delta(p-2q),\n$$\nand\n$$\nE[Y^{2}]=\\delta^{2}\\cdot p+(4\\delta^{2})\\cdot q+0\\cdot(1-p-q)=\\delta^{2}(p+4q).\n$$\n\nConditioning on $N(t)=n$, we have a sum of $n$ i.i.d. terms, so\n$$\nE[X(t)\\mid N(t)=n]=n\\,E[Y]=n\\,\\delta(p-2q),\n$$\nand\n$$\n\\text{Var}(X(t)\\mid N(t)=n)=n\\,\\text{Var}(Y)=n\\,(E[Y^{2}]-(E[Y])^{2}).\n$$\n\nUsing the law of total expectation and that $E[N(t)]=\\lambda t$ for $N(t)\\sim\\text{Poisson}(\\lambda t)$, we obtain\n$$\nE[X(t)]=E\\big[E[X(t)\\mid N(t)]\\big]=E[N(t)]\\,E[Y]=\\lambda t\\,\\delta(p-2q).\n$$\n\nUsing the law of total variance,\n$$\n\\text{Var}(X(t))=E\\big[\\text{Var}(X(t)\\mid N(t))\\big]+\\text{Var}\\big(E[X(t)\\mid N(t)]\\big).\n$$\nSubstituting the conditional moments and noting $\\text{Var}(N(t))=\\lambda t$ gives\n$$\n\\text{Var}(X(t))=E[N(t)]\\,\\text{Var}(Y)+\\text{Var}(N(t))\\,(E[Y])^{2}\n=\\lambda t\\big(\\text{Var}(Y)+(E[Y])^{2}\\big)=\\lambda t\\,E[Y^{2}].\n$$\nWith $E[Y^{2}]=\\delta^{2}(p+4q)$ from above, we conclude\n$$\n\\text{Var}(X(t))=\\lambda t\\,\\delta^{2}(p+4q).\n$$\n\nTherefore, the mean and variance are\n$$\nE[X(t)]=\\lambda t\\,\\delta(p-2q),\\qquad \\text{Var}(X(t))=\\lambda t\\,\\delta^{2}(p+4q).\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\lambda t \\delta (p-2 q)  \\lambda t \\delta^{2} (p+4 q)\\end{pmatrix}}$$", "id": "1317627"}, {"introduction": "Now, let's reverse our perspective with an \"inverse problem\". In many scientific applications, we can observe the macroscopic properties of a system—like its average growth and variability—but the underlying microscopic events are not directly visible. This exercise [@problem_id:715429] challenges you to act like a detective: using the known mean and variance of the aggregate process, you will deduce a key property of the individual jump sizes. This type of reverse-engineering is a powerful problem-solving skill that reinforces your understanding of the fundamental relationships in a compound Poisson process.", "problem": "A stochastic process $X(t)$, representing the cumulative effect of a series of events over time, is modeled as a compound Poisson process:\n$$\nX(t) = \\sum_{i=1}^{N(t)} Y_i\n$$\nIn this model, $N(t)$ is a homogeneous Poisson process with a constant rate $\\lambda  0$, which counts the number of events occurring up to time $t$. The terms $\\{Y_i\\}_{i=1,2,...}$ are independent and identically distributed (i.i.d.) random variables representing the magnitude, or 'jump size', of each event. The jump sizes are also independent of the counting process $N(t)$. It is assumed that the expected jump size is non-negative, i.e., $E[Y] \\ge 0$.\n\nThe mean and variance of the aggregate process $X(t)$ have been characterized and are found to be linear functions of time:\n1.  $E[X(t)] = At$\n2.  $\\text{Var}(X(t)) = Bt$\n\nHere, $A$ and $B$ are known positive constants.\n\nFurthermore, a specific feature of the system being modeled imposes a relationship between the rate of events and their expected magnitude. This relationship is given by:\n3.  $\\lambda = k E[Y]$\n\nwhere $k$ is a known positive constant of proportionality.\n\nUsing this information, derive a closed-form expression for the variance of the jump size distribution, $\\text{Var}(Y)$, in terms of the constants $A$, $B$, and $k$.", "solution": "1. For a compound Poisson process with rate $\\lambda$ and jump distribution $Y$ (with mean $\\mu=E[Y]$ and variance $\\sigma^2=\\text{Var}(Y)$),\n   $$E[N(t)]=\\lambda t,\\quad \\text{Var}(N(t))=\\lambda t.$$\n2. The mean of $X(t)=\\sum_{i=1}^{N(t)}Y_i$ is\n   $$E[X(t)]=E[N(t)]\\,\\mu=\\lambda t\\,\\mu=At\\quad\\Longrightarrow\\quad A=\\lambda\\mu.$$\n3. The variance of $X(t)$ is\n   $$\\text{Var}(X(t))=E[N(t)]\\,\\sigma^2+\\text{Var}(N(t))\\,\\mu^2=\\lambda t\\,\\sigma^2+\\lambda t\\,\\mu^2\n      =\\lambda(\\sigma^2+\\mu^2)\\,t=Bt\\quad\\Longrightarrow\\quad B=\\lambda(\\sigma^2+\\mu^2).$$\n4. Given the constraint \n   $$\\lambda=k\\,E[Y]=k\\,\\mu,$$\n   from $A=\\lambda\\mu$ we get\n   $$A=k\\,\\mu^2\\quad\\Longrightarrow\\quad \\mu^2=\\frac{A}{k}.$$\n5. From $B=\\lambda(\\sigma^2+\\mu^2)=k\\,\\mu\\;(\\sigma^2+\\mu^2)$ we have\n   $$\\frac{B}{k\\,\\mu}=\\sigma^2+\\mu^2\n      \\quad\\Longrightarrow\\quad\n      \\sigma^2=\\frac{B}{k\\,\\mu}-\\mu^2.$$\n6. Substitute $\\mu=\\sqrt{A/k}$:\n   $$\\text{Var}(Y)=\\sigma^2\n      =\\frac{B}{k\\sqrt{A/k}}-\\frac{A}{k}\n      =\\frac{B}{\\sqrt{kA}}-\\frac{A}{k}.$$", "answer": "$$\\boxed{\\frac{B}{\\sqrt{kA}}-\\frac{A}{k}}$$", "id": "715429"}, {"introduction": "Our final practice explores a more complex and realistic scenario where a system is influenced by multiple, independent random processes. Here, we will analyze a process that is the sum of two distinct compound Poisson processes, each with its own rate and jump size distribution. This problem [@problem_id:715555] introduces the important concept of superposition and requires using the law of total variance in a conditional setting. Successfully navigating this challenge demonstrates a deeper proficiency in stochastic modeling, preparing you for analyzing multifaceted real-world systems.", "problem": "Consider a stochastic process $S(t)$ defined as the sum of two independent compound Poisson processes:\n$$ S(t) = \\sum_{i=1}^{N_1(t)} X_i + \\sum_{j=1}^{N_2(t)} Y_j $$\nHere, $N_1(t)$ and $N_2(t)$ are independent Poisson processes with constant arrival rates $\\lambda_1  0$ and $\\lambda_2  0$, respectively.\n\nThe jump sizes, $\\{X_i\\}_{i\\geq 1}$, are independent and identically distributed (i.i.d.) random variables with mean $E[X_i] = \\mu_X$ and variance $\\text{Var}(X_i) = \\sigma_X^2$.\nSimilarly, the jump sizes, $\\{Y_j\\}_{j\\geq 1}$, are i.i.d. random variables with mean $E[Y_j] = \\mu_Y$ and variance $\\text{Var}(Y_j) = \\sigma_Y^2$.\n\nAll sources of randomness—the processes $N_1(t)$ and $N_2(t)$, and the sequences of jump sizes $\\{X_i\\}$ and $\\{Y_j\\}$—are mutually independent.\n\nThe total number of jumps occurring up to time $t$ is $N(t) = N_1(t) + N_2(t)$.\n\nFor a given positive integer $n$ and time $t0$, derive a closed-form expression for the conditional variance of the aggregate sum, $\\text{Var}(S(t) | N(t) = n)$, in terms of $n$, $\\lambda_1$, $\\lambda_2$, $\\mu_X$, $\\sigma_X^2$, $\\mu_Y$, and $\\sigma_Y^2$.", "solution": "1. Conditional distribution of $N_1(t)$ given $N(t)=n$:  \nSince $N_1(t)\\sim\\text{Poisson}(\\lambda_1 t)$, $N_2(t)\\sim\\text{Poisson}(\\lambda_2 t)$ and are independent,  \n$$N_1(t)\\mid N_1(t)+N_2(t)=n\\sim\\text{Bin}\\Bigl(n,\\;p\\Bigr),\\quad p=\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}.$$\n\n2. Write $S(t)=S_X+S_Y$ with  \n$$S_X=\\sum_{i=1}^{N_1}X_i,\\quad S_Y=\\sum_{j=1}^{N_2}Y_j.$$  \nCondition on $N_1=k$ and $N=n$:  \n$$\\text{Var}\\bigl(S\\mid N_1=k,N=n\\bigr)=k\\,\\sigma_X^2+(n-k)\\,\\sigma_Y^2,$$  \n$$E\\bigl[S\\mid N_1=k,N=n\\bigr]=k\\,\\mu_X+(n-k)\\,\\mu_Y.$$\n\n3. Use the law of total variance:  \n$$\\text{Var}(S\\mid N=n)=E\\bigl[\\text{Var}(S\\mid N_1,n)\\bigr]+\\text{Var}\\bigl[E(S\\mid N_1,n)\\bigr].$$  \nCompute each term:  \nE[Var] term:  \n$$E\\bigl[k\\,\\sigma_X^2+(n-k)\\sigma_Y^2\\bigr]\n=\\sigma_X^2\\,E[k]+\\sigma_Y^2\\,(n-E[k])\n=n\\Bigl(p\\,\\sigma_X^2+(1-p)\\,\\sigma_Y^2\\Bigr).$$  \nVar[E] term:  \n$$\\text{Var}\\bigl[k\\,\\mu_X+(n-k)\\mu_Y\\bigr]\n=(\\mu_X-\\mu_Y)^2\\,\\text{Var}(k)\n=(\\mu_X-\\mu_Y)^2\\,n\\,p(1-p).$$\n\n4. Combine and substitute $p=\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}$:  \n$$\\text{Var}(S\\mid N=n)\n=n\\Bigl(p\\,\\sigma_X^2+(1-p)\\sigma_Y^2\\Bigr)\n+n\\,p(1-p)\\,(\\mu_X-\\mu_Y)^2.$$\n$$\n\\text{Var}(S\\mid N=n)\n=n\\Bigl(\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}\\sigma_X^2\n+\\frac{\\lambda_2}{\\lambda_1+\\lambda_2}\\sigma_Y^2\\Bigr)\n+n\\frac{\\lambda_1\\lambda_2}{(\\lambda_1+\\lambda_2)^2}(\\mu_X-\\mu_Y)^2.\n$$", "answer": "$$\\boxed{n\\Bigl(\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}\\sigma_X^2+\\frac{\\lambda_2}{\\lambda_1+\\lambda_2}\\sigma_Y^2\\Bigr)+n\\frac{\\lambda_1\\lambda_2}{(\\lambda_1+\\lambda_2)^2}(\\mu_X-\\mu_Y)^2}$$", "id": "715555"}]}