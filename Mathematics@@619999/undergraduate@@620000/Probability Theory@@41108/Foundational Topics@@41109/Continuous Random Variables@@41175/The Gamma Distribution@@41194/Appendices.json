{"hands_on_practices": [{"introduction": "The Gamma distribution is defined by its shape parameter, $\\alpha$, and rate parameter, $\\beta$. This first practice provides a foundational skill: determining these defining parameters from a distribution's mean and variance. This exercise is crucial because in many real-world applications, we can estimate the average (mean) and spread (variance) from data, and we need to work backward to find the underlying distribution's parameters [@problem_id:1398490].", "problem": "The lifetime of a particular model of electronic component is modeled as a random variable, $X$, where $X$ is measured in thousands of hours. The probability distribution of $X$ is known to be a Gamma distribution. A random variable $Y$ that follows a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\beta > 0$, denoted as $Y \\sim \\Gamma(\\alpha, \\beta)$, has an expected value (mean) given by $E[Y] = \\frac{\\alpha}{\\beta}$ and a variance given by $\\text{Var}(Y) = \\frac{\\alpha}{\\beta^2}$.\n\nFrom extensive testing on a large sample of these components, the mean lifetime is determined to be 10,000 hours, and the variance of their lifetimes is 20,000,000 $hours^2$.\n\nFind the shape parameter, $\\alpha$, and the rate parameter, $\\beta$, for the probability distribution of the component's lifetime $X$.", "solution": "Let $T$ denote the lifetime in hours and $X$ denote the lifetime in thousands of hours, so $X=\\frac{T}{10^{3}}$. Using linearity of expectation and the scaling rule for variance, we have\n$$E[X]=\\frac{E[T]}{10^{3}}, \\quad \\text{Var}(X)=\\frac{\\text{Var}(T)}{10^{6}}.$$\nGiven $E[T]=10{,}000$ hours and $\\text{Var}(T)=20{,}000{,}000$ hours$^{2}$, it follows that\n$$E[X]=\\frac{10{,}000}{10^{3}}=10, \\quad \\text{Var}(X)=\\frac{20{,}000{,}000}{10^{6}}=20.$$\n\nAssuming $X \\sim \\Gamma(\\alpha,\\beta)$ with rate parameter $\\beta$, the mean and variance are\n$$E[X]=\\frac{\\alpha}{\\beta}, \\quad \\text{Var}(X)=\\frac{\\alpha}{\\beta^{2}}.$$\nSet these equal to the empirical values:\n$$\\frac{\\alpha}{\\beta}=10, \\quad \\frac{\\alpha}{\\beta^{2}}=20.$$\nFrom $\\frac{\\alpha}{\\beta}=10$ we get $\\alpha=10\\beta$. Substitute into the variance equation:\n$$\\frac{10\\beta}{\\beta^{2}}=20 \\quad \\Longrightarrow \\quad \\frac{10}{\\beta}=20 \\quad \\Longrightarrow \\quad \\beta=\\frac{1}{2}.$$\nThen\n$$\\alpha=10\\left(\\frac{1}{2}\\right)=5.$$\n\nTherefore, the shape and rate parameters are $\\alpha=5$ and $\\beta=\\frac{1}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}5  \\frac{1}{2}\\end{pmatrix}}$$", "id": "1398490"}, {"introduction": "Beyond its flexible shape, the Gamma distribution's primary importance comes from its deep connection to the Poisson process. This hands-on exercise explores one of its most vital roles: modeling the waiting time until a certain number of random events occur. Working through this problem builds intuition for how these two fundamental stochastic concepts are linked, turning abstract theory into a powerful tool for analyzing phenomena like particle arrivals or equipment failures [@problem_id:1303888].", "problem": "A research satellite is designed to study rare cosmic phenomena. The arrivals of a specific type of high-energy particle at its detector are well-modeled by a Poisson process. The waiting time, $T$, until the $k$-th particle is detected follows a Gamma distribution. The convention used for the Gamma distribution is the (shape, rate) parameterization. The shape parameter is determined by the number of arrivals, $k$, and the rate parameter is the average arrival rate of the underlying Poisson process, $\\lambda$.\n\nFrom long-term observation data, the expected value of this waiting time $T$ is measured to be $\\tau$.\n\nAn observation window of duration $t_{obs}$ is scheduled. Find an expression for the probability that the detector registers at least one particle during this window. Your answer must be a self-contained analytic expression in terms of the parameters $k$, $\\tau$, and $t_{obs}$.", "solution": "Let the random variable $T$ be the waiting time until the $k$-th particle detection. The problem states that $T$ follows a Gamma distribution with shape parameter $k$ and rate parameter $\\lambda$. The probability density function of such a distribution is given by $f(t) = \\frac{\\lambda^k t^{k-1} e^{-\\lambda t}}{\\Gamma(k)}$ for $t \\ge 0$.\n\nThe expected value (mean) of a random variable following a Gamma($k, \\lambda$) distribution is given by:\n$$E[T] = \\frac{k}{\\lambda}$$\n\nThe problem states that this expected waiting time is $\\tau$. Therefore, we can establish a relationship between the given parameters and the underlying process rate $\\lambda$:\n$$\\tau = \\frac{k}{\\lambda}$$\n\nWe can rearrange this equation to solve for the rate parameter $\\lambda$ in terms of the known quantities $k$ and $\\tau$:\n$$\\lambda = \\frac{k}{\\tau}$$\nThis parameter $\\lambda$ represents the average number of particle detections per unit time.\n\nNow, we need to find the probability of detecting at least one particle in an observation window of duration $t_{obs}$. The number of events occurring in a fixed interval of time in a Poisson process follows a Poisson distribution. Let $N(t)$ be the random variable representing the number of particles detected in a time interval of length $t$. Then $N(t)$ follows a Poisson distribution with parameter $\\mu = \\lambda t$. The probability mass function is:\n$$P(N(t) = n) = \\frac{(\\lambda t)^n e^{-\\lambda t}}{n!}$$\n\nWe are interested in the probability of detecting at least one particle in the time interval $t_{obs}$, which can be written as $P(N(t_{obs}) \\ge 1)$. It is easier to calculate this probability by considering the complementary event: the probability of detecting zero particles, $P(N(t_{obs}) = 0)$.\n$$P(N(t_{obs}) \\ge 1) = 1 - P(N(t_{obs}) = 0)$$\n\nUsing the Poisson probability mass function with $n=0$ and $t=t_{obs}$:\n$$P(N(t_{obs}) = 0) = \\frac{(\\lambda t_{obs})^0 e^{-\\lambda t_{obs}}}{0!} = \\frac{1 \\cdot e^{-\\lambda t_{obs}}}{1} = e^{-\\lambda t_{obs}}$$\n\nSo, the probability of detecting at least one particle is:\n$$P(N(t_{obs}) \\ge 1) = 1 - e^{-\\lambda t_{obs}}$$\n\nFinally, we substitute the expression for $\\lambda$ that we found earlier, $\\lambda = k/\\tau$, into this equation to get the final answer in terms of the given parameters $k$, $\\tau$, and $t_{obs}$:\n$$P(N(t_{obs}) \\ge 1) = 1 - \\exp\\left(-\\left(\\frac{k}{\\tau}\\right) t_{obs}\\right) = 1 - \\exp\\left(-\\frac{k t_{obs}}{\\tau}\\right)$$\nThis is the desired analytic expression for the probability.", "answer": "$$\\boxed{1 - \\exp\\left(-\\frac{k t_{obs}}{\\tau}\\right)}$$", "id": "1303888"}, {"introduction": "While we can memorize formulas for the mean and variance, a more powerful and elegant method exists for understanding all the moments of a distribution. This practice introduces the moment-generating function (MGF), a mathematical \"factory\" for producing the moments of any order [@problem_id:1919334]. By deriving a general formula for the $k$-th moment, you will not only practice an important mathematical technique but also gain a deeper appreciation for the rich structure of the Gamma distribution.", "problem": "A continuous random variable $X$ is said to follow a Gamma distribution with a shape parameter $\\alpha  0$ and a rate parameter $\\beta  0$. The probability density function (PDF) for this distribution is given by\n$$f(x; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x), \\quad \\text{for } x  0$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function. The moment-generating function (MGF) of $X$, which is defined as $M_X(t) = E[\\exp(tX)]$, is given by\n$$M_X(t) = \\left(\\frac{\\beta}{\\beta - t}\\right)^{\\alpha}, \\quad \\text{for } t  \\beta$$\n\nWithout using direct integration on the PDF, find a general expression for the $k$-th raw moment of $X$, denoted by $E[X^k]$, for any positive integer $k$. Your final expression should be in terms of $\\alpha$, $\\beta$, $k$, and the Gamma function $\\Gamma(\\cdot)$.", "solution": "The $k$-th raw moment of a random variable $X$, denoted $E[X^k]$, can be found from its moment-generating function (MGF), $M_X(t)$, by computing the $k$-th derivative of the MGF with respect to $t$ and then evaluating it at $t=0$. The relationship is:\n$$E[X^k] = \\frac{d^k}{dt^k} M_X(t) \\bigg|_{t=0} = M_X^{(k)}(0)$$\n\nWe are given the MGF of the Gamma distribution:\n$$M_X(t) = \\left(\\frac{\\beta}{\\beta - t}\\right)^{\\alpha}$$\nFor easier differentiation, we can rewrite this function using a negative exponent:\n$$M_X(t) = \\beta^{\\alpha} (\\beta - t)^{-\\alpha}$$\n\nNow, we compute the first few derivatives with respect to $t$ to identify a pattern.\n\nFor $k=1$, the first derivative is:\n$$M_X^{(1)}(t) = \\frac{d}{dt} \\left[ \\beta^{\\alpha} (\\beta - t)^{-\\alpha} \\right] = \\beta^{\\alpha} (-\\alpha) (\\beta - t)^{-\\alpha-1} (-1) = \\alpha \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+1)}$$\n\nFor $k=2$, the second derivative is:\n$$M_X^{(2)}(t) = \\frac{d}{dt} \\left[ \\alpha \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+1)} \\right] = \\alpha \\beta^{\\alpha} (-(\\alpha+1)) (\\beta - t)^{-(\\alpha+2)} (-1) = \\alpha(\\alpha+1) \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+2)}$$\n\nFor $k=3$, the third derivative is:\n$$M_X^{(3)}(t) = \\frac{d}{dt} \\left[ \\alpha(\\alpha+1) \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+2)} \\right] = \\alpha(\\alpha+1) \\beta^{\\alpha} (-(\\alpha+2)) (\\beta - t)^{-(\\alpha+3)} (-1) = \\alpha(\\alpha+1)(\\alpha+2) \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+3)}$$\n\nBy observing the pattern, we can generalize for an arbitrary positive integer $k$. The $k$-th derivative is:\n$$M_X^{(k)}(t) = \\alpha(\\alpha+1)(\\alpha+2) \\cdots (\\alpha+k-1) \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+k)}$$\n\nThe product term $\\alpha(\\alpha+1)(\\alpha+2) \\cdots (\\alpha+k-1)$ is known as the rising factorial, which can be expressed using the Gamma function:\n$$\\alpha(\\alpha+1)\\cdots(\\alpha+k-1) = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)}$$\n\nSubstituting this into our expression for the $k$-th derivative gives:\n$$M_X^{(k)}(t) = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\beta^{\\alpha} (\\beta - t)^{-(\\alpha+k)}$$\n\nTo find the $k$-th raw moment, $E[X^k]$, we evaluate this expression at $t=0$:\n$$E[X^k] = M_X^{(k)}(0) = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\beta^{\\alpha} (\\beta - 0)^{-(\\alpha+k)}$$\n$$E[X^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\beta^{\\alpha} \\beta^{-(\\alpha+k)}$$\n\nUsing the property of exponents, $a^m a^n = a^{m+n}$, we simplify the terms involving $\\beta$:\n$$E[X^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\beta^{\\alpha - (\\alpha+k)} = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\beta^{-k}$$\n\nThis gives the final expression for the $k$-th raw moment:\n$$E[X^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha) \\beta^k}$$", "answer": "$$\\boxed{\\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha) \\beta^{k}}}$$", "id": "1919334"}]}