{"hands_on_practices": [{"introduction": "Understanding the key properties of a distribution starts with its moments. This first practice focuses on deriving the expected value, or mean, of a log-normal random variable. This exercise is crucial because it directly links the parameters $\\mu$ and $\\sigma$ of the underlying normal distribution to the average value of the log-normal variable, demonstrating how both parameters influence this key measure of central tendency. Completing this derivation will strengthen your skills in applying the definition of expectation and the change of variables method for transformed random variables [@problem_id:10657].", "problem": "A random variable $X$ is said to follow a log-normal distribution if its natural logarithm, $Y = \\ln(X)$, is normally distributed. The probability density function (PDF) for the normally distributed variable $Y$ is given by:\n$$f_Y(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$$\nwhere $\\mu$ is the mean and $\\sigma^2$ is the variance of $Y$.\n\nUsing the relationship between $X$ and $Y$ and the provided PDF for $Y$, derive the expected value of the random variable $X$, denoted as $E[X]$. Express your answer in terms of the parameters $\\mu$ and $\\sigma$.", "solution": "We wish to compute  \n$$E[X]\\;=\\;\\int_{0}^{\\infty} x\\,f_{X}(x)\\,dx$$  \nwhere $X=\\exp(Y)$ and $Y\\sim N(\\mu,\\sigma^2)$.  Using the change of variable $y=\\ln(x)$ gives $x=e^{y}$ and $dx=e^{y}dy$, and the PDF of $X$ is  \n$$f_{X}(x)=f_{Y}(\\ln x)\\,\\frac{1}{x} \n=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\,\\exp\\!\\Bigl(-\\frac{(\\ln x-\\mu)^{2}}{2\\sigma^{2}}\\Bigr)\\,\\frac{1}{x}\\,. $$  \nHence  \n$$\nE[X]\n=\\int_{0}^{\\infty}x\\;\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\n\\exp\\!\\Bigl(-\\frac{(\\ln x-\\mu)^{2}}{2\\sigma^{2}}\\Bigr)\\,\\frac{1}{x}\\;dx\n=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\n\\int_{0}^{\\infty}\n\\exp\\!\\Bigl(-\\frac{(\\ln x-\\mu)^{2}}{2\\sigma^{2}}\\Bigr)\\,dx.\n$$  \nWe now set \n$$t=\\frac{\\ln x-\\mu}{\\sigma},\\quad \\ln x=\\sigma t+\\mu,\\quad dx=\\sigma e^{\\sigma t+\\mu}\\,dt,$$  \nso that $x=e^{\\sigma t+\\mu}$ and the limits $x=0\\to t=-\\infty$, $x\\to\\infty\\to t=+\\infty$.  Substitution gives  \n$$\nE[X]\n=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\n\\int_{-\\infty}^{\\infty}\n\\exp\\!\\Bigl(-\\frac{(\\sigma t)^{2}}{2\\sigma^{2}}\\Bigr)\\,\\sigma e^{\\sigma t+\\mu}\\,dt\n=\\frac{\\sigma e^{\\mu}}{\\sqrt{2\\pi\\sigma^{2}}}\n\\int_{-\\infty}^{\\infty}\n\\exp\\!\\Bigl(-\\tfrac{t^{2}}{2}+\\sigma t\\Bigr)\\,dt.\n$$  \nComplete the square in the exponent:  \n$$\n-\\tfrac{t^{2}}{2}+\\sigma t\n=-\\tfrac{1}{2}\\bigl(t^{2}-2\\sigma t\\bigr)\n=-\\tfrac{1}{2}\\bigl((t-\\sigma)^{2}-\\sigma^{2}\\bigr)\n=-\\tfrac{(t-\\sigma)^{2}}{2}+\\tfrac{\\sigma^{2}}{2}\\,.\n$$  \nThus  \n$$\nE[X]\n=\\frac{e^{\\mu}}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^{\\infty}\n\\exp\\!\\Bigl(-\\tfrac{(t-\\sigma)^{2}}{2}+\\tfrac{\\sigma^{2}}{2}\\Bigr)\\,dt\n=e^{\\mu+\\frac{\\sigma^{2}}{2}}\n\\frac{1}{\\sqrt{2\\pi}}\n\\int_{-\\infty}^{\\infty}\n\\exp\\!\\Bigl(-\\tfrac{u^{2}}{2}\\Bigr)\\,du\n$$  \nafter setting $u=t-\\sigma$.  The remaining Gaussian integral is $\\sqrt{2\\pi}$, yielding the final result.", "answer": "$$\\boxed{e^{\\mu + \\frac{\\sigma^{2}}{2}}}$$", "id": "10657"}, {"introduction": "The primary utility of the log-normal distribution lies in its convenient relationship with the normal distribution. This exercise demonstrates the fundamental technique for solving probability problems: transforming a question about the log-normal variable $X$ into an equivalent, and much simpler, question about the normal variable $Y = \\ln(X)$. By working through this problem, you will practice standardizing a normal variable and using the standard normal cumulative distribution function (CDF) $\\Phi$, a core skill for any applied probability work [@problem_id:10634].", "problem": "A random variable $X$ is said to have a log-normal distribution if its natural logarithm, $Y = \\ln(X)$, is normally distributed. The probability density function (PDF) of the normal variable $Y$ is given by:\n$$f_Y(y) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$$\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation of $Y$.\n\nThe cumulative distribution function (CDF) of the *standard normal* distribution (a normal distribution with mean $0$ and standard deviation $1$), denoted by $\\Phi(z)$, is defined as the probability that a standard normal random variable is less than or equal to $z$. That is:\n$$\\Phi(z) = P(Z \\le z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt$$\n\nGiven this information, derive an expression for the probability that the log-normal variable $X$ exceeds the value $e^{\\mu+\\sigma}$, which is $P(X > e^{\\mu+\\sigma})$. Your final answer should be expressed in terms of the standard normal CDF, $\\Phi$.", "solution": "We have $X$ logâ€normally distributed with $Y=\\ln X\\sim N(\\mu,\\sigma^2)$.  Then\n$$P\\bigl(X>e^{\\mu+\\sigma}\\bigr)\n  =P\\bigl(\\ln X>\\mu+\\sigma\\bigr)\n  =P\\bigl(Y>\\mu+\\sigma\\bigr).$$\nStandardize $Y$ by $Z=(Y-\\mu)/\\sigma\\sim N(0,1)$:\n$$P\\bigl(Y>\\mu+\\sigma\\bigr)\n  =P\\Bigl(\\frac{Y-\\mu}{\\sigma}>1\\Bigr)\n  =P(Z>1)\n  =1-\\Phi(1).$$", "answer": "$$\\boxed{1-\\Phi(1)}$$", "id": "10634"}, {"introduction": "To apply a probability distribution to real-world data, we must first estimate its parameters. This advanced practice introduces a powerful technique from statistical inference: the method of maximum likelihood estimation (MLE). By deriving the MLEs for the log-normal parameters $\\mu$ and $\\sigma^2$, you will discover an elegant connection between this distribution and the process of data analysis, revealing how simple statistics on log-transformed data can be used to fit the model [@problem_id:1401201].", "problem": "A set of $n$ measurements, represented by the random variables $X_1, X_2, \\dots, X_n$, are considered to be independent and identically distributed (i.i.d.). These measurements are modeled by a log-normal distribution.\n\nA random variable $X$ follows a log-normal distribution with parameters $\\mu$ and $\\sigma^2$ if its natural logarithm, $\\ln(X)$, is normally distributed with mean $\\mu$ and variance $\\sigma^2$. The Probability Density Function (PDF) for $X$ is given by:\n$$\nf(x; \\mu, \\sigma^2) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(x) - \\mu)^2}{2\\sigma^2}\\right)\n$$\nfor $x > 0$, $\\mu \\in \\mathbb{R}$, and $\\sigma > 0$.\n\nGiven a set of observed sample data $x_1, x_2, \\dots, x_n$, find the maximum likelihood estimators (MLEs) for the parameters $\\mu$ and $\\sigma^2$, denoted as $\\hat{\\mu}$ and $\\hat{\\sigma}^2$, respectively. Express your final answers for $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ in terms of the sample observations.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with the log-normal density\n$$\nf(x;\\mu,\\sigma^{2})=\\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(\\ln(x)-\\mu)^{2}}{2\\sigma^{2}}\\right),\\quad x>0,\\ \\mu\\in\\mathbb{R},\\ \\sigma>0.\n$$\nThe likelihood for observations $x_{1},\\dots,x_{n}$ is\n$$\nL(\\mu,\\sigma^{2})=\\prod_{i=1}^{n}\\frac{1}{x_{i}\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(\\ln(x_{i})-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(\\mu,\\sigma^{2})=\\ln L(\\mu,\\sigma^{2})=-n\\ln\\sigma-\\sum_{i=1}^{n}\\ln(x_{i})-\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}\\left(\\ln(x_{i})-\\mu\\right)^{2}.\n$$\nEquivalently, using $\\ln\\sigma=\\frac{1}{2}\\ln(\\sigma^{2})$,\n$$\n\\ell(\\mu,\\sigma^{2})=-\\frac{n}{2}\\ln(\\sigma^{2})-\\sum_{i=1}^{n}\\ln(x_{i})-\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}\\left(\\ln(x_{i})-\\mu\\right)^{2}.\n$$\nDifferentiate with respect to $\\mu$:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu}=-\\frac{1}{2\\sigma^{2}}\\cdot 2\\sum_{i=1}^{n}\\left(\\ln(x_{i})-\\mu\\right)(-1)=\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}\\left(\\ln(x_{i})-\\mu\\right).\n$$\nSet to zero and solve:\n$$\n\\sum_{i=1}^{n}\\ln(x_{i})-n\\mu=0\\quad\\Rightarrow\\quad \\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{n}\\ln(x_{i}).\n$$\nThe second derivative $\\partial^{2}\\ell/\\partial\\mu^{2}=-n/\\sigma^{2}<0$ confirms a maximum in $\\mu$.\n\nNow differentiate with respect to $\\sigma^{2}$. Let $S(\\mu)=\\sum_{i=1}^{n}(\\ln(x_{i})-\\mu)^{2}$. Then\n$$\n\\ell(\\mu,\\sigma^{2})=-\\frac{n}{2}\\ln(\\sigma^{2})-\\sum_{i=1}^{n}\\ln(x_{i})-\\frac{n}{2}\\ln(2\\pi)-\\frac{S(\\mu)}{2\\sigma^{2}},\n$$\nso\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^{2}}=-\\frac{n}{2}\\frac{1}{\\sigma^{2}}+\\frac{S(\\mu)}{2}\\frac{1}{\\sigma^{4}}.\n$$\nSet to zero:\n$$\n-\\frac{n}{2}\\frac{1}{\\sigma^{2}}+\\frac{S(\\mu)}{2}\\frac{1}{\\sigma^{4}}=0\\ \\Longrightarrow\\ -n\\sigma^{2}+S(\\mu)=0\\ \\Longrightarrow\\ \\hat{\\sigma}^{2}=\\frac{S(\\hat{\\mu})}{n}.\n$$\nSubstitute $\\hat{\\mu}$:\n$$\n\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\ln(x_{i})-\\frac{1}{n}\\sum_{j=1}^{n}\\ln(x_{j})\\right)^{2}.\n$$\nThe second derivative test in terms of $t=\\sigma^{2}$ gives $\\ell''(t)=(n/2)t^{-2}-S(\\mu)t^{-3}$, which at $t=\\hat{\\sigma}^{2}$ with $S(\\hat{\\mu})=n\\hat{\\sigma}^{2}$ yields $\\ell''(\\hat{\\sigma}^{2})=-(n/2)\\hat{\\sigma}^{-4}<0$, confirming a maximum.\n\nTherefore, the MLEs are the sample mean and (uncorrected) sample variance of the transformed data $\\ln(x_{i})$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{n}\\sum_{i=1}^{n}\\ln(x_{i}) & \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\ln(x_{i})-\\frac{1}{n}\\sum_{j=1}^{n}\\ln(x_{j})\\right)^{2}\\end{pmatrix}}$$", "id": "1401201"}]}