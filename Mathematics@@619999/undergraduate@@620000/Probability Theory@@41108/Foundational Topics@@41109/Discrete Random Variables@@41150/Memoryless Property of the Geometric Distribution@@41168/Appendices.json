{"hands_on_practices": [{"introduction": "The memoryless property is often used to predict future probabilities given a known process parameter. This exercise inverts the scenario, challenging you to use an observed conditional probability to deduce the underlying success parameter $p$ of a geometric process [@problem_id:11734]. By working backward with the survival function, you will gain a deeper appreciation for the algebraic foundation of memorylessness and its practical use in parameter estimation.", "problem": "A discrete random variable $X$ follows a geometric distribution with parameter $p$, denoted as $X \\sim \\text{Geom}(p)$. This distribution models the number of independent Bernoulli trials required to get the first success. The probability of success on any single trial is $p$, where $0 < p < 1$.\n\nThe probability mass function (PMF) for this distribution is given by:\n$$P(X=k) = (1-p)^{k-1}p \\quad \\text{for } k = 1, 2, 3, \\dots$$\n\nA useful related function is the survival function, $P(X>k)$, which gives the probability that the first success occurs after the $k$-th trial. This is equivalent to the probability that the first $k$ trials are all failures. The probability of a single failure is $(1-p)$. Therefore, the survival function is:\n$$P(X > k) = (1-p)^k$$\n\nGiven that the conditional probability $P(X > n | X > m)$ for $n>m$ is $\\frac{8}{27}$ when $n=4$ and $m=1$, derive the exact value of the success probability $p$.", "solution": "The problem asks for the value of $p$ given the conditional probability $P(X > 4 | X > 1) = \\frac{8}{27}$.\n\nWe begin with the definition of conditional probability:\n$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n\nApplying this definition to our specific problem, we let $A$ be the event $\\{X > 4\\}$ and $B$ be the event $\\{X > 1\\}$.\n$$P(X > 4 | X > 1) = \\frac{P(\\{X > 4\\} \\cap \\{X > 1\\})}{P(X > 1)}$$\n\nNext, we analyze the intersection of the two events in the numerator, $\\{X > 4\\} \\cap \\{X > 1\\}$. If the random variable $X$ takes a value greater than 4, it is necessarily also greater than 1. For example, if $X=5$, then both $X>4$ and $X>1$ are true. Therefore, the event $\\{X > 4\\}$ is a subset of the event $\\{X > 1\\}$, and their intersection is simply the more restrictive event, $\\{X > 4\\}$.\n$$ \\{X > 4\\} \\cap \\{X > 1\\} = \\{X > 4\\} $$\n\nSubstituting this back into our conditional probability expression:\n$$P(X > 4 | X > 1) = \\frac{P(X > 4)}{P(X > 1)}$$\n\nNow, we use the provided survival function for the geometric distribution, $P(X > k) = (1-p)^k$. We substitute $k=4$ for the numerator and $k=1$ for the denominator.\n$$P(X > 4 | X > 1) = \\frac{(1-p)^4}{(1-p)^1}$$\n\nUsing the laws of exponents, we simplify the expression:\n$$\\frac{(1-p)^4}{(1-p)^1} = (1-p)^{4-1} = (1-p)^3$$\n\nWe are given that this conditional probability is equal to $\\frac{8}{27}$. So, we set our derived expression equal to this value:\n$$(1-p)^3 = \\frac{8}{27}$$\n\nTo solve for $p$, we take the cube root of both sides of the equation:\n$$\\sqrt[3]{(1-p)^3} = \\sqrt[3]{\\frac{8}{27}}$$\n$$1-p = \\frac{\\sqrt[3]{8}}{\\sqrt[3]{27}}$$\n$$1-p = \\frac{2}{3}$$\n\nFinally, we isolate $p$ by rearranging the terms:\n$$p = 1 - \\frac{2}{3}$$\n$$p = \\frac{3}{3} - \\frac{2}{3} = \\frac{1}{3}$$", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "11734"}, {"introduction": "A profound consequence of the memoryless property is that it applies to the entire distribution of future waiting time, not just the probability of the next single event. This practice explores this idea by asking for the variance of the *additional* trials required for a success, given that a number of failures have already occurred [@problem_id:1351951]. You will see how the memoryless nature of the geometric distribution allows for a surprisingly straightforward answer to what might otherwise be a complex conditional variance calculation.", "problem": "An experiment consists of repeatedly flipping a fair coin until a head first appears. Suppose it is observed that the first three flips of the coin all resulted in tails.\n\nGiven this observation, what is the variance of the number of *additional* flips required to obtain the first head?", "solution": "Let each flip be an independent Bernoulli trial with success probability $p=\\frac{1}{2}$ for a head. Let $X$ denote the total number of flips needed to obtain the first head. Then $X$ has the geometric distribution on $\\{1,2,\\ldots\\}$ with\n$$\n\\Pr(X=k)=(1-p)^{k-1}p \\quad \\text{for } k\\in \\{1,2,\\ldots\\}.\n$$\nThe observation that the first three flips are tails is the event $\\{X>3\\}$. Let $Y$ denote the number of additional flips required to obtain the first head after these three tails. Then $Y=X-3$ conditional on $\\{X>3\\}$. For $k\\in \\{1,2,\\ldots\\}$,\n$$\n\\Pr(Y=k \\mid X>3)=\\Pr(X=3+k \\mid X>3)=\\frac{\\Pr(X=3+k)}{\\Pr(X>3)}.\n$$\nUsing the geometric pmf and tail,\n$$\n\\Pr(X=3+k)=(1-p)^{3+k-1}p,\\qquad \\Pr(X>3)=(1-p)^{3},\n$$\nso\n$$\n\\Pr(Y=k \\mid X>3)=\\frac{(1-p)^{3+k-1}p}{(1-p)^{3}}=(1-p)^{k-1}p.\n$$\nThus $Y \\mid (X>3)$ is geometric with parameter $p$, by the memoryless property. The variance of a geometric random variable on $\\{1,2,\\ldots\\}$ with parameter $p$ is\n$$\n\\operatorname{Var}(Y)=\\frac{1-p}{p^{2}}.\n$$\nSubstituting $p=\\frac{1}{2}$ gives\n$$\n\\operatorname{Var}(Y)=\\frac{1-\\frac{1}{2}}{\\left(\\frac{1}{2}\\right)^{2}}=\\frac{\\frac{1}{2}}{\\frac{1}{4}}=2.\n$$\nTherefore, the variance of the number of additional flips required is $2$.", "answer": "$$\\boxed{2}$$", "id": "1351951"}, {"introduction": "We conclude by exploring one of the most significant consequences of the memoryless property: the independence between sequential waiting times in a Bernoulli process. This conceptual problem moves beyond pure calculation to test your understanding of how the process effectively \"resets\" after each success [@problem_id:1922961]. Recognizing that the time to the first success has no bearing on the time to the second is a cornerstone insight for modeling more complex random phenomena like Poisson processes.", "problem": "A research team is studying the spontaneous mutation of a specific gene in a bacterium culture. The experiment consists of a sequence of independent trials. In each trial, a single bacterium is isolated and tested for the mutation. The probability that any given bacterium has the mutation is a constant $p$, where $0 < p < 1$.\n\nLet the random variable $X_1$ denote the number of trials conducted up to and including the first observation of a mutated bacterium. Let the random variable $X_2$ denote the number of additional trials required after the first mutation was found, up to and including the second observation of a mutated bacterium.\n\nWhich of the following statements correctly describes the relationship between the random variables $X_1$ and $X_2$?\n\nA. $X_1$ and $X_2$ are independent.\n\nB. $X_1$ and $X_2$ are positively correlated, meaning that a larger value of $X_1$ tends to be associated with a larger value of $X_2$.\n\nC. $X_1$ and $X_2$ are negatively correlated, meaning that a larger value of $X_1$ tends to be associated with a smaller value of $X_2$.\n\nD. The relationship (independence or dependence) between $X_1$ and $X_2$ depends on the specific value of the probability $p$.\n\nE. $X_1$ and $X_2$ are dependent, but their covariance is zero.", "solution": "Let $\\{Y_{n}\\}_{n\\geq 1}$ be an independent and identically distributed sequence with $Y_{n}\\sim\\text{Bernoulli}(p)$, where $0<p<1$. A mutation corresponds to $Y_{n}=1$. Define\n$$\nX_{1}=\\min\\{n\\geq 1:Y_{n}=1\\},\n$$\nthe trial index of the first mutation, and\n$$\nX_{2}=\\min\\{m\\geq 1:Y_{X_{1}+m}=1\\},\n$$\nthe additional number of trials after the first mutation until the second mutation occurs. Equivalently, if $T_{k}$ denotes the index of the $k$th success, then $X_{1}=T_{1}$ and $X_{2}=T_{2}-T_{1}$.\n\nBy independence of the $Y_{n}$, the distribution of $X_{1}$ is geometric with parameter $p$ supported on $\\{1,2,\\dots\\}$:\n$$\n\\mathbb{P}(X_{1}=a)=(1-p)^{a-1}p,\\quad a\\in\\{1,2,\\dots\\}.\n$$\nAfter time $X_{1}$, the future trials $\\{Y_{X_{1}+1},Y_{X_{1}+2},\\dots\\}$ are independent of the past and have the same distribution as the original sequence. Hence $X_{2}$ is also geometric with parameter $p$ on $\\{1,2,\\dots\\}$:\n$$\n\\mathbb{P}(X_{2}=b)=(1-p)^{b-1}p,\\quad b\\in\\{1,2,\\dots\\}.\n$$\n\nTo establish independence, compute the joint distribution for $a,b\\in\\{1,2,\\dots\\}$:\n$$\n\\mathbb{P}(X_{1}=a,X_{2}=b)=\\mathbb{P}(Y_{1}=0,\\dots,Y_{a-1}=0,Y_{a}=1,Y_{a+1}=0,\\dots,Y_{a+b-1}=0,Y_{a+b}=1).\n$$\nBy independence of the $Y_{n}$,\n$$\n\\mathbb{P}(X_{1}=a,X_{2}=b)=(1-p)^{a-1}p\\cdot(1-p)^{b-1}p=\\big((1-p)^{a-1}p\\big)\\big((1-p)^{b-1}p\\big).\n$$\nTherefore\n$$\n\\mathbb{P}(X_{1}=a,X_{2}=b)=\\mathbb{P}(X_{1}=a)\\,\\mathbb{P}(X_{2}=b),\n$$\nwhich proves that $X_{1}$ and $X_{2}$ are independent.\n\nConsequently, the correct statement is that $X_{1}$ and $X_{2}$ are independent. Options asserting positive or negative correlation, dependence varying with $p$, or dependence with zero covariance are all incorrect in this model.", "answer": "$$\\boxed{A}$$", "id": "1922961"}]}