{"hands_on_practices": [{"introduction": "The geometric distribution is a cornerstone for modeling \"time-to-first-event\" scenarios, such as the number of attempts needed for a success. This first exercise [@problem_id:1373232] provides a foundational practice in deriving the variance of a geometric random variable directly from its definition. Mastering this derivation is crucial, as it reinforces the core definitions of expectation and variance and illustrates the powerful technique of using series manipulation to find moments of a distribution.", "problem": "A deep space probe communicates with a ground station by sending data packets. Due to solar flares and cosmic radiation, there is a constant probability $p$ that any given packet gets corrupted during transmission. The transmissions of individual packets are independent events. The ground station's protocol involves continuously monitoring the stream of packets and initiating a full system reset as soon as the first corrupted packet is detected. Let the random variable $N$ be the total number of packets sent, including the first corrupted packet that triggers the reset. Find a symbolic expression for the variance of $N$ in terms of the probability $p$.", "solution": "The problem describes a sequence of independent Bernoulli trials, where each trial is the transmission of a single data packet. We can define a \"success\" as a packet being corrupted, which occurs with probability $p$. The random variable $N$ represents the number of trials required to observe the first success. This is the definition of a geometric random variable.\n\nThe probability mass function (PMF) for $N$ is given by $P(N=k) = (1-p)^{k-1}p$ for $k=1, 2, 3, \\dots$. This is because for the first success to occur on the $k$-th trial, we must have $k-1$ failures (uncorrupted packets), each with probability $1-p$, followed by one success (a corrupted packet), with probability $p$.\n\nThe variance of a random variable $N$ is defined as $\\text{Var}(N) = E[N^2] - (E[N])^2$. We will compute the first moment (mean) $E[N]$ and the second moment $E[N^2]$.\n\nFirst, let's find the mean, $E[N]$. By definition,\n$$E[N] = \\sum_{k=1}^{\\infty} k P(N=k) = \\sum_{k=1}^{\\infty} k (1-p)^{k-1}p$$\nLet $q = 1-p$. The expression becomes\n$$E[N] = p \\sum_{k=1}^{\\infty} k q^{k-1}$$\nWe use the formula for a geometric series: for $|q|<1$, $\\sum_{k=0}^{\\infty} q^k = \\frac{1}{1-q}$. Differentiating both sides with respect to $q$ gives:\n$$\\frac{d}{dq} \\sum_{k=0}^{\\infty} q^k = \\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{d}{dq} \\left(\\frac{1}{1-q}\\right) = \\frac{1}{(1-q)^2}$$\nSubstituting this back into the expression for $E[N]$:\n$$E[N] = p \\cdot \\frac{1}{(1-q)^2} = p \\cdot \\frac{1}{(1-(1-p))^2} = p \\cdot \\frac{1}{p^2} = \\frac{1}{p}$$\n\nNext, we find the second moment, $E[N^2]$.\n$$E[N^2] = \\sum_{k=1}^{\\infty} k^2 P(N=k) = \\sum_{k=1}^{\\infty} k^2 (1-p)^{k-1}p = p \\sum_{k=1}^{\\infty} k^2 q^{k-1}$$\nTo evaluate this sum, it is easier to first compute $E[N(N-1)]$.\n$$E[N(N-1)] = E[N^2 - N] = E[N^2] - E[N]$$\nLet's compute $E[N(N-1)]$ from its definition:\n$$E[N(N-1)] = \\sum_{k=1}^{\\infty} k(k-1)P(N=k) = \\sum_{k=1}^{\\infty} k(k-1) p q^{k-1} = p \\sum_{k=2}^{\\infty} k(k-1) q^{k-1}$$\nTo evaluate this sum, we differentiate the geometric series formula twice. We already have the first derivative: $\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{1}{(1-q)^2}$. Differentiating again with respect to $q$:\n$$\\frac{d}{dq} \\sum_{k=1}^{\\infty} k q^{k-1} = \\sum_{k=2}^{\\infty} k(k-1) q^{k-2} = \\frac{d}{dq} \\left(\\frac{1}{(1-q)^2}\\right) = \\frac{2}{(1-q)^3}$$\nMultiplying by $q$ does not give us the sum we want, so let's adjust the sum from $E[N(N-1)]$:\n$$E[N(N-1)] = p \\sum_{k=2}^{\\infty} k(k-1) q^{k-1} = pq \\sum_{k=2}^{\\infty} k(k-1) q^{k-2}$$\nNow we can substitute the second derivative result:\n$$E[N(N-1)] = pq \\left( \\frac{2}{(1-q)^3} \\right) = p(1-p) \\frac{2}{(1-(1-p))^3} = p(1-p) \\frac{2}{p^3} = \\frac{2(1-p)}{p^2}$$\nNow we can find $E[N^2]$ using the relation $E[N^2] = E[N(N-1)] + E[N]$.\n$$E[N^2] = \\frac{2(1-p)}{p^2} + \\frac{1}{p} = \\frac{2(1-p) + p}{p^2} = \\frac{2 - 2p + p}{p^2} = \\frac{2-p}{p^2}$$\nFinally, we can calculate the variance:\n$$\\text{Var}(N) = E[N^2] - (E[N])^2 = \\frac{2-p}{p^2} - \\left(\\frac{1}{p}\\right)^2 = \\frac{2-p}{p^2} - \\frac{1}{p^2} = \\frac{2-p-1}{p^2} = \\frac{1-p}{p^2}$$\nThus, the variance of the number of packets sent until the first corruption is $\\frac{1-p}{p^2}$.", "answer": "$$\\boxed{\\frac{1-p}{p^{2}}}$$", "id": "1373232"}, {"introduction": "A common point of nuance with the geometric distribution is whether one is counting the total number of trials until the first success, or the number of failures *before* the first success. This problem [@problem_id:1373263] focuses on the latter, a subtle but important distinction. By engaging with this variation, you will see firsthand how a simple shift in the random variable's definition affects its mean, but not its variance, thereby deepening your conceptual understanding of these statistical properties.", "problem": "An automated quality control system at a large-scale confectionery factory inspects cookies from a production line. Due to minor variations in the baking process, not all cookies are perfectly circular. The system has determined that the probability of any single cookie being perfectly circular is a constant value, $p$, where $0 < p < 1$. The quality of each cookie is independent of the others.\n\nAn inspector observes the process, counting the number of non-circular cookies that pass by before the first perfectly circular one is identified. What is the variance of the number of non-circular cookies counted before the first perfect one is found? Express your answer as a closed-form analytic expression in terms of $p$.", "solution": "Let $X$ denote the number of non-circular cookies observed before the first perfectly circular one appears. Each cookie is independently perfect with probability $p$, so $X$ follows the geometric distribution on $\\{0,1,2,\\ldots\\}$ with parameter $p$, with probability mass function\n$$\n\\Pr(X=k)=p(1-p)^{k}, \\quad k=0,1,2,\\ldots\n$$\nLet $r=1-p$, so $0<r<1$. To compute moments, use the power series identities derived from the geometric series. First,\n$$\n\\sum_{k=0}^{\\infty} r^{k}=\\frac{1}{1-r}.\n$$\nDifferentiating with respect to $r$ gives\n$$\n\\sum_{k=1}^{\\infty} k r^{k-1}=\\frac{1}{(1-r)^{2}},\n$$\nso multiplying by $r$ yields\n$$\n\\sum_{k=0}^{\\infty} k r^{k}=\\frac{r}{(1-r)^{2}}.\n$$\nDifferentiate $\\sum_{k=0}^{\\infty} k r^{k}=\\frac{r}{(1-r)^{2}}$ with respect to $r$:\n$$\n\\sum_{k=1}^{\\infty} k^{2} r^{k-1}=\\frac{d}{dr}\\left(\\frac{r}{(1-r)^{2}}\\right)=\\frac{1}{(1-r)^{2}}+\\frac{2r}{(1-r)^{3}}=\\frac{1+r}{(1-r)^{3}},\n$$\nand multiply by $r$ to obtain\n$$\n\\sum_{k=0}^{\\infty} k^{2} r^{k}=\\frac{r(1+r)}{(1-r)^{3}}.\n$$\nNow compute the first and second moments of $X$:\n$$\n\\mathbb{E}[X]=\\sum_{k=0}^{\\infty} k\\, p r^{k}=p \\sum_{k=0}^{\\infty} k r^{k}=p \\cdot \\frac{r}{(1-r)^{2}}=p \\cdot \\frac{r}{p^{2}}=\\frac{r}{p}=\\frac{1-p}{p},\n$$\n$$\n\\mathbb{E}[X^{2}]=\\sum_{k=0}^{\\infty} k^{2}\\, p r^{k}=p \\sum_{k=0}^{\\infty} k^{2} r^{k}=p \\cdot \\frac{r(1+r)}{(1-r)^{3}}=p \\cdot \\frac{r(1+r)}{p^{3}}=\\frac{r(1+r)}{p^{2}}.\n$$\nTherefore, the variance is\n$$\n\\mathrm{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{r(1+r)}{p^{2}}-\\left(\\frac{r}{p}\\right)^{2}=\\frac{r}{p^{2}}=\\frac{1-p}{p^{2}}.\n$$", "answer": "$$\\boxed{\\frac{1-p}{p^{2}}}$$", "id": "1373263"}, {"introduction": "The true power of probability theory is often revealed when it is applied to solve optimization problems in science and engineering. This final practice [@problem_id:1373244] elevates our analysis from pure calculation to practical decision-making. Here, you will use the formulas for the mean and variance as inputs into a cost function, and then apply calculus to find the optimal success probability $p$ that minimizes total cost, showcasing how statistical models guide real-world optimization.", "problem": "A team of physicists is calibrating a specialized quantum sensor designed to detect a rare particle. Each measurement cycle is considered an independent Bernoulli trial. The probability of a successful detection in any given cycle is denoted by $p$, which can be finely tuned by adjusting the sensor's energy input. The total cost associated with running an experiment until the first detection is modeled by the sum of two components:\n\n1.  An **operational cost**, which is proportional to a statistical measure of the experiment's duration. The duration is unpredictable, so this cost is modeled as being proportional to the sum of the mean and the variance of the number of cycles, $X$, needed to get the first detection. This cost is given by $C_{op} = \\mathcal{A} \\left( E[X] + \\text{Var}(X) \\right)$.\n\n2.  A **setup cost**, which is associated with the energy required to maintain the sensor's sensitivity. This cost is found to be proportional to the square of the success probability, given by $C_{setup} = \\mathcal{B} p^2$.\n\nThe total cost is $C(p) = C_{op} + C_{setup}$. Assume the number of cycles $X$ required for the first success (where $X=1, 2, 3, \\dots$) follows a geometric distribution. The positive constants are given as $\\mathcal{A} = 3$ and $\\mathcal{B} = 48$.\n\nDetermine the success probability $p \\in (0, 1)$ that minimizes the total cost $C(p)$. Express your answer as a fraction in simplest form.", "solution": "Let $X$ be the number of cycles until the first detection, with $X \\in \\{1,2,3,\\dots\\}$ and success probability $p \\in (0,1)$. For the geometric distribution on $\\{1,2,\\dots\\}$, the mean and variance are\n$$\nE[X] = \\frac{1}{p}, \\qquad \\text{Var}(X) = \\frac{1-p}{p^{2}}.\n$$\nThe operational cost is\n$$\nC_{op} = \\mathcal{A}\\left(E[X] + \\text{Var}(X)\\right) = \\mathcal{A}\\left(\\frac{1}{p} + \\frac{1-p}{p^{2}}\\right).\n$$\nCombine terms over the common denominator $p^{2}$:\n$$\n\\frac{1}{p} + \\frac{1-p}{p^{2}} = \\frac{p}{p^{2}} + \\frac{1-p}{p^{2}} = \\frac{p + 1 - p}{p^{2}} = \\frac{1}{p^{2}}.\n$$\nHence\n$$\nC_{op} = \\frac{\\mathcal{A}}{p^{2}}.\n$$\nThe setup cost is\n$$\nC_{setup} = \\mathcal{B} p^{2}.\n$$\nTherefore the total cost as a function of $p$ is\n$$\nC(p) = \\frac{\\mathcal{A}}{p^{2}} + \\mathcal{B} p^{2}.\n$$\nTo minimize $C(p)$ for $p \\in (0,1)$, differentiate and set the derivative to zero:\n$$\nC'(p) = -2\\mathcal{A} p^{-3} + 2\\mathcal{B} p = 0\n\\quad \\Longrightarrow \\quad\n2\\mathcal{B} p = \\frac{2\\mathcal{A}}{p^{3}}\n\\quad \\Longrightarrow \\quad\n\\mathcal{B} p^{4} = \\mathcal{A}\n\\quad \\Longrightarrow \\quad\np = \\left(\\frac{\\mathcal{A}}{\\mathcal{B}}\\right)^{\\frac{1}{4}}.\n$$\nTo confirm this critical point is a minimum, compute the second derivative:\n$$\nC''(p) = 6\\mathcal{A} p^{-4} + 2\\mathcal{B} > 0 \\quad \\text{for } p>0,\n$$\nso $C$ is strictly convex on $(0,\\infty)$ and the critical point is the unique global minimizer. Substituting the given constants $\\mathcal{A} = 3$ and $\\mathcal{B} = 48$,\n$$\np = \\left(\\frac{3}{48}\\right)^{\\frac{1}{4}} = \\left(\\frac{1}{16}\\right)^{\\frac{1}{4}} = \\frac{1}{2},\n$$\nwhich lies in $(0,1)$ and thus is valid.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1373244"}]}