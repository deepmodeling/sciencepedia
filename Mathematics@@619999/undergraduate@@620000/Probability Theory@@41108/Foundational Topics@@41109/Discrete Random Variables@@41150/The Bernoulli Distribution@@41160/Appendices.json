{"hands_on_practices": [{"introduction": "This first practice explores a fundamental question when dealing with multiple random events: what is the distribution of their sum? By calculating the probability for the sum of two independent Bernoulli trials, we lay the groundwork for understanding more complex models like the Binomial distribution. This exercise reinforces the core concepts of independence and mutually exclusive events. [@problem_id:682]", "problem": "A discrete random variable $X$ is said to follow a Bernoulli distribution with parameter $p$, denoted as $X \\sim \\text{Bernoulli}(p)$, if it describes a single trial with two possible outcomes: success (value 1) or failure (value 0). The probability of success is $P(X=1) = p$, and the probability of failure is $P(X=0) = 1-p$, where $0 \\le p \\le 1$.\n\nConsider two random variables, $X_1$ and $X_2$, that are independent and identically distributed (i.i.d.) according to a Bernoulli distribution with parameter $p$.\n\nDerive an expression for the probability that the sum of these two variables is exactly equal to one. That is, find $P(X_1 + X_2 = 1)$.", "solution": "Let $X_1$ and $X_2$ be two independent and identically distributed random variables, both following a Bernoulli distribution with parameter $p$.\nThe probability mass function (PMF) for each variable is given by:\n$$\nP(X_i=1) = p\n$$\n$$\nP(X_i=0) = 1-p\n$$\nfor $i=1, 2$.\n\nWe want to find the probability that their sum is equal to one, $P(X_1 + X_2 = 1)$. Since $X_1$ and $X_2$ can only take values of 0 or 1, their sum can be 0, 1, or 2. The event $X_1 + X_2 = 1$ can occur in two mutually exclusive ways:\n1.  $X_1 = 1$ and $X_2 = 0$.\n2.  $X_1 = 0$ and $X_2 = 1$.\n\nThe probability of the event $X_1 + X_2 = 1$ is the sum of the probabilities of these two mutually exclusive outcomes.\n$$\nP(X_1 + X_2 = 1) = P( (X_1=1 \\text{ and } X_2=0) \\text{ or } (X_1=0 \\text{ and } X_2=1) )\n$$\nBecause the two cases are mutually exclusive, we can write this as:\n$$\nP(X_1 + X_2 = 1) = P(X_1=1, X_2=0) + P(X_1=0, X_2=1)\n$$\n\nSince $X_1$ and $X_2$ are independent, the joint probability of any combination of their outcomes is the product of their individual probabilities.\nFor the first case, $(X_1=1, X_2=0)$:\n$$\nP(X_1=1, X_2=0) = P(X_1=1) \\times P(X_2=0)\n$$\nSubstituting the probabilities from the Bernoulli distribution definition:\n$$\nP(X_1=1, X_2=0) = p \\times (1-p) = p(1-p)\n$$\n\nFor the second case, $(X_1=0, X_2=1)$:\n$$\nP(X_1=0, X_2=1) = P(X_1=0) \\times P(X_2=1)\n$$\nSubstituting the probabilities:\n$$\nP(X_1=0, X_2=1) = (1-p) \\times p = p(1-p)\n$$\n\nNow, we sum the probabilities of these two cases to find the total probability of $X_1 + X_2 = 1$:\n$$\nP(X_1 + X_2 = 1) = p(1-p) + p(1-p)\n$$\n$$\nP(X_1 + X_2 = 1) = 2p(1-p)\n$$", "answer": "$$\\boxed{2p(1-p)}$$", "id": "682"}, {"introduction": "While we often calculate variance from a known probability, this exercise reverses the process. Given the variance of a Bernoulli-distributed event, we work backward to find the possible underlying probabilities of success. This practice deepens our understanding of variance as an intrinsic property of the distribution and reveals the symmetrical nature of uncertainty in a binary outcome. [@problem_id:1392758]", "problem": "A data analytics firm is studying consumer behavior for a new online subscription service. The action of a single, randomly selected consumer either purchasing the subscription or not is modeled as a discrete random event. A random variable $X$ is defined to represent this outcome: $X=1$ if the consumer makes a purchase, and $X=0$ if they do not. After analyzing a large sample of consumers, the firm determines that the variance of this random variable, $\\text{Var}(X)$, is $0.21$.\n\nLet $p$ represent the probability that a consumer makes a purchase. Based on the given variance, determine the two possible values for $p$. Present your two answers as decimal numbers in ascending order.", "solution": "Let $X$ be a Bernoulli random variable with success probability $p$. The variance of a Bernoulli random variable is given by\n$$\n\\mathrm{Var}(X)=p(1-p).\n$$\nWe are given $\\mathrm{Var}(X)=0.21$, so\n$$\np(1-p)=0.21.\n$$\nRewriting,\n$$\np-p^{2}=0.21 \\;\\;\\Longrightarrow\\;\\; p^{2}-p+0.21=0.\n$$\nSolving the quadratic equation using the quadratic formula,\n$$\np=\\frac{1\\pm \\sqrt{1-4\\cdot 0.21}}{2}=\\frac{1\\pm \\sqrt{0.16}}{2}=\\frac{1\\pm 0.4}{2}.\n$$\nThus the two solutions are\n$$\np=\\frac{1-0.4}{2}=0.3 \\quad \\text{and} \\quad p=\\frac{1+0.4}{2}=0.7,\n$$\nboth of which lie in the interval $[0,1]$. In ascending order, these are $0.3$ and $0.7$.", "answer": "$$\\boxed{\\begin{pmatrix}0.3 & 0.7\\end{pmatrix}}$$", "id": "1392758"}, {"introduction": "Building on the concept of variance, we now ask a question central to fields like information theory and engineering: when is a random binary event most unpredictable? This problem tasks us with finding the probability $p$ that maximizes the variance of a Bernoulli trial. The result is a cornerstone principle in statistics and has practical implications for designing systems where maximal randomness is desired. [@problem_id:1392744]", "problem": "A team of engineers is designing a novel binary communication channel. In this channel, individual bits are transmitted one at a time. Due to inherent noise in the system, a transmitted bit may be flipped. The outcome of transmitting a single bit is modeled as a random event. Let the random variable $X$ describe the state of the received bit, where $X=1$ if the bit is received correctly and $X=0$ if the bit is received incorrectly (i.e., flipped).\n\nThe probability of a correct transmission is given by $P(X=1) = p$, where $p$ is a design parameter of the channel that can be tuned, with $0 \\le p \\le 1$. Consequently, the probability of an incorrect transmission is $P(X=0) = 1-p$.\n\nFor a specific cryptographic application, the channel must be designed to have the maximum possible \"unpredictability\" in its outcome. This unpredictability is quantitatively measured by the statistical variance of the random variable $X$. Determine the value of the parameter $p$ that maximizes the variance of $X$. Your answer should be a single numerical value.", "solution": "We model the received bit as a Bernoulli random variable $X$ with $P(X=1)=p$ and $P(X=0)=1-p$. For a Bernoulli variable, $E[X]=p$ and, using $X^{2}=X$, we have $E[X^{2}]=E[X]=p$. Therefore, the variance is\n$$\n\\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}=p-p^{2}=p(1-p).\n$$\nTo maximize $\\operatorname{Var}(X)$ over $p \\in [0,1]$, define $f(p)=p(1-p)$. Compute the derivative:\n$$\nf'(p)=1-2p.\n$$\nSet $f'(p)=0$ to find critical points:\n$$\n1-2p=0 \\;\\Rightarrow\\; p=\\frac{1}{2}.\n$$\nThe second derivative is\n$$\nf''(p)=-2<0,\n$$\nwhich confirms that $p=\\frac{1}{2}$ is a local maximum. Evaluating endpoints, $f(0)=0$ and $f(1)=0$, while $f\\!\\left(\\frac{1}{2}\\right)=\\frac{1}{4}$, so the maximum over $[0,1]$ occurs at $p=\\frac{1}{2}$. Thus, the value of $p$ that maximizes the variance is $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1392744"}]}