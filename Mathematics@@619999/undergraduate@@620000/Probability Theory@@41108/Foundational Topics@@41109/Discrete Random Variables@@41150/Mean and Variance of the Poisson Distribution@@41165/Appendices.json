{"hands_on_practices": [{"introduction": "A cornerstone of working with any probability distribution is a firm grasp of its fundamental properties. For the Poisson distribution, the most remarkable property is the equality of its mean and variance. This first exercise [@problem_id:6536] guides you through the process of deriving the expectation $\\mathbb{E}[X]$ and variance $\\operatorname{Var}(X)$ directly from the probability mass function, reinforcing your understanding of these core concepts beyond mere memorization.", "problem": "A discrete random variable $X$ is said to follow a Poisson distribution with a parameter $\\lambda > 0$ if its probability mass function (PMF) is given by:\n$$P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad \\text{for } k = 0, 1, 2, \\dots$$\n\nThe expectation (or mean) of the random variable $X$ is defined as:\n$$\\mathbb{E}[X] = \\sum_{k=0}^{\\infty} k P(X=k)$$\n\nThe variance of $X$ is defined as:\n$$\\operatorname{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$$\nA common formula for calculating the variance is:\n$$\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\nwhere $\\mathbb{E}[X^2] = \\sum_{k=0}^{\\infty} k^2 P(X=k)$.\n\nYou may find the following Taylor series expansion for the exponential function useful:\n$$\\sum_{n=0}^{\\infty} \\frac{x^n}{n!} = e^x$$\n\n**Task:**\nFor a random variable $X$ that follows a Poisson distribution, it is given that the sum of its expectation and its variance is equal to a known positive constant $S$. That is, $\\mathbb{E}[X] + \\operatorname{Var}(X) = S$.\n\nStarting from the fundamental definitions provided, first derive the expressions for the expectation $\\mathbb{E}[X]$ and the variance $\\operatorname{Var}(X)$ in terms of the parameter $\\lambda$. Then, use these results to derive the value of $\\lambda$ in terms of $S$.", "solution": "The expectation is defined by\n$$\\mathbb{E}[X]=\\sum_{k=0}^\\infty kP(X=k)=\\sum_{k=1}^\\infty k\\frac{\\lambda^k e^{-\\lambda}}{k!}\n=e^{-\\lambda}\\sum_{k=1}^\\infty\\frac{\\lambda^k}{(k-1)!}\n=\\lambda e^{-\\lambda}\\sum_{m=0}^\\infty\\frac{\\lambda^m}{m!}\n=\\lambda.$$\n\nNext, observe $k^2=k(k-1)+k$ so\n$$\\mathbb{E}[X^2]=\\sum_{k=0}^\\infty k^2P(X=k)\n=\\sum_{k=2}^\\infty\\frac{k(k-1)\\lambda^k e^{-\\lambda}}{k!}+\\sum_{k=1}^\\infty k\\frac{\\lambda^k e^{-\\lambda}}{k!}\n=e^{-\\lambda}\\sum_{k=2}^\\infty\\frac{\\lambda^k}{(k-2)!}+\\lambda\n=\\lambda^2+\\lambda.$$\n\nTherefore\n$$\\operatorname{Var}(X)=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2=(\\lambda^2+\\lambda)-\\lambda^2=\\lambda.$$\n\nSince $S=\\mathbb{E}[X]+\\operatorname{Var}(X)=\\lambda+\\lambda=2\\lambda$, it follows that\n$$\\lambda=\\frac{S}{2}.$$", "answer": "$$\\boxed{\\frac{S}{2}}$$", "id": "6536"}, {"introduction": "Real-world phenomena are often the result of multiple, interacting random processes. Building on our understanding of a single Poisson variable, this practice [@problem_id:1373948] explores a scenario with two independent Poisson processes, a common model for systems like queues or population dynamics. You will apply the properties of expectation and variance to the difference of two variables, a crucial skill for analyzing the net effect in such systems.", "problem": "A computational biology server processes gene sequencing jobs. The number of jobs arriving at the server in a given one-hour interval, denoted by the random variable $X$, is modeled by a Poisson distribution. Concurrently, the number of jobs completed by the server in the same one-hour interval, denoted by $Y$, is also modeled by a Poisson distribution. The two processes are statistically independent.\n\nAn analyst observes the system and determines two key properties of the net change in the number of jobs in the server's queue, which is given by the difference $D = X - Y$. The expected value of this difference is found to be $\\mathbb{E}[D] = -2$, and the variance of this difference is $\\operatorname{Var}(D) = 12$.\n\nBased on this information, determine the variance of the number of arriving jobs, $\\operatorname{Var}(X)$, and the variance of the number of completed jobs, $\\operatorname{Var}(Y)$. Present your answer as a row matrix $\\begin{pmatrix} \\operatorname{Var}(X) & \\operatorname{Var}(Y) \\end{pmatrix}$.", "solution": "Let $X \\sim \\text{Poisson}(\\lambda_{X})$ and $Y \\sim \\text{Poisson}(\\lambda_{Y})$, with $X$ and $Y$ independent. For a Poisson random variable with rate parameter $\\lambda$, the mean and variance are both equal to $\\lambda$, so $\\mathbb{E}[X] = \\operatorname{Var}(X) = \\lambda_{X}$ and $\\mathbb{E}[Y] = \\operatorname{Var}(Y) = \\lambda_{Y}$.\n\nDefine $D = X - Y$. By linearity of expectation,\n$$\n\\mathbb{E}[D] = \\mathbb{E}[X - Y] = \\mathbb{E}[X] - \\mathbb{E}[Y] = \\lambda_{X} - \\lambda_{Y}.\n$$\nGiven $\\mathbb{E}[D] = -2$, we have\n$$\n\\lambda_{X} - \\lambda_{Y} = -2.\n$$\n\nSince $X$ and $Y$ are independent, the variance of their difference is the sum of their variances:\n$$\n\\operatorname{Var}(D) = \\operatorname{Var}(X - Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) = \\lambda_{X} + \\lambda_{Y}.\n$$\nGiven $\\operatorname{Var}(D) = 12$, we have\n$$\n\\lambda_{X} + \\lambda_{Y} = 12.\n$$\n\nSolving the system\n$$\n\\begin{cases}\n\\lambda_{X} - \\lambda_{Y} = -2, \\\\\n\\lambda_{X} + \\lambda_{Y} = 12,\n\\end{cases}\n$$\nadd the equations to obtain $2\\lambda_{X} = 10$, hence $\\lambda_{X} = 5$. Substituting into $\\lambda_{X} + \\lambda_{Y} = 12$ gives $\\lambda_{Y} = 7$.\n\nTherefore, $\\operatorname{Var}(X) = 5$ and $\\operatorname{Var}(Y) = 7$.", "answer": "$$\\boxed{\\begin{pmatrix} 5 & 7 \\end{pmatrix}}$$", "id": "1373948"}, {"introduction": "The unique properties of a distribution can have profound implications in the field of statistical inference. This final practice [@problem_id:1373957] moves from calculation to a more conceptual level, asking you to consider the statistical role of an observation from a Poisson distribution. This thought experiment highlights how the equality of mean and variance makes a single data point a surprisingly good—specifically, unbiased—estimator for the population's variability, connecting probability theory directly to the practical task of estimation.", "problem": "A data scientist is modeling the number of user clicks on a newly launched, obscure feature on a website within a one-minute interval. Based on previous experience with similar features, they assume that the number of clicks, represented by the random variable $X$, follows a Poisson distribution with an unknown parameter $\\lambda > 0$. The parameter $\\lambda$ represents the true average number of clicks per minute.\n\nThe data scientist is particularly interested in the inherent variability of the click count, which is quantified by the variance of the distribution, $\\theta = \\operatorname{Var}(X)$. Due to a temporary data logging issue, only a single one-minute interval is successfully recorded, yielding a single observation of the number of clicks. The data scientist proposes to use this single count, the value of the random variable $X$, as an estimator for the true variance $\\theta$. Let this estimator be denoted by $\\hat{\\theta} = X$.\n\nDetermine the nature of this estimator $\\hat{\\theta}$ for the true variance $\\theta$.\n\nA. The estimator is unbiased.\n\nB. The estimator is biased, with a positive bias that is independent of $\\lambda$.\n\nC. The estimator is biased, with a negative bias that is independent of $\\lambda$.\n\nD. The estimator is biased, and the sign of the bias depends on whether $\\lambda$ is greater than or less than 1.\n\nE. Whether the estimator is biased or unbiased cannot be determined without knowing the specific value of $\\lambda$.", "solution": "Let $X \\sim \\text{Poisson}(\\lambda)$ with $\\lambda > 0$. For a Poisson distribution, the mean and variance are both equal to $\\lambda$, so\n$$\n\\mathbb{E}[X] = \\lambda, \\quad \\operatorname{Var}(X) = \\lambda.\n$$\nThe parameter of interest is $\\theta = \\operatorname{Var}(X) = \\lambda$. The proposed estimator is $\\hat{\\theta} = X$. The bias of an estimator $\\hat{\\theta}$ for $\\theta$ is defined as\n$$\n\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta.\n$$\nCompute the expectation:\n$$\n\\mathbb{E}[\\hat{\\theta}] = \\mathbb{E}[X] = \\lambda.\n$$\nTherefore,\n$$\n\\operatorname{Bias}(\\hat{\\theta}) = \\lambda - \\lambda = 0.\n$$\nSince the bias is zero for all $\\lambda > 0$, the estimator $\\hat{\\theta} = X$ is unbiased for $\\theta$.\n\nThus, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1373957"}]}