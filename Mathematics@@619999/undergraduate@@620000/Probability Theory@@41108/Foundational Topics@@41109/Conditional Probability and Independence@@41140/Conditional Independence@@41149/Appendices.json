{"hands_on_practices": [{"introduction": "We begin our hands-on practice with a foundational scenario that neatly aligns with our intuition about sequences of events. This exercise explores a simple \"chain\" structure, where the outcome of one event influences the next, which in turn influences a third. By examining three consecutive coin tosses [@problem_id:2850], we can formally verify a key principle: knowing the outcome of the intermediate event can render the first and last events probabilistically independent. This concept is the cornerstone of powerful models like Markov chains, which are used to describe systems evolving through time.", "problem": "Consider a sequence of three independent tosses of a single fair coin. Let $A$ be the event that the first toss results in heads, $B$ be the event that the second toss results in heads, and $C$ be the event that the third toss results in heads.\n\nTwo events, $X$ and $Y$, are said to be conditionally independent given a third event $Z$ if and only if the following equality holds:\n$$P(X \\cap Y | Z) = P(X|Z)P(Y|Z)$$\n\nYour task is to determine if the outcome of the first toss is conditionally independent of the outcome of the third toss, given the outcome of the second toss. To do this, calculate the value of the quantity $\\Delta$, defined as:\n$$\\Delta = P(A \\cap C | B) - P(A|B)P(C|B)$$\nA value of $\\Delta = 0$ demonstrates conditional independence.", "solution": "The problem asks for the value of $\\Delta = P(A \\cap C | B) - P(A|B)P(C|B)$ for three independent tosses of a fair coin. We will calculate each term in the expression for $\\Delta$ separately.\n\nFirst, let's define the sample space, $\\Omega$, for three coin tosses. Let 'H' denote heads and 'T' denote tails. The sample space consists of $2^3 = 8$ equally likely outcomes:\n$$\\Omega = \\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\}$$\nSince the coin is fair, the probability of any single outcome is $P(\\omega_i) = \\frac{1}{8}$ for $\\omega_i \\in \\Omega$.\n\nThe events are defined as:\n- $A$: First toss is H. $A = \\{HHH, HHT, HTH, HTT\\}$. Thus, $P(A) = \\frac{4}{8} = \\frac{1}{2}$.\n- $B$: Second toss is H. $B = \\{HHH, HHT, THH, THT\\}$. Thus, $P(B) = \\frac{4}{8} = \\frac{1}{2}$.\n- $C$: Third toss is H. $C = \\{HHH, HTH, THH, TTH\\}$. Thus, $P(C) = \\frac{4}{8} = \\frac{1}{2}$.\n\nWe will use the definition of conditional probability, $P(X|Y) = \\frac{P(X \\cap Y)}{P(Y)}$.\n\n**Step 1: Calculate $P(A \\cap C | B)$**\nFirst, we need the intersection of events $A$ and $C$, which represents the first and third tosses being heads.\n$$A \\cap C = \\{HHH, HTH\\}$$\nNow, we need the intersection of this result with event $B$:\n$$(A \\cap C) \\cap B = \\{HHH, HTH\\} \\cap \\{HHH, HHT, THH, THT\\} = \\{HHH\\}$$\nThe probability of this event is $P((A \\cap C) \\cap B) = P(\\{HHH\\}) = \\frac{1}{8}$.\n\nUsing the definition of conditional probability:\n$$P(A \\cap C | B) = \\frac{P((A \\cap C) \\cap B)}{P(B)} = \\frac{1/8}{1/2} = \\frac{1}{4}$$\n\n**Step 2: Calculate $P(A|B)$**\nFirst, find the intersection of $A$ and $B$:\n$$A \\cap B = \\{HHH, HHT, HTH, HTT\\} \\cap \\{HHH, HHT, THH, THT\\} = \\{HHH, HHT\\}$$\nThe probability is $P(A \\cap B) = \\frac{2}{8} = \\frac{1}{4}$.\n\nNow, calculate the conditional probability:\n$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{1/4}{1/2} = \\frac{1}{2}$$\n\n**Step 3: Calculate $P(C|B)$**\nFirst, find the intersection of $C$ and $B$:\n$$C \\cap B = \\{HHH, HTH, THH, TTH\\} \\cap \\{HHH, HHT, THH, THT\\} = \\{HHH, THH\\}$$\nThe probability is $P(C \\cap B) = \\frac{2}{8} = \\frac{1}{4}$.\n\nNow, calculate the conditional probability:\n$$P(C|B) = \\frac{P(C \\cap B)}{P(B)} = \\frac{1/4}{1/2} = \\frac{1}{2}$$\n\n**Step 4: Calculate $\\Delta$**\nNow we substitute the calculated values back into the expression for $\\Delta$:\n$$\\Delta = P(A \\cap C | B) - P(A|B)P(C|B)$$\n$$\\Delta = \\frac{1}{4} - \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)$$\n$$\\Delta = \\frac{1}{4} - \\frac{1}{4}$$\n$$\\Delta = 0$$\n\nThe result $\\Delta=0$ demonstrates that events $A$ and $C$ are conditionally independent given event $B$.", "answer": "$$\\boxed{0}$$", "id": "2850"}, {"introduction": "Having seen how conditioning can confirm independence, we now turn to a classic problem that delightfully challenges our intuition. We often assume that events which are independent on their own will remain so under new information, but this is not always true. This exercise [@problem_id:2840] uses two fair dice to demonstrate how two initially independent events—the outcomes of each die—can become dependent once we are given information about a common consequence, in this case, their sum. Mastering this \"explaining away\" phenomenon is essential for avoiding common fallacies in statistical reasoning and for building accurate probabilistic models.", "problem": "Consider an experiment involving the rolling of two fair, six-sided dice. Let the outcome of a roll be represented by an ordered pair $(d_1, d_2)$, where $d_1$ is the result of the first die and $d_2$ is the result of the second die. The sample space of this experiment consists of 36 equally likely outcomes.\n\nWe define the following events:\n-   **A**: The event that the first die shows an odd number ($d_1 \\in \\{1, 3, 5\\}$).\n-   **B**: The event that the second die shows an odd number ($d_2 \\in \\{1, 3, 5\\}$).\n-   **C**: The event that the sum of the outcomes of the two dice is equal to 6 ($d_1 + d_2 = 6$).\n\nTwo events, $E_1$ and $E_2$, are defined as conditionally independent given a third event, $G$, if the following condition holds:\n$$P(E_1 \\cap E_2 | G) = P(E_1 | G) P(E_2 | G)$$\nYour task is to determine if events A and B are conditionally independent given event C. To do this, calculate the value of the difference, $D$, defined as:\n$$D = P(A \\cap B | C) - P(A | C) P(B | C)$$\nA value of $D=0$ would imply conditional independence.", "solution": "Let Ω be the 36 outcomes of two fair dice.  Since \n$$C=\\{(d_1,d_2):d_1+d_2=6\\}$$ \ncontains the 5 outcomes $(1,5),(2,4),(3,3),(4,2),(5,1)$, we have\n$$P(C)=\\frac{5}{36}\\,. $$\n\n1.  Compute $P(A\\cap B\\cap C)$.  The intersection $A\\cap B\\cap C$ requires $d_1$ odd, $d_2$ odd, and $d_1+d_2=6$, which holds for $(1,5),(3,3),(5,1)$, so\n$$P(A\\cap B\\cap C)=\\frac{3}{36}=\\frac{1}{12}\\,. $$\nHence\n$$P(A\\cap B\\mid C)=\\frac{P(A\\cap B\\cap C)}{P(C)}\n=\\frac{\\tfrac{1}{12}}{\\tfrac{5}{36}}\n=\\frac{1}{12}\\,\\frac{36}{5}\n=\\frac{3}{5}\\,. $$\n\n2.  Compute $P(A\\mid C)$.  Within $C$ the first die is odd in the same 3 outcomes, so\n$$P(A\\cap C)=\\frac{3}{36}=\\frac{1}{12},\n\\quad\nP(A\\mid C)=\\frac{\\tfrac{1}{12}}{\\tfrac{5}{36}}\n=\\frac{3}{5}\\,. $$\n\n3.  Compute $P(B\\mid C)$.  Within $C$ the second die is odd in the 3 outcomes $(1,5),(3,3),(5,1)$, so\n$$P(B\\mid C)=\\frac{\\tfrac{3}{36}}{\\tfrac{5}{36}}\n=\\frac{3}{5}\\,. $$\n\n4.  Finally,\n$$D=P(A\\cap B\\mid C)-P(A\\mid C)\\,P(B\\mid C)\n=\\frac{3}{5}-\\frac{3}{5}\\cdot\\frac{3}{5}\n=\\frac{3}{5}-\\frac{9}{25}\n=\\frac{6}{25}\\,. $$", "answer": "$$\\boxed{\\frac{6}{25}}$$", "id": "2840"}, {"introduction": "Finally, we elevate our understanding by applying the principles of conditional independence to model a dynamic, real-world system. This problem [@problem_id:1351023] introduces the Poisson process, a fundamental tool for modeling events that occur randomly in time or space, such as photon emissions or website traffic. You will investigate a profound and elegant property of this process: given a total count of events $n$ over a time period $T$, what is the probability distribution of the count $k$ within a shorter sub-interval of duration $\\tau$? Solving this will not only reveal a surprising connection to the binomial distribution but also equip you with a powerful technique for analyzing data from random processes.", "problem": "A research team is studying the spontaneous emission of photons from a quantum dot under continuous excitation. The emission events are modeled as a homogeneous Poisson process with a constant average rate of $\\lambda$ emissions per unit time. The team monitors the emissions over a total time period of length $T$. This period is divided into two consecutive, non-overlapping intervals: a first interval $I_1 = [0, \\tau)$ of duration $\\tau$, and a second interval $I_2 = [\\tau, T)$ of duration $T-\\tau$, where $0 < \\tau < T$.\n\nLet $N_1$ be the random variable for the number of photons detected in interval $I_1$, and $N_2$ be the random variable for the number of photons detected in interval $I_2$. An experimental run reveals that a total of $n$ photons were detected over the entire period $[0, T)$.\n\nGiven this observation that exactly $n$ photons were detected in total, determine the conditional probability that exactly $k$ of these photons were detected in the first interval, $I_1$. You may assume that $k$ is an integer such that $0 \\le k \\le n$. Express your final answer as a closed-form expression in terms of $k$, $n$, $\\tau$, and $T$.", "solution": "We model the emissions as a homogeneous Poisson process with rate $\\lambda$. By the independent increments property, the counts in disjoint intervals are independent Poisson random variables with parameters equal to the rate times the interval length. Therefore,\n$$\nN_{1} \\sim \\text{Poisson}(\\lambda \\tau), \\quad N_{2} \\sim \\text{Poisson}(\\lambda (T-\\tau)),\n$$\nand $N_{1}$ and $N_{2}$ are independent. The total count $N_{1}+N_{2}$ over $[0,T)$ is then $\\text{Poisson}(\\lambda T)$.\n\nWe seek $P(N_{1}=k \\mid N_{1}+N_{2}=n)$ for $0 \\le k \\le n$. Using the definition of conditional probability and independence,\n$$\nP(N_{1}=k \\mid N_{1}+N_{2}=n) = \\frac{P(N_{1}=k,\\,N_{2}=n-k)}{P(N_{1}+N_{2}=n)}.\n$$\nThe numerator factorizes as\n$$\nP(N_{1}=k)P(N_{2}=n-k) = \\left[\\exp(-\\lambda \\tau)\\frac{(\\lambda \\tau)^{k}}{k!}\\right]\\left[\\exp(-\\lambda (T-\\tau))\\frac{(\\lambda (T-\\tau))^{n-k}}{(n-k)!}\\right]\n= \\exp(-\\lambda T)\\frac{\\lambda^{n}\\tau^{k}(T-\\tau)^{n-k}}{k!(n-k)!}.\n$$\nThe denominator is the $\\text{Poisson}(\\lambda T)$ probability mass at $n$:\n$$\nP(N_{1}+N_{2}=n) = \\exp(-\\lambda T)\\frac{(\\lambda T)^{n}}{n!}.\n$$\nTaking the ratio gives\n$$\nP(N_{1}=k \\mid N_{1}+N_{2}=n) = \\frac{\\exp(-\\lambda T)\\frac{\\lambda^{n}\\tau^{k}(T-\\tau)^{n-k}}{k!(n-k)!}}{\\exp(-\\lambda T)\\frac{(\\lambda T)^{n}}{n!}}\n= \\frac{n!}{k!(n-k)!}\\left(\\frac{\\tau}{T}\\right)^{k}\\left(\\frac{T-\\tau}{T}\\right)^{n-k}.\n$$\nRecognizing the binomial form, this is\n$$\nP(N_{1}=k \\mid N_{1}+N_{2}=n) = \\binom{n}{k}\\left(\\frac{\\tau}{T}\\right)^{k}\\left(1-\\frac{\\tau}{T}\\right)^{n-k},\n$$\nvalid for integers $k$ with $0 \\le k \\le n$.", "answer": "$$\\boxed{\\binom{n}{k}\\left(\\frac{\\tau}{T}\\right)^{k}\\left(1-\\frac{\\tau}{T}\\right)^{n-k}}$$", "id": "1351023"}]}