{"hands_on_practices": [{"introduction": "Mastering Bayes' theorem involves moving from theory to application. This first practice problem provides a classic scenario to build your foundational skills. We will use the theorem to formally update our belief about a randomly selected die after observing a specific outcome, a common task in scientific inference where evidence guides our conclusions [@problem_id:345].", "problem": "A bag contains $N$ six-sided dice, which are visually indistinguishable. One of these dice is \"loaded,\" while the other $N-1$ dice are \"fair.\"\n\nThe probability of rolling a six with a fair die is $p_F$. The probability of rolling a six with the loaded die is $p_L$, where $p_L > p_F$.\n\nAn experiment is conducted as follows: a single die is chosen at random from the bag, with each die having an equal probability of being selected. The chosen die is then rolled once. The outcome of the roll is observed to be a six.\n\nDerive an expression for the posterior probability that the die chosen from the bag was the loaded one, given this observation. Express your answer in terms of $N$, $p_L$, and $p_F$.", "solution": "Let $L$ be the event “chosen die is loaded” and $F$ the event “chosen die is fair,” and let $S$ be the event “roll result is six.”  We have\n$$P(L)=\\frac1N,\\quad P(F)=\\frac{N-1}N,$$\n$$P(S\\mid L)=p_L,\\quad P(S\\mid F)=p_F.$$\nBy Bayes’ theorem,\n$$\nP(L\\mid S)\n=\\frac{P(S\\mid L)\\,P(L)}{P(S)}\n=\\frac{p_L\\,(1/N)}{P(S\\mid L)\\,(1/N)+P(S\\mid F)\\,((N-1)/N)}\n=\\frac{p_L/N}{(p_L+(N-1)p_F)/N}\n=\\frac{p_L}{p_L+(N-1)p_F}.\n$$", "answer": "$$\\boxed{\\frac{p_L}{p_L + (N-1)p_F}}$$", "id": "345"}, {"introduction": "Probability theory is famous for its paradoxes—problems where intuition can easily lead us astray. This variation on Bertrand's Box is a classic example, and this exercise challenges you to navigate it using the rigor of Bayes' theorem. By carefully defining the events after observing a gold coin, you will see how Bayesian reasoning provides a clear path to the correct, albeit counter-intuitive, answer [@problem_id:691205].", "problem": "Consider a variation of the classic Bertrand's Box Paradox. There are three boxes, indistinguishable from the outside.\n- Box 1 contains two gold coins (GG).\n- Box 2 contains two silver coins (SS).\n- Box 3 contains one gold and one silver coin (GS).\n\nUnlike the classic paradox, the boxes are not chosen with equal probability. The prior probability of selecting the GG box is $p_G$, of selecting the SS box is $p_S$, and of selecting the GS box is $p_M$. These probabilities are known constants, greater than zero, and sum to one: $p_G + p_S + p_M = 1$.\n\nAn experiment is conducted where a box is chosen according to these prior probabilities, and then a single coin is drawn uniformly at random from the selected box. Upon inspection, the drawn coin is found to be gold.\n\nDerive a closed-form analytic expression for the posterior probability that the *other* coin in the chosen box is also gold, given the observation that the first drawn coin is gold. Express your answer in terms of the prior probabilities $p_G$ and $p_M$.", "solution": "We denote the events $B_{GG},B_{GS},B_{SS}$ for choosing the respective boxes, and $D$ for drawing a gold coin. The priors and conditional probabilities are\n$$P(B_{GG})=p_G,\\quad P(B_{GS})=p_M,\\quad P(B_{SS})=p_S,$$\n$$P(D\\mid B_{GG})=1,\\quad P(D\\mid B_{GS})=\\frac{1}{2},\\quad P(D\\mid B_{SS})=0.$$\n\nThe total probability of drawing gold is\n$$P(D)=p_G\\cdot1 + p_M\\cdot\\frac{1}{2} + p_S\\cdot0 = p_G + \\frac{1}{2}p_M.$$\n\nBy Bayes' theorem,\n$$P(B_{GG}\\mid D)=\\frac{P(D\\mid B_{GG})\\,P(B_{GG})}{P(D)} = \\frac{p_G}{p_G + \\frac{1}{2}p_M}.$$\n\nSince only the $GG$ box yields the other coin gold, the desired posterior probability is\n$$P(\\text{other coin is gold}\\mid D)=P(B_{GG}\\mid D)=\\frac{p_G}{p_G + \\frac{1}{2}p_M}.$$", "answer": "$$\\boxed{\\frac{p_G}{p_G + \\frac{1}{2}p_M}}$$", "id": "691205"}, {"introduction": "The most sophisticated applications of Bayes' theorem involve handling subtle or partial information. This problem, a nuanced version of the classic prisoner's dilemma, illustrates that the *process* by which we receive information can be as significant as the information itself. Your task is to calculate how a prisoner should update his belief in being pardoned, considering not just what the warden said, but the specific protocol the warden followed in saying it [@problem_id:691467].", "problem": "In a high-security prison, there are $N$ inmates labeled $1, 2, \\ldots, N$, where $N \\ge 3$. The governor has decided to pardon exactly one prisoner, selected uniformly at random. Thus, each prisoner has a prior probability of $1/N$ of being pardoned.\n\nAn inquisitive prisoner, Prisoner 1, asks a warden, who knows the identity of the pardoned individual, to name a single prisoner from the set $\\{2, 3, \\ldots, N\\}$ who will definitely be executed. The warden is truthful and must answer.\n\nThe warden's a priori protocol for choosing which prisoner to name is dependent on who is pardoned:\n1.  If Prisoner 1 is the one to be pardoned, the warden's choice is dictated by a personal bias. He names Prisoner 2 with probability $\\alpha$, where $0 < \\alpha < 1$. The remaining probability, $1-\\alpha$, is distributed uniformly among the prisoners in the set $\\{3, 4, \\ldots, N\\}$.\n2.  If a prisoner $k \\in \\{2, 3, \\ldots, N\\}$ is the one to be pardoned, the warden is forbidden from naming prisoner $k$. He must choose a prisoner to name from the set of other candidates $\\{2, 3, \\ldots, N\\} \\setminus \\{k\\}$. The warden makes this choice uniformly at random from this set of $N-2$ available prisoners.\n\nAfter Prisoner 1 makes his request, the warden states: \"Prisoner 2 will be executed.\"\n\nDerive a symbolic expression for the posterior probability that Prisoner 1 is the one to be pardoned, given the warden's statement. Your answer should be in terms of the bias parameter $\\alpha$.", "solution": "We seek $P(\\text{pardoned}=1\\mid\\text{warden says “2”})$.  By Bayes’ theorem,\n$$P(1\\mid2)=\\frac{P(2\\mid1)\\,P(1)}{P(2)}\\,. $$\n\n1. Prior: $P(1)=1/N$.\n2. Likelihood if 1 is pardoned: $P(2\\mid1)=\\alpha$.\n3. For $k\\in\\{2,\\dots,N\\}$, $P(k)=1/N$.  \n   – If $k=2$, $P(2\\mid2)=0$.  \n   – If $k\\ge3$, the warden chooses among $N-2$ uniformly, so $P(2\\mid k)=1/(N-2)$.\n4. Total probability\n$$P(2)=P(2\\mid1)P(1)+\\sum_{k=2}^N P(2\\mid k)P(k)\n=\\frac{\\alpha}{N}+\\sum_{k=3}^N\\frac{1}{N}\\frac{1}{N-2}\n=\\frac{\\alpha+1}{N}\\,. $$\n5. Hence\n$$P(1\\mid2)\n=\\frac{\\frac{\\alpha}{N}}{\\frac{\\alpha+1}{N}}\n=\\frac{\\alpha}{\\alpha+1}\\,. $$", "answer": "$$\\boxed{\\frac{\\alpha}{\\alpha+1}}$$", "id": "691467"}]}