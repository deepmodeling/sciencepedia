{"hands_on_practices": [{"introduction": "We begin our hands-on exploration of the matching problem with a classic and foundational question. By analyzing the scenario of randomly distributing exams to students, we can uncover a surprisingly simple and constant answer for the average number of 'matches'. This practice [@problem_id:1401448] is designed to introduce the powerful method of indicator variables and the linearity of expectation, tools that allow us to solve complex problems by breaking them into simpler, manageable parts.", "problem": "In a large lecture course with 50 students, a professor finishes grading the final exams. The professor has a stack of 50 graded exams, each with a student's name on it. Instead of calling students up one by one, the professor decides to distribute them randomly. The stack of exams is thoroughly shuffled, and then handed out to the 50 students, who are seated in a fixed order. Each student receives exactly one exam.\n\nWhat is the expected number of students who receive their own exam?", "solution": "Let there be $n=50$ students and exams. Model the random distribution as a uniformly random permutation of the exams among the students. For each student $i \\in \\{1,2,\\dots,50\\}$, define the indicator random variable\n$$\nX_{i}=\\begin{cases}\n1, & \\text{if student } i \\text{ receives their own exam},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe total number of students who receive their own exam is\n$$\nX=\\sum_{i=1}^{50} X_{i}.\n$$\nWe compute the expectation using linearity of expectation:\n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{50} \\mathbb{E}[X_{i}].\n$$\nFor each $i$, by symmetry of the random permutation, student $i$ is equally likely to receive any one of the $50$ exams, so\n$$\n\\mathbb{P}(X_{i}=1)=\\frac{1}{50}.\n$$\nHence\n$$\n\\mathbb{E}[X_{i}]=1 \\cdot \\mathbb{P}(X_{i}=1)+0 \\cdot \\mathbb{P}(X_{i}=0)=\\frac{1}{50}.\n$$\nTherefore,\n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{50} \\frac{1}{50}=50 \\cdot \\frac{1}{50}=1.\n$$\nNo independence assumptions are required for the use of linearity of expectation.", "answer": "$$\\boxed{1}$$", "id": "1401448"}, {"introduction": "Building upon the basics, we now introduce a twist: we are given partial information about the outcome. This exercise [@problem_id:1401491] challenges you to apply the rules of conditional probability to the matching problem. To solve it, you will need to master the principle of inclusion-exclusion, a fundamental combinatorial technique for counting the size of unions of sets, which is essential for calculating the probability of complex events like 'at least one match'.", "problem": "A university proctor has just finished grading the final exams for a class of $n$ students. The proctor shuffles the pile of $n$ exams thoroughly and begins to hand them back to the students, who are lined up in a fixed order from 1 to $n$. We can assume that any permutation of the exams is equally likely.\n\nAn observer is told a piece of information: among the first $m$ students in the line, where $m$ is an integer such that $1 \\le m \\le n$, at least one student received their own exam paper.\n\nGiven this information, find the probability that the very first student in the line received their own exam. Express your answer as a closed-form analytic expression in terms of $n$ and $m$.", "solution": "Let $A$ be the event that student $1$ receives their own exam, and let $B$ be the event that among the first $m$ students, at least one receives their own exam. Since $1 \\leq m \\leq n$, student $1$ is among the first $m$ students, so $A \\subseteq B$. Therefore,\n$$\n\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A)}{\\mathbb{P}(B)}.\n$$\nFor a uniformly random permutation, $\\mathbb{P}(A) = \\frac{1}{n}$.\n\nIt remains to compute $\\mathbb{P}(B)$. Let $S=\\{1,2,\\ldots,m\\}$. The complement $B^{c}$ is the event that no student in $S$ receives their own exam. By inclusion-exclusion, the number of permutations of $\\{1,\\ldots,n\\}$ with no fixed points in $S$ is\n$$\n\\sum_{k=0}^{m} (-1)^{k} \\binom{m}{k} (n-k)!,\n$$\nsince fixing a specific set of $k$ students in $S$ leaves $(n-k)!$ permutations of the remaining elements. Dividing by $n!$ gives\n$$\n\\mathbb{P}(B^{c}) = \\frac{1}{n!} \\sum_{k=0}^{m} (-1)^{k} \\binom{m}{k} (n-k)!,\n$$\nhence\n$$\n\\mathbb{P}(B) = 1 - \\frac{1}{n!} \\sum_{k=0}^{m} (-1)^{k} \\binom{m}{k} (n-k)! = \\sum_{k=1}^{m} (-1)^{k+1} \\binom{m}{k} \\frac{(n-k)!}{n!}.\n$$\nTherefore,\n$$\n\\mathbb{P}(A \\mid B) = \\frac{\\frac{1}{n}}{\\sum_{k=1}^{m} (-1)^{k+1} \\binom{m}{k} \\frac{(n-k)!}{n!}} = \\frac{(n-1)!}{\\sum_{k=1}^{m} (-1)^{k+1} \\binom{m}{k} (n-k)!}.\n$$\nThis is a closed-form analytic expression in terms of $n$ and $m$.", "answer": "$$\\boxed{\\frac{(n-1)!}{\\sum_{k=1}^{m} (-1)^{k+1} \\binom{m}{k} (n-k)!}}$$", "id": "1401491"}, {"introduction": "Our final practice moves from calculating expectations and probabilities to measuring the relationship between different events. In this problem [@problem_id:1401440], we investigate whether a match occurring in one group of items makes a match in a different, disjoint group more or less likely. You will calculate the covariance between the number of matches in two separate zones, deepening your understanding of how indicator variables can be used to analyze the dependence structure between random variables.", "problem": "In a large-scale automated warehouse, there are $n$ distinct items and $n$ corresponding destination bins, both labeled from 1 to $n$. Due to a systemic glitch, the sorting system assigns the $n$ items to the $n$ bins according to a single permutation chosen uniformly at random from all possible $n!$ permutations. An item is considered \"correctly placed\" if item $i$ is assigned to bin $i$.\n\nTwo separate quality control zones are established to monitor the sorting accuracy. Zone $A$ consists of a set of $k$ distinct bins, and Zone $B$ consists of a set of $m$ distinct bins. The sets of bins for Zone $A$ and Zone $B$ are disjoint.\n\nLet $X_A$ be the random variable representing the number of items correctly placed in their designated bins within Zone $A$. Similarly, let $X_B$ be the random variable for the number of items correctly placed within Zone $B$. Assuming $n \\ge 2$, find a general expression for the covariance between $X_A$ and $X_B$, denoted $\\operatorname{Cov}(X_A, X_B)$, in terms of $n$, $k$, and $m$.", "solution": "The problem asks for the covariance between the number of fixed points in two disjoint subsets of indices under a random permutation. Let the set of all bin indices be $S = \\{1, 2, \\dots, n\\}$. Let the set of bin indices for Zone $A$ be $A \\subset S$ with $|A|=k$, and for Zone $B$ be $B \\subset S$ with $|B|=m$. We are given that $A \\cap B = \\emptyset$.\n\nThe random variables are $X_A$, the number of correctly placed items in Zone $A$, and $X_B$, the number of correctly placed items in Zone $B$. A permutation is chosen uniformly at random from the set of all $n!$ permutations of $S$.\n\nTo solve this, we will use the method of indicator variables. For each index $i \\in S$, let $I_i$ be an indicator random variable defined as:\n$$\nI_i = \\begin{cases} 1 & \\text{if item } i \\text{ is in bin } i \\text{ (a fixed point)} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nUsing these indicators, we can express $X_A$ and $X_B$ as sums:\n$$\nX_A = \\sum_{i \\in A} I_i \\quad \\text{and} \\quad X_B = \\sum_{j \\in B} I_j\n$$\nThe covariance is defined as $\\operatorname{Cov}(X_A, X_B) = \\mathbb{E}[X_A X_B] - \\mathbb{E}[X_A] \\mathbb{E}[X_B]$. We will compute each term separately.\n\nFirst, let's find the expectation of an indicator variable $I_i$. The expectation of an indicator variable is the probability of the event it indicates.\n$$\n\\mathbb{E}[I_i] = \\mathbb{P}(I_i = 1)\n$$\nThe event $I_i=1$ means that the random permutation $\\pi$ has a fixed point at $i$, i.e., $\\pi(i)=i$. The total number of permutations is $n!$. If we fix $\\pi(i)=i$, the remaining $n-1$ items can be permuted in $(n-1)!$ ways. Therefore,\n$$\n\\mathbb{E}[I_i] = \\mathbb{P}(\\pi(i)=i) = \\frac{(n-1)!}{n!} = \\frac{1}{n}\n$$\nThis holds for any $i \\in S$.\n\nNow, we can compute $\\mathbb{E}[X_A]$ and $\\mathbb{E}[X_B]$ using the linearity of expectation:\n$$\n\\mathbb{E}[X_A] = \\mathbb{E}\\left[\\sum_{i \\in A} I_i\\right] = \\sum_{i \\in A} \\mathbb{E}[I_i] = \\sum_{i \\in A} \\frac{1}{n} = \\frac{|A|}{n} = \\frac{k}{n}\n$$\n$$\n\\mathbb{E}[X_B] = \\mathbb{E}\\left[\\sum_{j \\in B} I_j\\right] = \\sum_{j \\in B} \\mathbb{E}[I_j] = \\sum_{j \\in B} \\frac{1}{n} = \\frac{|B|}{n} = \\frac{m}{n}\n$$\nThus, the product of the expectations is:\n$$\n\\mathbb{E}[X_A]\\mathbb{E}[X_B] = \\frac{k}{n} \\cdot \\frac{m}{n} = \\frac{km}{n^2}\n$$\nNext, we compute $\\mathbb{E}[X_A X_B]$.\n$$\n\\mathbb{E}[X_A X_B] = \\mathbb{E}\\left[\\left(\\sum_{i \\in A} I_i\\right) \\left(\\sum_{j \\in B} I_j\\right)\\right] = \\mathbb{E}\\left[\\sum_{i \\in A} \\sum_{j \\in B} I_i I_j\\right]\n$$\nBy linearity of expectation, we can move the expectation inside the sums:\n$$\n\\mathbb{E}[X_A X_B] = \\sum_{i \\in A} \\sum_{j \\in B} \\mathbb{E}[I_i I_j]\n$$\nThe term $\\mathbb{E}[I_i I_j]$ is the expectation of the product of two indicator variables. This is equal to the probability that both events occur:\n$$\n\\mathbb{E}[I_i I_j] = \\mathbb{P}(I_i=1 \\text{ and } I_j=1) = \\mathbb{P}(\\pi(i)=i \\text{ and } \\pi(j)=j)\n$$\nSince $i \\in A$ and $j \\in B$, and the sets $A$ and $B$ are disjoint, we know that $i \\neq j$. To count the number of permutations where both $\\pi(i)=i$ and $\\pi(j)=j$, we fix these two mappings. The remaining $n-2$ items can be permuted in $(n-2)!$ ways.\nTherefore, for $i \\neq j$:\n$$\n\\mathbb{E}[I_i I_j] = \\mathbb{P}(\\pi(i)=i, \\pi(j)=j) = \\frac{(n-2)!}{n!} = \\frac{1}{n(n-1)}\n$$\nThe double summation $\\sum_{i \\in A} \\sum_{j \\in B}$ has $|A| \\times |B| = km$ terms. For each term, the expectation $\\mathbb{E}[I_i I_j]$ is the same, $\\frac{1}{n(n-1)}$.\nSo,\n$$\n\\mathbb{E}[X_A X_B] = \\sum_{i \\in A} \\sum_{j \\in B} \\frac{1}{n(n-1)} = km \\cdot \\frac{1}{n(n-1)} = \\frac{km}{n(n-1)}\n$$\nFinally, we can calculate the covariance:\n$$\n\\operatorname{Cov}(X_A, X_B) = \\mathbb{E}[X_A X_B] - \\mathbb{E}[X_A] \\mathbb{E}[X_B]\n$$\n$$\n\\operatorname{Cov}(X_A, X_B) = \\frac{km}{n(n-1)} - \\frac{km}{n^2}\n$$\nTo simplify, we find a common denominator:\n$$\n\\operatorname{Cov}(X_A, X_B) = km \\left( \\frac{1}{n(n-1)} - \\frac{1}{n^2} \\right) = km \\left( \\frac{n}{n^2(n-1)} - \\frac{n-1}{n^2(n-1)} \\right)\n$$\n$$\n\\operatorname{Cov}(X_A, X_B) = km \\left( \\frac{n - (n-1)}{n^2(n-1)} \\right) = km \\left( \\frac{1}{n^2(n-1)} \\right)\n$$\nThis gives the final expression for the covariance:\n$$\n\\operatorname{Cov}(X_A, X_B) = \\frac{km}{n^2(n-1)}\n$$", "answer": "$$ \\boxed{\\frac{km}{n^{2}(n-1)}} $$", "id": "1401440"}]}