{"hands_on_practices": [{"introduction": "This first exercise provides a foundational look at how conditioning on an event directly impacts our calculations. Here, we consider the maximum of two random variables, but with the specific knowledge that their minimum value exceeds a certain threshold. This practice [@problem_id:1350718] is an excellent way to solidify your understanding of how a conditioning event effectively redefines the sample space, leading to a new, updated expectation.", "problem": "Consider two independent and identically distributed (i.i.d.) random variables, $X_1$ and $X_2$. Each variable follows a discrete uniform distribution on the set of integers $\\{1, 2, \\dots, M\\}$, where $M$ is an integer greater than 2. Let $Y = \\max(X_1, X_2)$ be the maximum of these two variables, and $Z = \\min(X_1, X_2)$ be the minimum.\n\nGiven that the minimum value $Z$ is strictly greater than an integer $k$, where $1 \\le k < M-1$, determine a closed-form expression for the conditional expectation of the maximum value, $\\mathbb{E}[Y | Z > k]$. Your final expression should be in terms of $M$ and $k$.", "solution": "The problem asks for the conditional expectation $\\mathbb{E}[Y | Z > k]$, where $Y = \\max(X_1, X_2)$, $Z = \\min(X_1, X_2)$, and $X_1, X_2$ are i.i.d. random variables uniformly distributed on $\\{1, 2, \\dots, M\\}$.\n\nThe condition $Z > k$ means that $\\min(X_1, X_2) > k$. This is equivalent to the event that both $X_1 > k$ and $X_2 > k$. Given this condition, the sample space for each random variable is restricted to the set of integers $\\{k+1, k+2, \\dots, M\\}$. The number of possible outcomes in this set is $M-k$.\n\nLet's define new random variables, $X_1^*$ and $X_2^*$, representing the values of $X_1$ and $X_2$ under the condition $Z > k$. These conditional variables are independent and uniformly distributed on the set $\\{k+1, k+2, \\dots, M\\}$. The problem is thus equivalent to finding the expectation of the maximum of these two new variables, i.e., $\\mathbb{E}[\\max(X_1^*, X_2^*)]$.\n\nTo simplify the calculation, we can perform a linear transformation. Let's define another set of variables $X'_i = X_i^* - k$ for $i=1,2$. These variables $X'_1$ and $X'_2$ will be i.i.d. and will follow a discrete uniform distribution on the set $\\{1, 2, \\dots, M-k\\}$. Let's denote the size of this new sample space by $M' = M-k$. So, $X'_i \\sim U\\{1, 2, \\dots, M'\\}$.\n\nThe maximum of the conditional variables is related to the maximum of the transformed variables:\n$\\max(X_1^*, X_2^*) = \\max(X'_1+k, X'_2+k) = \\max(X'_1, X'_2) + k$.\nLet $Y' = \\max(X'_1, X'_2)$. By the linearity of expectation, we have:\n$\\mathbb{E}[Y | Z > k] = \\mathbb{E}[\\max(X_1^*, X_2^*)] = \\mathbb{E}[Y' + k] = \\mathbb{E}[Y'] + k$.\n\nOur task now is to compute $\\mathbb{E}[Y']$, the expected maximum of two i.i.d. variables drawn from $U\\{1, 2, \\dots, M'\\}$.\nFor a non-negative integer-valued random variable, the expectation can be calculated using the formula:\n$\\mathbb{E}[Y'] = \\sum_{j=0}^{\\infty} P(Y' > j)$.\nSince the maximum possible value for $Y'$ is $M'$, this sum becomes:\n$\\mathbb{E}[Y'] = \\sum_{j=0}^{M'-1} P(Y' > j)$.\n\nWe can express $P(Y' > j)$ as $1 - P(Y' \\le j)$. The term $P(Y' \\le j)$ is the cumulative distribution function (CDF) of $Y'$, let's call it $F_{Y'}(j)$.\n$F_{Y'}(j) = P(Y' \\le j) = P(\\max(X'_1, X'_2) \\le j)$.\nThis is equivalent to $P(X'_1 \\le j \\text{ and } X'_2 \\le j)$. Since $X'_1$ and $X'_2$ are independent:\n$F_{Y'}(j) = P(X'_1 \\le j) \\cdot P(X'_2 \\le j)$.\n\nFor a discrete uniform variable $X'_i$ on $\\{1, \\dots, M'\\}$, the probability $P(X'_i \\le j)$ for an integer $j$ in the range $1 \\le j \\le M'$ is $\\frac{j}{M'}$.\nThus, the CDF of $Y'$ is $F_{Y'}(j) = \\left(\\frac{j}{M'}\\right)^2$ for $j \\in \\{1, \\dots, M'\\}$. Note that $F_{Y'}(0) = 0$.\n\nNow we can compute the expectation:\n$\\mathbb{E}[Y'] = \\sum_{j=0}^{M'-1} (1 - F_{Y'}(j)) = \\sum_{j=0}^{M'-1} \\left(1 - \\left(\\frac{j}{M'}\\right)^2\\right)$.\nWe can split the sum:\n$\\mathbb{E}[Y'] = \\sum_{j=0}^{M'-1} 1 - \\sum_{j=0}^{M'-1} \\left(\\frac{j}{M'}\\right)^2 = M' - \\frac{1}{(M')^2} \\sum_{j=0}^{M'-1} j^2$.\n\nThe sum of the first $N$ squares is given by the formula $\\sum_{j=1}^{N} j^2 = \\frac{N(N+1)(2N+1)}{6}$. We need the sum up to $M'-1$, so we set $N=M'-1$:\n$\\sum_{j=1}^{M'-1} j^2 = \\frac{(M'-1)((M'-1)+1)(2(M'-1)+1)}{6} = \\frac{(M'-1)M'(2M'-1)}{6}$.\nSince the $j=0$ term is zero, $\\sum_{j=0}^{M'-1} j^2 = \\sum_{j=1}^{M'-1} j^2$.\n\nSubstituting this back into the expression for $\\mathbb{E}[Y']$:\n$\\mathbb{E}[Y'] = M' - \\frac{1}{(M')^2} \\left( \\frac{(M'-1)M'(2M'-1)}{6} \\right)$.\n$\\mathbb{E}[Y'] = M' - \\frac{(M'-1)(2M'-1)}{6M'}$.\n\nTo simplify, we find a common denominator:\n$\\mathbb{E}[Y'] = \\frac{6(M')^2 - (M'-1)(2M'-1)}{6M'} = \\frac{6(M')^2 - (2(M')^2 - 3M' + 1)}{6M'} = \\frac{4(M')^2 + 3M' - 1}{6M'}$.\nThe numerator can be factored as $(4M'-1)(M'+1)$.\nSo, $\\mathbb{E}[Y'] = \\frac{(4M'-1)(M'+1)}{6M'}$.\n\nFinally, we substitute this result back into our original equation for the conditional expectation, remembering that $M' = M-k$:\n$\\mathbb{E}[Y | Z > k] = \\mathbb{E}[Y'] + k = \\frac{(4M'-1)(M'+1)}{6M'} + k$.\n$\\mathbb{E}[Y | Z > k] = \\frac{(4(M-k)-1)((M-k)+1)}{6(M-k)} + k$.\n\nThis is the final closed-form expression for the conditional expectation.", "answer": "$$\\boxed{\\frac{(4(M-k)-1)(M-k+1)}{6(M-k)} + k}$$", "id": "1350718"}, {"introduction": "Moving to a more dynamic scenario, this problem explores a process that unfolds over a random number of steps. We need to find the expected total outcome, given that the process has survived beyond a certain point. This exercise [@problem_id:1350716] will guide you in applying the law of total expectation, a powerful tool also known as the tower property, $E[X] = E[E[X|Y]]$. You will also see how the memoryless property of the geometric distribution plays a crucial role in simplifying the calculation.", "problem": "In a simplified model of quantum tunneling, a particle makes a series of attempts to pass through a potential barrier. The total number of attempts, $N$, before the particle successfully tunnels through is a random variable that follows a geometric distribution with probability mass function $P(N=n) = (1-p)^{n-1}p$ for $n = 1, 2, 3, \\dots$, where $p$ is the probability of successful tunneling on any single attempt.\n\nIn each attempt $i$ (for $i=1, 2, \\dots, N$), the particle emits a number of \"probe\" photons, $X_i$. The numbers of photons emitted in different attempts are independent and identically distributed (i.i.d.) random variables, where each $X_i$ follows a Poisson distribution with mean $\\lambda$. The total number of photons emitted throughout the entire process is $S_N = \\sum_{i=1}^{N} X_i$.\n\nAn experiment is conducted, but due to a limitation in the early-stage detection system, we only know that the particle did not tunnel through in any of the first $k$ attempts. Specifically, we are given the condition that $N > k$.\n\nGiven this information, find the expected total number of probe photons, $\\mathbb{E}[S_N | N > k]$, that will have been emitted by the time the particle finally tunnels through. Express your answer as a single closed-form analytic expression in terms of $p$, $\\lambda$, and $k$.", "solution": "We are asked to compute the conditional expectation $\\mathbb{E}[S_{N} \\mid N>k]$, where $S_{N}=\\sum_{i=1}^{N}X_{i}$, the $X_{i}$ are i.i.d. Poisson with mean $\\lambda$, and $N$ is geometric with $P(N=n)=(1-p)^{n-1}p$ for $n\\in\\{1,2,\\dots\\}$. The $X_{i}$ are independent of $N$.\n\nBy the law of total expectation and independence of $\\{X_{i}\\}$ and $N$,\n$$\n\\mathbb{E}[S_{N} \\mid N>k]\n= \\mathbb{E}\\!\\left[\\,\\mathbb{E}\\!\\left[\\sum_{i=1}^{N}X_{i}\\,\\middle|\\,N,\\,N>k\\right] \\middle| N>k\\right]\n= \\mathbb{E}\\!\\left[\\,\\mathbb{E}\\!\\left[\\sum_{i=1}^{N}X_{i}\\,\\middle|\\,N\\right] \\middle| N>k\\right].\n$$\nGiven $N$, we have\n$$\n\\mathbb{E}\\!\\left[\\sum_{i=1}^{N}X_{i}\\,\\middle|\\,N\\right] = \\sum_{i=1}^{N}\\mathbb{E}[X_{i}] = N\\,\\lambda,\n$$\nso\n$$\n\\mathbb{E}[S_{N} \\mid N>k] = \\mathbb{E}[N\\lambda \\mid N>k] = \\lambda\\,\\mathbb{E}[N \\mid N>k].\n$$\n\nWe now compute $\\mathbb{E}[N \\mid N>k]$. For $n>k$,\n$$\nP(N=n \\mid N>k) = \\frac{P(N=n)}{P(N>k)} = \\frac{(1-p)^{n-1}p}{(1-p)^{k}} = (1-p)^{n-k-1}p.\n$$\nThus $N-k$ conditional on $N>k$ has the geometric distribution with parameter $p$ on $\\{1,2,\\dots\\}$. Hence\n$$\n\\mathbb{E}[N \\mid N>k] = k + \\mathbb{E}[N-k \\mid N>k] = k + \\frac{1}{p}.\n$$\nFor completeness, this can be verified by summation. Let $q=1-p$. Then\n$$\n\\mathbb{E}[N \\mid N>k] = \\sum_{n=k+1}^{\\infty} n\\, q^{\\,n-k-1} p\n= \\sum_{m=1}^{\\infty} (k+m)\\, q^{\\,m-1} p\n= k \\sum_{m=1}^{\\infty} q^{\\,m-1} p + \\sum_{m=1}^{\\infty} m\\, q^{\\,m-1} p.\n$$\nUsing the geometric series identities\n$$\n\\sum_{m=1}^{\\infty} q^{\\,m-1} p = p \\sum_{m=0}^{\\infty} q^{m} = \\frac{p}{1-q} = 1,\n$$\nand\n$$\n\\sum_{m=1}^{\\infty} m\\, q^{\\,m-1} p = p \\frac{\\mathrm{d}}{\\mathrm{d}q} \\left(\\sum_{m=1}^{\\infty} q^{m}\\right)\n= p \\frac{\\mathrm{d}}{\\mathrm{d}q} \\left(\\frac{q}{1-q}\\right)\n= \\frac{p}{(1-q)^{2}} = \\frac{1}{p},\n$$\nwe obtain $\\mathbb{E}[N \\mid N>k] = k + \\frac{1}{p}$.\n\nTherefore,\n$$\n\\mathbb{E}[S_{N} \\mid N>k] = \\lambda \\left(k + \\frac{1}{p}\\right).\n$$", "answer": "$$\\boxed{\\lambda\\left(k+\\frac{1}{p}\\right)}$$", "id": "1350716"}, {"introduction": "This final practice tackles a classic and practical question: how long must we wait for a specific pattern to appear in a sequence of random events? A direct approach can be surprisingly complex, but conditioning provides an elegant solution. This problem [@problem_id:1350734] introduces a powerful state-based reasoning method, where we condition on the progress made so far. By defining states based on how much of the target sequence has been matched, you will learn to construct and solve a system of linear equations to find the conditional expected waiting time.", "problem": "A fair coin, where heads (H) and tails (T) are equally likely, is flipped repeatedly. An observer is waiting for the first occurrence of the specific five-flip sequence `HTHTH`.\n\nSuppose that the first three flips in an experiment have already occurred, and the outcomes were, in order, H, T, and H. Calculate the expected number of *additional* flips required from this point onward to observe the full sequence `HTHTH` for the first time.", "solution": "Let the target sequence be $S = \\text{HTHTH}$. We can solve this problem by defining states corresponding to the length of the prefix of $S$ that we have most recently observed. Let $E_p$ be the expected number of additional flips required to see the full sequence $S$, given that the last flips have formed the prefix $p$ of $S$. The state with an empty prefix is denoted by $E_{\\emptyset}$. The problem asks for the expected number of additional flips given that we have already observed 'HTH', which corresponds to the value of $E_{\\text{HTH}}$.\n\nThe possible prefixes of $S$ are 'H', 'HT', 'HTH', and 'HTHT'. We define the following expectations:\n- $E_{\\emptyset}$: Expected additional flips needed, given the last sequence of flips is not a prefix of $S$.\n- $E_{\\text{H}}$: Expected additional flips needed, given the last flip was 'H'.\n- $E_{\\text{HT}}$: Expected additional flips needed, given the last two flips were 'HT'.\n- $E_{\\text{HTH}}$: Expected additional flips needed, given the last three flips were 'HTH'.\n- $E_{\\text{HTHT}}$: Expected additional flips needed, given the last four flips were 'HTHT'.\n\nWhen the full sequence 'HTHTH' is observed, the process stops, so the expected number of additional flips becomes 0.\n\nWe can set up a system of linear equations by considering the outcome of the next flip from each state. Each flip costs 1 trial. Since the coin is fair, $P(H) = P(T) = 1/2$.\n\n1.  From state $\\emptyset$: We flip the coin.\n    - If it's H (prob 1/2), we have now matched the prefix 'H'. The new state is 'H'.\n    - If it's T (prob 1/2), we have observed 'T', which is not a prefix of $S$. The current matched prefix remains empty.\n    So, $E_{\\emptyset} = 1 + \\frac{1}{2}E_{\\text{H}} + \\frac{1}{2}E_{\\emptyset} \\implies \\frac{1}{2}E_{\\emptyset} = 1 + \\frac{1}{2}E_{\\text{H}} \\implies E_{\\emptyset} = 2 + E_{\\text{H}}$.\n\n2.  From state 'H': We have 'H' and we flip again.\n    - If it's T (prob 1/2), we get 'HT', matching the prefix of length 2. The new state is 'HT'.\n    - If it's H (prob 1/2), we get 'HH'. The longest suffix of 'HH' that is a prefix of $S$ is 'H'. So we return to state 'H'.\n    So, $E_{\\text{H}} = 1 + \\frac{1}{2}E_{\\text{HT}} + \\frac{1}{2}E_{\\text{H}} \\implies \\frac{1}{2}E_{\\text{H}} = 1 + \\frac{1}{2}E_{\\text{HT}} \\implies E_{\\text{H}} = 2 + E_{\\text{HT}}$.\n\n3.  From state 'HT': We have 'HT' and we flip again.\n    - If it's H (prob 1/2), we get 'HTH', matching the prefix of length 3. The new state is 'HTH'.\n    - If it's T (prob 1/2), we get 'HTT'. The longest suffix of 'HTT' that is a prefix of $S$ is the empty string. We return to state $\\emptyset$.\n    So, $E_{\\text{HT}} = 1 + \\frac{1}{2}E_{\\text{HTH}} + \\frac{1}{2}E_{\\emptyset}$.\n\n4.  From state 'HTH': We have 'HTH' and we flip again. This is the state we start in.\n    - If it's T (prob 1/2), we get 'HTHT', matching the prefix of length 4. The new state is 'HTHT'.\n    - If it's H (prob 1/2), we get 'HTHH'. The longest suffix of 'HTHH' that is a prefix of $S$ is 'H'. We return to state 'H'.\n    So, $E_{\\text{HTH}} = 1 + \\frac{1}{2}E_{\\text{HTHT}} + \\frac{1}{2}E_{\\text{H}}$.\n\n5.  From state 'HTHT': We have 'HTHT' and we flip again.\n    - If it's H (prob 1/2), we get 'HTHTH', which is the complete sequence. The process stops. The expected additional flips are 0.\n    - If it's T (prob 1/2), we get 'HTHTT'. The longest suffix of 'HTHTT' that is a prefix of $S$ is the empty string. We return to state $\\emptyset$.\n    So, $E_{\\text{HTHT}} = 1 + \\frac{1}{2}(0) + \\frac{1}{2}E_{\\emptyset} \\implies E_{\\text{HTHT}} = 1 + \\frac{1}{2}E_{\\emptyset}$.\n\nWe have a system of five linear equations. The value we need to find is $E_{\\text{HTH}}$.\nLet's solve the system:\n(i) $E_{\\emptyset} = 2 + E_{\\text{H}}$\n(ii) $E_{\\text{H}} = 2 + E_{\\text{HT}}$\n(iii) $E_{\\text{HT}} = 1 + \\frac{1}{2}E_{\\text{HTH}} + \\frac{1}{2}E_{\\emptyset}$\n(iv) $E_{\\text{HTH}} = 1 + \\frac{1}{2}E_{\\text{HTHT}} + \\frac{1}{2}E_{\\text{H}}$\n(v) $E_{\\text{HTHT}} = 1 + \\frac{1}{2}E_{\\emptyset}$\n\nFrom (ii), we can write $E_{\\text{HT}} = E_{\\text{H}} - 2$.\nSubstitute (i) into (v):\n$E_{\\text{HTHT}} = 1 + \\frac{1}{2}(2 + E_{\\text{H}}) = 1 + 1 + \\frac{1}{2}E_{\\text{H}} = 2 + \\frac{1}{2}E_{\\text{H}}$.\n\nNow substitute these expressions into (iii) and (iv) to get a system for $E_{\\text{H}}$ and $E_{\\text{HTH}}$.\nSubstitute into (iii):\n$E_{\\text{HT}} = 1 + \\frac{1}{2}E_{\\text{HTH}} + \\frac{1}{2}E_{\\emptyset}$\n$E_{\\text{H}} - 2 = 1 + \\frac{1}{2}E_{\\text{HTH}} + \\frac{1}{2}(2 + E_{\\text{H}})$\n$E_{\\text{H}} - 2 = 1 + \\frac{1}{2}E_{\\text{HTH}} + 1 + \\frac{1}{2}E_{\\text{H}}$\n$E_{\\text{H}} - 2 = 2 + \\frac{1}{2}E_{\\text{HTH}} + \\frac{1}{2}E_{\\text{H}}$\nMultiplying by 2 to clear fractions:\n$2E_{\\text{H}} - 4 = 4 + E_{\\text{HTH}} + E_{\\text{H}}$\n$E_{\\text{H}} - 8 = E_{\\text{HTH}}$.\n\nSubstitute into (iv):\n$E_{\\text{HTH}} = 1 + \\frac{1}{2}E_{\\text{HTHT}} + \\frac{1}{2}E_{\\text{H}}$\n$E_{\\text{HTH}} = 1 + \\frac{1}{2}(2 + \\frac{1}{2}E_{\\text{H}}) + \\frac{1}{2}E_{\\text{H}}$\n$E_{\\text{HTH}} = 1 + 1 + \\frac{1}{4}E_{\\text{H}} + \\frac{1}{2}E_{\\text{H}}$\n$E_{\\text{HTH}} = 2 + \\frac{3}{4}E_{\\text{H}}$.\n\nNow we have two expressions for $E_{\\text{HTH}}$ in terms of $E_{\\text{H}}$:\n$E_{\\text{HTH}} = E_{\\text{H}} - 8$\n$E_{\\text{HTH}} = 2 + \\frac{3}{4}E_{\\text{H}}$\n\nEquating them:\n$E_{\\text{H}} - 8 = 2 + \\frac{3}{4}E_{\\text{H}}$\n$E_{\\text{H}} - \\frac{3}{4}E_{\\text{H}} = 2 + 8$\n$\\frac{1}{4}E_{\\text{H}} = 10 \\implies E_{\\text{H}} = 40$.\n\nFinally, we find the required value, $E_{\\text{HTH}}$:\n$E_{\\text{HTH}} = E_{\\text{H}} - 8 = 40 - 8 = 32$.\n\nSo, the expected number of additional flips required is 32.", "answer": "$$\\boxed{32}$$", "id": "1350734"}]}