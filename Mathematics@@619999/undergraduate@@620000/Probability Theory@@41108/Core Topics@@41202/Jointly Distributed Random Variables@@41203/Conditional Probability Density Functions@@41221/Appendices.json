{"hands_on_practices": [{"introduction": "This first practice exercise grounds our understanding in the fundamental definition of a conditional probability density function. By working with a simple joint polynomial distribution, you will step through the essential calculations: normalizing the joint PDF, deriving the required marginal PDF, and then combining these to find the conditional density [@problem_id:1351401]. This problem builds the core computational skills needed for all subsequent work with conditional distributions.", "problem": "Consider a system described by three continuous random variables $X$, $Y$, and $Z$. The joint probability density function (PDF) for these variables is given by\n$$\nf_{X,Y,Z}(x,y,z) = \\begin{cases} c(x+y+z) & \\text{if } 0 \\le x \\le 1, 0 \\le y \\le 1, 0 \\le z \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nwhere $c$ is a normalization constant.\n\nSuppose a measurement of the system reveals that the variable $X$ has the specific value $X = 1/2$. Your task is to determine the conditional joint probability density function of $Y$ and $Z$ given this information, denoted as $f_{Y,Z|X}(y,z|1/2)$. The domain for this conditional PDF is $0 \\le y \\le 1$ and $0 \\le z \\le 1$.\n\nPresent your answer as a single closed-form analytic expression in terms of $y$ and $z$.", "solution": "We first determine the normalization constant $c$ by enforcing that the joint PDF integrates to $1$ over the unit cube. Using\n$$\n\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{1} c(x+y+z)\\,dx\\,dy\\,dz = 1,\n$$\nwe compute\n$$\n\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{1} (x+y+z)\\,dx\\,dy\\,dz\n= \\int_{0}^{1}\\int_{0}^{1}\\left(\\int_{0}^{1} x\\,dx + \\int_{0}^{1} y\\,dx + \\int_{0}^{1} z\\,dx\\right)dy\\,dz\n= \\int_{0}^{1}\\int_{0}^{1}\\left(\\frac{1}{2} + y + z\\right)dy\\,dz.\n$$\nSeparating terms and using symmetry,\n$$\n\\int_{0}^{1}\\int_{0}^{1}\\left(\\frac{1}{2} + y + z\\right)dy\\,dz\n= \\frac{1}{2}\\cdot 1 + \\left(\\int_{0}^{1} y\\,dy\\right)\\cdot 1 + \\left(\\int_{0}^{1} z\\,dz\\right)\\cdot 1\n= \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} = \\frac{3}{2}.\n$$\nThus $c \\cdot \\frac{3}{2} = 1$, so $c = \\frac{2}{3}$.\n\nNext, we compute the marginal density of $X$:\n$$\nf_{X}(x) = \\int_{0}^{1}\\int_{0}^{1} c(x+y+z)\\,dy\\,dz\n= c\\left[x\\int_{0}^{1}\\int_{0}^{1} dy\\,dz + \\int_{0}^{1}\\int_{0}^{1} y\\,dy\\,dz + \\int_{0}^{1}\\int_{0}^{1} z\\,dy\\,dz\\right]\n= c\\left(x + \\frac{1}{2} + \\frac{1}{2}\\right) = c(x+1),\n$$\nfor $0 \\le x \\le 1$. With $c=\\frac{2}{3}$, this becomes $f_{X}(x) = \\frac{2}{3}(x+1)$, so\n$$\nf_{X}\\!\\left(\\frac{1}{2}\\right) = \\frac{2}{3}\\left(\\frac{1}{2} + 1\\right) = 1.\n$$\n\nThe conditional joint PDF of $Y$ and $Z$ given $X=\\frac{1}{2}$ is, by definition,\n$$\nf_{Y,Z|X}\\!\\left(y,z \\mid \\frac{1}{2}\\right) = \\frac{f_{X,Y,Z}\\!\\left(\\frac{1}{2},y,z\\right)}{f_{X}\\!\\left(\\frac{1}{2}\\right)}\n= \\frac{c\\left(\\frac{1}{2}+y+z\\right)}{1}\n= \\frac{2}{3}\\left(y+z+\\frac{1}{2}\\right),\n$$\nfor $0 \\le y \\le 1$ and $0 \\le z \\le 1$. This integrates to $1$ over the unit square, confirming it is a valid conditional density.", "answer": "$$\\boxed{\\frac{2}{3}\\left(y+z+\\frac{1}{2}\\right)}$$", "id": "1351401"}, {"introduction": "Moving from abstract polynomials to a cornerstone of statistical modeling, this problem explores conditioning within the context of Gaussian variables [@problem_id:1351407]. By modeling a signal corrupted by two independent noise sources, you will discover the distribution of one noise component given that you know their sum. This practice is not just an exercise in calculation; it reveals a symmetric and elegant property of the Normal distribution that is fundamental in fields like signal processing and filtering theory.", "problem": "Consider a system where a signal is affected by two noise sources, modeled by random variables $X$ and $Y$. These noise components are assumed to be independent and identically distributed (i.i.d.), each following a standard Normal distribution with a mean of 0 and a variance of 1. An instrument measures only the total noise, which is the sum of the two components, $S = X + Y$. Given that a specific measurement of the total noise is $S=s$, determine the conditional probability density function (PDF) of the first noise component, $f_{X|S}(x|s)$. Express your answer as a function of $x$ and $s$.", "solution": "Let $X$ and $Y$ be independent standard Normal random variables, so $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$ and $f_{Y}(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{y^{2}}{2}\\right)$. Define $S = X + Y$. We seek $f_{X|S}(x|s)$.\n\nBy the definition of conditional density,\n$$\nf_{X|S}(x|s) = \\frac{f_{X,S}(x,s)}{f_{S}(s)}.\n$$\nUsing the relation $S = X + Y$ and independence, the conditional density of $S$ given $X=x$ is $f_{S|X}(s|x) = f_{Y}(s - x)$. Hence,\n$$\nf_{X,S}(x,s) = f_{X}(x) f_{S|X}(s|x) = f_{X}(x) f_{Y}(s-x) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\exp\\left(-\\frac{(s-x)^{2}}{2}\\right).\n$$\nThe marginal density of $S$ is the convolution of $f_{X}$ and $f_{Y}$. Since the sum of independent Normal variables is Normal with mean equal to the sum of means and variance equal to the sum of variances, we have $S \\sim \\mathcal{N}(0,2)$, so\n$$\nf_{S}(s) = \\frac{1}{\\sqrt{4\\pi}} \\exp\\left(-\\frac{s^{2}}{4}\\right).\n$$\nTherefore,\n$$\nf_{X|S}(x|s) = \\frac{\\frac{1}{2\\pi} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\exp\\left(-\\frac{(s-x)^{2}}{2}\\right)}{\\frac{1}{\\sqrt{4\\pi}} \\exp\\left(-\\frac{s^{2}}{4}\\right)}.\n$$\nCombine exponents in the numerator:\n$$\n-\\frac{x^{2}}{2} - \\frac{(s-x)^{2}}{2} = -\\frac{x^{2} + (s-x)^{2}}{2} = -\\frac{2x^{2} - 2sx + s^{2}}{2} = -x^{2} + sx - \\frac{s^{2}}{2}.\n$$\nSubtract the denominator exponent $-\\frac{s^{2}}{4}$ to get the total exponent:\n$$\n-x^{2} + sx - \\frac{s^{2}}{2} + \\frac{s^{2}}{4} = -x^{2} + sx - \\frac{s^{2}}{4}.\n$$\nComplete the square:\n$$\n-x^{2} + sx - \\frac{s^{2}}{4} = -\\left(x - \\frac{s}{2}\\right)^{2}.\n$$\nFor the prefactor,\n$$\n\\frac{\\frac{1}{2\\pi}}{\\frac{1}{\\sqrt{4\\pi}}} = \\frac{\\sqrt{4\\pi}}{2\\pi} = \\frac{1}{\\sqrt{\\pi}}.\n$$\nThus,\n$$\nf_{X|S}(x|s) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(-\\left(x - \\frac{s}{2}\\right)^{2}\\right),\n$$\nwhich is the density of a Normal distribution with mean $\\frac{s}{2}$ and variance $\\frac{1}{2}$, valid for all real $x$ and $s$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{\\pi}}\\exp\\left(-\\left(x-\\frac{s}{2}\\right)^{2}\\right)}$$", "id": "1351407"}, {"introduction": "This final practice elevates our analysis by introducing the powerful technique of variable transformation to study conditional relationships [@problem_id:1351413]. You will explore the relationship between the sum $S=X+Y$ and ratio $R=X/Y$ of two independent Gamma-distributed variables, a common model for waiting times or accumulated events. The journey involves a change of variables and calculating a Jacobian, leading to the surprising and important discovery that these two quantities are statistically independent, a cornerstone result in mathematical statistics.", "problem": "Let $X$ and $Y$ be two independent and identically distributed (i.i.d.) random variables. Both variables follow a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\beta > 0$. The probability density function (PDF) for a Gamma-distributed random variable $Z$ is given by:\n$$f_Z(z) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} z^{\\alpha-1} \\exp(-\\beta z) \\quad \\text{for } z > 0$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function.\n\nConsider two new random variables defined as the sum and ratio of $X$ and $Y$:\n$S = X+Y$\n$R = X/Y$\n\nDetermine the conditional probability density function, $f_{S|R}(s|r)$, of the sum $S$ given that the ratio $R$ has a specific value $r > 0$. Your final expression should be an analytic function of $s$, defined for $s>0$, possibly containing the parameters $\\alpha$ and $\\beta$.", "solution": "Let $X$ and $Y$ be independent and identically distributed with $X \\sim \\text{Gamma}(\\alpha,\\beta)$ and $Y \\sim \\text{Gamma}(\\alpha,\\beta)$, so their joint density is\n$$\nf_{X,Y}(x,y) = \\left(\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\right)^{2} x^{\\alpha-1} y^{\\alpha-1} \\exp\\big(-\\beta(x+y)\\big), \\quad x>0,\\; y>0.\n$$\nDefine the transformation\n$$\nS = X+Y, \\quad R = \\frac{X}{Y}.\n$$\nThe inverse mapping is\n$$\nX = \\frac{S R}{1+R}, \\quad Y = \\frac{S}{1+R},\n$$\nwith domain $s>0$ and $r>0$. The Jacobian matrix $\\partial(x,y)/\\partial(s,r)$ has entries\n$$\nx_{s} = \\frac{r}{1+r}, \\quad x_{r} = \\frac{s}{(1+r)^{2}}, \\quad y_{s} = \\frac{1}{1+r}, \\quad y_{r} = -\\frac{s}{(1+r)^{2}},\n$$\nso the Jacobian determinant is\n$$\nJ = \\left|\\begin{matrix}\n\\frac{r}{1+r} & \\frac{s}{(1+r)^{2}} \\\\\n\\frac{1}{1+r} & -\\frac{s}{(1+r)^{2}}\n\\end{matrix}\\right|\n= -\\frac{s}{(1+r)^{2}}, \\quad |J| = \\frac{s}{(1+r)^{2}}.\n$$\nTherefore, the joint density of $(S,R)$ is\n$$\n\\begin{aligned}\nf_{S,R}(s,r)\n&= f_{X,Y}\\!\\left(\\frac{s r}{1+r}, \\frac{s}{1+r}\\right)\\, \\frac{s}{(1+r)^{2}} \\\\\n&= \\left(\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\right)^{2} \\left(\\frac{s r}{1+r}\\right)^{\\alpha-1} \\left(\\frac{s}{1+r}\\right)^{\\alpha-1} \\exp(-\\beta s)\\, \\frac{s}{(1+r)^{2}} \\\\\n&= \\frac{\\beta^{2\\alpha}}{\\Gamma(\\alpha)^{2}}\\, s^{2\\alpha-1}\\, r^{\\alpha-1}\\, (1+r)^{-2\\alpha}\\, \\exp(-\\beta s), \\quad s>0,\\; r>0.\n\\end{aligned}\n$$\nNext, integrate over $s$ to obtain the marginal density of $R$:\n$$\n\\begin{aligned}\nf_{R}(r)\n&= \\int_{0}^{\\infty} f_{S,R}(s,r)\\, ds \\\\\n&= \\frac{\\beta^{2\\alpha}}{\\Gamma(\\alpha)^{2}}\\, r^{\\alpha-1}\\, (1+r)^{-2\\alpha} \\int_{0}^{\\infty} s^{2\\alpha-1} \\exp(-\\beta s)\\, ds \\\\\n&= \\frac{\\beta^{2\\alpha}}{\\Gamma(\\alpha)^{2}}\\, r^{\\alpha-1}\\, (1+r)^{-2\\alpha}\\, \\frac{\\Gamma(2\\alpha)}{\\beta^{2\\alpha}} \\\\\n&= \\frac{\\Gamma(2\\alpha)}{\\Gamma(\\alpha)^{2}}\\, r^{\\alpha-1}\\, (1+r)^{-2\\alpha}, \\quad r>0.\n\\end{aligned}\n$$\nHence, the conditional density is\n$$\n\\begin{aligned}\nf_{S\\mid R}(s\\mid r)\n&= \\frac{f_{S,R}(s,r)}{f_{R}(r)} \\\\\n&= \\frac{\\frac{\\beta^{2\\alpha}}{\\Gamma(\\alpha)^{2}}\\, s^{2\\alpha-1}\\, r^{\\alpha-1}\\, (1+r)^{-2\\alpha}\\, \\exp(-\\beta s)}{\\frac{\\Gamma(2\\alpha)}{\\Gamma(\\alpha)^{2}}\\, r^{\\alpha-1}\\, (1+r)^{-2\\alpha}} \\\\\n&= \\frac{\\beta^{2\\alpha}}{\\Gamma(2\\alpha)}\\, s^{2\\alpha-1}\\, \\exp(-\\beta s), \\quad s>0.\n\\end{aligned}\n$$\nThis shows that $S$ is independent of $R$ and $S \\sim \\text{Gamma}(2\\alpha,\\beta)$, so $f_{S\\mid R}(s\\mid r)$ does not depend on $r$.", "answer": "$$\\boxed{\\frac{\\beta^{2\\alpha}}{\\Gamma(2\\alpha)}\\, s^{2\\alpha-1}\\, \\exp(-\\beta s)}$$", "id": "1351413"}]}