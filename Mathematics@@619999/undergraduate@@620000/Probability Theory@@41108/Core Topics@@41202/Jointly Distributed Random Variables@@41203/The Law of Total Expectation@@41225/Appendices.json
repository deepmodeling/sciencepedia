{"hands_on_practices": [{"introduction": "The Law of Total Expectation provides an elegant \"divide and conquer\" strategy for complex probability problems. It allows us to calculate an overall expected value by breaking the problem into simpler, mutually exclusive cases and then calculating a weighted average of the expected values for each case. This introductory exercise [@problem_id:1928933] demonstrates this core principle in a clear-cut scenario involving choices with different probabilistic outcomes, forming a solid foundation for more advanced applications.", "problem": "A contestant on a game show faces three identical doors: Door 1, Door 2, and Door 3. Behind each door is a cash prize. The value of the prize behind any given door is a random variable, but statistical analysis of past shows has determined the mean prize amount for each door. The mean prize behind Door 1 is $1000, the mean prize behind Door 2 is $5000, and the mean prize behind Door 3 is $100. Due to the stage setup, contestants exhibit a known psychological bias: they are twice as likely to choose the central door (Door 2) as they are to choose Door 1. Their likelihood of choosing Door 3 is the same as that of choosing Door 1. What is the overall expected prize money for a contestant playing this game? Express your final answer in dollars, rounded to three significant figures.", "solution": "Let $D \\in \\{1,2,3\\}$ denote the chosen door and let $X$ be the prize won. The law of total expectation gives\n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{3}\\mathbb{E}[X \\mid D=i]\\,\\mathbb{P}(D=i).\n$$\nFrom the problem, the mean prize behind each door is $\\mu_{1}=1000$, $\\mu_{2}=5000$, and $\\mu_{3}=100$, so $\\mathbb{E}[X \\mid D=i]=\\mu_{i}$.\n\nLet $\\mathbb{P}(D=1)=p$. The bias implies $\\mathbb{P}(D=2)=2p$ and $\\mathbb{P}(D=3)=p$. Using normalization,\n$$\np+2p+p=1 \\;\\Rightarrow\\; 4p=1 \\;\\Rightarrow\\; p=\\frac{1}{4}.\n$$\nHence $\\mathbb{P}(D=1)=\\frac{1}{4}$, $\\mathbb{P}(D=2)=\\frac{1}{2}$, and $\\mathbb{P}(D=3)=\\frac{1}{4}$. Therefore,\n$$\n\\mathbb{E}[X]=\\frac{1}{4}\\cdot 1000+\\frac{1}{2}\\cdot 5000+\\frac{1}{4}\\cdot 100=250+2500+25=2775.\n$$\nRounding to three significant figures gives\n$$\n2.78 \\times 10^{3}.\n$$", "answer": "$$\\boxed{2.78 \\times 10^{3}}$$", "id": "1928933"}, {"introduction": "Many real-world phenomena can be modeled as sequential or multi-stage processes, where the outcome of one stage sets the parameters for the next. The Law of Total Expectation is perfectly suited for analyzing such systems. This practice challenges you to apply the law to a two-stage experiment where the conditional expectation is not a fixed number, but a function of the random outcome from the first stage [@problem_id:1400510]. This skill is fundamental for modeling everything from population dynamics to financial markets.", "problem": "A two-stage probabilistic game is defined as follows. In the first stage, a single fair six-sided die is rolled. Let the integer outcome of this roll be denoted by $N$. In the second stage, a fair coin is flipped $N$ times. Let $H$ denote the total number of heads observed from these coin flips. A player's score for the game, $S$, is calculated as the square of the number of heads, i.e., $S = H^2$.\n\nWhat is the expected value of the score, $E[S]$?", "solution": "Let $N$ be the die outcome, uniformly distributed on $\\{1,2,3,4,5,6\\}$ with $\\Pr(N=n)=\\frac{1}{6}$. Conditional on $N=n$, the number of heads $H$ is distributed as a binomial random variable with parameters $(n, p)$ where $p=\\frac{1}{2}$, i.e., $H \\mid N=n \\sim \\mathrm{Binomial}(n, \\frac{1}{2})$. The score is $S=H^{2}$, so we want $E[S]=E[H^{2}]$.\n\nBy the law of total expectation,\n$$\nE[H^{2}]=E\\!\\left(E[H^{2}\\mid N]\\right).\n$$\nFor a binomial random variable $X\\sim \\mathrm{Binomial}(n,p)$, the identity\n$$\nE[X^{2}]=\\operatorname{Var}(X)+\\left(E[X]\\right)^{2}=np(1-p)+n^{2}p^{2}\n$$\nholds. Applying this with $p=\\frac{1}{2}$ gives, for each fixed $n$,\n$$\nE[H^{2}\\mid N=n]=n\\cdot \\frac{1}{2}\\left(1-\\frac{1}{2}\\right)+n^{2}\\left(\\frac{1}{2}\\right)^{2}=\\frac{n}{4}+\\frac{n^{2}}{4}=\\frac{n+n^{2}}{4}.\n$$\nTherefore,\n$$\nE[S]=E[H^{2}]=E\\!\\left(\\frac{N+N^{2}}{4}\\right)=\\frac{1}{4}\\left(E[N]+E[N^{2}]\\right).\n$$\nSince $N$ is uniform on $\\{1,2,3,4,5,6\\}$,\n$$\nE[N]=\\frac{1}{6}\\sum_{n=1}^{6} n=\\frac{1}{6}\\cdot \\frac{6\\cdot 7}{2}=\\frac{7}{2},\n$$\nand\n$$\nE[N^{2}]=\\frac{1}{6}\\sum_{n=1}^{6} n^{2}=\\frac{1}{6}\\cdot \\frac{6\\cdot 7\\cdot 13}{6}=\\frac{91}{6}.\n$$\nHence,\n$$\nE[S]=\\frac{1}{4}\\left(\\frac{7}{2}+\\frac{91}{6}\\right)=\\frac{1}{4}\\cdot \\frac{112}{6}=\\frac{112}{24}=\\frac{14}{3}.\n$$", "answer": "$$\\boxed{\\frac{14}{3}}$$", "id": "1400510"}, {"introduction": "The logic of conditioning is not limited to calculating mean values; it can be extended to understand variability through the Law of Total Variance. This powerful law, which is derived from the Law of Total Expectation, separates the total variance of a random variable into two parts: the expected variance within each condition and the variance of the conditional expectations themselves. This problem [@problem_id:1928914] will guide you through this advanced concept by analyzing a random walk where even the fundamental probability of a step is itself a random variable drawn from a continuous distribution.", "problem": "A particle undergoes a one-dimensional random walk consisting of $N$ discrete steps. At the very beginning of the process, a single value for the probability of moving right, $p$, is chosen by drawing from a continuous uniform distribution on the interval $[0, 1]$. This same probability $p$ is then used for all $N$ steps. In each step, the particle moves by $+1$ with probability $p$ or by $-1$ with probability $1-p$. Let the random variable $S_N$ denote the final position of the particle after $N$ steps, assuming it starts at the origin.\n\nFind the variance of the final position, $\\text{Var}(S_N)$. Express your answer as a function of $N$.", "solution": "Let $X_{i}$ denote the $i$-th step, with $X_{i}\\in\\{-1,+1\\}$. Conditional on the randomly chosen $p\\in[0,1]$, the steps are independent and identically distributed with\n$$\n\\mathbb{E}[X_{i}\\,|\\,p]=(+1)p+(-1)(1-p)=2p-1,\n$$\nand since $X_{i}^{2}=1$,\n$$\n\\operatorname{Var}(X_{i}\\,|\\,p)=\\mathbb{E}[X_{i}^{2}\\,|\\,p]-\\left(\\mathbb{E}[X_{i}\\,|\\,p]\\right)^{2}=1-(2p-1)^{2}=4p(1-p).\n$$\nThe position after $N$ steps is $S_{N}=\\sum_{i=1}^{N}X_{i}$. Conditional on $p$,\n$$\n\\mathbb{E}[S_{N}\\,|\\,p]=N(2p-1),\\qquad \\operatorname{Var}(S_{N}\\,|\\,p)=N\\cdot 4p(1-p)=4N\\,p(1-p).\n$$\nBy the law of total variance,\n$$\n\\operatorname{Var}(S_{N})=\\mathbb{E}\\!\\left[\\operatorname{Var}(S_{N}\\,|\\,p)\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[S_{N}\\,|\\,p]\\right).\n$$\nFor $p\\sim\\operatorname{Unif}[0,1]$, compute\n$$\n\\mathbb{E}[p]=\\int_{0}^{1}p\\,dp=\\frac{1}{2},\\qquad \\mathbb{E}[p^{2}]=\\int_{0}^{1}p^{2}\\,dp=\\frac{1}{3},\n$$\nso\n$$\n\\mathbb{E}[p(1-p)]=\\mathbb{E}[p]-\\mathbb{E}[p^{2}]=\\frac{1}{2}-\\frac{1}{3}=\\frac{1}{6}.\n$$\nHence\n$$\n\\mathbb{E}\\!\\left[\\operatorname{Var}(S_{N}\\,|\\,p)\\right]=\\mathbb{E}\\!\\left[4N\\,p(1-p)\\right]=4N\\cdot\\frac{1}{6}=\\frac{2N}{3}.\n$$\nFor the second term,\n$$\n\\operatorname{Var}\\!\\left(\\mathbb{E}[S_{N}\\,|\\,p]\\right)=\\operatorname{Var}\\!\\left(N(2p-1)\\right)=N^{2}\\operatorname{Var}(2p-1)=N^{2}\\cdot 4\\,\\operatorname{Var}(p),\n$$\nand\n$$\n\\operatorname{Var}(p)=\\mathbb{E}[p^{2}]-(\\mathbb{E}[p])^{2}=\\frac{1}{3}-\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{12},\n$$\nso\n$$\n\\operatorname{Var}\\!\\left(\\mathbb{E}[S_{N}\\,|\\,p]\\right)=N^{2}\\cdot 4\\cdot\\frac{1}{12}=\\frac{N^{2}}{3}.\n$$\nAdding the two contributions gives\n$$\n\\operatorname{Var}(S_{N})=\\frac{2N}{3}+\\frac{N^{2}}{3}=\\frac{N(N+2)}{3}.\n$$", "answer": "$$\\boxed{\\frac{N(N+2)}{3}}$$", "id": "1928914"}]}