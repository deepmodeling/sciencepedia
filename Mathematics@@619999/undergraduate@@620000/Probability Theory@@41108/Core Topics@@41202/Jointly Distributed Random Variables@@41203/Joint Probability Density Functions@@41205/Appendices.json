{"hands_on_practices": [{"introduction": "Before we can use a probability model, we must ensure it is valid. In the context of continuous random variables, this means the total probability over the entire space of possibilities must equal one. This exercise [@problem_id:1926404] introduces the fundamental concept of normalization, where we find the constant $c$ that scales a joint probability density function (PDF) correctly. Mastering this step is crucial for building sound probabilistic models for real-world phenomena, such as the spatial distribution of microorganisms in this hypothetical scenario.", "problem": "A research team is studying the spatial distribution of a certain type of microorganism on a nutrient-rich substrate. The substrate is a flat plate, and the coordinates $(X, Y)$ of a microorganism are modeled as a pair of continuous random variables. The joint probability density function (PDF) for the location of a single microorganism is found to be $f(x, y) = c(x+y)$ for points $(x, y)$ in a specific region $R$, and $f(x, y) = 0$ otherwise. The region $R$ is defined by the area in the first quadrant of the xy-plane bounded by the curve $y=x^2$, the line $x=1$, and the x-axis (where $y=0$). For the PDF to be valid, it must be normalized over this region. Determine the value of the normalization constant $c$.", "solution": "For a joint probability density function to be valid, it must integrate to 1 over its support. The normalization condition is\n$$\n\\iint_{R} c(x+y)\\,dx\\,dy = 1.\n$$\nThe region $R$ is the set of points in the first quadrant bounded by $y=0$, $y=x^{2}$, and $x=1$, which can be written as\n$$\nR=\\{(x,y): 0 \\leq x \\leq 1,\\ 0 \\leq y \\leq x^{2}\\}.\n$$\nThus,\n$$\n\\iint_{R} c(x+y)\\,dx\\,dy = c \\int_{0}^{1} \\int_{0}^{x^{2}} (x+y)\\,dy\\,dx.\n$$\nEvaluate the inner integral:\n$$\n\\int_{0}^{x^{2}} (x+y)\\,dy = \\left[xy + \\frac{1}{2}y^{2}\\right]_{0}^{x^{2}} = x x^{2} + \\frac{1}{2} x^{4} = x^{3} + \\frac{1}{2} x^{4}.\n$$\nNow integrate with respect to $x$:\n$$\nc \\int_{0}^{1} \\left(x^{3} + \\frac{1}{2} x^{4}\\right)\\,dx = c \\left(\\left[\\frac{1}{4}x^{4}\\right]_{0}^{1} + \\left[\\frac{1}{10}x^{5}\\right]_{0}^{1}\\right) = c\\left(\\frac{1}{4} + \\frac{1}{10}\\right) = c \\cdot \\frac{7}{20}.\n$$\nSet this equal to 1 and solve for $c$:\n$$\nc \\cdot \\frac{7}{20} = 1 \\quad \\Rightarrow \\quad c = \\frac{20}{7}.\n$$", "answer": "$$\\boxed{\\frac{20}{7}}$$", "id": "1926404"}, {"introduction": "Often, we have a joint model for multiple variables but are interested in the characteristics of just one. This requires us to derive the marginal probability density function by \"integrating out\" the other variables. This exercise [@problem_id:9606] will guide you through this essential process to find the variance of a single variable, $\\text{Var}(X)$. This skill allows us to isolate and analyze the behavior and variability of one component within a complex, interconnected system.", "problem": "Consider two continuous random variables, $X$ and $Y$, described by a joint probability density function (PDF) given by:\n$$\nf(x, y) = \\begin{cases} 2 & \\text{for } 0 \\le x \\le y \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe region of support for this PDF is a triangle in the $xy$-plane.\n\nYour task is to derive the variance of the random variable $X$, denoted as $\\text{Var}(X)$. Recall that the variance of a random variable $X$ is defined as:\n$$\n\\text{Var}(X) = E[X^2] - (E[X])^2\n$$\nwhere $E[X]$ is the expected value of $X$ and $E[X^2]$ is the expected value of $X^2$.", "solution": "The joint PDF is \n$$f(x,y)=2\\quad(0\\le x\\le y\\le1).$$\nStep 1: marginal of $X$ \n$$f_X(x)=\\int_{y=x}^{1}2\\,dy=2(1-x),\\quad0\\le x\\le1.$$\nStep 2: compute $E[X]$\n$$E[X]=\\int_{0}^{1}x\\,f_X(x)\\,dx\n=\\int_{0}^{1}2x(1-x)\\,dx\n=2\\Bigl[\\tfrac{x^2}{2}-\\tfrac{x^3}{3}\\Bigr]_{0}^{1}\n=2\\bigl(\\tfrac12-\\tfrac13\\bigr)\n=\\tfrac13.$$\nStep 3: compute $E[X^2]$\n$$E[X^2]=\\int_{0}^{1}x^2\\,f_X(x)\\,dx\n=\\int_{0}^{1}2x^2(1-x)\\,dx\n=2\\Bigl[\\tfrac{x^3}{3}-\\tfrac{x^4}{4}\\Bigr]_{0}^{1}\n=2\\bigl(\\tfrac13-\\tfrac14\\bigr)\n=\\tfrac16.$$\nStep 4: variance\n$$\\mathrm{Var}(X)=E[X^2]-(E[X])^2\n=\\tfrac16-\\bigl(\\tfrac13\\bigr)^2\n=\\tfrac{1}{18}.$$", "answer": "$$\\boxed{\\frac{1}{18}}$$", "id": "9606"}, {"introduction": "Understanding the relationship between two random variables is a central goal of statistical analysis. While calculating the covariance is one way to measure this, we can often deduce the nature of the dependency more elegantly. This problem [@problem_id:1369429] challenges you to determine the sign of the covariance not by direct computation, but by analyzing the structure of the PDF's support region and considering the conditional expectation $E[X|Y]$. This approach builds a deeper, more intuitive understanding of how variables can be related, a concept with wide applications, such as modeling the lifetimes of dependent electronic components.", "problem": "Let $X$ and $Y$ be two continuous random variables representing the lifetimes of two electronic components. Their joint probability density function (PDF) is given by\n$$ f(x,y) = \\exp(-y) \\quad \\text{for } 0  x  y  \\infty $$\nand $f(x,y) = 0$ otherwise. The structure of the support region $0  x  y  \\infty$ implies a specific relationship between the two random variables.\n\nWithout performing the full computation of all required expected values for the covariance formula, analyze the dependence between $X$ and $Y$. Based on this analysis, what can you conclude about the sign of their covariance, $\\text{Cov}(X,Y)$?\n\nA. The covariance is positive.\n\nB. The covariance is negative.\n\nC. The covariance is zero.\n\nD. The sign of the covariance cannot be determined without the full computation.", "solution": "We are given the joint density $f(x,y)=\\exp(-y)$ on the support $0xy\\infty$ and $f(x,y)=0$ otherwise. First, compute the marginal density of $Y$:\n$$\nf_{Y}(y)=\\int_{0}^{y} f(x,y)\\,dx=\\int_{0}^{y} \\exp(-y)\\,dx = y \\exp(-y), \\quad y0.\n$$\nThen the conditional density of $X$ given $Y=y$ is\n$$\nf_{X|Y}(x|y)=\\frac{f(x,y)}{f_{Y}(y)}=\\frac{\\exp(-y)}{y \\exp(-y)}=\\frac{1}{y}, \\quad 0xy,\n$$\nwhich is the uniform distribution on $(0,y)$. Hence the conditional expectation is\n$$\n\\mathbb{E}[X \\mid Y=y]=\\int_{0}^{y} x \\cdot \\frac{1}{y}\\,dx=\\frac{y}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}[X \\mid Y]=\\frac{1}{2}Y,\n$$\nwhich is an increasing linear function of $Y$. Using the law of total covariance,\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}\\big[\\operatorname{Cov}(X,Y \\mid Y)\\big]+\\operatorname{Cov}\\big(\\mathbb{E}[X \\mid Y],Y\\big).\n$$\nSince conditioning on $Y$ makes $Y$ constant, $\\operatorname{Cov}(X,Y \\mid Y)=0$, so\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{Cov}\\left(\\frac{1}{2}Y,\\,Y\\right)=\\frac{1}{2}\\operatorname{Var}(Y).\n$$\nBecause $Y$ has a nondegenerate distribution with density $y\\exp(-y)$ on $(0,\\infty)$, we have $\\operatorname{Var}(Y)0$. Therefore,\n$$\n\\operatorname{Cov}(X,Y)=\\frac{1}{2}\\operatorname{Var}(Y)0,\n$$\nso the covariance is positive. This conclusion follows from the monotone (in fact linear) increase of $\\mathbb{E}[X \\mid Y]$ in $Y$, without needing the full computation of all expectations.", "answer": "$$\\boxed{A}$$", "id": "1369429"}]}