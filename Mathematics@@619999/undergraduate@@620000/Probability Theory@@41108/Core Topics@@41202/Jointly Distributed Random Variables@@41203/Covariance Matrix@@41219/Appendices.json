{"hands_on_practices": [{"introduction": "Understanding the relationship between the variance of combined signals and the covariance of their individual components is a fundamental skill. This exercise demonstrates how to extract the covariance $\\operatorname{Cov}(X, Y)$ by leveraging the known variances of the sum ($X+Y$) and difference ($X-Y$) of two random variables. Working through this problem reinforces the key algebraic properties of variance and provides a practical method for calculating covariance when direct measurement is not feasible [@problem_id:1354744].", "problem": "An engineer is calibrating a system with two noisy sensors, A and B. The dimensionless output signal from sensor A is modeled as a random variable $X$, and the output from sensor B is modeled as a random variable $Y$. To understand the system's combined behavior, two new signals are constructed: a sum signal, $S = X+Y$, and a difference signal, $D = X-Y$. After extensive testing, the variance of the sum signal is measured to be $\\operatorname{Var}(S) = 10$, and the variance of the difference signal is measured to be $\\operatorname{Var}(D) = 2$. Based on these measurements, determine the value of the covariance between the two sensor signals, $\\operatorname{Cov}(X, Y)$.", "solution": "Let $X$ and $Y$ be random variables with finite second moments. Denote $\\operatorname{Var}(X)=\\sigma_{X}^{2}$, $\\operatorname{Var}(Y)=\\sigma_{Y}^{2}$, and $\\operatorname{Cov}(X,Y)=C$. Define $S=X+Y$ and $D=X-Y$. We use the variance properties:\n- For any random variables $U$ and $V$, $\\operatorname{Var}(U\\pm V)=\\operatorname{Var}(U)+\\operatorname{Var}(V)\\pm 2\\operatorname{Cov}(U,V)$.\nApplying this to $S$ and $D$ gives\n$$\n\\operatorname{Var}(S)=\\sigma_{X}^{2}+\\sigma_{Y}^{2}+2C=10,\n$$\n$$\n\\operatorname{Var}(D)=\\sigma_{X}^{2}+\\sigma_{Y}^{2}-2C=2.\n$$\nLet $A=\\sigma_{X}^{2}+\\sigma_{Y}^{2}$. Then the system becomes\n$$\nA+2C=10,\\quad A-2C=2.\n$$\nSubtracting the second equation from the first yields\n$$\n4C=8 \\quad \\Rightarrow \\quad C=2.\n$$\nThus, $\\operatorname{Cov}(X,Y)=2$.", "answer": "$$\\boxed{2}$$", "id": "1354744"}, {"introduction": "A covariance matrix is not just any collection of numbers; it must satisfy specific mathematical properties to be valid. This practice problem delves into the concept of positive semi-definiteness, a crucial requirement for any covariance matrix. By determining the valid range for the covariance term $c$, you will gain a deeper appreciation for the mathematical structure that underpins the relationships between multiple random variables [@problem_id:1354713].", "problem": "A data scientist is modeling the relationship between two random variables, $X_1$ and $X_2$, which represent daily measurements from two correlated sensors. The theoretical variances of these measurements are known to be $\\operatorname{Var}(X_1) = 4$ and $\\operatorname{Var}(X_2) = 9$. The covariance between the two sensors, $\\operatorname{Cov}(X_1, X_2)$, is an unknown parameter denoted by $c$. The data scientist constructs the corresponding covariance matrix $\\Sigma$ for the random vector $\\mathbf{X} = [X_1, X_2]^T$ as:\n$$\n\\Sigma = \\begin{pmatrix} 4 & c \\\\ c & 9 \\end{pmatrix}\n$$\nFor this matrix to be a mathematically valid covariance matrix, the parameter $c$ must be restricted to a specific range of real numbers. Which of the following options represents the complete set of all possible values for $c$?\n\nA. $[0, 6]$\n\nB. $[-36, 36]$\n\nC. $[-6, 6]$\n\nD. $[-\\sqrt{6}, \\sqrt{6}]$\n\nE. $(-\\infty, \\infty)$", "solution": "A real covariance matrix must be symmetric positive semidefinite. For a symmetric $2 \\times 2$ matrix\n$$\n\\Sigma=\\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix},\n$$\npositive semidefiniteness is equivalent to the nonnegativity of all leading principal minors:\n$$\na \\geq 0,\\quad d \\geq 0,\\quad \\det(\\Sigma) \\geq 0.\n$$\nHere, $a=4$ and $d=9$ are already nonnegative. The determinant condition gives\n$$\n\\det(\\Sigma)=4 \\cdot 9 - c^{2}=36 - c^{2} \\geq 0,\n$$\nwhich implies\n$$\nc^{2} \\leq 36 \\quad \\Longrightarrow \\quad -6 \\leq c \\leq 6.\n$$\nEquivalently, by the Cauchyâ€“Schwarz inequality for random variables, $|\\operatorname{Cov}(X_{1},X_{2})| \\leq \\sqrt{\\operatorname{Var}(X_{1})\\operatorname{Var}(X_{2})}=\\sqrt{4 \\cdot 9}=6$, yielding the same interval. Therefore, the complete set of all possible values for $c$ is $[-6,6]$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1354713"}, {"introduction": "A common misconception is that zero covariance implies independence between two random variables. This thought experiment provides a powerful and intuitive counterexample to that notion, showing that variables can be perfectly dependent yet have a covariance of zero. By analyzing the coordinates of a point on a unit circle, you will see firsthand why covariance only measures linear relationships and can miss more complex, nonlinear dependencies [@problem_id:1354723].", "problem": "A point particle is constrained to move along the circumference of a unit circle centered at the origin of a Cartesian coordinate system. The position of the particle is described by a single random variable, $\\Theta$, which represents the angle its position vector makes with the positive x-axis. The angle $\\Theta$ is uniformly distributed over the interval $[0, 2\\pi)$, with the angle measured in radians.\n\nLet the Cartesian coordinates of this particle be represented by the random variables $X$ and $Y$. Therefore, the relationships between the random variables are $X = \\cos(\\Theta)$ and $Y = \\sin(\\Theta)$.\n\nCalculate the covariance between the random variables $X$ and $Y$, denoted as $\\operatorname{Cov}(X, Y)$. Provide a single numerical value as your answer.", "solution": "We use the definition of covariance, $\\operatorname{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$. With $\\Theta$ uniformly distributed on $[0,2\\pi)$, its density is $f_{\\Theta}(\\theta)=\\frac{1}{2\\pi}$ for $0\\leq \\theta<2\\pi$. The random variables are $X=\\cos(\\Theta)$ and $Y=\\sin(\\Theta)$.\n\nFirst compute the means. By definition of expectation for a function of a continuous random variable,\n$$\n\\mathbb{E}[X]=\\mathbb{E}[\\cos(\\Theta)]=\\int_{0}^{2\\pi}\\cos(\\theta)\\,\\frac{1}{2\\pi}\\,d\\theta=\\frac{1}{2\\pi}\\left[\\sin(\\theta)\\right]_{0}^{2\\pi}=0,\n$$\nand similarly,\n$$\n\\mathbb{E}[Y]=\\mathbb{E}[\\sin(\\Theta)]=\\int_{0}^{2\\pi}\\sin(\\theta)\\,\\frac{1}{2\\pi}\\,d\\theta=\\frac{1}{2\\pi}\\left[-\\cos(\\theta)\\right]_{0}^{2\\pi}=0.\n$$\n\nNext compute $\\mathbb{E}[XY]=\\mathbb{E}[\\cos(\\Theta)\\sin(\\Theta)]$. Using the identity $\\sin(2\\theta)=2\\sin(\\theta)\\cos(\\theta)$, we have\n$$\n\\mathbb{E}[XY]=\\int_{0}^{2\\pi}\\cos(\\theta)\\sin(\\theta)\\,\\frac{1}{2\\pi}\\,d\\theta=\\frac{1}{2\\pi}\\cdot\\frac{1}{2}\\int_{0}^{2\\pi}\\sin(2\\theta)\\,d\\theta.\n$$\nEvaluate the integral:\n$$\n\\int_{0}^{2\\pi}\\sin(2\\theta)\\,d\\theta=\\left[-\\frac{\\cos(2\\theta)}{2}\\right]_{0}^{2\\pi}=-\\frac{\\cos(4\\pi)-\\cos(0)}{2}=-\\frac{1-1}{2}=0,\n$$\nso\n$$\n\\mathbb{E}[XY]=0.\n$$\n\nTherefore,\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]=0-0\\cdot 0=0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1354723"}]}