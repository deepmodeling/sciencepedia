{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with a classic and intuitive scenario: rolling two fair dice. This exercise [@problem_id:18404] serves as a fundamental building block, guiding you to calculate the variance of the sum from first principles. By working through this problem, you will solidify your understanding of how the independence of two random variables, $X_1$ and $X_2$, allows for the simple addition of their individual variances, i.e., $\\text{Var}(X_1+X_2) = \\text{Var}(X_1) + \\text{Var}(X_2)$.", "problem": "Consider two standard, fair, six-sided dice. Let the random variable $X_1$ represent the outcome of rolling the first die, and $X_2$ represent the outcome of rolling the second die. The possible outcomes for each die are the integers $\\{1, 2, 3, 4, 5, 6\\}$, with each outcome having an equal probability of occurrence. The two rolls are independent events.\n\nLet a new random variable $S$ be defined as the sum of the outcomes of the two dice, such that $S = X_1 + X_2$.\n\nYour task is to derive the variance of the sum $S$, denoted as $\\text{Var}(S)$. Base your derivation on the fundamental definitions of expectation and variance.\n\n**Definitions:**\n- The **expectation** (or expected value) of a discrete random variable $Y$ is given by $E[Y] = \\sum_i y_i P(Y=y_i)$, where $y_i$ are the possible values of $Y$.\n- The **variance** of a random variable $Y$ is given by $\\text{Var}(Y) = E[(Y - E[Y])^2]$, which can be simplified to the computational formula $\\text{Var}(Y) = E[Y^2] - (E[Y])^2$.", "solution": "We have two independent random variables $X_1,X_2$ each uniform on $\\{1,2,3,4,5,6\\}$.  By definition,\n$$E[X_i]=\\sum_{k=1}^6k\\cdot\\frac16=\\frac{1+2+3+4+5+6}6=\\frac{21}6=\\frac72.$$\nNext,\n$$E[X_i^2]=\\sum_{k=1}^6k^2\\cdot\\frac16=\\frac{1^2+2^2+3^2+4^2+5^2+6^2}6=\\frac{91}6.$$\nThus the variance of one die is\n$$Var(X_i)=E[X_i^2]-\\bigl(E[X_i]\\bigr)^2\n=\\frac{91}6-\\Bigl(\\frac72\\Bigr)^2\n=\\frac{91}6-\\frac{49}4\n=\\frac{182-147}{12}\n=\\frac{35}{12}.$$\nSince $S=X_1+X_2$ and $X_1,X_2$ are independent,\n$$Var(S)=Var(X_1)+Var(X_2)=2\\cdot\\frac{35}{12}=\\frac{35}{6}.$$", "answer": "$$\\boxed{\\frac{35}{6}}$$", "id": "18404"}, {"introduction": "Real-world variables are often not independent; their outcomes can be linked. This practice problem [@problem_id:18394] moves beyond the simplifying assumption of independence by presenting a joint probability distribution where two variables are correlated. You will apply the complete formula for the variance of a sum, $\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)$, a crucial step that involves calculating the covariance to account for this relationship.", "problem": "Consider two discrete random variables, $X$ and $Y$. The variable $X$ can take values from the set $\\{0, a\\}$, and the variable $Y$ can take values from the set $\\{0, b\\}$, where $a$ and $b$ are distinct positive constants ($a > 0, b > 0, a \\neq b$). Their joint probability mass function, $P(X=x, Y=y)$, is defined by a parameter $p$ according to the table below.\n\n|           | $X=0$ | $X=a$ |\n|:---------:|:-----:|:-----:|\n|   **Y=0**   |  $p$  | $2p$  |\n|   **Y=b**   | $2p$  | $1-5p$|\n\nThe parameter $p$ is a real number such that $0 < p < \\frac{1}{5}$, which ensures all probabilities are positive.\n\nDefine a new random variable $Z = X + Y$. Derive a symbolic expression for the variance of $Z$, denoted as $\\text{Var}(Z)$, in terms of the constants $a$, $b$, and $p$.", "solution": "We use the identity  \n$$Var(Z)=Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y).$$  \nMarginally,  \n$$P(X=0)=p+2p=3p,\\quad P(X=a)=1-3p,$$  \n$$P(Y=0)=p+2p=3p,\\quad P(Y=b)=1-3p.$$  \nThus  \n$$E[X]=a(1-3p),\\quad E[Y]=b(1-3p).$$  \nCompute variances:  \n$$E[X^2]=a^2(1-3p),\\quad Var(X)=E[X^2]-E[X]^2 = a^2(1-3p)-a^2(1-3p)^2 =3a^2p(1-3p),$$  \n$$Var(Y)=3b^2p(1-3p).$$  \nFor the covariance,  \n$$E[XY]=ab\\,P(X=a,Y=b)=ab(1-5p),$$  \n$$Cov(X,Y)=E[XY]-E[X]E[Y] \n=ab\\bigl[(1-5p)-(1-3p)^2\\bigr]\n=p(1-9p)ab.$$  \nHence  \n$$Var(Z)=3p(1-3p)(a^2+b^2)+2p(1-9p)ab.$$", "answer": "$$\\boxed{3p(1-3p)(a^2 + b^2) + 2p(1-9p)ab}$$", "id": "18394"}, {"introduction": "To truly appreciate the role of covariance, we now examine an extreme case of dependence where one variable is perfectly negatively correlated with another. This thought experiment [@problem_id:18400] reveals the power of the full variance formula in a non-intuitive scenario, demonstrating how the covariance term can completely alter the outcome. Solving this will challenge and refine your understanding of how interacting variables can influence total system variability, sometimes in surprising ways.", "problem": "Let $X$ be a random variable with a well-defined mean $E[X] = \\mu$ and a finite, non-zero variance $\\text{Var}(X) = \\sigma^2$. Let a second random variable $Y$ be defined as a linear function of $X$, specifically $Y = -X$.\n\nThe variance of a random variable $A$ is defined as $\\text{Var}(A) = E[(A - E[A])^2]$. The covariance between two random variables $A$ and $B$ is defined as $\\text{Cov}(A,B) = E[(A - E[A])(B - E[B])]$.\n\nThe variance for the sum of two random variables $X$ and $Y$ is given by the general formula:\n$$\n\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)\n$$\n\nUsing these definitions and properties, derive the value of $\\text{Var}(X+Y)$.", "solution": "We have $Y=-X$.  First compute the mean and variance of $Y$:\n$$E[Y]=E[-X]=-E[X]=-\\mu,$$\n$$Var(Y)=E\\bigl[(Y-E[Y])^2\\bigr]\n=E\\bigl[(-X+\\mu)^2\\bigr]\n=E\\bigl[(X-\\mu)^2\\bigr]\n=\\sigma^2.$$\nNext compute the covariance:\n$$Cov(X,Y)=E\\bigl[(X-E[X])(Y-E[Y])\\bigr]\n=E\\bigl[(X-\\mu)(-X+\\mu)\\bigr]\n=-E\\bigl[(X-\\mu)^2\\bigr]\n=-\\sigma^2.$$\nNow apply the variance sum formula:\n$$Var(X+Y)=Var(X)+Var(Y)+2\\,Cov(X,Y)\n=\\sigma^2+\\sigma^2+2(-\\sigma^2)\n=0.$$", "answer": "$$\\boxed{0}$$", "id": "18400"}]}