{"hands_on_practices": [{"introduction": "The most direct way to understand conditional expectation is by calculating it from first principles. This exercise guides you through that fundamental process in a clear, geometric context [@problem_id:1381962]. By considering a random point chosen uniformly from a semi-disk, you will practice deriving the conditional probability density function and then use it to compute the expected value, reinforcing the core definition of $E[X|Y=y]$.", "id": "1381962", "problem": "An impurity is located at a random position $(X, Y)$ within the core of a cylindrical optical fiber. The cross-section of the fiber core is modeled as a disk of radius $R$ centered at the origin. Due to a specific manufacturing process, impurities are only found in the right half of the fiber core. Therefore, the position $(X,Y)$ of the impurity is a random point chosen from a uniform distribution over the semi-disk region defined by $x^2 + y^2 \\le R^2$ and $x \\ge 0$.\n\nA quality control measurement reveals the vertical position of the impurity to be $Y=y$, where $-R < y < R$. Determine the expected horizontal position of the impurity, $E[X|Y=y]$. Express your answer as a function of $R$ and $y$.\n\n", "solution": "Let $S=\\{(x,y):x^{2}+y^{2}\\le R^{2},\\,x\\ge 0\\}$ denote the right semi-disk. The joint density is uniform on $S$, so\n$$\nf_{X,Y}(x,y)=\\frac{1}{|S|}\\quad\\text{for }(x,y)\\in S,\\qquad |S|=\\frac{1}{2}\\pi R^{2}.\n$$\nFor a fixed $y$ with $|y|<R$, the admissible $x$ satisfy $0\\le x\\le \\sqrt{R^{2}-y^{2}}$. The marginal density of $Y$ at $y$ is\n$$\nf_{Y}(y)=\\int_{0}^{\\sqrt{R^{2}-y^{2}}} f_{X,Y}(x,y)\\,dx=\\frac{1}{|S|}\\int_{0}^{\\sqrt{R^{2}-y^{2}}} dx=\\frac{\\sqrt{R^{2}-y^{2}}}{|S|}.\n$$\nHence the conditional density of $X$ given $Y=y$ is\n$$\nf_{X\\mid Y}(x\\mid y)=\\frac{f_{X,Y}(x,y)}{f_{Y}(y)}=\\frac{1/|S|}{\\sqrt{R^{2}-y^{2}}/|S|}=\\frac{1}{\\sqrt{R^{2}-y^{2}}},\\quad 0\\le x\\le \\sqrt{R^{2}-y^{2}},\n$$\nwhich is uniform on $[0,\\sqrt{R^{2}-y^{2}}]$. Therefore,\n$$\nE[X\\mid Y=y]=\\int_{0}^{\\sqrt{R^{2}-y^{2}}} x\\,\\frac{1}{\\sqrt{R^{2}-y^{2}}}\\,dx=\\frac{1}{\\sqrt{R^{2}-y^{2}}}\\cdot \\frac{1}{2}\\left.x^{2}\\right|_{0}^{\\sqrt{R^{2}-y^{2}}}=\\frac{1}{2}\\sqrt{R^{2}-y^{2}}.\n$$\nThis holds for $-R<y<R$.", "answer": "$$\\boxed{\\frac{1}{2}\\sqrt{R^{2}-y^{2}}}$$"}, {"introduction": "Calculating conditional expectations does not always require cumbersome integration or summation. A key property states that if a random variable is already determined by the conditioning information, its conditional expectation is simply itself. This practice problem offers a clever illustration of this powerful principle, showing how the desired quantity can be expressed as a direct function of the conditioning variable, leading to a surprisingly simple solution [@problem_id:1438518].", "id": "1438518", "problem": "Two independent sensors, Sensor 1 and Sensor 2, are deployed to detect a specific environmental signature. For any given observation period, each sensor reports a binary outcome: '1' if the signature is detected and '0' otherwise. Let $X_1$ and $X_2$ be the random variables representing the reports from Sensor 1 and Sensor 2, respectively. Both sensors operate independently and are statistically identical, with a probability $p$ of reporting '1'. Thus, $X_1$ and $X_2$ can be modeled as independent and identically distributed Bernoulli random variables. An analyst needs to estimate the event of joint detection, which is captured by the product $X_1 X_2$. However, the only data available to the analyst is the total number of detections, i.e., the sum $S = X_1 + X_2$. Determine the analyst's best estimate for the joint detection event given only the information about the sum $S$. In the language of probability theory, find the conditional expectation $E[X_1 X_2 | S]$. Express your answer as a function of $S$.\n\n", "solution": "Let $X_{1}, X_{2} \\sim \\text{Bernoulli}(p)$ be independent and let $S = X_{1} + X_{2}$. The quantity of interest is $X_{1}X_{2}$, which equals $1$ if and only if both sensors report $1$.\n\nList the possible values of $S$ and determine $X_{1}X_{2}$ in each case:\n- If $S=0$, then $(X_{1},X_{2})=(0,0)$, so $X_{1}X_{2}=0$.\n- If $S=1$, then one of $X_{1},X_{2}$ is $1$ and the other is $0$, so $X_{1}X_{2}=0$.\n- If $S=2$, then $(X_{1},X_{2})=(1,1)$, so $X_{1}X_{2}=1$.\n\nThus $X_{1}X_{2}$ is completely determined by $S$ and equals the indicator of the event $\\{S=2\\}$. Equivalently, as a function of $S \\in \\{0,1,2\\}$,\n$$\nX_{1}X_{2} = \\frac{S(S-1)}{2},\n$$\nsince this expression evaluates to $0$ for $S=0$ or $S=1$ and to $1$ for $S=2$.\n\nBecause $X_{1}X_{2}$ is a measurable function of $S$, by the defining property of conditional expectation,\n$$\nE[X_{1}X_{2} \\mid S] = X_{1}X_{2} = \\frac{S(S-1)}{2}.\n$$\nThis result does not depend on $p$.", "answer": "$$\\boxed{\\frac{S(S-1)}{2}}$$"}, {"introduction": "Let's explore a scenario common in signal processing, where we want to estimate an original signal from a measurement of its power or intensity. This problem asks for the best estimate of a signal $V$ given its square, $P = V^2$ [@problem_id:1327108]. This exercise challenges you to handle conditioning on a non-injective function and demonstrates how to combine information from multiple possible values of the original signal to arrive at the optimal estimate, a process that has deep connections to filtering and state estimation.", "id": "1327108", "problem": "In a digital communication system, the amplitude of a received signal pulse, denoted by the random variable $V$, is modeled by a Gaussian distribution. This signal has a non-zero Direct Current (DC) component $\\mu$ and is corrupted by additive Gaussian noise with zero mean, resulting in a total signal standard deviation of $\\sigma$. Thus, the probability density function of $V$ is given by $f_V(v) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(v-\\mu)^2}{2\\sigma^2}\\right)$, where $\\mu$ and $\\sigma$ are positive real constants.\n\nA non-linear detector in the receiver circuit measures a quantity $P$ which is equal to the square of the signal amplitude, i.e., $P = V^2$. For optimal signal reconstruction, it is necessary to compute the best estimate of the original amplitude $V$ given this measurement $P$. In the sense of minimizing the mean squared error, this best estimate is the conditional expectation of $V$ given $P$.\n\nDetermine the conditional expectation $E[V | P]$. Your answer should be a function of the measured quantity $P$, the DC component $\\mu$, and the standard deviation $\\sigma$.\n\n", "solution": "Let $V \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ with density $f_{V}(v)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(v-\\mu)^{2}}{2\\sigma^{2}}\\right)$, and let $P=V^{2}$. We seek $E[V \\mid P]$, which must be a measurable function of $P$. For $p>0$, the mapping $h(v)=v^{2}$ has two preimages $v_{1}=\\sqrt{p}$ and $v_{2}=-\\sqrt{p}$ with derivative $h'(v)=2v$, so $|h'(v_{1})|=|h'(v_{2})|=2\\sqrt{p}$. The conditional distribution of $V$ given $P=p$ is discrete on $\\{\\sqrt{p},-\\sqrt{p}\\}$ with weights proportional to $f_{V}(v_{i})/|h'(v_{i})|$. Therefore,\n$$\nE[V \\mid P=p]\n=\\frac{\\sqrt{p}\\,\\frac{f_{V}(\\sqrt{p})}{2\\sqrt{p}}+(-\\sqrt{p})\\,\\frac{f_{V}(-\\sqrt{p})}{2\\sqrt{p}}}{\\frac{f_{V}(\\sqrt{p})}{2\\sqrt{p}}+\\frac{f_{V}(-\\sqrt{p})}{2\\sqrt{p}}}\n=\\sqrt{p}\\,\\frac{f_{V}(\\sqrt{p})-f_{V}(-\\sqrt{p})}{f_{V}(\\sqrt{p})+f_{V}(-\\sqrt{p})}.\n$$\nUsing the Gaussian density,\n$$\nf_{V}(\\sqrt{p})=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(\\sqrt{p}-\\mu)^{2}}{2\\sigma^{2}}\\right),\\quad\nf_{V}(-\\sqrt{p})=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(\\sqrt{p}+\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nLet\n$$\nA=\\exp\\!\\left(-\\frac{(\\sqrt{p}-\\mu)^{2}}{2\\sigma^{2}}\\right),\\quad\nB=\\exp\\!\\left(-\\frac{(\\sqrt{p}+\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nThen\n$$\nA=\\exp\\!\\left(-\\frac{p+\\mu^{2}}{2\\sigma^{2}}\\right)\\exp\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right),\\quad\nB=\\exp\\!\\left(-\\frac{p+\\mu^{2}}{2\\sigma^{2}}\\right)\\exp\\!\\left(-\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right),\n$$\nso\n$$\n\\frac{A-B}{A+B}\n=\\frac{\\exp\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)-\\exp\\!\\left(-\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)}{\\exp\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)+\\exp\\!\\left(-\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)}\n=\\tanh\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right).\n$$\nTherefore, for $p>0$,\n$$\nE[V \\mid P=p]=\\sqrt{p}\\,\\tanh\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right).\n$$\nAt $p=0$, the only preimage is $v=0$, hence $E[V \\mid P=0]=0$, which agrees with the limit of the above expression as $p \\to 0^{+}$. Thus, as a function of the measured $P$,\n$$\nE[V \\mid P]=\\sqrt{P}\\,\\tanh\\!\\left(\\frac{\\mu\\sqrt{P}}{\\sigma^{2}}\\right).\n$$", "answer": "$$\\boxed{\\sqrt{P}\\,\\tanh\\!\\left(\\frac{\\mu\\sqrt{P}}{\\sigma^{2}}\\right)}$$"}]}