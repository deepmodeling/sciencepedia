{"hands_on_practices": [{"introduction": "The Law of Total Variance provides a powerful way to decompose the total variance of a random variable into components. This first exercise [@problem_id:1401027] provides a foundational scenario involving a mixture of two simple uniform distributions. By working through it, you will practice the direct application of the formula $\\mathrm{Var}(T) = \\mathbb{E}[\\mathrm{Var}(T|R)] + \\mathrm{Var}(\\mathbb{E}[T|R])$, learning to distinguish and calculate the 'mean of the conditional variances' and the 'variance of the conditional means'.", "problem": "A data scientist's computer has two modes for running a complex simulation: a 'fast' mode that uses cloud resources and a 'local' mode that uses only the machine's own processor. The choice of mode is probabilistic. Let $p$ be the probability that the 'fast' mode is chosen, where $0 < p < 1$.\n\nThe time required to complete the simulation, measured in minutes, is a random variable that depends on the mode.\n- If the 'fast' mode is selected, the completion time is uniformly distributed over the interval $[15, 20]$ minutes.\n- If the 'local' mode is selected, the completion time is uniformly distributed over the interval $[20, 30]$ minutes.\n\nLet $T$ be the random variable representing the total time to complete the simulation. Determine the total variance of the completion time, $\\mathrm{Var}(T)$, as an analytic expression in terms of the probability $p$.", "solution": "Let $R$ be a random variable representing the mode chosen. Let $R=F$ for the 'fast' mode and $R=L$ for the 'local' mode. We are given the probabilities $P(R=F) = p$ and $P(R=L) = 1-p$.\n\nThe problem asks for the total variance of the simulation time $T$, which can be found using the Law of Total Variance:\n$$ \\mathrm{Var}(T) = \\mathbb{E}[\\mathrm{Var}(T|R)] + \\mathrm{Var}(\\mathbb{E}[T|R]) $$\n\nWe will calculate the two terms on the right-hand side separately.\n\nFirst, we need the mean and variance for a continuous uniform distribution $U(a, b)$:\nMean: $\\mathbb{E}[U(a,b)] = \\frac{a+b}{2}$\nVariance: $\\mathrm{Var}(U(a,b)) = \\frac{(b-a)^2}{12}$\n\nFor the 'fast' mode, the time $T_F$ is distributed as $U(15, 20)$.\nThe conditional mean is $\\mathbb{E}[T|R=F] = \\frac{15+20}{2} = \\frac{35}{2}$.\nThe conditional variance is $\\mathrm{Var}(T|R=F) = \\frac{(20-15)^2}{12} = \\frac{5^2}{12} = \\frac{25}{12}$.\n\nFor the 'local' mode, the time $T_L$ is distributed as $U(20, 30)$.\nThe conditional mean is $\\mathbb{E}[T|R=L] = \\frac{20+30}{2} = \\frac{50}{2} = 25$.\nThe conditional variance is $\\mathrm{Var}(T|R=L) = \\frac{(30-20)^2}{12} = \\frac{10^2}{12} = \\frac{100}{12}$.\n\n**Step 1: Calculate $\\mathbb{E}[\\mathrm{Var}(T|R)]$ (the mean of the conditional variances)**\n\nThe random variable $\\mathrm{Var}(T|R)$ takes the value $\\frac{25}{12}$ with probability $p$ and the value $\\frac{100}{12}$ with probability $1-p$. Its expected value is:\n$$ \\mathbb{E}[\\mathrm{Var}(T|R)] = \\mathrm{Var}(T|R=F)P(R=F) + \\mathrm{Var}(T|R=L)P(R=L) $$\n$$ \\mathbb{E}[\\mathrm{Var}(T|R)] = \\left(\\frac{25}{12}\\right)p + \\left(\\frac{100}{12}\\right)(1-p) $$\n$$ \\mathbb{E}[\\mathrm{Var}(T|R)] = \\frac{25p + 100 - 100p}{12} = \\frac{100 - 75p}{12} $$\n\n**Step 2: Calculate $\\mathrm{Var}(\\mathbb{E}[T|R])$ (the variance of the conditional means)**\n\nThe random variable $\\mathbb{E}[T|R]$ takes the value $\\frac{35}{2}$ with probability $p$ and the value $25$ with probability $1-p$.\nTo find its variance, we first find its expectation and the expectation of its square.\n\nThe expectation of $\\mathbb{E}[T|R]$ is the total expectation $\\mathbb{E}[T]$:\n$$ \\mathbb{E}[\\mathbb{E}[T|R]] = \\mathbb{E}[T|R=F]P(R=F) + \\mathbb{E}[T|R=L]P(R=L) $$\n$$ \\mathbb{E}[\\mathbb{E}[T|R]] = \\left(\\frac{35}{2}\\right)p + (25)(1-p) = \\frac{35}{2}p + 25 - 25p = 25 - \\frac{15}{2}p $$\n\nThe expectation of $(\\mathbb{E}[T|R])^2$ is:\n$$ \\mathbb{E}[(\\mathbb{E}[T|R])^2] = (\\mathbb{E}[T|R=F])^2 P(R=F) + (\\mathbb{E}[T|R=L])^2 P(R=L) $$\n$$ \\mathbb{E}[(\\mathbb{E}[T|R])^2] = \\left(\\frac{35}{2}\\right)^2 p + (25)^2 (1-p) $$\n$$ \\mathbb{E}[(\\mathbb{E}[T|R])^2] = \\frac{1225}{4}p + 625(1-p) = \\frac{1225p + 2500 - 2500p}{4} = \\frac{2500 - 1275p}{4} $$\n\nNow, using the variance formula $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$:\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\mathbb{E}[(\\mathbb{E}[T|R])^2] - (\\mathbb{E}[\\mathbb{E}[T|R]])^2 $$\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\left(\\frac{2500 - 1275p}{4}\\right) - \\left(25 - \\frac{15}{2}p\\right)^2 $$\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\frac{2500 - 1275p}{4} - \\left(625 - 2 \\cdot 25 \\cdot \\frac{15}{2}p + \\left(\\frac{15}{2}p\\right)^2\\right) $$\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\frac{2500 - 1275p}{4} - \\left(625 - 375p + \\frac{225}{4}p^2\\right) $$\nTo subtract, we put everything over a common denominator of 4:\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\frac{2500 - 1275p}{4} - \\frac{2500 - 1500p + 225p^2}{4} $$\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\frac{(2500 - 1275p) - (2500 - 1500p + 225p^2)}{4} $$\n$$ \\mathrm{Var}(\\mathbb{E}[T|R]) = \\frac{2500 - 1275p - 2500 + 1500p - 225p^2}{4} = \\frac{225p - 225p^2}{4} $$\n\n**Step 3: Combine the two terms**\n\nNow we add the results from Step 1 and Step 2 to find the total variance $\\mathrm{Var}(T)$.\n$$ \\mathrm{Var}(T) = \\mathbb{E}[\\mathrm{Var}(T|R)] + \\mathrm{Var}(\\mathbb{E}[T|R]) $$\n$$ \\mathrm{Var}(T) = \\frac{100 - 75p}{12} + \\frac{225p - 225p^2}{4} $$\nTo add these fractions, we find a common denominator, which is 12.\n$$ \\mathrm{Var}(T) = \\frac{100 - 75p}{12} + \\frac{3(225p - 225p^2)}{12} $$\n$$ \\mathrm{Var}(T) = \\frac{100 - 75p + 675p - 675p^2}{12} $$\n$$ \\mathrm{Var}(T) = \\frac{-675p^2 + 600p + 100}{12} $$\nWe can factor out 25 from the numerator:\n$$ \\mathrm{Var}(T) = \\frac{25(-27p^2 + 24p + 4)}{12} $$\nEither of the last two expressions is a valid final form. We will use the expanded polynomial form.", "answer": "$$\\boxed{\\frac{-675p^2 + 600p + 100}{12}}$$", "id": "1401027"}, {"introduction": "Building on the basic mixture model, this problem [@problem_id:1929482] introduces a common type of hierarchical structure where the outcome of a process depends on a preliminary random choice. We analyze packet delays in a network, where the number of potential delays (the number of routers) is itself a random variable. This problem demonstrates how to model composite experiments, such as a Binomial process where the number of trials $n$ is not fixed but is determined by an earlier random event.", "problem": "In a simplified model of a data network, a packet is sent from a source to a destination. The packet can be routed through one of three possible paths, chosen randomly.\n- Path A is chosen with a probability of $0.5$. This path has $3$ routers.\n- Path B is chosen with a probability of $0.3$. This path has $5$ routers.\n- Path C is chosen with a probability of $0.2$. This path has $8$ routers.\n\nAt each router the packet traverses, there is a fixed, independent probability of $p = 0.1$ that the packet experiences a minor processing delay. Let $Y$ be the total number of processing delays a packet experiences on its journey from source to destination.\n\nCalculate the variance of $Y$. Round your final answer to four significant figures.", "solution": "Let $N$ denote the number of routers on the chosen path. Then $N$ takes values $3, 5, 8$ with probabilities $0.5, 0.3, 0.2$, respectively. Conditional on $N=n$, the total number of processing delays $Y$ satisfies $Y \\mid N=n \\sim \\mathrm{Binomial}(n, p)$ with $p=0.1$, so\n$$\n\\mathbb{E}[Y \\mid N=n] = np, \\quad \\mathrm{Var}(Y \\mid N=n) = np(1-p).\n$$\nBy the law of total variance,\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid N)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid N]) = p(1-p)\\mathbb{E}[N] + p^{2}\\mathrm{Var}(N).\n$$\nCompute $\\mathbb{E}[N]$ and $\\mathrm{Var}(N)$ from the distribution of $N$:\n$$\n\\mathbb{E}[N] = 0.5 \\cdot 3 + 0.3 \\cdot 5 + 0.2 \\cdot 8 = 1.5 + 1.5 + 1.6 = 4.6,\n$$\n$$\n\\mathbb{E}[N^{2}] = 0.5 \\cdot 3^{2} + 0.3 \\cdot 5^{2} + 0.2 \\cdot 8^{2} = 0.5 \\cdot 9 + 0.3 \\cdot 25 + 0.2 \\cdot 64 = 4.5 + 7.5 + 12.8 = 24.8,\n$$\n$$\n\\mathrm{Var}(N) = \\mathbb{E}[N^{2}] - (\\mathbb{E}[N])^{2} = 24.8 - (4.6)^{2} = 24.8 - 21.16 = 3.64.\n$$\nSubstitute $p=0.1$, $\\mathbb{E}[N]=4.6$, and $\\mathrm{Var}(N)=3.64$ into the variance formula:\n$$\n\\mathrm{Var}(Y) = 0.1 \\cdot 0.9 \\cdot 4.6 + (0.1)^{2} \\cdot 3.64 = 0.414 + 0.0364 = 0.4504.\n$$\nRounded to four significant figures, the variance is $0.4504$.", "answer": "$$\\boxed{0.4504}$$", "id": "1929482"}, {"introduction": "This final practice problem [@problem_id:1929483] elevates the concept to a scenario with a continuous source of underlying uncertainty, a common feature in Bayesian analysis and sophisticated statistical modeling. Instead of choosing from a discrete set of scenarios, the very parameter governing the process—the probability of a defect, $\\Theta$—is a continuous random variable. This exercise will challenge you to apply the Law of Total Variance in a continuous setting, using integration to find the expected values and demonstrating the law's full power and versatility.", "problem": "In a semiconductor fabrication plant, a new manufacturing process is being tested. Due to unpredictable daily variations in environmental conditions (like temperature and humidity), the probability, $\\Theta$, that any given microchip is defective is not a fixed constant. Instead, it is considered a random variable. Based on historical data from similar processes, engineers model $\\Theta$ as being uniformly distributed over the interval $[0, 1]$.\n\nA quality control engineer selects a batch of $n$ chips from a single day's production run. Let $X$ be the random variable representing the total number of defective chips found in this batch. Assuming that the defect status of each chip is independent of the others, conditional on the value of $\\Theta$ for that day, determine the variance of $X$.\n\nExpress your answer in terms of $n$.", "solution": "Let $X$ be the random variable for the number of defective chips in a batch of size $n$, and let $\\Theta$ be the random variable for the probability of a single chip being defective.\n\nThe problem states that the distribution of $\\Theta$ is continuous and uniform on the interval $[0, 1]$. We write this as $\\Theta \\sim \\text{Uniform}(0, 1)$. The probability density function (PDF) of $\\Theta$ is $f(\\theta) = 1$ for $0 \\le \\theta \\le 1$, and $f(\\theta) = 0$ otherwise.\n\nConditional on the probability of defect being a specific value $\\Theta = \\theta$, the number of defective chips $X$ in a batch of $n$ independent trials follows a binomial distribution. We write this as $X | (\\Theta = \\theta) \\sim \\text{Binomial}(n, \\theta)$.\n\nTo find the unconditional variance of $X$, denoted $\\mathrm{Var}(X)$, we use the law of total variance:\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X|\\Theta)] + \\mathrm{Var}(\\mathbb{E}[X|\\Theta])\n$$\nWe need to calculate the two terms on the right-hand side separately.\n\nFirst, we find the conditional expectation and variance of $X$ given $\\Theta$. For a random variable $Y \\sim \\text{Binomial}(n, p)$, we know that $\\mathbb{E}[Y] = np$ and $\\mathrm{Var}(Y) = np(1-p)$. Applying this to our conditional distribution:\n1.  The conditional expectation of $X$ given $\\Theta$ is $\\mathbb{E}[X|\\Theta] = n\\Theta$.\n2.  The conditional variance of $X$ given $\\Theta$ is $\\mathrm{Var}(X|\\Theta) = n\\Theta(1-\\Theta) = n\\Theta - n\\Theta^2$.\n\nNow, we calculate the two terms for the law of total variance.\n\nThe first term is $\\mathbb{E}[\\mathrm{Var}(X|\\Theta)]$. This is the expectation of the conditional variance, taken over the distribution of $\\Theta$.\n$$\n\\mathbb{E}[\\mathrm{Var}(X|\\Theta)] = \\mathbb{E}[n\\Theta - n\\Theta^2] = n\\mathbb{E}[\\Theta] - n\\mathbb{E}[\\Theta^2]\n$$\nWe need to find the first two moments of the $\\text{Uniform}(0, 1)$ distribution.\nThe expected value of $\\Theta$ is:\n$$\n\\mathbb{E}[\\Theta] = \\int_0^1 \\theta f(\\theta) \\,d\\theta = \\int_0^1 \\theta \\cdot 1 \\,d\\theta = \\left[\\frac{\\theta^2}{2}\\right]_0^1 = \\frac{1}{2}\n$$\nThe expected value of $\\Theta^2$ is:\n$$\n\\mathbb{E}[\\Theta^2] = \\int_0^1 \\theta^2 f(\\theta) \\,d\\theta = \\int_0^1 \\theta^2 \\cdot 1 \\,d\\theta = \\left[\\frac{\\theta^3}{3}\\right]_0^1 = \\frac{1}{3}\n$$\nSubstituting these values back into the expression for $\\mathbb{E}[\\mathrm{Var}(X|\\Theta)]$:\n$$\n\\mathbb{E}[\\mathrm{Var}(X|\\Theta)] = n\\left(\\frac{1}{2}\\right) - n\\left(\\frac{1}{3}\\right) = n\\left(\\frac{3-2}{6}\\right) = \\frac{n}{6}\n$$\n\nThe second term is $\\mathrm{Var}(\\mathbb{E}[X|\\Theta])$. This is the variance of the conditional expectation.\n$$\n\\mathrm{Var}(\\mathbb{E}[X|\\Theta]) = \\mathrm{Var}(n\\Theta)\n$$\nUsing the property $\\mathrm{Var}(aY) = a^2\\mathrm{Var}(Y)$, we have:\n$$\n\\mathrm{Var}(n\\Theta) = n^2\\mathrm{Var}(\\Theta)\n$$\nThe variance of $\\Theta$ is given by $\\mathrm{Var}(\\Theta) = \\mathbb{E}[\\Theta^2] - (\\mathbb{E}[\\Theta])^2$. Using the moments we already calculated:\n$$\n\\mathrm{Var}(\\Theta) = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{4-3}{12} = \\frac{1}{12}\n$$\nNow, substitute this into the expression for $\\mathrm{Var}(\\mathbb{E}[X|\\Theta])$:\n$$\n\\mathrm{Var}(\\mathbb{E}[X|\\Theta]) = n^2 \\left(\\frac{1}{12}\\right) = \\frac{n^2}{12}\n$$\n\nFinally, we sum the two terms to find the total variance of $X$:\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X|\\Theta)] + \\mathrm{Var}(\\mathbb{E}[X|\\Theta]) = \\frac{n}{6} + \\frac{n^2}{12}\n$$\nTo simplify, we find a common denominator:\n$$\n\\mathrm{Var}(X) = \\frac{2n}{12} + \\frac{n^2}{12} = \\frac{n^2 + 2n}{12}\n$$\nFactoring the numerator gives the final expression:\n$$\n\\mathrm{Var}(X) = \\frac{n(n+2)}{12}\n$$", "answer": "$$\\boxed{\\frac{n(n+2)}{12}}$$", "id": "1929483"}]}