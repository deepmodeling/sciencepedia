{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) is a cornerstone of probability, stating that the sample average of a sequence of independent and identically distributed (i.i.d.) random variables converges to their common expectation. This first practice provides a tangible, geometric context for this powerful theorem, challenging you to find the long-term average distance of points randomly selected from a unit disk. Before applying the SLLN, you'll first need to determine the expected value by deriving the probability distribution of the distance, a key skill in applied probability. [@problem_id:1957055]", "problem": "Consider a sequence of points, $P_1, P_2, P_3, \\dots$, that are chosen independently from the same distribution, formally known as being independent and identically distributed (i.i.d.). The points are selected from a uniform distribution over the unit disk, defined as the set of all points $(x, y)$ in a Cartesian plane satisfying the inequality $x^2 + y^2 \\le 1$.\n\nFor each point $P_n$ in the sequence, let $D_n$ be its Euclidean distance from the center of the disk, the origin $(0,0)$. We are interested in the long-term behavior of the average of these distances. Let $\\bar{D}_N$ be the arithmetic mean of the first $N$ distances, that is, $\\bar{D}_N = \\frac{1}{N} \\sum_{n=1}^{N} D_n$.\n\nDetermine the value to which the average distance $\\bar{D}_N$ converges almost surely (i.e., with probability 1) as the number of points $N$ approaches infinity. Present your answer as a single closed-form analytic expression.", "solution": "Let $\\{P_{n}\\}_{n\\geq 1}$ be i.i.d. uniformly distributed on the unit disk $\\{(x,y): x^{2}+y^{2}\\leq 1\\}$, and let $D_{n}$ be the Euclidean distance from $P_{n}$ to the origin. The strong law of large numbers states that for i.i.d. random variables with finite mean,\n$$\n\\bar{D}_{N}=\\frac{1}{N}\\sum_{n=1}^{N}D_{n}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}],\n$$\nprovided $\\mathbb{E}[|D_{1}|]<\\infty$. Since $0\\leq D_{1}\\leq 1$, we have $\\mathbb{E}[|D_{1}|]<\\infty$. Therefore, it suffices to compute $\\mathbb{E}[D_{1}]$.\n\nLet $R$ denote the distance from the origin for a point uniformly distributed on the unit disk. Then\n$$\n\\mathbb{P}(R\\leq r)=\\frac{\\text{area of the disk of radius }r}{\\text{area of the unit disk}}=\\frac{\\pi r^{2}}{\\pi}=r^{2},\\quad 0\\leq r\\leq 1.\n$$\nDifferentiating gives the radial density\n$$\nf_{R}(r)=\\frac{d}{dr}\\big(r^{2}\\big)=2r,\\quad 0\\leq r\\leq 1.\n$$\nHence,\n$$\n\\mathbb{E}[R]=\\int_{0}^{1}r\\,f_{R}(r)\\,dr=\\int_{0}^{1}r\\cdot 2r\\,dr=\\int_{0}^{1}2r^{2}\\,dr=\\left.\\frac{2}{3}r^{3}\\right|_{0}^{1}=\\frac{2}{3}.\n$$\nBy the strong law of large numbers,\n$$\n\\bar{D}_{N}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}]=\\frac{2}{3}.\n$$\nThus the almost sure limit of the average distance is $\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1957055"}, {"introduction": "Building on the direct application of the SLLN, this exercise introduces an essential companion: the Continuous Mapping Theorem. You will explore a hypothetical scenario where the quantity of interest is not the sample average itself, but a continuous function of it. This practice is crucial for understanding how the convergence guaranteed by the SLLN extends to more complex statistics, a common situation in fields like Monte Carlo simulation and financial modeling. [@problem_id:1957105]", "problem": "A company specializes in running large-scale Monte Carlo simulations to price complex financial derivatives. A key component of their simulation involves repeatedly modeling a binary event, such as a stock price moving up or down. For a particular simulation, this is modeled as a sequence of experiments. In each experiment $i$, a total of $m$ independent Bernoulli trials are run, each with a success probability of $p$. Let the random variable $X_i$ be the total number of successes in experiment $i$.\n\nThe experiments are independent of each other, so the sequence of counts $X_1, X_2, \\ldots, X_n, \\ldots$ can be modeled as a sequence of independent and identically distributed (i.i.d.) random variables, with each $X_i$ following a binomial distribution with parameters $m$ and $p$.\n\nAnalysts at the company are studying the convergence properties of their estimators. They define the average proportion of successes over the first $n$ experiments as:\n$$ \\bar{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{m} $$\nThey are interested in a quantity related to the variance of the underlying process. Determine the value to which the quantity $V_n = \\bar{p}_n (1 - \\bar{p}_n)$ converges almost surely as the number of experiments $n$ approaches infinity. Your answer should be a symbolic expression in terms of the given parameters.", "solution": "Let $X_{i} \\sim \\text{Binomial}(m,p)$ be i.i.d. and define $Y_{i} = \\frac{X_{i}}{m}$. Then the average proportion is the sample mean\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}.\n$$\nWe compute the first two moments of $Y_{i}$ using linearity of expectation and the variance scaling rule:\n$$\n\\mathbb{E}[Y_{i}] = \\frac{1}{m}\\mathbb{E}[X_{i}] = \\frac{1}{m}\\cdot m p = p,\n$$\n$$\n\\operatorname{Var}(Y_{i}) = \\frac{1}{m^{2}}\\operatorname{Var}(X_{i}) = \\frac{1}{m^{2}}\\cdot m p(1-p) = \\frac{p(1-p)}{m}.\n$$\nThus $\\{Y_{i}\\}$ are i.i.d. with finite mean $\\mathbb{E}[Y_{i}]=p$. By the Strong Law of Large Numbers,\n$$\n\\bar{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i} \\xrightarrow{\\text{a.s.}} \\mathbb{E}[Y_{1}] = p.\n$$\n\nDefine the continuous function $g(x) = x(1-x)$. By the continuous mapping theorem applied to almost sure convergence,\n$$\nV_{n} = g(\\bar{p}_{n}) = \\bar{p}_{n}\\bigl(1-\\bar{p}_{n}\\bigr) \\xrightarrow{\\text{a.s.}} g(p) = p(1-p).\n$$\n\nTherefore, $V_{n}$ converges almost surely to $p(1-p)$.", "answer": "$$\\boxed{p(1-p)}$$", "id": "1957105"}, {"introduction": "The simplest form of the SLLN assumes the random variables are independent and identically distributed (i.i.d.), but what happens when these assumptions are relaxed? This advanced problem asks you to analyze the long-term average of a sequence that is not i.i.d. but is constructed from an underlying i.i.d. sequence. Solving it will require a clever decomposition technique, revealing that the power of the SLLN extends beyond the most basic cases and deepening your problem-solving toolkit. [@problem_id:1460782]", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on a common probability space. These variables are characterized by a mean of $E[X_n] = 0$ and a second moment of $E[X_n^2] = 1$ for all $n \\ge 1$.\n\nWe form a new sequence of sample averages defined as $A_n = \\frac{1}{n} \\sum_{i=1}^{n-1} X_i X_{i+1}$ for $n \\ge 2$.\n\nDetermine the value to which the sequence $A_n$ converges almost surely as $n$ approaches infinity.", "solution": "Define $Y_{i} = X_{i}X_{i+1}$ for $i \\ge 1$. Then $A_{n} = \\frac{1}{n} \\sum_{i=1}^{n-1} Y_{i}$ for $n \\ge 2$. By independence of the $X_{i}$ and identical distribution, each $Y_{i}$ has the same distribution, with\n$$\nE[Y_{i}] = E[X_{i}X_{i+1}] = E[X_{i}]\\,E[X_{i+1}] = 0,\n$$\nand\n$$\nE[Y_{i}^{2}] = E[X_{i}^{2}X_{i+1}^{2}] = E[X_{i}^{2}]\\,E[X_{i+1}^{2}] = 1.\n$$\nHence $E[|Y_{i}|] \\le \\sqrt{E[Y_{i}^{2}]} = 1$ by Cauchyâ€“Schwarz, so $Y_{i}$ are integrable.\n\nNext, observe the dependence structure: the family $\\{Y_{i}\\}_{i \\ge 1}$ is $1$-dependent, and more specifically the subsequences $\\{Y_{2j-1}\\}_{j \\ge 1}$ and $\\{Y_{2j}\\}_{j \\ge 1}$ consist of independent random variables because they depend on disjoint sets of the independent $X_{i}$. Moreover, each subsequence is identically distributed with mean $0$ and finite second moment. By the Kolmogorov strong law of large numbers applied separately to each subsequence,\n$$\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j-1} \\to 0 \\quad \\text{a.s.}, \n\\qquad\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j} \\to 0 \\quad \\text{a.s.}\n$$\n\nFor each $n \\ge 2$, let $N_{o}(n)$ be the number of odd indices in $\\{1,\\dots,n-1\\}$ and $N_{e}(n)$ the number of even indices in $\\{1,\\dots,n-1\\}$. Then $N_{o}(n) + N_{e}(n) = n-1$, with $N_{o}(n) = \\lceil (n-1)/2 \\rceil$ and $N_{e}(n) = \\lfloor (n-1)/2 \\rfloor$, so $N_{o}(n)/n \\to \\frac{1}{2}$ and $N_{e}(n)/n \\to \\frac{1}{2}$ deterministically as $n \\to \\infty$. Decompose\n$$\nA_{n} \n= \\frac{1}{n} \\sum_{i=1}^{n-1} Y_{i}\n= \\frac{N_{o}(n)}{n} \\left( \\frac{1}{N_{o}(n)} \\sum_{j=1}^{N_{o}(n)} Y_{2j-1} \\right)\n+ \\frac{N_{e}(n)}{n} \\left( \\frac{1}{N_{e}(n)} \\sum_{j=1}^{N_{e}(n)} Y_{2j} \\right).\n$$\nAs $n \\to \\infty$, we have $N_{o}(n) \\to \\infty$ and $N_{e}(n) \\to \\infty$, so by the almost sure limits of the subsequence averages and the deterministic limits of the weights, it follows that $A_{n} \\to 0$ almost surely.\n\nTherefore, the sequence $A_{n}$ converges almost surely to $0$.", "answer": "$$\\boxed{0}$$", "id": "1460782"}]}