{"hands_on_practices": [{"introduction": "The Central Limit Theorem provides a powerful tool for understanding the cumulative effect of many small, random events. This exercise offers a practical application by modeling a game of chance. By analyzing the sum of gains and losses over many independent plays, you will practice calculating the mean and variance of a discrete random variable and then use the CLT to approximate the probability distribution of a large sum, a fundamental skill in risk assessment and financial modeling. [@problem_id:1394705]", "problem": "An entertainment company has designed a new carnival game called \"Wizard's Wager\". To play one round, a player pays a fee of 3.00 gold coins. The player then draws a single magical stone from a large urn containing a vast number of stones. The stones are of three types:\n\n1.  A Ruby stone, which grants a prize of 15.00 gold coins. The probability of drawing a Ruby is 0.05.\n2.  A Sapphire stone, which grants a prize of 4.00 gold coins. The probability of drawing a Sapphire is 0.25.\n3.  An Obsidian stone, which grants no prize (0.00 gold coins). The probability of drawing an Obsidian stone is 0.70.\n\nA gambler decides to play the game 144 times in a row. Let the \"total loss\" be defined as the total amount paid minus the total prize money won. Assuming the outcome of each game is independent, use a normal approximation to find the probability that the gambler's total loss after 144 games is less than 200.00 gold coins.\n\nExpress your answer as a decimal rounded to four significant figures.", "solution": "Let $L$ denote the loss in one game, defined as payment minus prize. With payment $3$ and prize values $15$ (probability $0.05$), $4$ (probability $0.25$), and $0$ (probability $0.70$), the distribution of $L$ is\n$$\nL=\\begin{cases}\n-12 & \\text{with probability } 0.05,\\\\\n-1 & \\text{with probability } 0.25,\\\\\n3 & \\text{with probability } 0.70.\n\\end{cases}\n$$\nCompute the mean and variance of $L$:\n$$\n\\mu_{L}=\\mathbb{E}[L]=(-12)(0.05)+(-1)(0.25)+(3)(0.70)=1.25,\n$$\n$$\n\\mathbb{E}[L^{2}]=(-12)^{2}(0.05)+(-1)^{2}(0.25)+(3)^{2}(0.70)=13.75,\n$$\n$$\n\\sigma_{L}^{2}=\\operatorname{Var}(L)=\\mathbb{E}[L^{2}]-(\\mathbb{E}[L])^{2}=13.75-(1.25)^{2}=12.1875.\n$$\nLet $S=\\sum_{i=1}^{144}L_{i}$ be the total loss over $144$ independent plays. Then\n$$\n\\mu_{S}=\\mathbb{E}[S]=144\\,\\mu_{L}=144\\cdot 1.25=180,\\qquad \\sigma_{S}^{2}=\\operatorname{Var}(S)=144\\,\\sigma_{L}^{2}=144\\cdot 12.1875=1755.\n$$\nBy the Central Limit Theorem, $S$ is approximately normal with mean $180$ and variance $1755$. The required probability is\n$$\n\\mathbb{P}(S<200)\\approx \\Phi\\!\\left(\\frac{200-\\mu_{S}}{\\sigma_{S}}\\right)=\\Phi\\!\\left(\\frac{20}{\\sqrt{1755}}\\right).\n$$\nNumerically, $\\sqrt{1755}\\approx 41.8927$, so the standardized value is\n$$\nz=\\frac{20}{\\sqrt{1755}}\\approx 0.47741,\n$$\nand therefore\n$$\n\\mathbb{P}(S<200)\\approx \\Phi(0.47741)\\approx 0.6835,\n$$\nrounded to four significant figures.", "answer": "$$\\boxed{0.6835}$$", "id": "1394705"}, {"introduction": "Moving from the sum of random variables to their average is a crucial step in statistical thinking. This problem demonstrates the CLT's application to the sample mean, a cornerstone of statistical inference and quality control. You will explore how the theorem allows us to predict the behavior of a sample average, even when the underlying distribution of individual measurements is unknown, and see its direct relevance in an industrial context. [@problem_id:1959595]", "problem": "A technology company is developing a new type of solid-state battery for portable electronics. The energy capacity of a single production-grade battery cell is a random variable. While the exact distribution of the cell capacity is complex and unknown, extensive testing on the production line has established that the mean capacity is $\\mu = 4250$ milliampere-hours (mAh) and the standard deviation is $\\sigma = 90$ mAh. For quality control, a random sample of $n=36$ cells is selected from each production batch and their capacities are measured. A batch is approved for shipment if the average capacity of the 36 sampled cells exceeds a minimum threshold of $4220$ mAh. Assuming the capacities of the cells are independent and identically distributed, what is the approximate probability that a randomly selected batch will be approved for shipment?\n\nCalculate this probability and round your final answer to four significant figures.", "solution": "Let $X_{1},\\dots,X_{n}$ be iid cell capacities with mean $\\mu=4250$ and standard deviation $\\sigma=90$. The sample mean is $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ with $n=36$. By the Central Limit Theorem, for sufficiently large $n$, the distribution of $\\bar{X}$ is approximately normal with mean $\\mu$ and variance $\\sigma^{2}/n$:\n$$\n\\bar{X}\\approx \\mathcal{N}\\!\\left(\\mu,\\frac{\\sigma^{2}}{n}\\right).\n$$\nThus,\n$$\n\\mathbb{P}(\\bar{X}>4220)=\\mathbb{P}\\!\\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}>\\frac{4220-\\mu}{\\sigma/\\sqrt{n}}\\right).\n$$\nSubstitute $\\mu=4250$, $\\sigma=90$, and $n=36$ to compute the standardized threshold:\n$$\n\\frac{4220-4250}{90/\\sqrt{36}}=\\frac{-30}{90/6}=\\frac{-30}{15}=-2.\n$$\nLet $Z\\sim \\mathcal{N}(0,1)$. Then\n$$\n\\mathbb{P}(\\bar{X}>4220)=\\mathbb{P}(Z>-2)=1-\\Phi(-2)=\\Phi(2),\n$$\nusing the symmetry $\\Phi(-z)=1-\\Phi(z)$ of the standard normal CDF $\\Phi$. Numerically, $\\Phi(2)\\approx 0.977249868\\ldots$, which rounded to four significant figures is $0.9772$.", "answer": "$$\\boxed{0.9772}$$", "id": "1959595"}, {"introduction": "The true power of the Central Limit Theorem lies in its ability to serve as a foundation for more advanced statistical methods. This problem guides you through the use of the Delta Method, a technique that extends the CLT to find the approximate distribution of functions of random variables. By deriving the variance of the log-odds ratio, a critical statistic in medicine and social sciences, you will see how the CLT enables the analysis of complex estimators. [@problem_id:852421]", "problem": "Consider two independent random samples, of sizes $n_1$ and $n_2$ respectively, drawn from two distinct Bernoulli populations. The first population has a success probability of $p_1$, and the second has a success probability of $p_2$. Let $X_1$ and $X_2$ be the number of successes observed in the first and second samples, respectively. The sample proportions are then given by $\\hat{p}_1 = \\frac{X_1}{n_1}$ and $\\hat{p}_2 = \\frac{X_2}{n_2}$.\n\nIn many statistical analyses, particularly in epidemiology and clinical trials, the log-odds ratio is a quantity of significant interest. The true log-odds ratio is defined as $\\theta = \\log\\left(\\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\\right)$. An estimator for this quantity, based on the sample proportions, is the sample log-odds ratio:\n$$\n\\hat{\\theta} = \\log\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\right)\n$$\nAssuming that the sample sizes $n_1$ and $n_2$ are sufficiently large, the Central Limit Theorem can be extended via the Delta Method to find the approximate distribution of $\\hat{\\theta}$.\n\nDerive the asymptotic variance of the log-odds ratio estimator, $\\text{Var}(\\hat{\\theta})$.", "solution": "The problem asks for the asymptotic variance of the log-odds ratio estimator $\\hat{\\theta}$. We can find this using the multivariate Delta Method.\n\nFirst, by the Central Limit Theorem, for large sample sizes $n_1$ and $n_2$, the sample proportions $\\hat{p}_1$ and $\\hat{p}_2$ are approximately normally distributed:\n$$\n\\hat{p}_1 \\approx N\\left(p_1, \\frac{p_1(1-p_1)}{n_1}\\right)\n$$\n$$\n\\hat{p}_2 \\approx N\\left(p_2, \\frac{p_2(1-p_2)}{n_2}\\right)\n$$\nSince the two samples are independent, the random variables $\\hat{p}_1$ and $\\hat{p}_2$ are also independent. Therefore, the vector of sample proportions $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)^T$ has an asymptotic bivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\boldsymbol{\\mu} = E[\\hat{\\mathbf{p}}] = \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma} = \\text{Cov}(\\hat{\\mathbf{p}}) = \\begin{pmatrix} \\text{Var}(\\hat{p}_1) & \\text{Cov}(\\hat{p}_1, \\hat{p}_2) \\\\ \\text{Cov}(\\hat{p}_1, \\hat{p}_2) & \\text{Var}(\\hat{p}_2) \\end{pmatrix} = \\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1} & 0 \\\\ 0 & \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\nThe estimator $\\hat{\\theta}$ is a function of $\\hat{p}_1$ and $\\hat{p}_2$. Let this function be $g(x, y)$:\n$$\ng(x, y) = \\log\\left(\\frac{x/(1-x)}{y/(1-y)}\\right) = \\log(x) - \\log(1-x) - \\log(y) + \\log(1-y)\n$$\nThe multivariate Delta Method states that the asymptotic variance of a function $g(\\hat{\\mathbf{p}})$ is given by:\n$$\n\\text{Var}(g(\\hat{\\mathbf{p}})) \\approx (\\nabla g(\\boldsymbol{\\mu}))^T \\boldsymbol{\\Sigma} (\\nabla g(\\boldsymbol{\\mu}))\n$$\nwhere $\\nabla g(\\boldsymbol{\\mu})$ is the gradient of $g$ evaluated at the mean vector $\\boldsymbol{\\mu} = (p_1, p_2)^T$.\n\nFirst, we compute the gradient of $g(x, y)$:\n$$\n\\frac{\\partial g}{\\partial x} = \\frac{1}{x} - \\frac{1}{1-x}(-1) = \\frac{1}{x} + \\frac{1}{1-x} = \\frac{1-x+x}{x(1-x)} = \\frac{1}{x(1-x)}\n$$\n$$\n\\frac{\\partial g}{\\partial y} = -\\frac{1}{y} + \\frac{1}{1-y}(-1) = -\\frac{1}{y} - \\frac{1}{1-y} = -\\frac{1-y+y}{y(1-y)} = -\\frac{1}{y(1-y)}\n$$\nSo the gradient vector is $\\nabla g(x,y) = \\left(\\frac{1}{x(1-x)}, -\\frac{1}{y(1-y)}\\right)^T$.\n\nNext, we evaluate the gradient at the mean $\\boldsymbol{\\mu} = (p_1, p_2)^T$:\n$$\n\\nabla g(\\boldsymbol{\\mu}) = \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nNow, we can substitute the gradient and the covariance matrix into the Delta Method formula for the variance:\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} & -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n\\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1} & 0 \\\\ 0 & \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nWe perform the matrix multiplication from left to right. First, multiply the row vector (the transpose of the gradient) by the covariance matrix:\n$$\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\cdot \\frac{p_1(1-p_1)}{n_1} + (-\\frac{1}{p_2(1-p_2)}) \\cdot 0 & \\frac{1}{p_1(1-p_1)} \\cdot 0 + (-\\frac{1}{p_2(1-p_2)}) \\cdot \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\frac{1}{n_1} & -\\frac{1}{n_2} \\end{pmatrix}\n$$\nFinally, multiply this resulting row vector by the column vector (the gradient):\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \\begin{pmatrix} \\frac{1}{n_1} & -\\frac{1}{n_2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\n$$\n= \\left(\\frac{1}{n_1}\\right) \\left(\\frac{1}{p_1(1-p_1)}\\right) + \\left(-\\frac{1}{n_2}\\right) \\left(-\\frac{1}{p_2(1-p_2)}\\right)\n$$\n$$\n= \\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}\n$$\nThis is the asymptotic variance of the log-odds ratio estimator.", "answer": "$$\n\\boxed{\\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}}\n$$", "id": "852421"}]}