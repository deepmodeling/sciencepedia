{"hands_on_practices": [{"introduction": "The journey into probabilistic bounds often begins with a simple question: if we only know the average value of a non-negative quantity, what can we say about the chances of observing a very large value? Markov's inequality provides a surprisingly powerful answer. This first practice serves as a direct application, demonstrating how to compute a \"worst-case\" probability bound in a practical scenario, using nothing more than the expected value [@problem_id:1316830].", "problem": "A financial technology firm has developed a high-frequency trading algorithm. Let the random variable $T$ represent the execution time, in milliseconds, for a single trade cycle. The only information known about the performance of this algorithm, based on extensive simulations and historical data, is that the expected execution time $E[T]$ is 50 milliseconds. The execution time $T$ is inherently a non-negative quantity.\n\nA trade cycle is flagged for review if its execution time exceeds a critical threshold. The firm's risk analysts are interested in the worst-case scenario for a cycle taking at least 250 milliseconds, which could indicate a significant system lag or an anomaly.\n\nWithout making any further assumptions about the probability distribution of $T$, determine the maximum possible probability that a randomly selected trade cycle will take at least 250 milliseconds to execute. Express your answer as a decimal.", "solution": "The problem asks for the maximum possible probability of an event, given only the expectation of a non-negative random variable. This scenario is a direct application of Markov's inequality.\n\nLet $T$ be the non-negative random variable representing the execution time in milliseconds. We are given its expected value:\n$$E[T] = 50$$\n\nWe want to find the maximum possible value of the probability $P(T \\ge 250)$.\n\nMarkov's inequality states that for any non-negative random variable $X$ and any constant $a > 0$, the following relationship holds:\n$$P(X \\ge a) \\le \\frac{E[X]}{a}$$\nThis inequality provides an upper bound on the probability that the random variable $X$ is greater than or equal to some value $a$.\n\nIn our problem, the random variable is $T$, and the constant is $a = 250$. We can apply Markov's inequality by substituting these values:\n$$P(T \\ge 250) \\le \\frac{E[T]}{250}$$\n\nNow, we substitute the given expected value, $E[T] = 50$:\n$$P(T \\ge 250) \\le \\frac{50}{250}$$\n\nSimplifying the fraction gives the upper bound on the probability:\n$$P(T \\ge 250) \\le \\frac{1}{5}$$\n$$P(T \\ge 250) \\le 0.2$$\n\nThis result means that the probability of the execution time being 250 ms or greater cannot exceed 0.2, regardless of the specific shape of the probability distribution for $T$. Since the question asks for the maximum possible probability, the answer is this upper bound. This bound is tight, meaning there exists a probability distribution for which this probability is exactly 0.2 while satisfying the given expectation. For instance, consider a discrete distribution where $T=250$ with probability $p=0.2$ and $T=0$ with probability $1-p=0.8$. The expectation for this distribution is $E[T] = 250 \\cdot (0.2) + 0 \\cdot (0.8) = 50$, which matches the given information.\n\nTherefore, the maximum possible probability is 0.2.", "answer": "$$\\boxed{0.2}$$", "id": "1316830"}, {"introduction": "After applying a bound like Markov's inequality, it's crucial to ask whether the limit it provides is just a theoretical ceiling or a value that can actually be reached. This exercise delves into the concept of *sharpness* by challenging you to construct a scenario where the inequality holds as a perfect equality. By solving this, you will uncover the specific structure a probability distribution must have to represent the \"worst-case\" scenario that the inequality describes, deepening your understanding far beyond simple formula application [@problem_id:1933114].", "problem": "A factory uses a machine whose daily power consumption, modeled as a random variable $X$, can be in one of three states: an idle state with consumption $0$ kWh, a standard operating state with consumption $C_1$ kWh, and a high-performance state with consumption $C_2$ kWh, where $C_1$ and $C_2$ are positive constants. The probabilities for these states are $P(X=0)=p_0$, $P(X=C_1)=p_1$, and $P(X=C_2)=p_2$. These probabilities are non-negative and sum to 1.\n\nIt is known that $C_2$ is an integer multiple of $C_1$, such that $C_2 = k C_1$ for some integer $k > 1$. Through long-term monitoring, the following two statistical properties of the machine's daily consumption have been established:\n1. The mean consumption is $E[X] = 2 C_1$.\n2. The variance of the consumption is $\\text{Var}(X) = 16 C_1^2$.\n\nFurthermore, a peculiar property is discovered. For an energy threshold $a = C_2$, the probabilistic bound given by Markov's inequality, $P(X \\ge a) \\le \\frac{E[X]}{a}$, is not just a bound but holds as an exact equality.\n\nDetermine the integer value of $k$.", "solution": "Let $X$ take values $0$, $C_{1}$, and $C_{2}=k C_{1}$ with probabilities $p_{0}$, $p_{1}$, and $p_{2}$, respectively, where $p_{0}+p_{1}+p_{2}=1$, $C_{1}>0$, and $k>1$ is an integer. The mean and variance are given by\n$$\nE[X]=p_{1}C_{1}+p_{2}C_{2}=2C_{1},\n$$\n$$\n\\operatorname{Var}(X)=E[X^{2}]-\\left(E[X]\\right)^{2}=\\left(p_{1}C_{1}^{2}+p_{2}C_{2}^{2}\\right)-(2C_{1})^{2}=16 C_{1}^{2}.\n$$\n\nMarkov's inequality for a non-negative random variable $X$ at threshold $a$ holds with equality if and only if the random variable takes on only the values 0 and $a$. In our case, the threshold is $a=C_{2}$. For equality to hold, the probability mass of $X$ must be concentrated at points 0 and $C_2$. Since $C_1$ is a possible value between 0 and $C_2$ (as $k>1$), its probability must be zero. Therefore, $p_{1}=0$, and $X$ takes only the values $0$ and $C_{2}$ with probabilities $p_{0}$ and $p_{2}$ respectively, where $p_0 + p_2 = 1$.\n\nWith $p_{1}=0$, the mean condition becomes\n$$\nE[X]=p_{2}C_{2}=2C_{1}\\quad\\Longrightarrow\\quad p_{2}=\\frac{2C_{1}}{C_{2}}=\\frac{2}{k}.\n$$\nThe variance for a two-point distribution on $\\{0, C_2\\}$ is $\\operatorname{Var}(X)=p_{2}(1-p_{2})C_{2}^{2}$.\nSubstituting $p_{2}=\\frac{2}{k}$ and $C_{2}=k C_{1}$ yields\n$$\n\\operatorname{Var}(X)=\\frac{2}{k}\\left(1-\\frac{2}{k}\\right)(k C_{1})^{2}\n= \\frac{2}{k}\\left(\\frac{k-2}{k}\\right)k^{2} C_{1}^{2}\n= 2(k-2) C_{1}^{2}.\n$$\nImposing the given variance $16 C_{1}^{2}$ and using $C_{1}>0$ gives\n$$\n2(k-2) C_{1}^{2}=16 C_{1}^{2}\\quad\\Longrightarrow\\quad 2(k-2)=16\\quad\\Longrightarrow\\quad k-2=8\\quad\\Longrightarrow\\quad k=10.\n$$\nWe can verify this result. If $k=10$, then $p_2=2/10=1/5$, which is a valid probability. The equality condition for Markov's inequality requires $P(X \\ge C_2) = E[X]/C_2$. Here, $P(X \\ge C_2) = p_2 = 1/5$, and $E[X]/C_2 = (2C_1)/(10C_1) = 1/5$. The condition holds.", "answer": "$$\\boxed{10}$$", "id": "1933114"}, {"introduction": "The true elegance of fundamental concepts like Markov's inequality lies in their ability to serve as building blocks for more sophisticated tools. This advanced practice demonstrates a powerful technique used throughout probability theory: applying a basic inequality to a cleverly chosen function of a random variable and then optimizing the result. By following this method, you will derive Cantelli's inequality, a famous \"one-sided\" version of Chebyshev's inequality, and gain firsthand experience in the creative art of constructing new theoretical bounds [@problem_id:1933101].", "problem": "Let $X$ be a random variable with a finite mean $\\mu$ and a finite, non-zero variance $\\sigma^2$. We wish to establish a \"one-sided\" version of Chebyshev's inequality, providing an upper bound for the probability of a large positive deviation of $X$ from its mean.\n\nTo achieve this, we will use a specific application of Markov's inequality. Markov's inequality states that for any non-negative random variable $Z$ and any constant $a > 0$, the probability $P(Z \\ge a)$ is bounded by $P(Z \\ge a) \\le \\frac{E[Z]}{a}$.\n\nConsider the auxiliary random variable $Y = (X - \\mu + c)^2$, where $c$ is a real-valued parameter that can be chosen freely. For a given constant $k > 0$, the event $\\{X - \\mu \\ge k\\}$ is a subset of the event $\\{Y \\ge (k+c)^2\\}$, provided that $k+c > 0$. By applying Markov's inequality to the random variable $Y$ with a suitably chosen value for $a$, one can derive an upper bound for $P(X - \\mu \\ge k)$ that depends on the parameter $c$.\n\nYour task is to find the tightest possible upper bound for $P(X - \\mu \\ge k)$ that can be obtained through this method. To do this, you must first find the optimal value of the parameter $c$ that minimizes the bound, and then substitute this optimal $c$ back into your bound expression. Express this final, minimized bound as an analytic expression in terms of $\\sigma$ and $k$.", "solution": "Let $X$ have mean $\\mu$ and variance $\\sigma^{2}\\in(0,\\infty)$. Fix $k>0$ and choose a real parameter $c$. Define the nonnegative random variable $Y=(X-\\mu+c)^{2}$. For $k+c>0$, the implication $X-\\mu\\geq k\\implies X-\\mu+c\\geq k+c>0$ yields\n$$\n\\{X-\\mu\\geq k\\}\\subset\\{Y\\geq(k+c)^{2}\\}.\n$$\nBy Markov's inequality applied to $Y$ with $a=(k+c)^{2}$,\n$$\n\\mathbb{P}(X-\\mu\\geq k)\\leq\\mathbb{P}\\big(Y\\geq(k+c)^{2}\\big)\\leq\\frac{\\mathbb{E}[Y]}{(k+c)^{2}}.\n$$\nCompute $\\mathbb{E}[Y]$ using $\\mathbb{E}[X-\\mu]=0$ and $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$:\n$$\n\\mathbb{E}[Y]=\\mathbb{E}[(X-\\mu+c)^{2}]=\\mathbb{E}[(X-\\mu)^{2}]+2c\\,\\mathbb{E}[X-\\mu]+c^{2}=\\sigma^{2}+c^{2}.\n$$\nTherefore, for any $c>-k$,\n$$\n\\mathbb{P}(X-\\mu\\geq k)\\leq f(c):=\\frac{\\sigma^{2}+c^{2}}{(k+c)^{2}}.\n$$\nTo obtain the tightest bound via this method, minimize $f(c)$ over $c>-k$. Differentiate with respect to $c$:\n$$\nf'(c)=\\frac{2c(k+c)^{2}-2(k+c)(\\sigma^{2}+c^{2})}{(k+c)^{4}}=\\frac{2(k+c)\\big(c(k+c) - (\\sigma^{2}+c^{2})\\big)}{(k+c)^{4}} = \\frac{2(ck-\\sigma^{2})}{(k+c)^{3}}.\n$$\nSetting $f'(c)=0$ gives the unique critical point (since $k>0$) at\n$$\nc^{*}=\\frac{\\sigma^{2}}{k},\n$$\nwhich satisfies the condition $c^{*}>-k$ because $\\sigma^2>0$ and $k>0$. Evaluate $f$ at this optimal value $c^{*}$:\n$$\nf(c^{*})=\\frac{\\sigma^{2}+\\left(\\frac{\\sigma^{2}}{k}\\right)^{2}}{\\left(k+\\frac{\\sigma^{2}}{k}\\right)^{2}}=\\frac{\\sigma^{2}+\\frac{\\sigma^{4}}{k^{2}}}{\\left(\\frac{k^{2}+\\sigma^{2}}{k}\\right)^{2}}=\\frac{\\frac{\\sigma^{2}k^{2}+\\sigma^{4}}{k^2}}{\\frac{(k^{2}+\\sigma^{2})^{2}}{k^2}}=\\frac{\\sigma^{2}(k^{2}+\\sigma^{2})}{(k^{2}+\\sigma^{2})^{2}}=\\frac{\\sigma^{2}}{k^{2}+\\sigma^{2}}.\n$$\nThus the minimized (tightest via this method) upper bound is\n$$\n\\mathbb{P}(X-\\mu\\geq k)\\leq\\frac{\\sigma^{2}}{\\sigma^{2}+k^{2}}.\n$$", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{\\sigma^{2}+k^{2}}}$$", "id": "1933101"}]}