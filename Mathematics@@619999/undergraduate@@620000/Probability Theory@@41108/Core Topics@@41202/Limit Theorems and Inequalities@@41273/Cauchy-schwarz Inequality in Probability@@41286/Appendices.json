{"hands_on_practices": [{"introduction": "One of the most direct and powerful applications of the Cauchy-Schwarz inequality in probability is to establish an upper bound on the interaction between two random variables. This practice explores this fundamental use by calculating the maximum possible value for the expectation of a product, $E[XY]$, given the first and second moments of $X$ and $Y$. Mastering this technique is crucial for understanding the limits of covariance and correlation, which are cornerstone concepts in statistics and many applied fields. [@problem_id:1347664]", "problem": "In the study of coupled oscillating systems, the statistical properties of two state variables, represented by random variables $X$ and $Y$, are analyzed. Experimental measurements have established the following expected values:\nThe mean of $X$ is $E[X] = 0.1$.\nThe mean of $Y$ is $E[Y] = 0.2$.\nThe second moment of $X$ is $E[X^2] = 0.05$.\nThe second moment of $Y$ is $E[Y^2] = 0.13$.\nThe interaction between the two variables is characterized by the expectation of their product, $E[XY]$. According to the fundamental principles of probability theory, there is a maximum possible value for this interaction term.\nCalculate the maximum possible value of $E[XY]$ based on the given data. Express your answer as a single real number, rounded to three significant figures.", "solution": "We are given $E[X]=0.1$, $E[Y]=0.2$, $E[X^{2}]=0.05$, and $E[Y^{2}]=0.13$. First compute the variances using $\\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}$ and $\\operatorname{Var}(Y)=E[Y^{2}]-(E[Y])^{2}$:\n$$\\operatorname{Var}(X)=0.05-(0.1)^{2}=0.05-0.01=0.04,$$\n$$\\operatorname{Var}(Y)=0.13-(0.2)^{2}=0.13-0.04=0.09.$$\nWrite $E[XY]$ in terms of the covariance:\n$$E[XY]=\\operatorname{Cov}(X,Y)+E[X]E[Y].$$\nBy the Cauchyâ€“Schwarz inequality applied to the centered variables,\n$$|\\operatorname{Cov}(X,Y)| \\leq \\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}.$$\nTherefore the maximum possible value of $E[XY]$ is attained when $\\operatorname{Cov}(X,Y)=+\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}$, giving\n$$E[XY]_{\\max}=E[X]E[Y]+\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}=0.1\\cdot 0.2+\\sqrt{0.04\\cdot 0.09}=0.02+0.06=0.08.$$\nThis bound is attainable when $Y-E[Y]$ is a positive scalar multiple of $X-E[X]$, so it is the maximum consistent with the given moments. Rounding to three significant figures yields $0.0800$.", "answer": "$$\\boxed{0.0800}$$", "id": "1347664"}, {"introduction": "Beyond its standard applications, the Cauchy-Schwarz inequality serves as a versatile tool for deriving non-obvious bounds through creative problem formulation. This exercise challenges you to find a lower bound for the expectation $E[\\frac{1}{X+1}]$ by ingeniously defining two new random variables to which the inequality can be applied. This approach demonstrates a key problem-solving strategy: transforming a difficult problem into a simpler one where a known tool is effective. [@problem_id:1347651]", "problem": "Let $X$ be a random variable representing the number of successes in $n$ independent Bernoulli trials, where the probability of success in any given trial is $p$, with $0 < p < 1$. Consider a new random variable $Y$ defined as $Y = \\frac{1}{X+1}$. Determine a simple expression that serves as a lower bound for the expected value of $Y$, denoted $E[Y]$. Express your answer as a function of $n$ and $p$.", "solution": "Let $X \\sim \\mathrm{Bin}(n,p)$ with $0<p<1$, and define $Y=\\frac{1}{X+1}$. We seek a lower bound for $E[Y]$ in terms of $n$ and $p$.\nWe can use the Cauchy-Schwarz inequality, which states that for any two random variables $U$ and $V$, $(E[UV])^2 \\le E[U^2]E[V^2]$.\nLet's choose our variables creatively. Define $U = \\sqrt{X+1}$ and $V = \\frac{1}{\\sqrt{X+1}}$. Both are well-defined since $X \\ge 0$, so $X+1 \\ge 1$.\n\nNow let's compute the terms for the inequality:\n1.  The product $UV$ is $\\sqrt{X+1} \\cdot \\frac{1}{\\sqrt{X+1}} = 1$. So, its expectation is $E[UV] = E[1] = 1$.\n2.  The square of $U$ is $U^2 = (\\sqrt{X+1})^2 = X+1$. Its expectation is $E[U^2] = E[X+1] = E[X]+1$. For a binomial random variable, $E[X]=np$. Thus, $E[U^2] = np+1$.\n3.  The square of $V$ is $V^2 = \\left(\\frac{1}{\\sqrt{X+1}}\\right)^2 = \\frac{1}{X+1}$. Its expectation is what we want to bound: $E[V^2] = E\\left[\\frac{1}{X+1}\\right]$.\n\nSubstituting these into the Cauchy-Schwarz inequality:\n$$\n(E[UV])^2 \\le E[U^2]E[V^2]\n$$\n$$\n1^2 \\le (np+1) \\cdot E\\left[\\frac{1}{X+1}\\right]\n$$\nSince $n \\ge 1$ and $p>0$, $np+1 > 1$, so we can divide by it without changing the inequality's direction:\n$$\n\\frac{1}{np+1} \\le E\\left[\\frac{1}{X+1}\\right]\n$$\nThis gives the desired simple lower bound as a function of $n$ and $p$.", "answer": "$$\\boxed{\\frac{1}{np+1}}$$", "id": "1347651"}, {"introduction": "The Cauchy-Schwarz inequality can reveal profound connections between seemingly disparate properties of a random variable, such as its moments and its probability distribution. In this problem, you will use the inequality in conjunction with an indicator random variable to derive a tight lower bound on the probability $P(X \\gt 0)$ using only the first two moments of $X$. This powerful result, a specific case of the Paley-Zygmund inequality, is invaluable in scenarios where full distributional information is unavailable but moments can be measured or estimated. [@problem_id:1347643]", "problem": "A team of physicists is studying a quantum system where the energy of an emitted particle, represented by the non-negative random variable $X$, can fluctuate. The experimental setup allows for the precise measurement of the first two moments of the energy distribution. Let the mean energy be $E[X] = M_1$ and the mean squared energy be $E[X^2] = M_2$. It is known that the particle is not always at rest, so $M_1 > 0$.\n\nA crucial aspect of the system's stability depends on the particle having non-zero energy. The team needs to establish a guaranteed minimum probability that a particle emission has an energy greater than zero. Without making any other assumptions about the probability distribution of $X$, derive a general, tight lower bound for the probability $P(X > 0)$.\n\nExpress your answer as a closed-form analytic expression in terms of $M_1$ and $M_2$.", "solution": "Let $X$ be the non-negative random variable representing the particle's energy. We are given its first and second moments, $E[X] = M_1$ and $E[X^2] = M_2$. We are also given that $M_1 > 0$. Our goal is to find a lower bound for the probability $P(X > 0)$.\n\nFirst, we can express the probability $P(X > 0)$ as the expectation of an indicator random variable. Let $I_{X>0}$ be an indicator variable such that:\n$$\nI_{X>0} = \\begin{cases} 1 & \\text{if } X > 0 \\\\ 0 & \\text{if } X = 0 \\end{cases}\n$$\nBy the definition of expectation for a discrete random variable, the expected value of $I_{X>0}$ is:\n$$\nE[I_{X>0}] = 1 \\cdot P(I_{X>0} = 1) + 0 \\cdot P(I_{X>0} = 0) = P(X > 0)\n$$\nSo, our goal is to find a lower bound for $E[I_{X>0}]$.\n\nWe will use the Cauchy-Schwarz inequality for expectations. For any two random variables $Y$ and $Z$, the inequality states:\n$$\n(E[YZ])^2 \\le E[Y^2]E[Z^2]\n$$\n\nTo apply this inequality, we need to choose $Y$ and $Z$ appropriately. A clever choice involves connecting the random variable $X$ to the indicator $I_{X>0}$. Since $X$ is non-negative, we can write the identity $X = X \\cdot I_{X>0}$. This is because if $X>0$, $I_{X>0}=1$ and the identity is $X=X$. If $X=0$, both sides are zero.\n\nNow, let's take the expectation of this identity:\n$$\nE[X] = E[X \\cdot I_{X>0}]\n$$\nThis gives us a relationship between $M_1$ and the product of $X$ and our indicator. We can now apply the Cauchy-Schwarz inequality by setting $Y = X$ and $Z = I_{X>0}$.\n\nSubstituting these into the Cauchy-Schwarz inequality:\n$$\n(E[X \\cdot I_{X>0}])^2 \\le E[X^2] E[I_{X>0}^2]\n$$\n\nLet's evaluate each term in this inequality.\nThe left-hand side is $(E[X])^2 = M_1^2$.\nThe first term on the right-hand side is $E[X^2] = M_2$.\nThe second term on the right-hand side is $E[I_{X>0}^2]$. Since an indicator variable can only be 0 or 1, its square is equal to itself: $I_{X>0}^2 = I_{X>0}$. Therefore:\n$$\nE[I_{X>0}^2] = E[I_{X>0}] = P(X > 0)\n$$\n\nSubstituting these back into the inequality, we get:\n$$\nM_1^2 \\le M_2 \\cdot P(X > 0)\n$$\n\nSince we are given $M_1 > 0$, this implies that $X$ cannot be the zero random variable, so there must be some probability mass at values greater than zero. This ensures that $E[X^2] = M_2 > 0$ (if $M_2=0$, then $X=0$ almost surely, which would mean $M_1=0$, a contradiction). Thus, we can divide by $M_2$ without changing the direction of the inequality:\n$$\n\\frac{M_1^2}{M_2} \\le P(X > 0)\n$$\n\nThis provides the desired lower bound for the probability $P(X > 0)$. The bound is tight because one can construct a two-point distribution that achieves this bound.", "answer": "$$\\boxed{\\frac{M_1^2}{M_2}}$$", "id": "1347643"}]}