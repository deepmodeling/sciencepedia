{"hands_on_practices": [{"introduction": "This first exercise lays the groundwork for understanding the multinomial distribution. We start with a familiar scenario—rolling a die—and see how it gives rise to a multinomial experiment when outcomes are sorted into more than two categories. The practice [@problem_id:12523] will guide you through the two essential steps: defining the probabilities for each distinct category and then applying the multinomial formula to calculate the probability of a specific combinatorial result.", "problem": "Consider a fair die with $m$ faces, labeled with the integers $\\{1, 2, \\dots, m\\}$. The die is rolled $n$ independent times. The outcomes of the rolls are classified into three distinct categories based on two integer thresholds, $a$ and $b$, where $1 \\le a < b < m$.\n\nThe categories are defined as follows for a single roll:\n- **Category 1**: The outcome is less than or equal to $a$.\n- **Category 2**: The outcome is greater than $a$ and less than or equal to $b$.\n- **Category 3**: The outcome is greater than $b$.\n\nDerive a general expression for the probability of observing exactly $n_1$ outcomes in Category 1, $n_2$ outcomes in Category 2, and $n_3$ outcomes in Category 3, where $n_1 + n_2 + n_3 = n$. Express your answer in terms of $n$, $m$, $a$, $b$, $n_1$, $n_2$, and $n_3$.", "solution": "We partition each roll into three categories with probabilities  \n$$p_1=\\frac{a}{m},\\quad p_2=\\frac{b-a}{m},\\quad p_3=\\frac{m-b}{m},$$  \nand note that $p_1+p_2+p_3=1$ and $n_1+n_2+n_3=n$.  \n\nBy the multinomial distribution, the probability of observing exactly $n_1$ outcomes in Category 1, $n_2$ in Category 2, and $n_3$ in Category 3 is  \n$$\n\\frac{n!}{n_1!\\,n_2!\\,n_3!}\\,p_1^{n_1}\\,p_2^{n_2}\\,p_3^{n_3}.\n$$  \nSubstituting the expressions for $p_1,p_2,p_3$ yields the desired formula.", "answer": "$$\\boxed{\\frac{n!}{n_1!n_2!n_3!}\\Bigl(\\frac{a}{m}\\Bigr)^{n_1}\\Bigl(\\frac{b-a}{m}\\Bigr)^{n_2}\\Bigl(\\frac{m-b}{m}\\Bigr)^{n_3}}$$", "id": "12523"}, {"introduction": "In many practical applications, we may initially collect data with a fine level of detail but later decide to group certain categories together for a simpler analysis. This exercise [@problem_id:12547] explores a key structural property of the multinomial distribution: what happens when we combine outcomes. You will discover that collapsing categories in a multinomial experiment elegantly results in a new, valid multinomial distribution, demonstrating the model's flexibility and internal consistency.", "problem": "An experiment consists of $n$ independent trials. Each trial can result in one of four mutually exclusive outcomes, labeled Category 1, Category 2, Category 3, and Category 4. For any given trial, the probability of the outcome being Category $i$ is $p_i$, where $i \\in \\{1, 2, 3, 4\\}$. The probabilities are constant for all trials, and they sum to one: $p_1 + p_2 + p_3 + p_4 = 1$.\n\nThis scenario is described by a multinomial distribution. The random variables $X_1, X_2, X_3, X_4$ represent the number of times each category is observed in the $n$ trials, where $\\sum_{i=1}^4 X_i = n$.\n\nNow, suppose we are no longer interested in distinguishing between Category 1 and Category 2. We decide to group them into a single new category, let's call it \"Group A\". The other two categories remain distinct.\n\nLet the random variable $X_A$ be the number of outcomes in Group A, $X_3$ be the number of outcomes in Category 3, and $X_4$ be the number of outcomes in Category 4.\n\nDerive the probability mass function for the specific outcome of observing $x_A$ items in Group A, $x_3$ items in Category 3, and $x_4$ items in Category 4, where $x_A + x_3 + x_4 = n$. Express your answer in terms of $n$, $x_A$, $x_3$, $x_4$, and the initial probabilities $p_1, p_2, p_3, p_4$.", "solution": "We start from the four-category multinomial pmf for counts $x_1,x_2,x_3,x_4$ with $\\sum_{i=1}^4x_i=n$:\n$$\nP(X_1=x_1,X_2=x_2,X_3=x_3,X_4=x_4)\n=\\frac{n!}{x_1!\\,x_2!\\,x_3!\\,x_4!}\\,p_1^{x_1}p_2^{x_2}p_3^{x_3}p_4^{x_4}.\n$$\nWe wish to marginalize over $X_1,X_2$ subject to $x_1+x_2=x_A$. Summing over all splits,\n$$\nP(X_A=x_A,X_3=x_3,X_4=x_4)\n=\\sum_{x_1+x_2=x_A}\\frac{n!}{x_1!\\,x_2!\\,x_3!\\,x_4!}\\,p_1^{x_1}p_2^{x_2}p_3^{x_3}p_4^{x_4}.\n$$\nFactor out terms not depending on $x_1,x_2$ and note the binomial expansion:\n$$\nP=\\frac{n!}{x_3!\\,x_4!}\\,p_3^{x_3}p_4^{x_4}\n\\sum_{x_1+x_2=x_A}\\frac{1}{x_1!\\,x_2!}p_1^{x_1}p_2^{x_2}\n=\\frac{n!}{x_A!\\,x_3!\\,x_4!}\\,(p_1+p_2)^{x_A}p_3^{x_3}p_4^{x_4}.\n$$\nThis is the pmf for the grouped counts $(X_A,X_3,X_4)$ with $x_A+x_3+x_4=n$.", "answer": "$$\\boxed{\\frac{n!}{x_A!\\,x_3!\\,x_4!}(p_1+p_2)^{x_A}p_3^{x_3}p_4^{x_4}}$$", "id": "12547"}, {"introduction": "Our understanding of probabilities often changes as we receive new information. This problem [@problem_id:1402359] investigates the concept of conditional probability in the context of a multinomial experiment, a scenario frequently encountered in fields like quality assurance. By working through this, you will uncover the powerful principle that if we are given that an outcome belongs to a specific subset of categories, the distribution of counts *within* that subset follows a new, simpler probability model.", "problem": "In a semiconductor fabrication plant, microprocessors are produced and subjected to a quality control test. Each chip is independently categorized into one of four mutually exclusive states: 'Pass' (meets all specifications), 'Minor Flaw' (functional but with minor deviations), 'Major Flaw' (non-functional), or 'Test Error' (test result is inconclusive).\n\nFrom extensive historical data, the probabilities for each outcome are known:\n- The probability of a chip being 'Pass' is $p_P$.\n- The probability of a chip having a 'Minor Flaw' is $p_m$.\n- The probability of a chip having a 'Major Flaw' is $p_M$.\n- The probability of a 'Test Error' is $p_U$.\nThese probabilities sum to one: $p_P + p_m + p_M + p_U = 1$.\n\nA quality assurance engineer inspects a randomly selected batch of $n$ microprocessors. Upon reviewing the test results, the engineer finds that a total of exactly $k$ processors in this batch are flawed (i.e., they have either a 'Minor Flaw' or a 'Major Flaw'). Given this specific information about the batch, determine the probability that exactly $j$ of these $k$ flawed processors have a 'Minor Flaw', where $0 \\le j \\le k$.\n\nProvide your answer as a single closed-form analytic expression in terms of $j$, $k$, $p_m$, and $p_M$.", "solution": "Let $N_{m}$ and $N_{M}$ denote the counts of chips with 'Minor Flaw' and 'Major Flaw' in the batch of $n$ chips, respectively. Define $p_{f} = p_{m} + p_{M}$ as the total probability of a chip being flawed and $p_{o} = 1 - p_{f} = p_{P} + p_{U}$ as the probability of being non-flawed or test error. The triplet $(N_{m}, N_{M}, N_{o})$ with $N_{o} = n - N_{m} - N_{M}$ follows a multinomial distribution with parameters $n$ and category probabilities $(p_{m}, p_{M}, p_{o})$.\n\nWe are given the event $A = \\{N_{m} + N_{M} = k\\}$. We want\n$$\n\\Pr(N_{m} = j \\mid A) = \\frac{\\Pr(N_{m} = j, N_{M} = k - j)}{\\Pr(N_{m} + N_{M} = k)}.\n$$\nThe numerator is the multinomial probability with $N_{o} = n - k$:\n$$\n\\Pr(N_{m} = j, N_{M} = k - j) = \\frac{n!}{j!\\,(k - j)!\\,(n - k)!}\\,p_{m}^{j}\\,p_{M}^{k - j}\\,p_{o}^{n - k}.\n$$\nThe denominator depends only on the sum $N_{f} = N_{m} + N_{M}$, which is binomial with parameters $(n, p_{f})$:\n$$\n\\Pr(N_{m} + N_{M} = k) = \\binom{n}{k} p_{f}^{k} (1 - p_{f})^{n - k} = \\binom{n}{k} p_{f}^{k} p_{o}^{n - k}.\n$$\nTherefore,\n$$\n\\Pr(N_{m} = j \\mid N_{m} + N_{M} = k)\n= \\frac{\\frac{n!}{j!\\,(k - j)!\\,(n - k)!}\\,p_{m}^{j}\\,p_{M}^{k - j}\\,p_{o}^{n - k}}{\\binom{n}{k} p_{f}^{k} p_{o}^{n - k}}\n= \\frac{\\frac{n!}{j!\\,(k - j)!\\,(n - k)!}}{\\frac{n!}{k!\\,(n - k)!}} \\left(\\frac{p_{m}}{p_{f}}\\right)^{j} \\left(\\frac{p_{M}}{p_{f}}\\right)^{k - j}.\n$$\nUsing $\\frac{\\frac{n!}{j!\\,(k - j)!\\,(n - k)!}}{\\frac{n!}{k!\\,(n - k)!}} = \\frac{k!}{j!\\,(k - j)!} = \\binom{k}{j}$ and $p_{f} = p_{m} + p_{M}$, we obtain\n$$\n\\Pr(N_{m} = j \\mid N_{m} + N_{M} = k) = \\binom{k}{j} \\left(\\frac{p_{m}}{p_{m} + p_{M}}\\right)^{j} \\left(\\frac{p_{M}}{p_{m} + p_{M}}\\right)^{k - j}.\n$$\nThis is the binomial distribution with $k$ trials and success probability $\\frac{p_{m}}{p_{m} + p_{M}}$, as expected by conditioning on being flawed.", "answer": "$$\\boxed{\\binom{k}{j}\\left(\\frac{p_{m}}{p_{m}+p_{M}}\\right)^{j}\\left(\\frac{p_{M}}{p_{m}+p_{M}}\\right)^{k-j}}$$", "id": "1402359"}]}