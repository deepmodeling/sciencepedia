{"hands_on_practices": [{"introduction": "This first practice establishes the fundamental principle of calculating the expectation of a function of a discrete random variable. The core idea is to compute a weighted average of the function's possible values, where each weight is the probability of the corresponding state occurring. By analyzing a simplified model of a quantum bit (qubit), we will directly apply the formula $E[g(X)] = \\sum_{x} g(x) P(X=x)$ to determine the expected energy of the system [@problem_id:1915939].", "problem": "A simplified model for a futuristic quantum computing bit, or qubit, considers its state after a specific error-inducing operation. The qubit's state is represented by a random variable $X$. With a probability $p$, the operation fails, and the qubit collapses to an \"error state,\" represented by $X=1$. With probability $1-p$, the operation is successful, and the qubit remains in the \"ground state,\" represented by $X=0$.\n\nAn energy measurement is performed on the qubit after this operation. The energy function, $E(X)$, is dependent on the qubit's state and is given by the relation $E(X) = A \\cdot c^{X}$, where $A$ is a baseline energy constant (in Joules) and $c$ is a dimensionless constant representing the energy level multiplier for the error state.\n\nDetermine the expected energy of the qubit after the operation. Express your answer as a symbolic expression in terms of $A$, $c$, and $p$.", "solution": "The qubit state $X$ takes values $0$ and $1$ with probabilities $P(X=0)=1-p$ and $P(X=1)=p$. The energy as a function of state is $E(X)=A c^{X}$. Define the energy random variable $Y=E(X)=A c^{X}$. For a discrete random variable, the expected value is given by the sum of each value times its probability:\n$$\nE[Y]=\\sum_{x} E(x)\\,P(X=x).\n$$\nSince $x\\in\\{0,1\\}$, evaluate explicitly:\n$$\nE[Y]=E(0)\\,P(X=0)+E(1)\\,P(X=1)=A c^{0}(1-p)+A c^{1} p.\n$$\nSimplify using $c^{0}=1$:\n$$\nE[Y]=A\\big[(1-p)+cp\\big]=A\\big[1+p(c-1)\\big].\n$$\nThus, the expected energy after the operation is $A\\big[1+p(c-1)\\big]$.", "answer": "$$\\boxed{A\\left[1+p\\left(c-1\\right)\\right]}$$", "id": "1915939"}, {"introduction": "Moving from discrete sums to continuous integrals, this exercise demonstrates how to find the expectation of a function for a continuous random variable. We will use the foundational formula $E[g(V)] = \\int_{-\\infty}^{\\infty} g(v) f_V(v) dv$ to solve a practical problem from electronics: calculating the average power dissipated by a resistor subjected to a random noise voltage [@problem_id:1915911]. This practice not only reinforces the integration technique but also highlights the physical meaning of expected value in a tangible context.", "problem": "The noise in a certain electronic circuit is modeled as a random voltage source. The voltage $V$ produced by this source is a random variable described by a triangular Probability Density Function (PDF). The PDF is given by:\n$$ f_V(v) = \\begin{cases} \\frac{1}{v_0}\\left(1 - \\frac{|v|}{v_0}\\right) & \\text{for } -v_0 \\le v \\le v_0 \\\\ 0 & \\text{otherwise} \\end{cases} $$\nwhere $v_0$ is a positive constant representing the maximum possible voltage magnitude.\n\nThis noisy voltage source is connected across a purely resistive load with a constant resistance $R$. The instantaneous power dissipated by this resistor is given by $P = V^2/R$.\n\nDetermine the average power, $P_{\\text{avg}}$, dissipated in the resistor. Express your answer as an analytic expression in terms of $v_0$ and $R$.", "solution": "The instantaneous power in a purely resistive load is $P=\\frac{V^{2}}{R}$. The average (expected) power is therefore\n$$\nP_{\\text{avg}}=E\\left[\\frac{V^{2}}{R}\\right]=\\frac{1}{R}E\\left[V^{2}\\right].\n$$\nUsing the definition of expectation for a continuous random variable with PDF $f_{V}(v)$,\n$$\nE\\left[V^{2}\\right]=\\int_{-\\infty}^{\\infty} v^{2} f_{V}(v)\\, dv=\\int_{-v_{0}}^{v_{0}} v^{2}\\,\\frac{1}{v_{0}}\\left(1-\\frac{|v|}{v_{0}}\\right)\\, dv,\n$$\nsince $f_{V}(v)=0$ outside $[-v_{0},v_{0}]$. Because both $v^{2}$ and $f_{V}(v)$ are even functions, this becomes\n$$\nE\\left[V^{2}\\right]=2\\int_{0}^{v_{0}} v^{2}\\,\\frac{1}{v_{0}}\\left(1-\\frac{v}{v_{0}}\\right)\\, dv=\\frac{2}{v_{0}}\\int_{0}^{v_{0}}\\left(v^{2}-\\frac{v^{3}}{v_{0}}\\right)\\, dv.\n$$\nEvaluate the integrals term by term:\n$$\n\\int_{0}^{v_{0}} v^{2}\\, dv=\\left.\\frac{v^{3}}{3}\\right|_{0}^{v_{0}}=\\frac{v_{0}^{3}}{3},\\qquad \\int_{0}^{v_{0}} v^{3}\\, dv=\\left.\\frac{v^{4}}{4}\\right|_{0}^{v_{0}}=\\frac{v_{0}^{4}}{4}.\n$$\nHence,\n$$\nE\\left[V^{2}\\right]=\\frac{2}{v_{0}}\\left(\\frac{v_{0}^{3}}{3}-\\frac{1}{v_{0}}\\cdot\\frac{v_{0}^{4}}{4}\\right)=\\frac{2}{v_{0}}\\left(\\frac{v_{0}^{3}}{3}-\\frac{v_{0}^{3}}{4}\\right)=\\frac{2}{v_{0}}\\cdot v_{0}^{3}\\left(\\frac{1}{3}-\\frac{1}{4}\\right)=2v_{0}^{2}\\cdot\\frac{1}{12}=\\frac{v_{0}^{2}}{6}.\n$$\nTherefore,\n$$\nP_{\\text{avg}}=\\frac{1}{R}E\\left[V^{2}\\right]=\\frac{1}{R}\\cdot\\frac{v_{0}^{2}}{6}=\\frac{v_{0}^{2}}{6R}.\n$$", "answer": "$$\\boxed{\\frac{v_{0}^{2}}{6R}}$$", "id": "1915911"}, {"introduction": "This final practice introduces a more sophisticated and powerful tool in probability theory: the Law of Total Expectation. This law, expressed as $E[Y] = E[E[Y|X]]$, is invaluable for problems involving sequential processes where one random variable depends on another [@problem_id:1915929]. By analyzing a two-stage process, we will break down a complex problem by first calculating a conditional expectation and then averaging over the conditioning variable, showcasing an elegant and widely applicable problem-solving strategy.", "problem": "A two-stage computational process is modeled as follows. The completion time of the first stage, represented by a random variable $X$, is chosen uniformly from the interval $[0, 1]$. Subsequently, the completion time of the second stage, represented by a random variable $Y$, is chosen uniformly from the interval $[X, 1]$. Determine the unconditional expected value of $Y$, denoted as $E[Y]$. The final answer should be a single real number, expressed as a fraction.", "solution": "Let $X$ be uniformly distributed on $[0,1]$, so its density is $f_{X}(x)=1$ for $x\\in[0,1]$. Conditional on $X=x$, $Y$ is uniformly distributed on $[x,1]$, so the conditional density is\n$$\nf_{Y|X}(y|x)=\\frac{1}{1-x},\\quad y\\in[x,1].\n$$\nThe unconditional expected value $E[Y]$ can be computed using the law of total expectation as\n$$\nE[Y]=E\\!\\left[E[Y|X]\\right].\n$$\nFirst compute the conditional expectation $E[Y|X=x]$ for fixed $x\\in[0,1]$:\n$$\nE[Y|X=x]=\\int_{x}^{1} y\\, f_{Y|X}(y|x)\\, dy=\\int_{x}^{1} y\\, \\frac{1}{1-x}\\, dy=\\frac{1}{1-x}\\int_{x}^{1} y\\, dy.\n$$\nEvaluate the inner integral:\n$$\n\\int_{x}^{1} y\\, dy=\\left.\\frac{y^{2}}{2}\\right|_{y=x}^{y=1}=\\frac{1-x^{2}}{2}.\n$$\nThus,\n$$\nE[Y|X=x]=\\frac{1-x^{2}}{2(1-x)}=\\frac{(1-x)(1+x)}{2(1-x)}=\\frac{1+x}{2}.\n$$\nApply the law of total expectation:\n$$\nE[Y]=E\\!\\left[\\frac{1+X}{2}\\right]=\\frac{1}{2}\\left(1+E[X]\\right).\n$$\nSince $X \\sim \\text{Uniform}[0,1]$, its mean is\n$$\nE[X]=\\int_{0}^{1} x\\, f_{X}(x)\\, dx=\\int_{0}^{1} x\\, dx=\\left.\\frac{x^{2}}{2}\\right|_{0}^{1}=\\frac{1}{2}.\n$$\nTherefore,\n$$\nE[Y]=\\frac{1}{2}\\left(1+\\frac{1}{2}\\right)=\\frac{1}{2}\\cdot\\frac{3}{2}=\\frac{3}{4}.\n$$\nFor completeness, the same result follows by direct double integration using densities:\n$$\nE[Y]=\\int_{0}^{1}\\int_{x}^{1} y\\, f_{Y|X}(y|x)\\, f_{X}(x)\\, dy\\, dx=\\int_{0}^{1}\\int_{x}^{1} \\frac{y}{1-x}\\, dy\\, dx\n=\\int_{0}^{1}\\frac{1-x^{2}}{2(1-x)}\\, dx=\\int_{0}^{1}\\frac{1+x}{2}\\, dx=\\frac{3}{4}.\n$$\nThus the unconditional expected value is $\\frac{3}{4}$.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1915929"}]}