{"hands_on_practices": [{"introduction": "Understanding the combination of random variables is a cornerstone of probabilistic modeling. This first exercise [@problem_id:1408043] provides a concrete starting point by asking for the distribution of a profit $P$, defined as the difference between two independent random variables, $P = R - C$. Tackling this problem will give you hands-on practice with the convolution method, a fundamental tool for finding the probability density function of the sum or difference of independent variables.", "problem": "A fintech startup analyzes its financial performance. Its monthly revenue, $R$, and its monthly operational costs, $C$, are modeled as independent random variables. Based on historical data and market analysis, both $R$ and $C$ are found to be uniformly distributed on the interval $[10, 20]$, with units in thousands of dollars.\n\nThe monthly profit is defined as the difference between revenue and costs, $P = R - C$.\n\nDetermine the probability density function (PDF), $f_P(p)$, for the monthly profit $P$. The function should be defined for all real numbers $p$.", "solution": "Let $R$ and $C$ be independent with $R \\sim \\text{Uniform}(10,20)$ and $C \\sim \\text{Uniform}(10,20)$. Their probability density functions are\n$$\nf_{R}(r)=\\begin{cases}\n\\frac{1}{10},  10 \\leq r \\leq 20,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n\\qquad\nf_{C}(c)=\\begin{cases}\n\\frac{1}{10},  10 \\leq c \\leq 20,\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nDefine the profit $P=R-C$. For independent random variables, the density of the difference is given by\n$$\nf_{P}(p)=\\int_{-\\infty}^{\\infty} f_{R}(p+c)\\,f_{C}(c)\\,dc.\n$$\nSubstituting the uniform densities, the integrand equals $\\frac{1}{100}$ precisely when both $10 \\leq c \\leq 20$ and $10 \\leq p+c \\leq 20$ hold, and is $0$ otherwise. Therefore,\n$$\nf_{P}(p)=\\frac{1}{100}\\,\\lambda\\Big(\\,[10,20]\\cap[10-p,\\,20-p]\\,\\Big),\n$$\nwhere $\\lambda(\\cdot)$ denotes interval length. For fixed $p$, the intersection length is\n$$\nL(p)=\\max\\!\\left(0,\\,\\min(20,\\,20-p)-\\max(10,\\,10-p)\\right).\n$$\nWe evaluate $L(p)$ piecewise. If $p \\in [0,10]$, then $\\min(20,20-p)=20-p$ and $\\max(10,10-p)=10$, hence $L(p)=(20-p)-10=10-p$. If $p \\in [-10,0]$, then $\\min(20,20-p)=20$ and $\\max(10,10-p)=10-p$, hence $L(p)=20-(10-p)=10+p$. If $|p|10$, the intervals do not overlap and $L(p)=0$. Combining these, for $|p|\\leq 10$ we have $L(p)=10-|p|$, and otherwise $L(p)=0$. Thus\n$$\nf_{P}(p)=\\begin{cases}\n\\frac{10-|p|}{100},  -10 \\leq p \\leq 10,\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nAs a check, the integral equals the area of a triangle with base $20$ and height $\\frac{10}{100}$, which is $1$, confirming a valid density.", "answer": "$$\\boxed{f_{P}(p)=\\begin{cases}\\frac{10-|p|}{100},  -10 \\leq p \\leq 10,\\\\[4pt] 0,  \\text{otherwise}.\\end{cases}}$$", "id": "1408043"}, {"introduction": "We now move from simple algebraic sums to functions that depend on the entire sample of random variables. This practice [@problem_id:1407987] explores the sample range, $R = X_{(n)} - X_{(1)}$, a key metric for measuring variability in fields like quality control. To solve this, you will first need to derive the joint distribution of the minimum and maximum order statistics and then apply a transformation, providing a deeper insight into multivariate changes of variables.", "problem": "A quality control system is used to monitor the output of a manufacturing process. A key performance metric of a component, after normalization, is found to be well-modeled by a random variable from a Uniform distribution on the interval $[0, 1]$. To assess the consistency of a production batch, an engineer draws a random sample of $n$ components, where $n \\ge 2$. The variability within this sample is quantified by the sample range, defined as the difference between the maximum and minimum values of the metric observed in the sample.\n\nLet $X_1, X_2, \\ldots, X_n$ be the $n$ independent and identically distributed (i.i.d.) measurements, each drawn from a Uniform$[0, 1]$ distribution. Let their ordered values be $X_{(1)} \\le X_{(2)} \\le \\ldots \\le X_{(n)}$. The sample range is defined as $R = X_{(n)} - X_{(1)}$.\n\nFor a given sample size $n$, at what value of the range, $r$, is the probability density function of $R$ maximized? This value represents the most probable (or modal) sample range. Express your answer as a closed-form expression in terms of $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. Uniform$[0,1]$, and let $U=X_{(1)}$ and $V=X_{(n)}$. The joint density of $(U,V)$ for a continuous distribution with pdf $f$ and cdf $F$ is\n$$\nf_{U,V}(u,v)=n(n-1)\\,[F(v)-F(u)]^{n-2}f(u)f(v),\\quad u\\le v.\n$$\nFor Uniform$[0,1]$, $f(x)=1$ and $F(x)=x$, so on $0\\le u\\le v\\le 1$,\n$$\nf_{U,V}(u,v)=n(n-1)\\,(v-u)^{n-2}.\n$$\nDefine the transformation to the range and the minimum: $R=V-U$ and $W=U$, so $V=R+W$. The Jacobian determinant is $1$. The domain maps to $0\\le R\\le 1$ and $0\\le W\\le 1-R$. Hence the joint density of $(R,W)$ is\n$$\nf_{R,W}(r,w)=n(n-1)\\,r^{n-2},\\quad 0\\le r\\le 1,\\ 0\\le w\\le 1-r.\n$$\nMarginalizing over $W$ yields the density of $R$:\n$$\nf_{R}(r)=\\int_{0}^{1-r} n(n-1)\\,r^{n-2}\\,dw = n(n-1)\\,r^{n-2}(1-r),\\quad 0\\le r\\le 1.\n$$\nTo find the mode, maximize $f_{R}(r)$ over $[0,1]$. The constant factor $n(n-1)$ does not affect the argmax, so define $g(r)=r^{n-2}(1-r)$. For $n\\ge 2$,\n$$\ng'(r)=(n-2)r^{n-3}(1-r)-r^{n-2}=r^{n-3}\\big[(n-2)-(n-1)r\\big].\n$$\nCritical points occur at $r=0$ (when $n2$ by the factor $r^{n-3}$) and at\n$$\n(n-2)-(n-1)r=0\\quad\\Rightarrow\\quad r=\\frac{n-2}{n-1}.\n$$\nEvaluate boundary values: for $n2$, $g(0)=0$ and $g(1)=0$, while $g\\!\\left(\\frac{n-2}{n-1}\\right)0$, so the interior point is the unique maximizer. For $n=2$, $f_{R}(r)=2(1-r)$ is decreasing on $[0,1]$, so the maximum occurs at $r=0$, which coincides with $r=\\frac{n-2}{n-1}$ when $n=2$. Therefore, for all integer $n\\ge 2$, the mode of $R$ is\n$$\nr^{\\ast}=\\frac{n-2}{n-1}.\n$$", "answer": "$$\\boxed{\\frac{n-2}{n-1}}$$", "id": "1407987"}, {"introduction": "When direct integration methods like convolution become computationally prohibitive, we turn to more powerful, indirect techniques. This challenging exercise [@problem_id:1408028] tasks you with finding the distribution of $D = X_1 X_4 - X_2 X_3$, the determinant of a $2 \\times 2$ matrix with standard normal entries. Successfully navigating this problem requires the use of characteristic functions, demonstrating how transforming a problem into the frequency domain can lead to an elegant and surprisingly simple solution.", "problem": "Let $X_1, X_2, X_3$, and $X_4$ be four independent and identically distributed random variables, each following the standard normal distribution with a mean of 0 and a variance of 1. A new random variable, $D$, is constructed from these variables according to the expression $D = X_1 X_4 - X_2 X_3$. Determine the probability density function (PDF) of $D$, which we will denote as $f_D(d)$. Your final answer should be an expression in terms of the variable $d$.", "solution": "Let the random variable $D$ be defined as $D = X_1 X_4 - X_2 X_3$, where $X_i \\sim \\mathcal{N}(0, 1)$ are independent and identically distributed (i.i.d.) random variables for $i=1, 2, 3, 4$.\n\nTo find the probability density function (PDF) of $D$, we can use the method of characteristic functions. Let us define two intermediate random variables, $Y_1 = X_1 X_4$ and $Y_2 = X_2 X_3$. Since the set of variables $\\{X_1, X_4\\}$ is independent of the set $\\{X_2, X_3\\}$ and their distributions are identical, the random variables $Y_1$ and $Y_2$ are independent and identically distributed. The variable of interest is $D = Y_1 - Y_2$.\n\nThe characteristic function of $D$ is given by $\\phi_D(t) = E[\\exp(itD)]$. Using the independence of $Y_1$ and $Y_2$, we can write:\n$$ \\phi_D(t) = E[\\exp(it(Y_1 - Y_2))] = E[\\exp(itY_1)] E[\\exp(-itY_2)] = \\phi_{Y_1}(t) \\phi_{Y_2}(-t) $$\nSince $Y_1$ and $Y_2$ are i.i.d., their characteristic functions are identical, i.e., $\\phi_{Y_1}(t) = \\phi_{Y_2}(t)$. This implies $\\phi_{Y_2}(-t) = \\phi_{Y_1}(-t)$. Thus, we have:\n$$ \\phi_D(t) = \\phi_{Y_1}(t) \\phi_{Y_1}(-t) $$\nOur first step is to compute the characteristic function of $Y_1 = X_1 X_4$. The PDF of a standard normal variable $X$ is $f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$.\n$$ \\phi_{Y_1}(t) = E[\\exp(itX_1 X_4)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\exp(itx_1 x_4) f_{X_1}(x_1) f_{X_4}(x_4) dx_1 dx_4 $$\n$$ \\phi_{Y_1}(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\exp(itx_1 x_4) \\exp\\left(-\\frac{x_1^2}{2}\\right) \\exp\\left(-\\frac{x_4^2}{2}\\right) dx_1 dx_4 $$\nWe can evaluate this by iterated integration. Let's integrate with respect to $x_1$ first, treating $x_4$ as a constant:\n$$ I(x_4) = \\int_{-\\infty}^{\\infty} \\exp(itx_1 x_4) \\exp\\left(-\\frac{x_1^2}{2}\\right) dx_1 = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}(x_1^2 - 2itx_4 x_1)\\right) dx_1 $$\nWe complete the square in the exponent: $x_1^2 - 2itx_4 x_1 = (x_1 - itx_4)^2 - (itx_4)^2 = (x_1 - itx_4)^2 + t^2 x_4^2$.\n$$ I(x_4) = \\exp\\left(-\\frac{t^2 x_4^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x_1 - itx_4)^2}{2}\\right) dx_1 $$\nThe integral is a Gaussian integral. Even though the mean is imaginary, the integral over the real line is a standard result: $\\int_{-\\infty}^{\\infty} \\exp(-(z-c)^2 / (2\\sigma^2)) dz = \\sqrt{2\\pi\\sigma^2}$. Here, $\\sigma^2=1$, so the integral is $\\sqrt{2\\pi}$.\n$$ I(x_4) = \\sqrt{2\\pi} \\exp\\left(-\\frac{t^2 x_4^2}{2}\\right) $$\nAlternatively, one could recognize that $\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp(ikx_1 - x_1^2/2) dx_1$ is the characteristic function of a standard normal variable, $\\phi_{X_1}(k) = \\exp(-k^2/2)$. Setting $k=tx_4$, the integral is $\\sqrt{2\\pi}\\phi_{X_1}(tx_4) = \\sqrt{2\\pi}\\exp(-(tx_4)^2/2)$.\n\nNow, we substitute this result back into the expression for $\\phi_{Y_1}(t)$:\n$$ \\phi_{Y_1}(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\left( \\sqrt{2\\pi} \\exp\\left(-\\frac{t^2 x_4^2}{2}\\right) \\right) \\exp\\left(-\\frac{x_4^2}{2}\\right) dx_4 $$\n$$ \\phi_{Y_1}(t) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x_4^2}{2} (1+t^2)\\right) dx_4 $$\nThis is another Gaussian integral. Let $\\sigma_{eff}^2 = (1+t^2)^{-1}$. The integral is of the form $\\int_{-\\infty}^{\\infty} \\exp(-x^2/(2\\sigma_{eff}^2)) dx = \\sqrt{2\\pi \\sigma_{eff}^2} = \\sqrt{2\\pi} (1+t^2)^{-1/2}$.\n$$ \\phi_{Y_1}(t) = \\frac{1}{\\sqrt{2\\pi}} \\left( \\sqrt{2\\pi} (1+t^2)^{-1/2} \\right) = \\frac{1}{\\sqrt{1+t^2}} $$\nNow we can compute the characteristic function of $D$:\n$$ \\phi_{Y_1}(-t) = \\frac{1}{\\sqrt{1+(-t)^2}} = \\frac{1}{\\sqrt{1+t^2}} $$\n$$ \\phi_D(t) = \\phi_{Y_1}(t) \\phi_{Y_1}(-t) = \\frac{1}{\\sqrt{1+t^2}} \\frac{1}{\\sqrt{1+t^2}} = \\frac{1}{1+t^2} $$\nThe final step is to find the PDF $f_D(d)$ whose characteristic function is $\\phi_D(t) = \\frac{1}{1+t^2}$. We can do this by taking the inverse Fourier transform:\n$$ f_D(d) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-itd) \\phi_D(t) dt = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{\\exp(-itd)}{1+t^2} dt $$\nThis characteristic function is well-known. It corresponds to the Laplace distribution with location parameter $\\mu=0$ and scale parameter $b=1$. The general PDF for a Laplace distribution is $f(x; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)$.\nFor our case, with $\\mu=0$ and $b=1$, the PDF of $D$ is:\n$$ f_D(d) = \\frac{1}{2} \\exp(-|d|) $$", "answer": "$$\\boxed{\\frac{1}{2} \\exp(-|d|)}$$", "id": "1408028"}]}