{"hands_on_practices": [{"introduction": "When we combine random variables, understanding how their means and variances behave is fundamental. While the mean of a sum or difference follows our everyday intuition, the behavior of variance can be surprising. This exercise challenges you to explore this by comparing the variance of the sum of two independent normal variables, $Var(X+Y)$, with the variance of their difference, $Var(X-Y)$. Solving this will solidify your understanding of a crucial principle: for independent sources of variation, uncertainty, as measured by variance, always accumulates. [@problem_id:5878]", "problem": "Let $X$ and $Y$ be two independent, non-identically distributed random variables. The variable $X$ follows a normal distribution with mean $\\mu_X$ and variance $\\sigma_X^2$, denoted as $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$. Similarly, the variable $Y$ follows a normal distribution with mean $\\mu_Y$ and variance $\\sigma_Y^2$, denoted as $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$.\n\nIt is given that the variances are strictly positive, i.e., $\\sigma_X^2 > 0$ and $\\sigma_Y^2 > 0$.\n\nDerive the value of the ratio $\\frac{Var(X+Y)}{Var(X-Y)}$.", "solution": "Using independence and properties of variance, we have\n$$Var(X+Y)=Var(X)+Var(Y)+2\\,Cov(X,Y).$$\nSince $X$ and $Y$ are independent, $Cov(X,Y)=0$, hence\n$$Var(X+Y)=\\sigma_X^2+\\sigma_Y^2.$$\n\nSimilarly,\n$$Var(X-Y)=Var(X)+Var(-Y)+2\\,Cov(X,-Y).$$\nBut $Var(-Y)=Var(Y)=\\sigma_Y^2$ and $Cov(X,-Y)=-Cov(X,Y)=0$, so\n$$Var(X-Y)=\\sigma_X^2+\\sigma_Y^2.$$\n\nTherefore the ratio is\n$$\n\\frac{Var(X+Y)}{Var(X-Y)}\n=\\frac{\\sigma_X^2+\\sigma_Y^2}{\\sigma_X^2+\\sigma_Y^2}\n=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "5878"}, {"introduction": "With the rules for combining means and variances established, we can now tackle a practical, real-world scenario. Many complex outcomes, from the total score on an exam to the final dimension of a manufactured part, can be modeled as the sum of several independent, normally distributed components. This practice guides you through the complete process: defining the correct distribution for the sum, standardizing the result, and calculating the probability of achieving a specific outcome. [@problem_id:1347381]", "problem": "A university's computer science department has designed a comprehensive two-part qualifying exam for its graduate program. Part A covers theoretical foundations, and Part B covers practical applications. The scores for a student on each part are modeled as independent random variables.\n\nThe score on Part A, denoted by $S_A$, follows a normal distribution with a mean of $76.5$ and a standard deviation of $8.2$.\nThe score on Part B, denoted by $S_B$, follows a normal distribution with a mean of $81.0$ and a standard deviation of $9.5$.\n\nA student's total score is the sum of their scores on Part A and Part B. Calculate the probability that a randomly selected student achieves a total score greater than $165.0$.\n\nRound your final answer to four significant figures.", "solution": "Let $S_{A} \\sim \\mathcal{N}(76.5,\\,8.2^{2})$ and $S_{B} \\sim \\mathcal{N}(81.0,\\,9.5^{2})$, independent. The total score $T=S_{A}+S_{B}$ is normal with mean and variance given by the sum of means and variances (by independence and the closure of the normal distribution under addition):\n$$\n\\mu_{T}=\\mu_{A}+\\mu_{B}=76.5+81.0=157.5,\\qquad \\sigma_{T}^{2}=\\sigma_{A}^{2}+\\sigma_{B}^{2}=8.2^{2}+9.5^{2}=67.24+90.25=157.49.\n$$\nThus $T \\sim \\mathcal{N}(157.5,\\,157.49)$ and $\\sigma_{T}=\\sqrt{157.49}$. We seek\n$$\n\\mathbb{P}(T>165)=\\mathbb{P}\\!\\left(\\frac{T-\\mu_{T}}{\\sigma_{T}}>\\frac{165-157.5}{\\sqrt{157.49}}\\right)=\\mathbb{P}(Z>z),\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ and\n$$\nz=\\frac{165-157.5}{\\sqrt{157.49}}=\\frac{7.5}{\\sqrt{157.49}}\\approx \\frac{7.5}{12.5495}\\approx 0.597633.\n$$\nTherefore,\n$$\n\\mathbb{P}(T>165)=1-\\Phi(z)=1-\\Phi(0.597633).\n$$\nUsing the standard normal distribution, $\\Phi(0.597633)\\approx 0.72496$, hence\n$$\n\\mathbb{P}(T>165)\\approx 1-0.72496=0.27504.\n$$\nRounding to four significant figures gives $0.2750$.", "answer": "$$\\boxed{0.2750}$$", "id": "1347381"}, {"introduction": "The power of this concept extends beyond calculating probabilities and into the realm of statistical inference. In many scientific fields, a measured signal is a composite of a true underlying value and random noise, both of which can often be modeled by normal distributions. This exercise demonstrates how to use the properties of summed normal variables to work backward, applying the principle of Maximum Likelihood Estimation (MLE) to infer the most likely value of a hidden parameter from a single combined observation. [@problem_id:1391612]", "problem": "In radio astronomy, the signal received from a distant object can be complex. Consider a simplified model where the total measured voltage, $Z$, from a cosmic source is the sum of two independent components: a primary signal component $X$ and a secondary signal component $Y$.\n\nThe primary signal voltage $X$ is modeled as a random variable following a Normal distribution with an unknown mean $\\mu$ and a known variance $\\sigma_1^2$. The secondary signal voltage $Y$ is also modeled as a Normal random variable. Its mean is known to be linearly dependent on the primary mean, given by $E[Y] = \\alpha \\mu$, where $\\alpha$ is a known real constant. The variance of $Y$ is a known value $\\sigma_2^2$. The two signal components, $X$ and $Y$, are statistically independent. We are given that the constant $\\alpha \\neq -1$.\n\nAn astronomer makes a single measurement and records the total voltage $Z$. Your task is to find the formula for the Maximum Likelihood Estimator (MLE), denoted $\\hat{\\mu}$, for the unknown mean $\\mu$. Express your answer in terms of the total voltage measurement $Z$ and the known parameters $\\alpha$, $\\sigma_1$, and $\\sigma_2$.", "solution": "Let $X \\sim \\mathcal{N}(\\mu,\\sigma_{1}^{2})$ and $Y \\sim \\mathcal{N}(\\alpha \\mu,\\sigma_{2}^{2})$ be independent. For $Z=X+Y$, by linearity of expectation and independence,\n$$\nE[Z \\mid \\mu]=E[X]+E[Y]=\\mu+\\alpha \\mu=(1+\\alpha)\\mu,\n$$\nand\n$$\n\\operatorname{Var}(Z \\mid \\mu)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)=\\sigma_{1}^{2}+\\sigma_{2}^{2}.\n$$\nHence $Z \\mid \\mu \\sim \\mathcal{N}\\big((1+\\alpha)\\mu,\\ \\sigma_{1}^{2}+\\sigma_{2}^{2}\\big)$. For a single observation $Z=z$, the likelihood is\n$$\nL(\\mu;z)=\\frac{1}{\\sqrt{2\\pi(\\sigma_{1}^{2}+\\sigma_{2}^{2})}}\\exp\\!\\left(-\\frac{(z-(1+\\alpha)\\mu)^{2}}{2(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(\\mu;z)=-\\frac{1}{2}\\ln\\big(2\\pi(\\sigma_{1}^{2}+\\sigma_{2}^{2})\\big)-\\frac{1}{2(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\big(z-(1+\\alpha)\\mu\\big)^{2}.\n$$\nDifferentiate with respect to $\\mu$ and set to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu}=\\frac{(1+\\alpha)}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}\\big(z-(1+\\alpha)\\mu\\big)=0\n\\quad\\Longrightarrow\\quad\nz-(1+\\alpha)\\mu=0,\n$$\nwhere we used $\\alpha\\neq -1$ to divide by $(1+\\alpha)$. Thus,\n$$\n\\hat{\\mu}=\\frac{z}{1+\\alpha}.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2}\\ell}{\\partial \\mu^{2}}=-\\frac{(1+\\alpha)^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}<0,\n$$\nsince $\\sigma_{1}^{2}+\\sigma_{2}^{2}>0$ and $\\alpha\\neq -1$, confirming a maximum. Replacing $z$ by the observed $Z$ yields the MLE\n$$\n\\hat{\\mu}=\\frac{Z}{1+\\alpha}.\n$$", "answer": "$$\\boxed{\\frac{Z}{1+\\alpha}}$$", "id": "1391612"}]}