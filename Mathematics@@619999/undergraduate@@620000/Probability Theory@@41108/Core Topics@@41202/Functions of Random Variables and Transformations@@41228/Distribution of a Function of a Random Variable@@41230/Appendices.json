{"hands_on_practices": [{"introduction": "We begin our hands-on practice with discrete random variables, which provide the clearest illustration of how distributions are transformed. This first problem [@problem_id:5118] explores a simple linear transformation of a binomially distributed variable. The key principle is to map the probability of each outcome for the new variable $Y$ directly from the corresponding outcome of the original variable $X$, helping to build an intuitive foundation for more complex cases.", "problem": "Let $X$ be a discrete random variable that follows a binomial distribution, denoted as $X \\sim B(n, p)$. This distribution describes the number of successes in a sequence of $n$ independent experiments, each asking a yes-or-no question, where $p$ is the probability of success on a single trial.\n\nThe probability mass function (PMF) for $X$ is given by:\n$$P_X(k) = P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nfor $k \\in \\{0, 1, 2, \\dots, n\\}$, where $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient.\n\nNow, consider a new random variable $Y$ defined as a linear transformation of $X$:\n$$Y = n - X$$\nThis new variable $Y$ can be interpreted as the number of failures in the same sequence of $n$ experiments.\n\nDerive the probability mass function (PMF) for the random variable $Y$, denoted as $P_Y(y)$. Express your answer in terms of $y$, $n$, and $p$.", "solution": "Starting from the definition of $Y$, we have\n$$P_Y(y)=P(Y=y)=P(n-X=y).$$\nThe event $\\{n-X=y\\}$ is equivalent to $\\{X=n-y\\}$, hence\n$$P_Y(y)=P(X=n-y)=\\binom{n}{\\,n-y\\,}p^{\\,n-y}(1-p)^{\\,n-(n-y)}.$$\nNoting that $n-(n-y)=y$ and using the symmetry $\\binom{n}{n-y}=\\binom{n}{y}$, we get\n$$P_Y(y)=\\binom{n}{y}\\,p^{\\,n-y}(1-p)^{\\,y},\\qquad y\\in\\{0,1,\\dots,n\\}.$$", "answer": "$$\\boxed{\\binom{n}{y}p^{n-y}(1-p)^y}$$", "id": "5118"}, {"introduction": "Now we turn to continuous random variables, where we transform probability density functions (PDFs) instead of discrete probabilities. This exercise [@problem_id:5136] demonstrates the fundamental change of variable technique for a monotonic functionâ€”in this case, a square root. You will see how the derivative of the transformation function is used to stretch or compress the original density, a crucial step to ensure the new PDF integrates to one.", "problem": "A continuous random variable $X$ is said to be uniformly distributed on the interval $[a, b]$ if its probability density function (PDF), $f_X(x)$, is given by:\n$$\nf_X(x) = \\begin{cases} \\frac{1}{b-a} & \\text{for } a \\le x \\le b \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nConsider a random variable $X$ that is uniformly distributed on the interval $[0, L^2]$, where $L$ is a positive real constant. A new random variable $Y$ is defined as the positive square root of $X$, such that $Y = \\sqrt{X}$.\n\nFirst, derive the probability density function, $f_Y(y)$, for the random variable $Y$. Then, use this PDF to calculate the expected value of $Y$, denoted as $E[Y]$.", "solution": "We start with $X \\sim \\text{Unif}(0,L^2)$, so\n$$f_X(x)=\\begin{cases}\\frac{1}{L^2}&0\\le x\\le L^2,\\\\0&\\text{otherwise}.\\end{cases}$$\n\nLet $Y=\\sqrt{X} \\Rightarrow x=y^2$ and $dx/dy=2y$. Hence for $0 \\le y \\le L$:\n$$f_Y(y)=f_X(y^2)\\Bigl|\\frac{dx}{dy}\\Bigr|=\\frac{1}{L^2}\\cdot2y=\\frac{2y}{L^2}.$$\n\nThe expectation is\n$$E[Y]=\\int_0^L y\\,f_Y(y)\\,dy\n=\\int_0^L y\\frac{2y}{L^2}\\,dy\n=\\frac{2}{L^2}\\int_0^L y^2\\,dy\n=\\frac{2}{L^2}\\cdot\\frac{L^3}{3}\n=\\frac{2L}{3}.$$", "answer": "$$\\boxed{\\frac{2L}{3}}$$", "id": "5136"}, {"introduction": "Our final practice addresses the common and more complex case of non-monotonic transformations, where multiple input values map to a single output value. This problem [@problem_id:1356799] examines the squaring of a randomly generated signal, a classic 'many-to-one' function. The key skill here is to identify all parts of the original variable's domain that contribute to a specific output value and to correctly sum their respective probability densities.", "problem": "In a signal processing simulation, a random input signal $P$ is generated from a uniform distribution over the continuous interval $[-1, 3]$. A non-linear amplifier processes this signal, producing an output signal $Y$ that is related to the input by the function $Y = P^2$. The behavior of the system depends on the probability distribution of the output signal. Determine the value of the probability density function of $Y$, denoted $f_Y(y)$, evaluated at $y=4$.", "solution": "Let $P$ be uniformly distributed on $[-1,3]$, so its probability density function is\n$$\nf_{P}(p)=\\begin{cases}\n\\frac{1}{4}, & -1 \\leq p \\leq 3,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nThe output is $Y=P^{2}$. For a transformation $Y=g(P)$ with $g$ differentiable and one-to-one on branches, the density of $Y$ at $y$ is given by\n$$\nf_{Y}(y)=\\sum_{p_{i}:\\, g(p_{i})=y} \\frac{f_{P}(p_{i})}{|g'(p_{i})|},\n$$\nwhere the sum is over all roots $p_{i}$ in the support of $P$. Here $g(p)=p^{2}$ and $g'(p)=2p$. For $y>0$, the solutions to $p^{2}=y$ are $p=\\pm \\sqrt{y}$.\n\nWe evaluate at $y=4$. The roots are $p=2$ and $p=-2$. Among these, $p=2$ lies in $[-1,3]$ while $p=-2$ does not. Hence only $p=2$ contributes:\n$$\nf_{Y}(4)=\\frac{f_{P}(2)}{|g'(2)|}=\\frac{\\frac{1}{4}}{|2\\cdot 2|}=\\frac{1}{16}.\n$$", "answer": "$$\\boxed{\\frac{1}{16}}$$", "id": "1356799"}]}