{"hands_on_practices": [{"introduction": "The Gamma distribution is fundamentally the sum of independent exponential random variables, a concept crucial for modeling waiting times for a series of events. This exercise [@problem_id:1950927] provides a hands-on look at this relationship by focusing on variance. By working backward from the known variance of a total waiting time, you will deduce the properties of the individual, exponentially distributed time intervals between events, solidifying your understanding of how system-level properties emerge from component-level behavior.", "problem": "A customer support center models the time intervals between consecutive incoming calls as independent and identically distributed (i.i.d.) exponential random variables. Let $S_n$ represent the total time elapsed from the start of an observation period until the arrival of the $n$-th call. An analysis of call data reveals that for $n=5$ calls, the variance of the total waiting time, $\\operatorname{Var}(S_5)$, is $31.25$ minutes-squared.\n\nBased on this information, determine the expected time between consecutive calls. Express your answer in minutes.", "solution": "Let the interarrival times be $X_{1}, X_{2}, \\dots$ which are i.i.d. exponential with rate $\\lambda$, so $\\mathbb{E}[X_{1}] = \\frac{1}{\\lambda}$ and $\\operatorname{Var}(X_{1}) = \\frac{1}{\\lambda^{2}}$. The total time to the $n$-th arrival is $S_{n} = \\sum_{i=1}^{n} X_{i}$. By independence, variances add:\n$$\n\\operatorname{Var}(S_{n}) = \\sum_{i=1}^{n} \\operatorname{Var}(X_{i}) = n \\cdot \\frac{1}{\\lambda^{2}} = \\frac{n}{\\lambda^{2}}.\n$$\nFor $n=5$, the given $\\operatorname{Var}(S_{5}) = 31.25$ implies\n$$\n\\frac{5}{\\lambda^{2}} = 31.25.\n$$\nRewrite $31.25$ exactly as a fraction: $31.25 = \\frac{125}{4}$. Therefore\n$$\n\\lambda^{2} = \\frac{5}{31.25} = \\frac{5}{\\frac{125}{4}} = \\frac{5 \\cdot 4}{125} = \\frac{20}{125} = \\frac{4}{25},\n$$\nso, taking the positive root (since a rate is positive),\n$$\n\\lambda = \\sqrt{\\frac{4}{25}} = \\frac{2}{5}.\n$$\nThe expected time between consecutive calls is $\\mathbb{E}[X_{1}] = \\frac{1}{\\lambda}$, hence\n$$\n\\mathbb{E}[X_{1}] = \\frac{1}{\\frac{2}{5}} = \\frac{5}{2}.\n$$\nThus the expected interarrival time is $\\frac{5}{2}$ minutes.", "answer": "$$\\boxed{\\frac{5}{2}}$$", "id": "1950927"}, {"introduction": "Beyond understanding its construction, the real power of the Gamma distribution lies in its application to real-world problems like system reliability. In this scenario [@problem_id:1950932], we model a system with a primary component and a backup, a classic 'cold standby' setup. This practice challenges you to calculate the system's overall reliability by determining the probability that its total lifetime, which follows a Gamma distribution, exceeds a critical threshold.", "problem": "A deep-space probe is equipped with a primary and a secondary communication module. The modules operate in a sequential, 'cold standby' mode: the secondary module is only activated upon the failure of the primary one. The operational lifetime of each module is independent and can be modeled by an exponential distribution with a mean lifetime of $\\mu = 4000$ hours.\n\nCalculate the probability that the total operational lifetime provided by both modules exceeds 5000 hours. Round your final answer to four significant figures.", "solution": "Let $X_{1}$ and $X_{2}$ denote the independent operational lifetimes (in hours) of the primary and secondary modules, respectively. Each is exponentially distributed with mean $\\mu=4000$, so the rate is $\\lambda=\\frac{1}{\\mu}$. In cold standby, the total operational lifetime is the sum $T=X_{1}+X_{2}$.\n\nSince $X_{1}$ and $X_{2}$ are independent and identically distributed exponential random variables with rate $\\lambda$, $T$ has an Erlang (Gamma) distribution with shape $k=2$ and rate $\\lambda$. The density of $T$ is\n$$\nf_{T}(t)=\\lambda^{2} t \\exp(-\\lambda t), \\quad t \\ge 0.\n$$\nThe survival function is\n$$\nS_{T}(t)=\\mathbb{P}(T>t)=\\int_{t}^{\\infty} \\lambda^{2} s \\exp(-\\lambda s)\\, ds.\n$$\nIntegrating by parts with $u=s$ and $dv=\\lambda^{2}\\exp(-\\lambda s)\\,ds$ gives $v=-\\lambda \\exp(-\\lambda s)$, hence\n$$\nS_{T}(t)=\\left[s(-\\lambda \\exp(-\\lambda s))\\right]_{t}^{\\infty}-\\int_{t}^{\\infty}(-\\lambda \\exp(-\\lambda s))\\, ds\n= \\lambda t \\exp(-\\lambda t)+\\exp(-\\lambda t)\n=\\exp(-\\lambda t)\\left(1+\\lambda t\\right).\n$$\nTherefore,\n$$\n\\mathbb{P}(T>5000)=\\exp\\!\\left(-\\lambda \\cdot 5000\\right)\\left(1+\\lambda \\cdot 5000\\right)\n=\\exp\\!\\left(-\\frac{5000}{\\mu}\\right)\\left(1+\\frac{5000}{\\mu}\\right).\n$$\nWith $\\mu=4000$, this becomes\n$$\n\\mathbb{P}(T>5000)=\\exp(-1.25)\\cdot 2.25.\n$$\nNumerically, $\\exp(-1.25)\\approx 0.2865047969$, so\n$$\n\\mathbb{P}(T>5000)\\approx 2.25 \\times 0.2865047969=0.6446357926,\n$$\nwhich rounds to four significant figures as $0.6446$.", "answer": "$$\\boxed{0.6446}$$", "id": "1950932"}, {"introduction": "Our final practice delves into a more subtle and profound aspect of the relationship between exponential and Gamma distributions. What can we infer about a single stage of a process if we only know the total time it took to complete all stages? This thought-provoking problem [@problem_id:1950951] explores the conditional distribution of one exponential variable given the sum, revealing a surprising and elegant connection to the uniform distribution that has significant implications in statistical modeling.", "problem": "A specialized signal processing unit is designed with two sequential filtering stages. The time duration required for the first stage to process a signal is a random variable $X_1$, and the time for the second stage is $X_2$. The durations $X_1$ and $X_2$ are modeled as independent and identically distributed (i.i.d.) random variables, each following an exponential distribution with a rate parameter $\\lambda > 0$.\n\nFor quality control, a particular unit is tested. The total time taken for a signal to pass through both stages, $X_1 + X_2$, is measured to be a fixed constant value $T$. Given this empirical observation, we are interested in the statistical properties of the duration of the first stage, $X_1$.\n\nDetermine the conditional variance of the duration of the first stage, $\\operatorname{Var}(X_1 | X_1 + X_2 = T)$. Express your answer as a closed-form analytic expression in terms of the total measured time, $T$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent and identically distributed as $\\mathrm{Exp}(\\lambda)$ with joint density\n$$\nf_{X_{1},X_{2}}(x_{1},x_{2})=\\lambda^{2}\\exp\\!\\big(-\\lambda(x_{1}+x_{2})\\big), \\quad x_{1}\\geq 0,\\; x_{2}\\geq 0.\n$$\nDefine the sum $S=X_{1}+X_{2}$. Its density is obtained by convolution:\n$$\nf_{S}(s)=\\int_{0}^{s} f_{X_{1},X_{2}}(x,s-x)\\,dx=\\int_{0}^{s}\\lambda^{2}\\exp(-\\lambda s)\\,dx=\\lambda^{2}s\\exp(-\\lambda s), \\quad s\\geq 0.\n$$\nThe conditional density of $X_{1}$ given $S=s$ is\n$$\nf_{X_{1}\\mid S}(x\\mid s)=\\frac{f_{X_{1},X_{2}}(x,s-x)}{f_{S}(s)}=\\frac{\\lambda^{2}\\exp(-\\lambda s)}{\\lambda^{2}s\\exp(-\\lambda s)}=\\frac{1}{s}, \\quad 0<x<s,\n$$\nand $0$ otherwise. Thus $X_{1}\\mid S=s\\sim \\mathrm{Uniform}(0,s)$.\n\nCompute the conditional mean and second moment:\n$$\n\\mathbb{E}[X_{1}\\mid S=s]=\\int_{0}^{s} x\\cdot \\frac{1}{s}\\,dx=\\frac{1}{s}\\cdot \\frac{s^{2}}{2}=\\frac{s}{2},\n$$\n$$\n\\mathbb{E}[X_{1}^{2}\\mid S=s]=\\int_{0}^{s} x^{2}\\cdot \\frac{1}{s}\\,dx=\\frac{1}{s}\\cdot \\frac{s^{3}}{3}=\\frac{s^{2}}{3}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X_{1}\\mid S=s)=\\mathbb{E}[X_{1}^{2}\\mid S=s]-\\big(\\mathbb{E}[X_{1}\\mid S=s]\\big)^{2}=\\frac{s^{2}}{3}-\\left(\\frac{s}{2}\\right)^{2}=\\frac{s^{2}}{12}.\n$$\nSubstituting $s=T$ yields\n$$\n\\operatorname{Var}(X_{1}\\mid X_{1}+X_{2}=T)=\\frac{T^{2}}{12}.\n$$", "answer": "$$\\boxed{\\frac{T^{2}}{12}}$$", "id": "1950951"}]}