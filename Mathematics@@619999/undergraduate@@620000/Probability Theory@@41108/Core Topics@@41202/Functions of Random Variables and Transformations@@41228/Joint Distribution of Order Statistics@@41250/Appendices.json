{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the fundamental case of discrete random variables. This problem uses Bernoulli trials—representing simple success/failure outcomes—to build your intuition from the ground up. By analyzing the joint probability of the minimum and median outcomes from a small sample, you will learn a critical skill: translating a question about order statistics into a more straightforward question about the total number of successes [@problem_id:1368721]. This exercise is invaluable for understanding how ordering impacts probability in a finite and manageable setting.", "problem": "An automated quality control process tests three identical, independently manufactured electronic switches. The outcome of testing each switch is a random variable that takes the value 1 if the switch functions correctly (a \"success\") and 0 if it fails (a \"failure\"). The probability that any given switch functions correctly is $p$, where $0 < p < 1$.\n\nLet the outcomes of the three tests be the random variables $X_1, X_2,$ and $X_3$. These outcomes are then sorted in non-decreasing order to yield the order statistics $X_{(1)} \\le X_{(2)} \\le X_{(3)}$. In this context, $X_{(1)}$ represents the minimum outcome and $X_{(2)}$ represents the median outcome of the three tests.\n\nDetermine the joint probability that the minimum outcome is a failure and the median outcome is a success. Express your answer as an analytic expression in terms of $p$.", "solution": "Each test outcome is a Bernoulli random variable with success probability $p$, and the three outcomes are independent and identically distributed. Let $S=X_1+X_2+X_3$ be the total number of successes. The sorted outcomes $(X_{(1)},X_{(2)},X_{(3)})$ depend only on $S$:\n- If $S=0$, then $(0,0,0)$ so $X_{(1)}=0$, $X_{(2)}=0$.\n- If $S=1$, then $(0,0,1)$ so $X_{(1)}=0$, $X_{(2)}=0$.\n- If $S=2$, then $(0,1,1)$ so $X_{(1)}=0$, $X_{(2)}=1$.\n- If $S=3$, then $(1,1,1)$ so $X_{(1)}=1$, $X_{(2)}=1$.\n\nTherefore, the event $\\{X_{(1)}=0, X_{(2)}=1\\}$ occurs exactly when $S=2$. Since $S$ is binomial with parameters $n=3$ and $p$, its probability mass function is\n$$\n\\mathbb{P}(S=k)=\\binom{3}{k}p^{k}(1-p)^{3-k}.\n$$\nThus,\n$$\n\\mathbb{P}\\big(X_{(1)}=0, X_{(2)}=1\\big)=\\mathbb{P}(S=2)=\\binom{3}{2}p^{2}(1-p)=3p^{2}(1-p).\n$$", "answer": "$$\\boxed{3p^{2}(1-p)}$$", "id": "1368721"}, {"introduction": "Moving from discrete to continuous distributions, this practice explores order statistics from the uniform distribution, a cornerstone of probability theory. The exercise presents a quality control scenario where a product is evaluated based on where its ordered measurements fall relative to certain thresholds. The key here is to master the technique of partitioning a continuous range into discrete intervals, thereby transforming the problem into a multinomial probability calculation [@problem_id:1368681]. This powerful method provides a conceptual bridge between discrete and continuous worlds and is essential for tackling problems where multiple order statistics must lie within specific bounds.", "problem": "In a semiconductor fabrication facility, a process of chemical vapor deposition is used to grow a thin dielectric film on silicon wafers. The thickness of this film is a critical parameter for device performance. Due to minor fluctuations in temperature and gas flow, the film thickness is not perfectly uniform across the wafer.\n\nTo model this variability, the normalized thickness at any given point on the wafer is considered a random variable. A quality control procedure involves measuring the normalized thickness at four randomly selected locations. Let these four measurements be represented by the independent and identically distributed random variables $X_1, X_2, X_3, X_4$, each following a Uniform distribution on the interval $(0, 1)$.\n\nA wafer is flagged for further inspection if its thickness profile exhibits a specific type of non-uniformity: the two thinnest measurements must be below a lower threshold of $1/3$, and the two thickest measurements must be above an upper threshold of $2/3$. Let $X_{(1)} \\le X_{(2)} \\le X_{(3)} \\le X_{(4)}$ be the ordered measurements (order statistics) from the sample of four. The condition for flagging the wafer is therefore the event $\\{X_{(1)} < 1/3, X_{(2)} < 1/3, X_{(3)} > 2/3, X_{(4)} > 2/3\\}$.\n\nCalculate the probability that a randomly selected wafer will be flagged for further inspection. Express your answer as a single fraction in simplest form.", "solution": "Let $X_1,X_2,X_3,X_4$ be i.i.d. $\\operatorname{Uniform}(0,1)$. The event $\\{X_{(1)}<\\frac{1}{3},\\,X_{(2)}<\\frac{1}{3},\\,X_{(3)}>\\frac{2}{3},\\,X_{(4)}>\\frac{2}{3}\\}$ is equivalent to the event that exactly two observations fall in $(0,\\frac{1}{3})$, exactly two fall in $(\\frac{2}{3},1)$, and none fall in $(\\frac{1}{3},\\frac{2}{3})$. Because the distribution is continuous, the strict inequalities have the same probability as the corresponding non-strict ones.\n\nDefine three disjoint categories with probabilities\n$$\np_{L}=\\mathbb{P}\\!\\left(X\\in\\left(0,\\tfrac{1}{3}\\right)\\right)=\\tfrac{1}{3},\\quad\np_{M}=\\mathbb{P}\\!\\left(X\\in\\left(\\tfrac{1}{3},\\tfrac{2}{3}\\right)\\right)=\\tfrac{1}{3},\\quad\np_{U}=\\mathbb{P}\\!\\left(X\\in\\left(\\tfrac{2}{3},1\\right)\\right)=\\tfrac{1}{3}.\n$$\nBy independence, the probability of exactly $k_{L}=2$, $k_{M}=0$, $k_{U}=2$ across the four draws is given by the multinomial formula\n$$\n\\mathbb{P}=\\frac{4!}{2!\\,0!\\,2!}\\,p_{L}^{2}\\,p_{M}^{0}\\,p_{U}^{2}\n=\\frac{4!}{2!\\,2!}\\left(\\tfrac{1}{3}\\right)^{2}\\left(\\tfrac{1}{3}\\right)^{0}\\left(\\tfrac{1}{3}\\right)^{2}\n=6\\left(\\tfrac{1}{3}\\right)^{4}\n=\\frac{6}{81}\n=\\frac{2}{27}.\n$$\nThus, the probability a wafer is flagged is $\\frac{2}{27}$.", "answer": "$$\\boxed{\\frac{2}{27}}$$", "id": "1368681"}, {"introduction": "This final practice elevates our analysis from calculating probabilities to computing expectations, a key step in fully characterizing a distribution. The problem focuses on conditional expectation, introducing a profound and elegant property unique to order statistics from a uniform distribution. You will discover that when the minimum and maximum values of a sample are known, the intermediate order statistics behave in a very predictable way [@problem_id:1368704]. Understanding this principle provides a powerful shortcut for solving seemingly complex problems and offers deeper insight into the underlying structure of ordered data.", "problem": "Let $X_1, X_2, X_3, X_4$ be a random sample of size four from a continuous uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$. The corresponding order statistics are denoted by $X_{(1)} \\le X_{(2)} \\le X_{(3)} \\le X_{(4)}$. Given that the smallest observation is $X_{(1)} = u$ and the largest observation is $X_{(4)} = v$, where $0 < u < v < \\theta$ are fixed constants, determine the conditional expectation of the sum of the two central order statistics, $E[X_{(2)} + X_{(3)} | X_{(1)} = u, X_{(4)} = v]$. Provide your answer as an analytic expression in terms of $u$ and $v$.", "solution": "Let the random sample be $X_1, X_2, X_3, X_4$, where each $X_i$ is independently drawn from a uniform distribution $U(0, \\theta)$. The probability density function (PDF) for each $X_i$ is $f(x) = \\frac{1}{\\theta}$ for $0 < x < \\theta$, and $F(x) = \\frac{x}{\\theta}$ is the cumulative distribution function (CDF).\n\nWe are asked to find the conditional expectation $E[X_{(2)} + X_{(3)} | X_{(1)} = u, X_{(4)} = v]$.\n\nA key property of order statistics from a uniform distribution is that, conditional on the values of the minimum and maximum statistics, the intermediate statistics behave like order statistics from a uniform distribution on the interval defined by the minimum and maximum.\nSpecifically, given $X_{(1)} = u$ and $X_{(n)} = v$, the set of intermediate order statistics $(X_{(2)}, \\dots, X_{(n-1)})$ has the same distribution as the order statistics of a sample of size $n-2$ drawn from a uniform distribution on the interval $(u, v)$.\n\nIn our problem, $n=4$. So, given $X_{(1)} = u$ and $X_{(4)} = v$, the two central order statistics $(X_{(2)}, X_{(3)})$ are distributed as the order statistics of a random sample of size $n-2 = 2$ from a uniform distribution on $(u, v)$.\n\nLet $Y_1, Y_2$ be two independent and identically distributed random variables from $U(u, v)$. Let their order statistics be $Y_{(1)}$ and $Y_{(2)}$. The conditional distribution of $(X_{(2)}, X_{(3)})$ given $X_{(1)}=u, X_{(4)}=v$ is the same as the distribution of $(Y_{(1)}, Y_{(2)})$.\n\nTherefore, we need to compute $E[Y_{(1)} + Y_{(2)}]$.\nBy the linearity of expectation, we have:\n$$E[Y_{(1)} + Y_{(2)}] = E[Y_{(1)}] + E[Y_{(2)}]$$\nA crucial property of order statistics is that the sum of all order statistics from a sample is equal to the sum of the original random variables in the sample. In our case, this means:\n$$Y_{(1)} + Y_{(2)} = Y_1 + Y_2$$\nTaking the expectation of both sides:\n$$E[Y_{(1)} + Y_{(2)}] = E[Y_1 + Y_2]$$\nAgain, by linearity of expectation:\n$$E[Y_1 + Y_2] = E[Y_1] + E[Y_2]$$\nThe random variables $Y_1$ and $Y_2$ are from a uniform distribution $U(u, v)$. The expected value of a random variable uniformly distributed on an interval $(a,b)$ is $\\frac{a+b}{2}$.\nThus, for $Y_1$ and $Y_2$:\n$$E[Y_1] = \\frac{u+v}{2}$$\n$$E[Y_2] = \\frac{u+v}{2}$$\nSubstituting these back, we get:\n$$E[Y_1 + Y_2] = \\frac{u+v}{2} + \\frac{u+v}{2} = u+v$$\nSo, the conditional expectation is:\n$$E[X_{(2)} + X_{(3)} | X_{(1)} = u, X_{(4)} = v] = E[Y_{(1)} + Y_{(2)}] = u+v$$\n\nAlternatively, we can solve this by direct integration. The joint conditional PDF of $X_{(2)}$ and $X_{(3)}$ given $X_{(1)}=u$ and $X_{(4)}=v$ is that of the order statistics of a sample of size 2 from $U(u,v)$. Let these be $x_2$ and $x_3$. The joint PDF of two i.i.d. variables from $U(u,v)$ is $1/(v-u)^2$ on the square $[u,v] \\times [u,v]$. The joint PDF of their order statistics is given by:\n$$f_{X_{(2)},X_{(3)}|X_{(1)},X_{(4)}}(x_2, x_3|u,v) = \\frac{2!}{(v-u)^2} = \\frac{2}{(v-u)^2} \\quad \\text{for } u  x_2  x_3  v$$\nThe conditional expectation is:\n$$E[X_{(2)} + X_{(3)} | u, v] = \\int_{u}^{v} \\int_{u}^{x_3} (x_2 + x_3) \\frac{2}{(v-u)^2} dx_2 dx_3$$\nFirst, we evaluate the inner integral with respect to $x_2$:\n$$\\int_{u}^{x_3} (x_2 + x_3) dx_2 = \\left[ \\frac{x_2^2}{2} + x_3 x_2 \\right]_{u}^{x_3} = \\left(\\frac{x_3^2}{2} + x_3^2\\right) - \\left(\\frac{u^2}{2} + u x_3\\right) = \\frac{3}{2}x_3^2 - u x_3 - \\frac{u^2}{2}$$\nNow, we integrate this result with respect to $x_3$ from $u$ to $v$:\n$$\\int_{u}^{v} \\left(\\frac{3}{2}x_3^2 - u x_3 - \\frac{u^2}{2}\\right) dx_3 = \\left[ \\frac{3}{2}\\frac{x_3^3}{3} - u\\frac{x_3^2}{2} - \\frac{u^2}{2}x_3 \\right]_{u}^{v}$$\n$$= \\left[ \\frac{x_3^3}{2} - \\frac{u x_3^2}{2} - \\frac{u^2 x_3}{2} \\right]_{u}^{v}$$\n$$= \\left(\\frac{v^3}{2} - \\frac{u v^2}{2} - \\frac{u^2 v}{2}\\right) - \\left(\\frac{u^3}{2} - \\frac{u^3}{2} - \\frac{u^3}{2}\\right)$$\n$$= \\frac{1}{2} (v^3 - u v^2 - u^2 v + u^3)$$\nWe can factor the term in the parenthesis:\n$$v^3 - u v^2 - u^2 v + u^3 = v^2(v-u) - u^2(v-u) = (v^2-u^2)(v-u) = (v+u)(v-u)(v-u) = (v+u)(v-u)^2$$\nSo the integral becomes $\\frac{1}{2}(v+u)(v-u)^2$.\nFinally, we multiply by the constant factor $\\frac{2}{(v-u)^2}$:\n$$E[X_{(2)} + X_{(3)} | u, v] = \\frac{2}{(v-u)^2} \\times \\frac{1}{2}(v+u)(v-u)^2 = u+v$$\nBoth methods yield the same result.", "answer": "$$\\boxed{u+v}$$", "id": "1368704"}]}