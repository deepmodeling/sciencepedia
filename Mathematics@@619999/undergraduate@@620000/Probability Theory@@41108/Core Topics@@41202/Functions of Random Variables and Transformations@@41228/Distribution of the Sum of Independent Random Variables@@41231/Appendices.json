{"hands_on_practices": [{"introduction": "Understanding the sum of independent random variables begins with the discrete case. This exercise provides a concrete starting point by using non-standard dice, demonstrating that the underlying principle of discrete convolution is not limited to symmetric cases. By systematically listing all possible outcomes and their probabilities, you will build the probability mass function for the sum from the ground up, providing a solid foundation for more complex scenarios [@problem_id:1358734].", "problem": "Two non-standard six-sided dice, Die A and Die B, are used in a game. The faces of Die A are labeled with the numbers $\\{1, 1, 2, 2, 5, 6\\}$. The faces of Die B are labeled with the numbers $\\{0, 1, 2, 3, 3, 4\\}$. When a die is rolled, each of its six faces has an equal probability of landing up. The two dice are rolled simultaneously, and their outcomes are independent of one another. Let the random variable $S$ be the sum of the numbers showing on the top faces of the two dice after a single roll. Determine the most likely value (or values) of $S$. If there is more than one such value, list them all.", "solution": "Let Die A have face-value multiplicities $m_{A}(1)=2$, $m_{A}(2)=2$, $m_{A}(5)=1$, $m_{A}(6)=1$, and let Die B have multiplicities $m_{B}(0)=1$, $m_{B}(1)=1$, $m_{B}(2)=1$, $m_{B}(3)=2$, $m_{B}(4)=1$. Because each die roll is uniform over its six faces and the dice are independent, each ordered pair of faces is equally likely with probability $\\frac{1}{36}$. Therefore, if $S$ is the sum, the number of ordered pairs yielding $S=s$ is\n$$\nN(s) \\;=\\; \\sum_{a\\in\\{1,2,5,6\\}} m_{A}(a)\\, m_{B}(s-a),\n$$\nwhere $m_{B}(y)=0$ for $y\\notin\\{0,1,2,3,4\\}$. The probability mass function is $P(S=s)=\\frac{N(s)}{36}$, so the most likely value(s) of $S$ are those maximizing $N(s)$.\n\nCompute $N(s)$ for $s=1,\\dots,10$:\n$$\n\\begin{aligned}\nN(1)&=m_{A}(1)m_{B}(0)=2\\cdot 1=2,\\\\\nN(2)&=m_{A}(1)m_{B}(1)+m_{A}(2)m_{B}(0)=2\\cdot 1+2\\cdot 1=4,\\\\\nN(3)&=m_{A}(1)m_{B}(2)+m_{A}(2)m_{B}(1)=2\\cdot 1+2\\cdot 1=4,\\\\\nN(4)&=m_{A}(1)m_{B}(3)+m_{A}(2)m_{B}(2)=2\\cdot 2+2\\cdot 1=6,\\\\\nN(5)&=m_{A}(1)m_{B}(4)+m_{A}(2)m_{B}(3)+m_{A}(5)m_{B}(0)=2\\cdot 1+2\\cdot 2+1\\cdot 1=7,\\\\\nN(6)&=m_{A}(2)m_{B}(4)+m_{A}(5)m_{B}(1)+m_{A}(6)m_{B}(0)=2\\cdot 1+1\\cdot 1+1\\cdot 1=4,\\\\\nN(7)&=m_{A}(5)m_{B}(2)+m_{A}(6)m_{B}(1)=1\\cdot 1+1\\cdot 1=2,\\\\\nN(8)&=m_{A}(5)m_{B}(3)+m_{A}(6)m_{B}(2)=1\\cdot 2+1\\cdot 1=3,\\\\\nN(9)&=m_{A}(5)m_{B}(4)+m_{A}(6)m_{B}(3)=1\\cdot 1+1\\cdot 2=3,\\\\\nN(10)&=m_{A}(6)m_{B}(4)=1\\cdot 1=1.\n\\end{aligned}\n$$\nThe maximum count is $N(5)=7$, so $S=5$ is the unique most likely value. Equivalently, $P(S=5)=\\frac{7}{36}$ exceeds all other values of $P(S=s)$.", "answer": "$$\\boxed{5}$$", "id": "1358734"}, {"introduction": "Transitioning from discrete to continuous variables, we can often use geometric intuition to solve problems that might otherwise require complex integration. This problem, involving the sum of two independent uniform random variables, is a classic example of this powerful technique [@problem_id:1358721]. By visualizing the joint sample space as a square, you can calculate the desired probability by simply finding the area of the corresponding region, making the concept of a joint distribution tangible and intuitive.", "problem": "In a small-scale automated assembly line, two robotic arms perform consecutive tasks to assemble a product. The time taken by the first arm, let's call it $T_1$, is a random variable that is uniformly distributed between 10 minutes and 20 minutes. The time taken by the second arm, $T_2$, is also uniformly distributed, independently of the first, over the same interval of 10 to 20 minutes. A quality control check is scheduled to occur exactly 35 minutes after the process starts. What is the probability that the two-arm assembly process is completed on or before the quality control check begins?\n\nExpress your answer as a fraction in simplest form.", "solution": "Let $T_{1}$ and $T_{2}$ be independent and uniformly distributed on $[10,20]$. The joint density is\n$$\nf_{T_{1},T_{2}}(t_{1},t_{2})=\\frac{1}{100}\\quad \\text{for }(t_{1},t_{2})\\in[10,20]^{2}, \\text{ and } 0 \\text{ otherwise}.\n$$\nWe need\n$$\n\\mathbb{P}(T_{1}+T_{2}\\leq 35)=\\iint_{D}\\frac{1}{100}\\,dt_{1}\\,dt_{2},\n$$\nwhere $D=\\{(t_{1},t_{2})\\in[10,20]^{2}: t_{1}+t_{2}\\leq 35\\}$.\n\nMake the change of variables $u=t_{1}-10$, $v=t_{2}-10$. Then $(u,v)\\in[0,10]^{2}$, the Jacobian is $1$, and the condition becomes $u+v\\leq 15$. Hence\n$$\n\\mathbb{P}(T_{1}+T_{2}\\leq 35)=\\frac{1}{100}\\cdot \\text{Area}\\left(\\{(u,v)\\in[0,10]^{2}: u+v\\leq 15\\}\\right).\n$$\nIn the square $[0,10]^{2}$, the line $u+v=15$ meets the top and right edges at $(5,10)$ and $(10,5)$, so the excluded region $u+v>15$ is a right triangle with vertices $(5,10)$, $(10,5)$, $(10,10)$ and legs of length $5$. Its area is\n$$\n\\frac{1}{2}\\cdot 5 \\cdot 5=\\frac{25}{2}.\n$$\nTherefore the desired area is\n$$\n100-\\frac{25}{2}=\\frac{175}{2},\n$$\nand the probability is\n$$\n\\frac{1}{100}\\cdot \\frac{175}{2}=\\frac{175}{200}=\\frac{7}{8}.\n$$", "answer": "$$\\boxed{\\frac{7}{8}}$$", "id": "1358721"}, {"introduction": "While geometric methods are insightful, the formal tool for finding the distribution of a sum or difference of continuous random variables is the convolution integral. This problem asks you to derive the full probability density function for the difference of two independent exponential variables, a fundamental exercise that solidifies your understanding of applying the convolution formula [@problem_id:5410]. Successfully solving this requires careful handling of integration limits defined by the variables' supports, a crucial skill in advanced probability.", "problem": "Let $X$ and $Y$ be two independent and identically distributed (i.i.d.) random variables. Both variables follow the standard exponential distribution. The probability density function (PDF) for a standard exponential random variable $T$ is given by:\n$$\nf_T(t) =\n\\begin{cases}\ne^{-t} & \\text{if } t \\ge 0 \\\\\n0 & \\text{if } t < 0\n\\end{cases}\n$$\nConsider a new random variable $Z$ defined as the difference between $X$ and $Y$, such that $Z = X - Y$.\n\nYour task is to derive the probability density function of $Z$, denoted by $f_Z(z)$.", "solution": "We use the convolution formula for the difference $Z=X-Y$ of independent random variables:\n$$\nf_Z(z)=\\int_{-\\infty}^{\\infty}f_X(z+y)\\,f_Y(y)\\,dy.\n$$\nSince $f_X(x)=e^{-x}$ for $x\\ge0$ and $f_Y(y)=e^{-y}$ for $y\\ge0$, the integrand is nonzero only if simultaneously\n$$\nz+y\\ge0,\\quad y\\ge0,\n$$\ni.e., $y\\ge\\max\\{0,-z\\}$. Thus\n$$\nf_Z(z)\n=\\int_{\\max\\{0,-z\\}}^{\\infty}e^{-(z+y)}\\,e^{-y}\\,dy\n=e^{-z}\\int_{\\max\\{0,-z\\}}^{\\infty}e^{-2y}\\,dy.\n$$\nCase 1: $z\\ge0$. Then $\\max\\{0,-z\\}=0$ and\n$$\nf_Z(z)=e^{-z}\\int_{0}^{\\infty}e^{-2y}\\,dy\n=e^{-z}\\cdot\\frac12\n=\\frac12e^{-z}.\n$$\nCase 2: $z<0$. Then $\\max\\{0,-z\\}=-z$ and\n$$\nf_Z(z)\n=e^{-z}\\int_{-z}^{\\infty}e^{-2y}\\,dy\n=e^{-z}\\cdot\\frac12e^{2z}\n=\\frac12e^{z}.\n$$\nCombining both cases gives the Laplace density\n$$\nf_Z(z)=\\frac12e^{-|z|},\\quad z\\in\\mathbb{R}.\n$$", "answer": "$$\\boxed{\\frac12e^{-|z|}}$$", "id": "5410"}]}