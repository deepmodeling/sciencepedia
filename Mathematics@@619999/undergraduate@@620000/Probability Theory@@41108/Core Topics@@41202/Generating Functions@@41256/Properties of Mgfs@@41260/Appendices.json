{"hands_on_practices": [{"introduction": "The Moment-Generating Function (MGF) is not just a computational tool; it's a unique signature for a probability distribution. This first exercise challenges you to reverse-engineer a distribution's identity directly from its MGF. By carefully inspecting the structure of the given function, you can decode the underlying Probability Mass Function (PMF), illustrating the powerful uniqueness property of MGFs [@problem_id:1382493].", "problem": "A discrete random variable $X$ has a Moment Generating Function (MGF), $M_X(t)$, defined for all real values of $t$. The MGF is given by the expression:\n$$M_X(t) = \\frac{1}{4}(1 + e^t)^2$$\nBy inspecting the form of this MGF, determine the Probability Mass Function (PMF) of the random variable $X$. Select the correct PMF from the options below.\n\nA. $P(X=0) = \\frac{1}{4}$, $P(X=1) = \\frac{1}{2}$, $P(X=2) = \\frac{1}{4}$\n\nB. $P(X=1) = \\frac{1}{2}$, $P(X=2) = \\frac{1}{2}$\n\nC. $P(X=0) = \\frac{1}{3}$, $P(X=1) = \\frac{1}{3}$, $P(X=2) = \\frac{1}{3}$\n\nD. $P(X=0) = \\frac{1}{2}$, $P(X=1) = \\frac{1}{2}$\n\nE. $P(X=0) = \\frac{1}{4}$, $P(X=t) = \\frac{1}{2}$, $P(X=2t) = \\frac{1}{4}$", "solution": "By definition, the moment generating function (MGF) of a discrete random variable $X$ is\n$$\nM_{X}(t)=\\mathbb{E}[\\exp(tX)]=\\sum_{x} P(X=x)\\exp(tx),\n$$\nwhere the sum runs over the support of $X$. Therefore, for a discrete $X$ with finitely many support points, $M_{X}(t)$ is a finite linear combination of exponentials $\\exp(tx)$ with coefficients equal to the corresponding probabilities.\n\nGiven\n$$\nM_{X}(t)=\\frac{1}{4}\\bigl(1+\\exp(t)\\bigr)^{2},\n$$\nexpand:\n$$\nM_{X}(t)=\\frac{1}{4}\\left(1+2\\exp(t)+\\exp(2t)\\right)\n=\\frac{1}{4}\\exp(0\\cdot t)+\\frac{1}{2}\\exp(t)+\\frac{1}{4}\\exp(2t).\n$$\nComparing with $\\sum_{x}P(X=x)\\exp(tx)$, the distinct exponents $0$, $1$, and $2$ identify the support points, and the corresponding coefficients are the probabilities:\n$$\nP(X=0)=\\frac{1}{4},\\quad P(X=1)=\\frac{1}{2},\\quad P(X=2)=\\frac{1}{4}.\n$$\nThis matches option A.\n\nEquivalently, note that $M_{X}(t)=\\left(\\frac{1}{2}(1+\\exp(t))\\right)^{2}$ is the square of the MGF of a $\\text{Bernoulli}\\left(\\frac{1}{2}\\right)$ variable, implying $X\\sim\\text{Binomial}\\left(2,\\frac{1}{2}\\right)$, which has the same PMF above.", "answer": "$$\\boxed{A}$$", "id": "1382493"}, {"introduction": "Beyond identification, the true power of an MGF lies in its name: it generates moments. This practice demonstrates the fundamental technique of calculating key statistical measures like mean and variance by differentiating the MGF. You will apply calculus to a given MGF to find the first two raw moments and, from them, the variance of the random variable, a core skill in probabilistic modeling [@problem_id:1382477].", "problem": "A research team is studying the occurrence of a rare type of solar flare. The number of such flares observed in a 24-hour period is modeled as a discrete random variable $X$. Through theoretical modeling and observational data, the team has determined that the moment-generating function (MGF) of $X$ is given by the expression:\n$$M_X(t) = \\exp(5(\\exp(t)-1))$$\nwhere $\\exp(z)$ denotes the exponential function $e^z$.\n\nCalculate the variance of the random variable $X$.", "solution": "We use the definition of the moment-generating function (MGF), $M_{X}(t)=\\mathbb{E}[\\exp(tX)]$, and the fact that the $n$-th derivative of the MGF at $t=0$ gives the $n$-th raw moment: $M_{X}'(0)=\\mathbb{E}[X]$ and $M_{X}''(0)=\\mathbb{E}[X^{2}]$. The variance is $\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}$.\n\nGiven $M_{X}(t)=\\exp(5(\\exp(t)-1))$, differentiate using the chain rule:\n$$\nM_{X}'(t)=\\exp(5(\\exp(t)-1))\\cdot 5\\exp(t).\n$$\nEvaluate at $t=0$ to get the mean:\n$$\n\\mathbb{E}[X]=M_{X}'(0)=\\exp(5(\\exp(0)-1))\\cdot 5\\exp(0)=\\exp(0)\\cdot 5\\cdot 1=5.\n$$\nDifferentiate again to obtain the second moment:\n$$\nM_{X}''(t)=\\frac{d}{dt}\\left(M_{X}(t)\\cdot 5\\exp(t)\\right)=M_{X}'(t)\\cdot 5\\exp(t)+M_{X}(t)\\cdot 5\\exp(t).\n$$\nUsing $M_{X}'(t)=M_{X}(t)\\cdot 5\\exp(t)$, this simplifies to\n$$\nM_{X}''(t)=M_{X}(t)\\cdot 25\\exp(2t)+M_{X}(t)\\cdot 5\\exp(t)=M_{X}(t)\\left(25\\exp(2t)+5\\exp(t)\\right).\n$$\nEvaluate at $t=0$:\n$$\n\\mathbb{E}[X^{2}]=M_{X}''(0)=M_{X}(0)\\left(25\\exp(0)+5\\exp(0)\\right)=1\\cdot(25+5)=30.\n$$\nThus,\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=30-5^{2}=5.\n$$\nAs a consistency check, $M_{X}(t)=\\exp(\\lambda(\\exp(t)-1))$ is the MGF of a Poisson distribution with parameter $\\lambda$, here $\\lambda=5$, for which $\\operatorname{Var}(X)=\\lambda=5$.", "answer": "$$\\boxed{5}$$", "id": "1382477"}, {"introduction": "One of the most elegant properties of MGFs is how they simplify the analysis of sums of independent random variables, a task that would otherwise require complex convolution integrals. This final practice explores this concept, which is crucial for understanding many phenomena in science and engineering. You will discover that the MGF of a sum of independent variables is simply the product of their individual MGFs, a property that greatly simplifies the analysis of aggregate processes [@problem_id:1382515].", "problem": "In a simplified model of a data processing center, incoming tasks are handled sequentially by a single server. The time required to complete each task is an independent and identically distributed random variable. Let $X_i$ be the processing time for the $i$-th task. Each $X_i$ follows an exponential distribution with a rate parameter $\\lambda > 0$.\n\nThe Moment Generating Function (MGF) of a single exponential random variable $X$ with rate parameter $\\lambda$ is given by:\n$$M_X(t) = \\frac{\\lambda}{\\lambda - t}$$\nfor $t < \\lambda$.\n\nConsider a batch of $n$ tasks. Let $S_n$ be the total time required to process all $n$ tasks, so that $S_n = \\sum_{i=1}^n X_i$.\n\nDetermine the MGF of the total processing time, $M_{S_n}(t)$. Express your answer as a function of $n$, $\\lambda$, and $t$.", "solution": "We are given independent and identically distributed random variables $X_{1},\\ldots,X_{n}$, each exponential with rate parameter $\\lambda>0$, with MGF\n$$\nM_{X}(t)=\\frac{\\lambda}{\\lambda-t}, \\quad t<\\lambda.\n$$\nDefine the total processing time $S_{n}=\\sum_{i=1}^{n}X_{i}$. By the definition of the moment generating function,\n$$\nM_{S_{n}}(t)=\\mathbb{E}\\left[\\exp\\left(tS_{n}\\right)\\right]=\\mathbb{E}\\left[\\exp\\left(t\\sum_{i=1}^{n}X_{i}\\right)\\right]=\\mathbb{E}\\left[\\prod_{i=1}^{n}\\exp\\left(tX_{i}\\right)\\right].\n$$\nUsing independence of the $X_{i}$, the expectation of a product equals the product of expectations:\n$$\nM_{S_{n}}(t)=\\prod_{i=1}^{n}\\mathbb{E}\\left[\\exp\\left(tX_{i}\\right)\\right]=\\prod_{i=1}^{n}M_{X}(t).\n$$\nSince the $X_{i}$ are identically distributed, each factor equals $M_{X}(t)$, hence\n$$\nM_{S_{n}}(t)=\\left(M_{X}(t)\\right)^{n}=\\left(\\frac{\\lambda}{\\lambda-t}\\right)^{n}, \\quad t<\\lambda.\n$$\nThis is the MGF of a gamma distribution with shape parameter $n$ and rate parameter $\\lambda$, consistent with $S_{n}$ being the sum of $n$ i.i.d. exponential variables.", "answer": "$$\\boxed{\\left(\\frac{\\lambda}{\\lambda-t}\\right)^{n}}$$", "id": "1382515"}]}