{"hands_on_practices": [{"introduction": "A primary application of a Probability Generating Function (PGF) is the efficient calculation of a distribution's moments, such as its mean and variance. The derivatives of the PGF, when evaluated at $s=1$, are directly linked to the factorial moments, from which the variance can be easily derived. This exercise [@problem_id:1380035] provides foundational practice in applying this powerful calculus-based method to a given PGF that models a common scenario in digital communications.", "problem": "In a simplified model of a digital communication channel, a message consisting of a sequence of bits is transmitted. Due to noise, each bit has a certain probability of being corrupted. Consider a packet of 10 bits being sent. Let the random variable $X$ represent the number of corrupted bits within this packet. The statistical properties of $X$ can be fully described by its Probability Generating Function (PGF), which is given by:\n$$G_X(s) = \\left( \\frac{1+s}{2} \\right)^{10}$$\nUsing this PGF, calculate the variance of the number of corrupted bits, $X$. Express your answer as a single number.", "solution": "The variance of a random variable $X$, denoted as $\\text{Var}(X)$, can be calculated from its Probability Generating Function (PGF), $G_X(s)$, using fundamental properties of PGFs. The formula for variance is $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n\nThe expected value, $\\mathbb{E}[X]$, and the second factorial moment, $\\mathbb{E}[X(X-1)]$, can be found by evaluating the derivatives of the PGF at $s=1$:\n$\\mathbb{E}[X] = G_X'(1)$\n$\\mathbb{E}[X(X-1)] = G_X''(1)$\n\nWe can express $\\mathbb{E}[X^2]$ in terms of these quantities:\n$\\mathbb{E}[X^2] = \\mathbb{E}[X^2 - X + X] = \\mathbb{E}[X(X-1) + X] = \\mathbb{E}[X(X-1)] + \\mathbb{E}[X]$\nSubstituting the derivative forms, we get:\n$\\mathbb{E}[X^2] = G_X''(1) + G_X'(1)$\n\nNow, we can write the variance entirely in terms of the PGF and its derivatives:\n$\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = (G_X''(1) + G_X'(1)) - [G_X'(1)]^2$\n\nOur first step is to calculate the first and second derivatives of the given PGF, $G_X(s) = \\left( \\frac{1+s}{2} \\right)^{10}$.\n\nStep 1: Calculate the first derivative, $G_X'(s)$.\nUsing the chain rule, where the outer function is $u^{10}$ and the inner function is $u = \\frac{1+s}{2}$:\n$$G_X'(s) = \\frac{d}{ds} \\left( \\frac{1+s}{2} \\right)^{10} = 10 \\left( \\frac{1+s}{2} \\right)^9 \\cdot \\frac{d}{ds}\\left( \\frac{1+s}{2} \\right)$$\n$$G_X'(s) = 10 \\left( \\frac{1+s}{2} \\right)^9 \\cdot \\frac{1}{2} = 5 \\left( \\frac{1+s}{2} \\right)^9$$\n\nStep 2: Evaluate $G_X'(1)$ to find $\\mathbb{E}[X]$.\n$$G_X'(1) = 5 \\left( \\frac{1+1}{2} \\right)^9 = 5 \\left( \\frac{2}{2} \\right)^9 = 5(1)^9 = 5$$\nSo, the expected number of corrupted bits is $\\mathbb{E}[X] = 5$.\n\nStep 3: Calculate the second derivative, $G_X''(s)$.\nWe differentiate $G_X'(s)$:\n$$G_X''(s) = \\frac{d}{ds} \\left[ 5 \\left( \\frac{1+s}{2} \\right)^9 \\right]$$\nApplying the chain rule again:\n$$G_X''(s) = 5 \\cdot 9 \\left( \\frac{1+s}{2} \\right)^8 \\cdot \\frac{d}{ds}\\left( \\frac{1+s}{2} \\right)$$\n$$G_X''(s) = 45 \\left( \\frac{1+s}{2} \\right)^8 \\cdot \\frac{1}{2} = \\frac{45}{2} \\left( \\frac{1+s}{2} \\right)^8$$\n\nStep 4: Evaluate $G_X''(1)$ to find $\\mathbb{E}[X(X-1)]$.\n$$G_X''(1) = \\frac{45}{2} \\left( \\frac{1+1}{2} \\right)^8 = \\frac{45}{2} \\left( \\frac{2}{2} \\right)^8 = \\frac{45}{2}(1)^8 = \\frac{45}{2} = 22.5$$\nSo, the second factorial moment is $\\mathbb{E}[X(X-1)] = 22.5$.\n\nStep 5: Calculate the variance.\nUsing the formula $\\text{Var}(X) = G_X''(1) + G_X'(1) - [G_X'(1)]^2$:\n$$\\text{Var}(X) = 22.5 + 5 - (5)^2$$\n$$\\text{Var}(X) = 27.5 - 25$$\n$$\\text{Var}(X) = 2.5$$", "answer": "$$\\boxed{2.5}$$", "id": "1380035"}, {"introduction": "While PGFs are excellent for finding moments, their name reveals their core purpose: they generate probabilities. A PGF is fundamentally a power series in a dummy variable $s$, where the coefficient of the $s^k$ term is precisely the probability $P(X=k)$. This practice problem [@problem_id:1380075] challenges you to work backwards, using techniques like the geometric series expansion to decompose a PGF and extract the underlying probability mass function to answer questions about the random variable.", "problem": "In the study of a particular branching process, the number of offspring, $X$, produced by an individual in a single generation is a discrete random variable whose support is the set of positive integers $\\{1, 2, 3, \\dots\\}$. The statistical behavior of this process is compactly described by its Probability Generating Function (PGF). The PGF for the random variable $X$ is given by\n$$G_X(s) = \\frac{s}{2-s}$$\nCalculate the probability that an individual produces at most two offspring, i.e., find $P(X \\le 2)$. Express your answer as an exact fraction.", "solution": "The probability generating function (PGF) of a nonnegative integer-valued random variable $X$ is defined as $G_{X}(s)=\\mathbb{E}[s^{X}]=\\sum_{k=0}^{\\infty}\\mathbb{P}(X=k)s^{k}$. Since the support here is $\\{1,2,3,\\dots\\}$, we have $\\mathbb{P}(X=0)=0$, so the series effectively starts at $k=1$.\n\nGiven $G_{X}(s)=\\dfrac{s}{2-s}$, rewrite it to extract the power series using the geometric series identity. First write\n$$\nG_{X}(s)=\\frac{s}{2-s}=\\frac{s}{2}\\cdot\\frac{1}{1-\\frac{s}{2}}.\n$$\nUsing the identity $\\dfrac{1}{1-r}=\\sum_{n=0}^{\\infty}r^{n}$ for $|r|<1$, with $r=\\dfrac{s}{2}$, we obtain\n$$\nG_{X}(s)=\\frac{s}{2}\\sum_{n=0}^{\\infty}\\left(\\frac{s}{2}\\right)^{n}\n=\\sum_{n=0}^{\\infty}\\frac{s^{n+1}}{2^{n+1}}\n=\\sum_{k=1}^{\\infty}\\frac{1}{2^{k}}\\,s^{k},\n$$\nwhere we relabeled $k=n+1$. Matching coefficients with the PGF definition gives\n$$\n\\mathbb{P}(X=k)=\\frac{1}{2^{k}},\\quad k\\geq 1.\n$$\nTherefore,\n$$\n\\mathbb{P}(X\\leq 2)=\\mathbb{P}(X=1)+\\mathbb{P}(X=2)=\\frac{1}{2}+\\frac{1}{4}=\\frac{3}{4}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1380075"}, {"introduction": "Moving beyond analyzing a given PGF, this final exercise demonstrates how to construct one for a more complex process. For problems involving waiting times for a specific pattern in a sequence of trials, we can formulate recurrence relations for the PGF using a technique called first-step analysis. This advanced problem [@problem_id:1325370] exemplifies the true analytical power of PGFs, allowing you to encapsulate the entire stochastic process of a handshake protocol into a single, elegant rational function.", "problem": "In a simplified model of a digital communication handshake protocol, a client repeatedly sends data packets to a server. Each packet transmission is an independent event and is successfully received by the server with a constant probability $p$, where $0 < p < 1$. A packet transmission that is not successful is considered a failure, which occurs with probability $q = 1-p$. The handshake protocol is considered successfully completed only after the server has received two consecutive successful packets.\n\nLet $N$ be the random variable representing the total number of packets the client must send until the handshake is successfully completed. Determine the probability generating function, $G(z) = \\mathbb{E}[z^N]$, for the random variable $N$. Express your answer as a single rational function in terms of $p$ and $z$.", "solution": "Let $N$ be the number of Bernoulli trials with success probability $p$ required to obtain two consecutive successes. Model the process by a Markov chain with states:\n- $S_{0}$: no current run of successes,\n- $S_{1}$: a current run of exactly one success,\n- absorbing after two consecutive successes.\n\nLet $G_{0}(z)$ be the probability generating function of the additional number of trials to absorption starting from $S_{0}$, and $G_{1}(z)$ similarly from $S_{1}$. The required generating function is $G(z)=G_{0}(z)$.\n\nBy first-step analysis:\n- From $S_{0}$, one trial occurs (factor $z$). With probability $p$ we move to $S_{1}$; with probability $1-p$ we remain in $S_{0}$. Hence\n$$\nG_{0}(z)=z\\left(p\\,G_{1}(z)+(1-p)\\,G_{0}(z)\\right).\n$$\n- From $S_{1}$, one trial occurs (factor $z$). With probability $p$ we are absorbed (no further trials, generating factor $1$); with probability $1-p$ we move to $S_{0}$. Hence\n$$\nG_{1}(z)=z\\left(p+(1-p)\\,G_{0}(z)\\right).\n$$\n\nSolve this linear system. From the first equation,\n$$\nG_{0}(z)\\left(1-(1-p)z\\right)=z\\,p\\,G_{1}(z)\\quad\\Rightarrow\\quad\nG_{1}(z)=\\frac{G_{0}(z)\\left(1-(1-p)z\\right)}{z\\,p}.\n$$\nSubstitute into the second equation:\n$$\n\\frac{G_{0}(z)\\left(1-(1-p)z\\right)}{z\\,p}=z\\left(p+(1-p)\\,G_{0}(z)\\right).\n$$\nMultiply by $z\\,p$ and collect terms in $G_{0}(z)$:\n$$\nG_{0}(z)\\left(1-(1-p)z - z^{2}p(1-p)\\right)=z^{2}p^{2}.\n$$\nTherefore,\n$$\nG(z)=G_{0}(z)=\\frac{p^{2}z^{2}}{1-(1-p)z - p(1-p)z^{2}}.\n$$\nThis is a single rational function in terms of $p$ and $z$.", "answer": "$$\\boxed{\\frac{p^{2} z^{2}}{1-(1-p) z-p(1-p) z^{2}}}$$", "id": "1325370"}]}