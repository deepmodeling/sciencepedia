{"hands_on_practices": [{"introduction": "The best way to understand characteristic functions is to compute one from scratch. This first exercise walks you through calculating the characteristic function for one of the simplest non-trivial random variables, which models a fair coin toss. By applying the fundamental definition $\\phi_X(t) = E[\\exp(itX)]$, you will transform a probability mass function into its corresponding characteristic function, a cornerstone skill in probabilistic analysis [@problem_id:1381803].", "problem": "Consider a random variable $X$ that models the outcome of a single toss of a fair coin. If the coin lands on heads, $X$ takes the value $1$. If the coin lands on tails, $X$ takes the value $-1$. The characteristic function of a random variable $Y$, denoted by $\\phi_Y(t)$, is defined as the expected value $E[\\exp(itY)]$, where $t$ is a real number and $i$ is the imaginary unit such that $i^2 = -1$.\n\nDetermine the characteristic function $\\phi_X(t)$ for the random variable $X$.", "solution": "We are given a fair coin modeled by a random variable $X$ taking values $1$ and $-1$ with probabilities $P(X=1)=\\frac{1}{2}$ and $P(X=-1)=\\frac{1}{2}$. The characteristic function of $X$ is defined for real $t$ by\n$$\n\\phi_{X}(t)=E[\\exp(itX)].\n$$\nUsing the law of the unconscious statistician (definition of expectation for discrete variables), we write\n$$\n\\phi_{X}(t)=\\sum_{x\\in\\{-1,1\\}}\\exp(itx)P(X=x)=\\exp(it)\\cdot\\frac{1}{2}+\\exp(-it)\\cdot\\frac{1}{2}.\n$$\nFactor out $\\frac{1}{2}$ to obtain\n$$\n\\phi_{X}(t)=\\frac{1}{2}\\left(\\exp(it)+\\exp(-it)\\right).\n$$\nBy the Euler identity for the cosine function,\n$$\n\\cos(t)=\\frac{\\exp(it)+\\exp(-it)}{2},\n$$\nwe conclude\n$$\n\\phi_{X}(t)=\\cos(t).\n$$", "answer": "$$\\boxed{\\cos(t)}$$", "id": "1381803"}, {"introduction": "One of the most powerful properties of characteristic functions is how they simplify the analysis of sums and differences of independent random variables. While finding the distribution of $Y = X_1 - X_2$ using probability density functions requires a convolution integral, characteristic functions turn this operation into a simple multiplication. This practice demonstrates this elegant property, where you will find the characteristic function of the difference between two independent component lifetimes [@problem_id:1381758].", "problem": "Let $X_1$ and $X_2$ be two independent and identically distributed random variables representing the operational lifetimes of two server components. Their common characteristic function, $\\phi_X(t)$, is given by\n$$ \\phi_X(t) = \\frac{\\lambda}{\\lambda - it} $$\nwhere $\\lambda$ is a positive real constant representing the failure rate parameter, $t$ is a real-valued variable, and $i$ is the imaginary unit ($i^2 = -1$).\n\nA system analyst is interested in the statistical properties of the difference in lifetimes between these two components. Let a new random variable $Y$ be defined as this difference, such that $Y = X_1 - X_2$.\n\nDetermine the characteristic function, $\\phi_Y(t)$, of the random variable $Y$. Express your answer as a single closed-form analytic expression in terms of $\\lambda$ and $t$.", "solution": "By definition, the characteristic function of a real-valued random variable $Z$ is $\\phi_{Z}(t)=\\mathbb{E}[\\exp(i t Z)]$. For $Y=X_{1}-X_{2}$ with $X_{1}$ and $X_{2}$ independent, we have\n$$\n\\phi_{Y}(t)=\\mathbb{E}\\left[\\exp\\left(i t (X_{1}-X_{2})\\right)\\right].\n$$\nUsing independence and the multiplicative property of characteristic functions for sums (and hence differences),\n$$\n\\phi_{Y}(t)=\\mathbb{E}\\left[\\exp(i t X_{1})\\right]\\mathbb{E}\\left[\\exp(-i t X_{2})\\right]=\\phi_{X}(t)\\,\\phi_{X}(-t).\n$$\nGiven $\\phi_{X}(t)=\\frac{\\lambda}{\\lambda - i t}$, we compute\n$$\n\\phi_{X}(-t)=\\frac{\\lambda}{\\lambda - i(-t)}=\\frac{\\lambda}{\\lambda + i t}.\n$$\nTherefore,\n$$\n\\phi_{Y}(t)=\\frac{\\lambda}{\\lambda - i t}\\cdot\\frac{\\lambda}{\\lambda + i t}=\\frac{\\lambda^{2}}{(\\lambda - i t)(\\lambda + i t)}=\\frac{\\lambda^{2}}{\\lambda^{2}+t^{2}}.\n$$\nThis is a single closed-form analytic expression in terms of $\\lambda$ and $t$.", "answer": "$$\\boxed{\\frac{\\lambda^{2}}{\\lambda^{2}+t^{2}}}$$", "id": "1381758"}, {"introduction": "Characteristic functions are not just theoretical constructs; they are powerful analytical tools for uncovering the properties of a distribution. A key application is the calculation of moments, such as the mean and variance, by taking derivatives of the characteristic function at the origin. In this exercise, you will use this technique to calculate the mean and variance for a Poisson-distributed random variable, and in doing so, verify its famous property that these two measures are equal [@problem_id:1381780].", "problem": "A random variable $X$ follows a Poisson distribution with a rate parameter $\\lambda > 0$. The characteristic function of $X$, denoted by $\\phi_X(t)$, is given by:\n$$ \\phi_X(t) = \\exp\\left(\\lambda\\left(e^{it} - 1\\right)\\right) $$\nwhere $t$ is a real number and $i$ is the imaginary unit, satisfying $i^2 = -1$.\n\nThe moments of a random variable can be determined from its characteristic function. The $k$-th raw moment, $E[X^k]$, is given by the formula:\n$$ E[X^k] = \\frac{1}{i^k} \\left[ \\frac{d^k \\phi_X(t)}{dt^k} \\right]_{t=0} $$\nwhere the expression in the brackets denotes the $k$-th derivative of the characteristic function evaluated at $t=0$.\n\nUsing this relationship, first calculate the mean of the distribution, $\\mu = E[X]$, and then calculate its variance, $\\sigma^2 = E[X^2] - (E[X])^2$.\n\nExpress your final answer as a single analytic expression in terms of $\\lambda$ for the mean and the variance, presented as a row vector $\\begin{pmatrix} \\mu & \\sigma^2 \\end{pmatrix}$.", "solution": "We are given the characteristic function $\\phi_{X}(t)=\\exp\\!\\left(\\lambda\\left(\\exp(i t)-1\\right)\\right)$ and the general relation for raw moments $E[X^{k}] = \\frac{1}{i^{k}} \\left.\\frac{d^{k}\\phi_{X}(t)}{dt^{k}}\\right|_{t=0}$.\n\nTo find the mean $\\mu=E[X]$, set $k=1$. Differentiate $\\phi_{X}(t)$ using the chain rule. Let $g(t)=\\lambda\\left(\\exp(i t)-1\\right)$, so $\\phi_{X}(t)=\\exp(g(t))$ and $g'(t)=\\lambda\\,i\\,\\exp(i t)$. Therefore,\n$$\n\\phi_{X}'(t)=\\exp(g(t))\\,g'(t)=\\phi_{X}(t)\\,\\lambda\\,i\\,\\exp(i t).\n$$\nEvaluate at $t=0$: $\\phi_{X}(0)=\\exp(\\lambda(\\exp(0)-1))=\\exp(\\lambda(1-1))=1$ and $\\exp(i\\cdot 0)=1$, hence\n$$\n\\phi_{X}'(0)=1\\cdot\\lambda\\,i\\cdot 1 = i\\lambda.\n$$\nThus,\n$$\nE[X]=\\frac{1}{i}\\,\\phi_{X}'(0)=\\frac{1}{i}\\,(i\\lambda)=\\lambda.\n$$\n\nTo find $E[X^{2}]$, set $k=2$. Differentiate $\\phi_{X}'(t)=\\phi_{X}(t)\\,A(t)$ where $A(t)=\\lambda\\,i\\,\\exp(i t)$. By the product rule,\n$$\n\\phi_{X}''(t)=\\phi_{X}'(t)\\,A(t)+\\phi_{X}(t)\\,A'(t)=\\phi_{X}(t)\\left(A(t)^{2}+A'(t)\\right).\n$$\nCompute $A'(t)=\\lambda\\,i\\cdot i\\,\\exp(i t)=\\lambda\\,i^{2}\\exp(i t)=-\\lambda\\,\\exp(i t)$ and $A(t)^{2}=\\lambda^{2}\\,i^{2}\\,\\exp(2 i t)=-\\lambda^{2}\\,\\exp(2 i t)$. Therefore,\n$$\n\\phi_{X}''(t)=\\phi_{X}(t)\\left(-\\lambda^{2}\\exp(2 i t)-\\lambda\\,\\exp(i t)\\right).\n$$\nAt $t=0$, using $\\phi_{X}(0)=1$ and $\\exp(0)=1$,\n$$\n\\phi_{X}''(0)=-\\lambda^{2}-\\lambda.\n$$\nHence,\n$$\nE[X^{2}]=\\frac{1}{i^{2}}\\,\\phi_{X}''(0)=-1\\cdot\\left(-\\lambda^{2}-\\lambda\\right)=\\lambda^{2}+\\lambda.\n$$\n\nFinally, the variance is $\\sigma^{2}=E[X^{2}]-(E[X])^{2}=(\\lambda^{2}+\\lambda)-\\lambda^{2}=\\lambda$. Therefore, the requested row vector of mean and variance is $\\begin{pmatrix}\\lambda & \\lambda\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\lambda & \\lambda \\end{pmatrix}}$$", "id": "1381780"}]}