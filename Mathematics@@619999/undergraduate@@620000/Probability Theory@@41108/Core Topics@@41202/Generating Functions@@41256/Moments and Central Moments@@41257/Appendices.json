{"hands_on_practices": [{"introduction": "The journey into understanding probability distributions often begins with their central tendency. The first moment, known as the expected value or mean, provides a single number summarizing the \"center\" of a distribution. This practice will guide you through the fundamental steps of calculating the expected value for a discrete random variable, a core skill that involves first normalizing the probability mass function and then applying the definition of expectation [@problem_id:1376505].", "problem": "In a simplified model for the stability of a small molecule, the number of shared electron pairs, denoted by the random variable $K$, determines the type of covalent bond. The possible values for $K$ are 1, 2, or 3, corresponding to a single, double, or triple bond, respectively. According to this model, the probability of a molecule forming with $k$ shared pairs is directly proportional to the square of the number of pairs. The probability mass function is therefore given by $P(K=k) = c \\cdot k^2$ for $k \\in \\{1, 2, 3\\}$, where $c$ is a normalization constant.\n\nCalculate the expected number of shared electron pairs for this molecular bond. Express your answer as an exact fraction in simplest form.", "solution": "The probability mass function is $P(K=k)=c\\,k^{2}$ for $k\\in\\{1,2,3\\}$, where $c$ is chosen so that the probabilities sum to $1$. The normalization condition is\n$$\n\\sum_{k=1}^{3} P(K=k)=1 \\quad \\Longrightarrow \\quad c\\sum_{k=1}^{3} k^{2}=1.\n$$\nCompute the sum of squares:\n$$\n\\sum_{k=1}^{3} k^{2}=1^{2}+2^{2}+3^{2}=1+4+9=14,\n$$\nso\n$$\nc=\\frac{1}{14}.\n$$\nThe expected value is\n$$\n\\mathbb{E}[K]=\\sum_{k=1}^{3} k\\,P(K=k)=\\sum_{k=1}^{3} k\\left(c\\,k^{2}\\right)=c\\sum_{k=1}^{3} k^{3}.\n$$\nCompute the sum of cubes:\n$$\n\\sum_{k=1}^{3} k^{3}=1^{3}+2^{3}+3^{3}=1+8+27=36.\n$$\nTherefore,\n$$\n\\mathbb{E}[K]=c\\cdot 36=\\frac{36}{14}=\\frac{18}{7}.\n$$", "answer": "$$\\boxed{\\frac{18}{7}}$$", "id": "1376505"}, {"introduction": "Beyond the central point of a distribution, we often need to understand its spread or variability. The variance, defined as the second central moment, is the most common measure for this dispersion. This problem transitions from discrete to continuous random variables, challenging you to compute the variance by first finding the first moment ($E[r]$) and the second moment ($E[r^2]$) through integration, a quintessential application of calculus in probability theory [@problem_id:1376543].", "problem": "In the quality control process for manufacturing circular silicon wafers, the location of surface defects is a critical parameter. For a specific fabrication technique, it is observed that defects are more likely to appear further from the center of the wafer.\n\nLet the random variable $r$ represent the radial distance of a defect from the center of a wafer which has a fixed radius $R$. The probability density function (PDF) for $r$ is modeled by the function:\n$$f(r) = \\begin{cases} C r^{n} & \\text{for } 0 \\leq r \\leq R \\\\ 0 & \\text{otherwise} \\end{cases}$$\nHere, $C$ is a normalization constant, and $n$ is a known positive constant that characterizes the defect distribution for the given process.\n\nDetermine the variance of the radial position of a defect, $\\text{Var}(r)$. Express your answer as a closed-form analytic expression in terms of the wafer radius $R$ and the process parameter $n$.", "solution": "We are given the radial distance random variable with PDF $f(r)=C r^{n}$ for $0 \\leq r \\leq R$ and $0$ otherwise, where $n>0$. First, determine the normalization constant $C$ by enforcing $\\int_{0}^{R} f(r)\\,dr=1$:\n$$\n\\int_{0}^{R} C r^{n}\\,dr = C \\int_{0}^{R} r^{n}\\,dr = C \\left[\\frac{r^{n+1}}{n+1}\\right]_{0}^{R} = C \\frac{R^{n+1}}{n+1} = 1.\n$$\nThus,\n$$\nC = \\frac{n+1}{R^{n+1}}.\n$$\nTo compute the variance, use $\\mathrm{Var}(r)=\\mathbb{E}[r^{2}] - \\left(\\mathbb{E}[r]\\right)^{2}$. For general moment $\\mathbb{E}[r^{k}]$,\n$$\n\\mathbb{E}[r^{k}] = \\int_{0}^{R} r^{k} f(r)\\,dr = C \\int_{0}^{R} r^{n+k}\\,dr = \\frac{n+1}{R^{n+1}} \\cdot \\frac{R^{n+k+1}}{n+k+1} = \\frac{n+1}{n+k+1} R^{k}.\n$$\nHence,\n$$\n\\mathbb{E}[r] = \\frac{n+1}{n+2} R,\\qquad \\mathbb{E}[r^{2}] = \\frac{n+1}{n+3} R^{2}.\n$$\nTherefore,\n$$\n\\mathrm{Var}(r) = \\frac{n+1}{n+3} R^{2} - \\left(\\frac{n+1}{n+2} R\\right)^{2} = R^{2}\\left(\\frac{n+1}{n+3} - \\frac{(n+1)^{2}}{(n+2)^{2}}\\right).\n$$\nCombine the terms over the common denominator $(n+3)(n+2)^{2}$:\n$$\n\\mathrm{Var}(r) = R^{2} \\cdot \\frac{(n+1)\\left[(n+2)^{2} - (n+1)(n+3)\\right]}{(n+3)(n+2)^{2}}.\n$$\nCompute the bracket:\n$$\n(n+2)^{2} - (n+1)(n+3) = (n^{2}+4n+4) - (n^{2}+4n+3) = 1.\n$$\nThus,\n$$\n\\mathrm{Var}(r) = \\frac{(n+1) R^{2}}{(n+3)(n+2)^{2}}.\n$$", "answer": "$$\\boxed{\\frac{(n+1) R^{2}}{(n+3)(n+2)^{2}}}$$", "id": "1376543"}, {"introduction": "Moments are not limited to single variables; they are also powerful tools for describing the relationship between two or more random variables. A key concept here is covariance, which measures how two variables change together. This exercise explores the subtle but critical distinction between uncorrelated variables (zero covariance) and independent variables, providing a hands-on opportunity to prove that the former does not necessarily imply the latter [@problem_id:1376519].", "problem": "Consider two discrete random variables, $X$ and $Y$. Their joint probability mass function, $P(X=x, Y=y)$, is non-zero only at four specific points. The probability is distributed uniformly over these points, as defined below:\n$$\nP(X=x, Y=y) = \n\\begin{cases} \n1/4 & \\text{if } (x,y) \\in \\{(-1, 0), (1, 0), (0, 1), (0, -1)\\} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nBased on this distribution, calculate the following four quantities:\n1.  The expectation of $X$, denoted as $E[X]$.\n2.  The expectation of the product of the two variables, $E[XY]$.\n3.  The product of the marginal probabilities $P_X(1) \\cdot P_Y(1)$.\n4.  The joint probability $P(X=1, Y=1)$.\n\nPresent your four answers as a row matrix, in the order they were requested.", "solution": "The joint probability mass function has support only at the four points $(-1,0)$, $(1,0)$, $(0,1)$, and $(0,-1)$, each with probability $\\frac{1}{4}$, and is zero elsewhere.\n\nThe expectation of $X$ is computed by the definition $E[X]=\\sum_{x}\\sum_{y} x\\,P(X=x,Y=y)$. Restricting to the support,\n$$\nE[X]=(-1)\\cdot \\frac{1}{4} + (1)\\cdot \\frac{1}{4} + (0)\\cdot \\frac{1}{4} + (0)\\cdot \\frac{1}{4} = 0.\n$$\n\nThe expectation of the product $XY$ is $E[XY]=\\sum_{x}\\sum_{y} xy\\,P(X=x,Y=y)$. On the support,\n$$\nE[XY]=(-1)\\cdot 0 \\cdot \\frac{1}{4} + (1)\\cdot 0 \\cdot \\frac{1}{4} + (0)\\cdot 1 \\cdot \\frac{1}{4} + (0)\\cdot (-1) \\cdot \\frac{1}{4} = 0.\n$$\n\nThe marginal $P_{X}(1)$ is $P_{X}(1)=\\sum_{y}P(X=1,Y=y)=P(1,0)=\\frac{1}{4}$. The marginal $P_{Y}(1)$ is $P_{Y}(1)=\\sum_{x}P(X=x,Y=1)=P(0,1)=\\frac{1}{4}$. Therefore,\n$$\nP_{X}(1)\\cdot P_{Y}(1)=\\frac{1}{4}\\cdot \\frac{1}{4}=\\frac{1}{16}.\n$$\n\nFinally, the joint probability at $(1,1)$ is\n$$\nP(X=1,Y=1)=P(1,1)=0,\n$$\nsince $(1,1)$ is not in the support.\n\nCollecting the results in the requested order and expressing them as a row matrix gives\n$$\n\\begin{pmatrix}\n0 & 0 & \\frac{1}{16} & 0\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0 & 0 & \\frac{1}{16} & 0\\end{pmatrix}}$$", "id": "1376519"}]}