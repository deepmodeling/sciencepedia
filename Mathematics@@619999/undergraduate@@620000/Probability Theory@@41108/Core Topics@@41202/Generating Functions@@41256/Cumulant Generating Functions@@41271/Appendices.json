{"hands_on_practices": [{"introduction": "Let's begin by building our intuition with the simplest possible scenario: a \"random\" variable that is not random at all. By deriving the Cumulant Generating Function (CGF) for a deterministic constant, we can clearly see how the CGF framework encodes basic properties like the mean and the complete absence of variance or skewness [@problem_id:1354912]. This exercise provides a foundational baseline for understanding more complex distributions.", "problem": "A random variable $X$ is defined to model a perfectly deterministic process. The outcome of this process is always a specific real-valued constant, $c$. Consequently, the probability mass function of $X$ is given by $P(X=c) = 1$.\n\nThe Moment Generating Function (MGF) of a random variable $Y$ is defined as $M_Y(t) = E[\\exp(tY)]$, where $E[\\cdot]$ denotes the expectation value. The Cumulant Generating Function (CGF), denoted $K_Y(t)$, is defined as the natural logarithm of the MGF, i.e., $K_Y(t) = \\ln(M_Y(t))$.\n\nDetermine the Cumulant Generating Function, $K_X(t)$, for the random variable $X$. Express your answer in terms of $t$ and the constant $c$.", "solution": "The random variable satisfies $P(X=c)=1$, so $X=c$ almost surely. For any measurable function $g$, this implies $E[g(X)]=g(c)$. Using the definition of the moment generating function,\n$$\nM_{X}(t)=E[\\exp(tX)].\n$$\nSince $X=c$ almost surely, $\\exp(tX)=\\exp(tc)$ almost surely, and the expectation of a constant equals that constant. Therefore,\n$$\nM_{X}(t)=\\exp(tc).\n$$\nBy definition, the cumulant generating function is the natural logarithm of the MGF:\n$$\nK_{X}(t)=\\ln(M_{X}(t))=\\ln(\\exp(tc))=tc.\n$$\nThus, the CGF exists for all real $t$ and is linear in $t$ with slope $c$.", "answer": "$$\\boxed{tc}$$", "id": "1354912"}, {"introduction": "A core utility of the Cumulant Generating Function is its role as a \"factory\" for producing moments. This practice puts that idea into action by having you calculate the expected value, or mean, of a Gamma-distributed random variable directly from its given CGF [@problem_id:1354878]. This demonstrates the powerful and direct relationship between the derivatives of the CGF and the cumulants of the distribution, where the first cumulant, $\\kappa_1$, is precisely the mean.", "problem": "Let $X$ be a continuous random variable that follows a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\beta > 0$.\n\nThe Cumulant Generating Function (CGF) for the random variable $X$ is given by the expression:\n$$ K_X(t) = \\alpha \\ln\\left(\\frac{\\beta}{\\beta-t}\\right) \\quad \\text{for } t < \\beta $$\nUsing the properties of the CGF, determine the expected value (mean) of the random variable $X$, denoted as $E[X]$. Express your answer in terms of $\\alpha$ and $\\beta$.", "solution": "We use the defining property of the cumulant generating function: for a random variable $X$ with CGF $K_{X}(t)=\\ln M_{X}(t)$, the mean is given by the first derivative of $K_{X}(t)$ at $t=0$, namely $E[X]=K_{X}'(0)$.\n\nGiven $K_{X}(t)=\\alpha \\ln\\!\\left(\\frac{\\beta}{\\beta-t}\\right)$ for $t<\\beta$ and $\\beta>0$, $t=0$ lies in the domain, so differentiation at $t=0$ is valid. Differentiate $K_{X}(t)$ with respect to $t$:\n$$\nK_{X}'(t)\n=\\alpha \\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\ln\\beta-\\ln(\\beta-t)\\right]\n=\\alpha\\left[0-\\frac{1}{\\beta-t}\\cdot(-1)\\right]\n=\\alpha\\,\\frac{1}{\\beta-t}.\n$$\nEvaluating at $t=0$ gives\n$$\nE[X]=K_{X}'(0)=\\alpha\\,\\frac{1}{\\beta}=\\frac{\\alpha}{\\beta}.\n$$", "answer": "$$\\boxed{\\frac{\\alpha}{\\beta}}$$", "id": "1354878"}, {"introduction": "The most elegant and powerful feature of CGFs is how they simplify the analysis of sums of independent random variables. This exercise guides you to discover this fundamental additivity property by exploring the CGF for the difference of two variables, $Z = X - Y$ [@problem_id:1354903]. Mastering this concept unlocks the reason why cumulants are such an essential tool in advanced probability theory, particularly in the study of limit theorems.", "problem": "Let $X$ and $Y$ be two independent random variables. The Moment Generating Function (MGF) of a random variable $W$ is defined as $M_W(t) = E[\\exp(tW)]$, provided the expectation exists for $t$ in some neighborhood of 0. The Cumulant Generating Function (CGF) is defined as the natural logarithm of the MGF, so $K_W(t) = \\ln(M_W(t))$.\n\nLet $K_X(t)$ be the CGF of $X$ and $K_Y(t)$ be the CGF of $Y$. A new random variable $Z$ is constructed as the difference $Z = X - Y$. Which of the following expressions correctly represents the CGF of $Z$, denoted as $K_Z(t)$, in terms of $K_X(t)$ and $K_Y(t)$?\n\nA. $K_X(t) + K_Y(t)$\n\nB. $K_X(t) - K_Y(t)$\n\nC. $K_X(t) + K_Y(-t)$\n\nD. $K_X(t) K_Y(-t)$\n\nE. $\\ln(K_X(t) - K_Y(t))$", "solution": "Our goal is to find the Cumulant Generating Function (CGF) of the random variable $Z = X - Y$, which is denoted by $K_Z(t)$.\n\nBy definition, the CGF of $Z$ is the natural logarithm of its Moment Generating Function (MGF), $M_Z(t)$.\n$$K_Z(t) = \\ln(M_Z(t))$$\n\nFirst, we need to find the MGF of $Z$. Using the definition of the MGF:\n$$M_Z(t) = E[\\exp(tZ)]$$\nSubstitute $Z = X - Y$:\n$$M_Z(t) = E[\\exp(t(X - Y))] = E[\\exp(tX - tY)]$$\nUsing the property of exponentials $\\exp(a-b) = \\exp(a)\\exp(-b)$, we can write:\n$$M_Z(t) = E[\\exp(tX)\\exp(-tY)]$$\nSince the random variables $X$ and $Y$ are independent, any functions of them, such as $\\exp(tX)$ and $\\exp(-tY)$, are also independent. For independent random variables, the expectation of their product is the product of their expectations:\n$$E[\\exp(tX)\\exp(-tY)] = E[\\exp(tX)] E[\\exp(-tY)]$$\nBy the definition of the MGF, $E[\\exp(tX)]$ is the MGF of $X$, which is $M_X(t)$. Similarly, $E[\\exp(-tY)]$ can be recognized as the MGF of $Y$ evaluated at $-t$, which is $M_Y(-t)$.\nTherefore, we have:\n$$M_Z(t) = M_X(t) M_Y(-t)$$\nNow we can substitute this expression for $M_Z(t)$ back into the formula for the CGF of $Z$:\n$$K_Z(t) = \\ln(M_X(t) M_Y(-t))$$\nUsing the property of logarithms that $\\ln(ab) = \\ln(a) + \\ln(b)$, we get:\n$$K_Z(t) = \\ln(M_X(t)) + \\ln(M_Y(-t))$$\nFrom the problem statement, we know the definition of the CGF is $K_W(t) = \\ln(M_W(t))$. Applying this, we can identify $\\ln(M_X(t))$ as $K_X(t)$. For the second term, $\\ln(M_Y(-t))$, we can use the definition of $K_Y(t)$ with the argument replaced by $-t$. That is, $K_Y(-t) = \\ln(M_Y(-t))$.\nSubstituting these back into our equation for $K_Z(t)$ gives the final relationship:\n$$K_Z(t) = K_X(t) + K_Y(-t)$$\nComparing this result with the given choices, we see that it matches option C.", "answer": "$$\\boxed{C}$$", "id": "1354903"}]}