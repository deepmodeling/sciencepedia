{"hands_on_practices": [{"introduction": "The cornerstone of working with sums of independent random variables is the multiplicative property of Moment Generating Functions (MGFs). This first practice provides a direct application of this fundamental rule, where the MGF of a sum $Z = X+Y$ is simply the product of the individual MGFs, $M_Z(t) = M_X(t)M_Y(t)$. By first identifying the MGFs for two independent error sources described in a manufacturing scenario, you will apply this property to find the MGF of the total combined error [@problem_id:1375470].", "problem": "A quality control department at a semiconductor company is analyzing two independent sources of error in their manufacturing and operations process.\n\nThe first source of error, represented by the random variable $X$, is the number of defective transistors found in a randomly selected chip, which contains a total of 5 distinct transistor blocks. Each block has a probability of being defective of $0.2$, and the defects occur independently from one block to another.\n\nThe second source of error, represented by the random variable $Y$, is the number of system warnings logged by the automated factory software in a one-hour period. Through long-term observation, the Moment Generating Function (MGF) for $Y$ has been empirically determined to be $M_Y(t) = \\exp(4(\\exp(t) - 1))$.\n\nAssuming that $X$ and $Y$ are independent random variables, determine the MGF of the total combined error score, defined as $Z = X + Y$.", "solution": "We are asked for the moment generating function (MGF) of $Z=X+Y$ where $X$ and $Y$ are independent. By the fundamental property of MGFs, if $X$ and $Y$ are independent random variables, then the MGF of their sum equals the product of their MGFs:\n$$\nM_{Z}(t)=M_{X}(t)M_{Y}(t).\n$$\n\nWe first determine $M_{X}(t)$. The random variable $X$ counts the number of defective transistors among $5$ independent blocks, each defective with probability $p=0.2$. Hence $X$ is binomial with parameters $n=5$ and $p=0.2$. The MGF of a binomial $\\operatorname{Bin}(n,p)$ is\n$$\nM_{X}(t)=\\left((1-p)+p\\exp(t)\\right)^{n}.\n$$\nSubstituting $n=5$ and $p=0.2$ gives\n$$\nM_{X}(t)=\\left(\\frac{4}{5}+\\frac{1}{5}\\exp(t)\\right)^{5}.\n$$\n\nWe are given the MGF of $Y$ directly as\n$$\nM_{Y}(t)=\\exp\\!\\left(4(\\exp(t)-1)\\right).\n$$\n\nUsing independence, we multiply the MGFs:\n$$\nM_{Z}(t)=M_{X}(t)M_{Y}(t)=\\left(\\frac{4}{5}+\\frac{1}{5}\\exp(t)\\right)^{5}\\exp\\!\\left(4(\\exp(t)-1)\\right).\n$$\nThis is the required MGF of $Z=X+Y$.", "answer": "$$\\boxed{\\left(\\frac{4}{5}+\\frac{1}{5}\\exp(t)\\right)^{5}\\exp\\!\\left(4(\\exp(t)-1)\\right)}$$", "id": "1375470"}, {"introduction": "Having learned to combine MGFs, we now explore the reverse process: decomposition. The uniqueness property of MGFs ensures that a given MGF corresponds to only one probability distribution, allowing us to 'unmix' a sum if we can factor its MGF. This powerful exercise [@problem_id:1375527] challenges you to inspect the MGF of a sum and, by recognizing the characteristic forms of standard distributions within its structure, identify the individual distributions of the underlying independent variables.", "problem": "Let $X$ and $Y$ be two independent, non-negative random variables. The Moment Generating Function (MGF) of their sum, $Z = X+Y$, is given by the expression\n$$M_Z(t) = \\left(\\frac{\\lambda}{\\lambda - t}\\right)^k \\exp\\left(\\mu(\\exp(t)-1)\\right)$$\nwhich is valid for $t < \\lambda$. In this expression, $\\lambda$ and $\\mu$ are positive real constants, and $k$ is a positive integer.\n\nBased on this information, calculate the value of the following expression:\n$$ \\frac{\\text{Var}(X) + \\text{E}[Y]}{\\text{E}[X] + \\text{Var}(Y)} $$\nExpress your answer as a single closed-form analytic expression in terms of the parameters $k$, $\\lambda$, and $\\mu$.", "solution": "The core principle we will use is the property of Moment Generating Functions (MGFs) for sums of independent random variables. If $X$ and $Y$ are independent, the MGF of their sum $Z = X+Y$ is the product of their individual MGFs:\n$$M_Z(t) = M_{X+Y}(t) = M_X(t) M_Y(t)$$\n\nThe given MGF for $Z$ is:\n$$M_Z(t) = \\left(\\frac{\\lambda}{\\lambda - t}\\right)^k \\exp\\left(\\mu(\\exp(t)-1)\\right)$$\n\nThis expression is a product of two distinct functional forms. By the uniqueness property of MGFs, we can identify the MGF of $X$ and $Y$ by decomposing this product. Let's assign one part of the product to $M_X(t)$ and the other to $M_Y(t)$.\n\nLet's identify the distribution corresponding to the first part of the expression:\n$$M_X(t) = \\left(\\frac{\\lambda}{\\lambda - t}\\right)^k$$\nThis is the MGF of a Gamma distribution. A random variable following a Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$, denoted as $\\text{Gamma}(\\alpha, \\beta)$, has an MGF of the form $M(t) = \\left(\\frac{\\beta}{\\beta - t}\\right)^\\alpha$. Comparing this with $M_X(t)$, we can identify that $X$ follows a Gamma distribution with shape parameter $\\alpha = k$ and rate parameter $\\beta = \\lambda$. Thus, $X \\sim \\text{Gamma}(k, \\lambda)$.\n\nNow, let's identify the distribution for the second part of the expression:\n$$M_Y(t) = \\exp\\left(\\mu(\\exp(t)-1)\\right)$$\nThis is the MGF of a Poisson distribution. A random variable following a Poisson distribution with parameter (mean) $\\mu'$, denoted as $\\text{Poisson}(\\mu')$, has an MGF of the form $M(t) = \\exp\\left(\\mu'(\\exp(t)-1)\\right)$. Comparing this with $M_Y(t)$, we identify that $Y$ follows a Poisson distribution with parameter $\\mu' = \\mu$. Thus, $Y \\sim \\text{Poisson}(\\mu)$.\n\nNext, we need to find the expected value (mean) and variance for each of these distributions.\n\nFor the Gamma distribution $X \\sim \\text{Gamma}(k, \\lambda)$:\nThe expected value is $\\text{E}[X] = \\frac{\\alpha}{\\beta} = \\frac{k}{\\lambda}$.\nThe variance is $\\text{Var}(X) = \\frac{\\alpha}{\\beta^2} = \\frac{k}{\\lambda^2}$.\n\nFor the Poisson distribution $Y \\sim \\text{Poisson}(\\mu)$:\nThe expected value is $\\text{E}[Y] = \\mu$.\nThe variance is $\\text{Var}(Y) = \\mu$.\n\nNow we can substitute these moments into the expression given in the problem statement:\n$$ \\frac{\\text{Var}(X) + \\text{E}[Y]}{\\text{E}[X] + \\text{Var}(Y)} = \\frac{\\frac{k}{\\lambda^2} + \\mu}{\\frac{k}{\\lambda} + \\mu} $$\n\nTo simplify this complex fraction, we find a common denominator for the numerator and the denominator of the main fraction.\nThe numerator becomes:\n$$ \\frac{k}{\\lambda^2} + \\mu = \\frac{k + \\mu\\lambda^2}{\\lambda^2} $$\nThe denominator becomes:\n$$ \\frac{k}{\\lambda} + \\mu = \\frac{k + \\mu\\lambda}{\\lambda} $$\nNow, we substitute these back into the expression:\n$$ \\frac{\\frac{k + \\mu\\lambda^2}{\\lambda^2}}{\\frac{k + \\mu\\lambda}{\\lambda}} $$\nTo divide the fractions, we multiply by the reciprocal of the denominator:\n$$ \\frac{k + \\mu\\lambda^2}{\\lambda^2} \\cdot \\frac{\\lambda}{k + \\mu\\lambda} $$\nWe can cancel one factor of $\\lambda$ from the numerator and denominator:\n$$ \\frac{k + \\mu\\lambda^2}{\\lambda(k + \\mu\\lambda)} $$\nThis is the final simplified expression.", "answer": "$$\\boxed{\\frac{k + \\mu\\lambda^{2}}{\\lambda(k + \\mu\\lambda)}}$$", "id": "1375527"}, {"introduction": "MGFs are more than just fingerprints for distributions; their structure contains deep information about a variable's moments, such as its mean and variance. This final practice moves into a more analytical application, asking you to compare the noise variances of two different system architectures [@problem_id:1375501]. You will see how the algebraic relationship between the MGFs of system components can be used to determine the exact relationship between their variances, a vital skill for statistical comparison and system design.", "problem": "In a simplified model for analyzing noise in electronic systems, the noise contribution of a fundamental component is described by a random variable. Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables representing the noise from these fundamental components. The common Moment Generating Function (MGF) of each $X_i$ is denoted by $M_X(t)$. We assume that the variance of $X_i$, $\\text{Var}(X)$, is finite and non-zero.\n\nTwo different circuit architectures are proposed to build a larger system.\n\nArchitecture A consists of combining $N=6$ fundamental components in series. The total noise in this architecture is the sum of the individual noise contributions, given by the random variable $Z_A = \\sum_{i=1}^{6} X_i$.\n\nArchitecture B uses a different design philosophy. It is built from $M=2$ 'compound modules'. These modules are also i.i.d. The noise contribution of a single compound module is represented by a random variable $Y$. Through empirical measurement and theoretical modeling, it has been established that the MGF of a compound module's noise, $M_Y(t)$, is related to the MGF of a fundamental component's noise by the equation $M_Y(t) = [M_X(t)]^2$. The total noise in Architecture B is the sum of the noise from its modules, given by $Z_B = \\sum_{j=1}^{2} Y_j$.\n\nTo compare the performance of the two architectures, a crucial metric is the ratio of their total noise variances. Calculate the value of the ratio $\\frac{\\text{Var}(Z_A)}{\\text{Var}(Z_B)}$.", "solution": "Let $X_{1},X_{2},\\dots$ be i.i.d. with MGF $M_{X}(t)$ and finite, non-zero variance $\\operatorname{Var}(X)$. For Architecture A, the total noise is $Z_{A}=\\sum_{i=1}^{6}X_{i}$. Using additivity of variance for independent sums, we have\n$$\n\\operatorname{Var}(Z_{A})=\\sum_{i=1}^{6}\\operatorname{Var}(X_{i})=6\\,\\operatorname{Var}(X).\n$$\n\nFor Architecture B, each compound module has MGF $M_{Y}(t)=[M_{X}(t)]^{2}$. Using MGF derivatives, recall\n$$\nM_{X}'(0)=\\mathbb{E}[X],\\quad M_{X}''(0)=\\mathbb{E}[X^{2}],\\quad \\operatorname{Var}(X)=M_{X}''(0)-\\big(M_{X}'(0)\\big)^{2}.\n$$\nDifferentiate $M_{Y}(t)$:\n$$\nM_{Y}'(t)=2\\,M_{X}(t)\\,M_{X}'(t),\\qquad M_{Y}''(t)=2\\big(M_{X}'(t)\\big)^{2}+2\\,M_{X}(t)\\,M_{X}''(t).\n$$\nEvaluate at $t=0$ using $M_{X}(0)=1$:\n$$\n\\mathbb{E}[Y]=M_{Y}'(0)=2\\,M_{X}'(0),\\qquad \\operatorname{Var}(Y)=M_{Y}''(0)-\\big(M_{Y}'(0)\\big)^{2}=2\\,M_{X}''(0)-2\\big(M_{X}'(0)\\big)^{2}=2\\,\\operatorname{Var}(X).\n$$\nSince $Z_{B}=Y_{1}+Y_{2}$ with $Y_{1},Y_{2}$ i.i.d. and independent, the variance adds:\n$$\n\\operatorname{Var}(Z_{B})=\\operatorname{Var}(Y_{1})+\\operatorname{Var}(Y_{2})=2\\,\\operatorname{Var}(Y)=2\\cdot 2\\,\\operatorname{Var}(X)=4\\,\\operatorname{Var}(X).\n$$\n\nTherefore, the ratio of total noise variances is\n$$\n\\frac{\\operatorname{Var}(Z_{A})}{\\operatorname{Var}(Z_{B})}=\\frac{6\\,\\operatorname{Var}(X)}{4\\,\\operatorname{Var}(X)}=\\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1375501"}]}