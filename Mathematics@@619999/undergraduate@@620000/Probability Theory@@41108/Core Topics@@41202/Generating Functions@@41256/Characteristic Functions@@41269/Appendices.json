{"hands_on_practices": [{"introduction": "The characteristic function serves as a unique fingerprint for a probability distribution. This exercise invites you to work backward from a given characteristic function to identify the underlying probability mass function. By deconstructing a simple trigonometric function using Euler's formula, you will practice matching its components to the definitional form of a characteristic function, building crucial intuition about how it encodes the distribution's properties [@problem_id:1348208].", "problem": "Let $X$ be a discrete random variable. The characteristic function of $X$, denoted by $\\phi_X(t)$, is defined as the expected value $E[\\exp(itX)]$, where $i$ is the imaginary unit and $t$ is a real number. For a discrete random variable, this can be written as $\\phi_X(t) = \\sum_{k} P(X=x_k) \\exp(itx_k)$, where $\\{x_k\\}$ are the possible values of $X$ and $P(X=x_k)$ are their corresponding probabilities.\n\nSuppose the characteristic function of a particular random variable $X$ is given by:\n$$ \\phi_X(t) = \\cos(2t) $$\nWhich of the following statements correctly describes the Probability Mass Function (PMF) of $X$?\n\nA. $X$ takes the values $1$ and $2$ with probabilities $P(X=1)=\\frac{1}{2}$ and $P(X=2)=\\frac{1}{2}$.\n\nB. $X$ takes the values $-2$ and $2$ with probabilities $P(X=-2)=\\frac{1}{2}$ and $P(X=2)=\\frac{1}{2}$.\n\nC. $X$ takes the values $-1$ and $1$ with probabilities $P(X=-1)=\\frac{1}{2}$ and $P(X=1)=\\frac{1}{2}$.\n\nD. $X$ takes a single value $0$ with probability $1$.\n\nE. $X$ is a continuous random variable uniformly distributed on the interval $[-2, 2]$.", "solution": "By definition, the characteristic function of a discrete random variable $X$ is $\\phi_{X}(t)=E[\\exp(i t X)]=\\sum_{k}P(X=x_{k})\\exp(i t x_{k})$.\n\nUsing Euler's identity, for any real $\\theta$,\n$$\n\\cos(\\theta)=\\frac{1}{2}\\left(\\exp(i\\theta)+\\exp(-i\\theta)\\right).\n$$\nGiven $\\phi_{X}(t)=\\cos(2t)$, we rewrite it as\n$$\n\\phi_{X}(t)=\\frac{1}{2}\\left(\\exp(i\\cdot 2 t)+\\exp(-i\\cdot 2 t)\\right)=\\frac{1}{2}\\exp(i t\\cdot 2)+\\frac{1}{2}\\exp(i t\\cdot(-2)).\n$$\nThis matches the form $\\sum_{k}P(X=x_{k})\\exp(i t x_{k})$ with $P(X=2)=\\frac{1}{2}$ and $P(X=-2)=\\frac{1}{2}$. Therefore, $X$ takes values $-2$ and $2$ with equal probabilities.\n\nTo confirm the uniqueness among the options:\n- Option A would yield $\\phi_{X}(t)=\\frac{1}{2}\\exp(i t)+\\frac{1}{2}\\exp(i 2 t)$, which is not equal to $\\cos(2t)$ for all $t$.\n- Option C corresponds to $\\phi_{X}(t)=\\cos(t)$, not $\\cos(2t)$.\n- Option D corresponds to $\\phi_{X}(t)=1$, not $\\cos(2t)$.\n- Option E, for a continuous uniform on $[-2,2]$, has $\\phi_{X}(t)=\\frac{\\sin(2 t)}{2 t}$, not $\\cos(2t)$.\n\nHence the correct description is that $X$ takes values $-2$ and $2$ with probabilities $\\frac{1}{2}$ each.", "answer": "$$\\boxed{B}$$", "id": "1348208"}, {"introduction": "One of the most powerful features of characteristic functions is their ability to generate moments through differentiation. This practice demonstrates how to compute the mean and variance of a random variable by taking derivatives of its characteristic function at the origin. This elegant technique often bypasses more complex calculations involving sums or integrals, showcasing the practical utility of characteristic functions as a problem-solving tool [@problem_id:1903213].", "problem": "A random variable $X$ has a characteristic function given by $\\phi_X(t) = \\exp(\\lambda(e^{it}-1))$, where $\\lambda$ is a positive real constant and $i$ is the imaginary unit. A new random variable $Y$ is constructed from $X$ through the linear transformation $Y = c_1 X + c_2$, where $c_1$ and $c_2$ are real constants with $c_1 \\neq 0$.\n\nDetermine an expression for the variance of the random variable $Y$. Your final answer should be in terms of the constants $\\lambda$, $c_1$, and $c_2$, if applicable.", "solution": "We are given the characteristic function of $X$ as $\\phi_{X}(t) = \\exp\\!\\big(\\lambda(\\exp(i t) - 1)\\big)$ with $\\lambda > 0$. From properties of characteristic functions, the moments can be obtained by derivatives at $t=0$:\n- $\\phi_{X}'(t) = i \\,\\mathbb{E}[X \\exp(i t X)]$, hence $\\phi_{X}'(0) = i\\,\\mathbb{E}[X]$ and $\\mathbb{E}[X] = \\frac{\\phi_{X}'(0)}{i}$.\n- $\\phi_{X}''(t) = (i)^{2}\\mathbb{E}[X^{2} \\exp(i t X)] = -\\mathbb{E}[X^{2} \\exp(i t X)]$, hence $\\phi_{X}''(0) = -\\mathbb{E}[X^{2}]$ and $\\mathbb{E}[X^{2}] = -\\phi_{X}''(0)$.\nTherefore $\\operatorname{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2} = -\\phi_{X}''(0) - \\left(\\frac{\\phi_{X}'(0)}{i}\\right)^{2}$.\n\nCompute derivatives of $\\phi_{X}(t)$. Let $g(t) = \\lambda(\\exp(i t) - 1)$, so $\\phi_{X}(t) = \\exp(g(t))$. Then $g'(t) = \\lambda i \\exp(i t)$ and $g''(t) = \\lambda (i)^{2} \\exp(i t) = -\\lambda \\exp(i t)$. Using the chain rule,\n$$\n\\phi_{X}'(t) = \\phi_{X}(t)\\, g'(t), \\quad\n\\phi_{X}''(t) = \\phi_{X}(t)\\, g''(t) + \\phi_{X}(t)\\, (g'(t))^{2}.\n$$\nEvaluating at $t=0$ and using $\\phi_{X}(0) = 1$, $\\exp(i\\cdot 0) = 1$ gives\n$$\n\\phi_{X}'(0) = 1 \\cdot \\lambda i \\cdot 1 = \\lambda i, \\quad\n\\phi_{X}''(0) = 1 \\cdot (-\\lambda \\cdot 1) + 1 \\cdot (\\lambda i \\cdot 1)^{2} = -\\lambda - \\lambda^{2}.\n$$\nHence\n$$\n\\mathbb{E}[X] = \\frac{\\phi_{X}'(0)}{i} = \\lambda, \\quad\n\\mathbb{E}[X^{2}] = -\\phi_{X}''(0) = \\lambda + \\lambda^{2},\n$$\nso\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2} = (\\lambda + \\lambda^{2}) - \\lambda^{2} = \\lambda.\n$$\n\nFor the linear transformation $Y = c_{1} X + c_{2}$ with $c_{1} \\neq 0$, the variance transforms as\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(c_{1} X + c_{2}) = \\mathbb{E}\\big[(c_{1} X + c_{2} - \\mathbb{E}[c_{1} X + c_{2}])^{2}\\big] = \\mathbb{E}\\big[(c_{1}(X - \\mathbb{E}[X]))^{2}\\big] = c_{1}^{2}\\operatorname{Var}(X).\n$$\nSubstituting $\\operatorname{Var}(X) = \\lambda$ yields\n$$\n\\operatorname{Var}(Y) = c_{1}^{2}\\lambda.\n$$\nThe constant $c_{2}$ does not affect the variance.", "answer": "$$\\boxed{c_{1}^{2}\\lambda}$$", "id": "1903213"}, {"introduction": "While simple discrete distributions can sometimes be identified by inspection, a more general and powerful method is needed to recover a probability density function (PDF) from its characteristic function. This exercise introduces the Fourier inversion theorem, a cornerstone of the theory that provides a direct formula for this task. You will apply this theorem to a continuous random variable, gaining hands-on experience with a fundamental technique that highlights the deep connection between probability and Fourier analysis [@problem_id:1348197].", "problem": "A continuous random variable $X$ has a characteristic function $\\phi_X(t)$ defined by the triangular function:\n$$\n\\phi_X(t) = \\begin{cases} 1 - |t| & \\text{if } |t| \\le 1 \\\\ 0 & \\text{if } |t| > 1 \\end{cases}\n$$\nThe characteristic function is related to the probability density function (PDF), $f_X(x)$, by the relation $\\phi_X(t) = \\int_{-\\infty}^{\\infty} e^{itx} f_X(x) \\, dx$.\n\nDetermine the probability density function, $f_X(x)$, for this random variable. Present your answer as a single closed-form analytic expression for $f_X(x)$.", "solution": "We use the Fourier inversion formula corresponding to the characteristic function convention $\\phi_{X}(t)=\\int_{-\\infty}^{\\infty}\\exp(i t x)f_{X}(x)\\,dx$, namely\n$$\nf_{X}(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}\\exp(-i t x)\\,\\phi_{X}(t)\\,dt.\n$$\nSubstituting the given $\\phi_{X}(t)$ yields\n$$\nf_{X}(x)=\\frac{1}{2\\pi}\\int_{-1}^{1}\\exp(-i t x)\\,(1-|t|)\\,dt.\n$$\nSince $(1-|t|)$ is even and $\\exp(-i t x)=\\cos(tx)-i\\sin(tx)$, the imaginary part integrates to zero and we obtain\n$$\nf_{X}(x)=\\frac{1}{\\pi}\\int_{0}^{1}(1-t)\\cos(t x)\\,dt.\n$$\nCompute the integral by splitting it:\n$$\n\\int_{0}^{1}(1-t)\\cos(t x)\\,dt=\\int_{0}^{1}\\cos(t x)\\,dt-\\int_{0}^{1}t\\cos(t x)\\,dt.\n$$\nFor the first term, with $u=tx$,\n$$\n\\int_{0}^{1}\\cos(t x)\\,dt=\\frac{\\sin x}{x}.\n$$\nFor the second term, integrate by parts with $u=t$, $dv=\\cos(tx)\\,dt$, so $du=dt$, $v=\\frac{\\sin(tx)}{x}$, giving\n$$\n\\int_{0}^{1}t\\cos(t x)\\,dt=\\left.\\frac{t\\sin(tx)}{x}\\right|_{0}^{1}-\\int_{0}^{1}\\frac{\\sin(tx)}{x}\\,dt=\\frac{\\sin x}{x}+\\frac{\\cos x-1}{x^{2}}.\n$$\nTherefore,\n$$\n\\int_{0}^{1}(1-t)\\cos(t x)\\,dt=\\frac{\\sin x}{x}-\\left(\\frac{\\sin x}{x}+\\frac{\\cos x-1}{x^{2}}\\right)=\\frac{1-\\cos x}{x^{2}}.\n$$\nHence,\n$$\nf_{X}(x)=\\frac{1}{\\pi}\\cdot\\frac{1-\\cos x}{x^{2}}.\n$$\nUsing $1-\\cos x=2\\sin^{2}(x/2)$, we can write this as\n$$\nf_{X}(x)=\\frac{2}{\\pi}\\cdot\\frac{\\sin^{2}(x/2)}{x^{2}}=\\frac{1}{2\\pi}\\left(\\frac{\\sin(x/2)}{x/2}\\right)^{2}.\n$$\nThis expression has a removable singularity at $x=0$ with the continuous value $f_{X}(0)=\\frac{1}{2\\pi}$, and it integrates to $1$ over $\\mathbb{R}$, so it is a valid PDF.", "answer": "$$\\boxed{\\frac{1}{2\\pi}\\left(\\frac{\\sin(x/2)}{x/2}\\right)^{2}}$$", "id": "1348197"}]}