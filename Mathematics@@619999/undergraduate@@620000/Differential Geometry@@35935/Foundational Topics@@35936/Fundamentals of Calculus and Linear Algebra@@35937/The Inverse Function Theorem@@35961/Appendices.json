{"hands_on_practices": [{"introduction": "This first practice is a fundamental exercise in applying the Inverse Function Theorem in its most direct form. By calculating the derivative of an inverse function for a single-variable case [@problem_id:30441], you will master a skill that forms the bedrock for more complex applications. This problem reinforces the core formula from the theorem and the essential procedure of relating a point on the inverse function back to a corresponding point on the original function.", "problem": "Consider a real-valued function $f: \\mathbb{R} \\to \\mathbb{R}$ defined by the polynomial:\n$$\nf(x) = x^5 + 2x^3 + x\n$$\nThe derivative of this function is $f'(x) = 5x^4 + 6x^2 + 1$. Since $f'(x) > 0$ for all $x \\in \\mathbb{R}$, the function $f$ is strictly increasing and therefore possesses a differentiable inverse function, which we denote by $f^{-1}$.\n\nThe derivative of the inverse function at a point $y_0$ in the range of $f$ can be found using the formula from the Inverse Function Theorem:\n$$\n(f^{-1})'(y_0) = \\frac{1}{f'(x_0)}\n$$\nwhere $x_0$ is the unique value such that $f(x_0) = y_0$.\n\nUsing this information, calculate the exact value of the derivative of the inverse function $f^{-1}$ at the point $y_0 = 4$.", "solution": "We have \n$$f(x)=x^5+2x^3+x,$$\nso\n$$f'(x)=5x^4+6x^2+1.$$\nSince for all real $x$, $5x^4\\ge0$, $6x^2\\ge0$ and $1>0$, it follows that $f'(x)>0$.  Hence $f$ is strictly increasing and admits a differentiable inverse $f^{-1}$.  By the Inverse Function Theorem, for $y_0\\in \\mathrm{Range}(f)$ and the unique $x_0$ with $f(x_0)=y_0$,\n$$(f^{-1})'(y_0)=\\frac1{f'(x_0)}.$$\n\nWe wish to compute $(f^{-1})'(4)$.  First find $x_0$ such that\n$$f(x_0)=x_0^5+2x_0^3+x_0=4.$$\nBy inspection $x_0=1$ gives $1^5+2\\cdot1^3+1=1+2+1=4$.  Thus $x_0=1$.\n\nNext, evaluate $f'$ at $x_0=1$:\n$$f'(1)=5\\cdot1^4+6\\cdot1^2+1=5+6+1=12.$$\n\nTherefore,\n$$(f^{-1})'(4)=\\frac1{f'(1)}=\\frac1{12}.$$", "answer": "$$\\boxed{\\frac{1}{12}}$$", "id": "30441"}, {"introduction": "The Inverse Function Theorem brilliantly tells us when a function's inverse is differentiable, but what happens when its conditions are not met? This thought-provoking problem [@problem_id:2325071] probes the boundary of the theorem's domain of application. By analyzing a specific function where the derivative becomes zero at a critical point, you will gain a much deeper appreciation for why the condition $f'(x) \\neq 0$ is so crucial for guaranteeing a differentiable inverse.", "problem": "Let the function $f: \\mathbb{R} \\to \\mathbb{R}$ be defined by $f(x) = x|x|$. It is known that this function is invertible. Let $g(y)$ denote the inverse function $f^{-1}(y)$. Which of the following statements correctly identifies a point of non-differentiability for the inverse function $g(y)$ and provides the correct reason related to the properties of the original function $f(x)$?\n\nA. The inverse function $g(y)$ is not differentiable at $y=1$ because $f(x)$ is not monotonic on the entire real line.\n\nB. The inverse function $g(y)$ is not differentiable at $y=0$ because the original function $f(x)$ is not differentiable at the corresponding point $x=0$.\n\nC. The inverse function $g(y)$ is not differentiable at $y=0$ because the derivative of the original function $f'(x)$ is zero at the corresponding point $x=0$, which violates a key condition of the Inverse Function Theorem.\n\nD. The inverse function $g(y)$ is differentiable for all real numbers $y$.\n\nE. The inverse function $g(y)$ is not differentiable at $y=0$ because the original function $f(x)$ is not continuous at the corresponding point $x=0$.", "solution": "To determine the correct option, we need to perform a three-part analysis: first, analyze the original function $f(x)$; second, find its inverse $g(y)$; and third, analyze the differentiability of the inverse $g(y)$ and relate it to the properties of $f(x)$.\n\n**Step 1: Analyze the function $f(x) = x|x|$.**\nThe function can be written in a piecewise form:\n$$\nf(x) = \\begin{cases} x \\cdot x = x^2, & \\text{if } x \\ge 0 \\\\ x \\cdot (-x) = -x^2, & \\text{if } x < 0 \\end{cases}\n$$\nLet's check the differentiability of $f(x)$, particularly at $x=0$.\nFor $x > 0$, the derivative is $f'(x) = \\frac{d}{dx}(x^2) = 2x$.\nFor $x < 0$, the derivative is $f'(x) = \\frac{d}{dx}(-x^2) = -2x$.\nTo find the derivative at $x=0$, we check the limit of the difference quotient:\nThe right-hand derivative at $x=0$ is:\n$$ \\lim_{h \\to 0^+} \\frac{f(0+h) - f(0)}{h} = \\lim_{h \\to 0^+} \\frac{h^2 - 0}{h} = \\lim_{h \\to 0^+} h = 0 $$\nThe left-hand derivative at $x=0$ is:\n$$ \\lim_{h \\to 0^-} \\frac{f(0+h) - f(0)}{h} = \\lim_{h \\to 0^-} \\frac{-h^2 - 0}{h} = \\lim_{h \\to 0^-} -h = 0 $$\nSince the left-hand and right-hand derivatives both exist and are equal to 0, the function $f(x)$ is differentiable at $x=0$, and $f'(0) = 0$.\nThe derivative of $f(x)$ can be written compactly as $f'(x) = 2|x|$. The function $f(x)$ is continuous and differentiable for all $x \\in \\mathbb{R}$.\n\n**Step 2: Find the inverse function $g(y) = f^{-1}(y)$.**\nWe set $y = f(x)$ and solve for $x$. We must consider the same two cases.\nCase 1: $x \\ge 0$. In this case, $y = x^2$. Since $x \\ge 0$, we also have $y \\ge 0$. Solving for $x$ gives $x = \\sqrt{y}$.\nCase 2: $x < 0$. In this case, $y = -x^2$. Since $x < 0$, we have $x^2 > 0$, so $y < 0$. Solving for $x$ gives $x^2 = -y$, and since $x$ must be negative, we take the negative root: $x = -\\sqrt{-y}$.\nCombining these results, the inverse function $g(y)$ is:\n$$\ng(y) = \\begin{cases} \\sqrt{y}, & \\text{if } y \\ge 0 \\\\ -\\sqrt{-y}, & \\text{if } y < 0 \\end{cases}\n$$\n\n**Step 3: Analyze the differentiability of the inverse $g(y)$.**\nWe find the derivative of $g(y)$ for $y \\neq 0$.\nFor $y > 0$, $g(y) = y^{1/2}$, so $g'(y) = \\frac{1}{2}y^{-1/2} = \\frac{1}{2\\sqrt{y}}$.\nFor $y < 0$, $g(y) = -(-y)^{1/2}$, so using the chain rule, $g'(y) = -\\frac{1}{2}(-y)^{-1/2} \\cdot (-1) = \\frac{1}{2\\sqrt{-y}}$.\nNow we investigate the differentiability at $y=0$. We look at the limit of the difference quotient for $g(y)$ at $y=0$.\nThe right-hand derivative at $y=0$ is:\n$$ \\lim_{h \\to 0^+} \\frac{g(0+h) - g(0)}{h} = \\lim_{h \\to 0^+} \\frac{\\sqrt{h} - 0}{h} = \\lim_{h \\to 0^+} \\frac{1}{\\sqrt{h}} = +\\infty $$\nThe derivative from the right does not exist as a finite number. This is sufficient to conclude that $g(y)$ is not differentiable at $y=0$. The graph of $g(y)$ has a vertical tangent at $y=0$.\n\n**Step 4: Evaluate the options.**\n- Option D is incorrect because we have shown that $g(y)$ is not differentiable at $y=0$.\n- Option A is incorrect because the point of non-differentiability is $y=0$, not $y=1$. Also, the function $f(x)$ is strictly increasing, hence monotonic.\n- We are left with options B, C, and E, which all correctly identify the point of non-differentiability as $y=0$. We must check the reason.\n- The corresponding point on the original function is $x_0$ such that $f(x_0)=y_0=0$. This gives $x_0|x_0|=0$, so $x_0=0$.\n- Option E states the reason is that $f(x)$ is not continuous at $x=0$. This is false; $f(x)=x|x|$ is continuous everywhere.\n- Option B states the reason is that $f(x)$ is not differentiable at $x=0$. This is false; our analysis in Step 1 showed that $f(x)$ is differentiable at $x=0$ and $f'(0)=0$.\n- Option C states the reason is that $f'(x)=0$ at the corresponding point $x=0$, which violates a key condition of the Inverse Function Theorem. The Inverse Function Theorem states that if $f$ is continuously differentiable in a neighborhood of $x_0$ and $f'(x_0) \\neq 0$, then its inverse $f^{-1}$ is differentiable at $y_0 = f(x_0)$. In our case, $f$ is continuously differentiable, but at $x_0=0$, we have $f'(0)=0$. This violates the condition $f'(x_0) \\neq 0$. The theorem's conclusion of differentiability is therefore not guaranteed, and as we have found, the inverse is indeed not differentiable at the corresponding point $y_0=f(0)=0$. This statement is correct.\n\nThus, the correct option is C.", "answer": "$$\\boxed{C}$$", "id": "2325071"}, {"introduction": "Moving from a single dimension to a plane, the concept of a derivative generalizes to a matrix of partial derivatives called the Jacobian. This practice [@problem_id:1677204] translates the core idea of the Inverse Function Theorem into a multi-dimensional, applied context. You will calculate a specific component of the inverse Jacobian matrix, a quantity interpreted as \"sensitivity\" in a hypothetical sensor system, demonstrating the theorem's immense power in fields like engineering and physics.", "problem": "A sensor in a 2D plane with Cartesian coordinates $(x, y)$ measures two signals, $u$ and $v$. These signals are related to the sensor's position by the mapping $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ given by:\n$$ u(x, y) = (x+a)^2 + y^2 $$\n$$ v(x, y) = (x-a)^2 + y^2 $$\nwhere $a$ is a known positive constant representing a characteristic length of the setup.\n\nAn operator needs to determine the sensor's position $(x, y)$ from the measured signals $(u, v)$. This requires understanding the local behavior of the inverse map $F^{-1}(u, v) = (x(u,v), y(u,v))$. A key quantity for this analysis is the sensitivity of the inferred $x$-position to a small change in the signal $u$. This sensitivity is given by the partial derivative $\\frac{\\partial x}{\\partial u}$.\n\nCalculate the value of this partial derivative, $\\frac{\\partial x}{\\partial u}$, for the local inverse map, evaluated at the specific sensor position $(x_0, y_0) = (0, b)$, where $b$ is a known positive constant. Express your answer as an analytic expression in terms of $a$ and $b$.", "solution": "We are given the forward map $F:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ defined by\n$$\nu(x,y)=(x+a)^{2}+y^{2},\\qquad v(x,y)=(x-a)^{2}+y^{2},\n$$\nwith $a>0$. The local inverse map $F^{-1}(u,v)=(x(u,v),y(u,v))$ exists where the Jacobian determinant of $F$ is nonzero, and its Jacobian is the matrix inverse of the Jacobian of $F$ by the inverse function theorem:\n$$\nJ_{F^{-1}}(u,v)=\\left(J_{F}(x,y)\\right)^{-1},\\quad \\text{with}\\quad J_{F}=\\begin{pmatrix}\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\\end{pmatrix}.\n$$\nCompute the partial derivatives:\n$$\n\\frac{\\partial u}{\\partial x}=2(x+a),\\quad \\frac{\\partial u}{\\partial y}=2y,\\quad \\frac{\\partial v}{\\partial x}=2(x-a),\\quad \\frac{\\partial v}{\\partial y}=2y.\n$$\nThus,\n$$\nJ_{F}(x,y)=\\begin{pmatrix}2(x+a) & 2y \\\\ 2(x-a) & 2y\\end{pmatrix}.\n$$\nThe determinant is\n$$\n\\det J_{F}= \\left(2(x+a)\\right)\\left(2y\\right)-\\left(2y\\right)\\left(2(x-a)\\right)=4y\\left[(x+a)-(x-a)\\right]=8ay.\n$$\nAt $(x_{0},y_{0})=(0,b)$ with $b>0$, we have $\\det J_{F}=8ab\\neq 0$, so the inverse exists locally. The inverse of a $2\\times 2$ matrix $\\begin{pmatrix}A&B\\\\ C&D\\end{pmatrix}$ is $\\frac{1}{AD-BC}\\begin{pmatrix}D&-B\\\\ -C&A\\end{pmatrix}$. Applying this to $J_{F}$ gives\n$$\n\\left(J_{F}\\right)^{-1}=\\frac{1}{8ay}\\begin{pmatrix}2y & -2y \\\\ -2(x-a) & 2(x+a)\\end{pmatrix}.\n$$\nBy definition,\n$$\nJ_{F^{-1}}=\\begin{pmatrix}\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\\end{pmatrix}=\\left(J_{F}\\right)^{-1}.\n$$\nTherefore, the desired sensitivity is the $(1,1)$ entry:\n$$\n\\frac{\\partial x}{\\partial u}=\\frac{2y}{8ay}=\\frac{1}{4a}.\n$$\nEvaluating at $(x_{0},y_{0})=(0,b)$ leaves the same value since $y=b>0$ cancels:\n$$\n\\left.\\frac{\\partial x}{\\partial u}\\right|_{(x_{0},y_{0})=(0,b)}=\\frac{1}{4a}.\n$$", "answer": "$$\\boxed{\\frac{1}{4a}}$$", "id": "1677204"}]}