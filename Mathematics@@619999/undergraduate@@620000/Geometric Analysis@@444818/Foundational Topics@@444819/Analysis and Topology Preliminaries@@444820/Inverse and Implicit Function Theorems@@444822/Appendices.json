{"hands_on_practices": [{"introduction": "The Inverse Function Theorem provides a powerful criterion for determining if a function is locally invertible by examining its derivative at a point. We begin our practice with a familiar example: the transformation from polar to Cartesian coordinates. This exercise [@problem_id:3053837] asks you to apply the theorem to demonstrate rigorously why this map can be inverted locally, so long as we stay away from the origin, a point where the mapping's invertibility breaks down.", "problem": "Let $U=(0,\\infty)\\times\\mathbb{R}$ and define $F:U\\to\\mathbb{R}^{2}$ by $F(r,\\theta)=(r\\cos\\theta,r\\sin\\theta)$, where $\\theta$ is measured in radians. Fix a point $(r_{0},\\theta_{0})\\in U$ with $r_{0}0$. Using the definition of the derivative as a best linear approximation and the Inverse Function Theorem, do the following:\n- Compute the Jacobian matrix $DF(r,\\theta)$ by taking partial derivatives with respect to $r$ and $\\theta$.\n- Compute $\\det DF(r,\\theta)$ and evaluate it at $(r_{0},\\theta_{0})$.\n- Based on your computation, justify rigorously that $F$ is locally invertible at $(r_{0},\\theta_{0})$.\n\nYour argument should be grounded in core definitions of differentiability of maps between Euclidean spaces, the standard derivatives of trigonometric functions (with angles in radians), and the statement of the Inverse Function Theorem. As your final answer, provide the value of $\\det DF(r_{0},\\theta_{0})$ as a single simplified analytic expression. No numerical rounding is required and no units are involved.", "solution": "The problem requires us to analyze the local invertibility of the function $F:U\\to\\mathbb{R}^{2}$ defined by $F(r,\\theta)=(r\\cos\\theta, r\\sin\\theta)$ on the domain $U=(0,\\infty)\\times\\mathbb{R}$, at a specific point $(r_{0},\\theta_{0})\\in U$ where $r_{0}0$. The analysis will be based on the Inverse Function Theorem.\n\nFirst, we compute the Jacobian matrix of $F$, denoted $DF(r,\\theta)$. The function $F$ can be written in terms of its component functions, $F(r,\\theta) = (x(r,\\theta), y(r,\\theta))$, where $x(r,\\theta) = r\\cos\\theta$ and $y(r,\\theta) = r\\sin\\theta$. The Jacobian matrix is the matrix of all first-order partial derivatives:\n$$DF(r,\\theta) = \\begin{pmatrix} \\frac{\\partial x}{\\partial r}  \\frac{\\partial x}{\\partial \\theta} \\\\ \\frac{\\partial y}{\\partial r}  \\frac{\\partial y}{\\partial \\theta} \\end{pmatrix}$$\nWe compute each partial derivative. Since $\\theta$ is in radians, the standard rules for differentiating trigonometric functions apply.\n1. The partial derivative of $x$ with respect to $r$:\n   $$\\frac{\\partial x}{\\partial r} = \\frac{\\partial}{\\partial r}(r\\cos\\theta) = \\cos\\theta$$\n2. The partial derivative of $x$ with respect to $\\theta$:\n   $$\\frac{\\partial x}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(r\\cos\\theta) = -r\\sin\\theta$$\n3. The partial derivative of $y$ with respect to $r$:\n   $$\\frac{\\partial y}{\\partial r} = \\frac{\\partial}{\\partial r}(r\\sin\\theta) = \\sin\\theta$$\n4. The partial derivative of $y$ with respect to $\\theta$:\n   $$\\frac{\\partial y}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(r\\sin\\theta) = r\\cos\\theta$$\n\nAssembling these into the matrix, we get the Jacobian matrix of $F$:\n$$DF(r,\\theta) = \\begin{pmatrix} \\cos\\theta  -r\\sin\\theta \\\\ \\sin\\theta  r\\cos\\theta \\end{pmatrix}$$\nThis matrix represents the best linear approximation of the change in $F$ near a point $(r, \\theta)$.\n\nNext, we compute the determinant of the Jacobian matrix, $\\det DF(r,\\theta)$. For a $2\\times2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the determinant is $ad-bc$.\n$$\\det DF(r,\\theta) = (\\cos\\theta)(r\\cos\\theta) - (-r\\sin\\theta)(\\sin\\theta)$$\n$$\\det DF(r,\\theta) = r\\cos^{2}\\theta + r\\sin^{2}\\theta$$\nFactoring out $r$, we have:\n$$\\det DF(r,\\theta) = r(\\cos^{2}\\theta + \\sin^{2}\\theta)$$\nUsing the fundamental trigonometric identity $\\cos^{2}\\theta + \\sin^{2}\\theta = 1$, the expression simplifies to:\n$$\\det DF(r,\\theta) = r$$\n\nWe are asked to evaluate this determinant at the point $(r_{0},\\theta_{0})$. Substituting $r=r_{0}$ and $\\theta=\\theta_{0}$, we get:\n$$\\det DF(r_{0},\\theta_{0}) = r_{0}$$\n\nFinally, we must rigorously justify that $F$ is locally invertible at $(r_{0},\\theta_{0})$ using the Inverse Function Theorem.\nThe Inverse Function Theorem states: Let $F: U \\to \\mathbb{R}^{n}$ be a continuously differentiable ($C^{1}$) map on an open set $U \\subseteq \\mathbb{R}^{n}$. If the derivative of $F$ at a point $a \\in U$, which is the linear map represented by the Jacobian matrix $DF(a)$, is invertible, then $F$ is locally invertible at $a$. That is, there exist open neighborhoods $V$ of $a$ and $W$ of $F(a)$ such that the restriction $F|_{V}: V \\to W$ is a bijection and its inverse $F^{-1}: W \\to V$ is also continuously differentiable. A sufficient condition for the invertibility of the linear map $DF(a)$ is that its determinant is non-zero, i.e., $\\det DF(a) \\neq 0$.\n\nWe verify the conditions of the theorem for our function $F$ at the point $(r_{0},\\theta_{0})$:\n1.  **Domain:** The function $F$ is defined on $U = (0,\\infty)\\times\\mathbb{R}$, which is an open subset of $\\mathbb{R}^{2}$. The point $(r_{0},\\theta_{0})$ is in $U$.\n2.  **Differentiability:** The function $F$ must be continuously differentiable ($C^{1}$) on $U$. The component functions $x(r,\\theta) = r\\cos\\theta$ and $y(r,\\theta) = r\\sin\\theta$ are products of functions that are infinitely differentiable on their domains. Their partial derivatives, $\\cos\\theta$, $-r\\sin\\theta$, $\\sin\\theta$, and $r\\cos\\theta$, are all continuous functions for all $(r,\\theta) \\in U$. Therefore, $F$ is a $C^{1}$ function on its entire domain $U$.\n3.  **Invertibility of the Jacobian:** We must check if the Jacobian matrix $DF(r_{0},\\theta_{0})$ is invertible. As computed above, the determinant of the Jacobian at this point is $\\det DF(r_{0},\\theta_{0}) = r_{0}$. The problem states that $(r_{0},\\theta_{0}) \\in U$, which implies $r_{0} \\in (0,\\infty)$, so $r_{0}  0$. Since $r_{0}  0$, we have $\\det DF(r_{0},\\theta_{0}) = r_{0} \\neq 0$.\n\nSince all conditions of the Inverse Function Theorem are met at the point $(r_{0},\\theta_{0})$, we can conclude that the function $F$ is locally invertible at $(r_{0},\\theta_{0})$. This means there exists an open neighborhood of $(r_{0},\\theta_{0})$ in $U$ on which $F$ has a continuously differentiable inverse function. This inverse function would map Cartesian coordinates $(x,y)$ back to polar coordinates $(r,\\theta)$ in a local region.\n\nThe value of the determinant of the Jacobian at the specified point, which is the final requested answer, is $r_{0}$.", "answer": "$$\n\\boxed{r_{0}}\n$$", "id": "3053837"}, {"introduction": "While the Inverse Function Theorem deals with inverting functions, the closely related Implicit Function Theorem allows us to untangle relations to define functions. Many geometric shapes, like circles and ellipses, are most naturally described by an equation of the form $F(x,y)=c$. This practice problem [@problem_id:3053852] guides you through using the Implicit Function Theorem to confirm that an ellipse can be locally described as the graph of a function $y=f(x)$, and then to explore the properties of this implicitly defined function through differentiation.", "problem": "Let $F:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $F(x,y)=x^{2}+2xy+3y^{2}-1$. Consider the ellipse given as the zero set $\\{(x,y)\\in\\mathbb{R}^{2}:F(x,y)=0\\}$. Using only the foundational definitions of differentiability, the Jacobian, and the Implicit Function Theorem (IFT), proceed as follows:\n\n- Identify a point $(x_{0},y_{0})$ on the ellipse at which $\\partial_{y}F(x_{0},y_{0})\\neq 0$, and justify that the hypotheses of the Implicit Function Theorem are satisfied at that point.\n- Conclude that there exists an open interval $U\\subset\\mathbb{R}$ containing $x_{0}$ and a $C^{1}$ function $f:U\\to\\mathbb{R}$ such that $F(x,f(x))=0$ for all $x\\in U$, thereby giving a local $C^{1}$ parametrization $x\\mapsto (x,f(x))$ of the ellipse near $(x_{0},y_{0})$.\n- By differentiating the identity $F(x,f(x))=0$ as many times as needed and evaluating at $x=x_{0}$, determine the value of the second derivative $f''(x_{0})$.\n\nReport the value of $f''(x_{0})$ as your final answer. No rounding is required, and no units are involved. The answer must be a single real number.", "solution": "We are asked to analyze the ellipse defined by the zero set of the function $F:\\mathbb{R}^{2}\\to\\mathbb{R}$, where $F(x,y)=x^{2}+2xy+3y^{2}-1$.\n\nFirst, we must identify a point $(x_{0},y_{0})$ on the ellipse at which the Implicit Function Theorem (IFT) can be applied to express $y$ locally as a function of $x$. The required condition is $\\partial_{y}F(x_{0},y_{0})\\neq 0$. Let's choose a simple point to test. If we set $y_{0}=0$, the equation of the ellipse $F(x,y)=0$ becomes $x_{0}^{2}+2x_{0}(0)+3(0)^{2}-1=0$, which simplifies to $x_{0}^{2}=1$. We can choose $x_{0}=1$. So, we select the point $(x_{0},y_{0})=(1,0)$.\nWe verify that this point lies on the ellipse:\n$$F(1,0) = 1^{2}+2(1)(0)+3(0)^{2}-1 = 1-1=0$$\nThe point $(1,0)$ is indeed on the ellipse.\n\nNext, we verify the hypotheses of the Implicit Function Theorem at $(x_{0},y_{0})=(1,0)$:\n1. The function $F(x,y)=x^{2}+2xy+3y^{2}-1$ is a polynomial in $x$ and $y$, and thus is continuously differentiable ($C^{1}$, and in fact $C^{\\infty}$) on all of $\\mathbb{R}^{2}$.\n2. We have already confirmed that $F(x_{0},y_{0})=F(1,0)=0$.\n3. We must check that the partial derivative of $F$ with respect to $y$ is non-zero at $(1,0)$. The partial derivative is:\n$$\\partial_{y}F(x,y) = \\frac{\\partial}{\\partial y}(x^{2}+2xy+3y^{2}-1) = 2x+6y$$\nEvaluating this at $(x_{0},y_{0})=(1,0)$:\n$$\\partial_{y}F(1,0) = 2(1)+6(0) = 2$$\nSince $\\partial_{y}F(1,0)=2\\neq 0$, all hypotheses of the IFT are satisfied at the point $(1,0)$.\n\nThe Implicit Function Theorem thus guarantees the existence of an open interval $U \\subset \\mathbb{R}$ containing $x_{0}=1$ and a continuously differentiable ($C^{1}$) function $f:U\\to\\mathbb{R}$ such that $f(1)=0$ and for all $x \\in U$, we have $F(x,f(x))=0$. This gives the local $C^{1}$ parametrization $x\\mapsto(x,f(x))$ of the ellipse near $(1,0)$. Since $F$ is $C^\\infty$, the local function $f$ is also $C^\\infty$.\n\nNow, we must determine the value of the second derivative $f''(x_{0})=f''(1)$. We start with the identity which holds for all $x \\in U$:\n$$x^{2}+2xf(x)+3(f(x))^{2}-1 = 0$$\nWe differentiate this entire equation with respect to $x$, using the product rule and the chain rule:\n$$\\frac{d}{dx}[x^{2}+2xf(x)+3(f(x))^{2}-1] = \\frac{d}{dx}[0]$$\n$$2x + \\left(2f(x)+2xf'(x)\\right) + 6f(x)f'(x) - 0 = 0$$\n$$2x + 2f(x) + (2x+6f(x))f'(x) = 0$$\nTo find $f'(1)$, we evaluate this equation at $x=x_{0}=1$, using $f(1)=y_{0}=0$:\n$$2(1) + 2f(1) + (2(1)+6f(1))f'(1) = 0$$\n$$2 + 2(0) + (2+6(0))f'(1) = 0$$\n$$2 + 2f'(1) = 0$$\n$$f'(1) = -1$$\nNext, we differentiate the expression $2x + 2f(x) + (2x+6f(x))f'(x) = 0$ again with respect to $x$:\n$$\\frac{d}{dx}[2x + 2f(x) + (2x+6f(x))f'(x)] = 0$$\n$$2 + 2f'(x) + \\frac{d}{dx}[(2x+6f(x))f'(x)] = 0$$\nUsing the product rule on the last term:\n$$2 + 2f'(x) + \\left[\\left(\\frac{d}{dx}(2x+6f(x))\\right)f'(x) + (2x+6f(x))f''(x)\\right] = 0$$\n$$2 + 2f'(x) + (2+6f'(x))f'(x) + (2x+6f(x))f''(x) = 0$$\n$$2 + 4f'(x) + 6(f'(x))^{2} + (2x+6f(x))f''(x) = 0$$\nNow, we evaluate this second derivative equation at $x=x_{0}=1$, using $f(1)=0$ and $f'(1)=-1$:\n$$2 + 4f'(1) + 6(f'(1))^{2} + (2(1)+6f(1))f''(1) = 0$$\n$$2 + 4(-1) + 6(-1)^{2} + (2+6(0))f''(1) = 0$$\n$$2 - 4 + 6(1) + (2)f''(1) = 0$$\n$$2 - 4 + 6 + 2f''(1) = 0$$\n$$4 + 2f''(1) = 0$$\n$$2f''(1) = -4$$\n$$f''(1) = -2$$\nThe value of the second derivative at the chosen point is $-2$.", "answer": "$$\\boxed{-2}$$", "id": "3053852"}, {"introduction": "The power of a theorem is often clarified by understanding its limitations. The Implicit Function Theorem guarantees that a level set is locally a smooth manifold, provided the function's derivative has full rank. This exercise [@problem_id:3053777] examines a classic counterexample, the set defined by $xy=0$, where the theorem's conditions fail at the origin. By analyzing this singularity, you will gain a more profound appreciation for the geometric meaning of the theorem's hypotheses and the structure of sets that are not manifolds.", "problem": "Let $M = (\\mathbb{R}^{2}, g)$ be the Euclidean plane with its standard Riemannian metric $g$, and let $F : M \\to \\mathbb{R}$ be the smooth function defined by $F(x,y) = x y$. Consider the level set $S = F^{-1}(0) \\subset M$. Using only core definitions (smooth map, differential, rank, tangent space, and the statements of the Inverse Function Theorem and the Implicit Function Theorem), address the following in order to reach a final quantitative conclusion.\n\n1) Compute the differential $dF_{(0,0)} : T_{(0,0)}M \\to T_{0}\\mathbb{R}$ and its rank at the origin $(0,0)$. Explain, using the Implicit Function Theorem, why $S$ fails to be a $1$-dimensional embedded submanifold of $M$ in any neighborhood of $(0,0)$.\n\n2) Define the Bouligand (also called contingent) tangent cone of $S$ at $(0,0)$ by\n$$\nT^{\\mathrm{B}}_{(0,0)} S \\;=\\; \\left\\{ v \\in T_{(0,0)}M \\;\\middle|\\; \\exists\\, \\{p_{k}\\}\\subset S,\\ \\exists\\, t_{k} \\downarrow 0,\\ \\text{with } p_{k} \\to (0,0)\\ \\text{and}\\ \\frac{p_{k} - (0,0)}{t_{k}} \\to v \\right\\}.\n$$\nCompute $T^{\\mathrm{B}}_{(0,0)} S$ explicitly as a subset of $T_{(0,0)}M \\cong \\mathbb{R}^{2}$.\n\n3) Using the metric $g$ at $(0,0)$ to define angles in $T_{(0,0)}M$, consider the set of nonzero vectors in $T^{\\mathrm{B}}_{(0,0)} S$ normalized to unit length. Among all pairs of distinct unit vectors in this set, determine the smallest positive angle between them. Express your final answer in radians.", "solution": "The problem is analyzed in three parts as requested.\n\n1) Let $M = (\\mathbb{R}^{2}, g)$ be the Euclidean plane. The function is $F: \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $F(x, y) = xy$. The tangent space at any point $p \\in \\mathbb{R}^{2}$, $T_p M$, can be identified with $\\mathbb{R}^{2}$ itself. Similarly, the tangent space $T_c \\mathbb{R}$ for any $c \\in \\mathbb{R}$ can be identified with $\\mathbb{R}$.\n\nThe differential of $F$ at a point $p=(x,y)$, denoted $dF_p: T_p M \\to T_{F(p)} \\mathbb{R}$, is represented by the Jacobian matrix of $F$. The partial derivatives of $F$ are:\n$$\n\\frac{\\partial F}{\\partial x} = y\n$$\n$$\n\\frac{\\partial F}{\\partial y} = x\n$$\nThe Jacobian matrix at $(x,y)$ is a $1 \\times 2$ matrix:\n$$\nJ_F(x,y) = \\begin{pmatrix} \\frac{\\partial F}{\\partial x}  \\frac{\\partial F}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y  x \\end{pmatrix}\n$$\nWe are interested in the point $(0,0)$. At this point, the differential $dF_{(0,0)}$ is represented by the Jacobian matrix:\n$$\nJ_F(0,0) = \\begin{pmatrix} 0  0 \\end{pmatrix}\n$$\nThis represents the zero linear map from $\\mathbb{R}^{2}$ to $\\mathbb{R}$. The rank of a linear map is the dimension of its image. The image of the zero map is $\\{0\\}$, which has dimension $0$. Therefore, the rank of $dF_{(0,0)}$ is $0$.\n\nThe Implicit Function Theorem states that for a smooth map $F: \\mathbb{R}^{n} \\to \\mathbb{R}^{k}$, if the differential $dF_p$ at a point $p$ in the level set $S=F^{-1}(c)$ has full rank (i.e., $\\mathrm{rank}(dF_p) = k$), then in a neighborhood of $p$, the set $S$ is an $(n-k)$-dimensional embedded submanifold of $\\mathbb{R}^{n}$.\nIn our case, $F: \\mathbb{R}^{2} \\to \\mathbb{R}^{1}$, so $n=2$ and $k=1$. The level set is $S = F^{-1}(0)$. The theorem would guarantee that $S$ is a $(2-1)=1$-dimensional submanifold near a point $p \\in S$ if $\\mathrm{rank}(dF_p)=1$.\nAt the point $p=(0,0)$, we have computed that $\\mathrm{rank}(dF_{(0,0)}) = 0$. Since the rank is not $1$, the condition of the Implicit Function Theorem is not satisfied. This failure is the reason why $S$ is not a $1$-dimensional embedded submanifold in any neighborhood of $(0,0)$. Geometrically, the set $S = \\{(x,y) \\in \\mathbb{R}^2 \\mid xy=0\\}$ is the union of the $x$-axis ($y=0$) and the $y$-axis ($x=0$). At the origin, these two lines intersect, forming a 'cross' which is not locally homeomorphic to a line ($\\mathbb{R}^1$). Any neighborhood of $(0,0)$ in $S$ resembles this cross, and removing the point $(0,0)$ disconnects it into four components, whereas removing a point from a $1$-manifold would result in at most two components.\n\n2) We now compute the Bouligand tangent cone $T^{\\mathrm{B}}_{(0,0)} S$. The definition is given as:\n$$\nT^{\\mathrm{B}}_{(0,0)} S = \\left\\{ v \\in T_{(0,0)}M \\;\\middle|\\; \\exists\\, \\{p_{k}\\}\\subset S,\\ \\exists\\, t_{k} \\downarrow 0,\\ \\text{with } p_{k} \\to (0,0)\\ \\text{and}\\ \\frac{p_{k} - (0,0)}{t_{k}} \\to v \\right\\}\n$$\nWe identify $T_{(0,0)}M$ with $\\mathbb{R}^{2}$. The set $S$ is the union of the coordinate axes. We consider sequences $\\{p_k\\}$ approaching the origin along these axes.\n\nLet $v = (v_x, 0)$ be an arbitrary vector on the $x$-axis in $\\mathbb{R}^2$. We show that $v \\in T^{\\mathrm{B}}_{(0,0)} S$. Consider the sequence of points $p_k = (\\frac{v_x}{k}, 0)$ for $k \\in \\{1, 2, 3, \\dots\\}$. Each $p_k$ is in $S$ because its second component is $0$. As $k \\to \\infty$, $p_k \\to (0,0)$. Let us choose the sequence of real numbers $t_k = \\frac{1}{k}$. Then $t_k  0$ and $t_k \\to 0$, so $t_k \\downarrow 0$. We compute the limit of the quotients:\n$$\n\\lim_{k \\to \\infty} \\frac{p_k}{t_k} = \\lim_{k \\to \\infty} \\frac{(\\frac{v_x}{k}, 0)}{\\frac{1}{k}} = \\lim_{k \\to \\infty} (v_x, 0) = (v_x, 0) = v\n$$\nThus, any vector on the $x$-axis, $\\{(x,0) \\mid x \\in \\mathbb{R}\\}$, is in $T^{\\mathrm{B}}_{(0,0)} S$.\n\nBy a symmetrical argument, let $v = (0, v_y)$ be an arbitrary vector on the $y$-axis. Consider the sequence $p_k = (0, \\frac{v_y}{k})$ in $S$ and $t_k = \\frac{1}{k}$. Then $p_k \\to (0,0)$ and $t_k \\downarrow 0$.\n$$\n\\lim_{k \\to \\infty} \\frac{p_k}{t_k} = \\lim_{k \\to \\infty} \\frac{(0, \\frac{v_y}{k})}{\\frac{1}{k}} = \\lim_{k \\to \\infty} (0, v_y) = (0, v_y) = v\n$$\nThus, any vector on the $y$-axis, $\\{(0,y) \\mid y \\in \\mathbb{R}\\}$, is in $T^{\\mathrm{B}}_{(0,0)} S$.\n\nNow we must show that no other vectors are in the cone. Let $v=(v_x, v_y)$ with $v_x \\neq 0$ and $v_y \\neq 0$. Suppose $v \\in T^{\\mathrm{B}}_{(0,0)} S$. Then there exists a sequence $p_k = (x_k, y_k) \\in S$ converging to $(0,0)$ and a sequence $t_k \\downarrow 0$ such that $\\lim_{k\\to\\infty} \\frac{(x_k, y_k)}{t_k} = (v_x, v_y)$. This implies $\\lim_{k\\to\\infty} \\frac{x_k}{t_k} = v_x$ and $\\lim_{k\\to\\infty} \\frac{y_k}{t_k} = v_y$. Since $p_k \\in S$, we have $x_k y_k = 0$ for all $k$. This means that for each $k$, either $x_k=0$ or $y_k=0$.\nIf there is a subsequence $\\{k_j\\}$ such that $x_{k_j}=0$, then $\\frac{x_{k_j}}{t_{k_j}} = 0$, which implies that the limit $v_x$ must be $0$. Similarly, if there is a subsequence where $y_{k_j}=0$, the limit $v_y$ must be $0$. For the full sequence $\\frac{p_k}{t_k}$ to converge to $(v_x, v_y)$, the subsequences must converge to the same limit. If $v_x \\ne 0$ and $v_y \\ne 0$, then for $k$ large enough, we must have $x_k \\neq 0$ and $y_k \\neq 0$, which contradicts $x_k y_k = 0$. Therefore, at least one of $v_x$ or $v_y$ must be $0$.\n\nThe Bouligand tangent cone is the union of the sets we have found:\n$$\nT^{\\mathrm{B}}_{(0,0)} S = \\{(x,y) \\in \\mathbb{R}^2 \\mid x=0 \\text{ or } y=0\\}\n$$\nThis is the union of the coordinate axes in $T_{(0,0)}M \\cong \\mathbb{R}^2$.\n\n3) The metric $g$ at $(0,0)$ is the standard Euclidean inner product (dot product) on $\\mathbb{R}^2$. The angle $\\theta$ between two non-zero vectors $u,w \\in \\mathbb{R}^2$ is given by $\\cos\\theta = \\frac{u \\cdot w}{\\|u\\| \\|w\\|}$. We are looking for the smallest positive angle between pairs of distinct unit vectors in $T^{\\mathrm{B}}_{(0,0)} S$.\n\nThe set of unit vectors in $T^{\\mathrm{B}}_{(0,0)} S$ consists of the unit vectors lying on the coordinate axes. These are:\n- On the $x$-axis: $e_1 = (1,0)$ and $-e_1 = (-1,0)$.\n- On the $y$-axis: $e_2 = (0,1)$ and $-e_2 = (0,-1)$.\n\nLet's examine the angles between distinct pairs of these four vectors:\n- Angle between $e_1$ and $-e_1$: Their dot product is $(1,0) \\cdot (-1,0) = -1$. Since they are unit vectors, $\\cos\\theta = -1$, which gives $\\theta = \\pi$ radians.\n- Angle between $e_2$ and $-e_2$: Their dot product is $(0,1) \\cdot (0,-1) = -1$, giving $\\theta = \\pi$ radians.\n- Angle between a vector on the $x$-axis (e.g., $e_1$) and a vector on the $y$-axis (e.g., $e_2$): Their dot product is $(1,0) \\cdot (0,1) = 0$. The cosine of the angle is $0$, which gives $\\theta = \\frac{\\pi}{2}$ radians. This is true for any pair consisting of one vector from $\\{e_1, -e_1\\}$ and one from $\\{e_2, -e_2\\}$.\n\nThe possible positive angles between distinct unit vectors in $T^{\\mathrm{B}}_{(0,0)} S$ are $\\frac{\\pi}{2}$ and $\\pi$. The smallest of these positive angles is $\\frac{\\pi}{2}$.", "answer": "$$\\boxed{\\frac{\\pi}{2}}$$", "id": "3053777"}]}