{"hands_on_practices": [{"introduction": "Understanding tensors often begins with connecting their abstract definitions to concrete calculations in a chosen basis. This first practice grounds the concept of a symmetric bilinear form by starting with its matrix representation. By applying the rules of bilinearity directly, you will derive the coordinate expression for the form and explicitly verify its symmetry, seeing firsthand how the properties of the matrix translate to the properties of the tensor itself [@problem_id:3064500].", "problem": "Let $V=\\mathbb{R}^{2}$ with the standard basis $\\{e_{1},e_{2}\\}$. Consider the bilinear form $B:V\\times V\\to \\mathbb{R}$ whose matrix relative to this basis is the symmetric matrix\n$$\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}.\n$$\nLet $u=(a,b)$ and $v=(c,d)$ denote arbitrary vectors in $V$ written in the standard coordinates, so that $u=a\\,e_{1}+b\\,e_{2}$ and $v=c\\,e_{1}+d\\,e_{2}$. Using only the foundational definitions that a bilinear form is linear in each argument and that the matrix entries $B_{ij}$ record the values $B(e_{i},e_{j})$ relative to a basis, compute an explicit expression for $B(u,v)$ in terms of $a$, $b$, $c$, and $d$. Then, starting from these definitions, verify in coordinates that $B(u,v)=B(v,u)$.\n\nProvide your final answer as the single closed-form expression for $B(u,v)$ in terms of $a$, $b$, $c$, and $d$ (no units and no numeric rounding are required).", "solution": "The problem is well-posed and mathematically sound, containing all necessary information for a unique solution. We proceed with the derivation.\n\nLet $V = \\mathbb{R}^2$ be a real vector space with the standard basis $\\{e_1, e_2\\}$. We are given two vectors $u, v \\in V$ with coordinate representations $u = a e_1 + b e_2$ and $v = c e_1 + d e_2$ for some scalars $a, b, c, d \\in \\mathbb{R}$.\n\nWe are also given a bilinear form $B: V \\times V \\to \\mathbb{R}$. The matrix representation of $B$ with respect to the basis $\\{e_1, e_2\\}$ is given as:\n$$\n[B] = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nBy the foundational definition of the matrix of a bilinear form, the entries $B_{ij}$ are the values of the form on the basis vectors: $B_{ij} = B(e_i, e_j)$. Therefore, we have:\n$B(e_1, e_1) = 2$\n$B(e_1, e_2) = 1$\n$B(e_2, e_1) = 1$\n$B(e_2, e_2) = 3$\n\nThe problem requires the computation of $B(u, v)$ using only the definition that $B$ is linear in each of its arguments.\n\nFirst, we express $B(u, v)$ using the coordinate representations of $u$ and $v$:\n$$\nB(u, v) = B(a e_1 + b e_2, c e_1 + d e_2)\n$$\nUsing the linearity of $B$ in the first argument, we can expand this expression:\n$$\nB(a e_1 + b e_2, c e_1 + d e_2) = B(a e_1, c e_1 + d e_2) + B(b e_2, c e_1 + d e_2)\n$$\nApplying the scalar multiplication property of linearity in the first argument, we get:\n$$\n= a B(e_1, c e_1 + d e_2) + b B(e_2, c e_1 + d e_2)\n$$\nNext, we apply the linearity of $B$ in the second argument to each of the two terms:\n$$\na B(e_1, c e_1 + d e_2) = a \\left( B(e_1, c e_1) + B(e_1, d e_2) \\right) = a \\left( c B(e_1, e_1) + d B(e_1, e_2) \\right)\n$$\n$$\nb B(e_2, c e_1 + d e_2) = b \\left( B(e_2, c e_1) + B(e_2, d e_2) \\right) = b \\left( c B(e_2, e_1) + d B(e_2, e_2) \\right)\n$$\nCombining these expanded terms, we obtain the full expansion of $B(u, v)$:\n$$\nB(u, v) = a c B(e_1, e_1) + a d B(e_1, e_2) + b c B(e_2, e_1) + b d B(e_2, e_2)\n$$\nNow, we substitute the given values for $B(e_i, e_j)$ from the matrix $[B]$:\n$$\nB(u, v) = a c (2) + a d (1) + b c (1) + b d (3)\n$$\nSimplifying this expression yields the explicit formula for $B(u, v)$ in terms of the coordinates $a, b, c, d$:\n$$\nB(u, v) = 2ac + ad + bc + 3bd\n$$\nThis completes the first part of the problem.\n\nFor the second part, we must verify that $B(u, v) = B(v, u)$ using the same foundational definitions. We start by computing $B(v, u)$. The vectors are $v = c e_1 + d e_2$ and $u = a e_1 + b e_2$.\n$$\nB(v, u) = B(c e_1 + d e_2, a e_1 + b e_2)\n$$\nFollowing the same procedure of applying bilinearity, we expand the expression:\n$$\nB(v, u) = c B(e_1, a e_1 + b e_2) + d B(e_2, a e_1 + b e_2)\n$$\n$$\n= c \\left( a B(e_1, e_1) + b B(e_1, e_2) \\right) + d \\left( a B(e_2, e_1) + b B(e_2, e_2) \\right)\n$$\n$$\nB(v, u) = c a B(e_1, e_1) + c b B(e_1, e_2) + d a B(e_2, e_1) + d b B(e_2, e_2)\n$$\nNow, we substitute the values for $B(e_i, e_j)$:\n$$\nB(v, u) = c a (2) + c b (1) + d a (1) + d b (3)\n$$\n$$\nB(v, u) = 2ca + cb + da + 3db\n$$\nSince multiplication of real numbers is commutative ($ac = ca$, $ad = da$, $bc = cb$, $bd = db$), we can rearrange the terms to match the expression for $B(u, v)$:\n$$\nB(v, u) = 2ac + bc + ad + 3bd = 2ac + ad + bc + 3bd\n$$\nComparing this with our result for $B(u, v)$, we see that:\n$$\nB(u, v) = 2ac + ad + bc + 3bd = B(v, u)\n$$\nThe verification is complete. The symmetry of the bilinear form, $B(u, v) = B(v, u)$, is a direct consequence of the symmetry of its matrix representation with respect to the chosen basis, i.e., $B(e_i, e_j) = B_{ij} = B_{ji} = B(e_j, e_i)$. In this specific case, the fact that $B(e_1, e_2) = 1$ and $B(e_2, e_1) = 1$ ensures the symmetry.\nThe final required answer is the explicit expression for $B(u, v)$.", "answer": "$$\n\\boxed{2ac + ad + bc + 3bd}\n$$", "id": "3064500"}, {"introduction": "A crucial insight in tensor analysis is that any rank-2 tensor can be uniquely decomposed into a purely symmetric part and a purely alternating part. This practice guides you through the process of calculating these components for a given tensor [@problem_id:3066950]. Furthermore, you will investigate how these components transform under a change of basis, confirming that symmetry and alternation are intrinsic, coordinate-independent properties of the tensor.", "problem": "Let $(V,g)$ be a $3$-dimensional real inner product space modeling a tangent space of a Riemannian manifold at a point, with metric components $g_{ij}=\\delta_{ij}$ in an orthonormal basis $\\{e_{1},e_{2},e_{3}\\}$. Consider a $(0,2)$-tensor $B$ with components in this basis given by the matrix\n$$\n\\big(B_{ij}\\big)=\\begin{pmatrix}\n2 & 1 & -1\\\\\n3 & 0 & 4\\\\\n1 & -4 & 5\n\\end{pmatrix}.\n$$\nPerform the following tasks using only foundational definitions of symmetry/alternation for tensors and the definition of how tensor components transform under a change of basis.\n\n1) Compute the symmetric and alternating parts $B_{(ij)}$ and $B_{[ij]}$ of $B$ in the given orthonormal basis.\n\n2) Let $\\{e'_{1},e'_{2},e'_{3}\\}$ be another orthonormal basis obtained by rotating $\\{e_{1},e_{2},e_{3}\\}$ about $e_{3}$ by angle $\\theta=\\pi/3$. Denote the associated rotation matrix by\n$$\nR=\\begin{pmatrix}\n\\cos(\\pi/3) & -\\sin(\\pi/3) & 0\\\\\n\\sin(\\pi/3) & \\cos(\\pi/3) & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nUsing the definition of a $(0,2)$-tensor and the transformation of basis vectors, derive how the components of $B_{(ij)}$ and $B_{[ij]}$ transform from the unprimed to the primed basis, and explain why symmetry and alternation are preserved under this change of basis.\n\n3) With respect to $g$, define the squared norm of the alternating part by\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2}:=g^{ik}g^{jl}B_{[ij]}B_{[kl]}.\n$$\nCompute this quantity using the components found in part (1). Give your final answer as a single exact integer with no units.", "solution": "The problem is assessed to be valid as it is mathematically well-defined, self-contained, and grounded in the standard principles of tensor algebra on an inner product space. All provided information is consistent and sufficient to arrive at a unique solution.\n\nThe problem is addressed in three parts as requested.\n\n1) Computation of symmetric and alternating parts of $B$.\n\nA $(0,2)$-tensor $B$ with components $B_{ij}$ can be decomposed into its symmetric part, denoted $B_{(ij)}$, and its alternating (or anti-symmetric) part, denoted $B_{[ij]}$. The definitions for these components are:\n$$\nB_{(ij)} = \\frac{1}{2}(B_{ij} + B_{ji})\n$$\n$$\nB_{[ij]} = \\frac{1}{2}(B_{ij} - B_{ji})\n$$\nThe given components of $B$ in the orthonormal basis $\\{e_1, e_2, e_3\\}$ form the matrix:\n$$\n(B_{ij}) = \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix}\n$$\nThe transpose of this matrix gives the components $B_{ji}$:\n$$\n(B_{ji}) = \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix}\n$$\nNow, we compute the component matrices for the symmetric and alternating parts.\n\nFor the symmetric part $B_{(ij)}$:\n$$\n(B_{(ij)}) = \\frac{1}{2} \\left[ \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix} + \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2+2 & 1+3 & -1+1 \\\\\n3+1 & 0+0 & 4-4 \\\\\n1-1 & -4+4 & 5+5\n\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}\n4 & 4 & 0 \\\\\n4 & 0 & 0 \\\\\n0 & 0 & 10\n\\end{pmatrix}\n$$\n$$\n(B_{(ij)}) = \\begin{pmatrix}\n2 & 2 & 0 \\\\\n2 & 0 & 0 \\\\\n0 & 0 & 5\n\\end{pmatrix}\n$$\nFor the alternating part $B_{[ij]}$:\n$$\n(B_{[ij]}) = \\frac{1}{2} \\left[ \\begin{pmatrix}\n2 & 1 & -1 \\\\\n3 & 0 & 4 \\\\\n1 & -4 & 5\n\\end{pmatrix} - \\begin{pmatrix}\n2 & 3 & 1 \\\\\n1 & 0 & -4 \\\\\n-1 & 4 & 5\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2-2 & 1-3 & -1-1 \\\\\n3-1 & 0-0 & 4-(-4) \\\\\n1-(-1) & -4-4 & 5-5\n\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}\n0 & -2 & -2 \\\\\n2 & 0 & 8 \\\\\n2 & -8 & 0\n\\end{pmatrix}\n$$\n$$\n(B_{[ij]}) = \\begin{pmatrix}\n0 & -1 & -1 \\\\\n1 & 0 & 4 \\\\\n1 & -4 & 0\n\\end{pmatrix}\n$$\n\n2) Transformation of components and preservation of symmetry/alternation.\n\nLet $\\{e_i\\}$ be the original orthonormal basis and $\\{e'_j\\}$ be the new orthonormal basis. The transformation relating them is given by the rotation matrix $R$. The new basis vectors are expressed in terms of the old basis vectors as:\n$$\ne'_j = \\sum_{i=1}^3 R^i_{\\;j} e_i\n$$\nwhere $R^i_{\\;j}$ are the elements of the matrix $R$. A tensor is a geometric object, and its value is independent of the basis used to express it. For a $(0,2)$-tensor $B$ and two vectors $v, w \\in V$, we have $B(v,w)$. Let the components of the vectors be $v^i$ and $w^j$ in the $\\{e_i\\}$ basis and $v'^k$ and $w'^l$ in the $\\{e'_j\\}$ basis. The vectors themselves are invariant: $v = v^i e_i = v'^j e'_j$. This implies a transformation law for the components: $v^i = \\sum_j R^i_{\\;j} v'^j$.\n\nThe value of the tensor acting on the vectors is also invariant:\n$$\nB(v,w) = B_{ij} v^i w^j = B'_{kl} v'^k w'^l\n$$\nSubstituting the transformation for the vector components into the expression in the unprimed basis:\n$$\nB_{ij} \\left(\\sum_k R^i_{\\;k} v'^k\\right) \\left(\\sum_l R^j_{\\;l} w'^l\\right) = \\sum_{k,l} \\left(\\sum_{i,j} B_{ij} R^i_{\\;k} R^j_{\\;l}\\right) v'^k w'^l\n$$\nComparing this with $B'_{kl} v'^k w'^l$, we deduce the transformation law for the components of a $(0,2)$-tensor:\n$$\nB'_{kl} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} B_{ij}\n$$\nNow we show that symmetry and alternation are preserved under this transformation.\nLet $S$ be a symmetric $(0,2)$-tensor, with components $S_{ij} = S_{ji}$. Its components in the primed basis are $S'_{kl} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} S_{ij}$. We check if $S'$ is symmetric by computing the components $S'_{lk}$:\n$$\nS'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} S_{ij}\n$$\nSince $S$ is symmetric, $S_{ij} = S_{ji}$. We can substitute this into the expression:\n$$\nS'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} S_{ji}\n$$\nNow, we can rename the dummy summation indices (swap $i \\leftrightarrow j$):\n$$\nS'_{lk} = \\sum_{j,i} R^j_{\\;l} R^i_{\\;k} S_{ij}\n$$\nRearranging the terms in the sum (since they are just numbers) gives:\n$$\nS'_{lk} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} S_{ij}\n$$\nThis is exactly the definition of $S'_{kl}$. Therefore, $S'_{lk} = S'_{kl}$, and the tensor $S'$ is symmetric. Symmetry is a coordinate-independent property.\n\nLet $A$ be an alternating $(0,2)$-tensor, with components $A_{ij} = -A_{ji}$. Its transformed components are $A'_{kl} = \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} A_{ij}$. We check if $A'$ is alternating by computing $A'_{lk}$:\n$$\nA'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} A_{ij}\n$$\nUsing the alternating property $A_{ij} = -A_{ji}$:\n$$\nA'_{lk} = \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} (-A_{ji}) = - \\sum_{i,j} R^i_{\\;l} R^j_{\\;k} A_{ji}\n$$\nRenaming the dummy summation indices ($i \\leftrightarrow j$):\n$$\nA'_{lk} = - \\sum_{j,i} R^j_{\\;l} R^i_{\\;k} A_{ij}\n$$\nRearranging the terms gives:\n$$\nA'_{lk} = - \\sum_{i,j} R^i_{\\;k} R^j_{\\;l} A_{ij} = -A'_{kl}\n$$\nThus, $A'$ is alternating. Alternation is also a coordinate-independent property. This shows that applying the symmetrization or alternation operations commutes with a change of basis. Therefore, the symmetric part of $B$ in one basis transforms to the symmetric part of the transformed tensor $B'$ in the new basis, and similarly for the alternating part.\n\n3) Computation of the squared norm of the alternating part.\n\nThe squared norm of the alternating part $B_{[ij]}$ is defined as:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} := g^{ik}g^{jl}B_{[ij]}B_{[kl]}\n$$\nThe problem states that we are in an orthonormal basis $\\{e_i\\}$, in which the metric components are $g_{ij} = \\delta_{ij}$. The matrix of components of the contravariant metric tensor, $g^{ij}$, is the inverse of the matrix of $g_{ij}$. Since the matrix $(g_{ij})$ is the identity matrix $I$, its inverse is also the identity matrix. Thus, $g^{ij} = \\delta^{ij}$, where $\\delta^{ij}$ are the components of the identity matrix (numerically same as $\\delta_{ij}$).\n\nSubstituting $g^{ik} = \\delta^{ik}$ and $g^{jl} = \\delta^{jl}$ into the norm definition:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\delta^{ik}\\delta^{jl}B_{[ij]}B_{[kl]}\n$$\nThe summation is over all four indices $i,j,k,l$. The Kronecker deltas simplify the sum. The term $\\delta^{ik}$ is non-zero (equal to $1$) only when $k=i$. Summing over $k$:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\sum_{i,j,l} \\delta^{jl}B_{[ij]}B_{[il]}\n$$\nSimilarly, the term $\\delta^{jl}$ is non-zero (equal to $1$) only when $l=j$. Summing over $l$:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = \\sum_{i,j} B_{[ij]}B_{[ij]} = \\sum_{i=1}^3 \\sum_{j=1}^3 (B_{[ij]})^2\n$$\nThis is the sum of the squares of all the components of the matrix for $B_{[ij]}$. From part (1), we have:\n$$\n(B_{[ij]}) = \\begin{pmatrix}\n0 & -1 & -1 \\\\\n1 & 0 & 4 \\\\\n1 & -4 & 0\n\\end{pmatrix}\n$$\nThe sum of the squares of these components is:\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = (0)^2 + (-1)^2 + (-1)^2 + (1)^2 + (0)^2 + (4)^2 + (1)^2 + (-4)^2 + (0)^2\n$$\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = 0 + 1 + 1 + 1 + 0 + 16 + 1 + 16 + 0\n$$\n$$\n\\|B_{[\\,\\cdot\\,\\cdot\\,]}\\|^{2} = 36\n$$\nThe squared norm is a scalar invariant, meaning its value does not depend on the choice of orthonormal basis. The calculation in the primed basis would yield the same result.", "answer": "$$\\boxed{36}$$", "id": "3066950"}, {"introduction": "Alternating tensors and the wedge product are not just algebraic curiosities; they are fundamental tools for describing geometry. This final practice reveals the profound connection between the alternating part of a tensor product and the concept of area [@problem_id:3066952]. You will first derive the coordinate-free formula linking the area of a triangle to the norm of a bivector and then apply it to compute a specific area, solidifying the geometric intuition behind these abstract structures.", "problem": "Let $\\left(M,g\\right)$ be a $2$-dimensional Riemannian manifold, and let $p \\in M$ be a point. The Riemannian metric $g_{p}$ at $p$ is a symmetric positive-definite bilinear form on the tangent space $T_{p}M$. The exterior square $\\wedge^{2}T_{p}M$ is a $1$-dimensional space of alternating tensors, and $g_{p}$ induces an inner product on $\\wedge^{2}T_{p}M$ via the standard construction for simple bivectors. Starting only from these core definitions, derive a coordinate-free expression for the area of the triangle with sides given by vectors $u,v \\in T_{p}M$ in terms of the wedge product $u \\wedge v$ and the inner product induced by $g_{p}$ on $\\wedge^{2}T_{p}M$.\n\nThen, apply your result in the following concrete example. Let $M=\\mathbb{R}^{2}$ with global coordinates $\\left(x,y\\right)$ and let $g$ be the constant Riemannian metric whose matrix in the coordinate basis $\\left(\\partial_{x},\\partial_{y}\\right)$ is\n$$\nG \\;=\\; \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n$$\nAt the point $p=(0,0)$, consider $u = 2\\,\\partial_{x} - 1\\,\\partial_{y}$ and $v = 1\\,\\partial_{x} + 3\\,\\partial_{y}$. Using only the definitions of the induced inner product on $\\wedge^{2}T_{p}M$ and the area measure arising from $g$, compute the area of the triangle determined by $u$ and $v$. Give your final answer in exact form as a single expression.", "solution": "The problem is valid as it is scientifically grounded in standard Riemannian geometry, is well-posed, completely specified, and objective.\n\nThe solution is divided into two parts. First, we derive the coordinate-free expression for the area of a triangle in terms of the wedge product and the induced metric. Second, we apply this result to the given concrete example.\n\nLet $\\left(M, g\\right)$ be a $2$-dimensional Riemannian manifold and let $p \\in M$. The tangent space $T_p M$ at $p$ is a $2$-dimensional real vector space equipped with an inner product $g_p$.\n\nThe area of the parallelogram spanned by two vectors $u, v \\in T_p M$ is given by the square root of the determinant of the Gram matrix of these vectors. Let this area be denoted by $A_{\\text{para}}$. The square of the area is:\n$$\nA_{\\text{para}}^{2} = \\det \\begin{pmatrix} g_p(u,u) & g_p(u,v) \\\\ g_p(v,u) & g_p(v,v) \\end{pmatrix}\n$$\nSince the metric tensor $g_p$ is symmetric, i.e., $g_p(u,v) = g_p(v,u)$, this expands to:\n$$\nA_{\\text{para}}^{2} = g_p(u,u)g_p(v,v) - \\left(g_p(u,v)\\right)^{2}\n$$\nNow, we consider the exterior algebra on $T_p M$. The space of $2$-vectors on $T_p M$ is denoted by $\\wedge^{2} T_p M$. A simple $2$-vector (or bivector) formed from $u$ and $v$ is their wedge product, $u \\wedge v$. The inner product $g_p$ on $T_p M$ induces an inner product on the space $\\wedge^k T_p M$ for any $k$. For two simple $k$-vectors $\\alpha = v_1 \\wedge \\dots \\wedge v_k$ and $\\beta = w_1 \\wedge \\dots \\wedge w_k$, this induced inner product, which we denote by $\\langle \\cdot, \\cdot \\rangle_{\\wedge^k}$, is defined as the determinant of the matrix of inner products of their constituent vectors:\n$$\n\\langle \\alpha, \\beta \\rangle_{\\wedge^k} = \\det\\left(g_p(v_i, w_j)\\right)_{i,j=1,\\dots,k}\n$$\nWe are interested in the norm of the bivector $u \\wedge v \\in \\wedge^2 T_p M$. The norm squared of $u \\wedge v$ is given by its inner product with itself:\n$$\n\\|u \\wedge v\\|_{\\wedge^2}^2 = \\langle u \\wedge v, u \\wedge v \\rangle_{\\wedge^2}\n$$\nApplying the definition for $k=2$ with $v_1=w_1=u$ and $v_2=w_2=v$:\n$$\n\\|u \\wedge v\\|_{\\wedge^2}^2 = \\det \\begin{pmatrix} g_p(u,u) & g_p(u,v) \\\\ g_p(v,u) & g_p(v,v) \\end{pmatrix}\n$$\nComparing this with the expression for the squared area of the parallelogram, we find a direct relationship:\n$$\nA_{\\text{para}}^{2} = \\|u \\wedge v\\|_{\\wedge^2}^2\n$$\nTaking the square root, we find that the area of the parallelogram is equal to the norm of the wedge product of the vectors spanning it:\n$$\nA_{\\text{para}} = \\|u \\wedge v\\|_{\\wedge^2}\n$$\nThe problem asks for the area of the triangle with sides given by vectors $u$ and $v$. This area, $A_{\\text{tri}}$, is half the area of the parallelogram spanned by these vectors.\n$$\nA_{\\text{tri}} = \\frac{1}{2} A_{\\text{para}} = \\frac{1}{2} \\|u \\wedge v\\|_{\\wedge^2}\n$$\nThis is the required coordinate-free expression.\n\nNow, we apply this result to the given example. We are on $M=\\mathbb{R}^{2}$ with the standard coordinate basis $(\\partial_x, \\partial_y)$. Let's denote $e_1 = \\partial_x$ and $e_2 = \\partial_y$. The metric $g$ has the constant component matrix:\n$$\nG = \\begin{pmatrix} g(e_1, e_1) & g(e_1, e_2) \\\\ g(e_2, e_1) & g(e_2, e_2) \\end{pmatrix} = \\begin{pmatrix} g_{11} & g_{12} \\\\ g_{21} & g_{22} \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\nThe vectors at $p=(0,0)$ are given as $u = 2\\,\\partial_{x} - 1\\,\\partial_{y} = 2e_1 - e_2$ and $v = 1\\,\\partial_{x} + 3\\,\\partial_{y} = e_1 + 3e_2$.\n\nFirst, we compute the wedge product $u \\wedge v$:\n$$\nu \\wedge v = (2e_1 - e_2) \\wedge (e_1 + 3e_2)\n$$\nUsing the distributive property of the wedge product and its anticommutativity ($e_1 \\wedge e_2 = -e_2 \\wedge e_1$) and the property that $e_i \\wedge e_i = 0$:\n$$\nu \\wedge v = 2(e_1 \\wedge e_1) + 6(e_1 \\wedge e_2) - (e_2 \\wedge e_1) - 3(e_2 \\wedge e_2)\n$$\n$$\nu \\wedge v = 0 + 6(e_1 \\wedge e_2) - (-e_1 \\wedge e_2) - 0 = (6+1)(e_1 \\wedge e_2) = 7(e_1 \\wedge e_2)\n$$\nNext, we need to find the norm of this bivector, $\\|u \\wedge v\\|_{\\wedge^2}$. We have:\n$$\n\\|u \\wedge v\\|_{\\wedge^2} = \\|7(e_1 \\wedge e_2)\\|_{\\wedge^2} = |7| \\cdot \\|e_1 \\wedge e_2\\|_{\\wedge^2} = 7 \\|e_1 \\wedge e_2\\|_{\\wedge^2}\n$$\nTo calculate $\\|e_1 \\wedge e_2\\|_{\\wedge^2}$, we compute its square using the definition of the induced inner product:\n$$\n\\|e_1 \\wedge e_2\\|_{\\wedge^2}^2 = \\langle e_1 \\wedge e_2, e_1 \\wedge e_2 \\rangle_{\\wedge^2} = \\det \\begin{pmatrix} g(e_1, e_1) & g(e_1, e_2) \\\\ g(e_2, e_1) & g(e_2, e_2) \\end{pmatrix} = \\det(G)\n$$\nThe determinant of the metric matrix $G$ is:\n$$\n\\det(G) = \\det \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix} = (4)(1) - (1)(1) = 3\n$$\nSo, $\\|e_1 \\wedge e_2\\|_{\\wedge^2}^2 = 3$, which implies $\\|e_1 \\wedge e_2\\|_{\\wedge^2} = \\sqrt{3}$. This quantity is the area of the parallelogram spanned by the basis vectors, also known as the volume element with respect to the metric $g$.\n\nNow we can compute the norm of $u \\wedge v$:\n$$\n\\|u \\wedge v\\|_{\\wedge^2} = 7 \\|e_1 \\wedge e_2\\|_{\\wedge^2} = 7\\sqrt{3}\n$$\nFinally, the area of the triangle determined by $u$ and $v$ is half of this norm:\n$$\nA_{\\text{tri}} = \\frac{1}{2} \\|u \\wedge v\\|_{\\wedge^2} = \\frac{1}{2} (7\\sqrt{3}) = \\frac{7\\sqrt{3}}{2}\n$$\nThe computed area is a scalar value, as expected.", "answer": "$$\\boxed{\\frac{7\\sqrt{3}}{2}}$$", "id": "3066952"}]}