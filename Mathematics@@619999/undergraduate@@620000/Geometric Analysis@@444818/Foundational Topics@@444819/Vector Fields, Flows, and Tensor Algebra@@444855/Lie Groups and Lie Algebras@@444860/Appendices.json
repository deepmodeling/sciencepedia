{"hands_on_practices": [{"introduction": "Understanding a Lie group often begins with its linear approximation, the Lie algebra. This first practice focuses on the foundational skill of identifying the elements that constitute a Lie algebra by applying its defining properties. By constructing a basis for $\\mathfrak{su}(2)$, the Lie algebra of the special unitary group $SU(2)$, you will gain a concrete understanding of the vector space structure that underpins the symmetries of quantum systems.[@problem_id:1678762]", "problem": "In quantum mechanics, the state of a spin-1/2 particle is described by a vector in a two-dimensional complex Hilbert space. The transformations on these states that preserve physical properties are represented by the Special Unitary group in two dimensions, $SU(2)$. The \"infinitesimal generators\" of these transformations form the Lie algebra of the group, denoted $\\mathfrak{su}(2)$.\n\nThe Lie algebra $\\mathfrak{su}(2)$ is defined as the set of all $2 \\times 2$ complex matrices $X$ that satisfy two specific conditions:\n1.  $X$ is skew-hermitian, meaning its conjugate transpose $X^\\dagger$ is equal to its negative, i.e., $X^\\dagger = -X$.\n2.  $X$ is traceless, meaning the sum of its diagonal elements is zero, i.e., $\\text{tr}(X) = 0$.\n\nThis set of matrices forms a vector space over the real numbers. Which of the following sets of matrices constitutes a basis for the real vector space $\\mathfrak{su}(2)$?\n\nA. $\\left\\{ \\begin{pmatrix} 0  i \\\\ i  0 \\end{pmatrix}, \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\begin{pmatrix} i  0 \\\\ 0  -i \\end{pmatrix} \\right\\}$\n\nB. $\\left\\{ \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}, \\begin{pmatrix} 0  -i \\\\ i  0 \\end{pmatrix}, \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\right\\}$\n\nC. $\\left\\{ \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\begin{pmatrix} i  0 \\\\ 0  -i \\end{pmatrix} \\right\\}$\n\nD. $\\left\\{ \\begin{pmatrix} 0  i \\\\ i  0 \\end{pmatrix}, \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\begin{pmatrix} 0  1+i \\\\ -1+i  0 \\end{pmatrix} \\right\\}$\n\nE. $\\left\\{ \\begin{pmatrix} i  0 \\\\ 0  i \\end{pmatrix}, \\begin{pmatrix} 0  i \\\\ -i  0 \\end{pmatrix}, \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\right\\}$", "solution": "To solve this problem, we first determine the general form of a matrix belonging to the Lie algebra $\\mathfrak{su}(2)$ and then check which of the given options provides a valid basis for the space of such matrices. A basis must consist of a set of linearly independent vectors (in this case, matrices) that span the entire vector space.\n\nLet's consider an arbitrary $2 \\times 2$ complex matrix $X$:\n$$\nX = \\begin{pmatrix} z_1  z_2 \\\\ z_3  z_4 \\end{pmatrix}\n$$\nwhere $z_1, z_2, z_3, z_4$ are complex numbers.\n\nFirst, we apply the traceless condition: $\\text{tr}(X) = 0$.\n$$\nz_1 + z_4 = 0 \\implies z_4 = -z_1\n$$\nSo, the matrix must have the form:\n$$\nX = \\begin{pmatrix} z_1  z_2 \\\\ z_3  -z_1 \\end{pmatrix}\n$$\n\nNext, we apply the skew-hermitian condition: $X^\\dagger = -X$. The conjugate transpose $X^\\dagger$ is:\n$$\nX^\\dagger = \\begin{pmatrix} \\overline{z_1}  \\overline{z_3} \\\\ \\overline{z_2}  \\overline{z_4} \\end{pmatrix} = \\begin{pmatrix} \\overline{z_1}  \\overline{z_3} \\\\ \\overline{z_2}  -\\overline{z_1} \\end{pmatrix}\n$$\nAnd $-X$ is:\n$$\n-X = \\begin{pmatrix} -z_1  -z_2 \\\\ -z_3  z_1 \\end{pmatrix}\n$$\nEquating the two matrices, $X^\\dagger = -X$, gives us a set of conditions on the elements:\n1. $\\overline{z_1} = -z_1$\n2. $\\overline{z_3} = -z_2$\n\nFrom condition (1), if we write $z_1 = a + ib$ where $a, b \\in \\mathbb{R}$, then $\\overline{z_1} = a - ib$. The condition becomes $a - ib = -(a+ib) = -a - ib$, which implies $2a = 0$, so $a=0$. This means $z_1$ must be purely imaginary. Let's write $z_1 = ic$ for some real number $c$.\n\nFrom condition (2), let's write $z_2 = x + iy$ and $z_3 = u + iv$ for real numbers $x, y, u, v$. The condition $\\overline{z_3} = -z_2$ becomes $u - iv = -(x+iy) = -x - iy$. Equating real and imaginary parts, we get $u = -x$ and $-v = -y \\implies v = y$.\nSo, $z_3 = -x + iy$.\nWe see that $z_3 = -(x-iy) = -\\overline{z_2}$. This is just a restatement of the condition.\n\nSo, the elements are constrained as:\n$z_1 = ic$\n$z_2 = x + iy$\n$z_3 = -x + iy$\n$z_4 = -z_1 = -ic$\nwhere $x, y, c$ are arbitrary real numbers.\n\nSubstituting these back into the matrix $X$:\n$$\nX = \\begin{pmatrix} ic  x+iy \\\\ -x+iy  -ic \\end{pmatrix}\n$$\nWe can decompose this general form into a linear combination over the real numbers $x, y, c$:\n$$\nX = x \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} + y \\begin{pmatrix} 0  i \\\\ i  0 \\end{pmatrix} + c \\begin{pmatrix} i  0 \\\\ 0  -i \\end{pmatrix}\n$$\nThis shows that any matrix in $\\mathfrak{su}(2)$ is a real linear combination of the three matrices:\n$$\nB_1 = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\quad B_2 = \\begin{pmatrix} 0  i \\\\ i  0 \\end{pmatrix}, \\quad B_3 = \\begin{pmatrix} i  0 \\\\ 0  -i \\end{pmatrix}\n$$\nThese three matrices are linearly independent over the real numbers. Therefore, the dimension of the real vector space $\\mathfrak{su}(2)$ is 3, and the set $\\{B_1, B_2, B_3\\}$ forms a basis.\n\nNow we evaluate the given options:\n\nA. The set is $\\left\\{ \\begin{pmatrix} 0  i \\\\ i  0 \\end{pmatrix}, \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\begin{pmatrix} i  0 \\\\ 0  -i \\end{pmatrix} \\right\\}$. This is exactly the set $\\{B_2, B_1, B_3\\}$ we derived. Since the order of elements in a set does not matter, this is a valid basis for $\\mathfrak{su}(2)$.\n\nB. The set is $\\left\\{ M_1, M_2, M_3 \\right\\} = \\left\\{ \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}, \\begin{pmatrix} 0  -i \\\\ i  0 \\end{pmatrix}, \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\right\\}$. These are the Pauli matrices. Let's check $M_1$. $M_1^\\dagger = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = M_1$. Since $M_1^\\dagger \\neq -M_1$, it is not skew-hermitian. Thus, this set is not a basis for $\\mathfrak{su}(2)$. (These matrices form a basis for the space of $2 \\times 2$ traceless *hermitian* matrices).\n\nC. The first matrix is $M_1 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$. Its trace is $\\text{tr}(M_1) = 1+1=2 \\neq 0$. It is not in $\\mathfrak{su}(2)$. Thus, this set cannot be a basis.\n\nD. The set is $\\left\\{ M_1, M_2, M_3 \\right\\} = \\left\\{ \\begin{pmatrix} 0  i \\\\ i  0 \\end{pmatrix}, \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\begin{pmatrix} 0  1+i \\\\ -1+i  0 \\end{pmatrix} \\right\\}$. We can see that $M_3 = M_2 + M_1$. Since one matrix is a linear combination of the others, the set is linearly dependent over the real numbers and cannot form a basis.\n\nE. The first matrix is $M_1 = \\begin{pmatrix} i  0 \\\\ 0  i \\end{pmatrix}$. Its trace is $\\text{tr}(M_1) = i+i=2i \\neq 0$. It is not in $\\mathfrak{su}(2)$. Thus, this set cannot be a basis.\n\nBased on our analysis, only the set in Option A satisfies all the properties required for a basis of $\\mathfrak{su}(2)$.", "answer": "$$\\boxed{A}$$", "id": "1678762"}, {"introduction": "A Lie algebra is more than just a vector space; it possesses a rich algebraic structure defined by the Lie bracket. This operation captures the essence of the group's non-commutativity in an infinitesimal form through coefficients known as structure constants. This exercise provides direct practice in computing the Lie bracket for the algebra $\\mathfrak{sl}(2, \\mathbb{R})$, allowing you to calculate the structure constants that define the algebra's fundamental commutation relations.[@problem_id:1678779]", "problem": "The Special Linear Group in two dimensions, denoted $SL(2, \\mathbb{R})$, is the set of all $2 \\times 2$ real matrices with a determinant of 1. Its associated Lie algebra, denoted $\\mathfrak{sl}(2, \\mathbb{R})$, is the vector space of all $2 \\times 2$ real matrices with a trace of zero. The Lie bracket operation on this algebra is defined by the commutator: $[A, B] = AB - BA$ for any two matrices $A, B \\in \\mathfrak{sl}(2, \\mathbb{R})$.\n\nConsider the following set of matrices, which forms a basis for $\\mathfrak{sl}(2, \\mathbb{R})$:\n$$A_1 = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}, \\quad A_3 = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}$$\nThe Lie bracket of any two basis elements is itself an element of the algebra and can therefore be expressed as a linear combination of the basis elements. For the specific bracket $[A_1, A_2]$, we can write:\n$$[A_1, A_2] = c_{12}^1 A_1 + c_{12}^2 A_2 + c_{12}^3 A_3$$\nwhere the coefficients $c_{12}^k$ are real numbers known as structure constants.\n\nDetermine the values of the coefficients in the triplet $(c_{12}^1, c_{12}^2, c_{12}^3)$. Your final answer should be presented as a row matrix with three entries.", "solution": "We use the Lie bracket definition $[A_{1},A_{2}] = A_{1}A_{2} - A_{2}A_{1}$ and compute each product explicitly.\n\nFirst, compute $A_{1}A_{2}$:\n$$\nA_{1}A_{2} =\n\\begin{pmatrix}\n0  1 \\\\\n-1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1 \\\\\n1  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\cdot 0 + 1\\cdot 1  0\\cdot 1 + 1\\cdot 0 \\\\\n-1\\cdot 0 + 0\\cdot 1  -1\\cdot 1 + 0\\cdot 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  0 \\\\\n0  -1\n\\end{pmatrix}\n= A_{3}.\n$$\n\nNext, compute $A_{2}A_{1}$:\n$$\nA_{2}A_{1} =\n\\begin{pmatrix}\n0  1 \\\\\n1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1 \\\\\n-1  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\cdot 0 + 1\\cdot (-1)  0\\cdot 1 + 1\\cdot 0 \\\\\n1\\cdot 0 + 0\\cdot (-1)  1\\cdot 1 + 0\\cdot 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1  0 \\\\\n0  1\n\\end{pmatrix}\n= -A_{3}.\n$$\n\nTherefore,\n$$\n[A_{1},A_{2}] = A_{1}A_{2} - A_{2}A_{1} = A_{3} - (-A_{3}) = 2A_{3}.\n$$\n\nExpressing $[A_{1},A_{2}]$ as a linear combination of the basis $\\{A_{1},A_{2},A_{3}\\}$, we have\n$$\n[A_{1},A_{2}] = 0\\cdot A_{1} + 0\\cdot A_{2} + 2\\cdot A_{3}.\n$$\nThus the structure constants are $(c_{12}^{1}, c_{12}^{2}, c_{12}^{3}) = (0, 0, 2)$.", "answer": "$$\\boxed{\\begin{pmatrix} 0  0  2 \\end{pmatrix}}$$", "id": "1678779"}, {"introduction": "The crucial link between a Lie algebra and its Lie group is the exponential map, which allows us to recover the group's structure from its infinitesimal generators. This practice illustrates this fundamental connection by tasking you with computing a one-parameter subgroup of $SU(2)$ from an element of its algebra $\\mathfrak{su}(2)$. By calculating the matrix exponential, you will see exactly how an algebra element generates a continuous path of transformations within the group.[@problem_id:1523094]", "problem": "In the study of continuous symmetries, the Special Unitary group of degree 2, denoted $SU(2)$, is of paramount importance. It is the group of all $2 \\times 2$ unitary matrices with a determinant of 1. Its corresponding Lie algebra, denoted $\\mathfrak{su}(2)$, is the space of $2 \\times 2$ traceless and anti-hermitian matrices. Each element of the Lie algebra acts as an \"infinitesimal generator\" for a path within the Lie group. This path is known as a one-parameter subgroup.\n\nConsider the Lie algebra element $X$ from $\\mathfrak{su}(2)$ given by:\n$$\nX = \\frac{i}{2} \\sigma_x\n$$\nwhere $i$ is the imaginary unit and $\\sigma_x$ is the first Pauli matrix, defined as:\n$$\n\\sigma_x = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\n$$\nFind the matrix representation of the one-parameter subgroup $\\gamma(t)$ generated by the element $X$ for a real parameter $t$. Express your answer as a $2 \\times 2$ matrix with entries as functions of $t$.", "solution": "The one-parameter subgroup $\\gamma(t)$ generated by a Lie algebra element $X$ is found by computing the matrix exponential, $\\gamma(t) = \\exp(tX)$.\nGiven the generator $X = \\frac{i}{2}\\sigma_x$, the matrix we need to exponentiate is $tX$. Let's call this matrix $A$ for simplicity.\n$$\nA = tX = t \\left(\\frac{i}{2}\\sigma_x\\right) = \\frac{it}{2} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\n$$\nTo compute the matrix exponential $\\exp(A)$, we use its Taylor series definition:\n$$\n\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!} = I + A + \\frac{A^{2}}{2!} + \\frac{A^{3}}{3!} + \\dots\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. We first calculate the powers of $A$. Let's start with $A^2$:\n$$\nA^2 = \\left(\\frac{it}{2}\\right)^2 \\sigma_x^2 = \\frac{i^2 t^2}{4} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = -\\frac{t^2}{4} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = -\\left(\\frac{t}{2}\\right)^2 I\n$$\nThis result simplifies the computation of higher powers. We can establish a general pattern for even and odd powers of $A$:\nFor even powers, $k=2n$ (where $n=0, 1, 2, \\dots$):\n$$\nA^{2n} = (A^2)^n = \\left(-\\left(\\frac{t}{2}\\right)^2 I\\right)^n = (-1)^n \\left(\\frac{t}{2}\\right)^{2n} I\n$$\nFor odd powers, $k=2n+1$:\n$$\nA^{2n+1} = A \\cdot A^{2n} = A \\cdot (-1)^n \\left(\\frac{t}{2}\\right)^{2n}\n$$\nNow we can substitute these into the Taylor series by splitting the sum into even and odd terms:\n$$\n\\exp(A) = \\sum_{n=0}^{\\infty} \\frac{A^{2n}}{(2n)!} + \\sum_{n=0}^{\\infty} \\frac{A^{2n+1}}{(2n+1)!}\n$$\nSubstituting our expressions for the powers of $A$:\n$$\n\\exp(A) = \\sum_{n=0}^{\\infty} \\frac{(-1)^n (t/2)^{2n}}{(2n)!} I + \\sum_{n=0}^{\\infty} \\frac{(-1)^n (t/2)^{2n}}{(2n+1)!} A\n$$\nThe coefficient of the identity matrix $I$ is the Taylor series for $\\cos(u)$ with $u=t/2$:\n$$\n\\sum_{n=0}^{\\infty} \\frac{(-1)^n (t/2)^{2n}}{(2n)!} = \\cos\\left(\\frac{t}{2}\\right)\n$$\nFor the second term's coefficient, we notice its relation to the Taylor series for $\\sin(u)$:\n$$\n\\sin(u) = \\sum_{n=0}^{\\infty} \\frac{(-1)^n u^{2n+1}}{(2n+1)!} = u \\sum_{n=0}^{\\infty} \\frac{(-1)^n u^{2n}}{(2n+1)!}\n$$\nWith $u = t/2$, this means:\n$$\n\\sum_{n=0}^{\\infty} \\frac{(-1)^n (t/2)^{2n}}{(2n+1)!} = \\frac{\\sin(t/2)}{t/2}\n$$\nSo the second term in the expansion of $\\exp(A)$ is:\n$$\n\\left(\\frac{\\sin(t/2)}{t/2}\\right) A = \\left(\\frac{\\sin(t/2)}{t/2}\\right) \\left(\\frac{it}{2}\\sigma_x\\right) = i \\sin\\left(\\frac{t}{2}\\right) \\sigma_x\n$$\nCombining both parts, we arrive at the Euler-like formula for the matrix exponential:\n$$\n\\gamma(t) = \\exp(A) = \\cos\\left(\\frac{t}{2}\\right) I + i \\sin\\left(\\frac{t}{2}\\right) \\sigma_x\n$$\nTo get the final matrix, we substitute the matrix forms for $I$ and $\\sigma_x$:\n$$\n\\gamma(t) = \\cos\\left(\\frac{t}{2}\\right) \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + i \\sin\\left(\\frac{t}{2}\\right) \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\n$$\n$$\n\\gamma(t) = \\begin{pmatrix} \\cos\\left(\\frac{t}{2}\\right)  0 \\\\ 0  \\cos\\left(\\frac{t}{2}\\right) \\end{pmatrix} + \\begin{pmatrix} 0  i\\sin\\left(\\frac{t}{2}\\right) \\\\ i\\sin\\left(\\frac{t}{2}\\right)  0 \\end{pmatrix}\n$$\nAdding the matrices gives the final result for the one-parameter subgroup:\n$$\n\\gamma(t) = \\begin{pmatrix} \\cos\\left(\\frac{t}{2}\\right)  i\\sin\\left(\\frac{t}{2}\\right) \\\\ i\\sin\\left(\\frac{t}{2}\\right)  \\cos\\left(\\frac{t}{2}\\right) \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\cos\\left(\\frac{t}{2}\\right)  i\\sin\\left(\\frac{t}{2}\\right) \\\\ i\\sin\\left(\\frac{t}{2}\\right)  \\cos\\left(\\frac{t}{2}\\right) \\end{pmatrix}}\n$$", "id": "1523094"}]}