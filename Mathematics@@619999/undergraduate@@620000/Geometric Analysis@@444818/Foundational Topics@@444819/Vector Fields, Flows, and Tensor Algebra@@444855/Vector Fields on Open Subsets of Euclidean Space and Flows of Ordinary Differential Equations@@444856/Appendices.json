{"hands_on_practices": [{"introduction": "Linear systems provide the foundation for understanding dynamical systems, as their behavior is exceptionally regular and predictable. This exercise invites you to explore the flow of a general linear system, $\\dot{x} = Ax$. By deriving the Jacobian of the flow from first principles, you will uncover the fundamental result that it is given by the matrix exponential $\\exp(tA)$, providing a deep connection between the system's dynamics and its generating matrix [@problem_id:3078144].", "problem": "Let $n \\in \\mathbb{N}$ and let $A \\in \\mathbb{R}^{n \\times n}$ be a fixed constant matrix. Consider the smooth vector field $X : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ given by $X(x) = A x$ on the open subset $\\mathbb{R}^{n} \\subset \\mathbb{R}^{n}$. Let $\\varphi_{t} : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ denote the flow of the ordinary differential equation (ODE) $\\dot{x} = X(x)$, characterized by the properties $\\frac{d}{dt}\\varphi_{t}(x) = X(\\varphi_{t}(x))$ and $\\varphi_{0}(x) = x$ for all $x \\in \\mathbb{R}^{n}$ and all $t \\in \\mathbb{R}$ for which the solution exists. Using only the fundamental definitions of flow, Jacobian (derivative with respect to initial condition), and the characterization of the matrix exponential via its power series, derive the linear variational equation governing the Jacobian $D\\varphi_{t}(x)$ and solve it explicitly. Prove from first principles that $D\\varphi_{t}(x)$ does not depend on $x$. Express your final result in a single closed-form analytic expression in terms of $A$ and $t$. No rounding is required, and no units are involved.", "solution": "The task is to derive the equation governing the Jacobian of the flow of a linear ODE, demonstrate its independence from the initial condition, and solve it explicitly using first principles.\n\nLet the flow be denoted by $\\varphi_t(x)$. By definition, it satisfies the initial value problem:\n$$\n\\frac{d}{dt}\\varphi_{t}(x) = X(\\varphi_{t}(x)), \\quad \\varphi_{0}(x) = x\n$$\nHere, the vector field is $X(x) = Ax$. Thus, the equation for the flow is:\n$$\n\\frac{d}{dt}\\varphi_{t}(x) = A\\varphi_{t}(x)\n$$\nThe Jacobian of the flow, denoted by $J(t, x) = D\\varphi_{t}(x)$, is the matrix of partial derivatives of the components of $\\varphi_t(x)$ with respect to the components of the initial condition $x$. That is, $(J(t, x))_{ij} = \\frac{\\partial (\\varphi_t)_i}{\\partial x_j}(x)$.\n\nTo find the differential equation governing $J(t, x)$, we differentiate the flow's ODE with respect to $x$. Since the flow of an autonomous ODE is smooth with respect to both $t$ and $x$, we can interchange the order of differentiation (by Schwarz's theorem on the equality of mixed partials):\n$$\n\\frac{d}{dt} (D\\varphi_{t}(x)) = D\\left(\\frac{d}{dt}\\varphi_{t}(x)\\right)\n$$\nSubstituting the definition of the flow:\n$$\n\\frac{d}{dt} J(t, x) = D(X(\\varphi_{t}(x)))\n$$\nWe apply the chain rule to the right-hand side. The derivative is with respect to the variable $x$:\n$$\nD(X(\\varphi_{t}(x))) = (DX)(\\varphi_{t}(x)) \\cdot D\\varphi_{t}(x)\n$$\nwhere $(DX)(y)$ is the Jacobian of the vector field $X$ evaluated at a point $y$.\nThus, the general linear variational equation is:\n$$\n\\frac{d}{dt} J(t, x) = (DX)(\\varphi_{t}(x)) \\cdot J(t, x)\n$$\nFor the specific vector field given, $X(x) = Ax$, we compute its Jacobian. Let $X_i(x) = \\sum_{k=1}^n A_{ik} x_k$. The partial derivative with respect to $x_j$ is:\n$$\n\\frac{\\partial X_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^n A_{ik} x_k \\right) = A_{ik} \\delta_{kj} = A_{ij}\n$$\nwhere $\\delta_{kj}$ is the Kronecker delta. Therefore, the Jacobian matrix $DX$ is the constant matrix $A$ itself, i.e., $DX(y) = A$ for all $y \\in \\mathbb{R}^n$.\n\nSubstituting this result into the variational equation, we find:\n$$\n\\frac{d}{dt} J(t, x) = A \\cdot J(t, x)\n$$\nThis is the linear variational equation for the given system.\n\nNext, we determine the initial condition for $J(t, x)$. The flow at time $t=0$ is given by $\\varphi_{0}(x) = x$. We take the Jacobian with respect to $x$:\n$$\nJ(0, x) = D\\varphi_{0}(x) = D(x) = I\n$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nThe Jacobian $J(t, x) = D\\varphi_t(x)$ is therefore the solution to the matrix initial value problem (IVP):\n$$\n\\frac{dJ}{dt} = AJ, \\quad J(0) = I\n$$\nThis IVP for $J$ is a system of linear homogeneous ODEs with constant coefficients, and the initial condition is a constant matrix. Crucially, neither the differential equation nor the initial condition depends on the variable $x$ or the state $\\varphi_t(x)$. By the theory of ODEs, this IVP has a unique solution. Since the formulation of the IVP is independent of $x$, its unique solution must also be independent of $x$. This proves from first principles that $D\\varphi_t(x)$ does not depend on $x$. We may thus write $J(t)$ instead of $J(t, x)$.\n\nFinally, we solve this IVP using the power series definition of the matrix exponential. The matrix exponential is defined as:\n$$\n\\exp(tA) = \\sum_{k=0}^{\\infty} \\frac{(tA)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k\n$$\nWe propose the solution $J(t) = \\exp(tA)$ and verify that it satisfies the IVP.\n\nFirst, we check the initial condition at $t=0$:\n$$\nJ(0) = \\exp(0 \\cdot A) = \\sum_{k=0}^{\\infty} \\frac{0^k}{k!} A^k\n$$\nFor $k=0$, the term is $\\frac{0^0}{0!}A^0 = \\frac{1}{1}I = I$. For all $k \\ge 1$, the term $\\frac{0^k}{k!}A^k$ is the zero matrix. Thus, $J(0) = I$, which matches the initial condition.\n\nSecond, we check the differential equation. We differentiate the power series term by term with respect to $t$. This is permissible as the series has an infinite radius of convergence for any matrix $A$.\n$$\n\\frac{dJ}{dt} = \\frac{d}{dt} \\exp(tA) = \\frac{d}{dt} \\left( \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k \\right)\n$$\n$$\n= \\sum_{k=0}^{\\infty} \\frac{d}{dt}\\left(\\frac{t^k}{k!}\\right) A^k = \\sum_{k=1}^{\\infty} \\frac{k t^{k-1}}{k!} A^k\n$$\nThe summation starts from $k=1$ because the $k=0$ term is the constant matrix $I$, whose derivative is the zero matrix.\n$$\n\\frac{dJ}{dt} = \\sum_{k=1}^{\\infty} \\frac{t^{k-1}}{(k-1)!} A^k = A \\left( \\sum_{k=1}^{\\infty} \\frac{t^{k-1}}{(k-1)!} A^{k-1} \\right)\n$$\nLet's define a new index $j = k-1$. As $k$ goes from $1$ to $\\infty$, $j$ goes from $0$ to $\\infty$.\n$$\n\\frac{dJ}{dt} = A \\left( \\sum_{j=0}^{\\infty} \\frac{t^j}{j!} A^j \\right) = A \\exp(tA)\n$$\nSince we defined $J(t) = \\exp(tA)$, we have shown that $\\frac{dJ}{dt} = AJ$.\nThe function $J(t) = \\exp(tA)$ satisfies both the differential equation and the initial condition. By the existence and uniqueness theorem for linear ODEs, this is the unique solution.\n\nTherefore, the Jacobian of the flow, $D\\varphi_t(x)$, is given by the matrix exponential $\\exp(tA)$. This expression is a closed-form analytic solution in terms of $A$ and $t$.", "answer": "$$\\boxed{\\exp(tA)}$$", "id": "3078144"}, {"introduction": "In contrast to the globally well-behaved nature of linear systems, nonlinear dynamics can exhibit surprising behavior. This practice problem explores the concept of finite-time blowup using the simple, smooth vector field $X(x) = x^{2}+1$. By explicitly solving the trajectory and determining its maximal interval of existence, you will see firsthand how solutions to a well-defined ODE can escape to infinity in a finite amount of time [@problem_id:3078146].", "problem": "Consider the smooth autonomous vector field $X : \\mathbb{R} \\to \\mathbb{R}$ on the open subset $\\mathbb{R} \\subset \\mathbb{R}$ given by $X(x) = x^{2} + 1$. Let a trajectory be a maximal integral curve $t \\mapsto x(t)$ solving the ordinary differential equation $\\dot{x}(t) = X(x(t))$ with an initial condition $x(0) = a \\in \\mathbb{R}$. Using only fundamental principles of ordinary differential equations on Euclidean space—namely, existence and uniqueness for locally Lipschitz vector fields, separation of variables, and basic integration—derive the explicit form of the trajectory and justify from first principles that it blows up in finite time in both forward and backward directions. Then determine the maximal open interval of existence as a function of the initial condition $a$. Express any angle in radians. Your final reported quantity must be the ordered pair of endpoints of this maximal interval as a closed-form analytic expression in terms of $a$.", "solution": "The problem is to solve the initial value problem (IVP) given by the differential equation:\n$$ \\frac{dx}{dt} = x^{2} + 1 $$\nwith the initial condition $x(0) = a$.\n\nFirst, we derive the explicit form of the trajectory $x(t)$. The equation is a first-order autonomous ODE and is separable. We note that the right-hand side, $x^2 + 1$, is strictly positive for all real values of $x$. This implies that there are no equilibrium points, and any solution $x(t)$ must be strictly monotonic. Since $\\dot{x} = x^2 + 1 > 0$, the solution $x(t)$ is strictly increasing for all $t$ in its domain of existence.\n\nWe can separate the variables $x$ and $t$:\n$$ \\frac{dx}{x^{2} + 1} = dt $$\nTo find the particular solution satisfying the initial condition $x(0) = a$, we integrate both sides from the initial state $(t,x) = (0,a)$ to a general state $(t, x(t))$:\n$$ \\int_{a}^{x(t)} \\frac{d\\xi}{\\xi^{2} + 1} = \\int_{0}^{t} d\\tau $$\nThe integral on the left is the standard form for the arctangent function, and the integral on the right is trivial. Performing the integration yields:\n$$ \\left[ \\arctan(\\xi) \\right]_{a}^{x(t)} = \\left[ \\tau \\right]_{0}^{t} $$\n$$ \\arctan(x(t)) - \\arctan(a) = t - 0 $$\nRearranging for $\\arctan(x(t))$ gives the implicit solution:\n$$ \\arctan(x(t)) = t + \\arctan(a) $$\nTo obtain the explicit form of the trajectory $x(t)$, we take the tangent of both sides of the equation. Note that all angles are in radians.\n$$ x(t) = \\tan(t + \\arctan(a)) $$\nThis is the explicit solution to the IVP. We can verify the initial condition: $x(0) = \\tan(0 + \\arctan(a)) = \\tan(\\arctan(a)) = a$.\n\nNext, we determine the maximal open interval of existence and justify the finite-time blowup from first principles. The explicit solution $x(t) = \\tan(t + \\arctan(a))$ involves the tangent function. The tangent function $\\tan(\\theta)$ is defined and continuous for arguments $\\theta$ that are not of the form $\\frac{\\pi}{2} + k\\pi$ for any integer $k$. A solution to an ODE must be continuous. The trajectory $x(t)$ starting at $t=0$ must therefore exist on a single continuous branch of the tangent function.\n\nAt the initial time $t=0$, the argument of the tangent function is $\\theta_0 = 0 + \\arctan(a) = \\arctan(a)$. The principal value of the arctangent function has a range of $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$. Thus, the initial argument $\\arctan(a)$ lies strictly within this interval.\n$$ -\\frac{\\pi}{2} < \\arctan(a) < \\frac{\\pi}{2} $$\nFor the solution $x(t)$ to be continuous, its argument, $t + \\arctan(a)$, must remain within this same open interval, $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$. This provides the following inequality for the time variable $t$:\n$$ -\\frac{\\pi}{2} < t + \\arctan(a) < \\frac{\\pi}{2} $$\nSolving this inequality for $t$ gives the bounds on its domain:\n$$ -\\frac{\\pi}{2} - \\arctan(a) < t < \\frac{\\pi}{2} - \\arctan(a) $$\nThis defines the maximal open interval of existence for the solution $x(t)$, which we denote as $I_{max}$:\n$$ I_{max} = \\left( -\\frac{\\pi}{2} - \\arctan(a), \\frac{\\pi}{2} - \\arctan(a) \\right) $$\nLet the endpoints of this interval be $t_{min} = -\\frac{\\pi}{2} - \\arctan(a)$ and $t_{max} = \\frac{\\pi}{2} - \\arctan(a)$.\n\nNow we justify, \"from first principles,\" that the solution blows up in finite time in both forward and backward directions. This justification stems directly from the behavior of the derived explicit solution at the endpoints of its maximal interval of existence.\n\nFor forward time, we examine the limit of $x(t)$ as $t$ approaches $t_{max}$ from below:\n$$ \\lim_{t \\to t_{max}^{-}} x(t) = \\lim_{t \\to \\left(\\frac{\\pi}{2} - \\arctan(a)\\right)^{-}} \\tan(t + \\arctan(a)) $$\nAs $t \\to (\\frac{\\pi}{2} - \\arctan(a))^{-}$, the argument of the tangent function, $t + \\arctan(a)$, approaches $\\frac{\\pi}{2}$ from the left side. The limit of the tangent function as its argument approaches $\\frac{\\pi}{2}$ from the left is $+\\infty$.\n$$ \\lim_{t \\to t_{max}^{-}} x(t) = +\\infty $$\nThis shows that the solution \"blows up\" (escapes to positive infinity) at the finite forward time $t_{max}$.\n\nFor backward time, we examine the limit of $x(t)$ as $t$ approaches $t_{min}$ from above:\n$$ \\lim_{t \\to t_{min}^{+}} x(t) = \\lim_{t \\to \\left(-\\frac{\\pi}{2} - \\arctan(a)\\right)^{+}} \\tan(t + \\arctan(a)) $$\nAs $t \\to (-\\frac{\\pi}{2} - \\arctan(a))^{+}$, the argument of the tangent function, $t + \\arctan(a)$, approaches $-\\frac{\\pi}{2}$ from the right side. The limit of the tangent function as its argument approaches $-\\frac{\\pi}{2}$ from the right is $-\\infty$.\n$$ \\lim_{t \\to t_{min}^{+}} x(t) = -\\infty $$\nThis shows that the solution \"blows up\" (escapes to negative infinity) at the finite backward time $t_{min}$.\n\nAccording to the theory of ODEs, a solution defined on a maximal interval $(t_{min}, t_{max})$ must have the property that the trajectory leaves any compact subset of its domain as $t \\to t_{min}$ or $t \\to t_{max}$. Since the domain is $\\mathbb{R}$, this means that if $t_{max}$ is finite, $|x(t)| \\to \\infty$ as $t \\to t_{max}$, and similarly for $t_{min}$. Our explicit calculation confirms this behavior. The solution cannot be extended beyond this interval because it becomes undefined at the endpoints. Thus, the interval we found is indeed the maximal interval of existence.\n\nThe problem asks for the ordered pair of endpoints of this maximal interval. The endpoints are $t_{min} = -\\frac{\\pi}{2} - \\arctan(a)$ and $t_{max} = \\frac{\\pi}{2} - \\arctan(a)$. The ordered pair is $(t_{min}, t_{max})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{\\pi}{2} - \\arctan(a) & \\frac{\\pi}{2} - \\arctan(a) \\end{pmatrix}}\n$$", "id": "3078146"}, {"introduction": "Our exploration culminates by examining what happens when a vector field is not sufficiently regular, leading to a breakdown in the uniqueness of solutions. This exercise focuses on a classic example, $X(x) = |x|^{\\alpha}$ with $\\alpha \\in (0,1)$, which is continuous but not Lipschitz at the origin. You will analyze why the crucial Flow-Box Theorem fails in this case and quantify the strange behavior by calculating the finite time it takes for a trajectory to escape the singular point, highlighting the importance of the hypotheses in our fundamental theorems [@problem_id:3078150].", "problem": "Let $U \\subset \\mathbb{R}$ be an open interval containing $0$, and let $X : U \\to \\mathbb{R}$ be the continuous vector field defined by $X(x) = |x|^{\\alpha}$ with fixed parameter $\\alpha \\in (0,1)$. Recall that a flow of an ordinary differential equation (ODE) is a map $\\Phi : I \\times U \\to U$ satisfying $\\frac{d}{dt}\\Phi(t,x) = X(\\Phi(t,x))$ and $\\Phi(0,x) = x$, where $I \\subset \\mathbb{R}$ is an interval of time, and that the Flow-Box Theorem (rectification theorem) asserts that if $X(p) \\neq 0$ and $X$ is continuously differentiable, then there exists a local coordinate change near $p$ in which $X$ becomes a constant nonzero vector field. Starting from these fundamental facts and the definition of the flow, consider the behavior near the zero $x=0$ of $X$.\n\nYour tasks are:\n- Provide this explicit example and explain why the flow-box conclusion fails at the zero $x=0$ for $X(x) = |x|^{\\alpha}$ with $\\alpha \\in (0,1)$, based on the core definitions of flows and the standard existence-uniqueness theory (Picard–Lindelöf theorem).\n- As a quantitative measure of the obstruction caused by the vanishing of $X$ and the failure of a local rectification map, compute the arrival-time integral from $0$ to $x_{0} > 0$,\n$$\nT(x_{0}) = \\int_{0}^{x_{0}} \\frac{dx}{X(x)} \\, ,\n$$\nand express your final result as an exact closed-form analytic expression in terms of $x_{0}$ and $\\alpha$.\n\nYour final answer must be a single closed-form analytic expression. No rounding is required, and no units apply.", "solution": "The problem asks for two tasks related to the vector field $X(x) = |x|^{\\alpha}$ on an open interval $U \\subset \\mathbb{R}$ containing $0$, where $\\alpha$ is a fixed parameter in the interval $(0,1)$.\n\nFirst, we will explain why the conclusion of the Flow-Box Theorem (rectification theorem) fails at the point $x=0$. The theorem states that for a continuously differentiable vector field $X$ on an open set, if $X(p) \\neq 0$ for some point $p$, then there exists a local coordinate system in a neighborhood of $p$ where the vector field is constant. For the given vector field $X(x) = |x|^{\\alpha}$, the point of interest is $x=0$, where $X(0) = |0|^{\\alpha} = 0$. A primary hypothesis of the theorem, $X(p) \\neq 0$, is violated at $p=0$. This is the immediate reason the theorem does not apply.\n\nHowever, a deeper analysis reveals the fundamental obstruction. Let us assume, for the sake of contradiction, that a rectification map exists near $x=0$. This would mean there is a local diffeomorphism $\\psi$ defined on a neighborhood of $0$ such that the pushforward vector field $\\psi_*X$ is constant. Since $X(0)=0$, the transformed vector field must be zero at the transformed point $\\psi(0)$, i.e., $(\\psi_*X)(\\psi(0))=0$. A constant vector field which is zero at one point must be the zero vector field everywhere. This implies that in the new coordinates, all points are fixed points. Transforming back to the original coordinates, this would mean that all points in a neighborhood of $x=0$ are fixed points for the flow of $X$. This is a contradiction, as $X(x)=|x|^{\\alpha} \\neq 0$ for any $x \\neq 0$. Thus, no such local rectification is possible.\n\nThe core reason for this failure resides in the non-uniqueness of solutions to the ordinary differential equation (ODE) $\\frac{dx}{dt} = X(x)$ with the initial condition $x(0)=0$. The Picard-Lindelöf theorem guarantees the existence and uniqueness of solutions if the vector field is, among other conditions, locally Lipschitz continuous. Let's analyze the Lipschitz property of $X(x)=|x|^{\\alpha}$ near $x=0$. A function $f(x)$ is locally Lipschitz at $x=0$ if there exists a constant $L>0$ and a neighborhood of $0$ such that $|f(x) - f(0)| \\leq L|x-0|$ for all $x$ in that neighborhood.\nFor $X(x)$, this condition becomes:\n$$ | |x|^{\\alpha} - |0|^{\\alpha} | \\leq L |x-0| $$\n$$ |x|^{\\alpha} \\leq L |x| $$\nFor $x \\neq 0$, we can divide by $|x|$ to get:\n$$ |x|^{\\alpha-1} \\leq L $$\nSince the parameter $\\alpha$ is in the interval $(0,1)$, the exponent $\\alpha-1$ is in $(-1,0)$. As $x$ approaches $0$, $|x|^{\\alpha-1}$ approaches infinity. For example, let $\\beta = 1-\\alpha \\in (0,1)$. The condition is $\\frac{1}{|x|^{\\beta}} \\leq L$. It is impossible to find a constant $L$ that bounds this quantity in any neighborhood of $0$. Therefore, $X(x)=|x|^{\\alpha}$ is not locally Lipschitz continuous at $x=0$, and the uniqueness of solutions guaranteed by the Picard-Lindelöf theorem does not apply.\n\nWe can demonstrate this non-uniqueness explicitly. One solution to the initial value problem $\\frac{dx}{dt} = |x|^{\\alpha}$ with $x(0)=0$ is the trivial solution $x_1(t) = 0$ for all time $t$. This represents a particle remaining at the fixed point.\nTo find other solutions, let's consider $x>0$, so the ODE is $\\frac{dx}{dt} = x^{\\alpha}$. We solve this by separation of variables:\n$$ x^{-\\alpha} dx = dt $$\nIntegrating from an initial state $(x_i, t_i)=(0, t_0)$ to a final state $(x,t)$, we get:\n$$ \\int_{0}^{x} \\xi^{-\\alpha} d\\xi = \\int_{t_0}^{t} d\\tau $$\n$$ \\left[ \\frac{\\xi^{1-\\alpha}}{1-\\alpha} \\right]_{0}^{x} = t - t_0 $$\nSince $1-\\alpha > 0$, the evaluation at the lower bound $\\xi=0$ yields $0$. So we have:\n$$ \\frac{x^{1-\\alpha}}{1-\\alpha} = t - t_0 $$\n$$ x(t) = \\left( (1-\\alpha)(t-t_0) \\right)^{\\frac{1}{1-\\alpha}} $$\nThis formula is valid for $t \\geq t_0$. We can now construct an infinite family of solutions to the initial value problem $x(0)=0$ by choosing any \"escape time\" $t_0 \\geq 0$:\n$$ x_{t_0}(t) = \\begin{cases} 0 & \\text{if } t \\leq t_0 \\\\ \\left( (1-\\alpha)(t-t_0) \\right)^{\\frac{1}{1-\\alpha}} & \\text{if } t > t_0 \\end{cases} $$\nOne can verify that this function is continuous and differentiable everywhere, including at $t=t_0$ where the derivative is $0$, and satisfies the ODE for all $t \\in \\mathbb{R}$. Since we have infinitely many distinct solutions passing through the point $(t,x)=(0,0)$, there is no unique integral curve. The concept of a flow map $\\Phi(t,x_0)$, which by definition must assign a unique point for each $(t,x_0)$, breaks down at $x_0=0$. The failure to rectify the vector field at $x=0$ is a direct consequence of this fundamental breakdown of uniqueness.\n\nSecond, we compute the arrival-time integral $T(x_0) = \\int_{0}^{x_0} \\frac{dx}{X(x)}$ for $x_0 > 0$.\nThe vector field for $x>0$ is $X(x)=x^{\\alpha}$. The integral is therefore:\n$$ T(x_{0}) = \\int_{0}^{x_{0}} \\frac{dx}{x^{\\alpha}} = \\int_{0}^{x_{0}} x^{-\\alpha} dx $$\nThis is an improper integral due to the singularity of the integrand at the lower limit $x=0$. We check for convergence. Since $\\alpha \\in (0,1)$, the exponent $p=-\\alpha$ is in the interval $(-1,0)$. The integral $\\int_0^a x^p dx$ is known to converge if and only if $p > -1$. This condition is satisfied, so the integral converges. We can evaluate it as a limit:\n$$ T(x_0) = \\lim_{\\epsilon \\to 0^+} \\int_{\\epsilon}^{x_0} x^{-\\alpha} dx $$\nThe antiderivative of $x^{-\\alpha}$ is $\\frac{x^{-\\alpha+1}}{-\\alpha+1} = \\frac{x^{1-\\alpha}}{1-\\alpha}$.\n$$ T(x_0) = \\lim_{\\epsilon \\to 0^+} \\left[ \\frac{x^{1-\\alpha}}{1-\\alpha} \\right]_{\\epsilon}^{x_0} = \\lim_{\\epsilon \\to 0^+} \\left( \\frac{x_0^{1-\\alpha}}{1-\\alpha} - \\frac{\\epsilon^{1-\\alpha}}{1-\\alpha} \\right) $$\nSince $\\alpha \\in (0,1)$, the exponent $1-\\alpha$ is positive. Consequently, as $\\epsilon \\to 0^+$, the term $\\epsilon^{1-\\alpha}$ goes to $0$.\nThe limit evaluates to:\n$$ T(x_0) = \\frac{x_0^{1-\\alpha}}{1-\\alpha} - 0 = \\frac{x_0^{1-\\alpha}}{1-\\alpha} $$\nThis finite arrival time from $x=0$ to $x_0>0$ is a quantitative measure of the non-uniqueness; a particle can escape the fixed point $x=0$ and reach any neighboring point in finite time, a phenomenon forbidden when solutions are unique.", "answer": "$$\\boxed{\\frac{x_{0}^{1-\\alpha}}{1-\\alpha}}$$", "id": "3078150"}]}