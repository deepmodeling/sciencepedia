{"hands_on_practices": [{"introduction": "The Rayleigh-Ritz method is a cornerstone technique that translates the abstract min-max principle into a powerful computational tool for approximating eigenvalues. By restricting the Rayleigh quotient to a finite-dimensional subspace of \"trial functions,\" we convert an infinite-dimensional problem into a manageable matrix eigenvalue problem. This first practice [@problem_id:3072684] will guide you through this process in a simple, one-dimensional setting, revealing how even a small, well-chosen subspace can yield remarkably accurate eigenvalue estimates.", "problem": "Let $L$ be the one-dimensional Dirichlet Laplacian defined by $L u = -\\frac{d^{2}u}{dx^{2}}$ on the interval $(0,1)$ with boundary conditions $u(0)=u(1)=0$. The first eigenvalue $\\lambda_{1}$ of $L$ admits the min-max characterization via the Rayleigh quotient: for nonzero $u \\in H_{0}^{1}(0,1)$, $$R[u] = \\frac{\\int_{0}^{1} |u'(x)|^{2} \\, dx}{\\int_{0}^{1} |u(x)|^{2} \\, dx},$$ and $\\lambda_{1}$ is the minimum of $R[u]$ over $H_{0}^{1}(0,1)\\setminus\\{0\\}$. Use the Rayleigh–Ritz method by restricting $R[u]$ to the two-dimensional subspace $V=\\mathrm{span}\\{\\sin(\\pi x), \\sin(2\\pi x)\\}$ and compute the smallest Ritz eigenvalue obtained from the induced generalized eigenvalue problem on $V$. Report this smallest Ritz eigenvalue as a simplified exact expression. In your derivation, justify how it compares to the exact first eigenvalue for this boundary value problem. Provide only the value of the smallest Ritz eigenvalue as your final answer; do not round.", "solution": "The problem is first validated. The givens are: the operator $L u = -\\frac{d^{2}u}{dx^{2}}$ on the interval $(0,1)$, boundary conditions $u(0)=u(1)=0$, Sobolev space $H_{0}^{1}(0,1)$, the Rayleigh quotient $R[u] = \\frac{\\int_{0}^{1} |u'(x)|^{2} \\, dx}{\\int_{0}^{1} |u(x)|^{2} \\, dx}$, the definition of the first eigenvalue $\\lambda_{1}$ as the minimum of $R[u]$, and the trial subspace $V=\\mathrm{span}\\{\\sin(\\pi x), \\sin(2\\pi x)\\}$ for the Rayleigh-Ritz method. The problem is to find the smallest Ritz eigenvalue. The problem is scientifically grounded in the spectral theory of differential operators, a core topic in mathematical physics and analysis. It is well-posed, as the Rayleigh-Ritz method applied to a finite-dimensional subspace leads to a well-defined generalized matrix eigenvalue problem, which has a unique set of solutions. The problem statement is objective and complete. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe Rayleigh-Ritz method seeks an approximation of the eigenvalues of an operator by minimizing the Rayleigh quotient over a finite-dimensional subspace of the full domain. Let the chosen subspace be $V = \\mathrm{span}\\{\\phi_1(x), \\phi_2(x)\\}$, where $\\phi_1(x) = \\sin(\\pi x)$ and $\\phi_2(x) = \\sin(2\\pi x)$. Both basis functions satisfy the Dirichlet boundary conditions, i.e., $\\phi_i(0) = \\phi_i(1) = 0$ for $i=1,2$, so $V$ is a subspace of $H_{0}^{1}(0,1)$.\n\nAny function $u(x)$ in $V$ can be expressed as a linear combination of the basis functions:\n$$u(x) = c_1 \\phi_1(x) + c_2 \\phi_2(x) = c_1 \\sin(\\pi x) + c_2 \\sin(2\\pi x)$$\nfor some coefficients $c_1, c_2 \\in \\mathbb{R}$. The derivative is $u'(x) = c_1 \\pi \\cos(\\pi x) + c_2 2\\pi \\cos(2\\pi x)$.\n\nSubstituting this into the Rayleigh quotient $R[u]$ gives:\n$$R[u] = \\frac{\\int_{0}^{1} (c_1 \\pi \\cos(\\pi x) + c_2 2\\pi \\cos(2\\pi x))^{2} \\, dx}{\\int_{0}^{1} (c_1 \\sin(\\pi x) + c_2 \\sin(2\\pi x))^{2} \\, dx}$$\nThe minimization of this quotient with respect to the coefficients $(c_1, c_2)$ is equivalent to solving the generalized eigenvalue problem $Ac = \\tilde{\\lambda} Bc$, where $\\tilde{\\lambda}$ are the Ritz eigenvalues, $c = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$ is the vector of coefficients, and the matrices $A$ (stiffness matrix) and $B$ (mass matrix) are defined by their entries:\n$$A_{ij} = \\int_{0}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$$\n$$B_{ij} = \\int_{0}^{1} \\phi_i(x) \\phi_j(x) \\, dx$$\nThe derivatives of our basis functions are $\\phi_1'(x) = \\pi \\cos(\\pi x)$ and $\\phi_2'(x) = 2\\pi \\cos(2\\pi x)$.\n\nLet us compute the entries of the matrix $B$:\n$B_{11} = \\int_{0}^{1} \\sin^2(\\pi x) \\, dx = \\int_{0}^{1} \\frac{1 - \\cos(2\\pi x)}{2} \\, dx = \\left[ \\frac{x}{2} - \\frac{\\sin(2\\pi x)}{4\\pi} \\right]_{0}^{1} = \\frac{1}{2}$.\n$B_{22} = \\int_{0}^{1} \\sin^2(2\\pi x) \\, dx = \\int_{0}^{1} \\frac{1 - \\cos(4\\pi x)}{2} \\, dx = \\left[ \\frac{x}{2} - \\frac{\\sin(4\\pi x)}{8\\pi} \\right]_{0}^{1} = \\frac{1}{2}$.\nFor the off-diagonal terms, we use the orthogonality of sine functions with different integer frequencies on $[0,1]$:\n$B_{12} = B_{21} = \\int_{0}^{1} \\sin(\\pi x) \\sin(2\\pi x) \\, dx = 0$.\nSo the mass matrix is $B = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/2 \\end{pmatrix}$.\n\nNext, we compute the entries of the matrix $A$:\n$A_{11} = \\int_{0}^{1} (\\pi \\cos(\\pi x))^2 \\, dx = \\pi^2 \\int_{0}^{1} \\cos^2(\\pi x) \\, dx = \\pi^2 \\int_{0}^{1} \\frac{1 + \\cos(2\\pi x)}{2} \\, dx = \\pi^2 \\left[ \\frac{x}{2} + \\frac{\\sin(2\\pi x)}{4\\pi} \\right]_{0}^{1} = \\frac{\\pi^2}{2}$.\n$A_{22} = \\int_{0}^{1} (2\\pi \\cos(2\\pi x))^2 \\, dx = 4\\pi^2 \\int_{0}^{1} \\cos^2(2\\pi x) \\, dx = 4\\pi^2 \\int_{0}^{1} \\frac{1 + \\cos(4\\pi x)}{2} \\, dx = 4\\pi^2 \\left[ \\frac{x}{2} + \\frac{\\sin(4\\pi x)}{8\\pi} \\right]_{0}^{1} = 2\\pi^2$.\nFor the off-diagonal terms, we use the orthogonality of cosine functions with different non-zero integer frequencies on $[0,1]$:\n$A_{12} = A_{21} = \\int_{0}^{1} (\\pi \\cos(\\pi x))(2\\pi \\cos(2\\pi x)) \\, dx = 2\\pi^2 \\int_{0}^{1} \\cos(\\pi x) \\cos(2\\pi x) \\, dx = 0$.\nSo the stiffness matrix is $A = \\begin{pmatrix} \\pi^2/2 & 0 \\\\ 0 & 2\\pi^2 \\end{pmatrix}$.\n\nThe generalized eigenvalue problem $Ac = \\tilde{\\lambda} Bc$ is:\n$$\\begin{pmatrix} \\pi^2/2 & 0 \\\\ 0 & 2\\pi^2 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\tilde{\\lambda} \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/2 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$$\nTo find the Ritz eigenvalues $\\tilde{\\lambda}$, we solve the characteristic equation $\\det(A - \\tilde{\\lambda}B) = 0$:\n$$\\det \\left( \\begin{pmatrix} \\pi^2/2 & 0 \\\\ 0 & 2\\pi^2 \\end{pmatrix} - \\tilde{\\lambda} \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/2 \\end{pmatrix} \\right) = \\det \\begin{pmatrix} \\frac{\\pi^2}{2} - \\frac{\\tilde{\\lambda}}{2} & 0 \\\\ 0 & 2\\pi^2 - \\frac{\\tilde{\\lambda}}{2} \\end{pmatrix} = 0$$\nThis gives the equation $\\left(\\frac{\\pi^2 - \\tilde{\\lambda}}{2}\\right)\\left(\\frac{4\\pi^2 - \\tilde{\\lambda}}{2}\\right) = 0$.\nThe solutions are the Ritz eigenvalues: $\\tilde{\\lambda}_1 = \\pi^2$ and $\\tilde{\\lambda}_2 = 4\\pi^2$.\nThe smallest Ritz eigenvalue is $\\tilde{\\lambda}_1 = \\pi^2$.\n\nTo compare this to the exact first eigenvalue, we solve the boundary value problem $L u = \\lambda u$, which is $-\\frac{d^2u}{dx^2} = \\lambda u$ with $u(0)=u(1)=0$. The general solution is $u(x) = C_1 \\cos(\\sqrt{\\lambda}x) + C_2 \\sin(\\sqrt{\\lambda}x)$. The boundary condition $u(0)=0$ implies $C_1=0$. The condition $u(1)=0$ implies $C_2 \\sin(\\sqrt{\\lambda})=0$. For a non-trivial solution, $\\sin(\\sqrt{\\lambda})=0$, which means $\\sqrt{\\lambda} = n\\pi$ for $n \\in \\{1, 2, 3, \\ldots\\}$. The exact eigenvalues are $\\lambda_n = n^2\\pi^2$. The first (smallest) exact eigenvalue is $\\lambda_1 = \\pi^2$.\n\nThe smallest Ritz eigenvalue, $\\tilde{\\lambda}_1 = \\pi^2$, is identical to the exact first eigenvalue $\\lambda_1 = \\pi^2$. According to the min-max principle, the Rayleigh-Ritz method always provides an upper bound for the true eigenvalues. That is, the $k$-th Ritz eigenvalue $\\tilde{\\lambda}_k$ is greater than or equal to the $k$-th true eigenvalue $\\lambda_k$. In our case, $\\tilde{\\lambda}_1 \\ge \\lambda_1$. The equality $\\tilde{\\lambda}_1 = \\lambda_1$ occurs because the subspace $V$ chosen for the approximation, $V=\\mathrm{span}\\{\\sin(\\pi x), \\sin(2\\pi x)\\}$, contains the exact eigenfunction $u_1(x) = \\sin(\\pi x)$ corresponding to the first eigenvalue $\\lambda_1$. When the trial subspace contains an exact eigenfunction, the Rayleigh-Ritz method will recover the corresponding exact eigenvalue. In this case, the subspace fortuitously also contains the second exact eigenfunction $u_2(x)=\\sin(2\\pi x)$, so the method also recovers the exact second eigenvalue $\\lambda_2=4\\pi^2$.\n\nThe problem asks for the smallest Ritz eigenvalue obtained. This is $\\pi^2$.", "answer": "$$\\boxed{\\pi^{2}}$$", "id": "3072684"}, {"introduction": "The min-max principle, which guarantees that the Rayleigh quotient is bounded by the extremal eigenvalues, relies critically on the operator being self-adjoint (or symmetric in the case of real matrices). Why is this condition so crucial? This exercise [@problem_id:3072678] invites you to investigate this question directly by exploring a non-symmetric matrix. By calculating the Rayleigh quotient and comparing its range to the matrix's eigenvalues, you will uncover precisely why symmetry is the linchpin for this fundamental variational principle.", "problem": "Let $V=\\mathbb{R}^{2}$ be equipped with the standard Euclidean inner product $\\langle x,y\\rangle = x^{\\top}y$ and norm $\\|x\\|=\\sqrt{\\langle x,x\\rangle}$. For a real matrix $A\\in\\mathbb{R}^{2\\times 2}$ and a nonzero vector $x\\in V$, the Rayleigh quotient is defined by $R_{A}(x)=\\dfrac{x^{\\top}Ax}{x^{\\top}x}$. In geometric analysis, the min–max principle for self-adjoint operators (for instance, the Laplace–Beltrami operator on a compact Riemannian manifold) guarantees that the Rayleigh quotient is bounded between the smallest and largest eigenvalues because the spectrum is real and the operator is symmetric. In contrast, for a non-symmetric matrix, eigenvalues need not be real, and the variational bounds need not relate to the real parts of eigenvalues.\n\nConsider the specific non-symmetric matrix\n$$\nA=\\begin{pmatrix}\n0 & -3 \\\\\n1 & 0\n\\end{pmatrix}.\n$$\nStarting from the given definitions, and without invoking any shortcut formulas, do the following:\n- Determine the eigenvalues of $A$ and state their real parts.\n- Compute $R_{A}(x)$ for a unit vector $x$, parameterized by $x=(\\cos\\theta,\\sin\\theta)$ with $\\theta\\in\\mathbb{R}$, and determine $\\max_{\\|x\\|=1} R_{A}(x)$.\n- Explain, using first principles, why symmetry ($A=A^{\\top}$) is crucial for bounding $R_{A}(x)$ between extremal eigenvalues via the min–max principle, and why such a bound can fail for non-symmetric $A$.\n\nReport as your final answer the single numerical value of $\\max_{\\|x\\|=1} R_{A}(x)$. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in linear algebra, well-posed, and objective. All necessary information is provided, and the questions are specific and formalizable. We proceed with the solution.\n\nThe problem asks for three tasks concerning the non-symmetric matrix $A=\\begin{pmatrix} 0 & -3 \\\\ 1 & 0 \\end{pmatrix}$. We address them in order.\n\n**Part 1: Eigenvalues of $A$**\n\nThe eigenvalues $\\lambda$ of a matrix $A$ are the solutions to the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the identity matrix.\nFor the given matrix $A$, we have:\n$$\nA - \\lambda I = \\begin{pmatrix} 0 & -3 \\\\ 1 & 0 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda & -3 \\\\ 1 & -\\lambda \\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(A - \\lambda I) = (-\\lambda)(-\\lambda) - (-3)(1) = \\lambda^{2} + 3\n$$\nSetting the determinant to zero gives the characteristic equation:\n$$\n\\lambda^{2} + 3 = 0 \\implies \\lambda^{2} = -3\n$$\nThe solutions are complex:\n$$\n\\lambda = \\pm\\sqrt{-3} = \\pm i\\sqrt{3}\n$$\nThe two eigenvalues are $\\lambda_1 = i\\sqrt{3}$ and $\\lambda_2 = -i\\sqrt{3}$. The real part of both eigenvalues is $0$.\n$$\n\\operatorname{Re}(\\lambda_1) = 0 \\quad \\text{and} \\quad \\operatorname{Re}(\\lambda_2) = 0\n$$\n\n**Part 2: Rayleigh Quotient and its Maximum**\n\nThe Rayleigh quotient is defined as $R_{A}(x)=\\dfrac{x^{\\top}Ax}{x^{\\top}x}$. We need to compute this for a unit vector $x \\in \\mathbb{R}^2$, which can be parameterized as $x = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$ for $\\theta \\in \\mathbb{R}$. For a unit vector, the denominator $x^{\\top}x = \\|x\\|^2 = 1$, so $R_{A}(x) = x^{\\top}Ax$.\n\nLet's compute the quadratic form $x^{\\top}Ax$:\n$$\nx^{\\top}Ax = \\begin{pmatrix} \\cos\\theta & \\sin\\theta \\end{pmatrix} \\begin{pmatrix} 0 & -3 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}\n$$\nFirst, we multiply the matrix and the column vector:\n$$\n\\begin{pmatrix} 0 & -3 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix} = \\begin{pmatrix} -3\\sin\\theta \\\\ \\cos\\theta \\end{pmatrix}\n$$\nNext, we perform the dot product with the row vector:\n$$\nx^{\\top}Ax = \\begin{pmatrix} \\cos\\theta & \\sin\\theta \\end{pmatrix} \\begin{pmatrix} -3\\sin\\theta \\\\ \\cos\\theta \\end{pmatrix} = (\\cos\\theta)(-3\\sin\\theta) + (\\sin\\theta)(\\cos\\theta) = -2\\sin\\theta\\cos\\theta\n$$\nUsing the trigonometric double-angle identity $\\sin(2\\theta) = 2\\sin\\theta\\cos\\theta$, we can simplify the expression:\n$$\nR_{A}(x) = -\\sin(2\\theta)\n$$\nWe want to find the maximum value of this expression as $x$ varies over all unit vectors, which corresponds to $\\theta$ varying over $\\mathbb{R}$. The maximum value of the function $f(\\theta) = -\\sin(2\\theta)$ is sought. The sine function, $\\sin(\\phi)$, has a range of $[-1, 1]$. Therefore, $-\\sin(\\phi)$ also has a range of $[-1, 1]$. The maximum value is $1$, which occurs when $\\sin(2\\theta) = -1$. This happens, for instance, when $2\\theta = \\frac{3\\pi}{2} + 2k\\pi$ for an integer $k$.\nThus, the maximum value of the Rayleigh quotient is:\n$$\n\\max_{\\|x\\|=1} R_{A}(x) = \\max_{\\theta \\in \\mathbb{R}} (-\\sin(2\\theta)) = 1\n$$\n\n**Part 3: The Role of Symmetry**\n\nThe min-max principle guarantees that for a symmetric (or self-adjoint) matrix $A=A^{\\top}$, the Rayleigh quotient $R_A(x)$ is bounded by the smallest and largest eigenvalues, $\\lambda_{\\min}$ and $\\lambda_{\\max}$ respectively. That is, $\\lambda_{\\min} \\le R_A(x) \\le \\lambda_{\\max}$ for all non-zero $x$. This principle fails for non-symmetric matrices, as demonstrated by our specific example.\n\nThe reasoning is as follows:\n1.  **Foundation for Symmetric Matrices**: For a real symmetric matrix $A$, the spectral theorem guarantees two crucial properties:\n    a. All eigenvalues of $A$ are real. Let them be ordered: $\\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$.\n    b. There exists an orthonormal basis of eigenvectors $\\{v_1, v_2, \\dots, v_n\\}$ corresponding to these eigenvalues.\n    Any vector $x$ can be expressed as a linear combination of this basis: $x = \\sum_{i=1}^{n} c_i v_i$. Due to orthonormality, $\\|x\\|^2 = x^{\\top}x = \\sum_{i=1}^{n} c_i^2$.\n    The action of $A$ on $x$ is $Ax = A(\\sum_{i=1}^{n} c_i v_i) = \\sum_{i=1}^{n} c_i (Av_i) = \\sum_{i=1}^{n} c_i \\lambda_i v_i$.\n    The numerator of the Rayleigh quotient becomes $x^{\\top}Ax = (\\sum_{j=1}^{n} c_j v_j)^{\\top}(\\sum_{i=1}^{n} c_i \\lambda_i v_i) = \\sum_{i,j} c_j c_i \\lambda_i (v_j^{\\top}v_i) = \\sum_{i=1}^{n} c_i^2 \\lambda_i$, since $v_j^{\\top}v_i = \\delta_{ij}$.\n    So, for a symmetric matrix, $R_A(x) = \\frac{\\sum_{i=1}^{n} c_i^2 \\lambda_i}{\\sum_{i=1}^{n} c_i^2}$. This is a weighted average of the eigenvalues, and its value must lie between the minimum and maximum eigenvalues.\n\n2.  **Breakdown for Non-Symmetric Matrices**: When $A$ is not symmetric, the properties underlying the proof above no longer hold.\n    a. **Eigenvalues may be complex**. As we found for our matrix $A$, the eigenvalues are $\\lambda = \\pm i\\sqrt{3}$. A bound of the form $\\lambda_{\\min} \\le R_A(x) \\le \\lambda_{\\max}$ is ill-defined because the complex numbers are not ordered. If one were to consider the real parts of the eigenvalues, for our matrix $A$, this would imply the interval $[\\min(0,0), \\max(0,0)] = \\{0\\}$. However, we found the range of $R_A(x)$ to be $[-1, 1]$. Clearly, the bound fails.\n    b. **No guaranteed orthonormal basis of eigenvectors**. For a general non-symmetric matrix, eigenvectors corresponding to distinct eigenvalues are not necessarily orthogonal, and a full basis of eigenvectors might not even exist (if the matrix is not diagonalizable). This prevents the decomposition of $R_A(x)$ into a simple weighted average of eigenvalues.\n\n3.  **The Correct Interpretation**: The value of the Rayleigh quotient $R_A(x) = \\frac{x^{\\top}Ax}{x^{\\top}x}$ depends only on the symmetric part of $A$. Any matrix $A$ can be written as $A=S+K$, where $S=\\frac{1}{2}(A+A^{\\top})$ is the symmetric part and $K=\\frac{1}{2}(A-A^{\\top})$ is the skew-symmetric part. The quadratic form $x^{\\top}Ax$ becomes:\n    $$\n    x^{\\top}Ax = x^{\\top}(S+K)x = x^{\\top}Sx + x^{\\top}Kx\n    $$\n    For any skew-symmetric matrix $K$, $x^{\\top}Kx = 0$. This is because $x^{\\top}Kx$ is a scalar, so it equals its own transpose: $x^{\\top}Kx = (x^{\\top}Kx)^{\\top} = x^{\\top}K^{\\top}x$. Since $K$ is skew-symmetric, $K^{\\top}=-K$, so $x^{\\top}Kx = x^{\\top}(-K)x = -x^{\\top}Kx$. This implies $2x^{\\top}Kx=0$, so $x^{\\top}Kx=0$.\n    Therefore, $R_A(x) = \\frac{x^{\\top}Sx}{x^{\\top}x} = R_S(x)$.\n    The range of values of $R_A(x)$ for a non-symmetric $A$ is determined by the eigenvalues of its symmetric part $S$.\n    For our matrix $A=\\begin{pmatrix} 0 & -3 \\\\ 1 & 0 \\end{pmatrix}$, the symmetric part is:\n    $$\n    S = \\frac{1}{2}\\left( \\begin{pmatrix} 0 & -3 \\\\ 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 1 \\\\ -3 & 0 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 0 & -2 \\\\ -2 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 \\\\ -1 & 0 \\end{pmatrix}\n    $$\n    The eigenvalues $\\mu$ of $S$ are given by $\\det(S-\\mu I) = \\det\\begin{pmatrix} -\\mu & -1 \\\\ -1 & -\\mu \\end{pmatrix} = \\mu^2 - 1 = 0$, so $\\mu = \\pm 1$.\n    Since $S$ is symmetric, the range of $R_S(x)$ is $[\\mu_{\\min}, \\mu_{\\max}] = [-1, 1]$. Because $R_A(x)=R_S(x)$, this is also the range of $R_A(x)$. This confirms our direct calculation of $\\max R_A(x) = 1$.\n\nIn summary, symmetry ($A=A^{\\top}$) is crucial because it ensures that the eigenvalues of $A$ are the same as the eigenvalues of its symmetric part ($S=A$), thus directly linking the spectrum of $A$ to the range of its Rayleigh quotient via the spectral theorem. For non-symmetric $A$, this link is broken.", "answer": "$$\n\\boxed{1}\n$$", "id": "3072678"}, {"introduction": "Having established the Rayleigh-Ritz method and the importance of symmetry, we now elevate our analysis to a more general, n-dimensional domain. This practice [@problem_id:3072661] challenges you to compute the Rayleigh quotient for a specific test function on a unit hypercube. The calculation not only reinforces your multivariable calculus skills but also provides a concrete verification of the min-max principle: when the chosen test function is the true eigenfunction, the Rayleigh quotient does not merely provide an upper bound but delivers the exact eigenvalue.", "problem": "Let $\\Omega=(0,1)^{n}\\subset\\mathbb{R}^{n}$ with $n\\in\\mathbb{N}$ be the unit cube, and consider the Dirichlet Laplacian $-\\Delta$ on $\\Omega$ with homogeneous boundary condition on $\\partial\\Omega$. For a nonzero function $u\\in H_{0}^{1}(\\Omega)$, the Rayleigh quotient is defined by\n$$\nR(u)=\\frac{\\int_{\\Omega}|\\nabla u(x)|^{2}\\,dx}{\\int_{\\Omega}|u(x)|^{2}\\,dx}.\n$$\nUsing only this definition, basic properties of the Lebesgue integral, and one-dimensional trigonometric identities, compute the Rayleigh quotient $R(u)$ for the test function\n$$\nu(x)=\\prod_{i=1}^{n}\\sin\\!\\big(\\pi x_{i}\\big),\n$$\nand then, starting from first principles of separation of variables for the Dirichlet Laplacian on a rectangular domain, determine the exact first eigenvalue of $-\\Delta$ on $\\Omega$ and compare it with $R(u)$. Provide your final answer as the exact value of $R(u)$ in closed form. No rounding is required and no units are involved.", "solution": "The problem requires the computation of the Rayleigh quotient for a specific test function on the $n$-dimensional unit cube, and a comparison of this result with the first eigenvalue of the Dirichlet Laplacian, derived from first principles.\n\nThe Rayleigh quotient is defined as\n$$\nR(u)=\\frac{\\int_{\\Omega}|\\nabla u(x)|^{2}\\,dx}{\\int_{\\Omega}|u(x)|^{2}\\,dx}\n$$\nfor a nonzero function $u\\in H_{0}^{1}(\\Omega)$. The domain is the unit cube $\\Omega=(0,1)^{n}$, and the test function is given by $u(x)=\\prod_{i=1}^{n}\\sin(\\pi x_{i})$, where $x=(x_{1}, \\dots, x_{n})$.\n\nWe will compute the numerator and the denominator of the Rayleigh quotient separately.\n\nFirst, we calculate the denominator, which is the squared $L^2$-norm of $u$, denoted by $\\|u\\|_{L^2(\\Omega)}^2 = \\int_{\\Omega}|u(x)|^{2}\\,dx$.\n$$\n\\int_{\\Omega}|u(x)|^{2}\\,dx = \\int_{(0,1)^{n}} \\left| \\prod_{i=1}^{n}\\sin(\\pi x_{i}) \\right|^{2} \\,dx_{1} \\dots dx_{n} = \\int_{(0,1)^{n}} \\prod_{i=1}^{n}\\sin^{2}(\\pi x_{i}) \\,dx_{1} \\dots dx_{n}\n$$\nUsing Fubini's theorem, we can separate this multidimensional integral into a product of one-dimensional integrals:\n$$\n\\int_{\\Omega}|u(x)|^{2}\\,dx = \\prod_{i=1}^{n} \\int_{0}^{1} \\sin^{2}(\\pi x_{i}) \\,dx_{i}\n$$\nThe one-dimensional integral is evaluated using the trigonometric identity $\\sin^{2}(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$\n\\int_{0}^{1} \\sin^{2}(\\pi t) \\,dt = \\int_{0}^{1} \\frac{1 - \\cos(2\\pi t)}{2} \\,dt = \\frac{1}{2} \\left[t - \\frac{\\sin(2\\pi t)}{2\\pi}\\right]_{0}^{1} = \\frac{1}{2} \\left( (1-0) - (\\frac{\\sin(2\\pi)}{2\\pi} - \\frac{\\sin(0)}{2\\pi}) \\right) = \\frac{1}{2}\n$$\nThe value of this integral is $\\frac{1}{2}$ for each dimension. Therefore, the denominator is:\n$$\n\\int_{\\Omega}|u(x)|^{2}\\,dx = \\prod_{i=1}^{n} \\frac{1}{2} = \\left(\\frac{1}{2}\\right)^{n} = \\frac{1}{2^{n}}\n$$\n\nNext, we calculate the numerator, $\\int_{\\Omega}|\\nabla u(x)|^{2}\\,dx$. The gradient of $u$ is $\\nabla u = (\\frac{\\partial u}{\\partial x_{1}}, \\dots, \\frac{\\partial u}{\\partial x_{n}})$. The partial derivative with respect to the $j$-th coordinate, for $j \\in \\{1, \\dots, n\\}$, is:\n$$\n\\frac{\\partial u}{\\partial x_{j}} = \\frac{\\partial}{\\partial x_{j}} \\left(\\prod_{i=1}^{n}\\sin(\\pi x_{i})\\right) = \\left(\\frac{d}{dx_{j}} \\sin(\\pi x_{j})\\right) \\prod_{i \\neq j} \\sin(\\pi x_{i}) = \\pi\\cos(\\pi x_{j}) \\prod_{i \\neq j} \\sin(\\pi x_{i})\n$$\nThe squared magnitude of the gradient is $|\\nabla u|^{2} = \\sum_{j=1}^{n} \\left(\\frac{\\partial u}{\\partial x_{j}}\\right)^{2}$:\n$$\n|\\nabla u(x)|^{2} = \\sum_{j=1}^{n} \\left(\\pi\\cos(\\pi x_{j}) \\prod_{i \\neq j} \\sin(\\pi x_{i})\\right)^{2} = \\sum_{j=1}^{n} \\pi^{2}\\cos^{2}(\\pi x_{j}) \\prod_{i \\neq j} \\sin^{2}(\\pi x_{i})\n$$\nWe integrate this expression over $\\Omega$. By linearity of the integral, we can interchange the sum and the integral:\n$$\n\\int_{\\Omega}|\\nabla u(x)|^{2}\\,dx = \\sum_{j=1}^{n} \\int_{(0,1)^{n}} \\pi^{2}\\cos^{2}(\\pi x_{j}) \\prod_{i \\neq j} \\sin^{2}(\\pi x_{i}) \\,dx_{1} \\dots dx_{n}\n$$\nFor each term $j$ in the summation, we apply Fubini's theorem again:\n$$\n\\pi^{2} \\int_{(0,1)^{n}} \\cos^{2}(\\pi x_{j}) \\prod_{i \\neq j} \\sin^{2}(\\pi x_{i}) \\,dV = \\pi^{2} \\left(\\int_{0}^{1}\\cos^{2}(\\pi x_{j})\\,dx_{j}\\right) \\prod_{i \\neq j} \\left(\\int_{0}^{1}\\sin^{2}(\\pi x_{i})\\,dx_{i}\\right)\n$$\nWe need the integral of $\\cos^{2}(\\pi t)$, which we find using the identity $\\cos^{2}(\\theta) = \\frac{1 + \\cos(2\\theta)}{2}$:\n$$\n\\int_{0}^{1} \\cos^{2}(\\pi t) \\,dt = \\int_{0}^{1} \\frac{1 + \\cos(2\\pi t)}{2} \\,dt = \\frac{1}{2} \\left[t + \\frac{\\sin(2\\pi t)}{2\\pi}\\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, for each fixed $j$, the integral term evaluates to:\n$$\n\\pi^{2} \\left(\\frac{1}{2}\\right) \\prod_{i \\neq j} \\left(\\frac{1}{2}\\right) = \\pi^{2} \\left(\\frac{1}{2}\\right) \\left(\\frac{1}{2}\\right)^{n-1} = \\frac{\\pi^{2}}{2^{n}}\n$$\nThe numerator is the sum of these $n$ identical terms:\n$$\n\\int_{\\Omega}|\\nabla u(x)|^{2}\\,dx = \\sum_{j=1}^{n} \\frac{\\pi^{2}}{2^{n}} = \\frac{n\\pi^{2}}{2^{n}}\n$$\nNow, we can compute the Rayleigh quotient $R(u)$:\n$$\nR(u) = \\frac{n\\pi^{2}/2^{n}}{1/2^{n}} = n\\pi^{2}\n$$\n\nAs stipulated, we now determine the eigenvalues of the Dirichlet Laplacian $-\\Delta$ on $\\Omega$ using separation of variables. The eigenvalue problem is $-\\Delta \\psi = \\lambda \\psi$ in $\\Omega$, with boundary condition $\\psi=0$ on $\\partial\\Omega$. We assume a separable solution $\\psi(x) = \\prod_{i=1}^{n} X_{i}(x_{i})$. Substituting this into the PDE and dividing by $\\psi$ gives:\n$$\n\\sum_{i=1}^{n} \\frac{-X_{i}''(x_{i})}{X_{i}(x_{i})} = \\lambda\n$$\nEach term must be a constant, denoted $\\lambda_{i}$, leading to $n$ one-dimensional Sturm-Liouville problems: $-X_{i}''(x_{i}) = \\lambda_{i}X_{i}(x_{i})$ on $(0,1)$ with $X_{i}(0)=0$ and $X_{i}(1)=0$. The non-trivial solutions are $X_{i}(x_{i}) = \\sin(m_{i}\\pi x_{i})$ with corresponding eigenvalues $\\lambda_{i} = (m_{i}\\pi)^{2}$ for integers $m_{i} \\in \\{1, 2, 3, \\dots\\}$. The eigenvalues of $-\\Delta$ are the sums $\\lambda = \\sum_{i=1}^{n} \\lambda_{i}$:\n$$\n\\lambda_{m_{1},\\dots,m_{n}} = \\pi^{2} \\sum_{i=1}^{n} m_{i}^{2}\n$$\nThe first eigenvalue, $\\lambda_1$, is the minimum of these values, which occurs when all $m_{i}=1$:\n$$\n\\lambda_1 = \\pi^{2} \\sum_{i=1}^{n} 1^{2} = n\\pi^{2}\n$$\nThe corresponding eigenfunction is $\\psi_{1,\\dots,1}(x) = \\prod_{i=1}^{n} \\sin(\\pi x_{i})$, which is precisely the test function $u(x)$. The fact that $R(u)=\\lambda_1$ is a direct consequence of the variational characterization of the first eigenvalue (min-max principle), which states that $\\lambda_1$ is the minimum of the Rayleigh quotient over all admissible functions, a minimum that is achieved by the first eigenfunction. Our direct calculation confirms this fundamental result.\n\nThe requested final answer is the exact value of $R(u)$.", "answer": "$$\n\\boxed{n\\pi^{2}}\n$$", "id": "3072661"}]}