{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to get our hands on the core mechanics of Picard's iteration method. This practice involves applying the recursive integral formula to a non-linear initial value problem to generate the first few function approximations [@problem_id:1282577]. This direct computational experience is crucial for building an intuitive understanding of how a sequence of functions can converge to the solution of a differential equation.", "problem": "Consider the initial value problem given by the first-order ordinary differential equation $y'(t) = t + (y(t))^2$ with the initial condition $y(0) = 0$.\n\nA sequence of functions, denoted by $\\{y_n(t)\\}_{n=0}^\\infty$, can be constructed to approximate the true solution of this problem. This sequence is defined by a starting function $y_0(t)$ and the following recursive relation for $n \\geq 0$:\n$$y_{n+1}(t) = y(0) + \\int_{0}^{t} \\left(s + (y_n(s))^2\\right) \\, ds$$\nStarting with the initial approximation $y_0(t) = 0$ for all $t$, determine the function $y_3(t)$ that results from this iterative process. Express your answer as a polynomial in the variable $t$.", "solution": "We use Picard iteration for the initial value problem $y'(t)=t+(y(t))^{2}$ with $y(0)=0$, defined by\n$$\ny_{n+1}(t)=y(0)+\\int_{0}^{t}\\left(s+(y_{n}(s))^{2}\\right)\\,ds.\n$$\nGiven $y(0)=0$ and the starting function $y_{0}(t)=0$, this reduces to\n$$\ny_{n+1}(t)=\\int_{0}^{t}\\left(s+(y_{n}(s))^{2}\\right)\\,ds.\n$$\n\nFirst iteration:\nSince $y_{0}(s)=0$, we have\n$$\ny_{1}(t)=\\int_{0}^{t}\\left(s+0^{2}\\right)\\,ds=\\int_{0}^{t}s\\,ds=\\frac{t^{2}}{2}.\n$$\n\nSecond iteration:\nCompute $(y_{1}(s))^{2}=\\left(\\frac{s^{2}}{2}\\right)^{2}=\\frac{s^{4}}{4}$, hence\n$$\ny_{2}(t)=\\int_{0}^{t}\\left(s+\\frac{s^{4}}{4}\\right)\\,ds=\\frac{t^{2}}{2}+\\frac{t^{5}}{20}.\n$$\n\nThird iteration:\nCompute $(y_{2}(s))^{2}$ with $y_{2}(s)=\\frac{s^{2}}{2}+\\frac{s^{5}}{20}$. Expanding,\n$$\n\\left(\\frac{s^{2}}{2}+\\frac{s^{5}}{20}\\right)^{2}\n=\\left(\\frac{s^{2}}{2}\\right)^{2}+2\\left(\\frac{s^{2}}{2}\\right)\\left(\\frac{s^{5}}{20}\\right)+\\left(\\frac{s^{5}}{20}\\right)^{2}\n=\\frac{s^{4}}{4}+\\frac{s^{7}}{20}+\\frac{s^{10}}{400}.\n$$\nTherefore,\n$$\ny_{3}(t)=\\int_{0}^{t}\\left(s+\\frac{s^{4}}{4}+\\frac{s^{7}}{20}+\\frac{s^{10}}{400}\\right)\\,ds\n=\\frac{t^{2}}{2}+\\frac{t^{5}}{20}+\\frac{t^{8}}{160}+\\frac{t^{11}}{4400}.\n$$\nThis is a polynomial in $t$, as required.", "answer": "$$\\boxed{\\frac{t^{2}}{2}+\\frac{t^{5}}{20}+\\frac{t^{8}}{160}+\\frac{t^{11}}{4400}}$$", "id": "1282577"}, {"introduction": "Having practiced the iterative process, we now explore a special case that reveals an elegant connection between Picard's method and classical analysis. By solving the fundamental initial value problem $y'(t) = y(t)$ with $y(0) = 1$, you will demonstrate that the successive approximations generated are precisely the Taylor polynomials of the true solution, $y(t)=\\exp(t)$ [@problem_id:1282605]. This exercise provides a concrete example of convergence and highlights the deep relationship between iterative solutions and series expansions.", "problem": "Picard's method of successive approximations is a fundamental tool in the study of differential equations for establishing the existence and uniqueness of solutions. It is based on reformulating an Initial Value Problem (IVP) as an integral equation and then solving it iteratively. For an IVP given by $y'(t) = f(t, y(t))$ with an initial condition $y(t_0) = y_0$, the sequence of approximations, $\\{\\phi_n(t)\\}_{n=0}^{\\infty}$, is generated by the recursive relations:\n$$ \\phi_0(t) = y_0 $$\n$$ \\phi_{n+1}(t) = y_0 + \\int_{t_0}^t f(s, \\phi_n(s)) ds \\quad \\text{for } n \\ge 0 $$\n\nConsider the specific IVP defined by the differential equation $y'(t) = y(t)$ with the initial condition $y(0) = 1$. For this problem, each iterate $\\phi_n(t)$ is a polynomial in the variable $t$. Specifically, for any non-negative integer $n$, the $n$-th iterate can be written in the form $\\phi_n(t) = \\sum_{k=0}^{n} c_k t^k$ for some coefficients $c_k$.\n\nDetermine the general expression for the coefficient $c_k$, where $k$ is any integer satisfying $0 \\le k \\le n$. Your answer should be a closed-form expression in terms of $k$.", "solution": "We start from the IVP $y'(t)=y(t)$ with $y(0)=1$. By the fundamental theorem of calculus, any differentiable solution satisfies the integral equation\n$$\ny(t)=y(0)+\\int_{0}^{t}y(s)\\,ds=1+\\int_{0}^{t}y(s)\\,ds.\n$$\nPicard's method defines the sequence\n$$\n\\phi_{0}(t)=1,\\qquad \\phi_{n+1}(t)=1+\\int_{0}^{t}\\phi_{n}(s)\\,ds\\quad\\text{for }n\\ge 0.\n$$\nWe claim that for each $n\\ge 0$,\n$$\n\\phi_{n}(t)=\\sum_{k=0}^{n}\\frac{t^{k}}{k!}.\n$$\nWe prove this by induction on $n$.\n\nBase case $n=0$: $\\phi_{0}(t)=1=\\sum_{k=0}^{0}\\frac{t^{k}}{k!}$, which holds.\n\nInductive step: Assume $\\phi_{n}(t)=\\sum_{k=0}^{n}\\frac{t^{k}}{k!}$. Then\n$$\n\\phi_{n+1}(t)=1+\\int_{0}^{t}\\phi_{n}(s)\\,ds\n=1+\\int_{0}^{t}\\left(\\sum_{k=0}^{n}\\frac{s^{k}}{k!}\\right)ds\n=1+\\sum_{k=0}^{n}\\frac{1}{k!}\\int_{0}^{t}s^{k}\\,ds\n=1+\\sum_{k=0}^{n}\\frac{t^{k+1}}{(k+1)k!}.\n$$\nLetting $j=k+1$ gives\n$$\n\\phi_{n+1}(t)=1+\\sum_{j=1}^{n+1}\\frac{t^{j}}{j!}=\\sum_{j=0}^{n+1}\\frac{t^{j}}{j!}.\n$$\nThus the inductive step holds, and the claimed form is proved for all $n$.\n\nTherefore, writing $\\phi_{n}(t)=\\sum_{k=0}^{n}c_{k}\\,t^{k}$, we identify the coefficients as\n$$\nc_{k}=\\frac{1}{k!}\\quad\\text{for }0\\le k\\le n.\n$$\nAn equivalent derivation via coefficient recursion is as follows. Suppose $\\phi_{n}(t)=\\sum_{k=0}^{n}c_{k}^{(n)}t^{k}$. Then\n$$\n\\phi_{n+1}(t)=1+\\sum_{k=0}^{n}c_{k}^{(n)}\\frac{t^{k+1}}{k+1},\n$$\nso $c_{0}^{(n+1)}=1$ and $c_{j}^{(n+1)}=\\frac{c_{j-1}^{(n)}}{j}$ for $j\\ge 1$. With $c_{0}^{(0)}=1$, iteration yields $c_{j}^{(n)}=\\frac{1}{j!}$ for $j\\le n$, consistent with the result above.\n\nHence, the closed-form expression for the coefficient $c_{k}$ is $c_{k}=\\frac{1}{k!}$.", "answer": "$$\\boxed{\\frac{1}{k!}}$$", "id": "1282605"}, {"introduction": "While the Contraction Mapping Theorem typically guarantees a solution only on a small, local interval, its power can be extended. This practice introduces a sophisticated technique that uses a specially designed \"weighted norm\" to prove the existence and uniqueness of solutions over an arbitrarily large interval for linear equations [@problem_id:1282586]. By finding the right metric, you will ensure the Picard operator becomes a contraction globally, demonstrating the flexibility and power of the fixed-point framework in analysis.", "problem": "Consider the space of all real-valued continuous functions on the interval $[0, T]$, which we denote by $C([0, T])$. For any real parameter $\\beta > 0$, we can define a weighted norm on this space as:\n$$ \\|y\\|_\\beta = \\sup_{t \\in [0, T]} |e^{-\\beta t} y(t)| $$\nThis norm induces a metric $d_\\beta(y_1, y_2) = \\|y_1 - y_2\\|_\\beta$, turning $(C([0, T]), d_\\beta)$ into a complete metric space.\n\nNow, let's analyze the Initial Value Problem (IVP) defined by the differential equation\n$$ y'(t) = L y(t) $$\nwith the initial condition $y(0) = 1$. For this problem, we are given the constant $L = 10.5$ and the interval endpoint $T=5$.\n\nThe IVP can be reformulated as a fixed-point problem using the Picard-LindelÃ¶f operator, $\\mathcal{P}$. This operator maps a function $y \\in C([0, T])$ to a new function $(\\mathcal{P}y) \\in C([0, T])$ according to the integral equation:\n$$ (\\mathcal{P}y)(t) = 1 + \\int_0^t L y(s) \\, ds $$\nA solution to the IVP is a function $y$ such that $\\mathcal{P}y = y$.\n\nThe operator $\\mathcal{P}$ is called a contraction mapping on the metric space $(C([0,T]), d_\\beta)$ if there exists a constant $k \\in [0, 1)$, known as the contraction constant, such that for any two functions $y_1, y_2 \\in C([0, T])$, the following inequality is satisfied:\n$$ \\|\\mathcal{P}y_1 - \\mathcal{P}y_2\\|_\\beta \\le k \\|y_1 - y_2\\|_\\beta $$\n\nYour task is to determine the smallest positive integer value of the parameter $\\beta$ that guarantees the Picard operator $\\mathcal{P}$ is a contraction mapping with a contraction constant $k \\le 0.5$.", "solution": "We consider the Picard operator defined by\n$$\n(\\mathcal{P}y)(t) = 1 + \\int_{0}^{t} L\\, y(s)\\, ds,\n$$\nwith $L \\in \\mathbb{R}$ constant, acting on $(C([0,T]),\\|\\cdot\\|_{\\beta})$ where\n$$\n\\|y\\|_{\\beta} = \\sup_{t \\in [0,T]} \\left|\\exp(-\\beta t)\\, y(t)\\right|.\n$$\nTo determine a contraction constant, take $y_{1}, y_{2} \\in C([0,T])$ and set $z = y_{1} - y_{2}$. Then for each $t \\in [0,T]$,\n$$\n(\\mathcal{P}y_{1} - \\mathcal{P}y_{2})(t) = \\int_{0}^{t} L\\, z(s)\\, ds.\n$$\nMultiply by $\\exp(-\\beta t)$ and take absolute values:\n$$\n\\left|\\exp(-\\beta t)\\, (\\mathcal{P}y_{1} - \\mathcal{P}y_{2})(t)\\right|\n\\le \\int_{0}^{t} |L|\\, \\exp(-\\beta t)\\, |z(s)|\\, ds.\n$$\nUsing the definition of the weighted norm, we have for all $s \\in [0,T]$,\n$$\n|z(s)| \\le \\exp(\\beta s)\\, \\|z\\|_{\\beta}.\n$$\nTherefore,\n$$\n\\left|\\exp(-\\beta t)\\, (\\mathcal{P}y_{1} - \\mathcal{P}y_{2})(t)\\right|\n\\le |L| \\int_{0}^{t} \\exp\\!\\big(-\\beta (t-s)\\big)\\, ds \\, \\|z\\|_{\\beta}.\n$$\nCompute the integral explicitly:\n$$\n\\int_{0}^{t} \\exp\\!\\big(-\\beta (t-s)\\big)\\, ds\n= \\frac{1 - \\exp(-\\beta t)}{\\beta}.\n$$\nTaking the supremum over $t \\in [0,T]$ yields\n$$\n\\|\\mathcal{P}y_{1} - \\mathcal{P}y_{2}\\|_{\\beta}\n\\le |L| \\left(\\sup_{t \\in [0,T]} \\frac{1 - \\exp(-\\beta t)}{\\beta}\\right) \\|y_{1} - y_{2}\\|_{\\beta}\n= |L| \\frac{1 - \\exp(-\\beta T)}{\\beta} \\|y_{1} - y_{2}\\|_{\\beta}.\n$$\nHence a valid contraction constant is\n$$\nk(\\beta) = |L| \\frac{1 - \\exp(-\\beta T)}{\\beta}.\n$$\nNote that the function $g(\\beta) = \\frac{1 - \\exp(-\\beta T)}{\\beta}$ is strictly decreasing for $\\beta > 0$, since\n$$\ng'(\\beta) = \\frac{\\exp(-\\beta T)(\\beta T + 1) - 1}{\\beta^{2}} < 0\n$$\nfor all $\\beta > 0$, using the inequality $\\exp(u) \\ge 1 + u$ with $u = \\beta T$ which implies $\\exp(-\\beta T)(\\beta T + 1) \\le 1$.\n\nWe are given $L = 10.5 = \\frac{21}{2}$ and $T = 5$, so\n$$\nk(\\beta) = \\frac{21}{2} \\cdot \\frac{1 - \\exp(-5 \\beta)}{\\beta}.\n$$\nWe require $k(\\beta) \\le \\frac{1}{2}$, i.e.\n$$\n\\frac{21}{2} \\cdot \\frac{1 - \\exp(-5 \\beta)}{\\beta} \\le \\frac{1}{2}\n\\quad \\Longleftrightarrow \\quad\n21\\big(1 - \\exp(-5 \\beta)\\big) \\le \\beta.\n$$\nTo find the smallest positive integer $\\beta$ that satisfies this, observe first that\n$$\n\\frac{1 - \\exp(-5 \\beta)}{\\beta} \\le \\frac{1}{\\beta},\n$$\nso a sufficient condition is\n$$\n\\frac{21}{2} \\cdot \\frac{1}{\\beta} \\le \\frac{1}{2}\n\\quad \\Longleftrightarrow \\quad\n\\beta \\ge 21.\n$$\nWe now check the boundary integers:\n- For $\\beta = 20$,\n$$\nk(20) = \\frac{21}{2} \\cdot \\frac{1 - \\exp(-100)}{20}\n= \\frac{21}{40}\\big(1 - \\exp(-100)\\big) > \\frac{21}{40} = 0.525 > 0.5,\n$$\nso $\\beta = 20$ fails.\n- For $\\beta = 21$,\n$$\nk(21) = \\frac{21}{2} \\cdot \\frac{1 - \\exp(-105)}{21}\n= \\frac{1}{2}\\big(1 - \\exp(-105)\\big) < \\frac{1}{2},\n$$\nso $\\beta = 21$ succeeds.\n\nSince $k(\\beta)$ is strictly decreasing in $\\beta$, the smallest positive integer $\\beta$ satisfying $k(\\beta) \\le \\frac{1}{2}$ is $\\beta = 21$.", "answer": "$$\\boxed{21}$$", "id": "1282586"}]}