{"hands_on_practices": [{"introduction": "This first practice demonstrates one of the most powerful tools in your analytical toolkit: the Weierstrass M-test. By finding a simple, convergent series of numbers that bounds our series of functions, we can elegantly prove uniform convergence. This exercise [@problem_id:2332404] illustrates the profound connection between uniform convergence and continuity, showing how a property of the individual terms can be passed on to the infinite sum.", "problem": "Consider the function $f(x)$ defined by the series of functions:\n$$f(x) = \\sum_{n=1}^{\\infty} \\frac{1}{n^2 + x^2}$$\nThe domain of this function is the set of all real numbers, $\\mathbb{R}$. Analyze the continuity of $f(x)$ on its domain. Which of the following statements is correct?\n\nA. The function $f(x)$ is continuous on $\\mathbb{R}$ because the series converges uniformly on $\\mathbb{R}$.\n\nB. The function $f(x)$ is continuous on $\\mathbb{R}$ because it is a sum of continuous functions, and any sum of continuous functions is also continuous.\n\nC. The function $f(x)$ is not continuous on $\\mathbb{R}$ because the series of functions does not converge uniformly on $\\mathbb{R}$.\n\nD. The function $f(x)$ is not continuous on $\\mathbb{R}$ because the series diverges when $x=0$.\n\nE. The continuity of $f(x)$ cannot be determined because the convergence of the series depends on the value of $x$.", "solution": "Let $g_{n}(x) = \\frac{1}{n^{2} + x^{2}}$ for each $n \\in \\mathbb{N}$. For every fixed $n$, the denominator satisfies $n^{2} + x^{2} \\geq n^{2} > 0$ for all $x \\in \\mathbb{R}$, hence $g_{n}$ is well defined and continuous on $\\mathbb{R}$.\n\nTo analyze the convergence of the series of functions $\\sum_{n=1}^{\\infty} g_{n}(x)$ uniformly on $\\mathbb{R}$, apply the Weierstrass M-test. For all $x \\in \\mathbb{R}$ and all $n \\in \\mathbb{N}$,\n$$\n0 \\leq g_{n}(x) = \\frac{1}{n^{2} + x^{2}} \\leq \\frac{1}{n^{2}} =: M_{n}.\n$$\nThe numerical series $\\sum_{n=1}^{\\infty} M_{n} = \\sum_{n=1}^{\\infty} \\frac{1}{n^{2}}$ converges. Therefore, by the Weierstrass M-test, the series $\\sum_{n=1}^{\\infty} g_{n}(x)$ converges uniformly on $\\mathbb{R}$.\n\nSince each $g_{n}$ is continuous on $\\mathbb{R}$ and the series converges uniformly on $\\mathbb{R}$, the sum\n$$\nf(x) = \\sum_{n=1}^{\\infty} \\frac{1}{n^{2} + x^{2}}\n$$\nis the uniform limit of continuous functions and hence is continuous on $\\mathbb{R}$.\n\nWe now evaluate the options:\n- A is true: the function is continuous on $\\mathbb{R}$ because the series converges uniformly on $\\mathbb{R}$ by the Weierstrass M-test.\n- B is false: an infinite sum of continuous functions is not necessarily continuous without uniform convergence; here continuity follows specifically from uniform convergence.\n- C is false: the series does converge uniformly on $\\mathbb{R}$, as shown.\n- D is false: at $x=0$, one has $f(0) = \\sum_{n=1}^{\\infty} \\frac{1}{n^{2}}$, which converges.\n- E is false: the continuity can be determined, and the series converges uniformly for all $x \\in \\mathbb{R}$.\n\nThus, the correct statement is A.", "answer": "$$\\boxed{A}$$", "id": "2332404"}, {"introduction": "Uniform convergence is a stricter condition than pointwise convergence, and understanding the difference is key. This exercise explores a classic sequence of functions that converges pointwise but not uniformly, using the intuitive model of a transient signal. By analyzing the behavior of the signal's peak intensity [@problem_id:1342740], you will see firsthand why the interchange of a limit and a supremum is not always permissible and why the \"uniform\" condition is so essential.", "problem": "In a simplified model of a transient physical process, the intensity of a signal at a normalized position $x \\in [0, 1]$ is described by the function $I_n(x) = n x (1-x)^n$, where $n$ is a positive integer parameter characterizing the process. For each value of $n$, the signal has a peak intensity, $M_n$, which is the maximum value of $I_n(x)$ on the interval $[0, 1]$. To assess the behavior of this system for very large parameter values, we are interested in the asymptotic limit of these peak intensities.\n\nCalculate the exact value of the limit $L = \\lim_{n \\to \\infty} M_n$.", "solution": "We consider $I_{n}(x) = n x (1-x)^{n}$ for $x \\in [0,1]$ and fixed positive integer $n$. The endpoints give $I_{n}(0) = 0$ and $I_{n}(1) = 0$. To find the interior extrema, compute the derivative:\n$$\nI_{n}'(x) = n (1-x)^{n} + n x \\cdot n (1-x)^{n-1} \\cdot (-1) = n (1-x)^{n-1} \\big( (1-x) - n x \\big).\n$$\nThus\n$$\nI_{n}'(x) = n (1-x)^{n-1} \\big( 1 - (n+1) x \\big).\n$$\nSetting $I_{n}'(x)=0$ yields either $1-x=0$ (i.e., $x=1$, an endpoint) or $1-(n+1)x=0$, which gives the unique interior critical point\n$$\nx^{\\ast} = \\frac{1}{n+1}.\n$$\nFor $x \\in (0,1)$, the factor $n(1-x)^{n-1}$ is positive, so the sign of $I_{n}'(x)$ is determined by $1-(n+1)x$. Hence $I_{n}'(x) > 0$ for $x < \\frac{1}{n+1}$ and $I_{n}'(x) < 0$ for $x > \\frac{1}{n+1}$. Therefore $x^{\\ast}$ is the unique maximizer on $(0,1)$, and the maximum value is\n$$\nM_{n} = I_{n}\\!\\left( \\frac{1}{n+1} \\right) = n \\cdot \\frac{1}{n+1} \\left( 1 - \\frac{1}{n+1} \\right)^{n} = \\frac{n}{n+1} \\left( \\frac{n}{n+1} \\right)^{n}.\n$$\nTo find the limit $L = \\lim_{n \\to \\infty} M_{n}$, we use the well-known limit $\\lim_{k \\to \\infty} (1+a/k)^k = e^a$. We can rewrite $M_n$ as:\n$$ M_n = \\frac{n}{n+1} \\left(1 - \\frac{1}{n+1}\\right)^n $$\nAs $n \\to \\infty$, the first term $\\frac{n}{n+1} \\to 1$. The second term's limit can be found by rewriting it to use the standard limit form:\n$$ \\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n+1}\\right)^n = \\lim_{n \\to \\infty} \\frac{\\left(1 - \\frac{1}{n+1}\\right)^{n+1}}{1 - \\frac{1}{n+1}} $$\nThe numerator approaches $\\exp(-1)$ and the denominator approaches $1$. Thus, the limit of the second factor is $\\exp(-1)$. Combining the factors, we have:\n$$ L = 1 \\cdot \\exp(-1) = \\exp(-1). $$\nThus the exact asymptotic limit is $\\exp(-1)$.", "answer": "$$\\boxed{\\exp(-1)}$$", "id": "1342740"}, {"introduction": "Now that we have a grasp of proving and identifying uniform convergence, we can explore its algebraic structure. Does the set of uniformly convergent sequences behave nicely under standard operations like multiplication? This problem [@problem_id:2332353] challenges you to investigate this question, revealing that an extra condition—boundedness of the limit functions—is necessary to guarantee that the product of two uniformly convergent sequences also converges uniformly.", "problem": "Let $(f_n)_{n=1}^\\infty$ and $(g_n)_{n=1}^\\infty$ be two sequences of real-valued functions defined on a non-empty set $E \\subseteq \\mathbb{R}$. Suppose that $(f_n)$ converges uniformly to a function $f$ on $E$, and $(g_n)$ converges uniformly to a function $g$ on $E$.\n\nLet $(h_n)_{n=1}^\\infty$ be the sequence of functions formed by the product of the corresponding functions in the original sequences, such that $h_n(x) = f_n(x) g_n(x)$ for all $x \\in E$. The pointwise limit of this sequence is the function $h(x) = f(x)g(x)$.\n\nUnder which one of the following additional conditions is the sequence $(h_n)$ guaranteed to converge uniformly to $h$ on the set $E$?\n\nA. The set $E$ is a closed and bounded interval.\n\nB. Each function $f_n$ and $g_n$ in the sequences is continuous on $E$.\n\nC. The set $E$ is the set of all real numbers, $\\mathbb{R}$.\n\nD. The limit functions $f$ and $g$ are bounded on $E$.\n\nE. The sequences $(f_n)$ and $(g_n)$ converge pointwise on $E$.", "solution": "The correct answer is D. We will first prove that condition D is sufficient to guarantee the uniform convergence of the product sequence $(h_n)$. Then, we will provide counterexamples to show that conditions A, B, and C are not sufficient, and explain why E is an incorrect choice.\n\n**Proof of Sufficiency for Condition D**\n\nWe want to show that if $f$ and $g$ are bounded, then $h_n = f_n g_n$ converges uniformly to $h = fg$ on $E$. By definition of uniform convergence, we need to show that for any $\\epsilon > 0$, there exists an integer $N$ such that for all $n \\ge N$, $|h_n(x) - h(x)| < \\epsilon$ for all $x \\in E$. This is equivalent to showing that $\\sup_{x \\in E} |h_n(x) - h(x)| \\to 0$ as $n \\to \\infty$.\n\nLet's start by analyzing the expression $|h_n(x) - h(x)|$:\n$$|h_n(x) - h(x)| = |f_n(x)g_n(x) - f(x)g(x)|$$\nWe can add and subtract the term $f_n(x)g(x)$ inside the absolute value, a standard technique often called \"adding zero\":\n$$|f_n(x)g_n(x) - f_n(x)g(x) + f_n(x)g(x) - f(x)g(x)|$$\nBy the triangle inequality, this is less than or equal to:\n$$|f_n(x)g_n(x) - f_n(x)g(x)| + |f_n(x)g(x) - f(x)g(x)|$$\nFactoring out common terms, we get:\n$$|f_n(x)| |g_n(x) - g(x)| + |g(x)| |f_n(x) - f(x)|$$\nNow, we use the given conditions.\nCondition D states that $f$ and $g$ are bounded on $E$. This means there exist positive constants $M_f$ and $M_g$ such that $|f(x)| \\le M_f$ and $|g(x)| \\le M_g$ for all $x \\in E$.\n\nThe sequence $(f_n)$ converges uniformly to $f$. This implies that for $\\epsilon_0 = 1$, there exists an integer $N_1$ such that for all $n \\ge N_1$ and for all $x \\in E$, we have $|f_n(x) - f(x)| < 1$.\nUsing the triangle inequality again, for $n \\ge N_1$, we can bound $|f_n(x)|$:\n$$|f_n(x)| = |f_n(x) - f(x) + f(x)| \\le |f_n(x) - f(x)| + |f(x)| < 1 + M_f$$\nLet $M'_f = 1 + M_f$. For $n \\ge N_1$, the sequence of functions $(f_n)$ is uniformly bounded by $M'_f$.\n\nSubstituting these bounds into our inequality for $|h_n(x) - h(x)|$, we have for all $n \\ge N_1$:\n$$|h_n(x) - h(x)| \\le M'_f |g_n(x) - g(x)| + M_g |f_n(x) - f(x)|$$\nThis inequality holds for all $x \\in E$. Therefore, we can take the supremum over $x \\in E$ on both sides:\n$$\\sup_{x \\in E}|h_n(x) - h(x)| \\le M'_f \\sup_{x \\in E}|g_n(x) - g(x)| + M_g \\sup_{x \\in E}|f_n(x) - f(x)|$$\nSince $f_n \\to f$ uniformly and $g_n \\to g$ uniformly, we know that:\n$$\\lim_{n \\to \\infty} \\sup_{x \\in E}|f_n(x) - f(x)| = 0$$\n$$\\lim_{n \\to \\infty} \\sup_{x \\in E}|g_n(x) - g(x)| = 0$$\nTherefore, the right-hand side of our supremum inequality approaches $M'_f \\cdot 0 + M_g \\cdot 0 = 0$ as $n \\to \\infty$. By the squeeze theorem, the left-hand side must also approach 0.\n$$\\lim_{n \\to \\infty} \\sup_{x \\in E}|h_n(x) - h(x)| = 0$$\nThis proves that $(h_n)$ converges uniformly to $h$ on $E$. Thus, condition D is sufficient.\n\n**Analysis of Other Options**\n\nA. **The set $E$ is a closed and bounded interval.** This is not sufficient. A counterexample can be constructed with an unbounded limit function on a compact set. Let $E=[0,1]$. Define a function $f$ on $E$ as $f(x) = 1/x$ for $x \\in (0, 1]$ and $f(0) = 0$. This function is unbounded on $E$.\nLet $f_n(x) = f(x)$ for all $n$, and let $g_n(x) = f(x) + 1/n$.\n- The sequence $(f_n)$ converges uniformly to $f$ since $|f_n(x) - f(x)| = 0$ for all $x$ and $n$.\n- The sequence $(g_n)$ converges uniformly to $f$ since $\\sup_{x\\in E} |g_n(x) - f(x)| = \\sup_{x\\in E} |1/n| = 1/n$, which tends to 0.\n- The product sequence is $h_n(x) = f_n(x)g_n(x) = f(x)(f(x) + 1/n) = (f(x))^2 + f(x)/n$.\n- The limit of the product is $h(x) = (f(x))^2$.\n- The difference is $|h_n(x) - h(x)| = |f(x)/n|$.\n- To check for uniform convergence, we look at the supremum: $\\sup_{x \\in E} |h_n(x) - h(x)| = \\frac{1}{n}\\sup_{x \\in E}|f(x)|$. Since $f$ is unbounded on $E$, this supremum is infinite for every $n$. Thus, the convergence is not uniform. This counterexample on a closed and bounded interval shows that condition A is not sufficient.\n\nB. and C. **Each function $f_n, g_n$ is continuous** and **$E=\\mathbb{R}$**. These are not sufficient. Consider the following counterexample which addresses both simultaneously. Let $E = \\mathbb{R}$. Define $f_n(x) = x + 1/n$ and $g_n(x) = x + 1/n$.\n- Each $f_n$ and $g_n$ is a linear function, hence continuous on $\\mathbb{R}$. This addresses B.\n- The domain is $\\mathbb{R}$. This addresses C.\n- The limit functions are $f(x) = \\lim_{n\\to\\infty} (x+1/n) = x$ and $g(x) = x$.\n- The convergence is uniform: $\\sup_{x \\in \\mathbb{R}}|f_n(x) - f(x)| = \\sup_{x \\in \\mathbb{R}}|1/n| = 1/n \\to 0$.\n- The product sequence is $h_n(x) = (x+1/n)^2 = x^2 + 2x/n + 1/n^2$. The limit is $h(x) = x^2$.\n- The difference is $|h_n(x) - h(x)| = |2x/n + 1/n^2|$.\n- The supremum of this difference over $E=\\mathbb{R}$ is $\\sup_{x \\in \\mathbb{R}}|2x/n + 1/n^2| = \\infty$ for any fixed $n$. Therefore, the convergence is not uniform. The limit functions $f(x)=x$ and $g(x)=x$ are not bounded on $\\mathbb{R}$, which is the reason for failure.\n\nE. **The sequences $(f_n)$ and $(g_n)$ converge pointwise on $E$.** The problem premise states that the sequences converge uniformly. Uniform convergence is a stronger condition than pointwise convergence, meaning it implies pointwise convergence. Therefore, this condition is already satisfied by the premises of the problem and adds no new information. It cannot be the required *additional* condition.", "answer": "$$\\boxed{D}$$", "id": "2332353"}]}