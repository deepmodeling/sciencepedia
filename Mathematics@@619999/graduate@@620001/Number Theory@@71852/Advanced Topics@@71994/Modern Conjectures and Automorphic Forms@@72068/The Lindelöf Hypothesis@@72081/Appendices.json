{"hands_on_practices": [{"introduction": "The functional equation for the Riemann zeta function, $\\zeta(s)$, is the cornerstone of its modern study, revealing a profound symmetry about the critical line $\\Re(s) = 1/2$. This practice guides you through a classical derivation, mirroring one of Riemann's original proofs that connects $\\zeta(s)$ to the Jacobi theta function. By mastering this derivation [@problem_id:3027774], you will gain a deep appreciation for the analytic tools that underpin the entire theory, including the LindelÃ¶f hypothesis.", "problem": "Let $\\zeta(s)$ denote the Riemann zeta function and $\\Gamma(s)$ the Euler gamma function. Consider the Jacobi theta function defined for $t>0$ by $\\theta(t)=\\sum_{n\\in\\mathbb{Z}}\\exp(-\\pi n^{2} t)$. You may use the following foundational facts without proof: (i) the Poisson summation formula applied to the Gaussian implies the theta transformation $\\theta(t)=t^{-1/2}\\theta(1/t)$ for all $t>0$; (ii) for $\\operatorname{Re}(s)>0$ and $a>0$, $\\int_{0}^{\\infty}\\exp(-a t)\\, t^{s-1}\\, dt=\\Gamma(s)\\, a^{-s}$; (iii) the Euler reflection formula $\\Gamma(s)\\Gamma(1-s)=\\pi/\\sin(\\pi s)$ and the Legendre duplication formula $\\Gamma(z)\\Gamma(z+1/2)=2^{1-2z}\\sqrt{\\pi}\\,\\Gamma(2z)$. Starting from these facts and no others, perform the following tasks for complex $s$ with $0<\\operatorname{Re}(s)<1$.\n\n1. Show that the Mellin transform identity\n$$\\frac{1}{2}\\int_{0}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt=\\pi^{-\\frac{s}{2}}\\Gamma\\!\\left(\\frac{s}{2}\\right)\\zeta(s)$$\nholds by interchanging sum and integral on the left-hand side and justifying absolute convergence.\n\n2. Use the theta transformation $\\theta(t)=t^{-1/2}\\theta(1/t)$, together with an appropriate change of variables and splitting of the integral at $t=1$, to prove the functional equation in its symmetric form\n$$\\Lambda(s)=\\Lambda(1-s),\\quad\\text{where}\\quad \\Lambda(s)=\\pi^{-\\frac{s}{2}}\\Gamma\\!\\left(\\frac{s}{2}\\right)\\zeta(s).$$\n\n3. Rearrange the symmetric functional equation to the form $\\zeta(s)=\\chi(s)\\,\\zeta(1-s)$ and explicitly compute the factor $\\chi(s)$ as a closed-form analytic expression in terms of elementary functions and $\\Gamma$. Simplify your expression using only the reflection and duplication formulas stated above.\n\nProvide your final expression for $\\chi(s)$ in a single closed form. No numerical approximation is required, and no rounding is needed. Your final answer must be a single analytic expression.", "solution": "The problem is assessed to be valid as it is scientifically grounded in established mathematical principles of analytic number theory, is well-posed with clear objectives, and is formulated using precise, objective language. All given information and definitions are standard and consistent. I will proceed with the solution.\n\nThe solution is presented in three parts, as requested by the problem statement.\n\n**Part 1: Mellin Transform Identity**\n\nWe are asked to prove the identity $\\frac{1}{2}\\int_{0}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt=\\pi^{-\\frac{s}{2}}\\Gamma\\!\\left(\\frac{s}{2}\\right)\\zeta(s)$.\nWe begin with the left-hand side (LHS). The Jacobi theta function is defined as $\\theta(t)=\\sum_{n\\in\\mathbb{Z}}\\exp(-\\pi n^{2} t)$.\nThe term $\\theta(t)-1$ can be written as:\n$$ \\theta(t)-1 = \\left(\\sum_{n\\in\\mathbb{Z}}\\exp(-\\pi n^{2} t)\\right) - 1 = \\left(1 + \\sum_{n \\in \\mathbb{Z}, n \\neq 0}\\exp(-\\pi n^{2} t)\\right) - 1 $$\n$$ = \\sum_{n \\in \\mathbb{Z}, n \\neq 0}\\exp(-\\pi n^{2} t) = \\sum_{n=1}^{\\infty}\\exp(-\\pi (-n)^{2} t) + \\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t) = 2\\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t) $$\nSubstituting this into the integral on the LHS gives:\n$$ \\text{LHS} = \\frac{1}{2}\\int_{0}^{\\infty}\\left(2\\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t)\\right)\\, t^{\\frac{s}{2}-1}\\, dt = \\int_{0}^{\\infty}\\sum_{n=1}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\frac{s}{2}-1}\\, dt $$\nWe now interchange the order of summation and integration. This step is justified by the Fubini-Tonelli theorem if the integral of the sum of absolute values converges. For real $t>0$, the terms are positive, so we must check the convergence of $\\sum_{n=1}^{\\infty}\\int_{0}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\operatorname{Re}(s/2)-1}\\, dt$.\nLet's proceed formally with the interchange:\n$$ \\text{LHS} = \\sum_{n=1}^{\\infty}\\int_{0}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\frac{s}{2}-1}\\, dt $$\nFor each integral in the sum, we use the given identity $\\int_{0}^{\\infty}\\exp(-a t)\\, t^{z-1}\\, dt=\\Gamma(z)\\, a^{-z}$ with $z = s/2$ and $a = \\pi n^2$.\n$$ \\int_{0}^{\\infty}\\exp(-\\pi n^{2} t)\\, t^{\\frac{s}{2}-1}\\, dt = \\Gamma\\left(\\frac{s}{2}\\right) (\\pi n^2)^{-s/2} = \\Gamma\\left(\\frac{s}{2}\\right) \\pi^{-s/2} (n^2)^{-s/2} = \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) n^{-s} $$\nThis is valid for $\\operatorname{Re}(s/2) > 0$, i.e., $\\operatorname{Re}(s) > 0$.\nSubstituting this back into the sum:\n$$ \\text{LHS} = \\sum_{n=1}^{\\infty} \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) n^{-s} = \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) \\sum_{n=1}^{\\infty} n^{-s} $$\nThe sum $\\sum_{n=1}^{\\infty} n^{-s}$ is the definition of the Riemann zeta function, $\\zeta(s)$, which converges for $\\operatorname{Re}(s)>1$.\nTherefore, for $\\operatorname{Re}(s)>1$, we have:\n$$ \\text{LHS} = \\pi^{-s/2} \\Gamma\\left(\\frac{s}{2}\\right) \\zeta(s) $$\nThis confirms the identity. The interchange of summation and integration is justified for $\\operatorname{Re}(s)>1$ because the sum $\\sum_{n=1}^{\\infty} n^{-\\operatorname{Re}(s)}$ converges. Although the problem is stated for $0 < \\operatorname{Re}(s) < 1$, the identity is established in the region of absolute convergence $\\operatorname{Re}(s)>1$ and then extended by analytic continuation to other values of $s$.\n\n**Part 2: Symmetric Functional Equation**\n\nWe start with the identity from Part 1, writing $\\Lambda(s) = \\pi^{-s/2}\\Gamma(s/2)\\zeta(s)$:\n$$ \\Lambda(s) = \\frac{1}{2}\\int_{0}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt $$\nThe integral defines an analytic function of $s$. To obtain its analytic continuation, we split the integral at $t=1$:\n$$ \\Lambda(s) = \\frac{1}{2}\\int_{0}^{1}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt + \\frac{1}{2}\\int_{1}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt $$\nWe transform the first integral using the given theta transformation $\\theta(t)=t^{-1/2}\\theta(1/t)$:\n$$ \\int_{0}^{1}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt = \\int_{0}^{1}\\left(t^{-1/2}\\theta(1/t) - 1\\right)\\, t^{\\frac{s}{2}-1}\\, dt = \\int_{0}^{1} t^{s/2-3/2}\\theta(1/t)\\, dt - \\int_{0}^{1} t^{s/2-1}\\, dt $$\nThe second term is $\\int_{0}^{1} t^{s/2-1}\\, dt = \\left[\\frac{t^{s/2}}{s/2}\\right]_0^1 = \\frac{2}{s}$ for $\\operatorname{Re}(s)>0$.\nFor the first term, we make the change of variables $u=1/t$, so $t=1/u$ and $dt=-u^{-2}du$. The limits of integration change from $[0, 1]$ to $[\\infty, 1]$.\n$$ \\int_{0}^{1} t^{s/2-3/2}\\theta(1/t)\\, dt = \\int_{\\infty}^{1} (u^{-1})^{s/2-3/2}\\theta(u)\\,(-u^{-2}du) = \\int_{1}^{\\infty} u^{-s/2+3/2}\\theta(u)\\,u^{-2}du = \\int_{1}^{\\infty} u^{-s/2-1/2}\\theta(u)\\,du $$\nWe rewrite $\\theta(u) = (\\theta(u)-1)+1$:\n$$ \\int_{1}^{\\infty} u^{-s/2-1/2}\\big((\\theta(u)-1)+1\\big)\\,du = \\int_{1}^{\\infty} (\\theta(u)-1)u^{-s/2-1/2}\\,du + \\int_{1}^{\\infty} u^{-s/2-1/2}\\,du $$\nThe second integral is $\\int_{1}^{\\infty} u^{-(s+1)/2}\\,du = \\left[\\frac{u^{-(s+1)/2+1}}{-(s+1)/2+1}\\right]_1^\\infty = \\left[\\frac{u^{(1-s)/2}}{(1-s)/2}\\right]_1^\\infty$. For $\\operatorname{Re}(1-s)<0$, or $\\operatorname{Re}(s)>1$, this evaluates to $0 - \\frac{1}{(1-s)/2} = -\\frac{2}{1-s}$. By analytic continuation, this result holds.\nCombining these results for the integral from $0$ to $1$:\n$$ \\frac{1}{2}\\int_{0}^{1}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt = \\frac{1}{2}\\left( \\int_{1}^{\\infty} (\\theta(t)-1)t^{-s/2-1/2}\\,dt - \\frac{2}{1-s} \\right) - \\frac{1}{2}\\left(\\frac{2}{s}\\right) = \\frac{1}{2}\\int_{1}^{\\infty} (\\theta(t)-1)t^{-(s+1)/2}\\,dt - \\frac{1}{1-s} - \\frac{1}{s} $$\nNow, we substitute this back into the expression for $\\Lambda(s)$:\n$$ \\Lambda(s) = \\left( \\frac{1}{2}\\int_{1}^{\\infty} (\\theta(t)-1)t^{-(s+1)/2}\\,dt - \\frac{1}{s} - \\frac{1}{1-s} \\right) + \\frac{1}{2}\\int_{1}^{\\infty}\\big(\\theta(t)-1\\big)\\, t^{\\frac{s}{2}-1}\\, dt $$\n$$ \\Lambda(s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{(s/2)-1} + t^{(-s/2)-(1/2)} \\right)dt - \\left(\\frac{1}{s} + \\frac{1}{1-s}\\right) $$\nThe term in parentheses can be written as $t^{s/2-1} + t^{-(s+1)/2} = t^{s/2-1} + t^{(1-s-2)/2} = t^{s/2-1} + t^{(1-s)/2 - 1}$. The term $-\\left(\\frac{1}{s} + \\frac{1}{1-s}\\right) = -\\frac{1-s+s}{s(1-s)} = -\\frac{1}{s(1-s)}$.\nLet's define $F(s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{s/2-1} + t^{(1-s)/2-1} \\right)dt - \\frac{1}{s(1-s)}$.\nThis expression for $\\Lambda(s)$ is valid for all $s$ where the integral converges. Since $\\theta(t)-1$ decays exponentially as $t\\to\\infty$, the integral converges for all complex $s$. This expression gives the analytic continuation of $\\Lambda(s)$ to the whole complex plane.\nWe now check the symmetry under the transformation $s \\mapsto 1-s$:\n$$ F(1-s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{(1-s)/2-1} + t^{(1-(1-s))/2-1} \\right)dt - \\frac{1}{(1-s)(1-(1-s))} $$\n$$ F(1-s) = \\frac{1}{2}\\int_{1}^{\\infty}(\\theta(t)-1)\\left( t^{(1-s)/2-1} + t^{s/2-1} \\right)dt - \\frac{1}{(1-s)s} = F(s) $$\nSince $\\Lambda(s) = F(s)$, we have shown that $\\Lambda(s) = \\Lambda(1-s)$.\n\n**Part 3: Calculation of $\\chi(s)$**\n\nWe start from the symmetric functional equation $\\Lambda(s)=\\Lambda(1-s)$, where $\\Lambda(s)=\\pi^{-s/2}\\Gamma(s/2)\\zeta(s)$.\n$$ \\pi^{-s/2}\\Gamma\\left(\\frac{s}{2}\\right)\\zeta(s) = \\pi^{-(1-s)/2}\\Gamma\\left(\\frac{1-s}{2}\\right)\\zeta(1-s) $$\nWe are looking for $\\chi(s)$ such that $\\zeta(s)=\\chi(s)\\zeta(1-s)$. By rearranging the equation, we get:\n$$ \\chi(s) = \\frac{\\pi^{-(1-s)/2}\\Gamma\\left(\\frac{1-s}{2}\\right)}{\\pi^{-s/2}\\Gamma\\left(\\frac{s}{2}\\right)} = \\pi^{-1/2+s/2+s/2} \\frac{\\Gamma\\left(\\frac{1-s}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = \\pi^{s-1/2} \\frac{\\Gamma\\left(\\frac{1-s}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} $$\nTo simplify this expression, we use the given Euler reflection and Legendre duplication formulas for the Gamma function. A useful identity derived from them connects $\\Gamma(s)$ and $\\Gamma(1-s)$ arguments:\nFrom duplication, $\\Gamma(s) = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2)\\Gamma((s+1)/2)$.\nFrom reflection, $\\Gamma(s)\\Gamma(1-s)=\\pi/\\sin(\\pi s)$. Also $\\sin(\\pi s) = 2\\sin(\\pi s/2)\\cos(\\pi s/2)$.\nCombining these: $\\frac{\\pi}{\\Gamma(1-s)\\sin(\\pi s)} = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2)\\Gamma((s+1)/2)$.\n$$ \\frac{\\pi}{\\Gamma(1-s)2\\sin(\\pi s/2)\\cos(\\pi s/2)} = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2)\\Gamma((s+1)/2) $$\nAnother reflection formula identity for $z=(s+1)/2$ is $\\Gamma((s+1)/2)\\Gamma(1-(s+1)/2) = \\pi/\\sin(\\pi(s+1)/2)$, which simplifies to $\\Gamma((s+1)/2)\\Gamma((1-s)/2) = \\pi/\\cos(\\pi s/2)$.\nWe can write $\\Gamma((s+1)/2) = \\frac{\\pi}{\\cos(\\pi s/2)\\Gamma((1-s)/2)}$. Substituting this into the combined identity:\n$$ \\frac{\\pi}{\\Gamma(1-s)2\\sin(\\pi s/2)\\cos(\\pi s/2)} = 2^{s-1}\\pi^{-1/2}\\Gamma(s/2) \\frac{\\pi}{\\cos(\\pi s/2)\\Gamma((1-s)/2)} $$\nWe can cancel $\\pi$ and $\\cos(\\pi s/2)$ from both sides (for $s$ not an odd integer):\n$$ \\frac{1}{\\Gamma(1-s)2\\sin(\\pi s/2)} = \\frac{2^{s-1}\\pi^{-1/2}\\Gamma(s/2)}{\\Gamma((1-s)/2)} $$\nWe want to find the ratio $\\frac{\\Gamma((1-s)/2)}{\\Gamma(s/2)}$. Rearranging the equation:\n$$ \\frac{\\Gamma((1-s)/2)}{\\Gamma(s/2)} = \\Gamma(1-s) \\cdot 2\\sin(\\pi s/2) \\cdot 2^{s-1}\\pi^{-1/2} = \\Gamma(1-s)\\sin(\\pi s/2) 2^s \\pi^{-1/2} $$\nNow we substitute this ratio back into our expression for $\\chi(s)$:\n$$ \\chi(s) = \\pi^{s-1/2} \\left( 2^s \\pi^{-1/2} \\Gamma(1-s)\\sin\\left(\\frac{\\pi s}{2}\\right) \\right) $$\n$$ \\chi(s) = 2^s \\pi^{s-1/2-1/2} \\Gamma(1-s)\\sin\\left(\\frac{\\pi s}{2}\\right) $$\n$$ \\chi(s) = 2^s \\pi^{s-1} \\sin\\left(\\frac{\\pi s}{2}\\right) \\Gamma(1-s) $$\nThis is the required closed-form expression for $\\chi(s)$.", "answer": "$$\\boxed{2^{s} \\pi^{s-1} \\sin\\left(\\frac{\\pi s}{2}\\right) \\Gamma(1-s)}$$", "id": "3027774"}, {"introduction": "The LindelÃ¶f hypothesis is fundamentally a statement about immense cancellation among the terms of the Dirichlet series for $\\zeta(s)$ on the critical line. This exercise [@problem_id:3027778] makes this abstract principle tangible by analyzing the mean-square of a finite Dirichlet polynomial that models $\\zeta(1/2+it)$. You will demonstrate how the oscillatory \"off-diagonal\" terms average to zero, a phenomenon that explains the relatively slow growth of the mean-square of $\\zeta(1/2+it)$ and provides strong heuristic support for the hypothesis itself.", "problem": "Let $T \\geq 3$ be a large real parameter and define the Dirichlet polynomial\n$$\nS_{T}(t) \\coloneqq \\sum_{n \\leq \\sqrt{T}} n^{-1/2 - i t}.\n$$\nWrite\n$$\n|S_{T}(t)|^{2} \\;=\\; \\sum_{m \\leq \\sqrt{T}} \\sum_{n \\leq \\sqrt{T}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big) \\;=\\; D_{T} \\;+\\; \\mathrm{Off}_{T}(t),\n$$\nwhere the diagonal part is\n$$\nD_{T} \\coloneqq \\sum_{n \\leq \\sqrt{T}} n^{-1},\n$$\nand the off-diagonal part is\n$$\n\\mathrm{Off}_{T}(t) \\coloneqq \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big).\n$$\nStarting only from core properties of exponential integrals and basic estimates for logarithms, compute the following averaged off-diagonal contribution:\n$$\nL \\coloneqq \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{T}^{2T} \\mathrm{Off}_{T}(t)\\, dt.\n$$\nYour derivation must be self-contained and justify all steps, including any exchange of limits and summations and any use of comparison tests.\n\nFinally, explain, based on your computation and standard facts such as the approximate functional equation for the Riemann zeta function $\\zeta(s)$, how cancellation among off-diagonal terms influences mean-square bounds for $|\\zeta(1/2 + i t)|$ and why this is consistent with the LindelÃ¶f hypothesis. Provide the exact value of $L$ as your final answer. No rounding is required.", "solution": "The problem asks for the computation of a limit $L$ related to the mean value of the off-diagonal part of the squared modulus of a Dirichlet polynomial, and an explanation of its connection to the theory of the Riemann zeta function.\n\nFirst, we validate the problem. The problem is formulated in the context of analytic number theory, specifically concerning mean value theorems for Dirichlet polynomials. All definitions are standard and mathematically sound. The decomposition of $|S_T(t)|^2$ into diagonal and off-diagonal parts is a standard technique. The question is well-posed, objective, and scientifically grounded. It requires a rigorous mathematical derivation based on fundamental principles. Therefore, the problem is valid and we may proceed with the solution.\n\nThe primary task is to compute the limit:\n$$\nL \\coloneqq \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{T}^{2T} \\mathrm{Off}_{T}(t)\\, dt\n$$\nwhere\n$$\n\\mathrm{Off}_{T}(t) \\coloneqq \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big).\n$$\nLet us denote the expression inside the limit by $L_T$.\n$$\nL_T \\coloneqq \\frac{1}{T} \\int_{T}^{2T} \\left( \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\exp\\!\\big(-i t \\ln(n/m)\\big) \\right) dt.\n$$\nThe sum is over a finite number of terms, specifically $(\\lfloor\\sqrt{T}\\rfloor)^2 - \\lfloor\\sqrt{T}\\rfloor$ terms. The integrand in each term is a continuous function of $t$. Therefore, we can interchange the order of summation and integration:\n$$\nL_T = \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} (mn)^{-1/2} \\frac{1}{T} \\int_{T}^{2T} \\exp\\!\\big(-i t \\ln(n/m)\\big) dt.\n$$\nSince $m \\neq n$, the term $\\ln(n/m)$ is non-zero. We can evaluate the integral directly:\n$$\n\\int_{T}^{2T} \\exp\\!\\big(-i t \\ln(n/m)\\big) dt = \\left[ \\frac{\\exp(-i t \\ln(n/m))}{-i \\ln(n/m)} \\right]_{t=T}^{t=2T} = \\frac{\\exp(-2iT \\ln(n/m)) - \\exp(-iT \\ln(n/m))}{-i \\ln(n/m)}.\n$$\nSubstituting this back into the expression for $L_T$:\n$$\nL_T = \\frac{1}{T} \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} \\frac{(mn)^{-1/2}}{-i \\ln(n/m)} \\left( \\exp\\!\\big(-2iT \\ln(n/m)\\big) - \\exp\\!\\big(-iT \\ln(n/m)\\big) \\right).\n$$\nTo find the limit $L = \\lim_{T \\to \\infty} L_T$, we will bound the magnitude of $L_T$. Using the triangle inequality and the fact that $|\\exp(ix)| = 1$ for real $x$, we have:\n$$\n|\\exp\\!\\big(-2iT \\ln(n/m)\\big) - \\exp\\!\\big(-iT \\ln(n/m)\\big)| \\leq |\\exp\\!\\big(-2iT \\ln(n/m)\\big)| + |\\exp\\!\\big(-iT \\ln(n/m)\\big)| \\leq 1 + 1 = 2.\n$$\nThus, the magnitude of $L_T$ is bounded by:\n$$\n|L_T| \\leq \\frac{1}{T} \\sum_{\\substack{m,n \\leq \\sqrt{T} \\\\ m \\neq n}} \\frac{(mn)^{-1/2}}{|\\ln(n/m)|} \\cdot 2.\n$$\nLet $X = \\lfloor\\sqrt{T}\\rfloor$. The sum can be written as:\n$$\n\\sum_{\\substack{1 \\leq m,n \\leq X \\\\ m \\neq n}} \\frac{(mn)^{-1/2}}{|\\ln(n/m)|}.\n$$\nBy swapping the roles of $m$ and $n$, we note that the sum over $m<n$ is equal to the sum over $n<m$, because $|\\ln(n/m)| = |\\ln(m/n)|$. So, we can write:\n$$\n|L_T| \\leq \\frac{4}{T} \\sum_{1 \\leq m < n \\leq X} \\frac{(mn)^{-1/2}}{\\ln(n/m)}.\n$$\nLet us estimate the sum $S(X) = \\sum_{1 \\leq m < n \\leq X} \\frac{(mn)^{-1/2}}{\\ln(n/m)}$. We split the sum over $n$ into two ranges: $m < n \\leq 2m$ and $2m < n \\leq X$.\n\nCase 1: $2m < n \\leq X$. This requires $m < X/2$. In this range, $n/m > 2$, so $\\ln(n/m) > \\ln(2)$.\n$$\n\\sum_{m=1}^{\\lfloor X/2 \\rfloor} \\sum_{n=2m+1}^{X} \\frac{(mn)^{-1/2}}{\\ln(n/m)} \\leq \\frac{1}{\\ln(2)} \\sum_{m=1}^{\\lfloor X/2 \\rfloor} m^{-1/2} \\sum_{n=2m+1}^{X} n^{-1/2}.\n$$\nUsing the comparison of a sum with an integral, $\\sum_{k=a}^b k^{-1/2} \\leq \\int_{a-1}^b x^{-1/2} dx = 2(\\sqrt{b}-\\sqrt{a-1})$ for $a>1$. A simpler bound is $\\sum_{k=a}^b k^{-1/2} \\leq \\int_{a-1}^b x^{-1/2} dx = O(\\sqrt{b})$. The inner sum is $\\sum_{n=2m+1}^X n^{-1/2} = O(\\sqrt{X})$. The outer sum is then $\\sum_{m=1}^{\\lfloor X/2 \\rfloor} m^{-1/2} O(\\sqrt{X}) = O(\\sqrt{X}) \\sum_{m=1}^{\\lfloor X/2 \\rfloor} m^{-1/2} = O(\\sqrt{X}) \\cdot O(\\sqrt{X/2}) = O(X)$.\n\nCase 2: $m < n \\leq 2m$. Let $n=m+k$ for $1 \\leq k \\leq m$. We use the inequality $\\ln(1+u) \\geq \\frac{u}{2}$ for $u \\in [0, 1]$. Since $k/m \\in (0, 1]$, we have $\\ln(n/m) = \\ln(1+k/m) \\geq \\frac{k}{2m}$.\nThe term $(mn)^{-1/2}$ is $\\frac{1}{\\sqrt{m(m+k)}} \\approx \\frac{1}{m}$ for $k \\leq m$.\nThe sum for this case is $\\sum_{m=1}^{X-1} \\sum_{n=m+1}^{\\min(X, 2m)} \\frac{(mn)^{-1/2}}{\\ln(n/m)}$.\n$$\n\\sum_{n=m+1}^{\\min(X, 2m)} \\frac{(mn)^{-1/2}}{\\ln(n/m)} = \\sum_{k=1}^{\\min(X-m, m)} \\frac{1}{\\sqrt{m(m+k)}\\ln(1+k/m)} \\leq \\sum_{k=1}^{m} \\frac{1}{\\sqrt{m^2}} \\frac{2m}{k} = \\sum_{k=1}^{m} \\frac{2}{k}.\n$$\nThe sum $\\sum_{k=1}^m \\frac{2}{k} = O(\\ln m)$. Summing over $m$ from $1$ to $X-1$:\n$$\n\\sum_{m=1}^{X-1} O(\\ln m) \\approx \\int_1^X \\ln x \\, dx = [x \\ln x - x]_1^X = X \\ln X - X + 1 = O(X \\ln X).\n$$\nCombining both cases, the total sum is $S(X) = O(X) + O(X \\ln X) = O(X \\ln X)$.\nSince $X = \\lfloor\\sqrt{T}\\rfloor$, we have $S(\\lfloor\\sqrt{T}\\rfloor) = O(\\sqrt{T}\\ln(\\sqrt{T})) = O(\\sqrt{T}\\ln T)$.\nSubstituting this bound back into the inequality for $|L_T|$:\n$$\n|L_T| \\leq \\frac{4}{T} S(\\lfloor\\sqrt{T}\\rfloor) = \\frac{4}{T} O(\\sqrt{T} \\ln T) = O\\left(\\frac{\\ln T}{\\sqrt{T}}\\right).\n$$\nAs $T \\to \\infty$, the term $(\\ln T)/\\sqrt{T}$ approaches $0$. Therefore, by the Squeeze Theorem, we have:\n$$\nL = \\lim_{T \\to \\infty} L_T = 0.\n$$\nThe average contribution of the off-diagonal terms is zero.\n\nNow, we explain the significance of this result in the context of the Riemann zeta function $\\zeta(s)$ and the LindelÃ¶f hypothesis.\n\nThe Dirichlet polynomial $S_T(t) = \\sum_{n \\leq \\sqrt{T}} n^{-1/2 - i t}$ serves as a model for the Riemann zeta function $\\zeta(1/2+it)$. According to the approximate functional equation for $\\zeta(s)$, for large $t$, $\\zeta(1/2+it)$ can be approximated by a sum of two such Dirichlet polynomials of length about $\\sqrt{t/(2\\pi)}$. Thus, the mean-square behavior of $|S_T(t)|^2$ with $T \\approx t$ is indicative of the behavior of $|\\zeta(1/2+it)|^2$.\n\nWe have shown that $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_T^{2T} \\mathrm{Off}_T(t) dt = 0$. This means that, on average, the off-diagonal terms do not contribute to the mean value of $|S_T(t)|^2$. The entire contribution comes from the diagonal part, $D_T = \\sum_{n \\leq \\sqrt{T}} n^{-1}$.\nThe mean value of the diagonal part is simply:\n$$\n\\frac{1}{T} \\int_T^{2T} D_T dt = D_T = \\sum_{n \\leq \\sqrt{T}} \\frac{1}{n} \\approx \\ln(\\sqrt{T}) + \\gamma = \\frac{1}{2}\\ln T + \\gamma,\n$$\nwhere $\\gamma$ is the Euler-Mascheroni constant.\nThis implies $\\frac{1}{T} \\int_T^{2T} |S_T(t)|^2 dt \\sim \\frac{1}{2} \\ln T$. This result for the model polynomial mirrors the classic result of Hardy and Littlewood for the zeta function itself: $\\frac{1}{T}\\int_0^T |\\zeta(1/2+it)|^2 dt \\sim \\ln T$.\n\nThe LindelÃ¶f hypothesis is the conjecture that for any $\\epsilon > 0$, $|\\zeta(1/2+it)| = O(|t|^\\epsilon)$ as $|t| \\to \\infty$. A trivial estimation using the triangle inequality on the Dirichlet series for $\\zeta(1/2+it)$ gives $|\\zeta(1/2+it)| \\le \\sum_{n \\le t} n^{-1/2} = O(t^{1/2})$, far from the conjectured bound. The LindelÃ¶f hypothesis is thus a profound statement about the vast amount of cancellation among the terms $n^{-it}$ due to their oscillating phases.\n\nOur calculation of $L=0$ is a precise demonstration of this cancellation principle, albeit in an averaged (mean-square) sense. The `off-diagonal` terms in $|S_T(t)|^2$ are all formed by products like $m^{-1/2-it} n^{-1/2+it}$ with $m \\ne n$. These terms contain oscillatory factors $\\exp(-it\\ln(n/m))$. The fact that their integral average is zero shows that these oscillations lead to large-scale cancellation over long intervals of $t$. If these terms did not cancel, the mean-square value of $\\zeta(1/2+it)$ could grow much faster, potentially as fast as $O(T)$, which would correspond to $|\\zeta(1/2+it)|$ having a typical size of $O(t^{1/2})$. The logarithmic growth of the mean square, which results from the cancellation of off-diagonal terms, is consistent with the much smaller growth rate of $|\\zeta(1/2+it)|$ predicted by the LindelÃ¶f hypothesis. In summary, cancellation among off-diagonal terms is the essential mechanism that tames the growth of the zeta function on the critical line.", "answer": "$$\n\\boxed{0}\n$$", "id": "3027778"}, {"introduction": "While mean-value theorems provide crucial information about the average size of $\\zeta(1/2+it)$, they do not on their own imply the pointwise bounds predicted by the LindelÃ¶f hypothesis. This practice [@problem_id:3027768] challenges you to formalize this crucial distinction using rigorous arguments from real analysis. Understanding why an $L^2$ bound (from a mean-square theorem) does not preclude the existence of rare, large values is key to appreciating the profound depth of the subconvexity problem and the LindelÃ¶f hypothesis.", "problem": "Let $\\zeta(s)$ denote the Riemann zeta function, defined for $\\Re(s)>1$ by $\\zeta(s)=\\sum_{n=1}^{\\infty}n^{-s}$ and extended to a meromorphic function on $\\mathbb{C}$ with a simple pole at $s=1$. A standard mean square formula states that\n$$\n\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt \\asymp T\\log T\n$$\nas $T\\to\\infty$. Consider the pointwise (uniform-in-$t$) bound\n$$\n\\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll_{\\epsilon} T^{\\epsilon}\\quad\\text{for every }\\epsilon>0,\n$$\nwhich is the type of uniform control asserted by the LindelÃ¶f hypothesis. Using only fundamental definitions and well-tested facts such as the mean square formula and basic inequalities from real analysis (for example, Chebyshevâs inequality and HÃ¶lderâs inequality), determine which of the following statements correctly explain why such a uniform supremum bound is not implied by the mean square formula and requires additional uniform control of the values of $\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)$.\n\nChoose all that apply.\n\nA. The mean square estimate controls the average size of $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|$ but allows concentration of large values on sets of very small measure; therefore it cannot alone prevent rare spikes exceeding $T^{\\epsilon}$, so a uniform supremum bound requires additional information such as the LindelÃ¶f hypothesis.\n\nB. By the CauchyâSchwarz inequality one has $\\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll\\left(\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}dt\\right)^{1/2}$, so the mean square formula already implies the desired bound after optimizing $\\epsilon$.\n\nC. The continuity of $\\zeta(s)$ on the line $\\Re(s)=\\tfrac{1}{2}$ implies boundedness on compact intervals, and hence, together with the mean square formula, it forces $\\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll_{\\epsilon} T^{\\epsilon}$.\n\nD. Chebyshevâs inequality applied to $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}$ yields\n$$\n\\operatorname{meas}\\left\\{t\\in[0,T]:\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ge T^{\\epsilon}\\right\\}\\ll \\frac{T\\log T}{T^{2\\epsilon}},\n$$\nwhich does not rule out the existence of $t$ with $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ge T^{\\epsilon}$ for each $T$, so a uniform supremum bound is not a consequence of the mean square alone.\n\nE. The PhragmÃ©nâLindelÃ¶f convexity principle combined with the functional equation gives $ \\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll t^{1/4+\\epsilon} $, and this immediately implies the desired bound $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll t^{\\epsilon}$.", "solution": "The user wants me to act as a meticulous STEM professor, critically analyzing a problem in analytic number theory concerning the Riemann zeta function.\n\n### Step 1: Extract Givens\n*   **Function:** The Riemann zeta function, $\\zeta(s)$, defined by $\\zeta(s)=\\sum_{n=1}^{\\infty}n^{-s}$ for $\\Re(s)>1$ and extended meromorphically to $\\mathbb{C}$.\n*   **Pole:** A simple pole at $s=1$.\n*   **Mean Square Formula:** $\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt \\asymp T\\log T$ as $T\\to\\infty$. This means there exist positive constants $c_1, c_2$ such that for all sufficiently large $T$, $c_1 T\\log T \\le \\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt \\le c_2 T\\log T$.\n*   **Pointwise Bound (LindelÃ¶f Hypothesis type):** $\\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll_{\\epsilon} T^{\\epsilon}$ for every $\\epsilon>0$. This is equivalent to $|\\zeta(\\frac{1}{2}+it)| \\ll_{\\epsilon} |t|^{\\epsilon}$ for $|t| \\ge 1$.\n*   **Question:** Determine which statements correctly explain why the mean square formula does not, on its own, imply the pointwise bound.\n*   **Allowed Tools:** Fundamental definitions, the mean square formula, and basic real analysis inequalities (e.g., Chebyshev, HÃ¶lder).\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective. It concerns standard, well-established concepts and results in analytic number theory: the definition of $\\zeta(s)$, the classical mean-square theorem of Hardy and Littlewood, and the LindelÃ¶f hypothesis. The core of the question is to distinguish between the analytical implications of an $L^2$-norm estimate (an integral average) and an $L^\\infty$-norm estimate (a uniform supremum). This is a fundamental and non-trivial topic in mathematical analysis. The problem does not violate any principles of scientific or mathematical rigor. It is not incomplete, contradictory, unrealistic, or ill-posed.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed to provide a full solution, including an analysis of each option.\n\n### Principle-Based Derivation\nThe central issue is the relationship between different Lebesgue norms, specifically the $L^2$-norm and the $L^\\infty$-norm. Let $f(t) = \\zeta(\\frac{1}{2}+it)$. The mean square formula provides information about the $L^2$-norm of $f$ over the interval $[0, T]$:\n$$\n\\|f\\|_{L^2([0,T])}^2 = \\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt \\asymp T\\log T\n$$\nThis implies that the average value of $|\\zeta(\\frac{1}{2}+it)|^2$ on $[0, T]$ is of the order $\\log T$, because\n$$\n\\frac{1}{T}\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt \\asymp \\log T\n$$\nThe LindelÃ¶f hypothesis concerns the $L^\\infty$-norm of $f$ over the interval $[0, T]$:\n$$\n\\|f\\|_{L^\\infty([0,T])} = \\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\n$$\nThe hypothesis states that this quantity is bounded by $T^\\epsilon$ for any $\\epsilon > 0$.\n\nIn general, for a function space over a domain of measure $T$, there is no inequality that bounds the $L^\\infty$-norm by the $L^2$-norm without a strong dependence on $T$. A function can have a small $L^2$-norm while having a very large $L^\\infty$-norm if its large values are concentrated on a set of very small measure. For instance, consider a function $g_T(t)$ on $[0, T]$ that is $\\sqrt{T \\log T}/\\sqrt{\\delta}$ on a small interval of length $\\delta$ and $0$ elsewhere. Its $L^2$-integral is $(\\sqrt{T \\log T}/\\sqrt{\\delta})^2 \\cdot \\delta = T \\log T$, matching the growth rate for $\\zeta$. However, its maximum value is $\\sqrt{T \\log T}/\\sqrt{\\delta}$, which can be made arbitrarily large by choosing $\\delta$ to be small enough (e.g., if $\\delta = T^{-M}$, the maximum value is $T^{(M+1)/2} \\sqrt{\\log T}$).\n\nTherefore, an integral bound like the mean square formula, which describes average behavior, cannot by itself rule out the existence of rare, exceptionally large \"spikes\" in the function's value. To control the supremum norm, one needs additional information that guarantees a certain degree of \"uniformity\" and prevents such concentrations. The LindelÃ¶f hypothesis is precisely such a statement of uniform control.\n\n### Option-by-Option Analysis\n\n**A. The mean square estimate controls the average size of $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|$ but allows concentration of large values on sets of very small measure; therefore it cannot alone prevent rare spikes exceeding $T^{\\epsilon}$, so a uniform supremum bound requires additional information such as the LindelÃ¶f hypothesis.**\n\nThis statement accurately describes the analytical reason why an $L^2$ bound is insufficient to establish an $L^\\infty$ bound. The mean square formula bounds the average of $|\\zeta(\\frac{1}{2}+it)|^2$, but this average can be achieved even if the function has very large values on a small set, as long as it is smaller elsewhere. An estimate on the average does not constrain the maximum. The reasoning is perfectly sound and aligns with the principle-based derivation above.\n\n**Verdict: Correct**\n\n**B. By the CauchyâSchwarz inequality one has $\\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll\\left(\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}dt\\right)^{1/2}$, so the mean square formula already implies the desired bound after optimizing $\\epsilon$.**\n\nThis statement asserts a false mathematical inequality. There is no general principle or theorem in analysis that states $\\max_{t\\in[0,T]} |f(t)| \\ll (\\int_0^T |f(t)|^2 dt)^{1/2}$. As demonstrated in the derivation above with a simple \"spike\" function, the left side can be made arbitrarily larger than the right side. For instance, for our function $g_T(t)$, the maximum is $\\sqrt{T \\log T}/\\sqrt{\\delta}$ while the square root of the integral is $\\sqrt{T \\log T}$. The inequality would require $1/\\sqrt{\\delta}$ to be bounded by a constant, which is false for arbitrarily small $\\delta>0$. The Cauchy-Schwarz inequality is misapplied or misinterpreted here.\n\n**Verdict: Incorrect**\n\n**C. The continuity of $\\zeta(s)$ on the line $\\Re(s)=\\tfrac{1}{2}$ implies boundedness on compact intervals, and hence, together with the mean square formula, it forces $\\max_{0\\le t\\le T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll_{\\epsilon} T^{\\epsilon}$.**\n\nThe function $t \\mapsto \\zeta(\\frac{1}{2}+it)$ is continuous for $t \\in \\mathbb{R}$. By the extreme value theorem, it is bounded on any compact interval $[0, T]$. However, this merely guarantees that for any *fixed* $T < \\infty$, the maximum value is finite. It provides absolutely no information about how this maximum value grows as a function of $T$. The problem is precisely to bound this growth rate as $T \\to \\infty$. Continuity on its own is a topological property that does not yield asymptotic bounds. Combining it with the mean square formula does not change this conclusion, as explained in the analysis of option A.\n\n**Verdict: Incorrect**\n\n**D. Chebyshevâs inequality applied to $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}$ yields\n$$\n\\operatorname{meas}\\left\\{t\\in[0,T]:\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ge T^{\\epsilon}\\right\\}\\ll \\frac{T\\log T}{T^{2\\epsilon}},\n$$\nwhich does not rule out the existence of $t$ with $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ge T^{\\epsilon}$ for each $T$, so a uniform supremum bound is not a consequence of the mean square alone.**\n\nThis statement provides a rigorous, quantitative argument based on a standard inequality. Let $g(t) = |\\zeta(\\frac{1}{2}+it)|^2$ and let the threshold be $A = (T^\\epsilon)^2 = T^{2\\epsilon}$. The general form of Chebyshev's inequality (also known as Markov's inequality for integrals) states that for a non-negative function $g$, the measure of the set where $g$ exceeds a threshold $A$ is at most the integral of $g$ divided by $A$.\n$$\n\\operatorname{meas}\\left\\{t\\in[0,T]: g(t) \\ge A\\right\\} \\le \\frac{1}{A}\\int_0^T g(t)\\,dt\n$$\nSubstituting our functions and bounds:\n$$\n\\operatorname{meas}\\left\\{t\\in[0,T]:\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^2 \\ge T^{2\\epsilon}\\right\\} \\le \\frac{1}{T^{2\\epsilon}}\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt\n$$\nUsing $\\int_{0}^{T}\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|^{2}\\,dt \\asymp T\\log T$, we indeed get that the measure is $\\ll \\frac{T\\log T}{T^{2\\epsilon}} = T^{1-2\\epsilon}\\log T$.\nThe crucial part of the argument is the conclusion: for $\\epsilon \\in (0, 1/2)$, the bound $T^{1-2\\epsilon}\\log T$ is positive for $T>1$. The inequality only shows that the set of points where $|\\zeta(\\frac{1}{2}+it)|$ is large has a measure that is small relative to $T$, but it does not show that this set is empty. Therefore, it does not preclude the possibility that $\\max_{t \\in [0,T]}|\\zeta(\\frac{1}{2}+it)| \\ge T^\\epsilon$. This is a correct and precise formalization of the argument in option A.\n\n**Verdict: Correct**\n\n**E. The PhragmÃ©nâLindelÃ¶f convexity principle combined with the functional equation gives $ \\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll t^{1/4+\\epsilon} $, and this immediately implies the desired bound $\\bigl|\\zeta\\!\\left(\\tfrac{1}{2}+it\\right)\\bigr|\\ll t^{\\epsilon}$.**\n\nThis statement contains a correct fact followed by a completely false conclusion. The \"convexity bound\" $|\\zeta(\\frac{1}{2}+it)| \\ll |t|^{1/4+\\epsilon'}$ (for any $\\epsilon'>0$) is a well-known, standard result that serves as a baseline for estimates on the critical line. However, the claim that this *implies* the LindelÃ¶f hypothesis, $|\\zeta(\\frac{1}{2}+it)| \\ll |t|^\\epsilon$, is absurd. The LindelÃ¶f hypothesis is equivalent to showing that the exponent can be taken to be arbitrarily small, whereas the convexity bound establishes an exponent of $1/4$. Reducing this exponent has been a central goal of research for over a century, with the current best result being an exponent slightly less than $1/6$. To claim that $t^{1/4+\\epsilon'} \\ll t^\\epsilon$ is an \"immediate implication\" is to trivialize one of the deepest open problems in number theory.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{AD}$$", "id": "3027768"}]}