{"hands_on_practices": [{"introduction": "Many proofs in analytic number theory, including that of the Bombieri-Vinogradov theorem, involve splitting a complex sum into more manageable parts, often called 'Type I' and 'Type II' sums using tools like Vaughan's identity. This exercise [@problem_id:3025077] demonstrates the crucial tactical step of choosing a parameter to optimally balance the error contributions from these different parts. Mastering this balancing act is key to obtaining the strongest possible results.", "problem": "Let $x$ be a large real parameter and let $Q$ satisfy $Q \\leq x^{1/2} (\\ln x)^{-B_{0}}$ for some fixed $B_{0} > 0$. In a proof of the Bombieri-Vinogradov theorem, one applies Vaughan’s identity with a splitting parameter $U$ and the Large Sieve inequality to obtain an upper bound for the averaged discrepancy of primes in arithmetic progressions of the form\n$$\n\\sum_{q \\leq Q} \\max_{(a,q)=1} \\left| \\psi(x; q, a) - \\frac{x}{\\varphi(q)} \\right| \\ll \\mathcal{E}(U),\n$$\nwhere, up to powers of $\\ln x$ that you may treat as harmless, the bound decomposes into a Type I contribution and a Type II contribution given by\n$$\n\\mathcal{E}(U) \\asymp A \\, x \\, Q \\, U^{-1} \\;+\\; B \\, x^{1/2} \\, Q \\, U^{1/2},\n$$\nfor some fixed positive constants $A$ and $B$ depending only on the implicit constants in the application of Vaughan’s identity and the Large Sieve inequality. Assume $U$ is a real parameter constrained by $1 \\leq U \\leq x$, and you may ignore all polylogarithmic factors in $x$ and $Q$ when optimizing.\n\nStarting from the standard forms of Vaughan’s identity and the Large Sieve inequality as foundational tools, and without invoking any additional pre-packaged optimization shortcuts, determine the exact analytic expression for the choice of $U$ that asymptotically minimizes the main size of $\\mathcal{E}(U)$ when the polylogarithmic factors are suppressed. Your final answer should be a single closed-form expression for the optimal $U$ in terms of $x$, $A$, and $B$. Do not include any inequalities in your final answer.", "solution": "The user has provided a problem from analytic number theory concerning the optimization of a parameter in a proof of the Bombieri-Vinogradov theorem.\n\n### Step 1: Extract Givens\n- Let $x$ be a large real parameter.\n- Let $Q$ satisfy $Q \\leq x^{1/2} (\\ln x)^{-B_{0}}$ for some fixed $B_{0} > 0$.\n- The averaged discrepancy of primes is bounded as:\n$$\n\\sum_{q \\leq Q} \\max_{(a,q)=1} \\left| \\psi(x; q, a) - \\frac{x}{\\varphi(q)} \\right| \\ll \\mathcal{E}(U)\n$$\n- The error bound $\\mathcal{E}(U)$ has the asymptotic form:\n$$\n\\mathcal{E}(U) \\asymp A \\, x \\, Q \\, U^{-1} \\;+\\; B \\, x^{1/2} \\, Q \\, U^{1/2}\n$$\n- $A$ and $B$ are fixed positive constants.\n- $U$ is a real parameter constrained by $1 \\leq U \\leq x$.\n- Polylogarithmic factors in $x$ and $Q$ may be ignored.\n- The task is to find the exact analytic expression for the choice of $U$ that asymptotically minimizes $\\mathcal{E}(U)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n\n- **Scientifically Grounded:** The problem is firmly rooted in analytic number theory. The Bombieri-Vinogradov theorem, the Chebyshev function $\\psi(x; q, a)$, Vaughan's identity, and the Large Sieve inequality are all standard, well-established concepts and tools in this field. The decomposition of the error term into Type I ($U^{-1}$ dependence) and Type II ($U^{1/2}$ dependence) sums is a characteristic feature of proofs employing Vaughan's identity. The provided functional form of $\\mathcal{E}(U)$ is a simplified but standard model for the expression that needs to be optimized in this context. The problem is scientifically sound.\n\n- **Well-Posed:** The task is to minimize a given function of a single variable $U$ over a specified interval. The function is continuous and differentiable on the interior of its domain. The constants are well-defined. This is a standard optimization problem that admits a unique solution.\n\n- **Objective:** The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n\nBased on this analysis, the problem is deemed valid as it is scientifically grounded, well-posed, and objective. It does not violate any of the invalidity criteria. Therefore, I will proceed to the solution.\n\n### Step 3: Derivation of the Optimal Parameter $U$\n\nThe goal is to find the value of the parameter $U$ that minimizes the error term $\\mathcal{E}(U)$. As instructed, we ignore any polylogarithmic factors and treat the asymptotic relation as an equality for the purpose of optimization. Let the function to be minimized be $f(U)$, defined for $U \\in [1, x]$:\n$$\nf(U) = A x Q U^{-1} + B x^{1/2} Q U^{1/2}\n$$\nHere, $A$, $B$, $x$, and $Q$ are treated as positive constants with respect to the optimization variable $U$.\n\nTo find the minimum, we employ differential calculus. We first compute the derivative of $f(U)$ with respect to $U$:\n$$\n\\frac{df}{dU} = \\frac{d}{dU} \\left( A x Q U^{-1} + B x^{1/2} Q U^{1/2} \\right)\n$$\nUsing the power rule for differentiation, $(c u^n)' = c n u^{n-1}$, we get:\n$$\n\\frac{df}{dU} = A x Q (-1) U^{-2} + B x^{1/2} Q \\left(\\frac{1}{2}\\right) U^{-1/2}\n$$\n$$\n\\frac{df}{dU} = - A x Q U^{-2} + \\frac{1}{2} B x^{1/2} Q U^{-1/2}\n$$\nTo find the critical points, we set the derivative equal to zero:\n$$\n- A x Q U^{-2} + \\frac{1}{2} B x^{1/2} Q U^{-1/2} = 0\n$$\nWe can rearrange the equation to solve for $U$:\n$$\n\\frac{1}{2} B x^{1/2} Q U^{-1/2} = A x Q U^{-2}\n$$\nSince $Q$ is positive (it is a sum limit), we can divide both sides by $Q$. Similarly, we can divide by other non-zero constants.\n$$\n\\frac{1}{2} B x^{1/2} U^{-1/2} = A x U^{-2}\n$$\nTo solve for $U$, we can multiply both sides by $2U^2$ to eliminate the negative powers of $U$:\n$$\nB x^{1/2} U^{2 - 1/2} = 2 A x\n$$\n$$\nB x^{1/2} U^{3/2} = 2 A x\n$$\nNow, we isolate $U^{3/2}$:\n$$\nU^{3/2} = \\frac{2 A x}{B x^{1/2}} = \\frac{2A}{B} x^{1 - 1/2} = \\frac{2A}{B} x^{1/2}\n$$\nFinally, we raise both sides to the power of $\\frac{2}{3}$ to find $U$:\n$$\nU = \\left( \\frac{2A}{B} x^{1/2} \\right)^{2/3}\n$$\n$$\nU = \\left( \\frac{2A}{B} \\right)^{2/3} (x^{1/2})^{2/3} = \\left( \\frac{2A}{B} \\right)^{2/3} x^{1/3}\n$$\nThis is the critical point. To confirm that it corresponds to a minimum, we can examine the second derivative, $\\frac{d^2f}{dU^2}$:\n$$\n\\frac{d^2f}{dU^2} = \\frac{d}{dU} \\left( - A x Q U^{-2} + \\frac{1}{2} B x^{1/2} Q U^{-1/2} \\right)\n$$\n$$\n\\frac{d^2f}{dU^2} = - A x Q (-2) U^{-3} + \\frac{1}{2} B x^{1/2} Q \\left(-\\frac{1}{2}\\right) U^{-3/2}\n$$\n$$\n\\frac{d^2f}{dU^2} = 2 A x Q U^{-3} - \\frac{1}{4} B x^{1/2} Q U^{-3/2}\n$$\nAt the critical point $U_{0} = \\left( \\frac{2A}{B} \\right)^{2/3} x^{1/3}$, we know that $A x Q U_{0}^{-2} = \\frac{1}{2} B x^{1/2} Q U_{0}^{-1/2}$. Substituting this into the expression for the second derivative:\n$$\n\\frac{d^2f}{dU^2}\\bigg|_{U=U_0} = \\frac{2}{U_0} (A x Q U_0^{-2}) - \\frac{1}{4} B x^{1/2} Q U_0^{-3/2} = \\frac{2}{U_0} \\left( \\frac{1}{2} B x^{1/2} Q U_0^{-1/2} \\right) - \\frac{1}{4} B x^{1/2} Q U_0^{-3/2}\n$$\n$$\n= B x^{1/2} Q U_0^{-3/2} - \\frac{1}{4} B x^{1/2} Q U_0^{-3/2} = \\frac{3}{4} B x^{1/2} Q U_0^{-3/2}\n$$\nSince $A, B, x, Q$ are positive constants, the optimal value $U_0$ is also positive. Therefore, $\\frac{d^2f}{dU^2}|_{U=U_0} > 0$, confirming that this critical point is a local minimum. As it is the only critical point for $U>0$, it is the global minimum.\n\nThe problem specifies that $x$ is large. For large $x$, the optimal value $U = \\left( \\frac{2A}{B} \\right)^{2/3} x^{1/3}$ satisfies the constraint $1 \\leq U \\leq x$, since $x^{1/3}$ grows slower than $x$ but faster than a constant. Thus, the unconstrained minimum lies within the specified interval for sufficiently large $x$.\n\nThe exact analytic expression for the optimal choice of $U$ is therefore:\n$$\nU = \\left( \\frac{2A}{B} \\right)^{2/3} x^{1/3}\n$$\nThis choice of $U$ balances the contributions from the Type I and Type II sums to minimize the overall error bound.", "answer": "$$\\boxed{\\left( \\frac{2A}{B} \\right)^{2/3} x^{1/3}}$$", "id": "3025077"}, {"introduction": "After establishing a bound on the error term for a 'dyadic block' of moduli—a range of the form $(Q, 2Q]$—the next step in proving the Bombieri-Vinogradov theorem is to combine these individual estimates. This practice [@problem_id:3025102] simulates this process, showing how the total error is controlled by summing over all blocks up to a carefully chosen cutoff. It reveals the trade-off that determines how large an average the theorem can handle.", "problem": "Let $x$ be a sufficiently large real number. For integers $q \\ge 1$ and $a$ with $(a,q)=1$, let $\\varphi(q)$ denote Euler’s totient function and let the Chebyshev function in arithmetic progressions be defined by\n$$\n\\psi(x;q,a) \\;=\\; \\sum_{\\substack{n \\le x \\\\ n \\equiv a \\,(\\bmod\\, q)}} \\Lambda(n),\n$$\nwhere $\\Lambda(n)$ is the von Mangoldt function. A standard approach to the Bombieri-Vinogradov theorem (Bombieri-Vinogradov (BV) theorem) decomposes the sum over moduli into dyadic blocks $(Q,2Q]$ and uses a large sieve type input to bound the contribution from each block. Assume the following dyadic block bound holds for all $A \\ge 1$, all $Q$ with $1 \\le Q \\le x^{1/2}$, and some fixed absolute constants $C>0$ and integer $K \\ge 1$:\n$$\n\\sum_{q \\in (Q,2Q]} \\max_{(a,q)=1} \\left|\\psi(x;q,a) - \\frac{x}{\\varphi(q)}\\right|\n\\;\\le\\;\nC\\left( \\frac{x}{(\\ln x)^{A+1}} \\;+\\; Q^{2} \\, (\\ln x)^{K} \\right).\n$$\nSet a cutoff\n$$\nQ_{\\max} \\;=\\; x^{1/2} (\\ln x)^{-B},\n$$\nwhere $B > 0$ is a parameter to be chosen. Consider the total contribution obtained by summing the displayed dyadic bound over all dyadic blocks $(Q,2Q]$ with $Q \\le Q_{\\max}$.\n\nStarting from the fundamental definitions above and using only well-tested facts about dyadic decompositions and geometric series, determine the smallest real value $B_{\\star} = B_{\\star}(A,K)$ such that the total contribution from all blocks with $Q \\le Q_{\\max}$ is bounded by a constant multiple of $x(\\ln x)^{-A}$ as $x \\to \\infty$.\n\nYour final answer must be a single closed-form expression for $B_{\\star}(A,K)$ with no rounding required.", "solution": "We begin by summing the given dyadic block bound over all dyadic $Q$ up to $Q_{\\max} = x^{1/2} (\\ln x)^{-B}$. Write the set of dyadic parameters as $Q_{j} = 2^{j}$ for integers $j$ with $Q_{j} \\le Q_{\\max}$. The number of dyadic blocks is\n$$\nJ \\;=\\; \\left\\lfloor \\log_{2} Q_{\\max} \\right\\rfloor + 1 \\;\\asymp\\; \\ln Q_{\\max} \\;\\ll\\; \\ln x,\n$$\nsince $Q_{\\max} = x^{1/2} (\\ln x)^{-B}$ and hence $\\ln Q_{\\max} = \\tfrac{1}{2} \\ln x - B \\ln \\ln x \\ll \\ln x$ for fixed $B$.\n\nSumming the block bound over $j$ with $Q_{j} \\le Q_{\\max}$ gives\n\\begin{align*}\n\\sum_{\\substack{Q \\text{ dyadic}\\\\ Q \\le Q_{\\max}}}\n\\sum_{q \\in (Q,2Q]} \\max_{(a,q)=1} \\left|\\psi(x;q,a) - \\frac{x}{\\varphi(q)}\\right|\n&\\;\\le\\; \\sum_{\\substack{Q \\text{ dyadic}\\\\ Q \\le Q_{\\max}}}\nC\\left( \\frac{x}{(\\ln x)^{A+1}} + Q^{2} (\\ln x)^{K} \\right) \\\\\n&=\\; C \\sum_{\\substack{Q \\text{ dyadic}\\\\ Q \\le Q_{\\max}}} \\frac{x}{(\\ln x)^{A+1}}\n\\;+\\; C (\\ln x)^{K} \\sum_{\\substack{Q \\text{ dyadic}\\\\ Q \\le Q_{\\max}}} Q^{2}.\n\\end{align*}\n\nWe control each sum separately.\n\nFor the first sum,\n$$\n\\sum_{\\substack{Q \\text{ dyadic}\\\\ Q \\le Q_{\\max}}} \\frac{x}{(\\ln x)^{A+1}}\n\\;=\\; \\frac{x}{(\\ln x)^{A+1}} \\cdot J\n\\;\\ll\\; \\frac{x}{(\\ln x)^{A+1}} \\cdot \\ln x\n\\;=\\; x \\, (\\ln x)^{-A}.\n$$\nThus the total contribution of the first term across all dyadic blocks is $\\ll x (\\ln x)^{-A}$.\n\nFor the second sum, note that for dyadic $Q$ the sum $\\sum_{Q \\le Q_{\\max}} Q^{2}$ is dominated by the largest term and is comparable to $Q_{\\max}^{2}$, because\n$$\n\\sum_{j=0}^{J} (2^{j})^{2} \\;=\\; \\sum_{j=0}^{J} 4^{j} \\;=\\; \\frac{4^{J+1}-1}{3} \\;\\asymp\\; 4^{J} \\;\\asymp\\; Q_{\\max}^{2}.\n$$\nHence\n$$\n(\\ln x)^{K} \\sum_{\\substack{Q \\text{ dyadic}\\\\ Q \\le Q_{\\max}}} Q^{2}\n\\;\\ll\\; (\\ln x)^{K} \\, Q_{\\max}^{2}\n\\;=\\; (\\ln x)^{K} \\, \\left( x^{1/2} (\\ln x)^{-B} \\right)^{2}\n\\;=\\; x \\, (\\ln x)^{K - 2B}.\n$$\n\nTo ensure that this second contribution is $\\ll x (\\ln x)^{-A}$, we require\n$$\n(\\ln x)^{K - 2B} \\;\\ll\\; (\\ln x)^{-A},\n$$\nwhich, in terms of exponents, is equivalent to\n$$\nK - 2B \\;\\le\\; -A.\n$$\nRearranging gives\n$$\n2B \\;\\ge\\; A + K \\quad\\Longleftrightarrow\\quad B \\;\\ge\\; \\frac{A+K}{2}.\n$$\n\nCombining the two parts, the total contribution from all dyadic blocks $(Q,2Q]$ with $Q \\le Q_{\\max}$ is $\\ll x (\\ln x)^{-A}$ provided that $B \\ge \\frac{A+K}{2}$. The smallest real value that satisfies this constraint is\n$$\nB_{\\star}(A,K) \\;=\\; \\frac{A+K}{2}.\n$$\n\nThis completes the calculation of the dependency of $B$ on $A$ ensuring the target bound.", "answer": "$$\\boxed{\\frac{A+K}{2}}$$", "id": "3025102"}, {"introduction": "While theoretical proofs provide profound insight, computational experiments can offer a tangible feel for the deep truths of number theory. This hands-on programming challenge [@problem_id:3025116] bridges the gap between the abstract statement of the Bombieri-Vinogradov theorem and its numerical consequences. By implementing a diagnostic tool to measure equidistribution, you will directly observe how the theorem's 'on average' guarantee plays out with real numbers.", "problem": "Let $x$ be a large real parameter and let $q$ be a positive integer modulus. For each reduced residue class $a$ modulo $q$ with $\\gcd(a,q)=1$, define the progression-restricted Chebyshev function by\n$$\n\\theta(x;q,a) \\;=\\; \\sum_{\\substack{p \\le x \\\\ p \\equiv a \\,(\\bmod q)}} \\log p,\n$$\nand recall Euler’s totient function $\\varphi(q)$. Consider the aggregate deviation\n$$\nS_{\\theta}(x,Q) \\;=\\; \\sum_{1 \\le q \\le Q} \\; \\max_{\\substack{0 \\le a < q \\\\ \\gcd(a,q)=1}} \\left| \\theta(x;q,a) \\;-\\; \\frac{x}{\\varphi(q)} \\right|.\n$$\nThe Bombieri–Vinogradov theorem (BV) states that, on average over moduli $q$ up to a scale tied to $x^{1/2}$, the prime distribution in arithmetic progressions behaves as if primes were equidistributed among the reduced residue classes. Without appealing to any unproven hypotheses, a computational test of this equidistribution can be designed by examining how $S_{\\theta}(x,Q)$ scales with $x$ and $\\log x$ as $Q$ varies with $x$.\n\nYour task is to implement a program that, for given triples $(x,\\alpha,A)$, computes the following normalized diagnostic based only on the above definitions:\n$$\nM_A(x,Q) \\;=\\; \\frac{(\\log x)^A}{x} \\, S_{\\theta}(x,Q),\n$$\nwhere $Q$ is determined from $(x,\\alpha)$ by\n$$\nQ \\;=\\; \\left\\lfloor \\frac{x^{1/2}}{(\\log x)^{\\alpha}} \\right\\rfloor,\n$$\nwith the convention that $Q$ is set to $1$ if the right-hand side is less than $1$. All logarithms are the natural logarithm.\n\nThis diagnostic $M_A(x,Q)$ scales the aggregate deviation $S_{\\theta}(x,Q)$ by $x$ and $(\\log x)^A$ to reflect average-error normalization suggested by the qualitative content of the Bombieri–Vinogradov theorem (BV), without using any explicit inequality from the theorem in the computation. Your program must compute $M_A(x,Q)$ exactly as defined, using only the definitions of $\\theta(x;q,a)$ and $\\varphi(q)$, for each test case.\n\nDesign your implementation from first principles:\n- Use only the definitions of $\\theta(x;q,a)$, $\\varphi(q)$, and basic prime generation to compute the sums and maxima exactly as defined.\n- Ensure that the computation over all $q \\le Q$ is efficient enough to handle $Q$ near $x^{1/2}$.\n- Use the natural logarithm for all occurrences of $\\log$.\n\nTest Suite:\nCompute $M_A(x,Q)$ for the following four parameter triples $(x,\\alpha,A)$:\n1. $(x,\\alpha,A) = (10^6, 1.0, 1.0)$.\n2. $(x,\\alpha,A) = (3 \\cdot 10^5, 0.0, 1.0)$.\n3. $(x,\\alpha,A) = (5 \\cdot 10^4, 0.0, 2.0)$.\n4. $(x,\\alpha,A) = (2 \\cdot 10^3, 3.0, 1.0)$.\n\nInput and Output:\n- There is no input. Your program must hard-code the above test suite.\n- Your program must produce a single line of output containing the results for the four test cases as a comma-separated list of floating-point values, in the order listed, enclosed in square brackets. For example, the format must be exactly like\n$[m_1,m_2,m_3,m_4]$\nwhere each $m_i$ is the computed value of $M_A(x,Q)$ for the corresponding test case. Express each $m_i$ as a decimal with exactly six digits after the decimal point.\n\nNotes:\n- The implementation must be universal and purely mathematical, independent of any physical units.\n- Ensure numerical stability and avoid redundant recomputation by reusing prime data across test cases where applicable.\n- Your design should include careful handling of the boundary case $Q=1$.", "solution": "The problem as stated is subjected to validation.\n\n### Step 1: Extract Givens\n- **Chebyshev function $\\theta(x;q,a)$**: Defined for a real parameter $x$, positive integer modulus $q$, and reduced residue class $a$ modulo $q$ ($\\gcd(a,q)=1$) as $\\theta(x;q,a) = \\sum_{\\substack{p \\le x \\\\ p \\equiv a \\,(\\bmod q)}} \\log p$, where the sum is over primes $p$.\n- **Euler's totient function $\\varphi(q)$**: The count of positive integers up to a given integer $q$ that are relatively prime to $q$.\n- **Aggregate deviation $S_{\\theta}(x,Q)$**: Defined as $S_{\\theta}(x,Q) = \\sum_{1 \\le q \\le Q} \\; \\max_{\\substack{0 \\le a < q \\\\ \\gcd(a,q)=1}} \\left| \\theta(x;q,a) - \\frac{x}{\\varphi(q)} \\right|$.\n- **Scale parameter $Q$**: Determined from a pair $(x,\\alpha)$ by $Q = \\left\\lfloor \\frac{x^{1/2}}{(\\log x)^{\\alpha}} \\right\\rfloor$. If $\\frac{x^{1/2}}{(\\log x)^{\\alpha}}  1$, then $Q$ is set to $1$.\n- **Normalized diagnostic $M_A(x,Q)$**: Defined for a triple $(x,\\alpha,A)$ as $M_A(x,Q) = \\frac{(\\log x)^A}{x} \\, S_{\\theta}(x,Q)$.\n- **Logarithm**: All occurrences of $\\log$ denote the natural logarithm.\n- **Task**: To implement a program that computes $M_A(x,Q)$ for a given test suite.\n- **Test Suite**: Four parameter triples $(x,\\alpha,A)$:\n    1. $(10^6, 1.0, 1.0)$\n    2. $(3 \\cdot 10^5, 0.0, 1.0)$\n    3. $(5 \\cdot 10^4, 0.0, 2.0)$\n    4. $(2 \\cdot 10^3, 3.0, 1.0)$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is based on fundamental, well-established concepts in analytic number theory. The functions $\\theta(x;q,a)$ and $\\varphi(q)$, and the context of the Bombieri-Vinogradov theorem, are standard in the field. The task is a direct computational realization of defined mathematical objects. The problem is sound.\n- **Well-Posed**: All functions ($\\theta, \\varphi, S_{\\theta}, M_A$) and parameters ($x, \\alpha, A, Q$) are defined with mathematical precision. The calculation for any given input triple is deterministic, ensuring a unique and meaningful solution exists. The problem is well-posed.\n- **Objective**: The problem is formulated using objective mathematical language, free from any subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem statement is self-contained. It provides all necessary definitions, formulas, and conditions, including the boundary condition for $Q$. There are no internal contradictions.\n- **Feasibility**: The largest value of $x$ is $10^6$. The corresponding values of $Q$ are on the order of several hundred. Computing primes up to $10^6$ and performing the specified summations are computationally intensive but entirely feasible with modern hardware and efficient algorithms.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined computational task in number theory. A solution will be constructed based on the provided definitions.\n\n### Solution Design\nThe computation of $M_A(x,Q)$ for each test case $(x, \\alpha, A)$ will be performed by following a principled, step-by-step algorithm that directly implements the given mathematical definitions.\n\n1.  **Objective Formulation**: The primary goal is to compute the quantity $M_A(x,Q) = \\frac{(\\log x)^A}{x} S_{\\theta}(x,Q)$. This requires the prior computation of the aggregate deviation $S_{\\theta}(x,Q)$, which is defined as $S_{\\theta}(x,Q) = \\sum_{1 \\le q \\le Q} \\max_{\\substack{0 \\le a  q \\\\ \\gcd(a,q)=1}} \\left| \\theta(x;q,a) - \\frac{x}{\\varphi(q)} \\right|$. The terms $\\theta(x;q,a)$ themselves are sums over primes, $\\theta(x;q,a) = \\sum_{p \\le x, p \\equiv a \\,(\\bmod q)} \\log p$.\n\n2.  **Pre-computation Strategy**: To ensure computational efficiency, shared data across test cases are pre-computed.\n    - **Primes and Logarithms**: The maximum value of $x$ in the test suite is $10^6$. All prime numbers $p$ up to $10^6$ are generated once using a Sieve of Eratosthenes. The natural logarithm $\\log p$ for each of these primes is also pre-computed and stored.\n    - **Euler's Totient Function $\\varphi(q)$**: The maximum value of $Q$ across all test cases is determined. A sieve-based algorithm is used to efficiently pre-compute $\\varphi(q)$ for all integers $q$ up to this maximum $Q$.\n\n3.  **Main Computation Loop**: The program iterates over each of the four test cases $(x, \\alpha, A)$. For each case, the following sequence of calculations is performed.\n\n4.  **Parameter $Q$ Calculation**: The scale parameter $Q$ is determined using the formula $Q = \\lfloor x^{1/2} (\\log x)^{-\\alpha} \\rfloor$. The mandate that $Q=1$ if the expression is less than $1$ is strictly followed.\n\n5.  **Computation of $\\theta(x;q,a)$ Sums**: The sums $\\theta(x;q,a)$ are the most computationally intensive part. A direct and efficient method is to iterate once through the pre-computed list of primes $p \\le x$. For each prime $p$, we iterate through all relevant moduli $q$ from $1$ to $Q$. The value of $\\log p$ is then added to the accumulator for $\\theta(x;q,a)$ where $a = p \\pmod q$. This approach avoids redundant iterations over the list of primes. The sums are stored in a data structure mapping $(q,a)$ to the corresponding sum.\n\n6.  **Computation of Aggregate Deviation $S_{\\theta}(x,Q)$**: After all $\\theta(x;q,a)$ sums for a given $x$ and $Q$ have been computed, $S_{\\theta}(x,Q)$ is calculated. This is done by iterating $q$ from $1$ to $Q$. For each $q$, the term $\\frac{x}{\\varphi(q)}$ is calculated. Then, the maximum of the absolute deviations $|\\theta(x;q,a) - \\frac{x}{\\varphi(q)}|$ is found by examining all reduced residue classes $a$ modulo $q$. These maximum deviations are summed over $q$ to yield $S_{\\theta}(x,Q)$. For a given $q$, if a reduced residue class $a$ never appeared as $p \\pmod q$, its corresponding $\\theta(x;q,a)$ is correctly taken as $0$.\n\n7.  **Final Diagnostic $M_A(x,Q)$**: The final result for the test case is computed by combining the previously calculated values: $M_A(x,Q) = S_{\\theta}(x,Q) \\cdot (\\log x)^A \\cdot x^{-1}$.\n\n8.  **Output Formatting**: The results for all four test cases are collected and formatted into a single string as specified: a comma-separated list of floating-point numbers with six digits of precision after the decimal point, enclosed in square brackets. This procedure guarantees a solution that is both correct with respect to the problem's definitions and computationally feasible.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef precompute_primes_and_logs(limit):\n    \"\"\"\n    Generates primes up to a limit using the Sieve of Eratosthenes\n    and computes their natural logarithms.\n    \"\"\"\n    is_prime = np.ones(limit + 1, dtype=bool)\n    is_prime[0:2] = False\n    for i in range(2, int(np.sqrt(limit)) + 1):\n        if is_prime[i]:\n            is_prime[i*i::i] = False\n    primes = np.where(is_prime)[0]\n    log_primes_map = {p: np.log(p) for p in primes}\n    return primes, log_primes_map\n\ndef precompute_phi(limit):\n    \"\"\"\n    Computes Euler's totient function phi(n) for all n = limit\n    using a sieve-based method.\n    \"\"\"\n    if limit  0:\n        return np.array([], dtype=int)\n    phi = np.arange(limit + 1, dtype=int)\n    if limit = 1:\n        phi[1] = 1\n    for i in range(2, limit + 1):\n        if phi[i] == i:  # i is prime\n            for j in range(i, limit + 1, i):\n                phi[j] -= phi[j] // i\n    return phi\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10**6, 1.0, 1.0),\n        (3 * 10**5, 0.0, 1.0),\n        (5 * 10**4, 0.0, 2.0),\n        (2 * 10**3, 3.0, 1.0),\n    ]\n\n    # Pre-computation based on the maximum values in test cases.\n    max_x = int(max(tc[0] for tc in test_cases))\n    \n    # Pre-compute primes and their logs up to max_x.\n    primes, log_primes_map = precompute_primes_and_logs(max_x)\n    \n    # Determine the maximum Q across all test cases to size the phi-sieve.\n    max_q_candidate = 0\n    for x_val, alpha_val, _ in test_cases:\n        log_x_val = np.log(x_val)\n        # alpha_val  0 checks prevent division by zero for x=1, though not in these cases.\n        q_expr = x_val**0.5 / (log_x_val**alpha_val) if log_x_val  0 or alpha_val = 0 else 0\n        max_q_candidate = max(max_q_candidate, int(q_expr))\n    \n    # Q must be at least 1, so phi table must accommodate at least phi(1).\n    max_q = max(1, max_q_candidate)\n    phi_values = precompute_phi(max_q)\n\n    results = []\n    for x, alpha, A in test_cases:\n        log_x = np.log(x)\n\n        # 1. Calculate Q for the current test case.\n        q_pre = x**0.5 / (log_x**alpha) if log_x  0 or alpha = 0 else 0\n        Q = int(q_pre) if q_pre = 1 else 1\n\n        # 2. Compute theta(x; q, a) for p = x, q = Q.\n        theta_sums = [{} for _ in range(Q + 1)]\n\n        # Efficiently select primes = x using binary search on the sorted prime array.\n        end_idx = np.searchsorted(primes, x, side='right')\n        current_primes = primes[:end_idx]\n\n        for p in current_primes:\n            log_p = log_primes_map[p]\n            for q in range(1, Q + 1):\n                a = p % q\n                theta_sums[q][a] = theta_sums[q].get(a, 0.0) + log_p\n\n        # 3. Compute S_theta(x, Q).\n        s_theta = 0.0\n        for q in range(1, Q + 1):\n            phi_q = phi_values[q]\n            expected_val = x / phi_q\n            max_dev = 0.0\n            \n            # Iterate through all reduced residue classes a mod q.\n            for a in range(q):\n                if math.gcd(a, q) == 1:\n                    theta_val = theta_sums[q].get(a, 0.0)\n                    dev = abs(theta_val - expected_val)\n                    if dev  max_dev:\n                        max_dev = dev\n            \n            s_theta += max_dev\n\n        # 4. Compute M_A(x, Q).\n        m_a = (log_x**A / x) * s_theta\n        results.append(m_a)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3025116"}]}