{"hands_on_practices": [{"introduction": "A deep understanding of any differential operator begins with analyzing its behavior under basic transformations. This first practice provides a foundational exercise in computing the Monge-Ampère operator for a simple quadratic potential and investigating how it transforms under affine scalings [@problem_id:3033130]. Unlike the Laplacian, the Monge-Ampère operator is not invariant, and mastering its transformation rule is a crucial first step toward tackling more complex geometric problems and understanding its role in contexts like optimal transport.", "problem": "Let $n \\geq 2$ and consider the convex potential $u : \\mathbb{R}^{n} \\to \\mathbb{R}$ defined by $u(x) = \\frac{1}{2} \\sum_{i=1}^{n} a_{i} x_{i}^{2}$ with $a_{i} > 0$ for all $i$. The Hessian matrix $D^{2} u(x)$ and its determinant $\\det D^{2} u(x)$ define the Monge-Ampère operator, and the Monge-Ampère equation is the fully nonlinear partial differential equation (PDE) $\\det D^{2} u(x) = f(x)$ for a given nonnegative function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$. \n\nUsing only the definitions of the gradient and Hessian, the chain rule for composition with affine maps, and the multiplicativity of the determinant under matrix multiplication, do the following:\n\n1. Compute $\\det D^{2} u(x)$ for the given $u(x)$ in terms of the parameters $a_{1}, \\ldots, a_{n}$. \n2. Fix an anisotropic affine scaling of coordinates given by $y = \\Lambda x + b$, where $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\ldots, \\lambda_{n})$ with $\\lambda_{i} > 0$ and $b \\in \\mathbb{R}^{n}$. Define the rescaled potential $v : \\mathbb{R}^{n} \\to \\mathbb{R}$ by $v(y) = u(\\Lambda^{-1}(y - b))$. Derive the expression for $\\det D^{2}_{y} v(y)$ explicitly in terms of $a_{1}, \\ldots, a_{n}$ and $\\lambda_{1}, \\ldots, \\lambda_{n}$, and state how the Monge-Ampère operator transforms under this anisotropic affine scaling.\n\nYour final answer should be the closed-form analytic expression for $\\det D^{2}_{y} v(y)$ in terms of $a_{1}, \\ldots, a_{n}$ and $\\lambda_{1}, \\ldots, \\lambda_{n}$. No rounding is required.", "solution": "The problem is validated as self-contained, scientifically grounded in the theory of partial differential equations, and mathematically well-posed. We may proceed with a formal derivation.\n\nThe problem is divided into two parts. First, we compute the Monge-Ampère operator for the given potential $u(x)$. Second, we analyze how this operator transforms under an anisotropic affine scaling of coordinates.\n\n**Part 1: Computation of $\\det D^{2} u(x)$**\n\nThe potential function is given by $u : \\mathbb{R}^{n} \\to \\mathbb{R}$, defined as:\n$$u(x) = \\frac{1}{2} \\sum_{i=1}^{n} a_{i} x_{i}^{2}$$\nwhere $x = (x_{1}, \\ldots, x_{n}) \\in \\mathbb{R}^{n}$ and $a_{i} > 0$ are constants.\n\nWe begin by computing the gradient of $u(x)$, denoted by $\\nabla u(x)$ or $Du(x)$. The components of the gradient are the partial derivatives $\\frac{\\partial u}{\\partial x_{j}}$ for $j = 1, \\ldots, n$.\nFor a specific component $j$, we have:\n$$\\frac{\\partial u}{\\partial x_{j}} = \\frac{\\partial}{\\partial x_{j}} \\left( \\frac{1}{2} \\sum_{i=1}^{n} a_{i} x_{i}^{2} \\right) = \\frac{1}{2} \\cdot (2 a_{j} x_{j}) = a_{j} x_{j}$$\nThus, the gradient vector is $\\nabla u(x) = (a_{1}x_{1}, a_{2}x_{2}, \\ldots, a_{n}x_{n})^{T}$.\n\nNext, we compute the Hessian matrix of $u(x)$, denoted by $D^{2} u(x)$. The entry in the $k$-th row and $j$-th column of this matrix is given by the second-order partial derivative $\\frac{\\partial^{2} u}{\\partial x_{k} \\partial x_{j}}$.\n$$\\left(D^{2} u(x)\\right)_{kj} = \\frac{\\partial^{2} u}{\\partial x_{k} \\partial x_{j}} = \\frac{\\partial}{\\partial x_{k}} \\left( \\frac{\\partial u}{\\partial x_{j}} \\right) = \\frac{\\partial}{\\partial x_{k}} (a_{j} x_{j})$$\nThis derivative is equal to $a_{j}$ if $k=j$, and $0$ if $k \\neq j$. We can express this using the Kronecker delta, $\\delta_{kj}$:\n$$\\left(D^{2} u(x)\\right)_{kj} = a_{j} \\delta_{kj}$$\nThis shows that the Hessian matrix $D^{2} u(x)$ is a diagonal matrix with the constants $a_{i}$ on its diagonal:\n$$D^{2} u(x) = \\begin{pmatrix} a_{1} & 0 & \\cdots & 0 \\\\ 0 & a_{2} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & a_{n} \\end{pmatrix}$$\nAn important observation is that the Hessian matrix is constant; it is independent of the point $x$.\n\nThe determinant of a diagonal matrix is the product of its diagonal entries. Therefore, the Monge-Ampère operator applied to $u(x)$ yields:\n$$\\det D^{2} u(x) = \\det \\left( \\mathrm{diag}(a_{1}, \\ldots, a_{n}) \\right) = \\prod_{i=1}^{n} a_{i}$$\n\n**Part 2: Transformation under Anisotropic Affine Scaling**\n\nWe are given an affine transformation of coordinates from $x$ to $y$:\n$$y = \\Lambda x + b \\quad \\Leftrightarrow \\quad x = \\Lambda^{-1}(y - b)$$\nwhere $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\ldots, \\lambda_{n})$ with $\\lambda_{i} > 0$, and $b \\in \\mathbb{R}^{n}$. Let us denote the coordinate transformation function by $x(y) = \\Lambda^{-1}(y-b)$.\n\nThe rescaled potential $v : \\mathbb{R}^{n} \\to \\mathbb{R}$ is defined by the composition $v(y) = u(x(y))$. We must find its Hessian with respect to the $y$ coordinates, $D^{2}_{y} v(y)$. We use the chain rule for Hessians. For a composition $v(y) = u(x(y))$, the general formula is:\n$$(D^{2}_{y} v)_{ij} = \\sum_{k,l} \\frac{\\partial^{2} u}{\\partial x_{k} \\partial x_{l}} \\frac{\\partial x_{k}}{\\partial y_{i}} \\frac{\\partial x_{l}}{\\partial y_{j}} + \\sum_{k} \\frac{\\partial u}{\\partial x_{k}} \\frac{\\partial^{2} x_{k}}{\\partial y_{i} \\partial y_{j}}$$\nSince our transformation $x(y)$ is affine, its second derivatives are zero ($\\frac{\\partial^{2} x_{k}}{\\partial y_{i} \\partial y_{j}} = 0$). The formula simplifies significantly. In matrix notation, this is:\n$$D^{2}_{y} v(y) = (D_{y}x(y))^{T} \\left( D^{2}_{x}u \\right)(x(y)) (D_{y}x(y))$$\nwhere $D_{y}x(y)$ is the Jacobian matrix of the transformation $x(y)$.\n\nLet's compute the Jacobian $D_{y}x(y)$. The transformation is $x(y) = \\Lambda^{-1}y - \\Lambda^{-1}b$. The Jacobian of an affine map $y \\mapsto My+c$ is simply the matrix $M$. Therefore:\n$$D_{y}x(y) = \\Lambda^{-1}$$\nThe matrix $\\Lambda$ is diagonal, so its inverse $\\Lambda^{-1}$ is also diagonal:\n$$\\Lambda^{-1} = \\mathrm{diag}(\\lambda_{1}^{-1}, \\ldots, \\lambda_{n}^{-1})$$\nSince $\\Lambda^{-1}$ is a diagonal matrix, it is symmetric, i.e., $(\\Lambda^{-1})^{T} = \\Lambda^{-1}$.\n\nSubstituting into the chain rule formula for the Hessian:\n$$D^{2}_{y} v(y) = (\\Lambda^{-1})^{T} \\left( D^{2}_{x}u \\right)(x(y)) \\Lambda^{-1} = \\Lambda^{-1} \\left( D^{2}_{x}u \\right)(x(y)) \\Lambda^{-1}$$\nFrom Part 1, we know that $D^{2}_{x}u$ is the constant matrix $\\mathrm{diag}(a_{1}, \\ldots, a_{n})$. So, $\\left( D^{2}_{x}u \\right)(x(y)) = \\mathrm{diag}(a_{1}, \\ldots, a_{n})$.\n$$D^{2}_{y} v(y) = \\mathrm{diag}(\\lambda_{1}^{-1}, \\ldots, \\lambda_{n}^{-1}) \\mathrm{diag}(a_{1}, \\ldots, a_{n}) \\mathrm{diag}(\\lambda_{1}^{-1}, \\ldots, \\lambda_{n}^{-1})$$\nThe product of these diagonal matrices is a diagonal matrix whose entries are the products of the corresponding entries:\n$$D^{2}_{y} v(y) = \\mathrm{diag}(a_{1}\\lambda_{1}^{-2}, a_{2}\\lambda_{2}^{-2}, \\ldots, a_{n}\\lambda_{n}^{-2})$$\nThe determinant of this resulting Hessian is:\n$$\\det D^{2}_{y} v(y) = \\prod_{i=1}^{n} (a_{i} \\lambda_{i}^{-2}) = \\left( \\prod_{i=1}^{n} a_{i} \\right) \\left( \\prod_{i=1}^{n} \\lambda_{i}^{-2} \\right)$$\nThis expression can be written as:\n$$\\det D^{2}_{y} v(y) = \\frac{\\prod_{i=1}^{n} a_{i}}{\\left(\\prod_{i=1}^{n} \\lambda_{i}\\right)^{2}}$$\n\nAlternatively, using the multiplicativity of the determinant property:\n$$\\det D^{2}_{y} v(y) = \\det(\\Lambda^{-1} (D^{2}_{x}u) \\Lambda^{-1})$$\n$$\\det D^{2}_{y} v(y) = \\det(\\Lambda^{-1}) \\det(D^{2}_{x}u) \\det(\\Lambda^{-1}) = (\\det \\Lambda^{-1})^{2} \\det(D^{2}_{x}u)$$\nWe have $\\det(D^{2}_{x}u) = \\prod_{i=1}^{n} a_{i}$ and $\\det \\Lambda = \\prod_{i=1}^{n} \\lambda_{i}$, so $\\det \\Lambda^{-1} = (\\det \\Lambda)^{-1} = (\\prod_{i=1}^{n} \\lambda_{i})^{-1}$.\nSubstituting these results yields:\n$$\\det D^{2}_{y} v(y) = \\left( \\left(\\prod_{i=1}^{n} \\lambda_{i}\\right)^{-1} \\right)^{2} \\left( \\prod_{i=1}^{n} a_{i} \\right) = \\frac{\\prod_{i=1}^{n} a_{i}}{\\left(\\prod_{i=1}^{n} \\lambda_{i}\\right)^{2}}$$\nThis confirms the previous calculation.\n\nRegarding the transformation of the Monge-Ampère operator, if we let $MA[u](x) = \\det D^{2}_{x} u(x)$, then under the coordinate change $y = \\Lambda x + b$ and potential rescaling $v(y) = u(\\Lambda^{-1}(y-b))$, the operator transforms as:\n$$MA[v](y) = (\\det \\Lambda)^{-2} MA[u](x(y))$$\nThis shows that the Monge-Ampère operator is not invariant under affine transformations, but transforms by a factor related to the Jacobian determinant of the transformation. For the specific problem, with $MA[u]$ being constant, the relation is $MA[v] = (\\det \\Lambda)^{-2} MA[u]$.\n\nThe final expression required is for $\\det D^{2}_{y} v(y)$.", "answer": "$$ \\boxed{ \\frac{\\prod_{i=1}^{n} a_{i}}{\\left(\\prod_{i=1}^{n} \\lambda_{i}\\right)^{2}} } $$", "id": "3033130"}, {"introduction": "We now move from operator mechanics to solving a complete boundary value problem, a cornerstone of PDE theory. This exercise invites you to find the explicit radial solution to the Monge-Ampère equation with a constant right-hand side on a ball, a canonical example in the field [@problem_id:3033144]. More importantly, it bridges this explicit solution with the celebrated Alexandroff–Bakelman–Pucci (ABP) maximum principle, allowing you to see how a powerful, general a priori estimate performs in a concrete and calculable setting.", "problem": "Let $n \\geq 2$ and let $R>0$, $\\lambda>0$. Consider the Dirichlet problem for the Monge-Ampère equation in the ball $B_{R}(0) \\subset \\mathbb{R}^{n}$: find a radial, convex, twice continuously differentiable function $u : \\overline{B_{R}(0)} \\to \\mathbb{R}$ such that\n$$\n\\det D^{2} u = \\lambda \\quad \\text{in } B_{R}(0), \\qquad u = 0 \\quad \\text{on } \\partial B_{R}(0).\n$$\nWork in the radial ansatz $u(x) = \\varphi(r)$ with $r = |x|$ and use the following foundational facts:\n- For a radial function $u(x)=\\varphi(|x|)$, the Hessian matrix $D^{2}u$ has one eigenvalue $\\varphi''(r)$ in the radial direction and $(n-1)$ identical eigenvalues $\\varphi'(r)/r$ in tangential directions.\n- The solution is convex, so it satisfies $\\varphi''(r) \\geq 0$ and $\\varphi'(r) \\geq 0$ for $r \\in [0,R]$.\n- Along any radius, the fundamental theorem of calculus implies $u(R) - u(r) = \\int_{r}^{R} \\varphi'(s)\\,ds$.\n\nTasks:\n1. Derive the ordinary differential equation satisfied by $\\varphi$ from $\\det D^{2} u = \\lambda$, and solve it under the conditions of convexity and regularity at $r=0$, together with the boundary condition $u(R)=0$.\n2. Using the radial representation and convexity, justify the Alexandroff–Bakelman–Pucci (ABP) type bound in this setting\n$$\n\\sup_{B_{R}(0)}\\big(-u\\big) \\leq R \\sup_{B_{R}(0)} |\\nabla u|,\n$$\nand compute the right-hand side explicitly for the derived solution.\n3. Compute the exact minimum value of $u$ in $B_{R}(0)$ and compare it to the ABP bound by forming the ratio\n$$\n\\frac{\\text{ABP bound for } \\sup(-u)}{\\text{exact value of } \\sup(-u)}.\n$$\n\nYour final answer must be this ratio as a single real number (no units). No rounding is required. Express your final result in exact form.", "solution": "The problem is to find a radial, convex, $C^2$ solution to the Monge-Ampère equation $\\det D^2 u = \\lambda$ in the ball $B_R(0) \\subset \\mathbb{R}^n$ with Dirichlet boundary condition $u=0$ on $\\partial B_R(0)$, and then to analyze a related inequality.\n\n### Part 1: Derivation and Solution of the ODE\n\nWe are given the ansatz $u(x) = \\varphi(r)$ where $r = |x|$. The Hessian matrix $D^2u$ of a radial function has one eigenvalue in the radial direction, $\\varphi''(r)$, and $n-1$ identical eigenvalues in the tangential directions, $\\frac{\\varphi'(r)}{r}$. The determinant of the Hessian is the product of its eigenvalues. Therefore, the Monge-Ampère equation $\\det D^2 u = \\lambda$ becomes an ordinary differential equation (ODE) for $\\varphi(r)$:\n$$\n\\varphi''(r) \\left( \\frac{\\varphi'(r)}{r} \\right)^{n-1} = \\lambda\n$$\nThis ODE is valid for $r \\in (0, R)$. We can rearrange it as:\n$$\n\\varphi''(r) (\\varphi'(r))^{n-1} = \\lambda r^{n-1}\n$$\nThe left side of the equation can be expressed as a derivative. Noting that $\\frac{d}{dr}[(\\varphi'(r))^n] = n(\\varphi'(r))^{n-1}\\varphi''(r)$, we can write the ODE as:\n$$\n\\frac{1}{n} \\frac{d}{dr}\\left[ (\\varphi'(r))^n \\right] = \\lambda r^{n-1}\n$$\nWe integrate this equation from an arbitrary small $\\epsilon > 0$ to $r$:\n$$\n\\frac{1}{n} \\left[ (\\varphi'(r))^n - (\\varphi'(\\epsilon))^n \\right] = \\int_{\\epsilon}^{r} \\lambda s^{n-1} \\, ds = \\frac{\\lambda}{n} (r^n - \\epsilon^n)\n$$\nFor the solution $u$ to be twice continuously differentiable on $\\overline{B_R(0)}$, the gradient $\\nabla u(x) = \\varphi'(|x|) \\frac{x}{|x|}$ must be well-defined at $x=0$. This requires $\\varphi'(0)=0$. Taking the limit as $\\epsilon \\to 0$:\n$$\n(\\varphi'(r))^n = \\lambda r^n\n$$\nTaking the $n$-th root, we get $\\varphi'(r) = \\lambda^{1/n} r$. We choose the positive root because the convexity of $u$ implies that it must have a minimum at the center, so $\\varphi'(r) \\ge 0$ for $r \\ge 0$. This choice also satisfies the convexity condition $\\varphi''(r) = \\lambda^{1/n} > 0$ since $\\lambda > 0$.\n\nNow, we integrate $\\varphi'(r)$ to find $\\varphi(r)$:\n$$\n\\varphi(r) = \\int \\varphi'(r) \\, dr = \\int \\lambda^{1/n} r \\, dr = \\frac{\\lambda^{1/n}}{2} r^2 + C\n$$\nThe constant of integration $C$ is determined by the boundary condition $u=0$ on $\\partial B_R(0)$, which means $\\varphi(R)=0$:\n$$\n\\varphi(R) = \\frac{\\lambda^{1/n}}{2} R^2 + C = 0 \\implies C = -\\frac{\\lambda^{1/n}}{2} R^2\n$$\nThus, the solution is given by:\n$$\nu(x) = \\varphi(|x|) = \\frac{\\lambda^{1/n}}{2} (|x|^2 - R^2)\n$$\n\n### Part 2: Justification and Calculation of the ABP-type Bound\n\nWe are asked to justify the inequality $\\sup_{B_{R}(0)}(-u) \\leq R \\sup_{B_{R}(0)} |\\nabla u|$.\nSince $u$ is convex and $u=0$ on the boundary, $u(x) \\le 0$ for all $x \\in B_R(0)$. The minimum of $u$ occurs at the center $x=0$, where $\\nabla u(0) = 0$. Thus, $\\sup_{B_R(0)}(-u) = -u_{\\min} = -u(0) = -\\varphi(0)$.\n\nUsing the provided fact from the fundamental theorem of calculus, $u(R) - u(r) = \\int_{r}^{R} \\varphi'(s) \\, ds$. Setting $r=0$ and using $u(R)=\\varphi(R)=0$, we get:\n$$\n-u(0) = -\\varphi(0) = \\int_{0}^{R} \\varphi'(s) \\, ds\n$$\nSo, $\\sup_{B_{R}(0)}(-u) = \\int_{0}^{R} \\varphi'(s) \\, ds$.\nThe magnitude of the gradient is $|\\nabla u(x)| = |\\varphi'(|x|)|$. Since $\\varphi'(r) = \\lambda^{1/n}r \\ge 0$ for $r \\ge 0$, we have $|\\nabla u(x)| = \\varphi'(|x|)$.\nThe convexity condition $\\varphi''(r) \\ge 0$ implies that $\\varphi'(r)$ is a non-decreasing function. Therefore, for any $s \\in [0, R]$, we have $\\varphi'(s) \\le \\varphi'(R)$. The supremum of the gradient's magnitude is thus achieved at the boundary $r=R$:\n$$\n\\sup_{B_{R}(0)} |\\nabla u| = \\sup_{r \\in [0,R]} \\varphi'(r) = \\varphi'(R)\n$$\nWe can now bound the expression for $\\sup(-u)$:\n$$\n\\sup_{B_{R}(0)}(-u) = \\int_{0}^{R} \\varphi'(s) \\, ds \\le \\int_{0}^{R} \\varphi'(R) \\, ds = \\varphi'(R) \\int_{0}^{R} 1 \\, ds = R \\cdot \\varphi'(R)\n$$\nSubstituting $\\varphi'(R) = \\sup_{B_{R}(0)} |\\nabla u|$, we arrive at the desired inequality:\n$$\n\\sup_{B_{R}(0)}(-u) \\leq R \\sup_{B_{R}(0)} |\\nabla u|\n$$\nNow, we compute the value of the right-hand side, which is the ABP bound. From our solution, $\\varphi'(r) = \\lambda^{1/n} r$.\n$$\n\\sup_{B_{R}(0)} |\\nabla u| = \\varphi'(R) = \\lambda^{1/n} R\n$$\nTherefore, the ABP bound is $R \\cdot (\\lambda^{1/n} R) = \\lambda^{1/n} R^2$.\n\n### Part 3: Comparison and Ratio\n\nFirst, we compute the exact value of $\\sup_{B_{R}(0)}(-u)$, which is $-u(0) = -\\varphi(0)$. Using our expression for $\\varphi(r)$:\n$$\n\\varphi(0) = \\frac{\\lambda^{1/n}}{2} (0^2 - R^2) = -\\frac{\\lambda^{1/n}}{2} R^2\n$$\nSo, the exact value is:\n$$\n\\text{exact value of } \\sup(-u) = -\\varphi(0) = \\frac{\\lambda^{1/n}}{2} R^2\n$$\nFinally, we compute the ratio of the ABP bound to the exact value:\n$$\n\\frac{\\text{ABP bound for } \\sup(-u)}{\\text{exact value of } \\sup(-u)} = \\frac{\\lambda^{1/n} R^2}{\\frac{\\lambda^{1/n}}{2} R^2}\n$$\nThe terms $\\lambda^{1/n} R^2$ cancel out.\n$$\n\\text{Ratio} = \\frac{1}{\\frac{1}{2}} = 2\n$$\nThe result is a constant, independent of the parameters $n$, $R$, and $\\lambda$.", "answer": "$$\\boxed{2}$$", "id": "3033144"}, {"introduction": "The classical theory of partial differential equations often assumes solutions are smooth, but the world of convex geometry is richer and more complex. This practice illustrates a crucial feature of the Monge-Ampère equation: its solutions are not always twice differentiable, necessitating a 'weak' or generalized framework [@problem_id:3033133]. By constructing a simple, non-differentiable convex function, you will directly compute its associated Monge-Ampère measure and discover that it can be singular, motivating the powerful theory of Alexandrov solutions.", "problem": "Let $\\Omega \\subset \\mathbb{R}^{2}$ be the open rectangle $\\Omega = (-1,1) \\times (0,1)$ and fix a parameter $a \\in (0,\\infty)$. Consider the convex function $u_{a} : \\Omega \\to \\mathbb{R}$ defined by $u_{a}(x_{1},x_{2}) = a\\,|x_{1}| + \\tfrac{1}{2}\\,x_{2}^{2}$. The function $u_{a}$ is nondifferentiable along the line segment $\\Gamma = \\{(0,t) : t \\in (0,1)\\} \\subset \\Omega$. Let $\\mu_{u_{a}}$ denote the Monge-Ampère measure of $u_{a}$ in the sense of Aleksandrov, defined for each Borel set $E \\subset \\Omega$ by the formula $\\mu_{u_{a}}(E) = |\\partial u_{a}(E)|$, where $\\partial u_{a}(x)$ is the subgradient of $u_{a}$ at $x$ and $|\\cdot|$ denotes the Lebesgue measure on $\\mathbb{R}^{2}$. Using only the fundamental definitions of convexity, subgradients, and the Aleksandrov Monge-Ampère measure (and well-tested facts such as the characterization of the absolutely continuous part of $\\mu_{u_{a}}$ by $\\det D^{2}u_{a}$ where $u_{a}$ is twice differentiable), prove that the absolutely continuous part of $\\mu_{u_{a}}$ with respect to the Lebesgue measure on $\\mathbb{R}^{2}$ vanishes and that the entire measure $\\mu_{u_{a}}$ is singular with respect to the Lebesgue measure, supported on $\\Gamma$. Show that there exists a constant $c_{a} > 0$ such that the singular part can be written as $\\mu_{u_{a}}^{s} = c_{a}\\,\\mathcal{H}^{1}\\!\\restriction_{\\Gamma}$, where $\\mathcal{H}^{1}$ denotes the one-dimensional Hausdorff measure and $\\mathcal{H}^{1}\\!\\restriction_{\\Gamma}$ is the restriction of $\\mathcal{H}^{1}$ to $\\Gamma$. Compute $c_{a}$ explicitly as a function of $a$. Your final answer must be given as a closed-form expression in $a$.", "solution": "The problem is valid as it is mathematically well-posed, scientifically grounded in convex analysis, and all terms are formally defined. We proceed with the solution.\n\nThe convex function is given by $u_{a}(x_{1},x_{2}) = a\\,|x_{1}| + \\tfrac{1}{2}\\,x_{2}^{2}$ for $(x_1, x_2) \\in \\Omega = (-1,1) \\times (0,1)$, with a parameter $a > 0$. The function $u_a$ is a sum of two convex functions, $x_1 \\mapsto a\\,|x_1|$ and $x_2 \\mapsto \\tfrac{1}{2}\\,x_2^2$, and is therefore convex on $\\Omega$.\n\nFirst, we prove that the absolutely continuous part of the Monge-Ampère measure $\\mu_{u_{a}}$ vanishes. The absolutely continuous part, $\\mu_{u_a}^{ac}$, is given by the measure with density $\\det(D^2u_a)$ with respect to the two-dimensional Lebesgue measure $\\mathcal{L}^2$. We compute the Hessian of $u_a$ on the set where it is twice differentiable. This set is $\\Omega \\setminus \\Gamma$, where $\\Gamma = \\{(0,t) : t \\in (0,1)\\}$.\nFor any point $(x_1, x_2) \\in \\Omega \\setminus \\Gamma$, we have $x_1 \\neq 0$. The gradient of $u_a$ at such a point is:\n$$\nD u_a(x_1, x_2) = \\left( \\frac{\\partial u_a}{\\partial x_1}, \\frac{\\partial u_a}{\\partial x_2} \\right) = (a \\cdot \\text{sgn}(x_1), x_2)\n$$\nwhere $\\text{sgn}(x_1)$ is $1$ if $x_1 > 0$ and $-1$ if $x_1 < 0$.\nThe Hessian matrix is the matrix of second partial derivatives:\n$$\nD^2 u_a(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial^2 u_a}{\\partial x_1^2} & \\frac{\\partial^2 u_a}{\\partial x_2 \\partial x_1} \\\\ \\frac{\\partial^2 u_a}{\\partial x_1 \\partial x_2} & \\frac{\\partial^2 u_a}{\\partial x_2^2} \\end{pmatrix}\n$$\nFor $x_1 \\neq 0$, the derivatives are:\n$\\frac{\\partial}{\\partial x_1} (a \\cdot \\text{sgn}(x_1)) = 0$,\n$\\frac{\\partial}{\\partial x_2} (a \\cdot \\text{sgn}(x_1)) = 0$,\n$\\frac{\\partial}{\\partial x_1} (x_2) = 0$,\n$\\frac{\\partial}{\\partial x_2} (x_2) = 1$.\nSo, the Hessian matrix for $(x_1, x_2) \\in \\Omega \\setminus \\Gamma$ is:\n$$\nD^2 u_a(x_1, x_2) = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe determinant of the Hessian is therefore $\\det(D^2 u_a(x_1, x_2)) = 0 \\cdot 1 - 0 \\cdot 0 = 0$ for all $(x_1, x_2) \\in \\Omega \\setminus \\Gamma$.\nThe set $\\Gamma$ has Lebesgue measure zero in $\\mathbb{R}^2$. Thus, $\\det(D^2 u_a) = 0$ almost everywhere in $\\Omega$.\nThe density of the absolutely continuous part $\\mu_{u_a}^{ac}$ is zero almost everywhere, which implies that $\\mu_{u_a}^{ac}$ is the zero measure.\nBy the Lebesgue decomposition of the measure $\\mu_{u_a}$ into its absolutely continuous and singular parts, $\\mu_{u_a} = \\mu_{u_a}^{ac} + \\mu_{u_a}^{s}$, we have $\\mu_{u_a} = \\mu_{u_a}^{s}$. This proves that the entire measure $\\mu_{u_a}$ is singular with respect to the Lebesgue measure on $\\mathbb{R}^2$.\n\nNext, we identify the support of $\\mu_{u_a}$. The Monge-Ampère measure is defined by $\\mu_{u_a}(E) = |\\partial u_a(E)|$ for any Borel set $E \\subset \\Omega$, where $\\partial u_a(E) = \\bigcup_{x \\in E} \\partial u_a(x)$ and $|\\cdot|$ is the Lebesgue measure on $\\mathbb{R}^2$. We characterize the subgradient $\\partial u_a(x)$.\nThe function $u_a$ is separable: $u_a(x_1, x_2) = f(x_1) + g(x_2)$ with $f(t) = a|t|$ and $g(t) = \\tfrac{1}{2}t^2$. The subgradient is the Cartesian product of the subgradients of the component functions: $\\partial u_a(x_1, x_2) = \\partial f(x_1) \\times \\partial g(x_2)$.\nThe subgradient of $g(t) = \\tfrac{1}{2}t^2$ is $\\partial g(t) = \\{g'(t)\\} = \\{t\\}$.\nThe subgradient of $f(t) = a|t|$ is:\n\\begin{itemize}\n    \\item $\\partial f(t) = \\{a \\cdot \\text{sgn}(t)\\}$ if $t \\neq 0$.\n    \\item $\\partial f(0) = [-a, a]$.\n\\end{itemize}\nCombining these results for $x=(x_1, x_2) \\in \\Omega$:\n\\begin{itemize}\n    \\item If $x_1 \\in (-1,0)$, $\\partial u_a(x) = \\{-a\\} \\times \\{x_2\\} = \\{(-a, x_2)\\}$.\n    \\item If $x_1 \\in (0,1)$, $\\partial u_a(x) = \\{a\\} \\times \\{x_2\\} = \\{(a, x_2)\\}$.\n    \\item If $x_1=0$, so $x \\in \\Gamma$, $\\partial u_a(x) = [-a, a] \\times \\{x_2\\}$.\n\\end{itemize}\nLet $E \\subset \\Omega$ be a Borel set such that $E \\cap \\Gamma = \\emptyset$. Then for any $x \\in E$, $x_1 \\neq 0$. The image $\\partial u_a(E)$ is a subset of the union of two vertical line segments in the gradient space: $(\\{-a\\} \\times (0,1)) \\cup (\\{a\\} \\times (0,1))$. This union has Lebesgue measure zero in $\\mathbb{R}^2$. Therefore, $|\\partial u_a(E)| = 0$. This implies $\\mu_{u_a}(E) = 0$ if $E$ does not intersect $\\Gamma$. This proves that the measure $\\mu_{u_a}$ is supported on the line segment $\\Gamma$.\n\nFinally, we compute the constant $c_a$. Since $\\mu_{u_a}$ is supported on $\\Gamma$, we can write it as $\\mu_{u_a} = c_a \\mathcal{H}^1\\restriction_{\\Gamma}$, where $\\mathcal{H}^1$ is the one-dimensional Hausdorff measure and $c_a$ is the density we want to find. It suffices to compute the measure of a generic Borel subset of $\\Gamma$.\nLet $E$ be a Borel subset of $\\Gamma$. Then $E$ can be written as $E = \\{(0,t) : t \\in S\\}$ for some Borel set $S \\subset (0,1)$. The $\\mathcal{H}^1$ measure of $E$ is the one-dimensional Lebesgue measure of $S$, i.e., $\\mathcal{H}^1(E) = \\mathcal{L}^1(S)$.\nWe compute $\\mu_{u_a}(E)$ using its definition:\n$$\n\\mu_{u_a}(E) = |\\partial u_a(E)| = \\left| \\bigcup_{x \\in E} \\partial u_a(x) \\right|\n$$\nSubstituting the expression for $E$ and the subgradient on $\\Gamma$:\n$$\n\\mu_{u_a}(E) = \\left| \\bigcup_{t \\in S} ([-a, a] \\times \\{t\\}) \\right| = |[-a, a] \\times S|\n$$\nThe set $[-a, a] \\times S$ is a measurable subset of $\\mathbb{R}^2$. Its Lebesgue measure can be calculated using Fubini's theorem:\n$$\n|[-a, a] \\times S| = \\int_{-a}^{a} \\left( \\int_S dt \\right) dp_1 = \\mathcal{L}^1(S) \\int_{-a}^{a} dp_1 = \\mathcal{L}^1(S)[p_1]_{-a}^{a} = \\mathcal{L}^1(S)(a - (-a)) = 2a \\cdot \\mathcal{L}^1(S)\n$$\nSince $\\mathcal{H}^1(E) = \\mathcal{L}^1(S)$, we have shown that for any Borel set $E \\subset \\Gamma$:\n$$\n\\mu_{u_a}(E) = 2a \\cdot \\mathcal{H}^1(E)\n$$\nThis relationship holds for all Borel subsets of $\\Gamma$. For a general Borel set $F \\subset \\Omega$, we have $\\mu_{u_a}(F) = \\mu_{u_a}(F \\cap \\Gamma) = 2a \\cdot \\mathcal{H}^1(F \\cap \\Gamma)$.\nThis means that the measure $\\mu_{u_a}$ is equal to the measure $2a \\cdot \\mathcal{H}^1\\restriction_\\Gamma$.\nWe started from $\\mu_{u_a} = \\mu_{u_a}^s = c_a \\mathcal{H}^1\\restriction_\\Gamma$. By comparison, we find the constant $c_a$.\n$$\nc_a = 2a\n$$\nThe constant represents the magnitude of the jump in the gradient component $\\frac{\\partial u_a}{\\partial x_1}$ across the line of singularity $\\Gamma$, which is from $-a$ to $a$.", "answer": "$$\\boxed{2a}$$", "id": "3033133"}]}