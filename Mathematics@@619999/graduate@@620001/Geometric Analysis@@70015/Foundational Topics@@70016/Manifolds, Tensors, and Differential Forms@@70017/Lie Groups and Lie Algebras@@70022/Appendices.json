{"hands_on_practices": [{"introduction": "The cornerstone of any Lie algebra is its bracket operation. For matrix Lie algebras, this is typically realized as the commutator, $[X, Y] = XY - YX$. This exercise provides direct, hands-on practice in computing this fundamental quantity for basis elements of $\\mathfrak{sl}(2, \\mathbb{R})$, the Lie algebra of the special linear group $SL(2, \\mathbb{R})$, laying the groundwork for understanding more complex algebraic structures. [@problem_id:1523080]", "problem": "Consider the real vector space $V = \\mathbb{R}^2$ equipped with the standard basis $\\{e_1, e_2\\}$, where $e_1 = (1, 0)$ and $e_2 = (0, 1)$. Two linear transformations, $T_1: V \\to V$ and $T_2: V \\to V$, are defined by their action on these basis vectors as follows:\n\n$T_1(e_1) = (0, 0)$ and $T_1(e_2) = e_1$.\n$T_2(e_1) = e_2$ and $T_2(e_2) = (0, 0)$.\n\nLet $X$ and $Y$ be the $2 \\times 2$ matrix representations of the linear transformations $T_1$ and $T_2$ with respect to the standard basis, respectively. These matrices are elements of the special linear Lie algebra $\\mathfrak{sl}(2, \\mathbb{R})$, which is the set of all $2 \\times 2$ real matrices with a trace of zero.\n\nThe Lie bracket of two matrices $A$ and $B$ is defined by the commutator operation $[A, B] = AB - BA$.\n\nCalculate the Lie bracket $[X, Y]$. Your answer should be a $2 \\times 2$ matrix.", "solution": "We use the fact that the matrix of a linear transformation with respect to a basis has columns given by the images of the basis vectors expressed in that basis.\n\nGiven $T_{1}(e_{1})=(0,0)$ and $T_{1}(e_{2})=e_{1}=(1,0)$, the matrix $X$ representing $T_{1}$ in the standard basis is\n$$\nX=\\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}.\n$$\nGiven $T_{2}(e_{1})=e_{2}=(0,1)$ and $T_{2}(e_{2})=(0,0)$, the matrix $Y$ representing $T_{2}$ in the standard basis is\n$$\nY=\\begin{pmatrix}\n0 & 0 \\\\\n1 & 0\n\\end{pmatrix}.\n$$\nThe Lie bracket is the commutator $[X,Y]=XY-YX$. Compute\n$$\nXY=\\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 0 \\\\\n1 & 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{pmatrix},\n\\quad\nYX=\\begin{pmatrix}\n0 & 0 \\\\\n1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n[X,Y]=XY-YX=\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix}}$$", "id": "1523080"}, {"introduction": "The exponential map provides the crucial bridge from the linear space of a Lie algebra to the curved manifold of its corresponding Lie group. While it establishes a local diffeomorphism, its global behavior is more complex, and it is not always surjective. This practice problem delves into this subtlety by examining the image of the exponential map for $SL(2, \\mathbb{R})$, a classic and illuminating counterexample that deepens our understanding of the global group-algebra relationship. [@problem_id:1678758]", "problem": "Consider the Lie group of 2x2 real matrices with determinant 1, denoted by $SL(2, \\mathbb{R})$. Its corresponding Lie algebra, denoted by $\\mathfrak{sl}(2, \\mathbb{R})$, consists of all 2x2 real matrices with a trace of zero. The exponential map, $\\exp: \\mathfrak{sl}(2, \\mathbb{R}) \\to SL(2, \\mathbb{R})$, is defined by the standard matrix exponential series $\\exp(X) = \\sum_{k=0}^{\\infty} \\frac{1}{k!}X^k$ for any $X \\in \\mathfrak{sl}(2, \\mathbb{R})$. The image of this map is the set of all matrices in $SL(2, \\mathbb{R})$ that can be written as $\\exp(X)$ for some $X \\in \\mathfrak{sl}(2, \\mathbb{R})$.\n\nOne of the following matrices is an element of $SL(2, \\mathbb{R})$ but is not in the image of the exponential map. Identify which one.\n\nA. $\\begin{pmatrix} 1 & 3 \\\\ 0 & 1 \\end{pmatrix}$\n\nB. $\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$\n\nC. $\\begin{pmatrix} -2 & 0 \\\\ 0 & -1/2 \\end{pmatrix}$\n\nD. $\\begin{pmatrix} 3 & 0 \\\\ 0 & 1/3 \\end{pmatrix}$\n\nE. $\\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$", "solution": "We consider the exponential map $\\exp:\\mathfrak{sl}(2,\\mathbb{R})\\to SL(2,\\mathbb{R})$, where for any $X\\in\\mathfrak{sl}(2,\\mathbb{R})$ one has $\\det(\\exp X)=\\exp(\\operatorname{tr}X)=1$, so $\\exp(\\mathfrak{sl}(2,\\mathbb{R}))\\subset SL(2,\\mathbb{R})$. Conversely, a matrix $A\\in SL(2,\\mathbb{R})$ lies in the image if and only if it admits a real logarithm $X$ with $\\exp(X)=A$. Any such $X$ must have $\\operatorname{tr}X=\\ln(\\det A)=0$, hence $X\\in\\mathfrak{sl}(2,\\mathbb{R})$. Thus the question reduces to whether each listed matrix admits a real logarithm.\n\nWe use the standard criterion (Culver's theorem): an invertible real matrix $A$ has a real logarithm if and only if for each negative real eigenvalue, the number of Jordan blocks associated to that eigenvalue is even. In particular, in dimension $2$, if $A$ has two distinct negative real eigenvalues, then each occurs in exactly one $1\\times 1$ Jordan block, which is odd, so no real logarithm exists. If both eigenvalues are positive, a real logarithm exists. If $A$ is a rotation in $SO(2)$, it is $\\exp$ of a real skew-symmetric matrix. If $A$ is unipotent with a single Jordan block, it is $\\exp$ of a nilpotent matrix.\n\nNow we check each option.\n\nA. $\\begin{pmatrix} 1 & 3 \\\\ 0 & 1 \\end{pmatrix}$. Let $N=\\begin{pmatrix} 0 & 3 \\\\ 0 & 0 \\end{pmatrix}$, which is nilpotent with $N^{2}=0$. Then\n$$\n\\exp(N)=I+N=\\begin{pmatrix} 1 & 3 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nThus A is in the image.\n\nB. $\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$. For $\\theta\\in\\mathbb{R}$, let $X_{\\theta}=\\begin{pmatrix} 0 & -\\theta \\\\ \\theta & 0 \\end{pmatrix}$. Then\n$$\n\\exp(X_{\\theta})=\\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}.\n$$\nChoosing $\\theta=\\frac{\\pi}{2}$ gives B, so B is in the image.\n\nC. $\\begin{pmatrix} -2 & 0 \\\\ 0 & -\\frac{1}{2} \\end{pmatrix}$. This matrix has two distinct negative real eigenvalues $-2$ and $-\\frac{1}{2}$, each appearing in a single $1\\times 1$ Jordan block. By the criterion above, no real logarithm exists. Hence C is not in the image.\n\nD. $\\begin{pmatrix} 3 & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix}$. Define $X=\\begin{pmatrix} \\ln 3 & 0 \\\\ 0 & -\\ln 3 \\end{pmatrix}$, which satisfies $\\operatorname{tr}X=0$. Then\n$$\n\\exp(X)=\\begin{pmatrix} \\exp(\\ln 3) & 0 \\\\ 0 & \\exp(-\\ln 3) \\end{pmatrix}=\\begin{pmatrix} 3 & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix}.\n$$\nThus D is in the image.\n\nE. $\\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$. Let $X=\\begin{pmatrix} 0 & -\\pi \\\\ \\pi & 0 \\end{pmatrix}$. Then\n$$\n\\exp(X)=\\begin{pmatrix} \\cos\\pi & -\\sin\\pi \\\\ \\sin\\pi & \\cos\\pi \\end{pmatrix}=\\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}.\n$$\nThus E is in the image.\n\nTherefore, among the listed matrices, the only element of $SL(2,\\mathbb{R})$ that is not in the image of the exponential map is option C.", "answer": "$$\\boxed{C}$$", "id": "1678758"}, {"introduction": "The Lie algebra $\\mathfrak{so}(3)$ is central to geometry and physics, as it generates the group of rotations in three-dimensional space, $SO(3)$. A thorough examination of $\\mathfrak{so}(3)$ serves as a powerful case study, connecting the abstract definition of a Lie algebra as the tangent space at the identity to concrete calculations involving its basis, structure constants, and the fundamental Jacobi identity. This comprehensive exercise guides you through such an analysis, revealing the deep isomorphism between the algebra of skew-symmetric matrices and the familiar algebra of vector cross products. [@problem_id:3031872]", "problem": "Let $G$ denote the Special Orthogonal group (SO(3)) consisting of all real $3 \\times 3$ matrices $A$ with $A^{\\mathsf{T}}A=I$ and $\\det(A)=1$, viewed as a Lie group inside the General Linear group (GL) $GL(3,\\mathbb{R})$. Using only the definition of the Lie algebra of a matrix Lie group as the tangent space at the identity and the commutator bracket of matrices, proceed as follows:\n\n1. Identify the Lie algebra $\\mathfrak{so}(3)$ as the subspace of $M_{3}(\\mathbb{R})$ of real $3 \\times 3$ matrices tangent to $G$ at the identity, and characterize it intrinsically by an equation involving matrix transpose.\n\n2. Show that every element $X \\in \\mathfrak{so}(3)$ can be written uniquely in the form\n$$\nX=\\begin{pmatrix}\n0 & -x_{3} & x_{2} \\\\\nx_{3} & 0 & -x_{1} \\\\\n-x_{2} & x_{1} & 0\n\\end{pmatrix},\n$$\nfor some $(x_{1},x_{2},x_{3}) \\in \\mathbb{R}^{3}$. Using this, choose the standard basis $\\{E_{1},E_{2},E_{3}\\}$ given by the matrices obtained by setting one of $x_{1},x_{2},x_{3}$ equal to $1$ and the others to $0$.\n\n3. Compute the Lie brackets $[E_{i},E_{j}]$ using the commutator $[X,Y]=XY-YX$ and hence determine the structure constants $c_{ij}^{k}$ with respect to the basis $\\{E_{1},E_{2},E_{3}\\}$ defined by $[E_{i},E_{j}]=c_{ij}^{k}E_{k}$ (Einstein summation convention in force).\n\n4. Verify the Jacobi identity in terms of the structure constants, namely that\n$$\nc_{ij}^{m}c_{mk}^{\\;\\;l}+c_{jk}^{m}c_{mi}^{\\;\\;l}+c_{ki}^{m}c_{mj}^{\\;\\;l}=0\n$$\nfor all indices $i,j,k,l \\in \\{1,2,3\\}$, using only multilinear algebra identities valid in $\\mathbb{R}^{3}$.\n\nFinally, define the scalar\n$$\nS:=c_{ij}^{k}c_{ij}^{k},\n$$\nwhere repeated indices are summed from $1$ to $3$. Report the exact value of $S$ as your final answer. Express your final answer as a single number without units. No rounding is required for this problem.", "solution": "The problem asks for a step-by-step analysis of the Lie algebra $\\mathfrak{so}(3)$ of the special orthogonal group $SO(3)$, culminating in the calculation of a scalar invariant $S$ derived from its structure constants.\n\n**Part 1: Characterization of the Lie Algebra $\\mathfrak{so}(3)$**\n\nThe Lie algebra $\\mathfrak{so}(3)$ is the tangent space to the Lie group $SO(3)$ at the identity element, $I$. An element $X \\in \\mathfrak{so}(3)$ is the tangent vector at $t=0$ of a smooth curve $\\gamma(t)$ in $SO(3)$ such that $\\gamma(0) = I$. That is, $X = \\gamma'(0)$.\n\nThe defining property of any matrix $A \\in SO(3)$ is $A^{\\mathsf{T}}A = I$ and $\\det(A)=1$. For a curve $\\gamma(t)$ lying entirely in $SO(3)$, this means $\\gamma(t)^{\\mathsf{T}}\\gamma(t) = I$ for all $t$ in some interval around $0$. Differentiating this equation with respect to $t$ using the product rule yields:\n$$\n\\frac{d}{dt}(\\gamma(t)^{\\mathsf{T}}\\gamma(t)) = \\frac{d}{dt}(I)\n$$\n$$\n\\gamma'(t)^{\\mathsf{T}}\\gamma(t) + \\gamma(t)^{\\mathsf{T}}\\gamma'(t) = 0\n$$\nEvaluating this at $t=0$, we use $\\gamma(0)=I$ and $\\gamma'(0)=X$:\n$$\nX^{\\mathsf{T}}I + I^{\\mathsf{T}}X = 0\n$$\n$$\nX^{\\mathsf{T}} + X = 0\n$$\nThis demonstrates that any element $X$ of the tangent space must be a skew-symmetric matrix.\n\nConversely, for any skew-symmetric matrix $X \\in M_3(\\mathbb{R})$ (i.e., $X^{\\mathsf{T}} = -X$), the matrix exponential $\\gamma(t) = \\exp(tX)$ defines a curve in $GL(3,\\mathbb{R})$. We check if it lies in $SO(3)$.\nFirst, $(\\exp(tX))^{\\mathsf{T}} = \\exp(tX^{\\mathsf{T}}) = \\exp(-tX) = (\\exp(tX))^{-1}$. Thus, $\\gamma(t)^{\\mathsf{T}}\\gamma(t)=I$.\nSecond, $\\det(\\exp(tX)) = \\exp(\\text{tr}(tX)) = \\exp(t \\cdot \\text{tr}(X))$. For any skew-symmetric matrix, all diagonal elements are zero, so $\\text{tr}(X) = 0$. This gives $\\det(\\exp(tX)) = \\exp(0) = 1$.\nThe curve $\\gamma(t)$ with $\\gamma(0) = I$ lies in $SO(3)$, and its tangent vector at $t=0$ is $\\gamma'(0) = X \\exp(0 \\cdot X) = X$. Thus, any real $3 \\times 3$ skew-symmetric matrix is in $\\mathfrak{so}(3)$.\n\nTherefore, the Lie algebra $\\mathfrak{so}(3)$ is precisely the space of all $3 \\times 3$ real skew-symmetric matrices, characterized by the equation $X^{\\mathsf{T}} + X = 0$.\n\n**Part 2: Parametrization and Basis for $\\mathfrak{so}(3)$**\n\nLet $X$ be an arbitrary element of $\\mathfrak{so}(3)$. The condition $X^{\\mathsf{T}} + X = 0$ implies that its components $x_{ij}$ must satisfy $x_{ji} + x_{ij} = 0$.\nFor the diagonal elements ($i=j$), this means $2x_{ii}=0$, so $x_{11}=x_{22}=x_{33}=0$.\nFor the off-diagonal elements, we have $x_{21}=-x_{12}$, $x_{31}=-x_{13}$, and $x_{32}=-x_{23}$.\nThe space of such matrices is $3$-dimensional. We can introduce three independent parameters $x_1, x_2, x_3 \\in \\mathbb{R}$ by setting $x_{32}=x_1$, $x_{13}=x_2$, and $x_{21}=x_3$. This yields the specified form:\n$$\nX = \\begin{pmatrix}\nx_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23} \\\\\nx_{31} & x_{32} & x_{33}\n\\end{pmatrix} = \\begin{pmatrix}\n0 & -x_3 & x_2 \\\\\nx_3 & 0 & -x_1 \\\\\n-x_2 & x_1 & 0\n\\end{pmatrix}\n$$\nThis representation is unique for any given $X \\in \\mathfrak{so}(3)$. The standard basis $\\{E_1, E_2, E_3\\}$ is obtained by setting one of the parameters to $1$ and the others to $0$:\n- For $(x_1, x_2, x_3) = (1, 0, 0)$: $E_1 = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix}$\n- For $(x_1, x_2, x_3) = (0, 1, 0)$: $E_2 = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 0 \\end{pmatrix}$\n- For $(x_1, x_2, x_3) = (0, 0, 1)$: $E_3 = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$\n\n**Part 3: Lie Brackets and Structure Constants**\n\nThe Lie bracket is the matrix commutator, $[X,Y]=XY-YX$. We compute the brackets for the basis elements:\n$[E_1, E_2] = E_1E_2 - E_2E_1$:\n$$\nE_1E_2 = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nE_2E_1 = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\n[E_1, E_2] = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = E_3\n$$\nBy cyclic permutation of indices $(1,2,3)$, or by direct calculation:\n$[E_2, E_3] = E_1$\n$[E_3, E_1] = E_2$\n\nThe structure constants $c_{ij}^k$ are defined by the relation $[E_i, E_j] = c_{ij}^k E_k$ (summation over $k$ is implied).\nFrom our calculations:\n- $[E_1, E_2] = E_3 = 0 \\cdot E_1 + 0 \\cdot E_2 + 1 \\cdot E_3 \\implies c_{12}^1=0, c_{12}^2=0, c_{12}^3=1$.\n- $[E_2, E_3] = E_1 = 1 \\cdot E_1 + 0 \\cdot E_2 + 0 \\cdot E_3 \\implies c_{23}^1=1, c_{23}^2=0, c_{23}^3=0$.\n- $[E_3, E_1] = E_2 = 0 \\cdot E_1 + 1 \\cdot E_2 + 0 \\cdot E_3 \\implies c_{31}^1=0, c_{31}^2=1, c_{31}^3=0$.\n\nUsing the anti-symmetry of the bracket, $[E_j, E_i] = -[E_i, E_j]$, we have $c_{ji}^k = -c_{ij}^k$. Also, $[E_i, E_i]=0 \\implies c_{ii}^k=0$.\nThe full set of non-zero structure constants can be compactly described using the Levi-Civita symbol $\\epsilon_{ijk}$:\n$c_{ij}^k = \\epsilon_{ijk}$, where $\\epsilon_{123}=1$ and it is anti-symmetric under the exchange of any two indices.\n\n**Part 4: Verification of the Jacobi Identity**\n\nThe Jacobi identity for a Lie algebra is $[[X,Y],Z] + [[Y,Z],X] + [[Z,X],Y] = 0$. In terms of the structure constants, this is expressed as:\n$$\nc_{ij}^{m}c_{mk}^{\\;\\;l}+c_{jk}^{m}c_{mi}^{\\;\\;l}+c_{ki}^{m}c_{mj}^{\\;\\;l}=0\n$$\nfor all $i,j,k,l \\in \\{1,2,3\\}$.\nSubstituting $c_{ij}^k = \\epsilon_{ijk}$, the identity becomes:\n$$\n\\epsilon_{ijm}\\epsilon_{mkl} + \\epsilon_{jkm}\\epsilon_{mil} + \\epsilon_{kim}\\epsilon_{mjl} = 0\n$$\nThis corresponds to the Jacobi identity for the vector cross product in $\\mathbb{R}^3$. Let $\\{e_1, e_2, e_3\\}$ be an orthonormal basis for $\\mathbb{R}^3$. The cross product is given by $e_i \\times e_j = \\sum_k \\epsilon_{ijk} e_k$. The Lie algebra structure of $(\\mathbb{R}^3, \\times)$ is isomorphic to $(\\mathfrak{so}(3), [\\cdot,\\cdot])$ via the map $e_i \\mapsto E_i$.\nThe Jacobi identity for vectors $a, b, c \\in \\mathbb{R}^3$ is $a \\times (b \\times c) + b \\times (c \\times a) + c \\times (a \\times b) = 0$.\nWe can prove this using the \"BAC-CAB\" rule, which is a fundamental identity of multilinear algebra in $\\mathbb{R}^3$: $u \\times (v \\times w) = v(u \\cdot w) - w(u \\cdot v)$.\nApplying this rule to each term:\n- $a \\times (b \\times c) = b(a \\cdot c) - c(a \\cdot b)$\n- $b \\times (c \\times a) = c(b \\cdot a) - a(b \\cdot c)$\n- $c \\times (a \\times b) = a(c \\cdot b) - b(c \\cdot a)$\nSumming these three equations and using the commutativity of the dot product (e.g., $a \\cdot c = c \\cdot a$):\n$$\n[b(a \\cdot c) - b(c \\cdot a)] + [-c(a \\cdot b) + c(b \\cdot a)] + [-a(b \\cdot c) + a(c \\cdot b)] = 0 + 0 + 0 = 0\n$$\nThis identity holds for any vectors $a,b,c$. By choosing $a=e_i$, $b=e_j$, $c=e_k$ and taking the $l$-th component, we recover the indexed form of the Jacobi identity for the structure constants. The verification is thus complete.\n\n**Final Calculation of $S$**\n\nThe scalar $S$ is defined as $S := c_{ij}^{k}c_{ij}^{k}$, with summation over all repeated indices $i,j,k$ from $1$ to $3$.\nSubstituting $c_{ij}^k = \\epsilon_{ijk}$, we have:\n$$\nS = \\sum_{i=1}^3 \\sum_{j=1}^3 \\sum_{k=1}^3 \\epsilon_{ijk} \\epsilon_{ijk} = \\sum_{i,j,k} (\\epsilon_{ijk})^2\n$$\nThe value of $(\\epsilon_{ijk})^2$ is $1$ if $\\{i,j,k\\}$ is a permutation of $\\{1,2,3\\}$, and $0$ if any two indices are equal.\nTherefore, the sum consists of adding $1$ for each distinct permutation of $(1,2,3)$. The number of such permutations is $3! = 6$. These are $(1,2,3)$, $(2,3,1)$, $(3,1,2)$ (even permutations, $\\epsilon=1$) and $(1,3,2)$, $(3,2,1)$, $(2,1,3)$ (odd permutations, $\\epsilon=-1$). In all six cases, $(\\epsilon_{ijk})^2 = 1$.\nThus, $S$ is the sum of six terms, each equal to $1$.\n$$\nS = 1+1+1+1+1+1 = 6\n$$\nAlternatively, using the identity for the contraction of two Levi-Civita symbols, $\\sum_{k=1}^3 \\epsilon_{ijk} \\epsilon_{abk} = \\delta_{ia}\\delta_{jb} - \\delta_{ib}\\delta_{ja}$:\n$$\nS = \\sum_{i=1}^3 \\sum_{j=1}^3 \\left( \\sum_{k=1}^3 \\epsilon_{ijk}\\epsilon_{ijk} \\right)\n$$\nFor the inner sum, we set $a=i$ and $b=j$:\n$$\n\\sum_{k=1}^3 \\epsilon_{ijk}\\epsilon_{ijk} = \\delta_{ii}\\delta_{jj} - \\delta_{ij}\\delta_{ji}\n$$\nNow we sum over $i$ and $j$:\n$$\nS = \\sum_{i=1}^3 \\sum_{j=1}^3 (\\delta_{ii}\\delta_{jj} - \\delta_{ij}\\delta_{ji}) = \\left(\\sum_{i=1}^3 \\delta_{ii}\\right)\\left(\\sum_{j=1}^3 \\delta_{jj}\\right) - \\sum_{i=1}^3 \\sum_{j=1}^3 \\delta_{ij}\\delta_{ji}\n$$\nThe sum $\\sum_{i=1}^3 \\delta_{ii} = \\delta_{11}+\\delta_{22}+\\delta_{33} = 1+1+1 = 3$.\nThe term $\\sum_{i,j} \\delta_{ij}\\delta_{ji} = \\sum_{i,j} \\delta_{ij}$ (since $\\delta_{ji}=\\delta_{ij}$ and $\\delta_{ij}^2 = \\delta_{ij}$). This sum is non-zero only when $i=j$, so it becomes $\\sum_{i=1}^3 \\delta_{ii} = 3$.\nTherefore,\n$$\nS = (3)(3) - 3 = 9 - 3 = 6\n$$\nBoth methods yield the same result.", "answer": "$$\\boxed{6}$$", "id": "3031872"}]}