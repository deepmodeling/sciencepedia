{"hands_on_practices": [{"introduction": "A deep understanding of symmetric spaces begins with their underlying algebraic framework, the Cartan decomposition of the Lie algebra $\\mathfrak{g} = \\mathfrak{k} \\oplus \\mathfrak{p}$. This first practice [@problem_id:2969878] provides a concrete, hands-on opportunity to demystify these abstract definitions for the canonical example of $SL(n,\\mathbb{R})/SO(n)$. Mastering the skills of identifying the subspace $\\mathfrak{p}$ and constructing a maximal abelian subspace $\\mathfrak{a}$ from first principles is a foundational step for any further exploration of the subject.", "problem": "Let $G$ be the Special Linear group (SL) $SL(n,\\mathbb{R})$, and let $K$ be the Special Orthogonal group (SO) $SO(n)$. Consider the Riemannian symmetric space $M = G/K$ with Lie algebra $\\mathfrak{g} = \\mathfrak{sl}(n,\\mathbb{R})$, and let $\\theta : \\mathfrak{g} \\to \\mathfrak{g}$ be the Cartan involution defined by $\\theta(X) = -X^{\\top}$. The Cartan decomposition is the vector space decomposition $\\mathfrak{g} = \\mathfrak{k} \\oplus \\mathfrak{p}$ into the $+1$ and $-1$ eigenspaces of $\\theta$.\n\nStarting from these definitions and using only the following fundamental facts:\n- $\\mathfrak{sl}(n,\\mathbb{R})$ consists of all real $n \\times n$ matrices with trace equal to $0$,\n- the eigenspaces of the Cartan involution $\\theta(X) = -X^{\\top}$ are $\\mathfrak{k} = \\{X \\in \\mathfrak{sl}(n,\\mathbb{R}) : X^{\\top} = -X\\}$ and $\\mathfrak{p} = \\{X \\in \\mathfrak{sl}(n,\\mathbb{R}) : X^{\\top} = X\\}$,\n\nperform the following tasks:\n1. Identify explicitly the subspace $\\mathfrak{p} \\subset \\mathfrak{sl}(n,\\mathbb{R})$.\n2. Construct an explicit vector subspace $\\mathfrak{a} \\subset \\mathfrak{p}$ that is abelian under the Lie bracket and justify from first principles why it is abelian.\n3. Prove that your chosen $\\mathfrak{a}$ is maximal abelian in $\\mathfrak{p}$, in the sense that if $X \\in \\mathfrak{p}$ commutes with every element of $\\mathfrak{a}$, then $X \\in \\mathfrak{a}$.\n4. Determine the dimension $\\dim \\mathfrak{a}$ as a function of $n$.\n\nExpress your final answer as a single simplified algebraic expression in $n$. Do not round your answer. Your final answer must be the value of $\\dim \\mathfrak{a}$ only.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the theory of Lie algebras and symmetric spaces, with all definitions and constraints being correct and self-contained. I will proceed with a full solution.\n\nThe problem asks for a four-part analysis of the Lie algebra $\\mathfrak{g} = \\mathfrak{sl}(n,\\mathbb{R})$ associated with the symmetric space $G/K = SL(n,\\mathbb{R})/SO(n)$. The analysis revolves around the Cartan decomposition $\\mathfrak{g} = \\mathfrak{k} \\oplus \\mathfrak{p}$ induced by the involution $\\theta(X) = -X^{\\top}$.\n\n**1. Identification of the subspace $\\mathfrak{p}$**\n\nThe Lie algebra $\\mathfrak{g} = \\mathfrak{sl}(n, \\mathbb{R})$ consists of all real $n \\times n$ matrices with trace zero. The subspace $\\mathfrak{p}$ is defined as the $-1$-eigenspace of the Cartan involution $\\theta(X) = -X^{\\top}$. This means $\\mathfrak{p}$ consists of all elements $X \\in \\mathfrak{g}$ such that $\\theta(X) = -X$.\nSubstituting the definition of $\\theta$, we get:\n$$-X^{\\top} = -X$$\n$$X^{\\top} = X$$\nThis condition states that the matrices in $\\mathfrak{p}$ must be symmetric. Combining this with the condition that these matrices must belong to $\\mathfrak{sl}(n, \\mathbb{R})$, we find that they must also have a trace of zero.\nTherefore, the subspace $\\mathfrak{p}$ is explicitly the set of all real, $n \\times n$ symmetric matrices with trace equal to $0$.\n$$\\mathfrak{p} = \\{X \\in M_n(\\mathbb{R}) : X^{\\top} = X \\text{ and } \\operatorname{tr}(X) = 0\\}$$\n\n**2. Construction and justification of an abelian subspace $\\mathfrak{a} \\subset \\mathfrak{p}$**\n\nWe seek a vector subspace $\\mathfrak{a}$ of $\\mathfrak{p}$ such that for any two elements $X, Y \\in \\mathfrak{a}$, their Lie bracket is zero: $[X,Y] = 0$. For matrix Lie algebras, the Lie bracket is the commutator $[X,Y] = XY - YX$.\nA well-known family of commuting matrices is the set of diagonal matrices. Let us consider the set of all diagonal matrices that belong to $\\mathfrak{p}$.\nLet $\\mathfrak{a}$ be the set of all $n \\times n$ real diagonal matrices with trace zero.\n$$ \\mathfrak{a} = \\{ \\operatorname{diag}(d_1, d_2, \\dots, d_n) : d_i \\in \\mathbb{R} \\text{ and } \\sum_{i=1}^n d_i = 0 \\} $$\nWe must verify that $\\mathfrak{a}$ is a subspace of $\\mathfrak{p}$ and that it is abelian.\nFirst, any diagonal matrix $D$ is symmetric, since $D^{\\top} = D$. By definition, every element of $\\mathfrak{a}$ also has trace zero. Thus, any element of $\\mathfrak{a}$ satisfies the conditions to be in $\\mathfrak{p}$, so $\\mathfrak{a} \\subset \\mathfrak{p}$.\nSecond, $\\mathfrak{a}$ is a vector subspace. Let $D_1 = \\operatorname{diag}(a_1, \\dots, a_n)$ and $D_2 = \\operatorname{diag}(b_1, \\dots, b_n)$ be in $\\mathfrak{a}$, and let $c \\in \\mathbb{R}$ be a scalar. Then $\\sum a_i = 0$ and $\\sum b_i = 0$.\nThe sum $D_1 + D_2 = \\operatorname{diag}(a_1+b_1, \\dots, a_n+b_n)$ is a diagonal matrix. Its trace is $\\sum(a_i+b_i) = \\sum a_i + \\sum b_i = 0+0=0$. So $D_1+D_2 \\in \\mathfrak{a}$.\nThe scalar multiple $c D_1 = \\operatorname{diag}(ca_1, \\dots, ca_n)$ is a diagonal matrix. Its trace is $\\sum(ca_i) = c \\sum a_i = c \\cdot 0 = 0$. So $c D_1 \\in \\mathfrak{a}$.\nHence, $\\mathfrak{a}$ is a vector subspace of $\\mathfrak{p}$.\nFinally, we must show that $\\mathfrak{a}$ is abelian. Let $D_1 = \\operatorname{diag}(a_1, \\dots, a_n)$ and $D_2 = \\operatorname{diag}(b_1, \\dots, b_n)$ be two elements of $\\mathfrak{a}$. Their product is:\n$$ D_1 D_2 = \\operatorname{diag}(a_1 b_1, \\dots, a_n b_n) $$\n$$ D_2 D_1 = \\operatorname{diag}(b_1 a_1, \\dots, b_n a_n) $$\nSince scalar multiplication is commutative ($a_i b_i = b_i a_i$), we have $D_1 D_2 = D_2 D_1$. Therefore, the Lie bracket is:\n$$ [D_1, D_2] = D_1 D_2 - D_2 D_1 = 0 $$\nThis holds for all $D_1, D_2 \\in \\mathfrak{a}$, so $\\mathfrak{a}$ is an abelian subspace.\n\n**3. Proof of maximal abelian property**\n\nWe need to prove that $\\mathfrak{a}$ is a *maximal* abelian subspace of $\\mathfrak{p}$. This means that if there is an element $X \\in \\mathfrak{p}$ that commutes with every element of $\\mathfrak{a}$, then $X$ must be an element of $\\mathfrak{a}$.\nLet $X = (x_{ij})$ be an element of $\\mathfrak{p}$. This means $X$ is symmetric ($x_{ij} = x_{ji}$) and has trace zero ($\\operatorname{tr}(X) = 0$).\nAssume that $[X, A] = 0$ for all $A \\in \\mathfrak{a}$. This means $XA = AX$ for all diagonal matrices $A$ with trace zero.\nLet $A = \\operatorname{diag}(a_1, a_2, \\dots, a_n)$ with $\\sum a_i = 0$. The $(i,j)$-th entry of the matrix equation $XA = AX$ is:\n$$ (XA)_{ij} = \\sum_{k=1}^n x_{ik} A_{kj} = x_{ij} a_j $$\n$$ (AX)_{ij} = \\sum_{k=1}^n A_{ik} x_{kj} = a_i x_{ij} $$\nThus, the condition $XA = AX$ implies $x_{ij} a_j = a_i x_{ij}$ for all $i,j \\in \\{1, \\dots, n\\}$. This can be rewritten as:\n$$ (a_i - a_j) x_{ij} = 0 $$\nThis equation must hold for any choice of $a_1, \\dots, a_n$ such that $\\sum a_k = 0$. To show that $X$ is a diagonal matrix (i.e., $x_{ij}=0$ for $i \\neq j$), we need to find, for any pair of distinct indices $i \\neq j$, an element $A \\in \\mathfrak{a}$ such that $a_i \\neq a_j$.\nLet us construct such a matrix $A$. Assume $n \\ge 2$. Consider the matrix $A_k = E_{kk} - \\frac{1}{n}I$, where $E_{kk}$ is the matrix with a $1$ at position $(k,k)$ and $0$ elsewhere, and $I$ is the identity matrix. The trace of $A_k$ is $1 - \\frac{1}{n} \\cdot n = 0$, so $A_k \\in \\mathfrak{a}$ for any $k \\in \\{1,\\ldots,n\\}$.\nLet's choose a specific $A \\in \\mathfrak{a}$. For $n \\ge 2$, consider the diagonal matrix $A' = \\operatorname{diag}(1, -1, 0, \\dots, 0)$. Its trace is $1-1=0$, so $A' \\in \\mathfrak{a}$. For this matrix, $a_1 = 1$, $a_2 = -1$, and $a_i = 0$ for $i > 2$.\nSince $[X, A'] = 0$, we have $(a_i - a_j)x_{ij} = 0$. For $i=1, j=2$, we have $(1 - (-1))x_{12} = 2x_{12} = 0$, which implies $x_{12}=0$.\nTo generalize this, let us pick an element $A \\in \\mathfrak{a}$ that has all its diagonal entries distinct. For $n \\geq 2$, such a matrix exists. For example, let $A = \\operatorname{diag}(1, 2, \\dots, n) - cI$, where $c = \\frac{1}{n}\\sum_{k=1}^n k = \\frac{n+1}{2}$. The diagonal entries are $a_k = k - c$, which are all distinct, and the trace is $\\sum_{k=1}^n (k-c) = (\\sum k) - nc = 0$. So this $A$ is in $\\mathfrak{a}$.\nSince the condition $[X,A]=0$ must hold for this specific $A$, we have $(a_i - a_j)x_{ij} = 0$ for all $i,j$. Because all diagonal entries $a_k$ of this particular $A$ are distinct, $a_i - a_j \\neq 0$ whenever $i \\neq j$.\nThis forces $x_{ij} = 0$ for all $i \\neq j$.\nThis shows that $X$ must be a diagonal matrix.\nSo, we have an element $X \\in \\mathfrak{p}$ which we have now shown must be a diagonal matrix. The properties of $X \\in \\mathfrak{p}$ are that it is symmetric (which is true for any diagonal matrix) and that $\\operatorname{tr}(X)=0$.\nA diagonal matrix with trace zero is, by our definition, an element of $\\mathfrak{a}$.\nTherefore, $X \\in \\mathfrak{a}$. This completes the proof that $\\mathfrak{a}$ is a maximal abelian subspace of $\\mathfrak{p}$.\n\n**4. Determination of the dimension of $\\mathfrak{a}$**\n\nThe subspace $\\mathfrak{a}$ consists of all real $n \\times n$ diagonal matrices of the form $D = \\operatorname{diag}(d_1, d_2, \\dots, d_n)$. The space of all such diagonal matrices is an $n$-dimensional real vector space, with a basis given by the matrices $E_{ii}$ for $i=1, \\dots, n$.\nThe elements of $\\mathfrak{a}$ are subject to a single additional linear constraint:\n$$ \\operatorname{tr}(D) = d_1 + d_2 + \\dots + d_n = 0 $$\nThis is one non-trivial linear equation on the $n$ coordinates $(d_1, \\dots, d_n)$. In an $n$-dimensional vector space, a single linear constraint defines a hyperplane, which is a subspace of dimension $n-1$.\nThus, the dimension of $\\mathfrak{a}$ is $n-1$.\nAlternatively, we can construct an explicit basis for $\\mathfrak{a}$. Consider the set of $n-1$ matrices:\n$$ H_1 = E_{11} - E_{22} = \\operatorname{diag}(1, -1, 0, \\dots, 0) $$\n$$ H_2 = E_{22} - E_{33} = \\operatorname{diag}(0, 1, -1, \\dots, 0) $$\n$$ \\vdots $$\n$$ H_{n-1} = E_{n-1,n-1} - E_{n,n} = \\operatorname{diag}(0, \\dots, 0, 1, -1) $$\nEach $H_i$ is a diagonal matrix with trace $0$, so $H_i \\in \\mathfrak{a}$. They are linearly independent, since $\\sum_{i=1}^{n-1} c_i H_i = \\operatorname{diag}(c_1, c_2-c_1, \\dots, c_{n-1}-c_{n-2}, -c_{n-1})$. If this sum is the zero matrix, then $c_1=0$, which implies $c_2=0$, and so on, yielding all $c_i=0$.\nAny element $A = \\operatorname{diag}(a_1, \\dots, a_n) \\in \\mathfrak{a}$ can be written as a linear combination of these basis elements. The coefficients can be found sequentially: $c_1 = a_1$, $c_2 = a_1+a_2$, and generally $c_k = \\sum_{j=1}^k a_j$. The final component is consistent because $\\sum_{j=1}^n a_j = 0$.\nSince we have found a basis consisting of $n-1$ elements, the dimension of $\\mathfrak{a}$ is confirmed to be $n-1$.\n\nThe final answer requested is the dimension of $\\mathfrak{a}$.\n$$\\dim \\mathfrak{a} = n-1$$", "answer": "$$\\boxed{n-1}$$", "id": "2969878"}, {"introduction": "The algebraic Cartan decomposition has a powerful geometric analogue at the Lie group level, the $G=KAK$ decomposition, which provides a unique 'non-Euclidean polar coordinate' system on the group. This exercise [@problem_id:2969854] makes this decomposition tangible by having you derive it for the accessible case of $SL(2, \\mathbb{R})$. By connecting the decomposition to the eigenvalues of a matrix, you will gain a deeper intuition for the Cartan projection $\\mu(g)$ and its role in measuring the 'non-compact' part of a group element.", "problem": "Let $G$ be the Special Linear Group (SL) $SL(2,\\mathbb{R})$ and let $K$ be the Special Orthogonal Group (SO) $SO(2)$. Consider the Riemannian symmetric space $G/K$ with the Cartan decomposition $G = K \\exp(\\mathfrak{a}) K$, where $\\mathfrak{a} \\subset \\mathfrak{p}$ is a maximal abelian subalgebra in the $(-1)$-eigenspace $\\mathfrak{p}$ of the Cartan involution associated to $K$. For $SL(2,\\mathbb{R})$, identify $K = SO(2)$ and $A = \\exp(\\mathfrak{a}) = \\{ \\operatorname{diag}(\\exp(t), \\exp(-t)) : t \\in \\mathbb{R} \\}$, and let $\\mathfrak{a}^{+} = \\{ \\operatorname{diag}(t, -t) : t \\geq 0 \\}$ denote the positive Weyl chamber. The Cartan projection $\\mu : G \\to \\mathfrak{a}^{+}$ maps $g \\in G$ to the unique $t \\geq 0$ such that in the $KAK$ decomposition $g = k_{1} a k_{2}$ with $k_{1}, k_{2} \\in K$ and $a \\in A$, one has $a = \\operatorname{diag}(\\exp(t), \\exp(-t))$.\n\nStarting from the fundamental definitions of the Cartan decomposition and the polar decomposition, and using the characterization of $K$ as the fixed point set of the Cartan involution, derive the $KAK$ form for a general element $g \\in SL(2,\\mathbb{R})$ with entries $g = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ satisfying $ad - bc = 1$. Then, express the Cartan projection $\\mu(g)$ explicitly in terms of the matrix entries $a$, $b$, $c$, $d$. Your final answer must be a single closed-form analytic expression for $\\mu(g)$.", "solution": "The problem requires the derivation of an explicit formula for the Cartan projection $\\mu(g)$ for an element $g = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in G = SL(2,\\mathbb{R})$. The Cartan projection $\\mu(g)$ is defined via the Cartan decomposition $G = KAK$, where $K = SO(2)$ and $A = \\{ \\operatorname{diag}(\\exp(t), \\exp(-t)) : t \\in \\mathbb{R} \\}$. Specifically, for any $g \\in G$, there exist $k_1, k_2 \\in K$ and a unique $t \\ge 0$ such that $g = k_1 a k_2$ with $a = \\operatorname{diag}(\\exp(t), \\exp(-t))$. The value of this unique $t$ is $\\mu(g)$.\n\nOur derivation begins with the connection between the Cartan decomposition and the polar decomposition of a matrix in $G$. Any element $g \\in G$ admits a unique polar decomposition $g = kp$, where $k \\in K = SO(2)$ is a special orthogonal matrix and $p$ is a symmetric positive-definite matrix with determinant $1$. The set of such matrices $p$ forms the space $P = \\exp(\\mathfrak{p})$, where $\\mathfrak{p}$ is the space of trace-zero symmetric $2 \\times 2$ real matrices.\n\nFrom the decomposition $g=kp$, we can determine $p$. We have $g^T = (kp)^T = p^T k^T$. Since $p$ is symmetric, $p^T = p$. Since $k$ is orthogonal, $k^T k = I$, where $I$ is the identity matrix. Thus,\n$$\ng^T g = (p k^T)(kp) = p (k^T k) p = p I p = p^2.\n$$\nThis shows that the symmetric part $p$ is the unique positive-definite square root of the matrix $g^T g$, i.e., $p = \\sqrt{g^T g}$.\n\nThe next step is to relate this matrix $p$ to the element $a$ in the Cartan decomposition. Since $p$ is a real symmetric matrix, the spectral theorem guarantees that it can be diagonalized by an orthogonal matrix. For the group $G = SL(2,\\mathbb{R})$ and subgroup $K = SO(2)$, it is a standard result that any $p \\in P$ can be diagonalized by an element of $K$. This means there exists a matrix $k_2 \\in K$ and a diagonal matrix $a'$ such that\n$$\np = k_2 a' k_2^{-1}.\n$$\nThe diagonal entries of $a'$ are the eigenvalues of $p$. Since $p$ is positive-definite and $\\det(p) = \\det(g)/\\det(k) = 1/1=1$, its eigenvalues must be of the form $\\lambda$ and $1/\\lambda$ for some $\\lambda > 0$. We can write $\\lambda = \\exp(t)$ for some $t \\in \\mathbb{R}$. By convention, we can order the eigenvalues such that $t \\ge 0$. This gives $a' = \\operatorname{diag}(\\exp(t), \\exp(-t))$, which is precisely the form of the matrix $a \\in A$ in the Cartan decomposition, with $t = \\mu(g)$. So, $p = k_2 a k_2^{-1}$ (or $p=k_2^{-1}ak_2$, the choice of which doesn't affect the eigenvalues).\n\nSubstituting this back into the polar decomposition $g=kp$, we obtain\n$$\ng = k(k_2 a k_2^{-1}).\n$$\nLetting $k_1 = k k_2$, we see that $k_1 \\in K$ since $K$ is a group. Then $g = k_1 a k_2^{-1}$. Since $K$ is a group, $k_2^{-1}$ is also in $K$. This gives the $KAK$ decomposition $g = k_1 a k_2'$ with $k_2' = k_2^{-1}$.\n\nThe crucial insight is that the diagonal entries of $a$ are the eigenvalues of $p$. Therefore, the squares of the eigenvalues of $p$ are the eigenvalues of $p^2 = g^Tg$. The eigenvalues of $a = \\operatorname{diag}(\\exp(t), \\exp(-t))$ are $\\exp(t)$ and $\\exp(-t)$. Consequently, the eigenvalues of $p^2$ are $(\\exp(t))^2 = \\exp(2t)$ and $(\\exp(-t))^2 = \\exp(-2t)$.\n\nThe sum of the eigenvalues of a matrix is equal to its trace. Therefore,\n$$\n\\operatorname{Tr}(p^2) = \\exp(2t) + \\exp(-2t).\n$$\nUsing the definition of the hyperbolic cosine function, $\\cosh(x) = (\\exp(x) + \\exp(-x))/2$, we can write this as\n$$\n\\operatorname{Tr}(p^2) = 2\\cosh(2t).\n$$\nNow, we compute the trace of $g^Tg$ using the given matrix entries $g = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. First, we find $g^Tg$:\n$$\ng^T g = \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} a^2+c^2 & ab+cd \\\\ ab+cd & b^2+d^2 \\end{pmatrix}.\n$$\nThe trace of this matrix is\n$$\n\\operatorname{Tr}(g^T g) = (a^2+c^2) + (b^2+d^2) = a^2+b^2+c^2+d^2.\n$$\nEquating the two expressions for the trace, we get\n$$\n2\\cosh(2t) = a^2+b^2+c^2+d^2.\n$$\nSolving for $\\cosh(2t)$:\n$$\n\\cosh(2t) = \\frac{a^2+b^2+c^2+d^2}{2}.\n$$\nTo ensure this equation has a real solution for $2t$, the right-hand side must be greater than or equal to $1$. We can show this using the condition $ad-bc=1$:\n$$\n(a-d)^2 + (b+c)^2 = a^2 - 2ad + d^2 + b^2 + 2bc + c^2 = (a^2+b^2+c^2+d^2) - 2(ad-bc) = (a^2+b^2+c^2+d^2) - 2.\n$$\nSince the left side is a sum of squares of real numbers, it is non-negative. Therefore, $(a^2+b^2+c^2+d^2) - 2 \\ge 0$, which implies $a^2+b^2+c^2+d^2 \\ge 2$. Consequently, $\\frac{a^2+b^2+c^2+d^2}{2} \\ge 1$.\n\nThe problem specifies that $\\mu(g) = t \\ge 0$, so $2t \\ge 0$. The inverse hyperbolic cosine function, $\\arccosh(x)$, for $x \\ge 1$, gives a non-negative value. Applying $\\arccosh$ to our equation yields\n$$\n2t = \\arccosh\\left(\\frac{a^2+b^2+c^2+d^2}{2}\\right).\n$$\nFinally, solving for $t = \\mu(g)$, we obtain the explicit expression in terms of the matrix entries:\n$$\n\\mu(g) = \\frac{1}{2}\\arccosh\\left(\\frac{a^2+b^2+c^2+d^2}{2}\\right).\n$$\nThis formula provides the value of the Cartan projection for any given element $g \\in SL(2,\\mathbb{R})$.", "answer": "$$\n\\boxed{\\frac{1}{2}\\arccosh\\left(\\frac{a^2+b^2+c^2+d^2}{2}\\right)}\n$$", "id": "2969854"}, {"introduction": "Once a maximal abelian subspace $\\mathfrak{a}$ is established, it can be used to probe the structure of the entire Lie algebra, giving rise to the fundamental concept of the restricted root system $\\Sigma$. This practice [@problem_id:2969891] develops your skills in calculating these roots and their multiplicities, which are essential for constructing the Iwasawa decomposition and for understanding the geometry of the space. By comparing the results for $SL(n,\\mathbb{R})/SO(n)$ and $SU(n,1)/S(U(n)\\times U(1))$, you will directly encounter the important distinction between reduced and non-reduced root systems.", "problem": "Let $G$ be a connected, noncompact, semisimple Lie group with finite center and $K$ a maximal compact subgroup, so that $(G,K)$ is a Riemannian symmetric pair of the noncompact type. Denote the corresponding Lie algebras by $\\mathfrak{g}$ and $\\mathfrak{k}$, with the Cartan decomposition $\\mathfrak{g} = \\mathfrak{k} \\oplus \\mathfrak{p}$ defined by a Cartan involution $\\theta$. A maximal abelian subspace of $\\mathfrak{p}$ will be denoted by $\\mathfrak{a}$. The restricted root system $\\Sigma \\subset \\mathfrak{a}^{*}$ is defined using the decomposition of $\\mathfrak{g}$ into simultaneous eigenspaces of $\\operatorname{ad}(H)$ for $H \\in \\mathfrak{a}$, with restricted root multiplicities $m_{\\alpha} = \\dim \\mathfrak{g}_{\\alpha}$. The Iwasawa decomposition is the direct sum $\\mathfrak{g} = \\mathfrak{k} \\oplus \\mathfrak{a} \\oplus \\mathfrak{n}$, where $\\mathfrak{n} = \\bigoplus_{\\alpha \\in \\Sigma^{+}} \\mathfrak{g}_{\\alpha}$ for a choice of positive system $\\Sigma^{+}$.\n\nUsing only the definitions above and first principles of linear algebra applied to explicit matrix models of the symmetric pairs, carry out the following:\n\n1. For the symmetric space $G/K = SL(n,\\mathbb{R})/SO(n)$, with $n \\geq 3$, determine the restricted root system $\\Sigma$ with respect to the standard choice $\\mathfrak{a}$ of diagonal, trace-zero real matrices in $\\mathfrak{p}$, and compute the multiplicities $m_{\\alpha}$ for all $\\alpha \\in \\Sigma$.\n\n2. For the symmetric space $G/K = SU(n,1)/S(U(n)\\times U(1))$, with $n \\geq 2$, choose a standard one-dimensional $\\mathfrak{a} \\subset \\mathfrak{p}$ and determine the restricted root system $\\Sigma$. Explicitly verify that $\\Sigma$ is nonreduced by exhibiting roots $\\alpha$ and $2\\alpha$, and compute the corresponding multiplicities $m_{\\alpha}$ and $m_{2\\alpha}$.\n\n3. Using your computations, determine $\\dim \\mathfrak{n}$ for each of the two symmetric spaces above and then compute the ratio\n$$\nR(n) \\;=\\; \\frac{\\dim \\mathfrak{n}\\text{ for }SL(n,\\mathbb{R})/SO(n)}{\\dim \\mathfrak{n}\\text{ for }SU(n,1)/S(U(n)\\times U(1))}.\n$$\nProvide your final answer as a single closed-form expression in $n$. No numerical rounding is required.", "solution": "The problem statement has been validated and found to be self-contained, scientifically grounded, and well-posed. It is a standard exercise in the a priori calculation of restricted root data for two fundamental series of Riemannian symmetric spaces of the noncompact type. I will now proceed with the solution.\n\nThe solution is divided into three parts as requested.\n\n**Part 1: $G/K = SL(n,\\mathbb{R})/SO(n)$**\n\nThe Lie group is $G = SL(n,\\mathbb{R})$, the group of $n \\times n$ real matrices with determinant $1$. Its Lie algebra is $\\mathfrak{g} = \\mathfrak{sl}(n,\\mathbb{R})$, the space of $n \\times n$ real matrices with trace $0$. We are given $n \\geq 3$. The maximal compact subgroup is $K = SO(n)$, the group of orthogonal matrices with determinant $1$. Its Lie algebra is $\\mathfrak{k} = \\mathfrak{so}(n)$, the space of $n \\times n$ real skew-symmetric matrices.\n\nThe Cartan involution $\\theta: \\mathfrak{g} \\to \\mathfrak{g}$ is given by $\\theta(X) = -X^T$. The eigenspace for eigenvalue $+1$ is $\\mathfrak{k} = \\{ X \\in \\mathfrak{g} \\mid X^T = -X \\}$, which is indeed $\\mathfrak{so}(n)$. The eigenspace for eigenvalue $-1$ is the orthogonal complement $\\mathfrak{p} = \\{ X \\in \\mathfrak{g} \\mid X^T = X \\}$, the space of $n \\times n$ real symmetric matrices with trace $0$. This gives the Cartan decomposition $\\mathfrak{g} = \\mathfrak{k} \\oplus \\mathfrak{p}$.\n\nThe problem specifies the choice of a maximal abelian subspace $\\mathfrak{a} \\subset \\mathfrak{p}$ as the set of all real diagonal matrices with trace $0$.\nLet $H \\in \\mathfrak{a}$. Then $H$ can be written as $H = \\operatorname{diag}(h_1, h_2, \\ldots, h_n)$ with $h_i \\in \\mathbb{R}$ and $\\sum_{i=1}^n h_i = 0$.\n\nTo find the restricted roots, we must find the eigenvalues of the adjoint action $\\operatorname{ad}(H)$ on $\\mathfrak{g}$ for a generic $H \\in \\mathfrak{a}$. Let $E_{ij}$ denote the standard matrix unit with a $1$ in the $(i,j)$ position and $0$ elsewhere. For $i \\neq j$, the matrices $E_{ij}$ form a basis for the off-diagonal matrices.\nThe commutation relation is:\n$$[H, E_{ij}] = H E_{ij} - E_{ij} H = h_i E_{ij} - h_j E_{ij} = (h_i - h_j) E_{ij}.$$\nThe eigenvalues of $\\operatorname{ad}(H)$ are the values $h_i - h_j$ for $1 \\leq i, j \\leq n$, $i \\neq j$. A restricted root $\\alpha \\in \\mathfrak{a}^*$ is a linear functional on $\\mathfrak{a}$ that maps $H$ to one of these eigenvalues. Let us define linear functionals $\\epsilon_i \\in \\mathfrak{a}^*$ by $\\epsilon_i(H) = h_i$. Then the restricted root system is:\n$$\\Sigma = \\{ \\epsilon_i - \\epsilon_j \\mid 1 \\leq i \\neq j \\leq n \\}.$$\nThe root space $\\mathfrak{g}_{\\alpha}$ for a root $\\alpha = \\epsilon_i - \\epsilon_j$ is the eigenspace for the eigenvalue $\\alpha(H) = h_i - h_j$. From the calculation above, this eigenspace is spanned by the matrix $E_{ij}$.\n$$\\mathfrak{g}_{\\epsilon_i - \\epsilon_j} = \\mathbb{R} E_{ij}.$$\nThe dimension of this space is the multiplicity of the root, $m_{\\alpha}$. Therefore, for every root $\\alpha \\in \\Sigma$, its multiplicity is:\n$$m_{\\alpha} = \\dim(\\mathfrak{g}_{\\alpha}) = 1.$$\n\n**Part 2: $G/K = SU(n,1)/S(U(n)\\times U(1))$**\n\nThe Lie group is $G = SU(n,1)$, the group of $(n+1) \\times (n+1)$ complex matrices $A$ with $\\det(A)=1$ that preserve the Hermitian form of signature $(n,1)$, i.e., $A^* I_{n,1} A = I_{n,1}$, where $I_{n,1} = \\operatorname{diag}(1, \\ldots, 1, -1)$. We have $n \\geq 2$. Its Lie algebra is $\\mathfrak{g} = \\mathfrak{su}(n,1) = \\{ X \\in \\mathfrak{sl}(n+1, \\mathbb{C}) \\mid X^*I_{n,1} + I_{n,1}X = 0 \\}$. The maximal compact subgroup is $K=S(U(n)\\times U(1))$ and its Lie algebra is $\\mathfrak{k} \\cong \\mathfrak{s}(\\mathfrak{u}(n)\\oplus \\mathfrak{u}(1))$.\n\nA standard Cartan involution is $\\theta(X) = -X^*$. The fixed point set $\\mathfrak{k}$ consists of skew-Hermitian matrices in $\\mathfrak{g}$, which take the block-diagonal form $X=\\operatorname{diag}(A,d)$ where $A \\in \\mathfrak{u}(n)$ and $d \\in \\mathfrak{u}(1)$. The eigenspace $\\mathfrak{p}$ consists of matrices $X$ such that $X^*=X$. For $X \\in \\mathfrak{su}(n,1)$, this means $X \\in \\mathfrak{p}$ has the form $X = \\begin{pmatrix} 0 & B \\\\ B^* & 0 \\end{pmatrix}$ where $B$ is an $n \\times 1$ complex column vector.\n\nA choice of maximal abelian subspace $\\mathfrak{a} \\subset \\mathfrak{p}$ is given by real matrices. For instance, let $B$ be a real multiple of the first standard basis vector $e_1$.\n$$\\mathfrak{a} = \\left\\{ t \\begin{pmatrix} 0 & e_1 \\\\ e_1^T & 0 \\end{pmatrix} \\mid t \\in \\mathbb{R} \\right\\}.$$\nLet $H_0 = E_{1,n+1} + E_{n+1,1} \\in \\mathfrak{a}$. The space $\\mathfrak{a}$ is one-dimensional, so this is a rank-one symmetric space.\n\nTo determine the root system, we diagonalize $H_0$. Its eigenvalues are $1, -1$, and $n-1$ times $0$. Let $P$ be an orthogonal matrix that diagonalizes $H_0$, for instance by mapping the basis $(e_1, \\dots, e_{n+1})$ to $(v_1, v_{-1}, e_2, \\dots, e_n)$ where $v_{\\pm 1} = (e_1 \\pm e_{n+1})/\\sqrt{2}$. In this new basis, $H_0$ becomes $H' = \\operatorname{diag}(1, -1, 0, \\ldots, 0)$. The Lie algebra is conjugated to $\\mathfrak{g}' = P^{-1}\\mathfrak{g}P$, whose elements $X'$ satisfy $(X')^*I' + I'X' = 0$ and $\\operatorname{tr}(X')=0$, where $I' = P^T I_{n,1} P = \\begin{pmatrix} J & 0 \\\\ 0 & I_{n-1} \\end{pmatrix}$ with $J=\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n\nThe eigenvalues of $\\operatorname{ad}(H')$ are of the form $\\lambda_i-\\lambda_j$, where $\\lambda = (1, -1, 0, \\ldots, 0)$ are the diagonal entries of $H'$. The possible nonzero eigenvalues are $\\pm 1$ and $\\pm 2$. Let $\\alpha \\in \\mathfrak{a}^*$ be the functional such that $\\alpha(tH')=t$. Then the restricted roots are $\\Sigma = \\{\\pm\\alpha, \\pm 2\\alpha\\}$. This root system is nonreduced, as it contains both a root and its double.\n\nWe now compute the multiplicities. An element $X' \\in \\mathfrak{g}'$ can be written in block form corresponding to the partition $(2, n-1)$: $X' = \\begin{pmatrix} A & B \\\\ C & D \\end{pmatrix}$. The condition $(X')^*I'+I'X'=0$ translates to:\n$1.$ $A^*J+JA=0$\n$2.$ $D^*+D=0$ (i.e. $D$ is skew-Hermitian)\n$3.$ $C = -B^*J$\n\nThe restricted root spaces are eigenspaces of $\\operatorname{ad}(H')$.\n- **Root $2\\alpha$ (eigenvalue $2$):** An eigenvector $X'$ must satisfy $[H',X']=2X'$. This forces $B, C, D$ to be zero, and for $A=\\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}$, we need $[\\operatorname{diag}(1,-1), A]=2A$. This implies $a_{11}=a_{22}=a_{21}=0$.\nSo $X'$ is of the form $a_{12}E_{12}$. The condition $A^*J+JA=0$ becomes $(a_{12}+\\bar{a}_{12})E_{22}=0$, so $a_{12}$ must be purely imaginary. Thus, the root space $\\mathfrak{g}_{2\\alpha}$ is spanned by $iE_{12}$ over $\\mathbb{R}$.\n$$m_{2\\alpha} = \\dim_{\\mathbb{R}}(\\mathfrak{g}_{2\\alpha}) = 1.$$\n- **Root $\\alpha$ (eigenvalue $1$):** An eigenvector $X'$ for eigenvalue $1$ must satisfy $[H',X']=X'$. This implies $A=0, D=0$. For the off-diagonal blocks $B$ and $C$, we need $\\operatorname{diag}(1,-1)B=B$ and $-C\\operatorname{diag}(1,-1)=C$.\nThis means $B$ has only a non-zero top row, $\\mathbf{b} \\in \\mathbb{C}^{n-1}$, and $C$ has only a non-zero second column, $\\mathbf{c} \\in \\mathbb{C}^{n-1}$.\nThe condition $C=-B^*J$ connects $\\mathbf{b}$ and $\\mathbf{c}$. Writing out the blocks explicitly, $B = \\begin{pmatrix} \\mathbf{b} \\\\ \\mathbf{0}^T \\end{pmatrix}$ and $C= \\begin{pmatrix} \\mathbf{0} & \\mathbf{c} \\end{pmatrix}$.\n$C = - B^* J = - \\begin{pmatrix} \\bar{\\mathbf{b}}^* & \\mathbf{0}\\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = -\\begin{pmatrix} \\mathbf{0} & \\bar{\\mathbf{b}}^*\\end{pmatrix}$.\nSo we must have $\\mathbf{c} = -\\bar{\\mathbf{b}}^*$. The space of such matrices is parametrized by the vector $\\mathbf{b} \\in \\mathbb{C}^{n-1}$, which is an $(n-1)$-dimensional complex vector space. The real dimension is $2(n-1)$.\n$$m_{\\alpha} = \\dim_{\\mathbb{R}}(\\mathfrak{g}_{\\alpha}) = 2(n-1).$$\n\n**Part 3: Dimension Ratio**\n\nThe dimension of the nilpotent Lie algebra $\\mathfrak{n}$ in the Iwasawa decomposition is the sum of the multiplicities of the positive roots.\n\n- **For $SL(n,\\mathbb{R})/SO(n)$:** A choice of positive roots is $\\Sigma^+ = \\{\\epsilon_i - \\epsilon_j \\mid 1 \\leq i < j \\leq n\\}$. The number of positive roots is $|\\Sigma^+| = \\binom{n}{2} = \\frac{n(n-1)}{2}$. Since each root has multiplicity $m_\\alpha=1$, the dimension of $\\mathfrak{n}$ is:\n$$\\dim \\mathfrak{n}_{SL} = \\sum_{\\alpha \\in \\Sigma^+} m_{\\alpha} = \\frac{n(n-1)}{2} \\times 1 = \\frac{n(n-1)}{2}.$$\n\n- **For $SU(n,1)/S(U(n)\\times U(1))$:** A choice of positive roots is $\\Sigma^+ = \\{\\alpha, 2\\alpha\\}$. The dimension of $\\mathfrak{n}$ is the sum of their multiplicities:\n$$\\dim \\mathfrak{n}_{SU} = m_{\\alpha} + m_{2\\alpha} = 2(n-1) + 1 = 2n - 1.$$\n\n- **The Ratio $R(n)$:** We compute the ratio of these two dimensions, for $n \\geq 3$ where both are defined.\n$$R(n) = \\frac{\\dim \\mathfrak{n}_{\\text{ for }SL(n,\\mathbb{R})/SO(n)}}{\\dim \\mathfrak{n}_{\\text{ for }SU(n,1)/S(U(n)\\times U(1))}} = \\frac{\\frac{n(n-1)}{2}}{2n-1}.$$\nThis simplifies to the final expression.", "answer": "$$\\boxed{\\frac{n(n-1)}{2(2n-1)}}$$", "id": "2969891"}]}