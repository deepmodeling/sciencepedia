{"hands_on_practices": [{"introduction": "Understanding index raising and lowering begins with the algebraic machinery that underpins these operations. This first exercise [@problem_id:2980489] strips away the complexity of curved manifolds to focus on the fundamental role of the metric tensor and its inverse. By working with a non-standard, non-diagonal metric on $\\mathbb{R}^2$, you will directly apply the definitions of the musical isomorphisms to transform vector components to covector components and vice versa, reinforcing the core computational skills from first principles.", "problem": "Let $\\mathbb{R}^{2}$ be equipped with the Riemannian metric $g$ whose matrix of components in the standard basis $\\{e_{1},e_{2}\\}$ is\n$$\nG = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix},\n$$\nso that for vectors $v, w \\in \\mathbb{R}^{2}$ with column coordinate vectors (in the standard basis) $[v]$ and $[w]$, the inner product is $g(v,w)=[v]^{\\mathsf{T}}\\,G\\,[w]$. The musical isomorphisms are the linear isomorphisms $g^{\\flat}:\\mathbb{R}^{2}\\to(\\mathbb{R}^{2})^{\\ast}$ and $g^{\\sharp}:(\\mathbb{R}^{2})^{\\ast}\\to\\mathbb{R}^{2}$ defined by $g^{\\flat}(v)(\\cdot)=g(v,\\cdot)$ and $g^{\\sharp}(\\alpha)$ is the unique vector satisfying $g(g^{\\sharp}(\\alpha),\\cdot)=\\alpha(\\cdot)$. Work from these definitions.\n\nTasks:\n1. Construct an explicit basis $\\mathcal{B}=\\{b_{1},b_{2}\\}$ of $\\mathbb{R}^{2}$ that is not $g$-orthonormal, and justify that it is not $g$-orthonormal using only the definition of $g$.\n2. Compute the inverse matrix $G^{-1}$ of the metric in the standard basis from first principles.\n3. Let $v\\in\\mathbb{R}^{2}$ be the vector with contravariant components $v^{1}=1$ and $v^{2}=-2$ in the standard basis, and let $\\alpha\\in(\\mathbb{R}^{2})^{\\ast}$ be the covector with covariant components $\\alpha_{1}=3$ and $\\alpha_{2}=1$ in the dual of the standard basis. Using only the definitions of the musical isomorphisms, compute the lowered components $v_{i}$ of $g^{\\flat}(v)$ and the raised components $\\alpha^{i}$ of $g^{\\sharp}(\\alpha)$, both with respect to the standard bases.\n\nReport your final results concatenated in the order: the four entries of $G^{-1}$ listed row-wise, then the two lowered components $v_{i}$ (listed as $v_{1}$, then $v_{2}$), then the two raised components $\\alpha^{i}$ (listed as $\\alpha^{1}$, then $\\alpha^{2}$). Express the final answer as a single row matrix. No rounding is required; give exact values.", "solution": "The problem is well-posed and internally consistent, grounded in the standard principles of Riemannian geometry. All necessary definitions and data are provided. We proceed with the solution.\n\nThe Riemannian metric $g$ on $\\mathbb{R}^{2}$ is given by the matrix of its components in the standard basis $\\{e_{1}, e_{2}\\}$, which is $G = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. The inner product of two vectors $v, w \\in \\mathbb{R}^{2}$ with coordinate vectors $[v]$ and $[w]$ is $g(v,w)=[v]^{\\mathsf{T}}G[w]$.\n\nFirst, we construct a basis $\\mathcal{B}=\\{b_{1}, b_{2}\\}$ that is not $g$-orthonormal. We can test the standard basis $\\mathcal{B}=\\{e_{1}, e_{2}\\}$ itself. A basis is $g$-orthonormal if $g(b_{i}, b_{j})=\\delta_{ij}$, the Kronecker delta. We compute the inner products of the standard basis vectors using the given definition of $g$. The coordinate vectors for $e_{1}$ and $e_{2}$ are $[e_{1}] = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $[e_{2}] = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe inner product of $e_{1}$ with itself is:\n$$g(e_{1}, e_{1}) = [e_{1}]^{\\mathsf{T}}G[e_{1}] = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2(1) + 1(0) = 2.$$\nThe inner product of $e_{1}$ with $e_{2}$ is:\n$$g(e_{1}, e_{2}) = [e_{1}]^{\\mathsf{T}}G[e_{2}] = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 2(0) + 1(1) = 1.$$\nSince $g(e_{1},e_{1})=2 \\neq 1$, the vector $e_{1}$ is not of unit length. Since $g(e_{1}, e_{2})=1 \\neq 0$, the vectors $e_{1}$ and $e_{2}$ are not orthogonal with respect to the metric $g$. Therefore, the standard basis $\\{e_{1}, e_{2}\\}$ is not $g$-orthonormal. This serves as the required example.\n\nSecond, we compute the inverse matrix $G^{-1}$ from first principles. This requires finding a matrix, which we denote $G^{-1} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, such that $G G^{-1} = I$, where $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ is the identity matrix. The matrix equation is:\n$$ \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}. $$\nThis single matrix equation decomposes into two independent systems of linear equations. For the first column of $G^{-1}$:\n$$ \\begin{cases} 2a + c = 1 \\\\ a + 2c = 0 \\end{cases} $$\nFrom the second equation, $a = -2c$. Substituting this into the first equation gives $2(-2c) + c = 1$, which simplifies to $-3c = 1$, so $c = -\\frac{1}{3}$. Then $a = -2(-\\frac{1}{3}) = \\frac{2}{3}$.\nFor the second column of $G^{-1}$:\n$$ \\begin{cases} 2b + d = 0 \\\\ b + 2d = 1 \\end{cases} $$\nFrom the first equation, $d = -2b$. Substituting this into the second equation gives $b + 2(-2b) = 1$, which simplifies to $-3b = 1$, so $b = -\\frac{1}{3}$. Then $d = -2(-\\frac{1}{3}) = \\frac{2}{3}$.\nThus, the inverse matrix is $G^{-1} = \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}$.\n\nThird, we compute the lowered and raised components.\nLet $v \\in \\mathbb{R}^{2}$ be the vector with contravariant components $v^{1}=1$ and $v^{2}=-2$ in the standard basis. So, $v = 1e_{1} - 2e_{2}$, and its coordinate vector is $[v] = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$. The lowered covector is $\\omega = g^{\\flat}(v)$, and its components $v_{i}$ in the dual basis $\\{e^{1}, e^{2}\\}$ are given by $v_{i} = \\omega(e_{i})$. Using the definition of the flat map, $g^{\\flat}(v)(\\cdot) = g(v, \\cdot)$, we have $v_{i} = g(v, e_{i})$.\nFor $i=1$:\n$$v_{1} = g(v, e_{1}) = [v]^{\\mathsf{T}}G[e_{1}] = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 1(2) + (-2)(1) = 0.$$\nFor $i=2$:\n$$v_{2} = g(v, e_{2}) = [v]^{\\mathsf{T}}G[e_{2}] = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1(1) + (-2)(2) = -3.$$\nThe lowered components are $v_{1} = 0$ and $v_{2} = -3$.\n\nLet $\\alpha \\in (\\mathbb{R}^{2})^{\\ast}$ be the covector with covariant components $\\alpha_{1}=3$ and $\\alpha_{2}=1$. So, $\\alpha = 3e^{1} + 1e^{2}$. The raised vector is $u = g^{\\sharp}(\\alpha)$, and we want to find its components $\\alpha^{i}$ in the standard basis, so that $u = \\alpha^{1}e_{1} + \\alpha^{2}e_{2}$. The coordinate vector is $[u] = \\begin{pmatrix} \\alpha^{1} \\\\ \\alpha^{2} \\end{pmatrix}$.\nThe defining property of the sharp map is $g(u, w) = \\alpha(w)$ for any vector $w \\in \\mathbb{R}^{2}$. We test this property with the basis vectors $w=e_{1}$ and $w=e_{2}$.\nFor $w=e_{1}$:\n$$g(u, e_{1}) = [u]^{\\mathsf{T}}G[e_{1}] = \\begin{pmatrix} \\alpha^{1} & \\alpha^{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2\\alpha^{1} + \\alpha^{2}.$$\nAlso, $\\alpha(e_{1}) = (3e^{1} + 1e^{2})(e_{1}) = 3e^{1}(e_{1}) + 1e^{2}(e_{1}) = 3(1) + 1(0) = 3$.\nThis gives the equation $2\\alpha^{1} + \\alpha^{2} = 3$.\nFor $w=e_{2}$:\n$$g(u, e_{2}) = [u]^{\\mathsf{T}}G[e_{2}] = \\begin{pmatrix} \\alpha^{1} & \\alpha^{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\alpha^{1} + 2\\alpha^{2}.$$\nAlso, $\\alpha(e_{2}) = (3e^{1} + 1e^{2})(e_{2}) = 3e^{1}(e_{2}) + 1e^{2}(e_{2}) = 3(0) + 1(1) = 1$.\nThis gives the equation $\\alpha^{1} + 2\\alpha^{2} = 1$.\nWe now solve the system:\n$$ \\begin{cases} 2\\alpha^{1} + \\alpha^{2} = 3 \\\\ \\alpha^{1} + 2\\alpha^{2} = 1 \\end{cases} $$\nFrom the first equation, $\\alpha^{2} = 3 - 2\\alpha^{1}$. Substituting into the second: $\\alpha^{1} + 2(3-2\\alpha^{1})=1$, which gives $\\alpha^{1} + 6 - 4\\alpha^{1} = 1$, or $-3\\alpha^{1} = -5$. Thus, $\\alpha^{1} = \\frac{5}{3}$.\nThen, $\\alpha^{2} = 3 - 2(\\frac{5}{3}) = 3 - \\frac{10}{3} = \\frac{9-10}{3} = -\\frac{1}{3}$.\nThe raised components are $\\alpha^{1} = \\frac{5}{3}$ and $\\alpha^{2} = -\\frac{1}{3}$.\n\nThe final results, in the specified order, are the entries of $G^{-1}$ (row-wise), the components $v_{i}$, and the components $\\alpha^{i}$.\nEntries of $G^{-1}$: $\\frac{2}{3}, -\\frac{1}{3}, -\\frac{1}{3}, \\frac{2}{3}$.\nComponents $v_{i}$: $0, -3$.\nComponents $\\alpha^{i}$: $\\frac{5}{3}, -\\frac{1}{3}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} & \\frac{2}{3} & 0 & -3 & \\frac{5}{3} & -\\frac{1}{3} \\end{pmatrix}}\n$$", "id": "2980489"}, {"introduction": "Having mastered the algebraic basics, we now apply these concepts to a quintessential example in geometry: the unit sphere $S^2$. This practice [@problem_id:2980504] guides you through computing the famous 'round metric' induced by the sphere's embedding in Euclidean space. You will then find the inverse metric and see how the abstract process of index manipulation manifests in a concrete, curved setting using spherical coordinates.", "problem": "Let $S^{2}$ be the unit $2$-sphere embedded in $\\mathbb{R}^{3}$ with the standard Euclidean inner product. Consider the spherical coordinate chart $(\\theta,\\varphi)$ with $\\theta \\in (0,\\pi)$ and $\\varphi \\in (0,2\\pi)$ given by the smooth embedding $F:(0,\\pi)\\times(0,2\\pi)\\to \\mathbb{R}^{3}$ defined by $F(\\theta,\\varphi)=\\left(\\sin\\theta\\cos\\varphi,\\sin\\theta\\sin\\varphi,\\cos\\theta\\right)$. The round Riemannian metric $g$ on $S^{2}$ is the pullback of the Euclidean metric via $F$, so its coordinate components $g_{ij}$ are given by the fundamental definition $g_{ij}=\\langle \\partial_{i}F,\\partial_{j}F\\rangle$, where $\\langle \\cdot,\\cdot\\rangle$ denotes the Euclidean inner product and $\\partial_{i}$ denotes partial differentiation with respect to the coordinate $x^{i}$, with $x^{1}=\\theta$ and $x^{2}=\\varphi$.\n\nUsing only this definition of the induced metric, compute the inverse metric $g^{-1}=(g^{ij})$ in these coordinates by inverting the $2\\times 2$ matrix $(g_{ij})$, and then verify from first principles the index-raising identity $g^{ik}g_{kj}=\\delta^{i}{}_{j}$, where $\\delta^{i}{}_{j}$ is the Kronecker delta and the Einstein summation convention is adopted. Your final answer should be the explicit $2\\times 2$ matrix expression for $g^{-1}$ in terms of $\\theta$ and $\\varphi$. No numerical approximations are required.", "solution": "The problem is valid. It is a standard, well-posed exercise in elementary Riemannian geometry and is scientifically sound.\n\nOur objective is to compute the components of the induced metric tensor $g = (g_{ij})$ on the unit $2$-sphere $S^2$ in spherical coordinates, find its inverse $g^{-1} = (g^{ij})$, and verify the identity $g^{ik}g_{kj} = \\delta^{i}{}_{j}$.\n\nThe embedding of the sphere in $\\mathbb{R}^3$ is given by the map $F(\\theta,\\varphi) = (\\sin\\theta\\cos\\varphi, \\sin\\theta\\sin\\varphi, \\cos\\theta)$, where we associate the coordinates $x^1 = \\theta$ and $x^2 = \\varphi$.\n\nFirst, we compute the tangent vectors to the coordinate curves, $\\partial_{\\theta}F$ and $\\partial_{\\varphi}F$. These form a basis for the tangent space at each point on the sphere.\n$$\n\\partial_{\\theta}F = \\frac{\\partial F}{\\partial \\theta} = (\\cos\\theta\\cos\\varphi, \\cos\\theta\\sin\\varphi, -\\sin\\theta)\n$$\n$$\n\\partial_{\\varphi}F = \\frac{\\partial F}{\\partial \\varphi} = (-\\sin\\theta\\sin\\varphi, \\sin\\theta\\cos\\varphi, 0)\n$$\n\nThe components of the metric tensor, $g_{ij}$, are defined as the Euclidean inner products of these basis vectors: $g_{ij} = \\langle \\partial_{i}F, \\partial_{j}F \\rangle$. We compute each component:\n\n$1$. $g_{\\theta\\theta} = g_{11} = \\langle \\partial_{\\theta}F, \\partial_{\\theta}F \\rangle$\n$$\ng_{\\theta\\theta} = (\\cos\\theta\\cos\\varphi)^{2} + (\\cos\\theta\\sin\\varphi)^{2} + (-\\sin\\theta)^{2}\n$$\n$$\ng_{\\theta\\theta} = \\cos^{2}\\theta\\cos^{2}\\varphi + \\cos^{2}\\theta\\sin^{2}\\varphi + \\sin^{2}\\theta\n$$\n$$\ng_{\\theta\\theta} = \\cos^{2}\\theta(\\cos^{2}\\varphi + \\sin^{2}\\varphi) + \\sin^{2}\\theta\n$$\n$$\ng_{\\theta\\theta} = \\cos^{2}\\theta(1) + \\sin^{2}\\theta = 1\n$$\n\n$2$. $g_{\\theta\\varphi} = g_{12} = \\langle \\partial_{\\theta}F, \\partial_{\\varphi}F \\rangle$\n$$\ng_{\\theta\\varphi} = (\\cos\\theta\\cos\\varphi)(-\\sin\\theta\\sin\\varphi) + (\\cos\\theta\\sin\\varphi)(\\sin\\theta\\cos\\varphi) + (-\\sin\\theta)(0)\n$$\n$$\ng_{\\theta\\varphi} = -\\sin\\theta\\cos\\theta\\cos\\varphi\\sin\\varphi + \\sin\\theta\\cos\\theta\\sin\\varphi\\cos\\varphi = 0\n$$\nSince the metric tensor is symmetric, we have $g_{\\varphi\\theta} = g_{21} = g_{\\theta\\varphi} = 0$.\n\n$3$. $g_{\\varphi\\varphi} = g_{22} = \\langle \\partial_{\\varphi}F, \\partial_{\\varphi}F \\rangle$\n$$\ng_{\\varphi\\varphi} = (-\\sin\\theta\\sin\\varphi)^{2} + (\\sin\\theta\\cos\\varphi)^{2} + (0)^{2}\n$$\n$$\ng_{\\varphi\\varphi} = \\sin^{2}\\theta\\sin^{2}\\varphi + \\sin^{2}\\theta\\cos^{2}\\varphi\n$$\n$$\ng_{\\varphi\\varphi} = \\sin^{2}\\theta(\\sin^{2}\\varphi + \\cos^{2}\\varphi)\n$$\n$$\ng_{\\varphi\\varphi} = \\sin^{2}\\theta(1) = \\sin^{2}\\theta\n$$\n\nAssembling these components, we obtain the matrix representation of the metric tensor:\n$$\n(g_{ij}) = \\begin{pmatrix} g_{\\theta\\theta} & g_{\\theta\\varphi} \\\\ g_{\\varphi\\theta} & g_{\\varphi\\varphi} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sin^2\\theta \\end{pmatrix}\n$$\n\nNext, we compute the inverse metric tensor $(g^{ij})$, which is the matrix inverse of $(g_{ij})$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $(g_{ij})$ is $\\det(g_{ij}) = (1)(\\sin^2\\theta) - (0)(0) = \\sin^2\\theta$. Since the coordinate chart is defined for $\\theta \\in (0, \\pi)$, we have $\\sin\\theta > 0$, and thus $\\det(g_{ij}) \\neq 0$.\nThe inverse matrix is:\n$$\n(g^{ij}) = \\frac{1}{\\sin^2\\theta} \\begin{pmatrix} \\sin^2\\theta & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\sin^2\\theta} \\end{pmatrix}\n$$\nThus, the components of the inverse metric are $g^{\\theta\\theta}=1$, $g^{\\theta\\varphi}=g^{\\varphi\\theta}=0$, and $g^{\\varphi\\varphi}=\\frac{1}{\\sin^2\\theta}$.\n\nFinally, we verify the identity $g^{ik}g_{kj} = \\delta^{i}{}_{j}$ from first principles. This corresponds to the matrix multiplication $(g^{ik})(g_{kj})$, which must yield the identity matrix $I$.\n$$\n(g^{ik})(g_{kj}) = \\begin{pmatrix} g^{\\theta\\theta} & g^{\\theta\\varphi} \\\\ g^{\\varphi\\theta} & g^{\\varphi\\varphi} \\end{pmatrix} \\begin{pmatrix} g_{\\theta\\theta} & g_{\\theta\\varphi} \\\\ g_{\\varphi\\theta} & g_{\\varphi\\varphi} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\sin^2\\theta} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sin^2\\theta \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n\\begin{pmatrix} (1)(1) + (0)(0) & (1)(0) + (0)(\\sin^2\\theta) \\\\ (0)(1) + (\\frac{1}{\\sin^2\\theta})(0) & (0)(0) + (\\frac{1}{\\sin^2\\theta})(\\sin^2\\theta) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis resulting matrix is the identity matrix, which corresponds to the Kronecker delta $\\delta^{i}{}_{j}$ in index notation. Let's explicitly check the four component equations using the Einstein summation convention over the index $k \\in \\{\\theta, \\varphi\\}$:\nFor $(i,j) = (\\theta,\\theta)$: $g^{\\theta k}g_{k\\theta} = g^{\\theta\\theta}g_{\\theta\\theta} + g^{\\theta\\varphi}g_{\\varphi\\theta} = (1)(1) + (0)(0) = 1 = \\delta^{\\theta}{}_{\\theta}$.\nFor $(i,j) = (\\theta,\\varphi)$: $g^{\\theta k}g_{k\\varphi} = g^{\\theta\\theta}g_{\\theta\\varphi} + g^{\\theta\\varphi}g_{\\varphi\\varphi} = (1)(0) + (0)(\\sin^2\\theta) = 0 = \\delta^{\\theta}{}_{\\varphi}$.\nFor $(i,j) = (\\varphi,\\theta)$: $g^{\\varphi k}g_{k\\theta} = g^{\\varphi\\theta}g_{\\theta\\theta} + g^{\\varphi\\varphi}g_{\\varphi\\theta} = (0)(1) + (\\frac{1}{\\sin^2\\theta})(0) = 0 = \\delta^{\\varphi}{}_{\\theta}$.\nFor $(i,j) = (\\varphi,\\varphi)$: $g^{\\varphi k}g_{k\\varphi} = g^{\\varphi\\theta}g_{\\theta\\varphi} + g^{\\varphi\\varphi}g_{\\varphi\\varphi} = (0)(0) + (\\frac{1}{\\sin^2\\theta})(\\sin^2\\theta) = 1 = \\delta^{\\varphi}{}_{\\varphi}$.\nThe verification is complete. The problem asks for the explicit matrix expression for $g^{-1}$, which is $(g^{ij})$.\n$$\ng^{-1} = (g^{ij}) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\sin^2\\theta} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\sin^{2}\\theta} \\end{pmatrix}}\n$$", "id": "2980504"}, {"introduction": "The precision of tensor notation, with its careful placement of upper and lower indices, is essential for avoiding errors in complex geometric calculations. This final practice [@problem_id:2980527] challenges you to translate this notational rigor into a computational model. By designing a program that explicitly tracks index variance and enforces the rules of contraction, you will gain a profound and practical understanding of how musical isomorphisms are robustly implemented in modern computational geometry.", "problem": "A Riemannian metric $g$ on a smooth manifold assigns to each point a symmetric positive-definite (SPD) bilinear form on the tangent space. In local coordinates, this is represented by a symmetric matrix with entries $g_{ij}$. The metric $g$ yields canonical isomorphisms between the tangent space and the cotangent space, known as the musical isomorphisms: the flat map $\\,\\flat: TM \\to T^*M$ and the sharp map $\\,\\sharp: T^*M \\to TM$. By definition, for a tangent vector $v$ and a covector $\\alpha$, the flat map $v^\\flat$ satisfies $v^\\flat(\\cdot) = g(v,\\cdot)$, and the sharp map $\\alpha^\\sharp$ is the unique vector such that $g(\\cdot,\\alpha^\\sharp) = \\alpha(\\cdot)$. In coordinates, the flat map is $v^\\flat_i = g_{ij} v^j$ and the sharp map is $\\alpha^\\sharp{}^i = g^{ij} \\alpha_j$, where $g^{ij}$ are the components of the inverse metric matrix.\n\nDesign a programmatic representation of tensors that stores explicit index positions and variances (upper indices for contravariant components and lower indices for covariant components) to avoid ambiguity in computations. Your program must implement:\n- The flat musical isomorphism $\\,\\flat$ using a covariant rank-$2$ metric tensor with components $g_{ij}$ to map a contravariant rank-$1$ vector $v^i$ to a covariant rank-$1$ covector $v^\\flat_i$.\n- The sharp musical isomorphism $\\,\\sharp$ using the contravariant rank-$2$ inverse metric tensor with components $g^{ij}$ to map a covariant rank-$1$ covector $\\alpha_i$ to a contravariant rank-$1$ vector $\\alpha^\\sharp{}^i$.\n- Explicit contraction operations that require one upper index and one lower index for the contracted pair, rejecting any operation that attempts to contract two upper indices or two lower indices.\n\nStart from the fundamental definitions:\n- A Riemannian metric is a symmetric bilinear form $g: TM \\times TM \\to \\mathbb{R}$ that is positive-definite.\n- The musical isomorphisms $\\,\\flat$ and $\\,\\sharp$ are defined by $v^\\flat(\\cdot) = g(v,\\cdot)$ and $g(\\cdot,\\alpha^\\sharp) = \\alpha(\\cdot)$, respectively.\n\nFrom these definitions, derive how to implement raising and lowering indices and contractions in a way that respects index variance and ordering. The design must ensure that ambiguity is avoided by enforcing variance compatibility in contractions and by making the contraction axes explicit.\n\nImplement the following test suite with the given coordinate representations. In each case, treat $g$ as a matrix $(g_{ij})$ and its inverse $g^{-1}$ as $(g^{ij})$. All numbers below are exact constants and must be used as is.\n\nCase $A$ (dimension $n = 2$):\n- Metric $g_{ij} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}$.\n- Vector $v^i = (3,-1)$.\n- Covector $\\alpha_i = (1,2)$.\n- Vector $w^i = (0.5,-2)$.\n\nCase $B$ (dimension $n = 3$):\n- Metric $g_{ij} = \\begin{pmatrix} 4 & 1 & 0 \\\\ 1 & 5 & 2 \\\\ 0 & 2 & 6 \\end{pmatrix}$.\n- Vector $v^i = (1,0,-1)$.\n- Covector $\\alpha_i = (2,-3,4)$.\n- Vector $w^i = (0,1,1)$.\n\nCase $C$ (dimension $n = 2$):\n- Metric $g_{ij} = \\begin{pmatrix} 10^{-6} & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n- Vector $v^i = (1,1)$.\n- Covector $\\alpha_i = (-1,2)$.\n- Vector $w^i = (3,3)$.\n\nFor each case, compute the following outputs:\n- $o_1$: a boolean indicating whether $g$ is symmetric positive-definite (that is, $g_{ij} = g_{ji}$ and all eigenvalues are strictly positive).\n- $o_2$: a boolean indicating whether $(v^\\flat)^\\sharp$ equals $v$ within an absolute tolerance of $10^{-12}$.\n- $o_3$: a boolean indicating whether $(\\alpha^\\sharp)^\\flat$ equals $\\alpha$ within an absolute tolerance of $10^{-12}$.\n- $o_4$: a float equal to $\\left|\\alpha(v) - g\\big(v,\\alpha^\\sharp\\big)\\right|$, where $\\alpha(v) = \\alpha_i v^i$ and $g\\big(v,\\alpha^\\sharp\\big) = g_{ij} v^i \\alpha^\\sharp{}^j$.\n- $o_5$: a float equal to the Euclidean norm of the difference between the two ways of contracting $g^{ij}$ with $\\alpha_j$ to produce $\\alpha^\\sharp{}^i$: using the first upper index of $g^{ij}$ versus using the second upper index. Since $g^{ij}$ is symmetric, this value should be numerically small.\n- $o_6$: a boolean indicating whether attempting to contract $v^i$ with $w^i$ (two upper indices) is correctly rejected by the program.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by cases and outputs as\n$[o_1^A,o_2^A,o_3^A,o_4^A,o_5^A,o_6^A,o_1^B,o_2^B,o_3^B,o_4^B,o_5^B,o_6^B,o_1^C,o_2^C,o_3^C,o_4^C,o_5^C,o_6^C]$.\nNo physical units or angles appear in this problem; all outputs are dimensionless. The program must be entirely self-contained and must not read any input. The numerical comparisons should use absolute tolerance $10^{-12}$ where specified.", "solution": "The problem requires the implementation of a programmatic framework for tensor operations in Riemannian geometry, specifically the musical isomorphisms (flat $\\flat$ and sharp $\\sharp$) and tensor contraction, while rigorously enforcing rules of index variance. The solution is structured around a `Tensor` class that encapsulates the numerical data and the variance of each index.\n\nFirst, we define a tensor. A tensor of type $(p,q)$ is a multilinear map. In a chosen basis, it is represented by a multi-dimensional array of components, with $p$ contravariant (upper) indices and $q$ covariant (lower) indices. To avoid ambiguity, our programmatic representation, the `Tensor` class, will store not only the `numpy` array of its components but also a signature for its indices, a list of strings indicating whether each index is `'up'` (contravariant) or `'down'` (covariant). For instance, a metric tensor $g_{ij}$ is a rank-$2$ covariant tensor, which we represent with indices `('down', 'down')`. A vector $v^i$ is a rank-$1$ contravariant tensor, represented with index `('up',)`.\n\nThe core operation is tensor contraction, which involves summing over a pair of indices, one contravariant and one covariant. For two tensors $A$ and $B$, contracting the $k$-th index of $A$ with the $l$-th index of $B$ is valid only if one index is contravariant and the other is covariant. Our `contract` function enforces this rule. It takes two `Tensor` objects and the integer positions of the axes to be contracted. It first validates that the index variances are compatible (`'up'` with `'down'`). If the contraction is valid, it uses `numpy.tensordot` to perform the numerical computation. The resulting `Tensor` object is constructed with the remaining indices from the original tensors in the correct order. An attempt to contract indices of the same variance (e.g., `'up'` with `'up'`) must be rejected, which we implement by raising a `TypeError`.\n\nThe musical isomorphisms are implemented using this contraction mechanism.\n- The flat map, $\\flat$, lowers the index of a contravariant vector $v^i$ to produce a covariant vector (covector) $v^\\flat_i$. The defining relation is $v^\\flat(\\cdot) = g(v, \\cdot)$, which in local coordinates becomes $v^\\flat_i = g_{ij} v^j$. This is a contraction of the contravariant index $j$ of the vector $v^j$ with the second covariant index of the metric tensor $g_{ij}$. In our framework, this corresponds to `contract(g, 1, v, 0)`, where `g` is a `Tensor` of type `('down', 'down')` and `v` is of type `('up',)`. The result is a `Tensor` of type `('down',)`.\n\n- The sharp map, $\\sharp$, raises the index of a covariant vector $\\alpha_i$ to produce a contravariant vector $\\alpha^{\\sharp i}$. The defining relation is $g(\\cdot, \\alpha^\\sharp) = \\alpha(\\cdot)$. This requires the inverse metric tensor $g^{ij}$, whose components form a matrix that is the inverse of the matrix of $g_{ij}$. The coordinate expression is $\\alpha^{\\sharp i} = g^{ij} \\alpha_j$. This is a contraction of the covariant index $j$ of the covector $\\alpha_j$ with the second contravariant index of the inverse metric $g^{ij}$. This corresponds to `contract(g_inv, 1, alpha, 0)`, where `g_inv` is a `Tensor` of type `('up', 'up')` and `alpha` is of type `('down',)`. The result is a `Tensor` of type `('up',)`.\n\nThe test suite requires computing six outputs ($o_1$ to $o_6$) for each case. Here is the methodology for each:\n- $o_1$: A Riemannian metric must be symmetric and positive-definite. We verify symmetry by checking if the matrix of components $G = (g_{ij})$ is equal to its transpose, $G=G^T$. We verify positive-definiteness by computing the eigenvalues of $G$ and checking if they are all strictly positive. Since $G$ is symmetric, all its eigenvalues are real.\n\n- $o_2$ and $o_3$: The flat and sharp maps are inverses of each other when the metric is non-degenerate. This means $(v^\\flat)^\\sharp = v$ and $(\\alpha^\\sharp)^\\flat = \\alpha$. We verify this by composing the implemented operations and comparing the resulting tensor's data with the original tensor's data using an absolute tolerance of $10^{-12}$.\n\n- $o_4$: This output verifies the definition of the sharp map, $g(\\cdot, \\alpha^\\sharp) = \\alpha(\\cdot)$. Evaluating both sides on a vector $v$ should yield the same scalar: $g(v, \\alpha^\\sharp) = \\alpha(v)$. In coordinates, this is the identity $g_{ij} v^i \\alpha^{\\sharp j} = \\alpha_j v^j$. We compute both sides of the equation using our contraction framework and calculate the absolute difference of the resulting scalars.\n\n- $o_5$: This output tests the consequence of the metric tensor's symmetry. Since $g_{ij}$ is symmetric, its inverse $g^{ij}$ is also symmetric, meaning $g^{ij} = g^{ji}$. The sharp operation $\\alpha^{\\sharp i} = g^{ij}\\alpha_j$ can be thought of as contracting $\\alpha_j$ with the second index of $g^{ij}$. If we were to contract with the first index instead, we would compute $g^{ji} \\alpha_i$ (after relabeling the summation index). Due to symmetry, the results should be identical. We compute both results, which in our framework correspond to `contract(g_inv, 1, alpha, 0)` and `contract(g_inv, 0, alpha, 0)`, and then find the Euclidean norm of the difference between their data arrays. Analytically, this difference is zero.\n\n- $o_6$: This output verifies that the contraction logic correctly rejects invalid operations. Contracting two contravariant vectors, such as $v^i$ and $w^i$, is not a defined operation in standard tensor algebra. Our `contract` function is designed to raise an exception in this scenario. The test is successful if this exception is caught, so we wrap the invalid contraction attempt in a `try...except` block.", "answer": "```python\nimport numpy as np\n\nclass Tensor:\n    \"\"\"\n    Represents a tensor with explicit index variance.\n    \"\"\"\n    def __init__(self, data, indices):\n        \"\"\"\n        Initializes a Tensor.\n\n        Args:\n            data (array-like): The numerical components of the tensor.\n            indices (tuple of str): A tuple where each element is 'up' (contravariant)\n                                    or 'down' (covariant).\n        \"\"\"\n        self.data = np.array(data, dtype=float)\n        self.indices = tuple(indices)\n        if self.data.ndim != len(self.indices):\n            raise ValueError(\"Number of dimensions of data must match the number of indices.\")\n\n    def __repr__(self):\n        return f\"Tensor(data=\\n{self.data},\\n indices={self.indices})\"\n\ndef contract(t1, axis1, t2, axis2):\n    \"\"\"\n    Contracts two tensors along specified axes, enforcing index variance rules.\n    \"\"\"\n    if not ((t1.indices[axis1] == 'up' and t2.indices[axis2] == 'down') or\n            (t1.indices[axis1] == 'down' and t2.indices[axis2] == 'up')):\n        raise TypeError(f\"Invalid contraction: cannot contract {t1.indices[axis1]} \"\n                        f\"(axis {axis1} of T1) with {t2.indices[axis2]} (axis {axis2} of T2).\")\n\n    new_data = np.tensordot(t1.data, t2.data, axes=([axis1], [axis2]))\n    \n    t1_indices_rem = list(t1.indices)\n    t1_indices_rem.pop(axis1)\n    \n    t2_indices_rem = list(t2.indices)\n    t2_indices_rem.pop(axis2)\n    \n    new_indices = tuple(t1_indices_rem + t2_indices_rem)\n    \n    return Tensor(new_data, new_indices)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and generate results.\n    \"\"\"\n    test_cases = [\n        {\n            \"g_data\": [[2, 0], [0, 3]],\n            \"v_data\": [3, -1],\n            \"alpha_data\": [1, 2],\n            \"w_data\": [0.5, -2]\n        },\n        {\n            \"g_data\": [[4, 1, 0], [1, 5, 2], [0, 2, 6]],\n            \"v_data\": [1, 0, -1],\n            \"alpha_data\": [2, -3, 4],\n            \"w_data\": [0, 1, 1]\n        },\n        {\n            \"g_data\": [[1e-6, 0], [0, 1]],\n            \"v_data\": [1, 1],\n            \"alpha_data\": [-1, 2],\n            \"w_data\": [3, 3]\n        }\n    ]\n\n    results = []\n    TOL = 1e-12\n\n    for case in test_cases:\n        # 1. Initialize tensors\n        g = Tensor(case[\"g_data\"], ('down', 'down'))\n        v = Tensor(case[\"v_data\"], ('up',))\n        alpha = Tensor(case[\"alpha_data\"], ('down',))\n        w = Tensor(case[\"w_data\"], ('up',))\n        \n        try:\n            g_inv_data = np.linalg.inv(g.data)\n            g_inv = Tensor(g_inv_data, ('up', 'up'))\n        except np.linalg.LinAlgError:\n            # Handle non-invertible metric if it occurred\n            results.extend([False, False, False, float('nan'), float('nan'), False])\n            continue\n\n        # o1: is g symmetric positive-definite?\n        is_symmetric = np.allclose(g.data, g.data.T, atol=TOL, rtol=0)\n        try:\n            eigenvalues = np.linalg.eigvalsh(g.data)\n            is_positive_definite = np.all(eigenvalues > 0)\n        except np.linalg.LinAlgError:\n            is_positive_definite = False\n        o1 = is_symmetric and is_positive_definite\n        results.append(str(o1).lower())\n\n        # o2: is (v_flat)_sharp == v?\n        v_flat = contract(g, 1, v, 0)\n        v_flat_sharp = contract(g_inv, 1, v_flat, 0)\n        o2 = np.allclose(v_flat_sharp.data, v.data, atol=TOL, rtol=0)\n        results.append(str(o2).lower())\n        \n        # o3: is (alpha_sharp)_flat == alpha?\n        alpha_sharp = contract(g_inv, 1, alpha, 0)\n        alpha_sharp_flat = contract(g, 1, alpha_sharp, 0)\n        o3 = np.allclose(alpha_sharp_flat.data, alpha.data, atol=TOL, rtol=0)\n        results.append(str(o3).lower())\n\n        # o4: |alpha(v) - g(v, alpha_sharp)|\n        alpha_v = contract(alpha, 0, v, 0) # alpha_i v^i\n        \n        # g(v, alpha_sharp) = g_{ij} v^i alpha_sharp^j\n        temp_cov = contract(g, 0, v, 0) # Result is (g_ij v^i) with free index j\n        g_v_alpha_sharp = contract(temp_cov, 0, alpha_sharp, 0)\n        \n        o4 = np.abs(alpha_v.data.item() - g_v_alpha_sharp.data.item())\n        results.append(o4)\n\n        # o5: Norm of difference between two ways of contracting g_inv with alpha\n        # Way 1: contract second index of g_inv -> g^{ij}alpha_j\n        res1 = contract(g_inv, 1, alpha, 0)\n        # Way 2: contract first index of g_inv -> g^{ji}alpha_i\n        res2 = contract(g_inv, 0, alpha, 0)\n        o5 = np.linalg.norm(res1.data - res2.data)\n        results.append(o5)\n\n        # o6: Is contracting v^i with w^i rejected?\n        o6 = False\n        try:\n            contract(v, 0, w, 0)\n        except TypeError:\n            o6 = True\n        results.append(str(o6).lower())\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2980527"}]}