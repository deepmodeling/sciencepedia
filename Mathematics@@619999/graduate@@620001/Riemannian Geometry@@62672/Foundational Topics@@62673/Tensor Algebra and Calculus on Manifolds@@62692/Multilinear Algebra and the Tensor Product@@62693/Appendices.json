{"hands_on_practices": [{"introduction": "A tensor is fundamentally defined by how its components transform under a change of basis. This exercise provides essential practice in navigating these transformations, a cornerstone skill in differential geometry. You will manipulate the component representations of a metric and another covariant tensor to determine the components of a related endomorphism in a new basis, reinforcing the interplay between algebraic operations (like raising an index) and coordinate changes [@problem_id:2984695].", "problem": "Let $(M,g)$ be a two-dimensional Riemannian manifold and fix a point $p \\in M$. Let $T_p M$ have an ordered basis $\\{e_1,e_2\\}$. The Riemannian metric $g$ at $p$ and a symmetric covariant $2$-tensor $h$ at $p$ have components in the basis $\\{e_1,e_2\\}$ given by the matrices\n$$\n[g]_{e} = \\begin{pmatrix} 3 & 1 \\\\[4pt] 1 & 2 \\end{pmatrix},\n\\qquad\n[h]_{e} = \\begin{pmatrix} 2 & 1 \\\\[4pt] 1 & 4 \\end{pmatrix}.\n$$\nConsider the change of basis $\\{\\tilde e_1,\\tilde e_2\\}$ defined by\n$$\n\\tilde e_j = A^{i}{}_{j}\\, e_i,\n\\qquad\nA = \\begin{pmatrix} 2 & 1 \\\\[4pt] 1 & 1 \\end{pmatrix},\n$$\nso that the columns of $A$ are the coordinates of $\\tilde e_j$ in the basis $\\{e_i\\}$.\n\nDefine the $(1,1)$-tensor $H$ at $p$ by the requirement that, for all $X,Y \\in T_p M$,\n$$\ng\\big(HX,\\,Y\\big) = h(X,Y).\n$$\nEquivalently, $H$ is obtained from $h$ by raising the first index with $g$. Let $[H]_{\\tilde e}$ denote the matrix of $H$ in the basis $\\{\\tilde e_1,\\tilde e_2\\}$, so that $H(\\tilde e_j) = H^{i}{}_{j}(\\tilde e)\\, \\tilde e_i$.\n\nStarting only from the multilinear definitions of a Riemannian metric as a symmetric positive-definite bilinear form and the induced isomorphism between tangent and cotangent spaces, together with the definition of tensorial change of basis via the rule $\\tilde e_j = A^{i}{}_{j} e_i$, determine the exact value of the single component\n$$\n\\big[H\\big]_{\\tilde e}^{\\,1}{}_{\\,1}.\n$$\nProvide your answer as an exact number. No rounding is required and no units are involved.", "solution": "## Solution\n\nThe objective is to compute the component $[H]_{\\tilde e}^{\\,1}{}_{\\,1}$ of the $(1,1)$-tensor $H$ in the basis $\\{\\tilde e_1, \\tilde e_2\\}$. We are given the definition of $H$ as the endomorphism on $T_pM$ that satisfies $g(HX, Y) = h(X, Y)$ for all $X, Y \\in T_p M$.\n\nFirst, we determine the matrix representation of $H$ in the original basis $\\{e_1, e_2\\}$, which we denote by $[H]_e$. The action of $H$ on the basis vectors is $H(e_j) = ([H]_e)^i{}_j e_i$.\nSubstituting $X=e_j$ and $Y=e_k$ into the defining relation for $H$:\n$$\ng(H(e_j), e_k) = h(e_j, e_k)\n$$\nThe left-hand side can be expanded using the component representations:\n$$\ng(H(e_j), e_k) = g\\big(([H]_e)^l{}_j e_l, e_k\\big) = ([H]_e)^l{}_j g(e_l, e_k) = ([H]_e)^l{}_j g_{lk} = \\sum_l g_{kl} ([H]_e)^l{}_j\n$$\nThe right-hand side is simply the component $h_{jk}$ of the tensor $h$. Thus, we have the component-wise relation:\n$$\n\\sum_l g_{kl} ([H]_e)^l{}_j = h_{kj}\n$$\nIn matrix notation, where $[g]_e$ has entries $g_{ij}$, $[h]_e$ has entries $h_{ij}$, and $[H]_e$ has entries $([H]_e)^i{}_j$, this equation reads:\n$$\n[g]_e [H]_e = [h]_e\n$$\nSince $g$ is a Riemannian metric, its matrix $[g]_e$ is invertible. We can therefore solve for $[H]_e$:\n$$\n[H]_e = [g]_e^{-1} [h]_e\n$$\nThe given matrices are $[g]_e = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and $[h]_e = \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix}$.\nFirst, we compute the inverse of $[g]_e$:\n$$\n\\det([g]_e) = (3)(2) - (1)(1) = 5\n$$\n$$\n[g]_e^{-1} = \\frac{1}{5} \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}\n$$\nNow, we can compute $[H]_e$:\n$$\n[H]_e = \\frac{1}{5} \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 4 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} (2)(2)+(-1)(1) & (2)(1)+(-1)(4) \\\\ (-1)(2)+(3)(1) & (-1)(1)+(3)(4) \\end{pmatrix}\n$$\n$$\n[H]_e = \\frac{1}{5} \\begin{pmatrix} 3 & -2 \\\\ 1 & 11 \\end{pmatrix}\n$$\nNext, we need to find the matrix of $H$ in the new basis $\\{\\tilde e_j\\}$, which we denote as $[H]_{\\tilde e}$. The basis transformation is given by $\\tilde e_j = A^i{}_j e_i$, with $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}$. For a $(1,1)$-tensor (an endomorphism), the matrix representation transforms according to the similarity transformation:\n$$\n[H]_{\\tilde e} = A^{-1} [H]_e A\n$$\nWe need to compute the inverse of $A$:\n$$\n\\det(A) = (2)(1) - (1)(1) = 1\n$$\n$$\nA^{-1} = \\frac{1}{1} \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\nNow, we perform the matrix multiplication to find $[H]_{\\tilde e}$:\n$$\n[H]_{\\tilde e} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix} \\left( \\frac{1}{5} \\begin{pmatrix} 3 & -2 \\\\ 1 & 11 \\end{pmatrix} \\right) \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\nLet's first multiply the last two matrices:\n$$\n\\left( \\frac{1}{5} \\begin{pmatrix} 3 & -2 \\\\ 1 & 11 \\end{pmatrix} \\right) \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} (3)(2)+(-2)(1) & (3)(1)+(-2)(1) \\\\ (1)(2)+(11)(1) & (1)(1)+(11)(1) \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 & 1 \\\\ 13 & 12 \\end{pmatrix}\n$$\nNow, we complete the multiplication:\n$$\n[H]_{\\tilde e} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix} \\left( \\frac{1}{5} \\begin{pmatrix} 4 & 1 \\\\ 13 & 12 \\end{pmatrix} \\right) = \\frac{1}{5} \\begin{pmatrix} (1)(4)+(-1)(13) & (1)(1)+(-1)(12) \\\\ (-1)(4)+(2)(13) & (-1)(1)+(2)(12) \\end{pmatrix}\n$$\n$$\n[H]_{\\tilde e} = \\frac{1}{5} \\begin{pmatrix} 4-13 & 1-12 \\\\ -4+26 & -1+24 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} -9 & -11 \\\\ 22 & 23 \\end{pmatrix} = \\begin{pmatrix} -\\frac{9}{5} & -\\frac{11}{5} \\\\ \\frac{22}{5} & \\frac{23}{5} \\end{pmatrix}\n$$\nThe component $[H]_{\\tilde e}^{\\,1}{}_{\\,1}$ is the entry in the first row and first column of the matrix $[H]_{\\tilde e}$.\n$$\n[H]_{\\tilde e}^{\\,1}{}_{\\,1} = -\\frac{9}{5}\n$$", "answer": "$$\\boxed{-\\frac{9}{5}}$$", "id": "2984695"}, {"introduction": "The Riemannian metric does more than measure distances; it induces a rich algebraic structure on the space of tensors. This practice explores a fundamental consequence: the ability to decompose tensors into geometrically meaningful parts. You will perform the canonical decomposition of a symmetric $(0,2)$-tensor into its trace and trace-free components, a procedure essential in general relativity and geometric analysis [@problem_id:2984655]. This exercise highlights how the metric provides a natural inner product and allows for the orthogonal projection of tensors.", "problem": "Let $(M,g)$ be a Riemannian manifold of dimension $n$, and fix a point $p \\in M$. The metric $g$ induces a canonical inner product on the space of symmetric $(0,2)$-tensors $\\mathrm{Sym}^{2} T^{\\ast}_{p}M$ by contraction with the inverse metric. The trace of a $(0,2)$-tensor with respect to $g$ is defined by contracting one copy of $g^{-1}$ against the tensor.\n\nConsider $n=3$. In local coordinates $(x^{1},x^{2},x^{3})$ centered at $p$, suppose the metric and a symmetric $(0,2)$-tensor $T$ have the following component matrices with respect to the coordinate basis at $p$:\n$$\ng_{ij}(p) = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 6\n\\end{pmatrix},\n\\qquad\nT_{ij}(p) = \\begin{pmatrix}\n4 & 1 & 2 \\\\\n1 & 6 & 3 \\\\\n2 & 3 & 5\n\\end{pmatrix}.\n$$\nWorking strictly from the core definitions of contraction, inverse metric, trace with respect to $g$, and the inner product on covariant tensors induced by $g$, do the following:\n\n1. Derive, from first principles, the unique decomposition $T = S + c\\, g$ at $p$ such that $S$ is trace-free with respect to $g$ and $S$ is $g$-orthogonal to $g$ under the canonical inner product induced by $g$ on symmetric $(0,2)$-tensors. Express $c$ in terms of the $g$-trace of $T$ and the dimension $n$.\n\n2. Using the specific matrices above, compute the $g$-trace of $T$, the scalar $c$, and the components of the traceless part $S$ at $p$.\n\n3. Compute the squared norm $\\|S\\|^{2}$ at $p$ with respect to the inner product induced by $g$ on $\\mathrm{Sym}^{2} T^{\\ast}_{p}M$.\n\nExpress your final answer to part $3$ as a single exact algebraic number. No rounding is required.", "solution": "The problem is divided into three parts. We will address them in order.\n\n### Part 1: Derivation of the Decomposition\nLet $T$ be a symmetric $(0,2)$-tensor on a Riemannian manifold $(M,g)$ of dimension $n$. We seek a decomposition of the form $T = S + c\\,g$, where $S$ is a symmetric $(0,2)$-tensor, and $c$ is a scalar function. The conditions imposed on $S$ are that it is trace-free with respect to $g$ and $g$-orthogonal to $g$.\n\nIn component form, the decomposition is $T_{ij} = S_{ij} + c\\,g_{ij}$, which implies $S_{ij} = T_{ij} - c\\,g_{ij}$.\n\nThe first condition is that $S$ is trace-free with respect to $g$. The trace of a $(0,2)$-tensor $A$ with respect to $g$ is defined as $\\mathrm{tr}_g(A) = g^{ij}A_{ij}$, where $g^{ij}$ are the components of the inverse metric tensor $g^{-1}$.\nApplying this condition to $S$:\n$$\n\\mathrm{tr}_g(S) = g^{ij}S_{ij} = 0\n$$\nSubstituting the expression for $S_{ij}$:\n$$\ng^{ij}(T_{ij} - c\\,g_{ij}) = 0\n$$\nUsing the linearity of the contraction operator:\n$$\ng^{ij}T_{ij} - c\\,g^{ij}g_{ij} = 0\n$$\nWe recognize $g^{ij}T_{ij}$ as the trace of $T$ with respect to $g$, denoted $\\mathrm{tr}_g(T)$. The term $g^{ij}g_{ij}$ is the trace of the identity map on the tangent space. By definition, $g^{ij}g_{jk} = \\delta^i_k$, so contracting with $g_{ij}$ (or $g_{ik}$) gives:\n$$\ng^{ij}g_{ij} = \\delta^j_j = \\sum_{j=1}^{n} 1 = n\n$$\nSubstituting these back into the equation for $c$:\n$$\n\\mathrm{tr}_g(T) - c\\,n = 0\n$$\nSolving for $c$ yields the desired expression:\n$$\nc = \\frac{\\mathrm{tr}_g(T)}{n}\n$$\nThis uniquely determines the scalar $c$ and consequently the tensor $S = T - \\frac{\\mathrm{tr}_g(T)}{n}g$.\n\nThe second condition is that $S$ is $g$-orthogonal to $g$. The inner product on the space of symmetric $(0,2)$-tensors $\\mathrm{Sym}^2 T^*_p M$ induced by $g$ is given by $\\langle A, B \\rangle_g = g^{ik}g^{jl}A_{ij}B_{kl}$ for any two tensors $A, B \\in \\mathrm{Sym}^2 T^*_p M$. We need to show that $\\langle S, g \\rangle_g = 0$.\nLet us compute this inner product:\n$$\n\\langle S, g \\rangle_g = g^{ik}g^{jl}S_{ij}g_{kl}\n$$\nWe can first contract the pair of indices $l$ and $k$. Since $g^{jl}g_{kl} = \\delta^j_k$:\n$$\n\\langle S, g \\rangle_g = g^{ik} S_{ij} \\delta^j_k\n$$\nContracting with the Kronecker delta $\\delta^j_k$ replaces the index $j$ with $k$:\n$$\n\\langle S, g \\rangle_g = g^{ik}S_{ik}\n$$\nThis is precisely the definition of the trace of $S$ with respect to $g$, $\\mathrm{tr}_g(S)$.\n$$\n\\langle S, g \\rangle_g = \\mathrm{tr}_g(S)\n$$\nSince we constructed $S$ to be trace-free (i.e., $\\mathrm{tr}_g(S) = 0$), it follows directly that $\\langle S, g \\rangle_g = 0$. Thus, the condition of being trace-free implies the condition of being $g$-orthogonal to $g$. The decomposition is therefore unique and consistent with both stated properties.\n\n### Part 2: Computation of Traces and Components\nWe are given the metric and tensor components at point $p$ in a specific coordinate system where $n=3$:\n$$\ng_{ij}(p) = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 6\n\\end{pmatrix},\n\\qquad\nT_{ij}(p) = \\begin{pmatrix}\n4 & 1 & 2 \\\\\n1 & 6 & 3 \\\\\n2 & 3 & 5\n\\end{pmatrix}\n$$\nFirst, we find the components of the inverse metric $g^{ij}$. Since $g_{ij}$ is a diagonal matrix, $g^{ij}$ is also a diagonal matrix whose diagonal entries are the reciprocals of the diagonal entries of $g_{ij}$:\n$$\ng^{ij}(p) = \\begin{pmatrix}\n1/2 & 0 & 0 \\\\\n0 & 1/3 & 0 \\\\\n0 & 0 & 1/6\n\\end{pmatrix}\n$$\nNext, we compute the $g$-trace of $T$, $\\mathrm{tr}_g(T) = g^{ij}T_{ij}$. As $g^{ij}$ is diagonal, the sum simplifies:\n$$\n\\mathrm{tr}_g(T) = g^{11}T_{11} + g^{22}T_{22} + g^{33}T_{33} = \\frac{1}{2}(4) + \\frac{1}{3}(6) + \\frac{1}{6}(5) = 2 + 2 + \\frac{5}{6} = 4 + \\frac{5}{6} = \\frac{29}{6}\n$$\nNow, we compute the scalar $c$ using the formula derived in Part 1, with $n=3$:\n$$\nc = \\frac{\\mathrm{tr}_g(T)}{n} = \\frac{29/6}{3} = \\frac{29}{18}\n$$\nFinally, we compute the components of the traceless part $S$ using $S_{ij} = T_{ij} - c\\,g_{ij}$:\n$$\nS_{ij} = T_{ij} - \\frac{29}{18}g_{ij}\n$$\nThe diagonal components are:\n$$\nS_{11} = T_{11} - \\frac{29}{18}g_{11} = 4 - \\frac{29}{18}(2) = 4 - \\frac{29}{9} = \\frac{36-29}{9} = \\frac{7}{9}\n$$\n$$\nS_{22} = T_{22} - \\frac{29}{18}g_{22} = 6 - \\frac{29}{18}(3) = 6 - \\frac{29}{6} = \\frac{36-29}{6} = \\frac{7}{6}\n$$\n$$\nS_{33} = T_{33} - \\frac{29}{18}g_{33} = 5 - \\frac{29}{18}(6) = 5 - \\frac{29}{3} = \\frac{15-29}{3} = -\\frac{14}{3}\n$$\nThe off-diagonal components of $g_{ij}$ are zero, so the off-diagonal components of $S_{ij}$ are the same as those of $T_{ij}$:\n$$\nS_{12} = T_{12} = 1, \\quad S_{13} = T_{13} = 2, \\quad S_{23} = T_{23} = 3\n$$\nDue to symmetry, $S_{21}=1$, $S_{31}=2$, and $S_{32}=3$. The component matrix for $S$ is:\n$$\nS_{ij}(p) = \\begin{pmatrix}\n7/9 & 1 & 2 \\\\\n1 & 7/6 & 3 \\\\\n2 & 3 & -14/3\n\\end{pmatrix}\n$$\n\n### Part 3: Computation of the Squared Norm of S\nThe squared norm of $S$ is given by the inner product of $S$ with itself:\n$$\n\\|S\\|^2 = \\langle S, S \\rangle_g = g^{ik}g^{jl}S_{ij}S_{kl}\n$$\nSince $g^{ij}$ is diagonal, i.e., $g^{ab}=0$ for $a \\neq b$, the only non-zero terms in the sum occur when $i=k$ and $j=l$. The expression simplifies to:\n$$\n\\|S\\|^2 = \\sum_{i=1}^3 \\sum_{j=1}^3 g^{ii}g^{jj}S_{ij}S_{kl}\\delta_i^k\\delta_j^l = \\sum_{i=1}^3 \\sum_{j=1}^3 g^{ii}g^{jj}(S_{ij})^2\n$$\nWe can split this sum into contributions from the diagonal components ($i=j$) and the off-diagonal components ($i \\neq j$).\n$$\n\\|S\\|^2 = \\sum_{i=1}^3 (g^{ii})^2(S_{ii})^2 + \\sum_{i \\neq j} g^{ii}g^{jj}(S_{ij})^2\n$$\nSince $S_{ij} = S_{ji}$, the off-diagonal sum can be written as $2 \\sum_{1 \\le i < j \\le 3} g^{ii}g^{jj}(S_{ij})^2$.\n\nFirst, the diagonal contribution:\n$$\n\\sum_{i=1}^3 (g^{ii})^2(S_{ii})^2 = (g^{11})^2(S_{11})^2 + (g^{22})^2(S_{22})^2 + (g^{33})^2(S_{33})^2\n$$\n$$\n= \\left(\\frac{1}{2}\\right)^2\\left(\\frac{7}{9}\\right)^2 + \\left(\\frac{1}{3}\\right)^2\\left(\\frac{7}{6}\\right)^2 + \\left(\\frac{1}{6}\\right)^2\\left(-\\frac{14}{3}\\right)^2\n$$\n$$\n= \\frac{1}{4}\\frac{49}{81} + \\frac{1}{9}\\frac{49}{36} + \\frac{1}{36}\\frac{196}{9} = \\frac{49}{324} + \\frac{49}{324} + \\frac{196}{324} = \\frac{49+49+196}{324} = \\frac{294}{324}\n$$\nSimplifying the fraction: $\\frac{294}{324} = \\frac{147}{162} = \\frac{49}{54}$.\n\nNext, the off-diagonal contribution:\n$$\n2 \\sum_{1 \\le i < j \\le 3} g^{ii}g^{jj}(S_{ij})^2 = 2\\left( g^{11}g^{22}(S_{12})^2 + g^{11}g^{33}(S_{13})^2 + g^{22}g^{33}(S_{23})^2 \\right)\n$$\n$$\n= 2\\left( \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{3}\\right)(1)^2 + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{6}\\right)(2)^2 + \\left(\\frac{1}{3}\\right)\\left(\\frac{1}{6}\\right)(3)^2 \\right)\n$$\n$$\n= 2\\left( \\frac{1}{6} + \\frac{4}{12} + \\frac{9}{18} \\right) = 2\\left( \\frac{1}{6} + \\frac{1}{3} + \\frac{1}{2} \\right)\n$$\n$$\n= 2\\left( \\frac{1+2+3}{6} \\right) = 2\\left( \\frac{6}{6} \\right) = 2\n$$\nThe total squared norm is the sum of these two contributions:\n$$\n\\|S\\|^2 = \\frac{49}{54} + 2 = \\frac{49 + 108}{54} = \\frac{157}{54}\n$$\nThe number $157$ is prime, so this fraction is in its simplest form.", "answer": "$$\n\\boxed{\\frac{157}{54}}\n$$", "id": "2984655"}, {"introduction": "Beyond component manipulations, multilinear algebra seeks to uncover the intrinsic, basis-independent properties of tensors. The concept of tensor rank addresses the minimal number of simple \"decomposable\" tensors needed to construct a more complex one, offering a measure of its structural complexity. In this advanced problem, you will determine the rank of a canonical tensor by relating it to the rank of an associated linear map, a powerful technique known as \"flattening\" that provides deep insight into a tensor's structure [@problem_id:2984666].", "problem": "Let $\\left(M,g\\right)$ be a smooth Riemannian manifold of dimension $n \\in \\mathbb{N}$, and fix a point $p \\in M$. Let $V := T_{p}M$ denote the tangent space at $p$, and let $g_{p}$ denote the inner product induced by $g$ on $V$. Choose a $g_{p}$-orthonormal basis $\\left\\{e_{1},\\dots,e_{n}\\right\\}$ of $V$ with dual coframe $\\left\\{e^{1},\\dots,e^{n}\\right\\} \\subset V^{*}$, where $V^{*}$ is the dual space of $V$. Consider the order-$3$ tensor $T \\in V^{*} \\otimes V^{*} \\otimes V$ defined by\n$$\nT \\;=\\; \\sum_{i=1}^{n} e^{i} \\otimes e^{i} \\otimes e_{i}.\n$$\nA decomposable tensor in $V^{*} \\otimes V^{*} \\otimes V$ is one of the form $a \\otimes b \\otimes c$ with $a,b \\in V^{*}$ and $c \\in V$. The (tensor) rank of $T$, denoted $\\operatorname{rank}_{\\otimes}(T)$, is the smallest integer $r \\in \\mathbb{N}$ for which there exist $\\left\\{a_{k}\\right\\}_{k=1}^{r} \\subset V^{*}$, $\\left\\{b_{k}\\right\\}_{k=1}^{r} \\subset V^{*}$, and $\\left\\{c_{k}\\right\\}_{k=1}^{r} \\subset V$ such that\n$$\nT \\;=\\; \\sum_{k=1}^{r} a_{k} \\otimes b_{k} \\otimes c_{k}.\n$$\nUsing only the foundational definitions of the tensor product, dual spaces, and linear maps, determine $\\operatorname{rank}_{\\otimes}(T)$ explicitly as a function of $n$. Express your final answer as a single closed-form expression in terms of $n$.", "solution": "The problem asks for the tensor rank of the tensor $T \\in V^{*} \\otimes V^{*} \\otimes V$ defined at a point $p \\in M$ on the tangent space $V=T_pM$. The tangent space $V$ is an $n$-dimensional real vector space. Let $\\left\\{e_{1},\\dots,e_{n}\\right\\}$ be a basis for $V$ and $\\left\\{e^{1},\\dots,e^{n}\\right\\}$ be its dual basis for $V^{*}$, satisfying $e^{i}(e_{j}) = \\delta^{i}_{j}$, where $\\delta^{i}_{j}$ is the Kronecker delta. Note that for this problem, the fact that the basis is $g_p$-orthonormal is not strictly necessary; the result holds for any basis. The tensor $T$ is given by:\n$$\nT = \\sum_{i=1}^{n} e^{i} \\otimes e^{i} \\otimes e_{i}\n$$\nThe tensor rank, $\\operatorname{rank}_{\\otimes}(T)$, is the smallest integer $r$ such that $T$ can be expressed as a sum of $r$ decomposable tensors:\n$$\nT = \\sum_{k=1}^{r} a_{k} \\otimes b_{k} \\otimes c_{k}\n$$\nwhere $a_k, b_k \\in V^{*}$ and $c_k \\in V$.\n\nThe derivation of $\\operatorname{rank}_{\\otimes}(T)$ will proceed in two parts: first establishing an upper bound, and second, establishing a lower bound.\n\n**Part 1: Upper Bound for $\\operatorname{rank}_{\\otimes}(T)$**\n\nThe definition of $T$ is given as a sum of $n$ terms:\n$$\nT = (e^{1} \\otimes e^{1} \\otimes e_{1}) + (e^{2} \\otimes e^{2} \\otimes e_{2}) + \\dots + (e^{n} \\otimes e^{n} \\otimes e_{n})\n$$\nEach term in this sum, $e^{i} \\otimes e^{i} \\otimes e_{i}$, is a decomposable tensor of the form $a \\otimes b \\otimes c$, with $a = e^{i}$, $b = e^{i}$, and $c = e_{i}$.\nAccording to the definition of tensor rank, the rank is the smallest number of such decomposable tensors required to represent $T$. Since we have an explicit representation of $T$ as a sum of $n$ decomposable tensors, the rank can be at most $n$.\nTherefore, we have the upper bound:\n$$\n\\operatorname{rank}_{\\otimes}(T) \\le n\n$$\n\n**Part 2: Lower Bound for $\\operatorname{rank}_{\\otimes}(T)$**\n\nTo establish a lower bound, we will employ the technique of \"flattening\" or \"matricization\". A tensor in $V^{*} \\otimes V^{*} \\otimes V$ can be interpreted as a bilinear map from $V \\times V$ to $V$. Let's define this map explicitly. For any pair of vectors $(u,v) \\in V \\times V$, the action of $T$ is given by:\n$$\nT(u,v) = \\sum_{i=1}^{n} e^{i}(u) e^{i}(v) e_{i}\n$$\nLet's analyze this map. Let $u = \\sum_{j=1}^{n} u^{j}e_{j}$ and $v = \\sum_{k=1}^{n} v^{k}e_{k}$ be the expressions for $u$ and $v$ in the basis $\\left\\{e_i\\right\\}$. The components are given by $u^j = e^j(u)$ and $v^k = e^k(v)$. Substituting these into the expression for $T(u,v)$:\n$$\nT(u,v) = \\sum_{i=1}^{n} u^{i} v^{i} e_{i}\n$$\nThis expression shows that $T(u,v)$ computed in the basis $\\left\\{e_i\\right\\}$ corresponds to the element-wise (or Hadamard) product of the coordinate vectors of $u$ and $v$.\n\nNow, consider the linear map $\\Phi: V \\to L(V,V)$, where $L(V,V)$ is the space of linear operators from $V$ to $V$, obtained by fixing the first argument of $T$. This map is defined by $\\Phi(u) = T(u, \\cdot)$, which means $\\Phi(u)$ is the linear map that sends $v \\in V$ to $T(u,v)$:\n$$\n\\Phi(u)(v) = T(u,v) = \\sum_{i=1}^{n} u^{i} v^{i} e_{i}\n$$\nThe operator $\\Phi(u)$ maps a vector $v = \\sum_{j=1}^{n} v^{j}e_{j}$ to the vector $\\sum_{i=1}^{n} u^{i} v^{i} e_{i}$. In the basis $\\left\\{e_1, \\dots, e_n\\right\\}$, the matrix representation of the operator $\\Phi(u)$ is a diagonal matrix with the components of $u$ on the diagonal:\n$$\n[\\Phi(u)] = \\begin{pmatrix} u^1 & 0 & \\cdots & 0 \\\\ 0 & u^2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & u^n \\end{pmatrix}\n$$\nThe image of the map $\\Phi$, denoted $\\operatorname{Im}(\\Phi)$, is the set of all such linear operators as $u$ varies over all of $V$. As the coordinate vector $(u^1, \\dots, u^n)$ can be any vector in $\\mathbb{R}^n$, the image $\\operatorname{Im}(\\Phi)$ is the space of all linear operators on $V$ that are diagonal in the basis $\\left\\{e_i\\right\\}$. This space has a basis consisting of the $n$ operators whose matrices are $E_{11}, E_{22}, \\dots, E_{nn}$ (where $E_{ii}$ has a $1$ in the $(i,i)$ position and zeros elsewhere). Thus, the dimension of this space is $n$.\nThe rank of the linear map $\\Phi$ is the dimension of its image:\n$$\n\\operatorname{rank}(\\Phi) = \\dim(\\operatorname{Im}(\\Phi)) = n\n$$\n\nNow, we relate this to the tensor rank of $T$. Suppose $T$ has rank $r$, so it can be written as:\n$$\nT = \\sum_{k=1}^{r} a_{k} \\otimes b_{k} \\otimes c_{k}\n$$\nApplying our interpretation, the action on $(u,v)$ is:\n$$\nT(u,v) = \\sum_{k=1}^{r} a_{k}(u) b_{k}(v) c_{k}\n$$\nThe corresponding linear map $\\Phi(u) = T(u,\\cdot)$ is:\n$$\n\\Phi(u)(v) = \\sum_{k=1}^{r} a_{k}(u) b_{k}(v) c_{k}\n$$\nWe can write this as a linear combination of operators:\n$$\n\\Phi(u) = \\sum_{k=1}^{r} a_{k}(u) (v \\mapsto b_{k}(v) c_{k})\n$$\nLet $M_k: V \\to V$ be the linear operator defined by $M_k(v) = b_k(v) c_k$. Each $M_k$ is a rank-1 operator (unless $b_k$ or $c_k$ is zero). The expression for $\\Phi(u)$ shows that for any $u \\in V$, the operator $\\Phi(u)$ is a linear combination of the $r$ fixed operators $M_1, \\dots, M_r$.\nThis implies that the image of $\\Phi$ is a subspace of the vector space spanned by these $r$ operators:\n$$\n\\operatorname{Im}(\\Phi) \\subseteq \\operatorname{span}\\left\\{M_1, \\dots, M_r\\right\\}\n$$\nThe dimension of a subspace is less than or equal to the dimension of the ambient space. Therefore,\n$$\n\\dim(\\operatorname{Im}(\\Phi)) \\le \\dim(\\operatorname{span}\\left\\{M_1, \\dots, M_r\\right\\}) \\le r\n$$\nWe have already calculated that $\\dim(\\operatorname{Im}(\\Phi)) = n$. And by definition, $r = \\operatorname{rank}_{\\otimes}(T)$.\nSubstituting these into the inequality gives:\n$$\nn \\le \\operatorname{rank}_{\\otimes}(T)\n$$\nThis establishes the required lower bound.\n\n**Conclusion**\n\nWe have shown that $\\operatorname{rank}_{\\otimes}(T) \\le n$ and $n \\le \\operatorname{rank}_{\\otimes}(T)$. The only way to satisfy both inequalities simultaneously is for them to be an equality.\nTherefore, the tensor rank of $T$ is precisely $n$.\n$$\n\\operatorname{rank}_{\\otimes}(T) = n\n$$", "answer": "$$\\boxed{n}$$", "id": "2984666"}]}