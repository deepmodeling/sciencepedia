{"hands_on_practices": [{"introduction": "This exercise serves as a crucial bridge between abstract definitions and concrete computations. By starting from the first principles of derivations, you will construct the familiar tangent and cotangent spaces of Euclidean space $\\mathbb{R}^n$, demonstrating how tangent vectors and covectors are rigorously identified with column and row vectors, respectively. This foundational practice solidifies the core concepts and demystifies the relationship between the algebraic structure of derivations and standard linear algebra. [@problem_id:2994006]", "problem": "Let $M = \\mathbb{R}^{n}$ with its standard smooth structure and global coordinates $x^{1}, \\dots, x^{n}$. Fix a point $p \\in \\mathbb{R}^{n}$. Using only the definition of the tangent space $T_{p}M$ as derivations at $p$ and the definition of the cotangent space $T_{p}^{*}M$ as the dual vector space of $T_{p}M$, answer the following in a logically complete derivation:\n\n- Construct the canonical basis $\\left\\{\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right\\}_{i=1}^{n}$ of $T_{p}M$ and the dual basis $\\{dx^{i}|_{p}\\}_{i=1}^{n}$ of $T_{p}^{*}M$ from first principles, and justify the duality relation $dx^{i}|_{p}\\!\\left(\\left.\\frac{\\partial}{\\partial x^{j}}\\right|_{p}\\right) = \\delta^{i}{}_{j}$.\n\n- Using only the above definitions, identify $T_{p}M$ with column vectors in $\\mathbb{R}^{n}$ via the basis $\\left\\{\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right\\}$, and identify $T_{p}^{*}M$ with row vectors in $\\mathbb{R}^{n}$ via the basis $\\{dx^{i}|_{p}\\}$. Verify that the natural pairing $T_{p}^{*}M \\times T_{p}M \\to \\mathbb{R}$ is given by row-by-column matrix multiplication.\n\nFinally, let $\\alpha \\in T_{p}^{*}M$ be specified by its values on the basis vectors $\\alpha\\!\\left(\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right) = a_{i}$ for $i \\in \\{1,\\dots,n\\}$, and let $v \\in T_{p}M$ be given by $v = \\sum_{i=1}^{n} v^{i}\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}$. Compute the single analytic expression for $\\alpha(v)$ in terms of the symbols $a_{i}$ and $v^{i}$ (no numerical evaluation is required). Express your final answer as a single closed-form analytic expression.", "solution": "The problem as stated is valid, being well-posed, objective, and scientifically grounded in the standard framework of differential geometry. We shall proceed with the derivation from the specified first principles.\n\nLet $M = \\mathbb{R}^{n}$ be the smooth manifold with its standard smooth structure and global coordinate chart $(M, \\text{id})$, where the coordinate functions are $x^{1}, \\dots, x^{n}$. Let $p \\in M$ be a fixed point.\n\n**Part 1: Construction of the Tangent Space Basis and Dual Basis**\n\nThe tangent space $T_{p}M$ is defined as the vector space of all derivations at $p$. A derivation at $p$ is a linear map $v: C^{\\infty}(M) \\to \\mathbb{R}$ satisfying the product rule (Leibniz rule):\n$$v(fg) = f(p)v(g) + g(p)v(f) \\quad \\forall f, g \\in C^{\\infty}(M)$$\nwhere $C^{\\infty}(M)$ is the algebra of smooth real-valued functions on $M$.\n\nFor each coordinate $x^{i}$, we define an operator $\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}$ by its action on any smooth function $f \\in C^{\\infty}(M)$:\n$$\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}(f) := \\frac{\\partial f}{\\partial x^{i}}(p)$$\nThis operator is a derivation because it is linear and satisfies the Leibniz rule, inheriting these properties directly from the standard partial derivative.\n\nTo show that the set $\\left\\{\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right\\}_{i=1}^{n}$ forms a basis for $T_{p}M$, we must prove linear independence and that it spans the space.\n- **Linear Independence**: Assume a linear combination is the zero derivation: $\\sum_{i=1}^{n} c^{i} \\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p} = 0$ for some coefficients $c^{i} \\in \\mathbb{R}$. Applying this derivation to the coordinate function $x^{j}$ for any $j \\in \\{1,\\dots,n\\}$ gives:\n$$0 = \\left(\\sum_{i=1}^{n} c^{i} \\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right)(x^{j}) = \\sum_{i=1}^{n} c^{i} \\frac{\\partial x^{j}}{\\partial x^{i}}(p) = \\sum_{i=1}^{n} c^{i} \\delta^{j}{}_{i} = c^{j}$$\nThus, $c^{j}=0$ for all $j$, proving linear independence.\n- **Spanning**: Let $v \\in T_{p}M$ be an arbitrary derivation. For any $f \\in C^{\\infty}(M)$, Taylor's theorem at $p$ gives $f(x) = f(p) + \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x^{i}}(p)(x^{i} - p^{i}) + O((x-p)^{2})$. Any derivation $v$ annihilates constants, so $v(f(p))=0$. Applying $v$ to the expansion and using the Leibniz rule shows that $v$ annihilates terms of order $2$ and higher in $(x-p)$. Thus, $v(f) = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial x^{i}}(p) v(x^{i})$. If we define the coefficients $v^{i} := v(x^{i})$, we can write $v(f) = \\sum_{i=1}^{n} v^{i} \\frac{\\partial f}{\\partial x^{i}}(p) = \\left(\\sum_{i=1}^{n} v^{i} \\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right)(f)$. Since this holds for all $f$, we have $v = \\sum_{i=1}^{n} v^{i} \\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}$, which proves the set spans $T_{p}M$.\n\nThe cotangent space $T_{p}^{*}M$ is the dual vector space to $T_{p}M$. For any smooth function $f \\in C^{\\infty}(M)$, its differential at $p$, denoted $df|_{p}$, is an element of $T_{p}^{*}M$ defined by its action on any $v \\in T_{p}M$:\n$$df|_{p}(v) = v(f)$$\nWe construct the dual basis $\\{dx^{i}|_{p}\\}_{i=1}^{n}$ by taking the differentials of the coordinate functions $x^{i}$. To verify the duality relation, we apply $dx^{i}|_{p}$ to a basis vector $\\left.\\frac{\\partial}{\\partial x^{j}}\\right|_{p}$:\n$$dx^{i}|_{p}\\left(\\left.\\frac{\\partial}{\\partial x^{j}}\\right|_{p}\\right) = \\left.\\frac{\\partial}{\\partial x^{j}}\\right|_{p}(x^{i}) = \\frac{\\partial x^{i}}{\\partial x^{j}}(p) = \\delta^{i}{}_{j}$$\nThis equation is the defining property of a dual basis, thus confirming that $\\{dx^{i}|_{p}\\}$ is the basis of $T_{p}^{*}M$ dual to $\\left\\{\\left.\\frac{\\partial}{\\partial x^{j}}\\right|_{p}\\right\\}$.\n\n**Part 2: Representation and Pairing**\n\nA tangent vector $v \\in T_{p}M$ is written in the basis as $v = \\sum_{i=1}^{n} v^{i} \\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}$. This establishes an isomorphism with the space of column vectors in $\\mathbb{R}^{n}$:\n$$v \\longleftrightarrow [v] = \\begin{pmatrix} v^{1} \\\\ \\vdots \\\\ v^{n} \\end{pmatrix}$$\nA cotangent vector (or covector) $\\alpha \\in T_{p}^{*}M$ is written in the dual basis as $\\alpha = \\sum_{j=1}^{n} a_{j} dx^{j}|_{p}$. The components $a_j$ are found by evaluating $\\alpha$ on the basis vectors: $a_{j} = \\alpha\\left(\\left.\\frac{\\partial}{\\partial x^{j}}\\right|_{p}\\right)$. This establishes an isomorphism with the space of row vectors in $\\mathbb{R}^{n}$:\n$$\\alpha \\longleftrightarrow [\\alpha] = \\begin{pmatrix} a_{1} & \\dots & a_{n} \\end{pmatrix}$$\nThe natural pairing $\\alpha(v)$ is a linear map from $T_{p}^{*}M \\times T_{p}M \\to \\mathbb{R}$. We compute its value using the basis expansions:\n$$\\alpha(v) = \\left(\\sum_{j=1}^{n} a_{j} dx^{j}|_{p}\\right) \\left(\\sum_{i=1}^{n} v^{i} \\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right) = \\sum_{i,j=1}^{n} a_{j}v^{i} \\left( dx^{j}|_{p}\\left(\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right) \\right)$$\nUsing the duality relation $dx^{j}|_{p}\\left(\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right) = \\delta^{j}{}_{i}$:\n$$\\alpha(v) = \\sum_{i,j=1}^{n} a_{j}v^{i} \\delta^{j}{}_{i} = \\sum_{i=1}^{n} a_{i}v^{i}$$\nThis scalar result is precisely the result of row-by-column matrix multiplication:\n$$[\\alpha][v] = \\begin{pmatrix} a_{1} & \\dots & a_{n} \\end{pmatrix} \\begin{pmatrix} v^{1} \\\\ \\vdots \\\\ v^{n} \\end{pmatrix} = \\sum_{i=1}^{n} a_{i}v^{i}$$\nThis verifies the correspondence.\n\n**Part 3: Final Computation**\n\nWe are given a covector $\\alpha \\in T_{p}^{*}M$ defined by its action on the basis vectors, $\\alpha\\left(\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right) = a_{i}$, and a vector $v \\in T_{p}M$ given by its basis expansion $v = \\sum_{i=1}^{n} v^{i}\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}$.\nWe compute the value of $\\alpha(v)$ using the linearity of $\\alpha$:\n$$\\alpha(v) = \\alpha\\left(\\sum_{i=1}^{n} v^{i}\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right)$$\n$$\\alpha(v) = \\sum_{i=1}^{n} v^{i} \\alpha\\left(\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}\\right)$$\nSubstituting the given definition for the components $a_{i}$:\n$$\\alpha(v) = \\sum_{i=1}^{n} v^{i} a_{i} = \\sum_{i=1}^{n} a_{i}v^{i}$$\nThis is the final analytical expression.", "answer": "$$\\boxed{\\sum_{i=1}^{n} a_{i}v^{i}}$$", "id": "2994006"}, {"introduction": "A key skill in differential geometry is the ability to work seamlessly in different coordinate systems. This problem challenges you to translate between Cartesian and polar coordinates, not just for points, but for the covector fields themselves. You will apply the chain rule to find the representation of an orthonormal coframe in polar coordinates and then express a given differential one-form in this new basis, reinforcing the concept of a covector as a coordinate-independent geometric object. [@problem_id:2994052]", "problem": "Let $M = \\mathbb{R}^{2} \\setminus \\{(0,0)\\}$ be equipped with the standard Euclidean Riemannian metric $g = dx^{2} + dy^{2}$. Consider polar coordinates $(r,\\theta)$ on $M$ defined by the smooth map $\\Phi(r,\\theta) = (r\\cos\\theta, r\\sin\\theta)$, with $\\theta$ measured in radians. The cotangent space $T^{\\ast}_{p}M$ at a point $p \\in M$ is the vector space of covectors at $p$. The differentials $dr$ and $d\\theta$ at $p$ are the covectors obtained by differentiating the polar coordinate functions.\n\nFix a point $p \\in M$ with Cartesian coordinates $(x_{0},y_{0})$ and define $r_{0} = \\sqrt{x_{0}^{2} + y_{0}^{2}}$ (so $r_{0} \\neq 0$). Consider the smooth function $f : M \\to \\mathbb{R}$ given by\n$$\nf(x,y) = x^{2} y + \\sin(x y) + \\ln(x^{2} + y^{2}),\n$$\nand let $\\omega = df$ be its differential. The Euclidean Riemannian metric makes the polar coframe $\\{dr, r\\,d\\theta\\}$ an orthonormal coframe on $M$.\n\nUsing only the chain rule for differentials, the definition of the cotangent space as the dual of the tangent space, and the duality between frames and coframes induced by coordinate changes:\n\n1) Derive explicit expressions for $dr\\vert_{p}$ and $(r\\,d\\theta)\\vert_{p}$ as linear combinations of the Cartesian coframe $\\{dx\\vert_{p}, dy\\vert_{p}\\}$.\n\n2) Express the covector $\\omega_{p}$ in the coframe $\\{dr\\vert_{p}, (r\\,d\\theta)\\vert_{p}\\}$, i.e., find scalars $c_{1}, c_{2} \\in \\mathbb{R}$ such that\n$$\n\\omega_{p} = c_{1}\\,dr\\vert_{p} + c_{2}\\,(r\\,d\\theta)\\vert_{p}.\n$$\n\nProvide your final answer as the row matrix $\\begin{pmatrix} c_{1} & c_{2} \\end{pmatrix}$ in terms of $x_{0}$ and $y_{0}$. No numerical approximation is required.", "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem within the field of differential geometry. All provided information is consistent, and the objectives are clearly defined.\n\nThe solution proceeds in two parts as requested.\n\n**Part 1: Derive explicit expressions for $dr\\vert_{p}$ and $(r\\,d\\theta)\\vert_{p}$ as linear combinations of the Cartesian coframe $\\{dx\\vert_{p}, dy\\vert_{p}\\}$.**\n\nThe relationship between Cartesian coordinates $(x,y)$ and polar coordinates $(r,\\theta)$ is given by $x = r\\cos\\theta$ and $y = r\\sin\\theta$. To find the transformation for the covectors (differentials), we apply the chain rule by taking the exterior derivative of these relations:\n$$dx = \\frac{\\partial x}{\\partial r}dr + \\frac{\\partial x}{\\partial \\theta}d\\theta = (\\cos\\theta)dr - (r\\sin\\theta)d\\theta$$\n$$dy = \\frac{\\partial y}{\\partial r}dr + \\frac{\\partial y}{\\partial \\theta}d\\theta = (\\sin\\theta)dr + (r\\cos\\theta)d\\theta$$\nThis can be written as a matrix equation relating the covector bases $\\{dx, dy\\}$ and $\\{dr, d\\theta\\}$:\n$$\n\\begin{pmatrix} dx \\\\ dy \\end{pmatrix}\n=\n\\begin{pmatrix} \\cos\\theta & -r\\sin\\theta \\\\ \\sin\\theta & r\\cos\\theta \\end{pmatrix}\n\\begin{pmatrix} dr \\\\ d\\theta \\end{pmatrix}\n$$\nTo express $\\{dr, d\\theta\\}$ in terms of $\\{dx, dy\\}$, we must invert the Jacobian matrix. The determinant of the matrix is $r\\cos^2\\theta - (-r\\sin^2\\theta) = r(\\cos^2\\theta+\\sin^2\\theta) = r$. Since the manifold is $M = \\mathbb{R}^2 \\setminus \\{(0,0)\\}$, we have $r \\ne 0$, so the matrix is invertible. The inverse matrix is:\n$$\n\\begin{pmatrix} \\cos\\theta & -r\\sin\\theta \\\\ \\sin\\theta & r\\cos\\theta \\end{pmatrix}^{-1}\n=\n\\frac{1}{r}\n\\begin{pmatrix} r\\cos\\theta & r\\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{pmatrix}\n=\n\\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\frac{\\sin\\theta}{r} & \\frac{\\cos\\theta}{r} \\end{pmatrix}\n$$\nApplying the inverse transformation, we solve for $dr$ and $d\\theta$:\n$$\n\\begin{pmatrix} dr \\\\ d\\theta \\end{pmatrix}\n=\n\\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\frac{\\sin\\theta}{r} & \\frac{\\cos\\theta}{r} \\end{pmatrix}\n\\begin{pmatrix} dx \\\\ dy \\end{pmatrix}\n$$\nThis gives the explicit relations:\n$$dr = (\\cos\\theta)dx + (\\sin\\theta)dy$$\n$$d\\theta = \\left(-\\frac{\\sin\\theta}{r}\\right)dx + \\left(\\frac{\\cos\\theta}{r}\\right)dy$$\nWe evaluate these expressions at the point $p \\in M$ with Cartesian coordinates $(x_0, y_0)$. At this point, $r=r_0=\\sqrt{x_0^2+y_0^2}$, $\\cos\\theta = \\cos\\theta_0 = x_0/r_0$, and $\\sin\\theta = \\sin\\theta_0 = y_0/r_0$.\nSubstituting these into the expression for $dr$ at $p$:\n$$dr\\vert_p = \\left(\\frac{x_0}{r_0}\\right)dx\\vert_p + \\left(\\frac{y_0}{r_0}\\right)dy\\vert_p = \\frac{x_0}{\\sqrt{x_0^2+y_0^2}}dx\\vert_p + \\frac{y_0}{\\sqrt{x_0^2+y_0^2}}dy\\vert_p$$\nFor the second covector, $(r\\,d\\theta)\\vert_p$, we multiply the expression for $d\\theta$ by the coordinate function $r$ and then evaluate at $p$. This is equivalent to $r(p) \\cdot (d\\theta\\vert_p) = r_0 \\,d\\theta\\vert_p$.\n$$(r\\,d\\theta)\\vert_p = r_0 \\left( \\left(-\\frac{\\sin\\theta_0}{r_0}\\right)dx\\vert_p + \\left(\\frac{\\cos\\theta_0}{r_0}\\right)dy\\vert_p \\right) = (-\\sin\\theta_0)dx\\vert_p + (\\cos\\theta_0)dy\\vert_p$$\nSubstituting the Cartesian expressions for $\\sin\\theta_0$ and $\\cos\\theta_0$:\n$$(r\\,d\\theta)\\vert_p = \\left(-\\frac{y_0}{r_0}\\right)dx\\vert_p + \\left(\\frac{x_0}{r_0}\\right)dy\\vert_p = \\frac{-y_0}{\\sqrt{x_0^2+y_0^2}}dx\\vert_p + \\frac{x_0}{\\sqrt{x_0^2+y_0^2}}dy\\vert_p$$\n\n**Part 2: Express the covector $\\omega_{p}$ in the coframe $\\{dr\\vert_{p}, (r\\,d\\theta)\\vert_{p}\\}$.**\n\nFirst, we compute the differential $\\omega = df$ of the function $f(x,y) = x^2 y + \\sin(xy) + \\ln(x^2 + y^2)$.\n$$ \\omega = df = \\frac{\\partial f}{\\partial x}dx + \\frac{\\partial f}{\\partial y}dy $$\nThe partial derivatives are:\n$$ \\frac{\\partial f}{\\partial x} = 2xy + y\\cos(xy) + \\frac{2x}{x^2+y^2} $$\n$$ \\frac{\\partial f}{\\partial y} = x^2 + x\\cos(xy) + \\frac{2y}{x^2+y^2} $$\nAt the point $p=(x_0, y_0)$, the covector $\\omega_p$ is:\n$$ \\omega_p = \\left(2x_0y_0 + y_0\\cos(x_0y_0) + \\frac{2x_0}{x_0^2+y_0^2}\\right)dx\\vert_p + \\left(x_0^2 + x_0\\cos(x_0y_0) + \\frac{2y_0}{x_0^2+y_0^2}\\right)dy\\vert_p $$\nLet's denote the components in the $\\{dx\\vert_p, dy\\vert_p\\}$ basis as $A$ and $B$:\n$$ A = 2x_0y_0 + y_0\\cos(x_0y_0) + \\frac{2x_0}{r_0^2} $$\n$$ B = x_0^2 + x_0\\cos(x_0y_0) + \\frac{2y_0}{r_0^2} $$\nWe want to find scalars $c_1, c_2$ such that $\\omega_p = c_1 dr\\vert_p + c_2 (r\\,d\\theta)\\vert_p$. We substitute the expressions for $dr\\vert_p$ and $(r\\,d\\theta)\\vert_p$ from Part 1:\n$$ \\omega_p = c_1\\left(\\frac{x_0}{r_0}dx\\vert_p + \\frac{y_0}{r_0}dy\\vert_p\\right) + c_2\\left(-\\frac{y_0}{r_0}dx\\vert_p + \\frac{x_0}{r_0}dy\\vert_p\\right) $$\n$$ \\omega_p = \\left(c_1\\frac{x_0}{r_0} - c_2\\frac{y_0}{r_0}\\right)dx\\vert_p + \\left(c_1\\frac{y_0}{r_0} + c_2\\frac{x_0}{r_0}\\right)dy\\vert_p $$\nBy equating the coefficients of $dx\\vert_p$ and $dy\\vert_p$ with $A$ and $B$ respectively, we obtain a system of linear equations for $c_1$ and $c_2$:\n$$ (1) \\quad A = \\frac{c_1 x_0 - c_2 y_0}{r_0} \\implies r_0 A = c_1 x_0 - c_2 y_0 $$\n$$ (2) \\quad B = \\frac{c_1 y_0 + c_2 x_0}{r_0} \\implies r_0 B = c_1 y_0 + c_2 x_0 $$\nTo solve for $c_1$, we multiply (1) by $x_0$ and (2) by $y_0$ and add them:\n$$ r_0(A x_0 + B y_0) = c_1(x_0^2 + y_0^2) = c_1 r_0^2 \\implies c_1 = \\frac{Ax_0 + By_0}{r_0} $$\nTo solve for $c_2$, we multiply (1) by $-y_0$ and (2) by $x_0$ and add them:\n$$ r_0(-A y_0 + B x_0) = c_2(y_0^2 + x_0^2) = c_2 r_0^2 \\implies c_2 = \\frac{Bx_0 - Ay_0}{r_0} $$\nNow we substitute the expressions for $A$ and $B$:\n$$ Ax_0 + By_0 = \\left(2x_0y_0 + y_0\\cos(x_0y_0) + \\frac{2x_0}{r_0^2}\\right)x_0 + \\left(x_0^2 + x_0\\cos(x_0y_0) + \\frac{2y_0}{r_0^2}\\right)y_0 $$\n$$ = 2x_0^2y_0 + x_0 y_0\\cos(x_0y_0) + \\frac{2x_0^2}{r_0^2} + x_0^2y_0 + x_0 y_0\\cos(x_0y_0) + \\frac{2y_0^2}{r_0^2} $$\n$$ = 3x_0^2y_0 + 2x_0y_0\\cos(x_0y_0) + \\frac{2(x_0^2+y_0^2)}{r_0^2} = 3x_0^2y_0 + 2x_0y_0\\cos(x_0y_0) + 2 $$\nSo, $c_1 = \\frac{3x_0^2y_0 + 2x_0y_0\\cos(x_0y_0) + 2}{r_0} = \\frac{3x_0^2y_0 + 2x_0y_0\\cos(x_0y_0) + 2}{\\sqrt{x_0^2+y_0^2}}$.\n\nNext, we compute $Bx_0 - Ay_0$:\n$$ Bx_0 - Ay_0 = \\left(x_0^2 + x_0\\cos(x_0y_0) + \\frac{2y_0}{r_0^2}\\right)x_0 - \\left(2x_0y_0 + y_0\\cos(x_0y_0) + \\frac{2x_0}{r_0^2}\\right)y_0 $$\n$$ = x_0^3 + x_0^2\\cos(x_0y_0) + \\frac{2x_0y_0}{r_0^2} - 2x_0y_0^2 - y_0^2\\cos(x_0y_0) - \\frac{2x_0y_0}{r_0^2} $$\n$$ = x_0^3 - 2x_0y_0^2 + (x_0^2-y_0^2)\\cos(x_0y_0) $$\nSo, $c_2 = \\frac{x_0^3 - 2x_0y_0^2 + (x_0^2-y_0^2)\\cos(x_0y_0)}{r_0} = \\frac{x_0^3 - 2x_0y_0^2 + (x_0^2-y_0^2)\\cos(x_0y_0)}{\\sqrt{x_0^2+y_0^2}}$.\nThe coefficients are thus determined in terms of $x_0$ and $y_0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3x_{0}^{2}y_{0} + 2x_{0}y_{0}\\cos(x_{0}y_{0}) + 2}{\\sqrt{x_{0}^{2}+y_{0}^{2}}} & \\frac{x_{0}^{3} - 2x_{0}y_{0}^{2} + (x_{0}^{2} - y_{0}^{2})\\cos(x_{0}y_{0})}{\\sqrt{x_{0}^{2}+y_{0}^{2}}}\n\\end{pmatrix}\n}\n$$", "id": "2994052"}, {"introduction": "The Riemannian metric does more than measure lengths of tangent vectors; it endows the cotangent space with a rich geometric structure via an induced inner product. This exercise provides hands-on practice with this concept by asking you to apply the Gram-Schmidt algorithm to a set of covectors, transforming them into an orthonormal coframe with respect to a non-Euclidean metric. Completing this task will deepen your understanding of how the inverse metric components $g^{ij}$ are used to compute lengths and angles in the cotangent space. [@problem_id:2994026]", "problem": "Let $(M,g)$ be a Riemannian manifold and let $p \\in M$. Denote by $T_{p}M$ the tangent space at $p$ and by $T^{*}_{p}M$ the cotangent space at $p$. The Riemannian metric $g$ is a symmetric, positive-definite bilinear form on $T_{p}M$ and induces an inner product $g^{-1}$ on $T^{*}_{p}M$ via the inverse metric tensor, defined by the requirement that $g^{-1}(\\alpha,\\beta) = g(g^{-1}\\alpha,g^{-1}\\beta)$ for all $\\alpha,\\beta \\in T^{*}_{p}M$, where $g^{-1}:T^{*}_{p}M \\to T_{p}M$ is the musical isomorphism mapping covectors to vectors through the inverse metric.\n\nConsider a coordinate chart $(x^{1},x^{2})$ near $p$ and the coordinate frame $\\{\\partial/\\partial x^{1},\\partial/\\partial x^{2}\\}$ of $T_{p}M$, with dual coframe $\\{dx^{1},dx^{2}\\}$ of $T^{*}_{p}M$. Suppose the metric at $p$ has matrix\n$$\nG \\;=\\; \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nwith respect to the basis $\\{\\partial/\\partial x^{1},\\partial/\\partial x^{2}\\}$, so the induced inner product $g^{-1}$ on $T^{*}_{p}M$ is represented by the matrix $G^{-1}$ with respect to $\\{dx^{1},dx^{2}\\}$. Let the initial coframe $\\{\\theta^{1},\\theta^{2}\\}$ at $p$ be given by\n$$\n\\theta^{1} \\;=\\; dx^{1} + dx^{2}, \\qquad \\theta^{2} \\;=\\; 2\\,dx^{1} + dx^{2}.\n$$\n\nStarting from the fundamental definitions of a Riemannian metric and the induced inner product on $T^{*}_{p}M$, explain why performing the Gramâ€“Schmidt process on $\\{\\theta^{1},\\theta^{2}\\}$ with respect to $g^{-1}$ produces an orthonormal coframe at $p$. Then carry out this process explicitly to compute the transformation matrix $B$ such that\n$$\n\\begin{pmatrix} \\omega^{1} \\\\ \\omega^{2} \\end{pmatrix} \\;=\\; B \\begin{pmatrix} \\theta^{1} \\\\ \\theta^{2} \\end{pmatrix},\n$$\nwhere $\\{\\omega^{1},\\omega^{2}\\}$ is the resulting $g^{-1}$-orthonormal coframe. Express your final answer for $B$ in exact form with radicals if necessary. No rounding is required, and no physical units are involved. Provide the final matrix as your answer.", "solution": "**Part 1: Justification of the Gram-Schmidt Process**\n\nThe cotangent space $T^*_pM$ is a 2-dimensional real vector space. The Riemannian metric $g$ on the tangent space $T_pM$ induces an inner product on the cotangent space $T^*_pM$, which we denote by $g^{-1}$. This makes $(T^*_pM, g^{-1})$ a finite-dimensional inner product space. The Gram-Schmidt process is a standard algorithm that takes a finite, linearly independent set of vectors in any inner product space and generates an orthonormal set of vectors that spans the same subspace. The given covectors $\\{\\theta^1, \\theta^2\\}$ are linearly independent (their coefficient vectors $(1,1)$ and $(2,1)$ are not collinear), so they form a basis for $T^*_pM$. Therefore, applying the Gram-Schmidt algorithm to this basis with respect to the inner product $g^{-1}$ will produce an orthonormal basis $\\{\\omega^1, \\omega^2\\}$, which is the desired orthonormal coframe. Orthonormality means that $g^{-1}(\\omega^i, \\omega^j) = \\delta^{ij}$, where $\\delta^{ij}$ is the Kronecker delta.\n\n**Part 2: Explicit Gram-Schmidt Orthonormalization**\n\nThe matrix of the metric $g$ at $p$ with respect to the basis $\\{\\frac{\\partial}{\\partial x^1}, \\frac{\\partial}{\\partial x^2}\\}$ is given as\n$$G = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\nThe components are $g_{11} = 2$, $g_{22} = 1$, and $g_{12} = g_{21} = 0$.\nThe matrix of the induced inner product $g^{-1}$ on $T^*_pM$ with respect to the dual basis $\\{dx^1, dx^2\\}$ is the inverse of $G$:\n$$G^{-1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 1 \\end{pmatrix}$$\nThe components are $g^{11} = 1/2$, $g^{22} = 1$, and $g^{12} = g^{21} = 0$.\nFor any two covectors $\\alpha = \\alpha_1 dx^1 + \\alpha_2 dx^2$ and $\\beta = \\beta_1 dx^1 + \\beta_2 dx^2$, their inner product is\n$g^{-1}(\\alpha, \\beta) = \\sum_{i,j=1}^2 \\alpha_i \\beta_j g^{ij} = \\frac{1}{2}\\alpha_1\\beta_1 + \\alpha_2\\beta_2$.\nThe norm-squared of a covector $\\alpha$ is $\\|\\alpha\\|^2_{g^{-1}} = g^{-1}(\\alpha, \\alpha)$.\n\nLet's apply the Gram-Schmidt process to $\\{\\theta^1, \\theta^2\\}$.\n1.  **Construct $\\omega^1$**:\n    Let the first vector of the new basis be proportional to $\\theta^1$. We normalize it.\n    $\\theta^1 = 1 \\cdot dx^1 + 1 \\cdot dx^2$.\n    The norm-squared of $\\theta^1$ is:\n    $\\|\\theta^1\\|^2_{g^{-1}} = g^{-1}(\\theta^1, \\theta^1) = \\frac{1}{2}(1)^2 + (1)^2 = \\frac{1}{2} + 1 = \\frac{3}{2}$.\n    The norm is $\\|\\theta^1\\|_{g^{-1}} = \\sqrt{\\frac{3}{2}}$.\n    The first orthonormal covector is:\n    $\\omega^1 = \\frac{\\theta^1}{\\|\\theta^1\\|_{g^{-1}}} = \\frac{1}{\\sqrt{3/2}}\\theta^1 = \\sqrt{\\frac{2}{3}}\\theta^1$.\n\n2.  **Construct $\\omega^2$**:\n    First, we construct a vector $\\alpha^2$ orthogonal to $\\omega^1$ (and thus to $\\theta^1$):\n    $\\alpha^2 = \\theta^2 - \\text{proj}_{\\omega^1}\\theta^2 = \\theta^2 - g^{-1}(\\theta^2, \\omega^1)\\omega^1$.\n    We compute the inner product:\n    $g^{-1}(\\theta^2, \\omega^1) = g^{-1}(\\theta^2, \\sqrt{\\frac{2}{3}}\\theta^1) = \\sqrt{\\frac{2}{3}}g^{-1}(\\theta^2, \\theta^1)$.\n    $\\theta^2 = 2 \\cdot dx^1 + 1 \\cdot dx^2$.\n    $g^{-1}(\\theta^2, \\theta^1) = \\frac{1}{2}(2)(1) + (1)(1) = 1 + 1 = 2$.\n    So, $g^{-1}(\\theta^2, \\omega^1) = 2\\sqrt{\\frac{2}{3}}$.\n    Substituting this back into the expression for $\\alpha^2$:\n    $\\alpha^2 = \\theta^2 - \\left(2\\sqrt{\\frac{2}{3}}\\right) \\omega^1 = \\theta^2 - \\left(2\\sqrt{\\frac{2}{3}}\\right) \\left(\\sqrt{\\frac{2}{3}}\\theta^1\\right) = \\theta^2 - \\frac{4}{3}\\theta^1$.\n    Now, we normalize $\\alpha^2$ to get $\\omega^2$. Let's first express $\\alpha^2$ in the $\\{dx^1, dx^2\\}$ basis:\n    $\\alpha^2 = (2dx^1 + dx^2) - \\frac{4}{3}(dx^1 + dx^2) = (2 - \\frac{4}{3})dx^1 + (1 - \\frac{4}{3})dx^2 = \\frac{2}{3}dx^1 - \\frac{1}{3}dx^2$.\n    The norm-squared of $\\alpha^2$ is:\n    $\\|\\alpha^2\\|^2_{g^{-1}} = g^{-1}(\\alpha^2, \\alpha^2) = \\frac{1}{2}\\left(\\frac{2}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 = \\frac{1}{2}\\left(\\frac{4}{9}\\right) + \\frac{1}{9} = \\frac{2}{9} + \\frac{1}{9} = \\frac{3}{9} = \\frac{1}{3}$.\n    The norm is $\\|\\alpha^2\\|_{g^{-1}} = \\sqrt{\\frac{1}{3}} = \\frac{1}{\\sqrt{3}}$.\n    The second orthonormal covector is:\n    $\\omega^2 = \\frac{\\alpha^2}{\\|\\alpha^2\\|_{g^{-1}}} = \\frac{1}{1/\\sqrt{3}}\\alpha^2 = \\sqrt{3}\\alpha^2 = \\sqrt{3}\\left(\\theta^2 - \\frac{4}{3}\\theta^1\\right)$.\n\n**Part 3: Determination of the Transformation Matrix $B$**\n\nWe have found the expressions for $\\{\\omega^1, \\omega^2\\}$ in terms of $\\{\\theta^1, \\theta^2\\}$:\n$\\omega^1 = \\sqrt{\\frac{2}{3}}\\theta^1 + 0 \\cdot \\theta^2$\n$\\omega^2 = \\sqrt{3}\\left(-\\frac{4}{3}\\theta^1 + \\theta^2\\right) = -\\frac{4\\sqrt{3}}{3}\\theta^1 + \\sqrt{3}\\theta^2$\n\nThe problem requires finding the matrix $B$ such that:\n$$\n\\begin{pmatrix} \\omega^{1} \\\\ \\omega^{2} \\end{pmatrix} \\;=\\; B \\begin{pmatrix} \\theta^{1} \\\\ \\theta^{2} \\end{pmatrix} \\;=\\; \\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix} \\begin{pmatrix} \\theta^{1} \\\\ \\theta^{2} \\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n$\\omega^1 = b_{11}\\theta^1 + b_{12}\\theta^2$\n$\\omega^2 = b_{21}\\theta^1 + b_{22}\\theta^2$\n\nBy comparing coefficients with our derived expressions, we identify the elements of $B$:\n$b_{11} = \\sqrt{\\frac{2}{3}}$\n$b_{12} = 0$\n$b_{21} = -\\frac{4\\sqrt{3}}{3}$\n$b_{22} = \\sqrt{3}$\n\nThus, the transformation matrix $B$ is:\n$$\nB = \\begin{pmatrix} \\sqrt{\\frac{2}{3}} & 0 \\\\ -\\frac{4\\sqrt{3}}{3} & \\sqrt{3} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{\\frac{2}{3}} & 0 \\\\ -\\frac{4\\sqrt{3}}{3} & \\sqrt{3} \\end{pmatrix}}\n$$", "id": "2994026"}]}