{"hands_on_practices": [{"introduction": "Vector fields, as sections of the tangent bundle, are central to differential geometry. A key tool for understanding their interaction with other geometric objects is the Lie derivative, which measures the rate of change of a function or tensor along the flow of a vector field. This first practice [@problem_id:1687888] offers a concrete calculation of a Lie derivative, starting from a vector field constructed through a simple geometric rotation, thereby reinforcing the computational skills essential for working with sections.", "problem": "Consider the two-dimensional Euclidean plane $\\mathbb{R}^2$ with standard Cartesian coordinates $(x, y)$. A smooth vector field on $\\mathbb{R}^2$ can be conceptualized as an assignment of a tangent vector to each point in the plane.\n\nLet $v$ be the *radial position vector field*, where for any point $p=(x,y)$, the vector $v_p$ at that point has components $(x,y)$ with respect to the standard basis $\\{\\frac{\\partial}{\\partial x}|_p, \\frac{\\partial}{\\partial y}|_p\\}$. Thus, $v = x \\frac{\\partial}{\\partial x} + y \\frac{\\partial}{\\partial y}$.\n\nAt each point $p \\in \\mathbb{R}^2$, we define a linear map on the tangent space $J_p: T_p\\mathbb{R}^2 \\to T_p\\mathbb{R}^2$ which corresponds to a counterclockwise rotation by an angle of $\\frac{\\pi}{2}$ radians.\n\nA new vector field, which we denote as $X$, is constructed by applying this rotation to the radial position vector field at every point. That is, for each point $p$, the vector $X_p$ is defined as $X_p = J_p(v_p)$.\n\nNow, consider a scalar function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ given by $f(x,y) = \\cos(\\alpha x) \\sinh(\\beta y)$, where $\\alpha$ and $\\beta$ are non-zero real constants. The Lie derivative of $f$ with respect to $X$, denoted by $\\mathcal{L}_X f$, measures the rate of change of the function $f$ along the integral curves defined by the vector field $X$.\n\nCalculate the value of the Lie derivative $\\mathcal{L}_X f$ at the point $p_0 = (\\frac{\\pi}{4\\alpha}, \\frac{1}{\\beta})$. Express your final answer as a single closed-form analytic expression in terms of the constants $\\alpha$ and $\\beta$.", "solution": "The linear map $J_{p}$ is the counterclockwise rotation by $\\frac{\\pi}{2}$, which in standard coordinates has matrix $\\begin{pmatrix}0 & -1 \\\\ 1 & 0\\end{pmatrix}$. Applying $J_{p}$ to the radial vector $v_{p}=(x,y)$ gives\n$$\nX_{p}=J_{p}(v_{p})=(-y,x),\n$$\nso the vector field is\n$$\nX=-y\\,\\frac{\\partial}{\\partial x}+x\\,\\frac{\\partial}{\\partial y}.\n$$\nFor the scalar function $f(x,y)=\\cos(\\alpha x)\\sinh(\\beta y)$, its partial derivatives are\n$$\n\\frac{\\partial f}{\\partial x}=-\\alpha \\sin(\\alpha x)\\sinh(\\beta y), \\qquad \\frac{\\partial f}{\\partial y}=\\beta \\cos(\\alpha x)\\cosh(\\beta y).\n$$\nThe Lie derivative of $f$ along $X$ is $\\,\\mathcal{L}_{X}f=X(f)$, hence\n$$\n\\mathcal{L}_{X}f=-y\\,\\frac{\\partial f}{\\partial x}+x\\,\\frac{\\partial f}{\\partial y}\n=\\alpha y \\sin(\\alpha x)\\sinh(\\beta y)+\\beta x \\cos(\\alpha x)\\cosh(\\beta y).\n$$\nEvaluating at $p_{0}=\\left(\\frac{\\pi}{4\\alpha},\\frac{1}{\\beta}\\right)$ gives $\\alpha x=\\frac{\\pi}{4}$ and $\\beta y=1$, so\n$$\n\\sin(\\alpha x)=\\sin\\!\\left(\\frac{\\pi}{4}\\right)=\\frac{\\sqrt{2}}{2},\\qquad\n\\cos(\\alpha x)=\\cos\\!\\left(\\frac{\\pi}{4}\\right)=\\frac{\\sqrt{2}}{2},\\qquad\n\\sinh(\\beta y)=\\sinh(1),\\qquad\n\\cosh(\\beta y)=\\cosh(1).\n$$\nTherefore,\n$$\n\\mathcal{L}_{X}f\\bigg|_{p_{0}}\n=\\alpha\\left(\\frac{1}{\\beta}\\right)\\left(\\frac{\\sqrt{2}}{2}\\right)\\sinh(1)\n+\\beta\\left(\\frac{\\pi}{4\\alpha}\\right)\\left(\\frac{\\sqrt{2}}{2}\\right)\\cosh(1)\n=\\frac{\\sqrt{2}}{2}\\left(\\frac{\\alpha}{\\beta}\\sinh(1)+\\frac{\\beta\\pi}{4\\alpha}\\cosh(1)\\right).\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{2}\\left(\\frac{\\alpha}{\\beta}\\sinh(1)+\\frac{\\beta\\pi}{4\\alpha}\\cosh(1)\\right)}$$", "id": "1687888"}, {"introduction": "Moving from local computations to global properties, we now explore how the topology of the base manifold constrains the sections it admits. The celebrated Poincaré-Hopf theorem reveals a deep connection between the zeros of a vector field—a local property of a section—and the Euler characteristic, a global topological invariant of the manifold. This exercise [@problem_id:1687864] invites you to apply this theorem to determine the minimal number of zeros a vector field on a genus-two surface can have, and to identify the local structure of such a zero.", "problem": "A smooth vector field, which is a smooth section of the tangent bundle, is constructed on a genus-two surface $\\Sigma_2$. A genus-two surface is a compact, orientable surface without boundary, topologically equivalent to a double torus. The Poincaré-Hopf theorem states that for any such surface, the sum of the indices of the zeros of any smooth vector field is equal to the Euler characteristic $\\chi(\\Sigma_2)$ of the surface. For a surface of genus $g$, the Euler characteristic is given by the formula $\\chi(\\Sigma_g) = 2 - 2g$.\n\nThe index of an isolated zero is an integer value that characterizes the behavior of the vector field in its immediate vicinity. For a vector field to have the minimum possible number of zeros on $\\Sigma_2$, it must consist of one or more zeros whose indices sum to $\\chi(\\Sigma_2)$.\n\nWhat is the minimum number of zeros possible for a smooth vector field on $\\Sigma_2$, and which of the following local expressions for a vector field $X(x,y)$ could represent a zero at the origin $(0,0)$ that would be present in such a minimal configuration?\n\nA. Minimum zeros: 2; Local form: $X(x,y) = (x, -y)$\n\nB. Minimum zeros: 1; Local form: $X(x,y) = (x^2-y^2, -2xy)$\n\nC. Minimum zeros: 1; Local form: $X(x,y) = (y, -x)$\n\nD. Minimum zeros: 2; Local form: $X(x,y) = (x, y)$\n\nE. Minimum zeros: 0; Local form: $X(x,y) = (1, 0)$", "solution": "By the Poincaré–Hopf theorem, for any smooth vector field on a compact, orientable surface without boundary, the sum of the indices of its isolated zeros equals the Euler characteristic of the surface:\n$$\n\\sum_{p \\in \\text{zeros}} \\operatorname{ind}_{p}(X) \\,=\\, \\chi(\\Sigma_{g}).\n$$\nFor a genus-$g$ surface, the Euler characteristic is\n$$\n\\chi(\\Sigma_{g}) \\,=\\, 2 - 2g.\n$$\nFor $\\Sigma_{2}$ we have\n$$\n\\chi(\\Sigma_{2}) \\,=\\, 2 - 2\\cdot 2 \\,=\\, -2.\n$$\nTherefore any smooth vector field on $\\Sigma_{2}$ must satisfy\n$$\n\\sum_{p} \\operatorname{ind}_{p}(X) \\,=\\, -2.\n$$\n\nTo minimize the number of zeros, we seek the smallest number of isolated zeros whose indices sum to $-2$. Since the index of an isolated zero is an integer and degenerate zeros are allowed, one can realize a single zero with index $-2$. Thus the minimum possible number of zeros is\n$$\n1.\n$$\n\nWe now analyze the local expressions offered to determine which could represent a zero at the origin compatible with such a minimal configuration, i.e., a zero of index $-2$.\n\n- For $X(x,y) = (x,-y)$: this is linear with Jacobian matrix $\\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix}$, whose determinant is $-1$. For a nondegenerate linear zero in the plane, the index equals $\\operatorname{sign}(\\det J)$, hence $\\operatorname{ind}_{0}(X) = -1$. This would require at least two zeros to sum to $-2$, so it is not compatible with a single zero minimal configuration.\n\n- For $X(x,y) = (x^{2} - y^{2}, -2xy)$: write $(x,y) = r(\\cos \\theta, \\sin \\theta)$. Then\n$$\nX(x,y) = \\big(r^{2}\\cos 2\\theta,\\,-r^{2}\\sin 2\\theta\\big) = r^{2}\\big(\\cos(-2\\theta),\\,\\sin(-2\\theta)\\big).\n$$\nOn a small circle $r = \\text{const}$, the direction of $X$ has angle $-2\\theta$, so as $\\theta$ increases by $2\\pi$, the direction winds by $-4\\pi$. Hence the degree of the map $S^{1}\\to S^{1}$ given by the normalized vector field is $-2$, and therefore\n$$\n\\operatorname{ind}_{0}(X) = -2.\n$$\nThis realizes a single zero of index $-2$, matching $\\chi(\\Sigma_{2})$, and thus is compatible with the minimal configuration of one zero.\n\n- For $X(x,y) = (y,-x)$: the Jacobian matrix $\\begin{pmatrix}0 & 1 \\\\ -1 & 0\\end{pmatrix}$ has determinant $+1$, so the index at the origin is $+1$. A single zero of index $+1$ cannot satisfy the required total of $-2$.\n\n- For $X(x,y) = (x,y)$: the Jacobian has determinant $+1$, so the index is $+1$, not compatible with summing to $-2$ with only two zeros.\n\n- For $X(x,y) = (1,0)$: there is no zero at the origin; moreover, a vector field with no zeros would have total index $0$, contradicting $\\chi(\\Sigma_{2}) = -2$.\n\nTherefore, the minimum number of zeros is $1$, and among the given local forms, the one that can represent a zero at the origin in such a minimal configuration is $X(x,y) = (x^{2}-y^{2}, -2xy)$.\n\nThe correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1687864"}, {"introduction": "To generalize differentiation to sections of an arbitrary vector bundle, we must introduce the structure of a connection. A central concept in this theory is curvature, which measures the failure of parallelism to be a path-independent notion. This final practice [@problem_id:3005912] delves into the fundamental correspondence between a \"flat\" (zero curvature) connection and the existence of a frame of parallel sections on a simply connected manifold, a result with deep implications for both geometry and physics. You will first articulate this theoretical principle and then apply it to explicitly solve for a parallel frame, thus bridging abstract theory with concrete computation.", "problem": "Let $U \\subset \\mathbb{R}^{2}$ be a simply connected open set equipped with the standard Riemannian metric, and let $\\pi : E \\to U$ be the trivial rank-$2$ real vector bundle $E = U \\times \\mathbb{R}^{2}$. A linear connection on $E$ is a covariant derivative operator $\\nabla : \\Gamma(E) \\to \\Omega^{1}(U; E)$ satisfying linearity and the Leibniz rule with respect to smooth functions on $U$. With respect to the standard global trivial frame $\\{s_{1}, s_{2}\\}$ of $E$ (the constant sections corresponding to the standard basis of $\\mathbb{R}^{2}$), such a connection can be written as $\\nabla = d + A$, where $A$ is a matrix of $1$-forms, $A \\in \\Omega^{1}(U; \\operatorname{End}(\\mathbb{R}^{2}))$. Its curvature $2$-form is $F^{\\nabla} = \\nabla^{2} = dA + A \\wedge A \\in \\Omega^{2}(U; \\operatorname{End}(\\mathbb{R}^{2}))$.\n\nStarting from these foundational definitions, do the following:\n\n1. Explain why the vanishing curvature condition $F^{\\nabla} = 0$ on a simply connected open set $U$ implies the existence of a local frame of parallel sections $\\{e_{1}, e_{2}\\}$ on $E$, namely sections satisfying $\\nabla e_{i} = 0$ for $i = 1, 2$, that form a pointwise basis of $E$ over $U$.\n\n2. Consider the explicit connection on $E$ given in the standard trivialization by the $\\operatorname{End}(\\mathbb{R}^{2})$-valued $1$-form\n$$\nA = \\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix} \\, dx + \\begin{pmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix} \\, dy,\n$$\nwhere $x, y$ are the standard coordinates on $\\mathbb{R}^{2}$. Verify that its curvature vanishes. Then, using only the fundamental definitions of parallel transport, curvature, and integrability, construct a local frame of parallel sections $\\{e_{1}, e_{2}\\}$ on $U$ by solving the linear system for a frame matrix $G(x,y) \\in \\operatorname{GL}(2, \\mathbb{R})$ that satisfies the first-order system of partial differential equations $\\mathrm{d}G + A G = 0$ with the initial condition $G(0,0) = I$, where $I$ is the identity matrix.\n\nProvide your final answer as the single $2 \\times 2$ matrix $G(x,y)$ whose columns are the parallel sections $e_{1}(x,y)$ and $e_{2}(x,y)$ expressed in the trivial frame $\\{s_{1}, s_{2}\\}$. No rounding is required. Your answer must be a closed-form analytic expression.", "solution": "The problem is divided into two parts. The first part requires a theoretical explanation of why a flat connection on a simply connected domain admits a global frame of parallel sections. The second part requires an explicit calculation of such a frame for a given connection.\n\nPart 1: Theoretical Foundation\n\nA section $s \\in \\Gamma(E)$ is said to be parallel with respect to the connection $\\nabla$ if $\\nabla s = 0$. The vector bundle $E$ is trivial, $E = U \\times \\mathbb{R}^{2}$, so any section $s$ can be written globally in terms of the constant frame fields $\\{s_{1}, s_{2}\\}$, which correspond to the standard basis of $\\mathbb{R}^{2}$. A section $s$ can be represented by a column vector of its component functions, $f = \\begin{pmatrix} f^{1} \\\\ f^{2} \\end{pmatrix}$, such that $s = f^{1}s_{1} + f^{2}s_{2}$.\n\nThe connection $\\nabla$ is given by $\\nabla = d + A$, where $d$ is the exterior derivative and $A \\in \\Omega^{1}(U; \\operatorname{End}(\\mathbb{R}^{2}))$ is the connection $1$-form matrix. The condition for a section $s$ to be parallel, $\\nabla s = 0$, translates into a condition on its component vector $f$:\n$$\n\\nabla s = (d+A)(f) = df + A \\wedge f = df + Af = 0\n$$\nThe wedge product is implicit when a matrix of $1$-forms acts on a vector of functions (0-forms). Thus, we have the first-order system of partial differential equations $df = -Af$.\n\nWe are looking for a local frame of parallel sections $\\{e_{1}, e_{2}\\}$. A frame is a set of sections that are pointwise linearly independent. Let the component vectors of $e_{1}$ and $e_{2}$ with respect to the frame $\\{s_{1}, s_{2}\\}$ be arranged as the columns of a $2 \\times 2$ matrix $G(x,y)$. For $\\{e_{1}, e_{2}\\}$ to be a frame, the matrix $G(x,y)$ must be invertible for all $(x,y) \\in U$, i.e., $G \\in \\operatorname{GL}(2, \\mathbb{R})$.\n\nThe condition that both $e_{1}$ and $e_{2}$ are parallel sections is equivalent to the matrix equation:\n$$\ndG + AG = 0\n$$\nThis is a system of first-order partial differential equations for the matrix-valued function $G$. A necessary condition for the existence of a solution to this system is the integrability condition. We can derive this condition by applying the exterior derivative $d$ to the equation $dG = -AG$.\nUsing the property $d^{2} = 0$ on the left side, we get $d(dG) = 0$.\nApplying $d$ to the right side and using the product rule for exterior derivatives ($d(M \\wedge N) = dM \\wedge N + (-1)^{p} M \\wedge dN$ for a $p$-form $M$):\n$$\nd(-AG) = -(dA)G + A \\wedge (dG)\n$$\nHere, $A$ is a matrix of $1$-forms and $G$ is a matrix of $0$-forms, so the product rule simplifies. Setting the results equal:\n$$\n0 = -(dA)G + A \\wedge (dG)\n$$\nNow, substitute $dG = -AG$ into this equation:\n$$\n0 = -(dA)G + A \\wedge (-AG) = -(dA)G - (A \\wedge A)G = -(dA + A \\wedge A)G\n$$\nThe expression $F^{\\nabla} = dA + A \\wedge A$ is the curvature $2$-form of the connection. So the equation becomes:\n$$\n-F^{\\nabla}G = 0\n$$\nSince $G$ is an invertible matrix (as it represents a frame), we can multiply by its inverse $G^{-1}$ from the right to obtain:\n$$\nF^{\\nabla} = 0\n$$\nThis shows that the vanishing of the curvature, $F^{\\nabla}=0$, is a necessary condition for the existence of a parallel frame. The Frobenius integrability theorem states that on a simply connected domain $U$, this condition is also sufficient. The simple-connectedness of $U$ guarantees that for any two points $p_{0}, p_{1} \\in U$, the result of parallel transporting an initial frame from $p_{0}$ to $p_{1}$ is independent of the path taken between them. This allows for a well-defined parallel frame $G(p)$ for all $p \\in U$ by choosing an initial condition $G(p_{0}) = G_{0}$ (e.g., $G_{0}=I$) and parallel transporting it everywhere.\n\nPart 2: Explicit Construction\n\nWe are given the connection $1$-form on $U = \\mathbb{R}^{2}$:\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\, dx + \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\, dy\n$$\nLet $A_{x} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$ and $A_{y} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$. So, $A = A_{x}dx$.\n\nFirst, we verify that the curvature $F^{\\nabla} = dA + A \\wedge A$ vanishes.\nThe term $dA$ is:\n$$\ndA = d(A_{x}dx + A_{y}dy) = d(A_{x}) \\wedge dx + d(A_{y}) \\wedge dy\n$$\nSince $A_{x}$ and $A_{y}$ are constant matrices, their exterior derivatives $d(A_{x})$ and $d(A_{y})$ are zero. Thus, $dA = 0$.\nThe term $A \\wedge A$ is:\n$$\nA \\wedge A = (A_{x}dx + A_{y}dy) \\wedge (A_{x}dx + A_{y}dy) = (A_{x}A_{y} - A_{y}A_{x}) dx \\wedge dy\n$$\nWe compute the matrix commutator:\n$$\nA_{x}A_{y} - A_{y}A_{x} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nSo, $A \\wedge A = 0$. Therefore, $F^{\\nabla} = 0 + 0 = 0$. The connection is flat.\n\nNow, we construct the parallel frame $G(x,y)$ by solving $dG + AG = 0$ with the initial condition $G(0,0) = I$.\nWriting $dG = \\frac{\\partial G}{\\partial x}dx + \\frac{\\partial G}{\\partial y}dy$ and $A = A_x dx$, the equation $dG = -AG$ splits into a system of two matrix partial differential equations:\n$$\n\\frac{\\partial G}{\\partial x} = -A_{x}G\n$$\n$$\n\\frac{\\partial G}{\\partial y} = -A_{y}G\n$$\nSubstituting the matrices $A_{x}$ and $A_{y}$:\n$$\n(1) \\quad \\frac{\\partial G}{\\partial x} = - \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} G\n$$\n$$\n(2) \\quad \\frac{\\partial G}{\\partial y} = - \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} G = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nFrom equation $(2)$, we see that the partial derivatives of all components of $G$ with respect to $y$ are zero. This implies that $G$ is a function of $x$ only, so we write $G(x,y) = G(x)$.\nEquation $(1)$ becomes an ordinary differential equation for $G(x)$:\n$$\n\\frac{dG}{dx} = - \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} G(x)\n$$\nLet $G(x) = \\begin{pmatrix} a(x) & b(x) \\\\ c(x) & d(x) \\end{pmatrix}$. The ODE system is:\n$$\n\\frac{d}{dx}\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = -\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} -c & -d \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis gives four scalar ODEs:\n(a) $\\frac{da}{dx} = -c$\n(b) $\\frac{db}{dx} = -d$\n(c) $\\frac{dc}{dx} = 0$\n(d) $\\frac{dd}{dx} = 0$\n\nThe initial condition is $G(0,0)=I$, which means $G(0) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$. This gives the initial values: $a(0)=1$, $b(0)=0$, $c(0)=0$, $d(0)=1$.\n\nFrom (c), $\\frac{dc}{dx}=0 \\implies c(x)$ is a constant. Using $c(0)=0$, we have $c(x)=0$ for all $x$.\nFrom (d), $\\frac{dd}{dx}=0 \\implies d(x)$ is a constant. Using $d(0)=1$, we have $d(x)=1$ for all $x$.\n\nSubstitute these results into (a) and (b):\nFrom (a), $\\frac{da}{dx} = -c(x) = 0 \\implies a(x)$ is a constant. Using $a(0)=1$, we have $a(x)=1$.\nFrom (b), $\\frac{db}{dx} = -d(x) = -1$. Integrating with respect to $x$ gives $b(x) = \\int -1 \\, dx = -x + K$ for some constant $K$. Using $b(0)=0$, we get $0 = -0 + K \\implies K=0$. So, $b(x) = -x$.\n\nCombining these components, we find the matrix $G(x,y)$:\n$$\nG(x,y) = \\begin{pmatrix} 1 & -x \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe columns of this matrix are the component vectors of the parallel sections $e_1(x,y) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $e_2(x,y) = \\begin{pmatrix} -x \\\\ 1 \\end{pmatrix}$ in the trivial frame $\\{s_1, s_2\\}$.\nThese sections are $e_1 = s_1$ and $e_2 = -x s_1 + s_2$. For any $(x,y) \\in \\mathbb{R}^2$, $\\det(G) = 1 \\neq 0$, so they indeed form a frame.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & -x \\\\ 0 & 1 \\end{pmatrix}}\n$$", "id": "3005912"}]}