{"hands_on_practices": [{"introduction": "在计算生物学中，分析脚本很容易变得冗长而复杂，从而难以调试、维护和复用。本练习将引导你应用模块化设计的核心原则，将一个庞大的整体脚本重构为一组目标明确的小函数。通过掌握这种将复杂问题分解为独立、可管理部分的能力，你将能够构建出更清晰、更可靠且更易于协作的可复现工作流 [@problem_id:1463184]。", "problem": "一名系统生物学入门课程的学生编写了一个300行的Python脚本，用于分析微阵列实验中的基因表达数据集。该脚本执行一个完整的端到端分析。操作顺序如下：\n\n1.  从指定的逗号分隔值（CSV）文件中加载原始基因表达数据到一个数据结构中。\n2.  过滤掉在所有实验样本中表达水平低于某一阈值的基因，因为这些很可能是实验噪声。\n3.  对过滤后的数据进行分位数归一化，以确保每个样本的表达分布在统计上是可比较的。\n4.  进行基因层面的t检验，以识别在‘处理组’和‘对照组’之间存在统计学显著差异表达的基因。\n5.  生成火山图，以可视化每个基因的倍数变化和统计显著性。\n6.  最后，将显著差异表达基因的列表保存到一个新的CSV文件中，并将生成的火山图保存为PNG图像文件。\n\n为了提高脚本的可读性、使其更易于调试，并允许部分分析在其他项目中重用，该学生决定将这个庞大的脚本重构为一组更小、更专注的函数。\n\n基于每个函数应具有单一、明确职责的原则，以下哪组函数代表了对原始脚本最合乎逻辑且可维护的模块化方案？\n\nA. `load_expression_data(filepath)`\n   `filter_low_expression_genes(data)`\n   `normalize_data(data)`\n   `find_differential_genes(data)`\n   `create_volcano_plot(statistics)`\n   `save_results(gene_list, plot_object)`\n\nB. `prepare_and_normalize_data(filepath)`\n   `analyze_and_visualize(data)`\n   `save_all_outputs(results)`\n\nC. `handle_file_io(input_path, output_path_prefix)`\n   `compute_statistics(raw_data)`\n   `generate_visuals(stats_results)`\n\nD. `run_full_analysis(file_path)`\n   `configure_parameters()`\n\nE. `load_data_and_filter(filepath)`\n   `normalize_and_analyze(filtered_data)`\n   `plot_and_save(analysis_results, output_path)`", "solution": "我们想要一种遵循单一职责原则的模块化分解：每个函数应执行一个连贯的任务，具有清晰的输入和输出，易于独立测试，并可在不同上下文中重用。原始脚本的工作流程包括六个概念上不同的步骤：加载数据、过滤低表达基因、归一化、进行差异表达的统计检验、创建火山图以及保存输出（列表和图）。\n\n评估每个选项：\n\n- 选项A为每个概念步骤定义了一个函数：\n  - load_expression_data(filepath)：仅负责将CSV文件读取到数据结构中。\n  - filter_low_expression_genes(data)：仅负责按表达阈值进行过滤。\n  - normalize_data(data)：仅负责归一化（分位数归一化）。\n  - find_differential_genes(data)：仅负责统计检验（例如，基因层面的t检验和倍数变化）。\n  - create_volcano_plot(statistics)：仅负责根据检验统计量生成可视化对象。\n  - save_results(gene_list, plot_object)：仅负责持久化输出（CSV和PNG）。\n  这种方式将六个步骤一一对应，最大限度地减少了不同关注点之间的耦合，最大化了每个函数内部的内聚性，便于单元测试（例如，为每个函数模拟输入），并促进了重用（例如，独立重用归一化或绘图功能）。\n\n- 选项B将多个不同的职责组合到宽泛的函数中（例如，prepare_and_normalize_data合并了加载和归一化；analyze_and_visualize合并了统计和绘图），从而降低了清晰度、可测试性和可重用性。\n\n- 选项C将读写操作合并到单个IO函数中，并将过滤、归一化和检验合并在compute_statistics下，这模糊了预处理和推断之间的界限。这降低了模块化程度和灵活性。\n\n- 选项D基本上是非模块化的：run_full_analysis包揽了所有工作，而configure_parameters本身不是分析流程的一部分。\n\n- 选项E仍然捆绑了任务（例如，load_data_and_filter、normalize_and_analyze、plot_and_save），违反了单一职责原则，使重用和测试更加困难。\n\n因此，选项A是最合乎逻辑且可维护的模块化方案，因为它将流程中的每一步都清晰地分离到一个定义明确的函数中，每个函数直接对应原始工作流程的一个阶段，从而提高了可读性、可调试性和重用潜力。", "answer": "$$\\boxed{A}$$", "id": "1463184"}, {"introduction": "版本控制是可复现研究的基石，而 Git 是这一领域的事实标准。然而，仅仅使用 Git 是不够的；你的提交历史必须清晰地讲述项目演变的故事。本练习将重点放在编写信息丰富的 Git 提交信息这一关键技能上，展示一个精心撰写的提交信息如何将含糊的更新记录转变为对“为什么”进行这项更改的宝贵解释，从而极大地增强项目的可理解性和可复现性 [@problem_id:1463216]。", "problem": "一个系统生物学实验室的研究人员正在开发一个计算工作流程，用于分析来自质谱仪的蛋白质组学数据。他们正在使用版本控制系统 Git 来跟踪其主分析脚本 `process_quantification.py` 的变更。该研究人员刚刚修改了脚本，引入了一个新的质量控制步骤：任何基于少于两个唯一肽段而被鉴定出的蛋白质现在都将被过滤掉，并从最终结果中排除。他们现在需要将此更改提交到仓库中。\n\n他们最初的、信息量不足的提交信息是 `git commit -m \"updated script\"`。遵循维护可复现且易于理解的项目历史的最佳实践，以下哪个备选提交信息对于此更改是最有效且信息最丰富的？\n\nA. `git commit -m \"I added a new filtering step to process_quantification.py\"`\n\nB. `git commit -m \"Bugfix: Correct protein quantification values\"`\n\nC. `git commit -m \"Feat: Add filter for low-confidence proteins\n\n   Proteins identified by fewer than two unique peptides are now\n   removed from the analysis. This improves the reliability of the\n   downstream analysis by eliminating poorly supported protein\n   identifications.\"`\n\nD. `git commit -m \"Updated process_quantification.py on line 87 to add a filter where df['unique_peptides'] >= 2.\"`\n\nE. `git commit -m \"misc changes and updates\"`", "solution": "目标：选择最有效、信息最丰富的提交信息，以维护一个可复现且易于理解的项目历史。\n\n最佳实践标准：\n- 使用简洁、祈使语气的主题行，描述此更改“做了什么”，而不是“我做了什么”（例如，使用“Add filter”而不是“I added”或“Updated”）。\n- （可选）使用常规提交类型（例如，Feat、Fix）对更改进行分类。\n- 在正文中解释更改了什么以及为什么更改，重点关注行为和基本原理，而不是底层的实现细节。\n- 避免含糊不清的信息，并避免错误的分类（例如，将新功能称为错误修复）。\n- 避免使用不稳定的具体信息，如行号或可能变化的临时变量名，这些信息无助于未来的读者理解意图。\n\n评估选项：\n- A：使用了第一人称和过去时（“I added ...”），缺乏基本原理和细节；不太符合祈使语气和最佳实践。\n- B：标记为错误修复（bugfix），但所描述的更改是一种新的过滤行为；分类错误且具有误导性。\n- C：使用了“Feat”类型，祈使语气的“Add”，清晰地陈述了行为变更（过滤掉唯一肽段少于两个的蛋白质），并解释了其基本原理和影响（通过移除支持度不足的蛋白质鉴定来提高可靠性）。这最能支持可复现性和可理解性。\n- D：提到了行号和具体的代码表达式；这些信息不稳定，对理解意图没有帮助，并且主题是“Updated”，这很模糊且不符合祈使语气。\n- E：含糊不清，信息量不足。\n\n结论：根据既定的最佳实践，最佳选项是 C。", "answer": "$$\\boxed{C}$$", "id": "1463216"}, {"introduction": "像 Jupyter Notebook 这样的交互式计算环境是进行探索性数据分析的强大工具，但它们也带来了独特的可复现性挑战。笔记本的“状态”取决于代码单元的执行顺序，而非其在文档中的位置，这可能导致看似正确的代码产生错误的结果。这个练习通过一个常见的场景，揭示了乱序执行的潜在危害，强调了按顺序运行整个笔记本以验证最终结果的重要性 [@problem_id:1463183]。", "problem": "计算生物学家 Dr. Anya Sharma 正在使用 Jupyter Notebook 分析一个系统生物学数据集，该数据集包含来自细胞培养实验的蛋白质丰度测量值。该实验有两个条件：“对照组”（control）和“药物处理组”（drug-treated）。数据最初存储在一个名为 `protein_data` 的 DataFrame 中。Dr. Sharma 知道，任何丰度值正好为 0.0 的测量值都是质谱仪产生的伪迹，必须在分析前过滤掉。\n\n她的工作流程按以下单元格结构组织：\n\n**单元格 [1]: 数据加载**\n```python\n##\n## Some code to load data from a file into a pandas DataFrame.\n##\nprotein_data = load_data('experiment_results.csv')\n## After this cell runs, protein_data contains both valid and zero-value abundance measurements.\n```\n\n**单元格 [2]: 数据过滤**\n```python\n##\n## Filter out rows where abundance is 0.0.\n##\nfiltered_data = protein_data[protein_data['abundance'] > 0.0]\n```\n\n**单元格 [3]: 第一个分析图**\n```python\n##\n## Generate a box plot to compare abundances between 'control' and 'drug-treated' groups.\n##\ngenerate_plot(dataframe=filtered_data, title='Distribution of Filtered Protein Abundances')\n```\n\n最初，Dr. Sharma 按顺序运行单元格：单元格 [1]，然后是单元格 [2]，再然后是单元格 [3]。由单元格 [3] 生成的图表正确显示了经过滤的有效蛋白质丰度的分布。\n\n后来，她决定回到笔记本的开头，为她的数据加载单元格添加一条注释。添加注释后，她重新执行了单元格 [1]。然后她向下滚动到笔记本的末尾，并编写了一个新的单元格 [4]，以创建该数据的另一种可视化（小提琴图）。她的新单元格如下：\n\n**单元格 [4]: 新的分析图**\n```python\n##\n## Generate a violin plot, but I'll use the original dataframe to double-check something.\n##\ngenerate_violin_plot(dataframe=protein_data, title='Violin Plot of Protein Abundances')\n```\n她在重新执行单元格 [1] 后立即执行了这个新的单元格 [4]。\n\n根据这个确切的操作序列（初始运行：单元格 [1] → 单元格 [2] → 单元格 [3]；后续运行：单元格 [1] → 单元格 [4]），以下哪个陈述最准确地描述了单元格 [4] 生成的图表中所可视化的数据？\n\nA. 该图表将显示过滤后的数据，因为变量 `filtered_data` 在第一次执行单元格 [2] 后仍存储在笔记本的内存中。\n\nB. 执行单元格 [4] 将导致错误，因为在最近的执行序列中跳过了单元格 [2] 的过滤步骤。\n\nC. 该图表将显示原始的、未经过滤的数据，包括零值伪迹，因为单元格 [4] 明确使用了刚被单元格 [1] 的执行所重新定义的变量 `protein_data`。\n\nD. 该图表将是空的，因为重新运行单元格 [1] 会清除所有先前定义的变量，包括 `protein_data`。\n\nE. 该图表将与单元格 [3] 生成的图表相同，因为 Jupyter Notebook 会自动跟踪并使用数据集的最新过滤版本。", "solution": "- 在初始执行序列（单元格 [1] → 单元格 [2] → 单元格 [3]）中：\n  - 单元格 [1] 将加载的数据集（包含零值伪迹）赋给变量 protein_data。\n  - 单元格 [2] 通过应用布尔掩码 abundance > 0.0 来构建 filtered_data，从而排除了所有零值伪迹。在 pandas 中，这种布尔索引会创建一个新的 DataFrame 对象，其中只包含满足条件的行。\n  - 单元格 [3] 从 filtered_data 生成一个图表，正确地只反映了有效的（非零）丰度。\n\n- 在后续执行序列（单元格 [1] → 单元格 [4]）中：\n  - 重新执行单元格 [1] 会将 protein_data 重新绑定到一个从文件中新加载的 DataFrame，该 DataFrame 再次包含零值伪迹。当单个单元格被重新运行时，Jupyter Notebook 不会清除内核状态；它只是重新为变量 protein_data 赋值。\n  - 早先运行中产生的变量 filtered_data 仍然存在于内存中，但在此处是无关紧要的，因为单元格 [4] 明确地将 dataframe=protein_data 传递给 generate_violin_plot。\n  - 因此，单元格 [4] 操作的是 protein_data 的当前值，该值刚刚被单元格 [1] 重新定义为原始的、未经过滤的数据集。不会发生错误，因为绘图函数接收到一个有效的 DataFrame。也不存在任何隐式链接会导致 Jupyter 替换为 filtered_data 或自动应用之前的过滤器。\n\n结论：单元格 [4] 中的小提琴图将显示未经过滤的蛋白质丰度，包括零值伪迹，因为它使用的是由单元格 [1] 重新加载的 protein_data。", "answer": "$$\\boxed{C}$$", "id": "1463183"}]}