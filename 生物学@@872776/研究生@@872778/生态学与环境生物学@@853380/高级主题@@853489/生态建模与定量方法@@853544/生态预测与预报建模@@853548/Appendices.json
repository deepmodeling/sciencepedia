{"hands_on_practices": [{"introduction": "生态预测建模的一个关键步骤是深入理解模型本身的行为。在本练习中，我们将探索经典的Ricker种群模型，这是理论生态学的基石之一。通过分析其确定性稳定性和对随机扰动的响应，您将实践线性化和稳定性分析的基本技能，这对于解释和使用动态模型进行预测至关重要。[@problem_id:2482834]", "problem": "考虑一个受环境波动影响的单一、封闭种群的离散时间随机 Ricker 模型，\n$$\nN_{t+1} \\;=\\; N_t \\,\\exp\\!\\big(r\\,(1 - N_t/K) + \\epsilon_t\\big),\n$$\n其中 $N_t$ 表示在时间 $t$ 的种群丰度，$r0$ 是内禀增长率，$K0$ 是环境承载力，而 $\\{\\epsilon_t\\}_{t\\in\\mathbb{Z}}$ 是独立同分布的环境冲击，满足 $\\mathbb{E}[\\epsilon_t]=0$ 和 $\\operatorname{Var}(\\epsilon_t)=\\sigma^2$，且 $\\sigma^2$ 很小。假设噪声是高斯的，因此 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n你的任务是：\n- 使用一维离散时间动力系统理论，分析无噪声（$\\sigma^2=0$）映射，以确定正平衡点，并推导其局部渐近稳定性的关于 $r$ 的条件。\n- 在小环境噪声下，引入对数偏差变量 $y_t = \\ln(N_t/K)$，并在平衡点附近将关于 $y_t$ 的随机递推关系对 $y_t$ 和 $\\epsilon_t$ 线性化至主导阶。利用由独立冲击驱动的线性随机差分方程的平稳解的性质，刻画 $y_t$ 的近似平稳分布，并通过变换，得到相应的 $N_t$ 的近似平稳分布。\n- 作为你的最终答案，给出在线性化和稳定性条件有效的条件下，$y_t$ 的平稳方差关于 $r$ 和 $\\sigma^2$ 的闭式解析表达式。无需四舍五入。以无单位形式表示最终答案。\n\n所有的假设和步骤都必须从一维映射稳定性的基本定义和线性随机递推的经过充分检验的性质出发进行论证。不要使用任何未经证明的快捷公式或未经推导就引用结果。", "solution": "所陈述的问题在科学上是合理的、自洽的且适定的。这是理论生态学中关于随机种群模型分析的一个标准练习。我们将进行严格、分步的推导。\n\n我们考虑的模型是随机 Ricker 映射：\n$$\nN_{t+1} = N_t \\exp\\big(r(1 - N_t/K) + \\epsilon_t\\big)\n$$\n其中 $N_t$ 是种群丰度，$r > 0$ 是内禀增长率，$K > 0$ 是环境承载力，而 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是代表环境噪声的独立同分布随机变量，并假设 $\\sigma^2$ 很小。\n\n首先，我们通过将噪声项设为零（$\\sigma^2 = 0$）来分析确定性系统。其动力学由一维映射 $N_{t+1} = f(N_t)$ 控制，其中函数 $f$ 由下式给出：\n$$\nf(N) = N \\exp\\big(r(1 - N/K)\\big)\n$$\n该映射的平衡点（或不动点）$N^*$ 是方程 $N^* = f(N^*)$ 的解。\n$$\nN^* = N^* \\exp\\big(r(1 - N^*/K)\\big)\n$$\n该方程立即给出两个可能的解。\n1. 平凡平衡点：$N_1^* = 0$。\n2. 对于非平凡平衡点（$N^* > 0$），我们可以将方程两边同除以 $N^*$：\n$$\n1 = \\exp\\big(r(1 - N^*/K)\\big)\n$$\n对两边取自然对数，得到：\n$$\n\\ln(1) = 0 = r(1 - N^*/K)\n$$\n因为给定 $r > 0$，这意味着 $1 - N^*/K = 0$，从而得到正平衡点：\n$$\nN_2^* = K\n$$\n这个平衡点代表了环境的承载力。\n\n接下来，我们研究这个正平衡点 $N^* = K$ 的局部渐近稳定性。稳定性由映射的导数 $f'(N)$ 在平衡点处的值的绝对值决定。如果 $|f'(N^*)|  1$，则该平衡点是局部渐近稳定的。我们使用乘法法则计算导数：\n$$\n\\begin{aligned}\nf'(N) = \\frac{d}{dN} \\left[ N \\exp\\big(r(1 - N/K)\\big) \\right] \\\\\n= (1) \\cdot \\exp\\big(r(1 - N/K)\\big) + N \\cdot \\exp\\big(r(1 - N/K)\\big) \\cdot \\left(-\\frac{r}{K}\\right) \\\\\n= \\exp\\big(r(1 - N/K)\\big) \\left(1 - \\frac{rN}{K}\\right)\n\\end{aligned}\n$$\n在平衡点 $N^* = K$ 处计算此导数的值：\n$$\nf'(K) = \\exp\\big(r(1 - K/K)\\big) \\left(1 - \\frac{rK}{K}\\right) = \\exp(0) \\cdot (1 - r) = 1 - r\n$$\n局部渐近稳定性的条件是 $|f'(K)|  1$，这转化为：\n$$\n|1 - r|  1\n$$\n这个不等式等价于复合不等式 $-1  1 - r  1$。\n右边，$1 - r  1$，意味着 $-r  0$，即 $r > 0$，这已是给定条件。\n左边，$-1  1 - r$，意味着 $r  2$。\n因此，在确定性 Ricker 模型中，正平衡点 $N^* = K$ 局部渐近稳定的条件是 $0  r  2$。\n\n现在，我们进行随机分析。我们引入相对环境承载力的对数偏差 $y_t = \\ln(N_t/K)$。这意味着 $N_t = K \\exp(y_t)$。平衡点 $N^*=K$ 对应于 $y^*=\\ln(K/K)=\\ln(1)=0$。将 $N_t = K\\exp(y_t)$ 代入原始随机方程：\n$$\nK\\exp(y_{t+1}) = K\\exp(y_t) \\exp\\big(r(1 - K\\exp(y_t)/K) + \\epsilon_t\\big)\n$$\n两边同除以 $K$，然后取自然对数，得到关于 $y_t$ 的精确递推关系：\n$$\ny_{t+1} = y_t + r\\big(1 - \\exp(y_t)\\big) + \\epsilon_t\n$$\n这是一个非线性随机差分方程。鉴于假设噪声方差 $\\sigma^2$ 很小且确定性系统是稳定的（$0  r  2$），种群 $N_t$ 将在稳定平衡点 $K$ 附近表现出小幅波动。因此，$y_t$ 将在其平衡值 $0$ 附近表现出小幅波动。这为在 $y_t=0$ 附近对 $y_t$ 的递推关系进行线性化提供了依据。我们使用 $\\exp(y_t)$ 在 $y_t=0$ 附近的泰勒展开：\n$$\n\\exp(y_t) = 1 + y_t + O(y_t^2)\n$$\n将此代入 $y_{t+1}$ 的方程，并保留 $y_t$ 和 $\\epsilon_t$ 的主导阶项：\n$$\n\\begin{aligned}\ny_{t+1} \\approx y_t + r\\big(1 - (1+y_t)\\big) + \\epsilon_t \\\\\ny_{t+1} \\approx y_t - ry_t + \\epsilon_t \\\\\ny_{t+1} \\approx (1-r)y_t + \\epsilon_t\n\\end{aligned}\n$$\n这是一个线性随机差分方程，一个1阶自回归过程 (AR(1))。如果自回归项的系数的绝对值小于1，即 $|1-r|  1$，该过程就存在平稳解，这恰好是我们为确定性系统推导出的稳定性条件 $0  r  2$。\n\n假设该过程已达到其平稳状态，那么 $y_t$ 的统计矩不随时间变化。设 $\\mathbb{E}[y_t] = \\mu_y$ 和 $\\operatorname{Var}(y_t) = \\sigma_y^2$。在平稳状态下，$\\mathbb{E}[y_{t+1}] = \\mathbb{E}[y_t] = \\mu_y$。对线性化方程取期望：\n$$\n\\mathbb{E}[y_{t+1}] = \\mathbb{E}[(1-r)y_t + \\epsilon_t] = (1-r)\\mathbb{E}[y_t] + \\mathbb{E}[\\epsilon_t]\n$$\n给定 $\\mathbb{E}[\\epsilon_t]=0$，上式变为 $\\mu_y = (1-r)\\mu_y$。这意味着 $r\\mu_y = 0$。由于 $r>0$，所以 $y_t$ 的平稳均值为 $\\mu_y=0$。\n\n接下来，我们求平稳方差 $\\sigma_y^2$。在平稳状态下，$\\operatorname{Var}(y_{t+1}) = \\operatorname{Var}(y_t) = \\sigma_y^2$。我们对线性化方程取方差：\n$$\n\\operatorname{Var}(y_{t+1}) = \\operatorname{Var}((1-r)y_t + \\epsilon_t)\n$$\n变量 $y_t$ 是过去冲击 $\\{\\epsilon_{t-1}, \\epsilon_{t-2}, \\dots\\}$ 的函数。由于冲击 $\\{\\epsilon_t\\}$ 是独立的，$y_t$ 与当前的冲击 $\\epsilon_t$ 不相关。因此，和的方差等于方差的和：\n$$\n\\operatorname{Var}(y_{t+1}) = \\operatorname{Var}((1-r)y_t) + \\operatorname{Var}(\\epsilon_t) = (1-r)^2\\operatorname{Var}(y_t) + \\operatorname{Var}(\\epsilon_t)\n$$\n代入平稳方差 $\\sigma_y^2$ 和给定的噪声方差 $\\sigma^2$：\n$$\n\\sigma_y^2 = (1-r)^2\\sigma_y^2 + \\sigma^2\n$$\n现在我们求解 $\\sigma_y^2$：\n$$\n\\sigma_y^2 - (1-r)^2\\sigma_y^2 = \\sigma^2\n$$\n$$\n\\sigma_y^2 \\big(1 - (1-r)^2\\big) = \\sigma^2\n$$\n$$\n\\sigma_y^2 \\big(1 - (1 - 2r + r^2)\\big) = \\sigma^2\n$$\n$$\n\\sigma_y^2 (2r - r^2) = \\sigma^2\n$$\n这就给出了 $y_t$ 的平稳方差的表达式：\n$$\n\\sigma_y^2 = \\frac{\\sigma^2}{2r - r^2}\n$$\n此表达式在稳定性条件 $0  r  2$ 下有效，该条件确保了分母为正。\n\n作为最后的刻画，由于 $y_t$ 的过程是高斯变量 $\\epsilon_t$ 的线性变换，其平稳分布也是高斯的。根据计算出的均值和方差，$y_t$ 的近似平稳分布为 $\\mathcal{N}(0, \\frac{\\sigma^2}{2r - r^2})$。因此，由于 $N_t = K\\exp(y_t)$，种群数量 $N_t$ 近似服从对数正态分布，具体为 $N_t \\sim \\text{Log-Normal}(\\ln(K), \\frac{\\sigma^2}{2r-r^2})$。\n\n问题要求的是 $y_t$ 的平稳方差的闭式解析表达式。这正是上面推导出的结果。", "answer": "$$\\boxed{\\frac{\\sigma^2}{2r - r^2}}$$", "id": "2482834"}, {"introduction": "只有当模型与数据结合时，它们才能发挥作用，但这个过程常常揭示出复杂的不确定性。本练习旨在解决“殊途同归”（equifinality）这一常见挑战，即不同的模型参数组合可能产生几乎相同的预测效果，从而难以确定“真实”的内在机制。您将使用剖面似然法和卡尔曼滤波器实现一种强大的计算方法，来诊断状态空间模型中的这一问题，这是进行稳健模型拟合和推断的一项关键技能。[@problem_id:2482790]", "problem": "您的任务是编写一个完整、可运行的程序，该程序使用剖面似然来检测生态预测模型中参数集之间的等效终局性，然后提出额外的数据来解决该问题。生态背景是针对对数丰度时间序列的 Gompertz 种群模型的线性高斯状态空间表述。您的程序必须遵循基于原则的设计，从标准概率法则和关于高斯状态空间模型的经过充分检验的事实出发，并且必须生成具有指定内容和格式的单行输出。\n\n模型和基础：\n- 考虑由下式定义的状态过程\n$$\nz_{t+1} = \\phi z_t + c + \\eta_t,\n$$\n其中 $\\eta_t \\sim \\mathcal{N}(0,\\sigma_p^2)$ 是独立同分布的过程新息，$\\mathcal{N}$ 表示具有给定方差的高斯分布。观测过程为\n$$\ny_t = z_t + \\epsilon_t,\n$$\n其中 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_o^2)$ 是独立同分布的测量误差，且与 $\\eta_t$ 独立。参数向量为 $\\theta = (\\phi,c,\\sigma_p,\\sigma_o)$。\n\n- 对于给定的时间序列 $\\{y_t\\}_{t=1}^T$，在此线性高斯状态空间模型下，单步预测分布是高斯分布。通过迭代应用高斯共轭性所蕴含的 Kalman 滤波器预测-更新递归，可以计算出精确的对数似然：\n  1. 预测步骤（状态）：从后验 $(m_t, P_t)$，通过以下方式得到先验 $(m_{t+1|t}, P_{t+1|t})$\n  $$\n  m_{t+1|t} = \\phi m_t + c,\\quad P_{t+1|t} = \\phi^2 P_t + \\sigma_p^2.\n  $$\n  2. 对 $y_{t+1}$ 的观测预测：\n  $$\n  \\hat{y}_{t+1} = m_{t+1|t},\\quad S_{t+1} = P_{t+1|t} + \\sigma_o^2,\\quad v_{t+1} = y_{t+1} - \\hat{y}_{t+1}.\n  $$\n  3. 更新步骤（状态）：使用 Kalman 增益 $K_{t+1} = P_{t+1|t} / S_{t+1}$，\n  $$\n  m_{t+1} = m_{t+1|t} + K_{t+1} v_{t+1},\\quad P_{t+1} = P_{t+1|t} - K_{t+1}^2 S_{t+1}.\n  $$\n  $y_{t+1}$ 的单步预测对数似然贡献为\n  $$\n  \\ell_{t+1} = -\\tfrac{1}{2}\\left(\\log(2\\pi) + \\log S_{t+1} + \\frac{v_{t+1}^2}{S_{t+1}}\\right).\n  $$\n  通过初始化 $m_1 = y_1$ 和一个大方差（例如，$P_1 = 10^6$）来为状态使用弥散先验，然后对 $t = 1,\\dots,T-1$ 应用上述步骤，以求和得到 $\\ell = \\sum_{t=1}^{T-1} \\ell_{t+1}$。\n\n- 预测技能度量：将 $y_{t+1}$ 的点预测定义为 $\\hat{y}_{t+1}=m_{t+1|t}$。将单步预测均方根误差 (RMSE) 定义为\n$$\n\\mathrm{RMSE}(\\theta) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\left(y_{t+1} - \\hat{y}_{t+1}\\right)^2}.\n$$\n\n剖面似然与等效终局性：\n- 对于一个焦点标量参数 $\\psi \\in \\{\\phi, c, \\sigma_p, \\sigma_o\\}$，其在网格 $\\mathcal{G}_\\psi$ 上的剖面对数似然为\n$$\n\\ell_p(\\psi) = \\max_{\\theta \\setminus \\{\\psi\\}} \\ \\ell(\\theta),\n$$\n其中 $\\ell(\\theta)$ 是 Kalman 滤波器的对数似然。类似地，将每个固定 $\\psi$ 下可达到的最小预测误差定义为\n$$\n\\mathrm{RMSE}_p(\\psi) = \\min_{\\theta \\setminus \\{\\psi\\}} \\ \\mathrm{RMSE}(\\theta).\n$$\n- 按如下方式检测等效终局性。对每个 $\\psi$ 计算：\n  - 集合 $\\mathcal{A}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\ell_p(\\psi) \\ge \\ell_{\\max} - \\Delta\\}$，其中 $\\ell_{\\max} = \\max_{\\psi \\in \\mathcal{G}_\\psi} \\ell_p(\\psi)$ 且 $\\Delta = \\tfrac{1}{2}\\chi^2_{1,0.95}$，$\\chi^2_{1,0.95}$ 是自由度为 1 的卡方分布的 $0.95$ 分位数。\n  - 集合 $\\mathcal{B}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\mathrm{RMSE}_p(\\psi) \\le (1+\\varepsilon)\\min_{\\psi \\in \\mathcal{G}_\\psi} \\mathrm{RMSE}_p(\\psi)\\}$，其中 $\\varepsilon = 0.05$。\n  - 交集 $\\mathcal{I}_\\psi = \\mathcal{A}_\\psi \\cap \\mathcal{B}_\\psi$。定义离散宽度比\n  $$\n  w_\\psi = \\frac{|\\mathcal{I}_\\psi|}{|\\mathcal{G}_\\psi|}.\n  $$\n  若 $\\max_{\\psi} w_\\psi \\ge \\tau$ (其中 $\\tau = 0.3$)，则宣告存在等效终局性。指示最强等效终局性的参数是达到最大 $w_\\psi$ 的那个 $\\psi$ (若有平局，则选择下面定义的最小索引)。\n- 为量化所检测到的 $\\psi^\\star$ 的等效终局参数值之间的预测技能相似性，计算技能差距\n$$\ng = \\frac{\\max_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi) - \\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}{\\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}.\n$$\n若未检测到等效终局性，则设 $g = 0$。\n\n程序要求：\n- 实现上文指定的基于 Kalman 滤波器的精确对数似然和单步预测 RMSE。\n- 为保证可复现性，使用以下固定网格：\n  - 剖面网格：\n    - $\\phi$: $\\mathcal{G}_\\phi = \\{\\text{在 } 0.2 \\text{ 到 } 1.2 \\text{ 之间取 } 21 \\text{ 个点的 linspace}\\}$。\n    - $c$: $\\mathcal{G}_c = \\{\\text{在 } -0.1 \\text{ 到 } 0.5 \\text{ 之间取 } 21 \\text{ 个点的 linspace}\\}$。\n    - $\\sigma_p$: $\\mathcal{G}_{\\sigma_p} = \\{\\text{在 } 0.03 \\text{ 到 } 0.6 \\text{ 之间取 } 21 \\text{ 个点的 logspace}\\}$。\n    - $\\sigma_o$: $\\mathcal{G}_{\\sigma_o} = \\{\\text{在 } 0.03 \\text{ 到 } 0.6 \\text{ 之间取 } 21 \\text{ 个点的 logspace}\\}$。\n  - 用于剖面分析的无关参数网格：\n    - $\\phi$: $\\{\\text{在 } 0.0 \\text{ 到 } 1.2 \\text{ 之间取 } 11 \\text{ 个点的 linspace}\\}$。\n    - $c$: $\\{\\text{在 } -0.3 \\text{ 到 } 0.7 \\text{ 之间取 } 11 \\text{ 个点的 linspace}\\}$。\n    - $\\sigma_p$: $\\{\\text{在 } 0.01 \\text{ 到 } 1.0 \\text{ 之间取 } 11 \\text{ 个点的 logspace}\\}$。\n    - $\\sigma_o$: $\\{\\text{在 } 0.01 \\text{ 到 } 1.0 \\text{ 之间取 } 11 \\text{ 个点的 logspace}\\}$。\n  此处，“linspace” 指在线性尺度上均匀间隔取点，“logspace” 指在对数尺度上对严格为正的边界均匀间隔取点。\n\n测试套件：\n- 精确使用以下三个时间序列（无量纲的对数丰度），每个序列都是按给定顺序列出的实数列表。\n  - 案例 1 (长度 16):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.1,\\,\n    0.285,\\,\n    0.44225,\\,\n    0.5769125,\\,\n    0.690375625,\\,\n    0.78681928125,\\,\n    0.8687963890625,\\,\n    0.938477930703125,\\,\n    0.9977062410976562,\\,\n    1.0480503049320078,\\,\n    1.0908427591922066,\\,\n    1.1272163453133756,\\,\n    1.1581338935163692,\\,\n    1.1844138094889148,\\,\n    1.2067517380655776,\\,\n    1.22573997735574\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n  - 案例 2 (长度 8):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.22,\\,\n    0.28,\\,\n    0.29,\\,\n    0.35,\\,\n    0.34,\\,\n    0.42,\\,\n    0.40,\\,\n    0.47\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n  - 案例 3 (长度 16):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.52,\\,\n    0.49,\\,\n    0.51,\\,\n    0.5,\\,\n    0.53,\\,\n    0.48,\\,\n    0.5,\\,\n    0.51,\\,\n    0.49,\\,\n    0.5,\\,\n    0.52,\\,\n    0.47,\\,\n    0.5,\\,\n    0.51,\\,\n    0.5,\\,\n    0.49\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n\n索引、决策阈值和建议代码：\n- 参数索引必须按如下方式编码为整数：$\\phi \\rightarrow 0$，$c \\rightarrow 1$，$\\sigma_p \\rightarrow 2$，$\\sigma_o \\rightarrow 3$。\n- 等效终局性决策阈值必须精确地为上文定义的 $\\Delta = \\tfrac{1}{2}\\chi^2_{1,0.95}$，$\\varepsilon = 0.05$ 和 $\\tau = 0.3$。\n- 如果检测到宽度比 $w_\\psi$ 最大的参数存在等效终局性，则使用以下整数代码提出一种单一的额外数据类型以最直接地解决该问题：\n  - $0$：无需额外数据（仅在未检测到等效终局性时使用）。\n  - $1$：提高时间分辨率或时间序列长度（更多时间点）。\n  - $2$：在每个时间点重复观测，以更好地估计 $\\sigma_o$。\n  - $3$：复制过程单元（并行种群），以更好地估计 $\\sigma_p$。\n  - $4$：测量影响截距 $c$ 的环境协变量。\n  映射规则：如果检测到的参数是 $\\phi$，输出 $1$；如果是 $c$，输出 $4$；如果是 $\\sigma_o$，输出 $2$；如果是 $\\sigma_p$，输出 $3$。\n\n输出规范：\n- 对于三个案例中的每一个，计算：\n  - 一个整数等效终局性标志 $f \\in \\{0,1\\}$（$1$ 表示检测到等效终局性），\n  - 具有最大 $w_\\psi$ 的参数的整数索引 $p$（如果 $f=0$，则使用 $-1$），\n  - 整数建议代码 $d$（如果 $f=0$，则使用 $0$），\n  - 如上定义的技能差距浮点数 $g$（如果 $f=0$，则使用 $0.0$）。\n- 您的程序应生成单行输出，其中包含一个由三个案例结果组成的逗号分隔列表，每个结果本身都是 $[f,p,d,g]$ 形式的列表。例如，一个语法正确的输出是\n$$\n[[1,0,1,0.0375],[0,-1,0,0.0],[1,3,2,0.0123]]\n$$\n其中的实际数值由您在所提供的测试套件上的实现确定。不得打印任何额外文本。", "solution": "所提出的问题是开发并实现一个计算流程，用于检测和表征生态预测模型中的等效终局性。该模型是 Gompertz 种群动态的线性高斯状态空间表示。该流程必须基于剖面似然原理。如果检测到等效终局性，则必须根据预定义的启发式方法提供收集额外数据的建议。问题陈述的有效性得到确认；它有科学依据、适定、客观，并为唯一解提供了所有必要信息。我们现在从第一性原理出发，推导所需的算法。\n\n该系统由对数丰度 $z_t$ 的状态过程和测量值 $y_t$ 的观测过程描述。\n状态过程：\n$$\nz_{t+1} = \\phi z_t + c + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_p^2)\n$$\n观测过程：\n$$\ny_t = z_t + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_o^2)\n$$\n完整的参数向量是 $\\theta = (\\phi, c, \\sigma_p, \\sigma_o)$。对于给定的观测时间序列 $\\{y_t\\}_{t=1}^T$，我们的首要任务是计算对数似然函数 $\\ell(\\theta) = \\log P(\\{y_t\\}_{t=1}^T | \\theta)$ 和单步预测误差。\n\n**1. 通过 Kalman 滤波器计算对数似然和预测误差**\n\n对于线性高斯状态空间模型，精确的对数似然可通过预测误差分解来计算，其中递归地寻找条件分布的算法是 Kalman 滤波器。状态 $z_t$ 在给定截至时间 $t$ 的观测值 $Y_t = \\{y_1, \\dots, y_t\\}$ 条件下的分布是高斯分布：$P(z_t | Y_t, \\theta) = \\mathcal{N}(z_t | m_t, P_t)$。对于每个时间点 $t = 1, \\dots, T-1$，滤波器分两步进行。\n\n首先是 **预测步骤**：将状态分布在时间上向前投影。时间 $t$ 的后验 $\\mathcal{N}(m_t, P_t)$ 通过状态方程传播，得到时间 $t+1$ 的先验：\n$$\nP(z_{t+1} | Y_t, \\theta) = \\mathcal{N}(z_{t+1} | m_{t+1|t}, P_{t+1|t})\n$$\n其中矩为：\n$$\nm_{t+1|t} = \\phi m_t + c\n$$\n$$\nP_{t+1|t} = \\phi^2 P_t + \\sigma_p^2\n$$\n状态 $z_{t+1}$ 上的这个先验会导出下一个观测值 $y_{t+1}$ 的预测分布：\n$$\nP(y_{t+1} | Y_t, \\theta) = \\mathcal{N}(y_{t+1} | \\hat{y}_{t+1}, S_{t+1})\n$$\n其均值为 $\\hat{y}_{t+1} = m_{t+1|t}$，方差为 $S_{t+1} = P_{t+1|t} + \\sigma_o^2$。项 $v_{t+1} = y_{t+1} - \\hat{y}_{t+1}$ 是单步预测误差或新息。\n\n其次是 **更新步骤**：使用新的观测值 $y_{t+1}$ 通过贝叶斯法则更新状态分布，从而得到时间 $t+1$ 的后验：\n$$\nP(z_{t+1} | Y_{t+1}, \\theta) = \\mathcal{N}(z_{t+1} | m_{t+1}, P_{t+1})\n$$\n更新后的矩由以下公式给出：\n$$\nK_{t+1} = P_{t+1|t} / S_{t+1} \\quad (\\text{Kalman 增益})\n$$\n$$\nm_{t+1} = m_{t+1|t} + K_{t+1} v_{t+1}\n$$\n$$\nP_{t+1} = P_{t+1|t} - K_{t+1}^2 S_{t+1}\n$$\n总对数似然是单步预测的对数似然之和：\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T-1} \\log P(y_{t+1} | Y_t, \\theta) = \\sum_{t=1}^{T-1} \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log S_{t+1} - \\frac{1}{2}\\frac{v_{t+1}^2}{S_{t+1}} \\right)\n$$\n递归以状态 $z_1$ 的弥散先验进行初始化。根据规范，我们设置 $m_1 = y_1$ 和 $P_1 = 10^6$。单步预测均方根误差 (RMSE) 是根据新息计算的：\n$$\n\\mathrm{RMSE}(\\theta) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} v_{t+1}^2}\n$$\n\n**2. 剖面似然和剖面 RMSE**\n\n等效终局性是一种多个不同参数集对数据产生相似良好拟合的状况，它使用剖面似然进行分析。对于一个焦点参数 $\\psi$（可以是 $\\phi, c, \\sigma_p, \\sigma_o$ 中的任意一个），剖面对数似然 $\\ell_p(\\psi)$ 是通过改变所有其他（无关）参数可实现的最大对数似然。\n$$\n\\ell_p(\\psi) = \\max_{\\theta \\setminus \\{\\psi\\}} \\ell(\\theta)\n$$\n类似地，剖面 RMSE 衡量固定 $\\psi$ 下的最小预测误差：\n$$\n\\mathrm{RMSE}_p(\\psi) = \\min_{\\theta \\setminus \\{\\psi\\}} \\mathrm{RMSE}(\\theta)\n$$\n优化是通过在指定的无关参数网格上进行网格搜索来执行的。对于焦点参数网格 $\\mathcal{G}_\\psi$ 中的每个点 $\\psi_j$，我们对无关参数各自网格中的所有值组合，评估 $\\ell(\\theta)$ 和 $\\mathrm{RMSE}(\\theta)$，并保留最大的 $\\ell$ 和最小的 RMSE。\n\n**3. 等效终局性检测**\n\n通过检查剖面似然和剖面 RMSE 曲面的平坦度来检测等效终局性。\n$\\psi$ 的一个基于似然的置信集定义为：\n$$\n\\mathcal{A}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\ell_p(\\psi) \\ge \\ell_{\\max} - \\Delta\\}\n$$\n其中 $\\ell_{\\max} = \\max_{\\psi \\in \\mathcal{G}_\\psi} \\ell_p(\\psi)$ 且 $\\Delta = \\frac{1}{2}\\chi^2_{1,0.95}$ 是基于 Wilks 定理的 $95\\%$ 置信区间的阈值。\n一个基于预测技能的合理参数集定义为：\n$$\n\\mathcal{B}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\mathrm{RMSE}_p(\\psi) \\le (1+\\varepsilon)\\min_{\\psi \\in \\mathcal{G}_\\psi} \\mathrm{RMSE}_p(\\psi)\\}\n$$\n其相对误差容忍度为 $\\varepsilon = 0.05$。\n这些集合的交集 $\\mathcal{I}_\\psi = \\mathcal{A}_\\psi \\cap \\mathcal{B}_\\psi$ 包含了既在统计上合理又能产生近乎最优预测的参数值。参数 $\\psi$ 的等效终局性程度由该集合的相对大小来量化：\n$$\nw_\\psi = \\frac{|\\mathcal{I}_\\psi|}{|\\mathcal{G}_\\psi|}\n$$\n如果所有参数中的最大宽度比超过阈值 $\\tau=0.3$，即 $\\max_\\psi w_\\psi \\ge 0.3$，则宣告存在等效终局性。最大化 $w_\\psi$ 的参数 $\\psi^\\star$ 被认为是受等效终局性影响最大的参数。\n\n**4. 技能差距与数据建议**\n\n如果检测到参数 $\\psi^\\star$ 存在等效终局性，我们通过技能差距 $g$ 来量化等效终局参数值之间的预测技能变化：\n$$\ng = \\frac{\\max_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi) - \\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}{\\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE_p}(\\psi)}\n$$\n这量化了等效终局集内表现最佳和最差的参数之间预测误差的相对差异。如果未检测到等效终局性，则将 $g$ 设置为 $0$。\n\n最后，根据哪个参数 $\\psi^\\star$ 表现出最强的等效终局性，来提出新数据收集的建议。该映射基于模型中每个参数的角色：\n- $\\phi$ 控制时间动态：通过更多时间点解决（建议代码 $1$）。\n- $c$ 是截距/均值回归水平：通过测量环境驱动因素解决（建议代码 $4$）。\n- $\\sigma_p$ 是过程方差：通过复制过程单元（例如，并行种群）解决（建议代码 $3$）。\n- $\\sigma_o$ 是观测方差：通过在每个时间点重复观测解决（建议代码 $2$）。\n\n如果未检测到等效终局性，则不建议新数据（代码 $0$）。记录等效终局性最强的参数的索引，如果未检测到，则设为 $-1$。这完成了算法的形式化规约。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to perform equifinality analysis for all test cases.\n    \"\"\"\n    \n    # --- Define problem constants, thresholds, and grids ---\n    \n    # Test cases\n    test_cases = [\n        [0.1, 0.285, 0.44225, 0.5769125, 0.690375625, 0.78681928125,\n         0.8687963890625, 0.938477930703125, 0.9977062410976562, 1.0480503049320078,\n         1.0908427591922066, 1.1272163453133756, 1.1581338935163692, 1.1844138094889148,\n         1.2067517380655776, 1.22573997735574],\n        [0.22, 0.28, 0.29, 0.35, 0.34, 0.42, 0.40, 0.47],\n        [0.52, 0.49, 0.51, 0.5, 0.53, 0.48, 0.5, 0.51, 0.49, 0.5,\n         0.52, 0.47, 0.5, 0.51, 0.5, 0.49]\n    ]\n\n    # Parameter names and indices\n    PARAM_NAMES = ['phi', 'c', 'sigma_p', 'sigma_o']\n    PARAM_INDICES = {name: i for i, name in enumerate(PARAM_NAMES)}\n\n    # Parameter grids for profiling\n    PROFILE_GRIDS = {\n        'phi': np.linspace(0.2, 1.2, 21),\n        'c': np.linspace(-0.1, 0.5, 21),\n        'sigma_p': np.logspace(np.log10(0.03), np.log10(0.6), 21),\n        'sigma_o': np.logspace(np.log10(0.03), np.log10(0.6), 21)\n    }\n\n    # Nuisance parameter grids for optimization\n    NUISANCE_GRIDS = {\n        'phi': np.linspace(0.0, 1.2, 11),\n        'c': np.linspace(-0.3, 0.7, 11),\n        'sigma_p': np.logspace(np.log10(0.01), np.log10(1.0), 11),\n        'sigma_o': np.logspace(np.log10(0.01), np.log10(1.0), 11)\n    }\n    \n    # Decision thresholds and constants\n    DELTA = 0.5 * chi2.ppf(0.95, df=1)\n    EPSILON = 0.05\n    TAU = 0.3\n    \n    # Proposal code mapping: param_index - data_code\n    PROPOSAL_MAP = {0: 1, 1: 4, 2: 3, 3: 2}\n\n    # --- Core functions ---\n\n    def kalman_filter_loglik_rmse(theta, y):\n        \"\"\"\n        Computes log-likelihood and RMSE using the Kalman filter.\n        theta: tuple (phi, c, sigma_p, sigma_o)\n        y: numpy array of time series data\n        \"\"\"\n        phi, c, sigma_p, sigma_o = theta\n        T = len(y)\n        \n        m_t = y[0]\n        P_t = 1e6\n        \n        log_likelihood_sum = 0.0\n        squared_error_sum = 0.0\n        \n        for t in range(1, T):\n            # Prediction\n            m_pred = phi * m_t + c\n            P_pred = phi**2 * P_t + sigma_p**2\n            \n            # Observation prediction\n            y_hat = m_pred\n            v = y[t] - y_hat\n            S = P_pred + sigma_o**2\n            \n            if S = 0: # Numerical stability\n                return -np.inf, np.inf\n\n            # Log-likelihood contribution\n            log_likelihood_sum += -0.5 * (np.log(2 * np.pi) + np.log(S) + v**2 / S)\n            squared_error_sum += v**2\n\n            # Update\n            K = P_pred / S\n            m_t = m_pred + K * v\n            P_t = P_pred - K**2 * S\n\n        rmse = np.sqrt(squared_error_sum / (T - 1))\n        return log_likelihood_sum, rmse\n\n    # --- Main processing loop ---\n    \n    final_results = []\n    \n    for y_case in test_cases:\n        y_data = np.array(y_case)\n        w_values = []\n        all_profiles = {}\n\n        # Profile each of the 4 parameters\n        for focal_param_idx, focal_param_name in enumerate(PARAM_NAMES):\n            profile_loglik = []\n            profile_rmse = []\n            \n            nuisance_param_names = [p for p in PARAM_NAMES if p != focal_param_name]\n            nuisance_grids = [NUISANCE_GRIDS[name] for name in nuisance_param_names]\n\n            for focal_val in PROFILE_GRIDS[focal_param_name]:\n                max_loglik = -np.inf\n                min_rmse = np.inf\n                \n                # Grid search over nuisance parameters\n                for nuisance_vals in itertools.product(*nuisance_grids):\n                    params = {focal_param_name: focal_val}\n                    for i, name in enumerate(nuisance_param_names):\n                        params[name] = nuisance_vals[i]\n                    \n                    theta = (params['phi'], params['c'], params['sigma_p'], params['sigma_o'])\n                    \n                    loglik, rmse = kalman_filter_loglik_rmse(theta, y_data)\n                    \n                    max_loglik = max(max_loglik, loglik)\n                    min_rmse = min(min_rmse, rmse)\n                \n                profile_loglik.append(max_loglik)\n                profile_rmse.append(min_rmse)\n\n            profile_loglik = np.array(profile_loglik)\n            profile_rmse = np.array(profile_rmse)\n            all_profiles[focal_param_name] = {'loglik': profile_loglik, 'rmse': profile_rmse}\n\n            # Calculate width ratio w_psi\n            l_max = np.max(profile_loglik)\n            rmse_min = np.min(profile_rmse)\n\n            # Check for invalid profiles (e.g., all -inf)\n            if np.isinf(l_max) or np.isinf(rmse_min):\n                w = 0.0\n            else:\n                A_indices = np.where(profile_loglik = l_max - DELTA)[0]\n                B_indices = np.where(profile_rmse = (1.0 + EPSILON) * rmse_min)[0]\n                I_indices = np.intersect1d(A_indices, B_indices)\n                w = len(I_indices) / len(PROFILE_GRIDS[focal_param_name])\n            \n            w_values.append(w)\n\n        # Equifinality decision logic\n        max_w = np.max(w_values)\n        \n        if max_w = TAU:\n            f = 1\n            p = np.argmax(w_values)\n            d = PROPOSAL_MAP[p]\n            \n            # Calculate skill gap g\n            winning_param_name = PARAM_NAMES[p]\n            profile_for_g = all_profiles[winning_param_name]\n            \n            l_max_g = np.max(profile_for_g['loglik'])\n            rmse_min_g = np.min(profile_for_g['rmse'])\n            \n            A_indices_g = np.where(profile_for_g['loglik'] = l_max_g - DELTA)[0]\n            B_indices_g = np.where(profile_for_g['rmse'] = (1.0 + EPSILON) * rmse_min_g)[0]\n            I_indices_g = np.intersect1d(A_indices_g, B_indices_g)\n            \n            rmses_in_I = profile_for_g['rmse'][I_indices_g]\n            min_rmse_in_I = np.min(rmses_in_I)\n            max_rmse_in_I = np.max(rmses_in_I)\n            \n            g = (max_rmse_in_I - min_rmse_in_I) / min_rmse_in_I if min_rmse_in_I  0 else 0.0\n\n        else:\n            f = 0\n            p = -1\n            d = 0\n            g = 0.0\n            \n        final_results.append([f, p, d, g])\n\n    # Format and print the final output\n    case_strings = [f\"[{res[0]},{res[1]},{res[2]},{res[3]:.4g}]\" for res in final_results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "2482790"}, {"introduction": "真正有效的预测不仅仅是做出预测，更在于指导我们如何改进预测。这个高级练习将您置于决策者的角色，任务是在固定预算下优化监测网络。通过应用贝叶斯实验设计的原理，您将确定如何分配采样精力以最有效地减少未来的预测不确定性，这展示了预测模型如何为战略性资源管理提供定量基础。[@problem_id:2482812]", "problem": "您的任务是设计一个程序，通过在观测站点间分配固定的监测精力，来增强一个生态观测网络，从而最小化生态状态某个特定线性泛函的后验预测方差。其数学设定如下。未知的生态状态是一个向量 $\\theta \\in \\mathbb{R}^d$，服从高斯先验 $\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$，其中 $\\mu_0 \\in \\mathbb{R}^d$，$\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$ 是对称正定矩阵。您可以假设 $\\mu_0$ 已知，但它不影响方差计算。现有 $m$ 个候选观测站点。如果在站点 $i \\in \\{1,\\dots,m\\}$ 上投入非负精力 $e_i \\ge 0$ 进行采样，将获得一个标量观测，其模型为 $y_i = h_i^\\top \\theta + \\varepsilon_i$，其中 $h_i \\in \\mathbb{R}^d$ 是已知的，观测误差 $\\varepsilon_i$ 相互独立且 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2 / e_i)$。这遵循了抽样理论中常见的方差-精力关系，即方差与精力成反比。所有精力必须满足一个固定的总预算 $\\sum_{i=1}^m e_i = B$，其中 $B \\ge 0$ 是给定的。预测目标是一个线性泛函 $\\psi = g^\\top \\theta$，其中 $g \\in \\mathbb{R}^d$ 是已知的。目标是选择精力向量 $e = (e_1,\\dots,e_m)$，以在观测任何数据之前最小化 $\\psi$ 的期望后验预测方差。期望信息增益可以通过高斯模型下关于 $\\psi$ 的不确定性减少来量化，在线性高斯设定中，期望后验方差仅取决于设计 $e$，而与尚未观测到的数据无关。\n\n仅从高斯条件化法则、观测误差的独立性以及上述指定的 $\\varepsilon_i$ 的方差-精力关系出发，推导期望后验预测方差 $\\psi$ 作为 $e$ 的函数的优化目标，并建立一个计算方法，以在 $\\sum_i e_i = B$ 和 $e_i \\ge 0$ 的约束下找到最优的 $e$。您的程序应使用带梯度的、有原则的连续优化方法，对下面的每个测试案例进行数值求解此分配问题，然后报告优化后的精力分配以及最小化的 $\\psi$ 的期望后验预测方差。所有算术和线性代数运算必须在实数上进行。\n\n测试套件。对于每个案例 $k$，给定维度 $d_k$、站点数 $m_k$、先验协方差 $\\Sigma_{0,k}$、站点向量 $h_{i,k}$、噪声尺度参数 $\\sigma_{i,k}^2$、目标向量 $g_k$ 和预算 $B_k$。在每个案例中，假设 $\\mu_{0,k}$ 是任意的，且不影响方差计算。\n\n- 案例 $1$ (一般情况，多方向信息丰富):\n  - $d_1 = 3$, $m_1 = 3$.\n  - $\\Sigma_{0,1} = \\begin{bmatrix} 1.0  0.2  0.1 \\\\ 0.2  0.5  0.1 \\\\ 0.1  0.1  1.5 \\end{bmatrix}$。\n  - $h_{1,1}^\\top = [1.0, 0.2, 0.0]$, $h_{2,1}^\\top = [0.3, 1.0, 0.5]$, $h_{3,1}^\\top = [0.0, 0.2, 1.0]$。\n  - $\\sigma_{1,1}^2 = 2.0$, $\\sigma_{2,1}^2 = 1.5$, $\\sigma_{3,1}^2 = 3.0$。\n  - $g_1^\\top = [0.5, 0.3, 0.2]$。\n  - $B_1 = 4.0$。\n\n- 案例 $2$ (边界情况：零预算):\n  - 与案例 $1$ 相同，除了 $B_2 = 0.0$。\n\n- 案例 $3$ (混合信息几何，站点数多于状态维度):\n  - $d_3 = 2$, $m_3 = 4$。\n  - $\\Sigma_{0,3} = \\begin{bmatrix} 1.0  0.3 \\\\ 0.3  1.2 \\end{bmatrix}$。\n  - $h_{1,3}^\\top = [1.0, 0.0]$, $h_{2,3}^\\top = [0.0, 1.0]$, $h_{3,3}^\\top = [0.7, 0.7]$, $h_{4,3}^\\top = [1.0, -0.2]$。\n  - $\\sigma_{1,3}^2 = 0.6$, $\\sigma_{2,3}^2 = 2.0$, $\\sigma_{3,3}^2 = 1.5$, $\\sigma_{4,3}^2 = 0.8$。\n  - $g_3^\\top = [1.0, 0.5]$。\n  - $B_3 = 3.0$。\n\n您的程序必须对每个案例 $k \\in \\{1,2,3\\}$ 计算一个最优分配 $e^\\star_k$ 和 $\\psi_k = g_k^\\top \\theta_k$ 的最小化期望后验预测方差 $v^\\star_k$。然后生成单行输出，包含一个长度为 $3$ 的列表，其中第 $k$ 个元素本身是一个列表，由 $e^\\star_k$ 的 $m_k$ 个元素后跟 $v^\\star_k$ 组成。将每个精力 $e^\\star_{i,k}$ 四舍五入到恰好 $4$ 位小数，每个最小化方差 $v^\\star_k$ 四舍五入到恰好 $6$ 位小数。最终输出必须是 $[\\,[e^\\star_{1,1},\\dots,e^\\star_{m_1,1},v^\\star_1],\\,[e^\\star_{1,2},\\dots,e^\\star_{m_2,2},v^\\star_2],\\,[e^\\star_{1,3},\\dots,e^\\star_{m_3,3},v^\\star_3]\\,]$ 形式的单行文本，逗号和括号需完全符合所示格式，且不插入空格。\n\n角度单位不适用。没有需要报告的物理单位。所有输出必须是实数。程序不得读取任何输入，必须运行至完成并仅打印指定的单行输出。", "solution": "所呈现的问题是贝叶斯最优实验设计中一个适定且有科学依据的练习。它是 c-最优性的一个具体实例，其目标是最小化未知状态参数的单个线性组合的后验方差。该问题是有效的，可以使用贝叶斯统计和凸优化的既定原则来解决。我们将着手推导解决方案。\n\n该问题在线性高斯框架内定义。未知状态是一个 $d$ 维向量 $\\theta \\in \\mathbb{R}^d$，其先验分布为高斯分布：\n$$\n\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n$$\n其中 $\\mu_0$ 是先验均值，$\\Sigma_0$ 是 $d \\times d$ 的对称正定先验协方差矩阵。\n\n在 $m$ 个站点进行观测。对于每个站点 $i \\in \\{1, \\dots, m\\}$，观测值 $y_i$ 是状态 $\\theta$ 的线性函数，并被独立的 高斯噪声所污染：\n$$\ny_i = h_i^\\top \\theta + \\varepsilon_i\n$$\n观测向量 $h_i \\in \\mathbb{R}^d$ 是已知的。噪声项 $\\varepsilon_i$ 是独立的，且服从正态分布 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2 / e_i)$，其中 $e_i \\ge 0$ 是站点 $i$ 的采样精力，$\\sigma_i^2$ 是一个已知的比例因子。总精力受预算 $B$ 的限制，即 $\\sum_{i=1}^m e_i = B$。\n\n我们可以将整套观测值 $y = (y_1, \\dots, y_m)^\\top$ 表示为一个单一的向量方程：\n$$\ny = H\\theta + \\varepsilon\n$$\n其中 $H$ 是一个 $m \\times d$ 的设计矩阵，其行是 $h_i^\\top$，而 $\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_m)^\\top$ 是噪声项向量。由于 $\\varepsilon_i$ 的独立性，$\\varepsilon$ 的协方差矩阵是对角矩阵：\n$$\n\\Sigma_\\varepsilon(e) = \\text{Cov}(\\varepsilon) = \\text{diag}\\left(\\frac{\\sigma_1^2}{e_1}, \\frac{\\sigma_2^2}{e_2}, \\dots, \\frac{\\sigma_m^2}{e_m}\\right)\n$$\n因此，给定状态 $\\theta$ 的数据 $y$ 的似然是 $y|\\theta \\sim \\mathcal{N}(H\\theta, \\Sigma_\\varepsilon(e))$。\n\n我们关心的是在整合了观测信息后 $\\theta$ 的后验分布。根据适用于线性高斯模型的贝叶斯定理，后验分布 $p(\\theta|y)$ 也是高斯分布，即 $\\theta|y \\sim \\mathcal{N}(\\mu_e, \\Sigma_e)$。后验协方差矩阵 $\\Sigma_e$ 由后验精度矩阵 $\\Lambda_e$ 的逆给出，后者是先验精度 $\\Lambda_0 = \\Sigma_0^{-1}$ 与从数据中获得的精度之和，该精度由似然的费雪信息矩阵 $H^\\top\\Sigma_\\varepsilon(e)^{-1}H$ 表示。\n$$\n\\Lambda_e = \\Sigma_e^{-1} = \\Sigma_0^{-1} + H^\\top\\Sigma_\\varepsilon(e)^{-1}H\n$$\n对角噪声协方差矩阵的逆是：\n$$\n\\Sigma_\\varepsilon(e)^{-1} = \\text{diag}\\left(\\frac{e_1}{\\sigma_1^2}, \\frac{e_2}{\\sigma_2^2}, \\dots, \\frac{e_m}{\\sigma_m^2}\\right)\n$$\n将此代入数据精度的表达式中得到：\n$$\nH^\\top\\Sigma_\\varepsilon(e)^{-1}H = \\sum_{i=1}^m h_i \\left(\\frac{e_i}{\\sigma_i^2}\\right) h_i^\\top = \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top\n$$\n这里，$h_i h_i^\\top$ 是一个 $d \\times d$ 的秩一矩阵（一个外积）。因此，$\\theta$ 的后验协方差是：\n$$\n\\Sigma_e = \\left( \\Sigma_0^{-1} + \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top \\right)^{-1}\n$$\n该模型的一个关键特征是，后验协方差 $\\Sigma_e$ 取决于实验设计（精力分配 $e$），但并不取决于将要观测到的具体数据值 $y$。因此，“期望后验预测方差”就是用这个依赖于设计的协方差矩阵计算出的后验方差。\n\n目标是最小化线性泛函 $\\psi = g^\\top\\theta$ 的后验预测方差。对于给定的 $\\theta$ 后验分布，$\\psi$ 的方差是：\n$$\n\\text{Var}(\\psi|\\text{data}) = g^\\top \\text{Var}(\\theta|\\text{data}) g = g^\\top \\Sigma_e g\n$$\n这给了我们待最小化的目标函数 $V(e)$：\n$$\nV(e) = g^\\top \\left( \\Sigma_0^{-1} + \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top \\right)^{-1} g\n$$\n优化问题是找到精力向量 $e = (e_1, \\dots, e_m)^\\top$ 来解决：\n$$\n\\min_{e \\in \\mathbb{R}^m} V(e) \\quad \\text{subject to} \\quad \\sum_{i=1}^m e_i = B \\quad \\text{and} \\quad e_i \\ge 0 \\text{ for all } i.\n$$\n这是一个凸优化问题。目标函数 $V(e)$ 是矩阵凸函数 $X \\mapsto g^\\top X^{-1} g$（在正定矩阵锥上）与从 $e$到后验精度矩阵的仿射映射的复合。约束集是一个单纯形，也是凸的。凸性保证了任何由合适算法找到的局部最小值也是全局最小值。\n\n为了使用基于梯度的优化算法，我们必须计算 $V(e)$ 的梯度。令 $M(e) = \\Sigma_0^{-1} + \\sum_{i=1}^m \\frac{e_i}{\\sigma_i^2} h_i h_i^\\top = \\Sigma_e^{-1}$。使用链式法则和矩阵逆的导数恒等式 $\\frac{d}{dt}A^{-1} = -A^{-1} \\frac{dA}{dt} A^{-1}$，可以找到 $V(e)$ 关于 $e_k$ 的偏导数：\n$$\n\\frac{\\partial V(e)}{\\partial e_k} = g^\\top \\frac{\\partial M(e)^{-1}}{\\partial e_k} g = g^\\top \\left( -M(e)^{-1} \\frac{\\partial M(e)}{\\partial e_k} M(e)^{-1} \\right) g\n$$\n$M(e)$ 关于 $e_k$ 的导数就是 $e_k$ 的系数：\n$$\n\\frac{\\partial M(e)}{\\partial e_k} = \\frac{1}{\\sigma_k^2} h_k h_k^\\top\n$$\n将其代回，并回顾 $\\Sigma_e = M(e)^{-1}$，我们有：\n$$\n\\frac{\\partial V(e)}{\\partial e_k} = - g^\\top \\Sigma_e \\left( \\frac{1}{\\sigma_k^2} h_k h_k^\\top \\right) \\Sigma_e g\n$$\n由于 $g^\\top \\Sigma_e h_k$ 是一个标量，我们可以重新排列表达式：\n$$\n\\frac{\\partial V(e)}{\\partial e_k} = - \\frac{1}{\\sigma_k^2} (g^\\top \\Sigma_e h_k) (h_k^\\top \\Sigma_e g) = - \\frac{1}{\\sigma_k^2} (h_k^\\top \\Sigma_e g)^2\n$$\n梯度向量是 $\\nabla_e V(e) = \\left(\\frac{\\partial V(e)}{\\partial e_1}, \\dots, \\frac{\\partial V(e)}{\\partial e_m}\\right)^\\top$。\n\n数值解将使用序列最小二乘规划（SLSQP）算法获得，这是一种适用于约束非线性优化的基于梯度的方法。该方法在 `scipy.optimize.minimize` 库函数中可用。我们将提供目标函数 $V(e)$、其解析雅可比矩阵（梯度）$\\nabla_e V(e)$、等式约束 $\\sum_i e_i - B = 0$ 和非负性边界 $e_i \\ge 0$。均匀分配 $e_i = B/m$ 可作为优化器的一个合理的初始猜测值。\n\n对于预算 $B=0$ 的特殊情况，无法进行采样，因此所有 $i$ 的 $e_i=0$。后验分布与先验分布相同，方差即为 $\\psi$ 的先验方差，即 $g^\\top\\Sigma_0 g$。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the ecological monitoring effort allocation problem for the given test cases.\n    \"\"\"\n    \n    # Define test cases\n    test_cases = [\n        # Case 1\n        {\n            \"d\": 3, \"m\": 3,\n            \"Sigma0\": np.array([[1.0, 0.2, 0.1], [0.2, 0.5, 0.1], [0.1, 0.1, 1.5]]),\n            \"H\": np.array([[1.0, 0.2, 0.0], [0.3, 1.0, 0.5], [0.0, 0.2, 1.0]]),\n            \"sigma2\": np.array([2.0, 1.5, 3.0]),\n            \"g\": np.array([0.5, 0.3, 0.2]),\n            \"B\": 4.0\n        },\n        # Case 2\n        {\n            \"d\": 3, \"m\": 3,\n            \"Sigma0\": np.array([[1.0, 0.2, 0.1], [0.2, 0.5, 0.1], [0.1, 0.1, 1.5]]),\n            \"H\": np.array([[1.0, 0.2, 0.0], [0.3, 1.0, 0.5], [0.0, 0.2, 1.0]]),\n            \"sigma2\": np.array([2.0, 1.5, 3.0]),\n            \"g\": np.array([0.5, 0.3, 0.2]),\n            \"B\": 0.0\n        },\n        # Case 3\n        {\n            \"d\": 2, \"m\": 4,\n            \"Sigma0\": np.array([[1.0, 0.3], [0.3, 1.2]]),\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0], [0.7, 0.7], [1.0, -0.2]]),\n            \"sigma2\": np.array([0.6, 2.0, 1.5, 0.8]),\n            \"g\": np.array([1.0, 0.5]),\n            \"B\": 3.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        e_star, v_star = solve_one_case(**case)\n        formatted_e = [f\"{val:.4f}\" for val in e_star]\n        formatted_v = f\"{v_star:.6f}\"\n        results.append(f'[{\",\".join(map(str, formatted_e))},{formatted_v}]')\n\n    print(f'[{\",\".join(results)}]')\n\ndef solve_one_case(d, m, Sigma0, H, sigma2, g, B):\n    \"\"\"\n    Solves a single instance of the effort allocation problem.\n    \"\"\"\n    # Handle the trivial case of zero budget\n    if B == 0.0:\n        e_star = np.zeros(m)\n        v_star = g.T @ Sigma0 @ g\n        return e_star, v_star\n\n    # Pre-compute the inverse of the prior covariance\n    Sigma0_inv = np.linalg.inv(Sigma0)\n\n    def objective(e, g_vec, S0_inv, H_mat, s2_vec):\n        \"\"\" The objective function V(e) to be minimized. \"\"\"\n        # Compute information gain from observations: sum(e_i/sigma_i^2 * h_i h_i^T)\n        info_gain = np.einsum('i,ij,ik-jk', e / s2_vec, H_mat, H_mat)\n        \n        # Posterior precision matrix M(e)\n        M = S0_inv + info_gain\n        \n        try:\n            # Posterior covariance matrix Sigma_e = M(e)^-1\n            Sigma_e = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            # Return a large number if matrix is singular\n            return np.inf\n\n        # Posterior variance of psi = g^T theta\n        variance = g_vec.T @ Sigma_e @ g_vec\n        return variance\n\n    def jacobian(e, g_vec, S0_inv, H_mat, s2_vec):\n        \"\"\" The Jacobian (gradient) of the objective function. \"\"\"\n        # Compute information gain and posterior covariance as in the objective\n        info_gain = np.einsum('i,ij,ik-jk', e / s2_vec, H_mat, H_mat)\n        M = S0_inv + info_gain\n        \n        try:\n            Sigma_e = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            return np.full_like(e, np.inf)\n\n        # Gradient component: grad_k = - (h_k^T Sigma_e g)^2 / sigma_k^2\n        # Vectorized computation for all k:\n        term = H_mat @ Sigma_e @ g_vec\n        grad = - (term**2) / s2_vec\n        return grad\n\n    # Initial guess: uniform allocation\n    e_init = np.full(m, B / m)\n\n    # Constraints: sum(e_i) = B\n    constraints = ({'type': 'eq', 'fun': lambda e: np.sum(e) - B})\n\n    # Bounds: e_i = 0\n    bounds = [(0, None) for _ in range(m)]\n\n    # Perform the optimization\n    res = minimize(\n        objective,\n        e_init,\n        args=(g, Sigma0_inv, H, sigma2),\n        method='SLSQP',\n        jac=jacobian,\n        constraints=constraints,\n        bounds=bounds,\n        tol=1e-12,\n        options={'maxiter': 500}\n    )\n\n    e_star = res.x\n    # clean up small negative values from numerical precision issues\n    e_star[e_star  1e-9] = 0.0\n\n    v_star = res.fun\n    \n    return e_star, v_star\n\nsolve()\n```", "id": "2482812"}]}