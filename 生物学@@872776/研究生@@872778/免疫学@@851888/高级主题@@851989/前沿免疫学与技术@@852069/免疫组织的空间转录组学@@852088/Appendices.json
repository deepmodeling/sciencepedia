{"hands_on_practices": [{"introduction": "在分析空间转录组数据时，一个基本问题是确定一个基因的表达模式在多大程度上是由其空间位置决定的。本练习将指导您使用高斯过程模型，这是一种强大的统计工具，用于将基因表达的总方差分解为空间效应分量和非空间噪声分量。通过为趋化因子基因 `CXCL13` 估计空间方差解释比例（PVE），您将学会量化基因表达中空间结构的强度。[@problem_id:2890143]", "problem": "给定人类淋巴结组织切片中趋化因子基因 `CXCL13` 的空间分辨信使核糖核酸（mRNA）测量数据。假设以下基本前提：分子生物学中心法则（脱氧核糖核酸到核糖核酸到蛋白质），以及一个经过充分验证的观察结果，即由于免疫组织中的微解剖生态位，对数归一化的空间转录本计数呈现空间自相关性，并叠加有独立的测量噪声。将所有空间点上的对数归一化 `CXCL13` 表达建模为一个空间随机效应模型：对于一个在坐标 $\\{\\mathbf{x}_i \\in \\mathbb{R}^2\\}_{i=1}^n$（单位为微米）处有 $n$ 个空间点的样本，观测到的表达向量 $\\mathbf{y} \\in \\mathbb{R}^n$ 满足\n$$\n\\mathbf{y} = \\mu \\mathbf{1} + \\mathbf{s} + \\boldsymbol{\\varepsilon},\n$$\n其中 $\\mu \\in \\mathbb{R}$ 是一个未知的全局均值，$\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_s^2 \\mathbf{K})$ 是一个零均值高斯过程空间效应，其方差为 $\\sigma_s^2 \\ge 0$，各向同性平方指数核 $\\mathbf{K}$ 定义为\n$$\nK_{ij} = \\exp\\left(-\\frac{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2^2}{2 \\ell^2}\\right),\n$$\n而 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I})$ 是独立残差噪声，其方差为 $\\sigma_e^2 \\ge 0$。长度尺度 $\\ell > 0$ 控制空间衰减，对于每个样本，它被确定性地设置为所有不同空间点之间两两欧几里得距离的中位数，单位为微米。由空间效应解释的方差比例（PVE）定义为\n$$\n\\mathrm{PVE} = \\frac{\\sigma_s^2}{\\sigma_s^2 + \\sigma_e^2}.\n$$\n\n任务：对于下面列出的每个样本，通过最大化上述模型下 $\\mathbf{y}$ 的高斯边际对数似然来估计 $\\sigma_s^2$ 和 $\\sigma_e^2$。在此过程中，将 $\\ell$ 按指定方式固定，并将 $\\mu$ 视为一个未知的讨厌参数，通过其最大似然值联合估计。使用基于协方差矩阵的Cholesky分解的数值稳定方法。如果 $\\mathbf{y}$ 的样本总方差在数值上可以忽略不计（定义为小于 $10^{-12}$），则将该样本的 $\\mathrm{PVE}$ 定义为 $0.0$。报告每个样本估计出的 $\\mathrm{PVE}$ 值，该值为 $[0,1]$ 区间内的小数，并四舍五入到 $3$ 位小数。\n\n物理单位：所有坐标均以微米（$\\mu$m）为单位。输出是无量纲的，必须以小数形式表示。\n\n角度单位：不适用。\n\n百分比：不适用；以小数形式报告。\n\n测试套件（四个样本）。所有样本共享相同的 $n = 9$ 个空间位置，这些位置由笛卡尔积 $\\{0,50,100\\} \\times \\{0,50,100\\}$（单位为微米）给出，排序如下：\n- 空间点 $1$：$(0,0)$\n- 空间点 $2$：$(0,50)$\n- 空间点 $3$：$(0,100)$\n- 空间点 $4$：$(50,0)$\n- 空间点 $5$：$(50,50)$\n- 空间点 $6$：$(50,100)$\n- 空间点 $7$：$(100,0)$\n- 空间点 $8$：$(100,50)$\n- 空间点 $9$：$(100,100)$\n\n对于每个样本，观测到的对数归一化`CXCL13`表达向量 $\\mathbf{y}$（按上述空间点顺序）为：\n\n- 样本A（中心周围有明显的空间生态位）：$\\mathbf{y}^{(A)} = [4.2, 5.8, 4.1, 6.0, 10.5, 6.2, 3.9, 5.7, 4.0]$。\n- 样本B（空间结构较弱）：$\\mathbf{y}^{(B)} = [5.1, 5.0, 5.2, 4.9, 5.1, 5.0, 5.2, 5.0, 5.3]$。\n- 样本C（无明显空间结构；异质噪声）：$\\mathbf{y}^{(C)} = [7.4, 3.1, 5.6, 8.0, 2.7, 4.9, 6.2, 7.9, 3.5]$。\n- 样本D（边界情况；基本无方差）：$\\mathbf{y}^{(D)} = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]$。\n\n算法要求：\n- 对每个样本，计算 $\\ell$ 为 $\\{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2 : 1 \\le i  j \\le 9\\}$ 的中位数。\n- 对于固定的 $\\ell$，关于 $(\\sigma_s^2, \\sigma_e^2)$ 最大化高斯边际对数似然，其中 $\\sigma_s^2 \\ge 0$ 且 $\\sigma_e^2 \\ge 0$，并通过其最大似然估计将 $\\mu$ 剖面化处理掉。\n- 通过协方差矩阵 $\\boldsymbol{\\Sigma} = \\sigma_s^2 \\mathbf{K} + \\sigma_e^2 \\mathbf{I}$ 的Cholesky分解，对对数似然进行数值稳定的评估。\n- 如上定义 $\\mathrm{PVE}$，并四舍五入到 $3$ 位小数。\n\n最终输出格式：您的程序应生成单行输出，其中包含与样本 A、B、C 和 D 对应的四个 $\\mathrm{PVE}$ 值，按此顺序排列，四舍五入到 $3$ 位小数，并以逗号分隔的列表形式用方括号括起来（例如，$[0.732,0.058,0.113,0.000]$）。", "solution": "该问题是有效的。它在科学上基于空间统计学和分子生物学的原理，特别是使用了高斯过程模型，这是分析空间转录组学等空间分辨数据的标准方法。该问题定义明确，提供了所有必要的数据和清晰的目标：通过最大化剖面对数似然来估计方差分量。其表述是客观且数学上精确的。\n\n解决方案首先推导出目标参数——方差解释比例（PVE）——的剖面对数似然函数，然后对每个数据样本进行数值最大化。\n\n**1. 模型设定与似然函数**\n\n观测到的对数归一化表达向量 $\\mathbf{y} \\in \\mathbb{R}^n$ 的模型由以下公式给出：\n$$\n\\mathbf{y} = \\mu \\mathbf{1} + \\mathbf{s} + \\boldsymbol{\\varepsilon}\n$$\n其中 $\\mu$ 是全局均值，$\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_s^2 \\mathbf{K})$ 是空间效应，而 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_e^2 \\mathbf{I})$ 是独立噪声。因此，总数据向量 $\\mathbf{y}$ 服从多元正态分布：\n$$\n\\mathbf{y} \\sim \\mathcal{N}(\\mu\\mathbf{1}, \\boldsymbol{\\Sigma})\n$$\n其中协方差矩阵 $\\boldsymbol{\\Sigma}$ 为 $\\boldsymbol{\\Sigma} = \\sigma_s^2 \\mathbf{K} + \\sigma_e^2 \\mathbf{I}$。需要估计的参数是方差 $\\sigma_s^2 \\ge 0$ 和 $\\sigma_e^2 \\ge 0$，而均值 $\\mu$ 是一个讨厌参数。\n\n忽略常数项，$\\mathbf{y}$ 的对数似然函数为：\n$$\n\\mathcal{L}(\\mu, \\sigma_s^2, \\sigma_e^2) = -\\frac{1}{2} (\\mathbf{y} - \\mu \\mathbf{1})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\mu \\mathbf{1}) - \\frac{1}{2} \\log \\det(\\boldsymbol{\\Sigma})\n$$\n\n**2. 参数化与剖面化**\n\n为简化优化过程，我们根据方差解释比例（PVE，记为 $\\alpha \\in [0, 1]$）和一个总方差项对协方差矩阵进行重新参数化。设 $\\alpha = \\frac{\\sigma_s^2}{\\sigma_s^2 + \\sigma_e^2}$。我们可以定义一个总方差参数，但为了进行剖面化，使用 $\\sigma_e^2$ 和比率 $v = \\sigma_s^2 / \\sigma_e^2$ 更为方便。一种更稳定、更直接的方法是直接通过 $\\alpha$ 和一个总尺度因子进行参数化。设总方差为 $\\sigma_{tot}^2 = \\sigma_s^2 + \\sigma_e^2$。则 $\\sigma_s^2 = \\alpha \\sigma_{tot}^2$ 且 $\\sigma_e^2 = (1-\\alpha) \\sigma_{tot}^2$。协方差矩阵变为：\n$$\n\\boldsymbol{\\Sigma}(\\alpha, \\sigma_{tot}^2) = \\sigma_{tot}^2 \\left( \\alpha \\mathbf{K} + (1-\\alpha) \\mathbf{I} \\right) = \\sigma_{tot}^2 \\mathbf{V}(\\alpha)\n$$\n其中 $\\mathbf{V}(\\alpha) = \\alpha \\mathbf{K} + (1-\\alpha) \\mathbf{I}$。对数似然则为：\n$$\n\\mathcal{L}(\\mu, \\alpha, \\sigma_{tot}^2) = -\\frac{1}{2\\sigma_{tot}^2} (\\mathbf{y} - \\mu\\mathbf{1})^\\top \\mathbf{V}(\\alpha)^{-1} (\\mathbf{y} - \\mu\\mathbf{1}) - \\frac{n}{2} \\log(\\sigma_{tot}^2) - \\frac{1}{2} \\log\\det(\\mathbf{V}(\\alpha))\n$$\n我们针对参数最大化此似然函数。问题要求将 $\\mu$ 视为一个讨厌参数。我们可以通过将其替换为其最大似然估计（MLE）来“剖掉”它，对于固定的 $\\alpha$ 和 $\\sigma_{tot}^2$，该估计是广义最小二乘（GLS）估计：\n$$\n\\hat{\\mu}(\\alpha) = \\frac{\\mathbf{1}^\\top \\mathbf{V}(\\alpha)^{-1} \\mathbf{y}}{\\mathbf{1}^\\top \\mathbf{V}(\\alpha)^{-1} \\mathbf{1}}\n$$\n我们也可以剖掉总方差 $\\sigma_{tot}^2$。对 $\\mathcal{L}$ 关于 $\\sigma_{tot}^2$ 求导并令其为零，得到其 MLE：\n$$\n\\hat{\\sigma}_{tot}^2(\\alpha) = \\frac{1}{n} (\\mathbf{y} - \\hat{\\mu}(\\alpha)\\mathbf{1})^\\top \\mathbf{V}(\\alpha)^{-1} (\\mathbf{y} - \\hat{\\mu}(\\alpha)\\mathbf{1}) = \\frac{Q(\\alpha)}{n}\n$$\n其中我们将 $Q(\\alpha)$ 定义为 GLS 残差平方和。将 $\\hat{\\mu}(\\alpha)$ 和 $\\hat{\\sigma}_{tot}^2(\\alpha)$ 代回似然函数，得到仅是 $\\alpha$ 函数的剖面对数似然：\n$$\n\\mathcal{L}_{prof}(\\alpha) \\propto -\\frac{n}{2} \\log(Q(\\alpha)) - \\frac{1}{2} \\log\\det(\\mathbf{V}(\\alpha))\n$$\n最大化 $\\mathcal{L}_{prof}(\\alpha)$ 等价于最小化其负数（乘以2）：\n$$\nf(\\alpha) = n \\log(Q(\\alpha)) + \\log\\det(\\mathbf{V}(\\alpha))\n$$\nPVE 的估计值是使该目标函数最小化的 $\\alpha \\in [0, 1]$ 的值。\n\n**3. 算法与数值策略**\n\n每个样本的总体算法如下：\n\n1.  **特殊情况处理**：根据问题要求，如果 $\\mathbf{y}$ 的样本方差小于 $10^{-12}$，我们设置 $\\mathrm{PVE} = 0.0$ 并且不继续进行优化。这处理了没有方差可解释的情况，例如样本D。\n\n2.  **核的预计算**：所有样本的空间点坐标 $\\{\\mathbf{x}_i\\}_{i=1}^n$ 是固定的。\n    a. 我们计算所有两两欧几里得距离 $\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2$（对于 $i  j$）。对于 $n=9$ 个点，这会得到 $9 \\times 8 / 2 = 36$ 个唯一距离。\n    b. 长度尺度 $\\ell$ 被确定为这 $36$ 个距离的中位数。\n    c. 然后计算核矩阵 $\\mathbf{K}$，其元素为 $K_{ij} = \\exp\\left(-\\frac{\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert_2^2}{2 \\ell^2}\\right)$。\n\n3.  **数值优化**：我们通过使用数值优化程序，在区间 $[0, 1]$ 上最小化目标函数 $f(\\alpha)$ 来找到最优的 $\\alpha$。\n\n4.  **目标函数的稳定评估**：对于 $f(\\alpha)$ 的每次评估，我们必须以数值稳定的方式计算 $Q(\\alpha)$ 和 $\\log\\det(\\mathbf{V}(\\alpha))$。\n    a. 首先，构造矩阵 $\\mathbf{V}(\\alpha) = \\alpha \\mathbf{K} + (1-\\alpha) \\mathbf{I}$。\n    b. 执行Cholesky分解 $\\mathbf{V}(\\alpha) = \\mathbf{L}\\mathbf{L}^\\top$，其中 $\\mathbf{L}$ 是一个下三角矩阵。这是检查正定性并保持数值稳定性的稳健方法。\n    c. 对数行列式计算为 $\\log\\det(\\mathbf{V}(\\alpha)) = 2 \\sum_{i=1}^n \\log(L_{ii})$。\n    d. 涉及 $\\mathbf{V}(\\alpha)^{-1}$ 的项，如 $\\mathbf{V}(\\alpha)^{-1}\\mathbf{y}$，通过使用 $\\mathbf{L}$ 求解线性方程组来计算。对于向量 $\\mathbf{v}$，我们通过先求解 $\\mathbf{L}\\mathbf{w} = \\mathbf{v}$ 得到 $\\mathbf{w}$（前向代入），然后求解 $\\mathbf{L}^\\top\\mathbf{z} = \\mathbf{w}$ 得到 $\\mathbf{z}$（后向代入），来找到 $\\mathbf{z} = \\mathbf{V}(\\alpha)^{-1}\\mathbf{v}$。这避免了显式矩阵求逆。\n    e. 利用这些稳定组件，我们计算 $\\hat{\\mu}(\\alpha)$、残差向量 $\\mathbf{r} = \\mathbf{y} - \\hat{\\mu}(\\alpha)\\mathbf{1}$，最后计算二次型 $Q(\\alpha) = \\mathbf{r}^\\top \\mathbf{V}(\\alpha)^{-1} \\mathbf{r}$。计算 $Q(\\alpha)$ 的一种稳定方法是求解 $\\mathbf{L}\\mathbf{w}_r = \\mathbf{r}$ 得到 $\\mathbf{w}_r$，然后 $Q(\\alpha) = \\mathbf{w}_r^\\top \\mathbf{w}_r$。\n\n5.  **最终结果**：$f(\\alpha)$ 的最小值点即为估计的 PVE，然后按要求四舍五入到 $3$ 位小数。此过程应用于四个样本中的每一个。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.optimize import minimize_scalar\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the spatial transcriptomics variance component estimation problem.\n    \"\"\"\n    # Define the fixed spatial locations for all samples\n    coords_list = [\n        (0.0, 0.0), (0.0, 50.0), (0.0, 100.0),\n        (50.0, 0.0), (50.0, 50.0), (50.0, 100.0),\n        (100.0, 0.0), (100.0, 50.0), (100.0, 100.0)\n    ]\n    coords = np.array(coords_list, dtype=np.float64)\n    n = coords.shape[0]\n\n    # Define the observed expression vectors for each sample\n    test_cases = [\n        np.array([4.2, 5.8, 4.1, 6.0, 10.5, 6.2, 3.9, 5.7, 4.0], dtype=np.float64), # Sample A\n        np.array([5.1, 5.0, 5.2, 4.9, 5.1, 5.0, 5.2, 5.0, 5.3], dtype=np.float64), # Sample B\n        np.array([7.4, 3.1, 5.6, 8.0, 2.7, 4.9, 6.2, 7.9, 3.5], dtype=np.float64), # Sample C\n        np.array([2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], dtype=np.float64)  # Sample D\n    ]\n\n    # --- Pre-computation of the spatial kernel ---\n    # 1. Compute length-scale 'l' as the median of pairwise distances\n    pairwise_distances = pdist(coords)\n    l_scale = np.median(pairwise_distances)\n\n    # 2. Compute the squared-exponential kernel matrix 'K'\n    sq_dist_matrix = squareform(pairwise_distances**2)\n    K = np.exp(-sq_dist_matrix / (2 * l_scale**2))\n    \n    # --- Negative Profiled Log-Likelihood Function Factory ---\n    def get_objective_function(y, K_matrix):\n        \"\"\"\n        Returns the objective function to be minimized.\n        The objective function is the negative profiled log-likelihood for the PVE parameter 'alpha'.\n        \"\"\"\n        n_spots = len(y)\n        ones_vec = np.ones(n_spots)\n\n        def objective(alpha):\n            \"\"\"\n            Computes the value of the objective function for a given alpha (PVE).\n            alpha is in [0, 1].\n            \"\"\"\n            # Define V(alpha) = alpha*K + (1-alpha)*I\n            V_matrix = alpha * K_matrix + (1 - alpha) * np.identity(n_spots)\n\n            try:\n                # Cholesky factorization for stability\n                L = cholesky(V_matrix, lower=True)\n            except np.linalg.LinAlgError:\n                # If matrix is not positive definite, return a large value\n                # to penalize this region of the parameter space.\n                return np.inf\n\n            # log determinant of V\n            log_det_V = 2 * np.sum(np.log(np.diag(L)))\n\n            # Solve for V_inv @ y and V_inv @ 1 using forward/backward substitution\n            y_tilde = solve_triangular(L, y, lower=True)\n            z_y = solve_triangular(L.T, y_tilde, lower=False)\n            \n            o_tilde = solve_triangular(L, ones_vec, lower=True)\n            z_1 = solve_triangular(L.T, o_tilde, lower=False)\n\n            # GLS estimate of the mean mu\n            mu_hat = np.sum(z_y) / np.sum(z_1)\n\n            # Quadratic form Q(alpha)\n            residual = y - mu_hat\n            w_r = solve_triangular(L, residual, lower=True)\n            Q = np.dot(w_r, w_r)\n\n            # Guard against log(0) for perfect fits\n            if Q  1e-20:\n                return -np.inf\n\n            # Final objective value (from profiled log-likelihood)\n            return n_spots * np.log(Q) + log_det_V\n\n        return objective\n\n    # --- Process each sample ---\n    results = []\n    for y_vec in test_cases:\n        # Special case: if sample has negligible variance, PVE is defined as 0\n        if np.var(y_vec, ddof=0)  1e-12:\n            pve = 0.0\n        else:\n            # Create the objective function for the current sample\n            objective_func = get_objective_function(y_vec, K)\n            \n            # Find the PVE (alpha) that minimizes the objective function\n            res = minimize_scalar(\n                objective_func,\n                bounds=(0, 1),\n                method='bounded'\n            )\n            pve = res.x\n            \n        results.append(pve)\n    \n    # Format results as specified\n    rounded_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(rounded_results)}]\")\n\nsolve()\n```", "id": "2890143"}, {"introduction": "在确定了单个基因的空间模式后，下一步是整合多个基因的表达来识别功能性的组织微结构，例如在慢性炎症或癌症中形成的三级淋巴结构（TLS）。本练习将引导您完成一个真实世界的生物信息学分析流程，从未加工的基因计数开始，通过聚类分析来识别表达关键标志物（`CXCL13`、`CCL19`和`PNAd`）的细胞热点。您还将学习如何应用空间连续性过滤器来提炼您的预测，并使用 $F_1$ 分数来验证其准确性。[@problem_id:2890083]", "problem": "您将获得代表发炎肺组织并可能包含三级淋巴结构（TLS）的合成空间转录组学数据。您的目标是通过对三个配体/特征通道——`CXCL13`、`CCL19`和外周节点地址素（PNAd）——进行聚类，以算法方式识别 TLS，并根据提供的二元组织学衍生的 TLS 注释来验证预测的 TLS。三级淋巴结构（TLS）是异位淋巴聚集体，通常富含趋化因子 `CXCL13` 和 `CCL19`，并以表达外周节点地址素（PNAd）的特化脉管系统为特征。\n\n您的程序必须仅使用第一性原理和核心定义来实现以下流程：\n\n- 从分子生物学中心法则出发，即转录产生信使核糖核酸（mRNA），其在空间点中可以作为计数进行量化。用一个三维特征向量表示每个点，该向量由三个通道的观测计数值构成。\n- 使用自然对数对计数值应用单调方差稳定变换：对于每个观测计数值 $x \\in \\mathbb{R}_{\\ge 0}$，通过 $x_{\\mathrm{log}} = \\log(1 + x)$ 进行变换。\n- 使用 $z$-分数对每个特征在所有点上进行标准化：对于一个样本均值为 $\\mu_f$、样本标准差为 $\\sigma_f$ 的特征 $f$，每个变换后的值为 $z = \\frac{x_{\\mathrm{log}} - \\mu_f}{\\sigma_f}$。约定如果 $\\sigma_f = 0$，则该特征的所有标准化值都设为 $0$。\n- 在标准化的三维空间中，使用 $k$-均值目标将点聚类成 $k = 2$ 个簇。该目标是最小化簇内平方欧几里得距离之和。使用欧几里得距离 $d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2$，通过选择特征总和为最小和最大的点的特征向量作为初始质心来进行确定性初始化，并迭代直至分配稳定。如果出现空簇，则将其质心重新初始化为距离另一个簇的质心最远的点。\n- 将质心在三个标准化特征上的平均值较大的簇指定为“TLS簇”。这编码了生物学上的预期，即 TLS 与共同升高的 `CXCL13`、`CCL19` 和 `PNAd` 特征共定位。\n- 通过仅保留那些属于大小至少为指定阈值 $\\tau$ 的连通分量（使用 $4$-邻域邻接：上、下、左、右）的预测 TLS 点来强制空间连续性。移除所有较小的预测 TLS 连通分量。\n- 使用 $F_1$ 分数验证预测的 TLS 与组织学 TLS 注释。设真阳性为 $TP$，假阳性为 $FP$，假阴性为 $FN$。计算精确率 $P = \\frac{TP}{TP + FP}$（如果 $TP + FP = 0$ 则定义为 $0$），召回率 $R = \\frac{TP}{TP + FN}$（如果 $TP + FN = 0$ 则定义为 $0$），以及\n$$\nF_1 =\n\\begin{cases}\n1,  \\text{如果预测和基准真相中的正例数量均为 } 0 \\\\\n\\frac{2 P R}{P + R},  \\text{如果 } P + R > 0 \\\\\n0,  \\text{其他情况}\n\\end{cases}\n$$\n这个约定避免了当预测和基准真相都没有正例点时出现未定义值的情况。\n\n实现上述流程并在以下三个测试用例上运行。在每个测试用例中，您将获得一个大小为 $5 \\times 5$ 的空间点网格。每个网格都以三个计数矩阵（分别对应 `CXCL13`、`CCL19` 和 `PNAd` 通道）和一个二元组织学掩模（其中 $1$ 表示 TLS 标签，$0$ 表示背景）的形式给出。对于邻接，在此网格上使用 $4$-邻域连通性。您不能接受任何输入；程序必须嵌入这些数据集。\n\n测试用例 1 (理想情况：紧凑的高表达TLS):\n- 网格大小: $5 \\times 5$。\n- `CXCL13` 计数值:\n  $\n  \\begin{bmatrix}\n  1  1  1  1  1 \\\\\n  1  60  60  60  1 \\\\\n  1  60  60  60  1 \\\\\n  1  60  60  60  1 \\\\\n  1  1  1  1  1\n  \\end{bmatrix}\n  $\n- `CCL19` 计数值:\n  $\n  \\begin{bmatrix}\n  1  1  1  1  1 \\\\\n  1  50  50  50  1 \\\\\n  1  50  50  50  1 \\\\\n  1  50  50  50  1 \\\\\n  1  1  1  1  1\n  \\end{bmatrix}\n  $\n- `PNAd` 特征计数值:\n  $\n  \\begin{bmatrix}\n  1  1  1  1  1 \\\\\n  1  40  40  40  1 \\\\\n  1  40  40  40  1 \\\\\n  1  40  40  40  1 \\\\\n  1  1  1  1  1\n  \\end{bmatrix}\n  $\n- 组织学 TLS 掩模:\n  $\n  \\begin{bmatrix}\n  0  0  0  0  0 \\\\\n  0  1  1  1  0 \\\\\n  0  1  1  1  0 \\\\\n  0  1  1  1  0 \\\\\n  0  0  0  0  0\n  \\end{bmatrix}\n  $\n- 连续性阈值: $\\tau = 5$。\n\n测试用例 2 (边缘情况：无组织学TLS的分散高表达):\n- 网格大小: $5 \\times 5$。\n- `CXCL13` 计数值:\n  $\n  \\begin{bmatrix}\n  60  1  1  1  60 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  60  1  1  1  60\n  \\end{bmatrix}\n  $\n- `CCL19` 计数值:\n  $\n  \\begin{bmatrix}\n  50  1  1  1  50 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  50  1  1  1  50\n  \\end{bmatrix}\n  $\n- `PNAd` 特征计数值:\n  $\n  \\begin{bmatrix}\n  40  1  1  1  40 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  40  1  1  1  40\n  \\end{bmatrix}\n  $\n- 组织学 TLS 掩模:\n  $\n  \\begin{bmatrix}\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0\n  \\end{bmatrix}\n  $\n- 连续性阈值: $\\tau = 3$。\n\n测试用例 3 (边界条件：组织学TLS小于连续性阈值):\n- 网格大小: $5 \\times 5$。\n- `CXCL13` 计数值:\n  $\n  \\begin{bmatrix}\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  20  20  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1\n  \\end{bmatrix}\n  $\n- `CCL19` 计数值:\n  $\n  \\begin{bmatrix}\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  18  18  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1\n  \\end{bmatrix}\n  $\n- `PNAd` 特征计数值:\n  $\n  \\begin{bmatrix}\n  1  1  1  1  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  15  15  1 \\\\\n  1  1  1  1  1 \\\\\n  1  1  1  1  1\n  \\end{bmatrix}\n  $\n- 组织学 TLS 掩模:\n  $\n  \\begin{bmatrix}\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  1  1  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0\n  \\end{bmatrix}\n  $\n- 连续性阈值: $\\tau = 3$。\n\n角度单位不适用。没有需要报告的物理单位。您的程序应生成单行输出，其中包含三个测试用例的 $F_1$ 分数，形式为用方括号括起来的逗号分隔列表，每个值四舍五入到三位小数（例如，$[0.997,0.000,1.000]$）。输出必须严格遵守此单行格式，不含多余空格。\n\n您的实现必须是自包含的，无需用户输入，并遵循上述规则。最终输出格式严格为一行：$[f_1,f_2,f_3]$，其中每个 $f_i$ 是一个四舍五入到三位小数的十进制数。", "solution": "提交评估的问题陈述被认为是有效的。它具有科学依据、提法明确且客观。它基于分子生物学、统计学和计算机科学中已确立的原理，提出了一个明确定义的算法任务。数据、约束条件和评估指标的规定足够精确，足以得出一个唯一且可验证的解。我们现在将继续进行所需流程的形式化推导和实现。\n\n任务是从合成的空间转录组学数据中识别三级淋巴结构（TLS）。解决方案被构建为一个多步骤的计算流程，我们将从第一性原理开始构建。\n\n**1. 数据表示与预处理**\n\n分子生物学中心法则假定遗传信息从DNA流向RNA，再到蛋白质。空间转录组学在空间分辨的位置（或称“点”）量化转录的信使RNA（mRNA）产物。对于由网格坐标 $(i, j)$ 定位的每个点，数据提供了三个特定基因特征的mRNA计数：CXCL13、CCL19和PNAd。因此，网格中的 $N$ 个点中的每一个都由一个三维非负整数计数特征向量 $\\mathbf{x} = [x_{CXCL13}, x_{CCL19}, x_{PNAd}]^T \\in \\mathbb{N}_0^3$ 表示。\n\n已知转录组学计数数据表现出大的动态范围和异方差性（方差依赖于均值）。一个常见且强制的步骤是应用方差稳定变换。根据问题规范，我们使用对数变换。对于每个计数值 $x_k$，变换后的值 $x_{k, \\mathrm{log}}$ 计算如下：\n$$\nx_{k, \\mathrm{log}} = \\log(1 + x_k)\n$$\n加 $1$ 确保零计数映射到 $0$，并避免了对数在零处的奇点。\n\n变换之后，每个特征（基因特征）必须被标准化，以便为基于距离的聚类提供一个共同的尺度。我们采用 $z$-分数变换。对于每个特征 $f \\in \\{CXCL13, CCL19, PNAd\\}$，我们计算它在所有 $N$ 个点上的样本均值 $\\mu_f$ 和样本标准差 $\\sigma_f$。然后，每个对数变换后的值 $x_{f, \\mathrm{log}}$ 被标准化为 $z_f$：\n$$\nz_f = \\frac{x_{f, \\mathrm{log}} - \\mu_f}{\\sigma_f}\n$$\n如果一个特征在所有点上的方差为零（$\\sigma_f = 0$），则其所有标准化值都定义为 $0$。结果是一组 $N$ 个标准化的特征向量 $\\{\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_N\\}$，其中每个 $\\mathbf{z}_i \\in \\mathbb{R}^3$。\n\n**2. K-均值聚类**\n\n识别任务的核心是根据点的基因特征谱将它们划分为两组——潜在的TLS和背景组织。我们使用 $k=2$ 的 $k$-均值聚类。目标是找到 $N$ 个点的一个划分 $S = \\{S_1, S_2\\}$，以最小化簇内平方欧几里得距离之和。设 $\\mathbf{c}_j$ 是簇 $S_j$ 的质心。要最小化的目标函数是：\n$$\n\\sum_{j=1}^{2} \\sum_{\\mathbf{z} \\in S_j} \\|\\mathbf{z} - \\mathbf{c}_j\\|_2^2\n$$\n其中 $\\|\\cdot\\|_2$ 是欧几里得范数。算法按以下步骤进行：\n\n- **初始化**：质心必须确定性地初始化。我们计算每个标准化特征向量 $\\mathbf{z}_i$ 的分量之和。选择其特征向量产生最小和最大和的两个点，它们的特征向量成为初始质心 $\\mathbf{c}_1$ 和 $\\mathbf{c}_2$。\n- **迭代**：算法在两个步骤之间迭代，直到簇分配不再改变。\n    1. **分配步骤**：每个点的特征向量 $\\mathbf{z}_i$ 被分配给对应于最近质心 $\\mathbf{c}_j$ 的簇 $S_j$。分配基于最小化欧几里得距离：\n        $$\n        j^* = \\arg\\min_{j \\in \\{1, 2\\}} d(\\mathbf{z}_i, \\mathbf{c}_j) = \\arg\\min_{j \\in \\{1, 2\\}} \\|\\mathbf{z}_i - \\mathbf{c}_j\\|_2\n        $$\n    2. **更新步骤**：每个簇的质心被重新计算为分配给它的所有特征向量的均值：\n        $$\n        \\mathbf{c}_j = \\frac{1}{|S_j|} \\sum_{\\mathbf{z} \\in S_j} \\mathbf{z}\n        $$\n- **空簇处理**：如果更新步骤导致一个空簇 $S_j$（即 $|S_j| = 0$），其质心 $\\mathbf{c}_j$ 将被重新初始化。它被设置为距离另一个非空簇的质心最远的特征向量 $\\mathbf{z}_i$。这可以防止算法坍缩为单个簇。\n\n**3. TLS簇指定**\n\n收敛后，我们得到两个簇。生物学上，TLS的特征是CXCL13、CCL19和PNAd的共同上调。在标准化的特征空间中，这对应于一个其质心在三个分量上具有较高平均值的簇。设最终质心为 $\\mathbf{c}_1 = [c_{11}, c_{12}, c_{13}]^T$ 和 $\\mathbf{c}_2 = [c_{21}, c_{22}, c_{23}]^T$。我们将“TLS簇”指定为质心分量均值最大的簇 $j^*$：\n$$\nj^* = \\arg\\max_{j \\in \\{1, 2\\}} \\left( \\frac{1}{3} \\sum_{l=1}^{3} c_{jl} \\right)\n$$\n所有分配给簇 $S_{j^*}$ 的点都被赋予一个初步的预测标签 $1$ (TLS)，所有其他的点被标记为 $0$ (非TLS)。这产生了一个二元预测掩模。\n\n**4. 空间连续性过滤**\n\nTLS是有组织的结构，而非分散的单个细胞。因此，一个有效的预测必须强制空间连续性。我们对二元预测掩模应用一个后处理滤波器。我们识别标记为 $1$ 的点的连通分量。连通性由网格上的 $4$-邻域定义：如果两个点共享一边（上、下、左或右），则它们是相邻的。这可以通过图遍历算法（如广度优先搜索 (BFS) 或深度优先搜索 (DFS)）来实现。\n对于每个预测的TLS点的连通分量，我们计算它的大小（点的数量）。任何大小小于给定阈值 $\\tau$ 的分量都被认为是生物学上无意义的噪声或伪影。此类小分量内的所有点，其标签都从 $1$ 恢复为 $0$。这个过滤步骤通过仅保留具有最小尺寸的空间上连贯的结构来精炼预测。\n\n**5. 性能验证**\n\n最后一步是定量地验证过滤后的预测掩模与基准真相的组织学注释。我们使用 $F_1$ 分数，它是精确率和召回率的调和平均数。首先，我们计算真阳性 ($TP$)、假阳性 ($FP$) 和假阴性 ($FN$) 的数量：\n- $TP$：被正确预测为TLS的点数（预测=1，真相=1）。\n- $FP$：被错误预测为TLS的点数（预测=1，真相=0）。\n- $FN$：被错误预测为非TLS的点数（预测=0，真相=1）。\n\n然后计算精确率 ($P$) 和召回率 ($R$)：\n$$\nP = \\frac{TP}{TP + FP} \\quad (\\text{如果 } TP + FP = 0 \\text{ 则定义为 0})\n$$\n$$\nR = \\frac{TP}{TP + FN} \\quad (\\text{如果 } TP + FN = 0 \\text{ 则定义为 0})\n$$\n$F_1$ 分数根据提供的特定规则计算：\n$$\nF_1 =\n\\begin{cases}\n1,  \\text{如果 } TP+FP=0 \\text{ 且 } TP+FN=0 \\\\\n\\frac{2 P R}{P + R},  \\text{如果 } P + R > 0 \\\\\n0,  \\text{其他情况}\n\\end{cases}\n$$\n第一种情况正确处理了预测和基准真相都不包含正例的场景，这应被视为完美匹配（$F_1 = 1$）。然后将这整个流程应用于提供的三个测试用例中的每一个。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full TLS identification and validation pipeline.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"cxcl13\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 60, 60, 60, 1],\n                [1, 60, 60, 60, 1],\n                [1, 60, 60, 60, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"ccl19\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 50, 50, 50, 1],\n                [1, 50, 50, 50, 1],\n                [1, 50, 50, 50, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"pnad\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 40, 40, 40, 1],\n                [1, 40, 40, 40, 1],\n                [1, 40, 40, 40, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"histology\": np.array([\n                [0, 0, 0, 0, 0],\n                [0, 1, 1, 1, 0],\n                [0, 1, 1, 1, 0],\n                [0, 1, 1, 1, 0],\n                [0, 0, 0, 0, 0]\n            ]),\n            \"tau\": 5\n        },\n        # Test case 2\n        {\n            \"cxcl13\": np.array([\n                [60, 1, 1, 1, 60],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [60, 1, 1, 1, 60]\n            ]),\n            \"ccl19\": np.array([\n                [50, 1, 1, 1, 50],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [50, 1, 1, 1, 50]\n            ]),\n            \"pnad\": np.array([\n                [40, 1, 1, 1, 40],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [40, 1, 1, 1, 40]\n            ]),\n            \"histology\": np.array([\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]),\n            \"tau\": 3\n        },\n        # Test case 3\n        {\n            \"cxcl13\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 20, 20, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"ccl19\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 18, 18, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"pnad\": np.array([\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 15, 15, 1],\n                [1, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1]\n            ]),\n            \"histology\": np.array([\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 1, 1, 0],\n                [0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0]\n            ]),\n            \"tau\": 3\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        cxcl13, ccl19, pnad, histology, tau = case[\"cxcl13\"], case[\"ccl19\"], case[\"pnad\"], case[\"histology\"], case[\"tau\"]\n        \n        # Combine channels into a feature matrix\n        shape = cxcl13.shape\n        n_spots = shape[0] * shape[1]\n        features = np.stack([cxcl13.flatten(), ccl19.flatten(), pnad.flatten()], axis=1)\n\n        # 1. Log transform\n        log_features = np.log1p(features)\n        \n        # 2. Z-score standardization\n        mean = np.mean(log_features, axis=0)\n        std = np.std(log_features, axis=0)\n        \n        # Handle std == 0 case\n        z_scores = np.zeros_like(log_features)\n        non_zero_std_mask = std != 0\n        if np.any(non_zero_std_mask):\n            z_scores[:, non_zero_std_mask] = (log_features[:, non_zero_std_mask] - mean[non_zero_std_mask]) / std[non_zero_std_mask]\n\n        # 3. K-means clustering (k=2)\n        # Initialization\n        feature_sums = np.sum(z_scores, axis=1)\n        min_idx, max_idx = np.argmin(feature_sums), np.argmax(feature_sums)\n        centroids = np.array([z_scores[min_idx], z_scores[max_idx]])\n        \n        assignments = np.zeros(n_spots, dtype=int)\n        \n        while True:\n            old_assignments = assignments.copy()\n            \n            # Assignment step\n            distances = np.sqrt(((z_scores[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2).sum(axis=2))\n            assignments = np.argmin(distances, axis=1)\n            \n            # Update step\n            new_centroids = np.zeros_like(centroids)\n            for i in range(2):\n                points_in_cluster = z_scores[assignments == i]\n                if len(points_in_cluster) == 0:\n                    # Empty cluster handling\n                    other_centroid = centroids[1 - i]\n                    dists_to_other = np.linalg.norm(z_scores - other_centroid, axis=1)\n                    farthest_point_idx = np.argmax(dists_to_other)\n                    new_centroids[i] = z_scores[farthest_point_idx]\n                else:\n                    new_centroids[i] = np.mean(points_in_cluster, axis=0)\n            \n            centroids = new_centroids\n            \n            if np.array_equal(assignments, old_assignments):\n                break\n\n        # 4. Designate TLS cluster\n        centroid_means = np.mean(centroids, axis=1)\n        tls_cluster_label = np.argmax(centroid_means)\n        \n        prediction_mask_flat = (assignments == tls_cluster_label).astype(int)\n        prediction_mask = prediction_mask_flat.reshape(shape)\n\n        # 5. Spatial contiguity filtering\n        visited = np.zeros(shape, dtype=bool)\n        filtered_prediction_mask = np.zeros(shape, dtype=int)\n        \n        for r in range(shape[0]):\n            for c in range(shape[1]):\n                if prediction_mask[r, c] == 1 and not visited[r, c]:\n                    component = []\n                    q = [(r, c)]\n                    visited[r, c] = True\n                    \n                    while q:\n                        curr_r, curr_c = q.pop(0)\n                        component.append((curr_r, curr_c))\n                        \n                        for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                            nr, nc = curr_r + dr, curr_c + dc\n                            if 0 = nr  shape[0] and 0 = nc  shape[1] and \\\n                               prediction_mask[nr, nc] == 1 and not visited[nr, nc]:\n                                visited[nr, nc] = True\n                                q.append((nr, nc))\n                    \n                    if len(component) >= tau:\n                        for comp_r, comp_c in component:\n                            filtered_prediction_mask[comp_r, comp_c] = 1\n\n        # 6. Validation with F1 score\n        pred_flat = filtered_prediction_mask.flatten()\n        truth_flat = histology.flatten()\n        \n        tp = np.sum((pred_flat == 1)  (truth_flat == 1))\n        fp = np.sum((pred_flat == 1)  (truth_flat == 0))\n        fn = np.sum((pred_flat == 0)  (truth_flat == 1))\n        \n        num_pos_pred = tp + fp\n        num_pos_true = tp + fn\n        \n        if num_pos_pred == 0 and num_pos_true == 0:\n            f1 = 1.0\n        else:\n            precision = tp / num_pos_pred if num_pos_pred > 0 else 0.0\n            recall = tp / num_pos_true if num_pos_true > 0 else 0.0\n            \n            if precision + recall > 0:\n                f1 = 2 * (precision * recall) / (precision + recall)\n            else:\n                f1 = 0.0\n        \n        results.append(f\"{f1:.3f}\")\n\n    # Final print statement\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2890083"}, {"introduction": "细胞间的通讯是免疫功能的基石，而配体及其受体在空间上的共定位是推断信号通路活跃的关键证据。然而，在空间转录组数据中，技术差异（如不同空间位点的测序深度不同）可能会产生误导性的相关性。本练习将向您介绍一种严谨的统计方法——条件置换检验，以检验配体-受体的共定位，同时精确地控制每个空间位点总文库大小这一重要的混杂因素。[@problem_id:2890015]", "problem": "给定一个关于免疫组织空间转录组学实验的简化抽象模型，旨在测试配体及其同源受体是否在相邻的空间点上共定位。此问题的基本依据是分子生物学中心法则和核糖核酸（RNA）测序的标准测量特性，具体而言，空间转录组学为每个点和每个基因生成非负整数的分子计数，并且点文库大小（从一个点捕获的总计数）是一个主要的干扰因素。在零假设下，即以每个点的文库大小为条件，配体与受体读段的标签分配是可交换的，可以通过在每个点内排列配体和受体标签，同时保持该点的文库大小不变，来构建一个条件随机化检验。\n\n您的任务是实现一个程序，为每个指定的测试用例，使用一个保持点文库大小不变的条件置换来计算正向配体-受体共定位的经验单边p值。共定位检验统计量必须是一个空间互相关，它将一个点的配体表达与其空间邻域内受体的平均表达进行比较，邻域由欧几里得空间中的k近邻（kNN）定义。\n\n使用的定义：\n- 设有个 $n$ 点，索引为 $i \\in \\{1,\\dots,n\\}$，每个点都有坐标 $\\mathbf{c}_i \\in \\mathbb{R}^2$。\n- 设 $x_i \\in \\mathbb{N}_0$ 是在点 $i$ 处观测到的配体计数， $y_i \\in \\mathbb{N}_0$ 是在点 $i$ 处观测到的受体计数。\n- 每个点 $i$ 的点文库大小为 $t_i = x_i + y_i$。\n- 对于固定的邻居数量 $k \\in \\mathbb{N}$ 且 $1 \\le k \\le n-1$，将邻居集合 $\\mathcal{N}(i)$ 定义为与点 $i$ 的欧几里得距离 $\\|\\mathbf{c}_i - \\mathbf{c}_j\\|_2$ 最小的 $k$ 个不同点 $j \\ne i$，并通过较小的索引 $j$ 来打破平局。\n- 将点 $i$ 处的邻居平均受体值定义为 $m_i = \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j \\in \\mathcal{N}(i)} y_j$。\n- 将空间互相关统计量定义为 $\\{x_i\\}_{i=1}^n$ 和 $\\{m_i\\}_{i=1}^n$ 之间的 Pearson 相关性：\n$$\nT_{\\mathrm{obs}} \\;=\\; \\frac{\\frac{1}{n} \\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)\\left(m_i - \\bar{m}\\right)}{\\sqrt{\\left(\\frac{1}{n} \\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)^2\\right)\\left(\\frac{1}{n} \\sum_{i=1}^n \\left(m_i - \\bar{m}\\right)^2\\right)}},\n$$\n其中 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ 且 $\\bar{m} = \\frac{1}{n}\\sum_{i=1}^n m_i$。如果任一分母项为零（即零方差），则定义 $T_{\\mathrm{obs}} = 0$。\n\n置换零假设：\n- 对于给定的置换次数 $B \\in \\mathbb{N}$，生成 $B$ 个独立的置换数据集，方法是：对每个点 $i$，抛掷一枚均匀的硬币来决定是否交换配体和受体的计数。形式上，对于每个置换 $b \\in \\{1,\\dots,B\\}$ 和每个点 $i$，独立地抽取 $Z_i^{(b)} \\sim \\mathrm{Bernoulli}(1/2)$，并设置\n$$\nx_i^{(b)} \\;=\\; \\begin{cases}\ny_i,  \\text{如果 } Z_i^{(b)} = 1,\\\\\nx_i,  \\text{如果 } Z_i^{(b)} = 0,\n\\end{cases}\n\\qquad\ny_i^{(b)} \\;=\\; \\begin{cases}\nx_i,  \\text{如果 } Z_i^{(b)} = 1,\\\\\ny_i,  \\text{如果 } Z_i^{(b)} = 0.\n\\end{cases}\n$$\n这对于每个点 $i$ 和每个置换 $b$ 都保持了 $t_i = x_i + y_i$。\n- 对于每个置换 $b$，计算置换统计量 $T^{(b)}$，其计算方式与 $T_{\\mathrm{obs}}$ 完全相同，但使用 $x_i^{(b)}$ 和从 $y_j^{(b)}$ 计算出的邻居平均值 $m_i^{(b)}$。\n\n经验单边p值：\n- 将正向共定位的p值报告为\n$$\np \\;=\\; \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\!\\left\\{\\, T^{(b)} \\ge T_{\\mathrm{obs}} \\,\\right\\}}{B + 1}.\n$$\n\n算法要求：\n- 使用欧几里得距离寻找邻居。\n- 使用所描述的通过较小索引打破平局的规则。\n- 排除自身作为邻居。\n- 如果一个向量的方差为零，则按照上述规定，使用相应相关性为 $0$ 的惯例。\n\n测试套件：\n实现您的程序，为以下所有三个测试用例计算p值。每个测试用例指定 $(\\mathbf{C}, \\mathbf{x}, \\mathbf{y}, k, B, s)$，其中 $\\mathbf{C}$ 是坐标列表，$\\mathbf{x}$ 是配体计数，$\\mathbf{y}$ 是受体计数，$k$ 是邻居数量，$B$ 是置换次数，而 $s$ 是用于初始化该测试用例的随机数生成器的随机种子。\n\n- 测试用例 $1$（带有移位基序的直线；预期有强正向共定位）：\n  - $n = 12$，坐标 $\\mathbf{c}_i = (i-1, 0)$ 对于 $i \\in \\{1,\\dots,12\\}$，即 $\\big[(0,0),(1,0),(2,0),(3,0),(4,0),(5,0),(6,0),(7,0),(8,0),(9,0),(10,0),(11,0)\\big]$。\n  - 配体计数 $\\mathbf{x} = [\\,1,\\,1,\\,2,\\,3,\\,5,\\,8,\\,13,\\,8,\\,5,\\,3,\\,2,\\,1\\,]$。\n  - 受体计数 $\\mathbf{y} = [\\,1,\\,2,\\,3,\\,5,\\,8,\\,13,\\,8,\\,5,\\,3,\\,2,\\,1,\\,1\\,]$。\n  - $k = 2$，$B = 200$，种子 $s = 12345$。\n\n- 测试用例 $2$（具有异构计数的二维网格；预期行为接近零假设）：\n  - $n = 20$，坐标由 $4 \\times 5$ 网格 $\\{(i,j)\\,:\\, i \\in \\{0,1,2,3\\},\\, j \\in \\{0,1,2,3,4\\}\\}$ 按字典序 $(i,j)$ 给出，即\n    $\\big[(0,0),(0,1),(0,2),(0,3),(0,4),(1,0),(1,1),(1,2),(1,3),(1,4),(2,0),(2,1),(2,2),(2,3),(2,4),(3,0),(3,1),(3,2),(3,3),(3,4)\\big]$。\n  - 配体计数 $\\mathbf{x} = [\\,3,\\,7,\\,0,\\,5,\\,2,\\,6,\\,1,\\,8,\\,4,\\,9,\\,2,\\,3,\\,7,\\,0,\\,6,\\,5,\\,4,\\,2,\\,1,\\,8\\,]$。\n  - 受体计数 $\\mathbf{y} = [\\,5,\\,2,\\,6,\\,1,\\,3,\\,0,\\,9,\\,2,\\,7,\\,4,\\,8,\\,1,\\,0,\\,6,\\,2,\\,3,\\,5,\\,7,\\,4,\\,0\\,]$。\n  - $k = 4$，$B = 200$，种子 $s = 2021$。\n\n- 测试用例 $3$（具有反相关分布的直线；预期有负向共定位）：\n  - $n = 12$，坐标 $\\mathbf{c}_i = (i-1, 0)$ 对于 $i \\in \\{1,\\dots,12\\}$，与测试用例 $1$ 相同。\n  - 配体计数 $\\mathbf{x} = [\\,0,\\,1,\\,2,\\,3,\\,4,\\,5,\\,6,\\,7,\\,8,\\,9,\\,10,\\,11\\,]$。\n  - 受体计数 $\\mathbf{y} = [\\,11,\\,10,\\,9,\\,8,\\,7,\\,6,\\,5,\\,4,\\,3,\\,2,\\,1,\\,0\\,]$。\n  - $k = 2$，$B = 200$，种子 $s = 7$。\n\n您的程序必须生成单行输出，其中包含对应于三个测试用例的三个p值，格式为用方括号括起来的逗号分隔列表（例如，$[0.03125,0.5,0.992]$）。不应打印任何额外文本。此问题中没有物理单位或角度。所有输出必须是标准十进制表示的实数。", "solution": "问题陈述经过严格验证，确认有效。其科学基础植根于空间转录组学和统计假设检验的原理，问题定义良好，具有精确的数学定义和约束，并且是客观的。不存在会妨碍得出唯一且可验证解的矛盾、歧义或信息缺失。\n\n任务是根据模拟的空间转录组学数据，计算配体和受体正向共定位的单边经验p值。解决方案涉及实现一个基于空间互相关统计量的条件置换检验。该方法论针对三个不同的测试用例执行。\n\n对于每个测试用例，算法分为四个主要阶段：\n1.  识别每个点的空间邻居。\n2.  计算观测到的检验统计量 $T_{\\mathrm{obs}}$。\n3.  通过置换生成检验统计量的零分布。\n4.  计算经验单边p值。\n\n首先，我们必须为每个点识别其邻居集合。给定 $n$ 个点，其坐标为 $\\mathbf{c}_i \\in \\mathbb{R}^2$（其中 $i \\in \\{1,\\dots,n\\}$），以及一个指定的整数 $k$，点 $i$ 的邻居集合 $\\mathcal{N}(i)$ 由与 $i$ 的欧几里得距离 $\\|\\mathbf{c}_i - \\mathbf{c}_j\\|_2$ 最近的 $k$ 个不同点 $j \\neq i$ 组成。问题指定了一个打破平局的规则：如果多个点等距，则按其索引 $j$ 的升序优先。此过程的实现方法是，首先计算所有点之间的成对欧几里得距离矩阵。然后，对于每个点 $i$，我们创建一个元组列表，每个元组包含所有其他点 $j$ 的距离和索引。主要按距离、次要按索引对此列表进行排序，可以明确地选择前 $k$ 个索引，这些索引构成了邻居 $\\mathcal{N}(i)$。\n\n其次，计算观测到的检验统计量 $T_{\\mathrm{obs}}$。该统计量被定义为配体表达向量 $\\{x_i\\}_{i=1}^n$ 与邻居平均受体表达向量 $\\{m_i\\}_{i=1}^n$ 之间的 Pearson 相关系数。每个点 $i$ 的值 $m_i$ 计算为该点邻居 $j \\in \\mathcal{N}(i)$ 的受体计数 $\\{y_j\\}$ 的算术平均值：\n$$\nm_i = \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j \\in \\mathcal{N}(i)} y_j\n$$\n在确定了向量 $\\{x_i\\}$ 和 $\\{m_i\\}$ 后，使用以下公式计算它们的 Pearson 相关性：\n$$\nT_{\\mathrm{obs}} \\;=\\; \\frac{\\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)\\left(m_i - \\bar{m}\\right)}{\\sqrt{\\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)^2 \\sum_{i=1}^n \\left(m_i - \\bar{m}\\right)^2}}\n$$\n其中 $\\bar{x}$ 和 $\\bar{m}$ 分别是样本均值。根据问题规范，如果向量 $\\{x_i\\}$ 或 $\\{m_i\\}$ 的方差为零，则相关性 $T_{\\mathrm{obs}}$ 定义为 $0$。\n\n第三，我们为检验统计量构建一个经验零分布。零假设指出，在给定每个点的总文库大小 $t_i = x_i + y_i$ 的条件下，配体和受体标签是可交换的。这通过置换方案进行模拟。对于指定的置换次数 $B$，我们生成 $B$ 个置换数据集。在每次置換 $b \\in \\{1, \\dots, B\\}$ 中，对于每个点 $i$，我们以1/2的概率随机交换配体和受体计数 $x_i$ 和 $y_i$。这是通过抽取 $Z_i^{(b)} \\sim \\mathrm{Bernoulli}(1/2)$ 并相应地设置置换计数 $x_i^{(b)}$ 和 $y_i^{(b)}$ 来实现的。此过程保持每个点的文库大小 $t_i$ 不变。对于每个置换数据集，使用置换后的计数 $\\{x_i^{(b)}\\}$ 和相应的邻居平均置换受体计数 $\\{m_i^{(b)}\\}$ 计算一个新的检验统计量 $T^{(b)}$。\n\n最后，计算正向共定位的经验单边p值。该p值量化了在零假设下观测到至少与 $T_{\\mathrm{obs}}$ 一样大的检验统计量的概率。其计算公式为：\n$$\np \\;=\\; \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\!\\left\\{\\, T^{(b)} \\ge T_{\\mathrm{obs}} \\,\\right\\}}{B + 1}\n$$\n在分子和分母中都包含1，是为了将观测到的统计量本身视为零分布中的一个值。\n\n该实现利用 `NumPy` 进行高效的向量化操作，特别是在计算均值、标准差以及生成和应用置换时。使用 `SciPy` 库来高效计算成对距离矩阵。为确保可复现性，每个测试用例都设定了随机数生成器的种子。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ndef find_neighbors(coords, k):\n    \"\"\"\n    Finds the k-nearest neighbors for each spot.\n\n    Args:\n        coords (np.ndarray): An (n, 2) array of spot coordinates.\n        k (int): The number of neighbors to find.\n\n    Returns:\n        list: A list of lists, where the i-th list contains the indices\n              of the k-nearest neighbors for spot i.\n    \"\"\"\n    n_spots = coords.shape[0]\n    dist_mat = distance_matrix(coords, coords)\n    \n    neighbor_indices = []\n    for i in range(n_spots):\n        # Create a list of (distance, index) for all other spots\n        distances_to_others = []\n        for j in range(n_spots):\n            if i == j:\n                continue\n            distances_to_others.append((dist_mat[i, j], j))\n        \n        # Sort by distance, then by index for tie-breaking\n        distances_to_others.sort(key=lambda item: (item[0], item[1]))\n        \n        # Get the indices of the k-nearest neighbors\n        k_neighbors = [item[1] for item in distances_to_others[:k]]\n        neighbor_indices.append(k_neighbors)\n    \n    return neighbor_indices\n\ndef pearson_corr(x, y):\n    \"\"\"\n    Calculates the Pearson correlation, handling the zero-variance case.\n\n    Args:\n        x (np.ndarray): First vector.\n        y (np.ndarray): Second vector.\n\n    Returns:\n        float: The Pearson correlation coefficient. Returns 0 if either\n               vector has zero variance.\n    \"\"\"\n    std_x = np.std(x)\n    std_y = np.std(y)\n\n    if std_x == 0.0 or std_y == 0.0:\n        return 0.0\n    \n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    \n    cov = np.mean((x - mean_x) * (y - mean_y))\n    corr = cov / (std_x * std_y)\n    \n    return corr\n\ndef compute_p_value(C, x, y, k, B, s):\n    \"\"\"\n    Computes the empirical p-value for a single test case.\n\n    Args:\n        C (list or np.ndarray): Spot coordinates.\n        x (list or np.ndarray): Ligand counts.\n        y (list or np.ndarray): Receptor counts.\n        k (int): Number of neighbors.\n        B (int): Number of permutations.\n        s (int): Random seed.\n    \n    Returns:\n        float: The computed one-sided p-value.\n    \"\"\"\n    np.random.seed(s)\n    \n    coords = np.array(C)\n    ligand_counts = np.array(x, dtype=float)\n    receptor_counts = np.array(y, dtype=float)\n    n_spots = len(ligand_counts)\n    \n    # 1. Find neighbors\n    neighbor_idx_map = find_neighbors(coords, k)\n    neighbor_indices = np.array(neighbor_idx_map)\n    \n    # 2. Calculate observed statistic T_obs\n    m_obs = np.mean(receptor_counts[neighbor_indices], axis=1)\n    T_obs = pearson_corr(ligand_counts, m_obs)\n    \n    # 3. Permutation loop\n    extreme_count = 0\n    for _ in range(B):\n        # Generate permuted data for this iteration\n        swaps = np.random.randint(0, 2, size=n_spots, dtype=bool)\n        x_perm = np.where(swaps, receptor_counts, ligand_counts)\n        y_perm = np.where(swaps, ligand_counts, receptor_counts)\n        \n        # Calculate permuted statistic T_perm\n        m_perm = np.mean(y_perm[neighbor_indices], axis=1)\n        T_perm = pearson_corr(x_perm, m_perm)\n        \n        if T_perm >= T_obs:\n            extreme_count += 1\n            \n    # 4. Calculate p-value\n    p_value = (1.0 + extreme_count) / (B + 1.0)\n    \n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test case 1\n    n1 = 12\n    C1 = [(i, 0) for i in range(n1)]\n    x1 = [1, 1, 2, 3, 5, 8, 13, 8, 5, 3, 2, 1]\n    y1 = [1, 2, 3, 5, 8, 13, 8, 5, 3, 2, 1, 1]\n    k1, B1, s1 = 2, 200, 12345\n\n    # Test case 2\n    C2 = [(i, j) for i in range(4) for j in range(5)]\n    x2 = [3, 7, 0, 5, 2, 6, 1, 8, 4, 9, 2, 3, 7, 0, 6, 5, 4, 2, 1, 8]\n    y2 = [5, 2, 6, 1, 3, 0, 9, 2, 7, 4, 8, 1, 0, 6, 2, 3, 5, 7, 4, 0]\n    k2, B2, s2 = 4, 200, 2021\n    \n    # Test case 3\n    n3 = 12\n    C3 = [(i, 0) for i in range(n3)]\n    x3 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n    y3 = [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n    k3, B3, s3 = 2, 200, 7\n    \n    test_cases = [\n        (C1, x1, y1, k1, B1, s1),\n        (C2, x2, y2, k2, B2, s2),\n        (C3, x3, y3, k3, B3, s3),\n    ]\n\n    results = []\n    for C, x, y, k, B, s in test_cases:\n        p_val = compute_p_value(C, x, y, k, B, s)\n        results.append(p_val)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2890015"}]}