{"hands_on_practices": [{"introduction": "模型选择的核心在于平衡模型的拟合优度（由对数似然值 $\\ln \\hat{L}$ 衡量）与模型的复杂性（由自由参数数量 $k$ 表示）。本练习提供了一个直接的实践机会，让您将赤池信息准则（AIC）和贝叶斯信息准则（BIC）的公式应用于一个真实的系统发育场景。通过这个练习，您将巩固对这些关键准则基本计算方法的理解，并增强在实际研究中执行这些重要计算的信心。[@problem_id:2734801]", "problem": "一个长度为 $5000$ 个位点的核苷酸序列比对，在一个关联 $12$ 个分类单元的固定、完全解析的无根拓扑结构上进行了分析。对于一个具有 $m$ 个分类单元的无根二叉系统发育树，有 $2m - 3$ 个分支，每个分支都有一个独立的分支长度参数。在五个时间同质、时间可逆的替换模型下获得的最大对数似然值（自然对数）如下所示，所有值都是在相同的拓扑结构和序列比对上估计的：\n\n- JC69: $\\ln \\hat{L}_{\\mathrm{JC69}} = -7423.6$\n- HKY: $\\ln \\hat{L}_{\\mathrm{HKY}} = -7089.2$\n- GTR: $\\ln \\hat{L}_{\\mathrm{GTR}} = -7012.8$\n- GTR+G（四个离散伽马类别，具有单个形状参数）: $\\ln \\hat{L}_{\\mathrm{GTR+G}} = -6844.5$\n- GTR+G+I（伽马分布如上，外加一个不变位点的比例）: $\\ln \\hat{L}_{\\mathrm{GTR+G+I}} = -6839.7$\n\n假设核苷酸模型使用标准参数化：JC69 模型具有相等的碱基频率和相等的可交换性，除了分支长度外，不贡献任何自由的替换过程参数；HKY 模型增加了一个转换/颠换速率比参数 $\\kappa$ 和三个独立的碱基频率参数；GTR 模型有五个独立的可交换性参数和三个独立的碱基频率参数；$+G$ 扩展增加了一个伽马形状参数 $\\alpha$；$+I$ 扩展增加了一个不变位点比例的参数。离散伽马类别的数量是固定的，不增加参数。\n\n使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的经典定义，其中每个准则都在最大化对数似然值与一个惩罚项之间进行权衡，该惩罚项随自由参数数量的增加而增加，对于 BIC，还随样本大小（此处为序列比对位点的数量）的增加而增加，请完成以下任务：\n\n1. 确定每个模型的自由参数总数 $k$，包括所有分支长度和模型特定参数。\n2. 计算每个模型的 AIC 和 BIC。\n3. 根据 AIC 和 BIC 确定最优模型（即每个准则值最小的模型）。\n4. 作为您的最终数值答案，报告 AIC 最优模型的赤池权重（基于 AIC 的归一化相对似然），以小数形式表示。将您的最终数值答案四舍五入到四位有效数字。", "solution": "该问题要求使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 对五个嵌套的核苷酸替换模型进行比较分析。目标是确定拟合最好的模型，并通过其赤池权重来量化其支持度。\n\n首先，我们必须定义这些准则。AIC 由以下公式给出：\n$$\nAIC = 2k - 2\\ln \\hat{L}\n$$\n其中 $k$ 是模型中的自由参数数量，$\\ln \\hat{L}$ 是最大化的自然对数似然值。BIC 由以下公式给出：\n$$\nBIC = k \\ln(n) - 2\\ln \\hat{L}\n$$\n其中 $n$ 是样本大小，在此上下文中是核苷酸序列比对中的位点数，给定为 $n = 5000$。\n\n第一步是正确确定五个模型中每个模型的自由参数总数 $k$。参数总数是分支长度参数数量和替换过程参数数量之和。\n对于一个有 $m$ 个分类单元的无根二叉系统发育树，分支数量为 $2m - 3$。在这里，有 $m = 12$ 个分类单元，分支数量为 $2(12) - 3 = 21$。每个分支都有一个长度参数，因此所有模型都共有 $k_{branches} = 21$ 个参数。\n\n我们现在确定每个模型如前所述的替换过程参数数量 ($k_{subst}$):\n- **JC69**：该模型假设碱基频率相等且只有一个替换速率。它没有用于替换过程的自由参数。因此，$k_{subst, \\mathrm{JC69}} = 0$。\n- **HKY**：该模型引入了一个转换/颠换速率比参数（$\\kappa$）并允许不相等的碱基频率。由于四个碱基频率之和必须为 $1$，因此有 $3$ 个自由的频率参数。因此，$k_{subst, \\mathrm{HKY}} = 1 + 3 = 4$。\n- **GTR**：这是一般时间可逆模型。它有 $6$ 个可交换性参数，但是它们是相对的，所以只有 $5$ 个是自由的。它还有 $3$ 个自由的碱基频率参数。因此，$k_{subst, \\mathrm{GTR}} = 5 + 3 = 8$。\n- **GTR+G**：该模型为跨位点速率的伽马分布增加了一个参数，即形状参数 $\\alpha$。因此，$k_{subst, \\mathrm{GTR+G}} = k_{subst, \\mathrm{GTR}} + 1 = 8 + 1 = 9$。\n- **GTR+G+I**：该模型增加了一个不变位点比例的参数 $p_{inv}$。因此，$k_{subst, \\mathrm{GTR+G+I}} = k_{subst, \\mathrm{GTR+G}} + 1 = 9 + 1 = 10$。\n\n每个模型的参数总数（$k$）是：\n- $k_{\\mathrm{JC69}} = 21 + 0 = 21$\n- $k_{\\mathrm{HKY}} = 21 + 4 = 25$\n- $k_{\\mathrm{GTR}} = 21 + 8 = 29$\n- $k_{\\mathrm{GTR+G}} = 21 + 9 = 30$\n- $k_{\\mathrm{GTR+G+I}} = 21 + 10 = 31$\n\n有了 $k$、$\\ln \\hat{L}$ 和 $n=5000$ 的值，我们就可以计算每个模型的 AIC 和 BIC。\n\n1. **JC69**: $k = 21$, $\\ln \\hat{L} = -7423.6$\n   $AIC_{\\mathrm{JC69}} = 2(21) - 2(-7423.6) = 42 + 14847.2 = 14889.2$\n   $BIC_{\\mathrm{JC69}} = 21\\ln(5000) - 2(-7423.6) \\approx 21(8.5172) + 14847.2 = 178.8612 + 14847.2 = 15026.0612$\n\n2. **HKY**: $k = 25$, $\\ln \\hat{L} = -7089.2$\n   $AIC_{\\mathrm{HKY}} = 2(25) - 2(-7089.2) = 50 + 14178.4 = 14228.4$\n   $BIC_{\\mathrm{HKY}} = 25\\ln(5000) - 2(-7089.2) \\approx 25(8.5172) + 14178.4 = 212.9300 + 14178.4 = 14391.3300$\n\n3. **GTR**: $k = 29$, $\\ln \\hat{L} = -7012.8$\n   $AIC_{\\mathrm{GTR}} = 2(29) - 2(-7012.8) = 58 + 14025.6 = 14083.6$\n   $BIC_{\\mathrm{GTR}} = 29\\ln(5000) - 2(-7012.8) \\approx 29(8.5172) + 14025.6 = 246.9988 + 14025.6 = 14272.5988$\n\n4. **GTR+G**: $k = 30$, $\\ln \\hat{L} = -6844.5$\n   $AIC_{\\mathrm{GTR+G}} = 2(30) - 2(-6844.5) = 60 + 13689.0 = 13749.0$\n   $BIC_{\\mathrm{GTR+G}} = 30\\ln(5000) - 2(-6844.5) \\approx 30(8.5172) + 13689.0 = 255.5160 + 13689.0 = 13944.5160$\n\n5. **GTR+G+I**: $k = 31$, $\\ln \\hat{L} = -6839.7$\n   $AIC_{\\mathrm{GTR+G+I}} = 2(31) - 2(-6839.7) = 62 + 13679.4 = 13741.4$\n   $BIC_{\\mathrm{GTR+G+I}} = 31\\ln(5000) - 2(-6839.7) \\approx 31(8.5172) + 13679.4 = 264.0332 + 13679.4 = 13943.4332$\n\n最优模型是使信息准则得分最小化的模型。\n- 对于 AIC，最小值为 $AIC_{min} = 13741.4$，对应于 **GTR+G+I** 模型。\n- 对于 BIC，最小值为 $BIC_{min} \\approx 13943.43$，也对应于 **GTR+G+I** 模型。\n\n最后，我们必须计算 AIC 下最优模型（即 GTR+G+I）的赤池权重。模型 $i$ 的赤池权重 ($w_i$) 是其在一组候选模型中相对支持度的度量。其计算公式为：\n$$\nw_i = \\frac{\\exp(-\\frac{1}{2}\\Delta_i)}{\\sum_{j=1}^{R} \\exp(-\\frac{1}{2}\\Delta_j)}\n$$\n其中 $\\Delta_i = AIC_i - AIC_{min}$ 且 $R=5$ 是模型数量。\n\n首先，我们计算每个模型相对于 $AIC_{min} = AIC_{\\mathrm{GTR+G+I}} = 13741.4$ 的 $\\Delta_i$ 值。\n- $\\Delta_{\\mathrm{JC69}} = 14889.2 - 13741.4 = 1147.8$\n- $\\Delta_{\\mathrm{HKY}} = 14228.4 - 13741.4 = 487.0$\n- $\\Delta_{\\mathrm{GTR}} = 14083.6 - 13741.4 = 342.2$\n- $\\Delta_{\\mathrm{GTR+G}} = 13749.0 - 13741.4 = 7.6$\n- $\\Delta_{\\mathrm{GTR+G+I}} = 13741.4 - 13741.4 = 0$\n\n接下来，我们计算最优模型（GTR+G+I）的赤池权重的分子：\n$\\exp(-\\frac{1}{2}\\Delta_{\\mathrm{GTR+G+I}}) = \\exp(-\\frac{1}{2} \\times 0) = \\exp(0) = 1$。\n\n现在，我们计算分母，即所有模型相对似然值的总和：\n$\\sum_{j=1}^{5} \\exp(-\\frac{1}{2}\\Delta_j) = \\exp(-\\frac{1147.8}{2}) + \\exp(-\\frac{487.0}{2}) + \\exp(-\\frac{342.2}{2}) + \\exp(-\\frac{7.6}{2}) + \\exp(-\\frac{0}{2})$\n$\\sum_{j} \\exp(-\\frac{1}{2}\\Delta_j) = \\exp(-573.9) + \\exp(-243.5) + \\exp(-171.1) + \\exp(-3.8) + \\exp(0)$\n前三项小到可以忽略不计，在数值上可以忽略。我们计算：\n$\\exp(-3.8) \\approx 0.0223708$\n$\\exp(0) = 1$\n所以，总和约为 $0 + 0 + 0 + 0.0223708 + 1 = 1.0223708$。\n\nGTR+G+I 模型的赤池权重为：\n$w_{\\mathrm{GTR+G+I}} = \\frac{1}{1.0223708} \\approx 0.978119$\n\n四舍五入到四位有效数字，AIC 最优模型的赤池权重为 $0.9781$。这表明，在给定数据和候选模型集的情况下，GTR+G+I 模型在 Kullback-Leibler 意义上是最佳模型的概率约为 $97.81\\%$。", "answer": "$$\\boxed{0.9781}$$", "id": "2734801"}, {"introduction": "在上一个练习的基础上，我们需要认识到系统发育中的模型选择通常包含两个截然不同的任务：为固定的拓扑结构选择一个替换模型，或者在固定的模型下选择一个树的拓扑结构。这个练习旨在阐明信息准则在这两种情况下是如何被不同地应用的，特别是关于惩罚项的使用，这是一个常见的混淆点。通过解决这个问题，您将能够更清晰地分辨和处理系统发育分析中不同层面的模型选择问题。[@problem_id:2734810]", "problem": "一个包含$N=10$个分类单元的多序列比对有$n=2000$个位点。在给定系统发育树和替换模型的条件下，这些位点被假设为在各站点间独立同分布。考虑两个不同但密切相关的模型选择任务：\n\n- 任务1（在固定拓扑上的替换模型选择）：无根、完全二分叉的树拓扑$T^\\star$保持固定。你将比较三种在参数化上有所不同的时间可逆、平稳、同质的替换过程模型。对于每个模型$M_i$，令$k_i$表示自由参数的总数，包括所有分支长度和所有替换过程参数。候选模型及其最大化对数似然值如下：\n  - $M_1=\\text{JC69}$，其中$k_1=17$且$\\ell_1=\\log L(M_1,T^\\star \\mid \\text{data})=-5500.0$。\n  - $M_2=\\text{HKY}$，其中$k_2=21$且$\\ell_2=\\log L(M_2,T^\\star \\mid \\text{data})=-5205.0$。\n  - $M_3=\\text{GTR}+\\Gamma+I$，其中$k_3=27$且$\\ell_3=\\log L(M_3,T^\\star \\mid \\text{data})=-5195.0$。\n\n- 任务2（在固定替换模型下的拓扑选择）：替换模型固定为$M_2=\\text{HKY}$。你将比较针对相同的$N=10$个分类单元的三种无根、完全二分叉的拓扑$T_A,T_B,T_C$。对于每种拓扑，所有分支长度和所有$M_2$的参数都进行了优化。因为这三种拓扑都是在相同分类单元集上、相同替换模型下的完全解析树，所以它们的总参数数量相同，即$k_A=k_B=k_C=21$。最大化对数似然值如下：\n  - $\\ell_A=\\log L(M_2,T_A \\mid \\text{data})=-5200.0$，\n  - $\\ell_B=\\log L(M_2,T_B \\mid \\text{data})=-5192.0$，\n  - $\\ell_C=\\log L(M_2,T_C \\mid \\text{data})=-5195.0$。\n\n仅使用基于似然的模型比较和信息准则的一般原理，选择最准确、最完整地完成以下任务的唯一选项：\n\n1) 阐明在固定拓扑上选择替换模型与在固定替换模型下选择树拓扑之间的概念区别，包括复杂度惩罚项在这两个任务中是如何以及为何以不同方式引入的。\n\n2) 根据上面给出的数值，正确说明在任务1中，赤池信息准则（AIC）和贝叶斯信息准则（BIC）分别选择了哪个候选模型；在任务2中，两种准则都选择了哪个拓扑，并说明为何在这种设置下两种准则对拓扑的排序结果相同。\n\nA. 在固定拓扑$T^\\star$下，三种替换模型$M_1,M_2,M_3$构成具有不同总参数数量$k_i$的独立统计模型，因此AIC和BIC都需要在拟合度的提高与增加的替换过程和位点间速率参数之间进行权衡。根据给定的$\\ell_i$和$n=2000$，AIC选择$M_3$，而BIC选择$M_2$。在固定的替换模型和针对相同分类单元的三种完全二分叉拓扑下，总参数数量$k$（所有分支长度加上替换参数）在$T_A,T_B,T_C$之间是相同的，因此惩罚项是相等的常数，AIC和BIC都简化为选择最大似然拓扑，此处为$T_B$。\n\nB. 贝叶斯信息准则应使用分类单元数$N=10$作为样本量，因此其惩罚项对更丰富的替换模型占主导地位；所以在任务1中，AIC和BIC都选择$M_1$。在任务2中，BIC在拓扑上偏好最平衡的拓扑，所以即使$T_A$的似然值较低，它也会选择$T_A$。\n\nC. 信息准则不能应用于像树拓扑这样的离散对象；只有似然比检验是合适的。因为$T_A$与$T_B$是非嵌套的，所以AIC和BIC都没有意义，应该改用简约法选择$T_A$。\n\nD. 在固定替换模型下比较拓扑时，参数数量$k$因树间的分支长度估计值不同而不同，因此BIC会惩罚总分支长度更长的树；根据给定的数值，它会选择$T_C$。在任务1中，AIC和BIC都必然选择$M_3$，因为它具有最高的似然值。\n\nE. 在固定拓扑下，根据似然值，AIC和BIC都选择最丰富的模型$M_3$；在固定替换模型下，两种准则都选择$T_B$；然而，由于即使模型固定，BIC的惩罚项在不同拓扑之间也会变化，因此原则上它可以逆转具有相同分类单元数的完全二分叉树之间的似然排序。", "solution": "我们从用于系统发育推断的标准似然框架开始，该框架基于时间可逆、平稳、同质的替换模型和位点独立性假设。给定一个有$n$个位点的比对和一个候选模型（由一个替换过程参数化以及一个树拓扑及其分支长度组成），似然值为\n$$\nL(\\text{model} \\mid \\text{data}) \\;=\\; \\prod_{i=1}^{n} p\\big(X_i \\mid \\text{model}\\big),\n$$\n其中$X_i$是位点$i$的位点模式，$p(\\cdot \\mid \\text{model})$通过Felsenstein的剪枝算法计算，该算法对未观察到的祖先状态进行积分。对数似然在各位点间是可加的：\n$$\n\\ell \\;=\\; \\log L \\;=\\; \\sum_{i=1}^{n} \\log p\\big(X_i \\mid \\text{model}\\big).\n$$\n\n信息准则提供了有原则的、近似的方法，用于在数据拟合与模型复杂度之间进行权衡。赤池信息准则（AIC）和贝叶斯信息准则（BIC）定义如下\n$$\n\\mathrm{AIC} \\;=\\; -2 \\ell \\;+\\; 2k, \n\\qquad\n\\mathrm{BIC} \\;=\\; -2 \\ell \\;+\\; k \\log n,\n$$\n其中$k$是模型的自由参数总数，$n$是样本量。在常规的位点独立假设下，系统发育学中BIC的合适样本量$n$等于比对的位点数。值越小表示模型越优。\n\n从这些定义中可以直接得出两个概念要点：\n\n- 在任务1中，拓扑$T^\\star$是固定的，因此$M_1,M_2,M_3$之间$k$的差异来自于替换过程的参数化（例如，可交换性速率、碱基频率、位点间速率变异参数），而分支长度参数的数量在各个$M_i$之间是相同的。因此，AIC和BIC将在不同替换模型间权衡$\\ell$的提高与$k$的增加。\n\n- 在任务2中，替换模型固定为$M_2$，并且三种拓扑$T_A,T_B,T_C$都是针对相同$N=10$个分类单元的无根、完全二分叉树。这样的树都具有相同数量的分支长度参数，即$2N-3=17$，以及相同数量的替换过程参数（$M_2$的参数），因此在候选拓扑之间总的$k$是相同的。因此，惩罚项$2k$（对于AIC）和$k \\log n$（对于BIC）在$T_A,T_B,T_C$之间是相等的常数，两种准则都简化为仅按$\\ell$对拓扑进行排序，即选择最大似然（ML）拓扑。\n\n我们现在进行所要求的计算。\n\n任务1（固定拓扑$T^\\star$，比较$M_1,M_2,M_3$）：\n\n- 为每个模型计算$-2\\ell$：\n  - $-2\\ell_1 = -2(-5500.0) = 11000.0$,\n  - $-2\\ell_2 = -2(-5205.0) = 10410.0$,\n  - $-2\\ell_3 = -2(-5195.0) = 10390.0$。\n\n- AIC值：\n  - $\\mathrm{AIC}(M_1) = 11000.0 + 2 \\times 17 = 11034.0$,\n  - $\\mathrm{AIC}(M_2) = 10410.0 + 2 \\times 21 = 10452.0$,\n  - $\\mathrm{AIC}(M_3) = 10390.0 + 2 \\times 27 = 10444.0$。\n  最小的是$\\mathrm{AIC}(M_3)=10444.0$，所以AIC选择$M_3$。\n\n- BIC值（$n=2000$个位点，$\\log n = \\log 2000 \\approx 7.600902$）：\n  - 惩罚项：$k_1 \\log n \\approx 17 \\times 7.600902 \\approx 129.215$, $k_2 \\log n \\approx 21 \\times 7.600902 \\approx 159.619$, $k_3 \\log n \\approx 27 \\times 7.600902 \\approx 205.224$。\n  - $\\mathrm{BIC}(M_1) \\approx 11000.0 + 129.215 = 11129.215$,\n  - $\\mathrm{BIC}(M_2) \\approx 10410.0 + 159.619 = 10569.619$,\n  - $\\mathrm{BIC}(M_3) \\approx 10390.0 + 205.224 = 10595.224$。\n  最小的是$\\mathrm{BIC}(M_2)\\approx 10569.619$，所以BIC选择$M_2$。\n\n因此，在任务1中，两种准则按预期产生了分歧：AIC的参数惩罚不随$n$缩放，因此更宽容，选择了更丰富的$M_3$；而BIC的惩罚随$\\log n$增长，在$n=2000$时更保守，选择了中等复杂度的$M_2$。\n\n任务2（固定模型$M_2$，比较拓扑$T_A,T_B,T_C$）：\n\n- 计算$-2\\ell$：\n  - $-2\\ell_A = -2(-5200.0) = 10400.0$,\n  - $-2\\ell_B = -2(-5192.0) = 10384.0$,\n  - $-2\\ell_C = -2(-5195.0) = 10390.0$。\n  由于$k_A=k_B=k_C=21$，AIC和BIC在不同拓扑之间的差异仅在于$-2\\ell$，因此两者都选择$\\ell$最大（$-2\\ell$最小）的拓扑，即$T_B$。\n\n逐项分析选项：\n\n- 选项A：该选项正确阐明了概念上的区别：在任务1中，不同替换模型间$k$的差异驱动了惩罚项的权衡；而在任务2中，对于相同分类单元在固定替换模型下的完全二分叉树，$k$在不同拓扑间是相同的，因此惩罚项是相等的常数，两种准则都简化为ML排序。它还根据给定的数值正确地确定了所选模型：在任务1中，AIC选择$M_3$，BIC选择$M_2$；在任务2中，两者都选择$T_B$。结论 — 正确。\n\n- 选项B：该选项断言BIC应将分类单元数$N=10$作为样本量。在位点独立模型下，合适的$n$是位点数，而不是分类单元数。它还声称AIC和BIC都选择$M_1$，这与上面的明确计算相矛盾。它还援引“平衡拓扑”作为BIC的偏好，但这并非信息准则的运作方式。结论 — 不正确。\n\n- 选项C：该选项声称信息准则不能应用于像树拓扑这样的离散对象，并建议改用似然比检验。AIC和BIC都是明确设计用于比较任何有限集合的已拟合模型的，包括像树拓扑这样的离散备选方案。此外，似然比检验要求模型具有嵌套关系，而不同的完全解析拓扑之间通常不满足此条件；信息准则正是用于比较这类非嵌套模型的。结论 — 不正确。\n\n- 选项D：该选项声称$k$在不同拓扑间不同，因为分支长度的估计值不同。估计值不同，但自由参数的数量$k$不变：所有针对相同分类单元集的无根完全二分叉树都有$2N-3$个分支长度，因此$k$值相同。它还断言BIC会惩罚更长的总分支长度，这是错误的；惩罚项取决于参数数量，而不是参数的大小。它声称在任务1中AIC和BIC“必然选择$M_3$”，这与上面的BIC计算相矛盾。结论 — 不正确。\n\n- 选项E：虽然它在任务2中正确地确定了$T_B$，但它错误地断言在任务1中AIC和BIC都选择$M_3$。它还声称，即使模型和分类单元数相同，BIC的惩罚项在不同的完全二分叉拓扑之间也会变化，这在这种设置下是错误的，因为$k$在$T_A,T_B,T_C$之间是相同的。结论 — 不正确。\n\n因此，只有选项A是正确的。", "answer": "$$\\boxed{A}$$", "id": "2734810"}, {"introduction": "现在，我们转向一个更高级、也更具警示意义的案例，它揭示了更复杂的模型并非总是更优选择。本模拟练习将演示一个关键概念：一个参数过多的（复杂的）模型，有时在*错误*的树拓扑结构上，其似然值甚至会高于一个简单模型在*正确*的拓扑结构上的似然值。这个实践突显了模型错误指定（model misspecification）的潜在危险，提醒我们在解释模型选择结果时需要保持批判性思维。[@problem_id:2406821]", "problem": "你的任务是研究系统发育学中模型选择的一个基本问题：一个具有不正确树拓扑的复杂替换模型，是否能比一个在正确拓扑上的简单替换模型获得更高的最大似然值？请在固定树拓扑上的时间可逆核苷酸替换模型的框架内进行研究，并应用树上的连续时间马尔可夫链（CTMC）的标准似然函数。\n\n从以下基础开始：\n- 沿每个分支的核苷酸替换过程是一个CTMC，其瞬时速率矩阵记为 $Q$，沿持续时间为 $t$ 的分支的转移概率由 $P(t) = \\exp(Qt)$ 给出，其中 $\\exp$ 表示矩阵指数。\n- 在时间可逆模型下，一个固定有根二叉树上的比对序列的似然值可以通过Felsenstein的剪枝算法计算得出，该算法使用根节点的平稳分布 $\\boldsymbol{\\pi}$ 和每个分支的转移概率。对于一个位点，其似然值是在所有根节点状态上求和，求和项为该状态的平稳概率乘以从叶节点向上传播的条件似然的乘积。\n- 为模拟位点间的速率变异，使用具有 $K$ 个类别的离散伽马近似：单位点速率 $r$ 从形状参数为 $\\alpha$ 且均值为 $1$ 的伽马分布中抽取，并用 $K$ 个具有固定速率 $\\{r_k\\}_{k=1}^K$ 的等概率类别来近似。总的位点似然值是所有类别的平均值。\n\n你的任务是实现一个模拟和似然评估，以证明在某些情况下，一个在错误拓扑上的具有伽马分布的位点间速率异质性的通用时间可逆（GTR）模型（GTR+$\\Gamma$），能够比一个在正确拓扑上的Jukes-Cantor 1969（JC69）模型获得更高的对数似然值。\n\n请遵循以下规范。\n\n1) 树拓扑和分支长度。\n- 使用标记为 $\\{A,B,C,D\\}$ 的 $4$ 个分类单元，并将树表示为有根的严格二叉树，其根位于一个内部分支点上，该分支点分裂成两个内部分支点，每个内部分支点再连接两个叶节点。在时间可逆性下，根的位置是任意的，但在模拟和似然计算中必须保持一致。\n- “正确”拓扑为 $((A,B),(C,D))$。“错误”拓扑为 $((A,C),(B,D))$。\n- 每个有根树有 $6$ 个分支：从根到左侧内部分支点（$\\ell_{RL}$），从根到右侧内部分支点（$\\ell_{RR}$），以及四个连接到叶节点的悬垂分支（$\\ell_A,\\ell_B,\\ell_C,\\ell_D$）。分支长度向量为 $\\mathbf{\\ell} = [\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_{RL},\\ell_{RR}]$。\n- 在错误拓扑上评估似然值时，请重用相同的分支长度向量 $\\mathbf{\\ell}$，即将相同的悬垂分支长度分配给相应的叶节点，并将相同的两个从根到内部分支点的长度分配给两个内部边。\n\n2) 替换模型。\n- 简单模型是JC69，具有相等的碱基频率和相等的替换速率，经过缩放以使单位时间的预期替换率为 $1$。也就是说，对于 $i \\neq j$，$Q_{ij} = \\frac{1}{3}$ 且 $Q_{ii} = -1$，平稳分布为 $\\boldsymbol{\\pi}_{JC} = [\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}]$。\n- 复杂模型是GTR+$\\Gamma$。使用一个通用时间可逆（GTR）速率矩阵，其平稳碱基频率为 $\\boldsymbol{\\pi} = [\\pi_A,\\pi_C,\\pi_G,\\pi_T] = [0.36, 0.14, 0.24, 0.26]$，以及针对无序核苷酸对的对称可交换性参数：\n  - $r_{AC} = 0.7$, $r_{AG} = 2.0$, $r_{AT} = 0.3$, $r_{CG} = 0.5$, $r_{CT} = 1.5$, $r_{GT} = 0.7$。\n  - 对于 $i \\neq j$，$Q_{ij} = r_{ij}\\,\\pi_j$，$Q_{ii} = -\\sum_{j\\neq i} Q_{ij}$。缩放 $Q$ 以使平均替换率为 $1$，即 $-\\sum_i \\pi_i Q_{ii} = 1$。使用 $\\boldsymbol{\\pi}$ 作为根节点的平稳分布。\n- 对于位点间的伽马分布速率，使用 $K=4$ 个类别，形状参数为 $\\alpha$，均值为 $1$；通过 $\\mathrm{Gamma}(\\alpha,\\text{scale} = \\frac{1}{\\alpha})$ 分布的中位分位数来近似类别速率 $\\{r_k\\}$，并重新缩放它们，以使 $\\frac{1}{K}\\sum_{k=1}^K r_k = 1$。\n\n3) 比对序列的模拟。\n- 对于每个测试用例，在正确拓扑上，使用指定的 $\\boldsymbol{\\pi}$、可交换性参数 $\\{r_{ij}\\}$、形状参数 $\\alpha$ 和分支长度向量 $\\mathbf{\\ell}$，在GTR+$\\Gamma$ 模型下模拟一个包含 $L$ 个独立同分布位点的比对序列。对于每个位点，从 $K$ 个类别中均匀抽取一个速率类别，并通过从根节点（从平稳分布 $\\boldsymbol{\\pi}$ 中抽取）沿树向下传播，在每个分支上使用 $P(t) = \\exp(Q r_k t)$，来生成叶节点的核苷酸状态。\n\n4) 用于比较的似然评估。\n- 在JC69模型（无伽马）下，使用与模拟相同的分支长度向量 $\\mathbf{\\ell}$，评估在正确拓扑上的对数似然值。将此值表示为 $\\mathcal{L}_{\\text{JC},\\text{correct}}$。\n- 在GTR+$\\Gamma$模型下，使用真实的 $\\boldsymbol{\\pi}$、$\\{r_{ij}\\}$ 以及相同的 $\\alpha$ 和 $K=4$，使用相同的分支长度向量 $\\mathbf{\\ell}$，评估在错误拓扑上的对数似然值。将此值表示为 $\\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}}$。\n- 使用Felsenstein的剪枝算法，并在内部分支点进行数值稳定的缩放。对于伽马混合模型，通过对各类别求平均来合并单位点似然值。\n\n5) 输出和决策规则。\n- 对于每个测试用例，计算差值 $\\Delta = \\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}} - \\mathcal{L}_{\\text{JC},\\text{correct}}$，然后通过测试 $\\Delta > 0$ 是否成立将其转换为布尔值答案。\n- 程序应输出单行文本，其中包含一个布尔值列表，每个测试用例对应一个布尔值，按下面给出的顺序排列，格式为Python风格的列表，例如：“[True,False,True]”。\n\n6) 测试套件。\n使用以下三个测试用例，每个用例都由一个随机种子、比对长度 $L$、伽马形状参数 $\\alpha$ 和分支长度向量 $\\mathbf{\\ell} = [\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_{RL},\\ell_{RR}]$ 完全指定：\n- 用例1（短内部分支，强速率变异）：种子 $= 7$，$L = 500$，$\\alpha = 0.3$，$\\mathbf{\\ell} = [0.3, 0.3, 0.3, 0.3, 0.01, 0.01]$。\n- 用例2（中等内部分支，强速率变异）：种子 $= 13$，$L = 1500$，$\\alpha = 0.3$，$\\mathbf{\\ell} = [0.3, 0.3, 0.3, 0.3, 0.2, 0.2]$。\n- 用例3（异质悬垂分支长度，中等速率变异）：种子 $= 23$，$L = 2000$，$\\alpha = 1.5$，$\\mathbf{\\ell} = [0.5, 0.2, 0.5, 0.2, 0.4, 0.4]$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含按三个测试用例顺序排列的结果，格式为逗号分隔的Python风格布尔值列表，例如：“[True,False,True]”。", "solution": "所提出的问题是对分子系统发育学中模型错误指定现象的有效探究，这是计算生物学中的一个核心重要课题。它旨在探究一个在不正确树拓扑上的复杂替换模型（GTR+$\\Gamma$）是否能比一个在正确拓扑上的更简单模型（JC69）产生更高的数据似然值。这是一个定义明确、有科学依据的问题，并且已经为计算实验提供了所有必要的参数和程序。因此，我们将着手提供一个正式的解决方案。\n\n该解决方案包括三个主要部分：定义系统发育模型和树结构，在指定场景下模拟序列数据，以及在两个相互竞争的假设下计算此数据的对数似然值。\n\n**1. 系统发育模型与连续时间马尔可夫链**\n\n沿系统发育树分支的单个位点上核苷酸的进化被建模为连续时间马尔可夫链（CTMC）。该过程由一个 $4 \\times 4$ 的瞬时速率矩阵 $Q$ 定义，其中元素 $Q_{ij}$（$i \\neq j$）表示从核苷酸 $i$ 替换为核苷酸 $j$ 的速率。对角线元素定义为 $Q_{ii} = -\\sum_{j\\neq i} Q_{ij}$。状态对应四种核苷酸，我们将其索引为 $\\{A \\to 0, C \\to 1, G \\to 2, T \\to 3\\}$。\n\n在长度（持续时间）为 $t$ 的分支上，从状态 $i$ 变为状态 $j$ 的概率由矩阵指数 $P(t) = \\exp(Qt)$ 给出，其中 $(P(t))_{ij}$ 是所需的转移概率。所有模型都假定为时间可逆的，这意味着对于所有 $i, j$，$\\pi_i Q_{ij} = \\pi_j Q_{ji}$ 成立，其中 $\\boldsymbol{\\pi} = [\\pi_A, \\pi_C, \\pi_G, \\pi_T]$ 是该过程的平稳分布。\n\n**1.1. Jukes-Cantor 1969 (JC69) 模型**\n\n这是最简单的替换模型。它假设碱基频率相等，替换速率也相等。\n- 平稳分布是均匀的：$\\boldsymbol{\\pi}_{JC} = [\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}]$。\n- 速率矩阵的结构使得所有非对角线元素都相等。为确保每个位点单位时间的预期替换数为 $1$，该矩阵被缩放为：\n$$\nQ_{JC} = \\begin{pmatrix} -1   1/3  1/3  1/3 \\\\ 1/3  -1  1/3  1/3 \\\\ 1/3  1/3  -1  1/3 \\\\ 1/3  1/3  1/3  -1 \\end{pmatrix}\n$$\n\n**1.2. 通用时间可逆（GTR）模型**\n\nGTR模型是普适性最强的时间可逆模型。它由以下部分定义：\n- 一个平稳分布向量 $\\boldsymbol{\\pi} = [\\pi_A, \\pi_C, \\pi_G, \\pi_T]$。\n- 一个对称的可交换性速率矩阵 $R$，其中 $r_{ij} = r_{ji}$。\n未缩放的GTR速率矩阵 $Q'$ 的非对角线元素为 $Q'_{ij} = r_{ij}\\pi_j$。问题提供了 $\\boldsymbol{\\pi} = [0.36, 0.14, 0.24, 0.26]$ 和以下可交换性参数：$r_{AC}=0.7$, $r_{AG}=2.0$, $r_{AT}=0.3$, $r_{CG}=0.5$, $r_{CT}=1.5$, $r_{GT}=0.7$。\n矩阵 $Q$ 经过缩放，使得平均替换率为 $1$：\n$$\n-\\sum_{i=0}^{3} \\pi_i Q_{ii} = 1\n$$\n这通过计算一个缩放因子 $\\beta = 1 / (-\\sum_i \\pi_i Q'_{ii})$ 并设置 $Q = \\beta Q'$ 来实现。\n\n**1.3. 位点间速率异质性（$\\Gamma$ 模型）**\n\n为了解释比对序列中不同位点的不同进化速率，我们使用离散伽马分布。速率 $r$ 从一个均值为 $1$ 的 $\\mathrm{Gamma}(\\alpha, \\text{scale}=1/\\alpha)$ 分布中抽取。这个连续分布由 $K=4$ 个离散的速率类别来近似。每个类别的速率 $r_k$ 由伽马分布的分位数中点确定。具体来说，对于 $k \\in \\{1, 2, 3, 4\\}$，我们在累积概率为 $(2k-1)/(2K)$ 处找到值 $r'_k$。然后对这些速率进行重新缩放，以确保其均值为 $1$：$r_k = r'_k / (\\frac{1}{K}\\sum_j r'_j)$。一个位点的似然值是为 $K$ 个速率类别中的每一个计算出的似然值的平均值。\n\n**2. 序列数据的模拟**\n\n序列数据（一个比对）是在“正确”的树拓扑 $((A,B),(C,D))$ 上，使用GTR+$\\Gamma$ 模型进行模拟的。这个有根二叉树有一个根节点、两个内部分支点和四个标记为 $A, B, C, D$ 的叶（末端）节点。分支长度由向量 $\\mathbf{\\ell} = [\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_{RL},\\ell_{RR}]$ 给出。\n\n对于比对中的 $L$ 个位点中的每一个：\n1.  均匀随机地选择一个速率类别 $k \\in \\{1, 2, 3, 4\\}$。设其对应的速率为 $r_k$。\n2.  从平稳分布 $\\boldsymbol{\\pi}$ 中为根节点抽取一个核苷酸状态。\n3.  这个状态沿着树向下传播。对于每个长度为 $t$ 的分支，子节点的状态是基于转移概率 $P(t \\cdot r_k) = \\exp(Q_{GTR} \\cdot t \\cdot r_k)$ 抽取的，该概率以父节点的状态为条件。\n4.  这个过程持续进行，直到所有四个叶节点的状态都被确定，从而形成比对序列的一列。\n重复此过程 $L$ 次以生成完整的比对序列。\n\n**3. 通过Felsenstein剪枝算法计算似然值**\n\n模拟比对的对数似然值在两种情况下进行计算。其核心是Felsenstein的剪枝算法，该算法能高效地计算给定固定树上某一位点数据的似然值。\n\n设 $\\mathbf{D}_s$ 为位点 $s$ 的数据（即四个叶节点的核苷酸）。该算法为树中的每个节点 $u$ 计算条件似然向量 $\\mathbf{L}_u^{(s)}$。这是一个包含四个元素的向量，其中第 $i$ 个元素 $L_{u,i}^{(s)}$ 是在给定节点 $u$ 处于状态 $i$ 的条件下，观察到从 $u$ 向下延伸的子树数据的概率。\n\n-   **在叶节点处**：对于一个观察到核苷酸状态为 $j$ 的叶节点 $u$，其条件似然向量是一个“独热”向量：$L_{u,i}^{(s)} = \\delta_{ij}$，其中 $\\delta_{ij}$ 是克罗内克（Kronecker）δ。\n-   **在内部分支点处**：对于一个有子节点 $v$ 和 $w$ 的内部分支点 $u$，它们通过长度为 $t_v$ 和 $t_w$ 的分支相连，递归公式为：\n    $$\n    L_{u,i}^{(s)} = \\left( \\sum_{j=0}^3 (P(t_v))_{ij} L_{v,j}^{(s)} \\right) \\left( \\sum_{k=0}^3 (P(t_w))_{ik} L_{w,k}^{(s)} \\right)\n    $$\n    这是一个从叶节点到根节点的后序遍历。\n\n对于给定的具有平稳分布 $\\boldsymbol{\\pi}$ 的模型，位点 $s$ 的总似然值在根节点（节点 $R$）处计算：\n$$\n\\mathcal{L}_s = P(\\mathbf{D}_s | \\text{Tree, Model}) = \\sum_{i=0}^3 \\pi_i L_{R,i}^{(s)}\n$$\n为防止长比对序列出现数值下溢，条件似然向量在每个内部分支点处都会被重新缩放。缩放因子的对数被累加，并加到最终的对数似然值上。\n\n整个比对序列的总对数似然值是各个位点对数似然值的总和：\n$$\n\\mathcal{L} = \\sum_{s=1}^L \\log(\\mathcal{L}_s)\n$$\n\n对于GTR+$\\Gamma$模型，一个位点的似然值是 $K$ 个速率类别的平均值：\n$$\n\\mathcal{L}_s = \\frac{1}{K} \\sum_{k=1}^K P(\\mathbf{D}_s | \\text{Tree, Model with rate } r_k)\n$$\n\n**4. 比较与决策**\n\n我们对步骤2中模拟的比对序列进行两次似然评估：\n1.  $\\mathcal{L}_{\\text{JC},\\text{correct}}$：在正确拓扑 $((A,B),(C,D))$ 上，使用JC69模型（无伽马速率，即 $K=1, r_1=1$）计算的对数似然值。\n2.  $\\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}}$：在错误拓扑 $((A,C),(B,D))$ 上，使用GTR+$\\Gamma$ 模型（采用真实的模拟参数）计算的对数似然值。分支长度向量 $\\mathbf{\\ell}$ 被重用，即将悬垂分支长度映射到相应的叶节点，并将内部分支长度映射到两个新的内部分支上。\n\n最终输出是一个布尔值，指示 $\\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}}  \\mathcal{L}_{\\text{JC},\\text{correct}}$ 是否成立。当一个更复杂的模型（GTR+$\\Gamma$）在一个不正确的拓扑上比一个更简单、灵活性较低的模型（JC69）即使在正确的拓扑上能获得的拟合度还要好时，这种现象就会发生。这凸显了系统发育模型选择中的一个潜在陷阱，即模型错误指定可能会误导推断。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\nfrom scipy.stats import gamma\nimport collections\n\n# Define nucleotide and taxa mapping\nNUCS = ['A', 'C', 'G', 'T']\nNUC_MAP = {n: i for i, n in enumerate(NUCS)}\nTAXA = ['A', 'B', 'C', 'D']\nTAXA_MAP = {t: i for i, t in enumerate(TAXA)}\n\ndef get_gtr_q_matrix(pi, r_params):\n    \"\"\"Constructs and scales the GTR rate matrix.\"\"\"\n    q_unscaled = np.zeros((4, 4))\n    r_mat = np.zeros((4, 4))\n    r_mat[0, 1] = r_mat[1, 0] = r_params['ac']\n    r_mat[0, 2] = r_mat[2, 0] = r_params['ag']\n    r_mat[0, 3] = r_mat[3, 0] = r_params['at']\n    r_mat[1, 2] = r_mat[2, 1] = r_params['cg']\n    r_mat[1, 3] = r_mat[3, 1] = r_params['ct']\n    r_mat[2, 3] = r_mat[3, 2] = r_params['gt']\n\n    for i in range(4):\n        for j in range(4):\n            if i != j:\n                q_unscaled[i, j] = r_mat[i, j] * pi[j]\n    \n    for i in range(4):\n        q_unscaled[i, i] = -np.sum(q_unscaled[i, :])\n\n    # Scale the matrix so the average rate is 1\n    mean_rate = -np.sum(pi * np.diag(q_unscaled))\n    if mean_rate == 0:\n        return q_unscaled \n    return q_unscaled / mean_rate\n\ndef get_jc69_q_matrix():\n    \"\"\"Constructs the JC69 rate matrix.\"\"\"\n    q = np.full((4, 4), 1/3)\n    np.fill_diagonal(q, -1)\n    return q\n\ndef get_gamma_rates(alpha, K):\n    \"\"\"Computes K discrete gamma rates with mean 1.\"\"\"\n    if alpha == np.inf: # Case of no rate variation\n        return np.ones(K)\n    \n    # Use mid-quantile approximation\n    probs = (np.arange(K) + 0.5) / K\n    rates = gamma.ppf(probs, a=alpha, scale=1/alpha)\n    \n    # Rescale to have a mean of 1\n    rates /= np.mean(rates)\n    return rates\n\ndef build_tree_from_case(case, topology_type):\n    \"\"\"Builds a tree dictionary based on topology and branch lengths.\"\"\"\n    l_A, l_B, l_C, l_D, l_RL, l_RR = case['l']\n    \n    leaf_A = {'name': 'A', 'bl': l_A, 'children': []}\n    leaf_B = {'name': 'B', 'bl': l_B, 'children': []}\n    leaf_C = {'name': 'C', 'bl': l_C, 'children': []}\n    leaf_D = {'name': 'D', 'bl': l_D, 'children': []}\n\n    if topology_type == 'correct':\n        # Topology: ((A,B),(C,D))\n        internal_1 = {'name': 'N1', 'bl': l_RL, 'children': [leaf_A, leaf_B]}\n        internal_2 = {'name': 'N2', 'bl': l_RR, 'children': [leaf_C, leaf_D]}\n        root = {'name': 'root', 'bl': 0.0, 'children': [internal_1, internal_2]}\n    elif topology_type == 'wrong':\n        # Topology: ((A,C),(B,D))\n        internal_1 = {'name': 'N1', 'bl': l_RL, 'children': [leaf_A, leaf_C]}\n        internal_2 = {'name': 'N2', 'bl': l_RR, 'children': [leaf_B, leaf_D]}\n        root = {'name': 'root', 'bl': 0.0, 'children': [internal_1, internal_2]}\n    else:\n        raise ValueError(\"Unknown topology type\")\n        \n    return root\n\ndef simulate_alignment(tree, q_matrix, pi, gamma_rates, L, rng):\n    \"\"\"Simulates a sequence alignment on the given tree.\"\"\"\n    K = len(gamma_rates)\n    alignment = np.zeros((L, 4), dtype=int)\n    \n    # Pre-calculate transition matrices for each branch and rate category\n    memo_p = {}\n    \n    for site_idx in range(L):\n        rate_idx = rng.choice(K)\n        rate = gamma_rates[rate_idx]\n        \n        # Simulate one column of the alignment top-down\n        q = collections.deque()\n        root_state = rng.choice(4, p=pi)\n        q.append((tree, root_state))\n        leaf_states = {}\n        \n        while q:\n            node, parent_state = q.popleft()\n            for child in node['children']:\n                bl = child['bl']\n                # Memoize P(t) computation\n                key = (bl, rate_idx)\n                if key not in memo_p:\n                    memo_p[key] = expm(q_matrix * bl * rate)\n                P = memo_p[key]\n                \n                child_state = rng.choice(4, p=P[parent_state, :])\n                if not child['children']:  # Leaf node\n                    leaf_states[child['name']] = child_state\n                else:  # Internal node\n                    q.append((child, child_state))\n    \n        for i, taxon_name in enumerate(TAXA):\n            alignment[site_idx, i] = leaf_states[taxon_name]\n            \n    return alignment\n\ndef calculate_log_likelihood(alignment, tree, q_matrix, pi, gamma_rates):\n    \"\"\"Calculates the log-likelihood of an alignment given a tree and model.\"\"\"\n    L = alignment.shape[0]\n    K = len(gamma_rates)\n    total_log_likelihood = 0.0\n    \n    # Memoize P(t) matrices\n    memo_p_lik = {}\n    \n    for site_idx in range(L):\n        site_data = {taxon: alignment[site_idx, tax_idx] for tax_idx, taxon in enumerate(TAXA)}\n        \n        per_category_likelihoods = np.zeros(K)\n        for k in range(K):\n            rate = gamma_rates[k]\n            \n            # Recursive computation of conditional likelihoods\n            log_scaler_sum, root_likelihood_vec = _pruning_recursion(tree, site_data, q_matrix, rate, memo_p_lik)\n            \n            site_lik_for_category = np.dot(pi, root_likelihood_vec)\n            \n            # Add log of site likelihood for this category\n            if site_lik_for_category > 0:\n                log_lik_for_category = np.log(site_lik_for_category) + log_scaler_sum\n                per_category_likelihoods[k] = np.exp(log_lik_for_category)\n\n        # Average likelihoods over categories\n        site_avg_likelihood = np.mean(per_category_likelihoods)\n        \n        if site_avg_likelihood > 0:\n            total_log_likelihood += np.log(site_avg_likelihood)\n            \n    return total_log_likelihood\n\ndef _pruning_recursion(node, site_data, q_matrix, rate, memo):\n    \"\"\"Recursive helper for Felsenstein's pruning algorithm.\"\"\"\n    if not node['children']: # Leaf node\n        likelihood_vec = np.zeros(4)\n        likelihood_vec[site_data[node['name']]] = 1.0\n        return 0.0, likelihood_vec\n\n    child_results = []\n    total_log_scaler = 0.0\n    \n    for child in node['children']:\n        log_scaler, child_likelihood_vec = _pruning_recursion(child, site_data, q_matrix, rate, memo)\n        total_log_scaler += log_scaler\n        \n        bl = child['bl']\n        key = (bl, rate)\n        if key not in memo:\n            memo[key] = expm(q_matrix * bl * rate)\n        P = memo[key]\n        \n        transformed_vec = P @ child_likelihood_vec\n        child_results.append(transformed_vec)\n    \n    # Element-wise product of children's likelihood vectors\n    node_likelihood_vec = child_results[0]\n    for i in range(1, len(child_results)):\n        node_likelihood_vec *= child_results[i]\n        \n    # Numerical scaling\n    scaler = np.sum(node_likelihood_vec)\n    if scaler > 1e-300: # Avoid log(0)\n        node_likelihood_vec /= scaler\n        total_log_scaler += np.log(scaler)\n        \n    return total_log_scaler, node_likelihood_vec\n\n\ndef solve():\n    test_cases = [\n        {'seed': 7, 'L': 500, 'alpha': 0.3, 'l': [0.3, 0.3, 0.3, 0.3, 0.01, 0.01]},\n        {'seed': 13, 'L': 1500, 'alpha': 0.3, 'l': [0.3, 0.3, 0.3, 0.3, 0.2, 0.2]},\n        {'seed': 23, 'L': 2000, 'alpha': 1.5, 'l': [0.5, 0.2, 0.5, 0.2, 0.4, 0.4]},\n    ]\n    \n    gtr_pi = np.array([0.36, 0.14, 0.24, 0.26])\n    gtr_r = {'ac': 0.7, 'ag': 2.0, 'at': 0.3, 'cg': 0.5, 'ct': 1.5, 'gt': 0.7}\n    \n    results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        \n        # --- 1. Simulation Setup ---\n        # GTR+Gamma model for simulation\n        q_gtr = get_gtr_q_matrix(gtr_pi, gtr_r)\n        gamma_rates_sim = get_gamma_rates(case['alpha'], K=4)\n        correct_tree = build_tree_from_case(case, 'correct')\n        \n        # Simulate alignment on CORRECT topology with GTR+Gamma\n        alignment = simulate_alignment(correct_tree, q_gtr, gtr_pi, gamma_rates_sim, case['L'], rng)\n        \n        # --- 2. Likelihood Calculation ---\n        \n        # Likelihood on CORRECT topology with JC69 model\n        q_jc69 = get_jc69_q_matrix()\n        pi_jc69 = np.array([0.25, 0.25, 0.25, 0.25])\n        # JC69 has no rate variation, so K=1, rate=1.0 is equivalent\n        gamma_rates_jc = np.array([1.0])\n        \n        log_lik_jc_correct = calculate_log_likelihood(alignment, correct_tree, q_jc69, pi_jc69, gamma_rates_jc)\n\n        # Likelihood on WRONG topology with GTR+Gamma model\n        wrong_tree = build_tree_from_case(case, 'wrong')\n        gamma_rates_gtr = get_gamma_rates(case['alpha'], K=4) # Use case alpha\n        \n        log_lik_gtr_wrong = calculate_log_likelihood(alignment, wrong_tree, q_gtr, gtr_pi, gamma_rates_gtr)\n        \n        # --- 3. Comparison ---\n        delta = log_lik_gtr_wrong - log_lik_jc_correct\n        results.append(delta > 0)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406821"}]}