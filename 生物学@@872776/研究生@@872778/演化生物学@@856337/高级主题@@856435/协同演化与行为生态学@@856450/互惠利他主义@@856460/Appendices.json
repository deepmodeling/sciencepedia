{"hands_on_practices": [{"introduction": "互惠利他主义的核心在于一个基本的成本-收益权衡。这个基础练习将通过计算在回报不确定的情况下的期望净适应度变化，来巩固这一核心思想。这个练习 [@problem_id:1877294] 为理解更复杂的策略互动奠定了数学基础，展示了即使在存在风险的情况下，合作行为在何种条件下依然是有利的。", "problem": "在动物行为研究中，互惠利他主义是用来解释无亲缘关系个体之间合作行为的一种模型。考虑一个吸血蝙蝠群落中血液共享的简化模型，其中适应度的增益和损失以抽象的“能量单位”来量化。捐赠蝙蝠分享血餐的成本表示为 $c$。接受者（通常濒临饥饿）获得的收益表示为 $b$。\n\n现在，考虑两只无亲缘关系的蝙蝠，蝙蝠A和蝙蝠B，之间的一次互动序列。已知蝙蝠A是“合作者”，它通过与蝙蝠B分享其血餐来发起这个序列。然而，蝙蝠B并不总会回报。对于它接受的每一次善举，蝙蝠B在未来的相遇中回报的概率为 $p$。假设回报行为如果发生，会涉及相同的成本和收益值。\n\n给定以下参数：\n- 捐赠成本，$c = 8$ 能量单位\n- 接受收益，$b = 30$ 能量单位\n- 回报概率，$p = 0.75$\n\n计算由蝙蝠A发起的这整个互动序列中，蝙蝠A的预期净适应度变化和蝙蝠B的预期净适应度变化。以一对数值的形式表示你的答案，单位为能量单位，分别代表蝙蝠A和蝙蝠B的预期净变化。", "solution": "问题要求计算两只蝙蝠，蝙蝠A和蝙蝠B，各自的预期净适应度变化。一个依赖于概率性结果的量的期望值，是通过将每个可能结果的值与其发生概率的乘积相加来计算的。\n\n首先，我们来分析蝙蝠A的预期净适应度变化。对于蝙蝠A，互动序列包括两个部分：初始的确定成本和潜在的未来收益。\n1.  蝙蝠A发起分享。这是一个确定事件。蝙蝠A的成本是 $c$，所以其适应度变化为 $-c$。\n2.  蝙蝠B之后可能会也可能不会回报。蝙蝠A的收益取决于蝙蝠B的行为。\n    -   以概率 $p$，蝙蝠B会回报，蝙蝠A获得收益 $b$。\n    -   以概率 $(1-p)$，蝙蝠B不回报，蝙蝠A没有收益（变化为0）。\n蝙蝠A从此概率事件中获得的预期收益是（结果的值 $\\times$ 结果的概率）之和：\n$$E[\\text{Benefit for A}] = (b \\times p) + (0 \\times (1-p)) = pb$$\n蝙蝠A的总预期净适应度变化，记为 $E_A$，是初始成本和预期未来收益之和：\n$$E_A = -c + pb$$\n代入给定值：\n$$E_A = -8 + (0.75)(30) = -8 + 22.5 = 14.5$$\n所以，蝙蝠A的预期净适应度变化是 14.5 能量单位。\n\n接下来，我们分析蝙蝠B的预期净适应度变化。对于蝙蝠B，互动序列同样包括两个部分：初始的确定收益和潜在的未来成本。\n1.  蝙蝠B从蝙蝠A处接受分享的血餐。这是一个确定事件。蝙蝠B的收益是 $b$，所以其适应度变化为 $+b$。\n2.  蝙蝠B之后可能会也可能不会回报。蝙蝠B的成本取决于其自身的行为。\n    -   以概率 $p$，蝙蝠B会回报，产生一个成本 $c$。与此成本相关的适应度变化为 $-c$。\n    -   以概率 $(1-p)$，蝙蝠B不回报，不产生任何成本（变化为0）。\n蝙蝠B因其自身潜在行为而产生的预期适应度变化是：\n$$E[\\text{Cost for B}] = (-c \\times p) + (0 \\times (1-p)) = -pc$$\n蝙蝠B的总预期净适应度变化，记为 $E_B$，是初始收益和预期未来成本之和：\n$$E_B = b - pc$$\n代入给定值：\n$$E_B = 30 - (0.75)(8) = 30 - 6 = 24$$\n所以，蝙蝠B的预期净适应度变化是 24 能量单位。\n\n最终答案是蝙蝠A和蝙蝠B各自的预期净适应度变化对。", "answer": "$$\\boxed{\\begin{pmatrix} 14.5  24 \\end{pmatrix}}$$", "id": "1877294"}, {"introduction": "从个体间的单次互动，我们进而转向群体动态的宏观视角。一种合作策略要想在演化中胜出，它必须能够成功地“入侵”一个由非合作者组成的种群。这个练习 [@problem_id:1877275] 将探讨经典的“一报还一报”（TFT）策略的入侵条件，引入了重复博弈和群体内部的非随机匹配（assortment）这两个关键概念，揭示了它们是如何为合作的涌现铺平道路的。", "problem": "在演化博弈论中，自利个体组成的种群中合作行为的出现是一个核心难题。所进行的博弈的结构至关重要。考虑一个由重复博弈建模的场景，在每一轮中，两个参与者可以选择合作 (C) 或背叛 (D)。一个参与者的收益由以下矩阵给出，其中该参与者的行动在行上，其对手的行动在列上：\n- (C, C) 的收益：$R$ (奖励)\n- (C, D) 的收益：$S$ (傻瓜)\n- (D, C) 的收益：$T$ (诱惑)\n- (D, D) 的收益：$P$ (惩罚)\n\n如果 $T  R  P  S$，则该博弈是囚徒困境。每一轮结束后，有恒定的概率 $w$ 会在相同的两个个体之间进行下一轮博弈。总期望收益是所有轮次收益的总和。\n\n考虑一个大型种群，其中几乎全部由采用“始终背叛” (ALLD) 策略的个体组成。引入一小群采用“以牙还牙” (TFT) 策略的突变者。一个 TFT 参与者在第一步选择合作，此后每一轮都复制其对手上一轮的行动。\n\n由于社会结构或亲缘关系，突变的 TFT 参与者之间相互互动的频率可能高于随机情况下的预期。设 $k$ 为分类匹配系数，定义为一个 TFT 个体与另一个 TFT 个体互动的概率。因此，一个 TFT 参与者与庞大的 ALLD 种群中的一个随机成员互动的概率为 $1-k$。\n\n为了让合作行为得以涌现，TFT 策略必须能够入侵 ALLD 种群。这要求一个 TFT 个体的期望收益要大于一个 ALLD 个体的期望收益。\n\n给定囚徒困境的收益为 $T=4, R=2, P=1, S=0$，以及未来互动的概率 $w=0.5$，计算 TFT 策略入侵 ALLD 种群所需的最小分类匹配系数 $k_{crit}$。将您的答案表示为一个分数。", "solution": "为了确定TFT策略入侵ALLD种群的条件，我们需要比较TFT个体和ALLD个体的期望收益。设TFT个体在种群中的比例极小。\n\n1.  **计算不同策略对的总期望收益**\n    在无限重复博弈中，未来互动的概率为 $w$。总期望收益是各轮收益的折扣总和。\n    *   两个TFT个体互动 (CC)：收益始终为 $R$。总收益 $V_{TFT-TFT} = \\sum_{t=0}^{\\infty} w^t R = \\frac{R}{1-w}$。\n    *   一个TFT个体（行）对一个ALLD个体（列）(CD)：第一轮TFT合作，ALLD背叛，TFT收益为 $S$。此后TFT始终背叛，ALLD也始终背叛，后续每轮收益均为 $P$。总收益 $V_{TFT-ALLD} = S + \\sum_{t=1}^{\\infty} w^t P = S + \\frac{wP}{1-w}$。\n    *   一个ALLD个体（行）对一个ALLD个体（列）(DD)：收益始终为 $P$。总收益 $V_{ALLD-ALLD} = \\sum_{t=0}^{\\infty} w^t P = \\frac{P}{1-w}$。\n\n2.  **计算TFT个体的期望收益**\n    一个TFT个体以概率 $k$ 与另一个TFT个体互动，以概率 $1-k$ 与一个ALLD个体互动。其总期望收益为：\n    $$E_{TFT} = k \\cdot V_{TFT-TFT} + (1-k) \\cdot V_{TFT-ALLD}$$\n    $$E_{TFT} = k \\frac{R}{1-w} + (1-k) \\left( S + \\frac{wP}{1-w} \\right)$$\n\n3.  **计算ALLD个体的期望收益**\n    由于TFT是稀有突变体，一个典型的ALLD个体几乎总是与其他ALLD个体互动。因此，其期望收益近似为：\n    $$E_{ALLD} \\approx V_{ALLD-ALLD} = \\frac{P}{1-w}$$\n\n4.  **建立入侵条件**\n    TFT策略能够入侵的条件是 $E_{TFT} > E_{ALLD}$：\n    $$k \\frac{R}{1-w} + (1-k) \\left( S + \\frac{wP}{1-w} \\right) > \\frac{P}{1-w}$$\n\n5.  **代入数值求解**\n    给定参数 $T=4, R=2, P=1, S=0, w=0.5$。因此 $1-w=0.5$。\n    $$k \\frac{2}{0.5} + (1-k) \\left( 0 + \\frac{0.5 \\cdot 1}{0.5} \\right) > \\frac{1}{0.5}$$\n    $$4k + (1-k)(1) > 2$$\n    $$4k + 1 - k > 2$$\n    $$3k > 1$$\n    $$k > \\frac{1}{3}$$\n    因此，TFT策略入侵所需的最小分类匹配系数 $k_{crit}$ 是 $\\frac{1}{3}$。", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1877275"}, {"introduction": "现实世界充满了不确定性和偶然的错误。一个在理想条件下表现优异的策略，在充满“噪声”的环境中可能并非最佳选择。这个高级练习 [@problem_id:2527576] 将场景的复杂性提升到更真实的层次，通过比较“一报还一报”（TFT）与“赢则坚守，输则转换”（WSLS）策略在有执行错误时的表现，训练我们使用马尔可夫链分析来评估策略的稳健性，并确定合作策略在不同环境下的优劣。", "problem": "考虑一个无限视界重复囚徒困境（PD），其形式为捐赠博弈，这是生态学中研究无亲缘关系个体间合作的经典模型。在每一轮中，每个参与者选择合作（$C$）或背叛（$D$）。合作给对方带来收益 $b0$，同时自身付出成本 $c0$。因此，对于行参与者而言，单次囚徒困境的支付如下：相互合作（$CC$）得到 $R=b-c$，单方面合作而对方背叛（$CD$）得到 $S=-c$，单方面背叛而对方合作（$DC$）得到 $T=b$，相互背叛（$DD$）得到 $P=0$。假设 $bc0$，因此满足囚徒困境的要求 $TRPS$。\n\n参与者采用两种仅依赖于上一轮已实现结果的记忆一策略之一：\n- 一报还一报 (TFT)：当且仅当对方在上一轮合作时，本轮才合作。\n- 赢定输移 (WSLS)：如果上一轮的支付是“赢”（$R$ 或 $T$），则重复上一轮的行动；如果上一轮的支付是“输”（$P$ 或 $S$），则转换行动。\n\n执行错误独立发生：在每次行动中，参与者的意图行动有 $\\epsilon \\in (0,1)$ 的概率被翻转（$C \\leftrightarrow D$），此错误在参与者和回合之间是独立的。对于任何固定的策略对，四个已实现的联合行动状态 $\\{CC, CD, DC, DD\\}$ 上的动态构成一个时间齐次马尔可夫链。由于对于 $0\\epsilon1$，该链是有限、不可约且非周期的，因此存在唯一的平稳分布。\n\n将策略对的每回合长期平均支付定义为在 $\\{CC, CD, DC, DD\\}$ 上的平稳分布下的期望单次支付。如果 WSLS 自我对局中的长期平均支付超过 TFT 自我对局中的长期平均支付，我们称 WSLS “表现优于” TFT。\n\n仅从上述定义和马尔可夫链的全概率定律出发，符号化地推导在噪声水平 $\\epsilon$ 下，TFT对TFT以及WSLS对WSLS的平稳分布，计算它们各自的长期平均支付，并确定唯一的临界错误概率 $\\epsilon^{\\ast}$，在该概率下，WSLS自我对局和TFT自我对局具有相等的长期平均支付。你的最终答案必须是 $\\epsilon^{\\ast}$ 的闭式解。不需要四舍五入，答案无单位。", "solution": "问题要求推导临界错误概率 $\\epsilon^{\\ast}$，在该概率下，一报还一报 (TFT) 自我对局的长期平均支付 $V_{TFT}$ 等于赢定输移 (WSLS) 自我对局的长期平均支付 $V_{WSLS}$。该博弈是一个重复囚徒困境，支付为 $R=b-c$, $S=-c$, $T=b$ 和 $P=0$，其中 $bc0$。系统的状态是两位参与者在上一轮的联合行动，来自集合 $\\{CC, CD, DC, DD\\}$。意图行动以概率 $\\epsilon \\in (0,1)$ 被翻转。\n\n首先，我们分析TFT对TFT的互动。TFT参与者当且仅当其伙伴在上一轮合作时才合作。给定当前状态 $(A_1, A_2)$，下一轮的意图行动 $(I_1, I_2)$ 是：\n- 从 $CC$：两位参与者都看到了合作，所以都意图合作。意图状态是 $(C,C)$。\n- 从 $CD$：参与者1看到了背叛，参与者2看到了合作。意图状态是 $(D,C)$。\n- 从 $DC$：参与者1看到了合作，参与者2看到了背叛。意图状态是 $(C,D)$。\n- 从 $DD$：两位参与者都看到了背叛，所以都意图背叛。意图状态是 $(D,D)$。\n\n一个意图合作 ($I_C$) 以 $1-\\epsilon$ 的概率成为实际合作 ($C$)，以 $\\epsilon$ 的概率成为实际背叛 ($D$)。一个意图背叛 ($I_D$) 以 $1-\\epsilon$ 的概率成为实际背叛 ($D$)，以 $\\epsilon$ 的概率成为实际合作 ($C$)。因此，对于状态集 $\\{CC, CD, DC, DD\\}$ 上的马尔可夫链，其转移矩阵 $M_{TFT}$ 为：\n$$\nM_{TFT} =\n\\begin{pmatrix}\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2 \\\\\n\\epsilon(1-\\epsilon)  \\epsilon^2  (1-\\epsilon)^2  \\epsilon(1-\\epsilon) \\\\\n\\epsilon(1-\\epsilon)  (1-\\epsilon)^2  \\epsilon^2  \\epsilon(1-\\epsilon) \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2\n\\end{pmatrix}\n$$\n$M_{TFT}$ 矩阵的每一列的和都为1，这意味着该矩阵是双随机的。对于一个不可约且非周期的有限马尔可夫链，双随机转移矩阵意味着唯一的平稳分布是均匀分布。设 $\\pi_{TFT} = (p_{CC}, p_{CD}, p_{DC}, p_{DD})$ 为此分布。因此，$p_{CC} = p_{CD} = p_{DC} = p_{DD} = \\frac{1}{4}$。\nTFT参与者的长期平均支付是在此平稳分布下的期望支付：\n$$V_{TFT} = p_{CC}R + p_{CD}S + p_{DC}T + p_{DD}P$$\n$$V_{TFT} = \\frac{1}{4} (b-c) + \\frac{1}{4} (-c) + \\frac{1}{4} (b) + \\frac{1}{4} (0) = \\frac{1}{4}(2b - 2c) = \\frac{b-c}{2}$$\n\n接下来，我们分析WSLS对WSLS的互动。WSLS参与者在“赢”（支付为 $R$ 或 $T$）后重复其上一步行动，在“输”（支付为 $P$ 或 $S$）后转换其行动。意图行动如下：\n- 从 $CC$：两位参与者都收到 $R$（赢）。两者都重复他们的行动 ($C$)。意图状态是 $(C,C)$。\n- 从 $CD$：参与者1收到 $S$（输），参与者2收到 $T$（赢）。参与者1从 $C$ 转换到 $D$，参与者2重复 $D$。意图状态是 $(D,D)$。\n- 从 $DC$：参与者1收到 $T$（赢），参与者2收到 $S$（输）。参与者1重复 $D$，参与者2从 $C$ 转换到 $D$。意图状态是 $(D,D)$。\n- 从 $DD$：两者都收到 $P$（输）。两者都将他们的行动从 $D$ 转换到 $C$。意图状态是 $(C,C)$。\n\n转移矩阵 $M_{WSLS}$ 基于这些意图行动构建：\n$$\nM_{WSLS} =\n\\begin{pmatrix}\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2 \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2 \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2 \\\\\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2\n\\end{pmatrix}\n$$\n设平稳分布为 $\\pi_{WSLS} = (q_{CC}, q_{CD}, q_{DC}, q_{DD})$。由于对称性，$q_{CD}=q_{DC}$。通过考虑进入状态 $CD$ 的概率流，我们可以求解平稳分布。根据平稳性条件 $\\pi = \\pi M$：\n$$q_{CD} = q_{CC}\\epsilon(1-\\epsilon) + q_{CD}\\epsilon(1-\\epsilon) + q_{DC}\\epsilon(1-\\epsilon) + q_{DD}\\epsilon(1-\\epsilon)$$\n$$q_{CD} = (q_{CC} + q_{CD} + q_{DC} + q_{DD})\\epsilon(1-\\epsilon)$$\n由于概率之和 $\\sum q_{ij} = 1$，我们得到 $q_{CD} = \\epsilon(1-\\epsilon)$，因此 $q_{DC} = \\epsilon(1-\\epsilon)$。\n利用归一化条件 $q_{CC} + q_{CD} + q_{DC} + q_{DD} = 1$，我们有 $q_{CC} + q_{DD} = 1 - 2\\epsilon(1-\\epsilon)$。\n现在，我们利用进入CC和DD的概率流。从任何对称状态（CC或DD），意图都是（C,C）。从任何非对称状态（CD或DC），意图都是（D,D）。\n$q_{CC} = (q_{CC}+q_{DD})(1-\\epsilon)^2 + (q_{CD}+q_{DC})\\epsilon^2$\n$q_{DD} = (q_{CC}+q_{DD})\\epsilon^2 + (q_{CD}+q_{DC})(1-\\epsilon)^2$\n两式相减：\n$q_{CC}-q_{DD} = (q_{CC}+q_{DD})((1-\\epsilon)^2-\\epsilon^2) + (q_{CD}+q_{DC})(\\epsilon^2-(1-\\epsilon)^2)$\n$q_{CC}-q_{DD} = (q_{CC}+q_{DD} - (q_{CD}+q_{DC}))((1-\\epsilon)^2-\\epsilon^2) = (q_{CC}+q_{DD} - (q_{CD}+q_{DC}))(1-2\\epsilon)$\n代入 $q_{CC}+q_{DD} = 1 - 2\\epsilon(1-\\epsilon)$ 和 $q_{CD}+q_{DC} = 2\\epsilon(1-\\epsilon)$:\n$q_{CC}-q_{DD} = (1 - 2\\epsilon(1-\\epsilon) - 2\\epsilon(1-\\epsilon))(1-2\\epsilon) = (1-4\\epsilon+4\\epsilon^2)(1-2\\epsilon) = (1-2\\epsilon)^2(1-2\\epsilon) = (1-2\\epsilon)^3$。\n我们现在有了一个关于 $q_{CC}$ 和 $q_{DD}$ 的二元线性方程组：\n$q_{CC}+q_{DD} = 1 - 2\\epsilon + 2\\epsilon^2$\n$q_{CC}-q_{DD} = (1-2\\epsilon)^3 = 1 - 6\\epsilon + 12\\epsilon^2 - 8\\epsilon^3$\n两式相加得到 $2q_{CC} = 2 - 8\\epsilon + 14\\epsilon^2 - 8\\epsilon^3$, 所以 $q_{CC} = 1 - 4\\epsilon + 7\\epsilon^2 - 4\\epsilon^3$。\n\nWSLS的长期平均支付是：\n$$V_{WSLS} = q_{CC}R + q_{CD}S + q_{DC}T + q_{DD}P$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(-c) + \\epsilon(1-\\epsilon)(b) + q_{DD}(0)$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(b-c) = (q_{CC} + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1-4\\epsilon+7\\epsilon^2-4\\epsilon^3 + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n\n最后，我们通过令支付相等来找到临界错误概率 $\\epsilon^{\\ast}$：\n$$V_{TFT} = V_{WSLS}$$\n$$\\frac{b-c}{2} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n由于 $bc$，所以 $b-c0$，我们可以用它来除以两边：\n$$\\frac{1}{2} = 1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n$$0 = \\frac{1}{2} - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n乘以-2：\n$$0 = 8\\epsilon^3 - 12\\epsilon^2 + 6\\epsilon - 1$$\n这是立方展开式：\n$$0 = (2\\epsilon - 1)^3$$\n通过将底数设为零，可以找到在区间 $(0,1)$ 内的唯一实数解：\n$$2\\epsilon^{\\ast} - 1 = 0 \\implies \\epsilon^{\\ast} = \\frac{1}{2}$$\n这就是TFT和WSLS自我对局的长期平均支付相等的临界错误概率。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2527576"}]}