{"hands_on_practices": [{"introduction": "在从基因组数据中检测水平基因转移（HGT）之前，理解其背后的基本动力学至关重要。本练习将指导您从一个经典的实验室实验中推导出一个模型，用于估计接合（一种主要的HGT机制）的速率。通过将细胞群体动力学的微分方程模型与统计估计量联系起来，您将深入了解如何量化基本的生物学速率。[@problem_id:2806039]", "problem": "经典的终点平板接合分析法被用于量化质粒介导的接合速率，这是一种水平基因转移的形式。将携带自转移性质粒的供体细胞（初始密度为 $D_0$，单位为菌落形成单位/毫升）与初始密度为 $R_0$ 的受体细胞在滤膜上混合，并孵育固定时间 $t$。该分析在营养限制条件下进行，以防止在接合期间发生细胞分裂，仅发生接合作用（质粒从供体转移到受体）。在时间 $t$，将细胞重悬并铺板于只计数接合子的选择性培养基上，得到观测到的接合子计数 $T_t$（单位为重悬液的每毫升）。\n\n采用以下基于质量作用定律的机理描述：每个受体细胞在单位时间内转化为接合子的时间齐次风险等于 $\\beta D(t)$，其中 $\\beta$ 是接合速率常数，而 $D(t)$ 是供体密度。在该分析中，供体在接合后保留质粒，不会因转移而被消耗。在接合期间，假设 $D(t) \\equiv D_0$（供体无生长也无损失），受体转化为接合子而无分裂或死亡，不存在质粒的分离丢失，并且细胞在滤膜上均匀分布，因此质量作用接触率适用。\n\n从这些假设出发，并且在所述模型之外不引入任何特别的近似，推导出一个接合速率 $\\beta$ 的闭式最大似然估计量 $\\hat{\\beta}$，该表达式仅用 $D_0$、$R_0$、$T_t$ 和 $t$ 表示。明确说明该估计量在何种最小生物学和统计学假设下是渐近无偏的，并说明在何种极限情况下，稀有事件近似能产生一个无偏的一阶估计量。请以单一符号表达式的形式提供 $\\hat{\\beta}$；在最终表达式中不要包含任何单位。", "solution": "所陈述的问题具有科学依据，提法得当，且内部一致。它描述了微生物遗传学中一个标准的实验情景，使用了经典的质量作用动力学模型。所有推导所需统计估计量的必要参数和条件均已提供。因此，我们可以开始推导。\n\n模型的核心是受体细胞密度 $R(t)$ 的变化率。根据质量作用定律和问题的设定，受体转化为接合子的速率与受体密度 $R(t)$ 和供体密度 $D(t)$ 的乘积成正比。单个受体细胞的风险率被设定为 $\\beta D(t)$。因此，受体群体密度的变化率为：\n$$ \\frac{dR(t)}{dt} = - (\\beta D(t)) R(t) $$\n问题陈述中假设供体密度恒定，即 $D(t) \\equiv D_0$，因为在接合间隔 $t$ 内供体不被消耗也不生长。该微分方程简化为：\n$$ \\frac{dR(t)}{dt} = -\\beta D_0 R(t) $$\n这是一个一阶可分离常微分方程，初始条件为 $R(t=0) = R_0$。我们通过积分求解：\n$$ \\int_{R_0}^{R(t)} \\frac{dR'}{R'} = \\int_0^t -\\beta D_0 dt' $$\n$$ \\ln(R(t)) - \\ln(R_0) = -\\beta D_0 t $$\n$$ R(t) = R_0 \\exp(-\\beta D_0 t) $$\n该方程描述了在时间 $t$ 剩余的受体细胞密度。接合子密度 $T(t)$ 是初始受体密度减去剩余受体密度，因为细胞只发生转化而无死亡或分裂：\n$$ T(t) = R_0 - R(t) = R_0 (1 - \\exp(-\\beta D_0 t)) $$\n此表达式代表接合子的期望密度。观测值是经验测量值 $T_t$。\n\n为了推导最大似然估计量（MLE），我们必须为观测到的计数建立一个概率模型。让我们考虑给定体积内初始数量为 $N_R$ 的受体细胞群体的命运。每个细胞都经历一个随机过程，要么转化为接合子，要么保持为受体。根据我们的确定性模型，任何单个受体细胞在时间 $t$ 之前成为接合子的概率 $p$ 为：\n$$ p = \\frac{T(t)}{R_0} = 1 - \\exp(-\\beta D_0 t) $$\n其保持为受体的概率是 $1-p = \\exp(-\\beta D_0 t)$。假设每个细胞的命运是独立事件，那么从初始 $N_R$ 个受体中观测到的接合子数量 $N_T$ 服从二项分布：\n$$ P(N_T | N_R, p) = \\binom{N_R}{N_T} p^{N_T} (1-p)^{N_R - N_T} $$\n参数 $\\beta$ 的似然函数 $L(\\beta)$ 与此概率成正比。我们可以用观测到的密度 $T_t$ 和 $R_0$ 来表示它，注意到 $N_T/N_R = T_t/R_0$。\n$$ L(\\beta) \\propto \\left(1 - \\exp(-\\beta D_0 t)\\right)^{N_T} \\left(\\exp(-\\beta D_0 t)\\right)^{N_R - N_T} $$\n最大化对数似然函数 $\\ell(\\beta) = \\ln L(\\beta)$ 更为方便：\n$$ \\ell(\\beta) = C + N_T \\ln\\left(1 - \\exp(-\\beta D_0 t)\\right) + (N_R - N_T) \\ln\\left(\\exp(-\\beta D_0 t)\\right) $$\n$$ \\ell(\\beta) = C + N_T \\ln\\left(1 - \\exp(-\\beta D_0 t)\\right) - (N_R - N_T) \\beta D_0 t $$\n为了找到最大似然估计量 $\\hat{\\beta}$，我们将 $\\ell(\\beta)$ 对 $\\beta$ 的导数设为零：\n$$ \\frac{d\\ell}{d\\beta} = N_T \\frac{D_0 t \\exp(-\\beta D_0 t)}{1 - \\exp(-\\beta D_0 t)} - (N_R - N_T) D_0 t = 0 $$\n假设 $D_0 t \\neq 0$，我们可以除以这一项：\n$$ \\frac{N_T \\exp(-\\hat{\\beta} D_0 t)}{1 - \\exp(-\\hat{\\beta} D_0 t)} = N_R - N_T $$\n$$ N_T \\exp(-\\hat{\\beta} D_0 t) = (N_R - N_T) (1 - \\exp(-\\hat{\\beta} D_0 t)) $$\n$$ N_T \\exp(-\\hat{\\beta} D_0 t) = N_R - N_T - (N_R - N_T) \\exp(-\\hat{\\beta} D_0 t) $$\n$$ (N_T + N_R - N_T) \\exp(-\\hat{\\beta} D_0 t) = N_R - N_T $$\n$$ N_R \\exp(-\\hat{\\beta} D_0 t) = N_R - N_T $$\n两边同除以 $N_R$ 并用观测到的密度替换（用 $T_t/R_0$ 替换 $N_T/N_R$）：\n$$ \\exp(-\\hat{\\beta} D_0 t) = 1 - \\frac{N_T}{N_R} = 1 - \\frac{T_t}{R_0} $$\n通过取自然对数求解 $\\hat{\\beta}$：\n$$ -\\hat{\\beta} D_0 t = \\ln\\left(1 - \\frac{T_t}{R_0}\\right) $$\n$$ \\hat{\\beta} = -\\frac{1}{D_0 t} \\ln\\left(1 - \\frac{T_t}{R_0}\\right) $$\n这就是 $\\beta$ 的闭式最大似然估计量。它在 $0 \\le T_t  R_0$ 的条件下有定义。\n\n为使该最大似然估计量 $\\hat{\\beta}$ 渐近无偏，所需的最小假设是：\n$1$. **生物学假设**：接合过程必须遵循指定的模型。这意味着细胞充分混合，每个受体的接合事件是独立的，并且所述条件（无生长、无死亡、无质粒丢失、供体密度恒定）是对现实的良好近似。\n$2$. **统计学假设**：初始受体细胞的数量 $N_R$ 必须很大。最大似然估计量的渐近性质在大样本极限下成立，此处即为 $N_R \\to \\infty$。在这些条件下，$\\hat{\\beta}$ 的分布收敛于以 $\\beta$ 的真实值为中心的正态分布。\n\n稀有事件近似会产生一个一阶估计量。该机制对应于接合概率很小的情况，即当乘积 $\\beta D_0 t$ 很小时发生。在这种情况下，我们预期接合子比例 $T_t/R_0$ 会很小。我们使用自然对数的泰勒展开，对于小的 $x$，有 $\\ln(1-x) \\approx -x$。令 $x = T_t/R_0$：\n$$ \\hat{\\beta} \\approx -\\frac{1}{D_0 t} \\left(-\\frac{T_t}{R_0}\\right) = \\frac{T_t}{R_0 D_0 t} $$\n这是常用的一阶估计量。要使该估计量无偏，其期望值必须等于真实参数 $\\beta$。观测密度 $T_t$ 的期望值是模型预测值 $E[T_t] = R_0 (1 - \\exp(-\\beta D_0 t))$。\n$$ E\\left[\\frac{T_t}{R_0 D_0 t}\\right] = \\frac{E[T_t]}{R_0 D_0 t} = \\frac{R_0 (1 - \\exp(-\\beta D_0 t))}{R_0 D_0 t} = \\frac{1 - \\exp(-\\beta D_0 t)}{D_0 t} $$\n将其设为等于 $\\beta$ 需要 $\\frac{1 - \\exp(-\\beta D_0 t)}{\\beta D_0 t} = 1$。此方程仅在 $\\beta D_0 t \\to 0$ 的极限情况下才成立。因此，该一阶估计量仅在无限稀有事件的极限状态下是无偏的。对于任何有限的 $\\beta D_0 t > 0$，该估计量是有偏的，会低估 $\\beta$ 的真实值，因为对于 $z>0$ 有 $1 - \\exp(-z)  z$。", "answer": "$$ \\boxed{-\\frac{1}{D_0 t} \\ln\\left(1 - \\frac{T_t}{R_0}\\right)} $$", "id": "2806039"}, {"introduction": "水平转移的基因通常因其非典型的序列特征而暴露其外来来源，形成所谓的“基因组岛”。本练习要求您实现一种强大的统计方法——使用动态规划进行变点检测——来自动识别这些基因组岛。您将学习如何基于GC含量和密码子偏好等特征对染色体进行分割，并使用贝叶斯信息准则（Bayesian Information Criterion, BIC）来平衡模型的拟合度与复杂性。[@problem_id:2805995]", "problem": "设计并实现一个程序，该程序使用基于统计学原理的准则，对一个由双特征基因组窗口组成的一维序列执行变点分割，以识别候选的水平转移基因组岛。其生物学基础如下。水平基因转移将DNA片段引入宿主基因组，由于不同的突变偏好和选择机制，这些片段的碱基组成和密码子使用通常会偏离基因组背景，这与分子生物学中心法则（DNA → RNA → 蛋白质）相符。捕获这些偏差的两个稳健的窗口级特征是鸟嘌呤-胞嘧啶（GC）含量和密码子使用偏好性度量（密码子偏好，CB）。这些特征在同质基因组区域内可以被建模为近似平稳的，而在对应于潜在基因组岛的片段中则会发生偏移。\n\n从一个基本的统计建模基础出发，假设观测到的特征序列具有以下生成模型。设有 $n$ 个窗口，索引为 $i \\in \\{1,\\dots,n\\}$。对于每个窗口 $i$，我们观测到一个特征向量 $x_i \\in \\mathbb{R}^2$，其两个分量分别是GC含量和密码子偏好。基因组被划分为 $K$ 个连续的片段，其断点 $1 \\le \\tau_1  \\tau_2  \\cdots  \\tau_{K-1}  n$ 未知。对于每个包含索引 $i \\in \\{\\tau_{s-1}+1,\\dots,\\tau_s\\}$ 的片段 $s \\in \\{1,\\dots,K\\}$（其中 $\\tau_0 = 0$ 且 $\\tau_K = n$），数据遵循一个多元正态分布，该分布具有片段特定的均值和共享的协方差：\n$$\nx_i \\mid s \\sim \\mathcal{N}(\\mu_s, \\Sigma), \\quad \\mu_s \\in \\mathbb{R}^2, \\quad \\Sigma \\in \\mathbb{R}^{2 \\times 2} \\text{ positive definite, independent of } s.\n$$\n在实际分析中，$\\Sigma$ 是未知的，应使用无偏样本协方差从整个序列中估计。为了数值稳定性，会加上一个小的岭正则化项 $\\epsilon I_2$，其中 $I_2$ 表示 $2 \\times 2$ 的单位矩阵，$\\epsilon  0$ 是一个非常小的数。\n\n基于此模型，定义一个选择 $K$ 和断点 $\\{\\tau_s\\}$ 的统计准则，即在给定片段均值 $\\{\\mu_s\\}$ 的情况下最大化高斯对数似然，同时使用贝叶斯信息准则（BIC）对模型复杂度进行惩罚。具体来说，当维度 $d = 2$ 时，通过对每个片段施加一个惩罚项，来线性地惩罚自由均值参数的数量（与 $K$ 成正比）：\n$$\n\\gamma = \\frac{1}{2} d \\log n.\n$$\n你的程序必须计算出在所有满足最小片段长度约束 $m_{\\min} \\in \\mathbb{N}$ 的分割中，能使惩罚目标函数达到最大值的精确解：\n- 对于一个跨越索引 $a+1,\\dots,b$ 且长度为 $\\ell = b-a$ 的候选片段，令 $\\bar{x}_{a:b} = \\frac{1}{\\ell}\\sum_{i=a+1}^b x_i$ 表示其经验均值。证明在对 $\\mu_s$ 最大化高斯似然之后，此片段对未惩罚目标函数的贡献（不计一个与分割无关的加法常数）可以写为\n$$\nS(a,b) = \\ell \\, \\bar{x}_{a:b}^{\\top} \\Sigma^{-1} \\bar{x}_{a:b}.\n$$\n- 那么，需要最大化的整体惩罚目标函数是\n$$\n\\sum_{s=1}^K S(\\tau_{s-1}, \\tau_s) - \\gamma K,\n$$\n约束条件为对所有 $s$ 都有 $\\tau_s - \\tau_{s-1} \\ge m_{\\min}$。\n\n算法要求：\n- 实现一个时间复杂度为 $\\mathcal{O}(n^2)$ 的精确动态规划算法，以找到最优集合 $\\{\\tau_s\\}_{s=1}^{K-1}$ 和 $K$。使用累积和来确保对于任何 $ab$，片段得分 $S(a,b)$ 可以在 $\\mathcal{O}(1)$ 时间内计算出来。", "solution": "所提出的问题经过了严格验证，并被认定是有效的。它在统计遗传学方面有科学依据，是良定的，并且为获得唯一的可计算解提供了所有必要的参数。因此，我们可以着手进行求解。\n\n目标是找到一组断点 $\\{\\tau_s\\}_{s=0}^K$（其中 $\\tau_0=0$ 且 $\\tau_K=n$），将一个包含 $n$ 个二维特征向量的序列 $\\{x_i\\}_{i=1}^n$ 划分为 $K$ 个连续的片段。此划分必须最大化以下惩罚目标函数：\n$$\n\\mathcal{O}(\\{\\tau_s\\}) = \\sum_{s=1}^K S(\\tau_{s-1}, \\tau_s) - \\gamma K\n$$\n其中，从索引 $a+1$ 到 $b$ 的片段得分 $S(a,b)$ 和惩罚项 $\\gamma$ 已在问题中定义。该分割受到最小片段长度 $m_{\\min}$ 的约束，即对于所有 $s=1, \\dots, K$，都有 $\\tau_s - \\tau_{s-1} \\ge m_{\\min}$。\n\n首先，我们必须推导片段得分 $S(a,b)$ 的表达式。片段 $s$（从索引 $a+1$ 到 $b$）中的数据 $x_i$ 被建模为从多元正态分布 $\\mathcal{N}(\\mu_s, \\Sigma)$ 中抽取的样本。此片段的对数似然为：\n$$\n\\log\\mathcal{L}_s(\\mu_s) = \\sum_{i=a+1}^{b} \\log p(x_i \\mid \\mu_s, \\Sigma) = \\sum_{i=a+1}^{b} \\left( C - \\frac{1}{2}(x_i - \\mu_s)^\\top \\Sigma^{-1} (x_i - \\mu_s) \\right)\n$$\n其中 $C = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma|$ 是一个不依赖于 $\\mu_s$ 的常数。为了对 $\\mu_s$ 最大化此似然，我们必须最小化二次型之和。对 $\\mu_s$ 求梯度并令其为零，可以得到均值的最大似然估计（MLE），即该片段的样本均值：\n$$\n\\hat{\\mu}_s = \\frac{1}{b-a} \\sum_{i=a+1}^{b} x_i = \\bar{x}_{a:b}\n$$\n将 $\\hat{\\mu}_s$ 代回到对数似然函数中，得到该片段的最大化值：\n$$\n\\log\\mathcal{L}_s^* = \\max_{\\mu_s} \\log\\mathcal{L}_s(\\mu_s) = (b-a)C - \\frac{1}{2} \\sum_{i=a+1}^{b} (x_i - \\bar{x}_{a:b})^\\top \\Sigma^{-1} (x_i - \\bar{x}_{a:b})\n$$\n二次项可以展开为：\n$$\n\\sum_{i=a+1}^{b} (x_i - \\bar{x}_{a:b})^\\top \\Sigma^{-1} (x_i - \\bar{x}_{a:b}) = \\left(\\sum_{i=a+1}^{b} x_i^\\top \\Sigma^{-1} x_i\\right) - (b-a) \\bar{x}_{a:b}^\\top \\Sigma^{-1} \\bar{x}_{a:b}\n$$\n一个划分 $\\{\\tau_s\\}$ 的总未惩罚目标是所有片段的最大化对数似然之和，即 $\\sum_{s=1}^K \\log\\mathcal{L}_s^*：$\n$$\n\\sum_{s=1}^K \\log\\mathcal{L}_s^* = \\sum_{s=1}^K \\left[ (b_s-a_s)C - \\frac{1}{2} \\left( \\left(\\sum_{i=a_s+1}^{b_s} x_i^\\top \\Sigma^{-1} x_i\\right) - (b_s-a_s) \\bar{x}_{a_s:b_s}^\\top \\Sigma^{-1} \\bar{x}_{a_s:b_s} \\right) \\right]\n$$\n其中 $(a_s, b_s) = (\\tau_{s-1}, \\tau_s)$。重新整理并使用符号 $S(a,b) = (b-a)\\bar{x}_{a:b}^\\top\\Sigma^{-1}\\bar{x}_{a:b}$ 和 $\\ell_s=b_s-a_s$：\n$$\n\\sum_{s=1}^K \\log\\mathcal{L}_s^* = nC - \\frac{1}{2} \\sum_{i=1}^{n} x_i^\\top \\Sigma^{-1} x_i + \\frac{1}{2} \\sum_{s=1}^K S(\\tau_{s-1}, \\tau_s)\n$$\n项 $nC$ 和 $\\sum_{i=1}^{n} x_i^\\top \\Sigma^{-1} x_i$ 相对于划分的选择是常数。因此，最大化未惩罚的对数似然等价于最大化 $\\sum_{s=1}^K S(\\tau_{s-1}, \\tau_s)$。问题定义了一个惩罚目标 $\\sum S - \\gamma K$，这是变点分析中的标准形式。这证实了使用 $S(a,b)$ 作为片段质量得分核心的有效性。\n\n该优化问题可以使用动态规划精确求解。令 $dp[j]$ 为包含前 $j$ 个窗口 $\\{x_1, \\dots, x_j\\}$ 的数据前缀进行分割时，惩罚目标函数的最大值。我们的目标是计算 $dp[n]$。递推关系通过考虑倒数第二个片段的所有可能终点 $i$ 来推导，其中最后一个片段跨越从 $i+1$ 到 $j$。\n$$\ndp[j] = \\max_{0 \\le i  j, \\, j-i \\ge m_{\\min}} \\left\\{ dp[i] + S(i, j) - \\gamma \\right\\}\n$$\n此处 $S(i,j)$ 中的索引 $i$ 和 $j$ 指的是从0到 $n$ 的、基于0的序列索引，因此从 $i$ 到 $j-1$ 的片段长度为 $j-i$。基本情况是 $dp[0]=0$，代表一个得分为零的空前缀。每向划分中添加一个片段，就会产生一个惩罚 $\\gamma$。\n\n算法流程如下：\n$1.$ 对于每个测试用例，使用指定的片段长度、均值、协方差和随机种子，生成总长度为 $n$ 的数据序列 $X = (x_0, \\dots, x_{n-1})$。\n$2.$ 估计共享协方差矩阵 $\\Sigma$。首先，计算整个序列 $X$ 的无偏样本协方差。然后，为了数值稳定性，加上一个小的岭正则化项 $\\epsilon I_2$，其中 $\\epsilon=10^{-6}$。最后，计算其逆矩阵 $\\Sigma^{-1}$。\n$3. \\!$ 计算惩罚项 $\\gamma = \\frac{1}{2}d\\log n = \\log n$，因为特征维度 $d=2$。\n$4. \\!$ 为了实现片段得分的 $\\mathcal{O}(1)$ 计算，预先计算特征向量的累积和。令 $C_X[k] = \\sum_{l=0}^{k-1} x_l$。从索引 $i$ 到 $j-1$ 的片段中的向量总和为 $C_X[j] - C_X[i]$。那么，这个长度为 $\\ell=j-i$ 的片段得分 $S(i, j)$ 就是 $\\frac{1}{\\ell} (C_X[j]-C_X[i])^\\top \\Sigma^{-1} (C_X[j]-C_X[i])$。\n$5. \\!$ 初始化两个大小为 $n+1$ 的数组：`dp` 用于存储最大得分，`ptr` 用于存储回溯指针以重构最优划分。设置 $dp[0]=0$，所有其他的 $dp[j]$ 设置为 $-\\infty$。\n$6. \\!$ 将 $j$ 从 $1$ 迭代到 $n$。对于每个 $j$，将 $i$ 从 $0$ 迭代到 $j - m_{\\min}$。计算以片段 $[i, j-1]$ 结尾的新划分的得分：`score = dp[i] + S(i, j) - gamma`。如果该得分大于当前的 $dp[j]$，则将 $dp[j]$ 更新为 `score` 并设置 `ptr[j] = i`。\n$7. \\!$ DP表填充完毕后，从 $ptr[n]$ 开始通过回溯来重构最优断点。从 `curr = n` 开始，重复寻找前一个断点 `prev = ptr[curr]`，并将 `prev-1` 添加到断点列表中（如果 `prev > 0`）。反转列表以获得最终排序好的断点。这个过程也隐式地确定了最优的片段数量 $K$。\n\n由于对 $j$ 和 $i$ 的嵌套循环，此动态规划算法的时间复杂度为 $\\mathcal{O}(n^2)$，存储DP表的空间复杂度为 $\\mathcal{O}(n)$。在给定的约束条件下，这在计算上是可行的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the change-point segmentation for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"lengths\": [60, 40, 80],\n            \"means\": [(0.36, 0.48), (0.52, 0.62), (0.37, 0.49)],\n            \"variances\": [0.0004, 0.0004],\n            \"m_min\": 5,\n            \"seed\": 7,\n        },\n        {\n            \"lengths\": [150],\n            \"means\": [(0.40, 0.55)],\n            \"variances\": [0.0009, 0.0009],\n            \"m_min\": 5,\n            \"seed\": 11,\n        },\n        {\n            \"lengths\": [50, 30, 40, 25, 35],\n            \"means\": [(0.38, 0.50), (0.50, 0.60), (0.39, 0.51), (0.53, 0.63), (0.38, 0.49)],\n            \"variances\": [0.0004, 0.0004],\n            \"m_min\": 5,\n            \"seed\": 13,\n        },\n        {\n            \"lengths\": [80, 30, 70],\n            \"means\": [(0.40, 0.55), (0.41, 0.56), (0.40, 0.55)],\n            \"variances\": [0.0009, 0.0009],\n            \"m_min\": 5,\n            \"seed\": 17,\n        },\n        {\n            \"lengths\": [6, 3, 3],\n            \"means\": [(0.36, 0.48), (0.52, 0.62), (0.36, 0.48)],\n            \"variances\": [0.0004, 0.0004],\n            \"m_min\": 3,\n            \"seed\": 19,\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # 1. Generate synthetic data\n        rng = np.random.default_rng(case[\"seed\"])\n        segments_data = []\n        cov_true = np.diag(case[\"variances\"])\n        for length, mean in zip(case[\"lengths\"], case[\"means\"]):\n            segment = rng.multivariate_normal(mean, cov_true, size=length)\n            segments_data.append(segment)\n        \n        X = np.concatenate(segments_data, axis=0)\n        n, d = X.shape\n        m_min = case[\"m_min\"]\n\n        # 2. Estimate shared covariance and penalty\n        if n > 1:\n            cov_est = np.cov(X, rowvar=False, ddof=1)\n        else: # Handle case with a single data point\n             cov_est = np.zeros((d,d))\n\n        epsilon = 1e-6\n        sigma_reg = cov_est + epsilon * np.identity(d)\n        sigma_inv = np.linalg.inv(sigma_reg)\n        \n        gamma = d / 2.0 * np.log(n) if n > 0 else 0\n\n        # 3. Pre-compute cumulative sums\n        # cum_sums[k] stores sum of x_0 to x_{k-1}\n        cum_sums = np.zeros((n + 1, d))\n        cum_sums[1:] = np.cumsum(X, axis=0)\n        \n        # 4. Dynamic Programming\n        dp = np.full(n + 1, -np.inf)\n        pointers = np.zeros(n + 1, dtype=int)\n        dp[0] = 0\n\n        for j in range(1, n + 1):\n            for i in range(j):\n                length = j - i\n                if length >= m_min:\n                    # Calculate S(i, j)\n                    sum_vec = cum_sums[j] - cum_sums[i]\n                    # S(a,b) = l * x_bar.T * Sigma_inv * x_bar\n                    #      = l * (sum/l).T * Sigma_inv * (sum/l)\n                    #      = (1/l) * sum.T * Sigma_inv * sum\n                    s_ij = (1.0 / length) * (sum_vec.T @ sigma_inv @ sum_vec)\n                    \n                    score = dp[i] + s_ij - gamma\n                    if score > dp[j]:\n                        dp[j] = score\n                        pointers[j] = i\n\n        # 5. Backtrack to find breakpoints\n        breakpoints = []\n        current_idx = n\n        while current_idx > 0:\n            prev_idx = pointers[current_idx]\n            if prev_idx > 0:\n                # Breakpoint is between (prev_idx - 1) and prev_idx\n                breakpoints.append(prev_idx - 1)\n            current_idx = prev_idx\n            \n        breakpoints.sort()\n        all_results.append(breakpoints)\n\n    # Final print statement in the exact required format\n    results_str = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "2805995"}, {"introduction": "水平基因转移最明确的证据，来自于基因的进化历史与其宿主物种的进化历史之间的不一致。本练习将深入探讨系统发育HGT推断的核心，要求您计算将基因树与物种树进行协调所需的最少转移事件数。您将实现一个算法来计算子树剪切和重接（subtree prune-and-regraft, SPR）距离，从而具体地理解基于简约法的树协调方法。[@problem_id:2806011]", "problem": "要求您形式化一个基于简约性（parsimony-based）的重构方法，用于处理基因树和物种树之间的水平基因转移。请完全在单拷贝（single-copy）设定下进行操作：每个物种在基因树中只出现一次。在此机制下，当只计算水平基因转移事件（不允许基因复制和丢失）时，重构基因树与物种树所需的最小转移边数等于两个有根树之间的有根子树剪接（rooted subtree prune-and-regraft）距离。\n\n从以下基本定义和事实出发：\n- 一个有根、叶节点标记、严格二叉的树，其每个内部节点恰好有两个子节点，并有一个指定的根。每个叶节点都由一个物种名称标记。\n- 在只允许转移的单拷贝机制中，简约性框架下的基因树与物种树重构，对应于最小化水平转移边的数量。这些转移边应用后，能将物种树的拓扑结构转换为基因树的拓扑结构。该数量等于有根子树剪接距离，即所需操作的最小次数。其中，一次操作指通过剪切一条边来移除一个连接的子树，然后通过细分目标边（在该处创建一个新的内部节点）来重新连接它，从而保持树的有根、严格二叉结构。\n- 对于具有相同叶节点集的有根树，在这种只考虑转移的简约性设定下，每个转移事件恰好对应一次有根子树剪接移动。\n\n您的任务是实现一个程序，该程序：\n- 解析以 Newick 字符串形式给出的、共享四个分类单元 $\\{A,B,C,D\\}$ 叶节点集的有根二叉树。\n- 计算重构基因树与物种树所需的最小水平转移边数，该数量等于两棵树之间的有根子树剪接距离。\n\n您必须实现一个通用算法，通过广度优先搜索（breadth-first search）来探索有根子树剪接邻居的空间，以计算精确的最小距离。您的实现必须：\n- 将树表示为有根、严格二叉、叶节点标记的结构。\n- 通过剪除任何非根节点的子树，并将其重新连接到剩余树的任意边上（通过细分该边）或在剩余树之上创建一个新根，来生成所有单步有根子树剪接邻居。通过将其剩余的子节点重新连接到其父节点，来抑制剪切操作产生的任何度为$1$的内部节点，从而保持树的有根、严格二叉结构。\n- 使用一种规范的、对顺序不敏感的有根树编码方式，以便在搜索过程中，仅因子节点顺序不同而等价的树被识别为相同的树。\n\n测试套件中的所有输入都恰好涉及四个分类单元，并要求输出整数。不涉及物理单位或角度单位。最终输出必须是整数。\n\n测试套件：\n- 案例 $1$：物种树 $S_1 = \\text{\"((A,B),(C,D))\"}$，基因树 $G_1 = \\text{\"((A,B),(C,D))\"}$。预期的最小转移数是一个整数。\n- 案例 $2$：物种树 $S_2 = \\text{\"((A,B),(C,D))\"}$，基因树 $G_2 = \\text{\"(((A,B),C),D)\"}$。预期的最小转移数是一个整数。\n- 案例 $3$：物种树 $S_3 = \\text{\"((A,B),(C,D))\"}$，基因树 $G_3 = \\text{\"((A,C),(B,D))\"}$。预期的最小转移数是一个整数。\n- 案例 $4$：物种树 $S_4 = \\text{\"(((A,C),B),D)\"}$，基因树 $G_4 = \\text{\"(((A,B),C),D)\"}$。预期的最小转移数是一个整数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[r1,r2,r3,r4]”），其中 $r_i$ 是按上述顺序为案例 $i$ 计算出的最小转移边数。", "solution": "所提出的问题是计算系统发育学中一个明确定义的问题。它要求在一个特定的简约性模型下，计算重构基因树与物种树所需的最小水平基因转移事件数。该模型将转移数等同于两棵树之间的有根子树剪接（rSPR）距离。这个问题在科学上是合理的、自洽的，并且在算法上是可行的，因为它涉及生物信息学中的既定概念。\n\n任务是在一个图中找到物种树 $S$ 和基因树 $G$ 之间的最短路径，该图的顶点代表给定叶节点集上所有可能的有根、严格二叉树拓扑结构，边代表单次 rSPR 操作。给定四个分类单元的叶节点集 $\\{A, B, C, D\\}$，可能的树拓扑结构数量是有限的，具体为 $(2 \\times 4 - 3)!! = 5 \\times 3 \\times 1 = 15$。由于每次 rSPR 操作的成本统一为 $1$，该问题可以使用广度优先搜索（BFS）算法解决。\n\n解决方案的核心组成部分如下：\n\n_1. 树的表示_：树被表示为一个相互连接的节点系统。每个节点都拥有指向其父节点和两个子节点的指针。叶节点通过分类单元标签（例如，'$A$'）来区分，而内部节点没有直接标签，而是由它们所根植的子树中所有叶节点标签的集合来定义。\n\n_2. 规范的树编码_：为了在搜索过程中管理状态空间，特别是为了识别先前访问过的树，需要一种规范的表示方法。这种编码必须对任何内部节点处子节点的顺序不敏感。一个递归定义提供了这样的编码：\n- 叶节点的规范形式是其标签。\n- 内部节点的规范形式是通过首先获取其两个子节点 $c_1$ 和 $c_2$ 的规范形式来构建的。然后按字典序对这两个字符串进行排序，得到 $s_1$ 和 $s_2$，使得 $s_1 \\le s_2$。该内部节点的规范形式即为字符串 $\"(\" + s_1 + \",\" + s_2 + \")\"$.\n例如，一个结构为 $((D,C),(B,A))$ 的树将被规范化为 `\"((A,B),(C,D))\"`。\n\n_3. rSPR 邻居生成_：BFS 算法通过为给定树 $T$ 生成所有单步 rSPR 邻居来探索状态空间。此过程包括两个步骤：剪枝和再嫁接。\n- _剪枝_：选择并剪断连接父节点 $p$ 和非根子节点 $c$ 的边 $(p, c)$。这将分离以 $c$ 为根的子树 $T_c$。节点 $p$ 现在只有一个子节点，它被抑制：其剩余的子节点直接连接到 $p$ 的父节点（如果 $p$ 是根，则成为新的根）。这留下了一棵残余树 $T'$。\n- _再嫁接_：然后重新连接被剪枝的子树 $T_c$。有两种再嫁接的可能性：\n    a. 细分残余树 $T'$ 中的一条现有边 $(u,v)$。沿此边插入一个新节点 $n$，成为 $v$ 的父节点。被剪枝的子树 $T_c$ 作为 $n$ 的另一个子节点被连接。\n    b. 创建一个新根。为整棵树创建一个新根节点，其两个子节点分别是残余树 $T'$ 的根和被剪枝子树 $T_c$ 的根。\n\n_4. 广度优先搜索（BFS）_：BFS 算法系统地寻找最短路径。\n- 用起始物种树和距离 $0$ 初始化一个队列。\n- 一个集合 `visited` 存储已遇到树的规范形式，以防止重复计算。\n- 算法通过从队列中取出一棵树，检查它是否是目标基因树来进行。如果不是，则生成其所有唯一的 rSPR 邻居。\n- 任何其规范形式尚未被访问的邻居都会被添加到 `visited` 集合中，并以增加后的距离入队。\n- 当出队树的规范形式与目标基因树的规范形式匹配时，搜索终止。相关联的距离即为所需的最小转移次数。\n\n这种结构化的方法保证了能够找到最短的 rSPR 距离，从而按要求正确地形式化了基于简约性的水平基因转移重构。实现将首先把 Newick 字符串解析为基于节点的树结构，然后应用 BFS 算法，并结合所描述的规范编码和邻居生成逻辑。", "answer": "```python\nimport collections\nimport copy\n\nclass Node:\n    \"\"\"Represents a node in a rooted binary tree.\"\"\"\n    def __init__(self, label=None):\n        self.label = label\n        self.parent = None\n        self.left = None\n        self.right = None\n\n    def __repr__(self):\n        if self.label:\n            return f\"Node('{self.label}')\"\n        return f\"Node(internal)\"\n\ndef parse_newick(newick_str):\n    \"\"\"\n    Parses a Newick string into a tree of Node objects.\n    This parser is designed for strictly binary trees.\n    \"\"\"\n    s = newick_str.strip()\n    if not s.startswith('('):\n        return Node(label=s)\n\n    # Remove outer parentheses\n    content = s[1:-1]\n    \n    # Find the split comma\n    balance = 0\n    split_idx = -1\n    for i, char in enumerate(content):\n        if char == '(':\n            balance += 1\n        elif char == ')':\n            balance -= 1\n        elif char == ',' and balance == 0:\n            split_idx = i\n            break\n            \n    left_str = content[:split_idx]\n    right_str = content[split_idx+1:]\n\n    parent_node = Node()\n    parent_node.left = parse_newick(left_str)\n    parent_node.right = parse_newick(right_str)\n    \n    # Set parent pointers\n    if parent_node.left:\n        parent_node.left.parent = parent_node\n    if parent_node.right:\n        parent_node.right.parent = parent_node\n        \n    return parent_node\n\ndef get_canonical_form(node):\n    \"\"\"\n    Computes a canonical, order-invariant string representation of the tree.\n    \"\"\"\n    if node.label:\n        return node.label\n    \n    left_form = get_canonical_form(node.left)\n    right_form = get_canonical_form(node.right)\n    \n    # Sort children's canonical forms lexicographically\n    if left_form > right_form:\n        left_form, right_form = right_form, left_form\n        \n    return f\"({left_form},{right_form})\"\n\ndef get_all_nodes(root):\n    \"\"\"Returns a list of all nodes in the tree via pre-order traversal.\"\"\"\n    nodes = []\n    stack = [root]\n    while stack:\n        node = stack.pop()\n        if node:\n            nodes.append(node)\n            stack.append(node.right)\n            stack.append(node.left)\n    return nodes\n    \ndef generate_spr_neighbors(root):\n    \"\"\"\n    Generates all unique rSPR neighbors for a given tree.\n    Returns a set of canonical forms of the neighbors.\n    \"\"\"\n    neighbors = set()\n    original_root_form = get_canonical_form(root)\n\n    # Get all nodes that can be pruned (any non-root node)\n    nodes_in_tree = get_all_nodes(root)\n    nodes_to_prune = [n for n in nodes_in_tree if n.parent is not None]\n\n    for node_to_prune in nodes_to_prune:\n        # --- 1. Prune step ---\n        # Deepcopy to create a mutable version of the tree for this prune operation\n        root_copy = copy.deepcopy(root)\n        \n        # Find the corresponding nodes in the copied tree\n        # This mapping is safe because deepcopy preserves structure\n        path_to_prune_node = []\n        curr = node_to_prune\n        while curr.parent:\n            if curr.parent.left == curr:\n                path_to_prune_node.append('L')\n            else:\n                path_to_prune_node.append('R')\n            curr = curr.parent\n        \n        pruned_node_copy = root_copy\n        for move in reversed(path_to_prune_node):\n            pruned_node_copy = pruned_node_copy.left if move == 'L' else pruned_node_copy.right\n\n        parent_copy = pruned_node_copy.parent\n        sibling_copy = parent_copy.left if parent_copy.right == pruned_node_copy else parent_copy.right\n        grandparent_copy = parent_copy.parent\n\n        # Perform wiring to suppress the degree-2 node\n        if grandparent_copy:\n            if grandparent_copy.left == parent_copy:\n                grandparent_copy.left = sibling_copy\n            else:\n                grandparent_copy.right = sibling_copy\n            sibling_copy.parent = grandparent_copy\n            residual_root = root_copy\n        else: # parent was the root\n            residual_root = sibling_copy\n            sibling_copy.parent = None\n        \n        pruned_subtree_root = pruned_node_copy\n        pruned_subtree_root.parent = None\n\n        # --- 2. Regraft step ---\n        # Get all edges in the residual tree\n        residual_nodes = get_all_nodes(residual_root)\n        edges_to_graft = []\n        for n in residual_nodes:\n            if n.parent is not None:\n                edges_to_graft.append((n.parent, n))\n\n        # a) Regraft by subdividing an edge\n        for p_graft, c_graft in edges_to_graft:\n            # Need fresh copies for each new topology\n            res_root_copy = copy.deepcopy(residual_root)\n            pruned_part_copy = copy.deepcopy(pruned_subtree_root)\n            \n            # Find the graft edge in the copied residual tree\n            path_to_child = []\n            curr = c_graft\n            while curr.parent:\n                if curr.parent.left == curr:\n                    path_to_child.append('L')\n                else:\n                    path_to_child.append('R')\n                curr = curr.parent\n\n            graft_child_copy = res_root_copy\n            for move in reversed(path_to_child):\n                graft_child_copy = graft_child_copy.left if move == 'L' else graft_child_copy.right\n\n            graft_parent_copy = graft_child_copy.parent\n\n            # Create new node and insert\n            new_node = Node()\n            new_node.left = graft_child_copy\n            new_node.right = pruned_part_copy\n            graft_child_copy.parent = new_node\n            pruned_part_copy.parent = new_node\n            \n            if graft_parent_copy.left == graft_child_copy:\n                graft_parent_copy.left = new_node\n            else:\n                graft_parent_copy.right = new_node\n            new_node.parent = graft_parent_copy\n            \n            new_tree_form = get_canonical_form(res_root_copy)\n            if new_tree_form != original_root_form:\n                 neighbors.add(new_tree_form)\n\n        # b) Regraft above the residual root\n        new_root = Node()\n        new_root.left = residual_root\n        new_root.right = pruned_subtree_root\n        residual_root.parent = new_root\n        pruned_subtree_root.parent = new_root\n        \n        new_tree_form = get_canonical_form(new_root)\n        if new_tree_form != original_root_form:\n            neighbors.add(new_tree_form)\n\n    return neighbors\n\ndef solve():\n    test_cases = [\n        (\"((A,B),(C,D))\", \"((A,B),(C,D))\"),\n        (\"((A,B),(C,D))\", \"(((A,B),C),D)\"),\n        (\"((A,B),(C,D))\", \"((A,C),(B,D))\"),\n        (\"(((A,C),B),D)\", \"(((A,B),C),D)\")\n    ]\n\n    results = []\n    for species_str, gene_str in test_cases:\n        species_tree = parse_newick(species_str)\n        gene_tree = parse_newick(gene_str)\n\n        start_form = get_canonical_form(species_tree)\n        target_form = get_canonical_form(gene_tree)\n\n        if start_form == target_form:\n            results.append(0)\n            continue\n\n        # BFS for shortest path\n        queue = collections.deque([(start_form, species_tree, 0)])\n        visited = {start_form}\n\n        distance = -1\n        while queue:\n            current_form, current_tree, dist = queue.popleft()\n\n            if current_form == target_form:\n                distance = dist\n                break\n\n            neighbor_forms = generate_spr_neighbors(current_tree)\n            \n            for neighbor_form in neighbor_forms:\n                if neighbor_form not in visited:\n                    visited.add(neighbor_form)\n                    # We need the tree structure to generate the next layer of neighbors\n                    neighbor_tree = parse_newick(neighbor_form)\n                    queue.append((neighbor_form, neighbor_tree, dist + 1))\n        \n        results.append(distance)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2806011"}]}