{"hands_on_practices": [{"introduction": "成瘾的神经基础深植于奖赏回路中持久的突触可塑性。此练习将引导您从生物物理学的第一性原理出发，模拟药物引起的突触变化如何改变神经元的电生理特性。通过对兴奋性突触后电流（$EPSC$）进行建模，您将量化AMPA与NMDA受体比例的变化如何影响信号的幅度和时间动态，这是理解药物如何重塑大脑回路学习和记忆能力的核心技能 [@problem_id:2605797]。", "problem": "一个伏隔核中型多棘神经元表现出药物诱发的突触可塑性，该可塑性改变了谷氨酸能突触的受体组成。考虑由脉冲样谷氨酸瞬变共同激活 α-氨基-3-羟基-5-甲基-4-异恶唑丙酸受体 (AMPAR) 和 N-甲基-D-天冬氨酸受体 (NMDAR) 的电导所产生的宏观兴奋性突触后电流 (EPSC)。将 AMPAR 组分建模为 $$I_{\\mathrm{A}}(t,V)=g_{\\mathrm{A},0}\\,s_{\\mathrm{A}}(t)\\,(V-E_{\\mathrm{A}}),$$ 其中 $$s_{\\mathrm{A}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{A}}}\\right),\\quad E_{\\mathrm{A}}=0\\,\\mathrm{mV},$$ 将 NMDAR 组分建模为 $$I_{\\mathrm{N}}(t,V)=g_{\\mathrm{N},0}\\,s_{\\mathrm{N}}(t)\\,B(V)\\,(V-E_{\\mathrm{N}}),$$ 其中 $$s_{\\mathrm{N}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{N}}}\\right),\\quad E_{\\mathrm{N}}=0\\,\\mathrm{mV},$$ 镁离子阻断因子采用经过充分验证的形式 $$B(V)=\\frac{1}{1+\\dfrac{[\\mathrm{Mg}^{2+}]}{3.57}\\,\\exp(-0.062\\,V)},$$ 其中 $$[\\mathrm{Mg}^{2+}]=1\\,\\mathrm{mM}.$$ 总 EPSC 为 $$I_{\\mathrm{tot}}(t,V)=I_{\\mathrm{A}}(t,V)+I_{\\mathrm{N}}(t,V).$$ 设衰减时间常数为 $$\\tau_{\\mathrm{A}}=5\\,\\mathrm{ms},\\quad \\tau_{\\mathrm{N}}=100\\,\\mathrm{ms}.$$\n\n将 AMPA/NMDA 比率 $$R$$ 操作性地定义为：在 $$V=-70\\,\\mathrm{mV}$$ 时的峰值 AMPAR 介导电流的绝对值，与刺激后 $$t=50\\,\\mathrm{ms}$$ 时在 $$V=+40\\,\\mathrm{mV}$$ 处评估的 NMDAR 介导电流的绝对值之比（每个组分为测量而进行了药理学分离）。在基线条件下，$$R_{\\mathrm{pre}}=0.40$$，而在促进钙离子通透性 AMPAR (CP-AMPAR) 插入的干预之后，$$R_{\\mathrm{post}}=1.20$$。\n\n任务 1. 从上述定义出发，仅使用普适物理原理（例如离子电流的 Ohm 定律和通道动力学的指数松弛），推导一个表达式，说明 $$R$$ 从 $$R_{\\mathrm{pre}}$$ 增加到 $$R_{\\mathrm{post}}$$ 如何改变 (i) $$V=-70\\,\\mathrm{mV}$$ 时 $$I_{\\mathrm{tot}}(t,V)$$ 的峰值振幅，以及 (ii) $$V=-70\\,\\mathrm{mV}$$ 时的有效加权衰减常数。该常数定义为峰值时刻两个指数组分的振幅加权平均值 $$\\tau_{\\mathrm{w}}(V)$$，$$\\tau_{\\mathrm{w}}(V)=\\dfrac{A_{\\mathrm{A}}(V)\\,\\tau_{\\mathrm{A}}+A_{\\mathrm{N}}(V)\\,\\tau_{\\mathrm{N}}}{A_{\\mathrm{A}}(V)+A_{\\mathrm{N}}(V)},$$ 其中 $$A_{\\mathrm{A}}(V)$$ 和 $$A_{\\mathrm{N}}(V)$$ 分别是 $$t=0^{+}$$ 时 AMPAR 和 NMDAR 组分的瞬时振幅。用 $$R,$$ $$\\tau_{\\mathrm{A}},$$ $$\\tau_{\\mathrm{N}},$$ $$B(V),$$ 和驱动力来表示你的结果。陈述你所做的任何近似，并从生理学上证明其合理性。\n\n任务 2. 干预后，你药理学分离出 AMPAR 介导的电流，并测量了以下稳态电流-电压数据点：在 $$V=-70\\,\\mathrm{mV},\\ I=-180\\,\\mathrm{pA};$$ 在 $$V=-40\\,\\mathrm{mV},\\ I=-120\\,\\mathrm{pA};$$ 在 $$V=0\\,\\mathrm{mV},\\ I=0\\,\\mathrm{pA};$$ 以及在 $$V=+40\\,\\mathrm{mV},\\ I=+25\\,\\mathrm{pA}.$$ 假设 AMPAR 反转电位为 $$E_{\\mathrm{A}}=0\\,\\mathrm{mV}.$$ 将整流指数定义为 $$V=-70\\,\\mathrm{mV}$$ 和 $$V=+40\\,\\mathrm{mV}$$ 处的弦电导绝对值之比，$$\\mathrm{RI}=\\dfrac{|I(-70)|/|{-}70-E_{\\mathrm{A}}|}{|I(+40)|/|{+}40-E_{\\mathrm{A}}|}.$$ 根据给定数据计算 $$\\mathrm{RI}$$。\n\n最终答案只报告 $$\\mathrm{RI}$$ 的值，无单位，四舍五入到四位有效数字。", "solution": "在尝试求解之前，对问题陈述进行了严格验证。\n\n步骤 1：提取已知条件。\n提供的信息如下：\nAMPAR 电流：$I_{\\mathrm{A}}(t,V)=g_{\\mathrm{A},0}\\,s_{\\mathrm{A}}(t)\\,(V-E_{\\mathrm{A}})$\nAMPAR 动力学：$s_{\\mathrm{A}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{A}}}\\right)$\nAMPAR 反转电位：$E_{\\mathrm{A}}=0\\,\\mathrm{mV}$\nAMPAR 衰减时间常数：$\\tau_{\\mathrm{A}}=5\\,\\mathrm{ms}$\n\nNMDAR 电流：$I_{\\mathrm{N}}(t,V)=g_{\\mathrm{N},0}\\,s_{\\mathrm{N}}(t)\\,B(V)\\,(V-E_{\\mathrm{N}})$\nNMDAR 动力学：$s_{\\mathrm{N}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{N}}}\\right)$\nNMDAR 反转电位：$E_{\\mathrm{N}}=0\\,\\mathrm{mV}$\nNMDAR 衰减时间常数：$\\tau_{\\mathrm{N}}=100\\,\\mathrm{ms}$\n镁离子阻断因子：$B(V)=\\frac{1}{1+\\dfrac{[\\mathrm{Mg}^{2+}]}{3.57}\\,\\exp(-0.062\\,V)}$\n镁离子浓度：$[\\mathrm{Mg}^{2+}]=1\\,\\mathrm{mM}$\n常数 $3.57$ 的单位是 $\\mathrm{mM}$，而 $0.062$ 的单位是 $\\mathrm{mV}^{-1}$。\n\n总电流：$I_{\\mathrm{tot}}(t,V)=I_{\\mathrm{A}}(t,V)+I_{\\mathrm{N}}(t,V)$\n\nAMPA/NMDA 比率定义：$R = \\frac{|I_{\\mathrm{A, peak}} \\text{ at } V=-70\\,\\mathrm{mV}|}{|I_{\\mathrm{N}} \\text{ at } V=+40\\,\\mathrm{mV}, t=50\\,\\mathrm{ms}|}$\n基线比率：$R_{\\mathrm{pre}}=0.40$\n干预后比率：$R_{\\mathrm{post}}=1.20$\n\n任务 1 定义：\n$V=-70\\,\\mathrm{mV}$ 时 $I_{\\mathrm{tot}}(t,V)$ 的峰值振幅。\n有效加权衰减常数：$\\tau_{\\mathrm{w}}(V)=\\dfrac{A_{\\mathrm{A}}(V)\\,\\tau_{\\mathrm{A}}+A_{\\mathrm{N}}(V)\\,\\tau_{\\mathrm{N}}}{A_{\\mathrm{A}}(V)+A_{\\mathrm{N}}(V)}$，其中 $A_{\\mathrm{A}}(V)$ 和 $A_{\\mathrm{N}}(V)$ 是 $t=0^{+}$ 时的瞬时振幅。\n\n任务 2 数据（干预后，药理学分离的 AMPAR 电流）：\n在 $V=-70\\,\\mathrm{mV}$ 时，$I=-180\\,\\mathrm{pA}$\n在 $V=-40\\,\\mathrm{mV}$ 时，$I=-120\\,\\mathrm{pA}$\n在 $V=0\\,\\mathrm{mV}$ 时，$I=0\\,\\mathrm{pA}$\n在 $V=+40\\,\\mathrm{mV}$ 时，$I=+25\\,\\mathrm{pA}$\n\n任务 2 定义：\n整流指数：$\\mathrm{RI}=\\dfrac{|I(-70)|/|{-}70-E_{\\mathrm{A}}|}{|I(+40)|/|{+}40-E_{\\mathrm{A}}|}$\n\n步骤 2：使用提取的已知条件进行验证。\n该问题具有科学依据。AMPAR 和 NMDAR 电流的模型，包括电压依赖性镁离子阻断，是计算神经科学和突触生理学中使用的标准、经验证实的表示方法。参数值（$\\tau_{\\mathrm{A}}$、$\\tau_{\\mathrm{N}}$、$[\\mathrm{Mg}^{2+}]$）在生理学上是现实的。AMPA/NMDA 比率的概念及其通过突触可塑性的调节，包括表现出内向整流的 CP-AMPAR 的插入，是现代神经科学的基石。该问题是良定的，为任务 1 的推导和任务 2 的计算提供了所有必要的定义和数据。任务是客观且无歧义的。问题是自洽且内部一致的。\n\n步骤 3：结论与行动。\n问题有效。现在将提供完整的解决方案。\n\n解决方案分为两部分，对应于两个任务。\n\n任务 1：EPSC 特性变化的推导\n\n基于问题描述的“促进钙离子通透性 AMPAR 插入的干预”，核心假设是 AMPA/NMDA 比率 $R$ 的变化是由于最大 AMPAR 电导 $g_{\\mathrm{A},0}$ 的变化，而最大 NMDAR 电导 $g_{\\mathrm{N},0}$ 保持不变。\n\n首先，我们将实验比率 $R$ 和生物物理参数之间的关系形式化。根据定义：\n$$R = \\frac{|I_{\\mathrm{A, peak}}(-70\\,\\mathrm{mV})|}{|I_{\\mathrm{N}}(t=50\\,\\mathrm{ms}, V=+40\\,\\mathrm{mV})|}$$\n由于指数衰减模型，峰值 AMPAR 电流出现在 $t=0^{+}$。\n$$|I_{\\mathrm{A, peak}}(-70\\,\\mathrm{mV})| = |I_{\\mathrm{A}}(0, -70)| = |g_{\\mathrm{A},0} \\exp(-0) (-70 - 0)| = 70\\,g_{\\mathrm{A},0}$$\n在指定点处的 NMDAR 电流为：\n$$|I_{\\mathrm{N}}(50, 40)| = |g_{\\mathrm{N},0} \\exp(-50/\\tau_{\\mathrm{N}}) B(40) (40 - 0)| = 40\\,g_{\\mathrm{N},0} B(40) \\exp(-50/\\tau_{\\mathrm{N}})$$\n将这些代入 $R$ 的定义中：\n$$R = \\frac{70\\,g_{\\mathrm{A},0}}{40\\,g_{\\mathrm{N},0} B(40) \\exp(-50/\\tau_{\\mathrm{N}})}$$\n我们可以定义峰值电导之比 $\\gamma = g_{\\mathrm{A},0} / g_{\\mathrm{N},0}$。上述方程表明 $\\gamma$ 与 $R$ 成正比。\n$$\\gamma = \\frac{g_{\\mathrm{A},0}}{g_{\\mathrm{N},0}} = R \\cdot \\left( \\frac{40 B(40) \\exp(-50/\\tau_{\\mathrm{N}})}{70} \\right) = \\frac{R}{K}$$\n其中 $K$ 是一个定义如下的常数：\n$$K = \\frac{70}{40 B(40) \\exp(-50/\\tau_{\\mathrm{N}})} = \\frac{7}{4 B(40) \\exp(-1/2)}$$\n\n(i) $V=-70\\,\\mathrm{mV}$ 时 $I_{\\mathrm{tot}}(t,V)$ 峰值振幅的变化。\n总电流为 $I_{\\mathrm{tot}}(t, -70) = I_{\\mathrm{A}}(t, -70) + I_{\\mathrm{N}}(t, -70)$。两个组分都由从 $t=0$ 开始的衰减指数描述，并且具有相同的符号（内向电流）。因此，总电流的峰值大小出现在 $t=0^{+}$。\n$$I_{\\mathrm{peak}} = I_{\\mathrm{tot}}(0, -70) = g_{\\mathrm{A},0}(-70) + g_{\\mathrm{N},0} B(-70) (-70)$$\n峰值振幅是绝对值：\n$$A_{\\mathrm{tot}}(-70) = |I_{\\mathrm{peak}}| = 70 (g_{\\mathrm{A},0} + g_{\\mathrm{N},0} B(-70))$$\n我们使用电导比 $\\gamma = g_{\\mathrm{A},0}/g_{\\mathrm{N},0}$ 来表示这个振幅：\n$$A_{\\mathrm{tot}}(-70) = 70 g_{\\mathrm{N},0} (\\gamma + B(-70))$$\n代入 $\\gamma = R/K$：\n$$A_{\\mathrm{tot}}(-70) = 70 g_{\\mathrm{N},0} \\left(\\frac{R}{K} + B(-70)\\right)$$\n为了量化当 $R$ 从 $R_{\\mathrm{pre}}$ 变为 $R_{\\mathrm{post}}$ 时振幅如何变化，我们计算干预后与干预前振幅之比。由于 $g_{\\mathrm{N},0}$ 是常数，它被消去了。\n$$\\frac{A_{\\mathrm{tot, post}}}{A_{\\mathrm{tot, pre}}} = \\frac{70 g_{\\mathrm{N},0} \\left(\\frac{R_{\\mathrm{post}}}{K} + B(-70)\\right)}{70 g_{\\mathrm{N},0} \\left(\\frac{R_{\\mathrm{pre}}}{K} + B(-70)\\right)} = \\frac{R_{\\mathrm{post}} + K B(-70)}{R_{\\mathrm{pre}} + K B(-70)}$$\n这个表达式显示了 $V=-70\\,\\mathrm{mV}$ 处的总峰值电流振幅如何随 AMPA/NMDA 比率 $R$ 变化。\n\n(ii) 有效加权衰减常数 $\\tau_{\\mathrm{w}}(-70)$ 的变化。\n定义为 $\\tau_{\\mathrm{w}}(V)=\\dfrac{A_{\\mathrm{A}}(V)\\,\\tau_{\\mathrm{A}}+A_{\\mathrm{N}}(V)\\,\\tau_{\\mathrm{N}}}{A_{\\mathrm{A}}(V)+A_{\\mathrm{N}}(V)}$。\n在 $V=-70\\,\\mathrm{mV}$ 时，$t=0^{+}$ 时的组分振幅为：\n$$A_{\\mathrm{A}}(-70) = |I_{\\mathrm{A}}(0, -70)| = 70 g_{\\mathrm{A},0}$$\n$$A_{\\mathrm{N}}(-70) = |I_{\\mathrm{N}}(0, -70)| = 70 g_{\\mathrm{N},0} B(-70)$$\n将这些代入 $\\tau_{\\mathrm{w}}$ 的定义中：\n$$\\tau_{\\mathrm{w}}(-70) = \\frac{(70 g_{\\mathrm{A},0})\\tau_{\\mathrm{A}} + (70 g_{\\mathrm{N},0} B(-70))\\tau_{\\mathrm{N}}}{(70 g_{\\mathrm{A},0}) + (70 g_{\\mathrm{N},0} B(-70))}$$\n将分子和分母同除以 $70 g_{\\mathrm{N},0}$：\n$$\\tau_{\\mathrm{w}}(-70) = \\frac{(g_{\\mathrm{A},0}/g_{\\mathrm{N},0})\\tau_{\\mathrm{A}} + B(-70)\\tau_{\\mathrm{N}}}{(g_{\\mathrm{A},0}/g_{\\mathrm{N},0}) + B(-70)} = \\frac{\\gamma \\tau_{\\mathrm{A}} + B(-70)\\tau_{\\mathrm{N}}}{\\gamma + B(-70)}$$\n代入 $\\gamma = R/K$，我们得到加权时间常数作为 $R$ 的函数的表达式：\n$$\\tau_{\\mathrm{w}}(R) = \\frac{\\frac{R}{K} \\tau_{\\mathrm{A}} + B(-70)\\tau_{\\mathrm{N}}}{\\frac{R}{K} + B(-70)} = \\frac{R \\tau_{\\mathrm{A}} + K B(-70)\\tau_{\\mathrm{N}}}{R + K B(-70)}$$\n这个表达式描述了有效衰减动力学如何随 $R$ 变化。由于 $\\tau_{\\mathrm{A}} << \\tau_{\\mathrm{N}}$，R（以及 $\\gamma$）的增加会增加更快的 AMPA 组分的权重，导致总体加权时间常数 $\\tau_{\\mathrm{w}}$ 减小。\n\n任务 2：整流指数的计算\n\n整流指数 ($\\mathrm{RI}$) 是针对干预后的 AMPAR 介导电流，使用所提供的数据和定义计算的。反转电位为 $E_{\\mathrm{A}}=0\\,\\mathrm{mV}$。定义为：\n$$\\mathrm{RI}=\\frac{\\text{chord conductance at } -70\\,\\mathrm{mV}}{\\text{chord conductance at } +40\\,\\mathrm{mV}} = \\frac{|I(-70)|/|-70 - E_{\\mathrm{A}}|}{|I(+40)|/|+40 - E_{\\mathrm{A}}|}$$\n为干预后状态提供的数据是：\n$I(-70) = -180\\,\\mathrm{pA}$\n$I(+40) = +25\\,\\mathrm{pA}$\n\n将这些值代入公式中：\n$$\\mathrm{RI} = \\frac{|-180\\,\\mathrm{pA}| / |-70 - 0\\,\\mathrm{mV}|}{|+25\\,\\mathrm{pA}| / |+40 - 0\\,\\mathrm{mV}|}$$\n$\\mathrm{pA/mV}$（电导）的单位被消去。\n$$\\mathrm{RI} = \\frac{180/70}{25/40} = \\frac{18/7}{5/8}$$\n$$\\mathrm{RI} = \\frac{18}{7} \\times \\frac{8}{5} = \\frac{144}{35}$$\n转换为小数值：\n$$\\mathrm{RI} = 4.1142857...$$\n按照问题陈述的要求，四舍五入到四位有效数字：\n$$\\mathrm{RI} \\approx 4.114$$\n这个值显著大于 $1$，表明存在强烈的内向整流，这与所描述的通常缺少 GluA2 亚基的钙离子通透性 AMPAR 的插入是一致的。", "answer": "$$\\boxed{4.114}$$", "id": "2605797"}, {"introduction": "现代成瘾治疗策略常常利用精妙的药理学原理，而部分激动剂的使用便是一个经典范例。这个练习是一个思想实验，旨在揭示部分激动剂的双重作用：在缺少强效激动剂的环境中，它能提供温和的奖赏效应；而在强效激动剂存在时，它又能扮演功能性拮抗剂的角色，降低整体的奖赏信号。理解这一基于竞争性结合和内在效能的机制，对于掌握现代成瘾药理学和治疗方法的精髓至关重要 [@problem_id:2605751]。", "problem": "一位研究者建立模型，研究在成瘾治疗中使用的部分激动剂如何在一个已被完全激动剂驱动至高效能状态的受体系统中产生功能性拮抗作用。考虑一个单一的受体群体，它整合了两种配体的竞争性结合，并将占有率转导为下游信号，该信号通过中脑边缘通路中的时相性多巴胺信号来调节强化概率。该研究者做出以下与受体理论和奖赏学习一致的假设：(i) 竞争性结合遵循质量作用定律；(ii) 每种配体对净受体激活的贡献与其内在效能成比例；(iii) 净受体激活与线索和结果诱发的时相性多巴胺的振幅单调相关，后者是奖赏预测误差（RPE）的生物学基础，正RPE会增强强化，而负RPE会削弱强化。\n\n两种配体与相同的受体相互作用：一种完全激动剂 $F$ 和一种部分激动剂 $P$。完全激动剂的平衡解离常数 $K_A^F = 100\\,\\mathrm{nM}$，内在效能 $\\varepsilon_F = 1.0$（归一化值），而部分激动剂的 $K_A^P = 5\\,\\mathrm{nM}$，内在效能 $\\varepsilon_P = 0.4$。系统的最大转导能力并不仅仅由受体数量决定饱和（也就是说，不存在无限的受体储备），并且在本文所考虑的操作范围内，净受体激活可以近似地视为与竞争性平衡下每种配体经效能加权的部分占有率成正比。\n\n研究者研究了两种情境：\n\n1) 高效能环境（共同给药）：一只经过训练以自我给药产生有效浓度为 $C_F = 300\\,\\mathrm{nM}$ 的完全激动剂 $F$ 的动物（其稳定的期望是基于先前单独使用 $F$ 时所经历的结果），现在给予含有 $C_F = 300\\,\\mathrm{nM}$ 的 $F$ 和 $C_P = 50\\,\\mathrm{nM}$ 的部分激动剂 $P$ 的混合物。\n\n2) 低效能环境（替代）：在另一次没有完全激动剂的实验中（例如，在戒断期间），同一只动物仅接受浓度为 $C_P = 50\\,\\mathrm{nM}$ 的 $P$，在给药时刻对药物效果的期望很低。\n\n仅使用质量作用定律和受体效能概念的标准推论，以及净受体激活与奖赏预测误差符号之间的联系，选择最准确地解释部分激动剂如何能在高效能环境中充当功能性拮抗剂，并预测其在所述两种情境中对强化影响的选项。\n\nA) 在高效能环境中，$P$ 由于具有更高的亲和力而竞争性地从受体上取代 $F$，用低效能的占有取代部分高效能的占有，从而降低相对于单独使用 $F$ 时的净受体激活；这产生一个负的奖赏预测误差并削弱强化。在低效能环境中，单独使用 $P$ 会产生中等程度的受体激活、一个小的正奖赏预测误差和适度的强化。\n\nB) 根据定义，部分激动剂不能拮抗；无论浓度如何，它们总是增加完全激动剂的效果，从而在两种环境中都增加净受体激活并产生更强的强化。\n\nC) 在高效能环境中，$P$ 通过防止受体脱敏来增强强化，使线索诱发的多巴胺高于单独由 $F$ 产生的水平；在低效能环境中，$P$ 没有效果，因为强化需要完全激动剂。\n\nD) $P$ 对强化的影响仅取决于 $\\varepsilon_P$，而不取决于 $K_A^P$ 或 $F$ 的现行浓度；部分激动剂既不取代完全激动剂，也不会以浓度依赖的方式改变奖赏预测误差，因此在两种环境中强化都保持不变。", "solution": "在尝试任何解答之前，对问题陈述进行严格验证。\n\n### 步骤1：提取已知条件\n- **系统**：一个单一的受体群体。\n- **配体**：一种完全激动剂 $F$ 和一种部分激动剂 $P$ 竞争性地结合到受体上。\n- **信号转导**：受体激活被转导成影响时相性多巴胺释放的信号，该信号作为奖赏预测误差（RPE）发挥作用。\n- **假设**：\n    - (i) 竞争性结合遵循质量作用定律。\n    - (ii) 每种配体对净受体激活的贡献与其内在效能（$\\varepsilon$）成比例。\n    - (iii) 净受体激活与时相性多巴胺振幅（RPE）单调相关。正RPE增强强化；负RPE削弱强化。\n- **完全激动剂（$F$）的参数**：\n    - 平衡解离常数：$K_A^F = 100\\,\\mathrm{nM}$\n    - 内在效能：$\\varepsilon_F = 1.0$\n- **部分激动剂（$P$）的参数**：\n    - 平衡解离常数：$K_A^P = 5\\,\\mathrm{nM}$\n    - 内在效能：$\\varepsilon_P = 0.4$\n- **系统约束**：系统没有无限的受体储备。净受体激活近似与每种配体经效能加权的部分占有率成正比。\n- **情境1：高效能环境（共同给药）**\n    - 一只动物用单独的完全激动剂 $F$ 训练，建立了稳定的期望。\n    - 给予一种混合物，包含：\n        - $F$ 的浓度：$C_F = 300\\,\\mathrm{nM}$\n        - $P$ 的浓度：$C_P = 50\\,\\mathrm{nM}$\n- **情境2：低效能环境（替代）**\n    - 动物仅接受部分激动剂 $P$。\n    - $P$ 的浓度：$C_P = 50\\,\\mathrm{nM}$\n    - 期望：对药物效果的期望很低。\n\n### 步骤2：使用提取的已知条件进行验证\n对问题的有效性进行评估。\n\n- **科学基础**：该问题建立在药理学基本且公认的原理之上，特别是受体理论（竞争性结合、质量作用定律、部分占有率、内在效能）和神经科学的一个基石概念（多巴胺在奖赏预测误差中的作用）。部分激动剂在完全激动剂存在下充当功能性拮抗剂的情景是药理学教学中使用的经典例子。参数和浓度在药理学上是合理的。该问题在科学上是合理的。\n- **适定性**：问题提供了所有必要的定量数据（$K_A$值、浓度、效能）和一个清晰的理论框架以推导出唯一解。问题表述精确，要求解释部分激动剂的双重作用及其对强化的影响，这可以从所提供的模型中确定。\n- **客观性**：问题以客观、技术性的语言陈述，没有歧义或主观断言。\n\n问题陈述没有显示任何缺陷。它不是科学上不合理、不可形式化、不完整、矛盾、不现实或不适定的。\n\n### 步骤3：结论和行动\n问题陈述是**有效的**。将推导解答。\n\n### 推导\n\n净受体激活，我们表示为 $E_{net}$，被给定为与配体经效能加权的部分占有率之和成正比。我们可以将这种关系写为：\n$$E_{net} = \\varepsilon_F f_F + \\varepsilon_P f_P$$\n其中 $f_F$ 和 $f_P$ 分别是完全激动剂 $F$ 和部分激动剂 $P$ 对受体的部分占有率。根据竞争性结合的质量作用定律，它们由以下公式给出：\n$$f_F = \\frac{[F]/K_A^F}{1 + [F]/K_A^F + [P]/K_A^P}$$\n$$f_P = \\frac{[P]/K_A^P}{1 + [F]/K_A^F + [P]/K_A^P}$$\n这里，$[F]$ 和 $[P]$ 是配体的浓度。\n\n我们分析三个相关状态：设定期望的基线状态、高效能情境和低效能情境。\n\n**1. 基线状态（期望）**\n动物用浓度为 $C_F = 300\\,\\mathrm{nM}$ 的完全激动剂 $F$ 单独训练。这设定了期望的受体激活水平。在此状态下，$[F] = 300\\,\\mathrm{nM}$ 且 $[P] = 0$。\n$F$ 的部分占有率为：\n$$f_{F, baseline} = \\frac{[F]/K_A^F}{1 + [F]/K_A^F} = \\frac{300\\,\\mathrm{nM} / 100\\,\\mathrm{nM}}{1 + 300\\,\\mathrm{nM} / 100\\,\\mathrm{nM}} = \\frac{3}{1 + 3} = \\frac{3}{4} = 0.75$$\n基线受体激活 $E_{baseline}$ 为：\n$$E_{baseline} = \\varepsilon_F f_{F, baseline} = 1.0 \\times 0.75 = 0.75$$\n这个值 $0.75$ 代表了期望的结果。\n\n**2. 情境1：高效能环境（共同给药）**\n动物接受 $F$ 和 $P$ 的混合物。浓度为 $[F] = 300\\,\\mathrm{nM}$ 和 $[P] = 50\\,\\mathrm{nM}$。我们首先计算归一化浓度项：\n$$[F]/K_A^F = 300\\,\\mathrm{nM} / 100\\,\\mathrm{nM} = 3$$\n$$[P]/K_A^P = 50\\,\\mathrm{nM} / 5\\,\\mathrm{nM} = 10$$\n部分占有率方程的公分母是 $1 + 3 + 10 = 14$。\n部分占有率为：\n$$f_F = \\frac{3}{14}$$\n$$f_P = \\frac{10}{14} = \\frac{5}{7}$$\n在此情境下的净受体激活 $E_1$ 为：\n$$E_1 = \\varepsilon_F f_F + \\varepsilon_P f_P = (1.0) \\left(\\frac{3}{14}\\right) + (0.4) \\left(\\frac{10}{14}\\right) = \\frac{3}{14} + \\frac{4}{14} = \\frac{7}{14} = 0.5$$\n将实际结果 $E_1 = 0.5$ 与期望结果 $E_{baseline} = 0.75$ 相比较，我们发现 $E_1 < E_{baseline}$。这对应于一个负的奖赏预测误差（RPE）。根据假设(iii)，负RPE会削弱强化。\n其机制是，部分激动剂 $P$ 由于其高亲和力（低 $K_A^P = 5\\,\\mathrm{nM}$），有效地与完全激动剂 $F$（亲和力较低 $K_A^F = 100\\,\\mathrm{nM}$）竞争并取代之。由于被取代的高效能配体（$\\varepsilon_F = 1.0$）被低效能配体（$\\varepsilon_P = 0.4$）所替代，总的受体激活水平下降。这就是功能性拮抗的定义。\n\n**3. 情境2：低效能环境（替代）**\n动物仅接受浓度为 $[P] = 50\\,\\mathrm{nM}$ 的部分激动剂 $P$。在此状态下，$[F] = 0$。问题陈述说对药物效果的期望很低，我们将其建模为期望激活 $E_{expected} \\approx 0$。\n$P$ 的部分占有率为：\n$$f_{P, alone} = \\frac{[P]/K_A^P}{1 + [P]/K_A^P} = \\frac{50\\,\\mathrm{nM} / 5\\,\\mathrm{nM}}{1 + 50\\,\\mathrm{nM} / 5\\,\\mathrm{nM}} = \\frac{10}{1 + 10} = \\frac{10}{11}$$\n在此情境下的净受体激活 $E_2$ 为：\n$$E_2 = \\varepsilon_P f_{P, alone} = 0.4 \\times \\frac{10}{11} = \\frac{4}{11} \\approx 0.364$$\n将实际结果 $E_2 \\approx 0.364$ 与期望结果 $E_{expected} \\approx 0$ 相比较，我们发现 $E_2 > E_{expected}$。这对应于一个正的奖赏预测误差（RPE）。根据假设(iii)，正RPE会增强强化。在此情境下，部分激动剂本身作为一种激动剂，产生中等程度的受体激活和适度的强化。\n\n### 逐项分析\n\n**A) 在高效能环境中，$P$ 由于具有更高的亲和力而竞争性地从受体上取代 $F$，用低效能的占有取代部分高效能的占有，从而降低相对于单独使用 $F$ 时的净受体激活；这产生一个负的奖赏预测误差并削弱强化。在低效能环境中，单独使用 $P$ 会产生中等程度的受体激活、一个小的正奖赏预测误差和适度的强化。**\n这个陈述是对上述定量推导的精确定性总结。部分激动剂 $P$ 具有更高的亲和力（$K_A^P=5\\,\\mathrm{nM}$ vs $K_A^F=100\\,\\mathrm{nM}$）并取代了 $F$。这种替换将净激活从 $0.75$ 降低到 $0.5$，产生了一个负的RPE并削弱了强化。在没有 $F$ 的情况下，$P$ 本身从接近零的基线产生了约 $0.364$ 的激活，创造了一个正的RPE和适度的强化。根据所提供的模型，这个描述是完美准确的。\n**结论：正确。**\n\n**B) 根据定义，部分激动剂不能拮抗；无论浓度如何，它们总是增加完全激动剂的效果，从而在两种环境中都增加净受体激活并产生更强的强化。**\n这个陈述在药理学上是错误的。如果部分激动剂降低了共同给药的完全激动剂产生的净效应，那么它就起到了拮抗剂的作用。我们的计算明确显示了净激活的减少（$0.75 \\rightarrow 0.5$）。“部分激动剂不能拮抗”的前提是一个不正确的概括。\n**结论：不正确。**\n\n**C) 在高效能环境中，$P$ 通过防止受体脱敏来增强强化，使线索诱发的多巴胺高于单独由 $F$ 产生的水平；在低效能环境中，$P$ 没有效果，因为强化需要完全激动剂。**\n这个选项引入了受体脱敏的概念，而问题陈述或其假设中并未提及。我们必须仅使用所提供的信息来解决问题。此外，我们的计算显示了净激活的*减少*，而不是增加。声称 $P$ 在低效能环境中“没有效果”也是错误的；作为一种激动剂（尽管是部分的），它产生了显著的效果（$E_2 \\approx 0.364$）。\n**结论：不正确。**\n\n**D) $P$ 对强化的影响仅取决于 $\\varepsilon_P$，而不取决于 $K_A^P$ 或 $F$ 的现行浓度；部分激动剂既不取代完全激动剂，也不会以浓度依赖的方式改变奖赏预测误差，因此在两种环境中强化都保持不变。**\n这个陈述与质量作用定律相矛盾，后者是问题陈述中给出的一个基本原则（假设i）。部分占有率，以及因此的净效应，明确是所有配体浓度及其各自解离常数（$K_A$）的函数。$P$ 对 $F$ 的取代是竞争性结合的直接结果，而竞争性结合是浓度和亲和力依赖的。整个分析表明，RPE和强化在两种情境下确实都发生了变化。\n**结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "2605751"}, {"introduction": "要真正理解奖赏学习的动态过程，我们必须借助计算模型。此练习将指导您编写一个程序来实现时间差（TD）学习算法，这是解释多巴胺神经元如何编码奖赏预测误差（$RPE$）的基石模型。通过模拟一个经典的条件反射任务，您将亲手重现一个里程碑式的发现：随着学习的进行，编码预测误差的多巴胺信号如何从奖赏本身“转移”到最早能够预测奖赏的线索上 [@problem_id:2605752]。", "problem": "编写一个程序，模拟在具有可变线索-奖励延迟的确定性线索-奖励任务中的时相性多巴胺反应，以展示随着价值估计的收敛，该反应如何从奖励发放转移到线索出现。使用离散时间、分幕式设置，并基于以下基本依据和假设，这些在关于奖励回路和成瘾生理学的中脑边缘多巴胺系统功能的强化学习模型中被广泛接受：\n\n- 时相性多巴胺信号与未来主要奖励的预期折扣总和的连续估计值之间的瞬时差异，加上该时刻收到的任何主要奖励成正比。\n- 试验是独立的幕。每幕包含在时间步 $0$ 出现的一个简短预测性线索，随后是 $D$ 秒的固定延迟，之后发放大小为 $R$ 的非负主要奖励。时间被离散化为 $1$ 秒的步长，因此以步长计的延迟等于 $D$。\n- 智能体为试验中线索出现后、奖励发放前的每个时间步维护一个标量状态值表。线索前基线状态的固定值为 $0$ 且不被更新。奖励发放后，终止状态的值为 $0$。\n- 在所有试验中，智能体使用恒定的学习率 $\\alpha$ 和恒定的折扣因子 $\\gamma$。在每幕试验内，智能体使用单步自举法，通过将上述差异按比例 $\\alpha$ 移动以减小该差异，从而顺序地更新每个试验内状态的价值。对于确定性任务，这些估计值会随着试验的进行而收敛。\n- 将给定试验中在线索出现时模型预测的时相性多巴胺反应定义为从线索前基线（固定值为 $0$）到第一个线索后状态的状态转移中计算出的差异。将奖励发放时模型预测的时相性多巴胺反应定义为在主要奖励发放时，从最后一个奖励前状态到终止状态的转移中计算出的差异。如果延迟为 $0$ 秒，则将该幕视为在线索-奖励同时发生时从基线到终止状态的单次转移；在这种边界情况下，报告相同的差异作为与线索对齐和与奖励对齐的反应。\n\n完全按照上述说明实现该模拟，并为下面测试套件中的每个参数集计算以下四个量：\n- 试验 $1$ 中与线索对齐的反应，\n- 试验 $1$ 中与奖励对齐的反应，\n- 最后一次试验中与线索对齐的反应，\n- 最后一次试验中与奖励对齐的反应。\n\n所有反应都是无单位的（任意单位）。将报告的每个浮点数四舍五入到三位小数。时间步长为 $1$ 秒；延迟 $D$ 以秒为单位提供。不涉及角度。\n\n测试套件（每个元组为 $(\\gamma,\\alpha,R,D,T)$）：\n- 案例 A (理想路径): $\\gamma=0.95$, $\\alpha=0.20$, $R=1.0$, $D=3$, $T=200$。\n- 案例 B (长延迟，慢学习): $\\gamma=0.99$, $\\alpha=0.10$, $R=1.0$, $D=10$, $T=400$。\n- 案例 C (即时奖励边界): $\\gamma=0.95$, $\\alpha=0.30$, $R=1.0$, $D=0$, $T=100$。\n- 案例 D (高折扣): $\\gamma=0.50$, $\\alpha=0.20$, $R=1.0$, $D=4$, $T=200$。\n\n您的程序应生成单行输出，其中包含所有案例的结果，格式为逗号分隔的列表的列表，不含空格，并采用以下确切格式：\n[[cue1_caseA,reward1_caseA,cuelast_caseA,rewardlast_caseA],[cue1_caseB,reward1_caseB,cuelast_caseB,rewardlast_caseB],[cue1_caseC,reward1_caseC,cuelast_caseC,rewardlast_caseC],[cue1_caseD,reward1_caseD,cuelast_caseD,rewardlast_caseD]]", "solution": "问题陈述待验证。\n\n**第 1 步：提取已知信息**\n- **模型原理：** 时相性多巴胺信号与时间差分 (TD) 误差成正比：即对未来预期奖励的连续估计值之间的差异，加上任何即时奖励。\n- **任务结构：** 离散时间，分幕式。每次试验在线索时间步 $0$ 出现，延迟 $D$ 秒，然后在时间步 $D$ 发放主要奖励 $R \\ge 0$。时间步长为 $1$ 秒。\n- **状态值：**\n    - 线索前基线状态值固定为 $V_{pre-cue} = 0$。\n    - 终止状态值（奖励后）固定为 $V_{terminal} = 0$。\n    - 试验内状态 $s_0, s_1, \\dots, s_{D-1}$ 的值是学习得来的。设它们的值为 $V_0, V_1, \\dots, V_{D-1}$。\n    - 所有可学习状态的初始值为 $0$。\n- **学习参数：** 恒定学习率 $\\alpha$，恒定折扣因子 $\\gamma$。\n- **更新规则：** 价值更新使用单步自举法：$V(s) \\leftarrow V(s) + \\alpha \\cdot (\\text{差异})$。在一次试验中，对每个状态的更新是顺序的。根据标准的 TD 学习，这被解释为在每次试验结束时进行批量更新，其中对于每个状态 $s_t$，使用 TD 误差 $\\delta_t$ 更新 $V_t$。\n- **反应定义：**\n    - 与线索对齐的反应：从线索前状态到第一个线索后状态（$s_0$）的转移上的 TD 误差。\n    - 与奖励对齐的反应：从最后一个奖励前状态（$s_{D-1}$）到终止状态的转移上的 TD 误差。\n- **边界情况 ($D=0$)：**\n    - 该任务是从线索前基线到终止状态的单次转移。\n    - 奖励 $R$ 在线索出现时发放。\n    - 与线索对齐和与奖励对齐的反应是相同的，等于这次单次转移的 TD 误差。\n- **输入参数：** 一组元组 $(\\gamma, \\alpha, R, D, T)$，其中 $T$ 是总试验次数。\n- **要求输出：** 对于每个参数集，一个包含四个量的列表：[试验 1 的线索反应，试验 1 的奖励反应，试验 T 的线索反应，试验 T 的奖励反应]，每个值都四舍五入到三位小数。\n\n**第 2 步：使用提取的已知信息进行验证**\n根据验证标准评估问题。\n\n- **科学依据：** 该问题是多巴胺功能的时间差分 (TD) 模型的直接实现，这是计算神经科学和强化学习中的一个基本概念。该模型由 Schultz、Dayan 和 Montague 提出，假定时相性多巴胺活动编码了奖励预测误差 (RPE)，从而驱动学习。所指定的机制是分幕式、确定性巴甫洛夫条件反射范式中 TD(0) 学习的标准公式。它在科学上是合理的。\n- **良构的：** 该问题是良构的。系统的动态（价值更新）由一组差分方程明确定义。初始条件已指定（试验 1 中所有可学习状态的 $V_t=0$）。参数已提供。对于一个确定性任务，当 $\\gamma < 1$ 且学习率为恒定正数时，价值估计保证收敛到一个唯一的稳定解。需要计算的量被精确定义。\n- **客观性：** 问题以客观、数学化的语言陈述，没有主观或模糊的术语。\n- **完整性和一致性：** 问题提供了所有必要信息：模型方程、初始/边界条件、学习参数以及模拟过程的完整描述。对于 $D=0$ 的特殊情况有明确且一致的定义。没有矛盾之处。\n- **可行性：** 测试套件中给出的参数值在物理上和计算上都是合理的。该模拟在计算上是可行的。\n\n**第 3 步：结论和行动**\n该问题有效。这是神经奖励系统计算建模中的一个典型练习。将提供一个解决方案。\n\n**基于原理的解决方案**\n\n该问题要求模拟一个时间差分 (TD) 学习智能体，以建模时相性多巴胺反应。其核心科学原理是，多巴胺神经元发出奖励预测误差 (RPE) 信号，即实际结果与其预测之间的差异。此 RPE 用于更新对未来事件的预测。\n\n**1. TD 模型的形式化**\n令 $V_t^{(k)}$ 为在试验 $k$ 开始时处于状态 $s_t$ 的价值，其中 $t \\in \\{0, 1, \\dots, D-1\\}$ 对应于自线索出现以来经过的时间（秒）。初始状态值为对所有 $t$ 都有 $V_t^{(1)} = 0$。线索前和终止状态的值是固定的：$V_{pre-cue} = 0$ 和 $V_{terminal} = 0$。\n\n从状态 $s_t$ 到状态 $s_{t+1}$ 并伴有即时奖励 $r_{t+1}$ 的转移的单步 TD 误差 $\\delta_t$ 为：\n$$ \\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) $$\n在这个特定任务中，奖励只在最后发放。设试验 $k$ 开始时的价值向量为 $\\mathbf{V}^{(k)} = [V_0^{(k)}, V_1^{(k)}, \\dots, V_{D-1}^{(k)}]$。\n\n在试验 $k$ 内每个状态转移的 TD 误差为：\n- 对于延迟状态之间的转移，$s_t \\to s_{t+1}$ 其中 $t \\in \\{0, 1, \\dots, D-2\\}$：\n$$ \\delta_t^{(k)} = (r=0) + \\gamma V_{t+1}^{(k)} - V_t^{(k)} $$\n- 对于从最后一个延迟状态 $s_{D-1}$ 到终止状态的最终转移：\n$$ \\delta_{D-1}^{(k)} = R + \\gamma V_{terminal} - V_{D-1}^{(k)} = R - V_{D-1}^{(k)} $$\n\n然后，这些误差被用来更新下一次试验 $k+1$ 的状态值：\n$$ V_t^{(k+1)} = V_t^{(k)} + \\alpha \\delta_t^{(k)} \\quad \\text{for } t \\in \\{0, 1, \\dots, D-1\\} $$\n\n**2. 时相性多巴胺反应**\n问题将模型的“多巴胺反应”定义为特定的 TD 误差。\n- **与线索对齐的反应：** 这是从固定值的线索前状态 ($V_{pre-cue}=0$) 到第一个线索后状态 $s_0$ 的转移上的 TD 误差。\n$$ \\delta_{cue}^{(k)} = (r=0) + \\gamma V_0^{(k)} - V_{pre-cue} = \\gamma V_0^{(k)} $$\n- **与奖励对齐的反应：** 这是从最后一个奖励前状态 $s_{D-1}$到终止状态的转移上的 TD 误差，即 $\\delta_{D-1}^{(k)}$。\n$$ \\delta_{reward}^{(k)} = R + \\gamma V_{terminal} - V_{D-1}^{(k)} = R - V_{D-1}^{(k)} $$\n\n**3. 收敛性分析**\n随着试验次数 $k \\to \\infty$，价值收敛到一个稳态 $\\mathbf{V}^*$，此时更新量为零（$\\delta_t^* = 0$）。\n对于 $t \\in \\{0, 1, \\dots, D-2\\}$，$\\delta_t^* = \\gamma V_{t+1}^* - V_t^* = 0 \\implies V_t^* = \\gamma V_{t+1}^*$。\n对于 $t = D-1$，$\\delta_{D-1}^* = R - V_{D-1}^* = 0 \\implies V_{D-1}^* = R$。\n通过反向归纳法，任何状态 $s_t$ 的收敛价值是未来奖励 $R$ 的折扣价值：\n$$ V_t^* = \\gamma^{D-1-t} R $$\n因此，收敛后的反应为：\n- $\\delta_{cue}^* = \\gamma V_0^* = \\gamma (\\gamma^{D-1}R) = \\gamma^D R$\n- $\\delta_{reward}^* = R - V_{D-1}^* = R - R = 0$\n这个结果展示了预测误差信号从奖励时刻转移到最早的预测性线索时刻的典型特征。\n\n**4. 边界情况 ($D=0$)**\n当延迟 $D=0$ 时，线索和奖励是同时发生的。该事件是从线索前状态到终止状态的单次转移。没有需要学习价值的中间状态 $s_t$。这次单次转移的 TD 误差为：\n$$ \\delta = R + \\gamma V_{terminal} - V_{pre-cue} = R + \\gamma(0) - 0 = R $$\n问题规定，对于所有试验，这个单一的误差值 $R$ 同时作为与线索对齐和与奖励对齐的反应进行报告，因为没有学习发生。\n\n**5. 算法实现**\n模拟实现如下：\n1. 对于每个测试案例 $(\\gamma, \\alpha, R, D, T)$，首先检查是否 $D=0$。如果是，结果为 $[R, R, R, R]$。\n2. 如果 $D > 0$，初始化一个大小为 $D$、元素全为零的 NumPy 数组 `V`。这代表状态价值 $V_t^{(1)}$。\n3. 计算试验 $1$ 的反应：$\\delta_{cue}^{(1)} = \\gamma V_0^{(1)}$ 和 $\\delta_{reward}^{(1)} = R - V_{D-1}^{(1)}$。\n4. 循环 $T-1$ 次迭代以模拟从试验 $1$ 到试验 $T-1$ 的更新。在每次迭代中：\n    a. 使用当前价值向量 `V` 计算 TD 误差向量 `td_errors`。\n    b. 更新价值向量：`V = V + alpha * td_errors`。\n5. 循环结束后，`V` 包含试验 $T$ 开始时的价值，即 $V_t^{(T)}$。\n6. 计算最后一次试验的反应：$\\delta_{cue}^{(T)} = \\gamma V_0^{(T)}$ 和 $\\delta_{reward}^{(T)} = R - V_{D-1}^{(T)}$。\n7. 收集四个计算出的反应，将它们四舍五入到三位小数，并按指定格式化输出。\n这个过程正确地计算了价值函数以及相关预测误差在多次试验中的演变。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (0.95, 0.20, 1.0, 3, 200),  # Case A\n        (0.99, 0.10, 1.0, 10, 400), # Case B\n        (0.95, 0.30, 1.0, 0, 100),  # Case C\n        (0.50, 0.20, 1.0, 4, 200),  # Case D\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = simulate_td_learning(*params)\n        formatted_result = f\"[{','.join(f'{x:.3f}' for x in result)}]\"\n        all_results.append(formatted_result)\n\n    print(f\"[[{','.join(all_results)}]]\")\n\n\ndef simulate_td_learning(gamma, alpha, R, D, T):\n    \"\"\"\n    Simulates a deterministic cue-reward task using a temporal difference (TD) model.\n\n    Args:\n        gamma (float): Discount factor.\n        alpha (float): Learning rate.\n        R (float): Reward magnitude.\n        D (int): Delay in time steps (seconds) from cue to reward.\n        T (int): Total number of trials.\n\n    Returns:\n        list: A list of four floats: [cue_response_trial_1, reward_response_trial_1,\n               cue_response_last_trial, reward_response_last_trial].\n    \"\"\"\n    # Handle the boundary case where D=0 (immediate reward)\n    if D == 0:\n        # No learning occurs as there are no intermediate states.\n        # The TD error is R for the single transition from pre-cue to terminal.\n        # This error is reported for both cue and reward response on all trials.\n        return [R, R, R, R]\n\n    # Initialize state values V_t for t=0..D-1 for the first trial (k=1).\n    # V_t^(1) = 0 for all t.\n    V = np.zeros(D, dtype=float)\n\n    # --- Calculate responses for Trial 1 ---\n    # Cue response is the TD error for pre-cue -> s_0 transition: delta_cue = gamma * V_0 - V_precue\n    cue_response_trial_1 = gamma * V[0]\n    # Reward response is the TD error for s_{D-1} -> terminal transition: delta_reward = R - V_{D-1}\n    reward_response_trial_1 = R - V[D-1]\n\n    # --- Simulate T trials to find values at the start of the last trial (T) ---\n    # We need to perform T-1 updates to get from the values for trial 1 to trial T.\n    for _ in range(T - 1):\n        td_errors = np.zeros(D, dtype=float)\n\n        # Calculate TD errors for transitions s_t -> s_{t+1} for t=0..D-2\n        # delta_t = gamma * V_{t+1} - V_t\n        if D > 1:\n            td_errors[:-1] = gamma * V[1:] - V[:-1]\n\n        # Calculate TD error for transition s_{D-1} -> terminal\n        # delta_{D-1} = R - V_{D-1}\n        td_errors[-1] = R - V[-1]\n\n        # Update all state values for the next trial\n        # V_t^(k+1) = V_t^(k) + alpha * delta_t^(k)\n        V += alpha * td_errors\n\n    # At this point, V contains the values for the start of trial T.\n    \n    # --- Calculate responses for the last trial (T) ---\n    cue_response_last_trial = gamma * V[0]\n    reward_response_last_trial = R - V[-1]\n\n    return [\n        cue_response_trial_1,\n        reward_response_trial_1,\n        cue_response_last_trial,\n        reward_response_last_trial,\n    ]\n\nsolve()\n```", "id": "2605752"}]}