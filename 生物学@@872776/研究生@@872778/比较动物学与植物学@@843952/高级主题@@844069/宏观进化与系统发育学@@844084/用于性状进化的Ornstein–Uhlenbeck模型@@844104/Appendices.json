{"hands_on_practices": [{"introduction": "这项实践从定义Ornstein-Uhlenbeck (OU)过程的基础——随机微分方程(SDE)入手。通过推导性状方差随时间变化的方程，您将学习到模型参数（如 $\\alpha$ 和 $\\sigma^2$）如何直接影响演化轨迹。这项练习不仅能巩固模型的理论基础，还能让您切实体验如何校准模型以匹配观察到的生物学模式。[@problem_id:2592891]", "problem": "一位比较植物学家正在一棵高度为 $T$ 的超度量系统发育树上，为一个连续的植物性状（例如，单位面积叶质量）校准一个 Ornstein–Uhlenbeck (OU) 模型。该性状根据以下随机微分方程演化：\n$$\n\\mathrm{d}X_t \\;=\\; \\alpha\\,(\\theta - X_t)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}B_t,\n$$\n其中 $B_t$ 是标准布朗运动，$\\alpha \\gt 0$ 是朝向长期最优值 $\\theta$ 的稳定选择强度，$\\sigma \\gt 0$ 是扩散（微观演化）方差参数。化石约束将根节点的状态锚定在最优值，因此 $X_0 = \\theta$，且不存在祖先方差和测量误差。对于一棵超度量树，树尖处的边际方差等于沿长度为 $T$ 的谱系上 $X_T$ 的方差。\n\n给定：\n- 树高 $T = 12$ 百万年 (Myr)，记为 $T = 12$。\n- 选择强度 $\\alpha = 0.8$ 每百万年，记为 $\\alpha = 0.8$。\n- 目标树尖间方差（在移除物种内采样方差后）$V_T = 3.5$，单位为性状值的平方，记为 $V_T = 3.5$。\n\n从上述随机微分方程出发，并且仅使用线性随机微分方程的基本性质和 Itô 等距定理，推导树尖方差在时间 $T$ 与参数 $\\alpha$、$\\sigma$ 和 $T$ 之间的关系，并用它来校准 $\\sigma^2$，使得时间 $T$ 的边际方差等于 $V_T$。然后使用给定的 $T$、$\\alpha$ 和 $V_T$ 对你的表达式进行数值计算。\n\n将最终数值答案四舍五入到四位有效数字。将校准后的扩散方差参数以“性状值的平方每百万年”为单位表示。仅提供最终数值作为答案。", "solution": "该问题被确定为有效，因为它是一个在定量演化生物学领域中适定的、有科学依据的问题。它提供了推导唯一且有意义的解所需的所有信息和参数。我们将进行正式推导。\n\n性状 $X_t$ 的演化由 Ornstein–Uhlenbeck (OU) 随机微分方程 (SDE) 描述：\n$$\n\\mathrm{d}X_t = \\alpha(\\theta - X_t)\\mathrm{d}t + \\sigma\\mathrm{d}B_t\n$$\n初始条件为 $X_0 = \\theta$。参数 $\\alpha > 0$、$\\sigma > 0$ 和 $\\theta$ 是常数，$B_t$ 是一个标准布朗运动过程。\n\n为了推导该过程的方差，在分析上，将过程围绕其长期均值 $\\theta$ 进行中心化会很方便。我们定义一个新过程 $Y_t = X_t - \\theta$。该过程的初始条件为 $Y_0 = X_0 - \\theta = \\theta - \\theta = 0$。$Y_t$ 的微分为 $\\mathrm{d}Y_t = \\mathrm{d}X_t$。将 $X_t = Y_t + \\theta$ 代入 OU SDE，得到 $Y_t$ 的 SDE：\n$$\n\\mathrm{d}Y_t = \\alpha(\\theta - (Y_t + \\theta))\\mathrm{d}t + \\sigma\\mathrm{d}B_t\n$$\n$$\n\\mathrm{d}Y_t = -\\alpha Y_t \\mathrm{d}t + \\sigma\\mathrm{d}B_t\n$$\n这是一个齐次线性 SDE。我们可以使用积分因子来求解它。我们定义一个新过程 $Z_t = e^{\\alpha t} Y_t$。对函数 $f(t, y) = e^{\\alpha t}y$ 使用 Itô 引理，微分 $\\mathrm{d}Z_t$ 为：\n$$\n\\mathrm{d}Z_t = \\frac{\\partial f}{\\partial t}\\mathrm{d}t + \\frac{\\partial f}{\\partial y}\\mathrm{d}Y_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial y^2}(\\mathrm{d}Y_t)^2\n$$\n偏导数为 $\\frac{\\partial f}{\\partial t} = \\alpha e^{\\alpha t}Y_t$、$\\frac{\\partial f}{\\partial y} = e^{\\alpha t}$ 和 $\\frac{\\partial^2 f}{\\partial y^2} = 0$。将这些代入 $\\mathrm{d}Z_t$ 的公式中：\n$$\n\\mathrm{d}Z_t = (\\alpha e^{\\alpha t}Y_t)\\mathrm{d}t + e^{\\alpha t}\\mathrm{d}Y_t\n$$\n现在，我们代入 $\\mathrm{d}Y_t$ 的表达式：\n$$\n\\mathrm{d}Z_t = \\alpha e^{\\alpha t}Y_t\\mathrm{d}t + e^{\\alpha t}(-\\alpha Y_t \\mathrm{d}t + \\sigma\\mathrm{d}B_t)\n$$\n$$\n\\mathrm{d}Z_t = \\alpha e^{\\alpha t}Y_t\\mathrm{d}t - \\alpha e^{\\alpha t}Y_t\\mathrm{d}t + \\sigma e^{\\alpha t}\\mathrm{d}B_t\n$$\n$$\n\\mathrm{d}(e^{\\alpha t}Y_t) = \\sigma e^{\\alpha t}\\mathrm{d}B_t\n$$\n将此式从 $s=0$ 积分到 $s=t$：\n$$\n\\int_0^t \\mathrm{d}(e^{\\alpha s}Y_s) = \\int_0^t \\sigma e^{\\alpha s}\\mathrm{d}B_s\n$$\n$$\ne^{\\alpha t}Y_t - e^{\\alpha \\cdot 0}Y_0 = \\sigma \\int_0^t e^{\\alpha s}\\mathrm{d}B_s\n$$\n在初始条件 $Y_0=0$ 下，方程简化为 $e^{\\alpha t}Y_t = \\sigma \\int_0^t e^{\\alpha s}\\mathrm{d}B_s$。那么 $Y_t$ 的显式解为：\n$$\nY_t = e^{-\\alpha t} \\sigma \\int_0^t e^{\\alpha s}\\mathrm{d}B_s = \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s\n$$\n原始过程 $X_t$ 的方差等于中心化过程 $Y_t$ 的方差，因为 $\\theta$ 是一个常数：\n$$\nV(t) = \\mathrm{Var}(X_t) = \\mathrm{Var}(\\theta + Y_t) = \\mathrm{Var}(Y_t)\n$$\n方差由 $\\mathrm{Var}(Y_t) = \\mathrm{E}[Y_t^2] - (\\mathrm{E}[Y_t])^2$ 给出。具有确定性被积函数的 Itô 积分的期望为零：\n$$\n\\mathrm{E}[Y_t] = \\mathrm{E}\\left[ \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s \\right] = \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{E}[\\mathrm{d}B_s] = 0\n$$\n因此，方差等于二阶矩，即 $\\mathrm{Var}(Y_t) = \\mathrm{E}[Y_t^2]$。我们使用 Itô 等距定理来计算它，该定理指出，对于一个确定性被积函数 $g(s)$，$\\mathrm{E}\\left[\\left(\\int_0^t g(s)\\mathrm{d}B_s\\right)^2\\right] = \\int_0^t g(s)^2 \\mathrm{d}s$。\n$$\n\\mathrm{E}[Y_t^2] = \\mathrm{E}\\left[ \\left( \\sigma \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s \\right)^2 \\right] = \\sigma^2 \\mathrm{E}\\left[ \\left( \\int_0^t e^{-\\alpha(t-s)}\\mathrm{d}B_s \\right)^2 \\right]\n$$\n$$\n\\mathrm{E}[Y_t^2] = \\sigma^2 \\int_0^t \\left(e^{-\\alpha(t-s)}\\right)^2 \\mathrm{d}s = \\sigma^2 \\int_0^t e^{-2\\alpha(t-s)} \\mathrm{d}s\n$$\n计算该积分：\n$$\n\\int_0^t e^{-2\\alpha(t-s)} \\mathrm{d}s = \\left[ \\frac{e^{-2\\alpha(t-s)}}{2\\alpha} \\right]_{s=0}^{s=t} = \\frac{e^{-2\\alpha(t-t)}}{2\\alpha} - \\frac{e^{-2\\alpha(t-0)}}{2\\alpha} = \\frac{1 - e^{-2\\alpha t}}{2\\alpha}\n$$\n因此，性状在时间 $t$ 的方差为：\n$$\nV(t) = \\mathrm{Var}(X_t) = \\frac{\\sigma^2}{2\\alpha} (1 - e^{-2\\alpha t})\n$$\n这就是推导出的树尖方差与模型参数之间的关系。问题要求校准 $\\sigma^2$，使得在树高 $T$ 处的边际方差（记为 $V_T$）等于目标值。我们令 $t=T$ 并求解 $\\sigma^2$：\n$$\nV_T = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T))\n$$\n$$\n\\sigma^2 = \\frac{2\\alpha V_T}{1 - \\exp(-2\\alpha T)}\n$$\n这就是校准后的扩散方差参数的解析表达式。我们代入给定的数值：$T = 12$，$\\alpha = 0.8$，和 $V_T = 3.5$。\n$$\n\\sigma^2 = \\frac{2(0.8)(3.5)}{1 - \\exp(-2(0.8)(12))} = \\frac{1.6 \\times 3.5}{1 - \\exp(-19.2)} = \\frac{5.6}{1 - \\exp(-19.2)}\n$$\n指数项 $\\exp(-19.2)$ 是一个非常小的正数，约等于 $4.605 \\times 10^{-9}$。$\\sigma^2$ 的数值为：\n$$\n\\sigma^2 \\approx \\frac{5.6}{1 - 4.605 \\times 10^{-9}} \\approx \\frac{5.6}{0.999999995395} \\approx 5.6000000257\n$$\n将此结果四舍五入到四位有效数字，得到 $5.600$。", "answer": "$$\n\\boxed{5.600}\n$$", "id": "2592891"}, {"introduction": "比较方法学中的一项关键任务不仅是拟合模型，还要判断它是否是数据的最佳模型。本练习将介绍修正的赤池信息准则（AICc），这是一个在模型拟合优度和复杂性之间进行权衡的强大工具。通过将OU模型与更简单的布朗运动（BM）模型进行比较，您将练习一项检验适应性演化假说的关键技能。[@problem_id:2592906]", "problem": "一个比较数据集记录了$n$个近缘植物物种的一个连续功能性状（例如，比叶面积），这些物种跨越一个单一的分支，该性状在已知的超度量系统发育树的末端进行测量。在这个系统发育树上，通过最大似然法拟合了两种性状演化的随机过程模型：布朗运动（BM）模型和Ornstein-Uhlenbeck（OU）模型。对于具有自由根均值和扩散率的BM模型拟合，最大对数似然为$\\ell_{\\mathrm{BM}}=-42.3$，参数数量为$k_{\\mathrm{BM}}=2$。对于具有自由选择强度、平稳均值、扩散率和根状态的OU模型拟合，最大对数似然为$\\ell_{\\mathrm{OU}}=-39.5$，参数数量为$k_{\\mathrm{OU}}=4$。末端（tips）数量为$n=18$。使用校正后的赤池信息准则（AICc），根据预期信息损失原理，计算并比较这两个模型在相同数据上的拟合优度，从模型复杂度和基于似然的拟合度的标准定义出发。具体来说，根据给定的$n$、参数数量和最大化对数似然，确定每个模型的AICc值，然后计算AICc差值$\\Delta = \\mathrm{AICc}_{\\mathrm{OU}} - \\mathrm{AICc}_{\\mathrm{BM}}$。根据$\\Delta$的符号和大小，判断哪个模型更受支持，并在推理中解释当$n$不大且不同模型的$k$不同时，有限样本惩罚项如何影响比较结果。最终答案仅报告$\\Delta$的数值，四舍五入到四位有效数字。最终答案不带单位。", "solution": "所述问题具有科学依据，提法恰当，客观且自成一体。它代表了演化生物学领域中统计模型选择的一项标准任务。所有必要的数据都已提供，没有矛盾或含糊之处。因此，我将开始解答。\n\n该问题要求使用校正后的赤池信息准则（$\\mathrm{AICc}$）比较两种演化模型：布朗运动（BM）和Ornstein-Uhlenbeck（OU）。目标是确定哪个模型得到数据的更好支持，同时考虑拟合优度和模型复杂度，尤其是在小样本量的情况下。\n\n校正后的赤池信息准则的公式为：\n$$ \\mathrm{AICc} = -2\\ell + 2k + \\frac{2k(k+1)}{n-k-1} $$\n其中$\\ell$是模型的最大对数似然， $k$是估计的参数数量， $n$是样本量。对于这个系统发育问题，样本量$n$对应于末端的数量，即$n=18$。\n\n首先，我们计算布朗运动（BM）模型的$\\mathrm{AICc}$。给定参数为$\\ell_{\\mathrm{BM}} = -42.3$和$k_{\\mathrm{BM}} = 2$。\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = -2(\\ell_{\\mathrm{BM}}) + 2k_{\\mathrm{BM}} + \\frac{2k_{\\mathrm{BM}}(k_{\\mathrm{BM}}+1)}{n-k_{\\mathrm{BM}}-1} $$\n代入数值：\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = -2(-42.3) + 2(2) + \\frac{2(2)(2+1)}{18-2-1} $$\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = 84.6 + 4 + \\frac{12}{15} $$\n$$ \\mathrm{AICc}_{\\mathrm{BM}} = 88.6 + 0.8 = 89.4 $$\n\n接下来，我们计算Ornstein-Uhlenbeck（OU）模型的$\\mathrm{AICc}$。给定参数为$\\ell_{\\mathrm{OU}} = -39.5$和$k_{\\mathrm{OU}} = 4$。\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = -2(\\ell_{\\mathrm{OU}}) + 2k_{\\mathrm{OU}} + \\frac{2k_{\\mathrm{OU}}(k_{\\mathrm{OU}}+1)}{n-k_{\\mathrm{OU}}-1} $$\n代入数值：\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = -2(-39.5) + 2(4) + \\frac{2(4)(4+1)}{18-4-1} $$\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = 79.0 + 8 + \\frac{40}{13} $$\n$$ \\mathrm{AICc}_{\\mathrm{OU}} = 87.0 + \\frac{40}{13} $$\n为了合并这些项，我们可以将分数表示为小数：$\\frac{40}{13} \\approx 3.076923...$\n$$ \\mathrm{AICc}_{\\mathrm{OU}} \\approx 87.0 + 3.076923 = 90.076923... $$\n\n问题要求计算差值 $\\Delta = \\mathrm{AICc}_{\\mathrm{OU}} - \\mathrm{AICc}_{\\mathrm{BM}}$。\n$$ \\Delta \\approx 90.076923 - 89.4 $$\n$$ \\Delta \\approx 0.676923... $$\n\n问题要求对结果进行解释，特别是有限样本惩罚项的影响。OU模型对数据的拟合更好，其更高的最大对数似然值证明了这一点（$\\ell_{\\mathrm{OU}} = -39.5 > \\ell_{\\mathrm{BM}} = -42.3$）。然而，这种改进的拟合度是以增加复杂性为代价的，因为OU模型多了两个参数（$k_{\\mathrm{OU}} = 4$ 对比 $k_{\\mathrm{BM}} = 2$）。$\\mathrm{AICc}$会对这种复杂性进行惩罚。惩罚项$\\frac{2k(k+1)}{n-k-1}$对参数数量$k$和较小的样本量$n=18$都很敏感。对于BM模型，惩罚项为$\\frac{12}{15} = 0.8$。对于OU模型，惩罚项要大得多：$\\frac{40}{13} \\approx 3.077$。OU模型受到的显著惩罚是其较高参数数量和小样本量的直接后果，这超过了其更好的拟合优度。这导致OU模型的$\\mathrm{AICc}$值高于BM模型。\n\n差值$\\Delta$为正，这表明BM模型的$\\mathrm{AICc}$分数较低，因此是更优选的模型。$\\mathrm{AICc}$较低的模型在近似现实时估计会损失更少的信息。然而，差值的大小$\\Delta \\approx 0.6769$很小（通常认为$\\Delta  2$时差异不显著）。这表明虽然BM模型在形式上更受青睐，但OU模型仍然有相当大的支持度，不能被排除为一个合理的备选方案。小样本量惩罚项在扭转结论方面起到了关键作用，如果仅根据原始似然值或标准AIC，OU模型会显得更优。\n\n将$\\Delta$的最终数值四舍五入到四位有效数字，得到$0.6769$。", "answer": "$$\\boxed{0.6769}$$", "id": "2592906"}, {"introduction": "现代系统发育比较方法在很大程度上依赖于贝叶斯推断，而这通常通过马尔可夫链蒙特卡洛（MCMC）算法实现。这项高级练习将指导您编写Metropolis-Hastings算法的一个更新步骤，这是MCMC的核心组成部分。通过这项实践，您将揭开贝叶斯软件“黑箱”的神秘面纱，并为开发自定义的演化模型打下基础。[@problem_id:2592918]", "problem": "你将使用对数尺度上的随机游走提议，为 Ornstein–Uhlenbeck (OU) 选择强度参数 $ \\alpha $ 实现一次 Metropolis–Hastings 更新，并计算几种参数设置下相应的接受概率。其背景是一个沿着固定系统发育树演化的单变量连续性状。\n\n基本原理与假设：\n- 性状 $ X_t $ 服从一个 Ornstein–Uhlenbeck (OU) 随机微分方程 (SDE)：$ dX_t = -\\alpha (X_t - \\theta)\\,dt + \\sigma\\,dB_t $，其中 $ \\alpha  0 $ 是选择强度，$ \\theta \\in \\mathbb{R} $ 是最适值，$ \\sigma  0 $ 是扩散尺度，$ B_t $ 是标准布朗运动。\n- 假设 OU 过程的根是平稳的，因此叶尖处的性状均值为 $ \\theta $，且叶尖性状向量是多元正态分布，其均值向量为 $ \\mu = \\theta \\mathbf{1}_n $，协方差矩阵为 $ V(\\alpha) $，其元素为\n$$\nV_{ij}(\\alpha) = \\frac{\\sigma^2}{2\\alpha}\\,\\exp\\!\\left(-\\alpha\\,d_{ij}\\right),\n$$\n其中 $ d_{ij} $ 是树上叶尖 $ i $ 和叶尖 $ j $ 之间的谱系距离，$ n $ 是叶尖的数量。\n- $ \\alpha $ 的先验是形状参数为 $ k $、速率参数为 $ r $ 的伽马分布，其密度为 $ p(\\alpha) \\propto \\alpha^{k-1} \\exp(-r \\alpha) $，对于 $ \\alpha  0 $。\n- Metropolis–Hastings (MH) 提议在对数尺度上进行：如果当前值为 $ \\alpha $，则提议 $ \\ell' \\sim \\mathcal{N}(\\log \\alpha, s^2) $ 并设置 $ \\alpha' = \\exp(\\ell') $。接受概率由应用于 $ \\alpha $ 的后验以及正向和反向提议密度的 Metropolis–Hastings 法则定义。\n\n本问题的数据和常数：\n- 叶尖数量 $ n = 4 $。\n- 性状最适值 $ \\theta = 0.5 $。\n- 扩散尺度平方 $ \\sigma^2 = 0.8 $。\n- 伽马先验参数：形状参数 $ k = 2.5 $，速率参数 $ r = 1.0 $。\n- 由超度量树 $ ((A:1,B:1):1,(C:1,D:1):1) $ 导出的谱系距离矩阵 $ D = (d_{ij}) $，即\n$$\nD = \\begin{bmatrix}\n0  2  4  4 \\\\\n2  0  4  4 \\\\\n4  4  0  2 \\\\\n4  4  2  0 \\\\\n\\end{bmatrix}.\n$$\n- 观测到的叶尖性状向量 $ x = [0.30,\\, 0.60,\\, 0.45,\\, 0.70] $。\n\n你的任务：\n- 对于每个测试用例，在给定当前 $ \\alpha $ 的情况下，计算提议的 $ \\alpha' $ 的 Metropolis–Hastings 接受概率。计算时使用参数为 $ \\theta $ 和 $ \\sigma^2 $ 的 OU 多元正态似然、$ \\alpha $ 的伽马先验，以及上述在对数尺度上的提议。使用均值为 $ \\mu = \\theta \\mathbf{1}_n $、协方差为 $ V(\\alpha) $（如上定义）的精确多元正态似然，并通过数值稳定的方法实现。\n- 接受概率应以 $ [0,1] $ 范围内的浮点数形式返回。\n\n测试套件：\n对于每个元组 $ (\\alpha, \\Delta, s) $，将 $ \\Delta $ 解释为对数尺度上的加性扰动，即 $ \\ell' = \\log \\alpha + \\Delta $ 且 $ \\alpha' = \\exp(\\ell') $。对数尺度上的提议标准差为 $ s $。\n\n- 测试 $ 1 $: $ (\\alpha, \\Delta, s) = (0.8,\\, 0.2,\\, 0.3) $。\n- 测试 $ 2 $: $ (\\alpha, \\Delta, s) = (0.8,\\, -0.2,\\, 0.3) $。\n- 测试 $ 3 $: $ (\\alpha, \\Delta, s) = (0.2,\\, -1.0,\\, 0.5) $。\n- 测试 $ 4 $: $ (\\alpha, \\Delta, s) = (2.0,\\, 0.0,\\, 0.7) $。\n\n实现要求：\n- 将 $ \\theta $、$ \\sigma^2 $、$ k $、$ r $、$ D $ 和 $ x $ 视为上面给出的固定常数。\n- 对于似然，使用 $ V(\\alpha) $ 和 $ \\mu = \\theta \\mathbf{1}_n $ 的精确多元正态密度，通过数值稳定的分解（例如，Cholesky 分解）计算以获得对数行列式和二次型。\n- 对于先验，使用上面给出的伽马分布的形状-速率参数化。\n- 对于提议密度 $ q(\\alpha' \\mid \\alpha) $ 和 $ q(\\alpha \\mid \\alpha') $，使用由标准差为 $ s $ 的 $ \\log \\alpha $ 上的高斯随机游走所蕴含的相应对数正态密度。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含四个测试的接受概率，格式为逗号分隔的浮点数列表，四舍五入到 $ 6 $ 位小数，并用方括号括起来。例如，$ [0.123456,0.654321,0.500000,1.000000] $。\n\n不涉及物理单位。不涉及角度。不涉及百分比。所有答案必须是指定的浮点数。", "solution": "我们从 Ornstein–Uhlenbeck (OU) 过程对沿谱系演化的连续性状 $ X_t $ 的基本描述开始，该过程由随机微分方程 $ dX_t = -\\alpha (X_t - \\theta)\\,dt + \\sigma\\,dB_t $ 控制，其中 $ \\alpha  0 $，$ \\theta \\in \\mathbb{R} $，$ \\sigma  0 $。在系统发育树上，对于 OU 过程采用平稳根假设，叶尖处的性状值向量 $ x \\in \\mathbb{R}^n $ 服从多元正态分布，其均值向量为 $ \\mu = \\theta \\mathbf{1}_n $，协方差矩阵为 $ V(\\alpha) $，其元素遵循经过充分检验的公式\n$$\nV_{ij}(\\alpha) = \\frac{\\sigma^2}{2\\alpha}\\,\\exp\\!\\left(-\\alpha d_{ij}\\right),\n$$\n其中 $ d_{ij} $ 是叶尖 $ i $ 和 $ j $ 之间的谱系距离。这个结果是树上平稳 OU 过程的一个标准属性：平稳方差为 $ \\sigma^2/(2\\alpha) $，并且由于以速率 $ \\alpha $ 进行均值回归，互协方差随共享的进化距离呈指数衰减。\n\n似然。在参数 $ \\alpha $、$ \\theta $ 和 $ \\sigma^2 $ 下，$ x $ 的似然是\n$$\nL(x \\mid \\alpha) = \\mathcal{N}\\big(x; \\mu(\\theta), V(\\alpha)\\big),\n$$\n其对数似然为\n$$\n\\log L(x \\mid \\alpha) = -\\frac{1}{2} \\left( n \\log(2\\pi) + \\log \\det V(\\alpha) + (x - \\mu)^\\top V(\\alpha)^{-1} (x - \\mu) \\right).\n$$\n为保证数值稳定性，我们使用 Cholesky 分解 $ V(\\alpha) = L L^\\top $（其中 $ L $ 是下三角矩阵）来计算 $ \\log \\det V(\\alpha) $ 和二次型。于是，$ \\log \\det V(\\alpha) = 2 \\sum_{i=1}^n \\log L_{ii} $，而 $ (x - \\mu)^\\top V^{-1} (x - \\mu) $ 则通过求解 $ L y = (x - \\mu) $ 然后求解 $ L^\\top z = y $ 来计算，其中二次型等于 $ (x - \\mu)^\\top z $。\n\n先验。我们对 $ \\alpha $ 采用形状-速率参数为 $ k $ 和 $ r $ 的伽马先验：\n$$\np(\\alpha) = \\frac{r^k}{\\Gamma(k)} \\alpha^{k-1} e^{-r\\alpha}, \\quad \\alpha  0.\n$$\n对数先验是 $ \\log p(\\alpha) = k \\log r - \\log \\Gamma(k) + (k-1) \\log \\alpha - r \\alpha $。在 Metropolis–Hastings 对数接受差值中，加性常数 $ k \\log r - \\log \\Gamma(k) $ 在当前状态和提议状态之间会抵消，因此在计算中可以省略。\n\n提议。我们在 $ \\ell = \\log \\alpha $ 中实现一个随机游走。给定当前的 $ \\alpha $，我们抽取一个提议值 $ \\ell' \\sim \\mathcal{N}(\\log \\alpha, s^2) $ 并设置 $ \\alpha' = \\exp(\\ell') $。在 $ \\alpha $ 尺度上对应的提议密度是对数正态分布的：\n$$\nq(\\alpha' \\mid \\alpha) = \\operatorname{LogNormal}\\!\\left(\\alpha'; \\mu = \\log \\alpha, \\sigma = s\\right), \\quad\nq(\\alpha \\mid \\alpha') = \\operatorname{LogNormal}\\!\\left(\\alpha; \\mu = \\log \\alpha', \\sigma = s\\right).\n$$\nMetropolis–Hastings 接受概率为\n$$\n\\mathrm{acc} = \\min\\left( 1, \\frac{L(x \\mid \\alpha') \\, p(\\alpha') \\, q(\\alpha \\mid \\alpha')}{L(x \\mid \\alpha) \\, p(\\alpha) \\, q(\\alpha' \\mid \\alpha)} \\right).\n$$\n在对数尺度上计算是数值稳定的：\n$$\n\\log R = \\left[ \\log L(x \\mid \\alpha') - \\log L(x \\mid \\alpha) \\right] + \\left[ \\log p(\\alpha') - \\log p(\\alpha) \\right] + \\left[ \\log q(\\alpha \\mid \\alpha') - \\log q(\\alpha' \\mid \\alpha) \\right],\n$$\n然后计算 $ \\mathrm{acc} = \\min\\left(1, \\exp(\\log R)\\right) $。\n\nHastings 校正简化。对于具有相同 $ s $ 的对数正态提议，$ \\ell $ 中的高斯对称性意味着对数提议密度的差值简化为：\n$$\n\\log q(\\alpha \\mid \\alpha') - \\log q(\\alpha' \\mid \\alpha) = \\log \\alpha' - \\log \\alpha.\n$$\n这可以通过写出对数正态密度\n$$\n\\log q(x \\mid m, s) = -\\log x - \\frac{1}{2}\\log(2\\pi) - \\log s - \\frac{(\\log x - m)^2}{2 s^2}\n$$\n（其中 $ m = \\log \\alpha $ 或 $ m = \\log \\alpha' $）并观察到二次项抵消来验证。因此，Hastings 校正对 $ \\log R $ 的贡献恰好为 $ \\log(\\alpha'/\\alpha) $。等价地，在 $ \\ell $ 中使用对称提议的接受率，与评估包含雅可比因子 $ \\alpha $ 的 $ \\ell $ 的后验密度是吻合的。\n\n每个测试用例 $ (\\alpha, \\Delta, s) $ 的算法步骤：\n1. 构造 $ \\ell = \\log \\alpha $ 和 $ \\ell' = \\ell + \\Delta $，然后 $ \\alpha' = \\exp(\\ell') $。\n2. 使用 OU 多元正态分布，其中 $ \\mu = \\theta \\mathbf{1}_n $ 和 $ V_{ij}(\\alpha) = \\frac{\\sigma^2}{2\\alpha} e^{-\\alpha d_{ij}} $，通过 Cholesky 分解计算 $ \\log L(x \\mid \\alpha) $ 和 $ \\log L(x \\mid \\alpha') $。\n3. 使用伽马形状-速率先验，计算 $ \\log p(\\alpha) $ 和 $ \\log p(\\alpha') $（忽略加性常数）：$ \\log p(\\alpha) = (k-1)\\log \\alpha - r \\alpha $。\n4. 直接以 $ \\log \\alpha' - \\log \\alpha $ 的形式计算 Hastings 校正，或通过评估对数正态提议密度并相减来计算。\n5. 将三个差值相加得到 $ \\log R $，然后计算 $ \\mathrm{acc} = \\min(1, \\exp(\\log R)) $。\n6. 将 $ \\mathrm{acc} $ 四舍五入到 $ 6 $ 位小数。\n\n数值细节：\n- 我们使用给定的常数 $ n = 4 $、$ \\theta = 0.5 $、$ \\sigma^2 = 0.8 $、$ k = 2.5 $、$ r = 1.0 $、如上指定的距离矩阵 $ D $，以及观测到的 $ x = [0.30, 0.60, 0.45, 0.70] $。\n- Cholesky 分解提供了对多元正态对数似然的稳定评估，避免了显式矩阵求逆。\n- 对于四个测试，当 $ \\Delta = 0.0 $ 时，有 $ \\alpha' = \\alpha $，因此根据构造 $ \\log R = 0 $ 且接受概率等于 $ 1.0 $；其他情况反映了通过 MH 法则在似然拟合和先验正则化之间的平衡。\n\n最终的程序实现了这些步骤，并按要求打印一行包含四个接受概率的字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cholesky_logpdf(x, mean, cov):\n    \"\"\"\n    Compute the multivariate normal log-density log N(x; mean, cov)\n    using Cholesky factorization for numerical stability.\n    \"\"\"\n    y = x - mean\n    # Cholesky decomposition: cov = L @ L.T, with L lower triangular\n    L = np.linalg.cholesky(cov)\n    # Solve L * v = y\n    v = np.linalg.solve(L, y)\n    # Solve L.T * w = v\n    w = np.linalg.solve(L.T, v)\n    # Log determinant\n    logdet = 2.0 * np.sum(np.log(np.diag(L)))\n    n = x.shape[0]\n    quad = float(y @ w)\n    log2pi = n * np.log(2.0 * np.pi)\n    return -0.5 * (log2pi + logdet + quad)\n\ndef ou_covariance(alpha, sigma2, D):\n    \"\"\"\n    Construct the OU covariance matrix V(alpha) with stationary root:\n    V_ij = (sigma2 / (2*alpha)) * exp(-alpha * d_ij)\n    \"\"\"\n    factor = sigma2 / (2.0 * alpha)\n    return factor * np.exp(-alpha * D)\n\ndef log_prior_gamma_shape_rate(alpha, k, r):\n    \"\"\"\n    Gamma prior with shape k and rate r.\n    Return log prior up to additive constant: (k-1) * log(alpha) - r * alpha\n    The additive constant cancels in MH ratio.\n    \"\"\"\n    return (k - 1.0) * np.log(alpha) - r * alpha\n\ndef log_q_lognormal(x, mean_log, sd_log):\n    \"\"\"\n    Log-density of LogNormal(x | mean_log, sd_log) derived from Normal(log x; mean_log, sd_log^2).\n    \"\"\"\n    if x == 0.0:\n        return -np.inf\n    z = (np.log(x) - mean_log) / sd_log\n    return -np.log(x) - np.log(sd_log) - 0.5 * np.log(2.0 * np.pi) - 0.5 * z * z\n\ndef mh_acceptance_probability(alpha_curr, delta_log, s, x, theta, sigma2, D, k, r):\n    \"\"\"\n    Compute the MH acceptance probability for a log-scale random-walk proposal:\n    ell' = log(alpha_curr) + delta_log, alpha' = exp(ell')\n    \"\"\"\n    ell_curr = np.log(alpha_curr)\n    ell_prop = ell_curr + delta_log\n    alpha_prop = np.exp(ell_prop)\n\n    # Means\n    n = x.shape[0]\n    mean = np.full(n, theta)\n\n    # Likelihoods\n    V_curr = ou_covariance(alpha_curr, sigma2, D)\n    V_prop = ou_covariance(alpha_prop, sigma2, D)\n    loglik_curr = cholesky_logpdf(x, mean, V_curr)\n    loglik_prop = cholesky_logpdf(x, mean, V_prop)\n\n    # Priors (up to constant)\n    logprior_curr = log_prior_gamma_shape_rate(alpha_curr, k, r)\n    logprior_prop = log_prior_gamma_shape_rate(alpha_prop, k, r)\n\n    # Proposal densities (lognormal) for Hastings ratio\n    logq_forward = log_q_lognormal(alpha_prop, ell_curr, s)   # q(alpha' | alpha)\n    logq_reverse = log_q_lognormal(alpha_curr, ell_prop, s)   # q(alpha | alpha')\n\n    logR = (loglik_prop - loglik_curr) + (logprior_prop - logprior_curr) + (logq_reverse - logq_forward)\n    acc = 1.0 if logR = 0.0 else np.exp(logR)\n    # Clamp to [0,1] for numerical safety\n    acc = max(0.0, min(1.0, acc))\n    return acc\n\ndef solve():\n    # Constants from the problem statement\n    theta = 0.5\n    sigma2 = 0.8\n    k = 2.5\n    r = 1.0\n\n    # Distance matrix D for tree: ((A:1,B:1):1,(C:1,D:1):1)\n    D = np.array([\n        [0.0, 2.0, 4.0, 4.0],\n        [2.0, 0.0, 4.0, 4.0],\n        [4.0, 4.0, 0.0, 2.0],\n        [4.0, 4.0, 2.0, 0.0],\n    ], dtype=float)\n\n    # Observed trait vector x\n    x = np.array([0.30, 0.60, 0.45, 0.70], dtype=float)\n\n    # Test cases: (alpha, Delta, s)\n    test_cases = [\n        (0.8, 0.2, 0.3),\n        (0.8, -0.2, 0.3),\n        (0.2, -1.0, 0.5),\n        (2.0, 0.0, 0.7),\n    ]\n\n    results = []\n    for alpha, delta, s in test_cases:\n        acc = mh_acceptance_probability(alpha, delta, s, x, theta, sigma2, D, k, r)\n        results.append(f\"{acc:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2592918"}]}