{"hands_on_practices": [{"introduction": "在开始任何形态学分析之前，一个至关重要的步骤是评估我们数据的质量。我们观察到的变异有多少是真实的生物学差异，又有多少仅仅是测量过程中的误差？本练习将指导你使用普氏方差分析（Procrustes ANOVA）来分解变异的来源，并计算重复性（repeatability），这是一个衡量测量可靠性的关键指标。通过这个实践 [@problem_id:2577647]，你将学会如何判断你的数据是否足够精确，足以支撑后续的生物学解释。", "problem": "在一项研究中，为了量化几何形态测量学的测量误差，您研究了单一物种的叶片形状，收集了 $n$ 个数字化样本，并由同一操作员对每个样本获取了 $k$ 个独立的重复标志点构型。经过广义普氏分析（Generalized Procrustes Analysis, GPA）后，您拟合了一个单因素普氏方差分析（Procrustes Analysis of Variance, ANOVA）模型，其中包含随机效应“个体”，以及代表数字化误差的个体内残差。采用标准的形状坐标单因素随机效应模型，\n$Y_{ij}=\\mu + A_i + E_{ij}$，\n其中 $A_i$ 和 $E_{ij}$ 独立，$A_i \\sim \\mathcal{N}(0,\\sigma^2_{\\text{indiv}})$，并且 $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2_{\\text{error}})$。假设设计是平衡的，并且误差在所有形状坐标上是各向同性的，因此通常的随机效应均方期望适用。\n\n针对个体项和误差项的普氏方差分析（Procrustes ANOVA）摘要（对所有形状坐标求和）如下：\n- 个体平方和 $SS_{\\text{indiv}}=0.120$，自由度 $df_{\\text{indiv}}=n-1=14$，\n- 误差平方和 $SS_{\\text{error}}=0.090$，自由度 $df_{\\text{error}}=n(k-1)=30$，\n其中 $n=15$ 且 $k=3$。\n\n使用单因素随机效应模型和普氏方差分析（Procrustes ANOVA）的基本原理，选择一个选项，该选项正确地说明了如何从方差分析中估计方差分量 $\\sigma^2_{\\text{indiv}}$ 和 $\\sigma^2_{\\text{error}}$，并给出了形状测量的可重复性（组内相关系数）的相应数值估计。可重复性定义为可归因于个体间差异的方差比例。\n\nA. 使用标准单因素随机效应矩量法的均方进行估计：$\\hat{\\sigma}^2_{\\text{indiv}}=(MS_{\\text{indiv}}-MS_{\\text{error}})/k$，$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}$，因此 $\\hat{R}=\\hat{\\sigma}^2_{\\text{indiv}}/(\\hat{\\sigma}^2_{\\text{indiv}}+\\hat{\\sigma}^2_{\\text{error}})=13/34\\approx 0.382$。\n\nB. 直接将个体均方视为个体间方差，并将误差均方除以重复次数：$\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}$，$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}/k$，因此 $\\hat{R}\\approx 0.895$。\n\nC. 忽略重复次数，将均方差作为个体间方差：$\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}-MS_{\\text{error}}$，$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}$，因此 $\\hat{R}\\approx 0.650$。\n\nD. 直接使用平方和而非均方，因为普氏方差分析（Procrustes ANOVA）对坐标进行了求和：$\\hat{\\sigma}^2_{\\text{indiv}}=(SS_{\\text{indiv}}-SS_{\\text{error}})/k$，$\\hat{\\sigma}^2_{\\text{error}}=SS_{\\text{error}}$，因此 $\\hat{R}=0.100$。", "solution": "我们从单因素随机效应模型 $Y_{ij}=\\mu + A_i + E_{ij}$ 开始，其中 $A_i \\sim \\mathcal{N}(0,\\sigma^2_{\\text{indiv}})$ 和 $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2_{\\text{error}})$ 相互独立。在一个每个个体有 $k$ 次重复的平衡设计中，单因素随机效应方差分析（ANOVA）中均方的经典期望为\n$$\\mathbb{E}[MS_{\\text{error}}]=\\sigma^2_{\\text{error}},\\quad \\mathbb{E}[MS_{\\text{indiv}}]=\\sigma^2_{\\text{error}}+k\\,\\sigma^2_{\\text{indiv}}.$$\n这些期望是根据独立性和相等重复次数下的方差分解得出的。在普氏方差分析（Procrustes ANOVA）的背景下，平方和是在广义普氏分析（GPA）之后对所有形状坐标进行汇总的。在各向同性误差的假设下，跨坐标的比例是恒定的，因此这些关系逐坐标适用，也同样适用于汇总的均方。因此，矩量法估计量为\n$$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}},\\quad \\hat{\\sigma}^2_{\\text{indiv}}=\\frac{MS_{\\text{indiv}}-MS_{\\text{error}}}{k}.$$\n\n根据给定的平方和和自由度计算均方：\n$$MS_{\\text{indiv}}=\\frac{SS_{\\text{indiv}}}{df_{\\text{indiv}}}=\\frac{0.120}{14}=\\frac{3}{350}\\approx 0.0085714,$$\n$$MS_{\\text{error}}=\\frac{SS_{\\text{error}}}{df_{\\text{error}}}=\\frac{0.090}{30}=\\frac{3}{1000}=0.003.$$\n\n然后\n$$\\hat{\\sigma}^2_{\\text{indiv}}=\\frac{MS_{\\text{indiv}}-MS_{\\text{error}}}{k}=\\frac{\\frac{3}{350}-\\frac{3}{1000}}{3}=\\frac{\\frac{60}{7000}-\\frac{21}{7000}}{3}=\\frac{\\frac{39}{7000}}{3}=\\frac{13}{7000}\\approx 0.0018571,$$\n$$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}=\\frac{3}{1000}=0.003.$$\n\n可重复性（组内相关系数）是可归因于个体间差异的总方差比例：\n$$\\hat{R}=\\frac{\\hat{\\sigma}^2_{\\text{indiv}}}{\\hat{\\sigma}^2_{\\text{indiv}}+\\hat{\\sigma}^2_{\\text{error}}}=\\frac{\\frac{13}{7000}}{\\frac{13}{7000}+\\frac{3}{1000}}=\\frac{\\frac{13}{7000}}{\\frac{34}{7000}}=\\frac{13}{34}\\approx 0.3823529.$$\n\n逐项分析：\n\n- 选项A：此选项使用了正确的矩量法估计量，这些估计量是在具有 $k$ 次重复的单因素随机效应模型中从 $\\mathbb{E}[MS_{\\text{error}}]$ 和 $\\mathbb{E}[MS_{\\text{indiv}}]$ 推导出来的。数值计算与推导相符：$MS_{\\text{indiv}}=\\frac{3}{350}$，$MS_{\\text{error}}=\\frac{3}{1000}$，得到 $\\hat{R}=\\frac{13}{34}\\approx 0.382$。结论 — 正确。\n\n- 选项B：此选项错误地设定 $\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}$ 并将误差均方除以 $k$。期望 $\\mathbb{E}[MS_{\\text{indiv}}]=\\sigma^2_{\\text{error}}+k\\,\\sigma^2_{\\text{indiv}}$ 表明 $MS_{\\text{indiv}}$ 被 $\\sigma^2_{\\text{error}}$ 夸大了，因此不能直接估计 $\\sigma^2_{\\text{indiv}}$。将 $MS_{\\text{error}}$ 除以 $k$ 也是不合理的，因为 $\\mathbb{E}[MS_{\\text{error}}]=\\sigma^2_{\\text{error}}$。因此，所得的 $\\hat{R}\\approx 0.895$ 是基于一个有缺陷的程序。结论 — 错误。\n\n- 选项C：此选项设定 $\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}-MS_{\\text{error}}$ 但省略了除以 $k$。从期望 $\\mathbb{E}[MS_{\\text{indiv}}]-\\mathbb{E}[MS_{\\text{error}}]=k\\,\\sigma^2_{\\text{indiv}}$ 可知，正确的估计量必须除以 $k$。因此，产生的 $\\hat{R}\\approx 0.650$ 高估了可重复性。结论 — 错误。\n\n- 选项D：此选项使用平方和而不是均方来估计方差分量。方差分量与均方的期望有关，而不是平方和，并且必须考虑自由度。使用平方和会产生维度不一致且有偏的估计；计算出的 $\\hat{R}=0.100$ 并未基于适当的基本原理。结论 — 错误。", "answer": "$$\\boxed{A}$$", "id": "2577647"}, {"introduction": "在确认数据质量可靠之后，下一步便是探索形态变异的主要模式。主成分分析（Principal Component Analysis, PCA）是几何形态测量学中用于降维和可视化形态空间的核心工具。本练习 [@problem_id:2577650] 将带你从零开始实现PCA的核心步骤，学习如何将高维的坐标点数据投影到低维的“形态空间”（morphospace）中。更重要的是，你将掌握如何将主成分轴上的变异“反向转换”回原始坐标空间，从而直观地展示沿各主成分轴发生的具体形状变化，让抽象的统计结果变得一目了然。", "problem": "给定一个二维的、经普氏叠加（Procrustes-aligned）对齐后的地标坐标矩阵，其中每个样本都已通过广义普氏分析（Generalized Procrustes Analysis, GPA）对齐，使得列均值为零。同时，还给出一个表示平均形状的独立向量。地标以坐标顺序 $\\left[x_1,y_1,x_2,y_2,\\dots,x_p,y_p\\right]$ 堆叠，因此共有 $2p$ 列。您的目标是重建几何形态计量学中主成分分析（Principal Component Analysis, PCA）的核心步骤，并实现原始地标空间与主成分（Principal Component, PC）形态空间之间的可逆变换。\n\n使用的基本原理：\n- 经验协方差矩阵的正交对角化。对于一个中心化的数据矩阵 $\\mathbf{Y}\\in\\mathbb{R}^{n\\times d}$（其中 $d=2p$），经验协方差为 $\\mathbf{S}=\\frac{1}{n-1}\\mathbf{Y}^\\top\\mathbf{Y}$。由于 $\\mathbf{S}$ 是对称半正定的，它允许进行特征分解 $\\mathbf{S}=\\mathbf{V}\\,\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)\\,\\mathbf{V}^\\top$，其中 $\\mathbf{v}_k$ 是标准正交列，$\\lambda_k$ 是非负特征值。\n- 在标准正交基中的坐标。对于任何中心化向量 $\\mathbf{y}\\in\\mathbb{R}^d$，其在特征基中的坐标是该向量与基向量的内积。\n\n需要实现的任务（根据这些原则推导，不要使用任何提供给您的简化公式）：\n1. 计算 $\\mathbf{S}$ 的特征分解，并将特征对排序，使得 $\\lambda_1\\ge \\lambda_2\\ge \\dots\\ge \\lambda_d\\ge 0$。对每个特征向量 $\\mathbf{v}_k$ 强制执行一个确定性的符号约定：找到绝对值最大的坐标索引 $j^\\star$，如果 $\\left(\\mathbf{v}_k\\right)_{j^\\star}0$，则将 $\\mathbf{v}_k$ 乘以 $-1$。\n2. 将 $\\mathbf{Y}$ 的每一行在标准正交特征基 $\\mathbf{V}$ 中表示，从而为所有样本构建PC分数。\n3. 实现沿单一PC轴 $k$ 到原始地标空间的反向变换：给定平均形状向量 $\\mathbf{m}\\in\\mathbb{R}^{d}$、轴索引 $k$ 以及一个以标准差为单位的纯量 $t$（即 $t$ 乘以 $\\sqrt{\\lambda_k}$），构造形变后的形状\n$$\n\\mathbf{x}(t)\\;=\\;\\mathbf{m}\\;+\\;t\\,\\sqrt{\\lambda_k}\\,\\mathbf{v}_k.\n$$\n这种形变应可解释为在PC形态空间中沿轴 $k$ 移动 $t$ 个标准差，然后反向变换回原始地标坐标。\n\n此问题不涉及角度单位。不涉及物理单位。所有输出均为无因次的实数。当指定舍入指令时，四舍五入到 $6$ 位小വു。\n\n测试套件与所需输出：\n采用列堆叠顺序 $\\left[x_1,y_1,x_2,y_2,\\dots\\right]$。\n\n- 测试案例 A（具有两种非零变异模式的理想路径）：\n  - 设 $n=4$, $p=3$，所以 $d=6$。\n  - 平均形状 $\\mathbf{m}_A=\\left[0,0,1,0,0,1\\right]$。\n  - 数据矩阵 $\\mathbf{Y}_A\\in\\mathbb{R}^{4\\times 6}$，其行为\n    - $\\left[0,0,-2,0,0,0\\right]$,\n    - $\\left[0,0,2,0,0,0\\right]$,\n    - $\\left[0,0,0,0,0,-1\\right]$,\n    - $\\left[0,0,0,0,0,1\\right]$。\n  - 使用第一个PC轴（按特征值降序排序后，$k=0$）。\n  - 对于 $t$ 值 $[0.0,\\,1.0,\\,-1.0,\\,2.5]$，反向变换到 $\\mathbf{x}(t)$ 并报告地标 $2$ 的 $x$ 坐标（即向量中索引为 $2$ 的坐标）。输出一个包含四个浮点数的列表，每个浮点数四舍五入到 $6$ 位小数。\n\n- 测试案例 B（对每个地标的欧几里得效應进行类边界检查）：\n  - 设 $n=2$, $p=2$，所以 $d=4$。\n  - 平均形状 $\\mathbf{m}_B=\\left[2,2,0,0\\right]$。\n  - 数据矩阵 $\\mathbf{Y}_B\\in\\mathbb{R}^{2\\times 4}$，其行为\n    - $\\left[\\frac{3}{\\sqrt{2}},\\frac{3}{\\sqrt{2}},0,0\\right]$,\n    - $\\left[-\\frac{3}{\\sqrt{2}},-\\frac{3}{\\sqrt{2}},0,0\\right]$。\n  - 使用第一个PC轴（$k=0$）。\n  - 计算 $\\mathbf{x}(+1)$ 和 $\\mathbf{x}(-1)$，并仅返回地标 $1$ 两个位置之间的欧几里得距离，即使用 $x_1,y_1$ 条目在 $\\mathbb{R}^2$ 中的范数。返回一个四舍五入到 $6$ 位小数的单一浮点数。\n\n- 测试案例 C（在次要轴上的正向和反向变换的一致性）：\n  - 重用测试案例 A 的 $\\left(\\mathbf{Y}_A,\\mathbf{m}_A\\right)$。\n  - 使用第二个PC轴（排序后 $k=1$）。\n  - 设置 $t=-1.5$。构造 $\\mathbf{x}(t)$ 并计算中心化向量 $\\mathbf{y}(t)=\\mathbf{x}(t)-\\mathbf{m}_A$。将 $\\mathbf{y}(t)$ 投影到轴 $k$上（与 $\\mathbf{v}_k$ 求内积），以获得该轴上的纯量PC分数。返回此纯量，四舍五入到 $6$ 位小数。\n\n最终输出格式：\n您的程序应产生单行输出，其中包含一个以逗号分隔的列表，用方括號括起来，顺序为 $\\left[\\text{A的结果},\\text{B的结果},\\text{C的结果}\\right]$。A的结果本身是一个包含四个浮点数的列表。例如，您的输出应类似于 $\\left[[a_1,a_2,a_3,a_4],b,c\\right]$，其中每个符号代表一个如上所述的数值。", "solution": "此問題提法明確，有科學依據，並包含唯一解所需的所有必要資訊。問題中描述的幾何形態計量學中的主成分分析（PCA）程序是標準的，且數學上是正確的。我們將按照指定的任務來解決這個問題。\n\n分析的核心是經驗協方差矩陣 $\\mathbf{S}$ 的特徵分解。給定一個中心化數據矩陣 $\\mathbf{Y} \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是樣本數量，$d=2p$ 是地標坐標的數量，協方差矩陣定義為：\n$$\n\\mathbf{S} = \\frac{1}{n-1}\\mathbf{Y}^\\top\\mathbf{Y}\n$$\n$\\mathbf{S}$ 的特徵值 $\\lambda_k$ 表示數據沿相應主成分（PC）軸的變異數，而特徵向量 $\\mathbf{v}_k$ 定義了這些軸在原始 $d$ 維地標空間中的方向。這些特徵向量構成一個標準正交基。具有中心化坐標 $\\mathbf{y}_i$ 的樣本的PC分數是其在這個新基底中的坐標，通過計算內積 $\\mathbf{y}_i \\cdot \\mathbf{v}_k$ 得到。\n\n可以透過從平均形狀 $\\mathbf{m}$ 沿著特定PC軸 $k$ 移動來重建形狀。沿軸 $k$ 移動 $t$ 個標準差的位移對應於將向量 $t\\sqrt{\\lambda_k}\\mathbf{v}_k$ 加到平均形狀向量上。沿軸 $k$ 的分數的標準差為 $\\sqrt{\\lambda_k}$。因此，重建的形狀 $\\mathbf{x}(t)$ 由下式給出：\n$$\n\\mathbf{x}(t) = \\mathbf{m} + t\\sqrt{\\lambda_k}\\mathbf{v}_k\n$$\n我們將把這些原則應用於每個測試案例。\n\n**測試案例 A**\n\n給定：一個包含 $n=4$ 個樣本的樣本集，每個樣本有 $p=3$ 個地標，因此維度為 $d=2 \\times 3=6$。\n平均形狀向量為 $\\mathbf{m}_A = [0, 0, 1, 0, 0, 1]^\\top$。\n中心化數據矩陣為：\n$$\n\\mathbf{Y}_A = \\begin{pmatrix} 0  0  -2  0  0  0 \\\\ 0  0  2  0  0  0 \\\\ 0  0  0  0  0  -1 \\\\ 0  0  0  0  0  1 \\end{pmatrix}\n$$\n首先，我們計算矩陣 $\\mathbf{Y}_A^\\top\\mathbf{Y}_A$：\n$$\n\\mathbf{Y}_A^\\top\\mathbf{Y}_A = \\begin{pmatrix} 0  0  -2  0  0  0 \\\\ 0  0  2  0  0  0 \\\\ 0  0  0  0  0  -1 \\\\ 0  0  0  0  0  1 \\end{pmatrix}^\\top \\begin{pmatrix} 0  0  -2  0  0  0 \\\\ 0  0  2  0  0  0 \\\\ 0  0  0  0  0  -1 \\\\ 0  0  0  0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  8  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  0  0  0  2 \\end{pmatrix}\n$$\n協方差矩陣 $\\mathbf{S}_A$ 是：\n$$\n\\mathbf{S}_A = \\frac{1}{4-1}\\mathbf{Y}_A^\\top\\mathbf{Y}_A = \\frac{1}{3}\\begin{pmatrix} 0  0  0  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  8  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  0  0  0  2 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  8/3  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  0  0  0  0  2/3 \\end{pmatrix}\n$$\n由於 $\\mathbf{S}_A$ 是一個對角矩陣，其特徵值是對角線上的元素，其特徵向量是標準基向量。按降序排序，非零特徵值及其對應的特徵向量為：\n$\\lambda_0 = 8/3$，其特徵向量為 $\\mathbf{v}_0 = [0, 0, 1, 0, 0, 0]^\\top$。\n$\\lambda_1 = 2/3$，其特徵向量為 $\\mathbf{v}_1 = [0, 0, 0, 0, 0, 1]^\\top$。\n其餘的特徵值均為 $0$。\n符號約定要求檢查最大絕對值的元素。對於 $\\mathbf{v}_0$ 和 $\\mathbf{v}_1$，這個元素都是 $1$（一個正值），所以它們的符號不變。\n\n我們使用第一個PC軸（$k=0$）。反向變換為：\n$$\n\\mathbf{x}(t) = \\mathbf{m}_A + t\\sqrt{\\lambda_0}\\,\\mathbf{v}_0 = [0, 0, 1, 0, 0, 1]^\\top + t\\sqrt{8/3}\\,[0, 0, 1, 0, 0, 0]^\\top = [0, 0, 1 + t\\sqrt{8/3}, 0, 0, 1]^\\top\n$$\n我們需要找到地標 $2$ 的 $x$ 坐標，即 $\\mathbf{x}(t)$ 中索引為 $2$ 的元素。該坐標為 $1 + t\\sqrt{8/3}$。\n對於 $t \\in [0.0, 1.0, -1.0, 2.5]$：\n- $t=0.0$: $1 + 0.0\\sqrt{8/3} = 1.0$\n- $t=1.0$: $1 + 1.0\\sqrt{8/3} \\approx 1 + 1.632993 = 2.632993$\n- $t=-1.0$: $1 - 1.0\\sqrt{8/3} \\approx 1 - 1.632993 = -0.632993$\n- $t=2.5$: $1 + 2.5\\sqrt{8/3} \\approx 1 + 4.082483 = 5.082483$\n結果四捨五入到 $6$ 位小數後為 $[1.000000, 2.632993, -0.632993, 5.082483]$。\n\n**測試案例 B**\n\n給定：$n=2$ 個樣本，$p=2$ 個地標，$d=4$。\n平均形狀：$\\mathbf{m}_B = [2, 2, 0, 0]^\\top$。\n數據矩陣：\n$$\n\\mathbf{Y}_B = \\begin{pmatrix} 3/\\sqrt{2}  3/\\sqrt{2}  0  0 \\\\ -3/\\sqrt{2}  -3/\\sqrt{2}  0  0 \\end{pmatrix}\n$$\n協方差矩陣為 $\\mathbf{S}_B = \\frac{1}{2-1}\\mathbf{Y}_B^\\top\\mathbf{Y}_B = \\mathbf{Y}_B^\\top\\mathbf{Y}_B$：\n$$\n\\mathbf{S}_B = \\begin{pmatrix} 3/\\sqrt{2}  -3/\\sqrt{2} \\\\ 3/\\sqrt{2}  -3/\\sqrt{2} \\\\ 0  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 3/\\sqrt{2}  3/\\sqrt{2}  0  0 \\\\ -3/\\sqrt{2}  -3/\\sqrt{2}  0  0 \\end{pmatrix} = \\begin{pmatrix} 9  9  0  0 \\\\ 9  9  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}\n$$\n左上角 $2\\times2$ 區塊 $\\begin{pmatrix} 9  9 \\\\ 9  9 \\end{pmatrix}$ 的特徵值 $\\lambda$ 可由特徵方程式 $(9-\\lambda)^2 - 81 = 0$ 解得，結果為 $\\lambda = 18$ 和 $\\lambda = 0$。\n最大特徵值是 $\\lambda_0 = 18$。對應的特徵向量 $\\mathbf{v}_0 = [c_1, c_2, 0, 0]^\\top$ 滿足 $(9-18)c_1 + 9c_2 = 0$，即 $-9c_1 + 9c_2 = 0$，意味著 $c_1=c_2$。為了標準化，$c_1^2 + c_2^2 = 1 \\implies 2c_1^2 = 1 \\implies c_1 = 1/\\sqrt{2}$。因此，$\\mathbf{v}_0 = [1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top$。\n最大絕對值的元素為正，因此無需改變符號。\n\n使用第一個PC軸（$k=0$）。反向變換為：\n$$\n\\mathbf{x}(t) = \\mathbf{m}_B + t\\sqrt{\\lambda_0}\\,\\mathbf{v}_0 = [2, 2, 0, 0]^\\top + t\\sqrt{18}\\,[1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top\n$$\n由於 $\\sqrt{18} = 3\\sqrt{2}$，上式可簡化為：\n$$\n\\mathbf{x}(t) = [2, 2, 0, 0]^\\top + t(3\\sqrt{2})\\,[1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top = [2, 2, 0, 0]^\\top + t[3, 3, 0, 0]^\\top = [2+3t, 2+3t, 0, 0]^\\top\n$$\n我們計算 $\\mathbf{x}(+1)$ 和 $\\mathbf{x}(-1)$：\n$\\mathbf{x}(+1) = [2+3, 2+3, 0, 0]^\\top = [5, 5, 0, 0]^\\top$。\n$\\mathbf{x}(-1) = [2-3, 2-3, 0, 0]^\\top = [-1, -1, 0, 0]^\\top$。\n地標 $1$ 的坐標是前兩個元素。當 $t=+1$ 時，位置是 $(5, 5)$。當 $t=-1$ 時，位置是 $(-1, -1)$。\n這兩個位置之間的歐幾里得距離是：\n$$\nd = \\sqrt{(5 - (-1))^2 + (5 - (-1))^2} = \\sqrt{6^2 + 6^2} = \\sqrt{72} \\approx 8.48528137\n$$\n四捨五入到 $6$ 位小數得到 $8.485281$。\n\n**測試案例 C**\n\n我們重用案例A的數據和特徵分解。我們使用第二個PC軸（$k=1$），其特徵值為 $\\lambda_1 = 2/3$，特徵向量為 $\\mathbf{v}_1 = [0, 0, 0, 0, 0, 1]^\\top$。給定 $t=-1.5$。\n我們首先構造形狀 $\\mathbf{x}(t)$：\n$$\n\\mathbf{x}(-1.5) = \\mathbf{m}_A + (-1.5)\\sqrt{\\lambda_1}\\,\\mathbf{v}_1\n$$\n接著，我們求中心化向量 $\\mathbf{y}(t) = \\mathbf{x}(t) - \\mathbf{m}_A$：\n$$\n\\mathbf{y}(-1.5) = (\\mathbf{m}_A - 1.5\\sqrt{\\lambda_1}\\mathbf{v}_1) - \\mathbf{m}_A = -1.5\\sqrt{\\lambda_1}\\mathbf{v}_1\n$$\n問題要求的是PC分數，即該向量在軸 $\\mathbf{v}_1$ 上的投影。這可通過計算內積 $\\mathbf{y}(-1.5) \\cdot \\mathbf{v}_1$ 得到：\n$$\n\\text{Score} = (-1.5\\sqrt{\\lambda_1}\\mathbf{v}_1) \\cdot \\mathbf{v}_1 = -1.5\\sqrt{\\lambda_1}(\\mathbf{v}_1 \\cdot \\mathbf{v}_1)\n$$\n由於 $\\mathbf{v}_1$ 是單位向量，$\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = 1$。分數可簡化為：\n$$\n\\text{Score} = -1.5\\sqrt{\\lambda_1} = -1.5\\sqrt{2/3} \\approx -1.5 \\times 0.81649658 \\approx -1.22474487\n$$\n這證明了一致性：一個沿著軸 $k$ 移動 $t$ 個標準差所構造的形狀，在該軸上的PC分數恰好是 $t\\sqrt{\\lambda_k}$。\n四捨五入到 $6$ 位小數得到 $-1.224745$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the geometric morphometrics problem by implementing PCA steps\n    and back-transformation for three test cases.\n    \"\"\"\n\n    def perform_pca_and_get_components(Y, n):\n        \"\"\"\n        Computes covariance matrix, its eigendecomposition, and sorts\n        and normalizes the components according to the problem statement.\n        \"\"\"\n        d = Y.shape[1]\n        \n        # Handle the case n=1 where variance is undefined, though not in test cases\n        if n == 1:\n            return np.zeros(d), np.eye(d)\n\n        # 1. Compute the covariance matrix S\n        S = (1 / (n - 1)) * (Y.T @ Y)\n\n        # 2. Eigendecomposition of S. eigh is for symmetric matrices.\n        # It returns eigenvalues in ascending order.\n        eigvals, eigvecs = np.linalg.eigh(S)\n\n        # 3. Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigvals)[::-1]\n        sorted_eigvals = eigvals[sorted_indices]\n        sorted_eigvecs = eigvecs[:, sorted_indices]\n        \n        # 4. Enforce deterministic sign convention on eigenvectors\n        for k in range(d):\n            vk = sorted_eigvecs[:, k]\n            max_abs_idx = np.argmax(np.abs(vk))\n            if vk[max_abs_idx]  0:\n                sorted_eigvecs[:, k] = -vk\n        \n        return sorted_eigvals, sorted_eigvecs\n\n    # --- Test Case A ---\n    n_A = 4\n    m_A = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0])\n    Y_A = np.array([\n        [0.0, 0.0, -2.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 2.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, -1.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n    ])\n    \n    eigvals_A, eigvecs_A = perform_pca_and_get_components(Y_A, n_A)\n    \n    # First PC axis (k=0)\n    lambda0_A = eigvals_A[0]\n    v0_A = eigvecs_A[:, 0]\n    \n    t_values_A = [0.0, 1.0, -1.0, 2.5]\n    results_A = []\n    \n    for t in t_values_A:\n        # Back-transform to shape coordinates\n        x_t = m_A + t * np.sqrt(lambda0_A) * v0_A\n        # Get x-coordinate of landmark 2 (index 2)\n        coord = x_t[2]\n        results_A.append(round(coord, 6))\n\n    # --- Test Case B ---\n    n_B = 2\n    m_B = np.array([2.0, 2.0, 0.0, 0.0])\n    Y_B = np.array([\n        [3/np.sqrt(2), 3/np.sqrt(2), 0.0, 0.0],\n        [-3/np.sqrt(2), -3/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    eigvals_B, eigvecs_B = perform_pca_and_get_components(Y_B, n_B)\n\n    # First PC axis (k=0)\n    lambda0_B = eigvals_B[0]\n    v0_B = eigvecs_B[:, 0]\n\n    # Compute shapes at t=+1 and t=-1\n    x_plus_1 = m_B + 1.0 * np.sqrt(lambda0_B) * v0_B\n    x_minus_1 = m_B + (-1.0) * np.sqrt(lambda0_B) * v0_B\n    \n    # Landmark 1 coordinates are at indices 0 and 1\n    lm1_plus_1 = x_plus_1[:2]\n    lm1_minus_1 = x_minus_1[:2]\n    \n    # Euclidean distance between the two landmark 1 positions\n    dist_B = np.linalg.norm(lm1_plus_1 - lm1_minus_1)\n    result_B = round(dist_B, 6)\n\n    # --- Test Case C ---\n    \n    # Reuse PCA results from Case A\n    # Second PC axis (k=1)\n    lambda1_A = eigvals_A[1]\n    v1_A = eigvecs_A[:, 1]\n    \n    t_C = -1.5\n    \n    # The centered vector y(t) represents the deviation from the mean\n    y_t = t_C * np.sqrt(lambda1_A) * v1_A\n    \n    # Project y(t) onto the PC axis v1 to get the score.\n    # Mathematically, this simplifies to t * sqrt(lambda_k).\n    score_C = np.dot(y_t, v1_A)\n    result_C = round(score_C, 6)\n\n    # --- Final Output Formatting ---\n    # The required format is [[a1, a2, a3, a4], b, c]\n    # To match the no-space format in the list for A:\n    str_A = f\"[{','.join(f'{x:.6f}' for x in results_A)}]\"\n    str_B = f'{result_B:.6f}'\n    str_C = f'{result_C:.6f}'\n\n    print(f\"[{str_A},{str_B},{str_C}]\")\n\nsolve()\n\n```", "id": "2577650"}, {"introduction": "几何形态测量学的一个核心应用是检验不同生物群体（如物种、亚种或不同环境下的居群）之间是否存在显著的形态差异。线性判别分析（Linear Discriminant Analysis, LDA）是实现这一目标的经典方法，但其有效性建立在各组协方差矩阵相等的关键假设之上。本练习 [@problem_id:2577658] 提供了一个严谨的分析流程：首先使用 Box's M 检验来评估协方差同质性这一前提，然后根据检验结果和数据的稳定性，在LDA、二次判别分析（QDA）或正则化判别分析（RDA）之间做出明智的选择，以确保比较分析的统计鲁棒性。", "problem": "一位比较形态计量学家研究了多个生物群组在Procrustes切空间中的Procrustes对齐地标点构型，每个样本产生一个维度为 $p$ 的多变量形状向量。该分析人员考虑基于Mahalanobis距离进行组间判别，但意识到标准的合并协方差Mahalanobis距离假设组内协方差是同质的。您的任务是针对几个具体案例，确定同质性假设是否成立，然后在适当时算法性地从线性判别分析 (LDA)、二次判别分析 (QDA) 或正则化判别分析 (RDA) 中推荐一个替代方案。\n\n基础理论：\n- 假设在每个组 $i \\in \\{1,\\dots,g\\}$ 内，形状向量是从一个多变量正态分布中独立抽取的，其均值为 $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^p$，协方差为 $\\mathbf{S}_i \\in \\mathbb{R}^{p \\times p}$，其中 $\\mathbf{S}_i$ 是对称正定矩阵。\n- 合并协方差估计量定义为\n$$\n\\mathbf{S}_p \\;=\\; \\frac{\\sum_{i=1}^g (n_i - 1)\\,\\mathbf{S}_i}{\\sum_{i=1}^g (n_i - 1)} \\,,\n$$\n其中 $n_i \\ge 2$ 是组 $i$ 的样本量，$N = \\sum_{i=1}^g n_i$ 是总样本量。\n- Box M检验（多变量正态性下的对数似然比）用于评估各组间协方差的同质性。定义\n$$\nM \\;=\\; (N-g)\\,\\ln|\\mathbf{S}_p| \\;-\\; \\sum_{i=1}^g (n_i - 1)\\,\\ln|\\mathbf{S}_i| \\,,\n$$\n小样本校正为\n$$\nc \\;=\\; \\frac{2p^2 + 3p - 1}{6(p+1)(g-1)}\\left(\\sum_{i=1}^g \\frac{1}{n_i - 1} - \\frac{1}{N - g}\\right) \\,,\n$$\n卡方近似为\n$$\nX^2 \\;=\\; (1-c)\\,M \\quad\\text{with degrees of freedom}\\quad \\nu \\;=\\; \\frac{(g-1)\\,p\\,(p+1)}{2} \\,.\n$$\n设 $p$ 值由自由度为 $\\nu$ 的卡方分布的上尾计算得出。使用显著性水平 $\\alpha = 0.05$。\n- 条件数 $\\kappa(\\mathbf{S}_i)$ 量化了矩阵求逆的数值稳定性，其中\n$$\n\\kappa(\\mathbf{S}_i) \\;=\\; \\frac{\\lambda_{\\max}(\\mathbf{S}_i)}{\\lambda_{\\min}(\\mathbf{S}_i)} \\,,\n$$\n$\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是 $\\mathbf{S}_i$ 的最大和最小特征值。使用阈值 $\\kappa_{\\max} = 10^8$ 来标记病态条件（ill-conditioning）。\n\n推荐分类器的决策逻辑：\n- 如果 Box M检验不显著（即 $p$ 值 $\\ge \\alpha$），推荐 LDA（代码 $0$）。\n- 如果显著（$p$ 值 $ \\alpha$）且所有组的 $\\kappa(\\mathbf{S}_i) \\le \\kappa_{\\max}$，推荐 QDA（代码 $1$）。\n- 如果显著且至少有一个组的 $\\kappa(\\mathbf{S}_i) > \\kappa_{\\max}$，推荐 RDA（代码 $2$），这里理解为一种能够稳定矩阵求逆的岭正则化协方差方法。\n\n实现一个程序，对下面的每个测试用例，计算：\n- Box M检验的 $p$ 值（四舍五入到 $6$ 位小数）。\n- 一个布尔值，指示协方差的异质性（如果 $p$ 值 $ \\alpha$，则为真）。\n- 整数推荐代码（$0$ 代表 LDA，$1$ 代表 QDA，$2$ 代表 RDA）。\n\n角度和物理单位在此情景下不适用。\n\n测试套件：\n- 案例 A：$g=2$，$p=3$，$n_1=30$，$n_2=28$，其中\n$$\n\\mathbf{S}_1 \\;=\\; \\begin{bmatrix}\n1.0  0.2  0.1\\\\\n0.2  1.0  0.05\\\\\n0.1  0.05  0.5\n\\end{bmatrix},\n\\quad\n\\mathbf{S}_2 \\;=\\; \\begin{bmatrix}\n1.0  0.2  0.1\\\\\n0.2  1.0  0.05\\\\\n0.1  0.05  0.5\n\\end{bmatrix}.\n$$\n- 案例 B：$g=3$，$p=3$，$n_1=25$，$n_2=35$，$n_3=40$，其中\n$$\n\\mathbf{S}_1 \\;=\\; \\begin{bmatrix}\n0.8  0.1  0.0\\\\\n0.1  0.4  0.05\\\\\n0.0  0.05  0.6\n\\end{bmatrix},\\quad\n\\mathbf{S}_2 \\;=\\; \\begin{bmatrix}\n1.5  0.4  0.2\\\\\n0.4  1.2  0.3\\\\\n0.2  0.3  1.0\n\\end{bmatrix},\\quad\n\\mathbf{S}_3 \\;=\\; \\begin{bmatrix}\n0.7  -0.2  0.1\\\\\n-0.2  0.9  -0.15\\\\\n0.1  -0.15  0.5\n\\end{bmatrix}.\n$$\n- 案例 C：$g=2$，$p=3$，$n_1=12$，$n_2=12$，其中\n$$\n\\mathbf{S}_1 \\;=\\; \\begin{bmatrix}\n10^{-8}  0.0  0.0\\\\\n0.0  1.0  0.2\\\\\n0.0  0.2  1.2\n\\end{bmatrix},\\quad\n\\mathbf{S}_2 \\;=\\; \\begin{bmatrix}\n0.9  0.1  0.0\\\\\n0.1  0.9  0.05\\\\\n0.0  0.05  0.9\n\\end{bmatrix}.\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果是一个形式为 $[\\text{pvalue}, \\text{heterogeneous}, \\text{recommendation}]$ 的三元素列表。例如，三个用例的输出应如下所示\n$$\n[\\,[0.123456,\\text{True},1],[0.987654,\\text{False},0],[0.000001,\\text{True},2]\\,].\n$$\n- $p$ 值必须四舍五入到 $6$ 位小数。布尔值必须是 $\\text{True}$ 或 $\\text{False}$，推荐必须是整数代码 $0$、$1$ 或 $2$。", "solution": "问题陈述已经过严格验证，并被确定为是合理的。它在多变量统计学领域内，特别是在应用于形态计量分析时，提出了一个良构问题。所有参数、定义和条件都得到了明确陈述，具有科学依据，并且没有内部矛盾或歧义。未发现任何会使问题无效的缺陷。因此，我们将继续提供一个完整的解答。\n\n任务是算法性地推荐一种分类方法——线性判别分析 (LDA)、二次判别分析 (QDA) 或正则化判别分析 (RDA)——用于判别由形状向量描述的几个生物群组。该决策基于组内样本协方差矩阵的统计特性。这个决策过程依赖于两个基本支柱：各组间协方差的同质性和这些矩阵的数值稳定性。\n\nLDA和QDA之间的主要区别在于协方差同质性的假设。LDA在一个简化假设下运行，即所有组 $i \\in \\{1, \\dots, g\\}$ 共享一个单一的、公共的协方差矩阵，记为 $\\mathbf{S}$。相比之下，QDA放宽了这一约束，允许每个组拥有其自己独有的协方差矩阵 $\\mathbf{S}_i$。因此，这些模型之间的选择由一个关于原假设 $H_0: \\mathbf{S}_1 = \\mathbf{S}_2 = \\dots = \\mathbf{S}_g$ 的统计检验来决定。如果这个假设不被拒绝，LDA通常是更简约因而更受青睐的模型，因为它需要估计的参数更少，因此更稳健，尤其是在样本量有限的情况下。如果假设被拒绝，表明存在显著的异质性，那么采用限制性的LDA模型可能导致次优的分类准确率，从而使得QDA成为理论上更优越的选择。\n\n为了正式检验同质性假设，我们采用Box M检验。这是Bartlett方差相等性检验的一个多变量扩展。假设每个组中的形状向量都从一个多变量正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_i, \\mathbf{S}_i)$ 中抽样得到，检验统计量 $M$ 是基于一个似然比准则构建的：\n$$\nM = (N-g)\\ln|\\mathbf{S}_p| - \\sum_{i=1}^g (n_i - 1)\\ln|\\mathbf{S}_i|\n$$\n在此表达式中，$g$ 代表组的数量，$p$ 是形状向量的维度，$n_i$ 是组 $i$ 的样本量，而 $N = \\sum_{i=1}^g n_i$ 是总样本量。矩阵 $\\mathbf{S}_p$ 是合并协方差矩阵，计算方式为各个组协方差矩阵 $\\mathbf{S}_i$ 的加权平均：\n$$\n\\mathbf{S}_p = \\frac{\\sum_{i=1}^g (n_i - 1)\\mathbf{S}_i}{N - g}\n$$\n$M$ 统计量的分布近似于一个卡方分布。为了提高准确性，应用了一个校正因子，得到统计量 $X^2 = (1-c)M$，该统计量与一个自由度为 $\\nu$ 的 $\\chi^2$ 分布进行比较。校正因子 $c$ 和自由度 $\\nu$ 定义如下：\n$$\nc = \\frac{2p^2 + 3p - 1}{6(p+1)(g-1)}\\left(\\sum_{i=1}^g \\frac{1}{n_i - 1} - \\frac{1}{N - g}\\right)\n$$\n$$\n\\nu = \\frac{(g-1)p(p+1)}{2}\n$$\n$p$ 值从此分布的上尾计算得出，即 $P(\\chi^2_\\nu \\ge X^2)$。如果此 $p$ 值不小于指定的显著性水平 $\\alpha = 0.05$，则保留同质性原假设，并推荐LDA（编码为 $0$）。\n\n相反，如果 $p$ 值小于 $\\alpha$，则拒绝原假设。这表明各组的协方差矩阵存在显著差异。虽然在这种异质性情况下，QDA是指定的模型，但其实施需要计算每个组协方差矩阵的逆矩阵 $\\mathbf{S}_i^{-1}$。如果任何一个 $\\mathbf{S}_i$ 是病态的（即接近奇异），这个求逆过程在数值上是不稳定的。求逆的数值稳定性通过矩阵条件数 $\\kappa(\\mathbf{S}_i)$ 来评估，即矩阵最大特征值与最小特征值的比率：\n$$\n\\kappa(\\mathbf{S}_i) = \\frac{\\lambda_{\\max}(\\mathbf{S}_i)}{\\lambda_{\\min}(\\mathbf{S}_i)}\n$$\n一个大的条件数警告说，逆矩阵对数据中的微小误差高度敏感，可能产生不可靠的结果。我们采用一个预定义的阈值 $\\kappa_{\\max} = 10^8$ 来标记这种不稳定性。\n\n因此，完整的算法决策逻辑结构如下：\n1.  计算与Box M协方差同质性检验相关的 $p$ 值。\n2.  如果 $p$ 值大于或等于 $0.05$，则认为同质性假设成立。推荐使用LDA（代码 $0$）。\n3.  如果 $p$ 值小于 $0.05$，则拒绝同质性假设。必须对所有组的协方差矩阵评估其条件数 $\\kappa(\\mathbf{S}_i)$。\n    a. 如果对于所有组 $i$ 都有 $\\kappa(\\mathbf{S}_i) \\le 10^8$：所有矩阵都被认为是良态的，求逆在数值上是稳定的。推荐使用QDA（代码 $1$）。\n    b. 如果至少有一个组 $i$ 满足 $\\kappa(\\mathbf{S}_i) > 10^8$：至少有一个矩阵是病态的。标准的QDA将是不可靠的。需要一种正则化方法来确保解的稳定性。推荐使用RDA（代码 $2$）。\n\n以下程序实现了这一逻辑，以系统地分析所提供的测试用例并生成指定的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"id\": \"A\", \"g\": 2, \"p\": 3, \"n\": [30, 28],\n            \"S\": [\n                np.array([[1.0, 0.2, 0.1], [0.2, 1.0, 0.05], [0.1, 0.05, 0.5]]),\n                np.array([[1.0, 0.2, 0.1], [0.2, 1.0, 0.05], [0.1, 0.05, 0.5]])\n            ]\n        },\n        # Case B\n        {\n            \"id\": \"B\", \"g\": 3, \"p\": 3, \"n\": [25, 35, 40],\n            \"S\": [\n                np.array([[0.8, 0.1, 0.0], [0.1, 0.4, 0.05], [0.0, 0.05, 0.6]]),\n                np.array([[1.5, 0.4, 0.2], [0.4, 1.2, 0.3], [0.2, 0.3, 1.0]]),\n                np.array([[0.7, -0.2, 0.1], [-0.2, 0.9, -0.15], [0.1, -0.15, 0.5]])\n            ]\n        },\n        # Case C\n        {\n            \"id\": \"C\", \"g\": 2, \"p\": 3, \"n\": [12, 12],\n            \"S\": [\n                np.array([[1e-8, 0.0, 0.0], [0.0, 1.0, 0.2], [0.0, 0.2, 1.2]]),\n                np.array([[0.9, 0.1, 0.0], [0.1, 0.9, 0.05], [0.0, 0.05, 0.9]])\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_value, heterogeneous, recommendation = _solve_case(case)\n        results.append([p_value, heterogeneous, recommendation])\n\n    # Final print statement in the exact required format.\n    result_strings = [f\"[{p:.6f},{'True' if h else 'False'},{r}]\" for p, h, r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef _solve_case(case_data):\n    \"\"\"\n    Processes a single test case to determine covariance homogeneity and\n    recommend a classifier.\n\n    Args:\n        case_data (dict): A dictionary containing the parameters for a single case.\n\n    Returns:\n        tuple: A tuple containing (p-value, heterogeneous_flag, recommendation_code).\n    \"\"\"\n    g = case_data[\"g\"]\n    p = case_data[\"p\"]\n    n_i_list = case_data[\"n\"]\n    S_i_list = case_data[\"S\"]\n    \n    alpha = 0.05\n    kappa_max = 1e8\n\n    # Calculate total sample size N and degrees of freedom\n    N = sum(n_i_list)\n    df_i_list = [n - 1 for n in n_i_list]\n    df_total = N - g # This is sum(df_i_list)\n\n    # Calculate pooled covariance matrix S_p\n    S_p_numerator = np.zeros((p, p))\n    for i in range(g):\n        S_p_numerator += df_i_list[i] * S_i_list[i]\n    S_p = S_p_numerator / df_total\n    \n    # Calculate determinants and their logarithms.\n    # np.linalg.slogdet is used for stability, but problem guarantees PD matrices.\n    # Using np.linalg.det for simplicity as per problem setup.\n    det_S_p = np.linalg.det(S_p)\n    log_det_S_p = np.log(det_S_p)\n\n    sum_log_det_S_i = 0\n    for i in range(g):\n        det_S_i = np.linalg.det(S_i_list[i])\n        log_det_S_i = np.log(det_S_i)\n        sum_log_det_S_i += df_i_list[i] * log_det_S_i\n    \n    # Calculate Box's M statistic\n    M = df_total * log_det_S_p - sum_log_det_S_i\n\n    # Handle the case where matrices are identical, M can be exactly 0\n    if np.isclose(M, 0):\n        p_value = 1.0\n    else:\n        # Calculate Box's small-sample correction factor c\n        c_factor1 = (2 * p**2 + 3 * p - 1) / (6 * (p + 1) * (g - 1))\n        c_factor2 = sum(1.0/df for df in df_i_list) - (1.0 / df_total)\n        c = c_factor1 * c_factor2\n        \n        # Calculate chi-square statistic and degrees of freedom\n        X2 = (1 - c) * M\n        nu = (g - 1) * p * (p + 1) / 2\n        \n        # Calculate p-value from the chi-square distribution's survival function\n        p_value = chi2.sf(X2, nu)\n\n    # Apply decision logic based on p-value and condition numbers\n    heterogeneous = p_value  alpha\n    recommendation = -1  # Placeholder for an undefined recommendation\n\n    if not heterogeneous:\n        recommendation = 0  # LDA\n    else:\n        # Covariances are heterogeneous, check matrix conditioning for QDA\n        is_ill_conditioned = False\n        for S_i in S_i_list:\n            # eigvalsh is for symmetric matrices and returns sorted eigenvalues\n            eigenvalues = np.linalg.eigvalsh(S_i)\n            # Smallest eigenvalue must be positive for positive-definiteness\n            if eigenvalues[0] = 0:\n                is_ill_conditioned = True\n                break\n            \n            kappa = eigenvalues[-1] / eigenvalues[0]\n            if kappa > kappa_max:\n                is_ill_conditioned = True\n                break\n        \n        if is_ill_conditioned:\n            recommendation = 2  # RDA\n        else:\n            recommendation = 1  # QDA\n            \n    return p_value, heterogeneous, recommendation\n\nsolve()\n```", "id": "2577658"}]}