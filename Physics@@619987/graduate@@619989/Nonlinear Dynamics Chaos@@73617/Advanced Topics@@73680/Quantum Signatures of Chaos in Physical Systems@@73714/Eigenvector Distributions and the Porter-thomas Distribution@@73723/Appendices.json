{"hands_on_practices": [{"introduction": "The Porter-Thomas distribution, fundamental to describing chaotic systems with time-reversal symmetry, is a specific instance of the chi-squared distribution with one degree of freedom. This exercise explores the additive properties of this distribution, a crucial concept when considering combined physical effects like the total decay width from multiple independent channels. By calculating the probability of a sum of two variables exceeding a certain threshold, you will build a deeper intuition for the shape and scale of this important distribution [@problem_id:868881].", "problem": "In the study of complex quantum systems, whose classical dynamics are chaotic, the statistical properties of observables are often described using Random Matrix Theory (RMT). For systems exhibiting time-reversal symmetry, the Gaussian Orthogonal Ensemble (GOE) is of particular importance. One of the key predictions of GOE concerns the statistical distribution of the squared components of eigenvectors. These quantities are directly related to physical observables such as partial decay widths in nuclear reactions or scattering amplitudes. This distribution is known as the Porter-Thomas distribution.\n\nThe probability density function (PDF) for a random variable $X$ that follows the Porter-Thomas distribution is given by:\n$$\nP(x; \\mu) = \\frac{1}{\\sqrt{2\\pi \\mu x}} e^{-x/(2\\mu)}, \\quad x \\ge 0\n$$\nwhere $\\mu = \\langle X \\rangle$ is the mean of the distribution. This distribution is a scaled version of the chi-squared distribution with one degree of freedom ($\\chi_1^2$).\n\nConsider two independent and identically distributed (i.i.d.) random variables, $X_1$ and $X_2$, both drawn from a Porter-Thomas distribution with the same mean $\\mu$. Let $Z$ be the random variable representing their sum, $Z = X_1 + X_2$.\n\nYour task is to calculate the probability that the sum $Z$ exceeds twice the mean of a single variable. In other words, find the exact value of $P(Z > 2\\mu)$.", "solution": "We have $X_i\\sim\\mathrm{Porter\\text{-}Thomas}(\\mu)$ with PDF \n$$P(x;\\mu)=\\frac1{\\sqrt{2\\pi\\mu x}}e^{-x/(2\\mu)},\\quad x\\ge0.$$\nObserve that if we set $Y_i=X_i/\\mu$, then\n$$P_Y(y)=\\frac1{\\sqrt{2\\pi y}}e^{-y/2},\\quad y\\ge0,$$\nwhich is the $\\chi^2$ distribution with one degree of freedom. Hence \n$$X_i=\\mu\\,Y_i,\\qquad Y_i\\sim\\chi^2_1.$$\nThe sum \n$$Z=X_1+X_2=\\mu(Y_1+Y_2).$$\nSince $Y_1,Y_2\\sim\\chi^2_1$ i.i.d., their sum $Y_1+Y_2\\sim\\chi^2_2$, which is a Gamma distribution with shape $k=1$ and scale $\\theta=2$. Thus \n$$Z\\sim\\mathrm{Gamma}(k=1,\\;\\theta=2\\mu),$$ \ni.e.\\ an exponential distribution with mean $2\\mu$. Its survival function is \n$$P(Zz)=\\exp\\bigl(-z/(2\\mu)\\bigr).$$\nEvaluating at $z=2\\mu$ gives\n$$P(Z2\\mu)=\\exp\\bigl(-2\\mu/(2\\mu)\\bigr)=e^{-1}.$$", "answer": "$$\\boxed{e^{-1}}$$", "id": "868881"}, {"introduction": "A theoretical model is only as good as its ability to describe real-world data. The Porter-Thomas distribution describes the statistics of quantities like resonance widths, but its mean value, $\\lambda$, must be determined from measurements. This practice introduces the powerful method of maximum likelihood estimation to find the best-fit value for this mean from a set of experimental data, bridging the gap between abstract theory and practical data analysis [@problem_id:868915].", "problem": "In the study of quantum chaos, the statistical properties of energy levels and wavefunctions of complex systems are often described by Random Matrix Theory (RMT). For systems with time-reversal symmetry, the appropriate ensemble is the Gaussian Orthogonal Ensemble (GOE). A key prediction of the GOE is that the components of the normalized eigenvectors are Gaussian-distributed random variables.\n\nConsequently, the distribution of transition intensities (or resonance widths) follows a chi-squared distribution with one degree of freedom, known as the Porter-Thomas distribution. If we consider a set of intensities $\\{I_k\\}$ and normalize them by their mean, $x = I / \\langle I \\rangle$, the probability density function (PDF) for the normalized intensity $x$ is given by:\n$$\nP(x) = \\frac{1}{\\sqrt{2\\pi x}} e^{-x/2}\n$$\nfor $x \\ge 0$.\n\nAn experiment measures a set of $M$ independent transition intensities, $\\{I_1, I_2, \\ldots, I_M\\}$, which are all drawn from a distribution with the same functional form but with an unknown mean value, $\\lambda = \\langle I \\rangle$. Your task is to apply the method of maximum likelihood to find the estimator for this mean, $\\hat{\\lambda}$.\n\nDetermine the maximum likelihood estimator $\\hat{\\lambda}$ for the mean parameter $\\lambda$ of the distribution, expressed in terms of the measured intensities $\\{I_1, I_2, \\ldots, I_M\\}$ and the number of measurements $M$.", "solution": "We start from the Porter–Thomas PDF for normalized intensity $x=I/\\lambda$,\n$$P_x(x)=\\frac1{\\sqrt{2\\pi x}}e^{-x/2},\\quad x\\ge0.$$\nChange variables to $I=\\lambda x$, so $dx=dI/\\lambda$.  The PDF of $I$ is\n$$f(I;\\lambda)\n=P_x\\bigl(I/\\lambda\\bigr)\\frac{1}{\\lambda}\n=\\frac{1}{\\sqrt{2\\pi\\,(I/\\lambda)}}e^{-I/(2\\lambda)}\\frac{1}{\\lambda}\n=\\frac{1}{\\sqrt{2\\pi\\,I\\,\\lambda}}e^{-I/(2\\lambda)}.$$\nFor $M$ independent measurements $\\{I_k\\}$ the likelihood is\n$$L(\\lambda)=\\prod_{k=1}^M f(I_k;\\lambda)\n=\\prod_{k=1}^M\\frac{1}{\\sqrt{2\\pi\\,I_k\\,\\lambda}}\n\\exp\\!\\Bigl(-\\frac{I_k}{2\\lambda}\\Bigr).$$\nThe log‐likelihood is\n$$\\ln L(\\lambda)\n=-\\frac M2\\ln(2\\pi)-\\frac12\\sum_{k=1}^M\\ln I_k\n-\\frac M2\\ln\\lambda-\\frac{1}{2\\lambda}\\sum_{k=1}^M I_k.$$\nDifferentiate w.r.t. $\\lambda$ and set to zero:\n$$\\frac{d}{d\\lambda}\\ln L\n=-\\frac M{2\\lambda}+\\frac{1}{2\\lambda^2}\\sum_{k=1}^M I_k\n=0\n\\;\\Longrightarrow\\;\n\\sum_{k=1}^M I_k\n=M\\,\\hat\\lambda\n\\;\\Longrightarrow\\;\n\\hat\\lambda=\\frac{1}{M}\\sum_{k=1}^M I_k.$$\nThus the maximum‐likelihood estimator of $\\lambda$ is the sample mean.", "answer": "$$\\boxed{\\frac{1}{M}\\sum_{k=1}^M I_k}$$", "id": "868915"}, {"introduction": "The statistical laws governing quantum chaos are deeply tied to the underlying symmetries of the system. While the Porter-Thomas distribution characterizes systems with time-reversal symmetry (described by the Gaussian Orthogonal Ensemble, or GOE), breaking this symmetry leads to a different statistical signature (described by the Gaussian Unitary Ensemble, or GUE). This exercise provides a direct comparison of the two, challenging you to quantify the observable consequences of this fundamental symmetry difference by calculating the probability that an intensity from a GOE system is greater than one from a GUE system [@problem_id:868983].", "problem": "In Random Matrix Theory (RMT), the statistical properties of complex quantum systems are modeled by large random matrices. The Gaussian Orthogonal Ensemble (GOE) and Gaussian Unitary Ensemble (GUE) are two fundamental ensembles used to describe systems with and without time-reversal symmetry, respectively.\n\nFor a large $N \\times N$ matrix drawn from one of these ensembles, the components of its normalized eigenvectors are random variables. The \"intensity\" of a component is defined as the squared magnitude of the component, rescaled to have a mean of one. Let this intensity be denoted by $y$.\n\nFor the GOE, the probability distribution of the intensity $y$ is given by the Porter-Thomas distribution:\n$$\nP_{GOE}(y) = \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}, \\quad \\text{for } y  0\n$$\n\nFor the GUE, the intensity distribution is a simple exponential:\n$$\nP_{GUE}(y) = e^{-y}, \\quad \\text{for } y \\ge 0\n$$\nBoth distributions are normalized such that $\\int_0^\\infty P(y) \\, dy = 1$ and have a mean intensity $\\langle y \\rangle = \\int_0^\\infty y P(y) \\, dy = 1$.\n\nConsider two independent random variables: $Y_{GOE}$, an intensity drawn from the GOE distribution, and $Y_{GUE}$, an intensity drawn from the GUE distribution. Calculate the probability that the intensity from the GOE case is greater than the intensity from the GUE case, i.e., compute $P(Y_{GOE}  Y_{GUE})$.", "solution": "Let $Y_{GOE}$ be a random variable with probability density function (PDF) $p_1(y_1) = P_{GOE}(y_1)$ and $Y_{GUE}$ be a random variable with PDF $p_2(y_2) = P_{GUE}(y_2)$.\nWe are given:\n$$\np_1(y_1) = \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2}, \\quad y_1  0\n$$\n$$\np_2(y_2) = e^{-y_2}, \\quad y_2 \\ge 0\n$$\nWe want to calculate the probability $P(Y_{GOE}  Y_{GUE})$. Since the two random variables are independent, their joint PDF is the product of their individual PDFs:\n$$\np(y_1, y_2) = p_1(y_1) p_2(y_2) = \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} e^{-y_2}\n$$\nThe desired probability is the integral of the joint PDF over the region of the $(y_1, y_2)$-plane where $y_1  y_2$, $y_1  0$, and $y_2 \\ge 0$. The condition $y_1  y_2$ combined with $y_2 \\ge 0$ implies $y_1  0$. So the integration domain is $0 \\le y_2  y_1  \\infty$.\n\nWe can set up the double integral as follows:\n$$\nP(Y_{GOE}  Y_{GUE}) = \\int_{0}^{\\infty} \\int_{y_2}^{\\infty} p(y_1, y_2) \\, dy_1 \\, dy_2\n$$\nSubstituting the PDFs:\n$$\nP(Y_{GOE}  Y_{GUE}) = \\int_{0}^{\\infty} e^{-y_2} \\left( \\int_{y_2}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} \\, dy_1 \\right) dy_2\n$$\nThis order of integration can be complex. It is often simpler to change the order of integration. The domain of integration is described by $0 \\le y_2  \\infty$ and $y_2  y_1  \\infty$. This is equivalent to the domain described by $0  y_1  \\infty$ and $0 \\le y_2  y_1$.\n\nSo we can rewrite the integral as:\n$$\nP(Y_{GOE}  Y_{GUE}) = \\int_{0}^{\\infty} \\int_{0}^{y_1} p(y_1, y_2) \\, dy_2 \\, dy_1\n$$\n$$\nP(Y_{GOE}  Y_{GUE}) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} \\left( \\int_{0}^{y_1} e^{-y_2} \\, dy_2 \\right) dy_1\n$$\nFirst, we evaluate the inner integral with respect to $y_2$:\n$$\n\\int_{0}^{y_1} e^{-y_2} \\, dy_2 = \\left[ -e^{-y_2} \\right]_{0}^{y_1} = -e^{-y_1} - (-e^{-0}) = 1 - e^{-y_1}\n$$\nNow, substitute this result back into the outer integral:\n$$\nP(Y_{GOE}  Y_{GUE}) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} (1 - e^{-y_1}) \\, dy_1\n$$\nWe can split this into two integrals:\n$$\nP(Y_{GOE}  Y_{GUE}) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} \\, dy_1 - \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} e^{-y_1} \\, dy_1\n$$\nThe first integral is the integral of the $P_{GOE}(y_1)$ distribution over its entire domain, which must be equal to 1 by definition of a probability distribution.\n$$\n\\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-y_1/2} \\, dy_1 = \\int_{0}^{\\infty} p_1(y_1) \\, dy_1 = 1\n$$\nThe second integral is:\n$$\nI = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi y_1}} e^{-3y_1/2} \\, dy_1\n$$\nTo evaluate this integral, we use a substitution. Let $u = \\frac{3y_1}{2}$. This implies $y_1 = \\frac{2u}{3}$ and $dy_1 = \\frac{2}{3} du$. The term $\\frac{1}{\\sqrt{y_1}}$ becomes $\\frac{1}{\\sqrt{2u/3}} = \\sqrt{\\frac{3}{2u}}$.\nSubstituting these into the integral $I$:\n$$\nI = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\sqrt{\\frac{3}{2u}} e^{-u} \\left(\\frac{2}{3}\\right) du\n$$\nSimplifying the constants:\n$$\nI = \\frac{1}{\\sqrt{2\\pi}} \\frac{\\sqrt{3}}{\\sqrt{2}} \\frac{2}{3} \\int_{0}^{\\infty} u^{-1/2} e^{-u} \\, du = \\frac{\\sqrt{3}}{2\\sqrt{\\pi}} \\frac{2}{3} \\int_{0}^{\\infty} u^{-1/2} e^{-u} \\, du = \\frac{1}{\\sqrt{3}\\sqrt{\\pi}} \\int_{0}^{\\infty} u^{-1/2} e^{-u} \\, du\n$$\nThe remaining integral is in the form of the Gamma function, $\\Gamma(z) = \\int_{0}^{\\infty} t^{z-1} e^{-t} dt$.\nHere, $z-1 = -1/2$, so $z=1/2$.\n$$\n\\int_{0}^{\\infty} u^{-1/2} e^{-u} \\, du = \\Gamma(1/2)\n$$\nThe value of the Gamma function at $1/2$ is a known result, $\\Gamma(1/2) = \\sqrt{\\pi}$.\nSubstituting this value back:\n$$\nI = \\frac{1}{\\sqrt{3}\\sqrt{\\pi}} \\Gamma(1/2) = \\frac{1}{\\sqrt{3}\\sqrt{\\pi}} \\sqrt{\\pi} = \\frac{1}{\\sqrt{3}}\n$$\nFinally, we combine the results for the two integrals:\n$$\nP(Y_{GOE}  Y_{GUE}) = 1 - I = 1 - \\frac{1}{\\sqrt{3}}\n$$", "answer": "$$ \\boxed{1 - \\frac{1}{\\sqrt{3}}} $$", "id": "868983"}]}