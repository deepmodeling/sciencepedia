{"hands_on_practices": [{"introduction": "To truly understand a statistical distribution, we must explore its fundamental mathematical properties. This first exercise invites you to analyze the shape of the celebrated Wigner surmise for the Gaussian Orthogonal Ensemble (GOE), a cornerstone for describing quantum systems with time-reversal symmetry. By calculating a key feature like an inflection point, you will develop a more tactile understanding of the function that mathematically embodies the principle of level repulsion [@problem_id:881630].", "problem": "In random matrix theory, the statistical properties of energy levels of complex quantum systems are modeled by the eigenvalues of large random matrices. A key statistic is the distribution of spacings between adjacent energy levels, $s$, after a normalization procedure known as \"unfolding\" which sets the mean spacing to unity.\n\nFor systems with time-reversal invariance, the Hamiltonian can be represented by a real symmetric matrix, and the relevant matrix ensemble is the Gaussian Orthogonal Ensemble (GOE). The probability distribution of the unfolded level spacing, $P(s)$, is well-approximated by the Wigner surmise, which is derived from the exact solution for 2x2 matrices. The GOE Wigner surmise is given by the probability density function:\n$$\nP(s) = \\frac{\\pi s}{2} \\exp\\left(-\\frac{\\pi s^2}{4}\\right) \\quad \\text{for } s \\ge 0\n$$\nThis distribution exhibits level repulsion, meaning $P(s) \\to 0$ as $s \\to 0$. It has a single peak and decays rapidly for large spacings.\n\nAn inflection point of a function is a point on its graph where the curvature or concavity changes sign. For a twice-differentiable function, this occurs where its second derivative is zero and changes sign.\n\nDetermine the value of the unfolded level spacing, $s$, that corresponds to the non-trivial inflection point of the GOE Wigner surmise distribution $P(s)$.", "solution": "We have \n$$P(s)=\\frac{\\pi s}{2}\\exp\\!\\biggl(-\\frac{\\pi s^2}{4}\\biggr)\\,. $$\n1. Compute the first derivative:\n$$P'(s)=\\frac{\\pi}{2}\\frac{d}{ds}\\Bigl[s\\,e^{-B s^2}\\Bigr],\\quad B=\\frac{\\pi}{4}.$$\nUsing the product rule,\n$$P'(s)=\\frac{\\pi}{2}\\Bigl(e^{-B s^2}+s(-2B s)e^{-B s^2}\\Bigr)\n=\\frac{\\pi}{2}e^{-B s^2}\\bigl(1-2B s^2\\bigr).$$\n2. Compute the second derivative:\n$$P''(s)=\\frac{\\pi}{2}\\frac{d}{ds}\\Bigl[e^{-B s^2}(1-2B s^2)\\Bigr].$$\nAgain the product rule gives\n$$P''(s)=\\frac{\\pi}{2}\\,e^{-B s^2}\\Bigl[-2B s(1-2B s^2)-4B s\\Bigr]\n=\\frac{\\pi}{2}e^{-B s^2}\\bigl(4B^2s^3-6Bs\\bigr).$$\nFactor out $2B s$:\n$$P''(s)=\\frac{\\pi}{2}e^{-B s^2}\\,2B s\\,(2B s^2-3).$$\n3. Inflection points satisfy $P''(s)=0$.  Excluding the trivial $s=0$, we set\n$$2B s^2-3=0\\quad\\Longrightarrow\\quad s^2=\\frac{3}{2B}=\\frac{3}{2\\cdot(\\pi/4)}=\\frac{6}{\\pi}\\,. $$\nThus the non-trivial inflection point is\n$$s=\\sqrt{\\frac{6}{\\pi}}\\,. $$", "answer": "$$\\boxed{\\sqrt{\\frac{6}{\\pi}}}$$", "id": "881630"}, {"introduction": "Theoretical models in physics gain their power from their ability to predict experimental or numerical results. This computational practice challenges you to verify the Wigner surmise from the ground up by simulating the eigenvalue statistics of the Gaussian Orthogonal Ensemble itself [@problem_id:2387505]. This exercise directly connects abstract theory with concrete data, demonstrating how a simple statistical assumption about matrix elements leads to the emergent phenomenon of level spacing statistics.", "problem": "Consider the Gaussian Orthogonal Ensemble (GOE). A GOE matrix of size $N$ is a real symmetric random matrix $H \\in \\mathbb{R}^{N \\times N}$ constructed such that the entries above the diagonal are independent and identically distributed normal random variables with mean $0$ and variance $1$, the diagonal entries are independent normal random variables with mean $0$ and variance $2$, and $H_{ij} = H_{ji}$. Equivalently, one may define $H = (R + R^{\\mathsf{T}})/\\sqrt{2}$ where $R$ has independent and identically distributed standard normal entries. For each matrix $H$, let its ordered eigenvalues be $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_N$. Define the nearest-neighbor spacings $s_k = \\lambda_{k+1} - \\lambda_k$ for $k = 1, 2, \\ldots, N-1$. Aggregate all such spacings over a specified number of independently drawn matrices for a given $N$. Normalize these spacings by dividing by their sample mean so that the normalized spacings have unit mean. For the GOE, the Wigner surmise for the probability density function of the normalized nearest-neighbor spacing is\n$$\nP_{\\mathrm{W}}(s) = \\frac{\\pi}{2} s \\, e^{-\\frac{\\pi}{4} s^2}, \\quad s \\ge 0,\n$$\nwith cumulative distribution function\n$$\nF_{\\mathrm{W}}(s) = \\int_0^s P_{\\mathrm{W}}(t)\\, dt = 1 - e^{-\\frac{\\pi}{4} s^2}.\n$$\nLet $F_n(s)$ denote the empirical cumulative distribution function built from the normalized spacings in a given test case. Quantify agreement with the Wigner surmise using the Kolmogorov–Smirnov distance\n$$\nD = \\sup_{s \\ge 0} \\left| F_n(s) - F_{\\mathrm{W}}(s) \\right|.\n$$\nYour task is to produce a complete program that, for each test case below, constructs the GOE spacings as described, normalizes them to unit mean, computes the Kolmogorov–Smirnov distance $D$ between the empirical distribution of normalized spacings and $F_{\\mathrm{W}}(s)$, and outputs $D$.\n\nTest suite (each test case is a triple $(N, M, s)$ where $N$ is the matrix size, $M$ is the number of independently drawn matrices, and $s$ is the seed for the pseudorandom number generator):\n- Case $1$: $(N, M, s) = (2, 10000, 17)$.\n- Case $2$: $(N, M, s) = (40, 120, 2024)$.\n- Case $3$: $(N, M, s) = (3, 200, 7)$.\n\nAll numerical outputs must be expressed as pure decimal numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above, with each Kolmogorov–Smirnov distance rounded to six decimal places (for example, $[0.012345,0.067890,0.123456]$).", "solution": "The problem as stated is valid. It presents a well-defined computational task rooted in the established principles of Random Matrix Theory (RMT), a key area of mathematical physics. The problem is scientifically grounded, with correct definitions for the Gaussian Orthogonal Ensemble (GOE), the Wigner surmise for level spacings, and the a ppropriate statistical metric, the Kolmogorov-Smirnov distance. The parameters for the numerical simulation are fully specified, including matrix dimensions, sample sizes, and random seeds, which ensures the problem is self-contained and the solution is reproducible. There are no logical contradictions, ambiguities, or factual inaccuracies.\n\nThe solution will be obtained through a direct numerical simulation, following the explicit steps outlined in the problem statement. For each test case, characterized by a matrix dimension $N$, a number of matrix realizations $M$, and a pseudorandom number generator seed $s$, we will perform the following systematic procedure.\n\nFirst, we initialize the random number generator using the provided seed $s$. This is crucial for reproducibility of the results.\n\nThe core of the procedure is the generation of a statistical ensemble of eigenvalue spacings. We repeat the following steps $M$ times:\n1.  Construction of a GOE matrix: We generate an $N \\times N$ GOE matrix $H$. The problem gives an efficient constructive definition, $H = (R + R^{\\mathsf{T}})/\\sqrt{2}$, where $R$ is a matrix with entries $R_{ij}$ drawn independently from the standard normal distribution $\\mathcal{N}(0, 1)$. This method correctly produces a real symmetric matrix $H$ where the diagonal elements $H_{ii}$ are distributed as $\\mathcal{N}(0, 2)$ and the off-diagonal elements $H_{ij}$ (for $i \\neq j$) are distributed as $\\mathcal{N}(0, 1)$, satisfying the definition of the GOE.\n2.  Eigenvalue computation: For each generated matrix $H$, we compute its $N$ eigenvalues. Since $H$ is real and symmetric, its eigenvalues are guaranteed to be real. We use a numerically stable algorithm optimized for symmetric matrices, such as that provided by the `numpy.linalg.eigh` function. This function has the added benefit of returning the eigenvalues in ascending order, $\\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_N$.\n3.  Spacing calculation: From the sorted list of eigenvalues, we compute the $N-1$ nearest-neighbor spacings, defined as $s_k = \\lambda_{k+1} - \\lambda_k$ for $k=1, 2, \\ldots, N-1$.\n\nAfter repeating this for all $M$ matrices, we aggregate the resulting $M(N-1)$ spacings into a single dataset.\n\nThe next step is normalization. The theoretical Wigner distribution is predicated on a unit mean spacing. To compare our empirical data against this theory, we must rescale our dataset. We compute the sample mean $\\bar{s}$ of all collected spacings and then normalize each spacing $s_k$ by dividing by this mean: $s'_k = s_k / \\bar{s}$. The resulting set of normalized spacings $\\{s'_k\\}$ now has a sample mean of $1$.\n\nFinally, we quantify the agreement between the empirical distribution of our normalized spacings and the theoretical Wigner surmise using the Kolmogorov-Smirnov (KS) distance, $D$. The KS distance is the maximum absolute difference between the empirical cumulative distribution function, $F_n(s)$, of the data and the theoretical cumulative distribution function, $F_{\\mathrm{W}}(s)$:\n$$\nD = \\sup_{s \\ge 0} \\left| F_n(s) - F_{\\mathrm{W}}(s) \\right|\n$$\nThe CDF for the Wigner surmise is provided:\n$$\nF_{\\mathrm{W}}(s) = 1 - e^{-\\frac{\\pi}{4} s^2}\n$$\nThe empirical CDF, $F_n(s)$, is a step function constructed from the normalized spacings. To compute $D$, we will utilize a standard, robust implementation of the one-sample KS test, available in the `scipy.stats` library. This function, `kstest`, takes as input our sample of normalized spacings and a callable representing the theoretical CDF, $F_{\\mathrm{W}}(s)$, and returns the KS distance $D$.\n\nThis computational procedure is executed for each of the specified test cases. The resulting values of $D$ are then formatted to six decimal places and presented in the required output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For each case, it generates GOE matrices, computes eigenvalue spacings,\n    normalizes them, and calculates the Kolmogorov-Smirnov distance\n    to the Wigner surmise CDF.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2, 10000, 17),    # Case 1: (N, M, seed)\n        (40, 120, 2024),   # Case 2: (N, M, seed)\n        (3, 200, 7),       # Case 3: (N, M, seed)\n    ]\n\n    results = []\n    \n    # Define the Wigner surmise cumulative distribution function (CDF)\n    # for the Gaussian Orthogonal Ensemble (GOE).\n    def F_W(s):\n        return 1.0 - np.exp(-np.pi / 4.0 * s**2)\n\n    for N, M, seed in test_cases:\n        # Initialize the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        all_spacings = []\n\n        # Generate M matrices and collect their eigenvalue spacings.\n        for _ in range(M):\n            # Generate an N x N matrix with i.i.d. standard normal entries.\n            R = rng.standard_normal(size=(N, N))\n            \n            # Construct the symmetric GOE matrix H.\n            H = (R + R.T) / np.sqrt(2.0)\n            \n            # Calculate eigenvalues. eigh is for Hermitian (real-symmetric)\n            # matrices and returns sorted eigenvalues.\n            eigenvalues = np.linalg.eigh(H)[0]\n            \n            # Calculate the nearest-neighbor spacings.\n            spacings = np.diff(eigenvalues)\n            all_spacings.append(spacings)\n\n        # Concatenate all spacings from all matrices into a single array.\n        # This check is for the edge case where M*(N-1) is 0.\n        if not all_spacings:\n            # If no spacings, KS distance is undefined.\n            # As per problem constraints N>=2, so this won't happen.\n            ks_dist = 0.0\n        else:\n            flat_spacings = np.concatenate(all_spacings)\n            \n            # Normalize the spacings to have a unit mean.\n            mean_spacing = np.mean(flat_spacings)\n            if mean_spacing > 0:\n                normalized_spacings = flat_spacings / mean_spacing\n            else:\n                # This case is highly improbable with this generator.\n                # If mean is 0, normalization is not meaningful.\n                # We can skip KS-test as the data is pathological.\n                # We expect positive spacings from sorted distinct eigenvalues.\n                normalized_spacings = flat_spacings\n            \n            # Compute the Kolmogorov-Smirnov distance D between the\n            # empirical distribution of spacings and the Wigner surmise CDF.\n            # The kstest returns the statistic (D) and the p-value.\n            ks_dist, _ = kstest(normalized_spacings, F_W)\n\n        results.append(ks_dist)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of D values rounded to six decimal places.\n    print(f\"[{','.join(f'{d:.6f}' for d in results)}]\")\n\nsolve()\n```", "id": "2387505"}, {"introduction": "The concept of level repulsion, a hallmark of quantum chaos modeled by ensembles like GOE, is not a universal law but a feature of specific systems. This final problem provides a crucial contrast by exploring a hypothetical ensemble of matrices whose elements are drawn from a discrete probability distribution [@problem_id:881592]. By calculating the non-zero probability of exact eigenvalue degeneracy, you will see firsthand how the continuous nature of the matrix element distribution in standard ensembles is essential for the strong level repulsion they exhibit.", "problem": "In random matrix theory, the statistical properties of eigenvalues of matrices with random elements are studied. A central concept is level repulsion, which describes the tendency of eigenvalues to avoid clustering. For many standard ensembles, such as the Gaussian Orthogonal Ensemble (GOE), the probability of finding two exactly equal eigenvalues (a degeneracy) is zero. This problem explores a non-standard ensemble where this is not the case.\n\nConsider the ensemble of $2 \\times 2$ real symmetric matrices of the form:\n$$\nH = \\begin{pmatrix} H_{11} & H_{12} \\\\ H_{12} & H_{22} \\end{pmatrix}\n$$\nThe three unique matrix elements, $H_{11}$, $H_{22}$, and $H_{12}$, are independent random variables. Their values are non-negative integers drawn from the same geometric distribution $P(k;p)$:\n$$\nP(k;p) = p(1-p)^k, \\quad k \\in \\{0, 1, 2, \\ldots\\}\n$$\nwhere $p$ is a parameter satisfying $0 < p < 1$.\n\nCalculate the probability, $P_{\\text{deg}}$, that the matrix $H$ has degenerate eigenvalues. Express your answer as a closed-form analytic expression in terms of $p$.", "solution": "1. The eigenvalues of \n$$H=\\begin{pmatrix}H_{11}&H_{12}\\\\H_{12}&H_{22}\\end{pmatrix}$$ \nare degenerate iff the discriminant vanishes:\n$$\\Delta=(H_{11}-H_{22})^2+4H_{12}^2=0\\;\\Longrightarrow\\;H_{11}=H_{22},\\;H_{12}=0.$$\n2. By independence, \n$$P_{\\rm deg}=P(H_{12}=0)\\,P(H_{11}=H_{22}).$$\n3. From $P(k;p)=p(1-p)^k$,\n$$P(H_{12}=0)=p.$$\n4. Also\n$$P(H_{11}=H_{22})\n=\\sum_{k=0}^\\infty P(H_{11}=k)\\,P(H_{22}=k)\n=\\sum_{k=0}^\\infty p(1-p)^k\\;p(1-p)^k\n=p^2\\sum_{k=0}^\\infty(1-p)^{2k}\n=\\frac{p^2}{1-(1-p)^2}\n=\\frac{p}{2-p}.$$\n5. Hence\n$$P_{\\rm deg}\n=p\\;\\frac{p}{2-p}\n=\\frac{p^2}{2-p}.$$", "answer": "$$\\boxed{\\frac{p^2}{2-p}}$$", "id": "881592"}]}