{"hands_on_practices": [{"introduction": "To build a solid intuition for entropy in dynamical systems, we begin with the simplest case: a system that is entirely predictable. The Kolmogorov-Sinai (KS) entropy quantifies the rate of information generation, so for a non-chaotic system where future states are perfectly determined and nearby trajectories converge, we should expect zero entropy. This exercise [@problem_id:1688745] confirms this fundamental idea by having you calculate the KS entropy for a simple linear contracting map, demonstrating that the absence of complexity corresponds to an entropy of zero.", "problem": "Consider a simple model for a signal attenuator in a discrete-time control system. The state of the signal at any given time, $x$, is represented by a real number in the interval $X = [-1, 1]$. The system evolves according to the linear map $T: X \\to X$ defined by $T(x) = \\frac{x}{3}$. The Kolmogorov-Sinai (KS) entropy, denoted $h_{KS}(T)$, provides a measure of the system's complexity and rate of information generation over time. For this specific attenuator system, calculate the value of the KS entropy $h_{KS}(T)$.", "solution": "The Kolmogorov-Sinai (KS) entropy of a dynamical system quantifies the rate at which the system produces information. For differentiable dynamical systems, the KS entropy is related to the Lyapunov exponents of the system through Pesin's Identity. For a one-dimensional map $T$ on an interval $X$, this identity simplifies. If the system admits an absolutely continuous invariant measure $\\mu$, the KS entropy is given by the integral of the positive part of the local Lyapunov exponent over the phase space:\n$$h_{KS}(T) = \\int_{X} \\max(0, \\lambda(x)) d\\mu(x)$$\nwhere $\\lambda(x)$ is the local Lyapunov exponent at point $x$.\n\nFirst, we need to find the local Lyapunov exponent for the given map, $T(x) = \\frac{x}{3}$. The local Lyapunov exponent is defined in terms of the derivative of the map, $\\lambda(x) = \\ln|T'(x)|$.\n\nLet's calculate the derivative of $T(x)$:\n$$T'(x) = \\frac{d}{dx} \\left(\\frac{x}{3}\\right) = \\frac{1}{3}$$\n\nThe derivative is a constant, $T'(x) = 1/3$, for all $x$ in the interval $[-1, 1]$.\nTherefore, the local Lyapunov exponent is also constant for all $x$:\n$$\\lambda(x) = \\ln\\left|\\frac{1}{3}\\right| = \\ln\\left(\\frac{1}{3}\\right) = -\\ln(3)$$\n\nNow we apply this to Pesin's Identity. We need to evaluate $\\max(0, \\lambda(x))$:\n$$\\max(0, \\lambda(x)) = \\max(0, -\\ln(3))$$\nSince $\\ln(3) > \\ln(1) = 0$, the value $-\\ln(3)$ is negative.\nThus,\n$$\\max(0, -\\ln(3)) = 0$$\n\nThe integrand in the formula for KS entropy is therefore 0 for all $x \\in X$.\nSubstituting this into the integral expression for the KS entropy:\n$$h_{KS}(T) = \\int_{-1}^{1} 0 \\ d\\mu(x) = 0$$\n\nThe KS entropy for this system is 0.\n\nThis result can also be understood intuitively. The map $T(x) = x/3$ is a contracting map because $|T'(x)| = 1/3 < 1$. Under repeated application of this map, any initial point $x_0 \\in [-1, 1]$ will have its trajectory converge to the fixed point at $x=0$. For example, $x_n = T^n(x_0) = x_0/3^n$, which approaches 0 as $n \\to \\infty$. A small interval of initial conditions will shrink in length with each iteration. Since all trajectories converge to a single, predictable point, the system does not exhibit any chaotic behavior or unpredictability. The KS entropy measures the rate of generation of new information (or unpredictability), so for a completely predictable, non-chaotic system like this contracting map, the KS entropy must be zero.", "answer": "$$\\boxed{0}$$", "id": "1688745"}, {"introduction": "Having established our baseline with a predictable system, we now turn to a classic example of chaos: the skew tent map. Unlike contracting maps, chaotic systems exhibit sensitive dependence on initial conditions, meaning they actively generate information over time. This practice [@problem_id:871213] allows you to quantify this complexity by applying Pesin's identity, a cornerstone result connecting the map's average local stretching, given by its derivative, to its KS entropy. This calculation provides a concrete link between the geometric properties of a map and its informational complexity.", "problem": "Consider the one-dimensional skew tent map $T_a: [0, 1] \\to [0, 1]$, defined by the parameter $a \\in (0, 1)$. The map is given by the piecewise linear function:\n$$\nT_a(x) = \\begin{cases}\n\\frac{x}{a} & \\text{if } 0 \\le x \\le a \\\\\n\\frac{1-x}{1-a} & \\text{if } a < x \\le 1\n\\end{cases}\n$$\nThis map is a classic example of a chaotic dynamical system. For any value of the parameter $a$, the map possesses an ergodic invariant measure that is absolutely continuous with respect to the Lebesgue measure. In fact, for the skew tent map, the Lebesgue measure itself (with density $\\rho(x) = 1$ for $x \\in [0, 1]$) is an invariant measure.\n\nThe Kolmogorov-Sinai (KS) entropy, $h_{KS}$, quantifies the rate of information production of a dynamical system. For a one-dimensional map $T(x)$ with an invariant probability density $\\rho(x)$, the KS entropy can be calculated using Pesin's identity:\n$$\nh_{KS}(T) = \\int_0^1 \\ln|T'(x)| \\rho(x) \\, dx\n$$\nwhere $T'(x)$ is the derivative of the map.\n\nYour task is to compute the Kolmogorov-Sinai entropy $h_{KS}(T_a)$ for the skew tent map with respect to the invariant Lebesgue measure. Express your answer as a closed-form analytic expression in terms of the parameter $a$.", "solution": "We use Pesin’s identity for a one‐dimensional map with invariant density $\\rho(x)=1$:\n$$\nh_{KS}(T_a)=\\int_0^1\\ln\\bigl|T_a'(x)\\bigr|\\rho(x)\\,dx\n$$.\nOn the two linear branches,\n$$\nT_a'(x)=\\begin{cases}\n\\dfrac1a,&0\\le x\\le a,\\\\\n-\\dfrac1{1-a},&a<x\\le1.\n\\end{cases}\n$$\nHence\n$$\nh_{KS}(T_a)\n=\\int_0^a\\ln\\!\\Bigl(\\frac1a\\Bigr)\\,dx\n+\\int_a^1\\ln\\!\\Bigl(\\frac1{1-a}\\Bigr)\\,dx\n=a\\ln\\!\\Bigl(\\frac1a\\Bigr)+(1-a)\\ln\\!\\Bigl(\\frac1{1-a}\\Bigr)\n$$.\nUsing $\\ln(1/u)=-\\ln u$ gives\n$$\nh_{KS}(T_a)\n=-\\,a\\ln a\\;-\\;(1-a)\\ln(1-a)\n$$.", "answer": "$$\\boxed{-\\,a\\ln a \\;-\\;(1-a)\\ln(1-a)}$$", "id": "871213"}, {"introduction": "Beyond continuous maps on an interval, entropy provides a powerful lens for analyzing symbolic dynamics, where trajectories are represented as infinite sequences of symbols. This exercise [@problem_id:871313] explores the \"golden mean subshift,\" a system defined by a simple forbidden rule. You will use the transfer matrix method to compute the system's topological entropy, which measures the exponential growth rate of distinct orbits. The problem culminates in using the variational principle, which equates the topological entropy to the KS entropy of a special \"measure of maximal entropy,\" elegantly connecting the combinatorial complexity of the system to its measure-theoretic information production.", "problem": "Consider a symbolic dynamical system defined on the space of bi-infinite sequences of symbols from the alphabet $\\mathcal{A} = \\{0, 1\\}$. The dynamics are governed by a discrete-time shift map $T$, where $(T(x))_i = x_{i+1}$ for a sequence $x = (\\dots, x_{-1}, x_0, x_1, \\dots)$.\n\nWe restrict the space of all possible sequences to a specific subset called a subshift of finite type (SFT). This subset, denoted $\\Sigma_A$, consists of all sequences where only certain transitions between symbols are allowed. The allowed transitions are encoded in a transition matrix $A$, where $A_{ij}=1$ if the transition from symbol $i$ to symbol $j$ is allowed, and $A_{ij}=0$ otherwise. A sequence $x \\in \\Sigma_A$ if and only if $A_{x_i, x_{i+1}} = 1$ for all $i \\in \\mathbb{Z}$.\n\nThe \"golden mean subshift\" is a specific SFT on the alphabet $\\mathcal{A} = \\{0, 1\\}$ defined by the rule that the block of symbols '11' is forbidden.\n\nFor a dynamical system $(X, T)$, the topological entropy, $h_{top}(T)$, measures the exponential growth rate of the number of distinguishable orbits. For an SFT with transition matrix $A$, it is given by $h_{top}(T) = \\ln(\\lambda_{max})$, where $\\lambda_{max}$ is the Perron-Frobenius eigenvalue (the largest positive real eigenvalue) of $A$.\n\nThe Kolmogorov-Sinai (KS) entropy, $h_{\\mu}(T)$, quantifies the average rate of information production for a specific $T$-invariant measure $\\mu$. The variational principle connects these two entropies by the relation $h_{top}(T) = \\sup_{\\mu} h_{\\mu}(T)$, where the supremum is taken over all $T$-invariant probability measures. A measure $\\mu_{MME}$ for which this supremum is attained, i.e., $h_{\\mu_{MME}}(T) = h_{top}(T)$, is called a measure of maximal entropy (MME).\n\nYour task is to compute the Kolmogorov-Sinai (KS) entropy for the measure of maximal entropy associated with the golden mean subshift.", "solution": "1. The transition matrix for the golden mean subshift on $\\{0,1\\}$ (forbidding “11”) is\n$$\nA=\\begin{pmatrix}1&1\\\\1&0\\end{pmatrix}\n$$.\n2. Its Perron–Frobenius eigenvalue $\\lambda_{\\max}$ satisfies the characteristic equation\n$$\n\\det\\bigl(A-\\lambda I\\bigr)\n=\\begin{vmatrix}1-\\lambda&1\\\\1&-\\lambda\\end{vmatrix}\n=(1-\\lambda)(-\\lambda)-1\n=\\lambda^2-\\lambda-1=0\n$$.\nSolving gives\n$$\n\\lambda_{\\max}=\\frac{1+\\sqrt{5}}{2}\n$$,\nthe golden ratio.\n3. The topological entropy of the SFT is\n$$\nh_{\\text{top}}(T)=\\ln(\\lambda_{\\max})\n=\\ln\\!\\biggl(\\frac{1+\\sqrt{5}}{2}\\biggr)\n$$.\nBy the variational principle, the KS entropy of the measure of maximal entropy equals $h_{\\text{top}}(T)$.", "answer": "$$\\boxed{\\ln\\left(\\frac{1+\\sqrt{5}}{2}\\right)}$$", "id": "871313"}]}