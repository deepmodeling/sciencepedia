{"hands_on_practices": [{"introduction": "The Picard-Lindelöf theorem is the bedrock for the local existence and uniqueness of solutions to ordinary differential equations. At its heart lies a constructive proof based on successive approximations. This exercise [@problem_id:1675298] invites you to apply this very method, known as Picard's iteration. By manually computing the first few steps of the approximation sequence for a driven, damped system, you will gain a concrete understanding of how a sequence of functions can be generated to converge towards the true solution of an initial value problem.", "problem": "In a simplified model for the dynamics of a driven, damped system, a quantity of interest $y(t)$ evolves according to the first-order Ordinary Differential Equation (ODE):\n$$ \\frac{dy}{dt} = \\sin(t) - k y(t) $$\nwhere $k$ is a positive real constant representing a damping factor and the $\\sin(t)$ term represents a periodic driving force. The system is known to be at its zero state initially, meaning $y(0) = 0$.\n\nPicard's method of successive approximations provides a way to construct a sequence of functions, $y_n(t)$, that converge to the true solution of this Initial Value Problem (IVP). The process begins with an initial approximation $y_0(t)$ based on the initial condition. Each subsequent approximation is generated by integrating the governing ODE with the previous approximation substituted into it.\n\nYour task is to determine the third Picard approximation, $y_3(t)$, which is the third function generated by the iterative process after the initial guess $y_0(t)$. Express your final answer as an analytic function of $t$ and $k$.", "solution": "We apply Picard’s successive approximations to the IVP $y^{\\prime}(t)=\\sin(t)-k y(t)$ with $y(0)=0$. The iterative scheme is\n$$\ny_{n+1}(t)=y(0)+\\int_{0}^{t}\\left[\\sin(s)-k\\,y_{n}(s)\\right]\\,ds,\n$$\nstarting from $y_{0}(t)=0$.\n\nFirst approximation:\n$$\ny_{1}(t)=\\int_{0}^{t}\\sin(s)\\,ds=1-\\cos(t).\n$$\n\nSecond approximation:\n$$\ny_{2}(t)=\\int_{0}^{t}\\left[\\sin(s)-k\\,y_{1}(s)\\right]ds\n=\\int_{0}^{t}\\sin(s)\\,ds-k\\int_{0}^{t}\\left[1-\\cos(s)\\right]ds,\n$$\n$$\ny_{2}(t)=\\left[1-\\cos(t)\\right]-k\\left[t-\\sin(t)\\right]=1-\\cos(t)-k t+k\\sin(t).\n$$\n\nThird approximation:\n$$\ny_{3}(t)=\\int_{0}^{t}\\left[\\sin(s)-k\\,y_{2}(s)\\right]ds\n=\\int_{0}^{t}\\sin(s)\\,ds-k\\int_{0}^{t}\\left[1-\\cos(s)-k s+k\\sin(s)\\right]ds.\n$$\nCompute the integrals term by term:\n$$\n\\int_{0}^{t}\\sin(s)\\,ds=1-\\cos(t),\\quad \\int_{0}^{t}1\\,ds=t,\\quad \\int_{0}^{t}-\\cos(s)\\,ds=-\\sin(t),\n$$\n$$\n\\int_{0}^{t}-k s\\,ds=-\\frac{k}{2}t^{2},\\quad \\int_{0}^{t}k\\sin(s)\\,ds=k\\left[1-\\cos(t)\\right].\n$$\nTherefore,\n$$\n\\int_{0}^{t}y_{2}(s)\\,ds=t-\\sin(t)-\\frac{k}{2}t^{2}+k\\left[1-\\cos(t)\\right],\n$$\nand\n$$\ny_{3}(t)=\\left[1-\\cos(t)\\right]-k\\left[t-\\sin(t)-\\frac{k}{2}t^{2}+k\\left(1-\\cos(t)\\right)\\right].\n$$\nSimplifying,\n$$\ny_{3}(t)=\\left(1-k^{2}\\right)+\\left(k^{2}-1\\right)\\cos(t)+k\\sin(t)-k t+\\frac{k^{2}}{2}t^{2}\n= k\\sin(t)+\\left(k^{2}-1\\right)\\left[\\cos(t)-1\\right]-k t+\\frac{k^{2}}{2}t^{2}.\n$$\nThis is the third Picard approximation.", "answer": "$$\\boxed{k\\sin(t)+\\left(k^{2}-1\\right)\\left(\\cos(t)-1\\right)-k t+\\frac{k^{2}}{2}t^{2}}$$", "id": "1675298"}, {"introduction": "The existence of a solution is one half of the story; its uniqueness is the other. The Picard-Lindelöf theorem hinges on the function defining the ODE being Lipschitz continuous. But what happens when this condition is violated? This problem [@problem_id:2172748] explores this critical question through the lens of a simple-looking yet profound initial value problem, $\\frac{dy}{dt} = |y|^p$. By investigating how the nature of the solution set changes with the parameter $p$, you will discover why the Lipschitz condition is not just a technical detail, but a fundamental guarantor of determinism in dynamical systems.", "problem": "Consider the initial value problem (IVP) given by the ordinary differential equation\n$$ \\frac{dy}{dt} = |y|^{p} $$\nwith the initial condition $y(0) = 0$. The parameter $p$ is a positive real number, i.e., $p > 0$.\n\nDepending on the value of $p$, the IVP may have a unique solution, or it may have multiple solutions. Determine the condition on the parameter $p$ that guarantees the existence of a unique solution for this IVP in a neighborhood of $t=0$.\n\nWhich of the following statements is correct?\n\nA. The solution is unique if and only if $p > 1$.\n\nB. The solution is unique if and only if $p \\geq 1$.\n\nC. The solution is unique if and only if $0  p  1$.\n\nD. The solution is unique for all $p > 0$.\n\nE. The solution is unique if and only if $p = 1$.", "solution": "We consider the IVP\n$$\n\\frac{dy}{dt} = |y|^{p}, \\quad y(0)=0, \\quad p>0.\n$$\nBy the Picard–Lindelöf theorem, a sufficient condition for local uniqueness is that the right-hand side $f(y)=|y|^{p}$ be locally Lipschitz in $y$ near $y=0$.\n\nFirst, we check local Lipschitz continuity of $f$ near $0$.\n\n- For $p>1$: For $y\\neq 0$, \n$$\nf'(y)=\\frac{d}{dy}\\left(|y|^{p}\\right)=p|y|^{p-1}\\operatorname{sgn}(y).\n$$\nAs $y\\to 0$, $|f'(y)|=p|y|^{p-1}\\to 0$. Hence $f'$ is bounded on a neighborhood of $0$, so $f$ is locally Lipschitz near $0$. Therefore, by Picard–Lindelöf, the IVP has a unique local solution.\n\n- For $p=1$: $f(y)=|y|$ satisfies\n$$\n\\big||y|-|z|\\big|\\leq |y-z|\n$$\nfor all $y,z$, so $f$ is globally Lipschitz with Lipschitz constant $1$. Thus, uniqueness holds.\n\n- For $0p1$: $f$ is not locally Lipschitz at $0$. Indeed,\n$$\n\\frac{\\big||y|^{p}-0\\big|}{|y-0|}=|y|^{p-1}\\to \\infty \\quad \\text{as } y\\to 0,\n$$\nso no finite Lipschitz constant can hold near $0$. Moreover, nonuniqueness actually occurs. Besides the trivial solution $y(t)\\equiv 0$, another solution is given by\n$$\ny(t)=\n\\begin{cases}\n0,  t\\leq 0,\\\\\n\\left((1-p)t\\right)^{\\frac{1}{1-p}},  t\\geq 0,\n\\end{cases}\n$$\nwhich satisfies $y(0)=0$ and, for $t0$,\n$$\n\\frac{dy}{dt}=\\frac{1}{1-p}(1-p)\\left((1-p)t\\right)^{\\frac{1}{1-p}-1}=\\left((1-p)t\\right)^{\\frac{p}{1-p}}=y(t)^{p}=|y(t)|^{p}.\n$$\nAt $t=0$, both sides are $0$, so the ODE holds. More generally, for any $T\\geq 0$,\n$$\ny_{T}(t)=\n\\begin{cases}\n0,  t\\leq T,\\\\\n\\left((1-p)(t-T)\\right)^{\\frac{1}{1-p}},  t\\geq T,\n\\end{cases}\n$$\nis also a solution with $y_{T}(0)=0$, showing infinitely many solutions. Hence uniqueness fails for $0p1$.\n\nCombining these cases, the IVP has a unique local solution if and only if $p\\geq 1$. Therefore, among the options, the correct statement is B.", "answer": "$$\\boxed{B}$$", "id": "2172748"}, {"introduction": "While linear ODEs have solutions that exist for all time, the nonlinear world is full of surprises, one of which is the phenomenon of finite-time blow-up. A solution that starts from a perfectly finite initial condition can escape to infinity in a finite amount of time, a behavior with profound implications for modeling physical systems. This practice problem [@problem_id:872278] provides a classic example, asking you to connect an initial condition directly to the lifespan of its solution. By solving this, you will gain tangible insight into how the local existence guaranteed by the theorem can have a strictly finite temporal domain.", "problem": "In the study of nonlinear ordinary differential equations (ODEs), one of the key distinctions from linear theory is the possibility of finite-time blow-up. While solutions to linear ODEs with continuous coefficients are guaranteed to exist for all time, solutions to nonlinear ODEs may diverge to infinity at a finite time, even if the vector field is smooth. This phenomenon is critical for understanding the domain of existence and uniqueness of solutions.\n\nConsider the following initial value problem (IVP) for a state variable $x(t) \\in \\mathbb{R}$:\n$$\n\\frac{dx}{dt} = x^3\n$$\nwith the initial condition $x(0) = x_0$, where $x_0$ is a positive real constant.\n\nThe solution $x(t)$ to this IVP is said to \"blow up\" at a finite time $t=T  0$ if $\\lim_{t \\to T^-} x(t) = \\infty$. Your task is to find the specific value of the initial condition $x_0 > 0$ for which the solution blows up at exactly time $T=2$.", "solution": "We solve the IVP \n$$\\frac{dx}{dt}=x^3,\\quad x(0)=x_00.$$\n1. Separate variables:\n$$\\int x^{-3}\\,dx=\\int dt.$$\n2. Perform the integrals:\n$$-\\,\\frac1{2x^2}=t+C.$$\n3. Impose the initial condition $x(0)=x_0$:\n$$-\\frac1{2x_0^2}=C.$$\nHence\n$$-\\,\\frac1{2x^2}=t-\\frac1{2x_0^2}.$$\n4. Multiply by $-2$ and solve for $x^2$:\n$$\\frac1{x^2}=-2t+\\frac1{x_0^2}\n\\quad\\Longrightarrow\\quad\nx^2=\\frac1{\\frac1{x_0^2}-2t}.$$\n5. Thus the solution is\n$$x(t)=\\frac1{\\sqrt{\\frac1{x_0^2}-2t}}.$$\n6. Blow-up occurs when the denominator vanishes at $t=T$:\n$$\\frac1{x_0^2}-2T=0\n\\quad\\Longrightarrow\\quad\nx_0^2=\\frac1{2T}\n\\quad\\Longrightarrow\\quad\nx_0=\\frac1{\\sqrt{2T}}.$$\n7. For blow-up time $T=2$,\n$$x_0=\\frac1{\\sqrt{2\\cdot2}}=\\frac1{2}.$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "872278"}]}