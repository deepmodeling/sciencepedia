{"hands_on_practices": [{"introduction": "In information geometry, families of probability distributions are treated as points on a curved manifold. This exercise [@problem_id:132036] invites you to explore this fascinating connection by calculating the total arc length of the manifold of Bernoulli distributions, using the Fisher information metric to define distance. This practice provides a concrete example of how abstract geometric concepts can quantify the \"statistical distance\" between different states of a system.", "problem": "In the field of information geometry, a parametric family of probability distributions can be viewed as a Riemannian manifold, where the metric tensor is given by the Fisher information metric. This metric quantifies the \"statistical distance\" between nearby distributions.\n\nConsider the family of Bernoulli distributions, which is fundamental in probability theory and quantum computation (as it describes a qubit measurement in a fixed basis). A Bernoulli random variable $X$ takes the value $k=1$ with probability $p$ and the value $k=0$ with probability $1-p$. The parameter $p$ can take any value in the open interval $(0, 1)$. The probability mass function is given by:\n$$ P(k; p) = p^k (1-p)^{1-k}, \\quad k \\in \\{0, 1\\} $$\nThe set of all Bernoulli distributions forms a one-dimensional statistical manifold parameterized by the coordinate $p$.\n\nFor a one-dimensional manifold parameterized by a single parameter $p$, the Fisher information metric has a single component, $g_{pp}(p)$, which is defined as:\n$$ g_{pp}(p) = \\sum_{k} P(k; p) \\left( \\frac{\\partial \\ln P(k; p)}{\\partial p} \\right)^2 $$\nwhere the sum is over all possible outcomes $k$ of the random variable.\n\nThe total arc length $L$ of this one-dimensional manifold, which represents the total statistical distance between the deterministic outcomes $p=0$ and $p=1$, is given by the integral of the arc length element $ds = \\sqrt{g_{pp}(p)} \\, dp$ over the entire range of the parameter $p$:\n$$ L = \\int_{0}^{1} \\sqrt{g_{pp}(p)} \\, dp $$\n\nCalculate the total arc length $L$ of the statistical manifold of Bernoulli distributions.", "solution": "1. The Fisher information metric for a one-dimensional parameter $p$ is\n$$\ng_{pp}(p)\n=\\sum_{k=0}^{1}P(k;p)\\Bigl(\\frac{\\partial}{\\partial p}\\ln P(k;p)\\Bigr)^2.\n$$\nFor the Bernoulli distribution,\n$$\nP(1;p)=p,\\quad P(0;p)=1-p,\n\\quad\n\\frac{\\partial}{\\partial p}\\ln P(1;p)=\\frac1p,\n\\quad\n\\frac{\\partial}{\\partial p}\\ln P(0;p)=-\\frac1{1-p}.\n$$\n\n2. Compute $g_{pp}(p)$:\n$$\ng_{pp}(p)\n=p\\Bigl(\\frac1p\\Bigr)^2+(1-p)\\Bigl(-\\frac1{1-p}\\Bigr)^2\n=\\frac1p+\\frac1{1-p}\n=\\frac{1}{p(1-p)}.\n$$\n\n3. The total arc length is\n$$\nL=\\int_{0}^{1}\\sqrt{g_{pp}(p)}\\,dp\n=\\int_{0}^{1}\\frac{1}{\\sqrt{p(1-p)}}\\,dp.\n$$\nRecognize the Beta integral:\n$$\n\\int_{0}^{1}p^{-1/2}(1-p)^{-1/2}\\,dp\n=B\\bigl(\\tfrac12,\\tfrac12\\bigr)\n=\\frac{\\Gamma(\\frac12)\\,\\Gamma(\\frac12)}{\\Gamma(1)}\n=\\pi.\n$$\n\n4. Therefore, the total statistical distance is\n$$\nL=\\pi.\n$$", "answer": "$$\\boxed{\\pi}$$", "id": "132036"}, {"introduction": "Understanding how entropy behaves under transformations is a core skill in information theory. This problem [@problem_id:132200] explores this by examining a random variable defined on a finite field $\\mathbb{F}_{p^n}$ that is transformed by the absolute trace map. You will discover how the algebraic properties of this map, specifically its nature as a surjective linear functional, lead to a surprisingly simple and elegant result for the entropy of the output variable.", "problem": "Let $p$ be a prime number and $n$ be a positive integer. Consider the finite field $\\mathbb{F}_{p^n}$ with $p^n$ elements. The absolute trace map from $\\mathbb{F}_{p^n}$ to its prime subfield $\\mathbb{F}_p$ is a function $\\text{Tr}: \\mathbb{F}_{p^n} \\to \\mathbb{F}_p$ defined for any $x \\in \\mathbb{F}_{p^n}$ as\n$$\n\\text{Tr}(x) = x + x^p + x^{p^2} + \\dots + x^{p^{n-1}}.\n$$\nThe result of this sum is always an element of the prime subfield $\\mathbb{F}_p = \\{0, 1, \\dots, p-1\\}$.\n\nLet $X$ be a random variable that is uniformly distributed over the elements of $\\mathbb{F}_{p^n}$. This means that for any element $\\alpha \\in \\mathbb{F}_{p^n}$, the probability of $X$ taking that value is $P(X=\\alpha) = 1/p^n$.\n\nA new random variable $Y$ is defined by applying the trace map to $X$, i.e., $Y = \\text{Tr}(X)$. The possible values of $Y$ are the elements of $\\mathbb{F}_p$.\n\nThe Shannon entropy of a discrete random variable $Z$ with probability mass function $P(Z=z_i) = p_i$ is given by $H(Z) = -\\sum_i p_i \\log_b(p_i)$. The choice of base $b$ for the logarithm determines the units of entropy.\n\nCompute the Shannon entropy $H(Y)$ of the random variable $Y$ in units of bits (i.e., using logarithm base 2). The final answer should be an expression in terms of $p$ and/or $n$.", "solution": "The Shannon entropy of the random variable $Y$ is given by the formula:\n$$\nH(Y) = -\\sum_{y \\in \\mathbb{F}_p} P(Y=y) \\log_2(P(Y=y))\n$$\nThe random variable $Y$ is defined as $Y=\\text{Tr}(X)$, where $X$ is uniformly distributed over $\\mathbb{F}_{p^n}$. The probability of $Y$ taking a specific value $y \\in \\mathbb{F}_p$ is the sum of probabilities of all $x \\in \\mathbb{F}_{p^n}$ such that $\\text{Tr}(x)=y$. Since $X$ is uniform, $P(X=x) = 1/p^n$ for all $x \\in \\mathbb{F}_{p^n}$. Thus, the probability mass function for $Y$ is:\n$$\nP(Y=y) = \\sum_{x \\in \\mathbb{F}_{p^n}, \\text{Tr}(x)=y} P(X=x) = \\sum_{x \\in \\mathbb{F}_{p^n}, \\text{Tr}(x)=y} \\frac{1}{p^n} = \\frac{|\\{x \\in \\mathbb{F}_{p^n} \\mid \\text{Tr}(x)=y\\}|}{p^n}\n$$\nwhere $|\\{x \\in \\mathbb{F}_{p^n} \\mid \\text{Tr}(x)=y\\}|$ is the number of elements in $\\mathbb{F}_{p^n}$ whose trace is equal to $y$. Let's denote this quantity by $N_y$. To compute $H(Y)$, we first need to determine the distribution of $Y$, which means finding $N_y$ for each $y \\in \\mathbb{F}_p$.\n\nWe can view the finite field $\\mathbb{F}_{p^n}$ as an $n$-dimensional vector space over its prime subfield $\\mathbb{F}_p$. The trace map $\\text{Tr}: \\mathbb{F}_{p^n} \\to \\mathbb{F}_p$ is an $\\mathbb{F}_p$-linear map. This can be verified from its definition: For $a, b \\in \\mathbb{F}_p$ and $x, z \\in \\mathbb{F}_{p^n}$, we have $\\text{Tr}(ax+bz) = \\sum_{i=0}^{n-1} (ax+bz)^{p^i} = \\sum_{i=0}^{n-1} (a^{p^i}x^{p^i} + b^{p^i}z^{p^i})$. Since $a, b \\in \\mathbb{F}_p$, they are fixed by the Frobenius automorphism, i.e., $a^{p^i} = a$ and $b^{p^i} = b$. Thus, $\\text{Tr}(ax+bz) = a\\sum x^{p^i} + b\\sum z^{p^i} = a\\text{Tr}(x) + b\\text{Tr}(z)$.\n\nThe trace map is a non-zero linear functional. A key property of the trace map is that it is surjective, meaning its image, $\\text{im}(\\text{Tr})$, is the entire field $\\mathbb{F}_p$. As a vector space over itself, $\\dim_{\\mathbb{F}_p}(\\mathbb{F}_p) = 1$.\n\nAccording to the rank-nullity theorem for linear maps, the dimension of the domain is the sum of the dimensions of the kernel and the image:\n$$\n\\dim_{\\mathbb{F}_p}(\\mathbb{F}_{p^n}) = \\dim_{\\mathbb{F}_p}(\\ker(\\text{Tr})) + \\dim_{\\mathbb{F}_p}(\\text{im}(\\text{Tr}))\n$$\nWe have $\\dim_{\\mathbb{F}_p}(\\mathbb{F}_{p^n}) = n$ and $\\dim_{\\mathbb{F}_p}(\\text{im}(\\text{Tr})) = \\dim_{\\mathbb{F}_p}(\\mathbb{F}_p) = 1$.\nTherefore, the dimension of the kernel of the trace map is:\n$$\n\\dim_{\\mathbb{F}_p}(\\ker(\\text{Tr})) = n - 1\n$$\nThe kernel, $\\ker(\\text{Tr}) = \\{x \\in \\mathbb{F}_{p^n} \\mid \\text{Tr}(x)=0\\}$, is a subspace of dimension $n-1$. The number of elements in a vector space of dimension $d$ over $\\mathbb{F}_p$ is $p^d$. Thus, the number of elements with trace 0 is:\n$$\nN_0 = |\\ker(\\text{Tr})| = p^{n-1}\n$$\nNow we need to find $N_y$ for $y \\neq 0$. Since the trace map is surjective, for any $y \\in \\mathbb{F}_p$, there exists at least one element $x_y \\in \\mathbb{F}_{p^n}$ such that $\\text{Tr}(x_y) = y$. The set of all elements with trace $y$, $\\{x \\in \\mathbb{F}_{p^n} \\mid \\text{Tr}(x)=y\\}$, is a fiber of the linear map. For any $x$ in this fiber, we can write $x = x_y + (x - x_y)$. By linearity, $\\text{Tr}(x-x_y) = \\text{Tr}(x) - \\text{Tr}(x_y) = y - y = 0$. This means $x - x_y \\in \\ker(\\text{Tr})$.\nTherefore, the set of elements with trace $y$ is the coset $x_y + \\ker(\\text{Tr}) = \\{x_y + k \\mid k \\in \\ker(\\text{Tr})\\}$. All cosets of a subspace have the same cardinality as the subspace itself. So, for any $y \\in \\mathbb{F}_p$:\n$$\nN_y = |x_y + \\ker(\\text{Tr})| = |\\ker(\\text{Tr})| = p^{n-1}\n$$\nThis shows that every element $y \\in \\mathbb{F}_p$ is the trace of exactly $p^{n-1}$ elements from $\\mathbb{F}_{p^n}$.\n\nNow we can calculate the probability $P(Y=y)$:\n$$\nP(Y=y) = \\frac{N_y}{p^n} = \\frac{p^{n-1}}{p^n} = \\frac{1}{p}\n$$\nThis is true for all $y \\in \\mathbb{F}_p$. This means that the random variable $Y$ is uniformly distributed over the $p$ elements of $\\mathbb{F}_p$.\n\nFinally, we compute the Shannon entropy of this uniform distribution in bits (logarithm base 2):\n$$\nH(Y) = -\\sum_{y \\in \\mathbb{F}_p} P(Y=y) \\log_2(P(Y=y)) = -\\sum_{y=0}^{p-1} \\frac{1}{p} \\log_2\\left(\\frac{1}{p}\\right)\n$$\nSince there are $p$ identical terms in the sum:\n$$\nH(Y) = -p \\cdot \\left(\\frac{1}{p} \\log_2\\left(\\frac{1}{p}\\right)\\right) = -\\log_2\\left(\\frac{1}{p}\\right)\n$$\nUsing the logarithm property $\\log(a^{-1}) = -\\log(a)$:\n$$\nH(Y) = -(-\\log_2(p)) = \\log_2(p)\n$$\nThe Shannon entropy of $Y$ is $\\log_2(p)$ bits. Notably, the result is independent of the degree $n$ of the field extension.", "answer": "$\\boxed{\\log_2(p)}$", "id": "132200"}, {"introduction": "The familiar properties of Shannon entropy, such as the non-negativity of conditional mutual information, are not universally guaranteed for all entropy-like functions. This advanced problem [@problem_id:132051] asks you to compute a conditional mutual information for an \"entropy\" function constructed from the rank of a V치mos matroid. This exercise is a powerful demonstration that there exist mathematical structures that violate the strong subadditivity inequality, pushing you to think critically about the fundamental axioms of information theory.", "problem": "In the study of generalized information theories, one may encounter entropy-like functions that do not satisfy all the properties of the standard Shannon entropy. A key property of Shannon entropy is strong subadditivity, which ensures that the conditional mutual information $I(A;B|C)$ is always non-negative. This problem explores a scenario where this property is violated.\n\nConsider a set of eight random variables $X_1, X_2, \\dots, X_8$. The \"entropy\" of a subset of these variables, indexed by a set $S \\subseteq E = \\{1, 2, \\dots, 8\\}$, is given by a function $H(S)$. This function is constructed from the rank function $r(S)$ of the V치mos matroid $V_8$, with an added perturbation.\n\nThe rank function $r(S)$ of the V치mos matroid $V_8$ on the ground set $E$ is defined as follows. First, identify the five \"special\" 4-element subsets, which are designated as dependent:\n$$\n\\mathcal{B}_{dep} = \\{\\{1,2,5,6\\}, \\{1,2,7,8\\}, \\{3,4,5,6\\}, \\{3,4,7,8\\}, \\{1,3,5,7\\}\\}\n$$\nThe rank $r(S)$ for any $S \\subseteq E$ is determined by the size of the largest independent subset of $S$. This leads to the following rules:\n-   If $|S| \\le 3$, $r(S) = |S|$.\n-   If $|S| = 4$ and $S \\in \\mathcal{B}_{dep}$, $r(S) = 3$.\n-   If $|S| = 4$ and $S \\notin \\mathcal{B}_{dep}$, $r(S) = 4$.\n-   If $|S| \\ge 5$, $r(S) = 4$.\n\nThe entropy-like function $H(S)$ for this problem is defined as a perturbation of this rank function:\n$$\nH(S) = r(S) + 3 \\cdot \\delta_{S, \\{1,2,3,5,6,7\\}}\n$$\nwhere $\\delta_{S,T}$ is the Kronecker delta, which is 1 if $S=T$ and 0 otherwise.\n\nThe conditional mutual information between two sets of variables $X_A$ and $X_B$, given a third set $X_C$, is defined as:\n$$\nI(X_A; X_B | X_C) = H(A \\cup C) + H(B \\cup C) - H(A \\cup B \\cup C) - H(C)\n$$\nwhere $A$, $B$, and $C$ are the index sets for the variables.\n\nDerive the value of the conditional mutual information $I(X_{1,5}; X_{2,6} | X_{3,7})$.", "solution": "To compute the conditional mutual information $I(X_{1,5}; X_{2,6} | X_{3,7})$, use the definition:\n\n$$\nI(X_A; X_B | X_C) = H(A \\cup C) + H(B \\cup C) - H(A \\cup B \\cup C) - H(C)\n$$\n\nwhere $A = \\{1,5\\}$, $B = \\{2,6\\}$, and $C = \\{3,7\\}$. Thus:\n- $A \\cup C = \\{1,5\\} \\cup \\{3,7\\} = \\{1,3,5,7\\}$\n- $B \\cup C = \\{2,6\\} \\cup \\{3,7\\} = \\{2,3,6,7\\}$\n- $A \\cup B \\cup C = \\{1,5\\} \\cup \\{2,6\\} \\cup \\{3,7\\} = \\{1,2,3,5,6,7\\}$\n- $C = \\{3,7\\}$\n\nThe entropy function is $H(S) = r(S) + 3 \\cdot \\delta_{S, \\{1,2,3,5,6,7\\}}$, with $r(S)$ being the rank function of the V치mos matroid $V_8$.\n\nCompute each entropy term:\n1. For $H(\\{1,3,5,7\\})$:\n   - The set $\\{1,3,5,7\\}$ has size 4 and is in $\\mathcal{B}_{dep}$ (given as $\\{1,3,5,7\\}$ is dependent), so $r(\\{1,3,5,7\\}) = 3$.\n   - $\\delta_{S, \\{1,2,3,5,6,7\\}} = 0$ since $\\{1,3,5,7\\} \\neq \\{1,2,3,5,6,7\\}$.\n   - Thus, $H(\\{1,3,5,7\\}) = 3 + 3 \\cdot 0 = 3$.\n\n2. For $H(\\{2,3,6,7\\})$:\n   - The set $\\{2,3,6,7\\}$ has size 4 and is not in $\\mathcal{B}_{dep}$ (as $\\mathcal{B}_{dep}$ contains $\\{1,2,5,6\\}, \\{1,2,7,8\\}, \\{3,4,5,6\\}, \\{3,4,7,8\\}, \\{1,3,5,7\\}$; $\\{2,3,6,7\\}$ is absent), so $r(\\{2,3,6,7\\}) = 4$.\n   - $\\delta_{S, \\{1,2,3,5,6,7\\}} = 0$ since $\\{2,3,6,7\\} \\neq \\{1,2,3,5,6,7\\}$.\n   - Thus, $H(\\{2,3,6,7\\}) = 4 + 3 \\cdot 0 = 4$.\n\n3. For $H(\\{1,2,3,5,6,7\\})$:\n   - The set $\\{1,2,3,5,6,7\\}$ has size 6, and for $|S| \\geq 5$, $r(S) = 4$.\n   - $\\delta_{S, \\{1,2,3,5,6,7\\}} = 1$ since the sets are equal.\n   - Thus, $H(\\{1,2,3,5,6,7\\}) = 4 + 3 \\cdot 1 = 7$.\n\n4. For $H(\\{3,7\\})$:\n   - The set $\\{3,7\\}$ has size 2, and for $|S| \\leq 3$, $r(S) = |S|$, so $r(\\{3,7\\}) = 2$.\n   - $\\delta_{S, \\{1,2,3,5,6,7\\}} = 0$ since $\\{3,7\\} \\neq \\{1,2,3,5,6,7\\}$.\n   - Thus, $H(\\{3,7\\}) = 2 + 3 \\cdot 0 = 2$.\n\nSubstitute into the conditional mutual information formula:\n\n$$\nI(X_{1,5}; X_{2,6} | X_{3,7}) = H(\\{1,3,5,7\\}) + H(\\{2,3,6,7\\}) - H(\\{1,2,3,5,6,7\\}) - H(\\{3,7\\}) = 3 + 4 - 7 - 2\n$$\n\nSimplify the expression:\n\n$$\n3 + 4 = 7, \\quad 7 - 7 = 0, \\quad 0 - 2 = -2\n$$\n\nThus, the result is $-2$.", "answer": "$$\\boxed{-2}$$", "id": "132051"}]}