{"hands_on_practices": [{"introduction": "A primary application of distance measures in information theory is to quantify the distinguishability of signals after they have passed through a noisy channel. This exercise provides an opportunity to apply the Jensen-Shannon Divergence (JSD), a symmetrized and bounded version of the Kullback-Leibler divergence, to a canonical channel model. By calculating the JSD between the output distributions of a Binary Symmetric Channel for its two possible inputs, you will gain a concrete understanding of how a channel's crossover probability $p$ directly impacts the distinguishability of its outputs. [@problem_id:69258]", "problem": "In classical information theory, the distinguishability of different inputs to a communication channel can be quantified by measuring the distance between the corresponding output probability distributions. One such measure is the Jensen-Shannon divergence.\n\nA Binary Symmetric Channel (BSC) is a simple communication model with a binary input alphabet $\\mathcal{X}=\\{0, 1\\}$ and a binary output alphabet $\\mathcal{Y}=\\{0, 1\\}$. The channel is characterized by a single parameter, the crossover probability $p \\in [0, 1]$, which is the probability that a bit is flipped during transmission. The conditional probabilities are given by:\n$P(Y=0|X=0) = 1-p$\n$P(Y=1|X=0) = p$\n$P(Y=0|X=1) = p$\n$P(Y=1|X=1) = 1-p$\n\nThe Jensen-Shannon Divergence (JSD) between two probability distributions $P=\\{p_i\\}$ and $Q=\\{q_i\\}$ over the same alphabet is defined as:\n$$ JSD(P || Q) = H\\left(\\frac{P+Q}{2}\\right) - \\frac{1}{2}H(P) - \\frac{1}{2}H(Q) $$\nwhere $M = \\frac{P+Q}{2}$ is the mixture distribution, and $H(P) = -\\sum_i p_i \\log_2(p_i)$ is the Shannon entropy in bits. The logarithm is taken to base 2.\n\nLet $P_0$ be the probability distribution of the output $Y$ when the input is $X=0$, and let $P_1$ be the probability distribution of the output $Y$ when the input is $X=1$.\n\nCalculate the Jensen-Shannon divergence $JSD(P_0 || P_1)$ as a function of the crossover probability $p$.", "solution": "The output distributions for the Binary Symmetric Channel are:\n- $P_0$: $Y$ when $X=0$, so $P_0(0) = 1-p$ and $P_0(1) = p$\n- $P_1$: $Y$ when $X=1$, so $P_1(0) = p$ and $P_1(1) = 1-p$\n\nThe Jensen-Shannon Divergence is defined as:\n$$\nJSD(P_0 \\parallel P_1) = H\\left(M\\right) - \\frac{1}{2}H(P_0) - \\frac{1}{2}H(P_1)\n$$\nwhere $M = \\frac{P_0 + P_1}{2}$ is the mixture distribution.\n\nCompute $M$:\n- For outcome $0$: $M(0) = \\frac{P_0(0) + P_1(0)}{2} = \\frac{(1-p) + p}{2} = \\frac{1}{2}$\n- For outcome $1$: $M(1) = \\frac{P_0(1) + P_1(1)}{2} = \\frac{p + (1-p)}{2} = \\frac{1}{2}$\nThus, $M$ is uniform: $M(0) = \\frac{1}{2}$, $M(1) = \\frac{1}{2}$.\n\nEntropy of $M$:\n$$\nH(M) = -\\sum_{y \\in \\{0,1\\}} M(y) \\log_2 M(y) = -\\left[ \\frac{1}{2} \\log_2 \\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\log_2 \\left(\\frac{1}{2}\\right) \\right]\n$$\nSince $\\log_2(1/2) = -1$:\n$$\nH(M) = -\\left[ \\frac{1}{2} \\cdot (-1) + \\frac{1}{2} \\cdot (-1) \\right] = -\\left[ -\\frac{1}{2} - \\frac{1}{2} \\right] = -\\left[ -1 \\right] = 1\n$$\n\nEntropy of $P_0$:\n$$\nH(P_0) = -\\sum_{y \\in \\{0,1\\}} P_0(y) \\log_2 P_0(y) = -\\left[ (1-p) \\log_2 (1-p) + p \\log_2 p \\right]\n$$\n\nEntropy of $P_1$:\n$$\nH(P_1) = -\\sum_{y \\in \\{0,1\\}} P_1(y) \\log_2 P_1(y) = -\\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right] = H(P_0)\n$$\nThus, $H(P_0) = H(P_1) = - \\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right]$.\n\nSubstitute into the JSD formula:\n$$\nJSD(P_0 \\parallel P_1) = 1 - \\frac{1}{2} \\left( - \\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right] \\right) - \\frac{1}{2} \\left( - \\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right] \\right)\n$$\nSimplify:\n$$\nJSD(P_0 \\parallel P_1) = 1 + \\frac{1}{2} \\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right] + \\frac{1}{2} \\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right]\n$$\nCombine terms:\n$$\nJSD(P_0 \\parallel P_1) = 1 + \\left[ p \\log_2 p + (1-p) \\log_2 (1-p) \\right]\n$$\n\nNote: By convention, $0 \\log_2 0 = 0$ at $p=0$ and $p=1$, so the expression is defined for all $p \\in [0,1]$.", "answer": "$$ \\boxed{1 + p \\log_2 p + (1 - p) \\log_2 (1 - p)} $$", "id": "69258"}, {"introduction": "A cornerstone of information theory is the Data Processing Inequality, which states that processing information cannot increase the distinguishability between two distributions. This principle can be made quantitative through the concept of a contraction coefficient, which measures the maximum possible reduction in divergence caused by a channel. This practice challenges you to compute this exact coefficient for the $\\chi^2$-divergence under the action of a Binary Erasure Channel, moving beyond the qualitative statement of the inequality to a precise, quantitative understanding of information loss. [@problem_id:69124]", "problem": "In information theory, distance measures quantify the difference between probability distributions. One such measure is the **chi-squared divergence** ($\\chi^2$-divergence). For two discrete probability distributions $P = \\{p_x\\}_{x \\in \\mathcal{X}}$ and $Q = \\{q_x\\}_{x \\in \\mathcal{X}}$ defined on the same finite alphabet $\\mathcal{X}$, the $\\chi^2$-divergence of $P$ from $Q$ is given by:\n$$\n\\chi^2(P || Q) = \\sum_{x \\in \\mathcal{X}} \\frac{(p_x - q_x)^2}{q_x}\n$$\nThis divergence is defined for pairs $(P,Q)$ where the support of $P$ is a subset of the support of $Q$.\n\nA noisy communication channel can be described by a stochastic map $W$ that transforms an input probability distribution $P$ on an input alphabet $\\mathcal{X}$ into an output probability distribution $P' = W(P)$ on an output alphabet $\\mathcal{Y}$. A fundamental property of information-theoretic distances is that they are non-increasing under the action of such channels. This is known as the **Data Processing Inequality**. The **contraction coefficient** $\\eta_D(W)$ for a divergence $D$ and a channel $W$ quantifies the maximum possible shrinkage of the distance:\n$$\n\\eta_D(W) = \\sup_{P, Q : P \\neq Q} \\frac{D(W(P) || W(Q))}{D(P || Q)}\n$$\n\nConsider a **Binary Erasure Channel** (BEC) with erasure probability $\\epsilon \\in [0, 1]$. The input alphabet is $\\mathcal{X} = \\{0, 1\\}$, and the output alphabet is $\\mathcal{Y} = \\{0, 1, e\\}$. The channel transmits the input bit correctly with probability $1-\\epsilon$, and with probability $\\epsilon$, it replaces the bit with an \"erasure\" symbol $e$. The channel is defined by the conditional probabilities $W(y|x)$ for $x \\in \\mathcal{X}, y \\in \\mathcal{Y}$:\n- $W(0|0) = 1-\\epsilon$\n- $W(1|1) = 1-\\epsilon$\n- $W(e|0) = \\epsilon$\n- $W(e|1) = \\epsilon$\n- All other conditional probabilities are zero.\n\nYour task is to compute the exact contraction coefficient, $\\eta_{\\chi^2}(W_{BEC})$, for the $\\chi^2$-divergence under the action of the Binary Erasure Channel. Express your answer as a function of the erasure probability $\\epsilon$.", "solution": "1. Definitions and setup  \nLet $P=(p,1-p)$ and $Q=(q,1-q)$ on $\\{0,1\\}$. Under the BEC with erasure probability $\\epsilon$, the outputs on $\\{0,1,e\\}$ are  \n$$\nW(P)=(\\,(1-\\epsilon)p,\\;(1-\\epsilon)(1-p),\\;\\epsilon\\,), \n\\quad\nW(Q)=(\\,(1-\\epsilon)q,\\;(1-\\epsilon)(1-q),\\;\\epsilon\\,).\n$$\n\n2. Compute $\\chi^2(P\\|Q)$  \n$$\n\\chi^2(P\\|Q)\n=\\sum_{x\\in\\{0,1\\}}\\frac{(P(x)-Q(x))^2}{Q(x)}\n=\\frac{(p-q)^2}{q}+\\frac{((1-p)-(1-q))^2}{1-q}\n=(p-q)^2\\Bigl(\\frac1q+\\frac1{1-q}\\Bigr)\n=\\frac{(p-q)^2}{q(1-q)}.\n$$\n\n3. Compute $\\chi^2(W(P)\\|W(Q))$  \nLet $\\Delta=p-q$.  Then\n$$\nW(P)(0)-W(Q)(0)=(1-\\epsilon)\\Delta,\\quad\nW(P)(1)-W(Q)(1)=-(1-\\epsilon)\\Delta,\\quad\nW(P)(e)-W(Q)(e)=0.\n$$\nThus\n\\begin{align*}\n\\chi^2(W(P)\\|W(Q))\n&=\\frac{((1-\\epsilon)\\Delta)^2}{(1-\\epsilon)q}\n +\\frac{((1-\\epsilon)\\Delta)^2}{(1-\\epsilon)(1-q)}\n = (1-\\epsilon)\\Delta^2\\Bigl(\\frac1q+\\frac1{1-q}\\Bigr)\\\\\n&=\\;(1-\\epsilon)\\frac{\\Delta^2}{q(1-q)}\n=\\;(1-\\epsilon)\\,\\chi^2(P\\|Q).\n\\end{align*}\n\n4. Contraction coefficient  \nSince the ratio is independent of $p,q$ (with $P\\neq Q$),\n$$\n\\eta_{\\chi^2}(W_{BEC})\n=\\sup_{P\\neq Q}\\frac{\\chi^2(W(P)\\|W(Q))}{\\chi^2(P\\|Q)}\n=1-\\epsilon.\n$$", "answer": "$$\\boxed{1-\\epsilon}$$", "id": "69124"}, {"introduction": "Distance measures provide a powerful geometric framework for understanding probabilistic models, allowing us to think in terms of \"projections\" onto simpler families of distributions. This exercise explores this concept by asking you to find the \"closest\" product distribution to a given correlated bivariate normal distribution, where closeness is measured by the Kullback-Leibler (KL) divergence. The beautiful result you will uncover is that this minimum distance is exactly the mutual information between the variables, elegantly connecting the geometric concept of an information projection to a fundamental measure of statistical dependence. [@problem_id:69220]", "problem": "Consider a bivariate random variable $(X,Y)$ following a bivariate normal distribution $P(x,y)$ with zero means, unit variances, and a correlation coefficient $\\rho$, where $-1 < \\rho < 1$. The probability density function is given by:\n$$ p(x, y; \\rho) = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2(1-\\rho^2)}(x^2 - 2\\rho xy + y^2)\\right) $$\nLet $\\mathcal{M}$ be the manifold of all product distributions on $\\mathbb{R}^2$, i.e., distributions $Q(x,y)$ such that their probability density function can be written as $q(x,y) = q_X(x) q_Y(y)$ for some marginal density functions $q_X$ and $q_Y$.\n\nThe information projection of $P$ onto $\\mathcal{M}$ is defined as the distribution $P^* \\in \\mathcal{M}$ that minimizes the Kullback-Leibler (KL) divergence from $P$ to any distribution in $\\mathcal{M}$. The KL divergence between two continuous distributions $P$ and $Q$ with densities $p(z)$ and $q(z)$ respectively is defined using the natural logarithm:\n$$ D_{KL}(P || Q) = \\int p(z) \\ln\\left(\\frac{p(z)}{q(z)}\\right) dz $$\nYour task is to calculate the minimum KL divergence, $D_{KL}(P || P^*)$, as a function of the correlation coefficient $\\rho$.", "solution": "1. The KL divergence between $P(x,y)$ and any product distribution $Q(x,y)=q_X(x)q_Y(y)$ is\n$$\nD_{KL}(P\\Vert Q)\n=\\iint p(x,y)\\ln\\frac{p(x,y)}{q_X(x)q_Y(y)}\\,dx\\,dy.\n$$\nUnder the constraint $Q(x,y)=q_X(x)q_Y(y)$, the minimizer is \n$$q_X(x)=p_X(x),\\quad q_Y(y)=p_Y(y),$$ \nthe marginals of $P$.\n\n2. Hence the minimum KL divergence is the mutual information\n$$\nD_{KL}(P\\Vert P^*)\n=\\iint p(x,y)\\ln\\frac{p(x,y)}{p_X(x)p_Y(y)}\\,dx\\,dy\n=I(X;Y).\n$$\n\n3. For a zero-mean bivariate normal with unit variances and correlation $\\rho$, the differential entropies are\n$$\nH(X,Y)=\\tfrac12\\ln\\bigl((2\\pi e)^2(1-\\rho^2)\\bigr),\n\\quad\nH(X)=H(Y)=\\tfrac12\\ln(2\\pi e).\n$$\n\n4. Therefore\n$$\nI(X;Y)\n=H(X)+H(Y)-H(X,Y)\n=\\ln(2\\pi e)-\\Bigl[\\tfrac12\\ln\\bigl((2\\pi e)^2(1-\\rho^2)\\bigr)\\Bigr]\n=-\\tfrac12\\ln(1-\\rho^2).\n$$\n\n5. Thus the minimum KL divergence is\n$$\nD_{KL}(P\\Vert P^*)=-\\tfrac12\\ln(1-\\rho^2).\n$$", "answer": "$$\\boxed{\\;-\\tfrac12\\ln\\bigl(1-\\rho^2\\bigr)\\;}$$", "id": "69220"}]}