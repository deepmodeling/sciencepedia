{"hands_on_practices": [{"introduction": "Beyond theoretical limits, practical data compression relies on clever algorithms. The Lempel-Ziv-Welch (LZW) algorithm is a cornerstone of modern compression, building a dictionary of phrases on-the-fly to encode data efficiently. This exercise [@problem_id:53455] provides a hands-on walk-through of the LZW process, revealing how an adaptive dictionary grows and how variable-length codes are used to represent an input stream, giving you a practical feel for this powerful technique.", "problem": "The Lempel-Ziv-Welch (LZW) algorithm is a universal lossless data compression algorithm. Its operation relies on building a dictionary of strings encountered during compression.\n\nThe LZW algorithm can be summarized as follows:\n1.  Initialize a dictionary with a set of single-character strings.\n2.  Set `W` (the working string) to the first character of the input stream.\n3.  Loop over the input stream:\n    a. Read the next character `K`.\n    b. If the string `W + K` exists in the dictionary, set `W = W + K`.\n    c. If `W + K` is not in the dictionary, then:\n        i. Output the dictionary code corresponding to `W`.\n        ii. Add the new string `W + K` to the dictionary with a new code.\n        iii. Set `W = K`.\n4.  After the input stream is exhausted, output the code for the final working string `W`.\n\nConsider a specific implementation of LZW with the following specifications:\n*   The initial dictionary contains 256 entries for the 8-bit ASCII characters. The codes for these characters are their corresponding ASCII values (0-255). The next available code for new dictionary entries is 256.\n*   The number of bits used to represent an output code is determined by the dictionary size at the moment of output. If the dictionary contains $N$ entries, the output code is represented using $\\lceil \\log_2(N) \\rceil$ bits.\n*   The input character string to be compressed is `BANANA_BANDANA`.\n\nYour task is to calculate the total number of bits in the final compressed output sequence.", "solution": "The input string is \"BANANA_BANDANA\". The LZW compression process is simulated step by step to determine the output codes and the dictionary size at the time of each output. The dictionary starts with 256 entries (0 to 255 for ASCII characters). The next available code is 256.\n\n**Step-by-step simulation:**\n\n1. Initialize dictionary with ASCII 0-255. Set W = first character 'B'.\n2. Read next character 'A': \"BA\" not in dictionary.\n   - Output code for 'B' (66).\n   - Add \"BA\" to dictionary with code 256.\n   - Set W = 'A'.\n   - Dictionary size at output: 256 → bits = $ \\lceil \\log_2(256) \\rceil = 8 $.\n3. Read next character 'N': \"AN\" not in dictionary.\n   - Output code for 'A' (65).\n   - Add \"AN\" to dictionary with code 257.\n   - Set W = 'N'.\n   - Dictionary size at output: 257 → bits = $ \\lceil \\log_2(257) \\rceil = 9 $ (since $256  257  512$).\n4. Read next character 'A': \"NA\" not in dictionary.\n   - Output code for 'N' (78).\n   - Add \"NA\" to dictionary with code 258.\n   - Set W = 'A'.\n   - Dictionary size at output: 258 → bits = 9.\n5. Read next character 'N': \"A\" + \"N\" = \"AN\" is in dictionary (code 257). Set W = \"AN\".\n6. Read next character 'A': \"AN\" + \"A\" = \"ANA\" not in dictionary.\n   - Output code for \"AN\" (257).\n   - Add \"ANA\" to dictionary with code 259.\n   - Set W = 'A'.\n   - Dictionary size at output: 259 → bits = 9.\n7. Read next character '_' (underscore): \"A\" + \"_\" = \"A_\" not in dictionary.\n   - Output code for 'A' (65).\n   - Add \"A_\" to dictionary with code 260.\n   - Set W = '_'.\n   - Dictionary size at output: 260 → bits = 9.\n8. Read next character 'B': \"_\" + \"B\" = \"_B\" not in dictionary.\n   - Output code for '_' (95, ASCII value).\n   - Add \"_B\" to dictionary with code 261.\n   - Set W = 'B'.\n   - Dictionary size at output: 261 → bits = 9.\n9. Read next character 'A': \"B\" + \"A\" = \"BA\" is in dictionary (code 256). Set W = \"BA\".\n10. Read next character 'N': \"BA\" + \"N\" = \"BAN\" not in dictionary.\n    - Output code for \"BA\" (256).\n    - Add \"BAN\" to dictionary with code 262.\n    - Set W = 'N'.\n    - Dictionary size at output: 262 → bits = 9.\n11. Read next character 'D': \"N\" + \"D\" = \"ND\" not in dictionary.\n    - Output code for 'N' (78).\n    - Add \"ND\" to dictionary with code 263.\n    - Set W = 'D'.\n    - Dictionary size at output: 263 → bits = 9.\n12. Read next character 'A': \"D\" + \"A\" = \"DA\" not in dictionary.\n    - Output code for 'D' (68).\n    - Add \"DA\" to dictionary with code 264.\n    - Set W = 'A'.\n    - Dictionary size at output: 264 → bits = 9.\n13. Read next character 'N': \"A\" + \"N\" = \"AN\" is in dictionary (code 257). Set W = \"AN\".\n14. Read next character 'A': \"AN\" + \"A\" = \"ANA\" is in dictionary (code 259). Set W = \"ANA\". End of input.\n    - Output code for \"ANA\" (259).\n    - Dictionary size at output: 265 (since last addition was code 264, and no new string added here) → bits = $ \\lceil \\log_2(265) \\rceil = 9 $.\n\n**Output codes and bit counts:**\n- Output 1: 66 → 8 bits\n- Output 2: 65 → 9 bits\n- Output 3: 78 → 9 bits\n- Output 4: 257 → 9 bits\n- Output 5: 65 → 9 bits\n- Output 6: 95 → 9 bits\n- Output 7: 256 → 9 bits\n- Output 8: 78 → 9 bits\n- Output 9: 68 → 9 bits\n- Output 10: 259 → 9 bits\n\n**Total bits calculation:**\n- Output 1: 8 bits\n- Outputs 2 to 10: 9 outputs × 9 bits each = 81 bits\n- Total bits = 8 + 81 = 89", "answer": "\\boxed{89}", "id": "53455"}, {"introduction": "While lossless compression is ideal, many applications require trading some fidelity for a much lower data rate. Rate-distortion theory provides the fundamental limits for this trade-off, defining the minimum rate $R(D)$ required for a given distortion $D$. This exercise [@problem_id:53350] explores this concept for a correlated, two-dimensional Gaussian source, demonstrating the \"reverse water-filling\" principle. You will see how to optimally allocate distortion between the source's components, a key insight in compressing multidimensional data.", "problem": "Consider a memoryless bivariate Gaussian source, where each sample is an independent draw of a two-dimensional random vector $X = (X_1, X_2)^T$. The vector $X$ follows a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$ with zero mean and a covariance matrix given by\n$$\n\\Sigma = \\sigma^2 \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\nwhere $\\sigma^2  0$ is the variance of each component and $\\rho$ is the correlation coefficient, satisfying $1/2  \\rho  1$.\n\nWe wish to compress this source and reconstruct it as $\\hat{X} = (\\hat{X}_1, \\hat{X}_2)^T$. The quality of the reconstruction is measured by the total mean-squared error (MSE) distortion, defined as $d(X, \\hat{X}) = E[\\|X - \\hat{X}\\|^2] = E[(X_1 - \\hat{X}_1)^2 + (X_2 - \\hat{X}_2)^2]$.\n\nThe rate-distortion function, $R(D)$, for this source specifies the minimum achievable rate (in bits per 2D vector sample) for a given maximum allowed average distortion $D$.\n\nCalculate the value of the rate-distortion function $R(D)$ for a total average distortion of exactly $D = \\sigma^2$.", "solution": "The rate-distortion function for a multivariate Gaussian source with covariance matrix $\\Sigma$ under mean-squared error distortion is found by reverse water-filling on the eigenvalues of $\\Sigma$. The covariance matrix is:\n$$\n\\Sigma = \\sigma^2 \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda_1$ and $\\lambda_2$ of $\\Sigma$ are obtained from the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} \\sigma^2 - \\lambda  \\sigma^2 \\rho \\\\ \\sigma^2 \\rho  \\sigma^2 - \\lambda \\end{pmatrix} = (\\sigma^2 - \\lambda)^2 - (\\sigma^2 \\rho)^2 = 0\n$$\nSolving:\n$$\n(\\sigma^2 - \\lambda - \\sigma^2 \\rho)(\\sigma^2 - \\lambda + \\sigma^2 \\rho) = 0\n$$\nThus:\n$$\n\\lambda_1 = \\sigma^2 (1 + \\rho), \\quad \\lambda_2 = \\sigma^2 (1 - \\rho)\n$$\nGiven $1/2  \\rho  1$, it follows that $\\lambda_1  \\lambda_2  0$.\n\nThe rate-distortion function is:\n$$\nR(D) = \\min_{\\substack{D_1, D_2 \\\\ D_1 + D_2 \\leq D}} \\sum_{i=1}^2 \\frac{1}{2} \\log_2 \\left( \\frac{\\lambda_i}{D_i} \\right)\n$$\nsubject to $0 \\leq D_i \\leq \\lambda_i$ for $i=1,2$. The minimization is achieved by reverse water-filling, setting $D_i = \\min\\{\\lambda_i, \\theta\\}$ for a water-level $\\theta$ chosen such that $D_1 + D_2 = D$.\n\nFor $D = \\sigma^2$, consider the cases for $\\theta$:\n\n- If $\\theta \\geq \\lambda_1$, then $D_1 = \\lambda_1$ and $D_2 = \\lambda_2$, so $D_1 + D_2 = \\lambda_1 + \\lambda_2 = 2\\sigma^2  \\sigma^2$, which is too large.\n- If $\\lambda_2 \\leq \\theta  \\lambda_1$, then $D_1 = \\theta$ and $D_2 = \\lambda_2$. Set $D_1 + D_2 = D$:\n  $$\n  \\theta + \\sigma^2 (1 - \\rho) = \\sigma^2\n  $$\n  Solving for $\\theta$:\n  $$\n  \\theta = \\sigma^2 \\rho\n  $$\n  Check the condition $\\lambda_2 \\leq \\theta  \\lambda_1$:\n  $$\n  \\sigma^2 (1 - \\rho) \\leq \\sigma^2 \\rho  \\sigma^2 (1 + \\rho)\n  $$\n  Since $\\rho  1/2$, $1 - \\rho  \\rho$ holds, and $\\rho  1 + \\rho$ is always true. Thus, the condition is satisfied.\n- If $\\theta  \\lambda_2$, then $D_1 = \\theta$ and $D_2 = \\theta$, so $2\\theta = \\sigma^2$ gives $\\theta = \\sigma^2 / 2$. But $\\theta  \\lambda_2$ implies $\\sigma^2 / 2  \\sigma^2 (1 - \\rho)$, or $\\rho  1/2$, which contradicts $\\rho  1/2$. Thus, this case is invalid.\n\nTherefore, the distortion allocation is $D_1 = \\sigma^2 \\rho$ and $D_2 = \\sigma^2 (1 - \\rho)$. The rate-distortion function is:\n$$\nR(D) = \\frac{1}{2} \\log_2 \\left( \\frac{\\lambda_1}{D_1} \\right) + \\frac{1}{2} \\log_2 \\left( \\frac{\\lambda_2}{D_2} \\right)\n$$\nSubstituting the values:\n$$\nR(D) = \\frac{1}{2} \\log_2 \\left( \\frac{\\sigma^2 (1 + \\rho)}{\\sigma^2 \\rho} \\right) + \\frac{1}{2} \\log_2 \\left( \\frac{\\sigma^2 (1 - \\rho)}{\\sigma^2 (1 - \\rho)} \\right)\n$$\nSimplifying:\n$$\nR(D) = \\frac{1}{2} \\log_2 \\left( \\frac{1 + \\rho}{\\rho} \\right) + \\frac{1}{2} \\log_2 (1) = \\frac{1}{2} \\log_2 \\left( \\frac{1 + \\rho}{\\rho} \\right)\n$$\nsince $\\log_2(1) = 0$.", "answer": "$$ \\boxed{\\dfrac{1}{2} \\log_{2} \\left( \\dfrac{1 + \\rho}{\\rho} \\right)} $$", "id": "53350"}, {"introduction": "The capacity of a communication channel defines the ultimate speed limit for error-free data transmission. For channels where noise is not uniform across all frequencies, simply broadcasting with uniform power is inefficient. This problem [@problem_id:53407] introduces the elegant \"water-filling\" solution for optimally allocating signal power across a frequency band to maximize the total capacity. By working through this example with a non-uniform noise spectrum, you will gain practical understanding of one of the most important resource allocation principles in communication theory.", "problem": "Consider a continuous-time communication channel affected by additive colored Gaussian noise. The channel output $Y(t)$ is given by $Y(t) = X(t) + N(t)$, where $X(t)$ is the input signal and $N(t)$ is the noise.\n\nThe input signal $X(t)$ is constrained in its total average power to $P$. The transmission is band-limited. The noise process $N(t)$ is a zero-mean stationary Gaussian process with a one-sided power spectral density (PSD) given by:\n$$\nS_N(f) = \\begin{cases}\n\\alpha f  \\text{for } f \\in [W_1, W_2] \\\\\n\\infty  \\text{otherwise}\n\\end{cases}\n$$\nHere, $f$ is the frequency, and $\\alpha$, $W_1$, and $W_2$ are positive constants with $W_2  W_1  0$.\n\nThe capacity $C$ of such a channel for a real-valued signal is found by maximizing the integral of the spectral information rate over all valid signal power allocations $S_X(f)$ that satisfy the power constraint $\\int_{W_1}^{W_2} S_X(f) df = P$. The capacity formula is:\n$$\nC = \\max_{S_X(f)} \\int_{W_1}^{W_2} \\log_2 \\left(1 + \\frac{S_X(f)}{S_N(f)}\\right) df\n$$\nDetermine the capacity $C$ of this channel. You should operate under the assumption that the total signal power $P$ is sufficiently small, such that the optimal power allocation strategy does not utilize the entire available frequency band $[W_1, W_2]$. Express your answer in terms of $P$, $\\alpha$, and $W_1$.", "solution": "We apply the water-filling solution over the band $[W_1,W_2]$, under the assumption that only $[W_1,F]$ is used with $F\\le W_2$.  The noise PSD is $S_N(f)=\\alpha f$.  Let the water-level be $\\mu$.  Then \n1. Power allocation: \n$$S_X(f)=\\bigl[\\mu-\\alpha f\\bigr]_+\\,,\\quad f\\in[W_1,W_2].$$\nUnder the small-$P$ assumption we have support $f\\in[W_1,F]$ with $\\mu=\\alpha F$. \n2. The total power constraint \n$$\\int_{W_1}^F(\\mu-\\alpha f)\\,df\n=\\int_{W_1}^F(\\alpha F-\\alpha f)\\,df\n=\\alpha\\int_{W_1}^F(F-f)\\,df\n=\\frac{\\alpha}{2}(F-W_1)^2\n= P$$\ngives \n$$F-W_1=\\sqrt{\\frac{2P}{\\alpha}}\\,,\\qquad F=W_1+\\sqrt{\\frac{2P}{\\alpha}}\\,. $$\n3. The capacity is\n$$\nC=\\int_{W_1}^F\\log_2\\!\\biggl(1+\\frac{S_X(f)}{\\alpha f}\\biggr)\\,df\n=\\int_{W_1}^F\\log_2\\!\\frac{\\alpha F}{\\alpha f}\\,df\n=\\int_{W_1}^F\\bigl[\\log_2(\\alpha F)-\\log_2(\\alpha f)\\bigr]df.\n$$ \nCompute each term: with $D=F-W_1$,\n$$\n\\int_{W_1}^F\\log_2(\\alpha F)\\,df=D\\log_2(\\alpha F),\\quad\n\\int_{W_1}^F\\log_2(\\alpha f)\\,df\n=\\frac{1}{\\ln2}\\bigl[f\\ln(\\alpha f)-f\\bigr]_{W_1}^{F}.\n$$\nHence\n$$\nC=\\frac{1}{\\ln2}\\Bigl[D\\ln(\\alpha F)-\\bigl(F\\ln(\\alpha F)-F-W_1\\ln(\\alpha W_1)+W_1\\bigr)\\Bigr].\n$$\nUsing $D=F-W_1$ and $\\ln(\\alpha F)=\\ln(\\alpha W_1)+\\ln\\bigl(1+\\frac{D}{W_1}\\bigr)$ one obtains\n$$\nC=\\frac{1}{\\ln2}\\Bigl[-W_1\\ln\\bigl(1+\\tfrac{D}{W_1}\\bigr)+D\\Bigr]\n=\\frac{1}{\\ln2}\\Bigl[-W_1\\ln\\!\\bigl(1+\\sqrt{\\tfrac{2P}{\\alpha}}/W_1\\bigr)\n+\\sqrt{\\tfrac{2P}{\\alpha}}\\Bigr].\n$$\nThis is the capacity in bits per second.", "answer": "$$\\boxed{\\frac{\\sqrt{\\tfrac{2P}{\\alpha}}-W_{1}\\,\\ln\\!\\bigl(1+\\tfrac{1}{W_{1}}\\sqrt{\\tfrac{2P}{\\alpha}}\\bigr)}{\\ln2}}$$", "id": "53407"}]}