{"hands_on_practices": [{"introduction": "The binary entropy function is central to quantifying uncertainty and information. A primary application in quantum information is calculating the mutual information $I(X:Y)$, which measures the correlation between a sender's state preparation, $X$, and a receiver's measurement outcome, $Y$. This exercise provides a foundational practice in this calculation, demonstrating how much information can be extracted when the prepared quantum states are non-orthogonal, a common scenario in quantum communication [@problem_id:144103].", "problem": "In a quantum communication scenario, a source prepares a qubit in one of two states, chosen from the set $\\{|s_0\\rangle, |s_1\\rangle\\}$. The choice of state is described by a classical random variable $X \\in \\{0, 1\\}$. The two states are the computational basis state $|0\\rangle$ and the Hadamard state $|+\\rangle$, defined as:\n$$\n|s_0\\rangle = |0\\rangle, \\quad |s_1\\rangle = |+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)\n$$\nThe source chooses between these two states with equal prior probability, i.e., $P(X=0) = P(X=1) = 1/2$.\n\nThe prepared qubit is sent to a receiver who performs a measurement in the computational basis $\\{|0\\rangle, |1\\rangle\\}$. The measurement outcome is described by a classical random variable $Y \\in \\{0, 1\\}$, corresponding to projections onto $|0\\rangle$ and $|1\\rangle$ respectively.\n\nThe amount of information the receiver's outcome $Y$ provides about the source's preparation choice $X$ is quantified by the mutual information $I(X:Y)$. It is defined as:\n$$\nI(X:Y) = H(Y) - H(Y|X)\n$$\nwhere $H(Z) = -\\sum_i p(z_i) \\log_2(p(z_i))$ is the Shannon entropy of a random variable $Z$ with probability distribution $p(z_i)$, and $H(Y|X) = \\sum_j P(X=x_j) H(Y|X=x_j)$ is the conditional entropy. The entropy is measured in bits.\n\nCalculate the mutual information $I(X:Y)$ for this specific preparation and measurement scheme.", "solution": "To calculate the mutual information $I(X:Y) = H(Y) - H(Y|X)$, we first determine the conditional probabilities $P(Y|X)$ and the marginal probabilities.\n\nThe states are:\n- $|s_0\\rangle = |0\\rangle$\n- $|s_1\\rangle = |+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)$\n\nThe receiver measures in the computational basis $\\{|0\\rangle, |1\\rangle\\}$, so:\n- When $X=0$, the state is $|0\\rangle$:\n  \n$$\n  P(Y=0|X=0) = |\\langle 0|0\\rangle|^2 = 1, \\quad P(Y=1|X=0) = |\\langle 1|0\\rangle|^2 = 0\n  $$\n\n- When $X=1$, the state is $|+\\rangle$:\n  \n$$\n  P(Y=0|X=1) = \\left|\\langle 0|+\\rangle\\right|^2 = \\left|\\frac{1}{\\sqrt{2}}\\right|^2 = \\frac{1}{2}, \\quad P(Y=1|X=1) = \\left|\\langle 1|+\\rangle\\right|^2 = \\frac{1}{2}\n  $$\n\n\nGiven $P(X=0) = P(X=1) = \\frac{1}{2}$, the joint probabilities are:\n\n$$\nP(X=0, Y=0) = P(Y=0|X=0)P(X=0) = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}\n$$\n\n\n$$\nP(X=0, Y=1) = 0 \\cdot \\frac{1}{2} = 0\n$$\n\n\n$$\nP(X=1, Y=0) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\n$$\n\n\n$$\nP(X=1, Y=1) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\n$$\n\n\nThe marginal probability $P(Y)$ is:\n\n$$\nP(Y=0) = P(X=0,Y=0) + P(X=1,Y=0) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\n$$\n\n\n$$\nP(Y=1) = P(X=0,Y=1) + P(X=1,Y=1) = 0 + \\frac{1}{4} = \\frac{1}{4}\n$$\n\n\nThe entropy $H(Y)$ is:\n\n$$\nH(Y) = -\\sum_{y} P(Y=y) \\log_2 P(Y=y) = -\\left[ P(Y=0) \\log_2 P(Y=0) + P(Y=1) \\log_2 P(Y=1) \\right]\n$$\n\nSubstituting the values:\n\n$$\nH(Y) = -\\left[ \\frac{3}{4} \\log_2 \\frac{3}{4} + \\frac{1}{4} \\log_2 \\frac{1}{4} \\right]\n$$\n\nSince $\\log_2 \\frac{1}{4} = \\log_2 (2^{-2}) = -2$:\n\n$$\nH(Y) = -\\left[ \\frac{3}{4} \\log_2 \\frac{3}{4} + \\frac{1}{4} \\cdot (-2) \\right] = -\\left[ \\frac{3}{4} \\log_2 \\frac{3}{4} - \\frac{1}{2} \\right]\n$$\n\n\nThe conditional entropy $H(Y|X)$ is:\n\n$$\nH(Y|X) = \\sum_{x} P(X=x) H(Y|X=x)\n$$\n\nFor $X=0$:\n\n$$\nH(Y|X=0) = -\\sum_{y} P(Y=y|X=0) \\log_2 P(Y=y|X=0) = -\\left[ 1 \\cdot \\log_2 1 + 0 \\cdot \\log_2 0 \\right]\n$$\n\nUsing continuity, $0 \\log_2 0 \\to 0$ and $\\log_2 1 = 0$, so:\n\n$$\nH(Y|X=0) = 0\n$$\n\nFor $X=1$:\n\n$$\nH(Y|X=1) = -\\left[ \\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{2} \\log_2 \\frac{1}{2} \\right] = -\\left[ \\frac{1}{2} \\cdot (-1) + \\frac{1}{2} \\cdot (-1) \\right] = -\\left[ -\\frac{1}{2} - \\frac{1}{2} \\right] = -[-1] = 1\n$$\n\nThus:\n\n$$\nH(Y|X) = P(X=0) H(Y|X=0) + P(X=1) H(Y|X=1) = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n$$\n\n\nThe mutual information is:\n\n$$\nI(X:Y) = H(Y) - H(Y|X) = -\\left[ \\frac{3}{4} \\log_2 \\frac{3}{4} - \\frac{1}{2} \\right] - \\frac{1}{2} = -\\frac{3}{4} \\log_2 \\frac{3}{4}\n$$\n\nSince $-\\log_2 \\frac{3}{4} = \\log_2 \\frac{4}{3}$:\n\n$$\nI(X:Y) = \\frac{3}{4} \\log_2 \\frac{4}{3}\n$$", "answer": "$$\\boxed{\\dfrac{3}{4} \\log_{2} \\dfrac{4}{3}}$$", "id": "144103"}, {"introduction": "Beyond its roots in communication theory, the binary entropy finds a profound application in quantifying quantum entanglement. The entanglement entropy of a pure bipartite state is defined as the von Neumann entropy of its reduced subsystems. This practice explores this connection in the context of a fermionic system, a cornerstone of condensed matter physics, by calculating the entanglement entropy of a Bogoliubov vacuum state [@problem_id:143950]. You will see how the probability of creating a quasiparticle pair directly maps to the parameter of a binary entropy function, beautifully illustrating how a simple information-theoretic measure can quantify a fundamental quantum-mechanical resource.", "problem": "Consider a system of two distinct fermionic modes, described by the creation and annihilation operators $(c_1^\\dagger, c_1)$ and $(c_2^\\dagger, c_2)$. These operators obey the canonical anti-commutation relations:\n$$\n\\{c_i, c_j^\\dagger\\} = \\delta_{ij}, \\quad \\{c_i, c_j\\} = 0, \\quad \\{c_i^\\dagger, c_j^\\dagger\\} = 0,\n$$\nfor $i,j \\in \\{1,2\\}$. The Fock space for this system is built upon the vacuum state $|00\\rangle$, which is annihilated by both annihilation operators, i.e., $c_1|00\\rangle = 0$ and $c_2|00\\rangle = 0$. The basis states are $|n_1 n_2\\rangle = (c_1^\\dagger)^{n_1}(c_2^\\dagger)^{n_2}|00\\rangle$, where $n_i \\in \\{0,1\\}$.\n\nA Bogoliubov transformation defines a new set of fermionic quasiparticle operators $(b_1, b_2)$ as follows:\n$$\nb_1 = u c_1 - v c_2^\\dagger \\\\\nb_2 = v c_1^\\dagger + u c_2\n$$\nwhere $u$ and $v$ are real parameters satisfying $u^2+v^2=1$, which ensures the new operators also satisfy the canonical anti-commutation relations. We assume $u,v \\ge 0$.\n\nThe Bogoliubov vacuum state, denoted $|\\Psi\\rangle$, is the unique normalized state that is annihilated by the new quasiparticle operators:\n$$\nb_1 |\\Psi\\rangle = 0 \\quad \\text{and} \\quad b_2 |\\Psi\\rangle = 0.\n$$\nThis state represents the ground state of a certain fermionic Hamiltonian.\n\nThe task is to compute the entanglement entropy of the first fermionic mode. The state of the first mode is described by the reduced density matrix $\\rho_1 = \\text{Tr}_2(|\\Psi\\rangle\\langle\\Psi|)$, where the trace is taken over the Hilbert space of the second mode. The entanglement entropy is the von Neumann entropy of this reduced state:\n$$\nS_1 = -\\text{Tr}_1(\\rho_1 \\log_2 \\rho_1).\n$$\nExpress the final answer as a function of the parameter $p = v^2$.", "solution": "The first step is to determine the explicit form of the Bogoliubov vacuum state $|\\Psi\\rangle$ in the original Fock basis $\\{|00\\rangle, |10\\rangle, |01\\rangle, |11\\rangle\\}$. Let's write a general state in this basis as $|\\Psi\\rangle = A|00\\rangle + B|10\\rangle + C|01\\rangle + D|11\\rangle$. We apply the conditions $b_1|\\Psi\\rangle=0$ and $b_2|\\Psi\\rangle=0$.\n\nUsing the anti-commutation relations, the action of the $c_i$ operators on the basis states is:\n$c_1|00\\rangle=0$, $c_1|10\\rangle=|00\\rangle$, $c_1|01\\rangle=0$, $c_1|11\\rangle=|01\\rangle$.\n$c_2|00\\rangle=0$, $c_2|10\\rangle=0$, $c_2|01\\rangle=|00\\rangle$, $c_2|11\\rangle=-|10\\rangle$.\nThe minus sign for $c_2|11\\rangle$ comes from $c_2 c_1^\\dagger c_2^\\dagger |00\\rangle = -c_1^\\dagger c_2 c_2^\\dagger|00\\rangle = -c_1^\\dagger|00\\rangle = -|10\\rangle$.\nSimilarly, for the creation operators:\n$c_1^\\dagger|00\\rangle=|10\\rangle$, $c_1^\\dagger|10\\rangle=0$, $c_1^\\dagger|01\\rangle=|11\\rangle$, $c_1^\\dagger|11\\rangle=0$.\n$c_2^\\dagger|00\\rangle=|01\\rangle$, $c_2^\\dagger|10\\rangle=-|11\\rangle$, $c_2^\\dagger|01\\rangle=0$, $c_2^\\dagger|11\\rangle=0$.\n\nNow we apply the first condition, $b_1|\\Psi\\rangle = (u c_1 - v c_2^\\dagger)|\\Psi\\rangle = 0$:\n$$\nu(B|00\\rangle + D|01\\rangle) - v(A|01\\rangle - B|11\\rangle) = 0\n$$\n$$\n(uB)|00\\rangle + (uD - vA)|01\\rangle + (vB)|11\\rangle = 0\n$$\nSince the basis states are orthogonal, the coefficient of each must be zero. This gives us two equations:\n1. $uB = 0 \\implies B=0$ (since $p=v^2 < 1$ implies $u \\ne 0$).\n2. $uD - vA = 0 \\implies uD = vA$.\n\nNext, we apply the second condition, $b_2|\\Psi\\rangle = (v c_1^\\dagger + u c_2)|\\Psi\\rangle = 0$:\n$$\nv(A|10\\rangle + C|11\\rangle) + u(C|00\\rangle - D|10\\rangle) = 0\n$$\n$$\n(uC)|00\\rangle + (vA - uD)|10\\rangle + (vC)|11\\rangle = 0\n$$\nThis gives two more equations:\n3. $uC = 0 \\implies C=0$.\n4. $vA - uD = 0$, which is the same as equation (2).\n\nFrom these conditions, we have $B=0$, $C=0$, and $uD=vA$. The state has the form $|\\Psi\\rangle = A|00\\rangle + D|11\\rangle$. We can set $A=u$ and $D=v$ to satisfy $uD=vA$. This gives $|\\Psi\\rangle = N(u|00\\rangle + v|11\\rangle)$ for some normalization constant $N$.\nThe normalization condition is $\\langle\\Psi|\\Psi\\rangle = 1$:\n$$\nN^2 (u^*\\langle 00| + v^*\\langle 11|) (u|00\\rangle + v|11\\rangle) = N^2(u^2+v^2) = 1\n$$\nSince $u^2+v^2=1$, we have $N^2=1$. We choose $N=1$. The state is:\n$$\n|\\Psi\\rangle = u|00\\rangle + v|11\\rangle\n$$\nThe problem defines $p=v^2$ and we have $u^2+v^2=1$, so $u^2=1-p$. Since $u,v \\ge 0$, we have $u=\\sqrt{1-p}$ and $v=\\sqrt{p}$. The state is:\n$$\n|\\Psi\\rangle = \\sqrt{1-p}|00\\rangle + \\sqrt{p}|11\\rangle\n$$\nNext, we compute the density matrix of the full system, $|\\Psi\\rangle\\langle\\Psi|$:\n$$\n|\\Psi\\rangle\\langle\\Psi| = (\\sqrt{1-p}|00\\rangle + \\sqrt{p}|11\\rangle)(\\sqrt{1-p}\\langle 00| + \\sqrt{p}\\langle 11|)\n$$\n$$\n= (1-p)|00\\rangle\\langle 00| + \\sqrt{p(1-p)}|00\\rangle\\langle 11| + \\sqrt{p(1-p)}|11\\rangle\\langle 00| + p|11\\rangle\\langle 11|\n$$\nNow, we find the reduced density matrix for the first mode, $\\rho_1 = \\text{Tr}_2(|\\Psi\\rangle\\langle\\Psi|)$, by tracing over the basis states $\\{|0\\rangle_2, |1\\rangle_2\\}$ of the second mode.\n$$\n\\rho_1 = \\langle 0|_2 (|\\Psi\\rangle\\langle\\Psi|) |0\\rangle_2 + \\langle 1|_2 (|\\Psi\\rangle\\langle\\Psi|) |1\\rangle_2\n$$\nThe first term is:\n$$\n\\langle 0|_2 \\left[ (1-p)|0\\rangle_1|0\\rangle_2\\langle 0|_1\\langle 0|_2 + \\dots \\right] |0\\rangle_2 = (1-p)|0\\rangle_1\\langle 0|_1\n$$\nThe second term is:\n$$\n\\langle 1|_2 \\left[ \\dots + p|1\\rangle_1|1\\rangle_2\\langle 1|_1\\langle 1|_2 \\right] |1\\rangle_2 = p|1\\rangle_1\\langle 1|_1\n$$\nAll cross-terms vanish due to the orthogonality of the second mode's basis states, e.g., $\\langle 0|1\\rangle_2=0$.\nSo the reduced density matrix for the first mode is:\n$$\n\\rho_1 = (1-p)|0\\rangle_1\\langle 0|_1 + p|1\\rangle_1\\langle 1|_1\n$$\nIn the basis $\\{|0\\rangle_1, |1\\rangle_1\\}$, this matrix is:\n$$\n\\rho_1 = \\begin{pmatrix} 1-p & 0 \\\\ 0 & p \\end{pmatrix}\n$$\nThe eigenvalues of $\\rho_1$ are $\\lambda_1 = 1-p$ and $\\lambda_2 = p$.\nFinally, we compute the von Neumann entropy $S_1 = -\\text{Tr}(\\rho_1 \\log_2 \\rho_1)$. For a diagonal matrix, this simplifies to $S_1 = -\\sum_i \\lambda_i \\log_2 \\lambda_i$.\n$$\nS_1 = - \\left( (1-p) \\log_2(1-p) + p \\log_2(p) \\right)\n$$\nThis expression is the binary entropy function, often denoted $H(p)$.", "answer": "$$ \\boxed{-p\\log_2(p) - (1-p)\\log_2(1-p)} $$", "id": "143950"}, {"introduction": "The physical significance of the binary entropy extends beyond just counting states or measuring information; its very mathematical structure is imprinted on measurable thermodynamic properties. This problem reveals a striking relationship between the heat capacity $C_V$ of a two-level system and the binary entropy $H(p)$ of its thermal occupation probabilities, a phenomenon related to the Schottky anomaly [@problem_id:143918]. By completing this exercise, you will directly link a dynamic thermal property to the geometry of the entropy function, appreciating how the rate of change of information with respect to state probability governs the system's ability to absorb heat.", "problem": "Consider a quantum system with two non-degenerate energy levels, a ground state with energy $E_0=0$ and an excited state with energy $E_1=\\Delta E > 0$. The system is in thermal equilibrium with a heat reservoir at a temperature $T$. The Boltzmann constant is denoted by $k_B$.\n\nLet $p$ be the thermal occupation probability of the excited state. The heat capacity of the system at constant volume is given by $C_V = \\left(\\frac{\\partial \\langle E \\rangle}{\\partial T}\\right)_V$, where $\\langle E \\rangle$ is the average energy of the system. The uncertainty in the thermal state can be quantified by the binary entropy, $H(p)$, defined using the base-2 logarithm as $H(p) = -p \\log_2(p) - (1-p)\\log_2(1-p)$.\n\nIt can be shown that the heat capacity $C_V$ is related to the rate of change of the binary entropy with respect to the occupation probability, $\\frac{dH(p)}{dp}$. This relationship takes the form:\n$$C_V = \\mathcal{C} \\cdot k_B \\cdot p(1-p) \\cdot \\left(\\frac{dH(p)}{dp}\\right)^2$$\nwhere $\\mathcal{C}$ is a dimensionless constant, independent of temperature and the energy gap $\\Delta E$.\n\nYour task is to determine the exact value of the constant $\\mathcal{C}$.", "solution": "The system has two energy levels: ground state energy $E_0 = 0$ and excited state energy $E_1 = \\Delta E > 0$. The partition function is:\n\n$$\nZ = e^{-\\beta E_0} + e^{-\\beta E_1} = 1 + e^{-\\beta \\Delta E}\n$$\n\nwhere $\\beta = \\frac{1}{k_B T}$. The occupation probability of the excited state is:\n\n$$\np = \\frac{e^{-\\beta \\Delta E}}{Z} = \\frac{e^{-\\beta \\Delta E}}{1 + e^{-\\beta \\Delta E}} = \\frac{1}{1 + e^{\\beta \\Delta E}}\n$$\n\nThe average energy is:\n\n$$\n\\langle E \\rangle = 0 \\cdot (1 - p) + \\Delta E \\cdot p = \\Delta E  p\n$$\n\nThe heat capacity at constant volume is:\n\n$$\nC_V = \\left( \\frac{\\partial \\langle E \\rangle}{\\partial T} \\right)_V = \\Delta E \\frac{\\partial p}{\\partial T}\n$$\n\nExpress $p$ in terms of $x = \\beta \\Delta E = \\frac{\\Delta E}{k_B T}$:\n\n$$\np = \\frac{1}{1 + e^x}\n$$\n\nThe derivative with respect to temperature is:\n\n$$\n\\frac{\\partial p}{\\partial T} = \\frac{\\partial p}{\\partial x} \\frac{\\partial x}{\\partial T}, \\quad \\frac{\\partial x}{\\partial T} = -\\frac{\\Delta E}{k_B T^2}\n$$\n\nCompute $\\frac{\\partial p}{\\partial x}$:\n\n$$\n\\frac{\\partial p}{\\partial x} = -\\frac{e^x}{(1 + e^x)^2} = -p(1 - p)\n$$\n\nsince $1 - p = \\frac{e^x}{1 + e^x}$ and $p(1 - p) = \\frac{e^x}{(1 + e^x)^2}$. Thus:\n\n$$\n\\frac{\\partial p}{\\partial T} = [-p(1 - p)] \\left( -\\frac{\\Delta E}{k_B T^2} \\right) = p(1 - p) \\frac{\\Delta E}{k_B T^2}\n$$\n\nSubstitute into $C_V$:\n\n$$\nC_V = \\Delta E \\cdot p(1 - p) \\frac{\\Delta E}{k_B T^2} = \\frac{(\\Delta E)^2}{k_B T^2} p(1 - p)\n$$\n\nThe binary entropy is:\n\n$$\nH(p) = -p \\log_2 p - (1 - p) \\log_2 (1 - p) = -\\frac{1}{\\ln 2} \\left[ p \\ln p + (1 - p) \\ln (1 - p) \\right]\n$$\n\nIts derivative with respect to $p$ is:\n\n$$\n\\frac{dH}{dp} = -\\frac{1}{\\ln 2} \\frac{d}{dp} \\left[ p \\ln p + (1 - p) \\ln (1 - p) \\right]\n$$\n\nCompute the derivative inside:\n\n$$\n\\frac{d}{dp} \\left[ p \\ln p + (1 - p) \\ln (1 - p) \\right] = \\ln p + 1 - \\ln (1 - p) - 1 = \\ln \\left( \\frac{p}{1 - p} \\right)\n$$\n\nSo:\n\n$$\n\\frac{dH}{dp} = -\\frac{1}{\\ln 2} \\ln \\left( \\frac{p}{1 - p} \\right) = \\frac{1}{\\ln 2} \\ln \\left( \\frac{1 - p}{p} \\right) = \\log_2 \\left( \\frac{1 - p}{p} \\right)\n$$\n\nFrom the expression for $p$:\n\n$$\n\\frac{1 - p}{p} = e^{\\beta \\Delta E} = e^{\\Delta E / (k_B T)}\n$$\n\nThus:\n\n$$\n\\frac{dH}{dp} = \\log_2 \\left( e^{\\Delta E / (k_B T)} \\right) = \\frac{\\Delta E}{k_B T} \\log_2 e = \\frac{\\Delta E}{k_B T \\ln 2}\n$$\n\nSquare the derivative:\n\n$$\n\\left( \\frac{dH}{dp} \\right)^2 = \\left( \\frac{\\Delta E}{k_B T \\ln 2} \\right)^2\n$$\n\nThe given relation is:\n\n$$\nC_V = \\mathcal{C} \\cdot k_B \\cdot p(1 - p) \\cdot \\left( \\frac{dH}{dp} \\right)^2\n$$\n\nSubstitute the expressions for $C_V$ and $\\left( \\frac{dH}{dp} \\right)^2$:\n\n$$\n\\frac{(\\Delta E)^2}{k_B T^2} p(1 - p) = \\mathcal{C} \\cdot k_B \\cdot p(1 - p) \\cdot \\left( \\frac{\\Delta E}{k_B T \\ln 2} \\right)^2\n$$\n\nAfter expanding and simplifying the right hand side, we get:\n\n$$\n\\frac{(\\Delta E)^2}{k_B T^2} p(1 - p) = \\mathcal{C} \\cdot \\frac{(\\Delta E)^2 p(1 - p)}{k_B T^2 (\\ln 2)^2}\n$$\n\nCancel the common term $\\frac{(\\Delta E)^2 p(1 - p)}{k_B T^2}$ from both sides (assuming non-zero temperature and $p \\neq 0, 1$):\n\n$$\n1 = \\frac{\\mathcal{C}}{(\\ln 2)^2}\n$$\n\nSolve for $\\mathcal{C}$:\n\n$$\n\\mathcal{C} = (\\ln 2)^2\n$$", "answer": "$$\n\\boxed{(\\ln 2)^2}\n$$", "id": "143918"}]}