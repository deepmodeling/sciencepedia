{"hands_on_practices": [{"introduction": "The concept of a typical set, a cornerstone of information theory, can seem abstract at first. This exercise provides a concrete, hands-on entry point by asking you to apply the definition of the weak typical set directly. By calculating the size of this set for a simple binary source with a finite sequence length, you will translate the formal probabilistic condition into a tangible combinatorial counting problem, building a solid intuition for how the Asymptotic Equipartition Property (AEP) begins to take shape even for small systems. [@problem_id:56674]", "problem": "An independent and identically distributed (i.i.d.) discrete memoryless source is characterized by a random variable $X$ over a finite alphabet $\\mathcal{X}$, with a probability mass function $p(x) = \\text{Pr}(X=x)$. A sequence of $n$ symbols emitted by this source is denoted by $x^n = (x_1, x_2, \\ldots, x_n)$, and its probability is given by $p(x^n) = \\prod_{i=1}^n p(x_i)$.\n\nThe Shannon entropy of the source, using the logarithm of base 2, is defined as:\n$$ H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) $$\n\nThe *weak typical set* $A_\\epsilon^{(n)}$ for a given sequence length $n$ and a tolerance parameter $\\epsilon > 0$ is the set of all sequences $x^n$ whose sample entropy is close to the true entropy of the source. Formally, it is defined as:\n$$ A_\\epsilon^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\le \\epsilon \\right\\} $$\n\nConsider a binary memoryless source with alphabet $\\mathcal{X} = \\{0, 1\\}$ and probabilities $p(1) = p$ and $p(0) = 1-p$.\nFor a specific instance of this source with probability $p=1/3$, sequence length $n=9$, and tolerance $\\epsilon=1/6$, calculate the total number of sequences in the weak typical set, i.e., find the size of the set $|A_\\epsilon^{(n)}|$.", "solution": "The weak typical set $A_\\epsilon^{(n)}$ is defined as:\n\n$$\nA_\\epsilon^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\le \\epsilon \\right\\}\n$$\n\nFor a binary source with $p(1) = p = \\frac{1}{3}$ and $p(0) = 1 - p = \\frac{2}{3}$, the probability of a sequence $x^n$ with $k$ ones and $n - k$ zeros is:\n\n$$\np(x^n) = p^k (1 - p)^{n - k} = \\left(\\frac{1}{3}\\right)^k \\left(\\frac{2}{3}\\right)^{n - k}\n$$\n\nThus:\n\n$$\n\\log_2 p(x^n) = k \\log_2 \\left(\\frac{1}{3}\\right) + (n - k) \\log_2 \\left(\\frac{2}{3}\\right) = -k \\log_2 3 + (n - k) (1 - \\log_2 3)\n$$\n\nSimplifying:\n\n$$\n\\log_2 p(x^n) = -k \\log_2 3 + n - k - n \\log_2 3 + k \\log_2 3 = n(1 - \\log_2 3) - k\n$$\n\nSo:\n\n$$\n-\\frac{1}{n} \\log_2 p(x^n) = -\\frac{1}{n} [n(1 - \\log_2 3) - k] = -1 + \\log_2 3 + \\frac{k}{n}\n$$\n\nThe entropy $H(X)$ is:\n\n$$\nH(X) = -p \\log_2 p - (1 - p) \\log_2 (1 - p) = -\\frac{1}{3} \\log_2 \\frac{1}{3} - \\frac{2}{3} \\log_2 \\frac{2}{3} = \\frac{1}{3} \\log_2 3 + \\frac{2}{3} \\log_2 \\frac{3}{2}\n$$\n\n\n$$\nH(X) = \\frac{1}{3} \\log_2 3 + \\frac{2}{3} (\\log_2 3 - \\log_2 2) = \\frac{1}{3} \\log_2 3 + \\frac{2}{3} \\log_2 3 - \\frac{2}{3} = \\log_2 3 - \\frac{2}{3}\n$$\n\nThe typicality condition becomes:\n\n$$\n\\left| \\left( -1 + \\log_2 3 + \\frac{k}{n} \\right) - \\left( \\log_2 3 - \\frac{2}{3} \\right) \\right| \\le \\epsilon\n$$\n\nSimplifying the expression inside the absolute value:\n\n$$\n-1 + \\log_2 3 + \\frac{k}{n} - \\log_2 3 + \\frac{2}{3} = \\frac{k}{n} - \\frac{1}{3}\n$$\n\nThus:\n\n$$\n\\left| \\frac{k}{n} - \\frac{1}{3} \\right| \\le \\epsilon\n$$\n\nWith $n = 9$ and $\\epsilon = \\frac{1}{6}$:\n\n$$\n\\left| \\frac{k}{9} - \\frac{1}{3} \\right| \\le \\frac{1}{6}\n$$\n\nSolving for $k$:\n\n$$\n-\\frac{1}{6} \\le \\frac{k}{9} - \\frac{1}{3} \\le \\frac{1}{6}\n$$\n\n\n$$\n\\frac{1}{3} - \\frac{1}{6} \\le \\frac{k}{9} \\le \\frac{1}{3} + \\frac{1}{6}\n$$\n\n\n$$\n\\frac{1}{6} \\le \\frac{k}{9} \\le \\frac{1}{2}\n$$\n\n\n$$\n\\frac{9}{6} \\le k \\le \\frac{9}{2} \\implies 1.5 \\le k \\le 4.5\n$$\n\nSince $k$ must be an integer, $k = 2, 3, 4$.\n\nThe size of $A_\\epsilon^{(n)}$ is the number of sequences with $k$ ones, which is the sum of binomial coefficients:\n\n$$\n|A_\\epsilon^{(n)}| = \\sum_{k=2}^{4} \\binom{9}{k}\n$$\n\nCalculating:\n\n$$\n\\binom{9}{2} = \\frac{9 \\times 8}{2} = 36, \\quad \\binom{9}{3} = \\frac{9 \\times 8 \\times 7}{6} = 84, \\quad \\binom{9}{4} = \\frac{9 \\times 8 \\times 7 \\times 6}{24} = 126\n$$\n\n\n$$\n|A_\\epsilon^{(n)}| = 36 + 84 + 126 = 246\n$$", "answer": "$$ \\boxed{246} $$", "id": "56674"}, {"introduction": "Information theory distinguishes between weak and strong typicality, with the latter imposing stricter constraints on sequence structure. This practice is designed to make that crucial distinction tangible by having you identify and analyze sequences that belong to the weak typical set but fail the conditions for strong typicality. By explicitly computing the probability of this difference set for a small system, you will gain a deeper appreciation for the nuances of these definitions and understand why strong typicality provides a more powerful tool for many theoretical proofs. [@problem_id:56805]", "problem": "Consider an information source that produces a sequence of symbols $x_1, x_2, \\ldots, x_n$ which are independent and identically distributed (i.i.d.) according to a probability mass function $p(x)$ over a finite alphabet $\\mathcal{X}$.\n\nThe **weakly typical set** $A_\\epsilon^{(n)}$ with respect to a tolerance $\\epsilon > 0$ is the set of all sequences $x^n = (x_1, \\ldots, x_n)$ whose sample entropy is close to the Shannon entropy $H(X)$ of the source:\n$$A_\\epsilon^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\le \\epsilon \\right\\}$$\nwhere $p(x^n) = \\prod_{i=1}^n p(x_i)$ and $H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$.\n\nThe **strongly typical set** $T_\\delta^{(n)}$ with respect to a tolerance $\\delta > 0$ is the set of all sequences $x^n$ for which the empirical frequency of each symbol is close to its true probability. Let $N(x|x^n)$ be the number of occurrences of symbol $x$ in the sequence $x^n$. Then:\n$$T_\\delta^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| \\frac{N(x|x^n)}{n} - p(x) \\right| \\le \\delta \\text{ for all } x \\in \\mathcal{X}, \\text{ and } N(x|x^n)=0 \\text{ if } p(x)=0 \\right\\}$$\n\nNow, consider a specific ternary source with alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ and probabilities $p(0) = p_0$, $p(1) = p_1$, and $p(2) = p_2$. For the particular case where $p_0=1/2$, $p_1=1/4$, $p_2=1/4$, and for a sequence length of $n=4$ with tolerances $\\epsilon = 3/10$ and $\\delta = 3/10$, calculate the total probability of all sequences that are weakly typical but not strongly typical. That is, compute $P(A_\\epsilon^{(n)} \\setminus T_\\delta^{(n)})$.", "solution": "The problem involves a ternary source with alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ and probabilities $p(0) = \\frac{1}{2}$, $p(1) = \\frac{1}{4}$, $p(2) = \\frac{1}{4}$. The sequence length is $n = 4$, and tolerances are $\\epsilon = \\frac{3}{10}$ and $\\delta = \\frac{3}{10}$. The goal is to compute $P(A_\\epsilon^{(4)} \\setminus T_\\delta^{(4)})$, the probability of sequences that are weakly typical but not strongly typical.\n\nFirst, the Shannon entropy $H(X)$ is calculated:\n\n$$\nH(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) = -\\left[ \\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{4} \\log_2 \\frac{1}{4} + \\frac{1}{4} \\log_2 \\frac{1}{4} \\right].\n$$\n\nUsing $\\log_2 \\frac{1}{2} = -1$ and $\\log_2 \\frac{1}{4} = -2$:\n\n$$\nH(X) = -\\left[ \\frac{1}{2} \\cdot (-1) + \\frac{1}{4} \\cdot (-2) + \\frac{1}{4} \\cdot (-2) \\right] = -\\left[ -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right] = -\\left[ -\\frac{3}{2} \\right] = \\frac{3}{2}.\n$$\n\n\nFor a sequence $x^4 = (x_1, x_2, x_3, x_4)$, let $n_0$, $n_1$, $n_2$ be the counts of symbols $0$, $1$, $2$ respectively, with $n_0 + n_1 + n_2 = 4$. The probability is:\n\n$$\np(x^4) = \\prod_{i=1}^4 p(x_i) = \\left( \\frac{1}{2} \\right)^{n_0} \\left( \\frac{1}{4} \\right)^{n_1} \\left( \\frac{1}{4} \\right)^{n_2}.\n$$\n\nThe sample entropy is:\n\n$$\n-\\frac{1}{4} \\log_2 p(x^4) = -\\frac{1}{4} \\log_2 \\left[ \\left( \\frac{1}{2} \\right)^{n_0} \\left( \\frac{1}{4} \\right)^{n_1 + n_2} \\right].\n$$\n\nSince $\\log_2 \\frac{1}{2} = -1$ and $\\log_2 \\frac{1}{4} = -2$:\n\n$$\n-\\frac{1}{4} \\log_2 p(x^4) = -\\frac{1}{4} \\left[ n_0 \\cdot (-1) + (n_1 + n_2) \\cdot (-2) \\right] = \\frac{1}{4} (n_0 + 2n_1 + 2n_2).\n$$\n\nSubstituting $n_1 + n_2 = 4 - n_0$:\n\n$$\n\\frac{1}{4} (n_0 + 2(4 - n_0)) = \\frac{1}{4} (n_0 + 8 - 2n_0) = \\frac{1}{4} (8 - n_0) = 2 - \\frac{n_0}{4}.\n$$\n\n\nThe weakly typical set condition is:\n\n$$\n\\left| 2 - \\frac{n_0}{4} - \\frac{3}{2} \\right| \\leq \\frac{3}{10} \\implies \\left| \\frac{1}{2} - \\frac{n_0}{4} \\right| \\leq \\frac{3}{10}.\n$$\n\nThis simplifies to:\n\n$$\n-\\frac{3}{10} \\leq \\frac{1}{2} - \\frac{n_0}{4} \\leq \\frac{3}{10}.\n$$\n\nSolving the inequalities:\n\n$$\n-\\frac{3}{10} - \\frac{1}{2} \\leq -\\frac{n_0}{4} \\leq \\frac{3}{10} - \\frac{1}{2} \\implies -\\frac{8}{10} \\leq -\\frac{n_0}{4} \\leq -\\frac{2}{10} \\implies \\frac{2}{10} \\leq \\frac{n_0}{4} \\leq \\frac{8}{10}.\n$$\n\nMultiplying by 4:\n\n$$\n0.8 \\leq n_0 \\leq 3.2.\n$$\n\nSince $n_0$ is an integer, $n_0 \\in \\{1, 2, 3\\}$. Thus, weakly typical sequences have $n_0 = 1, 2,$ or $3$.\n\nFor the strongly typical set, the empirical frequencies must satisfy $\\left| \\frac{N(x|x^4)}{4} - p(x) \\right| \\leq \\frac{3}{10}$ for each $x$:\n- For $x=0$: $\\left| \\frac{n_0}{4} - \\frac{1}{2} \\right| \\leq \\frac{3}{10}$, which gives $0.8 \\leq n_0 \\leq 3.2$, so $n_0 \\in \\{1, 2, 3\\}$ (same as weak typicality).\n- For $x=1$: $\\left| \\frac{n_1}{4} - \\frac{1}{4} \\right| \\leq \\frac{3}{10} \\implies |n_1 - 1| \\leq 1.2 \\implies -0.2 \\leq n_1 \\leq 2.2$. Since $n_1$ is a nonnegative integer, $n_1 \\in \\{0, 1, 2\\}$.\n- For $x=2$: similarly, $\\left| \\frac{n_2}{4} - \\frac{1}{4} \\right| \\leq \\frac{3}{10} \\implies n_2 \\in \\{0, 1, 2\\}$.\n\nAdditionally, $n_0 + n_1 + n_2 = 4$. A sequence is not strongly typical if $n_1 \\notin \\{0, 1, 2\\}$ or $n_2 \\notin \\{0, 1, 2\\}$, but since $n_1, n_2 \\geq 0$, the only violations occur when $n_1 > 2$ or $n_2 > 2$. Given the constraints:\n- If $n_0 = 1$, then $n_1 + n_2 = 3$. Possible pairs $(n_1, n_2)$: $(0,3)$, $(1,2)$, $(2,1)$, $(3,0)$. The pairs violating strong typicality are $(0,3)$ (since $n_2=3 > 2$) and $(3,0)$ (since $n_1=3 > 2$).\n- If $n_0 = 2$, then $n_1 + n_2 = 2$. Possible pairs: $(0,2)$, $(1,1)$, $(2,0)$. All satisfy $n_1 \\leq 2$, $n_2 \\leq 2$.\n- If $n_0 = 3$, then $n_1 + n_2 = 1$. Possible pairs: $(0,1)$, $(1,0)$. All satisfy $n_1 \\leq 2$, $n_2 \\leq 2$.\n\nThus, only sequences with $n_0=1$ and $(n_1, n_2) = (0,3)$ or $(3,0)$ are weakly typical but not strongly typical.\n\nThe probability for a sequence with counts $(n_0, n_1, n_2)$ is:\n\n$$\np(x^4) = \\left( \\frac{1}{2} \\right)^{n_0} \\left( \\frac{1}{4} \\right)^{n_1} \\left( \\frac{1}{4} \\right)^{n_2}.\n$$\n\nThe number of such sequences is the multinomial coefficient:\n\n$$\n\\binom{4}{n_0, n_1, n_2} = \\frac{4!}{n_0! \\, n_1! \\, n_2!}.\n$$\n\n\nFor $(n_0, n_1, n_2) = (1, 0, 3)$:\n\n$$\n\\text{Number of sequences} = \\frac{4!}{1! \\, 0! \\, 3!} = \\frac{24}{1 \\cdot 1 \\cdot 6} = 4,\n$$\n\n\n$$\n\\text{Probability per sequence} = \\left( \\frac{1}{2} \\right)^1 \\left( \\frac{1}{4} \\right)^0 \\left( \\frac{1}{4} \\right)^3 = \\frac{1}{2} \\cdot 1 \\cdot \\frac{1}{64} = \\frac{1}{128},\n$$\n\n\n$$\n\\text{Total probability} = 4 \\cdot \\frac{1}{128} = \\frac{4}{128} = \\frac{1}{32}.\n$$\n\n\nFor $(n_0, n_1, n_2) = (1, 3, 0)$:\n\n$$\n\\text{Number of sequences} = \\frac{4!}{1! \\, 3! \\, 0!} = \\frac{24}{1 \\cdot 6 \\cdot 1} = 4,\n$$\n\n\n$$\n\\text{Probability per sequence} = \\left( \\frac{1}{2} \\right)^1 \\left( \\frac{1}{4} \\right)^3 \\left( \\frac{1}{4} \\right)^0 = \\frac{1}{2} \\cdot \\frac{1}{64} \\cdot 1 = \\frac{1}{128},\n$$\n\n\n$$\n\\text{Total probability} = 4 \\cdot \\frac{1}{128} = \\frac{4}{128} = \\frac{1}{32}.\n$$\n\n\nThe total probability for sequences that are weakly typical but not strongly typical is the sum:\n\n$$\n\\frac{1}{32} + \\frac{1}{32} = \\frac{2}{32} = \\frac{1}{16}.\n$$", "answer": "$$ \\boxed{\\dfrac{1}{16}} $$", "id": "56805"}, {"introduction": "Beyond the introductory definitions, typicality can be elegantly described using the Kullback-Leibler (KL) divergence, which provides a geometric perspective on the space of probability distributions. This advanced exercise places you on the boundary of the strongly typical set and tasks you with a fascinating optimization problem: finding the empirical distribution that is \"maximally random,\" i.e., has the highest possible entropy, while remaining on the edge of typicality. Tackling this problem will connect the concept of typical sets to the powerful frameworks of large deviation theory and the maximum entropy principle, revealing deeper connections between information, probability, and statistical inference. [@problem_id:56796]", "problem": "In classical information theory, we consider a discrete memoryless source over a finite alphabet $\\mathcal{X} = \\{x_1, \\dots, x_m\\}$, characterized by a probability distribution $P = (p_1, \\dots, p_m)$, where $p_i = P(X=x_i)$.\n\nFor any probability distribution $Q = (q_1, \\dots, q_m)$ on $\\mathcal{X}$, its Shannon entropy is defined as:\n$$\nH(Q) = -\\sum_{i=1}^m q_i \\log_2 q_i\n$$\nThe Kullback-Leibler (KL) divergence, or relative entropy, between two distributions $Q$ and $P$ is given by:\n$$\nD(Q||P) = \\sum_{i=1}^m q_i \\log_2\\left(\\frac{q_i}{p_i}\\right)\n$$\nThe set of **strongly typical sequences** of length $n$ generated by the source $P$ is defined, for a given tolerance $\\delta > 0$, as the set of all sequences whose empirical probability distribution $Q$ is \"close\" to $P$, as measured by the KL divergence. Specifically, the empirical distribution $Q$ must satisfy $D(Q||P) \\le \\delta$.\n\nConsider a ternary memoryless source with the probability distribution $P = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\right)$. We are interested in the empirical distributions $Q=(q_1, q_2, q_3)$ that lie on the boundary of the strongly typical set, defined by the condition $D(Q||P) = \\delta$.\n\nYour task is to find the distribution $Q$ that maximizes the Shannon entropy $H(Q)$ subject to being on this boundary for a specific tolerance value of $\\delta = \\frac{5}{3} - \\log_2 3$.\n\nWhat is the value of this maximum entropy, $H(Q)$?", "solution": "1. Relevant definitions  \nThe Shannon entropy and KL divergence (base-2 logs) for $Q=(q_i)$ and $P=(p_i)$ are  \n$$H(Q)=-\\sum_{i=1}^3q_i\\log_2q_i,\\qquad\nD(Q\\|P)=\\sum_{i=1}^3q_i\\log_2\\frac{q_i}{p_i}\\,. $$\n\n2. Lagrange optimization  \nMaximize $H(Q)$ subject to $D(Q\\|P)=\\delta$ and $\\sum_iq_i=1$.  The Lagrangian is  \n$$\\mathcal L=-\\sum_iq_i\\log_2q_i\n-\\lambda\\Bigl(\\sum_iq_i\\log_2\\frac{q_i}{p_i}-\\delta\\Bigr)\n-\\mu\\Bigl(\\sum_iq_i-1\\Bigr)\\,. $$  \nStationarity $\\partial\\mathcal L/\\partial q_i=0$ gives  \n$$-(1+\\log_2q_i)-\\lambda\\bigl(1+\\log_2\\tfrac{q_i}{p_i}\\bigr)-\\mu=0\n\\;\\Rightarrow\\;q_i\\propto p_i^{\\frac{\\lambda}{1+\\lambda}}\\,. $$\n\n3. Parametric form  \nSet $A=\\frac{\\lambda}{1+\\lambda}$ and $S(A)=\\sum_ip_i^A$.  Then  \n$$q_i=\\frac{p_i^A}{S(A)}\\,. $$\n\n4. Impose the boundary condition $D(Q\\|P)=\\delta$  \nOne finds that $A=0$ solves  \n$$D\\Bigl(\\tfrac1{3},\\tfrac1{3},\\tfrac1{3}\\Bigm\\|P\\Bigr)\n=\\tfrac13\\log_2\\frac{(1/3)^3}{(1/2)(1/4)^2}\n=\\frac53-\\log_2 3=\\delta\\,. $$\n\n5. Maximum-entropy distribution  \nAt $A=0$, $q_i=1/3$ for all $i$, which indeed maximizes $H(Q)$.\n\n6. Resulting entropy  \n$$H(Q)=H\\bigl(\\tfrac13,\\tfrac13,\\tfrac13\\bigr)=\\log_2 3\\,. $$", "answer": "$$\\boxed{\\log_2 3}$$", "id": "56796"}]}