{"hands_on_practices": [{"introduction": "Simulating charged particles in condensed matter typically involves periodic boundary conditions to model an infinite crystal. However, the long-range nature of the Coulomb interaction, which decays as $1/r$, makes a direct summation over periodic images conditionally convergent and computationally intractable. This practice delves into the Ewald summation method, an essential technique that resolves this issue by splitting the interaction into a short-range part calculated in real space and a long-range part calculated in reciprocal space. Deriving the Ewald decomposition from first principles is a foundational skill for correctly modeling electrostatic interactions in periodic systems, a prerequisite for most QMC simulations of realistic materials [@problem_id:3012316].", "problem": "Consider a many-body system of $N$ point charges $\\{q_{i}\\}_{i=1}^{N}$ at positions $\\{\\mathbf{r}_{i}\\}_{i=1}^{N}$ in a three-dimensional periodic supercell of volume $\\Omega$, with Bravais lattice vectors $\\{\\mathbf{R}\\}$ and reciprocal lattice vectors $\\{\\mathbf{k}\\}$. Assume tin-foil (conducting) boundary conditions for the long-wavelength limit and atomic units so that energies are measured in Hartree. The system is neutral, i.e., $\\sum_{i=1}^{N} q_{i} = 0$. The Coulomb interaction under periodic boundary conditions is to be treated using Ewaldâ€™s method, which splits the $1/r$ kernel into short-range and long-range contributions by introducing a Gaussian screening of width parameter $\\alpha>0$. Starting from the Poisson equation $\\nabla^{2}\\phi(\\mathbf{r}) = -4\\pi \\rho(\\mathbf{r})$, the Fourier representation of periodic functions, and the definition of the Gaussian screening charge density $\\rho_{s}(\\mathbf{r}) = \\frac{\\alpha^{3}}{\\pi^{3/2}} \\exp(-\\alpha^{2} r^{2})$, derive the Ewald decomposition for the Coulomb potential energy of the neutral periodic system. Your derivation must (i) show how the short-range real-space sum emerges from the screened pair potential and (ii) show how the long-range reciprocal-space sum emerges from the Fourier-space solution of the Poisson equation for the complementary smooth charge. Ignore any net-surface term consistent with tin-foil boundary conditions. \n\nPresent, as your final answer, the two closed-form analytic expressions for the real-space contribution and the reciprocal-space contribution to the Ewald Coulomb energy in terms of $\\{q_{i}\\}$, $\\{\\mathbf{r}_{i}\\}$, $\\alpha$, the set of lattice vectors $\\{\\mathbf{R}\\}$, the set of reciprocal lattice vectors $\\{\\mathbf{k}\\}$, and the cell volume $\\Omega$. Use a prime on lattice sums to indicate omission of self-interactions where applicable. Express your final pair of expressions in analytic form; no numerical evaluation or rounding is required. Energies should be understood in Hartree.", "solution": "The problem asks for the derivation of the Ewald decomposition for the Coulomb potential energy of a charge-neutral periodic system of $N$ point charges $\\{q_{i}\\}$ at positions $\\{\\mathbf{r}_{i}\\}$ in a volume $\\Omega$. The total electrostatic energy of this system, including interactions with all periodic images, is given by the conditionally convergent sum:\n$$\nE = \\frac{1}{2} \\sideset{}{'}\\sum_{i,j=1}^{N} \\sum_{\\mathbf{R}} \\frac{q_{i} q_{j}}{|\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|}\n$$\nwhere $\\{\\mathbf{R}\\}$ are the Bravais lattice vectors of the supercell. The prime on the sum indicates that the term for $i=j$ is excluded when $\\mathbf{R}=0$ to avoid the infinite self-energy of a point charge. In atomic units (Hartree for energy), the Coulomb kernel is $1/r$.\n\nThe Ewald method splits this slowly converging sum into two rapidly converging sums: one in real space and one in reciprocal space. This is achieved by splitting the interaction kernel $1/r$ into a short-range part and a long-range part using the identity $\\text{erf}(x) + \\text{erfc}(x) = 1$. We write:\n$$\n\\frac{1}{r} = \\frac{\\text{erfc}(\\alpha r)}{r} + \\frac{\\text{erf}(\\alpha r)}{r}\n$$\nwhere $\\alpha > 0$ is a parameter that controls the width of the splitting. The complementary error function, $\\text{erfc}(\\alpha r)$, decays rapidly to zero for $r > 1/\\alpha$, making it a short-range interaction. The error function, $\\text{erf}(\\alpha r)$, smoothly goes to $1$ for $r > 1/\\alpha$, so $\\text{erf}(\\alpha r)/r$ has the same long-range $1/r$ behavior, but is non-singular at $r=0$.\n\nSubstituting this split into the energy expression gives:\n$$\nE = \\frac{1}{2} \\sideset{}{'}\\sum_{i,j,\\mathbf{R}} \\frac{q_i q_j \\text{erfc}(\\alpha |\\mathbf{r}_{ij} - \\mathbf{R}|)}{|\\mathbf{r}_{ij} - \\mathbf{R}|} + \\frac{1}{2} \\sideset{}{'}\\sum_{i,j,\\mathbf{R}} \\frac{q_i q_j \\text{erf}(\\alpha |\\mathbf{r}_{ij} - \\mathbf{R}|)}{|\\mathbf{r}_{ij} - \\mathbf{R}|}\n$$\nwhere $\\mathbf{r}_{ij} = \\mathbf{r}_i - \\mathbf{r}_j$.\n\n**Part (i): Real-Space Contribution**\n\nThe first term is the real-space contribution, $E_{real}$. Due to the rapid decay of the $\\text{erfc}$ function, this sum converges quickly. We only need to sum over pairs and lattice vectors for which $|\\mathbf{r}_{ij} - \\mathbf{R}|$ is not much larger than $1/\\alpha$.\n$$\nE_{real} = \\frac{1}{2} \\sideset{}{'}\\sum_{i,j,\\mathbf{R}} \\frac{q_i q_j \\text{erfc}(\\alpha |\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|)}{|\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|}\n$$\nThis is the required expression for the short-range real-space sum. The prime indicates the omission of the divergent $i=j, \\mathbf{R}=0$ term.\n\n**Part (ii): Reciprocal-Space Contribution**\n\nThe second term, involving the `erf` function, is long-ranged and converges slowly in real space. It is efficiently calculated in reciprocal space. This term can be interpreted as the interaction energy of a set of smooth charge distributions. The potential corresponding to the interaction kernel $q_j \\text{erf}(\\alpha r)/r$ is generated by a Gaussian charge distribution, as dictated by the Poisson equation. The problem provides the form of this screening charge density: $\\rho_{s}(\\mathbf{r}) = \\frac{\\alpha^{3}}{\\pi^{3/2}} \\exp(-\\alpha^{2} r^{2})$. A point charge $q_j$ and its associated screening Gaussian charge $-q_j \\rho_s(\\mathbf{r})$ produce the screened potential $q_j \\text{erfc}(\\alpha r)/r$. The complementary \"smooth\" charge distribution for the entire system is therefore:\n$$\n\\rho_{long}(\\mathbf{r}) = \\sum_{i=1}^{N} \\sum_{\\mathbf{R}} q_{i} \\rho_{s}(\\mathbf{r} - \\mathbf{r}_{i} - \\mathbf{R})\n$$\nThe energy of this smooth, periodic charge distribution must be calculated. The real-space sum for the energy of this distribution, $\\frac{1}{2} \\sum_{i,j,\\mathbf{R}} q_i q_j \\text{erf}(\\alpha |\\mathbf{r}_{ij}-\\mathbf{R}|)/|\\mathbf{r}_{ij}-\\mathbf{R}|$, includes the self-interaction of each Gaussian cloud ($i=j, \\mathbf{R}=0$). To reconcile with our original prime-sum which excluded this, we write:\n$$\n\\frac{1}{2} \\sideset{}{'}\\sum_{i,j,\\mathbf{R}} \\frac{q_i q_j \\text{erf}(\\dots)}{|\\dots|} = \\left(\\frac{1}{2} \\sum_{i,j,\\mathbf{R}} \\frac{q_i q_j \\text{erf}(\\dots)}{|\\dots|}\\right) - \\frac{1}{2} \\sum_{i=1}^{N} q_i^2 \\lim_{r \\to 0} \\frac{\\text{erf}(\\alpha r)}{r}\n$$\nThe full sum (in parentheses) is the energy of the smooth charge distribution, which we will calculate in reciprocal space, and let's call it $E_{recip}$. The second term is a self-energy correction, $E_{self}$. Using the series expansion $\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}(x - \\frac{x^3}{3} + \\dots)$, we find $\\lim_{r \\to 0} \\frac{\\text{erf}(\\alpha r)}{r} = \\frac{2\\alpha}{\\sqrt{\\pi}}$. So, the self-energy correction is $E_{self} = -\\sum_{i=1}^{N} q_i^2 \\frac{\\alpha}{\\sqrt{\\pi}}$. The total energy is $E = E_{real} + E_{recip} + E_{self}$. The problem asks for the expressions for $E_{real}$ and $E_{recip}$.\n\nTo find $E_{recip}$, we solve the Poisson equation $\\nabla^{2}\\phi_{long}(\\mathbf{r}) = -4\\pi \\rho_{long}(\\mathbf{r})$ in Fourier space. For a periodic function $f(\\mathbf{r}) = \\sum_{\\mathbf{k}} \\tilde{f}(\\mathbf{k}) e^{i\\mathbf{k}\\cdot\\mathbf{r}}$, the Fourier coefficients are $\\tilde{f}(\\mathbf{k}) = \\frac{1}{\\Omega} \\int_{\\Omega} f(\\mathbf{r}) e^{-i\\mathbf{k}\\cdot\\mathbf{r}} d^3\\mathbf{r}$.\nThe Fourier coefficients of the charge density $\\rho_{long}$ are:\n$$\n\\tilde{\\rho}_{long}(\\mathbf{k}) = \\frac{1}{\\Omega} \\int_{\\Omega} \\left( \\sum_{j,\\mathbf{R}} q_j \\rho_s(\\mathbf{r}-\\mathbf{r}_j-\\mathbf{R}) \\right) e^{-i\\mathbf{k}\\cdot\\mathbf{r}} d^3\\mathbf{r}\n$$\nBy changing the integration variable and using the property $e^{i\\mathbf{k}\\cdot\\mathbf{R}}=1$ for reciprocal lattice vectors $\\mathbf{k}$ and lattice vectors $\\mathbf{R}$, the integral extends over all space:\n$$\n\\tilde{\\rho}_{long}(\\mathbf{k}) = \\frac{1}{\\Omega} \\left( \\sum_{j=1}^{N} q_j e^{-i\\mathbf{k}\\cdot\\mathbf{r}_j} \\right) \\left( \\int_{\\mathbb{R}^3} \\rho_s(\\mathbf{r}') e^{-i\\mathbf{k}\\cdot\\mathbf{r}'} d^3\\mathbf{r}' \\right)\n$$\nThe first parenthesis is the structure factor $S(\\mathbf{k}) = \\sum_{j=1}^{N} q_j e^{-i\\mathbf{k}\\cdot\\mathbf{r}_j}$. The second parenthesis is the Fourier transform of the Gaussian screening function $\\rho_s$.\n$$\n\\tilde{\\rho}_{s,FT}(\\mathbf{k}) = \\int_{\\mathbb{R}^3} \\frac{\\alpha^3}{\\pi^{3/2}} e^{-\\alpha^2 r'^2} e^{-i\\mathbf{k}\\cdot\\mathbf{r}'} d^3\\mathbf{r}' = \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\nThus, the Fourier coefficients of the charge density are:\n$$\n\\tilde{\\rho}_{long}(\\mathbf{k}) = \\frac{S(\\mathbf{k})}{\\Omega} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\nIn Fourier space, the Poisson equation becomes $-k^2 \\tilde{\\phi}_{long}(\\mathbf{k}) = -4\\pi \\tilde{\\rho}_{long}(\\mathbf{k})$, which gives for $\\mathbf{k}\\neq 0$:\n$$\n\\tilde{\\phi}_{long}(\\mathbf{k}) = \\frac{4\\pi}{k^2} \\tilde{\\rho}_{long}(\\mathbf{k}) = \\frac{4\\pi S(\\mathbf{k})}{\\Omega k^2} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\nFor $\\mathbf{k}=0$, the overall charge neutrality $\\sum q_i = 0$ implies $S(0)=0$, so $\\tilde{\\rho}_{long}(0)=0$. The problem states to use tin-foil boundary conditions, which means the $\\mathbf{k}=0$ contribution to the energy (the surface term) is zero. Thus we only need to sum over $\\mathbf{k} \\neq 0$.\n\nThe energy contribution from this smooth charge distribution is $E_{recip} = \\frac{1}{2} \\int_\\Omega \\rho_{long}(\\mathbf{r}) \\phi_{long}(\\mathbf{r}) d^3\\mathbf{r}$. Alternatively, it can be written as the sum of interactions of each charge with the total potential:\n$$\nE_{recip} = \\frac{1}{2} \\sum_{i=1}^{N} q_i \\phi_{long}(\\mathbf{r}_i) = \\frac{1}{2} \\sum_{i=1}^{N} q_i \\left( \\sum_{\\mathbf{k}\\neq 0} \\tilde{\\phi}_{long}(\\mathbf{k})e^{i\\mathbf{k}\\cdot\\mathbf{r}_i} \\right)\n$$\nReordering the sums:\n$$\nE_{recip} = \\frac{1}{2} \\sum_{\\mathbf{k}\\neq 0} \\tilde{\\phi}_{long}(\\mathbf{k}) \\left( \\sum_{i=1}^{N} q_i e^{i\\mathbf{k}\\cdot\\mathbf{r}_i} \\right)\n$$\nThe second parenthesis is $S(-\\mathbf{k}) = S(\\mathbf{k})^*$. Substituting the expression for $\\tilde{\\phi}_{long}(\\mathbf{k})$:\n$$\nE_{recip} = \\frac{1}{2} \\sum_{\\mathbf{k}\\neq 0} \\left( \\frac{4\\pi S(\\mathbf{k})}{\\Omega k^2} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right) \\right) S(\\mathbf{k})^* = \\frac{2\\pi}{\\Omega} \\sum_{\\mathbf{k}\\neq 0} \\frac{|S(\\mathbf{k})|^2}{k^2} \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$\nThis is the required expression for the reciprocal-space contribution.\n\nThe two requested components of the Ewald energy are:\n1. The real-space contribution:\n$$\nE_{real} = \\frac{1}{2} \\sideset{}{'}\\sum_{i,j=1}^{N}\\sum_{\\mathbf{R}} \\frac{q_i q_j \\text{erfc}(\\alpha |\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|)}{|\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|}\n$$\n2. The reciprocal-space contribution:\n$$\nE_{recip} = \\frac{2\\pi}{\\Omega} \\sum_{\\mathbf{k}\\neq 0} \\frac{1}{k^2} \\left| \\sum_{j=1}^N q_j e^{-i\\mathbf{k}\\cdot\\mathbf{r}_j} \\right|^2 \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right)\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\displaystyle \\frac{1}{2} \\sideset{}{'}\\sum_{i,j,\\mathbf{R}} \\frac{q_i q_j \\text{erfc}(\\alpha |\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|)}{|\\mathbf{r}_{i} - \\mathbf{r}_{j} - \\mathbf{R}|} & \\displaystyle \\frac{2\\pi}{\\Omega} \\sum_{\\mathbf{k}\\neq 0} \\frac{1}{k^2} \\left| \\sum_{j=1}^N q_j \\exp(-i\\mathbf{k}\\cdot\\mathbf{r}_j) \\right|^2 \\exp\\left(-\\frac{k^2}{4\\alpha^2}\\right) \\end{pmatrix}}\n$$", "id": "3012316"}, {"introduction": "Quantum Monte Carlo methods gain their power by sampling configurations from a probability distribution related to the system's wavefunction, $\\Psi$. To improve efficiency, we often employ importance sampling, which uses a more tractable trial wavefunction, $\\Psi_T$, to guide the simulation toward physically relevant regions of the configuration space. This exercise addresses the crucial statistical consequences of this approach, specifically the bias that arises when the sampling distribution does not perfectly match the target distribution. By deriving the proper reweighting scheme and calculating the bias for an illustrative scenario, you will gain a deep, practical understanding of how to obtain unbiased estimators, a cornerstone of Variational and Diffusion Monte Carlo methods [@problem_id:3012335].", "problem": "Consider a Quantum Monte Carlo (QMC) estimate of an observable for a many-body state. Let $x$ denote a configuration variable drawn from a target probability density $p(x)$ proportional to the squared magnitude of a trial wavefunction for a subsystem of an interacting fermionic lattice in equilibrium. In practice, one often samples from an importance distribution $q(x)$ that is easier to draw from but does not match $p(x)$. The expectation value of an observable $O(x)$ under $p(x)$ is defined by the basic probability rule $ \\langle O \\rangle_{p} = \\int O(x)\\, p(x)\\, dx$, and samples $\\{x_i\\}_{i=1}^{N}$ are independently drawn from $q(x)$.\n\nStarting from the fundamental change-of-measure identity and properties of expectations, derive a sampling estimator for $\\langle O \\rangle_{p}$ that remains valid even when $p(x)$ is only known up to a multiplicative normalization constant and $q(x)$ is mismatched but has support wherever $p(x)$ is nonzero. Analyze the bias that arises if one instead computes the naive sample mean $\\frac{1}{N}\\sum_{i=1}^{N} O(x_i)$ using samples from $q(x)$ without any reweighting, and express this bias in terms of expectations with respect to $q(x)$ and $p(x)$.\n\nTo make the bias concrete, consider a one-dimensional marginal of the many-body configuration for which $p(x)$ is the standard normal density $p(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$, while sampling is performed from a Gaussian importance distribution $q(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)$ with parameters $\\mu \\in \\mathbb{R}$ and $\\sigma>0$. Let the observable be $O(x)=x^{2}$. Using only well-tested properties of Gaussian integrals and the definitions above, compute the exact bias of the naive estimator, defined as $\\mathbb{E}_{q}[O(x)] - \\mathbb{E}_{p}[O(x)]$, and provide your final answer as a single closed-form analytic expression in terms of $\\mu$ and $\\sigma$. No units are required. If simplification is possible, present the simplified expression. The final answer must be one analytic expression, without additional commentary or intermediate steps.", "solution": "The solution first requires deriving the general importance sampling estimator, then analyzing its bias, and finally calculating the bias for the specific case provided.\n\n**1. Importance Sampling Estimator**\n\nThe expectation of an observable $O(x)$ with respect to the probability density $p(x)$ is defined as:\n$$ \\langle O \\rangle_{p} = \\int O(x) p(x) \\, dx $$\nTo use samples from a different distribution $q(x)$, we perform a change of measure:\n$$ \\langle O \\rangle_{p} = \\int O(x) \\frac{p(x)}{q(x)} q(x) \\, dx = \\mathbb{E}_{q}\\left[ O(x) \\frac{p(x)}{q(x)} \\right] $$\nThe term $w(x) = p(x)/q(x)$ is the importance weight. For $N$ samples $\\{x_i\\}$ from $q(x)$, an estimator is $\\hat{O}_N = \\frac{1}{N} \\sum_{i=1}^{N} O(x_i) w(x_i)$.\n\nWhen $p(x)$ is only known up to a normalization constant, $p(x) \\propto \\tilde{p}(x)$, the expectation is a ratio:\n$$ \\langle O \\rangle_{p} = \\frac{\\int O(x) \\tilde{p}(x) \\, dx}{\\int \\tilde{p}(x) \\, dx} = \\frac{\\mathbb{E}_{q}\\left[ O(x) \\frac{\\tilde{p}(x)}{q(x)} \\right]}{\\mathbb{E}_{q}\\left[ \\frac{\\tilde{p}(x)}{q(x)} \\right]} $$\nThis leads to the self-normalized importance sampling (SNIS) estimator, using unnormalized weights $\\tilde{w}(x_i) = \\tilde{p}(x_i)/q(x_i)$:\n$$ \\hat{O}_{\\text{SNIS}} = \\frac{\\sum_{i=1}^{N} O(x_i) \\tilde{w}(x_i)}{\\sum_{j=1}^{N} \\tilde{w}(x_j)} $$\n\n**2. Bias of the Naive Estimator**\n\nThe naive estimator uses samples from $q(x)$ without reweighting: $\\hat{O}_{\\text{naive}} = \\frac{1}{N} \\sum_{i=1}^{N} O(x_i)$. Its expectation value is:\n$$ \\mathbb{E}[\\hat{O}_{\\text{naive}}] = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} O(x_i)\\right] = \\mathbb{E}_q[O(x)] $$\nThe bias is the difference between this expectation and the true value, $\\langle O \\rangle_p = \\mathbb{E}_p[O(x)]$:\n$$ \\text{Bias} = \\mathbb{E}_q[O(x)] - \\mathbb{E}_p[O(x)] $$\n\n**3. Bias Calculation for the Specific Case**\n\nWe are given:\n-   Target distribution $p(x)$: Standard normal $\\mathcal{N}(0, 1)$.\n-   Sampling distribution $q(x)$: General normal $\\mathcal{N}(\\mu, \\sigma^2)$.\n-   Observable $O(x)$: $x^2$.\n\nWe use the identity $\\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2$.\n\nFor the target distribution $p(x)$, the mean is 0 and the variance is 1.\n$$ \\mathbb{E}_p[x^2] = 1 + 0^2 = 1 $$\n\nFor the sampling distribution $q(x)$, the mean is $\\mu$ and the variance is $\\sigma^2$.\n$$ \\mathbb{E}_q[x^2] = \\sigma^2 + \\mu^2 $$\n\nThe bias is therefore the difference between these two expectations:\n$$ \\text{Bias} = \\mathbb{E}_q[x^2] - \\mathbb{E}_p[x^2] = (\\mu^2 + \\sigma^2) - 1 $$\nThis is the final expression for the bias. It is zero only when $\\mu=0$ and $\\sigma=1$, i.e., when $q(x)=p(x)$.", "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2} - 1}$$", "id": "3012335"}, {"introduction": "While QMC simulations excel at calculating expectation values of observables like energy, many key thermodynamic quantities, such as the Helmholtz free energy $F$ or entropy $S$, are not direct observables. This practice introduces thermodynamic integration, a powerful post-processing technique used to compute these quantities from a series of energy measurements performed at different temperatures. You will implement a numerical integration of the energy with respect to inverse temperature $\\beta$ to find the entropy, and, critically, you will learn to propagate the statistical uncertainties from the initial QMC data to the final result, ensuring the scientific reliability of your conclusions [@problem_id:3012293].", "problem": "You are given discrete-temperature Quantum Monte Carlo (QMC) measurements of the equilibrium energy for a many-body system. In units where the Boltzmann constant $k_{B}=1$, you must implement an entropy estimator based on thermodynamic integration and derive its statistical error from QMC data across inverse temperatures $\\beta=1/T$. You will work from the following foundational definitions and identities of equilibrium statistical mechanics: for the partition function $Z(\\beta)$, the Helmholtz free energy $F(\\beta)$ satisfies $F(\\beta)=-\\beta^{-1}\\ln Z(\\beta)$ and $$\\frac{d}{d\\beta}\\bigl(\\beta F(\\beta)\\bigr)=\\langle E\\rangle(\\beta),$$ where $\\langle E\\rangle(\\beta)$ is the thermal expectation of the energy. The thermodynamic entropy obeys $F=E-TS$, so $$S(\\beta)=\\beta\\bigl(\\langle E\\rangle(\\beta)-F(\\beta)\\bigr).$$\n\nStarting from these definitions, derive an estimator for $S(\\beta_{i})$ using discrete QMC data $\\{(\\beta_{j},\\langle E\\rangle_{j},\\sigma_{E,j})\\}_{j=0}^{i}$ and a known baseline value $B_{0}=\\beta_{0}F(\\beta_{0})$ at the lowest available inverse temperature $\\beta_{0}$. Combine the identities to write\n$$\\beta F(\\beta)=\\beta_{0}F(\\beta_{0})+\\int_{\\beta_{0}}^{\\beta}\\langle E\\rangle(\\beta')\\,d\\beta'.$$\nTherefore, with $B_{0}$ given, the target $S(\\beta_{i})$ is\n$$S(\\beta_{i})=\\beta_{i}\\langle E\\rangle_{i}-B_{0}-\\int_{\\beta_{0}}^{\\beta_{i}}\\langle E\\rangle(\\beta')\\,d\\beta'.$$\nImplement the integral using the composite trapezoidal rule on the discrete grid $\\beta_{0}<\\beta_{1}<\\cdots<\\beta_{i}$:\n$$\\int_{\\beta_{0}}^{\\beta_{i}}\\langle E\\rangle(\\beta')\\,d\\beta' \\approx \\sum_{j=0}^{i-1}\\frac{\\beta_{j+1}-\\beta_{j}}{2}\\bigl(\\langle E\\rangle_{j}+\\langle E\\rangle_{j+1}\\bigr),$$\nwhich can be rewritten as a linear combination $\\sum_{j=0}^{i}w_{j}^{(i)}\\langle E\\rangle_{j}$ with weights\n$$w_{0}^{(i)}=\\frac{\\beta_{1}-\\beta_{0}}{2},\\quad w_{j}^{(i)}=\\frac{\\beta_{j+1}-\\beta_{j}}{2}+\\frac{\\beta_{j}-\\beta_{j-1}}{2}\\ \\text{for}\\ 1\\le j\\le i-1,\\quad w_{i}^{(i)}=\\frac{\\beta_{i}-\\beta_{i-1}}{2}.$$\nAssume independent Gaussian statistical errors for the energy estimates at different temperatures, with standard deviations $\\sigma_{E,j}$. Derive the variance of the integral and of $S(\\beta_{i})$ under the trapezoidal estimator. Using linear error propagation, the integral variance is\n$$\\mathrm{Var}\\!\\left(\\sum_{j=0}^{i}w_{j}^{(i)}\\langle E\\rangle_{j}\\right)=\\sum_{j=0}^{i}\\left(w_{j}^{(i)}\\right)^{2}\\sigma_{E,j}^{2}.$$\nBecause $S(\\beta_{i})=\\beta_{i}\\langle E\\rangle_{i}-B_{0}-\\sum_{j=0}^{i}w_{j}^{(i)}\\langle E\\rangle_{j}$, the covariance between $\\beta_{i}\\langle E\\rangle_{i}$ and the integral contributes. Under independence across temperatures, $\\mathrm{Cov}(\\langle E\\rangle_{i},\\sum_{j}w_{j}^{(i)}\\langle E\\rangle_{j})=w_{i}^{(i)}\\sigma_{E,i}^{2}$. Therefore,\n$$\\mathrm{Var}\\bigl(S(\\beta_{i})\\bigr)=\\beta_{i}^{2}\\sigma_{E,i}^{2}+\\sum_{j=0}^{i}\\left(w_{j}^{(i)}\\right)^{2}\\sigma_{E,j}^{2}-2\\beta_{i}w_{i}^{(i)}\\sigma_{E,i}^{2}.$$\nYour program must:\n- For each test case, compute the vector of entropies $S(\\beta_{i})$ for all indices $i$ in the provided grid using the trapezoidal estimator and the corresponding standard uncertainties given by the square root of the above variance.\n- Accept that either a baseline $B_{0}=\\beta_{0}F(\\beta_{0})$ is provided directly or, if $\\beta_{0}=0$, that the Hilbert space dimension $D$ is known; in the latter case use $B_{0}=-\\ln D$ since $\\beta F(\\beta)=-\\ln Z(\\beta)$ and $Z(0)=D$.\n- Express each entropy in units of $k_{B}$ (dimensionless in the adopted units). No rounding requirement is imposed; report floating-point results.\n- Use radians or degrees does not apply.\n\nInput is embedded in the program as the following test suite of parameter sets (each test case includes arrays of inverse temperatures $\\beta$, mean energies $\\langle E\\rangle$, their standard deviations $\\sigma_{E}$, and either $D$ or $B_{0}$):\n1. Case A (happy path, finite Hilbert space, baseline at $\\beta_{0}=0$):\n   - $\\beta=[0.0,0.5,1.0,2.0]$\n   - $\\langle E\\rangle=[0.0,-0.35,-0.62,-0.95]$\n   - $\\sigma_{E}=[0.01,0.02,0.02,0.03]$\n   - $D=4$.\n2. Case B (boundary, two-point grid, baseline at $\\beta_{0}=0$):\n   - $\\beta=[0.0,3.0]$\n   - $\\langle E\\rangle=[0.0,-1.2]$\n   - $\\sigma_{E}=[0.02,0.05]$\n   - $D=8$.\n3. Case C (nonzero baseline at $\\beta_{0}>0$, no knowledge of $D$):\n   - $\\beta=[0.4,0.9,1.1,1.6]$\n   - $\\langle E\\rangle=[-0.2,-0.41,-0.46,-0.61]$\n   - $\\sigma_{E}=[0.03,0.04,0.04,0.05]$\n   - $B_{0}=-0.25$.\n4. Case D (edge, irregular grid with large errors, baseline at $\\beta_{0}=0$):\n   - $\\beta=[0.0,0.1,0.5,0.8]$\n   - $\\langle E\\rangle=[0.0,-0.05,-0.30,-0.45]$\n   - $\\sigma_{E}=[0.20,0.15,0.10,0.10]$\n   - $D=64$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list: the first element is the list of $S(\\beta_{i})$ values for that case, and the second element is the list of their standard uncertainties. For example, the output format must be like\n\"[[[S_A_0,S_A_1,...],[sigma_A_0,sigma_A_1,...]],[[S_B_0,S_B_1],[sigma_B_0,sigma_B_1]],...]\" with no additional text.", "solution": "The solution implements an estimator for the thermodynamic entropy, $S$, and its statistical uncertainty from discrete Quantum Monte Carlo (QMC) data. The calculation is based on the principles of equilibrium statistical mechanics, using units where the Boltzmann constant $k_{B}=1$.\n\nThe derivation starts with the fundamental definition of the Helmholtz free energy, $F(\\beta)$, at an inverse temperature $\\beta=1/T$:\n$$F(\\beta) = -\\frac{1}{\\beta} \\ln Z(\\beta)$$\nwhere $Z(\\beta)$ is the canonical partition function. This is equivalent to $\\beta F(\\beta) = -\\ln Z(\\beta)$. The thermal expectation value of the energy, $\\langle E \\rangle(\\beta)$, is related to the free energy through the thermodynamic identity:\n$$\\frac{d}{d\\beta}\\bigl(\\beta F(\\beta)\\bigr) = \\langle E\\rangle(\\beta)$$\nIntegrating this identity from a reference inverse temperature $\\beta_0$ to a target inverse temperature $\\beta$ yields:\n$$\\beta F(\\beta) - \\beta_0 F(\\beta_0) = \\int_{\\beta_0}^{\\beta} \\langle E\\rangle(\\beta') \\,d\\beta'$$\nThe thermodynamic entropy, $S(\\beta)$, is defined by the relation $F = \\langle E \\rangle - TS$, which can be rewritten as:\n$$S(\\beta) = \\beta\\bigl(\\langle E\\rangle(\\beta) - F(\\beta)\\bigr)$$\nSubstituting the integrated expression for $\\beta F(\\beta)$ into the entropy equation gives the desired estimator for $S$ at a discrete point $\\beta_i$:\n$$S(\\beta_i) = \\beta_i \\langle E\\rangle_i - \\beta_i F(\\beta_i) = \\beta_i \\langle E\\rangle_i - \\left( \\beta_0 F(\\beta_0) + \\int_{\\beta_0}^{\\beta_i} \\langle E\\rangle(\\beta') \\,d\\beta' \\right)$$\nLetting $B_0 = \\beta_0 F(\\beta_0)$ be the known baseline value, the expression for the entropy estimator becomes:\n$$S(\\beta_i) = \\beta_i \\langle E\\rangle_i - B_0 - \\int_{\\beta_0}^{\\beta_i} \\langle E\\rangle(\\beta') \\,d\\beta'$$\nThe baseline $B_0$ is either provided directly or determined from the high-temperature limit ($\\beta \\to 0$). In this limit, the partition function $Z(0)$ equals the total number of states, i.e., the Hilbert space dimension $D$. Thus, if $\\beta_0=0$, we have $B_0 = \\lim_{\\beta \\to 0} \\beta F(\\beta) = -\\ln Z(0) = -\\ln D$. At $\\beta_0=0$, the entropy is $S(0) = \\ln D$, and its statistical uncertainty is zero as it is a defining property of the system, not a measured quantity.\n\nThe algorithmic design proceeds by implementing this formula for a discrete set of QMC data points $\\{(\\beta_j, \\langle E\\rangle_j, \\sigma_{E,j})\\}_{j=0}^{M-1}$. The integral is approximated using the composite trapezoidal rule. For a target point $\\beta_i$, the integral is:\n$$\\int_{\\beta_0}^{\\beta_i} \\langle E\\rangle(\\beta') \\,d\\beta' \\approx \\sum_{j=0}^{i-1} \\frac{\\beta_{j+1}-\\beta_j}{2} \\bigl(\\langle E\\rangle_j + \\langle E\\rangle_{j+1}\\bigr) = \\sum_{j=0}^{i} w_j^{(i)} \\langle E\\rangle_j$$\nThe weights $w_j^{(i)}$ are given by the problem statement: $w_0^{(i)} = \\frac{\\beta_1-\\beta_0}{2}$, $w_j^{(i)} = \\frac{\\beta_{j+1}-\\beta_{j-1}}{2}$ for $1 \\le j < i$, and $w_i^{(i)} = \\frac{\\beta_i-\\beta_{i-1}}{2}$.\n\nTo determine the statistical uncertainty of $S(\\beta_i)$, we use linear error propagation. The energy estimates $\\langle E\\rangle_j$ at different temperatures are assumed to be independent random variables with variances $\\sigma_{E,j}^2$. The entropy estimator can be written as a linear combination of these variables:\n$$S(\\beta_i) = -B_0 - \\sum_{j=0}^{i-1} w_j^{(i)} \\langle E\\rangle_j + (\\beta_i - w_i^{(i)}) \\langle E\\rangle_i$$\nThe variance $\\mathrm{Var}(S(\\beta_i))$ is the sum of the variances of each term, as $B_0$ is a constant and the $\\langle E\\rangle_j$ are independent:\n$$\\mathrm{Var}\\bigl(S(\\beta_i)\\bigr) = \\sum_{j=0}^{i} c_j^2 \\sigma_{E,j}^2$$\nwhere the coefficients $c_j$ are given by $c_j = -w_j^{(i)}$ for $j < i$, and $c_i = \\beta_i - w_i^{(i)}$. This yields the final expression for the variance:\n$$\\mathrm{Var}\\bigl(S(\\beta_i)\\bigr) = \\left( \\sum_{j=0}^{i-1} (w_j^{(i)})^2 \\sigma_{E,j}^2 \\right) + (\\beta_i - w_i^{(i)})^2 \\sigma_{E,i}^2$$\nFor the starting point $i=0$, the integral is zero, so $S(\\beta_0) = \\beta_0 \\langle E\\rangle_0 - B_0$. The variance is simply $\\mathrm{Var}(S(\\beta_0)) = \\beta_0^2 \\sigma_{E,0}^2$.\n\nThe implemented program iterates through each point $\\beta_i$ of the input grid ($i=0, \\dots, M-1$). For each $i$, it first computes the value of the trapezoidal integral up to that point. It then computes the entropy $S(\\beta_i)$. Subsequently, it calculates the weights $w_j^{(i)}$ for $j=0, \\dots, i$ and uses these to compute $\\mathrm{Var}(S(\\beta_i))$ according to the derived formula. The standard uncertainty is the square root of this variance. This procedure is applied to each test case, and the results are formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"beta\": [0.0, 0.5, 1.0, 2.0],\n            \"energy\": [0.0, -0.35, -0.62, -0.95],\n            \"sigma_E\": [0.01, 0.02, 0.02, 0.03],\n            \"D\": 4,\n            \"B0\": None,\n        },\n        {\n            \"beta\": [0.0, 3.0],\n            \"energy\": [0.0, -1.2],\n            \"sigma_E\": [0.02, 0.05],\n            \"D\": 8,\n            \"B0\": None,\n        },\n        {\n            \"beta\": [0.4, 0.9, 1.1, 1.6],\n            \"energy\": [-0.2, -0.41, -0.46, -0.61],\n            \"sigma_E\": [0.03, 0.04, 0.04, 0.05],\n            \"D\": None,\n            \"B0\": -0.25,\n        },\n        {\n            \"beta\": [0.0, 0.1, 0.5, 0.8],\n            \"energy\": [0.0, -0.05, -0.30, -0.45],\n            \"sigma_E\": [0.20, 0.15, 0.10, 0.10],\n            \"D\": 64,\n            \"B0\": None,\n        },\n    ]\n\n    all_case_results = []\n\n    for case in test_cases:\n        betas = np.array(case[\"beta\"], dtype=float)\n        energies = np.array(case[\"energy\"], dtype=float)\n        sigmas_E = np.array(case[\"sigma_E\"], dtype=float)\n        D = case[\"D\"]\n        B0_val = case[\"B0\"]\n\n        M = len(betas)\n        entropies = np.zeros(M)\n        uncertainties = np.zeros(M)\n        \n        # Determine the baseline value B0\n        if D is not None:\n            # If Hilbert space dimension D is given, B0 = -ln(D).\n            # This applies when beta_0 = 0.\n            B0 = -np.log(D)\n        else:\n            # Otherwise, B0 is given directly.\n            B0 = B0_val\n\n        # Iterate through each temperature point i to calculate S(beta_i)\n        for i in range(M):\n            beta_i = betas[i]\n            energy_i = energies[i]\n            sigma_E_i = sigmas_E[i]\n\n            # --- Entropy Calculation ---\n            # The integral is computed using the composite trapezoidal rule from beta_0 to beta_i.\n            integral_val = 0.0\n            if i > 0:\n                for j in range(i):\n                    # Sum of trapezoid areas from j to j+1\n                    h = betas[j+1] - betas[j]\n                    avg_E = (energies[j] + energies[j+1])\n                    integral_val += h * avg_E / 2.0\n            \n            # S(beta_i) = beta_i * <E>_i - B0 - integral\n            entropies[i] = beta_i * energy_i - B0 - integral_val\n            \n            # --- Variance and Uncertainty Calculation ---\n            if i == 0:\n                # For i=0, integral is 0. S_0 = beta_0 * E_0 - B0.\n                # Var(S_0) = Var(beta_0 * E_0) = beta_0^2 * sigma_E_0^2.\n                var_S_i = (beta_i**2) * (sigma_E_i**2)\n            else:\n                # For i>0, use the full error propagation formula.\n                # Var(S_i) = (beta_i - w_i)^2 * sigma_E_i^2 + sum_{j=0}^{i-1} w_j^2 * sigma_E_j^2\n                \n                # Calculate weights w_j^(i) for the integral from beta_0 to beta_i\n                weights = np.zeros(i + 1)\n                weights[0] = (betas[1] - betas[0]) / 2.0\n                for j in range(1, i):\n                    weights[j] = (betas[j+1] - betas[j-1]) / 2.0\n                weights[i] = (betas[i] - betas[i-1]) / 2.0\n\n                # Variance from the i-th energy measurement\n                var_term_i = ((beta_i - weights[i])**2) * (sigma_E_i**2)\n\n                # Variance from all other energy measurements (j < i)\n                var_sum_rest = np.sum(weights[:i]**2 * sigmas_E[:i]**2)\n\n                var_S_i = var_term_i + var_sum_rest\n\n            uncertainties[i] = np.sqrt(var_S_i)\n\n        all_case_results.append([entropies.tolist(), uncertainties.tolist()])\n    \n    # Format the final output string to match the required format exactly.\n    def format_single_case_result(res):\n        s_list_str = f\"[{','.join(map(str, res[0]))}]\"\n        sigma_list_str = f\"[{','.join(map(str, res[1]))}]\"\n        return f\"[{s_list_str},{sigma_list_str}]\"\n    \n    final_strings = [format_single_case_result(r) for r in all_case_results]\n    print(f\"[{','.join(final_strings)}]\")\n\nsolve()\n```", "id": "3012293"}]}