{"hands_on_practices": [{"introduction": "The cornerstone of the scaling theory of localization is the idea that a single, scale-dependent parameter—the dimensionless conductance $g$—determines a disordered system's electronic properties. This first practice takes you back to the conceptual origins of this parameter, known as the Thouless conductance. By starting from fundamental physical principles connecting diffusion, energy broadening, and level spacing, you will derive the crucial scaling relationship for conductance, $g \\propto L^{d-2}$, which underpins the entire theory [@problem_id:3014241]. Completing this derivation is essential for understanding how microscopic material properties and spatial dimensionality conspire to dictate whether a material behaves as a metal or an insulator.", "problem": "Consider a $d$-dimensional hypercubic sample of side length $L$ containing non-interacting electrons subject to weak, quenched disorder that places the dynamics in the diffusive regime, with elastic mean free path $\\ell$ satisfying $\\ell \\ll L$ and negligible inelastic dephasing on the scale $L$. Let $D$ denote the diffusion constant and let $\\nu$ denote the single-particle density of states per unit volume at the Fermi energy. In the scaling theory of localization, the Thouless conductance (also called the Thouless number) $g_T$ is defined as the ratio of the characteristic energy scale associated with diffusion across the sample to the mean single-particle level spacing of the finite system.\n\nStarting only from foundational definitions that relate (i) the diffusive traversal of a length scale to a characteristic time, (ii) the connection between an energy scale and a characteristic time, and (iii) the relation between the density of states and the mean level spacing in a finite volume, derive an explicit expression for $g_T$ in terms of $L$, $D$, $\\nu$, and $d$. Then, extract its scaling with $L$ and confirm the power-law behavior in $L$ expected in $d$ spatial dimensions. Express your final answer as a single closed-form analytic expression for $g_T(L,D,\\nu,d)$, with no numerical evaluation required.", "solution": "The problem requires the derivation of an expression for the Thouless conductance, $g_T$, for a $d$-dimensional disordered system, based on its definition and a set of foundational physical relationships.\n\nFirst, we state the definition of the Thouless conductance $g_T$ as provided in the problem statement. It is the ratio of the Thouless energy, $E_c$, to the mean single-particle level spacing, $\\Delta E$:\n$$\ng_T = \\frac{E_c}{\\Delta E}\n$$\nWe proceed by deriving expressions for the numerator, $E_c$, and the denominator, $\\Delta E$, independently.\n\nThe Thouless energy, $E_c$, is the characteristic energy scale associated with the diffusion of an electron across the entire sample. This process is governed by a characteristic time, the Thouless time $\\tau_D$.\n(i) The first foundational principle relates the diffusive traversal of a length scale to a characteristic time. For a diffusive process characterized by a diffusion constant $D$, the mean-square displacement $\\langle r^2 \\rangle$ after a time $t$ is given by a relation of the form $\\langle r^2 \\rangle \\propto D t$. The characteristic time for an electron to diffuse across a hypercubic sample of side length $L$ is therefore obtained by setting the characteristic displacement to $L$. This gives the Thouless time $\\tau_D$ as:\n$$\n\\tau_D = \\frac{L^2}{D}\n$$\nThis relation captures the scaling of diffusion time with system size and is fundamental to the study of transport in disordered media.\n\n(ii) The second foundational principle connects this characteristic time to an energy scale. The finite time $\\tau_D$ that a particle spends within the sample volume leads to a broadening of its energy levels. This energy broadening is the Thouless energy $E_c$. The relationship between a characteristic time scale and its associated energy scale is given by the principles of quantum mechanics, akin to the energy-time uncertainty principle. The precise relation is:\n$$\nE_c = \\frac{\\hbar}{\\tau_D}\n$$\nwhere $\\hbar$ is the reduced Planck constant. The inclusion of $\\hbar$ is essential on both physical and dimensional grounds. Substituting our expression for $\\tau_D$ into this equation, we find the Thouless energy:\n$$\nE_c = \\frac{\\hbar D}{L^2}\n$$\n\nNext, we derive the expression for the mean single-particle level spacing, $\\Delta E$.\n(iii) The third foundational principle relates the density of states to the mean level spacing in a finite volume. The problem provides $\\nu$ as the single-particle density of states per unit volume at the Fermi energy. For a sample with a finite volume $V$, the total density of states, denoted $N(E_F)$, is the product of $\\nu$ and $V$. The sample is a $d$-dimensional hypercube of side length $L$, so its volume is:\n$$\nV = L^d\n$$\nThe total density of states at the Fermi energy is therefore:\n$$\nN(E_F) = \\nu V = \\nu L^d\n$$\nThis quantity has units of states per unit energy. The mean spacing between adjacent energy levels, $\\Delta E$, is the inverse of the total density of states:\n$$\n\\Delta E = \\frac{1}{N(E_F)} = \\frac{1}{\\nu L^d}\n$$\n\nWith expressions for both $E_c$ and $\\Delta E$, we can now assemble the expression for the Thouless conductance $g_T$:\n$$\ng_T = \\frac{E_c}{\\Delta E} = \\frac{\\frac{\\hbar D}{L^2}}{\\frac{1}{\\nu L^d}}\n$$\nBy simplifying this fraction, we arrive at the final expression for $g_T$:\n$$\ng_T = \\hbar D \\nu L^{d-2}\n$$\n\nThe problem also requires confirmation of the power-law scaling behavior. Our derived expression shows that, for fixed material parameters ($D$, $\\nu$) and dimensionality ($d$), the Thouless conductance scales with the system size $L$ as:\n$$\ng_T \\propto L^{d-2}\n$$\nThis power-law dependence is a central result of the scaling theory of localization. It implies that the conductivity of the material depends on its size in a manner dictated by its dimensionality. For dimensions $d>2$, $g_T$ increases with $L$, indicating metallic behavior. For dimensions $d<2$, $g_T$ decreases with $L$, indicating insulating behavior where wavefunctions are localized. The case $d=2$ is a marginal case where this simple scaling suggests $g_T$ is constant, although more advanced theories predict a slow, logarithmic decrease, leading to weak localization. The derived result successfully captures the fundamental power-law behavior.", "answer": "$$\\boxed{\\hbar D \\nu L^{d-2}}$$", "id": "3014241"}, {"introduction": "Building on the basic scaling relation, we now turn to the powerful formalism used to describe the metal-insulator transition: the renormalization group (RG). This framework is captured by the beta function, $\\beta(g)$, which describes the flow of the dimensionless conductance $g$ with changing system size. This exercise provides a direct application of RG methods to calculate a universal critical exponent, one of the key measurable predictions of the theory [@problem_id:1196076]. By finding the fixed point of a given beta function and linearizing the flow around it, you will see how the abstract machinery of RG yields concrete, physical quantities that characterize a quantum phase transition.", "problem": "In the scaling theory of Anderson localization, the change of the dimensionless conductance $g$ of a $d$-dimensional hypercube of side length $L$ under a change of scale is described by the Gell-Mann-Low or beta function, $\\beta(g) = \\frac{d \\ln g}{d \\ln L}$. The behavior of the system (whether it is a metal or an insulator) is determined by the flow of $g$ as $L \\to \\infty$.\n\nIn dimensions $d>2$, there can be a metal-insulator transition at a critical value of disorder. This transition corresponds to an unstable fixed point $g^*$ of the renormalization group (RG) flow, where $\\beta(g^*) = 0$. For a system described by a coupling constant (e.g., disorder strength) infinitesimally close to its critical value, the localization length $\\xi$ (in the insulating phase) or the correlation length (in the metallic phase) diverges as $\\xi \\propto |\\delta|^{-\\nu}$, where $\\delta$ measures the deviation from the critical point and $\\nu$ is a universal critical exponent.\n\nThe exponent $\\nu$ can be computed from the behavior of the RG flow near the fixed point. It is given by the relation $\\nu = 1/y_g$, where $y_g$ is the RG eigenvalue at the fixed point, defined as $y_g = \\frac{d}{dg}\\left(g \\beta(g)\\right)\\big|_{g=g^*}$.\n\nConsider a model of disordered electrons in dimension $d=2+\\epsilon$, where $\\epsilon$ is a small positive parameter. The beta function for this system has been computed up to two loops in the weak-coupling (large conductance) limit and is given by:\n$$\n\\beta(g) = \\epsilon - \\frac{c_1}{g} - \\frac{c_2}{g^2}\n$$\nwhere $c_1$ and $c_2$ are positive constants that depend on the symmetry class of the system.\n\nYour task is to calculate the critical exponent $\\nu$ for the localization length. Express your answer in terms of $\\epsilon$, $c_1$, and $c_2$. Your result should be accurate up to the first two terms in a small $\\epsilon$ expansion (i.e., terms of order $1/\\epsilon$ and order $\\epsilon^0$).", "solution": "The beta function is given by:\n\n$$\n\\beta(g) = \\epsilon - \\frac{c_1}{g} - \\frac{c_2}{g^2}\n$$\n\nThe fixed point $g^*$ satisfies $\\beta(g^*) = 0$:\n\n$$\n\\epsilon - \\frac{c_1}{g^*} - \\frac{c_2}{(g^*)^2} = 0\n$$\n\nMultiplying by $(g^*)^2$:\n\n$$\n\\epsilon (g^*)^2 - c_1 g^* - c_2 = 0\n$$\n\nSolving the quadratic equation for $g^*$:\n\n$$\ng^* = \\frac{c_1 \\pm \\sqrt{c_1^2 + 4\\epsilon c_2}}{2\\epsilon}\n$$\n\nTaking the positive root (since $g > 0$):\n\n$$\ng^* = \\frac{c_1 + \\sqrt{c_1^2 + 4\\epsilon c_2}}{2\\epsilon}\n$$\n\nFor small $\\epsilon$, expand using $\\sqrt{c_1^2 + 4\\epsilon c_2} = c_1 \\sqrt{1 + \\frac{4\\epsilon c_2}{c_1^2}} \\approx c_1 \\left(1 + \\frac{2\\epsilon c_2}{c_1^2} - \\frac{2\\epsilon^2 c_2^2}{c_1^4} + \\cdots \\right)$:\n\n$$\ng^* \\approx \\frac{c_1 + c_1 + \\frac{2\\epsilon c_2}{c_1} - \\frac{2\\epsilon^2 c_2^2}{c_1^3} + \\cdots}{2\\epsilon} = \\frac{2c_1}{2\\epsilon} + \\frac{2\\epsilon c_2}{2\\epsilon c_1} - \\frac{2\\epsilon^2 c_2^2}{2\\epsilon c_1^3} + \\cdots = \\frac{c_1}{\\epsilon} + \\frac{c_2}{c_1} - \\frac{\\epsilon c_2^2}{c_1^3} + \\cdots\n$$\n\nThus:\n\n$$\ng^* = \\frac{c_1}{\\epsilon} + \\frac{c_2}{c_1} + O(\\epsilon)\n$$\n\nThe RG eigenvalue $y_g$ is defined as:\n\n$$\ny_g = \\frac{d}{dg} \\left( g \\beta(g) \\right) \\bigg|_{g=g^*}\n$$\n\nFirst, compute $g \\beta(g)$:\n\n$$\ng \\beta(g) = g \\left( \\epsilon - \\frac{c_1}{g} - \\frac{c_2}{g^2} \\right) = \\epsilon g - c_1 - \\frac{c_2}{g}\n$$\n\nDifferentiate with respect to $g$:\n\n$$\n\\frac{d}{dg} \\left( g \\beta(g) \\right) = \\epsilon + \\frac{c_2}{g^2}\n$$\n\nEvaluate at $g = g^*$:\n\n$$\ny_g = \\epsilon + \\frac{c_2}{(g^*)^2}\n$$\n\nSubstitute $g^* \\approx \\frac{c_1}{\\epsilon} + \\frac{c_2}{c_1}$:\n\n$$\n(g^*)^2 = \\left( \\frac{c_1}{\\epsilon} + \\frac{c_2}{c_1} \\right)^2 = \\frac{c_1^2}{\\epsilon^2} + 2 \\cdot \\frac{c_1}{\\epsilon} \\cdot \\frac{c_2}{c_1} + \\left( \\frac{c_2}{c_1} \\right)^2 + O(\\epsilon) = \\frac{c_1^2}{\\epsilon^2} + \\frac{2c_2}{\\epsilon} + \\frac{c_2^2}{c_1^2} + O(\\epsilon)\n$$\n\nThen:\n\n$$\n\\frac{1}{(g^*)^2} = \\frac{1}{\\frac{c_1^2}{\\epsilon^2} \\left(1 + \\frac{2c_2 \\epsilon}{c_1^2} + \\frac{c_2^2 \\epsilon^2}{c_1^4} + O(\\epsilon^3) \\right)} = \\frac{\\epsilon^2}{c_1^2} \\left(1 + \\frac{2c_2 \\epsilon}{c_1^2} + \\frac{c_2^2 \\epsilon^2}{c_1^4} + O(\\epsilon^3) \\right)^{-1}\n$$\n\nExpand the inverse using $(1 + x)^{-1} \\approx 1 - x + x^2 - \\cdots$ with $x = \\frac{2c_2 \\epsilon}{c_1^2} + O(\\epsilon^2)$:\n\n$$\n\\left(1 + \\frac{2c_2 \\epsilon}{c_1^2} + \\frac{c_2^2 \\epsilon^2}{c_1^4} + O(\\epsilon^3) \\right)^{-1} = 1 - \\frac{2c_2 \\epsilon}{c_1^2} + \\left( \\frac{2c_2 \\epsilon}{c_1^2} \\right)^2 - \\cdots + O(\\epsilon^2) = 1 - \\frac{2c_2 \\epsilon}{c_1^2} + \\frac{4c_2^2 \\epsilon^2}{c_1^4} + O(\\epsilon^2)\n$$\n\nThus:\n\n$$\n\\frac{1}{(g^*)^2} = \\frac{\\epsilon^2}{c_1^2} \\left(1 - \\frac{2c_2 \\epsilon}{c_1^2} + O(\\epsilon^2) \\right) = \\frac{\\epsilon^2}{c_1^2} - \\frac{2c_2 \\epsilon^3}{c_1^4} + O(\\epsilon^4)\n$$\n\nNow:\n\n$$\ny_g = \\epsilon + c_2 \\left( \\frac{\\epsilon^2}{c_1^2} - \\frac{2c_2 \\epsilon^3}{c_1^4} + O(\\epsilon^4) \\right) = \\epsilon + \\frac{c_2 \\epsilon^2}{c_1^2} - \\frac{2c_2^2 \\epsilon^3}{c_1^4} + O(\\epsilon^4)\n$$\n\nThe critical exponent $\\nu = 1 / y_g$:\n\n$$\n\\nu = \\frac{1}{y_g} = \\frac{1}{\\epsilon + \\frac{c_2 \\epsilon^2}{c_1^2} + O(\\epsilon^3)} = \\frac{1}{\\epsilon} \\cdot \\frac{1}{1 + \\frac{c_2 \\epsilon}{c_1^2} + O(\\epsilon^2)}\n$$\n\nExpand the inverse:\n\n$$\n\\left(1 + \\frac{c_2 \\epsilon}{c_1^2} + O(\\epsilon^2) \\right)^{-1} = 1 - \\frac{c_2 \\epsilon}{c_1^2} + O(\\epsilon^2)\n$$\n\nThus:\n\n$$\n\\nu = \\frac{1}{\\epsilon} \\left(1 - \\frac{c_2 \\epsilon}{c_1^2} + O(\\epsilon^2) \\right) = \\frac{1}{\\epsilon} - \\frac{c_2}{c_1^2} + O(\\epsilon)\n$$\n\nRetaining terms up to order $1/\\epsilon$ and $\\epsilon^0$:\n\n$$\n\\nu = \\frac{1}{\\epsilon} - \\frac{c_2}{c_1^2} + O(\\epsilon)\n$$", "answer": "$$\n\\boxed{\\dfrac{1}{\\epsilon} - \\dfrac{c_2}{c_1^{2}}}\n$$", "id": "1196076"}, {"introduction": "A physical theory is only as strong as its verifiable predictions. The one-parameter scaling hypothesis makes a profound claim: the *entire probability distribution* of conductance, not just its average value, depends on the system size $L$ through a single scaling variable. This final practice is a hands-on computational exercise to test this hypothesis directly [@problem_id:3014270]. You will generate synthetic data based on a physically-grounded model for conductance fluctuations and use statistical tools to demonstrate \"data collapse,\" a hallmark of scaling. This exercise bridges the gap between abstract theory and modern data analysis, providing practical skills in designing numerical experiments to validate a fundamental physical concept.", "problem": "Consider a one-dimensional, single-channel disordered conductor described by a tight-binding chain with uncorrelated on-site disorder. In the localized regime, the Landauer picture connects the two-terminal conductance to transmission through the sample, and the transfer-matrix formulation expresses the evolution of the wavefunction amplitudes as a product of random matrices. By the Central Limit Theorem applied to this multiplicative process, the logarithm of the dimensionless conductance, denoted by $y = \\ln g$, is the sum of many weakly dependent increments and is expected to be approximately normally distributed. The scaling theory of localization posits one-parameter scaling: the full distribution $P(g;L)$ should depend on the system size $L$ only through a single scaling variable, such as $s = L/\\xi$, where $\\xi$ is the localization length, or equivalently through a single cumulant like the mean of $\\ln g$.\n\nYour task is to implement a numerical test of one-parameter scaling at the level of distributions using a synthetic but physically motivated model for $P(g;L)$. Work in the following idealized setting, which captures the localized regime in a single channel under general conditions:\n\n- Hypothesis H (Gaussian log-conductance model, justified by the Central Limit Theorem): The variable $y = \\ln g$ is Gaussian with a mean $m(s)$ and variance $v(s)$ that are both proportional to the single scaling variable $s = L/\\xi$. Without loss of generality, choose units such that $m(s) = -2 s$ and $v(s) = 4 s$. This normalization corresponds to an idealized regime where the Dorokhov–Mello–Pereyra–Kumar (DMPK) picture holds in one dimension and the first two cumulants of $y$ are linear in $s$.\n\n- Equivalence of distribution-level tests: Since the mapping $g \\mapsto \\ln g$ is strictly monotone on $(0,\\infty)$, any test of one-parameter scaling formulated in terms of $y = \\ln g$ is equivalent to a test on $g$ at the level of cumulative distribution ordering.\n\nImplement a program that generates independent ensembles of samples of $y = \\ln g$ using the Gaussian model above for a specified set of $(L,\\xi)$ or specified values of $s = L/\\xi$. Using these ensembles, perform the following two numerical tests:\n\n1. Same-scaling-variable collapse test: For different pairs $(L,\\xi)$ that share the same $s=L/\\xi$, the raw distributions of $y$ should be statistically indistinguishable. Quantify this by the two-sample Kolmogorov–Smirnov statistic on the raw $y$ samples. The test passes if all pairwise distances among the ensembles at the same $s$ are below a threshold $\\tau_{2s}$.\n\n2. Mean-only distribution collapse test: If one-parameter scaling holds at the level of distributions, then using only the sample mean of $y$ to fix the scaling variable should collapse distributions across different $s$. Define a rescaled variable for each ensemble by\n$$\nz = \\frac{y - \\overline{y}}{\\sqrt{-2\\,\\overline{y}}},\n$$\nwhere $\\overline{y}$ is the sample mean of that ensemble. Under the hypothesis above and for sufficiently large samples, $z$ should be close to a standard normal variable, and the rescaled distributions for different $s$ should coincide. Quantify this by the one-sample Kolmogorov–Smirnov statistic of $z$ against the standard normal distribution. The test passes if all distances are below a threshold $\\tau_{1s}$.\n\nDesign choices:\n- Use a fixed random seed for reproducibility.\n- Use the Gaussian model for $y$ with mean $m(s) = -2 s$ and variance $v(s) = 4 s$ to synthesize the data, consistent with Hypothesis H.\n- Use sufficiently large ensemble sizes so that sampling error does not dominate the outcome.\n\nTest suite:\n- Test 1 (same $s$ across different $(L,\\xi)$): Use three pairs with equal $s=2$,\n    - $(L,\\xi) = (200,100)$,\n    - $(L,\\xi) = (100,50)$,\n    - $(L,\\xi) = (50,25)$.\n  Evaluate the maximum pairwise two-sample Kolmogorov–Smirnov distance among these three ensembles of $y$ and compare it to $\\tau_{2s} = 0.06$. Output a boolean indicating whether all pairwise distances are less than or equal to $0.06$.\n\n- Test 2 (mean-only collapse across well-localized ensembles): Use $s \\in \\{0.5, 1.5, 3.0\\}$. For each $s$, compute the one-sample Kolmogorov–Smirnov distance of $z$ against the standard normal. The test passes if the maximum distance over these three ensembles is less than or equal to $\\tau_{1s} = 0.07$. Output a boolean for this test.\n\n- Test 3 (edge case, near-ballistic to weakly localized): Use $s \\in \\{0.05, 0.10, 0.20\\}$. Perform the same mean-only collapse test as in Test 2 with the same threshold $\\tau_{1s} = 0.07$. Output a boolean for this test.\n\nImplementation details:\n- Sample size: For each ensemble, draw exactly $N = 5000$ independent samples of $y$.\n- Use a fixed seed for the random number generator so that results are deterministic.\n- Numerical thresholds: Use the thresholds specified above: $\\tau_{2s} = 0.06$ for the two-sample collapse test and $\\tau_{1s} = 0.07$ for the one-sample collapse tests.\n- Angle units are not applicable. No physical units are required, as $g$ and $s$ are dimensionless.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each entry is a boolean corresponding to Tests 1, 2, and 3 in that order.", "solution": "We base the derivation on two fundamental and widely accepted pillars: the Landauer–Büttiker picture of quantum transport and the transfer-matrix description of wave propagation in disordered media.\n\n1. Landauer conductance and multiplicative random processes:\n   - In coherent two-terminal transport, the dimensionless conductance $g$ (in units of $2 e^2/h$) equals the transmission probability through the sample. In a single-channel one-dimensional disordered system, the wavefunction amplitudes evolve along the sample as $\\Psi_{n+1} = T_n \\Psi_n$, where $T_n$ is a random $2 \\times 2$ transfer matrix determined by the disordered potential at site $n$ and energy. The total evolution over length $L$ is the product $M_L = T_L T_{L-1} \\cdots T_1$, a multiplicative stochastic process.\n   - The transmission and reflection are expressible in terms of $M_L$; equivalently, $-\\ln g$ is proportional to twice the positive Lyapunov exponent times the length for large $L$.\n\n2. Central Limit Theorem and Gaussian fluctuations of $\\ln g$:\n   - The logarithm of the transmission (or conductance) is a sum of many weakly dependent increments arising from the local random transfer matrices. For a single channel with short-range-correlated disorder, a version of the Central Limit Theorem applies to the additive quantity $y = \\ln g$. Thus, $y$ approaches a normal distribution with a mean and variance both proportional to the system size $L$. Writing $s = L/\\xi$, where $\\xi$ is the localization length that sets the exponential decay scale of typical wavefunctions, we expect\n     $$\n     y \\sim \\mathcal{N}\\big(m(s), v(s)\\big), \\quad m(s) \\propto s, \\quad v(s) \\propto s.\n     $$\n   - In one-parameter scaling, all cumulants of $y$ are functions of a single parameter (for instance $s$ or the mean $m$). In the deep localized regime, the Dorokhov–Mello–Pereyra–Kumar (DMPK) approach and similar transfer-matrix analyses yield the specific relations\n     $$\n     \\langle \\ln g \\rangle = -2 s, \\qquad \\operatorname{Var}(\\ln g) = 4 s,\n     $$\n     implying $\\operatorname{Var}(\\ln g) = -2 \\langle \\ln g \\rangle$. These relations consistently realize one-parameter scaling, because the variance is determined by the mean.\n\n3. Distribution-level test for one-parameter scaling:\n   - Because $g \\mapsto \\ln g$ is monotone, testing one-parameter scaling for the distribution of $y = \\ln g$ is equivalent to testing it for $g$; any collapse of cumulative distributions in $y$ implies the same ordering in $g$.\n   - Same-scaling-variable collapse: If we pick different $(L,\\xi)$ with the same $s = L/\\xi$, then the underlying Gaussian distributions for $y$ share the same mean and variance, so they should be statistically indistinguishable. A two-sample Kolmogorov–Smirnov statistic on the raw $y$ samples then quantifies the collapse; small distances confirm consistency.\n   - Mean-only collapse across different $s$: One-parameter scaling at the distribution level implies the possibility of collapsing distributions using only a single parameter. Using the relationship $\\operatorname{Var}(\\ln g) = -2 \\langle \\ln g \\rangle$, one can define the rescaled variable\n     $$\n     z = \\frac{y - \\overline{y}}{\\sqrt{-2\\,\\overline{y}}},\n     $$\n     where $\\overline{y}$ is the sample mean serving as the estimator of the single scaling parameter. If the hypothesis holds and sample sizes are large, $z$ should follow a standard normal distribution independent of $s$, hence all rescaled ensembles should coincide. A one-sample Kolmogorov–Smirnov test against the standard normal quantifies the collapse quality.\n\n4. Algorithmic design:\n   - Set a fixed random seed to ensure reproducibility.\n   - For any specified $s$, synthesize $N$ independent samples of $y$ from a Gaussian with mean $m(s) = -2 s$ and variance $v(s) = 4 s$. This concretely instantiates the Central Limit Theorem-based model and fixes the otherwise arbitrary proportionality constants using a normalization convention.\n   - Test 1: Generate three ensembles with $(L,\\xi) = (200,100)$, $(100,50)$, $(50,25)$, all at $s=2$. Compute the three pairwise two-sample Kolmogorov–Smirnov distances on the raw samples of $y$; declare success if the maximum distance is less than or equal to $\\tau_{2s} = 0.06$.\n   - Test 2: Generate three ensembles at $s \\in \\{0.5, 1.5, 3.0\\}$. For each ensemble, compute $z$ using the sample mean $\\overline{y}$ and the mapping above. Compute the one-sample Kolmogorov–Smirnov distance of $z$ to the standard normal; declare success if the maximum over these three distances is less than or equal to $\\tau_{1s} = 0.07$.\n   - Test 3: Edge case with small $s \\in \\{0.05, 0.10, 0.20\\}$; repeat the mean-only collapse test with the same threshold $\\tau_{1s} = 0.07$.\n   - Use $N = 5000$ samples per ensemble to suppress statistical fluctuations so that the tests are stringent yet pass given the model.\n\n5. Output:\n   - Produce a single line with a Python list literal containing three booleans in order: Test 1, Test 2, Test 3.\n\nThis approach adheres to the principle-based derivation: starting from Landauer transport and transfer matrices, invoking the Central Limit Theorem for multiplicative processes, and operationalizing the one-parameter scaling hypothesis via a Gaussian model whose cumulants are linear in $s$. The numerical tests then directly quantify the claim that the full distribution depends on $L$ only via the single parameter $s$ or equivalently only via the mean of $\\ln g$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest, ks_2samp\n\ndef generate_ln_g_samples(s: float, n: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generate n samples of y = ln g for given scaling variable s = L/xi,\n    using the Gaussian log-conductance model:\n        y ~ Normal(mean = -2*s, variance = 4*s).\n    \"\"\"\n    mean = -2.0 * s\n    var = 4.0 * s\n    std = np.sqrt(var)\n    return rng.normal(loc=mean, scale=std, size=n)\n\ndef ks_pairwise_max(samples_list):\n    \"\"\"\n    Compute the maximum two-sample KS statistic over all unordered pairs\n    from a list of 1D sample arrays.\n    \"\"\"\n    m = len(samples_list)\n    max_stat = 0.0\n    for i in range(m):\n        for j in range(i+1, m):\n            stat = ks_2samp(samples_list[i], samples_list[j], alternative='two-sided', mode='auto').statistic\n            if stat > max_stat:\n                max_stat = stat\n    return max_stat\n\ndef mean_only_rescale_and_ks_to_norm(samples_list):\n    \"\"\"\n    For each sample array y, compute z = (y - mean(y)) / sqrt(-2*mean(y)),\n    then compute KS distance against standard normal.\n    Return the maximum KS statistic over the list.\n    \"\"\"\n    max_stat = 0.0\n    for y in samples_list:\n        y_mean = float(np.mean(y))\n        # Use only the sample mean to set the scale per one-parameter scaling.\n        scale = np.sqrt(-2.0 * y_mean)\n        # To avoid division by zero or NaNs in pathological cases, enforce positivity.\n        # Given our synthetic model, y_mean < 0 with overwhelming probability.\n        z = (y - y_mean) / scale\n        stat = kstest(z, 'norm').statistic\n        if stat > max_stat:\n            max_stat = stat\n    return max_stat\n\ndef solve():\n    rng = np.random.default_rng(20230921)\n    N = 5000\n\n    # Thresholds\n    tau_2s = 0.06  # for two-sample KS among same-s ensembles\n    tau_1s = 0.07  # for one-sample KS to N(0,1) after mean-only rescaling\n\n    # Test 1: Same s across different (L, xi)\n    # (200,100), (100,50), (50,25) all have s = 2\n    s_same = 2.0\n    samples_test1 = [\n        generate_ln_g_samples(s_same, N, rng),\n        generate_ln_g_samples(s_same, N, rng),\n        generate_ln_g_samples(s_same, N, rng),\n    ]\n    max_ks_t1 = ks_pairwise_max(samples_test1)\n    result1 = (max_ks_t1 <= tau_2s)\n\n    # Test 2: Mean-only collapse across different s in localized regime\n    s_list_t2 = [0.5, 1.5, 3.0]\n    samples_test2 = [generate_ln_g_samples(s, N, rng) for s in s_list_t2]\n    max_ks_t2 = mean_only_rescale_and_ks_to_norm(samples_test2)\n    result2 = (max_ks_t2 <= tau_1s)\n\n    # Test 3: Edge case near ballistic to weakly localized (small s)\n    s_list_t3 = [0.05, 0.10, 0.20]\n    samples_test3 = [generate_ln_g_samples(s, N, rng) for s in s_list_t3]\n    max_ks_t3 = mean_only_rescale_and_ks_to_norm(samples_test3)\n    result3 = (max_ks_t3 <= tau_1s)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [result1, result2, result3]))}]\")\n\nsolve()\n```", "id": "3014270"}]}