{"hands_on_practices": [{"introduction": "Understanding the behavior of electrons in disordered materials is a formidable challenge. A powerful approach is to use self-consistent theories that effectively average over the disorder to find the typical properties of the system. This exercise [@problem_id:1228323] guides you through a cornerstone calculation using the self-consistent Born approximation (SCBA) to analytically determine the mobility edge $E_c$. By employing a simplified semi-circular density of states, a common and tractable model, you will see precisely how the interplay between the clean system's bandwidth $B$ and the disorder strength $W$ gives rise to a sharp boundary separating extended and localized states.", "problem": "A spinless quantum particle moves on a large lattice described by a tight-binding Hamiltonian with on-site disorder. The Hamiltonian is given by $H = H_0 + V$, where $H_0$ describes the nearest-neighbor hopping with amplitude $t$, and $V$ represents the on-site potential energy. The on-site energies are independent random variables $\\epsilon_i$ at each site $i$, drawn from a Gaussian distribution with zero mean $\\langle \\epsilon_i \\rangle = 0$ and variance $\\langle \\epsilon_i \\epsilon_j \\rangle = W^2 \\delta_{ij}$.\n\nThe properties of this system can be studied using the disorder-averaged single-particle Green's function, which in the self-consistent Born approximation (SCBA) is determined by the self-energy $\\Sigma(E)$. The self-energy obeys the equation:\n$$\n\\Sigma(E) = W^2 G_{\\text{loc}}(E - \\Sigma(E))\n$$\nwhere $G_{\\text{loc}}(z) = \\int_{-\\infty}^{\\infty} \\frac{\\rho_0(E')}{z - E'} dE'$ is the local Green's function of the clean system ($W=0$), and $\\rho_0(E')$ is its density of states.\n\nWithin this framework, the mobility edge, $E_c$, is defined as the energy that separates extended states (where the self-energy $\\Sigma(E)$ is complex) from localized states (where $\\Sigma(E)$ is real). This transition occurs at the energy $E$ for which the function $E(z) = z + W^2 G_{\\text{loc}}(z)$ has an extremum, where $z=E-\\Sigma(E)$.\n\nTo make the problem analytically tractable, we approximate the density of states of the clean system by a semi-circular form:\n$$\n\\rho_0(E) = \\begin{cases} \\frac{1}{\\pi B^2 / 2} \\sqrt{B^2 - E^2}  \\text{for } |E| \\le B \\\\ 0  \\text{for } |E| > B \\end{cases}\n$$\nHere, $B$ is the half-bandwidth of the clean system.\n\nDerive the expression for the mobility edge $E_c$ as a function of the disorder strength $W$ and the half-bandwidth $B$. Assume we are interested in the positive energy mobility edge.", "solution": "The problem asks for the mobility edge $E_c$ for a particle on a lattice with on-site disorder, using the self-consistent Born approximation (SCBA) with a semi-circular density of states (DOS) for the clean system.\n\nThe mobility edge $E_c$ is defined as the energy where the self-energy $\\Sigma(E)$ transitions from being complex to real. The physical energy $E$ is related to the shifted complex energy $z = E - \\Sigma(E)$ via the self-consistent equation: $E = z + \\Sigma(E)$. Using the SCBA relation $\\Sigma(E) = W^2 G_{\\text{loc}}(z)$, we can express $E$ as a function of $z$:\n$$\nE(z) = z + W^2 G_{\\text{loc}}(z)\n$$\nThe mobility edge corresponds to the value of $E$ at an extremum of this function $E(z)$, for which the condition is $\\frac{dE}{dz} = 0$.\n$$\n\\frac{dE}{dz} = 1 + W^2 \\frac{dG_{\\text{loc}}(z)}{dz} = 0\n$$\n\nFirst, we need to calculate the local Green's function $G_{\\text{loc}}(z)$ for the given semi-circular DOS. For a complex variable $z$ not on the real axis interval $[-B, B]$, the integral for $G_{\\text{loc}}(z)$ is:\n$$\nG_{\\text{loc}}(z) = \\int_{-B}^{B} \\frac{2}{\\pi B^2} \\frac{\\sqrt{B^2 - E'^2}}{z - E'} dE'\n$$\nThis is a standard integral. The result for a semi-circular DOS is:\n$$\nG_{\\text{loc}}(z) = \\frac{2}{B^2} \\left( z - \\sqrt{z^2 - B^2} \\right)\n$$\nThe sign of the square root is chosen such that $G_{\\text{loc}}(z) \\sim 1/z$ for large $|z|$.\n\nNext, we compute the derivative of $G_{\\text{loc}}(z)$ with respect to $z$:\n$$\n\\frac{dG_{\\text{loc}}(z)}{dz} = \\frac{2}{B^2} \\left( 1 - \\frac{z}{\\sqrt{z^2 - B^2}} \\right)\n$$\nNow, we substitute this derivative into the extremum condition $\\frac{dE}{dz}=0$:\n$$\n1 + W^2 \\frac{2}{B^2} \\left( 1 - \\frac{z_c}{\\sqrt{z_c^2 - B^2}} \\right) = 0\n$$\nwhere $z_c$ is the value of $z$ at which the extremum occurs. Rearranging this equation to solve for $z_c$:\n$$\n\\frac{B^2}{2W^2} = - \\left( 1 - \\frac{z_c}{\\sqrt{z_c^2 - B^2}} \\right) = \\frac{z_c}{\\sqrt{z_c^2 - B^2}} - 1\n$$\n$$\n\\frac{B^2}{2W^2} + 1 = \\frac{z_c}{\\sqrt{z_c^2 - B^2}}\n$$\n$$\n\\frac{B^2 + 2W^2}{2W^2} = \\frac{z_c}{\\sqrt{z_c^2 - B^2}}\n$$\nTo solve for $z_c$, we can square both sides:\n$$\n\\left( \\frac{B^2 + 2W^2}{2W^2} \\right)^2 = \\frac{z_c^2}{z_c^2 - B^2}\n$$\nLet's invert the equation to solve for $z_c^2$:\n$$\n\\left( \\frac{2W^2}{B^2 + 2W^2} \\right)^2 = \\frac{z_c^2 - B^2}{z_c^2} = 1 - \\frac{B^2}{z_c^2}\n$$\n$$\n\\frac{B^2}{z_c^2} = 1 - \\left( \\frac{2W^2}{B^2 + 2W^2} \\right)^2 = \\frac{(B^2 + 2W^2)^2 - (2W^2)^2}{(B^2 + 2W^2)^2}\n$$\nThe numerator is a difference of squares: $(B^2 + 2W^2 - 2W^2)(B^2 + 2W^2 + 2W^2) = B^2(B^2 + 4W^2)$.\n$$\n\\frac{B^2}{z_c^2} = \\frac{B^2(B^2 + 4W^2)}{(B^2 + 2W^2)^2}\n$$\nCanceling $B^2$ and inverting gives the expression for $z_c^2$:\n$$\nz_c^2 = \\frac{(B^2 + 2W^2)^2}{B^2 + 4W^2}\n$$\nTaking the positive root (for the positive energy mobility edge):\n$$\nz_c = \\frac{B^2 + 2W^2}{\\sqrt{B^2 + 4W^2}}\n$$\nFinally, the mobility edge $E_c$ is the value of $E(z)$ at $z=z_c$:\n$$\nE_c = z_c + W^2 G_{\\text{loc}}(z_c) = z_c + W^2 \\frac{2}{B^2} \\left( z_c - \\sqrt{z_c^2 - B^2} \\right)\n$$\nWe can simplify the term in the parenthesis. From the relation $\\frac{B^2 + 2W^2}{2W^2} \\sqrt{z_c^2 - B^2} = z_c$, we have:\n$$\n\\sqrt{z_c^2 - B^2} = z_c \\frac{2W^2}{B^2 + 2W^2}\n$$\nSubstituting this into the expression for $E_c$:\n$$\nE_c = z_c + \\frac{2W^2}{B^2} \\left( z_c - z_c \\frac{2W^2}{B^2 + 2W^2} \\right) = z_c \\left[ 1 + \\frac{2W^2}{B^2} \\left( 1 - \\frac{2W^2}{B^2 + 2W^2} \\right) \\right]\n$$\n$$\nE_c = z_c \\left[ 1 + \\frac{2W^2}{B^2} \\left( \\frac{B^2 + 2W^2 - 2W^2}{B^2 + 2W^2} \\right) \\right] = z_c \\left[ 1 + \\frac{2W^2}{B^2} \\frac{B^2}{B^2 + 2W^2} \\right]\n$$\n$$\nE_c = z_c \\left[ 1 + \\frac{2W^2}{B^2 + 2W^2} \\right] = z_c \\left( \\frac{B^2 + 2W^2 + 2W^2}{B^2 + 2W^2} \\right) = z_c \\left( \\frac{B^2 + 4W^2}{B^2 + 2W^2} \\right)\n$$\nNow, substitute the expression for $z_c$:\n$$\nE_c = \\left( \\frac{B^2 + 2W^2}{\\sqrt{B^2 + 4W^2}} \\right) \\left( \\frac{B^2 + 4W^2}{B^2 + 2W^2} \\right)\n$$\n$$\nE_c = \\frac{\\sqrt{B^2 + 4W^2} \\sqrt{B^2 + 4W^2}}{\\sqrt{B^2 + 4W^2}} = \\sqrt{B^2 + 4W^2}\n$$\nThe mobility edge is located at $E_c = \\sqrt{B^2 + 4W^2}$.", "answer": "$$\n\\boxed{\\sqrt{B^2+4W^2}}\n$$", "id": "1228323"}, {"introduction": "The transition between metallic and insulating phases is a critical phenomenon, characterized by universal properties that are independent of microscopic details. The theory of finite-size scaling provides a powerful framework for studying these transitions by analyzing how physical quantities change with system size $L$. In this practice [@problem_id:1206664], you will act as a data scientist of quantum physics, using a provided set of 'experimental' data points to test the scaling hypothesis. By requiring that data for different system sizes collapse onto a single universal curve, you will extract the non-universal mobility edge energy $E_c$ and the universal critical exponent $\\nu$ that governs the divergence of the localization length $\\xi$.", "problem": "In the study of Anderson localization, the metal-insulator transition is characterized by a mobility edge, $E_c$, which separates localized from extended electronic states. The localization length, $\\xi$, which describes the spatial extent of localized wavefunctions, is expected to diverge at the mobility edge following a power law:\n$$\n\\xi(E) \\propto |E - E_c|^{-\\nu}\n$$\nwhere $\\nu$ is a universal critical exponent.\n\nAccording to the one-parameter scaling theory of localization, for a system of finite size $L$, a dimensionless quantity $\\Lambda$ (related to conductance, e.g., the normalized localization length $\\xi/L$) near the transition should be a function of a single variable, $L/\\xi$. This implies a finite-size scaling law of the form:\n$$\n\\Lambda(E, L) = F\\left( (E - E_c) L^{1/\\nu} \\right)\n$$\nwhere $F$ is a universal scaling function. This means that if one plots $\\Lambda$ versus the scaled variable $x = (E - E_c) L^{1/\\nu}$, data for different system sizes $L$ and energies $E$ will collapse onto a single curve, provided the correct values of $E_c$ and $\\nu$ are used.\n\nSuppose that \"experimental\" measurements of the quantity $\\Lambda$ have been performed on a disordered three-dimensional system for two different cubic sample sizes, $L_1$ and $L_2$, where $L_2 = 8 L_1$. The following data points were obtained:\n1.  At energy $E_1 = 10$ (in some arbitrary units) for the system of size $L_1$, the measured value is $\\Lambda(E_1, L_1) = \\Lambda_0$.\n2.  At energy $E_2 = 7$ for the system of size $L_2$, the measured value is also $\\Lambda(E_2, L_2) = \\Lambda_0$.\n3.  At energy $E_3 = 14$ for the system of size $L_1$, the measured value is $\\Lambda(E_3, L_1) = \\Lambda_1$.\n4.  At energy $E_4 = 8$ for the system of size $L_2$, the measured value is also $\\Lambda(E_4, L_2) = \\Lambda_1$.\n\nAssuming the finite-size scaling law is exact for these data, determine the mobility edge energy $E_c$ and the critical exponent $\\nu$. Your final answer should be the ratio $\\nu / E_c$.", "solution": "The finite-size scaling law is given by:\n\n$$\n\\Lambda(E, L) = F\\left( (E - E_c) L^{1/\\nu} \\right)\n$$\n\nwhere $F$ is a universal scaling function. The data points are:\n- $\\Lambda(E_1 = 10, L_1) = \\Lambda_0$\n- $\\Lambda(E_2 = 7, L_2) = \\Lambda_0$\n- $\\Lambda(E_3 = 14, L_1) = \\Lambda_1$\n- $\\Lambda(E_4 = 8, L_2) = \\Lambda_1$\nwith $L_2 = 8 L_1$.\n\nSince $\\Lambda$ is the same for pairs of points, the arguments of $F$ must be equal (assuming $F$ is one-to-one). Thus:\n1. For $\\Lambda_0$:\n\n$$\n(10 - E_c) L_1^{1/\\nu} = (7 - E_c) L_2^{1/\\nu}\n$$\n\nSubstituting $L_2 = 8 L_1$:\n\n$$\n(10 - E_c) L_1^{1/\\nu} = (7 - E_c) (8 L_1)^{1/\\nu} = (7 - E_c) 8^{1/\\nu} L_1^{1/\\nu}\n$$\n\nDividing both sides by $L_1^{1/\\nu}$ (valid since $L_1 > 0$):\n\n$$\n10 - E_c = (7 - E_c) 8^{1/\\nu}\n$$\n\nLet $r = 8^{1/\\nu}$, so:\n\n$$\n10 - E_c = (7 - E_c) r \\quad \\text{(Equation A)}\n$$\n\n\n2. For $\\Lambda_1$:\n\n$$\n(14 - E_c) L_1^{1/\\nu} = (8 - E_c) L_2^{1/\\nu} = (8 - E_c) 8^{1/\\nu} L_1^{1/\\nu}\n$$\n\nDividing by $L_1^{1/\\nu}$:\n\n$$\n14 - E_c = (8 - E_c) 8^{1/\\nu} = (8 - E_c) r \\quad \\text{(Equation B)}\n$$\n\n\nNow solve Equations A and B:\n\n$$\n10 - E_c = r (7 - E_c)\n$$\n\n\n$$\n14 - E_c = r (8 - E_c)\n$$\n\n\nRearrange Equation A:\n\n$$\n10 - E_c = 7r - r E_c \\implies -E_c + r E_c = 7r - 10 \\implies E_c (r - 1) = 7r - 10\n$$\n\n\nRearrange Equation B:\n\n$$\n14 - E_c = 8r - r E_c \\implies -E_c + r E_c = 8r - 14 \\implies E_c (r - 1) = 8r - 14\n$$\n\n\nSet the expressions for $E_c (r - 1)$ equal:\n\n$$\n7r - 10 = 8r - 14\n$$\n\nSolve for $r$:\n\n$$\n7r - 10 - 8r = -14 \\implies -r = -4 \\implies r = 4\n$$\n\n\nSince $r = 8^{1/\\nu}$:\n\n$$\n8^{1/\\nu} = 4\n$$\n\nExpress in terms of powers of 2:\n\n$$\n(2^3)^{1/\\nu} = 2^2 \\implies 2^{3/\\nu} = 2^2\n$$\n\nEquate exponents:\n\n$$\n\\frac{3}{\\nu} = 2 \\implies \\nu = \\frac{3}{2}\n$$\n\n\nSubstitute $r = 4$ into Equation A to find $E_c$:\n\n$$\nE_c (4 - 1) = 7 \\cdot 4 - 10 \\implies 3 E_c = 28 - 10 \\implies 3 E_c = 18 \\implies E_c = 6\n$$\n\n\nVerify with Equation B:\n\n$$\nE_c (4 - 1) = 8 \\cdot 4 - 14 \\implies 3 E_c = 32 - 14 \\implies 3 E_c = 18 \\implies E_c = 6\n$$\n\n\nThe ratio is:\n\n$$\n\\frac{\\nu}{E_c} = \\frac{3/2}{6} = \\frac{3}{2} \\cdot \\frac{1}{6} = \\frac{3}{12} = \\frac{1}{4}\n$$", "answer": "$$ \\boxed{\\dfrac{1}{4}} $$", "id": "1206664"}, {"introduction": "Moving from analytical theory to direct computation provides invaluable insight into the physics of localization. This hands-on programming exercise [@problem_id:3005640] delves into the numerical backbone of localization studies: the transfer matrix method. You will implement a numerically stable algorithm to calculate the largest Lyapunov exponent $\\gamma$, the quantity that determines the exponential decay of wavefunctions and is inversely related to the localization length $\\xi$. This practice not only provides a concrete method for computing $\\xi$ but also offers a practical encounter with the fundamental concept of self-averaging, revealing how statistical properties of a disordered system emerge in the large-system limit.", "problem": "You are asked to write a complete, runnable program that estimates, for a one-dimensional disordered tight-binding chain, the empirical distribution of finite-size Lyapunov exponents and uses it to assess self-averaging properties relevant to estimating the localization length. The goal is to design from first principles a numerically stable estimator for the largest Lyapunov exponent based on transfer matrices and to quantify how its distribution narrows with increasing system size.\n\nYou must base your derivation and algorithm on the following fundamental starting points:\n\n- The one-dimensional tight-binding Schrödinger equation with onsite disorder,\n  $$\\psi_{i+1} + \\psi_{i-1} + \\varepsilon_i \\psi_i = E \\psi_i,$$\n  where $E$ is energy in units of the nearest-neighbor hopping amplitude (so the hopping is set to $1$), and $\\varepsilon_i$ are independent identically distributed onsite potentials drawn from a uniform distribution on $\\left[-\\tfrac{W}{2}, \\tfrac{W}{2}\\right]$ with width $W$.\n- The associated $2 \\times 2$ transfer matrix formulation that evolves a two-component state across lattice sites,\n  $$\\begin{pmatrix}\\psi_{i+1}\\\\ \\psi_i\\end{pmatrix} = \\begin{pmatrix} E - \\varepsilon_i  -1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix}\\psi_i\\\\ \\psi_{i-1}\\end{pmatrix}.$$\n- The largest Lyapunov exponent $\\gamma$ of the random matrix product, which quantifies the exponential growth rate of typical solutions, defined in terms of the long-chain limit of matrix products in accordance with Oseledec’s multiplicative ergodic theorem. For a one-dimensional Anderson problem, the inverse of the largest Lyapunov exponent equals the localization length, i.e., $\\xi = 1/\\gamma$.\n\nYour program must implement a numerically stable finite-length estimator for the largest Lyapunov exponent for an ensemble of disorder realizations and produce the empirical distribution over the ensemble. The estimator should be obtained by iteratively multiplying by the transfer matrix and renormalizing at each site to avoid overflow, accumulating the logarithms of per-step norm growth factors. Specifically, if at site $i$ the two-component state vector is normalized to unit Euclidean norm, then after multiplying by the transfer matrix for that site, the norm increases by a factor $n_i$; the finite-length Lyapunov exponent for a single realization is then\n$$\\gamma_L = \\frac{1}{L} \\sum_{i=1}^{L} \\ln n_i.$$\nFrom the ensemble of $N$ independent realizations, you must compute the sample mean $\\mu_L = \\frac{1}{N} \\sum_{j=1}^{N} \\gamma_L^{(j)}$ and the unbiased sample variance $s_L^2 = \\frac{1}{N-1} \\sum_{j=1}^{N} \\left(\\gamma_L^{(j)} - \\mu_L\\right)^2$. Use $\\hat{\\xi}_{\\mathrm{typ}} = 1/\\mu_L$ as the typical localization length estimator. Interpret self-averaging by examining how $s_L^2$ scales with $L$; for independent disorder $\\varepsilon_i$, the sum of $\\ln n_i$ is expected to satisfy a central limit theorem so that $s_L^2 \\propto 1/L$ for large $L$.\n\nImplementation requirements and constraints:\n\n- All energies are dimensionless in units of the hopping amplitude. All lengths are in the number of lattice sites. There are no physical units to convert.\n- Use a numerically stable renormalization at each step. Initialize the two-component state in each realization to a unit-norm vector, for example $\\left(\\psi_1,\\psi_0\\right) = (1,0)$, and at each site compute the new pair, its Euclidean norm, accumulate the logarithm of the norm, and renormalize the pair to unit norm before the next step.\n- To obtain the empirical distribution efficiently, evolve all realizations in parallel using vectorized array operations where possible.\n- For each test case below, report the sample mean $\\mu_L$, the unbiased sample variance $s_L^2$, and the localization length estimator $\\hat{\\xi}_{\\mathrm{typ}} = 1/\\mu_L$.\n\nSelf-averaging check:\n\n- To quantify self-averaging, compare two cases that have the same disorder strength $W$ and energy $E$ but different lengths $L$. Compute the ratio\n  $$R = \\frac{s_{L_1}^2 \\, L_1}{s_{L_2}^2 \\, L_2}.$$\n  In the self-averaging regime, $R$ should be close to $1$. Your program must also output a boolean that is true if $\\lvert R - 1 \\rvert  0.5$ and false otherwise.\n\nTest suite:\n\n- Case 1 (baseline, moderate disorder): $L = 2000$, $W = 2.0$, $E = 0.0$, $N = 1000$, seed $= 12345$.\n- Case 2 (weak disorder at band center): $L = 2000$, $W = 0.5$, $E = 0.0$, $N = 1000$, seed $= 23456$.\n- Case 3 (short chain, same parameters as Case 1 to probe self-averaging): $L = 400$, $W = 2.0$, $E = 0.0$, $N = 1000$, seed $= 34567$.\n- Case 4 (near band edge at weak disorder): $L = 2000$, $W = 0.5$, $E = 1.8$, $N = 1000$, seed $= 45678$.\n\nFinal output specification:\n\n- For each case $i \\in \\{1,2,3,4\\}$, produce a list $[\\mu_i, s_i^2, \\hat{\\xi}_i]$ of three floats in this order, where $\\mu_i$ is the sample mean of $\\gamma_L$, $s_i^2$ is the unbiased sample variance of $\\gamma_L$, and $\\hat{\\xi}_i = 1/\\mu_i$.\n- Also produce the self-averaging ratio and boolean for Cases 1 and 3 as a list $[R_{1,3}, \\mathrm{SA\\_ok}_{1,3}]$, where $R_{1,3}$ is a float and $\\mathrm{SA\\_ok}_{1,3}$ is a boolean as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with nested lists for each case and the self-averaging check. Use six decimal places for all floats. For example, the printed output must have the exact form:\n  [[mu1,var1,xi1],[mu2,var2,xi2],[mu3,var3,xi3],[mu4,var4,xi4],[R13,True]]\nReplace the placeholders with the computed numbers, and print True or False as appropriate; do not print any other text.", "solution": "The user has requested the formulation and implementation of a numerical algorithm to estimate the largest Lyapunov exponent for a $1$-dimensional disordered tight-binding chain, analyze its statistical distribution, and verify the property of self-averaging.\n\nThe physical system is described by the discrete, time-independent Schrödinger equation on a $1$-dimensional lattice:\n$$\n\\psi_{i+1} + \\psi_{i-1} + \\varepsilon_i \\psi_i = E \\psi_i\n$$\nHere, $\\psi_i$ is the wavefunction amplitude at site $i$, $E$ is the energy, and the nearest-neighbor hopping amplitude is set to $1$. The disorder is introduced through the onsite potentials $\\varepsilon_i$, which are independent and identically distributed random variables drawn from a uniform distribution $U[-\\frac{W}{2}, \\frac{W}{2}]$, where $W$ is the disorder strength.\n\nTo analyze the spatial evolution of the wavefunction, we can rearrange the Schrödinger equation into a recursive form. This gives rise to the transfer matrix formalism. Let us define a $2$-component state vector at site $i$ as $\\vec{\\Psi}_{i-1} = \\begin{pmatrix}\\psi_i \\\\ \\psi_{i-1}\\end{pmatrix}$. The equation $\\psi_{i+1} = (E - \\varepsilon_i)\\psi_i - \\psi_{i-1}$ dictates the evolution to the next site. The new state vector $\\vec{\\Psi}_i = \\begin{pmatrix}\\psi_{i+1} \\\\ \\psi_i\\end{pmatrix}$ is thus related to the previous one by a linear transformation:\n$$\n\\vec{\\Psi}_i = \\begin{pmatrix} E - \\varepsilon_i  -1 \\\\ 1  0 \\end{pmatrix} \\vec{\\Psi}_{i-1} \\equiv M_i \\vec{\\Psi}_{i-1}\n$$\nThe matrix $M_i$ is the transfer matrix at site $i$. Its elements depend on the random potential $\\varepsilon_i$.\n\nAfter $L$ sites, the wavefunction vector $\\vec{\\Psi}_L$ is related to the initial vector $\\vec{\\Psi}_0 = \\begin{pmatrix}\\psi_1 \\\\ \\psi_0\\end{pmatrix}$ by the product of $L$ random matrices:\n$$\n\\vec{\\Psi}_L = M_L M_{L-1} \\cdots M_1 \\vec{\\Psi}_0 = P_L \\vec{\\Psi}_0\n$$\nAccording to Oseledec's multiplicative ergodic theorem, for almost any initial vector $\\vec{\\Psi}_0$, the norm of $\\vec{\\Psi}_L$ grows or decays exponentially with $L$. The rate of this exponential change is given by the largest Lyapunov exponent, $\\gamma$:\n$$\n\\gamma = \\lim_{L \\to \\infty} \\frac{1}{L} \\ln \\left\\| P_L \\vec{\\Psi}_0 \\right\\|\n$$\nIn the context of the $1$-dimensional Anderson localization problem, all quantum states are localized for any non-zero disorder strength $W>0$. The wavefunction envelope decays exponentially from its localization center as $|\\psi_i| \\sim e^{-|i-i_0|/\\xi}$, where $\\xi$ is the localization length. The localization length is directly related to the largest Lyapunov exponent by $\\xi = 1/\\gamma$.\n\nA naive numerical computation of $\\gamma$ by first calculating the product matrix $P_L$ and then its norm is numerically unstable. The norm of $\\vec{\\Psi}_i$ grows exponentially, quickly leading to floating-point overflow. To circumvent this, a numerically stable procedure involving iterative renormalization is employed. This method is analogous to the power iteration method for finding the largest eigenvalue of a single matrix, but adapted for a product of matrices.\n\nThe algorithm proceeds as follows for a single realization of disorder:\n1.  Initialize a state vector $\\vec{v}_0$ with unit Euclidean norm, e.g., $\\vec{v}_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, corresponding to an initial choice of $(\\psi_1, \\psi_0) = (1,0)$.\n2.  For each site $i$ from $1$ to $L$:\n    a. Generate a random onsite potential $\\varepsilon_i$ and form the transfer matrix $M_i$.\n    b. Apply the transfer matrix to the current normalized vector: $\\vec{u}_i = M_i \\vec{v}_{i-1}$.\n    c. Calculate the Euclidean norm of the resulting vector: $n_i = \\|\\vec{u}_i\\|$. This factor represents the growth of the norm at this step.\n    d. Renormalize the vector to maintain unit norm for the next iteration: $\\vec{v}_i = \\vec{u}_i / n_i$.\n    e. Store the logarithm of the norm, $\\ln n_i$.\n\nAfter $L$ steps, the total product of norms gives the overall growth factor. The finite-size Lyapunov exponent, $\\gamma_L$, for this single realization is the average of the logarithmic growth factors:\n$$\n\\gamma_L = \\frac{1}{L} \\sum_{i=1}^L \\ln n_i\n$$\nTo study the statistical properties, this procedure is repeated for an ensemble of $N$ independent disorder realizations. This yields a set of $N$ values $\\{\\gamma_L^{(j)}\\}_{j=1}^N$. From this empirical distribution, we compute the sample mean $\\mu_L$ and the unbiased sample variance $s_L^2$:\n$$\n\\mu_L = \\frac{1}{N} \\sum_{j=1}^{N} \\gamma_L^{(j)}\n\\quad \\text{and} \\quad\ns_L^2 = \\frac{1}{N-1} \\sum_{j=1}^{N} (\\gamma_L^{(j)} - \\mu_L)^2\n$$\nThe typical localization length is then estimated as $\\hat{\\xi}_{\\mathrm{typ}} = 1/\\mu_L$.\n\nThe property of self-averaging is crucial in the physics of disordered systems. It implies that for sufficiently large systems, a single large sample behaves like the ensemble average. For the Lyapunov exponent, this means that the distribution of $\\gamma_L$ becomes progressively narrower as $L$ increases. The terms $\\ln n_i$ in the sum for $\\gamma_L$ are random variables with short-range correlations. By the Central Limit Theorem, the variance of their sum scales linearly with $L$, and thus the variance of their mean, $s_L^2 = \\text{Var}(\\gamma_L)$, should scale as $1/L$:\n$$\ns_L^2 \\propto \\frac{1}{L}\n$$\nThis implies that the product $s_L^2 L$ should be approximately constant for large $L$. To verify this, we compare two simulations with identical parameters $(W, E)$ but different lengths $L_1$ and $L_2$. We compute the ratio:\n$$\nR = \\frac{s_{L_1}^2 L_1}{s_{L_2}^2 L_2}\n$$\nIf self-averaging holds, this ratio $R$ should be close to $1$. The problem defines a tolerance $|R-1|  0.5$ to test this.\n\nThe program implementation will perform these calculations for $N$ realizations in parallel using vectorized `numpy` operations for efficiency. A single function will encapsulate the calculation for a given set of parameters $(L, W, E, N, \\text{seed})$. An array of shape $(N, 2)$ will hold the state vectors for all realizations simultaneously. At each lattice site $i$, an array of $N$ random potentials $\\varepsilon_i$ is generated, and the transfer matrix operation, norm calculation, and renormalization are applied to all $N$ vectors at once. The final statistics and the self-averaging check are then computed from the resulting ensemble of $\\gamma_L$ values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_stats(L, W, E, N, seed):\n    \"\"\"\n    Calculates the distribution of finite-size Lyapunov exponents for a 1D\n    tight-binding chain.\n\n    Args:\n        L (int): Length of the chain.\n        W (float): Disorder strength.\n        E (float): Energy.\n        N (int): Number of disorder realizations (ensemble size).\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing:\n            - mu_L (float): Sample mean of the Lyapunov exponent.\n            - s_L_sq (float): Unbiased sample variance of the Lyapunov exponent.\n            - xi_typ (float): Estimated localization length (1 / mu_L).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialize N state vectors, each as (psi_1, psi_0) = (1, 0).\n    # Shape: (N, 2)\n    psi_vectors = np.zeros((N, 2))\n    psi_vectors[:, 0] = 1.0\n\n    # Array to accumulate the sum of log-norms for each realization.\n    log_norm_sum = np.zeros(N)\n\n    for _ in range(L):\n        # Generate N random onsite potentials from U[-W/2, W/2].\n        epsilons = rng.uniform(-W / 2.0, W / 2.0, size=N)\n\n        # Apply the transfer matrix to all N vectors simultaneously.\n        # Vector is (psi_current, psi_previous).\n        # New vector is (psi_next, psi_current).\n        psi_current = psi_vectors[:, 0]\n        psi_previous = psi_vectors[:, 1]\n        \n        psi_next = (E - epsilons) * psi_current - psi_previous\n        \n        # This temporary copy is needed for vectorization.\n        new_psi_vectors = np.empty_like(psi_vectors)\n        new_psi_vectors[:, 0] = psi_next\n        new_psi_vectors[:, 1] = psi_current\n\n        # Calculate the Euclidean norms of the N new vectors.\n        norms = np.linalg.norm(new_psi_vectors, axis=1)\n\n        # Accumulate the logarithm of the norms.\n        log_norm_sum += np.log(norms)\n\n        # Renormalize the state vectors to unit norm for the next step.\n        # Use np.newaxis to ensure correct broadcasting (N, 2) / (N, 1).\n        psi_vectors = new_psi_vectors / norms[:, np.newaxis]\n\n    # Calculate finite-length Lyapunov exponent for each realization.\n    gamma_L = log_norm_sum / L\n\n    # Compute sample mean and unbiased sample variance over the ensemble.\n    mu_L = np.mean(gamma_L)\n    s_L_sq = np.var(gamma_L, ddof=1)\n\n    # Estimate the typical localization length.\n    xi_typ = 1.0 / mu_L if mu_L > 0 else np.inf\n\n    return mu_L, s_L_sq, xi_typ\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, W, E, N, seed)\n        (2000, 2.0, 0.0, 1000, 12345),  # Case 1\n        (2000, 0.5, 0.0, 1000, 23456),  # Case 2\n        (400, 2.0, 0.0, 1000, 34567),   # Case 3\n        (2000, 0.5, 1.8, 1000, 45678),  # Case 4\n    ]\n\n    case_results = []\n    for params in test_cases:\n        mu, var, xi = calculate_stats(*params)\n        case_results.append([mu, var, xi])\n\n    # Self-averaging check for Cases 1 and 3.\n    # L1 from Case 1, L2 from Case 3 (using problem notation)\n    # The problem has L1 and L2, but my cases are 1 and 3.\n    # L_case1 = 2000, s_case1_sq = case_results[0][1]\n    # L_case3 = 400, s_case3_sq = case_results[2][1]\n    # Let's map problem L1 to case 1, problem L2 to case 3 to match the spirit of the check.\n    # But the formula is symmetric, so it doesn't matter.\n    L1 = test_cases[0][0]\n    s_L1_sq = case_results[0][1]\n\n    L3 = test_cases[2][0]\n    s_L3_sq = case_results[2][1]\n\n    R13 = (s_L1_sq * L1) / (s_L3_sq * L3)\n    SA_ok = abs(R13 - 1.0)  0.5\n\n    # Prepare for final output string generation.\n    final_list = case_results + [[R13, SA_ok]]\n\n    # Format the results into the required single-line string.\n    output_parts = []\n    # Format the four main cases.\n    for i in range(4):\n        mu, var, xi = final_list[i]\n        output_parts.append(f\"[{mu:.6f},{var:.6f},{xi:.6f}]\")\n    \n    # Format the self-averaging check part.\n    r_val, sa_bool = final_list[4]\n    sa_bool_str = 'True' if sa_bool else 'False'\n    output_parts.append(f\"[{r_val:.6f},{sa_bool_str}]\")\n\n    print(f\"[{','.join(output_parts)}]\")\n\n\nsolve()\n```", "id": "3005640"}]}