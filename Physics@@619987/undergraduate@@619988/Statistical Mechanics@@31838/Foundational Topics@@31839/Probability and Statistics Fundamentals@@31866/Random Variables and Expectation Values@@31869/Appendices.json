{"hands_on_practices": [{"introduction": "This first exercise is a fundamental warm-up, designed to solidify your understanding of the two most basic properties of a discrete random variable. You'll first need to determine the normalization constant for a given probability mass function, ensuring that the total probability is one. Then, you'll apply the core definition to calculate the expected value, $E[K]$, a key measure of central tendency. Mastering this two-step process [@problem_id:14334] is the first essential skill in the statistical description of physical systems.", "problem": "Let $K$ be a discrete random variable whose set of possible outcomes (its support) is $S = \\{1, 2, 3, 4\\}$. The probability mass function (PMF) of $K$, denoted by $p(k) = P(K=k)$, is known to be directly proportional to the outcome value $k$. In other words, the PMF can be written as:\n$$ p(k) = Ck \\quad \\text{for } k \\in S $$\nwhere $C$ is a normalization constant.\n\nFor a discrete random variable, the PMF must satisfy the normalization condition:\n$$ \\sum_{k \\in S} p(k) = 1 $$\n\nThe expected value (or mean) of the random variable $K$, denoted by $E[K]$, is defined as:\n$$ E[K] = \\sum_{k \\in S} k \\cdot p(k) $$\n\nUsing the information provided, derive the exact numerical value of the expected value, $E[K]$.", "solution": "First, apply the normalization condition:\n$$\\sum_{k\\in S} p(k) = \\sum_{k=1}^{4} Ck = C\\sum_{k=1}^{4} k = C(1+2+3+4) = 1.$$\n\nHence\n$$C\\cdot 10 = 1 \\quad\\Longrightarrow\\quad C = \\tfrac{1}{10}.$$\n\nNext, compute the expected value:\n$$E[K] = \\sum_{k\\in S} k\\,p(k) = \\sum_{k=1}^{4} k \\cdot \\frac{1}{10}k = \\frac{1}{10}\\sum_{k=1}^{4} k^2 = \\frac{1}{10}(1^2+2^2+3^2+4^2) = \\frac{30}{10} = 3.$$", "answer": "$$\\boxed{3}$$", "id": "14334"}, {"introduction": "Now, let's transition from discrete steps to the continuous domain, a shift that is crucial in many areas of physics. This problem [@problem_id:3239] asks you to derive the expected value for a uniform distribution, the simplest continuous probability distribution. By replacing the summation from the discrete case with an integral, you will not only practice a core mathematical technique but also arrive at an intuitive and widely applicable result for the 'average' outcome.", "problem": "A continuous random variable $X$ is said to follow a uniform distribution on the interval $[a, b]$, denoted as $X \\sim U(a, b)$, if its probability density function (PDF) is constant over the interval and zero elsewhere. The PDF, $f(x)$, is given by:\n\n$$\nf(x) = \\begin{cases}\n\\frac{1}{b-a}  \\text{if } a \\le x \\le b \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n\nThe expected value (or mean) of a continuous random variable $X$, denoted as $E[X]$, is defined as:\n\n$$\nE[X] = \\int_{-\\infty}^{\\infty} x f(x) \\, dx\n$$\n\nUsing these definitions, derive the general expression for the expected value, $E[X]$, of a random variable $X$ that follows a continuous uniform distribution on the interval $[a, b]$, where $a  b$.", "solution": "To find the expected value $E[X]$ for a random variable $X \\sim U(a, b)$, we start with the definition of the expected value for a continuous random variable:\n\n$$\nE[X] = \\int_{-\\infty}^{\\infty} x f(x) \\, dx\n$$\n\nNext, we substitute the probability density function (PDF) for the uniform distribution into this integral. The function $f(x)$ is non-zero only for $x$ in the interval $[a, b]$. Therefore, the integral over $(-\\infty, \\infty)$ can be reduced to an integral over $[a, b]$, as $f(x)=0$ outside this interval, making the integrand $x f(x)$ zero everywhere else.\n\n$$\nE[X] = \\int_{a}^{b} x \\left( \\frac{1}{b-a} \\right) \\, dx\n$$\n\nThe term $\\frac{1}{b-a}$ is a constant with respect to the integration variable $x$, so it can be factored out of the integral:\n\n$$\nE[X] = \\frac{1}{b-a} \\int_{a}^{b} x \\, dx\n$$\n\nNow, we compute the definite integral of $x$. The antiderivative of $x$ is $\\frac{x^2}{2}$.\n\n$$\n\\int_{a}^{b} x \\, dx = \\left[ \\frac{x^2}{2} \\right]_{a}^{b}\n$$\n\nWe evaluate the antiderivative at the upper and lower limits of integration:\n\n$$\n\\left[ \\frac{x^2}{2} \\right]_{a}^{b} = \\frac{b^2}{2} - \\frac{a^2}{2} = \\frac{b^2 - a^2}{2}\n$$\n\nSubstitute this result back into the expression for $E[X]$:\n\n$$\nE[X] = \\frac{1}{b-a} \\left( \\frac{b^2 - a^2}{2} \\right)\n$$\n\nTo simplify this expression, we use the difference of squares factorization, $b^2 - a^2 = (b-a)(b+a)$.\n\n$$\nE[X] = \\frac{1}{b-a} \\left( \\frac{(b-a)(b+a)}{2} \\right)\n$$\n\nSince $a  b$, the term $(b-a)$ is non-zero, so we can cancel it from the numerator and the denominator.\n\n$$\nE[X] = \\frac{a+b}{2}\n$$\n\nThis is the general expression for the expected value of a random variable uniformly distributed on the interval $[a, b]$. It represents the midpoint of the interval.", "answer": "$$\n\\boxed{\\frac{a+b}{2}}\n$$", "id": "3239"}, {"introduction": "Here, we move from single random variables to a dynamic process: the random walk. This problem [@problem_id:1989246] models a particle's movement under a bias, a scenario common in statistical mechanics. The key to finding the final expected position, $\\langle x_N \\rangle$, is not to track every possible path, but to use the powerful principle of linearity of expectation. This practice will show you how to decompose a complex, multi-step system into the expectation of a single step, a vital problem-solving strategy in physics and beyond.", "problem": "Consider a simplified one-dimensional model for the phenomenon of electrophoretic drift, where a charged particle moves through a gel matrix under the influence of a uniform external electric field. The particle starts at an origin point, $x=0$, and performs a random walk consisting of $N$ discrete steps on a lattice.\n\nEach step is of a fixed length $\\ell$. The external field creates a bias in the direction of the steps. The probability of the particle taking a step in the positive direction is given by $p = \\frac{1}{2}(1 + \\tanh(\\gamma))$, and consequently, the probability of taking a step in the negative direction is $1-p$. The dimensionless parameter $\\gamma$ is a constant that quantifies the strength of the external field's influence relative to thermal fluctuations.\n\nAssuming each of the $N$ steps is statistically independent, determine the expectation value of the particle's final position, $\\langle x_N \\rangle$. Express your answer as a closed-form analytic expression in terms of $N$, $\\ell$, and $\\gamma$.", "solution": "Let $X_{i}$ denote the displacement during step $i$, with $X_{i} \\in \\{+\\ell,-\\ell\\}$, where $+\\ell$ occurs with probability $p=\\frac{1}{2}\\left(1+\\tanh(\\gamma)\\right)$ and $-\\ell$ occurs with probability $1-p$. The total position after $N$ steps starting from $x_{0}=0$ is\n$$\nx_{N}=\\sum_{i=1}^{N}X_{i}.\n$$\nBy linearity of expectation and the statistical independence of steps,\n$$\n\\langle x_{N}\\rangle=\\sum_{i=1}^{N}\\langle X_{i}\\rangle=N\\langle X_{1}\\rangle.\n$$\nFor a single step,\n$$\n\\langle X_{1}\\rangle=\\ell\\cdot p+(-\\ell)\\cdot(1-p)=\\ell\\,(2p-1).\n$$\nSubstituting $p=\\frac{1}{2}\\left(1+\\tanh(\\gamma)\\right)$ gives\n$$\n2p-1=\\tanh(\\gamma),\n$$\nhence\n$$\n\\langle X_{1}\\rangle=\\ell\\,\\tanh(\\gamma).\n$$\nTherefore,\n$$\n\\langle x_{N}\\rangle=N\\ell\\,\\tanh(\\gamma).\n$$\nThis is a closed-form expression in terms of $N$, $\\ell$, and $\\gamma$.", "answer": "$$\\boxed{N\\ell\\tanh(\\gamma)}$$", "id": "1989246"}]}