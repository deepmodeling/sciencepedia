{"hands_on_practices": [{"introduction": "Statistical mechanics builds macroscopic understanding from microscopic rules. This first exercise provides a classic example of this principle by exploring the spatial distribution of ideal gas particles. By applying basic combinatorial reasoning, you will derive the probability of finding a specific number of particles in a sub-volume, revealing the binomial distribution as the governing law for these fluctuations [@problem_id:1962013].", "problem": "An isolated rigid container of total volume $V$ is partitioned into two regions by a fixed, permeable membrane. The first region has a volume $V_1$ and the second has a volume $V_2$, such that $V = V_1 + V_2$. The container holds a total of $N$ non-interacting, indistinguishable particles of an ideal gas in thermal equilibrium. A single particle has an equal probability of being found at any location within the total volume $V$.\n\nLet $n_1$ be the number of particles found in the first region (volume $V_1$) at any given instant. Determine the probability distribution function, $P(n_1)$, which gives the probability of finding exactly $n_1$ particles in the volume $V_1$. Express your answer as a function of $N$, $n_1$, $V_1$, and $V_2$.", "solution": "We consider an ideal gas of $N$ non-interacting, indistinguishable particles uniformly distributed over the total volume $V = V_{1} + V_{2}$. The non-interacting nature implies that particle positions are independent, and uniform spatial distribution implies that the probability for any single particle to be in $V_{1}$ is the volume fraction\n$$\np = \\frac{V_{1}}{V} = \\frac{V_{1}}{V_{1} + V_{2}}.\n$$\nFor each particle $i \\in \\{1,\\dots,N\\}$, define the indicator random variable $X_{i}$ by $X_{i} = 1$ if the particle is in $V_{1}$ and $X_{i} = 0$ if it is in $V_{2}$. Then $X_{i}$ are independent and identically distributed Bernoulli random variables with parameter $p$, i.e.,\n$$\n\\Pr(X_{i} = 1) = p, \\quad \\Pr(X_{i} = 0) = 1 - p.\n$$\nThe number of particles in $V_{1}$ is\n$$\nn_{1} = \\sum_{i=1}^{N} X_{i}.\n$$\nBy the properties of sums of independent Bernoulli random variables, $n_{1}$ follows the binomial distribution. To derive the probability mass function explicitly, note that the event of having exactly $n_{1}$ particles in $V_{1}$ corresponds to choosing which $n_{1}$ of the $N$ particles are in $V_{1}$ (there are $\\binom{N}{n_{1}}$ such choices), and for any such choice the probability is $p^{n_{1}}(1-p)^{N-n_{1}}$ due to independence. Therefore,\n$$\nP(n_{1}) = \\binom{N}{n_{1}} p^{n_{1}} (1-p)^{N-n_{1}}.\n$$\nSubstituting $p = \\frac{V_{1}}{V_{1}+V_{2}}$ and $1-p = \\frac{V_{2}}{V_{1}+V_{2}}$ gives\n$$\nP(n_{1}) = \\binom{N}{n_{1}} \\left(\\frac{V_{1}}{V_{1}+V_{2}}\\right)^{n_{1}} \\left(\\frac{V_{2}}{V_{1}+V_{2}}\\right)^{N-n_{1}}.\n$$\nAs a consistency check, normalization holds by the binomial theorem:\n$$\n\\sum_{n_{1}=0}^{N} P(n_{1}) = \\sum_{n_{1}=0}^{N} \\binom{N}{n_{1}} p^{n_{1}} (1-p)^{N-n_{1}} = (p + (1-p))^{N} = 1^{N} = 1.\n$$\nHence, the required probability distribution function is the binomial distribution with parameter $p = \\frac{V_{1}}{V_{1}+V_{2}}$.", "answer": "$$\\boxed{\\binom{N}{n_{1}}\\left(\\frac{V_{1}}{V_{1}+V_{2}}\\right)^{n_{1}}\\left(\\frac{V_{2}}{V_{1}+V_{2}}\\right)^{N-n_{1}}}$$", "id": "1962013"}, {"introduction": "Moving from spatial arrangements to energy, this problem introduces the powerful canonical ensemble, which describes systems in thermal contact with a heat bath. You will calculate the probability of a specific total energy for a small system of molecules with quantized energy levels. This practice is key to understanding how temperature dictates the distribution of energy among available states, a central concept in all of statistical thermodynamics [@problem_id:1962016].", "problem": "Consider a simplified model for a nanoscale data storage device. The device consists of a linear array of $N=4$ distinguishable, non-interacting molecular units. Each molecular unit can exist in one of two distinct energy states: a ground state with energy $0$ or an excited state with energy $\\epsilon  0$. The entire array is maintained in thermal equilibrium with its surroundings, acting as a heat bath at a constant absolute temperature $T$. The Boltzmann constant is denoted by $k_B$.\n\nDetermine the probability that the total energy of the entire 4-unit system is exactly $2\\epsilon$. Provide your answer as a closed-form analytical expression in terms of $\\epsilon$, $T$, and $k_B$.", "solution": "We model the array in the canonical ensemble at temperature $T$. For a microstate $s$ with energy $E_{s}$, the canonical probability is\n$$\nP(s)=\\frac{\\exp(-\\beta E_{s})}{Z}, \\quad \\beta=\\frac{1}{k_{B}T}, \\quad Z=\\sum_{s}\\exp(-\\beta E_{s}).\n$$\nBecause the $4$ units are distinguishable and non-interacting, the total partition function factorizes as\n$$\nZ=z^{4}, \\quad z=\\sum_{\\text{single-unit states}} \\exp(-\\beta E)=1+\\exp(-\\beta \\epsilon),\n$$\nso\n$$\nZ=\\left(1+\\exp(-\\beta \\epsilon)\\right)^{4}.\n$$\nIf exactly $k$ units are excited, the total energy is $E=k\\epsilon$, and the number of such microstates (degeneracy) is $\\binom{4}{k}$. All these microstates have the same Boltzmann weight $\\exp(-\\beta k\\epsilon)$. Therefore, the probability that the total energy equals $k\\epsilon$ is\n$$\nP(E=k\\epsilon)=\\frac{\\binom{4}{k}\\exp(-\\beta k\\epsilon)}{\\left(1+\\exp(-\\beta \\epsilon)\\right)^{4}}.\n$$\nFor $k=2$,\n$$\nP(E=2\\epsilon)=\\frac{\\binom{4}{2}\\exp\\!\\left(-2\\beta \\epsilon\\right)}{\\left(1+\\exp(-\\beta \\epsilon)\\right)^{4}}=\\frac{6\\,\\exp\\!\\left(-\\frac{2\\epsilon}{k_{B}T}\\right)}{\\left(1+\\exp\\!\\left(-\\frac{\\epsilon}{k_{B}T}\\right)\\right)^{4}}.\n$$\nThis is the requested closed-form expression in terms of $\\epsilon$, $T$, and $k_{B}$.", "answer": "$$\\boxed{\\frac{6\\,\\exp\\!\\left(-\\frac{2\\epsilon}{k_{B}T}\\right)}{\\left(1+\\exp\\!\\left(-\\frac{\\epsilon}{k_{B}T}\\right)\\right)^{4}}}$$", "id": "1962016"}, {"introduction": "Our focus now shifts from discrete counts to continuous variables, a hallmark of kinetic theory. Starting with the two-dimensional Maxwell-Boltzmann distribution, this exercise challenges you to derive the probability distribution for a new variable: the ratio of velocity components. This hands-on practice demonstrates the essential mathematical technique of changing variables for probability densities and reveals how a new, and perhaps unexpected, distribution can emerge from a familiar one [@problem_id:1962025].", "problem": "Consider a two-dimensional (2D) ideal gas, which can serve as a simple model for a system of classical particles adsorbed on a flat surface. The gas is in thermal equilibrium at a constant temperature $T$. The particles, each of mass $m$, move freely in the $xy$-plane. The probability of finding a particle with velocity components in the infinitesimal range $[v_x, v_x+dv_x]$ and $[v_y, v_y+dv_y]$ is described by the joint probability density function $f(v_x, v_y)$, given by the Maxwell-Boltzmann distribution for two dimensions:\n\n$$ f(v_x, v_y) = \\frac{m}{2\\pi k_B T} \\exp\\left(-\\frac{m(v_x^2 + v_y^2)}{2k_B T}\\right) $$\n\nwhere $k_B$ is the Boltzmann constant.\n\nLet's define a new dimensionless random variable $\\eta$ as the ratio of the velocity components:\n\n$$ \\eta = \\frac{v_y}{v_x} $$\n\nThis variable characterizes the direction of the particle's velocity vector relative to the $x$-axis. Determine the probability density function, $P(\\eta)$, for this variable. Present your final answer as a single closed-form analytic expression in terms of $\\eta$ and fundamental mathematical constants.", "solution": "The goal is to find the probability density function $P(\\eta)$ for the variable $\\eta = v_y/v_x$, starting from the joint probability density function $f(v_x, v_y)$ for the velocity components $v_x$ and $v_y$.\n\nThe given joint probability density function is:\n$$ f(v_x, v_y) = \\frac{m}{2\\pi k_B T} \\exp\\left(-\\frac{m(v_x^2 + v_y^2)}{2k_B T}\\right) $$\nLet's define a constant $\\alpha = \\frac{m}{2k_B T}$ to simplify the expression:\n$$ f(v_x, v_y) = \\frac{\\alpha}{\\pi} \\exp\\left(-\\alpha(v_x^2 + v_y^2)\\right) $$\nWe are performing a change of variables from $(v_x, v_y)$ to a new set of variables that includes $\\eta$. Let's choose the new variables to be $(\\eta, u)$, where $\\eta = v_y/v_x$ and we introduce a second variable, for simplicity, $u = v_x$.\n\nThe next step is to express the original variables $(v_x, v_y)$ in terms of the new variables $(\\eta, u)$:\n$$ v_x = u $$\n$$ v_y = \\eta u $$\n\nThe probability density function in the new variables, let's call it $g(\\eta, u)$, is related to the original one by multiplying by the absolute value of the Jacobian determinant of the transformation. The Jacobian determinant $J$ is given by:\n$$ J = \\det \\begin{pmatrix} \\frac{\\partial v_x}{\\partial \\eta}  \\frac{\\partial v_x}{\\partial u} \\\\ \\frac{\\partial v_y}{\\partial \\eta}  \\frac{\\partial v_y}{\\partial u} \\end{pmatrix} $$\nWe compute the partial derivatives:\n$$ \\frac{\\partial v_x}{\\partial \\eta} = 0, \\quad \\frac{\\partial v_x}{\\partial u} = 1 $$\n$$ \\frac{\\partial v_y}{\\partial \\eta} = u, \\quad \\frac{\\partial v_y}{\\partial u} = \\eta $$\nSubstituting these into the determinant formula:\n$$ J = \\det \\begin{pmatrix} 0  1 \\\\ u  \\eta \\end{pmatrix} = (0)(\\eta) - (1)(u) = -u $$\nThe absolute value of the Jacobian is $|J| = |-u| = |u|$.\n\nThe joint probability density function for the new variables $(\\eta, u)$ is given by the transformation formula:\n$$ g(\\eta, u) = f(v_x(\\eta, u), v_y(\\eta, u)) |J| $$\nSubstituting $v_x = u$, $v_y = \\eta u$, and $|J| = |u|$:\n$$ g(\\eta, u) = \\frac{\\alpha}{\\pi} \\exp\\left(-\\alpha(u^2 + (\\eta u)^2)\\right) |u| $$\n$$ g(\\eta, u) = \\frac{\\alpha}{\\pi} |u| \\exp\\left(-\\alpha u^2(1 + \\eta^2)\\right) $$\nTo find the probability density function $P(\\eta)$ for the variable $\\eta$, we must integrate the joint density $g(\\eta, u)$ over all possible values of the other variable, $u$. Since $u=v_x$, its range is from $-\\infty$ to $+\\infty$.\n$$ P(\\eta) = \\int_{-\\infty}^{\\infty} g(\\eta, u) \\, du = \\int_{-\\infty}^{\\infty} \\frac{\\alpha}{\\pi} |u| \\exp\\left(-\\alpha u^2(1 + \\eta^2)\\right) \\, du $$\nThe integrand is an even function of $u$ (since it involves $|u|$ and $u^2$). Therefore, we can simplify the integral:\n$$ P(\\eta) = 2 \\int_{0}^{\\infty} \\frac{\\alpha}{\\pi} u \\exp\\left(-\\alpha u^2(1 + \\eta^2)\\right) \\, du $$\n$$ P(\\eta) = \\frac{2\\alpha}{\\pi} \\int_{0}^{\\infty} u \\exp\\left(-\\alpha(1 + \\eta^2)u^2\\right) \\, du $$\nTo solve this integral, we use a substitution. Let $w = \\alpha(1 + \\eta^2)u^2$.\nThen, the differential is $dw = 2\\alpha(1 + \\eta^2)u \\, du$.\nRearranging this gives $u \\, du = \\frac{dw}{2\\alpha(1 + \\eta^2)}$.\nWe also need to transform the integration limits. When $u=0$, $w=0$. As $u \\to \\infty$, $w \\to \\infty$. The limits remain $[0, \\infty)$.\n\nSubstituting $w$ and $u \\, du$ into the integral for $P(\\eta)$:\n$$ P(\\eta) = \\frac{2\\alpha}{\\pi} \\int_{0}^{\\infty} \\exp(-w) \\left(\\frac{dw}{2\\alpha(1 + \\eta^2)}\\right) $$\nThe constants can be taken out of the integral:\n$$ P(\\eta) = \\frac{2\\alpha}{\\pi} \\frac{1}{2\\alpha(1 + \\eta^2)} \\int_{0}^{\\infty} \\exp(-w) \\, dw $$\nThe term $\\frac{2\\alpha}{2\\alpha}$ cancels out. We are left with:\n$$ P(\\eta) = \\frac{1}{\\pi(1 + \\eta^2)} \\int_{0}^{\\infty} \\exp(-w) \\, dw $$\nThe remaining definite integral is a standard result:\n$$ \\int_{0}^{\\infty} \\exp(-w) \\, dw = [-\\exp(-w)]_{0}^{\\infty} = (-\\exp(-\\infty)) - (-\\exp(-0)) = 0 - (-1) = 1 $$\nTherefore, the probability density function for $\\eta$ is:\n$$ P(\\eta) = \\frac{1}{\\pi(1 + \\eta^2)} $$\nThis is the probability density function for a standard Cauchy (or Lorentz) distribution. Notably, the final expression is independent of the particle mass $m$ and the temperature $T$, as the constant $\\alpha$ has cancelled out.", "answer": "$$ \\boxed{\\frac{1}{\\pi(1 + \\eta^{2})}} $$", "id": "1962025"}]}