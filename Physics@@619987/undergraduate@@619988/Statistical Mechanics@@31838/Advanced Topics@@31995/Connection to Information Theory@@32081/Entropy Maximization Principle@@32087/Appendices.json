{"hands_on_practices": [{"introduction": "Our first practice problem grounds the principle of maximum entropy in a tangible, discrete system. We will determine the most probable distribution of microorganisms across different ecological niches, given only the average metabolic energy cost for the colony [@problem_id:1963872]. This exercise demonstrates how to apply the method of Lagrange multipliers to a discrete probability distribution, leading to the celebrated Boltzmann-Gibbs distribution, a cornerstone of statistical mechanics.", "problem": "A simplified model for resource competition in a biological system considers a colony of microorganisms distributed among three distinct ecological niches, labeled by an index $x \\in \\{1, 2, 3\\}$. Occupying a niche with index $x$ imposes a specific metabolic energy cost on a microorganism, given by the function $E(x) = \\epsilon_0 x$, where $\\epsilon_0$ is a positive constant representing a fundamental unit of energy.\n\nThe system has reached a steady state, and experimental measurements reveal that the average metabolic energy cost per microorganism across the entire colony is precisely $\\langle E \\rangle = 1.5 \\epsilon_0$.\n\nThe colony is assumed to adopt the most probable macroscopic state, which, according to the principles of statistical mechanics, corresponds to the probability distribution $\\{p(x)\\}$ that maximizes the system's Shannon entropy, $S = -C \\sum_{x=1}^{3} p(x) \\ln(p(x))$, subject to the given constraints. Here, $p(x)$ is the probability of finding a randomly selected microorganism in niche $x$, and $C$ is a positive constant.\n\nFollowing this principle of maximum entropy, determine the probability $p(1)$ of finding a microorganism in niche 1. Round your final answer to four significant figures.", "solution": "We maximize the Shannon entropy subject to normalization and fixed average energy. The entropy is $S=-C\\sum_{x=1}^{3}p(x)\\ln p(x)$ with constraints $\\sum_{x=1}^{3}p(x)=1$ and $\\sum_{x=1}^{3}p(x)E(x)=\\langle E\\rangle=1.5\\,\\epsilon_{0}$, where $E(x)=\\epsilon_{0}x$. Introduce Lagrange multipliers $\\alpha$ and $\\beta$ and define the Lagrangian\n$$\n\\mathcal{L}=-C\\sum_{x=1}^{3}p(x)\\ln p(x)-\\alpha\\left(\\sum_{x=1}^{3}p(x)-1\\right)-\\beta\\left(\\sum_{x=1}^{3}p(x)E(x)-1.5\\,\\epsilon_{0}\\right).\n$$\nStationarity with respect to $p(x)$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p(x)}=-C\\left(\\ln p(x)+1\\right)-\\alpha-\\beta E(x)=0,\n$$\nso\n$$\n\\ln p(x)=-1-\\frac{\\alpha}{C}-\\frac{\\beta}{C}E(x)\\quad\\Longrightarrow\\quad p(x)=A\\,\\exp\\!\\left(-\\lambda E(x)\\right),\n$$\nwhere $A=\\exp\\!\\left(-1-\\frac{\\alpha}{C}\\right)$ and $\\lambda=\\frac{\\beta}{C}$. Normalizing yields the Gibbs form\n$$\np(x)=\\frac{\\exp\\!\\left(-\\lambda E(x)\\right)}{Z(\\lambda)},\\qquad Z(\\lambda)=\\sum_{y=1}^{3}\\exp\\!\\left(-\\lambda E(y)\\right).\n$$\nWith $E(x)=\\epsilon_{0}x$, define $a\\equiv \\exp\\!\\left(-\\lambda\\epsilon_{0}\\right)>0$, so\n$$\np(x)=\\frac{a^{x}}{\\sum_{y=1}^{3}a^{y}}=\\frac{a^{x}}{a+a^{2}+a^{3}}.\n$$\nThe energy constraint $\\langle E\\rangle=\\epsilon_{0}\\langle x\\rangle=1.5\\,\\epsilon_{0}$ gives $\\langle x\\rangle=1.5$, i.e.\n$$\n\\frac{a+2a^{2}+3a^{3}}{a+a^{2}+a^{3}}=1.5.\n$$\nClearing the denominator and simplifying,\n$$\na+2a^{2}+3a^{3}=1.5(a+a^{2}+a^{3})\\;\\Longrightarrow\\;-a+a^{2}+3a^{3}=0\\;\\Longrightarrow\\;a\\left(3a^{2}+a-1\\right)=0.\n$$\nSince $a>0$, solve $3a^{2}+a-1=0$ to obtain\n$$\na=\\frac{-1+\\sqrt{13}}{6}.\n$$\nThen\n$$\np(1)=\\frac{a}{a+a^{2}+a^{3}}=\\frac{1}{1+a+a^{2}}.\n$$\nUsing $3a^{2}+a-1=0\\Rightarrow a^{2}=\\frac{1-a}{3}$,\n$$\n1+a+a^{2}=1+a+\\frac{1-a}{3}=\\frac{4}{3}+\\frac{2}{3}a=\\frac{2}{3}(2+a),\n$$\nso\n$$\np(1)=\\frac{1}{\\frac{2}{3}(2+a)}=\\frac{3}{2(2+a)}.\n$$\nSubstitute $a=\\frac{\\sqrt{13}-1}{6}$:\n$$\n2+a=\\frac{11+\\sqrt{13}}{6}\\quad\\Longrightarrow\\quad p(1)=\\frac{3}{2}\\cdot\\frac{6}{11+\\sqrt{13}}=\\frac{9}{11+\\sqrt{13}}=\\frac{11-\\sqrt{13}}{12}.\n$$\nNumerically, $\\sqrt{13}\\approx 3.605551275$, hence $p(1)\\approx \\frac{11-3.605551275}{12}\\approx 0.616204060$, which rounded to four significant figures is $0.6162$.", "answer": "$$\\boxed{0.6162}$$", "id": "1963872"}, {"introduction": "Next, we move from discrete states to a continuous domain by modeling the speeds of cars on a highway [@problem_id:1963861]. Knowing only the average speed, what is the most unbiased way to describe the distribution of individual speeds? This problem introduces the use of calculus of variations to maximize entropy for a continuous variable, revealing the exponential distribution as the natural outcome when only the mean of a positive variable is known.", "problem": "A new traffic monitoring system is being tested on a very long, straight highway. This system, due to data bandwidth limitations, only records a single value: the arithmetic mean of the speeds of all observed cars, denoted as $\\bar{v}$. Individual speed measurements are discarded. We are tasked with creating a statistical model for the distribution of car speeds, $v$, based solely on this information. We assume car speeds can be modeled as a continuous non-negative variable, so the domain for the speed is $v \\in [0, \\infty)$.\n\nYour task is to determine the least biased probability density function $p(v)$ that is consistent with the given average speed $\\bar{v}$. The least biased distribution is the one that makes the fewest assumptions about the unknown information while still satisfying all known constraints.\n\nExpress your answer as an analytic expression for $p(v)$ in terms of the speed $v$ and the average speed $\\bar{v}$.", "solution": "We are given only the average speed $\\bar{v}$ and that speeds are continuous with support $v \\in [0,\\infty)$. The least biased distribution under such moment constraints is obtained by maximizing the differential entropy subject to the known constraints. Let $p(v)$ be the density on $[0,\\infty)$.\n\nWe maximize the Shannon differential entropy\n$$\nS[p] = -\\int_{0}^{\\infty} p(v)\\,\\ln p(v)\\,dv\n$$\nsubject to the constraints of normalization and fixed mean:\n$$\n\\int_{0}^{\\infty} p(v)\\,dv = 1, \\quad \\int_{0}^{\\infty} v\\,p(v)\\,dv = \\bar{v}.\n$$\nIntroduce Lagrange multipliers $\\alpha$ and $\\beta$ and define the Lagrangian functional\n$$\n\\mathcal{L}[p] = -\\int_{0}^{\\infty} p(v)\\,\\ln p(v)\\,dv - \\alpha\\left(\\int_{0}^{\\infty} p(v)\\,dv - 1\\right) - \\beta\\left(\\int_{0}^{\\infty} v\\,p(v)\\,dv - \\bar{v}\\right).\n$$\nTake the functional derivative with respect to $p(v)$ and set it to zero for optimality:\n$$\n\\frac{\\delta \\mathcal{L}}{\\delta p(v)} = -\\left(\\ln p(v) + 1\\right) - \\alpha - \\beta v = 0.\n$$\nThis yields\n$$\n\\ln p(v) = -1 - \\alpha - \\beta v \\quad \\Rightarrow \\quad p(v) = \\exp(-1-\\alpha)\\,\\exp(-\\beta v).\n$$\nLet $A = \\exp(-1-\\alpha)$; then\n$$\np(v) = A\\,\\exp(-\\beta v), \\quad v \\ge 0.\n$$\nImpose normalization:\n$$\n\\int_{0}^{\\infty} A\\,\\exp(-\\beta v)\\,dv = \\frac{A}{\\beta} = 1 \\quad \\Rightarrow \\quad A = \\beta,\n$$\nwhich requires $\\beta > 0$ for integrability. Next impose the mean constraint:\n$$\n\\int_{0}^{\\infty} v\\,A\\,\\exp(-\\beta v)\\,dv = \\frac{A}{\\beta^{2}} = \\bar{v}.\n$$\nSubstituting $A = \\beta$ gives\n$$\n\\frac{\\beta}{\\beta^{2}} = \\frac{1}{\\beta} = \\bar{v} \\quad \\Rightarrow \\quad \\beta = \\frac{1}{\\bar{v}}.\n$$\nTherefore, the maximum entropy (least biased) density on $[0,\\infty)$ with given mean $\\bar{v}$ is the exponential distribution\n$$\np(v) = \\frac{1}{\\bar{v}}\\,\\exp\\!\\left(-\\frac{v}{\\bar{v}}\\right), \\quad v \\in [0,\\infty).\n$$\nThis satisfies non-negativity, normalization, and the specified mean, and by construction maximizes entropy under these constraints.", "answer": "$$\\boxed{\\frac{1}{\\bar{v}}\\,\\exp\\!\\left(-\\frac{v}{\\bar{v}}\\right)}$$", "id": "1963861"}, {"introduction": "In our final exercise, we add another layer of information to our continuous system: not only do we know the mean $\\mu$, but we also know the variance $\\sigma^2$ [@problem_id:1963846]. This is a common scenario in data analysis where we have information about both the central tendency and the spread of our data. By maximizing entropy under these two constraints, we will derive the Gaussian (or normal) distribution, elegantly explaining its fundamental importance and ubiquity across the sciences.", "problem": "In statistical mechanics, the principle of maximum entropy states that, subject to known constraints, the most appropriate probability distribution to model a system is the one that maximizes the information entropy. This ensures that the model is as unbiased as possible, incorporating only the information given by the constraints.\n\nConsider a continuous random variable $x$ defined on the entire real line, from $-\\infty$ to $\\infty$. The information content of its probability distribution function, $p(x)$, is quantified by the differential entropy, given by the functional:\n$$S[p] = -\\int_{-\\infty}^{\\infty} p(x) \\ln(p(x)) \\, dx$$\n\nWe seek the specific probability distribution $p(x)$ that maximizes this entropy $S[p]$ while satisfying the following three constraints:\n1.  Normalization: The total probability over the entire domain must be one.\n    $$\\int_{-\\infty}^{\\infty} p(x) \\, dx = 1$$\n2.  Fixed Mean: The distribution must have a specified mean value $\\mu$.\n    $$\\int_{-\\infty}^{\\infty} x p(x) \\, dx = \\mu$$\n3.  Fixed Variance: The distribution must have a specified non-zero variance $\\sigma^2$.\n    $$\\int_{-\\infty}^{\\infty} (x-\\mu)^2 p(x) \\, dx = \\sigma^2$$\n\nDetermine the functional form of this maximally random probability distribution $p(x)$. Your final answer should be a closed-form analytic expression for $p(x)$ in terms of the variable $x$, the mean $\\mu$, and the standard deviation $\\sigma$.", "solution": "We apply the maximum entropy principle with constraints using the method of Lagrange multipliers for functionals. Introduce multipliers $\\lambda_{0}$, $\\lambda_{1}$, and $\\lambda_{2}$ for normalization, mean, and variance, respectively, and define the augmented functional\n$$\n\\mathcal{J}[p] = -\\int_{-\\infty}^{\\infty} p(x)\\ln p(x)\\,dx - \\lambda_{0}\\left(\\int_{-\\infty}^{\\infty} p(x)\\,dx - 1\\right) - \\lambda_{1}\\left(\\int_{-\\infty}^{\\infty} x\\,p(x)\\,dx - \\mu\\right) - \\lambda_{2}\\left(\\int_{-\\infty}^{\\infty} (x-\\mu)^{2} p(x)\\,dx - \\sigma^{2}\\right).\n$$\nStationarity under variations $\\delta p(x)$ subject to $p(x)\\ge 0$ yields the Euler–Lagrange condition from the vanishing functional derivative:\n$$\n\\frac{\\delta \\mathcal{J}}{\\delta p(x)} = -\\left(\\ln p(x) + 1\\right) - \\lambda_{0} - \\lambda_{1} x - \\lambda_{2} (x-\\mu)^{2} = 0.\n$$\nSolving for $p(x)$ gives\n$$\n\\ln p(x) = -1 - \\lambda_{0} - \\lambda_{1} x - \\lambda_{2} (x-\\mu)^{2},\n\\quad\\text{so}\\quad\np(x) = A\\,\\exp\\left(-\\lambda_{1} x - \\lambda_{2} (x-\\mu)^{2}\\right),\n$$\nwhere $A = \\exp(-1-\\lambda_{0})$ absorbs constants. For $p(x)$ to be normalizable on $(-\\infty,\\infty)$, we must have $\\lambda_{2} > 0$; otherwise the integral diverges.\n\nComplete the square in the exponent to identify the implied mean. Write\n$$\n-\\lambda_{1} x - \\lambda_{2} (x-\\mu)^{2} = -\\lambda_{2}\\left[x^{2} - 2\\mu x + \\mu^{2}\\right] - \\lambda_{1} x\n= -\\lambda_{2}\\left(x - m\\right)^{2} - \\lambda_{2}\\mu^{2} + \\lambda_{2} m^{2},\n$$\nwhere $m$ is chosen so that $2\\lambda_{2} m = 2\\lambda_{2}\\mu - \\lambda_{1}$, i.e.,\n$$\nm = \\mu - \\frac{\\lambda_{1}}{2\\lambda_{2}}.\n$$\nThus,\n$$\np(x) \\propto \\exp\\left(-\\lambda_{2} (x - m)^{2}\\right),\n$$\nwhich is a Gaussian with mean $m$. Enforcing the mean constraint $\\int x\\,p(x)\\,dx = \\mu$ implies $m = \\mu$, hence\n$$\n\\lambda_{1} = 0.\n$$\nWith $\\lambda_{1}=0$, the density becomes\n$$\np(x) = A\\,\\exp\\left(-\\lambda_{2} (x-\\mu)^{2}\\right).\n$$\nNormalization determines $A$:\n$$\n1 = \\int_{-\\infty}^{\\infty} p(x)\\,dx = A \\int_{-\\infty}^{\\infty} \\exp\\left(-\\lambda_{2} (x-\\mu)^{2}\\right)\\,dx = A\\,\\sqrt{\\frac{\\pi}{\\lambda_{2}}},\n\\quad\\Rightarrow\\quad\nA = \\sqrt{\\frac{\\lambda_{2}}{\\pi}}.\n$$\nThe variance constraint fixes $\\lambda_{2}$. Compute\n$$\n\\sigma^{2} = \\int_{-\\infty}^{\\infty} (x-\\mu)^{2} p(x)\\,dx\n= A \\int_{-\\infty}^{\\infty} (x-\\mu)^{2} \\exp\\left(-\\lambda_{2} (x-\\mu)^{2}\\right)\\,dx\n= \\sqrt{\\frac{\\lambda_{2}}{\\pi}} \\cdot \\frac{\\sqrt{\\pi}}{2 \\lambda_{2}^{3/2}} = \\frac{1}{2\\lambda_{2}}.\n$$\nTherefore,\n$$\n\\lambda_{2} = \\frac{1}{2\\sigma^{2}},\n\\quad\\text{and}\\quad\nA = \\sqrt{\\frac{1}{2\\sigma^{2}\\pi}} = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}.\n$$\nSubstituting these into $p(x)$ yields\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nSince the differential entropy functional is strictly concave in $p$ and the constraints are linear, this stationary solution is the unique global maximizer. Thus the maximally random distribution under the given constraints is the Gaussian with mean $\\mu$ and variance $\\sigma^{2}$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)}$$", "id": "1963846"}]}