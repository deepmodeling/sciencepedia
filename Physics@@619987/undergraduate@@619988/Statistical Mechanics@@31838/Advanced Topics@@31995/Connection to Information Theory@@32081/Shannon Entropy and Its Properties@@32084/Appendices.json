{"hands_on_practices": [{"introduction": "We begin with the most fundamental skill: calculating the Shannon entropy for a system with a discrete set of states. This practice provides a direct application of the Gibbs-Shannon entropy formula, $S = -k_B \\sum_i p_i \\ln(p_i)$, which is a cornerstone of statistical mechanics. By working through a concrete example of a molecular switch [@problem_id:1991849], you will solidify your understanding of how to translate a set of probabilities into a quantitative measure of a system's uncertainty or disorder.", "problem": "Consider a simplified model of a biological molecular switch, which can exist in one of four distinct conformational states at thermal equilibrium. Let the probabilities of the molecule being in State 1, State 2, State 3, and State 4 be $p_1, p_2, p_3,$ and $p_4$, respectively. The measured probabilities for these states are $p_1 = \\frac{1}{2}$, $p_2 = \\frac{1}{4}$, and $p_3 = p_4 = \\frac{1}{8}$.\n\nGiven the value of the Boltzmann constant, $k_B = 1.380649 \\times 10^{-23}$ J/K, and the natural logarithm of 2, $\\ln(2) \\approx 0.693147$, calculate the statistical entropy (or Gibbs entropy) of this system.\n\nExpress your final answer in units of Joules per Kelvin (J/K), rounded to four significant figures.", "solution": "The statistical (Gibbs) entropy for a discrete set of microstates with probabilities $\\{p_{i}\\}$ is given by\n$$\nS=-k_{B}\\sum_{i=1}^{4}p_{i}\\ln(p_{i}).\n$$\nWith $p_{1}=\\frac{1}{2}$, $p_{2}=\\frac{1}{4}$, and $p_{3}=p_{4}=\\frac{1}{8}$, compute each logarithm using $\\ln\\!\\left(\\frac{1}{2}\\right)=-\\ln(2)$, $\\ln\\!\\left(\\frac{1}{4}\\right)=-2\\ln(2)$, and $\\ln\\!\\left(\\frac{1}{8}\\right)=-3\\ln(2)$:\n$$\n\\begin{aligned}\nS&=-k_{B}\\left[\\frac{1}{2}\\ln\\!\\left(\\frac{1}{2}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{1}{4}\\right)+\\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)+\\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)\\right] \\\\\n&=-k_{B}\\left[\\frac{1}{2}(-\\ln 2)+\\frac{1}{4}(-2\\ln 2)+\\frac{1}{8}(-3\\ln 2)+\\frac{1}{8}(-3\\ln 2)\\right] \\\\\n&=k_{B}\\left[\\frac{1}{2}\\ln 2+\\frac{1}{2}\\ln 2+\\frac{3}{8}\\ln 2+\\frac{3}{8}\\ln 2\\right] \\\\\n&=k_{B}\\left(\\frac{7}{4}\\ln 2\\right).\n\\end{aligned}\n$$\nNow substitute the provided numerical values $k_{B}=1.380649\\times 10^{-23}$ J/K and $\\ln(2)\\approx 0.693147$:\n$$\nS=(1.380649\\times 10^{-23})\\left(\\frac{7}{4}\\right)(0.693147)\\ \\text{J/K}.\n$$\nCompute $\\left(\\frac{7}{4}\\right)(0.693147)=1.21300725$, hence\n$$\nS=(1.380649\\times 10^{-23})(1.21300725)=1.67473724670525\\times 10^{-23}\\ \\text{J/K}.\n$$\nRounded to four significant figures,\n$$\nS=1.675\\times 10^{-23}\\ \\text{J/K}.\n$$", "answer": "$$\\boxed{1.675 \\times 10^{-23}}$$", "id": "1991849"}, {"introduction": "Having practiced the direct calculation, let's now build our intuition about what entropy represents. Entropy is fundamentally a measure of uncertainty; a system with a more predictable outcome has lower entropy than one with a less predictable outcome. This exercise [@problem_id:1991837] challenges you to compare the entropies of several simple systems based on their probability distributions alone, without performing any complex calculations, thereby sharpening your conceptual grasp of the link between probability and entropy.", "problem": "An engineer is analyzing three competing designs for a simple quantum memory bit, Models A, B, and C. Each bit can be in one of two fundamental states, labeled '0' and '1'. Due to differences in their physical implementation, the probability of observing the bit in a particular state at thermal equilibrium differs for each model. The probability distributions $(p_0, p_1)$ for the states ('0', '1') for each model are as follows:\n\n*   **Model A:** $(0.70, 0.30)$\n*   **Model B:** $(0.90, 0.10)$\n*   **Model C:** $(0.55, 0.45)$\n\nThe Shannon entropy of a system is a measure of the uncertainty or unpredictability of its state. Let $S_A$, $S_B$, and $S_C$ be the Shannon entropies corresponding to Models A, B, and C, respectively. Based on the fundamental properties of entropy, determine the correct ordering of these entropies from lowest to highest. You should be able to determine the answer by reasoning about the nature of the probability distributions, without needing to perform a full numerical calculation of the entropy for each model.\n\nWhich of the following options represents the correct ordering?\n\nA. $S_A < S_B < S_C$\n\nB. $S_B < S_A < S_C$\n\nC. $S_C < S_A < S_B$\n\nD. $S_B < S_C < S_A$\n\nE. $S_C < S_B < S_A$", "solution": "For a binary distribution with probabilities $(p,1-p)$, the Shannon entropy (up to a constant factor given by the choice of logarithm base) can be written as\n$$\nS(p)=-p\\ln p-(1-p)\\ln(1-p).\n$$\nThis function is symmetric about $p=\\frac{1}{2}$ since $S(p)=S(1-p)$, and its derivative is\n$$\nS'(p)=\\frac{d}{dp}\\left[-p\\ln p-(1-p)\\ln(1-p)\\right]=\\ln\\left(\\frac{1-p}{p}\\right).\n$$\nHence $S'(p)=0$ at $p=\\frac{1}{2}$, and for $p>\\frac{1}{2}$ we have $\\frac{1-p}{p}<1$ so $S'(p)<0$, while for $p<\\frac{1}{2}$ we have $S'(p)>0$. Therefore, $S(p)$ is maximized at $p=\\frac{1}{2}$ and strictly decreases as $|p-\\frac{1}{2}|$ increases.\n\nThus, among the given models, the entropy is largest for the distribution closest to uniform and smallest for the one farthest from uniform. Comparing distances from $\\frac{1}{2}$:\n- Model C: $|0.55-\\frac{1}{2}|=0.05$ (closest to uniform) implies highest entropy.\n- Model A: $|0.70-\\frac{1}{2}|=0.20$ implies intermediate entropy.\n- Model B: $|0.90-\\frac{1}{2}|=0.40$ (farthest from uniform) implies lowest entropy.\n\nTherefore, the ordering from lowest to highest entropy is $S_{B}<S_{A}<S_{C}$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1991837"}, {"introduction": "Our final practice ventures into the realm of dynamic systems and statistical mechanics, exploring how entropy is defined for processes that evolve over time. Many physical systems, from polymer growth to sequences in DNA, can be modeled as Markov chains where the next state depends on the current one. This advanced problem [@problem_id:1991855] asks you to derive the \"entropy rate\"—the entropy per unit in an infinite chain—connecting the abstract concept of Shannon entropy to the tangible properties of complex, correlated systems.", "problem": "A new type of synthetic biomaterial, a \"memory polymer,\" is constructed by a self-assembly process where monomers are added sequentially to form a long one-dimensional chain. There are $M$ distinct types of monomers, which we can label with an index $j \\in \\{1, 2, \\dots, M\\}$.\n\nThe assembly process exhibits a nearest-neighbor memory effect. The probability that the next monomer to be added to the chain is of type $j$, given that the current last monomer is of type $k$, is given by the elements of a time-independent $M \\times M$ transition matrix $P$. Specifically, $P_{jk}$ denotes the conditional probability $P(\\text{next}=j|\\text{last}=k)$. This matrix is stochastic, meaning its columns sum to one: $\\sum_{j=1}^{M} P_{jk} = 1$ for all $k \\in \\{1, \\dots, M\\}$.\n\nFor a very long chain, the process reaches a steady state. In this state, the unconditional probability of finding a monomer of type $j$ at an arbitrary position is a constant, denoted by $\\pi_j$. These stationary probabilities form a vector $\\mathbf{\\pi} = (\\pi_1, \\pi_2, \\dots, \\pi_M)^T$ that satisfies the steady-state condition $\\pi_j = \\sum_{k=1}^{M} P_{jk} \\pi_k$ for each $j$, along with the normalization $\\sum_{j=1}^{M} \\pi_j = 1$. We assume that this stationary distribution is unique and all its components $\\pi_j$ are non-zero.\n\nThe total configurational Shannon entropy of a chain with $N$ monomers, specified by the sequence $\\sigma_N = (s_1, s_2, \\dots, s_N)$, is given by $H_N = -\\sum_{\\sigma_N} p(\\sigma_N) \\ln(p(\\sigma_N))$, where the sum is over all possible sequences of length $N$ and $p(\\sigma_N)$ is the probability of a given sequence.\n\nDetermine a general expression for the configurational entropy per monomer, $h = \\lim_{N\\to\\infty} \\frac{H_N}{N}$, for an infinitely long chain. Your final expression should be in terms of the transition probabilities $P_{jk}$ and the stationary probabilities $\\pi_k$.", "solution": "Let $\\{S_{n}\\}_{n\\ge 1}$ denote the monomer type at position $n$. The process is a time-homogeneous first-order Markov chain with transition probabilities $P_{jk}=\\Pr(S_{n}=j\\mid S_{n-1}=k)$ and a unique stationary distribution $\\pi=(\\pi_{1},\\dots,\\pi_{M})^{T}$ satisfying\n$$\n\\pi_{j}=\\sum_{k=1}^{M}P_{jk}\\pi_{k},\\quad \\sum_{j=1}^{M}\\pi_{j}=1,\\quad \\pi_{j}>0.\n$$\nThe configurational Shannon entropy of sequences of length $N$ is\n$$\nH_{N}=H(S_{1},\\dots,S_{N})=-\\sum_{\\sigma_{N}}p(\\sigma_{N})\\ln p(\\sigma_{N}).\n$$\nUsing the chain rule for entropy,\n$$\nH(S_{1},\\dots,S_{N})=\\sum_{n=1}^{N}H(S_{n}\\mid S_{1},\\dots,S_{n-1})=H(S_{1})+\\sum_{n=2}^{N}H(S_{n}\\mid S_{n-1}),\n$$\nwhere the Markov property $H(S_{n}\\mid S_{1},\\dots,S_{n-1})=H(S_{n}\\mid S_{n-1})$ for $n\\ge 2$ has been used. Under stationarity, $H(S_{n}\\mid S_{n-1})$ is independent of $n$, hence\n$$\nH_{N}=H(S_{1})+(N-1)H(S_{2}\\mid S_{1}).\n$$\nDividing by $N$ and taking the limit $N\\to\\infty$ yields the entropy rate per monomer\n$$\nh=\\lim_{N\\to\\infty}\\frac{H_{N}}{N}=H(S_{2}\\mid S_{1}),\n$$\nsince $\\lim_{N\\to\\infty}H(S_{1})/N=0$. The conditional entropy $H(S_{2}\\mid S_{1})$ is computed by averaging the conditional entropy given $S_{1}=k$ over the stationary distribution of $S_{1}$:\n$$\nH(S_{2}\\mid S_{1})=\\sum_{k=1}^{M}\\Pr(S_{1}=k)\\,H(S_{2}\\mid S_{1}=k)=\\sum_{k=1}^{M}\\pi_{k}\\left(-\\sum_{j=1}^{M}P_{jk}\\ln P_{jk}\\right).\n$$\nTherefore,\n$$\nh=-\\sum_{k=1}^{M}\\pi_{k}\\sum_{j=1}^{M}P_{jk}\\ln P_{jk},\n$$\nwith the standard convention that terms with $P_{jk}=0$ contribute $0$ because $\\lim_{x\\to 0^{+}}x\\ln x=0$.", "answer": "$$\\boxed{-\\sum_{k=1}^{M}\\pi_{k}\\sum_{j=1}^{M}P_{jk}\\ln P_{jk}}$$", "id": "1991855"}]}