{"hands_on_practices": [{"introduction": "The cornerstone of information theory is Shannon entropy, a powerful concept that quantifies the uncertainty or 'surprise' associated with a random variable. In the context of statistical mechanics, we can equate this uncertainty with our lack of knowledge about a system's specific microstate. This first exercise provides a direct, hands-on calculation of Shannon entropy for a simple, idealized physical system, building your intuition for how information content is fundamentally measured. [@problem_id:1956735]", "problem": "In the field of molecular computing, researchers are exploring the use of individual molecules to store information. Consider a hypothetical, specially synthesized molecule designed for this purpose. This molecule possesses exactly 10 distinct and stable quantum states, which can be used to encode data. After a standard preparation procedure, a measurement is performed on the molecule. It is found that the molecule has an equal probability of being in any one of these 10 available states. According to information theory, the uncertainty associated with the outcome of this measurement is quantified by the Shannon entropy. Calculate the Shannon entropy of this molecular system. Express your answer in bits, rounded to four significant figures.", "solution": "Shannon entropy for a discrete distribution $\\{p_{i}\\}$ measured in bits is defined as\n$$H=-\\sum_{i} p_{i}\\log_{2}(p_{i}).$$\nHere there are $10$ distinct states with equal probability, so $p_{i}=\\frac{1}{10}$ for $i=1,2,\\dots,10$. Substituting,\n$$H=-\\sum_{i=1}^{10} \\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)=-10\\cdot\\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)=-\\log_{2}\\!\\left(\\frac{1}{10}\\right)=\\log_{2}(10).$$\nUsing the change-of-base identity,\n$$\\log_{2}(10)=\\frac{\\ln(10)}{\\ln(2)}\\approx 3.32192809489\\ldots.$$\nRounding to four significant figures gives $3.322$ bits.", "answer": "$$\\boxed{3.322}$$", "id": "1956735"}, {"introduction": "Building on the static definition of entropy, we now investigate how it changes during a physical process. This is crucial for understanding the direction of spontaneous change in nature. This problem models one of the most fundamental processes in thermodynamics—the expansion of a gas—from an information-theoretic perspective. By analyzing a simplified particle-in-a-box model, you will see how increasing the number of accessible microstates directly leads to an increase in information entropy. [@problem_id:1956759]", "problem": "A theoretical model for a nanoscale memory device uses the position of a single particle to store information. The device consists of a one-dimensional array of discrete, equally-sized cells.\n\nInitially, the particle is confined within a region containing $N_1$ such cells. After a reconfiguration process, the barriers confining the particle are expanded, making a total of $N_2$ cells accessible to it. The expansion is characterized by a dimensionless factor $k > 1$, such that $N_2 = k N_1$.\n\nAssume that the particle has an equal probability of being in any of the available cells at any given time. The information entropy, $I$, of a system that can be in any of $\\Omega$ equally probable microstates is given by the formula $I = \\ln(\\Omega)$, where $\\ln$ denotes the natural logarithm.\n\nCalculate the change in the information entropy of the system, $\\Delta I = I_{final} - I_{initial}$, that occurs due to this expansion. Express your answer as a closed-form analytic expression in terms of the expansion factor $k$.", "solution": "The problem asks for the change in information entropy, $\\Delta I$, when the number of accessible cells for a particle changes from $N_1$ to $N_2$. We are given the formula for information entropy for a system with $\\Omega$ equally probable microstates as $I = \\ln(\\Omega)$.\n\nFirst, we determine the initial information entropy, $I_{initial}$. In the initial state, the particle is confined to a region with $N_1$ cells. Since the particle has an equal probability of being found in any of these cells, the number of possible microstates for the system is equal to the number of available cells.\nTherefore, the initial number of microstates is $\\Omega_{initial} = N_1$.\n\nUsing the given formula, the initial information entropy is:\n$$I_{initial} = \\ln(\\Omega_{initial}) = \\ln(N_1)$$\n\nNext, we determine the final information entropy, $I_{final}$. After the expansion, the particle can be found in any of the $N_2$ accessible cells. Again, assuming equal probability for each cell, the final number of microstates is equal to the new total number of cells.\nSo, the final number of microstates is $\\Omega_{final} = N_2$.\n\nThe problem states that $N_2 = k N_1$. Substituting this into our expression for the final number of microstates, we get:\n$$\\Omega_{final} = k N_1$$\n\nNow, we calculate the final information entropy using the given formula:\n$$I_{final} = \\ln(\\Omega_{final}) = \\ln(k N_1)$$\n\nFinally, we calculate the change in information entropy, $\\Delta I$, which is defined as the difference between the final and initial entropies:\n$$\\Delta I = I_{final} - I_{initial}$$\n\nSubstituting the expressions we found for $I_{final}$ and $I_{initial}$:\n$$\\Delta I = \\ln(k N_1) - \\ln(N_1)$$\n\nTo simplify this expression, we use the property of logarithms that states $\\ln(a) - \\ln(b) = \\ln\\left(\\frac{a}{b}\\right)$. Applying this property, we get:\n$$\\Delta I = \\ln\\left(\\frac{k N_1}{N_1}\\right)$$\n\nThe term $N_1$ cancels out in the numerator and the denominator:\n$$\\Delta I = \\ln(k)$$\n\nThus, the change in information entropy depends only on the dimensionless expansion factor $k$.", "answer": "$$\\boxed{\\ln(k)}$$", "id": "1956759"}, {"introduction": "Our final practice extends the analysis from a single particle to a large ensemble, tackling the classic thermodynamic phenomenon of mixing. We will use the concept of 'locational uncertainty' to quantify the information change when two distinguishable gases are allowed to intermingle. This exercise powerfully demonstrates how the principles of information theory apply to macroscopic systems and provides a clear, statistical explanation for the 'entropy of mixing,' a key concept in both physics and chemistry. [@problem_id:1956769]", "problem": "A system is composed of a rigid, insulated container divided into two chambers of equal volume, the \"Left\" and \"Right\" chambers, separated by a thin, removable partition. Initially, the Left chamber contains $N$ particles of a monatomic ideal gas of type 'A', and the Right chamber contains $N$ particles of a different monatomic ideal gas of type 'B'. Both chambers are at the same initial temperature $T$ and pressure $P$. Particles of type A are distinguishable from particles of type B.\n\nWe define a quantity called \"locational uncertainty\" for a single particle, which measures our lack of knowledge about which chamber it occupies. This uncertainty is defined using the natural logarithm as $H_{\\text{particle}} = -p_L \\ln(p_L) - p_R \\ln(p_R)$, where $p_L$ and $p_R$ are the probabilities of finding the particle in the Left and Right chambers, respectively. The total locational uncertainty for the system, $H_{\\text{total}}$, is the sum of the uncertainties for all individual particles.\n\nAt a certain moment, the partition is removed, allowing the two gases to mix until they reach a new equilibrium state. This mixing process occurs isothermally, with the final temperature of the combined system remaining at $T$.\n\nCalculate the total change in the locational uncertainty of the system, $\\Delta H_{\\text{total}} = H_{\\text{final}} - H_{\\text{initial}}$, after the mixing is complete. Express your answer as a symbolic expression in terms of the number of particles of one type, $N$.", "solution": "Initially, the partition fixes each particle’s chamber. For any specific particle of type A (all in the Left chamber initially), the probabilities are $p_{L}=1$ and $p_{R}=0$. The locational uncertainty for such a particle is\n$$\nH_{\\text{particle, init}}=-p_{L}\\ln(p_{L})-p_{R}\\ln(p_{R})=-1\\cdot\\ln(1)-0\\cdot\\ln(0).\n$$\nUsing $\\ln(1)=0$ and the limit $\\lim_{x\\to 0^{+}}x\\ln x=0$, this gives $H_{\\text{particle, init}}=0$. The same reasoning applies to any particle of type B (all in the Right chamber initially), yielding zero uncertainty as well. Summing over all $2N$ particles,\n$$\nH_{\\text{initial}}=\\sum_{i=1}^{2N}0=0.\n$$\n\nAfter the partition is removed and the system reaches equilibrium, each particle is uniformly distributed over the total volume (which is twice the volume of either initial chamber). Therefore, for any particle,\n$$\np_{L}=\\frac{V_{\\text{left}}}{V_{\\text{total}}}=\\frac{1}{2},\\qquad p_{R}=\\frac{1}{2}.\n$$\nThe locational uncertainty for any particle in this mixed state is\n$$\nH_{\\text{particle, final}}=-\\frac{1}{2}\\ln\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\ln\\!\\left(\\frac{1}{2}\\right)=-\\ln\\!\\left(\\frac{1}{2}\\right)=\\ln 2.\n$$\nSince there are $2N$ particles in total, the final total uncertainty is\n$$\nH_{\\text{final}}=(2N)\\ln 2.\n$$\n\nTherefore, the total change in locational uncertainty is\n$$\n\\Delta H_{\\text{total}}=H_{\\text{final}}-H_{\\text{initial}}=(2N)\\ln 2-0=2N\\ln 2.\n$$", "answer": "$$\\boxed{2N\\ln 2}$$", "id": "1956769"}]}