{"hands_on_practices": [{"introduction": "We begin our exploration of the Principle of Maximum Entropy with its most fundamental application in physics: determining the state probabilities for a system with discrete energy levels. This exercise provides the blueprint for connecting information theory to statistical mechanics. By maximizing the Gibbs entropy subject to a fixed average energy, you will derive the celebrated Gibbs distribution, demonstrating how this statistical law emerges from a principle of maximal uncertainty. [@problem_id:2006969]", "problem": "A physical system can be found in one of three distinct, non-degenerate energy states with energies given by $E_1 = 0$, $E_2 = \\epsilon$, and $E_3 = 2\\epsilon$, where $\\epsilon$ is a positive energy constant. The ensemble average energy of the system is constrained to be $\\langle E \\rangle = \\frac{4}{5}\\epsilon$. The probability distribution $\\{p_1, p_2, p_3\\}$ that describes the system is the one that maximizes the Gibbs entropy, subject to the given constraints. Using this principle of maximum entropy, determine the exact analytical expression for the probability, $p_1$, of finding the system in the ground state (energy $E_1=0$).", "solution": "We maximize the Gibbs entropy $S=-k_{B}\\sum_{i=1}^{3} p_{i}\\ln p_{i}$ subject to the constraints $\\sum_{i=1}^{3} p_{i}=1$ and $\\sum_{i=1}^{3} p_{i}E_{i}=\\frac{4}{5}\\epsilon$, with $E_{1}=0$, $E_{2}=\\epsilon$, $E_{3}=2\\epsilon$. Introduce Lagrange multipliers $\\alpha$ and $\\gamma$ and consider\n$$\n\\mathcal{L}=-k_{B}\\sum_{i=1}^{3} p_{i}\\ln p_{i}-\\alpha\\left(\\sum_{i=1}^{3} p_{i}-1\\right)-\\gamma\\left(\\sum_{i=1}^{3} p_{i}E_{i}-\\frac{4}{5}\\epsilon\\right).\n$$\nStationarity with respect to $p_{i}$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{i}}=-k_{B}\\left(\\ln p_{i}+1\\right)-\\alpha-\\gamma E_{i}=0 \\quad \\Rightarrow \\quad \\ln p_{i}=-1-\\frac{\\alpha}{k_{B}}-\\frac{\\gamma}{k_{B}}E_{i}.\n$$\nHence $p_{i}=A\\,\\exp(-\\beta E_{i})$ with $A=\\exp\\!\\left(-1-\\frac{\\alpha}{k_{B}}\\right)$ and $\\beta=\\frac{\\gamma}{k_{B}}$. Normalization fixes $A$ via the partition function $Z=\\sum_{i=1}^{3}\\exp(-\\beta E_{i})$, so\n$$\np_{i}=\\frac{\\exp(-\\beta E_{i})}{Z}, \\quad Z=1+\\exp(-\\beta \\epsilon)+\\exp(-2\\beta \\epsilon).\n$$\nDefine $x=\\exp(-\\beta \\epsilon)>0$. Then\n$$\np_{1}=\\frac{1}{1+x+x^{2}}, \\quad p_{2}=\\frac{x}{1+x+x^{2}}, \\quad p_{3}=\\frac{x^{2}}{1+x+x^{2}}.\n$$\nThe energy constraint $\\langle E\\rangle=\\frac{4}{5}\\epsilon$ becomes\n$$\n\\frac{\\epsilon x+2\\epsilon x^{2}}{1+x+x^{2}}=\\frac{4}{5}\\epsilon \\quad \\Rightarrow \\quad \\frac{x+2x^{2}}{1+x+x^{2}}=\\frac{4}{5}.\n$$\nClearing denominators yields\n$$\n5(x+2x^{2})=4(1+x+x^{2}) \\quad \\Rightarrow \\quad 6x^{2}+x-4=0.\n$$\nSolving the quadratic equation gives\n$$\nx=\\frac{-1\\pm\\sqrt{1+96}}{12}=\\frac{-1\\pm\\sqrt{97}}{12}.\n$$\nSince $x>0$, we select $x=\\frac{-1+\\sqrt{97}}{12}$. Therefore\n$$\np_{1}=\\frac{1}{1+x+x^{2}}=\\frac{1}{1+\\frac{\\sqrt{97}-1}{12}+\\left(\\frac{\\sqrt{97}-1}{12}\\right)^{2}}.\n$$\nCompute the denominator explicitly:\n$$\n1+\\frac{\\sqrt{97}-1}{12}+\\left(\\frac{\\sqrt{97}-1}{12}\\right)^{2}\n=1+\\frac{\\sqrt{97}-1}{12}+\\frac{97-2\\sqrt{97}+1}{144}\n=\\frac{115+5\\sqrt{97}}{72}.\n$$\nThus\n$$\np_{1}=\\frac{72}{115+5\\sqrt{97}}.\n$$\nThis is the exact analytical expression for the ground-state probability under the maximum-entropy distribution with the given mean energy.", "answer": "$$\\boxed{\\frac{72}{115+5\\sqrt{97}}}$$", "id": "2006969"}, {"introduction": "Moving from discrete states to continuous variables, this problem extends the maximum entropy principle to a more general and widely applicable scenario. You will be given constraints on the second moments (variance and covariance) of two variables and tasked with finding their joint probability distribution. This practice is crucial as it reveals why the multivariate Gaussian distribution holds a special place in science; it is the most unbiased statistical model when only the mean and covariance structure are known. [@problem_id:2006959]", "problem": "A stationary stochastic process $X(t)$ is a sequence of random variables where the statistical properties do not change over time. Consider two data points, $x_1$ and $x_2$, sampled from such a process. Due to stationarity, we can assume the process has zero mean, such that $\\langle x_1 \\rangle = \\langle x_2 \\rangle = 0$. From empirical measurements, we have determined the following properties: the variance of each data point is $\\langle x_1^2 \\rangle = \\langle x_2^2 \\rangle = \\sigma^2$, and the covariance between them is $\\langle x_1 x_2 \\rangle = C$. The constants $\\sigma^2$ and $C$ are known, with $\\sigma^2 > 0$ and $\\sigma^4 > C^2$.\n\nUsing the principle of maximum entropy, determine the joint probability density function $p(x_1, x_2)$ that is most consistent with this limited information. The principle of maximum entropy states that, subject to a set of constraints, the most unbiased probability distribution is the one that maximizes the information entropy.\n\nExpress your answer as a single, closed-form analytic expression in terms of $x_1, x_2, \\sigma$, and $C$.", "solution": "We are given two real-valued random variables $x_{1}$ and $x_{2}$ sampled from a stationary stochastic process with constraints\n$$\\langle x_{1} \\rangle = 0, \\quad \\langle x_{2} \\rangle = 0, \\quad \\langle x_{1}^{2} \\rangle = \\sigma^{2}, \\quad \\langle x_{2}^{2} \\rangle = \\sigma^{2}, \\quad \\langle x_{1} x_{2} \\rangle = C,$$\nwith $\\sigma^{2} > 0$ and $\\sigma^{4} > C^{2}$.\n\nBy the principle of maximum entropy for continuous variables, the density that maximizes the differential entropy\n$$H[p] = - \\int_{\\mathbb{R}^{2}} p(x_{1},x_{2}) \\ln p(x_{1},x_{2}) \\, dx_{1} dx_{2}$$\nsubject to the constraints of normalization and the above moment constraints has the exponential family form obtained via Lagrange multipliers. Introducing multipliers for normalization, means, variances, and covariance, the variational problem yields an extremal density of the form\n$$p(x_{1},x_{2}) = Z^{-1} \\exp\\!\\left( - a x_{1}^{2} - b x_{2}^{2} - 2 d x_{1} x_{2} - u x_{1} - v x_{2} \\right),$$\nwhere $a$, $b$, $d$, $u$, and $v$ are constants determined by the constraints, and $Z$ is the normalization constant. Writing this in matrix form as\n$$p(\\mathbf{x}) = Z^{-1} \\exp\\!\\left( - \\mathbf{x}^{T} K \\mathbf{x} - \\mathbf{h}^{T} \\mathbf{x} \\right), \\quad \\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}, \\quad K = \\begin{pmatrix} a & d \\\\ d & b \\end{pmatrix}, \\quad \\mathbf{h} = \\begin{pmatrix} u \\\\ v \\end{pmatrix},$$\nwe complete the square:\n$$- \\mathbf{x}^{T} K \\mathbf{x} - \\mathbf{h}^{T} \\mathbf{x} = - (\\mathbf{x} - \\boldsymbol{\\mu})^{T} K (\\mathbf{x} - \\boldsymbol{\\mu}) + \\boldsymbol{\\mu}^{T} K \\boldsymbol{\\mu}, \\quad \\boldsymbol{\\mu} = - \\tfrac{1}{2} K^{-1} \\mathbf{h}.$$\nHence the mean is $\\langle \\mathbf{x} \\rangle = \\boldsymbol{\\mu}$. Enforcing $\\langle x_{1} \\rangle = 0$ and $\\langle x_{2} \\rangle = 0$ requires $\\boldsymbol{\\mu} = \\mathbf{0}$, which implies $\\mathbf{h} = \\mathbf{0}$. Therefore the maximum entropy density simplifies to a centered Gaussian,\n$$p(\\mathbf{x}) = Z^{-1} \\exp\\!\\left( - \\mathbf{x}^{T} K \\mathbf{x} \\right).$$\nFor such a density, the covariance matrix is related to $K$ by\n$$\\langle \\mathbf{x} \\mathbf{x}^{T} \\rangle = \\tfrac{1}{2} K^{-1},$$\nso to match the prescribed covariance matrix $\\Sigma$ we must set $K = \\tfrac{1}{2} \\Sigma^{-1}$. It is then standard to write the centered multivariate normal in the canonical form\n$$p(\\mathbf{x}) = \\frac{1}{(2 \\pi)^{n/2} \\sqrt{\\det \\Sigma}} \\exp\\!\\left( - \\tfrac{1}{2} \\mathbf{x}^{T} \\Sigma^{-1} \\mathbf{x} \\right), \\quad n=2.$$\nWith the given second moments, the covariance matrix is\n$$\\Sigma = \\begin{pmatrix} \\sigma^{2} & C \\\\ C & \\sigma^{2} \\end{pmatrix}, \\quad \\det \\Sigma = \\sigma^{4} - C^{2} > 0,$$\nwhich is positive definite by the assumption $\\sigma^{4} > C^{2}$. The inverse is\n$$\\Sigma^{-1} = \\frac{1}{\\sigma^{4} - C^{2}} \\begin{pmatrix} \\sigma^{2} & -C \\\\ -C & \\sigma^{2} \\end{pmatrix}.$$\nThus the maximum-entropy joint density is the zero-mean bivariate normal\n$$p(x_{1},x_{2}) = \\frac{1}{2 \\pi \\sqrt{\\sigma^{4} - C^{2}}} \\exp\\!\\left( - \\frac{1}{2(\\sigma^{4} - C^{2})} \\left[ \\sigma^{2} \\left( x_{1}^{2} + x_{2}^{2} \\right) - 2 C x_{1} x_{2} \\right] \\right),$$\nwhich uniquely satisfies the given constraints and maximizes the entropy subject to them.", "answer": "$$\\boxed{\\frac{1}{2 \\pi \\sqrt{\\sigma^{4} - C^{2}}}\\,\\exp\\!\\left(-\\frac{\\sigma^{2}\\!\\left(x_{1}^{2}+x_{2}^{2}\\right)-2 C x_{1} x_{2}}{2\\left(\\sigma^{4}-C^{2}\\right)}\\right)}$$", "id": "2006959"}, {"introduction": "This final problem showcases the profound generality of the maximum entropy principle by applying it to the abstract realm of random matrix theory. Instead of particles, we consider an ensemble of matrices and seek the probability distribution of their eigenvalues, given constraints on the traces. This exercise bridges statistical mechanics with advanced topics in theoretical physics, showing how the same inferential framework can be used to model complex quantum systems where only macroscopic averages are observable. [@problem_id:2006940]", "problem": "Consider an ensemble of $2 \\times 2$ real symmetric matrices, $M$. Such matrices are often used in physics to model simplified Hamiltonians of complex quantum systems where the detailed interactions are unknown. The statistical properties of the ensemble can be described by the joint probability distribution of the eigenvalues, $p(\\lambda_1, \\lambda_2)$, of these matrices. For a real symmetric matrix, its eigenvalues are always real.\n\nSuppose that the only information known about this ensemble is that the average of the trace of the matrices is a constant $c_1$, and the average of the trace of the square of the matrices is another constant $c_2$. Specifically, these constraints are:\n1. $\\langle \\text{Tr}(M) \\rangle = c_1$\n2. $\\langle \\text{Tr}(M^2) \\rangle = c_2$\n\nAssume the eigenvalues $\\lambda_1$ and $\\lambda_2$ can take any real value. Invoke the principle of maximum entropy to determine the most unbiased joint probability distribution $p(\\lambda_1, \\lambda_2)$ consistent with these constraints. The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge is the one with the largest entropy, subject to the known constraints. Find the analytical expression for $p(\\lambda_1, \\lambda_2)$ in terms of $\\lambda_1$, $\\lambda_2$, $c_1$, and $c_2$. It is given that the constants $c_1$ and $c_2$ satisfy the condition $2c_2 > c_1^2$.", "solution": "We maximize the differential entropy of the joint density $p(\\lambda_{1},\\lambda_{2})$ on $\\mathbb{R}^{2}$ subject to the constraints\n$$\n\\int_{\\mathbb{R}^{2}} p(\\lambda_{1},\\lambda_{2})\\,d\\lambda_{1}\\,d\\lambda_{2}=1,\\quad \\int_{\\mathbb{R}^{2}}(\\lambda_{1}+\\lambda_{2})\\,p\\,d\\lambda_{1}\\,d\\lambda_{2}=c_{1},\\quad \\int_{\\mathbb{R}^{2}}(\\lambda_{1}^{2}+\\lambda_{2}^{2})\\,p\\,d\\lambda_{1}\\,d\\lambda_{2}=c_{2}.\n$$\nThe entropy is $S[p]=-\\int p\\ln p$. Introducing Lagrange multipliers $\\alpha$, $\\mu$, and $\\nu$ for the three constraints, we set up the variational functional\n$$\n\\mathcal{L}[p]=-\\int p\\ln p\\,d\\lambda_{1}\\,d\\lambda_{2}-\\alpha\\left(\\int p-1\\right)-\\mu\\left(\\int(\\lambda_{1}+\\lambda_{2})\\,p-c_{1}\\right)-\\nu\\left(\\int(\\lambda_{1}^{2}+\\lambda_{2}^{2})\\,p-c_{2}\\right).\n$$\nStationarity under variations $\\delta p$ yields\n$$\n-\\left(\\ln p+1\\right)-\\alpha-\\mu(\\lambda_{1}+\\lambda_{2})-\\nu(\\lambda_{1}^{2}+\\lambda_{2}^{2})=0,\n$$\nso the maximizing density has the exponential-family form\n$$\np(\\lambda_{1},\\lambda_{2})=\\frac{1}{Z(\\mu,\\nu)}\\exp\\!\\left(-\\mu(\\lambda_{1}+\\lambda_{2})-\\nu(\\lambda_{1}^{2}+\\lambda_{2}^{2})\\right),\n$$\nwith partition function\n$$\nZ(\\mu,\\nu)=\\int_{\\mathbb{R}^{2}}\\exp\\!\\left(-\\mu(\\lambda_{1}+\\lambda_{2})-\\nu(\\lambda_{1}^{2}+\\lambda_{2}^{2})\\right)\\,d\\lambda_{1}\\,d\\lambda_{2}.\n$$\nThe integral factorizes, and completing the square gives, for $\\nu>0$,\n$$\n\\int_{-\\infty}^{\\infty}\\exp\\!\\left(-\\nu \\lambda^{2}-\\mu \\lambda\\right)\\,d\\lambda=\\sqrt{\\frac{\\pi}{\\nu}}\\exp\\!\\left(\\frac{\\mu^{2}}{4\\nu}\\right),\n$$\nhence\n$$\nZ(\\mu,\\nu)=\\left(\\sqrt{\\frac{\\pi}{\\nu}}\\exp\\!\\left(\\frac{\\mu^{2}}{4\\nu}\\right)\\right)^{2}=\\frac{\\pi}{\\nu}\\exp\\!\\left(\\frac{\\mu^{2}}{2\\nu}\\right).\n$$\nEquivalently, the density factorizes into identical one-dimensional Gaussians:\n$$\np(\\lambda_{1},\\lambda_{2})=\\left(\\sqrt{\\frac{\\nu}{\\pi}}\\exp\\!\\left(-\\nu \\lambda_{1}^{2}-\\mu \\lambda_{1}\\right)\\right)\\left(\\sqrt{\\frac{\\nu}{\\pi}}\\exp\\!\\left(-\\nu \\lambda_{2}^{2}-\\mu \\lambda_{2}\\right)\\right).\n$$\nThe constraints determine $\\mu$ and $\\nu$. Using the standard relations $E[\\lambda_{1}+\\lambda_{2}]=-\\,\\partial_{\\mu}\\ln Z$ and $E[\\lambda_{1}^{2}+\\lambda_{2}^{2}]=-\\,\\partial_{\\nu}\\ln Z$, together with\n$$\n\\ln Z(\\mu,\\nu)=\\ln \\pi-\\ln \\nu+\\frac{\\mu^{2}}{2\\nu},\n$$\nwe obtain\n$$\nE[\\lambda_{1}+\\lambda_{2}]=-\\,\\frac{\\partial \\ln Z}{\\partial \\mu}=-\\frac{\\mu}{\\nu}=c_{1}\\quad\\Rightarrow\\quad \\mu=-\\nu\\,c_{1},\n$$\nand\n$$\nE[\\lambda_{1}^{2}+\\lambda_{2}^{2}]=-\\,\\frac{\\partial \\ln Z}{\\partial \\nu}=\\frac{1}{\\nu}+\\frac{\\mu^{2}}{2\\nu^{2}}=c_{2}.\n$$\nSubstituting $\\mu=-\\nu c_{1}$ into the second equation gives\n$$\nc_{2}=\\frac{1}{\\nu}+\\frac{c_{1}^{2}}{2}\\quad\\Rightarrow\\quad \\nu=\\frac{1}{c_{2}-\\frac{c_{1}^{2}}{2}}.\n$$\nThe given condition $2c_{2}>c_{1}^{2}$ ensures $\\nu>0$ and hence normalizability. Inserting $\\mu=-\\nu c_{1}$ into the exponent and completing the square yields the more transparent form\n$$\np(\\lambda_{1},\\lambda_{2})=\\frac{\\nu}{\\pi}\\exp\\!\\left(-\\nu\\left[(\\lambda_{1}-\\tfrac{c_{1}}{2})^{2}+(\\lambda_{2}-\\tfrac{c_{1}}{2})^{2}\\right]\\right),\n$$\nand substituting $\\nu=\\left(c_{2}-\\frac{c_{1}^{2}}{2}\\right)^{-1}$ finally gives\n$$\np(\\lambda_{1},\\lambda_{2})=\\frac{1}{\\pi\\left(c_{2}-\\frac{c_{1}^{2}}{2}\\right)}\\exp\\!\\left(-\\frac{\\left(\\lambda_{1}-\\frac{c_{1}}{2}\\right)^{2}+\\left(\\lambda_{2}-\\frac{c_{1}}{2}\\right)^{2}}{c_{2}-\\frac{c_{1}^{2}}{2}}\\right).\n$$\nThis is the maximum-entropy joint density on $\\mathbb{R}^{2}$ consistent with the given constraints, symmetric under exchange of $\\lambda_{1}$ and $\\lambda_{2}$, with independent and identically distributed Gaussian marginals of mean $\\frac{c_{1}}{2}$ and variance $\\frac{1}{2}\\left(c_{2}-\\frac{c_{1}^{2}}{2}\\right)$.", "answer": "$$\\boxed{\\frac{1}{\\pi\\left(c_{2}-\\frac{c_{1}^{2}}{2}\\right)}\\,\\exp\\!\\left(-\\frac{\\left(\\lambda_{1}-\\frac{c_{1}}{2}\\right)^{2}+\\left(\\lambda_{2}-\\frac{c_{1}}{2}\\right)^{2}}{c_{2}-\\frac{c_{1}^{2}}{2}}\\right)}$$", "id": "2006940"}]}