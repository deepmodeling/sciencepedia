{"hands_on_practices": [{"introduction": "The cornerstone of statistical mechanics is the profound connection between the macroscopic properties of a system, like entropy, and its microscopic details. This first exercise grounds the abstract concept of entropy in the tangible world of quantum mechanics. By counting the number of ways identical particles can be arranged in energy levels to achieve a specific total energy, you will directly calculate the system's statistical entropy using Boltzmann's celebrated formula, $S = k_B \\ln \\Omega$. This practice is essential for building an intuition for what entropy physically represents: a measure of the microscopic multiplicity corresponding to a single macroscopic state [@problem_id:1963626].", "problem": "A simplified model for a trapped quantum system consists of four distinct, non-degenerate single-particle energy levels: $E_1 = \\epsilon$, $E_2 = 2\\epsilon$, $E_3 = 3\\epsilon$, and $E_4 = 4\\epsilon$, where $\\epsilon$ is a positive energy constant. Three identical, non-interacting bosons are placed in this system. The only information available about the state of the system is that its total energy is exactly $E_{total} = 6\\epsilon$. Based on this information, determine the statistical entropy of the system. Express your answer as an analytical expression in terms of the Boltzmann constant, $k_B$.", "solution": "For three identical, non-interacting bosons in non-degenerate single-particle levels, microstates are uniquely specified by occupation numbers. Let $n_{i}$ be the number of bosons in level $E_{i}=i\\epsilon$ for $i=1,2,3,4$. The constraints are\n$$\nn_{1}+n_{2}+n_{3}+n_{4}=3,\\qquad \\epsilon\\left(1\\cdot n_{1}+2\\cdot n_{2}+3\\cdot n_{3}+4\\cdot n_{4}\\right)=6\\epsilon.\n$$\nDividing the energy constraint by $\\epsilon$ and subtracting the particle number constraint gives\n$$\nn_{2}+2n_{3}+3n_{4}=3,\n$$\nwith $n_{i}\\in\\{0,1,2,\\dots\\}$. Enumerating solutions:\n\n- If $n_{4}=0$, then $n_{2}+2n_{3}=3$, which yields $(n_{2},n_{3})=(3,0)$ and $(1,1)$. The corresponding $(n_{1},n_{2},n_{3},n_{4})$ are $(0,3,0,0)$ and $(1,1,1,0)$.\n- If $n_{4}=1$, then $n_{2}+2n_{3}=0$, giving $(n_{2},n_{3})=(0,0)$ and hence $(n_{1},n_{2},n_{3},n_{4})=(2,0,0,1)$.\n\nNo other $n_{4}$ are allowed since $3n_{4}\\leq 3$. Therefore, the number of distinct bosonic Fock states (microstates) consistent with the total energy is\n$$\n\\Omega=3.\n$$\nIn the microcanonical ensemble, the statistical entropy is\n$$\nS=k_{B}\\ln\\Omega=k_{B}\\ln 3.\n$$", "answer": "$$\\boxed{k_{B}\\ln 3}$$", "id": "1963626"}, {"introduction": "Having established that entropy quantifies the number of possible microstates, we can now explore how entropy changes as we gain information. This problem uses a compelling analogy from cryptography to illustrate that information is not an abstract notion, but a physical quantity that corresponds to a reduction in uncertainty. You will calculate the \"information gain\" upon discovering a piece of a secret message, seeing firsthand how learning more about a system decreases its entropy. This exercise reinforces the idea of entropy as \"missing information\" and provides a quantitative framework for understanding the value of data [@problem_id:1963591].", "problem": "In cryptography, the uncertainty associated with a secret message can be quantified by information entropy. Imagine a scenario where a passphrase is known to be a 20-character string formed by an anagram of the letters in \"STATISTICALMECHANICS\" (ignoring any spaces). All possible distinct anagrams are considered equally likely to be the correct passphrase.\n\nThe information entropy $S$ for a system with $W$ equally probable configurations (or microstates) is given by the formula $S = \\ln(W)$, where $\\ln$ is the natural logarithm. The unit of entropy in this context is the \"nat\".\n\nSuppose an intelligence analyst makes a breakthrough and discovers that the first three characters of the passphrase are \"SSS\". Calculate the information gain resulting from this discovery. The information gain is defined as the reduction in the information entropy of the set of possible passphrases.\n\nExpress your answer as a single closed-form analytic expression in terms of the natural logarithm.", "solution": "The multiset of letters in the string \"STATISTICALMECHANICS\" (ignoring spaces) has length 20 with frequencies:\n- $S,T,A,I,C$ each appearing $3$ times,\n- $L,M,E,H,N$ each appearing $1$ time.\n\nThus the total number of distinct anagrams is\n$$\nW_{0}=\\frac{20!}{(3!)^{5}}.\n$$\nFor equally likely configurations, the information entropy is $S=\\ln(W)$, so the initial entropy is\n$$\nS_{0}=\\ln\\!\\left(\\frac{20!}{(3!)^{5}}\\right).\n$$\n\nGiven the discovery that the first three characters are $SSS$, all $3$ occurrences of $S$ are fixed in the first three positions. The remaining $17$ positions must arrange the multiset with counts $T^{3},A^{3},I^{3},C^{3}$ and $L,M,E,H,N$ each once. The number of compatible anagrams is\n$$\nW_{1}=\\frac{17!}{(3!)^{4}},\n$$\nso the posterior entropy is\n$$\nS_{1}=\\ln\\!\\left(\\frac{17!}{(3!)^{4}}\\right).\n$$\n\nThe information gain is the reduction in entropy:\n$$\n\\Delta S=S_{0}-S_{1}\n=\\ln\\!\\left(\\frac{20!}{(3!)^{5}}\\right)-\\ln\\!\\left(\\frac{17!}{(3!)^{4}}\\right)\n=\\ln\\!\\left(\\frac{20!}{3!\\,17!}\\right)\n=\\ln\\!\\left(\\binom{20}{3}\\right).\n$$", "answer": "$$\\boxed{\\ln\\!\\left(\\binom{20}{3}\\right)}$$", "id": "1963591"}, {"introduction": "Our previous examples assumed that all accessible microstates are equally probable. However, in many real-world physical and biological systems, certain configurations are more likely than others. This final practice introduces the more general Gibbs entropy formula, $S = -k_B \\sum_i p_i \\ln p_i$, which accounts for non-uniform probabilities. By modeling the growth of a polymer chain as a step-by-step stochastic process, you will first determine the probability of each possible sequence and then compute the entropy of the entire ensemble. This advanced problem bridges the gap between simple counting and the probabilistic reasoning that underpins modern statistical physics, from materials science to molecular biology [@problem_id:1963635].", "problem": "A biopolymer chain is constructed monomer by monomer from a solution containing two types of monomers, A and B. The assembly process follows a set of simple, local rules. If the last monomer added to the chain was type A, the next monomer must be of type B. If the last monomer added was type B, the next monomer is chosen to be type A or type B with equal probability.\n\nA specific chain of length 5 is synthesized in a reaction vessel. It is known that the first monomer in this chain is of type B. Given this starting condition and the probabilistic rules of assembly, a specific set of possible 5-monomer sequences can be generated, each with a certain probability.\n\nLet the Boltzmann constant be denoted by $k_B$. Calculate the statistical entropy (also known as the Gibbs entropy) associated with the ensemble of all possible 5-monomer sequences that can be formed under these conditions.\n\nExpress your answer as a single closed-form analytic expression in terms of $k_B$ and mathematical constants.", "solution": "We model the assembly as a Markov process on monomer types with the stated rules: from $A$ the next monomer is deterministically $B$, and from $B$ the next monomer is $A$ or $B$ with probability $\\frac{1}{2}$ each. The Gibbs entropy of the ensemble of possible sequences is $S=-k_{B}\\sum_{i} p_{i}\\ln p_{i}$, where $p_{i}$ are the sequence probabilities.\n\nStarting with $s_{1}=B$, we enumerate all length-$5$ sequences and their probabilities by multiplying factors of $\\frac{1}{2}$ for each step that follows a $B$ (since $B\\to A$ and $B\\to B$ each have probability $\\frac{1}{2}$) and a factor of $1$ for each step that follows an $A$ (since $A\\to B$ is deterministic):\n- BABAB with probability $\\frac{1}{4}$,\n- BABBA with probability $\\frac{1}{8}$,\n- BABBB with probability $\\frac{1}{8}$,\n- BBABA with probability $\\frac{1}{8}$,\n- BBABB with probability $\\frac{1}{8}$,\n- BBBAB with probability $\\frac{1}{8}$,\n- BBBBA with probability $\\frac{1}{16}$,\n- BBBBB with probability $\\frac{1}{16}$.\n\nThese probabilities sum to $1$, as required. The Gibbs entropy is then\n$$\nS=-k_{B}\\left[\\frac{1}{4}\\ln\\!\\left(\\frac{1}{4}\\right)+5\\cdot\\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)+2\\cdot\\frac{1}{16}\\ln\\!\\left(\\frac{1}{16}\\right)\\right].\n$$\nUsing $\\ln(1/4)=-\\ln 4$, $\\ln(1/8)=-\\ln 8$, and $\\ln(1/16)=-\\ln 16$, this becomes\n$$\nS=k_{B}\\left[\\frac{1}{4}\\ln 4+\\frac{5}{8}\\ln 8+\\frac{1}{8}\\ln 16\\right]\n= k_{B}\\left[\\frac{1}{4}\\cdot 2\\ln 2+\\frac{5}{8}\\cdot 3\\ln 2+\\frac{1}{8}\\cdot 4\\ln 2\\right]\n= k_{B}\\left[\\frac{1}{2}+\\frac{15}{8}+\\frac{1}{2}\\right]\\ln 2\n= \\frac{23}{8}\\,k_{B}\\ln 2.\n$$\n\nAs a consistency check, the entropy equals $k_{B}\\ln 2$ times the expected number of branching steps (times the previous monomer is $B$) from positions $1$ through $4$. With $p_{1}=1$ and $p_{t}=1-\\frac{1}{2}p_{t-1}$, we obtain $p_{1}=1$, $p_{2}=\\frac{1}{2}$, $p_{3}=\\frac{3}{4}$, $p_{4}=\\frac{5}{8}$, whose sum is $\\frac{23}{8}$, yielding the same $S=\\frac{23}{8}k_{B}\\ln 2$.", "answer": "$$\\boxed{\\frac{23}{8}\\,k_{B}\\ln 2}$$", "id": "1963635"}]}