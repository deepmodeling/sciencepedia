{"hands_on_practices": [{"introduction": "The Law of Large Numbers provides a powerful bridge between probability and deterministic values. This practice demonstrates one of the most elegant examples of this principle: estimating the value of $\\pi$ using a simple Monte Carlo simulation. By modeling the random 'throwing of darts' at a square target, we can see how the proportion of 'hits' within an inscribed circle converges to a value directly related to $\\pi$, offering a tangible understanding of convergence in probability [@problem_id:1406798].", "problem": "A computer simulation generates a sequence of independent random points, $P_1, P_2, P_3, \\dots$, in a two-dimensional Cartesian plane. Each point $P_i$ has coordinates $(X_i, Y_i)$, where $X_i$ and $Y_i$ are independent random variables drawn from a uniform distribution on the interval $[0, 1]$. We define a region $\\mathcal{C}$ to be the set of all points $(x, y)$ that satisfy the inequality $(x - 0.5)^2 + (y - 0.5)^2 \\le 0.25$.\n\nLet $S_n$ be the number of points among the first $n$ points ($P_1, \\dots, P_n$) that fall inside or on the boundary of the region $\\mathcal{C}$. Consider the random variable $A_n = 4 \\frac{S_n}{n}$.\n\nDetermine the value to which the sequence of random variables $A_n$ converges almost surely as $n \\to \\infty$. Express your answer as a single closed-form analytic expression.", "solution": "Let $I_{i}$ be the indicator of the event that $P_{i}$ falls in $\\mathcal{C}$, i.e., $I_{i}=1$ if $(X_{i},Y_{i})\\in\\mathcal{C}$ and $I_{i}=0$ otherwise. Then $S_{n}=\\sum_{i=1}^{n} I_{i}$ and $A_{n}=4\\frac{S_{n}}{n}=4\\left(\\frac{1}{n}\\sum_{i=1}^{n} I_{i}\\right)$.\n\nBecause $(X_{i},Y_{i})$ are i.i.d. uniformly distributed on $[0,1]^{2}$, the $I_{i}$ are i.i.d. Bernoulli with parameter $p=\\mathbb{P}\\big((X_{1},Y_{1})\\in\\mathcal{C}\\big)$. Since $\\mathcal{C}$ is the disk centered at $(\\tfrac{1}{2},\\tfrac{1}{2})$ of radius $r$ determined by $(x-\\tfrac{1}{2})^{2}+(y-\\tfrac{1}{2})^{2}\\le \\tfrac{1}{4}$, we have $r^{2}=\\tfrac{1}{4}$ and hence $r=\\tfrac{1}{2}$. This disk lies entirely within the unit square $[0,1]^{2}$, so\n$$\np=\\frac{\\text{area}(\\mathcal{C})}{\\text{area}([0,1]^{2})}=\\pi r^{2}=\\pi\\left(\\frac{1}{2}\\right)^{2}=\\frac{\\pi}{4}.\n$$\n\nBy the strong law of large numbers, since the $I_{i}$ are i.i.d. with finite mean, we have\n$$\n\\frac{S_{n}}{n}=\\frac{1}{n}\\sum_{i=1}^{n} I_{i}\\to \\mathbb{E}[I_{1}]=p=\\frac{\\pi}{4}\\quad\\text{almost surely}.\n$$\nMultiplying by $4$, it follows that\n$$\nA_{n}=4\\frac{S_{n}}{n}\\to 4\\cdot\\frac{\\pi}{4}=\\pi\\quad\\text{almost surely}.\n$$\nTherefore, the sequence $A_{n}$ converges almost surely to $\\pi$ as $n\\to\\infty$.", "answer": "$$\\boxed{\\pi}$$", "id": "1406798"}, {"introduction": "A deep understanding of any scientific law requires knowing not just where it applies, but also where it breaks down. This exercise explores a famous counterexample to the Law of Large Numbers using the Cauchy distribution. By examining why the sample mean of Cauchy random variables fails to converge to a single value, you will gain a crucial insight into the necessary conditions, like the existence of a finite expected value, that underpin the law's predictive power [@problem_id:1967315].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\ldots, X_n, \\ldots$ drawn from the standard Cauchy distribution. The probability density function (PDF) for this distribution is given by:\n$$f(x) = \\frac{1}{\\pi(1+x^2)}, \\quad \\text{for } x \\in (-\\infty, \\infty)$$\nLet $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean of the first $n$ variables. A notable and unusual property of the Cauchy distribution is that for any integer $n \\ge 1$, the sample mean $\\bar{X}_n$ also follows the exact same standard Cauchy distribution.\n\nIn the context of the Weak Law of Large Numbers (WLLN), which describes the convergence of the sample mean, it is instructive to examine distributions that may not satisfy the law's standard conditions. For the sequence of Cauchy random variables described, calculate the limiting value of the probability that the sample mean deviates from the origin by more than a constant $k$.\n\nSpecifically, find an expression for the limit $L = \\lim_{n \\to \\infty} P(|\\bar{X}_n| > k)$, where $k$ is a positive real constant. Express your answer as a function of $k$.", "solution": "We have a sequence of i.i.d. random variables $X_{1}, X_{2}, \\ldots$ with the standard Cauchy density $f(x) = \\frac{1}{\\pi(1+x^{2})}$. The standard Cauchy distribution is strictly stable with index $1$, so for any integer $n \\geq 1$, the sample mean $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$ has the same distribution as $X_{1}$, i.e., $\\bar{X}_{n} \\sim \\text{Cauchy}(0,1)$. Consequently, for any fixed $k > 0$ and any $n$,\n$$\nP(|\\bar{X}_{n}| > k) = P(|X| > k),\n$$\nwhere $X \\sim \\text{Cauchy}(0,1)$. Therefore, the limit $L = \\lim_{n \\to \\infty} P(|\\bar{X}_{n}| > k)$ equals this constant tail probability for the standard Cauchy distribution.\n\nWe compute $P(|X| > k)$ using the density. By symmetry,\n$$\nP(|X| > k) = 2 \\int_{k}^{\\infty} \\frac{1}{\\pi(1+x^{2})} \\, dx.\n$$\nUsing the antiderivative $\\int \\frac{1}{1+x^{2}} \\, dx = \\arctan(x)$ and the limit $\\lim_{x \\to \\infty} \\arctan(x) = \\frac{\\pi}{2}$, we obtain\n$$\nP(|X| > k) = \\frac{2}{\\pi} \\left[ \\arctan(x) \\right]_{x=k}^{x=\\infty} = \\frac{2}{\\pi} \\left( \\frac{\\pi}{2} - \\arctan(k) \\right) = 1 - \\frac{2}{\\pi} \\arctan(k).\n$$\nHence,\n$$\nL = \\lim_{n \\to \\infty} P(|\\bar{X}_{n}| > k) = 1 - \\frac{2}{\\pi} \\arctan(k).\n$$\nThis shows that the probability does not converge to zero as $n \\to \\infty$, illustrating the failure of the Weak Law of Large Numbers for the Cauchy distribution.", "answer": "$$\\boxed{1 - \\frac{2}{\\pi}\\arctan(k)}$$", "id": "1967315"}, {"introduction": "In many real-world physical systems, measurements are not truly independent but are influenced by a common, fluctuating source. This problem models such a scenario using a network of sensors, where each reading combines a shared environmental variable and unique sensor noise. You will investigate how the Law of Large Numbers behaves in the presence of such correlations, revealing that averaging can eliminate independent noise but not systemic effects, leading to a surprising limit [@problem_id:1668567].", "problem": "A large-scale environmental sensing network is deployed to monitor a certain physical quantity, $T$. The value of $T$ is not a fixed constant but fluctuates over time due to complex environmental dynamics. For the purpose of analysis, $T$ is modeled as a random variable with a well-defined but unknown mean $E[T] = \\mu$ and a finite, non-zero variance $\\text{Var}(T) = \\sigma_T^2$.\n\nThe network consists of $n$ identical sensors. The reading of the $i$-th sensor, denoted by $X_i$, is a sum of the true quantity $T$ and an independent internal noise term $\\epsilon_i$. Thus, $X_i = T + \\epsilon_i$. The noise terms $\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n$ are modeled as independent and identically distributed (i.i.d.) random variables, each with an expected value of $E[\\epsilon_i] = 0$ and a finite variance $\\text{Var}(\\epsilon_i) = \\sigma_\\epsilon^2$. Furthermore, the noise terms are independent of the physical quantity $T$.\n\nTo estimate the mean quantity $\\mu$, an engineer computes the sample average of all sensor readings:\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i $$\nWhile it is often assumed that such an average converges to the true mean $\\mu$ for large $n$, the common influence of $T$ on all sensors introduces a systemic correlation. Your task is to determine the limit of the sample average $\\bar{X}_n$ as the number of sensors $n$ approaches infinity. Find the value $L$ to which $\\bar{X}_n$ converges in probability, meaning that for any arbitrary small positive number $\\delta$, the probability $P(|\\bar{X}_n - L| \\geq \\delta)$ approaches zero as $n \\to \\infty$.\n\nExpress your answer for $L$ as an analytic expression in terms of the variables defined.", "solution": "We start from the given measurement model for each sensor,\n$$\nX_{i} = T + \\epsilon_{i},\n$$\nwhere $T$ is a random variable with $E[T] = \\mu$ and $\\text{Var}(T) = \\sigma_{T}^{2}$, and the $\\epsilon_{i}$ are i.i.d. with $E[\\epsilon_{i}] = 0$ and $\\text{Var}(\\epsilon_{i}) = \\sigma_{\\epsilon}^{2}$, and are independent of $T$.\n\nThe sample average over $n$ sensors is\n$$\n\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}.\n$$\nSubstituting the model into the average gives\n$$\n\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(T + \\epsilon_{i}\\right) = T + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_{i}.\n$$\nDefine the noise average\n$$\n\\bar{\\epsilon}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_{i}.\n$$\nThen\n$$\n\\bar{X}_{n} = T + \\bar{\\epsilon}_{n}.\n$$\n\nBy the Strong Law of Large Numbers (or the Weak Law, which suffices for convergence in probability), since the $\\epsilon_{i}$ are i.i.d. with finite mean $E[\\epsilon_{i}] = 0$, we have\n$$\n\\bar{\\epsilon}_{n} \\xrightarrow{p} 0 \\quad \\text{as } n \\to \\infty.\n$$\nTherefore, for any $\\delta > 0$,\n$$\nP\\left(\\left|\\bar{X}_{n} - T\\right| \\geq \\delta\\right) \n= P\\left(\\left|T + \\bar{\\epsilon}_{n} - T\\right| \\geq \\delta\\right) \n= P\\left(\\left|\\bar{\\epsilon}_{n}\\right| \\geq \\delta\\right) \\to 0 \\quad \\text{as } n \\to \\infty.\n$$\nThis directly proves that\n$$\n\\bar{X}_{n} \\xrightarrow{p} T.\n$$\nEquivalently, the limit in probability $L$ is the random variable $T$ (not the constant $\\mu$), because the common term $T$ does not average out across sensors whereas the independent noises do. For completeness, note that $E[\\bar{X}_{n}] = \\mu$ for all $n$, and $\\text{Var}(\\bar{X}_{n}) = \\sigma_{T}^{2} + \\sigma_{\\epsilon}^{2}/n \\to \\sigma_{T}^{2}$, consistent with convergence in probability to $T$ rather than to a constant.", "answer": "$$\\boxed{T}$$", "id": "1668567"}]}