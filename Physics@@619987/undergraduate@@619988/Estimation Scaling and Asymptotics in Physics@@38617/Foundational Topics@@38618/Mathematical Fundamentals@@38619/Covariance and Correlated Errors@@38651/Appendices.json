{"hands_on_practices": [{"introduction": "This exercise provides a foundational example of correlated errors. We will explore how calculating two related physical quantities, momentum ($p$) and kinetic energy ($K$), from a single measurement of velocity ($v$) naturally leads to a covariance between their uncertainties. By working through this problem, you will apply the first-order propagation of uncertainty formula to quantify this correlation, a fundamental skill in experimental data analysis. [@problem_id:1892970]", "problem": "In a physics laboratory experiment, a student measures the velocity of a small particle of mass $m$. The mass $m$ is known precisely, with negligible uncertainty. The single velocity measurement yields a value $v_0$ with an associated standard uncertainty of $\\sigma_v$. From this measurement, the student calculates two derived quantities: the particle's momentum, $p = mv$, and its kinetic energy, $K = \\frac{1}{2}mv^2$. Because both $p$ and $K$ are calculated from the same single measurement of $v$, their uncertainties are correlated.\n\nDetermine the covariance between the kinetic energy and the momentum, denoted as $\\text{Cov}(K, p)$. Express your answer as an analytic expression in terms of the given parameters $m$, $v_0$, and $\\sigma_v$.", "solution": "Let $v$ be a random variable representing the measured velocity with mean $E[v]=v_{0}$ and variance $\\text{Var}(v)=\\sigma_{v}^{2}$. The derived quantities are $p(v)=m v$ and $K(v)=\\frac{1}{2} m v^{2}$.\n\nUsing first-order uncertainty propagation (the delta method) for functions of a single variable, the covariance between $f(v)$ and $g(v)$ is approximated by\n$$\n\\text{Cov}(f(v),g(v)) \\approx f'(v_{0})\\,g'(v_{0})\\,\\text{Var}(v).\n$$\nHere, $f(v)=K(v)$ and $g(v)=p(v)$, so\n$$\n\\frac{dK}{dv}=m v \\quad \\Rightarrow \\quad \\left.\\frac{dK}{dv}\\right|_{v_{0}}=m v_{0}, \\qquad \\frac{dp}{dv}=m.\n$$\nTherefore,\n$$\n\\text{Cov}(K,p) \\approx (m v_{0})(m)\\,\\sigma_{v}^{2}=m^{2} v_{0} \\sigma_{v}^{2}.\n$$\n\nMoreover, if $v$ is normally distributed (a standard assumption for measurement uncertainty), this result is exact. Using the definition $\\text{Cov}(K,p)=E[K p]-E[K]E[p]$,\n$$\n\\text{Cov}(K,p)=\\frac{m^{2}}{2}\\left(E[v^{3}]-E[v^{2}]\\,E[v]\\right).\n$$\nFor a normal $v$ with mean $v_{0}$ and variance $\\sigma_{v}^{2}$, $E[v^{2}]=v_{0}^{2}+\\sigma_{v}^{2}$ and $E[v^{3}]=v_{0}^{3}+3 v_{0}\\sigma_{v}^{2}$, giving\n$$\n\\text{Cov}(K,p)=\\frac{m^{2}}{2}\\left(v_{0}^{3}+3 v_{0}\\sigma_{v}^{2}-v_{0}(v_{0}^{2}+\\sigma_{v}^{2})\\right)=\\frac{m^{2}}{2}\\left(2 v_{0}\\sigma_{v}^{2}\\right)=m^{2} v_{0} \\sigma_{v}^{2}.\n$$\nThus, the covariance is $m^{2} v_{0} \\sigma_{v}^{2}$.", "answer": "$$\\boxed{m^{2} v_{0} \\sigma_{v}^{2}}$$", "id": "1892970"}, {"introduction": "Correlated errors are not just a consequence of shared variables in formulas; they also arise from the very nature of statistical fitting. This practice explores the common scenario of fitting a straight line to data points, a cornerstone of experimental science. You will develop a qualitative and intuitive understanding of why the estimated slope ($m$) and intercept ($c$) often have strongly anti-correlated uncertainties, particularly when extrapolating far from the data. [@problem_id:1892979]", "problem": "An astronomer is tracking a newly discovered asteroid. Over a short observation window, the asteroid's trajectory across the sky can be well-approximated by a straight line. The astronomer records a series of data points $(x_i, y_i)$, where $x$ represents the time of observation and $y$ represents the asteroid's angular position. All observations are made over a few nights, long after the initial discovery at time $x=0$. Consequently, all the recorded $x_i$ values are large and clustered far away from the $y$-axis (the origin of time).\n\nThe astronomer fits a linear model of the form $y(x) = mx + c$ to the data points using the method of least squares to estimate the slope $m$ and the y-intercept $c$. The slope $m$ represents the asteroid's angular velocity, and the intercept $c$ represents its extrapolated angular position at time $x=0$. Due to measurement uncertainties, the estimated values of $m$ and $c$ have statistical errors. It is observed that these errors are strongly anti-correlated; that is, a random error that causes an overestimation of the slope $m$ is highly likely to be accompanied by an underestimation of the intercept $c$, and vice-versa.\n\nWhich of the following statements provides the best qualitative explanation for this strong anti-correlation between the errors in $m$ and $c$?\n\nA. The best-fit line must pass through the center of the data cloud. Since the data cloud is far from the $y$-axis, a small upward tilt (increase in slope) pivots the line around this distant point, causing a large downward shift in its $y$-intercept.\n\nB. The errors in the parameters $m$ and $c$ are independent of the errors in the data points $(x_i, y_i)$, and the principle of least squares requires the product of the parameter errors to be negative.\n\nC. Because the data is far from the $y$-axis, the value of the intercept $c$ is determined by a long extrapolation. This large extrapolation magnifies the uncertainty, which by definition creates an anti-correlation with the slope.\n\nD. Any random error that increases the slope $m$ makes the fitted line steeper. To maintain the best fit for data points far from the origin, the line must be shifted upwards, which corresponds to an increase in the intercept $c$.\n\nE. The formulas for the standard errors of $m$ and $c$ both depend on the sum of the squared residuals. To keep this sum at a minimum, an increase in one parameter's error must be offset by a decrease in the other's.", "solution": "Let the data follow the linear model $y_{i}=m x_{i}+c+\\epsilon_{i}$ with $\\epsilon_{i}$ independent, mean zero, variance $\\sigma^{2}$. The ordinary least squares estimators satisfy\n$$\n\\hat{m}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\\quad \\hat{c}=\\bar{y}-\\hat{m}\\,\\bar{x},\n$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means and $S_{xx}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$.\n\nStandard OLS variance and covariance formulas (for fixed regressors) give\n$$\n\\operatorname{Var}(\\hat{m})=\\frac{\\sigma^{2}}{S_{xx}},\\quad \\operatorname{Var}(\\hat{c})=\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\bar{x}^{2}}{S_{xx}}\\right),\\quad \\operatorname{Cov}(\\hat{m},\\hat{c})=-\\frac{\\sigma^{2}\\bar{x}}{S_{xx}}.\n$$\nEquivalently, using $\\hat{c}=\\bar{y}-\\hat{m}\\bar{x}$, small perturbations satisfy $\\delta \\hat{c}=-\\bar{x}\\,\\delta \\hat{m}+\\delta \\bar{y}$. Since $\\operatorname{Var}(\\bar{y})=\\sigma^{2}/n$ while $\\operatorname{Var}(\\hat{m})=\\sigma^{2}/S_{xx}$, the term $-\\bar{x}\\,\\delta \\hat{m}$ dominates when $|\\bar{x}|$ is large compared with the scale $\\sqrt{S_{xx}/n}$, yielding a strong negative coupling between errors in $\\hat{m}$ and $\\hat{c}$. The correlation is\n$$\n\\operatorname{Corr}(\\hat{m},\\hat{c})=\\frac{-\\sigma^{2}\\bar{x}/S_{xx}}{\\sqrt{(\\sigma^{2}/S_{xx})\\cdot \\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\bar{x}^{2}}{S_{xx}}\\right)}}=-\\frac{\\bar{x}}{\\sqrt{\\frac{S_{xx}}{n}+\\bar{x}^{2}}},\n$$\nwhich approaches $-1$ when $|\\bar{x}|^{2}\\gg S_{xx}/n$. This is precisely the situation described: all $x_{i}$ are large and clustered far from the $y$-axis, so the line effectively pivots about the data cloud near $(\\bar{x},\\bar{y})$. A small increase in slope around this distant pivot forces a decrease in the intercept to keep the fitted line close to the same cloud, producing strong anti-correlation.\n\nQualitatively, this matches statement A: the best-fit line passes near the data cloudâ€™s center, and when that center is far from the $y$-axis, a slight tilt in slope causes a large opposite change in the intercept at $x=0$. Statements B, C, E describe incorrect principles, and D asserts the wrong direction of intercept change.", "answer": "$$\\boxed{A}$$", "id": "1892979"}, {"introduction": "This final practice delves into a sophisticated application from condensed matter physics, demonstrating how uncertainty in a single underlying model parameter can induce correlations between other derived quantities. We will examine the extraction of critical exponents near a phase transition, where an imprecise knowledge of the critical temperature ($T_c$) creates a covariance between the exponents for specific heat ($\\alpha$) and magnetic susceptibility ($\\gamma$). This problem highlights the crucial role of covariance analysis in cutting-edge physical modeling and parameter estimation. [@problem_id:1892935]", "problem": "In the study of critical phenomena, the behavior of physical quantities near a continuous phase transition is described by power laws characterized by critical exponents. Consider a ferromagnetic material with a critical temperature $T_c$. Near this temperature, the specific heat capacity $C_s$ and the magnetic susceptibility $\\chi$ are expected to diverge as:\n\n$$C_s(T) = A_s |T - T_c|^{-\\alpha}$$\n$$\\chi(T) = A_\\chi |T - T_c|^{-\\gamma}$$\n\nwhere $\\alpha$ and $\\gamma$ are the critical exponents for the specific heat and susceptibility, respectively, and $A_s$ and $A_\\chi$ are material-dependent amplitudes.\n\nAn experimentalist performs two separate experiments to determine these exponents.\n1.  In the first experiment, the specific heat capacity is measured at two distinct temperatures, $T_1$ and $T_2$, yielding values $C_{s,1}$ and $C_{s,2}$.\n2.  In the second experiment, the magnetic susceptibility is measured at the same two temperatures, $T_1$ and $T_2$, yielding values $\\chi_1$ and $\\chi_2$.\n\nFor this analysis, assume the temperature range is always on one side of the transition, such that $T_1 > T_2 > T_c$. The exponents $\\alpha$ and $\\gamma$ are then calculated from these pairs of measurements.\n\nWhile the measurements of $T_1$, $T_2$, $C_s$, and $\\chi$ are considered to be perfectly precise, the critical temperature $T_c$ is known only from a separate estimation process. The best estimate for the critical temperature is $T_{c,0}$, and this estimate has a small, known uncertainty characterized by a standard deviation $\\sigma_{T_c}$. All sources of error are statistically independent.\n\nLet $\\alpha_0$ and $\\gamma_0$ be the values of the exponents calculated using the best estimate $T_{c,0}$. Derive an analytical expression for the covariance between the estimators for $\\alpha$ and $\\gamma$, denoted $\\text{Cov}(\\alpha, \\gamma)$, that arises solely due to the uncertainty in $T_c$. Your final answer should be expressed in terms of $\\alpha_0$, $\\gamma_0$, $T_1$, $T_2$, $T_{c,0}$, and $\\sigma_{T_c}$.", "solution": "Because $T_{1}>T_{2}>T_{c}$, the absolute values drop and\n$$\\frac{C_{s,1}}{C_{s,2}}=\\left(\\frac{T_{1}-T_{c}}{T_{2}-T_{c}}\\right)^{-\\alpha},\\qquad \\frac{\\chi_{1}}{\\chi_{2}}=\\left(\\frac{T_{1}-T_{c}}{T_{2}-T_{c}}\\right)^{-\\gamma}.$$\nTaking natural logarithms gives\n$$\\ln\\!\\left(\\frac{C_{s,1}}{C_{s,2}}\\right)=-\\alpha\\,\\ln\\!\\left(\\frac{T_{1}-T_{c}}{T_{2}-T_{c}}\\right),\\qquad\n\\ln\\!\\left(\\frac{\\chi_{1}}{\\chi_{2}}\\right)=-\\gamma\\,\\ln\\!\\left(\\frac{T_{1}-T_{c}}{T_{2}-T_{c}}\\right).$$\nDefine $L(T_{c})=\\ln\\!\\left(\\frac{T_{1}-T_{c}}{T_{2}-T_{c}}\\right)$ and the data-determined constants\n$$a\\equiv -\\ln\\!\\left(\\frac{C_{s,1}}{C_{s,2}}\\right),\\qquad b\\equiv -\\ln\\!\\left(\\frac{\\chi_{1}}{\\chi_{2}}\\right).$$\nThen\n$$\\alpha(T_{c})=\\frac{a}{L(T_{c})},\\qquad \\gamma(T_{c})=\\frac{b}{L(T_{c})}.$$\nLet $T_{c,0}$ be the best estimate and $\\sigma_{T_{c}}^{2}$ the variance of $T_{c}$. Define $L_{0}=L(T_{c,0})$. The exponents computed at $T_{c,0}$ are\n$$\\alpha_{0}=\\frac{a}{L_{0}},\\qquad \\gamma_{0}=\\frac{b}{L_{0}},\\qquad\\text{so}\\quad a=\\alpha_{0}L_{0},\\; b=\\gamma_{0}L_{0}.$$\nDifferentiate $L(T_{c})$:\n$$\\frac{dL}{dT_{c}}=\\frac{d}{dT_{c}}\\big[\\ln(T_{1}-T_{c})-\\ln(T_{2}-T_{c})\\big]=-\\frac{1}{T_{1}-T_{c}}+\\frac{1}{T_{2}-T_{c}}=\\frac{T_{1}-T_{2}}{(T_{1}-T_{c})(T_{2}-T_{c})}.$$\nDifferentiate $\\alpha$ and $\\gamma$ with respect to $T_{c}$:\n$$\\frac{d\\alpha}{dT_{c}}=-\\frac{a}{L^{2}}\\frac{dL}{dT_{c}},\\qquad \\frac{d\\gamma}{dT_{c}}=-\\frac{b}{L^{2}}\\frac{dL}{dT_{c}}.$$\nEvaluated at $T_{c,0}$ and using $a=\\alpha_{0}L_{0}$, $b=\\gamma_{0}L_{0}$:\n$$\\left.\\frac{d\\alpha}{dT_{c}}\\right|_{0}=-\\alpha_{0}\\frac{1}{L_{0}}\\left.\\frac{dL}{dT_{c}}\\right|_{0},\\qquad\n\\left.\\frac{d\\gamma}{dT_{c}}\\right|_{0}=-\\gamma_{0}\\frac{1}{L_{0}}\\left.\\frac{dL}{dT_{c}}\\right|_{0},$$\nwith\n$$\\left.\\frac{dL}{dT_{c}}\\right|_{0}=\\frac{T_{1}-T_{2}}{(T_{1}-T_{c,0})(T_{2}-T_{c,0})},\\qquad L_{0}=\\ln\\!\\left(\\frac{T_{1}-T_{c,0}}{T_{2}-T_{c,0}}\\right).$$\nSince the only uncertainty is $T_{c}$ with variance $\\sigma_{T_{c}}^{2}$ and all errors are independent, first-order (delta-method) error propagation for a single scalar uncertainty gives\n$$\\text{Cov}(\\alpha,\\gamma)\\approx \\left.\\frac{d\\alpha}{dT_{c}}\\right|_{0}\\left.\\frac{d\\gamma}{dT_{c}}\\right|_{0}\\sigma_{T_{c}}^{2}.$$\nSubstituting the derivatives yields\n$$\\text{Cov}(\\alpha,\\gamma)=\\alpha_{0}\\gamma_{0}\\,\\frac{\\left(\\left.\\frac{dL}{dT_{c}}\\right|_{0}\\right)^{2}}{L_{0}^{2}}\\,\\sigma_{T_{c}}^{2}\n=\\alpha_{0}\\gamma_{0}\\,\\frac{(T_{1}-T_{2})^{2}}{(T_{1}-T_{c,0})^{2}(T_{2}-T_{c,0})^{2}\\left[\\ln\\!\\left(\\frac{T_{1}-T_{c,0}}{T_{2}-T_{c,0}}\\right)\\right]^{2}}\\,\\sigma_{T_{c}}^{2}.$$\nThis expression depends only on $\\alpha_{0}$, $\\gamma_{0}$, $T_{1}$, $T_{2}$, $T_{c,0}$, and $\\sigma_{T_{c}}$, as required, and captures the covariance arising solely from the uncertainty in $T_{c}$ to leading order in $\\sigma_{T_{c}}$.", "answer": "$$\\boxed{\\alpha_{0}\\gamma_{0}\\,\\sigma_{T_{c}}^{2}\\,\\frac{(T_{1}-T_{2})^{2}}{(T_{1}-T_{c,0})^{2}(T_{2}-T_{c,0})^{2}\\left[\\ln\\!\\left(\\frac{T_{1}-T_{c,0}}{T_{2}-T_{c,0}}\\right)\\right]^{2}}}$$", "id": "1892935"}]}