{"hands_on_practices": [{"introduction": "This first practice problem introduces the geometric mean in a context where its use is highly intuitive: finding a representative value for a quantity that spans several orders of magnitude. You will explore a simplified model of a neuron's cell membrane, which has vastly different electrical resistances in its resting and active states. By calculating the geometric mean, you'll discover how to find a single effective resistance that is \"logarithmically centered\" between these two extremes, a concept with broad applications in modeling physical and biological systems [@problem_id:1903338].", "problem": "In the field of computational neuroscience, a crucial task is to create simplified models of biological neurons. A neuron's cell membrane exhibits vastly different electrical properties depending on its state. In its resting state, the membrane is a strong insulator, characterized by a high electrical resistance, $R_{rest}$. During an action potential, numerous ion channels open, causing the resistance to drop dramatically to a much lower value, $R_{active}$.\n\nA physicist is developing a 'single-value' model for a patch of a neuron's membrane. To represent the vast range of resistance with a single characteristic value, $R_{eff}$, they adopt a principle common in order-of-magnitude physics: the characteristic value should be positioned 'logarithmically' between the two extremes. This means the ratio of the characteristic resistance to the minimum resistance should be equal to the ratio of the maximum resistance to the characteristic resistance.\n\nGiven the resting resistance $R_{rest} = 1.2 \\times 10^9 \\, \\Omega$ and the active resistance during an action potential $R_{active} = 3.0 \\times 10^5 \\, \\Omega$, calculate the effective resistance $R_{eff}$.\n\nExpress your answer in mega-ohms ($M\\Omega$). Round your final answer to three significant figures.", "solution": "The stated logarithmic positioning condition is that the ratio of the characteristic value to the minimum equals the ratio of the maximum to the characteristic:\n$$\n\\frac{R_{\\mathrm{eff}}}{R_{\\min}}=\\frac{R_{\\max}}{R_{\\mathrm{eff}}}.\n$$\nHere, $R_{\\max}=R_{\\mathrm{rest}}$ and $R_{\\min}=R_{\\mathrm{active}}$, so\n$$\n\\frac{R_{\\mathrm{eff}}}{R_{\\mathrm{active}}}=\\frac{R_{\\mathrm{rest}}}{R_{\\mathrm{eff}}}\n\\;\\;\\Rightarrow\\;\\;\nR_{\\mathrm{eff}}^{2}=R_{\\mathrm{rest}}\\,R_{\\mathrm{active}}\n\\;\\;\\Rightarrow\\;\\;\nR_{\\mathrm{eff}}=\\sqrt{R_{\\mathrm{rest}}\\,R_{\\mathrm{active}}}.\n$$\nSubstitute the given values $R_{\\mathrm{rest}}=1.2\\times 10^{9}\\,\\Omega$ and $R_{\\mathrm{active}}=3.0\\times 10^{5}\\,\\Omega$:\n$$\nR_{\\mathrm{rest}}\\,R_{\\mathrm{active}}=(1.2\\times 10^{9})(3.0\\times 10^{5})\\,\\Omega^{2}\n=3.6\\times 10^{14}\\,\\Omega^{2},\n$$\nhence\n$$\nR_{\\mathrm{eff}}=\\sqrt{3.6\\times 10^{14}}\\,\\Omega=\\sqrt{3.6}\\times 10^{7}\\,\\Omega.\n$$\nNumerically, $\\sqrt{3.6}\\approx 1.897366596$, so\n$$\nR_{\\mathrm{eff}}\\approx 1.897366596\\times 10^{7}\\,\\Omega.\n$$\nExpress in mega-ohms using $1\\,\\text{M}\\Omega=10^{6}\\,\\Omega$:\n$$\nR_{\\mathrm{eff}}\\;(\\text{M}\\Omega)=\\frac{1.897366596\\times 10^{7}}{10^{6}}\\approx 18.97366596\\,\\text{M}\\Omega.\n$$\nRounded to three significant figures,\n$$\nR_{\\mathrm{eff}}\\approx 19.0\\,\\text{M}\\Omega.\n$$", "answer": "$$\\boxed{19.0}$$", "id": "1903338"}, {"introduction": "Having seen how the geometric mean finds a middle ground between two related values, let's explore its power in a more extreme estimation scenario. This exercise challenges you to estimate the cruising speed of a migratory bird using bounds that seem comically mismatched: the slow descent of a single feather and the immense speed of an orbiting satellite. This practice [@problem_id:1903306] illustrates the remarkable robustness of geometric mean estimation, a cornerstone technique in the \"art of approximation\" in physics, where even poor initial information can lead to a plausible result.", "problem": "In the field of estimation physics, the geometric mean is a powerful tool for approximating a quantity when one can establish plausible, albeit extremely wide, lower and upper bounds. Your task is to estimate the typical cruising airspeed of a migratory bird.\n\nFor this estimation, you will use two bounds derived from vastly different physical regimes. The lower bound, $v_{low}$, is the terminal velocity of a single bird feather falling through the air. Model the feather as a small, flat object of mass $m_f$ and surface area $A_f$ falling with its largest surface oriented horizontally. The upper bound, $v_{high}$, is the orbital speed of a satellite in a circular Low Earth Orbit (LEO). Assume the altitude of the orbit is negligible compared to the Earth's radius.\n\nThe drag force on the feather is given by the formula $F_d = \\frac{1}{2} C_d \\rho_{air} A_f v^2$, where $v$ is the speed of the feather.\n\nUse the following constants for your calculation:\n- Feather mass, $m_f = 2.0 \\times 10^{-5}$ kg\n- Feather area, $A_f = 5.0 \\times 10^{-4}$ m$^2$\n- Drag coefficient for the feather, $C_d = 1.5$\n- Density of air at sea level, $\\rho_{air} = 1.225$ kg/m$^3$\n- Radius of the Earth, $R_E = 6.37 \\times 10^6$ m\n- Standard gravitational acceleration at Earth's surface, $g = 9.81$ m/s$^2$\n\nCalculate the estimate for the bird's cruising speed by taking the geometric mean of $v_{low}$ and $v_{high}$. Express your final answer in m/s, rounded to three significant figures.", "solution": "We estimate a typical cruising airspeed by taking the geometric mean of a very low speed (the terminal velocity of a feather) and a very high speed (a circular LEO orbital speed near Earthâ€™s surface).\n\nLower bound (terminal velocity of a feather): At terminal velocity, weight balances drag:\n$$\nm_{f} g = \\frac{1}{2} C_{d} \\rho_{air} A_{f} v_{low}^{2}.\n$$\nSolving for $v_{low}$ gives\n$$\nv_{low} = \\sqrt{\\frac{2 m_{f} g}{C_{d} \\rho_{air} A_{f}}}.\n$$\nUpper bound (orbital speed in circular LEO with negligible altitude): For a circular orbit of radius $R_{E}$, \n$$\nv_{high} = \\sqrt{\\frac{G M_{E}}{R_{E}}}.\n$$\nUsing $g = \\frac{G M_{E}}{R_{E}^{2}}$, this becomes\n$$\nv_{high} = \\sqrt{g R_{E}}.\n$$\nThe estimate is the geometric mean:\n$$\nv_{est} = \\sqrt{v_{low}\\, v_{high}} = \\left(\\frac{2 m_{f} g^{2} R_{E}}{C_{d} \\rho_{air} A_{f}}\\right)^{\\frac{1}{4}}.\n$$\nNow substitute the provided numerical values: $m_{f} = 2.0 \\times 10^{-5}$ kg, $A_{f} = 5.0 \\times 10^{-4}$ m$^2$, $C_{d} = 1.5$, $\\rho_{air} = 1.225$ kg/m$^3$, $R_{E} = 6.37 \\times 10^{6}$ m, and $g = 9.81$ m/s$^2$.\n\nFirst compute the bounds:\n$$\nv_{low} = \\sqrt{\\frac{2(2.0 \\times 10^{-5}) (9.81)}{(1.5)(1.225)(5.0 \\times 10^{-4})}} \\approx 0.6535 \\text{ m/s},\n$$\n$$\nv_{high} = \\sqrt{(9.81)(6.37 \\times 10^{6})} \\approx 7.905 \\times 10^{3} \\text{ m/s}.\n$$\nThen take the geometric mean:\n$$\nv_{est} = \\sqrt{v_{low} v_{high}} \\approx \\sqrt{(0.6535)(7.905 \\times 10^{3})} \\text{ m/s} \\approx 71.9 \\text{ m/s}.\n$$\nRounded to three significant figures, the estimate is $71.9$ m/s.", "answer": "$$\\boxed{71.9}$$", "id": "1903306"}, {"introduction": "Our final practice problem takes a significant step from simply calculating an estimate to quantifying its uncertainty. While the geometric mean provides a powerful point estimate, any real scientific measurement requires an error bar. This advanced computational exercise [@problem_id:2404364] guides you to compare two fundamental resampling methods, the jackknife and the bootstrap, for estimating the standard error of a geometric mean. This hands-on coding problem bridges the gap between theoretical estimation and practical data analysis, equipping you with essential skills for modern computational science.", "problem": "You are given independent and identically distributed data points $X_1,\\dots,X_n$ generated from a log-normal distribution with parameters $(\\mu,\\sigma^2)$, meaning $\\ln X_i$ is normally distributed with mean $\\mu$ and variance $\\sigma^2$, using the natural logarithm base $e$. Consider the geometric mean estimator\n$$\n\\hat{G} = \\exp\\left(\\frac{1}{n}\\sum_{i=1}^n \\ln X_i\\right).\n$$\nFrom the core definitions of the normal and log-normal distributions and the definition of standard error as the standard deviation of an estimator, it follows that one can derive a closed-form expression for the true standard error of $\\hat{G}$ as a function of $\\mu$, $\\sigma$, and $n$. Your task is to implement a program that does all of the following from first principles:\n- Derive, justify, and implement the true standard error of $\\hat{G}$ for $X_i \\sim \\mathrm{LogNormal}(\\mu,\\sigma^2)$, expressed in terms of $\\mu$, $\\sigma$, and $n$, without appealing to any result not derivable from the properties of the normal and log-normal distributions and standard transformations.\n- Implement the nonparametric jackknife standard error estimate: form the $n$ leave-one-out samples (each of size $n-1$), compute the corresponding leave-one-out geometric mean estimates, and use the standard jackknife variance formula based on the dispersion of the leave-one-out estimates. The jackknife standard error is the square root of this variance estimate.\n- Implement the nonparametric bootstrap standard error estimate with $B$ bootstrap replicates: resample the data with replacement $B$ times, compute the geometric mean for each resample, and take the sample standard deviation across bootstrap replicates as the bootstrap standard error. Use the unbiased sample standard deviation (denominator $B-1$), i.e., degrees of freedom equal to $1$.\n- For each test case, compare the jackknife and bootstrap standard error estimates against the true standard error by reporting the absolute relative error\n$$\n\\varepsilon = \\frac{\\left|\\widehat{\\mathrm{SE}} - \\mathrm{SE}_{\\text{true}}\\right|}{\\mathrm{SE}_{\\text{true}}}.\n$$\nIf $\\mathrm{SE}_{\\text{true}} = 0$, instead report the absolute error $\\left|\\widehat{\\mathrm{SE}} - 0\\right|$. All reported errors must be expressed as decimal fractions (not as percentages).\n\nFundamental base you may rely on:\n- If $Y \\sim \\mathcal{N}(\\mu,\\tau^2)$, then $Z=\\exp(Y)$ is log-normal and its moments follow from the moment-generating function of a normal random variable.\n- If $Y_1,\\dots,Y_n$ are independent and identically distributed normal random variables with variance $\\sigma^2$, then $\\bar{Y}=(1/n)\\sum_{i=1}^n Y_i$ is normal with variance $\\sigma^2/n$.\n- The standard error of an estimator is the square root of its variance.\n\nAlgorithmic requirements:\n- Use the transformation $Y_i = \\ln X_i$ to express $\\hat{G}$ in terms of $\\bar{Y}$ and derive the true standard error in closed form.\n- Implement the jackknife standard error using the $n$ leave-one-out estimates and the standard jackknife variance formula.\n- Implement the bootstrap standard error by resampling with replacement $B$ times and taking the unbiased sample standard deviation across the bootstrap estimates.\n\nTest suite:\nFor each parameter set $(n,\\mu,\\sigma,B,\\text{seed})$, generate a synthetic dataset by drawing $n$ samples from a log-normal distribution with those parameters using the given random seed, compute the jackknife and bootstrap standard error estimates, compute the true standard error using your derived formula, and then compute the error metrics as specified above.\n\nUse the following parameter sets, which respectively test a typical case, higher noise, a minimal-sample boundary for jackknife, a shifted mean with moderate noise, and a zero-noise boundary:\n- Case $1$: $n=50$, $\\mu=0.0$, $\\sigma=0.5$, $B=4000$, seed $42$.\n- Case $2$: $n=50$, $\\mu=0.0$, $\\sigma=1.2$, $B=6000$, seed $314159$.\n- Case $3$: $n=2$, $\\mu=0.0$, $\\sigma=0.5$, $B=2000$, seed $7$.\n- Case $4$: $n=100$, $\\mu=1.0$, $\\sigma=0.7$, $B=5000$, seed $123456$.\n- Case $5$: $n=30$, $\\mu=-0.5$, $\\sigma=0.0$, $B=1000$, seed $99$.\n\nFinal output format:\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output two floats in order: first the jackknife error metric for that case, then the bootstrap error metric for that case.\n- Concatenate these pairs for the $5$ cases into one flat list, so the output has $10$ floats total: $[\\varepsilon_{\\text{jack,1}},\\varepsilon_{\\text{boot,1}},\\varepsilon_{\\text{jack,2}},\\varepsilon_{\\text{boot,2}},\\dots,\\varepsilon_{\\text{jack,5}},\\varepsilon_{\\text{boot,5}}]$.\n- Round each float to $6$ decimal places.", "solution": "The problem presented is a well-posed exercise in computational statistics. It is scientifically sound, internally consistent, and requires the application of fundamental principles of statistical estimation theory. We will proceed with the solution, which involves three main parts: the analytical derivation of the true standard error, the implementation of two standard nonparametric resampling estimators (jackknife and bootstrap), and the quantitative comparison of their performance on simulated data.\n\n### 1. Analytical Derivation of the True Standard Error\n\nThe task is to find the standard error of the geometric mean estimator, $\\hat{G}$, for a sample $X_1, \\dots, X_n$ of independent and identically distributed (i.i.d.) random variables drawn from a log-normal distribution, $\\mathrm{LogNormal}(\\mu, \\sigma^2)$. The standard error is, by definition, the square root of the variance of the estimator.\n\nLet the random variables $X_i$ be defined by the transformation $X_i = \\exp(Y_i)$, where $Y_i$ are i.i.d. random variables from a normal distribution, $Y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n\nThe geometric mean estimator $\\hat{G}$ is given by:\n$$\n\\hat{G} = \\exp\\left(\\frac{1}{n}\\sum_{i=1}^n \\ln X_i\\right)\n$$\nSubstituting $\\ln X_i = Y_i$, the estimator simplifies to:\n$$\n\\hat{G} = \\exp\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\exp(\\bar{Y})\n$$\nwhere $\\bar{Y}$ is the sample mean of the normally distributed variables $Y_i$.\n\nAs the $Y_i$ are i.i.d. normal random variables, their sample mean $\\bar{Y}$ is also normally distributed. The parameters of this distribution are:\n$$\nE[\\bar{Y}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n Y_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[Y_i] = \\frac{1}{n}(n\\mu) = \\mu\n$$\n$$\n\\mathrm{Var}(\\bar{Y}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\mathrm{Var}(Y_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}\n$$\nThus, the distribution of the sample mean of the logarithms is $\\bar{Y} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)$.\n\nThe estimator $\\hat{G}$ is the exponential of a normal random variable, which means $\\hat{G}$ itself follows a log-normal distribution. To find its variance, we must compute its first and second moments, $E[\\hat{G}]$ and $E[\\hat{G}^2]$. These moments can be derived from the moment-generating function (MGF) of the normal variable $\\bar{Y}$. The MGF of a normal variable $W \\sim \\mathcal{N}(\\mu_W, \\sigma_W^2)$ is $M_W(t) = E[e^{tW}] = \\exp(\\mu_W t + \\frac{1}{2}\\sigma_W^2 t^2)$. For $\\bar{Y}$, we have $\\mu_{\\bar{Y}} = \\mu$ and $\\sigma_{\\bar{Y}}^2 = \\sigma^2/n$.\n\nThe expected value of $\\hat{G}$ is:\n$$\nE[\\hat{G}] = E[\\exp(\\bar{Y})] = M_{\\bar{Y}}(1) = \\exp\\left(\\mu \\cdot 1 + \\frac{1}{2}\\frac{\\sigma^2}{n} \\cdot 1^2\\right) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right)\n$$\nThe expected value of $\\hat{G}^2$ is:\n$$\nE[\\hat{G}^2] = E[(\\exp(\\bar{Y}))^2] = E[\\exp(2\\bar{Y})] = M_{\\bar{Y}}(2) = \\exp\\left(\\mu \\cdot 2 + \\frac{1}{2}\\frac{\\sigma^2}{n} \\cdot 2^2\\right) = \\exp\\left(2\\mu + \\frac{2\\sigma^2}{n}\\right)\n$$\nThe variance of $\\hat{G}$ is given by $\\mathrm{Var}(\\hat{G}) = E[\\hat{G}^2] - (E[\\hat{G}])^2$:\n$$\n\\mathrm{Var}(\\hat{G}) = \\exp\\left(2\\mu + \\frac{2\\sigma^2}{n}\\right) - \\left[\\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right)\\right]^2\n$$\n$$\n= \\exp\\left(2\\mu + \\frac{2\\sigma^2}{n}\\right) - \\exp\\left(2\\mu + \\frac{\\sigma^2}{n}\\right)\n$$\nFactoring out the term $\\exp(2\\mu + \\sigma^2/n)$ yields:\n$$\n\\mathrm{Var}(\\hat{G}) = \\exp\\left(2\\mu + \\frac{\\sigma^2}{n}\\right) \\left[\\exp\\left(\\frac{\\sigma^2}{n}\\right) - 1\\right]\n$$\nThe true standard error, $\\mathrm{SE}_{\\text{true}}(\\hat{G})$, is the square root of the variance:\n$$\n\\mathrm{SE}_{\\text{true}}(\\hat{G}) = \\sqrt{\\mathrm{Var}(\\hat{G})} = \\sqrt{\\exp\\left(2\\mu + \\frac{\\sigma^2}{n}\\right) \\left[\\exp\\left(\\frac{\\sigma^2}{n}\\right) - 1\\right]}\n$$\nThis can be written more cleanly as:\n$$\n\\mathrm{SE}_{\\text{true}}(\\hat{G}) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) \\sqrt{\\exp\\left(\\frac{\\sigma^2}{n}\\right) - 1}\n$$\nIn the special case where $\\sigma = 0$, the variance is zero. The data points are constant, $X_i = \\exp(\\mu)$, and so is the estimator $\\hat{G} = \\exp(\\mu)$. The standard error is correctly found to be $0$ by this formula.\n\n### 2. Implementation of Resampling-Based Standard Error Estimators\n\nWe will implement two nonparametric methods for estimating the standard error from the data itself, without knowledge of the true parameters $\\mu$ and $\\sigma$.\n\n#### Jackknife Standard Error\nThe jackknife method generates $n$ resamples by systematically leaving out one observation at a time. For a dataset of size $n$, this creates $n$ \"leave-one-out\" samples, each of size $n-1$.\nLet the original data be $X = (X_1, \\dots, X_n)$. The estimator calculated on the full sample is $\\hat{G}$. The $i$-th leave-one-out sample is $X_{(i)} = (X_1, \\dots, X_{i-1}, X_{i+1}, \\dots, X_n)$. The estimator computed on this sample is $\\hat{G}_{(i)}$.\n\nThe jackknife estimate of the variance is given by the scaled variance of these leave-one-out estimates:\n$$\n\\widehat{\\mathrm{Var}}_{\\text{jack}}(\\hat{G}) = \\frac{n-1}{n} \\sum_{i=1}^n \\left(\\hat{G}_{(i)} - \\bar{G}_{(\\cdot)}\\right)^2\n$$\nwhere $\\bar{G}_{(\\cdot)} = \\frac{1}{n} \\sum_{i=1}^n \\hat{G}_{(i)}$ is the mean of the leave-one-out estimates. The jackknife standard error is the square root of this variance:\n$$\n\\widehat{\\mathrm{SE}}_{\\text{jack}}(\\hat{G}) = \\sqrt{\\widehat{\\mathrm{Var}}_{\\text{jack}}(\\hat{G})}\n$$\nComputationally, it is more efficient to operate on the log-transformed data $Y_i = \\ln X_i$. Let $S_Y = \\sum_{k=1}^n Y_k$. Then $\\hat{G}_{(i)} = \\exp\\left(\\frac{1}{n-1} \\sum_{j \\ne i} Y_j\\right) = \\exp\\left(\\frac{S_Y - Y_i}{n-1}\\right)$. This avoids redundant calculations.\n\n#### Bootstrap Standard Error\nThe bootstrap method involves generating a large number, $B$, of resamples by drawing $n$ observations from the original dataset with replacement. For each bootstrap resample $X^{*b} = (X_1^{*b}, \\dots, X_n^{*b})$, where $b=1, \\dots, B$, we compute the estimator $\\hat{G}^{*b}$.\nThis process yields a distribution of bootstrap estimates $\\{\\hat{G}^{*1}, \\dots, \\hat{G}^{*B}\\}$. The bootstrap estimate of the standard error is the sample standard deviation of this distribution. As specified, we use the unbiased sample standard deviation:\n$$\n\\widehat{\\mathrm{SE}}_{\\text{boot}}(\\hat{G}) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B \\left(\\hat{G}^{*b} - \\bar{G}^{*}\\right)^2}\n$$\nwhere $\\bar{G}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{G}^{*b}$ is the mean of the bootstrap estimates.\n\n### 3. Comparison and Computational Procedure\n\nThe performance of the jackknife and bootstrap estimators is evaluated by comparing their estimates to the analytically derived true standard error. The metric for comparison is the absolute relative error:\n$$\n\\varepsilon = \\frac{\\left|\\widehat{\\mathrm{SE}} - \\mathrm{SE}_{\\text{true}}\\right|}{\\mathrm{SE}_{\\text{true}}}\n$$\nFor the case where $\\mathrm{SE}_{\\text{true}} = 0$ (i.e., $\\sigma=0$), this metric is undefined. In this circumstance, we use the absolute error $\\varepsilon = |\\widehat{\\mathrm{SE}} - 0| = |\\widehat{\\mathrm{SE}}|$.\n\nThe overall algorithm for each test case is as follows:\n1.  Generate a synthetic dataset of size $n$ from $\\mathrm{LogNormal}(\\mu, \\sigma^2)$ using the provided random seed.\n2.  Calculate $\\mathrm{SE}_{\\text{true}}$ using the derived formula with the known parameters $\\mu, \\sigma, n$.\n3.  Calculate $\\widehat{\\mathrm{SE}}_{\\text{jack}}$ from the generated data using the jackknife procedure.\n4.  Calculate $\\widehat{\\mathrm{SE}}_{\\text{boot}}$ from the generated data using the bootstrap procedure with $B$ replicates.\n5.  Compute the error metrics $\\varepsilon_{\\text{jack}}$ and $\\varepsilon_{\\text{boot}}$ and report the results.\n\nThis procedure rigorously tests the accuracy of the resampling methods against a known ground truth across a variety of conditions.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares true, jackknife, and bootstrap standard errors\n    for the geometric mean of log-normally distributed data.\n    \"\"\"\n    test_cases = [\n        # (n, mu, sigma, B, seed)\n        (50, 0.0, 0.5, 4000, 42),\n        (50, 0.0, 1.2, 6000, 314159),\n        (2, 0.0, 0.5, 2000, 7),\n        (100, 1.0, 0.7, 5000, 123456),\n        (30, -0.5, 0.0, 1000, 99)\n    ]\n\n    results = []\n\n    for n, mu, sigma, B, seed in test_cases:\n        # Seed the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate synthetic data from a log-normal distribution.\n        # np.random.lognormal takes mean and sigma of the underlying normal distribution.\n        X = rng.lognormal(mean=mu, sigma=sigma, size=n)\n        Y = np.log(X)\n\n        # 2. Compute the true standard error using the derived formula.\n        if sigma == 0.0:\n            se_true = 0.0\n        else:\n            # For numerical stability, np.expm1(x) computes exp(x) - 1.\n            term1 = np.exp(mu + (sigma**2) / (2 * n))\n            term2 = np.sqrt(np.expm1(sigma**2 / n))\n            se_true = term1 * term2\n\n        # 3. Compute the jackknife standard error.\n        # The estimator is the geometric mean, which is exp(mean(log(X))).\n        sum_Y = np.sum(Y)\n        G_hat_jack_samples = np.empty(n)\n        for i in range(n):\n            # Leave-one-out geometric mean estimate\n            G_hat_jack_samples[i] = np.exp((sum_Y - Y[i]) / (n - 1))\n        \n        # Standard jackknife variance formula\n        jack_var = ((n - 1) / n) * np.sum((G_hat_jack_samples - np.mean(G_hat_jack_samples))**2)\n        se_jack = np.sqrt(jack_var)\n\n        # 4. Compute the bootstrap standard error.\n        # Generate bootstrap indices to resample the original data.\n        # This is more memory-efficient than storing all bootstrap samples.\n        bootstrap_indices = rng.integers(0, n, size=(B, n))\n        bootstrap_samples_Y = Y[bootstrap_indices]\n        \n        # Compute the geometric mean for each bootstrap sample.\n        bootstrap_means_Y = np.mean(bootstrap_samples_Y, axis=1)\n        G_hat_boot_samples = np.exp(bootstrap_means_Y)\n\n        # Compute the unbiased sample standard deviation (ddof=1).\n        se_boot = np.std(G_hat_boot_samples, ddof=1)\n\n        # 5. Compute the error metrics.\n        if se_true == 0.0:\n            err_jack = np.abs(se_jack)\n            err_boot = np.abs(se_boot)\n        else:\n            err_jack = np.abs(se_jack - se_true) / se_true\n            err_boot = np.abs(se_boot - se_true) / se_true\n        \n        results.append(round(err_jack, 6))\n        results.append(round(err_boot, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2404364"}]}