{"hands_on_practices": [{"introduction": "At the heart of statistical mechanics lies the fundamental connection between the microscopic world of particles and the macroscopic properties we observe, like temperature and pressure. The concept of entropy, defined by Ludwig Boltzmann's famous equation $S = k_B \\ln \\Omega$, quantifies this link. This first practice problem provides a direct and foundational exercise in applying this principle by asking you to calculate the entropy for a specific arrangement of particles in a simplified gas model. By working through this, you will gain hands-on experience with the multinomial coefficient, a crucial tool for counting the number of microscopic arrangements, or microstates ($\\Omega$), that correspond to a single macroscopic state [@problem_id:1891788].", "problem": "Consider a simplified toy model for a classical gas composed of $N$ distinguishable, non-interacting particles. The particles are in a system where their momentum is quantized, meaning it can only take values from a discrete set of momentum states.\n\nA specific macroscopic state (macrostate) of this gas is defined by its distribution of particles among these momentum states. For the system in question, a measurement reveals a specific macrostate where exactly half of the particles ($N/2$) occupy a momentum state $p_1$, one-quarter of the particles ($N/4$) occupy a different momentum state $p_2$, and the remaining one-quarter of the particles ($N/4$) occupy a third momentum state $p_3$. All other possible momentum states are empty.\n\nAssume that the total number of particles $N$ is very large, which justifies the use of Stirling's approximation for the natural logarithm of a factorial: $\\ln(n!) \\approx n \\ln n - n$.\n\nBased on this information, determine the statistical entropy $S$ of this particular macrostate. Express your final answer as a closed-form analytic expression in terms of the total number of particles $N$ and the Boltzmann constant $k_B$.", "solution": "We use Boltzmann’s definition of entropy for a macrostate: $S = k_{B} \\ln \\Omega$, where $\\Omega$ is the number of microstates compatible with the given macrostate.\n\nFor $N$ distinguishable, non-interacting particles distributed among momentum states with occupation numbers $\\{n_{i}\\}$, the number of microstates is given by the multinomial count\n$$\n\\Omega = \\frac{N!}{\\prod_{i} n_{i}!}.\n$$\nIn this problem, only three momentum states are occupied with $n_{1} = N/2$, $n_{2} = N/4$, and $n_{3} = N/4$. Hence\n$$\n\\Omega = \\frac{N!}{\\left(\\frac{N}{2}\\right)!\\left(\\frac{N}{4}\\right)!\\left(\\frac{N}{4}\\right)!}.\n$$\nTherefore,\n$$\nS = k_{B} \\left[ \\ln N! - \\ln\\left(\\frac{N}{2}!\\right) - 2 \\ln\\left(\\frac{N}{4}!\\right) \\right].\n$$\nFor large $N$, apply Stirling’s approximation $\\ln(n!) \\approx n \\ln n - n$ to each factorial:\n$$\n\\ln N! \\approx N \\ln N - N,\n$$\n$$\n\\ln\\left(\\frac{N}{2}!\\right) \\approx \\frac{N}{2} \\ln\\left(\\frac{N}{2}\\right) - \\frac{N}{2},\n$$\n$$\n\\ln\\left(\\frac{N}{4}!\\right) \\approx \\frac{N}{4} \\ln\\left(\\frac{N}{4}\\right) - \\frac{N}{4}.\n$$\nSubstituting into $\\ln \\Omega$ gives\n$$\n\\ln \\Omega \\approx \\left(N \\ln N - N\\right) - \\left[\\frac{N}{2} \\ln\\left(\\frac{N}{2}\\right) - \\frac{N}{2}\\right] - 2\\left[\\frac{N}{4} \\ln\\left(\\frac{N}{4}\\right) - \\frac{N}{4}\\right].\n$$\nCollecting terms,\n$$\n\\ln \\Omega \\approx N \\ln N - \\frac{N}{2} \\ln\\left(\\frac{N}{2}\\right) - \\frac{N}{2} \\ln\\left(\\frac{N}{4}\\right),\n$$\nbecause the linear terms in $N$ cancel: $-N + \\frac{N}{2} + \\frac{N}{2} = 0$. Combine the logarithms:\n$$\n\\ln \\Omega \\approx N \\ln N - \\frac{N}{2} \\ln\\left(\\frac{N^{2}}{8}\\right) = N \\ln N - \\frac{N}{2}\\left(2 \\ln N - \\ln 8\\right) = \\frac{N}{2} \\ln 8.\n$$\nSince $\\ln 8 = 3 \\ln 2$, this is\n$$\n\\ln \\Omega \\approx \\frac{3N}{2} \\ln 2.\n$$\nThus the entropy is\n$$\nS = k_{B} \\ln \\Omega \\approx \\frac{3}{2} N k_{B} \\ln 2.\n$$\nThis is the closed-form analytic expression in terms of $N$ and $k_{B}$.", "answer": "$$\\boxed{\\frac{3}{2}\\,N\\,k_{B}\\,\\ln 2}$$", "id": "1891788"}, {"introduction": "Building on the basic counting of microstates, we now turn to a model that is fundamental to many areas of physics, from polymer science to magnetism: the one-dimensional random walk. This scenario, which models a chain of segments that can orient in one of two directions, is mathematically identical to a system of coin flips or magnetic spins. This exercise explores the configurational entropy associated with a specific macroscopic outcome—the total length of the polymer chain. This practice will solidify your understanding of how to use the binomial coefficient to determine multiplicity and appreciate how a simple statistical model can describe a wide range of physical phenomena [@problem_id:1891757].", "problem": "Consider a simplified one-dimensional model of a polymer chain consisting of $N$ rigid segments linked end-to-end. Each segment has an identical length $L$ and can be oriented either to the right or to the left along a straight line. The state of the entire chain is specified by the orientation of each of its $N$ segments. A macroscopic state, or \"macrostate,\" of the polymer is defined by its total end-to-end displacement from the origin.\n\nLet's assume the first segment starts at the origin. A particular macrostate is observed where the final end of the chain has a net displacement equivalent to $m$ segments to the right. This means the total end-to-end length is $mL$. It is given that $N$ and $m$ are integers of the same parity (both even or both odd), and $|m| \\leq N$.\n\nUsing the principles of statistical mechanics, determine the statistical entropy, $S$, of this specific macrostate. Your answer should be a single closed-form analytic expression in terms of the total number of segments $N$, the net displacement in segment units $m$, and the Boltzmann constant $k_B$.", "solution": "Each segment can point to the right or left along a line, which we denote by a variable $s_{i}=\\pm 1$ for $i=1,\\dots,N$. The end-to-end displacement in segment units is the sum $\\sum_{i=1}^{N}s_{i}$, and the given macrostate specifies\n$$\n\\sum_{i=1}^{N}s_{i}=m,\n$$\nwith $|m|\\leq N$ and $N$ and $m$ of the same parity so that this constraint is attainable.\n\nLet $n_{+}$ be the number of right-pointing segments ($s_{i}=+1$) and $n_{-}$ the number of left-pointing segments ($s_{i}=-1$). These satisfy\n$$\nn_{+}+n_{-}=N,\\qquad n_{+}-n_{-}=m.\n$$\nSolving these linear equations gives\n$$\nn_{+}=\\frac{N+m}{2},\\qquad n_{-}=\\frac{N-m}{2}.\n$$\nThe number of microstates (multiplicity) consistent with this macrostate is the number of ways to choose which $n_{+}$ segments point right among $N$ segments:\n$$\n\\Omega(N,m)=\\binom{N}{\\frac{N+m}{2}}.\n$$\nBy the Boltzmann definition of entropy,\n$$\nS=k_{B}\\ln\\Omega(N,m)=k_{B}\\ln\\!\\left[\\binom{N}{\\frac{N+m}{2}}\\right].\n$$\nThis expression is valid under the stated parity and bound conditions and depends only on $N$, $m$, and $k_{B}$, as required.", "answer": "$$\\boxed{k_{B}\\ln\\!\\left[\\binom{N}{\\frac{N+m}{2}}\\right]}$$", "id": "1891757"}, {"introduction": "After learning how to calculate the entropy for a given macrostate, a deeper question arises: how does entropy behave across the *entire* set of possible macrostates? This advanced problem guides you to one of the most profound insights of statistical mechanics: for any large system, the vast majority of microstates belong to a very narrow range of macrostates centered around the most probable one. By calculating the change in entropy for a state that deviates only slightly from the maximum, you will use powerful tools like Stirling's approximation to quantify the sharpness of the entropy peak. This result forms the statistical basis of the Second Law of Thermodynamics, explaining why macroscopic systems irreversibly evolve toward and remain in their state of maximum entropy [@problem_id:1891776].", "problem": "Consider a simplified model of a paramagnetic material consisting of $N$ distinguishable, non-interacting magnetic dipoles, where $N$ is a very large number. Each dipole moment can align either parallel ('spin-up') or anti-parallel ('spin-down') to an external axis, with equal intrinsic probability for each state. A macrostate of the system is specified by the number of spin-up dipoles, $n$.\n\nThe most probable macrostate is the one with the highest multiplicity (the greatest number of corresponding microscopic configurations), which occurs when the number of spin-up dipoles is $n_{max} = N/2$. For such a system, the statistical fluctuation in the number of spin-up dipoles is characterized by a standard deviation $\\sigma = \\sqrt{N}/2$.\n\nUsing the Boltzmann definition of entropy and applying Stirling's approximation for the logarithm of factorials of large numbers, determine the change in entropy, $\\Delta S = S_{max} - S_{dev}$. Here, $S_{max}$ is the entropy of the most probable macrostate, and $S_{dev}$ is the entropy of a macrostate where the number of spin-up dipoles deviates from the most probable value by one standard deviation, i.e., $n_{dev} = n_{max} + \\sigma$.\n\nProvide your answer as a closed-form analytic expression in terms of the Boltzmann constant, $k_B$.", "solution": "The problem asks for the difference in entropy between the most probable macrostate and a macrostate that deviates by one standard deviation.\n\nFirst, we define the entropy of a macrostate using the Boltzmann formula:\n$$S = k_B \\ln \\Omega$$\nwhere $k_B$ is the Boltzmann constant and $\\Omega$ is the multiplicity of the macrostate. For a system of $N$ distinguishable two-state particles, the multiplicity of a macrostate with $n$ particles in the 'spin-up' state is given by the binomial coefficient:\n$$\\Omega(N, n) = \\binom{N}{n} = \\frac{N!}{n!(N-n)!}$$\nThe entropy is therefore:\n$$S(N, n) = k_B \\ln\\left(\\frac{N!}{n!(N-n)!}\\right) = k_B (\\ln(N!) - \\ln(n!) - \\ln((N-n)!))$$\nSince $N$ and $n$ are very large, we can use Stirling's approximation for the natural logarithm of a factorial:\n$$\\ln(x!) \\approx x \\ln x - x$$\nApplying this approximation to our entropy equation:\n$$S(N, n) \\approx k_B [ (N \\ln N - N) - (n \\ln n - n) - ((N-n)\\ln(N-n) - (N-n)) ]$$\n$$S(N, n) \\approx k_B [ N \\ln N - n \\ln n - (N-n)\\ln(N-n) ]$$\nWe are interested in the behavior of the entropy near the most probable macrostate, where $n = n_{max} = N/2$. Let's analyze a state that deviates from this maximum by a small amount $\\delta$, such that $n = N/2 + \\delta$. Consequently, $N-n = N - (N/2 + \\delta) = N/2 - \\delta$. Substituting these into the entropy expression:\n$$S(N/2 + \\delta) \\approx k_B \\left[ N \\ln N - \\left(\\frac{N}{2} + \\delta\\right)\\ln\\left(\\frac{N}{2} + \\delta\\right) - \\left(\\frac{N}{2} - \\delta\\right)\\ln\\left(\\frac{N}{2} - \\delta\\right) \\right]$$\nWe can simplify the logarithm terms:\n$$\\ln\\left(\\frac{N}{2} + \\delta\\right) = \\ln\\left(\\frac{N}{2}\\left(1 + \\frac{2\\delta}{N}\\right)\\right) = \\ln\\left(\\frac{N}{2}\\right) + \\ln\\left(1 + \\frac{2\\delta}{N}\\right)$$\n$$\\ln\\left(\\frac{N}{2} - \\delta\\right) = \\ln\\left(\\frac{N}{2}\\left(1 - \\frac{2\\delta}{N}\\right)\\right) = \\ln\\left(\\frac{N}{2}\\right) + \\ln\\left(1 - \\frac{2\\delta}{N}\\right)$$\nSince $N$ is large, the deviation $\\delta$ is small compared to $N$, so $2\\delta/N \\ll 1$. We can use the Taylor expansion for the natural logarithm: $\\ln(1+x) \\approx x - x^2/2$ for small $x$.\n$$\\ln\\left(1 + \\frac{2\\delta}{N}\\right) \\approx \\frac{2\\delta}{N} - \\frac{1}{2}\\left(\\frac{2\\delta}{N}\\right)^2 = \\frac{2\\delta}{N} - \\frac{2\\delta^2}{N^2}$$\n$$\\ln\\left(1 - \\frac{2\\delta}{N}\\right) \\approx -\\frac{2\\delta}{N} - \\frac{1}{2}\\left(-\\frac{2\\delta}{N}\\right)^2 = -\\frac{2\\delta}{N} - \\frac{2\\delta^2}{N^2}$$\nNow, let's substitute these back into the entropy expression.\nFirst, for the term $\\left(\\frac{N}{2} + \\delta\\right)\\ln\\left(\\frac{N}{2} + \\delta\\right)$:\n$$\\left(\\frac{N}{2} + \\delta\\right)\\left[\\ln\\left(\\frac{N}{2}\\right) + \\frac{2\\delta}{N} - \\frac{2\\delta^2}{N^2}\\right] = \\frac{N}{2}\\ln\\left(\\frac{N}{2}\\right) + \\delta - \\frac{\\delta^2}{N} + \\delta\\ln\\left(\\frac{N}{2}\\right) + \\frac{2\\delta^2}{N} - \\dots$$\n$$ \\approx \\frac{N}{2}\\ln\\left(\\frac{N}{2}\\right) + \\delta\\ln\\left(\\frac{N}{2}\\right) + \\delta + \\frac{\\delta^2}{N}$$\nNext, for the term $\\left(\\frac{N}{2} - \\delta\\right)\\ln\\left(\\frac{N}{2} - \\delta\\right)$:\n$$\\left(\\frac{N}{2} - \\delta\\right)\\left[\\ln\\left(\\frac{N}{2}\\right) - \\frac{2\\delta}{N} - \\frac{2\\delta^2}{N^2}\\right] = \\frac{N}{2}\\ln\\left(\\frac{N}{2}\\right) - \\delta - \\frac{\\delta^2}{N} - \\delta\\ln\\left(\\frac{N}{2}\\right) + \\frac{2\\delta^2}{N} - \\dots$$\n$$ \\approx \\frac{N}{2}\\ln\\left(\\frac{N}{2}\\right) - \\delta\\ln\\left(\\frac{N}{2}\\right) - \\delta + \\frac{\\delta^2}{N}$$\nSumming these two terms:\n$$\\left(\\frac{N}{2} + \\delta\\right)\\ln\\left(\\frac{N}{2} + \\delta\\right) + \\left(\\frac{N}{2} - \\delta\\right)\\ln\\left(\\frac{N}{2} - \\delta\\right) \\approx N\\ln\\left(\\frac{N}{2}\\right) + \\frac{2\\delta^2}{N}$$\nNow, substitute this sum back into the full expression for entropy:\n$$S(N/2 + \\delta) \\approx k_B \\left[ N \\ln N - \\left(N\\ln\\left(\\frac{N}{2}\\right) + \\frac{2\\delta^2}{N}\\right) \\right]$$\n$$S(N/2 + \\delta) \\approx k_B \\left[ N \\ln N - N(\\ln N - \\ln 2) - \\frac{2\\delta^2}{N} \\right]$$\n$$S(N/2 + \\delta) \\approx k_B \\left[ N \\ln 2 - \\frac{2\\delta^2}{N} \\right]$$\nThis equation gives the entropy for a macrostate with a deviation $\\delta$ from the mean.\n\nThe entropy of the most probable macrostate, $S_{max}$, corresponds to $\\delta=0$:\n$$S_{max} = S(N/2) = k_B N \\ln 2$$\nThe problem specifies a deviation equal to the standard deviation, $\\sigma = \\sqrt{N}/2$. So, for the deviated state, we set $\\delta = \\sigma = \\sqrt{N}/2$. The entropy $S_{dev}$ is:\n$$S_{dev} = S(N/2 + \\sigma) \\approx k_B \\left[ N \\ln 2 - \\frac{2(\\sqrt{N}/2)^2}{N} \\right]$$\n$$S_{dev} \\approx k_B \\left[ N \\ln 2 - \\frac{2(N/4)}{N} \\right] = k_B \\left[ N \\ln 2 - \\frac{N/2}{N} \\right]$$\n$$S_{dev} \\approx k_B \\left[ N \\ln 2 - \\frac{1}{2} \\right]$$\nFinally, we calculate the required difference in entropy, $\\Delta S = S_{max} - S_{dev}$:\n$$\\Delta S = (k_B N \\ln 2) - \\left(k_B \\left[ N \\ln 2 - \\frac{1}{2} \\right]\\right)$$\n$$\\Delta S = k_B N \\ln 2 - k_B N \\ln 2 + \\frac{1}{2} k_B$$\n$$\\Delta S = \\frac{1}{2} k_B$$", "answer": "$$\\boxed{\\frac{1}{2}k_{B}}$$", "id": "1891776"}]}