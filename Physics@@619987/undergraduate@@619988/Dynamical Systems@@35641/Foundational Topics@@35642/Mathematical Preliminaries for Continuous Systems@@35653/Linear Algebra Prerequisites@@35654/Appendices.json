{"hands_on_practices": [{"introduction": "Understanding a dynamical system often begins with translating a physical process into the language of linear algebra. This exercise provides foundational practice in this translation by taking an intuitive geometric transformation—a rotation and scaling—and guiding you to construct the matrix that governs the system's evolution. By iterating this transformation [@problem_id:1690221], you will gain a concrete understanding of how a matrix repeatedly acts on a state vector to trace out a system's trajectory over time.", "problem": "Consider a two-dimensional discrete-time linear dynamical system whose state at time step $k$ is described by a vector $\\vec{v}_k = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}$. The evolution of the system from one step to the next is governed by a linear transformation $L$ such that $\\vec{v}_{k+1} = L(\\vec{v}_k)$. The transformation $L$ applied to any vector consists of two operations performed in sequence: first, a counter-clockwise rotation about the origin by an angle of 45 degrees, and second, a uniform expansion by a factor of $s = \\sqrt{2}$.\n\nIf the initial state of the system at $k=0$ is $\\vec{v}_0 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$, determine the components of the state vector $\\vec{v}_4$ after 4 iterations of the transformation. Present your answer as a row matrix containing the two components $(x_4, y_4)$.", "solution": "The problem asks for the state of a system, $\\vec{v}_4$, after four applications of a linear transformation $L$ on an initial state $\\vec{v}_0$. The relationship is given by $\\vec{v}_{k+1} = L(\\vec{v}_k)$, which can be represented by a matrix multiplication $\\vec{v}_{k+1} = M \\vec{v}_k$, where $M$ is the matrix corresponding to the transformation $L$.\n\nFirst, we must find the matrix $M$. The transformation $L$ is a composition of two operations: a rotation followed by a scaling. Let's find the matrix for each operation.\n\nThe first operation is a counter-clockwise rotation about the origin by an angle $\\theta = 45^\\circ$. The standard matrix for a counter-clockwise rotation by an angle $\\theta$ in a 2D plane is:\n$$\nR(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}\n$$\nFor $\\theta = 45^\\circ$, we have $\\cos(45^\\circ) = \\frac{\\sqrt{2}}{2} = \\frac{1}{\\sqrt{2}}$ and $\\sin(45^\\circ) = \\frac{\\sqrt{2}}{2} = \\frac{1}{\\sqrt{2}}$. Substituting these values gives the rotation matrix $R_{45}$:\n$$\nR_{45} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\n\nThe second operation is a uniform scaling by a factor of $s = \\sqrt{2}$. The matrix for such a scaling is $S_s = sI$, where $I$ is the identity matrix:\n$$\nS_{\\sqrt{2}} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & \\sqrt{2} \\end{pmatrix}\n$$\n\nThe total transformation $L$ is a composition of the rotation followed by the scaling. If $\\vec{v}$ is a vector, applying the rotation first gives $R_{45} \\vec{v}$. Applying the scaling to this result gives $S_{\\sqrt{2}} (R_{45} \\vec{v})$. Therefore, the matrix $M$ for the full transformation $L$ is the product of the individual matrices, with the matrix for the first operation on the right:\n$$\nM = S_{\\sqrt{2}} R_{45} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nM = \\begin{pmatrix} \\sqrt{2} \\cdot \\frac{1}{\\sqrt{2}} + 0 \\cdot \\frac{1}{\\sqrt{2}} & \\sqrt{2} \\cdot (-\\frac{1}{\\sqrt{2}}) + 0 \\cdot \\frac{1}{\\sqrt{2}} \\\\ 0 \\cdot \\frac{1}{\\sqrt{2}} + \\sqrt{2} \\cdot \\frac{1}{\\sqrt{2}} & 0 \\cdot (-\\frac{1}{\\sqrt{2}}) + \\sqrt{2} \\cdot \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n\nWe need to find the state vector $\\vec{v}_4$. Since $\\vec{v}_{k+1} = M \\vec{v}_k$, we have $\\vec{v}_1 = M \\vec{v}_0$, $\\vec{v}_2 = M \\vec{v}_1 = M(M \\vec{v}_0) = M^2 \\vec{v}_0$, and in general, $\\vec{v}_k = M^k \\vec{v}_0$. Thus, we need to compute $\\vec{v}_4 = M^4 \\vec{v}_0$.\n\nWe can calculate $M^4$ by repeated squaring:\n$$\nM^2 = M \\cdot M = \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + (-1)(1) & 1(-1) + (-1)(1) \\\\ 1(1) + 1(1) & 1(-1) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 0 & -2 \\\\ 2 & 0 \\end{pmatrix}\n$$\n$$\nM^4 = M^2 \\cdot M^2 = \\begin{pmatrix} 0 & -2 \\\\ 2 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -2 \\\\ 2 & 0 \\end{pmatrix} = \\begin{pmatrix} 0(0) + (-2)(2) & 0(-2) + (-2)(0) \\\\ 2(0) + 0(2) & 2(-2) + 0(0) \\end{pmatrix} = \\begin{pmatrix} -4 & 0 \\\\ 0 & -4 \\end{pmatrix}\n$$\nAlternatively, one can recognize that $M = s R(\\theta)$ and use the property that $(R(\\theta))^n = R(n\\theta)$.\n$M^4 = (S_{\\sqrt{2}} R_{45})^4 = (\\sqrt{2} I \\cdot R_{45})^4 = (\\sqrt{2})^4 (R_{45})^4$.\n$(\\sqrt{2})^4 = 4$.\n$(R_{45})^4 = R(4 \\times 45^\\circ) = R(180^\\circ)$.\nThe matrix for a $180^\\circ$ rotation is:\n$$\nR(180^\\circ) = \\begin{pmatrix} \\cos(180^\\circ) & -\\sin(180^\\circ) \\\\ \\sin(180^\\circ) & \\cos(180^\\circ) \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I\n$$\nSo, $M^4 = 4 \\cdot (-I) = -4I = \\begin{pmatrix} -4 & 0 \\\\ 0 & -4 \\end{pmatrix}$. This confirms the previous calculation.\n\nNow we can find $\\vec{v}_4$ by applying $M^4$ to the initial vector $\\vec{v}_0 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$:\n$$\n\\vec{v}_4 = M^4 \\vec{v}_0 = \\begin{pmatrix} -4 & 0 \\\\ 0 & -4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -4(2) + 0(0) \\\\ 0(2) + (-4)(0) \\end{pmatrix} = \\begin{pmatrix} -8 \\\\ 0 \\end{pmatrix}\n$$\nThe components of the state vector $\\vec{v}_4$ are $x_4 = -8$ and $y_4 = 0$. The problem asks for the answer as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} -8 & 0 \\end{pmatrix}}$$", "id": "1690221"}, {"introduction": "Once a system is described by a matrix, we can analyze its properties to predict important behaviors without simulating every step. This thought experiment explores the crucial concept of uniqueness, asking under what conditions two different initial states could merge into one, implying a loss of information about the system's past. Solving this problem [@problem_id:1690194] reveals the direct link between a fundamental matrix property, its determinant, and the system's ability to preserve information, a cornerstone of system analysis.", "problem": "A team of ecologists is studying a simplified model of population dynamics for three interacting species in a closed environment. The state of the system at any given month $k$ is described by a state vector $x_k = \\begin{pmatrix} P_k \\\\ M_k \\\\ C_k \\end{pmatrix}$, where $P_k$, $M_k$, and $C_k$ represent the populations of a prey species, a predator species, and a competing species, respectively. The evolution of the population from one month to the next is governed by the linear discrete-time system $x_{k+1} = A x_k$.\n\nThe evolution matrix $A$ has been partially determined from field data and is given by:\n$$ A = \\begin{pmatrix} 1.1 & -0.4 & -0.1 \\\\ 0.3 & 0.8 & 0 \\\\ \\alpha & 0 & 0.9 \\end{pmatrix} $$\nThe parameter $\\alpha$ represents a hypothesized interaction effect where the growth of the competing species is influenced by the prey population. Its value is currently unknown.\n\nA key theoretical question arises: under what condition could two different initial population states, say $x_0$ and $x'_0$, evolve into the exact same population state $x_1$ after just one month? Such a scenario would mean that observing the state at $k=1$ is not enough to uniquely determine the state at $k=0$, implying a loss of information in the system's evolution. For this phenomenon to be possible, the parameter $\\alpha$ must take a specific value.\n\nDetermine this specific numerical value of $\\alpha$.", "solution": "We seek a condition under which there exist two distinct initial states $x_{0}$ and $x'_{0}$ such that $A x_{0} = A x'_{0}$. This is equivalent to $A(x_{0} - x'_{0}) = 0$ with $x_{0} - x'_{0} \\neq 0$. Therefore, a necessary and sufficient condition is that the linear map $A$ is not injective, which holds if and only if the nullspace of $A$ is nontrivial, i.e., $\\det(A) = 0$.\n\nWith\n$$\nA = \\begin{pmatrix}\n1.1 & -0.4 & -0.1 \\\\\n0.3 & 0.8 & 0 \\\\\n\\alpha & 0 & 0.9\n\\end{pmatrix},\n$$\ncompute $\\det(A)$ by expansion along the third column:\n- The cofactor from $(1,3)$ is $(-1)^{1+3} a_{13} \\det\\begin{pmatrix} 0.3 & 0.8 \\\\ \\alpha & 0 \\end{pmatrix} = (-0.1)\\left(0.3\\cdot 0 - 0.8 \\alpha\\right) = 0.08\\alpha$.\n- The cofactor from $(2,3)$ is zero because $a_{23} = 0$.\n- The cofactor from $(3,3)$ is $(-1)^{3+3} a_{33} \\det\\begin{pmatrix} 1.1 & -0.4 \\\\ 0.3 & 0.8 \\end{pmatrix} = 0.9\\left(1.1\\cdot 0.8 - (-0.4)\\cdot 0.3\\right) = 0.9\\left(0.88 + 0.12\\right) = 0.9$.\n\nThus,\n$$\n\\det(A) = 0.08\\alpha + 0.9.\n$$\nFor non-injectivity, set $\\det(A) = 0$:\n$$\n0.08\\alpha + 0.9 = 0 \\quad \\Longrightarrow \\quad \\alpha = -\\frac{0.9}{0.08}.\n$$\nExpressing in exact fractional form, $0.9 = \\frac{9}{10}$ and $0.08 = \\frac{8}{100} = \\frac{2}{25}$, so\n$$\n\\alpha = -\\frac{9/10}{8/100} = -\\frac{9}{10}\\cdot\\frac{100}{8} = -\\frac{90}{8} = -\\frac{45}{4}.\n$$\nTherefore, the unique value of $\\alpha$ that makes two different initial states map to the same next state after one step is $\\alpha = -\\frac{45}{4}$.", "answer": "$$\\boxed{-\\frac{45}{4}}$$", "id": "1690194"}, {"introduction": "Linear algebra offers more than just basic operations; it contains powerful theorems that reveal deep structural properties of matrices and the systems they represent. This problem introduces the Cayley-Hamilton theorem not as an abstract fact, but as a practical tool for calculating the inverse of a system's evolution matrix. By expressing the inverse matrix as a polynomial of the original [@problem_id:1690201], you will uncover an elegant method for defining the time-reversed dynamics of a discrete system.", "problem": "Consider a discrete-time linear dynamical system whose state vector $x_k \\in \\mathbb{R}^3$ at integer time step $k$ evolves according to the equation $x_{k+1} = Ax_k$. The system is governed by the constant matrix $A$ given by:\n$$A = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 1 & 1 & 1 \\end{pmatrix}$$\nThe time-reversed dynamics of this system, which describe how to determine the past state $x_k$ from a future state $x_{k+1}$, are given by an equation of the form $x_k = Bx_{k+1}$. It can be shown that the matrix $B$ for the time-reversed system is expressible as a quadratic polynomial in the matrix $A$:\n$$B = c_2 A^2 + c_1 A + c_0 I$$\nwhere $I$ is the $3 \\times 3$ identity matrix and $c_2, c_1, c_0$ are scalar coefficients.\n\nDetermine the values of the coefficients $(c_2, c_1, c_0)$. Present your answer as a single row matrix $\\begin{pmatrix} c_2 & c_1 & c_0 \\end{pmatrix}$ using exact fractions.", "solution": "We seek the time-reversal matrix $B$ such that $x_{k} = B x_{k+1}$ for the system $x_{k+1} = A x_{k}$, hence $B = A^{-1}$. For a $3 \\times 3$ matrix $A$ with characteristic polynomial\n$$\np(\\lambda) = \\det(\\lambda I - A) = \\lambda^{3} - s_{1} \\lambda^{2} + s_{2} \\lambda - s_{3},\n$$\nthe Cayley-Hamilton theorem gives\n$$\nA^{3} - s_{1} A^{2} + s_{2} A - s_{3} I = 0.\n$$\nAssuming $A$ is invertible (which we will verify), we multiply by $A^{-1}$ to obtain\n$$\nA^{2} - s_{1} A + s_{2} I - s_{3} A^{-1} = 0,\n$$\nhence\n$$\nA^{-1} = \\frac{1}{s_{3}} \\left( A^{2} - s_{1} A + s_{2} I \\right).\n$$\nTherefore, the coefficients in $B = c_{2} A^{2} + c_{1} A + c_{0} I$ are $c_{2} = \\frac{1}{s_{3}}$, $c_{1} = -\\frac{s_{1}}{s_{3}}$, and $c_{0} = \\frac{s_{2}}{s_{3}}$, where $s_{1} = \\operatorname{tr}(A)$, $s_{2} = \\frac{1}{2}\\left((\\operatorname{tr} A)^{2} - \\operatorname{tr}(A^{2})\\right)$, and $s_{3} = \\det(A)$.\n\nWe now compute $s_{1}$, $s_{2}$, and $s_{3}$ for\n$$\nA = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 1 & 1 & 1 \\end{pmatrix}.\n$$\nFirst, the trace is\n$$\ns_{1} = \\operatorname{tr}(A) = 1 + 1 + 1 = 3.\n$$\nNext, we compute $A^{2}$ to obtain $\\operatorname{tr}(A^{2})$:\n$$\nA^{2} = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 1 & 1 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 1 & 1 & 1 \\end{pmatrix}\n= \\begin{pmatrix} 1 & 4 & 4 \\\\ 2 & 3 & 4 \\\\ 2 & 4 & 3 \\end{pmatrix},\n$$\nso\n$$\n\\operatorname{tr}(A^{2}) = 1 + 3 + 3 = 7.\n$$\nThus, we find $s_2$:\n$$\ns_{2} = \\frac{1}{2}\\left((\\operatorname{tr} A)^{2} - \\operatorname{tr}(A^{2})\\right) = \\frac{1}{2}\\left(3^{2} - 7\\right) = \\frac{1}{2}\\left(9 - 7\\right) = 1.\n$$\nFinally, we compute the determinant, which is $s_3$:\n$$\ns_{3} = \\det(A) = 1(1\\cdot 1 - 2\\cdot 1) - 2(0\\cdot 1 - 2\\cdot 1) + 0 = 1(-1) - 2(-2) = 3.\n$$\nSince $s_3 = \\det(A) = 3 \\neq 0$, the matrix is invertible.\nThe coefficients are:\n$$ c_2 = \\frac{1}{s_3} = \\frac{1}{3} $$\n$$ c_1 = -\\frac{s_1}{s_3} = -\\frac{3}{3} = -1 $$\n$$ c_0 = \\frac{s_2}{s_3} = \\frac{1}{3} $$\nTherefore, the coefficients are $c_2 = \\frac{1}{3}$, $c_1 = -1$, and $c_0 = \\frac{1}{3}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} & -1 & \\frac{1}{3} \\end{pmatrix}}$$", "id": "1690201"}]}