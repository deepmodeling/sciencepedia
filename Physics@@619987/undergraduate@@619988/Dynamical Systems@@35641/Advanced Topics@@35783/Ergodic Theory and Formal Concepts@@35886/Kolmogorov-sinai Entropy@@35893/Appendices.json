{"hands_on_practices": [{"introduction": "The Kolmogorov-Sinai (KS) entropy quantifies the unpredictability of a dynamical system. To build intuition, we start with the simplest case: a sequence of random events. This exercise [@problem_id:1688717] invites you to compare the KS entropy of a fair coin toss with a biased one, directly applying the Shannon entropy formula. By doing so, you will develop a concrete understanding of how entropy measures information and how it changes with the underlying probabilities of the system.", "problem": "In the study of dynamical systems, the Kolmogorov-Sinai (KS) entropy is a fundamental concept that quantifies the rate of information generation, or the level of unpredictability, of a system over time. For a simple stochastic process consisting of a sequence of independent and identically distributed (i.i.d.) trials, the KS entropy, $h_{KS}$, measured in bits per trial, is given by the Shannon entropy formula:\n$$h_{KS} = - \\sum_{i=1}^{N} p_i \\log_2(p_i)$$\nwhere $N$ is the number of possible outcomes for each trial and $p_i$ is the probability of the $i$-th outcome.\n\nConsider two such processes, each with two possible outcomes, which we can label 'heads' (H) and 'tails' (T).\n\n1.  A **fair process**, where the probabilities of the two outcomes are equal: $p_H = 0.5$ and $p_T = 0.5$.\n2.  A **biased process**, where the probability of 'heads' is $p'_H = 0.9$ and the probability of 'tails' is $p'_T = 0.1$.\n\nCalculate the numerical ratio of the KS entropy of the biased process to the KS entropy of the fair process. Round your final answer to three significant figures.", "solution": "We are given the Kolmogorov-Sinai (KS) entropy for an i.i.d. process in bits per trial as the Shannon entropy\n$$\nh_{KS} = - \\sum_{i=1}^{N} p_{i} \\log_{2}(p_{i}).\n$$\nFor the fair process with $p_{H} = \\frac{1}{2}$ and $p_{T} = \\frac{1}{2}$, the entropy is\n$$\nh_{\\text{fair}} = -\\left(\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2} + \\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\right) = -\\log_{2}\\tfrac{1}{2} = 1,\n$$\nsince $\\log_{2}\\tfrac{1}{2} = -1$.\n\nFor the biased process with $p'_{H} = 0.9$ and $p'_{T} = 0.1$, the entropy is\n$$\nh_{\\text{biased}} = -\\left(0.9\\,\\log_{2}(0.9) + 0.1\\,\\log_{2}(0.1)\\right).\n$$\nUsing the change-of-base identity $\\log_{2}(x) = \\frac{\\ln(x)}{\\ln(2)}$ to evaluate numerically,\n$$\n\\log_{2}(0.9) \\approx -0.152003093,\\qquad \\log_{2}(0.1) \\approx -3.321928095,\n$$\nso\n$$\nh_{\\text{biased}} \\approx -\\left(0.9 \\cdot (-0.152003093) + 0.1 \\cdot (-3.321928095)\\right) \\approx 0.468995594.\n$$\nThe required ratio is\n$$\n\\frac{h_{\\text{biased}}}{h_{\\text{fair}}} = \\frac{0.468995594}{1} \\approx 0.468995594.\n$$\nRounding to three significant figures gives $0.469$.", "answer": "$$\\boxed{0.469}$$", "id": "1688717"}, {"introduction": "Chaotic systems are characterized by their sensitive dependence on initial conditions, which manifests as the exponential separation of nearby trajectories. This geometric stretching is directly linked to the generation of information. This problem [@problem_id:1688770] uses a simplified model of fluid mixing to explore the connection between the rate of phase space expansion (quantified by Lyapunov exponents) and the KS entropy. Solving it will help you understand Pesin's identity, a key theorem that elegantly bridges the geometric dynamics of chaos with the information-theoretic concept of entropy.", "problem": "Consider a simplified two-dimensional model for the mixing of a fluid within a square container. The state of any fluid particle is represented by a point in the unit square, $[0, 1] \\times [0, 1]$. The dynamics are observed at discrete time intervals. After one time step, an infinitesimally small rectangular region of the fluid, initially with sides aligned with the coordinate axes, is transformed into a new rectangular region. This transformation uniformly stretches the region's dimension along the x-axis by a factor of $\\sigma = 3$ and compresses its dimension along the y-axis by a factor of $1/\\sigma = 1/3$. This stretching and compression behavior is the same at every point in the unit square and for every time step. The system is area-preserving.\n\nFor such a system, the rate of information generation due to its chaotic behavior is quantified by the Kolmogorov-Sinai (KS) entropy per iteration. Calculate the exact value of the KS entropy, $h_{KS}$, for this dynamical system. Express your answer as an analytic expression.", "solution": "We consider a smooth, invertible, area-preserving map on the unit square that uniformly stretches by a factor $\\sigma$ along the $x$-axis and compresses by a factor $1/\\sigma$ along the $y$-axis at each iteration. The local linearization (Jacobian) of the map at any point is\n$$\nJ=\\begin{pmatrix}\n\\sigma & 0\\\\\n0 & \\frac{1}{\\sigma}\n\\end{pmatrix},\n$$\nwhich satisfies $\\det J=\\sigma\\cdot \\frac{1}{\\sigma}=1$, confirming area preservation.\n\nThe Lyapunov exponents are given by the logarithms of the moduli of the eigenvalues (equivalently, singular values) of $J$. Since the map is uniform in space and time, these exponents are constant and equal to\n$$\n\\lambda_{1}=\\ln \\sigma,\\qquad \\lambda_{2}=-\\ln \\sigma.\n$$\nBy Pesin's identity for smooth invertible maps preserving an SRB (in this case Lebesgue) measure, the Kolmogorov-Sinai entropy equals the sum of the positive Lyapunov exponents:\n$$\nh_{KS}=\\sum_{\\lambda_{i}>0}\\lambda_{i}=\\ln \\sigma.\n$$\nSubstituting the given $\\sigma=3$, we obtain\n$$\nh_{KS}=\\ln 3.\n$$\nThis is the KS entropy per iteration.", "answer": "$$\\boxed{\\ln 3}$$", "id": "1688770"}, {"introduction": "The KS entropy is defined as a *rate* of information generation per unit of time. But what happens if we change our definition of a 'time step' by observing the system less frequently? This thought experiment [@problem_id:1688734] explores the fundamental scaling property of KS entropy by considering an iterated map. Determining how the entropy changes will solidify your understanding of it as a rate and reveal a simple but powerful relationship for analyzing dynamical systems over different time scales.", "problem": "A deep-space probe is monitoring a complex, periodic signal emanating from a newly discovered celestial object. The state of the signal at discrete time intervals is described by a variable $x_n$ in a state space $\\mathcal{X}$, where $n$ is an integer representing the time step. The evolution of the system is governed by a deterministic, chaotic map $T: \\mathcal{X} \\to \\mathcal{X}$, such that $x_{n+1} = T(x_n)$.\n\nAfter extensive analysis of the full data stream $x_0, x_1, x_2, \\ldots$, mission scientists have calculated the Kolmogorov-Sinai (KS) entropy for this system to be $h_{KS}(T) = H_0$. The KS entropy measures the rate of information generation, or the unpredictability, of the dynamical system.\n\nUnfortunately, a glitch in the probe's data transmission system causes a data loss. Instead of receiving the full sequence of states, the ground station now only receives every second state, yielding the sequence $x_0, x_2, x_4, \\ldots$. This effectively creates a new dynamical system for the observers on the ground, where the state evolves from one observed point to the next.\n\nLet the map governing this new, observed system be $T_{obs}$. Determine the KS entropy of this observed system, $h_{KS}(T_{obs})$, in terms of the original entropy $H_0$.", "solution": "We model the full system as a measure-preserving transformation $(\\mathcal{X},\\mathcal{B},\\mu,T)$ on a probability space, with Kolmogorov-Sinai entropy $h_{KS}(T)=H_{0}$. Due to observing every second state, the observed dynamics is generated by the second iterate $T_{obs}=T^{2}$.\n\nWe use the standard definition of KS entropy via finite measurable partitions. For a finite partition $\\mathcal{P}$, define\n$$\nh_{\\mu}(T,\\mathcal{P})=\\lim_{n\\to\\infty}\\frac{1}{n}H\\!\\left(\\bigvee_{i=0}^{n-1}T^{-i}\\mathcal{P}\\right),\n$$\nand\n$$\nh_{KS}(T)=\\sup_{\\mathcal{P}}h_{\\mu}(T,\\mathcal{P}),\n$$\nwhere $H(\\cdot)$ is the Shannon entropy and $\\bigvee$ denotes the join of partitions.\n\nFix $k\\in\\mathbb{N}$ and any finite partition $\\mathcal{P}$. Set\n$$\n\\mathcal{Q}=\\bigvee_{r=0}^{k-1}T^{-r}\\mathcal{P}.\n$$\nThen\n$$\n\\bigvee_{j=0}^{n-1}(T^{k})^{-j}\\mathcal{Q}\n=\\bigvee_{j=0}^{n-1}\\bigvee_{r=0}^{k-1}T^{-kj-r}\\mathcal{P}\n=\\bigvee_{i=0}^{kn-1}T^{-i}\\mathcal{P}.\n$$\nTaking entropies and limits yields\n$$\nh_{\\mu}(T^{k},\\mathcal{Q})\n=\\lim_{n\\to\\infty}\\frac{1}{n}H\\!\\left(\\bigvee_{i=0}^{kn-1}T^{-i}\\mathcal{P}\\right)\n=k\\lim_{m\\to\\infty}\\frac{1}{m}H\\!\\left(\\bigvee_{i=0}^{m-1}T^{-i}\\mathcal{P}\\right)\n=kh_{\\mu}(T,\\mathcal{P}),\n$$\nwhere we set $m=kn$.\n\nTaking the supremum over $\\mathcal{P}$ gives\n$$\nh_{KS}(T^{k})\\geq k\\,h_{KS}(T).\n$$\nFor the reverse inequality, for any finite partition $\\mathcal{R}$,\n$$\nh_{\\mu}(T,\\mathcal{R})\n=\\frac{1}{k}h_{\\mu}\\!\\left(T^{k},\\bigvee_{r=0}^{k-1}T^{-r}\\mathcal{R}\\right)\n\\leq \\frac{1}{k}h_{KS}(T^{k}),\n$$\nand taking the supremum over $\\mathcal{R}$ implies\n$$\nh_{KS}(T)\\leq \\frac{1}{k}h_{KS}(T^{k}).\n$$\nCombining both inequalities yields the standard scaling law\n$$\nh_{KS}(T^{k})=k\\,h_{KS}(T).\n$$\n\nApplying this with $k=2$ to the observed map $T_{obs}=T^{2}$ gives\n$$\nh_{KS}(T_{obs})=h_{KS}(T^{2})=2\\,h_{KS}(T)=2H_{0}.\n$$", "answer": "$$\\boxed{2H_{0}}$$", "id": "1688734"}]}