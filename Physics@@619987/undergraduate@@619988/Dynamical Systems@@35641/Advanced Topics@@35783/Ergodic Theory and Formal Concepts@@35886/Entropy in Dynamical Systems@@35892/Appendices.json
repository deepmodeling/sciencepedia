{"hands_on_practices": [{"introduction": "Understanding the complexity of a dynamical system often begins with counting its possible trajectories. This exercise introduces topological entropy in the context of a symbolic system, which provides a simplified yet powerful model for real-world phenomena like neural activity. By translating the system's rules into a mathematical structure, you will calculate the exponential growth rate of distinct state sequences, offering a precise measure of its dynamic richness and revealing a surprising connection to the golden ratio $\\frac{1+\\sqrt{5}}{2}$. [@problem_id:1674470]", "problem": "Consider a simplified model of a single neuron's activity over discrete time steps $t=0, 1, 2, \\dots$. The neuron can be in one of two states at any given time: 'Quiescent' (Q) or 'Firing' (F). The sequence of states over time constitutes a symbolic dynamical system governed by the following biophysical constraints:\n\n1.  A neuron in the 'Firing' (F) state has just produced an action potential and must enter a mandatory refractory period. Therefore, a 'Firing' state must always be followed by a 'Quiescent' (Q) state in the next time step.\n2.  A neuron in the 'Quiescent' (Q) state is ready to fire. It can either remain 'Quiescent' (Q) or transition to a 'Firing' (F) state in the next time step.\n\nAny sequence of states that violates these rules is considered impossible. The topological entropy of this system quantifies the complexity of the possible activity patterns, specifically the exponential growth rate of the number of unique, allowed sequences of states.\n\nCalculate the topological entropy of this system. Provide your answer as an exact analytic expression.", "solution": "Let the state set be $\\{Q,F\\}$. The biophysical constraints imply the following allowable one-step transitions: from $Q$ to $Q$ or $F$, and from $F$ only to $Q$. This defines a subshift of finite type with adjacency matrix\n$$\nA=\\begin{pmatrix}\n1 & 1\\\\\n1 & 0\n\\end{pmatrix},\n$$\nwhere rows index the current state $(Q,F)$ and columns the next state $(Q,F)$.\n\nFor a subshift of finite type with adjacency matrix $A$, the number of allowed words of length $n$ grows asymptotically like $\\rho(A)^{n}$, where $\\rho(A)$ is the spectral radius of $A$. The topological entropy is the exponential growth rate of the number of admissible words, therefore\n$$\nh_{\\text{top}}=\\ln\\big(\\rho(A)\\big).\n$$\nTo compute $\\rho(A)$, find the eigenvalues of $A$ from the characteristic polynomial:\n$$\np(\\lambda)=\\det(A-\\lambda I)=\\det\\begin{pmatrix}1-\\lambda & 1\\\\ 1 & -\\lambda\\end{pmatrix}=(1-\\lambda)(-\\lambda)-1=\\lambda^{2}-\\lambda-1.\n$$\nThe roots of $p(\\lambda)=0$ are\n$$\n\\lambda=\\frac{1\\pm \\sqrt{1+4}}{2}=\\frac{1\\pm \\sqrt{5}}{2}.\n$$\nThus the spectral radius is the Perron eigenvalue\n$$\n\\rho(A)=\\frac{1+\\sqrt{5}}{2}.\n$$\nTherefore, the topological entropy is\n$$\nh_{\\text{top}}=\\ln\\!\\left(\\frac{1+\\sqrt{5}}{2}\\right).\n$$\nAs a consistency check, the constraint “$F$ must be followed by $Q$” is equivalent to forbidding consecutive $F$ symbols, so the number of admissible length-$n$ words satisfies the Fibonacci recursion, implying exponential growth with base $\\frac{1+\\sqrt{5}}{2}$ and hence the same entropy.", "answer": "$$\\boxed{\\ln\\left(\\frac{1+\\sqrt{5}}{2}\\right)}$$", "id": "1674470"}, {"introduction": "While symbolic systems offer a clear way to count configurations, many dynamical systems evolve over a continuous state space like the interval $[0, 1]$. This practice extends the concept of topological entropy to an interval map, revealing a powerful geometric intuition: complexity arises from the map's stretching and folding action. You will learn to calculate entropy by identifying how many segments of the domain are expanded to cover the entire range of possible states, a key indicator of chaotic behavior. [@problem_id:1674436]", "problem": "In the study of one-dimensional dynamical systems, topological entropy is a non-negative real number that serves as a measure of the complexity of the system. For a certain class of piecewise-defined maps $T: [0, 1] \\to [0, 1]$, the topological entropy, denoted $h_{top}(T)$, can be determined by a straightforward method. Specifically, if the domain $[0,1]$ can be partitioned into a finite number of subintervals on which the map acts in a simple way, the entropy is given by $h_{top}(T) = \\ln(k)$. Here, $k$ is the total count of those subintervals from the partition whose image under the map $T$ is the entire interval $[0, 1]$.\n\nConsider the following map $T$, which is defined on the interval $[0, 1]$ and maps it to itself:\n$$ T(x) = \\begin{cases} 3x & \\text{if } 0 \\le x \\le 1/3 \\\\ 1/2 & \\text{if } 1/3 < x < 2/3 \\\\ 3x-2 & \\text{if } 2/3 \\le x \\le 1 \\end{cases} $$\nCalculate the topological entropy $h_{top}(T)$ for this map. Present your answer as a single closed-form analytic expression.", "solution": "The problem asks for the topological entropy $h_{top}(T)$ of a given piecewise map $T: [0, 1] \\to [0, 1]$. We are provided with a specific formula for this class of maps: $h_{top}(T) = \\ln(k)$, where $k$ is the number of subintervals in the domain partition whose image under $T$ is the entire interval $[0, 1]$.\n\nThe map is defined as:\n$$ T(x) = \\begin{cases} 3x & \\text{if } 0 \\le x \\le 1/3 \\\\ 1/2 & \\text{if } 1/3 < x < 2/3 \\\\ 3x-2 & \\text{if } 2/3 \\le x \\le 1 \\end{cases} $$\n\nThe domain $[0, 1]$ is naturally partitioned by this definition into three subintervals:\n1.  $I_1 = [0, 1/3]$\n2.  $I_2 = (1/3, 2/3)$\n3.  $I_3 = [2/3, 1]$\n\nWe need to find the image of each of these subintervals under the map $T$ to determine the value of $k$.\n\nFor the first subinterval, $I_1 = [0, 1/3]$, the map is $T(x) = 3x$. This is a linear function. To find the image of the interval, we can evaluate the function at the endpoints of the interval.\nAt $x = 0$, $T(0) = 3(0) = 0$.\nAt $x = 1/3$, $T(1/3) = 3(1/3) = 1$.\nSince $T(x)$ is continuous and increasing on this interval, the image of $I_1$ is the interval $[T(0), T(1/3)] = [0, 1]$.\nSo, the image of the first subinterval is the entire interval $[0, 1]$. This subinterval contributes to the count $k$.\n\nFor the second subinterval, $I_2 = (1/3, 2/3)$, the map is the constant function $T(x) = 1/2$. The image of this entire subinterval is the single point $\\{1/2\\}$. This is not the interval $[0, 1]$, so this subinterval does not contribute to the count $k$.\n\nFor the third subinterval, $I_3 = [2/3, 1]$, the map is $T(x) = 3x - 2$. This is also a linear function. We evaluate the function at the endpoints of the interval.\nAt $x = 2/3$, $T(2/3) = 3(2/3) - 2 = 2 - 2 = 0$.\nAt $x = 1$, $T(1) = 3(1) - 2 = 1$.\nSince $T(x)$ is continuous and increasing on this interval, the image of $I_3$ is the interval $[T(2/3), T(1)] = [0, 1]$.\nSo, the image of the third subinterval is also the entire interval $[0, 1]$. This subinterval also contributes to the count $k$.\n\nNow we count the number of subintervals, $k$, whose image is $[0, 1]$. We found that the subintervals $I_1 = [0, 1/3]$ and $I_3 = [2/3, 1]$ both map onto $[0, 1]$. Therefore, the value of $k$ is 2.\n\nUsing the formula provided in the problem statement, the topological entropy is:\n$h_{top}(T) = \\ln(k) = \\ln(2)$.", "answer": "$$\\boxed{\\ln(2)}$$", "id": "1674436"}, {"introduction": "Topological entropy catalogues all possible behaviors but does not distinguish between the likely and the rare. Metric entropy provides this crucial refinement by incorporating probabilities, an idea with deep roots in information theory. This hands-on problem uses a practical data transmission scenario to illustrate the concept, asking you to quantify the \"wasted\" information from an inefficient encoding scheme, thereby providing a tangible meaning to the entropy of a process. [@problem_id:1674450]", "problem": "A deep-space probe is performing spectral analysis on the atmosphere of a newly discovered exoplanet. The probe's primary sensor detects a continuous stream of particles corresponding to one of three atmospheric gases, which mission scientists have named Azurine (A), Beryl-Hydride (B), and Carbyl (C).\n\nFor the initial design of the data compression and transmission system, engineers made a simplifying assumption that each gas would be detected with equal probability. This assumption formed the basis of an optimal encoding scheme intended to minimize the average number of bits transmitted per detection.\n\nHowever, after a year of data collection, analysis reveals a different reality. The long-term empirical probabilities of detecting each gas are found to be $p_A = 0.900$, $p_B = 0.050$, and $p_C = 0.050$. The probe continues to use the original encoding scheme based on the outdated equal-probability model. This mismatch results in an inefficient use of the communication channel.\n\nYour task is to quantify this inefficiency. Calculate the average number of \"wasted\" bits per symbol transmitted. This value is defined as the difference between the average number of bits per symbol required by the original, suboptimal encoding scheme (when applied to the actual data stream) and the theoretical minimum average number of bits per symbol that would be required by a new encoding scheme perfectly optimized for the empirical probabilities.\n\nProvide your answer in units of bits per symbol, rounded to three significant figures. All calculations should use the base-2 logarithm.", "solution": "Let the true symbol distribution be $p = (p_{A},p_{B},p_{C}) = (0.900,0.050,0.050)$ and the model used to design the original code be the uniform $q = (q_{A},q_{B},q_{C}) = \\left(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right)$.\n\nAn optimal code for the model $q$ assigns code lengths $l_{i} = -\\log_{2}(q_{i})$, so the average length under the true distribution $p$ (the cross-entropy) is\n$$\nL_{\\text{old}} = -\\sum_{i} p_{i}\\log_{2}(q_{i}) \\, .\n$$\nFor $q_{i} = \\frac{1}{3}$, this simplifies to\n$$\nL_{\\text{old}} = -\\sum_{i} p_{i}\\log_{2}\\!\\left(\\tfrac{1}{3}\\right) = \\left(\\sum_{i} p_{i}\\right)\\log_{2}(3) = \\log_{2}(3) \\, .\n$$\n\nThe theoretical minimum average length achievable with a code optimized for $p$ is the entropy\n$$\nH(p) = -\\sum_{i} p_{i}\\log_{2}(p_{i}) \\, .\n$$\nTherefore, the wasted bits per symbol are\n$$\nW = L_{\\text{old}} - H(p) = \\log_{2}(3) + \\sum_{i} p_{i}\\log_{2}(p_{i})\n= \\sum_{i} p_{i}\\log_{2}\\!\\left(\\frac{p_{i}}{q_{i}}\\right) \\, .\n$$\nNow compute $H(p)$:\n$$\nH(p) = -\\left[0.900\\,\\log_{2}(0.900) + 0.050\\,\\log_{2}(0.050) + 0.050\\,\\log_{2}(0.050)\\right] \\, .\n$$\nUsing base-2 logarithms,\n$$\n\\log_{2}(0.900) \\approx -0.152003093,\\quad \\log_{2}(0.050) \\approx -4.321928095 \\, ,\n$$\nso\n$$\nH(p) \\approx -\\left[0.900(-0.152003093) + 0.050(-4.321928095) + 0.050(-4.321928095)\\right]\n= 0.568995594 \\text{ bits/symbol} \\, .\n$$\nThus,\n$$\nW = \\log_{2}(3) - H(p) \\approx 1.584962501 - 0.568995594 = 1.015966907 \\text{ bits/symbol} \\, .\n$$\nRounded to three significant figures, the wasted bits per symbol are $1.02$.", "answer": "$$\\boxed{1.02}$$", "id": "1674450"}]}