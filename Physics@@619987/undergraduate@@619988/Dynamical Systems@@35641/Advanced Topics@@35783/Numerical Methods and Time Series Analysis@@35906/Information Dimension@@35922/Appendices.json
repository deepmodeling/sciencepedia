{"hands_on_practices": [{"introduction": "To understand the information dimension, we first need to master its fundamental component: Shannon entropy. This exercise provides a direct calculation of the Shannon information content, $I$, for a system with a non-uniform probability distribution across different regions, a common scenario in dynamical systems. By working through this foundational problem [@problem_id:1684817], you will build a concrete understanding of how the probability measure of an attractor determines its information content.", "problem": "Consider a one-dimensional dynamical system whose attractor is probed using a set of measurement partitions. At a specific coarse-graining level with resolution $\\epsilon$, the attractor is found to occupy four distinct, non-overlapping intervals of this size. The natural measure of the system, which represents the long-term probability of finding a trajectory in a given region, assigns the probabilities $P_1 = \\frac{1}{2}$, $P_2 = \\frac{1}{4}$, $P_3 = \\frac{1}{8}$, and $P_4 = \\frac{1}{8}$ to these four intervals respectively. Calculate the Shannon information content, $I$, associated with this partition. Express your answer as a single closed-form analytic expression.", "solution": "The Shannon information content (entropy) of a partition with probabilities $\\{P_{i}\\}$ is defined by\n$$\nI=-\\sum_{i} P_{i}\\ln P_{i}.\n$$\nWith $P_{1}=\\frac{1}{2}$, $P_{2}=\\frac{1}{4}$, $P_{3}=\\frac{1}{8}$, and $P_{4}=\\frac{1}{8}$, compute each term:\n$$\nP_{1}\\ln P_{1}=\\frac{1}{2}\\ln\\!\\left(\\frac{1}{2}\\right)=\\frac{1}{2}(-\\ln 2)=-\\frac{1}{2}\\ln 2,\n$$\n$$\nP_{2}\\ln P_{2}=\\frac{1}{4}\\ln\\!\\left(\\frac{1}{4}\\right)=\\frac{1}{4}(-2\\ln 2)=-\\frac{1}{2}\\ln 2,\n$$\n$$\nP_{3}\\ln P_{3}=\\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)=\\frac{1}{8}(-3\\ln 2)=-\\frac{3}{8}\\ln 2,\n$$\n$$\nP_{4}\\ln P_{4}=\\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)=-\\frac{3}{8}\\ln 2.\n$$\nSumming,\n$$\n\\sum_{i=1}^{4}P_{i}\\ln P_{i}=-\\left(\\frac{1}{2}+\\frac{1}{2}+\\frac{3}{8}+\\frac{3}{8}\\right)\\ln 2=-\\frac{7}{4}\\ln 2.\n$$\nTherefore,\n$$\nI=-\\sum_{i}P_{i}\\ln P_{i}=\\frac{7}{4}\\ln 2.\n$$", "answer": "$$\\boxed{\\frac{7}{4}\\ln 2}$$", "id": "1684817"}, {"introduction": "With a grasp of Shannon entropy, we can now take the next step: calculating the information dimension, $D_1$, by examining how entropy scales with resolution. This practice [@problem_id:1684780] presents a hypothetical attractor composed of both a single point (dimension 0) and an interval (dimension 1), each visited with different probabilities. Solving this problem reveals how the information dimension elegantly captures a weighted average of the system's geometric and probabilistic properties, yielding a non-integer value that reflects the attractor's composite nature.", "problem": "Consider a simplified one-dimensional model for the dynamics of a neuron's membrane potential, $x$. Over long time scales, the system's trajectory settles onto an attractor. This attractor is composed of two distinct parts: a single resting state at $x=0$, and a continuous range of excited states corresponding to the interval $[1, 2]$.\nAn analysis of the long-term behavior reveals that a typical trajectory spends a fraction $p_1 = 0.3$ of its time in an infinitesimally small neighborhood around the resting state $x=0$. The remaining fraction of time, $p_2 = 0.7$, is spent with the potential $x$ distributed uniformly across the interval $[1, 2]$.\nCalculate the information dimension, $D_1$, of this attractor.", "solution": "The information dimension, $D_1$, is defined in terms of the scaling of the information entropy $S(\\epsilon)$ of the attractor as the partition box size $\\epsilon$ approaches zero. The formula is:\n$$ D_1 = \\lim_{\\epsilon \\to 0} \\frac{S(\\epsilon)}{-\\ln(\\epsilon)} $$\nwhere the information entropy is given by $S(\\epsilon) = -\\sum_{i=1}^{N(\\epsilon)} P_i \\ln(P_i)$. Here, $N(\\epsilon)$ is the number of boxes of size $\\epsilon$ required to cover the attractor, and $P_i$ is the probability measure (or the fraction of time the trajectory spends) in the $i$-th box.\n\nThe attractor in this problem consists of two parts: the point $x=0$ and the interval $[1, 2]$. We need to determine the probabilities $P_i$ for boxes covering these two parts.\n\nFirst, let's consider the fixed point at $x=0$. For any small box size $\\epsilon$, this point is contained within a single box. The problem states that the system spends a fraction $p_1 = 0.3$ of its time near this point. Therefore, the probability for this one box is $P_0 = p_1 = 0.3$.\n\nNext, let's consider the interval $[1, 2]$. The length of this interval is $L = 2 - 1 = 1$. To cover this interval with boxes of size $\\epsilon$, we need $N_{int} = \\lceil L/\\epsilon \\rceil = \\lceil 1/\\epsilon \\rceil$ boxes. For small $\\epsilon$, we can approximate this as $N_{int} \\approx 1/\\epsilon$. The total probability associated with this interval is $p_2 = 0.7$. Since the distribution is uniform over the interval, this probability is divided equally among the $N_{int}$ boxes. The probability for any single box covering part of the interval is:\n$$ P_{int} = \\frac{p_2}{N_{int}} = \\frac{0.7}{1/\\epsilon} = 0.7\\epsilon $$\n\nNow we can write the total information entropy $S(\\epsilon)$ by summing the contributions from the fixed point and the interval.\n$$ S(\\epsilon) = - \\left( P_0 \\ln(P_0) + \\sum_{i=1}^{N_{int}} P_{int} \\ln(P_{int}) \\right) $$\nThe sum over the interval has $N_{int}$ identical terms.\n$$ S(\\epsilon) = - (P_0 \\ln(P_0) + N_{int} \\cdot P_{int} \\ln(P_{int})) $$\nSubstitute the expressions for $P_0$, $N_{int}$, and $P_{int}$:\n$$ S(\\epsilon) = - (0.3 \\ln(0.3) + (1/\\epsilon) \\cdot (0.7\\epsilon) \\ln(0.7\\epsilon)) $$\n$$ S(\\epsilon) = - (0.3 \\ln(0.3) + 0.7 \\ln(0.7\\epsilon)) $$\nUsing the logarithm property $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$ S(\\epsilon) = - (0.3 \\ln(0.3) + 0.7 (\\ln(0.7) + \\ln(\\epsilon))) $$\n$$ S(\\epsilon) = -0.3 \\ln(0.3) - 0.7 \\ln(0.7) - 0.7 \\ln(\\epsilon) $$\nThe first two terms, $-0.3 \\ln(0.3) - 0.7 \\ln(0.7)$, constitute a constant value which we can call $C$.\n$$ S(\\epsilon) = C - 0.7 \\ln(\\epsilon) $$\n\nNow we can compute the information dimension using its definition:\n$$ D_1 = \\lim_{\\epsilon \\to 0} \\frac{S(\\epsilon)}{-\\ln(\\epsilon)} = \\lim_{\\epsilon \\to 0} \\frac{C - 0.7 \\ln(\\epsilon)}{-\\ln(\\epsilon)} $$\nWe can split this into two terms:\n$$ D_1 = \\lim_{\\epsilon \\to 0} \\left( \\frac{C}{-\\ln(\\epsilon)} + \\frac{-0.7 \\ln(\\epsilon)}{-\\ln(\\epsilon)} \\right) $$\nAs $\\epsilon \\to 0$, $\\ln(\\epsilon) \\to -\\infty$, so $-\\ln(\\epsilon) \\to \\infty$. The first term has a constant numerator $C$ and a denominator that goes to infinity, so this term goes to zero.\n$$ \\lim_{\\epsilon \\to 0} \\frac{C}{-\\ln(\\epsilon)} = 0 $$\nThe second term simplifies to:\n$$ \\frac{-0.7 \\ln(\\epsilon)}{-\\ln(\\epsilon)} = 0.7 $$\nCombining these results, we find the information dimension:\n$$ D_1 = 0 + 0.7 = 0.7 $$", "answer": "$$\\boxed{0.7}$$", "id": "1684780"}, {"introduction": "Real-world chaotic systems often generate attractors where the natural measure itself has a complex, fractal structure. This advanced problem [@problem_id:1684797] models such a scenario using symbolic dynamics, where a statistical bias in visiting different regions of the phase space leads to a non-uniform measure. By deriving the information dimension for this measure, you will uncover a profound relationship between the statistical properties of a chaotic map and the fractal dimension of its attractor, showing that $D_1$ can be less than the topological dimension of the space the attractor inhabits.", "problem": "A certain one-dimensional chaotic map $f: [0,1] \\to [0,1]$ generates orbits that are dense over the entire interval. Despite this topological mixing, the natural invariant measure $\\mu$ produced by the dynamics is not uniform. A long-term statistical analysis of the system reveals a persistent bias: an orbit spends a fraction of time $p$ in the left sub-interval $I_L = [0, 1/2)$ and a fraction $1-p$ in the right sub-interval $I_R = [1/2, 1]$, where $p$ is a constant and $p \\neq 1/2$.\n\nTo analyze the fractal structure of this natural measure, the phase space is partitioned using a dyadic grid. At the $n$-th level of refinement, the interval $[0,1]$ is divided into $2^n$ bins of equal size $\\epsilon = 2^{-n}$. Each bin can be uniquely identified by a symbolic sequence of length $n$ made up of 'L's and 'R's, indicating which sub-interval the orbit occupied at each of the first $n$ effective time steps. Based on the observed statistics, a bin corresponding to a specific symbolic sequence containing $k$ occurrences of 'L' and $n-k$ occurrences of 'R' is assigned a measure $\\mu_{\\text{sequence}} = p^k (1-p)^{n-k}$.\n\nThe information dimension $D_1$ of the measure $\\mu$ is defined by the scaling of the information entropy $S(\\epsilon)$ for small partitions. The relationship is given by:\n$$D_1 = \\lim_{\\epsilon \\to 0} \\frac{S(\\epsilon)}{\\ln(1/\\epsilon)}$$\nwhere $S(\\epsilon) = -\\sum_{i} \\mu_i \\ln(\\mu_i)$ is the information entropy of the partition, and the sum is over all bins $i$ of size $\\epsilon$.\n\nUsing this model, determine a closed-form analytic expression for the information dimension $D_1$ of the natural measure in terms of the probability $p$.", "solution": "We are given a dyadic partition at level $n$ with $\\epsilon = 2^{-n}$ and $2^{n}$ bins corresponding to all symbolic sequences of length $n$ over $\\{L,R\\}$. A sequence with $k$ occurrences of $L$ has measure $\\mu_{\\text{sequence}} = p^{k} (1-p)^{n-k}$. The information entropy of this partition is\n$$\nS(\\epsilon) = -\\sum_{i} \\mu_{i} \\ln(\\mu_{i}).\n$$\nAt level $n$, grouping bins by the number $k$ of $L$ symbols, there are $\\binom{n}{k}$ sequences with $k$ $L$'s, so\n$$\nS_{n} \\equiv S(\\epsilon) = -\\sum_{k=0}^{n} \\binom{n}{k} p^{k} (1-p)^{n-k} \\ln\\!\\big(p^{k} (1-p)^{n-k}\\big).\n$$\nUsing $\\ln\\!\\big(p^{k} (1-p)^{n-k}\\big) = k \\ln p + (n-k) \\ln(1-p)$, we obtain\n$$\nS_{n} = -\\sum_{k=0}^{n} \\binom{n}{k} p^{k} (1-p)^{n-k} \\big[k \\ln p + (n-k) \\ln(1-p)\\big].\n$$\nSplit the sum into two parts:\n$$\nS_{n} = -(\\ln p) \\sum_{k=0}^{n} \\binom{n}{k} k p^{k} (1-p)^{n-k} - \\ln(1-p) \\sum_{k=0}^{n} \\binom{n}{k} (n-k) p^{k} (1-p)^{n-k}.\n$$\nEvaluate the first sum using the identity $k \\binom{n}{k} = n \\binom{n-1}{k-1}$:\n$$\n\\sum_{k=0}^{n} \\binom{n}{k} k p^{k} (1-p)^{n-k}\n= n p \\sum_{k=1}^{n} \\binom{n-1}{k-1} p^{k-1} (1-p)^{(n-1)-(k-1)}\n= n p \\sum_{j=0}^{n-1} \\binom{n-1}{j} p^{j} (1-p)^{(n-1)-j}\n= n p.\n$$\nSimilarly, using $(n-k)\\binom{n}{k} = n \\binom{n-1}{k}$,\n$$\n\\sum_{k=0}^{n} \\binom{n}{k} (n-k) p^{k} (1-p)^{n-k}\n= n (1-p) \\sum_{k=0}^{n-1} \\binom{n-1}{k} p^{k} (1-p)^{(n-1)-k}\n= n (1-p).\n$$\nTherefore,\n$$\nS_{n} = -n p \\ln p - n (1-p) \\ln(1-p) \\equiv n H(p),\n$$\nwhere $H(p) = -p \\ln p - (1-p) \\ln(1-p)$ is the binary entropy in nats.\n\nThe information dimension is defined as\n$$\nD_{1} = \\lim_{\\epsilon \\to 0} \\frac{S(\\epsilon)}{\\ln(1/\\epsilon)}.\n$$\nSince $\\epsilon = 2^{-n}$, we have $\\ln(1/\\epsilon) = \\ln(2^{n}) = n \\ln 2$. Hence\n$$\nD_{1} = \\lim_{n \\to \\infty} \\frac{S_{n}}{n \\ln 2}\n= \\frac{-p \\ln p - (1-p) \\ln(1-p)}{\\ln 2}.\n$$\nThis gives the closed-form expression for the information dimension of the natural (Bernoulli) measure on the dyadic cylinders with bias $p$.", "answer": "$$\\boxed{\\frac{-p \\ln p - (1-p)\\ln(1-p)}{\\ln 2}}$$", "id": "1684797"}]}