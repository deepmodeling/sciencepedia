{"hands_on_practices": [{"introduction": "At the heart of every adaptive solver is a simple but powerful rule for adjusting the step size to meet a desired accuracy. This exercise strips away the complexities of a full implementation to focus on this core principle [@problem_id:1659045]. By deriving the fundamental scaling law that connects local error, step size $h$, and the method's order $p$, you will grasp the theoretical foundation that makes adaptive integration both efficient and reliable.", "problem": "In the field of computational science, numerical methods are used to approximate solutions to Ordinary Differential Equations (ODEs). A crucial aspect of modern solvers is adaptive step-size control, which adjusts the integration step size, $h$, to maintain a desired level of accuracy while minimizing computational cost.\n\nConsider a numerical integration method of order $p$. The local truncation error, $\\epsilon$, incurred in a single step is known to be proportional to the step size raised to the power of $(p+1)$. This relationship can be expressed as $\\epsilon \\propto h^{p+1}$.\n\nAn engineer is using such a solver with a method of order $p=4$. The solver is configured with a constant target tolerance, $tol$, for the local error. After taking a step with size $h_{old}$, the error estimation module reports a local error of $\\epsilon_{old} = \\frac{1}{2} tol$. To prepare for the next integration step, the control algorithm must propose a new step size, $h_{new}$. The new step size is chosen such that the predicted error for the next step, $\\epsilon_{new}$, would be precisely equal to the target tolerance, $tol$.\n\nAssuming the proportionality constant relating the error to the step size does not change significantly between these two consecutive steps, determine the expression for the proposed new step size $h_{new}$ in terms of the old step size $h_{old}$.", "solution": "The local truncation error for a method of order $p$ satisfies $\\epsilon = C h^{p+1}$, where $C$ is an approximately constant proportionality factor between consecutive steps. For the old step,\n$$\n\\epsilon_{old} = C h_{old}^{p+1} = \\frac{1}{2} tol.\n$$\nFor the proposed new step, we require\n$$\n\\epsilon_{new} = C h_{new}^{p+1} = tol.\n$$\nTaking the ratio,\n$$\n\\frac{\\epsilon_{new}}{\\epsilon_{old}} = \\frac{C h_{new}^{p+1}}{C h_{old}^{p+1}} = \\left(\\frac{h_{new}}{h_{old}}\\right)^{p+1} = \\frac{tol}{\\frac{1}{2} tol} = 2.\n$$\nThus,\n$$\n\\left(\\frac{h_{new}}{h_{old}}\\right)^{p+1} = 2 \\quad \\Rightarrow \\quad \\frac{h_{new}}{h_{old}} = 2^{\\frac{1}{p+1}}.\n$$\nFor $p=4$, we obtain\n$$\nh_{new} = 2^{\\frac{1}{5}} h_{old}.\n$$", "answer": "$$\\boxed{2^{\\frac{1}{5}} h_{old}}$$", "id": "1659045"}, {"introduction": "Moving from theory to practice, this problem challenges you to build a complete adaptive solver from the ground up [@problem_id:2395159]. You will implement the logic for error estimation by comparing results from different step strategies, manage step acceptance and rejection, and apply a robust control law to govern step-size changes. This comprehensive exercise solidifies understanding by turning abstract formulas into a tangible, working piece of scientific software capable of solving a variety of ordinary differential equations.", "problem": "Write a complete, runnable program that advances solutions of initial value problems for ordinary differential equations using the Forward Euler method with adaptive step-size control based on a local truncation error estimate. For an attempted step from time $t$ to $t + h$, let $y^{[h]}$ denote the one-step Forward Euler update and let $y^{[h/2]}$ denote the composition of two Forward Euler half-steps of size $h/2$. Define the local error estimate for that attempted step as $e = \\| y^{[h/2]} - y^{[h]} \\|$, where for scalar states the absolute value is used and for vector states the Euclidean norm is used. A step is accepted if and only if $e \\le \\epsilon$, in which case the solution is advanced to $t \\leftarrow t + h$ with state $y \\leftarrow y^{[h/2]}$. A step is rejected if $e > \\epsilon$, in which case $(t,y)$ are unchanged and a smaller step size must be retried. The tentative next step size is updated after each acceptance or rejection according to\n$$\nh_{\\text{new}} \\;=\\; h \\cdot \\sigma \\cdot \\mathrm{clip}\\!\\left(\\left(\\frac{\\epsilon}{\\max(e, e_{\\min})}\\right)^{1/2}, \\gamma_{\\min}, \\gamma_{\\max}\\right),\n$$\nwith fixed parameters $\\sigma = 0.9$, $e_{\\min} = 10^{-16}$, $\\gamma_{\\min} = 0.2$, and $\\gamma_{\\max} = 5.0$. On every attempt, enforce the bounds $h \\ge h_{\\min}$ with $h_{\\min} = 10^{-12}$ and do not overshoot the final time by replacing $h$ with $\\min(h, T_{\\text{end}} - t)$ before the step attempt. Use the Forward Euler updates\n$$\ny^{[h]} \\;=\\; y \\;+\\; h\\, f(t, y), \\qquad\ny^{[h/2]} \\;=\\; y \\;+\\; \\frac{h}{2}\\, f(t, y) \\;+\\; \\frac{h}{2}\\, f\\!\\left(t+\\frac{h}{2},\\, y + \\frac{h}{2}\\, f(t, y)\\right).\n$$\nTerminate when $t = T_{\\text{end}}$.\n\nApply the above to the following test suite of initial value problems. For each case, compute the absolute error (scalar) or Euclidean norm of the error (vector) at the final time by comparing with the exact solution specified.\n\n- Test case A (scalar, nonlinear):\n  - Differential equation: $y'(t) = -\\,y(t)^2$.\n  - Initial condition: $y(0) = 1$.\n  - Time interval: $t \\in [0, 1]$.\n  - Tolerance: $\\epsilon = 10^{-6}$.\n  - Initial step size: $h_0 = 0.4$.\n  - Exact solution at time $t$: $y(t) = \\dfrac{1}{1 + t}$.\n\n- Test case B (scalar, linear, non-autonomous):\n  - Differential equation: $y'(t) = \\cos(t) - y(t)$.\n  - Initial condition: $y(0) = 0$.\n  - Time interval: $t \\in [0, 3]$.\n  - Tolerance: $\\epsilon = 10^{-5}$.\n  - Initial step size: $h_0 = 0.3$.\n  - Exact solution at time $t$: $y(t) = \\dfrac{\\sin t + \\cos t}{2} \\;-\\; \\dfrac{1}{2}\\, e^{-t}$.\n\n- Test case C (vector, linear, decoupled/forced):\n  - Differential equation: \n    $$\n    \\begin{bmatrix} y_1'(t) \\\\[4pt] y_2'(t) \\end{bmatrix}\n    \\;=\\;\n    \\begin{bmatrix} -2\\, y_1(t) \\\\[4pt] -\\tfrac{1}{2}\\, y_2(t) + \\sin t \\end{bmatrix}\n    $$\n  - Initial condition: $\\begin{bmatrix} y_1(0) \\\\[2pt] y_2(0) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\[2pt] 0 \\end{bmatrix}$.\n  - Time interval: $t \\in [0, 2]$.\n  - Tolerance: $\\epsilon = 10^{-6}$.\n  - Initial step size: $h_0 = 0.2$.\n  - Exact solution at time $t$: $y_1(t) = e^{-2 t}$ and $y_2(t) = \\dfrac{ \\tfrac{1}{2} \\sin t - \\cos t + e^{-t/2}}{1.25}$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases A, B, C. Each result must be a floating-point number equal to the absolute error (for scalar problems) or the Euclidean norm of the error (for the vector problem) at the final time $T_{\\text{end}}$, rounded to $10$ decimal places using standard rounding. For example, the required output format is `[err_A,err_B,err_C]`,\nwith each of $\\text{err\\_A}$, $\\text{err\\_B}$, and $\\text{err\\_C}$ shown with exactly $10$ digits after the decimal point and no additional whitespace.", "solution": "### Principle-Based Solution\n\nThe task is to implement an adaptive-step integration scheme for initial value problems of the form $y'(t) = f(t, y)$ with $y(t_0) = y_0$. The core of the method is the first-order Forward Euler method, enhanced with a mechanism for automatic step-size adjustment to control the local error.\n\nThe Forward Euler method approximates the solution at time $t_{n+1} = t_n + h$ using the Taylor expansion truncated after the first-order term:\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\nThe local truncation error of this method is of order $O(h^2)$.\n\nTo adapt the step size $h$, we need an estimate of this local error. The problem specifies a common technique where the result from a single step of size $h$ is compared with the result from two successive steps of size $h/2$.\n\nLet $y_n$ be the solution at time $t_n$.\n1.  We compute an approximation to $y(t_n+h)$ using one Forward Euler step of size $h$:\n    $$\n    y^{[h]} = y_n + h f(t_n, y_n)\n    $$\n2.  We compute another approximation by taking two consecutive half-steps:\n    First half-step from $t_n$ to $t_n+h/2$:\n    $$\n    y_{n+1/2} = y_n + \\frac{h}{2} f(t_n, y_n)\n    $$\n    Second half-step from $t_n+h/2$ to $t_n+h$:\n    $$\n    y^{[h/2]} = y_{n+1/2} + \\frac{h}{2} f(t_n + \\frac{h}{2}, y_{n+1/2})\n    $$\n    Substituting $y_{n+1/2}$ into the second equation yields the formula for $y^{[h/2]}$ as given in the problem statement. This two-step process is equivalent to a single step of the second-order Runge-Kutta method known as the explicit midpoint rule, which has a local truncation error of $O(h^3)$.\n\nThe difference between these two approximations, $e = \\|y^{[h/2]} - y^{[h]}\\|$, serves as an estimate for the error of the lower-order method, $y^{[h]}$. Specifically, since the exact solution $y_{\\text{exact}}(t_n+h)$ can be written as $y_{\\text{exact}} \\approx y^{[h]} + C h^2$ and $y_{\\text{exact}} \\approx y^{[h/2]} + O(h^3)$, the difference $e$ is a good proxy for the term $C h^2$.\n\nWith this error estimate, we apply step-size control. For a given tolerance $\\epsilon$, a step is successful if $e \\le \\epsilon$. If so, we advance the solution using the more accurate approximation: $t_{n+1} = t_n + h$ and $y_{n+1} = y^{[h/2]}$. This is a form of local extrapolation. If the step is rejected ($e > \\epsilon$), we must retry from $t_n$ with a smaller step size.\n\nThe new step size, $h_{\\text{new}}$, is determined with the goal of making the error in the next step approximately equal to $\\epsilon$. Since $e \\approx C h^2$, we desire $h_{\\text{new}}$ such that $\\epsilon \\approx C h_{\\text{new}}^2$. This leads to the relation $h_{\\text{new}} \\approx h (\\epsilon/e)^{1/2}$. The problem provides a more robust control formula that includes a safety factor $\\sigma$ and clips the step-size change factor to prevent overly aggressive adjustments:\n$$\nh_{\\text{new}} = h \\cdot \\sigma \\cdot \\mathrm{clip}\\!\\left(\\left(\\frac{\\epsilon}{\\max(e, e_{\\min})}\\right)^{1/2}, \\gamma_{\\min}, \\gamma_{\\max}\\right)\n$$\nThis update is performed after every attempt, whether it was accepted or rejected, using the attempted step size $h$ and the resulting error $e$.\n\nThe overall algorithm is implemented as a loop that advances the solution from the initial time $t_0$ to the final time $T_{\\text{end}}$, adjusting the step size $h$ at each iteration based on the estimated local error. The implementation will handle both scalar and vector ODEs and apply the specified constraints on the step size. Finally, the accuracy of the method is evaluated by comparing the computed solution at $T_{\\text{end}}$ to the provided exact analytical solution.", "answer": "```python\nimport numpy as np\n\ndef adaptive_euler_solver(f, t_span, y0, h0, eps, params):\n    \"\"\"\n    Solves an IVP using Forward Euler with adaptive step-size control.\n    \"\"\"\n    t_start, t_end = t_span\n    t = t_start\n    # Ensure y is a numpy array for consistent vector/scalar operations\n    y = np.array(y0, dtype=np.float64)\n    h = h0\n\n    sigma = params['sigma']\n    e_min = params['e_min']\n    gamma_min = params['gamma_min']\n    gamma_max = params['gamma_max']\n    h_min = params['h_min']\n\n    while t  t_end:\n        # Prevent floating point precision issues near the end\n        if np.isclose(t, t_end):\n            break\n        \n        # Adjust step size to not overshoot T_end and respect h_min\n        h_try = min(h, t_end - t)\n        h_try = max(h_try, h_min)\n\n        # One full step (Forward Euler)\n        # y_h = y + h_try * f(t, y)\n        k1 = f(t, y)\n        y_h = y + h_try * k1\n\n        # Two half-steps (Explicit Midpoint)\n        # y_h_half = y + h/2*f + h/2*f(t+h/2, y+h/2*f)\n        y_mid = y + (h_try / 2.0) * k1\n        k2 = f(t + h_try / 2.0, y_mid)\n        y_h_half = y_mid + (h_try / 2.0) * k2\n        \n        # Error estimate\n        error_vec = y_h_half - y_h\n        e = np.linalg.norm(error_vec)\n\n        # Step acceptance/rejection\n        if e = eps:\n            # Accept step and advance solution with the more accurate result\n            t = t + h_try\n            y = y_h_half\n\n        # Update step size for the next attempt (always)\n        ratio = eps / max(e, e_min)\n        factor = np.clip(np.sqrt(ratio), gamma_min, gamma_max)\n        h = h_try * sigma * factor\n        \n    return y\n\ndef solve():\n    \"\"\"\n    Sets up and solves the test cases, then prints the final errors.\n    \"\"\"\n    # Universal parameters for the solver\n    solver_params = {\n        'sigma': 0.9,\n        'e_min': 1e-16,\n        'gamma_min': 0.2,\n        'gamma_max': 5.0,\n        'h_min': 1e-12,\n    }\n\n    # Test Case A\n    def f_a(t, y):\n        return -y[0]**2\n    def y_exact_a(t):\n        return np.array([1.0 / (1.0 + t)])\n\n    # Test Case B\n    def f_b(t, y):\n        return np.cos(t) - y[0]\n    def y_exact_b(t):\n        return np.array([(np.sin(t) + np.cos(t) - np.exp(-t)) / 2.0])\n\n    # Test Case C\n    def f_c(t, y):\n        dy1_dt = -2.0 * y[0]\n        dy2_dt = -0.5 * y[1] + np.sin(t)\n        return np.array([dy1_dt, dy2_dt])\n    def y_exact_c(t):\n        y1 = np.exp(-2.0 * t)\n        y2 = (0.5 * np.sin(t) - np.cos(t) + np.exp(-t / 2.0)) / 1.25\n        return np.array([y1, y2])\n\n    test_cases = [\n        {\n            'f': f_a, 'y_exact': y_exact_a, 't_span': [0.0, 1.0],\n            'y0': [1.0], 'h0': 0.4, 'eps': 1e-6\n        },\n        {\n            'f': f_b, 'y_exact': y_exact_b, 't_span': [0.0, 3.0],\n            'y0': [0.0], 'h0': 0.3, 'eps': 1e-5\n        },\n        {\n            'f': f_c, 'y_exact': y_exact_c, 't_span': [0.0, 2.0],\n            'y0': [1.0, 0.0], 'h0': 0.2, 'eps': 1e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        t_end = case['t_span'][1]\n        \n        y_final = adaptive_euler_solver(\n            case['f'], case['t_span'], case['y0'],\n            case['h0'], case['eps'], solver_params\n        )\n        \n        y_true = case['y_exact'](t_end)\n        error = np.linalg.norm(y_final - y_true)\n        results.append(error)\n\n    # Format output as specified\n    formatted_results = [f\"{res:.10f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2395159"}, {"introduction": "An adaptive solver's behavior can be a powerful diagnostic tool, revealing hidden properties of the system being modeled. This problem explores the fascinating scenario where a solver's choice of step size signals an impending finite-time singularity—a point where the solution blows up to infinity [@problem_id:1659002]. By analytically connecting the solution's behavior to the step-size selection algorithm, you will learn to interpret the computational process itself as a source of profound mathematical insight.", "problem": "An engineer is using a numerical solver to simulate a specific chemical reaction. The concentration of a substance, denoted by $y(t)$, is governed by the ordinary differential equation:\n$$\n\\frac{dy}{dt} = A y^{n}\n$$\nwhere $t$ is time. For this particular reaction, the constant parameters are $A  0$ and $n=3$. The initial concentration at $t=0$ is $y(0) = y_0  0$. It is known that the solution $y(t)$ to this equation experiences a finite-time singularity, meaning it goes to infinity at a finite time $t_s  0$.\n\nThe numerical solver employs an adaptive step-size control strategy. It uses an integration method of order $p=4$. The core of the adaptive algorithm is to adjust the step size $h$ to maintain a nearly constant local truncation error, $\\epsilon$, for each step. The local truncation error for a single step is estimated by the formula:\n$$\n\\text{Err} \\approx C |y^{(p+1)}(t)| h^{p+1}\n$$\nwhere $y^{(p+1)}(t)$ is the $(p+1)$-th derivative of the exact solution with respect to time, and $C$ is a constant characteristic of the numerical method. The solver adjusts $h$ so that $\\text{Err} \\approx \\epsilon$.\n\nAs the simulation time $t$ approaches the singularity time $t_s$, the step size $h$ is observed to shrink following a power-law relationship with respect to the remaining time, $\\tau = t_s - t$. This relationship can be expressed as:\n$$\nh \\approx K \\tau^{\\beta}\n$$\nfor some constant $K$ and a scaling exponent $\\beta$, in the limit where $\\tau \\to 0$.\n\nDetermine the numerical value of the exponent $\\beta$.", "solution": "We start from the given ordinary differential equation with $n=3$:\n$$\n\\frac{dy}{dt} = A y^{3}, \\quad A0, \\quad y(0)=y_{0}0.\n$$\nSeparate variables and integrate:\n$$\n\\int y^{-3}\\,dy = \\int A\\,dt \\quad \\Longrightarrow \\quad -\\frac{1}{2} y^{-2} = A t + c.\n$$\nImpose $y(0)=y_{0}$ to find $c$:\n$$\n-\\frac{1}{2} y_{0}^{-2} = c \\quad \\Longrightarrow \\quad y^{-2}(t) = y_{0}^{-2} - 2 A t.\n$$\nThe blow-up (finite-time singularity) occurs at the time $t_{s}$ where $y^{-2}(t_{s})=0$, hence\n$$\nt_{s} = \\frac{y_{0}^{-2}}{2A}.\n$$\nDefine the remaining time $\\tau = t_{s}-t$. Near $t_{s}$ (i.e., as $\\tau \\to 0^{+}$),\n$$\ny^{-2}(t) = 2A \\tau \\quad \\Longrightarrow \\quad y(t) \\sim (2A)^{-\\frac{1}{2}} \\tau^{-\\frac{1}{2}}.\n$$\nThus, asymptotically, $y(t)$ behaves like $c \\tau^{-m}$ with $c=(2A)^{-\\frac{1}{2}}$ and $m=\\frac{1}{2}$. Derivatives with respect to $t$ can be related to derivatives with respect to $\\tau$ by $d/dt = - d/d\\tau$. For $k \\in \\mathbb{N}$,\n$$\n\\frac{d^{k}}{dt^{k}} y(t) = (-1)^{k} \\frac{d^{k}}{d\\tau^{k}} \\big(c \\tau^{-m}\\big) \\sim \\text{const} \\cdot \\tau^{-m-k},\n$$\nwhere the constant depends on $c$, $m$, and $k$, but the power of $\\tau$ is $-m-k$. Therefore, for a method of order $p=4$, we need the $(p+1)$-th derivative, i.e., $k=5$, and we obtain\n$$\n\\big|y^{(5)}(t)\\big| \\sim \\text{const} \\cdot \\tau^{-\\left(m+5\\right)} = \\text{const} \\cdot \\tau^{-\\frac{11}{2}}.\n$$\nThe adaptive step-size control uses the local truncation error model\n$$\n\\text{Err} \\approx C \\big|y^{(p+1)}(t)\\big| h^{p+1},\n$$\nand maintains $\\text{Err} \\approx \\epsilon$ with $p=4$, so\n$$\n\\epsilon \\approx C \\big|y^{(5)}(t)\\big| h^{5} \\quad \\Longrightarrow \\quad h^{5} \\approx \\frac{\\epsilon}{C \\big|y^{(5)}(t)\\big|}.\n$$\nUsing the asymptotic scaling $\\big|y^{(5)}(t)\\big| \\sim \\text{const} \\cdot \\tau^{-\\frac{11}{2}}$, we find\n$$\nh \\sim \\left(\\frac{\\epsilon}{C}\\right)^{\\frac{1}{5}} \\big(\\text{const}\\big)^{-\\frac{1}{5}} \\tau^{\\frac{11}{10}}.\n$$\nHence, in the relation $h \\approx K \\tau^{\\beta}$ as $\\tau \\to 0^{+}$, the exponent is\n$$\n\\beta = \\frac{11}{10}.\n$$", "answer": "$$\\boxed{\\frac{11}{10}}$$", "id": "1659002"}]}