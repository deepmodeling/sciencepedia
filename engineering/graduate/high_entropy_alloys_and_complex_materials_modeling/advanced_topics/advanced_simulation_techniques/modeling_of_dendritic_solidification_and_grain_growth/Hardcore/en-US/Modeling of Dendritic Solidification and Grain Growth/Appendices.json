{
    "hands_on_practices": [
        {
            "introduction": "The microstructure formed during solidification is rarely stable. This first practice explores the fundamental process of normal grain growth, where the system reduces its total energy by eliminating grain boundaries. By applying the principle that boundary motion is driven by local curvature , you will derive and apply the classic parabolic growth law, a cornerstone for predicting microstructural evolution in annealed materials.",
            "id": "3751532",
            "problem": "A thin-section of a High-Entropy Alloy (HEA) immediately after dendritic solidification undergoes normal grain growth in two dimensions driven by capillarity. Assume isotropic Grain Boundary (GB) energy and boundary mobility such that the product $M\\gamma$ is constant and equal to $1.0 \\times 10^{-12}$ $\\mathrm{m}^2/\\mathrm{s}$ at the processing temperature. The GB motion is governed locally by mean-curvature flow according to the Mullins relation $v_{n} = M\\gamma\\,\\kappa$, where $v_{n}$ is the normal velocity of the boundary and $\\kappa$ is the mean curvature of the boundary in the plane. Under statistically self-similar coarsening, define the characteristic (average) grain size $R(t)$ as the single emergent length scale of the network that reflects the typical radius of curvature of GBs and the mean intercept length, and suppose it evolves consistently with the curvature-driven kinetics implied by $v_{n} = M\\gamma\\,\\kappa$.\n\nStarting from $R(0) = 10\\ \\mu\\mathrm{m}$, determine the time $t$ required for $R(t)$ to reach $20\\ \\mu\\mathrm{m}$ under this two-dimensional model. Round your answer to four significant figures. Express the final time in seconds.",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **System**: A thin-section of a High-Entropy Alloy (HEA) in two dimensions ($2$D).\n- **Process**: Normal grain growth driven by capillarity.\n- **Assumptions**: Isotropic Grain Boundary (GB) energy, $\\gamma$, and boundary mobility, $M$.\n- **Constant**: The product $M\\gamma$ is constant and equals $1.0 \\times 10^{-12}$ $\\mathrm{m}^2/\\mathrm{s}$.\n- **Governing Law**: The normal velocity of the GB, $v_n$, is governed by the Mullins relation: $v_{n} = M\\gamma\\,\\kappa$, where $\\kappa$ is the mean curvature of the boundary in the plane.\n- **Microstructure Evolution**: The process occurs under statistically self-similar coarsening.\n- **Characteristic Length Scale**: The average grain size is denoted by $R(t)$. It is stated that $R(t)$ evolves consistently with the curvature-driven kinetics.\n- **Initial Condition**: $R(t=0) = R_0 = 10\\ \\mu\\mathrm{m}$.\n- **Target Condition**: $R(t) = R_f = 20\\ \\mu\\mathrm{m}$.\n- **Objective**: Determine the time $t$ required to reach the target condition.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is examined for scientific validity, completeness, and clarity.\n- **Scientific Grounding**: The problem describes normal grain growth, a fundamental materials science phenomenon. The governing equation, $v_{n} = M\\gamma\\,\\kappa$, is the well-established Mullins relation for curvature-driven boundary motion. The concept of a self-similar coarsening regime characterized by a single evolving length scale, $R(t)$, is a cornerstone of grain growth theory. The provided value for the kinetic parameter $M\\gamma$ is physically plausible for solid-state processes. The problem is scientifically sound.\n- **Well-Posedness**: The problem provides a governing physical law, initial and final conditions for the characteristic variable, and a well-defined objective. It is structured to allow for the derivation and solution of a differential equation for $R(t)$, leading to a unique answer.\n- **Objectivity**: The language is precise, technical, and free of subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. It does not violate any of the invalidity criteria. Therefore, a complete solution will be provided.\n\n### Solution Derivation\nThe evolution of the average grain size, $R(t)$, can be determined from the local law for grain boundary motion. The problem states that grain boundary motion is governed by the Mullins relation:\n$$v_{n} = M\\gamma\\,\\kappa$$\nwhere $v_n$ is the normal velocity of the boundary and $\\kappa$ is its local curvature.\n\nThe system is described as undergoing statistically self-similar coarsening, with $R(t)$ being the single characteristic length scale. In such a system, all statistical properties of the microstructure remain invariant if all lengths are scaled by $R(t)$. This has two important consequences:\n$1$. The characteristic rate of change of the system's length scale must be proportional to the typical boundary velocity, $v_n$. Thus, the rate of change of the average grain size, $\\frac{dR}{dt}$, must scale with $v_n$:\n$$\\frac{dR}{dt} \\propto v_n$$\n$2$. The curvature, $\\kappa$, has dimensions of inverse length. In a system with a single length scale $R(t)$, the average or characteristic curvature must be inversely proportional to $R(t)$:\n$$\\kappa \\propto \\frac{1}{R(t)}$$\nCombining these scaling relationships with the Mullins relation, we have:\n$$\\frac{dR}{dt} \\propto M\\gamma \\frac{1}{R(t)}$$\nThis can be expressed as a differential equation:\n$$\\frac{dR}{dt} = C \\frac{M\\gamma}{R(t)}$$\nwhere $C$ is a dimensionless constant of order unity that accounts for geometric factors of the grain network. Given that the problem states $R(t)$ evolves \"consistently with the curvature-driven kinetics\" without providing additional geometric details, it is standard to assume the most direct relationship, which corresponds to $C=1$. This yields the classic parabolic growth law derivation. The differential equation for the evolution of the average grain size is therefore:\n$$\\frac{dR}{dt} = \\frac{M\\gamma}{R(t)}$$\nThis is a separable first-order ordinary differential equation. We rearrange the terms to separate the variables $R$ and $t$:\n$$R(t) \\, dR = M\\gamma \\, dt$$\nTo find the time required for the grain size to grow from an initial value $R_0 = R(0)$ to a final value $R_f = R(t)$, we integrate both sides over the corresponding intervals:\n$$\\int_{R_0}^{R_f} R \\, dR = \\int_{0}^{t} M\\gamma \\, d\\tau$$\nSince $M\\gamma$ is a constant, the integration yields:\n$$\\left[ \\frac{1}{2}R^2 \\right]_{R_0}^{R_f} = M\\gamma \\left[ \\tau \\right]_{0}^{t}$$\n$$\\frac{1}{2}(R_f^2 - R_0^2) = M\\gamma t$$\nThis is the integrated form of the parabolic growth law. We can now solve for the time, $t$:\n$$t = \\frac{R_f^2 - R_0^2}{2 M\\gamma}$$\nThe problem provides the following values, which must be converted to SI units for consistency:\n- $M\\gamma = 1.0 \\times 10^{-12}\\ \\mathrm{m}^2/\\mathrm{s}$\n- $R_0 = 10\\ \\mu\\mathrm{m} = 10 \\times 10^{-6}\\ \\mathrm{m} = 1.0 \\times 10^{-5}\\ \\mathrm{m}$\n- $R_f = 20\\ \\mu\\mathrm{m} = 20 \\times 10^{-6}\\ \\mathrm{m} = 2.0 \\times 10^{-5}\\ \\mathrm{m}$\n\nSubstituting these values into the expression for $t$:\n$$t = \\frac{(2.0 \\times 10^{-5})^2 - (1.0 \\times 10^{-5})^2}{2 \\times (1.0 \\times 10^{-12})}$$\n$$t = \\frac{(4.0 \\times 10^{-10}) - (1.0 \\times 10^{-10})}{2.0 \\times 10^{-12}}$$\n$$t = \\frac{3.0 \\times 10^{-10}}{2.0 \\times 10^{-12}}$$\n$$t = 1.5 \\times 10^{(-10 - (-12))}$$\n$$t = 1.5 \\times 10^2 = 150\\ \\mathrm{s}$$\nThe problem requests the answer to be rounded to four significant figures. Therefore, the time is $150.0$ seconds.",
            "answer": "$$\\boxed{150.0}$$"
        },
        {
            "introduction": "Moving from analytical models to computer simulations requires translating continuous differential equations into discrete algorithms. This exercise addresses a critical aspect of this translation: numerical stability. You will perform a von Neumann stability analysis for the diffusion equation , a core component of many solidification and grain growth models, to understand the constraints on time step and grid spacing required for a simulation to be physically meaningful.",
            "id": "3751535",
            "problem": "In modeling dendritic solidification and subsequent grain growth in multicomponent high-entropy alloys, the transport of solute or heat is commonly represented by a diffusion equation in two spatial dimensions. Consider a forward-in-time, centered-in-space explicit finite-difference update of the scalar field $c(x,y,t)$ on a uniform square grid with spacing $\\Delta x$ and time step $\\Delta t$, with an isotropic and constant diffusion coefficient $D$. Assume equal grid spacing in both directions and periodic boundary conditions so that a Fourier mode analysis is valid. Starting from the continuum diffusion equation $\\partial c/\\partial t = D \\nabla^{2} c$, derive from first principles the non-dimensional diffusive Courant number (also known as the Fourier number) that controls linear stability for this explicit scheme in two spatial dimensions, and obtain the associated stability requirement in terms of this non-dimensional number. Then, for a two-dimensional simulation relevant to nanoscale dendrite–interdendrite transport in a high-entropy alloy melt with $\\Delta x = 2\\,\\mathrm{nm}$, $D = 10^{-9}\\,\\mathrm{m^{2}/s}$, and $\\Delta t = 10^{-12}\\,\\mathrm{s}$, compute the value of the diffusive Courant number and use your derived condition to assess whether the scheme is linearly stable.\n\nProvide as your final answer only the numerical value of the diffusive Courant number, expressed as a pure number without units. Round your answer to three significant figures.",
            "solution": "The problem asks for the derivation of the stability condition for the forward-in-time, centered-in-space (FTCS) explicit finite-difference scheme for the two-dimensional diffusion equation, and then to apply this condition to a specific numerical case.\n\nThe governing continuum diffusion equation for a scalar field $c(x,y,t)$ with a constant, isotropic diffusion coefficient $D$ in two spatial dimensions is:\n$$\n\\frac{\\partial c}{\\partial t} = D \\nabla^2 c = D \\left( \\frac{\\partial^2 c}{\\partial x^2} + \\frac{\\partial^2 c}{\\partial y^2} \\right)\n$$\n\nWe discretize this equation on a uniform square grid where the grid spacing is $\\Delta x = \\Delta y$ and the time step is $\\Delta t$. Let $c_{i,j}^n$ represent the value of the scalar field at the grid point $(x_i, y_j) = (i\\Delta x, j\\Delta x)$ and at time $t_n = n\\Delta t$.\n\nThe FTCS scheme uses a forward difference for the time derivative and a centered difference for the spatial derivatives:\n$$\n\\frac{\\partial c}{\\partial t} \\approx \\frac{c_{i,j}^{n+1} - c_{i,j}^n}{\\Delta t}\n$$\n$$\n\\frac{\\partial^2 c}{\\partial x^2} \\approx \\frac{c_{i+1,j}^n - 2c_{i,j}^n + c_{i-1,j}^n}{(\\Delta x)^2}\n$$\n$$\n\\frac{\\partial^2 c}{\\partial y^2} \\approx \\frac{c_{i,j+1}^n - 2c_{i,j}^n + c_{i,j-1}^n}{(\\Delta y)^2}\n$$\n\nSubstituting these approximations into the diffusion equation, and using $\\Delta x = \\Delta y$, we get:\n$$\n\\frac{c_{i,j}^{n+1} - c_{i,j}^n}{\\Delta t} = D \\left( \\frac{c_{i+1,j}^n - 2c_{i,j}^n + c_{i-1,j}^n}{(\\Delta x)^2} + \\frac{c_{i,j+1}^n - 2c_{i,j}^n + c_{i,j-1}^n}{(\\Delta x)^2} \\right)\n$$\nRearranging to solve for $c_{i,j}^{n+1}$:\n$$\nc_{i,j}^{n+1} = c_{i,j}^n + \\frac{D \\Delta t}{(\\Delta x)^2} \\left( c_{i+1,j}^n + c_{i-1,j}^n + c_{i,j+1}^n + c_{i,j-1}^n - 4c_{i,j}^n \\right)\n$$\nThe problem asks for the non-dimensional diffusive Courant number. We define this as:\n$$\n\\alpha = \\frac{D \\Delta t}{(\\Delta x)^2}\n$$\nThis dimensionless group is also known as the Fourier number for mass transfer. The update equation becomes:\n$$\nc_{i,j}^{n+1} = c_{i,j}^n + \\alpha \\left( c_{i+1,j}^n + c_{i-1,j}^n + c_{i,j+1}^n + c_{i,j-1}^n - 4c_{i,j}^n \\right)\n$$\n\nTo determine the linear stability of this scheme, we perform a von Neumann stability analysis. We consider the evolution of a single generic Fourier mode of the error, which has the same form as the solution:\n$$\nc_{i,j}^n = G(k_x, k_y)^n \\exp(I(k_x x_i + k_y y_j)) = G^n \\exp(I(k_x i \\Delta x + k_y j \\Delta x))\n$$\nwhere $k_x$ and $k_y$ are the components of the wavevector, $I = \\sqrt{-1}$ is the imaginary unit, and $G$ is the amplification factor per time step for that mode. For the scheme to be stable, the magnitude of the amplification factor must be less than or equal to $1$ for all possible wavevectors, i.e., $|G| \\leq 1$.\n\nSubstituting the Fourier mode into the discrete update equation:\n\\begin{align*}\nG^{n+1} e^{I(k_x i \\Delta x + k_y j \\Delta x)} = G^n e^{I(k_x i \\Delta x + k_y j \\Delta x)} + \\alpha \\left[ G^n e^{I(k_x(i+1)\\Delta x + k_y j \\Delta x)} + G^n e^{I(k_x(i-1)\\Delta x + k_y j \\Delta x)} \\right. \\\\\n\\left. + G^n e^{I(k_x i \\Delta x + k_y(j+1)\\Delta x)} + G^n e^{I(k_x i \\Delta x + k_y(j-1)\\Delta x)} - 4G^n e^{I(k_x i \\Delta x + k_y j \\Delta x)} \\right]\n\\end{align*}\nDividing by the common term $G^n \\exp(I(k_x i \\Delta x + k_y j \\Delta x))$ gives the amplification factor $G = G^{n+1}/G^n$:\n$$\nG = 1 + \\alpha \\left( e^{I k_x \\Delta x} + e^{-I k_x \\Delta x} + e^{I k_y \\Delta x} + e^{-I k_y \\Delta x} - 4 \\right)\n$$\nUsing Euler's identity, $2\\cos\\theta = e^{I\\theta} + e^{-I\\theta}$:\n$$\nG = 1 + \\alpha \\left( 2\\cos(k_x \\Delta x) + 2\\cos(k_y \\Delta x) - 4 \\right)\n$$\n$$\nG = 1 - 2\\alpha \\left( 2 - \\cos(k_x \\Delta x) - \\cos(k_y \\Delta x) \\right)\n$$\nUsing the half-angle identity $1 - \\cos\\theta = 2\\sin^2(\\theta/2)$:\n$$\nG = 1 - 2\\alpha \\left( (1 - \\cos(k_x \\Delta x)) + (1 - \\cos(k_y \\Delta x)) \\right)\n$$\n$$\nG = 1 - 4\\alpha \\left( \\sin^2\\left(\\frac{k_x \\Delta x}{2}\\right) + \\sin^2\\left(\\frac{k_y \\Delta x}{2}\\right) \\right)\n$$\nThe stability condition $|G| \\leq 1$ translates to $-1 \\leq G \\leq 1$.\nSince $\\alpha  0$ and $\\sin^2(\\cdot) \\geq 0$, the term being subtracted from $1$ is always non-negative, so $G \\leq 1$ is always satisfied.\nThe critical condition is $G \\geq -1$:\n$$\n1 - 4\\alpha \\left( \\sin^2\\left(\\frac{k_x \\Delta x}{2}\\right) + \\sin^2\\left(\\frac{k_y \\Delta x}{2}\\right) \\right) \\geq -1\n$$\n$$\n2 \\geq 4\\alpha \\left( \\sin^2\\left(\\frac{k_x \\Delta x}{2}\\right) + \\sin^2\\left(\\frac{k_y \\Delta x}{2}\\right) \\right)\n$$\n$$\n\\alpha \\leq \\frac{1}{2 \\left( \\sin^2\\left(\\frac{k_x \\Delta x}{2}\\right) + \\sin^2\\left(\\frac{k_y \\Delta x}{2}\\right) \\right)}\n$$\nThis inequality must hold for all possible wavenumbers $k_x$ and $k_y$. The most restrictive condition (the \"worst case\") occurs when the right-hand side is minimized, which corresponds to maximizing the denominator. The term $\\sin^2(\\cdot)$ has a maximum value of $1$. This occurs for the highest frequency modes resolvable on the grid (the Nyquist frequencies), where $k_x \\Delta x = \\pi$ and $k_y \\Delta x = \\pi$.\nSubstituting these values:\n$$\n\\sin^2\\left(\\frac{\\pi}{2}\\right) + \\sin^2\\left(\\frac{\\pi}{2}\\right) = 1^2 + 1^2 = 2\n$$\nThe stability requirement becomes:\n$$\n\\alpha \\leq \\frac{1}{2(2)} = \\frac{1}{4}\n$$\nThus, the stability requirement for the $2$D FTCS scheme is that the diffusive Courant number must satisfy $\\alpha = \\frac{D \\Delta t}{(\\Delta x)^2} \\leq \\frac{1}{4}$.\n\nNow we apply this to the given numerical case. The parameters are:\n$\\Delta x = 2\\,\\text{nm} = 2 \\times 10^{-9}\\,\\text{m}$\n$D = 10^{-9}\\,\\text{m}^2/\\text{s}$\n$\\Delta t = 10^{-12}\\,\\text{s}$\n\nWe compute the value of the diffusive Courant number $\\alpha$:\n$$\n\\alpha = \\frac{D \\Delta t}{(\\Delta x)^2} = \\frac{(10^{-9}\\,\\text{m}^2/\\text{s})(10^{-12}\\,\\text{s})}{(2 \\times 10^{-9}\\,\\text{m})^2} = \\frac{10^{-21}\\,\\text{m}^2}{4 \\times 10^{-18}\\,\\text{m}^2} = \\frac{1}{4} \\times 10^{-3} = 0.25 \\times 10^{-3} = 0.00025\n$$\nTo assess stability, we compare this value to the derived limit of $\\frac{1}{4} = 0.25$:\n$$\n0.00025 \\leq 0.25\n$$\nThe condition is satisfied, so the scheme is linearly stable for the given parameters.\n\nThe problem asks for the numerical value of the diffusive Courant number, rounded to three significant figures.\nThe calculated value is $0.00025$.\nTo three significant figures, this is $0.000250$, or in scientific notation, $2.50 \\times 10^{-4}$.",
            "answer": "$$\n\\boxed{2.50 \\times 10^{-4}}\n$$"
        },
        {
            "introduction": "A sophisticated model is only as good as its parameters, which are often difficult to measure directly. This final practice introduces a powerful technique from modern computational science: Bayesian inference for parameter estimation . By treating model parameters as probabilistic quantities, you will learn how to systematically update your knowledge about them using synthetic experimental data, a crucial step in building predictive models of complex phenomena like dendritic solidification.",
            "id": "3751487",
            "problem": "Consider dendritic solidification in a high-entropy alloy modeled by a phase-field description under the thin-interface limit. The interface temperature obeys the Gibbs–Thomson relation with kinetic undercooling, which in one-dimensional steady propagation at the tip can be written as a scalar balance, where the local undercooling $\\Delta T$ satisfies $\\Delta T = d_0 \\kappa + \\beta V$. The curvature $\\kappa$ has units of $\\text{m}^{-1}$, the tip velocity $V$ has units of $\\text{m}\\,\\text{s}^{-1}$, and the undercooling $\\Delta T$ has units of $\\text{K}$. In the thin-interface mapping of the phase-field parameters, the capillary length $d_0$ and the kinetic coefficient $\\beta$ are proportional to the interface width $W$ and the relaxation time $\\tau$ as $d_0 \\propto W$ and $\\beta \\propto \\tau/W$. For the purposes of this problem, fold all material constants (including the proportionality and thermodynamic factors) into two known positive scalars $c_1$ and $c_2$ with units of $\\text{K}$, such that the observation model becomes\n$$\n\\Delta T_i = c_1\\,W\\,\\kappa_i + c_2\\,\\frac{\\tau}{W}\\,V_i + \\varepsilon_i,\n$$\nfor $i=1,\\dots,N$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent Gaussian noises with variance $\\sigma^2$ (in $\\text{K}^2$). Define the parameter vector $\\boldsymbol{\\theta} = [W,\\; r]^\\top$ where $r = \\tau/W$ has units of $\\text{s}\\,\\text{m}^{-1}$. Then the observation model is linear in $\\boldsymbol{\\theta}$,\n$$\n\\Delta T_i = (c_1\\,\\kappa_i)\\,W + (c_2\\,V_i)\\,r + \\varepsilon_i,\n$$\nor in matrix form $\\mathbf{y} = \\mathbf{X}\\,\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, with\n$$\n\\mathbf{y} = \\begin{bmatrix}\\Delta T_1 \\\\ \\vdots \\\\ \\Delta T_N \\end{bmatrix},\\quad\n\\mathbf{X} = \\begin{bmatrix} c_1\\,\\kappa_1  c_2\\,V_1 \\\\ \\vdots  \\vdots \\\\ c_1\\,\\kappa_N  c_2\\,V_N \\end{bmatrix},\\quad\n\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0},\\,\\sigma^2\\,\\mathbf{I}_N).\n$$\nAssume a Gaussian prior on $\\boldsymbol{\\theta}$,\n$$\n\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{m}_0,\\,\\mathbf{S}_0),\n$$\nwith prior mean $\\mathbf{m}_0 = [m_{0,W},\\; m_{0,r}]^\\top$ and prior covariance $\\mathbf{S}_0$ (a symmetric positive-definite $2\\times 2$ matrix). Under the Gaussian likelihood and Gaussian prior, the posterior for $\\boldsymbol{\\theta}$ is Gaussian,\n$$\n\\boldsymbol{\\theta}\\mid \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{m}_N,\\,\\mathbf{S}_N),\n$$\nwith\n$$\n\\mathbf{S}_N = \\left(\\mathbf{S}_0^{-1} + \\frac{1}{\\sigma^2}\\,\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1},\\quad\n\\mathbf{m}_N = \\mathbf{S}_N\\left(\\mathbf{S}_0^{-1}\\mathbf{m}_0 + \\frac{1}{\\sigma^2}\\,\\mathbf{X}^\\top \\mathbf{y}\\right).\n$$\nThe posterior mean of $W$ is simply the first component of $\\mathbf{m}_N$, i.e., $\\mathbb{E}[W\\mid \\mathbf{y}] = (\\mathbf{m}_N)_1$. The relaxation time $\\tau$ is not a direct component of $\\boldsymbol{\\theta}$ but satisfies $\\tau = W\\,r$. For the bivariate Gaussian posterior on $(W,r)$, the posterior mean of $\\tau$ is\n$$\n\\mathbb{E}[\\tau\\mid \\mathbf{y}] = \\mathbb{E}[W\\,r\\mid \\mathbf{y}] = (\\mathbf{m}_N)_1\\,(\\mathbf{m}_N)_2 + (\\mathbf{S}_N)_{12}.\n$$\nIn addition to computing posterior means, determine whether $\\boldsymbol{\\theta}$ is identifiable from the data alone, which in this linear-Gaussian setting is equivalent to the design matrix $\\mathbf{X}$ having full column rank. That is, define an identifiability indicator as\n$$\n\\text{identifiable} = \\begin{cases}\n\\text{True},  \\text{if }\\operatorname{rank}(\\mathbf{X}) = 2,\\\\\n\\text{False},  \\text{otherwise}.\n\\end{cases}\n$$\n\nYour task is to write a program that:\n1. Generates synthetic observations $\\mathbf{y}$ according to the model for given ground-truth parameters $W^\\star$ and $\\tau^\\star$ and specified $\\kappa_i$, $V_i$, $c_1$, $c_2$, and $\\sigma$.\n2. Computes the posterior mean of $W$ and the posterior mean of $\\tau$ using the formulas above.\n3. Computes the identifiability indicator by checking the rank of $\\mathbf{X}$.\n4. Outputs, for each test case, a list containing the posterior mean of $W$ (in meters), the posterior mean of $\\tau$ (in seconds), and the identifiability boolean.\n\nUse the following test suite. For all test cases, use ground-truth $W^\\star = 1.0\\times 10^{-7}$ m and $\\tau^\\star = 8.0\\times 10^{-7}$ s, so $r^\\star = \\tau^\\star/W^\\star = 8.0$ s m$^{-1}$, and set $c_1 = 5.0$ K and $c_2 = 2.0$ K. Use a fixed random seed to ensure reproducibility. The prior mean and covariance are shared across test cases: $\\mathbf{m}_0 = [9.0\\times 10^{-8},\\; 7.0]^\\top$ and $\\mathbf{S}_0 = \\operatorname{diag}\\left((2.0\\times 10^{-8})^2,\\; (2.0)^2\\right)$.\n\nTest Case $1$ (well-conditioned, moderate noise):\n- $\\kappa = [8.0\\times 10^{5},\\; 1.2\\times 10^{6},\\; 9.0\\times 10^{5},\\; 7.0\\times 10^{5},\\; 1.1\\times 10^{6},\\; 1.0\\times 10^{6}]$ $\\text{m}^{-1}$,\n- $V = [2.0\\times 10^{-2},\\; 5.0\\times 10^{-2},\\; 3.0\\times 10^{-2},\\; 1.0\\times 10^{-2},\\; 4.0\\times 10^{-2},\\; 6.0\\times 10^{-2}]$ $\\text{m}\\,\\text{s}^{-1}$,\n- $\\sigma = 5.0\\times 10^{-2}$ $\\text{K}$.\n\nTest Case $2$ (well-conditioned, high noise):\n- Same $\\kappa$ and $V$ as Test Case $1$,\n- $\\sigma = 5.0\\times 10^{-1}$ $\\text{K}$.\n\nTest Case $3$ (collinear features, non-identifiable):\n- $V = [1.0\\times 10^{-2},\\; 2.0\\times 10^{-2},\\; 3.0\\times 10^{-2},\\; 4.0\\times 10^{-2}]$ $\\text{m}\\,\\text{s}^{-1}$,\n- $\\kappa_i = 5.0\\times 10^{7}\\,V_i$ $\\text{m}^{-1}$ for each $i$ (so the columns of $\\mathbf{X}$ are proportional),\n- $\\sigma = 5.0\\times 10^{-2}$ $\\text{K}$.\n\nTest Case $4$ (single observation, non-identifiable):\n- $\\kappa = [1.0\\times 10^{6}]$ $\\text{m}^{-1}$,\n- $V = [3.0\\times 10^{-2}]$ $\\text{m}\\,\\text{s}^{-1}$,\n- $\\sigma = 5.0\\times 10^{-2}$ $\\text{K}$.\n\nYour program must:\n- Use $\\mathbf{X}$ as defined, generate $\\mathbf{y}$ as $\\mathbf{y} = \\mathbf{X}\\,[W^\\star,\\; r^\\star]^\\top + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0},\\,\\sigma^2\\,\\mathbf{I}_N)$.\n- Compute $\\mathbf{m}_N$ and $\\mathbf{S}_N$ exactly by the closed-form Gaussian update.\n- Report $\\mathbb{E}[W\\mid \\mathbf{y}]$ in meters and $\\mathbb{E}[\\tau\\mid \\mathbf{y}]$ in seconds for each test case, along with the identifiability boolean.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each $result_i$ is itself a list of the form $[W\\_\\text{mean},\\tau\\_\\text{mean},\\text{identifiable}]$ for the corresponding test case, with $W\\_\\text{mean}$ and $\\tau\\_\\text{mean}$ as floating-point numbers and $\\text{identifiable}$ as a boolean. Express $W\\_\\text{mean}$ in meters and $\\tau\\_\\text{mean}$ in seconds without any unit symbols in the output.",
            "solution": "The problem is deemed valid. It is scientifically grounded in the established theory of Bayesian linear regression, applied to a physical model of dendritic solidification. The problem is well-posed, with all necessary data, equations, and conditions provided to arrive at a unique solution. The language is objective and unambiguous. We may therefore proceed to the solution.\n\nThe core of the problem is to perform Bayesian parameter estimation for a linear model. The physical model relating undercooling $\\Delta T$ to interface width $W$ and relaxation time $\\tau$ is given by:\n$$\n\\Delta T_i = c_1\\,W\\,\\kappa_i + c_2\\,\\frac{\\tau}{W}\\,V_i + \\varepsilon_i\n$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ represents measurement noise. This model is non-linear in the physically interpretable parameters $W$ and $\\tau$. However, by defining a new parameter vector $\\boldsymbol{\\theta} = [W, r]^\\top$, where $r = \\tau/W$, the observation model becomes linear in $\\boldsymbol{\\theta}$:\n$$\n\\Delta T_i = (c_1\\,\\kappa_i)\\,W + (c_2\\,V_i)\\,r + \\varepsilon_i\n$$\nThis can be expressed in matrix form for a set of $N$ observations:\n$$\n\\mathbf{y} = \\mathbf{X}\\,\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^N$ is the vector of undercooling measurements, $\\mathbf{X} \\in \\mathbb{R}^{N \\times 2}$ is the design matrix, $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$ is the parameter vector, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^N$ is the noise vector. The design matrix $\\mathbf{X}$ is constructed using the known experimental conditions $(\\kappa_i, V_i)$ and constants $(c_1, c_2)$:\n$$\n\\mathbf{X} = \\begin{bmatrix} c_1\\,\\kappa_1  c_2\\,V_1 \\\\ \\vdots  \\vdots \\\\ c_1\\,\\kappa_N  c_2\\,V_N \\end{bmatrix}\n$$\nThe noise model is $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I}_N)$, which implies a Gaussian likelihood for the data given the parameters: $p(\\mathbf{y}|\\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y}; \\mathbf{X}\\boldsymbol{\\theta}, \\sigma^2\\mathbf{I}_N)$.\n\nA Gaussian prior is assumed for the parameter vector $\\boldsymbol{\\theta}$:\n$$\np(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta}; \\mathbf{m}_0, \\mathbf{S}_0)\n$$\nwhere $\\mathbf{m}_0$ is the prior mean and $\\mathbf{S}_0$ is the prior covariance matrix.\n\nAccording to Bayes' theorem, the posterior distribution $p(\\boldsymbol{\\theta}|\\mathbf{y})$ is proportional to the product of the likelihood and the prior, $p(\\boldsymbol{\\theta}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})$. For a Gaussian likelihood and a Gaussian prior, the posterior is also Gaussian, $\\boldsymbol{\\theta}|\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{m}_N, \\mathbf{S}_N)$. The posterior covariance $\\mathbf{S}_N$ and posterior mean $\\mathbf{m}_N$ are given by the standard update rules:\n$$\n\\mathbf{S}_N = \\left(\\mathbf{S}_0^{-1} + \\frac{1}{\\sigma^2}\\,\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}\n$$\n$$\n\\mathbf{m}_N = \\mathbf{S}_N\\left(\\mathbf{S}_0^{-1}\\mathbf{m}_0 + \\frac{1}{\\sigma^2}\\,\\mathbf{X}^\\top \\mathbf{y}\\right)\n$$\nThe quantities to be computed are the posterior means of $W$ and $\\tau$, and an identifiability indicator.\n\n1.  **Posterior Mean of $W$**: Since $W$ is the first component of $\\boldsymbol{\\theta}$, its posterior mean is simply the first component of the posterior mean vector $\\mathbf{m}_N$:\n    $$\n    \\mathbb{E}[W\\mid \\mathbf{y}] = (\\mathbf{m}_N)_1\n    $$\n\n2.  **Posterior Mean of $\\tau$**: The parameter $\\tau$ is related to the components of $\\boldsymbol{\\theta}$ by $\\tau=W\\,r$. Since the joint posterior distribution of $(W, r)$ is bivariate Gaussian $\\mathcal{N}(\\mathbf{m}_N, \\mathbf{S}_N)$, the expected value of their product is given by:\n    $$\n    \\mathbb{E}[\\tau\\mid \\mathbf{y}] = \\mathbb{E}[W\\,r\\mid \\mathbf{y}] = \\mathbb{E}[W\\mid\\mathbf{y}]\\,\\mathbb{E}[r\\mid\\mathbf{y}] + \\operatorname{Cov}(W, r\\mid\\mathbf{y})\n    $$\n    In terms of the posterior parameters, this is:\n    $$\n    \\mathbb{E}[\\tau\\mid \\mathbf{y}] = (\\mathbf{m}_N)_1\\,(\\mathbf{m}_N)_2 + (\\mathbf{S}_N)_{12}\n    $$\n    where $(\\mathbf{m}_N)_1$ and $(\\mathbf{m}_N)_2$ are the components of the posterior mean vector, and $(\\mathbf{S}_N)_{12}$ is the off-diagonal element of the posterior covariance matrix (representing $\\operatorname{Cov}(W, r)$).\n\n3.  **Identifiability Indicator**: Parameter identifiability from the data alone (in a frequentist sense, i.e., unique maximum likelihood estimate) requires that the columns of the design matrix $\\mathbf{X}$ be linearly independent. Since $\\mathbf{X}$ is an $N \\times 2$ matrix, this is equivalent to its rank being equal to $2$.\n    $$\n    \\text{identifiable} = (\\operatorname{rank}(\\mathbf{X}) = 2)\n    $$\n    In a Bayesian context, the prior can ensure a well-defined posterior even if the rank is deficient, but the rank of $\\mathbf{X}$ still indicates how much information is provided by the data versus the prior.\n\nThe algorithm for each test case is as follows:\n1.  Define all constants, ground-truth parameters ($W^\\star, \\tau^\\star, r^\\star$), prior parameters ($\\mathbf{m}_0, \\mathbf{S}_0$), and test-case specific parameters ($\\boldsymbol{\\kappa}, \\mathbf{V}, \\sigma$).\n2.  Construct the $N \\times 2$ design matrix $\\mathbf{X}$ from $\\boldsymbol{\\kappa}$ and $\\mathbf{V}$.\n3.  Compute the identifiability indicator by calculating the rank of $\\mathbf{X}$.\n4.  Generate synthetic observation data $\\mathbf{y}$. First, form the true parameter vector $\\boldsymbol{\\theta}^\\star = [W^\\star, r^\\star]^\\top$. Calculate the mean response $\\mathbf{X}\\boldsymbol{\\theta}^\\star$. Then, generate a noise vector $\\boldsymbol{\\varepsilon}$ of length $N$ by drawing from $\\mathcal{N}(0, \\sigma^2)$. The observations are $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta}^\\star + \\boldsymbol{\\varepsilon}$. A fixed random seed ensures reproducibility.\n5.  Compute the posterior parameters $\\mathbf{S}_N$ and $\\mathbf{m}_N$ using the provided matrix equations. This involves matrix inversion and multiplication.\n6.  Extract the posterior mean of $W$ from the first element of $\\mathbf{m}_N$.\n7.  Calculate the posterior mean of $\\tau$ using the formula involving the elements of $\\mathbf{m}_N$ and $\\mathbf{S}_N$.\n8.  Store the three computed values: $\\mathbb{E}[W\\mid\\mathbf{y}]$, $\\mathbb{E}[\\tau\\mid\\mathbf{y}]$, and the identifiability boolean.\nThis procedure will be repeated for all four test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian parameter estimation problem for dendritic solidification.\n\n    For each test case, the function:\n    1. Generates synthetic observation data based on ground-truth parameters.\n    2. Performs a Bayesian update to find the posterior distribution of the parameters.\n    3. Computes the posterior means of the interface width W and relaxation time tau.\n    4. Determines if the parameters are identifiable from the data alone.\n    \"\"\"\n    \n    # Set a fixed random seed for reproducibility of noise generation\n    np.random.seed(0)\n\n    # Shared parameters across all test cases\n    W_star = 1.0e-7  # Ground-truth interface width [m]\n    tau_star = 8.0e-7 # Ground-truth relaxation time [s]\n    r_star = tau_star / W_star  # Ground-truth r = tau/W [s/m]\n    theta_star = np.array([W_star, r_star])\n\n    c1 = 5.0  # Constant c1 [K]\n    c2 = 2.0  # Constant c2 [K]\n\n    # Prior distribution parameters\n    m0 = np.array([9.0e-8, 7.0])\n    S0 = np.diag([(2.0e-8)**2, 2.0**2])\n    S0_inv = np.linalg.inv(S0)\n    S0_inv_m0 = S0_inv @ m0\n\n    # Test cases definition\n    test_cases = [\n        {\n            \"kappa\": np.array([8.0e5, 1.2e6, 9.0e5, 7.0e5, 1.1e6, 1.0e6]),\n            \"V\": np.array([2.0e-2, 5.0e-2, 3.0e-2, 1.0e-2, 4.0e-2, 6.0e-2]),\n            \"sigma\": 5.0e-2\n        },\n        {\n            \"kappa\": np.array([8.0e5, 1.2e6, 9.0e5, 7.0e5, 1.1e6, 1.0e6]),\n            \"V\": np.array([2.0e-2, 5.0e-2, 3.0e-2, 1.0e-2, 4.0e-2, 6.0e-2]),\n            \"sigma\": 5.0e-1\n        },\n        {\n            \"V\": np.array([1.0e-2, 2.0e-2, 3.0e-2, 4.0e-2]),\n            \"kappa\": 5.0e7 * np.array([1.0e-2, 2.0e-2, 3.0e-2, 4.0e-2]), # kappa is proportional to V\n            \"sigma\": 5.0e-2\n        },\n        {\n            \"kappa\": np.array([1.0e6]),\n            \"V\": np.array([3.0e-2]),\n            \"sigma\": 5.0e-2\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        kappa = case[\"kappa\"]\n        V = case[\"V\"]\n        sigma = case[\"sigma\"]\n        sigma_sq = sigma**2\n        N = len(kappa)\n\n        # 1. Construct the design matrix X\n        X = np.zeros((N, 2))\n        X[:, 0] = c1 * kappa\n        X[:, 1] = c2 * V\n\n        # 2. Check for identifiability\n        identifiable = (np.linalg.matrix_rank(X) == 2)\n\n        # 3. Generate synthetic observation data y\n        mean_y = X @ theta_star\n        epsilon = np.random.normal(0, sigma, N)\n        y = mean_y + epsilon\n        \n        # 4. Perform Bayesian update\n        # Posterior covariance S_N\n        XTX = X.T @ X\n        SN_inv = S0_inv + (1 / sigma_sq) * XTX\n        SN = np.linalg.inv(SN_inv)\n\n        # Posterior mean m_N\n        XTY = X.T @ y\n        mN = SN @ (S0_inv_m0 + (1 / sigma_sq) * XTY)\n        \n        # 5. Compute posterior means of W and tau\n        E_W_post = mN[0]\n        E_tau_post = mN[0] * mN[1] + SN[0, 1]\n\n        results.append([E_W_post, E_tau_post, identifiable])\n        \n    # Format and print the final output as a single-line string\n    print(f\"[{','.join(map(str, results))}]\")\n\n\nsolve()\n```"
        }
    ]
}