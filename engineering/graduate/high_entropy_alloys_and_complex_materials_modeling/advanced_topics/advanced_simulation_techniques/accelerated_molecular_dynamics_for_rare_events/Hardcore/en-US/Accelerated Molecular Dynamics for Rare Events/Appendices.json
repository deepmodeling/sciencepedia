{
    "hands_on_practices": [
        {
            "introduction": "The core principle of many accelerated molecular dynamics methods, like hyperdynamics, is the introduction of a bias potential $\\Delta V$ to speed up escapes from metastable states. To recover the true physical timescale, this distortion must be corrected. This practice will guide you through implementing the fundamental time-reweighting procedure, where the effective time $t_{\\mathrm{eff}}$ is calculated by integrating a \"boost factor\" along the biased trajectory . Mastering this calculation is the first step toward turning biased simulation frames into physically meaningful kinetics.",
            "id": "3729122",
            "problem": "Consider a trajectory of an atomic configuration evolving under a biased potential in an accelerated Molecular Dynamics (MD) scheme for rare events (hyperdynamics). Let the unbiased potential be denoted by $U(\\mathbf{r})$ and let the bias be a nonnegative, time-dependent function $\\Delta V(t)$ added in metastable basins so that the biased potential is $U_{b}(\\mathbf{r}, t) = U(\\mathbf{r}) + \\Delta V(t)$. The goal is to recover the physically correct clock time advanced by the simulation, consistent with Transition State Theory (TST), by accumulating a boost along the simulated path. The foundational base for this mapping is that escape rates in TST scale as $\\exp(-\\beta U)$ for a thermal system at temperature $T$, where $\\beta = 1/(k_{B} T)$ and $k_{B}$ is the Boltzmann constant. Under the addition of a bias $\\Delta V$, the microscopic time scale is distorted. To obtain the effective physical time, the infinitesimal time increment $dt$ in the biased simulation must be reweighted by the factor $\\exp(\\beta \\Delta V(t))$, and the total effective time is the path integral\n$$\nt_{\\mathrm{eff}} = \\int_{0}^{t_{\\mathrm{sim}}} \\exp\\!\\big(\\beta \\Delta V(t)\\big)\\, dt,\n$$\nwhere $t_{\\mathrm{sim}}$ is the biased simulation clock time. The quantity $t_{\\mathrm{eff}}$ must be expressed in seconds.\n\nImplement a complete program that:\n- Computes $\\beta = \\frac{1}{k_{B} T}$ using $k_{B} = 8.617333262145 \\times 10^{-5}$ electronvolts per Kelvin (eV/K), with $T$ in Kelvin and $\\Delta V$ in electronvolts (eV).\n- Given a discrete trajectory with time points $\\{t_i\\}$ in seconds and corresponding bias values $\\{\\Delta V(t_i)\\}$ in electronvolts, computes\n$$\nt_{\\mathrm{eff}} \\approx \\int \\exp\\!\\big(\\beta \\Delta V(t)\\big)\\, dt\n$$\nnumerically on the provided grid in a manner accurate for nonuniform time steps.\n- Produces a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$, where each result is a floating-point number in seconds.\n\nUse the following test suite to validate correctness and coverage (all outputs must be in seconds):\n\n- Test Case $1$ (constant bias, uniform grid, baseline accelerated time): $T = 300$ Kelvin, $t_{\\mathrm{sim}} = 1.0 \\times 10^{-9}$ seconds, $N = 1001$ equally spaced points from $t = 0$ to $t = t_{\\mathrm{sim}}$, and $\\Delta V(t) = 0.1$ electronvolts for all $t$. This tests the case $\\exp(\\beta \\Delta V)$ constant in time.\n\n- Test Case $2$ (time-dependent nonnegative bias, uniform grid, sinusoidal variability): $T = 600$ Kelvin, $t_{\\mathrm{sim}} = 2.0 \\times 10^{-8}$ seconds, $N = 20001$ equally spaced points, frequency $f = 5.0 \\times 10^{9}$ Hertz, amplitude $A = 0.05$ electronvolts, and $\\Delta V(t) = A \\big(1 + \\sin(2\\pi f t)\\big)$ for all $t$. This tests nontrivial time dependence with nonnegative bias.\n\n- Test Case $3$ (piecewise bias, nonuniform grid): $T = 450$ Kelvin, nonuniform time points $t = [0, 2.0\\times 10^{-12}, 5.0\\times 10^{-12}, 1.1\\times 10^{-11}, 1.5\\times 10^{-11}]$ seconds, and corresponding $\\Delta V$ values $[0.2, 0.2, 0.1, 0.0, 0.0]$ electronvolts. This tests nonuniform temporal sampling and a bias that goes to zero near exit.\n\n- Test Case $4$ (zero bias baseline): $T = 300$ Kelvin, $t_{\\mathrm{sim}} = 1.0 \\times 10^{-9}$ seconds, $N = 1001$ equally spaced points, and $\\Delta V(t) = 0.0$ electronvolts for all $t$. This verifies that $t_{\\mathrm{eff}}$ reduces to $t_{\\mathrm{sim}}$ when there is no bias.\n\nYour program must produce a single line of output containing the results of the four test cases as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4]$, where $x_i$ are floating-point numbers representing $t_{\\mathrm{eff}}$ in seconds.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of accelerated molecular dynamics, specifically the hyperdynamics method. The physical parameters and mathematical formulation are consistent and well-posed, permitting a unique numerical solution.\n\nThe fundamental principle underlying this problem is the reweighting of simulation time in a biased molecular dynamics trajectory to recover the true physical time evolution of the system. In statistical mechanics, the rate of thermally activated rare events, such as atomic diffusion or chemical reactions, is governed by the Arrhenius law, which states that the rate $k$ is proportional to $\\exp(-\\beta E_a)$, where $E_a$ is the activation energy barrier and $\\beta = 1/(k_B T)$ is the inverse thermal energy. Here, $k_B$ is the Boltzmann constant and $T$ is the absolute temperature.\n\nIn the hyperdynamics method, a non-negative bias potential, $\\Delta V(\\mathbf{r}) \\ge 0$, is added to the true potential energy surface $U(\\mathbf{r})$ within a metastable state. The dynamics are then evolved on the biased potential surface $U_b(\\mathbf{r}) = U(\\mathbf{r}) + \\Delta V(\\mathbf{r})$. This bias potential effectively lowers the activation barriers, accelerating the escape from the potential well. The key requirement for the bias to be valid is that it must be zero at the transition state surface separating basins, so as not to alter the relative probabilities of different escape pathways.\n\nThe acceleration of the dynamics comes at the cost of distorting the time scale. To recover the real physical time, each infinitesimal step $dt$ of the biased simulation must be reweighted by a \"boost factor.\" This factor is derived from the ratio of probabilities (or residence times) in the biased and unbiased ensembles. In a state characterized by coordinates $\\mathbf{r}$, the probability density is proportional to $\\exp(-\\beta U(\\mathbf{r}))$. With the bias, it becomes $\\exp(-\\beta U_b(\\mathbf{r})) = \\exp(-\\beta U(\\mathbf{r})) \\exp(-\\beta \\Delta V(\\mathbf{r}))$. The simulation spends a time that is enhanced by a local factor of $\\exp(\\beta \\Delta V(\\mathbf{r},t))$. To obtain the correct total elapsed physical time, $t_{\\mathrm{eff}}$, we must integrate this boost factor along the biased simulation trajectory of duration $t_{\\mathrm{sim}}$:\n$$\nt_{\\mathrm{eff}} = \\int_{0}^{t_{\\mathrm{sim}}} \\exp\\!\\big(\\beta \\Delta V(t)\\big)\\, dt\n$$\nHere, $\\Delta V(t)$ represents the value of the bias potential at the system's position $\\mathbf{r}(t)$ at simulation time $t$. The problem provides the time series of the bias potential directly.\n\nThe task is to numerically evaluate this integral for discrete time series data $\\{t_i\\}$ and corresponding bias potential values $\\{\\Delta V(t_i)\\}$. A robust method for numerical integration, which is accurate for both uniform and non-uniform grids as required by the problem statement, is the trapezoidal rule. For a function $f(t)$ sampled at points $(t_i, f_i)$, the integral is approximated by summing the areas of the trapezoids formed between consecutive points:\n$$\n\\int_{t_0}^{t_{N-1}} f(t) \\, dt \\approx \\sum_{i=0}^{N-2} \\frac{f(t_i) + f(t_{i+1})}{2} (t_{i+1} - t_i)\n$$\nIn our case, the integrand is $f(t_i) = \\exp(\\beta \\Delta V(t_i))$.\n\nThe computational implementation will proceed as follows for each test case:\n1.  Compute the inverse thermal energy $\\beta = \\frac{1}{k_B T}$, ensuring consistent units. With $k_B$ in $\\mathrm{eV}/\\mathrm{K}$, $T$ in $\\mathrm{K}$, and $\\Delta V$ in $\\mathrm{eV}$, the product $\\beta \\Delta V$ is correctly dimensionless.\n2.  Generate the discrete grid of time points $\\{t_i\\}$ and the corresponding bias potential values $\\{\\Delta V_i = \\Delta V(t_i)\\}$. For uniform grids, this involves using `np.linspace`. For non-uniform grids, the provided points are used directly. For functional forms of $\\Delta V(t)$, the function is evaluated on the time grid.\n3.  Calculate the integrand values $y_i = \\exp(\\beta \\Delta V_i)$ for each point $i$.\n4.  Utilize a numerical integration routine implementing the trapezoidal rule, such as `numpy.trapz`, to compute the definite integral. This function takes the array of integrand values $\\{y_i\\}$ and the array of time points $\\{t_i\\}$ as input, correctly handling both uniform and non-uniform spacing between the points $t_i$.\n\n- **Test Case 1** (constant bias): The integrand $\\exp(\\beta \\Delta V)$ is constant. The integral simplifies to $t_{\\mathrm{eff}} = \\exp(\\beta \\Delta V) \\times t_{\\mathrm{sim}}$. The numerical method should accurately reproduce this analytical result.\n- **Test Case 2** (sinusoidal bias): The full time-dependent integrand must be evaluated at each point on the fine, uniform grid, followed by numerical integration.\n- **Test Case 3** (piecewise bias, non-uniform grid): This case specifically tests the robustness of the integration method on a non-uniformly sampled grid. The trapezoidal rule is explicitly designed to handle this.\n- **Test Case 4** (zero bias): With $\\Delta V = 0$, the boost factor is $\\exp(0) = 1$. The integral must return $t_{\\mathrm{eff}} = \\int_0^{t_{\\mathrm{sim}}} 1 \\, dt = t_{\\mathrm{sim}}$, serving as a crucial validation of the implementation's baseline correctness.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the accelerated molecular dynamics time-recovery problem for four test cases.\n    \"\"\"\n\n    # Boltzmann constant in eV/K\n    k_B = 8.617333262145e-5\n\n    def compute_teff(T, t_points, delta_V_points):\n        \"\"\"\n        Computes the effective physical time from a biased MD trajectory.\n\n        Args:\n            T (float): Temperature in Kelvin.\n            t_points (np.ndarray): Array of time points in seconds.\n            delta_V_points (np.ndarray): Array of bias potential values in eV.\n\n        Returns:\n            float: The effective time t_eff in seconds.\n        \"\"\"\n        if T <= 0:\n            raise ValueError(\"Temperature must be positive.\")\n        \n        # Calculate inverse temperature in 1/eV\n        beta = 1.0 / (k_B * T)\n        \n        # Calculate the integrand, exp(beta * DeltaV(t))\n        integrand = np.exp(beta * delta_V_points)\n        \n        # Numerically integrate using the trapezoidal rule, which is suitable\n        # for both uniform and non-uniform grids.\n        t_eff = np.trapz(integrand, t_points)\n        \n        return t_eff\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: Constant bias, uniform grid\n        {\n            \"T\": 300.0,\n            \"t_sim\": 1.0e-9,\n            \"N\": 1001,\n            \"delta_V_func\": lambda t: 0.1\n        },\n        # Test Case 2: Time-dependent bias, uniform grid\n        {\n            \"T\": 600.0,\n            \"t_sim\": 2.0e-8,\n            \"N\": 20001,\n            \"delta_V_func\": lambda t: 0.05 * (1 + np.sin(2 * np.pi * 5.0e9 * t))\n        },\n        # Test Case 3: Piecewise bias, nonuniform grid\n        {\n            \"T\": 450.0,\n            \"t_points\": np.array([0.0, 2.0e-12, 5.0e-12, 1.1e-11, 1.5e-11]),\n            \"delta_V_points\": np.array([0.2, 0.2, 0.1, 0.0, 0.0])\n        },\n        # Test Case 4: Zero bias baseline\n        {\n            \"T\": 300.0,\n            \"t_sim\": 1.0e-9,\n            \"N\": 1001,\n            \"delta_V_func\": lambda t: 0.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        T = case[\"T\"]\n        \n        if \"t_points\" in case:\n            # Case with explicitly defined non-uniform grid\n            t_points = case[\"t_points\"]\n            delta_V_points = case[\"delta_V_points\"]\n        else:\n            # Cases with uniform grid and functional form of delta_V\n            t_sim = case[\"t_sim\"]\n            N = case[\"N\"]\n            delta_V_func = case[\"delta_V_func\"]\n            \n            t_points = np.linspace(0, t_sim, N)\n            \n            # For constant functions, we can create a full array\n            if np.isscalar(delta_V_func(0)):\n                delta_V_points = np.full(N, delta_V_func(0))\n            else: # For time-dependent functions\n                delta_V_points = delta_V_func(t_points)\n\n        result = compute_teff(T, t_points, delta_V_points)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Observing rare events is only half the battle; the ultimate goal is to quantify their kinetics. Once a series of exit events has been collected from an accelerated simulation, you must process the biased residence times to estimate the true, unbiased physical rate constant, $k$. This exercise demonstrates how to use the powerful technique of Maximum Likelihood Estimation (MLE) to find the most probable value of $k$ from your data and to construct a confidence interval that quantifies its statistical uncertainty .",
            "id": "3729149",
            "problem": "In accelerated molecular dynamics applied to a refractory high-entropy alloy, consider the escape from a metastable micro-basin associated with a single-vacancy migration event under a bias potential constructed in hyperdynamics. Assume that the unbiased residence time in the basin is modeled as an independent and identically distributed exponential random variable with rate $k$, meaning the probability density of an individual unbiased residence time $t$ is $f(t \\mid k) = k \\exp(-k t)$ for $t \\ge 0$. In the simulation, a constant boost factor is maintained within each residence episode, so that the unbiased residence time $t_i$ for episode $i$ is related to the observed biased residence time $\\tau_i^{b}$ via $t_i = s_i \\, \\tau_i^{b}$, where $s_i$ is the known per-episode acceleration (boost) factor.\n\nYour tasks are:\n- Starting from the independence of the $t_i$ and the exponential model, derive the maximum likelihood estimator for the exit rate $k$ in terms of the unbiased residence times $\\{t_i\\}_{i=1}^{n}$.\n- For a finite sample of size $n$, construct a two-sided confidence interval for $k$ using the large-sample normal approximation grounded in the Fisher information. Express the interval in terms of the estimator, the sample size, and the standard normal quantile corresponding to a two-sided $95\\%$ level.\n- Using the following observed data for $n = 4$ escape events,\n  - Episode $1$: $\\tau_1^{b} = 0.50 \\ \\mathrm{ps}$, $s_1 = 20$,\n  - Episode $2$: $\\tau_2^{b} = 0.40 \\ \\mathrm{ps}$, $s_2 = 25$,\n  - Episode $3$: $\\tau_3^{b} = 1.00 \\ \\mathrm{ps}$, $s_3 = 10$,\n  - Episode $4$: $\\tau_4^{b} = 0.25 \\ \\mathrm{ps}$, $s_4 = 40$,\n  compute the reweighted unbiased residence times $\\{t_i\\}$, evaluate your estimator and the $95\\%$ confidence interval numerically.\n\nAnswer specification:\n- Express the final rate and the two confidence interval bounds in $\\mathrm{ps}^{-1}$.\n- Report the final result as three numbers ordered as $\\big(k_{\\text{MLE}}, k_{\\text{lower}}, k_{\\text{upper}}\\big)$.\n- Round each reported number to four significant figures.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of statistical mechanics and computational materials science, specifically accelerated molecular dynamics. It is well-posed, providing all necessary information and a clear objective. The language is objective and the setup is internally consistent.\n\nFirst, we derive the maximum likelihood estimator (MLE) for the exit rate, $k$. The problem states that the unbiased residence times, $\\{t_i\\}_{i=1}^{n}$, are independent and identically distributed random variables from an exponential distribution with probability density function $f(t_i | k) = k \\exp(-k t_i)$. The likelihood function, $L(k)$, for a sample of size $n$ is the product of the individual probability densities:\n$$L(k; \\{t_i\\}_{i=1}^{n}) = \\prod_{i=1}^{n} f(t_i | k) = \\prod_{i=1}^{n} k \\exp(-k t_i) = k^n \\exp\\left(-k \\sum_{i=1}^{n} t_i\\right)$$\nTo find the maximum of $L(k)$, we first take the natural logarithm to obtain the log-likelihood function, $\\ell(k)$:\n$$\\ell(k) = \\ln(L(k)) = \\ln\\left(k^n \\exp\\left(-k \\sum_{i=1}^{n} t_i\\right)\\right) = n \\ln(k) - k \\sum_{i=1}^{n} t_i$$\nWe then differentiate $\\ell(k)$ with respect to $k$ and set the result to zero to find the value of $k$ that maximizes the log-likelihood:\n$$\\frac{d\\ell(k)}{dk} = \\frac{n}{k} - \\sum_{i=1}^{n} t_i = 0$$\nSolving for $k$ gives the maximum likelihood estimator, denoted $\\hat{k}$:\n$$\\hat{k} = \\frac{n}{\\sum_{i=1}^{n} t_i}$$\nThis shows that the MLE for the rate $k$ is the inverse of the sample mean of the unbiased residence times.\n\nNext, we construct a two-sided $95\\%$ confidence interval for $k$ using the large-sample normal approximation. The asymptotic variance of an MLE is given by the inverse of the Fisher information. The Fisher information for a single observation, $I(k)$, is defined as $I(k) = -E\\left[\\frac{d^2 \\ln f(t|k)}{dk^2}\\right]$.\nThe first derivative of the log-PDF is $\\frac{d \\ln f(t|k)}{dk} = \\frac{1}{k} - t$. The second derivative is:\n$$\\frac{d^2 \\ln f(t|k)}{dk^2} = -\\frac{1}{k^2}$$\nSince the second derivative is a constant with respect to $t$, its expectation is the constant itself. Thus, the Fisher information for one observation is:\n$$I(k) = -E\\left[-\\frac{1}{k^2}\\right] = \\frac{1}{k^2}$$\nFor a sample of $n$ independent observations, the total Fisher information is $I_n(k) = n I(k) = \\frac{n}{k^2}$. The asymptotic variance of the MLE, $\\hat{k}$, is the inverse of $I_n(k)$:\n$$\\text{Var}(\\hat{k}) \\approx [I_n(k)]^{-1} = \\frac{k^2}{n}$$\nThe standard error of $\\hat{k}$ is estimated by substituting $\\hat{k}$ for $k$:\n$$SE(\\hat{k}) = \\sqrt{\\frac{\\hat{k}^2}{n}} = \\frac{\\hat{k}}{\\sqrt{n}}$$\nFor large $n$, the distribution of $\\hat{k}$ is approximately normal, $\\hat{k} \\sim N\\left(k, \\frac{k^2}{n}\\right)$. A two-sided $100(1-\\alpha)\\%$ confidence interval for $k$ is given by $\\hat{k} \\pm z_{1-\\alpha/2} SE(\\hat{k})$, where $z_{1-\\alpha/2}$ is the upper $(1-\\alpha/2)$ quantile of the standard normal distribution. For a $95\\%$ confidence level, $\\alpha=0.05$, so we use $z_{1-0.05/2} = z_{0.975} \\approx 1.96$. The confidence interval is:\n$$ \\left( \\hat{k} - z_{0.975} \\frac{\\hat{k}}{\\sqrt{n}}, \\hat{k} + z_{0.975} \\frac{\\hat{k}}{\\sqrt{n}} \\right) = \\hat{k} \\left( 1 \\pm \\frac{z_{0.975}}{\\sqrt{n}} \\right) $$\n\nFinally, we compute the numerical values using the provided data. First, we calculate the unbiased residence times $t_i = s_i \\tau_i^b$:\n- $t_1 = 20 \\times 0.50 \\ \\mathrm{ps} = 10.0 \\ \\mathrm{ps}$\n- $t_2 = 25 \\times 0.40 \\ \\mathrm{ps} = 10.0 \\ \\mathrm{ps}$\n- $t_3 = 10 \\times 1.00 \\ \\mathrm{ps} = 10.0 \\ \\mathrm{ps}$\n- $t_4 = 40 \\times 0.25 \\ \\mathrm{ps} = 10.0 \\ \\mathrm{ps}$\n\nThe sum of the unbiased residence times is $\\sum_{i=1}^{4} t_i = 10.0+10.0+10.0+10.0 = 40.0 \\ \\mathrm{ps}$.\nThe sample size is $n=4$. The MLE for $k$ is:\n$$\\hat{k}_{\\text{MLE}} = \\frac{4}{40.0 \\ \\mathrm{ps}} = 0.1 \\ \\mathrm{ps}^{-1}$$\nFor the confidence interval, we have $n=4$ so $\\sqrt{n}=2$, and $z_{0.975} \\approx 1.96$.\nThe lower bound of the confidence interval, $k_{\\text{lower}}$, is:\n$$k_{\\text{lower}} = 0.1 \\left( 1 - \\frac{1.96}{2} \\right) = 0.1 (1 - 0.98) = 0.1(0.02) = 0.002 \\ \\mathrm{ps}^{-1}$$\nThe upper bound of the confidence interval, $k_{\\text{upper}}$, is:\n$$k_{\\text{upper}} = 0.1 \\left( 1 + \\frac{1.96}{2} \\right) = 0.1 (1 + 0.98) = 0.1(1.98) = 0.198 \\ \\mathrm{ps}^{-1}$$\nThe problem asks for the results to be rounded to four significant figures.\n- $\\hat{k}_{\\text{MLE}} = 0.1 \\ \\mathrm{ps}^{-1}$ becomes $0.1000 \\ \\mathrm{ps}^{-1}$.\n- $k_{\\text{lower}} = 0.002 \\ \\mathrm{ps}^{-1}$ becomes $0.002000 \\ \\mathrm{ps}^{-1}$.\n- $k_{\\text{upper}} = 0.198 \\ \\mathrm{ps}^{-1}$ becomes $0.1980 \\ \\mathrm{ps}^{-1}$.\nThe final result is the ordered triple $(k_{\\text{MLE}}, k_{\\text{lower}}, k_{\\text{upper}})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1000 & 0.002000 & 0.1980\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The validity of time acceleration in methods like hyperdynamics or Temperature Accelerated Dynamics (TAD) hinges on a critical assumption: the escape from a metastable basin is a memoryless, Poisson process. This implies that the distribution of unbiased exit times should be exponential. This practice will equip you with a rigorous statistical tool, the Likelihood Ratio Test, to check for deviations from this essential exponential behavior, allowing you to validate your simulation's results or diagnose underlying problems in the model .",
            "id": "3729086",
            "problem": "You are modeling rare-event kinetics in a High-Entropy Alloy (HEA) using Accelerated Molecular Dynamics (AMD). Under the Markovian assumption and in the presence of a Quasi-Stationary Distribution (QSD) within a metastable basin, the exit time from that basin is memoryless with a constant hazard rate. From first principles, this implies that the exit-time distribution is exponential with a single rate parameter. Deviations from exponentiality indicate either non-Markovian effects, heterogeneous microstates within the basin, or violations of AMD assumptions (e.g., incorrect boost factors or insufficient decorrelation), which invalidate the time-acceleration mapping and yield biased kinetics.\n\nStarting from the definitions of the survival function, hazard function, and Maximum Likelihood Estimation (MLE) under independent exit times, derive and implement a statistical test that detects deviations from exponential exit times. Formulate this test as a Likelihood Ratio Test (LRT) that compares an exponential model (constant hazard) against a more general Weibull model (time-dependent hazard). Your program must:\n\n- Derive the log-likelihood for the exponential and Weibull models from first principles and compute the LRT statistic.\n- Use the asymptotic distribution of the LRT statistic under the null hypothesis to compute a $p$-value.\n- Return a boolean decision for each dataset indicating whether the null hypothesis of exponential exit times is rejected at a significance level $\\alpha = 0.05$.\n\nDefinitions to use:\n- The survival function is $S(t) = \\mathbb{P}(T > t)$ and the hazard function is $h(t) = \\frac{f(t)}{S(t)}$, where $f(t)$ is the probability density function of the exit time $T$.\n- Under the null hypothesis of a constant hazard, $T$ is exponentially distributed with scale parameter $\\lambda > 0$, so $f(t) = \\frac{1}{\\lambda} \\exp\\!\\left(-\\frac{t}{\\lambda}\\right)$ for $t \\ge 0$.\n- Under the alternative, $T$ follows a Weibull distribution with shape $k > 0$ and scale $\\lambda > 0$, so $f(t) = \\frac{k}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{k-1} \\exp\\!\\left(-\\left(\\frac{t}{\\lambda}\\right)^{k}\\right)$ for $t \\ge 0$.\n\nPhysical units and data generation:\n- Exit times must be treated in seconds and generated as independent samples in seconds.\n- For reproducibility, use a pseudorandom generator with a fixed seed $s = 314159$.\n- Construct the following test suite of datasets, each a list of independent exit times in seconds:\n    1. Exponential exit times with scale $\\lambda = 2.0$ seconds, sample size $n = 200$.\n    2. Weibull exit times with shape $k = 0.7$ and scale $\\lambda = 2.0$ seconds, sample size $n = 200$.\n    3. Mixture of exponentials: with probability $0.5$ draw from $\\text{Exponential}(\\lambda_1 = 1.0)$ seconds and with probability $0.5$ draw from $\\text{Exponential}(\\lambda_2 = 3.3333333333)$ seconds, sample size $n = 200$.\n    4. Exponential exit times with scale $\\lambda = 2.0$ seconds, sample size $n = 20$ (boundary case: low statistical power).\n    5. Weibull exit times with shape $k = 1.5$ and scale $\\lambda = 2.0$ seconds, sample size $n = 200$.\n\nAlgorithmic requirements:\n- Estimate the exponential scale parameter $\\lambda$ under the null hypothesis by MLE based on the given data.\n- Estimate the Weibull shape $k$ and scale $\\lambda$ under the alternative hypothesis by MLE based on the given data.\n- Compute the LRT statistic using the difference in maximized log-likelihoods and obtain a $p$-value from the chi-square distribution with one degree of freedom.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, in the same order as the datasets above (e.g., \"[True,False,True,False,True]\"). Each boolean must indicate whether the null hypothesis of exponential exit times is rejected at $\\alpha = 0.05$ for that dataset.",
            "solution": "The problem requires the derivation and implementation of a Likelihood Ratio Test (LRT) to validate the assumption of exponential exit times in Accelerated Molecular Dynamics (AMD) simulations. This assumption is fundamental, as it stems from the simulation setup approximating a memoryless, Markovian process for rare events. Deviations from an exponential distribution, which has a constant hazard rate, suggest that the underlying kinetics are more complex (e.g., non-Markovian or involving heterogeneous states) and that the time-acceleration obtained from AMD may be biased. We will compare the null hypothesis of an exponential distribution against a more general alternative, the Weibull distribution, which allows for a time-dependent hazard rate.\n\n### 1. Theoretical Formulation of the Likelihood Ratio Test\n\nLet the observed exit times be a set of $n$ independent and identically distributed (i.i.d.) positive random variables $\\mathbf{T} = \\{T_1, T_2, \\ldots, T_n\\}$, with observed values $\\mathbf{t} = \\{t_1, t_2, \\ldots, t_n\\}$. The likelihood function is given by $L(\\theta | \\mathbf{t}) = \\prod_{i=1}^n f(t_i | \\theta)$, where $f(t|\\theta)$ is the probability density function (PDF) parameterized by $\\theta$. We will work with the log-likelihood, $\\ell(\\theta | \\mathbf{t}) = \\ln L(\\theta | \\mathbf{t}) = \\sum_{i=1}^n \\ln f(t_i | \\theta)$.\n\n#### Null Hypothesis ($H_0$): Exponential Exit Times\n\nUnder the null hypothesis, the exit times follow an exponential distribution, characterized by a constant hazard rate. The PDF is given by:\n$$\nf(t|\\lambda) = \\frac{1}{\\lambda} \\exp\\left(-\\frac{t}{\\lambda}\\right), \\quad t \\ge 0, \\lambda > 0\n$$\nwhere $\\lambda$ is the scale parameter, representing the mean exit time. The log-likelihood function for a sample $\\mathbf{t}$ is:\n$$\n\\ell_0(\\lambda | \\mathbf{t}) = \\sum_{i=1}^n \\ln\\left(\\frac{1}{\\lambda} \\exp\\left(-\\frac{t_i}{\\lambda}\\right)\\right) = \\sum_{i=1}^n \\left(-\\ln\\lambda - \\frac{t_i}{\\lambda}\\right) = -n\\ln\\lambda - \\frac{1}{\\lambda}\\sum_{i=1}^n t_i\n$$\nTo find the Maximum Likelihood Estimator (MLE) for $\\lambda$, we differentiate $\\ell_0$ with respect to $\\lambda$ and set the result to zero:\n$$\n\\frac{\\partial \\ell_0}{\\partial \\lambda} = -\\frac{n}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n t_i = 0\n$$\nSolving for $\\lambda$ gives the MLE, denoted $\\hat{\\lambda}_0$:\n$$\n\\hat{\\lambda}_0 = \\frac{1}{n}\\sum_{i=1}^n t_i = \\bar{t}\n$$\nThe MLE for the exponential scale parameter is the sample mean. The maximized log-likelihood under $H_0$, denoted $\\hat{\\ell}_0$, is found by substituting $\\hat{\\lambda}_0$ back into $\\ell_0(\\lambda | \\mathbf{t})$:\n$$\n\\hat{\\ell}_0 = \\ell_0(\\hat{\\lambda}_0 | \\mathbf{t}) = -n\\ln(\\bar{t}) - \\frac{1}{\\bar{t}}\\sum_{i=1}^n t_i = -n(\\ln(\\bar{t}) + 1)\n$$\n\n#### Alternative Hypothesis ($H_1$): Weibull Exit Times\n\nUnder the alternative hypothesis, we use the two-parameter Weibull distribution, which accommodates a time-dependent hazard function $h(t) \\propto t^{k-1}$. The PDF is:\n$$\nf(t|k, \\lambda) = \\frac{k}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{t}{\\lambda}\\right)^k\\right), \\quad t \\ge 0, k > 0, \\lambda > 0\n$$\nHere, $k$ is the shape parameter and $\\lambda$ is the scale parameter. The exponential distribution is a special case of the Weibull distribution where $k=1$. The log-likelihood function for a sample $\\mathbf{t}$ is:\n$$\n\\ell_1(k, \\lambda | \\mathbf{t}) = \\sum_{i=1}^n \\ln\\left[\\frac{k}{\\lambda}\\left(\\frac{t_i}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{t_i}{\\lambda}\\right)^k\\right)\\right] = n\\ln k - nk\\ln\\lambda + (k-1)\\sum_{i=1}^n \\ln t_i - \\sum_{i=1}^n\\left(\\frac{t_i}{\\lambda}\\right)^k\n$$\nThe MLEs for $k$ and $\\lambda$, denoted $(\\hat{k}_1, \\hat{\\lambda}_1)$, do not have a closed-form solution. They must be found by numerically maximizing $\\ell_1(k, \\lambda | \\mathbf{t})$. The maximized log-likelihood under $H_1$, denoted $\\hat{\\ell}_1$, is $\\ell_1(\\hat{k}_1, \\hat{\\lambda}_1 | \\mathbf{t})$.\n\n#### The Likelihood Ratio Test Statistic\n\nThe LRT is used to compare two nested models. Since the exponential model is a special case of the Weibull model ($k=1$), they are nested. The LRT statistic, $\\Lambda$, is defined as twice the difference in the maximized log-likelihoods:\n$$\n\\Lambda = 2(\\hat{\\ell}_1 - \\hat{\\ell}_0)\n$$\nAccording to Wilks' theorem, under the null hypothesis $H_0$, the test statistic $\\Lambda$ asymptotically follows a chi-squared ($\\chi^2$) distribution. The number of degrees of freedom ($df$) is the difference in the number of free parameters between the alternative and null models. In our case, the Weibull model has two parameters ($k, \\lambda$) and the exponential model has one ($\\lambda$), so $df = 2 - 1 = 1$.\n$$\n\\Lambda \\sim \\chi^2_1 \\quad (\\text{under } H_0)\n$$\n\n### 2. Algorithmic Procedure and Decision Rule\n\nThe statistical test is implemented as follows for each dataset of exit times:\n\n1.  **Generate Datasets:** For reproducibility, a pseudorandom number generator with a fixed seed $s = 314159$ is used to construct the five specified test cases of exit times.\n\n2.  **Estimate under $H_0$:** Calculate the MLE of the exponential parameter, $\\hat{\\lambda}_0 = \\bar{t}$. Then compute the maximized log-likelihood, $\\hat{\\ell}_0 = -n(\\ln(\\hat{\\lambda}_0) + 1)$.\n\n3.  **Estimate under $H_1$:** Numerically find the parameters $(\\hat{k}_1, \\hat{\\lambda}_1)$ that maximize the Weibull log-likelihood function $\\ell_1(k, \\lambda | \\mathbf{t})$. This is achieved by minimizing the negative log-likelihood, $-\\ell_1(k, \\lambda | \\mathbf{t})$, using a numerical optimization algorithm such as L-BFGS-B, which can handle boundary constraints ($k>0, \\lambda>0$). The resulting maximized log-likelihood is $\\hat{\\ell}_1$.\n\n4.  **Compute Test Statistic and $p$-value:** Calculate the LRT statistic $\\Lambda = 2(\\hat{\\ell}_1 - \\hat{\\ell}_0)$. The value of $\\Lambda$ must be non-negative. Any small negative value resulting from floating-point inaccuracies is treated as zero. The $p$-value is then computed as the probability of observing a value from the $\\chi^2_1$ distribution that is greater than or equal to the calculated $\\Lambda$:\n    $$\n    p = \\mathbb{P}(\\chi^2_1 \\ge \\Lambda) = 1 - F_{\\chi^2_1}(\\Lambda)\n    $$\n    where $F_{\\chi^2_1}$ is the cumulative distribution function of the $\\chi^2_1$ distribution. This is equivalent to the survival function.\n\n5.  **Decision:** Compare the $p$-value to the pre-defined significance level $\\alpha = 0.05$. If $p < \\alpha$, we reject the null hypothesis $H_0$. This suggests that the exit times do not follow an exponential distribution and that the simpler constant-hazard model is inadequate. Otherwise, if $p \\ge \\alpha$, we fail to reject $H_0$.\n\nThis rigorous procedure provides a quantitative diagnostic tool for assessing the validity of critical assumptions in AMD simulations of complex materials.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Derives and implements a Likelihood Ratio Test to detect deviations from\n    exponential exit times in simulated molecular dynamics data.\n    \"\"\"\n\n    def generate_datasets(seed):\n        \"\"\"Generates the five test datasets as specified in the problem.\"\"\"\n        rng = np.random.default_rng(seed)\n        datasets = []\n\n        # Dataset 1: Exponential(lambda=2.0), n=200\n        datasets.append(rng.exponential(scale=2.0, size=200))\n\n        # Dataset 2: Weibull(k=0.7, lambda=2.0), n=200\n        # numpy.random.weibull(a) has scale=1, so we multiply by the desired scale.\n        datasets.append(2.0 * rng.weibull(a=0.7, size=200))\n\n        # Dataset 3: Mixture of exponentials, n=200\n        n3 = 200\n        lam1, lam2 = 1.0, 10.0 / 3.0  # Use fraction for precision\n        mixture_draws = rng.uniform(size=n3)\n        data3 = np.array([\n            rng.exponential(scale=lam1) if u < 0.5 \n            else rng.exponential(scale=lam2) \n            for u in mixture_draws\n        ])\n        datasets.append(data3)\n\n        # Dataset 4: Exponential(lambda=2.0), n=20 (low power)\n        datasets.append(rng.exponential(scale=2.0, size=20))\n\n        # Dataset 5: Weibull(k=1.5, lambda=2.0), n=200\n        datasets.append(2.0 * rng.weibull(a=1.5, size=200))\n        \n        return datasets\n\n    def perform_lrt(data, alpha):\n        \"\"\"\n        Performs the Likelihood Ratio Test on a single dataset.\n        \n        Args:\n            data (np.ndarray): Array of exit times.\n            alpha (float): Significance level.\n\n        Returns:\n            bool: True if the null hypothesis is rejected, False otherwise.\n        \"\"\"\n        n = len(data)\n        \n        # H0: Exponential distribution\n        # MLE for lambda is the sample mean.\n        lambda_exp_mle = np.mean(data)\n        # Maximized log-likelihood for the exponential model.\n        log_L0 = -n * (np.log(lambda_exp_mle) + 1.0)\n        \n        # H1: Weibull distribution\n        def neg_log_likelihood_weibull(params, t_data):\n            k, lam = params\n            if k <= 0 or lam <= 0:\n                return np.inf\n            \n            # To avoid log(0) if any data point is zero.\n            if np.any(t_data <= 0):\n                return np.inf\n\n            # Log-likelihood expression for Weibull\n            log_t = np.log(t_data)\n            term1 = n * np.log(k)\n            term2 = -n * k * np.log(lam)\n            term3 = (k - 1) * np.sum(log_t)\n            term4 = -np.sum((t_data / lam)**k)\n            \n            log_L = term1 + term2 + term3 + term4\n            return -log_L\n\n        # Initial guess for optimization: k=1 (exponential) and lambda=sample_mean\n        initial_guess = [1.0, lambda_exp_mle]\n        bounds = [(1e-9, None), (1e-9, None)]  # k > 0, lambda > 0\n        \n        opt_result = minimize(\n            neg_log_likelihood_weibull,\n            x0=initial_guess,\n            args=(data,),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n        \n        # Maximized log-likelihood for the Weibull model\n        log_L1 = -opt_result.fun\n        \n        # Compute the LRT statistic\n        # Should be non-negative, but can be slightly negative due to precision.\n        lrt_statistic = 2 * (log_L1 - log_L0)\n        if lrt_statistic < 0:\n            lrt_statistic = 0.0\n\n        # Compute p-value from chi-squared distribution with 1 degree of freedom\n        p_value = chi2.sf(lrt_statistic, df=1)\n\n        return p_value < alpha\n\n    # --- Main execution logic ---\n    \n    # Define parameters\n    seed = 314159\n    alpha = 0.05\n    \n    # Generate the test suite of datasets\n    test_cases = generate_datasets(seed)\n    \n    results = []\n    for data in test_cases:\n        # Perform the LRT for each dataset and store the boolean result\n        reject_h0 = perform_lrt(data, alpha)\n        results.append(reject_h0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}