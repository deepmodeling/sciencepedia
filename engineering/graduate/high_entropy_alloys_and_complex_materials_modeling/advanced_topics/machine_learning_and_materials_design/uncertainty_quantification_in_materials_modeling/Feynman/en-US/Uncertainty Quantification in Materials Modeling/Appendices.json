{
    "hands_on_practices": [
        {
            "introduction": "Propagating uncertainty from model inputs to outputs is a fundamental task in materials modeling. This practice introduces Polynomial Chaos Expansion (PCE), a powerful technique for creating a surrogate model that efficiently captures this relationship. By combining PCE with $\\ell_{1}$-regularized regression, you will learn how to build a sparse, interpretable model that identifies the most influential sources of uncertainty, a crucial skill for guiding experimental and computational efforts.",
            "id": "3763791",
            "problem": "A high-entropy alloy (HEA) with equiatomic baseline composition exhibits variability in yield strength due to fluctuations in composition and microstructure. Model the random inputs as follows: the composition deviation is represented by a scalar random variable $\\xi_{1} \\sim \\mathrm{Uniform}([-1,1])$ and the microstructural fluctuation (e.g., normalized log-grain-size deviation) is represented by $\\xi_{2} \\sim \\mathcal{N}(0,1)$. Assume $\\xi_{1}$ and $\\xi_{2}$ are independent.\n\nConstruct a sparse Polynomial Chaos Expansion (PCE) for the yield strength $Y$ using a total-degree-$1$ tensor-product basis formed from orthonormal polynomials. Let $L_{n}(\\xi_{1})$ denote the orthonormalized Legendre polynomials with respect to the uniform distribution on $[-1,1]$, and let $\\mathrm{He}_{n}(\\xi_{2})$ denote the orthonormalized probabilists’ Hermite polynomials with respect to the standard normal distribution. Specifically, use the three basis functions\n$$\n\\psi_{0}(\\xi_{1},\\xi_{2}) = 1,\\quad \\psi_{1}(\\xi_{1},\\xi_{2}) = L_{1}(\\xi_{1}),\\quad \\psi_{2}(\\xi_{1},\\xi_{2}) = \\mathrm{He}_{1}(\\xi_{2}),\n$$\nwith $L_{0}(\\xi_{1}) = 1$, $L_{1}(\\xi_{1}) = \\sqrt{3}\\,\\xi_{1}$, $\\mathrm{He}_{0}(\\xi_{2}) = 1$, and $\\mathrm{He}_{1}(\\xi_{2}) = \\xi_{2}$. Assume these basis functions are orthonormal under the joint distribution of $(\\xi_{1},\\xi_{2})$.\n\nSuppose a physics-based surrogate for the true yield strength (in megapascals, MPa) is given by\n$$\nY_{\\mathrm{true}}(\\xi_{1},\\xi_{2}) \\;=\\; 950 \\;+\\; 150\\,L_{1}(\\xi_{1}) \\;-\\; 100\\,\\mathrm{He}_{1}(\\xi_{2}) \\;+\\; 50\\,L_{1}(\\xi_{1})\\,\\mathrm{He}_{1}(\\xi_{2}) \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon$ is a zero-mean random error, independent of $(\\xi_{1},\\xi_{2})$, with finite variance.\n\nConstruct the sparse PCE\n$$\nY(\\xi_{1},\\xi_{2}) \\;\\approx\\; c_{0}\\,\\psi_{0}(\\xi_{1},\\xi_{2}) \\;+\\; c_{1}\\,\\psi_{1}(\\xi_{1},\\xi_{2}) \\;+\\; c_{2}\\,\\psi_{2}(\\xi_{1},\\xi_{2}),\n$$\nby estimating the coefficients $\\{c_{j}\\}_{j=0}^{2}$ via $\\ell_{1}$-regularized regression at the population level:\n$$\n\\min_{c_{0},c_{1},c_{2}} \\;\\; \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\left(Y_{\\mathrm{true}} - c_{0}\\psi_{0} - c_{1}\\psi_{1} - c_{2}\\psi_{2}\\right)^{2}\\right] \\;+\\; \\lambda\\left(|c_{1}| + |c_{2}|\\right),\n$$\nwhere the intercept $c_{0}$ is not penalized. Take the regularization parameter to be $\\lambda = 120$ (in megapascals, MPa).\n\nDerive the coefficient estimators from first principles based on the orthonormality of $\\{\\psi_{j}\\}$ and independence of inputs, and then compute the numerical values of $(c_{0},c_{1},c_{2})$. Express each coefficient in megapascals (MPa) and round your answers to four significant figures. Provide the final vector of coefficients as a row matrix in the order $(c_{0},\\,c_{1},\\,c_{2})$.",
            "solution": "The user wants to find the coefficients $(c_0, c_1, c_2)$ of a sparse Polynomial Chaos Expansion (PCE) for the yield strength $Y$. These coefficients are determined by solving an $\\ell_1$-regularized regression problem, commonly known as LASSO (Least Absolute Shrinkage and Selection Operator).\n\nThe objective function to be minimized is given by:\n$$\nJ(c_{0},c_{1},c_{2}) = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\left(Y_{\\mathrm{true}} - \\left(c_{0}\\psi_{0} + c_{1}\\psi_{1} + c_{2}\\psi_{2}\\right)\\right)^{2}\\right] \\;+\\; \\lambda\\left(|c_{1}| + |c_{2}|\\right)\n$$\nwhere $Y_{\\mathrm{true}}$ is the true value of the yield strength, $\\{\\psi_{j}\\}_{j=0}^{2}$ are the orthonormal basis functions, $\\{c_j\\}_{j=0}^2$ are the coefficients to be determined, and $\\lambda = 120$ is the regularization parameter. The expectation $\\mathbb{E}[\\cdot]$ is taken over the joint probability distribution of the random inputs $(\\xi_{1}, \\xi_{2})$.\n\nThe problem is simplified by the fact that the basis functions $\\psi_0$, $\\psi_1$, and $\\psi_2$ are given as orthonormal. This means that $\\mathbb{E}[\\psi_j \\psi_k] = \\delta_{jk}$, where $\\delta_{jk}$ is the Kronecker delta. This property allows for a direct analytical solution to the minimization problem. The set of orthonormal basis functions is specified as:\n$$\n\\psi_{0} = 1, \\quad \\psi_{1} = L_{1}(\\xi_{1}), \\quad \\psi_{2} = \\mathrm{He}_{1}(\\xi_{2})\n$$\nwhere $L_1$ and $\\mathrm{He}_1$ are orthonormal polynomials.\n\nThe solution to the minimization problem is found by setting the subgradient of the convex objective function $J$ with respect to each coefficient to zero.\n\n**1. Derivation for the unpenalized coefficient $c_{0}$**\nSince the intercept $c_0$ is not included in the $\\ell_1$-penalty term, we can find its value by standard partial differentiation:\n$$\n\\frac{\\partial J}{\\partial c_{0}} = \\mathbb{E}\\!\\left[\\frac{\\partial}{\\partial c_{0}} \\frac{1}{2}\\left(Y_{\\mathrm{true}} - c_{0}\\psi_{0} - c_{1}\\psi_{1} - c_{2}\\psi_{2}\\right)^{2}\\right] = -\\mathbb{E}\\!\\left[\\left(Y_{\\mathrm{true}} - c_{0}\\psi_{0} - c_{1}\\psi_{1} - c_{2}\\psi_{2}\\right)\\psi_{0}\\right]\n$$\nSetting the derivative to zero and using the linearity of expectation:\n$$\n\\mathbb{E}[Y_{\\mathrm{true}}\\psi_0] - c_0\\mathbb{E}[\\psi_0^2] - c_1\\mathbb{E}[\\psi_1\\psi_0] - c_2\\mathbb{E}[\\psi_2\\psi_0] = 0\n$$\nApplying the orthonormality conditions ($\\mathbb{E}[\\psi_0^2]=1$, $\\mathbb{E}[\\psi_1\\psi_0]=0$, $\\mathbb{E}[\\psi_2\\psi_0]=0$):\n$$\n\\mathbb{E}[Y_{\\mathrm{true}}\\psi_0] - c_0 = 0 \\implies c_0 = \\mathbb{E}[Y_{\\mathrm{true}}\\psi_0]\n$$\nSince $\\psi_0=1$, this is simply the expected value of the true model: $c_0 = \\mathbb{E}[Y_{\\mathrm{true}}]$.\n\n**2. Derivation for the penalized coefficients $c_{1}$ and $c_{2}$**\nFor the penalized coefficients, we use the subgradient. The subdifferential of $|c_j|$ is $\\partial|c_j| = \\mathrm{sgn}(c_j)$ if $c_j \\neq 0$ and the interval $[-1,1]$ if $c_j = 0$. The optimality condition for $c_1$ requires that zero is in the subgradient:\n$$\n0 \\in \\frac{\\partial J}{\\partial c_{1}} = -\\mathbb{E}\\!\\left[\\left(Y_{\\mathrm{true}} - c_{0}\\psi_{0} - c_{1}\\psi_{1} - c_{2}\\psi_{2}\\right)\\psi_{1}\\right] + \\lambda\\,\\partial|c_1|\n$$\nUsing orthonormality, this simplifies to:\n$$\n0 \\in -(\\mathbb{E}[Y_{\\mathrm{true}}\\psi_1] - c_1) + \\lambda\\,\\partial|c_1|\n$$\nThis condition is satisfied by the soft-thresholding operator, $S_{\\lambda}(\\cdot)$. The solution for $c_1$ is:\n$$\nc_1 = S_{\\lambda}(\\mathbb{E}[Y_{\\mathrm{true}}\\psi_1]) = \\mathrm{sgn}(\\mathbb{E}[Y_{\\mathrm{true}}\\psi_1]) \\max\\left(|\\mathbb{E}[Y_{\\mathrm{true}}\\psi_1]| - \\lambda, 0\\right)\n$$\nAn identical derivation for $c_2$ yields:\n$$\nc_2 = S_{\\lambda}(\\mathbb{E}[Y_{\\mathrm{true}}\\psi_2]) = \\mathrm{sgn}(\\mathbb{E}[Y_{\\mathrm{true}}\\psi_2]) \\max\\left(|\\mathbb{E}[Y_{\\mathrm{true}}\\psi_2]| - \\lambda, 0\\right)\n$$\nThe terms $\\mathbb{E}[Y_{\\mathrm{true}}\\psi_j]$ are the standard Galerkin projection coefficients. Let's denote them by $y_j = \\mathbb{E}[Y_{\\mathrm{true}}\\psi_j]$.\n\n**3. Computation of the expectation terms**\nThe true model for yield strength is given as:\n$$\nY_{\\mathrm{true}} = 950 + 150\\,L_{1}(\\xi_{1}) - 100\\,\\mathrm{He}_{1}(\\xi_{2}) + 50\\,L_{1}(\\xi_{1})\\,\\mathrm{He}_{1}(\\xi_{2}) + \\varepsilon\n$$\nIn terms of our basis functions, this is $Y_{\\mathrm{true}} = 950\\psi_0 + 150\\psi_1 - 100\\psi_2 + 50\\psi_1\\psi_2 + \\varepsilon$.\nWe use the properties that $\\xi_1$ and $\\xi_2$ are independent, $\\varepsilon$ is independent of both with $\\mathbb{E}[\\varepsilon]=0$, and from orthonormality with $\\psi_0=1$, we have $\\mathbb{E}[\\psi_1] = \\mathbb{E}[L_1(\\xi_1)]=0$ and $\\mathbb{E}[\\psi_2] = \\mathbb{E}[\\mathrm{He}_1(\\xi_2)]=0$.\n\n- **Computation for $c_0$**:\n$c_0 = \\mathbb{E}[Y_{\\mathrm{true}}] = \\mathbb{E}[950\\psi_0 + 150\\psi_1 - 100\\psi_2 + 50\\psi_1\\psi_2 + \\varepsilon]$\n$c_0 = 950\\mathbb{E}[\\psi_0] + 150\\mathbb{E}[\\psi_1] - 100\\mathbb{E}[\\psi_2] + 50\\mathbb{E}[\\psi_1\\psi_2] + \\mathbb{E}[\\varepsilon]$\nDue to independence, $\\mathbb{E}[\\psi_1\\psi_2] = \\mathbb{E}[\\psi_1]\\mathbb{E}[\\psi_2] = 0 \\cdot 0 = 0$.\n$c_0 = 950(1) + 150(0) - 100(0) + 50(0) + 0 = 950$.\n\n- **Computation for $c_1$**:\nFirst, we compute the projection coefficient $y_1 = \\mathbb{E}[Y_{\\mathrm{true}}\\psi_1]$:\n$y_1 = \\mathbb{E}[(950\\psi_0 + 150\\psi_1 - 100\\psi_2 + 50\\psi_1\\psi_2 + \\varepsilon)\\psi_1]$\n$y_1 = 950\\mathbb{E}[\\psi_0\\psi_1] + 150\\mathbb{E}[\\psi_1^2] - 100\\mathbb{E}[\\psi_2\\psi_1] + 50\\mathbb{E}[\\psi_1^2\\psi_2] + \\mathbb{E}[\\varepsilon\\psi_1]$\nUsing orthonormality ($\\mathbb{E}[\\psi_0\\psi_1]=0$, $\\mathbb{E}[\\psi_1^2]=1$, $\\mathbb{E}[\\psi_2\\psi_1]=0$) and independence ($\\mathbb{E}[\\psi_1^2\\psi_2]=\\mathbb{E}[\\psi_1^2]\\mathbb{E}[\\psi_2]=1 \\cdot 0=0$; $\\mathbb{E}[\\varepsilon\\psi_1]=\\mathbb{E}[\\varepsilon]\\mathbb{E}[\\psi_1]=0 \\cdot 0=0$):\n$y_1 = 950(0) + 150(1) - 100(0) + 50(0) + 0 = 150$.\nNow, apply the soft-thresholding with $\\lambda=120$. Since $|y_1|=150 > 120$:\n$c_1 = S_{120}(150) = 150 - 120 = 30$.\n\n- **Computation for $c_2$**:\nFirst, we compute the projection coefficient $y_2 = \\mathbb{E}[Y_{\\mathrm{true}}\\psi_2]$:\n$y_2 = \\mathbb{E}[(950\\psi_0 + 150\\psi_1 - 100\\psi_2 + 50\\psi_1\\psi_2 + \\varepsilon)\\psi_2]$\n$y_2 = 950\\mathbb{E}[\\psi_0\\psi_2] + 150\\mathbb{E}[\\psi_1\\psi_2] - 100\\mathbb{E}[\\psi_2^2] + 50\\mathbb{E}[\\psi_1\\psi_2^2] + \\mathbb{E}[\\varepsilon\\psi_2]$\nUsing orthonormality ($\\mathbb{E}[\\psi_0\\psi_2]=0$, $\\mathbb{E}[\\psi_1\\psi_2]=0$, $\\mathbb{E}[\\psi_2^2]=1$) and independence ($\\mathbb{E}[\\psi_1\\psi_2^2]=\\mathbb{E}[\\psi_1]\\mathbb{E}[\\psi_2^2]=0 \\cdot 1=0$; $\\mathbb{E}[\\varepsilon\\psi_2]=\\mathbb{E}[\\varepsilon]\\mathbb{E}[\\psi_2]=0 \\cdot 0=0$):\n$y_2 = 950(0) + 150(0) - 100(1) + 50(0) + 0 = -100$.\nNow, apply the soft-thresholding with $\\lambda=120$. Since $|y_2|=100 \\le 120$:\n$c_2 = S_{120}(-100) = 0$.\n\n**4. Final Coefficients**\nThe computed coefficients are $(c_0, c_1, c_2) = (950, 30, 0)$. The problem asks for the answers to be rounded to four significant figures.\n- $c_0 = 950$ becomes $950.0$.\n- $c_1 = 30$ becomes $30.00$.\n- $c_2 = 0$ becomes $0.000$ to indicate precision, although $0$ has no non-zero significant figures. This formatting is chosen for consistency.\n\nThe final coefficient vector is $(950.0, 30.00, 0.000)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 950.0 & 30.00 & 0.000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While forward uncertainty quantification propagates known input uncertainties, inverse UQ aims to infer unknown model parameters from experimental data. This exercise goes to the heart of modern Bayesian inference by exploring the Metropolis-Hastings algorithm, the foundational Markov chain Monte Carlo (MCMC) method. By deriving its acceptance probability from first principles and considering the practicalities of proposal tuning, you will gain a core competency for calibrating complex materials models.",
            "id": "3763823",
            "problem": "A high-entropy alloy (HEA) property model is described by a forward mapping from a parameter vector $\\theta \\in \\mathbb{R}^d$ to predicted observables, with experimental data $D$ collected under known noise characteristics. A Bayesian formulation is adopted with prior density $p(\\theta)$ and likelihood $p(D \\mid \\theta)$, leading to the posterior density $p(\\theta \\mid D) \\propto p(D \\mid \\theta)\\,p(\\theta)$. To perform uncertainty quantification for downstream microstructural predictions, the research team employs Markov chain Monte Carlo (MCMC) via the Metropolis–Hastings (MH) algorithm with a general proposal density $q(\\theta' \\mid \\theta)$ that is not necessarily symmetric.\n\nStarting from the definitions of a Markov chain transition kernel and the principle that the posterior density must be invariant under the chain, derive the functional form of the MH acceptance probability that enforces detailed balance with respect to $p(\\theta \\mid D)$. Then, considering practical tuning of proposals in high-dimensional HEA parameter spaces when using random-walk Gaussian proposals, evaluate the impact of proposal scale on acceptance rate and exploration efficiency.\n\nSelect all options that are correct.\n\nA. In MH with an arbitrary (possibly non-symmetric) proposal density $q(\\theta' \\mid \\theta)$, the acceptance probability must be\n$$\n\\alpha(\\theta \\to \\theta') = \\min\\left\\{\\,1,\\; \\frac{p(D \\mid \\theta')\\,p(\\theta')\\,q(\\theta \\mid \\theta')}{p(D \\mid \\theta)\\,p(\\theta)\\,q(\\theta' \\mid \\theta)} \\,\\right\\}.\n$$\n\nB. In MH with an arbitrary proposal density $q(\\theta' \\mid \\theta)$, the acceptance probability\n$$\n\\alpha(\\theta \\to \\theta') = \\min\\left\\{\\,1,\\; \\frac{p(D \\mid \\theta')\\,p(\\theta')}{p(D \\mid \\theta)\\,p(\\theta)} \\,\\right\\}\n$$\nis sufficient to ensure detailed balance for $p(\\theta \\mid D)$.\n\nC. For random-walk Gaussian proposals in moderate to high dimension $d$, decreasing the proposal variance to achieve an acceptance rate in the range $0.7$–$0.9$ generally improves mixing and reduces autocorrelation, and is recommended for efficient exploration of $p(\\theta \\mid D)$.\n\nD. For random-walk Gaussian proposals targeting approximately smooth, unimodal posteriors in dimension $d$ under standard regularity conditions, there is an asymptotic optimal acceptance rate around $0.234$, achieved by scaling the proposal step size on the order of $d^{-1/2}$, which balances local move size and acceptance probability to maximize efficiency.\n\nE. In an independence sampler with $q(\\theta' \\mid \\theta) = q(\\theta')$, if $q(\\theta')$ underestimates the heaviness of the posterior tails, acceptance will typically be high because large proposed jumps enable aggressive exploration of parameter space, improving mixing.",
            "solution": "The problem statement is a valid exercise in Bayesian computational statistics, specifically concerning the Metropolis–Hastings (MH) algorithm and its application to parameter inference in materials science.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n- **Model Context:** A high-entropy alloy (HEA) property model.\n- **Parameter Space:** A parameter vector $\\theta \\in \\mathbb{R}^d$.\n- **Data:** Experimental data $D$.\n- **Bayesian Components:**\n  - Prior density: $p(\\theta)$.\n  - Likelihood function: $p(D \\mid \\theta)$.\n  - Posterior density: $p(\\theta \\mid D) \\propto p(D \\mid \\theta)\\,p(\\theta)$.\n- **Inference Method:** Markov chain Monte Carlo (MCMC) using the Metropolis–Hastings (MH) algorithm.\n- **Proposal Density:** A general, not necessarily symmetric, proposal density $q(\\theta' \\mid \\theta)$.\n- **Core Tasks:**\n  1. Derive the functional form of the MH acceptance probability, $\\alpha(\\theta \\to \\theta')$, that enforces detailed balance with respect to the posterior $p(\\theta \\mid D)$.\n  2. Evaluate the impact of proposal scale on the efficiency of random-walk Gaussian proposals in high-dimensional parameter spaces.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the established principles of Bayesian statistics and MCMC methods. The MH algorithm is a cornerstone of modern computational science, and its application to parameter inference for materials models like those for HEAs is a standard and scientifically rigorous practice. All concepts—prior, likelihood, posterior, detailed balance, proposal density—are standard and correctly defined.\n- **Well-Posed:** The problem is well-posed. The derivation of the MH acceptance probability is a standard textbook derivation with a unique, correct result. The analysis of proposal tuning relates to well-studied theoretical and practical aspects of MCMC efficiency.\n- **Objective:** The problem is stated in precise, objective, and standard terminology from statistics and computational science. There are no subjective or ambiguous statements.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, and objective. It presents a standard, non-trivial task that is central to the topic of uncertainty quantification. Therefore, the problem is **valid**. We proceed with the solution.\n\n### **Derivation and Analysis**\n\nThe objective of the MCMC algorithm is to generate a sequence of samples $\\{\\theta_1, \\theta_2, \\ldots, \\theta_N\\}$ that are drawn from the posterior distribution, $p(\\theta \\mid D)$. For clarity, let us denote the target posterior density as $\\pi(\\theta) \\equiv p(\\theta \\mid D)$. A Markov chain with transition kernel $K(\\theta' \\mid \\theta)$ will have $\\pi(\\theta)$ as its invariant (stationary) distribution if the kernel satisfies the detailed balance condition:\n$$\n\\pi(\\theta) K(\\theta' \\mid \\theta) = \\pi(\\theta') K(\\theta \\mid \\theta')\n$$\nfor all pairs of states $(\\theta, \\theta')$.\n\nIn the Metropolis–Hastings algorithm, the transition from a state $\\theta_t = \\theta$ to a new state $\\theta_{t+1}$ is a two-step process:\n1.  **Propose:** A candidate state $\\theta'$ is drawn from a proposal density $q(\\theta' \\mid \\theta)$.\n2.  **Accept/Reject:** The candidate state $\\theta'$ is accepted and becomes the next state ($\\theta_{t+1} = \\theta'$) with an acceptance probability $\\alpha(\\theta \\to \\theta')$. If the proposal is rejected (with probability $1 - \\alpha(\\theta \\to \\theta')$), the chain remains at the current state ($\\theta_{t+1} = \\theta$).\n\nThe transition kernel for a move from $\\theta$ to a different state $\\theta'$ ($\\theta' \\neq \\theta$) is the probability of proposing $\\theta'$ and then accepting it:\n$$\nK(\\theta' \\mid \\theta) = q(\\theta' \\mid \\theta) \\alpha(\\theta \\to \\theta') \\quad \\text{for } \\theta' \\neq \\theta\n$$\nSubstituting this into the detailed balance equation, we get:\n$$\n\\pi(\\theta) q(\\theta' \\mid \\theta) \\alpha(\\theta \\to \\theta') = \\pi(\\theta') q(\\theta \\mid \\theta') \\alpha(\\theta' \\to \\theta)\n$$\nTo satisfy this equation while maximizing the acceptance rate (and thus the movement of the chain), the standard Metropolis choice for the acceptance probability is:\n$$\n\\alpha(\\theta \\to \\theta') = \\min\\left\\{\\,1, \\; \\frac{\\pi(\\theta') q(\\theta \\mid \\theta')}{\\pi(\\theta) q(\\theta' \\mid \\theta)} \\,\\right\\}\n$$\nThis form ensures that detailed balance is met. If we let $R = \\frac{\\pi(\\theta') q(\\theta \\mid \\theta')}{\\pi(\\theta) q(\\theta' \\mid \\theta)}$, then $\\alpha(\\theta \\to \\theta') = \\min\\{1, R\\}$ and $\\alpha(\\theta' \\to \\theta) = \\min\\{1, 1/R\\}$. If $R > 1$, then $\\alpha(\\theta \\to \\theta') = 1$ and $\\alpha(\\theta' \\to \\theta) = 1/R$, so $\\pi(\\theta) q(\\theta' \\mid \\theta) \\cdot 1 = \\pi(\\theta') q(\\theta \\mid \\theta') \\cdot (1/R) = \\pi(\\theta) q(\\theta' \\mid \\theta)$, satisfying the condition. A similar argument holds for $R \\le 1$.\n\nFinally, we substitute the definition of the posterior, $\\pi(\\theta) \\propto p(D \\mid \\theta) p(\\theta)$. The normalization constant of the posterior cancels out in the ratio $\\pi(\\theta')/\\pi(\\theta)$:\n$$\n\\frac{\\pi(\\theta')}{\\pi(\\theta)} = \\frac{p(D \\mid \\theta') p(\\theta')}{p(D \\mid \\theta) p(\\theta)}\n$$\nThus, the final expression for the MH acceptance probability is:\n$$\n\\alpha(\\theta \\to \\theta') = \\min\\left\\{\\,1, \\; \\frac{p(D \\mid \\theta') p(\\theta')}{p(D \\mid \\theta) p(\\theta)} \\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)} \\,\\right\\} = \\min\\left\\{\\,1, \\; \\frac{p(D \\mid \\theta') p(\\theta') q(\\theta \\mid \\theta')}{p(D \\mid \\theta) p(\\theta) q(\\theta' \\mid \\theta)} \\,\\right\\}\n$$\n\n### **Evaluation of Options**\n\n**A. In MH with an arbitrary (possibly non-symmetric) proposal density $q(\\theta' \\mid \\theta)$, the acceptance probability must be...**\nThe provided formula is:\n$$ \\alpha(\\theta \\to \\theta') = \\min\\left\\{\\,1,\\; \\frac{p(D \\mid \\theta')\\,p(\\theta')\\,q(\\theta \\mid \\theta')}{p(D \\mid \\theta)\\,p(\\theta)\\,q(\\theta' \\mid \\theta)} \\,\\right\\} $$\nThis expression exactly matches our derivation. It correctly combines the ratio of posterior densities with the ratio of proposal densities (the \"Hastings correction term\"), which is necessary for a general, non-symmetric proposal $q(\\theta' \\mid \\theta)$.\n**Verdict: Correct.**\n\n**B. In MH with an arbitrary proposal density $q(\\theta' \\mid \\theta)$, the acceptance probability ... is sufficient to ensure detailed balance...**\nThe provided formula is:\n$$ \\alpha(\\theta \\to \\theta') = \\min\\left\\{\\,1,\\; \\frac{p(D \\mid \\theta')\\,p(\\theta')}{p(D \\mid \\theta)\\,p(\\theta)} \\,\\right\\} $$\nThis simplified formula omits the proposal density ratio, $q(\\theta \\mid \\theta')/q(\\theta' \\mid \\theta)$. It is valid only if this ratio is equal to $1$, which occurs when the proposal is symmetric, i.e., $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$. The problem statement explicitly allows for a \"not necessarily symmetric\" proposal. Therefore, this form is not sufficient for the general case. It describes the original Metropolis algorithm, not the more general Metropolis-Hastings algorithm.\n**Verdict: Incorrect.**\n\n**C. For random-walk Gaussian proposals... decreasing the proposal variance to achieve an acceptance rate in the range $0.7$–$0.9$ generally improves mixing...**\nA random-walk proposal is of the form $q(\\theta' \\mid \\theta) = f(\\theta' - \\theta)$, where $f$ is a zero-mean density like a Gaussian. Decreasing the proposal variance means the proposed steps $\\theta' - \\theta$ become very small. When $\\theta'$ is very close to $\\theta$, the posterior density ratio $\\pi(\\theta')/\\pi(\\theta)$ will be very close to $1$, leading to a very high acceptance rate (close to $100\\%$, so certainly in the $0.7$–$0.9$ range). However, because the steps are miniscule, the sampler explores the parameter space exceedingly slowly. This behavior is known as a \"random walk with very high acceptance,\" and it results in very high autocorrelation between successive samples and, consequently, poor mixing and low exploration efficiency. The statement that this improves mixing is a direct contradiction of established MCMC theory and practice.\n**Verdict: Incorrect.**\n\n**D. For random-walk Gaussian proposals... there is an asymptotic optimal acceptance rate around $0.234$, achieved by scaling the proposal step size on the order of $d^{-1/2}$...**\nThis statement refers to a well-known theoretical result in MCMC, established by Roberts, Gelman, and Gilks (1997). For a target posterior that is asymptotically a product of independent and identically distributed components (a reasonable approximation for many regular, unimodal posteriors after reparameterization), it is shown that the MCMC sampler's efficiency is maximized when the proposal covariance is scaled such that the acceptance rate converges to approximately $0.234$ as the dimension $d \\to \\infty$. This optimal scaling involves setting the proposal's variance proportional to $d^{-1}$, which means the typical step size scales as $d^{-1/2}$. This rate balances the trade-off between making large moves (which are often rejected) and small moves (which are often accepted but explore slowly). The value $0.234$ is a canonical benchmark for tuning high-dimensional random-walk Metropolis samplers.\n**Verdict: Correct.**\n\n**E. In an independence sampler with $q(\\theta' \\mid \\theta) = q(\\theta')$, if $q(\\theta')$ underestimates the heaviness of the posterior tails...**\nAn independence sampler proposes new states from a fixed density $q(\\theta')$ irrespective of the current state $\\theta$. The acceptance probability is $\\alpha(\\theta \\to \\theta') = \\min\\left\\{1, \\frac{\\pi(\\theta') q(\\theta)}{\\pi(\\theta) q(\\theta')}\\right\\}$. If the proposal density $q(\\theta')$ has lighter tails than the target posterior $\\pi(\\theta)$, it means that in regions where $\\theta$ is far from the mode, the ratio $\\pi(\\theta)/q(\\theta)$ can become arbitrarily large. If the sampler happens to land in such a tail region, the term $q(\\theta)/\\pi(\\theta)$ in the acceptance probability will be extremely small. This will cause nearly all subsequent proposals (which will be drawn from the body of $q$, not the tails) to be rejected. The chain becomes \"stuck\" in the tail, exhibiting extremely poor mixing. Acceptance will be very low, not high. For an independence sampler to be effective, its tails must be at least as heavy as the posterior's tails, ensuring the ratio $\\pi(\\theta)/q(\\theta)$ is bounded.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Generating samples with a Markov chain Monte Carlo (MCMC) algorithm is not enough; we must rigorously verify that the chain has converged to the correct posterior distribution. This practice addresses the critical final step of the Bayesian workflow: convergence diagnostics. You will explore the theoretical conditions for ergodicity and apply key quantitative tools, such as the Gelman-Rubin statistic ($\\hat{R}$) and Effective Sample Size (ESS), to assess the quality of MCMC output from a realistic calibration task.",
            "id": "3763799",
            "problem": "A Bayesian calibration is performed for a constitutive model of a face-centered cubic high-entropy alloy, where the parameter vector is $\\theta=(K,G,\\alpha)\\in\\Theta\\subset\\mathbb{R}^3$ consisting of bulk modulus $K$, shear modulus $G$, and a short-range order parameter $\\alpha$. The physically admissible parameter domain $\\Theta$ is compact and connected. The posterior density $\\pi(\\theta)\\propto L(y\\mid\\theta)p(\\theta)$ is positive and continuous on $\\Theta$, with $p(\\theta)$ supported on $\\Theta$. A random-walk Metropolis algorithm is used with a symmetric Gaussian proposal truncated to $\\Theta$, $q(\\theta'\\mid\\theta)$, and with the usual acceptance rule that includes the possibility of rejection.\n\nYou run $m=3$ independent Markov Chain Monte Carlo (MCMC) chains after warm-up, each with $n=2000$ retained iterations, to calibrate $G$ (reported in gigapascals, but treat it as a dimensionless numerical variable). The chain-wise sample means and variances for $G$ are:\n- Chain $1$: mean $79.5$, variance $9.0$,\n- Chain $2$: mean $80.1$, variance $8.0$,\n- Chain $3$: mean $79.9$, variance $10.0$.\n\nFrom the pooled draws across all chains for $G$, the estimated lag autocorrelations are:\n- $\\rho_1=0.5$, $\\rho_2=0.3$, $\\rho_3=0.1$, $\\rho_4=-0.05$, $\\rho_5=-0.02$, $\\rho_6=-0.01$.\n\nUsing fundamental definitions of Markov chain irreducibility, aperiodicity, Harris recurrence, and drift/minorization, together with variance decomposition across and within chains, and the autocorrelation-based notion of effective sample size (ESS), decide which of the following statements are correct about ergodicity conditions and convergence diagnostics for this calibration task. Gelman–Rubin potential scale reduction factor (PSRF), denoted $\\hat{R}$, and effective sample size (ESS) refer to their standard uses in MCMC convergence assessment.\n\nA. Under the stated conditions (posterior density $\\pi(\\theta)$ positive and continuous on compact $\\Theta$, symmetric proposal $q(\\theta'\\mid\\theta)$ with positive density on neighborhoods, and a nonzero probability of rejection), the Metropolis chain is irreducible and aperiodic and hence Harris recurrent; moreover, because $\\Theta$ is compact and $\\pi(\\theta)$ is bounded away from zero and above on $\\Theta$ with $q$ locally bounded below, a global minorization condition holds on $\\Theta$, which implies geometric ergodicity.\n\nB. The chain is geometrically ergodic if and only if the measurement noise distribution has finite variance; if the posterior has heavy tails, random-walk Metropolis becomes non-ergodic.\n\nC. For $G$, the Gelman–Rubin PSRF $\\hat{R}$ computed from the supplied summaries is approximately $1.005$, and the autocorrelation-based ESS from the pooled autocorrelations is approximately $2200$ out of $6000$ total retained draws.\n\nD. A single long chain suffices to compute $\\hat{R}$, and $\\hat{R}$ can be less than $1$ when autocorrelation is near zero.\n\nE. The ESS equals the total number of retained iterations regardless of autocorrelation; thus for $3$ chains with $2000$ retained iterations each, $\\mathrm{ESS}=6000$.\n\nSelect all correct options.",
            "solution": "The user has requested a detailed validation of the problem statement, followed by a full derivation of the solution and an evaluation of all options.\n\n### Step 1: Extract Givens\n\n-   **Model**: Constitutive model for a face-centered cubic high-entropy alloy.\n-   **Parameter Vector**: $\\theta=(K,G,\\alpha)\\in\\Theta\\subset\\mathbb{R}^3$.\n-   **Parameter Domain**: $\\Theta$ is compact and connected.\n-   **Bayesian Setup**: Posterior density $\\pi(\\theta)\\propto L(y\\mid\\theta)p(\\theta)$ is positive and continuous on $\\Theta$. Prior $p(\\theta)$ is supported on $\\Theta$.\n-   **Algorithm**: Random-walk Metropolis with a symmetric Gaussian proposal $q(\\theta'\\mid\\theta)$ truncated to $\\Theta$. The acceptance rule is standard, with a non-zero rejection probability.\n-   **MCMC Run Details**: $m=3$ independent chains, run after warm-up. Each chain has $n=2000$ retained iterations. The total number of retained draws is $N=mn=6000$.\n-   **Data for Shear Modulus $G$**:\n    -   Chain $1$: mean $\\bar{G}_1 = 79.5$, variance $s_1^2 = 9.0$.\n    -   Chain $2$: mean $\\bar{G}_2 = 80.1$, variance $s_2^2 = 8.0$.\n    -   Chain $3$: mean $\\bar{G}_3 = 79.9$, variance $s_3^2 = 10.0$.\n-   **Autocorrelation Data for $G$ (from pooled draws)**:\n    -   $\\rho_1=0.5$, $\\rho_2=0.3$, $\\rho_3=0.1$, $\\rho_4=-0.05$, $\\rho_5=-0.02$, $\\rho_6=-0.01$.\n-   **Question**: Evaluate the correctness of given statements about ergodicity conditions and convergence diagnostics, specifically the Gelman–Rubin potential scale reduction factor ($\\hat{R}$) and effective sample size (ESS).\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientific Grounding**: The problem is well-grounded in the fields of computational materials science and Bayesian statistics. The application (calibrating a constitutive model for a high-entropy alloy) is a realistic and important research area. The methods described (Bayesian inference, MCMC, Metropolis algorithm) and the diagnostics used ($\\hat{R}$, ESS) are standard tools. The parameters ($K$, $G$, $\\alpha$) are physically meaningful.\n-   **Well-Posedness**: The problem is well-posed. It provides a clear theoretical setup and sufficient numerical data to evaluate the claims made in each option. The question is unambiguous.\n-   **Objectivity**: The language is technical, precise, and free from subjective or opinion-based statements.\n-   **Completeness and Consistency**: The problem is self-contained. The theoretical conditions on the posterior and the state space are clearly stated. The numerical data is internally consistent and sufficient for the required calculations. There are no contradictions in the setup.\n-   **Realism and Feasibility**: The numerical values are realistic. A shear modulus $G$ around $80$ GPa is typical for many metallic alloys. The autocorrelation values indicate a moderately inefficient but plausible MCMC sampler.\n-   **Other Flaws**: The problem is not trivial, tautological, or ill-posed. It requires a solid understanding of both the theory and practice of MCMC methods.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, and contains sufficient information to proceed with a full analysis.\n\n### Solution and Option Analysis\n\nThe problem is valid. We now proceed to analyze each statement.\n\n**A. Under the stated conditions (posterior density $\\pi(\\theta)$ positive and continuous on compact $\\Theta$, symmetric proposal $q(\\theta'\\mid\\theta)$ with positive density on neighborhoods, and a nonzero probability of rejection), the Metropolis chain is irreducible and aperiodic and hence Harris recurrent; moreover, because $\\Theta$ is compact and $\\pi(\\theta)$ is bounded away from zero and above on $\\Theta$ with $q$ locally bounded below, a global minorization condition holds on $\\Theta$, which implies geometric ergodicity.**\n\nThis statement makes several claims about the theoretical properties of the described Markov chain.\n1.  **Irreducibility**: The proposal distribution $q(\\theta'\\mid\\theta)$ is a truncated Gaussian, which has positive density in a neighborhood of $\\theta$ within the connected space $\\Theta$. The target density $\\pi(\\theta)$ is strictly positive on $\\Theta$. This ensures that from any state $\\theta$, there is a positive probability of accepting a move to a state $\\theta'$ in its neighborhood. Since $\\Theta$ is connected, any state can be reached from any other state in a finite number of steps. Thus, the chain is $\\pi$-irreducible.\n2.  **Aperiodicity**: The Metropolis algorithm has a non-zero probability of rejection, which means the chain can stay in the same state ($\\theta_t = \\theta_{t+1}$). This self-loop at every state is sufficient to ensure aperiodicity.\n3.  **Harris Recurrence**: An irreducible Markov chain that possesses a stationary distribution is Harris recurrent. By construction, the Metropolis algorithm has $\\pi(\\theta)$ as its stationary distribution. Given irreducibility, it must be Harris recurrent.\n4.  **Geometric Ergodicity**: Geometric ergodicity implies that convergence to the stationary distribution is exponentially fast. A sufficient condition for this is the existence of a drift condition or, more simply in this context, the property that the entire state space is a \"small set.\" For a Markov chain on a compact state space $\\Theta$, if the target density $\\pi(\\theta)$ is continuous and strictly positive (and therefore bounded below, i.e., $\\pi(\\theta) \\ge \\delta > 0$), and the proposal density is sufficiently well-behaved (e.g., locally bounded below), a minorization condition holds: $P(\\theta, A) \\ge \\epsilon \\nu(A)$ for some $\\epsilon > 0$ and probability measure $\\nu$. This implies uniform ergodicity, which is stronger than geometric ergodicity. The reasoning presented in the option is a standard result from advanced MCMC theory. All conditions described in the option are sufficient to establish geometric ergodicity.\n\nThe chain of logic in the statement is sound and aligns with established MCMC theory.\n\nVerdict: **Correct**.\n\n**B. The chain is geometrically ergodic if and only if the measurement noise distribution has finite variance; if the posterior has heavy tails, random-walk Metropolis becomes non-ergodic.**\n\nThis statement presents two claims regarding ergodicity.\n1.  **\"if and only if\" claim**: Geometric ergodicity depends on the global properties of the posterior distribution and the proposal mechanism on the state space $\\Theta$. Tying it with an \"if and only if\" condition to a single aspect of the likelihood, such as the variance of the measurement noise, is an oversimplification and generally false. For instance, even with Gaussian noise (finite variance), a non-identifiable model or a heavy-tailed prior could lead to a posterior that is not geometrically ergodic with a random-walk Metropolis sampler on an *unbounded* space. Conversely, in this problem, the state space $\\Theta$ is compact. As established in A, this compactness, along with a positive continuous posterior, is a very strong condition that ensures geometric ergodicity, regardless of the noise model's tail behavior.\n2.  **\"heavy tails\" claim**: This claim asserts that a heavy-tailed posterior makes the chain \"non-ergodic.\" This conflates ergodicity with the *rate* of convergence. A chain is ergodic if it converges to its stationary distribution. Heavy tails can cause the chain to fail to be *geometrically* ergodic, meaning convergence can be polynomially slow rather than exponentially fast. However, the chain is typically still ergodic (i.e., it converges). Therefore, stating it becomes \"non-ergodic\" is incorrect.\n\nBoth parts of this statement are incorrect.\n\nVerdict: **Incorrect**.\n\n**C. For $G$, the Gelman–Rubin PSRF $\\hat{R}$ computed from the supplied summaries is approximately $1.005$, and the autocorrelation-based ESS from the pooled autocorrelations is approximately $2200$ out of $6000$ total retained draws.**\n\nThis statement requires numerical verification.\n-   **Gelman-Rubin PSRF ($\\hat{R}$)**:\n    The formula is $\\hat{R} = \\sqrt{\\frac{\\widehat{\\mathrm{Var}}^+}{W}}$, where $W$ is the within-chain variance and $\\widehat{\\mathrm{Var}}^+$ is the estimated marginal posterior variance.\n    -   Number of chains $m=3$, iterations per chain $n=2000$.\n    -   Within-chain variance $W = \\frac{1}{m}\\sum_{j=1}^m s_j^2 = \\frac{1}{3}(9.0 + 8.0 + 10.0) = \\frac{27.0}{3} = 9.0$.\n    -   Overall mean $\\bar{\\bar{G}} = \\frac{1}{m}\\sum_{j=1}^m \\bar{G}_j = \\frac{1}{3}(79.5 + 80.1 + 79.9) = \\frac{239.5}{3} \\approx 79.8333$.\n    -   Between-chain variance $B = \\frac{n}{m-1}\\sum_{j=1}^m (\\bar{G}_j - \\bar{\\bar{G}})^2 = \\frac{2000}{2} \\left[ (79.5 - \\frac{239.5}{3})^2 + (80.1 - \\frac{239.5}{3})^2 + (79.9 - \\frac{239.5}{3})^2 \\right] = 1000 \\left[ (-\\frac{1}{3})^2 + (\\frac{0.8}{3})^2 + (\\frac{0.2}{3})^2 \\right] = 1000 \\left[ \\frac{1}{9} + \\frac{0.64}{9} + \\frac{0.04}{9} \\right] = 1000 \\frac{1.68}{9} \\approx 186.67$.\n    -   Estimated variance $\\widehat{\\mathrm{Var}}^+ = \\frac{n-1}{n}W + \\frac{1}{n}B = \\frac{1999}{2000}(9.0) + \\frac{186.67}{2000} \\approx (0.9995)(9.0) + 0.093335 = 8.9955 + 0.093335 = 9.088835$.\n    -   PSRF $\\hat{R} = \\sqrt{\\frac{9.088835}{9.0}} = \\sqrt{1.00987...} \\approx 1.0049$.\n    The calculated value $\\hat{R} \\approx 1.0049$ is consistent with the claimed value of approximately $1.005$.\n\n-   **Effective Sample Size (ESS)**:\n    The formula is $\\mathrm{ESS} = \\frac{N}{1 + 2\\sum_{t=1}^T \\rho_t}$, where $N = 6000$ and $\\rho_t$ is the autocorrelation at lag $t$. The sum is truncated at some lag $T$. A common heuristic is to sum while the autocorrelations are positive and decreasing, or more reliably, sum up to the first lag $t$ before the estimates become too noisy (often indicated by negative values). Let's use the rule of summing terms up to the lag $T$ before which the pairwise sum of autocorrelations $\\rho_t + \\rho_{t+1}$ becomes negative. Here $\\rho_3+\\rho_4=0.05 > 0$ and $\\rho_4+\\rho_5=-0.07  0$. A reasonable choice is to sum up to $T=4$.\n    -   $\\sum_{t=1}^4 \\rho_t = 0.5 + 0.3 + 0.1 - 0.05 = 0.85$.\n    -   $\\mathrm{ESS} = \\frac{6000}{1 + 2(0.85)} = \\frac{6000}{1 + 1.7} = \\frac{6000}{2.7} \\approx 2222.2$.\n    This calculated value of $\\approx 2222$ is very close to the claimed value of approximately $2200$. The small difference is negligible and likely due to different truncation/windowing rules for the autocorrelation sum, none of which are specified. The approximation is valid.\n\nBoth parts of the statement are verified by standard calculations.\n\nVerdict: **Correct**.\n\n**D. A single long chain suffices to compute $\\hat{R}$, and $\\hat{R}$ can be less than $1$ when autocorrelation is near zero.**\n\n1.  **\"A single long chain suffices to compute $\\hat{R}$\"**: This is fundamentally incorrect. The Gelman-Rubin diagnostic $\\hat{R}$ is explicitly designed to compare the variance *between* multiple parallel chains ($B$) with the variance *within* each chain ($W$). By its very definition, it requires multiple chains ($m>1$). It is not possible to compute $\\hat{R}$ from a single chain.\n2.  **\"$\\hat{R}$ can be less than $1$...\"**: The formula for $\\hat{R}^2$ is $\\frac{n-1}{n} + \\frac{B}{nW}$. For $\\hat{R}$ to be less than $1$, we need $\\hat{R}^2  1$, which implies $\\frac{n-1}{n} + \\frac{B}{nW}  1$. This simplifies to $B  W$. It is possible for the sample-based estimate of between-chain variance $B$ to be less than the sample-based estimate of within-chain variance $W$ due to sampling variability, particularly in pre-converged chains or with small $n$. So, mathematically, $\\hat{R}$ can be less than $1$. However, the first part of the statement is definitively false, rendering the entire conjoined statement incorrect.\n\nVerdict: **Incorrect**.\n\n**E. The ESS equals the total number of retained iterations regardless of autocorrelation; thus for $3$ chains with $2000$ retained iterations each, $\\mathrm{ESS}=6000$.**\n\nThis statement fundamentally misunderstands the definition of Effective Sample Size (ESS). The ESS is defined as $\\mathrm{ESS} = N / \\tau$, where $N$ is the total number of samples and $\\tau = 1 + 2\\sum_{t=1}^\\infty \\rho_t$ is the integrated autocorrelation time.\n-   The ESS is equal to the total number of iterations $N$ if and only if $\\tau=1$, which requires that all autocorrelations $\\rho_t$ for $t \\ge 1$ are zero. This corresponds to a set of perfectly independent samples.\n-   MCMC algorithms generate correlated samples, so $\\rho_t > 0$ for at least some initial lags, which means $\\tau > 1$ and $\\mathrm{ESS}  N$.\n-   The problem explicitly provides non-zero autocorrelations ($\\rho_1=0.5$, etc.), and our calculation in C confirmed that $\\mathrm{ESS} \\approx 2200$, which is significantly less than $N=6000$.\n-   The statement that ESS is equal to the total number of draws *regardless of autocorrelation* is a direct contradiction of its definition and purpose.\n\nVerdict: **Incorrect**.\n\n### Summary of Verdicts\n\n-   A: Correct\n-   B: Incorrect\n-   C: Correct\n-   D: Incorrect\n-   E: Incorrect\n\nThe correct options are A and C.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}