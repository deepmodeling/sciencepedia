{
    "hands_on_practices": [
        {
            "introduction": "A fundamental principle in modeling materials is that a composition's properties do not depend on the arbitrary order in which we list its constituent elements. This exercise challenges you to implement a permutation-invariant model architecture, a crucial step for building physically realistic machine learning predictors for HEAs. By contrasting it with an order-sensitive baseline, you will gain practical experience in designing and verifying models that respect this essential symmetry. ",
            "id": "3750170",
            "problem": "You are tasked with designing and implementing a compositional invariance test harness for predictive models of High-Entropy Alloy (HEA) properties. A High-Entropy Alloy (HEA) consists of multiple elements mixed in near-equiatomic proportions; a core physical requirement in modeling such systems is that the predicted property of a composition must be independent of the arbitrary ordering of its constituent elements. In machine learning terms, the mapping from a multiset of element descriptors and their fractions to a scalar property should be invariant under permutations of the constituent index. Your program must be a complete, runnable implementation that constructs two predictors, builds a permutation-invariance test harness, and reports automated unit test results for a specified test suite.\n\nFundamental base and physical realism: For ideal mixture behavior and many mixture rules used in materials science, the target macroscopic property is modeled by aggregated statistics over constituent-specific descriptors. A canonical example is the rule of mixtures, which in one form states that a property $P$ is given by a weighted sum $P = \\sum_{i=1}^{n} w_i P_i$, where $w_i$ is a physically meaningful weight such as a mole fraction and $P_i$ is a constituent-specific quantity. This aggregation is symmetric: it depends only on the set of constituents and their weights, not their enumeration. In learning systems consistent with these laws, permutation invariance can be structurally enforced by symmetric operations such as summation over per-constituent embeddings.\n\nYour tasks:\n1. Implement two scalar-output predictors that take a composition defined by element descriptors and fractions as input:\n   - An order-invariant predictor based on a symmetric aggregator consistent with ideal mixture composition. This model must use a shared per-element embedding followed by a symmetric reduction over elements and a final readout. Specifically, use a shared element-wise transformation and a summation over elements weighted by the element fractions to obtain an intermediate representation, followed by a linear readout to produce the scalar property. This design must not depend on the positional index of the elements.\n   - An intentionally order-sensitive baseline predictor that uses position-specific parameters to form a linear combination of per-element contributions; this model must depend on the index position and therefore should fail invariance checks for multi-element inputs.\n\n2. Construct a test harness that, for a given composition, enumerates all permutations of the constituent index and checks whether the predictor output is unchanged within a specified tolerance across permutations. The test harness must report a boolean indicating pass or fail of permutation invariance for each composition.\n\n3. Use the following numerically specified parameters and test suite. All descriptors are three-dimensional vectors representing physically plausible features: Pauling electronegativity (dimensionless), covalent radius in angstroms, and an integer-like valence electron count. Fractions are mole fractions expressed as decimals; they must sum to $1$.\n   - Shared element-wise embedding parameters for the invariant model (use the same $\\phi$ for every element):\n     - Weight matrix $W_{\\phi}$:\n       $$\n       W_{\\phi} = \\begin{bmatrix}\n       0.5  -0.2  0.1 \\\\\n       0.3  0.4  -0.1 \\\\\n       -0.25  0.15  0.6\n       \\end{bmatrix}\n       $$\n     - Bias vector $b_{\\phi}$: $[0.05, -0.03, 0.02]$.\n     - Readout weight $w_{\\rho}$: $[1.2, -0.7, 0.5]$.\n     - Readout bias $c_{\\rho}$: $-0.1$.\n   - Position-specific weight vectors for the order-sensitive baseline (use the first $n$ vectors for an $n$-element composition in the order they appear):\n     - $b_1 = [0.1, 0.2, 0.3]$, $b_2 = [-0.2, 0.1, 0.4]$, $b_3 = [0.3, -0.1, 0.2]$, $b_4 = [0.4, 0.5, -0.3]$.\n     - Baseline bias $c_0 = 0.0$.\n   - Test suite of compositions (each case is a pair of fractions and a descriptor matrix; rows correspond to elements):\n     1. Case A (three elements, typical HEA-like):\n        - Fractions: $[0.40, 0.35, 0.25]$.\n        - Descriptors: $\\big[ [1.83, 1.26, 8.00], [1.91, 1.24, 10.00], [1.66, 1.28, 6.00] \\big]$.\n     2. Case B (single-element boundary):\n        - Fractions: $[1.00]$.\n        - Descriptors: $\\big[ [1.61, 1.43, 3.00] \\big]$.\n     3. Case C (four elements, tiny and zero fractions edge case):\n        - Fractions: $[0.001, 0.499, 0.500, 0.000]$.\n        - Descriptors: $\\big[ [1.63, 1.34, 5.00], [1.88, 1.25, 9.00], [1.54, 1.47, 4.00], [2.16, 1.39, 6.00] \\big]$.\n     4. Case D (three elements, equal fractions with duplicate descriptors):\n        - Fractions: $[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$.\n        - Descriptors: $\\big[ [1.55, 1.27, 7.00], [1.55, 1.27, 7.00], [1.90, 1.28, 11.00] \\big]$.\n   - Tolerance for invariance checks: $\\varepsilon = 10^{-12}$.\n\n4. Program logic:\n   - Define the invariant predictor using a shared element mapping $\\phi$ followed by a symmetric sum weighted by the fractions and a linear readout. The output must be a single scalar.\n   - Define the non-invariant baseline predictor using the first $n$ position-specific weights $b_i$ for an $n$-element input in the given order; the output must be a single scalar.\n   - Implement a function that, for each test case, checks the predictor outputs across all permutations of the element index and returns a boolean indicating whether all outputs are equal within $\\varepsilon$.\n\n5. Final output format:\n   - For each test case, produce a pair of booleans $[b_{\\text{inv}}, b_{\\text{noninv}}]$ where $b_{\\text{inv}}$ indicates whether the invariant model passed permutation invariance and $b_{\\text{noninv}}$ indicates whether the baseline model passed permutation invariance.\n   - Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the pair for a test case. For example: $[[\\text{True},\\text{False}],[\\text{True},\\text{True}],\\dots]$.\n\nAngle units are not applicable. Physical units for intermediate descriptors are as stated, but the final outputs are booleans and therefore unitless. The test suite covers typical, boundary, and edge cases, including multi-element compositions, a single-element composition, tiny and zero fractions, and duplicate descriptors under equal fractions, to probe different facets of permutation invariance.",
            "solution": "We begin by grounding the design in physically motivated symmetry and aggregation principles. The rule of mixtures provides a canonical example where a macroscopic property $P$ is modeled by the sum $P = \\sum_{i=1}^{n} w_i P_i$, with $w_i$ denoting a physical weight such as a mole fraction and $P_i$ denoting a constituent-specific quantity. This expression is invariant under permutations of the indexing set $\\{1,2,\\dots,n\\}$, because the sum is commutative and associative. In learning systems for compositions, a structure that respects this symmetry is the Deep Sets formulation, where the function over a multiset $\\{x_i\\}_{i=1}^n$ is modeled as $f(\\{x_i\\}) = \\rho\\left(\\sum_{i=1}^n \\phi(x_i)\\right)$ for suitable functions $\\phi$ and $\\rho$. For HEA compositions, including fractions $x_i$ (here, $x_i$ denotes mole fractions, not to be confused with the descriptor $d_i$), a consistent generalization is $f(\\{(x_i,d_i)\\}) = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right)$.\n\nPermutation invariance derivation: Let $\\pi$ be any permutation of $\\{1,\\dots,n\\}$. Consider the invariant model\n$$\ny_{\\text{inv}} = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right),\n$$\nwith $\\phi$ applied identically to each element descriptor $d_i$. Under permutation, the input sequence $(x_i, d_i)$ is transformed to $(x_{\\pi(i)}, d_{\\pi(i)})$; the symmetric aggregator yields\n$$\n\\sum_{i=1}^n x_{\\pi(i)} \\, \\phi(d_{\\pi(i)}) = \\sum_{j=1}^n x_j \\, \\phi(d_j),\n$$\nafter the change of index $j = \\pi(i)$, since $\\pi$ is a bijection. Therefore $y_{\\text{inv}}$ is identical under any $\\pi$.\n\nOrder sensitivity derivation for the baseline: Let the baseline predictor use position-specific parameters $b_i$ to form a scalar\n$$\ny_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle,\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the Euclidean inner product in $\\mathbb{R}^3$. Under a permutation $\\pi$, this becomes\n$$\ny_{\\text{base}}^{(\\pi)} = c_0 + \\sum_{i=1}^n x_{\\pi(i)} \\, \\langle b_i, d_{\\pi(i)} \\rangle.\n$$\nUnless the $b_i$ are all identical or special cancellations occur for the specific descriptors and fractions, $y_{\\text{base}}^{(\\pi)} \\neq y_{\\text{base}}$. Therefore, this predictor is generally order-sensitive and will fail permutation-invariance tests for multi-element inputs.\n\nAlgorithmic design:\n- Implement $\\phi(d) = W_{\\phi} d + b_{\\phi}$ with $W_{\\phi} \\in \\mathbb{R}^{3 \\times 3}$ and $b_{\\phi} \\in \\mathbb{R}^3$ as given. This is a shared affine map applied to each element descriptor $d_i$.\n- Implement $\\rho(v) = \\langle w_{\\rho}, v \\rangle + c_{\\rho}$ with $w_{\\rho} \\in \\mathbb{R}^3$ and $c_{\\rho} \\in \\mathbb{R}$ as given.\n- The invariant predictor computes $v = \\sum_{i=1}^n x_i \\, \\phi(d_i)$ and then $y_{\\text{inv}} = \\rho(v)$.\n- The baseline predictor computes $y_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle$ using the first $n$ position-specific vectors $b_i$ in the current order.\n- The invariance test harness: For each composition with fractions $(x_1,\\dots,x_n)$ and descriptors $(d_1,\\dots,d_n)$, enumerate all permutations $\\pi$ of $\\{1,\\dots,n\\}$, compute the model output for each permuted arrangement, and check that all outputs are equal to the reference output within a tolerance $\\varepsilon = 10^{-12}$. Formally, for outputs $y^{(\\pi)}$, compute $\\max_{\\pi} \\left| y^{(\\pi)} - y^{(\\text{id})} \\right|$ and declare pass if this maximum is less than or equal to $\\varepsilon$.\n\nTest suite coverage:\n- Case A probes a typical multi-element HEA-like composition with realistic features and heterogeneous fractions; the invariant model should pass and the baseline should fail.\n- Case B is a single-element boundary case; both models should pass because there is only one permutation.\n- Case C includes tiny and zero fractions, exercising numerical stability and ensuring elements with zero fraction have no effect; the invariant model should pass and the baseline should fail unless there is coincidental equality across all permutations, which is highly unlikely.\n- Case D includes equal fractions and duplicate descriptors, probing whether invariance holds even when swapping identical elements; the invariant model should pass and the baseline should still fail because position-specific weights create dependence that does not vanish across all permutations.\n\nNumerical stability: Summation over floating-point values can introduce minor ordering-dependent round-off differences. The tolerance $\\varepsilon = 10^{-12}$ is selected to be safely above machine epsilon for double precision while being strict enough to catch genuine order dependence. The invariant model, by construction, is symmetric; any differences across permutations should be at most round-off, hence below $\\varepsilon$. The baseline model is expected to exceed $\\varepsilon$ in multi-element cases.\n\nFinal output: For each case, produce $[b_{\\text{inv}}, b_{\\text{noninv}}]$, and print the list over all cases on a single line: $[[b_{\\text{inv}}^{A}, b_{\\text{noninv}}^{A}], [b_{\\text{inv}}^{B}, b_{\\text{noninv}}^{B}], [b_{\\text{inv}}^{C}, b_{\\text{noninv}}^{C}], [b_{\\text{inv}}^{D}, b_{\\text{noninv}}^{D}]]$.\n\nThis design integrates foundational physical symmetry with algorithmic checks to enforce and verify permutation invariance in HEA property prediction models, aligning with advanced graduate-level expectations for the correctness and robustness of machine learning models in complex materials modeling.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef deep_sets_predict(fractions, descriptors):\n    \"\"\"\n    Order-invariant predictor using shared element-wise embedding phi and\n    symmetric aggregation followed by a linear readout rho.\n    \"\"\"\n    W_phi = np.array([\n        [0.5,  -0.2,  0.1],\n        [0.3,   0.4, -0.1],\n        [-0.25, 0.15, 0.6]\n    ], dtype=float)\n    b_phi = np.array([0.05, -0.03, 0.02], dtype=float)\n    w_rho = np.array([1.2, -0.7, 0.5], dtype=float)\n    c_rho = -0.1\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    # Apply shared embedding phi to each descriptor\n    phi_vals = (descriptors @ W_phi.T) + b_phi  # shape (n, 3)\n    # Symmetric aggregation weighted by fractions\n    v = np.einsum('i,ij->j', fractions, phi_vals)  # shape (3,)\n    # Linear readout\n    y = float(np.dot(w_rho, v) + c_rho)\n    return y\n\ndef baseline_order_sensitive_predict(fractions, descriptors):\n    \"\"\"\n    Order-sensitive baseline predictor using position-specific weights b_i.\n    Uses first n weight vectors for an n-element composition in given order.\n    \"\"\"\n    b_list = [\n        np.array([0.1, 0.2, 0.3], dtype=float),\n        np.array([-0.2, 0.1, 0.4], dtype=float),\n        np.array([0.3, -0.1, 0.2], dtype=float),\n        np.array([0.4, 0.5, -0.3], dtype=float),\n    ]\n    c0 = 0.0\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    assert descriptors.shape == (n, 3), \"Descriptors shape must be (n, 3)\"\n    # Use the first n b_i vectors in order\n    contrib = 0.0\n    for i in range(n):\n        contrib += fractions[i] * float(np.dot(b_list[i], descriptors[i]))\n    y = c0 + contrib\n    return y\n\ndef invariance_test(model_fn, fractions, descriptors, eps=1e-12):\n    \"\"\"\n    Returns True if model_fn output is invariant within eps across all permutations\n    of elements (jointly permuting fractions and descriptors).\n    \"\"\"\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    # Reference output on identity ordering\n    y_ref = model_fn(fractions, descriptors)\n    max_diff = 0.0\n    # Enumerate all permutations\n    for perm in itertools.permutations(range(n)):\n        perm = np.array(perm, dtype=int)\n        f_perm = fractions[perm]\n        d_perm = descriptors[perm]\n        y_perm = model_fn(f_perm, d_perm)\n        diff = abs(y_perm - y_ref)\n        if diff > max_diff:\n            max_diff = diff\n        # Early exit if above tolerance to save time\n        if max_diff > eps:\n            return False\n    return True\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: 3 elements, typical\n        (\n            [0.40, 0.35, 0.25],\n            [\n                [1.83, 1.26,  8.00],\n                [1.91, 1.24, 10.00],\n                [1.66, 1.28,  6.00],\n            ],\n        ),\n        # Case B: single element boundary\n        (\n            [1.00],\n            [\n                [1.61, 1.43, 3.00],\n            ],\n        ),\n        # Case C: 4 elements with tiny and zero fractions\n        (\n            [0.001, 0.499, 0.500, 0.000],\n            [\n                [1.63, 1.34,  5.00],\n                [1.88, 1.25,  9.00],\n                [1.54, 1.47,  4.00],\n                [2.16, 1.39,  6.00],\n            ],\n        ),\n        # Case D: 3 elements, equal fractions with duplicate descriptors\n        (\n            [1.0/3.0, 1.0/3.0, 1.0/3.0],\n            [\n                [1.55, 1.27,  7.00],\n                [1.55, 1.27,  7.00],\n                [1.90, 1.28, 11.00],\n            ],\n        ),\n    ]\n\n    eps = 1e-12\n    results = []\n    for fractions, descriptors in test_cases:\n        inv_pass = invariance_test(deep_sets_predict, fractions, descriptors, eps)\n        base_pass = invariance_test(baseline_order_sensitive_predict, fractions, descriptors, eps)\n        results.append([inv_pass, base_pass])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    # We format booleans as their Python literals True/False.\n    print(f\"[{','.join([str(pair).replace(' ', '') for pair in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "High-entropy alloys are often described by a multitude of potential features, but not all are equally important for predicting a specific property. This practice introduces LASSO regression as a powerful tool for automatically selecting a minimal, yet predictive, set of descriptors from a larger pool. You will implement the algorithm from first principles and use bootstrap resampling to test the stability of your feature selection, a critical step for building robust and interpretable scientific models. ",
            "id": "3750182",
            "problem": "You are asked to implement a complete algorithmic pipeline to select a minimal descriptor set for predicting hardness in High-Entropy Alloys (HEAs) using the Least Absolute Shrinkage and Selection Operator (LASSO), and to test the stability of the selected features via bootstrap resampling. Consider a synthetic but scientifically plausible descriptor space for HEAs and a linear response model for hardness. The task must be framed purely in mathematical terms and must produce quantifiable outputs according to the specified format.\n\nAssume the following setting. Let there be $n$ alloy samples and $p$ descriptors for each sample collected into a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a hardness response vector $y \\in \\mathbb{R}^{n}$. Hardness is a physical quantity and must be expressed in gigapascals (GPa). While your program will only output indices and lists without units, ensure that simulated hardness values internally use the unit GPa. Define $p = 8$ descriptors corresponding to typical HEA design features: atomic size mismatch $\\delta$, valence electron concentration $\\mathrm{VEC}$, average electronegativity $\\chi_{\\mathrm{avg}}$, mixing enthalpy $\\Delta H_{\\mathrm{mix}}$, average melting point $T_{m,\\mathrm{avg}}$, average bulk modulus $K_{\\mathrm{avg}}$, average atomic mass $M_{\\mathrm{avg}}$, and average shear modulus $G_{\\mathrm{avg}}$. Let the matrix $X$ be generated from correlated latent variables to reflect realistic HEA descriptor relationships.\n\nUse the following generative hardness model with additive noise:\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon,\n$$\nwhere $\\beta_0 \\in \\mathbb{R}$ is an intercept, $\\beta^\\star \\in \\mathbb{R}^{p}$ is a sparse coefficient vector with nonzero entries on a subset of descriptors, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ is independent noise. The descriptor matrix $X$ must be standardized columnwise to zero mean and unit variance, and the response $y$ must be centered to zero mean before fitting; the intercept should be handled consistently with centering.\n\nDefine the LASSO objective for coefficient vector $\\beta \\in \\mathbb{R}^{p}$:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm. Implement the LASSO solver via coordinate descent based on first principles and convex optimization. The intercept term must be unpenalized and treated by centering $y$. Feature selection is defined by the indices $j$ for which the fitted coefficient $\\hat{\\beta}_j$ is nonzero.\n\nTo test feature stability, perform bootstrap resampling: generate $B$ bootstrap datasets by sampling $n$ rows of $(X,y)$ with replacement, refit the LASSO for each bootstrap replicate at the same $\\lambda$, and record the indicator that a feature is selected (nonzero coefficient). The selection frequency for feature $j$ is the fraction of bootstrap replicates in which $j$ is selected. Define the stable set at threshold $\\tau \\in [0,1]$ as those indices $j$ whose frequency is at least $\\tau$.\n\nYour program must:\n- Generate $X$ and $y$ according to the specified HEA-inspired correlated latent structure with parameters given by the test suite.\n- Standardize $X$ and center $y$ for each fit and each bootstrap replicate.\n- Implement coordinate descent for the LASSO objective with a soft-thresholding update, ensuring convergence to a minimizer within a prescribed tolerance.\n- Compute selection frequencies across bootstrap replicates and return the stable feature indices for each test case.\n\nTest suite parameterization and data generation:\n- For each test case, use $n = 240$ samples, $p = 8$ descriptors, an intercept $\\beta_0 = 2.0$ (in GPa), and a sparse true coefficient vector\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5],\n$$\nwhich corresponds to nonzero effects on $\\delta$, $\\mathrm{VEC}$, $K_{\\mathrm{avg}}$, and $G_{\\mathrm{avg}}$ respectively. Generate $X$ as linear combinations of latent variables $(u_1,u_2,u_3,u_4)$ with correlation strength $c \\in [0,1]$:\n- $X_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4$,\n- $X_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4$,\n- $X_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4$,\nwith $(u_1,u_2,u_3,u_4)$ drawn independently from $\\mathcal{N}(0,1)$ per sample. Let noise standard deviation be $\\sigma$ (in GPa).\n\nTest suite:\n- Case $1$ (general case): $\\lambda = 0.2$, $B = 200$, $\\tau = 0.6$, $c = 0.3$, $\\sigma = 0.3$.\n- Case $2$ (strong regularization boundary): $\\lambda = 1.2$, $B = 200$, $\\tau = 0.6$, $c = 0.3$, $\\sigma = 0.3$.\n- Case $3$ (strong descriptor correlation edge): $\\lambda = 0.25$, $B = 200$, $\\tau = 0.9$, $c = 0.9$, $\\sigma = 0.3$.\n\nAnswer specification:\n- For each test case, return the list of stable feature indices (zero-based) sorted in ascending order. Each list must be formatted with brackets and comma-separated integers, with no spaces, for example $[0,1,7]$.\n- The final program output must be a single line containing the three case results as a comma-separated list enclosed in square brackets, for example $[[i_{1,1},\\dots],[i_{2,1},\\dots],[i_{3,1},\\dots]]$.",
            "solution": "The problem statement is valid. It is scientifically grounded in established principles of statistical machine learning (LASSO regression, coordinate descent optimization, bootstrap resampling) and their application to a plausible materials science scenario (predicting hardness in high-entropy alloys). The problem is well-posed, providing a complete and consistent set of givens, including a generative data model, a precise optimization objective, and a clear algorithmic specification. All parameters are defined, and the task is computationally feasible. We may therefore proceed with a solution.\n\nThe objective is to implement a complete algorithmic pipeline to identify a stable set of descriptors for predicting High-Entropy Alloy (HEA) hardness. This involves three main stages: $1$) generating a synthetic, yet realistic, dataset; $2$) implementing the Least Absolute Shrinkage and Selection Operator (LASSO) using coordinate descent to perform feature selection; and $3$) assessing the stability of the selected features using bootstrap resampling.\n\n**1. Data Generation Model**\n\nThe problem defines a synthetic data generation process that mimics the correlated nature of physical descriptors in HEAs. We are given $n=240$ samples and $p=8$ descriptors. The design matrix $X \\in \\mathbb{R}^{n \\times p}$ is constructed from $4$ latent variables, $u_1, u_2, u_3, u_4$, where each latent variable is a vector of $n$ independent draws from a standard normal distribution, $\\mathcal{N}(0,1)$. The columns of $X$, representing the $p=8$ HEA descriptors, are linear combinations of these latent variables, with a parameter $c$ controlling the correlation strength, particularly between the first two descriptors. The specific linear combinations are:\n$$\n\\begin{aligned}\nX_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4 \\\\\nX_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4 \\\\\nX_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4\n\\end{aligned}\n$$\nThe response variable, hardness $y \\in \\mathbb{R}^n$, is generated by a linear model with a sparse true coefficient vector $\\beta^\\star \\in \\mathbb{R}^p$, an intercept $\\beta_0 \\in \\mathbb{R}$, and additive Gaussian noise $\\varepsilon \\in \\mathbb{R}^n$:\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon\n$$\nThe given parameters are an intercept $\\beta_0 = 2.0$ GPa, a noise standard deviation $\\sigma$, and a sparse coefficient vector:\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5]\n$$\nThis vector indicates that true effects are present for descriptors $0, 1, 5$, and $7$, which correspond to $\\delta$, $\\mathrm{VEC}$, $K_{\\mathrm{avg}}$, and $G_{\\mathrm{avg}}$, respectively. The noise vector $\\varepsilon$ is drawn from $\\mathcal{N}(0, \\sigma^2 I)$, where $I$ is the $n \\times n$ identity matrix.\n\n**2. LASSO via Coordinate Descent**\n\nThe core of the feature selection is the LASSO, which we solve by minimizing the following objective function for the coefficient vector $\\beta \\in \\mathbb{R}^p$:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1\n$$\nHere, $\\lambda \\ge 0$ is a hyperparameter that controls the strength of the $\\ell_1$-norm penalty, $\\|\\beta\\|_1 = \\sum_{j=0}^{p-1} |\\beta_j|$. This penalty encourages sparsity, driving some coefficients to exactly zero.\n\nBefore minimization, the data must be pre-processed. The design matrix $X$ is standardized column-wise to have zero mean and unit variance. The response vector $y$ is centered to have zero mean. This centering implicitly handles the intercept term $\\beta_0$, which is thereafter ignored in the LASSO fit for $\\beta$.\n\nWe implement the solver using coordinate descent. This iterative algorithm optimizes the objective function with respect to a single coefficient $\\beta_j$ at a time, holding all other coefficients $\\beta_{k \\ne j}$ fixed. For a standardized $X$ (where each column has a squared $\\ell_2$-norm of $n$), the objective for $\\beta_j$ is:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(r_i^{(j)} - X_{ij}\\beta_j\\right)^2 + \\lambda |\\beta_j|  + \\text{const}\n$$\nwhere $r_i^{(j)} = y_i - \\sum_{k \\ne j} X_{ik}\\beta_k$ is the $i$-th component of the partial residual. The part of the objective dependent on $\\beta_j$ simplifies to $\\frac{1}{2}\\beta_j^2 - \\rho_j\\beta_j + \\lambda|\\beta_j|$, where $\\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} r_i^{(j)}$. The value of $\\beta_j$ that minimizes this is given by the soft-thresholding operator $\\mathcal{S}_{\\lambda}(\\cdot)$:\n$$\n\\hat{\\beta}_j \\leftarrow \\mathcal{S}_{\\lambda}(\\rho_j) = \\text{sign}(\\rho_j) \\max(|\\rho_j| - \\lambda, 0)\n$$\nThe value $\\rho_j$ can be efficiently computed without re-forming the partial residual at each step. It is equal to $\\frac{1}{n}X_j^T(y - X\\beta) + \\beta_j$, where $y-X\\beta$ is the full residual vector using the current $\\beta$ estimate.\n\nThe algorithm proceeds by initializing $\\beta = 0$ and repeatedly cycling through all features $j=0, \\dots, p-1$, applying the update rule until the change in the $\\beta$ vector between cycles falls below a small tolerance.\n\n**3. Bootstrap Stability Analysis**\n\nLASSO's feature selection can be sensitive to the specific realization of the data, especially with correlated predictors. To assess the stability of our selections, we employ bootstrap resampling. The procedure is as follows:\n1. Generate $B$ bootstrap datasets. Each dataset $(X_b, y_b)$ is formed by sampling $n$ rows with replacement from the original dataset $(X, y)$.\n2. For each bootstrap replicate $b = 1, \\dots, B$:\n    a. Pre-process the bootstrap sample: standardize the columns of $X_b$ and center $y_b$. Note that the mean and standard deviation for standardization are computed *from the current bootstrap sample* $(X_b, y_b)$.\n    b. Fit the LASSO model on the processed $(X'_b, y'_b)$ using the given regularization parameter $\\lambda$ to obtain the coefficient estimate $\\hat{\\beta}^{(b)}$.\n    c. Record an indicator $I_j^{(b)} = 1$ if the coefficient $\\hat{\\beta}_j^{(b)}$ is non-zero (i.e., $|\\hat{\\beta}_j^{(b)}|  \\epsilon$ for some small numerical tolerance $\\epsilon  0$), and $I_j^{(b)} = 0$ otherwise.\n3. After all bootstrap fits are complete, calculate the selection frequency for each feature $j$ as the average of its indicators: $F_j = \\frac{1}{B} \\sum_{b=1}^{B} I_j^{(b)}$.\n4. The final stable feature set is defined as the collection of all indices $j$ for which the selection frequency $F_j$ meets or exceeds a stability threshold $\\tau$, i.e., $\\{j \\mid F_j \\ge \\tau\\}$.\n\nThis full pipeline is applied to each test case, which specifies a unique combination of model parameters $(\\lambda, B, \\tau, c, \\sigma)$, to produce a sorted list of stable feature indices.",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef lasso_cd(X, y, lambda_val, tol=1e-5, max_iter=1000):\n    \"\"\"\n    Solves the LASSO problem using coordinate descent.\n    Assumes X is standardized and y is centered.\n    Objective: min_beta 1/(2n) * ||y - X*beta||_2^2 + lambda_val * ||beta||_1\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            # The term rho_j is the simple least squares coefficient for feature j\n            # on the partial residual y - sum_{k!=j} X_k*beta_k.\n            # It can be computed efficiently from the full residual.\n            # rho_j = X_j^T * (y - X*beta + X_j*beta_j) / n\n            #       = (X_j^T * (y - X*beta) + X_j^T*X_j*beta_j) / n\n            # Since X_j is standardized, X_j^T*X_j = n.\n            # So, rho_j = (X_j^T * (y - X*beta))/n + beta_j\n            rho_j = (X[:, j].T @ (y - X @ beta)) / n + beta[j]\n            beta[j] = soft_threshold(rho_j, lambda_val)\n        \n        if np.max(np.abs(beta - beta_old))  tol:\n            break\n            \n    return beta\n\ndef generate_data(n, p, beta_0, beta_star, c, sigma):\n    \"\"\"Generates synthetic HEA descriptor and hardness data.\"\"\"\n    # Latent variables\n    u = np.random.randn(n, 4)\n    u1, u2, u3, u4 = u[:, 0], u[:, 1], u[:, 2], u[:, 3]\n\n    X = np.zeros((n, p))\n    X[:, 0] = u1 + c * u2 + 0.1 * u4\n    X[:, 1] = u2 + c * u1 + 0.1 * u4\n    X[:, 2] = 0.5 * u2 + 0.5 * u3 + 0.1 * u4\n    X[:, 3] = -0.8 * u1 + 0.2 * u4\n    X[:, 4] = 0.1 * u1 + u3 + 0.1 * u4\n    X[:, 5] = u3 + 0.2 * u2 + 0.1 * u4\n    X[:, 6] = 0.3 * u1 - 0.1 * u3 + 0.1 * u4\n    X[:, 7] = u3 + 0.3 * u1 + 0.1 * u4\n    \n    # Generate response variable y\n    epsilon = np.random.randn(n) * sigma\n    y = beta_0 + X @ beta_star + epsilon\n    \n    return X, y\n\ndef run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma):\n    \"\"\"Runs the full pipeline for one test case.\"\"\"\n    # 1. Generate the original, full dataset for this case\n    X_orig, y_orig = generate_data(n, p, beta_0, beta_star, c, sigma)\n    \n    selection_counts = np.zeros(p)\n    \n    # 2. Bootstrap stability analysis\n    for _ in range(B):\n        # Create bootstrap sample\n        indices = np.random.choice(n, size=n, replace=True)\n        X_b, y_b = X_orig[indices], y_orig[indices]\n\n        # Pre-processing for the bootstrap sample\n        y_b_centered = y_b - np.mean(y_b)\n        \n        X_b_mean = np.mean(X_b, axis=0)\n        X_b_std = np.std(X_b, axis=0)\n        # Handle columns with zero variance to avoid division by zero\n        X_b_std[X_b_std == 0] = 1.0\n        X_b_stdized = (X_b - X_b_mean) / X_b_std\n        \n        # Fit LASSO model\n        beta_hat = lasso_cd(X_b_stdized, y_b_centered, lambda_val)\n\n        # Record selected features (non-zero coefficients)\n        selected_features = np.where(np.abs(beta_hat) > 1e-7)[0]\n        selection_counts[selected_features] += 1\n        \n    # 3. Determine stable set\n    frequencies = selection_counts / B\n    stable_indices = np.where(frequencies >= tau)[0]\n    \n    return sorted(stable_indices.tolist())\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    # Common parameters defined in the problem\n    n = 240\n    p = 8\n    beta_0 = 2.0\n    beta_star = np.array([0.9, 0.7, 0.0, 0.0, 0.0, 0.2, 0.0, 0.5])\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (lambda_val, B, tau, c, sigma)\n        (0.2, 200, 0.6, 0.3, 0.3),    # Case 1\n        (1.2, 200, 0.6, 0.3, 0.3),    # Case 2\n        (0.25, 200, 0.9, 0.9, 0.3),   # Case 3\n    ]\n\n    # A fixed random seed is used to ensure the stochastic parts of the\n    # algorithm (data generation, bootstrapping) are reproducible.\n    np.random.seed(42)\n\n    case_results = []\n    for case_params in test_cases:\n        lambda_val, B, tau, c, sigma = case_params\n        stable_set = run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma)\n        \n        # Format each list as a string \"[i1,i2,...]\" with no spaces\n        list_as_string = f\"[{','.join(map(str, stable_set))}]\"\n        case_results.append(list_as_string)\n\n    # Final output must be a single line, formatted as a list of lists.\n    # e.g., [[0,1,7],[0],[5,7]]\n    print(f\"[{','.join(case_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A primary goal of materials informatics is to predict the properties of novel, unseen compositions, a task that requires extrapolation. Standard cross-validation methods are often insufficient as they test a model's interpolative power. This exercise guides you through the design and analysis of a leave-one-element-out cross-validation (LOEO-CV) scheme, a rigorous method for assessing a model's ability to generalize to chemical spaces beyond its training data. Understanding its principles and potential failure modes is essential for developing trustworthy predictive models for materials discovery. ",
            "id": "3750184",
            "problem": "Consider a dataset $\\mathcal{D} = \\{(x_i, y_i, p_i)\\}_{i=1}^N$ of High-Entropy Alloy (HEA) compositions $x_i$ and measured properties $y_i$ (for example, yield strength) under processing descriptors $p_i$ (for example, heat treatment schedules, cooling rates). Let the universe of possible elements be $E$, and each composition $x_i$ is represented by an element-fraction vector $(x_{i,e})_{e \\in E}$ that lies on the simplex, so that $x_{i,e} \\geq 0$ for all $e \\in E$ and $\\sum_{e \\in E} x_{i,e} = 1$. The prediction model uses a feature mapping $\\phi: \\Delta^{|E|} \\times \\mathcal{P} \\to \\mathbb{R}^d$, where $\\Delta^{|E|}$ denotes the composition simplex and $\\mathcal{P}$ denotes the processing descriptor space, and outputs $\\hat{y}_i = f_\\theta(\\phi(x_i, p_i))$ with parameters $\\theta$. Assume supervised learning with a loss function $\\ell(\\hat{y}, y)$ and training via empirical risk minimization.\n\nIndependent and identically distributed (i.i.d.) generalization is measured by the expected risk $R_P(f) = \\mathbb{E}_{(x,y,p) \\sim P}[\\ell(f(\\phi(x,p)), y)]$ under a data-generating distribution $P$. When testing robustness to novel element sets, a distribution shift occurs: test points contain an element $e^\\star \\in E$ that is entirely absent from training. A scientifically meaningful protocol must avoid leakage, respect the composition manifold, and isolate the effect of leaving an element out.\n\nYou are asked to identify a correct leave-one-element-out cross-validation (LOEO-CV) protocol and analyze its failure modes using first principles from statistical learning theory (for example, support coverage, covariate shift, and bias-variance trade-off) and physical realism in HEAs (for example, composition cardinality, element property distributions, and processing).\n\nWhich option(s) correctly implement and justify LOEO-CV to test generalization to novel elements and correctly analyze its principal failure modes?\n\nA. For each $e^\\star \\in E$, define $I_{\\text{train}}(e^\\star) = \\{i: x_{i,e^\\star} = 0\\}$ and $I_{\\text{test}}(e^\\star) = \\{i: x_{i,e^\\star}  0\\}$. Train $f_\\theta$ on $\\{(x_i,y_i,p_i): i \\in I_{\\text{train}}(e^\\star)\\}$ and evaluate on $\\{(x_i,y_i,p_i): i \\in I_{\\text{test}}(e^\\star)\\}$. Use per-element descriptors $g: E \\to \\mathbb{R}^m$ (for example, electronegativity, atomic radius, valence electron count) to construct $\\phi(x,p) = \\big[\\sum_{e \\in E} x_{e} g(e),\\, h(p)\\big]$, where $h$ encodes processing. Justify that this enables zero-shot extrapolation to $e^\\star$ because $g(e^\\star)$ is known a priori, and identify failure modes: support mismatch where $\\phi(P_{\\text{test}})$ is outside the convex hull of $\\phi(P_{\\text{train}})$, strong nonadditive interactions between elements not captured by $\\sum_{e} x_e g(e)$, and leakage risks if any normalization uses statistics from $I_{\\text{test}}(e^\\star)$. Report LOEO risk $R_{e^\\star}(f)$ and summarize performance by the average and worst case over $e^\\star$.\n\nB. Perform leave-one-alloy-out cross-validation: for each single alloy $i$, set $I_{\\text{train}} = \\{j \\neq i\\}$ and $I_{\\text{test}} = \\{i\\}$. Use one-hot element identity features within $\\phi$. Assert that this tests generalization to novel elements because the held-out alloy is unseen, and that one-hot features suffice.\n\nC. Use the protocol in Option A but normalize the target by per-element baselines computed from all of $\\mathcal{D}$, i.e., define $y'_i = y_i - \\sum_{e \\in E} x_{i,e} \\mu_e$ where $\\mu_e$ is the mean property over all alloys containing $e$. Train and evaluate on $y'_i$. Justify that this reduces variance and does not introduce leakage because the normalization is “unsupervised.”\n\nD. For LOEO with element $e^\\star$, exclude from training only those alloys with $x_{i,e^\\star} \\geq \\epsilon$ for a small threshold $\\epsilon  0$, but retain alloys with $0  x_{i,e^\\star}  \\epsilon$ in training to increase data size. Use any feature mapping $\\phi$, and claim that trace amounts of $e^\\star$ in training do not affect generalization.\n\nE. Implement LOEO as in Option A but stratify the splits to match the distribution of composition cardinality $k(x) = |\\{e: x_e  0\\}|$ and processing descriptors $p$, ensuring that $P_{\\text{train}}(k,p)$ matches $P_{\\text{test}}(k,p)$ conditional on the absence/presence of $e^\\star$. Use a compositional model $\\phi(x,p)$ that aggregates element-wise features via a permutation-invariant architecture (for example, a Deep Sets encoder or a Graph Neural Network (GNN) over element nodes with features $g(e)$ and edges weighted by pairwise mixing descriptors), and analyze failure modes including: co-occurrence pattern shift when $e^\\star$ commonly appears with rare element subsets; support mismatch in $\\phi$-space; and miscalibrated uncertainty under domain shift. Propose reporting both $\\max_{e^\\star} R_{e^\\star}(f)$ and calibration metrics under LOEO, and discuss potential remedies such as reweighting to align $P_{\\text{train}}(\\phi)$ with $P_{\\text{test}}(\\phi)$ and including interaction terms informed by thermodynamic mixing rules.\n\nSelect all that apply.",
            "solution": "The problem statement is valid. It presents a clear, scientifically grounded, and well-posed question within the domain of machine learning for materials science. The setup is formal and contains all necessary information to evaluate the proposed options.\n\nThe core task is to identify a correct leave-one-element-out cross-validation (LOEO-CV) protocol to test a model's ability to generalize to high-entropy-alloy (HEA) compositions containing an element that was entirely absent during training. A correct protocol must satisfy several key principles derived from statistical learning theory and materials science.\n\n1.  **Correct Data Splitting**: For each element $e^\\star$ in the universe of elements $E$, the training set must contain only data points $\\{(x_i, y_i, p_i)\\}$ for which the fraction of $e^\\star$, $x_{i,e^\\star}$, is exactly $0$. The test set must consist of all data points where $x_{i,e^\\star}  0$. This ensures the model is evaluated on its ability to perform zero-shot generalization to the novel element $e^\\star$.\n\n2.  **Extrapolative Feature Representation**: The model must be able to make predictions for compositions containing the unseen element $e^\\star$. This is impossible if features are based on element identities (e.g., one-hot encodings), as the model would have no learned parameters for $e^\\star$. The feature map $\\phi$ must therefore be constructed from intrinsic, element-agnostic descriptors, such as a vector of physical properties $g(e)$ (e.g., atomic radius, electronegativity) for each element $e \\in E$. A common approach is to form a composition-level feature by taking a weighted average, $\\sum_{e \\in E} x_e g(e)$. The model learns a mapping from this property space to the target, and because $g(e^\\star)$ is known, it can compute features for test compositions and extrapolate.\n\n3.  **Prevention of Data Leakage**: All steps of model training, including any data preprocessing like normalization or feature scaling, must be performed using only the training data for the current fold. Applying statistics (e.g., mean, standard deviation) computed from the full dataset $\\mathcal{D}$ before splitting causes information from the test set to \"leak\" into the training process, leading to an overly optimistic and invalid estimate of generalization performance.\n\n4.  **Meaningful Failure Analysis**: A robust protocol involves not just measuring error but also understanding why a model might fail. Key failure modes in this context include:\n    *   **Covariate Shift**: The distribution of features in the test set, $P_{\\text{test}}(\\phi)$, may differ significantly from the training set, $P_{\\text{train}}(\\phi)$. In particular, test-set feature vectors may lie outside the support of the training-set feature vectors, forcing the model to extrapolate, which is often unreliable.\n    *   **Model Misspecification**: The feature representation may be too simplistic to capture the complex physics of HEAs. For instance, a linear combination of elemental properties, $\\sum_{e \\in E} x_e g(e)$, neglects non-additive interaction effects between elements, which are physically crucial.\n    *   **Confounding Variables**: The presence or absence of an element $e^\\star$ might be correlated with other factors, such as the number of elements in the alloy (composition cardinality, $k(x)$) or the processing conditions $p$. A naive split can lead to the model learning these spurious correlations instead of the true effect of $e^\\star$.\n\nBased on these principles, we evaluate each option.\n\n**Option A Analysis:**\n-   **Data Splitting**: The option correctly defines the training and test sets for each fold $e^\\star$: $I_{\\text{train}}(e^\\star) = \\{i: x_{i,e^\\star} = 0\\}$ and $I_{\\text{test}}(e^\\star) = \\{i: x_{i,e^\\star}  0\\}$. This rigorously implements the LOEO principle.\n-   **Feature Representation**: It proposes using per-element physical descriptors $g(e)$ to build a compositional feature vector $\\phi(x,p) = \\big[\\sum_{e \\in E} x_{e} g(e),\\, h(p)\\big]$. This is a standard and valid approach that enables zero-shot extrapolation, as correctly justified by the fact that $g(e^\\star)$ is known a priori.\n-   **Failure Analysis**: It correctly identifies three primary failure modes: support mismatch in the feature space ($\\phi(P_{\\text{test}})$ outside the convex hull of $\\phi(P_{\\text{train}})$), model misspecification due to ignored non-additive interactions, and the risk of data leakage from improper normalization. This analysis is sound.\n-   **Evaluation**: The proposal to report average and worst-case risk across folds, $R_{e^\\star}(f)$, is a standard and robust way to summarize performance.\n\nThe protocol and analysis are scientifically and statistically sound.\n**Verdict: Correct**\n\n**Option B Analysis:**\n-   **Data Splitting**: This option describes leave-one-alloy-out cross-validation, where one sample $(x_i, y_i, p_i)$ is held out. This is not LOEO-CV. This procedure tests the model's ability to interpolate within the existing element space, not to extrapolate to a novel element. The training data will almost always contain all the elements present in the single held-out test alloy.\n-   **Feature Representation**: It proposes using one-hot element identity features. As explained above, this representation makes it impossible for the model to generalize to an element it has not seen in the training data.\n-   **Justification**: The claim that this tests generalization to novel elements is false. It fundamentally misunderstands the problem.\n\nThe proposed protocol is incorrect for the stated task.\n**Verdict: Incorrect**\n\n**Option C Analysis:**\n-   **Data Leakage**: This option proposes to normalize the target variable $y_i$ by subtracting a baseline, $y'_i = y_i - \\sum_{e \\in E} x_{i,e} \\mu_e$. The baseline term $\\mu_e$ is defined as the mean property over all alloys containing element $e$, computed from the entire dataset $\\mathcal{D}$. In a LOEO-CV fold where element $e^\\star$ is held out, the test set is, by definition, all alloys containing $e^\\star$. Thus, the calculation of $\\mu_e$ for any element $e$ that co-occurs with $e^\\star$ in the dataset will use data from the test set of the $e^\\star$ fold. This information is then used to transform the training data, constituting a severe form of data leakage.\n-   **Justification**: The claim that this does not introduce leakage because it's \"unsupervised\" is false. The term is misleading, as the target variable $y$ is used to compute the $\\mu_e$ values. More importantly, it violates the golden rule of cross-validation: no information from the test fold can be used to prepare the training data or model for that fold.\n\nThe proposed normalization scheme is flawed and introduces data leakage.\n**Verdict: Incorrect**\n\n**Option D Analysis:**\n-   **Data Splitting**: It proposes to keep alloys with trace amounts ($0  x_{i,e^\\star}  \\epsilon$) of the held-out element $e^\\star$ in the training set. This violates the core premise of LOEO-CV, which is to test generalization from complete absence to presence. By including trace amounts, the problem is changed from zero-shot extrapolation to low-data interpolation/extrapolation, which is a different and easier task.\n-   **Justification**: The claim that trace amounts do not affect generalization is scientifically and statistically baseless. In materials, even small dopant levels can have significant effects. From a learning perspective, any non-zero presence of $e^\\star$ provides the model with information, defeating the purpose of the zero-shot test.\n\nThe protocol is fundamentally flawed for the stated goal.\n**Verdict: Incorrect**\n\n**Option E Analysis:**\n-   **Data Splitting**: It begins with the correct LOEO split from Option A. It then improves it by proposing stratification based on composition cardinality $k(x)$ and processing descriptors $p$. This is an advanced technique to control for confounding variables and ensures that a difference in performance is more likely due to the element's effect rather than spurious correlations, making the evaluation more robust.\n-   **Feature Representation**: It proposes using more powerful, permutation-invariant architectures like GNNs or Deep Sets. These models go beyond the simple additive assumption of Option A and can learn complex, non-linear interaction effects between elements, which is more physically realistic for HEAs. This directly addresses one of the key failure modes identified in Option A.\n-   **Failure Analysis**: It provides a more nuanced analysis of failure modes, including co-occurrence pattern shift (a deeper form of covariate shift) and miscalibrated uncertainty, which is critical for out-of-distribution generalization.\n-   **Evaluation and Remedies**: It suggests reporting calibration metrics in addition to risk, which is best practice for models intended for scientific discovery. It also discusses appropriate remedies like covariate-shift reweighting and physics-informed modeling.\n\nThis option represents a more sophisticated, rigorous, and state-of-the-art implementation of the LOEO-CV concept. It builds upon the correct foundations of Option A and enhances them significantly.\n**Verdict: Correct**\n\nBoth Option A and Option E describe correct protocols. Option A outlines a valid, fundamental approach, while Option E describes a more advanced and robust version of the same core idea. As the question asks to select all that apply, both are correct.",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}