## 引言
[高熵合金](@entry_id:141320)（HEAs）因其独特的成分设计理念和优异的潜在性能，开辟了先进金属材料研究的新纪元。然而，其广阔无垠的成分空间也为传统的“试错法”实验探索带来了巨大挑战。机器学习作为一种强大的数据驱动范式，为在这片浩瀚的材料宇宙中高效导航、加速发现具有特定性能的新型合金提供了前所未有的机遇。

尽管机器学习的应用日益增多，但将算法简单地应用于原始数据往往会得到不可靠甚至具有误导性的结果。真正的挑战在于如何构建一个既遵循物理规律又在统计学上稳健的预测框架，从而将“黑箱”模型转变为透明、可信赖的科学工具。本文旨在填补这一知识鸿沟，系统性地介绍一套用于[高熵合金](@entry_id:141320)性能预测的严谨机器学习方法论。

本文将带领读者踏上一段从理论到实践的旅程。在“原理与机制”一章中，我们将深入探讨[材料表征](@entry_id:161346)、特征工程、[模型评估](@entry_id:164873)和[不确定性量化](@entry_id:138597)的核心理论基础。接着，在“应用与交叉学科联系”一章中，我们将展示这些原理如何应用于连接模拟与实验、实现[主动学习](@entry_id:157812)和[材料逆向设计](@entry_id:1126672)等前沿场景。最后，在“动手实践”一章中，读者将有机会通过具体的编程练习来巩固所学知识。

这段旅程的起点，是理解如何将一种材料的本质——其化学成分与结构——转化为机器学习模型能够理解和利用的语言。让我们从构建可靠预测模型的第一块基石开始：材料的严谨表征。

## 原理与机制

在利用机器学习方法预测[高熵合金](@entry_id:141320)（HEAs）的性能时，一个严谨的框架是至关重要的。这不仅关乎选择一个“最佳”算法，更在于如何以物理上合理且统计学上稳健的方式来表征材料、构建特征、评估模型并量化预测的不确定性。本章将深入探讨支撑这些任务的核心原理与机制，为构建可靠的、数据驱动的材料发现模型奠定理论基础。

### 材料的表征：从组分到结构

[机器学习模型](@entry_id:262335)的起点是对材料的数学表征。对于[高熵合金](@entry_id:141320)而言，最基础也是最核心的信息是其化学成分。然而，如何将这些成分信息有效地传递给模型，并进一步融入结构信息，是预测性能的关键第一步。

#### [成分数据](@entry_id:153479)的挑战：[闭包](@entry_id:148169)约束与[单纯形几何](@entry_id:1131660)

[高熵合金](@entry_id:141320)的成分通常由一个包含 $N$ 种元素[摩尔分数](@entry_id:145460)（或原子分数）的向量 $\mathbf{x} = (x_1, x_2, \dots, x_N)$ 来描述。这个向量必须满足两个基本约束：非负性 ($x_i \ge 0$) 和[闭包](@entry_id:148169)性（或称求和为一约束，$\sum_{i=1}^{N} x_i = 1$)。这两个约束决定了所有可能成分构成的空间在几何上的本质。

具体来说，所有满足这些约束的 $N$ 维向量 $\mathbf{x}$ 构成了一个嵌入在 $\mathbb{R}^N$ 空间中的 $(N-1)$ 维标准单纯形（standard simplex）。例如，对于三元合金（$N=3$），成分空间是一个位于三维空间中的等边三角形；对于二元合金（$N=2$），它是一条线段。这个几何空间的维度是 $N-1$ 而非 $N$，因为[闭包](@entry_id:148169)约束 $\sum x_i = 1$ 移除了一个自由度；给定前 $N-1$ 个组分的分数，第 $N$ 个组分的分数就完全确定了 。

[闭包](@entry_id:148169)约束带来了深刻的统计学挑战。由于各组分分数不是相互独立的，直接在原始组分向量上应用标准的统计或机器学习方法（如计算协方差、欧氏距离等）会产生误导性甚至虚假的结果。例如，考虑组分向量 $\mathbf{X}$ 的协方差矩阵 $\mathbf{\Sigma}$。由于 $\sum X_i = 1$ 是一个常数，其方差为零：
$$
\mathrm{Var}\left(\sum_{i=1}^{N} X_i\right) = \sum_{i=1}^{N} \sum_{j=1}^{N} \mathrm{Cov}(X_i, X_j) = 0
$$
这直接导致[协方差矩阵](@entry_id:139155) $\mathbf{\Sigma}$ 是奇异的（singular），其秩最多为 $N-1$。更进一步，对于任何一个组分 $X_k$，我们有 $\sum_{i \neq k} \mathrm{Cov}(X_k, X_i) = -\mathrm{Var}(X_k)$。这意味着，即使在某个潜在空间中各组分的变化是独立的，[闭包](@entry_id:148169)约束也会人为地引入负协方差。这种虚假的负相关性会严重干扰那些依赖于协方差或[相关性分析](@entry_id:893403)的模型 。

同样，在单纯形上使用标准的欧氏距离 $d_E(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2$ 也是有问题的。[成分数据](@entry_id:153479)的内在几何结构是相对的，而非绝对的。例如，从 $(0.5, 0.25, 0.25)$ 变为 $(0.6, 0.2, 0.2)$ 和从 $(0.1, 0.45, 0.45)$ 变为 $(0.2, 0.4, 0.4)$，在欧氏距离上是相同的，但其在化学和物理意义上的“扰动”幅度可能完全不同。一个更深刻的问题是，欧氏距离不具备一种称为“扰动不变性”的关键性质。在[成分数据分析](@entry_id:152698)中，一个自然的运算是对成分进行乘法扰动，例如将 $\mathbf{x}$ 变为 $C(\mathbf{x} \odot \mathbf{p})$，其中 $C(\cdot)$ 是[闭包算子](@entry_id:747392)，$ \odot $ 是逐元素相乘。一个理想的[距离度量](@entry_id:636073)应该在所有数据点都受到相同的乘法扰动时保持不变。欧氏距离不满足此性质，而 **[艾奇逊距离](@entry_id:918333) (Aitchison distance)** $d_A$ 则满足。这可以通过一个思想实验来检验：当整个数据集和查询点都受到相同的乘法扰动时，基于[艾奇逊距离](@entry_id:918333)的最近邻预测结果保持不变，而基于欧氏距离的预测结果则会发生改变，从而引入度量诱导的偏差 。

#### [成分数据分析](@entry_id:152698)：一个有原则的解决方案

为了克服这些挑战，我们需要将[成分数据](@entry_id:153479)从受约束的单纯形空间映射到一个无约束的欧氏空间。这正是 **[成分数据分析](@entry_id:152698) (Compositional Data Analysis, CoDa)** 的核心思想。通过 **对数比变换 (log-ratio transformations)**，我们可以“打开”单纯形，使得标准的统计方法得以应用。

其中最重要和最严谨的变换是 **等距对数比 (isometric log-ratio, ilr)** 变换。ilr变换将 $D$ 维单纯形等距地映射到 $\mathbb{R}^{D-1}$ 空间，这意味着在单纯形上的[艾奇逊距离](@entry_id:918333)等于在ilr坐标系下的欧氏距离。ilr坐标是相互正交的，并且其协方差矩阵是无约束且满秩的，从而彻底解决了奇异性问题  。

ilr坐标的构建基于一个 **序贯二元划分 (Sequential Binary Partition, SBP)**。SBP将 $D$ 个组分通过一系列层级逐步划分为两个不相交的子集，直到每个子集只包含一个组分。在每一步划分中，一个ilr坐标（也称为一个 **平衡 (balance)**）被定义为两个子集组分[几何平均数](@entry_id:275527)的对数比，并带有一个[标准化](@entry_id:637219)的缩放因子。对于一个划分为包含 $r$ 个组分的“正”组（$+$）和包含 $s$ 个组分的“负”组（$-$）的平衡，其ilr坐标 $z$ 的计算公式为：
$$
z = \sqrt{\frac{rs}{r+s}} \ln\left(\frac{g(\mathbf{x}_{+})}{g(\mathbf{x}_{-})}\right)
$$
其中 $g(\mathbf{x}_{\cdot})$ 是对应组分分[数的几何](@entry_id:192990)平均值，例如 $g(\mathbf{x}_{+}) = (\prod_{i \in +} x_i)^{1/r}$。

例如，对于一个包含 $\{\mathrm{Al}, \mathrm{Co}, \mathrm{Cr}, \mathrm{Fe}, \mathrm{Ni}\}$ 五种元素的合金，我们可以根据物理或化学直觉定义一个SBP。假设第一步我们将 $\{\mathrm{Al}, \mathrm{Co}, \mathrm{Cr}\}$ 与 $\{\mathrm{Fe}, \mathrm{Ni}\}$ 分开，则对应的第一个ilr坐标将是：
$$
z_{1} = \sqrt{\frac{3 \cdot 2}{3 + 2}} \,\ln\!\left(\frac{(x_{\mathrm{Al}} x_{\mathrm{Co}} x_{\mathrm{Cr}})^{1/3}}{(x_{\mathrm{Fe}} x_{\mathrm{Ni}})^{1/2}}\right)
$$
通过继续这个过程直到产生 $D-1=4$ 个坐标，我们就得到了一个完整的、无约束的、能够捕捉原始成分所有信息的[特征向量](@entry_id:151813) 。在机器学习实践中，将原始成分通过ilr变换转换成新的[特征向量](@entry_id:151813)，是处理[成分数据](@entry_id:153479)的最严谨方法。

#### [特征工程](@entry_id:174925)：从元素到物理描述符

尽管ilr变换解决了[成分数据](@entry_id:153479)的几何问题，但变换后的坐标本身可能缺乏直接的物理解释。为了构建具有高预测能力的模型，通常需要设计能捕捉合金内在物理和化学规律的 **描述符 (descriptors)** 或 **特征 (features)**。

##### 物理驱动的描述符

[高熵合金](@entry_id:141320)的设计理念本身就植根于[热力学](@entry_id:172368)。因此，基于[热力学](@entry_id:172368)和[电子结构理论](@entry_id:172375)的描述符是[特征工程](@entry_id:174925)的基石。

一个核心描述符是 **[构型熵](@entry_id:147820) (configurational entropy)**。对于一个理想的多元[固溶体](@entry_id:137535)，其摩尔构型熵可以通过统计力学推导得出。从[玻尔兹曼原理](@entry_id:148572) $S = k_B \ln \Omega$ 出发，其中 $\Omega$ 是将 $N$ 种原子排布在[晶格](@entry_id:148274)上的微观状态数，利用[斯特林近似](@entry_id:137296)，可以推导出摩尔[构型熵](@entry_id:147820)的著名公式 ：
$$
S_{\mathrm{conf}} = -R \sum_{i=1}^{N} x_i \ln x_i
$$
其中 $R$ 是摩尔气体常数。该熵在等原子比成分（$x_i = 1/N$）时达到最大值 $S_{\mathrm{conf}}^{\max} = R \ln N$。[高熵合金](@entry_id:141320)和中熵合金的常见操作性定义正是基于这一数值，例如，当 $S_{\mathrm{conf}} \ge 1.5 R$ 时称为[高熵合金](@entry_id:141320) 。

[构型熵](@entry_id:147820)的稳定化效应与 **[混合焓](@entry_id:158999) ($\Delta H_{\mathrm{mix}}$)** 相互竞争，共同决定了固溶体相的稳定性。[混合吉布斯自由能](@entry_id:137582) $\Delta G_{\mathrm{mix}} = \Delta H_{\mathrm{mix}} - T \Delta S_{\mathrm{mix}}$ 必须为负，固溶体才能稳定形成。我们可以定义一个无量纲参数 $\Omega = \frac{T_m S_{\mathrm{conf}}}{|\Delta H_{\mathrm{mix}}|}$ 来量化在熔点 $T_m$ 附近[熵稳定化](@entry_id:1124557)效应与焓驱动力的相对强度。当 $\Delta H_{\mathrm{mix}} > 0$ 时，$\Omega > 1$ 是形成单相[固溶体](@entry_id:137535)的一个重要判据 。这些[热力学](@entry_id:172368)量因此成为预测[相形成](@entry_id:1129580)能力的关键特征。

另一个广泛使用的描述符是 **[价电子浓度](@entry_id:192529) (Valence Electron Concentration, VEC)**，定义为组分价电子数的成分加权平均值，$\mathrm{VEC} = \sum x_i v_i$。VEC与[晶体结构](@entry_id:140373)的稳定性密切相关，例如，在许多过渡金属HEAs中，低VEC倾向于形成[BCC结构](@entry_id:159577)，而高[VEC](@entry_id:192529)倾向于形成FCC结构。

##### 系统化的特征生成

除了这些特定的物理描述符，我们还可以采用更系统化的方法来生成特征。一个强大的范式是将合金成分 $\mathbf{x}$ 视为一个[离散概率分布](@entry_id:166565)，该分布作用于各元素的基本物理性质（如[原子半径](@entry_id:139257)、电负性、熔点等）。在此框架下，我们可以计算这些性质分布的统计矩作为特征。最常用的包括 ：
- **一阶矩（均值）**：$\mu_p = \sum_{i=1}^{N} x_i p_i$，代表了性质的平均值，例如[VEC](@entry_id:192529)就是价电子数的均值。
- **二阶[中心矩](@entry_id:270177)（方差）**：$\sigma_p^2 = \sum_{i=1}^{N} x_i (p_i - \mu_p)^2$，代表了性质的离散程度。例如，[原子半径](@entry_id:139257)的方差（通常用其平方根，即[原子尺寸失配](@entry_id:1121229)度 $\delta$ 来表示）与[晶格畸变](@entry_id:1127106)密切相关。
- **三阶标准[中心矩](@entry_id:270177)（偏度）**：$\gamma_{1,p} = \frac{\sum x_i (p_i - \mu_p)^3}{(\sigma_p^2)^{3/2}}$，描述了性质分布的不对称性。

通过对多种元素性质（如[原子半径](@entry_id:139257)、电负性、熔点、[弹性模量](@entry_id:198862)等）计算这些统计矩，可以快速生成一个丰富且具有物理解释性的高维[特征向量](@entry_id:151813)。

##### 物理启发的特征变换

有时，原始特征与目标属性之间的关系是[非线性](@entry_id:637147)的。在这种情况下，直接使用[线性模型](@entry_id:178302)或某些[核方法](@entry_id:276706)可能会表现不佳。一个高级的[特征工程](@entry_id:174925)策略是根据物理原理设计[非线性](@entry_id:637147)特征变换。

例如，在利用[VEC](@entry_id:192529)预测FCC与BCC相稳定性时，实验观察到相变通常发生在一个狭窄的[VEC](@entry_id:192529)区间内。从[能带理论](@entry_id:139801)来看，相的[相对稳定性](@entry_id:262615)与d带填充有关，其能量差随电子填充数的变化通常是非单调的。这意味着，描述相稳定性的自由能差 $\Delta G$ 与VEC的关系可能是[非线性](@entry_id:637147)的。如果一个[线性分类器](@entry_id:637554)在相变区域附近频繁出错，这表明线性决策边界不足以描述该区域的物理。一个有原则的改进方法是，在相变阈值 $v_0$ 附近对VEC进行泰勒展开，并引入高阶项。例如，在模型中加入二次项 $(\mathrm{VEC}-v_0)^2$ 来捕捉能量[曲线的曲率](@entry_id:267366)，并引入交互项如 $(\mathrm{VEC}-v_0)\delta$ 来模拟[原子尺寸失配](@entry_id:1121229)对相变边界的影响。这种物理启发的[特征工程](@entry_id:174925)比盲目地使用高次多项式或[径向基函数](@entry_id:754004)等通用[非线性变换](@entry_id:636115)更有效，也更不容易[过拟合](@entry_id:139093) 。

#### 超越组分：结构化表征

对于许多性能（如[力学性能](@entry_id:201145)），仅有成分信息是不够的，原子排列的几何结构至关重要。近年来，随着[图神经网络](@entry_id:136853)（GNNs）的发展，直接将[晶体结构](@entry_id:140373)编码为 **图 (graph)** 成为一种前沿方法。

在一个晶体图表征中，**节点 (nodes)** 代表原子，**边 (edges)** 代表原子间的相互作用或“键”。为了构建一个适用于周期性晶体且在[消息传递神经网络](@entry_id:751916)（[MPN](@entry_id:910658)N）中有效的图，必须严格遵守物理原理 ：
- **节[点特征](@entry_id:155984)**：每个节点（原子）的[特征向量](@entry_id:151813)应包含不依赖于其绝对位置的信息，以保证[平移不变性](@entry_id:195885)。这通常包括代表元素身份的[独热编码](@entry_id:170007) (one-hot encoding)，以及其他元素特有的标量属性，如[原子序数](@entry_id:139400)、[电负性](@entry_id:147633)、原子质量等。
- **边与边特征**：边的构建必须考虑 **周期性边界条件 (Periodic Boundary Conditions, PBC)**。原子间的相互作用距离不是它们在单个晶胞中坐标的直接距离，而是跨越周期性边界的 **最小镜像距离 (minimal image distance)**。给定[晶格](@entry_id:148274)矩阵 $\mathbf{A}$ 和两个原子的[分数坐标](@entry_id:203215) $\mathbf{s}_i, \mathbf{s}_j$，它们之间的真实位移向量是 $\mathbf{r}_{ij} = \mathbf{A}(\mathbf{s}_j - \mathbf{s}_i + \mathbf{n})$，其中 $\mathbf{n}$ 是一个整数向量，代表晶胞平移。必须找到使 $\|\mathbf{r}_{ij}\|$ 最小化的 $\mathbf{n}^\star$。只有当这个最小距离小于某个物理[截断半径](@entry_id:136708) $r_{\mathrm{cut}}$ 时，才在两个原子间建立一条边。
- **边特征**：边的特征应编码局部几何环境。最基础的特征是最小镜像距离本身。为了使模型对旋转不敏感（如果预测的是标量属性），通常将距离通过一组[径向基函数](@entry_id:754004)（如高斯函数）展开成一个[特征向量](@entry_id:151813)。如果需要保留方向信息（例如预测张量属性），则可以将归一化的位移向量 $\mathbf{r}_{ij}/\|\mathbf{r}_{ij}\|$ 作为边特征的一部分。

这种基于图的表征方法能够捕捉到原子尺度的局部化学环境和几何构型，为预测那些强烈依赖于微观结构和短程有序的性能提供了强大的工具。

### 建模与评估范式

拥有了合适的[材料表征](@entry_id:161346)和特征后，接下来的挑战是选择合适的模型，并设计一个严谨的流程来训练和评估它，最终得到一个不仅准确而且可靠的预测器。

#### 模型选择与[偏差-方差权衡](@entry_id:138822)

对于给定的预测任务，可供选择的模型众多，主要包括[线性模型](@entry_id:178302)、基于树的集成模型和神经网络。模型的选择应基于对数据集特性和 **[偏差-方差权衡](@entry_id:138822) (bias-variance tradeoff)** 的理解。

材料科学中的数据集通常[样本量](@entry_id:910360)不大（例如，$N$ 在几百的量级），而特征维度可能不低（$d$ 在几十的量级） 。在这种“小N大d”或“小N中d”的情境下：
- **高容量模型**，如具有多层和宽层的大型神经网络，虽然理论上可以拟合任何复杂函数，但极易在小数据集上 **[过拟合](@entry_id:139093) (overfitting)**。它们的 **方差 (variance)** 会非常高，即模型会对训练数据中的随机噪声产生过度反应，导致其在未见过的数据上泛化能力很差。
- **低容量模型**，如简单的线性回归，方差较低，但可能因为无法捕捉材料属性与特征之间的复杂[非线性](@entry_id:637147)关系而存在高 **偏差 (bias)**，即 **[欠拟合](@entry_id:634904) (underfitting)**。
- **中等容量的正则化模型** 通常是最佳选择。例如，**带有[L2正则化](@entry_id:162880)（[岭回归](@entry_id:140984)）的多项式线性模型** 或 **基于浅层[决策树](@entry_id:265930)的[梯度提升](@entry_id:636838)机 (gradient boosting machines)**。特别是[梯度提升](@entry_id:636838)机，通过迭代地拟合残差，能够有效地学习复杂的[非线性](@entry_id:637147)函数和[特征交互](@entry_id:145379)，同时通过限制树的深度、使用[学习率](@entry_id:140210)和集成等技术来控制模型复杂度，从而在[偏差和方差](@entry_id:170697)之间取得良好平衡。因此，对于典型的HEAs表格数据预测任务，[梯度提升](@entry_id:636838)机通常是一个非常强大且可靠的基线模型。

#### 严谨的[模型评估](@entry_id:164873)

评估模型性能的黄金法则是使用独立于训练数据的测试集。然而，在小数据集上，简单的单次划分（如80/20划分）结果随机性很大。**K折[交叉验证](@entry_id:164650) (K-fold cross-validation)** 是更稳健的选择。在HEAs研究中，数据集常常包含对同一个独特成分的多次 **重复测量 (replicate measurements)**。这种数据结构给[交叉验证](@entry_id:164650)带来了陷阱。

如果将这些[重复测量](@entry_id:896842)点随机分配到不同的折中，模型在训练时可能会看到成分A的某个测量值，而在测试时预测成分A的另一个测量值。由于这些点来自同一成分，它们的信息高度相关，这会导致[训练集](@entry_id:636396)向[测试集](@entry_id:637546)发生 **信息泄露 (data leakage)**。其结果是，交叉验证会给出过于乐观的性能估计，无法真实反映模型对全新成分的泛化能力。

正确的做法是采用 **[分组交叉验证](@entry_id:634144) (Grouped Cross-Validation)**，其中“组”就是独特的材料成分。在划分数据时，必须确保来自同一成分的所有[重复测量](@entry_id:896842)点要么全部进入[训练集](@entry_id:636396)，要么全部进入测试集 。

此外，几乎所有[机器学习模型](@entry_id:262335)都包含需要调整的 **超参数 (hyperparameters)**（如正则化强度 $\lambda$、学习率 $\eta$ 等）。为了公正地比较不同模型家族并找到最佳模型，必须对每个模型都进行[超参数优化](@entry_id:168477)。最严谨的方法是 **[嵌套交叉验证](@entry_id:176273) (Nested Cross-Validation)**。外层循环用于性能评估，将数据划分为K个分组折。内层循环则在每个外层[训练集](@entry_id:636396)上再次进行交叉验证，以寻找最佳的超参数组合。只有这样，最终报告的性能才是对模型在未知数据上经过调优后表现的[无偏估计](@entry_id:756289)。

#### 处理实验噪声：异方差性

实验测量总伴随着噪声。一个常见的但往往不成立的假设是噪声的方差是恒定的（**[同方差性](@entry_id:634679), homoscedasticity**）。在材料科学中，测量的[绝对误差](@entry_id:139354)常常与测量值本身相关，导致噪声方差随输入变化，这种现象称为 **异方差性 (heteroscedasticity)**。例如，硬度越高的材料，其硬度测量的方差也可能越大 。

当数据集包含[重复测量](@entry_id:896842)时，我们可以直接从数据中估计这种[异方差性](@entry_id:895761)。对于每个独特成分 $x_i$，其重复测量的样本方差 $s_i^2$ 就是对该点真实噪声方差 $\sigma^2(x_i)$ 的一个估计。

在模型训练中，我们可以利用这些方差信息来[提升模型](@entry_id:909156)性能。**[加权最小二乘法](@entry_id:177517) (Weighted Least Squares, WLS)** 的原理是，在拟合模型时，赋予噪声较小的样本点更大的权重，而噪声较大的样本点权重则较小。具体来说，每个样本点的权重 $w_i$ 应与其噪声方差的倒数成正比，即 $w_i \propto 1/\sigma^2(x_i)$。在实践中，我们可以使用 $w_i = 1/s_i^2$ 对聚合后的训练点 $(\mathbf{x}_i, \bar{y}_i)$ 进行加权训练。对于支持样本权重的模型（如[梯度提升](@entry_id:636838)机和线性模型），这样做可以得到更精确、[统计效率](@entry_id:164796)更高的估计。

#### 量化预测的不确定性

在科学应用中，一个单一的“点预测”值往往是不够的；我们还需要知道模型对这个预测有多大的信心。预测的总不确定性可以分解为两种截然不同的类型 。

1.  **[偶然不确定性](@entry_id:634772) (Aleatoric Uncertainty)**：这是源于系统内在随机性的、不可减少的不确定性。在材料预测中，它对应于即使在成分和处理过程完全相同的条件下，由于微观结构（如[晶界](@entry_id:144275)、位错）的细微差异或测量过程本身的噪声所导致的性能波动。
    - **估计方法**：[偶然不确定性](@entry_id:634772)只能通过实验来量化。其最直接的估计来自于对同一输入条件下的 **重复实验**。如前所述，在给定成分 $\mathbf{x}$ 下的重复测量样本方差 $s^2(\mathbf{x})$ 就是对[偶然不确定性](@entry_id:634772)方差 $\sigma^2_{\text{aleatoric}}(\mathbf{x})$ 的直接估计。一个更复杂的模型可以被训练来直接预测这种异方差的噪声，例如，让神经网络不仅输出一个均值预测，还输出一个方差预测。

2.  **认知不确定性 (Epistemic Uncertainty)**：这是源于我们知识的局限性的、可以减少的不确定性。它反映了由于训练数据有限或模型本身不完美而导致的[模型参数不确定性](@entry_id:752081)。在数据稀疏的区域，认知不确定性通常较高。通过增加更多的数据，特别是目标区域的数据，可以有效地降低认知不确定性。
    - **估计方法**：认知不确定性可以通过评估模型在给定数据下的[后验分布](@entry_id:145605)来估计。常见的实用方法包括：
        - **贝叶斯方法**：如[贝叶斯神经网络](@entry_id:746725)或[高斯过程回归](@entry_id:276025)，它们直接输出一个预测值的概率分布，其方差反映了认知不确定性（在[高斯过程](@entry_id:182192)中，后验方差通常同时包含偶然和认知部分）。
        - **[集成方法](@entry_id:895145) (Ensembles)**：训练多个模型（例如，在不同的数据子集上，或从不同的随机初始化开始），然后观察它们预测值的分歧程度。一个由多个模型组成的集成，其预测值的方差是认知不确定性的一个良好度量。

根据 **[全方差公式](@entry_id:177482) (law of total variance)**，总的预测方差是这两类不确定性方差之和：
$$
\sigma_{\text{total}}^{2} = \sigma_{\text{aleatoric}}^{2} + \sigma_{\text{epistemic}}^{2}
$$
一个完整的预测模型应该能够分别报告这两部分不确定性。这不仅为用户提供了预测的置信区间，还为后续的[实验设计](@entry_id:142447)提供了宝贵的指导：如果[偶然不确定性](@entry_id:634772)主导，那么需要改进实验或测量技术；如果认知不确定性主导，那么需要在该区域采集更多的数据。