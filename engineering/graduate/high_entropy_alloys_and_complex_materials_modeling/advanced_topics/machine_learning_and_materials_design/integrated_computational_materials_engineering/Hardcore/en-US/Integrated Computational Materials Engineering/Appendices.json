{
    "hands_on_practices": [
        {
            "introduction": "In the ICME framework, models are often constructed by fitting to high-fidelity data, such as energies from Density Functional Theory. A robust model, however, requires more than just best-fit parameters; it requires a principled quantification of uncertainty. This practice introduces Bayesian linear regression as a powerful tool to determine the uncertainty in the parameters of a Cluster Expansion model, demonstrating how to derive the posterior distribution of the effective cluster interactions and use it to predict the uncertainty in the energy of a new configuration .",
            "id": "3746316",
            "problem": "A high-entropy alloy (HEA) on a face-centered cubic lattice is modeled using the cluster expansion, where the configurational energy of a supercell is written as a linear combination of a finite set of symmetry-distinct cluster functions. In the Integrated Computational Materials Engineering (ICME) workflow, Density Functional Theory (DFT) energies are used to fit the effective cluster interactions by solving a regression problem with uncertainty quantification. Consider a truncation to three cluster functions with coefficients $J_{1}$, $J_{2}$, and $J_{3}$, and denote by $\\boldsymbol{J} \\in \\mathbb{R}^{3}$ the vector of unknown interactions. Let $\\boldsymbol{\\phi}^{(i)} \\in \\mathbb{R}^{3}$ be the vector of the three cluster functions evaluated on the $i$-th training configuration.\n\nAssume the following data and modeling assumptions:\n- The training dataset consists of $m = 3$ supercells, whose cluster function vectors form the design matrix $\\boldsymbol{X} \\in \\mathbb{R}^{3 \\times 3}$ with rows $\\boldsymbol{\\phi}^{(1)} = (1, 0, 0)$, $\\boldsymbol{\\phi}^{(2)} = (0, 1, 0)$, and $\\boldsymbol{\\phi}^{(3)} = (0, 0, 1)$, i.e., $\\boldsymbol{X} = \\boldsymbol{I}_{3}$.\n- The DFT energies $\\boldsymbol{y} \\in \\mathbb{R}^{3}$ are modeled by a homoscedastic Gaussian noise model $y_{i} = \\boldsymbol{\\phi}^{(i)} \\cdot \\boldsymbol{J} + \\varepsilon_{i}$ with independent $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$ and known noise variance $\\sigma^{2} = 1.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$.\n- A zero-mean Gaussian prior is placed on $\\boldsymbol{J}$: $\\boldsymbol{J} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{S}_{0})$ with diagonal prior covariance $\\boldsymbol{S}_{0} = \\mathrm{diag}(s_{1}^{2}, s_{2}^{2}, s_{3}^{2})$ given by $s_{1}^{2} = 1.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$, $s_{2}^{2} = 4.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$, and $s_{3}^{2} = 9.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$.\n\nStarting from Bayes’ theorem and the linear-Gaussian model, derive the posterior covariance of $\\boldsymbol{J}$ and compute the posterior variances for $J_{1}$, $J_{2}$, and $J_{3}$. Then, for a new configuration with cluster function vector $\\boldsymbol{\\phi}_{\\ast} = (0.6, -0.3, 0.2)$, quantify the predictive uncertainty by deriving and evaluating the posterior predictive variance for a new noisy energy observation $y_{\\ast}$ in the same units and noise model.\n\nExpress all requested variances in $\\mathrm{eV}^{2}$ and round each to four significant figures. Provide your final answer as a single row matrix containing, in order: the posterior variance of $J_{1}$, the posterior variance of $J_{2}$, the posterior variance of $J_{3}$, and the posterior predictive variance of $y_{\\ast}$.",
            "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, objective, and self-contained. All necessary information is provided to derive the requested quantities. The problem is a standard application of Bayesian linear regression, which is a core technique in uncertainty quantification for materials modeling within the ICME framework.\n\nThe problem asks for the posterior variances of the model parameters $\\boldsymbol{J}$ and the posterior predictive variance for a new observation $y_{\\ast}$, based on a linear-Gaussian model. We begin with Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(\\boldsymbol{J} | \\boldsymbol{y}, \\boldsymbol{X}) \\propto p(\\boldsymbol{y} | \\boldsymbol{J}, \\boldsymbol{X}) p(\\boldsymbol{J})$$\n\nThe likelihood function is derived from the assumption of independent, identically distributed Gaussian noise $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$ for each observation $y_{i} = \\boldsymbol{\\phi}^{(i)} \\cdot \\boldsymbol{J} + \\varepsilon_{i}$. For the entire dataset of $m=3$ observations, this can be written in vector form as $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{J} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^{2}\\boldsymbol{I})$. The likelihood is thus a multivariate Gaussian distribution:\n$$p(\\boldsymbol{y} | \\boldsymbol{J}, \\boldsymbol{X}, \\sigma^2) = \\mathcal{N}(\\boldsymbol{y} | \\boldsymbol{XJ}, \\sigma^{2}\\boldsymbol{I}_{3})$$\n\nThe prior distribution on the parameters $\\boldsymbol{J}$ is given as a zero-mean Gaussian:\n$$p(\\boldsymbol{J} | \\boldsymbol{S}_{0}) = \\mathcal{N}(\\boldsymbol{J} | \\boldsymbol{0}, \\boldsymbol{S}_{0})$$\n\nSince the likelihood and prior are both Gaussian, the posterior distribution for $\\boldsymbol{J}$ is also Gaussian, $p(\\boldsymbol{J} | \\boldsymbol{y}, \\boldsymbol{X}) = \\mathcal{N}(\\boldsymbol{J} | \\boldsymbol{\\mu}_{N}, \\boldsymbol{S}_{N})$. The posterior covariance matrix $\\boldsymbol{S}_{N}$ is found by summing the precision matrices (inverse covariance matrices) of the prior and the likelihood data term. The precision of the prior is $\\boldsymbol{S}_{0}^{-1}$, and the precision from the data is $\\frac{1}{\\sigma^{2}}\\boldsymbol{X}^{T}\\boldsymbol{X}$.\nThe inverse of the posterior covariance is given by:\n$$\\boldsymbol{S}_{N}^{-1} = \\boldsymbol{S}_{0}^{-1} + \\frac{1}{\\sigma^{2}}\\boldsymbol{X}^{T}\\boldsymbol{X}$$\n\nWe are given the following values:\nThe design matrix is the identity matrix, $\\boldsymbol{X} = \\boldsymbol{I}_{3}$. Therefore, $\\boldsymbol{X}^{T}\\boldsymbol{X} = \\boldsymbol{I}_{3}^{T}\\boldsymbol{I}_{3} = \\boldsymbol{I}_{3}$.\nThe noise variance is $\\sigma^{2} = 1.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$.\nThe prior covariance matrix is diagonal: $\\boldsymbol{S}_{0} = \\mathrm{diag}(s_{1}^{2}, s_{2}^{2}, s_{3}^{2})$, with $s_{1}^{2} = 1.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$, $s_{2}^{2} = 4.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$, and $s_{3}^{2} = 9.0 \\times 10^{-2} \\ \\mathrm{eV}^{2}$.\n\nFirst, we compute the inverse of the prior covariance matrix, $\\boldsymbol{S}_{0}^{-1}$:\n$$\\boldsymbol{S}_{0}^{-1} = \\mathrm{diag}\\left(\\frac{1}{s_{1}^{2}}, \\frac{1}{s_{2}^{2}}, \\frac{1}{s_{3}^{2}}\\right) = \\mathrm{diag}\\left(\\frac{1}{1.0 \\times 10^{-2}}, \\frac{1}{4.0 \\times 10^{-2}}, \\frac{1}{9.0 \\times 10^{-2}}\\right) = \\mathrm{diag}\\left(100, 25, \\frac{100}{9}\\right) \\ \\mathrm{eV}^{-2}$$\n\nNext, we calculate the data precision term:\n$$\\frac{1}{\\sigma^{2}}\\boldsymbol{X}^{T}\\boldsymbol{X} = \\frac{1}{1.0 \\times 10^{-2}}\\boldsymbol{I}_{3} = 100 \\boldsymbol{I}_{3} = \\mathrm{diag}(100, 100, 100) \\ \\mathrm{eV}^{-2}$$\n\nNow we sum the precision matrices to find the posterior precision matrix $\\boldsymbol{S}_{N}^{-1}$:\n$$\\boldsymbol{S}_{N}^{-1} = \\mathrm{diag}\\left(100, 25, \\frac{100}{9}\\right) + \\mathrm{diag}(100, 100, 100)$$\n$$\\boldsymbol{S}_{N}^{-1} = \\mathrm{diag}\\left(100+100, 25+100, \\frac{100}{9}+100\\right) = \\mathrm{diag}\\left(200, 125, \\frac{1000}{9}\\right) \\ \\mathrm{eV}^{-2}$$\n\nThe posterior covariance matrix $\\boldsymbol{S}_{N}$ is the inverse of $\\boldsymbol{S}_{N}^{-1}$:\n$$\\boldsymbol{S}_{N} = (\\boldsymbol{S}_{N}^{-1})^{-1} = \\mathrm{diag}\\left(\\frac{1}{200}, \\frac{1}{125}, \\frac{9}{1000}\\right) \\ \\mathrm{eV}^{2}$$\n$$\\boldsymbol{S}_{N} = \\mathrm{diag}(0.005, 0.008, 0.009) \\ \\mathrm{eV}^{2}$$\n\nThe posterior variances for the parameters $J_{1}$, $J_{2}$, and $J_{3}$ are the diagonal elements of $\\boldsymbol{S}_{N}$:\n$\\mathrm{Var}(J_{1} | \\boldsymbol{y}) = (\\boldsymbol{S}_{N})_{11} = 0.005 \\ \\mathrm{eV}^{2}$\n$\\mathrm{Var}(J_{2} | \\boldsymbol{y}) = (\\boldsymbol{S}_{N})_{22} = 0.008 \\ \\mathrm{eV}^{2}$\n$\\mathrm{Var}(J_{3} | \\boldsymbol{y}) = (\\boldsymbol{S}_{N})_{33} = 0.009 \\ \\mathrm{eV}^{2}$\n\nNext, we derive the posterior predictive variance for a new observation $y_{\\ast}$ at a new input vector $\\boldsymbol{\\phi}_{\\ast} = (0.6, -0.3, 0.2)$. The model for the new observation is $y_{\\ast} = \\boldsymbol{\\phi}_{\\ast}^{T}\\boldsymbol{J} + \\varepsilon_{\\ast}$, where $\\varepsilon_{\\ast} \\sim \\mathcal{N}(0, \\sigma^{2})$. The predictive variance $\\mathrm{Var}(y_{\\ast} | \\boldsymbol{y})$ accounts for two sources of uncertainty: the uncertainty in the parameters $\\boldsymbol{J}$ (captured by the posterior covariance $\\boldsymbol{S}_{N}$) and the inherent observation noise $\\sigma^{2}$.\nThe predictive variance is given by the sum of the variance of the predicted mean and the variance of the noise:\n$$\\mathrm{Var}(y_{\\ast} | \\boldsymbol{y}, \\boldsymbol{X}, \\boldsymbol{\\phi}_{\\ast}) = \\mathrm{Var}(\\boldsymbol{\\phi}_{\\ast}^{T}\\boldsymbol{J} + \\varepsilon_{\\ast} | \\boldsymbol{y}) = \\mathrm{Var}(\\boldsymbol{\\phi}_{\\ast}^{T}\\boldsymbol{J} | \\boldsymbol{y}) + \\mathrm{Var}(\\varepsilon_{\\ast})$$\n$$\\mathrm{Var}(y_{\\ast} | \\boldsymbol{y}) = \\boldsymbol{\\phi}_{\\ast}^{T} \\mathrm{Var}(\\boldsymbol{J} | \\boldsymbol{y}) \\boldsymbol{\\phi}_{\\ast} + \\sigma^{2} = \\boldsymbol{\\phi}_{\\ast}^{T} \\boldsymbol{S}_{N} \\boldsymbol{\\phi}_{\\ast} + \\sigma^{2}$$\n\nWe substitute the values for $\\boldsymbol{\\phi}_{\\ast}$, $\\boldsymbol{S}_{N}$, and $\\sigma^{2}$:\n$$\\boldsymbol{\\phi}_{\\ast}^{T} \\boldsymbol{S}_{N} \\boldsymbol{\\phi}_{\\ast} = \\begin{pmatrix} 0.6  -0.3  0.2 \\end{pmatrix} \\begin{pmatrix} 0.005  0  0 \\\\ 0  0.008  0 \\\\ 0  0  0.009 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ -0.3 \\\\ 0.2 \\end{pmatrix}$$\n$$= (0.6)^{2}(0.005) + (-0.3)^{2}(0.008) + (0.2)^{2}(0.009)$$\n$$= (0.36)(0.005) + (0.09)(0.008) + (0.04)(0.009)$$\n$$= 0.0018 + 0.00072 + 0.00036 = 0.00288 \\ \\mathrm{eV}^{2}$$\n\nFinally, we add the noise variance $\\sigma^{2}$:\n$$\\mathrm{Var}(y_{\\ast} | \\boldsymbol{y}) = 0.00288 + \\sigma^{2} = 0.00288 + 0.01 = 0.01288 \\ \\mathrm{eV}^{2}$$\n\nThe problem requires rounding each value to four significant figures.\n- $\\mathrm{Var}(J_{1} | \\boldsymbol{y}) = 0.005 \\rightarrow 0.005000 \\ \\mathrm{eV}^{2}$\n- $\\mathrm{Var}(J_{2} | \\boldsymbol{y}) = 0.008 \\rightarrow 0.008000 \\ \\mathrm{eV}^{2}$\n- $\\mathrm{Var}(J_{3} | \\boldsymbol{y}) = 0.009 \\rightarrow 0.009000 \\ \\mathrm{eV}^{2}$\n- $\\mathrm{Var}(y_{\\ast} | \\boldsymbol{y}) = 0.01288 \\rightarrow 0.01288 \\ \\mathrm{eV}^{2}$\n\nThe requested results are assembled into a single row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.005000  0.008000  0.009000  0.01288\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once a statistical mechanics model of a material is parameterized, a key goal is to compute its macroscopic thermodynamic properties, particularly the Helmholtz free energy, which governs phase stability. Direct calculation of the absolute free energy is often intractable, but free energy *differences* can be found using methods like thermodynamic integration. This exercise guides you through the fundamental principles of thermodynamic integration, applying it to a model system to calculate the free energy difference between an ordered and disordered state, a common task in the study of high-entropy alloys .",
            "id": "3746287",
            "problem": "Consider a coarse-grained lattice model of a binary high-entropy alloy (HEA) within the framework of Integrated Computational Materials Engineering (ICME), where each lattice site can adopt one of two configurational states representing locally ordered or disordered environments. Let there be $N$ independent and equivalent lattice sites. At the disordered reference state, both configurations are energetically degenerate with energy $0$, while at the ordered target state, the locally ordered configuration is energetically stabilized by an energy $-\\Delta \\varepsilon$ relative to the disordered configuration. Introduce a coupling parameter $\\lambda \\in [0,1]$ that linearly interpolates between the two Hamiltonians, such that the $\\lambda$-dependent Hamiltonian is given by $H(\\lambda) = (1-\\lambda)H_{\\mathrm{A}} + \\lambda H_{\\mathrm{B}}$, where $H_{\\mathrm{A}}$ denotes the disordered reference Hamiltonian and $H_{\\mathrm{B}}$ denotes the ordered target Hamiltonian. For a single site with state variable $s \\in \\{0,1\\}$ (with $s=1$ denoting locally ordered and $s=0$ denoting disordered), the site Hamiltonian has the form $H_{\\text{site}}(\\lambda;s) = -\\lambda \\Delta \\varepsilon \\, s$. The system is in the canonical ensemble at absolute temperature $T$.\n\nStarting from the canonical partition function definition $Z(\\lambda) = \\sum_{\\text{all states}} \\exp\\!\\left(-\\beta H(\\lambda)\\right)$ with $\\beta = 1/(k_{\\mathrm{B}}T)$ and the Helmholtz free energy $F(\\lambda) = -k_{\\mathrm{B}}T \\ln Z(\\lambda)$, derive a thermodynamic integration relationship that expresses the free energy difference between the ordered and disordered states, $F_{\\mathrm{B}}-F_{\\mathrm{A}}$, as a one-dimensional integral over $\\lambda$ of an ensemble average involving the derivative of the Hamiltonian with respect to $\\lambda$. Then, for the given two-state non-interacting site model, construct the explicit integrand from first principles and numerically evaluate the integral.\n\nYour program must:\n- Use the canonical ensemble to compute the $\\lambda$-dependent ensemble average required by thermodynamic integration for the given model, based only on the definitions above and fundamental statistical mechanics.\n- Perform a numerical quadrature over $\\lambda \\in [0,1]$ to estimate the free energy difference $F_{\\mathrm{B}}-F_{\\mathrm{A}}$ for each provided test case.\n- Treat $k_{\\mathrm{B}}$ as the physical Boltzmann constant with value $k_{\\mathrm{B}} = 8.617333262145\\times 10^{-5}$ in electronvolt per Kelvin (eV/K), and express all final free energy differences in electronvolt (eV), rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$: $N = 100$, $\\Delta \\varepsilon = 0.05$ eV, $T = 1000$ K.\n- Case $2$: $N = 100$, $\\Delta \\varepsilon = 0.00$ eV, $T = 800$ K.\n- Case $3$: $N = 10$, $\\Delta \\varepsilon = 0.10$ eV, $T = 50$ K.\n- Case $4$: $N = 1000$, $\\Delta \\varepsilon = 0.02$ eV, $T = 300$ K.\n- Case $5$: $N = 1$, $\\Delta \\varepsilon = -0.05$ eV, $T = 300$ K.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the computed free energy difference $F_{\\mathrm{B}}-F_{\\mathrm{A}}$ in eV for the corresponding test case, rounded to $6$ decimal places.",
            "solution": "We begin from the canonical ensemble. The partition function at fixed $\\lambda$ is $Z(\\lambda) = \\sum_{\\text{states}} \\exp\\!\\left(-\\beta H(\\lambda)\\right)$, where $\\beta = 1/(k_{\\mathrm{B}}T)$. The Helmholtz free energy is $F(\\lambda) = -k_{\\mathrm{B}}T \\ln Z(\\lambda)$. Differentiating with respect to $\\lambda$ and using the chain rule yields\n\n$$\n\\frac{\\partial F}{\\partial \\lambda} = -k_{\\mathrm{B}}T \\frac{1}{Z(\\lambda)} \\frac{\\partial Z(\\lambda)}{\\partial \\lambda}.\n$$\n\nWe evaluate $\\partial Z/\\partial \\lambda$,\n\n$$\n\\frac{\\partial Z(\\lambda)}{\\partial \\lambda} = \\sum_{\\text{states}} \\left( -\\beta \\frac{\\partial H(\\lambda)}{\\partial \\lambda} \\right) \\exp\\!\\left(-\\beta H(\\lambda)\\right).\n$$\n\nTherefore,\n\n$$\n\\frac{\\partial F}{\\partial \\lambda} = \\frac{1}{Z(\\lambda)} \\sum_{\\text{states}} \\left( \\frac{\\partial H(\\lambda)}{\\partial \\lambda} \\right) \\exp\\!\\left(-\\beta H(\\lambda)\\right) = \\left\\langle \\frac{\\partial H(\\lambda)}{\\partial \\lambda} \\right\\rangle_{\\lambda},\n$$\n\nwhich is the thermodynamic integration relation. The free energy difference between the ordered and disordered end states is\n\n$$\nF_{\\mathrm{B}} - F_{\\mathrm{A}} = \\int_{0}^{1} \\left\\langle \\frac{\\partial H(\\lambda)}{\\partial \\lambda} \\right\\rangle_{\\lambda} \\, d\\lambda.\n$$\n\n\nWe now construct the ensemble average for the given model. The system consists of $N$ independent, equivalent sites, each with a state variable $s \\in \\{0,1\\}$. The site Hamiltonian is $H_{\\text{site}}(\\lambda; s) = -\\lambda \\Delta \\varepsilon \\, s$. Hence, for the full system,\n\n$$\nH(\\lambda; \\{s_i\\}) = -\\lambda \\Delta \\varepsilon \\sum_{i=1}^{N} s_i,\n$$\n\nand\n\n$$\n\\frac{\\partial H(\\lambda)}{\\partial \\lambda} = -\\Delta \\varepsilon \\sum_{i=1}^{N} s_i.\n$$\n\nBecause the sites are independent, the ensemble average factorizes, and we have\n\n$$\n\\left\\langle \\sum_{i=1}^{N} s_i \\right\\rangle_{\\lambda} = N \\left\\langle s \\right\\rangle_{\\lambda},\n$$\n\nwhere the one-site average is computed with Boltzmann weights. The one-site partition function is\n\n$$\nZ_{\\text{site}}(\\lambda) = \\sum_{s \\in \\{0,1\\}} \\exp\\!\\left(-\\beta H_{\\text{site}}(\\lambda;s)\\right) = 1 + \\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right).\n$$\n\nThe one-site probability of $s=1$ is\n\n$$\np_{\\lambda}(s=1) = \\frac{\\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}{1 + \\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)},\n$$\n\nand thus\n\n$$\n\\left\\langle s \\right\\rangle_{\\lambda} = \\frac{\\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}{1 + \\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}.\n$$\n\nTherefore,\n\n$$\n\\left\\langle \\frac{\\partial H(\\lambda)}{\\partial \\lambda} \\right\\rangle_{\\lambda} = -\\Delta \\varepsilon \\, N \\, \\frac{\\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}{1 + \\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}.\n$$\n\nThe thermodynamic integration becomes\n\n$$\nF_{\\mathrm{B}} - F_{\\mathrm{A}} = -\\Delta \\varepsilon \\, N \\int_{0}^{1} \\frac{\\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}{1 + \\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)} \\, d\\lambda.\n$$\n\nDefine $a = \\beta \\Delta \\varepsilon$. Noting that $\\frac{\\exp(ax)}{1+\\exp(ax)}$ is the logistic function $\\sigma(ax)$, we use the identity $\\int \\sigma(ax) \\, dx = \\frac{1}{a} \\ln\\!\\left(1+\\exp(ax)\\right) + C$. Hence,\n\n$$\n\\int_{0}^{1} \\frac{\\exp\\!\\left(a \\lambda\\right)}{1 + \\exp\\!\\left(a \\lambda\\right)} \\, d\\lambda = \\frac{1}{a} \\left[ \\ln\\!\\left(1+\\exp(a)\\right) - \\ln\\!\\left(1+\\exp(0)\\right) \\right] = \\frac{1}{a} \\left[ \\ln\\!\\left(1+\\exp(a)\\right) - \\ln(2) \\right].\n$$\n\nThus, the free energy difference has the closed form\n\n$$\nF_{\\mathrm{B}} - F_{\\mathrm{A}} = -\\Delta \\varepsilon \\, N \\cdot \\frac{1}{\\beta \\Delta \\varepsilon} \\left[ \\ln\\!\\left( \\frac{1+\\exp(\\beta \\Delta \\varepsilon)}{2} \\right) \\right] = -\\frac{N}{\\beta} \\ln\\!\\left( \\frac{1+\\exp(\\beta \\Delta \\varepsilon)}{2} \\right).\n$$\n\nThis closed form serves as a useful analytical check. However, the numerical solution requested is obtained by performing the quadrature\n\n$$\nF_{\\mathrm{B}} - F_{\\mathrm{A}} = \\int_{0}^{1} g(\\lambda) \\, d\\lambda, \\quad \\text{where} \\quad g(\\lambda) = -\\Delta \\varepsilon \\, N \\, \\frac{\\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}{1 + \\exp\\!\\left(\\beta \\lambda \\Delta \\varepsilon\\right)}.\n$$\n\n\nAlgorithmic plan for the program:\n- For each test case, parse $N$, $\\Delta \\varepsilon$ (in eV), and $T$ (in K).\n- Compute $\\beta = 1/(k_{\\mathrm{B}} T)$ with $k_{\\mathrm{B}}$ in eV/K. The integrand $g(\\lambda)$ is in eV, so the integral yields eV directly.\n- Define a numerically stable logistic function for $x = \\beta \\lambda \\Delta \\varepsilon$ to avoid overflow for large $|x|$: use $\\sigma(x) = \\begin{cases} \\frac{1}{1+\\exp(-x)},  x \\ge 0 \\\\ \\frac{\\exp(x)}{1+\\exp(x)},  x  0 \\end{cases}$.\n- Numerically integrate $g(\\lambda)$ over $\\lambda \\in [0,1]$ using an adaptive quadrature method with absolute and relative tolerances appropriate for eV-scale accuracy.\n- Round the resulting free energy difference to $6$ decimal places and collect the results for all test cases.\n- Print a single line in the required format: a comma-separated list enclosed in square brackets.\n\nEdge cases and coverage:\n- Case with $\\Delta \\varepsilon = 0$ yields $g(\\lambda) \\equiv 0$, so the integral gives $0$ eV exactly.\n- Negative $\\Delta \\varepsilon$ is physically meaningful for unfavorable ordering; the algorithm remains stable due to the logistic function’s symmetric properties.\n- High temperature (small $\\beta$) and low temperature (large $\\beta$) regimes are handled robustly by the stable logistic evaluation and adaptive integration.\n\nAll outputs are expressed in electronvolt (eV) and rounded to $6$ decimal places, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\n# Boltzmann constant in eV/K\nK_B_EV_PER_K = 8.617333262145e-5\n\ndef stable_logistic(x: float) - float:\n    \"\"\"\n    Numerically stable logistic function: sigma(x) = 1 / (1 + exp(-x))\n    Handles large |x| without overflow.\n    \"\"\"\n    if x = 0:\n        # For non-negative x, use the standard form\n        # Avoid overflow for large x by limiting exp(-x)\n        return 1.0 / (1.0 + np.exp(-x))\n    else:\n        # For negative x, use exp(x) / (1 + exp(x)) to avoid overflow in exp(-x)\n        ex = np.exp(x)\n        return ex / (1.0 + ex)\n\ndef ti_free_energy_difference(N: int, delta_e_ev: float, T_K: float) - float:\n    \"\"\"\n    Compute F_B - F_A via thermodynamic integration for the two-state lattice model.\n    Returns value in eV.\n    \"\"\"\n    beta = 1.0 / (K_B_EV_PER_K * T_K)\n\n    # Integrand g(lambda) = -N * delta_e_ev * sigma(beta * delta_e_ev * lambda)\n    def integrand(lmbda):\n        x = beta * delta_e_ev * lmbda\n        return -N * delta_e_ev * stable_logistic(x)\n\n    # Perform adaptive quadrature over lambda in [0, 1]\n    # Set tolerances to ensure accuracy at 1e-10 absolute, 1e-10 relative\n    result, _ = quad(integrand, 0.0, 1.0, epsabs=1e-12, epsrel=1e-12, limit=200)\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, delta_e_ev, T_K)\n    test_cases = [\n        (100, 0.05, 1000.0),   # Case 1\n        (100, 0.00, 800.0),    # Case 2\n        (10,  0.10, 50.0),     # Case 3\n        (1000,0.02, 300.0),    # Case 4\n        (1,  -0.05, 300.0),    # Case 5\n    ]\n\n    results = []\n    for N, delta_e_ev, T_K in test_cases:\n        f_diff_ev = ti_free_energy_difference(N, delta_e_ev, T_K)\n        # Round to 6 decimal places as required\n        results.append(f\"{f_diff_ev:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical link in the ICME chain is the connection between finite-scale simulations and macroscopic experimental reality. Computer simulations are performed on finite-sized systems, which can introduce systematic biases in measured quantities compared to the thermodynamic limit. This practice addresses the essential post-processing step of correcting for these finite-size effects, using polynomial regression and model selection to extrapolate short-range order parameters to infinite system size, thereby yielding results that are directly comparable to experimental measurements .",
            "id": "3746268",
            "problem": "Consider a substitutional multicomponent lattice model representative of a high-entropy alloy, studied within Integrated Computational Materials Engineering (ICME). The system is simulated using standard Markov Chain Monte Carlo (MCMC) under periodic boundary conditions in a cubic box of linear size $L$ (in lattice parameters) and total number of lattice sites $N=L^3$. Let $c_j$ denote the global concentration of species $j$ and let $P_{ij}$ be the conditional probability of finding species $j$ on a nearest-neighbor site of a site occupied by species $i$. The Warren–Cowley short-range order parameter is defined by the fundamental relation\n$$\\alpha_{ij} = 1 - \\frac{P_{ij}}{c_j},$$\nwhich is dimensionless. Monte Carlo sampling estimates $\\alpha_{ij}$ for finite $L$, giving $\\alpha_{ij}(L)$. Because the simulation enforces composition conservation within a finite volume and uses periodic boundaries, finite-size effects arise from constrained fluctuations and the finite integral extent of the pair correlation function. Under standard assumptions (finite correlation length smaller than $L$ and a well-mixed ergodic canonical ensemble), the leading bias in extensive observables scales inversely with system size $N=L^3$, implying that for nearest-neighbor short-range order one can model the systematic finite-size dependence as a smooth function of $x = 1/L^3$ with an analytic expansion. The objective is to extrapolate $\\alpha_{ij}(L)$ to its infinite-size limit $\\alpha_{ij}(\\infty)$ by modeling\n$$\\alpha_{ij}(L) \\approx a + b\\,x + c\\,x^2,$$\nwhere $a=\\alpha_{ij}(\\infty)$ and $x=1/L^3$, and selecting the simplest model consistent with the data using a principled criterion.\n\nStarting only from the canonical Boltzmann distribution, the definition of $\\alpha_{ij}$, and the constraint-driven $1/L^3$ leading-order finite-size bias, design an algorithm that, given measurements $\\{\\alpha_{ij}(L_k)\\}$ at different sizes $\\{L_k\\}$ with optional standard errors $\\{\\sigma_k\\}$, performs a weighted polynomial regression in $x=1/L^3$ of degree $1$ or $2$, selects the degree via the Bayesian Information Criterion (BIC), and returns the extrapolated infinite-size limit $a$ as a float. Use weights $w_k = 1/\\sigma_k^2$ when $\\sigma_k$ are provided, and uniform weights otherwise. Your regression must solve the normal equations exactly for the chosen polynomial degree and compute the residual sum of squares consistently with the weights. When comparing candidate models via BIC, prefer the model with the lower BIC; in the event of numerical ties within machine precision, prefer the model with fewer parameters.\n\nYour program must implement this algorithm and apply it to the following test suite. All $\\alpha_{ij}$ values are dimensionless. The final outputs must be floats rounded to six decimal places.\n\nTest Suite:\n1. Happy path, moderate sizes:\n   - $L = [8, 12, 16, 20, 24]$\n   - $\\alpha_{ij}(L) = [-0.10046875,\\,-0.114212962963,\\,-0.11755859375,\\,-0.11875,\\,-0.119276620370]$\n   - No $\\sigma_k$ provided.\n\n2. Boundary case, small sizes with stronger finite-size bias:\n   - $L = [4, 6, 8, 10]$\n   - $\\alpha_{ij}(L) = [0.435625,\\;0.160740740741,\\;0.093828125,\\;0.07]$\n   - No $\\sigma_k$ provided.\n\n3. Edge case, nearly zero infinite-size limit:\n   - $L = [10, 14, 18, 22, 26, 30]$\n   - $\\alpha_{ij}(L) = [-0.005,\\;-0.001822157434,\\;-0.000857338820,\\;-0.000469315264,\\;-0.000284471002,\\;-0.000185185185]$\n   - No $\\sigma_k$ provided.\n\n4. Weighted case with a mild outlier and provided uncertainties:\n   - $L = [10, 12, 14, 16, 18, 20]$\n   - $\\alpha_{ij}(L) = [-0.066,\\;-0.067685185185,\\;-0.060542274052,\\;-0.0690234375,\\;-0.069314865,\\;-0.0695]$\n   - $\\sigma = [0.0025,\\;0.0018,\\;0.0030,\\;0.0015,\\;0.0012,\\;0.0010]$\n\nYour program should produce a single line of output containing the extrapolated infinite-size limits for the four test cases, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (e.g., \"[a1,a2,a3,a4]\"). No physical units are required because $\\alpha_{ij}$ is dimensionless. No angles are involved. No percentages are used or required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistical mechanics and computational materials science, is well-posed with a clear objective, and provides all necessary information for a unique solution. The finite-size scaling model and the use of the Bayesian Information Criterion for model selection are standard, rigorous practices in the field.\n\nThe objective is to extrapolate the infinite-size Warren-Cowley short-range order parameter, $\\alpha_{ij}(\\infty)$, from a series of finite-size measurements, $\\{\\alpha_{ij}(L_k)\\}$. This is achieved by modeling the finite-size dependence and performing a regression analysis. The core of the algorithm involves fitting two candidate models to the data and using a principled criterion to select the most appropriate one.\n\nThe physical basis for the model is the systematic bias introduced in canonical ensemble simulations due to the strict conservation of composition in a finite volume. For extensive observables, this bias manifests as a leading-order correction proportional to the inverse of the system volume, $N=L^3$. The model provided, $\\alpha_{ij}(L) \\approx a + b x + c x^2$ with $x = 1/L^3$, represents a second-order Taylor expansion of the short-range order parameter as a function of the inverse system volume, $x$. The term $a$ corresponds to the extrapolated value in the thermodynamic limit ($L \\to \\infty$, hence $x \\to 0$).\n\nThe algorithmic procedure is as follows:\n1.  **Data Transformation**: For a given set of measurements of the short-range order parameter, $\\alpha_{ij}(L_k)$, at system sizes $L_k$, we define the dependent variable $y_k = \\alpha_{ij}(L_k)$ and the independent variable $x_k = 1/L_k^3$. The problem is now to fit $y_k$ as a function of $x_k$.\n\n2.  **Model Fitting by Weighted Least Squares**: We consider two polynomial models for the relationship between $y$ and $x$:\n    - **Model 1 (Linear)**: $y = a_1 + b_1 x$. This model has $p_1 = 2$ parameters, $\\boldsymbol{\\beta}_1 = [a_1, b_1]^T$.\n    - **Model 2 (Quadratic)**: $y = a_2 + b_2 x + c_2 x^2$. This model has $p_2 = 3$ parameters, $\\boldsymbol{\\beta}_2 = [a_2, b_2, c_2]^T$.\n\n    The parameters for each model are determined by solving the normal equations for weighted least squares, which minimizes the weighted residual sum of squares, $\\chi^2 = \\sum_{k=1}^{n} w_k (y_k - f(x_k, \\boldsymbol{\\beta}))^2$. The normal equations are expressed in matrix form as:\n    $$ (X^T W X) \\boldsymbol{\\beta} = X^T W \\boldsymbol{y} $$\n    Here, $\\boldsymbol{\\beta}$ is the vector of model parameters, $\\boldsymbol{y}$ is the vector of observed $y_k$ values, $X$ is the design matrix, and $W$ is a diagonal matrix of weights. The weights $w_k$ are defined as $w_k = 1/\\sigma_k^2$ if standard errors $\\sigma_k$ are provided, and $w_k = 1$ (uniform weights) otherwise.\n    For the linear model (Model 1), the design matrix for $n$ data points is:\n    $$ X_1 = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{pmatrix} $$\n    For the quadratic model (Model 2), the design matrix is:\n    $$ X_2 = \\begin{pmatrix} 1  x_1  x_1^2 \\\\ 1  x_2  x_2^2 \\\\ \\vdots  \\vdots  \\vdots \\\\ 1  x_n  x_n^2 \\end{pmatrix} $$\n    These linear systems are solved exactly for the parameter vectors $\\boldsymbol{\\beta}_1$ and $\\boldsymbol{\\beta}_2$. The desired extrapolated value, $a$, is the first element of the solution vector $\\boldsymbol{\\beta}$ in both cases.\n\n3.  **Model Selection using Bayesian Information Criterion (BIC)**: To prevent overfitting and select the simplest model that is consistent with the data, we use the BIC. The BIC penalizes models with more parameters. The model with the lower BIC is preferred. The specific form of the BIC depends on whether the error variances are considered known (from the provided $\\sigma_k$) or must be estimated from the data.\n    - **Unweighted Case (no $\\sigma_k$ provided)**: The error variance is unknown and is estimated from the residual sum of squares, $RSS = \\sum_{k=1}^n (y_k - f(x_k, \\boldsymbol{\\beta}))^2$. The BIC formula is:\n      $$ BIC = n \\ln\\left(\\frac{RSS}{n}\\right) + p \\ln(n) $$\n      where $n$ is the number of data points and $p$ is the number of model parameters.\n    - **Weighted Case ($\\sigma_k$ provided)**: The error variances are assumed to be known and equal to $\\sigma_k^2$. The weights are $w_k = 1/\\sigma_k^2$. The BIC is calculated from the minimized weighted residual sum of squares, $\\chi^2 = \\sum_{k=1}^n w_k (y_k - f(x_k, \\boldsymbol{\\beta}))^2$:\n      $$ BIC = \\chi^2 + p \\ln(n) $$\n\n4.  **Final Extrapolation**: For each test case, the BIC values for the linear model ($BIC_1$) and the quadratic model ($BIC_2$) are computed.\n    - If $BIC_1 \\le BIC_2$, the linear model (Model 1) is selected.\n    - If $BIC_2  BIC_1$, the quadratic model (Model 2) is selected.\n    The extrapolated infinite-size limit $a = \\alpha_{ij}(\\infty)$ is the intercept parameter ($a_1$ or $a_2$) from the selected model.\n\nThis procedure provides a robust and statistically principled method for executing the analysis specified in the problem statement, from raw simulation data to the extrapolated physical quantity.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"L\": [8, 12, 16, 20, 24],\n            \"alpha\": [-0.10046875, -0.114212962963, -0.11755859375, -0.11875, -0.119276620370],\n            \"sigma\": None\n        },\n        {\n            \"L\": [4, 6, 8, 10],\n            \"alpha\": [0.435625, 0.160740740741, 0.093828125, 0.07],\n            \"sigma\": None\n        },\n        {\n            \"L\": [10, 14, 18, 22, 26, 30],\n            \"alpha\": [-0.005, -0.001822157434, -0.000857338820, -0.000469315264, -0.000284471002, -0.000185185185],\n            \"sigma\": None\n        },\n        {\n            \"L\": [10, 12, 14, 16, 18, 20],\n            \"alpha\": [-0.066, -0.067685185185, -0.060542274052, -0.0690234375, -0.069314865, -0.0695],\n            \"sigma\": [0.0025, 0.0018, 0.0030, 0.0015, 0.0012, 0.0010]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        extrapolated_value = select_model_and_extrapolate(\n            case[\"L\"], case[\"alpha\"], case[\"sigma\"]\n        )\n        results.append(extrapolated_value)\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef solve_regression(x, y, weights, degree):\n    \"\"\"\n    Solves a weighted polynomial regression problem using normal equations.\n\n    Args:\n        x (np.ndarray): Independent variable data.\n        y (np.ndarray): Dependent variable data.\n        weights (np.ndarray): Weights for each data point.\n        degree (int): Degree of the polynomial to fit.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The fitted model parameters [a, b, c, ...].\n            - float: The weighted residual sum of squares (chi^2 or RSS).\n    \"\"\"\n    num_params = degree + 1\n    # The `np.vander` function with increasing=True gives [1, x, x^2, ...],\n    # which matches the parameter order [a, b, c, ...].\n    X = np.vander(x, num_params, increasing=True)\n    W = np.diag(weights)\n\n    # Normal equations: (X^T W X) beta = X^T W y\n    XT_W_X = X.T @ W @ X\n    XT_W_y = X.T @ W @ y\n    \n    try:\n        beta = np.linalg.solve(XT_W_X, XT_W_y)\n    except np.linalg.LinAlgError:\n        # This case should not be reached with the given test data.\n        return np.full(num_params, np.nan), np.inf\n\n    y_pred = X @ beta\n    rss_weighted = np.sum(weights * (y - y_pred)**2)\n\n    return beta, rss_weighted\n\ndef select_model_and_extrapolate(L_vals, alpha_vals, sigma_vals=None):\n    \"\"\"\n    Performs regression for linear and quadratic models, selects the best one\n    using BIC, and returns the extrapolated infinite-size limit.\n\n    Args:\n        L_vals (list): List of system sizes L.\n        alpha_vals (list): List of measured SRO parameters.\n        sigma_vals (list, optional): List of standard errors. Defaults to None.\n\n    Returns:\n        float: The extrapolated infinite-size limit (the intercept 'a').\n    \"\"\"\n    n = len(L_vals)\n    x = 1.0 / (np.array(L_vals, dtype=float)**3)\n    y = np.array(alpha_vals, dtype=float)\n\n    is_weighted = sigma_vals is not None\n    if is_weighted:\n        weights = 1.0 / (np.array(sigma_vals, dtype=float)**2)\n    else:\n        weights = np.ones(n, dtype=float)\n\n    # --- Model 1: Linear Fit (y = a + bx) ---\n    p1 = 2  # Number of parameters\n    beta1, rss_w1 = solve_regression(x, y, weights, degree=1)\n    a1 = beta1[0]\n\n    # --- Model 2: Quadratic Fit (y = a + bx + cx^2) ---\n    p2 = 3  # Number of parameters\n    beta2, rss_w2 = solve_regression(x, y, weights, degree=2)\n    a2 = beta2[0]\n\n    # --- Calculate BIC for both models ---\n    if is_weighted:\n        # For weighted fits (known variance), BIC = chi^2 + p * ln(n)\n        bic1 = rss_w1 + p1 * np.log(n)\n        bic2 = rss_w2 + p2 * np.log(n)\n    else:\n        # For unweighted fits (unknown variance), BIC = n*ln(RSS/n) + p*ln(n)\n        # Note: rss_w1 and rss_w2 are unweighted RSS since weights are 1.\n        if rss_w1 = 1e-15: # Avoid log of zero or negative\n            bic1 = -np.inf if rss_w1 == 0 else np.inf\n        else:\n            bic1 = n * np.log(rss_w1 / n) + p1 * np.log(n)\n\n        if rss_w2 = 1e-15:\n            bic2 = -np.inf if rss_w2 == 0 else np.inf\n        else:  \n            bic2 = n * np.log(rss_w2 / n) + p2 * np.log(n)\n\n    # --- Model Selection ---\n    # Prefer lower BIC. In case of a tie, prefer the simpler model (Model 1).\n    if bic1 = bic2:\n        return a1\n    else:\n        return a2\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}