{
    "hands_on_practices": [
        {
            "introduction": "Understanding tipping points begins with the system's underlying deterministic structure. This practice guides you through a numerical exploration of a canonical model for critical transitions, allowing you to trace the system's equilibria as a control parameter changes and witness how stable states can suddenly vanish at a saddle-node bifurcation. By simulating the system's path-dependent response, you will construct a hysteresis loop, a hallmark of systems with memory and irreversible shifts.",
            "id": "4105512",
            "problem": "Consider the one-dimensional ordinary differential equation (ODE) $dx/dt = \\mu - x^3 + x$, where $x$ is a scalar state variable and $\\mu$ is a real-valued control parameter. This canonical form arises as a reduced-model representation of slow-fast components in numerical weather prediction and climate modeling when analyzing tipping points and critical transitions, specifically the fold (saddle-node) bifurcation structure leading to hysteresis. Your task is to determine the equilibrium branches, assess their linear stability, and compute a hysteresis loop under slow parameter variation from first principles.\n\nStart from the foundational definitions:\n- An equilibrium $x^\\star$ satisfies $dx/dt=0$ for a fixed value of $\\mu$.\n- Linear stability of an equilibrium is determined by the sign of the derivative of the right-hand side with respect to $x$ at $x^\\star$. Negative derivative implies local asymptotic stability; positive derivative implies linear instability; zero derivative indicates neutral stability at a bifurcation point.\n\nDesign and implement an algorithm that:\n1. Determines, for any given $\\mu$, all real equilibria $x^\\star$ that satisfy the equilibrium condition and classifies their linear stability via the derivative with respect to $x$ (stable if negative, unstable if positive, neutral if zero). Use a numerically reliable procedure to find real roots of the cubic equation and to deduplicate near-coincident roots with a small tolerance.\n2. Computes a quasi-static hysteresis loop by sweeping $\\mu$ from a minimum value $\\mu_{\\min}$ to a maximum value $\\mu_{\\max}$ and back. For each incremental step in $\\mu$, the system state is assumed to adiabatically track the nearest available stable equilibrium branch. When the currently tracked stable branch disappears because of a saddle-node bifurcation, the state should jump to the nearest remaining stable branch. This selection rule models slow forcing and rapid relaxation typical of climate subsystems exhibiting multiple stable regimes.\n\nUse the following test suite of parameter values and requirements:\n- For equilibrium analysis, evaluate at the five parameter values $\\mu \\in \\{-0.5,\\,-\\frac{2}{3\\sqrt{3}},\\,0,\\,\\frac{2}{3\\sqrt{3}},\\,0.5\\}$. For each value of $\\mu$:\n  - Compute the number of distinct real equilibria (after deduplication).\n  - Return the sorted list of real equilibria $x^\\star$ as floating-point numbers.\n  - Return a list of stability codes for the same equilibria in the same order, where stability codes are integers: $-1$ for linearly stable, $+1$ for linearly unstable, and $0$ for neutral stability (derivative equal to zero).\n- For the hysteresis loop, use $\\mu_{\\min}=-0.5$, $\\mu_{\\max}=0.5$, and a uniform step size $\\Delta\\mu=0.01$ for both the upward and downward sweeps. Initialize the upward sweep from the lowest stable equilibrium at $\\mu_{\\min}$, and the downward sweep from the highest stable equilibrium at $\\mu_{\\max}$. Report the tracked state $x$ at the three sample points $\\mu \\in \\{-0.3,\\,0.0,\\,0.3\\}$ for both the upward sweep and the downward sweep as two lists of floating-point numbers.\n- Additionally, include a boolean result that indicates whether hysteresis is present at $\\mu=0.3$ by checking if the absolute difference between the upward-sweep state and the downward-sweep state at $\\mu=0.3$ is at least $10^{-6}$ (return the boolean $True$ if the difference is greater than or equal to $10^{-6}$, otherwise return $False$).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, the output must be a list with three items:\n  - The first item is a list containing, for each of the five test values of $\\mu$, a list with four entries: the floating-point $\\mu$, the integer number of real equilibria, the list of sorted real equilibria, and the list of corresponding stability codes.\n  - The second item is a list with two entries: the list of tracked states at the three sample points for the upward sweep, and the corresponding list for the downward sweep.\n  - The third item is the boolean hysteresis indicator at $\\mu=0.3$.\nNo physical units are involved in this problem, and there are no angles or percentages. The final output must be printed exactly as a single line in the specified format.",
            "solution": "The problem is assessed to be valid. It is scientifically sound, well-posed, and contains all necessary information to derive a unique, verifiable solution. The underlying model, $dx/dt = \\mu - x^3 + x$, represents the normal form of a saddle-node bifurcation, a canonical model in the study of tipping points and critical transitions, particularly relevant in climate science and other nonlinear dynamical systems. The task requires a systematic analysis of equilibria and a numerical simulation of hysteresis, both of which are standard procedures in this field.\n\nThe solution will be developed in two main parts: first, an analysis of the system's equilibria and their stability as a function of the parameter $\\mu$; second, a numerical simulation of the hysteresis loop based on quasi-static tracking of stable states.\n\n**Part 1: Equilibrium and Stability Analysis**\n\nAn equilibrium state, denoted $x^\\star$, is a point where the system's state variable does not change over time. This is mathematically expressed as the condition $dx/dt = 0$. For the given ordinary differential equation (ODE), this leads to the equilibrium equation:\n$$\n\\mu - (x^\\star)^3 + x^\\star = 0\n$$\nThis can be rearranged into the standard form of a depressed cubic equation:\n$$\n(x^\\star)^3 - x^\\star - \\mu = 0\n$$\nFor any given value of the control parameter $\\mu$, the real roots of this polynomial are the equilibrium points of the system. A cubic polynomial can have one or three real roots, which will be found numerically.\n\nThe linear stability of an equilibrium $x^\\star$ determines whether small perturbations away from it will decay (stable) or grow (unstable). This is determined by the sign of the derivative of the right-hand side of the ODE, let's call it $f(x, \\mu) = \\mu - x^3 + x$, with respect to $x$, evaluated at the equilibrium point $x^\\star$. The derivative is:\n$$\nf_x(x) = \\frac{\\partial f}{\\partial x} = 1 - 3x^2\n$$\nThe stability criteria are as follows:\n- If $f_x(x^\\star) = 1 - 3(x^\\star)^2  0$, the equilibrium is locally asymptotically stable. We assign a stability code of $-1$.\n- If $f_x(x^\\star) = 1 - 3(x^\\star)^2 > 0$, the equilibrium is unstable. We assign a stability code of $+1$.\n- If $f_x(x^\\star) = 1 - 3(x^\\star)^2 = 0$, the equilibrium is neutrally stable, indicating a bifurcation point. We assign a stability code of $0$.\n\nThe condition for neutral stability, $1 - 3(x^\\star)^2 = 0$, occurs at $x^\\star = \\pm 1/\\sqrt{3}$. Substituting these values back into the equilibrium equation $\\mu = (x^\\star)^3 - x^\\star$ yields the critical parameter values $\\mu_c$ where saddle-node bifurcations occur:\n- For $x^\\star = 1/\\sqrt{3}$, $\\mu_c = (1/\\sqrt{3})^3 - (1/\\sqrt{3}) = \\frac{1}{3\\sqrt{3}} - \\frac{1}{\\sqrt{3}} = -\\frac{2}{3\\sqrt{3}}$.\n- For $x^\\star = -1/\\sqrt{3}$, $\\mu_c = (-1/\\sqrt{3})^3 - (-1/\\sqrt{3}) = -\\frac{1}{3\\sqrt{3}} + \\frac{1}{\\sqrt{3}} = \\frac{2}{3\\sqrt{3}}$.\n\nFor the numerical implementation, we will use a robust polynomial root-finding algorithm (`numpy.roots`) to solve for the equilibria $x^\\star$ for each given $\\mu$. The algorithm will then filter for real roots, sort them, and remove near-duplicates using a small tolerance. Subsequently, for each unique real equilibrium, the stability will be determined by calculating $1 - 3(x^\\star)^2$ and applying the sign criteria described above.\n\n**Part 2: Hysteresis Loop Simulation**\n\nHysteresis is the phenomenon where the state of a system depends on its history. In this context, it arises because of the existence of multiple stable states for a given range of the parameter $\\mu$ (specifically, for $|\\mu|  2/(3\\sqrt{3})$).\n\nThe algorithm proceeds as follows:\n1.  **Discretize Parameter Sweep**: The parameter $\\mu$ is varied in small, uniform steps $\\Delta\\mu = 0.01$ from a minimum value $\\mu_{\\min} = -0.5$ to a maximum value $\\mu_{\\max} = 0.5$ (upward sweep), and then back from $\\mu_{\\max}$ to $\\mu_{\\min}$ (downward sweep).\n2.  **Initialize Trajectory**:\n    - For the upward sweep, the initial state is the lowest-valued stable equilibrium at $\\mu_{\\min} = -0.5$.\n    - For the downward sweep, the initial state is the highest-valued stable equilibrium at $\\mu_{\\max} = 0.5$.\n3.  **Path Following (Adiabatic Tracking)**: At each step of the parameter sweep (e.g., from $\\mu_i$ to $\\mu_{i+1}$), the system is assumed to relax to the \"nearest\" available stable equilibrium. The algorithm implements this by:\n    a.  Calculating all stable equilibria at the new parameter value $\\mu_{i+1}$.\n    b.  Choosing the one that is numerically closest in value to the system's state at the previous parameter value, $x(\\mu_i)$.\n4.  **Jumps at Bifurcations**: This path-following logic naturally captures the sudden jumps characteristic of tipping points. When a stable branch ceases to exist at a saddle-node bifurcation point, the state at the previous step, $x(\\mu_i)$, is near the bifurcation point. At $\\mu_{i+1}$, that branch has vanished. The algorithm, seeking the nearest available stable equilibrium, will find that the only remaining option is on a different, distant stable branch. This selection of the nearest remaining stable state constitutes the \"jump\".\n\nBy recording the state variable $x$ for both the upward and downward sweeps, we can observe the hysteresis loop. For the same value of $\\mu$ within the bistable region, the system will occupy different stable states depending on whether $\\mu$ was approached from a lower or higher value. This difference is a direct measure of hysteresis. The test for hysteresis at $\\mu=0.3$ involves comparing the state values from the two sweeps at that specific parameter value.\n\nThe implementation will collect the results from the equilibrium analysis for the five specified $\\mu$ values, the sampled states from the upward and downward hysteresis sweeps, and the boolean result of the hysteresis check into a single data structure for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the bifurcation and hysteresis problem for the ODE dx/dt = mu - x^3 + x.\n    \"\"\"\n\n    # --- Part 1: Equilibrium and Stability Analysis ---\n\n    def analyze_equilibria(mu, tol=1e-9):\n        \"\"\"\n        For a given mu, finds all real equilibria, sorts them, and determines their stability.\n        \n        Args:\n            mu (float): The control parameter.\n            tol (float): Tolerance for numerical comparisons (real roots, duplicates, zero derivative).\n\n        Returns:\n            tuple: (number_of_equilibria, list_of_equilibria, list_of_stability_codes)\n        \"\"\"\n        # Equilibrium equation is x^3 - x - mu = 0\n        coeffs = [1, 0, -1, -mu]\n        roots = np.roots(coeffs)\n\n        # Filter for real roots\n        real_roots = roots[np.abs(roots.imag)  tol].real\n        \n        # Sort and deduplicate real roots\n        real_roots = np.sort(real_roots)\n        unique_roots = []\n        if len(real_roots)  0:\n            unique_roots.append(real_roots[0])\n            for i in range(1, len(real_roots)):\n                if np.abs(real_roots[i] - real_roots[i - 1])  tol:\n                    unique_roots.append(real_roots[i])\n    \n        unique_roots = np.array(unique_roots)\n        num_equilibria = len(unique_roots)\n\n        # Determine stability for each unique root\n        stability_codes = []\n        for x_star in unique_roots:\n            # Derivative is 1 - 3*x^2\n            deriv = 1 - 3 * x_star**2\n            if np.abs(deriv)  tol:\n                stability_codes.append(0)  # Neutral stability\n            elif deriv  0:\n                stability_codes.append(-1) # Stable\n            else:\n                stability_codes.append(1)  # Unstable\n        \n        return num_equilibria, list(unique_roots), stability_codes\n\n    mu_test_values = [-0.5, -2 / (3 * np.sqrt(3)), 0, 2 / (3 * np.sqrt(3)), 0.5]\n    equilibrium_results = []\n    for mu_val in mu_test_values:\n        num_eq, eqs, stabs = analyze_equilibria(mu_val)\n        equilibrium_results.append([mu_val, num_eq, eqs, stabs])\n\n    # --- Part 2: Hysteresis Loop Calculation ---\n\n    mu_min, mu_max, d_mu = -0.5, 0.5, 0.01\n\n    def get_stable_equilibria(mu):\n        \"\"\"Helper to return only stable equilibria for a given mu.\"\"\"\n        _, eqs, stabilities = analyze_equilibria(mu)\n        return np.array([eq for eq, stab in zip(eqs, stabilities) if stab == -1])\n\n    # Upward sweep\n    mu_up = np.arange(mu_min, mu_max + d_mu / 2, d_mu)\n    x_up_trajectory = []\n    \n    # Initialize: start at the lowest stable equilibrium at mu_min\n    initial_stable_eqs_up = get_stable_equilibria(mu_up[0])\n    x_current = np.min(initial_stable_eqs_up)\n\n    for mu in mu_up:\n        stable_eqs = get_stable_equilibria(mu)\n        if len(stable_eqs)  0:\n            # Find the stable equilibrium closest to the previous state\n            distances = np.abs(stable_eqs - x_current)\n            x_current = stable_eqs[np.argmin(distances)]\n        else:\n            # This handles the moment of jump, where no stable root is found.\n            # a robust solver may look at unstable roots but per problem \"track stable\"\n            # this case should not be hit with small steps as the jump happens 'after' the bifurcation\n            # For this problem, we will assume at least one stable branch exists.\n            pass\n        x_up_trajectory.append(x_current)\n\n    # Downward sweep\n    mu_down = np.arange(mu_max, mu_min - d_mu / 2, -d_mu)\n    x_down_trajectory = []\n    \n    # Initialize: start at the highest stable equilibrium at mu_max\n    initial_stable_eqs_down = get_stable_equilibria(mu_down[0])\n    x_current = np.max(initial_stable_eqs_down)\n\n    for mu in mu_down:\n        stable_eqs = get_stable_equilibria(mu)\n        if len(stable_eqs)  0:\n            distances = np.abs(stable_eqs - x_current)\n            x_current = stable_eqs[np.argmin(distances)]\n        x_down_trajectory.append(x_current)\n\n    # Sample the trajectories\n    sample_mus = [-0.3, 0.0, 0.3]\n    up_samples = []\n    down_samples = []\n\n    for s_mu in sample_mus:\n        idx_up = np.where(np.isclose(mu_up, s_mu))[0][0]\n        up_samples.append(x_up_trajectory[idx_up])\n        \n        idx_down = np.where(np.isclose(mu_down, s_mu))[0][0]\n        down_samples.append(x_down_trajectory[idx_down])\n\n    hysteresis_results = [up_samples, down_samples]\n\n    # --- Part 3: Hysteresis Presence Check ---\n    \n    x_up_at_0_3 = up_samples[2]\n    x_down_at_0_3 = down_samples[2]\n    is_hysteresis_present = bool(np.abs(x_up_at_0_3 - x_down_at_0_3) = 1e-6)\n\n    # --- Final Output Assembly ---\n\n    final_result = [equilibrium_results, hysteresis_results, is_hysteresis_present]\n    \n    # Python's default string representation for lists matches the required bracketed format.\n    print(str(final_result))\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world climate systems are inherently noisy. This exercise moves from the deterministic picture to a stochastic one, exploring how the stability of an equilibrium affects its response to random forcing. By deriving the stationary variance and lag-$1$ autocorrelation for the Ornstein-Uhlenbeck process from first principles, you will uncover the phenomenon of 'critical slowing down'—the theoretical foundation explaining why these statistics serve as powerful early warning signals.",
            "id": "4105519",
            "problem": "Consider a linearized one-dimensional climate subsystem subject to stochastic forcing, modeled as the Ornstein-Uhlenbeck (OU) process, also known as a mean-reverting stochastic differential equation, given by $dx(t) = -\\lambda\\,x(t)\\,dt + \\sigma\\,dW_t$, where $W_t$ is standard Wiener process (Brownian motion), $\\lambda0$ is the local stability parameter of the linearized drift near a stable fixed point, and $\\sigma0$ is the noise amplitude. Assume the system is stationary and sampled at a fixed interval $\\Delta t0$. Starting from the properties of the Wiener process and Itô calculus, and without invoking any pre-derived OU formulas, derive the stationary variance of $x(t)$ and the lag-$1$ autocorrelation of the discretely sampled series $x_n \\equiv x(n\\,\\Delta t)$ in terms of $\\lambda$, $\\sigma$, and $\\Delta t$. Then use your derived expressions to explain, in the context of numerical weather prediction and climate modeling, how these quantities change as the system approaches a tipping point characterized by $\\lambda\\to 0^{+}$ (critical slowing down). Express your final answer as closed-form analytic expressions for the stationary variance and lag-$1$ autocorrelation. No numerical evaluation is required, and no units are to be reported in the final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in the theory of stochastic processes with direct and significant applications to the study of tipping points in climate science. All necessary parameters and conditions are provided, and the task is a rigorous mathematical derivation followed by a physical interpretation.\n\nThe system is described by the Ornstein-Uhlenbeck stochastic differential equation (SDE):\n$$\ndx(t) = -\\lambda\\,x(t)\\,dt + \\sigma\\,dW_t\n$$\nwhere $x(t)$ is the state variable, $\\lambda > 0$ is the stability parameter, $\\sigma > 0$ is the noise amplitude, and $W_t$ is a standard Wiener process. We are tasked with deriving the stationary variance and lag-$1$ autocorrelation from first principles of Itô calculus.\n\nFirst, we solve the linear SDE for $x(t)$. We use an integrating factor, similar to the method for first-order linear ordinary differential equations. Let the integrating factor be $e^{\\lambda t}$. We consider the differential of the product $e^{\\lambda t} x(t)$. Using the Itô product rule for a deterministic function $f(t)$ and a stochastic process $X_t$, which is $d(f(t)X_t) = f'(t)X_t dt + f(t)dX_t$, we have:\n$$\nd(e^{\\lambda t} x(t)) = (\\lambda e^{\\lambda t} dt)x(t) + e^{\\lambda t} dx(t)\n$$\nSubstituting the SDE for $dx(t)$:\n$$\nd(e^{\\lambda t} x(t)) = \\lambda e^{\\lambda t} x(t) dt + e^{\\lambda t} (-\\lambda x(t) dt + \\sigma dW_t)\n$$\nThe drift terms cancel out:\n$$\nd(e^{\\lambda t} x(t)) = \\lambda e^{\\lambda t} x(t) dt - \\lambda e^{\\lambda t} x(t) dt + \\sigma e^{\\lambda t} dW_t\n$$\n$$\nd(e^{\\lambda t} x(t)) = \\sigma e^{\\lambda t} dW_t\n$$\nIntegrating this from a time $s$ to $t > s$:\n$$\n\\int_s^t d(e^{\\lambda \\tau} x(\\tau)) = \\int_s^t \\sigma e^{\\lambda \\tau} dW_\\tau\n$$\n$$\ne^{\\lambda t} x(t) - e^{\\lambda s} x(s) = \\sigma \\int_s^t e^{\\lambda \\tau} dW_\\tau\n$$\nSolving for $x(t)$, we obtain the general solution:\n$$\nx(t) = e^{-\\lambda(t-s)} x(s) + \\sigma \\int_s^t e^{-\\lambda(t-\\tau)} dW_\\tau\n$$\nThe problem states that the process is stationary. This implies that its statistical properties are independent of time. For this to hold, we can consider the process as having started at $s \\to -\\infty$. Since $\\lambda > 0$, the term $e^{-\\lambda(t-s)}x(s)$ vanishes as $s \\to -\\infty$, assuming $x(s)$ does not grow exponentially, which is true for a stationary process. Thus, the stationary solution is represented by the Itô integral over the infinite past:\n$$\nx(t) = \\sigma \\int_{-\\infty}^t e^{-\\lambda(t-\\tau)} dW_\\tau\n$$\n\nNow, we derive the stationary variance, $\\text{Var}(x(t))$. First, we calculate the mean, $E[x(t)]$:\n$$\nE[x(t)] = E\\left[ \\sigma \\int_{-\\infty}^t e^{-\\lambda(t-\\tau)} dW_\\tau \\right]\n$$\nThe expectation of an Itô integral with a deterministic integrand is zero, because $E[dW_\\tau] = 0$.\n$$\nE[x(t)] = \\sigma \\int_{-\\infty}^t e^{-\\lambda(t-\\tau)} E[dW_\\tau] = 0\n$$\nSince the mean is zero, the variance is equal to the second moment, $\\text{Var}(x(t)) = E[x(t)^2]$.\n$$\nE[x(t)^2] = E\\left[ \\left( \\sigma \\int_{-\\infty}^t e^{-\\lambda(t-\\tau)} dW_\\tau \\right)^2 \\right] = \\sigma^2 E\\left[ \\left( \\int_{-\\infty}^t e^{-\\lambda(t-\\tau)} dW_\\tau \\right)^2 \\right]\n$$\nWe apply the Itô isometry property, which states that for a deterministic function $f(\\tau)$, $E\\left[ \\left(\\int_a^b f(\\tau) dW_\\tau\\right)^2 \\right] = \\int_a^b f(\\tau)^2 d\\tau$.\n$$\nE[x(t)^2] = \\sigma^2 \\int_{-\\infty}^t \\left( e^{-\\lambda(t-\\tau)} \\right)^2 d\\tau = \\sigma^2 \\int_{-\\infty}^t e^{-2\\lambda(t-\\tau)} d\\tau\n$$\nTo evaluate the integral, we use the substitution $u = t-\\tau$, which gives $du = -d\\tau$. The limits of integration change from $\\tau \\in (-\\infty, t]$ to $u \\in [0, \\infty)$.\n$$\n\\int_{-\\infty}^t e^{-2\\lambda(t-\\tau)} d\\tau = \\int_{\\infty}^0 e^{-2\\lambda u} (-du) = \\int_0^{\\infty} e^{-2\\lambda u} du = \\left[ -\\frac{1}{2\\lambda}e^{-2\\lambda u} \\right]_0^{\\infty} = 0 - \\left(-\\frac{1}{2\\lambda}\\right) = \\frac{1}{2\\lambda}\n$$\nTherefore, the stationary variance is:\n$$\n\\text{Var}(x(t)) = \\frac{\\sigma^2}{2\\lambda}\n$$\n\nNext, we derive the lag-$1$ autocorrelation of the discretely sampled series $x_n = x(n\\Delta t)$. The autocorrelation is defined as $\\rho(\\Delta t) = \\frac{\\text{Cov}(x(t+\\Delta t), x(t))}{\\text{Var}(x(t))}$. Due to stationarity, this is independent of $t$ and is equal to the lag-$1$ autocorrelation $\\rho(1)$ for the series $x_n$.\nThe covariance is $\\text{Cov}(x(t+\\Delta t), x(t)) = E[x(t+\\Delta t)x(t)] - E[x(t+\\Delta t)]E[x(t)]$. Since the mean is zero, this simplifies to $E[x(t+\\Delta t)x(t)]$.\nUsing the general solution for $x(t)$ with $s=t$ and time step $\\Delta t$:\n$$\nx(t+\\Delta t) = e^{-\\lambda \\Delta t} x(t) + \\sigma \\int_t^{t+\\Delta t} e^{-\\lambda(t+\\Delta t - \\tau)} dW_\\tau\n$$\nNow we compute the expectation of the product $x(t+\\Delta t)x(t)$:\n$$\nE[x(t+\\Delta t)x(t)] = E\\left[ \\left( e^{-\\lambda \\Delta t} x(t) + \\sigma \\int_t^{t+\\Delta t} e^{-\\lambda(t+\\Delta t - \\tau)} dW_\\tau \\right) x(t) \\right]\n$$\nBy linearity of expectation:\n$$\nE[x(t+\\Delta t)x(t)] = E[e^{-\\lambda \\Delta t} x(t)^2] + E\\left[ x(t) \\cdot \\sigma \\int_t^{t+\\Delta t} e^{-\\lambda(t+\\Delta t - \\tau)} dW_\\tau \\right]\n$$\nThe first term is $e^{-\\lambda \\Delta t} E[x(t)^2] = e^{-\\lambda \\Delta t} \\text{Var}(x(t)) = e^{-\\lambda \\Delta t} \\frac{\\sigma^2}{2\\lambda}$.\nThe second term involves the product of $x(t)$ and a stochastic integral over a future time interval $[t, t+\\Delta t]$. The process $x(t)$ is a function of the Wiener process $W_\\tau$ for $\\tau \\le t$. The increments of the Wiener process $dW_\\tau$ for $\\tau > t$ are independent of the history of the process up to time $t$ (i.e., independent of the filtration $\\mathcal{F}_t$). The state $x(t)$ is $\\mathcal{F}_t$-measurable. The integral $\\int_t^{t+\\Delta t} f(\\tau) dW_\\tau$ has an expectation of zero, and its value is independent of $x(t)$.\n$$\nE\\left[ x(t) \\cdot \\sigma \\int_t^{t+\\Delta t} e^{-\\lambda(t+\\Delta t - \\tau)} dW_\\tau \\right] = \\sigma E[x(t)] \\cdot E\\left[ \\int_t^{t+\\Delta t} e^{-\\lambda(t+\\Delta t - \\tau)} dW_\\tau \\right] = \\sigma \\cdot 0 \\cdot 0 = 0\n$$\nA more formal argument using conditional expectation is $E[ x(t) \\int... ] = E[E[x(t) \\int... | \\mathcal{F}_t]] = E[x(t) E[\\int... | \\mathcal{F}_t]] = E[x(t) \\cdot 0] = 0$.\nThus, the autocovariance is:\n$$\n\\text{Cov}(x(t+\\Delta t), x(t)) = e^{-\\lambda \\Delta t} \\frac{\\sigma^2}{2\\lambda}\n$$\nThe lag-$1$ autocorrelation is then:\n$$\n\\rho(1) = \\frac{\\text{Cov}(x(t+\\Delta t), x(t))}{\\text{Var}(x(t))} = \\frac{e^{-\\lambda \\Delta t} \\frac{\\sigma^2}{2\\lambda}}{\\frac{\\sigma^2}{2\\lambda}} = e^{-\\lambda \\Delta t}\n$$\n\nFinally, we analyze these quantities in the context of a tipping point, characterized by \"critical slowing down,\" which corresponds to the limit $\\lambda \\to 0^+$. The parameter $\\lambda$ represents the restoring force or the rate of reversion to the mean. As $\\lambda$ approaches zero, this restoring force weakens, and the system takes longer to recover from perturbations.\n\nThe behavior of the variance as $\\lambda \\to 0^+$ is:\n$$\n\\lim_{\\lambda \\to 0^+} \\text{Var}(x(t)) = \\lim_{\\lambda \\to 0^+} \\frac{\\sigma^2}{2\\lambda} = \\infty\n$$\nThis means that as the system approaches the tipping point, the amplitude of its fluctuations around the stable state grows without bound. In a climate modeling context, this would manifest as increasingly large and erratic swings in the modeled variable (e.g., temperature, ice volume), a key early warning signal of an impending critical transition.\n\nThe behavior of the lag-$1$ autocorrelation as $\\lambda \\to 0^+$ is:\n$$\n\\lim_{\\lambda \\to 0^+} \\rho(1) = \\lim_{\\lambda \\to 0^+} e^{-\\lambda \\Delta t} = e^0 = 1\n$$\nThis indicates that as the system nears the tipping point, the state of the system at successive time steps becomes almost perfectly correlated. This is a direct consequence of \"slowing down\"; the system's memory increases, and it becomes more sluggish. A perturbation persists for a much longer time. This increased \"memory\" or temporal correlation is another critical early warning signal used in climate science to detect the proximity of a tipping point.\n\nThe two derived expressions are the stationary variance and the lag-$1$ autocorrelation.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sigma^2}{2\\lambda}  \\exp(-\\lambda \\Delta t) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Applying theoretical early warning signals to observational data requires a robust computational workflow. This exercise simulates the practical challenge of detecting critical slowing down in a time series that, like many climate records, is dominated by strong seasonal cycles and measurement noise. You will implement a complete analysis pipeline, from generating data and removing seasonality to computing rolling-window statistics and performing trend tests, thereby bridging the gap between abstract theory and applied data analysis.",
            "id": "4105489",
            "problem": "Consider the task of evaluating Early Warning Signals (EWS) for tipping points in synthetic methane flux anomaly records that exhibit strong seasonal cycles and external noise. The goal is to determine whether the records show statistically significant increasing trends in both rolling variance and lag-$1$ autocorrelation, which are classical EWS associated with critical slowing down prior to critical transitions. You must construct the signals, remove seasonality using harmonic regression, compute rolling-window diagnostics, and perform a non-parametric trend test. All angles used in trigonometric functions must be interpreted in radians.\n\nFundamental base and assumptions:\n- A univariate time series is constructed as the sum of a seasonal cycle, a stochastic autoregressive component, and independent Gaussian measurement noise. The seasonal cycle is represented as a finite sum of sines and cosines at a known seasonal angular frequency. The stochastic component follows a nonstationary Autoregressive process of order $1$ (AR($1$)) with time-varying autoregressive coefficient, and the measurement noise is independent and identically distributed.\n- The AR($1$) process is defined by $x_{t} = \\phi_{t}\\,x_{t-1} + \\eta_{t}$ for $t \\in \\{1,2,\\dots,N-1\\}$, with $x_{0} = 0$, where $\\{\\eta_{t}\\}$ are independent Gaussian random variables with zero mean and variance $\\sigma_{\\eta}^{2}$, and $\\phi_{t}$ varies smoothly in time from an initial value $\\phi_{0}$ to a final value $\\phi_{1}$.\n- The observed series is $y_{t} = s(t) + x_{t} + \\epsilon_{t}$, where $s(t)$ is the seasonal component, and $\\{\\epsilon_{t}\\}$ are independent Gaussian random variables with zero mean and variance $\\sigma_{\\epsilon}^{2}$.\n- The seasonal component is modeled for removal by harmonic regression using a design matrix with columns consisting of an intercept, $\\sin(\\omega t)$, $\\cos(\\omega t)$, $\\sin(2\\omega t)$, and $\\cos(2\\omega t)$, where $\\omega = 2\\pi/T$ and $T$ is the seasonal period measured in samples. The fitted seasonal cycle is subtracted from $y_{t}$ to obtain residuals $r_{t}$.\n- Rolling-window diagnostics are computed on $r_{t}$ using a window of size $W$. The rolling variance for window $i$ is $v_{i} = \\frac{1}{W-1}\\sum_{j=0}^{W-1}(r_{i+j} - \\bar{r}_{i})^{2}$ with $\\bar{r}_{i} = \\frac{1}{W}\\sum_{j=0}^{W-1} r_{i+j}$. The lag-$1$ autocorrelation within the same window is $a_{i} = \\frac{\\sum_{j=1}^{W-1} (r_{i+j}-\\bar{r}_{i})(r_{i+j-1}-\\bar{r}_{i})}{\\sum_{j=0}^{W-1} (r_{i+j}-\\bar{r}_{i})^{2}}$ provided the denominator is positive; otherwise, $a_{i}$ is treated as not available.\n- To assess monotonic trends in $\\{v_{i}\\}$ and $\\{a_{i}\\}$ over the index $i$, use Kendall’s tau correlation coefficient between the window index and the diagnostic series. Let $\\tau_{v}$ and $p_{v}$ be the Kendall tau and its associated p-value for $\\{v_{i}\\}$, and $\\tau_{a}$ and $p_{a}$ for $\\{a_{i}\\}$. Define an EWS detection decision as true if both $\\tau_{v} > 0$ with $p_{v}  \\alpha$ and $\\tau_{a} > 0$ with $p_{a}  \\alpha$, where $\\alpha$ is the significance level; otherwise the decision is false.\n\nYour program must:\n- Synthesize the time series $\\{y_{t}\\}_{t=0}^{N-1}$ for each test case using the specified parameters. Use a fixed random seed per test case for reproducibility.\n- Remove the seasonal cycle via least squares harmonic regression using the specified basis functions, yielding residuals $\\{r_{t}\\}$.\n- Compute rolling-window variance and lag-$1$ autocorrelation for the residual series, then assess monotonic trends using Kendall’s tau and its p-value for each diagnostic.\n- Produce the EWS detection decision per test case as a boolean, following the rule stated above.\n\nAll quantities in the program are dimensionless; there are no physical units to report. Angles used in $\\sin(\\cdot)$ and $\\cos(\\cdot)$ are in radians. The final output must be a single line containing the list of boolean results for all test cases, as a comma-separated list enclosed in square brackets (for example, $[{\\rm True},{\\rm False}]$).\n\nTest suite:\n- Test case $1$ (happy path, strong seasonality, increasing autoregressive coefficient):\n  - $N = 2048$, $T = 365$, $A_{1} = 4.0$, $A_{2} = 2.0$, $\\phi_{0} = 0.30$, $\\phi_{1} = 0.97$, $\\sigma_{\\eta} = 0.50$, $\\sigma_{\\epsilon} = 0.70$, $W = 128$, $\\alpha = 0.01$, ${\\rm seed} = 11$.\n- Test case $2$ (boundary condition, constant autoregressive coefficient):\n  - $N = 2048$, $T = 365$, $A_{1} = 4.0$, $A_{2} = 2.0$, $\\phi_{0} = 0.30$, $\\phi_{1} = 0.30$, $\\sigma_{\\eta} = 0.50$, $\\sigma_{\\epsilon} = 0.70$, $W = 128$, $\\alpha = 0.01$, ${\\rm seed} = 22$.\n- Test case $3$ (strong seasonality stress test with increasing autoregressive coefficient):\n  - $N = 2048$, $T = 365$, $A_{1} = 12.0$, $A_{2} = 6.0$, $\\phi_{0} = 0.20$, $\\phi_{1} = 0.98$, $\\sigma_{\\eta} = 0.40$, $\\sigma_{\\epsilon} = 0.50$, $W = 128$, $\\alpha = 0.01$, ${\\rm seed} = 33$.\n- Test case $4$ (edge case, short record length, large window):\n  - $N = 512$, $T = 365$, $A_{1} = 4.0$, $A_{2} = 2.0$, $\\phi_{0} = 0.20$, $\\phi_{1} = 0.95$, $\\sigma_{\\eta} = 0.50$, $\\sigma_{\\epsilon} = 0.70$, $W = 256$, $\\alpha = 0.05$, ${\\rm seed} = 44$.\n- Test case $5$ (external noise stress test with increasing autoregressive coefficient):\n  - $N = 2048$, $T = 365$, $A_{1} = 4.0$, $A_{2} = 2.0$, $\\phi_{0} = 0.40$, $\\phi_{1} = 0.99$, $\\sigma_{\\eta} = 0.50$, $\\sigma_{\\epsilon} = 2.50$, $W = 128$, $\\alpha = 0.05$, ${\\rm seed} = 55$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[{\\rm True},{\\rm False},{\\rm True},{\\rm False},{\\rm True}]$).",
            "solution": "The user has provided a well-defined computational problem in the domain of time series analysis, specifically concerning the detection of Early Warning Signals (EWS) for critical transitions. The problem is scientifically grounded, internally consistent, and computationally tractable. I will therefore proceed with a full solution.\n\nMy approach is structured into four main stages, following the logic outlined in the problem statement:\n1.  **Time Series Synthesis**: Generation of the synthetic data record $y_t$ for each test case, incorporating seasonal, autoregressive, and noise components.\n2.  **Harmonic Deseasonalization**: Removal of the seasonal cycle from $y_t$ using linear least-squares regression to obtain the residual series $r_t$.\n3.  **Rolling Window Diagnostics**: Computation of rolling-window variance $v_i$ and lag-$1$ autocorrelation $a_i$ on the residual series $r_t$.\n4.  **Trend Analysis and Decision**: Assessment of monotonic trends in the diagnostic series using Kendall's tau correlation test and application of the specified decision rule to determine the presence of EWS.\n\nBefore proceeding, I will explicitly state two minor assumptions made to resolve slight underspecification in the problem statement, ensuring a fully determined and reproducible procedure.\n-   The time-varying autoregressive coefficient $\\phi_t$ is assumed to change linearly with time from its initial value $\\phi_0$ to its final value $\\phi_1$. This is the simplest form of a \"smooth\" variation.\n    $$ \\phi_t = \\phi_0 + (\\phi_1 - \\phi_0) \\frac{t}{N-1} \\quad \\text{for } t = 0, 1, \\dots, N-1 $$\n-   The seasonal component $s(t)$, for which the amplitudes of the first and second harmonics ($A_1$ and $A_2$) are provided, is generated using sine functions with zero phase.\n    $$ s(t) = A_1 \\sin(\\omega t) + A_2 \\sin(2\\omega t) \\quad \\text{where } \\omega = 2\\pi/T $$\nThis generated signal lies within the vector space spanned by the basis functions used for the harmonic regression, ensuring the validity of the deseasonalization step.\n\nWith these clarifications, I will now detail each step of the solution.\n\n**Step 1: Time Series Synthesis**\nFor each test case, we synthesize a time series $\\{y_t\\}_{t=0}^{N-1}$ of length $N$. A random number generator is initialized with the specified seed for reproducibility.\n\n-   **Time Vector and Angular Frequency**: A time vector $t = [0, 1, \\dots, N-1]$ is created. The fundamental seasonal angular frequency is $\\omega = 2\\pi/T$.\n\n-   **Seasonal Component $s_t$**: The seasonal signal is generated as $s_t = A_1 \\sin(\\omega t) + A_2 \\sin(2\\omega t)$.\n\n-   **AR($1$) Component $x_t$**: The stochastic autoregressive process is generated iteratively. First, the vector of autoregressive coefficients $\\{\\phi_t\\}_{t=0}^{N-1}$ is computed using the linear ramp specified above. Then, a vector of innovations $\\{\\eta_t\\}_{t=0}^{N-1}$ is drawn from a Gaussian distribution $N(0, \\sigma_\\eta^2)$. The AR($1$) series $\\{x_t\\}$ is initialized with $x_0 = 0$ as per the problem, and subsequent values are computed via the recurrence relation:\n    $$ x_t = \\phi_t x_{t-1} + \\eta_t \\quad \\text{for } t=1, 2, \\dots, N-1 $$\n\n-   **Measurement Noise $\\epsilon_t$**: A vector of independent measurement noise $\\{\\epsilon_t\\}_{t=0}^{N-1}$ is drawn from a Gaussian distribution $N(0, \\sigma_\\epsilon^2)$.\n\n-   **Observed Series $y_t$**: The final observed time series is the sum of the three components:\n    $$ y_t = s_t + x_t + \\epsilon_t $$\n\n**Step 2: Harmonic Regression and Deseasonalization**\nTo isolate the stochastic dynamics, the strong seasonal component must be removed. This is achieved via ordinary least squares (OLS) regression.\n\n-   **Design Matrix**: We construct an $N \\times 5$ design matrix $\\mathbf{X}$, where each row corresponds to a time point $t$. The columns of $\\mathbf{X}$ are the basis functions evaluated at each time point:\n    $$ \\mathbf{X} = \\begin{bmatrix} 1  \\sin(\\omega \\cdot 0)  \\cos(\\omega \\cdot 0)  \\sin(2\\omega \\cdot 0)  \\cos(2\\omega \\cdot 0) \\\\ 1  \\sin(\\omega \\cdot 1)  \\cos(\\omega \\cdot 1)  \\sin(2\\omega \\cdot 1)  \\cos(2\\omega \\cdot 1) \\\\ \\vdots  \\vdots  \\vdots  \\vdots  \\vdots \\\\ 1  \\sin(\\omega(N-1))  \\cos(\\omega(N-1))  \\sin(2\\omega(N-1))  \\cos(2\\omega(N-1)) \\end{bmatrix} $$\n\n-   **Least-Squares Fit**: We solve the linear system $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon'}$ for the coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4]^T$. The OLS solution is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.\n\n-   **Residuals**: The fitted seasonal cycle is $\\hat{\\mathbf{s}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$. The residual series, which represents the non-seasonal dynamics, is then calculated as $\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{s}}$.\n\n**Step 3: Rolling Window Diagnostics**\nWe compute the EWS diagnostics, rolling variance and lag-$1$ autocorrelation, on the residual series $\\{r_t\\}$ using a sliding window of size $W$. This results in $N - W + 1$ diagnostic values.\n\n-   **Rolling Variance**: For each window $i$, starting from $i=0$ to $N-W$, the sample variance is calculated:\n    $$ v_i = \\frac{1}{W-1} \\sum_{j=0}^{W-1} (r_{i+j} - \\bar{r}_i)^2 $$\n    where $\\bar{r}_i$ is the mean of the data in the $i$-th window, $\\{r_k\\}_{k=i}^{i+W-1}$. This is the unbiased sample variance.\n\n-   **Rolling Lag-$1$ Autocorrelation**: For the same window $i$, the lag-$1$ autocorrelation is:\n    $$ a_i = \\frac{\\sum_{j=1}^{W-1} (r_{i+j} - \\bar{r}_i)(r_{i+j-1} - \\bar{r}_i)}{\\sum_{j=0}^{W-1} (r_{i+j} - \\bar{r}_i)^2} $$\n    If the denominator is zero (which occurs if all values in the window are identical), the autocorrelation is undefined and treated as a `Not-a-Number` (NaN) value.\n\n**Step 4: Trend Analysis and EWS Decision**\nThe final step is to test for a statistically significant positive monotonic trend in both diagnostic series.\n\n-   **Trend Test**: We use Kendall's tau correlation coefficient to measure the association between the window index, $i \\in \\{0, 1, ..., N-W\\}$, and the corresponding diagnostic values, $\\{v_i\\}$ and $\\{a_i\\}$. The test is performed for both series. We are interested in a positive trend, so a one-sided hypothesis test is appropriate (alternative hypothesis: $\\tau > 0$). The `scipy.stats.kendalltau` function is used with `alternative='greater'`. This yields the coefficients $\\tau_v, \\tau_a$ and their one-sided p-values $p_v, p_a$. Any NaN values in the diagnostic series are excluded from the test.\n\n-   **Decision Rule**: An EWS detection is registered as `True` if and only if both diagnostic trends are positive and statistically significant at the level $\\alpha$. The formal condition is:\n    $$ \\text{EWS} = (\\tau_v > 0 \\text{ and } p_v  \\alpha) \\quad \\text{AND} \\quad (\\tau_a > 0 \\text{ and } p_a  \\alpha) $$\n    Otherwise, the decision is `False`. This entire procedure is applied to each test case to generate the final list of boolean results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kendalltau\n\ndef run_ews_analysis(N, T, A1, A2, phi0, phi1, sigma_eta, sigma_eps, W, alpha, seed):\n    \"\"\"\n    Performs the entire EWS analysis for a single test case.\n\n    Args:\n        N (int): Time series length.\n        T (int): Seasonal period in samples.\n        A1 (float): Amplitude of the first seasonal harmonic.\n        A2 (float): Amplitude of the second seasonal harmonic.\n        phi0 (float): Initial AR(1) coefficient.\n        phi1 (float): Final AR(1) coefficient.\n        sigma_eta (float): Standard deviation of AR(1) innovations.\n        sigma_eps (float): Standard deviation of measurement noise.\n        W (int): Rolling window size.\n        alpha (float): Significance level for trend tests.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        bool: True if EWS are detected, False otherwise.\n    \"\"\"\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # === Step 1: Time Series Synthesis ===\n    t = np.arange(N)\n    \n    # Seasonal component\n    omega = 2 * np.pi / T\n    s_t = A1 * np.sin(omega * t) + A2 * np.sin(2 * omega * t) # Explicit assumption\n\n    # AR(1) component\n    phi_t = np.linspace(phi0, phi1, N) # Explicit assumption of linear trend\n    eta_t = rng.normal(loc=0, scale=sigma_eta, size=N)\n    x_t = np.zeros(N)\n    # Per problem: x_0 = 0, and process is for t in {1, ..., N-1}\n    for i in range(1, N):\n        x_t[i] = phi_t[i] * x_t[i-1] + eta_t[i]\n        \n    # Measurement noise\n    epsilon_t = rng.normal(loc=0, scale=sigma_eps, size=N)\n\n    # Observed series\n    y_t = s_t + x_t + epsilon_t\n\n    # === Step 2: Harmonic Regression and Deseasonalization ===\n    # Construct design matrix X for harmonic regression\n    design_matrix = np.vstack([\n        np.ones(N),\n        np.sin(omega * t),\n        np.cos(omega * t),\n        np.sin(2 * omega * t),\n        np.cos(2 * omega * t)\n    ]).T\n\n    # Solve for regression coefficients using least squares\n    coeffs, _, _, _ = np.linalg.lstsq(design_matrix, y_t, rcond=None)\n\n    # Calculate fitted seasonal cycle and residuals\n    s_fit = design_matrix @ coeffs\n    r_t = y_t - s_fit\n\n    # === Step 3: Rolling Window Diagnostics ===\n    num_windows = N - W + 1\n    if num_windows = 1:\n        # Not enough data to compute a trend\n        return False\n        \n    rolling_var = np.empty(num_windows)\n    rolling_ac1 = np.empty(num_windows)\n\n    for i in range(num_windows):\n        window = r_t[i : i + W]\n        \n        # Rolling variance (unbiased estimator)\n        rolling_var[i] = np.var(window, ddof=1)\n        \n        # Rolling lag-1 autocorrelation\n        mean_win = np.mean(window)\n        centered_win = window - mean_win\n        \n        # Denominator corresponds to (W-1)*variance\n        denom = np.sum(centered_win**2)\n\n        if denom  1e-12: # Check for non-zero variance\n            # Numerator is lag-1 autocovariance\n            num = np.sum(centered_win[1:] * centered_win[:-1])\n            rolling_ac1[i] = num / denom\n        else:\n            rolling_ac1[i] = np.nan\n\n    # === Step 4: Trend Analysis and EWS Decision ===\n    window_indices = np.arange(num_windows)\n\n    # Kendall's tau for variance\n    # Use 'greater' for one-sided test (H1: tau  0)\n    tau_v, p_v = kendalltau(window_indices, rolling_var, nan_policy='omit', alternative='greater')\n    \n    # Kendall's tau for autocorrelation\n    tau_a, p_a = kendalltau(window_indices, rolling_ac1, nan_policy='omit', alternative='greater')\n\n    # EWS decision rule\n    # The condition tau  0 is implicit if p  0.5 for a one-sided 'greater' test,\n    # but we include it for pedantic correctness as per the problem statement.\n    var_trend_detected = (tau_v  0 and p_v  alpha)\n    ac1_trend_detected = (tau_a  0 and p_a  alpha)\n    \n    is_ews_detected = var_trend_detected and ac1_trend_detected\n\n    return is_ews_detected\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1: Happy path\n        {'N': 2048, 'T': 365, 'A1': 4.0, 'A2': 2.0, 'phi0': 0.30, 'phi1': 0.97, 'sigma_eta': 0.50, 'sigma_eps': 0.70, 'W': 128, 'alpha': 0.01, 'seed': 11},\n        # Test case 2: Constant phi (no trend expected)\n        {'N': 2048, 'T': 365, 'A1': 4.0, 'A2': 2.0, 'phi0': 0.30, 'phi1': 0.30, 'sigma_eta': 0.50, 'sigma_eps': 0.70, 'W': 128, 'alpha': 0.01, 'seed': 22},\n        # Test case 3: Strong seasonality stress test\n        {'N': 2048, 'T': 365, 'A1': 12.0, 'A2': 6.0, 'phi0': 0.20, 'phi1': 0.98, 'sigma_eta': 0.40, 'sigma_eps': 0.50, 'W': 128, 'alpha': 0.01, 'seed': 33},\n        # Test case 4: Short record, large window\n        {'N': 512, 'T': 365, 'A1': 4.0, 'A2': 2.0, 'phi0': 0.20, 'phi1': 0.95, 'sigma_eta': 0.50, 'sigma_eps': 0.70, 'W': 256, 'alpha': 0.05, 'seed': 44},\n        # Test case 5: High measurement noise\n        {'N': 2048, 'T': 365, 'A1': 4.0, 'A2': 2.0, 'phi0': 0.40, 'phi1': 0.99, 'sigma_eta': 0.50, 'sigma_eps': 2.50, 'W': 128, 'alpha': 0.05, 'seed': 55},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_ews_analysis(**params)\n        results.append(result)\n\n    # Format output as a comma-separated list of booleans in square brackets\n    # e.g., [True,False,True,False,True]\n    result_str = f\"[{','.join(str(r) for r in results)}]\"\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}