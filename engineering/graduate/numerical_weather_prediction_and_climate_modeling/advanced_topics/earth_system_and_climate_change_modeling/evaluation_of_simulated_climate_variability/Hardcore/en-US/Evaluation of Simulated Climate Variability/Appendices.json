{
    "hands_on_practices": [
        {
            "introduction": "Climate models produce vast datasets on regular latitude-longitude grids, but to understand global trends, we need to compute summary statistics like the global mean temperature. A naive average of all grid points is incorrect because grid cells represent vastly different surface areas on the sphere. This foundational practice guides you through the geometric derivation of the cosine-latitude weighting required for accurate area-weighted averages, a cornerstone of analyzing global climate data. ",
            "id": "4040719",
            "problem": "A global climate model produces a scalar field $X(\\phi,\\lambda,t)$ (for example, near-surface air temperature anomaly) on a regular latitude–longitude grid, where latitude $\\phi \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]$ and longitude $\\lambda \\in [0,2\\pi)$ are measured in radians. There are $M$ latitude rows at central latitudes $\\{\\phi_{i}\\}_{i=1}^{M}$, equally spaced in $\\phi$ with spacing $\\Delta \\phi$, and $N$ longitude points at $\\{\\lambda_{j}\\}_{j=1}^{N}$, equally spaced with spacing $\\Delta \\lambda = \\frac{2\\pi}{N}$. The Earth is approximated as a perfect sphere of radius $R$. To evaluate simulated climate variability on global scales, you require an area-weighted global mean at time $t$, defined as the spatial average of $X(\\phi,\\lambda,t)$ over the sphere with respect to surface area.\n\nStarting only from fundamental spherical geometry of a sphere of radius $R$ and the definition of a surface-area average over the sphere, derive the latitude dependence of the weights needed to compute the global mean of $X(\\phi,\\lambda,t)$ from gridded data on this regular grid. In particular:\n\n- Derive, from first principles, the expression for the surface-area element on a sphere in terms of $(\\phi,\\lambda)$.\n- Using this, construct the discrete area weight for a grid cell centered at $(\\phi_{i},\\lambda_{j})$ and then the corresponding normalized per-cell weight $w_{ij}$ for forming the global mean on the regular grid described above.\n- In the continuum limit $M,N \\to \\infty$ with uniform spacing, derive the normalized latitude-only weighting function $w(\\phi)$ such that $\\int_{-\\pi/2}^{\\pi/2} w(\\phi)\\,d\\phi = 1$ and\n$$\n\\overline{X}(t) \\;=\\; \\int_{-\\pi/2}^{\\pi/2} w(\\phi)\\,\\left[\\frac{1}{2\\pi}\\int_{0}^{2\\pi} X(\\phi,\\lambda,t)\\,d\\lambda\\right] d\\phi.\n$$\n\nAngles must be treated in radians throughout. Provide your final answer as a single closed-form analytic expression for the normalized latitude-only weighting function $w(\\phi)$. No rounding is required.",
            "solution": "The problem requires the derivation of the latitude-dependent weighting function used for calculating the area-weighted global mean of a scalar field on a sphere. The derivation will proceed in three stages: first, deriving the differential surface area element on a sphere; second, constructing the discrete weights for a gridded representation; and third, deriving the continuous, normalized latitude-only weighting function.\n\n**Part 1: Derivation of the Surface Area Element $dA$**\n\nWe begin by defining the position vector $\\vec{r}$ of a point on the surface of a sphere of radius $R$ using latitude $\\phi \\in [-\\frac{\\pi}{2}, \\frac{\\pi}{2}]$ and longitude $\\lambda \\in [0, 2\\pi)$. In a Cartesian coordinate system $(x, y, z)$ with the $z$-axis passing through the poles and the origin at the center of the sphere, the coordinates are given by:\n$x = R \\cos\\phi \\cos\\lambda$\n$y = R \\cos\\phi \\sin\\lambda$\n$z = R \\sin\\phi$\n\nThe position vector is $\\vec{r}(\\phi, \\lambda) = (R \\cos\\phi \\cos\\lambda) \\hat{i} + (R \\cos\\phi \\sin\\lambda) \\hat{j} + (R \\sin\\phi) \\hat{k}$.\n\nTo find the differential surface area element $dA$, we compute the infinitesimal vectors along the lines of constant longitude and constant latitude. These are found by taking partial derivatives of $\\vec{r}$ with respect to $\\phi$ and $\\lambda$:\n\nThe vector tangent to a meridian (constant $\\lambda$) is:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\phi} = (-R \\sin\\phi \\cos\\lambda) \\hat{i} + (-R \\sin\\phi \\sin\\lambda) \\hat{j} + (R \\cos\\phi) \\hat{k}\n$$\n\nThe vector tangent to a latitude circle (constant $\\phi$) is:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\lambda} = (-R \\cos\\phi \\sin\\lambda) \\hat{i} + (R \\cos\\phi \\cos\\lambda) \\hat{j} + 0 \\hat{k}\n$$\n\nThe differential area element $d\\vec{A}$ is a vector normal to the surface, whose magnitude is the area of the infinitesimal parallelogram spanned by $\\frac{\\partial\\vec{r}}{\\partial\\phi}d\\phi$ and $\\frac{\\partial\\vec{r}}{\\partial\\lambda}d\\lambda$. This is given by their cross product:\n$$\nd\\vec{A} = \\left(\\frac{\\partial\\vec{r}}{\\partial\\phi} \\times \\frac{\\partial\\vec{r}}{\\partial\\lambda}\\right) d\\phi \\, d\\lambda\n$$\nThe cross product is calculated as:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\phi} \\times \\frac{\\partial\\vec{r}}{\\partial\\lambda} = \\det\n\\begin{pmatrix}\n\\hat{i} & \\hat{j} & \\hat{k} \\\\\n-R \\sin\\phi \\cos\\lambda & -R \\sin\\phi \\sin\\lambda & R \\cos\\phi \\\\\n-R \\cos\\phi \\sin\\lambda & R \\cos\\phi \\cos\\lambda & 0\n\\end{pmatrix}\n$$\n$$\n= \\hat{i}(0 - R^2 \\cos^2\\phi \\cos\\lambda) - \\hat{j}(0 - (-R^2 \\cos\\phi \\sin\\lambda \\cos\\phi)) + \\hat{k}(-R^2 \\sin\\phi \\cos\\phi \\cos^2\\lambda - R^2 \\sin\\phi \\cos\\phi \\sin^2\\lambda)\n$$\n$$\n= (-R^2 \\cos^2\\phi \\cos\\lambda) \\hat{i} - (R^2 \\cos^2\\phi \\sin\\lambda) \\hat{j} - (R^2 \\sin\\phi \\cos\\phi)(\\cos^2\\lambda + \\sin^2\\lambda) \\hat{k}\n$$\n$$\n= (-R^2 \\cos^2\\phi \\cos\\lambda) \\hat{i} - (R^2 \\cos^2\\phi \\sin\\lambda) \\hat{j} - (R^2 \\sin\\phi \\cos\\phi) \\hat{k}\n$$\nThe scalar area element $dA$ is the magnitude of this vector:\n$$\ndA = \\left| \\frac{\\partial\\vec{r}}{\\partial\\phi} \\times \\frac{\\partial\\vec{r}}{\\partial\\lambda} \\right| d\\phi \\, d\\lambda\n$$\n$$\n\\left| \\frac{\\partial\\vec{r}}{\\partial\\phi} \\times \\frac{\\partial\\vec{r}}{\\partial\\lambda} \\right|^2 = (-R^2 \\cos^2\\phi \\cos\\lambda)^2 + (-R^2 \\cos^2\\phi \\sin\\lambda)^2 + (-R^2 \\sin\\phi \\cos\\phi)^2\n$$\n$$\n= R^4 \\cos^4\\phi (\\cos^2\\lambda + \\sin^2\\lambda) + R^4 \\sin^2\\phi \\cos^2\\phi\n$$\n$$\n= R^4 \\cos^4\\phi + R^4 \\sin^2\\phi \\cos^2\\phi = R^4 \\cos^2\\phi (\\cos^2\\phi + \\sin^2\\phi) = R^4 \\cos^2\\phi\n$$\nSince $\\phi \\in [-\\frac{\\pi}{2}, \\frac{\\pi}{2}]$, $\\cos\\phi \\ge 0$. Thus, the magnitude is:\n$$\n\\left| \\frac{\\partial\\vec{r}}{\\partial\\phi} \\times \\frac{\\partial\\vec{r}}{\\partial\\lambda} \\right| = \\sqrt{R^4 \\cos^2\\phi} = R^2 \\cos\\phi\n$$\nTherefore, the surface area element on a sphere is:\n$$\ndA = R^2 \\cos\\phi \\, d\\phi \\, d\\lambda\n$$\n\n**Part 2: Discrete Area Weights for a Regular Grid**\n\nFor a regular grid with $M$ latitude rows and $N$ longitude columns, a grid cell centered at $(\\phi_i, \\lambda_j)$ covers the latitude range $[\\phi_i - \\frac{\\Delta\\phi}{2}, \\phi_i + \\frac{\\Delta\\phi}{2}]$ and longitude range $[\\lambda_j - \\frac{\\Delta\\lambda}{2}, \\lambda_j + \\frac{\\Delta\\lambda}{2}]$. The area of this grid cell, $\\Delta A_{ij}$, is found by integrating $dA$ over this domain:\n$$\n\\Delta A_{ij} = \\int_{\\lambda_j - \\Delta\\lambda/2}^{\\lambda_j + \\Delta\\lambda/2} \\int_{\\phi_i - \\Delta\\phi/2}^{\\phi_i + \\Delta\\phi/2} R^2 \\cos\\phi \\, d\\phi \\, d\\lambda\n$$\nThe integration with respect to $\\lambda$ yields a factor of $\\Delta\\lambda$:\n$$\n\\Delta A_{ij} = R^2 \\Delta\\lambda \\int_{\\phi_i - \\Delta\\phi/2}^{\\phi_i + \\Delta\\phi/2} \\cos\\phi \\, d\\phi = R^2 \\Delta\\lambda [\\sin\\phi]_{\\phi_i - \\Delta\\phi/2}^{\\phi_i + \\Delta\\phi/2}\n$$\n$$\n\\Delta A_{ij} = R^2 \\Delta\\lambda \\left( \\sin\\left(\\phi_i + \\frac{\\Delta\\phi}{2}\\right) - \\sin\\left(\\phi_i - \\frac{\\Delta\\phi}{2}\\right) \\right)\n$$\nThis is the discrete area weight for cell $(i,j)$. Note that it depends only on the latitude index $i$.\nThe global mean is calculated as a weighted sum, $\\overline{X} = \\sum_{i,j} w_{ij} X_{ij}$, where the weights $w_{ij}$ are normalized. To normalize, we need the total surface area of the sphere, $A_{total} = 4\\pi R^2$. The normalized weight $w_{ij}$ is the fraction of the total area represented by the cell:\n$$\nw_{ij} = \\frac{\\Delta A_{ij}}{A_{total}} = \\frac{R^2 \\Delta\\lambda \\left( \\sin(\\phi_i + \\frac{\\Delta\\phi}{2}) - \\sin(\\phi_i - \\frac{\\Delta\\phi}{2}) \\right)}{4\\pi R^2}\n$$\n$$\nw_{ij} = \\frac{\\Delta\\lambda}{4\\pi} \\left( \\sin\\left(\\phi_i + \\frac{\\Delta\\phi}{2}\\right) - \\sin\\left(\\phi_i - \\frac{\\Delta\\phi}{2}\\right) \\right)\n$$\nGiven $\\Delta\\lambda = \\frac{2\\pi}{N}$, this can also be written as:\n$$\nw_{ij} = \\frac{1}{2N} \\left( \\sin\\left(\\phi_i + \\frac{\\Delta\\phi}{2}\\right) - \\sin\\left(\\phi_i - \\frac{\\Delta\\phi}{2}\\right) \\right)\n$$\n\n**Part 3: Derivation of the Continuous Latitude-only Weighting Function $w(\\phi)$**\n\nThe formal definition of the area-weighted global mean $\\overline{X}(t)$ is the integral of the scalar field $X(\\phi, \\lambda, t)$ over the entire surface of the sphere, divided by the total surface area:\n$$\n\\overline{X}(t) = \\frac{1}{A_{total}} \\iint_S X(\\phi, \\lambda, t) \\, dA\n$$\nSubstituting $A_{total} = 4\\pi R^2$ and $dA = R^2 \\cos\\phi \\, d\\phi \\, d\\lambda$, we have:\n$$\n\\overline{X}(t) = \\frac{1}{4\\pi R^2} \\int_0^{2\\pi} \\int_{-\\pi/2}^{\\pi/2} X(\\phi, \\lambda, t) \\, R^2 \\cos\\phi \\, d\\phi \\, d\\lambda\n$$\nThe $R^2$ terms cancel. We can reorder the integration:\n$$\n\\overline{X}(t) = \\frac{1}{4\\pi} \\int_{-\\pi/2}^{\\pi/2} \\cos\\phi \\left( \\int_0^{2\\pi} X(\\phi, \\lambda, t) \\, d\\lambda \\right) d\\phi\n$$\nThe problem specifies the global mean in the form:\n$$\n\\overline{X}(t) = \\int_{-\\pi/2}^{\\pi/2} w(\\phi) \\left[ \\frac{1}{2\\pi} \\int_0^{2\\pi} X(\\phi, \\lambda, t) \\, d\\lambda \\right] d\\phi\n$$\nThe term in the square brackets is the zonal mean of $X$ at latitude $\\phi$. To match our derived expression to this form, we introduce a factor of $2\\pi$ and its reciprocal:\n$$\n\\overline{X}(t) = \\frac{2\\pi}{4\\pi} \\int_{-\\pi/2}^{\\pi/2} \\cos\\phi \\left( \\frac{1}{2\\pi} \\int_0^{2\\pi} X(\\phi, \\lambda, t) \\, d\\lambda \\right) d\\phi\n$$\n$$\n\\overline{X}(t) = \\int_{-\\pi/2}^{\\pi/2} \\left( \\frac{1}{2} \\cos\\phi \\right) \\left[ \\frac{1}{2\\pi} \\int_0^{2\\pi} X(\\phi, \\lambda, t) \\, d\\lambda \\right] d\\phi\n$$\nBy direct comparison of this equation with the form given in the problem statement, we can identify the normalized latitude-only weighting function $w(\\phi)$:\n$$\nw(\\phi) = \\frac{1}{2} \\cos\\phi\n$$\nFinally, we must verify that this weighting function is normalized such that $\\int_{-\\pi/2}^{\\pi/2} w(\\phi) \\, d\\phi = 1$:\n$$\n\\int_{-\\pi/2}^{\\pi/2} w(\\phi) \\, d\\phi = \\int_{-\\pi/2}^{\\pi/2} \\frac{1}{2} \\cos\\phi \\, d\\phi = \\frac{1}{2} [\\sin\\phi]_{-\\pi/2}^{\\pi/2}\n$$\n$$\n= \\frac{1}{2} \\left[ \\sin\\left(\\frac{\\pi}{2}\\right) - \\sin\\left(-\\frac{\\pi}{2}\\right) \\right] = \\frac{1}{2} [1 - (-1)] = \\frac{1}{2}(2) = 1\n$$\nThe normalization condition is satisfied. The derived function $w(\\phi)$ is the required closed-form analytic expression.",
            "answer": "$$\\boxed{\\frac{1}{2}\\cos(\\phi)}$$"
        },
        {
            "introduction": "Ensemble modeling is a key strategy to separate a forced climate signal from the noise of internal variability. This practice explores the statistical power of averaging across an ensemble, first under the ideal assumption of independent members where uncertainty decreases rapidly. It then reveals the crucial complication introduced by inter-model correlations—a realistic feature of projects like CMIP—which limits how much we can reduce uncertainty by simply adding more models. ",
            "id": "4040649",
            "problem": "You are evaluating how well an ensemble of climate model simulations represents interannual variability of a seasonal-mean near-surface temperature anomaly at a fixed grid point. Let $X_{i}$ denote the anomaly (relative to a reference climatology) predicted by ensemble member $i$ for a particular season and year, with $i \\in \\{1,\\dots,m\\}$. Assume that each $X_{i}$ is a second-order random variable with common mean $\\mu$ and common variance $\\sigma^{2}$, and define the ensemble mean $\\bar{X} = \\frac{1}{m}\\sum_{i=1}^{m} X_{i}$. \n\na) Starting from the definitions of variance and covariance, and the properties of variance under linear combinations, derive an expression for $\\operatorname{Var}(\\bar{X})$ under the assumption that the ensemble members are independent. Avoid any shortcut formulas and show all steps beginning from $\\operatorname{Var}(aY)$ for scalar $a$ and random variable $Y$, and the expansion of $\\operatorname{Var}\\!\\left(\\sum_{i=1}^{m} X_{i}\\right)$ in terms of variances and covariances.\n\nb) Generalize the result to the case in which the ensemble members are not independent but have a constant pairwise covariance $\\operatorname{Cov}(X_{i},X_{j}) = c$ for all $i \\neq j$. Express $\\operatorname{Var}(\\bar{X})$ in terms of $m$, $\\sigma^{2}$, and $c$. Then rewrite your expression in terms of the constant pairwise correlation $\\rho$ defined by $c=\\rho\\,\\sigma^{2}$.\n\nc) In a realistic multi-model or perturbed-physics ensemble, shared external forcing and common parameterization choices can induce positive correlations among members. Suppose $m=15$, $\\sigma^{2}=2.25$ (Kelvin squared), and $\\rho=0.2$. Using your expression from part (b), compute the variance of the ensemble mean, $\\operatorname{Var}(\\bar{X})$. Express the final variance in Kelvin squared. If you obtain a terminating decimal, report it exactly. The final answer must be a single number.",
            "solution": "The problem is found to be valid as it is scientifically grounded in statistical analysis methods common to climate science, well-posed with all necessary information, and stated objectively. We proceed with the solution.\n\na) We are asked to derive an expression for the variance of the ensemble mean, $\\operatorname{Var}(\\bar{X})$, under the assumption of independence. The ensemble mean is defined as $\\bar{X} = \\frac{1}{m}\\sum_{i=1}^{m} X_{i}$.\n\nWe begin by applying the property of variance for a random variable scaled by a constant factor. For a random variable $Y$ and a scalar constant $a$, the variance is given by $\\operatorname{Var}(aY) = a^2\\operatorname{Var}(Y)$. Applying this property to $\\bar{X}$, we identify $a = \\frac{1}{m}$ and $Y = \\sum_{i=1}^{m} X_{i}$.\n$$\n\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left(\\frac{1}{m}\\sum_{i=1}^{m} X_{i}\\right) = \\left(\\frac{1}{m}\\right)^2 \\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right) = \\frac{1}{m^2} \\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right)\n$$\nNext, we expand the term $\\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right)$ using the fundamental relationship between variance and covariance, which states that the variance of a sum of random variables is the sum of all elements in their covariance matrix.\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right) = \\operatorname{Cov}\\left(\\sum_{i=1}^{m} X_{i}, \\sum_{j=1}^{m} X_{j}\\right)\n$$\nBy the bilinearity of the covariance operator, this expands to a double summation:\n$$\n\\operatorname{Cov}\\left(\\sum_{i=1}^{m} X_{i}, \\sum_{j=1}^{m} X_{j}\\right) = \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\operatorname{Cov}(X_{i}, X_{j})\n$$\nWe can separate this double summation into terms where the indices are equal ($i=j$) and terms where they are not ($i \\neq j$):\n$$\n\\sum_{i=1}^{m} \\sum_{j=1}^{m} \\operatorname{Cov}(X_{i}, X_{j}) = \\sum_{i=1}^{m} \\operatorname{Cov}(X_{i}, X_{i}) + \\sum_{i=1}^{m} \\sum_{j=1, j \\neq i}^{m} \\operatorname{Cov}(X_{i}, X_{j})\n$$\nBy definition, $\\operatorname{Cov}(X_{i}, X_{i}) = \\operatorname{Var}(X_{i})$. For part (a), the ensemble members $X_{i}$ are assumed to be independent. A key property of independent random variables is that their covariance is zero. Therefore, $\\operatorname{Cov}(X_{i}, X_{j}) = 0$ for all $i \\neq j$.\nThis simplifies the expansion significantly, as the second term vanishes:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right) = \\sum_{i=1}^{m} \\operatorname{Var}(X_{i}) + 0 = \\sum_{i=1}^{m} \\operatorname{Var}(X_{i})\n$$\nWe are given that all ensemble members have a common variance $\\operatorname{Var}(X_{i}) = \\sigma^{2}$. The sum therefore becomes:\n$$\n\\sum_{i=1}^{m} \\operatorname{Var}(X_{i}) = \\sum_{i=1}^{m} \\sigma^{2} = m\\sigma^{2}\n$$\nSubstituting this result back into our initial expression for $\\operatorname{Var}(\\bar{X})$:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{1}{m^2} (m\\sigma^{2}) = \\frac{\\sigma^{2}}{m}\n$$\nThis is the final expression for the variance of the ensemble mean under the assumption of independence.\n\nb) We now generalize the result to the case where the ensemble members are not independent. The assumption of independence is replaced with the condition that the pairwise covariance is constant for all distinct members: $\\operatorname{Cov}(X_{i},X_{j}) = c$ for all $i \\neq j$.\n\nWe return to the general expansion for the variance of a sum:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right) = \\sum_{i=1}^{m} \\operatorname{Var}(X_{i}) + \\sum_{i=1}^{m} \\sum_{j=1, j \\neq i}^{m} \\operatorname{Cov}(X_{i}, X_{j})\n$$\nThe first term remains the sum of the variances. With $\\operatorname{Var}(X_i) = \\sigma^2$, this is $\\sum_{i=1}^{m} \\sigma^{2} = m\\sigma^{2}$.\nThe second term is the sum of all off-diagonal covariance terms. There are $m$ choices for the index $i$ and, for each $i$, there are $m-1$ choices for the index $j$ such that $j \\neq i$. Thus, there are a total of $m(m-1)$ off-diagonal terms. Since each of these terms has the value $c$, the sum is:\n$$\n\\sum_{i=1}^{m} \\sum_{j=1, j \\neq i}^{m} \\operatorname{Cov}(X_{i}, X_{j}) = m(m-1)c\n$$\nCombining these results, the variance of the sum is:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{m} X_{i}\\right) = m\\sigma^{2} + m(m-1)c\n$$\nSubstituting this into the expression for $\\operatorname{Var}(\\bar{X})$:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{1}{m^2} \\left[ m\\sigma^{2} + m(m-1)c \\right] = \\frac{m\\sigma^{2}}{m^2} + \\frac{m(m-1)c}{m^2}\n$$\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^{2}}{m} + \\frac{m-1}{m}c\n$$\nThis is the expression for $\\operatorname{Var}(\\bar{X})$ in terms of $m$, $\\sigma^{2}$, and $c$.\n\nNext, we rewrite this expression using the constant pairwise correlation $\\rho$. The correlation is defined as $\\rho = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sqrt{\\operatorname{Var}(X_i)\\operatorname{Var}(X_j)}}$. Given $\\operatorname{Cov}(X_i, X_j) = c$ and $\\operatorname{Var}(X_i) = \\sigma^2$, we have:\n$$\n\\rho = \\frac{c}{\\sqrt{\\sigma^2 \\cdot \\sigma^2}} = \\frac{c}{\\sigma^2}\n$$\nFrom this, we express the covariance as $c = \\rho\\,\\sigma^{2}$. Substituting this into our expression for $\\operatorname{Var}(\\bar{X})$:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^{2}}{m} + \\frac{m-1}{m}(\\rho\\,\\sigma^{2})\n$$\nWe can factor out the term $\\frac{\\sigma^{2}}{m}$ to obtain a more compact form:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^{2}}{m} \\left( 1 + (m-1)\\rho \\right)\n$$\nThis is the generalized expression for the variance of the ensemble mean in terms of $m$, $\\sigma^{2}$, and $\\rho$.\n\nc) Finally, we compute the numerical value of $\\operatorname{Var}(\\bar{X})$ using the derived formula and the provided values: $m=15$, $\\sigma^{2}=2.25$ (Kelvin squared), and $\\rho=0.2$.\n\nUsing the expression from part (b):\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{\\sigma^{2}}{m} \\left[ 1 + (m-1)\\rho \\right]\n$$\nSubstituting the given numerical values:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{2.25}{15} \\left[ 1 + (15-1)(0.2) \\right]\n$$\nFirst, we evaluate the term in the brackets:\n$$\n1 + (14)(0.2) = 1 + 2.8 = 3.8\n$$\nNext, we evaluate the pre-factor:\n$$\n\\frac{2.25}{15} = 0.15\n$$\nNow, we multiply these two results to find the final variance:\n$$\n\\operatorname{Var}(\\bar{X}) = 0.15 \\times 3.8\n$$\n$$\n\\operatorname{Var}(\\bar{X}) = 0.57\n$$\nThe computed variance of the ensemble mean is $0.57$ Kelvin squared.",
            "answer": "$$\\boxed{0.57}$$"
        },
        {
            "introduction": "Beyond the structure between models, data within a single simulation often has \"memory,\" where one year's state influences the next, a property called temporal autocorrelation. This exercise demonstrates how this persistence violates the assumption of independence underlying many statistical tools, leading to biased variance estimates and an inflated sense of confidence. You will derive the concept of an \"effective sample size\" to properly account for this memory and ensure robust statistical inference. ",
            "id": "4040651",
            "problem": "A climate model produces daily anomalies of near-surface air temperature that can be idealized as a weakly stationary, zero-mean stochastic process $\\{X_{t}\\}$ with constant variance $\\sigma^{2}$ and autocorrelation function $\\rho(h) = \\mathbb{E}[X_{t} X_{t+h}]/\\sigma^{2}$, where $t$ and $h$ are in integer days. Consider the evaluation of simulated climate variability for a seasonal period in which the model output is temporally subsampled every $m$ days to reduce storage, yielding a subsampled series $\\{Y_{j}\\}$ defined by $Y_{j} = X_{jm}$ for $j = 1, 2, \\dots, M$. Suppose the underlying process follows a first-order autoregressive structure, so that $\\rho(1) = \\rho$ with $0 < \\rho < 1$ and $\\rho(h) = \\rho^{|h|}$.\n\nAnalysts often estimate the variance of the subsampled seasonal-mean anomaly $\\bar{Y} = M^{-1} \\sum_{j=1}^{M} Y_{j}$ by assuming independence and using the large-sample approximation $\\widehat{V}_{\\text{naive}} = s_{Y}^{2}/M$, where $s_{Y}^{2} = (M-1)^{-1} \\sum_{j=1}^{M} (Y_{j} - \\bar{Y})^{2}$ is the sample variance of $\\{Y_{j}\\}$. In the presence of autocorrelation, this independence-based estimator is biased.\n\nStarting from the definitions of covariance and variance, derive the expected variance $\\operatorname{Var}(\\bar{Y})$ of the subsampled mean in terms of $\\sigma^{2}$, $\\rho$, $m$, and $M$, and show explicitly how the independence-based estimator is biased relative to the true value. Then, define the effective sample size $n_{\\text{eff}}$ by the requirement that $\\operatorname{Var}(\\bar{Y}) = \\sigma^2/n_{\\text{eff}}$ and obtain a closed-form expression for $n_{\\text{eff}}$ under the large-sample approximation $M \\to \\infty$.\n\nExpress your final answer for $n_{\\text{eff}}$ as an analytic expression in terms of $M$, $\\rho$, and $m$. Do not provide numerical values and do not include units in the final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of time series analysis as applied to climate science, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. There are no contradictions, ambiguities, or unsound premises. We may therefore proceed with the solution.\n\nThe core of the problem is to determine the variance of the mean of a temporally subsampled autocorrelated time series and to derive the corresponding effective sample size.\n\nFirst, let's derive the exact variance of the subsampled mean, $\\bar{Y}$.\nThe subsampled series is $\\{Y_j\\}$ where $Y_j = X_{jm}$ for $j = 1, 2, \\dots, M$. The mean is $\\bar{Y} = \\frac{1}{M} \\sum_{j=1}^{M} Y_j$.\nThe variance of $\\bar{Y}$ is given by:\n$$ \\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\frac{1}{M} \\sum_{j=1}^{M} Y_j\\right) $$\nUsing the properties of variance, we can write:\n$$ \\operatorname{Var}(\\bar{Y}) = \\frac{1}{M^2} \\operatorname{Var}\\left(\\sum_{j=1}^{M} Y_j\\right) = \\frac{1}{M^2} \\sum_{j=1}^{M} \\sum_{k=1}^{M} \\operatorname{Cov}(Y_j, Y_k) $$\nThe process $\\{X_t\\}$ is defined to be zero-mean, which implies that the subsampled process $\\{Y_j\\}$ is also zero-mean, i.e., $\\mathbb{E}[Y_j] = \\mathbb{E}[X_{jm}] = 0$ for all $j$.\nTherefore, the covariance is simply the expectation of the product:\n$$ \\operatorname{Cov}(Y_j, Y_k) = \\mathbb{E}[Y_j Y_k] - \\mathbb{E}[Y_j]\\mathbb{E}[Y_k] = \\mathbb{E}[Y_j Y_k] $$\nSubstituting the definition of $Y_j$:\n$$ \\operatorname{Cov}(Y_j, Y_k) = \\mathbb{E}[X_{jm} X_{km}] $$\nThis expectation can be expressed using the variance $\\sigma^2$ and the autocorrelation function $\\rho(h)$ of the original process $\\{X_t\\}$. The time lag between the two points is $h = km - jm = (k-j)m$.\n$$ \\mathbb{E}[X_{jm} X_{km}] = \\sigma^2 \\rho(|(k-j)m|) $$\nThe problem states that the underlying process is a first-order autoregressive, AR(1), process, for which $\\rho(h) = \\rho^{|h|}$. Thus:\n$$ \\operatorname{Cov}(Y_j, Y_k) = \\sigma^2 (\\rho^m)^{|k-j|} $$\nsince $m > 0$ and $0 < \\rho < 1$.\nNow we substitute this back into the expression for $\\operatorname{Var}(\\bar{Y})$:\n$$ \\operatorname{Var}(\\bar{Y}) = \\frac{\\sigma^2}{M^2} \\sum_{j=1}^{M} \\sum_{k=1}^{M} (\\rho^m)^{|k-j|} $$\nThe double summation can be simplified by considering the lag $h = k-j$. The lag $h$ ranges from $-(M-1)$ to $M-1$.\nThe term for $h=0$ (i.e., $j=k$) occurs $M$ times, and its value is $(\\rho^m)^0 = 1$.\nFor any non-zero lag $h$, there are $M-|h|$ pairs of $(j, k)$ that satisfy $k-j=h$. The summation can be rewritten as:\n$$ \\sum_{j=1}^{M} \\sum_{k=1}^{M} (\\rho^m)^{|k-j|} = M + \\sum_{h=1-M, h\\ne 0}^{M-1} (M-|h|)(\\rho^m)^{|h|} = M + 2 \\sum_{h=1}^{M-1} (M-h)(\\rho^m)^h $$\nSubstituting this back into the variance expression:\n$$ \\operatorname{Var}(\\bar{Y}) = \\frac{\\sigma^2}{M^2} \\left[ M + 2 \\sum_{h=1}^{M-1} (M-h)(\\rho^m)^h \\right] $$\nFactoring out $M$ from the bracket gives the exact expression for the variance:\n$$ \\operatorname{Var}(\\bar{Y}) = \\frac{\\sigma^2}{M} \\left[ 1 + 2 \\sum_{h=1}^{M-1} \\left(1 - \\frac{h}{M}\\right)(\\rho^m)^h \\right] $$\nThis is the first required result.\n\nNext, we must show how the independence-based estimator $\\widehat{V}_{\\text{naive}} = s_Y^2/M$ is biased. For large $M$, the sample variance $s_Y^2$ is a consistent estimator of the true variance of a single observation, $\\operatorname{Var}(Y_j) = \\operatorname{Var}(X_{jm}) = \\sigma^2$. Therefore, for large $M$, the expectation of the naive estimator approaches $\\sigma^2/M$:\n$$ \\mathbb{E}[\\widehat{V}_{\\text{naive}}] \\approx \\frac{\\sigma^2}{M} $$\nThe true variance, as derived above, is:\n$$ \\operatorname{Var}(\\bar{Y}) = \\frac{\\sigma^2}{M} \\left[ 1 + 2 \\sum_{h=1}^{M-1} \\left(1 - \\frac{h}{M}\\right)(\\rho^m)^h \\right] $$\nSince $0 < \\rho < 1$, we have $\\rho^m > 0$. The summation term is a sum of positive values, so the entire term in the square brackets is greater than $1$.\n$$ \\left[ 1 + 2 \\sum_{h=1}^{M-1} \\left(1 - \\frac{h}{M}\\right)(\\rho^m)^h \\right] > 1 $$\nThis implies that $\\operatorname{Var}(\\bar{Y}) > \\sigma^2/M$.\nComparing the true variance with the large-sample expectation of the naive estimator, we see that $\\mathbb{E}[\\widehat{V}_{\\text{naive}}] < \\operatorname{Var}(\\bar{Y})$. The naive estimator, which assumes independence, systematically underestimates the true variance of the mean in the presence of positive autocorrelation. This demonstrates its negative bias.\n\nFinally, we derive the effective sample size $n_{\\text{eff}}$. It is defined by the relation $\\operatorname{Var}(\\bar{Y}) = \\sigma^2/n_{\\text{eff}}$.\nUsing our expression for $\\operatorname{Var}(\\bar{Y})$, we can solve for $n_{\\text{eff}}$:\n$$ \\frac{\\sigma^2}{n_{\\text{eff}}} = \\frac{\\sigma^2}{M} \\left[ 1 + 2 \\sum_{h=1}^{M-1} \\left(1 - \\frac{h}{M}\\right)(\\rho^m)^h \\right] $$\n$$ n_{\\text{eff}} = \\frac{M}{1 + 2 \\sum_{h=1}^{M-1} \\left(1 - \\frac{h}{M}\\right)(\\rho^m)^h} $$\nThe problem asks for this expression under the large-sample approximation $M \\to \\infty$. In this limit, the term $(1 - h/M)$ approaches $1$ for any fixed $h$, and the sum extends to infinity. The expression becomes:\n$$ n_{\\text{eff}} \\approx \\frac{M}{1 + 2 \\sum_{h=1}^{\\infty} (\\rho^m)^h} $$\nThe sum is an infinite geometric series with first term $a = \\rho^m$ and ratio $r = \\rho^m$. Since $0 < \\rho < 1$ and $m \\ge 1$, the ratio $r$ satisfies $0 < r < 1$, so the series converges to:\n$$ \\sum_{h=1}^{\\infty} (\\rho^m)^h = \\frac{\\rho^m}{1 - \\rho^m} $$\nSubstituting this result into the denominator of the expression for $n_{\\text{eff}}$:\n$$ 1 + 2 \\sum_{h=1}^{\\infty} (\\rho^m)^h = 1 + 2 \\frac{\\rho^m}{1 - \\rho^m} = \\frac{1 - \\rho^m + 2\\rho^m}{1 - \\rho^m} = \\frac{1 + \\rho^m}{1 - \\rho^m} $$\nFinally, substituting this back into the expression for $n_{\\text{eff}}$ gives the large-sample approximation:\n$$ n_{\\text{eff}} \\approx M \\left( \\frac{1 - \\rho^m}{1 + \\rho^m} \\right) $$\nThis is the closed-form expression for the effective sample size $n_{\\text{eff}}$ in terms of $M$, $\\rho$, and $m$ under the large-sample approximation.",
            "answer": "$$\n\\boxed{M \\frac{1 - \\rho^m}{1 + \\rho^m}}\n$$"
        }
    ]
}