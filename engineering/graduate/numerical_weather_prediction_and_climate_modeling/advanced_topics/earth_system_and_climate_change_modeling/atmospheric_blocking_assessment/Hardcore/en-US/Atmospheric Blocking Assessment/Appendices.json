{
    "hands_on_practices": [
        {
            "introduction": "The first step in any quantitative analysis is to correctly implement the diagnostic tool. This exercise will guide you through the process of translating the definition of the widely-used Tibaldi-Molteni (TM) index into a working algorithm . By using a controlled, synthetic dataset, you will develop a practical understanding of core concepts like meridional gradient calculations and the application of a persistence criterion.",
            "id": "4013020",
            "problem": "You are given a conceptual daily dataset of 500-hectopascal geopotential height (Z500) over the Northern Hemisphere. The acronym Z500 stands for 500-hectopascal geopotential height. The task is to implement the Tibaldi–Molteni (TM) index for atmospheric blocking detection and compute a blocking frequency climatology by longitude and season, using a synthetic but scientifically consistent dataset. All angles must be treated in degrees, and all geopotential heights must be treated in meters. Frequencies must be expressed as decimal fractions.\n\nFoundational base and definitions:\n- Atmospheric blocking in midlatitudes is associated with a reversal of the meridional gradient of geopotential height at 500 hectopascals, which implies a reversal of the large-scale geostrophic wind. From geostrophic balance, the zonal geostrophic wind $u_g$ is approximately proportional to the meridional gradient of geopotential height, $u_g \\propto -\\partial Z / \\partial \\phi$, where $\\phi$ is latitude and $Z$ is geopotential height. Therefore, a sign reversal in $\\partial Z / \\partial \\phi$ across a latitude band is indicative of blocking.\n- The Tibaldi–Molteni (TM) index detects blocking through finite differences of $Z$ between three latitudes: a southern latitude $\\phi_S$, a central latitude $\\phi_0$, and a northern latitude $\\phi_N$, at fixed longitude $\\lambda$. Define the southern gradient\n$$\n\\mathrm{GHGS}(\\lambda, t) = \\frac{Z(\\phi_0,\\lambda,t) - Z(\\phi_S,\\lambda,t)}{\\phi_0 - \\phi_S} \\quad \\text{[m per degree]},\n$$\nand the northern gradient\n$$\n\\mathrm{GHGN}(\\lambda, t) = \\frac{Z(\\phi_N,\\lambda,t) - Z(\\phi_0,\\lambda,t)}{\\phi_N - \\phi_0} \\quad \\text{[m per degree]}.\n$$\nA day $t$ at longitude $\\lambda$ is classified as blocked by the TM index if $\\mathrm{GHGS}(\\lambda, t) > 0$ and $\\mathrm{GHGN}(\\lambda, t)  -10$ [m per degree], which represent the standard thresholds used in the TM index.\n- Persistence requirement: A longitude is considered blocked on day $t$ only if the TM blocking condition holds for at least $5$ consecutive days at that longitude. That is, if there is a run of $L \\geq 5$ consecutive days for which the above TM condition is satisfied, then all those $L$ days are marked as blocked; isolated shorter runs are ignored.\n- Blocking frequency climatology by season: For each season $S$ and longitude $\\lambda$, define the blocking frequency\n$$\nf_S(\\lambda) = \\frac{B_S(\\lambda)}{N_S},\n$$\nwhere $B_S(\\lambda)$ is the number of days in season $S$ at longitude $\\lambda$ that meet the persistence-qualified TM blocking condition, and $N_S$ is the total number of days in season $S$. The seasons are defined by calendar months: boreal winter December–January–February (DJF), spring March–April–May (MAM), summer June–July–August (JJA), and autumn September–October–November (SON). Use a non-leap year with monthly day counts $\\{31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31\\}$ for January to December, respectively, so $N_{\\mathrm{DJF}} = 90$, $N_{\\mathrm{MAM}} = 92$, $N_{\\mathrm{JJA}} = 92$, and $N_{\\mathrm{SON}} = 91$.\n\nGrid and data specification:\n- Latitude grid: $\\phi \\in \\{20^\\circ, 22.5^\\circ, \\dots, 80^\\circ\\}$ with $2.5^\\circ$ spacing. Longitude grid: $\\lambda \\in \\{0^\\circ, 2.5^\\circ, \\dots, 357.5^\\circ\\}$ with $2.5^\\circ$ spacing. Daily time axis for one non-leap year ($365$ days).\n- TM index latitudes: use $\\phi_S = 40^\\circ$, $\\phi_0 = 60^\\circ$, and $\\phi_N = 80^\\circ$.\n- Base field: construct a zonal-mean geopotential height profile decreasing with latitude,\n$$\nZ_0(\\phi) = Z_{\\mathrm{ref}} - \\alpha \\, \\phi,\n$$\nwith $Z_{\\mathrm{ref}} = 5800$ [m] and $\\alpha = 6$ [m per degree]. The full field is defined initially by $Z(\\phi,\\lambda,t) = Z_0(\\phi)$ for all $\\lambda$ and $t$ (no noise). This produces background meridional gradients around $-6$ [m per degree] in midlatitudes, consistent with typical midlatitude conditions (non-blocked).\n\nTM anomalies to define the test suite:\nYou must superimpose localized anomalies to create and test blocking detection. The anomalies below must be added to $Z(\\phi,\\lambda,t)$ only at the specified latitude(s), longitude(s), and day ranges, with all other values unmodified.\n\n- Test Case 1 (happy path, DJF, strong blocking): At longitude $\\lambda = 0^\\circ$, for days January $15$ through January $21$ inclusive (i.e., $7$ consecutive days), add $+300$ [m] to $Z(\\phi_0,\\lambda,t)$ at latitude $\\phi_0 = 60^\\circ$. No other changes. Expected effect: $\\mathrm{GHGS} > 0$ and $\\mathrm{GHGN} \\ll -10$ [m per degree].\n- Test Case 2 (boundary threshold case, MAM, should not block): At longitude $\\lambda = 30^\\circ$, for days April $10$ through April $14$ inclusive (i.e., $5$ consecutive days), add $+80$ [m] to $Z(\\phi_0,\\lambda,t)$ at latitude $\\phi_0 = 60^\\circ$, and add $-100$ [m] to $Z(\\phi_S,\\lambda,t)$ at latitude $\\phi_S = 40^\\circ$. No other changes. Expected effect: $\\mathrm{GHGS} > 0$ and $\\mathrm{GHGN} = -10$ [m per degree] exactly, which fails the strict inequality $\\mathrm{GHGN}  -10$, so no blocking should be registered even though the duration is $5$ days.\n- Test Case 3 (non-persistence case, JJA, should not block): At longitude $\\lambda = 90^\\circ$, for days July $5$ through July $8$ inclusive (i.e., $4$ consecutive days), add $+300$ [m] to $Z(\\phi_0,\\lambda,t)$ at latitude $\\phi_0 = 60^\\circ$. No other changes. Expected effect: TM condition holds, but persistence fails because $L = 4  5$, so no blocking.\n- Test Case 4 (persistence edge, SON, exact $5$ days, should block): At longitude $\\lambda = 180^\\circ$, for days October $1$ through October $5$ inclusive (i.e., $5$ consecutive days), add $+300$ [m] to $Z(\\phi_0,\\lambda,t)$ at latitude $\\phi_0 = 60^\\circ$. No other changes. Expected effect: TM condition holds and persistence threshold is met exactly.\n\nComputation requirements:\n- Implement the TM index condition exactly as stated using the above $\\phi_S$, $\\phi_0$, and $\\phi_N$, with the gradient units understood as meters per degree. Treat angles in degrees.\n- Apply the persistence rule of at least $5$ consecutive days at each longitude independently.\n- Compute $f_S(\\lambda)$ for each of the four seasons.\n- Extract the following four scalar results:\n  1. $f_{\\mathrm{DJF}}(0^\\circ)$,\n  2. $f_{\\mathrm{MAM}}(30^\\circ)$,\n  3. $f_{\\mathrm{JJA}}(90^\\circ)$,\n  4. $f_{\\mathrm{SON}}(180^\\circ)$.\n- Express each frequency as a decimal fraction.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the four frequencies in the order listed above. For example, a syntactic illustration of the format is\n$[x_1,x_2,x_3,x_4]$.",
            "solution": "The user's request is to implement the Tibaldi–Molteni (TM) atmospheric blocking index, apply a persistence criterion, and calculate seasonal blocking frequencies for a set of specified test cases using a synthetic geopotential height dataset.\n\n### Step 1: Problem Validation\n\nThe problem statement provides a comprehensive and scientifically sound description of the task.\n\n- **Givens**:\n    - **Physical concept**: Atmospheric blocking detection using the Tibaldi–Molteni (TM) index based on the meridional gradient of 500-hPa geopotential height ($Z$).\n    - **TM Index Formulas**:\n        - Southern gradient: $\\mathrm{GHGS}(\\lambda, t) = \\frac{Z(\\phi_0,\\lambda,t) - Z(\\phi_S,\\lambda,t)}{\\phi_0 - \\phi_S}$.\n        - Northern gradient: $\\mathrm{GHGN}(\\lambda, t) = \\frac{Z(\\phi_N,\\lambda,t) - Z(\\phi_0,\\lambda,t)}{\\phi_N - \\phi_0}$.\n    - **TM Blocking Condition**: $\\mathrm{GHGS}(\\lambda, t) > 0$ and $\\mathrm{GHGN}(\\lambda, t)  -10$ m/degree.\n    - **Persistence**: The condition must hold for at least $5$ consecutive days.\n    - **Frequency Calculation**: $f_S(\\lambda) = B_S(\\lambda) / N_S$, where $B_S(\\lambda)$ is the number of blocked days and $N_S$ is the total days in season $S$.\n    - **Seasons  Year**: DJF ($N_{\\mathrm{DJF}}=90$), MAM ($N_{\\mathrm{MAM}}=92$), JJA ($N_{\\mathrm{JJA}}=92$), SON ($N_{\\mathrm{SON}}=91$), for a non-leap year ($365$ days).\n    - **Grid**: Latitude $\\phi \\in \\{20^\\circ, 22.5^\\circ, \\dots, 80^\\circ\\}$; Longitude $\\lambda \\in \\{0^\\circ, 2.5^\\circ, \\dots, 357.5^\\circ\\}$; daily time step.\n    - **TM Latitudes**: $\\phi_S = 40^\\circ, \\phi_0 = 60^\\circ, \\phi_N = 80^\\circ$.\n    - **Base Field**: $Z_0(\\phi) = Z_{\\mathrm{ref}} - \\alpha \\, \\phi$, with $Z_{\\mathrm{ref}} = 5800$ m and $\\alpha = 6$ m/degree.\n    - **Test Cases**: Four specific anomalies are defined with location, timing, and magnitude.\n    - **Required Outputs**: Four specific seasonal frequencies: $f_{\\mathrm{DJF}}(0^\\circ)$, $f_{\\mathrm{MAM}}(30^\\circ)$, $f_{\\mathrm{JJA}}(90^\\circ)$, $f_{\\mathrm{SON}}(180^\\circ)$.\n\n- **Validation Verdict**:\n    - The problem is **scientifically grounded**, based on a standard meteorological index. The physical premises and parameter values are realistic.\n    - It is **well-posed**, providing all necessary data, formulas, and constraints to arrive at a unique, deterministic solution.\n    - The language is **objective** and precise.\n    - The setup is **complete and consistent**. No contradictions or ambiguities are present.\n\nThe problem is deemed **valid**. We proceed to the solution.\n\n### Step 2: Algorithmic Design and Calculation\n\nThe solution follows a structured, principle-based approach.\n\n**2.1. Grid and Data Structure Initialization**\nFirst, we establish the computational domain. We create grids for time, latitude, and longitude. The time grid runs from day index $0$ to $364$. The spatial grids are defined by their start, end, and step size.\n- Latitude grid $\\phi$: from $20^\\circ$ to $80^\\circ$ in steps of $2.5^\\circ$.\n- Longitude grid $\\lambda$: from $0^\\circ$ to $357.5^\\circ$ in steps of $2.5^\\circ$.\n\nThe geopotential height field $Z$ is a 3D array of size (time, latitude, longitude). We initialize this field with the zonal-mean base state $Z(\\phi,\\lambda,t) = Z_0(\\phi) = 5800 - 6\\phi$. This is achieved by creating a 1D latitude-dependent profile and broadcasting it across the time and longitude dimensions.\n\n**2.2. Application of Test Case Anomalies**\nThe four specified test cases are superimposed onto the base field. This requires mapping calendar dates to day-of-year indices and spatial coordinates to grid indices. Let $t_{idx}$, $\\phi_{idx}$, and $\\lambda_{idx}$ be the indices for day, latitude, and longitude, respectively.\n\n- **Day-of-year mapping**: Day indices run from $0$ (Jan 1) to $364$ (Dec 31).\n    - Case 1: Jan 15-21 corresponds to day indices $14$ through $20$.\n    - Case 2: Apr 10-14 corresponds to day indices $99$ through $103$.\n    - Case 3: Jul 5-8 corresponds to day indices $185$ through $188$.\n    - Case 4: Oct 1-5 corresponds to day indices $273$ through $277$.\n\n- **Grid Indexing**: We find the array indices corresponding to the required latitudes ($\\phi_S=40^\\circ, \\phi_0=60^\\circ, \\phi_N=80^\\circ$) and longitudes ($\\lambda = 0^\\circ, 30^\\circ, 90^\\circ, 180^\\circ$).\n\n- **Anomaly Addition**: For each test case, the specified geopotential height anomaly is added to the corresponding slice of the $Z$ array.\n\n**2.3. Calculation of TM Gradients**\nThe core of the TM index lies in two meridional geopotential height gradients, $\\mathrm{GHGS}$ and $\\mathrm{GHGN}$.\nWe extract the time-longitude data slices at the three key latitudes: $Z_S(t, \\lambda) = Z(\\phi_S, t, \\lambda)$, $Z_0(t, \\lambda) = Z(\\phi_0, t, \\lambda)$, and $Z_N(t, \\lambda) = Z(\\phi_N, t, \\lambda)$.\n\nThe gradients are then computed for all longitudes and days simultaneously using vectorized operations:\n$$\n\\mathrm{GHGS} = \\frac{Z_0 - Z_S}{\\phi_0 - \\phi_S} = \\frac{Z_0 - Z_S}{20}\n$$\n$$\n\\mathrm{GHGN} = \\frac{Z_N - Z_0}{\\phi_N - \\phi_0} = \\frac{Z_N - Z_0}{20}\n$$\nThe results are two 2D arrays, $\\mathrm{GHGS}(t, \\lambda)$ and $\\mathrm{GHGN}(t, \\lambda)$.\n\n**2.4. Instantaneous Blocking Detection**\nA boolean mask, `is_blocked_instantaneous`, of shape (time, longitude) is created by applying the TM index conditions:\n$$\n\\text{is\\_blocked\\_instantaneous}(t, \\lambda) = (\\mathrm{GHGS}(t, \\lambda) > 0) \\land (\\mathrm{GHGN}(t, \\lambda)  -10)\n$$\n\n**2.5. Persistence Criterion Application**\nTo satisfy the persistence requirement, a longitude must be instantaneously blocked for $L \\ge 5$ consecutive days. We process the `is_blocked_instantaneous` mask longitude by longitude. For each longitude's time series, we identify contiguous blocks of `True` values. If a block has a length of $5$ days or more, all days within that block are marked as persistently blocked in a new boolean mask, `is_blocked_persistent`. This is efficiently implemented using `scipy.ndimage.label` to identify the contiguous blocks and `scipy.ndimage.sum_labels` to find their lengths.\n\n**2.6. Manual Verification of Test Cases**\nLet's verify the outcome for each case based on our algorithm. The base state gradients are $\\mathrm{GHGS} = \\mathrm{GHGN} = -6$ m/degree.\n\n- **Case 1**: $\\lambda = 0^\\circ$, days 14-20 (7 days). Anomaly: $\\Delta Z(\\phi_0) = +300$ m.\n    - $\\mathrm{GHGS} = ( (5440+300) - 5560 ) / 20 = 180/20 = 9 > 0$.\n    - $\\mathrm{GHGN} = ( 5320 - (5440+300) ) / 20 = -420/20 = -21  -10$.\n    - The condition holds for $7$ days. Since $7 \\ge 5$, all $7$ days are persistently blocked. These days are in DJF.\n    - $B_{\\mathrm{DJF}}(0^\\circ) = 7$, so $f_{\\mathrm{DJF}}(0^\\circ) = 7 / 90$.\n\n- **Case 2**: $\\lambda = 30^\\circ$, days 99-103 (5 days). Anomaly: $\\Delta Z(\\phi_0) = +80$ m, $\\Delta Z(\\phi_S) = -100$ m.\n    - $\\mathrm{GHGS} = ( (5440+80) - (5560-100) ) / 20 = 60/20 = 3 > 0$.\n    - $\\mathrm{GHGN} = ( 5320 - (5440+80) ) / 20 = -200/20 = -10$.\n    - The condition $\\mathrm{GHGN}  -10$ is not met. No days are instantaneously blocked.\n    - $B_{\\mathrm{MAM}}(30^\\circ) = 0$, so $f_{\\mathrm{MAM}}(30^\\circ) = 0 / 92 = 0$.\n\n- **Case 3**: $\\lambda = 90^\\circ$, days 185-188 (4 days). Anomaly: $\\Delta Z(\\phi_0) = +300$ m.\n    - As in Case 1, the instantaneous condition holds.\n    - The duration is $4$ days. Since $4  5$, the persistence criterion is not met.\n    - $B_{\\mathrm{JJA}}(90^\\circ) = 0$, so $f_{\\mathrm{JJA}}(90^\\circ) = 0 / 92 = 0$.\n\n- **Case 4**: $\\lambda = 180^\\circ$, days 273-277 (5 days). Anomaly: $\\Delta Z(\\phi_0) = +300$ m.\n    - The instantaneous condition holds.\n    - The duration is $5$ days. Since $5 \\ge 5$, the persistence criterion is met. All $5$ days are persistently blocked. These days are in SON.\n    - $B_{\\mathrm{SON}}(180^\\circ) = 5$, so $f_{\\mathrm{SON}}(180^\\circ) = 5 / 91$.\n\n**2.7. Final Frequency Calculation**\nThe final step is to compute the required frequencies by counting the number of `True` values in the `is_blocked_persistent` mask within the appropriate seasonal day ranges and at the specified longitudes, then dividing by the total number of days in that season.\n- Seasonal Day Indices:\n    - DJF: days $0-58$ (Jan, Feb) and $334-364$ (Dec).\n    - MAM: days $59-150$.\n    - JJA: days $151-242$.\n    - SON: days $243-333$.\n- Final results:\n    1. $f_{\\mathrm{DJF}}(0^\\circ) = 7/90$.\n    2. $f_{\\mathrm{MAM}}(30^\\circ) = 0.0$.\n    3. $f_{\\mathrm{JJA}}(90^\\circ) = 0.0$.\n    4. $f_{\\mathrm{SON}}(180^\\circ) = 5/91$.\nThese values are then computed as floating-point numbers and formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import ndimage\n\ndef solve():\n    \"\"\"\n    Implements the Tibaldi-Molteni blocking index and calculates seasonal blocking frequencies.\n    \"\"\"\n    # 1. Foundational base and definitions\n    Z_REF = 5800.0  # meters\n    ALPHA = 6.0  # m per degree\n    PHI_S, PHI_0, PHI_N = 40.0, 60.0, 80.0  # degrees\n    GHGN_THRESHOLD = -10.0  # m per degree\n    PERSISTENCE_DAYS = 5\n    \n    # Seasonal day counts\n    N_DAYS_MONTH = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n    N_DAYS_YEAR = sum(N_DAYS_MONTH)\n    N_DJF, N_MAM, N_JJA, N_SON = 90, 92, 92, 91\n\n    # 2. Grid and data specification\n    lat_grid = np.arange(20.0, 80.1, 2.5)\n    lon_grid = np.arange(0.0, 357.6, 2.5)\n    time_grid = np.arange(N_DAYS_YEAR)\n\n    n_time, n_lat, n_lon = len(time_grid), len(lat_grid), len(lon_grid)\n\n    # 3. Construct the base Z500 field\n    z500 = np.zeros((n_time, n_lat, n_lon))\n    z0_profile = Z_REF - ALPHA * lat_grid\n    z500[:, :, :] = z0_profile[np.newaxis, :, np.newaxis]\n\n    # Helper to find grid indices\n    def get_idx(grid, value):\n        return np.where(grid == value)[0][0]\n\n    lat_idx_S = get_idx(lat_grid, PHI_S)\n    lat_idx_0 = get_idx(lat_grid, PHI_0)\n    lat_idx_N = get_idx(lat_grid, PHI_N)\n\n    # 4. Apply test case anomalies\n    cum_days = np.cumsum([0] + N_DAYS_MONTH[:-1])\n\n    # Case 1: DJF, strong blocking\n    lon_idx_1 = get_idx(lon_grid, 0.0)\n    day_start_1 = cum_days[0] + 15 - 1\n    day_end_1 = cum_days[0] + 21 - 1\n    z500[day_start_1 : day_end_1 + 1, lat_idx_0, lon_idx_1] += 300.0\n\n    # Case 2: MAM, boundary threshold\n    lon_idx_2 = get_idx(lon_grid, 30.0)\n    day_start_2 = cum_days[3] + 10 - 1\n    day_end_2 = cum_days[3] + 14 - 1\n    z500[day_start_2 : day_end_2 + 1, lat_idx_0, lon_idx_2] += 80.0\n    z500[day_start_2 : day_end_2 + 1, lat_idx_S, lon_idx_2] -= 100.0\n\n    # Case 3: JJA, non-persistence\n    lon_idx_3 = get_idx(lon_grid, 90.0)\n    day_start_3 = cum_days[6] + 5 - 1\n    day_end_3 = cum_days[6] + 8 - 1\n    z500[day_start_3 : day_end_3 + 1, lat_idx_0, lon_idx_3] += 300.0\n\n    # Case 4: SON, persistence edge\n    lon_idx_4 = get_idx(lon_grid, 180.0)\n    day_start_4 = cum_days[9] + 1 - 1\n    day_end_4 = cum_days[9] + 5 - 1\n    z500[day_start_4 : day_end_4 + 1, lat_idx_0, lon_idx_4] += 300.0\n\n    # 5. Compute TM gradients and instantaneous blocking\n    z_S = z500[:, lat_idx_S, :]\n    z_0 = z500[:, lat_idx_0, :]\n    z_N = z500[:, lat_idx_N, :]\n    \n    ghgs = (z_0 - z_S) / (PHI_0 - PHI_S)\n    ghgn = (z_N - z_0) / (PHI_N - PHI_0)\n\n    is_blocked_instantaneous = (ghgs > 0)  (ghgn  GHGN_THRESHOLD)\n\n    # 6. Apply persistence requirement\n    is_blocked_persistent = np.full_like(is_blocked_instantaneous, False, dtype=bool)\n\n    for j in range(n_lon):\n        # Identify contiguous runs of True\n        labeled_array, num_features = ndimage.label(is_blocked_instantaneous[:, j])\n        if num_features > 0:\n            # Find the length of each run\n            run_indices = np.arange(1, num_features + 1)\n            run_lengths = ndimage.sum_labels(is_blocked_instantaneous[:, j], labeled_array, index=run_indices)\n            \n            # Find runs that meet the persistence criterion\n            persistent_runs = run_indices[run_lengths >= PERSISTENCE_DAYS]\n            \n            if persistent_runs.size > 0:\n                # Mark all days in those runs as persistently blocked\n                mask = np.isin(labeled_array, persistent_runs)\n                is_blocked_persistent[:, j] = mask\n\n    # 7. Compute seasonal frequencies for the required longitudes\n    # Seasonal day index ranges\n    # DJF: Jan, Feb, Dec\n    day_idx_jan_end = cum_days[1]\n    day_idx_feb_end = cum_days[2]\n    day_idx_dec_start = cum_days[11]\n    \n    # MAM: Mar, Apr, May\n    day_idx_mar_start = cum_days[2]\n    day_idx_may_end = cum_days[5]\n\n    # JJA: Jun, Jul, Aug\n    day_idx_jun_start = cum_days[5]\n    day_idx_aug_end = cum_days[8]\n    \n    # SON: Sep, Oct, Nov\n    day_idx_sep_start = cum_days[8]\n    day_idx_nov_end = cum_days[11]\n\n    # Result 1: f_DJF(0°)\n    blocked_days_djf_1 = np.sum(is_blocked_persistent[0:day_idx_feb_end, lon_idx_1])\n    blocked_days_djf_2 = np.sum(is_blocked_persistent[day_idx_dec_start:, lon_idx_1])\n    f_djf = (blocked_days_djf_1 + blocked_days_djf_2) / N_DJF\n    \n    # Result 2: f_MAM(30°)\n    blocked_days_mam = np.sum(is_blocked_persistent[day_idx_mar_start:day_idx_may_end, lon_idx_2])\n    f_mam = blocked_days_mam / N_MAM\n\n    # Result 3: f_JJA(90°)\n    blocked_days_jja = np.sum(is_blocked_persistent[day_idx_jun_start:day_idx_aug_end, lon_idx_3])\n    f_jja = blocked_days_jja / N_JJA\n\n    # Result 4: f_SON(180°)\n    blocked_days_son = np.sum(is_blocked_persistent[day_idx_sep_start:day_idx_nov_end, lon_idx_4])\n    f_son = blocked_days_son / N_SON\n\n    results = [f_djf, f_mam, f_jja, f_son]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An implemented index has parameters, and the choice of these parameters affects the outcome. This practice introduces the critical skill of sensitivity analysis, where you will evaluate how the detection of blocking events changes with different thresholds . You will set up a numerical experiment with a known 'ground truth' and use Receiver Operating Characteristic (ROC) analysis to quantify performance, providing a robust method for understanding an algorithm's behavior.",
            "id": "4013005",
            "problem": "You are given the task of assessing the sensitivity of detected atmospheric blocking to threshold choices in the Tibaldi–Molteni method using a parameter sweep and computing Receiver Operating Characteristic (ROC) curves against an independently defined manual blocking catalog. The goal is to build a fully specified numerical experiment in a controlled synthetic setting, starting from fundamental dynamical principles and leading to quantifiable detection metrics.\n\nBegin from the geostrophic balance for large-scale midlatitude flow at constant pressure, where the geostrophic wind is related to the gradient of geopotential height. Let $Z(\\phi,\\lambda,t)$ denote the geopotential height at $500$ hectopascals as a function of latitude $\\phi$ (in degrees), longitude $\\lambda$ (in degrees), and time $t$ (in days). Under geostrophic balance, the zonal geostrophic wind $u_g$ at a given latitude satisfies\n$$\nu_g = -\\frac{g}{f}\\frac{\\partial Z}{\\partial y},\n$$\nwhere $g$ is the acceleration due to gravity and $f$ is the Coriolis parameter. A reversal of the meridional gradient of $Z$ north of $60^\\circ\\mathrm{N}$ indicates a reversal of $u_g$, a hallmark of an atmospheric block. A variant of the Tibaldi–Molteni method operationalizes this via finite differences of $Z$ across latitude bands. For a latitude band centered on $60^\\circ\\mathrm{N}$, define the meridional finite-difference proxies\n$$\nG_N(\\lambda,t) = \\frac{Z(80^\\circ\\mathrm{N},\\lambda,t) - Z(60^\\circ\\mathrm{N},\\lambda,t)}{20^\\circ}, \\quad\nG_S(\\lambda,t) = \\frac{Z(60^\\circ\\mathrm{N},\\lambda,t) - Z(40^\\circ\\mathrm{N},\\lambda,t)}{20^\\circ},\n$$\nwhere $20^\\circ$ is the latitude separation. A longitude $\\lambda$ at time $t$ is classified as blocked if both a northern gradient reversal and sufficient southern westerly flow are present, expressed by\n$$\nG_N(\\lambda,t) \\ge T_N, \\quad G_S(\\lambda,t) \\le -T_S,\n$$\nwhere $T_N \\ge 0$ and $T_S \\ge 0$ are thresholds (in $\\mathrm{m\\,deg^{-1}}$). A day $t$ is classified as blocked if the fraction of longitudes satisfying both inequalities equals or exceeds a prescribed fraction $q$, i.e.,\n$$\n\\frac{1}{L}\\sum_{\\lambda} \\mathbb{I}\\left(G_N(\\lambda,t) \\ge T_N \\ \\wedge \\ G_S(\\lambda,t) \\le -T_S\\right) \\ge q,\n$$\nwhere $L$ is the number of longitudes and $\\mathbb{I}$ is the indicator function.\n\nConstruct a synthetic but scientifically plausible set of 500-hectopascal geopotential height fields over $L=36$ longitudes and $D=100$ days on the latitudes $\\phi \\in \\{40^\\circ\\mathrm{N},60^\\circ\\mathrm{N},80^\\circ\\mathrm{N}\\}$. Use a zonally uniform base state that decreases poleward, and superimpose a localized planetary-scale anomaly that creates intermittent gradient reversals north of $60^\\circ\\mathrm{N}$. Specifically:\n\n- Let the base state be\n$$\nZ_{\\text{base}}(\\phi) = 5800\\,\\mathrm{m} - 10\\,\\mathrm{m\\,deg^{-1}}\\times(\\phi - 40^\\circ).\n$$\n- Let the anomaly be\n$$\nA(\\phi,\\lambda,t) = \\alpha(\\phi)\\,a(t)\\,\\max\\left[\\cos\\left(k\\,\\lambda\\right),\\,0\\right],\n$$\nwith $k=2$, and latitude weights $\\alpha(40^\\circ)=100\\,\\mathrm{m}$, $\\alpha(60^\\circ)=-200\\,\\mathrm{m}$, $\\alpha(80^\\circ)=400\\,\\mathrm{m}$. The amplitude $a(t)$ is nonzero only during three blocking episodes at days $t\\in\\{20,21,22,23,24\\}$, $t\\in\\{50,51,52,53,54\\}$, and $t\\in\\{75,76,77,78,79\\}$, with values $a(t)$ set respectively to $[0.2,0.4,0.6,0.4,0.2]$, $[0.3,0.6,0.9,0.6,0.3]$, and $[0.1,0.2,0.3,0.2,0.1]$. Outside these episodes, $a(t)=0$.\n- The full $Z$ field is then\n$$\nZ(\\phi,\\lambda,t) = Z_{\\text{base}}(\\phi) + A(\\phi,\\lambda,t).\n$$\n\nFrom these $Z(\\phi,\\lambda,t)$ fields, compute $G_N(\\lambda,t)$ and $G_S(\\lambda,t)$ in $\\mathrm{m\\,deg^{-1}}$. Define an independent manual blocking catalog by applying fixed thresholds to the same gradients but with stricter criteria: a day $t$ is manually labeled blocked if\n$$\n\\frac{1}{L}\\sum_{\\lambda} \\mathbb{I}\\left(G_N(\\lambda,t) \\ge T_N^{\\mathrm{man}} \\ \\wedge \\ G_S(\\lambda,t) \\le -T_S^{\\mathrm{man}}\\right) \\ge q_{\\mathrm{man}},\n$$\nwith $T_N^{\\mathrm{man}}=5\\,\\mathrm{m\\,deg^{-1}}$, $T_S^{\\mathrm{man}}=10\\,\\mathrm{m\\,deg^{-1}}$, and $q_{\\mathrm{man}}=0.4$. The operational detection will use the same longitude-fraction approach with $q=0.3$ but with thresholds $(T_N,T_S)$ varied across a grid.\n\nCompute the True Positive Rate and False Positive Rate against the manual catalog for a parameter sweep of thresholds:\n- $T_N \\in \\{0,5,10,15,20,25,30\\}$ in $\\mathrm{m\\,deg^{-1}}$,\n- $T_S \\in \\{5,10,12,15,20,25\\}$ in $\\mathrm{m\\,deg^{-1}}$.\nFor each pair $(T_N,T_S)$, compute\n$$\n\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}, \\quad \\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}},\n$$\nwhere $\\mathrm{TP}$, $\\mathrm{FP}$, $\\mathrm{TN}$, and $\\mathrm{FN}$ are the standard confusion matrix counts over $D$ days. Use these $(\\mathrm{FPR},\\mathrm{TPR})$ points to compute the Area Under the Curve (AUC) of the ROC using the trapezoidal rule along the increasing $\\mathrm{FPR}$ axis. To ensure coverage from $0$ to $1$, augment the set with the trivial classifier points $(0,0)$ and $(1,1)$; when multiple points share identical $\\mathrm{FPR}$, retain the one with maximal $\\mathrm{TPR}$.\n\nPhysical units must be handled carefully:\n- Express geopotential heights in meters ($\\mathrm{m}$).\n- Express meridional gradients in meters per degree latitude ($\\mathrm{m\\,deg^{-1}}$).\n- Angles in the anomaly definition must be in degrees.\n\nYour program must implement the above and, for validation, report the following test suite results:\n- Compute the ROC AUC over the full sweep grid as a single float.\n- Compute the $\\mathrm{TPR}$ and $\\mathrm{FPR}$ for the five specified threshold pairs:\n  1. $(T_N,T_S) = (0,5)$,\n  2. $(T_N,T_S) = (5,10)$,\n  3. $(T_N,T_S) = (10,10)$,\n  4. $(T_N,T_S) = (15,12)$,\n  5. $(T_N,T_S) = (25,20)$,\nall thresholds in $\\mathrm{m\\,deg^{-1}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$$\n[\\mathrm{AUC}, \\mathrm{TPR}_{(0,5)}, \\mathrm{FPR}_{(0,5)}, \\mathrm{TPR}_{(5,10)}, \\mathrm{FPR}_{(5,10)}, \\mathrm{TPR}_{(10,10)}, \\mathrm{FPR}_{(10,10)}, \\mathrm{TPR}_{(15,12)}, \\mathrm{FPR}_{(15,12)}, \\mathrm{TPR}_{(25,20)}, \\mathrm{FPR}_{(25,20)}].\n$$\nAll numerical outputs must be plain decimals without any percentage sign. There is no unit in the final output because the metrics are dimensionless. The code must be fully self-contained, take no input, and run deterministically.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of atmospheric dynamics (geostrophic balance), employs established diagnostic methods (Tibaldi–Molteni index), and presents a well-posed, deterministic numerical experiment. The setup is self-contained, with all necessary parameters, equations, and procedures clearly defined for constructing a synthetic dataset and evaluating a detection algorithm's performance using standard metrics (ROC analysis). The problem is objective and free of contradictions or ambiguities.\n\nThe solution proceeds by implementing the specified numerical experiment step-by-step.\n\n**Step 1: Generation of Synthetic Geopotential Height Data**\n\nThe core of the problem is to generate a synthetic 500-hectopascal ($500\\,\\mathrm{hPa}$) geopotential height field, $Z(\\phi,\\lambda,t)$, over a discrete grid. The grid consists of $D=100$ days, $L=36$ longitudes, and $3$ specified latitudes.\n- Latitudes: $\\phi \\in \\{40^\\circ\\mathrm{N}, 60^\\circ\\mathrm{N}, 80^\\circ\\mathrm{N}\\}$.\n- Longitudes: A regular grid of $L=36$ points is assumed, with a spacing of $10^\\circ$. We define this as $\\lambda \\in \\{0^\\circ, 10^\\circ, \\dots, 350^\\circ\\}$.\n- Time: $t \\in \\{0, 1, \\dots, 99\\}$ representing $D=100$ days.\n\nThe field $Z(\\phi,\\lambda,t)$ is the sum of a base state $Z_{\\text{base}}(\\phi)$ and a time- and space-varying anomaly $A(\\phi,\\lambda,t)$.\n\nThe base state $Z_{\\text{base}}(\\phi)$ is a zonally uniform field that depends only on latitude:\n$$\nZ_{\\text{base}}(\\phi) = 5800\\,\\mathrm{m} - 10\\,\\mathrm{m\\,deg^{-1}}\\times(\\phi - 40^\\circ)\n$$\nFor the specified latitudes, this yields:\n- $Z_{\\text{base}}(40^\\circ\\mathrm{N}) = 5800\\,\\mathrm{m}$\n- $Z_{\\text{base}}(60^\\circ\\mathrm{N}) = 5800 - 10 \\times (60-40) = 5600\\,\\mathrm{m}$\n- $Z_{\\text{base}}(80^\\circ\\mathrm{N}) = 5800 - 10 \\times (80-40) = 5400\\,\\mathrm{m}$\n\nThe anomaly $A(\\phi,\\lambda,t)$ introduces a localized, wave-like disturbance:\n$$\nA(\\phi,\\lambda,t) = \\alpha(\\phi)\\,a(t)\\,\\max\\left[\\cos\\left(k\\,\\lambda\\right),\\,0\\right]\n$$\nThe parameters are:\n- Wavenumber $k=2$.\n- Latitude-dependent weights: $\\alpha(40^\\circ)=100\\,\\mathrm{m}$, $\\alpha(60^\\circ)=-200\\,\\mathrm{m}$, $\\alpha(80^\\circ)=400\\,\\mathrm{m}$.\n- Time-dependent amplitude $a(t)$: It is non-zero only for three episodes.\n  - For $t \\in \\{20, \\dots, 24\\}$: $a(t) = [0.2, 0.4, 0.6, 0.4, 0.2]$\n  - For $t \\in \\{50, \\dots, 54\\}$: $a(t) = [0.3, 0.6, 0.9, 0.6, 0.3]$\n  - For $t \\in \\{75, \\dots, 79\\}$: $a(t) = [0.1, 0.2, 0.3, 0.2, 0.1]$\n  - Otherwise, $a(t)=0$.\nThe angle $\\lambda$ is in degrees; numerical computation of the cosine requires conversion to radians. The `max` function ensures the anomaly is positive-definite, representing a high-pressure ridge.\n\nThe full field is $Z(\\phi,\\lambda,t) = Z_{\\text{base}}(\\phi) + A(\\phi,\\lambda,t)$. This will be implemented as a $3$-dimensional NumPy array of shape $(3, 36, 100)$ for latitudes, longitudes, and time, respectively.\n\n**Step 2: Computation of Meridional Gradient Proxies**\n\nThe problem uses a standard finite-difference approximation of the meridional geopotential height gradient, where gradients are computed as `(north - south) / distance`. We compute the northern gradient, $G_N$, and the southern gradient, $G_S$, for each longitude $\\lambda$ and time $t$:\n$$\nG_N(\\lambda,t) = \\frac{Z(80^\\circ\\mathrm{N},\\lambda,t) - Z(60^\\circ\\mathrm{N},\\lambda,t)}{20^\\circ}\n$$\n$$\nG_S(\\lambda,t) = \\frac{Z(60^\\circ\\mathrm{N},\\lambda,t) - Z(40^\\circ\\mathrm{N},\\lambda,t)}{20^\\circ}\n$$\nThe units are correctly given in $\\mathrm{m\\,deg^{-1}}$. These will be computed as two $2$-dimensional NumPy arrays of shape $(36, 100)$.\n\n**Step 3: Day Classification for Manual and Operational Catalogs**\n\nA day $t$ is classified as blocked if a sufficient fraction of its longitudes are blocked. A longitude $\\lambda$ is blocked if $G_N(\\lambda,t) \\ge T_N$ and $G_S(\\lambda,t) \\le -T_S$.\n\nThe classification for a day $t$ is performed using the indicator function $\\mathbb{I}$:\n$$\n\\text{is_blocked}(t) = \\left[ \\frac{1}{L}\\sum_{\\lambda=1}^{L} \\mathbb{I}\\left(G_N(\\lambda,t) \\ge T_N \\ \\wedge \\ G_S(\\lambda,t) \\le -T_S\\right) \\ge q \\right]\n$$\n\nWe generate two catalogs (boolean arrays of length $D=100$):\n1.  **Manual Catalog (Ground Truth):** Uses fixed, strict criteria: $T_N^{\\mathrm{man}}=5\\,\\mathrm{m\\,deg^{-1}}$, $T_S^{\\mathrm{man}}=10\\,\\mathrm{m\\,deg^{-1}}$, and $q_{\\mathrm{man}}=0.4$. This catalog serves as the \"true\" classification against which the operational method is tested.\n2.  **Operational Catalog:** Uses a fixed longitude fraction $q=0.3$, but varies the thresholds $(T_N, T_S)$ over a specified grid.\n    - $T_N \\in \\{0,5,10,15,20,25,30\\}\\,\\mathrm{m\\,deg^{-1}}$\n    - $T_S \\in \\{5,10,12,15,20,25\\}\\,\\mathrm{m\\,deg^{-1}}$\n\n**Step 4: ROC Curve Construction and Metric Calculation**\n\nFor each pair of operational thresholds $(T_N, T_S)$, we generate a classification for all $100$ days and compare it to the manual catalog to build a confusion matrix:\n- **True Positives (TP):** Days blocked in both operational and manual catalogs.\n- **False Positives (FP):** Days blocked in operational but not manual catalog.\n- **True Negatives (TN):** Days not blocked in either catalog.\n- **False Negatives (FN):** Days not blocked in operational but blocked in manual catalog.\n\nFrom these counts, the True Positive Rate (TPR) and False Positive Rate (FPR) are computed:\n$$\n\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{P}} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}, \\quad \\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{N}} = \\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}}\n$$\nwhere $\\mathrm{P}$ is the total number of positives (blocked days) and $\\mathrm{N}$ is the total number of negatives (non-blocked days) in the manual catalog. If a denominator is zero, the respective rate is taken to be zero.\n\nThe collection of $(\\mathrm{FPR}, \\mathrm{TPR})$ pairs from the parameter sweep forms the basis of the Receiver Operating Characteristic (ROC) curve.\n\n**Step 5: Area Under the Curve (AUC) Calculation**\n\nTo compute the AUC, the set of calculated $(\\mathrm{FPR}, \\mathrm{TPR})$ points is processed as follows:\n1.  **Augmentation:** The set is augmented with the trivial points $(0,0)$ (never classifying as positive) and $(1,1)$ (always classifying as positive) to ensure the curve spans the full unit square.\n2.  **Uniqueness:** For any set of points that share the same $\\mathrm{FPR}$ value, only the point with the maximum $\\mathrm{TPR}$ is retained. This ensures the ROC curve is a monotonically non-decreasing function.\n3.  **Sorting:** The resulting unique points are sorted in ascending order of their $\\mathrm{FPR}$ values.\n4.  **Integration:** The Area Under the Curve (AUC) is calculated by applying the trapezoidal rule to the sorted points. This is readily done using `numpy.trapz`.\n\nFinally, the calculated AUC and the specific $(\\mathrm{TPR}, \\mathrm{FPR})$ values for the five requested threshold pairs are collected and formatted into the required output string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to assess atmospheric blocking detection sensitivity.\n    \"\"\"\n    # === Step 0: Define constants and grids ===\n    D = 100  # Number of days\n    L = 36   # Number of longitudes\n    \n    # Define spatial and temporal grids\n    t_vals = np.arange(D)\n    phi_vals = np.array([40, 60, 80])\n    lambda_vals = np.arange(0, 360, 10) # 0, 10, ..., 350\n\n    # === Step 1: Generate Synthetic Geopotential Height Data ===\n    \n    # Base state Z_base(phi)\n    Z_base = 5800 - 10 * (phi_vals - 40)\n\n    # Anomaly field A(phi, lambda, t)\n    # Time-dependent amplitude a(t)\n    a_t = np.zeros(D)\n    blocking_episodes = {\n        (20, 25): [0.2, 0.4, 0.6, 0.4, 0.2],\n        (50, 55): [0.3, 0.6, 0.9, 0.6, 0.3],\n        (75, 80): [0.1, 0.2, 0.3, 0.2, 0.1]\n    }\n    for (start, end), values in blocking_episodes.items():\n        a_t[start:end] = values\n\n    # Latitude weights alpha(phi)\n    alpha_vals = np.array([100, -200, 400])\n\n    # Wavenumber k\n    k = 2\n\n    # Assemble the full field Z = Z_base + A\n    # Reshape arrays for broadcasting: (phi, lambda, time)\n    Z_base_3d = Z_base.reshape(3, 1, 1)\n    alpha_3d = alpha_vals.reshape(3, 1, 1)\n    a_t_3d = a_t.reshape(1, 1, D)\n    \n    # Convert lambda from degrees to radians for numpy's cos function\n    lambda_rad = np.deg2rad(lambda_vals)\n    cos_term = np.maximum(np.cos(k * lambda_rad), 0).reshape(1, L, 1)\n    \n    A = alpha_3d * a_t_3d * cos_term\n    Z = Z_base_3d + A\n\n    # === Step 2: Compute Gradients G_N and G_S ===\n    # Z has shape (3, 36, 100) corresponding to (phi, lambda, t)\n    Z_40N = Z[0, :, :]\n    Z_60N = Z[1, :, :]\n    Z_80N = Z[2, :, :]\n    \n    G_N = (Z_80N - Z_60N) / 20.0\n    G_S = (Z_60N - Z_40N) / 20.0\n\n    # === Step 3: Define Manual Catalog (Ground Truth) ===\n    T_N_man, T_S_man, q_man = 5.0, 10.0, 0.4\n    \n    manual_cond = (G_N >= T_N_man)  (G_S = -T_S_man)\n    manual_frac_blocked = np.mean(manual_cond, axis=0) # Fraction over longitudes for each day\n    manual_catalog = manual_frac_blocked >= q_man\n\n    P = np.sum(manual_catalog)  # Total positives\n    N = D - P                   # Total negatives\n\n    # === Step 4: Parameter Sweep and Metric Calculation ===\n    T_N_sweep = [0, 5, 10, 15, 20, 25, 30]\n    T_S_sweep = [5, 10, 12, 15, 20, 25]\n    q_op = 0.3\n\n    roc_points = []\n    specific_results = {}\n    test_cases = [(0, 5), (5, 10), (10, 10), (15, 12), (25, 20)]\n\n    for T_N in T_N_sweep:\n        for T_S in T_S_sweep:\n            # Generate operational catalog for this (T_N, T_S) pair\n            op_cond = (G_N >= T_N)  (G_S = -T_S)\n            op_frac_blocked = np.mean(op_cond, axis=0)\n            op_catalog = op_frac_blocked >= q_op\n\n            # Compute confusion matrix elements\n            TP = np.sum(op_catalog  manual_catalog)\n            FP = np.sum(op_catalog  ~manual_catalog)\n            \n            TPR = TP / P if P > 0 else 0.0\n            FPR = FP / N if N > 0 else 0.0\n            \n            roc_points.append((FPR, TPR))\n            \n            if (T_N, T_S) in test_cases:\n                specific_results[(T_N, T_S)] = (TPR, FPR)\n\n    # === Step 5: AUC Calculation ===\n    # Augment with trivial classifiers\n    roc_points.extend([(0.0, 0.0), (1.0, 1.0)])\n\n    # Retain maximal TPR for each unique FPR\n    fpr_tpr_map = {}\n    for fpr, tpr in roc_points:\n        fpr_tpr_map[fpr] = max(fpr_tpr_map.get(fpr, -1.0), tpr)\n\n    # Sort points by FPR\n    sorted_roc_points = sorted(fpr_tpr_map.items())\n    \n    final_fprs = [p[0] for p in sorted_roc_points]\n    final_tprs = [p[1] for p in sorted_roc_points]\n    \n    # Compute AUC using trapezoidal rule\n    AUC = np.trapz(final_tprs, final_fprs)\n\n    # === Step 6: Format and Print Output ===\n    final_output = [AUC]\n    for case in test_cases:\n        tpr, fpr = specific_results.get(case, (float('nan'), float('nan')))\n        final_output.extend([tpr, fpr])\n        \n    print(f\"[{','.join(map(str, final_output))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Validation often needs to go beyond simple day-by-day counts. Since atmospheric blocking events are contiguous objects in space and time, a more meaningful validation compares detected events to reference events. This advanced exercise introduces a sophisticated framework for event-based validation using the Jaccard similarity index and an optimal matching algorithm, enabling a nuanced assessment of skill through metrics like precision, recall, and specific errors in event timing .",
            "id": "4013038",
            "problem": "You are given a curated manual catalog of atmospheric blocking events and outputs from an automated detection algorithm. Each event is represented as a spatiotemporal object on a periodic longitude circle of circumference $360^{\\circ}$ and a discrete-time axis in days. A single event is completely described by a four-tuple $(\\ell, w, t_s, t_e)$ where $\\ell$ is the longitude center in degrees, $w$ is the longitudinal width in degrees with $0  w \\le 360$, $t_s$ is the onset day index, and $t_e$ is the cessation day index with $t_s \\le t_e$. The longitudinal arc occupied by an event is the set of longitudes\n$$\nA(\\ell,w) = \\{ \\lambda \\in [0,360) : \\text{the shortest arc centered at } \\ell \\text{ of length } w \\text{ contains } \\lambda \\},\n$$\ninterpreted on the circle with period $360^{\\circ}$. The temporal support of an event is the inclusive set of days\n$$\nI(t_s,t_e) = \\{ t \\in \\mathbb{Z} : t_s \\le t \\le t_e \\}.\n$$\n\nYour task is to validate the detector against the manual catalog by:\n- establishing one-to-one matches between manual and detected events using a similarity criterion based on set theory,\n- computing skill measures (precision, recall, and F1 score),\n- quantifying event timing accuracy in onset and cessation in days, and\n- reporting failure characteristics via counts of false positives and false negatives.\n\nFundamental base and definitions to use:\n- For any two sets $X$ and $Y$ on a common domain with finite, nonnegative measure, the Jaccard similarity is defined by the ratio $J(X,Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}$, where $|\\cdot|$ denotes measure (length on the circle for longitude arcs in degrees, and cardinality for discrete-day intervals, with inclusive day counting).\n- Let $r_s$ denote the spatial Jaccard similarity $J(A(\\ell_m,w_m),A(\\ell_d,w_d))$ and let $r_t$ denote the temporal Jaccard similarity $J(I(t_{s,m},t_{e,m}), I(t_{s,d},t_{e,d}))$ for a manual event and a detected event, respectively. A candidate match is admissible if and only if $r_s \\ge \\tau_s$ and $r_t \\ge \\tau_t$ for given thresholds $\\tau_s$ and $\\tau_t$ in $[0,1]$.\n- Define a combined similarity score for admissible pairs by $S = r_s \\times r_t$ and $S = 0$ for inadmissible pairs. Establish a one-to-one matching between manual and detected events by maximizing the sum of $S$ over matched pairs subject to each event being used at most once.\n- Let $TP$ be the number of matched pairs with admissible similarity, $FN$ the number of manual events not matched, and $FP$ the number of detected events not matched. Precision is $P = \\frac{TP}{TP+FP}$ with the convention $P = 0$ if $TP+FP = 0$. Recall is $R = \\frac{TP}{TP+FN}$ with the convention $R = 0$ if $TP+FN = 0$. The F1 score is $F1 = \\frac{2PR}{P+R}$ with the convention $F1 = 0$ if $P+R = 0$.\n- For each matched manual–detected pair, define the onset timing error $e_s = t_{s,d} - t_{s,m}$ and the cessation timing error $e_e = t_{e,d} - t_{e,m}$. The event timing accuracy measures to report are the mean absolute onset error $\\overline{|e_s|}$ and mean absolute cessation error $\\overline{|e_e|}$, each expressed in days. If there are no matched pairs, report both timing accuracy measures as $0$ days.\n\nImportant implementation details:\n- Longitude geometry is periodic with period $360^{\\circ}$. Lengths of arc unions and intersections must be computed on the circle, honoring wrap-around at $0^{\\circ}$ and $360^{\\circ}$. Treat longitudinal arcs by their geometric length in degrees. Treat temporal intervals as inclusive on integer days, so that the length of $I(t_s,t_e)$ is $t_e - t_s + 1$ days.\n- The one-to-one matching must maximize the sum of combined similarity scores $S$ and respect admissibility defined by the thresholds. If multiple pairings achieve equal maximum score, any such optimal pairing is acceptable as long as it is consistent with the admissibility constraints.\n\nUse the following thresholds for all cases: $\\tau_s = 0.5$ and $\\tau_t = 0.5$.\n\nTest suite:\nEach test case is a pair of lists. The first list is the manual catalog, and the second list is the detections, both as lists of tuples $(\\ell, w, t_s, t_e)$ with $\\ell$ in degrees, $w$ in degrees, and $t_s,t_e$ in days.\n\n- Case 1 (general well-matched): manual $=[(30, 60, 2, 10),(200, 50, 15, 22)]$; detections $=[(35, 70, 3, 11),(205, 40, 16, 20)]$.\n- Case 2 (periodic wrap and one spurious): manual $=[(350, 40, 5, 12)]$; detections $=[(355, 50, 6, 13),(150, 30, 7, 10)]$.\n- Case 3 (duplicate detections for one manual event): manual $=[(100, 60, 0, 9)]$; detections $=[(102, 70, 0, 9),(98, 50, 2, 8)]$.\n- Case 4 (temporal fragmentation and a miss): manual $=[(250, 80, 10, 25),(300, 50, 5, 8)]$; detections $=[(252, 70, 10, 17),(252, 70, 18, 25),(30, 40, 0, 3)]$.\n- Case 5 (no detections): manual $=[(180, 100, 0, 10)]$; detections $=[]$.\n\nRequired outputs for each test case:\n- Precision $P$ as a real number rounded to three decimals.\n- Recall $R$ as a real number rounded to three decimals.\n- F1 score $F1$ as a real number rounded to three decimals.\n- Mean absolute onset error $\\overline{|e_s|}$ in days as a real number rounded to two decimals (unit: days).\n- Mean absolute cessation error $\\overline{|e_e|}$ in days as a real number rounded to two decimals (unit: days).\n- The integer count $FP$.\n- The integer count $FN$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of per-case result lists, with no spaces. For example, the output must look like\n$[ [P_1,R_1,F1_1,\\overline{|e_s|}_1,\\overline{|e_e|}_1,FP_1,FN_1], [P_2,R_2,F1_2,\\overline{|e_s|}_2,\\overline{|e_e|}_2,FP_2,FN_2], \\dots ]$\nbut without spaces. Concretely, your program should print a single line in the exact format\n\"[case1_list,case2_list,case3_list,case4_list,case5_list]\".",
            "solution": "The problem requires the validation of an automated atmospheric blocking detection algorithm against a manual reference catalog. The validation involves a multi-step procedure: quantifying the similarity of individual events, establishing an optimal one-to-one matching between manual and detected events, and finally, computing a suite of standard performance metrics. The entire process is grounded in set theory and combinatorial optimization.\n\nThe fundamental algorithm proceeds as follows:\n1.  For every possible pair of a manual event and a detected event, calculate their spatial and temporal similarity.\n2.  Use these similarities to compute a combined score, which is non-zero only for pairs that meet predefined admissibility thresholds.\n3.  Solve the assignment problem to find a one-to-one matching between manual and detected events that maximizes the total combined similarity score.\n4.  Based on this optimal matching, classify events into true positives, false positives, and false negatives.\n5.  Compute the final skill scores (precision, recall, F1) and timing accuracy measures (mean absolute onset and cessation errors).\n\nLet a manual event be $m_i = (\\ell_{m,i}, w_{m,i}, t_{s,m,i}, t_{e,m,i})$ and a detected event be $d_j = (\\ell_{d,j}, w_{d,j}, t_{s,d,j}, t_{e,d,j})$.\n\n**Step 1: Spatial Jaccard Similarity ($r_s$)**\n\nThe longitudinal extent of an event is an arc on a circle of circumference $C = 360^{\\circ}$. The similarity between two arcs, $A_m = A(\\ell_m, w_m)$ and $A_d = A(\\ell_d, w_d)$, is given by the Jaccard index $r_s = J(A_m, A_d) = \\frac{|A_m \\cap A_d|}{|A_m \\cup A_d|}$. The measure $|\\cdot|$ corresponds to the length of the arc in degrees.\n\nThe length of the union is given by $|A_m \\cup A_d| = |A_m| + |A_d| - |A_m \\cap A_d| = w_m + w_d - |A_m \\cap A_d|$.\nThe primary challenge is computing the intersection length $|A_m \\cap A_d|$ on a periodic domain. For two arcs defined by their center and width, the intersection length can be calculated elegantly. Let $\\Delta\\ell = |\\ell_m - \\ell_d|$ be the absolute difference in their longitude centers. The shortest distance between the centers on the circle is $\\Delta\\ell_{\\text{circ}} = \\min(\\Delta\\ell, 360 - \\Delta\\ell)$. Two arcs overlap if and only if this distance is less than the average of their widths, i.e., $\\Delta\\ell_{\\text{circ}}  (w_m + w_d)/2$. The length of their intersection is the extent of this overlap:\n$$\n|A_m \\cap A_d| = \\max\\left(0, \\frac{w_m + w_d}{2} - \\Delta\\ell_{\\text{circ}}\\right)\n$$\nWith this, the spatial Jaccard similarity is:\n$$\nr_s = \\frac{|A_m \\cap A_d|}{w_m + w_d - |A_m \\cap A_d|}\n$$\nIf the denominator is zero (which occurs only if $w_m=w_d=0$, not possible here), $r_s$ would be undefined but the case is excluded by $w > 0$.\n\n**Step 2: Temporal Jaccard Similarity ($r_t$)**\n\nThe temporal support is an inclusive interval of integers (days). Let $I_m = I(t_{s,m}, t_{e,m})$ and $I_d = I(t_{s,d}, t_{e,d})$. The measure is cardinality. The length of an interval $I(t_s, t_e)$ is $|I| = t_e - t_s + 1$. The intersection of two intervals is $I_m \\cap I_d = [\\max(t_{s,m}, t_{s,d}), \\min(t_{e,m}, t_{e,d})]$. Its length is:\n$$\n|I_m \\cap I_d| = \\max\\left(0, \\min(t_{e,m}, t_{e,d}) - \\max(t_{s,m}, t_{s,d}) + 1\\right)\n$$\nThe temporal Jaccard similarity is then:\n$$\nr_t = \\frac{|I_m \\cap I_d|}{|I_m| + |I_d| - |I_m \\cap I_d|}\n$$\nIf the denominator is zero (i.e., both intervals have zero length, which cannot happen as $t_s \\le t_e$), $r_t$ is taken as $0$.\n\n**Step 3: Combined Similarity and Optimal Matching**\n\nFor each pair $(m_i, d_j)$, we compute $r_{s,ij}$ and $r_{t,ij}$. The pair is admissible if $r_{s,ij} \\ge \\tau_s$ and $r_{t,ij} \\ge \\tau_t$, where the problem specifies $\\tau_s = 0.5$ and $\\tau_t = 0.5$. The combined similarity score $S_{ij}$ is defined as:\n$$\nS_{ij} = \\begin{cases} r_{s,ij} \\times r_{t,ij}  \\text{if } r_{s,ij} \\ge \\tau_s \\text{ and } r_{t,ij} \\ge \\tau_t \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThis forms a similarity matrix $S$ of size $N_m \\times N_d$, where $N_m$ and $N_d$ are the number of manual and detected events, respectively. The goal is to find a set of one-to-one pairs $(i, j)$ that maximizes the total sum $\\sum S_{ij}$. This is a classic assignment problem, also known as maximum weight bipartite matching. It can be solved by converting it into a minimization problem by defining a cost matrix $C_{ij} = -S_{ij}$. Standard algorithms, such as the Hungarian algorithm, can then find the assignment that minimizes the total cost, which is equivalent to maximizing the total similarity. This is implemented in `scipy.optimize.linear_sum_assignment`.\n\n**Step 4: Calculation of Performance Metrics**\n\nThe solution to the assignment problem provides a set of matched pairs. A pair $(m_i, d_j)$ from this optimal matching is considered a true positive ($TP$) only if its similarity score $S_{ij}$ is greater than $0$. This intrinsically enforces the admissibility criteria.\n- **True Positives ($TP$)**: The number of matched pairs with $S_{ij} > 0$.\n- **False Negatives ($FN$)**: The number of manual events that are not part of a true positive pair. $FN = N_m - TP$.\n- **False Positives ($FP$)**: The number of detected events that are not part of a true positive pair. $FP = N_d - TP$.\n\nWith these counts, the skill scores are calculated:\n- **Precision ($P$)**: $P = \\frac{TP}{TP+FP}$. If $TP+FP=0$, $P=0$.\n- **Recall ($R$)**: $R = \\frac{TP}{TP+FN}$. If $TP+FN=0$, $R=0$.\n- **F1 Score ($F1$)**: $F1 = \\frac{2PR}{P+R}$. If $P+R=0$, $F1=0$.\n\nFor each true positive pair $(m_i, d_j)$, the onset and cessation timing errors are calculated:\n- Onset error: $e_{s,ij} = t_{s,d,j} - t_{s,m,i}$\n- Cessation error: $e_{e,ij} = t_{e,d,j} - t_{e,m,i}$\n\nThe final timing accuracy measures are the mean absolute errors over all $TP$ pairs:\n- Mean absolute onset error: $\\overline{|e_s|} = \\frac{1}{TP} \\sum_{k=1}^{TP} |e_{s,k}|$\n- Mean absolute cessation error: $\\overline{|e_e|} = \\frac{1}{TP} \\sum_{k=1}^{TP} |e_{e,k}|$\nIf $TP=0$, both mean absolute errors are reported as $0$. The final numerical results are rounded as specified in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        (\n            [(30, 60, 2, 10), (200, 50, 15, 22)],\n            [(35, 70, 3, 11), (205, 40, 16, 20)],\n        ),\n        # Case 2\n        (\n            [(350, 40, 5, 12)],\n            [(355, 50, 6, 13), (150, 30, 7, 10)],\n        ),\n        # Case 3\n        (\n            [(100, 60, 0, 9)],\n            [(102, 70, 0, 9), (98, 50, 2, 8)],\n        ),\n        # Case 4\n        (\n            [(250, 80, 10, 25), (300, 50, 5, 8)],\n            [(252, 70, 10, 17), (252, 70, 18, 25), (30, 40, 0, 3)],\n        ),\n        # Case 5\n        (\n            [(180, 100, 0, 10)],\n            [],\n        ),\n    ]\n\n    # Thresholds as defined in the problem\n    tau_s = 0.5\n    tau_t = 0.5\n\n    all_results = []\n    for manual_catalog, detected_events in test_cases:\n        result = _solve_single_case(manual_catalog, detected_events, tau_s, tau_t)\n        all_results.append(result)\n\n    # Format the final output string exactly as required\n    formatted_results = [\n        f\"[{res[0]:.3f},{res[1]:.3f},{res[2]:.3f},{res[3]:.2f},{res[4]:.2f},{res[5]},{res[6]}]\"\n        for res in all_results\n    ]\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _solve_single_case(manual_catalog, detected_events, tau_s, tau_t):\n    \"\"\"\n    Solves a single validation case.\n    \"\"\"\n    num_manual = len(manual_catalog)\n    num_detected = len(detected_events)\n\n    if num_manual == 0 or num_detected == 0:\n        tp = 0\n        fp = num_detected\n        fn = num_manual\n        p = 0.0 if (tp + fp) == 0 else tp / (tp + fp)\n        r = 0.0 if (tp + fn) == 0 else tp / (tp + fn)\n        f1 = 0.0 if (p + r) == 0 else 2 * p * r / (p + r)\n        mean_abs_onset_err = 0.0\n        mean_abs_cessation_err = 0.0\n        return [p, r, f1, mean_abs_onset_err, mean_abs_cessation_err, fp, fn]\n\n    similarity_matrix = np.zeros((num_manual, num_detected))\n\n    for i, m_event in enumerate(manual_catalog):\n        for j, d_event in enumerate(detected_events):\n            # Decompose events\n            l_m, w_m, ts_m, te_m = m_event\n            l_d, w_d, ts_d, te_d = d_event\n\n            # Spatial Jaccard\n            delta_l = abs(l_m - l_d)\n            delta_l_circ = min(delta_l, 360.0 - delta_l)\n            intersection_s = max(0.0, (w_m + w_d) / 2.0 - delta_l_circ)\n            union_s = w_m + w_d - intersection_s\n            r_s = intersection_s / union_s if union_s > 0 else 0.0\n            \n            # Temporal Jaccard\n            len_m = te_m - ts_m + 1\n            len_d = te_d - ts_d + 1\n            intersection_t = max(0, min(te_m, te_d) - max(ts_m, ts_d) + 1)\n            union_t = len_m + len_d - intersection_t\n            r_t = intersection_t / union_t if union_t > 0 else 0.0\n\n            # Combined score with admissibility\n            if r_s >= tau_s and r_t >= tau_t:\n                similarity_matrix[i, j] = r_s * r_t\n            else:\n                similarity_matrix[i, j] = 0.0\n\n    # Solve assignment problem (maximize similarity = minimize negative similarity)\n    cost_matrix = -similarity_matrix\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n\n    # Calculate metrics from matching\n    tp = 0\n    onset_errors = []\n    cessation_errors = []\n\n    for i, j in zip(row_ind, col_ind):\n        if similarity_matrix[i, j] > 0:\n            tp += 1\n            m_event = manual_catalog[i]\n            d_event = detected_events[j]\n            \n            onset_error = d_event[2] - m_event[2]\n            cessation_error = d_event[3] - m_event[3]\n            onset_errors.append(abs(onset_error))\n            cessation_errors.append(abs(cessation_error))\n\n    fp = num_detected - tp\n    fn = num_manual - tp\n\n    # Skill scores with conventions for zero denominators\n    p = 0.0 if (tp + fp) == 0 else tp / (tp + fp)\n    r = 0.0 if (tp + fn) == 0 else tp / (tp + fn)\n    f1 = 0.0 if (p + r) == 0 else 2 * p * r / (p + r)\n\n    # Timing accuracy with convention for no matches\n    mean_abs_onset_err = np.mean(onset_errors) if tp > 0 else 0.0\n    mean_abs_cessation_err = np.mean(cessation_errors) if tp > 0 else 0.0\n    \n    return [p, r, f1, mean_abs_onset_err, mean_abs_cessation_err, fp, fn]\n\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}