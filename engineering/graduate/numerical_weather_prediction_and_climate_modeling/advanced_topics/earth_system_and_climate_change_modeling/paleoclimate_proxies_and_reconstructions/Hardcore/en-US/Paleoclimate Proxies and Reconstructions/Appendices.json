{
    "hands_on_practices": [
        {
            "introduction": "Before we can reconstruct past climates, we must first establish a reliable relationship between a proxy measurement and a climate variable like temperature through calibration. A critical challenge is that our proxy measurements are themselves subject to analytical error, a complication that standard regression techniques ignore. This exercise  introduces the errors-in-variables framework to correctly estimate a calibration slope, allowing you to quantify the \"attenuation bias\"—a systematic underestimation of climate sensitivity—that arises if this proxy error is neglected.",
            "id": "4073673",
            "problem": "A paleoclimate calibration seeks to relate a marine sediment geochemical proxy to instrumental sea surface temperature using an errors-in-variables (EIV) framework. Let $Y_t$ denote the instrumental temperature (in degrees Celsius) and let $X_t$ denote the latent, true proxy state (in dimensionless proxy units). Observations of the proxy are contaminated by analytical measurement error so that the observed proxy is $X_t^{\\ast} = X_t + u_t$, where $u_t$ is a zero-mean, independent measurement error with constant variance. The physical calibration model linking temperature and the true proxy is $Y_t = \\alpha + \\beta X_t + \\varepsilon_t$, where $\\varepsilon_t$ is a zero-mean, independent noise term, and $(u_t)$ is independent of $(\\varepsilon_t)$ and $(X_t)$. Over a modern calibration interval, the following summary statistics are computed from paired observations $\\{(Y_t, X_t^{\\ast})\\}_{t=1}^{n}$:\n- The sample covariance between temperature and the observed proxy is $C_{Y X^{\\ast}} = 0.84$ (degrees Celsius times proxy unit).\n- The sample variance of the observed proxy is $V_{X^{\\ast}} = 1.50$ (proxy units squared).\n- Independent replicate analyses provide an estimate of the measurement error variance, $\\sigma_u^2 = 0.60$ (proxy units squared).\n\nUnder the model assumptions above, use an errors-in-variables approach grounded in first principles to estimate the calibration slope $\\beta$ and compute the attenuation bias that would result if the measurement error in $X_t^{\\ast}$ were ignored when regressing $Y_t$ on $X_t^{\\ast}$. Express the slope in degrees Celsius per proxy unit and the bias in degrees Celsius per proxy unit. Round both numbers to four significant figures. Provide your final answer as a row matrix whose first entry is the EIV slope estimate and whose second entry is the attenuation bias when measurement error is ignored.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically sound, and well-posed.\n\n### Step 1: Extract Givens\n-   The physical model relating true temperature $Y_t$ to the true latent proxy state $X_t$ is: $Y_t = \\alpha + \\beta X_t + \\varepsilon_t$.\n-   The measurement model for the observed proxy $X_t^{\\ast}$ is: $X_t^{\\ast} = X_t + u_t$.\n-   $\\varepsilon_t$ is a zero-mean, independent noise term.\n-   $u_t$ is a zero-mean, independent measurement error with constant variance, $\\sigma_u^2$.\n-   The error sequences $(\\varepsilon_t)$ and $(u_t)$ are independent of each other and of the true proxy state sequence $(X_t)$.\n-   Sample covariance between temperature and the observed proxy: $C_{Y X^{\\ast}} = 0.84$.\n-   Sample variance of the observed proxy: $V_{X^{\\ast}} = 1.50$.\n-   Variance of the measurement error: $\\sigma_u^2 = 0.60$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem describes a standard errors-in-variables (EIV) regression model. This is a fundamental statistical framework used widely in science and engineering for calibration problems where the predictor variable is measured with error. The model is mathematically and scientifically sound.\n-   **Well-Posed:** The problem provides all necessary model assumptions and summary statistics ($C_{Y X^{\\ast}}$, $V_{X^{\\ast}}$, $\\sigma_u^2$) to derive a unique estimate for the slope parameter $\\beta$ and the associated attenuation bias.\n-   **Objective:** The problem is stated using precise, standard terminology from statistics and regression analysis. There is no ambiguity or subjective language.\n\n### Step 3: Verdict and Action\n-   The problem is deemed **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe goal is to estimate the true calibration slope $\\beta$ from the model $Y_t = \\alpha + \\beta X_t + \\varepsilon_t$, given that we only observe $Y_t$ and a noisy version of the predictor, $X_t^{\\ast} = X_t + u_t$.\n\nLet us first establish the relationships between the population moments (variances and covariances) of the observed variables and the underlying model parameters. We denote the variance of the true proxy state as $\\sigma_X^2 = \\text{Var}(X_t)$.\n\nThe population covariance between the observed temperature $Y_t$ and the observed proxy $X_t^{\\ast}$ is:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\text{Cov}(\\alpha + \\beta X_t + \\varepsilon_t, X_t + u_t) $$\nUsing the bilinearity of the covariance operator and the independence assumptions ($X_t$, $\\varepsilon_t$, and $u_t$ are mutually independent), this simplifies to:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\text{Cov}(\\beta X_t, X_t) + \\text{Cov}(\\beta X_t, u_t) + \\text{Cov}(\\varepsilon_t, X_t) + \\text{Cov}(\\varepsilon_t, u_t) $$\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\beta \\text{Var}(X_t) + \\beta \\text{Cov}(X_t, u_t) + \\text{Cov}(\\varepsilon_t, X_t) + \\text{Cov}(\\varepsilon_t, u_t) $$\nSince $\\text{Cov}(X_t, u_t)=0$, $\\text{Cov}(\\varepsilon_t, X_t)=0$, and $\\text{Cov}(\\varepsilon_t, u_t)=0$, we are left with:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\beta \\text{Var}(X_t) = \\beta \\sigma_X^2 $$\n\nThe variance of the observed proxy $X_t^{\\ast}$ is:\n$$ \\text{Var}(X_t^{\\ast}) = \\text{Var}(X_t + u_t) $$\nSince $X_t$ and $u_t$ are independent, the variance of their sum is the sum of their variances:\n$$ \\text{Var}(X_t^{\\ast}) = \\text{Var}(X_t) + \\text{Var}(u_t) = \\sigma_X^2 + \\sigma_u^2 $$\n\nIf one were to ignore the measurement error $u_t$ and perform an ordinary least squares (OLS) regression of $Y_t$ on $X_t^{\\ast}$, the slope estimator, which we denote $\\hat{\\beta}_{OLS}$, would be calculated as:\n$$ \\hat{\\beta}_{OLS} = \\frac{C_{Y X^{\\ast}}}{V_{X^{\\ast}}} $$\nIn the large sample limit, this estimator converges to:\n$$ \\beta_{OLS} = \\frac{\\text{Cov}(Y_t, X_t^{\\ast})}{\\text{Var}(X_t^{\\ast})} = \\frac{\\beta \\sigma_X^2}{\\sigma_X^2 + \\sigma_u^2} = \\beta \\left( \\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_u^2} \\right) $$\nSince $\\sigma_X^2 > 0$ and $\\sigma_u^2 > 0$, the factor $\\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_u^2}$ is always less than $1$. This demonstrates that the OLS estimate is biased towards zero, a phenomenon known as attenuation bias.\n\nTo obtain an unbiased estimate of $\\beta$, we must correct for this effect. The EIV approach uses the knowledge of the measurement error variance $\\sigma_u^2$. From the variance equation for $X_t^{\\ast}$, we can express the unknown variance of the true proxy as:\n$$ \\sigma_X^2 = \\text{Var}(X_t^{\\ast}) - \\sigma_u^2 $$\nSubstituting this into the covariance equation:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\beta (\\text{Var}(X_t^{\\ast}) - \\sigma_u^2) $$\nSolving for the true slope $\\beta$ gives the EIV estimator, $\\hat{\\beta}_{EIV}$, by replacing population moments with their sample estimates:\n$$ \\hat{\\beta}_{EIV} = \\frac{C_{Y X^{\\ast}}}{V_{X^{\\ast}} - \\sigma_u^2} $$\nUsing the provided numerical values:\n$$ C_{Y X^{\\ast}} = 0.84 $$\n$$ V_{X^{\\ast}} = 1.50 $$\n$$ \\sigma_u^2 = 0.60 $$\nThe EIV slope estimate is:\n$$ \\hat{\\beta}_{EIV} = \\frac{0.84}{1.50 - 0.60} = \\frac{0.84}{0.90} = \\frac{14}{15} \\approx 0.933333... $$\nRounding to four significant figures, $\\hat{\\beta}_{EIV} = 0.9333$ degrees Celsius per proxy unit.\n\nNext, we compute the attenuation bias that results from using the naive OLS approach. This bias is the difference between the OLS slope estimate and the true slope, for which we use our best estimate, $\\hat{\\beta}_{EIV}$. The OLS slope estimate is:\n$$ \\hat{\\beta}_{OLS} = \\frac{C_{Y X^{\\ast}}}{V_{X^{\\ast}}} = \\frac{0.84}{1.50} = 0.56 $$\nThe attenuation bias is calculated as:\n$$ \\text{Bias} = \\hat{\\beta}_{OLS} - \\hat{\\beta}_{EIV} $$\n$$ \\text{Bias} = 0.56 - \\frac{14}{15} = \\frac{14}{25} - \\frac{14}{15} = 14 \\left( \\frac{1}{25} - \\frac{1}{15} \\right) = 14 \\left( \\frac{3 - 5}{75} \\right) = 14 \\left( \\frac{-2}{75} \\right) = -\\frac{28}{75} \\approx -0.373333... $$\nRounding to four significant figures, the attenuation bias is $-0.3733$ degrees Celsius per proxy unit. The negative sign indicates that the naive OLS regression underestimates the true magnitude of the slope.\n\nThe final answer requires the EIV slope estimate and the attenuation bias, presented as a row matrix.\nFirst entry: $\\hat{\\beta}_{EIV} = 0.9333$.\nSecond entry: $\\text{Bias} = -0.3733$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.9333 & -0.3733\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once proxies are calibrated, the central task of reconstruction begins: synthesizing sparse and indirect proxy data into a coherent climate history. This is a classic inverse problem, often ill-posed, which requires regularization to produce a stable and physically meaningful solution. This practice  provides a concrete, computational exercise in Tikhonov regularization, a cornerstone method that finds an optimal balance between fitting the proxy data and enforcing a desired property, such as smoothness, on the solution.",
            "id": "4073786",
            "problem": "Consider a linear paleoclimate proxy forward model where a latent climate state $x \\in \\mathbb{R}^{n}$ (dimensionless temperature anomalies) is mapped to proxy observations $y \\in \\mathbb{R}^{m}$ (dimensionless proxy anomalies) through a known smoothing operator $H \\in \\mathbb{R}^{m \\times n}$, with additive deterministic noise $\\epsilon \\in \\mathbb{R}^{m}$. All quantities are dimensionless. Angles appearing inside trigonometric functions must be interpreted in radians. The forward model is defined by $y = H x + \\epsilon$. Your task is to compute a regularized reconstruction $\\hat{x}$ for specified values of the regularization parameter $\\lambda$, and then evaluate reconstruction error metrics. The reconstruction should be obtained by minimizing the quadratic Tikhonov functional $J(x) = \\lVert H x - y \\rVert_2^2 + \\lambda^2 \\lVert L x \\rVert_2^2$, where $L$ is a first-difference operator that penalizes roughness in $x$.\n\nUse the following scientifically grounded and fully specified setup:\n\n- State dimension: $n = 64$. Observation dimension: $m = 48$.\n- Discrete time indices for the state: $t_j = j$ for $j = 0, 1, \\dots, n-1$.\n- Discrete time indices for the proxies: $p_i = i \\cdot \\frac{n}{m}$ for $i = 0, 1, \\dots, m-1$.\n- Smoothing scale (Gaussian kernel standard deviation): $\\ell = 3.0$.\n- The forward operator $H$ has entries\n$$\nH_{i j} = \\frac{\\exp\\!\\left(-\\frac{(p_i - t_j)^2}{2 \\ell^2}\\right)}{\\sum_{k=0}^{n-1} \\exp\\!\\left(-\\frac{(p_i - t_k)^2}{2 \\ell^2}\\right)} \\quad \\text{for } i = 0, \\dots, m-1,\\; j = 0, \\dots, n-1,\n$$\nthat is, each row of $H$ applies a normalized Gaussian-weighted average over $x$ centered at $p_i$.\n- The first-difference regularization operator $L \\in \\mathbb{R}^{(n-1) \\times n}$ is given by\n$$\n(L x)_k = x_{k+1} - x_k \\quad \\text{for } k = 0, 1, \\dots, n-2.\n$$\nEquivalently, $L_{k,k} = -1$, $L_{k,k+1} = 1$, and all other entries are $0$.\n- The true state $x^{\\star} \\in \\mathbb{R}^n$ is defined by a sum of sinusoids and cosines:\n$$\nx^{\\star}_j = \\sin\\!\\left(\\frac{2 \\pi \\cdot 3 \\cdot j}{n}\\right) + 0.6 \\cos\\!\\left(\\frac{2 \\pi \\cdot 9 \\cdot j}{n}\\right) + 0.3 \\sin\\!\\left(\\frac{2 \\pi \\cdot 15 \\cdot j}{n}\\right), \\quad j = 0, 1, \\dots, n-1.\n$$\n- The deterministic observation noise is\n$$\n\\epsilon_i = 0.03 \\left[\\sin\\!\\left(0.37 \\cdot i \\right) + 0.5 \\cos\\!\\left(0.13 \\cdot i \\right)\\right], \\quad i = 0, 1, \\dots, m-1.\n$$\n- Observations are given by $y = H x^{\\star} + \\epsilon$.\n\nFor each specified regularization strength $\\lambda$, compute the regularized solution $\\hat{x}$ that minimizes $J(x)$ and then evaluate the following error metrics:\n- The relative reconstruction error\n$$\nE_{\\mathrm{rel}}(\\hat{x}) = \\frac{\\lVert \\hat{x} - x^{\\star} \\rVert_2}{\\lVert x^{\\star} \\rVert_2}.\n$$\n- The normalized root-mean-square error (NRMSE) defined by\n$$\nE_{\\mathrm{nrmse}}(\\hat{x}) = \\frac{\\sqrt{\\frac{1}{n} \\sum_{j=0}^{n-1} \\left( \\hat{x}_j - x^{\\star}_j \\right)^2}}{\\sqrt{\\frac{1}{n} \\sum_{j=0}^{n-1} \\left( x^{\\star}_j - \\bar{x}^{\\star} \\right)^2}},\n$$\nwhere $\\bar{x}^{\\star} = \\frac{1}{n} \\sum_{j=0}^{n-1} x^{\\star}_j$ is the mean of $x^{\\star}$.\n- The data misfit per observation\n$$\nE_{\\mathrm{misfit}}(\\hat{x}) = \\frac{\\lVert H \\hat{x} - y \\rVert_2}{\\sqrt{m}}.\n$$\n- The roughness ratio\n$$\nE_{\\mathrm{rough}}(\\hat{x}) = \\frac{\\lVert L \\hat{x} \\rVert_2}{\\lVert L x^{\\star} \\rVert_2}.\n$$\n\nTest Suite:\n- Case $1$: $\\lambda = 10^{-6}$.\n- Case $2$: $\\lambda = 10^{-2}$.\n- Case $3$: $\\lambda = 5 \\cdot 10^{-1}$.\n\nYour program must:\n- Construct $H$, $L$, $x^{\\star}$, $\\epsilon$, and $y$ exactly as defined above.\n- For each test case, compute a numerically accurate minimizer $\\hat{x}$ of $J(x)$ consistent with the definitions.\n- Compute the four metrics for each case in the order $\\left[E_{\\mathrm{rel}}, E_{\\mathrm{nrmse}}, E_{\\mathrm{misfit}}, E_{\\mathrm{rough}}\\right]$.\n- Produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the four metrics rounded to six decimal places. The final output format must be exactly\n$$\n\\left[ [e_{11}, e_{12}, e_{13}, e_{14}], [e_{21}, e_{22}, e_{23}, e_{24}], [e_{31}, e_{32}, e_{33}, e_{34}] \\right]\n$$\nprinted as a single line with no spaces, for example\n$$\n\\left[[0.123456,0.234567,0.345678,0.456789],[\\dots],[\\dots]\\right].\n$$",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded, mathematically well-posed, objective, and self-contained. It presents a standard Tikhonov regularization problem for a linear inverse model, a common task in scientific computing and data analysis, with all parameters, operators, and functions explicitly defined. There are no contradictions, ambiguities, or falsehoods.\n\nThe task is to find the state vector $\\hat{x} \\in \\mathbb{R}^n$ that minimizes the Tikhonov functional $J(x)$:\n$$\nJ(x) = \\lVert H x - y \\rVert_2^2 + \\lambda^2 \\lVert L x \\rVert_2^2\n$$\nwhere $H \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $y \\in \\mathbb{R}^m$ is the observation vector, $L \\in \\mathbb{R}^{(n-1) \\times n}$ is the regularization operator, and $\\lambda$ is the regularization parameter. All quantities are provided in the problem description.\n\nThe functional $J(x)$ is a quadratic function of $x$. A unique minimizer $\\hat{x}$ exists and can be found by computing the gradient of $J(x)$ with respect to $x$ and setting it to zero. First, we expand the squared Euclidean norms using the transpose:\n$$\nJ(x) = (H x - y)^T (H x - y) + \\lambda^2 (L x)^T (L x)\n$$\nExpanding the products gives:\n$$\nJ(x) = (x^T H^T - y^T)(H x - y) + \\lambda^2 x^T L^T L x\n$$\n$$\nJ(x) = x^T H^T H x - x^T H^T y - y^T H x + y^T y + \\lambda^2 x^T L^T L x\n$$\nSince $y^T H x$ is a scalar, it is equal to its transpose, $(y^T H x)^T = x^T H^T y$. Thus, we can combine the cross-terms:\n$$\nJ(x) = x^T (H^T H + \\lambda^2 L^T L) x - 2 y^T H x + y^T y\n$$\nThe gradient of $J(x)$ with respect to the vector $x$ is:\n$$\n\\nabla_x J(x) = \\frac{\\partial}{\\partial x} \\left( x^T (H^T H + \\lambda^2 L^T L) x - 2 (H^T y)^T x + y^T y \\right)\n$$\nUsing standard rules of vector calculus, specifically that $\\nabla_x(x^T A x) = 2Ax$ for symmetric $A$ and $\\nabla_x(b^T x) = b$, we get:\n$$\n\\nabla_x J(x) = 2 (H^T H + \\lambda^2 L^T L) x - 2 H^T y\n$$\nSetting the gradient to the zero vector, $\\nabla_x J(\\hat{x}) = 0$, yields the system of linear equations known as the normal equations for the regularized problem:\n$$\n(H^T H + \\lambda^2 L^T L) \\hat{x} = H^T y\n$$\nThis is a linear system of the form $A\\hat{x} = b$, where the matrix $A = H^T H + \\lambda^2 L^T L$ is an $n \\times n$ matrix and the vector $b = H^T y$ is of size $n \\times 1$. For any $\\lambda > 0$, the matrix $A$ is symmetric and positive definite, which guarantees that a unique, stable solution $\\hat{x}$ exists. This solution can be found numerically by solving the linear system.\n\nThe computational procedure is as follows:\n$1$. Construct the vectors and matrices as specified: the state time indices $t_j$, proxy time indices $p_i$, the true state $x^{\\star}$, the noise vector $\\epsilon$, the forward operator $H$, and the regularization operator $L$.\n$2$. Compute the observation vector $y = H x^{\\star} + \\epsilon$.\n$3$. For each given value of the regularization parameter $\\lambda$ from the test suite:\n    a. Form the $n \\times n$ matrix $A = H^T H + \\lambda^2 L^T L$.\n    b. Form the $n \\times 1$ vector $b = H^T y$.\n    c. Solve the linear system $A\\hat{x} = b$ for the reconstructed state $\\hat{x}$.\n    d. Using the computed $\\hat{x}$ and the known true state $x^{\\star}$, calculate the four specified error metrics: the relative reconstruction error $E_{\\mathrm{rel}}(\\hat{x})$, the normalized root-mean-square error $E_{\\mathrm{nrmse}}(\\hat{x})$, the data misfit per observation $E_{\\mathrm{misfit}}(\\hat{x})$, and the roughness ratio $E_{\\mathrm{rough}}(\\hat{x})$.\n$4$. The final results for each test case are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov-regularized paleoclimate reconstruction problem\n    and computes the specified error metrics for different regularization strengths.\n    \"\"\"\n    \n    # Define the test cases (regularization parameters) from the problem statement.\n    test_cases = [1e-6, 1e-2, 5e-1]\n\n    # --- Step 1: Construct all specified model components ---\n\n    # Define dimensions and parameters\n    n = 64  # State dimension\n    m = 48  # Observation dimension\n    l_scale = 3.0  # Smoothing scale\n\n    # Define discrete time indices\n    t = np.arange(n, dtype=np.float64)\n    p = np.arange(m, dtype=np.float64) * (n / m)\n\n    # Construct the forward operator H\n    # Using broadcasting: p[:, None] is (m, 1), t[None, :] is (1, n)\n    h_numerator = np.exp(-(p[:, None] - t[None, :])**2 / (2 * l_scale**2))\n    h_denominator = np.sum(h_numerator, axis=1, keepdims=True)\n    H = h_numerator / h_denominator\n\n    # Construct the first-difference regularization operator L\n    L = np.zeros((n - 1, n), dtype=np.float64)\n    rows = np.arange(n - 1)\n    L[rows, rows] = -1.0\n    L[rows, rows + 1] = 1.0\n\n    # Construct the true state x_star\n    j_indices = np.arange(n, dtype=np.float64)\n    x_star = (np.sin(2 * np.pi * 3 * j_indices / n) +\n              0.6 * np.cos(2 * np.pi * 9 * j_indices / n) +\n              0.3 * np.sin(2 * np.pi * 15 * j_indices / n))\n\n    # Construct the deterministic observation noise epsilon\n    i_indices = np.arange(m, dtype=np.float64)\n    epsilon = 0.03 * (np.sin(0.37 * i_indices) + 0.5 * np.cos(0.13 * i_indices))\n\n    # Construct the observations y\n    y = H @ x_star + epsilon\n\n    # --- Step 2: Pre-compute components for efficiency ---\n\n    # Pre-calculate norms of true state components for metric calculations\n    norm_x_star = np.linalg.norm(x_star)\n    # Denominator for NRMSE: ||x* - mean(x*)||_2\n    norm_x_star_centered = np.linalg.norm(x_star - np.mean(x_star))\n    norm_Lx_star = np.linalg.norm(L @ x_star)\n    \n    # Pre-compute matrix products for the normal equations\n    HTH = H.T @ H\n    LTL = L.T @ L\n    HTy = H.T @ y\n\n    all_case_results = []\n    \n    # --- Step 3: Loop through test cases, solve, and evaluate metrics ---\n\n    for lam in test_cases:\n        # Form the system matrix for the normal equations\n        A = HTH + lam**2 * LTL\n        \n        # Solve the linear system A * x_hat = HTy for x_hat\n        x_hat = np.linalg.solve(A, HTy)\n\n        # Calculate the four error metrics\n        # 1. Relative reconstruction error\n        E_rel = np.linalg.norm(x_hat - x_star) / norm_x_star\n\n        # 2. Normalized root-mean-square error (NRMSE)\n        # NRMSE = RMSE(x_hat, x_star) / std(x_star)\n        # This is equivalent to norm(x_hat - x_star) / norm(x_star - mean(x_star))\n        E_nrmse = np.linalg.norm(x_hat - x_star) / norm_x_star_centered\n        \n        # 3. Data misfit per observation\n        E_misfit = np.linalg.norm(H @ x_hat - y) / np.sqrt(m)\n        \n        # 4. Roughness ratio\n        E_rough = np.linalg.norm(L @ x_hat) / norm_Lx_star\n\n        all_case_results.append([E_rel, E_nrmse, E_misfit, E_rough])\n\n    # --- Step 4: Format and print the final output ---\n\n    # Format each list of metrics into a comma-separated string rounded to 6 decimal places\n    case_strings = [f\"[{','.join(f'{value:.6f}' for value in metrics)}]\" for metrics in all_case_results]\n    \n    # Join all case strings into the final required format\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A robust reconstruction requires careful interpretation, as hidden biases within the proxy system can distort the climatic signal. Many proxies do not respond to climate uniformly throughout the year but have a \"seasonal bias,\" for instance, by primarily recording summer conditions. This synthetic experiment  reveals how the interaction between the seasonal cycle of climate and the seasonal sensitivity of a proxy can systematically corrupt the reconstructed annual mean, building vital intuition for evaluating the fidelity of paleoclimate records.",
            "id": "4073698",
            "problem": "Consider monthly resolved paleoclimate proxies whose response to surface air temperature is modeled by a linear transfer function with a known seasonal bias. Let the true monthly temperature series over one climatological year be represented by $T_j = T_0 + A \\cos(\\theta_j - \\phi)$ for $j \\in \\{0,1,\\dots,11\\}$, where $\\theta_j = 2\\pi j / 12$ are monthly angles in radians, $T_0$ is the annual mean temperature in degrees Celsius, $A$ is the seasonal amplitude in degrees Celsius, and $\\phi$ is the phase of the seasonal cycle in radians. The proxy sensitivity has a seasonal bias modeled as $H_j = H_{\\mathrm{bar}} \\left(1 + s \\cos(\\theta_j - \\psi)\\right)$, where $H_{\\mathrm{bar}}$ is the annual mean sensitivity (dimensionless), $s \\in [0,1)$ is the sensitivity seasonal amplitude (dimensionless), and $\\psi$ is the sensitivity phase in radians. Assume the proxy is monthly resolved and follows the forward model $P_j = H_j T_j$ without additional noise.\n\nA simple (but biased) annual mean reconstruction uses the naive estimator that assumes constant sensitivity equal to $H_{\\mathrm{bar}}$, defined as $\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} \\frac{P_j}{H_{\\mathrm{bar}}}$. The true annual mean is $T_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} T_j$. You are asked to quantify the impact of seasonality in $H_j$ on the annual mean reconstruction by computing the expected bias $B = \\hat{T}_{\\mathrm{ann}} - T_{\\mathrm{ann}}$ in degrees Celsius for a given parameter set $(T_0, A, H_{\\mathrm{bar}}, s, \\phi, \\psi)$.\n\nYour program must implement this synthetic experiment numerically as follows:\n- Construct $T_j$ and $H_j$ for $j \\in \\{0,1,\\dots,11\\}$ as specified above, with angles in radians.\n- Compute $P_j = H_j T_j$.\n- Compute $\\hat{T}_{\\mathrm{ann}}$ and $T_{\\mathrm{ann}}$, and then compute $B$ in degrees Celsius.\n- Express all angles in radians, and express the final bias $B$ in degrees Celsius.\n\nTest Suite:\nProvide the bias $B$ for each of the following parameter sets, covering a general case, boundary, and edge cases:\n- Case $1$: $(T_0, A, H_{\\mathrm{bar}}, s, \\phi, \\psi) = (2.0, 10.0, 1.0, 0.3, 0.0, 0.0)$\n- Case $2$: $(T_0, A, H_{\\mathrm{bar}}, s, \\phi, \\psi) = (-1.0, 10.0, 1.0, 0.0, 0.0, 0.7)$\n- Case $3$: $(T_0, A, H_{\\mathrm{bar}}, s, \\phi, \\psi) = (5.0, 12.0, 1.0, 0.5, 0.0, \\pi/2)$\n- Case $4$: $(T_0, A, H_{\\mathrm{bar}}, s, \\phi, \\psi) = (0.0, 8.0, 1.0, 0.4, 0.0, \\pi)$\n- Case $5$: $(T_0, A, H_{\\mathrm{bar}}, s, \\phi, \\psi) = (3.0, 6.0, 1.0, 0.9, 0.0, 0.3)$\n\nRequired Units and Output:\n- Report the bias $B$ for each case in degrees Celsius, rounded to $4$ decimal places.\n- Angles must be treated in radians.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[b_1,b_2,\\dots,b_5]$), where each $b_i$ is the bias for case $i$ in degrees Celsius rounded to $4$ decimal places.",
            "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a simplified but standard model used in paleoclimatology to understand the effects of seasonal bias in proxy records. All necessary parameters and definitions are provided for a unique solution.\n\nThe objective is to compute the bias, $B$, of a naive annual mean temperature reconstruction. The bias is defined as the difference between the estimated annual mean, $\\hat{T}_{\\mathrm{ann}}$, and the true annual mean, $T_{\\mathrm{ann}}$.\n$$B = \\hat{T}_{\\mathrm{ann}} - T_{\\mathrm{ann}}$$\n\nFirst, we establish the definitions provided in the problem statement. The true monthly temperature for month $j \\in \\{0, 1, \\dots, 11\\}$ is given by:\n$$T_j = T_0 + A \\cos(\\theta_j - \\phi), \\quad \\text{where } \\theta_j = \\frac{2\\pi j}{12}$$\nHere, $T_0$ is the true annual mean temperature, $A$ is the seasonal amplitude, and $\\phi$ is the phase of the temperature cycle.\n\nThe monthly sensitivity of the proxy is:\n$$H_j = H_{\\mathrm{bar}} (1 + s \\cos(\\theta_j - \\psi))$$\nwhere $H_{\\mathrm{bar}}$ is the mean sensitivity, $s$ is the seasonal amplitude of the sensitivity, and $\\psi$ is its phase.\n\nThe proxy value for month $j$ is modeled as:\n$$P_j = H_j T_j$$\n\nThe naive estimator for the annual mean temperature assumes a constant sensitivity $H_{\\mathrm{bar}}$:\n$$\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} \\frac{P_j}{H_{\\mathrm{bar}}}$$\n\nThe true annual mean temperature is:\n$$T_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} T_j$$\n\nWe can derive an analytical expression for the bias $B$. First, let us analyze $T_{\\mathrm{ann}}$. By substituting the expression for $T_j$:\n$$T_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} (T_0 + A \\cos(\\theta_j - \\phi))$$\n$$T_{\\mathrm{ann}} = \\frac{1}{12} \\left( \\sum_{j=0}^{11} T_0 + A \\sum_{j=0}^{11} \\cos(\\theta_j - \\phi) \\right)$$\nThe summation $\\sum_{j=0}^{11} \\cos(\\frac{2\\pi j}{12} - \\phi)$ over a full cycle is zero. Thus, the expression simplifies to:\n$$T_{\\mathrm{ann}} = \\frac{1}{12} (12 T_0 + 0) = T_0$$\nThis confirms that $T_0$ is indeed the true annual mean.\n\nNext, we expand the expression for the estimated mean, $\\hat{T}_{\\mathrm{ann}}$:\n$$\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} \\frac{H_j T_j}{H_{\\mathrm{bar}}}$$\nSubstituting the expressions for $H_j$ and $T_j$:\n$$\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} \\frac{H_{\\mathrm{bar}} (1 + s \\cos(\\theta_j - \\psi)) (T_0 + A \\cos(\\theta_j - \\phi))}{H_{\\mathrm{bar}}}$$\n$$\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} (1 + s \\cos(\\theta_j - \\psi)) (T_0 + A \\cos(\\theta_j - \\phi))$$\nExpanding the product inside the summation:\n$$\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} \\sum_{j=0}^{11} [T_0 + A \\cos(\\theta_j - \\phi) + s T_0 \\cos(\\theta_j - \\psi) + s A \\cos(\\theta_j - \\psi) \\cos(\\theta_j - \\phi)]$$\nWe can evaluate the sum of each term:\n1.  $\\sum_{j=0}^{11} T_0 = 12 T_0$\n2.  $\\sum_{j=0}^{11} A \\cos(\\theta_j - \\phi) = 0$, as established earlier.\n3.  $\\sum_{j=0}^{11} s T_0 \\cos(\\theta_j - \\psi) = 0$, for the same reason.\n4.  For the final term, we use the product-to-sum trigonometric identity $\\cos(X)\\cos(Y) = \\frac{1}{2}[\\cos(X-Y) + \\cos(X+Y)]$:\n    $$s A \\sum_{j=0}^{11} \\cos(\\theta_j - \\psi) \\cos(\\theta_j - \\phi) = s A \\sum_{j=0}^{11} \\frac{1}{2} [\\cos(\\phi - \\psi) + \\cos(2\\theta_j - \\psi - \\phi)]$$\n    The term $\\cos(\\phi - \\psi)$ is constant with respect to $j$, so its sum is $12 \\cos(\\phi - \\psi)$. The term $\\cos(2\\theta_j - \\psi - \\phi) = \\cos(\\frac{4\\pi j}{12} - (\\psi+\\phi))$ sums to zero over the full cycle because it is a sinusoid with frequency $k=2$.\n    Therefore, the sum of the fourth term is:\n    $$s A \\left( \\frac{1}{2} [12 \\cos(\\phi - \\psi) + 0] \\right) = 6 s A \\cos(\\phi - \\psi)$$\nCombining these results:\n$$\\hat{T}_{\\mathrm{ann}} = \\frac{1}{12} [12 T_0 + 0 + 0 + 6 s A \\cos(\\phi - \\psi)] = T_0 + \\frac{sA}{2} \\cos(\\phi - \\psi)$$\nFinally, we compute the bias $B$:\n$$B = \\hat{T}_{\\mathrm{ann}} - T_{\\mathrm{ann}} = \\left( T_0 + \\frac{sA}{2} \\cos(\\phi - \\psi) \\right) - T_0$$\n$$B = \\frac{sA}{2} \\cos(\\phi - \\psi)$$\nThis analytical result shows that the reconstruction bias is proportional to the product of the seasonal amplitudes of temperature ($A$) and sensitivity ($s$), modulated by the phase difference between them. The bias is independent of the mean temperature $T_0$ and mean sensitivity $H_{\\mathrm{bar}}$. The numerical implementation will compute the sums explicitly as requested by the problem, which must yield the same result.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias in annual mean temperature reconstruction due to seasonal\n    sensitivity in a paleoclimate proxy.\n    \"\"\"\n    # Test cases are defined as tuples: (T0, A, H_bar, s, phi, psi)\n    test_cases = [\n        # Case 1: In-phase seasonality\n        (2.0, 10.0, 1.0, 0.3, 0.0, 0.0),\n        # Case 2: No sensitivity seasonality (s=0)\n        (-1.0, 10.0, 1.0, 0.0, 0.0, 0.7),\n        # Case 3: Quadrature-phase seasonality (90 degrees)\n        (5.0, 12.0, 1.0, 0.5, 0.0, np.pi / 2),\n        # Case 4: Anti-phase seasonality (180 degrees)\n        (0.0, 8.0, 1.0, 0.4, 0.0, np.pi),\n        # Case 5: General case with a phase lag\n        (3.0, 6.0, 1.0, 0.9, 0.0, 0.3),\n    ]\n\n    results = []\n    \n    # Define monthly indices j = 0, 1, ..., 11\n    j = np.arange(12)\n    \n    # Define monthly angles theta_j in radians\n    theta_j = 2 * np.pi * j / 12\n\n    for case in test_cases:\n        T0, A, H_bar, s, phi, psi = case\n        \n        # Step 1: Construct the true monthly temperature series T_j\n        # T_j = T_0 + A * cos(theta_j - phi)\n        T_j = T0 + A * np.cos(theta_j - phi)\n        \n        # Step 1: Construct the proxy sensitivity series H_j\n        # H_j = H_bar * (1 + s * cos(theta_j - psi))\n        H_j = H_bar * (1 + s * np.cos(theta_j - psi))\n        \n        # Step 2: Compute the monthly proxy record P_j using the forward model\n        # P_j = H_j * T_j\n        P_j = H_j * T_j\n        \n        # Step 3: Compute the naive annual mean temperature estimate\n        # This assumes constant sensitivity H_bar\n        # T_ann_hat = (1/12) * sum(P_j / H_bar)\n        T_ann_hat = np.mean(P_j / H_bar)\n        \n        # Step 3: Compute the true annual mean temperature\n        # T_ann = (1/12) * sum(T_j)\n        T_ann = np.mean(T_j)\n        \n        # Step 4: Compute the bias B as the difference\n        # B = T_ann_hat - T_ann\n        B = T_ann_hat - T_ann\n        \n        results.append(B)\n\n    # Format the results to 4 decimal places and print in the required format.\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}