{
    "hands_on_practices": [
        {
            "introduction": "Accurate paleoclimate reconstructions begin with robust calibration, the process of relating a proxy measurement to a known climate variable like temperature. However, a common complication arises when the proxy data itself is subject to measurement error, a scenario addressed by the errors-in-variables (EIV) framework. This exercise  demonstrates the critical importance of accounting for this uncertainty, as ignoring it leads to a systematic underestimation of the true climate-proxy relationship, a phenomenon known as attenuation bias.",
            "id": "4073673",
            "problem": "A paleoclimate calibration seeks to relate a marine sediment geochemical proxy to instrumental sea surface temperature using an errors-in-variables (EIV) framework. Let $Y_t$ denote the instrumental temperature (in degrees Celsius) and let $X_t$ denote the latent, true proxy state (in dimensionless proxy units). Observations of the proxy are contaminated by analytical measurement error so that the observed proxy is $X_t^{\\ast} = X_t + u_t$, where $u_t$ is a zero-mean, independent measurement error with constant variance. The physical calibration model linking temperature and the true proxy is $Y_t = \\alpha + \\beta X_t + \\varepsilon_t$, where $\\varepsilon_t$ is a zero-mean, independent noise term, and $(u_t)$ is independent of $(\\varepsilon_t)$ and $(X_t)$. Over a modern calibration interval, the following summary statistics are computed from paired observations $\\{(Y_t, X_t^{\\ast})\\}_{t=1}^{n}$:\n- The sample covariance between temperature and the observed proxy is $C_{Y X^{\\ast}} = 0.84$ (degrees Celsius times proxy unit).\n- The sample variance of the observed proxy is $V_{X^{\\ast}} = 1.50$ (proxy units squared).\n- Independent replicate analyses provide an estimate of the measurement error variance, $\\sigma_u^2 = 0.60$ (proxy units squared).\n\nUnder the model assumptions above, use an errors-in-variables approach grounded in first principles to estimate the calibration slope $\\beta$ and compute the attenuation bias that would result if the measurement error in $X_t^{\\ast}$ were ignored when regressing $Y_t$ on $X_t^{\\ast}$. Express the slope in degrees Celsius per proxy unit and the bias in degrees Celsius per proxy unit. Round both numbers to four significant figures. Provide your final answer as a row matrix whose first entry is the EIV slope estimate and whose second entry is the attenuation bias when measurement error is ignored.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically sound, and well-posed.\n\n### Step 1: Extract Givens\n-   The physical model relating true temperature $Y_t$ to the true latent proxy state $X_t$ is: $Y_t = \\alpha + \\beta X_t + \\varepsilon_t$.\n-   The measurement model for the observed proxy $X_t^{\\ast}$ is: $X_t^{\\ast} = X_t + u_t$.\n-   $\\varepsilon_t$ is a zero-mean, independent noise term.\n-   $u_t$ is a zero-mean, independent measurement error with constant variance, $\\sigma_u^2$.\n-   The error sequences $(\\varepsilon_t)$ and $(u_t)$ are independent of each other and of the true proxy state sequence $(X_t)$.\n-   Sample covariance between temperature and the observed proxy: $C_{Y X^{\\ast}} = 0.84$.\n-   Sample variance of the observed proxy: $V_{X^{\\ast}} = 1.50$.\n-   Variance of the measurement error: $\\sigma_u^2 = 0.60$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem describes a standard errors-in-variables (EIV) regression model. This is a fundamental statistical framework used widely in science and engineering for calibration problems where the predictor variable is measured with error. The model is mathematically and scientifically sound.\n-   **Well-Posed:** The problem provides all necessary model assumptions and summary statistics ($C_{Y X^{\\ast}}$, $V_{X^{\\ast}}$, $\\sigma_u^2$) to derive a unique estimate for the slope parameter $\\beta$ and the associated attenuation bias.\n-   **Objective:** The problem is stated using precise, standard terminology from statistics and regression analysis. There is no ambiguity or subjective language.\n\n### Step 3: Verdict and Action\n-   The problem is deemed **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe goal is to estimate the true calibration slope $\\beta$ from the model $Y_t = \\alpha + \\beta X_t + \\varepsilon_t$, given that we only observe $Y_t$ and a noisy version of the predictor, $X_t^{\\ast} = X_t + u_t$.\n\nLet us first establish the relationships between the population moments (variances and covariances) of the observed variables and the underlying model parameters. We denote the variance of the true proxy state as $\\sigma_X^2 = \\text{Var}(X_t)$.\n\nThe population covariance between the observed temperature $Y_t$ and the observed proxy $X_t^{\\ast}$ is:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\text{Cov}(\\alpha + \\beta X_t + \\varepsilon_t, X_t + u_t) $$\nUsing the bilinearity of the covariance operator and the independence assumptions ($X_t$, $\\varepsilon_t$, and $u_t$ are mutually independent), this simplifies to:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\text{Cov}(\\beta X_t, X_t) + \\text{Cov}(\\beta X_t, u_t) + \\text{Cov}(\\varepsilon_t, X_t) + \\text{Cov}(\\varepsilon_t, u_t) $$\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\beta \\text{Var}(X_t) + \\beta \\text{Cov}(X_t, u_t) + \\text{Cov}(\\varepsilon_t, X_t) + \\text{Cov}(\\varepsilon_t, u_t) $$\nSince $\\text{Cov}(X_t, u_t)=0$, $\\text{Cov}(\\varepsilon_t, X_t)=0$, and $\\text{Cov}(\\varepsilon_t, u_t)=0$, we are left with:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\beta \\text{Var}(X_t) = \\beta \\sigma_X^2 $$\n\nThe variance of the observed proxy $X_t^{\\ast}$ is:\n$$ \\text{Var}(X_t^{\\ast}) = \\text{Var}(X_t + u_t) $$\nSince $X_t$ and $u_t$ are independent, the variance of their sum is the sum of their variances:\n$$ \\text{Var}(X_t^{\\ast}) = \\text{Var}(X_t) + \\text{Var}(u_t) = \\sigma_X^2 + \\sigma_u^2 $$\n\nIf one were to ignore the measurement error $u_t$ and perform an ordinary least squares (OLS) regression of $Y_t$ on $X_t^{\\ast}$, the slope estimator, which we denote $\\hat{\\beta}_{OLS}$, would be calculated as:\n$$ \\hat{\\beta}_{OLS} = \\frac{C_{Y X^{\\ast}}}{V_{X^{\\ast}}} $$\nIn the large sample limit, this estimator converges to:\n$$ \\beta_{OLS} = \\frac{\\text{Cov}(Y_t, X_t^{\\ast})}{\\text{Var}(X_t^{\\ast})} = \\frac{\\beta \\sigma_X^2}{\\sigma_X^2 + \\sigma_u^2} = \\beta \\left( \\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_u^2} \\right) $$\nSince $\\sigma_X^2 > 0$ and $\\sigma_u^2 > 0$, the factor $\\frac{\\sigma_X^2}{\\sigma_X^2 + \\sigma_u^2}$ is always less than $1$. This demonstrates that the OLS estimate is biased towards zero, a phenomenon known as attenuation bias.\n\nTo obtain an unbiased estimate of $\\beta$, we must correct for this effect. The EIV approach uses the knowledge of the measurement error variance $\\sigma_u^2$. From the variance equation for $X_t^{\\ast}$, we can express the unknown variance of the true proxy as:\n$$ \\sigma_X^2 = \\text{Var}(X_t^{\\ast}) - \\sigma_u^2 $$\nSubstituting this into the covariance equation:\n$$ \\text{Cov}(Y_t, X_t^{\\ast}) = \\beta (\\text{Var}(X_t^{\\ast}) - \\sigma_u^2) $$\nSolving for the true slope $\\beta$ gives the EIV estimator, $\\hat{\\beta}_{EIV}$, by replacing population moments with their sample estimates:\n$$ \\hat{\\beta}_{EIV} = \\frac{C_{Y X^{\\ast}}}{V_{X^{\\ast}} - \\sigma_u^2} $$\nUsing the provided numerical values:\n$$ C_{Y X^{\\ast}} = 0.84 $$\n$$ V_{X^{\\ast}} = 1.50 $$\n$$ \\sigma_u^2 = 0.60 $$\nThe EIV slope estimate is:\n$$ \\hat{\\beta}_{EIV} = \\frac{0.84}{1.50 - 0.60} = \\frac{0.84}{0.90} = \\frac{14}{15} \\approx 0.933333... $$\nRounding to four significant figures, $\\hat{\\beta}_{EIV} = 0.9333$ degrees Celsius per proxy unit.\n\nNext, we compute the attenuation bias that results from using the naive OLS approach. This bias is the difference between the OLS slope estimate and the true slope, for which we use our best estimate, $\\hat{\\beta}_{EIV}$. The OLS slope estimate is:\n$$ \\hat{\\beta}_{OLS} = \\frac{C_{Y X^{\\ast}}}{V_{X^{\\ast}}} = \\frac{0.84}{1.50} = 0.56 $$\nThe attenuation bias is calculated as:\n$$ \\text{Bias} = \\hat{\\beta}_{OLS} - \\hat{\\beta}_{EIV} $$\n$$ \\text{Bias} = 0.56 - \\frac{14}{15} = \\frac{14}{25} - \\frac{14}{15} = 14 \\left( \\frac{1}{25} - \\frac{1}{15} \\right) = 14 \\left( \\frac{3 - 5}{75} \\right) = 14 \\left( \\frac{-2}{75} \\right) = -\\frac{28}{75} \\approx -0.373333... $$\nRounding to four significant figures, the attenuation bias is $-0.3733$ degrees Celsius per proxy unit. The negative sign indicates that the naive OLS regression underestimates the true magnitude of the slope.\n\nThe final answer requires the EIV slope estimate and the attenuation bias, presented as a row matrix.\nFirst entry: $\\hat{\\beta}_{EIV} = 0.9333$.\nSecond entry: $\\text{Bias} = -0.3733$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.9333 & -0.3733\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once proxies are calibrated, they can be used to reconstruct past climate variations, a task that represents a classic inverse problem. We must infer the underlying climate state from indirect and often noisy proxy observations, a process that is frequently ill-posed and sensitive to noise. This practice  introduces Tikhonov regularization, a fundamental technique that finds a physically plausible solution by balancing fidelity to the proxy data with a smoothness constraint, allowing for stable reconstructions even from imperfect data.",
            "id": "4073786",
            "problem": "Consider a linear paleoclimate proxy forward model where a latent climate state $x \\in \\mathbb{R}^{n}$ (dimensionless temperature anomalies) is mapped to proxy observations $y \\in \\mathbb{R}^{m}$ (dimensionless proxy anomalies) through a known smoothing operator $H \\in \\mathbb{R}^{m \\times n}$, with additive deterministic noise $\\epsilon \\in \\mathbb{R}^{m}$. All quantities are dimensionless. Angles appearing inside trigonometric functions must be interpreted in radians. The forward model is defined by $y = H x + \\epsilon$. Your task is to compute a regularized reconstruction $\\hat{x}$ for specified values of the regularization parameter $\\lambda$, and then evaluate reconstruction error metrics. The reconstruction should be obtained by minimizing the quadratic Tikhonov functional $J(x) = \\lVert H x - y \\rVert_2^2 + \\lambda^2 \\lVert L x \\rVert_2^2$, where $L$ is a first-difference operator that penalizes roughness in $x$.\n\nUse the following scientifically grounded and fully specified setup:\n\n- State dimension: $n = 64$. Observation dimension: $m = 48$.\n- Discrete time indices for the state: $t_j = j$ for $j = 0, 1, \\dots, n-1$.\n- Discrete time indices for the proxies: $p_i = i \\cdot \\frac{n}{m}$ for $i = 0, 1, \\dots, m-1$.\n- Smoothing scale (Gaussian kernel standard deviation): $\\ell = 3.0$.\n- The forward operator $H$ has entries\n$$\nH_{i j} = \\frac{\\exp\\!\\left(-\\frac{(p_i - t_j)^2}{2 \\ell^2}\\right)}{\\sum_{k=0}^{n-1} \\exp\\!\\left(-\\frac{(p_i - t_k)^2}{2 \\ell^2}\\right)} \\quad \\text{for } i = 0, \\dots, m-1,\\; j = 0, \\dots, n-1,\n$$\nthat is, each row of $H$ applies a normalized Gaussian-weighted average over $x$ centered at $p_i$.\n- The first-difference regularization operator $L \\in \\mathbb{R}^{(n-1) \\times n}$ is given by\n$$\n(L x)_k = x_{k+1} - x_k \\quad \\text{for } k = 0, 1, \\dots, n-2.\n$$\nEquivalently, $L_{k,k} = -1$, $L_{k,k+1} = 1$, and all other entries are $0$.\n- The true state $x^{\\star} \\in \\mathbb{R}^n$ is defined by a sum of sinusoids and cosines:\n$$\nx^{\\star}_j = \\sin\\!\\left(\\frac{2 \\pi \\cdot 3 \\cdot j}{n}\\right) + 0.6 \\cos\\!\\left(\\frac{2 \\pi \\cdot 9 \\cdot j}{n}\\right) + 0.3 \\sin\\!\\left(\\frac{2 \\pi \\cdot 15 \\cdot j}{n}\\right), \\quad j = 0, 1, \\dots, n-1.\n$$\n- The deterministic observation noise is\n$$\n\\epsilon_i = 0.03 \\left[\\sin\\!\\left(0.37 \\cdot i \\right) + 0.5 \\cos\\!\\left(0.13 \\cdot i \\right)\\right], \\quad i = 0, 1, \\dots, m-1.\n$$\n- Observations are given by $y = H x^{\\star} + \\epsilon$.\n\nFor each specified regularization strength $\\lambda$, compute the regularized solution $\\hat{x}$ that minimizes $J(x)$ and then evaluate the following error metrics:\n- The relative reconstruction error\n$$\nE_{\\mathrm{rel}}(\\hat{x}) = \\frac{\\lVert \\hat{x} - x^{\\star} \\rVert_2}{\\lVert x^{\\star} \\rVert_2}.\n$$\n- The normalized root-mean-square error (NRMSE) defined by\n$$\nE_{\\mathrm{nrmse}}(\\hat{x}) = \\frac{\\sqrt{\\frac{1}{n} \\sum_{j=0}^{n-1} \\left( \\hat{x}_j - x^{\\star}_j \\right)^2}}{\\sqrt{\\frac{1}{n} \\sum_{j=0}^{n-1} \\left( x^{\\star}_j - \\bar{x}^{\\star} \\right)^2}},\n$$\nwhere $\\bar{x}^{\\star} = \\frac{1}{n} \\sum_{j=0}^{n-1} x^{\\star}_j$ is the mean of $x^{\\star}$.\n- The data misfit per observation\n$$\nE_{\\mathrm{misfit}}(\\hat{x}) = \\frac{\\lVert H \\hat{x} - y \\rVert_2}{\\sqrt{m}}.\n$$\n- The roughness ratio\n$$\nE_{\\mathrm{rough}}(\\hat{x}) = \\frac{\\lVert L \\hat{x} \\rVert_2}{\\lVert L x^{\\star} \\rVert_2}.\n$$\n\nTest Suite:\n- Case $1$: $\\lambda = 10^{-6}$.\n- Case $2$: $\\lambda = 10^{-2}$.\n- Case $3$: $\\lambda = 5 \\cdot 10^{-1}$.\n\nYour program must:\n- Construct $H$, $L$, $x^{\\star}$, $\\epsilon$, and $y$ exactly as defined above.\n- For each test case, compute a numerically accurate minimizer $\\hat{x}$ of $J(x)$ consistent with the definitions.\n- Compute the four metrics for each case in the order $\\left[E_{\\mathrm{rel}}, E_{\\mathrm{nrmse}}, E_{\\mathrm{misfit}}, E_{\\mathrm{rough}}\\right]$.\n- Produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the four metrics rounded to six decimal places. The final output format must be exactly\n$$\n\\left[ [e_{11}, e_{12}, e_{13}, e_{14}], [e_{21}, e_{22}, e_{23}, e_{24}], [e_{31}, e_{32}, e_{33}, e_{34}] \\right]\n$$\nprinted as a single line with no spaces, for example\n$$\n\\left[[0.123456,0.234567,0.345678,0.456789],[\\dots],[\\dots]\\right].\n$$",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded, mathematically well-posed, objective, and self-contained. It presents a standard Tikhonov regularization problem for a linear inverse model, a common task in scientific computing and data analysis, with all parameters, operators, and functions explicitly defined. There are no contradictions, ambiguities, or falsehoods.\n\nThe task is to find the state vector $\\hat{x} \\in \\mathbb{R}^n$ that minimizes the Tikhonov functional $J(x)$:\n$$\nJ(x) = \\lVert H x - y \\rVert_2^2 + \\lambda^2 \\lVert L x \\rVert_2^2\n$$\nwhere $H \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $y \\in \\mathbb{R}^m$ is the observation vector, $L \\in \\mathbb{R}^{(n-1) \\times n}$ is the regularization operator, and $\\lambda$ is the regularization parameter. All quantities are provided in the problem description.\n\nThe functional $J(x)$ is a quadratic function of $x$. A unique minimizer $\\hat{x}$ exists and can be found by computing the gradient of $J(x)$ with respect to $x$ and setting it to zero. First, we expand the squared Euclidean norms using the transpose:\n$$\nJ(x) = (H x - y)^T (H x - y) + \\lambda^2 (L x)^T (L x)\n$$\nExpanding the products gives:\n$$\nJ(x) = (x^T H^T - y^T)(H x - y) + \\lambda^2 x^T L^T L x\n$$\n$$\nJ(x) = x^T H^T H x - x^T H^T y - y^T H x + y^T y + \\lambda^2 x^T L^T L x\n$$\nSince $y^T H x$ is a scalar, it is equal to its transpose, $(y^T H x)^T = x^T H^T y$. Thus, we can combine the cross-terms:\n$$\nJ(x) = x^T (H^T H + \\lambda^2 L^T L) x - 2 y^T H x + y^T y\n$$\nThe gradient of $J(x)$ with respect to the vector $x$ is:\n$$\n\\nabla_x J(x) = \\frac{\\partial}{\\partial x} \\left( x^T (H^T H + \\lambda^2 L^T L) x - 2 (H^T y)^T x + y^T y \\right)\n$$\nUsing standard rules of vector calculus, specifically that $\\nabla_x(x^T A x) = 2Ax$ for symmetric $A$ and $\\nabla_x(b^T x) = b$, we get:\n$$\n\\nabla_x J(x) = 2 (H^T H + \\lambda^2 L^T L) x - 2 H^T y\n$$\nSetting the gradient to the zero vector, $\\nabla_x J(\\hat{x}) = 0$, yields the system of linear equations known as the normal equations for the regularized problem:\n$$\n(H^T H + \\lambda^2 L^T L) \\hat{x} = H^T y\n$$\nThis is a linear system of the form $A\\hat{x} = b$, where the matrix $A = H^T H + \\lambda^2 L^T L$ is an $n \\times n$ matrix and the vector $b = H^T y$ is of size $n \\times 1$. For any $\\lambda > 0$, the matrix $A$ is symmetric and positive definite, which guarantees that a unique, stable solution $\\hat{x}$ exists. This solution can be found numerically by solving the linear system.\n\nThe computational procedure is as follows:\n$1$. Construct the vectors and matrices as specified: the state time indices $t_j$, proxy time indices $p_i$, the true state $x^{\\star}$, the noise vector $\\epsilon$, the forward operator $H$, and the regularization operator $L$.\n$2$. Compute the observation vector $y = H x^{\\star} + \\epsilon$.\n$3$. For each given value of the regularization parameter $\\lambda$ from the test suite:\n    a. Form the $n \\times n$ matrix $A = H^T H + \\lambda^2 L^T L$.\n    b. Form the $n \\times 1$ vector $b = H^T y$.\n    c. Solve the linear system $A\\hat{x} = b$ for the reconstructed state $\\hat{x}$.\n    d. Using the computed $\\hat{x}$ and the known true state $x^{\\star}$, calculate the four specified error metrics: the relative reconstruction error $E_{\\mathrm{rel}}(\\hat{x})$, the normalized root-mean-square error $E_{\\mathrm{nrmse}}(\\hat{x})$, the data misfit per observation $E_{\\mathrm{misfit}}(\\hat{x})$, and the roughness ratio $E_{\\mathrm{rough}}(\\hat{x})$.\n$4$. The final results for each test case are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov-regularized paleoclimate reconstruction problem\n    and computes the specified error metrics for different regularization strengths.\n    \"\"\"\n    \n    # Define the test cases (regularization parameters) from the problem statement.\n    test_cases = [1e-6, 1e-2, 5e-1]\n\n    # --- Step 1: Construct all specified model components ---\n\n    # Define dimensions and parameters\n    n = 64  # State dimension\n    m = 48  # Observation dimension\n    l_scale = 3.0  # Smoothing scale\n\n    # Define discrete time indices\n    t = np.arange(n, dtype=np.float64)\n    p = np.arange(m, dtype=np.float64) * (n / m)\n\n    # Construct the forward operator H\n    # Using broadcasting: p[:, None] is (m, 1), t[None, :] is (1, n)\n    h_numerator = np.exp(-(p[:, None] - t[None, :])**2 / (2 * l_scale**2))\n    h_denominator = np.sum(h_numerator, axis=1, keepdims=True)\n    H = h_numerator / h_denominator\n\n    # Construct the first-difference regularization operator L\n    L = np.zeros((n - 1, n), dtype=np.float64)\n    rows = np.arange(n - 1)\n    L[rows, rows] = -1.0\n    L[rows, rows + 1] = 1.0\n\n    # Construct the true state x_star\n    j_indices = np.arange(n, dtype=np.float64)\n    x_star = (np.sin(2 * np.pi * 3 * j_indices / n) +\n              0.6 * np.cos(2 * np.pi * 9 * j_indices / n) +\n              0.3 * np.sin(2 * np.pi * 15 * j_indices / n))\n\n    # Construct the deterministic observation noise epsilon\n    i_indices = np.arange(m, dtype=np.float64)\n    epsilon = 0.03 * (np.sin(0.37 * i_indices) + 0.5 * np.cos(0.13 * i_indices))\n\n    # Construct the observations y\n    y = H @ x_star + epsilon\n\n    # --- Step 2: Pre-compute components for efficiency ---\n\n    # Pre-calculate norms of true state components for metric calculations\n    norm_x_star = np.linalg.norm(x_star)\n    # Denominator for NRMSE: ||x* - mean(x*)||_2\n    norm_x_star_centered = np.linalg.norm(x_star - np.mean(x_star))\n    norm_Lx_star = np.linalg.norm(L @ x_star)\n    \n    # Pre-compute matrix products for the normal equations\n    HTH = H.T @ H\n    LTL = L.T @ L\n    HTy = H.T @ y\n\n    all_case_results = []\n    \n    # --- Step 3: Loop through test cases, solve, and evaluate metrics ---\n\n    for lam in test_cases:\n        # Form the system matrix for the normal equations\n        A = HTH + lam**2 * LTL\n        \n        # Solve the linear system A * x_hat = HTy for x_hat\n        x_hat = np.linalg.solve(A, HTy)\n\n        # Calculate the four error metrics\n        # 1. Relative reconstruction error\n        E_rel = np.linalg.norm(x_hat - x_star) / norm_x_star\n\n        # 2. Normalized root-mean-square error (NRMSE)\n        # NRMSE = RMSE(x_hat, x_star) / std(x_star)\n        # This is equivalent to norm(x_hat - x_star) / norm(x_star - mean(x_star))\n        E_nrmse = np.linalg.norm(x_hat - x_star) / norm_x_star_centered\n        \n        # 3. Data misfit per observation\n        E_misfit = np.linalg.norm(H @ x_hat - y) / np.sqrt(m)\n        \n        # 4. Roughness ratio\n        E_rough = np.linalg.norm(L @ x_hat) / norm_Lx_star\n\n        all_case_results.append([E_rel, E_nrmse, E_misfit, E_rough])\n\n    # --- Step 4: Format and print the final output ---\n\n    # Format each list of metrics into a comma-separated string rounded to 6 decimal places\n    case_strings = [f\"[{','.join(f'{value:.6f}' for value in metrics)}]\" for metrics in all_case_results]\n    \n    # Join all case strings into the final required format\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The climate signal encoded within a natural archive is not always preserved perfectly, as post-depositional processes can alter the record. A prime example in marine sediments is bioturbation—the mixing of material by organisms—which effectively smooths the recorded climate signal and dampens high-frequency variability. In this practice , you will use concepts from signal processing, such as the convolution theorem, to model bioturbation as a filter and quantify its impact on the spectral characteristics of a climate record.",
            "id": "4073731",
            "problem": "A continuous Sea Surface Temperature (SST) signal recorded by foraminifera in sediments is smoothed by biological mixing (bioturbation). In a simple, widely used idealization, the effect of bioturbation on the recorded SST time series is represented as a convolution in the time domain between the original signal and a mixing kernel. Assume that the mixing kernel in depth is Gaussian with standard deviation $\\sigma_z$ (in $\\mathrm{cm}$), and that the sedimentation rate is constant with value $r_{\\mathrm{sed}}$ (in $\\mathrm{cm}\\,\\mathrm{ka}^{-1}$). Under these assumptions, the smoothing in the age (time) domain can be represented as a convolution with a Gaussian mixing kernel whose temporal standard deviation is $\\sigma_t = \\sigma_z / r_{\\mathrm{sed}}$ (in $\\mathrm{ka}$).\n\nUse the following fundamental base:\n- Convolution theorem: the Fourier transform of a convolution equals the product of the Fourier transforms.\n- The Fourier transform of a normalized Gaussian in time with standard deviation $\\sigma_t$ is a Gaussian in angular frequency with a width inversely related to $\\sigma_t$.\n- A white process has constant power spectral density across frequency.\n\nTask:\n- Starting from the statements above, derive from first principles a formula for the amplitude attenuation factor at frequency $f_0$ (in $\\mathrm{ka}^{-1}$) for a sinusoidal SST component after Gaussian smoothing in time with standard deviation $\\sigma_t$.\n- Assume the unsmoothed SST process has white power spectral density across a prescribed frequency band $[f_{\\mathrm{low}}, f_{\\mathrm{high}}]$ (in $\\mathrm{ka}^{-1}$). Derive from first principles a formula for the fraction of variance retained after smoothing within that band, expressed as the band-average of the post-smoothing power relative to the pre-smoothing power in that band.\n- Implement a program that, for each test case below, computes:\n  $1$) the amplitude attenuation factor at $f_0$, and\n  $2$) the fraction of variance retained in the band $[f_{\\mathrm{low}}, f_{\\mathrm{high}}]$.\n- All outputs must be dimensionless ratios expressed as decimals. No angles are involved. No percentages should be used; all ratios must be reported as decimals.\n\nTest suite (each item specifies $\\sigma_z$ in $\\mathrm{cm}$, $r_{\\mathrm{sed}}$ in $\\mathrm{cm}\\,\\mathrm{ka}^{-1}$, $f_0$ in $\\mathrm{ka}^{-1}$, $f_{\\mathrm{low}}$ in $\\mathrm{ka}^{-1}$, and $f_{\\mathrm{high}}$ in $\\mathrm{ka}^{-1}$):\n- Test case $1$: $\\sigma_z = 0.0$, $r_{\\mathrm{sed}} = 1.0$, $f_0 = 3.0$, $f_{\\mathrm{low}} = 1.0$, $f_{\\mathrm{high}} = 10.0$.\n- Test case $2$: $\\sigma_z = 5.0$, $r_{\\mathrm{sed}} = 1.0$, $f_0 = 2.0$, $f_{\\mathrm{low}} = 1.0$, $f_{\\mathrm{high}} = 5.0$.\n- Test case $3$: $\\sigma_z = 2.0$, $r_{\\mathrm{sed}} = 50.0$, $f_0 = 2.0$, $f_{\\mathrm{low}} = 1.0$, $f_{\\mathrm{high}} = 5.0$.\n- Test case $4$: $\\sigma_z = 10.0$, $r_{\\mathrm{sed}} = 0.5$, $f_0 = 0.1$, $f_{\\mathrm{low}} = 0.05$, $f_{\\mathrm{high}} = 0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list containing the amplitude attenuation factor at $f_0$ and the band variance-retained fraction, in that order. For example: $[[a_1, r_1],[a_2, r_2],\\dots]$, where each $a_i$ and $r_i$ are decimals.\n\nAssumptions and constraints:\n- Treat the Gaussian kernel in time as normalized to unit area.\n- The band variance-retained fraction must be computed as the ratio of post-smoothing to pre-smoothing band-averaged power under the assumption of a white pre-smoothing spectrum across the band.\n- Express all intermediate physical quantities using coherent units as specified above, and report the final ratios as unitless decimals.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in signal processing theory, well-posed with a clear objective, and internally consistent.\n\nThe solution is derived from first principles as requested.\n\n### Part 1: Amplitude Attenuation Factor\n\nThe bioturbation process is modeled as a linear time-invariant system where the smoothed Sea Surface Temperature (SST) signal, $s_{\\text{smooth}}(t)$, is the convolution of the original signal, $s(t)$, with a Gaussian mixing kernel, $g(t)$.\n$$s_{\\text{smooth}}(t) = (s * g)(t) = \\int_{-\\infty}^{\\infty} s(\\tau) g(t - \\tau) d\\tau$$\nThe problem specifies that the Gaussian kernel $g(t)$ is normalized to unit area and has a standard deviation of $\\sigma_t$. Its functional form is:\n$$g(t) = \\frac{1}{\\sigma_t\\sqrt{2\\pi}} e^{-t^2 / (2\\sigma_t^2)}$$\nThe relationship between the temporal standard deviation $\\sigma_t$ (in $\\mathrm{ka}$), the mixing depth standard deviation $\\sigma_z$ (in $\\mathrm{cm}$), and the sedimentation rate $r_{\\mathrm{sed}}$ (in $\\mathrm{cm}\\,\\mathrm{ka}^{-1}$) is given by:\n$$\\sigma_t = \\frac{\\sigma_z}{r_{\\mathrm{sed}}}$$\nAccording to the convolution theorem, the Fourier transform of a convolution is the product of the individual Fourier transforms. Let $\\mathcal{F}$ denote the Fourier transform operator, and let $S(f) = \\mathcal{F}\\{s(t)\\}$ and $G(f) = \\mathcal{F}\\{g(t)\\}$, where $f$ is the frequency in $\\mathrm{ka}^{-1}$. Then, the Fourier transform of the smoothed signal is:\n$$S_{\\text{smooth}}(f) = S(f) \\cdot G(f)$$\n$G(f)$ is the transfer function of the smoothing filter. It is found by taking the Fourier transform of the Gaussian kernel $g(t)$. Using the standard Fourier transform definition $\\mathcal{F}\\{x(t)\\}(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-i2\\pi ft} dt$, the transform of a normalized Gaussian is a Gaussian:\n$$G(f) = \\mathcal{F}\\left\\{ \\frac{1}{\\sigma_t\\sqrt{2\\pi}} e^{-t^2/(2\\sigma_t^2)} \\right\\} = e^{-2\\pi^2 f^2 \\sigma_t^2}$$\nFor a sinusoidal component of the signal at a specific frequency $f_0$, its amplitude is multiplied by the magnitude of the transfer function at that frequency, $|G(f_0)|$. Since $G(f)$ is real and positive for all $f$, the amplitude attenuation factor is simply $G(f_0)$.\n$$\\text{Amplitude Attenuation}(f_0) = G(f_0) = e^{-2\\pi^2 f_0^2 \\sigma_t^2}$$\nSubstituting the expression for $\\sigma_t$, we obtain the final formula for the amplitude attenuation factor at frequency $f_0$:\n$$A(f_0) = \\exp\\left(-2\\pi^2 f_0^2 \\left(\\frac{\\sigma_z}{r_{\\mathrm{sed}}}\\right)^2\\right)$$\nIn the case of no mixing, $\\sigma_z = 0$, which implies $\\sigma_t = 0$. The attenuation factor becomes $e^0 = 1$, corresponding to no change in amplitude, as expected.\n\n### Part 2: Fraction of Variance Retained\n\nThe power spectral density (PSD) of the smoothed signal, $P_{\\text{out}}(f)$, is related to the PSD of the original signal, $P_{\\text{in}}(f)$, by the squared magnitude of the transfer function:\n$$P_{\\text{out}}(f) = P_{\\text{in}}(f) \\cdot |G(f)|^2$$\nThe power transfer function is:\n$$|G(f)|^2 = \\left(e^{-2\\pi^2 f^2 \\sigma_t^2}\\right)^2 = e^{-4\\pi^2 f^2 \\sigma_t^2}$$\nThe problem states that the original, unsmoothed signal has a white power spectral density within the frequency band $[f_{\\mathrm{low}}, f_{\\mathrm{high}}]$. This means its PSD is constant over this band: $P_{\\text{in}}(f) = C$ for some constant $C > 0$ when $f \\in [f_{\\mathrm{low}}, f_{\\mathrm{high}}]$.\n\nThe band-averaged power of the original signal is:\n$$\\overline{P}_{\\text{in}} = \\frac{1}{f_{\\mathrm{high}} - f_{\\mathrm{low}}} \\int_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}} P_{\\text{in}}(f) df = \\frac{1}{f_{\\mathrm{high}} - f_{\\mathrm{low}}} \\int_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}} C \\, df = C$$\nThe band-averaged power of the smoothed signal is:\n$$\\overline{P}_{\\text{out}} = \\frac{1}{f_{\\mathrm{high}} - f_{\\mathrm{low}}} \\int_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}} P_{\\text{out}}(f) df = \\frac{1}{f_{\\mathrm{high}} - f_{\\mathrm{low}}} \\int_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}} C \\cdot e^{-4\\pi^2 f^2 \\sigma_t^2} df$$\nThe fraction of variance retained, $R$, is the ratio of the band-averaged power after smoothing to the band-averaged power before smoothing:\n$$R = \\frac{\\overline{P}_{\\text{out}}}{\\overline{P}_{\\text{in}}} = \\frac{1}{f_{\\mathrm{high}} - f_{\\mathrm{low}}} \\int_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}} e^{-4\\pi^2 f^2 \\sigma_t^2} df$$\nThis definite integral does not have an elementary antiderivative. It can be expressed using the error function, $\\mathrm{erf}(x)$, defined as:\n$$\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-u^2} du$$\nTo solve our integral, we use the substitution $u = 2\\pi\\sigma_t f$, which implies $du = 2\\pi\\sigma_t df$. The integral becomes:\n$$\\int_{f_{\\mathrm{low}}}^{f_{\\mathrm{high}}} e^{-(2\\pi\\sigma_t f)^2} df = \\frac{1}{2\\pi\\sigma_t} \\int_{2\\pi\\sigma_t f_{\\mathrm{low}}}^{2\\pi\\sigma_t f_{\\mathrm{high}}} e^{-u^2} du$$\nUsing the property $\\int_a^b e^{-u^2}du = \\frac{\\sqrt{\\pi}}{2}(\\mathrm{erf}(b) - \\mathrm{erf}(a))$, the integral evaluates to:\n$$\\frac{1}{2\\pi\\sigma_t} \\cdot \\frac{\\sqrt{\\pi}}{2} \\left[ \\mathrm{erf}(2\\pi\\sigma_t f_{\\mathrm{high}}) - \\mathrm{erf}(2\\pi\\sigma_t f_{\\mathrm{low}}) \\right] = \\frac{\\sqrt{\\pi}}{4\\pi\\sigma_t} \\left[ \\mathrm{erf}(2\\pi\\sigma_t f_{\\mathrm{high}}) - \\mathrm{erf}(2\\pi\\sigma_t f_{\\mathrm{low}}) \\right]$$\nSubstituting this back into the formula for $R$, we get the final expression for the fraction of variance retained, valid for $\\sigma_t > 0$:\n$$R = \\frac{\\sqrt{\\pi}}{4\\pi\\sigma_t(f_{\\mathrm{high}} - f_{\\mathrm{low}})} \\left[ \\mathrm{erf}(2\\pi\\sigma_t f_{\\mathrm{high}}) - \\mathrm{erf}(2\\pi\\sigma_t f_{\\mathrm{low}}) \\right]$$\nFor the special case of $\\sigma_t = 0$, the integrand $e^{-4\\pi^2 f^2 \\sigma_t^2}$ becomes $1$. The integral evaluates to $f_{\\mathrm{high}} - f_{\\mathrm{low}}$, so the fraction of retained variance $R$ is $1$.\n\nThese derived formulae are implemented in the following program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf, erfc\n\ndef solve():\n    \"\"\"\n    Computes the amplitude attenuation factor and fraction of variance retained\n    for several test cases of signal smoothing by bioturbation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains: sigma_z (cm), r_sed (cm/ka), f0 (1/ka), flow (1/ka), fhigh (1/ka)\n    test_cases = [\n        (0.0, 1.0, 3.0, 1.0, 10.0),\n        (5.0, 1.0, 2.0, 1.0, 5.0),\n        (2.0, 50.0, 2.0, 1.0, 5.0),\n        (10.0, 0.5, 0.1, 0.05, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma_z, r_sed, f0, f_low, f_high = case\n\n        # Calculate temporal standard deviation sigma_t in ka\n        # Handle potential division by zero, although not in test cases\n        if r_sed == 0:\n            # Infinite smoothing, only DC component remains\n            sigma_t = float('inf')\n        else:\n            sigma_t = sigma_z / r_sed\n\n        # --- Part 1: Amplitude Attenuation Factor ---\n        if sigma_t == 0:\n            # No smoothing, so no attenuation\n            attenuation_factor = 1.0\n        else:\n            # Formula: exp(-2 * pi^2 * f0^2 * sigma_t^2)\n            exponent = -2 * (np.pi**2) * (f0**2) * (sigma_t**2)\n            attenuation_factor = np.exp(exponent)\n\n        # --- Part 2: Fraction of Variance Retained ---\n        if sigma_t == 0:\n            # No smoothing, so 100% of variance is retained\n            variance_fraction = 1.0\n        else:\n            # The band must have a non-zero width\n            if f_high == f_low:\n                # In the limit of a zero-width band, the fraction is just the power attenuation at that frequency\n                power_atten = np.exp(-4 * (np.pi**2) * (f_low**2) * (sigma_t**2))\n                variance_fraction = power_atten\n            else:\n                # Formula uses the error function (erf). For numerical stability with large\n                # arguments, it's better to use the complementary error function, erfc,\n                # since erf(b) - erf(a) = erfc(a) - erfc(b), which avoids catastrophic\n                # cancellation when erf(a) and erf(b) are both close to 1.\n                const = 2 * np.pi * sigma_t\n                \n                erfc_diff = erfc(const * f_low) - erfc(const * f_high)\n                numerator = np.sqrt(np.pi) * erfc_diff\n                denominator = 2 * const * (f_high - f_low)\n\n                if denominator == 0:\n                    variance_fraction = 1.0\n                else:\n                    variance_fraction = numerator / denominator\n        \n        results.append([attenuation_factor, variance_fraction])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}