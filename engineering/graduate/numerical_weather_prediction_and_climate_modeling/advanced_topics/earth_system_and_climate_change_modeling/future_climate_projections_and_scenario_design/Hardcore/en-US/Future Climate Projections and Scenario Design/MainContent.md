## Introduction
Projecting the future of Earth's climate is one of the most critical scientific endeavors of our time, providing the foundation for international policy, [risk assessment](@entry_id:170894), and adaptation planning. This complex task requires more than just physical modeling; it demands a structured framework to translate plausible futures for human society into the quantitative inputs that drive Earth System Models. The central challenge lies in bridging the world of socioeconomics—population growth, economic development, and policy choices—with the physical world of greenhouse gas concentrations, radiative forcing, and climate response. Without a coherent methodology, projections would be inconsistent, incomparable, and of limited use to decision-makers.

This article provides a comprehensive overview of the principles and practices of modern [climate projection](@entry_id:1122479) and scenario design. In the first chapter, **Principles and Mechanisms**, we will deconstruct the architecture used by major international efforts like CMIP6, examining how socioeconomic narratives are combined with climate targets to generate model forcings. We will explore the physics of radiative forcing and define the essential metrics—ECS, TCR, and TCRE—used to characterize [climate sensitivity](@entry_id:156628). The second chapter, **Applications and Interdisciplinary Connections**, shifts from theory to practice, demonstrating how these scenarios and projections are used to evaluate carbon budgets, project sea-level rise, and inform critical decisions in fields ranging from public health and ecology to engineering and infrastructure design. Finally, the **Hands-On Practices** chapter provides opportunities to apply these concepts through guided computational exercises. We begin by exploring the foundational principles that make these projections possible.

## Principles and Mechanisms

### The Scenario Architecture: From Socioeconomics to Physical Forcing

The projection of future climate change fundamentally bridges the socioeconomic and physical sciences. Earth System Models (ESMs), which solve the governing equations of physics for the atmosphere, oceans, land, and cryosphere, require inputs in the form of physical quantities: time-varying concentrations of greenhouse gases, distributions of aerosols, and patterns of land use. However, the future evolution of these quantities is not a matter of physics alone; it is driven by human choices regarding population growth, economic development, technological innovation, and policy. A robust framework is therefore required to translate plausible socioeconomic futures into the physically consistent forcing data that ESMs can ingest.

The foundation of this framework is a causal chain that connects societal drivers to climatic effects . This can be conceptualized as:
$$S(t) \rightarrow E_i(t) \rightarrow C_i(t) \rightarrow \Delta F_i(t)$$
Here, $S(t)$ represents the socioeconomic drivers (e.g., population, GDP). These drivers determine the net emissions, $E_i(t)$, of various species $i$ (e.g., $\mathrm{CO}_2$, $\mathrm{CH}_4$, [sulfur dioxide](@entry_id:149582)). The emissions, in turn, alter the atmospheric concentrations, $C_i(t)$, via complex biogeochemical cycles. Finally, these changes in concentration produce a change in the net top-of-atmosphere radiative energy balance, known as radiative forcing, $\Delta F_i(t)$. The total radiative forcing, $\Delta F(t) = \sum_i \Delta F_i(t)$, is the direct physical impetus for climate change.

The Coupled Model Intercomparison Project Phase 6 (CMIP6) employs a sophisticated architecture to operationalize this chain. The process begins with **Shared Socioeconomic Pathways (SSPs)**. The SSPs are a set of five narrative storylines describing plausible alternative evolutions of society over the 21st century without assuming any new climate policy beyond what is already in place. For instance, SSP1 describes a sustainable path ("Taking the Green Road"), while SSP3 describes a future of regional rivalry and fragmentation ("A Rocky Road"). These SSPs define the $S(t)$ component of the causal chain.

The climate outcome, however, depends on the level of climate policy adopted. This is specified by a target for total radiative forcing at the end of the century, a concept inherited from the **Representative Concentration Pathways (RCPs)** used in the previous CMIP5 project. These targets are denoted by their approximate radiative forcing in 2100, such as $2.6$, $4.5$, or $8.5 \, \mathrm{W\,m^{-2}}$.

The **Scenario Model Intercomparison Project (ScenarioMIP)** provides the crucial link between the socioeconomic narratives (SSPs) and the climate targets (RCP levels) through a matrix framework . A specific scenario is defined by combining an SSP with a forcing target, for example, SSP2-4.5. This denotes a future following the "Middle of the Road" socioeconomic pathway (SSP2) with climate policies enacted to limit the 2100 radiative forcing to approximately $4.5 \, \mathrm{W\,m^{-2}}$. To translate this high-level definition into model-ready data, **Integrated Assessment Models (IAMs)** are used. IAMs take the SSP narrative as a boundary condition and simulate the technological and economic transitions required to meet the specified forcing target, generating detailed, internally consistent time series of multi-species emissions $E_i(t)$ and land-use changes. These pathways are then processed, typically by simpler climate and carbon-cycle models, to produce the corresponding time series of greenhouse gas concentrations $C_i(t)$ and other forcing agents.

These datasets form the boundary conditions for the comprehensive ESMs participating in CMIP. ScenarioMIP organizes a hierarchy of experiments. The **Tier 1** experiments represent the core set of scenarios (e.g., SSP1-2.6, SSP2-4.5, SSP3-7.0, and SSP5-8.5) that most modeling groups are expected to run. **Tier 2** includes a wider variety of scenarios, such as very low forcing pathways (SSP1-1.9), overshoot scenarios where forcing peaks and then declines, and long-term extensions to the year 2300 .

ESMs can be run in two primary configurations. In **concentration-driven** simulations, which form the core of ScenarioMIP, the model is forced with prescribed time series of atmospheric concentrations $C_i(t)$. This ensures that all models receive the same radiative forcing, making it easier to compare their physical climate responses (e.g., [climate sensitivity](@entry_id:156628)). In **emissions-driven** simulations, the model is forced with the prescribed emissions $E_i(t)$, and its own internal, interactive carbon cycle must calculate the resulting atmospheric concentrations. These experiments, formally part of the Coupled Carbon Cycle Climate Model Intercomparison Project (C4MIP), are crucial for studying feedbacks between the climate system and the carbon cycle .

### Radiative Forcing and Global Energy Balance

The concept of radiative forcing provides the essential link between the scenario architecture and the physical response of the climate system. The global energy balance can be described by a simple, yet powerful, linear model for the global-mean surface temperature anomaly $\Delta T(t)$:
$$ N(t) = F - \lambda \Delta T(t) $$
where $N(t)$ is the net downward [radiative flux](@entry_id:151732) anomaly at the top of the atmosphere (TOA), representing the rate of energy accumulation by the Earth system. $F$ is the imposed radiative forcing, and $\lambda$ is the **[climate feedback parameter](@entry_id:1122450)**, which quantifies how much the outgoing radiation increases as the planet warms. A stable climate requires $\lambda > 0$.

Radiative forcing itself is a nuanced concept. The most basic definition is the **Instantaneous Radiative Forcing (IRF)**, which is the immediate change in the TOA [radiative balance](@entry_id:1130505) upon the introduction of a forcing agent (like increasing $\mathrm{CO}_2$), calculated before any part of the climate system (temperatures, water vapor, clouds) has had time to respond. However, some components of the atmosphere, particularly the stratosphere, adjust to the forcing on very fast timescales (days to months), long before the surface temperature begins to change significantly. These rapid adjustments alter the TOA energy balance.

A more predictive and widely used concept is the **Effective Radiative Forcing (ERF)**. ERF is defined as the change in the TOA flux after these rapid adjustments have occurred, but with the global-mean surface temperature held fixed. For a forcing agent like $\mathrm{CO}_2$, the dominant rapid adjustment is [stratospheric cooling](@entry_id:188545). Increased $\mathrm{CO}_2$ enhances [radiative cooling](@entry_id:754014) in the stratosphere, which slightly reduces the initial downward forcing at the TOA. Fast adjustments in clouds and tropospheric water vapor can also contribute. Consequently, for a quadrupling of $\mathrm{CO}_2$, the ERF is typically less than the IRF. For example, a model might show an IRF of $+7.4 \, \mathrm{W\,m^{-2}}$ but an ERF of $+6.5 \, \mathrm{W\,m^{-2}}$ . ERF is considered the most reliable predictor of the eventual surface temperature response.

In practice, ERF and the feedback parameter $\lambda$ are not directly observable. They are diagnosed from specialized climate model experiments. In an "abrupt $4\times\mathrm{CO}_2$" experiment, for instance, the $\mathrm{CO}_2$ concentration is instantaneously quadrupled and the model's response is tracked for centuries. By plotting the time series of the TOA imbalance $N(t)$ against the global surface temperature anomaly $\Delta T(t)$, one obtains a [scatter plot](@entry_id:171568) that is often nearly linear. This is known as a **Gregory plot**. A linear regression on these points yields an equation of the form $N(t) = \hat{F} + m\Delta T(t)$. The [y-intercept](@entry_id:168689) $\hat{F}$ provides an estimate of the [effective radiative forcing](@entry_id:1124194) ($F_{\mathrm{eff}}$), while the slope $m$ provides an estimate of the negative of the feedback parameter ($-\lambda$) .

### Metrics of Climate Response: ECS, TCR, and TCRE

To compare the sensitivity of different climate models and to characterize the severity of various scenarios, several key metrics have been defined. These metrics can be understood through the lens of simple energy balance models (EBMs), which serve as powerful conceptual tools and emulators for more complex ESMs .

The most fundamental metric is the **Equilibrium Climate Sensitivity (ECS)**. ECS is defined as the global-mean surface warming that would occur after the climate system reaches a new equilibrium following a sustained doubling of atmospheric $\mathrm{CO}_2$ concentration. In the linear energy balance framework, equilibrium is reached when the net TOA flux $N(t)$ becomes zero. This implies that the equilibrium warming $\Delta T_{eq}$ is given by $\Delta T_{eq} = F / \lambda$. Thus, for a $\mathrm{CO}_2$ doubling, the ECS is:
$$ \mathrm{ECS} = \frac{F_{2\times}}{\lambda} $$
where $F_{2\times}$ is the [effective radiative forcing](@entry_id:1124194) from doubled $\mathrm{CO}_2}$ (approximately $3.7 \, \mathrm{W\,m^{-2}}$). Because reaching equilibrium requires the entire depth of the ocean to warm, the timescale for ECS is very long, on the order of centuries to millennia .

A metric more relevant to the 21st century is the **Transient Climate Response (TCR)**. TCR is defined as the global-mean warming at the specific moment of $\mathrm{CO}_2$ doubling in an idealized scenario where $\mathrm{CO}_2$ concentration increases by $1\%$ per year (which takes about 70 years). Unlike ECS, TCR describes a non-equilibrium state. At the time of doubling, the climate system is still warming, and a significant amount of energy is being taken up by the oceans. This ocean heat uptake effectively removes energy that would otherwise warm the surface. In a simple EBM, this effect can be parameterized by an **ocean heat uptake efficiency**, $\kappa$. The warming at any given time in a transient scenario is approximately given by $F(t) \approx (\lambda + \kappa) \Delta T(t)$. Therefore, the TCR can be expressed as:
$$ \mathrm{TCR} \approx \frac{F_{2\times}}{\lambda + \kappa} $$
Because the denominator is larger, TCR is always less than ECS. The difference between ECS and TCR reflects the amount of "warming in the pipeline" or "committed warming" that will be realized as the ocean slowly equilibrates to the forcing . The behavior of EBMs reveals that a simple **one-layer model** with a single heat capacity can capture the basic distinction between transient and equilibrium response, but a **two-layer model**—with separate surface and deep-ocean reservoirs—provides a more faithful emulation of ESMs by representing two distinct response timescales, capturing the fast initial warming and the slow, long-term adjustment .

While ECS and TCR are defined relative to $\mathrm{CO}_2$ concentration, policy and mitigation efforts are focused on controlling $\mathrm{CO}_2$ emissions. This motivates a third metric: the **Transient Climate Response to cumulative Carbon Emissions (TCRE)**. A robust finding from ESMs is that on multi-decadal to centennial timescales, the global-mean warming from $\mathrm{CO}_2$ is nearly directly proportional to the total cumulative amount of $\mathrm{CO}_2$ emitted since the preindustrial era, $G(t)$. This can be written as:
$$ \Delta T(t) \approx \gamma G(t) $$
where $\gamma$ is the TCRE, typically expressed in units of Kelvin per trillion tonnes of carbon (K/TtC). This remarkable linearity arises from a compensation between two opposing effects: as cumulative emissions rise, (1) the radiative forcing per unit of concentration increase diminishes due to the logarithmic nature of $\mathrm{CO}_2$ forcing, but (2) the efficiency of ocean and land carbon sinks decreases, allowing a larger fraction of emitted $\mathrm{CO}_2$ to remain in the atmosphere. These two nonlinearities happen to largely cancel each other out.

The TCRE concept has profound implications. It establishes the idea of a finite **carbon budget**: for any given global warming target $T^*$, there is a total amount of cumulative $\mathrm{CO}_2$ emissions, $B \approx T^*/\gamma$, that humankind can emit before that target is exceeded. Furthermore, to stabilize the temperature at any level, the cumulative emissions must stop increasing. This requires the net emissions rate to fall to zero. The TCRE relationship thus provides the fundamental physical basis for **net-zero $\mathrm{CO}_2$ emissions** targets that are central to international [climate policy](@entry_id:1122477). Warming from non-$\mathrm{CO}_2$ agents must be considered separately and can effectively modify the remaining carbon budget for a given total temperature target .

### Uncertainty in Climate Projections: Sources and Quantification

Every [climate projection](@entry_id:1122479) is subject to uncertainty. Understanding and quantifying the sources of this uncertainty is as important as calculating the central projection itself. There are three primary sources of uncertainty in century-scale climate projections.

First is **internal variability**. The climate system is inherently chaotic; even with identical external forcing, it exhibits unforced, spontaneous fluctuations, such as El Niño-Southern Oscillation events. To isolate the forced climate change signal from this natural "noise," climate modeling centers run **initial-condition ensembles**. These consist of multiple simulations using the exact same model and forcing scenario, but each started from a slightly different, physically plausible initial state. The average of this ensemble, the **ensemble mean**, provides a robust estimate of the **[forced response](@entry_id:262169)**. The spread of the individual members around this mean provides an estimate of the magnitude of internal variability. By the law of large numbers, the uncertainty in the estimated [forced response](@entry_id:262169) decreases as the size of the initial-condition ensemble grows . Under the **[ergodic hypothesis](@entry_id:147104)**, the statistical properties of [internal variability](@entry_id:1126630) can also be estimated from the temporal fluctuations within a single, very long simulation under constant forcing conditions (a "control run") .

The second source is **model uncertainty**. Different climate models, developed by different institutions, represent the climate system in different ways. These differences constitute **[structural uncertainty](@entry_id:1132557)**, which arises from choices in the fundamental equations (e.g., hydrostatic vs. non-hydrostatic), [numerical schemes](@entry_id:752822), and, most importantly, the functional form of parameterizations for unresolved processes like clouds and turbulence. Within a given model structure, uncertainty in the values of the parameters used in these schemes constitutes **[parametric uncertainty](@entry_id:264387)**. The CMIP archive represents an **ensemble of opportunity**—a collection of models that happen to be available, not a scientifically designed sample of the "space" of all possible models. Because these models often share components, scientific heritage, and are tuned against similar data, they are not fully independent. This lack of independence means the ensemble may not sample the full range of [structural uncertainty](@entry_id:1132557), potentially leading to an underestimation of the true uncertainty and overconfidence in the projections . Furthermore, selecting models based on their past performance can introduce a [selection bias](@entry_id:172119), which may also lead to an unrepresentative estimate of uncertainty . The spread across the ensemble means of different models is primarily a measure of this [model uncertainty](@entry_id:265539), not internal variability .

The third major source of uncertainty, particularly for the late 21st century, is **scenario uncertainty**, which reflects our lack of knowledge about future human activities and the resulting emissions. The SSP-RCP framework is designed specifically to explore this source of uncertainty.

### From Global Projections to Regional Information

While global mean temperature is a crucial metric, impacts are experienced locally. Therefore, a critical step in climate projection is **downscaling**: translating coarse-resolution GCM outputs (typically with grid cells of ~$100$ km) into high-resolution information relevant for regional impact assessments. Two primary strategies exist for this task.

**Dynamical Downscaling** employs a high-resolution Regional Climate Model (RCM) over a limited geographic domain. The RCM solves the same fundamental physical equations as a GCM but on a finer grid (e.g., $1-50$ km). It is driven at its boundaries by the large-scale atmospheric conditions produced by a GCM. The strength of this approach is its high physical fidelity. It can explicitly simulate mesoscale phenomena like topographically-forced precipitation and land-sea breezes, providing a physically consistent set of all climate variables. However, its major limitation is its immense computational cost, which restricts the number of scenarios and ensemble members that can be downscaled. Furthermore, RCMs inherit biases from their driving GCMs and introduce their own, often requiring [statistical bias](@entry_id:275818) correction of their output .

**Statistical Downscaling** encompasses a wide range of methods that use empirical relationships, learned from historical data, to link large-scale GCM outputs (predictors) to local climate variables (predictands). Its primary strength is its [computational efficiency](@entry_id:270255), allowing for the rapid downscaling of large ensembles. A fundamental challenge for all statistical methods is the assumption of **stationarity**: the assumption that the statistical relationship derived from the past will continue to hold in a future, warmer climate. A second critical assumption is **representativeness**: the method may fail if future large-scale climate states (the predictors) fall outside the range of conditions seen in the historical training data .

A prominent and simple form of [statistical downscaling](@entry_id:1132326) is **Pattern Scaling**. This method assumes that the spatial pattern of regional climate change is time-invariant and scales linearly with the global-mean temperature anomaly. The local change $\Delta Y(\mathbf{x})$ is estimated as:
$$ \Delta Y(\mathbf{x}, t) \approx P_Y(\mathbf{x}) \cdot \Delta T_g(t) $$
where $P_Y(\mathbf{x})$ is a normalized response pattern derived from GCM simulations, and $\Delta T_g(t)$ is the projected global warming from a given scenario. While computationally trivial, [pattern scaling](@entry_id:197207) has low physical fidelity compared to dynamical downscaling. It cannot capture changes in atmospheric circulation or other nonlinear responses that may alter the spatial pattern of change as the climate warms .

### Interpreting and Constraining Projections

The final step in the scenario design and projection process is the interpretation and, where possible, refinement of the results. This involves formally assessing the role of external forcing and using observations to reduce projection uncertainty.

A key distinction must be made between **climate change detection** and **attribution**. Detection is the statistical process of demonstrating that an observed change is inconsistent with internal variability alone. In the "optimal fingerprinting" framework, this corresponds to rejecting the null hypothesis that the coefficients linking observations to forced responses are zero. Attribution goes a step further by quantifying the contributions of specific forcing agents (e.g., anthropogenic vs. natural). This is achieved by estimating the scaling factors ($\beta_A, \beta_N$) in a model like $y(t) = \beta_A x_A(t) + \beta_N x_N(t) + \eta(t)$, using methods like Generalized Least Squares that properly account for the statistical structure of internal variability $\eta(t)$ .

Even with large multi-model ensembles, uncertainty in future projections, such as ECS, can remain large. An advanced technique to reduce this uncertainty is the use of **Emergent Constraints**. This method identifies a strong, physically-justified statistical relationship that "emerges" across a diverse ensemble of climate models between a future projection quantity, $Y$ (e.g., ECS), and a potentially observable feature of the present-day climate, $X$ (e.g., a specific [cloud feedback](@entry_id:1122515) metric). If such a relationship exists and can be explained by a common underlying physical mechanism, it can be used to constrain the future projection. By obtaining an observational estimate of the real-world value of the predictor, $X^*$, one can use the cross-model regression to derive a probability distribution for the true value of $Y$ that is narrower than the original range from the model ensemble. A rigorous application requires a clear physical basis for the constraint and a careful propagation of all uncertainties, including the regression error and the observational uncertainty in $X^*$ .