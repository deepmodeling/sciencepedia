{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a grounding in the fundamental physics of global temperature response. By analytically solving a simple one-box Energy Balance Model (EBM), you will explore the core concepts of radiative forcing, climate feedback, and the characteristic timescale of climate adjustment. Mastering this foundational model  is essential for building intuition about how the climate system behaves as a whole when subjected to external change.",
            "id": "4047383",
            "problem": "A commonly used pedagogical and scenario-design tool in future climate projections is the globally averaged one-box energy balance model, which represents the conservation of energy for the mixed-layer oceanâ€“atmosphere system. Let the globally averaged heat capacity be $C$ (in $\\mathrm{J}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$), the net climate feedback parameter be $\\lambda$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$), and the externally imposed radiative forcing be $F(t)$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}$). Energy conservation implies that the global-mean temperature anomaly $T(t)$ (in $\\mathrm{K}$) satisfies $C\\,\\frac{dT}{dt}=F(t)-\\lambda\\,T(t)$.\n\nConsider a scenario-design experiment in which an abrupt forcing step is applied at time $t=0$, representing an idealized transition to a new forcing level used to probe transient and equilibrium responses. Specifically, suppose $F(t)=F_{0}\\,H(t)$, where $F_{0}>0$ is a constant and $H(t)$ is the Heaviside step function. Assume the system is in equilibrium with vanishing anomaly before the step, i.e., $T(t)=0$ for $t\\leq 0$.\n\nStarting from the energy conservation statement above and without using any pre-derived solution formulas, analytically solve for $T(t)$ for $t\\geq 0$. Then define the intrinsic adjustment time scale $\\tau=C/\\lambda$ and compute $T(3\\tau)$. Express the final temperature change in $\\mathrm{K}$. If you choose to leave the final result in closed form, do not approximate it numerically.",
            "solution": "The problem will first be validated against the specified criteria.\n\n### Step 1: Extract Givens\nThe following information is provided in the problem statement:\n-   **Governing Equation**: The global-mean temperature anomaly $T(t)$ is governed by the energy conservation equation $C\\,\\frac{dT}{dt}=F(t)-\\lambda\\,T(t)$.\n-   **Constants and Variables**:\n    -   $C$: globally averaged heat capacity ($\\mathrm{J}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$).\n    -   $\\lambda$: net climate feedback parameter ($\\mathrm{W}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$).\n    -   $F(t)$: externally imposed radiative forcing ($\\mathrm{W}\\,\\mathrm{m}^{-2}$).\n    -   $T(t)$: global-mean temperature anomaly ($\\mathrm{K}$).\n-   **Forcing Scenario**: The radiative forcing is an abrupt step function, $F(t)=F_{0}\\,H(t)$, where $F_{0}>0$ is a constant and $H(t)$ is the Heaviside step function.\n-   **Initial Condition**: The system is in equilibrium with zero anomaly for $t \\le 0$, i.e., $T(t)=0$ for $t \\le 0$. This implies the specific initial condition $T(0)=0$.\n-   **Objective**:\n    1.  Analytically solve for $T(t)$ for $t\\geq 0$.\n    2.  Define the time scale $\\tau=C/\\lambda$.\n    3.  Compute the value of $T(3\\tau)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated for validity.\n\n-   **Scientifically Grounded**: The one-box energy balance model is a cornerstone of simplified climate dynamics. The equation $C\\,\\frac{dT}{dt}=F(t)-\\lambda\\,T(t)$ is a standard, physically-based formulation representing the balance between heat storage rate ($C\\,\\frac{dT}{dt}$), external energy input ($F(t)$), and radiative damping ($-\\lambda\\,T(t)$), which is a linearization of the Stefan-Boltzmann law. The model is scientifically sound for its intended pedagogical and conceptual purpose.\n-   **Well-Posed**: The problem specifies a first-order linear ordinary differential equation (ODE) with a given forcing function and a well-defined initial condition ($T(0)=0$). This constitutes a well-posed initial value problem, guaranteeing the existence of a unique and stable solution.\n-   **Objective**: The problem is stated using precise, unambiguous mathematical and physical terminology.\n-   **Dimensional Consistency**: The units are consistent. The term $C\\,\\frac{dT}{dt}$ has units of $(\\mathrm{J}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}) \\cdot (\\mathrm{K}\\,\\mathrm{s}^{-1}) = \\mathrm{J}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1} = \\mathrm{W}\\,\\mathrm{m}^{-2}$. The forcing term $F(t)$ and the feedback term $\\lambda\\,T(t)$ also have units of $\\mathrm{W}\\,\\mathrm{m}^{-2}$, as required by the equation.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically grounded, well-posed, objective, complete, and consistent.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full analytical solution will now be derived.\n\n### Solution Derivation\nThe governing differential equation is:\n$$C\\,\\frac{dT}{dt}=F(t)-\\lambda\\,T(t)$$\nFor $t \\geq 0$, the forcing is $F(t) = F_{0}\\,H(t) = F_{0}$, since $H(t)=1$ for $t>0$. The equation becomes:\n$$C\\,\\frac{dT}{dt} = F_{0} - \\lambda\\,T(t)$$\nThis is a first-order linear inhomogeneous ordinary differential equation. We can rearrange it into the standard form $\\frac{dy}{dx} + P(x)y = Q(x)$:\n$$\\frac{dT}{dt} + \\frac{\\lambda}{C}\\,T(t) = \\frac{F_{0}}{C}$$\nThis equation can be solved using the method of integrating factors. The integrating factor, $I(t)$, is given by:\n$$I(t) = \\exp\\left(\\int \\frac{\\lambda}{C} \\, dt\\right) = \\exp\\left(\\frac{\\lambda}{C}t\\right)$$\nMultiplying the standard form equation by the integrating factor $I(t)$ yields:\n$$\\exp\\left(\\frac{\\lambda}{C}t\\right)\\frac{dT}{dt} + \\frac{\\lambda}{C}\\exp\\left(\\frac{\\lambda}{C}t\\right)T(t) = \\frac{F_{0}}{C}\\exp\\left(\\frac{\\lambda}{C}t\\right)$$\nThe left-hand side is the derivative of the product $T(t)I(t)$:\n$$\\frac{d}{dt}\\left[T(t)\\exp\\left(\\frac{\\lambda}{C}t\\right)\\right] = \\frac{F_{0}}{C}\\exp\\left(\\frac{\\lambda}{C}t\\right)$$\nIntegrating both sides with respect to $t$:\n$$\\int \\frac{d}{dt}\\left[T(t)\\exp\\left(\\frac{\\lambda}{C}t\\right)\\right] dt = \\int \\frac{F_{0}}{C}\\exp\\left(\\frac{\\lambda}{C}t\\right) dt$$\n$$T(t)\\exp\\left(\\frac{\\lambda}{C}t\\right) = \\frac{F_{0}}{C} \\left(\\frac{C}{\\lambda}\\right) \\exp\\left(\\frac{\\lambda}{C}t\\right) + K$$\nwhere $K$ is the constant of integration.\n$$T(t)\\exp\\left(\\frac{\\lambda}{C}t\\right) = \\frac{F_{0}}{\\lambda}\\exp\\left(\\frac{\\lambda}{C}t\\right) + K$$\nSolving for $T(t)$ by multiplying by $\\exp\\left(-\\frac{\\lambda}{C}t\\right)$:\n$$T(t) = \\frac{F_{0}}{\\lambda} + K\\exp\\left(-\\frac{\\lambda}{C}t\\right)$$\nTo determine the constant $K$, we apply the initial condition $T(0)=0$.\n$$T(0) = 0 = \\frac{F_{0}}{\\lambda} + K\\exp(0)$$\n$$0 = \\frac{F_{0}}{\\lambda} + K \\implies K = -\\frac{F_{0}}{\\lambda}$$\nSubstituting this value of $K$ back into the general solution gives the particular solution for $T(t)$ for $t \\geq 0$:\n$$T(t) = \\frac{F_{0}}{\\lambda} - \\frac{F_{0}}{\\lambda}\\exp\\left(-\\frac{\\lambda}{C}t\\right)$$\n$$T(t) = \\frac{F_{0}}{\\lambda}\\left(1 - \\exp\\left(-\\frac{\\lambda}{C}t\\right)\\right)$$\nThis is the analytical solution for the temperature anomaly as a function of time.\n\nNext, we define the intrinsic adjustment time scale $\\tau = C/\\lambda$. We can substitute this into the solution for $T(t)$:\n$$T(t) = \\frac{F_{0}}{\\lambda}\\left(1 - \\exp\\left(-\\frac{t}{\\tau}\\right)\\right)$$\nThe problem requires the computation of $T(3\\tau)$. We substitute $t=3\\tau$ into this expression:\n$$T(3\\tau) = \\frac{F_{0}}{\\lambda}\\left(1 - \\exp\\left(-\\frac{3\\tau}{\\tau}\\right)\\right)$$\n$$T(3\\tau) = \\frac{F_{0}}{\\lambda}\\left(1 - \\exp(-3)\\right)$$\nThis is the final analytical expression for the temperature anomaly at time $t=3\\tau$. The units are Kelvins ($\\mathrm{K}$), as specified for $T(t)$.",
            "answer": "$$\\boxed{\\frac{F_{0}}{\\lambda}\\left(1 - \\exp(-3)\\right)}$$"
        },
        {
            "introduction": "While theoretical models provide insight, much of climate science involves analyzing data from complex simulations. This practice introduces the Gregory regression, a standard technique for diagnosing key climate parameters from the output of Global Climate Model (GCM) experiments . You will work with synthetic model data to estimate the climate feedback parameter and effective radiative forcing, connecting the abstract concepts of the EBM to practical data analysis and uncertainty quantification.",
            "id": "4047350",
            "problem": "You are provided with synthetic global annual-mean time series for the top-of-atmosphere net radiative imbalance $N(t)$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}$) and global-mean surface temperature anomaly $T(t)$ (in $\\mathrm{K}$) from a set of abrupt forcing model experiments. Your task is to estimate the climate feedback parameter $\\lambda$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$) and the effective radiative forcing for a doubling of carbon dioxide $F_{2\\times\\mathrm{CO_2}}$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}$) using a Gregory regression, and to quantify uncertainty in the Equilibrium Climate Sensitivity (ECS), defined as the equilibrium temperature anomaly (in $\\mathrm{K}$) where the net radiative imbalance returns to zero under the imposed forcing.\n\nFundamental base:\n- Conservation of energy at the top of the atmosphere implies that the net radiative imbalance $N(t)$ is equal to the rate of change of planetary heat content, which is nonnegative when the system is gaining heat. Under an externally imposed step change in radiative forcing (such as a sudden doubling of carbon dioxide), widely used linear feedback approximations relate the externally imposed forcing to the radiative response to temperature.\n- In a one-box energy balance framework, the transient evolution of $T(t)$ following a step forcing is well-approximated by an exponential approach to an equilibrium value characterized by a single e-folding timescale.\n\nThe synthetic data are generated as follows for each test case $i$:\n- Parameters $(F_i, \\lambda_i, \\tau_i, n_i, \\sigma_{T,i}, \\sigma_{N,i}, s_i)$ are given, where $F_i$ is the imposed effective radiative forcing in $\\mathrm{W}\\,\\mathrm{m}^{-2}$, $\\lambda_i$ is the climate feedback parameter in $\\mathrm{W}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$, $\\tau_i$ is the characteristic e-folding timescale in $\\mathrm{yr}$, $n_i$ is the number of years, $\\sigma_{T,i}$ is the standard deviation of internal variability noise in temperature in $\\mathrm{K}$, $\\sigma_{N,i}$ is the standard deviation of internal variability noise in net imbalance in $\\mathrm{W}\\,\\mathrm{m}^{-2}$, and $s_i$ is a random seed for reproducibility.\n- Define $T_{\\mathrm{eq},i} = F_i / \\lambda_i$ (in $\\mathrm{K}$).\n- For integers $t = 1, 2, \\dots, n_i$ (in $\\mathrm{yr}$),\n  - Draw independent Gaussian noises $\\eta_t \\sim \\mathcal{N}(0, \\sigma_{T,i}^2)$ and $\\xi_t \\sim \\mathcal{N}(0, \\sigma_{N,i}^2)$ using the seed $s_i$ to initialize the random number generator.\n  - Set $T_i(t) = T_{\\mathrm{eq},i}\\left(1 - \\exp\\left(-t / \\tau_i\\right)\\right) + \\eta_t$ (in $\\mathrm{K}$).\n  - Set $N_i(t) = F_i - \\lambda_i T_i(t) + \\xi_t$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}$).\n\nTasks:\n1. For each test case, perform a Gregory regression by fitting a straight line to the scatter of $(T_i(t), N_i(t))$ via ordinary least squares, with $N_i(t)$ as the response and $T_i(t)$ as the predictor, using the model $N_i(t) = a_i + b_i T_i(t) + \\varepsilon_t$ where $\\varepsilon_t$ captures residual variability. From the fitted coefficients, interpret $\\hat{F}_i = a_i$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}$) and $\\hat{\\lambda}_i = -b_i$ (in $\\mathrm{W}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$).\n2. Estimate the Equilibrium Climate Sensitivity for each test case as $\\widehat{\\mathrm{ECS}}_i = \\hat{F}_i / \\hat{\\lambda}_i$ (in $\\mathrm{K}$). Quantify uncertainty in $\\widehat{\\mathrm{ECS}}_i$ by propagating the ordinary least squares parameter uncertainty of $(a_i, b_i)$ using the first-order (delta method) normal approximation to obtain a two-sided $95\\%$ confidence interval $[\\mathrm{ECS}^{\\mathrm{L}}_i, \\mathrm{ECS}^{\\mathrm{U}}_i]$ (in $\\mathrm{K}$). Express the lower and upper bounds as decimals.\n3. For each test case, report the five quantities $[\\hat{\\lambda}_i, \\hat{F}_i, \\widehat{\\mathrm{ECS}}_i, \\mathrm{ECS}^{\\mathrm{L}}_i, \\mathrm{ECS}^{\\mathrm{U}}_i]$ with each value rounded to three decimal places.\n\nTest suite:\n- Case $1$: $(F_1, \\lambda_1, \\tau_1, n_1, \\sigma_{T,1}, \\sigma_{N,1}, s_1) = (3.71, 1.00, 4.00, 60, 0.05, 0.15, 17)$.\n- Case $2$: $(F_2, \\lambda_2, \\tau_2, n_2, \\sigma_{T,2}, \\sigma_{N,2}, s_2) = (3.50, 1.20, 3.00, 50, 0.00, 0.00, 0)$.\n- Case $3$: $(F_3, \\lambda_3, \\tau_3, n_3, \\sigma_{T,3}, \\sigma_{N,3}, s_3) = (3.80, 0.90, 10.00, 15, 0.10, 0.30, 123)$.\n\nPhysical units:\n- Report $\\hat{\\lambda}_i$ in $\\mathrm{W}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}$.\n- Report $\\hat{F}_i$ in $\\mathrm{W}\\,\\mathrm{m}^{-2}$.\n- Report $\\widehat{\\mathrm{ECS}}_i$, $\\mathrm{ECS}^{\\mathrm{L}}_i$, and $\\mathrm{ECS}^{\\mathrm{U}}_i$ in $\\mathrm{K}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a bracketed list in the order $[\\hat{\\lambda}_i, \\hat{F}_i, \\widehat{\\mathrm{ECS}}_i, \\mathrm{ECS}^{\\mathrm{L}}_i, \\mathrm{ECS}^{\\mathrm{U}}_i]$. For example, the output should look like $[[x_1,x_2,x_3,x_4,x_5],[y_1,y_2,y_3,y_4,y_5],[z_1,z_2,z_3,z_4,z_5]]$ with each value rounded to three decimal places as specified.",
            "solution": "We base the analysis on planetary energy balance and linear radiative feedback concepts used in climate modeling. The top-of-atmosphere net radiative imbalance $N(t)$ represents the net energy flux into the Earth system at time $t$, and conservation of energy implies $N(t)$ equals the rate of change of heat content. Under an externally imposed step increase in radiative forcing, many comprehensive climate models and simple energy balance models exhibit an approximately linear relationship between the externally imposed forcing and the radiative response to surface temperature changes. This motivates treating the relation between $N(t)$ and $T(t)$ as approximately linear during the transient response.\n\nStarting point: Under a step forcing $F$ at $t = 0$, a one-box energy balance model with effective heat capacity $C$ and climate feedback parameter $\\lambda$ obeys\n$$\nC \\frac{dT}{dt} = F - \\lambda T.\n$$\nThis ordinary differential equation has the solution\n$$\nT(t) = T_{\\mathrm{eq}}\\left(1 - e^{-t/\\tau}\\right),\n$$\nwhere $T_{\\mathrm{eq}} = \\frac{F}{\\lambda}$ is the equilibrium temperature anomaly and $\\tau = \\frac{C}{\\lambda}$ is the characteristic e-folding timescale. During the transient adjustment, the net radiative imbalance can be decomposed into the externally imposed forcing minus the linearized feedback response proportional to the temperature anomaly. Accounting for internal variability as stochastic noise, we consider synthetic observables generated by\n$$\nT(t) = T_{\\mathrm{eq}}\\left(1 - e^{-t/\\tau}\\right) + \\eta_t,\\quad \\eta_t \\sim \\mathcal{N}(0,\\sigma_T^2),\n$$\n$$\nN(t) = F - \\lambda T(t) + \\xi_t,\\quad \\xi_t \\sim \\mathcal{N}(0,\\sigma_N^2),\n$$\nwith $\\eta_t$ and $\\xi_t$ independent. This construction ensures scientific realism by satisfying the energy balance structure and by approaching $N(t) \\to 0$ as $t \\to \\infty$ (in the absence of noise), while including plausible internal variability.\n\nGregory regression: The Gregory method regresses $N(t)$ against $T(t)$ to estimate the effective forcing and the feedback. We fit the linear model\n$$\nN(t) = a + b\\,T(t) + \\varepsilon_t,\n$$\nusing ordinary least squares, where $a$ is the intercept, $b$ is the slope, and $\\varepsilon_t$ is a residual term. Interpreting the physical parameters from the regression,\n$$\n\\hat{F} = a,\\quad \\hat{\\lambda} = -b,\n$$\nbecause the underlying physical relation is approximately $N \\approx F - \\lambda T$. The Equilibrium Climate Sensitivity, defined as the equilibrium temperature anomaly at which $N = 0$, is\n$$\n\\widehat{\\mathrm{ECS}} = \\frac{\\hat{F}}{\\hat{\\lambda}}.\n$$\n\nOrdinary least squares estimation: Let $y \\in \\mathbb{R}^n$ denote the vector of $N(t)$ and $x \\in \\mathbb{R}^n$ denote the vector of $T(t)$ over $n$ years. Define the design matrix\n$$\nX = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix},\n$$\nthe parameter vector $\\beta = \\begin{bmatrix} a \\\\ b \\end{bmatrix}$, and model $y = X \\beta + \\varepsilon$. The ordinary least squares estimator is\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y,\n$$\nyielding $\\hat{a}$ and $\\hat{b}$. The residuals are $r = y - X \\hat{\\beta}$, and the residual variance is estimated by\n$$\n\\hat{\\sigma}^2 = \\frac{r^\\top r}{n - p},\n$$\nwhere $p = 2$ is the number of parameters. The estimated covariance matrix of $\\hat{\\beta}$ is\n$$\n\\widehat{\\mathrm{Cov}}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X^\\top X)^{-1}.\n$$\n\nUncertainty quantification for $\\widehat{\\mathrm{ECS}}$: The ECS estimator is a smooth function of $(\\hat{a}, \\hat{b})$:\n$$\ng(\\hat{a}, \\hat{b}) = \\frac{\\hat{F}}{\\hat{\\lambda}} = \\frac{\\hat{a}}{-\\hat{b}} = -\\frac{\\hat{a}}{\\hat{b}}.\n$$\nBy the delta method (first-order Taylor approximation), the variance of $g(\\hat{a}, \\hat{b})$ is approximated by\n$$\n\\mathrm{Var}\\left(g(\\hat{a}, \\hat{b})\\right) \\approx \\nabla g^\\top \\, \\widehat{\\mathrm{Cov}}(\\hat{\\beta}) \\, \\nabla g,\n$$\nwhere the gradient is\n$$\n\\nabla g = \\begin{bmatrix} \\frac{\\partial g}{\\partial \\hat{a}} \\\\ \\frac{\\partial g}{\\partial \\hat{b}} \\end{bmatrix} = \\begin{bmatrix} -\\frac{1}{\\hat{b}} \\\\ \\frac{\\hat{a}}{\\hat{b}^2} \\end{bmatrix}.\n$$\nThus,\n$$\n\\widehat{\\mathrm{SE}}(\\widehat{\\mathrm{ECS}}) = \\sqrt{\\mathrm{Var}\\left(g(\\hat{a}, \\hat{b})\\right)}.\n$$\nAssuming asymptotic normality of $(\\hat{a}, \\hat{b})$, a two-sided $95\\%$ confidence interval for ECS is\n$$\n\\left[\\widehat{\\mathrm{ECS}} - 1.96\\,\\widehat{\\mathrm{SE}}(\\widehat{\\mathrm{ECS}}),\\ \\widehat{\\mathrm{ECS}} + 1.96\\,\\widehat{\\mathrm{SE}}(\\widehat{\\mathrm{ECS}})\\right].\n$$\n\nAlgorithmic steps for each test case:\n1. Generate $T(t)$ and $N(t)$ sequences using the provided $(F, \\lambda, \\tau, n, \\sigma_T, \\sigma_N, s)$ according to the equations above, ensuring independent Gaussian noise terms with the specified standard deviations and random seed.\n2. Construct $X$ and compute $\\hat{\\beta} = (\\hat{a}, \\hat{b})^\\top$ via ordinary least squares using the normal equations.\n3. Compute $\\hat{F} = \\hat{a}$ and $\\hat{\\lambda} = -\\hat{b}$.\n4. Compute $\\widehat{\\mathrm{ECS}} = \\hat{F} / \\hat{\\lambda}$.\n5. Compute $\\widehat{\\mathrm{Cov}}(\\hat{\\beta})$ from the residual variance and $(X^\\top X)^{-1}$.\n6. Compute the gradient $\\nabla g$ at $(\\hat{a}, \\hat{b})$, the standard error via the delta method, and the $95\\%$ confidence interval bounds.\n7. Round $\\hat{\\lambda}$, $\\hat{F}$, $\\widehat{\\mathrm{ECS}}$, and the lower and upper confidence bounds to three decimal places.\n8. Aggregate the results for all test cases into the single-line output format specified: a bracketed, comma-separated list of lists, one per test case, in the order $[\\hat{\\lambda}, \\hat{F}, \\widehat{\\mathrm{ECS}}, \\mathrm{ECS}^{\\mathrm{L}}, \\mathrm{ECS}^{\\mathrm{U}}]$.\n\nThis design cleanly integrates the physical basis (energy balance and linear feedback), the statistical estimator (ordinary least squares), and uncertainty quantification (delta method), producing scientifically interpretable estimates with quantified uncertainty for ECS from transient model experiment data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_synthetic_data(F, lam, tau, n_years, sigma_T, sigma_N, seed):\n    \"\"\"\n    Generate synthetic annual mean time series T(t) [K] and N(t) [W/m^2]\n    following a one-box EBM transient with internal variability noise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    t = np.arange(1, n_years + 1, dtype=float)  # years since forcing\n    T_eq = F / lam  # equilibrium temperature [K]\n    T_clean = T_eq * (1.0 - np.exp(-t / tau))\n    # Draw independent noise sequences\n    eta = rng.normal(0.0, sigma_T, size=n_years) if sigma_T > 0 else np.zeros(n_years)\n    xi = rng.normal(0.0, sigma_N, size=n_years) if sigma_N > 0 else np.zeros(n_years)\n    T = T_clean + eta\n    N = F - lam * T + xi\n    return T, N\n\ndef ols_regression_with_cov(x, y):\n    \"\"\"\n    Perform OLS regression of y on x with intercept: y = a + b x + eps.\n    Returns a_hat, b_hat, cov_beta (2x2 covariance matrix), residual variance.\n    \"\"\"\n    n = len(x)\n    X = np.column_stack([np.ones(n), x])\n    # OLS solution\n    XtX = X.T @ X\n    XtX_inv = np.linalg.inv(XtX)\n    beta_hat = XtX_inv @ (X.T @ y)\n    a_hat, b_hat = beta_hat[0], beta_hat[1]\n    # Residuals and variance estimate\n    residuals = y - X @ beta_hat\n    dof = n - 2  # two parameters\n    # If dof <= 0 (n < 2), avoid division by zero: set variance to NaN\n    if dof > 0:\n        sigma2_hat = (residuals @ residuals) / dof\n    else:\n        sigma2_hat = np.nan\n    cov_beta = sigma2_hat * XtX_inv\n    return a_hat, b_hat, cov_beta, sigma2_hat\n\ndef ecs_with_uncertainty(a_hat, b_hat, cov_beta):\n    \"\"\"\n    Compute ECS = -a_hat / b_hat and 95% CI using delta method.\n    cov_beta is 2x2 covariance matrix of [a, b].\n    Returns ECS_hat, ECS_lower, ECS_upper.\n    \"\"\"\n    # Map to physical parameters\n    F_hat = a_hat\n    lam_hat = -b_hat\n    # ECS estimate\n    ecs_hat = F_hat / lam_hat  # = -a_hat / b_hat\n    # Delta method: g(a,b) = -a/b\n    # gradient: [-1/b, a/b^2]\n    # Handle potential near-zero b_hat\n    if np.isfinite(cov_beta).all() and b_hat != 0.0:\n        grad = np.array([-1.0 / b_hat, a_hat / (b_hat ** 2)])\n        var_g = grad @ cov_beta @ grad\n        # Numerical safety: var_g should be non-negative\n        if var_g < 0:\n            # Clamp to zero if negative due to numerical round-off\n            var_g = 0.0\n        se_g = np.sqrt(var_g)\n        lower = ecs_hat - 1.96 * se_g\n        upper = ecs_hat + 1.96 * se_g\n    else:\n        # If covariance is invalid or slope is zero, return NaNs for CI\n        lower = np.nan\n        upper = np.nan\n    return ecs_hat, lower, upper\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (F [W/m^2], lambda [W/m^2/K], tau [yr], n_years, sigma_T [K], sigma_N [W/m^2], seed)\n    test_cases = [\n        (3.71, 1.00, 4.00, 60, 0.05, 0.15, 17),\n        (3.50, 1.20, 3.00, 50, 0.00, 0.00, 0),\n        (3.80, 0.90, 10.00, 15, 0.10, 0.30, 123),\n    ]\n\n    results = []\n    for F, lam, tau, n, sigma_T, sigma_N, seed in test_cases:\n        # Generate synthetic data for this case\n        T, N = generate_synthetic_data(F, lam, tau, n, sigma_T, sigma_N, seed)\n        # Perform OLS regression: N = a + b T\n        a_hat, b_hat, cov_beta, _ = ols_regression_with_cov(T, N)\n        # Map to physical parameters\n        F_hat = a_hat\n        lam_hat = -b_hat\n        # Compute ECS and uncertainty\n        ecs_hat, ecs_lower, ecs_upper = ecs_with_uncertainty(a_hat, b_hat, cov_beta)\n        # Round to three decimals as specified\n        vals = [\n            round(lam_hat, 3),\n            round(F_hat, 3),\n            round(ecs_hat, 3),\n            round(ecs_lower, 3) if np.isfinite(ecs_lower) else float('nan'),\n            round(ecs_upper, 3) if np.isfinite(ecs_upper) else float('nan'),\n        ]\n        results.append(vals)\n\n    # Final print statement in the exact required format.\n    # Produce a single line: a bracketed comma-separated list of lists.\n    def fmt_list(lst):\n        return \"[\" + \",\".join(map(lambda v: \"nan\" if (isinstance(v, float) and np.isnan(v)) else f\"{v}\", lst)) + \"]\"\n\n    print(\"[\" + \",\".join(fmt_list(res) for res in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A primary challenge in future projections is the large spread across different climate models. This advanced exercise introduces the powerful concept of an \"emergent constraint,\" a state-of-the-art method for reducing projection uncertainty . By finding a statistical relationship between an observable present-day variable and a future climate response across a model ensemble, you can use real-world observations to narrow the range of plausible future outcomes.",
            "id": "4047342",
            "problem": "You are given a conceptual setup for an emergent constraint in climate modeling: across multiple Global Climate Models (GCMs), a present-day observable $X$ (dimensionless, e.g., an albedo-related metric) and a future-response variable $Y$ (e.g., snow-albedo feedback strength) are paired as $(X_i, Y_i)$, for $i = 1, \\dots, n$. The emergent constraint posits that systematic inter-model differences in $X$ explain inter-model differences in $Y$ through a physically motivated and statistically testable relationship. Assume a linear model across models, $Y_i = a + b\\,X_i + \\varepsilon_i$, where $\\varepsilon_i$ is Gaussian noise with zero mean and common variance, and $a$ and $b$ are unknown coefficients. You observe the real-world present-day quantity with measurement uncertainty, $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$, where $x_{\\mathrm{obs}}$ is the observed value and $s_x$ is the standard deviation of its uncertainty. The task is to quantify the constrained projection uncertainty of the real-world future quantity $Y^\\star$ using the emergent constraint implied by the model ensemble and the observation.\n\nStarting from first principles in statistical inference (Bayes' theorem for Gaussian likelihoods and noninformative priors), ordinary least squares (OLS) estimation as the maximum likelihood estimator for linear-Gaussian models, and the law of total expectation and the law of total variance, derive and implement a method to:\n- Estimate the linear relationship between $X$ and $Y$ across models.\n- Form the posterior predictive distribution for $Y^\\star$ conditioned on the uncertain real-world $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$.\n- Quantify the constrained projection uncertainty through the predictive mean and predictive standard deviation of $Y^\\star$.\n- Quantify the unconstrained projection uncertainty through the sample standard deviation of $Y_i$ across models, and report the reduction factor defined as the ratio of constrained predictive standard deviation to unconstrained ensemble standard deviation.\n\nAll outputs involving $Y$ must be expressed in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$ and be floating-point numbers. The reduction factor must be expressed as a floating-point decimal (unitless). Angles do not appear in this problem.\n\nImplement a complete Python program that computes, for each test case, the constrained predictive mean of $Y^\\star$, the constrained predictive standard deviation of $Y^\\star$, and the reduction factor (constrained standard deviation divided by the unconstrained ensemble standard deviation). The program should not read any input and must run as-is.\n\nUse the following test suite of parameter values covering a range of scenarios, including a high-correlation case, a weak-correlation case, a boundary case with zero measurement uncertainty, and a large measurement uncertainty case. In each case, $X$ is dimensionless and $Y$ is in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$:\n\n- Test case $1$ (high correlation, small measurement uncertainty):\n  - $X = [\\,0.2,\\,0.4,\\,0.6,\\,0.8,\\,1.0,\\,0.3,\\,0.7,\\,0.9\\,]$\n  - $Y = [\\,0.98,\\,1.25,\\,1.73,\\,2.08,\\,2.51,\\,1.10,\\,1.86,\\,2.32\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.65$\n  - $s_x = 0.05$\n\n- Test case $2$ (weak correlation, comparable noise):\n  - $X = [\\,0.1,\\,0.2,\\,0.5,\\,0.6,\\,0.8,\\,0.3,\\,0.4,\\,0.7,\\,0.9,\\,0.55\\,]$\n  - $Y = [\\,0.71,\\,1.27,\\,0.95,\\,1.26,\\,0.93,\\,1.08,\\,0.99,\\,1.17,\\,0.89,\\,1.205\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.50$\n  - $s_x = 0.05$\n\n- Test case $3$ (boundary: zero measurement uncertainty and observation at the ensemble mean):\n  - $X = [\\,0.2,\\,0.3,\\,0.4,\\,0.5,\\,0.6\\,]$\n  - $Y = [\\,0.0,\\,0.05,\\,0.22,\\,0.27,\\,0.41\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.40$\n  - $s_x = 0.00$\n\n- Test case $4$ (moderate correlation, large measurement uncertainty):\n  - $X = [\\,0.2,\\,0.5,\\,0.7,\\,1.0,\\,0.3,\\,0.9\\,]$\n  - $Y = [\\,0.70,\\,0.85,\\,1.40,\\,1.65,\\,0.65,\\,1.65\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.60$\n  - $s_x = 0.20$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a three-element list of floats in the order $[\\,\\text{predictive mean of }Y^\\star,\\,\\text{predictive standard deviation of }Y^\\star,\\,\\text{reduction factor}\\,]$, all formatted to six decimal places. For example, in the required format: $[[\\mu_1,\\sigma_1,r_1],[\\mu_2,\\sigma_2,r_2],[\\mu_3,\\sigma_3,r_3],[\\mu_4,\\sigma_4,r_4]]$.",
            "solution": "The problem requires the derivation and implementation of a method to calculate the constrained projection uncertainty for a future climate variable, $Y^\\star$, using an emergent constraint derived from an ensemble of model simulations. The constraint is a linear relationship between a present-day observable, $X$, and the future variable, $Y$. The real-world observation of the present-day quantity, $X^\\star$, is uncertain and described by a Gaussian distribution.\n\nThe derivation proceeds in three main stages: first, estimating the linear relationship from the model ensemble using Ordinary Least Squares (OLS); second, deriving the predictive mean and variance for $Y^\\star$ by propagating the uncertainty from the observation of $X^\\star$ through the linear model; and third, quantifying the reduction in uncertainty.\n\n**1. Estimation of the Linear Relationship via Ordinary Least Squares (OLS)**\n\nWe are given $n$ pairs of data points $(X_i, Y_i)$ from an ensemble of climate models. The assumed linear model is:\n$$\nY_i = a + b\\,X_i + \\varepsilon_i\n$$\nwhere $a$ and $b$ are the intercept and slope coefficients, respectively, and $\\varepsilon_i$ are independent and identically distributed random errors from a Gaussian distribution with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe OLS method finds the coefficient estimates, $\\hat{a}$ and $\\hat{b}$, that minimize the sum of squared residuals. These estimates are also the maximum likelihood estimates under the Gaussian error assumption. They are given by:\n$$\n\\hat{b} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{S_{XY}}{S_{XX}}\n$$\n$$\n\\hat{a} = \\bar{Y} - \\hat{b}\\,\\bar{X}\n$$\nwhere $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ and $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ are the sample means.\n\nThe variance of the residuals, $\\sigma^2$, which represents the scatter of the models around the regression line (a form of model structural uncertainty), is estimated by its unbiased estimator, $\\hat{\\sigma}^2$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i - (\\hat{a} + \\hat{b}\\,X_i))^2\n$$\nThe denominator is $n-2$ because two degrees of freedom are used to estimate the two parameters, $\\hat{a}$ and $\\hat{b}$.\n\n**2. Derivation of the Constrained Predictive Distribution**\n\nWe are tasked with predicting the real-world future quantity, $Y^\\star$, based on an uncertain observation of the real-world present-day quantity, $X^\\star$. The observation is given as a distribution: $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$. The quantity $Y^\\star$ is related to $X^\\star$ through the model $Y^\\star = \\hat{a} + \\hat{b}\\,X^\\star$, but this prediction itself is uncertain.\n\nThe total uncertainty in the prediction of $Y^\\star$ stems from three sources: (1) the intrinsic 'structural' uncertainty of the model relationship, captured by $\\hat{\\sigma}^2$; (2) the uncertainty in the estimated regression parameters $\\hat{a}$ and $\\hat{b}$; and (3) the uncertainty in the predictor variable $X^\\star$, captured by $s_x^2$.\n\nTo find the mean and variance of the posterior predictive distribution for $Y^\\star$, we use the law of total expectation and the law of total variance.\n\n**Predictive Mean of $Y^\\star$**\nThe law of total expectation states $\\mathbb{E}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$.\nThe inner expectation is the predicted value of $Y^\\star$ for a given value of $X^\\star$:\n$$\n\\mathbb{E}[Y^\\star | X^\\star = x] = \\hat{a} + \\hat{b}\\,x\n$$\nTaking the outer expectation over the distribution of $X^\\star$:\n$$\n\\mu_{Y^\\star} = \\mathbb{E}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\hat{a} + \\hat{b}\\,X^\\star] = \\hat{a} + \\hat{b}\\,\\mathbb{E}[X^\\star]\n$$\nSince $\\mathbb{E}[X^\\star] = x_{\\mathrm{obs}}$, the predictive mean is:\n$$\n\\mu_{Y^\\star} = \\hat{a} + \\hat{b}\\,x_{\\mathrm{obs}}\n$$\n\n**Predictive Variance of $Y^\\star$**\nThe law of total variance states $\\mathrm{Var}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] + \\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$.\n\nThe first term, $\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]]$, represents the average prediction uncertainty. The variance of a prediction for a *single new observation* at a *known* point $x$ is given by:\n$$\n\\mathrm{Var}[Y^\\star | X^\\star=x] = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(x - \\bar{X})^2}{S_{XX}} \\right)\n$$\nThis expression accounts for both the residual variance ($\\hat{\\sigma}^2$) and the uncertainty in the estimated regression line itself (the terms with $1/n$ and $(x - \\bar{X})^2/S_{XX}$). We now average this over the distribution of $X^\\star$:\n$$\n\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] = \\mathbb{E}_{X^\\star} \\left[ \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(X^\\star - \\bar{X})^2}{S_{XX}} \\right) \\right]\n$$\n$$\n= \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} \\right) + \\frac{\\hat{\\sigma}^2}{S_{XX}} \\mathbb{E}_{X^\\star}[(X^\\star - \\bar{X})^2]\n$$\nThe expectation $\\mathbb{E}[(X^\\star - \\bar{X})^2]$ can be expanded as $\\mathbb{E}[(X^\\star - x_{\\mathrm{obs}} + x_{\\mathrm{obs}} - \\bar{X})^2] = \\mathbb{E}[(X^\\star-x_{\\mathrm{obs}})^2] + (x_{\\mathrm{obs}}-\\bar{X})^2 = s_x^2 + (x_{\\mathrm{obs}}-\\bar{X})^2$.\nThus, the first term of the total variance is:\n$$\n\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{s_x^2 + (x_{\\mathrm{obs}} - \\bar{X})^2}{S_{XX}} \\right)\n$$\n\nThe second term, $\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$, represents the uncertainty in the mean prediction due to the uncertainty in the predictor $X^\\star$.\n$$\n\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]] = \\mathrm{Var}_{X^\\star}[\\hat{a} + \\hat{b}\\,X^\\star] = \\hat{b}^2\\,\\mathrm{Var}[X^\\star]\n$$\nSince $\\mathrm{Var}[X^\\star] = s_x^2$, this term is:\n$$\n\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]] = \\hat{b}^2 s_x^2\n$$\n\nCombining both terms, the total predictive variance, $\\sigma^2_{Y^\\star}$, is:\n$$\n\\sigma^2_{Y^\\star} = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(x_{\\mathrm{obs}} - \\bar{X})^2 + s_x^2}{S_{XX}} \\right) + \\hat{b}^2 s_x^2\n$$\nThe constrained predictive standard deviation is $\\sigma_{Y^\\star} = \\sqrt{\\sigma^2_{Y^\\star}}$.\n\n**3. Unconstrained Uncertainty and Reduction Factor**\n\nThe unconstrained projection uncertainty is defined as the spread of the future response variable $Y$ across the raw model ensemble, quantified by the sample standard deviation:\n$$\ns_Y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (Y_i - \\bar{Y})^2}\n$$\nThe reduction factor, $R$, quantifies the benefit of the emergent constraint. It is the ratio of the constrained predictive standard deviation to the unconstrained ensemble standard deviation:\n$$\nR = \\frac{\\sigma_{Y^\\star}}{s_Y}\n$$\nA value of $R < 1$ indicates that the constraint has successfully reduced the projection uncertainty.\n\nThe implementation will follow these derived formulas to calculate $(\\mu_{Y^\\star}, \\sigma_{Y^\\star}, R)$ for each provided test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes constrained projections and uncertainty reduction for a series of test cases\n    based on the emergent constraint methodology.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": [0.2, 0.4, 0.6, 0.8, 1.0, 0.3, 0.7, 0.9],\n            \"Y\": [0.98, 1.25, 1.73, 2.08, 2.51, 1.10, 1.86, 2.32],\n            \"x_obs\": 0.65,\n            \"s_x\": 0.05,\n        },\n        {\n            \"X\": [0.1, 0.2, 0.5, 0.6, 0.8, 0.3, 0.4, 0.7, 0.9, 0.55],\n            \"Y\": [0.71, 1.27, 0.95, 1.26, 0.93, 1.08, 0.99, 1.17, 0.89, 1.205],\n            \"x_obs\": 0.50,\n            \"s_x\": 0.05,\n        },\n        {\n            \"X\": [0.2, 0.3, 0.4, 0.5, 0.6],\n            \"Y\": [0.0, 0.05, 0.22, 0.27, 0.41],\n            \"x_obs\": 0.40,\n            \"s_x\": 0.00,\n        },\n        {\n            \"X\": [0.2, 0.5, 0.7, 1.0, 0.3, 0.9],\n            \"Y\": [0.70, 0.85, 1.40, 1.65, 0.65, 1.65],\n            \"x_obs\": 0.60,\n            \"s_x\": 0.20,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        X_data = np.array(case[\"X\"])\n        Y_data = np.array(case[\"Y\"])\n        x_obs = case[\"x_obs\"]\n        s_x = case[\"s_x\"]\n\n        n = len(X_data)\n        \n        # Step 1: OLS Estimation\n        X_mean = np.mean(X_data)\n        Y_mean = np.mean(Y_data)\n\n        S_xy = np.sum((X_data - X_mean) * (Y_data - Y_mean))\n        S_xx = np.sum((X_data - X_mean)**2)\n        \n        if S_xx == 0:\n            # Handle degenerate case where all X values are the same.\n            # This should not occur in the given test cases.\n            # In this scenario, b_hat is undefined and correlation is meaningless.\n            # For a valid solution, we would need to flag this as problematic.\n            # This simple handling prevents division by zero.\n            b_hat = 0\n            a_hat = Y_mean\n            sigma_hat_sq = np.var(Y_data, ddof=1) if n > 1 else 0\n        else:\n            b_hat = S_xy / S_xx\n            a_hat = Y_mean - b_hat * X_mean\n\n            Y_pred_model = a_hat + b_hat * X_data\n            residuals = Y_data - Y_pred_model\n            SSR = np.sum(residuals**2)\n            # Degrees of freedom are n-2 for simple linear regression\n            sigma_hat_sq = SSR / (n - 2)\n            \n        # Step 2: Calculate constrained predictive mean and variance\n        pred_mean_Y = a_hat + b_hat * x_obs\n        \n        # The total predictive variance calculation requires S_xx > 0\n        if S_xx > 0:\n            term1 = sigma_hat_sq * (1 + 1/n + ((x_obs - X_mean)**2 + s_x**2) / S_xx)\n            term2 = b_hat**2 * s_x**2\n            pred_var_Y = term1 + term2\n        else: # If S_xx=0, we cannot use the constraint. Uncertainty is just the sample variance.\n            pred_var_Y = np.var(Y_data, ddof=1) if n > 1 else 0\n\n        pred_std_Y = np.sqrt(pred_var_Y)\n        \n        # Step 3: Quantify uncertainty reduction\n        unconstrained_std_Y = np.std(Y_data, ddof=1)\n        \n        if unconstrained_std_Y > 0:\n            reduction_factor = pred_std_Y / unconstrained_std_Y\n        else:\n            # If original data has no spread, the concept of reduction is ill-defined.\n            reduction_factor = 1.0 if pred_std_Y == 0 else float('inf')\n\n        results.append([pred_mean_Y, pred_std_Y, reduction_factor])\n\n    # Final print statement in the exact required format.\n    # Format each float to 6 decimal places and then join.\n    formatted_results = []\n    for res_list in results:\n        formatted_list_str = f\"[{','.join([f'{val:.6f}' for val in res_list])}]\"\n        formatted_results.append(formatted_list_str)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}