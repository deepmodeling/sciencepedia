{
    "hands_on_practices": [
        {
            "introduction": "A central pillar of early warning signal theory is the concept of \"critical slowing down,\" where a system takes progressively longer to recover from small perturbations as it approaches a tipping point. This exercise  provides a direct, quantitative link between a statistical indicator from a discrete time series, the lag-1 autocorrelation coefficient $\\rho_1$, and the physical recovery timescale $\\tau$ of the underlying continuous system. By working through this problem, you will solidify your understanding of how an increase in autocorrelation serves as a direct measure of a system losing its resilience.",
            "id": "4033204",
            "problem": "A climate subsystem near a stable equilibrium, as represented in Numerical Weather Prediction (NWP) and climate modeling, can be locally approximated by linear relaxation dynamics with stochastic forcing, which in continuous time is modeled as an Ornstein–Uhlenbeck (OU) process. Under this approximation, the e-folding recovery time $\\tau$ characterizes the rate at which perturbations decay back to equilibrium. For a time series sampled discretely at interval $\\Delta t$, the corresponding lag-one autocorrelation coefficient $\\rho_1$ of the sampled series is related to the continuous-time decay by the identity $\\rho_1 = \\exp(-\\Delta t/\\tau)$, implying $\\tau = -\\Delta t/\\ln \\rho_1$. In early warning signal analysis for transitions, an increasing $\\rho_1$ indicates critical slowing down and a growing $\\tau$.\n\nConsider a daily-resolved climate index time series ($\\Delta t = 1$ day) whose estimated lag-one autocorrelation coefficient $\\rho_1(t)$ increases approximately linearly over time due to slow external forcing: $\\rho_1(t) \\approx \\rho_{1,0} + s t$, where $t$ is in years, $\\rho_{1,0} = 0.95$, and the trend $s = 0.004\\,\\text{year}^{-1}$ persists uniformly over $T = 10$ years. Assume $0  \\rho_1(t)  1$ throughout the interval, consistent with a stationary Autoregressive (AR) model of order one ($\\text{AR}(1)$).\n\nUsing the relationship $\\tau = -\\Delta t/\\ln \\rho_1$, compute the expected change in e-folding recovery time $\\Delta \\tau = \\tau(T) - \\tau(0)$ over the $T = 10$ years. Express your final answer in days and round your answer to four significant figures.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- The relationship between e-folding recovery time $\\tau$, lag-one autocorrelation coefficient $\\rho_1$, and sampling interval $\\Delta t$ is given by $\\rho_1 = \\exp(-\\Delta t/\\tau)$, which implies $\\tau = -\\Delta t/\\ln \\rho_1$.\n- The time series is daily-resolved, so the sampling interval is $\\Delta t = 1$ day.\n- The lag-one autocorrelation coefficient $\\rho_1$ varies with time $t$ (in years) according to the linear model $\\rho_1(t) \\approx \\rho_{1,0} + s t$.\n- The initial autocorrelation at $t=0$ is $\\rho_{1,0} = 0.95$.\n- The trend in autocorrelation is $s = 0.004\\,\\text{year}^{-1}$.\n- The total duration over which the change is to be computed is $T = 10$ years.\n- An assumption is provided: $0  \\rho_1(t)  1$ for the duration of the analysis.\n- The objective is to compute the change in e-folding recovery time, $\\Delta\\tau = \\tau(T) - \\tau(0)$, and express the result in days, rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is grounded in the established theory of early warning signals for critical transitions, specifically the concept of critical slowing down. The Ornstein-Uhlenbeck process is the canonical model for a stochastically forced linear system, and the relationship between its continuous-time decay rate and the discrete-time autocorrelation is standard and correct. The application to climate modeling is appropriate and realistic.\n- **Well-Posedness**: The problem provides all necessary parameters and a clear objective. A unique solution can be determined through direct calculation.\n- **Objectivity**: The problem is stated using precise, quantitative, and unbiased language.\n- **Completeness and Consistency**: The problem is self-contained. The units are consistent: $\\rho_1(t)$ is dimensionless as $\\rho_{1,0}$ is dimensionless and the product $st$ has units of $(\\text{year}^{-1}) \\times (\\text{year})$, which is also dimensionless. The formula for $\\tau$ will yield a result in the units of $\\Delta t$ (days), which matches the requirement for the final answer. The assumption $0  \\rho_1(t)  1$ is checked: at $t=0$, $\\rho_1(0) = 0.95$; at $t=T=10$, $\\rho_1(10) = 0.95 + 0.004 \\times 10 = 0.99$. Since $\\rho_1(t)$ is a linear function of $t$, it remains within the interval $(0, 1)$ for all $t$ in $[0, 10]$.\n- **Other Flaws**: The problem is not unrealistic, ill-posed, trivial, or unverifiable.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\nThe e-folding recovery time $\\tau$ is given as a function of the lag-one autocorrelation coefficient $\\rho_1$:\n$$ \\tau(\\rho_1) = -\\frac{\\Delta t}{\\ln \\rho_1} $$\nThe lag-one autocorrelation coefficient $\\rho_1$ is itself a function of time $t$:\n$$ \\rho_1(t) = \\rho_{1,0} + s t $$\nTherefore, the recovery time $\\tau$ as a function of time $t$ can be written as:\n$$ \\tau(t) = -\\frac{\\Delta t}{\\ln(\\rho_{1,0} + s t)} $$\nThe goal is to compute the change in recovery time, $\\Delta\\tau$, over the interval from $t=0$ to $t=T$.\n$$ \\Delta\\tau = \\tau(T) - \\tau(0) $$\n\nFirst, we calculate the recovery time at the initial time, $t=0$.\nThe autocorrelation at $t=0$ is:\n$$ \\rho_1(0) = \\rho_{1,0} + s(0) = \\rho_{1,0} = 0.95 $$\nThe corresponding recovery time is:\n$$ \\tau(0) = -\\frac{\\Delta t}{\\ln(\\rho_1(0))} = -\\frac{\\Delta t}{\\ln(0.95)} $$\n\nNext, we calculate the recovery time at the final time, $t=T=10$ years.\nThe autocorrelation at $t=10$ is:\n$$ \\rho_1(10) = \\rho_{1,0} + sT = 0.95 + (0.004 \\,\\text{year}^{-1})(10 \\,\\text{years}) = 0.95 + 0.04 = 0.99 $$\nThe corresponding recovery time is:\n$$ \\tau(10) = -\\frac{\\Delta t}{\\ln(\\rho_1(10))} = -\\frac{\\Delta t}{\\ln(0.99)} $$\n\nNow, we compute the change $\\Delta\\tau$:\n$$ \\Delta\\tau = \\tau(10) - \\tau(0) = \\left(-\\frac{\\Delta t}{\\ln(0.99)}\\right) - \\left(-\\frac{\\Delta t}{\\ln(0.95)}\\right) $$\n$$ \\Delta\\tau = \\Delta t \\left( \\frac{1}{\\ln(0.95)} - \\frac{1}{\\ln(0.99)} \\right) $$\nWe are given that the sampling interval is $\\Delta t = 1$ day. Substituting this value:\n$$ \\Delta\\tau = (1 \\text{ day}) \\left( \\frac{1}{\\ln(0.95)} - \\frac{1}{\\ln(0.99)} \\right) $$\nTo obtain the numerical value, we use the natural logarithms of the arguments:\n$$ \\ln(0.95) \\approx -0.05129329438 $$\n$$ \\ln(0.99) \\approx -0.01005033585 $$\nLet's compute the individual recovery times first, as this illustrates the physical meaning of \"slowing down\".\n$$ \\tau(0) = -\\frac{1}{\\ln(0.95)} \\approx -\\frac{1}{-0.05129329} \\approx 19.49576 \\text{ days} $$\n$$ \\tau(10) = -\\frac{1}{\\ln(0.99)} \\approx -\\frac{1}{-0.01005034} \\approx 99.49916 \\text{ days} $$\nThe change $\\Delta\\tau$ is the difference between these two values:\n$$ \\Delta\\tau = \\tau(10) - \\tau(0) \\approx 99.49916 - 19.49576 \\approx 80.0034 \\text{ days} $$\nThe problem requires the final answer to be rounded to four significant figures. The value $80.0034$ has more than four significant figures. The first four are $8$, $0$, $0$, and $0$. The fifth digit is $3$, which is less than $5$, so we round down.\n$$ \\Delta\\tau \\approx 80.00 \\text{ days} $$\nThe expected change in e-folding recovery time over the $10$-year period is $80.00$ days.",
            "answer": "$$\n\\boxed{80.00}\n$$"
        },
        {
            "introduction": "In an idealized model, an increasing autocorrelation unambiguously signals critical slowing down; however, real-world climate data are rarely stationary due to persistent external forcings that manifest as trends. This essential practice  challenges you to derive analytically how a simple linear trend can create a spurious increase in estimated autocorrelation, potentially mimicking an early warning signal. Mastering this concept is crucial for learning to distinguish true dynamical signals from artifacts of non-stationarity, a fundamental step in any rigorous EWS analysis.",
            "id": "4033195",
            "problem": "Consider a forced climate time series used in numerical weather prediction and climate modeling in which the observable, such as global-mean surface temperature anomaly, is the sum of a deterministic forced response and internal variability. Let the observable be modeled as $y_t = \\mu_0 + \\beta t + x_t$ for $t = 1, 2, \\dots, N$, where $\\mu_0$ is a constant, $\\beta$ is a constant linear trend due to external forcing, and $x_t$ is a zero-mean stationary first-order autoregressive process $\\mathrm{AR}(1)$ satisfying $x_t = \\phi x_{t-1} + \\varepsilon_t$ with $|\\phi|  1$ and $\\varepsilon_t$ independent and identically distributed Gaussian noise with variance $\\sigma_{\\varepsilon}^2$. Denote the stationary variance of $x_t$ by $\\sigma_x^2$.\n\nEarly warning signals (EWS) for transitions often monitor changes in the lag-$1$ autocorrelation, here denoted $\\rho_1$. In operational practice, a common estimator of $\\rho_1$ is computed from the mean-removed but not detrended series:\n$$\n\\hat{\\rho}_1 \\equiv \\frac{\\sum_{t=2}^{N} \\left(y_t - \\bar{y}\\right)\\left(y_{t-1} - \\bar{y}\\right)}{\\sum_{t=1}^{N} \\left(y_t - \\bar{y}\\right)^{2}},\n$$\nwhere $\\bar{y} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} y_t$ is the sample mean. Assume $x_t$ is independent of the deterministic trend component and that $N \\geq 4$.\n\nStarting from the definitions above and standard properties of the $\\mathrm{AR}(1)$ process and time-averages of deterministic sequences, derive a closed-form analytic expression for the large-sample expected bias of $\\hat{\\rho}_1$ when the linear trend is neglected, defined as $\\mathrm{Bias} \\equiv \\mathbb{E}[\\hat{\\rho}_1] - \\phi$, in terms of $N$, $\\beta$, $\\phi$, and $\\sigma_x^2$. In your derivation, use the leading-order large-$N$ approximations $\\mathbb{E}\\!\\left[\\sum_{t=1}^{N} \\left(x_t - \\bar{x}\\right)^{2}\\right] \\approx N \\sigma_x^2$ and $\\mathbb{E}\\!\\left[\\sum_{t=2}^{N} \\left(x_t - \\bar{x}\\right)\\left(x_{t-1} - \\bar{x}\\right)\\right] \\approx (N-1)\\phi \\sigma_x^2$, where $\\bar{x} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} x_t$. Also, use exact closed-form sums for the linear deterministic component.\n\nExplain briefly, based on your derivation, why detrending is essential for EWS detection in forced climate systems. Express the final bias as a single closed-form analytic expression. No rounding is required, and no units are needed in your final answer.",
            "solution": "The observable is $y_t = m_t + x_t$ with deterministic mean $m_t \\equiv \\mu_0 + \\beta t$ and stochastic variability $x_t$. The estimator of lag-$1$ autocorrelation is\n$$\n\\hat{\\rho}_1 = \\frac{\\sum_{t=2}^{N} \\left[(m_t-\\bar{m}) + (x_t-\\bar{x})\\right]\\left[(m_{t-1}-\\bar{m}) + (x_{t-1}-\\bar{x})\\right]}{\\sum_{t=1}^{N} \\left[(m_t-\\bar{m}) + (x_t-\\bar{x})\\right]^{2}},\n$$\nwhere $\\bar{m} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} m_t$ and $\\bar{x} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} x_t$. Because $m_t$ is deterministic and independent of $x_t$, the expectation over the stochastic component eliminates cross terms of the form $(m_t-\\bar{m})(x_{t-1}-\\bar{x})$ and $(x_t-\\bar{x})(m_{t-1}-\\bar{m})$, yielding\n$$\n\\mathbb{E}\\!\\left[\\sum_{t=2}^{N} (y_t-\\bar{y})(y_{t-1}-\\bar{y})\\right] = \\sum_{t=2}^{N} (m_t-\\bar{m})(m_{t-1}-\\bar{m}) + \\mathbb{E}\\!\\left[\\sum_{t=2}^{N} (x_t-\\bar{x})(x_{t-1}-\\bar{x})\\right],\n$$\nand\n$$\n\\mathbb{E}\\!\\left[\\sum_{t=1}^{N} (y_t-\\bar{y})^{2}\\right] = \\sum_{t=1}^{N} (m_t-\\bar{m})^{2} + \\mathbb{E}\\!\\left[\\sum_{t=1}^{N} (x_t-\\bar{x})^{2}\\right].\n$$\n\nWe compute the deterministic sums exactly. The sample mean of the linear sequence $m_t = \\mu_0 + \\beta t$ is $\\bar{m} = \\mu_0 + \\beta \\frac{N+1}{2}$. Define the centered deterministic sequence\n$$\nd_t \\equiv m_t - \\bar{m} = \\beta\\left(t - \\frac{N+1}{2}\\right).\n$$\nThen\n$$\n\\sum_{t=1}^{N} d_t^{2} = \\beta^2 \\sum_{t=1}^{N} \\left(t - \\frac{N+1}{2}\\right)^{2} = \\beta^2\\,\\frac{N\\left(N^2-1\\right)}{12},\n$$\na standard result for the sum of squares about the mean of the integers $1,\\dots,N$.\n\nFor the lag-$1$ deterministic covariance sum, note that\n$$\nd_t\\,d_{t-1} = \\beta^2\\left(t-\\frac{N+1}{2}\\right)\\left[(t-1)-\\frac{N+1}{2}\\right] = \\beta^2\\left[\\left(t-\\frac{N+1}{2}\\right)^{2} - \\left(t-\\frac{N+1}{2}\\right)\\right].\n$$\nTherefore,\n\\begin{align*}\n\\sum_{t=2}^{N} d_t\\,d_{t-1}\n= \\beta^2\\left[\\sum_{t=2}^{N} \\left(t-\\frac{N+1}{2}\\right)^{2} - \\sum_{t=2}^{N} \\left(t-\\frac{N+1}{2}\\right)\\right] \\\\\n= \\beta^2\\left[\\sum_{t=1}^{N} \\left(t-\\frac{N+1}{2}\\right)^{2} - \\left(1-\\frac{N+1}{2}\\right)^{2} - \\left(\\sum_{t=1}^{N} \\left(t-\\frac{N+1}{2}\\right) - \\left(1-\\frac{N+1}{2}\\right)\\right)\\right].\n\\end{align*}\nSince $\\sum_{t=1}^{N} \\left(t-\\frac{N+1}{2}\\right) = 0$, the second bracket simplifies to $+\\left(1-\\frac{N+1}{2}\\right)$. Using $\\left(1-\\frac{N+1}{2}\\right) = -\\frac{N-1}{2}$ and $\\left(1-\\frac{N+1}{2}\\right)^{2} = \\frac{(N-1)^{2}}{4}$, we obtain\n\\begin{align*}\n\\sum_{t=2}^{N} d_t\\,d_{t-1}\n= \\beta^2\\left[\\frac{N\\left(N^2-1\\right)}{12} - \\frac{(N-1)^2}{4} - \\frac{N-1}{2}\\right] \\\\\n= \\beta^2\\left[\\frac{N\\left(N^2-1\\right)}{12} - \\frac{N^2-1}{4}\\right] \\\\\n= \\beta^2\\,\\frac{(N^2-1)(N-3)}{12}.\n\\end{align*}\n\nNext, we use the large-$N$ expectations for the stochastic sums. For an $\\mathrm{AR}(1)$ process with lag-$1$ autocorrelation $\\phi$ and stationary variance $\\sigma_x^2$, the leading-order approximations are\n$$\n\\mathbb{E}\\!\\left[\\sum_{t=1}^{N} (x_t-\\bar{x})^{2}\\right] \\approx N\\,\\sigma_x^2, \\qquad \\mathbb{E}\\!\\left[\\sum_{t=2}^{N} (x_t-\\bar{x})(x_{t-1}-\\bar{x})\\right] \\approx (N-1)\\,\\phi\\,\\sigma_x^2.\n$$\n\nCombining deterministic and stochastic contributions, the expected numerator and denominator of $\\hat{\\rho}_1$ are\n\\begin{align*}\n\\mathbb{E}[\\text{num}] = \\beta^2\\,\\frac{(N^2-1)(N-3)}{12} + (N-1)\\,\\phi\\,\\sigma_x^2, \\\\\n\\mathbb{E}[\\text{den}] = \\beta^2\\,\\frac{N\\left(N^2-1\\right)}{12} + N\\,\\sigma_x^2.\n\\end{align*}\nThus,\n$$\n\\mathbb{E}[\\hat{\\rho}_1] \\approx \\frac{\\beta^2\\,\\frac{(N^2-1)(N-3)}{12} + (N-1)\\,\\phi\\,\\sigma_x^2}{\\beta^2\\,\\frac{N\\left(N^2-1\\right)}{12} + N\\,\\sigma_x^2}.\n$$\nThe bias relative to the true lag-$1$ autocorrelation $\\phi$ is\n\\begin{align*}\n\\mathrm{Bias} \\equiv \\mathbb{E}[\\hat{\\rho}_1] - \\phi\n= \\frac{\\beta^2\\,\\frac{(N^2-1)(N-3)}{12} + (N-1)\\,\\phi\\,\\sigma_x^2}{\\beta^2\\,\\frac{N\\left(N^2-1\\right)}{12} + N\\,\\sigma_x^2} - \\phi \\\\\n= \\frac{\\beta^2\\,\\frac{(N^2-1)(N-3)}{12} - \\phi\\,\\beta^2\\,\\frac{N\\left(N^2-1\\right)}{12} + (N-1)\\,\\phi\\,\\sigma_x^2 - \\phi\\,N\\,\\sigma_x^2}{\\beta^2\\,\\frac{N\\left(N^2-1\\right)}{12} + N\\,\\sigma_x^2} \\\\\n= \\frac{\\beta^2\\,\\frac{(N^2-1)}{12}\\left[(N-3) - \\phi N\\right] - \\phi\\,\\sigma_x^2}{N\\left(\\beta^2\\,\\frac{(N^2-1)}{12} + \\sigma_x^2\\right)}.\n\\end{align*}\n\nThis expression makes clear why detrending is essential for early warning signals detection in forced climate systems. As $|\\beta|$ increases or $N$ grows, the deterministic trend contributes strongly to both the numerator and the denominator. In the limit where the deterministic trend dominates the variance (i.e., $\\beta^2\\,\\frac{(N^2-1)}{12} \\gg \\sigma_x^2$), the expected estimator approaches the deterministic ratio $\\frac{(N-3)}{N}$, which is close to $1$ for large $N$, falsely signaling critical slowing down even if the true $\\phi$ is modest. Detrending removes the deterministic contribution, restoring an unbiased estimate of $\\rho_1$ attributable to internal variability alone and preventing spurious EWS detection driven by external forcing rather than proximity to a dynamical transition.",
            "answer": "$$\\boxed{\\frac{\\beta^{2}\\,\\frac{(N^{2}-1)}{12}\\left[(N-3) - \\phi N\\right] - \\phi\\,\\sigma_{x}^{2}}{N\\left(\\beta^{2}\\,\\frac{(N^{2}-1)}{12} + \\sigma_{x}^{2}\\right)}}$$"
        },
        {
            "introduction": "Detecting a trend in autocorrelation is only the first step; the critical next question is whether that trend is statistically significant or simply a product of random chance within a stationary process. This hands-on coding exercise  guides you through implementing a robust, non-parametric significance test using surrogate data. By generating a null distribution that preserves the autocorrelation structure (i.e., the \"color\") of the noise, you will learn to calculate the false positive rate and build a reliable framework for assessing the confidence of any detected early warning signal.",
            "id": "4033207",
            "problem": "You are tasked with implementing a numerical experiment to quantify the false positive rate for detecting an increasing trend in lag-$1$ autocorrelation, which is a commonly used early warning signal in climate modeling and numerical weather prediction, under the null hypothesis of stationary colored noise. The detection procedure must use surrogate data generated by phase randomization to construct a nonparametric null distribution for the trend statistic.\n\nBase your work on the following well-tested definitions and facts:\n- A stationary first-order autoregressive process (AR($1$)) is given by $x_t = \\phi x_{t-1} + \\epsilon_t$, where $|\\phi|  1$ ensures stationarity and $\\epsilon_t$ are independent and identically distributed Gaussian innovations with zero mean.\n- The discrete Fourier transform decomposes a real-valued sequence into amplitude and phase. Randomizing phases while preserving amplitudes and enforcing Hermitian symmetry yields a real surrogate series with the same power spectrum but randomized phase, which is a standard method for generating surrogates under the null of a stationary process.\n- The sample lag-$1$ autocorrelation in a window of size $w$ is defined as $r_1 = \\frac{\\sum_{i=1}^{w-1} (x_i - \\bar{x})(x_{i+1} - \\bar{x})}{\\sum_{i=1}^{w} (x_i - \\bar{x})^2}$, where $\\bar{x}$ is the sample mean over the window.\n- Kendall rank correlation coefficient (Kendall $\\tau$) between two sequences of equal length measures the strength of monotonic association and is defined based on the difference between the counts of concordant and discordant pairs, normalized to lie in $[-1,1]$.\n\nYour program must do the following for each test case:\n$1.$ Generate $R$ independent realizations of a stationary AR($1$) process of length $N$ with parameter $\\phi$ and Gaussian innovations of unit variance. Standardize each realization to have zero mean and unit variance before analysis.\n$2.$ For each realization, compute a sequence of rolling-window lag-$1$ autocorrelations using windows of length $w$ advancing by a fixed step $\\Delta$ (denoted as “step”). Let there be $M$ windows and let $a_j$ denote the lag-$1$ autocorrelation computed in window $j$, for $j = 1, 2, \\dots, M$. Compute Kendall $\\tau$ between the index sequence $\\{1, 2, \\dots, M\\}$ and the sequence $\\{a_j\\}_{j=1}^M$, denoted $\\tau_{\\mathrm{obs}}$.\n$3.$ For the same realization, generate $S$ surrogate series by phase randomization:\n- Compute the real-valued discrete Fourier transform using the real-to-complex transform, preserve the amplitude spectrum, draw independent phases uniformly from $[0, 2\\pi)$ for all positive frequencies, set the phase of the zero-frequency and Nyquist components (when present) to zero, and invert to obtain a real-valued surrogate. Standardize each surrogate to zero mean and unit variance.\n- For each surrogate, compute the rolling-window lag-$1$ autocorrelation sequence and its Kendall $\\tau$ in the same way as for the original series, yielding $\\{\\tau_s^{(k)}\\}_{k=1}^S$.\n$4.$ Perform a one-sided test for an increasing trend by comparing $\\tau_{\\mathrm{obs}}$ to the empirical $(1 - \\alpha)$ quantile of the surrogate distribution $\\{\\tau_s^{(k)}\\}_{k=1}^S$. Declare a detection if $\\tau_{\\mathrm{obs}}$ is strictly greater than this quantile.\n$5.$ Compute the false positive rate as the fraction of the $R$ realizations declared as detections, which estimates the probability of a spurious detection under the null hypothesis. Report this as a real number in $[0,1]$.\n\nAngle units are not involved. There are no physical units to report. All probabilities must be expressed as decimals.\n\nTest suite:\nFor each of the following parameter sets $(\\phi, N, w, \\Delta, R, S, \\alpha)$, compute the false positive rate as specified above. Use a fixed random seed so that the results are reproducible.\n- Case $1$: $(0, 512, 64, 4, 30, 100, 0.1)$.\n- Case $2$: $(0.5, 512, 64, 4, 30, 100, 0.1)$.\n- Case $3$: $(0.9, 512, 64, 4, 30, 100, 0.1)$.\n- Case $4$: $(0.5, 256, 128, 16, 20, 80, 0.05)$.\n\nImplementation constraints:\n- Standardize each time series (original and surrogates) prior to computing rolling autocorrelations.\n- If Kendall $\\tau$ is not defined due to degeneracy (for example, a constant sequence), treat it as zero for the purpose of the test.\n- Use a fixed random seed of $2025$ for all random number generation.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of floats rounded to three decimals and enclosed in square brackets, corresponding respectively to Cases $1$ through $4$ (for example, \"[$0.103,0.087,0.112,0.054$]\").",
            "solution": "The user has provided a scientifically sound and well-posed problem. The request is to implement a numerical experiment to estimate the false positive rate of a statistical test for an increasing trend in lag-$1$ autocorrelation, a common early warning signal for critical transitions. The test is performed under the null hypothesis of a stationary process, specifically a first-order autoregressive (AR($1$)) model. The methodology involves generating surrogate data via phase randomization to construct a nonparametric null distribution.\n\nThe problem is valid as it is grounded in standard principles of time series analysis and statistical hypothesis testing, provides a complete and unambiguous set of instructions and parameters, and is computationally feasible. All terms are clearly defined, and the procedure is logically consistent.\n\nThe solution will be constructed by systematically implementing the steps described.\n\n**1. Generation of the Null Hypothesis Data**\n\nThe null hypothesis is that the observed time series is a realization of a stationary, linear stochastic process. We model this using a stationary first-order autoregressive (AR($1$)) process for each of the $R$ realizations. The AR($1$) process is defined by the equation:\n$$x_t = \\phi x_{t-1} + \\epsilon_t$$\nwhere $t$ is the time index, $\\phi$ is the autoregressive coefficient ($|\\phi|  1$ for stationarity), and $\\epsilon_t$ is a sequence of independent and identically distributed (i.i.d.) random variables drawn from a standard normal distribution, $\\mathcal{N}(0, 1)$. For each realization of length $N$, we generate the time series and then standardize it to have a mean of $0$ and a standard deviation of $1$. This standardization is a crucial step to ensure that subsequent statistics are not influenced by the arbitrary mean or variance of the original realization.\n\n**2. Computation of the Observed Trend Statistic, $\\tau_{\\mathrm{obs}}$**\n\nFor each standardized realization, we analyze the trend in its lag-$1$ autocorrelation. This is done using a rolling window approach.\nFirst, we slide a window of a fixed size $w$ across the time series, advancing by a step size of $\\Delta$. The number of windows, $M$, is given by $M = \\lfloor (N - w) / \\Delta \\rfloor + 1$.\nIn each window $j = 1, \\dots, M$, we compute the sample lag-$1$ autocorrelation, $a_j$. As specified, the formula is:\n$$r_1 = \\frac{\\sum_{i=1}^{w-1} (x_i - \\bar{x})(x_{i+1} - \\bar{x})}{\\sum_{i=1}^{w} (x_i - \\bar{x})^2}$$\nwhere $\\bar{x}$ is the mean of the data points within that specific window. This calculation yields a new time series of autocorrelation values, $\\{a_j\\}_{j=1}^M$.\n\nNext, we quantify the monotonic trend in this autocorrelation series $\\{a_j\\}$. We use the Kendall rank correlation coefficient, $\\tau$, which measures the ordinal association between the window indices $\\{1, 2, \\dots, M\\}$ and the corresponding autocorrelation values $\\{a_j\\}_{j=1}^M$. A positive $\\tau$ indicates an increasing trend. This value is our observed test statistic, $\\tau_{\\mathrm{obs}}$. If the autocorrelation sequence is constant, leading to a degenerate case, $\\tau$ is treated as $0$.\n\n**3. Generation of the Surrogate Null Distribution**\n\nTo assess the statistical significance of $\\tau_{\\mathrm{obs}}$, we compare it against a distribution of $\\tau$ values expected under the null hypothesis. This null distribution is generated empirically using surrogate data. For each of the $R$ original realizations, we generate $S$ surrogate time series.\n\nThe surrogates are created using the phase randomization technique. This method preserves the power spectrum (and thus the autocorrelation structure) of the original series while destroying any non-linear or non-stationary features by randomizing the phase information in the frequency domain. The procedure is:\na. Compute the real-to-complex Discrete Fourier Transform (DFT) of the standardized time series. This yields a set of complex coefficients, each with an amplitude and a phase.\nb. The amplitudes are kept unchanged.\nc. New phases are drawn independently from a uniform distribution on $[0, 2\\pi)$.\nd. To ensure the resulting inverse-transformed series is real-valued, the phase for the zero-frequency (DC) component is set to $0$. If the series length $N$ is even, the phase for the Nyquist frequency is also set to $0$.\ne. New complex Fourier coefficients are constructed from the original amplitudes and a new set of random phases.\nf. The inverse DFT is applied to these new coefficients to obtain a real-valued surrogate series.\ng. Each of the $S$ resulting surrogate series is then standardized to have zero mean and unit variance.\n\nFor each surrogate, we repeat the procedure from Step 2: compute the rolling-window lag-$1$ autocorrelation sequence and its Kendall $\\tau$ statistic, denoted $\\tau_s^{(k)}$ for $k=1, \\dots, S$.\n\n**4. Hypothesis Testing**\n\nThe collection of surrogate statistics, $\\{\\tau_s^{(k)}\\}_{k=1}^S$, forms an empirical null distribution for the trend statistic. We perform a one-sided hypothesis test to determine if the observed trend $\\tau_{\\mathrm{obs}}$ is significantly positive. We find the critical value from the surrogate distribution, which is its $(1-\\alpha)$ quantile, where $\\alpha$ is the desired significance level. A detection (i.e., a rejection of the null hypothesis) is declared if $\\tau_{\\mathrm{obs}}$ is strictly greater than this critical value.\n\n**5. False Positive Rate Calculation**\n\nThis entire procedure (Steps 1 through 4) is repeated for $R$ independent realizations of the AR($1$) process. Since all these realizations are, by construction, from the stationary null model, any declared detection is a false positive (a Type I error). The false positive rate (FPR) is estimated as the total number of detections divided by the total number of realizations, $R$.\n$$\\text{FPR} = \\frac{\\text{Number of detections}}{R}$$\nThis value should, in theory, be close to the chosen significance level $\\alpha$. The algorithm will compute this FPR for each given parameter set. For reproducibility, all random number generation will be initialized from a single fixed seed.",
            "answer": "```python\nimport numpy as np\nimport scipy.stats\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment for all test cases.\n    \"\"\"\n\n    def generate_ar1(phi, N, rng):\n        \"\"\"\n        Generates a single realization of a stationary AR(1) process.\n        x_t = phi * x_{t-1} + epsilon_t\n        \"\"\"\n        epsilon = rng.normal(loc=0.0, scale=1.0, size=N)\n        x = np.zeros(N)\n        # No burn-in specified, start with x[0] = epsilon[0]\n        x[0] = epsilon[0]\n        for t in range(1, N):\n            x[t] = phi * x[t - 1] + epsilon[t]\n        return x\n\n    def standardize(series):\n        \"\"\"\n        Standardizes a time series to have zero mean and unit variance.\n        \"\"\"\n        s_std = np.std(series)\n        if s_std == 0:\n            # For a constant series, return a zero series.\n            return series - np.mean(series)\n        return (series - np.mean(series)) / s_std\n\n    def rolling_autocorr(series, w, delta):\n        \"\"\"\n        Computes rolling-window lag-1 autocorrelation based on the problem's formula.\n        \"\"\"\n        N = len(series)\n        autocorrs = []\n        # Iterate over the starting index of each window\n        for i in range(0, N - w + 1, delta):\n            window = series[i : i + w]\n            \n            # Use specified formula for lag-1 autocorrelation\n            mean_w = np.mean(window)\n            var_w = np.sum((window - mean_w)**2)\n\n            if var_w == 0:\n                autocorrs.append(0.0)\n                continue\n\n            # Numerator: sum_{i=1 to w-1} (x_i - x_bar)(x_{i+1} - x_bar)\n            x_lagged = window[:-1]\n            x_shifted = window[1:]\n            numerator = np.sum((x_lagged - mean_w) * (x_shifted - mean_w))\n            \n            autocorrs.append(numerator / var_w)\n            \n        return np.array(autocorrs)\n\n    def calculate_tau(series, w, delta):\n        \"\"\"\n        Calculates Kendall's tau for the trend in a rolling autocorrelation sequence.\n        \"\"\"\n        autocorrs = rolling_autocorr(series, w, delta)\n        M = len(autocorrs)\n        \n        # Tau is not well-defined for fewer than 2 points.\n        if M  2:\n            return 0.0\n        \n        # Calculate Kendall's tau against the window index sequence.\n        tau, _ = scipy.stats.kendalltau(np.arange(M), autocorrs)\n        \n        # Per problem spec, treat degenerate cases (which result in NaN) as zero.\n        if np.isnan(tau):\n            return 0.0\n        \n        return tau\n\n    def generate_surrogate(series, rng):\n        \"\"\"\n        Generates a surrogate time series using phase randomization.\n        \"\"\"\n        N = len(series)\n        \n        # 1. Compute DFT\n        fft_coeffs = scipy.fft.rfft(series)\n        amplitudes = np.abs(fft_coeffs)\n        \n        # 2. Randomize phases\n        num_freqs = len(fft_coeffs)\n        random_phases = rng.uniform(0, 2 * np.pi, size=num_freqs)\n        \n        # 3. Enforce real-valued constraints\n        # DC component (f=0) must have phase 0\n        random_phases[0] = 0\n        # Nyquist frequency (if N is even) must have phase 0\n        if N % 2 == 0:\n            random_phases[-1] = 0\n            \n        # 4. Create new complex coefficients\n        new_fft_coeffs = amplitudes * np.exp(1j * random_phases)\n        \n        # 5. Inverse DFT to get real-valued surrogate\n        surrogate = scipy.fft.irfft(new_fft_coeffs, n=N)\n        \n        return surrogate\n\n    # Per problem spec, use a fixed seed for all random number generation.\n    rng = np.random.default_rng(2025)\n\n    test_cases = [\n        # (phi, N, w, delta, R, S, alpha)\n        (0.0, 512, 64, 4, 30, 100, 0.1),\n        (0.5, 512, 64, 4, 30, 100, 0.1),\n        (0.9, 512, 64, 4, 30, 100, 0.1),\n        (0.5, 256, 128, 16, 20, 80, 0.05),\n    ]\n\n    final_results = []\n\n    for phi, N, w, delta, R, S, alpha in test_cases:\n        detection_count = 0\n        for _ in range(R):\n            # 1. Generate and standardize the original time series\n            original_series = generate_ar1(phi, N, rng)\n            std_original_series = standardize(original_series)\n\n            # 2. Compute the observed trend statistic\n            tau_obs = calculate_tau(std_original_series, w, delta)\n\n            # 3. Generate surrogate distribution of tau\n            tau_surrogates = np.empty(S)\n            for k in range(S):\n                surrogate_series = generate_surrogate(std_original_series, rng)\n                std_surrogate = standardize(surrogate_series)\n                tau_surrogates[k] = calculate_tau(std_surrogate, w, delta)\n\n            # 4. Perform the one-sided hypothesis test\n            # np.quantile interpolation='higher' is equivalent to the (1-alpha) empirical quantile\n            critical_value = np.quantile(tau_surrogates, 1 - alpha, interpolation='higher')\n            if tau_obs  critical_value:\n                detection_count += 1\n        \n        # 5. Compute the false positive rate\n        fpr = detection_count / R\n        final_results.append(fpr)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(f'{r:.3f}' for r in final_results)}]\")\n\nsolve()\n```"
        }
    ]
}