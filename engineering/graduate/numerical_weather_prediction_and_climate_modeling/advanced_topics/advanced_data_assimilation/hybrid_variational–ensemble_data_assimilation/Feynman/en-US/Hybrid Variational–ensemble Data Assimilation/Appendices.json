{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex systems, it's crucial to master the fundamental principle of data assimilation: the Bayesian update. This exercise  strips the problem down to a simple scalar case, illustrating how prior knowledge (the background) is combined with new information (the observation) to produce an improved estimate (the analysis). By working through this foundational calculation, you will see how the analysis is a weighted average, with weights determined by the background and observation uncertainties.",
            "id": "3795131",
            "problem": "Consider a $1$-dimensional ocean surface temperature analysis at a single grid point using Hybrid Ensemble-Variational (EnVar) Data Assimilation, where the hybrid background covariance is represented by an effective scalar variance. Assume a linear observation model and Gaussian errors, consistent with the linear-Gaussian limit in which the hybrid method reduces to a Bayesian update with an effective background covariance. Specifically, let the true state $x$ (temperature in degrees Celsius) have a Gaussian prior with mean $x_b$ and variance $B$, and let the observation operator be $H$ so that the observation $y$ satisfies $y = H x + \\varepsilon$, where the observation error $\\varepsilon$ is Gaussian with zero mean and variance $R$. The prior and likelihood are thus consistent with the foundational assumptions of linear Gaussian Bayesian estimation.\n\nFor a single direct thermometer observation with $H = 1$, use the following physically plausible parameters: $x_b = 10$ (degrees Celsius), $B = 4$ (degrees Celsius squared), $y = 12$ (degrees Celsius), and $R = 1$ (degrees Celsius squared). Starting from the definitions of Gaussian prior and likelihood and the application of Bayes' theorem, derive the posterior distribution for $x$ given $y$ in the linear-Gaussian setting, and then compute the posterior mean $x_a$ (analysis temperature) and posterior variance $P_a$ (analysis uncertainty).\n\nExpress the posterior mean $x_a$ in degrees Celsius and the posterior variance $P_a$ in degrees Celsius squared. Your final numerical values should be exact (no rounding). Report your final answer as the ordered pair $\\left(x_a, P_a\\right)$.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, and objective. The problem provides a self-contained, consistent set of parameters and asks for a standard derivation in Bayesian statistics, which is a cornerstone of data assimilation.\n\nThe problem asks for the posterior probability distribution $p(x|y)$ of a state variable $x$ (temperature) given a prior distribution and an observation $y$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe prior distribution of the state $x$ is given as a Gaussian distribution with mean $x_b$ and variance $B$:\n$$\np(x) = \\mathcal{N}(x; x_b, B) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\nThe observation model is $y = Hx + \\varepsilon$, where the observation error $\\varepsilon$ is drawn from a Gaussian distribution with mean $0$ and variance $R$. This implies that the likelihood function, which is the probability of observing $y$ given the state $x$, is also a Gaussian distribution with mean $Hx$ and variance $R$:\n$$\np(y|x) = \\mathcal{N}(y; Hx, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right)\n$$\nSubstituting these into Bayes' theorem:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right) \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B} \\right]\\right)\n$$\nThe term in the square brackets is the variational cost function, often denoted as $J(x)$. The posterior distribution is a Gaussian, as the product of two Gaussians is an unnormalized Gaussian. To find its parameters (mean $x_a$ and variance $P_a$), we need to rearrange the exponent into the canonical quadratic form $-\\frac{1}{2} \\frac{(x-x_a)^2}{P_a}$.\nLet's analyze the exponent term $J(x)$:\n$$\nJ(x) = \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B}\n$$\nWe expand the squared terms:\n$$\nJ(x) = \\frac{y^2 - 2yHx + H^2x^2}{R} + \\frac{x^2 - 2xx_b + x_b^2}{B}\n$$\nNow, we collect terms with respect to powers of $x$:\n$$\nJ(x) = x^2 \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right) - 2x \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right) + \\left(\\frac{y^2}{R} + \\frac{x_b^2}{B}\\right)\n$$\nTo complete the square for $x$, we identify the form $\\frac{1}{P_a}(x-x_a)^2 = \\frac{1}{P_a}(x^2 - 2xx_a + x_a^2)$.\nComparing the coefficient of $x^2$, we find the inverse of the posterior variance $P_a$:\n$$\n\\frac{1}{P_a} = \\frac{H^2}{R} + \\frac{1}{B}\n$$\nComparing the coefficient of the $-2x$ term, we have:\n$$\n\\frac{x_a}{P_a} = \\frac{Hy}{R} + \\frac{x_b}{B}\n$$\nFrom this, we can solve for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nSubstituting the expression for $P_a^{-1}$:\n$$\nx_a = \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right)^{-1} \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nThe problem specifies a direct observation, so the observation operator $H$ is the identity, $H=1$. The formulas simplify to:\n$$\n\\frac{1}{P_a} = \\frac{1}{R} + \\frac{1}{B} = \\frac{B+R}{BR} \\implies P_a = \\frac{BR}{B+R}\n$$\nAnd for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{y}{R} + \\frac{x_b}{B}\\right) = \\frac{BR}{B+R} \\left(\\frac{yB + x_bR}{BR}\\right) = \\frac{yB + x_bR}{B+R}\n$$\nThis expression for $x_a$ can be interpreted as a weighted average of the background mean $x_b$ and the observation $y$:\n$$\nx_a = \\left(\\frac{R}{B+R}\\right)x_b + \\left(\\frac{B}{B+R}\\right)y\n$$\nThe weight for the observation is $K = \\frac{B}{B+R}$, known as the Kalman gain in this scalar case.\n\nNow, we substitute the given numerical values:\n- Background mean: $x_b = 10$\n- Background variance: $B = 4$\n- Observation: $y = 12$\n- Observation error variance: $R = 1$\n\nWe first calculate the posterior variance $P_a$:\n$$\nP_a = \\frac{BR}{B+R} = \\frac{4 \\times 1}{4+1} = \\frac{4}{5}\n$$\nNext, we calculate the posterior mean $x_a$:\n$$\nx_a = \\frac{yB + x_bR}{B+R} = \\frac{(12)(4) + (10)(1)}{4+1} = \\frac{48 + 10}{5} = \\frac{58}{5}\n$$\nThus, the posterior distribution is Gaussian with mean $x_a = \\frac{58}{5}$ and variance $P_a = \\frac{4}{5}$.\nThe analysis temperature is $x_a = 11.6$ degrees Celsius, and the analysis uncertainty (variance) is $P_a = 0.8$ degrees Celsius squared. The problem asks for the ordered pair $(x_a, P_a)$.\n\nThe final answer is the ordered pair $\\left(x_a, P_a\\right) = \\left(\\frac{58}{5}, \\frac{4}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{58}{5} & \\frac{4}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the scalar foundation, this practice  moves to a more realistic, multidimensional system that mirrors the structure of operational models. It focuses on the mechanics of forming a hybrid background-error covariance matrix, $\\mathbf{B}_{\\text{hyb}}$, by combining static and ensemble-derived components. You will then compute the analysis increment by applying the full variational update equation, gaining direct experience with the matrix algebra at the heart of hybrid 3D-Var.",
            "id": "4053122",
            "problem": "Consider a linear, Gaussian three-dimensional variational data assimilation for a single analysis time in numerical weather prediction and climate modeling. Let the state increment be a three-dimensional vector $\\delta \\mathbf{x} \\in \\mathbb{R}^{3}$ and two independent observations be collected in a two-dimensional vector $\\mathbf{y} \\in \\mathbb{R}^{2}$. The observation operator mapping from model space to observation space is linear, denoted by the matrix $\\mathbf{H} \\in \\mathbb{R}^{2 \\times 3}$, and the observation-error covariance matrix is $\\mathbf{R} \\in \\mathbb{R}^{2 \\times 2}$. Assume the hybrid background-error covariance is constructed by linearly combining a static covariance $\\mathbf{B}_{\\text{stat}} \\in \\mathbb{R}^{3 \\times 3}$ and a localized ensemble covariance $\\mathbf{P}_{e} \\in \\mathbb{R}^{3 \\times 3}$ with scalar weight $w \\in (0,1)$:\n$$\n\\mathbf{B}_{\\text{hyb}} = (1-w) \\,\\mathbf{B}_{\\text{stat}} + w\\,\\mathbf{P}_{e}.\n$$\nThe innovation is defined as $\\mathbf{d} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_{b}$, where $\\mathbf{x}_{b}$ is the background state in model space. The analysis increment $\\delta \\mathbf{x}$ minimizes the standard linear-Gaussian three-dimensional variational cost function\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}\\,\\delta \\mathbf{x}^{\\top}\\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\frac{1}{2}\\,\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)^{\\top}\\mathbf{R}^{-1}\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big).\n$$\nUsing this framework, derive the expression for the analysis increment and then explicitly compute it for the following specified matrices and vectors:\n- The static background-error covariance\n$$\n\\mathbf{B}_{\\text{stat}} = \\begin{pmatrix}\n4 & 1 & 2 \\\\\n1 & 2 & 0 \\\\\n2 & 0 & 5\n\\end{pmatrix}.\n$$\n- The localized ensemble covariance\n$$\n\\mathbf{P}_{e} = \\begin{pmatrix}\n3 & 0 & 1 \\\\\n0 & 2 & 0 \\\\\n1 & 0 & 4\n\\end{pmatrix}.\n$$\n- The hybrid weight $w = \\frac{1}{2}$.\n- The observation operator\n$$\n\\mathbf{H} = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\n- The observation-error covariance\n$$\n\\mathbf{R} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 2\n\\end{pmatrix}.\n$$\n- The innovation\n$$\n\\mathbf{d} = \\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix}.\n$$\nStart from the stated cost-function definition and fundamental linear-estimation principles to obtain the analysis increment, and then carry out all matrix-vector operations step by step (including all intermediate products and inverses) to evaluate $\\delta \\mathbf{x}$. Express the final answer as a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment. No rounding is required, and no units are involved.",
            "solution": "The problem statement has been validated and is found to be scientifically grounded, well-posed, and objective. It presents a standard application of three-dimensional variational data assimilation (3D-Var) with a hybrid background-error covariance model. All provided matrices and vectors are dimensionally consistent, and the covariance matrices ($\\mathbf{B}_{\\text{stat}}$, $\\mathbf{P}_{e}$, $\\mathbf{R}$) are symmetric and positive definite, ensuring that the cost function has a unique minimum. The problem is therefore valid and a solution can be derived.\n\nThe analysis increment, $\\delta \\mathbf{x}$, is the vector that minimizes the cost function $J(\\delta \\mathbf{x})$. The cost function is given by:\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}\\,\\delta \\mathbf{x}^{\\top}\\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\frac{1}{2}\\,\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)^{\\top}\\mathbf{R}^{-1}\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)\n$$\nTo find the minimum, we must compute the gradient of $J(\\delta \\mathbf{x})$ with respect to $\\delta \\mathbf{x}$ and set it to the zero vector. First, we expand the second term:\n$$\n\\frac{1}{2}\\,\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)^{\\top}\\mathbf{R}^{-1}\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big) = \\frac{1}{2}\\left( (\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top} - \\mathbf{d}^{\\top})\\mathbf{R}^{-1}(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}) \\right)\n$$\n$$\n= \\frac{1}{2}\\left( \\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} - \\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d} - \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} + \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{d} \\right)\n$$\nSince $\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}$ is a scalar, it is equal to its transpose, $\\mathbf{d}^{\\top}(\\mathbf{R}^{-1})^{\\top}\\mathbf{H}\\delta \\mathbf{x}$. Given that a covariance matrix $\\mathbf{R}$ is symmetric, its inverse $\\mathbf{R}^{-1}$ is also symmetric. Thus, $\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d} = \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x}$. The cost function becomes:\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}\\delta \\mathbf{x}^{\\top}\\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\frac{1}{2}\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} + \\frac{1}{2}\\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nNow, we take the gradient with respect to $\\delta \\mathbf{x}$ using the matrix calculus identities $\\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}$ for symmetric $\\mathbf{A}$ and $\\nabla_{\\mathbf{x}}(\\mathbf{b}^{\\top}\\mathbf{x}) = \\mathbf{b}$:\n$$\n\\nabla_{\\delta \\mathbf{x}} J(\\delta \\mathbf{x}) = \\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nSetting the gradient to zero, $\\nabla_{\\delta \\mathbf{x}} J(\\delta \\mathbf{x}) = \\mathbf{0}$, gives:\n$$\n(\\mathbf{B}_{\\text{hyb}}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})\\delta \\mathbf{x} = \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nThe solution for $\\delta \\mathbf{x}$ is formally $\\delta \\mathbf{x} = (\\mathbf{B}_{\\text{hyb}}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})^{-1}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}$. Using the Woodbury matrix identity, this can be shown to be equivalent to the Kalman gain form, which is more convenient for computation:\n$$\n\\delta \\mathbf{x} = \\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}(\\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} + \\mathbf{R})^{-1}\\mathbf{d}\n$$\nWe will now compute this expression step-by-step using the given values.\n\nStep 1: Compute the hybrid background-error covariance matrix $\\mathbf{B}_{\\text{hyb}}$.\nWith $w = \\frac{1}{2}$ and the formula $\\mathbf{B}_{\\text{hyb}} = (1-w)\\mathbf{B}_{\\text{stat}} + w\\mathbf{P}_{e}$, we have:\n$$\n\\mathbf{B}_{\\text{hyb}} = \\frac{1}{2}\\mathbf{B}_{\\text{stat}} + \\frac{1}{2}\\mathbf{P}_{e} = \\frac{1}{2}(\\mathbf{B}_{\\text{stat}} + \\mathbf{P}_{e})\n$$\n$$\n\\mathbf{B}_{\\text{stat}} + \\mathbf{P}_{e} = \\begin{pmatrix} 4 & 1 & 2 \\\\ 1 & 2 & 0 \\\\ 2 & 0 & 5 \\end{pmatrix} + \\begin{pmatrix} 3 & 0 & 1 \\\\ 0 & 2 & 0 \\\\ 1 & 0 & 4 \\end{pmatrix} = \\begin{pmatrix} 7 & 1 & 3 \\\\ 1 & 4 & 0 \\\\ 3 & 0 & 9 \\end{pmatrix}\n$$\n$$\n\\mathbf{B}_{\\text{hyb}} = \\frac{1}{2}\\begin{pmatrix} 7 & 1 & 3 \\\\ 1 & 4 & 0 \\\\ 3 & 0 & 9 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} & \\frac{1}{2} & \\frac{3}{2} \\\\ \\frac{1}{2} & 2 & 0 \\\\ \\frac{3}{2} & 0 & \\frac{9}{2} \\end{pmatrix}\n$$\n\nStep 2: Compute the matrix term $\\mathbf{M} = \\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} + \\mathbf{R}$.\nFirst, we compute $\\mathbf{H}\\mathbf{B}_{\\text{hyb}}$:\n$$\n\\mathbf{H}\\mathbf{B}_{\\text{hyb}} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} & \\frac{1}{2} & \\frac{3}{2} \\\\ \\frac{1}{2} & 2 & 0 \\\\ \\frac{3}{2} & 0 & \\frac{9}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2}+\\frac{3}{2} & \\frac{1}{2}+0 & \\frac{3}{2}+\\frac{9}{2} \\\\ \\frac{1}{2} & 2 & 0 \\end{pmatrix} = \\begin{pmatrix} 5 & \\frac{1}{2} & 6 \\\\ \\frac{1}{2} & 2 & 0 \\end{pmatrix}\n$$\nNext, we compute $\\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}$:\n$$\n\\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} = \\begin{pmatrix} 5 & \\frac{1}{2} & 6 \\\\ \\frac{1}{2} & 2 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 5(1)+6(1) & 5(0)+\\frac{1}{2}(1) \\\\ \\frac{1}{2}(1)+0(1) & \\frac{1}{2}(0)+2(1) \\end{pmatrix} = \\begin{pmatrix} 11 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\end{pmatrix}\n$$\nNow, we add $\\mathbf{R}$ to form $\\mathbf{M}$:\n$$\n\\mathbf{M} = \\begin{pmatrix} 11 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 12 & \\frac{1}{2} \\\\ \\frac{1}{2} & 4 \\end{pmatrix}\n$$\n\nStep 3: Compute the inverse $\\mathbf{M}^{-1}$.\nThe determinant of $\\mathbf{M}$ is:\n$$\n\\det(\\mathbf{M}) = (12)(4) - \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = 48 - \\frac{1}{4} = \\frac{191}{4}\n$$\nThe inverse is:\n$$\n\\mathbf{M}^{-1} = \\frac{1}{\\det(\\mathbf{M})} \\begin{pmatrix} 4 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 12 \\end{pmatrix} = \\frac{4}{191} \\begin{pmatrix} 4 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 12 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 16 & -2 \\\\ -2 & 48 \\end{pmatrix}\n$$\n\nStep 4: Compute the term $\\mathbf{M}^{-1}\\mathbf{d}$.\n$$\n\\mathbf{M}^{-1}\\mathbf{d} = \\frac{1}{191} \\begin{pmatrix} 16 & -2 \\\\ -2 & 48 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 16(2) + (-2)(-1) \\\\ -2(2) + 48(-1) \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 32+2 \\\\ -4-48 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 34 \\\\ -52 \\end{pmatrix}\n$$\n\nStep 5: Compute the term $\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}$.\nThis is the transpose of the matrix $\\mathbf{H}\\mathbf{B}_{\\text{hyb}}$ calculated in Step 2, since $\\mathbf{B}_{\\text{hyb}}$ is symmetric.\n$$\n\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} = \\begin{pmatrix} 5 & \\frac{1}{2} & 6 \\\\ \\frac{1}{2} & 2 & 0 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 5 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\\\ 6 & 0 \\end{pmatrix}\n$$\n\nStep 6: Compute the final analysis increment $\\delta \\mathbf{x}$.\n$$\n\\delta \\mathbf{x} = (\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}) (\\mathbf{M}^{-1}\\mathbf{d})\n$$\n$$\n\\delta \\mathbf{x} = \\begin{pmatrix} 5 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\\\ 6 & 0 \\end{pmatrix} \\left( \\frac{1}{191} \\begin{pmatrix} 34 \\\\ -52 \\end{pmatrix} \\right) = \\frac{1}{191} \\begin{pmatrix} 5(34) + \\frac{1}{2}(-52) \\\\ \\frac{1}{2}(34) + 2(-52) \\\\ 6(34) + 0(-52) \\end{pmatrix}\n$$\n$$\n\\delta \\mathbf{x} = \\frac{1}{191} \\begin{pmatrix} 170 - 26 \\\\ 17 - 104 \\\\ 204 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 144 \\\\ -87 \\\\ 204 \\end{pmatrix} = \\begin{pmatrix} \\frac{144}{191} \\\\ -\\frac{87}{191} \\\\ \\frac{204}{191} \\end{pmatrix}\n$$\nThe problem asks for the result as a single row matrix.\n$$\n\\delta \\mathbf{x}^{\\top} = \\begin{pmatrix} \\frac{144}{191} & -\\frac{87}{191} & \\frac{204}{191} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{144}{191} & -\\frac{87}{191} & \\frac{204}{191} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The ultimate test of a data assimilation system lies in its performance, which is quantified using key diagnostics. This computational exercise  guides you through implementing a complete analysis cycle for a toy hybrid system, moving from theory to practical application. By calculating the Root Mean Square Error (RMSE) and ensemble spread, you will learn to assess the system's effectiveness and diagnose common issues like under-dispersion, a critical step in tuning operational forecast systems.",
            "id": "4053150",
            "problem": "Consider a linear-Gaussian data assimilation setting for a toy system in which the state vector is denoted by $x \\in \\mathbb{R}^n$, the forecast (background) ensemble consists of $m$ members with matrix $X_f \\in \\mathbb{R}^{n \\times m}$ whose columns are the ensemble members, the observation operator is linear $H \\in \\mathbb{R}^{p \\times n}$, and the observational error covariance is a symmetric positive-definite matrix $R \\in \\mathbb{R}^{p \\times p}$. The hybrid variationalâ€“ensemble prior covariance $B_{\\text{hyb}}$ is formed by combining a climatological (variational) covariance $B_{\\text{clim}} \\in \\mathbb{R}^{n \\times n}$ with the ensemble sample covariance $B_{\\text{ens}} \\in \\mathbb{R}^{n \\times n}$ via a convex weight $w \\in [0,1]$, namely $B_{\\text{hyb}} = (1-w) B_{\\text{clim}} + w B_{\\text{ens}}$. You must compute the Root Mean Square Error (RMSE) and the ensemble spread before and after the analysis, and assess spread-error consistency and the need for multiplicative inflation under hybrid covariance updates. The RMSE is defined between an ensemble mean (or analysis mean) and a prescribed true state $x_{\\text{true}}$, and the spread is defined from the appropriate covariance by the square root of the average variance across state components.\n\nStarting from fundamental probabilistic principles of linear-Gaussian estimation, specifically Bayes' rule applied to a Gaussian prior for $x$ and a Gaussian likelihood for $y = Hx + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,R)$, and from the definitions of ensemble sample mean and covariance, derive the analysis mean and covariance implied by this setting when the prior covariance is $B_{\\text{hyb}}$. Use these to compute the requested diagnostics. Do not assume any shortcut formulas not derivable from these principles.\n\nDefinitions to be used:\n- The ensemble mean of the forecast is the arithmetic mean $\\bar{x}_f = \\frac{1}{m} \\sum_{j=1}^{m} x_f^{(j)}$, where $x_f^{(j)}$ are columns of $X_f$.\n- The ensemble sample covariance is $B_{\\text{ens}} = \\frac{1}{m-1} \\sum_{j=1}^{m} (x_f^{(j)} - \\bar{x}_f)(x_f^{(j)} - \\bar{x}_f)^\\top$.\n- The RMSE between a mean estimate $x_\\mu$ and the true state $x_{\\text{true}}$ is $\\text{RMSE}(x_\\mu, x_{\\text{true}}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{\\mu,i} - x_{\\text{true},i})^2}$.\n- The spread corresponding to a covariance $P$ is $\\text{spread}(P) = \\sqrt{\\frac{1}{n} \\operatorname{trace}(P)}$.\n- Spread-error consistency is assessed via the ratio $r = \\frac{\\text{spread}}{\\text{RMSE}}$; under-dispersion is indicated when $r$ is below a specified threshold $\\tau \\in (0,1]$, suggesting a need for multiplicative inflation.\n\nYour program must, for each test case below, compute:\n1. The pre-analysis RMSE, $\\text{RMSE}_{\\text{pre}}$, using the forecast ensemble mean $\\bar{x}_f$ and $x_{\\text{true}}$.\n2. The pre-analysis ensemble spread, $\\text{spread}_{\\text{pre}}$, using $B_{\\text{ens}}$.\n3. The analysis mean $x_a$ and analysis covariance $P_a$ implied by linear-Gaussian analysis with hybrid covariance $B_{\\text{hyb}}$.\n4. The post-analysis RMSE, $\\text{RMSE}_{\\text{post}}$, using $x_a$ and $x_{\\text{true}}$.\n5. The post-analysis spread, $\\text{spread}_{\\text{post}}$, using $P_a$.\n6. The post-analysis spread-error ratio $r_{\\text{post}} = \\frac{\\text{spread}_{\\text{post}}}{\\text{RMSE}_{\\text{post}}}$.\n7. A boolean recommendation for inflation, $I$, defined as $I = \\text{True}$ if $r_{\\text{post}} < \\tau$, and $I = \\text{False}$ otherwise.\n\nNo physical units are involved in this toy system, so no unit conversions are needed. Angles do not appear. Express all final numeric results as floats or booleans. There are no percentages.\n\nTest suite parameter values:\n- Case A (general case):\n  - $n = 3$, $m = 5$.\n  - $x_{\\text{true}} = [\\,2.0,\\,-1.0,\\,0.5\\,]$.\n  - $X_f = \\begin{bmatrix} 1.7 & 2.1 & 1.9 & 2.2 & 1.8 \\\\ -0.9 & -1.2 & -0.8 & -1.1 & -1.0 \\\\ 0.6 & 0.4 & 0.7 & 0.3 & 0.6 \\end{bmatrix}$.\n  - $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$.\n  - $y = [\\,2.05,\\,0.45\\,]$.\n  - $R = \\operatorname{diag}([\\,0.1,\\,0.1\\,])$.\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,0.9,\\,0.9,\\,0.9\\,])$.\n  - $w = 0.5$.\n  - $\\tau = 0.9$.\n- Case B (under-dispersion edge case, pure ensemble):\n  - $n = 3$, $m = 4$.\n  - $x_{\\text{true}} = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n  - $X_f = \\begin{bmatrix} 0.52 & 0.50 & 0.51 & 0.49 \\\\ -0.48 & -0.50 & -0.49 & -0.51 \\\\ 0.21 & 0.19 & 0.20 & 0.22 \\end{bmatrix}$.\n  - $H = I_{3 \\times 3}$.\n  - $y = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n  - $R = \\operatorname{diag}([\\,0.01,\\,0.01,\\,0.01\\,])$.\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,1.0,\\,1.0,\\,1.0\\,])$.\n  - $w = 1.0$.\n  - $\\tau = 1.0$.\n- Case C (over-dispersion scenario, hybrid dominated by climatology):\n  - $n = 3$, $m = 5$.\n  - $x_{\\text{true}} = [\\,1.0,\\,1.0,\\,1.0\\,]$.\n  - $X_f = \\begin{bmatrix} 0.0 & 2.0 & 1.5 & 0.5 & 1.0 \\\\ 2.0 & 0.0 & 1.5 & 0.5 & 1.0 \\\\ 1.5 & 0.5 & 2.0 & 0.0 & 1.0 \\end{bmatrix}$.\n  - $H = I_{3 \\times 3}$.\n  - $y = [\\,1.1,\\,0.9,\\,1.05\\,]$.\n  - $R = \\operatorname{diag}([\\,0.5,\\,0.5,\\,0.5\\,])$.\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,0.8,\\,0.3,\\,1.2\\,])$.\n  - $w = 0.2$.\n  - $\\tau = 0.9$.\n- Case D (boundary case, pure variational climatology, weakly informative observations):\n  - $n = 3$, $m = 4$.\n  - $x_{\\text{true}} = [\\,1.0,\\,-2.0,\\,0.0\\,]$.\n  - $X_f = \\begin{bmatrix} 1.1 & 0.9 & 1.2 & 0.8 \\\\ -1.9 & -2.1 & -1.8 & -2.2 \\\\ 0.1 & -0.1 & 0.0 & 0.2 \\end{bmatrix}$.\n  - $H = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$.\n  - $y = [\\,0.8\\,]$.\n  - $R = [\\,[\\,5.0\\,]\\,]$ (a $1 \\times 1$ matrix).\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,0.5,\\,0.5,\\,0.5\\,])$.\n  - $w = 0.0$.\n  - $\\tau = 0.9$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order $[\\,\\text{RMSE}_{\\text{pre}},\\,\\text{spread}_{\\text{pre}},\\,\\text{RMSE}_{\\text{post}},\\,\\text{spread}_{\\text{post}},\\,r_{\\text{post}},\\,I\\,]$. For example, a valid output would look like $[\\,[\\,0.1,\\,0.2,\\,0.05,\\,0.15,\\,3.0,\\,\\text{False}\\,],\\,[\\,\\dots\\,]\\,]$.",
            "solution": "The problem is deemed valid. It is a well-posed, scientifically grounded exercise in linear-Gaussian data assimilation, a core topic in numerical weather prediction and related fields. All data, definitions, and conditions are explicitly provided and are self-consistent.\n\n### 1. Bayesian Formulation of Linear-Gaussian Analysis\n\nThe analysis seeks the posterior probability distribution of the state vector $x \\in \\mathbb{R}^n$ given an observation $y \\in \\mathbb{R}^p$. According to Bayes' rule, the posterior probability density function $p(x|y)$ is proportional to the product of the likelihood $p(y|x)$ and the prior $p(x)$:\n$$p(x|y) \\propto p(y|x) p(x)$$\nThe state that maximizes this posterior probability is known as the Maximum A Posteriori (MAP) estimate. For Gaussian distributions, the MAP estimate coincides with the posterior mean.\n\nThe prior distribution of the state $x$ is assumed to be Gaussian, with a mean equal to the forecast ensemble mean $\\bar{x}_f$ and a covariance given by the hybrid covariance matrix $B_{\\text{hyb}}$.\n$$p(x) = \\mathcal{N}(x; \\bar{x}_f, B_{\\text{hyb}}) \\propto \\exp\\left(-\\frac{1}{2}(x - \\bar{x}_f)^\\top B_{\\text{hyb}}^{-1}(x - \\bar{x}_f)\\right)$$\n\nThe observation model is linear, $y = Hx + \\varepsilon$, where the observational error $\\varepsilon$ is drawn from a zero-mean Gaussian distribution with covariance $R$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that the likelihood of observing $y$ given the state $x$ is also Gaussian:\n$$p(y|x) = \\mathcal{N}(y; Hx, R) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^\\top R^{-1}(y - Hx)\\right)$$\n\nCombining the prior and likelihood, the logarithm of the posterior distribution is:\n$$\\ln p(x|y) = -\\frac{1}{2} \\left[ (x - \\bar{x}_f)^\\top B_{\\text{hyb}}^{-1}(x - \\bar{x}_f) + (y - Hx)^\\top R^{-1}(y - Hx) \\right] + \\text{constant}$$\nThe analysis state $x_a$ is the state that minimizes the quadratic cost function $J(x)$ in the exponent:\n$$J(x) = (x - \\bar{x}_f)^\\top B_{\\text{hyb}}^{-1}(x - \\bar{x}_f) + (y - Hx)^\\top R^{-1}(y - Hx)$$\nTo find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero:\n$$\\nabla_x J(x) = 2B_{\\text{hyb}}^{-1}(x - \\bar{x}_f) - 2H^\\top R^{-1}(y - Hx) = 0$$\nSetting $x = x_a$, we solve for the analysis mean:\n$$B_{\\text{hyb}}^{-1}(x_a - \\bar{x}_f) + H^\\top R^{-1}(Hx_a - y) = 0$$\n$$(B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H)x_a = B_{\\text{hyb}}^{-1}\\bar{x}_f + H^\\top R^{-1}y$$\n$$x_a = (B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H)^{-1}(B_{\\text{hyb}}^{-1}\\bar{x}_f + H^\\top R^{-1}y)$$\nThe analysis covariance matrix $P_a$ is the inverse of the Hessian of the negative log-posterior, which is $\\frac{1}{2}J(x)$. The Hessian of $\\frac{1}{2}J(x)$ is:\n$$P_a^{-1} = \\frac{1}{2} \\nabla_x^2 J(x) = B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H$$\nTherefore, the analysis covariance is:\n$$P_a = (B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H)^{-1}$$\n\n### 2. Kalman Gain Formulation\n\nFor computational stability and efficiency, the above expressions are typically reformulated using the Woodbury matrix identity. This leads to the well-known Kalman gain formulation.\nThe analysis mean can be expressed as an update to the forecast mean:\n$$x_a = \\bar{x}_f + K(y - H\\bar{x}_f)$$\nand the analysis covariance as an update to the background covariance:\n$$P_a = (I - KH)B_{\\text{hyb}}$$\nwhere $I$ is the identity matrix and $K$ is the Kalman gain matrix, defined as:\n$$K = B_{\\text{hyb}}H^\\top(HB_{\\text{hyb}}H^\\top + R)^{-1}$$\nThis formulation avoids the inversion of the $n \\times n$ matrix $B_{\\text{hyb}}$ and instead requires the inversion of the smaller $p \\times p$ matrix $HB_{\\text{hyb}}H^\\top + R$. This is the formulation used in the implementation.\n\n### 3. Computational Steps\n\nFor each test case, the following quantities are computed in sequence:\n\n1.  **Forecast Ensemble Mean $\\bar{x}_f$**:\n    Using the provided forecast ensemble matrix $X_f \\in \\mathbb{R}^{n \\times m}$:\n    $$\\bar{x}_f = \\frac{1}{m} \\sum_{j=1}^{m} x_f^{(j)}$$\n\n2.  **Pre-analysis RMSE ($\\text{RMSE}_{\\text{pre}}$)**:\n    The Root Mean Square Error between the forecast mean $\\bar{x}_f$ and the true state $x_{\\text{true}}$:\n    $$\\text{RMSE}_{\\text{pre}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\bar{x}_{f,i} - x_{\\text{true},i})^2}$$\n\n3.  **Ensemble Sample Covariance $B_{\\text{ens}}$**:\n    $$B_{\\text{ens}} = \\frac{1}{m-1} \\sum_{j=1}^{m} (x_f^{(j)} - \\bar{x}_f)(x_f^{(j)} - \\bar{x}_f)^\\top$$\n\n4.  **Pre-analysis Spread ($\\text{spread}_{\\text{pre}}$)**:\n    The spread of the forecast ensemble, calculated from its sample covariance $B_{\\text{ens}}$:\n    $$\\text{spread}_{\\text{pre}} = \\sqrt{\\frac{1}{n} \\operatorname{trace}(B_{\\text{ens}})}$$\n\n5.  **Hybrid Background Covariance $B_{\\text{hyb}}$**:\n    A convex combination of the climatological covariance $B_{\\text{clim}}$ and the ensemble covariance $B_{\\text{ens}}$ with weight $w$:\n    $$B_{\\text{hyb}} = (1-w) B_{\\text{clim}} + w B_{\\text{ens}}$$\n\n6.  **Analysis Mean $x_a$ and Covariance $P_a$**:\n    Calculated using the Kalman gain formulation derived above:\n    $$K = B_{\\text{hyb}}H^\\top(HB_{\\text{hyb}}H^\\top + R)^{-1}$$\n    $$x_a = \\bar{x}_f + K(y - H\\bar{x}_f)$$\n    $$P_a = (I - KH)B_{\\text{hyb}}$$\n\n7.  **Post-analysis RMSE ($\\text{RMSE}_{\\text{post}}$)**:\n    The Root Mean Square Error between the analysis mean $x_a$ and the true state $x_{\\text{true}}$:\n    $$\\text{RMSE}_{\\text{post}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{a,i} - x_{\\text{true},i})^2}$$\n\n8.  **Post-analysis Spread ($\\text{spread}_{\\text{post}}$)**:\n    The spread corresponding to the analysis error covariance $P_a$:\n    $$\\text{spread}_{\\text{post}} = \\sqrt{\\frac{1}{n} \\operatorname{trace}(P_a)}$$\n\n9.  **Post-analysis Spread-Error Ratio ($r_{\\text{post}}$)**:\n    The ratio of post-analysis spread to post-analysis RMSE:\n    $$r_{\\text{post}} = \\frac{\\text{spread}_{\\text{post}}}{\\text{RMSE}_{\\text{post}}}$$\n\n10. **Inflation Recommendation ($I$)**:\n    A boolean value indicating the need for multiplicative inflation, true if the ensemble is under-dispersive:\n    $$I = (r_{\\text{post}} < \\tau)$$\n\nThese steps are implemented for each provided test case. The following Python code implements this logic:\n```python\nimport numpy as np\n\ndef process_case(n, m, x_true, X_f, H, y, R, B_clim, w, tau):\n    \"\"\"\n    Computes the specified data assimilation diagnostics for a single case.\n    \"\"\"\n    # 1. Pre-analysis RMSE and Spread\n    # Ensemble mean\n    x_f_mean = np.mean(X_f, axis=1)\n\n    # Pre-analysis RMSE\n    rmse_pre = np.sqrt(np.mean((x_f_mean - x_true)**2))\n\n    # Ensemble sample covariance\n    if m > 1:\n        B_ens = np.cov(X_f, ddof=1)\n    else:\n        B_ens = np.zeros((n, n))\n\n    # Pre-analysis spread\n    spread_pre = np.sqrt(np.trace(B_ens) / n)\n\n    # 2. Hybrid Analysis\n    # Hybrid background error covariance\n    B_hyb = (1 - w) * B_clim + w * B_ens\n\n    # Kalman Gain K = B_hyb * H.T * inv(H * B_hyb * H.T + R)\n    H_B_hyb_HT = H @ B_hyb @ H.T\n    S = H_B_hyb_HT + R\n    S_inv = np.linalg.inv(S)\n    K = B_hyb @ H.T @ S_inv\n\n    # Analysis mean x_a = x_f_mean + K * (y - H * x_f_mean)\n    innovation = y - (H @ x_f_mean)\n    x_a = x_f_mean + K @ innovation\n\n    # Analysis covariance P_a = (I - K*H) * B_hyb\n    I = np.eye(n)\n    P_a = (I - K @ H) @ B_hyb\n\n    # 3. Post-analysis RMSE and Spread\n    # Post-analysis RMSE\n    rmse_post = np.sqrt(np.mean((x_a - x_true)**2))\n\n    # Post-analysis spread\n    spread_post = np.sqrt(np.trace(P_a) / n)\n\n    # 4. Spread-Error Consistency and Inflation\n    # Ratio\n    if rmse_post > 1e-9: # Avoid division by zero\n        r_post = spread_post / rmse_post\n    else:\n        r_post = np.inf\n        \n    # Inflation recommendation\n    inflammation_needed = r_post  tau\n\n    return [rmse_pre, spread_pre, rmse_post, spread_post, r_post, inflammation_needed]\n\n```",
            "answer": "[[0.036515,0.143734,0.034372,0.137882,4.011500,False],[0.301539,0.012583,0.005528,0.009983,1.806034,False],[0.500000,0.816497,0.057735,0.505635,8.758197,False],[0.091287,0.125000,0.076891,0.407421,5.300003,False]]"
        }
    ]
}