{
    "hands_on_practices": [
        {
            "introduction": "To master hybrid data assimilation, we must begin with its theoretical bedrock: Bayesian inference. This first exercise strips the problem down to its simplest form—a single scalar state variable, such as the temperature at one location. By applying Bayes' theorem to a Gaussian prior (our background knowledge) and a Gaussian likelihood (our observation), you will derive the updated \"analysis\" state and its associated uncertainty. This foundational practice provides a clear and intuitive understanding of how information from a forecast and an observation are optimally weighted and combined, a principle that underpins all advanced data assimilation techniques .",
            "id": "3795131",
            "problem": "Consider a $1$-dimensional ocean surface temperature analysis at a single grid point using Hybrid Ensemble-Variational (EnVar) Data Assimilation, where the hybrid background covariance is represented by an effective scalar variance. Assume a linear observation model and Gaussian errors, consistent with the linear-Gaussian limit in which the hybrid method reduces to a Bayesian update with an effective background covariance. Specifically, let the true state $x$ (temperature in degrees Celsius) have a Gaussian prior with mean $x_b$ and variance $B$, and let the observation operator be $H$ so that the observation $y$ satisfies $y = H x + \\varepsilon$, where the observation error $\\varepsilon$ is Gaussian with zero mean and variance $R$. The prior and likelihood are thus consistent with the foundational assumptions of linear Gaussian Bayesian estimation.\n\nFor a single direct thermometer observation with $H = 1$, use the following physically plausible parameters: $x_b = 10$ (degrees Celsius), $B = 4$ (degrees Celsius squared), $y = 12$ (degrees Celsius), and $R = 1$ (degrees Celsius squared). Starting from the definitions of Gaussian prior and likelihood and the application of Bayes' theorem, derive the posterior distribution for $x$ given $y$ in the linear-Gaussian setting, and then compute the posterior mean $x_a$ (analysis temperature) and posterior variance $P_a$ (analysis uncertainty).\n\nExpress the posterior mean $x_a$ in degrees Celsius and the posterior variance $P_a$ in degrees Celsius squared. Your final numerical values should be exact (no rounding). Report your final answer as the ordered pair $\\left(x_a, P_a\\right)$.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, and objective. The problem provides a self-contained, consistent set of parameters and asks for a standard derivation in Bayesian statistics, which is a cornerstone of data assimilation.\n\nThe problem asks for the posterior probability distribution $p(x|y)$ of a state variable $x$ (temperature) given a prior distribution and an observation $y$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe prior distribution of the state $x$ is given as a Gaussian distribution with mean $x_b$ and variance $B$:\n$$\np(x) = \\mathcal{N}(x; x_b, B) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\nThe observation model is $y = Hx + \\varepsilon$, where the observation error $\\varepsilon$ is drawn from a Gaussian distribution with mean $0$ and variance $R$. This implies that the likelihood function, which is the probability of observing $y$ given the state $x$, is also a Gaussian distribution with mean $Hx$ and variance $R$:\n$$\np(y|x) = \\mathcal{N}(y; Hx, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right)\n$$\nSubstituting these into Bayes' theorem:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right) \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B} \\right]\\right)\n$$\nThe term in the square brackets is the variational cost function, often denoted as $J(x)$. The posterior distribution is a Gaussian, as the product of two Gaussians is an unnormalized Gaussian. To find its parameters (mean $x_a$ and variance $P_a$), we need to rearrange the exponent into the canonical quadratic form $-\\frac{1}{2} \\frac{(x-x_a)^2}{P_a}$.\nLet's analyze the exponent term $J(x)$:\n$$\nJ(x) = \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B}\n$$\nWe expand the squared terms:\n$$\nJ(x) = \\frac{y^2 - 2yHx + H^2x^2}{R} + \\frac{x^2 - 2xx_b + x_b^2}{B}\n$$\nNow, we collect terms with respect to powers of $x$:\n$$\nJ(x) = x^2 \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right) - 2x \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right) + \\left(\\frac{y^2}{R} + \\frac{x_b^2}{B}\\right)\n$$\nTo complete the square for $x$, we identify the form $\\frac{1}{P_a}(x-x_a)^2 = \\frac{1}{P_a}(x^2 - 2xx_a + x_a^2)$.\nComparing the coefficient of $x^2$, we find the inverse of the posterior variance $P_a$:\n$$\n\\frac{1}{P_a} = \\frac{H^2}{R} + \\frac{1}{B}\n$$\nComparing the coefficient of the $-2x$ term, we have:\n$$\n\\frac{x_a}{P_a} = \\frac{Hy}{R} + \\frac{x_b}{B}\n$$\nFrom this, we can solve for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nSubstituting the expression for $P_a^{-1}$:\n$$\nx_a = \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right)^{-1} \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nThe problem specifies a direct observation, so the observation operator $H$ is the identity, $H=1$. The formulas simplify to:\n$$\n\\frac{1}{P_a} = \\frac{1}{R} + \\frac{1}{B} = \\frac{B+R}{BR} \\implies P_a = \\frac{BR}{B+R}\n$$\nAnd for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{y}{R} + \\frac{x_b}{B}\\right) = \\frac{BR}{B+R} \\left(\\frac{yB + x_bR}{BR}\\right) = \\frac{yB + x_bR}{B+R}\n$$\nThis expression for $x_a$ can be interpreted as a weighted average of the background mean $x_b$ and the observation $y$:\n$$\nx_a = \\left(\\frac{R}{B+R}\\right)x_b + \\left(\\frac{B}{B+R}\\right)y\n$$\nThe weight for the observation is $K = \\frac{B}{B+R}$, known as the Kalman gain in this scalar case.\n\nNow, we substitute the given numerical values:\n- Background mean: $x_b = 10$\n- Background variance: $B = 4$\n- Observation: $y = 12$\n- Observation error variance: $R = 1$\n\nWe first calculate the posterior variance $P_a$:\n$$\nP_a = \\frac{BR}{B+R} = \\frac{4 \\times 1}{4+1} = \\frac{4}{5}\n$$\nNext, we calculate the posterior mean $x_a$:\n$$\nx_a = \\frac{yB + x_bR}{B+R} = \\frac{(12)(4) + (10)(1)}{4+1} = \\frac{48 + 10}{5} = \\frac{58}{5}\n$$\nThus, the posterior distribution is Gaussian with mean $x_a = \\frac{58}{5}$ and variance $P_a = \\frac{4}{5}$.\nThe analysis temperature is $x_a = 11.6$ degrees Celsius, and the analysis uncertainty (variance) is $P_a = 0.8$ degrees Celsius squared. The problem asks for the ordered pair $(x_a, P_a)$.\n\nThe final answer is the ordered pair $\\left(x_a, P_a\\right) = \\left(\\frac{58}{5}, \\frac{4}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{58}{5}  \\frac{4}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the scalar case, we now advance to a more realistic scenario involving a multi-dimensional state vector and multiple observations. This practice introduces the central component of hybrid assimilation: the hybrid background-error covariance matrix, $\\mathbf{B}_{\\text{hyb}}$. Here, you will construct $\\mathbf{B}_{\\text{hyb}}$ by explicitly blending a static, climatological covariance matrix with a flow-dependent one derived from an ensemble. By calculating the analysis increment for this small system, you will gain hands-on experience with the matrix operations that govern how observational information is spread from observed to unobserved variables according to the structure of the hybrid covariance .",
            "id": "4053122",
            "problem": "Consider a linear, Gaussian three-dimensional variational data assimilation for a single analysis time in numerical weather prediction and climate modeling. Let the state increment be a three-dimensional vector $\\delta \\mathbf{x} \\in \\mathbb{R}^{3}$ and two independent observations be collected in a two-dimensional vector $\\mathbf{y} \\in \\mathbb{R}^{2}$. The observation operator mapping from model space to observation space is linear, denoted by the matrix $\\mathbf{H} \\in \\mathbb{R}^{2 \\times 3}$, and the observation-error covariance matrix is $\\mathbf{R} \\in \\mathbb{R}^{2 \\times 2}$. Assume the hybrid background-error covariance is constructed by linearly combining a static covariance $\\mathbf{B}_{\\text{stat}} \\in \\mathbb{R}^{3 \\times 3}$ and a localized ensemble covariance $\\mathbf{P}_{e} \\in \\mathbb{R}^{3 \\times 3}$ with scalar weight $w \\in (0,1)$:\n$$\n\\mathbf{B}_{\\text{hyb}} = w \\,\\mathbf{B}_{\\text{stat}} + (1-w)\\,\\mathbf{P}_{e}.\n$$\nThe innovation is defined as $\\mathbf{d} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_{b}$, where $\\mathbf{x}_{b}$ is the background state in model space. The analysis increment $\\delta \\mathbf{x}$ minimizes the standard linear-Gaussian three-dimensional variational cost function\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}\\,\\delta \\mathbf{x}^{\\top}\\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\frac{1}{2}\\,\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)^{\\top}\\mathbf{R}^{-1}\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big).\n$$\nUsing this framework, derive the expression for the analysis increment and then explicitly compute it for the following specified matrices and vectors:\n- The static background-error covariance\n$$\n\\mathbf{B}_{\\text{stat}} = \\begin{pmatrix}\n4  1  2 \\\\\n1  2  0 \\\\\n2  0  5\n\\end{pmatrix}.\n$$\n- The localized ensemble covariance\n$$\n\\mathbf{P}_{e} = \\begin{pmatrix}\n3  0  1 \\\\\n0  2  0 \\\\\n1  0  4\n\\end{pmatrix}.\n$$\n- The hybrid weight $w = \\frac{1}{2}$.\n- The observation operator\n$$\n\\mathbf{H} = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}.\n$$\n- The observation-error covariance\n$$\n\\mathbf{R} = \\begin{pmatrix}\n1  0 \\\\\n0  2\n\\end{pmatrix}.\n$$\n- The innovation\n$$\n\\mathbf{d} = \\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix}.\n$$\nStart from the stated cost-function definition and fundamental linear-estimation principles to obtain the analysis increment, and then carry out all matrix-vector operations step by step (including all intermediate products and inverses) to evaluate $\\delta \\mathbf{x}$. Express the final answer as a single row matrix using the LaTeX pmatrix environment. No rounding is required, and no units are involved.",
            "solution": "The problem statement has been validated and is found to be scientifically grounded, well-posed, and objective. It presents a standard application of three-dimensional variational data assimilation (3D-Var) with a hybrid background-error covariance model. All provided matrices and vectors are dimensionally consistent, and the covariance matrices ($\\mathbf{B}_{\\text{stat}}$, $\\mathbf{P}_{e}$, $\\mathbf{R}$) are symmetric and positive definite, ensuring that the cost function has a unique minimum. The problem is therefore valid and a solution can be derived.\n\nThe analysis increment, $\\delta \\mathbf{x}$, is the vector that minimizes the cost function $J(\\delta \\mathbf{x})$. The cost function is given by:\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}\\,\\delta \\mathbf{x}^{\\top}\\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\frac{1}{2}\\,\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)^{\\top}\\mathbf{R}^{-1}\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)\n$$\nTo find the minimum, we must compute the gradient of $J(\\delta \\mathbf{x})$ with respect to $\\delta \\mathbf{x}$ and set it to the zero vector. First, we expand the second term:\n$$\n\\frac{1}{2}\\,\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big)^{\\top}\\mathbf{R}^{-1}\\big(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}\\big) = \\frac{1}{2}\\left( (\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top} - \\mathbf{d}^{\\top})\\mathbf{R}^{-1}(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}) \\right)\n$$\n$$\n= \\frac{1}{2}\\left( \\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} - \\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d} - \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} + \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{d} \\right)\n$$\nSince $\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}$ is a scalar, it is equal to its transpose, $\\mathbf{d}^{\\top}(\\mathbf{R}^{-1})^{\\top}\\mathbf{H}\\delta \\mathbf{x}$. Given that a covariance matrix $\\mathbf{R}$ is symmetric, its inverse $\\mathbf{R}^{-1}$ is also symmetric. Thus, $\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d} = \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x}$. The cost function becomes:\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}\\delta \\mathbf{x}^{\\top}\\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\frac{1}{2}\\delta \\mathbf{x}^{\\top}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} + \\frac{1}{2}\\mathbf{d}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nNow, we take the gradient with respect to $\\delta \\mathbf{x}$ using the matrix calculus identities $\\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}$ for symmetric $\\mathbf{A}$ and $\\nabla_{\\mathbf{x}}(\\mathbf{b}^{\\top}\\mathbf{x}) = \\mathbf{b}$:\n$$\n\\nabla_{\\delta \\mathbf{x}} J(\\delta \\mathbf{x}) = \\mathbf{B}_{\\text{hyb}}^{-1}\\delta \\mathbf{x} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nSetting the gradient to zero, $\\nabla_{\\delta \\mathbf{x}} J(\\delta \\mathbf{x}) = \\mathbf{0}$, gives:\n$$\n(\\mathbf{B}_{\\text{hyb}}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})\\delta \\mathbf{x} = \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nThe solution for $\\delta \\mathbf{x}$ is formally $\\delta \\mathbf{x} = (\\mathbf{B}_{\\text{hyb}}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})^{-1}\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}$. Using the Woodbury matrix identity, this can be shown to be equivalent to the Kalman gain form, which is more convenient for computation:\n$$\n\\delta \\mathbf{x} = \\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}(\\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} + \\mathbf{R})^{-1}\\mathbf{d}\n$$\nWe will now compute this expression step-by-step using the given values.\n\nStep 1: Compute the hybrid background-error covariance matrix $\\mathbf{B}_{\\text{hyb}}$.\nWith $w = \\frac{1}{2}$, we have:\n$$\n\\mathbf{B}_{\\text{hyb}} = \\frac{1}{2}\\mathbf{B}_{\\text{stat}} + \\left(1-\\frac{1}{2}\\right)\\mathbf{P}_{e} = \\frac{1}{2}(\\mathbf{B}_{\\text{stat}} + \\mathbf{P}_{e})\n$$\n$$\n\\mathbf{B}_{\\text{stat}} + \\mathbf{P}_{e} = \\begin{pmatrix} 4  1  2 \\\\ 1  2  0 \\\\ 2  0  5 \\end{pmatrix} + \\begin{pmatrix} 3  0  1 \\\\ 0  2  0 \\\\ 1  0  4 \\end{pmatrix} = \\begin{pmatrix} 7  1  3 \\\\ 1  4  0 \\\\ 3  0  9 \\end{pmatrix}\n$$\n$$\n\\mathbf{B}_{\\text{hyb}} = \\frac{1}{2}\\begin{pmatrix} 7  1  3 \\\\ 1  4  0 \\\\ 3  0  9 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2}  \\frac{1}{2}  \\frac{3}{2} \\\\ \\frac{1}{2}  2  0 \\\\ \\frac{3}{2}  0  \\frac{9}{2} \\end{pmatrix}\n$$\n\nStep 2: Compute the matrix term $\\mathbf{M} = \\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} + \\mathbf{R}$.\nFirst, we compute $\\mathbf{H}\\mathbf{B}_{\\text{hyb}}$:\n$$\n\\mathbf{H}\\mathbf{B}_{\\text{hyb}} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2}  \\frac{1}{2}  \\frac{3}{2} \\\\ \\frac{1}{2}  2  0 \\\\ \\frac{3}{2}  0  \\frac{9}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2}+\\frac{3}{2}  \\frac{1}{2}+0  \\frac{3}{2}+\\frac{9}{2} \\\\ \\frac{1}{2}  2  0 \\end{pmatrix} = \\begin{pmatrix} 5  \\frac{1}{2}  6 \\\\ \\frac{1}{2}  2  0 \\end{pmatrix}\n$$\nNext, we compute $\\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}$:\n$$\n\\mathbf{H}\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} = \\begin{pmatrix} 5  \\frac{1}{2}  6 \\\\ \\frac{1}{2}  2  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 5(1)+6(1)  5(0)+\\frac{1}{2}(1) \\\\ \\frac{1}{2}(1)+0(1)  \\frac{1}{2}(0)+2(1) \\end{pmatrix} = \\begin{pmatrix} 11  \\frac{1}{2} \\\\ \\frac{1}{2}  2 \\end{pmatrix}\n$$\nNow, we add $\\mathbf{R}$ to form $\\mathbf{M}$:\n$$\n\\mathbf{M} = \\begin{pmatrix} 11  \\frac{1}{2} \\\\ \\frac{1}{2}  2 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 12  \\frac{1}{2} \\\\ \\frac{1}{2}  4 \\end{pmatrix}\n$$\n\nStep 3: Compute the inverse $\\mathbf{M}^{-1}$.\nThe determinant of $\\mathbf{M}$ is:\n$$\n\\det(\\mathbf{M}) = (12)(4) - \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = 48 - \\frac{1}{4} = \\frac{191}{4}\n$$\nThe inverse is:\n$$\n\\mathbf{M}^{-1} = \\frac{1}{\\det(\\mathbf{M})} \\begin{pmatrix} 4  -\\frac{1}{2} \\\\ -\\frac{1}{2}  12 \\end{pmatrix} = \\frac{4}{191} \\begin{pmatrix} 4  -\\frac{1}{2} \\\\ -\\frac{1}{2}  12 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 16  -2 \\\\ -2  48 \\end{pmatrix}\n$$\n\nStep 4: Compute the term $\\mathbf{M}^{-1}\\mathbf{d}$.\n$$\n\\mathbf{M}^{-1}\\mathbf{d} = \\frac{1}{191} \\begin{pmatrix} 16  -2 \\\\ -2  48 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 16(2) + (-2)(-1) \\\\ -2(2) + 48(-1) \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 32+2 \\\\ -4-48 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 34 \\\\ -52 \\end{pmatrix}\n$$\n\nStep 5: Compute the term $\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}$.\nThis is the transpose of the matrix $\\mathbf{H}\\mathbf{B}_{\\text{hyb}}$ calculated in Step 2, since $\\mathbf{B}_{\\text{hyb}}$ is symmetric.\n$$\n\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top} = \\begin{pmatrix} 5  \\frac{1}{2}  6 \\\\ \\frac{1}{2}  2  0 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 5  \\frac{1}{2} \\\\ \\frac{1}{2}  2 \\\\ 6  0 \\end{pmatrix}\n$$\n\nStep 6: Compute the final analysis increment $\\delta \\mathbf{x}$.\n$$\n\\delta \\mathbf{x} = (\\mathbf{B}_{\\text{hyb}}\\mathbf{H}^{\\top}) (\\mathbf{M}^{-1}\\mathbf{d})\n$$\n$$\n\\delta \\mathbf{x} = \\begin{pmatrix} 5  \\frac{1}{2} \\\\ \\frac{1}{2}  2 \\\\ 6  0 \\end{pmatrix} \\left( \\frac{1}{191} \\begin{pmatrix} 34 \\\\ -52 \\end{pmatrix} \\right) = \\frac{1}{191} \\begin{pmatrix} 5(34) + \\frac{1}{2}(-52) \\\\ \\frac{1}{2}(34) + 2(-52) \\\\ 6(34) + 0(-52) \\end{pmatrix}\n$$\n$$\n\\delta \\mathbf{x} = \\frac{1}{191} \\begin{pmatrix} 170 - 26 \\\\ 17 - 104 \\\\ 204 \\end{pmatrix} = \\frac{1}{191} \\begin{pmatrix} 144 \\\\ -87 \\\\ 204 \\end{pmatrix} = \\begin{pmatrix} \\frac{144}{191} \\\\ -\\frac{87}{191} \\\\ \\frac{204}{191} \\end{pmatrix}\n$$\nThe problem asks for the result as a single row matrix.\n$$\n\\delta \\mathbf{x}^{\\top} = \\begin{pmatrix} \\frac{144}{191}  -\\frac{87}{191}  \\frac{204}{191} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{144}{191}  -\\frac{87}{191}  \\frac{204}{191} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Calculating an analysis update is only half the battle; assessing its quality is equally critical. This final practice moves from pure mechanics to system diagnostics, a vital skill for any practitioner. In this coding exercise, you will implement a complete analysis cycle for a toy model and evaluate its performance using standard metrics like Root Mean Square Error (RMSE) and ensemble spread. The goal is to compare the system's self-perceived uncertainty (spread) with its actual error (RMSE), allowing you to diagnose its health and determine if corrective measures like covariance inflation are needed. This exercise provides a crucial bridge from theory to the practical realities of developing and maintaining a robust data assimilation system .",
            "id": "4053150",
            "problem": "Consider a linear-Gaussian data assimilation setting for a toy system in which the state vector is denoted by $x \\in \\mathbb{R}^n$, the forecast (background) ensemble consists of $m$ members with matrix $X_f \\in \\mathbb{R}^{n \\times m}$ whose columns are the ensemble members, the observation operator is linear $H \\in \\mathbb{R}^{p \\times n}$, and the observational error covariance is a symmetric positive-definite matrix $R \\in \\mathbb{R}^{p \\times p}$. The hybrid variational–ensemble prior covariance $B_{\\text{hyb}}$ is formed by combining a climatological (variational) covariance $B_{\\text{clim}} \\in \\mathbb{R}^{n \\times n}$ with the ensemble sample covariance $B_{\\text{ens}} \\in \\mathbb{R}^{n \\times n}$ via a convex weight $w \\in [0,1]$, namely $B_{\\text{hyb}} = (1-w) B_{\\text{clim}} + w B_{\\text{ens}}$. You must compute the Root Mean Square Error (RMSE) and the ensemble spread before and after the analysis, and assess spread-error consistency and the need for multiplicative inflation under hybrid covariance updates. The RMSE is defined between an ensemble mean (or analysis mean) and a prescribed true state $x_{\\text{true}}$, and the spread is defined from the appropriate covariance by the square root of the average variance across state components.\n\nStarting from fundamental probabilistic principles of linear-Gaussian estimation, specifically Bayes' rule applied to a Gaussian prior for $x$ and a Gaussian likelihood for $y = Hx + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,R)$, and from the definitions of ensemble sample mean and covariance, derive the analysis mean and covariance implied by this setting when the prior covariance is $B_{\\text{hyb}}$. Use these to compute the requested diagnostics. Do not assume any shortcut formulas not derivable from these principles.\n\nDefinitions to be used:\n- The ensemble mean of the forecast is the arithmetic mean $\\bar{x}_f = \\frac{1}{m} \\sum_{j=1}^{m} x_f^{(j)}$, where $x_f^{(j)}$ are columns of $X_f$.\n- The ensemble sample covariance is $B_{\\text{ens}} = \\frac{1}{m-1} \\sum_{j=1}^{m} (x_f^{(j)} - \\bar{x}_f)(x_f^{(j)} - \\bar{x}_f)^\\top$.\n- The RMSE between a mean estimate $x_\\mu$ and the true state $x_{\\text{true}}$ is $\\text{RMSE}(x_\\mu, x_{\\text{true}}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{\\mu,i} - x_{\\text{true},i})^2}$.\n- The spread corresponding to a covariance $P$ is $\\text{spread}(P) = \\sqrt{\\frac{1}{n} \\operatorname{trace}(P)}$.\n- Spread-error consistency is assessed via the ratio $r = \\frac{\\text{spread}}{\\text{RMSE}}$; under-dispersion is indicated when $r$ is below a specified threshold $\\tau \\in (0,1]$, suggesting a need for multiplicative inflation.\n\nYour program must, for each test case below, compute:\n1. The pre-analysis RMSE, $\\text{RMSE}_{\\text{pre}}$, using the forecast ensemble mean $\\bar{x}_f$ and $x_{\\text{true}}$.\n2. The pre-analysis ensemble spread, $\\text{spread}_{\\text{pre}}$, using $B_{\\text{ens}}$.\n3. The analysis mean $x_a$ and analysis covariance $P_a$ implied by linear-Gaussian analysis with hybrid covariance $B_{\\text{hyb}}$.\n4. The post-analysis RMSE, $\\text{RMSE}_{\\text{post}}$, using $x_a$ and $x_{\\text{true}}$.\n5. The post-analysis spread, $\\text{spread}_{\\text{post}}$, using $P_a$.\n6. The post-analysis spread-error ratio $r_{\\text{post}} = \\frac{\\text{spread}_{\\text{post}}}{\\text{RMSE}_{\\text{post}}}$.\n7. A boolean recommendation for inflation, $I$, defined as $I = \\text{True}$ if $r_{\\text{post}}  \\tau$, and $I = \\text{False}$ otherwise.\n\nNo physical units are involved in this toy system, so no unit conversions are needed. Angles do not appear. Express all final numeric results as floats or booleans. There are no percentages.\n\nTest suite parameter values:\n- Case A (general case):\n  - $n = 3$, $m = 5$.\n  - $x_{\\text{true}} = [\\,2.0,\\,-1.0,\\,0.5\\,]$.\n  - $X_f = \\begin{bmatrix} 1.7  2.1  1.9  2.2  1.8 \\\\ -0.9  -1.2  -0.8  -1.1  -1.0 \\\\ 0.6  0.4  0.7  0.3  0.6 \\end{bmatrix}$.\n  - $H = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}$.\n  - $y = [\\,2.05,\\,0.45\\,]$.\n  - $R = \\operatorname{diag}([\\,0.1,\\,0.1\\,])$.\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,0.9,\\,0.9,\\,0.9\\,])$.\n  - $w = 0.5$.\n  - $\\tau = 0.9$.\n- Case B (under-dispersion edge case, pure ensemble):\n  - $n = 3$, $m = 4$.\n  - $x_{\\text{true}} = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n  - $X_f = \\begin{bmatrix} 0.52  0.50  0.51  0.49 \\\\ -0.48  -0.50  -0.49  -0.51 \\\\ 0.21  0.19  0.20  0.22 \\end{bmatrix}$.\n  - $H = I_{3 \\times 3}$.\n  - $y = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n  - $R = \\operatorname{diag}([\\,0.01,\\,0.01,\\,0.01\\,])$.\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,1.0,\\,1.0,\\,1.0\\,])$.\n  - $w = 1.0$.\n  - $\\tau = 1.0$.\n- Case C (over-dispersion scenario, hybrid dominated by climatology):\n  - $n = 3$, $m = 5$.\n  - $x_{\\text{true}} = [\\,1.0,\\,1.0,\\,1.0\\,]$.\n  - $X_f = \\begin{bmatrix} 0.0  2.0  1.5  0.5  1.0 \\\\ 2.0  0.0  1.5  0.5  1.0 \\\\ 1.5  0.5  2.0  0.0  1.0 \\end{bmatrix}$.\n  - $H = I_{3 \\times 3}$.\n  - $y = [\\,1.1,\\,0.9,\\,1.05\\,]$.\n  - $R = \\operatorname{diag}([\\,0.5,\\,0.5,\\,0.5\\,])$.\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,0.8,\\,0.3,\\,1.2\\,])$.\n  - $w = 0.2$.\n  - $\\tau = 0.9$.\n- Case D (boundary case, pure variational climatology, weakly informative observations):\n  - $n = 3$, $m = 4$.\n  - $x_{\\text{true}} = [\\,1.0,\\,-2.0,\\,0.0\\,]$.\n  - $X_f = \\begin{bmatrix} 1.1  0.9  1.2  0.8 \\\\ -1.9  -2.1  -1.8  -2.2 \\\\ 0.1  -0.1  0.0  0.2 \\end{bmatrix}$.\n  - $H = \\begin{bmatrix} 1  0  0 \\end{bmatrix}$.\n  - $y = [\\,0.8\\,]$.\n  - $R = [\\,[\\,5.0\\,]\\,]$ (a $1 \\times 1$ matrix).\n  - $B_{\\text{clim}} = \\operatorname{diag}([\\,0.5,\\,0.5,\\,0.5\\,])$.\n  - $w = 0.0$.\n  - $\\tau = 0.9$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order $[\\,\\text{RMSE}_{\\text{pre}},\\,\\text{spread}_{\\text{pre}},\\,\\text{RMSE}_{\\text{post}},\\,\\text{spread}_{\\text{post}},\\,r_{\\text{post}},\\,I\\,]$. For example, a valid output would look like $[\\,[\\,0.1,\\,0.2,\\,0.05,\\,0.15,\\,3.0,\\,\\text{False}\\,],\\,[\\,\\dots\\,]\\,]$.",
            "solution": "The problem is deemed valid. It is a well-posed, scientifically grounded exercise in linear-Gaussian data assimilation, a core topic in numerical weather prediction and related fields. All data, definitions, and conditions are explicitly provided and are self-consistent.\n\nThe solution proceeds by first deriving the analysis mean and covariance from fundamental principles of Bayesian inference. These results are then applied to the specific computational tasks outlined in the problem.\n\n### 1. Bayesian Formulation of Linear-Gaussian Analysis\n\nThe analysis seeks the posterior probability distribution of the state vector $x \\in \\mathbb{R}^n$ given an observation $y \\in \\mathbb{R}^p$. According to Bayes' rule, the posterior probability density function $p(x|y)$ is proportional to the product of the likelihood $p(y|x)$ and the prior $p(x)$:\n$$p(x|y) \\propto p(y|x) p(x)$$\nThe state that maximizes this posterior probability is known as the Maximum A Posteriori (MAP) estimate. For Gaussian distributions, the MAP estimate coincides with the posterior mean.\n\nThe prior distribution of the state $x$ is assumed to be Gaussian, with a mean equal to the forecast ensemble mean $\\bar{x}_f$ and a covariance given by the hybrid covariance matrix $B_{\\text{hyb}}$.\n$$p(x) = \\mathcal{N}(x; \\bar{x}_f, B_{\\text{hyb}}) \\propto \\exp\\left(-\\frac{1}{2}(x - \\bar{x}_f)^\\top B_{\\text{hyb}}^{-1}(x - \\bar{x}_f)\\right)$$\n\nThe observation model is linear, $y = Hx + \\varepsilon$, where the observational error $\\varepsilon$ is drawn from a zero-mean Gaussian distribution with covariance $R$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that the likelihood of observing $y$ given the state $x$ is also Gaussian:\n$$p(y|x) = \\mathcal{N}(y; Hx, R) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^\\top R^{-1}(y - Hx)\\right)$$\n\nCombining the prior and likelihood, the logarithm of the posterior distribution is:\n$$\\ln p(x|y) = -\\frac{1}{2} \\left[ (x - \\bar{x}_f)^\\top B_{\\text{hyb}}^{-1}(x - \\bar{x}_f) + (y - Hx)^\\top R^{-1}(y - Hx) \\right] + \\text{constant}$$\nThe analysis state $x_a$ is the state that minimizes the quadratic cost function $J(x)$ in the exponent:\n$$J(x) = (x - \\bar{x}_f)^\\top B_{\\text{hyb}}^{-1}(x - \\bar{x}_f) + (y - Hx)^\\top R^{-1}(y - Hx)$$\nTo find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero:\n$$\\nabla_x J(x) = 2B_{\\text{hyb}}^{-1}(x - \\bar{x}_f) - 2H^\\top R^{-1}(y - Hx) = 0$$\nSetting $x = x_a$, we solve for the analysis mean:\n$$B_{\\text{hyb}}^{-1}(x_a - \\bar{x}_f) + H^\\top R^{-1}(Hx_a - y) = 0$$\n$$(B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H)x_a = B_{\\text{hyb}}^{-1}\\bar{x}_f + H^\\top R^{-1}y$$\n$$x_a = (B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H)^{-1}(B_{\\text{hyb}}^{-1}\\bar{x}_f + H^\\top R^{-1}y)$$\nThe analysis covariance matrix $P_a$ is the inverse of the Hessian of the negative log-posterior, which is $\\frac{1}{2}J(x)$. The Hessian of $\\frac{1}{2}J(x)$ is:\n$$P_a^{-1} = \\frac{1}{2} \\nabla_x^2 J(x) = B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H$$\nTherefore, the analysis covariance is:\n$$P_a = (B_{\\text{hyb}}^{-1} + H^\\top R^{-1}H)^{-1}$$\n\n### 2. Kalman Gain Formulation\n\nFor computational stability and efficiency, the above expressions are typically reformulated using the Woodbury matrix identity. This leads to the well-known Kalman gain formulation.\nThe analysis mean can be expressed as an update to the forecast mean:\n$$x_a = \\bar{x}_f + K(y - H\\bar{x}_f)$$\nand the analysis covariance as an update to the background covariance:\n$$P_a = (I - KH)B_{\\text{hyb}}$$\nwhere $I$ is the identity matrix and $K$ is the Kalman gain matrix, defined as:\n$$K = B_{\\text{hyb}}H^\\top(HB_{\\text{hyb}}H^\\top + R)^{-1}$$\nThis formulation avoids the inversion of the $n \\times n$ matrix $B_{\\text{hyb}}$ and instead requires the inversion of the smaller $p \\times p$ matrix $HB_{\\text{hyb}}H^\\top + R$. This is the formulation used in the implementation.\n\n### 3. Computational Steps\n\nFor each test case, the following quantities are computed in sequence:\n\n1.  **Forecast Ensemble Mean $\\bar{x}_f$**:\n    Using the provided forecast ensemble matrix $X_f \\in \\mathbb{R}^{n \\times m}$:\n    $$\\bar{x}_f = \\frac{1}{m} \\sum_{j=1}^{m} x_f^{(j)}$$\n\n2.  **Pre-analysis RMSE ($\\text{RMSE}_{\\text{pre}}$)**:\n    The Root Mean Square Error between the forecast mean $\\bar{x}_f$ and the true state $x_{\\text{true}}$:\n    $$\\text{RMSE}_{\\text{pre}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\bar{x}_{f,i} - x_{\\text{true},i})^2}$$\n\n3.  **Ensemble Sample Covariance $B_{\\text{ens}}$**:\n    $$B_{\\text{ens}} = \\frac{1}{m-1} \\sum_{j=1}^{m} (x_f^{(j)} - \\bar{x}_f)(x_f^{(j)} - \\bar{x}_f)^\\top$$\n\n4.  **Pre-analysis Spread ($\\text{spread}_{\\text{pre}}$)**:\n    The spread of the forecast ensemble, calculated from its sample covariance $B_{\\text{ens}}$:\n    $$\\text{spread}_{\\text{pre}} = \\sqrt{\\frac{1}{n} \\operatorname{trace}(B_{\\text{ens}})}$$\n\n5.  **Hybrid Background Covariance $B_{\\text{hyb}}$**:\n    A convex combination of the climatological covariance $B_{\\text{clim}}$ and the ensemble covariance $B_{\\text{ens}}$ with weight $w$:\n    $$B_{\\text{hyb}} = (1-w) B_{\\text{clim}} + w B_{\\text{ens}}$$\n\n6.  **Analysis Mean $x_a$ and Covariance $P_a$**:\n    Calculated using the Kalman gain formulation derived above:\n    $$K = B_{\\text{hyb}}H^\\top(HB_{\\text{hyb}}H^\\top + R)^{-1}$$\n    $$x_a = \\bar{x}_f + K(y - H\\bar{x}_f)$$\n    $$P_a = (I - KH)B_{\\text{hyb}}$$\n\n7.  **Post-analysis RMSE ($\\text{RMSE}_{\\text{post}}$)**:\n    The Root Mean Square Error between the analysis mean $x_a$ and the true state $x_{\\text{true}}$:\n    $$\\text{RMSE}_{\\text{post}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{a,i} - x_{\\text{true},i})^2}$$\n\n8.  **Post-analysis Spread ($\\text{spread}_{\\text{post}}$)**:\n    The spread corresponding to the analysis error covariance $P_a$:\n    $$\\text{spread}_{\\text{post}} = \\sqrt{\\frac{1}{n} \\operatorname{trace}(P_a)}$$\n\n9.  **Post-analysis Spread-Error Ratio ($r_{\\text{post}}$)**:\n    The ratio of post-analysis spread to post-analysis RMSE:\n    $$r_{\\text{post}} = \\frac{\\text{spread}_{\\text{post}}}{\\text{RMSE}_{\\text{post}}}$$\n\n10. **Inflation Recommendation ($I$)**:\n    A boolean value indicating the need for multiplicative inflation, true if the ensemble is under-dispersive:\n    $$I = (r_{\\text{post}}  \\tau)$$\n\nThese steps are implemented for each provided test case to produce the final results.",
            "answer": "```\n[[0.1581138830,0.1779513042,0.0829871502,0.1788755694,2.1554625248,False],[0.4137233261,0.0132287566,0.0279624967,0.0097721894,0.3494747768,True],[0.2041241452,0.7224987393,0.0912870929,0.5966373111,6.5359146584,False],[0.1290994449,0.1652269932,0.0950337374,0.6558661449,6.8992147171,False]]\n```"
        }
    ]
}