{
    "hands_on_practices": [
        {
            "introduction": "Mastering Four-Dimensional Variational Data Assimilation (4D-Var) begins with the strong-constraint formulation, where the forecast model is assumed to be perfect. This exercise provides a foundational, hands-on calculation of the analysis increment for a small linear system . By deriving and solving the normal equations from first principles, you will gain direct experience with the mathematical machinery that translates Bayesian estimation theory into a practical solution for the optimal initial state.",
            "id": "4112085",
            "problem": "Consider Four-Dimensional Variational Data Assimilation (4D-Var) in the strong-constraint setting, where the model is assumed perfect over the assimilation window. Let the discrete-time linearized dynamics be given by the state-transition matrices $A_{1}$ and $A_{2}$, so that the state at time index $k$ is $x_{k} = M_{k:0} x_{0}$ with $M_{1:0} = A_{1}$ and $M_{2:0} = A_{2} A_{1}$. Assume a linear observation operator $H$ and additive Gaussian background and observational errors. The maximum a posteriori estimator is obtained by minimizing a least-squares cost function constructed from the background and observation error covariances.\n\nYou are given the following specific numerical instance:\n- The state dimension is $n = 2$, and the observation operator is $H = I$.\n- The background state is $x_{b} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, and the background error covariance is $B = \\sigma_{b}^{2} I$ with $\\sigma_{b}^{2} = 4$.\n- Two observation times $k = 1, 2$ are assimilated. The observation error covariances are $R_{1} = \\sigma_{r,1}^{2} I$ with $\\sigma_{r,1}^{2} = 1$ and $R_{2} = \\sigma_{r,2}^{2} I$ with $\\sigma_{r,2}^{2} = 9$.\n- The observations are $y_{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $y_{2} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}$.\n- The linear propagators are\n$$\nA_{1} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nA_{2} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\n\nStarting from first principles of Gaussian maximum a posteriori estimation for strong-constraint 4D-Var, derive the normal equations for the initial-time analysis state $x_{0}^{a}$ and solve them for this instance. Then compute the initial-time analysis increment $\\delta x_{0} = x_{0}^{a} - x_{b}$. Finally, compute the Euclidean norm $\\|\\delta x_{0}\\|_{2}$.\n\nAll state variables are dimensionless, so no physical units are required. Provide the final answer as the two components of $\\delta x_{0}$ followed by its Euclidean norm in a single row matrix. No rounding is required; give exact values.",
            "solution": "The user wants to solve a strong-constraint 4D-Var data assimilation problem.\n\n### Step 1: Extract Givens\n\n-   **State Dimension**: $n = 2$.\n-   **Linear Observation Operator**: $H = I$, where $I$ is the $2 \\times 2$ identity matrix.\n-   **Background State**: $x_{b} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n-   **Background Error Covariance**: $B = \\sigma_{b}^{2} I$ with $\\sigma_{b}^{2} = 4$. So, $B = 4I$.\n-   **Observation Times**: $k = 1, 2$.\n-   **Observation Error Covariance at $k=1$**: $R_{1} = \\sigma_{r,1}^{2} I$ with $\\sigma_{r,1}^{2} = 1$. So, $R_{1} = I$.\n-   **Observation Error Covariance at $k=2$**: $R_{2} = \\sigma_{r,2}^{2} I$ with $\\sigma_{r,2}^{2} = 9$. So, $R_{2} = 9I$.\n-   **Observations**: $y_{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $y_{2} = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}$.\n-   **Linear Propagators (State-Transition Matrices)**:\n    $A_{1} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$, $A_{2} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n-   **Model Dynamics**: The state at time $k$, $x_k$, is related to the initial state $x_0$ by $x_{k} = M_{k:0} x_{0}$, with $M_{1:0} = A_{1}$ and $M_{2:0} = A_{2} A_{1}$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is a standard application of Bayesian estimation theory (specifically, maximum a posteriori estimation for a linear-Gaussian system) to the field of data assimilation, representing a textbook strong-constraint 4D-Var scenario. This is scientifically sound.\n-   **Well-Posed**: The problem provides all necessary information (states, operators, covariances) to uniquely define the cost function. The resulting normal equations form a linear system for which a unique solution is expected, as the Hessian is typically positive definite.\n-   **Objective**: The problem is stated in precise mathematical language with no subjective or ambiguous terms.\n-   **Completeness**: All required matrices, vectors, and constants are provided. The problem is self-contained.\n-   **Consistency**: The dimensions of all matrices and vectors are consistent for the required operations (e.g., $A_1$, $A_2$, $x_b$, $y_1$, $y_2$ are all compatible with a state dimension of $n=2$).\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\n### Solution Derivation\n\nThe strong-constraint 4D-Var seeks to find the initial state $x_0$ that minimizes a cost function $J(x_0)$. This cost function measures the misfit between the model trajectory starting from $x_0$ and the available information: the background state $x_b$ and the observations $y_k$. Assuming Gaussian errors, this is equivalent to finding the maximum a posteriori estimate of $x_0$. The cost function is the sum of the background term $J_b$ and the observation term $J_o$:\n$$J(x_0) = J_b(x_0) + J_o(x_0)$$\nwhere\n$$J_b(x_0) = \\frac{1}{2} (x_0 - x_b)^T B^{-1} (x_0 - x_b)$$\n$$J_o(x_0) = \\frac{1}{2} \\sum_{k=1}^{2} (H_k x_k - y_k)^T R_k^{-1} (H_k x_k - y_k)$$\nSince the model is the strong constraint, we have $x_k = M_{k:0} x_0$. The observation operator $H_k = H = I$ for all $k$. The cost function becomes:\n$$J(x_0) = \\frac{1}{2} (x_0 - x_b)^T B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=1}^{2} (M_{k:0} x_0 - y_k)^T R_k^{-1} (M_{k:0} x_0 - y_k)$$\nTo find the analysis state $x_0^a$ that minimizes $J(x_0)$, we compute the gradient of $J$ with respect to $x_0$ and set it to zero:\n$$\\nabla_{x_0} J(x_0) = B^{-1}(x_0 - x_b) + \\sum_{k=1}^{2} M_{k:0}^T R_k^{-1} (M_{k:0} x_0 - y_k) = 0$$\nRearranging the terms to solve for the analysis state $x_0^a$:\n$$\\left( B^{-1} + \\sum_{k=1}^{2} M_{k:0}^T R_k^{-1} M_{k:0} \\right) x_0^a = B^{-1}x_b + \\sum_{k=1}^{2} M_{k:0}^T R_k^{-1} y_k$$\nThis is a linear system of the form $P x_0^a = q$, which are the normal equations.\n\nFirst, we compute the necessary matrices and their inverses:\n-   $B = 4I \\implies B^{-1} = \\frac{1}{4}I = \\begin{pmatrix} 1/4 & 0 \\\\ 0 & 1/4 \\end{pmatrix}$\n-   $R_1 = 1I \\implies R_1^{-1} = I$\n-   $R_2 = 9I \\implies R_2^{-1} = \\frac{1}{9}I$\n-   $M_{1:0} = A_1 = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$\n-   $M_{2:0} = A_2 A_1 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 \\\\ 0 & 1 \\end{pmatrix}$\n\nNext, we calculate the Hessian matrix $P = B^{-1} + M_{1:0}^T R_1^{-1} M_{1:0} + M_{2:0}^T R_2^{-1} M_{2:0}$:\n-   $M_{1:0}^T R_1^{-1} M_{1:0} = A_1^T I A_1 = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n-   $M_{2:0}^T R_2^{-1} M_{2:0} = M_{2:0}^T (\\frac{1}{9}I) M_{2:0} = \\frac{1}{9} \\begin{pmatrix} 2 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 4 & 4 \\\\ 4 & 5 \\end{pmatrix} = \\begin{pmatrix} 4/9 & 4/9 \\\\ 4/9 & 5/9 \\end{pmatrix}$\n-   $P = \\begin{pmatrix} 1/4 & 0 \\\\ 0 & 1/4 \\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 4/9 & 4/9 \\\\ 4/9 & 5/9 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}+1+\\frac{4}{9} & 1+\\frac{4}{9} \\\\ 1+\\frac{4}{9} & \\frac{1}{4}+2+\\frac{5}{9} \\end{pmatrix}$\n-   $P = \\begin{pmatrix} \\frac{9+36+16}{36} & \\frac{9+4}{9} \\\\ \\frac{9+4}{9} & \\frac{9+72+20}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{61}{36} & \\frac{13}{9} \\\\ \\frac{13}{9} & \\frac{101}{36} \\end{pmatrix} = \\frac{1}{36}\\begin{pmatrix} 61 & 52 \\\\ 52 & 101 \\end{pmatrix}$\n\nNow, we calculate the forcing vector $q = B^{-1}x_b + M_{1:0}^T R_1^{-1} y_1 + M_{2:0}^T R_2^{-1} y_2$:\n-   $B^{-1}x_b = \\frac{1}{4} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ -1/4 \\end{pmatrix}$\n-   $M_{1:0}^T R_1^{-1} y_1 = A_1^T I y_1 = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$\n-   $M_{2:0}^T R_2^{-1} y_2 = M_{2:0}^T (\\frac{1}{9}I) y_2 = \\frac{1}{9} \\begin{pmatrix} 2 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 8 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix} 8/9 \\\\ 10/9 \\end{pmatrix}$\n-   $q = \\begin{pmatrix} \\frac{1}{4}+3+\\frac{8}{9} \\\\ -\\frac{1}{4}+4+\\frac{10}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{9+108+32}{36} \\\\ \\frac{-9+144+40}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{149}{36} \\\\ \\frac{175}{36} \\end{pmatrix} = \\frac{1}{36}\\begin{pmatrix} 149 \\\\ 175 \\end{pmatrix}$\n\nWe solve the system $P x_0^a = q$:\n$\\frac{1}{36}\\begin{pmatrix} 61 & 52 \\\\ 52 & 101 \\end{pmatrix} x_0^a = \\frac{1}{36}\\begin{pmatrix} 149 \\\\ 175 \\end{pmatrix} \\implies \\begin{pmatrix} 61 & 52 \\\\ 52 & 101 \\end{pmatrix} x_0^a = \\begin{pmatrix} 149 \\\\ 175 \\end{pmatrix}$\nTo find $x_0^a$, we invert the matrix. The determinant is $\\det(P_{36}) = (61)(101) - (52)(52) = 6161 - 2704 = 3457$.\nThe inverse is $\\frac{1}{3457}\\begin{pmatrix} 101 & -52 \\\\ -52 & 61 \\end{pmatrix}$.\n$$x_0^a = \\frac{1}{3457}\\begin{pmatrix} 101 & -52 \\\\ -52 & 61 \\end{pmatrix} \\begin{pmatrix} 149 \\\\ 175 \\end{pmatrix} = \\frac{1}{3457}\\begin{pmatrix} (101)(149) - (52)(175) \\\\ (-52)(149) + (61)(175) \\end{pmatrix}$$\n$$x_0^a = \\frac{1}{3457}\\begin{pmatrix} 15049 - 9100 \\\\ -7748 + 10675 \\end{pmatrix} = \\frac{1}{3457}\\begin{pmatrix} 5949 \\\\ 2927 \\end{pmatrix}$$\nThe analysis state is $x_0^a = \\begin{pmatrix} 5949/3457 \\\\ 2927/3457 \\end{pmatrix}$.\n\nThe initial-time analysis increment is $\\delta x_0 = x_0^a - x_b$:\n$$\\delta x_0 = \\begin{pmatrix} \\frac{5949}{3457} \\\\ \\frac{2927}{3457} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{5949-3457}{3457} \\\\ \\frac{2927+3457}{3457} \\end{pmatrix} = \\begin{pmatrix} \\frac{2492}{3457} \\\\ \\frac{6384}{3457} \\end{pmatrix}$$\nThe two components of the increment are $\\delta x_{0,1} = \\frac{2492}{3457}$ and $\\delta x_{0,2} = \\frac{6384}{3457}$.\n\nFinally, we compute the Euclidean norm $\\|\\delta x_0\\|_2$:\n$$\\|\\delta x_0\\|_2^2 = \\left(\\frac{2492}{3457}\\right)^2 + \\left(\\frac{6384}{3457}\\right)^2 = \\frac{2492^2 + 6384^2}{3457^2}$$\n$$2492^2 = 6210064$$\n$$6384^2 = 40755456$$\n$$\\|\\delta x_0\\|_2^2 = \\frac{6210064 + 40755456}{3457^2} = \\frac{46965520}{3457^2}$$\nThe norm is the square root of this value:\n$$\\|\\delta x_0\\|_2 = \\frac{\\sqrt{46965520}}{3457}$$\nWe can simplify the term under the square root: $46965520 = 16 \\times 2935345$.\n$$\\|\\delta x_0\\|_2 = \\frac{\\sqrt{16 \\times 2935345}}{3457} = \\frac{4\\sqrt{2935345}}{3457}$$\nThe three requested values are the components of $\\delta x_0$ and its norm.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2492}{3457} & \\frac{6384}{3457} & \\frac{4\\sqrt{2935345}}{3457}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond the perfect model assumption, weak-constraint 4D-Var treats model error as an additional unknown to be estimated. This analytical exercise delves into the heart of this concept by asking you to derive the posterior mean of the model error for a simple scalar system . Your derivation will reveal how the analysis judiciously attributes forecast-observation misfit to either the initial conditions or the model dynamics, based on their respective prior uncertainties specified in the error covariance matrices $B$ and $Q$.",
            "id": "4112110",
            "problem": "Consider a single-step scalar linear data assimilation problem formulated under weak-constraint four-dimensional variational (4D-Var) data assimilation. Let the true system evolve according to the linear model with additive model error, where the model state at the next time is given by $x_1 = M x_0 + \\eta_0$. Here, $x_0$ is the initial state and $\\eta_0$ is the model error at time $0$. A single observation at time $1$ is given by $y_1 = H x_1 + \\varepsilon_1$. Assume the following probabilistic structure:\n- The background (prior) on the initial state is Gaussian: $x_0 \\sim \\mathcal{N}(x_b, B)$.\n- The model error is Gaussian: $\\eta_0 \\sim \\mathcal{N}(0, Q)$.\n- The observation error is Gaussian: $\\varepsilon_1 \\sim \\mathcal{N}(0, R)$.\n- The random variables $x_0$, $\\eta_0$, and $\\varepsilon_1$ are mutually independent.\n\nIn the weak-constraint formulation, the control vector includes both $x_0$ and $\\eta_0$. Starting from Bayes’ theorem for linear-Gaussian models and the independence assumptions above, derive the posterior mean of $\\eta_0$ conditioned on the observation $y_1$, expressed solely in terms of $y_1$, $H$, $M$, $x_b$, $B$, $Q$, and $R$. Provide your final result as a single closed-form analytic expression. Then, briefly explain in words how the magnitude of this posterior mean reflects a balance between the observation misfit and the prior model-error variance $Q$, and how the limiting cases $Q \\to 0$ and $Q \\to \\infty$ relate to the strong-constraint and weak-constraint behaviors.\n\nYour final answer must be the single analytic expression for the posterior mean of $\\eta_0$; no numerical approximation or units are required.",
            "solution": "The user wants me to solve a problem in the setting of weak-constraint 4D-Var.\n\n### Step 1: Extract Givens\n- **Model Evolution:** $x_1 = M x_0 + \\eta_0$.\n- **Observation Model:** $y_1 = H x_1 + \\varepsilon_1$.\n- **Prior on Initial State:** $x_0 \\sim \\mathcal{N}(x_b, B)$.\n- **Prior on Model Error:** $\\eta_0 \\sim \\mathcal{N}(0, Q)$.\n- **Prior on Observation Error:** $\\varepsilon_1 \\sim \\mathcal{N}(0, R)$.\n- **Independence:** The variables $x_0$, $\\eta_0$, and $\\varepsilon_1$ are mutually independent.\n- **Control Vector:** The analysis seeks to estimate both $x_0$ and $\\eta_0$.\n- **Objective:** Derive the posterior mean of the model error, $\\mathbb{E}[\\eta_0 | y_1]$, and provide a qualitative interpretation.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is a canonical example of Bayesian data assimilation under linear-Gaussian assumptions. It forms the theoretical basis for both Kalman filtering and variational methods (4D-Var). This is a well-established and fundamental topic in numerical weather prediction, oceanography, and other geophysical sciences. The problem is scientifically sound.\n- **Well-Posedness:** The problem is well-posed. All necessary probabilistic models (priors and likelihood) are specified, and the goal is to compute a posterior expectation, which is a uniquely defined quantity for the given Gaussian system.\n- **Objectivity:** The problem is stated in precise, objective, mathematical language.\n- **Completeness and Consistency:** All required variables, constants, and distributions are provided. There are no contradictions.\n- **Feasibility:** The setup is a standard, albeit simplified (scalar, single-step), model used for theoretical derivations. It is perfectly feasible.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and solution.\n\nThe problem is to find the posterior mean of the model error, $\\eta_0$, given the observation $y_1$. In a linear-Gaussian framework, the posterior distribution is also Gaussian. The mean of this posterior distribution is equivalent to the state that minimizes the weak-constraint 4D-Var cost function. This cost function, $J(x_0, \\eta_0)$, is derived from the negative logarithm of the posterior probability density function, $p(x_0, \\eta_0 | y_1)$.\n\nUsing Bayes' theorem, $p(x_0, \\eta_0 | y_1) \\propto p(y_1 | x_0, \\eta_0) p(x_0, \\eta_0)$. Given the independence of the prior estimates, $p(x_0, \\eta_0) = p(x_0) p(\\eta_0)$. The cost function is the sum of the negative logarithms of these probability densities (ignoring constant terms):\n$J(x_0, \\eta_0) = -2 \\ln p(x_0, \\eta_0 | y_1) + \\text{const.}$\n$J(x_0, \\eta_0) = J_b + J_q + J_o$\nwhere:\n- $J_b = (x_0 - x_b)^T B^{-1} (x_0 - x_b)$ is the background term for the initial state.\n- $J_q = \\eta_0^T Q^{-1} \\eta_0$ is the model error penalty term.\n- $J_o = (y_1 - Hx_1)^T R^{-1} (y_1 - Hx_1)$ is the observation term.\n\nSubstituting the model equation $x_1 = M x_0 + \\eta_0$ into $J_o$ and writing the expressions for the scalar case, the total cost function is:\n$$\nJ(x_0, \\eta_0) = \\frac{1}{2} \\frac{(x_0 - x_b)^2}{B} + \\frac{1}{2} \\frac{\\eta_0^2}{Q} + \\frac{1}{2} \\frac{(y_1 - H(M x_0 + \\eta_0))^2}{R}\n$$\nTo find the posterior mode (which is the mean for a Gaussian posterior), we must find the values of $x_0$ and $\\eta_0$ that minimize $J$. We do this by taking the partial derivatives with respect to $x_0$ and $\\eta_0$ and setting them to zero.\n\nThe partial derivative with respect to $x_0$ is:\n$$\n\\frac{\\partial J}{\\partial x_0} = \\frac{x_0 - x_b}{B} + \\frac{1}{R} (y_1 - H M x_0 - H \\eta_0) (-H M) = 0\n$$\n$$\n\\frac{x_0 - x_b}{B} = \\frac{H M}{R} (y_1 - H M x_0 - H \\eta_0) \\quad (*)$$\n\nThe partial derivative with respect to $\\eta_0$ is:\n$$\n\\frac{\\partial J}{\\partial \\eta_0} = \\frac{\\eta_0}{Q} + \\frac{1}{R} (y_1 - H M x_0 - H \\eta_0) (-H) = 0\n$$\n$$\n\\frac{\\eta_0}{Q} = \\frac{H}{R} (y_1 - H M x_0 - H \\eta_0) \\quad (**)\n$$\nFrom equation $(**)$, we can isolate the term in the parenthesis:\n$$\ny_1 - H M x_0 - H \\eta_0 = \\frac{R}{H Q} \\eta_0\n$$\nNow, substitute this result back into equation $(*)$:\n$$\n\\frac{x_0 - x_b}{B} = \\frac{H M}{R} \\left( \\frac{R}{H Q} \\eta_0 \\right)\n$$\n$$\n\\frac{x_0 - x_b}{B} = \\frac{M}{Q} \\eta_0\n$$\nThis gives us a relationship between the optimal corrections for $x_0$ and $\\eta_0$. We can express the optimal $x_0$ in terms of the optimal $\\eta_0$:\n$$\nx_0 = x_b + \\frac{M B}{Q} \\eta_0\n$$\nFinally, we substitute this expression for $x_0$ back into equation $(**)$ to solve for $\\eta_0$.\n$$\n\\frac{\\eta_0}{Q} = \\frac{H}{R} \\left( y_1 - H M \\left( x_b + \\frac{M B}{Q} \\eta_0 \\right) - H \\eta_0 \\right)\n$$\n$$\n\\frac{\\eta_0}{Q} = \\frac{H}{R} \\left( (y_1 - H M x_b) - \\frac{H M^2 B}{Q} \\eta_0 - H \\eta_0 \\right)\n$$\nNow we gather all terms involving $\\eta_0$ on one side:\n$$\n\\frac{\\eta_0}{Q} + \\frac{H^2 M^2 B}{R Q} \\eta_0 + \\frac{H^2}{R} \\eta_0 = \\frac{H}{R} (y_1 - H M x_b)\n$$\nFactor out $\\eta_0$:\n$$\n\\eta_0 \\left( \\frac{1}{Q} + \\frac{H^2 M^2 B}{R Q} + \\frac{H^2}{R} \\right) = \\frac{H}{R} (y_1 - H M x_b)\n$$\nTo simplify the term in the parenthesis, find a common denominator, which is $RQ$:\n$$\n\\eta_0 \\left( \\frac{R + H^2 M^2 B + H^2 Q}{R Q} \\right) = \\frac{H}{R} (y_1 - H M x_b)\n$$\nNow, we solve for $\\eta_0$:\n$$\n\\eta_0 = \\frac{R Q}{R + H^2 M^2 B + H^2 Q} \\cdot \\frac{H}{R} (y_1 - H M x_b)\n$$\n$$\n\\eta_0 = \\frac{Q H (y_1 - H M x_b)}{H^2 M^2 B + H^2 Q + R}\n$$\nThis expression is the posterior mean of the model error, $\\mathbb{E}[\\eta_0 | y_1]$.\n\n**Interpretation**\nThe derived expression for the posterior mean of the model error, $\\hat{\\eta}_0 = \\mathbb{E}[\\eta_0 | y_1]$, reveals how the assimilation system attributes discrepancies between the model forecast and observations. The term $(y_1 - H M x_b)$ is the innovation, or observation-minus-background residual, which represents the new information provided by the observation $y_1$ relative to the forecast starting from the background mean $x_b$.\n\nThe posterior mean $\\hat{\\eta}_0$ is proportional to this innovation. The coefficient of proportionality, or gain, is $\\frac{Q H}{H^2 M^2 B + H^2 Q + R}$. This gain reflects a balance between the different sources of uncertainty in the system:\n1. The numerator, $Q H$, shows that the magnitude of the estimated model error correction is directly proportional to the prior model error variance, $Q$. A larger $Q$ signifies less confidence in the model's perfection, making the system more willing to attribute the innovation to model error.\n2. The denominator, $H^2 M^2 B + H^2 Q + R$, is the total variance of the innovation. It is the sum of the background error variance propagated to observation space ($H^2 M^2 B$), the model error variance in observation space ($H^2 Q$), and the observation error variance ($R$). The estimated model error is thus scaled inversely by the total uncertainty.\n\n**Limiting Cases:**\n- **$Q \\to 0$ (Strong-Constraint Limit):** As the prior variance of the model error approaches zero, we are enforcing a belief that the model is perfect. In this limit, the numerator $Q H \\to 0$, causing $\\hat{\\eta}_0 \\to 0$. The system attributes none of the innovation to model error. Any discrepancy must be resolved by adjusting the initial condition $x_0$ alone. This is the definition of **strong-constraint 4D-Var**.\n\n- **$Q \\to \\infty$ (Weak-Constraint Behavior):** As the prior variance of the model error becomes infinite, we are expressing no confidence in the model dynamics. To analyze this limit, we can divide the numerator and denominator of the gain by $Q$:\n$$\n\\hat{\\eta}_0 = \\frac{H}{\\frac{R}{Q} + \\frac{H^2 M^2 B}{Q} + H^2} (y_1 - H M x_b)\n$$\nAs $Q \\to \\infty$, the terms $\\frac{R}{Q}$ and $\\frac{H^2 M^2 B}{Q}$ go to zero, and the expression simplifies to:\n$$\n\\hat{\\eta}_0 \\to \\frac{H}{H^2} (y_1 - H M x_b) = \\frac{1}{H} (y_1 - H M x_b)\n$$\nThis implies $H \\hat{\\eta}_0 \\to y_1 - H M x_b$. Since the model is untrusted, the analysis essentially decouples the state at time $t_1$ from the initial state $x_0$. The best estimate for the total model contribution $H(M x_0 + \\eta_0)$ is simply the observation $y_1$ (ignoring noise), so the model error term $H \\eta_0$ is adjusted to account for the entire innovation relative to the forecast from the background, $H M x_b$. This demonstrates the essence of the **weak-constraint** approach, where the model equations are not strictly enforced.",
            "answer": "$$\\boxed{\\frac{Q H (y_1 - H M x_b)}{H^2 M^2 B + H^2 Q + R}}$$"
        },
        {
            "introduction": "In operational settings, the control space of 4D-Var is high-dimensional, making direct inversion of the normal matrix computationally infeasible. This practice shifts our focus to the numerical methods used to solve these large linear systems . You will construct the Gauss-Newton normal matrix for both strong- and weak-constraint scenarios and employ the conjugate gradient algorithm to find the solution, gaining insight into the iterative techniques that underpin modern data assimilation.",
            "id": "4112097",
            "problem": "Consider Four-Dimensional Variational data assimilation (4D-Var) in a linear setting relevant to numerical weather prediction and climate modeling. The state vector is denoted by $x \\in \\mathbb{R}^n$, and the time index is $k \\in \\{0,1,\\dots,K\\}$. The discrete-time linear model is defined by $x_{k+1} = \\mathbf{M} x_k$ for the strong-constraint case and by $x_{k+1} = \\mathbf{M} x_k + w_k$ for the weak-constraint case, where $w_k$ denotes the model error at time $k$. Observations at times $k \\in \\{1,\\dots,K\\}$ are given by $y_k = \\mathbf{H}_k x_k^{\\text{true}}$ with known observation operators $\\mathbf{H}_k$. The background state is $x_b$ and background trajectory is obtained by propagating the background initial condition $x_0^{b}$ with zero model error. Background, observation, and model error covariances are denoted by $\\mathbf{B}$, $\\mathbf{R}_k$, and $\\mathbf{Q}_k$, respectively, and are symmetric positive definite.\n\nThe incremental 4D-Var cost functions to be minimized are defined as follows:\n- Strong-constraint 4D-Var cost function: a sum of a background term and observation misfit terms, under the assumption $w_k = 0$.\n- Weak-constraint 4D-Var cost function: a sum of a background term, observation misfit terms, and model error penalty terms.\n\nStarting only from these definitions and the linear model and observation operators, you must:\n- Derive the Gauss–Newton normal equations in control space for the strong-constraint and weak-constraint formulations.\n- Assemble the corresponding Gauss–Newton normal matrix and right-hand side using the provided parameters.\n- Solve the resulting symmetric positive definite linear systems using conjugate gradients, tracking the residual norms at each iteration, where the residual at iteration $i$ is $r_i = b - \\mathbf{A} x_i$ for normal matrix $\\mathbf{A}$ and right-hand side $b$.\n- Verify that the residual norms are non-increasing at each iteration to within a relative tolerance of $\\epsilon = 10^{-12}$; that is, check $||r_{i+1}||_2 \\leq (1 + \\epsilon) ||r_i||_2$ for all iterations.\n\nUse the following toy linear system and data to construct the test suite. The state dimension is $n = 2$, the number of observation times is $K = 3$, and the model matrix is constant:\n- Model matrix: $\\mathbf{M} = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0 & 0.95 \\end{bmatrix}$.\n- True initial state: $x_0^{\\text{true}} = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n- Background initial state: $x_0^{b} = \\begin{bmatrix} 0.5 \\\\ -1.5 \\end{bmatrix}$.\n- Observations at times $k = 1,2,3$: $y_k = \\mathbf{H}_k \\, x_k^{\\text{true}}$, where $x_k^{\\text{true}}$ is obtained by propagating $x_0^{\\text{true}}$ with the model and zero model error.\n\nDefine the following four test cases that vary the observation operators and covariances to probe different conditioning regimes:\n\n- Test case $1$ (strong-constraint, well-conditioned):\n  - Observation operators for all $k$: $\\mathbf{H}_k = \\mathbf{I}_2$.\n  - Background covariance: $\\mathbf{B} = \\operatorname{diag}([0.3^2, 0.5^2])$.\n  - Observation covariances for all $k$: $\\mathbf{R}_k = \\operatorname{diag}([0.2^2, 0.2^2])$.\n\n- Test case $2$ (strong-constraint, partial observations and ill-conditioned background):\n  - Observation operators for all $k$: $\\mathbf{H}_k = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\n  - Background covariance: $\\mathbf{B} = \\operatorname{diag}([0.01^2, 1.0^2])$.\n  - Observation covariances for all $k$: $\\mathbf{R}_k = \\operatorname{diag}([0.15^2])$.\n\n- Test case $3$ (weak-constraint, small model error penalty, full observations):\n  - Observation operators for all $k$: $\\mathbf{H}_k = \\mathbf{I}_2$.\n  - Background covariance: $\\mathbf{B} = \\operatorname{diag}([0.3^2, 0.5^2])$.\n  - Observation covariances for all $k$: $\\mathbf{R}_k = \\operatorname{diag}([0.2^2, 0.2^2])$.\n  - Model error covariances for $k = 0,1,2$: $\\mathbf{Q}_k = \\operatorname{diag}([0.05^2, 0.05^2])$.\n\n- Test case $4$ (weak-constraint, large model error penalty, partial observations):\n  - Observation operators for all $k$: $\\mathbf{H}_k = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix}$.\n  - Background covariance: $\\mathbf{B} = \\operatorname{diag}([0.3^2, 0.5^2])$.\n  - Observation covariances for all $k$: $\\mathbf{R}_k = \\operatorname{diag}([0.25^2])$.\n  - Model error covariances for $k = 0,1,2$: $\\mathbf{Q}_k = \\operatorname{diag}([0.5^2, 0.5^2])$.\n\nImplementation requirements:\n- Construct the background trajectory by propagating $x_0^{b}$ forward using the model with $w_k = 0$.\n- For strong-constraint, the control variable is the initial increment $\\delta x_0 \\in \\mathbb{R}^2$.\n- For weak-constraint, the control variable is the concatenation of $\\delta x_0 \\in \\mathbb{R}^2$ and model error increments $w_k \\in \\mathbb{R}^2$ for $k = 0,1,2$, yielding a control dimension of $m = 2 + 3 \\cdot 2 = 8$.\n- Assemble the Gauss–Newton normal matrix $\\mathbf{A}$ and right-hand side $b$ for each test case from first principles.\n- Solve $\\mathbf{A} \\, \\delta = b$ using conjugate gradients, starting from the zero vector, stopping when $||r_i||_2 \\leq 10^{-12}$ or when the iteration count reaches the control space dimension.\n- Record residual norms at each iteration and verify non-increasing behavior within the relative tolerance $\\epsilon = 10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the four test cases in order. Each result must be a boolean indicating whether the residual norms were non-increasing to within the specified tolerance across the iterations for that test case. For example, output should look like $[\\text{True},\\text{False},\\text{True},\\text{True}]$.\n\nNo physical units are involved in this problem; all computations are dimensionless. Angles do not appear. Percentages are not used; no percentage-format answers are required.",
            "solution": "The problem requires the derivation and solution of the Gauss-Newton normal equations for both strong-constraint and weak-constraint four-dimensional variational data assimilation (4D-Var) for a linear system. We will first derive the general forms of the cost functions and their corresponding normal equations. Subsequently, we will assemble and solve these equations for the provided test cases using the conjugate gradient method, and finally, verify a property of the residual norms.\n\nLet the state vector be $x \\in \\mathbb{R}^n$, the initial state increment be $\\delta x_0 = x_0 - x_0^b$, and the model error at time $k$ be $w_k$. The discrete-time linear model is $x_{k+1} = \\mathbf{M} x_k + w_k$. Let $\\mathbf{M}_{k,j}$ be the propagator matrix from time $j$ to $k$. For a time-invariant model matrix $\\mathbf{M}$, this is $\\mathbf{M}_{k,j} = \\mathbf{M}^{k-j}$. The state increment $\\delta x_k = x_k - x_k^b$ evolves according to $\\delta x_{k+1} = \\mathbf{M} \\delta x_k + w_k$ (where $w_k=0$ for strong constraint). The innovation vector at time $k$ is $d_k = y_k - \\mathbf{H}_k x_k^b$, where $y_k$ are the observations and $x_k^b$ is the background state at time $k$. The background, observation, and model error covariance matrices are $\\mathbf{B}$, $\\mathbf{R}_k$, and $\\mathbf{Q}_k$, respectively.\n\n**Strong-Constraint 4D-Var**\n\nIn the strong-constraint formulation, the model is assumed to be perfect, so $w_k = 0$ for all $k$. The control variable is the initial state increment $\\delta x_0 \\in \\mathbb{R}^n$. The state increment at any time $k$ is linearly related to the initial increment: $\\delta x_k = \\mathbf{M}^k \\delta x_0$. The cost function $J(\\delta x_0)$ is the sum of the background and observation penalty terms:\n$$\nJ(\\delta x_0) = \\frac{1}{2} \\delta x_0^T \\mathbf{B}^{-1} \\delta x_0 + \\frac{1}{2} \\sum_{k=1}^{K} (y_k - \\mathbf{H}_k x_k)^T \\mathbf{R}_k^{-1} (y_k - \\mathbf{H}_k x_k)\n$$\nSubstituting $x_k = x_k^b + \\delta x_k = x_k^b + \\mathbf{M}^k \\delta x_0$ and using the definition of $d_k$:\n$$\nJ(\\delta x_0) = \\frac{1}{2} \\delta x_0^T \\mathbf{B}^{-1} \\delta x_0 + \\frac{1}{2} \\sum_{k=1}^{K} (d_k - \\mathbf{H}_k \\mathbf{M}^k \\delta x_0)^T \\mathbf{R}_k^{-1} (d_k - \\mathbf{H}_k \\mathbf{M}^k \\delta x_0)\n$$\nThis is a quadratic cost function in $\\delta x_0$. To find the minimum, we compute the gradient $\\nabla_{\\delta x_0} J$ and set it to zero.\n$$\n\\nabla_{\\delta x_0} J = \\mathbf{B}^{-1} \\delta x_0 - \\sum_{k=1}^{K} (\\mathbf{H}_k \\mathbf{M}^k)^T \\mathbf{R}_k^{-1} (d_k - \\mathbf{H}_k \\mathbf{M}^k \\delta x_0) = 0\n$$\nRearranging terms gives the Gauss-Newton normal equations $\\mathbf{A} \\delta x_0 = b$:\n$$\n\\left( \\mathbf{B}^{-1} + \\sum_{k=1}^{K} (\\mathbf{M}^k)^T \\mathbf{H}_k^T \\mathbf{R}_k^{-1} \\mathbf{H}_k \\mathbf{M}^k \\right) \\delta x_0 = \\sum_{k=1}^{K} (\\mathbf{M}^k)^T \\mathbf{H}_k^T \\mathbf{R}_k^{-1} d_k\n$$\nThe normal matrix is $\\mathbf{A} = \\mathbf{B}^{-1} + \\sum_{k=1}^{K} (\\mathbf{M}^k)^T \\mathbf{H}_k^T \\mathbf{R}_k^{-1} \\mathbf{H}_k \\mathbf{M}^k$ and the right-hand side is $b = \\sum_{k=1}^{K} (\\mathbf{M}^k)^T \\mathbf{H}_k^T \\mathbf{R}_k^{-1} d_k$.\n\n**Weak-Constraint 4D-Var**\n\nIn the weak-constraint formulation, model errors $w_k$ are included as control variables. The control vector is $\\delta = [\\delta x_0^T, w_0^T, \\dots, w_{K-1}^T]^T \\in \\mathbb{R}^{n(K+1)}$. The state increment at time $k$ is given by the recursive formula $\\delta x_j = \\mathbf{M} \\delta x_{j-1} + w_{j-1}$, which unrolls to:\n$$\n\\delta x_k = \\mathbf{M}^k \\delta x_0 + \\sum_{j=0}^{k-1} \\mathbf{M}^{k-1-j} w_j\n$$\nThe cost function $J(\\delta)$ includes a penalty term for the model errors:\n$$\nJ(\\delta) = \\frac{1}{2} \\delta x_0^T \\mathbf{B}^{-1} \\delta x_0 + \\frac{1}{2} \\sum_{k=0}^{K-1} w_k^T \\mathbf{Q}_k^{-1} w_k + \\frac{1}{2} \\sum_{k=1}^{K} (d_k - \\mathbf{H}_k \\delta x_k)^T \\mathbf{R}_k^{-1} (d_k - \\mathbf{H}_k \\delta x_k)\n$$\nThis can be written compactly. Let $\\mathcal{B} = \\text{diag}(\\mathbf{B}, \\mathbf{Q}_0, \\dots, \\mathbf{Q}_{K-1})$ be the block-diagonal covariance matrix for the control vector $\\delta$. Let a linear operator $\\mathcal{H}$ map the control vector $\\delta$ to a stacked vector of observation increments, such that the $k$-th block of $\\mathcal{H}\\delta$ is $\\mathbf{H}_k \\delta x_k$. The cost function is:\n$$\nJ(\\delta) = \\frac{1}{2} \\delta^T \\mathcal{B}^{-1} \\delta + \\frac{1}{2} (D - \\mathcal{H}\\delta)^T \\mathcal{R}^{-1} (D - \\mathcal{H}\\delta)\n$$\nwhere $D = [d_1^T, \\dots, d_K^T]^T$ and $\\mathcal{R} = \\text{diag}(\\mathbf{R}_1, \\dots, \\mathbf{R}_K)$. The gradient is $\\nabla_{\\delta} J = \\mathcal{B}^{-1} \\delta - \\mathcal{H}^T \\mathcal{R}^{-1}(D - \\mathcal{H}\\delta)$. Setting the gradient to zero gives the normal equations $\\mathbf{A}\\delta = b$:\n$$\n\\mathbf{A} = \\mathcal{B}^{-1} + \\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H}\n$$\n$$\nb = \\mathcal{H}^T \\mathcal{R}^{-1} D\n$$\nThe operator $\\mathcal{H}$ can be represented as a block matrix whose entries relate the outputs $\\mathbf{H}_k\\delta x_k$ to the inputs $\\delta x_0, w_0, \\dots, w_{K-1}$. For the given problem with $K=3$, the operator $\\mathcal{H}$ maps from $\\mathbb{R}^8$ to the observation space and has the block structure:\n$$\n\\mathcal{H} = \\begin{bmatrix}\n\\mathbf{H}_1 \\mathbf{M} & \\mathbf{H}_1 \\mathbf{I} & \\mathbf{0} & \\mathbf{0} \\\\\n\\mathbf{H}_2 \\mathbf{M}^2 & \\mathbf{H}_2 \\mathbf{M} & \\mathbf{H}_2 \\mathbf{I} & \\mathbf{0} \\\\\n\\mathbf{H}_3 \\mathbf{M}^3 & \\mathbf{H}_3 \\mathbf{M}^2 & \\mathbf{H}_3 \\mathbf{M} & \\mathbf{H}_3 \\mathbf{I}\n\\end{bmatrix}\n$$\n\n**Solution and Verification**\n\nFor each test case, we first compute the true and background trajectories by propagating the initial states $x_0^{\\text{true}}$ and $x_0^b$ with the model $\\mathbf{M}$. Then, we generate the observations $y_k=\\mathbf{H}_k x_k^{\\text{true}}$ and the innovations $d_k=y_k - \\mathbf{H}_k x_k^b$. We assemble the normal matrix $\\mathbf{A}$ and the right-hand side vector $b$ according to the appropriate formulation (strong or weak constraint) and the specified parameters. The resulting linear system $\\mathbf{A}\\delta = b$ is symmetric and positive definite.\n\nWe solve this system using the conjugate gradient (CG) algorithm, starting with an initial guess $\\delta_0 = 0$. The iterations proceed as:\n$r_0 = b$, $p_0 = r_0$. For $i=0, 1, 2, \\dots$:\n$$\n\\alpha_i = \\frac{r_i^T r_i}{p_i^T \\mathbf{A} p_i}\n$$\n$$\n\\delta_{i+1} = \\delta_i + \\alpha_i p_i\n$$\n$$\nr_{i+1} = r_i - \\alpha_i \\mathbf{A} p_i\n$$\n$$\n\\beta_i = \\frac{r_{i+1}^T r_{i+1}}{r_i^T r_i}\n$$\n$$\np_{i+1} = r_{i+1} + \\beta_i p_i\n$$\nThe algorithm terminates when the Euclidean norm of the residual, $||r_i||_2$, falls below a tolerance of $10^{-12}$, or after a number of iterations equal to the dimension of the control space. The norm of the residual $||r_i||_2$ is recorded at each iteration. Finally, we verify that the residual norms are non-increasing up to a relative tolerance $\\epsilon=10^{-12}$ by checking if $||r_{i+1}||_2 \\leq (1 + \\epsilon) ||r_i||_2$ holds for all completed iterations. The boolean result of this check is reported for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the 4D-Var test cases and produce the final output.\n    \"\"\"\n    #\n    # --- Global Problem Parameters ---\n    #\n    n = 2  # State dimension\n    K = 3  # Number of observation times\n    M = np.array([[0.9, 0.1], [0, 0.95]])  # Model matrix\n    x0_true = np.array([1.0, -1.0])  # True initial state\n    x0_b = np.array([0.5, -1.5])  # Background initial state\n    cg_tol = 1e-12  # CG solver tolerance\n    check_epsilon = 1e-12  # Residual norm check relative tolerance\n\n    # --- Test Case Definitions ---\n    #\n    test_cases = [\n        {\n            \"type\": \"strong\",\n            \"Hk\": np.identity(n),\n            \"B_diag\": np.array([0.3**2, 0.5**2]),\n            \"Rk_diag\": np.array([0.2**2, 0.2**2]),\n        },\n        {\n            \"type\": \"strong\",\n            \"Hk\": np.array([[1.0, 0.0]]),\n            \"B_diag\": np.array([0.01**2, 1.0**2]),\n            \"Rk_diag\": np.array([0.15**2]),\n        },\n        {\n            \"type\": \"weak\",\n            \"Hk\": np.identity(n),\n            \"B_diag\": np.array([0.3**2, 0.5**2]),\n            \"Rk_diag\": np.array([0.2**2, 0.2**2]),\n            \"Qk_diag\": np.array([0.05**2, 0.05**2]),\n        },\n        {\n            \"type\": \"weak\",\n            \"Hk\": np.array([[0.0, 1.0]]),\n            \"B_diag\": np.array([0.3**2, 0.5**2]),\n            \"Rk_diag\": np.array([0.25**2]),\n            \"Qk_diag\": np.array([0.5**2, 0.5**2]),\n        },\n    ]\n\n    #\n    # --- Helper Functions ---\n    #\n    def run_cg(A, b, max_iter, tol):\n        \"\"\"\n        Solves Ax = b using the conjugate gradient method.\n        Starts with x_0 = 0.\n        \"\"\"\n        x = np.zeros_like(b)\n        r = b - A @ x\n        p = r.copy()\n        rs_old_sq = r @ r\n        residual_norms = [np.sqrt(rs_old_sq)]\n\n        if residual_norms[0] < tol:\n            return x, residual_norms\n            \n        for i in range(max_iter):\n            Ap = A @ p\n            alpha = rs_old_sq / (p @ Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new_sq = r @ r\n            residual_norms.append(np.sqrt(rs_new_sq))\n\n            if residual_norms[-1] < tol:\n                break\n            \n            p = r + (rs_new_sq / rs_old_sq) * p\n            rs_old_sq = rs_new_sq\n\n        return x, residual_norms\n\n    def check_residual_norms(norms, rel_tol):\n        \"\"\"\n        Checks if ||r_{i+1}|| <= (1 + eps) * ||r_i|| for all i.\n        \"\"\"\n        for i in range(len(norms) - 1):\n            if norms[i+1] > (1.0 + rel_tol) * norms[i]:\n                return False\n        return True\n\n    #\n    # --- Trajectory Generation ---\n    #\n    true_traj = [x0_true]\n    bkg_traj = [x0_b]\n    M_powers = [np.identity(n)]\n    current_M_power = np.identity(n)\n\n    for k in range(K):\n        true_traj.append(M @ true_traj[-1])\n        bkg_traj.append(M @ bkg_traj[-1])\n        current_M_power = M @ current_M_power\n        M_powers.append(current_M_power)\n\n    #\n    # --- Main Processing Loop ---\n    #\n    results = []\n    \n    for case in test_cases:\n        # Generate observations and innovations\n        # Obs are at k=1, 2, 3\n        innovations = []\n        for k in range(1, K + 1):\n            yk = case[\"Hk\"] @ true_traj[k]\n            Hk_xk_b = case[\"Hk\"] @ bkg_traj[k]\n            innovations.append(yk - Hk_xk_b)\n\n        # Assemble and solve system\n        if case[\"type\"] == \"strong\":\n            control_dim = n\n            \n            B_inv = np.diag(1.0 / case[\"B_diag\"])\n            Rk_inv = np.diag(1.0 / case[\"Rk_diag\"])\n            \n            # Assemble Hessian A\n            A = B_inv.copy()\n            for k in range(1, K + 1):\n                Mk = M_powers[k]\n                Hk = case[\"Hk\"]\n                A += Mk.T @ Hk.T @ Rk_inv @ Hk @ Mk\n\n            # Assemble right-hand side b\n            b = np.zeros(n)\n            for k in range(1, K + 1):\n                Mk = M_powers[k]\n                Hk = case[\"Hk\"]\n                dk = innovations[k - 1]\n                b += Mk.T @ Hk.T @ Rk_inv @ dk\n\n        elif case[\"type\"] == \"weak\":\n            control_dim = n * (K + 1)\n            \n            # Assemble block-diagonal inverse covariance matrix for control vector\n            B_inv = np.diag(1.0 / case[\"B_diag\"])\n            Qk_inv = np.diag(1.0 / case[\"Qk_diag\"])\n            B_cal_inv_blocks = [B_inv] + [Qk_inv] * K\n            B_cal_inv = np.block([\n                [B_cal_inv_blocks[i] if i == j else np.zeros((n,n)) \n                 for j in range(K+1)] \n                for i in range(K+1)\n            ])\n\n            # Assemble block-diagonal inverse observation error covariance matrix\n            Rk_inv = np.diag(1.0 / case[\"Rk_diag\"])\n            obs_dim_k = Rk_inv.shape[0]\n            R_cal_inv_blocks = [Rk_inv] * K\n            R_cal_inv = np.block([\n                [R_cal_inv_blocks[i] if i == j else np.zeros((obs_dim_k, obs_dim_k))\n                 for j in range(K)]\n                for i in range(K)\n            ])\n\n            # Assemble linearized observation operator H_cal\n            Hk = case[\"Hk\"]\n            H_cal_rows = []\n            for k in range(1, K + 1): # for obs at t=1, 2, 3\n                row = []\n                # Term for dx0\n                row.append(Hk @ M_powers[k])\n                # Terms for w_j, j = 0,...,k-1\n                for j in range(k):\n                    row.append(Hk @ M_powers[k-1-j])\n                # Zero padding for w_j, j = k,...,K-1\n                row.extend([np.zeros((obs_dim_k, n))] * (K-k))\n                H_cal_rows.append(row)\n            H_cal = np.block(H_cal_rows)\n            \n            # Assemble innovations vector D\n            D = np.concatenate(innovations)\n            \n            # Assemble Hessian A and RHS b\n            A = B_cal_inv + H_cal.T @ R_cal_inv @ H_cal\n            b = H_cal.T @ R_cal_inv @ D\n\n        # Solve and check residuals\n        _, residual_norms = run_cg(A, b, max_iter=control_dim, tol=cg_tol)\n        is_non_increasing = check_residual_norms(residual_norms, check_epsilon)\n        results.append(is_non_increasing)\n\n    # Final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}