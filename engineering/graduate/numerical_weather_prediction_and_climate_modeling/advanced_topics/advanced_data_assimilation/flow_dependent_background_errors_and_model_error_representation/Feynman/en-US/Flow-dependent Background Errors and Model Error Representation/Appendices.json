{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a simplified scalar model that isolates the core interplay between dynamic error growth and model error. This exercise  asks you to derive the stationary background error variance for a first-order autoregressive process. By solving this, you will develop a clear, quantitative understanding of how forecast error variance reaches an equilibrium as a balance between the flow's stability, represented by parameter $a$, and the strength of the injected model error, represented by parameter $q$.",
            "id": "4043311",
            "problem": "Consider a scalar linearized forecast error evolution along a flow in a numerical weather prediction system, modeled as the discrete-time process $e_{k+1}=a\\,e_k+\\eta_k$, where $|a|<1$ summarizes the local, one-step linear sensitivity of the flow (e.g., the dominant singular value of the tangent linear propagator over one assimilation window), and $\\eta_k$ is a zero-mean, serially independent Gaussian model error with distribution $\\eta_k \\sim \\mathcal{N}(0,q)$ that is independent of $e_k$ for each $k$. Let the background (forecast) error variance at time $k$ be $P_k=\\mathbb{E}[e_k^2]$. Assume that the system admits a stationary distribution for the error, so that $P_k \\to P^\\ast$ as $k \\to \\infty$.\n\nUsing only the stated error dynamics, the definitions of expectation and variance, and the independence and moment assumptions given above, derive from first principles a closed-form expression for the stationary background error variance $P^\\ast$ as a function of $a$ and $q$. Then, using this expression, interpret the parameter $q$ as a measure of model error strength by explaining how $P^\\ast$ scales with $q$ and how it depends on the flow sensitivity $a$ as $a \\to 0$ and as $a \\to 1^{-}$. Report only the analytic expression for $P^\\ast$ in your final answer. No numerical evaluation is required, and no units are needed.",
            "solution": "The problem requires the derivation of the stationary background error variance $P^\\ast$ for a scalar forecast error evolution model.\n\nThe model for the forecast error $e_k$ at discrete time step $k$ is given by the linear stochastic process:\n$$e_{k+1} = a\\,e_k + \\eta_k$$\nHere, $a$ is a scalar parameter representing the flow sensitivity, with $|a|<1$. The term $\\eta_k$ represents the model error, which is a zero-mean, serially independent Gaussian random variable with distribution $\\eta_k \\sim \\mathcal{N}(0,q)$. This implies that the expectation of $\\eta_k$ is $\\mathbb{E}[\\eta_k] = 0$ and its variance is $\\text{Var}(\\eta_k) = q$. Since the mean is zero, the second moment is equal to the variance: $\\mathbb{E}[\\eta_k^2] = \\text{Var}(\\eta_k) + (\\mathbb{E}[\\eta_k])^2 = q + 0^2 = q$. The problem also states that $\\eta_k$ is independent of $e_k$ for each $k$.\n\nThe background error variance at time $k$ is defined as $P_k = \\mathbb{E}[e_k^2]$. We seek the stationary value $P^\\ast = \\lim_{k \\to \\infty} P_k$.\n\nWe begin by establishing a recurrence relation for the variance $P_k$. By definition, the variance at the next time step, $k+1$, is:\n$$P_{k+1} = \\mathbb{E}[e_{k+1}^2]$$\nSubstituting the evolution equation for $e_{k+1}$ into this definition, we get:\n$$P_{k+1} = \\mathbb{E}[(a\\,e_k + \\eta_k)^2]$$\nExpanding the squared term inside the expectation gives:\n$$P_{k+1} = \\mathbb{E}[a^2 e_k^2 + 2a\\,e_k\\eta_k + \\eta_k^2]$$\nBy the linearity of the expectation operator, we can write this as:\n$$P_{k+1} = \\mathbb{E}[a^2 e_k^2] + \\mathbb{E}[2a\\,e_k\\eta_k] + \\mathbb{E}[\\eta_k^2]$$\nSince $a$ is a constant, we can pull it and the factor of $2$ out of the expectations:\n$$P_{k+1} = a^2 \\mathbb{E}[e_k^2] + 2a\\,\\mathbb{E}[e_k\\eta_k] + \\mathbb{E}[\\eta_k^2]$$\nNow, we evaluate each term using the information given in the problem statement:\n1.  The first term contains $\\mathbb{E}[e_k^2]$, which is the definition of the variance $P_k$. So, $a^2 \\mathbb{E}[e_k^2] = a^2 P_k$.\n2.  The third term, $\\mathbb{E}[\\eta_k^2]$, is the second moment of the model error $\\eta_k$. As established, since $\\eta_k$ has mean $0$ and variance $q$, its second moment is $q$. So, $\\mathbb{E}[\\eta_k^2] = q$.\n3.  The second term is $2a\\,\\mathbb{E}[e_k\\eta_k]$. The problem states that $e_k$ and $\\eta_k$ are independent. For independent random variables, the expectation of their product is the product of their expectations: $\\mathbb{E}[e_k\\eta_k] = \\mathbb{E}[e_k]\\mathbb{E}[\\eta_k]$. Since $\\mathbb{E}[\\eta_k]=0$, this term becomes $\\mathbb{E}[e_k\\eta_k] = \\mathbb{E}[e_k] \\cdot 0 = 0$.\n\nSubstituting these results back into the equation for $P_{k+1}$, we obtain the recurrence relation for the error variance:\n$$P_{k+1} = a^2 P_k + 2a(0) + q$$\n$$P_{k+1} = a^2 P_k + q$$\nThe problem states that the system admits a stationary distribution, which means that as $k \\to \\infty$, the variance converges to a constant value $P^\\ast$. In this stationary state, $P_{k+1} = P_k = P^\\ast$. We can find $P^\\ast$ by substituting it into the recurrence relation:\n$$P^\\ast = a^2 P^\\ast + q$$\nNow, we solve this algebraic equation for $P^\\ast$:\n$$P^\\ast - a^2 P^\\ast = q$$\n$$P^\\ast (1 - a^2) = q$$\n$$P^\\ast = \\frac{q}{1 - a^2}$$\nThis is the closed-form expression for the stationary background error variance. The condition $|a|<1$ ensures that $a^2 < 1$, so the denominator $1 - a^2$ is positive and non-zero, guaranteeing a well-defined, positive variance $P^\\ast$ (assuming $q > 0$).\n\nThe problem further asks for an interpretation of this result.\nFirst, the scaling of $P^\\ast$ with $q$: The expression shows that $P^\\ast$ is directly proportional to $q$. This means the stationary background error variance scales linearly with the model error variance. This is intuitive: a larger injection of random error at each step (larger $q$) leads to a correspondingly larger equilibrium error variance in the system. Thus, $q$ serves as a direct measure of the model error strength.\n\nSecond, the dependence on the flow sensitivity $a$:\n-   As $a \\to 0$, we examine the limit:\n    $$\\lim_{a \\to 0} P^\\ast = \\lim_{a \\to 0} \\frac{q}{1 - a^2} = \\frac{q}{1 - 0} = q$$\n    When $a=0$, the system has no memory of past errors ($e_{k+1} = \\eta_k$). The forecast error at any time is simply the model error injected at the previous step. Consequently, the forecast error variance is identical to the model error variance, $P^\\ast = q$.\n-   As $a \\to 1^{-}$ (approaching $1$ from below, respecting the constraint $|a|<1$):\n    $$\\lim_{a \\to 1^{-}} P^\\ast = \\lim_{a \\to 1^{-}} \\frac{q}{1 - a^2} = +\\infty$$\n    As $a$ approaches $1$, the system's memory becomes stronger. Errors are propagated from one step to the next with very little damping. The system acts as a near-perfect accumulator for the random shocks $\\eta_k$. The persistent accumulation of error causes the variance to grow without bound, hence the stationary variance diverges to infinity. This signifies a flow regime of high sensitivity, where small, persistent model errors can lead to explosive growth in forecast uncertainty. The existence of a finite stationary variance relies critically on the dissipative nature of the dynamics, guaranteed by $|a|<1$.",
            "answer": "$$\\boxed{\\frac{q}{1 - a^2}}$$"
        },
        {
            "introduction": "Having explored error evolution in a single variable, we now advance to a multivariate system to see how error correlations create value. This practice  demonstrates one of the most powerful concepts in data assimilation: the ability of an observation of one variable to correct the estimate of a different, unobserved variable. By deriving the change in variance for a wind component that is not directly observed, you will see precisely how the off-diagonal elements of the background error covariance matrix, $B$, transfer information across the state space.",
            "id": "4043372",
            "problem": "Consider a two-dimensional wind state at a single model grid point in a midlatitude baroclinic environment, represented as $x = (u, v)^{\\top}$, where $u$ and $v$ denote the zonal and meridional wind components, respectively. The background (prior) uncertainty is flow-dependent and represented by the error covariance matrix\n$$\nB = \\begin{pmatrix}\n\\sigma_{u}^{2} & \\rho\\,\\sigma_{u}\\sigma_{v} \\\\\n\\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{v}^{2}\n\\end{pmatrix},\n$$\nwhere $\\sigma_{u}^{2}$ and $\\sigma_{v}^{2}$ are the background error variances of $u$ and $v$, and $\\rho \\in [-1, 1]$ is their background error correlation coefficient, arising from dynamically coupled flow features captured by an ensemble.\n\nAssume a single in situ observation measuring only $u$ with independent Gaussian error, so that the observation model is linear and given by $y = H x + \\epsilon$, where $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $\\epsilon \\sim \\mathcal{N}(0, R)$ with $R = r > 0$. The background distribution of $x$ is Gaussian with zero mean and covariance $B$, reflecting both initial condition uncertainty and model error representation embedded within $B$.\n\nStarting from the linear-Gaussian Bayesian data assimilation framework and the definition of analysis (posterior) covariance for the Kalman Filter (KF), derive the exact closed-form expression for the analysis error variance of $v$ after assimilating this single $u$-only observation. Your derivation must begin from first principles appropriate to linear-Gaussian inference and proceed logically to the final expression. Express your final answer symbolically in terms of $\\sigma_{u}$, $\\sigma_{v}$, $\\rho$, and $r$. No numerical evaluation is required. If you introduce any acronyms, define them upon first use. Provide only the final scalar expression for the analysis error variance of $v$.",
            "solution": "The problem requires the derivation of the analysis error variance for the meridional wind component, $v$, after assimilating an observation of the zonal wind component, $u$. This is a standard problem in Bayesian data assimilation, which can be solved using the equations of the Kalman Filter (KF). The analysis, or posterior, state is an optimal combination of the background (prior) information and the new observation. The uncertainty of this analysis state is described by the analysis error covariance matrix.\n\nLet the state vector be $x = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$. The background information is given by a probability distribution with mean $x_b$ (assumed to be $\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ as per the problem statement) and a background error covariance matrix $B$. The observation is related to the state through a linear model, $y = Hx + \\epsilon$, where $H$ is the observation operator and $\\epsilon$ is the observation error with covariance matrix $R$.\n\nThe givens are:\nThe background error covariance matrix $B$:\n$$\nB = \\begin{pmatrix}\n\\sigma_{u}^{2} & \\rho\\,\\sigma_{u}\\sigma_{v} \\\\\n\\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{v}^{2}\n\\end{pmatrix}\n$$\nThe observation operator $H$:\n$$\nH = \\begin{pmatrix} 1 & 0 \\end{pmatrix}\n$$\nThe observation error covariance matrix $R$, which is a scalar in this case:\n$$\nR = r\n$$\nThe analysis error covariance matrix, which we will denote by $A$, is given by the formula:\n$$\nA = (I - KH)B\n$$\nwhere $I$ is the identity matrix and $K$ is the Kalman gain. The Kalman gain is defined as:\n$$\nK = B H^{\\top} (H B H^{\\top} + R)^{-1}\n$$\nOur goal is to find the $(2,2)$ element of the matrix $A$, which corresponds to the analysis error variance of the state component $v$.\n\nFirst, we compute the terms required for the Kalman gain $K$.\nThe transpose of the observation operator, $H^{\\top}$, is:\n$$\nH^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nNext, we calculate the term $H B H^{\\top}$. This represents the background error variance projected into the observation space.\n$$\nH B H^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\sigma_{u}^{2} & \\rho\\,\\sigma_{u}\\sigma_{v} \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{v}^{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\nH B H^{\\top} = \\begin{pmatrix} \\sigma_{u}^{2} & \\rho\\,\\sigma_{u}\\sigma_{v} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\sigma_{u}^{2}\n$$\nThe term $(H B H^{\\top} + R)$ is the total error variance in observation space. Since both $H B H^{\\top}$ and $R$ are scalars ($\\sigma_u^2$ and $r$, respectively), their sum is also a scalar:\n$$\nH B H^{\\top} + R = \\sigma_{u}^{2} + r\n$$\nThe inverse of this scalar term is simply its reciprocal:\n$$\n(H B H^{\\top} + R)^{-1} = \\frac{1}{\\sigma_{u}^{2} + r}\n$$\nNext, we compute the term $B H^{\\top}$:\n$$\nB H^{\\top} = \\begin{pmatrix} \\sigma_{u}^{2} & \\rho\\,\\sigma_{u}\\sigma_{v} \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{v}^{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sigma_{u}^{2} \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} \\end{pmatrix}\n$$\nNow we can assemble the Kalman gain $K$:\n$$\nK = (B H^{\\top}) (H B H^{\\top} + R)^{-1} = \\begin{pmatrix} \\sigma_{u}^{2} \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} \\end{pmatrix} \\frac{1}{\\sigma_{u}^{2} + r}\n$$\nThe Kalman gain $K$ is a $2 \\times 1$ column vector.\n\nWith the Kalman gain determined, we proceed to calculate the analysis error covariance matrix $A = (I - KH)B$.\nFirst, we find the matrix product $KH$:\n$$\nKH = \\frac{1}{\\sigma_{u}^{2} + r} \\begin{pmatrix} \\sigma_{u}^{2} \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\frac{1}{\\sigma_{u}^{2} + r} \\begin{pmatrix} \\sigma_{u}^{2} & 0 \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} & 0 \\end{pmatrix}\n$$\nNext, we compute the term $(I - KH)$:\n$$\nI - KH = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\frac{1}{\\sigma_{u}^{2} + r} \\begin{pmatrix} \\sigma_{u}^{2} & 0 \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} & 0 \\end{pmatrix}\n$$\n$$\nI - KH = \\frac{1}{\\sigma_{u}^{2} + r} \\left[ \\begin{pmatrix} \\sigma_{u}^{2} + r & 0 \\\\ 0 & \\sigma_{u}^{2} + r \\end{pmatrix} - \\begin{pmatrix} \\sigma_{u}^{2} & 0 \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} & 0 \\end{pmatrix} \\right]\n$$\n$$\nI - KH = \\frac{1}{\\sigma_{u}^{2} + r} \\begin{pmatrix} r & 0 \\\\ -\\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{u}^{2} + r \\end{pmatrix}\n$$\nNow we can compute the analysis error covariance $A$ by post-multiplying $(I-KH)$ by $B$:\n$$\nA = (I - KH)B = \\frac{1}{\\sigma_{u}^{2} + r} \\begin{pmatrix} r & 0 \\\\ -\\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{u}^{2} + r \\end{pmatrix} \\begin{pmatrix} \\sigma_{u}^{2} & \\rho\\,\\sigma_{u}\\sigma_{v} \\\\ \\rho\\,\\sigma_{u}\\sigma_{v} & \\sigma_{v}^{2} \\end{pmatrix}\n$$\nThe analysis error variance of $v$ is the element $A_{22}$, the second diagonal element of matrix $A$. We can compute this element by multiplying the second row of $(I-KH)$ by the second column of $B$:\n$$\nA_{22} = \\frac{1}{\\sigma_{u}^{2} + r} \\left[ (-\\rho\\,\\sigma_{u}\\sigma_{v})(\\rho\\,\\sigma_{u}\\sigma_{v}) + (\\sigma_{u}^{2} + r)(\\sigma_{v}^{2}) \\right]\n$$\nExpanding the terms in the square brackets:\n$$\nA_{22} = \\frac{1}{\\sigma_{u}^{2} + r} \\left[ -\\rho^{2}\\sigma_{u}^{2}\\sigma_{v}^{2} + \\sigma_{u}^{2}\\sigma_{v}^{2} + r\\sigma_{v}^{2} \\right]\n$$\nWe can factor out common terms to simplify the expression. Factoring $\\sigma_{v}^{2}$ from all terms in the brackets:\n$$\nA_{22} = \\frac{\\sigma_{v}^{2}}{\\sigma_{u}^{2} + r} \\left[ -\\rho^{2}\\sigma_{u}^{2} + \\sigma_{u}^{2} + r \\right]\n$$\nFactoring $\\sigma_{u}^{2}$ inside the brackets:\n$$\nA_{22} = \\frac{\\sigma_{v}^{2}}{\\sigma_{u}^{2} + r} \\left[ \\sigma_{u}^{2}(1 - \\rho^{2}) + r \\right]\n$$\nThis can be written as the final expression:\n$$\nA_{22} = \\sigma_{v}^{2} \\frac{r + \\sigma_{u}^{2}(1 - \\rho^{2})}{r + \\sigma_{u}^{2}}\n$$\nThis expression represents the analysis error variance of the meridional wind component $v$. It shows that the posterior variance of $v$ is reduced from its prior value $\\sigma_{v}^{2}$ due to the assimilation of the observation of $u$. This reduction is mediated by the background error correlation $\\rho$. If $\\rho=0$, there is no correlation, and the analysis variance of $v$ remains unchanged, i.e., $A_{22} = \\sigma_{v}^{2}$. If the correlation is perfect ($\\rho^2 = 1$) and the observation is perfect ($r=0$), the analysis variance of $v$ becomes $0$.",
            "answer": "$$\n\\boxed{\\sigma_{v}^{2} \\frac{r + \\sigma_{u}^{2}(1 - \\rho^{2})}{r + \\sigma_{u}^{2}}}\n$$"
        },
        {
            "introduction": "Modern data assimilation systems rely on ensembles to estimate flow-dependent background error covariances, but these estimates suffer from rank deficiency due to limited ensemble size. This advanced exercise  addresses this critical operational challenge by introducing the hybrid covariance method. You will construct a hybrid matrix and compute its effective rank, providing a quantitative measure of how blending the ensemble covariance with a static, full-rank component creates a more robust and realistic representation of background error.",
            "id": "4043344",
            "problem": "Consider a hybrid background-error covariance used in data assimilation for numerical weather prediction, which combines a flow-dependent ensemble-based component with a static climatological component to represent model error and mitigate sampling deficiencies. Let the state dimension be $m=4$ and the ensemble size be $N=3$. The ensemble anomalies matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has columns that sum to zero and is given by\n$$\nX = \\begin{pmatrix}\n1 & 0 & -1 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nThe ensemble-based background covariance is approximated by\n$$\nB_{e} \\approx \\frac{1}{N-1} X X^{T},\n$$\nand the static (climatological) component is\n$$\nB_{s} = 2 I_{4},\n$$\nwhere $I_{4}$ is the $4 \\times 4$ identity matrix. Define the hybrid covariance as\n$$\nB_{h} = \\alpha B_{e} + (1-\\alpha) B_{s},\n$$\nwith $\\alpha = 0.6$. Using the spectral-entropy definition of effective rank, where the effective rank $r_{\\mathrm{e}}$ is given by\n$$\nr_{\\mathrm{e}} = \\exp\\!\\left( -\\sum_{i=1}^{m} p_{i} \\ln p_{i} \\right),\n$$\nand $p_{i} = \\lambda_{i} / \\sum_{j=1}^{m} \\lambda_{j}$ are the normalized eigenvalues of $B_{h}$, compute $r_{\\mathrm{e}}$ for $B_{h}$. Round your final answer to four significant figures and report it as a dimensionless quantity. Then, briefly discuss why and how the inclusion of the static component alleviates rank deficiency in the flow-dependent ensemble covariance.",
            "solution": "The solution proceeds in several steps:\n1.  Compute the ensemble-based background-error covariance matrix, $B_e$.\n2.  Compute the hybrid background-error covariance matrix, $B_h$.\n3.  Determine the eigenvalues of $B_h$.\n4.  Calculate the normalized eigenvalues, $p_i$.\n5.  Compute the effective rank, $r_e$, using the spectral-entropy definition.\n6.  Discuss the role of the static covariance component.\n\nStep 1: Compute $B_e$\nThe ensemble-based covariance $B_e$ is given by $B_{e} = \\frac{1}{N-1} X X^{T}$. With $N=3$, the pre-factor is $\\frac{1}{2}$.\nThe ensemble anomalies matrix is $X = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$. Its transpose is $X^T = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -1 & -1 & 0 & 0 \\end{pmatrix}$.\nThe product $X X^T$ is:\n$$\nX X^{T} = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -1 & -1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 & 0 & 0 \\\\ 1 & 2 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\nThus, $B_e$ is:\n$$\nB_{e} = \\frac{1}{2} \\begin{pmatrix} 2 & 1 & 0 & 0 \\\\ 1 & 2 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} & 0 & 0 \\\\ \\frac{1}{2} & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\n\nStep 2: Compute $B_h$\nThe hybrid covariance $B_h$ is defined as $B_{h} = \\alpha B_{e} + (1-\\alpha) B_{s}$.\nGiven $\\alpha = 0.6$, it follows that $1-\\alpha = 0.4$. The static component is $B_s = 2I_4$.\n$$\nB_{h} = 0.6 \\begin{pmatrix} 1 & \\frac{1}{2} & 0 & 0 \\\\ \\frac{1}{2} & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} + 0.4 \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix}\n$$\n$$\nB_{h} = \\begin{pmatrix} 0.6 & 0.3 & 0 & 0 \\\\ 0.3 & 0.6 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0.8 & 0 & 0 & 0 \\\\ 0 & 0.8 & 0 & 0 \\\\ 0 & 0 & 0.8 & 0 \\\\ 0 & 0 & 0 & 0.8 \\end{pmatrix} = \\begin{pmatrix} 1.4 & 0.3 & 0 & 0 \\\\ 0.3 & 1.4 & 0 & 0 \\\\ 0 & 0 & 0.8 & 0 \\\\ 0 & 0 & 0 & 0.8 \\end{pmatrix}\n$$\n\nStep 3: Find the eigenvalues of $B_h$\nThe matrix $B_h$ is block-diagonal, so its eigenvalues are the eigenvalues of its diagonal blocks.\nThe lower-right $2 \\times 2$ block is $0.8 I_2$, which has two eigenvalues: $\\lambda_3 = 0.8$ and $\\lambda_4 = 0.8$.\nThe upper-left $2 \\times 2$ block is $A = \\begin{pmatrix} 1.4 & 0.3 \\\\ 0.3 & 1.4 \\end{pmatrix}$. The characteristic equation is $\\det(A - \\lambda I) = 0$:\n$$\n(1.4 - \\lambda)^{2} - (0.3)^{2} = 0 \\implies 1.4 - \\lambda = \\pm 0.3\n$$\nThis yields two eigenvalues:\n$\\lambda_1 = 1.4 + 0.3 = 1.7$\n$\\lambda_2 = 1.4 - 0.3 = 1.1$\nThe set of eigenvalues of $B_h$ is $\\{\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4\\} = \\{1.7, 1.1, 0.8, 0.8\\}$.\n\nStep 4: Calculate the normalized eigenvalues $p_i$\nThe sum of the eigenvalues is $\\sum_{j=1}^{4} \\lambda_j = 1.7 + 1.1 + 0.8 + 0.8 = 4.4$.\nThe normalized eigenvalues are:\n$p_1 = \\frac{\\lambda_1}{\\sum \\lambda_j} = \\frac{1.7}{4.4} = \\frac{17}{44}$\n$p_2 = \\frac{\\lambda_2}{\\sum \\lambda_j} = \\frac{1.1}{4.4} = \\frac{11}{44}$\n$p_3 = \\frac{\\lambda_3}{\\sum \\lambda_j} = \\frac{0.8}{4.4} = \\frac{8}{44}$\n$p_4 = \\frac{\\lambda_4}{\\sum \\lambda_j} = \\frac{0.8}{4.4} = \\frac{8}{44}$\n\nStep 5: Compute the effective rank $r_e$\nThe effective rank is $r_{\\mathrm{e}} = \\exp(-\\sum_{i=1}^{m} p_{i} \\ln p_{i})$. First, we calculate the entropy term $S = -\\sum p_i \\ln p_i$:\n$$\nS = -\\left( \\frac{17}{44} \\ln\\left(\\frac{17}{44}\\right) + \\frac{11}{44} \\ln\\left(\\frac{11}{44}\\right) + 2 \\times \\frac{8}{44} \\ln\\left(\\frac{8}{44}\\right) \\right)\n$$\nNumerically evaluating the terms:\n$p_1 \\ln p_1 \\approx (0.38636) \\times (-0.95098) \\approx -0.36743$\n$p_2 \\ln p_2 = 0.25 \\ln(0.25) \\approx 0.25 \\times (-1.38629) \\approx -0.34657$\n$p_3 \\ln p_3 + p_4 \\ln p_4 = 2 \\times \\frac{8}{44} \\ln(\\frac{8}{44}) \\approx 2 \\times (0.18182) \\times (-1.70475) \\approx -0.61991$\nSumming these gives $\\sum p_i \\ln p_i \\approx -0.36743 - 0.34657 - 0.61991 \\approx -1.33391$.\nThe entropy is $S \\approx -(-1.33391) = 1.33391$.\nThe effective rank is $r_e = \\exp(S) \\approx \\exp(1.33391) \\approx 3.79585$.\nRounding to four significant figures, $r_e = 3.796$.\n\nDiscussion on alleviating rank deficiency:\nThe flow-dependent ensemble covariance $B_e$ is rank-deficient because its rank is at most the ensemble size minus one ($N-1=2$), which is less than the state dimension ($m=4$). This means $B_e$ has a null space, which is a subspace of directions where it represents zero error variance. This severely restricts data assimilation corrections to the low-rank \"ensemble subspace\".\nThe inclusion of the static component $B_s = 2I_4$, which is a full-rank and positive-definite matrix, solves this problem. The hybrid covariance $B_h = \\alpha B_e + (1-\\alpha) B_s$ is a linear combination of a positive-semidefinite matrix ($B_e$) and a positive-definite matrix ($B_s$). For $\\alpha \\in(0,1)$, the resulting matrix $B_h$ is guaranteed to be positive-definite and thus full-rank. This is because any direction that had zero variance in $B_e$ is now assigned a positive variance from $B_s$. In this case, the two zero eigenvalues of $B_e$ are replaced by positive eigenvalues in $B_h$, making $B_h$ invertible and allowing for assimilation updates in all directions of the state space. Our calculated effective rank of $r_e \\approx 3.796$, being close to the full rank of $4$, confirms that the variance is spread across all dimensions, mitigating the rank deficiency.",
            "answer": "$$\\boxed{3.796}$$"
        }
    ]
}