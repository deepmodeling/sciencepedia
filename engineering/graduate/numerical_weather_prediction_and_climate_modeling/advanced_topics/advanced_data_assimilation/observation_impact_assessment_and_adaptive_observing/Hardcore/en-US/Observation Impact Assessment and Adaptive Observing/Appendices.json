{
    "hands_on_practices": [
        {
            "introduction": "In numerical weather prediction, we use various methods to assess observation impact, including the adjoint-based Forecast Sensitivity to Observation Impact (FSOI) and empirical Observing System Experiments (OSEs). This practice explores the fundamental relationship between these two approaches within a simplified, linear model framework. By demonstrating their mathematical equivalence in this idealized setting , you will gain a deeper appreciation for the core assumptions that connect these powerful diagnostic tools and understand why they may differ in more complex, nonlinear systems.",
            "id": "4071087",
            "problem": "Consider a one-dimensional periodic domain $x \\in [0,L]$ with $L>0$ and a perfect linear advection model governed by $\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} = 0$, where $c>0$ is constant. Assume the true state $u^{\\mathrm{true}}(x,t)$ is dominated by a single spatial Fourier mode at wavenumber $k = \\frac{2\\pi}{L}$, so that the forecast and analysis can be represented in terms of a single scalar amplitude $a(t)$ multiplying a fixed spatial pattern $\\phi(x) = \\cos\\!\\left(\\frac{2\\pi x}{L}\\right)$. The advection model preserves the amplitude of this mode, so $a(t)$ is constant in time under the forecast model.\n\nSuppose a single instantaneous point observation of the state is made at location $x_0 = \\frac{L}{3}$ and time $t_0>0$. The observation model is linear, $y = h\\,a(t_0) + \\varepsilon$, where $h = \\phi(x_0)$ and $\\varepsilon$ is zero-mean Gaussian noise with variance $R>0$. The background (prior) amplitude error at the analysis time $t_0$ is zero mean with variance $\\sigma_b^2>0$, and the data assimilation uses an optimal linear update consistent with a Gaussian background and observation errors.\n\nDefine the forecast verification metric at time $T>t_0$ by the spatially integrated squared error norm,\n$$\nJ_f = \\frac{1}{2} \\int_{0}^{L} \\left(u(x,T) - u^{\\mathrm{true}}(x,T)\\right)^{2} \\,\\mathrm{d}x,\n$$\nand note that for the single-mode representation and perfect advection, $u(x,T) - u^{\\mathrm{true}}(x,T) = e_a\\,\\phi(x - cT)$ with $e_a$ the scalar amplitude error at $t_0$, implying $J_f = \\frac{1}{2}\\,\\alpha\\, e_a^{2}$ where $\\alpha = \\int_{0}^{L} \\phi(x)^{2}\\,\\mathrm{d}x$.\n\nLet Forecast Sensitivity to Observation Impact (FSOI) be defined here as the expected reduction in the forecast error norm due to assimilating the single observation, that is the difference between the expected value of $J_f$ computed from the background variance and the expected value of $J_f$ computed from the analysis variance. Let an Observing System Experiment (OSE) skill difference be defined as the expected difference in $J_f$ between two assimilation configurations: one that includes the observation and one that excludes it.\n\nUsing the context above and starting from fundamental properties of linear advection and optimal linear Gaussian estimation, do the following:\n\n- Derive an analytical expression for the FSOI in terms of $\\sigma_b^2$, $R$, and $h$.\n- Derive the OSE-based expected skill difference in terms of the same quantities.\n- Compute the ratio of the FSOI to the OSE-based expected skill difference for the specific parameter values $L = 1000$, $x_0 = \\frac{L}{3}$, $k = \\frac{2\\pi}{L}$, $c = 10$, $\\sigma_b^2 = 4$, and $R = 1$. Use $\\phi(x) = \\cos\\!\\left(\\frac{2\\pi x}{L}\\right)$.\n\nExpress the final answer as a dimensionless decimal. No rounding instruction is necessary for this problem.",
            "solution": "The objective is to derive expressions for the Forecast Sensitivity to Observation Impact (FSOI) and the Observing System Experiment (OSE) skill difference based on the provided definitions, and then to compute their ratio. The problem is set in the context of a simplified one-dimensional linear advection model with a single Fourier mode representation.\n\nFirst, we analyze the data assimilation step, which is a scalar optimal linear estimation problem. The state is described by a single amplitude, $a$. The background estimate of this amplitude, $a_b$, has an associated error, $e_b = a_b - a_{\\mathrm{true}}$, with zero mean, $E[e_b] = 0$, and variance $\\sigma_b^2 = E[e_b^2]$.\n\nAn observation $y$ is made at time $t_0$, with the linear observation model $y = h a_{\\mathrm{true}} + \\varepsilon$. The observation operator for the amplitude is $h = \\phi(x_0)$, where $\\phi(x) = \\cos\\left(\\frac{2\\pi x}{L}\\right)$ and the observation location is $x_0 = \\frac{L}{3}$. The observation error $\\varepsilon$ is a zero-mean Gaussian random variable with variance $R$. We are given that the background and observation errors are uncorrelated, i.e., $E[e_b \\varepsilon] = 0$.\n\nThe analysis amplitude, $a_a$, is obtained by updating the background with the observation:\n$$\na_a = a_b + K (y - h a_b)\n$$\nwhere $K$ is the Kalman gain. The analysis error is $e_a = a_a - a_{\\mathrm{true}}$. Substituting the expressions for $a_a$ and $y$:\n$$\ne_a = \\left[a_b + K (h a_{\\mathrm{true}} + \\varepsilon - h a_b)\\right] - a_{\\mathrm{true}}\n$$\n$$\ne_a = (a_b - a_{\\mathrm{true}}) - K h (a_b - a_{\\mathrm{true}}) + K \\varepsilon\n$$\n$$\ne_a = (1 - K h) e_b + K \\varepsilon\n$$\nThe analysis error variance, $\\sigma_a^2 = E[e_a^2]$, is then:\n$$\n\\sigma_a^2 = E\\left[((1 - K h) e_b + K \\varepsilon)^2\\right] = E\\left[(1 - K h)^2 e_b^2 + 2 K (1 - K h) e_b \\varepsilon + K^2 \\varepsilon^2\\right]\n$$\nDue to the uncorrelation of $e_b$ and $\\varepsilon$, $E[e_b \\varepsilon] = 0$, so the expression simplifies to:\n$$\n\\sigma_a^2 = (1 - K h)^2 E[e_b^2] + K^2 E[\\varepsilon^2] = (1 - K h)^2 \\sigma_b^2 + K^2 R\n$$\nTo find the optimal gain $K$ that minimizes $\\sigma_a^2$, we differentiate with respect to $K$ and set the result to zero:\n$$\n\\frac{\\mathrm{d}\\sigma_a^2}{\\mathrm{d}K} = 2(1 - K h)(-h)\\sigma_b^2 + 2 K R = -2h\\sigma_b^2 + 2K h^2 \\sigma_b^2 + 2KR = 0\n$$\n$$\nK(h^2 \\sigma_b^2 + R) = h \\sigma_b^2\n$$\nThis gives the optimal Kalman gain:\n$$\nK = \\frac{h \\sigma_b^2}{h^2 \\sigma_b^2 + R}\n$$\nSubstituting this optimal gain back into the expression for $\\sigma_a^2$:\n$$\n\\sigma_a^2 = \\left(1 - \\frac{h^2 \\sigma_b^2}{h^2 \\sigma_b^2 + R}\\right)^2 \\sigma_b^2 + \\left(\\frac{h \\sigma_b^2}{h^2 \\sigma_b^2 + R}\\right)^2 R\n$$\n$$\n\\sigma_a^2 = \\left(\\frac{R}{h^2 \\sigma_b^2 + R}\\right)^2 \\sigma_b^2 + \\frac{h^2 (\\sigma_b^2)^2 R}{(h^2 \\sigma_b^2 + R)^2}\n$$\n$$\n\\sigma_a^2 = \\frac{R^2 \\sigma_b^2 + h^2 (\\sigma_b^2)^2 R}{(h^2 \\sigma_b^2 + R)^2} = \\frac{R \\sigma_b^2 (R + h^2 \\sigma_b^2)}{(h^2 \\sigma_b^2 + R)^2} = \\frac{R \\sigma_b^2}{h^2 \\sigma_b^2 + R}\n$$\nThis is the variance of the analysis error amplitude.\n\nNext, we relate this to the forecast verification metric $J_f$. The problem states $J_f = \\frac{1}{2} \\alpha e^2$, where $e$ is the amplitude error at the analysis time $t_0$, and $\\alpha = \\int_0^L \\phi(x)^2 \\,\\mathrm{d}x$. We compute $\\alpha$:\n$$\n\\alpha = \\int_0^L \\cos^2\\left(\\frac{2\\pi x}{L}\\right) \\,\\mathrm{d}x = \\int_0^L \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{4\\pi x}{L}\\right)\\right) \\,\\mathrm{d}x\n$$\n$$\n\\alpha = \\frac{1}{2}\\left[x + \\frac{L}{4\\pi}\\sin\\left(\\frac{4\\pi x}{L}\\right)\\right]_0^L = \\frac{1}{2}\\left(L + 0 - 0\\right) = \\frac{L}{2}\n$$\nThe expected value of $J_f$ is $E[J_f] = E[\\frac{1}{2} \\alpha e^2] = \\frac{1}{2} \\alpha E[e^2]$. For a forecast initialized from a state with error variance $\\sigma^2$, the expected metric is $E[J_f] = \\frac{1}{2} \\alpha \\sigma^2$.\n\nThe expected value of $J_f$ for a forecast initialized from the background state is therefore based on the background error variance $\\sigma_b^2$:\n$$\nE[J_f(\\text{background})] = \\frac{1}{2} \\alpha \\sigma_b^2\n$$\nThe expected value of $J_f$ for a forecast initialized from the analysis state is based on the analysis error variance $\\sigma_a^2$:\n$$\nE[J_f(\\text{analysis})] = \\frac{1}{2} \\alpha \\sigma_a^2 = \\frac{1}{2} \\alpha \\frac{R \\sigma_b^2}{h^2 \\sigma_b^2 + R}\n$$\n\nNow we derive the expression for FSOI. According to the problem's definition, FSOI is the difference between the expected value of $J_f$ computed from the background variance and that computed from the analysis variance.\n$$\n\\text{FSOI} = E[J_f(\\text{background})] - E[J_f(\\text{analysis})] = \\frac{1}{2} \\alpha \\sigma_b^2 - \\frac{1}{2} \\alpha \\sigma_a^2\n$$\n$$\n\\text{FSOI} = \\frac{1}{2} \\alpha (\\sigma_b^2 - \\sigma_a^2)\n$$\nSubstituting the expression for $\\sigma_a^2$:\n$$\n\\text{FSOI} = \\frac{1}{2} \\alpha \\left(\\sigma_b^2 - \\frac{R \\sigma_b^2}{h^2 \\sigma_b^2 + R}\\right) = \\frac{1}{2} \\alpha \\sigma_b^2 \\left(1 - \\frac{R}{h^2 \\sigma_b^2 + R}\\right)\n$$\n$$\n\\text{FSOI} = \\frac{1}{2} \\alpha \\sigma_b^2 \\left(\\frac{h^2 \\sigma_b^2 + R - R}{h^2 \\sigma_b^2 + R}\\right) = \\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}\n$$\nThis is the analytical expression for FSOI.\n\nNext, we derive the OSE-based expected skill difference. This is defined as the expected difference in $J_f$ between a configuration that excludes the observation and one that includes it. For a single realization, the error in the \"exclude\" run is $e_b$ and the error in the \"include\" run is $e_a$. The skill difference for this realization is $J_f(\\text{exclude}) - J_f(\\text{include}) = \\frac{1}{2}\\alpha e_b^2 - \\frac{1}{2}\\alpha e_a^2$. The OSE skill difference is the expectation of this quantity over all realizations:\n$$\n\\text{OSE} = E\\left[\\frac{1}{2}\\alpha e_b^2 - \\frac{1}{2}\\alpha e_a^2\\right]\n$$\nBy linearity of expectation:\n$$\n\\text{OSE} = \\frac{1}{2}\\alpha E[e_b^2] - \\frac{1}{2}\\alpha E[e_a^2] = \\frac{1}{2}\\alpha (\\sigma_b^2 - \\sigma_a^2)\n$$\nThis expression is identical to the one derived for FSOI. Therefore:\n$$\n\\text{OSE} = \\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}\n$$\nThe problem's specific definitions for FSOI and OSE skill difference render them mathematically equivalent in this idealized context.\n\nFinally, we compute the ratio of FSOI to the OSE-based skill difference. Since the analytical expressions for both quantities are identical, their ratio is necessarily $1$.\n$$\n\\frac{\\text{FSOI}}{\\text{OSE}} = \\frac{\\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}}{\\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}} = 1\n$$\nThis result is independent of the specific parameter values given in the problem ($L = 1000$, $x_0 = \\frac{L}{3}$, $\\sigma_b^2 = 4$, $R = 1$, etc.), provided that the denominators are non-zero, which is guaranteed as $h^2 \\sigma_b^2 \\ge 0$ and $R>0$. The ratio is a constant value of $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "While methods like FSOI focus on impact through the reduction of a specific error metric, we can quantify the value of an observation more fundamentally using information theory. This practice challenges you to connect the concept of Shannon differential entropy, a measure of uncertainty volume, to the Bayesian update in data assimilation . Deriving the expected entropy reduction reveals its direct link to the D-optimality criterion, providing a powerful theoretical basis for designing observation networks that maximally reduce forecast uncertainty.",
            "id": "4071099",
            "problem": "Consider a linear-Gaussian data assimilation setting representative of targeted adaptive observing in Numerical Weather Prediction (NWP). Let the state vector be $x \\in \\mathbb{R}^{n}$ with prior (background) distribution $x \\sim \\mathcal{N}(x_b, P_b)$, where $P_b \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Observations are given by the linear model $y = H x + \\varepsilon \\in \\mathbb{R}^{m}$, where $H \\in \\mathbb{R}^{m \\times n}$ is the observation operator, $\\varepsilon \\sim \\mathcal{N}(0, R)$, and $R \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite. Under these assumptions, the posterior (analysis) distribution is Gaussian with covariance $P_a \\in \\mathbb{R}^{n \\times n}$.\n\nTask 1. Starting from the definition of Shannon differential entropy for a multivariate Gaussian and the Bayesian linear-Gaussian update for covariance, derive a closed-form expression for the expected reduction in entropy of the state, defined as the difference between the prior and posterior entropies, in terms of $P_b$ and $P_a$. Then, re-express this reduction entirely in terms of $P_b$, $H$, and $R$.\n\nTask 2. Interpret the derived entropy reduction as an objective for determinant-optimal (D-optimal) experimental design. Show that maximizing the expected entropy reduction over admissible choices of $H$ is equivalent to minimizing $\\det(P_a)$, and equivalently to maximizing a determinant that depends only on $H$, $P_b$, and $R$.\n\nTask 3. For a two-dimensional state ($n = 2$) representing two leading empirical modes of lower-tropospheric temperature uncertainty over a targeted region, suppose the prior covariance is\n$$\nP_b = \\begin{pmatrix}\n4 & 1.2 \\\\\n1.2 & 3\n\\end{pmatrix},\n$$\nyou can deploy a single adaptive observation ($m = 1$) measuring the difference of the two modes with observation operator\n$$\nH = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix},\n$$\nand the instrument noise variance is\n$$\nR = \\begin{pmatrix}\n0.5\n\\end{pmatrix}.\n$$\nUsing your general expression from Task 1, compute the expected entropy reduction due to assimilating this single observation. Express the final entropy reduction in nats and round your answer to four significant figures.",
            "solution": "**Task 1: Derivation of the Expected Entropy Reduction**\n\nThe Shannon differential entropy for a multivariate random vector $z \\in \\mathbb{R}^k$ following a Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ is given by:\n$$\nS(z) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma) = \\frac{k}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\Sigma)\n$$\nThe prior state vector $x$ has a distribution $x \\sim \\mathcal{N}(x_b, P_b)$. Its entropy is:\n$$\nS_{\\text{prior}} = S(x) = \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(P_b)\n$$\nAfter assimilating the observation $y$, the posterior distribution for $x$ is also Gaussian, $x|y \\sim \\mathcal{N}(x_a, P_a)$, where $P_a$ is the posterior covariance matrix. The entropy of the posterior distribution is:\n$$\nS_{\\text{posterior}} = S(x|y) = \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(P_a)\n$$\nThe expected reduction in entropy is the difference between the prior and posterior entropies. This quantity is also the mutual information $\\mathcal{I}(x, y)$.\n$$\n\\Delta S = S_{\\text{prior}} - S_{\\text{posterior}} = \\left( \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(P_b) \\right) - \\left( \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(P_a) \\right)\n$$\n$$\n\\Delta S = \\frac{1}{2} \\ln \\det(P_b) - \\frac{1}{2} \\ln \\det(P_a)\n$$\nUsing the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$, we obtain the expression in terms of $P_b$ and $P_a$:\n$$\n\\Delta S = \\frac{1}{2} \\ln \\left( \\frac{\\det(P_b)}{\\det(P_a)} \\right)\n$$\nTo re-express this in terms of $P_b$, $H$, and $R$, we use the formula for the inverse of the posterior covariance in a linear-Gaussian setting:\n$$\nP_a^{-1} = P_b^{-1} + H^T R^{-1} H\n$$\nWe can rewrite the ratio of determinants as $\\det(P_b) \\det(P_a^{-1})$. Substituting the expression for $P_a^{-1}$:\n$$\n\\frac{\\det(P_b)}{\\det(P_a)} = \\det(P_b) \\det(P_b^{-1} + H^T R^{-1} H)\n$$\nUsing the property $\\det(A)\\det(B) = \\det(AB)$:\n$$\n\\frac{\\det(P_b)}{\\det(P_a)} = \\det(P_b (P_b^{-1} + H^T R^{-1} H)) = \\det(P_b P_b^{-1} + P_b H^T R^{-1} H) = \\det(I_n + P_b H^T R^{-1} H)\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix.\nNow, applying Sylvester's determinant identity, which states that $\\det(I_n + AB) = \\det(I_m + BA)$ for matrices $A \\in \\mathbb{R}^{n \\times m}$ and $B \\in \\mathbb{R}^{m \\times n}$, we can simplify this expression. Let $A = P_b H^T \\in \\mathbb{R}^{n \\times m}$ and $B = R^{-1} H \\in \\mathbb{R}^{m \\times n}$. Then:\n$$\n\\det(I_n + (P_b H^T)(R^{-1} H)) = \\det(I_m + (R^{-1} H)(P_b H^T)) = \\det(I_m + R^{-1} H P_b H^T)\n$$\nThis form is computationally advantageous when the number of observations $m$ is much smaller than the state dimension $n$.\nSubstituting this back into the entropy reduction formula, we get the desired expression in terms of $P_b$, $H$, and $R$:\n$$\n\\Delta S = \\frac{1}{2} \\ln(\\det(I_m + R^{-1} H P_b H^T))\n$$\n\n**Task 2: Interpretation as D-Optimal Design**\n\nThe goal of adaptive observing is to choose an observation strategy, represented by the operator $H$, to maximize the information gained. The expected entropy reduction, $\\Delta S$, is a direct measure of this information gain.\nThe optimization problem is:\n$$\n\\max_{H} \\Delta S\n$$\nUsing the first derived expression for $\\Delta S$:\n$$\n\\max_{H} \\frac{1}{2} \\ln \\left( \\frac{\\det(P_b)}{\\det(P_a)} \\right)\n$$\nThe natural logarithm is a strictly monotonic increasing function, so maximizing $\\ln(f(H))$ is equivalent to maximizing $f(H)$. The factor of $\\frac{1}{2}$ is a positive constant and does not affect the optimization. The prior covariance $P_b$ is given and does not depend on the choice of $H$. Therefore, $\\det(P_b)$ is a positive constant. The optimization problem is equivalent to:\n$$\n\\max_{H} \\frac{1}{\\det(P_a)} \\iff \\min_{H} \\det(P_a)\n$$\nThis is precisely the D-optimality criterion in experimental design. It seeks to minimize the determinant of the posterior covariance matrix, which is proportional to the squared volume of the posterior uncertainty ellipsoid.\n\nNow, using the second derived expression for $\\Delta S$:\n$$\n\\max_{H} \\frac{1}{2} \\ln(\\det(I_m + R^{-1} H P_b H^T))\n$$\nAgain, due to the monotonicity of the logarithm and the constant factor, this is equivalent to maximizing the argument of the logarithm:\n$$\n\\max_{H} \\det(I_m + R^{-1} H P_b H^T)\n$$\nThis demonstrates that maximizing the entropy reduction is equivalent to maximizing a determinant that is a function of the experimental design parameters ($H$) and the fixed prior statistics ($P_b$, $R$).\n\n**Task 3: Numerical Computation**\n\nWe are given the following values for a state of dimension $n=2$ and a single observation $m=1$:\n- Prior covariance: $P_b = \\begin{pmatrix} 4 & 1.2 \\\\ 1.2 & 3 \\end{pmatrix}$\n- Observation operator: $H = \\begin{pmatrix} 1 & -1 \\end{pmatrix}$\n- Observation error covariance: $R = \\begin{pmatrix} 0.5 \\end{pmatrix}$\n\nWe will use the formula $\\Delta S = \\frac{1}{2} \\ln(\\det(I_m + R^{-1} H P_b H^T))$.\nFirst, we compute the product $H P_b H^T$, which represents the prior variance in observation space.\n$$\nH P_b = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 & 1.2 \\\\ 1.2 & 3 \\end{pmatrix} = \\begin{pmatrix} 4 - 1.2 & 1.2 - 3 \\end{pmatrix} = \\begin{pmatrix} 2.8 & -1.8 \\end{pmatrix}\n$$\n$$\nH P_b H^T = \\begin{pmatrix} 2.8 & -1.8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (2.8)(1) + (-1.8)(-1) = 2.8 + 1.8 = 4.6\n$$\nNext, we find the inverse of the observation error covariance $R$. Since $R$ is a scalar ($1 \\times 1$ matrix), its inverse is its reciprocal:\n$$\nR^{-1} = (0.5)^{-1} = 2\n$$\nNow we compute the matrix inside the determinant. Since $m=1$, this will be a $1 \\times 1$ matrix (a scalar). $I_1 = (1)$.\n$$\nI_1 + R^{-1} H P_b H^T = 1 + (2)(4.6) = 1 + 9.2 = 10.2\n$$\nThe determinant of this scalar is the scalar value itself:\n$$\n\\det(I_1 + R^{-1} H P_b H^T) = 10.2\n$$\nFinally, we compute the expected entropy reduction:\n$$\n\\Delta S = \\frac{1}{2} \\ln(10.2)\n$$\n$$\n\\Delta S \\approx \\frac{1}{2} (2.3223877) \\approx 1.16119385 \\text{ nats}\n$$\nRounding the result to four significant figures, we get $1.161$.",
            "answer": "$$\n\\boxed{1.161}\n$$"
        },
        {
            "introduction": "Building on theoretical criteria for optimal design, such as the D-optimality related to the entropy reduction explored in , this exercise reframes adaptive observing as a practical resource allocation problem. The task is to select the most valuable measurements from a candidate set, each with a specific cost and potential information gain, without exceeding a total budget. By applying an A-optimal criterion, which minimizes the posterior error variance, you will discover how the goal of uncertainty reduction translates into a classic mixed-integer optimization known as the 0–1 knapsack problem .",
            "id": "4071080",
            "problem": "Adaptive observing in Numerical Weather Prediction (NWP) seeks to select informative measurements subject to logistical constraints to reduce forecast uncertainty. Consider a targeted design for a single nondimensional scalar forecast error mode $x$ to be reduced via Data Assimilation (DA). The prior is Gaussian with $x \\sim \\mathcal{N}(0, P_b)$, where $P_b > 0$ is known. There are $m$ candidate observations, indexed by $i \\in \\{1,\\dots,m\\}$, each measuring $y_i = h_i x + \\epsilon_i$, where $h_i \\in \\mathbb{R}$ is the known linearized observation operator coefficient and $\\epsilon_i$ is a zero-mean Gaussian measurement error with variance $r_i > 0$. Assume $\\epsilon_i$ are mutually independent across $i$. Each candidate observation $i$ carries a deployment cost $c_i > 0$, and the total budget is $B > 0$. Define binary decision variables $x_i \\in \\{0,1\\}$ indicating whether observation $i$ is selected.\n\nStarting from the linear-Gaussian DA model and the definition that the A-optimal criterion minimizes the trace of the posterior error covariance, derive a mixed-integer optimization formulation whose objective is the A-optimal trace of the posterior covariance of $x$ as a function of the selection vector $(x_1,\\dots,x_m)$, and whose constraints enforce the budget and binary decisions. Clearly state the mathematical optimization problem in terms of the decision variables, objective, and constraints.\n\nThen, for the specific numerical instance with $m=5$, prior variance $P_b = 1$, budget $B = 12$, and parameters\n- costs: $c_1 = 6$, $c_2 = 5$, $c_3 = 5$, $c_4 = 7$, $c_5 = 2$,\n- observation operator coefficients: $h_1 = 0.8$, $h_2 = 0.5$, $h_3 = 1.2$, $h_4 = 0.3$, $h_5 = 0.7$,\n- error variances: $r_1 = 0.09$, $r_2 = 0.04$, $r_3 = 0.36$, $r_4 = 0.01$, $r_5 = 0.49$,\n\ndetermine the optimal selection and compute the minimal posterior variance (which, for a scalar, equals the trace of the posterior covariance) achieved by this selection. The scalar $x$ is nondimensional; express the final answer as a nondimensional real number. Round your final answer to four significant figures.",
            "solution": "The problem is posed within the linear-Gaussian Data Assimilation (DA) framework. For a scalar state $x$ with prior $x \\sim \\mathcal{N}(0, P_b)$, and measurements $y_i = h_i x + \\epsilon_i$ with independent $\\epsilon_i \\sim \\mathcal{N}(0, r_i)$, the posterior distribution of $x$ given a selected subset of observations remains Gaussian. A core well-tested fact in linear-Gaussian inference is that information (precision) adds: the posterior precision equals the prior precision plus the sum of the information contributed by each measurement.\n\nWe formalize the selection via binary variables $x_i \\in \\{0,1\\}$. If observation $i$ is selected ($x_i = 1$), its likelihood contributes a term proportional to $h_i^2 / r_i$ to the Fisher information about $x$. To see this, we can write the negative log-posterior (up to an additive constant) given selected observations $\\{i : x_i = 1\\}$:\n\n$$\n\\frac{x^2}{2 P_b} + \\sum_{i=1}^{m} x_i \\frac{\\left(y_i - h_i x\\right)^2}{2 r_i}.\n$$\n\nExpanding the sum and collecting terms in $x$, the quadratic term in $x$ yields the posterior precision:\n\n$$\nP_a^{-1} = P_b^{-1} + \\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}.\n$$\n\nThus, the posterior variance is\n\n$$\nP_a = \\left(P_b^{-1} + \\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}\\right)^{-1}.\n$$\n\nFor a scalar state, the A-optimal criterion, defined as minimizing the trace of the posterior covariance, reduces to minimizing $P_a$ itself. Therefore, the mixed-integer optimization problem under an A-optimal objective is\n\n$$\n\\min_{x_1,\\dots,x_m} \\;\\; \\left(P_b^{-1} + \\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}\\right)^{-1}\n$$\n\nsubject to\n\n$$\n\\sum_{i=1}^{m} c_i x_i \\leq B, \\quad x_i \\in \\{0,1\\} \\quad \\text{for all } i \\in \\{1,\\dots,m\\}.\n$$\n\nBecause the function $f(s) = \\left(P_b^{-1} + s\\right)^{-1}$ is strictly decreasing in $s$ for $s \\ge 0$, minimizing $P_a$ is equivalent to maximizing the summed information contribution\n\n$$\n\\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}\n$$\n\nunder the same constraints. This reveals that, in the scalar case, the A-optimal design reduces to a 0–1 knapsack problem with item values $w_i = \\frac{h_i^2}{r_i}$ and weights $c_i$.\n\nWe now solve the provided numerical instance. Compute the information contributions $w_i = \\frac{h_i^2}{r_i}$:\n- For $i=1$: $h_1 = 0.8$, $r_1 = 0.09$, so $w_1 = \\frac{(0.8)^2}{0.09} = \\frac{0.64}{0.09} = 7.111\\overline{1}$, i.e., $w_1 = 7.\\overline{1}$.\n- For $i=2$: $h_2 = 0.5$, $r_2 = 0.04$, so $w_2 = \\frac{(0.5)^2}{0.04} = \\frac{0.25}{0.04} = 6.25$.\n- For $i=3$: $h_3 = 1.2$, $r_3 = 0.36$, so $w_3 = \\frac{(1.2)^2}{0.36} = \\frac{1.44}{0.36} = 4$.\n- For $i=4$: $h_4 = 0.3$, $r_4 = 0.01$, so $w_4 = \\frac{(0.3)^2}{0.01} = \\frac{0.09}{0.01} = 9$.\n- For $i=5$: $h_5 = 0.7$, $r_5 = 0.49$, so $w_5 = \\frac{(0.7)^2}{0.49} = \\frac{0.49}{0.49} = 1$.\n\nThe costs are $c_1 = 6$, $c_2 = 5$, $c_3 = 5$, $c_4 = 7$, $c_5 = 2$, with budget $B = 12$. We seek to maximize $\\sum_{i=1}^{5} x_i w_i$ subject to $\\sum_{i=1}^{5} c_i x_i \\le 12$ and $x_i \\in \\{0,1\\}$.\n\nWe enumerate feasible high-value combinations under the budget:\n- Pick $i=4$ and $i=2$: total cost $7 + 5 = 12$ (feasible), total value $9 + 6.25 = 15.25$.\n- Pick $i=4$ and $i=3$: total cost $7 + 5 = 12$ (feasible), total value $9 + 4 = 13$.\n- Pick $i=1$ and $i=2$: total cost $6 + 5 = 11$ (feasible), total value $7.111\\overline{1} + 6.25 = 13.361\\overline{1}$.\n- Pick $i=2$, $i=3$, and $i=5$: total cost $5 + 5 + 2 = 12$ (feasible), total value $6.25 + 4 + 1 = 11.25$.\n- Other feasible combinations, such as $i=4$ and $i=5$ (cost $9$, value $10$), $i=1$ and $i=3$ (cost $11$, value $11.111\\overline{1}$), or any addition of $i=5$ to $i=1$ and $i=2$ or $i=1$ and $i=3$ would exceed the budget ($13$ or higher).\n\nThe maximal value under the budget is therefore achieved by selecting $i=4$ and $i=2$, with summed information $s^{\\star} = 15.25$. With $P_b = 1$, the optimal posterior variance is\n\n$$\nP_a^{\\star} = \\left(P_b^{-1} + s^{\\star}\\right)^{-1} = \\left(1 + 15.25\\right)^{-1} = \\frac{1}{16.25} = 0.0615384615\\ldots\n$$\n\nRounded to four significant figures, the minimal posterior variance is $0.06154$ (nondimensional).",
            "answer": "$$\\boxed{0.06154}$$"
        }
    ]
}