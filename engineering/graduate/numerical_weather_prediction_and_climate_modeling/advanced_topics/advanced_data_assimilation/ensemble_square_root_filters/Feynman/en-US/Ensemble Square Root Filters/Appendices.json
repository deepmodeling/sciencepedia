{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the full analysis update, we must master a foundational concept: the separation of an ensemble's mean from its anomalies. This practice guides you through the derivation of the centering operator, a crucial tool that ensures the ensemble anomalies always sum to zero. Understanding this operator is key to grasping how Ensemble Square Root Filters can consistently update the mean and spread of the ensemble without interference .",
            "id": "4038090",
            "problem": "Consider an ensemble-based data assimilation step in numerical weather prediction and climate modeling using an Ensemble Square Root Filter (EnSRF). Let there be $m$ ensemble members in state space of dimension $n$, collected as columns of the forecast ensemble matrix $E^{f} \\in \\mathbb{R}^{n \\times m}$. The forecast ensemble mean is defined as $\\,\\bar{x}^{f} = \\frac{1}{m} E^{f} \\mathbf{1}\\,$, where $\\mathbf{1} \\in \\mathbb{R}^{m}$ denotes the column vector of ones. The forecast anomalies are defined as $\\,X^{f} = E^{f} - \\bar{x}^{f} \\mathbf{1}^{\\top}\\,$. The sample forecast covariance is $\\,P^{f} = \\frac{1}{m-1} X^{f} (X^{f})^{\\top}\\,$. The analysis mean increment $\\,\\delta \\bar{x}^{a} \\in \\mathbb{R}^{n}\\,$ is obtained from a valid Kalman update for the ensemble mean given the observation operator and gain, and is applied uniformly to all ensemble members, producing a temporarily mean-updated ensemble $\\,\\widetilde{E}^{a} = E^{f} + \\delta \\bar{x}^{a} \\mathbf{1}^{\\top}\\,$. Subsequently, an ensemble-space square-root transform $\\,T \\in \\mathbb{R}^{m \\times m}\\,$ is applied to anomalies to adjust the analysis spread, yielding analysis anomalies $\\,X^{a} = \\widetilde{E}^{a} T - \\big(\\frac{1}{m} \\widetilde{E}^{a} T \\mathbf{1}\\big) \\mathbf{1}^{\\top}\\,$.\n\nStarting from the fundamental definitions of ensemble mean and anomalies above, and the notion of orthogonal projection in $\\mathbb{R}^{m}$ onto the subspace orthogonal to $\\mathbf{1}$, derive a single right-multiplication operator $\\,C_{m} \\in \\mathbb{R}^{m \\times m}\\,$ which, when applied to any ensemble matrix $E \\in \\mathbb{R}^{n \\times m}$, produces centered anomalies $\\,X = E C_{m}\\,$ that satisfy $\\,X \\mathbf{1} = \\mathbf{0}\\,$. Your derivation must justify the properties required of $C_{m}$ to ensure that adding a uniform mean increment $\\,\\delta \\bar{x} \\mathbf{1}^{\\top}\\,$ to $E$ does not contaminate the anomalies, and explain why this centering is essential for maintaining $\\,X^{a} \\mathbf{1} = \\mathbf{0}\\,$ in EnSRF.\n\nProvide the final answer as the closed-form analytic expression for $\\,C_{m}\\,$ in terms of $m$ and $\\mathbf{1}$. No numerical evaluation is required. The final answer must be a single symbolic expression.",
            "solution": "The problem asks for the derivation of a right-multiplication operator $C_m \\in \\mathbb{R}^{m \\times m}$ that transforms an ensemble matrix $E \\in \\mathbb{R}^{n \\times m}$ into its corresponding centered anomaly matrix $X \\in \\mathbb{R}^{n \\times m}$. The derivation begins with the fundamental definitions provided.\n\nLet the ensemble matrix be $E$, whose columns are the $m$ ensemble members. The ensemble mean, $\\bar{x} \\in \\mathbb{R}^{n}$, is the average of these members.\n$$\n\\bar{x} = \\frac{1}{m} \\sum_{i=1}^{m} x_i = \\frac{1}{m} E \\mathbf{1}\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{m}$ is a column vector of ones.\n\nThe anomaly matrix, $X$, is defined as the ensemble matrix with the mean subtracted from each member. This can be written as:\n$$\nX = E - \\bar{x} \\mathbf{1}^{\\top}\n$$\nThe term $\\bar{x} \\mathbf{1}^{\\top}$ represents a matrix of size $n \\times m$ where each column is the mean vector $\\bar{x}$.\n\nTo derive the operator $C_m$ such that $X = E C_m$, we substitute the expression for the mean $\\bar{x}$ into the definition of the anomalies $X$:\n$$\nX = E - \\left(\\frac{1}{m} E \\mathbf{1}\\right) \\mathbf{1}^{\\top}\n$$\nUsing the associativity of matrix multiplication, the second term can be rewritten as:\n$$\nX = E - E \\left(\\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top}\\right)\n$$\nWe can now factor out the ensemble matrix $E$ on the left. To do this, we express the first term as $E$ multiplied by the $m \\times m$ identity matrix, $I$:\n$$\nX = E I - E \\left(\\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top}\\right)\n$$\nFactoring out $E$ gives:\n$$\nX = E \\left(I - \\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top}\\right)\n$$\nBy comparing this expression with the desired form $X = E C_m$, we can identify the centering operator $C_m$ as:\n$$\nC_m = I - \\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top}\n$$\nThe problem requires that the resulting anomalies $X$ satisfy the condition $X \\mathbf{1} = \\mathbf{0}$. We can verify this property by post-multiplying our expression for $X$ by $\\mathbf{1}$:\n$$\nX \\mathbf{1} = (E C_m) \\mathbf{1} = E (C_m \\mathbf{1})\n$$\nLet's evaluate the product $C_m \\mathbf{1}$:\n$$\nC_m \\mathbf{1} = \\left(I - \\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top}\\right) \\mathbf{1} = I \\mathbf{1} - \\frac{1}{m} \\mathbf{1} (\\mathbf{1}^{\\top} \\mathbf{1})\n$$\nThe product $\\mathbf{1}^{\\top} \\mathbf{1}$ is the dot product of the vector of ones with itself, which is the sum of $m$ ones, so $\\mathbf{1}^{\\top} \\mathbf{1} = m$. Substituting this result:\n$$\nC_m \\mathbf{1} = \\mathbf{1} - \\frac{1}{m} \\mathbf{1} (m) = \\mathbf{1} - \\mathbf{1} = \\mathbf{0}\n$$\nwhere $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^{m}$. Therefore, $X \\mathbf{1} = E \\mathbf{0} = \\mathbf{0}$ (the zero vector in $\\mathbb{R}^{n}$), confirming that the columns of the anomaly matrix $X$ sum to zero, as required for centered anomalies.\n\nThe operator $C_m$ is an orthogonal projection matrix. It projects vectors in $\\mathbb{R}^{m}$ onto the subspace orthogonal to the vector $\\mathbf{1}$. This is the subspace of vectors whose components sum to zero.\n\nNext, we must justify why adding a uniform mean increment $\\delta \\bar{x} \\mathbf{1}^{\\top}$ to an ensemble $E$ does not contaminate the anomalies computed using $C_m$. Let the new ensemble be $E' = E + \\delta \\bar{x} \\mathbf{1}^{\\top}$. The new anomaly matrix $X'$ is:\n$$\nX' = E' C_m = (E + \\delta \\bar{x} \\mathbf{1}^{\\top}) C_m = E C_m + (\\delta \\bar{x} \\mathbf{1}^{\\top}) C_m\n$$\nThe first term $E C_m$ is the original anomaly matrix $X$. The second term can be rewritten as:\n$$\n(\\delta \\bar{x} \\mathbf{1}^{\\top}) C_m = \\delta \\bar{x} (\\mathbf{1}^{\\top} C_m)\n$$\nThe term $\\mathbf{1}^{\\top} C_m$ is the transpose of $C_m^{\\top} \\mathbf{1}$. The matrix $C_m$ is symmetric since $(I - \\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top})^{\\top} = I^{\\top} - \\frac{1}{m} (\\mathbf{1}^{\\top})^{\\top} \\mathbf{1}^{\\top} = I - \\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top} = C_m$. Thus, $C_m^{\\top} = C_m$.\nSo, $\\mathbf{1}^{\\top} C_m = (C_m \\mathbf{1})^{\\top}$. As we showed previously, $C_m \\mathbf{1} = \\mathbf{0}$. Therefore, $\\mathbf{1}^{\\top} C_m = \\mathbf{0}^{\\top}$.\nThe contamination term is $\\delta \\bar{x} (\\mathbf{0}^{\\top}) = \\mathbf{0}$ (the $n \\times m$ zero matrix).\nThis proves that $X' = X + \\mathbf{0} = X$. The anomalies are invariant to any uniform shift of the ensemble, which is a direct consequence of the centering operator $C_m$ projecting onto the subspace orthogonal to the direction of the uniform shift (the vector $\\mathbf{1}$).\n\nFinally, we explain why this centering is essential for maintaining $X^a \\mathbf{1} = \\mathbf{0}$ in the context of EnSRF. The data assimilation update is conceptually separated into an update of the mean and an update of the anomalies (spread). The analysis ensemble $E^a$ is reconstituted from the updated analysis mean $\\bar{x}^a$ and the updated analysis anomalies $X^a$ as:\n$$\nE^a = \\bar{x}^a \\mathbf{1}^{\\top} + X^a\n$$\nThe mean of this newly formed ensemble $E^a$ is calculated as:\n$$\n\\text{mean}(E^a) = \\frac{1}{m} E^a \\mathbf{1} = \\frac{1}{m} (\\bar{x}^a \\mathbf{1}^{\\top} + X^a) \\mathbf{1}\n$$\n$$\n\\text{mean}(E^a) = \\frac{1}{m} \\bar{x}^a (\\mathbf{1}^{\\top} \\mathbf{1}) + \\frac{1}{m} X^a \\mathbf{1} = \\frac{1}{m} \\bar{x}^a (m) + \\frac{1}{m} X^a \\mathbf{1} = \\bar{x}^a + \\frac{1}{m} X^a \\mathbf{1}\n$$\nFor the mean of the analysis ensemble to be equal to the calculated analysis mean $\\bar{x}^a$, it is a mathematical necessity that the second term vanishes: $\\frac{1}{m} X^a \\mathbf{1} = \\mathbf{0}$, which requires $X^a \\mathbf{1} = \\mathbf{0}$. This condition means that the analysis anomalies must be centered. The definition of the analysis anomalies given in the problem, $X^{a} = \\widetilde{E}^{a} T - \\big(\\frac{1}{m} \\widetilde{E}^{a} T \\mathbf{1}\\big) \\mathbf{1}^{\\top}$, is precisely the application of this centering principle to the transformed ensemble $\\widetilde{E}^{a} T$. Using the operator $C_m$, this is written elegantly as $X^a = (\\widetilde{E}^{a} T) C_m$. This operation enforces the centering condition, which is fundamental to the consistency of the filter, ensuring that the statistics (mean and spread) of the final ensemble are exactly those prescribed by the assimilation algorithm.",
            "answer": "$$\n\\boxed{I - \\frac{1}{m} \\mathbf{1} \\mathbf{1}^{\\top}}\n$$"
        },
        {
            "introduction": "Now, let's move into the engine room of the Ensemble Transform Kalman Filter (ETKF), a prominent type of EnSRF. This exercise provides a concrete, small-scale example to walk you through the essential calculations performed in the low-dimensional ensemble space. By computing the analysis mean weights and the anomaly transform matrix, you will gain hands-on experience with the computational efficiency that makes these filters practical .",
            "id": "4038057",
            "problem": "Consider a linear, Gaussian data assimilation setting for an Ensemble Square Root Filter (EnSRF) or Ensemble Transform Kalman Filter (ETKF), applied to a forecast ensemble with $N_e = 4$ members. Let the observation operator be linear, the observation error be zero-mean Gaussian with covariance $R \\in \\mathbb{R}^{m \\times m}$, and the forecast ensemble anomalies in observation space be represented by the matrix $Y \\in \\mathbb{R}^{m \\times N_e}$, which satisfies $Y \\mathbf{1} = \\mathbf{0}$ (zero-sum across ensemble members). For a small example with $m = 2$, suppose\n$$\nR = I_2, \\quad\nY = \n\\begin{pmatrix}\n2 & -1 & -1 & 0 \\\\\n0 & 2 & -2 & 0\n\\end{pmatrix}, \\quad\nd = \n\\begin{pmatrix}\n3 \\\\\n0\n\\end{pmatrix},\n$$\nwhere $d$ is the innovation vector. Define the whitened projected ensemble anomalies \n$$\n\\tilde{Y} \\equiv R^{-1/2} Y \\left(N_e - 1\\right)^{-1/2}.\n$$\nStarting from the linear-Gaussian Bayesian update and the ensemble representation of forecast error covariance, derive the ensemble-space analysis mean weight vector $w^a \\in \\mathbb{R}^{N_e}$ and an ensemble-space symmetric transform $W^a \\in \\mathbb{R}^{N_e \\times N_e}$ that maps forecast anomalies to analysis anomalies consistently with the square root filtering principle. Compute $\\tilde{Y}$, $w^a$, and characterize $W^a$ via its spectrum. Explain the computational complexity of forming $\\tilde{Y}$, computing $w^a$, and constructing $W^a$ in terms of $N_e$ and $m$ (ignore the state dimension and assume matrix-vector multiplications are the dominant cost). Finally, compute the determinant of $W^a$ for the provided numerical example and express your final answer as an exact value with no rounding.",
            "solution": "The analysis step of an Ensemble Transform Kalman Filter (ETKF) updates the forecast ensemble mean $\\bar{x}^f$ and forecast anomalies $A^f$ to the analysis mean $\\bar{x}^a$ and analysis anomalies $A^a$. The updates are given by:\n$$\n\\bar{x}^a = \\bar{x}^f + A^f w^a\n$$\n$$\nA^a = A^f W^a\n$$\nwhere $w^a$ is the analysis mean weight vector and $W^a$ is a symmetric transform matrix. These are defined in the ensemble space of dimension $N_e$.\n\n**1. Derivations and Initial Computations**\n\nFirst, we define and compute the whitened projected anomalies $\\tilde{Y}$.\nGiven $R = I_2$, its square root inverse $R^{-1/2}$ is also $I_2$. With $N_e = 4$, we have $(N_e - 1)^{-1/2} = 1/\\sqrt{3}$.\n$$\n\\tilde{Y} = R^{-1/2} Y (N_e - 1)^{-1/2} = I_2 \\begin{pmatrix} 2 & -1 & -1 & 0 \\\\ 0 & 2 & -2 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{3}} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2 & -1 & -1 & 0 \\\\ 0 & 2 & -2 & 0 \\end{pmatrix}\n$$\nThe core of the ETKF is the $N_e \\times N_e$ matrix $C = \\tilde{Y}^T \\tilde{Y}$.\n$$\nC = \\tilde{Y}^T \\tilde{Y} = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2 & 0 \\\\ -1 & 2 \\\\ -1 & -2 \\\\ 0 & 0 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2 & -1 & -1 & 0 \\\\ 0 & 2 & -2 & 0 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} 4 & -2 & -2 & 0 \\\\ -2 & 5 & -3 & 0 \\\\ -2 & -3 & 5 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\n\n**2. Analysis Mean Weight Vector ($w^a$)**\n\nThe analysis mean weight vector $w^a$ is derived from the standard Kalman gain formula, expressed in ensemble space:\n$$\nw^a = \\frac{1}{\\sqrt{N_e - 1}} (I_{N_e} + \\tilde{Y}^T \\tilde{Y})^{-1} \\tilde{Y}^T \\tilde{d}\n$$\nwhere $\\tilde{d} = R^{-1/2} d$.\nFirst, we compute the terms needed:\n$\\tilde{d} = I_2 d = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$.\n$$\n\\tilde{Y}^T \\tilde{d} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2 & 0 \\\\ -1 & 2 \\\\ -1 & -2 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 6 \\\\ -3 \\\\ -3 \\\\ 0 \\end{pmatrix} = \\sqrt{3} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nNext, we compute $(I_{N_e} + C)^{-1}$.\n$$\nI_{N_e} + C = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 4 & -2 & -2 & 0 \\\\ -2 & 5 & -3 & 0 \\\\ -2 & -3 & 5 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 7 & -2 & -2 & 0 \\\\ -2 & 8 & -3 & 0 \\\\ -2 & -3 & 8 & 0 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix}\n$$\nLet the $3 \\times 3$ sub-matrix be $B = \\begin{pmatrix} 7 & -2 & -2 \\\\ -2 & 8 & -3 \\\\ -2 & -3 & 8 \\end{pmatrix}$. Its determinant is $\\det(B) = 7(64-9) + 2(-16-6) - 2(6+16) = 7(55) - 2(22) - 2(22) = 385 - 88 = 297$.\nThe inverse is $(I_{N_e} + C)^{-1} = 3 \\begin{pmatrix} B^{-1} & \\mathbf{0} \\\\ \\mathbf{0}^T & 1/3 \\end{pmatrix}$.\nWe find $B^{-1} = \\frac{1}{297} \\begin{pmatrix} 55 & 22 & 22 \\\\ 22 & 52 & 25 \\\\ 22 & 25 & 52 \\end{pmatrix}$.\nSo, $(I_{N_e} + C)^{-1} = \\frac{3}{297} \\begin{pmatrix} 55 & 22 & 22 & 0 \\\\ 22 & 52 & 25 & 0 \\\\ 22 & 25 & 52 & 0 \\\\ 0 & 0 & 0 & 99 \\end{pmatrix} = \\frac{1}{99} \\begin{pmatrix} 55 & 22 & 22 & 0 \\\\ 22 & 52 & 25 & 0 \\\\ 22 & 25 & 52 & 0 \\\\ 0 & 0 & 0 & 99 \\end{pmatrix}$.\nNow we compute $w^a$:\n$$\nw^a = \\frac{1}{\\sqrt{3}} \\left( \\frac{1}{99} \\begin{pmatrix} 55 & 22 & 22 & 0 \\\\ 22 & 52 & 25 & 0 \\\\ 22 & 25 & 52 & 0 \\\\ 0 & 0 & 0 & 99 \\end{pmatrix} \\right) \\left( \\sqrt{3} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\right)\n= \\frac{1}{99} \\begin{pmatrix} 55(2) + 22(-1) + 22(-1) \\\\ 22(2) + 52(-1) + 25(-1) \\\\ 22(2) + 25(-1) + 52(-1) \\\\ 0 \\end{pmatrix}\n$$\n$$\nw^a = \\frac{1}{99} \\begin{pmatrix} 110 - 22 - 22 \\\\ 44 - 52 - 25 \\\\ 44 - 25 - 52 \\\\ 0 \\end{pmatrix} = \\frac{1}{99} \\begin{pmatrix} 66 \\\\ -33 \\\\ -33 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\\\ 0 \\end{pmatrix}\n$$\n\n**3. Anomaly Transform Matrix ($W^a$)**\n\nThe symmetric transform matrix $W^a$ is the matrix square root of the analysis error covariance in ensemble space:\n$$\nW^a = \\left[ (I_{N_e} + \\tilde{Y}^T \\tilde{Y})^{-1} \\right]^{1/2} = \\left[ (I_{N_e} + C)^{-1} \\right]^{1/2}\n$$\nTo characterize $W^a$ via its spectrum, we find the eigenvalues of $C$, denoted $\\lambda_i$. The eigenvalues of $W^a$ will then be $(1+\\lambda_i)^{-1/2}$.\nThe eigenvalues of $C$ are $1/3$ times the eigenvalues of $3C = \\begin{pmatrix} 4 & -2 & -2 & 0 \\\\ -2 & 5 & -3 & 0 \\\\ -2 & -3 & 5 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$.\nOne eigenvalue is clearly $0$. The others are from the upper-left $3 \\times 3$ block, say $M$. The characteristic polynomial of $M$ is $\\det(M-\\lambda I) = -\\lambda(\\lambda-6)(\\lambda-8)$. The eigenvalues of $M$ are $\\{0, 6, 8\\}$.\nThus, the eigenvalues of $3C$ are $\\{8, 6, 0, 0\\}$.\nThe eigenvalues of $C$ are $\\lambda_i = \\{8/3, 2, 0, 0\\}$.\nThe spectrum of $W^a$ is therefore:\n$$\n\\left\\{ (1+8/3)^{-1/2}, (1+2)^{-1/2}, (1+0)^{-1/2}, (1+0)^{-1/2} \\right\\} = \\left\\{ (11/3)^{-1/2}, (3)^{-1/2}, 1, 1 \\right\\} = \\left\\{ \\sqrt{\\frac{3}{11}}, \\frac{1}{\\sqrt{3}}, 1, 1 \\right\\}\n$$\n\n**4. Computational Complexity**\n\n-   **Forming $\\tilde{Y}$**: Computing $R^{-1/2}$ for a general $m \\times m$ matrix $R$ costs $O(m^3)$. The matrix product $R^{-1/2}Y$ costs $O(m^2 N_e)$. Total complexity is $O(m^3 + m^2 N_e)$. If $R$ is diagonal, this reduces to $O(m^2 N_e)$.\n-   **Computing $w^a$**: The main steps are:\n    1.  Forming $C = \\tilde{Y}^T \\tilde{Y}$: an $N_e \\times m$ matrix times an $m \\times N_e$ matrix, cost $O(N_e^2 m)$.\n    2.  Inverting $(I_{N_e} + C)$: an $N_e \\times N_e$ matrix, cost $O(N_e^3)$.\n    3.  Matrix-vector products: cost $O(N_e m)$ and $O(N_e^2)$.\n    The dominant cost is $O(N_e^2 m + N_e^3)$. Since typically $N_e \\ll m$, this is often cited as $O(N_e^2 m)$.\n-   **Constructing $W^a$**: This requires the eigendecomposition of the $N_e \\times N_e$ matrix $(I_{N_e} + C)$, which costs $O(N_e^3)$.\n    1.  Forming $C$: $O(N_e^2 m)$.\n    2.  Eigendecomposition: $O(N_e^3)$.\n    3.  Reconstructing $W^a$ from its eigen-components: $O(N_e^3)$.\n    The total dominant cost is again $O(N_e^2 m + N_e^3)$.\n\n**5. Determinant of $W^a$**\n\nThe determinant of a matrix is the product of its eigenvalues. Using the spectrum of $W^a$ computed above:\n$$\n\\det(W^a) = 1 \\times 1 \\times \\frac{1}{\\sqrt{3}} \\times \\sqrt{\\frac{3}{11}} = \\frac{1}{\\sqrt{3}} \\frac{\\sqrt{3}}{\\sqrt{11}} = \\frac{1}{\\sqrt{11}}\n$$\nAlternatively, we can use the property $\\det(A^{1/2}) = \\sqrt{\\det(A)}$ and $\\det(A^{-1}) = (\\det(A))^{-1}$:\n$$\n\\det(W^a) = \\sqrt{\\det((I+C)^{-1})} = \\sqrt{(\\det(I+C))^{-1}} = (\\det(I+C))^{-1/2}\n$$\nWe compute $\\det(I+C)$:\n$$\n\\det(I+C) = \\det\\left( \\frac{1}{3} \\begin{pmatrix} 7 & -2 & -2 & 0 \\\\ -2 & 8 & -3 & 0 \\\\ -2 & -3 & 8 & 0 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} \\right) = \\left(\\frac{1}{3}\\right)^4 \\det \\begin{pmatrix} 7 & -2 & -2 & 0 \\\\ -2 & 8 & -3 & 0 \\\\ -2 & -3 & 8 & 0 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix}\n$$\nThe determinant of the block matrix is $3 \\times \\det(B) = 3 \\times 297$.\n$$\n\\det(I+C) = \\frac{1}{81} \\times (3 \\times 297) = \\frac{297}{27} = 11\n$$\nTherefore, $\\det(W^a) = (11)^{-1/2} = \\frac{1}{\\sqrt{11}}$. The results are consistent.\nThe final answer is required as an exact value.\n$$\n\\det(W^a) = \\frac{1}{\\sqrt{11}}\n$$",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{11}}}\n$$"
        },
        {
            "introduction": "The final step is to verify that the entire square-root filtering process achieves its goal. This practice connects the theoretical Bayesian update with the practical ensemble transformation you've just learned about. You will construct the final analysis ensemble members and confirm that their sample mean and covariance precisely match the target analysis statistics, demonstrating the mathematical consistency and elegance of the EnSRF framework .",
            "id": "4038070",
            "problem": "Consider a single linear Gaussian data assimilation step in a two-dimensional state space for a numerical weather prediction model. An Ensemble Square Root Filter (ESRF) is used with an ensemble size of $N_e = 3$. The forecast ensemble has sample mean $\\bar{x}^f \\in \\mathbb{R}^2$ and scaled anomaly matrix $A^f \\in \\mathbb{R}^{2 \\times 3}$, defined so that the forecast covariance satisfies $P^f = A^f (A^f)^{\\top}$. Observations are taken with a linear observation operator $H \\in \\mathbb{R}^{2 \\times 2}$ and observation error covariance $R \\in \\mathbb{R}^{2 \\times 2}$, and the observation vector is $y \\in \\mathbb{R}^2$.\n\nThe following specific, scientifically plausible configuration is given:\n- State dimension $m = 2$ and ensemble size $N_e = 3$.\n- Forecast mean $\\bar{x}^f = \\begin{pmatrix}0 \\\\ 0 \\end{pmatrix}$.\n- Scaled forecast anomalies\n$$\nA^f = \\begin{pmatrix}\n\\sqrt{\\frac{2}{3}} & -\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{6}} \\\\\n0 & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n$$\nwhich are constructed to be zero-mean across columns and isotropic in covariance.\n- Observation operator $H = I_2$ (the $2 \\times 2$ identity matrix).\n- Observation error covariance $R = 2 I_2$.\n- Observation vector $y = \\begin{pmatrix}3 \\\\ -1 \\end{pmatrix}$.\n\nStarting from first principles of linear Gaussian estimation (Bayesian derivation of the Kalman filter) and the definition of sample statistics for ensembles, do the following:\n1. Derive and compute the Kalman gain $K$, the analysis mean $\\bar{x}^a$, and the analysis covariance $P^a$.\n2. Derive and compute a symmetric square-root ensemble transform matrix $T \\in \\mathbb{R}^{3 \\times 3}$ in ensemble space such that the analysis scaled anomalies satisfy $A^a = A^f T$ and $A^a (A^a)^{\\top} = P^a$. Use only the isotropy implied by $H$, $R$, and $P^f$ for your derivation steps.\n3. Construct the analysis ensemble members from the analysis mean and scaled anomalies using the canonical square-root reconstruction rule, and compute their sample mean and sample covariance.\n4. Define the verification metric\n$$\nJ \\;=\\; \\left\\| \\bar{x}_{\\text{sample}}^a - \\bar{x}^a \\right\\|_2^2 \\;+\\; \\left\\| P_{\\text{sample}}^a - P^a \\right\\|_F^2,\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm and $\\|\\cdot\\|_F$ is the Frobenius norm. Compute $J$ exactly. No rounding is required. The final answer must be the single value of $J$ with no units.",
            "solution": "The solution is presented in four parts as requested by the problem statement.\n\n### Part 1: Kalman Gain, Analysis Mean, and Analysis Covariance\n\nThe foundation of the analysis step is the set of standard Kalman filter update equations derived from linear Gaussian Bayesian inference. The analysis mean $\\bar{x}^a$ and analysis covariance $P^a$ are given by:\n$$ \\bar{x}^a = \\bar{x}^f + K(y - H\\bar{x}^f) $$\n$$ P^a = (I - KH)P^f $$\nwhere the Kalman gain $K$ is defined as:\n$$ K = P^f H^{\\top} (H P^f H^{\\top} + R)^{-1} $$\n\nFirst, we compute the forecast covariance matrix $P^f$ from the given scaled anomaly matrix $A^f$. The problem defines the relationship as $P^f = A^f (A^f)^{\\top}$.\n$$ P^f = \\begin{pmatrix} \\sqrt{\\frac{2}{3}} & -\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\frac{2}{3}} & 0 \\\\ -\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n$$ P^f = \\begin{pmatrix} \\left(\\frac{2}{3}\\right) + \\left(\\frac{1}{6}\\right) + \\left(\\frac{1}{6}\\right) & 0 - \\frac{1}{\\sqrt{12}} + \\frac{1}{\\sqrt{12}} \\\\ 0 - \\frac{1}{\\sqrt{12}} + \\frac{1}{\\sqrt{12}} & 0 + \\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3}+\\frac{1}{3} & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nThe forecast covariance is the $2 \\times 2$ identity matrix, confirming the stated isotropy.\n\nNext, we compute the Kalman gain $K$. We are given $H=I_2$ and $R=2I_2$.\nThe term $H P^f H^{\\top} + R$ becomes:\n$$ H P^f H^{\\top} + R = I_2 I_2 I_2^{\\top} + 2I_2 = I_2 + 2I_2 = 3I_2 $$\nIts inverse is $(3I_2)^{-1} = \\frac{1}{3}I_2$.\nNow, we compute $K$:\n$$ K = (P^f H^{\\top}) (H P^f H^{\\top} + R)^{-1} = (I_2 I_2^{\\top}) \\left(\\frac{1}{3}I_2\\right) = I_2 \\left(\\frac{1}{3}I_2\\right) = \\frac{1}{3}I_2 $$\n\nUsing the computed gain $K$, we find the analysis mean $\\bar{x}^a$. We are given $\\bar{x}^f = \\begin{pmatrix}0 \\\\ 0 \\end{pmatrix}$ and $y = \\begin{pmatrix}3 \\\\ -1 \\end{pmatrix}$.\n$$ \\bar{x}^a = \\bar{x}^f + K(y - H\\bar{x}^f) = \\begin{pmatrix}0 \\\\ 0 \\end{pmatrix} + \\frac{1}{3}I_2 \\left( \\begin{pmatrix}3 \\\\ -1 \\end{pmatrix} - I_2 \\begin{pmatrix}0 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{3}\\begin{pmatrix}3 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix}1 \\\\ -\\frac{1}{3} \\end{pmatrix} $$\n\nFinally, we compute the analysis covariance $P^a$:\n$$ P^a = (I - KH)P^f = \\left(I_2 - \\left(\\frac{1}{3}I_2\\right)I_2\\right)I_2 = \\left(I_2 - \\frac{1}{3}I_2\\right)I_2 = \\frac{2}{3}I_2 $$\nThe analysis covariance is $P^a = \\begin{pmatrix} \\frac{2}{3} & 0 \\\\ 0 & \\frac{2}{3} \\end{pmatrix}$.\n\n### Part 2: Ensemble Transform Matrix\n\nWe need to find a symmetric matrix $T \\in \\mathbb{R}^{3 \\times 3}$ such that the analysis scaled anomalies $A^a = A^f T$ satisfy $A^a (A^a)^{\\top} = P^a$.\nThe problem specifies to use the isotropy of the system. Given that $P^f$, $H$, and $R$ are all isotropic (proportional to the identity matrix), the transformation from the forecast to the analysis space is also isotropic. This suggests that the simplest form for the transformation of anomalies is a simple scaling. Let us therefore propose a transform matrix of the form $T = c I_{N_e}$, where $c$ is a scalar and $N_e = 3$. This choice of $T$ is symmetric.\n\nSubstituting $A^a = c A^f$ into the covariance equation:\n$$ P^a = A^a (A^a)^{\\top} = (c A^f)(c A^f)^{\\top} = c^2 A^f (A^f)^{\\top} = c^2 P^f $$\nFrom Part 1, we know $P^a = \\frac{2}{3}I_2$ and $P^f=I_2$.\n$$ \\frac{2}{3}I_2 = c^2 I_2 $$\nThis implies $c^2 = \\frac{2}{3}$. For a positive-definite transformation, we choose the positive root $c = \\sqrt{\\frac{2}{3}}$.\nThus, a valid symmetric transform matrix is:\n$$ T = \\sqrt{\\frac{2}{3}} I_3 = \\begin{pmatrix} \\sqrt{\\frac{2}{3}} & 0 & 0 \\\\ 0 & \\sqrt{\\frac{2}{3}} & 0 \\\\ 0 & 0 & \\sqrt{\\frac{2}{3}} \\end{pmatrix} $$\nIt is worth noting that the standard Ensemble Square Root Filter formula for $T$ is $T_{\\text{std}} = (I + (HA^f)^\\top R^{-1} (HA^f))^{-1/2}$. For centered anomalies where the columns of $A^f$ sum to zero (as they do here), $A^f J = 0$, where $J$ is the matrix of all ones. In this case, $A^f T_{\\text{std}} = A^f (\\sqrt{\\frac{2}{3}} I_3)$. This confirms that our choice of $T$ based on isotropy leads to the correct physical result for the analysis anomalies.\n\n### Part 3: Analysis Ensemble Members and Sample Statistics\n\nThe analysis scaled anomalies are $A^a=A^f T = \\sqrt{\\frac{2}{3}}A^f$:\n$$ A^a = \\sqrt{\\frac{2}{3}} \\begin{pmatrix} \\sqrt{\\frac{2}{3}} & -\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\\\ 0 & \\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{3}} \\end{pmatrix} $$\nThe canonical reconstruction rule for ensemble members requires the unscaled anomaly matrix $X'^a$, whose columns are the deviations from the mean. The relation is $X'^a = \\sqrt{N_e - 1} A^a$. Here, $N_e=3$.\n$$ X'^a = \\sqrt{2} A^a = \\sqrt{2} \\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & -\\frac{1}{3} \\\\ 0 & \\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{2\\sqrt{2}}{3} & -\\frac{\\sqrt{2}}{3} & -\\frac{\\sqrt{2}}{3} \\\\ 0 & \\sqrt{\\frac{2}{3}} & -\\sqrt{\\frac{2}{3}} \\end{pmatrix} $$\nThe analysis ensemble members $x_i^a$ are constructed as $x_i^a = \\bar{x}^a + X'_{a,i}$ where $X'_{a,i}$ is the $i$-th column of $X'^a$.\n$$ x_1^a = \\begin{pmatrix}1 \\\\ -\\frac{1}{3} \\end{pmatrix} + \\begin{pmatrix} \\frac{2\\sqrt{2}}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{2\\sqrt{2}}{3} \\\\ -\\frac{1}{3} \\end{pmatrix} $$\n$$ x_2^a = \\begin{pmatrix}1 \\\\ -\\frac{1}{3} \\end{pmatrix} + \\begin{pmatrix} -\\frac{\\sqrt{2}}{3} \\\\ \\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{\\sqrt{2}}{3} \\\\ -\\frac{1}{3} + \\sqrt{\\frac{2}{3}} \\end{pmatrix} $$\n$$ x_3^a = \\begin{pmatrix}1 \\\\ -\\frac{1}{3} \\end{pmatrix} + \\begin{pmatrix} -\\frac{\\sqrt{2}}{3} \\\\ -\\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{\\sqrt{2}}{3} \\\\ -\\frac{1}{3} - \\sqrt{\\frac{2}{3}} \\end{pmatrix} $$\nNow, we compute the sample mean $\\bar{x}_{\\text{sample}}^a = \\frac{1}{3}(x_1^a + x_2^a + x_3^a)$:\n$$ \\bar{x}_{\\text{sample}}^a = \\frac{1}{3} \\left( 3\\bar{x}^a + (X'_{a,1} + X'_{a,2} + X'_{a,3}) \\right) = \\bar{x}^a + \\frac{1}{3} X'^a \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} $$\nThe sum of the columns of $X'^a$ is $\\begin{pmatrix} \\frac{2\\sqrt{2}}{3}-\\frac{\\sqrt{2}}{3}-\\frac{\\sqrt{2}}{3} \\\\ 0+\\sqrt{\\frac{2}{3}}-\\sqrt{\\frac{2}{3}} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This is guaranteed because the initial anomalies in $A^f$ were centered.\nTherefore, $\\bar{x}_{\\text{sample}}^a = \\bar{x}^a = \\begin{pmatrix}1 \\\\ -\\frac{1}{3} \\end{pmatrix}$.\n\nNext, we compute the sample covariance $P_{\\text{sample}}^a = \\frac{1}{N_e-1} X'^a (X'^a)^{\\top}$:\n$$ P_{\\text{sample}}^a = \\frac{1}{2} (\\sqrt{2}A^a) (\\sqrt{2}A^a)^{\\top} = \\frac{1}{2}(2)A^a(A^a)^{\\top} = A^a(A^a)^{\\top} $$\nBy definition of the analysis step, we required $A^a(A^a)^{\\top} = P^a$.\nTherefore, $P_{\\text{sample}}^a = P^a = \\frac{2}{3}I_2$.\n\n### Part 4: Verification Metric\n\nThe verification metric is defined as $ J = \\left\\| \\bar{x}_{\\text{sample}}^a - \\bar{x}^a \\right\\|_2^2 + \\left\\| P_{\\text{sample}}^a - P^a \\right\\|_F^2 $.\nFrom Part 3, we have rigorously shown that:\n1. $\\bar{x}_{\\text{sample}}^a = \\bar{x}^a$, which implies $\\bar{x}_{\\text{sample}}^a - \\bar{x}^a = \\mathbf{0}$.\n2. $P_{\\text{sample}}^a = P^a$, which implies $P_{\\text{sample}}^a - P^a = \\mathbf{0}$.\n\nSubstituting these results into the definition of $J$:\n$$ J = \\left\\| \\mathbf{0} \\right\\|_2^2 + \\left\\| \\mathbf{0} \\right\\|_F^2 $$\nThe Euclidean norm of a zero vector is $0$, and the Frobenius norm of a zero matrix is $0$.\n$$ J = 0^2 + 0^2 = 0 $$\nThe exact value of the verification metric $J$ is $0$. This result confirms that the Ensemble Square Root Filter methodology, when correctly applied, produces an analysis ensemble whose sample first and second moments exactly match the theoretical moments derived from the Kalman filter equations.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}