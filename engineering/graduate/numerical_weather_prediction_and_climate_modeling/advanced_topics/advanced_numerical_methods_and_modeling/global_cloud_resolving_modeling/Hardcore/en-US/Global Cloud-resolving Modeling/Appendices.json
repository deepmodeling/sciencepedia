{
    "hands_on_practices": [
        {
            "introduction": "Global cloud-resolving models represent one of the grand challenges in computational science. Before embarking on the complex task of developing or running such a model, it is a crucial first step to estimate its computational demands. This exercise guides you through a \"back-of-the-envelope\" calculation to quantify the immense scale of the problem, translating model resolution and simulation length into the total number of calculations and the required time on a modern supercomputer . This fundamental skill helps ground our ambitions in the reality of available computational resources.",
            "id": "4049766",
            "problem": "A global cloud-resolving model (CRM) that explicitly resolves deep convection is to be run on a quasi-uniform spherical mesh covering the entire Earth. Assume a horizontal grid spacing of $\\Delta x = 3\\,\\mathrm{km}$, a vertical grid spacing of $\\Delta z = 250\\,\\mathrm{m}$ up to a model top at $H = 30\\,\\mathrm{km}$, and a time step of $\\Delta t = 1\\,\\mathrm{s}$ for a total simulated duration of $T_{\\mathrm{sim}} = 30\\,\\mathrm{days}$. Let the Earth’s radius be $R = 6.371\\times 10^{6}\\,\\mathrm{m}$. For a quasi-uniform mesh, approximate the number of horizontal grid cells by the Earth’s surface area divided by the cell area, i.e., use $N_{h} \\approx \\dfrac{4\\pi R^{2}}{\\Delta x^{2}}$. The number of vertical levels is $N_{z} \\approx \\dfrac{H}{\\Delta z}$, and the number of time steps is $N_{t} \\approx \\dfrac{T_{\\mathrm{sim}}}{\\Delta t}$. Assume an average computational cost of $f = 2.0\\times 10^{7}$ floating-point operations per grid point per time step, aggregated over dynamics and physics. Define an “exascale” system as one that sustains $E = 1.0\\times 10^{18}$ floating-point operations per second (FLOPS).\n\nStarting from these definitions and fundamental geometric relations, derive symbolic expressions for $N_{h}$, $N_{z}$, $N_{t}$, and the total floating-point operations $F_{\\mathrm{tot}}$ required for the full $30\\,\\mathrm{day}$ integration. Then compute the wallclock time $T_{\\mathrm{wall}}$ (in seconds) needed on the ideal exascale system, and finally convert it to days. Express your final answer for $T_{\\mathrm{wall}}$ in $\\mathrm{days}$, rounded to three significant figures.",
            "solution": "The problem requires an estimation of the total computational cost and the resulting wallclock time for a global cloud-resolving model simulation on an exascale computing system. The validation process confirms that the problem is well-posed, scientifically grounded, and provides all necessary data for a unique solution.\n\nFirst, we establish the symbolic expressions for the number of grid cells and time steps based on the provided parameters.\n\nThe number of horizontal grid cells, $N_{h}$, is approximated by dividing the Earth's surface area, $4\\pi R^{2}$, by the area of a single grid cell, approximated as $\\Delta x^{2}$.\n$$N_{h} \\approx \\frac{4\\pi R^{2}}{\\Delta x^{2}}$$\n\nThe number of vertical levels, $N_{z}$, is the model top height, $H$, divided by the vertical grid spacing, $\\Delta z$.\n$$N_{z} \\approx \\frac{H}{\\Delta z}$$\n\nThe number of time steps, $N_{t}$, is the total simulation duration, $T_{\\mathrm{sim}}$, divided by the time step, $\\Delta t$.\n$$N_{t} \\approx \\frac{T_{\\mathrm{sim}}}{\\Delta t}$$\n\nThe total number of grid points in the three-dimensional model domain is the product of the horizontal and vertical grid cell counts.\n$$N_{\\mathrm{grid}} = N_{h} N_{z}$$\n\nThe total number of floating-point operations, $F_{\\mathrm{tot}}$, is the product of the total number of grid points, the number of time steps, and the computational cost per grid point per time step, $f$. The symbolic expression for $F_{\\mathrm{tot}}$ is:\n$$F_{\\mathrm{tot}} = N_{h} N_{z} N_{t} f$$\nSubstituting the expressions for $N_{h}$, $N_{z}$, and $N_{t}$, we obtain:\n$$F_{\\mathrm{tot}} \\approx \\left( \\frac{4\\pi R^{2}}{\\Delta x^{2}} \\right) \\left( \\frac{H}{\\Delta z} \\right) \\left( \\frac{T_{\\mathrm{sim}}}{\\Delta t} \\right) f$$\n\nThe wallclock time, $T_{\\mathrm{wall}}$, required to complete the simulation is the total number of floating-point operations, $F_{\\mathrm{tot}}$, divided by the sustained performance of the computer system, $E$.\n$$T_{\\mathrm{wall}} = \\frac{F_{\\mathrm{tot}}}{E} \\approx \\frac{1}{E} \\left[ \\left( \\frac{4\\pi R^{2}}{\\Delta x^{2}} \\right) \\left( \\frac{H}{\\Delta z} \\right) \\left( \\frac{T_{\\mathrm{sim}}}{\\Delta t} \\right) f \\right]$$\n\nTo compute the numerical value, we first ensure all parameters are in consistent SI units (meters, seconds).\nThe given parameters are:\n- Earth's radius: $R = 6.371\\times 10^{6}\\,\\mathrm{m}$\n- Horizontal grid spacing: $\\Delta x = 3\\,\\mathrm{km} = 3 \\times 10^{3}\\,\\mathrm{m}$\n- Model top height: $H = 30\\,\\mathrm{km} = 3 \\times 10^{4}\\,\\mathrm{m}$\n- Vertical grid spacing: $\\Delta z = 250\\,\\mathrm{m}$\n- Total simulation duration: $T_{\\mathrm{sim}} = 30\\,\\mathrm{days} = 30 \\times 24 \\times 3600\\,\\mathrm{s} = 2.592 \\times 10^{6}\\,\\mathrm{s}$\n- Time step: $\\Delta t = 1\\,\\mathrm{s}$\n- Computational cost: $f = 2.0\\times 10^{7}\\,\\mathrm{FLOPS / (grid\\,point \\cdot time\\,step)}$\n- System performance: $E = 1.0\\times 10^{18}\\,\\mathrm{FLOPS}$\n\nWe now compute the numerical values for $N_{h}$, $N_{z}$, and $N_{t}$.\n$$N_{h} \\approx \\frac{4\\pi (6.371\\times 10^{6}\\,\\mathrm{m})^{2}}{(3 \\times 10^{3}\\,\\mathrm{m})^{2}} = \\frac{4\\pi (6.371)^{2} \\times 10^{12}\\,\\mathrm{m}^{2}}{9 \\times 10^{6}\\,\\mathrm{m}^{2}} \\approx 5.6678 \\times 10^{7}$$\n$$N_{z} \\approx \\frac{3 \\times 10^{4}\\,\\mathrm{m}}{250\\,\\mathrm{m}} = 120$$\n$$N_{t} \\approx \\frac{2.592 \\times 10^{6}\\,\\mathrm{s}}{1\\,\\mathrm{s}} = 2.592 \\times 10^{6}$$\n\nNext, we calculate the total floating-point operations, $F_{\\mathrm{tot}}$.\n$$F_{\\mathrm{tot}} \\approx N_{h} N_{z} N_{t} f \\approx (5.6678 \\times 10^{7}) \\times (120) \\times (2.592 \\times 10^{6}) \\times (2.0 \\times 10^{7})$$\n$$F_{\\mathrm{tot}} \\approx (5.6678 \\times 120 \\times 2.592 \\times 2.0) \\times 10^{(7 + 6 + 7)} \\approx 3523.5 \\times 10^{20}$$\n$$F_{\\mathrm{tot}} \\approx 3.5235 \\times 10^{23}\\,\\mathrm{FLOPS}$$\n\nNow, we compute the wallclock time in seconds, $T_{\\mathrm{wall, s}}$.\n$$T_{\\mathrm{wall, s}} = \\frac{F_{\\mathrm{tot}}}{E} \\approx \\frac{3.5235 \\times 10^{23}\\,\\mathrm{FLOPS}}{1.0 \\times 10^{18}\\,\\mathrm{FLOPS}} = 3.5235 \\times 10^{5}\\,\\mathrm{s}$$\n\nFinally, we convert the wallclock time from seconds to days. There are $24 \\times 60 \\times 60 = 86400$ seconds in one day.\n$$T_{\\mathrm{wall, days}} = \\frac{T_{\\mathrm{wall, s}}}{86400\\,\\mathrm{s/day}} \\approx \\frac{3.5235 \\times 10^{5}\\,\\mathrm{s}}{86400\\,\\mathrm{s/day}} \\approx 4.0781\\,\\mathrm{days}$$\n\nRounding the result to three significant figures, as required, we get:\n$$T_{\\mathrm{wall, days}} \\approx 4.08\\,\\mathrm{days}$$",
            "answer": "$$\\boxed{4.08}$$"
        },
        {
            "introduction": "The computational cost estimated in the previous exercise is directly proportional to the number of time steps, which is determined by the length of each step, $\\Delta t$. In explicit numerical models, the choice of $\\Delta t$ is not arbitrary; it is strictly limited by stability criteria, most famously the Courant-Friedrichs-Lewy (CFL) condition. This practice explores how different physical processes, from air motion to the propagation of sound and gravity waves, each impose their own speed limit on the simulation, forcing us to identify the most restrictive constraint to ensure a stable model .",
            "id": "4049813",
            "problem": "A global Cloud-Resolving Model (CRM) in the nonhydrostatic, fully compressible formulation is discretized on a uniform grid with horizontal spacing $\\Delta x = 3\\,\\mathrm{km}$ and vertical spacing $\\Delta z = 100\\,\\mathrm{m}$. The maximum horizontal wind speed is $U = 50\\,\\mathrm{m\\,s^{-1}}$, and the speed of sound is $c = 340\\,\\mathrm{m\\,s^{-1}}$. For the fastest external gravity wave, adopt the shallow-water phase speed approximation $c_{g} = \\sqrt{g H}$ with gravitational acceleration $g = 9.81\\,\\mathrm{m\\,s^{-2}}$ and equivalent depth $H = 10\\,\\mathrm{km}$. Consider a proposed explicit large time step $\\Delta t = 1.0\\,\\mathrm{s}$.\n\nStarting from the principle that explicit numerical schemes must respect the Courant-Friedrichs-Lewy (CFL) stability condition, derive from first principles the process-specific Courant numbers for horizontal advection, vertical acoustic waves, and external gravity waves. Using these, compute the maximum among the three Courant numbers at the proposed $\\Delta t$. Round your answer to four significant figures and express it as a dimensionless number.\n\nAdditionally, in your derivation, explain how the CFL condition constrains the time step for each process and identify which process is most restrictive for the given grid and parameters. Do not provide any intermediate or final answers in units other than those specified, and do not include the final numerical value for any quantity other than the requested maximum Courant number.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is based on the fundamental principles of numerical fluid dynamics, specifically the Courant-Friedrichs-Lewy (CFL) stability condition for explicit numerical schemes. A solution can therefore be derived.\n\nThe Courant-Friedrichs-Lewy (CFL) condition is a necessary condition for the stability of explicit time-integration schemes when solving hyperbolic partial differential equations numerically. The principle dictates that the numerical domain of dependence must contain the physical domain of dependence. For a one-dimensional problem, this is expressed through the dimensionless Courant number, $C$, which must remain below a certain threshold, typically $1$, for stability. The Courant number is defined as the ratio of the distance a wave or signal travels in one time step, $\\Delta t$, to the grid spacing, $\\Delta x$. The general form is:\n$$C = \\frac{\\lambda \\Delta t}{\\Delta x} \\le C_{max}$$\nwhere $\\lambda$ is the fastest characteristic speed of the physical process being modeled, and $C_{max}$ is a constant of order $1$, dependent on the specific numerical scheme. For the simulation to be stable, the time step $\\Delta t$ must be chosen such that this condition is satisfied for all relevant wave-like and advective processes resolved on the computational grid. The process with the highest characteristic speed relative to its corresponding grid spacing will impose the most severe constraint on the maximum allowable time step.\n\nWe will now derive the process-specific Courant numbers for the three phenomena specified—horizontal advection, vertical acoustic waves, and external gravity waves—using the proposed time step $\\Delta t = 1.0\\,\\mathrm{s}$. The provided grid spacings must be in consistent units, so we convert kilometers to meters: $\\Delta x = 3\\,\\mathrm{km} = 3000\\,\\mathrm{m}$ and $H = 10\\,\\mathrm{km} = 10000\\,\\mathrm{m}$.\n\n1.  **Horizontal Advection:** Advection describes the transport of a substance or property by the bulk motion of the fluid. The characteristic speed for this process is the fluid velocity itself. The problem specifies a maximum horizontal wind speed of $U = 50\\,\\mathrm{m\\,s^{-1}}$. The relevant grid spacing is in the horizontal direction, $\\Delta x = 3000\\,\\mathrm{m}$. The Courant number for horizontal advection, $C_{adv}$, is therefore:\n    $$C_{adv} = \\frac{U \\Delta t}{\\Delta x}$$\n    Substituting the given values:\n    $$C_{adv} = \\frac{(50\\,\\mathrm{m\\,s^{-1}})(1.0\\,\\mathrm{s})}{3000\\,\\mathrm{m}} = \\frac{50}{3000} = \\frac{1}{60}$$\n\n2.  **External Gravity Waves:** These long, horizontally propagating waves are a feature of a fluid with a free surface or a sharp density interface under gravity. In atmospheric models, the \"equivalent depth\" $H$ represents the effective depth of the atmosphere for these large-scale waves. The problem specifies the shallow-water phase speed approximation, $c_g = \\sqrt{gH}$, where $g = 9.81\\,\\mathrm{m\\,s^{-2}}$ and $H = 10000\\,\\mathrm{m}$. This gives the characteristic speed:\n    $$c_g = \\sqrt{(9.81\\,\\mathrm{m\\,s^{-2}})(10000\\,\\mathrm{m})} = \\sqrt{98100}\\,\\mathrm{m\\,s^{-1}}$$\n    Since these waves propagate horizontally, the relevant grid spacing is $\\Delta x = 3000\\,\\mathrm{m}$. The Courant number for external gravity waves, $C_g$, is:\n    $$C_g = \\frac{c_g \\Delta t}{\\Delta x} = \\frac{\\sqrt{98100}\\,\\mathrm{m\\,s^{-1}} \\times 1.0\\,\\mathrm{s}}{3000\\,\\mathrm{m}} = \\frac{\\sqrt{98100}}{3000}$$\n\n3.  **Vertical Acoustic Waves:** The model formulation is \"fully compressible,\" meaning it resolves sound waves, which propagate at the speed of sound, $c$. The problem asks to consider vertically propagating acoustic waves. The characteristic speed is the speed of sound, $c = 340\\,\\mathrm{m\\,s^{-1}}$, and the relevant grid spacing is the vertical spacing, $\\Delta z = 100\\,\\mathrm{m}$. The Courant number for vertical acoustic waves, $C_{ac,z}$, is:\n    $$C_{ac,z} = \\frac{c \\Delta t}{\\Delta z}$$\n    Substituting the given values:\n    $$C_{ac,z} = \\frac{(340\\,\\mathrm{m\\,s^{-1}})(1.0\\,\\mathrm{s})}{100\\,\\mathrm{m}} = \\frac{340}{100} = 3.4$$\n\nNow, we compute the numerical values for comparison:\n- $C_{adv} = \\frac{1}{60} \\approx 0.016666...$\n- $C_g = \\frac{\\sqrt{98100}}{3000} \\approx \\frac{313.20919...}{3000} \\approx 0.104403...$\n- $C_{ac,z} = 3.4$\n\nThe CFL condition requires that the time step $\\Delta t$ be small enough to satisfy the stability criterion for all resolved phenomena simultaneously. This means $\\Delta t$ must be constrained by the most restrictive process, which is the one that yields the largest Courant number for a given $\\Delta t$. Comparing the three computed values:\n$$\\max(C_{adv}, C_g, C_{ac,z}) = \\max(0.0166..., 0.1044..., 3.4) = 3.4$$\nThe largest Courant number is $C_{ac,z} = 3.4$. This value is significantly greater than $1$, indicating that the proposed time step of $\\Delta t = 1.0\\,\\mathrm{s}$ would lead to catastrophic numerical instability in the model.\n\nThe most restrictive process for the given grid and parameters is the vertical propagation of acoustic waves. The combination of a high propagation speed ($c = 340\\,\\mathrm{m\\,s^{-1}}$) and a fine grid resolution in the vertical ($\\Delta z = 100\\,\\mathrm{m}$) imposes the most stringent limit on the time step. To maintain stability (assuming $C_{max}=1$), the time step would need to be reduced to at most $\\Delta t \\le \\frac{C_{max} \\Delta z}{c} = \\frac{1 \\times 100\\,\\mathrm{m}}{340\\,\\mathrm{m\\,s^{-1}}} \\approx 0.29\\,\\mathrm{s}$. This is a common challenge in nonhydrostatic models, often addressed with time-splitting techniques where fast-moving waves are integrated with a shorter time step.\n\nThe problem asks for the maximum among the three Courant numbers, rounded to four significant figures. The maximum value is exactly $3.4$, which, when expressed to four significant figures, is $3.400$.",
            "answer": "$$\n\\boxed{3.400}\n$$"
        },
        {
            "introduction": "Having grasped the scale of Global Cloud-Resolving Models and the constraints on their numerical design, we turn to the practical challenge of running them efficiently on thousands of processors. The distribution of clouds and storms is highly uneven across the globe, meaning some processors will have much more work to do than others if the domain is partitioned statically. This exercise explores the critical concept of load imbalance and tasks you with modeling a dynamic work-stealing strategy to overcome it, quantifying the potential gains in parallel efficiency .",
            "id": "4049755",
            "problem": "A global cloud-resolving model integrates nonhydrostatic dynamics and bulk microphysics on a quasi-uniform spherical grid. Consider a single time step dominated by column physics (microphysics and turbulence) that are computed independently per column. Let the computation use Message Passing Interface (MPI) over $P$ ranks with static, contiguous partitioning of columns from the cubed-sphere faces. Each rank processes $K$ columns. For a given meteorological situation with heterogeneous cloud activity, assume two regimes:\n- A storm band covers a subset of ranks with a high cloudy-column fraction. Specifically, $S$ ranks have cloudy fraction $p_H$, and each column there costs $t_H$ if cloudy and $t_L$ if clear.\n- The remaining $P-S$ ranks have cloudy fraction $p_L$ with the same per-column costs.\n\nAssume per-column costs are deterministic and independent across columns within each regime and that all ranks execute columns sequentially. The parallel time for the time step is the makespan, i.e., the maximum across ranks of their total column compute time in the step. Let the total work be the sum of all column costs. The parallel efficiency is defined as $E = T_{\\mathrm{serial}}/(P T_{\\mathrm{parallel}})$, where $T_{\\mathrm{serial}}$ is the total work executed by one processor.\n\nYou are asked to construct a load-balancing strategy that accounts for the heterogeneous cloud activity by using dynamic work stealing with task-based parallelism, and to estimate the efficiency gain relative to static partitioning. Use the following scientifically plausible parameters for a global cloud-resolving configuration:\n- $P = 64$, $K = 5000$.\n- $t_L = 1\\,\\mathrm{ms}$, $t_H = 5\\,\\mathrm{ms}$.\n- $S = 16$, $p_H = 0.6$, $p_L = 0.1$.\n\nFor dynamic work stealing, let tasks be chunks of $g$ columns taken from ranks in the storm band (victims). Each steal incurs overhead $o = 10\\,\\mathrm{ms}$ at the stealing rank (amortizing state transfer, queue management, and remote work requests). Assume $L = P-S = 48$ ranks act as thieves and share steals evenly. Introduce a global synchronization overhead $b = 50\\,\\mathrm{ms}$ associated with task pool finalization. For first-order estimation, model the steal count as the surplus storm-band work divided by the per-task time, and model fragmentation due to finite $g$ as a residual time $\\delta(g) \\approx (g/2)\\,t_s$, where $t_s = p_H t_H + (1-p_H)t_L$ is the average per-column time in the storm band.\n\nUnder these assumptions, select the option that proposes a sound work-stealing strategy and reports a correct first-order estimate of the parallel efficiency improvement relative to static partitioning.\n\nA. Use task-based parallelism with dynamic work stealing at chunk size $g = 50$; estimate $E_{\\mathrm{static}} \\approx 0.559$ and $E_{\\mathrm{ws}} \\approx 0.971$, for an efficiency gain of approximately $0.412$.\n\nB. Use task-based parallelism with dynamic work stealing at chunk size $g = 10$; estimate $E_{\\mathrm{static}} \\approx 0.559$ and $E_{\\mathrm{ws}} \\approx 0.990$, for an efficiency gain of approximately $0.431$.\n\nC. Use task-based parallelism with dynamic work stealing at chunk size $g = 500$; estimate $E_{\\mathrm{static}} \\approx 0.559$ and $E_{\\mathrm{ws}} \\approx 0.912$, for an efficiency gain of approximately $0.353$.\n\nD. Retain static partitioning but randomize rank-to-region assignment at each step to decorrelate load; estimate $E_{\\mathrm{static}} \\approx 0.900$ and no gain from dynamic work stealing is needed.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\nThe problem provides the following parameters and definitions for a parallel computation scenario in a global cloud-resolving model:\n- Total number of MPI ranks: $P = 64$\n- Number of columns per rank: $K = 5000$\n- Number of ranks in the storm band (high activity): $S = 16$\n- Number of ranks in the low-activity region: $L = P - S = 48$\n- Cloudy column fraction in the storm band: $p_H = 0.6$\n- Cloudy column fraction in the low-activity region: $p_L = 0.1$\n- Computational cost for a cloudy column: $t_H = 5\\,\\mathrm{ms}$\n- Computational cost for a clear-sky column: $t_L = 1\\,\\mathrm{ms}$\n- Parallelism model: Static partitioning of columns, with work stealing as a load-balancing strategy.\n- Parallel time definition: Makespan, the maximum time across all ranks.\n- Parallel efficiency definition: $E = T_{\\mathrm{serial}}/(P T_{\\mathrm{parallel}})$, where $T_{\\mathrm{serial}}$ is the total work.\n\nFor the dynamic work-stealing model:\n- Task chunk size: $g$ columns\n- Steal overhead per task: $o = 10\\,\\mathrm{ms}$\n- Global synchronization overhead: $b = 50\\,\\mathrm{ms}$\n- Fragmentation residual time model: $\\delta(g) \\approx (g/2)\\,t_s$, where $t_s$ is the average per-column time in the storm band.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in performance modeling for high-performance computing, applied to the domain of numerical weather prediction.\n-   **Scientifically Grounded:** The setup is a simplified but realistic representation of load imbalance in climate models. Cloud microphysics computations are indeed a major source of heterogeneous computational load, and their cost varies significantly between cloudy and clear regions. MPI, static partitioning, and work stealing are standard concepts in parallel computing. The parameter values are plausible for a coarse-grained simulation on a moderate number of processors. The models for overheads (steal cost, fragmentation, synchronization) are standard first-order approximations used in performance analysis.\n-   **Well-Posed:** The problem provides all necessary parameters and a clear set of definitions and simplified models to arrive at a unique quantitative answer for the parallel efficiencies under the given assumptions.\n-   **Objective:** The language is technical and precise. The question asks for a quantitative estimation based on a provided model, leaving no room for subjective interpretation.\n\nThe problem does not violate any of the invalidity criteria. It is a formalizable, scientifically relevant, and internally consistent problem.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the solution.\n\n---\n\n## Derivation of Solution\n\nThe objective is to calculate the parallel efficiency for two scenarios: static partitioning and dynamic work stealing, and then evaluate the given options.\n\n### 1. Analysis of Static Partitioning\n\nFirst, we calculate the expected computational time (workload) for a single rank in each of the two regimes.\n\nThe average computational cost per column in the storm band is:\n$$t_s = p_H t_H + (1-p_H)t_L = (0.6)(5\\,\\mathrm{ms}) + (1-0.6)(1\\,\\mathrm{ms}) = 3\\,\\mathrm{ms} + 0.4\\,\\mathrm{ms} = 3.4\\,\\mathrm{ms}$$\n\nThe average computational cost per column in the low-activity region is:\n$$t_l = p_L t_H + (1-p_L)t_L = (0.1)(5\\,\\mathrm{ms}) + (1-0.1)(1\\,\\mathrm{ms}) = 0.5\\,\\mathrm{ms} + 0.9\\,\\mathrm{ms} = 1.4\\,\\mathrm{ms}$$\n\nEach rank processes $K=5000$ columns. The total time for a rank in the storm band is:\n$$T_H = K \\cdot t_s = 5000 \\cdot 3.4\\,\\mathrm{ms} = 17000\\,\\mathrm{ms}$$\n\nThe total time for a rank in the low-activity region is:\n$$T_L = K \\cdot t_l = 5000 \\cdot 1.4\\,\\mathrm{ms} = 7000\\,\\mathrm{ms}$$\n\nUnder static partitioning, the parallel time for the step ($T_{\\mathrm{parallel, static}}$) is the makespan, which is the maximum of these individual rank times:\n$$T_{\\mathrm{parallel, static}} = \\max(T_H, T_L) = 17000\\,\\mathrm{ms}$$\n\nNext, we calculate the total serial work ($T_{\\mathrm{serial}}$), which is the sum of work across all ranks:\n$$T_{\\mathrm{serial}} = S \\cdot T_H + (P-S) \\cdot T_L = 16 \\cdot 17000\\,\\mathrm{ms} + 48 \\cdot 7000\\,\\mathrm{ms}$$\n$$T_{\\mathrm{serial}} = 272000\\,\\mathrm{ms} + 336000\\,\\mathrm{ms} = 608000\\,\\mathrm{ms}$$\n\nThe parallel efficiency for the static case is:\n$$E_{\\mathrm{static}} = \\frac{T_{\\mathrm{serial}}}{P \\cdot T_{\\mathrm{parallel, static}}} = \\frac{608000}{64 \\cdot 17000} = \\frac{608000}{1088000} = \\frac{19}{34} \\approx 0.55882$$\nRounding to three decimal places, $E_{\\mathrm{static}} \\approx 0.559$.\n\n### 2. Analysis of Dynamic Work Stealing\n\nThe goal of work stealing is to balance the load, making each rank's total execution time converge towards an ideal time, $T_{\\mathrm{ideal}}$.\n$$T_{\\mathrm{ideal}} = \\frac{T_{\\mathrm{serial}}}{P} = \\frac{608000\\,\\mathrm{ms}}{64} = 9500\\,\\mathrm{ms}$$\n\nThe surplus work from the high-load ranks (victims) that needs to be offloaded to the low-load ranks (thieves) is:\n$$W_{\\mathrm{surplus}} = S \\cdot (T_H - T_{\\mathrm{ideal}}) = 16 \\cdot (17000\\,\\mathrm{ms} - 9500\\,\\mathrm{ms}) = 16 \\cdot 7500\\,\\mathrm{ms} = 120000\\,\\mathrm{ms}$$\n\nThis work is transferred in tasks of size $g$. The average time to execute one such task is:\n$$T_{\\mathrm{task}}(g) = g \\cdot t_s = g \\cdot 3.4\\,\\mathrm{ms}$$\n\nThe total number of steals is estimated as:\n$$N_{\\mathrm{steals}}(g) = \\frac{W_{\\mathrm{surplus}}}{T_{\\mathrm{task}}(g)} = \\frac{120000}{g \\cdot 3.4}$$\n\nThe makespan under work stealing ($T_{\\mathrm{parallel, ws}}$) is modeled as the ideal time plus several overheads:\n1.  **Steal Overhead**: The total steal overhead is $N_{\\mathrm{steals}}(g) \\cdot o$. This is distributed among the $L=48$ thieves. The overhead added to the makespan contribution from the thieves is $\\frac{N_{\\mathrm{steals}}(g) \\cdot o}{L}$.\n2.  **Fragmentation Overhead**: An amount $\\delta(g) = (g/2) \\cdot t_s$ is added to the makespan.\n3.  **Synchronization Overhead**: A final global barrier cost $b$ is added.\n\nThe model for the parallel time is:\n$$T_{\\mathrm{parallel, ws}}(g) = T_{\\mathrm{ideal}} + \\frac{N_{\\mathrm{steals}}(g) \\cdot o}{L} + \\delta(g) + b$$\n\nSubstituting the expressions for $N_{\\mathrm{steals}}(g)$ and $\\delta(g)$:\n$$T_{\\mathrm{parallel, ws}}(g) = 9500 + \\frac{120000}{g \\cdot 3.4} \\cdot \\frac{10}{48} + \\frac{g \\cdot 3.4}{2} + 50$$\n\nWe now evaluate this for the chunk sizes proposed in the options.\n\n#### Case: $g=50$ (from Option A)\n$$T_{\\mathrm{parallel, ws}}(50) = 9500 + \\frac{120000}{50 \\cdot 3.4} \\cdot \\frac{10}{48} + \\frac{50 \\cdot 3.4}{2} + 50$$\n$$T_{\\mathrm{parallel, ws}}(50) = 9500 + \\frac{120000}{170} \\cdot \\frac{10}{48} + \\frac{170}{2} + 50$$\n$$T_{\\mathrm{parallel, ws}}(50) \\approx 9500 + (705.88) \\cdot (0.20833) + 85 + 50$$\n$$T_{\\mathrm{parallel, ws}}(50) \\approx 9500 + 147.06 + 85 + 50 = 9782.06\\,\\mathrm{ms}$$\n\nThe efficiency with work stealing for $g=50$ is:\n$$E_{\\mathrm{ws}}(50) = \\frac{T_{\\mathrm{serial}}}{P \\cdot T_{\\mathrm{parallel, ws}}(50)} = \\frac{608000}{64 \\cdot 9782.06} = \\frac{608000}{626051.84} \\approx 0.97116$$\nThe efficiency gain is $E_{\\mathrm{ws}}(50) - E_{\\mathrm{static}} \\approx 0.971 - 0.559 = 0.412$.\n\n#### Case: $g=10$ (from Option B)\n$$T_{\\mathrm{parallel, ws}}(10) = 9500 + \\frac{120000}{10 \\cdot 3.4} \\cdot \\frac{10}{48} + \\frac{10 \\cdot 3.4}{2} + 50$$\n$$T_{\\mathrm{parallel, ws}}(10) = 9500 + \\frac{120000}{34} \\cdot \\frac{10}{48} + 17 + 50$$\n$$T_{\\mathrm{parallel, ws}}(10) \\approx 9500 + (3529.41) \\cdot (0.20833) + 17 + 50$$\n$$T_{\\mathrm{parallel, ws}}(10) \\approx 9500 + 735.29 + 17 + 50 = 10302.29\\,\\mathrm{ms}$$\n\nThe efficiency with work stealing for $g=10$ is:\n$$E_{\\mathrm{ws}}(10) = \\frac{T_{\\mathrm{serial}}}{P \\cdot T_{\\mathrm{parallel, ws}}(10)} = \\frac{608000}{64 \\cdot 10302.29} = \\frac{608000}{659346.56} \\approx 0.9221$$\n\n#### Case: $g=500$ (from Option C)\n$$T_{\\mathrm{parallel, ws}}(500) = 9500 + \\frac{120000}{500 \\cdot 3.4} \\cdot \\frac{10}{48} + \\frac{500 \\cdot 3.4}{2} + 50$$\n$$T_{\\mathrm{parallel, ws}}(500) = 9500 + \\frac{120000}{1700} \\cdot \\frac{10}{48} + 850 + 50$$\n$$T_{\\mathrm{parallel, ws}}(500) \\approx 9500 + (70.588) \\cdot (0.20833) + 850 + 50$$\n$$T_{\\mathrm{parallel, ws}}(500) \\approx 9500 + 14.71 + 850 + 50 = 10414.71\\,\\mathrm{ms}$$\n\nThe efficiency with work stealing for $g=500$ is:\n$$E_{\\mathrm{ws}}(500) = \\frac{T_{\\mathrm{serial}}}{P \\cdot T_{\\mathrm{parallel, ws}}(500)} = \\frac{608000}{64 \\cdot 10414.71} = \\frac{608000}{666541.44} \\approx 0.9122$$\nThe efficiency gain is $E_{\\mathrm{ws}}(500) - E_{\\mathrm{static}} \\approx 0.912 - 0.559 = 0.353$.\n\n## Option-by-Option Analysis\n\n**A. Use task-based parallelism with dynamic work stealing at chunk size $g = 50$; estimate $E_{\\mathrm{static}} \\approx 0.559$ and $E_{\\mathrm{ws}} \\approx 0.971$, for an efficiency gain of approximately $0.412$.**\nOur calculation yields $E_{\\mathrm{static}} \\approx 0.559$. For $g=50$, our calculation yields $E_{\\mathrm{ws}} \\approx 0.971$ and a gain of $\\approx 0.412$. The estimates reported in this option are correct according to the provided model. The choice of $g=50$ results in a high efficiency by balancing the steal overhead (which decreases with $g$) and the fragmentation overhead (which increases with $g$). This represents a sound application of the work-stealing strategy.\n**Verdict: Correct.**\n\n**B. Use task-based parallelism with dynamic work stealing at chunk size $g = 10$; estimate $E_{\\mathrm{static}} \\approx 0.559$ and $E_{\\mathrm{ws}} \\approx 0.990$, for an efficiency gain of approximately $0.431$.**\nThe static efficiency estimate is correct. However, the estimated work-stealing efficiency $E_{\\mathrm{ws}} \\approx 0.990$ is incorrect. Our calculation for $g=10$ shows that the large number of steals leads to a very high steal overhead ($ \\approx 735\\,\\mathrm{ms}$), resulting in a much lower efficiency of $E_{\\mathrm{ws}} \\approx 0.922$. The option's estimate is unrealistically optimistic.\n**Verdict: Incorrect.**\n\n**C. Use task-based parallelism with dynamic work stealing at chunk size $g = 500$; estimate $E_{\\mathrm{static}} \\approx 0.559$ and $E_{\\mathrm{ws}} \\approx 0.912$, for an efficiency gain of approximately $0.353$.**\nThe static efficiency estimate is correct. Our calculation for $g=500$ confirms that $E_{\\mathrm{ws}} \\approx 0.912$ and the gain is $\\approx 0.353$. While the calculation is correct, the proposed strategy is not as \"sound\" as the one in Option A. The large chunk size $g=500$ incurs a very large fragmentation overhead ($\\delta(500) = 850\\,\\mathrm{ms}$), significantly limiting the achievable efficiency compared to the $g=50$ case ($E_{\\mathrm{ws}} \\approx 0.912$ vs $E_{\\mathrm{ws}} \\approx 0.971$). A \"sound\" strategy should represent a good choice of parameters. While arithmetically correct, this option proposes a suboptimal strategy. Given that Option A is both arithmetically correct and proposes a better strategy, it is the superior choice.\n**Verdict: Incorrect.** (The reported numbers are correct, but the proposed strategy is inferior to that in A, making A the better answer to the prompt which asks for a \"sound strategy\").\n\n**D. Retain static partitioning but randomize rank-to-region assignment at each step to decorrelate load; estimate $E_{\\mathrm{static}} \\approx 0.900$ and no gain from dynamic work stealing is needed.**\nThis option misinterprets the problem. For a single time step, the storm band is geographically fixed. Randomizing which group of ranks covers this region does not alter the load imbalance; there will still be $S=16$ heavily loaded ranks and $P-S=48$ lightly loaded ranks. The makespan remains $T_H = 17000\\,\\mathrm{ms}$ and the efficiency is still $E_{\\mathrm{static}} \\approx 0.559$. The claim of $E_{\\mathrm{static}} \\approx 0.900$ for this setup is factually incorrect. This strategy of randomization might balance load over many time steps if the storm band moves, but it does nothing to solve the load imbalance within this single, given time step.\n**Verdict: Incorrect.**\n\nBased on the analysis, Option A presents both a correct calculation of the efficiency gain and proposes a strategy ($g=50$) that is demonstrably more effective (\"sounder\") than the other options.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}