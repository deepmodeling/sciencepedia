{
    "hands_on_practices": [
        {
            "introduction": "The first step in parallelizing any grid-based model is to divide the computational domain among the available processors. This exercise explores the mechanics of a standard two-dimensional Cartesian decomposition, a cornerstone of parallel weather and climate models. By working through the distribution of grid cells, you will gain hands-on experience with how remainders from integer division lead to unavoidable static load imbalance and learn to quantify its magnitude. ",
            "id": "4032181",
            "problem": "A global atmospheric model for numerical weather prediction is discretized on a uniform latitude–longitude grid with horizontal dimensions $N_x$ by $N_y$, where $N_x$ is the number of longitudes and $N_y$ is the number of latitudes. The model advances prognostic variables using a finite-volume scheme with uniform computational cost per horizontal grid cell, so the total computational cost is proportional to the number of grid cells assigned to a rank.\n\nTo parallelize the model, a two-dimensional Cartesian Message Passing Interface (MPI) domain decomposition is used, partitioning the horizontal grid into $p_x$ blocks along the $x$-direction and $p_y$ blocks along the $y$-direction, with one MPI rank per subdomain. You are given $N_x = 1536$, $N_y = 768$, and a total of $p = 128$ MPI ranks. For architectural symmetry reasons, the project manager requires $p_x = p_y$ in the horizontal decomposition. The partitioning must adhere to the following widely used block distribution rule to preserve load balance under uniform per-cell cost:\n- Along each dimension $\\alpha \\in \\{x,y\\}$, let $q_{\\alpha} = \\lfloor N_{\\alpha} / p_{\\alpha} \\rfloor$ and $r_{\\alpha} = N_{\\alpha} \\bmod p_{\\alpha}$. The first $r_{\\alpha}$ blocks along dimension $\\alpha$ receive $q_{\\alpha} + 1$ cells, and the remaining $p_{\\alpha} - r_{\\alpha}$ blocks receive $q_{\\alpha}$ cells.\n\nStarting from these definitions and constraints, determine:\n1. The largest feasible integers $p_x$ and $p_y$ such that $p_x = p_y$ and $p_x p_y \\le p$.\n2. The base block sizes $q_x$ and $q_y$, the larger sizes $q_x + 1$ and $q_y + 1$, and the remainders $r_x$ and $r_y$.\n3. The counts of ranks in each of the four local size categories: $(q_x + 1) \\times (q_y + 1)$, $(q_x + 1) \\times q_y$, $q_x \\times (q_y + 1)$, and $q_x \\times q_y$.\n4. The number of unused MPI ranks (idle ranks) resulting from the $p_x = p_y$ constraint.\n5. Under the assumption of uniform per-cell cost, the ratio of the maximum local workload to the minimum local workload across ranks.\n\nExpress your final answer as a row matrix using the LaTeX $\\mathrm{pmatrix}$ environment with the entries in the following order: $p_x$, $p_y$, $q_x$, $q_x + 1$, $q_y$, $q_y + 1$, $r_x r_y$, $r_x (p_y - r_y)$, $(p_x - r_x) r_y$, $(p_x - r_x)(p_y - r_y)$, $p - p_x p_y$, and the workload ratio $\\frac{(q_x + 1)(q_y + 1)}{q_x q_y}$. No rounding is required, and the entries are pure numbers without units.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of parallel computing and numerical methods, specifically domain decomposition for grid-based models. The problem is well-posed, providing all necessary data and constraints ($N_x = 1536$, $N_y = 768$, $p = 128$, $p_x = p_y$) to arrive at a unique, meaningful solution. The definitions and objectives are stated clearly and free of ambiguity or contradiction.\n\nWe will proceed with the solution by addressing each of the five parts of the problem in sequence.\n\n1. Determination of $p_x$ and $p_y$\n\nThe problem requires a two-dimensional Cartesian decomposition of the grid into $p_x \\times p_y$ subdomains. We are given the constraints $p_x = p_y$ and that the total number of utilized MPI ranks, $p_x p_y$, must not exceed the total available ranks, $p = 128$.\nSubstituting the first constraint into the second gives:\n$$p_x \\cdot p_x \\le p$$\n$$p_x^2 \\le 128$$\nWe seek the largest integer $p_x$ that satisfies this inequality. We take the square root of both sides:\n$$p_x \\le \\sqrt{128} = \\sqrt{64 \\times 2} = 8\\sqrt{2}$$\nUsing the approximation $\\sqrt{2} \\approx 1.414$, we find $p_x \\le 8 \\times 1.414 = 11.312$. The largest integer satisfying this condition is $p_x = 11$.\nDue to the constraint $p_x = p_y$, we have $p_y = 11$.\nThe total number of ranks used is $p_x p_y = 11 \\times 11 = 121$, which is indeed less than or equal to the available $128$ ranks.\n\n2. Calculation of Block Sizes and Remainders\n\nThe problem defines a block distribution rule based on integer division. For each dimension $\\alpha \\in \\{x,y\\}$, the base block size is $q_{\\alpha} = \\lfloor N_{\\alpha} / p_{\\alpha} \\rfloor$ and the remainder is $r_{\\alpha} = N_{\\alpha} \\bmod p_{\\alpha}$.\n\nFor the $x$-direction (longitude):\nGiven $N_x = 1536$ and $p_x = 11$.\n$$q_x = \\lfloor \\frac{1536}{11} \\rfloor = \\lfloor 139.636... \\rfloor = 139$$\n$$r_x = 1536 \\bmod 11$$\nTo find the remainder, we use the division algorithm: $1536 = 11 \\times 139 + r_x$.\n$11 \\times 139 = 1529$.\n$1536 = 1529 + 7$, so $r_x = 7$.\nThe block sizes in the $x$-direction are the base size $q_x = 139$ and the larger size $q_x + 1 = 140$.\n\nFor the $y$-direction (latitude):\nGiven $N_y = 768$ and $p_y = 11$.\n$$q_y = \\lfloor \\frac{768}{11} \\rfloor = \\lfloor 69.818... \\rfloor = 69$$\n$$r_y = 768 \\bmod 11$$\nUsing the division algorithm: $768 = 11 \\times 69 + r_y$.\n$11 \\times 69 = 759$.\n$768 = 759 + 9$, so $r_y = 9$.\nThe block sizes in the $y$-direction are the base size $q_y = 69$ and the larger size $q_y + 1 = 70$.\n\n3. Tallying Ranks by Local Size Category\n\nThe total $p_x \\times p_y$ domain is partitioned into four groups based on local grid size.\n- In the $x$-direction, the first $r_x = 7$ blocks have size $q_x+1$, and the remaining $p_x - r_x = 11 - 7 = 4$ blocks have size $q_x$.\n- In the $y$-direction, the first $r_y = 9$ blocks have size $q_y+1$, and the remaining $p_y - r_y = 11 - 9 = 2$ blocks have size $q_y$.\n\nThe number of ranks in each of the four size categories is:\n- Subdomains of size $(q_x + 1) \\times (q_y + 1)$: These correspond to the intersection of the first $r_x$ columns and first $r_y$ rows.\n  Number of ranks = $r_x \\times r_y = 7 \\times 9 = 63$.\n- Subdomains of size $(q_x + 1) \\times q_y$: These correspond to the first $r_x$ columns and the remaining $p_y - r_y$ rows.\n  Number of ranks = $r_x \\times (p_y - r_y) = 7 \\times 2 = 14$.\n- Subdomains of size $q_x \\times (q_y + 1)$: These correspond to the remaining $p_x - r_x$ columns and the first $r_y$ rows.\n  Number of ranks = $(p_x - r_x) \\times r_y = 4 \\times 9 = 36$.\n- Subdomains of size $q_x \\times q_y$: These correspond to the remaining $p_x - r_x$ columns and the remaining $p_y - r_y$ rows.\n  Number of ranks = $(p_x - r_x) \\times (p_y - r_y) = 4 \\times 2 = 8$.\nThe sum of ranks is $63 + 14 + 36 + 8 = 121$, which matches $p_x p_y$.\n\n4. Determination of Unused Ranks\n\nThe number of unused (idle) MPI ranks is the difference between the total available ranks and the total ranks utilized in the decomposition.\nNumber of idle ranks = $p - p_x p_y = 128 - 121 = 7$.\n\n5. Calculation of the Workload Imbalance Ratio\n\nAssuming uniform computational cost per grid cell, the workload of a rank is proportional to the number of grid cells in its subdomain. The workload imbalance is the ratio of the maximum to the minimum workload.\nThe four possible subdomain sizes (workloads) are:\n- $W_{max} = (q_x + 1) \\times (q_y + 1) = (139 + 1) \\times (69 + 1) = 140 \\times 70 = 9800$.\n- $W_2 = (q_x + 1) \\times q_y = 140 \\times 69 = 9660$.\n- $W_3 = q_x \\times (q_y + 1) = 139 \\times 70 = 9730$.\n- $W_{min} = q_x \\times q_y = 139 \\times 69 = 9591$.\nThe maximum workload is $9800$ and the minimum is $9591$. The problem asks for the ratio of the maximum to the minimum, which is $\\frac{W_{max}}{W_{min}}$.\n$$\\text{Ratio} = \\frac{(q_x + 1)(q_y + 1)}{q_x q_y} = \\frac{9800}{9591}$$\nThis fraction is irreducible, as the prime factorization of the numerator is $9800 = 2^3 \\times 5^2 \\times 7^2$ and the denominator is $9591 = 3 \\times 23 \\times 139$. There are no common prime factors.\n\nThe final results are assembled according to the specified order.\n- $p_x = 11$\n- $p_y = 11$\n- $q_x = 139$\n- $q_x+1 = 140$\n- $q_y = 69$\n- $q_y+1 = 70$\n- $r_x r_y = 63$\n- $r_x(p_y-r_y) = 14$\n- $(p_x-r_x)r_y = 36$\n- $(p_x-r_x)(p_y-r_y) = 8$\n- $p - p_x p_y = 7$\n- Workload ratio = $\\frac{9800}{9591}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11 & 11 & 139 & 140 & 69 & 70 & 63 & 14 & 36 & 8 & 7 & \\frac{9800}{9591}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In parallel programs operating under a Single Program, Multiple Data (SPMD) model, synchronization points like halo exchanges act as barriers where faster processors must wait for the slowest one. This inherent inefficiency, caused by load imbalance, directly translates into wasted computational resources and reduced scalability. This practice will guide you to derive a precise relationship between the workload distribution and the fraction of processor cycles lost to idle time at these critical synchronization barriers. ",
            "id": "4032234",
            "problem": "A hydrostatic primitive-equation core for Numerical Weather Prediction (NWP) is executed in a Single Program, Multiple Data (SPMD) model over Message Passing Interface (MPI) ranks. The horizontal domain is decomposed into six subdomains whose per-timestep wall-clock times reflect realistic heterogeneity due to polar filtering and orography-triggered physics. Denote by $t_i$ the wall-clock time per timestep of rank $i$, and let the timestep end with a halo exchange that imposes barrier synchronization semantics. Measured per-timestep times are\n$$t_1 = 17.8~\\mathrm{ms},\\quad t_2 = 18.2~\\mathrm{ms},\\quad t_3 = 19.5~\\mathrm{ms},\\quad t_4 = 18.0~\\mathrm{ms},\\quad t_5 = 23.7~\\mathrm{ms},\\quad t_6 = 18.9~\\mathrm{ms}.$$\nStarting from first principles of barrier synchronization and parallel execution time in SPMD, define a load-imbalance factor that compares the slowest rank to the average rank. Then, derive a closed-form analytic expression relating this imbalance to the fraction of processor cycles that are wasted at the synchronization barrier each timestep. Using the given data, compute that fraction.\n\nExpress the final fraction as a decimal with no units and round your answer to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of parallel computing, well-posed with sufficient data, and objectively stated.\n\nThe problem asks for a derivation of the fraction of wasted processor cycles due to load imbalance in a Single Program, Multiple Data (SPMD) parallel execution model with barrier synchronization. We are given the per-timestep computational wall-clock times for $N=6$ Message Passing Interface (MPI) ranks.\n\nLet $N$ be the number of MPI ranks. In this problem, $N=6$.\nLet $t_i$ be the wall-clock time required for rank $i$ to complete its computational tasks within a single timestep, for $i \\in \\{1, 2, \\dots, N\\}$.\n\nThe execution model employs barrier synchronization at the end of each timestep. This means that all ranks must wait at the barrier until the last (slowest) rank has finished its computation. Therefore, the total wall-clock time for one complete timestep, denoted $T_{\\text{step}}$, is determined by the maximum of the individual rank times:\n$$T_{\\text{step}} = \\max_{i=1, \\dots, N} \\{t_i\\}$$\n\nDuring this time $T_{\\text{step}}$, a total of $N$ processors are allocated to the task. The total available processor time (or total processor cycles allocated) for one timestep is the sum of the time each processor is reserved, which is $N$ multiplied by $T_{\\text{step}}$.\n$$T_{\\text{total\\_alloc}} = N \\cdot T_{\\text{step}} = N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}$$\n\nThe amount of time spent performing useful computations across all ranks is the sum of the individual computational times:\n$$T_{\\text{useful}} = \\sum_{i=1}^{N} t_i$$\n\nThe time that is wasted is spent by the faster ranks idling at the synchronization barrier. This wasted time is the difference between the total allocated processor time and the useful computational time:\n$$T_{\\text{wasted}} = T_{\\text{total\\_alloc}} - T_{\\text{useful}} = N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\} - \\sum_{i=1}^{N} t_i$$\n\nThe fraction of processor cycles that are wasted, denoted $f_{\\text{waste}}$, is the ratio of the wasted time to the total allocated time:\n$$f_{\\text{waste}} = \\frac{T_{\\text{wasted}}}{T_{\\text{total\\_alloc}}} = \\frac{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\} - \\sum_{i=1}^{N} t_i}{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}} = 1 - \\frac{\\sum_{i=1}^{N} t_i}{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}}$$\n\nThe problem asks to first define a load-imbalance factor, $\\mathcal{L}$, that compares the slowest rank to the average rank. Let $t_{\\text{slowest}}$ be the time of the slowest rank and $t_{\\text{avg}}$ be the average time across all ranks.\n$$t_{\\text{slowest}} = \\max_{i=1, \\dots, N} \\{t_i\\}$$\n$$t_{\\text{avg}} = \\frac{1}{N} \\sum_{i=1}^{N} t_i$$\nA natural definition for the load-imbalance factor is the ratio of these two quantities:\n$$\\mathcal{L} = \\frac{t_{\\text{slowest}}}{t_{\\text{avg}}} = \\frac{\\max_{i=1, \\dots, N} \\{t_i\\}}{\\frac{1}{N} \\sum_{i=1}^{N} t_i}$$\n\nNow, we derive the relationship between $f_{\\text{waste}}$ and $\\mathcal{L}$. From the definition of $t_{\\text{avg}}$, we have $\\sum_{i=1}^{N} t_i = N \\cdot t_{\\text{avg}}$. Substituting this into the expression for $f_{\\text{waste}}$:\n$$f_{\\text{waste}} = 1 - \\frac{N \\cdot t_{\\text{avg}}}{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}} = 1 - \\frac{t_{\\text{avg}}}{\\max_{i=1, \\dots, N} \\{t_i\\}}$$\nRecognizing that the term $\\frac{t_{\\text{avg}}}{\\max_{i=1, \\dots, N} \\{t_i\\}}$ is the reciprocal of the load-imbalance factor $\\mathcal{L}$, we arrive at the closed-form analytic expression:\n$$f_{\\text{waste}} = 1 - \\frac{1}{\\mathcal{L}}$$\n\nFinally, we compute the numerical value of $f_{\\text{waste}}$ using the provided data:\n$$t_1 = 17.8~\\mathrm{ms}, \\quad t_2 = 18.2~\\mathrm{ms}, \\quad t_3 = 19.5~\\mathrm{ms}, \\quad t_4 = 18.0~\\mathrm{ms}, \\quad t_5 = 23.7~\\mathrm{ms}, \\quad t_6 = 18.9~\\mathrm{ms}$$\nThe number of ranks is $N=6$.\n\nFirst, find the time of the slowest rank:\n$$t_{\\text{slowest}} = \\max\\{17.8, 18.2, 19.5, 18.0, 23.7, 18.9\\} = 23.7~\\mathrm{ms}$$\n\nNext, compute the sum of the times:\n$$\\sum_{i=1}^{6} t_i = 17.8 + 18.2 + 19.5 + 18.0 + 23.7 + 18.9 = 116.1~\\mathrm{ms}$$\n\nNow, we can compute the fraction of wasted cycles directly. The units of milliseconds will cancel.\n$$f_{\\text{waste}} = 1 - \\frac{\\sum_{i=1}^{6} t_i}{6 \\cdot t_{\\text{slowest}}} = 1 - \\frac{116.1}{6 \\cdot 23.7}$$\n$$f_{\\text{waste}} = 1 - \\frac{116.1}{142.2}$$\n$$f_{\\text{waste}} \\approx 1 - 0.816455696...$$\n$$f_{\\text{waste}} \\approx 0.183544303...$$\nRounding the result to four significant figures gives $0.1835$.",
            "answer": "$$\\boxed{0.1835}$$"
        },
        {
            "introduction": "Static domain decomposition is effective when computational cost is uniform, but real-world atmospheric models feature heterogeneous workloads due to factors like orographic effects or complex cloud microphysics. To maintain efficiency, models must employ dynamic load balancing strategies that adapt to these variations. This practice challenges you to implement a sophisticated, heuristic-based algorithm that uses historical timing data to intelligently bundle computational tasks, providing a practical foundation for designing state-of-the-art adaptive load balancing systems. ",
            "id": "4032164",
            "problem": "You are given a rectangular horizontal grid used in Numerical Weather Prediction (NWP) and climate modeling, where physics parameterizations operate column-wise, and columns can be bundled into tasks to be executed sequentially on identical processing units. Suppose there are $N$ columns indexed by $i\\in\\{1,\\dots,N\\}$ and you possess $H$ recent measurements of their per-column wall-clock times in seconds, denoted $t_i^{(s)}$ for $s\\in\\{1,\\dots,H\\}$. Some measurements may be unavailable and are represented as missing values. The goal is to derive and implement a domain decomposition and load balancing scheme that bundles columns into $P$ tasks such that the sum of per-column costs in each task is approximately equal, using histogramming of recent per-column times to drive a bucketed decreasing-time greedy assignment.\n\nFundamental base and objective: Let the per-column computational cost estimate be a nonnegative scalar $c_i$ in seconds. The total load is $L=\\sum_{i=1}^N c_i$, and the ideal target per-task load is $L^\\star=\\frac{L}{P}$. The objective is to construct a partition of the set of columns into $P$ disjoint bundles that minimizes the maximum task load, equivalently seeks to make each task’s total load as close as possible to $L^\\star$.\n\nRequirements for estimation and binning:\n- Cost estimation from recent times: For each column $i$, compute an exponentially weighted moving average (EWMA) of its recent times to obtain $c_i$. Define $c_i^{(1)}=t_i^{(1)}$ if $t_i^{(1)}$ is available; otherwise, skip to the next available sample. For $s\\ge 2$, update by $c_i^{(s)}=\\alpha\\,t_i^{(s)}+(1-\\alpha)\\,c_i^{(s-1)}$ whenever $t_i^{(s)}$ is available; if $t_i^{(s)}$ is missing, set $c_i^{(s)}=c_i^{(s-1)}$. If all $t_i^{(s)}$ are missing, set $c_i=0$. Otherwise, set $c_i=c_i^{(H)}$. The parameter $\\alpha$ satisfies $0<\\alpha<1$.\n- Histogram binning: Let $B$ be the number of bins. Let $m=\\min_i c_i$ and $M=\\max_i c_i$. If $M>m$, define $B+1$ linearly spaced edges $e_b=m+\\frac{b}{B}(M-m)$ for $b\\in\\{0,1,\\dots,B\\}$. For each column $i$, assign it to a bin index $b_i=\\min\\left\\{B-1,\\left\\lfloor \\frac{c_i-m}{M-m}\\,B\\right\\rfloor\\right\\}$. If $M=m$, assign all columns to bin index $B-1$. This produces a histogram of columns by cost.\n- Bucketed greedy assignment: Initialize task loads $\\ell_j=0$ for $j\\in\\{1,\\dots,P\\}$. Iterate bins in descending order $b=B-1,B-2,\\dots,0$. Within each bin, traverse its columns (arbitrary order) and assign each column $i$ to the task $j^\\star$ with the currently minimum load, i.e., $j^\\star=\\arg\\min_{j\\in\\{1,\\dots,P\\}} \\ell_j$, then update $\\ell_{j^\\star}\\leftarrow \\ell_{j^\\star}+c_i$. The resulting bundles constitute the balanced decomposition.\n\nYour program must implement the above steps and output, for each test case, the maximum final task load $\\max_{j}\\ell_j$ in seconds as a floating-point number.\n\nUnits and numerical outputs:\n- All time quantities $t_i^{(s)}$, $c_i$, $L$, $L^\\star$, and task loads $\\ell_j$ are in seconds.\n- The outputs must be the maximum task load in seconds for each test case, expressed as floating-point numbers.\n\nTest suite and parameter specification:\nImplement the algorithm for the following test cases. In each case, construct the matrix of recent times using the specified random seeds and distribution parameters. Any randomness must be reproducible by a fixed pseudorandom number generator with the given seed.\n\n- Case $1$ (happy path):\n  - Columns: $N=16$, tasks: $P=4$, history length: $H=5$, histogram bins: $B=8$, EWMA parameter: $\\alpha=0.6$.\n  - Generate recent times $t_i^{(s)}$ by drawing from a lognormal distribution with underlying normal mean $\\mu=\\ln(0.01)$ and standard deviation $\\sigma=0.5$, in seconds, independently for each $i$ and $s$.\n  - Use pseudorandom seed $12345$.\n- Case $2$ (uniform costs boundary):\n  - Columns: $N=12$, tasks: $P=3$, history length: $H=4$, histogram bins: $B=4$, EWMA parameter: $\\alpha=0.5$.\n  - Set all recent times deterministically to $t_i^{(s)}=0.015$ seconds for every $i$ and $s$ (no randomness).\n- Case $3$ (heavy-tail with one outlier):\n  - Columns: $N=20$, tasks: $P=5$, history length: $H=5$, histogram bins: $B=10$, EWMA parameter: $\\alpha=0.7$.\n  - Generate recent times for $N-1$ columns from a lognormal with $\\mu=\\ln(0.008)$ and $\\sigma=0.6$, in seconds, using pseudorandom seed $2021$.\n  - For one distinguished column, set all recent times deterministically to $t_i^{(s)}=0.2$ seconds for every $s$.\n- Case $4$ (fewer columns than tasks with missing data):\n  - Columns: $N=3$, tasks: $P=5$, history length: $H=6$, histogram bins: $B=5$, EWMA parameter: $\\alpha=0.8$.\n  - Set recent times as follows (in seconds): column $1$ has $[0.02,\\text{missing},0.015,\\text{missing},0.017,0.018]$, column $2$ has $[0,0,0,0,0,0]$, column $3$ has all values missing.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$, for example, $[x_1,x_2,x_3,x_4]$, where each $x_k$ is the maximum task load in seconds for case $k$ as defined above.",
            "solution": "The user-provided problem statement is valid. It is scientifically grounded in the fields of numerical modeling and high-performance computing, specifically addressing the challenge of load balancing. The problem is well-posed, with all parameters, algorithms, and objective functions clearly and objectively defined. The setup is self-contained, consistent, and formalizable into a computational task. We can, therefore, proceed with a solution.\n\nThe problem requires the implementation of a specific load balancing scheme to distribute computational work (physics parameterizations on grid columns) across a set of processing units. The scheme is a heuristic method based on a greedy algorithm, which is a common approach for such NP-hard partitioning problems. The process consists of three main stages: cost estimation, column binning, and task assignment.\n\n_Step 1: Per-Column Cost Estimation_\n\nThe computational cost of each column $i$, denoted $c_i$, is not known a priori. It must be estimated from a history of $H$ recent wall-clock time measurements, $t_i^{(s)}$, for $s \\in \\{1, \\dots, H\\}$. The problem specifies using an Exponentially Weighted Moving Average (EWMA) for this estimation. The EWMA gives more weight to recent measurements, allowing the cost model to adapt to changing performance characteristics.\n\nThe calculation proceeds for each column $i \\in \\{1, \\dots, N\\}$. First, we find the earliest available time measurement $t_i^{(s)}$ in the history. Let the index of this first valid measurement be $s^\\star$. The initial cost estimate is set to this value, $c_i^{(s^\\star)} = t_i^{(s^\\star)}$. For all subsequent time steps $s > s^\\star$ up to $H$, the estimate is updated using the recurrence relation:\n$$\nc_i^{(s)} = \\alpha \\, t_i^{(s)} + (1-\\alpha) \\, c_i^{(s-1)}\n$$\nwhere $\\alpha \\in (0, 1)$ is the smoothing factor. If a measurement $t_i^{(s)}$ is unavailable (missing), the cost estimate is carried over from the previous step, i.e., $c_i^{(s)} = c_i^{(s-1)}$. The final estimated cost for column $i$ is the value after processing all $H$ historical data points, $c_i = c_i^{(H)}$. If a column has no valid time measurements in its history, its cost $c_i$ is defined to be $0$.\n\n_Step 2: Histogram Binning of Columns_\n\nTo manage the complexity of assigning a large number of columns with diverse costs, the columns are first grouped into a histogram with $B$ bins based on their estimated costs $c_i$. This bucketing strategy is a key feature of the specified algorithm, as it allows the subsequent greedy assignment to prioritize more expensive columns.\n\nLet $m = \\min_{i} c_i$ and $M = \\max_{i} c_i$ be the minimum and maximum estimated costs over all columns. If all columns have the same cost ($M=m$), they are all placed in a single bin, indexed $B-1$. Otherwise, the cost range $[m, M]$ is divided into $B$ equal-width intervals. The bin index $b_i$ for a column $i$ with cost $c_i$ is determined by the formula:\n$$\nb_i = \\min\\left\\{B-1, \\left\\lfloor \\frac{c_i - m}{M - m} B \\right\\rfloor\\right\\}\n$$\nThis assigns each column to one of $B$ bins, with bin $0$ containing the columns with the lowest costs and bin $B-1$ containing those with the highest costs. The `min` function ensures that a column with the maximum cost $M$ is correctly placed in the last bin (index $B-1$) rather than a non-existent bin $B$.\n\n_Step 3: Bucketed Decreasing-Time Greedy Assignment_\n\nThis is the core load balancing step. We have $P$ tasks (or processors), and we need to assign each column to one task. The goal is to make the total computational load of each task as equal as possible. Task loads, denoted $\\ell_j$ for $j \\in \\{1, \\dots, P\\}$, are initialized to zero, $\\ell_j = 0$.\n\nThe assignment algorithm is a greedy heuristic that approximates the optimal solution. It processes the bins created in the previous step in descending order of cost, from $b = B-1$ down to $b = 0$. Within each bin, it iterates through the columns. For each column $i$, it identifies the task $j^\\star$ that currently has the minimum accumulated load:\n$$\nj^\\star = \\arg\\min_{j \\in \\{1, \\dots, P\\}} \\ell_j\n$$\nThe column $i$ is then assigned to this task, and the task's load is updated by adding the column's cost:\n$$\n\\ell_{j^\\star} \\leftarrow \\ell_{j^\\star} + c_i\n$$\nThis procedure is a variant of the Longest Processing Time (LPT) scheduling algorithm. By assigning the most \"expensive\" columns first, it attempts to balance the largest chunks of work early, leaving smaller jobs to fill in the gaps and even out the final loads. This is a well-known and effective approximation algorithm for the makespan minimization problem.\n\n_Final Objective Calculation_\n\nAfter all columns have been assigned, the final loads $\\ell_j$ for all $P$ tasks are determined. The metric of success for the load balancing is the makespan, which is the maximum load over all tasks. The program's required output is this value:\n$$\n\\ell_{\\max} = \\max_{j \\in \\{1, \\dots, P\\}} \\ell_j\n$$\nThis quantity represents the total time required to complete all tasks, as the parallel computation is limited by its slowest-running task. Minimizing this value is the objective of the load balancing scheme. The implementation will calculate this value for each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_costs(times: np.ndarray, N: int, H: int, alpha: float) -> np.ndarray:\n    \"\"\"\n    Computes per-column cost estimates using an Exponentially Weighted Moving Average (EWMA).\n    \n    Args:\n        times: An (N, H) numpy array of recent wall-clock times. Missing values are np.nan.\n        N: Number of columns.\n        H: History length.\n        alpha: EWMA smoothing parameter.\n\n    Returns:\n        A numpy array of size N with the estimated cost for each column.\n    \"\"\"\n    costs = np.zeros(N)\n    for i in range(N):\n        time_series = times[i, :]\n        \n        first_valid_idx = -1\n        for s in range(H):\n            if not np.isnan(time_series[s]):\n                first_valid_idx = s\n                break\n        \n        if first_valid_idx == -1:\n            costs[i] = 0.0\n            continue\n            \n        current_c = time_series[first_valid_idx]\n        \n        for s in range(first_valid_idx + 1, H):\n            if not np.isnan(time_series[s]):\n                current_c = alpha * time_series[s] + (1.0 - alpha) * current_c\n        \n        costs[i] = current_c\n        \n    return costs\n\ndef bin_columns(costs: np.ndarray, N: int, B: int) -> list[list[int]]:\n    \"\"\"\n    Assigns columns to histogram bins based on their estimated costs.\n    \n    Args:\n        costs: A numpy array of size N with estimated costs.\n        N: Number of columns.\n        B: Number of histogram bins.\n\n    Returns:\n        A list of lists, where each inner list contains the indices of columns in that bin.\n    \"\"\"\n    if N == 0:\n        return [[] for _ in range(B)]\n        \n    bins = [[] for _ in range(B)]\n    m, M = np.min(costs), np.max(costs)\n    \n    if M == m:\n        if B > 0:\n            bins[B-1] = list(range(N))\n    else:\n        for i in range(N):\n            c_i = costs[i]\n            # Per problem statement: b_i = min(B-1, floor((c_i-m)/(M-m) * B))\n            bin_index_float = (c_i - m) / (M - m) * B\n            bin_index = int(np.floor(bin_index_float))\n            bin_index = min(B - 1, bin_index)\n            bins[bin_index].append(i)\n            \n    return bins\n\ndef greedy_assignment(costs: np.ndarray, bins: list[list[int]], P: int, B: int) -> float:\n    \"\"\"\n    Performs the bucketed decreasing-time greedy assignment of columns to tasks.\n    \n    Args:\n        costs: A numpy array of size N with estimated costs.\n        bins: A list of lists representing binned column indices.\n        P: Number of tasks.\n        B: Number of bins.\n\n    Returns:\n        The maximum final task load (makespan).\n    \"\"\"\n    if P == 0:\n        return 0.0\n\n    task_loads = np.zeros(P)\n    \n    for b in range(B - 1, -1, -1):\n        # The order of columns within a bin is \"arbitrary\". A standard loop over the list is deterministic.\n        for col_idx in bins[b]:\n            target_task = np.argmin(task_loads)\n            task_loads[target_task] += costs[col_idx]\n\n    if task_loads.size == 0:\n        return 0.0\n        \n    return np.max(task_loads)\n\ndef run_case(N: int, P: int, H: int, B: int, alpha: float, times: np.ndarray) -> float:\n    \"\"\"\n    Executes the full load-balancing algorithm for a given case.\n    \n    Args:\n        N, P, H, B, alpha: Parameters for the case.\n        times: The matrix of recent times.\n    \n    Returns:\n        The maximum final task load.\n    \"\"\"\n    if N == 0:\n        return 0.0\n    \n    costs = calculate_costs(times, N, H, alpha)\n    bins = bin_columns(costs, N, B)\n    max_load = greedy_assignment(costs, bins, P, B)\n    \n    return max_load\n\ndef solve():\n    \"\"\"\n    Defines, runs, and prints results for all test cases.\n    \"\"\"\n    test_cases_params = [\n        {'N': 16, 'P': 4, 'H': 5, 'B': 8, 'alpha': 0.6, 'seed': 12345, 'type': 'lognormal', 'mu': np.log(0.01), 'sigma': 0.5},\n        {'N': 12, 'P': 3, 'H': 4, 'B': 4, 'alpha': 0.5, 'type': 'constant', 'value': 0.015},\n        {'N': 20, 'P': 5, 'H': 5, 'B': 10, 'alpha': 0.7, 'seed': 2021, 'type': 'outlier', 'mu': np.log(0.008), 'sigma': 0.6, 'outlier_val': 0.2},\n        {'N': 3, 'P': 5, 'H': 6, 'B': 5, 'alpha': 0.8, 'type': 'manual'},\n    ]\n\n    results = []\n\n    for params in test_cases_params:\n        N, P, H, B, alpha = params['N'], params['P'], params['H'], params['B'], params['alpha']\n        \n        if params['type'] == 'lognormal':\n            rng = np.random.default_rng(params['seed'])\n            times = rng.lognormal(mean=params['mu'], sigma=params['sigma'], size=(N, H))\n        \n        elif params['type'] == 'constant':\n            times = np.full((N, H), params['value'])\n            \n        elif params['type'] == 'outlier':\n            rng = np.random.default_rng(params['seed'])\n            times = np.zeros((N, H))\n            times[:-1, :] = rng.lognormal(mean=params['mu'], sigma=params['sigma'], size=(N - 1, H))\n            times[-1, :] = params['outlier_val'] # Distinguished column is the last one\n        \n        elif params['type'] == 'manual':\n            times = np.array([\n                [0.02, np.nan, 0.015, np.nan, 0.017, 0.018],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n            ])\n            \n        max_load = run_case(N, P, H, B, alpha, times)\n        results.append(max_load)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}