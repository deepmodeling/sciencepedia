{
    "hands_on_practices": [
        {
            "introduction": "Understanding a model's performance limitations is the first step toward optimization. This exercise introduces the concept of arithmetic intensity, a fundamental metric that quantifies the ratio of computation to memory access for a given algorithm. By deriving this value from first principles, you will gain insight into whether a computational kernel is likely to be limited by the processor's floating-point capability or the speed of data movement from main memory .",
            "id": "4051457",
            "problem": "A three-dimensional geophysical fluid dynamics kernel updates a scalar field on a uniform Cartesian grid using a seven-point finite-difference stencil at each interior grid location. Assume a streaming execution on a cache-coherent shared-memory node without temporal blocking, so that data movement from main memory dominates and each update is independent in terms of counted traffic. Let the grid have $N_x$ points in the $x$-direction, $N_y$ points in the $y$-direction, and $N_z$ points in the $z$-direction, with $N \\equiv N_x N_y N_z$ interior updates per time step.\n\nYou are told that, per grid point, the kernel performs $n_f$ floating-point operations and causes $n_b$ bytes to be transferred to or from main memory (including all loads and stores needed for the update under the given execution assumptions). Starting from the definition of arithmetic intensity and from first principles of counting work and data movement, derive the arithmetic intensity of this stencil kernel as a closed-form analytic expression in terms of $n_f$ and $n_b$.\n\nExpress the final arithmetic intensity in floating-point operations per byte (flops per byte). No rounding is required.",
            "solution": "The problem requires the derivation of the arithmetic intensity of a stencil kernel based on first principles.\n\nThe arithmetic intensity, denoted by $I$, is formally defined as the ratio of the total number of floating-point operations performed to the total volume of data moved between the processor and main memory. The units are typically floating-point operations per byte.\n\n$$\nI = \\frac{\\text{Total Floating-Point Operations}}{\\text{Total Data Movement (Bytes)}}\n$$\n\nWe are given a computational kernel that operates on a three-dimensional Cartesian grid. The total number of interior grid points where an update is performed is given as $N = N_x N_y N_z$.\n\nFirst, let us determine the total number of floating-point operations, which we will denote as $W$. The problem states that the kernel performs $n_f$ floating-point operations for the update at each single grid point. Since there are $N$ such independent updates in total per time step, the total computational work is the product of the work per point and the number of points.\n\n$$\nW = N \\times n_f\n$$\n\nNext, we determine the total data movement, which we will denote as $Q$. The problem specifies a streaming execution model where data movement from main memory is the dominant factor, and cache effects are not considered for the purpose of counting byte traffic (i.e., each update is treated as independent in terms of data loads and stores from main memory). It is given that $n_b$ bytes are transferred to or from main memory for the update at each single grid point. As there are $N$ such updates, the total data volume transferred is the product of the bytes per point and the number of points.\n\n$$\nQ = N \\times n_b\n$$\n\nNow, we can substitute these expressions for total work $W$ and total data movement $Q$ into the definition of arithmetic intensity $I$.\n\n$$\nI = \\frac{W}{Q} = \\frac{N \\times n_f}{N \\times n_b}\n$$\n\nThe term representing the total number of grid points, $N$, appears in both the numerator and the denominator. We can simplify the expression by canceling this term. This demonstrates that for this model, the arithmetic intensity is an intrinsic property of the per-point computation and its associated memory access pattern, independent of the overall grid size.\n\n$$\nI = \\frac{n_f}{n_b}\n$$\n\nThis is the final closed-form analytic expression for the arithmetic intensity of the kernel. It is expressed in terms of the given parameters $n_f$ (flops per point) and $n_b$ (bytes per point), and its units are indeed floating-point operations per byte, as required. The contextual information regarding the specific stencil type (seven-point) and the grid dimensions ($N_x$, $N_y$, $N_z$) is implicitly accounted for within the provided parameters $n_f$ and $n_b$, and thus does not explicitly appear in the final expression.",
            "answer": "$$\n\\boxed{\\frac{n_f}{n_b}}\n$$"
        },
        {
            "introduction": "Scaling a geophysical model onto a distributed-memory supercomputer requires efficiently partitioning the computational domain across thousands of processors. The key to scalability is minimizing the communication overhead relative to the computation performed on each processor. This practice challenges you to solve a classic optimization problem: determining the ideal shape of a subdomain to minimize its surface-area-to-volume ratio, thereby minimizing communication for a fixed amount of memory and work .",
            "id": "4051428",
            "problem": "A structured-grid three-dimensional geophysical fluid model for Numerical Weather Prediction (NWP) is implemented on a distributed-memory High-Performance Computing (HPC) system using domain decomposition. Each compute node owns exactly one axis-aligned rectangular block of interior grid cells of dimensions $n_x \\times n_y \\times n_z$, augmented with a cell-based halo of width $h$ on all six faces to support stencil computations and Message Passing Interface (MPI) exchanges. The per-node memory capacity is $M$, and each grid cell holds $q$ prognostic or diagnostic arrays, each stored with $b$ bytes, for an effective per-cell memory footprint $\\beta = q b$. Assume the dominant inter-node communication each time step consists of exchanging halo slabs of thickness $h$ across the six faces.\n\nStarting from first principles of geometric optimization and resource constraints, and using only the definitions provided above, derive the interior block dimensions $(n_x,n_y,n_z)$ that minimize the total halo communication volume per time step, subject to exactly saturating the per-node memory capacity. Express the final optimal interior block dimensions as an analytic expression in terms of $M$, $h$, $b$, and $q$ only. Assume $M$ is sufficiently large that the optimal block dimensions are nonnegative. No rounding is required. Express the final answer as the number of cells along each dimension.",
            "solution": "The user wants me to validate and solve a problem from the domain of high-performance computing for geophysical modeling.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **System Configuration**: A distributed-memory HPC system with domain decomposition.\n*   **Per-Node Domain**: Each node holds one axis-aligned rectangular block of interior grid cells.\n*   **Interior Dimensions**: The dimensions of the interior block are $n_x \\times n_y \\times n_z$.\n*   **Halo Region**: The interior block is augmented with a cell-based halo of width $h$ on all six faces.\n*   **Memory Constraint**: The memory capacity per compute node is $M$.\n*   **Data Storage**: Each grid cell stores $q$ arrays, with each array element occupying $b$ bytes.\n*   **Per-Cell Memory Footprint**: The effective memory per cell is $\\beta = q b$.\n*   **Communication Model**: The dominant communication per time step involves exchanging halo slabs of thickness $h$ across the six faces.\n*   **Optimization Goal**: Minimize the total halo communication volume per time step.\n*   **Optimization Constraint**: The per-node memory capacity $M$ must be exactly saturated.\n*   **Desired Output**: An analytic expression for the optimal interior block dimensions $(n_x, n_y, n_z)$ in terms of $M$, $h$, $b$, and $q$.\n*   **Assumption**: $M$ is sufficiently large to ensure the optimal dimensions are non-negative.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientific Grounding**: The problem is scientifically grounded. It represents a standard and fundamental optimization problem in parallel scientific computing, specifically for stencil-based methods on structured grids. The concepts of domain decomposition, halo exchange, memory constraints, and the trade-off between computation (volume) and communication (surface area) are central to the field of high-performance computing for numerical simulations like NWP models.\n*   **Well-Posed**: The problem is well-posed. It specifies a clear objective function (communication volume) to be minimized, subject to a well-defined equality constraint (memory capacity). This structure is amenable to standard mathematical optimization techniques, such as the method of Lagrange multipliers, and is expected to yield a unique and meaningful solution.\n*   **Objective**: The problem is stated objectively using precise, standard terminology from the field of numerical modeling and HPC. All parameters are clearly defined symbols. There are no subjective or ambiguous statements.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid as it is scientifically sound, well-posed, and objective. It is a formalizable problem with a clear path to a solution. I will proceed with the derivation.\n\n### Solution Derivation\n\nThe problem requires us to find the interior grid dimensions $(n_x, n_y, n_z)$ that minimize inter-node communication volume while fully utilizing the available memory $M$ on each node.\n\n**1. Formulate the Memory Constraint**\n\nEach compute node stores an interior block of size $n_x \\times n_y \\times n_z$, plus a halo of width $h$ on all six faces. The total dimensions of the grid block stored in memory are therefore $(n_x + 2h) \\times (n_y + 2h) \\times (n_z + 2h)$.\n\nThe total number of grid cells stored on one node, $N_{total}$, is the volume of this augmented block:\n$$N_{total} = (n_x + 2h)(n_y + 2h)(n_z + 2h)$$\n\nEach grid cell requires $\\beta = qb$ bytes of memory. The total memory used per node, $V_{mem}$, is:\n$$V_{mem} = N_{total} \\cdot \\beta = (n_x + 2h)(n_y + 2h)(n_z + 2h) q b$$\n\nThe problem states that the per-node memory capacity $M$ is exactly saturated. This gives us the constraint equation:\n$$(n_x + 2h)(n_y + 2h)(n_z + 2h) q b = M$$\n\n**2. Formulate the Communication Volume Objective Function**\n\nCommunication consists of exchanging halo data with neighboring nodes. The amount of data to be exchanged is determined by the surface area of the interior block. For a halo of thickness $h$, a slab of data with thickness $h$ must be sent from the interior of the local domain to fill the halo of the adjacent domain.\n\nThe communication volume is proportional to the number of cells on the six faces of the interior block that are sent to neighbors. The total number of cells communicated per time step, $N_{comm}$, is the sum of the volumes of these slabs:\n$$N_{comm} = 2(n_x n_y h) + 2(n_x n_z h) + 2(n_y n_z h)$$\nHere, $n_y n_z$ is the area of a face in the y-z plane, and $h$ is the thickness of the data slab sent across it; this is done for two such faces ($-x$ and $+x$), and similarly for the other pairs of faces.\n\nThe total communication volume in bytes, $V_{comm}$, is $N_{comm} \\cdot \\beta$.\n$$V_{comm} = 2h(n_x n_y + n_x n_z + n_y n_z) q b$$\n\nTo minimize $V_{comm}$, given that $2$, $h$, $q$, and $b$ are positive constants, we must minimize the surface area function $S(n_x, n_y, n_z)$:\n$$S(n_x, n_y, n_z) = n_x n_y + n_x n_z + n_y n_z$$\n\n**3. Solve the Constrained Optimization Problem**\n\nWe need to minimize $S(n_x, n_y, n_z)$ subject to the constraint $g(n_x, n_y, n_z) = 0$, where:\n$$g(n_x, n_y, n_z) = (n_x + 2h)(n_y + 2h)(n_z + 2h) q b - M = 0$$\n\nWe use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\\mathcal{L}(n_x, n_y, n_z, \\lambda) = S(n_x, n_y, n_z) - \\lambda g(n_x, n_y, n_z)$$\n$$\\mathcal{L} = (n_x n_y + n_x n_z + n_y n_z) - \\lambda [ (n_x + 2h)(n_y + 2h)(n_z + 2h) q b - M ]$$\n\nWe find the optimum by setting the partial derivatives of $\\mathcal{L}$ with respect to $n_x$, $n_y$, and $n_z$ to zero:\n$$\\frac{\\partial \\mathcal{L}}{\\partial n_x} = n_y + n_z - \\lambda (1)(n_y + 2h)(n_z + 2h) q b = 0 \\quad (1)$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial n_y} = n_x + n_z - \\lambda (n_x + 2h)(1)(n_z + 2h) q b = 0 \\quad (2)$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial n_z} = n_x + n_y - \\lambda (n_x + 2h)(n_y + 2h)(1) q b = 0 \\quad (3)$$\n\nFrom these equations, we can isolate the terms containing $\\lambda$:\n$$n_y + n_z = \\lambda q b (n_y + 2h)(n_z + 2h)$$\n$$n_x + n_z = \\lambda q b (n_x + 2h)(n_z + 2h)$$\n$$n_x + n_y = \\lambda q b (n_x + 2h)(n_y + 2h)$$\n\nLet's divide the first two equations (assuming non-zero denominators, which is physically required):\n$$\\frac{n_y + n_z}{n_x + n_z} = \\frac{(n_y + 2h)(n_z + 2h)}{(n_x + 2h)(n_z + 2h)} = \\frac{n_y + 2h}{n_x + 2h}$$\n$$(n_y + n_z)(n_x + 2h) = (n_x + n_z)(n_y + 2h)$$\n$$n_x n_y + 2h n_y + n_x n_z + 2h n_z = n_x n_y + 2h n_x + n_y n_z + 2h n_z$$\n$$2h n_y + n_x n_z = 2h n_x + n_y n_z$$\n$$n_z(n_x - n_y) - 2h(n_x - n_y) = 0$$\n$$(n_z - 2h)(n_x - n_y) = 0$$\nThis implies that either $n_x = n_y$ or $n_z = 2h$. For a general solution not dependent on a specific relationship between dimension and halo width, we must have $n_x = n_y$.\n\nBy symmetry of the equations, applying the same procedure to other pairs of equations will yield $n_y = n_z$ and $n_z = n_x$. Therefore, the optimal shape for the interior block is a cube:\n$$n_x = n_y = n_z = n$$\n\n**4. Determine the Optimal Dimensions**\n\nNow we substitute this result into the memory constraint equation:\n$$(n + 2h)(n + 2h)(n + 2h) q b = M$$\n$$(n + 2h)^3 q b = M$$\n$$(n + 2h)^3 = \\frac{M}{q b}$$\nTaking the cube root of both sides:\n$$n + 2h = \\left(\\frac{M}{q b}\\right)^{1/3}$$\nFinally, we solve for $n$:\n$$n = \\left(\\frac{M}{q b}\\right)^{1/3} - 2h$$\n\nThis gives the optimal dimension for the interior cubical block. The problem asks for the triplet $(n_x, n_y, n_z)$.\n\nThe optimal dimensions are:\n$$n_x = n_y = n_z = \\left(\\frac{M}{q b}\\right)^{1/3} - 2h$$\nThe assumption that $M$ is sufficiently large ensures that this result is positive.",
            "answer": "$$\n\\boxed{n_x = n_y = n_z = \\left(\\frac{M}{q b}\\right)^{1/3} - 2h}\n$$"
        },
        {
            "introduction": "Once an optimal domain decomposition strategy is designed, it must be implemented correctly and robustly. This hands-on simulation task addresses one of the most critical challenges in parallel programming: implementing halo exchanges without creating deadlock, a state where processors are stuck in a circular wait for resources. You will design and test a communication strategy that guarantees deadlock freedom, a cornerstone of reliable and scalable parallel models .",
            "id": "4051447",
            "problem": "Consider a $3$-dimensional domain decomposition for a geophysical model on a Cartesian processor grid of size $N_x \\times N_y \\times N_z$, where each processor holds a subdomain and exchanges halo data with its six face-neighbor processors. In High-Performance Computing (HPC), halo exchanges are often implemented with nonblocking primitives in the Message Passing Interface (MPI). In a rendezvous protocol for large messages, a send operation can only complete after the matching receive is posted on the destination. Assume a simplified, abstracted semantics for nonblocking communication:\n\n- Each processor can post nonblocking receive operations, denoted as $P_r$, which do not consume injection capacity.\n- Each processor can post nonblocking send operations, denoted as $P_s$, up to an injection capacity of $C$ outstanding send messages. Injection capacity $C$ is measured as a count of messages, not bytes.\n- Each posted send of volume $V$ bytes behaves as follows:\n  - If $V \\le V_\\mathrm{RZ}$ (eager regime), it can complete without a prior matching receive, provided injection capacity allows $P_s$ to be posted.\n  - If $V > V_\\mathrm{RZ}$ (rendezvous regime), it can complete only after the destination has posted the matching $P_r$.\n- A wait operation, denoted as $W$, blocks until all previously posted operations in its scope have completed.\n- The grid is nonperiodic; boundary processors have fewer than six neighbors.\n\nDeadlock in this abstract model is defined via a wait-for graph: if there exists a cycle of operations where each processor is waiting for another processor to perform an action that cannot occur due to posted operations and capacity constraints, then the system is deadlocked.\n\nTask: Construct a deadlock-free nonblocking halo exchange for $3$-dimensional decomposition by ordering $P_r$, $P_s$, and $W$ operations correctly. Your algorithm must:\n\n1. Respect rendezvous semantics when $V > V_\\mathrm{RZ}$.\n2. Respect per-processor injection capacity $C$ messages.\n3. Avoid deadlock by ensuring the wait-for graph is acyclic.\n4. Complete halo exchanges along the $x$-, $y$-, and $z$-axes.\n\nYou must implement a simulation of this abstracted protocol in a program. The simulation should model processors at integer coordinates $(i,j,k)$ for $0 \\le i < N_x$, $0 \\le j < N_y$, $0 \\le k < N_z$, with unit-distance neighbors in the six axis-aligned directions where they exist. Each halo exchange along an axis consists of at most two directional messages per processor (one to the negative direction neighbor and one to the positive direction neighbor), constrained by boundary conditions.\n\nDesign an ordering strategy that achieves deadlock freedom and termination for rendezvous messages by phasing operations along each axis and using a checkerboard parity to serialize conflicting sends. Specifically, you must decide, for each axis, the sequence in which processors post $P_r$, post $P_s$, and invoke $W$, taking into account that $C$ may be as small as $1$.\n\nSimulation rules to implement:\n\n- The program must internally model the posting of $P_r$ and $P_s$ operations, their matching, and completion under the rendezvous constraint when $V > V_\\mathrm{RZ}$.\n- Posting a $P_s$ increases a processor’s outstanding send count by $1$; completion of that send decreases it by $1$.\n- A $P_r$ can be posted at any time and does not consume capacity.\n- A send in the rendezvous regime completes only if the destination has posted the matching $P_r$.\n- A wait $W$ for an axis must ensure all posted operations for that axis complete before proceeding to the next axis.\n- If, during a $W$, no operations can complete and no further posting is possible due to capacity or ordering constraints, the simulation declares deadlock for that test case.\n\nYour program should implement your ordering strategy and report whether the halo exchange completes without deadlock for a test suite of cases. Use $V_\\mathrm{RZ} = 512$ bytes. All message volumes $V$ in the test suite are in bytes.\n\nTest Suite (each case is a tuple $(N_x,N_y,N_z,V,C)$):\n\n- Case $1$: $(2,2,2,4096,1)$, rendezvous regime, balanced grid; expected to complete.\n- Case $2$: $(4,1,1,4096,1)$, rendezvous regime, $x$-line decomposition; expected to complete.\n- Case $3$: $(3,3,1,8192,1)$, rendezvous regime, slab decomposition; expected to complete.\n- Case $4$: $(3,3,3,8192,1)$, rendezvous regime, cubic grid; expected to complete.\n- Case $5$: $(2,2,2,8192,0)$, rendezvous regime with zero injection capacity; expected to deadlock.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[$r_1$,$r_2$,$r_3$,$r_4$,$r_5$]\" where each $r_i$ is a boolean indicating whether the halo exchange completes ($\\mathrm{True}$) or deadlocks ($\\mathrm{False}$) for case $i$.",
            "solution": "The problem requires the design and simulation of a deadlock-free nonblocking halo exchange algorithm for a $3$-dimensional Cartesian processor grid. The algorithm must be correct under the rendezvous communication protocol for large messages and a constrained per-processor injection capacity, $C$, which can be as low as $1$.\n\n### Principle-Based Design\n\nThe primary challenge is to avoid deadlock. Deadlock typically arises from circular \"wait-for\" dependencies. In our context, this occurs when two or more processors are waiting for each other to post a receive ($P_r$) for a rendezvous send ($P_s$) they have already posted. With a limited injection capacity $C$, a processor might use up its capacity posting sends and be unable to proceed to post the receives needed by its neighbors.\n\nThe standard and most robust solution, hinted at in the problem description, is to break these cyclical dependencies by structuring the communication in phases.\n\n1.  **Dimensional Phasing**: The full $3D$ halo exchange is decomposed into three independent, sequential exchanges, one for each axis ($x$, $y$, and $z$). A global synchronization barrier (a wait operation, $W$) is placed between each axis exchange. This ensures that, for instance, no processor is waiting for a message along the $x$-axis from a neighbor who is in turn waiting for a message along the $y$-axis. This confines all dependencies to a single axis at a time.\n\n2.  **Parity-Based Role Phasing (Checkerboard Pattern)**: Within a single axis exchange, we must still prevent deadlocks between adjacent processors (e.g., processor $i$ and $i+1$ both sending to each other). This is resolved by assigning roles based on processor coordinate parity. The exchange along an axis is split into two phases:\n    -   **Phase 1**: Processors with an even coordinate index along the axis are designated as \"senders,\" and processors with an odd coordinate index are \"receivers.\" The even-indexed processors post sends ($P_s$) to their odd-indexed neighbors, and odd-indexed processors post receives ($P_r$) from their even-indexed neighbors. Because sends are only directed from even to odd processors, the wait-for graph is a directed acyclic graph (DAG), and no deadlock can occur. A rendezvous send from an even processor will always find a corresponding receive already posted at its odd-numbered destination.\n    -   **Phase 2**: The roles are reversed. Odd-indexed processors become \"senders,\" and even-indexed processors become \"receivers.\" This phase handles communication in the opposite direction, is also deadlock-free for the same reason, and completes the bidirectional halo exchange for the axis.\n\nThis two-phase parity-based approach is crucial for correctness when injection capacity $C$ is small. For $C=1$, a sending processor with two neighbors can only post one send at a time. The simulation must correctly model that this send completes (as its destination is a receiving-only processor in this phase), freeing up injection capacity for the second send to be posted and completed.\n\n### Simulation Algorithm\n\nThe simulation will model the state of each processor and the progression of communication operations according to the rules.\n\n-   **Processor State**: Each processor at coordinates $(i, j, k)$ is modeled as an object maintaining its state:\n    -   Injection Capacity: $C$.\n    -   `outstanding_sends`: A list of send operations posted but not yet completed. The size of this list cannot exceed $C$.\n    -   `pending_sends`: A queue of send operations that are ready to be posted but are blocked by a full injection capacity.\n    -   `posted_receives`: A list of receive operations that have been posted.\n\n-   **Wait Operation ($W$) Simulation**: The `wait` operation is implemented as a simulation loop that continues until no more progress can be made. In each iteration, the loop performs two main actions:\n    1.  **Check for Completions**: It scans all `outstanding_sends`. A send operation can complete if it is either in the eager regime ($V \\le V_\\mathrm{RZ}$) or if a matching `posted_receive` exists at the destination processor. Upon completion, the send is removed from `outstanding_sends`, freeing injection capacity.\n    2.  **Post Pending Sends**: It scans all processors. If a processor has `pending_sends` and available injection capacity (because a previous send has completed), it moves a pending send to its `outstanding_sends` list.\n\n-   **Deadlock Detection**: The simulation is deadlocked if a full iteration of the `wait` loop occurs where no sends could be completed and no new sends could be posted, yet there is still unfinished work (i.e., non-empty `outstanding_sends` or `pending_sends` lists).\n\n-   **Overall Flow**:\n    1.  Initialize the $N_x \\times N_y \\times N_z$ grid of processors.\n    2.  For each axis `d` in {$x, y, z$}:\n        a.  **Phase 1 (Even → Odd)**:\n            i.  All odd-indexed processors on axis `d` post all their receives from neighbors.\n            ii. All even-indexed processors on axis `d` post their sends to neighbors (queuing them as pending if capacity is exceeded).\n            iii. Execute the `wait` simulation loop. If it returns deadlock, the entire exchange fails.\n        b.  **Phase 2 (Odd → Even)**:\n            i.  All even-indexed processors post receives.\n            ii. All odd-indexed processors post sends.\n            iii. Execute the `wait` simulation loop. If it returns deadlock, the exchange fails.\n    3.  If all phases for all axes complete successfully, the halo exchange is successful.\n\nThis design is guaranteed to be deadlock-free for $C \\ge 1$. For the test case with $C=0$, no processor can ever post a send, leading to an immediate and correctly identified deadlock.",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\n# Define simple data structures for communication operations\nSendOperation = namedtuple('SendOperation', ['dest_coords'])\nRecvOperation = namedtuple('RecvOperation', ['source_coords'])\n\nclass Processor:\n    \"\"\"Models a single processor in the Cartesian grid.\"\"\"\n    \n    def __init__(self, coords, capacity):\n        self.coords = coords\n        self.capacity = capacity\n        self.reset_phase_state()\n\n    def reset_phase_state(self):\n        \"\"\"Resets the state for a new communication phase.\"\"\"\n        self.pending_sends = []\n        self.outstanding_sends = []\n        self.posted_receives = []\n\n    def post_send(self, dest_coords):\n        \"\"\"Posts a send operation, respecting injection capacity.\"\"\"\n        op = SendOperation(dest_coords=dest_coords)\n        if len(self.outstanding_sends)  self.capacity:\n            self.outstanding_sends.append(op)\n        else:\n            self.pending_sends.append(op)\n            \n    def post_receive(self, source_coords):\n        \"\"\"Posts a receive operation.\"\"\"\n        op = RecvOperation(source_coords=source_coords)\n        self.posted_receives.append(op)\n\ndef simulate_wait_phase(processors, is_rendezvous):\n    \"\"\"\n    Simulates the network until all posted operations complete for a phase.\n    Returns True on success, False on deadlock.\n    \"\"\"\n    while True:\n        made_progress = False\n\n        # --- Part 1: Try to complete outstanding sends ---\n        # We iterate over a snapshot of all sends to allow modification of lists during the loop\n        sends_to_check = []\n        for p in processors.values():\n            for send_op in p.outstanding_sends:\n                sends_to_check.append((p, send_op))\n        \n        for sender_proc, send_op in sends_to_check:\n            # Check if this send is still outstanding (might have been completed already in this loop)\n            if send_op not in sender_proc.outstanding_sends:\n                continue\n\n            dest_proc = processors[send_op.dest_coords]\n            \n            matching_recv = next((recv for recv in dest_proc.posted_receives if recv.source_coords == sender_proc.coords), None)\n            \n            can_complete = (not is_rendezvous) or (is_rendezvous and matching_recv is not None)\n            \n            if can_complete:\n                sender_proc.outstanding_sends.remove(send_op)\n                if matching_recv:\n                    dest_proc.posted_receives.remove(matching_recv)\n                made_progress = True\n\n        # --- Part 2: Try to post pending sends ---\n        for p in processors.values():\n            while p.pending_sends and len(p.outstanding_sends)  p.capacity:\n                op_to_post = p.pending_sends.pop(0)\n                p.outstanding_sends.append(op_to_post)\n                made_progress = True\n\n        # --- Part 3: Check for completion or deadlock ---\n        all_done = all(not p.pending_sends and not p.outstanding_sends for p in processors.values())\n        \n        if all_done:\n            # All posted and pending sends for this phase are complete.\n            # We must also check that all receives were matched.\n            if any(p.posted_receives for p in processors.values()):\n                # This indicates an asymmetry or error in the algorithm setup\n                return False \n            return True # Phase successful\n\n        if not made_progress:\n            return False # Deadlock: work remains but no progress was made.\n\ndef run_simulation(Nx, Ny, Nz, V, C):\n    \"\"\"\n    Runs the simulation for a single test case.\n    Returns True if the halo exchange completes, False if it deadlocks.\n    \"\"\"\n    V_RZ = 512\n    is_rendezvous = V > V_RZ\n    \n    processors = {\n        (i, j, k): Processor(coords=(i, j, k), capacity=C)\n        for i in range(Nx) for j in range(Ny) for k in range(Nz)\n    }\n\n    dimensions = [(0, Nx), (1, Ny), (2, Nz)]\n\n    for dim_idx, N_dim in dimensions:\n        if N_dim = 1:\n            continue\n\n        # --- Phase 1: Even-indexed processors send, Odd-indexed receive ---\n        for p in processors.values():\n            p.reset_phase_state()\n\n        # Odd processors post receives from their even neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 != 0:\n                if coords[dim_idx] > 0: # has negative neighbor\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_receive(source_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1: # has positive neighbor\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_receive(source_coords=tuple(pos_coords))\n\n        # Even processors post sends to their odd neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 == 0:\n                if coords[dim_idx] > 0: # has negative neighbor\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_send(dest_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1: # has positive neighbor\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_send(dest_coords=tuple(pos_coords))\n\n        if not simulate_wait_phase(processors, is_rendezvous):\n            return False\n\n        # --- Phase 2: Odd-indexed processors send, Even-indexed receive ---\n        for p in processors.values():\n            p.reset_phase_state()\n\n        # Even processors post receives from their odd neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 == 0:\n                if coords[dim_idx] > 0:\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_receive(source_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1:\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_receive(source_coords=tuple(pos_coords))\n\n        # Odd processors post sends to their even neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 != 0:\n                if coords[dim_idx] > 0:\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_send(dest_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1:\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_send(dest_coords=tuple(pos_coords))\n\n        if not simulate_wait_phase(processors, is_rendezvous):\n            return False\n\n    return True\n\ndef solve():\n    test_cases = [\n        (2, 2, 2, 4096, 1),\n        (4, 1, 1, 4096, 1),\n        (3, 3, 1, 8192, 1),\n        (3, 3, 3, 8192, 1),\n        (2, 2, 2, 8192, 0),\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny, Nz, V, C = case\n        result = run_simulation(Nx, Ny, Nz, V, C)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}