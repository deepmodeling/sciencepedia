## Introduction
The fusion of physics-based modeling and machine learning represents a transformative frontier in computational science. While traditional physical models offer rigorous, law-driven predictions, they often struggle with inherent limitations, such as representing complex, unresolved processes, leading to significant model error. Conversely, purely data-driven models, though powerful pattern recognizers, lack physical [interpretability](@entry_id:637759) and can fail to generalize outside their training data. Hybrid physics–machine learning modeling addresses this gap by creating a powerful synthesis, augmenting the deductive strength of physical laws with the inductive power of machine learning. This article serves as a comprehensive guide to this burgeoning field. In the following chapters, you will first explore the foundational "Principles and Mechanisms," learning how to design, build, and stabilize these complex models. Next, "Applications and Interdisciplinary Connections" will demonstrate their real-world impact across fields from climate science to [bioinformatics](@entry_id:146759). Finally, "Hands-On Practices" will provide opportunities to apply these concepts directly. This structured approach will equip you with the knowledge to construct models that are not only more accurate but also more physically robust and scientifically trustworthy.

## Principles and Mechanisms

This chapter delves into the foundational principles and core mechanisms that underpin the design, implementation, and evaluation of hybrid physics–machine learning models. We will move from the conceptual justification for these models to the specific strategies for their construction, the physical and numerical principles they must obey, and the grand challenges they face in scientific applications.

### Conceptual Foundations of Hybrid Modeling

The primary motivation for integrating machine learning (ML) with physics-based models is the existence of **[model error](@entry_id:175815)**: the discrepancy between the behavior of the real-world system and its numerical representation. In the context of atmospheric and climate science, the evolution of a state vector $\mathbf{x}$ (comprising fields like temperature, wind, and humidity) is governed by a set of true physical tendencies, which we can denote abstractly as $\mathcal{F}_{\mathrm{true}}(\mathbf{x})$. A numerical model, however, implements a parameterized and discretized approximation of these tendencies, $\mathcal{F}_{\mathrm{num}}(\mathbf{x}; \boldsymbol{\theta})$, where $\boldsymbol{\theta}$ represents a set of tunable parameters.

The instantaneous [model error](@entry_id:175815) can thus be formally defined as the tendency difference:

$$
\boldsymbol{\varepsilon}(\mathbf{x}) = \mathcal{F}_{\mathrm{true}}(\mathbf{x}) - \mathcal{F}_{\mathrm{num}}(\mathbf{x}; \boldsymbol{\theta})
$$

This error term $\boldsymbol{\varepsilon}(\mathbf{x})$ is precisely what hybrid modeling seeks to represent and correct. The sources of this error are twofold .

**Parametric error** arises when the functional form of the model, $\mathcal{F}_{\mathrm{num}}$, is correct, but the chosen parameter values $\boldsymbol{\theta}$ are inaccurate. For instance, in a warm-rain microphysics scheme, the equation for [autoconversion](@entry_id:1121257) of cloud water to rainwater might be structurally sound, but the specific value used for the threshold cloud water [mixing ratio](@entry_id:1127970)—a parameter within $\boldsymbol{\theta}$—may be incorrect. In principle, parametric error can be reduced by calibrating or "tuning" $\boldsymbol{\theta}$ against observational data.

**Structural error**, by contrast, occurs when the functional form $\mathcal{F}_{\mathrm{num}}$ itself is deficient. It may be a simplification of the true physics or may omit certain processes entirely. For example, using a warm-rain-only microphysics scheme in a mixed-phase cloud environment where ice processes are significant is a structural error. Similarly, assuming clear-sky conditions in a longwave radiation calculation for a cloudy atmospheric column is a structural error. No amount of parameter tuning can fix a structural deficiency; it requires changing the model's governing equations. It is the ability of machine learning models, particularly [deep neural networks](@entry_id:636170), to learn highly complex and nonlinear functions from data that makes them powerful tools for correcting structural errors .

Much of this [model error](@entry_id:175815) is concentrated in the representation of **subgrid-scale processes**. When the continuous governing equations are filtered at the scale of a model's grid resolution, $\Delta$, unclosed terms emerge that represent the effects of unresolved motions on the resolved flow. For example, filtering the momentum equation yields a subgrid stress term, and filtering a [scalar transport equation](@entry_id:1131253) yields a subgrid flux term. The task of a **[subgrid parameterization](@entry_id:1132597)** is to represent these unclosed terms as a function of the resolved-scale variables .

Within a numerical model's time-stepping loop, it is crucial to distinguish between two types of variables. A **prognostic variable** is one that is advanced in time by integrating a tendency equation; its value is carried from one time step to the next. Examples include temperature, wind, and specific humidity. A **diagnostic variable** is computed "on the fly" within a single time step from the current state of the prognostic variables and is not carried over in time. For example, in a turbulence scheme based on a prognostic equation for Turbulent Kinetic Energy (TKE), TKE is prognostic, while the eddy diffusivities and turbulent fluxes derived from it are diagnostic. In a standard radiation scheme, the radiative fluxes and heating rates are computed diagnostically from the instantaneous atmospheric state. Hybrid models can be designed to emulate either prognostic tendencies or diagnostic quantities .

### A Taxonomy of Hybrid Modeling Strategies

While the overarching goal is to improve model fidelity by combining physics with data-driven methods, several distinct strategies have emerged, differing in how they leverage the existing physical model and the available data .

A useful distinction is between "in-model" corrections, which modify the core [prognostic equations](@entry_id:1130221), and post-processing techniques. A classic example of the latter is **Model Output Statistics (MOS)**, a technique widely used in operational weather forecasting. MOS applies a statistical correction *after* the full physics-based model has been integrated forward in time. It learns a mapping from the model's output (e.g., predicted temperature and pressure) to the observed quantities (e.g., station temperature), effectively correcting for systematic biases and representation errors. This contrasts sharply with hybrid in-model corrections, which are applied *during* the model integration, directly altering the model's trajectory . Within the realm of in-model corrections, we can identify three main paradigms.

**Black-Box Emulators** represent the most data-intensive approach. Here, the entire time-stepping operator of the physical model, $\mathcal{M}_{\Delta t}$, is replaced with a learned surrogate, $\hat{\mathcal{M}}_{\Delta t, \boldsymbol{\theta}}$. This emulator learns the mapping from the state at one time step to the state at the next, $\mathbf{x}_{t+\Delta t} \approx \hat{\mathcal{M}}_{\Delta t, \boldsymbol{\theta}}(\mathbf{x}_t)$, directly from [time-series data](@entry_id:262935). This approach makes no explicit use of the underlying governing equations. Its success hinges on the availability of a training dataset that sufficiently covers the system's attractor, and there is no intrinsic guarantee that the learned emulator will obey physical conservation laws unless they are explicitly enforced.

**Gray-Box Residual Models** are a more conservative and widely adopted strategy. This approach assumes that the existing physics-based model, $\mathcal{M}_{\Delta t}$, captures the dominant dynamics correctly but suffers from the model errors $\boldsymbol{\varepsilon}_{\Delta t}$ discussed previously. The ML model is then trained to predict this residual error. The hybrid model update takes the form $\mathbf{x}_{t+\Delta t} \approx \mathcal{M}_{\Delta t}(\mathbf{x}_t) + \hat{R}_{\boldsymbol{\phi}}(\mathbf{x}_t)$, where $\hat{R}_{\boldsymbol{\phi}}$ is the learned state-dependent correction. This "gray-box" approach retains the trusted physical core and uses ML for targeted corrections, making it more data-efficient and physically interpretable than a full black-box emulator.

**Physics-Informed Neural Networks (PINNs)** represent a different philosophy. Instead of learning from paired input-output data from a simulation, a PINN learns a solution $\mathbf{x}(\mathbf{r}, t)$ that conforms to the governing partial differential equations (PDEs). The PDE operator, $\mathcal{F}$, is incorporated directly into the training loss function. The network is trained to minimize the PDE residual, $\partial_t \mathbf{x} - \mathcal{F}(\mathbf{x}, \nabla \mathbf{x}, \ldots)$, along with boundary and initial conditions. This allows PINNs to be trained with sparse observational data, as the PDE provides a powerful physical constraint everywhere else. This approach requires the analytical form of the governing equations to be known and differentiable.

### Core Principles of Hybrid Model Design

For a hybrid physics-ML model to be a reliable and stable scientific tool, it cannot be a purely statistical pattern-matching algorithm. It must be imbued with fundamental physical principles.

#### Principle 1: Conservation Laws

Physical systems are governed by strict conservation laws for quantities like mass, momentum, and energy. A numerical model that violates these laws can drift into [unphysical states](@entry_id:153570) over long integrations, producing scientifically meaningless results. Since a standard ML model has no intrinsic knowledge of these laws, they must be explicitly enforced. This can be done through "soft" or "hard" constraints .

A **soft constraint** involves adding a penalty term to the ML model's loss function that discourages violations of the conservation law. For example, to conserve the total mass $M$ of a substance whose source is modeled by an ML term $S_{\theta}$, one could add a penalty proportional to $(\int S_{\theta} \, d\mathbf{x})^2$. While this encourages the model to be approximately conservative, it does not guarantee it. Small, persistent violations at each time step can accumulate, or "drift," over a long simulation, leading to significant unphysical changes in the conserved quantity. The total error after $N$ steps can grow to order $N\varepsilon$, where $\varepsilon$ is the average per-step error.

A **hard constraint** enforces the conservation law exactly at every time step, eliminating drift by design. This is strongly preferable for long-term climate simulations or simulations of chaotic systems where small errors can be amplified. There are two primary ways to implement hard constraints. One is through projection, where an unconstrained ML prediction is projected onto a manifold that satisfies the constraint. A more elegant method is to design the ML model's architecture to be **conservative by construction**.

In the widely used **Finite-Volume (FV)** numerical method, [discrete conservation](@entry_id:1123819) is achieved by formulating the update in **flux form**. The change in a conserved quantity within a grid cell is determined by the sum of fluxes across its faces. Global conservation is guaranteed if the numerical flux leaving one cell is identical to the flux entering the adjacent cell (a property called [antisymmetry](@entry_id:261893)). A hybrid model can inherit this property if the ML correction is not learned as a cell-centered tendency but as a correction to the face fluxes, and this learned flux correction is also enforced to be antisymmetric. Any cell-centered tendency can be made conservative if and only if it can be expressed as the discrete divergence of such an antisymmetric flux field .

A concrete example from atmospheric science is the conservation of **Moist Static Energy (MSE)**, defined as $h = c_p T + gz + L_v q_v$, where $c_p$ is the [specific heat](@entry_id:136923) at constant pressure, $T$ is temperature, $g$ is the [acceleration due to gravity](@entry_id:173411), $z$ is height, $L_v$ is the [latent heat of vaporization](@entry_id:142174), and $q_v$ is the water vapor specific humidity. For moist adiabatic processes without precipitation, MSE is a conserved quantity. A parameterization for convection, which vertically transports heat and moisture, should act only to redistribute MSE within the atmospheric column, not to create or destroy it. Therefore, a key diagnostic for the [thermodynamic consistency](@entry_id:138886) of a learned [convection parameterization](@entry_id:1123019) is to check that the column-integrated MSE tendency it produces is zero (or equal to the known external energy sources from radiation and surface fluxes). A significant non-zero residual indicates an unphysical creation or destruction of energy by the ML model .

#### Principle 2: Numerical Stability

Coupling a data-driven component into a numerical model can introduce new pathways to instability. A forecast that produces non-physical, rapidly growing values ("blows up") is useless. Ensuring stability is therefore a paramount concern. We distinguish between two key types of stability .

**Linear stability** analyzes the behavior of small perturbations around a steady state of the model. For an [explicit time integration](@entry_id:165797) scheme, the growth of these perturbations is governed by the eigenvalues $\lambda$ of the system's Jacobian matrix $J$ (which includes gradients from both the physics and ML components) and the integrator's **[stability function](@entry_id:178107)** $R(z)$. The scheme is linearly stable if, for all eigenvalues, the condition $|R(\Delta t \lambda)| \le 1$ is met. This means the scaled eigenvalues $\Delta t \lambda$ must lie within the integrator's stability region. This imposes a constraint on the maximum allowable time step $\Delta t$.

**Energy stability** is a stronger, nonlinear criterion. It requires that a physically meaningful, positive-definite quadratic energy $E(u) = \frac{1}{2} u^{\top} W u$ does not increase over time. For the continuous system, this is often achieved if the physical dynamics $\mathcal{L}$ are energy-conserving (or $W$-skew-symmetric, meaning $u^{\top} W \mathcal{L}(u) = 0$) and the subgrid ML parameterization $\mathcal{C}_{\theta}$ is energy-dissipative ($u^{\top} W \mathcal{C}_{\theta}(u) \le 0$). However, a standard explicit time integrator does not automatically preserve this property. Ensuring discrete [energy stability](@entry_id:748991), $E(u^{n+1}) \le E(u^n)$, typically requires either a sufficiently small time step $\Delta t$ or the use of specialized integrators like Strong Stability Preserving (SSP) schemes. Enforcing energy dissipation in the ML model is a powerful method for guaranteeing the stability of the coupled system.

#### Principle 3: Consistency at Equilibria

A well-behaved hybrid model should "do no harm." If the original physics-based model already performs well in certain regimes—for example, it correctly captures a state of physical equilibrium—the ML correction should not disrupt this. A fixed point, or equilibrium state, $x^{\ast}$ of the original numerical model is one where the numerical tendency vanishes, $\Psi(x^{\ast}) = 0$. For a residual-learning hybrid model with the update $x_{n+1} = x_{n} + \Delta t [\Psi(x_{n}) + \mathcal{C}(x_{n};\theta)]$, this state $x^{\ast}$ will remain a fixed point if and only if the learned correction also vanishes at that state: $\mathcal{C}(x^{\ast};\theta) = 0$. This is a crucial consistency check. Since the true [model error](@entry_id:175815) is zero at a perfect equilibrium, a well-trained residual model should naturally learn to produce a near-zero correction in this regime .

### Mechanisms for Coupling Physics and Machine Learning

The practical integration of a physics-based tendency operator $L(u)$ and an ML parameterization $G(u)$ within a single time step is typically handled using **operator splitting** methods . These methods decompose the evolution over a time step $\Delta t$ into a sequence of simpler sub-steps.

An **additive update** is the most straightforward approach, where the tendencies are simply summed: $u^{n+1} = u^n + \Delta t [L(u^n) + G(u^n)]$ for a Forward Euler scheme. A major challenge here is stiffness. ML parameterizations can often be very stiff (i.e., their Jacobians have eigenvalues with large negative real parts), which would impose a prohibitively small time step on a fully [explicit scheme](@entry_id:1124773). A powerful solution is to use an **Implicit-Explicit (IMEX)** scheme, which treats the stiff ML term $G(u)$ implicitly (e.g., $u^{n+1} = u^n + \Delta t [L(u^n) + G(u^{n+1})]$) while treating the non-stiff physics term $L(u)$ explicitly. This can dramatically enlarge the [stable time step](@entry_id:755325).

**Sequential (or Lie) splitting** applies the operators one after the other: $u^{n+1} = \varphi_G^{\Delta t} \circ \varphi_L^{\Delta t} (u^n)$, where $\varphi_L^{\Delta t}$ and $\varphi_G^{\Delta t}$ are numerical solvers for the $L$ and $G$ subsystems, respectively. This method is generally first-order accurate in $\Delta t$. The splitting error is proportional to the **commutator** of the operators, $[L,G] = LG - GL$. If the operators commute, the splitting is exact.

**Symmetric (or Strang) splitting** offers higher accuracy. It applies the operators in a symmetric sequence, such as $u^{n+1} = \varphi_G^{\Delta t/2} \circ \varphi_L^{\Delta t} \circ \varphi_G^{\Delta t/2} (u^n)$. This symmetric composition cancels the leading-order error term, resulting in a method that is second-order accurate. It is crucial to note that splitting itself does not confer stability; if a sub-solver (e.g., an explicit update for a stiff $G$) is unstable, the entire scheme will be unstable. However, a major advantage of splitting is modularity: if each sub-solver $\varphi_L^{\Delta t}$ and $\varphi_G^{\Delta t}$ is designed to preserve a [physical invariant](@entry_id:194750) (like energy), their composition will also preserve that invariant.

### Advanced Topics and Grand Challenges

While the principles and mechanisms described above provide a roadmap for building hybrid models, several advanced challenges remain at the forefront of research.

#### Scale Awareness

The division between resolved and unresolved scales is not fixed; it is a function of the model's grid spacing $\Delta$. As the grid is refined ( $\Delta \to 0$), motions that were previously subgrid become resolved. A physically consistent parameterization must be **scale-aware**: its behavior must explicitly depend on $\Delta$ such that its effect diminishes as more scales are resolved. In the limit $\Delta \to 0$, the subgrid terms vanish, and so must the parameterization . A scale-unaware parameterization, one designed for a single fixed resolution, can lead to **[double counting](@entry_id:260790)** when the grid is refined, where the parameterization continues to remove energy from scales that are now explicitly represented by the dynamical core. A formal way to enforce scale awareness is to design the ML closure such that it only acts on the unresolved part of the state, for instance by projecting its output onto high-wavenumber modes in Fourier space.

#### Out-of-Distribution Generalization

Perhaps the greatest challenge for hybrid models, especially in climate science, is **out-of-distribution (OOD) generalization**: the ability to perform skillfully in a regime significantly different from the one it was trained on. A model trained on the historical climate must be able to make reliable predictions for a future, warmer climate. The failure to generalize can be categorized into distinct types of [dataset shift](@entry_id:922271) .

-   **Covariate Shift**: The distribution of inputs changes ($P_{\text{test}}(\mathbf{x}) \neq P_{\text{train}}(\mathbf{x})$), but the physical relationship between inputs and outputs is invariant ($P_{\text{test}}(y | \mathbf{x}) = P_{\text{train}}(y | \mathbf{x})$). For example, climate change might alter the frequency of certain weather patterns, but the [subgrid physics](@entry_id:755602) within a given weather pattern remains the same.
-   **Concept Shift**: The fundamental relationship between inputs and outputs changes ($P_{\text{test}}(y | \mathbf{x}) \neq P_{\text{train}}(y | \mathbf{x})$). For example, changes in aerosol concentrations could alter cloud microphysics such that the same large-scale atmospheric state produces a different convective response. This is the most challenging type of shift, as the model has learned a relationship that is no longer correct.
-   **Label Shift**: For [classification tasks](@entry_id:635433), this occurs when the [marginal distribution](@entry_id:264862) of the output labels changes ($P_{\text{test}}(y) \neq P_{\text{train}}(y)$), but the input features corresponding to a given label are stable. For example, the frequency of different weather regimes might change in a future climate.

The "gray-box" residual modeling approach, which anchors the model to a physical core $\mathcal{M}_{\text{phys}}$, offers a promising path toward better OOD generalization. By relying on established physical laws that are expected to hold across climate regimes and using ML only to learn the (hopefully smaller) residual component, the model may be less susceptible to failure when confronted with novel conditions. Building hybrid models that are robust to concept shifts remains a grand challenge for the field.