{
    "hands_on_practices": [
        {
            "introduction": "At the heart of data-driven weather prediction lies data assimilation, the process of optimally combining model forecasts with real-world observations. This first exercise breaks down this complex process into its most fundamental form: a one-dimensional Bayesian update. By deriving the posterior distribution from first principles, you will gain a clear understanding of how a forecast (the prior) and an observation (the likelihood) are weighted by their respective uncertainties to produce a statistically optimal analysis (the posterior) .",
            "id": "4030133",
            "problem": "Consider a one-dimensional data assimilation step within a Numerical Weather Prediction (NWP) system for a single grid-point atmospheric scalar state, denoted by $x$. The background (prior) information about $x$ is modeled as a Gaussian random variable $x_b \\sim \\mathcal{N}(\\mu_b,\\sigma_b^2)$, where $x_b$ represents the background forecast, $\\mu_b$ is its mean, and $\\sigma_b^2$ is its error variance. A single collocated observation $y$ of the same state is available through a linear measurement model $y = x + \\epsilon$, where the observation error $\\epsilon$ is Gaussian with $\\epsilon \\sim \\mathcal{N}(0,\\sigma_o^2)$ and independent of the background error. Assume all distributions and parameters are correctly specified, and that $x$ is the latent true state to be inferred.\n\nStarting only from Bayes’ theorem and the definitions of the Gaussian prior and Gaussian likelihood, derive the posterior distribution $p(x \\mid y)$, explicitly obtaining closed-form expressions for the posterior mean $\\mu_a$ and the posterior variance $\\sigma_a^2$. Express the posterior mean as a convex combination of the background mean and the observation, $\\mu_a = w_b \\mu_b + w_o y$, and provide the analytic expressions for the weights $w_b$ and $w_o$ in terms of $\\sigma_b^2$ and $\\sigma_o^2$. State the final answers for $\\mu_a$, $\\sigma_a^2$, $w_b$, and $w_o$ as explicit functions of $\\mu_b$, $y$, $\\sigma_b^2$, and $\\sigma_o^2$. No numerical evaluation is required, and no rounding is needed. The final answer must be given as a single row vector containing $\\mu_a$, $\\sigma_a^2$, $w_b$, and $w_o$ in that order.",
            "solution": "The problem presents a standard Bayesian inference task in the context of data assimilation, often referred to as optimal interpolation in a scalar case. The problem is scientifically grounded, mathematically well-posed, completely specified, and objective. It is a valid problem.\n\nWe are asked to derive the posterior distribution $p(x | y)$ for the state $x$ given an observation $y$. The derivation begins with Bayes' theorem:\n$$\np(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}\n$$\nThe term $p(y)$ is a normalization constant that does not depend on $x$. Therefore, we can work with proportionality:\n$$\np(x \\mid y) \\propto p(y \\mid x) p(x)\n$$\n\nFirst, we define the prior distribution, $p(x)$. The problem states that the background information about $x$ is modeled as a Gaussian random variable with mean $\\mu_b$ and variance $\\sigma_b^2$. Thus, the prior distribution for the true state $x$ is:\n$$\np(x) = \\mathcal{N}(x \\mid \\mu_b, \\sigma_b^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} \\exp\\left( -\\frac{(x - \\mu_b)^2}{2\\sigma_b^2} \\right)\n$$\n\nNext, we define the likelihood function, $p(y \\mid x)$. The observation model is given by $y = x + \\epsilon$, where the observation error $\\epsilon$ is a zero-mean Gaussian random variable with variance $\\sigma_o^2$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma_o^2)$. This implies that given a true state $x$, the observation $y$ is a Gaussian random variable with mean $x$ and variance $\\sigma_o^2$. The likelihood function is therefore:\n$$\np(y \\mid x) = \\mathcal{N}(y \\mid x, \\sigma_o^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_o^2}} \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_o^2} \\right)\n$$\n\nNow, we substitute these expressions into the proportionality relation for the posterior:\n$$\np(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma_o^2}} \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_o^2} \\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} \\exp\\left( -\\frac{(x - \\mu_b)^2}{2\\sigma_b^2} \\right) \\right]\n$$\nIgnoring the constant multiplicative terms, the posterior is proportional to the exponential terms:\n$$\np(x \\mid y) \\propto \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_o^2} - \\frac{(x - \\mu_b)^2}{2\\sigma_b^2} \\right)\n$$\nThe product of two Gaussian distributions (or their PDFs) results in another Gaussian distribution. We know the posterior $p(x \\mid y)$ will be a Gaussian, which we denote as $\\mathcal{N}(x \\mid \\mu_a, \\sigma_a^2)$, having the form:\n$$\np(x \\mid y) \\propto \\exp\\left( -\\frac{(x - \\mu_a)^2}{2\\sigma_a^2} \\right)\n$$\nTo find the posterior mean $\\mu_a$ and variance $\\sigma_a^2$, we must manipulate the exponent of our derived posterior to match the canonical Gaussian form. This is achieved by the method of completing the square with respect to $x$. Let's analyze the exponent, which we denote as $E(x)$:\n$$\nE(x) = -\\frac{(y - x)^2}{2\\sigma_o^2} - \\frac{(x - \\mu_b)^2}{2\\sigma_b^2}\n$$\nWe expand the quadratic terms inside the parentheses:\n$$\nE(x) = -\\frac{1}{2}\\left( \\frac{y^2 - 2yx + x^2}{\\sigma_o^2} + \\frac{x^2 - 2x\\mu_b + \\mu_b^2}{\\sigma_b^2} \\right)\n$$\nWe collect terms based on powers of $x$:\n$$\nE(x) = -\\frac{1}{2}\\left[ x^2 \\left(\\frac{1}{\\sigma_o^2} + \\frac{1}{\\sigma_b^2}\\right) - 2x \\left(\\frac{y}{\\sigma_o^2} + \\frac{\\mu_b}{\\sigma_b^2}\\right) + \\left(\\frac{y^2}{\\sigma_o^2} + \\frac{\\mu_b^2}{\\sigma_b^2}\\right) \\right]\n$$\nThe general form of the exponent for a Gaussian $\\mathcal{N}(x \\mid \\mu_a, \\sigma_a^2)$ is:\n$$\n-\\frac{(x - \\mu_a)^2}{2\\sigma_a^2} = -\\frac{1}{2\\sigma_a^2}(x^2 - 2x\\mu_a + \\mu_a^2) = -\\frac{1}{2}\\left[ x^2\\left(\\frac{1}{\\sigma_a^2}\\right) - 2x\\left(\\frac{\\mu_a}{\\sigma_a^2}\\right) + \\frac{\\mu_a^2}{\\sigma_a^2} \\right]\n$$\nBy comparing the coefficients of the $x^2$ term in $E(x)$ with the general form, we can identify the inverse of the posterior variance, $\\sigma_a^2$:\n$$\n\\frac{1}{\\sigma_a^2} = \\frac{1}{\\sigma_o^2} + \\frac{1}{\\sigma_b^2} = \\frac{\\sigma_b^2 + \\sigma_o^2}{\\sigma_o^2 \\sigma_b^2}\n$$\nSolving for $\\sigma_a^2$ gives the posterior variance:\n$$\n\\sigma_a^2 = \\left( \\frac{\\sigma_b^2 + \\sigma_o^2}{\\sigma_o^2 \\sigma_b^2} \\right)^{-1} = \\frac{\\sigma_o^2 \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nNext, by comparing the coefficients of the linear $x$ term, we find:\n$$\n\\frac{\\mu_a}{\\sigma_a^2} = \\frac{y}{\\sigma_o^2} + \\frac{\\mu_b}{\\sigma_b^2}\n$$\nSolving for the posterior mean $\\mu_a$:\n$$\n\\mu_a = \\sigma_a^2 \\left( \\frac{y}{\\sigma_o^2} + \\frac{\\mu_b}{\\sigma_b^2} \\right)\n$$\nSubstituting our expression for $\\sigma_a^2$:\n$$\n\\mu_a = \\left( \\frac{\\sigma_o^2 \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} \\right) \\left( \\frac{y\\sigma_b^2 + \\mu_b\\sigma_o^2}{\\sigma_o^2 \\sigma_b^2} \\right)\n$$\nThe term $(\\sigma_o^2 \\sigma_b^2)$ cancels out, yielding:\n$$\n\\mu_a = \\frac{y\\sigma_b^2 + \\mu_b\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nThe problem requires expressing the posterior mean as a convex combination $\\mu_a = w_b \\mu_b + w_o y$. We can rearrange our expression for $\\mu_a$ to match this form:\n$$\n\\mu_a = \\left( \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2} \\right) \\mu_b + \\left( \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} \\right) y\n$$\nBy direct comparison with $\\mu_a = w_b \\mu_b + w_o y$, we identify the weights $w_b$ and $w_o$:\n$$\nw_b = \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\n$$\nw_o = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nThe sum of the weights is $w_b + w_o = \\frac{\\sigma_o^2 + \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} = 1$. Since variances are non-negative, the weights are also non-negative, confirming this is a proper convex combination.\n\nThe final required expressions are:\n1.  Posterior mean $\\mu_a = \\frac{\\mu_b \\sigma_o^2 + y \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}$\n2.  Posterior variance $\\sigma_a^2 = \\frac{\\sigma_b^2 \\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}$\n3.  Weight on background mean $w_b = \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}$\n4.  Weight on observation $w_o = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}$\n\nThese are the required analytical forms.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\mu_b \\sigma_o^2 + y \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} & \\frac{\\sigma_b^2 \\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2} & \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2} & \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While data assimilation helps us correct model errors, it cannot overcome the atmosphere's inherent unpredictability. This chaotic nature is quantified by the maximal Lyapunov exponent, which measures the rate at which small initial errors grow exponentially. This practice guides you in connecting this abstract mathematical concept to the tangible and critical metric of error doubling time, providing a fundamental insight into the limits of forecast skill for any prediction system, whether physics-based or data-driven .",
            "id": "4030143",
            "problem": "A data-driven atmospheric forecasting system is trained to emulate the short-time evolution of synoptic-scale states within the framework of Numerical Weather Prediction (NWP). Along a representative trajectory in state space, the maximal Lyapunov exponent is empirically estimated from the logarithmic growth of infinitesimal perturbations as follows: the maximal Lyapunov exponent $\\lambda$ is defined by\n$$\n\\lambda \\equiv \\lim_{t \\to \\infty} \\frac{1}{t} \\ln \\frac{\\|\\delta \\mathbf{x}(t)\\|}{\\|\\delta \\mathbf{x}(0)\\|},\n$$\nwhere $\\delta \\mathbf{x}(t)$ denotes a tangent-space perturbation to the model state at time $t$, and $\\|\\cdot\\|$ is a norm on state space. Assume the linear tangent approximation holds over short lead times so that small perturbations evolve approximately as $\\|\\delta \\mathbf{x}(t)\\| \\approx \\|\\delta \\mathbf{x}(0)\\| \\exp(\\lambda t)$, with growth dominated by the maximal Lyapunov exponent. For a synoptic regime in which $\\lambda = 0.5\\,\\mathrm{day}^{-1}$, derive from the definition the analytic expression for the doubling time $T_{d}$, defined as the time for which $\\|\\delta \\mathbf{x}(T_{d})\\| = 2 \\|\\delta \\mathbf{x}(0)\\|$, and then compute its numerical value. \n\nExpress your final answer in days and round to four significant figures. \n\nIn your reasoning, interpret the physical implication of this doubling time for short-range forecasts over lead times from $0$ to $2$ days in a data-driven model, but ensure the final reported answer is only the requested number.",
            "solution": "The problem requires the derivation of the analytical expression for the error doubling time, $T_{d}$, and its numerical calculation for a given maximal Lyapunov exponent, $\\lambda$. Subsequently, an interpretation of this value in the context of short-range weather forecasting is requested.\n\nThe starting point is the definition of the doubling time, $T_{d}$, which is the time at which the magnitude of an infinitesimal perturbation, $\\|\\delta \\mathbf{x}(t)\\|$, has grown to twice its initial value, $\\|\\delta \\mathbf{x}(0)\\|$. This is formally stated as:\n$$\n\\|\\delta \\mathbf{x}(T_{d})\\| = 2 \\|\\delta \\mathbf{x}(0)\\|\n$$\nThe problem states that for short lead times, the evolution of such a perturbation is well-approximated by the linear tangent model, where the growth is dominated by the maximal Lyapunov exponent, $\\lambda$. This approximation is given by:\n$$\n\\|\\delta \\mathbf{x}(t)\\| \\approx \\|\\delta \\mathbf{x}(0)\\| \\exp(\\lambda t)\n$$\nTo find the doubling time, $T_{d}$, we substitute $t=T_{d}$ into this approximation:\n$$\n\\|\\delta \\mathbf{x}(T_{d})\\| \\approx \\|\\delta \\mathbf{x}(0)\\| \\exp(\\lambda T_{d})\n$$\nWe can now equate the definition of the doubling time with this growth approximation. This establishes a relationship that allows for the determination of $T_{d}$:\n$$\n2 \\|\\delta \\mathbf{x}(0)\\| = \\|\\delta \\mathbf{x}(0)\\| \\exp(\\lambda T_{d})\n$$\nAssuming the initial perturbation is non-zero, i.e., $\\|\\delta \\mathbf{x}(0)\\| > 0$, we can divide both sides of the equation by $\\|\\delta \\mathbf{x}(0)\\|$:\n$$\n2 = \\exp(\\lambda T_{d})\n$$\nTo solve for $T_{d}$, we take the natural logarithm of both sides of the equation:\n$$\n\\ln(2) = \\ln(\\exp(\\lambda T_{d}))\n$$\nUsing the property that the natural logarithm is the inverse function of the exponential function, $\\ln(\\exp(x)) = x$, we get:\n$$\n\\ln(2) = \\lambda T_{d}\n$$\nIsolating $T_{d}$ yields the analytical expression for the doubling time:\n$$\nT_{d} = \\frac{\\ln(2)}{\\lambda}\n$$\nThis expression robustly links the doubling time of initial errors to the system's maximal Lyapunov exponent, a fundamental measure of its chaoticity.\n\nNext, we compute the numerical value of $T_{d}$ using the provided value for the maximal Lyapunov exponent for the synoptic regime, $\\lambda = 0.5\\,\\mathrm{day}^{-1}$. Substituting this into our derived expression:\n$$\nT_{d} = \\frac{\\ln(2)}{0.5\\,\\mathrm{day}^{-1}}\n$$\nUsing the value for the natural logarithm of $2$, $\\ln(2) \\approx 0.693147...$, the calculation is:\n$$\nT_{d} \\approx \\frac{0.693147}{0.5} \\, \\mathrm{days} = 1.386294 \\, \\mathrm{days}\n$$\nThe problem requires the answer to be rounded to four significant figures. Therefore, the numerical value for the doubling time is:\n$$\nT_{d} \\approx 1.386 \\, \\mathrm{days}\n$$\n\nThe physical implication of this doubling time for short-range forecasts, particularly for lead times from $0$ to $2$ days, is profound. A doubling time of approximately $1.4$ days signifies the fundamental limit of predictability inherent in the atmospheric system itself. It means that any small, unavoidable error in the initial conditions of a forecast—whether from measurement inaccuracies, data assimilation imperfections, or representation errors in the data-driven model—will, on average, double in magnitude every $1.4$ days. Within the $0$ to $2$ day forecast window, an initial error will have more than doubled by the end of day two. Specifically, at $t=2$ days, the error amplification factor is $\\exp(\\lambda t) = \\exp(0.5 \\times 2) = \\exp(1) \\approx 2.718$. This rapid, exponential error growth is the primary reason why weather forecast skill degrades over time. For a data-driven model, this implies that even if the model were a perfect emulator of the true atmospheric dynamics, its predictive utility is still fundamentally constrained by the chaotic nature of the system, which amplifies initial uncertainties at this characteristic rate. The concept of a finite doubling time underscores why even sophisticated NWP systems cannot produce skillful forecasts beyond a certain time horizon (typically $10$ to $14$ days for synoptic-scale weather), as initial errors inevitably grow to saturate the system.",
            "answer": "$$\n\\boxed{1.386}\n$$"
        },
        {
            "introduction": "Modern data-driven prediction often relies on complex models like Convolutional Neural Networks (CNNs), which can be perceived as 'black boxes'. To trust and improve these models, we must understand what they have learned. This coding exercise introduces saliency maps, a powerful technique for visualizing the gradient of the model's output with respect to its input, revealing which features are most influential for a prediction. By implementing this and relating it to physical principles, you will practice the crucial skill of model interpretation, ensuring that a data-driven model is not just accurate, but also scientifically sound .",
            "id": "4030093",
            "problem": "You are given a simplified, scientifically plausible Convolutional Neural Network (CNN) architecture for scalar rainfall prediction from gridded atmospheric input fields in the context of numerical weather prediction and climate modeling. The CNN uses cross-correlation with \"same\" padding, Rectified Linear Unit (ReLU) activation, and global average pooling to produce a single scalar prediction. Your task is to compute saliency maps for this CNN by evaluating the gradient of the predicted rainfall with respect to the input fields, and to summarize the physically meaningful concentration of saliency within regions of high moisture and positive convergence.\n\nFundamental base and definitions:\n- Convolutional Neural Network (CNN) is defined as follows. Let the input be $x \\in \\mathbb{R}^{C \\times H \\times W}$, with $C$ channels and spatial dimensions $H \\times W$. Here, $x^{(1)}$ is column-integrated water vapor in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$, $x^{(2)}$ is near-surface temperature anomaly in $\\mathrm{K}$, and $x^{(3)}$ is horizontal convergence in $\\mathrm{s}^{-1}$. You may assume $C = 3$, $H = W = 16$.\n- The CNN has $F$ filters (here $F = 2$). The pre-activation map for filter $f$ at location $(i,j)$ is\n$$\nz^{(f)}_{i,j} = \\sum_{c=1}^{C} \\sum_{u=1}^{k} \\sum_{v=1}^{k} K^{(f)}_{c,u,v}\\,x^{(c)}_{i+u',j+v'} + b^{(f)},\n$$\nwhere $K^{(f)} \\in \\mathbb{R}^{C \\times k \\times k}$ is the filter kernel, $b^{(f)} \\in \\mathbb{R}$ is a bias, $k=3$, indices $(u',v')$ implement \"same\" padding as in cross-correlation (no kernel rotation in the forward pass), and padding ensures the sum is well-defined for all $(i,j)$.\n- The activation is $a^{(f)}_{i,j} = \\max(0, z^{(f)}_{i,j})$, and the global average pooled feature is\n$$\ng^{(f)} = \\frac{1}{H W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} a^{(f)}_{i,j}.\n$$\n- The scalar rainfall prediction is\n$$\n\\hat{y} = \\sum_{f=1}^{F} w_f\\, g^{(f)} + b_{\\mathrm{out}},\n$$\nexpressed in $\\mathrm{mm}\\,\\mathrm{h}^{-1}$.\n\nSaliency and physical interpretation:\n- Define the saliency map for channel $c$ as\n$$\nS^{(c)}_{i,j} = \\frac{\\partial \\hat{y}}{\\partial x^{(c)}_{i,j}}.\n$$\n- Define the physically meaningful mask\n$$\nM_{i,j} = \\begin{cases}\n1 & \\text{if } x^{(1)}_{i,j} > q_{\\mathrm{th}} \\text{ and } x^{(3)}_{i,j} > 0, \\\\\n0 & \\text{otherwise},\n\\end{cases}\n$$\nwith threshold $q_{\\mathrm{th}} = 25$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$).\n- Define the saliency concentration fraction\n$$\n\\phi = \\frac{\\sum_{i=1}^{H} \\sum_{j=1}^{W} \\sum_{c=1}^{C} \\left| S^{(c)}_{i,j} \\right|\\, M_{i,j}}{\\sum_{i=1}^{H} \\sum_{j=1}^{W} \\sum_{c=1}^{C} \\left| S^{(c)}_{i,j} \\right|},\n$$\nwhich measures the fraction of total saliency magnitude that lies in high-moisture, convergent regions ($\\phi \\in [0,1]$, dimensionless).\n\nNetwork specification (fixed and known):\n- Filter $f=1$ (\"rain-support\"):\n  - $K^{(1)}_{q} = 0.02 \\times \\mathbf{1}_{3 \\times 3}$,\n  - $K^{(1)}_{T'} = 0.01 \\times \\mathbf{1}_{3 \\times 3}$,\n  - $K^{(1)}_{c} = 500 \\times \\mathbf{1}_{3 \\times 3}$,\n  - $b^{(1)} = 0$,\n  - $w_1 = 20$.\n- Filter $f=2$ (\"front detector\"):\n  - $K^{(2)}_{q} = \\mathbf{0}_{3 \\times 3}$,\n  - $K^{(2)}_{T'} = 0.5 \\times \\begin{bmatrix} -1 & 0 & 1 \\\\ -1 & 0 & 1 \\\\ -1 & 0 & 1 \\end{bmatrix}$,\n  - $K^{(2)}_{c} = \\mathbf{0}_{3 \\times 3}$,\n  - $b^{(2)} = 0$,\n  - $w_2 = 5$.\n- Output bias: $b_{\\mathrm{out}} = 0$.\n\nTest suite:\nWork on a spatial grid of size $H=W=16$ with indices $i,j \\in \\{0,1,\\dots,15\\}$ and construct the following $4$ test cases for $(x^{(1)}, x^{(2)}, x^{(3)})$:\n- Case $1$ (happy path, central moist-convergent plume):\n  - $x^{(1)}_{i,j} = 10 + 30 \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$,\n  - $x^{(2)}_{i,j} = 2 \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$,\n  - $x^{(3)}_{i,j} = 10^{-3} \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$.\n- Case $2$ (boundary condition, corner plume):\n  - $x^{(1)}_{i,j} = 10 + 30 \\, G(i,j; c_x=3, c_y=3, \\sigma=2)$,\n  - $x^{(2)}_{i,j} = 2 \\, G(i,j; c_x=3, c_y=3, \\sigma=2)$,\n  - $x^{(3)}_{i,j} = 10^{-3} \\, G(i,j; c_x=3, c_y=3, \\sigma=2)$.\n- Case $3$ (edge case, uniform fields):\n  - $x^{(1)}_{i,j} = 20$,\n  - $x^{(2)}_{i,j} = 0$,\n  - $x^{(3)}_{i,j} = 0$.\n- Case $4$ (counter-physical convergence, ring moisture and cold anomaly):\n  - $x^{(1)}_{i,j} = 10 + 35 \\left[ G(i,j; c_x=8, c_y=8, \\sigma=3) - 0.7\\, G(i,j; c_x=8, c_y=8, \\sigma=1.5) \\right]$ clipped below at $0$,\n  - $x^{(2)}_{i,j} = -2 \\, G(i,j; c_x=8, c_y=8, \\sigma=2.5)$,\n  - $x^{(3)}_{i,j} = -10^{-3} \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$.\n- Here the Gaussian is defined by\n$$\nG(i,j; c_x, c_y, \\sigma) = \\exp\\left( -\\frac{(i-c_x)^2 + (j-c_y)^2}{2\\sigma^2} \\right).\n$$\n\nTasks:\n1. Implement the specified CNN forward map to obtain $\\hat{y}$ for each case.\n2. Compute the saliency map $S^{(c)}_{i,j} = \\partial \\hat{y} / \\partial x^{(c)}_{i,j}$ exactly by applying the chain rule and the definition of cross-correlation. Use the fact that the derivative of ReLU is $1$ for positive pre-activation and $0$ otherwise, and that the derivative of global average pooling is a constant scaling by $1/(H W)$. For the backward mapping through cross-correlation, use a correlation of the upstream gradient with the kernel rotated by $180^\\circ$ per channel.\n3. Compute the mask $M_{i,j}$ using $q_{\\mathrm{th}} = 25$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$), and the concentration fraction $\\phi$ as defined.\n4. For each case, produce two quantities:\n   - The predicted rainfall $\\hat{y}$, expressed in $\\mathrm{mm}\\,\\mathrm{h}^{-1}$ and rounded to three decimal places.\n   - The dimensionless saliency concentration fraction $\\phi$, rounded to three decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\hat{y}_1, \\phi_1, \\hat{y}_2, \\phi_2, \\hat{y}_3, \\phi_3, \\hat{y}_4, \\phi_4]$, where subscript indices correspond to the numbered test cases above. Each number must be formatted with exactly three decimal places.\n\nEnsure scientific realism: the CNN and fields must be implemented as specified, and all computations must be deterministic and self-contained without external data. Angles are not used in this problem. Units used are $\\mathrm{kg}\\,\\mathrm{m}^{-2}$, $\\mathrm{K}$, $\\mathrm{s}^{-1}$ for inputs and $\\mathrm{mm}\\,\\mathrm{h}^{-1}$ for outputs.",
            "solution": "The user-provided problem is a well-defined computational exercise in the domain of scientific machine learning, specifically for interpreting a Convolutional Neural Network (CNN) used in a simplified weather prediction context. The problem is scientifically grounded, mathematically consistent, and all necessary parameters and definitions are provided for a unique solution to be determined. We will therefore proceed with a full solution.\n\nThe solution involves three main computational steps:\n1.  A **forward pass** through the CNN to compute the scalar rainfall prediction $\\hat{y}$ for each input atmospheric state.\n2.  A **backward pass** using the chain rule to compute the gradient of the prediction $\\hat{y}$ with respect to each input variable, yielding the saliency maps $S^{(c)}$.\n3.  An **analysis step** to compute the saliency concentration fraction $\\phi$ based on a physically motivated mask $M$.\n\nWe will now detail the mathematical formulation for each step.\n\n### 1. Forward Propagation\n\nThe forward pass computes the predicted rainfall $\\hat{y}$ from the input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, where $C=3$, $H=16$, $W=16$. The input channels are $x^{(1)}$ (water vapor), $x^{(2)}$ (temperature anomaly), and $x^{(3)}$ (convergence).\n\nFirst, for each of the $F=2$ filters, we compute a pre-activation map $z^{(f)}$. This is achieved by performing a multi-channel 2D cross-correlation of the input $x$ with the filter kernel $K^{(f)} \\in \\mathbb{R}^{C \\times k \\times k}$ ($k=3$), followed by the addition of a bias term $b^{(f)}$. The operation uses \"same\" padding, meaning the input is padded with zeros such that the output map has the same spatial dimensions $H \\times W$ as the input maps.\n\nThe pre-activation for filter $f$ at grid point $(i,j)$ is:\n$$\nz^{(f)}_{i,j} = b^{(f)} + \\sum_{c=1}^{C} (K^{(f)}_c \\star x^{(c)})_{i,j}\n$$\nwhere $\\star$ denotes 2D cross-correlation and $K^{(f)}_c$ is the kernel slice for channel $c$.\n\nSecond, a Rectified Linear Unit (ReLU) activation function is applied element-wise to the pre-activation maps. This introduces non-linearity into the model.\n$$\na^{(f)}_{i,j} = \\max(0, z^{(f)}_{i,j})\n$$\n\nThird, each activation map $a^{(f)}$ is reduced to a single scalar feature $g^{(f)}$ via global average pooling.\n$$\ng^{(f)} = \\frac{1}{H W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} a^{(f)}_{i,j}\n$$\n\nFinally, the scalar rainfall prediction $\\hat{y}$ (in $\\mathrm{mm}\\,\\mathrm{h}^{-1}$) is computed as a weighted sum of the features $g^{(f)}$, plus an output bias $b_{\\mathrm{out}}$.\n$$\n\\hat{y} = b_{\\mathrm{out}} + \\sum_{f=1}^{F} w_f g^{(f)}\n$$\nAll network parameters ($K^{(f)}$, $b^{(f)}$, $w_f$, $b_{\\mathrm{out}}$) are provided in the problem statement.\n\n### 2. Backward Propagation for Saliency Maps\n\nThe saliency map for an input channel $c$, $S^{(c)}$, is defined as the partial derivative of the output prediction $\\hat{y}$ with respect to the input map $x^{(c)}$. We compute this using the chain rule, propagating gradients backward from the output to the input.\n\n1.  **Gradient at the output layer**: The derivative of $\\hat{y}$ with respect to the pooled features $g^{(f)}$ is simply the output weight $w_f$.\n    $$\n    \\frac{\\partial \\hat{y}}{\\partial g^{(f)}} = w_f\n    $$\n\n2.  **Gradient at the pooling layer**: The derivative of $g^{(f)}$ with respect to a single element of the activation map $a^{(f)}_{i,j}$ is constant due to the averaging operation.\n    $$\n    \\frac{\\partial g^{(f)}}{\\partial a^{(f)}_{i,j}} = \\frac{1}{H W}\n    $$\n    By the chain rule, the gradient of $\\hat{y}$ with respect to $a^{(f)}_{i,j}$ is a constant map:\n    $$\n    \\frac{\\partial \\hat{y}}{\\partial a^{(f)}_{i,j}} = \\frac{\\partial \\hat{y}}{\\partial g^{(f)}} \\frac{\\partial g^{(f)}}{\\partial a^{(f)}_{i,j}} = \\frac{w_f}{H W}\n    $$\n\n3.  **Gradient at the activation layer**: The derivative of the ReLU activation $a^{(f)}_{i,j}$ with respect to its input $z^{(f)}_{i,j}$ is $1$ if $z^{(f)}_{i,j} > 0$ and $0$ otherwise. We denote this using the Heaviside step function $H(\\cdot)$.\n    $$\n    \\frac{\\partial a^{(f)}_{i,j}}{\\partial z^{(f)}_{i,j}} = H(z^{(f)}_{i,j}) = \\begin{cases} 1 & \\text{if } z^{(f)}_{i,j} > 0 \\\\ 0 & \\text{if } z^{(f)}_{i,j} \\le 0 \\end{cases}\n    $$\n    The upstream gradient for the pre-activation map is therefore:\n    $$\n    \\frac{\\partial \\hat{y}}{\\partial z^{(f)}_{i,j}} = \\frac{\\partial \\hat{y}}{\\partial a^{(f)}_{i,j}} \\frac{\\partial a^{(f)}_{i,j}}{\\partial z^{(f)}_{i,j}} = \\frac{w_f}{HW} H(z^{(f)}_{i,j})\n    $$\n    Let this upstream gradient map be denoted $\\delta z^{(f)}$.\n\n4.  **Gradient at the cross-correlation layer**: The final step is to find the gradient with respect to the input $x^{(c)}$. The pre-activation $z^{(f)}$ is a sum of cross-correlations over the input channels. The derivative of a cross-correlation operation with respect to its input is a convolution operation with the original kernel.\n    $$\n    S^{(c)} = \\frac{\\partial \\hat{y}}{\\partial x^{(c)}} = \\sum_{f=1}^{F} \\frac{\\partial z^{(f)}}{\\partial x^{(c)}} \\frac{\\partial \\hat{y}}{\\partial z^{(f)}} = \\sum_{f=1}^{F} (\\delta z^{(f)} \\circledast K^{(f)}_c)\n    $$\n    where $\\circledast$ denotes 2D convolution and $K^{(f)}_c$ is the kernel for filter $f$ and channel $c$. This operation is equivalent to cross-correlating the upstream gradient map $\\delta z^{(f)}$ with the 180-degree-rotated kernel $K^{(f)}_{c, \\text{rot180}}$. The saliency map for each channel $c$ is the sum of contributions from all filters $f$.\n\n### 3. Saliency Concentration Fraction\n\nThe fraction $\\phi$ quantifies how much of the model's sensitivity (saliency) is focused on regions that are physically conducive to rainfall.\n\nFirst, a binary mask $M \\in \\{0,1\\}^{H \\times W}$ is created. It identifies grid points with both high atmospheric moisture and positive convergence, which are prerequisites for many types of precipitation.\n$$\nM_{i,j} = \\begin{cases} 1 & \\text{if } x^{(1)}_{i,j} > q_{\\mathrm{th}} \\text{ and } x^{(3)}_{i,j} > 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nwhere the moisture threshold is $q_{\\mathrm{th}} = 25 \\, \\mathrm{kg}\\,\\mathrm{m}^{-2}$.\n\nThe saliency concentration fraction $\\phi$ is then calculated as the ratio of the total absolute saliency within the masked region to the total absolute saliency over the entire domain.\n$$\n\\phi = \\frac{\\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W} |S^{(c)}_{i,j}| M_{i,j}}{\\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W} |S^{(c)}_{i,j}|}\n$$\nA value of $\\phi$ close to $1$ indicates that the model primarily uses information from physically relevant regions to make its prediction, suggesting that it has learned a physically meaningful relationship. A value close to $0$ suggests the opposite. If the total saliency is zero, $\\phi$ is defined as $0$.\n\nThe implementation will follow these steps for each of the four test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Main function to solve the CNN saliency problem for all test cases.\n    \"\"\"\n    \n    # -------------------\n    # Problem Configuration\n    # -------------------\n    H, W = 16, 16\n    C, F = 3, 2\n    k = 3\n    q_th = 25.0\n\n    # Network parameters\n    kernels = np.zeros((F, C, k, k))\n    # Filter f=1 (index 0)\n    kernels[0, 0, :, :] = 0.02 * np.ones((k, k))  # K_q\n    kernels[0, 1, :, :] = 0.01 * np.ones((k, k))  # K_T'\n    kernels[0, 2, :, :] = 500.0 * np.ones((k, k)) # K_c\n    # Filter f=2 (index 1)\n    kernels[1, 0, :, :] = 0.0 * np.zeros((k, k)) # K_q\n    kernels[1, 1, :, :] = 0.5 * np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]) # K_T'\n    kernels[1, 2, :, :] = 0.0 * np.zeros((k, k)) # K_c\n\n    biases_conv = np.array([0.0, 0.0])\n    weights_out = np.array([20.0, 5.0])\n    bias_out = 0.0\n\n    # -------------------\n    # Helper Functions\n    # -------------------\n    def gaussian(i_grid, j_grid, cx, cy, sigma):\n        return np.exp(-((i_grid - cx)**2 + (j_grid - cy)**2) / (2 * sigma**2))\n\n    # Grid for generating fields\n    i_grid, j_grid = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')\n\n    # Test Case Definitions\n    test_cases = [\n        # Case 1: Central plume\n        (\n            10 + 30 * gaussian(i_grid, j_grid, 8, 8, 2),\n            2 * gaussian(i_grid, j_grid, 8, 8, 2),\n            1e-3 * gaussian(i_grid, j_grid, 8, 8, 2)\n        ),\n        # Case 2: Corner plume\n        (\n            10 + 30 * gaussian(i_grid, j_grid, 3, 3, 2),\n            2 * gaussian(i_grid, j_grid, 3, 3, 2),\n            1e-3 * gaussian(i_grid, j_grid, 3, 3, 2)\n        ),\n        # Case 3: Uniform fields\n        (\n            20 * np.ones((H, W)),\n            0 * np.ones((H, W)),\n            0 * np.ones((H, W))\n        ),\n        # Case 4: Ring moisture, central cold anomaly, divergence\n        (\n            np.maximum(0, 10 + 35 * (gaussian(i_grid, j_grid, 8, 8, 3) - 0.7 * gaussian(i_grid, j_grid, 8, 8, 1.5))),\n            -2 * gaussian(i_grid, j_grid, 8, 8, 2.5),\n            -1e-3 * gaussian(i_grid, j_grid, 8, 8, 2)\n        )\n    ]\n    \n    results = []\n    \n    for case_data in test_cases:\n        x = np.array(case_data)  # Shape (C, H, W)\n\n        # -------------------\n        # 1. Forward Pass\n        # -------------------\n        z_maps = np.zeros((F, H, W))\n        a_maps = np.zeros((F, H, W))\n        g_features = np.zeros(F)\n\n        for f in range(F):\n            z_f = np.full((H, W), biases_conv[f])\n            for c in range(C):\n                z_f += signal.correlate2d(x[c, :, :], kernels[f, c, :, :], mode='same', boundary='fill', fillvalue=0)\n            z_maps[f, :, :] = z_f\n            a_maps[f, :, :] = np.maximum(0, z_f)\n            g_features[f] = np.mean(a_maps[f, :, :])\n\n        y_hat = np.sum(weights_out * g_features) + bias_out\n\n        # -------------------\n        # 2. Backward Pass (Saliency)\n        # -------------------\n        saliency_maps = np.zeros((C, H, W))\n        \n        for f in range(F):\n            # Upstream gradient from output layer, through pooling\n            d_y_hat_d_g_f = weights_out[f]\n            d_g_f_d_a_f = 1.0 / (H * W)\n            d_y_hat_d_a_f = d_y_hat_d_g_f * d_g_f_d_a_f\n            \n            # Upstream gradient through ReLU\n            d_a_f_d_z_f = (z_maps[f, :, :] > 0).astype(float)\n            delta_z_f = d_y_hat_d_a_f * d_a_f_d_z_f\n\n            # Gradient w.r.t input x\n            for c in range(C):\n                # Gradient of cross-correlation is convolution with original kernel\n                saliency_maps[c, :, :] += signal.convolve2d(delta_z_f, kernels[f, c, :, :], mode='same', boundary='fill', fillvalue=0)\n        \n        # -------------------\n        # 3. Saliency Concentration Fraction (phi)\n        # -------------------\n        mask = ((x[0, :, :] > q_th) & (x[2, :, :] > 0)).astype(float)\n        \n        saliency_abs_total = np.sum(np.abs(saliency_maps))\n        \n        numerator = np.sum(np.abs(saliency_maps) * mask[np.newaxis, :, :])\n        \n        phi = 0.0\n        if saliency_abs_total > 1e-12: # Avoid division by zero\n            phi = numerator / saliency_abs_total\n            \n        # Store results\n        results.append(y_hat)\n        results.append(phi)\n        \n    # Format and print the final output\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}