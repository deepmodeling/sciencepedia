{
    "hands_on_practices": [
        {
            "introduction": "Before investing significant time in training a complex machine learning model, it's crucial to assess its computational feasibility. In operational settings like numerical weather prediction, emulators must deliver predictions within a strict time budget. This practice explores the trade-offs between model complexity and computational cost by comparing a simple Multi-Layer Perceptron (MLP) with a more sophisticated Fourier Neural Operator (FNO). By estimating parameter counts and floating-point operations (FLOPs), you will learn how to perform a critical analysis to determine if a proposed architecture meets the stringent performance requirements of an operational environment .",
            "id": "4061530",
            "problem": "A national weather center considers replacing a column physics parameterization with a Machine Learning emulator. Two candidate architectures are proposed: a Multi-Layer Perceptron (MLP) and a one-dimensional Fourier Neural Operator (FNO). The model is run on a horizontally discretized domain with $N_x = 1024$ and $N_y = 1024$, yielding $N_{\\text{col}} = N_x N_y$ independent vertical columns. The vertical grid has $N_z = 72$ levels. Each column provides $F_{\\text{in}} = 10$ input features per level and requires $F_{\\text{out}} = 6$ output features per level.\n\nThe operational constraint allocates a sustained performance budget of $P_{\\text{avail}} = 1.0 \\times 10^{15}$ floating-point operations per second (FLOPS) for physics emulation, with a per-call time budget of $B = 0.2$ seconds. Assume the following widely used operation counting approximations:\n- A fully connected layer mapping $\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ costs approximately $2 n m + m$ floating-point operations (FLOPs) per application (the $+m$ accounts for biases), and nonlinear activation costs are negligible compared to matrix multiplies.\n- A complex Fast Fourier Transform (FFT) of length $n$ requires approximately $5 n \\log_{2}(n)$ FLOPs, and an inverse FFT has the same cost.\n- A complex dot product of length $C$ requires approximately $6 C$ FLOPs for complex multiplications and $2 (C - 1)$ FLOPs for complex additions; for counting simplicity, use $8 C$ FLOPs per output when forming a complex weighted sum over $C$ inputs.\n- Spectral convolution in the FNO uses $K$ complex modes and mixes $C$ input channels to $C$ output channels per mode via complex weighted sums.\n\nArchitectures:\n1. The Multi-Layer Perceptron (MLP) operates identically and independently at each vertical level with shared weights across levels. It has two hidden layers of width $64$ and a final linear layer to $F_{\\text{out}}$. Thus, for each level, the layer sizes are $F_{\\text{in}} = 10 \\to 64 \\to 64 \\to F_{\\text{out}} = 6$.\n2. The one-dimensional Fourier Neural Operator (FNO) acts along the vertical dimension. It consists of:\n   - A pointwise lift linear layer at each level mapping $F_{\\text{in}}$ to $C = 32$ channels.\n   - $L = 3$ Fourier layers. Each layer performs: a forward FFT along the vertical for each of the $C$ channels; a spectral convolution using $K = 16$ complex modes, mixing $C$ inputs to $C$ outputs per mode by complex weighted sums; an inverse FFT for each channel; and a pointwise linear mixing from $C$ to $C$ per level.\n   - A final pointwise projection linear layer at each level mapping $C$ channels to $F_{\\text{out}}$.\n\nTasks:\n- Derive and compute the total parameter counts for the MLP and the FNO given the above specifications.\n- Derive and compute the per-column forward-pass FLOPs for the MLP and the FNO.\n- Compute the total FLOPs per emulator call over the full horizontal domain for each architecture by multiplying the per-column costs by $N_{\\text{col}}$.\n- Let the available FLOPs per call be $P_{\\text{avail}} B$. Define the feasibility fractions $r_{\\text{FNO}} = \\frac{\\text{FLOPs}_{\\text{FNO, total}}}{P_{\\text{avail}} B}$ and $r_{\\text{MLP}} = \\frac{\\text{FLOPs}_{\\text{MLP, total}}}{P_{\\text{avail}} B}$. Compute these two fractions.\n\nRound your final numerical answers for $(r_{\\text{FNO}}, r_{\\text{MLP}})$ to three significant figures and express them as dimensionless decimals. Provide the final pair as a row matrix.",
            "solution": "The user has provided a well-defined computational problem comparing two neural network architectures. Before proceeding to the solution, the problem statement must be validated.\n\n### Step 1: Extract Givens\n- Horizontal domain size: $N_x = 1024$, $N_y = 1024$.\n- Number of vertical columns: $N_{\\text{col}} = N_x N_y$.\n- Vertical grid levels: $N_z = 72$.\n- Input features per level: $F_{\\text{in}} = 10$.\n- Output features per level: $F_{\\text{out}} = 6$.\n- Sustained performance budget: $P_{\\text{avail}} = 1.0 \\times 10^{15}$ FLOPS.\n- Per-call time budget: $B = 0.2$ s.\n- FLOPs for a fully connected layer ($\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$): $2nm + m$.\n- FLOPs for a complex FFT/IFFT of length $n$: $5n \\log_{2}(n)$.\n- FLOPs for a complex weighted sum of $C$ inputs to produce one output: $8C$.\n- MLP architecture: per-level, shared weights, $10 \\to 64 \\to 64 \\to 6$.\n- FNO architecture: acts on vertical dimension, $L=3$ Fourier layers.\n  - Lift layer: pointwise $F_{\\text{in}} \\to C=32$ channels.\n  - Fourier layer: FFT, spectral convolution with $K=16$ modes mixing $C \\to C$ channels, IFFT, pointwise linear layer $C \\to C$.\n  - Projection layer: pointwise $C \\to F_{\\text{out}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a realistic application of machine learning in computational climate science. The architectures (MLP, FNO) are standard, and the specified FLOP counting approximations are common in performance modeling. The problem is well-posed, providing all necessary parameters and definitions for a unique solution. It is objective and free of ambiguity. No flaws from the checklist are present.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe analysis is performed in four stages as requested: parameter counts, per-column FLOPs, total FLOPs, and feasibility fractions.\n\n**1. Parameter Counts**\n\nThe number of trainable parameters ($\\Theta$) is calculated for each architecture.\n\n**Multi-Layer Perceptron (MLP):**\nThe MLP operates per-level with shared weights. We count the parameters for a single per-level network. The architecture consists of three layers: two hidden and one output.\n- Layer 1 ($F_{\\text{in}} \\to 64$): Parameters are weights and biases. $\\Theta_1 = F_{\\text{in}} \\times 64 + 64 = 10 \\times 64 + 64 = 640 + 64 = 704$.\n- Layer 2 ($64 \\to 64$): $\\Theta_2 = 64 \\times 64 + 64 = 4096 + 64 = 4160$.\n- Layer 3 ($64 \\to F_{\\text{out}}$): $\\Theta_3 = 64 \\times F_{\\text{out}} + F_{\\text{out}} = 64 \\times 6 + 6 = 384 + 6 = 390$.\n\nThe total parameter count for the MLP is the sum of parameters from all layers:\n$$ \\Theta_{\\text{MLP}} = \\Theta_1 + \\Theta_2 + \\Theta_3 = 704 + 4160 + 390 = 5254 $$\n\n**Fourier Neural Operator (FNO):**\nThe FNO's parameters consist of those in the lifting layer, the Fourier layers, and the projection layer.\n- Lift Layer (pointwise $F_{\\text{in}} \\to C$): This is a linear layer with weights shared across the $N_z$ levels.\n$$ \\Theta_{\\text{lift}} = F_{\\text{in}} \\times C + C = 10 \\times 32 + 32 = 352 $$\n- Fourier Layers ($L=3$ layers): Each Fourier layer contains parameters for spectral convolution and a pointwise linear transformation.\n  - Spectral Convolution: For each of the $L$ layers, there is a set of complex-valued weights. For each of the $K$ modes, a $C \\times C$ complex weight matrix is used. A complex number requires $2$ real numbers for storage.\n  $$ \\Theta_{\\text{spec}} = 2 \\times K \\times C \\times C = 2 \\times 16 \\times 32 \\times 32 = 32768 $$\n  - Pointwise Linear Layer ($C \\to C$):\n  $$ \\Theta_{\\text{pointwise}} = C \\times C + C = 32 \\times 32 + 32 = 1024 + 32 = 1056 $$\n  The total parameters for $L=3$ Fourier layers are:\n  $$ \\Theta_{\\text{fourier\\_layers}} = L \\times (\\Theta_{\\text{spec}} + \\Theta_{\\text{pointwise}}) = 3 \\times (32768 + 1056) = 3 \\times 33824 = 101472 $$\n- Projection Layer (pointwise $C \\to F_{\\text{out}}$):\n$$ \\Theta_{\\text{proj}} = C \\times F_{\\text{out}} + F_{\\text{out}} = 32 \\times 6 + 6 = 192 + 6 = 198 $$\nThe total parameter count for the FNO is the sum of all components:\n$$ \\Theta_{\\text{FNO}} = \\Theta_{\\text{lift}} + \\Theta_{\\text{fourier\\_layers}} + \\Theta_{\\text{proj}} = 352 + 101472 + 198 = 102022 $$\n\n**2. Per-Column Forward-Pass FLOPs**\n\n**MLP FLOPs per Column:**\nThe per-level MLP is applied independently at each of the $N_z$ vertical levels.\n- FLOPs for Layer 1 ($F_{\\text{in}} \\to 64$): $2 \\times F_{\\text{in}} \\times 64 + 64 = 2 \\times 10 \\times 64 + 64 = 1280 + 64 = 1344$.\n- FLOPs for Layer 2 ($64 \\to 64$): $2 \\times 64 \\times 64 + 64 = 8192 + 64 = 8256$.\n- FLOPs for Layer 3 ($64 \\to F_{\\text{out}}$): $2 \\times 64 \\times F_{\\text{out}} + F_{\\text{out}} = 2 \\times 64 \\times 6 + 6 = 768 + 6 = 774$.\nTotal FLOPs per level: $F_l = 1344 + 8256 + 774 = 10374$.\nTotal FLOPs per column is this value multiplied by the number of levels $N_z$:\n$$ \\text{FLOPs}_{\\text{MLP, col}} = N_z \\times F_l = 72 \\times 10374 = 746928 $$\n\n**FNO FLOPs per Column:**\n- Lift Layer (pointwise $F_{\\text{in}} \\to C$): Applied at $N_z$ levels.\n$$ F_{\\text{lift}} = N_z \\times (2 \\times F_{\\text{in}} \\times C + C) = 72 \\times (2 \\times 10 \\times 32 + 32) = 72 \\times 672 = 48384 $$\n- Fourier Layers ($L=3$ layers): Each layer has four computational steps.\n  - Forward FFT: For each of the $C$ channels, an FFT of length $N_z$ is performed.\n  $$ F_{\\text{FFT}} = C \\times (5 N_z \\log_2(N_z)) = 32 \\times (5 \\times 72 \\times \\log_2(72)) = 11520 \\log_2(72) $$\n  - Spectral Convolution: For each of the $K$ modes, we compute $C$ outputs. Each output is a complex weighted sum over $C$ inputs, costing $8C$ FLOPs.\n  $$ F_{\\text{spec}} = K \\times C \\times (8C) = 8 K C^2 = 8 \\times 16 \\times 32^2 = 131072 $$\n  - Inverse FFT: Same cost as the forward FFT.\n  $$ F_{\\text{IFFT}} = 11520 \\log_2(72) $$\n  - Pointwise Linear Layer ($C \\to C$): Applied at $N_z$ levels.\n  $$ F_{\\text{pointwise}} = N_z \\times (2 C^2 + C) = 72 \\times (2 \\times 32^2 + 32) = 72 \\times 2080 = 149760 $$\n  Total FLOPs for $L=3$ Fourier layers:\n  $$ F_{\\text{fourier\\_layers}} = L \\times (F_{\\text{FFT}} + F_{\\text{spec}} + F_{\\text{IFFT}} + F_{\\text{pointwise}}) $$\n  $$ F_{\\text{fourier\\_layers}} = 3 \\times (2 \\times 11520 \\log_2(72) + 131072 + 149760) = 3 \\times (23040 \\log_2(72) + 280832) $$\n  $$ F_{\\text{fourier\\_layers}} = 69120 \\log_2(72) + 842496 $$\n- Projection Layer (pointwise $C \\to F_{\\text{out}}$):\n$$ F_{\\text{proj}} = N_z \\times (2 C F_{\\text{out}} + F_{\\text{out}}) = 72 \\times (2 \\times 32 \\times 6 + 6) = 72 \\times 390 = 28080 $$\nTotal FLOPs per FNO column:\n$$ \\text{FLOPs}_{\\text{FNO, col}} = F_{\\text{lift}} + F_{\\text{fourier\\_layers}} + F_{\\text{proj}} $$\n$$ \\text{FLOPs}_{\\text{FNO, col}} = 48384 + (69120 \\log_2(72) + 842496) + 28080 $$\n$$ \\text{FLOPs}_{\\text{FNO, col}} = 918960 + 69120 \\log_2(72) $$\nUsing $\\log_2(72) \\approx 6.169925$:\n$$ \\text{FLOPs}_{\\text{FNO, col}} \\approx 918960 + 69120 \\times 6.169925 \\approx 918960 + 426523.23 \\approx 1345483.23 $$\n\n**3. Total FLOPs Per Emulator Call**\n\nThe total number of columns is $N_{\\text{col}} = N_x \\times N_y = 1024 \\times 1024 = 1048576$.\n- Total MLP FLOPs:\n$$ \\text{FLOPs}_{\\text{MLP, total}} = N_{\\text{col}} \\times \\text{FLOPs}_{\\text{MLP, col}} = 1048576 \\times 746928 = 783216963584 \\approx 7.832 \\times 10^{11} $$\n- Total FNO FLOPs:\n$$ \\text{FLOPs}_{\\text{FNO, total}} = N_{\\text{col}} \\times \\text{FLOPs}_{\\text{FNO, col}} = 1048576 \\times (918960 + 69120 \\log_2(72)) $$\n$$ \\text{FLOPs}_{\\text{FNO, total}} \\approx 1048576 \\times 1345483.23 \\approx 1410943265997 \\approx 1.411 \\times 10^{12} $$\n\n**4. Feasibility Fractions**\n\nThe available FLOPs per emulator call is the product of the performance budget and the time budget:\n$$ \\text{FLOPs}_{\\text{avail}} = P_{\\text{avail}} \\times B = (1.0 \\times 10^{15} \\text{ FLOPs/s}) \\times (0.2 \\text{ s}) = 2.0 \\times 10^{14} \\text{ FLOPs} $$\nThe feasibility fractions are the ratios of the required FLOPs to the available FLOPs.\n\n- MLP feasibility fraction:\n$$ r_{\\text{MLP}} = \\frac{\\text{FLOPs}_{\\text{MLP, total}}}{\\text{FLOPs}_{\\text{avail}}} = \\frac{783216963584}{2.0 \\times 10^{14}} \\approx 0.00391608 $$\nRounding to three significant figures, $r_{\\text{MLP}} \\approx 0.00392$.\n\n- FNO feasibility fraction:\n$$ r_{\\text{FNO}} = \\frac{\\text{FLOPs}_{\\text{FNO, total}}}{\\text{FLOPs}_{\\text{avail}}} = \\frac{1410943265997}{2.0 \\times 10^{14}} \\approx 0.00705472 $$\nRounding to three significant figures, $r_{\\text{FNO}} \\approx 0.00705$.\n\nThe final answer is the pair $(r_{\\text{FNO}}, r_{\\text{MLP}})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.00705 & 0.00392\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A primary challenge in using machine learning emulators for physical processes is ensuring they respect fundamental conservation laws. An emulator that does not conserve energy can lead to unrealistic long-term drifts in climate simulations. This exercise provides a hands-on method for validating an emulator against the first law of thermodynamics by comparing the total heating it predicts over a diurnal cycle with the heating calculated from first principles using radiative flux divergence. This practice  demonstrates a fundamental diagnostic technique for identifying systematic biases and validating the physical consistency of a data-driven model.",
            "id": "4061564",
            "problem": "Consider a single homogeneous atmospheric layer of thickness $\\Delta z$ used in Numerical Weather Prediction (NWP) and climate modeling. An emulation model from Machine Learning (ML) provides hourly layer-mean temperature tendencies that aim to reproduce the effect of radiative parameterization. To validate the emulator's units and sign conventions against energy conservation, use the first-principles dry-air energy equation and radiative flux convergence.\n\nAssume dry-air thermodynamics and energy conservation, where the layer-mean radiative heating rate $q_{\\mathrm{rad}}(t)$ satisfies\n$$\n\\rho c_p \\frac{dT}{dt} \\approx -\\frac{F_{\\mathrm{net}}(z_{\\mathrm{top}},t)-F_{\\mathrm{net}}(z_{\\mathrm{bot}},t)}{\\Delta z},\n$$\nwith $\\rho$ the air density, $c_p$ the specific heat at constant pressure, and $F_{\\mathrm{net}}(z,t)$ the net vertical radiative flux (positive upward). Over discrete hourly times $t_k=k\\,\\Delta t$, $k=0,1,\\dots,23$, take $\\Delta t=3600\\,\\mathrm{s}$, and\n- $\\rho=1.2\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $c_p=1004\\,\\mathrm{J}\\,\\mathrm{kg}^{-1}\\,\\mathrm{K}^{-1}$, $\\Delta z=1000\\,\\mathrm{m}$,\n- $F_{\\mathrm{net}}(z_{\\mathrm{top}},t_k)=F_{t0}-A\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)$ with $F_{t0}=0\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$ and $A=30\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$,\n- $F_{\\mathrm{net}}(z_{\\mathrm{bot}},t_k)=F_{b0}-B\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)$ with $F_{b0}=10\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$ and $B=45\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$.\n\nThe ML emulator supplies hourly layer-mean temperature tendencies in units of $\\mathrm{K}\\,\\mathrm{s}^{-1}$ given by\n$$\n\\hat{q}_k=\\alpha+\\beta\\sin\\!\\left(\\frac{2\\pi k}{24}\\right),\\quad \\alpha=\\frac{1}{120{,}480}+\\frac{1}{2{,}000{,}000},\\quad \\beta=-\\frac{1}{80{,}320}.\n$$\n\nTasks:\n1. Using the fluxes above, compute the $24$-hour accumulated physical radiative heating\n$$\n\\Delta T_{\\mathrm{rad}}=\\sum_{k=0}^{23} q_{\\mathrm{rad}}(t_k)\\,\\Delta t,\n$$\nwhere $q_{\\mathrm{rad}}(t_k)=-\\dfrac{F_{\\mathrm{net}}(z_{\\mathrm{top}},t_k)-F_{\\mathrm{net}}(z_{\\mathrm{bot}},t_k)}{\\rho c_p \\Delta z}$.\n2. Compute the $24$-hour accumulated emulator heating\n$$\n\\Delta T_{\\mathrm{ML}}=\\sum_{k=0}^{23} \\hat{q}_k\\,\\Delta t.\n$$\n3. Compute the diagnostic mismatch\n$$\n\\Delta=\\Delta T_{\\mathrm{ML}}-\\Delta T_{\\mathrm{rad}},\n$$\nas a way to validate the emulator's units and sign convention against radiative flux convergence.\n\nExpress your final answer for $\\Delta$ in kelvins and round your answer to four significant figures.",
            "solution": "The objective is to compute the diagnostic mismatch $\\Delta = \\Delta T_{\\mathrm{ML}} - \\Delta T_{\\mathrm{rad}}$ between the $24$-hour accumulated heating from a Machine Learning (ML) emulator and the physical radiative heating derived from flux-divergence. This serves as a validation of the emulator's energy conservation properties. The calculation is performed in three steps as requested.\n\nThe physical constants and parameters are:\nAir density, $\\rho=1.2\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$.\nSpecific heat at constant pressure, $c_p=1004\\,\\mathrm{J}\\,\\mathrm{kg}^{-1}\\,\\mathrm{K}^{-1}$.\nLayer thickness, $\\Delta z=1000\\,\\mathrm{m}$.\nTime step, $\\Delta t=3600\\,\\mathrm{s}$.\nThe product $\\rho c_p \\Delta z$ represents the heat capacity of the atmospheric layer per unit area.\n$$\n\\rho c_p \\Delta z = (1.2\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}) \\times (1004\\,\\mathrm{J}\\,\\mathrm{kg}^{-1}\\,\\mathrm{K}^{-1}) \\times (1000\\,\\mathrm{m}) = 1,204,800\\,\\mathrm{J}\\,\\mathrm{m}^{-2}\\,\\mathrm{K}^{-1}.\n$$\n\nFirst, we compute the $24$-hour accumulated physical radiative heating, $\\Delta T_{\\mathrm{rad}}$.\nThe physical radiative heating rate at discrete time $t_k = k \\Delta t$ is given by\n$$\nq_{\\mathrm{rad}}(t_k)=-\\dfrac{F_{\\mathrm{net}}(z_{\\mathrm{top}},t_k)-F_{\\mathrm{net}}(z_{\\mathrm{bot}},t_k)}{\\rho c_p \\Delta z}.\n$$\nThe net flux difference between the top and bottom of the layer is\n$$\nF_{\\mathrm{net}}(z_{\\mathrm{top}},t_k)-F_{\\mathrm{net}}(z_{\\mathrm{bot}},t_k) = \\left(F_{t0}-A\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)\\right) - \\left(F_{b0}-B\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)\\right) = (F_{t0}-F_{b0})-(A-B)\\sin\\!\\left(\\frac{2\\pi k}{24}\\right).\n$$\nSubstituting the given values $F_{t0}=0\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$, $F_{b0}=10\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$, $A=30\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$, and $B=45\\,\\mathrm{W}\\,\\mathrm{m}^{-2}$:\n$$\nF_{\\mathrm{net}}(z_{\\mathrm{top}},t_k)-F_{\\mathrm{net}}(z_{\\mathrm{bot}},t_k) = (0 - 10) - (30 - 45)\\sin\\!\\left(\\frac{2\\pi k}{24}\\right) = -10 + 15\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)\\,\\mathrm{W}\\,\\mathrm{m}^{-2}.\n$$\nThe radiative heating rate is then\n$$\nq_{\\mathrm{rad}}(t_k) = -\\frac{-10 + 15\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)}{1,204,800} = \\frac{10}{1,204,800} - \\frac{15}{1,204,800}\\sin\\!\\left(\\frac{2\\pi k}{24}\\right).\n$$\nSimplifying the coefficients, we get\n$$\nq_{\\mathrm{rad}}(t_k) = \\frac{1}{120,480} - \\frac{1}{80,320}\\sin\\!\\left(\\frac{2\\pi k}{24}\\right)\\,\\mathrm{K}\\,\\mathrm{s}^{-1}.\n$$\nThe total accumulated heating over $24$ hours is the sum over $k$ from $0$ to $23$:\n$$\n\\Delta T_{\\mathrm{rad}}=\\sum_{k=0}^{23} q_{\\mathrm{rad}}(t_k)\\,\\Delta t = \\Delta t \\sum_{k=0}^{23} \\left[ \\frac{1}{120,480} - \\frac{1}{80,320}\\sin\\!\\left(\\frac{2\\pi k}{24}\\right) \\right].\n$$\nThe sum of a sine function over a full discrete period is zero: $\\sum_{k=0}^{N-1} \\sin(2\\pi k/N) = 0$. In this case, $N=24$.\n$$\n\\sum_{k=0}^{23} \\sin\\!\\left(\\frac{2\\pi k}{24}\\right) = 0.\n$$\nTherefore, the summation simplifies to\n$$\n\\Delta T_{\\mathrm{rad}} = \\Delta t \\sum_{k=0}^{23} \\frac{1}{120,480} = (3600\\,\\mathrm{s}) \\times 24 \\times \\frac{1}{120,480} = \\frac{86,400}{120,480}\\,\\mathrm{K} = \\frac{8,640}{12,048}\\,\\mathrm{K} = \\frac{180}{251}\\,\\mathrm{K}.\n$$\n\nSecond, we compute the $24$-hour accumulated emulator heating, $\\Delta T_{\\mathrm{ML}}$.\nThe ML emulator provides temperature tendencies as\n$$\n\\hat{q}_k=\\alpha+\\beta\\sin\\!\\left(\\frac{2\\pi k}{24}\\right),\n$$\nwith $\\alpha=\\frac{1}{120,480}+\\frac{1}{2,000,000}$ and $\\beta=-\\frac{1}{80,320}$.\nThe total accumulated heating from the ML model is\n$$\n\\Delta T_{\\mathrm{ML}}=\\sum_{k=0}^{23} \\hat{q}_k\\,\\Delta t = \\Delta t \\sum_{k=0}^{23} \\left[ \\alpha+\\beta\\sin\\!\\left(\\frac{2\\pi k}{24}\\right) \\right].\n$$\nAgain, the sum of the sine term is zero.\n$$\n\\Delta T_{\\mathrm{ML}} = \\Delta t \\sum_{k=0}^{23} \\alpha = (3600\\,\\mathrm{s}) \\times 24 \\times \\alpha = 86,400 \\alpha.\n$$\nSubstituting the value of $\\alpha$:\n$$\n\\Delta T_{\\mathrm{ML}} = 86,400 \\left( \\frac{1}{120,480}+\\frac{1}{2,000,000} \\right) = \\frac{86,400}{120,480} + \\frac{86,400}{2,000,000}.\n$$\nThe first term is identical to $\\Delta T_{\\mathrm{rad}}$:\n$$\n\\Delta T_{\\mathrm{ML}} = \\frac{180}{251} + \\frac{86,400}{2,000,000} = \\frac{180}{251} + 0.0432\\,\\mathrm{K}.\n$$\n\nThird, we compute the diagnostic mismatch, $\\Delta$.\n$$\n\\Delta = \\Delta T_{\\mathrm{ML}} - \\Delta T_{\\mathrm{rad}} = \\left( \\frac{180}{251} + 0.0432 \\right) - \\frac{180}{251} = 0.0432\\,\\mathrm{K}.\n$$\nThe problem specifies that the final answer should be rounded to four significant figures. The number $0.0432$ has three significant figures (the digits $4$, $3$, and $2$). To express it with four significant figures, we add a trailing zero.\n$$\n\\Delta = 0.04320\\,\\mathrm{K}.\n$$\nThis mismatch arises from the small constant bias in the ML emulator's mean heating rate, represented by the term $\\frac{1}{2,000,000}$ in $\\alpha$. The emulator correctly captured the phase and amplitude of the diurnal cycle of heating (since $\\beta$ matches the physical value of $-\\frac{1}{80,320}$), but slightly misses the $24$-hour mean heating. This type of diagnostic is crucial for ensuring that ML emulators do not introduce systematic energy drifts in long climate simulations.",
            "answer": "$$\\boxed{0.04320}$$"
        },
        {
            "introduction": "After diagnosing an energy imbalance in an ML emulator, the next step is often to correct it. Post-hoc correction methods are frequently employed to enforce physical constraints when retraining the model is not feasible or guaranteed to succeed. This problem introduces a common technique for enforcing column-integrated energy conservation, where you will calculate the energy residual from an imperfect emulator and then derive a simple, physically-motivated correction term that adjusts temperature tendencies to close the energy budget. This exercise  moves from diagnosis to practical application, teaching you how to implement an \"energy fixer\" to ensure an ML parameterization adheres to conservation laws.",
            "id": "4061580",
            "problem": "A Machine Learning (ML) emulator is used to predict subgrid diabatic tendencies for temperature and specific humidity in a single atmospheric column within a Numerical Weather Prediction (NWP) model. The emulator outputs layer-wise tendencies $\\dot{T}_{i}$ and $\\dot{q}_{i}$ on a pressure discretization. The column-integrated moist static energy tendency must be consistent with the externally provided net diabatic energy flux convergence target $F_{\\mathrm{req}}$ to ensure column energy balance. Start from the moist static energy per unit mass $h = c_{p} T + L_{v} q + g z$, where $c_{p}$ is the specific heat of dry air at constant pressure, $L_{v}$ is the latent heat of vaporization, and $g$ is gravitational acceleration. Assume hydrostatic balance, neglecting changes in the gravitational potential term at fixed layer boundaries. The column-integrated moist static energy tendency is defined as the mass-weighted vertical integral\n$$\nE = \\frac{1}{g} \\int_{p_{\\mathrm{top}}}^{p_{\\mathrm{sfc}}} \\left( c_{p} \\,\\dot{T}(p) + L_{v} \\,\\dot{q}(p) \\right) \\, dp,\n$$\nand, under a layer-wise approximation with layer thicknesses $\\Delta p_{i}$, is discretized as\n$$\nE_{\\mathrm{emul}} = \\frac{1}{g} \\sum_{i=1}^{N} \\Delta p_{i} \\left( c_{p} \\,\\dot{T}_{i} + L_{v} \\,\\dot{q}_{i} \\right).\n$$\nYou are given a three-layer column ($N=3$) with the following data:\n- $g = 9.81\\,\\mathrm{m\\,s^{-2}}$, $c_{p} = 1004\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}$, $L_{v} = 2.5 \\times 10^{6}\\,\\mathrm{J\\,kg^{-1}}$,\n- $\\Delta p_{1} = 25000\\,\\mathrm{Pa}$, $\\Delta p_{2} = 40000\\,\\mathrm{Pa}$, $\\Delta p_{3} = 35000\\,\\mathrm{Pa}$,\n- $\\dot{T}_{1} = 1.5 \\times 10^{-5}\\,\\mathrm{K\\,s^{-1}}$, $\\dot{T}_{2} = -8.0 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}$, $\\dot{T}_{3} = 3.0 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}$,\n- $\\dot{q}_{1} = 2.0 \\times 10^{-8}\\,\\mathrm{kg\\,kg^{-1}\\,s^{-1}}$, $\\dot{q}_{2} = 0.5 \\times 10^{-8}\\,\\mathrm{kg\\,kg^{-1}\\,s^{-1}}$, $\\dot{q}_{3} = -1.0 \\times 10^{-8}\\,\\mathrm{kg\\,kg^{-1}\\,s^{-1}}$,\n- $F_{\\mathrm{req}} = 95\\,\\mathrm{W\\,m^{-2}}$, and tolerance $\\epsilon = 1\\,\\mathrm{W\\,m^{-2}}$.\n\n(a) Compute the emulator-predicted column-integrated moist static energy tendency $E_{\\mathrm{emul}}$ and the residual\n$$\nR = E_{\\mathrm{emul}} - F_{\\mathrm{req}}.\n$$\n(b) Formulate a physically consistent correction to the emulator tendencies that adjusts only the temperature tendencies by a uniform offset $\\delta \\dot{T}_{i} = \\alpha$ (constant in $i$) and leaves humidity tendencies unchanged ($\\delta \\dot{q}_{i} = 0$), such that the corrected column-integrated tendency satisfies the energy balance within the tolerance $\\epsilon$, i.e.,\n$$\n\\left| \\left( E_{\\mathrm{emul}} + \\frac{1}{g} \\sum_{i=1}^{N} \\Delta p_{i} c_{p} \\alpha \\right) - F_{\\mathrm{req}} \\right| \\le \\epsilon.\n$$\nAssuming the minimal correction that enforces exact balance whenever $|R| > \\epsilon$ and otherwise applies no correction, derive the expression for $\\alpha$ and evaluate its numerical value for the given data. Express the final $\\alpha$ in $\\mathrm{K\\,s^{-1}}$ and round your answer to four significant figures.",
            "solution": "The problem has been validated and is deemed scientifically grounded, well-posed, objective, complete, and consistent. All necessary data and definitions are provided to solve for the requested quantities. The physical context is standard in atmospheric science and numerical modeling, concerning the enforcement of conservation laws on machine learning parameterization schemes.\n\nThe problem is divided into two parts. First, we compute the emulator-predicted column-integrated moist static energy tendency, $E_{\\mathrm{emul}}$, and its residual, $R$, with respect to the required energy flux convergence, $F_{\\mathrm{req}}$. Second, we derive and compute a uniform correction, $\\alpha$, to the temperature tendencies to enforce energy balance.\n\nPart (a): Computation of $E_{\\mathrm{emul}}$ and $R$.\n\nThe column-integrated moist static energy tendency predicted by the emulator, $E_{\\mathrm{emul}}$, is given by the discretized formula:\n$$\nE_{\\mathrm{emul}} = \\frac{1}{g} \\sum_{i=1}^{N} \\Delta p_{i} \\left( c_{p} \\,\\dot{T}_{i} + L_{v} \\,\\dot{q}_{i} \\right)\n$$\nWe are given $N=3$ layers and the following constants and data:\n- $g = 9.81\\,\\mathrm{m\\,s^{-2}}$\n- $c_{p} = 1004\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}$\n- $L_{v} = 2.5 \\times 10^{6}\\,\\mathrm{J\\,kg^{-1}}$\n- Layer $1$: $\\Delta p_{1} = 25000\\,\\mathrm{Pa}$, $\\dot{T}_{1} = 1.5 \\times 10^{-5}\\,\\mathrm{K\\,s^{-1}}$, $\\dot{q}_{1} = 2.0 \\times 10^{-8}\\,\\mathrm{kg\\,kg^{-1}\\,s^{-1}}$\n- Layer $2$: $\\Delta p_{2} = 40000\\,\\mathrm{Pa}$, $\\dot{T}_{2} = -8.0 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}$, $\\dot{q}_{2} = 0.5 \\times 10^{-8}\\,\\mathrm{kg\\,kg^{-1}\\,s^{-1}}$\n- Layer $3$: $\\Delta p_{3} = 35000\\,\\mathrm{Pa}$, $\\dot{T}_{3} = 3.0 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}$, $\\dot{q}_{3} = -1.0 \\times 10^{-8}\\,\\mathrm{kg\\,kg^{-1}\\,s^{-1}}$\n\nLet's compute the term $h_{i}^{\\prime} = c_{p} \\,\\dot{T}_{i} + L_{v} \\,\\dot{q}_{i}$ for each layer $i$:\nFor layer $1$:\n$h_{1}^{\\prime} = (1004\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}) (1.5 \\times 10^{-5}\\,\\mathrm{K\\,s^{-1}}) + (2.5 \\times 10^{6}\\,\\mathrm{J\\,kg^{-1}}) (2.0 \\times 10^{-8}\\,\\mathrm{s^{-1}})$\n$h_{1}^{\\prime} = 0.01506\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}} + 0.05\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}} = 0.06506\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}}$\n\nFor layer $2$:\n$h_{2}^{\\prime} = (1004\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}) (-8.0 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}) + (2.5 \\times 10^{6}\\,\\mathrm{J\\,kg^{-1}}) (0.5 \\times 10^{-8}\\,\\mathrm{s^{-1}})$\n$h_{2}^{\\prime} = -0.008032\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}} + 0.0125\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}} = 0.004468\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}}$\n\nFor layer $3$:\n$h_{3}^{\\prime} = (1004\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}) (3.0 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}) + (2.5 \\times 10^{6}\\,\\mathrm{J\\,kg^{-1}}) (-1.0 \\times 10^{-8}\\,\\mathrm{s^{-1}})$\n$h_{3}^{\\prime} = 0.003012\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}} - 0.025\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}} = -0.021988\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}}$\n\nNow, we compute the sum $\\sum_{i=1}^{3} \\Delta p_{i} h_{i}^{\\prime}$:\n$\\sum_{i=1}^{3} \\Delta p_{i} h_{i}^{\\prime} = (25000\\,\\mathrm{Pa})(0.06506\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}}) + (40000\\,\\mathrm{Pa})(0.004468\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}}) + (35000\\,\\mathrm{Pa})(-0.021988\\,\\mathrm{J\\,kg^{-1}\\,s^{-1}})$\n$= 1626.5 + 178.72 - 769.58 = 1035.64\\,\\mathrm{Pa \\cdot J \\cdot kg^{-1} \\cdot s^{-1}}$.\nThe factor $1/g$ corrects the units. We verify the units of the final quantity $E$: $\\frac{1}{g} \\sum \\Delta p_i (\\dots)$ has units $\\frac{\\mathrm{s}^2}{\\mathrm{m}} \\cdot \\mathrm{Pa} \\cdot \\frac{\\mathrm{J}}{\\mathrm{kg} \\cdot \\mathrm{s}} = \\frac{\\mathrm{s}^2}{\\mathrm{m}} \\cdot \\frac{\\mathrm{kg}}{\\mathrm{m} \\cdot \\mathrm{s}^2} \\cdot \\frac{\\mathrm{J}}{\\mathrm{kg} \\cdot \\mathrm{s}} = \\frac{\\mathrm{J}}{\\mathrm{m}^2 \\cdot \\mathrm{s}} = \\mathrm{W\\,m^{-2}}$, which is a flux, as required.\n\nNow, we calculate $E_{\\mathrm{emul}}$:\n$$\nE_{\\mathrm{emul}} = \\frac{1035.64\\,\\mathrm{Pa \\cdot J \\cdot kg^{-1} \\cdot s^{-1}}}{9.81\\,\\mathrm{m\\,s^{-2}}} \\approx 105.57003\\,\\mathrm{W\\,m^{-2}}\n$$\nThe residual $R$ is defined as $R = E_{\\mathrm{emul}} - F_{\\mathrm{req}}$. Given $F_{\\mathrm{req}} = 95\\,\\mathrm{W\\,m^{-2}}$:\n$$\nR = 105.57003\\,\\mathrm{W\\,m^{-2}} - 95\\,\\mathrm{W\\,m^{-2}} = 10.57003\\,\\mathrm{W\\,m^{-2}}\n$$\n\nPart (b): Derivation and computation of the correction $\\alpha$.\n\nA correction is to be applied to the temperature tendencies only, such that $\\delta \\dot{T}_{i} = \\alpha$ for all layers $i$, and $\\delta \\dot{q}_{i} = 0$. The corrected emulator tendency, $E_{\\mathrm{corr}}$, is then:\n$$\nE_{\\mathrm{corr}} = \\frac{1}{g} \\sum_{i=1}^{N} \\Delta p_{i} \\left( c_{p} (\\dot{T}_{i} + \\alpha) + L_{v} \\dot{q}_{i} \\right)\n$$\nWe can separate this into the original tendency and the correction term:\n$$\nE_{\\mathrm{corr}} = \\frac{1}{g} \\sum_{i=1}^{N} \\Delta p_{i} \\left( c_{p} \\dot{T}_{i} + L_{v} \\dot{q}_{i} \\right) + \\frac{1}{g} \\sum_{i=1}^{N} \\Delta p_{i} c_{p} \\alpha\n$$\nThe first term is $E_{\\mathrm{emul}}$. Since $\\alpha$ and $c_p$ are constant, we can pull them out of the summation:\n$$\nE_{\\mathrm{corr}} = E_{\\mathrm{emul}} + \\frac{c_{p} \\alpha}{g} \\sum_{i=1}^{N} \\Delta p_{i}\n$$\nLet $\\Delta p_{\\mathrm{tot}} = \\sum_{i=1}^{N} \\Delta p_{i}$ be the total pressure thickness of the column.\n$\\Delta p_{\\mathrm{tot}} = 25000\\,\\mathrm{Pa} + 40000\\,\\mathrm{Pa} + 35000\\,\\mathrm{Pa} = 100000\\,\\mathrm{Pa}$.\nSo, the corrected tendency is:\n$$\nE_{\\mathrm{corr}} = E_{\\mathrm{emul}} + \\alpha \\frac{c_{p} \\Delta p_{\\mathrm{tot}}}{g}\n$$\nThe problem states that a correction is applied only if the absolute value of the residual exceeds the tolerance, i.e., $|R| > \\epsilon$.\nWe have $|R| \\approx 10.57\\,\\mathrm{W\\,m^{-2}}$ and $\\epsilon = 1\\,\\mathrm{W\\,m^{-2}}$. Since $|R| > \\epsilon$, a correction is necessary. The minimal correction is one that achieves exact energy balance: $E_{\\mathrm{corr}} = F_{\\mathrm{req}}$.\nSetting $E_{\\mathrm{corr}}$ to $F_{\\mathrm{req}}$, we get:\n$$\nF_{\\mathrm{req}} = E_{\\mathrm{emul}} + \\alpha \\frac{c_{p} \\Delta p_{\\mathrm{tot}}}{g}\n$$\nWe solve for $\\alpha$:\n$$\n\\alpha \\frac{c_{p} \\Delta p_{\\mathrm{tot}}}{g} = F_{\\mathrm{req}} - E_{\\mathrm{emul}} = -R\n$$\n$$\n\\alpha = -R \\left( \\frac{g}{c_{p} \\Delta p_{\\mathrm{tot}}} \\right)\n$$\nThis is the symbolic expression for $\\alpha$. Now we insert the numerical values:\n$$\n\\alpha = -(10.57003\\,\\mathrm{W\\,m^{-2}}) \\left( \\frac{9.81\\,\\mathrm{m\\,s^{-2}}}{(1004\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}) (100000\\,\\mathrm{Pa})} \\right)\n$$\n$$\n\\alpha = -10.57003 \\left( \\frac{9.81}{100400000} \\right) \\mathrm{K\\,s^{-1}}\n$$\n$$\n\\alpha \\approx -10.57003 \\times (9.770916 \\times 10^{-8}) \\mathrm{K\\,s^{-1}}\n$$\n$$\n\\alpha \\approx -1.03287 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}\n$$\nRounding the result to four significant figures as requested:\n$$\n\\alpha \\approx -1.033 \\times 10^{-6}\\,\\mathrm{K\\,s^{-1}}\n$$\nThis corresponds to a small, uniform cooling applied to all layers to offset the excess energy generated by the emulator.\n\nIf $|R| \\le \\epsilon$, the condition for applying a correction would not be met, and the solution would be $\\alpha=0$. Our calculated $R$ clearly exceeds the tolerance, so the non-zero correction is the correct path.\nFinal check of the rule: \"minimal correction that enforces exact balance whenever $|R| > \\epsilon$ and otherwise applies no correction\". Our procedure matches this rule perfectly.",
            "answer": "$$\n\\boxed{-1.033 \\times 10^{-6}}\n$$"
        }
    ]
}