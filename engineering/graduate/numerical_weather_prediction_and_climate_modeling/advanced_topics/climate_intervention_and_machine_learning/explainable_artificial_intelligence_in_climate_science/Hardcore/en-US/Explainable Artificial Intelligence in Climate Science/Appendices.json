{
    "hands_on_practices": [
        {
            "introduction": "Before we can explain what a complex model has learned, we must first understand what it is capable of \"seeing.\" The spatiotemporal receptive field defines the precise region of the input data that can theoretically influence a given output, providing a fundamental, architectural-based constraint on interpretation. This exercise guides you through the process of calculating this receptive field for a hybrid CNN-LSTM network, a common architecture in climate science, forcing a mechanistic understanding of the model's information flow .",
            "id": "4040909",
            "problem": "Consider a precipitation forecasting system within Numerical Weather Prediction (NWP) and climate modeling that uses a Convolutional Neural Network (CNN) followed by a Long Short-Term Memory (LSTM) network. Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) are standard architectures in machine learning; here they are used to process spatiotemporal precipitation fields to produce a forecast one step ahead. In the broader context of explainable artificial intelligence in climate science, the spatiotemporal receptive field quantifies which input grid cells and past time steps can theoretically influence a given forecast output, providing a mechanistic basis for interpreting model attributions and saliency maps.\n\nYou are given the following architecture and assumptions:\n\n- Inputs are sequences of $T$ hourly precipitation fields on a regular grid. At each time step, a spatial CNN processes the field to produce features at a coarser resolution determined by strides.\n- The spatial CNN consists of $4$ two-dimensional convolutional layers with square kernels and isotropic strides, applied in order on each input frame:\n  - Layer $\\mathrm{S1}$: kernel size $k_{1} = 3$, stride $s_{1} = 1$.\n  - Layer $\\mathrm{S2}$: kernel size $k_{2} = 5$, stride $s_{2} = 2$.\n  - Layer $\\mathrm{S3}$: kernel size $k_{3} = 3$, stride $s_{3} = 2$.\n  - Layer $\\mathrm{S4}$: kernel size $k_{4} = 3$, stride $s_{4} = 1$.\n  - There is no dilation and standard zero-padding is used to preserve alignment; when $s_{\\ell} = 1$ spatial resolution is preserved, and when $s_{\\ell} = 2$ the resolution is downsampled by a factor of $2$ along each spatial axis at that layer.\n- At each spatial location of the final feature map produced by $\\mathrm{S4}$, a $1 \\times 1$ convolution maps features to a scalar, yielding a per-location latent series over time.\n- The temporal module is a one-dimensional temporal convolution followed by an LSTM:\n  - Temporal convolution $\\mathrm{T1}$: kernel size $k_{t} = 5$, stride $s_{t} = 2$, applied along the time dimension at each spatial location independently, producing a downsampled sequence of latent features.\n  - The LSTM consumes only the most recent $W = 4$ outputs of $\\mathrm{T1}$ (a fixed sliding window) and emits a forecast for the next time step at the corresponding spatial location of the $\\mathrm{S4}$ feature map.\n- Focus on the forecast for the center spatial location of the $\\mathrm{S4}$ feature map and one-step-ahead time with respect to the most recent inputs.\n\nUsing first principles from discrete convolution and sampling theory, define the spatiotemporal receptive field of this CNN-LSTM architecture as the set of input grid cells across space and past time steps that can theoretically influence the chosen forecast output under the stated assumptions. Then, derive the effective receptive field size along the two spatial axes (width and height in input grid cells) and along the temporal axis (number of input time steps) in closed form from the kernel sizes and strides across layers, and compute their numeric values for the architecture above.\n\nProvide the final answer as a row matrix listing the spatial width, spatial height, and temporal length of the effective receptive field, in dimensionless counts (grid cells and time steps). No rounding is required; report exact integer counts. Express the final answer in the form $\\left[ \\text{width} \\;\\; \\text{height} \\;\\; \\text{time} \\right]$.",
            "solution": "We first formalize the spatiotemporal receptive field for the given architecture. For a forecast produced at a particular output spatial location and at one-step-ahead time, the receptive field is the set of input spatial grid cells and past time indices whose values can affect the output via the network’s operations. In a purely feedforward convolutional network without skip connections, this set is determined by the support of the kernels and the strides by which positions are sampled and mapped through the layers. In the temporal dimension, a one-dimensional convolution with stride followed by a recurrent computation over a fixed window implies a finite temporal support determined by the convolution’s kernel and stride and the LSTM window length. We proceed to derive the spatial and temporal components separately and then combine them.\n\nSpatial receptive field derivation (per frame):\nWe adopt the standard one-dimensional derivation for clarity and then extend to two dimensions by isotropy. Consider a stack of discrete convolutions with kernel sizes $k_{\\ell}$ and strides $s_{\\ell}$, where $\\ell \\in \\{1,2,3,4\\}$ indexes layers $\\mathrm{S1}$ through $\\mathrm{S4}$. Define the jump $J_{\\ell}$ as the spacing, in input coordinates, between adjacent receptive field centers at layer $\\ell$, and define the receptive field size $R_{\\ell}$ as the number of input grid cells along one axis that influence a single unit at layer $\\ell$. The base conditions are $J_{0} = 1$ and $R_{0} = 1$, corresponding to an input pixel’s self-receptive field and unit sampling.\n\nFor a convolutional layer with kernel size $k_{\\ell}$ and stride $s_{\\ell}$, the mapping rules are\n$$\nJ_{\\ell} = J_{\\ell-1} \\, s_{\\ell},\n$$\n$$\nR_{\\ell} = R_{\\ell-1} + (k_{\\ell} - 1) \\, J_{\\ell-1}.\n$$\nThese follow from the fact that a stride of $s_{\\ell}$ expands the spacing of receptive field centers by a factor $s_{\\ell}$ relative to the previous layer, and a kernel of size $k_{\\ell}$ covers $(k_{\\ell}-1)$ steps of size $J_{\\ell-1}$ beyond the center in the previous layer.\n\nApplying these recurrences to the given spatial layers:\n- Layer $\\mathrm{S1}$ has $k_{1} = 3$, $s_{1} = 1$. Thus\n$$\nJ_{1} = J_{0} \\, s_{1} = 1 \\cdot 1 = 1, \\quad R_{1} = R_{0} + (k_{1}-1) J_{0} = 1 + (3-1)\\cdot 1 = 3.\n$$\n- Layer $\\mathrm{S2}$ has $k_{2} = 5$, $s_{2} = 2$. Thus\n$$\nJ_{2} = J_{1} \\, s_{2} = 1 \\cdot 2 = 2, \\quad R_{2} = R_{1} + (k_{2}-1) J_{1} = 3 + (5-1)\\cdot 1 = 7.\n$$\n- Layer $\\mathrm{S3}$ has $k_{3} = 3$, $s_{3} = 2$. Thus\n$$\nJ_{3} = J_{2} \\, s_{3} = 2 \\cdot 2 = 4, \\quad R_{3} = R_{2} + (k_{3}-1) J_{2} = 7 + (3-1)\\cdot 2 = 11.\n$$\n- Layer $\\mathrm{S4}$ has $k_{4} = 3$, $s_{4} = 1$. Thus\n$$\nJ_{4} = J_{3} \\, s_{4} = 4 \\cdot 1 = 4, \\quad R_{4} = R_{3} + (k_{4}-1) J_{3} = 11 + (3-1)\\cdot 4 = 19.\n$$\n\nThe final $1 \\times 1$ convolution does not expand the receptive field ($k=1$, $s=1$), so it leaves $R_{4}$ unchanged. Therefore, along a single spatial axis, the receptive field size is $R_{4} = 19$. Because kernels and strides are isotropic and square, the receptive field is the same along both axes, yielding a spatial receptive field of $19 \\times 19$ input grid cells for a single unit at the $\\mathrm{S4}$ output location under consideration.\n\nTemporal receptive field derivation:\nFor the temporal dimension, the one-dimensional convolution $\\mathrm{T1}$ has kernel size $k_{t} = 5$ and stride $s_{t} = 2$. A single output of $\\mathrm{T1}$ depends on $k_{t}$ consecutive input time steps, centered according to padding conventions; adjacent outputs of $\\mathrm{T1}$ are separated by $s_{t}$ input steps. The LSTM consumes only the most recent $W = 4$ outputs of $\\mathrm{T1}$ to produce the forecast. The union of the $W$ most recent $\\mathrm{T1}$ outputs’ supports covers a contiguous block of input time steps whose length is\n$$\nR_{t} = k_{t} + (W - 1) \\, s_{t},\n$$\nbecause each subsequent $\\mathrm{T1}$ output extends coverage by $s_{t}$ new input steps beyond the previous output’s support when there is no dilation and fixed stride. Substituting the given values gives\n$$\nR_{t} = 5 + (4 - 1) \\cdot 2 = 11.\n$$\n\nSpatiotemporal receptive field summary:\nUnder the stated architecture and assumptions, the effective receptive field for the forecast at the chosen spatial location and one-step-ahead time spans $19$ input grid cells along each spatial axis and $11$ input time steps along the temporal axis. In explainable artificial intelligence terms, this theoretical support defines the maximal spatiotemporal neighborhood that can contribute to attribution scores or influence functions for the chosen forecast, providing a principled constraint on interpretability analyses.\n\nTherefore, the requested row matrix listing spatial width, spatial height, and temporal length is $\\left[ 19 \\;\\; 19 \\;\\; 11 \\right]$.",
            "answer": "$$\\boxed{\\begin{pmatrix}19  19  11\\end{pmatrix}}$$"
        },
        {
            "introduction": "A robust explanation method should satisfy certain desirable axioms, and one of the most important is completeness, which ensures that feature attributions fully sum to the model's output change relative to a baseline. This practice delves into Integrated Gradients, a method that satisfies completeness by construction, rooted in the Fundamental Theorem of Calculus. You will not only derive this property but also implement and numerically verify it, cementing the crucial link between XAI theory and its practical application in building trustworthy models .",
            "id": "4040883",
            "problem": "You are studying an attribution-based explanation method for a differentiable precipitation predictor within numerical weather prediction and climate modeling. Consider a predictor $f:\\mathbb{R}^d \\to \\mathbb{R}$, an input $\\mathbf{x} \\in \\mathbb{R}^d$, and a climatological baseline $\\mathbf{x}' \\in \\mathbb{R}^d$ representing long-term mean normalized features. You are asked to propose and verify a completeness property for feature attributions in this setting. The entire task must be approached from first principles, starting from the chain rule, the Fundamental Theorem of Calculus, and well-tested facts from multivariable calculus about line integrals.\n\nPropose a mathematically precise completeness property: a set of feature attributions $\\{\\phi_i\\}_{i=1}^d$ is complete with respect to the baseline $\\mathbf{x}'$ if and only if\n$$\n\\sum_{i=1}^d \\phi_i \\;=\\; f(\\mathbf{x}) - f(\\mathbf{x}') \\, .\n$$\nDerive an attribution scheme that satisfies this property by construction using a straight-line path from $\\mathbf{x}'$ to $\\mathbf{x}$. Then implement it and verify numerically that the completeness residual is small.\n\nDefine the precipitation predictor $f$ as follows. Let the features be ordered as $\\mathbf{x} = (m,u,s,t,o,c)$, representing normalized column moisture $m$, normalized upward velocity amplitude $u$, normalized vertical wind shear $s$, normalized temperature anomaly $t$, normalized orographic uplift factor $o$, and normalized convective available potential energy $c$. Let\n$$\nz(\\mathbf{x}) \\;=\\; a_0 + a_1 m + a_2 u + a_3 s + a_4 t + a_5 o + a_6 c + a_7 m c + a_8 u o - a_9 s t \\, ,\n$$\nwith coefficients\n$$\na_0=-1.2,\\; a_1=2.0,\\; a_2=1.5,\\; a_3=-0.7,\\; a_4=0.3,\\; a_5=1.2,\\; a_6=1.8,\\; a_7=2.5,\\; a_8=1.0,\\; a_9=0.6 \\, .\n$$\nUse the softplus link:\n$$\n\\mathrm{softplus}(z) \\;=\\; \\log\\!\\big(1+e^{z}\\big) \\, ,\n$$\nand define\n$$\nf(\\mathbf{x}) \\;=\\; \\mathrm{softplus}\\!\\left(z(\\mathbf{x})\\right) \\, .\n$$\nAll inputs are nondimensional and lie in the unit hypercube, so no physical units are required in the answer.\n\nUse the straight-line path $\\gamma(\\alpha) = \\mathbf{x}' + \\alpha(\\mathbf{x}-\\mathbf{x}')$ for $\\alpha \\in [0,1]$. Define the attribution for feature $i$ by a path integral of partial derivatives:\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') \\;=\\; (x_i - x_i') \\int_{0}^{1} \\frac{\\partial f\\big(\\gamma(\\alpha)\\big)}{\\partial x_i} \\, d\\alpha \\, .\n$$\nNumerically approximate the integral using the midpoint Riemann sum with $M$ uniform subintervals:\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') \\;\\approx\\; (x_i - x_i') \\cdot \\frac{1}{M}\\sum_{k=1}^{M} \\left.\\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\right|_{\\mathbf{x}=\\gamma\\!\\left(\\frac{k-\\tfrac{1}{2}}{M}\\right)} \\, .\n$$\nFor each test case below, compute the absolute completeness residual\n$$\n\\varepsilon \\;=\\; \\left| \\sum_{i=1}^{6} \\phi_i \\;-\\; \\big(f(\\mathbf{x}) - f(\\mathbf{x}')\\big) \\right|.\n$$\n\nUse the following baseline and test suite. The baseline is\n$$\n\\mathbf{x}' \\;=\\; (0.6,\\, 0.5,\\, 0.4,\\, 0.5,\\, 0.3,\\, 0.4) \\, .\n$$\nTest cases are tuples $(\\mathbf{x}, M)$:\n- Case A: $\\mathbf{x}_1 = (0.85,\\, 0.7,\\, 0.2,\\, 0.55,\\, 0.8,\\, 0.9)$ with $M=200$.\n- Case B: $\\mathbf{x}_2 = (0.6,\\, 0.5,\\, 0.4,\\, 0.5,\\, 0.3,\\, 0.4)$ with $M=50$.\n- Case C: $\\mathbf{x}_3 = (0.1,\\, 0.2,\\, 0.9,\\, 0.3,\\, 0.1,\\, 0.05)$ with $M=500$.\n- Case D: $\\mathbf{x}_4 = (0.95,\\, 0.9,\\, 0.1,\\, 0.6,\\, 0.95,\\, 0.95)$ with $M=50$.\n- Case E: $\\mathbf{x}_5 = (0.6,\\, 0.52,\\, 0.39,\\, 0.49,\\, 0.31,\\, 0.41)$ with $M=10$.\n- Case F: $\\mathbf{x}_6 = (0.9,\\, 0.1,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.9)$ with $M=20$.\n\nYour program must:\n- Implement $f$ and its gradient via the chain rule using $\\sigma(z) = 1/(1+e^{-z})$ for $\\frac{d}{dz}\\mathrm{softplus}(z)=\\sigma(z)$.\n- Compute $\\phi_i$ using the midpoint Riemann sum with the specified $M$ for each test case.\n- Return the list of absolute completeness residuals $\\varepsilon$ for the six cases.\n\nFinal output format: Your program should produce a single line of output containing the six residuals as a comma-separated list enclosed in square brackets, with each number rounded to ten decimal places (e.g., $[0.0000000000,0.1234567890,\\dots]$). No other text should be printed.",
            "solution": "The user has provided a well-posed and scientifically grounded problem in the domain of explainable artificial intelligence for climate modeling. The task is to first provide a theoretical justification for the completeness property of a specific feature attribution method and then to numerically verify this property for a given model and set of test cases.\n\n### Theoretical Derivation of the Completeness Property\n\nThe objective is to demonstrate that the proposed attribution scheme satisfies the completeness property by construction. The property states that the sum of the attributions for each feature, $\\{\\phi_i\\}_{i=1}^d$, must equal the total change in the predictor's output between the input $\\mathbf{x}$ and the baseline $\\mathbf{x}'$:\n$$\n\\sum_{i=1}^d \\phi_i = f(\\mathbf{x}) - f(\\mathbf{x}')\n$$\nThe derivation relies on the Fundamental Theorem of Calculus and the multivariable chain rule, applied along a specified path.\n\nLet $f: \\mathbb{R}^d \\to \\mathbb{R}$ be a continuously differentiable function. The problem defines a straight-line path $\\gamma(\\alpha)$ from the baseline $\\mathbf{x}'$ to the input $\\mathbf{x}$ parameterized by $\\alpha \\in [0, 1]$:\n$$\n\\gamma(\\alpha) = \\mathbf{x}' + \\alpha(\\mathbf{x} - \\mathbf{x}')\n$$\nObserve that $\\gamma(0) = \\mathbf{x}'$ and $\\gamma(1) = \\mathbf{x}$.\n\nLet us define a new single-variable function $g(\\alpha) = f(\\gamma(\\alpha))$, which represents the value of the predictor $f$ along the path. By the Fundamental Theorem of Calculus, the total change in $g$ from $\\alpha=0$ to $\\alpha=1$ is the integral of its derivative:\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = g(1) - g(0) = \\int_0^1 \\frac{dg}{d\\alpha} \\, d\\alpha\n$$\nTo compute the derivative $\\frac{dg}{d\\alpha}$, we apply the multivariable chain rule. The function $g(\\alpha)$ is a composition $f(\\gamma_1(\\alpha), \\gamma_2(\\alpha), \\dots, \\gamma_d(\\alpha))$, where $\\gamma_i(\\alpha) = x_i' + \\alpha(x_i - x_i')$ is the $i$-th component of the path vector. The derivative of each component with respect to $\\alpha$ is:\n$$\n\\frac{d\\gamma_i}{d\\alpha} = x_i - x_i'\n$$\nThe chain rule gives:\n$$\n\\frac{dg}{d\\alpha} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot \\frac{d\\gamma_i}{d\\alpha} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot (x_i - x_i')\n$$\nSubstituting this expression for the derivative back into the integral, we obtain:\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = \\int_0^1 \\left( \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot (x_i - x_i') \\right) \\, d\\alpha\n$$\nSince the summation is finite, we can exchange the order of integration and summation. The term $(x_i - x_i')$ is a constant with respect to $\\alpha$ and can be moved outside the integral:\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = \\sum_{i=1}^d (x_i - x_i') \\int_0^1 \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\, d\\alpha\n$$\nThe expression on the right-hand side is precisely the sum of the attributions $\\phi_i$ as they are defined in the problem statement:\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') = (x_i - x_i') \\int_{0}^{1} \\frac{\\partial f\\big(\\gamma(\\alpha)\\big)}{\\partial x_i} \\, d\\alpha\n$$\nThus, we have formally shown that $\\sum_{i=1}^d \\phi_i = f(\\mathbf{x}) - f(\\mathbf{x}')$. This specific attribution method, known as Integrated Gradients, is complete by construction.\n\n### Numerical Verification Procedure\n\nThe second part of the task is to numerically verify this property. The integral in the definition of $\\phi_i$ is approximated using the midpoint Riemann sum with $M$ uniform subintervals. The absolute completeness residual, $\\varepsilon$, measures the error of this numerical approximation.\n\n$$\n\\varepsilon = \\left| \\sum_{i=1}^{6} \\phi_i^{\\text{approx}} - \\big(f(\\mathbf{x}) - f(\\mathbf{x}')\\big) \\right|\n$$\nwhere $\\phi_i^{\\text{approx}}$ is the numerically computed attribution for feature $i$.\n\nThe predictor function $f(\\mathbf{x})$ is defined as $f(\\mathbf{x}) = \\mathrm{softplus}(z(\\mathbf{x}))$, where $\\mathbf{x} = (x_1, \\dots, x_6) = (m, u, s, t, o, c)$. The intermediate function $z(\\mathbf{x})$ is:\n$$\nz(\\mathbf{x}) = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 + a_5 x_5 + a_6 x_6 + a_7 x_1 x_6 + a_8 x_2 x_5 - a_9 x_3 x_4\n$$\nTo compute the attribution, we first need the gradient of $f$. Using the chain rule, $\\frac{\\partial f}{\\partial x_i} = \\frac{d\\mathrm{softplus}(z)}{dz} \\cdot \\frac{\\partial z}{\\partial x_i}$. The problem provides that $\\frac{d\\mathrm{softplus}(z)}{dz} = \\sigma(z) = 1/(1+e^{-z})$. The partial derivatives of $z(\\mathbf{x})$ are:\n$$\n\\frac{\\partial z}{\\partial x_1} = a_1 + a_7 x_6, \\quad \\frac{\\partial z}{\\partial x_2} = a_2 + a_8 x_5, \\quad \\frac{\\partial z}{\\partial x_3} = a_3 - a_9 x_4 $$\n$$\n\\frac{\\partial z}{\\partial x_4} = a_4 - a_9 x_3, \\quad \\frac{\\partial z}{\\partial x_5} = a_5 + a_8 x_2, \\quad \\frac{\\partial z}{\\partial x_6} = a_6 + a_7 x_1\n$$\nThe numerical approximation for $\\phi_i$ is then:\n$$\n\\phi_i^{\\text{approx}} = (x_i - x_i') \\cdot \\frac{1}{M}\\sum_{k=1}^{M} \\left. \\left( \\sigma(z(\\mathbf{x})) \\frac{\\partial z}{\\partial x_i} \\right) \\right|_{\\mathbf{x}=\\gamma(\\alpha_k)} \\quad \\text{where } \\alpha_k = \\frac{k - 1/2}{M}\n$$\nThe implementation will compute the sum of these approximate attributions, $\\sum_{i=1}^6 \\phi_i^{\\text{approx}}$, and compare it to the exact difference $f(\\mathbf{x}) - f(\\mathbf{x}')$ to find the residual $\\varepsilon$ for each test case. A vectorized implementation is employed for efficiency, where all points along the integration path are evaluated simultaneously. The residual $\\varepsilon$ is expected to be small and decrease with increasing $M$, reflecting the convergence of the numerical integration scheme. For the special case where $\\mathbf{x} = \\mathbf{x}'$, the path has zero length, so both the sum of attributions and the total change are exactly $0$, yielding a residual of $0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of numerically verifying the completeness property\n    for an attribution method in a climate science context.\n    \"\"\"\n\n    # Define coefficients and baseline from the problem statement\n    a = {\n        'a0': -1.2, 'a1': 2.0, 'a2': 1.5, 'a3': -0.7, 'a4': 0.3, \n        'a5': 1.2, 'a6': 1.8, 'a7': 2.5, 'a8': 1.0, 'a9': 0.6\n    }\n    x_prime = np.array([0.6, 0.5, 0.4, 0.5, 0.3, 0.4])\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (x_vector, M_steps).\n    test_cases = [\n        (np.array([0.85, 0.7, 0.2, 0.55, 0.8, 0.9]), 200),  # Case A\n        (np.array([0.6, 0.5, 0.4, 0.5, 0.3, 0.4]), 50),     # Case B\n        (np.array([0.1, 0.2, 0.9, 0.3, 0.1, 0.05]), 500),  # Case C\n        (np.array([0.95, 0.9, 0.1, 0.6, 0.95, 0.95]), 50),   # Case D\n        (np.array([0.6, 0.52, 0.39, 0.49, 0.31, 0.41]), 10), # Case E\n        (np.array([0.9, 0.1, 0.9, 0.9, 0.0, 0.9]), 20),     # Case F\n    ]\n\n    def z_func(x_vec):\n        \"\"\"Computes the intermediate function z(x), vectorized.\"\"\"\n        # x_vec can be a 1D array (single point) or 2D array (multiple points)\n        single_dim = x_vec.ndim == 1\n        if single_dim:\n            x_vec = x_vec[np.newaxis, :]\n\n        m, u, s, t, o, c = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2], x_vec[:, 3], x_vec[:, 4], x_vec[:, 5]\n        \n        val = (a['a0'] + a['a1'] * m + a['a2'] * u + a['a3'] * s + a['a4'] * t + \n               a['a5'] * o + a['a6'] * c + a['a7'] * m * c + \n               a['a8'] * u * o - a['a9'] * s * t)\n        \n        return val[0] if single_dim else val\n\n    def softplus(z_val):\n        \"\"\"Numerically stable softplus function, vectorized.\"\"\"\n        # For large z, softplus(z) approx z. This avoids overflow in np.exp(z).\n        return np.where(z_val  30, z_val, np.log1p(np.exp(z_val)))\n\n    def f_func(x_vec):\n        \"\"\"Computes the predictor function f(x), vectorized.\"\"\"\n        return softplus(z_func(x_vec))\n\n    def sigma(z_val):\n        \"\"\"Numerically stable sigmoid function, vectorized.\"\"\"\n        # For z = 0, use 1/(1+e^-z)\n        # For z  0, use e^z/(1+e^z) to avoid overflow\n        return np.where(\n            z_val = 0,\n            1.0 / (1.0 + np.exp(-z_val)),\n            np.exp(z_val) / (1.0 + np.exp(z_val))\n        )\n\n    def grad_z(x_vec):\n        \"\"\"Computes the gradient of z(x), vectorized.\"\"\"\n        single_dim = x_vec.ndim == 1\n        if single_dim:\n            x_vec = x_vec[np.newaxis, :]\n\n        n_pts = x_vec.shape[0]\n        grad = np.zeros((n_pts, 6))\n        \n        m, u, s, t, o, c = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2], x_vec[:, 3], x_vec[:, 4], x_vec[:, 5]\n\n        grad[:, 0] = a['a1'] + a['a7'] * c  # d/dm\n        grad[:, 1] = a['a2'] + a['a8'] * o  # d/du\n        grad[:, 2] = a['a3'] - a['a9'] * t  # d/ds\n        grad[:, 3] = a['a4'] - a['a9'] * s  # d/dt\n        grad[:, 4] = a['a5'] + a['a8'] * u  # d/do\n        grad[:, 5] = a['a6'] + a['a7'] * m  # d/dc\n\n        return grad[0] if single_dim else grad\n\n    def grad_f(x_vec):\n        \"\"\"Computes the gradient of f(x), vectorized.\"\"\"\n        z_val = z_func(x_vec)\n        sigma_val = sigma(z_val)\n        grad_z_val = grad_z(x_vec)\n        \n        # Broadcasting sigma_val for element-wise multiplication\n        if x_vec.ndim  1:\n            sigma_val = sigma_val[:, np.newaxis]\n            \n        return sigma_val * grad_z_val\n\n    def calculate_residual(x_vec, x_prime_vec, M):\n        \"\"\"\n        Calculates the absolute completeness residual for a given case.\n        \"\"\"\n        # Handle the trivial case where x equals the baseline\n        if np.array_equal(x_vec, x_prime_vec):\n            return 0.0\n\n        delta_x = x_vec - x_prime_vec\n        \n        # Generate midpoints for the Riemann sum\n        alphas = (np.arange(1, M + 1) - 0.5) / M\n        \n        # Create all points along the path in a vectorized manner\n        # path_points will be an (M, 6) array\n        path_points = x_prime_vec + alphas[:, np.newaxis] * delta_x\n        \n        # Compute gradients at all path points\n        all_grads = grad_f(path_points)\n        \n        # Average the gradients over the path\n        avg_grads = np.mean(all_grads, axis=0)\n        \n        # The sum of attributions is the dot product of delta_x and avg_grads\n        total_attribution = np.dot(delta_x, avg_grads)\n        \n        # The exact change in the function output\n        total_change = f_func(x_vec) - f_func(x_prime_vec)\n        \n        # The absolute completeness residual\n        residual = np.abs(total_attribution - total_change)\n        \n        return residual\n\n    results = []\n    for x, M in test_cases:\n        residual = calculate_residual(x, x_prime, M)\n        results.append(f\"{residual:.10f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In interconnected systems like the Earth's climate, correlation does not imply causation, and a major goal of XAI is to move beyond simple statistical attributions toward causal explanations. Confounding variables can create spurious associations that produce misleading feature importances, a critical challenge for scientifically-grounded interpretation. This hands-on simulation tasks you with building a Structural Causal Model (SCM) to demonstrate how confounding biases gradient-based explanations and how causal adjustment can correct for it, providing a clear path towards more robust and meaningful insights .",
            "id": "4040934",
            "problem": "Consider a synthetic climate modeling scenario where the relationships among standardized anomalies for Sea Surface Temperature ($\\mathrm{SST}$), mid-tropospheric geopotential height at $500\\,\\mathrm{hPa}$ ($\\mathrm{Z500}$), and a target scalar quantity $y$ (for example, regional precipitation anomaly) are governed by a linear Structural Causal Model (SCM). Let $u$ denote an unobserved seasonal-planetary state that acts as a common cause of both $\\mathrm{SST}$ and $\\mathrm{Z500}$. The data-generating process is specified by the following equations:\n$$\nu \\sim \\mathcal{N}(0,1),\n$$\n$$\n\\mathrm{SST} = a\\,u + \\varepsilon_s,\\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2),\n$$\n$$\n\\mathrm{Z500} = b\\,u + \\varepsilon_z,\\quad \\varepsilon_z \\sim \\mathcal{N}(0,\\sigma_z^2),\n$$\n$$\ny = d\\,\\mathrm{Z500} + \\varepsilon_y,\\quad \\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2).\n$$\nAll variables are standardized anomalies and therefore dimensionless. There is no direct causal effect of $\\mathrm{SST}$ on $y$; any statistical association between $\\mathrm{SST}$ and $y$ arises purely through confounding by $u$ and the resulting correlation between $\\mathrm{SST}$ and $\\mathrm{Z500}$. In this setting, a gradient-based attribution for a linear predictive model is the partial derivative of the prediction with respect to an input, which equals the model’s coefficient for that input.\n\nYou must construct a complete, runnable program that:\n- Generates synthetic datasets from the SCM for each test case below using a fixed random seed, independently for each case.\n- Trains two linear predictors using ridge regression (squared loss with $\\ell_2$ penalty). The ridge coefficient vector $\\beta$ must be computed as $\\beta = \\left(X^\\top X + \\lambda I\\right)^{-1} X^\\top y$, where $X$ is the feature matrix after mean-centering its columns and $y$ is mean-centered.\n- Computes a “naive” gradient-based attribution by training a ridge regression model using $\\mathrm{SST}$ only as the predictor of $y$, yielding a single coefficient that serves as the gradient-based attribution for $\\mathrm{SST}$.\n- Computes an “adjusted” causal attribution by training a ridge regression model using the adjustment set $\\{\\mathrm{SST},\\mathrm{Z500},u\\}$, yielding coefficients for $\\mathrm{SST}$ and $\\mathrm{Z500}$. In the SCM above, $\\{u\\}$ blocks the backdoor path that confounds $\\mathrm{SST}$ and $y$, and including $\\mathrm{Z500}$ identifies the direct physical driver of $y$.\n- Reports, for each test case, the following list in order: the naive $\\mathrm{SST}$ coefficient, the adjusted $\\mathrm{SST}$ coefficient, the adjusted $\\mathrm{Z500}$ coefficient, a boolean indicating whether the absolute adjusted $\\mathrm{SST}$ attribution is less than or equal to the absolute naive attribution, and a boolean indicating whether the adjusted $\\mathrm{Z500}$ coefficient is within a specified tolerance of the true causal effect $d$.\n\nFundamental base to reason from:\n- Definitions and properties of linear Structural Causal Models (SCMs) and the concept of backdoor adjustment.\n- The definition of ridge regression and its closed-form solution.\n- The gradient-based attribution for linear models equals the model’s coefficient for each input.\n- The notion that confounding by a common cause induces spurious associations that can bias naive attributions, and that conditioning on a valid adjustment set can recover causal explanations.\n\nTest suite:\n- Case A (strong confounding, well-sampled): $N=10000$, $a=1.5$, $b=1.0$, $d=2.0$, $\\sigma_s=0.3$, $\\sigma_z=0.3$, $\\sigma_y=0.5$, $\\lambda=1.0$, tolerance for $d$-closeness $=0.15$, seed $=42$.\n- Case B (no confounding, well-sampled): $N=10000$, $a=0.0$, $b=1.0$, $d=2.0$, $\\sigma_s=0.3$, $\\sigma_z=0.3$, $\\sigma_y=0.5$, $\\lambda=1.0$, tolerance $=0.15$, seed $=43$.\n- Case C (strong confounding, small sample and noisy): $N=100$, $a=1.5$, $b=1.0$, $d=2.0$, $\\sigma_s=1.0$, $\\sigma_z=1.0$, $\\sigma_y=1.0$, $\\lambda=2.0$, tolerance $=0.5$, seed $=44$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list of lists, in the exact order A, B, C, where each inner list is:\n$[\\text{naive\\_sst},\\text{adjusted\\_sst},\\text{adjusted\\_z500},\\text{bias\\_reduced},\\text{z500\\_close}]$\nFor example: \n$[[x_A,y_A,z_A,b_A,c_A],[x_B,y_B,z_B,b_B,c_B],[x_C,y_C,z_C,b_C,c_C]]$\nAll reported quantities are dimensionless; booleans must be either $\\texttt{True}$ or $\\texttt{False}$.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of causal inference and linear regression, well-posed with all necessary parameters and a clear objective, and free from any of the specified invalidity criteria. We may therefore proceed with a full solution.\n\nThe problem requires us to simulate a synthetic climate scenario described by a linear Structural Causal Model (SCM) and analyze the difference between a naive statistical attribution and a causally-informed attribution. The SCM defines the data-generating process as follows:\n$$\nu \\sim \\mathcal{N}(0,1)\n$$\n$$\n\\mathrm{SST} = a\\,u + \\varepsilon_s,\\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2)\n$$\n$$\n\\mathrm{Z500} = b\\,u + \\varepsilon_z,\\quad \\varepsilon_z \\sim \\mathcal{N}(0,\\sigma_z^2)\n$$\n$$\ny = d\\,\\mathrm{Z500} + \\varepsilon_y,\\quad \\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2)\n$$\nHere, $u$ is an unobserved common cause (confounder) of Sea Surface Temperature ($\\mathrm{SST}$) and mid-tropospheric geopotential height ($\\mathrm{Z500}$). The target variable $y$ is directly caused only by $\\mathrm{Z500}$. There is no direct causal link from $\\mathrm{SST}$ to $y$. However, a statistical model might find a spurious correlation between $\\mathrm{SST}$ and $y$ due to the backdoor path $\\mathrm{SST} \\leftarrow u \\rightarrow \\mathrm{Z500} \\rightarrow y$.\n\nOur task is to quantify this effect by comparing two linear models trained with ridge regression. The gradient-based attribution for a linear model is simply the coefficient associated with each input variable.\n\n**Step 1: Data Generation**\nFor each test case, we will generate a dataset of size $N$ according to the SCM. We will use a dedicated random number generator for each case, initialized with the specified seed, to ensure reproducibility. The process is as follows:\n$1$. Generate $N$ samples of the latent variable $u$ from a standard normal distribution $\\mathcal{N}(0,1)$.\n$2$. Generate $N$ samples for each noise term, $\\varepsilon_s$, $\\varepsilon_z$, and $\\varepsilon_y$, from their respective normal distributions, $\\mathcal{N}(0, \\sigma_s^2)$, $\\mathcal{N}(0, \\sigma_z^2)$, and $\\mathcal{N}(0, \\sigma_y^2)$.\n$3$. Compute the observable variables $\\mathrm{SST}$, $\\mathrm{Z500}$, and $y$ using the linear equations provided in the SCM.\n\n**Step 2: Ridge Regression Implementation**\nWe will implement a function to perform ridge regression. Given a feature matrix $X$ and a target vector $y$, the ridge regression coefficient vector $\\beta$ is calculated using the closed-form solution:\n$$\n\\beta = \\left(X_c^\\top X_c + \\lambda I\\right)^{-1} X_c^\\top y_c\n$$\nwhere $X_c$ is the feature matrix with its columns mean-centered, $y_c$ is the mean-centered target vector, $\\lambda$ is the regularization parameter, and $I$ is the identity matrix of appropriate dimension. Mean-centering ensures that we are estimating the slope coefficients of a model that passes through the mean of the data, effectively handling the intercept term implicitly.\n\n**Step 3: Naive Attribution Model**\nThe first model is a \"naive\" one that attempts to predict $y$ using only $\\mathrm{SST}$. The regression model is $y \\sim \\mathrm{SST}$.\nThe feature matrix $X_{\\text{naive}}$ will have dimensions $(N, 1)$ and contain the $\\mathrm{SST}$ data. After mean-centering, we apply the ridge regression formula. The resulting coefficient, $\\beta_{\\text{naive,SST}}$, represents the naive gradient-based attribution of $y$ to $\\mathrm{SST}$. When the confounding is strong (i.e., $a \\neq 0$), we expect this coefficient to be non-zero, spuriously suggesting a relationship between $\\mathrm{SST}$ and $y$. This is because $\\mathrm{SST}$ acts as a proxy for the true causal driver $u$.\n\n**Step 4: Adjusted Causal Attribution Model**\nThe second model is an \"adjusted\" one, designed to provide a better causal estimate. The problem states that the adjustment set is $\\{\\mathrm{SST}, \\mathrm{Z500}, u\\}$. The regression model is $y \\sim \\mathrm{SST} + \\mathrm{Z500} + u$.\nThe feature matrix $X_{\\text{adjusted}}$ will have dimensions $(N, 3)$, with columns corresponding to $\\mathrm{SST}$, $\\mathrm{Z500}$, and the (in practice, unobserved) confounder $u$. By including $u$ in the regression, we explicitly control for the common cause, blocking the backdoor path between $\\mathrm{SST}$ and $y$. This is the principle of backdoor adjustment. The coefficient for $\\mathrm{SST}$ in this model, $\\beta_{\\text{adj,SST}}$, should be an unbiased estimate of the true direct causal effect of $\\mathrm{SST}$ on $y$. According to the SCM, this effect is $0$. Therefore, we expect $\\beta_{\\text{adj,SST}}$ to be close to $0$. The coefficient for $\\mathrm{Z500}$, $\\beta_{\\text{adj,Z500}}$, should be an unbiased estimate of the true direct causal effect of $\\mathrm{Z500}$ on $y$, which is the parameter $d$.\n\n**Step 5: Reporting the Results**\nFor each test case, we compute and report five values:\n$1$. `naive_sst`: The coefficient $\\beta_{\\text{naive,SST}}$ from the naive model.\n$2$. `adjusted_sst`: The coefficient $\\beta_{\\text{adj,SST}}$ from the adjusted model.\n$3$. `adjusted_z500`: The coefficient $\\beta_{\\text{adj,Z500}}$ from the adjusted model.\n$4$. `bias_reduced`: A boolean value, `True` if $|\\beta_{\\text{adj,SST}}| \\le |\\beta_{\\text{naive,SST}}|$. This tests whether adjusting for the confounder reduces the magnitude of the (spurious) attribution to $\\mathrm{SST}$. We expect this to be `True` in cases of confounding.\n$5$. `z500_close`: A boolean value, `True` if $|\\beta_{\\text{adj,Z500}} - d| \\le \\text{tolerance}$. This tests whether the adjusted model correctly recovers the true causal parameter $d$ within a given numerical tolerance. The accuracy of this estimate will depend on sample size $N$ and noise levels.\n\nThis procedure will be applied to all three test cases, demonstrating how causal adjustment helps to correct biased attributions resulting from confounding, a critical consideration for building trustworthy XAI methods in complex systems like climate science.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating data from the SCM, training naive and\n    adjusted ridge regression models, and reporting the resulting coefficients\n    and comparisons.\n    \"\"\"\n    test_cases = [\n        # Case A (strong confounding, well-sampled)\n        {'N': 10000, 'a': 1.5, 'b': 1.0, 'd': 2.0, 'sigma_s': 0.3, 'sigma_z': 0.3,\n         'sigma_y': 0.5, 'lambda_val': 1.0, 'tolerance': 0.15, 'seed': 42},\n        # Case B (no confounding, well-sampled)\n        {'N': 10000, 'a': 0.0, 'b': 1.0, 'd': 2.0, 'sigma_s': 0.3, 'sigma_z': 0.3,\n         'sigma_y': 0.5, 'lambda_val': 1.0, 'tolerance': 0.15, 'seed': 43},\n        # Case C (strong confounding, small sample and noisy)\n        {'N': 100, 'a': 1.5, 'b': 1.0, 'd': 2.0, 'sigma_s': 1.0, 'sigma_z': 1.0,\n         'sigma_y': 1.0, 'lambda_val': 2.0, 'tolerance': 0.5, 'seed': 44},\n    ]\n\n    results = []\n\n    def run_ridge_regression(X: np.ndarray, y: np.ndarray, lambda_val: float) - np.ndarray:\n        \"\"\"\n        Computes ridge regression coefficients using the closed-form solution.\n        Assumes X and y are not yet mean-centered.\n        \"\"\"\n        # Mean-center the data\n        X_c = X - X.mean(axis=0)\n        y_c = y - y.mean()\n\n        n_samples, n_features = X_c.shape\n        \n        # Identity matrix for regularization term\n        I = np.identity(n_features)\n        \n        # Closed-form solution: beta = (X'X + lambda*I)^-1 * X'y\n        try:\n            A = X_c.T @ X_c + lambda_val * I\n            B = X_c.T @ y_c\n            beta = np.linalg.inv(A) @ B\n        except np.linalg.LinAlgError:\n            # In case of singularity, although unlikely with lambda  0\n            beta = np.zeros(n_features)\n            \n        return beta\n\n    for case in test_cases:\n        N = case['N']\n        a = case['a']\n        b = case['b']\n        d = case['d']\n        sigma_s = case['sigma_s']\n        sigma_z = case['sigma_z']\n        sigma_y = case['sigma_y']\n        lambda_val = case['lambda_val']\n        tolerance = case['tolerance']\n        seed = case['seed']\n        \n        # 1. Generate synthetic data from the SCM\n        rng = np.random.default_rng(seed)\n        \n        u = rng.normal(loc=0.0, scale=1.0, size=N)\n        eps_s = rng.normal(loc=0.0, scale=sigma_s, size=N)\n        eps_z = rng.normal(loc=0.0, scale=sigma_z, size=N)\n        eps_y = rng.normal(loc=0.0, scale=sigma_y, size=N)\n        \n        sst = a * u + eps_s\n        z500 = b * u + eps_z\n        y = d * z500 + eps_y\n\n        # 2. Compute naive attribution (y ~ SST)\n        X_naive = sst.reshape(-1, 1)\n        beta_naive = run_ridge_regression(X_naive, y, lambda_val)\n        naive_sst = beta_naive[0]\n\n        # 3. Compute adjusted attribution (y ~ SST + Z500 + u)\n        X_adjusted = np.vstack([sst, z500, u]).T\n        beta_adjusted = run_ridge_regression(X_adjusted, y, lambda_val)\n        adjusted_sst = beta_adjusted[0]\n        adjusted_z500 = beta_adjusted[1]\n        \n        # 4. Perform comparisons\n        bias_reduced = abs(adjusted_sst) = abs(naive_sst)\n        z500_close = abs(adjusted_z500 - d) = tolerance\n        \n        # 5. Store results for the case\n        case_results = [\n            naive_sst, \n            adjusted_sst, \n            adjusted_z500,\n            bool(bias_reduced),  # Ensure Python bool not numpy.bool_\n            bool(z500_close)\n        ]\n        results.append(case_results)\n\n    # Format the final output string\n    # E.g., [[x_A,y_A,z_A,b_A,c_A],[x_B,y_B,z_B,b_B,c_B],[x_C,y_C,z_C,b_C,c_C]]\n    # Using a list comprehension to build the string representation of each inner list\n    result_str = '[' + ','.join(['[' + ','.join(map(str, res)) + ']' for res in results]) + ']'\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}