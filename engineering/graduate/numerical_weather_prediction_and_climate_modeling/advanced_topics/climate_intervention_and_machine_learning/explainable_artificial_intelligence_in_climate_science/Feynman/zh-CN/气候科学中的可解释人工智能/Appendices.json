{
    "hands_on_practices": [
        {
            "introduction": "在气候科学中应用可解释人工智能（XAI）的第一步，是理解深度学习模型本身的结构如何影响其决策。感受野（receptive field）是一个基本概念，它界定了模型输出依赖于哪些输入区域。本练习  将通过一个用于降水预报的 CNN-LSTM 混合模型，引导您从第一性原理出发，计算模型的时空感受野，从而揭示模型架构如何从根本上决定其解释的范围。",
            "id": "4040909",
            "problem": "考虑一个在数值天气预报 (NWP) 和气候建模中使用的降水预报系统，该系统采用卷积神经网络 (CNN) 后接长短期记忆 (LSTM) 网络。卷积神经网络 (CNN) 和长短期记忆 (LSTM) 是机器学习中的标准架构；在这里，它们被用来处理时空降水场，以生成未来一个时间步的预报。在气候科学领域可解释人工智能的更广阔背景下，时空感受野量化了哪些输入网格单元和过去的时间步理论上可以影响给定的预报输出，为解释模型归因和显著图提供了机理基础。\n\n给定以下架构和假设：\n\n- 输入是在规则网格上的 $T$ 个小时降水场序列。在每个时间步，一个空间 CNN 处理该场，以生成由步长决定的更粗分辨率的特征。\n- 空间 CNN 由 4 个二维卷积层组成，这些层具有方形核和各向同性的步长，并按顺序应用于每个输入帧：\n  - 层 $\\mathrm{S1}$：核大小 $k_1=3$，步长 $s_1=1$。\n  - 层 $\\mathrm{S2}$：核大小 $k_2=5$，步长 $s_2=2$。\n  - 层 $\\mathrm{S3}$：核大小 $k_3=3$，步长 $s_3=2$。\n  - 层 $\\mathrm{S4}$：核大小 $k_4=3$，步长 $s_4=1$。\n  - 没有使用空洞卷积，并使用标准的零填充来保持对齐；当 $s_{\\ell}=1$ 时，空间分辨率得以保持，当 $s_{\\ell}=2$ 时，该层的分辨率在每个空间轴上都被下采样 2 倍。\n- 在 $\\mathrm{S4}$ 生成的最终特征图的每个空间位置，一个 $1 \\times 1$ 卷积将特征映射为一个标量，从而在每个位置上产生一个随时间变化的潜在序列。\n- 时间模块是一个一维时间卷积，后接一个 LSTM：\n  - 时间卷积 $\\mathrm{T1}$：核大小 $k_t=5$，步长 $s_t=2$，在每个空间位置上沿时间维度独立应用，生成一个下采样的潜在特征序列。\n  - LSTM 仅使用 $\\mathrm{T1}$ 的最近 $W=4$ 个输出（一个固定的滑动窗口），并在 $\\mathrm{S4}$ 特征图的相应空间位置上输出下一个时间步的预报。\n- 关注 $\\mathrm{S4}$ 特征图中心空间位置的预报，以及相对于最近输入的提前一个时间步的预报。\n\n使用离散卷积和采样理论的第一性原理，将此 CNN-LSTM 架构的时空感受野定义为在所述假设下，理论上能够影响所选预报输出的、跨越空间和过去时间步的输入网格单元集合。然后，根据各层的核大小和步长，以闭合形式推导沿两个空间轴（输入网格单元的宽度和高度）和沿时间轴（输入时间步数）的有效感受野大小，并计算上述架构的数值。\n\n以行矩阵的形式提供最终答案，列出有效感受野的空间宽度、空间高度和时间长度，单位为无量纲计数（网格单元和时间步）。无需四舍五入；报告精确的整数计数。以 $\\left[ \\text{width} \\;\\; \\text{height} \\;\\; \\text{time} \\right]$ 的形式表示最终答案。",
            "solution": "我们首先为给定架构形式化时空感受野。对于在特定输出空间位置和提前一个时间步生成的预报，感受野是其值可以通过网络操作影响输出的输入空间网格单元和过去时间索引的集合。在一个没有跳跃连接的纯前馈卷积网络中，这个集合由核的支撑集以及位置通过各层进行采样和映射的步长决定。在时间维度上，一个带步长的一维卷积，后接一个在固定窗口上的循环计算，意味着一个由该卷积的核、步长以及 LSTM 窗口长度决定的有限时间支撑集。我们接下来分别推导空间和时间分量，然后将它们组合起来。\n\n空间感受野推导（每帧）：\n为清晰起见，我们采用标准的一维推导方法，然后通过各向同性扩展到二维。考虑一个由离散卷积堆叠而成的结构，其核大小为 $k_{\\ell}$，步长为 $s_{\\ell}$，其中 $\\ell \\in \\{1,2,3,4\\}$ 索引从 $\\mathrm{S1}$ 到 $\\mathrm{S4}$ 的各层。定义跳跃 $J_{\\ell}$ 为在第 $\\ell$ 层上相邻感受野中心之间的间距（以输入坐标计），并定义感受野大小 $R_{\\ell}$ 为沿一个轴影响第 $\\ell$ 层单个单元的输入网格单元数。基本条件是 $J_0=1$ 和 $R_0=1$，对应于一个输入像素的自身感受野和单位采样。\n\n对于一个核大小为 $k_{\\ell}$、步长为 $s_{\\ell}$ 的卷积层，其映射规则为\n$$\nJ_{\\ell} = J_{\\ell-1} \\, s_{\\ell},\n$$\n$$\nR_{\\ell} = R_{\\ell-1} + (k_{\\ell} - 1) \\, J_{\\ell-1}.\n$$\n这些规则源于以下事实：步长 $s_{\\ell}$ 相对于前一层将感受野中心的间距扩大了 $s_{\\ell}$ 倍，而大小为 $k_{\\ell}$ 的核在前一层中覆盖了中心之外的 $(k_{\\ell}-1)$ 个大小为 $J_{\\ell-1}$ 的步长。\n\n将这些递推关系应用于给定的空间层：\n- 层 $\\mathrm{S1}$ 的核大小为 $k_1=3$，步长为 $s_1=1$。因此\n$$\nJ_{1} = J_{0} \\, s_{1} = 1 \\cdot 1 = 1, \\quad R_{1} = R_{0} + (k_{1}-1) J_{0} = 1 + (3-1)\\cdot 1 = 3.\n$$\n- 层 $\\mathrm{S2}$ 的核大小为 $k_2=5$，步长为 $s_2=2$。因此\n$$\nJ_{2} = J_{1} \\, s_{2} = 1 \\cdot 2 = 2, \\quad R_{2} = R_{1} + (k_{2}-1) J_{1} = 3 + (5-1)\\cdot 1 = 7.\n$$\n- 层 $\\mathrm{S3}$ 的核大小为 $k_3=3$，步长为 $s_3=2$。因此\n$$\nJ_{3} = J_{2} \\, s_{3} = 2 \\cdot 2 = 4, \\quad R_{3} = R_{2} + (k_{3}-1) J_{2} = 7 + (3-1)\\cdot 2 = 11.\n$$\n- 层 $\\mathrm{S4}$ 的核大小为 $k_4=3$，步长为 $s_4=1$。因此\n$$\nJ_{4} = J_{3} \\, s_{4} = 4 \\cdot 1 = 4, \\quad R_{4} = R_{3} + (k_{4}-1) J_{3} = 11 + (3-1)\\cdot 4 = 19.\n$$\n\n最后的 $1 \\times 1$ 卷积不扩大感受野（$k=1$, $s=1$），因此它保持 $R_{4}$ 不变。因此，沿单个空间轴，感受野大小为 $19$。由于核和步长是各向同性和方形的，感受野在两个轴上是相同的，从而对于所考虑的 $\\mathrm{S4}$ 输出位置的单个单元，其空间感受野为 $19 \\times 19$ 个输入网格单元。\n\n时间感受野推导：\n对于时间维度，一维卷积 $\\mathrm{T1}$ 的核大小为 $k_t=5$，步长为 $s_t=2$。$\\mathrm{T1}$ 的单个输出取决于 $k_t$ 个连续的输入时间步，根据填充约定居中；$\\mathrm{T1}$ 的相邻输出由 $s_t$ 个输入步长分隔。LSTM 仅使用 $\\mathrm{T1}$ 的最近 $W=4$ 个输出来生成预报。这 $W$ 个最近的 $\\mathrm{T1}$ 输出的支撑集的并集覆盖了一个连续的输入时间步块，其长度为\n$$\nR_{t} = k_{t} + (W - 1) \\, s_{t},\n$$\n因为在没有空洞卷积和固定步长的情况下，每个后续的 $\\mathrm{T1}$ 输出都将其覆盖范围在前一个输出的支撑集之外扩展了 $s_{t}$ 个新的输入步长。代入给定值可得\n$$\nR_{t} = 5 + (4 - 1) \\cdot 2 = 11.\n$$\n\n时空感受野总结：\n在所述架构和假设下，对于所选空间位置和提前一个时间步的预报，其有效感受野在每个空间轴上跨越 $19$ 个输入网格单元，在时间轴上跨越 $11$ 个输入时间步。用可解释人工智能的术语来说，这个理论支撑集定义了可以为所选预报的归因分数或影响函数做出贡献的最大时空邻域，从而为可解释性分析提供了原则性约束。\n\n因此，所要求的列出空间宽度、空间高度和时间长度的行矩阵为 $\\left[ 19 \\;\\; 19 \\;\\; 11 \\right]$。",
            "answer": "$$\\boxed{\\begin{pmatrix}19  19  11\\end{pmatrix}}$$"
        },
        {
            "introduction": "在理解了模型的结构限制后，下一步是学习如何生成精确的特征归因。一个理想的归因方法应满足“完备性”（completeness）公理，即所有特征的归因值之和应等于模型的输出与某个基线输出之差。本练习  将指导您从微积分基本定理出发，推导一种满足此性质的归因方法，并通过数值计算来验证其有效性，从而加深对路径积分归因方法核心思想的理解。",
            "id": "4040883",
            "problem": "您正在研究数值天气预报和气候模型中一种可微降水预测器的基于归因的解释方法。考虑一个预测器 $f:\\mathbb{R}^d \\to \\mathbb{R}$、一个输入 $\\mathbf{x} \\in \\mathbb{R}^d$ 和一个代表长期平均归一化特征的气候学基线 $\\mathbf{x}' \\in \\mathbb{R}^d$。您需要在此设定下，为特征归因提出并验证一个完备性属性。整个任务必须从第一性原理出发，基于链式法则、微积分基本定理以及多元微积分中关于线积分的成熟理论。\n\n提出一个数学上精确的完备性属性：一组特征归因 $\\{\\phi_i\\}_{i=1}^d$ 相对于基线 $\\mathbf{x}'$ 是完备的，当且仅当\n$$\n\\sum_{i=1}^d \\phi_i \\;=\\; f(\\mathbf{x}) - f(\\mathbf{x}') \\, .\n$$\n使用从 $\\mathbf{x}'$ 到 $\\mathbf{x}$ 的直线路径，推导出一个通过构造满足此属性的归因方案。然后实现该方案，并数值验证其完备性残差很小。\n\n将降水预测器 $f$ 定义如下。设特征排序为 $\\mathbf{x} = (m,u,s,t,o,c)$，分别代表归一化整层水汽 $m$、归一化上升速度振幅 $u$、归一化垂直风切变 $s$、归一化温度异常 $t$、归一化地形抬升因子 $o$ 和归一化对流有效位能 $c$。设\n$$\nz(\\mathbf{x}) \\;=\\; a_0 + a_1 m + a_2 u + a_3 s + a_4 t + a_5 o + a_6 c + a_7 m c + a_8 u o - a_9 s t \\, ,\n$$\n系数为\n$$\na_0=-1.2,\\; a_1=2.0,\\; a_2=1.5,\\; a_3=-0.7,\\; a_4=0.3,\\; a_5=1.2,\\; a_6=1.8,\\; a_7=2.5,\\; a_8=1.0,\\; a_9=0.6 \\, .\n$$\n使用 softplus 连接函数：\n$$\n\\mathrm{softplus}(z) \\;=\\; \\log\\!\\big(1+e^{z}\\big) \\, ,\n$$\n并定义\n$$\nf(\\mathbf{x}) \\;=\\; \\mathrm{softplus}\\!\\left(z(\\mathbf{x})\\right) \\, .\n$$\n所有输入都是无量纲的，并且位于单位超立方体内，因此答案中无需物理单位。\n\n使用直线路径 $\\gamma(\\alpha) = \\mathbf{x}' + \\alpha(\\mathbf{x}-\\mathbf{x}')$，其中 $\\alpha \\in [0,1]$。通过偏导数的路径积分来定义特征 $i$ 的归因：\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') \\;=\\; (x_i - x_i') \\int_{0}^{1} \\frac{\\partial f\\big(\\gamma(\\alpha)\\big)}{\\partial x_i} \\, d\\alpha \\, .\n$$\n使用包含 $M$ 个均匀子区间的中点黎曼和来数值近似该积分：\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') \\;\\approx\\; (x_i - x_i') \\cdot \\frac{1}{M}\\sum_{k=1}^{M} \\left.\\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\right|_{\\mathbf{x}=\\gamma\\!\\left(\\frac{k-\\tfrac{1}{2}}{M}\\right)} \\, .\n$$\n对于下方的每个测试用例，计算绝对完备性残差\n$$\n\\varepsilon \\;=\\; \\left| \\sum_{i=1}^{6} \\phi_i \\;-\\; \\big(f(\\mathbf{x}) - f(\\mathbf{x}')\\big) \\right|.\n$$\n\n使用以下基线和测试集。基线为\n$$\n\\mathbf{x}' \\;=\\; (0.6,\\, 0.5,\\, 0.4,\\, 0.5,\\, 0.3,\\, 0.4) \\, .\n$$\n测试用例是元组 $(\\mathbf{x}, M)$：\n- 用例 A: $\\mathbf{x}_1 = (0.85,\\, 0.7,\\, 0.2,\\, 0.55,\\, 0.8,\\, 0.9)$，$M=200$。\n- 用例 B: $\\mathbf{x}_2 = (0.6,\\, 0.5,\\, 0.4,\\, 0.5,\\, 0.3,\\, 0.4)$，$M=50$。\n- 用例 C: $\\mathbf{x}_3 = (0.1,\\, 0.2,\\, 0.9,\\, 0.3,\\, 0.1,\\, 0.05)$，$M=500$。\n- 用例 D: $\\mathbf{x}_4 = (0.95,\\, 0.9,\\, 0.1,\\, 0.6,\\, 0.95,\\, 0.95)$，$M=50$。\n- 用例 E: $\\mathbf{x}_5 = (0.6,\\, 0.52,\\, 0.39,\\, 0.49,\\, 0.31,\\, 0.41)$，$M=10$。\n- 用例 F: $\\mathbf{x}_6 = (0.9,\\, 0.1,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.9)$，$M=20$。\n\n您的程序必须：\n- 使用 $\\sigma(z) = 1/(1+e^{-z})$ 作为 $\\frac{d}{dz}\\mathrm{softplus}(z)=\\sigma(z)$，通过链式法则实现 $f$ 及其梯度。\n- 对每个测试用例，使用指定数量的 $M$ 通过中点黎曼和计算 $\\phi_i$。\n- 返回六个用例的绝对完备性残差 $\\varepsilon$ 列表。\n\n最终输出格式：您的程序应生成单行输出，其中包含六个残差，格式为方括号括起来的逗号分隔列表，每个数字四舍五入到十位小数（例如 $[0.0000000000,0.1234567890,\\dots]$）。不应打印任何其他文本。",
            "solution": "用户提供了一个在气候模型可解释性人工智能领域中，定义明确且具有科学依据的问题。任务首先是为一种特定特征归因方法的完备性属性提供理论证明，然后对给定的模型和一组测试用例进行数值验证。\n\n### 完备性属性的理论推导\n\n目标是证明所提出的归因方案通过构造法满足完备性属性。该属性指出，每个特征的归因之和 $\\{\\phi_i\\}_{i=1}^d$ 必须等于预测器在输入 $\\mathbf{x}$ 和基线 $\\mathbf{x}'$ 之间输出的总变化量：\n$$\n\\sum_{i=1}^d \\phi_i = f(\\mathbf{x}) - f(\\mathbf{x}')\n$$\n该推导依赖于微积分基本定理和多元链式法则，并沿指定路径应用。\n\n设 $f: \\mathbb{R}^d \\to \\mathbb{R}$ 是一个连续可微函数。问题定义了一条从基线 $\\mathbf{x}'$ 到输入 $\\mathbf{x}$ 的直线路径 $\\gamma(\\alpha)$，由 $\\alpha \\in [0, 1]$ 参数化：\n$$\n\\gamma(\\alpha) = \\mathbf{x}' + \\alpha(\\mathbf{x} - \\mathbf{x}')\n$$\n观察可知 $\\gamma(0) = \\mathbf{x}'$ 且 $\\gamma(1) = \\mathbf{x}$。\n\n我们定义一个新的单变量函数 $g(\\alpha) = f(\\gamma(\\alpha))$，它表示预测器 $f$ 沿着该路径的值。根据微积分基本定理，$g$ 从 $\\alpha=0$ 到 $\\alpha=1$ 的总变化是其导数的积分：\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = g(1) - g(0) = \\int_0^1 \\frac{dg}{d\\alpha} \\, d\\alpha\n$$\n为了计算导数 $\\frac{dg}{d\\alpha}$，我们应用多元链式法则。函数 $g(\\alpha)$ 是一个复合函数 $f(\\gamma_1(\\alpha), \\gamma_2(\\alpha), \\dots, \\gamma_d(\\alpha))$，其中 $\\gamma_i(\\alpha) = x_i' + \\alpha(x_i - x_i')$ 是路径向量的第 $i$ 个分量。每个分量相对于 $\\alpha$ 的导数是：\n$$\n\\frac{d\\gamma_i}{d\\alpha} = x_i - x_i'\n$$\n根据链式法则可得：\n$$\n\\frac{dg}{d\\alpha} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot \\frac{d\\gamma_i}{d\\alpha} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot (x_i - x_i')\n$$\n将此导数表达式代回积分，我们得到：\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = \\int_0^1 \\left( \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot (x_i - x_i') \\right) \\, d\\alpha\n$$\n由于求和是有限的，我们可以交换积分和求和的顺序。项 $(x_i - x_i')$ 相对于 $\\alpha$ 是一个常数，可以移到积分符号外面：\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = \\sum_{i=1}^d (x_i - x_i') \\int_0^1 \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\, d\\alpha\n$$\n右侧的表达式正是问题陈述中定义的归因 $\\phi_i$ 的总和：\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') = (x_i - x_i') \\int_{0}^{1} \\frac{\\partial f\\big(\\gamma(\\alpha)\\big)}{\\partial x_i} \\, d\\alpha\n$$\n因此，我们正式证明了 $\\sum_{i=1}^d \\phi_i = f(\\mathbf{x}) - f(\\mathbf{x}')$。这种被称为“积分梯度”（Integrated Gradients）的特定归因方法，通过其构造本身就是完备的。\n\n### 数值验证过程\n\n任务的第二部分是数值验证此属性。$\\phi_i$ 定义中的积分使用具有 $M$ 个均匀子区间的中点黎曼和进行近似。绝对完备性残差 $\\varepsilon$ 用于衡量该数值近似的误差。\n\n$$\n\\varepsilon = \\left| \\sum_{i=1}^{6} \\phi_i^{\\text{approx}} - \\big(f(\\mathbf{x}) - f(\\mathbf{x}')\\big) \\right|\n$$\n其中 $\\phi_i^{\\text{approx}}$ 是特征 $i$ 的数值计算归因。\n\n预测器函数 $f(\\mathbf{x})$ 定义为 $f(\\mathbf{x}) = \\mathrm{softplus}(z(\\mathbf{x}))$，其中 $\\mathbf{x} = (x_1, \\dots, x_6) = (m, u, s, t, o, c)$。中间函数 $z(\\mathbf{x})$ 为：\n$$\nz(\\mathbf{x}) = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 + a_5 x_5 + a_6 x_6 + a_7 x_1 x_6 + a_8 x_2 x_5 - a_9 x_3 x_4\n$$\n为了计算归因，我们首先需要 $f$ 的梯度。使用链式法则，$\\frac{\\partial f}{\\partial x_i} = \\frac{d\\mathrm{softplus}(z)}{dz} \\cdot \\frac{\\partial z}{\\partial x_i}$。问题中给出 $\\frac{d\\mathrm{softplus}(z)}{dz} = \\sigma(z) = 1/(1+e^{-z})$。$z(\\mathbf{x})$ 的偏导数是：\n$$\n\\frac{\\partial z}{\\partial x_1} = a_1 + a_7 x_6, \\quad \\frac{\\partial z}{\\partial x_2} = a_2 + a_8 x_5, \\quad \\frac{\\partial z}{\\partial x_3} = a_3 - a_9 x_4 $$\n$$\n\\frac{\\partial z}{\\partial x_4} = a_4 - a_9 x_3, \\quad \\frac{\\partial z}{\\partial x_5} = a_5 + a_8 x_2, \\quad \\frac{\\partial z}{\\partial x_6} = a_6 + a_7 x_1\n$$\n$\\phi_i$ 的数值近似则为：\n$$\n\\phi_i^{\\text{approx}} = (x_i - x_i') \\cdot \\frac{1}{M}\\sum_{k=1}^{M} \\left. \\left( \\sigma(z(\\mathbf{x})) \\frac{\\partial z}{\\partial x_i} \\right) \\right|_{\\mathbf{x}=\\gamma(\\alpha_k)} \\quad \\text{其中 } \\alpha_k = \\frac{k - 1/2}{M}\n$$\n实现过程将计算这些近似归因的总和 $\\sum_{i=1}^6 \\phi_i^{\\text{approx}}$，并将其与精确差值 $f(\\mathbf{x}) - f(\\mathbf{x}')$ 进行比较，以找出每个测试用例的残差 $\\varepsilon$。为了提高效率，采用了向量化实现，其中沿积分路径的所有点都同时进行评估。残差 $\\varepsilon$ 预计会很小，并随着 $M$ 的增加而减小，这反映了数值积分方案的收敛性。对于 $\\mathbf{x} = \\mathbf{x}'$ 的特殊情况，路径长度为零，因此归因总和和总变化量都精确为 $0$，从而产生 $0$ 的残差。",
            "answer": "完整的、可运行的 Python 3 代码如下。导入的库必须符合指定的执行环境。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of numerically verifying the completeness property\n    for an attribution method in a climate science context.\n    \"\"\"\n\n    # Define coefficients and baseline from the problem statement\n    a = {\n        'a0': -1.2, 'a1': 2.0, 'a2': 1.5, 'a3': -0.7, 'a4': 0.3, \n        'a5': 1.2, 'a6': 1.8, 'a7': 2.5, 'a8': 1.0, 'a9': 0.6\n    }\n    x_prime = np.array([0.6, 0.5, 0.4, 0.5, 0.3, 0.4])\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (x_vector, M_steps).\n    test_cases = [\n        (np.array([0.85, 0.7, 0.2, 0.55, 0.8, 0.9]), 200),  # Case A\n        (np.array([0.6, 0.5, 0.4, 0.5, 0.3, 0.4]), 50),     # Case B\n        (np.array([0.1, 0.2, 0.9, 0.3, 0.1, 0.05]), 500),  # Case C\n        (np.array([0.95, 0.9, 0.1, 0.6, 0.95, 0.95]), 50),   # Case D\n        (np.array([0.6, 0.52, 0.39, 0.49, 0.31, 0.41]), 10), # Case E\n        (np.array([0.9, 0.1, 0.9, 0.9, 0.0, 0.9]), 20),     # Case F\n    ]\n\n    def z_func(x_vec):\n        \"\"\"Computes the intermediate function z(x), vectorized.\"\"\"\n        # x_vec can be a 1D array (single point) or 2D array (multiple points)\n        single_dim = x_vec.ndim == 1\n        if single_dim:\n            x_vec = x_vec[np.newaxis, :]\n\n        m, u, s, t, o, c = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2], x_vec[:, 3], x_vec[:, 4], x_vec[:, 5]\n        \n        val = (a['a0'] + a['a1'] * m + a['a2'] * u + a['a3'] * s + a['a4'] * t + \n               a['a5'] * o + a['a6'] * c + a['a7'] * m * c + \n               a['a8'] * u * o - a['a9'] * s * t)\n        \n        return val[0] if single_dim else val\n\n    def softplus(z_val):\n        \"\"\"Numerically stable softplus function, vectorized.\"\"\"\n        # For large z, softplus(z) approx z. This avoids overflow in np.exp(z).\n        return np.where(z_val > 30, z_val, np.log1p(np.exp(z_val)))\n\n    def f_func(x_vec):\n        \"\"\"Computes the predictor function f(x), vectorized.\"\"\"\n        return softplus(z_func(x_vec))\n\n    def sigma(z_val):\n        \"\"\"Numerically stable sigmoid function, vectorized.\"\"\"\n        # For z >= 0, use 1/(1+e^-z)\n        # For z  0, use e^z/(1+e^z) to avoid overflow\n        return np.where(\n            z_val >= 0,\n            1.0 / (1.0 + np.exp(-z_val)),\n            np.exp(z_val) / (1.0 + np.exp(z_val))\n        )\n\n    def grad_z(x_vec):\n        \"\"\"Computes the gradient of z(x), vectorized.\"\"\"\n        single_dim = x_vec.ndim == 1\n        if single_dim:\n            x_vec = x_vec[np.newaxis, :]\n\n        n_pts = x_vec.shape[0]\n        grad = np.zeros((n_pts, 6))\n        \n        m, u, s, t, o, c = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2], x_vec[:, 3], x_vec[:, 4], x_vec[:, 5]\n\n        grad[:, 0] = a['a1'] + a['a7'] * c  # d/dm\n        grad[:, 1] = a['a2'] + a['a8'] * o  # d/du\n        grad[:, 2] = a['a3'] - a['a9'] * t  # d/ds\n        grad[:, 3] = a['a4'] - a['a9'] * s  # d/dt\n        grad[:, 4] = a['a5'] + a['a8'] * u  # d/do\n        grad[:, 5] = a['a6'] + a['a7'] * m  # d/dc\n\n        return grad[0] if single_dim else grad\n\n    def grad_f(x_vec):\n        \"\"\"Computes the gradient of f(x), vectorized.\"\"\"\n        z_val = z_func(x_vec)\n        sigma_val = sigma(z_val)\n        grad_z_val = grad_z(x_vec)\n        \n        # Broadcasting sigma_val for element-wise multiplication\n        if x_vec.ndim > 1:\n            sigma_val = sigma_val[:, np.newaxis]\n            \n        return sigma_val * grad_z_val\n\n    def calculate_residual(x_vec, x_prime_vec, M):\n        \"\"\"\n        Calculates the absolute completeness residual for a given case.\n        \"\"\"\n        # Handle the trivial case where x equals the baseline\n        if np.array_equal(x_vec, x_prime_vec):\n            return 0.0\n\n        delta_x = x_vec - x_prime_vec\n        \n        # Generate midpoints for the Riemann sum\n        alphas = (np.arange(1, M + 1) - 0.5) / M\n        \n        # Create all points along the path in a vectorized manner\n        # path_points will be an (M, 6) array\n        path_points = x_prime_vec + alphas[:, np.newaxis] * delta_x\n        \n        # Compute gradients at all path points\n        all_grads = grad_f(path_points)\n        \n        # Average the gradients over the path\n        avg_grads = np.mean(all_grads, axis=0)\n        \n        # The sum of attributions is the dot product of delta_x and avg_grads\n        total_attribution = np.dot(delta_x, avg_grads)\n        \n        # The exact change in the function output\n        total_change = f_func(x_vec) - f_func(x_prime_vec)\n        \n        # The absolute completeness residual\n        residual = np.abs(total_attribution - total_change)\n        \n        return residual\n\n    results = []\n    for x, M in test_cases:\n        residual = calculate_residual(x, x_prime, M)\n        results.append(f\"{residual:.10f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，为了使解释不仅是相关的，而且是科学上可靠的，我们必须从关联性归因转向因果解释。在地球系统中，变量之间普遍存在混杂（confounding）现象，这可能导致标准归因方法产生误导性结论。本练习  通过一个动手编程模拟，展示了混杂变量如何造成虚假关联，并演示了如何应用因果推断中的“后门调整”原理来修正归因，从而获得更具鲁棒性和科学意义的解释。",
            "id": "4040934",
            "problem": "考虑一个合成气候建模场景，其中海面温度（$\\mathrm{SST}$）、$500\\,\\mathrm{hPa}$ 对流层中层位势高度（$\\mathrm{Z500}$）的标准化异常值与目标标量 $y$（例如，区域降水异常）之间的关系由一个线性结构因果模型（SCM）控制。设 $u$ 表示一个未观测到的季节性行星尺度状态，它是 $\\mathrm{SST}$ 和 $\\mathrm{Z500}$ 的共同原因。数据生成过程由以下方程指定：\n$$\nu \\sim \\mathcal{N}(0,1),\n$$\n$$\n\\mathrm{SST} = a\\,u + \\varepsilon_s,\\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2),\n$$\n$$\n\\mathrm{Z500} = b\\,u + \\varepsilon_z,\\quad \\varepsilon_z \\sim \\mathcal{N}(0,\\sigma_z^2),\n$$\n$$\ny = d\\,\\mathrm{Z500} + \\varepsilon_y,\\quad \\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2).\n$$\n所有变量都是标准化的异常值，因此是无量纲的。$\\mathrm{SST}$ 对 $y$ 没有直接的因果效应；$\\mathrm{SST}$ 和 $y$ 之间的任何统计关联纯粹是通过 $u$ 的混淆以及由此产生的 $\\mathrm{SST}$ 和 $\\mathrm{Z500}$ 之间的相关性而产生的。在此设置中，线性预测模型基于梯度的归因是预测值相对于输入的偏导数，它等于模型对该输入的系数。\n\n您必须构建一个完整、可运行的程序，该程序能够：\n- 针对下方的每个测试用例，使用固定的随机种子从 SCM 生成合成数据集，每个用例独立生成。\n- 使用岭回归（带 $\\ell_2$ 惩罚项的平方损失）训练两个线性预测器。岭回归系数向量 $\\beta$ 必须计算为 $\\beta = \\left(X^\\top X + \\lambda I\\right)^{-1} X^\\top y$，其中 $X$ 是其列经过均值中心化后的特征矩阵，$y$ 是经过均值中心化的。\n- 通过训练一个仅使用 $\\mathrm{SST}$ 作为 $y$ 的预测变量的岭回归模型，来计算“朴素”的基于梯度的归因，从而得到一个单一系数，该系数作为 $\\mathrm{SST}$ 的基于梯度的归因。\n- 通过训练一个使用调整集 $\\{\\mathrm{SST},\\mathrm{Z500},u\\}$ 的岭回归模型，来计算“调整后”的因果归因，从而得到 $\\mathrm{SST}$ 和 $\\mathrm{Z500}$ 的系数。在上述 SCM 中，$\\{u\\}$ 阻断了混淆 $\\mathrm{SST}$ 和 $y$ 的后门路径，并且包含 $\\mathrm{Z500}$ 可以识别出 $y$ 的直接物理驱动因素。\n- 针对每个测试用例，按顺序报告以下列表：朴素的 $\\mathrm{SST}$ 系数、调整后的 $\\mathrm{SST}$ 系数、调整后的 $\\mathrm{Z500}$ 系数、一个布尔值（指示调整后的 $\\mathrm{SST}$ 归因的绝对值是否小于或等于朴素归因的绝对值），以及一个布尔值（指示调整后的 $\\mathrm{Z500}$ 系数是否在真实因果效应 $d$ 的指定容差范围内）。\n\n推理所依据的基本原理：\n- 线性结构因果模型（SCM）的定义和性质以及后门调整的概念。\n- 岭回归的定义及其闭式解。\n- 线性模型中基于梯度的归因等于模型对每个输入的系数。\n- 由共同原因引起的混淆会引入伪关联，这会使朴素归因产生偏差，而在一个有效的调整集上进行条件化可以恢复因果解释。\n\n测试套件：\n- 用例 A（强混淆，充分采样）：$N=10000$, $a=1.5$, $b=1.0$, $d=2.0$, $\\sigma_s=0.3$, $\\sigma_z=0.3$, $\\sigma_y=0.5$, $\\lambda=1.0$, $d$ 接近度的容差为 $0.15$，种子为 $42$。\n- 用例 B（无混淆，充分采样）：$N=10000$, $a=0.0$, $b=1.0$, $d=2.0$, $\\sigma_s=0.3$, $\\sigma_z=0.3$, $\\sigma_y=0.5$, $\\lambda=1.0$, 容差为 $0.15$，种子为 $43$。\n- 用例 C（强混淆，小样本且含噪声）：$N=100$, $a=1.5$, $b=1.0$, $d=2.0$, $\\sigma_s=1.0$, $\\sigma_z=1.0$, $\\sigma_y=1.0$, $\\lambda=2.0$, 容差为 $0.5$，种子为 $44$。\n\n必需的最终输出格式：\n您的程序应生成单行输出，其中包含三个测试用例的结果，形式为逗号分隔的列表的列表，严格按照 A、B、C 的顺序，其中每个内部列表为：\n`[naive_sst,adjusted_sst,adjusted_z500,bias_reduced,z500_close]`\n例如：\n`[[x_A,y_A,z_A,b_A,c_A],[x_B,y_B,z_B,b_B,c_B],[x_C,y_C,z_C,b_C,c_C]]`\n所有报告的量均为无量纲；布尔值必须是 `True` 或 `False`。",
            "solution": "用户提供的问题是有效的。它在科学上基于因果推断和线性回归的原理，问题陈述清晰，包含了所有必要的参数和明确的目标，并且没有任何指定的无效标准。因此，我们可以着手提供完整的解决方案。\n\n该问题要求我们模拟一个由线性结构因果模型（SCM）描述的合成气候场景，并分析朴素统计归因与基于因果的归因之间的差异。SCM 定义了如下的数据生成过程：\n$$\nu \\sim \\mathcal{N}(0,1)\n$$\n$$\n\\mathrm{SST} = a\\,u + \\varepsilon_s,\\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2)\n$$\n$$\n\\mathrm{Z500} = b\\,u + \\varepsilon_z,\\quad \\varepsilon_z \\sim \\mathcal{N}(0,\\sigma_z^2)\n$$\n$$\ny = d\\,\\mathrm{Z500} + \\varepsilon_y,\\quad \\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2)\n$$\n在这里，$u$ 是海面温度（$\\mathrm{SST}$）和对流层中层位势高度（$\\mathrm{Z500}$）的一个未观测到的共同原因（混淆变量）。目标变量 $y$ 仅由 $\\mathrm{Z500}$ 直接引起。从 $\\mathrm{SST}$到 $y$ 没有直接的因果联系。然而，由于后门路径 $\\mathrm{SST} \\leftarrow u \\rightarrow \\mathrm{Z500} \\rightarrow y$，统计模型可能会发现 $\\mathrm{SST}$ 和 $y$ 之间的伪相关。\n\n我们的任务是通过比较两个用岭回归训练的线性模型来量化这种效应。对于线性模型，基于梯度的归因就是与每个输入变量相关联的系数。\n\n**第 1 步：数据生成**\n对于每个测试用例，我们将根据 SCM 生成一个大小为 $N$ 的数据集。我们将为每个用例使用一个专用的随机数生成器，并用指定的种子进行初始化，以确保可复现性。过程如下：\n1.  从标准正态分布 $\\mathcal{N}(0,1)$ 中生成 $N$ 个潜变量 $u$ 的样本。\n2.  为每个噪声项 $\\varepsilon_s$、$\\varepsilon_z$ 和 $\\varepsilon_y$ 从它们各自的正态分布 $\\mathcal{N}(0, \\sigma_s^2)$、$\\mathcal{N}(0, \\sigma_z^2)$ 和 $\\mathcal{N}(0, \\sigma_y^2)$ 生成 $N$ 个样本。\n3.  使用 SCM 中提供的线性方程计算可观测变量 $\\mathrm{SST}$、$\\mathrm{Z500}$ 和 $y$。\n\n**第 2 步：岭回归实现**\n我们将实现一个函数来执行岭回归。给定一个特征矩阵 $X$ 和一个目标向量 $y$，岭回归系数向量 $\\beta$ 使用闭式解计算：\n$$\n\\beta = \\left(X_c^\\top X_c + \\lambda I\\right)^{-1} X_c^\\top y_c\n$$\n其中 $X_c$ 是其列经过均值中心化后的特征矩阵，$y_c$ 是均值中心化后的目标向量，$\\lambda$ 是正则化参数，$I$ 是适当维度的单位矩阵。均值中心化确保我们估计的是一个通过数据均值的模型的斜率系数，从而隐式地处理了截距项。\n\n**第 3 步：朴素归因模型**\n第一个模型是“朴素”模型，它尝试仅使用 $\\mathrm{SST}$ 来预测 $y$。回归模型是 $y \\sim \\mathrm{SST}$。\n特征矩阵 $X_{\\text{naive}}$ 的维度为 $(N, 1)$，包含 $\\mathrm{SST}$ 数据。在均值中心化后，我们应用岭回归公式。得到的系数 $\\beta_{\\text{naive,SST}}$ 代表了 $y$ 对 $\\mathrm{SST}$ 的朴素的基于梯度的归因。当混淆很强时（即 $a \\neq 0$），我们预计该系数不为零，从而错误地表明 $\\mathrm{SST}$ 和 $y$ 之间存在关系。这是因为 $\\mathrm{SST}$ 充当了真正因果驱动因素 $u$ 的代理。\n\n**第 4 步：调整后的因果归因模型**\n第二个模型是“调整后”的模型，旨在提供更好的因果估计。问题指出调整集是 $\\{\\mathrm{SST}, \\mathrm{Z500}, u\\}$。回归模型是 $y \\sim \\mathrm{SST} + \\mathrm{Z500} + u$。\n特征矩阵 $X_{\\text{adjusted}}$ 的维度将为 $(N, 3)$，其列分别对应 $\\mathrm{SST}$、$\\mathrm{Z500}$ 和（实际上未观测到的）混淆变量 $u$。通过在回归中包含 $u$，我们明确地控制了共同原因，从而阻断了 $\\mathrm{SST}$ 和 $y$ 之间的后门路径。这就是后门调整的原理。该模型中 $\\mathrm{SST}$ 的系数 $\\beta_{\\text{adj,SST}}$ 应该是 $\\mathrm{SST}$ 对 $y$ 的真实直接因果效应的无偏估计。根据 SCM，这个效应是 $0$。因此，我们预计 $\\beta_{\\text{adj,SST}}$ 会接近 $0$。$\\mathrm{Z500}$ 的系数 $\\beta_{\\text{adj,Z500}}$ 应该是 $\\mathrm{Z500}$ 对 $y$ 的真实直接因果效应的无偏估计，即参数 $d$。\n\n**第 5 步：报告结果**\n对于每个测试用例，我们计算并报告五个值：\n1.  `naive_sst`：来自朴素模型的系数 $\\beta_{\\text{naive,SST}}$。\n2.  `adjusted_sst`：来自调整后模型的系数 $\\beta_{\\text{adj,SST}}$。\n3.  `adjusted_z500`：来自调整后模型的系数 $\\beta_{\\text{adj,Z500}}$。\n4.  `bias_reduced`：一个布尔值，如果 $|\\beta_{\\text{adj,SST}}| \\le |\\beta_{\\text{naive,SST}}|$ 则为 `True`。这测试了对混淆变量进行调整是否减小了对 $\\mathrm{SST}$ 的（伪）归因的幅度。我们预计在存在混淆的情况下，这个值为 `True`。\n5.  `z500_close`：一个布尔值，如果 $|\\beta_{\\text{adj,Z500}} - d| \\le \\text{tolerance}$ 则为 `True`。这测试了调整后的模型是否在给定的数值容差内正确地恢复了真实的因果参数 $d$。该估计的准确性将取决于样本大小 $N$ 和噪声水平。\n\n该程序将应用于所有三个测试用例，展示因果调整如何帮助纠正由混淆导致的带偏见的归因，这对于在像气候科学这样的复杂系统中构建可信的 XAI 方法至关重要。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating data from the SCM, training naive and\n    adjusted ridge regression models, and reporting the resulting coefficients\n    and comparisons.\n    \"\"\"\n    test_cases = [\n        # Case A (strong confounding, well-sampled)\n        {'N': 10000, 'a': 1.5, 'b': 1.0, 'd': 2.0, 'sigma_s': 0.3, 'sigma_z': 0.3,\n         'sigma_y': 0.5, 'lambda_val': 1.0, 'tolerance': 0.15, 'seed': 42},\n        # Case B (no confounding, well-sampled)\n        {'N': 10000, 'a': 0.0, 'b': 1.0, 'd': 2.0, 'sigma_s': 0.3, 'sigma_z': 0.3,\n         'sigma_y': 0.5, 'lambda_val': 1.0, 'tolerance': 0.15, 'seed': 43},\n        # Case C (strong confounding, small sample and noisy)\n        {'N': 100, 'a': 1.5, 'b': 1.0, 'd': 2.0, 'sigma_s': 1.0, 'sigma_z': 1.0,\n         'sigma_y': 1.0, 'lambda_val': 2.0, 'tolerance': 0.5, 'seed': 44},\n    ]\n\n    results = []\n\n    def run_ridge_regression(X: np.ndarray, y: np.ndarray, lambda_val: float) -> np.ndarray:\n        \"\"\"\n        Computes ridge regression coefficients using the closed-form solution.\n        Assumes X and y are not yet mean-centered.\n        \"\"\"\n        # Mean-center the data\n        X_c = X - X.mean(axis=0)\n        y_c = y - y.mean()\n\n        n_samples, n_features = X_c.shape\n        \n        # Identity matrix for regularization term\n        I = np.identity(n_features)\n        \n        # Closed-form solution: beta = (X'X + lambda*I)^-1 * X'y\n        try:\n            A = X_c.T @ X_c + lambda_val * I\n            B = X_c.T @ y_c\n            beta = np.linalg.inv(A) @ B\n        except np.linalg.LinAlgError:\n            # In case of singularity, although unlikely with lambda > 0\n            beta = np.zeros(n_features)\n            \n        return beta\n\n    for case in test_cases:\n        N = case['N']\n        a = case['a']\n        b = case['b']\n        d = case['d']\n        sigma_s = case['sigma_s']\n        sigma_z = case['sigma_z']\n        sigma_y = case['sigma_y']\n        lambda_val = case['lambda_val']\n        tolerance = case['tolerance']\n        seed = case['seed']\n        \n        # 1. Generate synthetic data from the SCM\n        rng = np.random.default_rng(seed)\n        \n        u = rng.normal(loc=0.0, scale=1.0, size=N)\n        eps_s = rng.normal(loc=0.0, scale=sigma_s, size=N)\n        eps_z = rng.normal(loc=0.0, scale=sigma_z, size=N)\n        eps_y = rng.normal(loc=0.0, scale=sigma_y, size=N)\n        \n        sst = a * u + eps_s\n        z500 = b * u + eps_z\n        y = d * z500 + eps_y\n\n        # 2. Compute naive attribution (y ~ SST)\n        X_naive = sst.reshape(-1, 1)\n        beta_naive = run_ridge_regression(X_naive, y, lambda_val)\n        naive_sst = beta_naive[0]\n\n        # 3. Compute adjusted attribution (y ~ SST + Z500 + u)\n        X_adjusted = np.vstack([sst, z500, u]).T\n        beta_adjusted = run_ridge_regression(X_adjusted, y, lambda_val)\n        adjusted_sst = beta_adjusted[0]\n        adjusted_z500 = beta_adjusted[1]\n        \n        # 4. Perform comparisons\n        bias_reduced = abs(adjusted_sst) = abs(naive_sst)\n        z500_close = abs(adjusted_z500 - d) = tolerance\n        \n        # 5. Store results for the case\n        case_results = [\n            naive_sst, \n            adjusted_sst, \n            adjusted_z500,\n            bool(bias_reduced),  # Ensure Python bool not numpy.bool_\n            bool(z500_close)\n        ]\n        results.append(case_results)\n\n    # Format the final output string\n    # E.g., [[x_A,y_A,z_A,b_A,c_A],[x_B,y_B,z_B,b_B,c_B],[x_C,y_C,z_C,b_C,c_C]]\n    # Using a list comprehension to build the string representation of each inner list\n    result_str = '[' + ','.join(['[' + ','.join(map(str, res)) + ']' for res in results]) + ']'\n    # The output format has a space after comma in the prompt example, but not specified in the rules\n    # and the example for problem 2 doesn't have it. Sticking to no space.\n    # Also, Boolean values in Python are 'True'/'False'. The prompt requires 'True' or 'False'.\n    # map(str, res) will do this correctly.\n    final_str = str(results).replace(\"'\", \"\").replace(\" \", \"\")\n    print(final_str)\n\nsolve()\n```"
        }
    ]
}