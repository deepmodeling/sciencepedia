{
    "hands_on_practices": [
        {
            "introduction": "Traditional grid-point verification scores often condense complex forecast errors into a single, non-diagnostic number. The Structure-Amplitude-Location (SAL) method provides a more insightful alternative by separating forecast error into three distinct components. This practice challenges you to compute the SAL triplet for idealized Gaussian precipitation fields, helping you build a fundamental intuition for how each component quantifies specific aspects of forecast performance .",
            "id": "4090731",
            "problem": "Consider continuous, two-dimensional precipitation intensity fields defined on a circular domain $\\Omega = \\{\\mathbf{x} \\in \\mathbb{R}^{2}:\\ \\|\\mathbf{x}\\| \\leq R\\}$ of radius $R$, with the forecast field given by $F(\\mathbf{x}) = H_{f} \\exp\\!\\big(-\\|\\mathbf{x} - \\mathbf{x}_{f}\\|^{2}/(2\\sigma_{f}^{2})\\big)$ and the observed field given by $O(\\mathbf{x}) = H_{o} \\exp\\!\\big(-\\|\\mathbf{x} - \\mathbf{x}_{o}\\|^{2}/(2\\sigma_{o}^{2})\\big)$. The centers are colocated at the origin, that is, $\\mathbf{x}_{f} = \\mathbf{x}_{o} = \\mathbf{0}$. Assume isotropy and that the domain radius $R$ is sufficiently large relative to $\\sigma_{f}$ and $\\sigma_{o}$ so that truncation of Gaussian tails at $r=R$ is negligible to four significant figures.\n\nUsing the Structure–Amplitude–Location (SAL) verification triplet, compute the components $S$, $A$, and $L$ for the parameter choices\n- $H_{o} = 20$ in $\\mathrm{mm\\,h^{-1}}$, $\\sigma_{o} = 15$ in $\\mathrm{km}$,\n- $H_{f} = 12$ in $\\mathrm{mm\\,h^{-1}}$, $\\sigma_{f} = 30$ in $\\mathrm{km}$,\n- $R = 200$ in $\\mathrm{km}$.\n\nUse the following definitions:\n- Amplitude component: the domain means $\\overline{F} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}$ and $\\overline{O} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} O(\\mathbf{x})\\, d\\mathbf{x}$, and\n$$A = \\frac{2\\big(\\overline{F} - \\overline{O}\\big)}{\\overline{F} + \\overline{O}}.$$\n- Location component: $L = L_{1} + L_{2}$ with $L_{1} = \\frac{\\|\\mathbf{x}_{f} - \\mathbf{x}_{o}\\|}{2R}$ and the dispersion-based term\n$$r(F) = \\frac{\\int_{\\Omega} \\|\\mathbf{x} - \\mathbf{x}_{f}\\|\\, F(\\mathbf{x})\\, d\\mathbf{x}}{\\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}}, \\quad r(O) = \\frac{\\int_{\\Omega} \\|\\mathbf{x} - \\mathbf{x}_{o}\\|\\, O(\\mathbf{x})\\, d\\mathbf{x}}{\\int_{\\Omega} O(\\mathbf{x})\\, d\\mathbf{x}}, \\quad L_{2} = \\frac{|r(F) - r(O)|}{2R}.$$\n- Structure component: define the high-intensity core of each field at half-maximum,\n$$C(F) = \\left\\{\\mathbf{x} \\in \\Omega:\\ F(\\mathbf{x}) \\geq \\frac{H_{f}}{2}\\right\\}, \\quad C(O) = \\left\\{\\mathbf{x} \\in \\Omega:\\ O(\\mathbf{x}) \\geq \\frac{H_{o}}{2}\\right\\},$$\nand the normalized core-area fractions\n$$s(F) = \\frac{\\mathrm{area}\\big(C(F)\\big)}{\\pi R^{2}}, \\quad s(O) = \\frac{\\mathrm{area}\\big(C(O)\\big)}{\\pi R^{2}}, \\quad S = \\frac{2\\big(s(F) - s(O)\\big)}{s(F) + s(O)}.$$\n\nCompute $S$, $A$, and $L$ under the large-domain approximation stated above, and provide the final SAL triplet as dimensionless numbers. Round each component to four significant figures. Express the final answer as a single row vector using the LaTeX $\\mathrm{pmatrix}$ environment.",
            "solution": "The user has provided a problem that passes all validation criteria. It is scientifically grounded in the field of meteorological forecast verification, mathematically well-posed, objective, and contains all necessary information for a unique solution. I will therefore proceed with a full derivation of the Structure-Amplitude-Location (SAL) components.\n\nThe problem requires the computation of the SAL triplet $(S, A, L)$ for two-dimensional Gaussian precipitation fields. The forecast field is $F(\\mathbf{x}) = H_{f} \\exp(-\\|\\mathbf{x} - \\mathbf{x}_{f}\\|^{2}/(2\\sigma_{f}^{2}))$ and the observed field is $O(\\mathbf{x}) = H_{o} \\exp(-\\|\\mathbf{x} - \\mathbf{x}_{o}\\|^{2}/(2\\sigma_{o}^{2}))$. The centers are colocated at the origin, so $\\mathbf{x}_{f} = \\mathbf{x}_{o} = \\mathbf{0}$. The fields are defined on a circular domain $\\Omega$ of radius $R$. The large-domain approximation states that the contribution from the fields outside $\\Omega$ is negligible, allowing us to approximate the integrals over $\\Omega$ by integrals over the entire plane $\\mathbb{R}^{2}$.\n\nThe given parameters are:\n$H_{o} = 20$, $\\sigma_{o} = 15$\n$H_{f} = 12$, $\\sigma_{f} = 30$\n$R = 200$\nAll units are consistent (mm/h for intensity, km for length).\n\nWe will compute each component of the SAL triplet in turn.\n\n### 1. Amplitude Component ($A$)\n\nThe amplitude component $A$ is defined as\n$$A = \\frac{2\\big(\\overline{F} - \\overline{O}\\big)}{\\overline{F} + \\overline{O}}$$\nwhere $\\overline{F}$ and $\\overline{O}$ are the domain-averaged precipitation intensities.\n$\\overline{F} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}$ and $\\overline{O} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} O(\\mathbf{x})\\, d\\mathbf{x}$.\n\nLet's first compute the total precipitation volume, which is the integral of the intensity field over the domain. For the forecast field, with $\\mathbf{x}_{f} = \\mathbf{0}$:\n$$V_{f} = \\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x} = \\int_{\\Omega} H_{f} \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right)\\, d\\mathbf{x}$$\nUsing the large-domain approximation, we can extend the integral over the entire plane $\\mathbb{R}^{2}$. We switch to polar coordinates $(r, \\theta)$, where $\\|\\mathbf{x}\\| = r$ and $d\\mathbf{x} = r\\,dr\\,d\\theta$.\n$$V_{f} \\approx \\int_{0}^{2\\pi} \\int_{0}^{\\infty} H_{f} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) r\\,dr\\,d\\theta$$\nThe integral separates into an angular and a radial part:\n$$V_{f} = 2\\pi H_{f} \\int_{0}^{\\infty} r \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) dr$$\nLet $u = r^{2}/(2\\sigma_{f}^{2})$, so $du = r/\\sigma_{f}^{2} \\,dr$, which gives $r\\,dr = \\sigma_{f}^{2}\\,du$.\n$$V_{f} = 2\\pi H_{f} \\int_{0}^{\\infty} \\exp(-u) \\sigma_{f}^{2}\\,du = 2\\pi H_{f} \\sigma_{f}^{2} [-\\exp(-u)]_{0}^{\\infty} = 2\\pi H_{f} \\sigma_{f}^{2}$$\nBy exact analogy, the total observed volume is $V_{o} = 2\\pi H_{o} \\sigma_{o}^{2}$.\n\nNow we can compute the domain-averaged intensities:\n$$\\overline{F} = \\frac{V_{f}}{\\pi R^{2}} = \\frac{2\\pi H_{f} \\sigma_{f}^{2}}{\\pi R^{2}} = \\frac{2 H_{f} \\sigma_{f}^{2}}{R^{2}}$$\n$$\\overline{O} = \\frac{V_{o}}{\\pi R^{2}} = \\frac{2\\pi H_{o} \\sigma_{o}^{2}}{\\pi R^{2}} = \\frac{2 H_{o} \\sigma_{o}^{2}}{R^{2}}$$\nSubstituting the given values:\n$\\overline{F} = \\frac{2(12)(30)^{2}}{200^{2}} = \\frac{24 \\cdot 900}{40000} = \\frac{21600}{40000} = 0.54$.\n$\\overline{O} = \\frac{2(20)(15)^{2}}{200^{2}} = \\frac{40 \\cdot 225}{40000} = \\frac{9000}{40000} = 0.225$.\n\nFinally, we compute $A$:\n$$A = \\frac{2(0.54 - 0.225)}{0.54 + 0.225} = \\frac{2(0.315)}{0.765} = \\frac{0.63}{0.765} \\approx 0.823529...$$\nRounding to four significant figures, $A = 0.8235$.\n\n### 2. Location Component ($L$)\n\nThe location component is $L = L_{1} + L_{2}$.\nThe term $L_{1}$ is related to the displacement of the centers of mass:\n$$L_{1} = \\frac{\\|\\mathbf{x}_{f} - \\mathbf{x}_{o}\\|}{2R}$$\nSince the centers are given as colocated, $\\mathbf{x}_{f} = \\mathbf{x}_{o} = \\mathbf{0}$, we have $L_{1} = 0$.\n\nThe term $L_{2}$ is related to the difference in the average distance of precipitation from the respective centers:\n$$L_{2} = \\frac{|r(F) - r(O)|}{2R}$$\nwhere $r(F) = \\frac{\\int_{\\Omega} \\|\\mathbf{x} - \\mathbf{x}_{f}\\|\\, F(\\mathbf{x})\\, d\\mathbf{x}}{\\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}}$.\nWith $\\mathbf{x}_{f} = \\mathbf{0}$, the denominator is $V_{f} = 2\\pi H_{f} \\sigma_{f}^{2}$. The numerator is:\n$$N_{f} = \\int_{\\Omega} \\|\\mathbf{x}\\| F(\\mathbf{x})\\, d\\mathbf{x} = \\int_{\\Omega} \\|\\mathbf{x}\\| H_{f} \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right)\\, d\\mathbf{x}$$\nAgain, using polar coordinates and the large-domain approximation:\n$$N_{f} \\approx \\int_{0}^{2\\pi} \\int_{0}^{\\infty} r H_{f} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) r\\,dr\\,d\\theta = 2\\pi H_{f} \\int_{0}^{\\infty} r^{2} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) dr$$\nThis is a standard Gaussian integral of the form $\\int_{0}^{\\infty} x^{2} \\exp(-ax^{2})dx = \\frac{\\sqrt{\\pi}}{4a^{3/2}}$. Here, $a=1/(2\\sigma_{f}^{2})$.\n$$\\int_{0}^{\\infty} r^{2} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) dr = \\frac{\\sqrt{\\pi}}{4(1/(2\\sigma_{f}^{2}))^{3/2}} = \\frac{\\sqrt{\\pi}}{4/(2\\sigma_{f}^{2}\\sqrt{2\\sigma_{f}^{2}})} = \\frac{\\sqrt{\\pi} \\cdot 2\\sqrt{2} \\sigma_{f}^{3}}{4} = \\frac{\\sqrt{2\\pi}}{2}\\sigma_{f}^{3}$$\nSo, the numerator is $N_{f} = 2\\pi H_{f} \\left(\\frac{\\sqrt{2\\pi}}{2}\\sigma_{f}^{3}\\right) = \\pi\\sqrt{2\\pi}H_{f}\\sigma_{f}^{3}$.\nThen, $r(F)$ is the ratio of the numerator to the denominator:\n$$r(F) = \\frac{N_{f}}{V_{f}} = \\frac{\\pi\\sqrt{2\\pi}H_{f}\\sigma_{f}^{3}}{2\\pi H_{f}\\sigma_{f}^{2}} = \\frac{\\sqrt{2\\pi}}{2}\\sigma_{f}$$\nSimilarly, for the observed field, $r(O) = \\frac{\\sqrt{2\\pi}}{2}\\sigma_{o}$.\n\nNow we substitute the values:\n$r(F) = \\frac{\\sqrt{2\\pi}}{2}(30) = 15\\sqrt{2\\pi}$ km.\n$r(O) = \\frac{\\sqrt{2\\pi}}{2}(15) = 7.5\\sqrt{2\\pi}$ km.\n\n$L_{2} = \\frac{|15\\sqrt{2\\pi} - 7.5\\sqrt{2\\pi}|}{2(200)} = \\frac{7.5\\sqrt{2\\pi}}{400}$.\n$L_{2} \\approx \\frac{7.5(2.506628)}{400} \\approx \\frac{18.79971}{400} \\approx 0.046999...$\nThus, $L = L_{1} + L_{2} = 0 + L_{2} \\approx 0.046999...$.\nRounding to four significant figures, $L = 0.04700$.\n\n### 3. Structure Component ($S$)\n\nThe structure component $S$ is defined as\n$$S = \\frac{2\\big(s(F) - s(O)\\big)}{s(F) + s(O)}$$\nwhere $s(F)$ and $s(O)$ are normalized core-area fractions. The core $C(F)$ is defined as the region where the intensity is at least half its maximum value:\n$C(F) = \\{\\mathbf{x} \\in \\Omega:\\ F(\\mathbf{x}) \\geq H_{f}/2\\}$.\n$$H_{f} \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right) \\geq \\frac{H_{f}}{2} \\implies \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right) \\geq \\frac{1}{2}$$\nTaking the natural logarithm of both sides:\n$$\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}} \\geq \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2)$$\n$$\\|\\mathbf{x}\\|^{2} \\leq 2\\sigma_{f}^{2}\\ln(2) \\implies \\|\\mathbf{x}\\| \\leq \\sigma_{f}\\sqrt{2\\ln(2)}$$\nThis describes a circular disk centered at the origin. The radius of this core is $R_{f} = \\sigma_{f}\\sqrt{2\\ln(2)}$. The area of the core is $\\mathrm{area}(C(F)) = \\pi R_{f}^{2} = 2\\pi\\sigma_{f}^{2}\\ln(2)$.\nThe normalized core-area fraction is:\n$$s(F) = \\frac{\\mathrm{area}(C(F))}{\\pi R^{2}} = \\frac{2\\pi\\sigma_{f}^{2}\\ln(2)}{\\pi R^{2}} = \\frac{2\\sigma_{f}^{2}\\ln(2)}{R^{2}}$$\nSimilarly, $s(O) = \\frac{2\\sigma_{o}^{2}\\ln(2)}{R^{2}}$.\n\nNow we compute $S$:\n$$S = \\frac{2\\left(\\frac{2\\sigma_{f}^{2}\\ln(2)}{R^{2}} - \\frac{2\\sigma_{o}^{2}\\ln(2)}{R^{2}}\\right)}{\\frac{2\\sigma_{f}^{2}\\ln(2)}{R^{2}} + \\frac{2\\sigma_{o}^{2}\\ln(2)}{R^{2}}}$$\nThe common term $2\\ln(2)/R^{2}$ cancels from the numerator and denominator:\n$$S = \\frac{2(\\sigma_{f}^{2} - \\sigma_{o}^{2})}{\\sigma_{f}^{2} + \\sigma_{o}^{2}}$$\nSubstituting the given values for $\\sigma_{f}$ and $\\sigma_{o}$:\n$\\sigma_{f}^{2} = 30^{2} = 900$.\n$\\sigma_{o}^{2} = 15^{2} = 225$.\n$$S = \\frac{2(900 - 225)}{900 + 225} = \\frac{2(675)}{1125} = \\frac{1350}{1125} = 1.2$$\nTo four significant figures, $S = 1.200$.\n\n### Summary\n\nThe computed components of the SAL triplet are:\n- Structure $S = 1.200$\n- Amplitude $A = 0.8235$\n- Location $L = 0.04700$\n\nThe final SAL triplet, ordered as $(S, A, L)$, is $(1.200, 0.8235, 0.04700)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.200 & 0.8235 & 0.04700 \\end{pmatrix}}$$"
        },
        {
            "introduction": "A core challenge in object-based verification is to unambiguously pair forecast objects with their observed counterparts before any comparison can be made. This task introduces the formal assignment problem and its solution via the Hungarian algorithm, a cornerstone of combinatorial optimization. By applying this method to a given cost matrix, you will master the technique for creating an optimal, minimal-cost matching between two sets of objects, a critical step in any sophisticated object-based verification scheme .",
            "id": "4090744",
            "problem": "You are given two finite sets of spatial objects that arise in Numerical Weather Prediction (NWP) and climate modeling: a set of forecast objects and a set of observed objects. In object-based verification, we seek to match forecast objects to observed objects in a way that minimizes a total dissimilarity cost. Suppose that for each forecast object indexed by $i \\in \\{0,1,\\dots,n-1\\}$ and observed object indexed by $j \\in \\{0,1,\\dots,m-1\\}$ we have a nonnegative, dimensionless cost $c_{ij} \\ge 0$ that measures the dissimilarity between forecast object $i$ and observed object $j$. These costs are assumed to be constructed from scientifically meaningful, normalized features (for example, normalized centroid separation and normalized area difference), so that each $c_{ij}$ is dimensionless; therefore, the final outputs in this problem have no physical units.\n\nThe assignment problem is to select a set of index pairs $(i,j)$ such that each forecast object is matched to at most one observed object and each observed object is matched to at most one forecast object, with the objective of minimizing the total cost $\\sum_{(i,j)} c_{ij}$. When $n \\ne m$, we permit unmatched objects by introducing dummy assignments with a fixed, nonnegative, dimensionless penalty $c_u$ per unmatched object. To reduce the problem to a square-cost matrix suitable for the Hungarian algorithm, if $n < m$ you add $(m-n)$ dummy forecast rows in which every entry equals $c_u$, and if $n > m$ you add $(n-m)$ dummy observed columns in which every entry equals $c_u$. This produces a square matrix in which a perfect assignment (one pair per row and per column) minimizes the sum of both true matches and dummy penalties. A forecast object assigned to a dummy observed column is treated as unmatched, and an observed object assigned to a dummy forecast row is treated as unmatched.\n\nTask: Construct the minimal-cost assignment using the Hungarian algorithm for each provided test case and output, for each test case, the mapping from forecast indices to observed indices as a list, where unmatched forecasts are indicated by -1, together with the minimal total cost. Prove that the Hungarian algorithm returns a solution that minimizes the total cost by appealing to a principle-based derivation from linear programming duality and complementary slackness.\n\nTest Suite:\n- Case $1$ (balanced, dimensionless costs, general case):\n  $$\n  n = 3,\\quad m = 3,\\quad C^{(1)} =\n  \\begin{bmatrix}\n  4 & 1 & 3 \\\\\n  2 & 0 & 5 \\\\\n  3 & 2 & 2\n  \\end{bmatrix}.\n  $$\n  No dummy penalty is needed since $n = m$.\n\n- Case $2$ (unbalanced, one more observed object than forecast objects, with dummy penalty):\n  $$\n  n = 2,\\quad m = 3,\\quad C^{(2)} =\n  \\begin{bmatrix}\n  6 & 2 & 5 \\\\\n  4 & 3 & 7\n  \\end{bmatrix},\\quad c_u = 4.\n  $$\n  The padded square matrix is of size $m = 3$ with one dummy forecast row of all entries equal to $c_u$.\n\n- Case $3$ (balanced, tie case with multiple optimal assignments):\n  $$\n  n = 3,\\quad m = 3,\\quad C^{(3)} =\n  \\begin{bmatrix}\n  1 & 1 & 2 \\\\\n  1 & 1 & 2 \\\\\n  2 & 2 & 2\n  \\end{bmatrix}.\n  $$\n  No dummy penalty is needed since $n = m$.\n\nAnswer specification and final output format:\n- For each test case $t \\in \\{1,2,3\\}$, compute:\n  $$\n  \\text{mapping}^{(t)} = \\bigl[\\,\\text{obs}(0),\\text{obs}(1),\\dots,\\text{obs}(n-1)\\,\\bigr],\n  $$\n  where $\\text{obs}(i)$ is the observed index $j \\in \\{0,\\dots,m-1\\}$ matched to forecast index $i$, or -1 if forecast $i$ is unmatched.\n  Also compute the minimal total cost $J^{(t)}$ as an integer, which is the sum of the selected entries, including $c_u$ penalties when applicable.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be structured as $[\\text{mapping}^{(t)}, J^{(t)}]$ with zero-based indices and dimensionless costs. For example, the entire output should look like:\n  $$\n  [\\,[\\,[\\text{obs}(0),\\dots,\\text{obs}(n-1)],J^{(1)}],\\,[\\,[\\text{obs}(0),\\dots,\\text{obs}(n-1)],J^{(2)}],\\,[\\,[\\text{obs}(0),\\dots,\\text{obs}(n-1)],J^{(3)}]\\,].\n  $$\n\nThe solution must adhere to the specified runtime environment and use only permitted libraries. No external inputs are allowed; all data needed are in the test suite above. All angles and physical units are not applicable here, since all quantities are dimensionless. The final output must be exactly one line in the specified format.",
            "solution": "The problem of finding a minimal-cost assignment between a set of forecast objects and a set of observed objects is a classic combinatorial optimization problem known as the assignment problem. Before proceeding to the solution for the specific test cases, we first provide a theoretical justification for the use of the Hungarian algorithm, grounding its correctness in the principles of linear programming duality.\n\nThe problem is stated as finding a one-to-one matching between two sets of objects, forecasts indexed by $i \\in \\{0, 1, \\dots, n-1\\}$ and observations indexed by $j \\in \\{0, 1, \\dots, m-1\\}$, that minimizes the total dissimilarity cost. The dissimilarity is given by a cost matrix $C$ with non-negative entries $c_{ij}$. To handle cases where $n \\ne m$, the problem is converted into a balanced assignment problem on a square matrix. Let $K = \\max(n, m)$. We construct a $K \\times K$ cost matrix $C'$.\n- If $n < m$, so $K=m$, $C'$ is formed by augmenting the $n \\times m$ matrix $C$ with $m-n$ rows, where each entry in these new rows has the value of the unmatched penalty $c_u$.\n- If $n > m$, so $K=n$, $C'$ is formed by augmenting the $n \\times m$ matrix $C$ with $n-m$ columns, where each entry in these new columns has the value of $c_u$.\n- If $n = m$, $C' = C$.\n\nThe task is now to find a perfect matching in the complete bipartite graph represented by the square cost matrix $C'$ that minimizes the total cost. This is equivalent to selecting a set of entries in $C'$ such that exactly one entry is chosen from each row and each column, and the sum of these entries is minimized.\n\nThis assignment problem can be formulated as a linear program (LP). Let $x_{ij}$ be a binary variable such that $x_{ij} = 1$ if row $i$ is assigned to column $j$, and $x_{ij} = 0$ otherwise. The primal LP formulation is:\n$$\n\\text{Minimize} \\quad Z_P = \\sum_{i=0}^{K-1} \\sum_{j=0}^{K-1} c'_{ij} x_{ij}\n$$\nsubject to the constraints:\n$$\n\\sum_{j=0}^{K-1} x_{ij} = 1 \\quad \\text{for } i = 0, \\dots, K-1 \\quad \\text{(each row is assigned once)}\n$$\n$$\n\\sum_{i=0}^{K-1} x_{ij} = 1 \\quad \\text{for } j = 0, \\dots, K-1 \\quad \\text{(each column is assigned once)}\n$$\n$$\nx_{ij} \\ge 0 \\quad \\text{for all } i, j\n$$\nFor the assignment problem, the constraint matrix is totally unimodular, which guarantees that any basic feasible solution of this LP will have integer components. Since the variables are also bounded by $1$, any optimal solution will naturally satisfy $x_{ij} \\in \\{0, 1\\}$, so we do not need to state this as an explicit constraint.\n\nAccording to the theory of linear programming, every primal LP has a corresponding dual LP. Let $u_i$ and $v_j$ be the dual variables associated with the row and column constraints, respectively. The dual problem is:\n$$\n\\text{Maximize} \\quad Z_D = \\sum_{i=0}^{K-1} u_i + \\sum_{j=0}^{K-1} v_j\n$$\nsubject to the constraints:\n$$\nu_i + v_j \\le c'_{ij} \\quad \\text{for all } i, j = 0, \\dots, K-1\n$$\nThe weak duality theorem states that for any feasible primal solution $x$ and any feasible dual solution $(u, v)$, $Z_P \\ge Z_D$. The strong duality theorem states that if an optimal solution exists for either problem, then an optimal for the other also exists, and their objective values are equal, $Z_P^* = Z_D^*$.\n\nThe optimality of a pair of primal and dual feasible solutions is established by the complementary slackness conditions. For optimal solutions $x^*$ and $(u^*, v^*)$, these conditions are:\n$$\nx_{ij}^* > 0 \\implies u_i^* + v_j^* = c'_{ij}\n$$\nThis means that an assignment from row $i$ to column $j$ can be part of an optimal solution only if the corresponding dual constraint is met with equality.\n\nThe Hungarian algorithm is a primal-dual algorithm that systematically constructs a pair of primal and dual solutions that satisfy these optimality conditions. The algorithm operates on a reduced cost matrix, defined by $\\bar{c}_{ij} = c'_{ij} - u_i - v_j$. The dual feasibility condition $u_i + v_j \\le c'_{ij}$ is equivalent to requiring that all reduced costs are non-negative, $\\bar{c}_{ij} \\ge 0$. The complementary slackness condition implies that any matched pair $(i, j)$ in the optimal assignment must have a reduced cost of zero, i.e., $\\bar{c}_{ij} = 0$.\n\nThe algorithm proceeds as follows:\n1. It initializes dual variables $u_i, v_j$ to establish a non-negative reduced cost matrix with at least one zero in every row and column. This constitutes an initial dual feasible solution.\n2. It attempts to find a perfect matching (a primal feasible solution) using only the edges $(i,j)$ for which the reduced cost $\\bar{c}_{ij} = 0$. This set of edges forms the \"equality subgraph\".\n3. If a perfect matching of size $K$ is found, the algorithm terminates. This matching and the current dual variables form an optimal pair, as they satisfy primal feasibility, dual feasibility, and complementary slackness. The total cost is $\\sum_{(i,j) \\in \\text{matching}} c'_{ij} = \\sum_{(i,j) \\in \\text{matching}} (u_i + v_j) = \\sum_i u_i + \\sum_j v_j$, demonstrating that the primal and dual objectives are equal.\n4. If a perfect matching cannot be found, the algorithm identifies a maximum matching and a minimum vertex cover in the equality subgraph. Based on the vertex cover, it systematically updates the dual variables $u_i$ and $v_j$ in a way that preserves dual feasibility (all $\\bar{c}_{ij} \\ge 0$) and introduces new zero-cost edges, expanding the equality subgraph. It then repeats the search for a perfect matching.\n\nThis iterative process is guaranteed to terminate in a finite number of steps, yielding an assignment that satisfies the conditions for optimality derived from LP duality. Thus, the Hungarian algorithm correctly finds a minimal-cost assignment.\n\nApplying this validated method to the test cases:\n\n- **Case 1:** $n = 3$, $m = 3$. The cost matrix is $C^{(1)} = \\begin{bmatrix} 4 & 1 & 3 \\\\ 2 & 0 & 5 \\\\ 3 & 2 & 2 \\end{bmatrix}$. The problem is balanced, so no padding is required. The algorithm finds the assignment $(0,1)$, $(1,0)$, and $(2,2)$. The mapping from forecast to observed indices is $[\\,1, 0, 2\\,]$. The minimal total cost is $J^{(1)} = c_{01} + c_{10} + c_{22} = 1 + 2 + 2 = 5$.\n\n- **Case 2:** $n = 2$, $m = 3$, with unmatched penalty $c_u = 4$. This is an unbalanced problem. We augment the $2 \\times 3$ cost matrix $C^{(2)}$ with one dummy row, filled with the penalty $c_u=4$, to form a $3 \\times 3$ matrix:\n$$\nC'^{(2)} = \\begin{bmatrix} 6 & 2 & 5 \\\\ 4 & 3 & 7 \\\\ 4 & 4 & 4 \\end{bmatrix}\n$$\nThe algorithm finds an assignment on this padded matrix. An optimal assignment is $(0,1)$, $(1,0)$, and $(2,2)$. Forecast $0$ is matched to observation $1$, and forecast $1$ is matched to observation $0$. The dummy forecast (row $2$) is matched to observation $2$, which means observation $2$ is unmatched. Both real forecasts are matched. The mapping is $[\\,1, 0\\,]$. The minimal cost is $J^{(2)} = c_{01} + c_{10} + c'_{22} = 2 + 4 + 4 = 10$.\n\n- **Case 3:** $n = 3$, $m = 3$. The cost matrix is $C^{(3)} = \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 2 & 2 & 2 \\end{bmatrix}$. The problem is balanced. Due to identical rows and columns, multiple optimal solutions exist. For example, the assignment $(0,0)$, $(1,1)$, $(2,2)$ gives a cost of $J = 1 + 1 + 2 = 4$. An alternative optimal assignment is $(0,1)$, $(1,0)$, $(2,2)$, also with cost $J = 1 + 1 + 2 = 4$. The algorithm will find one such valid assignment. A possible output mapping is $[\\,1, 0, 2\\,]$ with a total cost of $J^{(3)} = 4$.",
            "answer": "[[[1,0,2],5],[[1,0],10],[[1,0,2],4]]"
        },
        {
            "introduction": "This exercise synthesizes the entire object-based verification workflow, guiding you from raw intensity fields to a final evaluation. You will identify objects using connectivity analysis, characterize them by calculating key attributes like area and eccentricity, and then perform an optimal matching based on a weighted cost function . This capstone practice provides a comprehensive, hands-on experience in implementing a complete object-based verification system from first principles.",
            "id": "4090773",
            "problem": "You are given a synthetic setup for evaluating spatial and object-based verification in Numerical Weather Prediction (NWP) and climate modeling. Specifically, you will compute attributes for binary-thresholded connected objects in two-dimensional gridded fields and then perform optimal matching between forecast and observed objects using a rigorously defined cost function and the Linear Sum Assignment (LSA) criterion.\n\nFundamental base:\n- Object-based verification (OBV) operates on thresholded binary fields defined from gridded intensities and partitions the field into connected components under a specified neighborhood rule. Attributes such as area, shape descriptors, and intensity statistics are computed per object.\n- For a two-dimensional (2D) gridded field, object boundaries arise from a threshold $T$ applied to intensities $I_{r,c}$ on a discrete grid indexed by row $r$ and column $c$.\n- Connectivity is defined via $8$-connectivity: two grid cells are neighbors if their row indices differ by at most $1$ and their column indices differ by at most $1$, including diagonal adjacency.\n\nDefinitions to be used:\n- Thresholding: For each grid cell $(r,c)$ with intensity $I_{r,c}$, define a binary mask $M_{r,c} = 1$ if $I_{r,c} \\ge T$ and $M_{r,c} = 0$ otherwise.\n- Connected object: A maximal set of cells with $M_{r,c} = 1$ that are connected under $8$-connectivity.\n- Area: The area of an object is the count of its grid cells. Express area as an integer count of grid cells.\n- Maximum intensity: The maximum of $I_{r,c}$ over the cells in the object. Express this as a real number.\n- Eccentricity: For an object with $n$ cells at coordinates $(x_i,y_i)$ where $x_i$ is the column index and $y_i$ is the row index, compute the centroid $(\\bar{x},\\bar{y})$ with\n$$\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i, \\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i.\n$$\nDefine the second central moment (covariance) matrix\n$$\n\\mathbf{C} = \\begin{pmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 & \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) & \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\bar{y})^2\n\\end{pmatrix}.\n$$\nLet $\\lambda_{\\max}$ and $\\lambda_{\\min}$ be the eigenvalues of $\\mathbf{C}$ with $\\lambda_{\\max} \\ge \\lambda_{\\min} \\ge 0$. Define the object eccentricity\n$$\ne = \\sqrt{1 - \\frac{\\lambda_{\\min}}{\\lambda_{\\max}}}\n$$\nfor $\\lambda_{\\max} > 0$, and set $e = 0$ if $\\lambda_{\\max} = 0$. Express eccentricity as a real number in $[0,1]$.\n\nPairwise attribute-difference normalization:\n- Let forecast object attributes be $(A_f, e_f, M_f)$ and observed object attributes be $(A_o, e_o, M_o)$. Define normalized attribute differences\n$$\nd_A = \\frac{|A_f - A_o|}{\\max(A_f, A_o)}, \\quad d_E = |e_f - e_o|, \\quad d_M = \\frac{|M_f - M_o|}{\\max(M_f, M_o)}.\n$$\nAll differences are dimensionless and bounded in $[0,1]$.\n\nMatching cost function:\n- Given weights $(w_A, w_E, w_M)$, define the pairwise cost\n$$\nc = w_A d_A + w_E d_E + w_M d_M.\n$$\n- For a rectangular forecast-versus-observed pairing with $N_f$ forecast objects and $N_o$ observed objects, the optimal assignment minimizes the sum of selected pair costs over a one-to-one matching of size $\\min(N_f, N_o)$, plus a penalty $p_u$ for each unmatched forecast object:\n$$\nJ = \\sum_{(i,j)\\in \\mathcal{M}} c_{ij} + p_u \\cdot (N_f - |\\mathcal{M}|),\n$$\nwhere $\\mathcal{M}$ is the set of matched index pairs and $|\\mathcal{M}| = \\min(N_f, N_o)$.\n\nData specification:\n- Grid dimension is $8 \\times 8$; indices $r$ and $c$ run from $0$ to $7$.\n- Forecast field intensities $I^{(f)}_{r,c}$: all unspecified cells have intensity $0$. The nonzero intensities are at the following coordinates, with each pair listed as $((r,c), I)$:\n    - $((1,1), 2.0)$, $((1,2), 1.6)$, $((2,1), 1.4)$, $((2,2), 1.2)$,\n    - $((3,4), 1.1)$, $((3,5), 1.4)$, $((3,6), 1.6)$, $((4,4), 1.0)$, $((4,5), 1.2)$, $((4,6), 1.3)$,\n    - $((5,1), 0.95)$, $((5,2), 1.0)$, $((6,1), 1.2)$, $((6,2), 1.3)$, $((7,1), 2.2)$, $((7,2), 2.0)$.\n- Observed field intensities $I^{(o)}_{r,c}$: all unspecified cells have intensity $0$. The nonzero intensities are at the following coordinates:\n    - $((3,2), 1.0)$, $((3,3), 1.3)$, $((3,4), 1.5)$, $((4,2), 0.95)$, $((4,3), 1.1)$, $((4,4), 1.2)$,\n    - $((4,6), 1.1)$, $((4,7), 1.2)$, $((5,6), 1.4)$, $((5,7), 1.6)$, $((6,6), 1.7)$, $((6,7), 1.8)$.\n- Threshold is fixed at $T = 1.0$ for all test cases.\n\nConnectivity rule:\n- Use $8$-connectivity to identify connected components in the thresholded binary masks.\n\nTest suite:\nFor each case, compute forecast and observed objects, derive $(A,e,M)$ for each object, form the pairwise cost matrix using the given weights, solve the optimal assignment, and compute the total objective $J$ including unmatched penalty. Return $J$ rounded to $4$ decimal places as a float.\n- Case $1$: $(w_A, w_E, w_M) = (1.0, 1.0, 1.0)$, $p_u = 0.25$.\n- Case $2$: $(w_A, w_E, w_M) = (0.5, 2.0, 1.0)$, $p_u = 0.25$.\n- Case $3$: $(w_A, w_E, w_M) = (1.5, 0.0, 1.0)$, $p_u = 0.40$.\n- Case $4$: $(w_A, w_E, w_M) = (0.2, 1.0, 3.0)$, $p_u = 0.10$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for Cases $1$ through $4$ as a comma-separated list enclosed in square brackets, with each float rounded to $4$ decimal places (for example, $[j_1,j_2,j_3,j_4]$).",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical weather prediction verification, algorithmically well-posed, internally consistent, and contains all necessary information to proceed to a unique solution.\n\nThe solution is developed through a sequence of four main steps:\n1.  Object Identification: Isolate connected objects in the forecast and observed fields based on the given intensity threshold.\n2.  Attribute Calculation: Compute the specified attributes (Area, Maximum Intensity, Eccentricity) for each identified object.\n3.  Cost Matrix Construction: For each test case, calculate the pairwise cost between every forecast object and every observed object using the case-specific weights.\n4.  Optimal Assignment and Total Cost: Solve the linear sum assignment problem to find the optimal matching and compute the total objective function $J$, including penalties for unmatched forecast objects.\n\nAll mathematical entities are typeset in LaTeX as per the requirements.\n\n**Step 1: Object Identification**\n\nFirst, the $8 \\times 8$ forecast ($I^{(f)}$) and observed ($I^{(o)}$) intensity grids are populated. A binary mask is then generated for each grid by identifying cells where the intensity $I_{r,c}$ is greater than or equal to the threshold $T=1.0$.\n\n$M_{r,c} = 1 \\text{ if } I_{r,c} \\ge T, \\text{ and } M_{r,c} = 0 \\text{ otherwise.}$\n\nUsing an $8$-connectivity connected components labeling algorithm (e.g., Breadth-First Search on the binary masks), the objects are identified.\n\nFor the **forecast field**, the following cells are above the threshold:\n- $\\{(1,1), (1,2), (2,1), (2,2)\\}$\n- $\\{(3,4), (3,5), (3,6), (4,4), (4,5), (4,6)\\}$\n- $\\{(5,2), (6,1), (6,2), (7,1), (7,2)\\}$\n\nThis process yields $N_f = 3$ distinct forecast objects:\n- $F_1$: A $2 \\times 2$ block of cells at the top-left.\n- $F_2$: A set of $6$ cells in the middle of the grid.\n- $F_3$: A group of $5$ cells in the bottom-left.\n\nFor the **observed field**, the following cells are above the threshold:\n- $\\{(3,2), (3,3), (3,4), (4,3), (4,4)\\}$\n- $\\{(4,6), (4,7), (5,6), (5,7), (6,6), (6,7)\\}$\n\nThis process yields $N_o = 2$ distinct observed objects:\n- $O_1$: A group of $5$ cells.\n- $O_2$: A $3 \\times 2$ block of $6$ cells.\n\n**Step 2: Attribute Calculation**\n\nFor each of the $N_f=3$ forecast objects and $N_o=2$ observed objects, we compute a set of three attributes: Area ($A$), Maximum Intensity ($M$), and Eccentricity ($e$). The coordinates $(x,y)$ are defined as (column index, row index).\n\n**Example Calculation for Forecast Object $F_2$:**\n- **Cells**: $\\{(3,4), (3,5), (3,6), (4,4), (4,5), (4,6)\\}$\n- **Area ($A_{F2}$)**: The object consists of $n=6$ cells, so $A_{F2} = 6$.\n- **Maximum Intensity ($M_{F2}$)**: The intensities are $\\{1.1, 1.4, 1.6, 1.0, 1.2, 1.3\\}$. The maximum is $M_{F2} = 1.6$.\n- **Eccentricity ($e_{F2}$)**:\n    - Coordinates $(x_i, y_i)$: $\\{(4,3), (5,3), (6,3), (4,4), (5,4), (6,4)\\}$.\n    - Centroid: $(\\bar{x}, \\bar{y}) = (\\frac{1}{6}\\sum x_i, \\frac{1}{6}\\sum y_i) = (5.0, 3.5)$.\n    - Covariance Matrix $\\mathbf{C}$:\n    $$ \\mathbf{C} = \\begin{pmatrix} \\frac{1}{n}\\sum(x_i-\\bar{x})^2 & \\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y})\\\\ \\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y}) & \\frac{1}{n}\\sum(y_i-\\bar{y})^2 \\end{pmatrix} = \\begin{pmatrix} 0.666... & 0.0 \\\\ 0.0 & 0.25 \\end{pmatrix} $$\n    - Eigenvalues: The eigenvalues of this diagonal matrix are $\\lambda_{\\max} = 2/3$ and $\\lambda_{\\min} = 0.25$.\n    - Eccentricity: $e_{F2} = \\sqrt{1 - \\frac{\\lambda_{\\min}}{\\lambda_{\\max}}} = \\sqrt{1 - \\frac{0.25}{2/3}} = \\sqrt{1 - 0.375} = \\sqrt{0.625} \\approx 0.79057$.\n    \nFollowing this procedure for all objects yields the following attributes:\n\n| Object | Area ($A$) | Max Intensity ($M$) | Eccentricity ($e$) |\n|:------:|:----------:|:-------------------:|:------------------:|\n| $F_1$  |     $4$    |        $2.0$        |     $0.0$          |\n| $F_2$  |     $6$    |        $1.6$        |  $\\approx 0.79057$ |\n| $F_3$  |     $5$    |        $2.2$        |  $\\approx 0.81650$ |\n| $O_1$  |     $5$    |        $1.5$        |  $\\approx 0.81650$ |\n| $O_2$  |     $6$    |        $1.8$        |  $\\approx 0.79057$ |\n\n**Step 3: Cost Matrix and Optimal Matching**\n\nFor each test case, we construct a $3 \\times 2$ cost matrix where entry $c_{ij}$ is the cost of matching forecast object $F_i$ with observed object $O_j$. The cost is $c_{ij} = w_A d_A + w_E d_E + w_M d_M$.\n\n**Example for Case 1**: $(w_A, w_E, w_M) = (1.0, 1.0, 1.0)$.\nLet's compute the cost $c_{22}$ for matching $F_2$ with $O_2$.\n- Attributes: $F_2(A=6, M=1.6, e\\approx0.79057)$ and $O_2(A=6, M=1.8, e\\approx0.79057)$.\n- $d_A = \\frac{|6-6|}{\\max(6,6)} = 0$.\n- $d_E = |0.79057 - 0.79057| = 0$.\n- $d_M = \\frac{|1.6-1.8|}{\\max(1.6, 1.8)} = \\frac{0.2}{1.8} \\approx 0.11111$.\n- $c_{22} = 1.0 \\cdot (0) + 1.0 \\cdot (0) + 1.0 \\cdot (0.11111) = 0.11111$.\n\nFilling the entire cost matrix for Case 1:\n$$ \\mathbf{C}_{\\text{cost}} = \\begin{pmatrix}\n1.2665 & 1.2239 \\\\\n0.2551 & 0.1111 \\\\\n0.3182 & 0.3744\n\\end{pmatrix} $$\nWe apply the linear sum assignment algorithm to this matrix. Since $N_f > N_o$, the algorithm finds the best matching for the $N_o=2$ observed objects. The optimal assignment for Case $1$ is to match $(F_2, O_2)$ and $(F_3, O_1)$, leaving $F_1$ unmatched.\n\n**Step 4: Total Objective Calculation**\n\nThe total objective function is $J = \\sum_{(i,j)\\in \\mathcal{M}} c_{ij} + p_u \\cdot (N_f - |\\mathcal{M}|)$, where $\\mathcal{M}$ is the set of optimal-cost pairs.\n\n**Example for Case 1**:\n- Weights: $(w_A, w_E, w_M) = (1.0, 1.0, 1.0)$.\n- Penalty: $p_u = 0.25$.\n- Optimal pairs: $(F_2, O_2)$ with cost $c_{22} \\approx 0.1111$ and $(F_3, O_1)$ with cost $c_{31} \\approx 0.3182$.\n- Sum of matched costs: $0.11111 + 0.31818 \\approx 0.4293$.\n- Unmatched forecast objects: $N_f - |\\mathcal{M}| = 3 - 2 = 1$.\n- Penalty cost: $p_u \\cdot 1 = 0.25$.\n- Total objective: $J_1 = 0.4293 + 0.25 = 0.6793$.\n\nThis entire process is repeated for the four test cases, each with different weights and penalties, to produce the final vector of results.",
            "answer": "[0.6793,0.6793,0.7475,1.0990]"
        }
    ]
}