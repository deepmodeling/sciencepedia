{
    "hands_on_practices": [
        {
            "introduction": "The power of adjoint-based sensitivity analysis lies in its ability to efficiently compute gradients through complex, nonlinear models. A crucial first step is understanding how to handle individual nonlinear components, such as observation operators. This exercise  provides essential practice by asking you to derive the tangent-linear and adjoint models for a simplified, physically-motivated radiative transfer equation. Successfully completing this task builds the foundational skill of translating a nonlinear mapping into the linear operators required for variational data assimilation and sensitivity studies.",
            "id": "4009413",
            "problem": "In a simplified one-dimensional, monochromatic radiative transfer setting used in variational data assimilation for numerical weather prediction, an observation operator maps a model state to an observed brightness quantity via the relation $y = \\exp(-\\kappa q)\\,T$, where $T$ is a scaled brightness temperature, $q$ is a scaled specific humidity, and $\\kappa$ is a dimensionless absorption coefficient. Assume all variables have been nondimensionalized and are therefore unitless. Consider a single observation and a corresponding model state consisting of the pair $(T,q) \\in \\mathbb{R}^{2}$, so that the observation operator is a mapping $H: \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $H(T,q) = \\exp(-\\kappa q)\\,T$.\n\nStarting only from the definitions of the Gateaux derivative (for the tangent-linear operator) and of the adjoint operator with respect to the standard Euclidean inner product on $\\mathbb{R}^{2}$ and $\\mathbb{R}$, do the following:\n\n1) Derive the tangent-linear model of $H$ about a nominal state $(\\overline{T},\\overline{q})$, that is, the linear map $L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q]$ that gives the first-order variation of $y$ in the perturbation direction $(\\delta T, \\delta q)$.\n\n2) Using the definition of the adjoint with respect to the Euclidean inner product, derive the adjoint action of the tangent-linear on a given observation-space residual $r \\in \\mathbb{R}$, i.e., determine the unique $(\\nabla_{T}, \\nabla_{q}) \\in \\mathbb{R}^{2}$ such that for all $(\\delta T, \\delta q) \\in \\mathbb{R}^{2}$, the Euclidean inner products satisfy $\\langle L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q], r \\rangle_{\\mathbb{R}} = \\langle (\\delta T, \\delta q), (\\nabla_{T}, \\nabla_{q}) \\rangle_{\\mathbb{R}^{2}}$. This adjoint action is the core object used in adjoint-based Forecast Sensitivity to Observations (FSO), mapping observation-space information back to the model-space sensitivities.\n\n3) Evaluate your adjoint expression numerically at $\\overline{T} = 1.3$, $\\overline{q} = 0.2$, $\\kappa = 2$, and $r = 0.6$. Report your final numerical answer as a single row vector containing $(\\nabla_{T}, \\nabla_{q})$, rounded to four significant figures. All quantities are nondimensional; do not include units in your answer.",
            "solution": "The problem statement has been meticulously validated and is found to be scientifically grounded, well-posed, and objective. It presents a standard, albeit simplified, scenario in radiative transfer and variational data assimilation. All definitions and parameters are provided, leading to a unique and meaningful solution. We may therefore proceed with the derivation.\n\nThe problem asks for three distinct but related tasks concerning the observation operator $H: \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $H(T,q) = \\exp(-\\kappa q) T$. The state vector is $(T, q)$ and the observation is $y$.\n\n1) Derivation of the tangent-linear model.\n\nThe tangent-linear model of $H$ at a nominal state $(\\overline{T},\\overline{q})$ is the Gateaux derivative of $H$ at that point. Let the state be denoted by the vector $x = (T,q)$ and a perturbation by $\\delta x = (\\delta T, \\delta q)$. The Gateaux derivative in the direction $\\delta x$ is defined as:\n$$L_{H}(\\overline{x})[\\delta x] = \\lim_{\\epsilon \\to 0} \\frac{H(\\overline{x} + \\epsilon \\delta x) - H(\\overline{x})}{\\epsilon} = \\frac{d}{d\\epsilon} \\left[ H(\\overline{x} + \\epsilon \\delta x) \\right]_{\\epsilon=0}$$\nSubstituting the explicit form of $H$ and the vectors $\\overline{x} = (\\overline{T}, \\overline{q})$ and $\\delta x = (\\delta T, \\delta q)$:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\frac{d}{d\\epsilon} \\left[ H(\\overline{T} + \\epsilon\\delta T, \\overline{q} + \\epsilon\\delta q) \\right]_{\\epsilon=0}$$\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\frac{d}{d\\epsilon} \\left[ \\exp(-\\kappa(\\overline{q} + \\epsilon\\delta q))(\\overline{T} + \\epsilon\\delta T) \\right]_{\\epsilon=0}$$\nWe apply the product rule for differentiation with respect to $\\epsilon$:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\left[ \\frac{d}{d\\epsilon}\\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) \\right]_{\\epsilon=0} (\\overline{T} + 0 \\cdot \\delta T) + \\left[ \\exp(-\\kappa\\overline{q} - 0 \\cdot \\kappa\\delta q) \\right] \\left[ \\frac{d}{d\\epsilon}(\\overline{T} + \\epsilon\\delta T) \\right]_{\\epsilon=0}$$\nPerforming the differentiations:\n$$ \\frac{d}{d\\epsilon}\\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) = \\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) \\cdot (-\\kappa\\delta q) $$\n$$ \\frac{d}{d\\epsilon}(\\overline{T} + \\epsilon\\delta T) = \\delta T $$\nEvaluating at $\\epsilon = 0$:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\left( \\exp(-\\kappa\\overline{q}) \\cdot (-\\kappa\\delta q) \\right) \\overline{T} + \\exp(-\\kappa\\overline{q}) \\cdot \\delta T$$\nRearranging the terms gives the final expression for the tangent-linear model:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\exp(-\\kappa\\overline{q}) \\delta T - \\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) \\delta q$$\nThis is a linear operator acting on the perturbation vector $(\\delta T, \\delta q)$.\n\n2) Derivation of the adjoint action.\n\nThe adjoint of the tangent-linear operator $L_H$ (which we denote $L_H^*$) is defined by the inner product relation. For any state-space perturbation $(\\delta T, \\delta q) \\in \\mathbb{R}^{2}$ and any observation-space residual $r \\in \\mathbb{R}$, the adjoint action $(\\nabla_{T}, \\nabla_{q}) = L_H^*[r]$ must satisfy:\n$$\\langle L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q], r \\rangle_{\\mathbb{R}} = \\langle (\\delta T, \\delta q), (\\nabla_{T}, \\nabla_{q}) \\rangle_{\\mathbb{R}^{2}}$$\nThe inner product on $\\mathbb{R}$ is standard multiplication, and on $\\mathbb{R}^{2}$ it is the standard dot product. Let us evaluate the left-hand side (LHS) of the equation using the result from part 1:\n$$\\text{LHS} = (L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q]) \\cdot r$$\n$$\\text{LHS} = \\left( \\exp(-\\kappa\\overline{q})\\delta T - \\kappa\\overline{T}\\exp(-\\kappa\\overline{q})\\delta q \\right) r$$\n$$\\text{LHS} = r\\exp(-\\kappa\\overline{q})\\delta T - r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q})\\delta q$$\nNow, we evaluate the right-hand side (RHS) of the definition:\n$$\\text{RHS} = (\\delta T)(\\nabla_{T}) + (\\delta q)(\\nabla_{q})$$\nFor the equality $\\text{LHS} = \\text{RHS}$ to hold for all possible perturbations $(\\delta T, \\delta q)$, the coefficients of $\\delta T$ and $\\delta q$ on both sides must be equal.\nBy comparing the coefficients of $\\delta T$:\n$$\\nabla_{T} = r\\exp(-\\kappa\\overline{q})$$\nBy comparing the coefficients of $\\delta q$:\n$$\\nabla_{q} = -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q})$$\nThus, the adjoint action of the tangent-linear operator on a residual $r$ maps it to the state-space vector $(\\nabla_{T}, \\nabla_{q})$:\n$$(\\nabla_{T}, \\nabla_{q}) = \\left( r\\exp(-\\kappa\\overline{q}), -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) \\right)$$\nThis vector represents the gradient of the observation with respect to the model state variables, weighted by the observation residual; it is fundamental to variational assimilation and sensitivity studies.\n\n3) Numerical evaluation.\n\nWe are given the following nondimensional values: $\\overline{T} = 1.3$, $\\overline{q} = 0.2$, $\\kappa = 2$, and $r = 0.6$. We substitute these values into the expressions for $\\nabla_{T}$ and $\\nabla_{q}$.\nFirst, we compute the common exponential term:\n$$\\exp(-\\kappa\\overline{q}) = \\exp(-2 \\times 0.2) = \\exp(-0.4)$$\nNow, we compute $\\nabla_{T}$:\n$$\\nabla_{T} = r\\exp(-\\kappa\\overline{q}) = 0.6 \\times \\exp(-0.4)$$\n$$\\nabla_{T} \\approx 0.6 \\times 0.670320045... = 0.402192027...$$\nNext, we compute $\\nabla_{q}$:\n$$\\nabla_{q} = -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) = -(0.6)(2)(1.3)\\exp(-0.4)$$\n$$\\nabla_{q} = -1.56 \\times \\exp(-0.4)$$\n$$\\nabla_{q} \\approx -1.56 \\times 0.670320045... = -1.04569927...$$\nRounding these results to four significant figures as required:\n$$\\nabla_{T} \\approx 0.4022$$\n$$\\nabla_{q} \\approx -1.046$$\nThe resulting adjoint vector is $(\\nabla_{T}, \\nabla_{q}) = (0.4022, -1.046)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4022 & -1.046\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In weather forecasting, we are often most interested in error growth over short, finite time intervals, not asymptotic stability. For the non-normal dynamics typical of atmospheric flows, this transient growth is governed by singular vectors, not eigenvectors. This practice  uses a simple linear model of shear to illustrate this fundamental concept, demonstrating that the initial perturbations that amplify most rapidly are the leading singular vectors of the forecast operator, which can only be found using its adjoint, $L^{\\ast}$. Understanding this distinction is key to appreciating why adjoint methods are the tool of choice for short-range predictability and sensitivity studies.",
            "id": "4009319",
            "problem": "Consider a short-window linearized forecast model used in Numerical Weather Prediction for adjoint-based sensitivity analysis and Forecast Sensitivity to Observations (FSO). Let the Tangent Linear Model (TLM) state increment be $x \\in \\mathbb{R}^{2}$ and the forecast propagator be the linear map $L : \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$, with\n$$\nL \\;=\\; \\begin{pmatrix}\n1 & 2 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nThis $L$ represents a simple shear amplification over a short forecast interval and is non-normal under the Euclidean inner product. In adjoint-based sensitivity analysis, the Euclidean-adjoint $L^{\\ast}$ equals the transpose $L^{\\top}$, and singular vectors of $L$ (defined by the eigenstructure of $L^{\\ast} L$) characterize energy amplification directions relevant to short-term forecast error growth and to FSO diagnostics.\n\nStarting from the fundamental base that the leading right singular vector $v_{\\star}$ of a linear operator $L$ under the Euclidean inner product is any unit eigenvector of $L^{\\ast} L$ associated with its largest eigenvalue, carry out the following steps:\n- Verify that $L$ is non-normal by demonstrating $L L^{\\ast} \\neq L^{\\ast} L$.\n- Determine the leading right singular vector $v_{\\star}$ of $L$ corresponding to the largest singular value (do not normalize if it is not needed for your subsequent calculation).\n- To show that $v_{\\star}$ is not an eigenvector of $L$, compute the angle $\\theta$ between $L v_{\\star}$ and $v_{\\star}$ using the Euclidean inner product definition\n$$\n\\cos(\\theta) \\;=\\; \\frac{\\langle v_{\\star}, L v_{\\star} \\rangle}{\\|v_{\\star}\\| \\,\\|L v_{\\star}\\|}.\n$$\n\nExpress the final answer as the exact value of $\\theta$ in radians. If you choose to present a numerical approximation, round your answer to four significant figures and express the angle in radians.",
            "solution": "The problem is valid as it is scientifically grounded in linear algebra and its application to numerical weather prediction, is well-posed with all necessary information provided, and is stated objectively.\n\nThe solution proceeds in three steps as requested by the problem statement.\n\nFirst, we verify that the forecast propagator $L$ is non-normal. A linear operator $L$ is normal if it commutes with its adjoint, i.e., $L L^{\\ast} = L^{\\ast} L$. The given operator is the matrix\n$$\nL = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}\n$$\nUnder the Euclidean inner product, the adjoint $L^{\\ast}$ is the conjugate transpose of $L$. Since $L$ has real entries, the adjoint is simply the transpose, $L^{\\ast} = L^{\\top}$.\n$$\nL^{\\ast} = L^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}\n$$\nWe compute the products $L L^{\\ast}$ and $L^{\\ast} L$:\n$$\nL L^{\\ast} = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 2(2) & 1(0) + 2(1) \\\\ 0(1) + 1(2) & 0(0) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix}\n$$\n$$\nL^{\\ast} L = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) & 1(2) + 0(1) \\\\ 2(1) + 1(0) & 2(2) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}\n$$\nSince $L L^{\\ast} \\neq L^{\\ast} L$, the operator $L$ is indeed non-normal.\n\nSecond, we determine the leading right singular vector $v_{\\star}$ of $L$. This is defined as an eigenvector of $L^{\\ast} L$ associated with its largest eigenvalue. Let the matrix be $A = L^{\\ast} L = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}$. We find the eigenvalues $\\lambda$ of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 1-\\lambda & 2 \\\\ 2 & 5-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)(5-\\lambda) - (2)(2) = 0\n$$\n$$\n5 - 6\\lambda + \\lambda^2 - 4 = 0\n$$\n$$\n\\lambda^2 - 6\\lambda + 1 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{-(-6) \\pm \\sqrt{(-6)^2 - 4(1)(1)}}{2(1)} = \\frac{6 \\pm \\sqrt{36 - 4}}{2} = \\frac{6 \\pm \\sqrt{32}}{2} = \\frac{6 \\pm 4\\sqrt{2}}{2} = 3 \\pm 2\\sqrt{2}\n$$\nThe largest eigenvalue is $\\lambda_{\\max} = 3 + 2\\sqrt{2}$. We now find the corresponding eigenvector $v_{\\star} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ by solving $(A - \\lambda_{\\max} I) v_{\\star} = 0$.\n$$\n\\begin{pmatrix} 1 - (3 + 2\\sqrt{2}) & 2 \\\\ 2 & 5 - (3 + 2\\sqrt{2}) \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} -2 - 2\\sqrt{2} & 2 \\\\ 2 & 2 - 2\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we have $(-2 - 2\\sqrt{2})x + 2y = 0$, which simplifies to $y = (1 + \\sqrt{2})x$. The second row provides a consistent equation. We can choose any non-zero vector satisfying this relation. For simplicity, we choose $x=1$, which gives $y=1+\\sqrt{2}$.\n$$\nv_{\\star} = \\begin{pmatrix} 1 \\\\ 1 + \\sqrt{2} \\end{pmatrix}\n$$\nThe calculation of the angle is independent of the vector's magnitude, so we do not need to normalize $v_{\\star}$.\n\nThird, we compute the angle $\\theta$ between $L v_{\\star}$ and $v_{\\star}$. We first compute the vector $L v_{\\star}$:\n$$\nL v_{\\star} = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 + \\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 1 + 2(1 + \\sqrt{2}) \\\\ 1 + \\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 3 + 2\\sqrt{2} \\\\ 1 + \\sqrt{2} \\end{pmatrix}\n$$\nNow we use the formula for the cosine of the angle between two vectors $u$ and $w$: $\\cos(\\theta) = \\frac{\\langle u, w \\rangle}{\\|u\\| \\|w\\|}$. Here, $u = v_{\\star}$ and $w = L v_{\\star}$.\nThe inner product in the numerator is:\n$$\n\\langle v_{\\star}, L v_{\\star} \\rangle = (1)(3 + 2\\sqrt{2}) + (1 + \\sqrt{2})(1 + \\sqrt{2}) = (3 + 2\\sqrt{2}) + (1 + 2\\sqrt{2} + 2) = 6 + 4\\sqrt{2}\n$$\nFor the denominator, we compute the norms. The squared norm of $v_{\\star}$ is:\n$$\n\\|v_{\\star}\\|^2 = 1^2 + (1 + \\sqrt{2})^2 = 1 + (1 + 2\\sqrt{2} + 2) = 4 + 2\\sqrt{2}\n$$\nThe squared norm of $L v_{\\star}$ is:\n$$\n\\|L v_{\\star}\\|^2 = (3 + 2\\sqrt{2})^2 + (1 + \\sqrt{2})^2 = (9 + 12\\sqrt{2} + 8) + (1 + 2\\sqrt{2} + 2) = 20 + 14\\sqrt{2}\n$$\nSo, the cosine of the angle is:\n$$\n\\cos(\\theta) = \\frac{6 + 4\\sqrt{2}}{\\sqrt{4 + 2\\sqrt{2}} \\sqrt{20 + 14\\sqrt{2}}} = \\frac{2(3 + 2\\sqrt{2})}{\\sqrt{(4 + 2\\sqrt{2})(20 + 14\\sqrt{2})}}\n$$\nAlternatively, we can use the property that for a right singular vector $v$, $\\|L v\\|^2 = \\sigma^2 \\|v\\|^2$, where $\\sigma^2$ is the corresponding eigenvalue of $L^{\\ast} L$. Here $\\sigma_{\\max}^2 = \\lambda_{\\max} = 3 + 2\\sqrt{2}$. Then $\\|L v_{\\star}\\| \\|v_{\\star}\\| = \\sigma_{\\max} \\|v_{\\star}\\|^2$.\nThe singular value is $\\sigma_{\\max} = \\sqrt{3+2\\sqrt{2}} = \\sqrt{3+\\sqrt{8}}$. This nested radical simplifies to $\\sqrt{\\frac{3+1}{2}} + \\sqrt{\\frac{3-1}{2}} = \\sqrt{2}+1$.\nThe denominator is $\\|L v_{\\star}\\| \\|v_{\\star}\\| = \\sigma_{\\max} \\|v_{\\star}\\|^2 = (1 + \\sqrt{2})(4 + 2\\sqrt{2}) = 4 + 2\\sqrt{2} + 4\\sqrt{2} + 4 = 8 + 6\\sqrt{2}$.\nTherefore,\n$$\n\\cos(\\theta) = \\frac{6 + 4\\sqrt{2}}{8 + 6\\sqrt{2}} = \\frac{2(3 + 2\\sqrt{2})}{2(4 + 3\\sqrt{2})} = \\frac{3 + 2\\sqrt{2}}{4 + 3\\sqrt{2}}\n$$\nTo simplify this fraction, we rationalize the denominator:\n$$\n\\cos(\\theta) = \\frac{3 + 2\\sqrt{2}}{4 + 3\\sqrt{2}} \\times \\frac{4 - 3\\sqrt{2}}{4 - 3\\sqrt{2}} = \\frac{(3)(4) - (3)(3\\sqrt{2}) + (2\\sqrt{2})(4) - (2\\sqrt{2})(3\\sqrt{2})}{4^2 - (3\\sqrt{2})^2}\n$$\n$$\n\\cos(\\theta) = \\frac{12 - 9\\sqrt{2} + 8\\sqrt{2} - 12}{16 - 18} = \\frac{-\\sqrt{2}}{-2} = \\frac{\\sqrt{2}}{2}\n$$\nThe angle $\\theta \\in [0, \\pi]$ for which $\\cos(\\theta) = \\frac{\\sqrt{2}}{2}$ is $\\theta = \\frac{\\pi}{4}$. This non-zero angle confirms that the leading right singular vector $v_{\\star}$ is not an eigenvector of the non-normal operator $L$.",
            "answer": "$$\\boxed{\\frac{\\pi}{4}}$$"
        },
        {
            "introduction": "Ultimately, the goal of Forecast Sensitivity to Observation (FSO) is to quantify the value of each measurement in improving forecast accuracy. This theoretical exercise  provides the crucial link between the abstract adjoint-based sensitivity and a tangible measure of impact. By working through the principles of linear Gaussian estimation, you will derive an expression for the expected reduction in forecast error variance resulting from assimilating a single observation, showing explicitly how its value is determined by the forecast sensitivity and the error characteristics of the observation and the model background.",
            "id": "4009367",
            "problem": "Consider a linear, unbiased, Gaussian data assimilation setting typical of numerical weather prediction and climate modeling. Let the forecast state be $x \\in \\mathbb{R}^{n}$ with background (prior) error covariance $P^{f} \\in \\mathbb{R}^{n \\times n}$. Let a scalar forecast metric be $J = c^{\\top} x_{f}$, where $c \\in \\mathbb{R}^{n}$ is the gradient of $J$ with respect to the forecast state evaluated via an adjoint model. Suppose a single scalar observation is added at analysis time with observation operator $H = e_{i}^{\\top}$ (i.e., it measures the $i$-th state component), zero-mean observation error $\\varepsilon$ that is independent of background error, and observation error variance $R = \\sigma_{o}^{2}$. Assume that the background error covariance is locally diagonal at the observed location such that $P^{f}_{ii} = \\sigma_{b}^{2}$ and off-diagonal elements involving index $i$ are negligible. Denote the Forecast Sensitivity to Observation (FSO) at the observed location by $g = c_{i}$, i.e., the component of the adjoint-based forecast gradient with respect to the $i$-th state variable.\n\nUsing only these assumptions and first principles of linear Gaussian estimation, derive the expected reduction in the forecast error variance of $J$ due to assimilating this observation optimally. That is, derive a closed-form expression for $\\Delta \\mathrm{Var}(J) = \\mathrm{Var}_{f}(J) - \\mathrm{Var}_{a}(J)$ in terms of $g$, $\\sigma_{b}^{2}$, and $\\sigma_{o}^{2}$.\n\nExpress your final answer as a single closed-form analytic expression in $g$, $\\sigma_{b}^{2}$, and $\\sigma_{o}^{2}$. No numerical rounding is required, and no units should be reported in the final expression.",
            "solution": "This problem is valid as it is scientifically grounded in the principles of linear Gaussian estimation theory, specifically within the context of data assimilation as applied to numerical weather prediction. It is well-posed, with a clear objective and a complete set of consistent assumptions that allow for the derivation of a unique analytical solution.\n\nLet the true state of the system be denoted by $x_t \\in \\mathbb{R}^{n}$. The forecast error is $\\delta x_f = x_f - x_t$, where $x_f$ is the forecast state. The scalar forecast metric is given as $J = c^{\\top} x_f$. The error in this metric is $\\delta J_f = c^{\\top} x_f - c^{\\top} x_t = c^{\\top} (x_f - x_t) = c^{\\top} \\delta x_f$.\n\nThe forecast error variance of $J$, denoted as $\\mathrm{Var}_f(J)$, is the expected value of the squared error in $J$.\n$$\n\\mathrm{Var}_f(J) = E[(\\delta J_f)^2] = E[(c^{\\top} \\delta x_f)(c^{\\top} \\delta x_f)^{\\top}] = E[c^{\\top} \\delta x_f \\delta x_f^{\\top} c]\n$$\nBy definition, the background (forecast) error covariance matrix is $P^f = E[\\delta x_f \\delta x_f^{\\top}]$. Therefore, the forecast error variance of $J$ is:\n$$\n\\mathrm{Var}_f(J) = c^{\\top} P^f c\n$$\nAfter assimilating a new observation, we obtain an analysis state $x_a$ and a corresponding analysis error covariance matrix $P^a$. The analysis error variance of $J$ is then:\n$$\n\\mathrm{Var}_a(J) = c^{\\top} P^a c\n$$\nIn an optimal linear estimation framework (equivalent to a single step of a Kalman filter), the analysis error covariance $P^a$ is related to the forecast error covariance $P^f$ by the formula:\n$$\nP^a = (I - KH)P^f\n$$\nwhere $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix, $H$ is the observation operator, and $K$ is the optimal Kalman gain. The Kalman gain $K$ is given by:\n$$\nK = P^f H^{\\top} (H P^f H^{\\top} + R)^{-1}\n$$\nHere, $R$ is the observation error covariance matrix.\n\nThe objective is to find the reduction in the forecast error variance of $J$ due to the assimilation, which is $\\Delta \\mathrm{Var}(J) = \\mathrm{Var}_f(J) - \\mathrm{Var}_a(J)$.\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} P^f c - c^{\\top} P^a c = c^{\\top} P^f c - c^{\\top} (I - KH)P^f c\n$$\nExpanding the second term gives:\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} P^f c - (c^{\\top} P^f c - c^{\\top} KH P^f c) = c^{\\top} K H P^f c\n$$\nNow, we substitute the specific forms and values provided in the problem statement.\nThe observation is a single scalar, so its error variance is a scalar $R = \\sigma_o^2$.\nThe observation operator is $H = e_i^{\\top}$, which is a row vector with a $1$ in the $i$-th position and zeros elsewhere.\n\nFirst, let's simplify the term $(H P^f H^{\\top} + R)^{-1}$ in the expression for $K$.\nThe term $H P^f H^{\\top}$ is a scalar:\n$$\nH P^f H^{\\top} = e_i^{\\top} P^f e_i\n$$\nThis expression extracts the element at the $i$-th row and $i$-th column of $P^f$, which is $P^f_{ii}$. The problem states that $P^f_{ii} = \\sigma_b^2$. Thus, $H P^f H^{\\top} = \\sigma_b^2$.\nThe inverse term becomes:\n$$\n(H P^f H^{\\top} + R)^{-1} = (\\sigma_b^2 + \\sigma_o^2)^{-1}\n$$\nThis is a scalar inverse. Now we can write the Kalman gain $K$:\n$$\nK = P^f H^{\\top} (\\sigma_b^2 + \\sigma_o^2)^{-1} = P^f e_i (\\sigma_b^2 + \\sigma_o^2)^{-1}\n$$\nHere, $P^f e_i$ is the $i$-th column of the matrix $P^f$, which we can denote as $P^f_{:,i}$. So, $K = \\frac{1}{\\sigma_b^2 + \\sigma_o^2} P^f_{:,i}$. $K$ is a column vector of size $n \\times 1$.\n\nNow, substitute the expressions for $K$ and $H$ into the equation for $\\Delta \\mathrm{Var}(J)$:\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} \\left( \\frac{1}{\\sigma_b^2 + \\sigma_o^2} P^f_{:,i} \\right) (e_i^{\\top}) (P^f c)\n$$\nWe can rearrange the scalar and vector products. Note that $e_i^{\\top} (P^f c)$ is a scalar, equal to the $i$-th component of the vector $P^f c$, denoted as $(P^f c)_i$. The term $c^{\\top} P^f_{:,i}$ is also a scalar, representing the inner product of the vector $c$ and the $i$-th column of $P^f$.\nSince $P^f$ is a covariance matrix, it is symmetric ($P^f = (P^f)^{\\top}$). Therefore:\n$$\nc^{\\top} P^f_{:,i} = c^{\\top} (P^f e_i) = (e_i^{\\top} (P^f)^{\\top} c)^{\\top} = (e_i^{\\top} P^f c)^{\\top}\n$$\nAs $e_i^{\\top} P^f c$ is a scalar, its transpose is itself. So, $c^{\\top} P^f_{:,i} = e_i^{\\top} P^f c = (P^f c)_i$.\nLet's define $S_i = (P^f c)_i$. The expression for the variance reduction becomes:\n$$\n\\Delta \\mathrm{Var}(J) = \\frac{1}{\\sigma_b^2 + \\sigma_o^2} S_i \\cdot S_i = \\frac{S_i^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nNow we must evaluate $S_i$ using the final assumption. $S_i$ is the $i$-th component of the vector $P^f c$:\n$$\nS_i = (P^f c)_i = \\sum_{j=1}^{n} P^f_{ij} c_j\n$$\nThe problem states that the background error covariance is locally diagonal at the observed location, such that off-diagonal elements involving index $i$ are negligible. This means we can make the approximation $P^f_{ij} \\approx 0$ for all $j \\neq i$.\nUnder this approximation, the summation for $S_i$ collapses to a single term:\n$$\nS_i \\approx P^f_{ii} c_i\n$$\nWe are given $P^f_{ii} = \\sigma_b^2$ and that the $i$-th component of the adjoint gradient $c$ is the Forecast Sensitivity to Observation (FSO), $g = c_i$.\nSubstituting these into the expression for $S_i$:\n$$\nS_i \\approx \\sigma_b^2 g\n$$\nFinally, substituting this result into the equation for $\\Delta \\mathrm{Var}(J)$:\n$$\n\\Delta \\mathrm{Var}(J) = \\frac{(\\sigma_b^2 g)^2}{\\sigma_b^2 + \\sigma_o^2} = \\frac{\\sigma_b^4 g^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nThis is the closed-form expression for the expected reduction in the forecast error variance of the metric $J$ due to the assimilation of the single observation.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{b}^{4} g^{2}}{\\sigma_{b}^{2} + \\sigma_{o}^{2}}}\n$$"
        }
    ]
}