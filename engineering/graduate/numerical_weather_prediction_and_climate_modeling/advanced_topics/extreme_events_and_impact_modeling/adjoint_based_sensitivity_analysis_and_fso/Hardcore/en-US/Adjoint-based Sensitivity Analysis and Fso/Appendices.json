{
    "hands_on_practices": [
        {
            "introduction": "To construct the adjoint of a complex forecast model, one must first understand how to derive the adjoint of its individual components. This exercise  focuses on a crucial component: the observation operator, which maps model variables to observable quantities. By starting with a simplified, nonlinear radiative transfer equation, you will practice the fundamental mechanics of deriving the tangent-linear model and its corresponding adjoint from first principles, a core skill for anyone working with variational data assimilation or sensitivity analysis.",
            "id": "4009413",
            "problem": "In a simplified one-dimensional, monochromatic radiative transfer setting used in variational data assimilation for numerical weather prediction, an observation operator maps a model state to an observed brightness quantity via the relation $y = \\exp(-\\kappa q)\\,T$, where $T$ is a scaled brightness temperature, $q$ is a scaled specific humidity, and $\\kappa$ is a dimensionless absorption coefficient. Assume all variables have been nondimensionalized and are therefore unitless. Consider a single observation and a corresponding model state consisting of the pair $(T,q) \\in \\mathbb{R}^{2}$, so that the observation operator is a mapping $H: \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $H(T,q) = \\exp(-\\kappa q)\\,T$.\n\nStarting only from the definitions of the Gateaux derivative (for the tangent-linear operator) and of the adjoint operator with respect to the standard Euclidean inner product on $\\mathbb{R}^{2}$ and $\\mathbb{R}$, do the following:\n\n1) Derive the tangent-linear model of $H$ about a nominal state $(\\overline{T},\\overline{q})$, that is, the linear map $L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q]$ that gives the first-order variation of $y$ in the perturbation direction $(\\delta T, \\delta q)$.\n\n2) Using the definition of the adjoint with respect to the Euclidean inner product, derive the adjoint action of the tangent-linear on a given observation-space residual $r \\in \\mathbb{R}$, i.e., determine the unique $(\\nabla_{T}, \\nabla_{q}) \\in \\mathbb{R}^{2}$ such that for all $(\\delta T, \\delta q) \\in \\mathbb{R}^{2}$, the Euclidean inner products satisfy $\\langle L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q], r \\rangle_{\\mathbb{R}} = \\langle (\\delta T, \\delta q), (\\nabla_{T}, \\nabla_{q}) \\rangle_{\\mathbb{R}^{2}}$. This adjoint action is the core object used in adjoint-based Forecast Sensitivity to Observations (FSO), mapping observation-space information back to the model-space sensitivities.\n\n3) Evaluate your adjoint expression numerically at $\\overline{T} = 1.3$, $\\overline{q} = 0.2$, $\\kappa = 2$, and $r = 0.6$. Report your final numerical answer as a single row vector containing $(\\nabla_{T}, \\nabla_{q})$, rounded to four significant figures. All quantities are nondimensional; do not include units in your answer.",
            "solution": "The problem statement has been meticulously validated and is found to be scientifically grounded, well-posed, and objective. It presents a standard, albeit simplified, scenario in radiative transfer and variational data assimilation. All definitions and parameters are provided, leading to a unique and meaningful solution. We may therefore proceed with the derivation.\n\nThe problem asks for three distinct but related tasks concerning the observation operator $H: \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $H(T,q) = \\exp(-\\kappa q) T$. The state vector is $(T, q)$ and the observation is $y$.\n\n1) Derivation of the tangent-linear model.\n\nThe tangent-linear model of $H$ at a nominal state $(\\overline{T},\\overline{q})$ is the Gateaux derivative of $H$ at that point. Let the state be denoted by the vector $x = (T,q)$ and a perturbation by $\\delta x = (\\delta T, \\delta q)$. The Gateaux derivative in the direction $\\delta x$ is defined as:\n$$L_{H}(\\overline{x})[\\delta x] = \\lim_{\\epsilon \\to 0} \\frac{H(\\overline{x} + \\epsilon \\delta x) - H(\\overline{x})}{\\epsilon} = \\frac{d}{d\\epsilon} \\left[ H(\\overline{x} + \\epsilon \\delta x) \\right]_{\\epsilon=0}$$\nSubstituting the explicit form of $H$ and the vectors $\\overline{x} = (\\overline{T}, \\overline{q})$ and $\\delta x = (\\delta T, \\delta q)$:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\frac{d}{d\\epsilon} \\left[ H(\\overline{T} + \\epsilon\\delta T, \\overline{q} + \\epsilon\\delta q) \\right]_{\\epsilon=0}$$\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\frac{d}{d\\epsilon} \\left[ \\exp(-\\kappa(\\overline{q} + \\epsilon\\delta q))(\\overline{T} + \\epsilon\\delta T) \\right]_{\\epsilon=0}$$\nWe apply the product rule for differentiation with respect to $\\epsilon$:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\left[ \\frac{d}{d\\epsilon}\\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) \\right]_{\\epsilon=0} (\\overline{T} + 0 \\cdot \\delta T) + \\left[ \\exp(-\\kappa\\overline{q} - 0 \\cdot \\kappa\\delta q) \\right] \\left[ \\frac{d}{d\\epsilon}(\\overline{T} + \\epsilon\\delta T) \\right]_{\\epsilon=0}$$\nPerforming the differentiations:\n$$ \\frac{d}{d\\epsilon}\\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) = \\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) \\cdot (-\\kappa\\delta q) $$\n$$ \\frac{d}{d\\epsilon}(\\overline{T} + \\epsilon\\delta T) = \\delta T $$\nEvaluating at $\\epsilon = 0$:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\left( \\exp(-\\kappa\\overline{q}) \\cdot (-\\kappa\\delta q) \\right) \\overline{T} + \\exp(-\\kappa\\overline{q}) \\cdot \\delta T$$\nRearranging the terms gives the final expression for the tangent-linear model:\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\exp(-\\kappa\\overline{q}) \\delta T - \\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) \\delta q$$\nThis is a linear operator acting on the perturbation vector $(\\delta T, \\delta q)$.\n\n2) Derivation of the adjoint action.\n\nThe adjoint of the tangent-linear operator $L_H$ (which we denote $L_H^*$) is defined by the inner product relation. For any state-space perturbation $(\\delta T, \\delta q) \\in \\mathbb{R}^{2}$ and any observation-space residual $r \\in \\mathbb{R}$, the adjoint action $(\\nabla_{T}, \\nabla_{q}) = L_H^*[r]$ must satisfy:\n$$\\langle L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q], r \\rangle_{\\mathbb{R}} = \\langle (\\delta T, \\delta q), (\\nabla_{T}, \\nabla_{q}) \\rangle_{\\mathbb{R}^{2}}$$\nThe inner product on $\\mathbb{R}$ is standard multiplication, and on $\\mathbb{R}^{2}$ it is the standard dot product. Let us evaluate the left-hand side (LHS) of the equation using the result from part 1:\n$$\\text{LHS} = (L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q]) \\cdot r$$\n$$\\text{LHS} = \\left( \\exp(-\\kappa\\overline{q})\\delta T - \\kappa\\overline{T}\\exp(-\\kappa\\overline{q})\\delta q \\right) r$$\n$$\\text{LHS} = r\\exp(-\\kappa\\overline{q})\\delta T - r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q})\\delta q$$\nNow, we evaluate the right-hand side (RHS) of the definition:\n$$\\text{RHS} = (\\delta T)(\\nabla_{T}) + (\\delta q)(\\nabla_{q})$$\nFor the equality $\\text{LHS} = \\text{RHS}$ to hold for all possible perturbations $(\\delta T, \\delta q)$, the coefficients of $\\delta T$ and $\\delta q$ on both sides must be equal.\nBy comparing the coefficients of $\\delta T$:\n$$\\nabla_{T} = r\\exp(-\\kappa\\overline{q})$$\nBy comparing the coefficients of $\\delta q$:\n$$\\nabla_{q} = -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q})$$\nThus, the adjoint action of the tangent-linear operator on a residual $r$ maps it to the state-space vector $(\\nabla_{T}, \\nabla_{q})$:\n$$(\\nabla_{T}, \\nabla_{q}) = \\left( r\\exp(-\\kappa\\overline{q}), -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) \\right)$$\nThis vector represents the gradient of the observation with respect to the model state variables, weighted by the observation residual; it is fundamental to variational assimilation and sensitivity studies.\n\n3) Numerical evaluation.\n\nWe are given the following nondimensional values: $\\overline{T} = 1.3$, $\\overline{q} = 0.2$, $\\kappa = 2$, and $r = 0.6$. We substitute these values into the expressions for $\\nabla_{T}$ and $\\nabla_{q}$.\nFirst, we compute the common exponential term:\n$$\\exp(-\\kappa\\overline{q}) = \\exp(-2 \\times 0.2) = \\exp(-0.4)$$\nNow, we compute $\\nabla_{T}$:\n$$\\nabla_{T} = r\\exp(-\\kappa\\overline{q}) = 0.6 \\times \\exp(-0.4)$$\n$$\\nabla_{T} \\approx 0.6 \\times 0.670320045... = 0.402192027...$$\nNext, we compute $\\nabla_{q}$:\n$$\\nabla_{q} = -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) = -(0.6)(2)(1.3)\\exp(-0.4)$$\n$$\\nabla_{q} = -1.56 \\times \\exp(-0.4)$$\n$$\\nabla_{q} \\approx -1.56 \\times 0.670320045... = -1.04569927...$$\nRounding these results to four significant figures as required:\n$$\\nabla_{T} \\approx 0.4022$$\n$$\\nabla_{q} \\approx -1.046$$\nThe resulting adjoint vector is $(\\nabla_{T}, \\nabla_{q}) = (0.4022, -1.046)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4022 & -1.046\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While deriving the adjoint of a single operator is instructive, the true power of the method comes from its application to a full time-evolving forecast. This practice  moves from a static operator to a dynamic system, requiring you to implement an adjoint model that propagates sensitivities backward through multiple time steps. A key part of this hands-on coding exercise is performing an \"adjoint check\" by comparing the analytically derived gradient to a finite-difference approximation, a standard and essential verification step in the development of any operational adjoint model.",
            "id": "4009415",
            "problem": "Consider a discrete-time forecast model used as a pedagogical proxy for adjoint-based sensitivity analysis in numerical weather prediction. The goal is to compute and compare adjoint-based sensitivities with finite-difference sensitivities for a nonlinear two-component system. This sensitivity machinery underpins Forecast Sensitivity to Observation (FSO), which is used to attribute forecast-error reductions to individual observations.\n\nYou are given a two-dimensional state vector $x \\in \\mathbb{R}^{2}$ with components $x = [x_1, x_2]^{\\top}$, and a nonlinear, dissipative, coupled right-hand side $f(x)$ defined componentwise by\n$$\nf_1(x) = -\\beta x_1 - \\gamma x_1^{3} + \\kappa x_2^{2}, \\quad\nf_2(x) = -\\beta x_2 - \\gamma x_2^{3} - \\kappa x_1 x_2.\n$$\nThe forecast model is the explicit Euler time-discretization\n$$\nx_{k+1} = x_k + \\Delta t \\, f(x_k), \\quad k = 0,1,\\dots,N-1,\n$$\nwith given initial condition $x_0 \\in \\mathbb{R}^{2}$ and forecast horizon $N$.\n\nDefine a scalar forecast metric (cost function) that depends only on the first component at final time,\n$$\nJ(x_0) = \\tfrac{1}{2} \\left( H x_N - y \\right)^{2}, \\quad H = \\begin{bmatrix}1 & 0\\end{bmatrix},\n$$\nwith $y \\in \\mathbb{R}$ a prescribed target. All quantities are nondimensional; no physical units are required in the answer.\n\nTask:\n- Starting from the chain rule and the definition of the discrete model update map, derive the discrete adjoint recursion to obtain the gradient $\\nabla_{x_0} J$ without using finite differences.\n- Implement both the adjoint-based gradient and a central-difference approximation of $\\nabla_{x_0} J$:\n  - For central differences in direction of the $i$-th canonical basis vector $e_i$, use\n    $$\n    \\left[\\nabla_{x_0} J\\right]_i \\approx \\frac{J(x_0 + h e_i) - J(x_0 - h e_i)}{2 h}.\n    $$\n- Quantify the discrepancy between the adjoint-based gradient and the finite-difference gradient via the relative $\\ell_2$ error\n  $$\n  E(h) = \\frac{\\left\\|\\nabla^{\\text{adj}} J - \\nabla^{\\text{FD}} J\\right\\|_2}{\\max\\left(\\left\\|\\nabla^{\\text{adj}} J\\right\\|_2, \\epsilon_{\\min}\\right)},\n  $$\n  where $\\epsilon_{\\min}$ is a small positive constant to prevent division by zero.\n\n- Additionally, verify a directional-derivative identity by comparing $g^{\\top} v$ to a central-difference directional derivative, where $g = \\nabla^{\\text{adj}} J$ and $v \\in \\mathbb{R}^{2}$ is a prescribed direction.\n\nUse the following fixed parameters for the model and cost:\n- Initial condition $x_0 = \\begin{bmatrix}0.7 \\\\ -0.5\\end{bmatrix}$.\n- Coefficients $\\beta = 0.4$, $\\gamma = 0.5$, $\\kappa = 0.3$.\n- Time step $\\Delta t = 0.05$ and forecast steps $N = 200$.\n- Observation operator $H = \\begin{bmatrix}1 & 0\\end{bmatrix}$ and target $y = -1$ except where specified otherwise.\n- Use $\\epsilon_{\\min} = 10^{-15}$ in the relative error.\n\nImplement a program that performs the following test suite and returns the specified outputs:\n- Test case A (happy path): Use a central-difference step $h = 10^{-6}$ to compute $E(h)$ as a floating-point number.\n- Test case B (coarse finite-difference): Use $h = 10^{-2}$ to compute $E(h)$ as a floating-point number.\n- Test case C (round-off stressed): Use $h = 10^{-10}$ to compute $E(h)$ as a floating-point number.\n- Test case D (directional-derivative check): With $h = 10^{-6}$ and direction $v = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix}$ normalized to unit length, compute the relative discrepancy\n  $$\n  \\frac{\\left|g^{\\top} v - \\frac{J(x_0 + h v) - J(x_0 - h v)}{2 h}\\right|}{\\max\\left(\\left|g^{\\top} v\\right|, \\epsilon_{\\min}\\right)},\n  $$\n  and return a boolean indicating whether this discrepancy is less than $10^{-7}$.\n- Test case E (zero-mismatch boundary condition): Set $y = H x_N$ using the forward model at the given $x_0$. Compute the adjoint-based gradient and return its $\\ell_2$ norm as a floating-point number.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD,resultE]\"). The results must appear in the order of the tests A, B, C, D, E.\n\nScientific realism and derivation constraints:\n- Begin derivations from fundamental principles: the chain rule for composite maps, the definition of the explicit Euler update, and the definition of the gradient of the terminal cost.\n- Do not use any pre-derived adjoint formulas or black-box automatic differentiation; instead, derive the discrete adjoint recurrence from first principles.\n- Ensure the implementation is numerically stable and self-consistent for the given parameters.\n\nInterpretation requirement:\n- In your solution, explain why the error $E(h)$ behaves differently for the choices $h = 10^{-2}$, $h = 10^{-6}$, and $h = 10^{-10}$, relating truncation and round-off errors, and connect the role of the adjoint gradient to the concept of Forecast Sensitivity to Observation (FSO).",
            "solution": "The problem requires the derivation and implementation of an adjoint-based method to compute the sensitivity of a forecast metric with respect to the initial conditions of a discrete-time nonlinear dynamical system. This is to be compared with finite-difference approximations.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **State Vector:** $x = [x_1, x_2]^{\\top} \\in \\mathbb{R}^{2}$\n*   **Nonlinear Dynamics Function $f(x)$:**\n    *   $f_1(x) = -\\beta x_1 - \\gamma x_1^{3} + \\kappa x_2^{2}$\n    *   $f_2(x) = -\\beta x_2 - \\gamma x_2^{3} - \\kappa x_1 x_2$\n*   **Forecast Model (Explicit Euler):** $x_{k+1} = x_k + \\Delta t \\, f(x_k)$, for $k = 0,1,\\dots,N-1$.\n*   **Scalar Cost Function:** $J(x_0) = \\tfrac{1}{2} \\left( H x_N - y \\right)^{2}$, with $H = \\begin{bmatrix}1 & 0\\end{bmatrix}$.\n*   **Fixed Parameters:**\n    *   Initial condition: $x_0 = \\begin{bmatrix}0.7 \\\\ -0.5\\end{bmatrix}$\n    *   Coefficients: $\\beta = 0.4$, $\\gamma = 0.5$, $\\kappa = 0.3$\n    *   Time step: $\\Delta t = 0.05$\n    *   Forecast steps: $N = 200$\n    *   Target value: $y = -1$ (except for Test E)\n    *   Denominator floor: $\\epsilon_{\\min} = 10^{-15}$\n*   **Tasks:**\n    1.  Derive the discrete adjoint recursion for $\\nabla_{x_0} J$.\n    2.  Implement the adjoint method and a central-difference approximation for $\\nabla_{x_0} J$.\n    3.  Compute the relative error $E(h) = \\frac{\\left\\|\\nabla^{\\text{adj}} J - \\nabla^{\\text{FD}} J\\right\\|_2}{\\max\\left(\\left\\|\\nabla^{\\text{adj}} J\\right\\|_2, \\epsilon_{\\min}\\right)}$ for $h \\in \\{10^{-6}, 10^{-2}, 10^{-10}\\}$.\n    4.  Verify a directional derivative identity using $v = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix}$ (normalized) and $h = 10^{-6}$.\n    5.  For a case where $y = H x_N$, compute the $\\ell_2$ norm of the adjoint-based gradient.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem describes a standard setup in numerical analysis, optimization, and data assimilation. It uses a nonlinear dynamical system (a valid mathematical object) discretized by the explicit Euler method (a standard numerical scheme). The concept of adjoint-based sensitivity analysis is a cornerstone of these fields. The problem is scientifically and mathematically sound.\n*   **Well-Posed:** The problem is clearly defined. The system of equations, parameters, initial conditions, and cost function are all explicitly given. The tasks are specific and lead to uniquely computable results.\n*   **Objective:** The problem is stated using precise mathematical language, free from subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a well-posed, scientifically sound exercise in implementing and understanding adjoint models.\n\n### Derivation of the Discrete Adjoint Model\n\nThe goal is to compute the gradient of the cost function $J$ with respect to the initial state $x_0$, denoted $\\nabla_{x_0} J$. The cost function $J$ depends on $x_0$ through a sequence of model updates:\n$x_0 \\xrightarrow{M_0} x_1 \\xrightarrow{M_1} \\cdots \\xrightarrow{M_{N-1}} x_N$, where $M_k(x) = x + \\Delta t f(x)$.\nThus, $J$ is a composite function: $J(x_0) = \\mathcal{J}(M_{N-1}(M_{N-2}(\\cdots M_0(x_0)\\cdots)))$, where $\\mathcal{J}(x_N) = \\frac{1}{2}(Hx_N - y)^2$.\n\nWe start with the total differential of $J$ with respect to a perturbation $dx_0$ in the initial state:\n$$\ndJ = (\\nabla_{x_0} J)^\\top dx_0\n$$\nUsing the chain rule, we can relate the differential of the cost function to the differential of the final state $dx_N$:\n$$\ndJ = (\\nabla_{x_N} \\mathcal{J})^\\top dx_N\n$$\nThe perturbation $dx_N$ is related to $dx_{N-1}$ through the linearization of the model update step $x_N = M_{N-1}(x_{N-1})$:\n$$\ndx_N = \\frac{\\partial M_{N-1}}{\\partial x_{N-1}} \\bigg|_{x_{N-1}} dx_{N-1} = \\mathbf{M}_{N-1} dx_{N-1}\n$$\nThe matrix $\\mathbf{M}_k = \\frac{\\partial x_{k+1}}{\\partial x_k}$ is the tangent linear model (TLM) propagator at step $k$. For the explicit Euler scheme, it is:\n$$\n\\mathbf{M}_k = \\frac{\\partial}{\\partial x_k} \\left(x_k + \\Delta t f(x_k)\\right) = \\mathbf{I} + \\Delta t \\frac{\\partial f}{\\partial x}\\bigg|_{x_k}\n$$\nwhere $\\mathbf{I}$ is the identity matrix. Applying this relationship recursively from $k=N-1$ down to $k=0$ gives:\n$$\ndx_N = \\mathbf{M}_{N-1} \\mathbf{M}_{N-2} \\cdots \\mathbf{M}_0 dx_0\n$$\nSubstituting this into the expression for $dJ$:\n$$\ndJ = (\\nabla_{x_N} \\mathcal{J})^\\top (\\mathbf{M}_{N-1} \\mathbf{M}_{N-2} \\cdots \\mathbf{M}_0) dx_0\n$$\nBy comparing this with $dJ = (\\nabla_{x_0} J)^\\top dx_0$, we identify the gradient:\n$$\n(\\nabla_{x_0} J)^\\top = (\\nabla_{x_N} \\mathcal{J})^\\top \\mathbf{M}_{N-1} \\mathbf{M}_{N-2} \\cdots \\mathbf{M}_0\n$$\nTaking the transpose of both sides:\n$$\n\\nabla_{x_0} J = (\\mathbf{M}_0)^\\top (\\mathbf{M}_1)^\\top \\cdots (\\mathbf{M}_{N-1})^\\top \\nabla_{x_N} \\mathcal{J}\n$$\nThe adjoint method provides an efficient way to evaluate this product by defining a sequence of adjoint state vectors $\\lambda_k \\in \\mathbb{R}^2$. Let the terminal adjoint state be the gradient of the cost function with respect to the final model state:\n$$\n\\lambda_N = \\nabla_{x_N} \\mathcal{J}\n$$\nThen, define the adjoint recursion by propagating this sensitivity backward in time:\n$$\n\\lambda_k = (\\mathbf{M}_k)^\\top \\lambda_{k+1} \\quad \\text{for } k = N-1, N-2, \\dots, 0\n$$\nUnrolling this recursion shows that $\\lambda_0 = (\\mathbf{M}_0)^\\top \\cdots (\\mathbf{M}_{N-1})^\\top \\lambda_N$, which is exactly the expression for $\\nabla_{x_0} J$.\n\nThe steps to compute the adjoint-based gradient are:\n1.  **Forward Integration:** Given $x_0$, compute and store the full state trajectory $\\{x_k\\}_{k=0}^N$ using the rule $x_{k+1} = x_k + \\Delta t f(x_k)$.\n2.  **Adjoint Initialization:** Compute the terminal adjoint state $\\lambda_N$.\n    $$\n    J = \\tfrac{1}{2}(x_{1,N} - y)^2 \\implies \\lambda_N = \\nabla_{x_N} J = \\begin{bmatrix} \\partial J/\\partial x_{1,N} \\\\ \\partial J/\\partial x_{2,N} \\end{bmatrix} = \\begin{bmatrix} x_{1,N} - y \\\\ 0 \\end{bmatrix} = \\mathbf{H}^\\top(\\mathbf{H}x_N - y)\n    $$\n3.  **Backward Recursion:** Iterate backward from $k=N-1$ to $0$:\n    *   Evaluate the Jacobian of $f$ at the stored state $x_k$: $\\mathbf{F}_k = \\frac{\\partial f}{\\partial x}|_{x_k}$.\n    *   Update the adjoint state: $\\lambda_k = (\\mathbf{I} + \\Delta t \\mathbf{F}_k)^\\top \\lambda_{k+1}$.\n4.  **Result:** The gradient is the initial adjoint state, $\\nabla_{x_0} J = \\lambda_0$.\n\nThe Jacobian of $f$ is required for this process:\n$$\n\\frac{\\partial f}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} -\\beta - 3\\gamma x_1^2 & 2\\kappa x_2 \\\\ -\\kappa x_2 & -\\beta - 3\\gamma x_2^2 - \\kappa x_1 \\end{bmatrix}\n$$\n\n### Interpretation of Results\n\n**Finite-Difference Error Behavior ($E(h)$):**\nThe relative error $E(h)$ between the adjoint gradient and the finite-difference approximation changes with the step size $h$ due to the interplay of two types of numerical errors:\n\n*   **Truncation Error:** The central-difference formula is an approximation derived from a Taylor series expansion, which truncates higher-order terms. The leading error term is of order $O(h^2)$. For a large step size like $h=10^{-2}$, this is the dominant error source. The error is large because the approximation is fundamentally coarse.\n*   **Round-off Error:** Digital computers have finite precision (machine epsilon $\\epsilon_m \\approx 10^{-16}$ for double precision). When $h$ is very small, like $h=10^{-10}$, the numerator of the finite-difference formula, $J(x_0+hv) - J(x_0-hv)$, involves the subtraction of two nearly identical numbers. This leads to a catastrophic cancellation, or loss of significant digits. This small, error-prone result is then divided by a very small number ($2h$), which amplifies the relative error.\n*   **Optimal Step Size:** The total error is a sum of these two components, roughly $Error(h) \\approx A h^2 + B \\epsilon_m/h$. There is an optimal $h$ that minimizes this sum. For double precision, this optimal value is typically in the range $10^{-8}$ to $10^{-5}$. The choice $h=10^{-6}$ is near this optimum, resulting in the smallest error among the three test cases, as it balances truncation and round-off effects.\n\n**Connection to Forecast Sensitivity to Observation (FSO):**\nThe adjoint gradient $\\nabla_{x_0} J$ quantifies the sensitivity of a forecast outcome (the metric $J$) to infinitesimal changes in the initial conditions $x_0$. This is the core concept behind FSO in numerical weather prediction.\n\nIn a real-world scenario, the initial condition $x_0$ for a forecast is the \"analysis,\" which is produced by a data assimilation system that blends a previous short-term forecast (the \"background\") with new observations. A typical forecast metric $J$ measures the error of a longer-term forecast (e.g., at 24 hours) against a verifying analysis or observations.\n\nThe adjoint model, by computing $\\nabla_{x_0} J$, propagates the sensitivity of this forecast error backward in time from the forecast verification time to the analysis time ($t=0$). This gradient tells us which aspects of the initial analysis have the largest impact on the forecast error. FSO techniques use this gradient to project the sensitivity onto the space of observations, thereby estimating the impact of each individual observation on the forecast accuracy. In essence, the calculation performed in this problem, sensitivity to initial conditions, is the fundamental building block of the full FSO diagnostic.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the adjoint sensitivity analysis problem.\n    \"\"\"\n    \n    # --- Define model parameters and constants ---\n    x0_val = np.array([0.7, -0.5])\n    beta = 0.4\n    gamma = 0.5\n    kappa = 0.3\n    dt = 0.05\n    N = 200\n    H = np.array([1.0, 0.0])\n    y_default = -1.0\n    eps_min = 1e-15\n\n    # --- Define core model functions ---\n    \n    def f(x):\n        \"\"\"The nonlinear right-hand side of the model.\"\"\"\n        x1, x2 = x\n        f1 = -beta * x1 - gamma * x1**3 + kappa * x2**2\n        f2 = -beta * x2 - gamma * x2**3 - kappa * x1 * x2\n        return np.array([f1, f2])\n\n    def jacobian_f(x):\n        \"\"\"The Jacobian of the nonlinear function f.\"\"\"\n        x1, x2 = x\n        df1_dx1 = -beta - 3 * gamma * x1**2\n        df1_dx2 = 2 * kappa * x2\n        df2_dx1 = -kappa * x2\n        df2_dx2 = -beta - 3 * gamma * x2**2 - kappa * x1\n        return np.array([[df1_dx1, df1_dx2], [df2_dx1, df2_dx2]])\n\n    def run_forward_model(x_start, n_steps, time_step):\n        \"\"\"\n        Runs the explicit Euler forward model and returns the full trajectory.\n        \"\"\"\n        traj = np.zeros((n_steps + 1, 2))\n        traj[0] = x_start\n        x_k = x_start\n        for k in range(n_steps):\n            x_k = x_k + time_step * f(x_k)\n            traj[k + 1] = x_k\n        return traj\n\n    def compute_cost(x_final, h_op, y_target):\n        \"\"\"Computes the scalar cost function J.\"\"\"\n        return 0.5 * (np.dot(h_op, x_final) - y_target)**2\n\n    def compute_adjoint_gradient(x0, n_steps, time_step, h_op, y_target):\n        \"\"\"\n        Computes the gradient of J w.r.t. x0 using the adjoint method.\n        \"\"\"\n        # 1. Forward run: Integrate model and store trajectory\n        trajectory = run_forward_model(x0, n_steps, time_step)\n        x_N = trajectory[-1]\n\n        # 2. Adjoint initialization: Gradient of J w.r.t. x_N\n        adj_k_plus_1 = h_op.T * (np.dot(h_op, x_N) - y_target)\n\n        # 3. Backward run: Adjoint recursion\n        for k in range(n_steps - 1, -1, -1):\n            x_k = trajectory[k]\n            F_k = jacobian_f(x_k)\n            # M_k^T = (I + dt * F_k)^T\n            M_k_T = np.identity(2) + time_step * F_k.T\n            adj_k = M_k_T @ adj_k_plus_1\n            adj_k_plus_1 = adj_k\n        \n        # 4. Result is the initial adjoint state\n        grad_adj = adj_k_plus_1\n        return grad_adj\n\n    def compute_fd_gradient(x0, h_step, n_steps, time_step, h_op, y_target):\n        \"\"\"\n        Computes the gradient of J w.r.t. x0 using central differences.\n        \"\"\"\n        grad_fd = np.zeros_like(x0)\n        for i in range(len(x0)):\n            ei = np.zeros_like(x0)\n            ei[i] = 1.0\n            \n            x0_plus_h = x0 + h_step * ei\n            traj_plus = run_forward_model(x0_plus_h, n_steps, time_step)\n            J_plus = compute_cost(traj_plus[-1], h_op, y_target)\n            \n            x0_minus_h = x0 - h_step * ei\n            traj_minus = run_forward_model(x0_minus_h, n_steps, time_step)\n            J_minus = compute_cost(traj_minus[-1], h_op, y_target)\n            \n            grad_fd[i] = (J_plus - J_minus) / (2 * h_step)\n        return grad_fd\n\n    # --- Execute Test Cases ---\n    \n    results = []\n    \n    # Pre-compute the adjoint gradient for tests A, B, C, D\n    grad_adj_base = compute_adjoint_gradient(x0_val, N, dt, H, y_default)\n\n    # Test Case A: Happy path, h = 1e-6\n    h_A = 1e-6\n    grad_fd_A = compute_fd_gradient(x0_val, h_A, N, dt, H, y_default)\n    norm_adj = np.linalg.norm(grad_adj_base)\n    err_A = np.linalg.norm(grad_adj_base - grad_fd_A) / max(norm_adj, eps_min)\n    results.append(err_A)\n\n    # Test Case B: Coarse finite difference, h = 1e-2\n    h_B = 1e-2\n    grad_fd_B = compute_fd_gradient(x0_val, h_B, N, dt, H, y_default)\n    err_B = np.linalg.norm(grad_adj_base - grad_fd_B) / max(norm_adj, eps_min)\n    results.append(err_B)\n\n    # Test Case C: Round-off stressed, h = 1e-10\n    h_C = 1e-10\n    grad_fd_C = compute_fd_gradient(x0_val, h_C, N, dt, H, y_default)\n    err_C = np.linalg.norm(grad_adj_base - grad_fd_C) / max(norm_adj, eps_min)\n    results.append(err_C)\n\n    # Test Case D: Directional derivative check\n    h_D = 1e-6\n    v = np.array([1.0, -2.0])\n    v_norm = v / np.linalg.norm(v)\n\n    # Adjoint-based directional derivative g^T * v\n    dir_deriv_adj = np.dot(grad_adj_base.T, v_norm)\n\n    # FD-based directional derivative\n    x0_plus_hv = x0_val + h_D * v_norm\n    x0_minus_hv = x0_val - h_D * v_norm\n    traj_plus_v = run_forward_model(x0_plus_hv, N, dt)\n    traj_minus_v = run_forward_model(x0_minus_hv, N, dt)\n    J_plus_v = compute_cost(traj_plus_v[-1], H, y_default)\n    J_minus_v = compute_cost(traj_minus_v[-1], H, y_default)\n    dir_deriv_fd = (J_plus_v - J_minus_v) / (2 * h_D)\n\n    discrepancy_D = abs(dir_deriv_adj - dir_deriv_fd) / max(abs(dir_deriv_adj), eps_min)\n    result_D = discrepancy_D < 1e-7\n    results.append(result_D)\n\n    # Test Case E: Zero-mismatch boundary condition\n    # First, find the \"true\" final state to set y\n    trajectory_E = run_forward_model(x0_val, N, dt)\n    y_E = np.dot(H, trajectory_E[-1])\n    \n    # Now compute the adjoint gradient with this y\n    grad_adj_E = compute_adjoint_gradient(x0_val, N, dt, H, y_E)\n    norm_E = np.linalg.norm(grad_adj_E)\n    results.append(norm_E)\n\n    # --- Format and Print Output ---\n    # Convert boolean to lowercase string 'true'/'false' for output\n    formatted_results = []\n    for res in results:\n        if isinstance(res, bool):\n            formatted_results.append(str(res).lower())\n        else:\n            formatted_results.append(f\"{res:.12e}\" if isinstance(res, float) else str(res))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After mastering the computation of sensitivities, a critical question arises: how can this information be used to improve forecasts? This final practice  connects the abstract concept of an adjoint-derived gradient to a tangible and vital application in Forecast Sensitivity to Observation (FSO). By working through this derivation, you will uncover the theoretical relationship between the forecast sensitivity at a specific location and the expected reduction in forecast error that can be achieved by adding an observation there, providing a quantitative basis for designing and evaluating observing networks.",
            "id": "4009367",
            "problem": "Consider a linear, unbiased, Gaussian data assimilation setting typical of numerical weather prediction and climate modeling. Let the forecast state be $x \\in \\mathbb{R}^{n}$ with background (prior) error covariance $P^{f} \\in \\mathbb{R}^{n \\times n}$. Let a scalar forecast metric be $J = c^{\\top} x_{f}$, where $c \\in \\mathbb{R}^{n}$ is the gradient of $J$ with respect to the forecast state evaluated via an adjoint model. Suppose a single scalar observation is added at analysis time with observation operator $H = e_{i}^{\\top}$ (i.e., it measures the $i$-th state component), zero-mean observation error $\\varepsilon$ that is independent of background error, and observation error variance $R = \\sigma_{o}^{2}$. Assume that the background error covariance is locally diagonal at the observed location such that $P^{f}_{ii} = \\sigma_{b}^{2}$ and off-diagonal elements involving index $i$ are negligible. Denote the Forecast Sensitivity to Observation (FSO) at the observed location by $g = c_{i}$, i.e., the component of the adjoint-based forecast gradient with respect to the $i$-th state variable.\n\nUsing only these assumptions and first principles of linear Gaussian estimation, derive the expected reduction in the forecast error variance of $J$ due to assimilating this observation optimally. That is, derive a closed-form expression for $\\Delta \\mathrm{Var}(J) = \\mathrm{Var}_{f}(J) - \\mathrm{Var}_{a}(J)$ in terms of $g$, $\\sigma_{b}^{2}$, and $\\sigma_{o}^{2}$.\n\nExpress your final answer as a single closed-form analytic expression in $g$, $\\sigma_{b}^{2}$, and $\\sigma_{o}^{2}$. No numerical rounding is required, and no units should be reported in the final expression.",
            "solution": "This problem is valid as it is scientifically grounded in the principles of linear Gaussian estimation theory, specifically within the context of data assimilation as applied to numerical weather prediction. It is well-posed, with a clear objective and a complete set of consistent assumptions that allow for the derivation of a unique analytical solution.\n\nLet the true state of the system be denoted by $x_t \\in \\mathbb{R}^{n}$. The forecast error is $\\delta x_f = x_f - x_t$, where $x_f$ is the forecast state. The scalar forecast metric is given as $J = c^{\\top} x_f$. The error in this metric is $\\delta J_f = c^{\\top} x_f - c^{\\top} x_t = c^{\\top} (x_f - x_t) = c^{\\top} \\delta x_f$.\n\nThe forecast error variance of $J$, denoted as $\\mathrm{Var}_f(J)$, is the expected value of the squared error in $J$.\n$$\n\\mathrm{Var}_f(J) = E[(\\delta J_f)^2] = E[(c^{\\top} \\delta x_f)(c^{\\top} \\delta x_f)^{\\top}] = E[c^{\\top} \\delta x_f \\delta x_f^{\\top} c]\n$$\nBy definition, the background (forecast) error covariance matrix is $P^f = E[\\delta x_f \\delta x_f^{\\top}]$. Therefore, the forecast error variance of $J$ is:\n$$\n\\mathrm{Var}_f(J) = c^{\\top} P^f c\n$$\nAfter assimilating a new observation, we obtain an analysis state $x_a$ and a corresponding analysis error covariance matrix $P^a$. The analysis error variance of $J$ is then:\n$$\n\\mathrm{Var}_a(J) = c^{\\top} P^a c\n$$\nIn an optimal linear estimation framework (equivalent to a single step of a Kalman filter), the analysis error covariance $P^a$ is related to the forecast error covariance $P^f$ by the formula:\n$$\nP^a = (I - KH)P^f\n$$\nwhere $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix, $H$ is the observation operator, and $K$ is the optimal Kalman gain. The Kalman gain $K$ is given by:\n$$\nK = P^f H^{\\top} (H P^f H^{\\top} + R)^{-1}\n$$\nHere, $R$ is the observation error covariance matrix.\n\nThe objective is to find the reduction in the forecast error variance of $J$ due to the assimilation, which is $\\Delta \\mathrm{Var}(J) = \\mathrm{Var}_f(J) - \\mathrm{Var}_a(J)$.\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} P^f c - c^{\\top} P^a c = c^{\\top} P^f c - c^{\\top} (I - KH)P^f c\n$$\nExpanding the second term gives:\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} P^f c - (c^{\\top} P^f c - c^{\\top} KH P^f c) = c^{\\top} K H P^f c\n$$\nNow, we substitute the specific forms and values provided in the problem statement.\nThe observation is a single scalar, so its error variance is a scalar $R = \\sigma_o^2$.\nThe observation operator is $H = e_i^{\\top}$, which is a row vector with a $1$ in the $i$-th position and zeros elsewhere.\n\nFirst, let's simplify the term $(H P^f H^{\\top} + R)^{-1}$ in the expression for $K$.\nThe term $H P^f H^{\\top}$ is a scalar:\n$$\nH P^f H^{\\top} = e_i^{\\top} P^f e_i\n$$\nThis expression extracts the element at the $i$-th row and $i$-th column of $P^f$, which is $P^f_{ii}$. The problem states that $P^f_{ii} = \\sigma_b^2$. Thus, $H P^f H^{\\top} = \\sigma_b^2$.\nThe inverse term becomes:\n$$\n(H P^f H^{\\top} + R)^{-1} = (\\sigma_b^2 + \\sigma_o^2)^{-1}\n$$\nThis is a scalar inverse. Now we can write the Kalman gain $K$:\n$$\nK = P^f H^{\\top} (\\sigma_b^2 + \\sigma_o^2)^{-1} = P^f e_i (\\sigma_b^2 + \\sigma_o^2)^{-1}\n$$\nHere, $P^f e_i$ is the $i$-th column of the matrix $P^f$, which we can denote as $P^f_{:,i}$. So, $K = \\frac{1}{\\sigma_b^2 + \\sigma_o^2} P^f_{:,i}$. $K$ is a column vector of size $n \\times 1$.\n\nNow, substitute the expressions for $K$ and $H$ into the equation for $\\Delta \\mathrm{Var}(J)$:\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} \\left( \\frac{1}{\\sigma_b^2 + \\sigma_o^2} P^f_{:,i} \\right) (e_i^{\\top}) (P^f c)\n$$\nWe can rearrange the scalar and vector products. Note that $e_i^{\\top} (P^f c)$ is a scalar, equal to the $i$-th component of the vector $P^f c$, denoted as $(P^f c)_i$. The term $c^{\\top} P^f_{:,i}$ is also a scalar, representing the inner product of the vector $c$ and the $i$-th column of $P^f$.\nSince $P^f$ is a covariance matrix, it is symmetric ($P^f = (P^f)^{\\top}$). Therefore:\n$$\nc^{\\top} P^f_{:,i} = c^{\\top} (P^f e_i) = (e_i^{\\top} (P^f)^{\\top} c)^{\\top} = (e_i^{\\top} P^f c)^{\\top}\n$$\nAs $e_i^{\\top} P^f c$ is a scalar, its transpose is itself. So, $c^{\\top} P^f_{:,i} = e_i^{\\top} P^f c = (P^f c)_i$.\nLet's define $S_i = (P^f c)_i$. The expression for the variance reduction becomes:\n$$\n\\Delta \\mathrm{Var}(J) = \\frac{1}{\\sigma_b^2 + \\sigma_o^2} S_i \\cdot S_i = \\frac{S_i^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nNow we must evaluate $S_i$ using the final assumption. $S_i$ is the $i$-th component of the vector $P^f c$:\n$$\nS_i = (P^f c)_i = \\sum_{j=1}^{n} P^f_{ij} c_j\n$$\nThe problem states that the background error covariance is locally diagonal at the observed location, such that off-diagonal elements involving index $i$ are negligible. This means we can make the approximation $P^f_{ij} \\approx 0$ for all $j \\neq i$.\nUnder this approximation, the summation for $S_i$ collapses to a single term:\n$$\nS_i \\approx P^f_{ii} c_i\n$$\nWe are given $P^f_{ii} = \\sigma_b^2$ and that the $i$-th component of the adjoint gradient $c$ is the Forecast Sensitivity to Observation (FSO), $g = c_i$.\nSubstituting these into the expression for $S_i$:\n$$\nS_i \\approx \\sigma_b^2 g\n$$\nFinally, substituting this result into the equation for $\\Delta \\mathrm{Var}(J)$:\n$$\n\\Delta \\mathrm{Var}(J) = \\frac{(\\sigma_b^2 g)^2}{\\sigma_b^2 + \\sigma_o^2} = \\frac{\\sigma_b^4 g^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nThis is the closed-form expression for the expected reduction in the forecast error variance of the metric $J$ due to the assimilation of the single observation.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{b}^{4} g^{2}}{\\sigma_{b}^{2} + \\sigma_{o}^{2}}}\n$$"
        }
    ]
}