{
    "hands_on_practices": [
        {
            "introduction": "Before embarking on a complex model tuning exercise, a critical first question is: can the available data actually constrain the parameters we want to tune? Some parameter combinations may have very similar effects on model output, making them difficult to distinguish, a property known as \"sloppiness.\" This practice introduces the Fisher Information Matrix ($I(\\theta)$) as a powerful tool to quantify parameter sensitivity and identifiability. By computing its eigenvalues for a simplified model, you will gain a concrete understanding of how to diagnose which parameter directions are well-constrained (\"stiff\") and which are poorly constrained (\"sloppy\") .",
            "id": "4065525",
            "problem": "Consider a simplified Single Column Model (SCM) used in numerical weather prediction and climate modeling. The SCM has two tunable parameters collected into a vector $ \\theta = (\\alpha, \\tau)^{\\top} $, where $ \\alpha $ is an entrainment efficiency (dimensionless) and $ \\tau $ is a convective adjustment timescale (in seconds). Let $ y(\\theta) \\in \\mathbb{R}^{3} $ denote a vector of three observable quantities (e.g., column-mean potential temperature at three distinct times) predicted by the model.\n\nAssume the following context-appropriate fundamental base:\n- Observational errors are zero-mean Gaussian with covariance $ R = \\sigma^{2} I $, where $ \\sigma = 0.2 $ (in the same physical units as $ y $) and $ I $ is the $ 3 \\times 3 $ identity matrix.\n- Near a nominal parameter value $ \\theta_{0} $, the model-observation map is linearizable as $ y(\\theta_{0} + \\delta \\theta) \\approx y(\\theta_{0}) + J \\, \\delta \\theta $, where $ J = \\partial y / \\partial \\theta \\big|_{\\theta_{0}} $ is the $ 3 \\times 2 $ sensitivity (Jacobian) matrix of observables with respect to parameters.\n- The Fisher information matrix $ I(\\theta_{0}) $ is defined from the expected curvature of the negative log-likelihood under these assumptions.\n\nAt $ \\theta = \\theta_{0} $, the sensitivities are measured to be\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n0.80 & 0.02 \\\\\n0.78 & 0.01 \\\\\n0.81 & 0.03\n\\end{pmatrix}.\n$$\n\nStarting from the base assumptions above, derive the Fisher information matrix $ I(\\theta_{0}) $ in terms of $ J $ and $ R $. Compute $ I(\\theta_{0}) $ numerically using the given data, then compute its two eigenvalues and interpret their magnitude in terms of parameter sloppiness (i.e., relative informativeness about directions in parameter space). Define a scalar sloppiness measure $ s $ as the ratio of the smaller eigenvalue to the larger eigenvalue, $ s = \\lambda_{\\min} / \\lambda_{\\max} $. Report the numerical value of $ s $.\n\nRound your final value of $ s $ to four significant figures. The final reported value is dimensionless; do not include any units in your final answer.",
            "solution": "We begin from the standard Gaussian likelihood for observational errors. If $ y^{\\text{obs}} \\in \\mathbb{R}^{3} $ are the observations and the model predicts $ y(\\theta) $, with errors $ \\varepsilon = y^{\\text{obs}} - y(\\theta) $ distributed as a multivariate Gaussian with covariance $ R $, then the negative log-likelihood is\n$$\n\\ell(\\theta)\n=\n\\frac{1}{2} \\left( y^{\\text{obs}} - y(\\theta) \\right)^{\\top} R^{-1} \\left( y^{\\text{obs}} - y(\\theta) \\right)\n+ \\text{const}.\n$$\nUnder linearization near a nominal parameter $ \\theta_{0} $, write $ y(\\theta_{0} + \\delta \\theta) \\approx y(\\theta_{0}) + J \\, \\delta \\theta $, where $ J = \\partial y / \\partial \\theta \\big|_{\\theta_{0}} $ is the $ 3 \\times 2 $ Jacobian. The second derivative (Hessian) of $ \\ell $ with respect to $ \\delta \\theta $ is, in expectation over the observational noise, given by\n$$\nI(\\theta_{0})\n=\nJ^{\\top} R^{-1} J,\n$$\nwhich is the Fisher information matrix. This result is a well-tested fact for Gaussian errors and linearized models in parameter estimation.\n\nWe are given $ R = \\sigma^{2} I $ with $ \\sigma = 0.2 $, so $ R = 0.04 \\, I $ and hence $ R^{-1} = 25 \\, I $. Therefore,\n$$\nI(\\theta_{0})\n=\nJ^{\\top} R^{-1} J\n=\n25 \\, J^{\\top} J.\n$$\nCompute $ J^{\\top} J $ from the given $ J $:\n$$\nJ\n=\n\\begin{pmatrix}\n0.80 & 0.02 \\\\\n0.78 & 0.01 \\\\\n0.81 & 0.03\n\\end{pmatrix},\n\\quad\nJ^{\\top} J\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{3} J_{i1}^{2} & \\sum_{i=1}^{3} J_{i1} J_{i2} \\\\\n\\sum_{i=1}^{3} J_{i1} J_{i2} & \\sum_{i=1}^{3} J_{i2}^{2}\n\\end{pmatrix}.\n$$\nCompute each element:\n- First column squared sum:\n$$\n\\sum_{i=1}^{3} J_{i1}^{2}\n=\n(0.80)^{2} + (0.78)^{2} + (0.81)^{2}\n=\n0.6400 + 0.6084 + 0.6561\n=\n1.9045.\n$$\n- Second column squared sum:\n$$\n\\sum_{i=1}^{3} J_{i2}^{2}\n=\n(0.02)^{2} + (0.01)^{2} + (0.03)^{2}\n=\n0.0004 + 0.0001 + 0.0009\n=\n0.0014.\n$$\n- Cross term sum:\n$$\n\\sum_{i=1}^{3} J_{i1} J_{i2}\n=\n(0.80)(0.02) + (0.78)(0.01) + (0.81)(0.03)\n=\n0.016 + 0.0078 + 0.0243\n=\n0.0481.\n$$\nThus,\n$$\nJ^{\\top} J\n=\n\\begin{pmatrix}\n1.9045 & 0.0481 \\\\\n0.0481 & 0.0014\n\\end{pmatrix},\n\\qquad\nI(\\theta_{0})\n=\n25 \\begin{pmatrix}\n1.9045 & 0.0481 \\\\\n0.0481 & 0.0014\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n47.6125 & 1.2025 \\\\\n1.2025 & 0.035\n\\end{pmatrix}.\n$$\n\nSince $ I(\\theta_{0}) $ is symmetric positive definite, its eigenvalues $ \\lambda_{\\max} $ and $ \\lambda_{\\min} $ are real and positive. For a $ 2 \\times 2 $ symmetric matrix\n$$\nM\n=\n\\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix},\n$$\nthe eigenvalues are\n$$\n\\lambda_{\\pm}\n=\n\\frac{a + c \\pm \\sqrt{(a + c)^{2} - 4(ac - b^{2})}}{2}.\n$$\nHere, $ a = 47.6125 $, $ b = 1.2025 $, and $ c = 0.035 $. Let $ t = a + c $ and $ d = ac - b^{2} $. Compute\n$$\nt\n=\n47.6125 + 0.035\n=\n47.6475,\n\\qquad\nac\n=\n(47.6125)(0.035)\n=\n1.6664375,\n\\qquad\nb^{2}\n=\n(1.2025)^{2}\n=\n1.44600625,\n$$\nso\n$$\nd\n=\nac - b^{2}\n=\n1.6664375 - 1.44600625\n=\n0.22043125.\n$$\nThen\n$$\n\\Delta\n=\n\\sqrt{t^{2} - 4d}\n=\n\\sqrt{(47.6475)^{2} - 4(0.22043125)}.\n$$\nCompute $ (47.6475)^{2} $ exactly via expansion:\n$$\n(47.6475)^{2}\n=\n(47.6 + 0.0475)^{2}\n=\n(47.6)^{2} + 2(47.6)(0.0475) + (0.0475)^{2}\n=\n2265.76 + 4.522 + 0.00225625\n=\n2270.28425625.\n$$\nTherefore,\n$$\nt^{2} - 4d\n=\n2270.28425625 - 0.881725\n=\n2269.40253125,\n\\quad\n\\Delta\n=\n\\sqrt{2269.40253125}\n\\approx\n47.63824644,\n$$\nand the eigenvalues are\n$$\n\\lambda_{\\max}\n=\n\\frac{t + \\Delta}{2}\n=\n\\frac{47.6475 + 47.63824644}{2}\n\\approx\n47.64287322,\n$$\n$$\n\\lambda_{\\min}\n=\n\\frac{t - \\Delta}{2}\n=\n\\frac{47.6475 - 47.63824644}{2}\n\\approx\n0.00462678.\n$$\n\nThe sloppiness measure $ s $ is defined as $ s = \\lambda_{\\min} / \\lambda_{\\max} $. Using the values above,\n$$\ns\n=\n\\frac{0.00462678}{47.64287322}\n\\approx\n9.711 \\times 10^{-5}.\n$$\nRounded to four significant figures, the final value is $ 9.711 \\times 10^{-5} $.\n\nInterpretation: The Fisher information matrix $ I(\\theta_{0}) $ encodes how strongly the data constrain directions in parameter space. A very small ratio $ s = \\lambda_{\\min} / \\lambda_{\\max} $ indicates strong anisotropy: one parameter combination (the eigenvector corresponding to $ \\lambda_{\\max} $) is tightly constrained by the data, while the orthogonal combination (corresponding to $ \\lambda_{\\min} $) is extremely weakly constrained. In this SCM example, the sensitivities of the observables to $ \\tau $ are much smaller than to $ \\alpha $ (and nearly collinear with the $ \\alpha $ direction), yielding $ s \\approx 9.711 \\times 10^{-5} $, which is characteristic of a sloppy parameter direction that would be difficult to estimate reliably without additional information or more informative observations.",
            "answer": "$$\\boxed{9.711 \\times 10^{-5}}$$"
        },
        {
            "introduction": "Once parameter identifiability is established, the next step is to employ a numerical algorithm to find the optimal parameter values. This exercise guides you through the derivation of the Levenberg-Marquardt (LM) algorithm, a cornerstone of nonlinear least-squares optimization. By understanding how the LM method dynamically blends the stability of steepest descent with the speed of the Gauss-Newton method, you will appreciate the mathematical foundation of one of the most effective and widely used techniques for tuning complex scientific models .",
            "id": "4065506",
            "problem": "Consider tuning a vector of uncertain subgrid process parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$ in a global climate model used for Numerical Weather Prediction (NWP) and long-term climate projections. Let an observational operator $H(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{m}$ map parameters to a set of $m$ diagnostic quantities (for example, regional mean precipitation, top-of-atmosphere radiation, and near-surface temperature) that are compared to observationally constrained targets $\\mathbf{y} \\in \\mathbb{R}^{m}$. Define the residual vector $r(\\boldsymbol{\\theta}) = H(\\boldsymbol{\\theta}) - \\mathbf{y}$, and the least-squares objective $\\Phi(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|r(\\boldsymbol{\\theta})\\|^{2}$, where $\\|\\cdot\\|$ denotes the Euclidean norm. At the current iterate $\\boldsymbol{\\theta}_{k}$, denote $r = r(\\boldsymbol{\\theta}_{k})$ and the Jacobian $J' = \\frac{\\partial r}{\\partial \\boldsymbol{\\theta}}\\big|_{\\boldsymbol{\\theta}_{k}} \\in \\mathbb{R}^{m \\times p}$.\n\nTo obtain a stable parameter update $\\delta \\in \\mathbb{R}^{p}$ that balances curvature information with robustness, you decide to use the Levenberg-Marquardt method (LM), also known as the damped Gauss-Newton method, which augments the local quadratic model with an isotropic regularization term controlled by a damping coefficient $\\lambda > 0$.\n\nStarting from the fundamental definitions of least-squares minimization and first-order Taylor expansion of the residual, derive the stationarity condition for the damped local model and obtain the explicit closed-form expression for the LM update $\\delta$ in terms of $J'$, $r$, and $\\lambda$. Your derivation should begin from the linearized residual around $\\boldsymbol{\\theta}_{k}$ and proceed by minimizing the damped quadratic approximation to the objective. Provide your final answer as a single closed-form analytical expression for $\\delta$. No numerical evaluation is required. If you introduce any additional symbols, clearly define them. Express your final answer without units.",
            "solution": "The goal is to find a parameter update $\\delta$ that minimizes the objective function $\\Phi(\\boldsymbol{\\theta})$. We aim to determine the update $\\delta$ such that the next iterate $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_{k} + \\delta$ reduces the value of $\\Phi$. The Levenberg-Marquardt method achieves this by minimizing a local, damped quadratic model of the objective function.\n\nThe objective function is $\\Phi(\\boldsymbol{\\theta}) = \\frac{1}{2} \\|r(\\boldsymbol{\\theta})\\|^{2}$. We want to analyze its value at the new point $\\boldsymbol{\\theta}_{k} + \\delta$. To do this, we first construct a linear approximation of the residual vector $r(\\boldsymbol{\\theta})$ around the current iterate $\\boldsymbol{\\theta}_{k}$ using a first-order Taylor series expansion:\n$$\nr(\\boldsymbol{\\theta}_{k} + \\delta) \\approx r(\\boldsymbol{\\theta}_{k}) + \\left.\\frac{\\partial r}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta}_{k}} \\delta\n$$\nUsing the notation provided in the problem statement, where $r = r(\\boldsymbol{\\theta}_{k})$ and $J' = \\frac{\\partial r}{\\partial \\boldsymbol{\\theta}}\\big|_{\\boldsymbol{\\theta}_{k}}$, this approximation becomes:\n$$\nr(\\boldsymbol{\\theta}_{k} + \\delta) \\approx r + J' \\delta\n$$\nSubstituting this linear approximation of the residual into the objective function $\\Phi$ gives a local quadratic model, which we denote as $\\tilde{\\Phi}(\\delta)$:\n$$\n\\Phi(\\boldsymbol{\\theta}_{k} + \\delta) \\approx \\tilde{\\Phi}(\\delta) = \\frac{1}{2} \\|r + J' \\delta\\|^{2}\n$$\nThe pure Gauss-Newton method seeks to minimize $\\tilde{\\Phi}(\\delta)$. However, this can be unstable if the Jacobian $J'$ is ill-conditioned or rank-deficient. The Levenberg-Marquardt method introduces a damping (or regularization) term, $\\frac{1}{2}\\lambda\\|\\delta\\|^2$, to stabilize the problem. This term penalizes large update steps, keeping the new iterate $\\boldsymbol{\\theta}_{k} + \\delta$ within a region where the linear approximation of the residual is more likely to be valid.\n\nThe damped objective function, which we will call $L(\\delta)$, is thus defined as:\n$$\nL(\\delta) = \\frac{1}{2} \\|r + J' \\delta\\|^{2} + \\frac{1}{2} \\lambda \\|\\delta\\|^{2}\n$$\nwhere $\\lambda > 0$ is the damping coefficient. Our goal is to find the update vector $\\delta$ that minimizes $L(\\delta)$. To do this, we rewrite the norms in terms of vector transposes:\n$$\nL(\\delta) = \\frac{1}{2} (r + J' \\delta)^{T} (r + J' \\delta) + \\frac{1}{2} \\lambda \\delta^{T} \\delta\n$$\nExpanding the first term:\n$$\n(r + J' \\delta)^{T} (r + J' \\delta) = (r^{T} + \\delta^{T} (J')^{T}) (r + J' \\delta) = r^{T}r + r^{T}J'\\delta + \\delta^{T}(J')^{T}r + \\delta^{T}(J')^{T}J'\\delta\n$$\nSince $r^{T}J'\\delta$ is a scalar, it is equal to its own transpose, $(\\delta^{T}(J')^{T}r)$. Therefore, $r^{T}J'\\delta + \\delta^{T}(J')^{T}r = 2r^{T}J'\\delta$.\nThe objective function becomes:\n$$\nL(\\delta) = \\frac{1}{2} (r^{T}r + 2 r^{T}J'\\delta + \\delta^{T}(J')^{T}J'\\delta) + \\frac{1}{2} \\lambda \\delta^{T} I \\delta\n$$\nwhere $I$ is the identity matrix of size $p \\times p$.\n\nTo find the minimum of $L(\\delta)$, we compute its gradient with respect to $\\delta$ and set it to the zero vector.\n$$\n\\nabla_{\\delta} L(\\delta) = \\frac{\\partial}{\\partial \\delta} \\left[ \\frac{1}{2} r^{T}r + r^{T}J'\\delta + \\frac{1}{2} \\delta^{T}(J')^{T}J'\\delta + \\frac{1}{2} \\lambda \\delta^{T} I \\delta \\right]\n$$\nUsing standard matrix calculus identities, specifically that $\\nabla_x (a^T x) = a$ and $\\nabla_x (\\frac{1}{2} x^T A x) = Ax$ for a symmetric matrix $A$:\n- The term $\\frac{1}{2} r^{T}r$ is constant with respect to $\\delta$, so its gradient is $\\mathbf{0}$.\n- The gradient of $r^{T}J'\\delta$ is $(r^{T}J')^{T} = (J')^{T}r$.\n- The matrix $(J')^{T}J'$ is symmetric, so the gradient of $\\frac{1}{2} \\delta^{T}(J')^{T}J'\\delta$ is $(J')^{T}J'\\delta$.\n- The matrix $\\lambda I$ is symmetric, so the gradient of $\\frac{1}{2} \\lambda \\delta^{T} I \\delta$ is $\\lambda I \\delta$.\n\nCombining these results, the gradient of $L(\\delta)$ is:\n$$\n\\nabla_{\\delta} L(\\delta) = (J')^{T}r + (J')^{T}J'\\delta + \\lambda I \\delta\n$$\nThe stationarity condition is found by setting the gradient to the zero vector, $\\nabla_{\\delta} L(\\delta) = \\mathbf{0}$:\n$$\n(J')^{T}J'\\delta + \\lambda I \\delta = -(J')^{T}r\n$$\nFactoring out $\\delta$ on the left-hand side gives:\n$$\n((J')^{T}J' + \\lambda I) \\delta = -(J')^{T}r\n$$\nThis is a system of linear equations for the update vector $\\delta$. The matrix $(J')^{T}J'$ is the \"approximate Hessian\" from the Gauss-Newton method; it is symmetric and positive semi-definite. Since $\\lambda > 0$, the matrix $(J')^{T}J' + \\lambda I$ is symmetric and positive definite, which guarantees that it is invertible. We can therefore solve for $\\delta$ by pre-multiplying both sides by the inverse of this matrix:\n$$\n\\delta = - ((J')^{T}J' + \\lambda I)^{-1} (J')^{T}r\n$$\nThis expression provides the closed-form analytical solution for the Levenberg-Marquardt update step $\\delta$ in terms of the Jacobian $J'$, the residual vector $r$, and the damping parameter $\\lambda$.",
            "answer": "$$\n\\boxed{-((J')^{T} J' + \\lambda I)^{-1} (J')^{T} r}\n$$"
        },
        {
            "introduction": "The efficiency of modern, gradient-based optimization methods hinges on the accurate computation of model sensitivities, which are typically provided by a Tangent-Linear (TL) model. An error in the TL model can silently derail the entire tuning process, leading to failed convergence or incorrect results. This final hands-on practice moves from theory to code, guiding you through the implementation of a \"tangent-linear test.\" This test is a fundamental and non-negotiable verification step in any professional workflow, ensuring that your core computational tools are trustworthy before you deploy them .",
            "id": "4065465",
            "problem": "Consider the verification of a Tangent-Linear (TL) model used in gradient-based parameter optimization for Numerical Weather Prediction (NWP) and climate modeling. Let a simplified single-column forward model map a state vector $x \\in \\mathbb{R}^3$ with components $x = (T, q, u)$ to its next-time-step state $M(x)$ via a forward Euler discretization of nonlinear physical tendencies. The components $T$, $q$, and $u$ denote temperature in Kelvin, specific humidity in kilograms per kilogram, and wind speed in meters per second, respectively. The time step is $\\Delta t$ in seconds. The forward model is defined as\n$$\nM(x) = x + \\Delta t \\, F(x),\n$$\nwhere $F(x) = (F_T(x), F_q(x), F_u(x))$ are nonlinear tendencies:\n- Radiative and latent heating temperature tendency:\n$$\nF_T(x) = -\\lambda \\left( T^4 - T_\\mathrm{eq}^4 \\right) + \\eta \\, C(q, T),\n$$\n- Condensation sink for specific humidity:\n$$\nF_q(x) = - C(q, T),\n$$\n- Quadratic-drag-like wind tendency:\n$$\nF_u(x) = -\\mu \\, u^3.\n$$\nThe condensation rate $C(q, T)$ is a smooth approximation to the positive part of supersaturation, defined by\n$$\nC(q, T) = \\kappa \\, \\frac{\\mathrm{softplus}\\left( s \\right)}{\\beta}, \\quad s = \\beta \\left( q - q_\\mathrm{sat}(T) \\right),\n$$\nwith\n$$\n\\mathrm{softplus}(s) = \\log\\left( 1 + e^{s} \\right), \\quad \\sigma(s) = \\frac{1}{1 + e^{-s}},\n$$\nand a simplified saturation specific humidity\n$$\nq_\\mathrm{sat}(T) = q_0 \\, \\exp\\left( c \\, (T - T_0) \\right).\n$$\nThe constants are given (with units where applicable) by\n$$\n\\Delta t = 600 \\, \\mathrm{s}, \\quad \\lambda = 10^{-12} \\, \\mathrm{K}^{-3}\\mathrm{s}^{-1}, \\quad T_\\mathrm{eq} = 270 \\, \\mathrm{K}, \\quad \\eta = 2000 \\, \\mathrm{K} \\, \\mathrm{(kg/kg)}^{-1},\n$$\n$$\n\\kappa = 10^{-4} \\, \\mathrm{s}^{-1}, \\quad \\beta = 200, \\quad q_0 = 0.01 \\, \\mathrm{kg/kg}, \\quad T_0 = 280 \\, \\mathrm{K}, \\quad c = 0.07 \\, \\mathrm{K}^{-1}, \\quad \\mu = 10^{-6}.\n$$\nThe tangent-linear operator $L(x)$ of $M(x)$ applied to a perturbation $p \\in \\mathbb{R}^3$ is the FrÃ©chet derivative evaluated at $x$ acting on $p$:\n$$\nL(x)p = p + \\Delta t \\, J_F(x) \\, p,\n$$\nwhere $J_F(x)$ is the Jacobian of $F(x)$ at $x$. Using the chain rule, the partial derivatives needed for $J_F(x)$ are\n$$\n\\frac{\\partial C}{\\partial q} = \\kappa \\, \\sigma(s), \\quad \\frac{\\partial C}{\\partial T} = - \\kappa \\, \\sigma(s) \\, \\frac{\\partial q_\\mathrm{sat}}{\\partial T}, \\quad \\frac{\\partial q_\\mathrm{sat}}{\\partial T} = c \\, q_\\mathrm{sat}(T).\n$$\nTherefore,\n$$\n\\frac{\\partial F_T}{\\partial T} = -\\lambda \\cdot 4 T^3 + \\eta \\cdot \\frac{\\partial C}{\\partial T}, \\quad \\frac{\\partial F_T}{\\partial q} = \\eta \\cdot \\frac{\\partial C}{\\partial q}, \\quad \\frac{\\partial F_T}{\\partial u} = 0,\n$$\n$$\n\\frac{\\partial F_q}{\\partial T} = - \\frac{\\partial C}{\\partial T}, \\quad \\frac{\\partial F_q}{\\partial q} = - \\frac{\\partial C}{\\partial q}, \\quad \\frac{\\partial F_q}{\\partial u} = 0,\n$$\n$$\n\\frac{\\partial F_u}{\\partial u} = -3 \\mu u^2, \\quad \\frac{\\partial F_u}{\\partial T} = 0, \\quad \\frac{\\partial F_u}{\\partial q} = 0.\n$$\nThe tangent-linear test evaluates the consistency between finite differences of the nonlinear model and the tangent-linear model. For a given base state $x$, perturbation direction $p$, and a set of magnitudes $\\epsilon \\in \\mathbb{R}^+$, define\n$$\n\\delta(\\epsilon) = M(x + \\epsilon p) - M(x), \\quad \\mathrm{TL}(\\epsilon) = \\epsilon \\, L(x) p.\n$$\nCompute the ratio\n$$\nr(\\epsilon) = \\frac{\\left\\| \\delta(\\epsilon) \\right\\|_2}{\\left\\| \\mathrm{TL}(\\epsilon) \\right\\|_2}\n$$\nand the residual norm\n$$\nD(\\epsilon) = \\left\\| \\delta(\\epsilon) - \\mathrm{TL}(\\epsilon) \\right\\|_2.\n$$\nBy first-order differentiability and Taylor expansion, one expects $r(\\epsilon) \\to 1$ as $\\epsilon \\to 0$ and $D(\\epsilon) = \\mathcal{O}(\\epsilon^2)$. This behavior underpins reliable gradient computation used in model tuning and parameter optimization.\n\nYour task is to implement a complete program that:\n- Computes $M(x)$, $L(x)p$, $r(\\epsilon)$, and $D(\\epsilon)$ for a given test suite.\n- For each test case, returns the maximum absolute deviation from unity among the ratios, $\\max_{\\epsilon} \\left| r(\\epsilon) - 1 \\right|$, and the least-squares slope $s$ of $\\log D(\\epsilon)$ versus $\\log \\epsilon$ over the provided $\\epsilon$ values, which ideally should be close to $2$.\n\nUse the following test suite of base states $x = (T, q, u)$ and perturbation vectors $p = (p_T, p_q, p_u)$ with all physical units explicitly stated:\n- Case $1$: $x = (280 \\, \\mathrm{K}, 0.012 \\, \\mathrm{kg/kg}, 5 \\, \\mathrm{m/s})$, $p = (0.5 \\, \\mathrm{K}, -0.001 \\, \\mathrm{kg/kg}, 0.5 \\, \\mathrm{m/s})$.\n- Case $2$: $x = (260 \\, \\mathrm{K}, 0.002 \\, \\mathrm{kg/kg}, 15 \\, \\mathrm{m/s})$, $p = (-0.5 \\, \\mathrm{K}, 0.0005 \\, \\mathrm{kg/kg}, -1.0 \\, \\mathrm{m/s})$.\n- Case $3$: $x = (295 \\, \\mathrm{K}, 0.03 \\, \\mathrm{kg/kg}, 2 \\, \\mathrm{m/s})$, $p = (0.1 \\, \\mathrm{K}, -0.005 \\, \\mathrm{kg/kg}, 0.2 \\, \\mathrm{m/s})$.\n- Case $4$: $x = (270 \\, \\mathrm{K}, 0.01 \\, \\mathrm{kg/kg}, 0.1 \\, \\mathrm{m/s})$, $p = (0.2 \\, \\mathrm{K}, 0.001 \\, \\mathrm{kg/kg}, -0.05 \\, \\mathrm{m/s})$.\n\nUse the set of perturbation magnitudes\n$$\n\\epsilon \\in \\{ 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \\}.\n$$\n\nAngle units are not applicable. All physical quantities must be handled in the units specified above. The final outputs for each test case must be numeric floats.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list $[ \\max_{\\epsilon} | r(\\epsilon) - 1 |, s ]$ in this order. For example, the output should look like $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$, where each $a_i$ and $b_i$ are floats for case $i$.",
            "solution": "The problem requires the verification of a tangent-linear (TL) model derived from a simplified nonlinear forward model $M(x)$ used in numerical weather prediction. The verification is based on the tangent-linear test, which assesses the consistency between the TL model and finite differences of the full nonlinear model. The expected outcome is that for small perturbations, the TL model provides a first-order accurate approximation of the nonlinear model's response.\n\nThe solution is structured as follows:\n1.  Implementation of the nonlinear forward model $M(x)$.\n2.  Derivation and implementation of the Jacobian $J_F(x)$ of the tendency vector $F(x)$ and the tangent-linear operator $L(x)$.\n3.  Execution of the tangent-linear test for each specified case by computing the metrics $r(\\epsilon)$ and $D(\\epsilon)$ over a range of perturbation magnitudes $\\epsilon$.\n4.  Calculation of the final summary statistics: the maximum deviation of the ratio $r(\\epsilon)$ from unity, and the slope of the convergence plot for the residual norm $D(\\epsilon)$, which is expected to be close to $2$.\n\nAll constants are used in their specified SI units. The state vector is $x = (T, q, u)$, where $T$ is temperature in Kelvin, $q$ is specific humidity in kilograms per kilogram, and $u$ is wind speed in meters per second.\n\nThe following code implements this entire procedure.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the tangent-linear model verification problem.\n    \"\"\"\n    \n    # Define physical constants\n    DT = 600.0\n    LAMBDA = 1e-12\n    T_EQ = 270.0\n    ETA = 2000.0\n    KAPPA = 1e-4\n    BETA = 200.0\n    Q0 = 0.01\n    T0 = 280.0\n    C_CONST = 0.07  # Named to avoid conflict with condensation function C\n    MU = 1e-6\n    \n    # Helper functions for the model physics\n    def softplus(s):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        return np.logaddexp(0, s)\n\n    def sigmoid(s):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return np.where(s >= 0,\n                      1 / (1 + np.exp(-s)),\n                      np.exp(s) / (1 + np.exp(s)))\n\n    def q_sat(T):\n        \"\"\"Saturation specific humidity.\"\"\"\n        return Q0 * np.exp(C_CONST * (T - T0))\n\n    def condensation_rate(q, T):\n        \"\"\"Condensation rate C(q, T).\"\"\"\n        s = BETA * (q - q_sat(T))\n        return KAPPA * softplus(s) / BETA\n\n    # Nonlinear model functions\n    def F(x):\n        \"\"\"Nonlinear tendency vector F(x).\"\"\"\n        T, q, u = x\n        C = condensation_rate(q, T)\n        F_T = -LAMBDA * (T**4 - T_EQ**4) + ETA * C\n        F_q = -C\n        F_u = -MU * u**3\n        return np.array([F_T, F_q, F_u])\n\n    def M(x):\n        \"\"\"Nonlinear forward model M(x).\"\"\"\n        return x + DT * F(x)\n\n    # Tangent-linear model functions\n    def J_F(x):\n        \"\"\"Jacobian of the tendency vector, J_F(x).\"\"\"\n        T, q, u = x\n        \n        # Derivatives for condensation rate C\n        s = BETA * (q - q_sat(T))\n        d_q_sat_dT = C_CONST * q_sat(T)\n        sig_s = sigmoid(s)\n        \n        dC_dq = KAPPA * sig_s\n        dC_dT = -KAPPA * sig_s * d_q_sat_dT\n        \n        # Jacobian matrix elements\n        J_11 = -4 * LAMBDA * T**3 + ETA * dC_dT\n        J_12 = ETA * dC_dq\n        J_13 = 0.0\n        \n        J_21 = -dC_dT\n        J_22 = -dC_dq\n        J_23 = 0.0\n        \n        J_31 = 0.0\n        J_32 = 0.0\n        J_33 = -3 * MU * u**2\n        \n        return np.array([[J_11, J_12, J_13],\n                         [J_21, J_22, J_23],\n                         [J_31, J_32, J_33]])\n\n    def L_p(x, p):\n        \"\"\"Action of the tangent-linear operator on perturbation p, L(x)p.\"\"\"\n        jacobian = J_F(x)\n        return p + DT * np.dot(jacobian, p)\n\n    # Main testing logic\n    def run_test_case(x, p):\n        \"\"\"Runs the TL test for a given state x and perturbation p.\"\"\"\n        epsilons = np.array([1e-1, 1e-2, 1e-3, 1e-4])\n        r_values = []\n        d_values = []\n\n        M_x = M(x)\n        L_x_p = L_p(x, p)\n\n        for eps in epsilons:\n            # Finite difference of nonlinear model\n            delta_eps = M(x + eps * p) - M_x\n            \n            # Scaled tangent-linear model output\n            tl_eps = eps * L_x_p\n            \n            norm_delta = np.linalg.norm(delta_eps)\n            norm_tl = np.linalg.norm(tl_eps)\n            \n            # Ratio r(epsilon)\n            r = norm_delta / norm_tl if norm_tl != 0 else np.inf\n            r_values.append(r)\n            \n            # Residual norm D(epsilon)\n            D = np.linalg.norm(delta_eps - tl_eps)\n            d_values.append(D)\n            \n        # 1. Maximum absolute deviation of r from 1\n        max_dev = np.max(np.abs(np.array(r_values) - 1))\n        \n        # 2. Least-squares slope of log D vs log eps\n        log_eps = np.log(epsilons)\n        log_d = np.log(np.array(d_values))\n        \n        # np.polyfit finds coefficients of a polynomial fit. For degree 1,\n        # the first coefficient is the slope.\n        slope, _ = np.polyfit(log_eps, log_d, 1)\n        \n        return [max_dev, slope]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'x': np.array([280.0, 0.012, 5.0]), 'p': np.array([0.5, -0.001, 0.5])},\n        {'x': np.array([260.0, 0.002, 15.0]), 'p': np.array([-0.5, 0.0005, -1.0])},\n        {'x': np.array([295.0, 0.03, 2.0]), 'p': np.array([0.1, -0.005, 0.2])},\n        {'x': np.array([270.0, 0.01, 0.1]), 'p': np.array([0.2, 0.001, -0.05])},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case['x'], case['p'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    inner_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_string = f\"[{','.join(inner_strings)}]\"\n    print(final_string)\n\n# This function would be called in a real execution environment.\n# solve()\n```",
            "answer": "[[0.03463973549749171,1.9961622340625945],[0.0485906371510444,1.9961816752003844],[0.07590823377755123,1.9960241088424565],[0.01511210874609804,1.9989912061245014]]"
        }
    ]
}