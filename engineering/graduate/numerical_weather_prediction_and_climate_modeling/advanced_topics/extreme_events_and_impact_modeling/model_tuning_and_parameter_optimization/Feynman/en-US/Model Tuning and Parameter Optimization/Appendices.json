{
    "hands_on_practices": [
        {
            "introduction": "At the heart of many automated tuning procedures lies a non-linear least-squares optimization algorithm. The Levenberg-Marquardt (LM) method is a cornerstone technique, elegantly blending the speed of the Gauss-Newton method with the stability of gradient descent. This practice challenges you to derive the famous LM update rule, providing a crucial look under the hood of a powerful tool used to minimize the mismatch between model output and observations. ",
            "id": "4065506",
            "problem": "Consider tuning a vector of uncertain subgrid process parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$ in a global climate model used for Numerical Weather Prediction (NWP) and long-term climate projections. Let an observational operator $H(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{m}$ map parameters to a set of $m$ diagnostic quantities (for example, regional mean precipitation, top-of-atmosphere radiation, and near-surface temperature) that are compared to observationally constrained targets $\\mathbf{y} \\in \\mathbb{R}^{m}$. Define the residual vector $r(\\boldsymbol{\\theta}) = H(\\boldsymbol{\\theta}) - \\mathbf{y}$, and the least-squares objective $\\Phi(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|r(\\boldsymbol{\\theta})\\|^{2}$, where $\\|\\cdot\\|$ denotes the Euclidean norm. At the current iterate $\\boldsymbol{\\theta}_{k}$, denote $r = r(\\boldsymbol{\\theta}_{k})$ and the Jacobian $J' = \\frac{\\partial r}{\\partial \\boldsymbol{\\theta}}\\big|_{\\boldsymbol{\\theta}_{k}} \\in \\mathbb{R}^{m \\times p}$.\n\nTo obtain a stable parameter update $\\delta \\in \\mathbb{R}^{p}$ that balances curvature information with robustness, you decide to use the Levenberg-Marquardt method (LM), also known as the damped Gauss-Newton method, which augments the local quadratic model with an isotropic regularization term controlled by a damping coefficient $\\lambda > 0$.\n\nStarting from the fundamental definitions of least-squares minimization and first-order Taylor expansion of the residual, derive the stationarity condition for the damped local model and obtain the explicit closed-form expression for the LM update $\\delta$ in terms of $J'$, $r$, and $\\lambda$. Your derivation should begin from the linearized residual around $\\boldsymbol{\\theta}_{k}$ and proceed by minimizing the damped quadratic approximation to the objective. Provide your final answer as a single closed-form analytical expression for $\\delta$. No numerical evaluation is required. If you introduce any additional symbols, clearly define them. Express your final answer without units.",
            "solution": "### Derivation of the Levenberg-Marquardt Update\nThe goal is to find a parameter update $\\delta$ that minimizes the objective function $\\Phi(\\boldsymbol{\\theta})$. We aim to determine the update $\\delta$ such that the next iterate $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_{k} + \\delta$ reduces the value of $\\Phi$. The Levenberg-Marquardt method achieves this by minimizing a local, damped quadratic model of the objective function.\n\nThe objective function is $\\Phi(\\boldsymbol{\\theta}) = \\frac{1}{2} \\|r(\\boldsymbol{\\theta})\\|^{2}$. We want to analyze its value at the new point $\\boldsymbol{\\theta}_{k} + \\delta$. To do this, we first construct a linear approximation of the residual vector $r(\\boldsymbol{\\theta})$ around the current iterate $\\boldsymbol{\\theta}_{k}$ using a first-order Taylor series expansion:\n$$\nr(\\boldsymbol{\\theta}_{k} + \\delta) \\approx r(\\boldsymbol{\\theta}_{k}) + \\left.\\frac{\\partial r}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta}_{k}} \\delta\n$$\nUsing the notation provided in the problem statement, where $r = r(\\boldsymbol{\\theta}_{k})$ and $J' = \\frac{\\partial r}{\\partial \\boldsymbol{\\theta}}\\big|_{\\boldsymbol{\\theta}_{k}}$, this approximation becomes:\n$$\nr(\\boldsymbol{\\theta}_{k} + \\delta) \\approx r + J' \\delta\n$$\nSubstituting this linear approximation of the residual into the objective function $\\Phi$ gives a local quadratic model, which we denote as $\\tilde{\\Phi}(\\delta)$:\n$$\n\\Phi(\\boldsymbol{\\theta}_{k} + \\delta) \\approx \\tilde{\\Phi}(\\delta) = \\frac{1}{2} \\|r + J' \\delta\\|^{2}\n$$\nThe pure Gauss-Newton method seeks to minimize $\\tilde{\\Phi}(\\delta)$. However, this can be unstable if the Jacobian $J'$ is ill-conditioned or rank-deficient. The Levenberg-Marquardt method introduces a damping (or regularization) term, $\\frac{1}{2}\\lambda\\|\\delta\\|^2$, to stabilize the problem. This term penalizes large update steps, keeping the new iterate $\\boldsymbol{\\theta}_{k} + \\delta$ within a region where the linear approximation of the residual is more likely to be valid.\n\nThe damped objective function, which we will call $L(\\delta)$, is thus defined as:\n$$\nL(\\delta) = \\frac{1}{2} \\|r + J' \\delta\\|^{2} + \\frac{1}{2} \\lambda \\|\\delta\\|^{2}\n$$\nwhere $\\lambda > 0$ is the damping coefficient. Our goal is to find the update vector $\\delta$ that minimizes $L(\\delta)$. To do this, we rewrite the norms in terms of vector transposes:\n$$\nL(\\delta) = \\frac{1}{2} (r + J' \\delta)^{T} (r + J' \\delta) + \\frac{1}{2} \\lambda \\delta^{T} \\delta\n$$\nExpanding the first term:\n$$\n(r + J' \\delta)^{T} (r + J' \\delta) = (r^{T} + \\delta^{T} (J')^{T}) (r + J' \\delta) = r^{T}r + r^{T}J'\\delta + \\delta^{T}(J')^{T}r + \\delta^{T}(J')^{T}J'\\delta\n$$\nSince $r^{T}J'\\delta$ is a scalar, it is equal to its own transpose, $(\\delta^{T}(J')^{T}r)$. Therefore, $r^{T}J'\\delta + \\delta^{T}(J')^{T}r = 2r^{T}J'\\delta$.\nThe objective function becomes:\n$$\nL(\\delta) = \\frac{1}{2} (r^{T}r + 2 r^{T}J'\\delta + \\delta^{T}(J')^{T}J'\\delta) + \\frac{1}{2} \\lambda \\delta^{T} I \\delta\n$$\nwhere $I$ is the identity matrix of size $p \\times p$.\n\nTo find the minimum of $L(\\delta)$, we compute its gradient with respect to $\\delta$ and set it to the zero vector.\n$$\n\\nabla_{\\delta} L(\\delta) = \\frac{\\partial}{\\partial \\delta} \\left[ \\frac{1}{2} r^{T}r + r^{T}J'\\delta + \\frac{1}{2} \\delta^{T}(J')^{T}J'\\delta + \\frac{1}{2} \\lambda \\delta^{T} I \\delta \\right]\n$$\nUsing standard matrix calculus identities, specifically that $\\nabla_x (a^T x) = a$ and $\\nabla_x (\\frac{1}{2} x^T A x) = Ax$ for a symmetric matrix $A$:\n- The term $\\frac{1}{2} r^{T}r$ is constant with respect to $\\delta$, so its gradient is $\\mathbf{0}$.\n- The gradient of $r^{T}J'\\delta$ is $(r^{T}J')^{T} = (J')^{T}r$.\n- The matrix $(J')^{T}J'$ is symmetric, so the gradient of $\\frac{1}{2} \\delta^{T}(J')^{T}J'\\delta$ is $(J')^{T}J'\\delta$.\n- The matrix $\\lambda I$ is symmetric, so the gradient of $\\frac{1}{2} \\lambda \\delta^{T} I \\delta$ is $\\lambda I \\delta$.\n\nCombining these results, the gradient of $L(\\delta)$ is:\n$$\n\\nabla_{\\delta} L(\\delta) = (J')^{T}r + (J')^{T}J'\\delta + \\lambda I \\delta\n$$\nThe stationarity condition is found by setting the gradient to the zero vector, $\\nabla_{\\delta} L(\\delta) = \\mathbf{0}$:\n$$\n(J')^{T}J'\\delta + \\lambda I \\delta = -(J')^{T}r\n$$\nFactoring out $\\delta$ on the left-hand side gives:\n$$\n((J')^{T}J' + \\lambda I) \\delta = -(J')^{T}r\n$$\nThis is a system of linear equations for the update vector $\\delta$. The matrix $(J')^{T}J'$ is the \"approximate Hessian\" from the Gauss-Newton method; it is symmetric and positive semi-definite. Since $\\lambda > 0$, the matrix $(J')^{T}J' + \\lambda I$ is symmetric and positive definite, which guarantees that it is invertible. We can therefore solve for $\\delta$ by pre-multiplying both sides by the inverse of this matrix:\n$$\n\\delta = - ((J')^{T}J' + \\lambda I)^{-1} (J')^{T}r\n$$\nThis expression provides the closed-form analytical solution for the Levenberg-Marquardt update step $\\delta$ in terms of the Jacobian $J'$, the residual vector $r$, and the damping parameter $\\lambda$.",
            "answer": "$$\n\\boxed{-((J')^{T} J' + \\lambda I)^{-1} (J')^{T} r}\n$$"
        },
        {
            "introduction": "An optimizer can only succeed if the parameters it is tuning are 'identifiable'—that is, if their effects on the model's output are distinct enough to be distinguished by observations. This exercise explores the critical issue of parameter non-identifiability, which arises when the sensitivities of model outputs to different parameters are collinear. By analyzing the null space of the sensitivity matrix, you will not only diagnose this problem but also learn to construct a reparameterization that isolates the 'sloppy,' or unconstrained, directions in the parameter space. ",
            "id": "4065497",
            "problem": "Consider a tuning task in a simplified column model used in numerical weather prediction and climate modeling, where three scalar parameters $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2}, \\theta_{3})^{\\top}$ control subgrid process parameterizations that affect a vector of $m$ observable quantities (e.g., shortwave radiative fluxes at multiple levels or times). Under small perturbations about a background state, assume a linearized relation between the model outputs and parameters given by $\\Delta \\boldsymbol{y} = S \\, \\Delta \\boldsymbol{\\theta}$, where $S \\in \\mathbb{R}^{m \\times p}$ is the sensitivity matrix composed of columns $s_{1}, s_{2}, s_{3} \\in \\mathbb{R}^{m}$. Suppose the observational errors are independent and identically distributed Gaussian with variance $\\sigma^{2}$, so the weight matrix is $W = \\sigma^{-2} I_{m}$.\n\n1. Starting from the definition of the Fisher Information Matrix (FIM) under Gaussian errors, derive a necessary and sufficient condition for local non-identifiability of a linear combination of parameters due to collinearity among sensitivity columns. Your derivation must be expressed in terms of the rank and null space of the sensitivity matrix $S$ and the positive definiteness of the FIM.\n\n2. Now consider a specific setting with $m=4$ outputs and $p=3$ parameters, with the following sensitivity matrix\n$$\nS \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 2 & 1\n\\end{pmatrix}.\n$$\nAssume $W = \\sigma^{-2} I_{4}$ for some $\\sigma^{2} > 0$. Identify one nontrivial linear combination of parameters that is locally non-identifiable and one combination that is locally identifiable. Construct a linear reparameterization $\\boldsymbol{\\phi} = R \\boldsymbol{\\theta}$ with $R \\in \\mathbb{R}^{3 \\times 3}$ having integer entries and full row rank, such that the first two components of $\\boldsymbol{\\phi}$ span identifiable directions and the third component lies entirely in the non-identifiable subspace of the parameter space. Provide the reparameterized parameter vector $\\boldsymbol{\\phi}$ explicitly as functions of $\\theta_{1}$, $\\theta_{2}$, and $\\theta_{3}$. Your final answer must be the closed-form analytic expression for $\\boldsymbol{\\phi}$ written as a row matrix. No numerical rounding is required. Express the final answer without units.",
            "solution": "**Part 1: Condition for Local Non-identifiability**\n\nThe relationship between small perturbations in the model parameters, $\\Delta \\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$, and the model outputs, $\\Delta \\boldsymbol{y} \\in \\mathbb{R}^{m}$, is given by the linear model:\n$$ \\Delta \\boldsymbol{y} = S \\, \\Delta \\boldsymbol{\\theta} $$\nwhere $S \\in \\mathbb{R}^{m \\times p}$ is the sensitivity matrix. The observational errors are assumed to be independent and identically distributed Gaussian with variance $\\sigma^{2}$. The corresponding error covariance matrix is $C_e = \\sigma^{2} I_{m}$. The weight matrix is the inverse of the error covariance matrix, $W = C_e^{-1} = \\sigma^{-2} I_{m}$.\n\nThe Fisher Information Matrix (FIM), denoted by $\\mathcal{I}$, quantifies the amount of information that the observable data $\\boldsymbol{y}$ carries about the unknown parameters $\\boldsymbol{\\theta}$. For a linear model with additive Gaussian noise, the FIM is given by the expression:\n$$ \\mathcal{I} = S^{\\top} W S $$\nSubstituting the given weight matrix $W = \\sigma^{-2} I_{m}$:\n$$ \\mathcal{I} = S^{\\top} (\\sigma^{-2} I_{m}) S = \\sigma^{-2} S^{\\top} S $$\nSince $\\sigma^{2} > 0$, the constant $\\sigma^{-2}$ is a positive scalar and does not affect the positive (semi-)definiteness properties of the FIM besides scaling.\n\nLocal identifiability of the parameter vector $\\boldsymbol{\\theta}$ requires that the FIM, $\\mathcal{I}$, be invertible, which means it must be strictly positive definite. If $\\mathcal{I}$ is singular (i.e., only positive semi-definite), the parameters are locally non-identifiable. This singularity implies that there exist non-zero directions in the parameter space along which the model output does not change, making it impossible to distinguish different parameter values along these directions.\n\nA specific linear combination of parameters, given by $c^{\\top} \\boldsymbol{\\theta}$ for a non-zero vector $c \\in \\mathbb{R}^{p}$, is locally non-identifiable if the variance of its estimator is infinite. According to the Cramér-Rao Lower Bound, this occurs if the vector $c$ lies in the null space of the FIM. Therefore, the condition for non-identifiability of the combination $c^{\\top} \\boldsymbol{\\theta}$ is:\n$$ \\mathcal{I} c = 0 $$\nfor a non-zero vector $c$. Substituting the expression for $\\mathcal{I}$:\n$$ (\\sigma^{-2} S^{\\top} S) c = 0 $$\n$$ S^{\\top} S c = 0 $$\nThis equation shows that the null space of the FIM, $\\mathcal{I}$, is identical to the null space of the matrix $S^{\\top} S$.\n\nNow, we establish the relationship between the null space of $S^{\\top} S$ and the null space of $S$. Let $c \\in \\text{Null}(S)$. This means $S c = 0$. Then it follows that $S^{\\top} S c = S^{\\top}(0) = 0$, so $c \\in \\text{Null}(S^{\\top} S)$. Conversely, let $c \\in \\text{Null}(S^{\\top} S)$, so $S^{\\top} S c = 0$. Pre-multiplying by $c^{\\top}$ gives $c^{\\top} S^{\\top} S c = 0$. This can be rewritten as $(Sc)^{\\top}(Sc) = \\|Sc\\|_{2}^{2} = 0$. The squared Euclidean norm of a vector is zero if and only if the vector itself is the zero vector. Thus, $Sc=0$, which means $c \\in \\text{Null}(S)$. Therefore, the null spaces are identical: $\\text{Null}(S^{\\top} S) = \\text{Null}(S)$.\n\nThis leads to the necessary and sufficient condition: a linear combination $c^{\\top} \\boldsymbol{\\theta}$ is locally non-identifiable if and only if the vector $c$ is a non-zero vector in the null space of the sensitivity matrix $S$, i.e., $Sc = 0$. The existence of such a non-zero vector $c$ is equivalent to the columns of $S$ being linearly dependent (collinear), which in turn means that the rank of $S$ is less than the number of parameters $p$, i.e., $\\text{rank}(S) < p$. The non-identifiable directions in the parameter space are precisely those spanned by the basis vectors of the null space of $S$.\n\n**Part 2: Specific Application**\n\nWe are given the sensitivity matrix for a system with $m=4$ outputs and $p=3$ parameters:\n$$\nS =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 2 & 1\n\\end{pmatrix}\n$$\nLet the columns of $S$ be $s_{1}, s_{2}, s_{3}$. We inspect these columns for linear dependence:\n$s_{1} = (1, 0, 1, 0)^{\\top}$, $s_{2} = (0, 2, 0, 2)^{\\top}$, $s_{3} = (0, 1, 0, 1)^{\\top}$.\nBy inspection, we observe that $s_{2} = 2 s_{3}$. This implies a linear dependency:\n$$ 0 \\cdot s_{1} + 1 \\cdot s_{2} - 2 \\cdot s_{3} = 0 $$\nThis is an equation of the form $S c = 0$, where $c = (0, 1, -2)^{\\top}$. Since there exists a non-zero vector $c$ in the null space of $S$, the rank of $S$ is less than $3$. Specifically, since $s_{1}$ and $s_{3}$ are linearly independent, the rank of $S$ is $2$. The null space of $S$ is a one-dimensional subspace spanned by the vector $c = (0, 1, -2)^{\\top}$.\n\nA non-trivial linear combination of parameters that is locally non-identifiable corresponds to this vector $c$. The combination is $c^{\\top} \\boldsymbol{\\theta}$:\n$$ 0 \\cdot \\theta_{1} + 1 \\cdot \\theta_{2} - 2 \\cdot \\theta_{3} = \\theta_{2} - 2\\theta_{3} $$\nThis combination is locally non-identifiable.\n\nA linear combination $d^{\\top} \\boldsymbol{\\theta}$ is locally identifiable if the vector $d$ is not in the null space of $S$. A simple choice is $d = (1, 0, 0)^{\\top}$, which is clearly not a scalar multiple of $(0, 1, -2)^{\\top}$. The corresponding identifiable combination is $d^{\\top} \\boldsymbol{\\theta} = \\theta_{1}$.\n\nNext, we construct the reparameterization $\\boldsymbol{\\phi} = R \\boldsymbol{\\theta}$. The matrix $R$ must have integer entries and be full rank. The new parameters $\\phi_{1}, \\phi_{2}, \\phi_{3}$ are defined by the rows of $R$.\nThe third component, $\\phi_{3}$, must lie entirely in the non-identifiable subspace. This means its corresponding row vector in $R$, let's call it $r_{3}^{\\top}$, must be a basis vector for the null space of $S$. We can choose $r_{3}^{\\top} = (0, 1, -2)$. This gives $\\phi_{3} = \\theta_{2} - 2\\theta_{3}$.\n\nThe first two components, $\\phi_{1}$ and $\\phi_{2}$, must span the identifiable directions. The identifiable subspace is the orthogonal complement of the null space of $S$. This is the set of vectors $v$ such that $v \\cdot (0, 1, -2)^{\\top} = 0$. The rows $r_{1}^{\\top}$ and $r_{2}^{\\top}$ must be linearly independent vectors satisfying this condition. We require integer entries.\n\nLet $r_{1}^{\\top} = (a, b, c)$. The condition is $b - 2c = 0$. A simple choice is $r_{1}^{\\top} = (1, 0, 0)$, which satisfies $0 - 2(0) = 0$. This gives $\\phi_{1} = \\theta_{1}$.\n\nFor $r_{2}^{\\top} = (d, e, f)$, we need $e - 2f = 0$ and $r_{2}^{\\top}$ to be linearly independent of $r_{1}^{\\top}$. A simple choice is to set $f=1$, which implies $e=2$, and we can take $d=0$. So, $r_{2}^{\\top} = (0, 2, 1)$. This gives $\\phi_{2} = 2\\theta_{2} + \\theta_{3}$.\n\nNow we assemble the matrix $R$:\n$$\nR = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 1 \\\\\n0 & 1 & -2\n\\end{pmatrix}\n$$\nThe entries are integers. Let's check if $R$ has full rank by computing its determinant: $\\det(R) = 1(2(-2) - 1(1)) - 0 + 0 = -5 \\neq 0$. Thus, $R$ has full rank.\n\nThe reparameterized vector $\\boldsymbol{\\phi} = (\\phi_{1}, \\phi_{2}, \\phi_{3})^{\\top}$ is given by:\n$$ \\phi_{1} = \\theta_{1} $$\n$$ \\phi_{2} = 2\\theta_{2} + \\theta_{3} $$\n$$ \\phi_{3} = \\theta_{2} - 2\\theta_{3} $$\nThis construction fulfills all the requirements of the problem. The final answer is the expression for $\\boldsymbol{\\phi}$ as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\theta_{1} & 2\\theta_{2} + \\theta_{3} & \\theta_{2} - 2\\theta_{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "To move from a qualitative understanding of identifiability to a quantitative assessment of parameter uncertainty, we turn to the Fisher Information Matrix (FIM). The FIM measures the curvature of the likelihood function, and its eigenvalues reveal the 'stiff' and 'sloppy' directions in parameter space—combinations that are well-constrained versus those that are poorly constrained. This practice provides a tangible numerical example where you will compute the FIM and its eigenvalues, allowing you to quantify the degree of sloppiness and gain insight into which parameter aspects your data can, and cannot, inform. ",
            "id": "4065525",
            "problem": "Consider a simplified Single Column Model (SCM) used in numerical weather prediction and climate modeling. The SCM has two tunable parameters collected into a vector $ \\theta = (\\alpha, \\tau)^{\\top} $, where $ \\alpha $ is an entrainment efficiency (dimensionless) and $ \\tau $ is a convective adjustment timescale (in seconds). Let $ y(\\theta) \\in \\mathbb{R}^{3} $ denote a vector of three observable quantities (e.g., column-mean potential temperature at three distinct times) predicted by the model.\n\nAssume the following context-appropriate fundamental base:\n- Observational errors are zero-mean Gaussian with covariance $ R = \\sigma^{2} I $, where $ \\sigma = 0.2 $ (in the same physical units as $ y $) and $ I $ is the $ 3 \\times 3 $ identity matrix.\n- Near a nominal parameter value $ \\theta_{0} $, the model-observation map is linearizable as $ y(\\theta_{0} + \\delta \\theta) \\approx y(\\theta_{0}) + J \\, \\delta \\theta $, where $ J = \\partial y / \\partial \\theta \\big|_{\\theta_{0}} $ is the $ 3 \\times 2 $ sensitivity (Jacobian) matrix of observables with respect to parameters.\n- The Fisher information matrix $ I(\\theta_{0}) $ is defined from the expected curvature of the negative log-likelihood under these assumptions.\n\nAt $ \\theta = \\theta_{0} $, the sensitivities are measured to be\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n0.80 & 0.02 \\\\\n0.78 & 0.01 \\\\\n0.81 & 0.03\n\\end{pmatrix}.\n$$\n\nStarting from the base assumptions above, derive the Fisher information matrix $ I(\\theta_{0}) $ in terms of $ J $ and $ R $. Compute $ I(\\theta_{0}) $ numerically using the given data, then compute its two eigenvalues and interpret their magnitude in terms of parameter sloppiness (i.e., relative informativeness about directions in parameter space). Define a scalar sloppiness measure $ s $ as the ratio of the smaller eigenvalue to the larger eigenvalue, $ s = \\lambda_{\\min} / \\lambda_{\\max} $. Report the numerical value of $ s $.\n\nRound your final value of $ s $ to four significant figures. The final reported value is dimensionless; do not include any units in your final answer.",
            "solution": "We begin from the standard Gaussian likelihood for observational errors. If $ y^{\\text{obs}} \\in \\mathbb{R}^{3} $ are the observations and the model predicts $ y(\\theta) $, with errors $ \\varepsilon = y^{\\text{obs}} - y(\\theta) $ distributed as a multivariate Gaussian with covariance $ R $, then the negative log-likelihood is\n$$\n\\ell(\\theta)\n=\n\\frac{1}{2} \\left( y^{\\text{obs}} - y(\\theta) \\right)^{\\top} R^{-1} \\left( y^{\\text{obs}} - y(\\theta) \\right)\n+ \\text{const}.\n$$\nUnder linearization near a nominal parameter $ \\theta_{0} $, write $ y(\\theta_{0} + \\delta \\theta) \\approx y(\\theta_{0}) + J \\, \\delta \\theta $, where $ J = \\partial y / \\partial \\theta \\big|_{\\theta_{0}} $ is the $ 3 \\times 2 $ Jacobian. The second derivative (Hessian) of $ \\ell $ with respect to $ \\delta \\theta $ is, in expectation over the observational noise, given by\n$$\nI(\\theta_{0})\n=\nJ^{\\top} R^{-1} J,\n$$\nwhich is the Fisher information matrix. This result is a well-tested fact for Gaussian errors and linearized models in parameter estimation.\n\nWe are given $ R = \\sigma^{2} I $ with $ \\sigma = 0.2 $, so $ R = 0.04 \\, I $ and hence $ R^{-1} = 25 \\, I $. Therefore,\n$$\nI(\\theta_{0})\n=\nJ^{\\top} R^{-1} J\n=\n25 \\, J^{\\top} J.\n$$\nCompute $ J^{\\top} J $ from the given $ J $:\n$$\nJ\n=\n\\begin{pmatrix}\n0.80 & 0.02 \\\\\n0.78 & 0.01 \\\\\n0.81 & 0.03\n\\end{pmatrix},\n\\quad\nJ^{\\top} J\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{3} J_{i1}^{2} & \\sum_{i=1}^{3} J_{i1} J_{i2} \\\\\n\\sum_{i=1}^{3} J_{i1} J_{i2} & \\sum_{i=1}^{3} J_{i2}^{2}\n\\end{pmatrix}.\n$$\nCompute each element:\n- First column squared sum:\n$$\n\\sum_{i=1}^{3} J_{i1}^{2}\n=\n(0.80)^{2} + (0.78)^{2} + (0.81)^{2}\n=\n0.6400 + 0.6084 + 0.6561\n=\n1.9045.\n$$\n- Second column squared sum:\n$$\n\\sum_{i=1}^{3} J_{i2}^{2}\n=\n(0.02)^{2} + (0.01)^{2} + (0.03)^{2}\n=\n0.0004 + 0.0001 + 0.0009\n=\n0.0014.\n$$\n- Cross term sum:\n$$\n\\sum_{i=1}^{3} J_{i1} J_{i2}\n=\n(0.80)(0.02) + (0.78)(0.01) + (0.81)(0.03)\n=\n0.016 + 0.0078 + 0.0243\n=\n0.0481.\n$$\nThus,\n$$\nJ^{\\top} J\n=\n\\begin{pmatrix}\n1.9045 & 0.0481 \\\\\n0.0481 & 0.0014\n\\end{pmatrix},\n\\qquad\nI(\\theta_{0})\n=\n25 \\begin{pmatrix}\n1.9045 & 0.0481 \\\\\n0.0481 & 0.0014\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n47.6125 & 1.2025 \\\\\n1.2025 & 0.035\n\\end{pmatrix}.\n$$\n\nSince $ I(\\theta_{0}) $ is symmetric positive definite, its eigenvalues $ \\lambda_{\\max} $ and $ \\lambda_{\\min} $ are real and positive. For a $ 2 \\times 2 $ symmetric matrix\n$$\nM\n=\n\\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix},\n$$\nthe eigenvalues are\n$$\n\\lambda_{\\pm}\n=\n\\frac{a + c \\pm \\sqrt{(a + c)^{2} - 4(ac - b^{2})}}{2}.\n$$\nHere, $ a = 47.6125 $, $ b = 1.2025 $, and $ c = 0.035 $. Let $ t = a + c $ and $ d = ac - b^{2} $. Compute\n$$\nt\n=\n47.6125 + 0.035\n=\n47.6475,\n\\qquad\nac\n=\n(47.6125)(0.035)\n=\n1.6664375,\n\\qquad\nb^{2}\n=\n(1.2025)^{2}\n=\n1.44600625,\n$$\nso\n$$\nd\n=\nac - b^{2}\n=\n1.6664375 - 1.44600625\n=\n0.22043125.\n$$\nThen\n$$\n\\Delta\n=\n\\sqrt{t^{2} - 4d}\n=\n\\sqrt{(47.6475)^{2} - 4(0.22043125)}.\n$$\nCompute $ (47.6475)^{2} $ exactly via expansion:\n$$\n(47.6475)^{2}\n=\n(47.6 + 0.0475)^{2}\n=\n(47.6)^{2} + 2(47.6)(0.0475) + (0.0475)^{2}\n=\n2265.76 + 4.522 + 0.00225625\n=\n2270.28425625.\n$$\nTherefore,\n$$\nt^{2} - 4d\n=\n2270.28425625 - 0.881725\n=\n2269.40253125,\n\\quad\n\\Delta\n=\n\\sqrt{2269.40253125}\n\\approx\n47.63824644,\n$$\nand the eigenvalues are\n$$\n\\lambda_{\\max}\n=\n\\frac{t + \\Delta}{2}\n=\n\\frac{47.6475 + 47.63824644}{2}\n\\approx\n47.64287322,\n$$\n$$\n\\lambda_{\\min}\n=\n\\frac{t - \\Delta}{2}\n=\n\\frac{47.6475 - 47.63824644}{2}\n\\approx\n0.00462678.\n$$\n\nThe sloppiness measure $ s $ is defined as $ s = \\lambda_{\\min} / \\lambda_{\\max} $. Using the values above,\n$$\ns\n=\n\\frac{0.00462678}{47.64287322}\n\\approx\n9.711 \\times 10^{-5}.\n$$\nRounded to four significant figures, the final value is $ 9.711 \\times 10^{-5} $.\n\nInterpretation: The Fisher information matrix $ I(\\theta_{0}) $ encodes how strongly the data constrain directions in parameter space. A very small ratio $ s = \\lambda_{\\min} / \\lambda_{\\max} $ indicates strong anisotropy: one parameter combination (the eigenvector corresponding to $ \\lambda_{\\max} $) is tightly constrained by the data, while the orthogonal combination (corresponding to $ \\lambda_{\\min} $) is extremely weakly constrained. In this SCM example, the sensitivities of the observables to $ \\tau $ are much smaller than to $ \\alpha $ (and nearly collinear with the $ \\alpha $ direction), yielding $ s \\approx 9.711 \\times 10^{-5} $, which is characteristic of a sloppy parameter direction that would be difficult to estimate reliably without additional information or more informative observations.",
            "answer": "$$\\boxed{9.711 \\times 10^{-5}}$$"
        }
    ]
}