{
    "hands_on_practices": [
        {
            "introduction": "The accuracy of extreme weather prediction depends heavily on the underlying numerical model, as numerical artifacts like artificial diffusion can suppress the very extremes we aim to study. This exercise contrasts a simple, diffusive first-order upwind scheme with a more sophisticated, high-resolution scheme to demonstrate how the choice of numerical method directly impacts the simulated peak intensity of a moisture field and, consequently, the predicted extreme precipitation. By implementing and comparing these schemes, you will gain a concrete understanding of why high-resolution methods are essential for capturing the sharp gradients that characterize extreme weather phenomena .",
            "id": "4065906",
            "problem": "Consider a one-dimensional periodic domain of length $L$ discretized into $N$ uniform cells of width $\\Delta x = L/N$. Let $q(x,t)$ denote the water vapor specific humidity (mixing ratio) in units of $\\mathrm{kg}\\,\\mathrm{kg}^{-1}$, transported by a constant wind speed $u$ (in $\\mathrm{m}\\,\\mathrm{s}^{-1}$). The evolution of $q$ is governed by the conservation law with condensation sink\n$$\n\\frac{\\partial q}{\\partial t} + u \\frac{\\partial q}{\\partial x} = -S(q),\n$$\nwhere the condensation sink $S(q)$ represents conversion of vapor to precipitation and is given by\n$$\nS(q) = \\max\\left(0, \\frac{q - q_s}{\\tau_c}\\right),\n$$\nwith $q_s$ the saturation specific humidity in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}$ and $\\tau_c$ a condensation time scale in $\\mathrm{s}$. Assume a single-layer atmosphere of depth $H$ (in $\\mathrm{m}$) and air density $\\rho_{\\mathrm{air}}$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-3}$). The instantaneous precipitation rate per unit area is the vertically integrated condensation mass flux,\n$$\nP(x,t) = \\rho_{\\mathrm{air}}\\,H\\,S(q(x,t)),\n$$\nwith units of $\\mathrm{kg}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$. For reporting precipitation intensity in $\\mathrm{mm}\\,\\mathrm{hr}^{-1}$, use the identity that $1\\,\\mathrm{mm}$ of rain corresponds to $1\\,\\mathrm{kg}\\,\\mathrm{m}^{-2}$ of water, so convert via\n$$\nP_{\\mathrm{mm/hr}}(x,t) = 3600\\,P(x,t).\n$$\n\nYour task is to implement two explicit finite-volume numerical schemes for the advection-sink equation on the periodic domain:\n\n1. A first-order upwind scheme (denoted U1) that uses the upwind-biased numerical flux for advection depending on the sign of $u$ and a forward Euler time step for the sink term.\n\n2. A high-resolution Total Variation Diminishing (TVD) Monotonic Upstream-centered Scheme for Conservation Laws (MUSCL) with the Van Leer flux limiter. The limiter function $\\phi(r)$, which depends on the ratio $r$ of successive discrete gradients, must be used to construct piecewise linear reconstructions that preserve monotonicity while achieving second-order spatial accuracy in smooth regions.\n\nBoth schemes must enforce periodic boundary conditions. Use a Courant number $\\nu = |u|\\Delta t/\\Delta x$ not exceeding $0.45$ for stability in the explicit scheme, and use a forward Euler update for the sink term. Enforce physical bounds by clipping $q$ to the interval $[0, q_{\\max}]$ after each time step, where $q_{\\max}$ is a prescribed upper bound for water vapor.\n\nThe initial condition is a uniform background $q_{\\mathrm{bg}}$ plus a rectangular moisture patch of width $W$, with a higher value $q_{\\mathrm{patch}}$:\n- $q(x,0) = q_{\\mathrm{patch}}$ for $x \\in [x_0, x_0 + W]$ modulo periodicity,\n- $q(x,0) = q_{\\mathrm{bg}}$ elsewhere.\n\nFor each test case below, simulate both schemes up to the specified final time $T_{\\mathrm{end}}$, compute the instantaneous precipitation $P_{\\mathrm{mm/hr}}(x,t)$ at every time step using the current $q$, and record the maximum value over all space and time, denoted $P_{\\max}^{\\mathrm{U1}}$ for the first-order upwind scheme and $P_{\\max}^{\\mathrm{HR}}$ for the high-resolution scheme. For each test case, report the ratio $R = P_{\\max}^{\\mathrm{HR}} / P_{\\max}^{\\mathrm{U1}}$ as a decimal (unitless).\n\nPhysical units must be adhered to as specified:\n- $L$ in $\\mathrm{m}$,\n- $u$ in $\\mathrm{m}\\,\\mathrm{s}^{-1}$,\n- $q_s$, $q_{\\mathrm{bg}}$, $q_{\\mathrm{patch}}$, $q_{\\max}$ in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}$,\n- $\\tau_c$ in $\\mathrm{s}$,\n- $H$ in $\\mathrm{m}$,\n- $\\rho_{\\mathrm{air}}$ in $\\mathrm{kg}\\,\\mathrm{m}^{-3}$,\n- $T_{\\mathrm{end}}$ in $\\mathrm{s}$,\n- $W$ in $\\mathrm{m}$.\n\nTest Suite:\n- Case 1 (general case): $L = 200000$, $N = 256$, $u = 10$, $q_s = 0.015$, $q_{\\mathrm{bg}} = 0.005$, $q_{\\mathrm{patch}} = 0.025$, $W = 50000$, $x_0 = 50000$, $\\tau_c = 900$, $H = 1000$, $\\rho_{\\mathrm{air}} = 1.2$, $q_{\\max} = 0.03$, $T_{\\mathrm{end}} = 7200$, Courant number target $\\nu = 0.45$.\n- Case 2 (coarse resolution): same as Case 1 but $N = 64$.\n- Case 3 (near Courant limit with faster wind): same as Case 1 but $u = 40$ and $T_{\\mathrm{end}} = 3600$.\n- Case 4 (weak supersaturation edge case): same as Case 1 but $q_{\\mathrm{patch}} = 0.016$.\n\nYour program must produce a single line of output containing the ratios $R$ for the four test cases, as a comma-separated list enclosed in square brackets. Each ratio must be rounded to six decimal places. For example:\n\"[R1,R2,R3,R4]\".\nNo other output is permitted.",
            "solution": "The user has provided a well-defined problem in the field of computational fluid dynamics, specifically applied to atmospheric modeling. The problem asks for the implementation and comparison of two numerical schemes for solving a one-dimensional advection equation with a source/sink term. The validation of the problem statement confirms that it is scientifically grounded, mathematically well-posed, objective, and contains all necessary information for a unique solution. Therefore, I will proceed with developing the solution.\n\nThe governing partial differential equation (PDE) for the water vapor specific humidity, $q(x,t)$, is a conservation law with a sink term:\n$$\n\\frac{\\partial q}{\\partial t} + u \\frac{\\partial q}{\\partial x} = -S(q)\n$$\nwhere $u$ is a constant positive wind speed, and $S(q)$ is the condensation sink term. This equation describes how the quantity $q$ is transported (advected) by the wind and simultaneously reduced by condensation. We will solve this on a periodic one-dimensional domain of length $L$, discretized into $N$ uniform cells of width $\\Delta x = L/N$. The cell-averaged value in cell $i$ is denoted $q_i(t)$.\n\nA finite-volume discretization of the governing PDE takes the form:\n$$\n\\frac{d q_i}{dt} = -\\frac{F_{i+1/2} - F_{i-1/2}}{\\Delta x} - S(q_i)\n$$\nHere, $F_{i+1/2}$ represents the numerical flux of $q$ across the interface between cell $i$ and cell $i+1$. The sink term is treated as a local source term within cell $i$. We use a forward Euler method for time integration:\n$$\nq_i^{n+1} = q_i^n + \\Delta t \\left( -\\frac{F_{i+1/2}^n - F_{i-1/2}^n}{\\Delta x} - S(q_i^n) \\right)\n$$\nwhere $q_i^n$ is the value at time step $n$. The two required schemes differ in their formulation of the numerical flux $F$.\n\nThe sink term $S(q)$ and the resulting precipitation rate $P_{\\mathrm{mm/hr}}$ are calculated as follows:\n$$\nS(q_i^n) = \\max\\left(0, \\frac{q_i^n - q_s}{\\tau_c}\\right)\n$$\n$$\nP_{\\mathrm{mm/hr}}(x_i, t^n) = 3600 \\cdot \\rho_{\\mathrm{air}} \\cdot H \\cdot S(q_i^n)\n$$\nThe maximum of this precipitation rate over all cells $i$ and all time steps $n$ is the quantity of interest.\n\nThe time step $\\Delta t$ is determined by the Courant-Friedrichs-Lewy (CFL) condition. Given the Courant number $\\nu = |u|\\Delta t/\\Delta x$, the time step is $\\Delta t = \\nu \\Delta x / |u|$. For all test cases, $u>0$.\n\nThe initial condition is a piecewise constant function:\n$$\nq_i(0) = \\begin{cases} q_{\\mathrm{patch}}  \\text{if } x_0 \\le x_i \\le x_0 + W \\\\ q_{\\mathrm{bg}}  \\text{otherwise} \\end{cases}\n$$\nwhere $x_i = (i+0.5)\\Delta x$ is the center of cell $i$.\n\nAfter each time step, the solution $q_i^{n+1}$ is clipped to the physical bounds $[0, q_{\\max}]$. Periodic boundary conditions are enforced, meaning that for a grid of $N$ cells indexed $0, \\dots, N-1$, the left neighbor of cell $0$ is cell $N-1$, and the right neighbor of cell $N-1$ is cell $0$.\n\n**1. First-Order Upwind Scheme (U1)**\n\nThis scheme is simple and robust but numerically diffusive. For a positive wind speed $u > 0$, the \"upwind\" direction is from the left. The state of the fluid entering the interface $i+1/2$ is simply the state in the upwind cell, $i$. Thus, the numerical flux is:\n$$\nF_{i+1/2}^{\\mathrm{U1}} = u \\cdot q_i\n$$\nSubstituting this into the finite volume update equation gives the tendency for the advection term:\n$$\n-\\frac{F_{i+1/2}^n - F_{i-1/2}^n}{\\Delta x} = -\\frac{u q_i^n - u q_{i-1}^n}{\\Delta x} = -u \\frac{q_i^n - q_{i-1}^n}{\\Delta x}\n$$\nThe full update equation for the U1 scheme is:\n$$\nq_i^{n+1} = q_i^n - \\frac{u \\Delta t}{\\Delta x} (q_i^n - q_{i-1}^n) - \\Delta t \\cdot \\max\\left(0, \\frac{q_i^n - q_s}{\\tau_c}\\right)\n$$\nThe term $u \\Delta t / \\Delta x$ is the Courant number $\\nu$.\n\n**2. High-Resolution TVD MUSCL Scheme (HR)**\n\nThe Monotonic Upstream-centered Scheme for Conservation Laws (MUSCL) achieves higher spatial accuracy by reconstructing a piecewise-linear representation of the data in each cell. The key is to limit the reconstructed slopes to avoid introducing new oscillations, a property known as Total Variation Diminishing (TVD).\n\nThe steps are as follows:\na. **Slope Calculation:** For each cell $i$, we estimate a slope. A common way to ensure monotonicity is to use a limited slope. We first compute the backward difference, $\\Delta_i^b = q_i - q_{i-1}$, and the forward difference, $\\Delta_i^f = q_{i+1} - q_i$. The specified Van Leer limiter combines these to produce a limited slope $\\sigma_i$ for cell $i$:\n$$\n\\sigma_i = \\begin{cases} \\frac{2 \\Delta_i^b \\Delta_i^f}{\\Delta_i^b + \\Delta_i^f}  \\text{if } \\Delta_i^b \\Delta_i^f > 0 \\\\ 0  \\text{if } \\Delta_i^b \\Delta_i^f \\le 0 \\end{cases}\n$$\nThis limiter is zero in regions of extrema (where the signs of the one-sided differences differ) and provides a harmonic mean of the slopes in monotonic regions, which is less aggressive than simply choosing the minimum magnitude.\n\nb. **Data Reconstruction:** The limited slope $\\sigma_i$ is used to extrapolate the values of $q$ from the cell center $x_i$ to its boundaries. Since the wind speed $u$ is positive, we only need the value at the right boundary of cell $i$ (which is the left boundary of the interface $i+1/2$), denoted $q_{i+1/2}^L$:\n$$\nq_{i+1/2}^L = q_i + \\frac{1}{2}\\sigma_i\n$$\nThe factor of $1/2$ comes from extrapolating over half a cell width, $(\\Delta x / 2)$. The slope was defined as a difference between cells, not per unit length, so the $\\Delta x$ terms cancel.\n\nc. **Flux Calculation:** The flux across the interface $i+1/2$ is determined by the upwind value, which for $u>0$ is $q_{i+1/2}^L$.\n$$\nF_{i+1/2}^{\\mathrm{HR}} = u \\cdot q_{i+1/2}^L = u \\left(q_i + \\frac{1}{2}\\sigma_i\\right)\n$$\nd. **Finite Volume Update:** The full update equation for the HR scheme is:\n$$\n\\frac{dq_i}{dt} = -\\frac{1}{\\Delta x} \\left( F_{i+1/2}^{\\mathrm{HR}} - F_{i-1/2}^{\\mathrm{HR}} \\right) - S(q_i) = -\\frac{u}{\\Delta x} \\left( \\left(q_i + \\frac{\\sigma_i}{2}\\right) - \\left(q_{i-1} + \\frac{\\sigma_{i-1}}{2}\\right) \\right) - S(q_i)\n$$\nThe time integration proceeds with forward Euler as before.\n\nThe fundamental difference between the schemes lies in their ability to resolve sharp gradients. The U1 scheme's numerical diffusion will smear the initial rectangular patch, lowering its peak value and thus the maximum computed precipitation. The HR scheme, being second-order accurate in smooth regions and monotonicity-preserving, will maintain a much sharper profile for the patch, yielding a higher peak humidity value and consequently a higher maximum precipitation rate. The ratio $R = P_{\\max}^{\\mathrm{HR}} / P_{\\max}^{\\mathrm{U1}}$ is expected to be greater than $1$, quantifying the impact of numerical diffusion on the prediction of this extreme event.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"L\": 200000.0, \"N\": 256, \"u\": 10.0, \"q_s\": 0.015, \"q_bg\": 0.005,\n            \"q_patch\": 0.025, \"W\": 50000.0, \"x_0\": 50000.0, \"tau_c\": 900.0,\n            \"H\": 1000.0, \"rho_air\": 1.2, \"q_max\": 0.03, \"T_end\": 7200.0, \"nu\": 0.45,\n        },\n        # Case 2 (coarse resolution)\n        {\n            \"L\": 200000.0, \"N\": 64, \"u\": 10.0, \"q_s\": 0.015, \"q_bg\": 0.005,\n            \"q_patch\": 0.025, \"W\": 50000.0, \"x_0\": 50000.0, \"tau_c\": 900.0,\n            \"H\": 1000.0, \"rho_air\": 1.2, \"q_max\": 0.03, \"T_end\": 7200.0, \"nu\": 0.45,\n        },\n        # Case 3 (near Courant limit with faster wind)\n        {\n            \"L\": 200000.0, \"N\": 256, \"u\": 40.0, \"q_s\": 0.015, \"q_bg\": 0.005,\n            \"q_patch\": 0.025, \"W\": 50000.0, \"x_0\": 50000.0, \"tau_c\": 900.0,\n            \"H\": 1000.0, \"rho_air\": 1.2, \"q_max\": 0.03, \"T_end\": 3600.0, \"nu\": 0.45,\n        },\n        # Case 4 (weak supersaturation edge case)\n        {\n            \"L\": 200000.0, \"N\": 256, \"u\": 10.0, \"q_s\": 0.015, \"q_bg\": 0.005,\n            \"q_patch\": 0.016, \"W\": 50000.0, \"x_0\": 50000.0, \"tau_c\": 900.0,\n            \"H\": 1000.0, \"rho_air\": 1.2, \"q_max\": 0.03, \"T_end\": 7200.0, \"nu\": 0.45,\n        }\n    ]\n\n    ratios = []\n    for params in test_cases:\n        ratio = run_simulation(**params)\n        ratios.append(ratio)\n\n    # Format the final output string\n    output_str = f\"[{','.join(f'{r:.6f}' for r in ratios)}]\"\n    print(output_str)\n\n\ndef run_simulation(L, N, u, q_s, q_bg, q_patch, W, x_0, tau_c, H, rho_air, q_max, T_end, nu):\n    \"\"\"\n    Runs one simulation case for both U1 and HR schemes and returns the ratio of max precipitation.\n    \"\"\"\n    # Grid and time step setup\n    dx = L / N\n    dt = nu * dx / abs(u)\n    \n    # Grid cell centers\n    x = (np.arange(N) + 0.5) * dx\n\n    # Initial condition\n    q_initial = np.full(N, q_bg)\n    # Determine indices for the patch based on cell centers\n    # i >= x0/dx - 0.5  and i = (x0+W)/dx - 0.5\n    start_idx = int(np.ceil(x_0 / dx - 0.5))\n    end_idx = int(np.floor((x_0 + W) / dx - 0.5))\n    if end_idx >= start_idx:\n        q_initial[start_idx : end_idx + 1] = q_patch\n\n    # --- Run simulation for both schemes ---\n    p_max_u1 = simulate_scheme(q_initial.copy(), 'U1', N, u, q_s, tau_c, H, rho_air, q_max, T_end, dx, dt, nu)\n    p_max_hr = simulate_scheme(q_initial.copy(), 'HR', N, u, q_s, tau_c, H, rho_air, q_max, T_end, dx, dt, nu)\n\n    if p_max_u1 > 0:\n        return p_max_hr / p_max_u1\n    elif p_max_hr > 0:\n        return np.inf # Should not happen with problem constraints, but for robustness\n    else:\n        return 1.0 # If both are zero, their ratio is 1\n\ndef simulate_scheme(q, scheme_type, N, u, q_s, tau_c, H, rho_air, q_max, T_end, dx, dt, nu):\n    \"\"\"\n    Core simulation loop for a single numerical scheme.\n    \"\"\"\n    \n    def calculate_max_p(current_q):\n        s = np.maximum(0, (current_q - q_s) / tau_c)\n        p_kg_m2_s = rho_air * H * s\n        p_mm_hr = 3600 * p_kg_m2_s\n        return np.max(p_mm_hr) if p_mm_hr.size > 0 else 0.0\n\n    max_p = calculate_max_p(q)\n    \n    time = 0.0\n    while time  T_end:\n        q_old = q.copy()\n\n        # Advection term\n        if scheme_type == 'U1':\n            q_prev = np.roll(q_old, 1) # upwind for u > 0\n            advection_tendency = -nu * (q_old - q_prev)\n        elif scheme_type == 'HR':\n            q_prev = np.roll(q_old, 1)\n            q_next = np.roll(q_old, -1)\n\n            # Van Leer limiter implementation\n            delta_b = q_old - q_prev\n            delta_f = q_next - q_old\n            \n            # Use a small epsilon to avoid division by zero in theory, though with floats it's unlikely\n            # if product is > 0.\n            product = delta_b * delta_f\n            slopes = np.zeros(N)\n            mask = product > 1e-12 # Threshold for numerical stability\n            \n            denominator = delta_b[mask] + delta_f[mask]\n            \n            # Avoid division by zero when denominator is close to zero\n            den_mask = np.abs(denominator) > 1e-12\n            \n            numerator = 2 * product[mask]\n            \n            # Allocate a temporary slopes array for the masked operation\n            masked_slopes = np.zeros_like(denominator)\n            masked_slopes[den_mask] = numerator[den_mask] / denominator[den_mask]\n            \n            slopes[mask] = masked_slopes\n\n            # Reconstruct fluxes\n            # F_{i+1/2} = u * (q_i + 0.5 * sigma_i)\n            flux_iph = u * (q_old + 0.5 * slopes)\n            # F_{i-1/2} = u * (q_{i-1} + 0.5 * sigma_{i-1})\n            flux_imh = np.roll(flux_iph, 1)\n            \n            advection_tendency = -(dt / dx) * (flux_iph - flux_imh)\n        else:\n            raise ValueError(\"Unknown scheme type\")\n            \n        # Sink term\n        sink_tendency = -dt * np.maximum(0, (q_old - q_s) / tau_c)\n        \n        # Update q\n        q = q_old + advection_tendency + sink_tendency\n        \n        # Enforce physical bounds\n        q = np.clip(q, 0.0, q_max)\n        \n        # Update max precipitation\n        max_p = max(max_p, calculate_max_p(q))\n        \n        time += dt\n        \n    return max_p\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Once reliable data is available, whether from observations or a high-fidelity model, the next step is to statistically characterize its extreme behavior. The block maxima method is a foundational approach in Extreme Value Theory (EVT) that involves fitting the Generalized Extreme Value (GEV) distribution to annual maxima of a variable like precipitation. In this practice, you will use the powerful technique of Maximum Likelihood Estimation (MLE) to find the distribution's parameters, which then allows for the calculation of the return level—a crucial metric for risk assessment like the \"100-year storm\" .",
            "id": "4065983",
            "problem": "You are tasked with developing a program to estimate parameters of the Generalized Extreme Value (GEV) distribution for annual block maxima of daily precipitation and to compute a $100$-year return level. The context is modeling extreme weather events as part of numerical weather prediction and climate modeling. The scientific basis for the problem is the well-established convergence of suitably normalized block maxima to the Generalized Extreme Value (GEV) distribution. The standard form for the cumulative distribution function of the GEV for block maxima, with location parameter $\\mu$, scale parameter $\\sigma$, and shape parameter $\\xi$, is defined for values $x$ that satisfy the domain constraint $1 + \\xi \\frac{x - \\mu}{\\sigma} > 0$.\n\nStarting from this definition and the independence of the annual block maxima across years, you must derive the likelihood and use Maximum Likelihood Estimation (MLE) to estimate $(\\mu,\\sigma,\\xi)$ for each dataset. Using the definition of the $T$-year return level as the quantile exceeded with probability $p = \\frac{1}{T}$ by the annual block maximum, derive the expression for the return level $z_T$ and compute $z_{100}$ for each dataset. If the shape parameter satisfies $\\xi \\rightarrow 0$, use the appropriate limiting form in your implementation.\n\nPhysical units and reporting requirements:\n- The block maxima values are annual maxima of daily precipitation, measured in millimeters per day (mm/day).\n- The return level $z_{100}$ must be expressed in mm/day.\n- Your program must round each reported numerical result to $3$ decimal places.\n\nTest suite and datasets:\nYou are provided three datasets of annual block maxima spanning $50$ years ($50$ values each), representing different tail behaviors relevant to extreme precipitation.\n- Dataset A (heavy tail candidate, plausible tropical extremes): \n[$91.2$, $104.5$, $97.8$, $112.3$, $85.7$, $123.4$, $146.1$, $132.7$, $108.9$, $119.5$, $140.2$, $88.6$, $101.3$, $125.9$, $167.4$, $154.3$, $173.2$, $189.8$, $205.1$, $221.7$, $236.8$, $249.5$, $263.4$, $278.9$, $294.1$, $313.5$, $295.6$, $271.4$, $242.1$, $210.4$, $195.7$, $176.5$, $158.4$, $149.2$, $133.8$, $121.6$, $111.1$, $102.5$, $95.3$, $89.7$, $84.1$, $79.9$, $76.4$, $74.0$, $71.8$, $69.7$, $68.1$, $67.2$, $66.5$, $65.9$]\n- Dataset B (near-Gumbel candidate, moderate spread typical of midlatitude extremes): \n[$82.3$, $95.6$, $110.2$, $120.7$, $135.3$, $98.4$, $112.1$, $126.8$, $141.5$, $155.9$, $168.3$, $180.5$, $92.7$, $105.4$, $117.9$, $130.6$, $143.2$, $156.7$, $170.1$, $183.4$, $97.8$, $109.6$, $123.3$, $136.9$, $149.7$, $162.4$, $175.8$, $188.6$, $101.2$, $114.5$, $127.1$, $139.8$, $152.6$, $165.4$, $178.2$, $190.9$, $106.3$, $118.9$, $131.5$, $144.2$, $156.8$, $169.4$, $181.9$, $194.6$, $89.4$, $102.1$, $115.7$, $128.3$, $141.0$, $154.6$]\n- Dataset C (bounded tail candidate, upper support constraint consistent with certain hydrological regimes): \n[$92.1$, $104.3$, $116.5$, $128.2$, $139.8$, $151.1$, $162.4$, $173.6$, $98.7$, $110.5$, $122.3$, $134.0$, $145.6$, $157.3$, $168.9$, $179.5$, $95.4$, $107.2$, $118.9$, $130.7$, $142.5$, $154.1$, $165.8$, $176.4$, $100.2$, $112.0$, $123.7$, $135.3$, $146.9$, $158.6$, $169.3$, $179.1$, $96.9$, $108.6$, $120.3$, $132.0$, $143.7$, $155.4$, $167.1$, $178.8$, $91.5$, $103.2$, $114.9$, $126.6$, $138.3$, $149.9$, $161.6$, $173.3$, $85.7$, $97.4$]\n\nComputational tasks and constraints:\n- For each dataset, start from the standard GEV cumulative distribution function and independently distributed annual maxima assumption to derive the likelihood and the corresponding maximum likelihood estimator.\n- Implement parameter estimation subject to the domain constraint $1 + \\xi \\frac{x - \\mu}{\\sigma} > 0$ for all data points and the scale constraint $\\sigma > 0$.\n- Compute the $100$-year return level using the derived quantile expression for $p = \\frac{1}{100}$, properly handling the limiting case when $\\xi \\to 0$.\n- Express $z_{100}$ in mm/day, rounded to $3$ decimals.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for the three datasets, where each dataset’s result is the list $[\\hat{\\mu},\\hat{\\sigma},\\hat{\\xi},z_{100}]$ with each entry rounded to $3$ decimals. The overall output must be a single list enclosing the three per-dataset lists, for example: \n\"[[m1,s1,x1,z1],[m2,s2,x2,z2],[m3,s3,x3,z3]]\".\n\nThe program must be self-contained, must not read input, and must run as-is. Angles are not involved in this problem. All quantities are to be given in mm/day and rounded to $3$ decimals.",
            "solution": "The problem requires the estimation of the parameters of the Generalized Extreme Value (GEV) distribution for three datasets representing annual maximum daily precipitation. The estimation is to be performed using the Maximum Likelihood Estimation (MLE) method. Subsequently, the $100$-year return level must be computed for each dataset. The entire process is grounded in the principles of extreme value theory and statistical inference.\n\nThe analysis begins with the formal definition of the GEV distribution and the derivation of the necessary statistical machinery. The GEV cumulative distribution function (CDF) for a random variable $X$ is given by:\n$$\nF(x; \\mu, \\sigma, \\xi) = \n\\begin{cases} \n\\exp\\left( -\\left[1 + \\xi \\left(\\frac{x - \\mu}{\\sigma}\\right)\\right]^{-1/\\xi} \\right)  \\text{for } \\xi \\neq 0 \\\\\n\\exp\\left( -\\exp\\left(-\\frac{x - \\mu}{\\sigma}\\right) \\right)  \\text{for } \\xi = 0 \n\\end{cases}\n$$\nHere, $\\mu$ is the location parameter, $\\sigma$ is the scale parameter, and $\\xi$ is the shape parameter. The distribution is defined for values of $x$ that satisfy the constraint $1 + \\xi \\frac{x - \\mu}{\\sigma} > 0$. The scale parameter must be positive, i.e., $\\sigma > 0$. The case $\\xi = 0$ corresponds to the Gumbel distribution (Type I), $\\xi > 0$ corresponds to the Fréchet distribution (Type II, heavy-tailed), and $\\xi  0$ corresponds to the reversed Weibull distribution (Type III, bounded upper tail).\n\nTo perform Maximum Likelihood Estimation, we first need the probability density function (PDF), $f(x; \\mu, \\sigma, \\xi) = \\frac{d}{dx}F(x; \\mu, \\sigma, \\xi)$. Differentiating the CDF yields:\nFor $\\xi \\neq 0$:\n$$ f(x; \\mu, \\sigma, \\xi) = \\frac{1}{\\sigma} \\left[1 + \\xi \\left(\\frac{x - \\mu}{\\sigma}\\right)\\right]^{-\\left(1 + \\frac{1}{\\xi}\\right)} \\exp\\left(-\\left[1 + \\xi \\left(\\frac{x - \\mu}{\\sigma}\\right)\\right]^{-1/\\xi}\\right) $$\nFor $\\xi = 0$:\n$$ f(x; \\mu, \\sigma, 0) = \\frac{1}{\\sigma} \\exp\\left(-\\frac{x - \\mu}{\\sigma}\\right) \\exp\\left(-\\exp\\left(-\\frac{x - \\mu}{\\sigma}\\right)\\right) $$\nGiven a sample of $n$ independent annual maxima, $x_1, x_2, \\ldots, x_n$, the likelihood function is the product of the individual densities: $L(\\mu, \\sigma, \\xi | \\mathbf{x}) = \\prod_{i=1}^n f(x_i; \\mu, \\sigma, \\xi)$. It is computationally more convenient to work with the log-likelihood function, $\\ell = \\ln L$. For $\\xi \\neq 0$, the log-likelihood is:\n$$ \\ell(\\mu, \\sigma, \\xi) = -n \\ln \\sigma - \\sum_{i=1}^{n} \\left(1 + \\frac{1}{\\xi}\\right) \\ln\\left[1 + \\xi \\left(\\frac{x_i - \\mu}{\\sigma}\\right)\\right] - \\sum_{i=1}^{n} \\left[1 + \\xi \\left(\\frac{x_i - \\mu}{\\sigma}\\right)\\right]^{-1/\\xi} $$\nFor $\\xi = 0$:\n$$ \\ell(\\mu, \\sigma, 0) = -n \\ln \\sigma - \\sum_{i=1}^{n} \\left(\\frac{x_i - \\mu}{\\sigma}\\right) - \\sum_{i=1}^{n} \\exp\\left(-\\left(\\frac{x_i - \\mu}{\\sigma}\\right)\\right) $$\nThe MLE parameter estimates $(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\xi})$ are the values that maximize $\\ell$. This is a numerical optimization problem, typically solved by minimizing the negative log-likelihood, $-\\ell(\\mu, \\sigma, \\xi)$. The optimization must be performed subject to the constraints $\\sigma > 0$ and $1 + \\xi (x_i - \\mu)/\\sigma > 0$ for all data points $x_i, i=1, \\dots, n$.\n\nThe second task is to compute the $T$-year return level, $z_T$. This is the value that is expected to be exceeded on average once every $T$ years. Its exceedance probability in any given year is $p = 1/T$. Thus, $z_T$ is the quantile of the distribution defined by $F(z_T) = 1-p = 1 - 1/T$. To find $z_T$, we invert the CDF:\nFor $\\xi \\neq 0$, solving $F(z_T) = 1 - 1/T$ for $z_T$ yields:\n$$ z_T = \\mu + \\frac{\\sigma}{\\xi} \\left[ \\left(-\\ln\\left(1 - \\frac{1}{T}\\right)\\right)^{-\\xi} - 1 \\right] $$\nFor the case $\\xi \\to 0$, we can take the limit of the above expression or directly invert the Gumbel CDF. Both methods yield the same result:\n$$ z_T = \\mu - \\sigma \\ln\\left(-\\ln\\left(1 - \\frac{1}{T}\\right)\\right) $$\nFor this problem, we are interested in the $100$-year return level, so we set $T=100$.\n\nThe implementation strategy involves creating a Python function that computes the negative log-likelihood for a given set of parameters and data. This function incorporates checks for the parameter constraints, returning a large value (penalty) if they are violated. We then use a numerical optimization routine, specifically `scipy.optimize.minimize` with the derivative-free Nelder-Mead method, to find the parameters $(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\xi})$ that minimize this function. To improve the robustness of the optimization, initial parameter guesses are derived from the method of moments for the Gumbel distribution. Once the optimal parameters are estimated, the $100$-year return level, $z_{100}$, is calculated using the appropriate formula derived above, depending on whether the estimated shape parameter $\\hat{\\xi}$ is close to zero. Finally, all results are rounded to $3$ decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Estimates GEV parameters and computes the 100-year return level for three datasets.\n    \"\"\"\n    \n    # Define the datasets of annual block maxima\n    datasets = {\n        'A': np.array([91.2, 104.5, 97.8, 112.3, 85.7, 123.4, 146.1, 132.7, 108.9, 119.5, 140.2, 88.6, 101.3, 125.9, 167.4, 154.3, 173.2, 189.8, 205.1, 221.7, 236.8, 249.5, 263.4, 278.9, 294.1, 313.5, 295.6, 271.4, 242.1, 210.4, 195.7, 176.5, 158.4, 149.2, 133.8, 121.6, 111.1, 102.5, 95.3, 89.7, 84.1, 79.9, 76.4, 74.0, 71.8, 69.7, 68.1, 67.2, 66.5, 65.9]),\n        'B': np.array([82.3, 95.6, 110.2, 120.7, 135.3, 98.4, 112.1, 126.8, 141.5, 155.9, 168.3, 180.5, 92.7, 105.4, 117.9, 130.6, 143.2, 156.7, 170.1, 183.4, 97.8, 109.6, 123.3, 136.9, 149.7, 162.4, 175.8, 188.6, 101.2, 114.5, 127.1, 139.8, 152.6, 165.4, 178.2, 190.9, 106.3, 118.9, 131.5, 144.2, 156.8, 169.4, 181.9, 194.6, 89.4, 102.1, 115.7, 128.3, 141.0, 154.6]),\n        'C': np.array([92.1, 104.3, 116.5, 128.2, 139.8, 151.1, 162.4, 173.6, 98.7, 110.5, 122.3, 134.0, 145.6, 157.3, 168.9, 179.5, 95.4, 107.2, 118.9, 130.7, 142.5, 154.1, 165.8, 176.4, 100.2, 112.0, 123.7, 135.3, 146.9, 158.6, 169.3, 179.1, 96.9, 108.6, 120.3, 132.0, 143.7, 155.4, 167.1, 178.8, 91.5, 103.2, 114.9, 126.6, 138.3, 149.9, 161.6, 173.3, 85.7, 97.4])\n    }\n\n    def neg_log_likelihood(params, data):\n        \"\"\"\n        Calculates the negative log-likelihood of the GEV distribution.\n        \"\"\"\n        mu, sigma, xi = params\n        n = len(data)\n        \n        # Constraint: sigma > 0\n        if sigma = 0:\n            return np.inf\n\n        # Scaled variable and GEV argument\n        y = (data - mu) / sigma\n        s = 1 + xi * y\n\n        # Constraint: 1 + xi * y > 0\n        if np.any(s = 0):\n            return np.inf\n\n        # Handle Gumbel case (xi -> 0)\n        if np.abs(xi)  1e-6:\n            log_likelihood = -n * np.log(sigma) - np.sum(y) - np.sum(np.exp(-y))\n        else: # General GEV case\n            log_likelihood = -n * np.log(sigma) - (1 + 1/xi) * np.sum(np.log(s)) - np.sum(s**(-1/xi))\n        \n        if np.isnan(log_likelihood) or not np.isfinite(log_likelihood):\n            return np.inf\n            \n        return -log_likelihood\n\n    def calculate_return_level(mu, sigma, xi, T):\n        \"\"\"\n        Calculates the T-year return level.\n        \"\"\"\n        p = 1.0 / T\n        y_p = -np.log(1 - p)\n        \n        if np.abs(xi)  1e-6:\n            z_T = mu - sigma * np.log(y_p)\n        else:\n            z_T = mu + (sigma / xi) * (np.power(y_p, -xi) - 1)\n        return z_T\n\n    results_for_print = []\n    \n    # Process each dataset\n    for key in ['A', 'B', 'C']:\n        data = datasets[key]\n        \n        # Initial parameter guess using method of moments for Gumbel\n        sample_mean = np.mean(data)\n        sample_std = np.std(data, ddof=1)\n        sigma0 = sample_std * np.sqrt(6) / np.pi\n        mu0 = sample_mean - 0.5772156649 * sigma0  # Euler-Mascheroni constant\n        xi0 = 0.1\n        initial_params = [mu0, sigma0, xi0]\n        \n        # Perform minimization\n        res = minimize(\n            neg_log_likelihood, \n            initial_params, \n            args=(data,), \n            method='Nelder-Mead',\n            options={'maxiter': 5000, 'xatol': 1e-8, 'fatol': 1e-8}\n        )\n        \n        mu_hat, sigma_hat, xi_hat = res.x\n        \n        # Calculate 100-year return level\n        z100 = calculate_return_level(mu_hat, sigma_hat, xi_hat, T=100)\n        \n        # Round results to 3 decimal places\n        mu_r = round(mu_hat, 3)\n        sigma_r = round(sigma_hat, 3)\n        xi_r = round(xi_hat, 3)\n        z100_r = round(z100, 3)\n        \n        # Format for final output string. str(list) adds spaces which we remove.\n        result_list_str = str([mu_r, sigma_r, xi_r, z100_r]).replace(\" \", \"\")\n        results_for_print.append(result_list_str)\n\n    # Print the final result in the exact required format\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the block maxima method is robust, it can be inefficient as it only uses one data point per block, such as one per year. This practice introduces the Peaks-Over-Threshold (POT) approach, a more data-rich alternative that analyzes all events exceeding a high threshold by fitting them to a Generalized Pareto Distribution (GPD). You will explore an alternative estimation technique, the method of L-moments, which is often more robust for small samples, and learn how to model the frequency and magnitude of extremes jointly to compute return levels .",
            "id": "4066104",
            "problem": "You are tasked with implementing a numerical procedure, suitable for extreme value analysis in numerical weather prediction and climate modeling, to estimate return levels of wind speed based on exceedances above a high threshold using a Peaks-Over-Threshold approach grounded in Extreme Value Theory (EVT). The core modeling assumptions must be derived from first principles: threshold exceedances follow a Generalized Pareto Distribution (GPD) under the Peaks-Over-Threshold limit, and the occurrence of exceedances in time is modeled by a homogeneous Poisson process.\n\nStarting from these bases, you must design an algorithm that:\n- Uses Probability-Weighted Moments (PWMs), specifically their linear combinations known as L-moments, to estimate the shape parameter $\\xi$ and scale parameter $\\beta$ of the GPD from exceedance data.\n- Uses the Poisson rate of exceedances per unit time to convert parameter estimates into a return level for a specified return period.\n- Implements a special-case limit for the exponential distribution when $\\xi$ is near zero.\n\nYour implementation must not rely on prepackaged estimation routines and must explicitly compute the sample L-moments from order statistics. Use the following definitions and requirements.\n\nDefinitions and requirements:\n- Let the exceedances be $y_i = x_i - u$ for observed wind speed $x_i$ above a fixed threshold $u$, with $y_i \\ge 0$.\n- Assume exceedance times follow a homogeneous Poisson process with rate $\\lambda$ (in events per year). If $N$ exceedances are observed over $T$ years, then $\\lambda = N/T$.\n- For the GPD, denote the cumulative distribution function by $F(y)$ for $y \\ge 0$ and the quantile function by $Q(p)$ for $p \\in (0,1)$.\n- Use sample L-moments for estimation:\n  - The first L-moment $\\ell_1$ equals the sample mean of exceedances: $\\ell_1 = \\frac{1}{n}\\sum_{i=1}^{n} y_{i:n}$, where $y_{i:n}$ are the order statistics in ascending order.\n  - The second L-moment $\\ell_2$ is computed from order statistics using the shifted Legendre weighting $L_1(p)=2p-1$, whose unbiased sample estimator is\n    $$\\ell_2 = \\frac{1}{n(n-1)} \\sum_{i=1}^{n} \\left(2i - n - 1\\right) y_{i:n}.$$\n- Derive the estimator for the GPD parameters $(\\xi,\\beta)$ using these L-moments, starting from the integral definitions of L-moments and using the quantile representation of the GPD.\n- Define the return level $z_P$ as the wind speed level (in m/s) expected to be exceeded once on average over a period of length $P$ years under the Poisson-GPD model, and derive the corresponding formula from first principles. Explicitly implement the appropriate limit when $\\xi \\to 0$.\n\nUnits and numerical specifications:\n- All wind speed quantities must be expressed in meters per second (m/s). The final return levels must be printed in m/s, rounded to three decimal places.\n- Angles are not involved in this problem.\n- No percentages are allowed; any probability-related quantity must be expressed as a decimal.\n\nTest suite and data generation:\nTo ensure reproducibility without randomness, generate the exceedance samples deterministically using the appropriate quantile functions evaluated at evenly spaced probabilities $p_i = \\frac{i - 0.5}{n}$ for $i = 1,\\dots,n$. Use the following three test cases:\n\n- Test Case 1 (heavy tail, happy path):\n  - Threshold: $u = 20$ m/s.\n  - True parameters for synthetic generation: $\\xi_{\\text{true}} = 0.2$, $\\beta_{\\text{true}} = 5$ m/s.\n  - Sample size: $n = 60$.\n  - Observation period: $T = 12$ years.\n  - Generate exceedances via the GPD quantile function $Q(p)$.\n  - Return period: $P = 50$ years.\n\n- Test Case 2 (near-exponential, boundary behavior):\n  - Threshold: $u = 25$ m/s.\n  - True parameters for synthetic generation: $\\xi_{\\text{true}} = 0$ (use the exponential limit), $\\beta_{\\text{true}} = 8$ m/s.\n  - Sample size: $n = 80$.\n  - Observation period: $T = 16$ years.\n  - Generate exceedances via the exponential limit of the GPD quantile: $Q(p) = -\\beta_{\\text{true}} \\ln(1 - p)$.\n  - Return period: $P = 100$ years.\n\n- Test Case 3 (bounded tail, negative shape):\n  - Threshold: $u = 18$ m/s.\n  - True parameters for synthetic generation: $\\xi_{\\text{true}} = -0.2$, $\\beta_{\\text{true}} = 6$ m/s.\n  - Sample size: $n = 40$.\n  - Observation period: $T = 8$ years.\n  - Generate exceedances via the GPD quantile function $Q(p)$ with negative $\\xi_{\\text{true}}$.\n  - Return period: $P = 200$ years.\n\nYour program should produce a single line of output containing the return levels for the three test cases, in m/s, rounded to three decimal places, aggregated into a comma-separated list enclosed in square brackets. For example: \"[z1,z2,z3]\". The output must be exactly one line, with no additional text.",
            "solution": "The problem is valid as it is scientifically grounded in Extreme Value Theory (EVT), is well-posed with a clear objective, and provides all necessary information for a unique, deterministic solution. The task is to implement a Peaks-Over-Threshold (POT) analysis to estimate wind speed return levels. The solution is developed from first principles as required.\n\nThe analysis is based on a composite model where exceedances of a high threshold $u$ follow a Generalized Pareto Distribution (GPD), and the occurrences of these exceedances are described by a homogeneous Poisson process.\n\nThe cumulative distribution function (CDF) of the GPD for an exceedance $y \\geq 0$ is given by:\n$$ F(y; \\xi, \\beta) = \\begin{cases} 1 - \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-1/\\xi}  \\text{if } \\xi \\neq 0 \\\\ 1 - \\exp\\left(-\\frac{y}{\\beta}\\right)  \\text{if } \\xi = 0 \\end{cases} $$\nwhere $\\xi$ is the shape parameter and $\\beta  0$ is the scale parameter. The support of the distribution is $y \\geq 0$ for $\\xi \\geq 0$ and $0 \\leq y \\leq -\\beta/\\xi$ for $\\xi  0$.\n\nThe corresponding quantile function $Q(p)$, which gives the value $y$ such that $F(y)=p$, is found by inverting the CDF:\n$$ Q(p; \\xi, \\beta) = \\begin{cases} \\frac{\\beta}{\\xi}\\left((1-p)^{-\\xi} - 1\\right)  \\text{if } \\xi \\neq 0 \\\\ -\\beta\\ln(1-p)  \\text{if } \\xi = 0 \\end{cases} $$\nfor $p \\in (0,1)$. This function is used to generate the deterministic exceedance samples $y_i$ from a set of uniform probabilities $p_i = (i-0.5)/n$.\n\nThe parameters $(\\xi, \\beta)$ are estimated using the method of L-moments. The first two theoretical L-moments, $L_1$ and $L_2$, are linear combinations of probability-weighted moments, defined by integrals of the quantile function:\n$$ L_1 = \\int_0^1 Q(p) \\, dp $$\n$$ L_2 = \\int_0^1 Q(p) (2p-1) \\, dp $$\nFor the GPD, these integrals can be evaluated analytically for $\\xi  1$. Substituting the GPD quantile function $Q(p)$ for $\\xi \\neq 0$ and performing the integration yields:\n$$ L_1 = \\frac{\\beta}{1-\\xi} $$\n$$ L_2 = \\frac{\\beta}{(1-\\xi)(2-\\xi)} $$\nThe ratio of the first two L-moments, $\\tau_2 = L_2/L_1$, provides a direct relationship to the shape parameter $\\xi$:\n$$ \\tau_2 = \\frac{L_2}{L_1} = \\frac{\\beta/((1-\\xi)(2-\\xi))}{\\beta/(1-\\xi)} = \\frac{1}{2-\\xi} $$\nSolving for $\\xi$ gives $\\xi = 2 - 1/\\tau_2$. Subsequently, solving for $\\beta$ using the expression for $L_1$ gives $\\beta = L_1(1-\\xi)$.\n\nThe method of L-moments estimates the parameters by replacing the theoretical L-moments $(L_1, L_2)$ with their sample counterparts $(\\ell_1, \\ell_2)$.\nThe sample L-moments are calculated from the ordered exceedance data $y_{1:n} \\le y_{2:n} \\le \\dots \\le y_{n:n}$:\n- The first sample L-moment, $\\ell_1$, is the sample mean: $\\ell_1 = \\frac{1}{n} \\sum_{i=1}^n y_{i:n}$.\n- The second sample L-moment, $\\ell_2$, is estimated by: $\\ell_2 = \\frac{1}{n(n-1)} \\sum_{i=1}^n (2i - n - 1) y_{i:n}$.\n\nThe L-moment estimators for the GPD parameters are therefore:\n$$ \\hat{\\tau}_2 = \\frac{\\ell_2}{\\ell_1} $$\n$$ \\hat{\\xi} = 2 - \\frac{1}{\\hat{\\tau}_2} = 2 - \\frac{\\ell_1}{\\ell_2} $$\n$$ \\hat{\\beta} = \\ell_1 (1-\\hat{\\xi}) $$\nThese estimators are implemented directly in the algorithm.\n\nThe final step is to calculate the $P$-year return level, $z_P$. This is the wind speed expected to be exceeded on average once every $P$ years. The rate of threshold exceedances is modeled as a Poisson process with rate $\\lambda_u = N/T$ events per year, where $N$ is the number of exceedances observed over $T$ years. In our case, $N=n$. The probability of an arbitrary exceedance $y$ being greater than some value $y_p$ is given by the GPD survival function, $S(y_p) = 1 - F(y_p)$.\n\nThe rate of exceedances of the level $z_P = u + y_P$ is given by thinning the Poisson process: $\\lambda_{z_P} = \\lambda_u S(y_P)$. For a return period of $P$ years, this rate must be $1/P$. Thus, we have the condition:\n$$ S(y_P) = \\frac{1}{\\lambda_u P} $$\nSubstituting the GPD survival function for $\\xi \\neq 0$:\n$$ \\left(1 + \\frac{\\xi y_P}{\\beta}\\right)^{-1/\\xi} = \\frac{1}{\\lambda_u P} $$\nSolving for $y_P$ yields:\n$$ y_P = \\frac{\\beta}{\\xi} \\left[ (\\lambda_u P)^{\\xi} - 1 \\right] $$\nThe return level is then $z_P = u + y_P$:\n$$ z_P = u + \\frac{\\beta}{\\xi} \\left[ (\\lambda_u P)^{\\xi} - 1 \\right] $$\nFor the special case where $\\xi \\to 0$ (the exponential distribution), we take the limit of the expression for $y_P$. Using the limit definition $\\lim_{h \\to 0} (a^h - 1)/h = \\ln(a)$:\n$$ \\lim_{\\xi \\to 0} y_P = \\lim_{\\xi \\to 0} \\beta \\frac{(\\lambda_u P)^{\\xi} - 1}{\\xi} = \\beta \\ln(\\lambda_u P) $$\nThis leads to the return level formula for the exponential case:\n$$ z_P = u + \\beta \\ln(\\lambda_u P) $$\nThe algorithm uses the estimated parameters $(\\hat{\\xi}, \\hat{\\beta})$ in these formulae. A numerical tolerance $|\\hat{\\xi}|  10^{-8}$ is used to switch to the logarithmic formula for stability. For robust computation when $\\xi$ is small but non-zero, the term $(m^{\\xi} - 1)$ is calculated as $\\text{expm1}(\\xi \\ln m)$, where $m = \\lambda_u P$. This avoids loss of precision. The final algorithm systematically implements these derived steps for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the extreme value analysis problem by deriving and implementing\n    a Peaks-Over-Threshold procedure using L-moment estimation.\n    \"\"\"\n\n    def calculate_return_level(u, xi_true, beta_true, n, T, P):\n        \"\"\"\n        Performs the complete Peaks-Over-Threshold analysis for a single test case.\n\n        Args:\n            u (float): The threshold for exceedances in m/s.\n            xi_true (float): The true shape parameter for data generation.\n            beta_true (float): The true scale parameter for data generation.\n            n (int): The number of exceedances (sample size).\n            T (int): The observation period in years.\n            P (int): The return period in years.\n\n        Returns:\n            float: The estimated return level z_P in m/s.\n        \"\"\"\n        # Step 1: Generate deterministic exceedance data based on GPD quantiles.\n        # The probabilities p_i = (i - 0.5) / n for i = 1, ..., n are used.\n        p_values = (np.arange(1, n + 1) - 0.5) / n\n        \n        if xi_true == 0:\n            # Case for exponential distribution (xi -> 0 limit of GPD)\n            # Q(p) = -beta * log(1-p)\n            # Using np.log1p for numerical stability as p -> 1.\n            y_samples = -beta_true * np.log1p(-p_values)\n        else:\n            # Case for GPD with xi != 0\n            # Q(p) = beta/xi * ((1-p)^(-xi) - 1)\n            # Using np.expm1 and np.log1p for stability.\n            y_samples = (beta_true / xi_true) * np.expm1(-xi_true * np.log1p(-p_values))\n            \n        # The generated samples are already sorted and represent the order statistics y_{i:n}.\n\n        # Step 2: Estimate GPD parameters using the method of L-moments.\n        # The first sample L-moment (l1) is the sample mean.\n        l1 = np.mean(y_samples)\n        \n        # The second sample L-moment (l2) is calculated from order statistics.\n        # The formula is l2 = 1/(n*(n-1)) * sum_{i=1 to n} (2i - n - 1) * y_{i:n}.\n        # For 0-based index j=i-1, the coefficient is (2(j+1) - n - 1) = (2j - n + 1).\n        if n > 1:\n            j_indices = np.arange(n)\n            l2_coeffs = 2 * j_indices - n + 1\n            l2_sum = np.sum(l2_coeffs * y_samples)\n            l2 = l2_sum / (n * (n - 1))\n        else: # Edge case for n=1, where l2 is undefined.\n            l2 = 0.0\n\n        # Now, derive parameter estimates from l1 and l2.\n        # An l2 near zero would make xi estimation unstable; this case is unlikely here.\n        if abs(l2)  1e-15:\n            xi_hat = 0.0  # Treat as exponential if l2 is effectively zero.\n        else:\n            # Estimator for xi from the L-moment ratio: xi = 2 - l1/l2\n            xi_hat = 2.0 - l1 / l2\n            \n        # Estimator for beta from l1 and xi_hat: beta = l1 * (1 - xi)\n        beta_hat = l1 * (1.0 - xi_hat)\n\n        # Step 3: Calculate the return level z_P.\n        # Poisson rate of threshold exceedances (events per year).\n        lambda_u = n / T\n        \n        # Mean number of exceedances over the return period P.\n        m = lambda_u * P\n        \n        # Use a small tolerance to select the appropriate formula for z_P\n        # to ensure numerical stability when xi_hat is close to zero.\n        if abs(xi_hat)  1e-8:\n            # Formula for the xi -> 0 limit (exponential case): y_P = beta * log(m)\n            y_p = beta_hat * np.log(m)\n        else:\n            # General GPD formula: y_P = beta/xi * (m^xi - 1)\n            # Implemented with np.expm1 for better precision.\n            y_p = (beta_hat / xi_hat) * np.expm1(xi_hat * np.log(m))\n            \n        z_p = u + y_p\n        \n        return z_p\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (u, xi_true, beta_true, n, T, P)\n        (20.0, 0.2, 5.0, 60, 12, 50),\n        (25.0,  0.0, 8.0, 80, 16, 100),\n        (18.0, -0.2, 6.0, 40, 8, 200),\n    ]\n\n    results = []\n    for case in test_cases:\n        u, xi_true, beta_true, n, T, P = case\n        z_p = calculate_return_level(u, xi_true, beta_true, n, T, P)\n        results.append(f\"{z_p:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}