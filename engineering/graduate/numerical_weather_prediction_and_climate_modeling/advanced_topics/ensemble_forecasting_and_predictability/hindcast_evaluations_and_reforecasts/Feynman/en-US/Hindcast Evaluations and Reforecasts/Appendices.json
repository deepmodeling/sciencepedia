{
    "hands_on_practices": [
        {
            "introduction": "The value of any forecast is measured by its improvement over simple, readily available benchmarks. This practice demonstrates how to quantify this improvement using the Root Mean Squared Error ($RMSE$) Skill Score, a standard metric in forecast verification. By calculating the skill of a hindcast against both climatology and persistence references, you will gain hands-on experience in establishing whether a model provides meaningful predictive value. ",
            "id": "4051836",
            "problem": "Consider the evaluation of weekly hindcasts for $2$-meter air temperature, expressed in degrees Celsius. The hindcasts are produced by a Numerical Weather Prediction (NWP) system and verified against weekly averaged observations. Two reference baselines are used: a climatology reference derived from a prior reforecast period and a persistence reference defined as the last available observed weekly mean prior to each forecast initialization. The objective is to compare these two references for weekly temperature hindcasts and to compute the skill score under Root Mean Squared Error (RMSE) for each reference across multiple lead times.\n\nFundamental base: For a set of forecasts and corresponding observations, the squared error for a single case is defined as the square of the difference between the forecast and the observation. The mean squared error (MSE) is defined as the arithmetic mean of the squared errors over all cases. The Root Mean Squared Error (RMSE) is defined as the square root of the MSE. The skill score under RMSE is a dimensionless quantity that quantifies improvement relative to a specified reference by normalizing RMSE with respect to the reference's RMSE.\n\nDataset description and test suite:\n- Number of independent hindcast cases: $N = 3$.\n- Number of lead weeks: $L = 4$, with lead times $\\ell \\in \\{1, 2, 3, 4\\}$.\n- Initialization weeks (week-of-year indices): [3, 20, 30]. For each case $i \\in \\{1, 2, 3\\}$, the verifying week-of-year for lead $\\ell$ is $w_{i,\\ell} = w_{0,i} + \\ell$.\n- Observation matrix $Y \\in \\mathbb{R}^{N \\times L}$ (weekly means in degrees Celsius):\n  - Case $1$: [2.5, 3.2, 4.1, 5.3]\n  - Case $2$: [15.2, 15.8, 16.9, 18.5]\n  - Case $3$: [22.3, 23.5, 24.7, 25.2]\n- Hindcast model matrix $H \\in \\mathbb{R}^{N \\times L}$ (weekly means in degrees Celsius):\n  - Case $1$: [2.0, 2.8, 4.5, 5.1]\n  - Case $2$: [14.5, 15.7, 17.2, 18.9]\n  - Case $3$: [22.7, 23.2, 24.2, 25.8]\n- Climatology reference matrix $C \\in \\mathbb{R}^{N \\times L}$ (weekly means in degrees Celsius), mapped to the verifying weeks:\n  - Case $1$ (verifying weeks [4, 5, 6, 7]): [2.0, 3.0, 4.0, 5.0]\n  - Case $2$ (verifying weeks [21, 22, 23, 24]): [15.0, 16.0, 17.0, 18.0]\n  - Case $3$ (verifying weeks [31, 32, 33, 34]): [22.0, 23.0, 24.0, 25.0]\n- Persistence reference vector $P \\in \\mathbb{R}^{N}$ (degrees Celsius), equal to the weekly mean observed temperature in the week preceding each initialization:\n  - Case $1$: $1.8$\n  - Case $2$: $14.9$\n  - Case $3$: $22.1$\n  For each lead $\\ell$, the persistence forecast remains equal to $P_i$ for case $i$.\n\nTasks:\n1. For each lead $\\ell \\in \\{1, 2, 3, 4\\}$, compute $RMSE_{model}(\\ell)$ across the $N$ cases using $H_{\\cdot,\\ell}$ and $Y_{\\cdot,\\ell}$, expressed in degrees Celsius.\n2. For each lead $\\ell$, compute $RMSE_{clim}(\\ell)$ across the $N$ cases using $C_{\\cdot,\\ell}$ and $Y_{\\cdot,\\ell}$, expressed in degrees Celsius.\n3. For each lead $\\ell$, compute $RMSE_{pers}(\\ell)$ across the $N$ cases using $P$ and $Y_{\\cdot,\\ell}$, expressed in degrees Celsius.\n4. Using the RMSE values, compute the skill score under RMSE for each reference and lead. Express every skill score as a decimal number (no percentage sign), rounded to $5$ decimal places.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with two sublists ordered by lead $\\ell = 1$ to $\\ell = 4$:\n- First sublist: $[\\,SS_{clim}(1),\\,SS_{clim}(2),\\,SS_{clim}(3),\\,SS_{clim}(4)\\,]$.\n- Second sublist: $[\\,SS_{pers}(1),\\,SS_{pers}(2),\\,SS_{pers}(3),\\,SS_{pers}(4)\\,]$.\nFor example: $[[s_1,s_2,s_3,s_4],[t_1,t_2,t_3,t_4]]$.\n\nScientific realism and coverage:\n- The dataset includes winter, late spring, and midsummer weeks ([3, 20, 30]) to cover seasonal variability.\n- The test suite simultaneously probes a typical case, relative disadvantages of persistence at longer leads, and the possibility of negative skill relative to climatology at short leads.\n- All quantities involving physical units must be reported in degrees Celsius as specified; skill scores are dimensionless and must be reported as decimals.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard exercise in the verification of numerical weather prediction hindcasts.\n\nThe objective is to compute the Root Mean Squared Error (RMSE) skill scores for a set of hindcasts against two reference forecasts: climatology and persistence. The solution proceeds by first defining the necessary statistical quantities and then systematically applying them to the provided dataset for each lead time.\n\nThe Mean Squared Error (MSE) for a set of $N$ forecasts $F_i$ and corresponding observations $Y_i$ is given by:\n$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (F_i - Y_i)^2$$\nThe Root Mean Squared Error (RMSE) is the square root of the MSE:\n$$RMSE = \\sqrt{MSE}$$\nThe skill score (SS) of a model forecast relative to a reference forecast is defined as:\n$$SS = 1 - \\frac{\\text{RMSE}_{\\text{model}}}{\\text{RMSE}_{\\text{ref}}}$$\nA positive skill score indicates that the model is more accurate than the reference, a negative score indicates it is less accurate, and a score of $0$ implies equal accuracy. A perfect forecast would have $RMSE_{\\text{model}}=0$, yielding a skill score of $1$.\n\nThe given data are:\n- Number of cases $N = 3$.\n- Observation matrix $Y = \\begin{pmatrix} 2.5 & 3.2 & 4.1 & 5.3 \\\\ 15.2 & 15.8 & 16.9 & 18.5 \\\\ 22.3 & 23.5 & 24.7 & 25.2 \\end{pmatrix}$.\n- Hindcast model matrix $H = \\begin{pmatrix} 2.0 & 2.8 & 4.5 & 5.1 \\\\ 14.5 & 15.7 & 17.2 & 18.9 \\\\ 22.7 & 23.2 & 24.2 & 25.8 \\end{pmatrix}$.\n- Climatology reference matrix $C = \\begin{pmatrix} 2.0 & 3.0 & 4.0 & 5.0 \\\\ 15.0 & 16.0 & 17.0 & 18.0 \\\\ 22.0 & 23.0 & 24.0 & 25.0 \\end{pmatrix}$.\n- Persistence reference vector $P = \\begin{pmatrix} 1.8 \\\\ 14.9 \\\\ 22.1 \\end{pmatrix}$.\n\nThe calculations are performed for each lead week $\\ell \\in \\{1, 2, 3, 4\\}$.\n\n**Lead Week $\\ell=1$**\nThe data vectors for this lead are:\n- Observations: $Y_{\\cdot,1} = [2.5, 15.2, 22.3]$\n- Model: $H_{\\cdot,1} = [2.0, 14.5, 22.7]$\n- Climatology: $C_{\\cdot,1} = [2.0, 15.0, 22.0]$\n- Persistence: $P = [1.8, 14.9, 22.1]$\n\nThe squared errors are computed for each case $i \\in \\{1,2,3\\}$:\n- $SE_{\\text{model}}(1) = [ (2.0-2.5)^2, (14.5-15.2)^2, (22.7-22.3)^2 ] = [0.25, 0.49, 0.16]$\n- $SE_{\\text{clim}}(1) = [ (2.0-2.5)^2, (15.0-15.2)^2, (22.0-22.3)^2 ] = [0.25, 0.04, 0.09]$\n- $SE_{\\text{pers}}(1) = [ (1.8-2.5)^2, (14.9-15.2)^2, (22.1-22.3)^2 ] = [0.49, 0.09, 0.04]$\n\nThe MSE and RMSE values are:\n- $MSE_{\\text{model}}(1) = \\frac{0.25+0.49+0.16}{3} = 0.30 \\implies RMSE_{\\text{model}}(1) \\approx 0.54772$\n- $MSE_{\\text{clim}}(1) = \\frac{0.25+0.04+0.09}{3} \\approx 0.12667 \\implies RMSE_{\\text{clim}}(1) \\approx 0.35590$\n- $MSE_{\\text{pers}}(1) = \\frac{0.49+0.09+0.04}{3} \\approx 0.20667 \\implies RMSE_{\\text{pers}}(1) \\approx 0.45461$\n\nThe skill scores for lead $1$ are:\n- $SS_{\\text{clim}}(1) = 1 - \\frac{0.54772}{0.35590} \\approx -0.53901$\n- $SS_{\\text{pers}}(1) = 1 - \\frac{0.54772}{0.45461} \\approx -0.20483$\n\n**Lead Week $\\ell=2$**\n- $Y_{\\cdot,2} = [3.2, 15.8, 23.5]$\n- $H_{\\cdot,2} = [2.8, 15.7, 23.2]$\n- $C_{\\cdot,2} = [3.0, 16.0, 23.0]$\n- $P = [1.8, 14.9, 22.1]$\n\nThe MSE and RMSE values are:\n- $MSE_{\\text{model}}(2) = \\frac{(2.8-3.2)^2+(15.7-15.8)^2+(23.2-23.5)^2}{3} = \\frac{0.16+0.01+0.09}{3} \\approx 0.08667 \\implies RMSE_{\\text{model}}(2) \\approx 0.29439$\n- $MSE_{\\text{clim}}(2) = \\frac{(3.0-3.2)^2+(16.0-15.8)^2+(23.0-23.5)^2}{3} = \\frac{0.04+0.04+0.25}{3} = 0.11 \\implies RMSE_{\\text{clim}}(2) \\approx 0.33166$\n- $MSE_{\\text{pers}}(2) = \\frac{(1.8-3.2)^2+(14.9-15.8)^2+(22.1-23.5)^2}{3} = \\frac{1.96+0.81+1.96}{3} \\approx 1.57667 \\implies RMSE_{\\text{pers}}(2) \\approx 1.25565$\n\nThe skill scores for lead $2$ are:\n- $SS_{\\text{clim}}(2) = 1 - \\frac{0.29439}{0.33166} \\approx 0.11234$\n- $SS_{\\text{pers}}(2) = 1 - \\frac{0.29439}{1.25565} \\approx 0.76555$\n\n**Lead Week $\\ell=3$**\n- $Y_{\\cdot,3} = [4.1, 16.9, 24.7]$\n- $H_{\\cdot,3} = [4.5, 17.2, 24.2]$\n- $C_{\\cdot,3} = [4.0, 17.0, 24.0]$\n- $P = [1.8, 14.9, 22.1]$\n\nThe MSE and RMSE values are:\n- $MSE_{\\text{model}}(3) = \\frac{(4.5-4.1)^2+(17.2-16.9)^2+(24.2-24.7)^2}{3} = \\frac{0.16+0.09+0.25}{3} \\approx 0.16667 \\implies RMSE_{\\text{model}}(3) \\approx 0.40825$\n- $MSE_{\\text{clim}}(3) = \\frac{(4.0-4.1)^2+(17.0-16.9)^2+(24.0-24.7)^2}{3} = \\frac{0.01+0.01+0.49}{3} = 0.17 \\implies RMSE_{\\text{clim}}(3) \\approx 0.41231$\n- $MSE_{\\text{pers}}(3) = \\frac{(1.8-4.1)^2+(14.9-16.9)^2+(22.1-24.7)^2}{3} = \\frac{5.29+4.00+6.76}{3} = 5.35 \\implies RMSE_{\\text{pers}}(3) \\approx 2.31301$\n\nThe skill scores for lead $3$ are:\n- $SS_{\\text{clim}}(3) = 1 - \\frac{0.40825}{0.41231} \\approx 0.00985$\n- $SS_{\\text{pers}}(3) = 1 - \\frac{0.40825}{2.31301} \\approx 0.82350$\n\n**Lead Week $\\ell=4$**\n- $Y_{\\cdot,4} = [5.3, 18.5, 25.2]$\n- $H_{\\cdot,4} = [5.1, 18.9, 25.8]$\n- $C_{\\cdot,4} = [5.0, 18.0, 25.0]$\n- $P = [1.8, 14.9, 22.1]$\n\nThe MSE and RMSE values are:\n- $MSE_{\\text{model}}(4) = \\frac{(5.1-5.3)^2+(18.9-18.5)^2+(25.8-25.2)^2}{3} = \\frac{0.04+0.16+0.36}{3} \\approx 0.18667 \\implies RMSE_{\\text{model}}(4) \\approx 0.43205$\n- $MSE_{\\text{clim}}(4) = \\frac{(5.0-5.3)^2+(18.0-18.5)^2+(25.0-25.2)^2}{3} = \\frac{0.09+0.25+0.04}{3} \\approx 0.12667 \\implies RMSE_{\\text{clim}}(4) \\approx 0.35590$\n- $MSE_{\\text{pers}}(4) = \\frac{(1.8-5.3)^2+(14.9-18.5)^2+(22.1-25.2)^2}{3} = \\frac{12.25+12.96+9.61}{3} \\approx 11.60667 \\implies RMSE_{\\text{pers}}(4) \\approx 3.40686$\n\nThe skill scores for lead $4$ are:\n- $SS_{\\text{clim}}(4) = 1 - \\frac{0.43205}{0.35590} \\approx -0.21396$\n- $SS_{\\text{pers}}(4) = 1 - \\frac{0.43205}{3.40686} \\approx 0.87318$\n\n**Summary of Results**\nThe calculated skill scores, rounded to $5$ decimal places, are:\n\n- Skill Score vs. Climatology:\n  - $SS_{\\text{clim}}(1) = -0.53901$\n  - $SS_{\\text{clim}}(2) = 0.11234$\n  - $SS_{\\text{clim}}(3) = 0.00985$\n  - $SS_{\\text{clim}}(4) = -0.21396$\n\n- Skill Score vs. Persistence:\n  - $SS_{\\text{pers}}(1) = -0.20483$\n  - $SS_{\\text{pers}}(2) = 0.76555$\n  - $SS_{\\text{pers}}(3) = 0.82350$\n  - $SS_{\\text{pers}}(4) = 0.87318$\n\nThese results demonstrate typical characteristics of forecast skill. The skill versus persistence starts negative, indicating that for a short lead of one week, a simple persistence forecast is more accurate than the NWP model for this small sample. However, the skill rapidly increases with lead time, showing the model's value in forecasting the evolution of the weather state. The skill versus climatology is more variable and remains low (even negative), suggesting the model provides little to no improvement over the long-term average at these lead times for these specific cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes hindcast skill scores for a numerical weather prediction model\n    against climatology and persistence references.\n    \"\"\"\n    # Number of independent hindcast cases\n    N = 3\n    # Number of lead weeks\n    L = 4\n    \n    # Observation matrix Y (N x L)\n    Y = np.array([\n        [2.5, 3.2, 4.1, 5.3],\n        [15.2, 15.8, 16.9, 18.5],\n        [22.3, 23.5, 24.7, 25.2]\n    ])\n\n    # Hindcast model matrix H (N x L)\n    H = np.array([\n        [2.0, 2.8, 4.5, 5.1],\n        [14.5, 15.7, 17.2, 18.9],\n        [22.7, 23.2, 24.2, 25.8]\n    ])\n\n    # Climatology reference matrix C (N x L)\n    C = np.array([\n        [2.0, 3.0, 4.0, 5.0],\n        [15.0, 16.0, 17.0, 18.0],\n        [22.0, 23.0, 24.0, 25.0]\n    ])\n\n    # Persistence reference vector P (N)\n    P = np.array([1.8, 14.9, 22.1])\n\n    ss_clim_list = []\n    ss_pers_list = []\n\n    # Iterate through each lead time from l=1 to L=4 (indices 0 to 3)\n    for lead_idx in range(L):\n        # Extract the column vectors for the current lead time\n        y_lead = Y[:, lead_idx]\n        h_lead = H[:, lead_idx]\n        c_lead = C[:, lead_idx]\n        # The persistence forecast is constant for all leads\n        p_lead = P\n\n        # Calculate Mean Squared Error (MSE) for model, climatology, and persistence\n        mse_model = np.mean((h_lead - y_lead)**2)\n        mse_clim = np.mean((c_lead - y_lead)**2)\n        mse_pers = np.mean((p_lead - y_lead)**2)\n\n        # Calculate Root Mean Squared Error (RMSE)\n        rmse_model = np.sqrt(mse_model)\n        rmse_clim = np.sqrt(mse_clim)\n        rmse_pers = np.sqrt(mse_pers)\n\n        # Calculate the skill score (SS) under RMSE\n        # SS = 1 - (RMSE_model / RMSE_reference)\n        ss_clim = 1 - (rmse_model / rmse_clim)\n        ss_pers = 1 - (rmse_model / rmse_pers)\n\n        # Append the rounded results to their respective lists\n        ss_clim_list.append(round(ss_clim, 5))\n        ss_pers_list.append(round(ss_pers, 5))\n\n    # Format the final output string as specified\n    output_str = f\"[[{','.join(map(str, ss_clim_list))}],[{','.join(map(str, ss_pers_list))}]]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A simple error metric like the Root Mean Squared Error ($RMSE$) on absolute temperature values can sometimes mask a model's true ability to predict important deviations from the long-term average. This exercise guides you through calculating metrics in both absolute and anomaly space to distinguish a model's skill in representing the background climate versus its skill in capturing specific weather patterns. This practice is crucial for diagnosing model behavior and understanding scores like the Anomaly Correlation Coefficient ($ACC$). ",
            "id": "4051766",
            "problem": "You are given a hindcast verification task designed to compare verification in absolute space and anomaly space using the same reforecast set. A reforecast time series comprises a model forecast sequence and a verifying observation sequence over a fixed period. For each test case, you are provided with four sequences of equal length: a model forecast sequence $F_t$, an observed sequence $O_t$, a model climatology sequence $C^{(f)}_t$, and an observed climatology sequence $C^{(o)}_t$ over times $t = 1, \\dots, N$. The anomaly sequences are defined by subtracting the corresponding climatologies, i.e., forecast anomalies $F^{\\prime}_t = F_t - C^{(f)}_t$ and observed anomalies $O^{\\prime}_t = O_t - C^{(o)}_t$. Your task is to compute, for each test case, two metrics under both representations (absolute values and anomalies) and report them in a fixed format.\n\nUse the following foundational definitions:\n- The Root Mean Square Error (RMSE) is defined as\n$$\n\\mathrm{RMSE}(X, Y) = \\sqrt{\\frac{1}{N} \\sum_{t=1}^{N} \\left( X_t - Y_t \\right)^2}.\n$$\n- The Anomaly Correlation Coefficient (ACC) is defined as the Pearson correlation coefficient, computed here as\n$$\n\\mathrm{ACC}(X, Y) = \\frac{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)\\left( Y_t - \\bar{Y} \\right)}{\\sqrt{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)^2} \\sqrt{\\sum_{t=1}^{N} \\left( Y_t - \\bar{Y} \\right)^2}},\n$$\nwhere $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^{N} X_t$ and $\\bar{Y} = \\frac{1}{N}\\sum_{t=1}^{N} Y_t$. In degenerate cases where the standard deviation of $X$ or $Y$ is zero (denominator equals zero), define $\\mathrm{ACC}(X, Y) = 0$.\n\nFor each test case, compute:\n1. Absolute-space RMSE: $\\mathrm{RMSE}(F, O)$.\n2. Absolute-space ACC: $\\mathrm{ACC}(F, O)$.\n3. Anomaly-space RMSE: $\\mathrm{RMSE}(F^{\\prime}, O^{\\prime})$.\n4. Anomaly-space ACC: $\\mathrm{ACC}(F^{\\prime}, O^{\\prime})$.\n\nAll values are unitless in this problem. Round each reported float to six decimal places.\n\nTest suite:\n- Shared observed climatology $C^{(o)}$ and observed anomalies $A^{(o)}$:\n  - $C^{(o)} = [0.0, 1.0, 2.0, 1.0, 0.0, -1.0, -2.0, -1.0]$,\n  - $A^{(o)} = [0.5, -0.5, 1.0, -1.0, 0.2, -0.2, 0.0, 0.0]$,\n  - Observations $O = C^{(o)} + A^{(o)}$.\n- Test case 1 (seasonal bias and amplitude error in model climatology; reasonably good anomaly forecast):\n  - $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$,\n  - $A^{(f)} = [0.4, -0.6, 0.9, -0.8, 0.3, -0.1, 0.1, -0.2]$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n- Test case 2 (climatology-only forecast; no forecasted anomalies):\n  - $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$,\n  - $A^{(f)} = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n- Test case 3 (perfect forecast; perfect climatology and perfect anomalies):\n  - $C^{(f)} = C^{(o)}$,\n  - $A^{(f)} = A^{(o)}$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n- Test case 4 (perfect climatology but anomalies with opposite sign):\n  - $C^{(f)} = C^{(o)}$,\n  - $A^{(f)} = - A^{(o)}$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists, one per test case, where each inner list is ordered as $[\\mathrm{RMSE}(F,O), \\mathrm{ACC}(F,O), \\mathrm{RMSE}(F^{\\prime},O^{\\prime}), \\mathrm{ACC}(F^{\\prime},O^{\\prime})]$. All floats must be rounded to six decimal places. For example, the overall structure should look like\n`[[x_11,x_12,x_13,x_14],[x_21,x_22,x_23,x_24],[x_31,x_32,x_33,x_34],[x_41,x_42,x_43,x_44]]`\nwith no additional text.",
            "solution": "The user has provided a well-defined hindcast verification problem. The task is to compute two standard verification metrics, the Root Mean Square Error (RMSE) and the Anomaly Correlation Coefficient (ACC), for four different forecast scenarios. The comparison is to be performed in both absolute space (using the full forecast and observation fields) and anomaly space (using deviations from climatology).\n\nFirst, the problem statement is validated.\n\n### Step 1: Extract Givens\n- **Sequences**: Forecast ($F_t$), Observation ($O_t$), Model Climatology ($C^{(f)}_t$), Observed Climatology ($C^{(o)}_t$), for $t=1, \\dots, N$.\n- **Anomalies**: Forecast Anomaly $F^{\\prime}_t = F_t - C^{(f)}_t$, Observed Anomaly $O^{\\prime}_t = O_t - C^{(o)}_t$.\n- **Metrics**:\n    - $\\mathrm{RMSE}(X, Y) = \\sqrt{\\frac{1}{N} \\sum_{t=1}^{N} \\left( X_t - Y_t \\right)^2}$.\n    - $\\mathrm{ACC}(X, Y) = \\frac{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)\\left( Y_t - \\bar{Y} \\right)}{\\sqrt{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)^2} \\sqrt{\\sum_{t=1}^{N} \\left( Y_t - \\bar{Y} \\right)^2}}$, with $\\mathrm{ACC}(X, Y)=0$ if the denominator is zero.\n- **Required outputs**: For each test case, compute $[\\mathrm{RMSE}(F,O), \\mathrm{ACC}(F,O), \\mathrm{RMSE}(F^{\\prime},O^{\\prime}), \\mathrm{ACC}(F^{\\prime},O^{\\prime})]$. All floats must be rounded to six decimal places.\n- **Data**:\n    - Shared Observed Data:\n        - $C^{(o)} = [0.0, 1.0, 2.0, 1.0, 0.0, -1.0, -2.0, -1.0]$\n        - $A^{(o)} = [0.5, -0.5, 1.0, -1.0, 0.2, -0.2, 0.0, 0.0]$\n        - Observation $O = C^{(o)} + A^{(o)}$\n    - Test Case 1: $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$, $A^{(f)} = [0.4, -0.6, 0.9, -0.8, 0.3, -0.1, 0.1, -0.2]$, $F = C^{(f)} + A^{(f)}$.\n    - Test Case 2: $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$, $A^{(f)} = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$, $F = C^{(f)} + A^{(f)}$.\n    - Test Case 3: $C^{(f)} = C^{(o)}$, $A^{(f)} = A^{(o)}$, $F = C^{(f)} + A^{(f)}$.\n    - Test Case 4: $C^{(f)} = C^{(o)}$, $A^{(f)} = -A^{(o)}$, $F = C^{(f)} + A^{(f)}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem uses standard, correctly defined metrics (RMSE, ACC) and concepts (climatology, anomaly) from the field of forecast verification. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed, providing all necessary data and definitions to compute a unique set of results. The special case for ACC with zero standard deviation is explicitly handled.\n- **Objective**: The problem is stated objectively with precise numerical data and mathematical formulas.\n- **Other criteria**: The problem is self-contained, consistent, realistic within a computational context, and non-trivial. It is an instructive exercise in forecast verification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe solution involves a systematic application of the provided formulas to the given data. We will first establish the full time series for the observations and then calculate the four required metrics for each of the four forecast test cases. The length of all time series is $N=8$.\n\n**Shared Data Series**\nThe observed climatology $C^{(o)}$ and observed raw anomalies $A^{(o)}$ are given as:\n$$ C^{(o)} = \\begin{bmatrix} 0.0 & 1.0 & 2.0 & 1.0 & 0.0 & -1.0 & -2.0 & -1.0 \\end{bmatrix} $$\n$$ A^{(o)} = \\begin{bmatrix} 0.5 & -0.5 & 1.0 & -1.0 & 0.2 & -0.2 & 0.0 & 0.0 \\end{bmatrix} $$\nThe full observation time series $O$ is the sum of its climatology and anomaly components, $O_t = C^{(o)}_t + A^{(o)}_t$:\n$$ O = \\begin{bmatrix} 0.5 & 0.5 & 3.0 & 0.0 & 0.2 & -1.2 & -2.0 & -1.0 \\end{bmatrix} $$\nThe observed anomaly sequence for verification, $O'$, is defined as $O^{\\prime}_t = O_t - C^{(o)}_t$, which simplifies to $O^{\\prime}_t = (C^{(o)}_t + A^{(o)}_t) - C^{(o)}_t = A^{(o)}_t$. So, $O' = A^{(o)}$.\n\nSimilarly, for each test case, the forecast anomaly sequence $F'$ is defined as $F^{\\prime}_t = F_t - C^{(f)}_t$. Since $F_t = C^{(f)}_t + A^{(f)}_t$, this simplifies to $F^{\\prime}_t = A^{(f)}_t$.\n\nThus, the core calculations for each case are:\n1.  Absolute-space RMSE: $\\mathrm{RMSE}(F, O)$\n2.  Absolute-space ACC: $\\mathrm{ACC}(F, O)$\n3.  Anomaly-space RMSE: $\\mathrm{RMSE}(A^{(f)}, A^{(o)})$\n4.  Anomaly-space ACC: $\\mathrm{ACC}(A^{(f)}, A^{(o)})$\n\nThe computational procedure will be implemented in Python using the `numpy` library for efficient vector operations. The ACC will be calculated according to the formula, handling the case of zero variance to prevent division by zero.\n\n**Test Case 1: Seasonal bias and amplitude error in climatology**\n- $C^{(f)} = 0.5 + 1.5 \\times C^{(o)} = \\begin{bmatrix} 0.5 & 2.0 & 3.5 & 2.0 & 0.5 & -1.0 & -2.5 & -1.0 \\end{bmatrix}$\n- $A^{(f)} = \\begin{bmatrix} 0.4 & -0.6 & 0.9 & -0.8 & 0.3 & -0.1 & 0.1 & -0.2 \\end{bmatrix}$\n- $F = C^{(f)} + A^{(f)} = \\begin{bmatrix} 0.9 & 1.4 & 4.4 & 1.2 & 0.8 & -1.1 & -2.4 & -1.2 \\end{bmatrix}$\n- Metrics are calculated for the pairs $(F, O)$ and $(A^{(f)}, A^{(o)})$.\n\n**Test Case 2: Climatology-only forecast**\n- $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$ (same as Case 1)\n- $A^{(f)} = \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix}$\n- $F = C^{(f)} + A^{(f)} = C^{(f)}$\n- The standard deviation of $A^{(f)}$ is $0$. Therefore, $\\mathrm{ACC}(A^{(f)}, A^{(o)})$ must be $0.0$ as per the problem definition.\n\n**Test Case 3: Perfect forecast**\n- $C^{(f)} = C^{(o)}$\n- $A^{(f)} = A^{(o)}$\n- $F = C^{(f)} + A^{(f)} = C^{(o)} + A^{(o)} = O$.\n- Since the forecast is identical to the observation, the error is zero and the correlation is one.\n- $\\mathrm{RMSE}(F, O) = 0.0$ and $\\mathrm{ACC}(F, O) = 1.0$.\n- Similarly, $A^{(f)} = A^{(o)}$, so $\\mathrm{RMSE}(A^{(f)}, A^{(o)}) = 0.0$ and $\\mathrm{ACC}(A^{(f)}, A^{(o)}) = 1.0$.\n\n**Test Case 4: Perfect climatology, opposite anomalies**\n- $C^{(f)} = C^{(o)}$\n- $A^{(f)} = -A^{(o)} = \\begin{bmatrix} -0.5 & 0.5 & -1.0 & 1.0 & -0.2 & 0.2 & 0.0 & 0.0 \\end{bmatrix}$\n- $F = C^{(f)} + A^{(f)} = C^{(o)} - A^{(o)}$\n- The error in the anomalies is $A^{(f)} - A^{(o)} = -2A^{(o)}$.\n- $\\mathrm{RMSE}(A^{(f)}, A^{(o)}) = \\sqrt{\\frac{1}{N}\\sum(-2A^{(o)}_t)^2} = 2 \\sqrt{\\frac{1}{N}\\sum(A^{(o)}_t)^2}$.\n- Since $A^{(f)}$ is perfectly anti-correlated with $A^{(o)}$, their correlation coefficient $\\mathrm{ACC}(A^{(f)}, A^{(o)})$ will be $-1.0$.\n\nThese steps will be implemented in the provided Python code block. For each test case, the four metrics will be computed, rounded to six decimal places, and collected into a list. The final output will be a list of these lists, formatted as a string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hindcast verification problem by calculating RMSE and ACC\n    in absolute and anomaly space for four test cases.\n    \"\"\"\n\n    def rmse(x, y):\n        \"\"\"Computes the Root Mean Square Error between two sequences.\"\"\"\n        return np.sqrt(np.mean((x - y) ** 2))\n\n    def acc(x, y):\n        \"\"\"\n        Computes the Anomaly Correlation Coefficient (Pearson correlation)\n        between two sequences. Handles the case of zero standard deviation.\n        \"\"\"\n        if x.size != y.size or x.size == 0:\n            return 0.0\n\n        mean_x = np.mean(x)\n        mean_y = np.mean(y)\n\n        var_x_sum = np.sum((x - mean_x) ** 2)\n        var_y_sum = np.sum((y - mean_y) ** 2)\n\n        # Handle degenerate cases where standard deviation is zero\n        if var_x_sum == 0 or var_y_sum == 0:\n            return 0.0\n\n        numerator = np.sum((x - mean_x) * (y - mean_y))\n        denominator = np.sqrt(var_x_sum) * np.sqrt(var_y_sum)\n        \n        return numerator / denominator\n\n    # Shared observed data\n    C_o = np.array([0.0, 1.0, 2.0, 1.0, 0.0, -1.0, -2.0, -1.0])\n    A_o = np.array([0.5, -0.5, 1.0, -1.0, 0.2, -0.2, 0.0, 0.0])\n    O = C_o + A_o\n    O_prime = A_o # Since O_prime = O - C_o\n\n    # Define the test cases\n    test_cases_defs = [\n        {\n            \"C_f_def\": lambda c: 0.5 + 1.5 * c,\n            \"A_f\": np.array([0.4, -0.6, 0.9, -0.8, 0.3, -0.1, 0.1, -0.2])\n        },\n        {\n            \"C_f_def\": lambda c: 0.5 + 1.5 * c,\n            \"A_f\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n        },\n        {\n            \"C_f_def\": lambda c: c,\n            \"A_f\": A_o\n        },\n        {\n            \"C_f_def\": lambda c: c,\n            \"A_f\": -A_o\n        }\n    ]\n\n    all_results = []\n\n    for case_def in test_cases_defs:\n        # Construct forecast series for the current case\n        C_f = case_def[\"C_f_def\"](C_o)\n        A_f = case_def[\"A_f\"]\n        F = C_f + A_f\n        F_prime = A_f # Since F_prime = F - C_f\n\n        # 1. Absolute-space RMSE\n        rmse_abs = rmse(F, O)\n        \n        # 2. Absolute-space ACC\n        acc_abs = acc(F, O)\n\n        # 3. Anomaly-space RMSE\n        rmse_anom = rmse(F_prime, O_prime)\n\n        # 4. Anomaly-space ACC\n        acc_anom = acc(F_prime, O_prime)\n\n        case_results = [\n            round(rmse_abs, 6),\n            round(acc_abs, 6),\n            round(rmse_anom, 6),\n            round(acc_anom, 6)\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    # The default str() for a list in Python adds spaces, which might be undesirable.\n    # To be precise and avoid spaces, we format it manually.\n    inner_strings = [f\"[{','.join(f'{val:.6f}' for val in res)}]\" for res in all_results]\n    final_output_str = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Different verification scores can tell conflicting stories about forecast quality, particularly after statistical post-processing is applied. This advanced exercise presents a seemingly paradoxical scenario where a bias correction technique improves a model's pattern correlation ($ACC$) while simultaneously degrading its overall error magnitude ($RMSE$). By analytically deriving the scores, you will uncover the mathematical reasons for this divergence and develop a deeper, more critical understanding of what each metric truly measures. ",
            "id": "4051808",
            "problem": "A seasonal hindcast experiment evaluates a single gridpoint near the subtropics where the observed near-surface temperature at time $t$ is modeled as $y(t) = \\mu_{o} + z(t)$, with $z(t)$ the observed anomaly satisfying $\\mathbb{E}[z(t)] = 0$ and $\\operatorname{Var}(z(t)) = 1$. A numerical weather prediction model produces a reforecast $x(t)$ with a systematic seasonal-mean bias $b(t)$ (zero mean over the evaluation period, but nonzero variance), a correctly phased signal relative to $z(t)$ with amplitude coefficient $c$, and an unpredictable error $\\epsilon(t)$, such that\n$$\nx(t) = \\mu_{o} + b(t) + c\\,z(t) + \\epsilon(t),\n$$\nwhere $b(t)$, $z(t)$, and $\\epsilon(t)$ are mutually uncorrelated processes, $\\mathbb{E}[b(t)] = 0$, $\\operatorname{Var}(b(t)) = \\sigma_{b}^{2}$, and $\\mathbb{E}[\\epsilon(t)] = 0$, $\\operatorname{Var}(\\epsilon(t)) = \\sigma_{\\epsilon}^{2}$. Consider the following reforecast-based bias correction that removes the seasonal bias and applies a multiplicative variance factor $\\alpha$ to the forecast anomalies relative to the observed climatology:\n$$\nx_{\\mathrm{corr}}(t) = \\mu_{o} + \\alpha\\big(x(t) - \\mu_{o} - b(t)\\big) = \\mu_{o} + \\alpha\\big(c\\,z(t) + \\epsilon(t)\\big).\n$$\nSuppose the modelâ€™s signal amplitude is undamped, $c = 1$, the unpredictable error variance is $\\sigma_{\\epsilon}^{2} = 0.01$, the seasonal bias variance is $\\sigma_{b}^{2} = 0.40$, and the multiplicative factor estimated from a short training set is $\\alpha = 1.8$ (which inflates the variance of the corrected forecast anomalies).\n\nUsing only first principles and core definitions, do the following:\n\n- Derive expressions for the pre-correction root mean square error (RMSE) on absolute levels, $\\mathrm{RMSE}_{\\mathrm{pre}} = \\sqrt{\\mathbb{E}\\big[(x(t) - y(t))^{2}\\big]}$, and the post-correction RMSE on absolute levels, $\\mathrm{RMSE}_{\\mathrm{post}} = \\sqrt{\\mathbb{E}\\big[(x_{\\mathrm{corr}}(t) - y(t))^{2}\\big]}$. Compute their numerical values in kelvin and compare them.\n- Define the Anomaly Correlation Coefficient (ACC) used in seasonal hindcast verification as the Pearson correlation between forecast anomalies relative to the observed climatology and the observed anomalies, that is $\\mathrm{ACC} = \\mathrm{Corr}\\big(x(t) - \\mu_{o},\\, y(t) - \\mu_{o}\\big) = \\mathrm{Corr}\\big(x(t) - \\mu_{o},\\, z(t)\\big)$. Derive the pre-correction and post-correction ACC expressions and compute their numerical values.\n- Explain, on the basis of the properties of these scores, how it is possible in this setup for the bias correction to increase the absolute RMSE while simultaneously improving the anomaly ACC.\n\nFinally, report the numerical value of the post-correction ACC. Round your answer to four significant figures. No units are required for the final answer.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of statistical forecast verification, well-posed with all necessary information provided, and objective in its formulation. All terms are clearly defined, and the premises are internally consistent and relevant to the fields of numerical weather prediction and climate modeling.\n\nWe proceed with the derivation of the requested verification scores.\n\nThe observed temperature is $y(t) = \\mu_{o} + z(t)$, where $\\mathbb{E}[z(t)] = 0$ and $\\operatorname{Var}(z(t)) = \\mathbb{E}[z(t)^2] = 1$.\nThe raw forecast is $x(t) = \\mu_{o} + b(t) + c\\,z(t) + \\epsilon(t)$, where $b(t)$, $z(t)$, and $\\epsilon(t)$ are mutually uncorrelated processes with $\\mathbb{E}[b(t)] = 0$, $\\operatorname{Var}(b(t)) = \\sigma_{b}^{2}$, $\\mathbb{E}[\\epsilon(t)] = 0$, and $\\operatorname{Var}(\\epsilon(t)) = \\sigma_{\\epsilon}^{2}$.\nThe corrected forecast is $x_{\\mathrm{corr}}(t) = \\mu_{o} + \\alpha\\big(c\\,z(t) + \\epsilon(t)\\big)$.\nThe given numerical values are $c = 1$, $\\sigma_{\\epsilon}^{2} = 0.01$, $\\sigma_{b}^{2} = 0.40$, and $\\alpha = 1.8$.\n\nFirst, we derive the expressions for the root mean square error (RMSE) before and after correction.\n\nThe pre-correction error is the difference between the forecast and the observation:\n$$\nx(t) - y(t) = \\big(\\mu_{o} + b(t) + c\\,z(t) + \\epsilon(t)\\big) - \\big(\\mu_{o} + z(t)\\big) = b(t) + (c-1)z(t) + \\epsilon(t)\n$$\nThe mean error is $\\mathbb{E}[x(t) - y(t)] = \\mathbb{E}[b(t)] + (c-1)\\mathbb{E}[z(t)] + \\mathbb{E}[\\epsilon(t)] = 0 + 0 + 0 = 0$.\nThe pre-correction mean square error ($\\mathrm{MSE}_{\\mathrm{pre}}$) is the variance of this error:\n$$\n\\mathrm{MSE}_{\\mathrm{pre}} = \\mathbb{E}\\big[(x(t) - y(t))^{2}\\big] = \\mathbb{E}\\big[\\big(b(t) + (c-1)z(t) + \\epsilon(t)\\big)^{2}\\big]\n$$\nBecause $b(t)$, $z(t)$, and $\\epsilon(t)$ are mutually uncorrelated and have zero mean, the expectation of their cross-product terms is zero. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{MSE}_{\\mathrm{pre}} = \\operatorname{Var}(b(t)) + \\operatorname{Var}((c-1)z(t)) + \\operatorname{Var}(\\epsilon(t)) = \\sigma_{b}^{2} + (c-1)^2\\operatorname{Var}(z(t)) + \\sigma_{\\epsilon}^{2}\n$$\nSubstituting $\\operatorname{Var}(z(t)) = 1$:\n$$\n\\mathrm{MSE}_{\\mathrm{pre}} = \\sigma_{b}^{2} + (c-1)^2 + \\sigma_{\\epsilon}^{2}\n$$\nThe pre-correction RMSE is $\\mathrm{RMSE}_{\\mathrm{pre}} = \\sqrt{\\sigma_{b}^{2} + (c-1)^2 + \\sigma_{\\epsilon}^{2}}$.\nUsing the given values:\n$$\n\\mathrm{RMSE}_{\\mathrm{pre}} = \\sqrt{0.40 + (1-1)^2 + 0.01} = \\sqrt{0.41} \\approx 0.6403 \\text{ K}\n$$\n\nThe post-correction error is:\n$$\nx_{\\mathrm{corr}}(t) - y(t) = \\big(\\mu_{o} + \\alpha(c\\,z(t) + \\epsilon(t))\\big) - \\big(\\mu_{o} + z(t)\\big) = (\\alpha c - 1)z(t) + \\alpha\\epsilon(t)\n$$\nThe mean error is $\\mathbb{E}[x_{\\mathrm{corr}}(t) - y(t)] = (\\alpha c - 1)\\mathbb{E}[z(t)] + \\alpha\\mathbb{E}[\\epsilon(t)] = 0$.\nThe post-correction mean square error ($\\mathrm{MSE}_{\\mathrm{post}}$) is the variance of this error. As $z(t)$ and $\\epsilon(t)$ are uncorrelated:\n$$\n\\mathrm{MSE}_{\\mathrm{post}} = \\mathbb{E}\\big[((\\alpha c - 1)z(t) + \\alpha\\epsilon(t))^{2}\\big] = (\\alpha c - 1)^2\\operatorname{Var}(z(t)) + \\alpha^2\\operatorname{Var}(\\epsilon(t))\n$$\n$$\n\\mathrm{MSE}_{\\mathrm{post}} = (\\alpha c - 1)^2 + \\alpha^2\\sigma_{\\epsilon}^{2}\n$$\nThe post-correction RMSE is $\\mathrm{RMSE}_{\\mathrm{post}} = \\sqrt{(\\alpha c - 1)^2 + \\alpha^2\\sigma_{\\epsilon}^{2}}$.\nUsing the given values:\n$$\n\\mathrm{RMSE}_{\\mathrm{post}} = \\sqrt{(1.8 \\cdot 1 - 1)^2 + (1.8)^2(0.01)} = \\sqrt{(0.8)^2 + 3.24(0.01)} = \\sqrt{0.64 + 0.0324} = \\sqrt{0.6724} \\approx 0.8200 \\text{ K}\n$$\nComparing the two, we find that $\\mathrm{RMSE}_{\\mathrm{post}} > \\mathrm{RMSE}_{\\mathrm{pre}}$. The bias correction procedure has increased the absolute RMSE.\n\nNext, we derive the expressions for the Anomaly Correlation Coefficient (ACC). The ACC is the Pearson correlation between the forecast anomaly and the observed anomaly.\nThe observed anomaly is $y(t) - \\mu_{o} = z(t)$. We know $\\operatorname{Var}(z(t)) = 1$. The correlation is defined as $\\mathrm{Corr}(X, Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$.\n\nFor the pre-correction ACC, the forecast anomaly is $x(t) - \\mu_{o} = b(t) + c\\,z(t) + \\epsilon(t)$.\nThe covariance term is:\n$$\n\\mathrm{Cov}\\big(x(t) - \\mu_{o}, z(t)\\big) = \\mathbb{E}\\big[(b(t) + c\\,z(t) + \\epsilon(t))z(t)\\big] = \\mathbb{E}[b(t)z(t)] + c\\mathbb{E}[z(t)^2] + \\mathbb{E}[\\epsilon(t)z(t)]\n$$\nDue to uncorrelation and zero means, this simplifies to $c\\mathbb{E}[z(t)^2] = c\\operatorname{Var}(z(t)) = c$.\nThe variance of the pre-correction forecast anomaly is:\n$$\n\\operatorname{Var}\\big(x(t) - \\mu_{o}\\big) = \\operatorname{Var}\\big(b(t) + c\\,z(t) + \\epsilon(t)\\big) = \\sigma_{b}^{2} + c^2\\operatorname{Var}(z(t)) + \\sigma_{\\epsilon}^{2} = \\sigma_{b}^{2} + c^2 + \\sigma_{\\epsilon}^{2}\n$$\nThus, the pre-correction ACC is:\n$$\n\\mathrm{ACC}_{\\mathrm{pre}} = \\frac{c}{\\sqrt{(\\sigma_{b}^{2} + c^2 + \\sigma_{\\epsilon}^{2})\\operatorname{Var}(z(t))}} = \\frac{c}{\\sqrt{\\sigma_{b}^{2} + c^2 + \\sigma_{\\epsilon}^{2}}}\n$$\nUsing the given values:\n$$\n\\mathrm{ACC}_{\\mathrm{pre}} = \\frac{1}{\\sqrt{0.40 + 1^2 + 0.01}} = \\frac{1}{\\sqrt{1.41}} \\approx 0.8422\n$$\n\nFor the post-correction ACC, the forecast anomaly is $x_{\\mathrm{corr}}(t) - \\mu_{o} = \\alpha(c\\,z(t) + \\epsilon(t))$.\nThe covariance term is:\n$$\n\\mathrm{Cov}\\big(x_{\\mathrm{corr}}(t) - \\mu_{o}, z(t)\\big) = \\mathbb{E}\\big[\\alpha(c\\,z(t) + \\epsilon(t))z(t)\\big] = \\alpha c\\mathbb{E}[z(t)^2] + \\alpha\\mathbb{E}[\\epsilon(t)z(t)] = \\alpha c\n$$\nThe variance of the post-correction forecast anomaly is:\n$$\n\\operatorname{Var}\\big(x_{\\mathrm{corr}}(t) - \\mu_{o}\\big) = \\operatorname{Var}\\big(\\alpha(c\\,z(t) + \\epsilon(t))\\big) = \\alpha^2\\operatorname{Var}(c\\,z(t) + \\epsilon(t)) = \\alpha^2(c^2 + \\sigma_{\\epsilon}^{2})\n$$\nThus, the post-correction ACC is:\n$$\n\\mathrm{ACC}_{\\mathrm{post}} = \\frac{\\alpha c}{\\sqrt{\\alpha^2(c^2 + \\sigma_{\\epsilon}^{2})\\operatorname{Var}(z(t))}} = \\frac{\\alpha c}{\\sqrt{\\alpha^2(c^2 + \\sigma_{\\epsilon}^{2})}} = \\frac{\\alpha c}{|\\alpha|\\sqrt{c^2 + \\sigma_{\\epsilon}^{2}}}\n$$\nSince $\\alpha = 1.8 > 0$, this simplifies to:\n$$\n\\mathrm{ACC}_{\\mathrm{post}} = \\frac{c}{\\sqrt{c^2 + \\sigma_{\\epsilon}^{2}}}\n$$\nUsing the given values:\n$$\n\\mathrm{ACC}_{\\mathrm{post}} = \\frac{1}{\\sqrt{1^2 + 0.01}} = \\frac{1}{\\sqrt{1.01}} \\approx 0.9950\n$$\nComparing the two, $\\mathrm{ACC}_{\\mathrm{post}} > \\mathrm{ACC}_{\\mathrm{pre}}$. The ACC has improved.\n\nFinally, we explain this contradictory result.\nThe ACC measures the linear correlation between forecast and observed anomalies. The pre-correction forecast anomaly $x(t)-\\mu_{o} = b(t) + c\\,z(t) + \\epsilon(t)$ contains the systematic bias term $b(t)$, which is uncorrelated with the true signal $z(t)$. This term acts as noise, decreasing the correlation. The correction procedure, $x_{\\mathrm{corr}}(t) = \\mu_{o} + \\alpha(x(t) - \\mu_{o} - b(t))$, explicitly removes $b(t)$. As seen in the derivation for $\\mathrm{ACC}_{\\mathrm{post}}$, the term $\\sigma_b^2$ is eliminated from the denominator, leading to a higher correlation. The ACC is insensitive to the linear scaling factor $\\alpha$ (for $\\alpha>0$), which cancels out from the numerator and denominator.\n\nThe RMSE, on the other hand, measures the magnitude of the total error $e(t) = \\text{forecast}(t) - y(t)$. Its square, the MSE, is the sum of squared error components.\n$$\n\\mathrm{MSE}_{\\mathrm{pre}} = \\sigma_{b}^{2} + (c-1)^2 + \\sigma_{\\epsilon}^{2} = 0.40 + 0 + 0.01 = 0.41\n$$\n$$\n\\mathrm{MSE}_{\\mathrm{post}} = (\\alpha c - 1)^2 + \\alpha^2\\sigma_{\\epsilon}^{2} = (1.8 - 1)^2 + (1.8)^2(0.01) = 0.64 + 0.0324 = 0.6724\n$$\nThe correction removes the error contribution from the systematic bias ($\\sigma_{b}^{2} = 0.40$). However, it applies a multiplicative factor $\\alpha = 1.8$. The problem states this was estimated from a short training set, implying it may be a poor, overfitted estimate. The optimal value of $\\alpha$ that minimizes $\\mathrm{MSE}_{\\mathrm{post}}$ is $\\alpha_{\\mathrm{opt}} = \\frac{c}{c^2+\\sigma_\\epsilon^2} = \\frac{1}{1+0.01} \\approx 0.99$. The applied value $\\alpha=1.8$ is far from optimal and severely over-inflates the forecast anomalies. This introduces a large conditional bias or signal amplitude error, represented by the term $(\\alpha c - 1)^2 = (1.8 - 1)^2 = 0.64$. This new error component is substantially larger than the original total MSE ($0.41$), causing the post-correction RMSE to increase despite the removal of the systematic bias $b(t)$. In summary, the correction improves the phasing of the anomalies (higher ACC) but degrades their amplitude (higher RMSE).\n\nThe numerical value of the post-correction ACC, rounded to four significant figures, is $\\frac{1}{\\sqrt{1.01}} \\approx 0.9950$.",
            "answer": "$$\\boxed{0.9950}$$"
        }
    ]
}