{
    "hands_on_practices": [
        {
            "introduction": "The first step in evaluating any forecast system is to determine if it performs better than simple, no-cost alternatives. This practice guides you through the fundamental process of calculating skill scores, which quantify the improvement of a hindcast over baseline references like climatology and persistence. By computing the Root Mean Square Error ($RMSE$) for the model and these references, you will learn how to assess the practical value of a forecast and how its skill evolves across different lead times .",
            "id": "4051836",
            "problem": "Consider the evaluation of weekly hindcasts for $2$-meter air temperature, expressed in degrees Celsius. The hindcasts are produced by a Numerical Weather Prediction (NWP) system and verified against weekly averaged observations. Two reference baselines are used: a climatology reference derived from a prior reforecast period and a persistence reference defined as the last available observed weekly mean prior to each forecast initialization. The objective is to compare these two references for weekly temperature hindcasts and to compute the skill score under Root Mean Squared Error (RMSE) for each reference across multiple lead times.\n\nFundamental base: For a set of forecasts and corresponding observations, the squared error for a single case is defined as the square of the difference between the forecast and the observation. The mean squared error (MSE) is defined as the arithmetic mean of the squared errors over all cases. The Root Mean Squared Error (RMSE) is defined as the square root of the MSE. The skill score under RMSE is a dimensionless quantity that quantifies improvement relative to a specified reference by normalizing RMSE with respect to the reference's RMSE.\n\nDataset description and test suite:\n- Number of independent hindcast cases: $N = 3$.\n- Number of lead weeks: $L = 4$, with lead times $\\ell \\in \\{1,2,3,4\\}$.\n- Initialization weeks (week-of-year indices): $[\\,3,\\,20,\\,30\\,]$. For each case $i \\in \\{1,2,3\\}$, the verifying week-of-year for lead $\\ell$ is $w_{i,\\ell} = w_{0,i} + \\ell$.\n- Observation matrix $Y \\in \\mathbb{R}^{N \\times L}$ (weekly means in degrees Celsius):\n  - Case $1$: $[\\,2.5,\\;3.2,\\;4.1,\\;5.3\\,]$\n  - Case $2$: $[\\,15.2,\\;15.8,\\;16.9,\\;18.5\\,]$\n  - Case $3$: $[\\,22.3,\\;23.5,\\;24.7,\\;25.2\\,]$\n- Hindcast model matrix $H \\in \\mathbb{R}^{N \\times L}$ (weekly means in degrees Celsius):\n  - Case $1$: $[\\,2.0,\\;2.8,\\;4.5,\\;5.1\\,]$\n  - Case $2$: $[\\,14.5,\\;15.7,\\;17.2,\\;18.9\\,]$\n  - Case $3$: $[\\,22.7,\\;23.2,\\;24.2,\\;25.8\\,]$\n- Climatology reference matrix $C \\in \\mathbb{R}^{N \\times L}$ (weekly means in degrees Celsius), mapped to the verifying weeks:\n  - Case $1$ (verifying weeks $[\\,4,5,6,7\\,]$): $[\\,2.0,\\;3.0,\\;4.0,\\;5.0\\,]$\n  - Case $2$ (verifying weeks $[\\,21,22,23,24\\,]$): $[\\,15.0,\\;16.0,\\;17.0,\\;18.0\\,]$\n  - Case $3$ (verifying weeks $[\\,31,32,33,34\\,]$): $[\\,22.0,\\;23.0,\\;24.0,\\;25.0\\,]$\n- Persistence reference vector $P \\in \\mathbb{R}^{N}$ (degrees Celsius), equal to the weekly mean observed temperature in the week preceding each initialization:\n  - Case $1$: $1.8$\n  - Case $2$: $14.9$\n  - Case $3$: $22.1$\n  For each lead $\\ell$, the persistence forecast remains equal to $P_i$ for case $i$.\n\nTasks:\n1. For each lead $\\ell \\in \\{1,2,3,4\\}$, compute $RMSE_{model}(\\ell)$ across the $N$ cases using $H_{\\cdot,\\ell}$ and $Y_{\\cdot,\\ell}$, expressed in degrees Celsius.\n2. For each lead $\\ell$, compute $RMSE_{clim}(\\ell)$ across the $N$ cases using $C_{\\cdot,\\ell}$ and $Y_{\\cdot,\\ell}$, expressed in degrees Celsius.\n3. For each lead $\\ell$, compute $RMSE_{pers}(\\ell)$ across the $N$ cases using $P$ and $Y_{\\cdot,\\ell}$, expressed in degrees Celsius.\n4. Using the RMSE values, compute the skill score under RMSE for each reference and lead. Express every skill score as a decimal number (no percentage sign), rounded to $5$ decimal places.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with two sublists ordered by lead $\\ell = 1$ to $\\ell = 4$:\n- First sublist: $[\\,SS_{clim}(1),\\,SS_{clim}(2),\\,SS_{clim}(3),\\,SS_{clim}(4)\\,]$.\n- Second sublist: $[\\,SS_{pers}(1),\\,SS_{pers}(2),\\,SS_{pers}(3),\\,SS_{pers}(4)\\,]$.\nFor example: $[[s_1,s_2,s_3,s_4],[t_1,t_2,t_3,t_4]]$.\n\nScientific realism and coverage:\n- The dataset includes winter, late spring, and midsummer weeks ($[\\,3,\\,20,\\,30\\,]$) to cover seasonal variability.\n- The test suite simultaneously probes a typical case, relative disadvantages of persistence at longer leads, and the possibility of negative skill relative to climatology at short leads.\n- All quantities involving physical units must be reported in degrees Celsius as specified; skill scores are dimensionless and must be reported as decimals.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard exercise in the verification of numerical weather prediction hindcasts.\n\nThe objective is to compute the Root Mean Squared Error (RMSE) skill scores for a set of hindcasts against two reference forecasts: climatology and persistence. The solution proceeds by first defining the necessary statistical quantities and then systematically applying them to the provided dataset for each lead time.\n\nThe Mean Squared Error (MSE) for a set of $N$ forecasts $F_i$ and corresponding observations $Y_i$ is given by:\n$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (F_i - Y_i)^2$$\nThe Root Mean Squared Error (RMSE) is the square root of the MSE:\n$$RMSE = \\sqrt{MSE}$$\nThe skill score (SS) of a model forecast relative to a reference forecast is defined as:\n$$SS = 1 - \\frac{RMSE_{model}}{RMSE_{ref}}$$\nA positive skill score indicates that the model is more accurate than the reference, a negative score indicates it is less accurate, and a score of $0$ implies equal accuracy. A perfect forecast would have $RMSE_{model}=0$, yielding a skill score of $1$.\n\nThe given data are:\n- Number of cases $N = 3$.\n- Observation matrix $Y = \\begin{pmatrix} 2.5 & 3.2 & 4.1 & 5.3 \\\\ 15.2 & 15.8 & 16.9 & 18.5 \\\\ 22.3 & 23.5 & 24.7 & 25.2 \\end{pmatrix}$.\n- Hindcast model matrix $H = \\begin{pmatrix} 2.0 & 2.8 & 4.5 & 5.1 \\\\ 14.5 & 15.7 & 17.2 & 18.9 \\\\ 22.7 & 23.2 & 24.2 & 25.8 \\end{pmatrix}$.\n- Climatology reference matrix $C = \\begin{pmatrix} 2.0 & 3.0 & 4.0 & 5.0 \\\\ 15.0 & 16.0 & 17.0 & 18.0 \\\\ 22.0 & 23.0 & 24.0 & 25.0 \\end{pmatrix}$.\n- Persistence reference vector $P = \\begin{pmatrix} 1.8 \\\\ 14.9 \\\\ 22.1 \\end{pmatrix}$.\n\nThe calculations are performed for each lead week $\\ell \\in \\{1, 2, 3, 4\\}$.\n\n**Lead Week $\\ell=1$**\nThe data vectors for this lead are:\n- Observations: $Y_{\\cdot,1} = [2.5, 15.2, 22.3]$\n- Model: $H_{\\cdot,1} = [2.0, 14.5, 22.7]$\n- Climatology: $C_{\\cdot,1} = [2.0, 15.0, 22.0]$\n- Persistence: $P = [1.8, 14.9, 22.1]$\n\nThe squared errors are computed for each case $i \\in \\{1,2,3\\}$:\n- $SE_{model}(1) = [ (2.0-2.5)^2, (14.5-15.2)^2, (22.7-22.3)^2 ] = [0.25, 0.49, 0.16]$\n- $SE_{clim}(1) = [ (2.0-2.5)^2, (15.0-15.2)^2, (22.0-22.3)^2 ] = [0.25, 0.04, 0.09]$\n- $SE_{pers}(1) = [ (1.8-2.5)^2, (14.9-15.2)^2, (22.1-22.3)^2 ] = [0.49, 0.09, 0.04]$\n\nThe MSE and RMSE values are:\n- $MSE_{model}(1) = \\frac{0.25+0.49+0.16}{3} = 0.30 \\implies RMSE_{model}(1) \\approx 0.54772$\n- $MSE_{clim}(1) = \\frac{0.25+0.04+0.09}{3} \\approx 0.12667 \\implies RMSE_{clim}(1) \\approx 0.35590$\n- $MSE_{pers}(1) = \\frac{0.49+0.09+0.04}{3} \\approx 0.20667 \\implies RMSE_{pers}(1) \\approx 0.45461$\n\nThe skill scores for lead $1$ are:\n- $SS_{clim}(1) = 1 - \\frac{0.54772}{0.35590} \\approx -0.53901$\n- $SS_{pers}(1) = 1 - \\frac{0.54772}{0.45461} \\approx -0.20483$\n\n**Lead Week $\\ell=2$**\n- $Y_{\\cdot,2} = [3.2, 15.8, 23.5]$\n- $H_{\\cdot,2} = [2.8, 15.7, 23.2]$\n- $C_{\\cdot,2} = [3.0, 16.0, 23.0]$\n- $P = [1.8, 14.9, 22.1]$\n\nThe MSE and RMSE values are:\n- $MSE_{model}(2) = \\frac{(2.8-3.2)^2+(15.7-15.8)^2+(23.2-23.5)^2}{3} = \\frac{0.16+0.01+0.09}{3} \\approx 0.08667 \\implies RMSE_{model}(2) \\approx 0.29439$\n- $MSE_{clim}(2) = \\frac{(3.0-3.2)^2+(16.0-15.8)^2+(23.0-23.5)^2}{3} = \\frac{0.04+0.04+0.25}{3} = 0.11 \\implies RMSE_{clim}(2) \\approx 0.33166$\n- $MSE_{pers}(2) = \\frac{(1.8-3.2)^2+(14.9-15.8)^2+(22.1-23.5)^2}{3} = \\frac{1.96+0.81+1.96}{3} \\approx 1.57667 \\implies RMSE_{pers}(2) \\approx 1.25565$\n\nThe skill scores for lead $2$ are:\n- $SS_{clim}(2) = 1 - \\frac{0.29439}{0.33166} \\approx 0.11234$\n- $SS_{pers}(2) = 1 - \\frac{0.29439}{1.25565} \\approx 0.76555$\n\n**Lead Week $\\ell=3$**\n- $Y_{\\cdot,3} = [4.1, 16.9, 24.7]$\n- $H_{\\cdot,3} = [4.5, 17.2, 24.2]$\n- $C_{\\cdot,3} = [4.0, 17.0, 24.0]$\n- $P = [1.8, 14.9, 22.1]$\n\nThe MSE and RMSE values are:\n- $MSE_{model}(3) = \\frac{(4.5-4.1)^2+(17.2-16.9)^2+(24.2-24.7)^2}{3} = \\frac{0.16+0.09+0.25}{3} \\approx 0.16667 \\implies RMSE_{model}(3) \\approx 0.40825$\n- $MSE_{clim}(3) = \\frac{(4.0-4.1)^2+(17.0-16.9)^2+(24.0-24.7)^2}{3} = \\frac{0.01+0.01+0.49}{3} = 0.17 \\implies RMSE_{clim}(3) \\approx 0.41231$\n- $MSE_{pers}(3) = \\frac{(1.8-4.1)^2+(14.9-16.9)^2+(22.1-24.7)^2}{3} = \\frac{5.29+4.00+6.76}{3} = 5.35 \\implies RMSE_{pers}(3) \\approx 2.31301$\n\nThe skill scores for lead $3$ are:\n- $SS_{clim}(3) = 1 - \\frac{0.40825}{0.41231} \\approx 0.00985$\n- $SS_{pers}(3) = 1 - \\frac{0.40825}{2.31301} \\approx 0.82350$\n\n**Lead Week $\\ell=4$**\n- $Y_{\\cdot,4} = [5.3, 18.5, 25.2]$\n- $H_{\\cdot,4} = [5.1, 18.9, 25.8]$\n- $C_{\\cdot,4} = [5.0, 18.0, 25.0]$\n- $P = [1.8, 14.9, 22.1]$\n\nThe MSE and RMSE values are:\n- $MSE_{model}(4) = \\frac{(5.1-5.3)^2+(18.9-18.5)^2+(25.8-25.2)^2}{3} = \\frac{0.04+0.16+0.36}{3} \\approx 0.18667 \\implies RMSE_{model}(4) \\approx 0.43205$\n- $MSE_{clim}(4) = \\frac{(5.0-5.3)^2+(18.0-18.5)^2+(25.0-25.2)^2}{3} = \\frac{0.09+0.25+0.04}{3} \\approx 0.12667 \\implies RMSE_{clim}(4) \\approx 0.35590$\n- $MSE_{pers}(4) = \\frac{(1.8-5.3)^2+(14.9-18.5)^2+(22.1-25.2)^2}{3} = \\frac{12.25+12.96+9.61}{3} \\approx 11.60667 \\implies RMSE_{pers}(4) \\approx 3.40686$\n\nThe skill scores for lead $4$ are:\n- $SS_{clim}(4) = 1 - \\frac{0.43205}{0.35590} \\approx -0.21396$\n- $SS_{pers}(4) = 1 - \\frac{0.43205}{3.40686} \\approx 0.87318$\n\n**Summary of Results**\nThe calculated skill scores, rounded to $5$ decimal places, are:\n\n- Skill Score vs. Climatology:\n  - $SS_{clim}(1) = -0.53901$\n  - $SS_{clim}(2) = 0.11234$\n  - $SS_{clim}(3) = 0.00985$\n  - $SS_{clim}(4) = -0.21396$\n\n- Skill Score vs. Persistence:\n  - $SS_{pers}(1) = -0.20483$\n  - $SS_{pers}(2) = 0.76555$\n  - $SS_{pers}(3) = 0.82350$\n  - $SS_{pers}(4) = 0.87318$\n\nThese results demonstrate typical characteristics of forecast skill. The skill versus persistence starts negative, indicating that for a short lead of one week, a simple persistence forecast is more accurate than the NWP model for this small sample. However, the skill rapidly increases with lead time, showing the model's value in forecasting the evolution of the weather state. The skill versus climatology is more variable and remains low (even negative), suggesting the model provides little to no improvement over the long-term average at these lead times for these specific cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes hindcast skill scores for a numerical weather prediction model\n    against climatology and persistence references.\n    \"\"\"\n    # Number of independent hindcast cases\n    N = 3\n    # Number of lead weeks\n    L = 4\n    \n    # Observation matrix Y (N x L)\n    Y = np.array([\n        [2.5, 3.2, 4.1, 5.3],\n        [15.2, 15.8, 16.9, 18.5],\n        [22.3, 23.5, 24.7, 25.2]\n    ])\n\n    # Hindcast model matrix H (N x L)\n    H = np.array([\n        [2.0, 2.8, 4.5, 5.1],\n        [14.5, 15.7, 17.2, 18.9],\n        [22.7, 23.2, 24.2, 25.8]\n    ])\n\n    # Climatology reference matrix C (N x L)\n    C = np.array([\n        [2.0, 3.0, 4.0, 5.0],\n        [15.0, 16.0, 17.0, 18.0],\n        [22.0, 23.0, 24.0, 25.0]\n    ])\n\n    # Persistence reference vector P (N)\n    P = np.array([1.8, 14.9, 22.1])\n\n    ss_clim_list = []\n    ss_pers_list = []\n\n    # Iterate through each lead time from l=1 to L=4 (indices 0 to 3)\n    for lead_idx in range(L):\n        # Extract the column vectors for the current lead time\n        y_lead = Y[:, lead_idx]\n        h_lead = H[:, lead_idx]\n        c_lead = C[:, lead_idx]\n        # The persistence forecast is constant for all leads\n        p_lead = P\n\n        # Calculate Mean Squared Error (MSE) for model, climatology, and persistence\n        mse_model = np.mean((h_lead - y_lead)**2)\n        mse_clim = np.mean((c_lead - y_lead)**2)\n        mse_pers = np.mean((p_lead - y_lead)**2)\n\n        # Calculate Root Mean Squared Error (RMSE)\n        rmse_model = np.sqrt(mse_model)\n        rmse_clim = np.sqrt(mse_clim)\n        rmse_pers = np.sqrt(mse_pers)\n\n        # Calculate the skill score (SS) under RMSE\n        # SS = 1 - (RMSE_model / RMSE_reference)\n        ss_clim = 1 - (rmse_model / rmse_clim)\n        ss_pers = 1 - (rmse_model / rmse_pers)\n\n        # Append the rounded results to their respective lists\n        ss_clim_list.append(round(ss_clim, 5))\n        ss_pers_list.append(round(ss_pers, 5))\n\n    # Format the final output string as specified\n    output_str = f\"[[{','.join(map(str, ss_clim_list))}],[{','.join(map(str, ss_pers_list))}]]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A single verification score rarely tells the whole story of a model's performance. This exercise delves into the critical distinction between verifying absolute values and verifying anomalies, a practice central to climate diagnostics and seasonal forecasting . By calculating both the Root Mean Square Error ($RMSE$) and the Anomaly Correlation Coefficient ($ACC$), you will discover how a model can have a large systematic bias yet still be valuable for skillfully predicting the pattern of weather or climate variations.",
            "id": "4051766",
            "problem": "You are given a hindcast verification task designed to compare verification in absolute space and anomaly space using the same reforecast set. A reforecast time series comprises a model forecast sequence and a verifying observation sequence over a fixed period. For each test case, you are provided with four sequences of equal length: a model forecast sequence $F_t$, an observed sequence $O_t$, a model climatology sequence $C^{(f)}_t$, and an observed climatology sequence $C^{(o)}_t$ over times $t = 1,\\dots,N$. The anomaly sequences are defined by subtracting the corresponding climatologies, i.e., forecast anomalies $F^{\\prime}_t = F_t - C^{(f)}_t$ and observed anomalies $O^{\\prime}_t = O_t - C^{(o)}_t$. Your task is to compute, for each test case, two metrics under both representations (absolute values and anomalies) and report them in a fixed format.\n\nUse the following foundational definitions:\n- The Root Mean Square Error (RMSE) is defined as\n$$\n\\mathrm{RMSE}(X, Y) = \\sqrt{\\frac{1}{N} \\sum_{t=1}^{N} \\left( X_t - Y_t \\right)^2}.\n$$\n- The Anomaly Correlation Coefficient (ACC) is defined as the Pearson correlation coefficient, computed here as\n$$\n\\mathrm{ACC}(X, Y) = \\frac{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)\\left( Y_t - \\bar{Y} \\right)}{\\sqrt{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)^2} \\sqrt{\\sum_{t=1}^{N} \\left( Y_t - \\bar{Y} \\right)^2}},\n$$\nwhere $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^{N} X_t$ and $\\bar{Y} = \\frac{1}{N}\\sum_{t=1}^{N} Y_t$. In degenerate cases where the standard deviation of $X$ or $Y$ is zero (denominator equals zero), define $\\mathrm{ACC}(X, Y) = 0$.\n\nFor each test case, compute:\n1. Absolute-space RMSE: $\\mathrm{RMSE}(F, O)$.\n2. Absolute-space ACC: $\\mathrm{ACC}(F, O)$.\n3. Anomaly-space RMSE: $\\mathrm{RMSE}(F^{\\prime}, O^{\\prime})$.\n4. Anomaly-space ACC: $\\mathrm{ACC}(F^{\\prime}, O^{\\prime})$.\n\nAll values are unitless in this problem. Round each reported float to six decimal places.\n\nTest suite:\n- Shared observed climatology $C^{(o)}$ and observed anomalies $A^{(o)}$:\n  - $C^{(o)} = [0.0, 1.0, 2.0, 1.0, 0.0, -1.0, -2.0, -1.0]$,\n  - $A^{(o)} = [0.5, -0.5, 1.0, -1.0, 0.2, -0.2, 0.0, 0.0]$,\n  - Observations $O = C^{(o)} + A^{(o)}$.\n- Test case 1 (seasonal bias and amplitude error in model climatology; reasonably good anomaly forecast):\n  - $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$,\n  - $A^{(f)} = [0.4, -0.6, 0.9, -0.8, 0.3, -0.1, 0.1, -0.2]$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n- Test case 2 (climatology-only forecast; no forecasted anomalies):\n  - $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$,\n  - $A^{(f)} = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n- Test case 3 (perfect forecast; perfect climatology and perfect anomalies):\n  - $C^{(f)} = C^{(o)}$,\n  - $A^{(f)} = A^{(o)}$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n- Test case 4 (perfect climatology but anomalies with opposite sign):\n  - $C^{(f)} = C^{(o)}$,\n  - $A^{(f)} = - A^{(o)}$,\n  - Forecast $F = C^{(f)} + A^{(f)}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists, one per test case, where each inner list is ordered as $[\\mathrm{RMSE}(F,O), \\mathrm{ACC}(F,O), \\mathrm{RMSE}(F^{\\prime},O^{\\prime}), \\mathrm{ACC}(F^{\\prime},O^{\\prime})]$. All floats must be rounded to six decimal places. For example, the overall structure should look like\n$[[x_{11},x_{12},x_{13},x_{14}],[x_{21},x_{22},x_{23},x_{24}],[x_{31},x_{32},x_{33},x_{34}],[x_{41},x_{42},x_{43},x_{44}]]$\nwith no additional text.",
            "solution": "The user has provided a well-defined hindcast verification problem. The task is to compute two standard verification metrics, the Root Mean Square Error (RMSE) and the Anomaly Correlation Coefficient (ACC), for four different forecast scenarios. The comparison is to be performed in both absolute space (using the full forecast and observation fields) and anomaly space (using deviations from climatology).\n\nFirst, the problem statement is validated.\n\n### Step 1: Extract Givens\n- **Sequences**: Forecast ($F_t$), Observation ($O_t$), Model Climatology ($C^{(f)}_t$), Observed Climatology ($C^{(o)}_t$), for $t=1, \\dots, N$.\n- **Anomalies**: Forecast Anomaly $F^{\\prime}_t = F_t - C^{(f)}_t$, Observed Anomaly $O^{\\prime}_t = O_t - C^{(o)}_t$.\n- **Metrics**:\n    - $\\mathrm{RMSE}(X, Y) = \\sqrt{\\frac{1}{N} \\sum_{t=1}^{N} \\left( X_t - Y_t \\right)^2}$.\n    - $\\mathrm{ACC}(X, Y) = \\frac{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)\\left( Y_t - \\bar{Y} \\right)}{\\sqrt{\\sum_{t=1}^{N} \\left( X_t - \\bar{X} \\right)^2} \\sqrt{\\sum_{t=1}^{N} \\left( Y_t - \\bar{Y} \\right)^2}}$, with $\\mathrm{ACC}(X, Y)=0$ if the denominator is zero.\n- **Required outputs**: For each test case, compute $[\\mathrm{RMSE}(F,O), \\mathrm{ACC}(F,O), \\mathrm{RMSE}(F^{\\prime},O^{\\prime}), \\mathrm{ACC}(F^{\\prime},O^{\\prime})]$. All floats must be rounded to six decimal places.\n- **Data**:\n    - Shared Observed Data:\n        - $C^{(o)} = [0.0, 1.0, 2.0, 1.0, 0.0, -1.0, -2.0, -1.0]$\n        - $A^{(o)} = [0.5, -0.5, 1.0, -1.0, 0.2, -0.2, 0.0, 0.0]$\n        - Observation $O = C^{(o)} + A^{(o)}$\n    - Test Case 1: $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$, $A^{(f)} = [0.4, -0.6, 0.9, -0.8, 0.3, -0.1, 0.1, -0.2]$, $F = C^{(f)} + A^{(f)}$.\n    - Test Case 2: $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$, $A^{(f)} = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$, $F = C^{(f)} + A^{(f)}$.\n    - Test Case 3: $C^{(f)} = C^{(o)}$, $A^{(f)} = A^{(o)}$, $F = C^{(f)} + A^{(f)}$.\n    - Test Case 4: $C^{(f)} = C^{(o)}$, $A^{(f)} = -A^{(o)}$, $F = C^{(f)} + A^{(f)}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem uses standard, correctly defined metrics (RMSE, ACC) and concepts (climatology, anomaly) from the field of forecast verification. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed, providing all necessary data and definitions to compute a unique set of results. The special case for ACC with zero standard deviation is explicitly handled.\n- **Objective**: The problem is stated objectively with precise numerical data and mathematical formulas.\n- **Other criteria**: The problem is self-contained, consistent, realistic within a computational context, and non-trivial. It is an instructive exercise in forecast verification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe solution involves a systematic application of the provided formulas to the given data. We will first establish the full time series for the observations and then calculate the four required metrics for each of the four forecast test cases. The length of all time series is $N=8$.\n\n**Shared Data Series**\nThe observed climatology $C^{(o)}$ and observed raw anomalies $A^{(o)}$ are given as:\n$$ C^{(o)} = \\begin{bmatrix} 0.0 & 1.0 & 2.0 & 1.0 & 0.0 & -1.0 & -2.0 & -1.0 \\end{bmatrix} $$\n$$ A^{(o)} = \\begin{bmatrix} 0.5 & -0.5 & 1.0 & -1.0 & 0.2 & -0.2 & 0.0 & 0.0 \\end{bmatrix} $$\nThe full observation time series $O$ is the sum of its climatology and anomaly components, $O_t = C^{(o)}_t + A^{(o)}_t$:\n$$ O = \\begin{bmatrix} 0.5 & 0.5 & 3.0 & 0.0 & 0.2 & -1.2 & -2.0 & -1.0 \\end{bmatrix} $$\nThe observed anomaly sequence for verification, $O'$, is defined as $O^{\\prime}_t = O_t - C^{(o)}_t$, which simplifies to $O^{\\prime}_t = (C^{(o)}_t + A^{(o)}_t) - C^{(o)}_t = A^{(o)}_t$. So, $O' = A^{(o)}$.\n\nSimilarly, for each test case, the forecast anomaly sequence $F'$ is defined as $F^{\\prime}_t = F_t - C^{(f)}_t$. Since $F_t = C^{(f)}_t + A^{(f)}_t$, this simplifies to $F^{\\prime}_t = A^{(f)}_t$.\n\nThus, the core calculations for each case are:\n1.  Absolute-space RMSE: $\\mathrm{RMSE}(F, O)$\n2.  Absolute-space ACC: $\\mathrm{ACC}(F, O)$\n3.  Anomaly-space RMSE: $\\mathrm{RMSE}(A^{(f)}, A^{(o)})$\n4.  Anomaly-space ACC: $\\mathrm{ACC}(A^{(f)}, A^{(o)})$\n\nThe computational procedure will be implemented in Python using the `numpy` library for efficient vector operations. The ACC will be calculated according to the formula, handling the case of zero variance to prevent division by zero.\n\n**Test Case 1: Seasonal bias and amplitude error in climatology**\n- $C^{(f)} = 0.5 + 1.5 \\times C^{(o)} = \\begin{bmatrix} 0.5 & 2.0 & 3.5 & 2.0 & 0.5 & -1.0 & -2.5 & -1.0 \\end{bmatrix}$\n- $A^{(f)} = \\begin{bmatrix} 0.4 & -0.6 & 0.9 & -0.8 & 0.3 & -0.1 & 0.1 & -0.2 \\end{bmatrix}$\n- $F = C^{(f)} + A^{(f)} = \\begin{bmatrix} 0.9 & 1.4 & 4.4 & 1.2 & 0.8 & -1.1 & -2.4 & -1.2 \\end{bmatrix}$\n- Metrics are calculated for the pairs $(F, O)$ and $(A^{(f)}, A^{(o)})$.\n\n**Test Case 2: Climatology-only forecast**\n- $C^{(f)} = 0.5 + 1.5 \\times C^{(o)}$ (same as Case 1)\n- $A^{(f)} = \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix}$\n- $F = C^{(f)} + A^{(f)} = C^{(f)}$\n- The standard deviation of $A^{(f)}$ is $0$. Therefore, $\\mathrm{ACC}(A^{(f)}, A^{(o)})$ must be $0.0$ as per the problem definition.\n\n**Test Case 3: Perfect forecast**\n- $C^{(f)} = C^{(o)}$\n- $A^{(f)} = A^{(o)}$\n- $F = C^{(f)} + A^{(f)} = C^{(o)} + A^{(o)} = O$.\n- Since the forecast is identical to the observation, the error is zero and the correlation is one.\n- $\\mathrm{RMSE}(F, O) = 0.0$ and $\\mathrm{ACC}(F, O) = 1.0$.\n- Similarly, $A^{(f)} = A^{(o)}$, so $\\mathrm{RMSE}(A^{(f)}, A^{(o)}) = 0.0$ and $\\mathrm{ACC}(A^{(f)}, A^{(o)}) = 1.0$.\n\n**Test Case 4: Perfect climatology, opposite anomalies**\n- $C^{(f)} = C^{(o)}$\n- $A^{(f)} = -A^{(o)} = \\begin{bmatrix} -0.5 & 0.5 & -1.0 & 1.0 & -0.2 & 0.2 & 0.0 & 0.0 \\end{bmatrix}$\n- $F = C^{(f)} + A^{(f)} = C^{(o)} - A^{(o)}$\n- The error in the anomalies is $A^{(f)} - A^{(o)} = -2A^{(o)}$.\n- $\\mathrm{RMSE}(A^{(f)}, A^{(o)}) = \\sqrt{\\frac{1}{N}\\sum(-2A^{(o)}_t)^2} = 2 \\sqrt{\\frac{1}{N}\\sum(A^{(o)}_t)^2}$.\n- Since $A^{(f)}$ is perfectly anti-correlated with $A^{(o)}$, their correlation coefficient $\\mathrm{ACC}(A^{(f)}, A^{(o)})$ will be $-1.0$.\n\nThese steps will be implemented in the provided Python code block. For each test case, the four metrics will be computed, rounded to six decimal places, and collected into a list. The final output will be a list of these lists, formatted as a string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hindcast verification problem by calculating RMSE and ACC\n    in absolute and anomaly space for four test cases.\n    \"\"\"\n\n    def rmse(x, y):\n        \"\"\"Computes the Root Mean Square Error between two sequences.\"\"\"\n        return np.sqrt(np.mean((x - y) ** 2))\n\n    def acc(x, y):\n        \"\"\"\n        Computes the Anomaly Correlation Coefficient (Pearson correlation)\n        between two sequences. Handles the case of zero standard deviation.\n        \"\"\"\n        if x.size != y.size or x.size == 0:\n            return 0.0\n\n        mean_x = np.mean(x)\n        mean_y = np.mean(y)\n\n        var_x_sum = np.sum((x - mean_x) ** 2)\n        var_y_sum = np.sum((y - mean_y) ** 2)\n\n        # Handle degenerate cases where standard deviation is zero\n        if var_x_sum == 0 or var_y_sum == 0:\n            return 0.0\n\n        numerator = np.sum((x - mean_x) * (y - mean_y))\n        denominator = np.sqrt(var_x_sum) * np.sqrt(var_y_sum)\n        \n        return numerator / denominator\n\n    # Shared observed data\n    C_o = np.array([0.0, 1.0, 2.0, 1.0, 0.0, -1.0, -2.0, -1.0])\n    A_o = np.array([0.5, -0.5, 1.0, -1.0, 0.2, -0.2, 0.0, 0.0])\n    O = C_o + A_o\n    O_prime = A_o # Since O_prime = O - C_o\n\n    # Define the test cases\n    test_cases_defs = [\n        {\n            \"C_f_def\": lambda c: 0.5 + 1.5 * c,\n            \"A_f\": np.array([0.4, -0.6, 0.9, -0.8, 0.3, -0.1, 0.1, -0.2])\n        },\n        {\n            \"C_f_def\": lambda c: 0.5 + 1.5 * c,\n            \"A_f\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n        },\n        {\n            \"C_f_def\": lambda c: c,\n            \"A_f\": A_o\n        },\n        {\n            \"C_f_def\": lambda c: c,\n            \"A_f\": -A_o\n        }\n    ]\n\n    all_results = []\n\n    for case_def in test_cases_defs:\n        # Construct forecast series for the current case\n        C_f = case_def[\"C_f_def\"](C_o)\n        A_f = case_def[\"A_f\"]\n        F = C_f + A_f\n        F_prime = A_f # Since F_prime = F - C_f\n\n        # 1. Absolute-space RMSE\n        rmse_abs = rmse(F, O)\n        \n        # 2. Absolute-space ACC\n        acc_abs = acc(F, O)\n\n        # 3. Anomaly-space RMSE\n        rmse_anom = rmse(F_prime, O_prime)\n\n        # 4. Anomaly-space ACC\n        acc_anom = acc(F_prime, O_prime)\n\n        case_results = [\n            round(rmse_abs, 6),\n            round(acc_abs, 6),\n            round(rmse_anom, 6),\n            round(acc_anom, 6)\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    # The default str() for a list in Python adds spaces, which might be undesirable.\n    # To be precise and avoid spaces, we format it manually.\n    inner_strings = [f\"[{','.join(f'{val:.6f}' for val in res)}]\" for res in all_results]\n    final_output_str = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Traditional grid-point metrics can be misleading for spatial forecasts like precipitation, where a small displacement of a storm can be unfairly penalized as a complete miss. This hands-on exercise introduces you to the modern paradigm of object-based evaluation, which provides a more physically meaningful assessment by comparing forecast and observed features as whole entities . You will design a basic algorithm to match objects, quantify their location and overlap errors, and learn how to decompose the total error into distinct components, gaining deeper insight into model behavior.",
            "id": "4051779",
            "problem": "Consider the object-based evaluation of Numerical Weather Prediction (NWP) hindcasts and reforecasts in a two-dimensional gridded domain. The task is to design an algorithm that matches forecasted precipitation objects to observed precipitation objects using centroid distance and overlap, assesses location errors quantitatively, and then attributes how much of the total penalty score is due to displacement. The derivation and implementation must start from fundamental mathematical definitions only, without relying on any pre-existing verification shortcut formulas.\n\nThe domain is a rectangular grid with $N_x$ columns and $N_y$ rows, with uniform grid spacing $\\Delta$ in both directions. Physical coordinates are obtained by multiplying grid indices by $\\Delta$. Each object is a set of grid cells described by an axis-aligned rectangle with inclusive integer bounds $(i_{\\min}, j_{\\min}, i_{\\max}, j_{\\max})$, where $0 \\le i_{\\min} \\le i_{\\max} \\le N_x - 1$ and $0 \\le j_{\\min} \\le j_{\\max} \\le N_y - 1$. An object’s centroid is defined as the arithmetic mean of the physical coordinates of the centers of all grid cells belonging to that object. The centroid distance between a forecast and observed object is defined by the Euclidean distance between their centroids. Overlap is defined using the classical set-theoretic Intersection-over-Union (IoU) of cell sets.\n\nYour algorithm must, for each forecast–observed pair:\n- Derive the centroid of each object from first principles and compute the centroid distance in kilometers.\n- Derive the Intersection-over-Union in terms of set intersection and union sizes.\n- Use a physically interpretable matching rule based on a maximum centroid distance threshold and a minimum overlap threshold.\n- Construct a total penalty score composed of a displacement penalty and an overlap penalty, each normalized to the interval $[0,1]$, and combined by a convex combination with prescribed weights. Then compute the fraction of the total penalty attributable to displacement.\n\nYour derivation must begin from these fundamental bases:\n- The Euclidean distance between points $(x_1, y_1)$ and $(x_2, y_2)$ is $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$.\n- The centroid of a finite set of points $\\{(x_k, y_k)\\}_{k=1}^{M}$ is $\\left(\\frac{1}{M} \\sum_{k=1}^{M} x_k, \\frac{1}{M} \\sum_{k=1}^{M} y_k\\right)$.\n- For finite sets $A$ and $B$, the Intersection-over-Union is $\\mathrm{IoU}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$.\n- Normalization must use the domain’s physical diagonal length to scale centroid distances and must use the overlap deficit $\\left(1 - \\mathrm{IoU}\\right)$ as the normalized overlap penalty.\n\nExplicit implementation requirements:\n- The domain parameters are $N_x = 20$, $N_y = 20$, and $\\Delta = 5$ kilometers. Physical coordinates of cell centers are $(x, y) = (i \\Delta, j \\Delta)$ for integer indices $(i, j)$.\n- The matching decision is a boolean that is true if and only if both of the following hold: the centroid distance is less than or equal to $D_{\\mathrm{thr}}$ and the $\\mathrm{IoU}$ is greater than or equal to $\\tau_{\\mathrm{thr}}$, with $D_{\\mathrm{thr}} = 50$ kilometers and $\\tau_{\\mathrm{thr}} = 0.1$.\n- The total penalty score is a convex combination of a normalized displacement penalty and a normalized overlap penalty with weights $w_d = 0.6$ and $w_o = 0.4$, where $w_d + w_o = 1$.\n- The displacement contribution fraction must be computed as the part of the total penalty score due to displacement divided by the total penalty score, expressed as a decimal. If the total penalty score is zero, define the displacement contribution fraction to be $0$.\n\nTest suite:\nFor each test case, you are given one forecast rectangle and one observed rectangle, both specified by $(i_{\\min}, j_{\\min}, i_{\\max}, j_{\\max})$. Use the domain and thresholds specified above. Compute the following outputs for each case:\n- The matching decision (boolean).\n- The centroid distance in kilometers, rounded to two decimal places.\n- The Intersection-over-Union, rounded to four decimal places.\n- The total penalty score, rounded to four decimal places.\n- The displacement contribution fraction, rounded to four decimal places.\n\nTest cases:\n- Case $1$: Forecast $(5, 5, 9, 9)$, Observed $(6, 7, 10, 11)$.\n- Case $2$: Forecast $(8, 4, 12, 8)$, Observed $(8, 4, 12, 8)$.\n- Case $3$: Forecast $(1, 1, 3, 3)$, Observed $(15, 15, 18, 18)$.\n- Case $4$: Forecast $(10, 10, 10, 10)$, Observed $(11, 10, 11, 10)$.\n- Case $5$: Forecast $(0, 0, 5, 5)$, Observed $(5, 5, 10, 10)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list in the exact order: $[\\text{match}, \\text{distance\\_km}, \\text{IoU}, \\text{total\\_score}, \\text{disp\\_fraction}]$. For example, the output must look like $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$ on a single line with no additional text. All distances must be expressed in kilometers and rounded to two decimal places; all other floats must be rounded to four decimal places as specified.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective, providing a complete and consistent set of definitions and parameters for a solvable computational task relevant to numerical weather prediction evaluation. We proceed with a step-by-step derivation of the required quantities.\n\nThe domain is a $2$-dimensional grid of size $N_x \\times N_y$, with grid indices $i$ and $j$ ranging from $0$ to $N_x - 1$ and $0$ to $N_y - 1$, respectively. The physical coordinate of the center of a grid cell $(i, j)$ is given by $(x, y) = (i\\Delta, j\\Delta)$, where $\\Delta$ is the uniform grid spacing.\n\nAn object is defined by an axis-aligned rectangle with inclusive integer bounds $(i_{\\min}, j_{\\min}, i_{\\max}, j_{\\max})$. This represents the set of all grid cells $(i, j)$ such that $i_{\\min} \\le i \\le i_{\\max}$ and $j_{\\min} \\le j \\le j_{\\max}$.\n\n**1. Centroid of an Object**\n\nThe centroid $(\\bar{x}, \\bar{y})$ of an object is the arithmetic mean of the physical coordinates of the centers of all its constituent grid cells. An object defined by $(i_{\\min}, j_{\\min}, i_{\\max}, j_{\\max})$ contains $M$ cells, where $M = (i_{\\max} - i_{\\min} + 1) \\times (j_{\\max} - j_{\\min} + 1)$.\n\nThe set of x-coordinates of the cell centers is $\\{i\\Delta \\mid i_{\\min} \\le i \\le i_{\\max}\\}$. Each x-coordinate is repeated $(j_{\\max} - j_{\\min} + 1)$ times.\nThe x-component of the centroid, $\\bar{x}$, is:\n$$ \\bar{x} = \\frac{1}{M} \\sum_{i=i_{\\min}}^{i_{\\max}} \\sum_{j=j_{\\min}}^{j_{\\max}} (i\\Delta) $$\nThe inner sum over $j$ is a sum of a constant term, so it simplifies to multiplication by the number of terms, which is $(j_{\\max} - j_{\\min} + 1)$.\n$$ \\bar{x} = \\frac{1}{(i_{\\max} - i_{\\min} + 1)(j_{\\max} - j_{\\min} + 1)} (j_{\\max} - j_{\\min} + 1) \\sum_{i=i_{\\min}}^{i_{\\max}} (i\\Delta) $$\n$$ \\bar{x} = \\frac{\\Delta}{i_{\\max} - i_{\\min} + 1} \\sum_{i=i_{\\min}}^{i_{\\max}} i $$\nThe sum is an arithmetic series $\\sum_{k=a}^{b} k = \\frac{(b-a+1)(a+b)}{2}$. Applying this:\n$$ \\sum_{i=i_{\\min}}^{i_{\\max}} i = \\frac{(i_{\\max} - i_{\\min} + 1)(i_{\\min} + i_{\\max})}{2} $$\nSubstituting this back into the expression for $\\bar{x}$:\n$$ \\bar{x} = \\frac{\\Delta}{i_{\\max} - i_{\\min} + 1} \\left( \\frac{(i_{\\max} - i_{\\min} + 1)(i_{\\min} + i_{\\max})}{2} \\right) = \\frac{i_{\\min} + i_{\\max}}{2} \\Delta $$\nBy symmetry, the y-component of the centroid is:\n$$ \\bar{y} = \\frac{j_{\\min} + j_{\\max}}{2} \\Delta $$\nThus, the centroid of an object defined by $(i_{\\min}, j_{\\min}, i_{\\max}, j_{\\max})$ is $(\\bar{x}, \\bar{y}) = \\left( \\frac{i_{\\min} + i_{\\max}}{2}\\Delta, \\frac{j_{\\min} + j_{\\max}}{2}\\Delta \\right)$.\n\n**2. Centroid Distance**\n\nGiven a forecast object $F$ and an observed object $O$ with centroids $(\\bar{x}_F, \\bar{y}_F)$ and $(\\bar{x}_O, \\bar{y}_O)$ respectively, the centroid distance $D$ is the Euclidean distance between them:\n$$ D = \\sqrt{(\\bar{x}_F - \\bar{x}_O)^2 + (\\bar{y}_F - \\bar{y}_O)^2} $$\n\n**3. Intersection-over-Union (IoU)**\n\nThe IoU for a forecast object $F$ and an observed object $O$ is defined as the ratio of the size of their intersection to the size of their union:\n$$ \\mathrm{IoU}(F, O) = \\frac{|F \\cap O|}{|F \\cup O|} $$\nThe size of a set of cells is its cardinality. The size of the union can be expressed using the principle of inclusion-exclusion: $|F \\cup O| = |F| + |O| - |F \\cap O|$.\nLet $F$ be defined by $(i_{F,\\min}, j_{F,\\min}, i_{F,\\max}, j_{F,\\max})$ and $O$ by $(i_{O,\\min}, j_{O,\\min}, i_{O,\\max}, j_{O,\\max})$.\nThe size of an object is the number of cells it contains:\n$$ |A| = (i_{A,\\max} - i_{A,\\min} + 1) \\times (j_{A,\\max} - j_{A,\\min} + 1) $$\nfor any object $A$.\nThe intersection $F \\cap O$ is also a rectangle, whose bounds are:\n- $i_{int,\\min} = \\max(i_{F,\\min}, i_{O,\\min})$\n- $j_{int,\\min} = \\max(j_{F,\\min}, j_{O,\\min})$\n- $i_{int,\\max} = \\min(i_{F,\\max}, i_{O,\\max})$\n- $j_{int,\\max} = \\min(j_{F,\\max}, j_{O,\\max})$\n\nThe width of the intersection is $w_{int} = i_{int,\\max} - i_{int,\\min} + 1$ and the height is $h_{int} = j_{int,\\max} - j_{int,\\min} + 1$. The intersection is non-empty only if $w_{int} > 0$ and $h_{int} > 0$. If the intersection is non-empty, its size is $|F \\cap O| = w_{int} \\times h_{int}$. Otherwise, its size is $0$.\n\n**4. Matching Rule**\n\nA forecast-observed pair is considered a match if and only if their centroid distance $D$ is less than or equal to a threshold $D_{\\mathrm{thr}}$ AND their IoU is greater than or equal to a threshold $\\tau_{\\mathrm{thr}}$.\nThe problem provides $D_{\\mathrm{thr}} = 50$ km and $\\tau_{\\mathrm{thr}} = 0.1$.\nMatch = $(D \\le D_{\\mathrm{thr}}) \\land (\\mathrm{IoU} \\ge \\tau_{\\mathrm{thr}})$.\n\n**5. Penalty Score Formulation**\n\nThe total penalty score $S_{total}$ is a convex combination of a normalized displacement penalty $P_d$ and a normalized overlap penalty $P_o$:\n$$ S_{total} = w_d P_d + w_o P_o $$\nwith weights $w_d = 0.6$ and $w_o = 0.4$.\n\nThe displacement penalty $P_d$ is the centroid distance $D$ normalized by the physical diagonal length of the domain, $L_{diag}$:\n$$ P_d = \\frac{D}{L_{diag}} $$\nThe physical width of the domain is $W = (N_x - 1)\\Delta$ and the height is $H = (N_y - 1)\\Delta$.\n$$ L_{diag} = \\sqrt{W^2 + H^2} = \\Delta \\sqrt{(N_x-1)^2 + (N_y-1)^2} $$\nFor the given parameters $N_x = 20$, $N_y = 20$, and $\\Delta = 5$ km:\n$$ L_{diag} = 5 \\sqrt{(20-1)^2 + (20-1)^2} = 5 \\sqrt{19^2 + 19^2} = 5 \\sqrt{2 \\cdot 19^2} = 95\\sqrt{2} \\text{ km} $$\n\nThe overlap penalty $P_o$ is the overlap deficit:\n$$ P_o = 1 - \\mathrm{IoU} $$\nSince $0 \\le \\mathrm{IoU} \\le 1$, $P_o$ is naturally normalized to the interval $[0, 1]$. We can assume $P_d$ is capped at $1$ if $D > L_{diag}$.\n\n**6. Displacement Contribution Fraction**\n\nThe fraction of the total penalty attributable to displacement, $F_{disp}$, is the ratio of the weighted displacement penalty component to the total penalty score:\n$$ F_{disp} = \\frac{w_d P_d}{S_{total}} = \\frac{w_d P_d}{w_d P_d + w_o P_o} $$\nIf $S_{total} = 0$, which occurs only for a perfect match where $D=0$ and $\\mathrm{IoU}=1$, the fraction $F_{disp}$ is defined to be $0$.\n\nThe algorithm will now be implemented to process the given test cases according to these derivations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the object-based evaluation problem for the given test cases.\n    \"\"\"\n    # Domain and model parameters\n    NX = 20\n    NY = 20\n    DELTA = 5.0  # kilometers\n    D_THR = 50.0  # kilometers\n    TAU_THR = 0.1\n    W_D = 0.6\n    W_O = 0.4\n\n    # Pre-calculate domain diagonal length for normalization\n    domain_diag_len = DELTA * np.sqrt((NX - 1)**2 + (NY - 1)**2)\n\n    def calculate_metrics(forecast_rect, observed_rect):\n        \"\"\"\n        Calculates all required metrics for a single forecast-observed pair.\n        \"\"\"\n        # Unpack rectangle bounds\n        i_f_min, j_f_min, i_f_max, j_f_max = forecast_rect\n        i_o_min, j_o_min, i_o_max, j_o_max = observed_rect\n\n        # 1. Centroid Calculation\n        # Centroid is at the center of the rectangle's index range, scaled by DELTA\n        cx_f = (i_f_min + i_f_max) / 2.0 * DELTA\n        cy_f = (j_f_min + j_f_max) / 2.0 * DELTA\n        cx_o = (i_o_min + i_o_max) / 2.0 * DELTA\n        cy_o = (j_o_min + j_o_max) / 2.0 * DELTA\n\n        # 2. Centroid Distance\n        dist_km = np.sqrt((cx_f - cx_o)**2 + (cy_f - cy_o)**2)\n\n        # 3. Intersection-over-Union (IoU)\n        # Calculate intersection rectangle\n        i_int_min = max(i_f_min, i_o_min)\n        j_int_min = max(j_f_min, j_o_min)\n        i_int_max = min(i_f_max, i_o_max)\n        j_int_max = min(j_f_max, j_o_max)\n\n        # Calculate intersection area (number of cells)\n        int_width = (i_int_max - i_int_min + 1)\n        int_height = (j_int_max - j_int_min + 1)\n        \n        if int_width > 0 and int_height > 0:\n            intersection_area = int_width * int_height\n        else:\n            intersection_area = 0\n\n        # Calculate area of each object\n        forecast_area = (i_f_max - i_f_min + 1) * (j_f_max - j_f_min + 1)\n        observed_area = (i_o_max - i_o_min + 1) * (j_o_max - j_o_min + 1)\n        \n        # Calculate union area\n        union_area = forecast_area + observed_area - intersection_area\n        \n        iou = intersection_area / union_area if union_area > 0 else 0.0\n\n        # 4. Matching Decision\n        is_match = (dist_km = D_THR) and (iou >= TAU_THR)\n\n        # 5. Penalty Score\n        # Normalized displacement penalty (capped at 1.0)\n        p_d = min(dist_km / domain_diag_len, 1.0)\n        # Normalized overlap penalty\n        p_o = 1.0 - iou\n        \n        total_score = W_D * p_d + W_O * p_o\n\n        # 6. Displacement Contribution Fraction\n        if total_score == 0:\n            disp_fraction = 0.0\n        else:\n            disp_fraction = (W_D * p_d) / total_score\n            \n        # Format results as specified\n        result = [\n            is_match,\n            round(dist_km, 2),\n            round(iou, 4),\n            round(total_score, 4),\n            round(disp_fraction, 4)\n        ]\n        return result\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ((5, 5, 9, 9), (6, 7, 10, 11)),   # Case 1\n        ((8, 4, 12, 8), (8, 4, 12, 8)),   # Case 2\n        ((1, 1, 3, 3), (15, 15, 18, 18)), # Case 3\n        ((10, 10, 10, 10), (11, 10, 11, 10)), # Case 4\n        ((0, 0, 5, 5), (5, 5, 10, 10))    # Case 5\n    ]\n\n    results = []\n    for f_rect, o_rect in test_cases:\n        results.append(calculate_metrics(f_rect, o_rect))\n\n    # Final print statement in the exact required format.\n    # The format [ [...] , [...] ] is the Python string representation of a list of lists.\n    # Using str() on the list of lists achieves this directly.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}