{
    "hands_on_practices": [
        {
            "introduction": "Before we can correct for deficiencies in an ensemble, we must first learn how to diagnose them. The rank histogram is a fundamental tool for assessing the statistical reliability of an ensemble forecast. This exercise challenges you to derive, from first principles, the expected signature of a \"perfect\" ensemble, where the forecast members and the verifying observation are statistically indistinguishable. Understanding this theoretical baseline is the first step toward interpreting the rank histograms of real-world forecasts and diagnosing common issues like underdispersion, which covariance inflation aims to correct .",
            "id": "4036312",
            "problem": "Consider a data assimilation system for numerical weather prediction using the Ensemble Kalman Filter (EnKF), where covariance localization and multiplicative inflation are applied to control sampling error and underdispersion. An ensemble forecast at a given verification time consists of $N$ members, denoted $\\{X_{1}, X_{2}, \\dots, X_{N}\\}$, and an independent observation $Y$. A rank histogram is constructed by sorting the ensemble values and counting in which of the $N+1$ intervals the observation falls: below $X_{(1)}$, between $X_{(k)}$ and $X_{(k+1)}$ for $k=1,\\dots,N-1$, or above $X_{(N)}$, where $X_{(1)} \\leq X_{(2)} \\leq \\dots \\leq X_{(N)}$ denote the order statistics of the ensemble. Under perfect reliability, the ensemble members and the observation are exchangeable draws from the same continuous predictive distribution, so that the ensemble spread and mean are statistically consistent with the realized $Y$. Starting from first principles and well-tested facts about order statistics and the Probability Integral Transform (PIT), derive the expected probability $P_{k}$ that $Y$ falls into the $k$-th rank bin, for $k=0,1,\\dots,N$, expressed as a closed-form function of $N$ only. Your final answer must be a single closed-form expression. No units are required. Do not round your answer.",
            "solution": "The problem asks for the expected probability, $P_k$, that an observation $Y$ falls into the $k$-th rank bin of an ensemble forecast comprised of $N$ members. The ranks are indexed from $k=0$ to $k=N$, corresponding to $N+1$ possible bins. The fundamental assumption is that the ensemble is \"perfectly reliable,\" which implies that the ensemble members $\\{X_1, X_2, \\dots, X_N\\}$ and the observation $Y$ are exchangeable draws from the same continuous predictive distribution. For a continuous distribution, this is equivalent to stating that a set of $N+1$ random variables $\\{X_1, \\dots, X_N, Y\\}$ are independent and identically distributed (i.i.d.).\n\nLet the common, continuous, and strictly monotonically increasing cumulative distribution function (CDF) of these random variables be $F(x) = P(X \\le x)$.\n\nFirst, we must formalize the definition of the rank bins. The problem text describes the $N+1$ bins as: \"below $X_{(1)}$\", \"between $X_{(k)}$ and $X_{(k+1)}$ for $k=1,\\dots,N-1$\", and \"above $X_{(N)}$\". Here, $X_{(j)}$ is the $j$-th order statistic of the ensemble, such that $X_{(1)} \\le X_{(2)} \\le \\dots \\le X_{(N)}$. Since the parent distribution is continuous, the probability of any two draws being equal is $0$, so we can use strict inequalities, $X_{(1)} < X_{(2)} < \\dots < X_{(N)}$.\n\nThe event that $Y$ falls into the $k$-th rank bin, for $k \\in \\{0, 1, \\dots, N\\}$, corresponds to the number of ensemble members that are smaller than $Y$. Let us verify this mapping:\n- If $Y$ falls \"below $X_{(1)}$\", then $Y < X_{(1)}$. This means $Y$ is smaller than all $N$ ensemble members. The number of ensemble members smaller than $Y$ is $0$. This corresponds to rank $k=0$.\n- If $Y$ falls \"between $X_{(j)}$ and $X_{(j+1)}$\" for $j \\in \\{1, \\dots, N-1\\}$, then $X_{(j)} < Y < X_{(j+1)}$. This means there are exactly $j$ ensemble members smaller than $Y$. This corresponds to rank $k=j$.\n- If $Y$ falls \"above $X_{(N)}$\", then $Y > X_{(N)}$. This means $Y$ is larger than all $N$ ensemble members. The number of ensemble members smaller than $Y$ is $N$. This corresponds to rank $k=N$.\n\nThis establishes a clear, unambiguous link: $P_k$ is the probability that exactly $k$ of the ensemble members $\\{X_i\\}$ are smaller than the observation $Y$.\n\nTo derive this probability, we use the Probability Integral Transform (PIT), as suggested by the problem statement. The PIT states that if a random variable $Z$ has a continuous CDF $F_Z(z)$, then the random variable $U = F_Z(Z)$ is uniformly distributed on the interval $[0, 1]$.\nWe apply this transform to all $N+1$ i.i.d. random variables:\n- Let $U_i = F(X_i)$ for $i = 1, \\dots, N$.\n- Let $V = F(Y)$.\nThe resulting set of $N+1$ random variables $\\{U_1, \\dots, U_N, V\\}$ are i.i.d. draws from a Uniform$(0,1)$ distribution.\n\nSince $F$ is a strictly increasing function, it preserves order. That is, $X_i < Y$ if and only if $F(X_i) < F(Y)$, which is equivalent to $U_i < V$. Therefore, the problem of finding the probability that exactly $k$ of the $X_i$ are less than $Y$ is identical to finding the probability that exactly $k$ of the $U_i$ are less than $V$.\n\nLet's calculate this probability, $P_k = P(\\text{exactly } k \\text{ of } \\{U_i\\}_{i=1}^N \\text{ are less than } V)$. We can use the law of total probability by conditioning on the value of $V$. The probability density function (PDF) of $V$ is $f_V(v) = 1$ for $v \\in (0, 1)$ and $0$ otherwise.\n$$P_k = \\int_0^1 P(\\text{exactly } k \\text{ of } \\{U_i\\} < v \\mid V=v) f_V(v) \\, dv$$\n\nFor a fixed value $V=v$, each $U_i$ is an independent draw from $U(0,1)$. The event $U_i < v$ is a Bernoulli trial with success probability $p = P(U_i < v) = v$. We are performing $N$ such independent trials. The number of successes (the number of $U_i$ that are less than $v$) follows a binomial distribution $B(N, p=v)$. The probability of exactly $k$ successes is given by the binomial probability mass function:\n$$P(\\text{exactly } k \\text{ of } \\{U_i\\} < v) = \\binom{N}{k} v^k (1-v)^{N-k}$$\n\nSubstituting this into our integral for $P_k$:\n$$P_k = \\int_0^1 \\binom{N}{k} v^k (1-v)^{N-k} \\cdot 1 \\, dv$$\nWe can take the binomial coefficient outside the integral, as it does not depend on $v$:\n$$P_k = \\binom{N}{k} \\int_0^1 v^k (1-v)^{N-k} \\, dv$$\n\nThe integral is a standard form related to the Beta function, which is defined as $B(x, y) = \\int_0^1 t^{x-1} (1-t)^{y-1} \\, dt$. Our integral corresponds to $B(k+1, N-k+1)$.\nThe Beta function can be expressed in terms of the Gamma function, $\\Gamma(z)$, as $B(x, y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$. For integer arguments $n$, $\\Gamma(n) = (n-1)!$.\nSo, we can evaluate the integral:\n$$\\int_0^1 v^k (1-v)^{N-k} \\, dv = B(k+1, N-k+1) = \\frac{\\Gamma(k+1)\\Gamma(N-k+1)}{\\Gamma((k+1)+(N-k+1))} = \\frac{\\Gamma(k+1)\\Gamma(N-k+1)}{\\Gamma(N+2)}$$\nUsing the factorial relation for the Gamma function:\n$$B(k+1, N-k+1) = \\frac{k!(N-k)!}{(N+1)!}$$\n\nNow we substitute this result back into the expression for $P_k$:\n$$P_k = \\binom{N}{k} \\frac{k!(N-k)!}{(N+1)!}$$\nThe binomial coefficient is defined as $\\binom{N}{k} = \\frac{N!}{k!(N-k)!}$. Substituting this definition:\n$$P_k = \\left( \\frac{N!}{k!(N-k)!} \\right) \\left( \\frac{k!(N-k)!}{(N+1)!} \\right)$$\nThe terms $k!$ and $(N-k)!$ cancel out:\n$$P_k = \\frac{N!}{(N+1)!}$$\nSince $(N+1)! = (N+1) \\times N!$, we get the final simplification:\n$$P_k = \\frac{N!}{(N+1) N!} = \\frac{1}{N+1}$$\n\nThis result is independent of the rank $k$. This means that for a perfectly reliable ensemble, the observation $Y$ is equally likely to fall into any of the $N+1$ rank bins. The expected shape of the rank histogram is therefore uniform, or \"flat\". The probability for any given bin $k \\in \\{0, 1, \\dots, N\\}$ is a function of the ensemble size $N$ only.",
            "answer": "$$\\boxed{\\frac{1}{N+1}}$$"
        },
        {
            "introduction": "Ensemble data assimilation systems often operate in a regime where the number of ensemble members ($N$) is far smaller than the dimension of the model state ($n$). This leads to a rank-deficient sample covariance matrix, creating spurious correlations and directions in state space with zero variance. This practice provides a stark, hands-on demonstration of what happens when an observation falls into one of these \"blind spots.\" By working through this simple system, you will see precisely how the Kalman gain becomes zero for an observed variable with no ensemble spread, causing the filter to completely ignore the observation's informationâ€”a critical failure that motivates the use of inflation and localization .",
            "id": "4036305",
            "problem": "Consider a linear Gaussian data assimilation setting in numerical weather prediction and climate modeling with a state vector $x \\in \\mathbb{R}^{3}$ representing three coarse-grained, unitless prognostic variables at nearby grid points along a transect. An ensemble of $N=2$ members is used to represent prior uncertainty for the background mean state $m^{b} \\in \\mathbb{R}^{3}$. Let the ensemble members be\n$$\nx^{(1)} = m^{b} + \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad\nx^{(2)} = m^{b} + \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nA single scalar observation targets the third component with observation operator $H = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}$, observed value $y \\in \\mathbb{R}$, and observation error variance $R = 0.25$. Suppose the innovation is $d = y - H m^{b} = 1$. Before analysis, a multiplicative inflation with factor $\\gamma = 1.5$ is applied to the ensemble anomalies. A covariance localization via the Schur product with localization matrix $L \\in \\mathbb{R}^{3 \\times 3}$ is then applied, where $L$ is the identity matrix.\n\nStarting from standard linear-Gaussian assumptions, the background ensemble sample covariance $P^{e}$ is defined by the ensemble anomalies and the normalized second-moment estimator, and the analysis mean increment is given by the linear update determined by the Kalman gain. Using only these fundamental bases, do the following:\n\n1. Construct the anomaly matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and explicitly compute the ensemble sample covariance $P^{e}$ for the given $N=2$ and $n=3$ configuration.\n2. Apply multiplicative inflation to obtain the inflated prior covariance $P^{f}$ from $P^{e}$, and then apply the stated localization to obtain $P^{\\text{loc}}$.\n3. Using the linear-Gaussian analysis rule, compute the analysis mean increment vector $\\Delta m^{a}$ produced by assimilating the single observation with innovation $d$.\n\nReport as your final answer the first component of the analysis mean increment, that is, the scalar $\\Delta m^{a}_{1}$. Express your final answer as a dimensionless real number. No rounding is required.",
            "solution": "The problem will be solved by following the required steps: constructing the prior ensemble covariance matrix, applying inflation and localization, and then computing the analysis mean increment using the Kalman gain formula from linear-Gaussian data assimilation theory.\n\n**Step 1: Construct the Anomaly Matrix and Ensemble Sample Covariance**\n\nThe state vector is in $\\mathbb{R}^{3}$ and the ensemble has $N=2$ members. The ensemble members are given as deviations from the background mean state $m^{b}$:\n$$\nx^{(1)} = m^{b} + \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad\nx^{(2)} = m^{b} + \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe ensemble mean is $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x^{(i)}$. For $N=2$, this is:\n$$\n\\bar{x} = \\frac{1}{2} \\left[ \\left( m^{b} + \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} \\right) + \\left( m^{b} + \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) \\right] = \\frac{1}{2} \\left[ 2m^b + \\begin{pmatrix} 2-2 \\\\ -1+1 \\\\ 0+0 \\end{pmatrix} \\right] = m^b\n$$\nThus, the background mean state $m^{b}$ is the ensemble mean. The ensemble anomalies (deviations from the mean) are:\n$$\nx'^{(1)} = x^{(1)} - \\bar{x} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad\nx'^{(2)} = x^{(2)} - \\bar{x} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe anomaly matrix $A \\in \\mathbb{R}^{3 \\times 2}$ is formed by these column vectors:\n$$\nA = \\begin{pmatrix} 2 & -2 \\\\ -1 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe ensemble sample covariance matrix $P^{e}$ is defined by the normalized second-moment estimator:\n$$\nP^{e} = \\frac{1}{N-1} A A^T\n$$\nWith $N=2$, the normalization factor is $\\frac{1}{2-1} = 1$. Therefore:\n$$\nP^{e} = A A^T = \\begin{pmatrix} 2 & -2 \\\\ -1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 0 \\\\ -2 & 1 & 0 \\end{pmatrix}\n$$\n$$\nP^{e} = \\begin{pmatrix} (2)(2)+(-2)(-2) & (2)(-1)+(-2)(1) & (2)(0)+(-2)(0) \\\\ (-1)(2)+(1)(-2) & (-1)(-1)+(1)(1) & (-1)(0)+(1)(0) \\\\ (0)(2)+(0)(-2) & (0)(-1)+(0)(1) & (0)(0)+(0)(0) \\end{pmatrix}\n$$\n$$\nP^{e} = \\begin{pmatrix} 4+4 & -2-2 & 0 \\\\ -2-2 & 1+1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 8 & -4 & 0 \\\\ -4 & 2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n\n**Step 2: Apply Multiplicative Inflation and Covariance Localization**\n\nMultiplicative inflation with a factor $\\gamma=1.5$ is applied to the ensemble anomalies. This is equivalent to scaling the covariance matrix by $\\gamma^2$. The inflated prior covariance $P^{f}$ is:\n$$\nP^{f} = \\gamma^2 P^{e} = (1.5)^2 P^{e} = 2.25 P^{e}\n$$\n$$\nP^{f} = 2.25 \\begin{pmatrix} 8 & -4 & 0 \\\\ -4 & 2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 18 & -9 & 0 \\\\ -9 & 4.5 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nNext, covariance localization is applied via the Schur (element-wise) product with the localization matrix $L$, which is the identity matrix $I$.\n$$\nP^{\\text{loc}} = L \\circ P^{f} = I \\circ P^{f}\n$$\n$$\nP^{\\text{loc}} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\circ \\begin{pmatrix} 18 & -9 & 0 \\\\ -9 & 4.5 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nP^{\\text{loc}} = \\begin{pmatrix} 1 \\times 18 & 0 \\times (-9) & 0 \\times 0 \\\\ 0 \\times (-9) & 1 \\times 4.5 & 0 \\times 0 \\\\ 0 \\times 0 & 0 \\times 0 & 1 \\times 0 \\end{pmatrix} = \\begin{pmatrix} 18 & 0 & 0 \\\\ 0 & 4.5 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThis matrix, $P^{\\text{loc}}$, is the final prior error covariance used for the analysis update. Let's denote it as $P_{b} = P^{\\text{loc}}$.\n\n**Step 3: Compute the Analysis Mean Increment**\n\nThe analysis mean update is given by $m^{a} = m^{b} + K(y - H m^{b})$, where $K$ is the Kalman gain and $d = y - Hm^{b}$ is the innovation. The analysis mean increment is $\\Delta m^{a} = m^{a} - m^{b} = K d$.\nThe Kalman gain is defined as:\n$$\nK = P_{b} H^T (H P_{b} H^T + R)^{-1}\n$$\nThe given quantities are:\n- Observation operator: $H = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}$\n- Observation error variance: $R = 0.25$\n- Innovation: $d = 1$\n- Prior error covariance: $P_{b} = P^{\\text{loc}} = \\begin{pmatrix} 18 & 0 & 0 \\\\ 0 & 4.5 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$\n\nWe compute the terms required for the Kalman gain:\n$$\nP_{b} H^T = \\begin{pmatrix} 18 & 0 & 0 \\\\ 0 & 4.5 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 18(0) + 0(0) + 0(1) \\\\ 0(0) + 4.5(0) + 0(1) \\\\ 0(0) + 0(0) + 0(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe numerator of the Kalman gain expression is the zero vector. This results from the fact that the prior ensemble has zero spread (and thus zero covariance) for the third state component, which is the only component being observed.\nThe term in the inverse is the prior variance in observation space plus the observation error variance:\n$$\nH P_{b} H^T = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 18 & 0 & 0 \\\\ 0 & 4.5 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0\n$$\n$$\nH P_{b} H^T + R = 0 + 0.25 = 0.25\n$$\nThe inverse is $(0.25)^{-1} = 4$.\nNow, we can compute the Kalman gain $K$:\n$$\nK = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} (4) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe analysis mean increment vector $\\Delta m^{a}$ is:\n$$\n\\Delta m^{a} = K d = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe problem asks for the first component of this vector, $\\Delta m^{a}_{1}$.\n$$\n\\Delta m^{a}_{1} = 0\n$$\nThe update is zero because the prior model state has zero uncertainty in the observed variable, meaning the assimilation system assigns zero weight to the new observation for updating the state.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Tuning the parameters of a data assimilation system, such as the inflation factor and localization radius, can often feel like an ad-hoc process. The Desroziers diagnostics offer a principled, quantitative method for this task by checking for statistical consistency between the filter's assumptions and its actual performance. This exercise guides you through the derivation and application of these powerful diagnostics, which link the covariances of observation-space departures to the error covariances assumed by the filter. Mastering this technique bridges the gap between abstract assimilation theory and the practical challenge of building a well-calibrated ensemble system .",
            "id": "4036278",
            "problem": "Consider a linear-Gaussian data assimilation setting typical of Numerical Weather Prediction and climate reanalysis. Let the true state be denoted by $x^{t} \\in \\mathbb{R}^{n}$, the background (prior) state by $x^{b} \\in \\mathbb{R}^{n}$, and the analysis (posterior) state by $x^{a} \\in \\mathbb{R}^{n}$. Observations are $y \\in \\mathbb{R}^{m}$ with a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ so that $y = H x^{t} + \\epsilon^{o}$. Assume unbiased, zero-mean, Gaussian errors with covariances $\\operatorname{cov}(\\epsilon^{o}) = R \\in \\mathbb{R}^{m \\times m}$ and $\\operatorname{cov}(\\epsilon^{b}) = P^{b} \\in \\mathbb{R}^{n \\times n}$, and assume $\\epsilon^{o}$ and $\\epsilon^{b}$ are independent. Define the departures in observation space\n$$\nd^{o} := y - H x^{t}, \\quad d^{b} := y - H x^{b}, \\quad d^{a} := y - H x^{a}.\n$$\nLet the analysis be defined by a linear Kalman update $x^{a} = x^{b} + K (y - H x^{b})$ with gain $K \\in \\mathbb{R}^{n \\times m}$, and define the analysis error covariance $P^{a} := \\operatorname{cov}(x^{a} - x^{t})$.\n\na) Starting from these definitions and the linear-Gaussian assumptions, derive explicit expressions for $\\mathbb{E}[d^{o}]$, $\\mathbb{E}[d^{o} (d^{b})^{\\top}]$, and $\\mathbb{E}[d^{o} (d^{a})^{\\top}]$ in terms of $R$, $H$, and $P^{a}$.\n\nb) In an Ensemble Kalman Filter (EnKF), background covariances are often localized by a Schur (element-wise) product $P^{b}_{\\text{loc}} = C(L) \\circ P^{b}_{\\text{ens}}$, where $P^{b}_{\\text{ens}}$ is the raw ensemble background covariance, $C(L)$ is a correlation matrix determined by a localization function with length scale $L$, and $\\circ$ denotes element-wise multiplication. Explain how the relationships from part (a) can be used to define a diagnostic objective in observation space that depends on $L$ and guides tuning of $L$ by reducing mismatches between empirical cross-statistics of departures and their theoretical targets.\n\nc) Consider a scalar ($n = m = 1$) instantiation used to co-tune multiplicative inflation with localization, focusing here on inflation. Let $H = 1$, the raw ensemble background variance be $p^{b}_{\\text{ens}} = 1.0$, and the observation error variance be $r = 0.5$. Apply scalar multiplicative inflation $\\lambda > 0$ to the background variance before analysis, so that $p^{b} = \\lambda \\, p^{b}_{\\text{ens}}$. Suppose the empirical estimate of $\\mathbb{E}[d^{o} d^{a}]$ computed from a long sequence of analysis cycles is $s = 0.35$. Under the linear-Gaussian Kalman assumptions, find the value of $\\lambda$ such that the diagnostic identity from part (a) is satisfied in expectation for $\\mathbb{E}[d^{o} d^{a}]$. Round your final answer to four significant figures. Express your final answer as a dimensionless number.",
            "solution": "a) The expressions for the expected values of the departure statistics are derived below. We assume the standard linear-Gaussian framework, zero-mean errors, and that the Kalman gain $K$ is optimal, which is implicit in this context.\n\nFirst, we express the departures in terms of the fundamental error quantities. The observation error is $\\epsilon^{o}$ and the background error is $\\epsilon^{b} = x^{b} - x^{t}$.\nThe departure $d^{o}$ (observation minus truth) is:\n$$\nd^{o} = y - H x^{t} = (H x^{t} + \\epsilon^{o}) - H x^{t} = \\epsilon^{o}\n$$\nThe departure $d^{b}$ (observation minus background), also known as the innovation, is:\n$$\nd^{b} = y - H x^{b} = (H x^{t} + \\epsilon^{o}) - H x^{b} = H(x^{t} - x^{b}) + \\epsilon^{o} = -H \\epsilon^{b} + \\epsilon^{o}\n$$\nThe departure $d^{a}$ (observation minus analysis), or analysis residual, is:\n$$\nd^{a} = y - H x^{a} = H(x^{t} - x^{a}) + \\epsilon^{o} = -H \\epsilon^{a} + \\epsilon^{o}\n$$\nwhere $\\epsilon^{a} = x^{a} - x^{t}$ is the analysis error.\n\nNow, we compute the requested expected values.\n\n1.  **$\\mathbb{E}[d^{o}]$**:\n    The expected value of the observation departure is the expected value of the observation error.\n    $$\n    \\mathbb{E}[d^{o}] = \\mathbb{E}[\\epsilon^{o}]\n    $$\n    By the problem's premise, the observation error is zero-mean.\n    $$\n    \\mathbb{E}[d^{o}] = 0\n    $$\n\n2.  **$\\mathbb{E}[d^{o} (d^{b})^{\\top}]$**:\n    We substitute the expressions for $d^{o}$ and $d^{b}$:\n    $$\n    \\mathbb{E}[d^{o} (d^{b})^{\\top}] = \\mathbb{E}[\\epsilon^{o} (-H \\epsilon^{b} + \\epsilon^{o})^{\\top}] = \\mathbb{E}[\\epsilon^{o} (-(\\epsilon^{b})^{\\top} H^{\\top} + (\\epsilon^{o})^{\\top})]\n    $$\n    Using the linearity of expectation:\n    $$\n    = -\\mathbb{E}[\\epsilon^{o} (\\epsilon^{b})^{\\top}] H^{\\top} + \\mathbb{E}[\\epsilon^{o} (\\epsilon^{o})^{\\top}]\n    $$\n    The problem states that the observation error $\\epsilon^{o}$ and background error $\\epsilon^{b}$ are independent. Since they are also zero-mean, their cross-covariance is zero: $\\mathbb{E}[\\epsilon^{o} (\\epsilon^{b})^{\\top}] = \\mathbb{E}[\\epsilon^{o}] \\mathbb{E}[(\\epsilon^{b})^{\\top}] = 0$. The covariance of the observation error is by definition $\\operatorname{cov}(\\epsilon^{o}) = \\mathbb{E}[\\epsilon^{o} (\\epsilon^{o})^{\\top}] = R$.\n    Therefore:\n    $$\n    \\mathbb{E}[d^{o} (d^{b})^{\\top}] = -0 \\cdot H^{\\top} + R = R\n    $$\n\n3.  **$\\mathbb{E}[d^{o} (d^{a})^{\\top}]$**:\n    We substitute the expressions for $d^{o}$ and $d^{a}$:\n    $$\n    \\mathbb{E}[d^{o} (d^{a})^{\\top}] = \\mathbb{E}[\\epsilon^{o} (-H \\epsilon^{a} + \\epsilon^{o})^{\\top}] = \\mathbb{E}[\\epsilon^{o} (-(\\epsilon^{a})^{\\top} H^{\\top} + (\\epsilon^{o})^{\\top})]\n    $$\n    $$\n    = -\\mathbb{E}[\\epsilon^{o} (\\epsilon^{a})^{\\top}] H^{\\top} + \\mathbb{E}[\\epsilon^{o} (\\epsilon^{o})^{\\top}] = R - \\mathbb{E}[\\epsilon^{o} (\\epsilon^{a})^{\\top}] H^{\\top}\n    $$\n    To evaluate this, we need the cross-covariance between the observation error $\\epsilon^{o}$ and the analysis error $\\epsilon^{a}$. The analysis error is given by:\n    $$\n    \\epsilon^{a} = x^{a} - x^{t} = x^{b} + K(y - Hx^{b}) - x^{t} = (x^{b}-x^{t}) + K(-H\\epsilon^{b} + \\epsilon^{o}) = (I - KH)\\epsilon^{b} + K\\epsilon^{o}\n    $$\n    Now, we compute $\\mathbb{E}[\\epsilon^{o} (\\epsilon^{a})^{\\top}]$:\n    $$\n    \\mathbb{E}[\\epsilon^{o} (\\epsilon^{a})^{\\top}] = \\mathbb{E}[\\epsilon^{o} ((I - KH)\\epsilon^{b} + K\\epsilon^{o})^{\\top}] = \\mathbb{E}[\\epsilon^{o} ((\\epsilon^{b})^{\\top}(I-KH)^{\\top} + (\\epsilon^{o})^{\\top}K^{\\top})]\n    $$\n    $$\n    = \\mathbb{E}[\\epsilon^{o}(\\epsilon^{b})^{\\top}](I-KH)^{\\top} + \\mathbb{E}[\\epsilon^{o}(\\epsilon^{o})^{\\top}]K^{\\top}\n    $$\n    The first term is zero due to independence, and the second term involves $R$.\n    $$\n    \\mathbb{E}[\\epsilon^{o} (\\epsilon^{a})^{\\top}] = R K^{\\top}\n    $$\n    Substituting this back into the expression for $\\mathbb{E}[d^{o} (d^{a})^{\\top}]$:\n    $$\n    \\mathbb{E}[d^{o} (d^{a})^{\\top}] = R - (R K^{\\top}) H^{\\top} = R - R K^{\\top} H^{\\top}\n    $$\n    To express this in terms of $P^{a}$, we use the standard identity for the optimal Kalman gain, $K = P^{a} H^{\\top} R^{-1}$. This assumes $R$ is invertible.\n    $$\n    K^{\\top} = (P^{a} H^{\\top} R^{-1})^{\\top} = (R^{-1})^{\\top} (H^{\\top})^{\\top} (P^{a})^{\\top} = R^{-1} H P^{a}\n    $$\n    since $R$ and $P^{a}$ are symmetric covariances. Substituting this into our expression:\n    $$\n    \\mathbb{E}[d^{o} (d^{a})^{\\top}] = R(I - (R^{-1} H P^{a}) H^{\\top}) = R - R R^{-1} H P^{a} H^{\\top} = R - H P^{a} H^{\\top}\n    $$\n    The derived expressions are: $\\mathbb{E}[d^{o}] = 0$, $\\mathbb{E}[d^{o} (d^{b})^{\\top}] = R$, and $\\mathbb{E}[d^{o} (d^{a})^{\\top}] = R - H P^{a} H^{\\top}$.\n\nb) The relationships derived in part (a) are theoretical identities that must hold if the covariance matrices used in the assimilation system, namely $P^{b}$ and $R$, accurately represent the true error statistics. In practical applications like the Ensemble Kalman Filter (EnKF), $P^b$ is an estimate, $P^b_{ens}$, which is known to have sampling errors. These errors are mitigated by applying localization, $P^{b}_{\\text{loc}} = C(L) \\circ P^{b}_{\\text{ens}}$, where $L$ is the localization length scale. The quality of the analysis depends critically on an appropriate choice for $L$.\n\nThe theoretical identities from part (a) provide a powerful diagnostic framework, often called Desroziers diagnostics, for tuning parameters like $L$. The core idea is to compare empirical statistics, computed from a long series of data assimilation cycles, with their theoretical counterparts, which are functions of the filter's internal parameters.\n\nFor example, consider the identity $\\mathbb{E}[d^{o} (d^{b})^{\\top}] = R$. Let $\\langle \\cdot \\rangle$ denote an average over many assimilation cycles. One can compute the empirical statistic $\\langle d^{o} (d^{b})^{\\top} \\rangle$ and compare it to the matrix $R$ used in the filter. A mismatch indicates that $R$ is misspecified. Note that computing $d^o$ requires knowledge of the true state $x^t$, so this tuning is typically done in a controlled setting like an Observing System Simulation Experiment (OSSE) where the \"truth\" is known.\n\nSimilarly, the identity $\\mathbb{E}[d^{o} (d^{a})^{\\top}] = R - H P^{a} H^{\\top}$ can be used. The Kalman filter equations produce a theoretical analysis error covariance, $P^{a}_{model}$, which depends on $P^{b}_{\\text{loc}}$, and thus on $L$. Specifically, $P^{a}_{model}(L) = (I - K(L)H)P^{b}_{\\text{loc}}(L)$, where the gain $K(L)$ also depends on $L$. The diagnostic objective is to find $L$ such that the empirically observed statistics are consistent with the filter's internal model.\nAn objective function $J(L)$ can be defined to quantify the mismatch, for instance, using a matrix norm:\n$$\nJ(L) = \\left\\| \\langle d^{o} (d^{a})^{\\top} \\rangle - \\left( R - H P^{a}_{model}(L) H^{\\top} \\right) \\right\\|^{2}\n$$\nThe localization length scale $L$ is then tuned by finding the value that minimizes this objective function, thereby ensuring that the statistics of the localized covariance model are as consistent as possible with the observed innovation and residual statistics. Other related identities, such as for the innovation covariance $\\mathbb{E}[d^b (d^b)^\\top] = HP^bH^\\top+R$, are also widely used for co-tuning inflation and localization.\n\nc) For this scalar, single-observation problem, we have $n=1, m=1, H=1$. The variables become scalars: $p^b, r, p^a, k$. The problem states $p^{b}_{\\text{ens}}=1.0$, $r=0.5$, and the inflated background variance is $p^b = \\lambda \\, p^{b}_{\\text{ens}} = \\lambda$. We are given the empirical estimate $s = \\langle d^o d^a \\rangle = 0.35$.\n\nFrom part (a), the relevant theoretical identity is $\\mathbb{E}[d^{o} d^{a}] = r - H p^{a} H^{\\top}$, which in this scalar case is:\n$$\n\\mathbb{E}[d^{o} d^{a}] = r - p^{a}\n$$\nThe Desroziers diagnostic method equates the observed statistic to its theoretical expectation. Thus, we set $s = r - p^a$.\n$$\n0.35 = 0.5 - p^{a} \\implies p^{a} = 0.5 - 0.35 = 0.15\n$$\nThis means the inflation factor $\\lambda$ must be chosen such that the theoretical analysis error variance produced by the Kalman filter equations is $0.15$.\n\nThe scalar Kalman filter equations for the analysis variance $p^a$ are:\nThe Kalman gain $k$ is:\n$$\nk = p^b (p^b + r)^{-1}\n$$\nThe analysis variance $p^a$ is:\n$$\np^a = (1-k)p^b = \\left(1 - \\frac{p^b}{p^b+r}\\right) p^b = \\left(\\frac{p^b+r-p^b}{p^b+r}\\right) p^b = \\frac{r p^b}{p^b+r}\n$$\nWe substitute $p^a = 0.15$, $r=0.5$, and $p^b = \\lambda$ into this equation to solve for $\\lambda$:\n$$\n0.15 = \\frac{0.5 \\lambda}{\\lambda + 0.5}\n$$\nWe now solve this algebraic equation for $\\lambda$:\n$$\n0.15 (\\lambda + 0.5) = 0.5 \\lambda\n$$\n$$\n0.15 \\lambda + 0.075 = 0.5 \\lambda\n$$\n$$\n0.075 = 0.5 \\lambda - 0.15 \\lambda\n$$\n$$\n0.075 = 0.35 \\lambda\n$$\n$$\n\\lambda = \\frac{0.075}{0.35} = \\frac{75}{350} = \\frac{15}{70} = \\frac{3}{14}\n$$\nConverting the fraction to a decimal gives:\n$$\n\\lambda \\approx 0.2142857...\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\lambda \\approx 0.2143\n$$",
            "answer": "$$\\boxed{0.2143}$$"
        }
    ]
}