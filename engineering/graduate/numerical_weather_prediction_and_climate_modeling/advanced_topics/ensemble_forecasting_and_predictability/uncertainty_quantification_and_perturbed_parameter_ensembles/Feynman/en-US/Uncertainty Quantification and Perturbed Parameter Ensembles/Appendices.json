{
    "hands_on_practices": [
        {
            "introduction": "The first step in quantifying uncertainty is often to understand how a model's output changes in response to small changes in its parameters, a process known as local sensitivity analysis. This practice uses the mathematical concept of the gradient, $\\nabla y$, to quantify these local sensitivities for a simplified diagnostic function. By completing this exercise, you will gain a concrete understanding of how to measure and compare the influence of different parameters at a specific operating point and appreciate the crucial role of parameter scaling in interpreting these sensitivities, a vital skill for practical model tuning .",
            "id": "4107171",
            "problem": "A convection parameterization in a numerical weather prediction (NWP) model is tuned using a simplified, locally nondimensional diagnostic cost function $y(\\theta_{1},\\theta_{2})$ that aggregates the normalized mean convective precipitation bias and plume energetics mismatch. Let the diagnostic be $y=\\theta_{1}^{2}+\\theta_{2}$, where $(\\theta_{1},\\theta_{2})$ are dimensionless control parameters obtained by nondimensionalizing physical parameters $(p_{1},p_{2})$ via known scales $(s_{1},s_{2})$ as $\\theta_{i}=p_{i}/s_{i}$ for $i\\in\\{1,2\\}$. Assume that $y$ is dimensionless and differentiable, and that small perturbations are interpreted through first-order Taylor linearization consistent with the local approach in uncertainty quantification and perturbed parameter ensembles.\n\nStarting from the fundamental definition of a partial derivative and the first-order Taylor linearization, compute the local sensitivity vector $\\nabla_{\\theta}y$ at $\\theta=(1,2)$, expressed as a row vector. Then, using the chain rule, discuss how the sensitivity with respect to the dimensional parameters $(p_{1},p_{2})$ depends on $(s_{1},s_{2})$, and interpret the units of these dimensional sensitivities as well as the implications of parameter scaling for tuning in a convection scheme. You must give the numerical values for the local sensitivity vector with respect to $(\\theta_{1},\\theta_{2})$ evaluated at $\\theta=(1,2)$; no rounding is required. Express your final sensitivity vector answer using a row matrix.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. It presents a clear, solvable problem in the context of parameter sensitivity analysis, a fundamental topic in numerical modeling and uncertainty quantification. All necessary definitions and data are provided, and there are no internal contradictions or ambiguities.\n\nThe problem requires the calculation of a local sensitivity vector and a subsequent discussion on a related transformation of variables. We begin with the first part.\n\nThe diagnostic cost function is given as $y(\\theta_{1}, \\theta_{2}) = \\theta_{1}^{2} + \\theta_{2}$, where $\\theta_{1}$ and $\\theta_{2}$ are dimensionless control parameters. The local sensitivity of the cost function $y$ with respect to these parameters is quantified by the gradient of $y$, denoted as $\\nabla_{\\theta}y$. The gradient is a vector whose components are the partial derivatives of the function with respect to each of its variables. For a scalar function of two variables, it is defined as:\n$$\n\\nabla_{\\theta}y = \\begin{pmatrix} \\frac{\\partial y}{\\partial \\theta_{1}} & \\frac{\\partial y}{\\partial \\theta_{2}} \\end{pmatrix}\n$$\nThe problem asks for this as a row vector, which is a common convention for the gradient of a scalar field.\n\nFirst, we compute the partial derivatives of $y$ with respect to $\\theta_{1}$ and $\\theta_{2}$:\nThe partial derivative with respect to $\\theta_{1}$ is:\n$$\n\\frac{\\partial y}{\\partial \\theta_{1}} = \\frac{\\partial}{\\partial \\theta_{1}} (\\theta_{1}^{2} + \\theta_{2}) = 2\\theta_{1}\n$$\nThe partial derivative with respect to $\\theta_{2}$ is:\n$$\n\\frac{\\partial y}{\\partial \\theta_{2}} = \\frac{\\partial}{\\partial \\theta_{2}} (\\theta_{1}^{2} + \\theta_{2}) = 1\n$$\nThus, the sensitivity vector as a function of $\\theta = (\\theta_{1}, \\theta_{2})$ is:\n$$\n\\nabla_{\\theta}y(\\theta_{1}, \\theta_{2}) = \\begin{pmatrix} 2\\theta_{1} & 1 \\end{pmatrix}\n$$\nThe problem requires this vector to be evaluated at the specific point $\\theta = (1, 2)$. We substitute $\\theta_{1} = 1$ and $\\theta_{2} = 2$ into the expression for the gradient:\n$$\n\\nabla_{\\theta}y(1, 2) = \\begin{pmatrix} 2(1) & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix}\n$$\nThis numerical vector represents the local sensitivity of the cost function to perturbations in the dimensionless parameters at the specified point. A change in $\\theta_{1}$ has twice the effect on $y$ as the same magnitude of change in $\\theta_{2}$ at this point. This vector is the first-order Taylor linearization of the change in $y$, i.e., for small perturbations $(\\delta\\theta_{1}, \\delta\\theta_{2})$, the change in $y$ is approximately $\\delta y \\approx 2\\delta\\theta_{1} + 1\\delta\\theta_{2}$.\n\nNext, we address the second part of the problem: the relationship between sensitivities with respect to the dimensionless parameters $(\\theta_{1}, \\theta_{2})$ and the dimensional physical parameters $(p_{1}, p_{2})$. The parameters are related by the scaling relations $\\theta_{i} = p_{i}/s_{i}$ for $i \\in \\{1, 2\\}$, where $(s_1, s_2)$ are known constant scaling factors.\n\nTo find the sensitivity of $y$ with respect to $(p_{1}, p_{2})$, we must apply the chain rule for multivariable functions. The function $y$ is a composition, $y(p_{1}, p_{2}) = y(\\theta_{1}(p_{1}), \\theta_{2}(p_{2}))$. The partial derivative of $y$ with respect to $p_{1}$ is:\n$$\n\\frac{\\partial y}{\\partial p_{1}} = \\frac{\\partial y}{\\partial \\theta_{1}} \\frac{\\partial \\theta_{1}}{\\partial p_{1}} + \\frac{\\partial y}{\\partial \\theta_{2}} \\frac{\\partial \\theta_{2}}{\\partial p_{1}}\n$$\nSimilarly, the partial derivative of $y$ with respect to $p_{2}$ is:\n$$\n\\frac{\\partial y}{\\partial p_{2}} = \\frac{\\partial y}{\\partial \\theta_{1}} \\frac{\\partial \\theta_{1}}{\\partial p_{2}} + \\frac{\\partial y}{\\partial \\theta_{2}} \\frac{\\partial \\theta_{2}}{\\partial p_{2}}\n$$\nWe compute the derivatives of the scaling relations:\n$$\n\\frac{\\partial \\theta_{1}}{\\partial p_{1}} = \\frac{\\partial}{\\partial p_{1}}\\left(\\frac{p_{1}}{s_{1}}\\right) = \\frac{1}{s_{1}}\n$$\n$$\n\\frac{\\partial \\theta_{2}}{\\partial p_{2}} = \\frac{\\partial}{\\partial p_{2}}\\left(\\frac{p_{2}}{s_{2}}\\right) = \\frac{1}{s_{2}}\n$$\nSince $\\theta_{1}$ is independent of $p_{2}$ and $\\theta_{2}$ is independent of $p_{1}$, we have:\n$$\n\\frac{\\partial \\theta_{1}}{\\partial p_{2}} = 0 \\quad \\text{and} \\quad \\frac{\\partial \\theta_{2}}{\\partial p_{1}} = 0\n$$\nSubstituting these into the chain rule expressions yields:\n$$\n\\frac{\\partial y}{\\partial p_{1}} = \\frac{\\partial y}{\\partial \\theta_{1}} \\left(\\frac{1}{s_{1}}\\right) + \\frac{\\partial y}{\\partial \\theta_{2}} (0) = \\frac{1}{s_{1}} \\frac{\\partial y}{\\partial \\theta_{1}}\n$$\n$$\n\\frac{\\partial y}{\\partial p_{2}} = \\frac{\\partial y}{\\partial \\theta_{1}} (0) + \\frac{\\partial y}{\\partial \\theta_{2}} \\left(\\frac{1}{s_{2}}\\right) = \\frac{1}{s_{2}} \\frac{\\partial y}{\\partial \\theta_{2}}\n$$\nThe sensitivity vector with respect to the dimensional parameters, $\\nabla_{p}y$, is therefore:\n$$\n\\nabla_{p}y = \\begin{pmatrix} \\frac{1}{s_{1}} \\frac{\\partial y}{\\partial \\theta_{1}} & \\frac{1}{s_{2}} \\frac{\\partial y}{\\partial \\theta_{2}} \\end{pmatrix}\n$$\n\nWe now interpret the units and implications of this result.\nLet the physical units of a quantity $q$ be denoted by $[q]$. The problem states that $y$, $\\theta_{1}$, and $\\theta_{2}$ are dimensionless. This implies $[y] = 1$ and $[\\theta_i] = 1$. The sensitivities $\\frac{\\partial y}{\\partial \\theta_i}$ are therefore also dimensionless, as $[\\frac{\\partial y}{\\partial \\theta_i}] = \\frac{[y]}{[\\theta_i]} = \\frac{1}{1} = 1$.\nThe physical parameters $p_i$ have dimensions, $[p_i]$. For $\\theta_i = p_i/s_i$ to be dimensionless, the scaling factor $s_i$ must have the same units as the corresponding physical parameter, i.e., $[s_i] = [p_i]$.\nThe units of the dimensional sensitivities $\\frac{\\partial y}{\\partial p_i}$ are then:\n$$\n\\left[\\frac{\\partial y}{\\partial p_i}\\right] = \\frac{[y]}{[p_i]} = \\frac{1}{[p_i]} = [p_i]^{-1}\n$$\nFor instance, if $p_{1}$ represents a convective entrainment rate with units of inverse length ($m^{-1}$), then the sensitivity $\\frac{\\partial y}{\\partial p_1}$ would have units of length ($m$). This signifies the change in the dimensionless cost function per unit change in the entrainment rate.\n\nThe implications for parameter tuning in a convection scheme are significant. The relationship $\\frac{\\partial y}{\\partial p_i} = \\frac{1}{s_i} \\frac{\\partial y}{\\partial \\theta_i}$ shows that the sensitivity of the model's cost function to a dimensional physical parameter $p_i$ is inversely proportional to its characteristic scaling factor $s_i$.\nIf a scaling factor $s_i$ is chosen to be large, it implies that a large absolute change in the physical parameter $p_i$ is required to produce a unit change in the dimensionless parameter $\\theta_i$. Consequently, a large $s_i$ makes the cost function $y$ less sensitive to $p_i$. This can make the parameter difficult to \"tune\" or constrain, as even substantial adjustments to its value may result in only minor changes to the model's performance as measured by $y$.\nConversely, if $s_i$ is small, the cost function becomes highly sensitive to $p_i$. Small adjustments to $p_i$ are magnified, leading to large changes in $\\theta_i$ and hence $y$. Such parameters must be handled with great care during tuning, as minor errors in their specification can lead to large degradations in model performance.\nThe choice of scaling factors $(s_1, s_2)$ is a critical step in parameter estimation and uncertainty quantification. These scales are typically chosen based on physical reasoning to represent a characteristic magnitude or range of variation for the parameters. Ideally, they are chosen such that the dimensionless sensitivities $\\frac{\\partial y}{\\partial \\theta_i}$ are of a similar order of magnitude (e.g., of order one), which can lead to a better-conditioned optimization problem. This process of nondimensionalization and scaling is fundamental to understanding and comparing the relative importance of different physical processes within a complex model like a convection parameterization.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond local sensitivity, a full uncertainty quantification requires a probabilistic approach to represent our state of knowledge. This exercise demonstrates the core of Bayesian updating, where a prior probability distribution, $p(\\theta)$, is combined with a likelihood function, $p(y|\\theta)$, to produce a posterior distribution, $p(\\theta|y)$, that reflects our updated belief in light of new data. By working through a classic example with Gaussian distributions, you will see precisely how observations constrain parameter uncertainty and calculate the concept of \"shrinkage,\" a fundamental feature of Bayesian estimation where the result is a principled compromise between prior knowledge and observed evidence .",
            "id": "4107138",
            "problem": "Consider a simplified calibration of a scalar uncertain parameter $\\theta$ in Numerical Weather Prediction (NWP) and climate modeling using a Perturbed Parameter Ensemble (PPE). Suppose $\\theta$ represents a subgrid process coefficient whose prior uncertainty across the ensemble is described by a Gaussian prior $\\theta \\sim \\mathcal{N}(0, 1)$. You obtain a single diagnostic observation $y$ from a model–data comparison, where the likelihood is $y \\mid \\theta \\sim \\mathcal{N}(\\theta, 1)$, reflecting additive observational and representation error with variance $1$. You observe $y = 2$.\n\nStarting from Bayes’ theorem and the definition of the Gaussian density, derive the posterior distribution $p(\\theta \\mid y)$ in closed form by explicitly combining the prior and likelihood and completing the square in the exponent. Then, identify the posterior mean and posterior variance, and explain, in terms of precision-weighted averaging, how the posterior mean shrinks relative to the observation $y$ and why the posterior variance is reduced relative to the prior variance in this PPE context.\n\nProvide your final numerical answer for the posterior mean and posterior variance as a row matrix with two entries, corresponding to the posterior mean and posterior variance in that order. No rounding is required.",
            "solution": "The problem is to derive the posterior distribution of an uncertain parameter $\\theta$ given a prior distribution and a single observation $y$ from a likelihood model. This is a standard application of Bayes' theorem in the context of conjugate priors.\n\nThe givens are:\n- Prior distribution of $\\theta$: $\\theta \\sim \\mathcal{N}(\\mu_p, \\sigma_p^2)$, with prior mean $\\mu_p = 0$ and prior variance $\\sigma_p^2 = 1$.\n- Likelihood model for observation $y$: $y \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma_l^2)$, with likelihood variance $\\sigma_l^2 = 1$.\n- A specific observation: $y = 2$.\n\nAccording to Bayes' theorem, the posterior probability distribution $p(\\theta \\mid y)$ is proportional to the product of the likelihood $p(y \\mid \\theta)$ and the prior $p(\\theta)$:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\n$$\nThe probability density function (PDF) for the Gaussian prior is:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_p^2}} \\exp\\left( -\\frac{(\\theta - \\mu_p)^2}{2\\sigma_p^2} \\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{\\theta^2}{2} \\right)\n$$\nThe PDF for the Gaussian likelihood is:\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_l^2}} \\exp\\left( -\\frac{(y - \\theta)^2}{2\\sigma_l^2} \\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{(y - \\theta)^2}{2} \\right)\n$$\nMultiplying the prior and the likelihood, we can ignore the normalization constants $(2\\pi)^{-1/2}$ as we are working with proportionality:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{\\theta^2}{2} \\right) \\exp\\left( -\\frac{(y - \\theta)^2}{2} \\right)\n$$\nWe can combine the exponents:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\theta^2 + (y - \\theta)^2 \\right] \\right)\n$$\nTo identify the form of the posterior distribution, we need to analyze the expression in the exponent. This involves expanding the terms and completing the square with respect to $\\theta$. The term inside the square brackets is:\n$$\n\\theta^2 + (y - \\theta)^2 = \\theta^2 + (y^2 - 2y\\theta + \\theta^2) = 2\\theta^2 - 2y\\theta + y^2\n$$\nNow, we complete the square for the terms involving $\\theta$:\n$$\n2\\theta^2 - 2y\\theta + y^2 = 2\\left(\\theta^2 - y\\theta\\right) + y^2\n$$\nTo complete the square for $\\theta^2 - y\\theta$, we add and subtract $(y/2)^2$:\n$$\n2\\left( \\left[\\theta^2 - y\\theta + \\left(\\frac{y}{2}\\right)^2\\right] - \\left(\\frac{y}{2}\\right)^2 \\right) + y^2 = 2\\left( \\left(\\theta - \\frac{y}{2}\\right)^2 - \\frac{y^2}{4} \\right) + y^2\n$$\n$$\n= 2\\left(\\theta - \\frac{y}{2}\\right)^2 - 2\\frac{y^2}{4} + y^2 = 2\\left(\\theta - \\frac{y}{2}\\right)^2 - \\frac{y^2}{2} + y^2 = 2\\left(\\theta - \\frac{y}{2}\\right)^2 + \\frac{y^2}{2}\n$$\nSubstituting this back into the expression for the posterior PDF:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ 2\\left(\\theta - \\frac{y}{2}\\right)^2 + \\frac{y^2}{2} \\right] \\right)\n$$\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\left(\\theta - \\frac{y}{2}\\right)^2 - \\frac{y^2}{4} \\right) = \\exp\\left( -\\left(\\theta - \\frac{y}{2}\\right)^2 \\right) \\exp\\left( -\\frac{y^2}{4} \\right)\n$$\nSince the term $\\exp(-y^2/4)$ does not depend on $\\theta$, it is a constant with respect to $\\theta$ and can be absorbed into the proportionality constant. Thus, we have:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\left(\\theta - \\frac{y}{2}\\right)^2 \\right)\n$$\nTo match this to the standard form of a Gaussian PDF, $p(x) \\propto \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)$, we rewrite the exponent:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{\\left(\\theta - \\frac{y}{2}\\right)^2}{2\\left(\\frac{1}{2}\\right)} \\right)\n$$\nThis is the kernel of a Gaussian distribution for $\\theta$. By inspection, we can identify the posterior mean and variance. The posterior distribution is $\\theta \\mid y \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$, where:\n- Posterior mean: $\\mu_{\\text{post}} = \\frac{y}{2}$\n- Posterior variance: $\\sigma_{\\text{post}}^2 = \\frac{1}{2}$\n\nFor the given observation $y=2$, the posterior mean is:\n$$\n\\mu_{\\text{post}} = \\frac{2}{2} = 1\n$$\nThe posterior variance is $\\sigma_{\\text{post}}^2 = \\frac{1}{2}$.\n\nExplanation of the results in terms of precision-weighted averaging:\nPrecision is defined as the inverse of the variance, $\\tau = 1/\\sigma^2$. It represents the certainty of an estimate.\n- The prior precision is $\\tau_p = 1/\\sigma_p^2 = 1/1 = 1$.\n- The data (likelihood) precision is $\\tau_l = 1/\\sigma_l^2 = 1/1 = 1$.\n\nThe posterior mean for a Gaussian-Gaussian model is a precision-weighted average of the prior mean and the observation:\n$$\n\\mu_{\\text{post}} = \\frac{\\tau_p \\mu_p + \\tau_l y}{\\tau_p + \\tau_l} = \\frac{(1)(0) + (1)(y)}{1 + 1} = \\frac{y}{2}\n$$\nThis shows how the posterior mean is a compromise between the prior belief ($\\mu_p=0$) and the information from the data ($y$). With $y=2$, the posterior mean is $1$. This value is \"shrunk\" from the observation $y=2$ towards the prior mean $\\mu_p=0$. Since the prior and the data have equal precision, the posterior mean lies exactly halfway between them. This shrinkage is a characteristic feature of Bayesian updating, regularizing the estimate by pulling it towards the prior belief.\n\nThe posterior precision is the sum of the prior and data precisions:\n$$\n\\tau_{\\text{post}} = \\tau_p + \\tau_l = 1 + 1 = 2\n$$\nThe posterior variance is the inverse of the posterior precision:\n$$\n\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{\\text{post}}} = \\frac{1}{2}\n$$\nThe posterior variance ($\\sigma_{\\text{post}}^2 = 1/2$) is smaller than both the prior variance ($\\sigma_p^2 = 1$) and the likelihood variance ($\\sigma_l^2 = 1$). This reflects the fact that by combining two sources of information (prior and data), we have reduced our uncertainty about the parameter $\\theta$. Our posterior knowledge is more precise than either our initial belief or the information from the single observation alone. In the context of a Perturbed Parameter Ensemble (PPE), this process demonstrates how observations constrain the initial ensemble of parameters, leading to a smaller, more refined ensemble and thus a reduction in parametric uncertainty.\n\nThe final numerical answers for the posterior mean and posterior variance are $1$ and $\\frac{1}{2}$, respectively.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 & \\frac{1}{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "When exploring the impact of multiple uncertain parameters, it is tempting to use simplified methods like perturbing one parameter at a time (OAT), but this can be misleading in complex systems where parameters interact. This problem confronts this issue directly by comparing the variance estimated from an OAT approach with the true variance for a system containing a parameter interaction term, $\\beta\\theta_1\\theta_2$. By analytically deriving the bias of the OAT estimator, you will uncover the two key sources of its error—neglected parameter correlations and non-linear interactions—providing a rigorous justification for using more sophisticated methods like full perturbed parameter ensembles (PPEs) .",
            "id": "4107110",
            "problem": "A common uncertainty quantification approach in Numerical Weather Prediction (NWP) and climate modeling is the Perturbed Parameter Ensemble (PPE), in which model parameters are treated as random variables. Another approach is the One-At-A-Time (OAT) deterministic perturbation strategy, in which each parameter is perturbed individually around a baseline while others are held fixed, and the resulting local sensitivities are used to approximate output variance. Consider a simplified PPE for a climatological diagnostic $Y$ whose dependence on two uncertain parameters $(\\theta_1,\\theta_2)$ is approximated by a bilinear response\n$$\nY \\;=\\; f(\\theta_1,\\theta_2) \\;=\\; \\alpha_1 \\,\\theta_1 \\;+\\; \\alpha_2 \\,\\theta_2 \\;+\\; \\beta \\,\\theta_1 \\theta_2,\n$$\nwhere $\\alpha_1$, $\\alpha_2$, and $\\beta$ are constants determined by the model’s physics and numerics. Assume $(\\theta_1,\\theta_2)$ is jointly Gaussian with means $(\\mu_1,\\mu_2)$, variances $(\\sigma_1^2,\\sigma_2^2)$, and correlation $\\rho$.\n\nIn the OAT deterministic perturbation design, the local linear sensitivities are evaluated at the baseline $(\\mu_1,\\mu_2)$, yielding\n$$\nc_1 \\;=\\; \\left.\\frac{\\partial f}{\\partial \\theta_1}\\right|_{(\\mu_1,\\mu_2)} \\;=\\; \\alpha_1 + \\beta \\mu_2,\n\\qquad\nc_2 \\;=\\; \\left.\\frac{\\partial f}{\\partial \\theta_2}\\right|_{(\\mu_1,\\mu_2)} \\;=\\; \\alpha_2 + \\beta \\mu_1.\n$$\nA standard OAT-based variance estimator then omits both covariance and curvature terms and uses\n$$\n\\widehat{\\mathrm{Var}}_{\\mathrm{OAT}} \\;=\\; c_1^2 \\,\\sigma_1^2 \\;+\\; c_2^2 \\,\\sigma_2^2.\n$$\n\nUsing first principles for random-variable transformations and properties of the multivariate normal distribution, derive the exact output variance $\\mathrm{Var}(Y)$ under the joint random perturbation ensemble and then compute the bias\n$$\n\\mathrm{Bias} \\;=\\; \\mathrm{Var}(Y) \\;-\\; \\widehat{\\mathrm{Var}}_{\\mathrm{OAT}}.\n$$\nExpress your final answer as a single closed-form analytic expression in terms of $\\alpha_1$, $\\alpha_2$, $\\beta$, $\\mu_1$, $\\mu_2$, $\\sigma_1$, $\\sigma_2$, and $\\rho$. No rounding is required. No physical units need to be reported in the final expression.",
            "solution": "The problem asks for the bias of a One-At-a-Time (OAT) variance estimator compared to the exact variance of a model output $Y$. The model is a bilinear function of two jointly Gaussian random parameters, $\\theta_1$ and $\\theta_2$. The bias is defined as $\\mathrm{Bias} = \\mathrm{Var}(Y) - \\widehat{\\mathrm{Var}}_{\\mathrm{OAT}}$.\n\nFirst, we derive the exact variance of the output, $\\mathrm{Var}(Y)$. The output $Y$ is given by\n$$\nY = f(\\theta_1, \\theta_2) = \\alpha_1 \\theta_1 + \\alpha_2 \\theta_2 + \\beta \\theta_1 \\theta_2\n$$\nThe parameters $(\\theta_1, \\theta_2)$ follow a bivariate normal distribution with means $E[\\theta_1] = \\mu_1$, $E[\\theta_2]=\\mu_2$, variances $\\mathrm{Var}(\\theta_1) = \\sigma_1^2$, $\\mathrm{Var}(\\theta_2) = \\sigma_2^2$, and correlation $\\mathrm{Corr}(\\theta_1, \\theta_2) = \\rho$. The covariance is $\\mathrm{Cov}(\\theta_1, \\theta_2) = \\rho\\sigma_1\\sigma_2$.\n\nThe variance of a random variable $Y$ is given by $\\mathrm{Var}(Y) = E[(Y - E[Y])^2]$. We begin by calculating the expected value of $Y$, $E[Y]$. Using the linearity of expectation:\n$$\nE[Y] = E[\\alpha_1 \\theta_1 + \\alpha_2 \\theta_2 + \\beta \\theta_1 \\theta_2] = \\alpha_1 E[\\theta_1] + \\alpha_2 E[\\theta_2] + \\beta E[\\theta_1 \\theta_2]\n$$\nThe term $E[\\theta_1 \\theta_2]$ is related to the covariance: $\\mathrm{Cov}(\\theta_1, \\theta_2) = E[\\theta_1 \\theta_2] - E[\\theta_1]E[\\theta_2]$.\nThus, $E[\\theta_1 \\theta_2] = \\mathrm{Cov}(\\theta_1, \\theta_2) + E[\\theta_1]E[\\theta_2] = \\rho\\sigma_1\\sigma_2 + \\mu_1\\mu_2$.\nSubstituting this into the expression for $E[Y]$ yields:\n$$\nE[Y] = \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\beta(\\mu_1 \\mu_2 + \\rho\\sigma_1\\sigma_2)\n$$\nNext, we express the centered variable $Y - E[Y]$. Let us define centered random variables $\\delta\\theta_1 = \\theta_1 - \\mu_1$ and $\\delta\\theta_2 = \\theta_2 - \\mu_2$. These have zero mean, $E[\\delta\\theta_1] = E[\\delta\\theta_2] = 0$. Their second-order moments are: $E[\\delta\\theta_1^2] = \\sigma_1^2$, $E[\\delta\\theta_2^2] = \\sigma_2^2$, and $E[\\delta\\theta_1 \\delta\\theta_2] = \\mathrm{Cov}(\\theta_1, \\theta_2) = \\rho\\sigma_1\\sigma_2$.\n\nWe rewrite $Y$ in terms of these centered variables:\n$$\nY = \\alpha_1(\\mu_1 + \\delta\\theta_1) + \\alpha_2(\\mu_2 + \\delta\\theta_2) + \\beta(\\mu_1 + \\delta\\theta_1)(\\mu_2 + \\delta\\theta_2)\n$$\n$$\nY = \\alpha_1\\mu_1 + \\alpha_1\\delta\\theta_1 + \\alpha_2\\mu_2 + \\alpha_2\\delta\\theta_2 + \\beta(\\mu_1\\mu_2 + \\mu_1\\delta\\theta_2 + \\mu_2\\delta\\theta_1 + \\delta\\theta_1\\delta\\theta_2)\n$$\nSubtracting $E[Y]$:\n$$\nY - E[Y] = (\\alpha_1\\mu_1 + \\alpha_2\\mu_2 + \\beta\\mu_1\\mu_2) + (\\alpha_1 + \\beta\\mu_2)\\delta\\theta_1 + (\\alpha_2 + \\beta\\mu_1)\\delta\\theta_2 + \\beta\\delta\\theta_1\\delta\\theta_2 - E[Y]\n$$\nSubstituting the expression for $E[Y]$ gives:\n$$\nY - E[Y] = (\\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\beta\\mu_1 \\mu_2) + (\\alpha_1 + \\beta\\mu_2)\\delta\\theta_1 + (\\alpha_2 + \\beta\\mu_1)\\delta\\theta_2 + \\beta\\delta\\theta_1\\delta\\theta_2 - (\\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\beta\\mu_1 \\mu_2 + \\beta\\rho\\sigma_1\\sigma_2)\n$$\n$$\nY - E[Y] = (\\alpha_1 + \\beta\\mu_2)\\delta\\theta_1 + (\\alpha_2 + \\beta\\mu_1)\\delta\\theta_2 + \\beta(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2)\n$$\nUsing the given definitions for the OAT sensitivities, $c_1 = \\alpha_1 + \\beta\\mu_2$ and $c_2 = \\alpha_2 + \\beta\\mu_1$, we have:\n$$\nY - E[Y] = c_1\\delta\\theta_1 + c_2\\delta\\theta_2 + \\beta(\\delta\\theta_1\\delta\\theta_2 - E[\\delta\\theta_1\\delta\\theta_2])\n$$\nNow we compute the variance, $\\mathrm{Var}(Y) = E[(Y - E[Y])^2]$:\n$$\n\\mathrm{Var}(Y) = E \\left[ \\left( c_1\\delta\\theta_1 + c_2\\delta\\theta_2 + \\beta(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2) \\right)^2 \\right]\n$$\nExpanding the square:\n$$\n\\mathrm{Var}(Y) = E \\left[ c_1^2\\delta\\theta_1^2 + c_2^2\\delta\\theta_2^2 + \\beta^2(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2)^2 + 2c_1c_2\\delta\\theta_1\\delta\\theta_2 + 2c_1\\beta\\delta\\theta_1(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2) + 2c_2\\beta\\delta\\theta_2(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2) \\right]\n$$\nBy linearity of expectation, we can evaluate each term:\n1.  $E[c_1^2\\delta\\theta_1^2] = c_1^2 E[\\delta\\theta_1^2] = c_1^2\\sigma_1^2$.\n2.  $E[c_2^2\\delta\\theta_2^2] = c_2^2 E[\\delta\\theta_2^2] = c_2^2\\sigma_2^2$.\n3.  $E[2c_1c_2\\delta\\theta_1\\delta\\theta_2] = 2c_1c_2 E[\\delta\\theta_1\\delta\\theta_2] = 2c_1c_2\\rho\\sigma_1\\sigma_2$.\n4.  $E[2c_1\\beta\\delta\\theta_1(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2)] = 2c_1\\beta(E[\\delta\\theta_1^2\\delta\\theta_2] - \\rho\\sigma_1\\sigma_2 E[\\delta\\theta_1])$. Since $\\delta\\theta_1$ and $\\delta\\theta_2$ are jointly Gaussian and centered, any odd moment is zero. Thus, $E[\\delta\\theta_1]=0$ and $E[\\delta\\theta_1^2\\delta\\theta_2]=0$. This term is zero.\n5.  Similarly, $E[2c_2\\beta\\delta\\theta_2(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2)] = 2c_2\\beta(E[\\delta\\theta_1\\delta\\theta_2^2] - \\rho\\sigma_1\\sigma_2 E[\\delta\\theta_2]) = 0$.\n6.  The final term is $E[\\beta^2(\\delta\\theta_1\\delta\\theta_2 - \\rho\\sigma_1\\sigma_2)^2] = \\beta^2 E[(\\delta\\theta_1\\delta\\theta_2 - E[\\delta\\theta_1\\delta\\theta_2])^2] = \\beta^2\\mathrm{Var}(\\delta\\theta_1\\delta\\theta_2)$. For centered bivariate normal variables, the variance of their product is $\\mathrm{Var}(\\delta\\theta_1\\delta\\theta_2) = (1+\\rho^2)\\sigma_1^2\\sigma_2^2$. This result follows from Isserlis' theorem, which gives $E[\\delta\\theta_1^2\\delta\\theta_2^2] = E[\\delta\\theta_1^2]E[\\delta\\theta_2^2] + 2(E[\\delta\\theta_1\\delta\\theta_2])^2 = \\sigma_1^2\\sigma_2^2 + 2(\\rho\\sigma_1\\sigma_2)^2 = (1+2\\rho^2)\\sigma_1^2\\sigma_2^2$. Then $\\mathrm{Var}(\\delta\\theta_1\\delta\\theta_2) = E[\\delta\\theta_1^2\\delta\\theta_2^2] - (E[\\delta\\theta_1\\delta\\theta_2])^2 = (1+2\\rho^2)\\sigma_1^2\\sigma_2^2 - (\\rho\\sigma_1\\sigma_2)^2 = (1+\\rho^2)\\sigma_1^2\\sigma_2^2$. Thus, this term equals $\\beta^2(1+\\rho^2)\\sigma_1^2\\sigma_2^2$.\n\nSumming the non-zero terms, the exact variance is:\n$$\n\\mathrm{Var}(Y) = c_1^2\\sigma_1^2 + c_2^2\\sigma_2^2 + 2c_1c_2\\rho\\sigma_1\\sigma_2 + \\beta^2(1+\\rho^2)\\sigma_1^2\\sigma_2^2\n$$\nThe problem specifies the OAT-based variance estimator as:\n$$\n\\widehat{\\mathrm{Var}}_{\\mathrm{OAT}} = c_1^2\\sigma_1^2 + c_2^2\\sigma_2^2\n$$\nThe bias is the difference between the exact variance and the estimated variance:\n$$\n\\mathrm{Bias} = \\mathrm{Var}(Y) - \\widehat{\\mathrm{Var}}_{\\mathrm{OAT}}\n$$\n$$\n\\mathrm{Bias} = (c_1^2\\sigma_1^2 + c_2^2\\sigma_2^2 + 2c_1c_2\\rho\\sigma_1\\sigma_2 + \\beta^2(1+\\rho^2)\\sigma_1^2\\sigma_2^2) - (c_1^2\\sigma_1^2 + c_2^2\\sigma_2^2)\n$$\n$$\n\\mathrm{Bias} = 2c_1c_2\\rho\\sigma_1\\sigma_2 + \\beta^2(1+\\rho^2)\\sigma_1^2\\sigma_2^2\n$$\nFinally, we substitute the expressions for $c_1$ and $c_2$ back to express the bias in terms of the fundamental model parameters:\n$$\nc_1 = \\alpha_1 + \\beta\\mu_2\n$$\n$$\nc_2 = \\alpha_2 + \\beta\\mu_1\n$$\n$$\n\\mathrm{Bias} = 2(\\alpha_1 + \\beta\\mu_2)(\\alpha_2 + \\beta\\mu_1)\\rho\\sigma_1\\sigma_2 + \\beta^2(1+\\rho^2)\\sigma_1^2\\sigma_2^2\n$$\nThis expression represents the two sources of error in the OAT estimator: the first term arises from neglecting the covariance between the parameters' effects (which is non-zero when $\\rho \\neq 0$), and the second term arises from neglecting the model's nonlinearity or curvature (which is non-zero when $\\beta \\neq 0$).",
            "answer": "$$\n\\boxed{2(\\alpha_1 + \\beta\\mu_2)(\\alpha_2 + \\beta\\mu_1)\\rho\\sigma_1\\sigma_2 + \\beta^2(1+\\rho^2)\\sigma_1^2\\sigma_2^2}\n$$"
        }
    ]
}