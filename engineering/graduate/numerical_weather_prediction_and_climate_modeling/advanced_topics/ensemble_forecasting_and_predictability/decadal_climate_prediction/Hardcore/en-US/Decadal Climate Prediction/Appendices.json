{
    "hands_on_practices": [
        {
            "introduction": "This practice explores the fundamental source of decadal predictability: the \"memory\" of the climate system. We will use a simple first-order autoregressive model, a common representation for slowly varying components like upper-ocean heat content, to analytically derive the skill of a persistence forecast. This exercise  demonstrates how the internal persistence of a system, quantified by its autocorrelation, directly translates into forecast skill that decays over time.",
            "id": "4030524",
            "problem": "In decadal climate prediction, slowly varying components such as basin-mean upper-ocean heat content anomalies are often modeled as a first-order autoregressive process. Consider a zero-mean Autoregressive of order 1 (AR(1)) process for the ocean heat content anomaly sequence $\\{X_t\\}$ at annual resolution given by $X_{t+1} = \\phi X_t + \\epsilon_t$, where $\\{ \\epsilon_t \\}$ is a zero-mean, serially uncorrelated (white) noise process with variance $\\sigma_{\\epsilon}^{2}$, independent of $\\{X_t\\}$, and $|\\phi| < 1$ so that the process is strictly stationary with finite variance. A persistence forecast issued at time $t$ for lead $L$ uses $F_{t+L|t} = X_t$ as the forecast of $X_{t+L}$. The Anomaly Correlation Coefficient (ACC) at lead $L$ is defined as the Pearson correlation between forecast anomalies and verifying anomalies taken over many independent forecast instances under the stationary distribution, that is, $\\mathrm{ACC}(L) = \\mathrm{Corr}(F_{t+L|t}, X_{t+L})$.\n\nStarting only from the AR(1) definition, the white-noise assumptions, and the definition of correlation, derive a closed-form expression for the expected $\\mathrm{ACC}(L)$ as a function of $\\phi$ and $L$. Provide the final expression in closed form. Then, interpret the expression in terms of the memory of the ocean heat content anomaly in this model. The final answer must be a single analytic expression with no units. No numerical rounding is required.",
            "solution": "We begin from the model definition for the ocean heat content anomaly, which is a zero-mean Autoregressive of order 1 (AR(1)) process:\n$$\nX_{t+1} = \\phi X_t + \\epsilon_t,\n$$\nwith $|\\phi| < 1$, and where $\\{\\epsilon_t\\}$ is a sequence of zero-mean, serially uncorrelated (white) noise innovations with variance $\\sigma_{\\epsilon}^{2}$ and independent of $\\{X_t\\}$. Under these conditions, the process is strictly stationary with finite variance.\n\nA persistence forecast at time $t$ for lead $L$ is defined by\n$$\nF_{t+L|t} = X_t.\n$$\nThe Anomaly Correlation Coefficient (ACC) at lead $L$ is the Pearson correlation between $F_{t+L|t}$ and the verifying anomaly $X_{t+L}$, averaged over many forecast instances drawn from the stationary distribution:\n$$\n\\mathrm{ACC}(L) = \\mathrm{Corr}(F_{t+L|t}, X_{t+L}) = \\frac{\\mathrm{Cov}(F_{t+L|t}, X_{t+L})}{\\sqrt{\\mathrm{Var}(F_{t+L|t}) \\, \\mathrm{Var}(X_{t+L})}}.\n$$\nUnder stationarity, $\\mathrm{Var}(X_{t+L}) = \\mathrm{Var}(X_t)$. Also, $\\mathrm{Var}(F_{t+L|t}) = \\mathrm{Var}(X_t)$. Therefore,\n$$\n\\mathrm{ACC}(L) = \\frac{\\mathrm{Cov}(X_t, X_{t+L})}{\\mathrm{Var}(X_t)}.\n$$\n\nIt remains to compute $\\mathrm{Cov}(X_t, X_{t+L})$. Iterating the AR(1) model forward $L$ steps gives the well-known linear decomposition (obtainable by repeated substitution):\n$$\nX_{t+L} = \\phi^{L} X_t + \\sum_{k=0}^{L-1} \\phi^{k} \\epsilon_{t+L-1-k}.\n$$\nThe innovation sequence $\\{\\epsilon_s\\}$ is independent of $X_t$ and serially uncorrelated. Hence,\n$$\n\\mathrm{Cov}(X_t, X_{t+L}) = \\mathrm{Cov}\\!\\left(X_t, \\phi^{L} X_t + \\sum_{k=0}^{L-1} \\phi^{k} \\epsilon_{t+L-1-k}\\right) = \\phi^{L} \\mathrm{Cov}(X_t, X_t) + \\sum_{k=0}^{L-1} \\phi^{k} \\mathrm{Cov}(X_t, \\epsilon_{t+L-1-k}).\n$$\nFor each $k$, $\\epsilon_{t+L-1-k}$ is independent of $X_t$, so $\\mathrm{Cov}(X_t, \\epsilon_{t+L-1-k}) = 0$. Therefore,\n$$\n\\mathrm{Cov}(X_t, X_{t+L}) = \\phi^{L} \\mathrm{Var}(X_t).\n$$\nSubstituting into the ACC expression gives\n$$\n\\mathrm{ACC}(L) = \\frac{\\phi^{L} \\mathrm{Var}(X_t)}{\\mathrm{Var}(X_t)} = \\phi^{L}.\n$$\n\nThis is the closed-form expression for the expected Anomaly Correlation Coefficient at lead $L$ for a persistence forecast under the AR(1) model.\n\nInterpretation in terms of memory: the ACC equals $\\phi^{L}$, which is the autocorrelation function at lag $L$ for an AR(1). Thus, forecast skill under persistence decays geometrically with lead time, with magnitude controlled by $|\\phi|$. The effective memory timescale, defined as the $e$-folding decorrelation time in units of the time step, is\n$$\n\\tau = -\\frac{1}{\\ln |\\phi|}.\n$$\nWhen $0 < \\phi < 1$, the ACC is positive and decays monotonically with $L$. If $\\phi < 0$, the sign of the ACC alternates with $L$ while its magnitude decays as $|\\phi|^{L}$. Larger $|\\phi|$ implies longer memory and slower loss of persistence-based skill with lead.",
            "answer": "$$\\boxed{\\phi^{L}}$$"
        },
        {
            "introduction": "After establishing a theoretical basis for predictability, the next critical step is to rigorously measure it. This exercise  focuses on the Mean Square Skill Score ($MSSS$), a standard metric for quantifying forecast performance relative to a baseline reference. By deriving the $MSSS$ and applying it to a hypothetical dataset, you will gain practical experience in skill assessment and, more importantly, critically examine how the choice of the reference \"climatology\" determines whether you are measuring skill in predicting climate anomalies or skill in predicting deviations from a long-term trend.",
            "id": "4030497",
            "problem": "Consider a decadal climate prediction experiment for a regional mean surface temperature anomaly time series. Let there be $n$ decadal verification times indexed by $i = 1, \\dots, n$. Denote the forecast anomalies by $f_i$, the observed anomalies by $o_i$, and a reference climatological forecast by $c_i$. In this experiment, a skill score is defined as the relative improvement in mean-square error of the forecast compared to the reference forecast.\n\nStarting from the fundamental definition of mean-square error as $MSE[X] = \\mathbb{E}\\left[(X - O)^{2}\\right]$ and using its unbiased sample estimator for finite $n$ as $\\frac{1}{n}\\sum_{i=1}^{n}(X_i - o_i)^{2}$, derive the analytic expression for the Mean Square Skill Score in terms of sums of squared errors of the forecast relative to the climatological reference.\n\nThen, apply your derived expression to the following eight-decadal dataset ($n = 8$). The observed anomalies are\n$o_1 = 0.12$, $o_2 = -0.08$, $o_3 = 0.05$, $o_4 = 0.10$, $o_5 = -0.02$, $o_6 = 0.03$, $o_7 = 0.07$, $o_8 = -0.01$.\nThe forecast anomalies are\n$f_1 = 0.10$, $f_2 = -0.05$, $f_3 = 0.00$, $f_4 = 0.12$, $f_5 = 0.00$, $f_6 = 0.02$, $f_7 = 0.06$, $f_8 = -0.02$.\nAssume the reference climatology is a fixed anomaly baseline given by $c_i = 0$ for all $i$ (i.e., anomalies are defined relative to a stationary baseline period so the climatological anomaly is zero throughout).\n\nCompute the Mean Square Skill Score for this dataset. Round your final result to four significant figures. Express the final skill score as a dimensionless decimal.\n\nFinally, explain how the choice of the reference climatology $c_i$ (e.g., fixed baseline versus a time-evolving climatology that includes a trend estimate) affects the interpretation of the skill score in decadal prediction.",
            "solution": "The solution is presented in three parts as requested: derivation of the skill score expression, calculation for the given dataset, and an explanation of the role of the reference climatology.\n\n**Part 1: Derivation of the Mean Square Skill Score (MSSS) Expression**\n\nThe Mean Square Skill Score (MSSS) is defined as the relative improvement in the mean-square error (MSE) of a forecast ($f$) compared to a reference forecast ($c$), both evaluated against observations ($o$). For an error metric like MSE, where a lower value is better and a perfect score is $0$, the skill score is given by:\n$$\nMSSS = \\frac{MSE_c - MSE_f}{MSE_c} = 1 - \\frac{MSE_f}{MSE_c}\n$$\nwhere $MSE_f$ is the MSE of the forecast and $MSE_c$ is the MSE of the reference.\n\nUsing the provided unbiased sample estimator for a time series of length $n$, we have:\n$$\nMSE_f = \\frac{1}{n}\\sum_{i=1}^{n}(f_i - o_i)^{2}\n$$\n$$\nMSE_c = \\frac{1}{n}\\sum_{i=1}^{n}(c_i - o_i)^{2}\n$$\nSubstituting these into the MSSS definition, the $\\frac{1}{n}$ terms cancel:\n$$\nMSSS = 1 - \\frac{\\sum_{i=1}^{n}(f_i - o_i)^{2}}{\\sum_{i=1}^{n}(c_i - o_i)^{2}}\n$$\nTo derive the expression in the terms requested by the problem, we can manipulate the numerator. Let's expand the term $(f_i - o_i)^2$ by adding and subtracting the reference forecast $c_i$:\n$$\n(f_i - o_i)^2 = ((f_i - c_i) - (o_i - c_i))^2 = (f_i - c_i)^2 - 2(f_i - c_i)(o_i - c_i) + (o_i - c_i)^2\n$$\nSumming over all $i=1, \\dots, n$:\n$$\n\\sum_{i=1}^{n}(f_i - o_i)^{2} = \\sum_{i=1}^{n}(f_i - c_i)^2 - 2\\sum_{i=1}^{n}(f_i - c_i)(o_i - c_i) + \\sum_{i=1}^{n}(o_i - c_i)^2\n$$\nNow, substitute this expanded numerator back into the MSSS expression:\n$$\nMSSS = 1 - \\frac{\\sum_{i=1}^{n}(f_i - c_i)^2 - 2\\sum_{i=1}^{n}(f_i - c_i)(o_i - c_i) + \\sum_{i=1}^{n}(o_i - c_i)^2}{\\sum_{i=1}^{n}(c_i - o_i)^{2}}\n$$\nRecognizing that $\\sum_{i=1}^{n}(o_i - c_i)^{2} = \\sum_{i=1}^{n}(c_i - o_i)^{2}$, we can separate the fraction:\n$$\nMSSS = 1 - \\left( \\frac{\\sum_{i=1}^{n}(f_i - c_i)^2}{\\sum_{i=1}^{n}(c_i - o_i)^{2}} - \\frac{2\\sum_{i=1}^{n}(f_i - c_i)(o_i - c_i)}{\\sum_{i=1}^{n}(c_i - o_i)^{2}} + 1 \\right)\n$$\n$$\nMSSS = \\frac{2\\sum_{i=1}^{n}(f_i - c_i)(o_i - c_i) - \\sum_{i=1}^{n}(f_i - c_i)^2}{\\sum_{i=1}^{n}(c_i - o_i)^{2}}\n$$\nThis is the desired analytic expression for the MSSS. For the specific case where the reference climatology is a zero anomaly, i.e., $c_i = 0$ for all $i$, this expression simplifies to:\n$$\nMSSS = \\frac{2\\sum_{i=1}^{n}f_i o_i - \\sum_{i=1}^{n}f_i^2}{\\sum_{i=1}^{n}o_i^2}\n$$\n\n**Part 2: Calculation for the Given Dataset**\n\nThe dataset is given for $n=8$ decades.\nObserved anomalies: $o = [0.12, -0.08, 0.05, 0.10, -0.02, 0.03, 0.07, -0.01]$.\nForecast anomalies: $f = [0.10, -0.05, 0.00, 0.12, 0.00, 0.02, 0.06, -0.02]$.\nReference climatology: $c_i = 0$ for all $i$.\n\nWe will use the simplified formula for $c_i=0$.\nFirst, we compute the denominator, $\\sum_{i=1}^{8} o_i^2$:\n$$\n\\sum_{i=1}^{8} o_i^2 = (0.12)^2 + (-0.08)^2 + (0.05)^2 + (0.10)^2 + (-0.02)^2 + (0.03)^2 + (0.07)^2 + (-0.01)^2\n$$\n$$\n\\sum_{i=1}^{8} o_i^2 = 0.0144 + 0.0064 + 0.0025 + 0.0100 + 0.0004 + 0.0009 + 0.0049 + 0.0001 = 0.0396\n$$\nNext, we compute the two terms in the numerator.\nThe first term is $2\\sum_{i=1}^{8} f_i o_i$:\n$$\n\\sum_{i=1}^{8} f_i o_i = (0.10)(0.12) + (-0.05)(-0.08) + (0.00)(0.05) + (0.12)(0.10) + (0.00)(-0.02) + (0.02)(0.03) + (0.06)(0.07) + (-0.02)(-0.01)\n$$\n$$\n\\sum_{i=1}^{8} f_i o_i = 0.0120 + 0.0040 + 0 + 0.0120 + 0 + 0.0006 + 0.0042 + 0.0002 = 0.0330\n$$\nSo, $2\\sum_{i=1}^{8} f_i o_i = 2 \\times 0.0330 = 0.0660$.\n\nThe second term is $\\sum_{i=1}^{8} f_i^2$:\n$$\n\\sum_{i=1}^{8} f_i^2 = (0.10)^2 + (-0.05)^2 + (0.00)^2 + (0.12)^2 + (0.00)^2 + (0.02)^2 + (0.06)^2 + (-0.02)^2\n$$\n$$\n\\sum_{i=1}^{8} f_i^2 = 0.0100 + 0.0025 + 0 + 0.0144 + 0 + 0.0004 + 0.0036 + 0.0004 = 0.0313\n$$\nNow, assemble the MSSS:\n$$\nMSSS = \\frac{2\\sum f_i o_i - \\sum f_i^2}{\\sum o_i^2} = \\frac{0.0660 - 0.0313}{0.0396} = \\frac{0.0347}{0.0396}\n$$\n$$\nMSSS \\approx 0.87626262...\n$$\nRounding to four significant figures, the result is $0.8763$.\n\n**Part 3: Explanation of the Reference Climatology's Role**\n\nThe choice of the reference forecast, $c_i$, is a critical aspect of interpreting a skill score like the MSSS, as it defines the baseline against which skill is measured. An MSSS value of $0$ means the forecast performs no better than this baseline.\n\n1.  **Fixed Baseline Climatology ($c_i = 0$ or $c_i = \\text{constant}$):** This reference, often called simply \"climatology,\" assumes that the future state will be the long-term average, implying zero anomaly. The MSE of this reference, $MSE_c = \\frac{1}{n} \\sum (o_i - 0)^2$, is the mean squared anomaly, which is closely related to the variance of the observed time series. A forecast achieves a positive skill score ($MSSS > 0$) against this reference if it is, on average, closer to the observed anomalies than a guess of zero. In the context of decadal prediction, the climate system exhibits a forced trend (e.g., due to anthropogenic warming) and low-frequency internal variability. A fixed baseline is a \"low bar\" for skill because any forecast that simply captures the long-term warming trend will easily outperform a reference that assumes no change. A high MSSS against a fixed baseline might primarily reflect the model's ability to reproduce the forced climate response, not necessarily its ability to predict the more chaotic internal decadal fluctuations.\n\n2.  **Time-Evolving Climatology (e.g., including a trend):** This is a more sophisticated and stringent reference. For instance, $c_i$ could be defined by a linear trend fitted to past observations, such as $c_i = a \\cdot t_i + b$. This reference forecast assumes that the future will follow the recently observed trend. The goal of decadal prediction is not just to capture this forced trend but to predict the *deviations from it*, which are driven by the initial state of the climate system (e.g., initial ocean heat content) and its internal dynamics (e.g., modes like the Atlantic Multidecadal Oscillation). To achieve a positive skill score against a trend-based reference, the forecast system ($f_i$) must provide a more accurate prediction of the full anomaly than the trend extrapolation alone. This means it must have 'true skill' in predicting the timing and amplitude of decadal-scale internal climate variability.\n\nIn summary, the choice of $c_i$ fundamentally alters the scientific question being answered. Using a fixed baseline assesses whether the forecast is better than assuming a static climate. Using a time-evolving baseline assesses whether the forecast can predict internally generated climate variations beyond the predictable, externally forced trend. The latter is a much higher standard and is more relevant for evaluating the progress and utility of advanced decadal prediction systems.",
            "answer": "$$\\boxed{0.8763}$$"
        },
        {
            "introduction": "Honest and robust skill assessment requires avoiding statistical pitfalls that can lead to artificially inflated results. This practical exercise  addresses a common issue: \"skill inflation\" arising from using the same data to both correct forecast bias and evaluate forecast skill. Through a computational task, you will implement and contrast a naive in-sample bias correction with a proper cross-validated approach, directly quantifying the optimistic bias of the former and solidifying your understanding of why rigorous out-of-sample verification is essential.",
            "id": "4030499",
            "problem": "You are given deterministic decadal hindcast anomalies and corresponding observed anomalies for a global-mean climate index. Consider forecast bias as the expected difference between forecast and observation. Starting from core statistical definitions grounded in Numerical Weather Prediction and Climate Modeling, construct a program that computes a cross-validated bias correction using Leave-One-Out (LOO) hindcasts, and quantify its impact on skill inflation compared to in-sample correction. Work in physically meaningful units.\n\nFundamental base:\n- Define the forecast at year index $t$ as $f_t$ and the observation as $y_t$, for $t \\in \\{1, \\dots, N\\}$.\n- Define the forecast error $e_t$ as $e_t = f_t - y_t$.\n- Define the constant bias $\\beta$ as the expectation $\\beta = \\mathbb{E}[e_t]$.\n- Use the sample mean to estimate bias from a set of $N$ years: $\\hat{\\beta} = \\frac{1}{N}\\sum_{t=1}^{N} e_t$.\n- In Leave-One-Out cross-validation, for each target year $t$, estimate bias using all other years: $\\hat{\\beta}_{-t} = \\frac{1}{N-1}\\sum_{s \\neq t} e_s$.\n\nSkill quantification:\n- Define the bias-corrected in-sample forecast at year $t$ as $\\tilde{f}_t^{\\text{in}} = f_t - \\hat{\\beta}$.\n- Define the bias-corrected cross-validated forecast at year $t$ as $\\tilde{f}_t^{\\text{cv}} = f_t - \\hat{\\beta}_{-t}$.\n- Define the Mean Squared Error (MSE) as $\\mathrm{MSE} = \\frac{1}{N}\\sum_{t=1}^{N} (\\tilde{f}_t - y_t)^2$, where $\\tilde{f}_t$ is the corrected forecast used in the evaluation.\n- Define the skill inflation (a positive quantity means in-sample correction appears artificially better) as the difference $\\Delta = \\mathrm{MSE}_{\\text{cv}} - \\mathrm{MSE}_{\\text{in}}$.\n\nUnits and numerical requirements:\n- All anomalies are provided in Kelvin ($\\mathrm{K}$). You must compute and report skill inflation $\\Delta$ in squared Kelvin ($\\mathrm{K}^2$). Return raw decimal values.\n- No angles are involved.\n- The final output must aggregate results across the test suite into a single line: a comma-separated list enclosed in square brackets, for example $[x_1, x_2, x_3]$.\n\nTest suite:\nFor each case below, treat the provided sequences as the full $N$-year hindcast-observation pairs. Compute $\\Delta$ as specified and return the four results in order.\n\n1. Happy path, moderate sample size $N = 10$ with approximately constant positive bias:\n   - Observations $y^{(1)}$: $[0.10, 0.20, 0.00, -0.10, 0.30, 0.40, 0.20, 0.00, -0.20, 0.10]$ $\\mathrm{K}$\n   - Hindcasts $f^{(1)}$: $[0.42, 0.49, 0.31, 0.20, 0.58, 0.73, 0.49, 0.32, 0.07, 0.40]$ $\\mathrm{K}$\n\n2. Boundary case, very small sample $N = 3$:\n   - Observations $y^{(2)}$: $[0.00, 0.40, -0.20]$ $\\mathrm{K}$\n   - Hindcasts $f^{(2)}$: $[0.70, 0.80, 0.20]$ $\\mathrm{K}$\n\n3. Edge case, perfect forecasts $N = 8$:\n   - Observations $y^{(3)}$: $[0.05, -0.05, 0.10, -0.10, 0.00, 0.02, -0.03, 0.04]$ $\\mathrm{K}$\n   - Hindcasts $f^{(3)}$: $[0.05, -0.05, 0.10, -0.10, 0.00, 0.02, -0.03, 0.04]$ $\\mathrm{K}$\n\n4. Trend mismatch with time-varying bias $N = 10$:\n   - Observations $y^{(4)}$: $[0.00, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]$ $\\mathrm{K}$\n   - Hindcasts $f^{(4)}$: $[0.20, 0.33, 0.46, 0.55, 0.64, 0.77, 0.83, 0.96, 1.04, 1.15]$ $\\mathrm{K}$\n\nImplementation requirements:\n- Compute, for each case, the in-sample bias $\\hat{\\beta}$, in-sample corrected forecasts $\\tilde{f}_t^{\\text{in}}$, and $\\mathrm{MSE}_{\\text{in}}$.\n- Compute, for each case, the LOO bias $\\hat{\\beta}_{-t}$ for each $t$, LOO-corrected forecasts $\\tilde{f}_t^{\\text{cv}}$, and $\\mathrm{MSE}_{\\text{cv}}$.\n- For each case, compute skill inflation $\\Delta = \\mathrm{MSE}_{\\text{cv}} - \\mathrm{MSE}_{\\text{in}}$ in $\\mathrm{K}^2$ and collect the four $\\Delta$ values.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[x_1,x_2,x_3,x_4]$), where each $x_i$ is a decimal number in $\\mathrm{K}^2$.\n\nThe data are fixed and self-contained. Your implementation must be deterministic and self-contained, using only standard numerical routines without external inputs or files. Leave-One-Out (LOO) and Mean Squared Error (MSE) must be implemented directly from their definitions above.",
            "solution": "We begin with the fundamental definitions appropriate for decadal climate hindcast evaluation. The forecast at year index $t$ is $f_t$ and the observation is $y_t$. The forecast error is $e_t = f_t - y_t$. Bias is the expected value of the error, $\\beta = \\mathbb{E}[e_t]$, and a standard, widely accepted estimator based on $N$ samples is the sample mean $\\hat{\\beta} = \\frac{1}{N}\\sum_{t=1}^{N} e_t$.\n\nBias correction applies a constant shift to the forecast to remove the mean error. For in-sample correction, the bias-corrected forecast is $\\tilde{f}_t^{\\text{in}} = f_t - \\hat{\\beta}$ for each $t$. The Mean Squared Error (MSE) under in-sample correction is\n$$\n\\mathrm{MSE}_{\\text{in}} = \\frac{1}{N}\\sum_{t=1}^{N} (\\tilde{f}_t^{\\text{in}} - y_t)^2 \n= \\frac{1}{N}\\sum_{t=1}^{N} (f_t - \\hat{\\beta} - y_t)^2 \n= \\frac{1}{N}\\sum_{t=1}^{N} (e_t - \\hat{\\beta})^2.\n$$\n\nTo avoid skill inflation from reusing the verifying target in the training set, Leave-One-Out (LOO) cross-validation estimates the bias for each target year $t$ using all other years:\n$$\n\\hat{\\beta}_{-t} = \\frac{1}{N-1}\\sum_{s \\neq t} e_s.\n$$\nThe LOO-corrected forecast for year $t$ is $\\tilde{f}_t^{\\text{cv}} = f_t - \\hat{\\beta}_{-t}$, and the corresponding Mean Squared Error is\n$$\n\\mathrm{MSE}_{\\text{cv}} = \\frac{1}{N}\\sum_{t=1}^{N} (\\tilde{f}_t^{\\text{cv}} - y_t)^2 \n= \\frac{1}{N}\\sum_{t=1}^{N} (e_t - \\hat{\\beta}_{-t})^2.\n$$\n\nWe now connect these definitions to quantify skill inflation. Denote the in-sample mean error by $m = \\hat{\\beta} = \\frac{1}{N}\\sum_{t=1}^{N} e_t$. By algebraic manipulation of the LOO mean, noting that the sum over all errors is $N m$, we have\n$$\n\\hat{\\beta}_{-t} = \\frac{N m - e_t}{N-1}.\n$$\nTherefore,\n$$\ne_t - \\hat{\\beta}_{-t} \n= e_t - \\frac{N m - e_t}{N-1} \n= \\frac{N(e_t - m)}{N-1}.\n$$\nSquaring and averaging over $t$ yields the exact identity\n$$\n\\mathrm{MSE}_{\\text{cv}} \n= \\frac{1}{N}\\sum_{t=1}^{N} \\left( \\frac{N(e_t - m)}{N-1} \\right)^2\n= \\left( \\frac{N}{N-1} \\right)^2 \\cdot \\frac{1}{N} \\sum_{t=1}^{N} (e_t - m)^2\n= \\left( \\frac{N}{N-1} \\right)^2 \\mathrm{MSE}_{\\text{in}}.\n$$\nThis demonstrates that for constant bias correction, the cross-validated MSE is larger than the in-sample MSE by a multiplicative factor $\\left( \\frac{N}{N-1} \\right)^2 > 1$ when $N > 1$. The skill inflation, defined as the difference\n$$\n\\Delta = \\mathrm{MSE}_{\\text{cv}} - \\mathrm{MSE}_{\\text{in}},\n$$\nis\n$$\n\\Delta = \\mathrm{MSE}_{\\text{in}} \\left[ \\left( \\frac{N}{N-1} \\right)^2 - 1 \\right].\n$$\nThis is strictly positive for $N > 1$, showing the in-sample bias correction is optimistically biased, a classic manifestation of skill inflation due to double use of information.\n\nAlgorithmic implementation:\n- For each case, form arrays of $f_t$ and $y_t$ (in $\\mathrm{K}$).\n- Compute errors $e_t = f_t - y_t$.\n- Compute $\\hat{\\beta} = \\frac{1}{N}\\sum e_t$ and construct $\\tilde{f}_t^{\\text{in}} = f_t - \\hat{\\beta}$.\n- Compute $\\mathrm{MSE}_{\\text{in}} = \\frac{1}{N}\\sum (\\tilde{f}_t^{\\text{in}} - y_t)^2 = \\frac{1}{N}\\sum (e_t - \\hat{\\beta})^2$.\n- For LOO, for each $t$, compute $\\hat{\\beta}_{-t} = \\frac{1}{N-1}\\sum_{s \\neq t} e_s$ and $\\tilde{f}_t^{\\text{cv}} = f_t - \\hat{\\beta}_{-t}$, then compute $\\mathrm{MSE}_{\\text{cv}} = \\frac{1}{N}\\sum ( \\tilde{f}_t^{\\text{cv}} - y_t )^2$.\n- Compute and return $\\Delta$ for each case in $\\mathrm{K}^2$.\n- Aggregate the four $\\Delta$ values into a single line output in the specified bracketed, comma-separated format.\n\nThe test suite covers:\n- A typical decadal hindcast with mild bias (case $1$).\n- A very small sample where cross-validation instability is most evident (case $2$).\n- A perfect model case to verify zero inflation (case $3$).\n- A trend mismatch case with time-varying bias to test the generality of the derivation (case $4$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_inflation(y, f):\n    \"\"\"\n    Compute skill inflation (MSE_cv - MSE_in) in K^2 using\n    in-sample constant bias correction and leave-one-out cross-validated bias correction.\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    f = np.asarray(f, dtype=float)\n    assert y.shape == f.shape, \"Observation and forecast arrays must have the same shape.\"\n    n = y.size\n    # Errors\n    e = f - y\n    # In-sample bias estimate and corrected forecasts\n    beta_in = np.mean(e)\n    f_corr_in = f - beta_in\n    mse_in = np.mean((f_corr_in - y) ** 2)\n    # Leave-one-out bias correction per element\n    f_corr_cv = np.empty_like(f_corr_in)\n    # Precompute sum of errors for efficiency\n    sum_e = np.sum(e)\n    for t in range(n):\n        # Leave-one-out mean of errors\n        beta_loo_t = (sum_e - e[t]) / (n - 1) if n > 1 else beta_in\n        f_corr_cv[t] = f[t] - beta_loo_t\n    mse_cv = np.mean((f_corr_cv - y) ** 2)\n    # Skill inflation: positive means in-sample appears better\n    inflation = mse_cv - mse_in\n    return float(inflation)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case 1: Happy path, moderate sample size N=10 with approximately constant positive bias\n    y1 = [0.10, 0.20, 0.00, -0.10, 0.30, 0.40, 0.20, 0.00, -0.20, 0.10]\n    f1 = [0.42, 0.49, 0.31, 0.20, 0.58, 0.73, 0.49, 0.32, 0.07, 0.40]\n\n    # Case 2: Boundary case, very small sample N=3\n    y2 = [0.00, 0.40, -0.20]\n    f2 = [0.70, 0.80, 0.20]\n\n    # Case 3: Edge case, perfect forecasts N=8\n    y3 = [0.05, -0.05, 0.10, -0.10, 0.00, 0.02, -0.03, 0.04]\n    f3 = [0.05, -0.05, 0.10, -0.10, 0.00, 0.02, -0.03, 0.04]\n\n    # Case 4: Trend mismatch with time-varying bias N=10\n    y4 = [0.00, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]\n    f4 = [0.20, 0.33, 0.46, 0.55, 0.64, 0.77, 0.83, 0.96, 1.04, 1.15]\n\n    test_cases = [\n        (y1, f1),\n        (y2, f2),\n        (y3, f3),\n        (y4, f4),\n    ]\n\n    results = []\n    for y, f in test_cases:\n        result = compute_inflation(y, f)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}