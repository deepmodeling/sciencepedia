{
    "hands_on_practices": [
        {
            "introduction": "The formulas for numerical integrators can often seem opaque, but they are rooted in simple, elegant ideas. This first practice demystifies the widely used Adams family of methods by returning to first principles: the integral form of an ordinary differential equation. By approximating the integrand with a simple polynomial, you will derive both an explicit Adams–Bashforth and an implicit Adams–Moulton scheme, gaining a foundational understanding of how multistep methods are constructed and the crucial distinction between explicit and implicit approaches. ",
            "id": "4084573",
            "problem": "In a semi-discrete model of a prognostic scalar in numerical weather prediction and climate modeling, spatial discretization of a conservation law yields an ordinary differential equation of the form $\\frac{d y}{dt} = f(t, y)$, where $y$ is the vector of gridpoint values and $f$ is a consistently discretized tendency. Consider uniform time steps of size $h > 0$ with time levels $t_n = t_0 + n h$ and denote $y_n \\approx y(t_n)$ and $f_n = f(t_n, y_n)$. The exact integral form over one step is $y_{n+1} = y_n + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt$.\n\nUsing only the following foundational elements:\n- The integral representation $y_{n+1} = y_n + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt$,\n- Polynomial interpolation of $f$ by a Lagrange interpolating polynomial on selected nodes in time,\n- Exact integration of that interpolant over $[t_n, t_{n+1}]$,\n\nderive from first principles the coefficients of:\n- The two-step Adams–Bashforth method of order $2$ (Adams–Bashforth (AB) $2$), obtained by interpolating $f$ at the past nodes $\\{t_{n}, t_{n-1}\\}$ and integrating the interpolant over $[t_n, t_{n+1}]$,\n- The second-order Adams–Moulton method (Adams–Moulton (AM) $2$), obtained by interpolating $f$ at the nodes $\\{t_{n+1}, t_{n}\\}$ and integrating the interpolant over $[t_n, t_{n+1}]$.\n\nExpress both formulas in the canonical form\n$y_{n+1} = y_n + h \\sum_{j} \\beta_j f_{n+j}$,\nand determine the numerical values of the coefficients. Report your final answer as the row vector $\\big(\\beta^{\\mathrm{AB2}}_{0}, \\beta^{\\mathrm{AB2}}_{-1}, \\beta^{\\mathrm{AM2}}_{1}, \\beta^{\\mathrm{AM2}}_{0}\\big)$, corresponding respectively to the coefficients multiplying $f_n$, $f_{n-1}$ in Adams–Bashforth $2$, and $f_{n+1}$, $f_n$ in Adams–Moulton $2$. Provide the exact rational values. No rounding is required. No physical units are needed in the final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It constitutes a standard derivation in the field of numerical analysis for ordinary differential equations. We proceed with the derivation as requested.\n\nThe fundamental principle for both Adams–Bashforth and Adams–Moulton methods is to approximate the integral in the exact relation $y_{n+1} = y_n + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt$. This is achieved by first approximating the integrand $f(t, y(t))$ with a polynomial $P(t)$ that interpolates known values of the tendency function, $f_k = f(t_k, y_k)$, at a set of previous or current time levels. The integral of this polynomial is then computed exactly.\n\nLet the time step be uniform, $h = t_{k+1} - t_k$ for any integer $k$. To simplify the integration, we introduce a non-dimensional time coordinate $s$ defined by the relation $t = t_n + sh$. Consequently, $dt = h \\, ds$. The integration interval $[t_n, t_{n+1}]$ corresponds to $s$ from $0$ to $1$.\n\n**Derivation of the Second-Order Adams–Bashforth (AB2) Method**\n\nThe AB2 method is an explicit two-step method. It approximates $f(t, y(t))$ over the interval $[t_n, t_{n+1}]$ by a polynomial that interpolates the known values $f_n = f(t_n, y_n)$ and $f_{n-1} = f(t_{n-1}, y_{n-1})$. Since there are two points, this is a linear polynomial (degree $1$).\n\nThe Lagrange interpolating polynomial $P_1^{\\mathrm{AB}}(t)$ passing through the points $(t_n, f_n)$ and $(t_{n-1}, f_{n-1})$ is given by:\n$$\nP_1^{\\mathrm{AB}}(t) = f_{n-1} \\frac{t - t_n}{t_{n-1} - t_n} + f_n \\frac{t - t_{n-1}}{t_n - t_{n-1}}\n$$\nRecognizing that $t_{n-1} - t_n = -h$ and $t_n - t_{n-1} = h$, we have:\n$$\nP_1^{\\mathrm{AB}}(t) = f_{n-1} \\frac{t - t_n}{-h} + f_n \\frac{t - t_{n-1}}{h}\n$$\nIn terms of the non-dimensional coordinate $s$, we have $t - t_n = sh$ and $t - t_{n-1} = (t_n + sh) - (t_n - h) = h(s+1)$. Substituting these into the polynomial yields:\n$$\nP_1^{\\mathrm{AB}}(t(s)) = f_{n-1} \\frac{sh}{-h} + f_n \\frac{h(s+1)}{h} = -s f_{n-1} + (s+1) f_n\n$$\nThe integral is now approximated as:\n$$\n\\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt \\approx \\int_{t_n}^{t_{n+1}} P_1^{\\mathrm{AB}}(t) \\, dt = \\int_{0}^{1} P_1^{\\mathrm{AB}}(t(s)) \\, h \\, ds\n$$\n$$\n= h \\int_{0}^{1} \\left( -s f_{n-1} + (s+1) f_n \\right) \\, ds\n$$\n$$\n= h \\left[ -\\frac{s^2}{2} f_{n-1} + \\left(\\frac{s^2}{2} + s\\right) f_n \\right]_0^1\n$$\n$$\n= h \\left( -\\frac{1}{2} f_{n-1} + \\left(\\frac{1}{2} + 1\\right) f_n \\right) - h(0)\n$$\n$$\n= h \\left( \\frac{3}{2} f_n - \\frac{1}{2} f_{n-1} \\right)\n$$\nSubstituting this result into the integral form of the ODE gives the AB2 method:\n$$\ny_{n+1} = y_n + h \\left( \\frac{3}{2} f_n - \\frac{1}{2} f_{n-1} \\right)\n$$\nFrom the canonical form $y_{n+1} = y_n + h \\sum_{j} \\beta_j f_{n+j}$, the coefficients are identified as:\n$\\beta^{\\mathrm{AB2}}_{0} = \\frac{3}{2}$ and $\\beta^{\\mathrm{AB2}}_{-1} = -\\frac{1}{2}$.\n\n**Derivation of the Second-Order Adams–Moulton (AM2) Method**\n\nThe AM2 method is an implicit one-step method. It approximates $f(t, y(t))$ over the interval $[t_n, t_{n+1}]$ by a linear polynomial interpolating the values $f_{n+1} = f(t_{n+1}, y_{n+1})$ and $f_n = f(t_n, y_n)$. This method is implicit because it requires the value $f_{n+1}$ at the future time step.\n\nThe Lagrange interpolating polynomial $P_1^{\\mathrm{AM}}(t)$ passing through the points $(t_{n+1}, f_{n+1})$ and $(t_n, f_n)$ is:\n$$\nP_1^{\\mathrm{AM}}(t) = f_n \\frac{t - t_{n+1}}{t_n - t_{n+1}} + f_{n+1} \\frac{t - t_n}{t_{n+1} - t_n}\n$$\nWith $t_n - t_{n+1} = -h$ and $t_{n+1} - t_n = h$, this becomes:\n$$\nP_1^{\\mathrm{AM}}(t) = f_n \\frac{t - t_{n+1}}{-h} + f_{n+1} \\frac{t - t_n}{h}\n$$\nUsing the same coordinate transformation $t = t_n + sh$, we have $t - t_n = sh$ and $t - t_{n+1} = (t_n + sh) - (t_n + h) = h(s-1)$. The polynomial in $s$ is:\n$$\nP_1^{\\mathrm{AM}}(t(s)) = f_n \\frac{h(s-1)}{-h} + f_{n+1} \\frac{sh}{h} = (1-s) f_n + s f_{n+1}\n$$\nThe integral approximation is:\n$$\n\\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt \\approx \\int_{t_n}^{t_{n+1}} P_1^{\\mathrm{AM}}(t) \\, dt = \\int_{0}^{1} P_1^{\\mathrm{AM}}(t(s)) \\, h \\, ds\n$$\n$$\n= h \\int_{0}^{1} \\left( (1-s) f_n + s f_{n+1} \\right) \\, ds\n$$\n$$\n= h \\left[ \\left(s - \\frac{s^2}{2}\\right) f_n + \\frac{s^2}{2} f_{n+1} \\right]_0^1\n$$\n$$\n= h \\left( \\left(1 - \\frac{1}{2}\\right) f_n + \\frac{1}{2} f_{n+1} \\right) - h(0)\n$$\n$$\n= h \\left( \\frac{1}{2} f_n + \\frac{1}{2} f_{n+1} \\right)\n$$\nThis results in the AM2 method, also known as the trapezoidal rule:\n$$\ny_{n+1} = y_n + \\frac{h}{2} (f_{n+1} + f_n)\n$$\nFrom the canonical form, the coefficients are identified as:\n$\\beta^{\\mathrm{AM2}}_{1} = \\frac{1}{2}$ and $\\beta^{\\mathrm{AM2}}_{0} = \\frac{1}{2}$.\n\n**Final Coefficients**\n\nCombining the results, the coefficients are:\n- For Adams–Bashforth 2: $\\beta^{\\mathrm{AB2}}_{0} = \\frac{3}{2}$, $\\beta^{\\mathrm{AB2}}_{-1} = -\\frac{1}{2}$\n- For Adams–Moulton 2: $\\beta^{\\mathrm{AM2}}_{1} = \\frac{1}{2}$, $\\beta^{\\mathrm{AM2}}_{0} = \\frac{1}{2}$\n\nThe requested row vector is $\\big(\\beta^{\\mathrm{AB2}}_{0}, \\beta^{\\mathrm{AB2}}_{-1}, \\beta^{\\mathrm{AM2}}_{1}, \\beta^{\\mathrm{AM2}}_{0}\\big)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{2} & -\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Knowing how to construct a method is only half the battle; knowing when to use it is what separates theory from effective practice. In numerical weather and climate modeling, the choice of integrator is frequently dictated by its stability properties, especially when dealing with stiff systems containing fast waves or rapid diffusion. This exercise introduces the cornerstone of stability analysis by having you derive the stability function for the classical fourth-order Runge-Kutta method. By analyzing this function, you will gain crucial insights into how a method’s stability region constrains the time step for different physical processes, a fundamental consideration in model design. ",
            "id": "4084714",
            "problem": "In a semi-implicit spectral core of a numerical weather prediction system, certain modes of the linearized discretized primitive equations can be isolated as eigenmodes of the spatial operator. After consistent spatial discretization, each such mode obeys the scalar ordinary differential equation $y'(t) = \\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$ is an eigenvalue of the discretized linear operator that may encode gravity-wave propagation ($\\lambda$ with nearly imaginary values) or diffusion ($\\lambda$ with negative real part). Consider applying the classical explicit four-stage fourth-order Runge–Kutta method (RK) with a fixed time step $h > 0$ to this scalar test problem. Let the method be defined by the stage equations\n$$\nk_{1} = f(y_{n}), \\quad\nk_{2} = f\\!\\left(y_{n} + \\frac{h}{2} k_{1}\\right), \\quad\nk_{3} = f\\!\\left(y_{n} + \\frac{h}{2} k_{2}\\right), \\quad\nk_{4} = f\\!\\left(y_{n} + h\\, k_{3}\\right),\n$$\nand the update\n$$\ny_{n+1} = y_{n} + \\frac{h}{6}\\left(k_{1} + 2 k_{2} + 2 k_{3} + k_{4}\\right),\n$$\nwhere $f(y) = \\lambda y$ and $z = h \\lambda$. Starting from these definitions and the scalar test equation, derive the stability function $R(z)$ defined by $y_{n+1} = R(z)\\, y_{n}$. Then, based on $R(z)$, qualitatively describe the absolute stability region $\\{z \\in \\mathbb{C} : |R(z)| \\leq 1\\}$ in the complex plane, indicating how it intersects the negative real axis and how it behaves near the imaginary axis. Briefly interpret these features in the context of explicit time stepping for gravity-wave and diffusive modes in numerical weather prediction and climate modeling, and contrast them with the qualitative stability properties of Adams-type multistep methods. Provide the analytical expression for $R(z)$ as your final answer. No rounding is required, and no units should be included in the final expression.",
            "solution": "The problem statement is well-posed, scientifically grounded, and contains all necessary information to derive the stability function for the classical fourth-order Runge–Kutta method and analyze its properties. The problem is valid.\n\nThe task is to derive the stability function $R(z)$ for the explicit four-stage, fourth-order Runge–Kutta (RK4) method when applied to the scalar test equation $y'(t) = \\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$. The stability function is defined by the relation $y_{n+1} = R(z) y_n$, where $z = h\\lambda$ and $h$ is the time step.\n\nThe RK4 method is defined by:\n$$\nk_{1} = f(y_{n})\n$$\n$$\nk_{2} = f\\left(y_{n} + \\frac{h}{2} k_{1}\\right)\n$$\n$$\nk_{3} = f\\left(y_{n} + \\frac{h}{2} k_{2}\\right)\n$$\n$$\nk_{4} = f\\left(y_{n} + h k_{3}\\right)\n$$\n$$\ny_{n+1} = y_{n} + \\frac{h}{6}\\left(k_{1} + 2 k_{2} + 2 k_{3} + k_{4}\\right)\n$$\n\nFor the test equation $y' = \\lambda y$, the function is $f(y) = \\lambda y$. We substitute this into the stage equations:\n\nStage 1:\n$$\nk_1 = \\lambda y_n\n$$\n\nStage 2:\n$$\nk_2 = \\lambda \\left(y_n + \\frac{h}{2} k_1\\right) = \\lambda \\left(y_n + \\frac{h}{2} (\\lambda y_n)\\right) = \\lambda y_n \\left(1 + \\frac{h\\lambda}{2}\\right)\n$$\n\nStage 3:\n$$\nk_3 = \\lambda \\left(y_n + \\frac{h}{2} k_2\\right) = \\lambda \\left(y_n + \\frac{h}{2} \\left[\\lambda y_n \\left(1 + \\frac{h\\lambda}{2}\\right)\\right]\\right) = \\lambda y_n \\left(1 + \\frac{h\\lambda}{2} + \\frac{(h\\lambda)^2}{4}\\right)\n$$\n\nStage 4:\n$$\nk_4 = \\lambda \\left(y_n + h k_3\\right) = \\lambda \\left(y_n + h \\left[\\lambda y_n \\left(1 + \\frac{h\\lambda}{2} + \\frac{(h\\lambda)^2}{4}\\right)\\right]\\right) = \\lambda y_n \\left(1 + h\\lambda + \\frac{(h\\lambda)^2}{2} + \\frac{(h\\lambda)^3}{4}\\right)\n$$\n\nNow, we substitute these expressions for the stages $k_1, k_2, k_3, k_4$ into the final update formula for $y_{n+1}$:\n$$\ny_{n+1} = y_n + \\frac{h}{6} \\left[ \\lambda y_n + 2\\lambda y_n \\left(1 + \\frac{h\\lambda}{2}\\right) + 2\\lambda y_n \\left(1 + \\frac{h\\lambda}{2} + \\frac{(h\\lambda)^2}{4}\\right) + \\lambda y_n \\left(1 + h\\lambda + \\frac{(h\\lambda)^2}{2} + \\frac{(h\\lambda)^3}{4}\\right) \\right]\n$$\nFactor out $y_n$ from the right-hand side and substitute $z = h\\lambda$:\n$$\ny_{n+1} = y_n \\left( 1 + \\frac{h\\lambda}{6} \\left[ 1 + 2\\left(1 + \\frac{h\\lambda}{2}\\right) + 2\\left(1 + \\frac{h\\lambda}{2} + \\frac{(h\\lambda)^2}{4}\\right) + \\left(1 + h\\lambda + \\frac{(h\\lambda)^2}{2} + \\frac{(h\\lambda)^3}{4}\\right) \\right] \\right)\n$$\n$$\ny_{n+1} = y_n \\left( 1 + \\frac{z}{6} \\left[ 1 + 2\\left(1 + \\frac{z}{2}\\right) + 2\\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right) + \\left(1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\\right) \\right] \\right)\n$$\nSimplify the expression within the brackets:\n$$\n[ \\dots ] = 1 + (2+z) + (2+z+\\frac{z^2}{2}) + (1+z+\\frac{z^2}{2}+\\frac{z^3}{4})\n$$\n$$\n[ \\dots ] = (1+2+2+1) + (z+z+z) + \\left(\\frac{z^2}{2}+\\frac{z^2}{2}\\right) + \\frac{z^3}{4} = 6 + 3z + z^2 + \\frac{z^3}{4}\n$$\nSubstitute this back into the expression for $y_{n+1}$:\n$$\ny_{n+1} = y_n \\left( 1 + \\frac{z}{6} \\left[ 6 + 3z + z^2 + \\frac{z^3}{4} \\right] \\right)\n$$\n$$\ny_{n+1} = y_n \\left( 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24} \\right)\n$$\nBy comparing this with the definition $y_{n+1} = R(z) y_n$, we identify the stability function $R(z)$ as:\n$$\nR(z) = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24} = \\sum_{k=0}^{4} \\frac{z^k}{k!}\n$$\nThis is the fourth-order Taylor series expansion of the exponential function $\\exp(z)$. The solution to the exact ODE is $y(t_{n+1}) = \\exp(h\\lambda) y(t_n) = \\exp(z) y(t_n)$, so $R(z)$ is a fourth-order approximation to $\\exp(z)$.\n\nThe region of absolute stability is the set $\\{z \\in \\mathbb{C} : |R(z)| \\leq 1\\}$.\nQualitatively, this region is a bounded, bean-shaped area in the left half of the complex plane, symmetric about the real axis and containing the origin.\n-   Its intersection with the negative real axis is an interval $[x_0, 0]$. The left endpoint $x_0$ is found by solving $R(x_0)=-1$, which yields $x_0 \\approx -2.785$. Thus, for purely diffusive problems ($z=h\\lambda$ is real and negative), the stability requirement is $|z| \\leq 2.785$.\n-   Its intersection with the imaginary axis is an interval $[-i\\omega_0, i\\omega_0]$. The endpoints are found by solving $|R(i\\omega)| = 1$. This occurs at $\\omega_0 = \\sqrt{8} = 2\\sqrt{2} \\approx 2.828$. Thus, for purely oscillatory problems ($z=h\\lambda$ is purely imaginary), the stability requirement is $|z| \\leq 2\\sqrt{2}$.\n\nIn the context of numerical weather prediction and climate modeling:\n-   **Diffusive modes** correspond to $\\lambda$ with a negative real part. The stability limit on the negative real axis ($|h\\lambda| \\leq 2.785$) imposes a constraint on the time step $h$ that is inversely proportional to the strength of the diffusion. For strong diffusion (large $|\\text{Re}(\\lambda)|$), $h$ must be very small.\n-   **Gravity-wave modes** correspond to $\\lambda$ being nearly imaginary. The limited extent of the stability region along the imaginary axis ($|h\\lambda| \\leq 2\\sqrt{2}$) is particularly restrictive. Fast-propagating gravity waves have large imaginary parts of $\\lambda$, which forces the use of a very small time step $h$ to maintain stability. This is the Courant–Friedrichs–Lewy (CFL) condition for these waves. This severe time step restriction is a primary motivation for using semi-implicit schemes in atmospheric models, where the terms responsible for fast waves are treated implicitly to remove this stability constraint.\n\nContrast with Adams-type multistep methods:\n-   **Adams-Bashforth (explicit) methods:** Like RK4, these are explicit methods with bounded stability regions. For a given order, their stability regions are generally smaller and more complex in shape than those of Runge–Kutta methods. For example, the 4th-order Adams-Bashforth method has a stability interval on the real axis of approximately $[-1.3, 0]$, which is much more restrictive than RK4. They are similarly unsuitable for the stiff components of atmospheric models.\n-   **Adams-Moulton (implicit) methods:** These have substantially larger stability regions. The 2nd-order Adams-Moulton method (the trapezoidal rule) is A-stable, meaning its stability region includes the entire left half-plane, $\\text{Re}(z) \\leq 0$. This makes it unconditionally stable for any diffusive or purely oscillatory stable mode. Higher-order Adams-Moulton methods are not A-stable but are \"stiffly stable,\" possessing much larger stability regions than any explicit method of the same order. This favorable stability property allows for much larger time steps, making them, and other implicit schemes, essential for the efficient long-term integration of atmospheric and climate models where stiffness from various physical processes is a dominant feature. Explicit methods like RK4 are often reserved for non-stiff components, such as advection, or for starting up multistep methods.",
            "answer": "$$\n\\boxed{1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24}}\n$$"
        },
        {
            "introduction": "Our theoretical analysis has so far assumed a fixed time step, $\\Delta t$. However, most modern, high-performance codes use adaptive time-stepping to balance accuracy and computational cost, taking small steps when the solution changes rapidly and large steps when it behaves smoothly. This final practice brings you to the forefront of numerical implementation by guiding you through the creation of an adaptive solver based on an embedded Runge–Kutta pair. You will implement the logic for estimating local error and controlling the step size, building a core component of a robust, real-world ordinary differential equation solver. ",
            "id": "4084691",
            "problem": "Consider the scalar linear ordinary differential equation $y'=\\lambda y$ with $\\lambda=-100\\,\\mathrm{s}^{-1}$ representing a rapidly decaying mode that arises when linearizing fast processes (for example, acoustic or mixing processes) in numerical weather prediction and climate modeling. The exact solution is $y(t)=y(0)\\,\\exp(\\lambda t)$, but explicit time integration schemes must select the time step $\\Delta t$ carefully to control local error and maintain stability.\n\nYou are to implement a single adaptive explicit Runge–Kutta step using the embedded Dormand–Prince pair of orders five and four (commonly denoted RK$5(4)$, also referred to as Runge–Kutta $45$). In this embedded approach, two approximations $y^{[5]}$ and $y^{[4]}$ are formed in one set of stages, and the local error estimate is computed from their difference. The accepted step size $\\Delta t$ is determined by an error controller that compares the scaled local error to a tolerance target and reduces $\\Delta t$ if necessary until acceptance.\n\nStart from the following fundamental base:\n- The definition of the initial value problem $y'=\\lambda y$, $y(0)=y_0$, with $\\lambda=-100\\,\\mathrm{s}^{-1}$.\n- The general $s$-stage explicit Runge–Kutta construction with stages $k_i=f\\!\\left(y_n+\\Delta t\\sum_{j=1}^{i-1} a_{ij}\\,k_j\\right)$, where $f(y)=\\lambda y$, and $y_{n+1}$ obtained as a weighted sum of stages.\n- Embedded Runge–Kutta pairs produce two solutions of different orders and an error estimate from their difference.\n- A standard adaptive time-step controller uses a safety factor and a power-law update in the local error to adjust $\\Delta t$.\n\nImplement the Dormand–Prince RK$5(4)$ method with the following well-tested coefficients (Butcher tableau), where each coefficient is to be used exactly as given:\n- Nodes $c$: $c_1=0$, $c_2=\\frac{1}{5}$, $c_3=\\frac{3}{10}$, $c_4=\\frac{4}{5}$, $c_5=\\frac{8}{9}$, $c_6=1$, $c_7=1$.\n- Lower-triangular $a_{ij}$, for $i=2,\\dots,7$ and $j<i$:\n  $$\n  a_{21}=\\frac{1}{5},\\quad\n  a_{31}=\\frac{3}{40},\\ a_{32}=\\frac{9}{40},\\quad\n  a_{41}=\\frac{44}{45},\\ a_{42}=-\\frac{56}{15},\\ a_{43}=\\frac{32}{9},\\quad\n  a_{51}=\\frac{19372}{6561},\\ a_{52}=-\\frac{25360}{2187},\\ a_{53}=\\frac{64448}{6561},\\ a_{54}=-\\frac{212}{729},\\quad\n  a_{61}=\\frac{9017}{3168},\\ a_{62}=-\\frac{355}{33},\\ a_{63}=\\frac{46732}{5247},\\ a_{64}=\\frac{49}{176},\\ a_{65}=-\\frac{5103}{18656},\\quad\n  a_{71}=\\frac{35}{384},\\ a_{72}=0,\\ a_{73}=\\frac{500}{1113},\\ a_{74}=\\frac{125}{192},\\ a_{75}=-\\frac{2187}{6784},\\ a_{76}=\\frac{11}{84}.\n  $$\n- Weights for the fifth-order solution $b^{[5]}$: $b^{[5]}_1=\\frac{35}{384}$, $b^{[5]}_2=0$, $b^{[5]}_3=\\frac{500}{1113}$, $b^{[5]}_4=\\frac{125}{192}$, $b^{[5]}_5=-\\frac{2187}{6784}$, $b^{[5]}_6=\\frac{11}{84}$, $b^{[5]}_7=0$.\n- Weights for the embedded fourth-order solution $b^{[4]}$: $b^{[4]}_1=\\frac{5179}{57600}$, $b^{[4]}_2=0$, $b^{[4]}_3=\\frac{7571}{16695}$, $b^{[4]}_4=\\frac{393}{640}$, $b^{[4]}_5=-\\frac{92097}{339200}$, $b^{[4]}_6=\\frac{187}{2100}$, $b^{[4]}_7=\\frac{1}{40}$.\n\nLet the scaled local error be computed as\n$$\n\\mathrm{err} \\;=\\; \\frac{\\left|y^{[5]}-y^{[4]}\\right|}{\\mathrm{atol} + \\mathrm{rtol}\\cdot \\max\\!\\left(\\left|y_n\\right|,\\left|y^{[5]}\\right|\\right)},\n$$\nwhere $\\mathrm{atol}$ is the absolute tolerance and $\\mathrm{rtol}$ is the relative tolerance. The step is accepted when $\\mathrm{err}\\leq 1$. If rejected, update the trial step $\\Delta t$ using\n$$\n\\Delta t \\leftarrow \\Delta t \\cdot \\gamma \\cdot \\mathrm{err}^{-1/5},\n$$\nand bound it by lower and upper multiplicative factors so that\n$$\n\\Delta t \\leftarrow \\Delta t \\cdot \\min\\!\\left(f_{\\max}, \\max\\!\\left(f_{\\min}, \\gamma \\cdot \\mathrm{err}^{-1/5}\\right)\\right),\n$$\nwith safety factor $\\gamma=0.9$, lower bound $f_{\\min}=0.2$, and upper bound $f_{\\max}=5.0$. Repeat until acceptance. If $\\mathrm{err}=0$, treat $\\mathrm{err}$ as a very small positive number to avoid division by zero. Use a minimum absolute step floor $\\Delta t_{\\min}=10^{-12}\\,\\mathrm{s}$ to ensure numerical robustness during rejection iterations.\n\nYour program must perform the above single adaptive step starting from $t_n=0$ and given $y_n=y_0$, $\\lambda=-100\\,\\mathrm{s}^{-1}$, and an initial guess $\\Delta t_0$ for each of the following test cases. For each case, compute and report the accepted $\\Delta t$ in seconds:\n- Case $1$ (happy path): $y_0=1.0$, $\\Delta t_0=0.05\\,\\mathrm{s}$, $\\mathrm{atol}=10^{-6}$, $\\mathrm{rtol}=10^{-3}$.\n- Case $2$ (rejection needed): $y_0=1.0$, $\\Delta t_0=0.5\\,\\mathrm{s}$, $\\mathrm{atol}=10^{-9}$, $\\mathrm{rtol}=10^{-6}$.\n- Case $3$ (loose tolerance): $y_0=1.0$, $\\Delta t_0=0.5\\,\\mathrm{s}$, $\\mathrm{atol}=10^{-6}$, $\\mathrm{rtol}=10^{-2}$.\n- Case $4$ (absolute tolerance dominates): $y_0=10^{-8}$, $\\Delta t_0=0.5\\,\\mathrm{s}$, $\\mathrm{atol}=10^{-12}$, $\\mathrm{rtol}=10^{-3}$.\n\nExpress the accepted $\\Delta t$ values in seconds. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\Delta t_1,\\Delta t_2,\\Delta t_3,\\Delta t_4]$), with each entry a floating-point number representing the accepted $\\Delta t$ for the corresponding test case in seconds.\n\nEnsure scientific realism and consistency with the stated coefficients and controller parameters. Although explicit Adams multistep integrators are widely used for smooth problems in climate models, this task focuses on the single-step adaptive Runge–Kutta approach because it avoids the need for starting procedures and is robust for transient fast modes.",
            "solution": "The scalar initial value problem $y'=\\lambda y$ with $\\lambda=-100\\,\\mathrm{s}^{-1}$ models an exponentially decaying mode, with exact solution $y(t)=y_0\\exp(\\lambda t)$. In numerical weather prediction and climate modeling, explicit time integrators are often employed for non-stiff components or when avoiding complex linear solves, but they require adaptive step-size control to manage local error and stability, especially when fast decay rates are present.\n\nThe Dormand–Prince embedded Runge–Kutta pair provides two approximations, of orders five and four, computed using the same set of stages. The difference $y^{[5]}-y^{[4]}$ serves as a local error estimate. To adapt the step size, one compares the scaled error to a target tolerance based on absolute tolerance $\\mathrm{atol}$ and relative tolerance $\\mathrm{rtol}$. The scalar error measure\n$$\n\\mathrm{err}=\\frac{\\left|y^{[5]}-y^{[4]}\\right|}{\\mathrm{atol}+\\mathrm{rtol}\\cdot\\max\\left(\\left|y_n\\right|,\\left|y^{[5]}\\right|\\right)}\n$$\nreflects the standard practice found in widely used solvers: the denominator combines an absolute floor with a proportional term to the magnitude of the solution, anchored by the larger of the current and trial magnitudes to stabilize scaling. Acceptance requires $\\mathrm{err}\\leq 1$; otherwise, the step is rejected and the time step updated.\n\nThe underlying principle of the adaptive update is that the local error behaves approximately as a power of the step size. For an embedded four/five pair, the error estimate scales like $\\Delta t^{5}$ in leading order. Therefore, a controller of the form\n$$\n\\Delta t_{\\text{new}}=\\Delta t \\cdot \\gamma \\cdot \\mathrm{err}^{-1/5}\n$$\nbrings $\\mathrm{err}$ closer to $1$ in the next attempt, with $\\gamma\\in(0,1)$ a safety factor to compensate for modeling uncertainty and nonlinearities. To avoid excessive oscillations and to maintain numerical robustness, the multiplicative change is bounded between $f_{\\min}$ and $f_{\\max}$, yielding\n$$\n\\Delta t \\leftarrow \\Delta t \\cdot \\min\\left(f_{\\max},\\max\\left(f_{\\min},\\gamma\\cdot \\mathrm{err}^{-1/5}\\right)\\right),\n$$\nwith typical values $\\gamma=0.9$, $f_{\\min}=0.2$, and $f_{\\max}=5.0$. If $\\mathrm{err}=0$, the difference $y^{[5]}-y^{[4]}$ vanishes to machine precision; replacing $\\mathrm{err}$ by a small positive number avoids division by zero without affecting acceptance, since $\\mathrm{err}\\leq 1$ triggers acceptance immediately.\n\nTo compute $y^{[5]}$ and $y^{[4]}$, we apply the explicit Runge–Kutta stages. Denote $f(y)=\\lambda y$. For the Dormand–Prince RK$5(4)$ method with seven stages, the stages are computed recursively:\n$$\n\\begin{aligned}\nk_1 &= f\\left(y_n\\right),\\\\\nk_2 &= f\\left(y_n+\\Delta t\\,a_{21}\\,k_1\\right),\\\\\nk_3 &= f\\left(y_n+\\Delta t\\,(a_{31}\\,k_1+a_{32}\\,k_2)\\right),\\\\\nk_4 &= f\\left(y_n+\\Delta t\\,(a_{41}\\,k_1+a_{42}\\,k_2+a_{43}\\,k_3)\\right),\\\\\nk_5 &= f\\left(y_n+\\Delta t\\,(a_{51}\\,k_1+a_{52}\\,k_2+a_{53}\\,k_3+a_{54}\\,k_4)\\right),\\\\\nk_6 &= f\\left(y_n+\\Delta t\\,(a_{61}\\,k_1+a_{62}\\,k_2+a_{63}\\,k_3+a_{64}\\,k_4+a_{65}\\,k_5)\\right),\\\\\nk_7 &= f\\left(y_n+\\Delta t\\,(a_{71}\\,k_1+a_{72}\\,k_2+a_{73}\\,k_3+a_{74}\\,k_4+a_{75}\\,k_5+a_{76}\\,k_6)\\right).\n\\end{aligned}\n$$\nThe two solutions are then\n$$\ny^{[5]}=y_n+\\Delta t\\sum_{i=1}^{7} b^{[5]}_i\\,k_i,\\qquad\ny^{[4]}=y_n+\\Delta t\\sum_{i=1}^{7} b^{[4]}_i\\,k_i.\n$$\nFor our scalar $f(y)=\\lambda y$, these operations are straightforward: each stage is a linear combination of previously computed stages, scaled by $\\lambda$ and $\\Delta t$.\n\nAlgorithmic steps for a single adaptive step:\n- Initialize $y_n=y_0$, $\\lambda=-100\\,\\mathrm{s}^{-1}$, and a trial step $\\Delta t=\\Delta t_0$.\n- Compute $k_1,\\dots,k_7$ using the stated $a_{ij}$.\n- Form $y^{[5]}$ and $y^{[4]}$ using $b^{[5]}$ and $b^{[4]}$ respectively.\n- Compute $\\mathrm{err}$ using $\\mathrm{atol}$ and $\\mathrm{rtol}$.\n- If $\\mathrm{err}\\leq 1$, accept and report $\\Delta t$; otherwise, update $\\Delta t$ using the controller with $\\gamma=0.9$, $f_{\\min}=0.2$, $f_{\\max}=5.0$, enforce $\\Delta t\\geq \\Delta t_{\\min}=10^{-12}\\,\\mathrm{s}$, and repeat.\n- Stop once the step is accepted.\n\nThis approach contrasts with explicit Adams multistep schemes that require multi-step history and a start-up procedure; the single-step Runge–Kutta method is self-starting and well-suited to transient behavior in fast modes. The controller exponent $-1/5$ targets the dominant $\\Delta t^{5}$ scaling in the embedded error, and the safety factor plus bounds mitigate over-aggressive changes.\n\nApplying this to the provided test suite:\n- Case $1$ with moderate tolerances typically accepts $\\Delta t_0$ directly if the scaled error is already $\\leq 1$.\n- Case $2$ with tight tolerances and a large $\\Delta t_0$ will reject initially and reduce $\\Delta t$ until the scaled error criterion is met.\n- Case $3$ with loose tolerance likely accepts the initial $\\Delta t_0$.\n- Case $4$ emphasizes the absolute tolerance component when $y_0$ is small, which can affect scaling and acceptance.\n\nThe program implements these steps with the Dormand–Prince coefficients and prints the accepted $\\Delta t$ values in seconds for all four cases as a single comma-separated list enclosed in square brackets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk45_single_adaptive(lambda_val, y0, dt0, atol, rtol,\n                         safety=0.9, fac_min=0.2, fac_max=5.0,\n                         dt_floor=1e-12, max_iters=1000):\n    \"\"\"\n    Perform a single adaptive explicit RK step using Dormand–Prince RK45 on y' = lambda * y.\n    Returns the accepted dt.\n    \"\"\"\n    # Dormand–Prince 5(4) coefficients\n    a = np.zeros((7, 7), dtype=float)\n    a[1,0] = 1.0/5.0\n    a[2,0] = 3.0/40.0; a[2,1] = 9.0/40.0\n    a[3,0] = 44.0/45.0; a[3,1] = -56.0/15.0; a[3,2] = 32.0/9.0\n    a[4,0] = 19372.0/6561.0; a[4,1] = -25360.0/2187.0; a[4,2] = 64448.0/6561.0; a[4,3] = -212.0/729.0\n    a[5,0] = 9017.0/3168.0; a[5,1] = -355.0/33.0; a[5,2] = 46732.0/5247.0; a[5,3] = 49.0/176.0; a[5,4] = -5103.0/18656.0\n    a[6,0] = 35.0/384.0; a[6,1] = 0.0; a[6,2] = 500.0/1113.0; a[6,3] = 125.0/192.0; a[6,4] = -2187.0/6784.0; a[6,5] = 11.0/84.0\n\n    b5 = np.array([35.0/384.0, 0.0, 500.0/1113.0, 125.0/192.0, -2187.0/6784.0, 11.0/84.0, 0.0], dtype=float)\n    b4 = np.array([5179.0/57600.0, 0.0, 7571.0/16695.0, 393.0/640.0, -92097.0/339200.0, 187.0/2100.0, 1.0/40.0], dtype=float)\n\n    dt = float(dt0)\n    y_n = float(y0)\n\n    for _ in range(max_iters):\n        # Compute stages k1..k7\n        k = np.zeros(7, dtype=float)\n        # k1\n        y_stage = y_n\n        k[0] = lambda_val * y_stage\n        # k2..k7\n        for i in range(1, 7):\n            y_stage = y_n + dt * np.dot(a[i, :i], k[:i])\n            k[i] = lambda_val * y_stage\n\n        # Fifth- and fourth-order solutions\n        y5 = y_n + dt * np.dot(b5, k)\n        y4 = y_n + dt * np.dot(b4, k)\n\n        # Scaled error\n        denom = atol + rtol * max(abs(y_n), abs(y5))\n        # Guard against zero denominator (should not happen if atol > 0 or rtol > 0)\n        if denom == 0.0:\n            denom = np.finfo(float).tiny\n        err = abs(y5 - y4) / denom\n\n        if err <= 1.0 or dt <= dt_floor:\n            # Accept the step; clamp to floor if needed\n            return max(dt, dt_floor)\n\n        # Update dt using controller with exponent 1/5\n        # Prevent division by zero or overflow by bounding err\n        err_safe = max(err, np.finfo(float).tiny)\n        factor = safety * err_safe ** (-1.0/5.0)\n        factor = min(fac_max, max(fac_min, factor))\n        dt = max(dt_floor, dt * factor)\n\n    # If we reach here, return the current dt (best effort)\n    return max(dt, dt_floor)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (y0, dt0, atol, rtol)\n    test_cases = [\n        (1.0, 0.05, 1e-6, 1e-3),      # Case 1: happy path\n        (1.0, 0.5, 1e-9, 1e-6),       # Case 2: rejection needed\n        (1.0, 0.5, 1e-6, 1e-2),       # Case 3: loose tolerance\n        (1e-8, 0.5, 1e-12, 1e-3),     # Case 4: absolute tolerance dominates\n    ]\n\n    lambda_val = -100.0  # s^-1\n    results = []\n    for y0, dt0, atol, rtol in test_cases:\n        accepted_dt = rk45_single_adaptive(lambda_val, y0, dt0, atol, rtol)\n        results.append(accepted_dt)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}