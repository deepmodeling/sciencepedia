{
    "hands_on_practices": [
        {
            "introduction": "这项练习旨在阐明三维变分同化（3D-Var）的核心数学过程。通过为一个简单的低维系统手动计算分析场和分析增量，你将具体理解背景信息和观测信息是如何根据其指定的误差协方差进行最优融合的。这是所有变分方法的基本构建模块。",
            "id": "3864622",
            "problem": "考虑一个双分量环境状态向量 $x \\in \\mathbb{R}^{2}$，它表示在线性设定下从单个卫星派生观测中待分析的空间聚合量。假设一个三维变分（3D-Var）数据同化框架，该框架源于高斯误差统计和线性观测算子，其中分析场 $x^{a}$ 最小化由背景场和观测值的不符量构建的二次代价函数。设背景误差协方差为 $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$，观测算子为 $H=\\begin{bmatrix}1  1\\end{bmatrix}$，观测误差协方差为 $R=1$，背景场（也称为先验）为 $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，观测值为 $y=3$。\n\n从在线性高斯假设下结合背景场和观测值不符量的 3D-Var 代价函数定义出发，推导出表征其最小值的的一阶最优性条件（正规方程）。然后，使用给定的数值，计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析状态 $x^{a}$。将最终结果表示为顺序为 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 的单行矩阵。\n\n无需单位。提供精确值，无需四舍五入。",
            "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤1：提取已知条件\n-   状态向量：$x \\in \\mathbb{R}^{2}$\n-   背景误差协方差：$B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$\n-   观测算子：$H=\\begin{bmatrix}1  1\\end{bmatrix}$\n-   观测误差协方差：$R=1$\n-   背景状态：$x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$\n-   观测值：$y=3$\n-   任务：推导正规方程，然后计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析状态 $x^{a}$。\n-   最终输出格式：一个单行矩阵 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$。\n\n### 步骤2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n-   **科学依据：** 该问题描述了一个标准的三维变分（3D-Var）数据同化场景。二次代价函数、背景/观测误差协方差和线性观测算子的使用是这种在遥感和环境建模中已确立的科学方法的典型要素。该框架是在线性高斯假设下从 Bayes 定理推导出来的。该问题在科学上是合理的。\n-   **适定性：** 该问题旨在寻找一个二次代价函数的最小值。背景误差协方差矩阵 $B$ 的特征值为 $1$ 和 $4$，均为正数，因此 $B$ 是正定的，从而可逆。观测误差协方差 $R=1$ 是一个正标量。代价函数的海森矩阵 $(B^{-1} + H^T R^{-1} H)$ 将被证明是正定的，这保证了唯一、稳定最小值的存在。所有必要信息都已提供，没有矛盾之处。该问题是适定的。\n-   **客观性：** 该问题使用精确的数学语言和定义（$x_b, B, H, R, y$）进行陈述。没有主观或模糊的术语。该问题是客观的。\n-   **其他缺陷：** 该问题并非微不足道，因为它需要推导和矩阵代数。对于一个教科书示例来说，它不是隐喻性的、不完整的或不切实际的。\n\n### 步骤3：结论与行动\n问题有效。将提供解答。\n\n3D-Var 分析状态 $x^a$ 是使代价函数 $J(x)$ 最小化的状态向量 $x$。代价函数衡量了与背景状态和观测值的不符量，并由它们各自的误差协方差进行加权。对于一个线性高斯系统，它由以下公式给出：\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\n为了找到最小值，我们计算 $J(x)$ 关于 $x$ 的梯度，并将其设为零。梯度 $\\nabla_x J(x)$ 为：\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\n将 $x=x^a$ 处的梯度设为零，得到一阶最优性条件，也称为正规方程：\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\n这就是所要求的正规方程的推导。\n\n我们被要求求解分析增量，其定义为 $\\delta x^a = x^a - x_b$。我们将 $x^a = x_b + \\delta x^a$ 代入正规方程：\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\n重新整理各项以求解 $\\delta x^a$：\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\n这个方程可以用来求解分析增量 $\\delta x^a$。\n\n现在，我们代入给定的数值：\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1  1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\n首先，我们计算方程中 $\\delta x^a$ 的各个组成部分。\n项 $(y - Hx_b)$ 是新息：\n$$y - Hx_b = 3 - \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\n方程的右侧是：\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\n方程左侧的矩阵是代价函数的海森矩阵：\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1  1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}$$\n为了求解 $\\delta x^a$，我们需要求这个矩阵的逆。其行列式为：\n$$\\det\\left(\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\n其逆矩阵为：\n$$\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{12}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix}$$\n现在我们可以求解 $\\delta x^a$：\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n所以，分析增量的分量是 $\\delta x^a_1 = \\frac{1}{2}$ 和 $\\delta x^a_2 = 2$。\n\n最后，我们计算分析状态 $x^a$：\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n分析状态的分量是 $x^a_1 = \\frac{1}{2}$ 和 $x^a_2 = 2$。\n\n最终结果要求以 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 的顺序表示为单行矩阵。这给出：\n$$\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "这项练习超越了单一计算，旨在探索三维变分（3D-Var）分析对背景误差协方差矩阵 $B$ 的敏感性。通过系统地缩放 $B$ 并观察其对分析增量和后验（分析）误差的影响，你将建立起对我们先验背景信息信度如何量化地影响最终分析的直观认识。这个编程练习演示了调试数据同化系统的一个关键方面。",
            "id": "4108358",
            "problem": "考虑线性三维变分数据同化（3D-Var），这是四维变分数据同化（4D-Var）在仅限于单个分析时间时的特例。在 3D-Var 中，状态向量表示为 $\\boldsymbol{x} \\in \\mathbb{R}^n$，背景态为 $\\boldsymbol{x}_b \\in \\mathbb{R}^n$，观测向量为 $\\boldsymbol{y} \\in \\mathbb{R}^m$，线性观测算子为 $\\boldsymbol{H} \\in \\mathbb{R}^{m \\times n}$。背景误差协方差为 $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$，观测误差协方差为 $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$，这两个协方差矩阵都是对称正定的。3D-Var 分析是通过在满足高斯-线性假设的条件下，最小化标准二次型目标函数得到的。本问题中的所有量都是无量纲的，不需要物理单位。\n\n您的任务是评估分析相对于基准情况 $s = 1.0$ 对背景误差协方差 $\\boldsymbol{B}$ 进行标量因子 $s \\in \\{0.5, 2.0\\}$ 缩放的敏感性，并量化由此在以下两个度量指标中引起的变化：\n- 后验方差度量，定义为分析误差协方差的迹，$\\operatorname{tr}(\\boldsymbol{A})$，其中 $\\boldsymbol{A}$ 是由最小化过程所隐含的分析误差协方差矩阵。\n- 分析增量的大小，定义为增量向量的欧几里得范数 $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$，其中 $\\boldsymbol{x}_a$ 是使目标函数最小化的分析态。\n\n从基本原理出发：具有背景项和观测项的 3D-Var 的高斯-线性公式，以及严格凸二次型函数的最小值点是通过将其梯度置零得到的这一事实。在未从这些原理推导的情况下，不得假设或使用任何简便公式。\n\n实现一个程序，对于下述每个测试用例，计算将缩放因子 $s \\in \\{1.0, 0.5, 2.0\\}$ 应用于 $\\boldsymbol{B}$（即使用 $\\boldsymbol{B}_s = s \\boldsymbol{B}$，同时保持 $\\boldsymbol{R}$、$\\boldsymbol{H}$、$\\boldsymbol{x}_b$ 和 $\\boldsymbol{y}$ 不变）时的上述两个度量指标。\n\n测试套件：\n- 用例 1（理想情况，标量状态及精确观测）：\n  - $n = 1$, $m = 1$,\n  - $\\boldsymbol{B} = [1.0]$,\n  - $\\boldsymbol{R} = [0.25]$,\n  - $\\boldsymbol{H} = [1.0]$,\n  - $\\boldsymbol{x}_b = [0.0]$,\n  - $\\boldsymbol{y} = [2.0]$,\n  - 评估 $s \\in \\{1.0, 0.5, 2.0\\}$。\n\n- 用例 2（边缘情况，二维状态，对第一个分量有单个带噪声的观测，且背景误差相关）：\n  - $n = 2$, $m = 1$,\n  - $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$,\n  - $\\boldsymbol{R} = [4.0]$,\n  - $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$,\n  - $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$,\n  - $\\boldsymbol{y} = [1.0]$,\n  - 评估 $s \\in \\{1.0, 0.5, 2.0\\}$。\n\n对于六次评估中的每一次（两个用例乘以三个缩放因子），计算：\n1. 后验方差度量 $\\operatorname{tr}(\\boldsymbol{A})$，作为一个浮点数。\n2. 增量大小 $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$，作为一个浮点数。\n\n最终输出格式：\n- 将每个浮点数四舍五入到六位小数。\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素是某次评估的 $[\\operatorname{tr}(\\boldsymbol{A}), \\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2]$ 对，顺序如下：\n  - 用例 1，$s=1.0$；用例 1，$s=0.5$；用例 1，$s=2.0$；用例 2，$s=1.0$；用例 2，$s=0.5$；用例 2，$s=2.0$。\n- 具体来说，程序必须打印形如以下格式的单行：\n  - $[[v_1,i_1],[v_2,i_2],[v_3,i_3],[v_4,i_4],[v_5,i_5],[v_6,i_6]]$\n  其中每个 $v_k$ 和 $i_k$ 都四舍五入到六位小数，并以十进制数表示（无百分号）。",
            "solution": "该问题经评估有效。它在科学上基于变分数据同化的原理，问题设定良好，成本函数的凸性保证了唯一解的存在，并且其表述是客观的。两个测试用例的所有必要数据和条件均已提供，没有矛盾或含糊之处。\n\n任务是分析 3D-Var 分析对背景误差协方差矩阵进行标量扰动的敏感性。这需要从第一性原理出发推导分析态和分析误差协方差。\n\n需要最小化的 3D-Var 成本函数 $J(\\boldsymbol{x})$，在高斯误差假设下，代表后验概率密度函数的负对数。它由两个二次项组成：一个背景项，衡量状态向量 $\\boldsymbol{x} \\in \\mathbb{R}^n$ 与背景态 $\\boldsymbol{x}_b \\in \\mathbb{R}^n$ 之间的偏离；一个观测项，衡量投影状态 $\\boldsymbol{H}\\boldsymbol{x}$ 与观测值 $\\boldsymbol{y} \\in \\mathbb{R}^m$ 之间的偏离。\n\n$$\nJ(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^T \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})^T \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})\n$$\n\n这里，$\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ 是背景误差协方差矩阵，$\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$ 是观测误差协方差矩阵。两者都被给定为对称正定，这确保了 $J(\\boldsymbol{x})$ 是一个严格凸函数并有唯一的最小值。\n\n为找到最小化 $J(\\boldsymbol{x})$ 的分析态 $\\boldsymbol{x}_a$，我们必须找到成本函数关于 $\\boldsymbol{x}$ 的梯度为零的点。根据标准矩阵微积分法则，梯度为：\n\n$$\n\\nabla_{\\boldsymbol{x}} J(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x} - \\boldsymbol{y})\n$$\n\n在 $\\boldsymbol{x} = \\boldsymbol{x}_a$ 处将梯度设为零：\n\n$$\n\\boldsymbol{B}^{-1}(\\boldsymbol{x}_a - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{y}) = 0\n$$\n\n重新整理各项以求解 $\\boldsymbol{x}_a$：\n\n$$\n\\boldsymbol{B}^{-1}\\boldsymbol{x}_a - \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y} = 0\n$$\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})\\boldsymbol{x}_a = \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y}\n$$\n\n因此，分析态为：\n$$\n\\boldsymbol{x}_a = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} (\\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y})\n$$\n\n分析误差协方差矩阵 $\\boldsymbol{A}$ 是在最小值点处计算的成本函数的海森矩阵的逆。海森矩阵是二阶导数矩阵：\n\n$$\n\\boldsymbol{A} = (\\nabla^2_{\\boldsymbol{x}} J(\\boldsymbol{x}))^{-1} = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1}\n$$\n\n虽然这些形式是正确的，但它们涉及对 $\\boldsymbol{B}$ 和 $\\boldsymbol{R}$ 求逆，这对于大的状态和观测空间来说计算成本可能很高。可以推导出一种替代的、更实用的公式。让我们通过重新整理梯度方程来表示分析增量 $\\delta\\boldsymbol{x} = \\boldsymbol{x}_a - \\boldsymbol{x}_b$：\n\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})(\\boldsymbol{x}_a - \\boldsymbol{x}_b) = \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n$$\n\\boldsymbol{x}_a - \\boldsymbol{x}_b = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n\n使用 Woodbury 矩阵恒等式，我们可以写出 $(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} = \\boldsymbol{B} - \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\\boldsymbol{H}\\boldsymbol{B}$。这是 $\\boldsymbol{A}$ 的表达式。\n将此代入增量方程很繁琐。一个更直接的方法是定义卡尔曼增益矩阵 $\\boldsymbol{K}$：\n\n$$\n\\boldsymbol{K} = \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n$$\n\n通过这个定义，分析增量和分析误差协方差可以高效地表示为：\n\n1.  分析增量：$\\boldsymbol{x}_a - \\boldsymbol{x}_b = \\boldsymbol{K}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)$\n2.  分析误差协方差：$\\boldsymbol{A} = (\\boldsymbol{I} - \\boldsymbol{K}\\boldsymbol{H})\\boldsymbol{B}$\n\n这种公式避免了对大矩阵 $\\boldsymbol{B}$ 求逆，而是需要对尺寸为 $m \\times m$ 的较小矩阵 $(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})$ 求逆。\n\n该问题要求评估对 $\\boldsymbol{B}$ 进行因子 $s \\in \\{1.0, 0.5, 2.0\\}$ 缩放的敏感性。我们定义 $\\boldsymbol{B}_s = s\\boldsymbol{B}$。每次评估的算法如下：\n\n令 $\\boldsymbol{B}_s = s\\boldsymbol{B}$。\n1.  计算缩放问题的卡尔曼增益矩阵：\n    $$\n    \\boldsymbol{K}_s = \\boldsymbol{B}_s \\boldsymbol{H}^T (\\boldsymbol{H} \\boldsymbol{B}_s \\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n    $$\n2.  计算分析增量向量：\n    $$\n    \\delta\\boldsymbol{x}_s = \\boldsymbol{x}_{a,s} - \\boldsymbol{x}_b = \\boldsymbol{K}_s(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n    $$\n3.  计算第一个度量，即分析增量的大小：\n    $$\n    \\text{度量 2: } \\lVert \\delta\\boldsymbol{x}_s \\rVert_2\n    $$\n4.  计算缩放问题的分析误差协方差矩阵：\n    $$\n    \\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s\n    $$\n5.  计算第二个度量，即后验方差度量：\n    $$\n    \\text{度量 1: } \\operatorname{tr}(\\boldsymbol{A}_s)\n    $$\n\n对每个测试用例和每个 $s$ 的值执行这五个步骤。然后根据问题规范整理和格式化结果。\n\n对于用例 1，标量情况（$n=1, m=1$）：$\\boldsymbol{B}=[1.0]$, $\\boldsymbol{R}=[0.25]$, $\\boldsymbol{H}=[1.0]$, $\\boldsymbol{x}_b=[0.0]$, $\\boldsymbol{y}=[2.0]$。\n对于 $s=1.0$：$\\boldsymbol{B}_s=[1.0]$。增益为 $K_s = 1.0 \\times (1.0 \\times 1.0 \\times 1.0 + 0.25)^{-1} = 0.8$。增量为 $0.8 \\times (2.0 - 0.0) = 1.6$。范数为 $1.6$。分析协方差为 $A_s = (1 - 0.8 \\times 1.0) \\times 1.0 = 0.2$。迹为 $0.2$。\n\n对于用例 2，二维状态（$n=2, m=1$）：$\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$, $\\boldsymbol{R} = [4.0]$, $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$, $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$, $\\boldsymbol{y} = [1.0]$。\n对于 $s=1.0$：$\\boldsymbol{B}_s = \\boldsymbol{B}$。新息为 $\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b = [1.0]$。项 $\\boldsymbol{H}\\boldsymbol{B}_s\\boldsymbol{H}^T + \\boldsymbol{R}$ 变为 $[1.0] + [4.0] = [5.0]$。增益矩阵为 $\\boldsymbol{K}_s = \\begin{bmatrix}1.0 \\\\ 0.3\\end{bmatrix} [5.0]^{-1} = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$。增量为 $\\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix} \\times 1.0 = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$。范数为 $\\sqrt{0.2^2 + 0.06^2} \\approx 0.208806$。分析协方差为 $\\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s = \\begin{bmatrix}0.8  0.24 \\\\ 0.24  1.482\\end{bmatrix}$。迹为 $0.8 + 1.482 = 2.282$。\n\n其余的计算对于其他 $s$ 值遵循相同的程序。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var sensitivity analysis problem.\n\n    The function computes two metrics for two different test cases, each evaluated\n    with three different scaling factors for the background-error covariance matrix.\n    The metrics are:\n    1. The trace of the analysis-error covariance matrix.\n    2. The Euclidean norm of the analysis increment.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 1,\n            \"B\": np.array([[1.0]]),\n            \"R\": np.array([[0.25]]),\n            \"H\": np.array([[1.0]]),\n            \"xb\": np.array([0.0]),\n            \"y\": np.array([2.0]),\n        },\n        {\n            \"n\": 2,\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[4.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0]),\n        }\n    ]\n\n    scales = [1.0, 0.5, 2.0]\n    \n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        R = case[\"R\"]\n        H = case[\"H\"]\n        xb = case[\"xb\"]\n        y = case[\"y\"]\n\n        for s in scales:\n            # Scale the background-error covariance matrix B\n            Bs = s * B\n\n            # Compute the innovation vector (d = y - H*xb)\n            innovation = y - H @ xb\n\n            # Compute the Kalman gain matrix K_s\n            # K_s = B_s * H^T * (H * B_s * H^T + R)^-1\n            HBH_T = H @ Bs @ H.T\n            inv_term = np.linalg.inv(HBH_T + R)\n            Ks = Bs @ H.T @ inv_term\n\n            # 1. Compute the analysis increment and its Euclidean norm\n            # inc = K_s * (y - H*xb)\n            increment = Ks @ innovation\n            increment_norm = np.linalg.norm(increment)\n\n            # 2. Compute the analysis-error covariance A_s and its trace\n            # A_s = (I - K_s * H) * B_s\n            identity_n = np.eye(n)\n            As = (identity_n - Ks @ H) @ Bs\n            trace_A = np.trace(As)\n\n            # Append the pair of results for this evaluation\n            all_results.append([trace_A, increment_norm])\n\n    # Format the final output string as specified in the problem statement.\n    # e.g., [[v1,i1],[v2,i2],...] with 6 decimal places.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "真实世界系统通常涉及非线性过程和观测算子，这项实践介绍了增量法——一种通过迭代线性化来处理变分同化中非线性的标准技术。通过在一个简化的非线性问题中比较单外循环和双外循环分析，你将能够量化线性化误差的影响，并理解为什么多次“外循环”对于在非线性情景下获得准确分析至关重要。",
            "id": "4108403",
            "problem": "考虑三维变分同化 (3D-Var) 和四维变分同化 (4D-Var) 的标准贝叶斯公式，其中高斯误差假设导致对先验和观测偏差进行二次惩罚。在 3D-Var 中，通过最小化一个标量值的代价函数来获得分析状态，该函数在一个可能非线性的观测算子下衡量与先验信息和观测的失配度。设状态向量为 $x \\in \\mathbb{R}^n$，先验（背景）状态为 $x_b \\in \\mathbb{R}^n$，其协方差为 $B \\in \\mathbb{R}^{n \\times n}$，观测向量为 $y \\in \\mathbb{R}^m$，其协方差为 $R \\in \\mathbb{R}^{m \\times m}$。观测算子 $h:\\mathbb{R}^n \\to \\mathbb{R}^m$通常是非线性的。3D-Var 的目标函数是\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1}(x - x_b) + \\frac{1}{2}\\big(h(x) - y\\big)^\\top R^{-1}\\big(h(x) - y\\big).\n$$\n在增量方法中，非线性观测算子使用一阶泰勒展开 $h(x_0 + \\delta x) \\approx h(x_0) + H(x_0)\\delta x$ 在参考状态 $x_0$ 附近进行线性化，其中 $H(x_0) \\in \\mathbb{R}^{m \\times n}$ 是 $h$ 在 $x_0$ 处求值的雅可比矩阵。这产生一个二次增量代价，其最小化器为外循环参考状态生成一个更新量 $\\delta x$。外循环的次数控制着线性化点的更新和重新线性化的程度，从而影响非线性情况下的分析。本问题的目的是通过在一个受控的非线性玩具问题中比较单外循环和双外循环，来量化线性化误差对最终分析的影响。\n\n您需要为以下玩具问题实现、分析和比较单外循环与双外循环的增量 3D-Var 分析：\n\n- 维度和结构：设 $n=m=5$。设真实状态固定为 $x_{\\text{true}} = [1.5,-1.0,0.5,-0.8,1.2]^\\top$。\n- 非线性观测算子：对于一个标量参数 $\\gamma \\ge 0$，定义逐分量的非线性算子\n$$\nh(x)_i = x_i + \\gamma x_i^3, \\quad i = 1,\\dots,n,\n$$\n及其在 $x$ 处的雅可比矩阵为对角矩阵 $H(x) = \\operatorname{diag}\\big(1 + 3\\gamma x_i^2\\big)$。\n- 协方差：设 $B = \\operatorname{diag}(\\sigma_b^2,\\dots,\\sigma_b^2)$ 和 $R = \\operatorname{diag}(\\sigma_r^2,\\dots,\\sigma_r^2)$。\n- 背景状态：设 $x_b = x_{\\text{true}} + \\alpha \\cdot p$，其中 $p = [0.7,-0.7,0.3,-0.3,0.5]^\\top$ 且 $\\alpha \\ge 0$ 缩放背景误差。\n- 观测：设 $y = h(x_{\\text{true}}) + \\beta \\cdot q$，其中 $q = [0.1,-0.05,0.03,-0.02,0.0]^\\top$ 且 $\\beta \\ge 0$ 缩放一个固定的确定性观测误差向量。不得使用任何随机数。\n\n通过最小化二次近似来定义在线性化点 $x_0$ 处的增量 3D-Var 更新\n$$\n\\tilde{J}(\\delta x; x_0) = \\frac{1}{2}\\delta x^\\top B^{-1}\\delta x + \\frac{1}{2}\\big(H(x_0)\\delta x - d(x_0)\\big)^\\top R^{-1}\\big(H(x_0)\\delta x - d(x_0)\\big),\n$$\n其中在线性化点处的新息为 $d(x_0) = y - h(x_0)$。一阶最优性条件产生正规方程\n$$\n\\big(B^{-1} + H(x_0)^\\top R^{-1} H(x_0)\\big)\\delta x = H(x_0)^\\top R^{-1} d(x_0).\n$$\n鉴于对角结构，该系统在各分量上解耦，并对每个分量 $i$ 都有闭式解：\n$$\n\\delta x_i = \\frac{H_{ii}(x_0) \\, d_i(x_0)/\\sigma_r^2}{1/\\sigma_b^2 + H_{ii}^2(x_0)/\\sigma_r^2}.\n$$\n单外循环分析是 $x^{(1)} = x_b + \\delta x(x_b)$，通过在 $x_0 = x_b$ 处线性化得到。双外循环分析是 $x^{(2)} = x^{(1)} + \\delta x(x^{(1)})$，其中第二次更新重用该公式，但在 $x_0 = x^{(1)}$ 处进行线性化。\n\n为量化线性化误差的影响，请为每个测试用例计算以下指标：\n- 单循环分析误差的欧几里得范数，$\\|x^{(1)} - x_{\\text{true}}\\|_2$。\n- 双循环分析误差的欧几里得范数，$\\|x^{(2)} - x_{\\text{true}}\\|_2$。\n- 第二次外循环带来的欧几里得范数的改进量，$\\|x^{(1)} - x_{\\text{true}}\\|_2 - \\|x^{(2)} - x_{\\text{true}}\\|_2$。\n- 单循环的线性化误差指标，定义为在 $x^{(1)}$ 处相对于 $x_0 = x_b$ 的一阶泰勒余项的欧几里得范数，\n$$\nE_{\\text{lin}}^{(1)} = \\big\\| h(x^{(1)}) - \\big(h(x_b) + H(x_b)(x^{(1)} - x_b)\\big) \\big\\|_2.\n$$\n- 双循环最终新息的欧几里得范数，$\\|y - h(x^{(2)})\\|_2$。\n\n实现一个程序，为以下四个测试用例计算这些指标，这些用例共同构成一个基础测试套件，用于探索非线性以及背景/观测误差尺度的不同状况：\n1. $\\gamma = 0.8$, $\\alpha = 1.0$, $\\beta = 1.0$, $\\sigma_b = 0.5$, $\\sigma_r = 0.2$。\n2. $\\gamma = 0.0$, $\\alpha = 1.0$, $\\beta = 1.0$, $\\sigma_b = 0.5$, $\\sigma_r = 0.2$。\n3. $\\gamma = 2.0$, $\\alpha = 2.0$, $\\beta = 0.5$, $\\sigma_b = 0.8$, $\\sigma_r = 0.1$。\n4. $\\gamma = 1.5$, $\\alpha = 0.3$, $\\beta = 0.0$, $\\sigma_b = 0.4$, $\\sigma_r = 0.05$。\n\n本问题中没有物理单位；所有量都是无量纲实数。不涉及角度。不得使用百分比。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按上述顺序输出包含五个浮点数的列表。将四个列表聚合到一个外部列表中，因此最终输出格式必须为\n$$\n\\big[[m_{11},m_{12},m_{13},m_{14},m_{15}], [m_{21},m_{22},m_{23},m_{24},m_{25}], [m_{31},m_{32},m_{33},m_{34},m_{35}], [m_{41},m_{42},m_{43},m_{44},m_{45}]\\big],\n$$\n其中 $m_{ij}$ 表示第 $i$ 个测试用例的第 $j$ 个指标。程序必须是自包含的，不得读取任何外部输入，也不得使用任何随机性。",
            "solution": "该问题要求对一个指定的非线性玩具问题，实现并比较单循环和双循环增量 3D-Var 分析。分析是通过顺序求解一个能最小化二次代价函数的增量来执行的，该二次代价函数是完整非线性代价函数的线性化。\n\n问题的核心在于增量 3D-Var 更新。在给定的线性化点 $x_0$，分析增量 $\\delta x$ 通过最小化二次代价函数找到：\n$$\n\\tilde{J}(\\delta x; x_0) = \\frac{1}{2}\\delta x^\\top B^{-1}\\delta x + \\frac{1}{2}\\big(H(x_0)\\delta x - d(x_0)\\big)^\\top R^{-1}\\big(H(x_0)\\delta x - d(x_0)\\big)\n$$\n其中 $d(x_0) = y - h(x_0)$ 是新息向量。最小化子 $\\delta x$ 是正规方程的解：\n$$\n\\big(B^{-1} + H(x_0)^\\top R^{-1} H(x_0)\\big)\\delta x = H(x_0)^\\top R^{-1} d(x_0)\n$$\n鉴于协方差矩阵 $B = \\sigma_b^2 I$ 和 $R = \\sigma_r^2 I$ 是对角的，并且雅可比矩阵 $H(x_0)$ 也是对角的，这个线性方程组是解耦的。对于每个分量 $i \\in \\{1, \\dots, n\\}$，方程为：\n$$\n\\left(\\frac{1}{\\sigma_b^2} + \\frac{H_{ii}(x_0)^2}{\\sigma_r^2}\\right) \\delta x_i = \\frac{H_{ii}(x_0) d_i(x_0)}{\\sigma_r^2}\n$$\n这给出了增量向量每个分量的闭式解：\n$$\n\\delta x_i = \\frac{H_{ii}(x_0) \\, d_i(x_0)/\\sigma_r^2}{1/\\sigma_b^2 + H_{ii}^2(x_0)/\\sigma_r^2}\n$$\n其中 $H_{ii}(x_0) = 1 + 3\\gamma (x_0)_i^2$ 且 $d_i(x_0) = y_i - h((x_0)_i)$。\n\n对于由参数 $(\\gamma, \\alpha, \\beta, \\sigma_b, \\sigma_r)$ 定义的每个测试用例，步骤如下：\n\n**1. 初始化**\n首先，我们为给定的测试用例建立固定的和参数定义的量。\n-   真实状态为 $x_{\\text{true}} = [1.5, -1.0, 0.5, -0.8, 1.2]^\\top$。\n-   背景状态计算为 $x_b = x_{\\text{true}} + \\alpha \\cdot p$，使用 $p = [0.7, -0.7, 0.3, -0.3, 0.5]^\\top$。\n-   观测算子为 $h(x)_i = x_i + \\gamma x_i^3$。\n-   观测生成为 $y = h(x_{\\text{true}}) + \\beta \\cdot q$，使用 $q = [0.1, -0.05, 0.03, -0.02, 0.0]^\\top$。\n\n**2. 单循环分析 ($x^{(1)}$)**\n第一个外循环使用背景状态 $x_b$ 作为线性化点。\n-   设置线性化点：$x_0 = x_b$。\n-   计算新息向量：$d(x_b) = y - h(x_b)$。\n-   计算雅可比矩阵的对角元素：$H_{ii}(x_b) = 1 + 3\\gamma (x_b)_i^2$。\n-   使用上面的逐分量公式计算第一个增量向量 $\\delta x^{(1)} = \\delta x(x_b)$。\n-   单循环分析即为 $x^{(1)} = x_b + \\delta x^{(1)}$。\n\n**3. 双循环分析 ($x^{(2)}$)**\n第二个外循环将线性化点更新为第一个循环的结果 $x^{(1)}$。\n-   设置新的线性化点：$x_0' = x^{(1)}$。\n-   计算新的新息向量：$d(x^{(1)}) = y - h(x^{(1)})$。\n-   计算新的雅可比矩阵对角元素：$H_{ii}(x^{(1)}) = 1 + 3\\gamma (x^{(1)})_i^2$。\n-   使用相同的逐分量公式，但使用更新后的输入，计算第二个增量向量 $\\delta x^{(2)} = \\delta x(x^{(1)})$。\n-   双循环分析即为 $x^{(2)} = x^{(1)} + \\delta x^{(2)}$。\n\n**4. 指标计算**\n计算出分析状态 $x^{(1)}$ 和 $x^{(2)}$ 后，我们计算五个所需的指标。这些指标量化了分析的准确性以及非线性的影响。\n-   $m_1 = \\|x^{(1)} - x_{\\text{true}}\\|_2$：单循环分析误差的欧几里得范数。\n-   $m_2 = \\|x^{(2)} - x_{\\text{true}}\\|_2$：双循环分析误差的欧几里得范数。\n-   $m_3 = m_1 - m_2$：从单循环到双循环分析误差范数的改进量。正值表示第二次循环改进了分析。\n-   $m_4 = E_{\\text{lin}}^{(1)} = \\big\\| h(x^{(1)}) - \\big(h(x_b) + H(x_b)(x^{(1)} - x_b)\\big) \\big\\|_2$：该指标衡量在 $x^{(1)}$ 处评估时，$h(x)$ 在 $x_b$ 附近的一阶泰勒近似的误差。它指示了非线性对第一步分析的影响程度。\n-   $m_5 = \\|y - h(x^{(2)})\\|_2$：第二次循环后最终新息（或观测减预报偏差）的欧几里得范数。这衡量了最终分析在观测空间中对观测的拟合程度。\n\n整个过程是确定性的，将以计算方式实现。实现的一个关键点是，由于所有操作都是逐分量的，它们可以利用像 NumPy 这样的库中的向量化计算来高效处理。这避免了对向量分量的显式循环。对于四个测试用例中的每一个，将执行这一系列计算以生成包含五个指标的列表。最终结果是这些列表的聚合。\n对于 $\\gamma = 0$ 的特殊情况，观测算子 $h(x) = x$ 是线性的。因此，其雅可比矩阵 $H(x) = I$ 是单位矩阵，与线性化点无关。增量代价函数 $\\tilde{J}$ 变得与完整的代价函数 $J$ 相同，后者是二次的。单个最小化步骤足以找到全局最小值。因此，第二个增量 $\\delta x^{(2)}$ 将为零，导致 $x^{(2)} = x^{(1)}$ 且改进指标 $m_3 = 0$。线性化误差 $m_4$ 也将为零，因为线性函数可以被其一阶泰勒级数完美表示。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the incremental 3D-Var problem for the given test cases.\n    \"\"\"\n    \n    # Define the fixed vectors and test cases from the problem statement.\n    x_true = np.array([1.5, -1.0, 0.5, -0.8, 1.2])\n    p = np.array([0.7, -0.7, 0.3, -0.3, 0.5])\n    q = np.array([0.1, -0.05, 0.03, -0.02, 0.0])\n\n    test_cases = [\n        # (gamma, alpha, beta, sigma_b, sigma_r)\n        (0.8, 1.0, 1.0, 0.5, 0.2), # Case 1\n        (0.0, 1.0, 1.0, 0.5, 0.2), # Case 2\n        (2.0, 2.0, 0.5, 0.8, 0.1), # Case 3\n        (1.5, 0.3, 0.0, 0.4, 0.05), # Case 4\n    ]\n\n    all_results = []\n\n    # Define the component-wise nonlinear observation operator and its Jacobian\n    def h(x, gamma):\n        return x + gamma * x**3\n\n    def H_diag(x, gamma):\n        return 1.0 + 3.0 * gamma * x**2\n\n    def calculate_increment(x0, y, gamma, sb, sr):\n        \"\"\"\n        Calculates the analysis increment delta_x at a linearization point x0.\n        \"\"\"\n        d = y - h(x0, gamma)\n        H_ii = H_diag(x0, gamma)\n        \n        var_b = sb**2\n        var_r = sr**2\n        \n        numerator = (H_ii * d) / var_r\n        denominator = 1.0 / var_b + (H_ii**2) / var_r\n        \n        # Handle potential division by zero if denominator is zero, though unlikely here\n        # given the problem constraints (sb > 0, sr > 0).\n        delta_x = numerator / denominator\n        return delta_x\n\n    for case in test_cases:\n        gamma, alpha, beta, sigma_b, sigma_r = case\n\n        # --- 1. Initialization ---\n        xb = x_true + alpha * p\n        y = h(x_true, gamma) + beta * q\n        \n        # --- 2. One-Loop Analysis (x^{(1)}) ---\n        # Linearization point is xb\n        dx1 = calculate_increment(xb, y, gamma, sigma_b, sigma_r)\n        x1 = xb + dx1\n\n        # --- 3. Two-Loop Analysis (x^{(2)}) ---\n        # Linearization point is x1\n        dx2 = calculate_increment(x1, y, gamma, sigma_b, sigma_r)\n        x2 = x1 + dx2\n        \n        # --- 4. Metric Computation ---\n        # Metric 1: ||x^(1) - x_true||_2\n        m1 = np.linalg.norm(x1 - x_true)\n\n        # Metric 2: ||x^(2) - x_true||_2\n        m2 = np.linalg.norm(x2 - x_true)\n\n        # Metric 3: Improvement ||x^(1) - x_true||_2 - ||x^(2) - x_true||_2\n        m3 = m1 - m2\n        \n        # Metric 4: Linearization error E_lin^(1)\n        # E_lin^(1) = || h(x^(1)) - (h(x_b) + H(x_b)(x^(1) - x_b)) ||_2\n        # Note that x^(1) - x_b is just dx1\n        linear_approx_at_x1 = h(xb, gamma) + H_diag(xb, gamma) * dx1\n        m4 = np.linalg.norm(h(x1, gamma) - linear_approx_at_x1)\n\n        # Metric 5: Final innovation norm ||y - h(x^(2))||_2\n        m5 = np.linalg.norm(y - h(x2, gamma))\n        \n        case_results = [m1, m2, m3, m4, m5]\n        all_results.append(case_results)\n\n    # Format the final output string as specified.\n    # The output string should be a list of lists, e.g., [[m11,...,m15],[m21,...,m25],...]\n    result_str = \"[\" + \", \".join([str(res) for res in all_results]) + \"]\"\n\n    # Printing must be exactly in the required format.\n    print(result_str.replace(\" \", \"\"))\n\n\nsolve()\n```"
        }
    ]
}