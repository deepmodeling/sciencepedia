{
    "hands_on_practices": [
        {
            "introduction": "梯度下降法等优化算法通过迭代更新来寻找代价函数的最小值。其中一个关键环节是确定每一步沿搜索方向应前进多远，即步长（step length）的选择。本练习将通过一个具体的3D-Var问题，让您手动执行Armijo回溯线搜索，计算满足充分下降条件的步长，从而加深对迭代优化核心机制的理解。",
            "id": "4063601",
            "problem": "在用于数值天气预报 (NWP) 的三维变分同化 (3D-Var) 中，分析状态向量 $\\mathbf{x} \\in \\mathbb{R}^{3}$ 最小化二次代价函数\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})^{\\top} \\mathbf{R}^{-1} (\\mathbf{H}\\mathbf{x} - \\mathbf{y}),\n$$\n其中 $\\mathbf{B}$ 是背景误差协方差，$\\mathbf{R}$ 是观测误差协方差，$\\mathbf{H}$ 是线性观测算子。$J$ 相对于 $\\mathbf{x}$ 的梯度可由二次型和线性映射的基本微积分法则得出。\n\n考虑一个维度为三的特定同化实例，其中矩阵和向量为\n$$\n\\mathbf{H} = \\mathbf{I}_{3}, \\quad \\mathbf{B}^{-1} = \\mathrm{diag}(1,\\,2,\\,3), \\quad \\mathbf{R}^{-1} = \\mathrm{diag}(4,\\,1,\\,2),\n$$\n$$\n\\mathbf{x}_b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\n设搜索方向为最速下降方向，$\\mathbf{p}_k = -\\nabla J(\\mathbf{x}_k)$，并执行 Armijo 回溯线搜索，初始步长为 $\\alpha_0 = 1$，缩减因子为 $\\beta = \\frac{1}{2}$，充分下降参数为 $c = 0.2$。需要满足的 Armijo 充分下降条件是\n$$\nJ(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq J(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k.\n$$\n\n从 $\\alpha = \\alpha_0$ 开始，通过设置 $\\alpha \\leftarrow \\beta \\alpha$ 进行回溯，直到满足上述不等式。计算满足 Armijo 条件的接受步长 $\\alpha_k$。将最终答案表示为一个实数。无需四舍五入。",
            "solution": "首先验证问题，以确保其具有科学依据、适定且客观。问题陈述描述了一个在三维变分数据同化 (3D-Var) 中使用二次代价函数的标准最小化问题。所有矩阵、向量和算法参数都已明确给出。矩阵 $\\mathbf{B}^{-1}$ 和 $\\mathbf{R}^{-1}$ 是对角矩阵，且对角线元素为正，确保它们代表有效的逆协方差矩阵（即，它们是正定的）。优化任务涉及应用定义明确的 Armijo 回溯线搜索和最速下降方向。该问题的设置是完整的、一致的且科学合理的。该问题被认定为有效。\n\n代价函数由下式给出\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})^{\\top} \\mathbf{R}^{-1} (\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\n代价函数相对于 $\\mathbf{x}$ 的梯度为\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\n我们已知以下信息：\n$$\n\\mathbf{H} = \\mathbf{I}_{3}, \\quad \\mathbf{B}^{-1} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad \\mathbf{R}^{-1} = \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix}\n$$\n$$\n\\mathbf{x}_b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\n将 $\\mathbf{x}_b = \\mathbf{0}$ 和 $\\mathbf{H} = \\mathbf{I}_{3}$ 代入梯度表达式，简化为：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}\\mathbf{x} + \\mathbf{R}^{-1}(\\mathbf{x} - \\mathbf{y}) = (\\mathbf{B}^{-1} + \\mathbf{R}^{-1})\\mathbf{x} - \\mathbf{R}^{-1}\\mathbf{y}\n$$\n首先，我们计算所需的矩阵：\n$$\n\\mathbf{B}^{-1} + \\mathbf{R}^{-1} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{{pmatrix} + \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix} = \\begin{pmatrix} 5  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix}\n$$\n$$\n\\mathbf{R}^{-1}\\mathbf{y} = \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 6 \\end{pmatrix}\n$$\n因此，梯度为：\n$$\n\\nabla J(\\mathbf{x}) = \\begin{pmatrix} 5  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\mathbf{x} - \\begin{pmatrix} 4 \\\\ -2 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 5x_1 - 4 \\\\ 3x_2 + 2 \\\\ 5x_3 - 6 \\end{pmatrix}\n$$\n现在，我们在当前迭代点 $\\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$ 处计算梯度：\n$$\n\\nabla J(\\mathbf{x}_k) = \\begin{pmatrix} 5(1) - 4 \\\\ 3(-1) + 2 \\\\ 5(2) - 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix}\n$$\n搜索方向是最速下降方向，$\\mathbf{p}_k = -\\nabla J(\\mathbf{x}_k)$：\n$$\n\\mathbf{p}_k = - \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix}\n$$\nArmijo 充分下降条件是 $J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq J(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$。\n我们需要计算右侧的各项。首先是 $J(\\mathbf{x}_k)$：\n$$\n\\mathbf{x}_k - \\mathbf{x}_b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{H}\\mathbf{x}_k - \\mathbf{y} = \\mathbf{I}_3 \\mathbf{x}_k - \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0  1  -1 \\end{pmatrix} \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} (1(1)^2 + 2(-1)^2 + 3(2)^2) + \\frac{1}{2} (4(0)^2 + 1(1)^2 + 2(-1)^2)\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} (1 + 2 + 12) + \\frac{1}{2} (0 + 1 + 2) = \\frac{15}{2} + \\frac{3}{2} = \\frac{18}{2} = 9\n$$\n接下来，我们计算方向导数项 $\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$：\n$$\n\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k = \\begin{pmatrix} 1  -1  4 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = (1)(-1) + (-1)(1) + (4)(-4) = -1 - 1 - 16 = -18\n$$\n当 $c = 0.2$ 时，Armijo 条件变为：\n$$\nJ(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq 9 + (0.2)\\alpha(-18) \\implies J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq 9 - 3.6\\alpha\n$$\n为了计算左侧，我们定义新点 $\\mathbf{x}_{new}(\\alpha) = \\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k$：\n$$\n\\mathbf{x}_{new}(\\alpha) = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} + \\alpha \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha \\\\ \\alpha-1 \\\\ 2-4\\alpha \\end{pmatrix}\n$$\n让我们定义一个函数 $\\phi(\\alpha) = J(\\mathbf{x}_{new}(\\alpha))$。由于 $J$ 是 $\\mathbf{x}$ 的二次函数，而 $\\mathbf{x}_{new}$ 是 $\\alpha$ 的线性函数，因此 $\\phi(\\alpha)$ 必定是 $\\alpha$ 的二次函数。我们可以将 $\\phi(\\alpha)$ 写成 $\\phi(\\alpha) = A\\alpha^2 + B\\alpha + C$。我们知道 $\\phi(0) = J(\\mathbf{x}_k) = 9$，所以 $C=9$。我们还知道 $\\phi'(0) = \\nabla J(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k = -18$，所以 $B=-18$。因此 $\\phi(\\alpha)$ 的表达式为 $\\phi(\\alpha) = A\\alpha^2 - 18\\alpha + 9$。$J$ 的 Hessian 矩阵是 $\\nabla^2 J(\\mathbf{x}) = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H} = \\mathbf{B}^{-1} + \\mathbf{R}^{-1}$。系数 $A$ 由 $A = \\frac{1}{2}\\mathbf{p}_k^{\\top}(\\mathbf{B}^{-1}+\\mathbf{R}^{-1})\\mathbf{p}_k$ 给出。\n$$\nA = \\frac{1}{2}\\begin{pmatrix} -1  1  -4 \\end{pmatrix} \\begin{pmatrix} 5  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -5  3  -20 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix}\n$$\n$$\nA = \\frac{1}{2}((-5)(-1) + (3)(1) + (-20)(-4)) = \\frac{1}{2}(5 + 3 + 80) = \\frac{88}{2} = 44\n$$\n所以，$J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) = 44\\alpha^2 - 18\\alpha + 9$。\nArmijo 不等式为：\n$$\n44\\alpha^2 - 18\\alpha + 9 \\leq 9 - 3.6\\alpha\n$$\n$$\n44\\alpha^2 - 18\\alpha \\leq -3.6\\alpha\n$$\n$$\n44\\alpha^2 - 14.4\\alpha \\leq 0\n$$\n因为我们在寻找 $\\alpha > 0$，我们可以用 $\\alpha$ 除以两边：\n$$\n44\\alpha - 14.4 \\leq 0 \\implies 44\\alpha \\leq 14.4 \\implies \\alpha \\leq \\frac{14.4}{44} = \\frac{144}{440} = \\frac{72}{220} = \\frac{36}{110} = \\frac{18}{55}\n$$\n回溯线搜索从 $\\alpha = \\alpha_0 = 1$ 开始，每次将其乘以因子 $\\beta = \\frac{1}{2}$ 进行缩减，直到满足条件 $\\alpha \\leq \\frac{18}{55}$。\n1.  尝试 $\\alpha = 1$：$1 \\leq \\frac{18}{55}$ 是否成立？不成立，因为 $1 = \\frac{55}{55}$。条件不满足。回溯。\n2.  尝试 $\\alpha = \\frac{1}{2}$：$\\frac{1}{2} \\leq \\frac{18}{55}$ 是否成立？这等价于 $55 \\leq 36$，这是错误的。条件不满足。回溯。\n3.  尝试 $\\alpha = \\frac{1}{4}$：$\\frac{1}{4} \\leq \\frac{18}{55}$ 是否成立？这等价于 $55 \\leq 72$，这是正确的。条件满足。\n\n接受的步长 $\\alpha_k$ 是序列 $1, \\frac{1}{2}, \\frac{1}{4}, \\dots$ 中第一个满足该条件的值。该值为 $\\frac{1}{4}$。",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "最速下降法虽然概念简单，但在许多实际问题中收敛速度较慢。L-BFGS等拟牛顿法通过近似代价函数的曲率信息（二阶导数或Hessian矩阵）来构造更优的搜索方向，从而显著提升收敛效率。本练习将引导您完成L-BFGS算法核心的双循环递归（two-loop recursion）计算，揭示该算法如何仅利用最近的梯度信息来高效地生成搜索方向，这对于理解现代大规模数据同化系统至关重要。",
            "id": "4063702",
            "problem": "在用于数值天气预报的强约束变分资料同化中，需要最小化一个二次可微的代价函数 $J(x)$。该函数的梯度 $\\nabla J(x)$ 是可用的，但其Hessian矩阵 $\\nabla^{2}J(x)$ 不会被显式地构造。一种常见的策略是使用拟牛顿法，该方法利用一阶信息构建逆Hessian矩阵的近似，并生成下降方向。考虑将有限内存的 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 方法应用于一个三维控制向量 $x \\in \\mathbb{R}^{3}$，该向量代表三个聚合状态分量的偏差订正后的分析增量。在当前迭代点 $x_{k}$，梯度为\n$$\ng_{k} \\equiv \\nabla J(x_{k}) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\n从最近两次迭代中，我们得到曲率（割线）对\n$$\ns_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\qquad\ns_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix},\n$$\n按从旧 $(i=1)$ 到新 $(i=2)$ 的顺序排列，并满足曲率条件 $y_{i}^{\\top} s_{i} > 0$。使用L-BFGS的标准双循环递归，其中初始逆Hessian矩阵的缩放选择为单位矩阵的标量倍，$H_{0} = \\gamma I$，而 $\\gamma$ 由最新的曲率对设置为\n$$\n\\gamma = \\frac{s_{2}^{\\top} y_{2}}{y_{2}^{\\top} y_{2}},\n$$\n计算L-BFGS的搜索方向\n$$\np_{k} = - H_{k} g_{k},\n$$\n其中 $H_{k}$ 是L-BFGS的逆Hessian矩阵近似，由应用于 $g_{k}$ 和给定的 $(s_{i}, y_{i})$ 的双循环递归隐式定义。\n\n以行向量形式给出 $p_{k}$ 的精确有理数向量。不要四舍五入。",
            "solution": "用户要求针对给定的梯度和更新历史，计算有限内存的 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 搜索方向 $p_k$。计算使用标准的双循环递归算法进行。\n\n问题给出了以下已知条件：\n控制向量在 $\\mathbb{R}^{3}$ 中。\n当前迭代点 $x_k$ 处的梯度是 $g_k = \\nabla J(x_k) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$。\n最近的两个更新对（内存 $m=2$），按从旧到新的顺序排列，是：\n$s_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$, $y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}$\n$s_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$, $y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix}$\n\n搜索方向由 $p_k = -H_k g_k$ 给出，其中 $H_k$ 是L-BFGS对逆Hessian矩阵的近似。乘积 $H_k g_k$ 使用双循环递归隐式计算。\n\n首先，我们计算标量值 $\\rho_i = \\frac{1}{y_i^\\top s_i}$，其中 $i=1, 2$。\n对于 $i=1$：\n$$y_1^\\top s_1 = \\begin{pmatrix} 2  0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = (2)(1) + (0)(0) + (-1)(-1) = 3$$\n$$\\rho_1 = \\frac{1}{3}$$\n对于 $i=2$：\n$$y_2^\\top s_2 = \\begin{pmatrix} 0  3  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} = (0)(0) + (3)(2) + (2)(1) = 8$$\n$$\\rho_2 = \\frac{1}{8}$$\n曲率条件 $y_i^\\top s_i > 0$ 得到满足。\n\n接下来，我们计算初始逆Hessian矩阵近似 $H_0 = \\gamma I$ 的缩放因子 $\\gamma$。公式为 $\\gamma = \\frac{s_2^\\top y_2}{y_2^\\top y_2}$。\n我们有 $s_2^\\top y_2 = y_2^\\top s_2 = 8$。\n分母是：\n$$y_2^\\top y_2 = \\begin{pmatrix} 0  3  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = (0)^{2} + (3)^{2} + (2)^{2} = 9 + 4 = 13$$\n因此，缩放因子为：\n$$\\gamma = \\frac{8}{13}$$\n\n现在，我们应用L-BFGS双循环递归。\n\n**循环 1 (反向传递):**\n这个循环计算向量 $q$ 和标量 $\\alpha_i$，其中 $i=m, \\dots, 1$。这里 $m=2$。\n初始化 $q = g_k = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$。\n\n对于 $i=2$：\n$$\\alpha_2 = \\rho_2 s_2^\\top q = \\frac{1}{8} \\begin{pmatrix} 0  2  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{8} (0 - 2 + 2) = 0$$\n$$q \\leftarrow q - \\alpha_2 y_2 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\n\n对于 $i=1$：\n$$\\alpha_1 = \\rho_1 s_1^\\top q = \\frac{1}{3} \\begin{pmatrix} 1  0  -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{3} (3 - 2) = \\frac{1}{3}$$\n$$q \\leftarrow q - \\alpha_1 y_1 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 - \\frac{2}{3} \\\\ -1 \\\\ 2 + \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix}$$\n\n**初始近似缩放：**\n第一个循环的结果 $q$ 现在通过初始逆Hessian矩阵近似 $H_0 = \\gamma I$ 进行缩放。令结果向量为 $r$。\n$$r = H_0 q = \\gamma q = \\frac{8}{13} \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix}$$\n\n**循环 2 (正向传递):**\n这个循环从上一步的向量 $r$ 开始，并对 $i=1, \\dots, m$ 更新它。\n\n对于 $i=1$：\n我们计算标量 $\\beta_1$ 并更新 $r$。\n$$\\beta_1 = \\rho_1 y_1^\\top r = \\frac{1}{3} \\begin{pmatrix} 2  0  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} = \\frac{1}{3} \\left( 2 \\cdot \\frac{56}{39} - \\frac{56}{39} \\right) = \\frac{1}{3} \\cdot \\frac{56}{39} = \\frac{56}{117}$$\n现在使用 $r \\leftarrow r + s_1(\\alpha_1 - \\beta_1)$ 更新 $r$。我们已知 $\\alpha_1 = \\frac{1}{3}$。\n$$\\alpha_1 - \\beta_1 = \\frac{1}{3} - \\frac{56}{117} = \\frac{39}{117} - \\frac{56}{117} = -\\frac{17}{117}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\left( -\\frac{17}{117} \\right) = \\begin{pmatrix} \\frac{56}{39} - \\frac{17}{117} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} + \\frac{17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{168 - 17}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{168 + 17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix}$$\n\n对于 $i=2$：\n我们计算 $\\beta_2$ 并再次更新 $r$。我们已知 $\\alpha_2=0$。\n$$\\beta_2 = \\rho_2 y_2^\\top r = \\frac{1}{8} \\begin{pmatrix} 0  3  2 \\end{pmatrix} \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} = \\frac{1}{8 \\cdot 117} \\left( 3(-72) + 2(185) \\right) = \\frac{-216 + 370}{936} = \\frac{154}{936}$$\n化简分数得到 $\\beta_2 = \\frac{77}{468}$。\n现在使用 $r \\leftarrow r + s_2(\\alpha_2 - \\beta_2)$ 更新 $r$。\n$$\\alpha_2 - \\beta_2 = 0 - \\frac{77}{468} = -\\frac{77}{468}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} \\left( -\\frac{77}{468} \\right) = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} - \\frac{154}{468} \\\\ \\frac{185}{117} - \\frac{77}{468} \\end{pmatrix}$$\n我们使用公分母 $468 = 4 \\cdot 117$。\n$$r_1 = \\frac{4 \\cdot 151}{468} = \\frac{604}{468} = \\frac{151}{117}$$\n$$r_2 = -\\frac{4 \\cdot 72}{468} - \\frac{154}{468} = \\frac{-288 - 154}{468} = \\frac{-442}{468} = -\\frac{221}{234}$$\n$$r_3 = \\frac{4 \\cdot 185}{468} - \\frac{77}{468} = \\frac{740 - 77}{468} = \\frac{663}{468} = \\frac{221}{156}$$\n所以递归结束时的最终向量 $r$ 是 $r = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{221}{234} \\\\ \\frac{221}{156} \\end{pmatrix}$。\n\nL-BFGS搜索方向是 $p_k = -r$。\n$$p_k = - \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{221}{234} \\\\ \\frac{221}{156} \\end{pmatrix} = \\begin{pmatrix} -\\frac{151}{117} \\\\ \\frac{221}{234} \\\\ -\\frac{221}{156} \\end{pmatrix}$$\n问题要求以行向量形式给出答案。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{151}{117} & \\frac{221}{234} & -\\frac{221}{156}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "变分系统的性能不仅取决于最小化算法的效率，也高度依赖于代价函数的数学构造。真实的观测数据常常包含与背景场或大多数观测显著偏离的“离群点”（outliers）。本练习通过一个包含离群观测的简化问题，让您对比标准二次惩罚与鲁棒的Huber惩罚，从而亲手验证和量化后者如何有效降低离群点的不良影响，得到更可靠、更符合物理实际的分析结果。",
            "id": "4063756",
            "problem": "在一个用于数值天气预报的一维变分资料同化问题中，考虑通过最小化一个由背景项和观测不符项组成的凸目标函数来估计单个格点的对流层低层温度状态 $x$（单位：开尔文）。背景（先验）状态为 $x_{b} = 290$，背景误差标准差为 $\\sigma_{b} = 1$。存在 2 个独立的观测，其观测算子为单位算子 $H(x) = x$：一个近乎一致的观测 $y_{1} = 291$，其误差标准差为 $\\sigma_{1} = 1$；以及一个离群观测 $y_{2} = 305$，其误差标准差为 $\\sigma_{2} = 1$。定义代价函数\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,\\frac{(x - x_{b})^{2}}{\\sigma_{b}^{2}} \\;+\\; \\sum_{i=1}^{2} \\rho\\!\\left(\\frac{y_{i} - x}{\\sigma_{i}}\\right),\n$$\n其中 $\\rho$ 是应用于标准化残差 $(y_{i} - x)/\\sigma_{i}$ 的观测不符惩罚。考虑 $\\rho$ 的两种选择：\n- 纯二次惩罚 $\\rho_{\\mathrm{Q}}(r) = \\frac{1}{2}\\,r^{2}$。\n- 阈值为 $\\kappa = 1.5$ 的 Huber 惩罚，\n$$\n\\rho_{\\mathrm{H}}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2}\\,r^{2}, & \\text{若 } |r| \\le \\kappa,\\\\[4pt]\n\\kappa\\,|r| - \\frac{1}{2}\\,\\kappa^{2}, & \\text{若 } |r| > \\kappa.\n\\end{cases}\n$$\n仅使用这些定义和凸最小化的标准微积分方法，计算当 $\\rho = \\rho_{\\mathrm{H}}$ 时 $J(x)$ 的唯一最小化子 $x_{\\mathrm{H}}$，以及当 $\\rho = \\rho_{\\mathrm{Q}}$ 时 $J(x)$ 的唯一最小化子 $x_{\\mathrm{Q}}$。设分析增量定义为分析与背景之差，$x - x_{b}$。报告 Huber 分析增量与二次分析增量之差的精确值，\n$$\n\\bigl(x_{\\mathrm{H}} - x_{b}\\bigr) - \\bigl(x_{\\mathrm{Q}} - x_{b}\\bigr),\n$$\n单位为开尔文。请将最终答案表示为精确值（不要四舍五入），单位为开尔文。",
            "solution": "用户希望计算在一个变分最小化问题中，使用两种不同的观测不符惩罚函数（标准二次惩罚和稳健的 Huber 惩罚）所得到的分析增量之差。\n\n首先，对问题进行验证。\n问题陈述提供了所有必要信息：状态变量 $x$、背景状态 $x_b = 290$、背景误差标准差 $\\sigma_b = 1$、两个观测 $y_1 = 291$ 和 $y_2 = 305$、它们对应的误差标准差 $\\sigma_1 = 1$ 和 $\\sigma_2 = 1$，以及总代价函数 $J(x)$ 和两种惩罚函数 $\\rho_{\\mathrm{Q}}(r)$ 和 $\\rho_{\\mathrm{H}}(r)$ 的显式数学形式，包括 Huber 阈值 $\\kappa = 1.5$。该问题是一个标准、良态的一维变分资料同化练习。两个代价函数都是一个严格凸函数与多个凸函数的和，因此它们都是严格凸的，并拥有唯一的最小化子。该问题具有科学依据，数学上一致且客观。因此，该问题被认为是有效的，可以推导出解。\n\n需要计算的量是 $(x_{\\mathrm{H}} - x_{b}) - (x_{\\mathrm{Q}} - x_{b})$，其中 $x_{\\mathrm{Q}}$ 和 $x_{\\mathrm{H}}$ 分别是二次惩罚和 Huber 惩罚下代价函数 $J(x)$ 的唯一最小化子。\n\n**第 1 部分：使用二次惩罚（$\\rho_{\\mathrm{Q}}$）进行最小化**\n\n使用二次惩罚 $\\rho_{\\mathrm{Q}}(r) = \\frac{1}{2}r^{2}$ 的代价函数记为 $J_{\\mathrm{Q}}(x)$。\n$$\nJ_{\\mathrm{Q}}(x) = \\frac{1}{2}\\frac{(x - x_{b})^{2}}{\\sigma_{b}^{2}} + \\sum_{i=1}^{2} \\frac{1}{2}\\left(\\frac{y_{i} - x}{\\sigma_{i}}\\right)^{2}\n$$\n代入给定值 $\\sigma_b = 1$、$\\sigma_1 = 1$ 和 $\\sigma_2 = 1$：\n$$\nJ_{\\mathrm{Q}}(x) = \\frac{1}{2}(x - x_{b})^{2} + \\frac{1}{2}(y_{1} - x)^{2} + \\frac{1}{2}(y_{2} - x)^{2}\n$$\n为了找到最小化子 $x_{\\mathrm{Q}}$，我们计算 $J_{\\mathrm{Q}}(x)$ 对 $x$ 的导数并将其设为零。\n$$\n\\frac{dJ_{\\mathrm{Q}}}{dx} = (x - x_{b}) - (y_{1} - x) - (y_{2} - x) = 3x - x_{b} - y_{1} - y_{2}\n$$\n将导数设为零，得到最优状态 $x_{\\mathrm{Q}}$：\n$$\n3x_{\\mathrm{Q}} - x_{b} - y_{1} - y_{2} = 0 \\implies x_{\\mathrm{Q}} = \\frac{x_{b} + y_{1} + y_{2}}{3}\n$$\n代入数值 $x_{b} = 290$、$y_{1} = 291$ 和 $y_{2} = 305$：\n$$\nx_{\\mathrm{Q}} = \\frac{290 + 291 + 305}{3} = \\frac{886}{3}\n$$\n二次惩罚情况下的分析增量为：\n$$\nx_{\\mathrm{Q}} - x_{b} = \\frac{886}{3} - 290 = \\frac{886}{3} - \\frac{870}{3} = \\frac{16}{3}\n$$\n\n**第 2 部分：使用 Huber 惩罚（$\\rho_{\\mathrm{H}}$）进行最小化**\n\n使用 Huber 惩罚的代价函数记为 $J_{\\mathrm{H}}(x)$。当 $\\sigma_i = 1$ 时，标准化残差就是 $r_i = y_i - x$。\n$$\nJ_{\\mathrm{H}}(x) = \\frac{1}{2}(x - x_b)^2 + \\rho_{\\mathrm{H}}(y_1 - x) + \\rho_{\\mathrm{H}}(y_2 - x)\n$$\n其中\n$$\n\\rho_{\\mathrm{H}}(r) =\n\\begin{cases}\n\\frac{1}{2}r^2  \\text{若 } |r| \\le \\kappa \\\\\n\\kappa|r| - \\frac{1}{2}\\kappa^2  \\text{若 } |r| > \\kappa\n\\end{cases}\n$$\n为了找到最小化子 $x_{\\mathrm{H}}$，我们对 $J_{\\mathrm{H}}(x)$ 求导。$\\rho_{\\mathrm{H}}(r)$ 的导数是影响函数 $\\psi_{\\mathrm{H}}(r)$：\n$$\n\\psi_{\\mathrm{H}}(r) = \\frac{d\\rho_{\\mathrm{H}}}{dr} =\n\\begin{cases}\nr  \\text{若 } |r| \\le \\kappa \\\\\n\\kappa\\,\\text{sgn}(r)  \\text{若 } |r| > \\kappa\n\\end{cases}\n$$\n使用链式法则，$\\frac{d}{dx}\\rho_{\\mathrm{H}}(y_i - x) = \\psi_{\\mathrm{H}}(y_i - x) \\cdot (-1) = -\\psi_{\\mathrm{H}}(y_i - x)$。代价函数的导数为：\n$$\n\\frac{dJ_{\\mathrm{H}}}{dx} = (x - x_b) - \\psi_{\\mathrm{H}}(y_1 - x) - \\psi_{\\mathrm{H}}(y_2 - x)\n$$\n在最小值 $x = x_{\\mathrm{H}}$ 处的优化条件是：\n$$\nx_{\\mathrm{H}} - x_b - \\psi_{\\mathrm{H}}(y_1 - x_{\\mathrm{H}}) - \\psi_{\\mathrm{H}}(y_2 - x_{\\mathrm{H}}) = 0\n$$\n这是一个关于 $x_{\\mathrm{H}}$ 的非线性方程。我们必须确定 Huber 函数的哪个区域适用于每个观测。数据点为 $x_b = 290$、$y_1 = 291$ 和 $y_2 = 305$。与 $x_b$ 和 $y_1$ 形成的簇相比，$y_2$ 是一个显著的离群值。Huber 惩罚的目的是减少此类离群值的影响。因此，可以合理地假设 $y_1$ 的残差会很小（在二次区域内），而 $y_2$ 的残差会很大（在线性区域内）。\n假设：\n1. $|y_1 - x_{\\mathrm{H}}| \\le \\kappa = 1.5$\n2. $|y_2 - x_{\\mathrm{H}}| > \\kappa = 1.5$\n\n在此假设下，$\\psi_{\\mathrm{H}}(y_1 - x_{\\mathrm{H}}) = y_1 - x_{\\mathrm{H}}$。对于第二项，我们预期 $x_{\\mathrm{H}}$ 接近 290-291，所以 $y_2 - x_{\\mathrm{H}} = 305 - x_{\\mathrm{H}}$ 将为正。因此，$\\text{sgn}(y_2 - x_{\\mathrm{H}}) = 1$ 且 $\\psi_{\\mathrm{H}}(y_2 - x_{\\mathrm{H}}) = \\kappa$。\n\n优化条件变为：\n$$\nx_{\\mathrm{H}} - x_b - (y_1 - x_{\\mathrm{H}}) - \\kappa = 0\n$$\n求解 $x_{\\mathrm{H}}$：\n$$\nx_{\\mathrm{H}} - x_b - y_1 + x_{\\mathrm{H}} - \\kappa = 0 \\\\\n2x_{\\mathrm{H}} = x_b + y_1 + \\kappa \\\\\nx_{\\mathrm{H}} = \\frac{x_b + y_1 + \\kappa}{2}\n$$\n代入数值 $x_b = 290$、$y_1 = 291$、$\\kappa = 1.5$：\n$$\nx_{\\mathrm{H}} = \\frac{290 + 291 + 1.5}{2} = \\frac{582.5}{2} = 291.25\n$$\n现在，我们必须使用这个 $x_{\\mathrm{H}}$ 的值来验证我们的假设：\n1. 检查 $|y_1 - x_{\\mathrm{H}}| \\le 1.5$：$|291 - 291.25| = |-0.25| = 0.25$。由于 $0.25 \\le 1.5$，这部分假设是有效的。\n2. 检查 $|y_2 - x_{\\mathrm{H}}| > 1.5$：$|305 - 291.25| = |13.75| = 13.75$。由于 $13.75 > 1.5$，这部分假设也是有效的。\n\n假设是一致的，因此 $x_{\\mathrm{H}} = 291.25$ 是正确的唯一最小化子。\nHuber 惩罚情况下的分析增量为：\n$$\nx_{\\mathrm{H}} - x_b = 291.25 - 290 = 1.25 = \\frac{5}{4}\n$$\n\n**第 3 部分：最终计算**\n\n所求的量是 Huber 分析增量与二次分析增量之差：\n$$\n(x_{\\mathrm{H}} - x_{b}) - (x_{\\mathrm{Q}} - x_{b}) = \\frac{5}{4} - \\frac{16}{3}\n$$\n为了进行分数减法，我们找到公分母，即 12：\n$$\n\\frac{5 \\times 3}{4 \\times 3} - \\frac{16 \\times 4}{3 \\times 4} = \\frac{15}{12} - \\frac{64}{12} = \\frac{15 - 64}{12} = -\\frac{49}{12}\n$$\n差值为负，这表明稳健的 Huber 惩罚正确地降低了离群值 $y_2$ 的影响，与标准二次惩罚相比，导致对背景状态的调整更小。",
            "answer": "$$\\boxed{-\\frac{49}{12}}$$"
        }
    ]
}