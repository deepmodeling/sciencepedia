{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Optimal Interpolation (OI), we must begin with its mathematical foundation as the Best Linear Unbiased Estimator (BLUE). This first practice guides you through the essential derivation of the OI analysis equations from first principles . By minimizing the analysis error variance, you will derive the optimal gain matrix $K$ and demonstrate a key result: the analysis is guaranteed to be more accurate (in a variance sense) than the background guess alone.",
            "id": "4070587",
            "problem": "Consider the linear, unbiased data assimilation setting used in Optimal Interpolation (OI), which is the Best Linear Unbiased Estimator (BLUE) under Gaussian, independent errors. Let the true state be $x \\in \\mathbb{R}^{n}$, the background (prior) be $x_{b} = x + e_{b}$ with background error $e_{b}$ satisfying $\\mathbb{E}[e_{b}] = 0$ and $\\operatorname{Cov}(e_{b}) = B \\in \\mathbb{R}^{n \\times n}$, and the observations be $y = H x + \\epsilon$ with linear observation operator $H \\in \\mathbb{R}^{m \\times n}$, observation error $\\epsilon$ satisfying $\\mathbb{E}[\\epsilon] = 0$ and $\\operatorname{Cov}(\\epsilon) = R \\in \\mathbb{R}^{m \\times m}$, and $\\operatorname{Cov}(e_{b}, \\epsilon) = 0$. The OI analysis is defined by the linear estimator $x_{a} = x_{b} + K \\left(y - H x_{b}\\right)$, where $K \\in \\mathbb{R}^{n \\times m}$ is a gain to be determined by minimizing analysis error variance subject to unbiasedness.\n\nStarting from these principles alone, do the following:\n\n- Derive the analysis error covariance matrix $A = \\operatorname{Cov}(x_{a} - x)$ in terms of $K$, $H$, $B$, and $R$, and determine the unique $K$ that minimizes the total analysis variance in the sense of minimizing $\\operatorname{tr}(A)$.\n- Using the optimal $K$ you derived, express $A$ as the difference between $B$ and a positive semidefinite matrix that depends on $H$, $B$, and $R$, and therefore demonstrate that each diagonal element of $A$ is less than or equal to the corresponding diagonal element of $B$; that is, the diagonal of $A$ gives analysis variance reductions relative to $B$.\n- For the scalar case where $x \\in \\mathbb{R}$, $H \\in \\mathbb{R}$, $B \\in \\mathbb{R}$, and $R \\in \\mathbb{R}$ with $H \\neq 0$, $B > 0$, and $R > 0$, compute the variance reduction factor $\\gamma = A / B$ as a closed-form expression in terms of $H$, $B$, and $R$.\n\nProvide the final answer as the single analytic expression for $\\gamma$. No numerical rounding is required.",
            "solution": "The problem as stated is a standard derivation in the field of data assimilation, specifically concerning Optimal Interpolation (OI) or the Best Linear Unbiased Estimator (BLUE). The givens are clearly defined, scientifically consistent, and the objectives are well-posed mathematical tasks. All provided terms and assumptions are standard in the literature of numerical weather prediction and climate modeling. Therefore, the problem is valid.\n\nWe begin by defining the analysis error, $e_a$, as the difference between the analysis state, $x_a$, and the true state, $x$:\n$$e_a = x_a - x$$\nThe analysis state $x_a$ is given by the linear estimator:\n$$x_a = x_b + K(y - Hx_b)$$\nwhere $K$ is the gain matrix, $x_b$ is the background state, $y$ is the observation vector, and $H$ is the observation operator. We are also given the relationships for the background and observations in terms of the true state $x$ and their respective errors, $e_b$ and $\\epsilon$:\n$$x_b = x + e_b$$\n$$y = Hx + \\epsilon$$\nThe errors are unbiased, i.e., $\\mathbb{E}[e_b] = 0$ and $\\mathbb{E}[\\epsilon] = 0$, and are uncorrelated, $\\operatorname{Cov}(e_b, \\epsilon) = \\mathbb{E}[e_b \\epsilon^T] = 0$.\n\nFirst, we express the analysis error $e_a$ in terms of the background error $e_b$ and the observation error $\\epsilon$:\n$$e_a = (x_b + K(y - Hx_b)) - x$$\nSubstituting the expressions for $x_b$ and $y$:\n$$e_a = (x + e_b) + K((Hx + \\epsilon) - H(x + e_b)) - x$$\n$$e_a = e_b + K(Hx + \\epsilon - Hx - He_b)$$\n$$e_a = e_b + K(\\epsilon - He_b)$$\n$$e_a = (I - KH)e_b + K\\epsilon$$\nwhere $I$ is the identity matrix of size $n \\times n$.\n\nThe analysis error covariance matrix $A$ is defined as $A = \\operatorname{Cov}(e_a) = \\mathbb{E}[e_a e_a^T]$. Using the expression for $e_a$, we have:\n$$A = \\mathbb{E}[((I - KH)e_b + K\\epsilon)((I - KH)e_b + K\\epsilon)^T]$$\n$$A = \\mathbb{E}[((I - KH)e_b + K\\epsilon)(e_b^T(I - KH)^T + \\epsilon^T K^T)]$$\nExpanding this expression gives four terms:\n$$A = \\mathbb{E}[(I - KH)e_b e_b^T (I - KH)^T] + \\mathbb{E}[(I - KH)e_b \\epsilon^T K^T] + \\mathbb{E}[K\\epsilon e_b^T (I - KH)^T] + \\mathbb{E}[K\\epsilon \\epsilon^T K^T]$$\nUsing the linearity of expectation and the given covariance information ($\\operatorname{Cov}(e_b) = \\mathbb{E}[e_b e_b^T] = B$, $\\operatorname{Cov}(\\epsilon) = \\mathbb{E}[\\epsilon \\epsilon^T] = R$, and $\\mathbb{E}[e_b \\epsilon^T] = 0$), the cross-terms vanish:\n$$\\mathbb{E}[(I - KH)e_b \\epsilon^T K^T] = (I - KH)\\mathbb{E}[e_b \\epsilon^T]K^T = 0$$\n$$\\mathbb{E}[K\\epsilon e_b^T (I - KH)^T] = K\\mathbb{E}[\\epsilon e_b^T](I - KH)^T = 0$$\nThe expression for $A$ simplifies to:\n$$A(K) = (I - KH) \\mathbb{E}[e_b e_b^T] (I - KH)^T + K \\mathbb{E}[\\epsilon \\epsilon^T] K^T$$\n$$A(K) = (I - KH)B(I - KH)^T + KRK^T$$\nThis is the analysis error covariance matrix in terms of $K$, $H$, $B$, and $R$.\n\nTo find the optimal gain $K$ that minimizes the total analysis variance, we minimize the trace of $A$, denoted $J(K) = \\operatorname{tr}(A(K))$. First, we expand the expression for $A(K)$:\n$$A(K) = (B - KHB)(I - H^T K^T) + KRK^T$$\n$$A(K) = B - KHB - BH^T K^T + KHBH^T K^T + KRK^T$$\n$$A(K) = B - KHB - BH^T K^T + K(HBH^T + R)K^T$$\nThe cost function is:\n$$J(K) = \\operatorname{tr}(B) - \\operatorname{tr}(KHB) - \\operatorname{tr}(BH^T K^T) + \\operatorname{tr}(K(HBH^T + R)K^T)$$\nUsing the cyclic property of the trace, $\\operatorname{tr}(BH^T K^T) = \\operatorname{tr}(K^T BH^T) = \\operatorname{tr}((KHB)^T) = \\operatorname{tr}(KHB)$. Thus:\n$$J(K) = \\operatorname{tr}(B) - 2\\operatorname{tr}(KHB) + \\operatorname{tr}(K(HBH^T + R)K^T)$$\nTo find the minimum, we compute the gradient of $J(K)$ with respect to $K$ and set it to zero. Using standard matrix calculus identities ($\\nabla_X \\operatorname{tr}(AXB) = A^T B^T$ and $\\nabla_X \\operatorname{tr}(AXA^T) = 2AX$ for symmetric $A$), we get:\n$$\\nabla_K J(K) = -2 (HB)^T + 2K(HBH^T + R)$$\nSince $B$ is a covariance matrix, it is symmetric ($B=B^T$). Also, $HBH^T+R$ is symmetric.\n$$\\nabla_K J(K) = -2BH^T + 2K(HBH^T + R)$$\nSetting the gradient to zero for optimality:\n$$-2BH^T + 2K(HBH^T + R) = 0$$\n$$K(HBH^T + R) = BH^T$$\nThe optimal gain matrix $K$ is therefore:\n$$K = BH^T (HBH^T + R)^{-1}$$\n\nNext, we express the analysis error covariance $A$ for this optimal $K$. It can be shown that for the optimal $K$, the expression for $A$ simplifies. Let's start from $A = B - KHB - BH^T K^T + K(HBH^T + R)K^T$. Substituting $K(HBH^T + R) = BH^T$:\n$$A = B - KHB - BH^T K^T + (BH^T)K^T$$\n$$A = B - KHB$$\nNow, substituting the expression for the optimal $K$:\n$$A = B - (BH^T (HBH^T + R)^{-1}) H B$$\nLet the matrix $M = BH^T (HBH^T + R)^{-1} H B$. Then $A = B - M$. To show that $M$ is positive semidefinite, we consider $z^T M z$ for an arbitrary vector $z \\in \\mathbb{R}^n$:\n$$z^T M z = z^T B H^T (HBH^T + R)^{-1} H B z$$\nSince $B$ is symmetric, we can write this as $(HBz)^T (HBH^T + R)^{-1} (HBz)$. Let $w = HBz \\in \\mathbb{R}^m$. The expression becomes $w^T (HBH^T + R)^{-1} w$. The matrix $B$ is positive semidefinite. The matrix $HBH^T$ is also positive semidefinite because for any vector $u \\in \\mathbb{R}^m$, $u^T(HBH^T)u = (H^Tu)^T B (H^Tu) \\ge 0$. The observation error covariance $R$ is assumed to be positive definite (as it must be invertible). The sum of a positive semidefinite matrix and a positive definite matrix is positive definite. Thus, $HBH^T + R$ is positive definite, and so is its inverse. Therefore, for any $w \\neq 0$, $w^T (HBH^T + R)^{-1} w > 0$, and for $w=0$, it is $0$. This means $z^T M z \\ge 0$ for all $z$, so $M$ is positive semidefinite.\n\nThe diagonal elements of $A$ and $B$, $A_{ii}$ and $B_{ii}$, represent the analysis and background variances for the $i$-th component of the state vector. We have $A = B - M$, so component-wise, $A_{ii} = B_{ii} - M_{ii}$. The diagonal elements of a positive semidefinite matrix are non-negative, so $M_{ii} \\ge 0$. This implies:\n$$A_{ii} \\le B_{ii}$$\nThis demonstrates that the analysis variance for each state variable is less than or equal to the background variance, which is the desired variance reduction.\n\nFinally, we consider the scalar case where $x, H, B, R$ are all scalar quantities, with $H \\neq 0$, $B > 0$, $R > 0$. The matrix equations become scalar equations. The optimal gain $K$ is:\n$$K = B H (H B H + R)^{-1} = \\frac{BH}{H^2 B + R}$$\nThe analysis error variance $A$ is given by $A = (1 - KH)B$:\n$$A = \\left(1 - \\left(\\frac{BH}{H^2 B + R}\\right) H\\right) B$$\n$$A = \\left(1 - \\frac{H^2 B}{H^2 B + R}\\right) B$$\n$$A = \\left(\\frac{H^2 B + R - H^2 B}{H^2 B + R}\\right) B$$\n$$A = \\frac{R}{H^2 B + R} B = \\frac{BR}{H^2 B + R}$$\nThe variance reduction factor $\\gamma$ is defined as the ratio $A / B$:\n$$\\gamma = \\frac{A}{B} = \\frac{1}{B} \\left(\\frac{BR}{H^2 B + R}\\right)$$\n$$\\gamma = \\frac{R}{H^2 B + R}$$\nThis is the closed-form expression for the variance reduction factor in the scalar case.",
            "answer": "$$\\boxed{\\frac{R}{H^{2} B + R}}$$"
        },
        {
            "introduction": "The optimality of the OI framework rests on knowing the true error statistics of the background ($B$) and observations ($R$), a condition rarely met in practice. This exercise explores the consequences of this imperfect knowledge by quantifying the degradation in analysis quality when misspecified error variances are used . Through a simple scalar example, you will compute the penalty for being 'suboptimal' and gain an appreciation for the sensitivity of data assimilation systems to their statistical inputs.",
            "id": "4070701",
            "problem": "Consider scalar Optimal Interpolation (OI) in the context of numerical weather prediction and climate modeling, with a single state variable $x$, a background (first guess) $x_b$, and a single observation $y$ measuring the same state component. Assume the linear observation model $y = x + \\varepsilon_o$ and the background model $x_b = x + \\varepsilon_b$. The background error $\\varepsilon_b$ and observation error $\\varepsilon_o$ are independent, zero-mean, and Gaussian with true variances $\\operatorname{Var}(\\varepsilon_b) = B_{\\text{true}}$ and $\\operatorname{Var}(\\varepsilon_o) = R_{\\text{true}}$. The analysis is formed as $x_a = x_b + K\\,(y - x_b)$, where $K$ is a scalar gain chosen by the analyst to minimize the expected squared analysis error using assumed error variances $B_{\\text{assumed}}$ and $R_{\\text{assumed}}$.\n\nStarting from these assumptions and the definitions of unbiasedness and variance, derive the bias and variance of the analysis $x_a$ as functions of $K$, $B_{\\text{true}}$, and $R_{\\text{true}}$. Then, obtain the gain $K$ that the analyst would choose based on the assumed variances $B_{\\text{assumed}}$ and $R_{\\text{assumed}}$, and use it to compute the actual analysis-error variance under the true error statistics. Also derive the optimal analysis-error variance that would result if the gain were chosen using the true error statistics.\n\nTo illustrate degraded optimality due to misspecification, consider a case where the true error variances are $B_{\\text{true}} = 4$ and $R_{\\text{true}} = 1$, but the analyst overestimates background error and underestimates observation error, using $B_{\\text{assumed}} = 9$ and $R_{\\text{assumed}} = 0.25$. Compute the ratio of the actual analysis-error variance (using the misspecified gain computed from $B_{\\text{assumed}}$ and $R_{\\text{assumed}}$) to the optimal analysis-error variance (using the gain computed from $B_{\\text{true}}$ and $R_{\\text{true}}$). Express your final ratio as a dimensionless number and round your answer to four significant figures.",
            "solution": "The problem is valid as it is a well-posed, scientifically grounded, and objective exercise in the theory of optimal interpolation, a fundamental concept in data assimilation. All necessary information is provided, and the premises are consistent with established principles.\n\nWe begin by formalizing the analysis error and its statistical properties. The state variable is denoted by $x$, the background estimate by $x_b$, and the observation by $y$. The background and observation models are given as:\n$$x_b = x + \\varepsilon_b$$\n$$y = x + \\varepsilon_o$$\nwhere $\\varepsilon_b$ and $\\varepsilon_o$ are the background and observation errors, respectively. These errors are assumed to be independent, zero-mean Gaussian random variables. Their true variances are $\\operatorname{Var}(\\varepsilon_b) = E[\\varepsilon_b^2] = B_{\\text{true}}$ and $\\operatorname{Var}(\\varepsilon_o) = E[\\varepsilon_o^2] = R_{\\text{true}}$.\n\nThe analysis, $x_a$, is formed by a linear combination of the background and the observation innovation ($y-x_b$):\n$$x_a = x_b + K(y - x_b)$$\nwhere $K$ is a scalar gain. We can rewrite this as:\n$$x_a = (1-K)x_b + Ky$$\n\nThe analysis error, $\\varepsilon_a$, is defined as the difference between the analysis and the true state:\n$$\\varepsilon_a = x_a - x$$\nSubstituting the expressions for $x_a$, $x_b$, and $y$:\n$$\\varepsilon_a = ((1-K)x_b + Ky) - x$$\n$$\\varepsilon_a = (1-K)(x + \\varepsilon_b) + K(x + \\varepsilon_o) - x$$\n$$\\varepsilon_a = (1-K)x + (1-K)\\varepsilon_b + Kx + K\\varepsilon_o - x$$\n$$\\varepsilon_a = (1-K+K-1)x + (1-K)\\varepsilon_b + K\\varepsilon_o$$\n$$\\varepsilon_a = (1-K)\\varepsilon_b + K\\varepsilon_o$$\n\nThe bias of the analysis is the expected value of the analysis error. Using the linearity of the expectation operator and the zero-mean property of the errors ($E[\\varepsilon_b]=0$, $E[\\varepsilon_o]=0$):\n$$E[\\varepsilon_a] = E[(1-K)\\varepsilon_b + K\\varepsilon_o] = (1-K)E[\\varepsilon_b] + KE[\\varepsilon_o] = (1-K)(0) + K(0) = 0$$\nThe analysis is unbiased for any choice of the scalar gain $K$.\n\nThe variance of the analysis error, denoted $A$, is given by $A = \\operatorname{Var}(\\varepsilon_a) = E[\\varepsilon_a^2] - (E[\\varepsilon_a])^2$. Since the bias is zero, this simplifies to $A = E[\\varepsilon_a^2]$. This variance is a function of the chosen gain $K$:\n$$A(K) = E[((1-K)\\varepsilon_b + K\\varepsilon_o)^2]$$\n$$A(K) = E[(1-K)^2\\varepsilon_b^2 + 2K(1-K)\\varepsilon_b\\varepsilon_o + K^2\\varepsilon_o^2]$$\nUsing the linearity of expectation and the independence of $\\varepsilon_b$ and $\\varepsilon_o$ (which implies $E[\\varepsilon_b\\varepsilon_o] = E[\\varepsilon_b]E[\\varepsilon_o] = 0$):\n$$A(K) = (1-K)^2 E[\\varepsilon_b^2] + 2K(1-K)E[\\varepsilon_b\\varepsilon_o] + K^2 E[\\varepsilon_o^2]$$\n$$A(K) = (1-K)^2 B_{\\text{true}} + K^2 R_{\\text{true}}$$\nThis equation gives the actual analysis-error variance for a given gain $K$ based on the true error statistics $B_{\\text{true}}$ and $R_{\\text{true}}$.\n\nThe analyst, however, does not know the true variances. They choose the gain $K$ by minimizing the *assumed* analysis-error variance, which is a function of their assumed variances, $B_{\\text{assumed}}$ and $R_{\\text{assumed}}$. Let this objective function be $J(K)$:\n$$J(K) = (1-K)^2 B_{\\text{assumed}} + K^2 R_{\\text{assumed}}$$\nTo find the gain that minimizes $J(K)$, we take the derivative with respect to $K$ and set it to zero:\n$$\\frac{dJ}{dK} = -2(1-K)B_{\\text{assumed}} + 2K R_{\\text{assumed}} = 0$$\n$$-2B_{\\text{assumed}} + 2KB_{\\text{assumed}} + 2KR_{\\text{assumed}} = 0$$\n$$2K(B_{\\text{assumed}} + R_{\\text{assumed}}) = 2B_{\\text{assumed}}$$\nThe gain chosen by the analyst, which we denote as $K_{\\text{subopt}}$ due to the misspecified variances, is:\n$$K_{\\text{subopt}} = \\frac{B_{\\text{assumed}}}{B_{\\text{assumed}} + R_{\\text{assumed}}}$$\n\nThe actual analysis-error variance resulting from using this suboptimal gain is found by substituting $K_{\\text{subopt}}$ into the equation for $A(K)$:\n$$A_{\\text{actual}} = (1 - K_{\\text{subopt}})^2 B_{\\text{true}} + (K_{\\text{subopt}})^2 R_{\\text{true}}$$\n$$A_{\\text{actual}} = \\left(1 - \\frac{B_{\\text{assumed}}}{B_{\\text{assumed}} + R_{\\text{assumed}}}\\right)^2 B_{\\text{true}} + \\left(\\frac{B_{\\text{assumed}}}{B_{\\text{assumed}} + R_{\\text{assumed}}}\\right)^2 R_{\\text{true}}$$\n$$A_{\\text{actual}} = \\left(\\frac{R_{\\text{assumed}}}{B_{\\text{assumed}} + R_{\\text{assumed}}}\\right)^2 B_{\\text{true}} + \\left(\\frac{B_{\\text{assumed}}}{B_{\\text{assumed}} + R_{\\text{assumed}}}\\right)^2 R_{\\text{true}}$$\n\nThe optimal analysis-error variance, $A_{\\text{opt}}$, is achieved if the gain is calculated using the true variances. The optimal gain, $K_{\\text{opt}}$, is:\n$$K_{\\text{opt}} = \\frac{B_{\\text{true}}}{B_{\\text{true}} + R_{\\text{true}}}$$\nThe resulting minimal variance is:\n$$A_{\\text{opt}} = (1-K_{\\text{opt}})^2 B_{\\text{true}} + (K_{\\text{opt}})^2 R_{\\text{true}}$$\n$$A_{\\text{opt}} = \\left(\\frac{R_{\\text{true}}}{B_{\\text{true}} + R_{\\text{true}}}\\right)^2 B_{\\text{true}} + \\left(\\frac{B_{\\text{true}}}{B_{\\text{true}} + R_{\\text{true}}}\\right)^2 R_{\\text{true}}$$\n$$A_{\\text{opt}} = \\frac{R_{\\text{true}}^2 B_{\\text{true}} + B_{\\text{true}}^2 R_{\\text{true}}}{(B_{\\text{true}} + R_{\\text{true}})^2} = \\frac{B_{\\text{true}} R_{\\text{true}} (R_{\\text{true}} + B_{\\text{true}})}{(B_{\\text{true}} + R_{\\text{true}})^2} = \\frac{B_{\\text{true}} R_{\\text{true}}}{B_{\\text{true}} + R_{\\text{true}}}$$\n\nNow we apply the specific numerical values provided: $B_{\\text{true}} = 4$, $R_{\\text{true}} = 1$, $B_{\\text{assumed}} = 9$, and $R_{\\text{assumed}} = 0.25$.\n\nFirst, we calculate the suboptimal gain chosen by the analyst:\n$$K_{\\text{subopt}} = \\frac{B_{\\text{assumed}}}{B_{\\text{assumed}} + R_{\\text{assumed}}} = \\frac{9}{9 + 0.25} = \\frac{9}{9.25} = \\frac{36}{37}$$\n\nNext, we calculate the actual analysis-error variance, $A_{\\text{actual}}$, using this gain:\n$$A_{\\text{actual}} = (1 - K_{\\text{subopt}})^2 B_{\\text{true}} + (K_{\\text{subopt}})^2 R_{\\text{true}}$$\n$$A_{\\text{actual}} = \\left(1 - \\frac{36}{37}\\right)^2 (4) + \\left(\\frac{36}{37}\\right)^2 (1) = \\left(\\frac{1}{37}\\right)^2 (4) + \\left(\\frac{36}{37}\\right)^2 (1)$$\n$$A_{\\text{actual}} = \\frac{4}{1369} + \\frac{1296}{1369} = \\frac{1300}{1369}$$\n\nThen, we calculate the optimal analysis-error variance, $A_{\\text{opt}}$, which would have been achieved with perfect knowledge of the error statistics:\n$$A_{\\text{opt}} = \\frac{B_{\\text{true}} R_{\\text{true}}}{B_{\\text{true}} + R_{\\text{true}}} = \\frac{4 \\times 1}{4 + 1} = \\frac{4}{5}$$\n\nFinally, we compute the ratio of the actual variance to the optimal variance:\n$$\\text{Ratio} = \\frac{A_{\\text{actual}}}{A_{\\text{opt}}} = \\frac{\\frac{1300}{1369}}{\\frac{4}{5}} = \\frac{1300}{1369} \\times \\frac{5}{4} = \\frac{325 \\times 5}{1369} = \\frac{1625}{1369}$$\nConverting this fraction to a decimal gives:\n$$\\text{Ratio} \\approx 1.1869978...$$\nRounding to four significant figures, the ratio is $1.187$. This value greater than $1$ quantifies the degradation in analysis quality due to the misspecification of the error variances.",
            "answer": "$$\\boxed{1.187}$$"
        },
        {
            "introduction": "Geophysical systems are inherently multivariate, with variables linked by physical laws. This advanced practice demonstrates how OI can leverage these relationships to create a physically consistent analysis . You will construct a background-error covariance matrix $B$ that explicitly includes geostrophic balance, and then calculate how a single pressure observation corrects not only the pressure field but also the related (and unobserved) streamfunction field.",
            "id": "4070656",
            "problem": "Consider a two-variable, two-location background-error covariance construction for objective analysis in numerical weather prediction and climate modeling, with geostrophic coupling between pressure and streamfunction. Let the model state at two grid points, a target grid point $G_0$ and an observation location $G_1$, be the four-component vector $\\mathbf{x} = \\big(p(G_0),\\, \\psi(G_0),\\, p(G_1),\\, \\psi(G_1)\\big)^{\\mathsf{T}}$, where $p$ is pressure and $\\psi$ is streamfunction. Assume the following physically based relationships and statistical structure as the starting point:\n\n- Geostrophic balance links geopotential $\\phi$ and streamfunction $\\psi$ via $\\phi = f\\,\\psi$, where $f$ is the Coriolis parameter. Pressure is $p = \\rho\\,\\phi$, so $p = \\gamma\\,\\psi$ with $\\gamma \\equiv \\rho f$.\n- Background errors are unbiased, second-order stationary, and homogeneous and isotropic in space. The background error covariance of the streamfunction is $\\operatorname{Cov}(\\psi(\\mathbf{r}_i), \\psi(\\mathbf{r}_j)) = \\sigma_{\\psi}^{2} C(r_{ij})$ with $C(r) = \\exp(-r/L)$, where $r_{ij}$ is the distance between grid points $i$ and $j$ and $L$ is the correlation length. The cross- and auto-covariances involving pressure follow from $p = \\gamma\\,\\psi$ by linearity.\n\nUsing these base assumptions, construct the $4 \\times 4$ background error covariance matrix $\\mathbf{B}$ for $\\mathbf{x}$ as a block matrix. Then, consider a single scalar pressure observation at $G_1$ with observation operator $\\mathbf{H} = \\big(0,\\, 0,\\, 1,\\, 0\\big)$ and unbiased observation error with variance $R = \\sigma_{o}^{2}$. Assume background and observation errors are uncorrelated.\n\nGiven the physically plausible parameter values:\n- $\\rho = 1.2\\,\\text{kg m}^{-3}$, $f = 1.0 \\times 10^{-4}\\,\\text{s}^{-1}$, so $\\gamma = \\rho f$,\n- $\\sigma_{\\psi} = 3.0 \\times 10^{6}\\,\\text{m}^{2}\\,\\text{s}^{-1}$,\n- $L = 3.0 \\times 10^{5}\\,\\text{m}$, distance between $G_0$ and $G_1$ is $r = 1.5 \\times 10^{5}\\,\\text{m}$,\n- observation-error standard deviation $\\sigma_{o} = 1.0 \\times 10^{2}\\,\\text{Pa}$,\n- a single observed pressure $y = 1.013 \\times 10^{5}\\,\\text{Pa}$ at $G_1$,\n- a background pressure at $G_1$ of $p_{b}(G_1) = 1.010 \\times 10^{5}\\,\\text{Pa}$,\n\nderive from first principles the optimal interpolation (OI) increment in pressure at $G_0$, that is, the analysis-minus-background increment $\\delta p(G_0)$ due to this single observation. Express your final numerical answer for $\\delta p(G_0)$ in pascals, and round your answer to four significant figures.",
            "solution": "The goal is to compute the analysis increment for pressure at grid point $G_0$, denoted $\\delta p(G_0)$, using the principles of optimal interpolation (OI).\n\nFirst, we construct the $4 \\times 4$ background error covariance matrix $\\mathbf{B} = \\operatorname{E}[(\\mathbf{x}-\\mathbf{x}_b)(\\mathbf{x}-\\mathbf{x}_b)^{\\mathsf{T}}]$, where $\\mathbf{x}_b$ is the background state. The components of the state vector are $x_1=p(G_0)$, $x_2=\\psi(G_0)$, $x_3=p(G_1)$, and $x_4=\\psi(G_1)$. The elements of $\\mathbf{B}$ are $B_{ij} = \\operatorname{Cov}(x_i, x_j)$.\n\nUsing the linear relationship $p = \\gamma \\psi$, we can express all covariances in terms of the streamfunction covariance:\n- $\\operatorname{Cov}(p_i, p_j) = \\operatorname{Cov}(\\gamma\\psi_i, \\gamma\\psi_j) = \\gamma^2 \\operatorname{Cov}(\\psi_i, \\psi_j) = \\gamma^2 \\sigma_{\\psi}^{2} C(r_{ij})$\n- $\\operatorname{Cov}(p_i, \\psi_j) = \\operatorname{Cov}(\\gamma\\psi_i, \\psi_j) = \\gamma \\operatorname{Cov}(\\psi_i, \\psi_j) = \\gamma \\sigma_{\\psi}^{2} C(r_{ij})$\n- $\\operatorname{Cov}(\\psi_i, \\psi_j) = \\sigma_{\\psi}^{2} C(r_{ij})$\n\nHere, indices $i,j \\in \\{0, 1\\}$ refer to grid points $G_0$ and $G_1$. The distances are $r_{00}=0$, $r_{11}=0$, and $r_{01}=r_{10}=r$. The correlation function at these distances gives $C(0)=\\exp(0)=1$ and $C(r)=\\exp(-r/L)$.\n\nLet's define a $2 \\times 2$ covariance block for a single point:\n$$\n\\mathbf{C}_{\\text{point}} = \\begin{pmatrix} \\operatorname{Cov}(p,p) & \\operatorname{Cov}(p,\\psi) \\\\ \\operatorname{Cov}(\\psi,p) & \\operatorname{Cov}(\\psi,\\psi) \\end{pmatrix} = \\begin{pmatrix} \\gamma^2 \\sigma_{\\psi}^2 & \\gamma \\sigma_{\\psi}^2 \\\\ \\gamma \\sigma_{\\psi}^2 & \\sigma_{\\psi}^2 \\end{pmatrix} = \\sigma_{\\psi}^2 \\begin{pmatrix} \\gamma^2 & \\gamma \\\\ \\gamma & 1 \\end{pmatrix}\n$$\nThe full $4 \\times 4$ matrix $\\mathbf{B}$ is a block matrix:\n$$\n\\mathbf{B} = \\begin{pmatrix} \\mathbf{B}_{00} & \\mathbf{B}_{01} \\\\ \\mathbf{B}_{10} & \\mathbf{B}_{11} \\end{pmatrix}\n$$\nwhere $\\mathbf{B}_{ij}$ is the covariance block between points $G_i$ and $G_j$.\n- $\\mathbf{B}_{00} = \\mathbf{C}_{\\text{point}} \\cdot C(r_{00}) = \\sigma_{\\psi}^2 \\begin{pmatrix} \\gamma^2 & \\gamma \\\\ \\gamma & 1 \\end{pmatrix}$\n- $\\mathbf{B}_{11} = \\mathbf{C}_{\\text{point}} \\cdot C(r_{11}) = \\sigma_{\\psi}^2 \\begin{pmatrix} \\gamma^2 & \\gamma \\\\ \\gamma & 1 \\end{pmatrix}$\n- $\\mathbf{B}_{01} = \\mathbf{B}_{10}^{\\mathsf{T}} = \\mathbf{C}_{\\text{point}} \\cdot C(r_{01}) = \\sigma_{\\psi}^2 \\exp(-r/L) \\begin{pmatrix} \\gamma^2 & \\gamma \\\\ \\gamma & 1 \\end{pmatrix}$\n\nSo, the full background error covariance matrix is:\n$$\n\\mathbf{B} = \\sigma_{\\psi}^2 \\begin{pmatrix}\n\\gamma^2 & \\gamma & \\gamma^2 \\exp(-r/L) & \\gamma \\exp(-r/L) \\\\\n\\gamma & 1 & \\gamma \\exp(-r/L) & \\exp(-r/L) \\\\\n\\gamma^2 \\exp(-r/L) & \\gamma \\exp(-r/L) & \\gamma^2 & \\gamma \\\\\n\\gamma \\exp(-r/L) & \\exp(-r/L) & \\gamma & 1\n\\end{pmatrix}\n$$\n\nThe analysis increment $\\delta \\mathbf{x} = \\mathbf{x}_a - \\mathbf{x}_b$ is given by the OI equation:\n$$\n\\delta \\mathbf{x} = \\mathbf{K} (y - \\mathbf{H}\\mathbf{x}_b)\n$$\nwhere $\\mathbf{K}$ is the analysis gain matrix. The term $d = y - \\mathbf{H}\\mathbf{x}_b$ is the innovation or departure. Given the background pressure at $G_1$, $p_b(G_1)$, we have $\\mathbf{H}\\mathbf{x}_b = p_b(G_1)$.\nThe gain matrix is defined as:\n$$\n\\mathbf{K} = \\mathbf{B}\\mathbf{H}^{\\mathsf{T}}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathsf{T}} + R)^{-1}\n$$\nWe compute the terms involving $\\mathbf{H} = \\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix}$:\n- $\\mathbf{H}^{\\mathsf{T}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$\n- $\\mathbf{B}\\mathbf{H}^{\\mathsf{T}}$ is the third column of $\\mathbf{B}$:\n  $$\n  \\mathbf{B}\\mathbf{H}^{\\mathsf{T}} = \\sigma_{\\psi}^2 \\begin{pmatrix} \\gamma^2 \\exp(-r/L) \\\\ \\gamma \\exp(-r/L) \\\\ \\gamma^2 \\\\ \\gamma \\end{pmatrix}\n  $$\n- $\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathsf{T}}$ is a scalar, which is the third element of the column vector $\\mathbf{B}\\mathbf{H}^{\\mathsf{T}}$:\n  $$\n  \\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathsf{T}} = \\sigma_{\\psi}^2 \\gamma^2\n  $$\n  This is the background error variance of pressure, $\\sigma_p^2 = \\operatorname{Var}(p_b(G_1))$.\n- The denominator term is a scalar: $\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\mathsf{T}} + R = \\sigma_{\\psi}^2 \\gamma^2 + \\sigma_o^2$.\n\nThe gain matrix $\\mathbf{K}$ is a $4 \\times 1$ column vector:\n$$\n\\mathbf{K} = \\frac{1}{\\sigma_{\\psi}^2 \\gamma^2 + \\sigma_o^2} \\mathbf{B}\\mathbf{H}^{\\mathsf{T}} = \\frac{\\sigma_{\\psi}^2}{\\sigma_{\\psi}^2 \\gamma^2 + \\sigma_o^2} \\begin{pmatrix} \\gamma^2 \\exp(-r/L) \\\\ \\gamma \\exp(-r/L) \\\\ \\gamma^2 \\\\ \\gamma \\end{pmatrix}\n$$\n\nThe analysis increment vector $\\delta \\mathbf{x}$ is:\n$$\n\\delta \\mathbf{x} = \\begin{pmatrix} \\delta p(G_0) \\\\ \\delta \\psi(G_0) \\\\ \\delta p(G_1) \\\\ \\delta \\psi(G_1) \\end{pmatrix} = \\frac{\\sigma_{\\psi}^2 (y - p_b(G_1))}{\\sigma_{\\psi}^2 \\gamma^2 + \\sigma_o^2} \\begin{pmatrix} \\gamma^2 \\exp(-r/L) \\\\ \\gamma \\exp(-r/L) \\\\ \\gamma^2 \\\\ \\gamma \\end{pmatrix}\n$$\n\nWe are interested in the first component, $\\delta p(G_0)$:\n$$\n\\delta p(G_0) = \\frac{\\sigma_{\\psi}^2 \\gamma^2 \\exp(-r/L)}{\\sigma_{\\psi}^2 \\gamma^2 + \\sigma_o^2} (y - p_b(G_1))\n$$\nThis can be interpreted as the ratio of the background error covariance between the analysis variable and the observed variable, to the total error variance at the observation location (background plus observation), multiplied by the innovation.\n\nNow we substitute the given numerical values.\n- $\\gamma = \\rho f = (1.2\\,\\text{kg m}^{-3})(1.0 \\times 10^{-4}\\,\\text{s}^{-1}) = 1.2 \\times 10^{-4}\\,\\text{kg m}^{-3}\\text{s}^{-1}$.\n- $\\sigma_{\\psi} = 3.0 \\times 10^{6}\\,\\text{m}^{2}\\,\\text{s}^{-1}$.\n- $r = 1.5 \\times 10^{5}\\,\\text{m}$ and $L = 3.0 \\times 10^{5}\\,\\text{m}$, so $r/L = 0.5$.\n- $\\exp(-r/L) = \\exp(-0.5)$.\n- $\\sigma_o = 1.0 \\times 10^2\\,\\text{Pa}$, so $R = \\sigma_o^2 = (1.0 \\times 10^2)^2\\,\\text{Pa}^2 = 1.0 \\times 10^4\\,\\text{Pa}^2$.\n- Innovation: $d = y - p_b(G_1) = (1.013 \\times 10^5 - 1.010 \\times 10^5)\\,\\text{Pa} = 300\\,\\text{Pa}$.\n\nCalculate the terms in the expression for $\\delta p(G_0)$:\n- Background pressure variance at $G_1$: $\\sigma_p^2 = \\sigma_{\\psi}^2 \\gamma^2 = (3.0 \\times 10^6)^2 (1.2 \\times 10^{-4})^2 = (9.0 \\times 10^{12}) (1.44 \\times 10^{-8}) = 12.96 \\times 10^4\\,\\text{Pa}^2$.\n- Background pressure covariance between $G_0$ and $G_1$: $\\operatorname{Cov}(p_0, p_1) = \\sigma_p^2 \\exp(-r/L) = (12.96 \\times 10^4) \\exp(-0.5)\\,\\text{Pa}^2$.\n- Total error variance at observation location: $\\sigma_p^2 + R = (12.96 \\times 10^4 + 1.0 \\times 10^4)\\,\\text{Pa}^2 = 13.96 \\times 10^4\\,\\text{Pa}^2$.\n\nNow, compute $\\delta p(G_0)$:\n$$\n\\delta p(G_0) = \\frac{12.96 \\times 10^4 \\exp(-0.5)}{13.96 \\times 10^4} \\times 300\\,\\text{Pa}\n$$\n$$\n\\delta p(G_0) = \\frac{12.96}{13.96} \\times \\exp(-0.5) \\times 300\\,\\text{Pa}\n$$\nUsing the values $\\exp(-0.5) \\approx 0.60653066$ and $12.96 / 13.96 \\approx 0.92836676$:\n$$\n\\delta p(G_0) \\approx (0.92836676) \\times (0.60653066) \\times 300\\,\\text{Pa}\n$$\n$$\n\\delta p(G_0) \\approx 0.56309606 \\times 300\\,\\text{Pa} \\approx 168.928819\\,\\text{Pa}\n$$\nRounding the result to four significant figures, we get:\n$$\n\\delta p(G_0) \\approx 168.9\\,\\text{Pa}\n$$\nThis positive increment means the analysis at $G_0$ corrects the pressure upward, influenced by the observation at $G_1$ which was higher than the background. The magnitude of the correction is attenuated by the distance between the points (via the correlation function) and by the relative certainty of the background versus the observation.",
            "answer": "$$\\boxed{168.9}$$"
        }
    ]
}