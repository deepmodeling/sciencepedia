{
    "hands_on_practices": [
        {
            "introduction": "The fundamental purpose of data assimilation is to combine information from a model forecast with new observations to produce a more accurate estimate of the state of a system. This exercise grounds this principle in Bayesian statistics, asking you to derive the posterior (analysis) error covariance from first principles. By working through this problem, you will mathematically demonstrate the core concept of uncertainty reduction, proving that the resulting analysis is statistically more certain than the background forecast alone .",
            "id": "4015013",
            "problem": "Consider a linear-Gaussian data assimilation setting for a two-dimensional state vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ in numerical weather prediction. Let the prior (background) distribution be Gaussian with mean $\\mathbf{x}_{b}$ and covariance matrix $\\mathbf{B}$, and assume the observation model is linear, $\\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon}$ is independent of $\\mathbf{x}$ and distributed as a Gaussian with zero mean and covariance matrix $\\mathbf{R}$. Using Bayes' theorem and the properties of multivariate Gaussian distributions, the posterior (analysis) distribution is also Gaussian with mean $\\mathbf{x}_{a}$ and covariance matrix $\\mathbf{A}$. Starting from these fundamental definitions and the independence of errors, derive the expression for the analysis covariance $\\mathbf{A}$ in terms of $\\mathbf{B}$, $\\mathbf{H}$, and $\\mathbf{R}$. Then, for the specific case\n$$\n\\mathbf{B} = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 2 \\end{pmatrix}, \\quad \\mathbf{H} = \\mathbf{I}, \\quad \\mathbf{R} = \\operatorname{diag}(0.25,\\,0.25),\n$$\ncompute the exact analysis covariance matrix $\\mathbf{A}$ and verify, by directly assessing the positive semidefiniteness of $\\mathbf{B}-\\mathbf{A}$, that $\\mathbf{A}$ is smaller than $\\mathbf{B}$ in the Loewner order. Express the final matrix entries exactly as rational numbers. No rounding is required.",
            "solution": "### Derivation of the Analysis Covariance $\\mathbf{A}$\n\nAccording to Bayes' theorem, the posterior probability density function $p(\\mathbf{x}|\\mathbf{y})$ is proportional to the product of the likelihood $p(\\mathbf{y}|\\mathbf{x})$ and the prior $p(\\mathbf{x})$:\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{x}) p(\\mathbf{x})$$\nThe prior distribution for the state $\\mathbf{x}$ is Gaussian:\n$$p(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^k |\\mathbf{B}|}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^T \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b)\\right)$$\nwhere $k$ is the dimension of the state vector.\n\nFrom the observation model $\\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$, the likelihood of observing $\\mathbf{y}$ given a state $\\mathbf{x}$ is the distribution of $\\mathbf{y}$, which is $\\mathcal{N}(\\mathbf{H}\\mathbf{x}, \\mathbf{R})$:\n$$p(\\mathbf{y}|\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^m |\\mathbf{R}|}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})\\right)$$\nwhere $m$ is the dimension of the observation vector.\n\nSubstituting these into Bayes' theorem and ignoring the normalization constants:\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^T \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b)\\right) \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})\\right)$$\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto \\exp\\left(-\\frac{1}{2}\\left[ (\\mathbf{x} - \\mathbf{x}_b)^T \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + (\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x}) \\right]\\right)$$\nThe expression inside the exponent is a cost function $J(\\mathbf{x})$. To find the covariance of the posterior distribution, we group the terms that are quadratic in $\\mathbf{x}$:\n$$J(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{B}^{-1} \\mathbf{x} - 2\\mathbf{x}_b^T \\mathbf{B}^{-1} \\mathbf{x} + \\mathbf{x}_b^T \\mathbf{B}^{-1} \\mathbf{x}_b + (\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})$$\nExpanding the observation term:\n$$(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x}) = \\mathbf{x}^T \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H} \\mathbf{x} - 2\\mathbf{y}^T \\mathbf{R}^{-1} \\mathbf{H} \\mathbf{x} + \\mathbf{y}^T \\mathbf{R}^{-1} \\mathbf{y}$$\nCombining all terms and isolating those involving $\\mathbf{x}$:\n$$J(\\mathbf{x}) = \\mathbf{x}^T (\\mathbf{B}^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H}) \\mathbf{x} - 2(\\mathbf{x}_b^T \\mathbf{B}^{-1} + \\mathbf{y}^T \\mathbf{R}^{-1} \\mathbf{H}) \\mathbf{x} + \\text{const}$$\nThe posterior distribution is given as Gaussian, $p(\\mathbf{x}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{x}; \\mathbf{x}_{a}, \\mathbf{A})$, which has the form:\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_a)^T \\mathbf{A}^{-1} (\\mathbf{x} - \\mathbf{x}_a)\\right)$$\nExpanding the exponent:\n$$(\\mathbf{x} - \\mathbf{x}_a)^T \\mathbf{A}^{-1} (\\mathbf{x} - \\mathbf{x}_a) = \\mathbf{x}^T \\mathbf{A}^{-1} \\mathbf{x} - 2\\mathbf{x}_a^T \\mathbf{A}^{-1} \\mathbf{x} + \\mathbf{x}_a^T \\mathbf{A}^{-1} \\mathbf{x}_a$$\nBy comparing the quadratic term in $\\mathbf{x}$ from the expansion of $J(\\mathbf{x})$ with the quadratic term from the posterior definition, we identify the inverse of the analysis covariance matrix $\\mathbf{A}$:\n$$\\mathbf{A}^{-1} = \\mathbf{B}^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H}$$\nTherefore, the analysis covariance matrix is:\n$$\\mathbf{A} = (\\mathbf{B}^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H})^{-1}$$\n\n### Computation for the Specific Case\n\nWe are given:\n$$\\mathbf{B} = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\end{pmatrix}$$\n$$\\mathbf{H} = \\mathbf{I} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\n$$\\mathbf{R} = \\begin{pmatrix} 0.25 & 0 \\\\ 0 & 0.25 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix}$$\n\nFirst, we compute the inverse matrices $\\mathbf{B}^{-1}$ and $\\mathbf{R}^{-1}$.\nFor $\\mathbf{B}$, the determinant is $\\det(\\mathbf{B}) = (1)(2) - (\\frac{1}{2})(\\frac{1}{2}) = 2 - \\frac{1}{4} = \\frac{7}{4}$.\n$$\\mathbf{B}^{-1} = \\frac{1}{\\det(\\mathbf{B})} \\begin{pmatrix} 2 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 1 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 2 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{7} & -\\frac{2}{7} \\\\ -\\frac{2}{7} & \\frac{4}{7} \\end{pmatrix}$$\nFor $\\mathbf{R}$, since it is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals:\n$$\\mathbf{R}^{-1} = \\begin{pmatrix} (\\frac{1}{4})^{-1} & 0 \\\\ 0 & (\\frac{1}{4})^{-1} \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix}$$\n\nNow we compute the inverse analysis covariance, $\\mathbf{A}^{-1}$. With $\\mathbf{H} = \\mathbf{I}$, $\\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H} = \\mathbf{R}^{-1}$.\n$$\\mathbf{A}^{-1} = \\mathbf{B}^{-1} + \\mathbf{R}^{-1} = \\begin{pmatrix} \\frac{8}{7} & -\\frac{2}{7} \\\\ -\\frac{2}{7} & \\frac{4}{7} \\end{pmatrix} + \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix}$$\n$$\\mathbf{A}^{-1} = \\begin{pmatrix} \\frac{8}{7} + \\frac{28}{7} & -\\frac{2}{7} \\\\ -\\frac{2}{7} & \\frac{4}{7} + \\frac{28}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{36}{7} & -\\frac{2}{7} \\\\ -\\frac{2}{7} & \\frac{32}{7} \\end{pmatrix}$$\n\nFinally, we compute $\\mathbf{A}$ by inverting $\\mathbf{A}^{-1}$. The determinant of $\\mathbf{A}^{-1}$ is:\n$$\\det(\\mathbf{A}^{-1}) = \\left(\\frac{36}{7}\\right)\\left(\\frac{32}{7}\\right) - \\left(-\\frac{2}{7}\\right)\\left(-\\frac{2}{7}\\right) = \\frac{1152}{49} - \\frac{4}{49} = \\frac{1148}{49}$$\nThe analysis covariance matrix $\\mathbf{A}$ is:\n$$\\mathbf{A} = (\\mathbf{A}^{-1})^{-1} = \\frac{1}{\\det(\\mathbf{A}^{-1})} \\begin{pmatrix} \\frac{32}{7} & \\frac{2}{7} \\\\ \\frac{2}{7} & \\frac{36}{7} \\end{pmatrix} = \\frac{49}{1148} \\begin{pmatrix} \\frac{32}{7} & \\frac{2}{7} \\\\ \\frac{2}{7} & \\frac{36}{7} \\end{pmatrix}$$\n$$\\mathbf{A} = \\frac{7}{1148} \\begin{pmatrix} 32 & 2 \\\\ 2 & 36 \\end{pmatrix} = \\frac{7}{1148} \\cdot 2 \\begin{pmatrix} 16 & 1 \\\\ 1 & 18 \\end{pmatrix} = \\frac{14}{1148} \\begin{pmatrix} 16 & 1 \\\\ 1 & 18 \\end{pmatrix}$$\nSimplifying the fraction $\\frac{14}{1148} = \\frac{7}{574}$. Since $574 = 7 \\times 82$, we have $\\frac{7}{574} = \\frac{1}{82}$.\n$$\\mathbf{A} = \\frac{1}{82} \\begin{pmatrix} 16 & 1 \\\\ 1 & 18 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{82} & \\frac{1}{82} \\\\ \\frac{1}{82} & \\frac{18}{82} \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{41} & \\frac{1}{82} \\\\ \\frac{1}{82} & \\frac{9}{41} \\end{pmatrix}$$\n\n### Verification of Uncertainty Reduction\n\nTo verify that $\\mathbf{A}$ is smaller than $\\mathbf{B}$ in the Loewner order (i.e., $\\mathbf{B} \\ge \\mathbf{A}$), we must show that the difference matrix $\\mathbf{D} = \\mathbf{B} - \\mathbf{A}$ is positive semidefinite.\n$$\\mathbf{B} = \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{82}{82} & \\frac{41}{82} \\\\ \\frac{41}{82} & \\frac{164}{82} \\end{pmatrix}$$\n$$\\mathbf{A} = \\begin{pmatrix} \\frac{16}{82} & \\frac{1}{82} \\\\ \\frac{1}{82} & \\frac{18}{82} \\end{pmatrix}$$\n$$\\mathbf{D} = \\mathbf{B} - \\mathbf{A} = \\begin{pmatrix} \\frac{82-16}{82} & \\frac{41-1}{82} \\\\ \\frac{41-1}{82} & \\frac{164-18}{82} \\end{pmatrix} = \\begin{pmatrix} \\frac{66}{82} & \\frac{40}{82} \\\\ \\frac{40}{82} & \\frac{146}{82} \\end{pmatrix} = \\frac{1}{41} \\begin{pmatrix} 33 & 20 \\\\ 20 & 73 \\end{pmatrix}$$\nA symmetric matrix is positive definite if and only if all its leading principal minors are positive.\nThe first leading principal minor is the top-left element:\n$$D_{11} = \\frac{33}{41} > 0$$\nThe second leading principal minor is the determinant of $\\mathbf{D}$:\n$$\\det(\\mathbf{D}) = \\det\\left(\\frac{1}{41} \\begin{pmatrix} 33 & 20 \\\\ 20 & 73 \\end{pmatrix}\\right) = \\left(\\frac{1}{41}\\right)^2 \\det \\begin{pmatrix} 33 & 20 \\\\ 20 & 73 \\end{pmatrix}$$\n$$\\det(\\mathbf{D}) = \\frac{1}{41^2} \\left( (33)(73) - (20)(20) \\right) = \\frac{1}{1681} (2409 - 400) = \\frac{2009}{1681} > 0$$\nSince all leading principal minors are positive, the matrix $\\mathbf{D} = \\mathbf{B} - \\mathbf{A}$ is positive definite. A positive definite matrix is, by definition, also positive semidefinite. Thus, we have verified that $\\mathbf{B} \\ge \\mathbf{A}$ in the Loewner order, which physically means that the data assimilation process has reduced the uncertainty in the state estimate.\n\nThe final answer is the computed matrix $\\mathbf{A}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{8}{41} & \\frac{1}{82} \\\\\n\\frac{1}{82} & \\frac{9}{41}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While knowing the analysis error covariance reveals the uncertainty of our final estimate, we also need the mechanism to compute the estimate itself. This practice focuses on deriving the Kalman gain vector, $K$, which provides the optimal weighting to blend the new observation with the background state. This exercise builds the machinery of the analysis update equation, illustrating how the background and observation error statistics, $B$ and $R$, directly control how information is assimilated .",
            "id": "4015086",
            "problem": "Consider a linear, Gaussian data assimilation setting typical of Numerical Weather Prediction (NWP) and climate model analysis, where a two-dimensional state vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ represents spatially averaged anomalies of a prognostic variable over two subdomains. The prior (background) state is modeled as $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}_{b}, \\mathbf{B})$, with background error covariance $\\mathbf{B}$. A single scalar observation $y$ measures the coarse-scale sum of the two subdomain anomalies, with an observation operator $\\mathbf{H} \\in \\mathbb{R}^{1 \\times 2}$ and observation error $\\varepsilon \\sim \\mathcal{N}(0, R)$, so that $y = \\mathbf{H}\\mathbf{x} + \\varepsilon$. Assume all errors are dimensionless.\n\nStarting from the linear-Gaussian model and Bayesâ€™ rule, and using only the definitions of a minimum mean-square error linear estimator, derive the expression for the optimal linear analysis update $\\mathbf{x}_{a} = \\mathbf{x}_{b} + \\mathbf{K}\\left(y - \\mathbf{H}\\mathbf{x}_{b}\\right)$ that minimizes the mean-square analysis error, where $\\mathbf{K}$ is the Kalman gain. Then, derive the analysis error variance along the observed direction, defined as the scalar $s_{a} = \\mathbf{H}\\mathbf{A}\\mathbf{H}^{\\top}$ where $\\mathbf{A}$ is the analysis error covariance.\n\nFinally, for the specific, physically consistent case\n$$\n\\mathbf{B} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad \\mathbf{H} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad R = 0.5,\n$$\ncompute the Kalman gain vector $\\mathbf{K} \\in \\mathbb{R}^{2}$ and the analysis variance $s_{a} \\in \\mathbb{R}$ along the observed direction. Express the final numerical values exactly as rational numbers with no rounding. Report the final answer as a row matrix containing the two components of $\\mathbf{K}$ followed by $s_{a}$.",
            "solution": "### Derivations and Solution\n\n#### Part 1: Derivation of the Optimal Kalman Gain ($\\mathbf{K}$)\n\nThe goal is to find the Kalman gain matrix $\\mathbf{K}$ that minimizes the mean-square analysis error. The analysis error is defined as $\\mathbf{e}_{a} = \\mathbf{x}_{a} - \\mathbf{x}$, where $\\mathbf{x}$ is the true state vector.\n\nSubstitute the expression for the analysis state $\\mathbf{x}_{a}$:\n$$\n\\mathbf{e}_{a} = \\left(\\mathbf{x}_{b} + \\mathbf{K}(y - \\mathbf{H}\\mathbf{x}_{b})\\right) - \\mathbf{x}\n$$\nNow, substitute the observation model $y = \\mathbf{H}\\mathbf{x} + \\varepsilon$:\n$$\n\\mathbf{e}_{a} = \\mathbf{x}_{b} + \\mathbf{K}(\\mathbf{H}\\mathbf{x} + \\varepsilon - \\mathbf{H}\\mathbf{x}_{b}) - \\mathbf{x}\n$$\nRearrange the terms by defining the background error $\\mathbf{e}_{b} = \\mathbf{x}_{b} - \\mathbf{x}$:\n$$\n\\mathbf{e}_{a} = (\\mathbf{x}_{b} - \\mathbf{x}) - \\mathbf{K}(\\mathbf{H}\\mathbf{x}_{b} - \\mathbf{H}\\mathbf{x} - \\varepsilon) = \\mathbf{e}_{b} - \\mathbf{K}(\\mathbf{H}(\\mathbf{x}_{b} - \\mathbf{x}) - \\varepsilon) = \\mathbf{e}_{b} - \\mathbf{K}(\\mathbf{H}\\mathbf{e}_{b} - \\varepsilon)\n$$\nThis gives the analysis error in terms of the background error and the observation error:\n$$\n\\mathbf{e}_{a} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H})\\mathbf{e}_{b} + \\mathbf{K}\\varepsilon\n$$\nThe analysis error covariance matrix $\\mathbf{A}$ is the expected value of the outer product of the analysis error with itself: $\\mathbf{A} = E[\\mathbf{e}_{a}\\mathbf{e}_{a}^{\\top}]$.\n$$\n\\mathbf{A} = E\\left[ ((\\mathbf{I} - \\mathbf{K}\\mathbf{H})\\mathbf{e}_{b} + \\mathbf{K}\\varepsilon) ((\\mathbf{I} - \\mathbf{K}\\mathbf{H})\\mathbf{e}_{b} + \\mathbf{K}\\varepsilon)^{\\top} \\right]\n$$\nExpanding this and taking the expectation, we use the facts that the background and observation errors are uncorrelated ($E[\\mathbf{e}_{b}\\varepsilon^{\\top}] = \\mathbf{0}$), $E[\\mathbf{e}_{b}\\mathbf{e}_{b}^{\\top}] = \\mathbf{B}$, and $E[\\varepsilon\\varepsilon^{\\top}] = E[\\varepsilon^2] = R$.\n$$\n\\mathbf{A} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H}) E[\\mathbf{e}_{b}\\mathbf{e}_{b}^{\\top}] (\\mathbf{I} - \\mathbf{K}\\mathbf{H})^{\\top} + \\mathbf{K} E[\\varepsilon\\varepsilon^{\\top}] \\mathbf{K}^{\\top}\n$$\n$$\n\\mathbf{A} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H}) \\mathbf{B} (\\mathbf{I} - \\mathbf{H}^{\\top}\\mathbf{K}^{\\top}) + \\mathbf{K}R\\mathbf{K}^{\\top}\n$$\nExpanding further, and using $\\mathbf{B} = \\mathbf{B}^{\\top}$:\n$$\n\\mathbf{A} = \\mathbf{B} - \\mathbf{K}\\mathbf{H}\\mathbf{B} - \\mathbf{B}\\mathbf{H}^{\\top}\\mathbf{K}^{\\top} + \\mathbf{K}\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}\\mathbf{K}^{\\top} + \\mathbf{K}R\\mathbf{K}^{\\top}\n$$\n$$\n\\mathbf{A} = \\mathbf{B} - \\mathbf{K}\\mathbf{H}\\mathbf{B} - \\mathbf{B}\\mathbf{H}^{\\top}\\mathbf{K}^{\\top} + \\mathbf{K}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)\\mathbf{K}^{\\top}\n$$\nThe mean-square error is the trace of $\\mathbf{A}$, $J(\\mathbf{K}) = \\text{Tr}(\\mathbf{A})$. To find the minimum, we differentiate $J(\\mathbf{K})$ with respect to $\\mathbf{K}$ and set the result to zero. Using standard matrix calculus identities:\n$$\n\\frac{\\partial J}{\\partial \\mathbf{K}} = \\frac{\\partial}{\\partial \\mathbf{K}} \\text{Tr}(\\mathbf{A}) = -\\mathbf{B}\\mathbf{H}^{\\top} - \\mathbf{B}\\mathbf{H}^{\\top} + 2\\mathbf{K}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R) = 0\n$$\n$$\n-2\\mathbf{B}\\mathbf{H}^{\\top} + 2\\mathbf{K}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R) = \\mathbf{0}\n$$\nSolving for $\\mathbf{K}$:\n$$\n\\mathbf{K}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R) = \\mathbf{B}\\mathbf{H}^{\\top}\n$$\nThe term $(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)$ is a scalar (the innovation covariance), which is invertible since $\\mathbf{B}$ is positive definite and $R > 0$. Thus, the optimal Kalman gain is:\n$$\n\\mathbf{K} = \\mathbf{B}\\mathbf{H}^{\\top}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)^{-1}\n$$\n\n#### Part 2: Derivation of the Analysis Variance ($s_a$)\n\nWe need to find $s_{a} = \\mathbf{H}\\mathbf{A}\\mathbf{H}^{\\top}$. We first find a simplified expression for $\\mathbf{A}$ by substituting the optimal $\\mathbf{K}$ back into the expression for $\\mathbf{A}$. A convenient form is $\\mathbf{A} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H})\\mathbf{B}$.\n$$\n\\mathbf{A} = \\mathbf{B} - \\mathbf{K}\\mathbf{H}\\mathbf{B}\n$$\nNow we compute $s_a$:\n$$\ns_a = \\mathbf{H}\\mathbf{A}\\mathbf{H}^{\\top} = \\mathbf{H}(\\mathbf{B} - \\mathbf{K}\\mathbf{H}\\mathbf{B})\\mathbf{H}^{\\top} = \\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} - \\mathbf{H}\\mathbf{K}\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}\n$$\nLet's evaluate the term $\\mathbf{H}\\mathbf{K}$. From the expression for $\\mathbf{K}$:\n$$\n\\mathbf{H}\\mathbf{K} = \\mathbf{H} \\left( \\mathbf{B}\\mathbf{H}^{\\top}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)^{-1} \\right) = (\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top})(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)^{-1}\n$$\nSubstituting this back into the expression for $s_a$:\n$$\ns_a = \\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} - (\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top})(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)^{-1} (\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top})\n$$\nLet $S_b = \\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}$ be the background variance in observation space. Then $S = S_b + R$ is the innovation covariance. The expression simplifies to:\n$$\ns_a = S_b - S_b (S_b + R)^{-1} S_b = S_b \\left(1 - \\frac{S_b}{S_b+R}\\right) = S_b \\left(\\frac{S_b+R-S_b}{S_b+R}\\right) = \\frac{S_b R}{S_b+R}\n$$\nThus, the analysis variance along the observed direction is:\n$$\ns_a = \\frac{(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}) R}{\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R}\n$$\n\n#### Part 3: Numerical Computation\n\nGiven the specific values:\n$$\n\\mathbf{B} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad \\mathbf{H} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad R = 0.5 = \\frac{1}{2}\n$$\nFirst, calculate the components needed for $\\mathbf{K}$.\nThe transpose of $\\mathbf{H}$ is $\\mathbf{H}^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe term $\\mathbf{B}\\mathbf{H}^{\\top}$ is:\n$$\n\\mathbf{B}\\mathbf{H}^{\\top} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe scalar term $\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}$ is:\n$$\n\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} = \\mathbf{H}(\\mathbf{B}\\mathbf{H}^{\\top}) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 1 \\cdot 2 + 1 \\cdot 1 = 3\n$$\nThe innovation covariance is $\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R$:\n$$\n3 + \\frac{1}{2} = \\frac{7}{2}\n$$\nIts inverse is $(\\frac{7}{2})^{-1} = \\frac{2}{7}$.\nNow, compute the Kalman gain $\\mathbf{K}$:\n$$\n\\mathbf{K} = (\\mathbf{B}\\mathbf{H}^{\\top})(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R)^{-1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\left(\\frac{2}{7}\\right) = \\begin{pmatrix} 4/7 \\\\ 2/7 \\end{pmatrix}\n$$\nNext, compute the analysis variance $s_a$ using the derived formula:\n$$\ns_a = \\frac{(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}) R}{\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} + R} = \\frac{3 \\cdot \\frac{1}{2}}{3 + \\frac{1}{2}} = \\frac{\\frac{3}{2}}{\\frac{7}{2}} = \\frac{3}{7}\n$$\nThe required output is a row matrix containing the two components of $\\mathbf{K}$ followed by $s_a$.\nThe components of $\\mathbf{K}$ are $K_1 = 4/7$ and $K_2 = 2/7$.\nThe value of $s_a$ is $3/7$.\nThe final result is the row matrix $(4/7, 2/7, 3/7)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{4}{7} & \\frac{2}{7} & \\frac{3}{7}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A robust data assimilation system requires not only performing the analysis but also diagnosing its quality and consistency. This advanced exercise introduces two powerful diagnostic tools used in operational weather prediction: the Degrees of Freedom for Signal (DFS) and the chi-square ($\\chi^2$) statistic. By calculating these quantities, you will learn how to assess the influence of observations on the analysis and check whether the assumed error statistics are consistent with the observed mismatches .",
            "id": "4015037",
            "problem": "Consider a one-dimensional, linear, Gaussian data assimilation setting typical of numerical weather prediction. Let the true state be denoted by $x$, the background (prior) be $x_b = x + \\xi$ with background error $\\xi \\sim \\mathcal{N}(0, B)$, and the observation be $y = H x + \\eta$ with observation error $\\eta \\sim \\mathcal{N}(0, R)$. Assume $\\xi$ and $\\eta$ are independent, unbiased, and Gaussian. The analysis (posterior) $x_a$ is defined as the minimizer of the quadratic cost function\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\top} B^{-1} (x - x_b) + \\frac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x),\n$$\nwith $B$, $R$, and $H$ given and all quantities scalar.\n\nUsing only the definitions above and the linear-Gaussian assumptions, derive the optimal analysis $x_a$, the Kalman gain $K$, and the analysis error variance $A$. Then, using the influence of the observations on the analysis, define the Degrees of Freedom for Signal (DFS) and express it in terms of $B$, $R$, and $H$. Next, define the chi-square statistic of the observation-space residual at the analysis as\n$$\n\\chi^2 \\equiv \\mathbb{E}\\big[(y - H x_a)^{\\top} R^{-1} (y - H x_a)\\big],\n$$\nwhere the expectation is taken over the joint distribution of $\\xi$ and $\\eta$, and compute it in terms of $B$, $R$, and $H$.\n\nFinally, evaluate both the DFS and $\\chi^2$ for the specific case $B = 4$, $R = 1$, and $H = 1$, and check whether the consistency relation between these quantities and the number of observations $p$ holds. Take the number of observations to be $p = 1$ in this scalar setting. Report your final numerical values for DFS and $\\chi^2$ in that order as a row matrix. These quantities are dimensionless; no units are required. Express your final answer exactly; do not round.",
            "solution": "The analysis state $x_a$ is the value of $x$ that minimizes the cost function $J(x)$. For the scalar case given, the cost function is:\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^2 B^{-1} + \\frac{1}{2} (y - H x)^2 R^{-1}\n$$\nTo find the minimum, we compute the derivative of $J(x)$ with respect to $x$ and set it to zero:\n$$\n\\frac{dJ}{dx} = B^{-1}(x - x_b) + R^{-1}(y - Hx)(-H) = 0\n$$\nLet $x_a$ be the value of $x$ that satisfies this equation:\n$$\nB^{-1}(x_a - x_b) - H R^{-1}(y - Hx_a) = 0\n$$\n$$\nB^{-1}x_a - B^{-1}x_b - H R^{-1}y + H^2 R^{-1}x_a = 0\n$$\nGrouping terms with $x_a$:\n$$\nx_a(B^{-1} + H^2 R^{-1}) = x_b B^{-1} + y H R^{-1}\n$$\nSolving for $x_a$:\n$$\nx_a = (B^{-1} + H^2 R^{-1})^{-1} (x_b B^{-1} + y H R^{-1})\n$$\nThis expression can be rearranged into the standard update form, $x_a = x_b + K(y - Hx_b)$, where $K$ is the Kalman gain.\n$$\nx_a = \\left(\\frac{1}{B} + \\frac{H^2}{R}\\right)^{-1} \\left(\\frac{x_b}{B} + \\frac{yH}{R}\\right) = \\left(\\frac{R + BH^2}{BR}\\right)^{-1} \\left(\\frac{Rx_b + BHy}{BR}\\right) = \\frac{BR}{R+BH^2} \\frac{Rx_b + BHy}{BR} = \\frac{Rx_b + BHy}{R+BH^2}\n$$\nTo find $K$, we rewrite $x_a$ as:\n$$\nx_a = \\frac{R}{R+BH^2}x_b + \\frac{BH}{R+BH^2}y\n$$\n$$\nx_a = \\frac{R+BH^2 - BH^2}{R+BH^2}x_b + \\frac{BH}{R+BH^2}y = \\left(1 - \\frac{BH^2}{R+BH^2}\\right)x_b + \\frac{BH}{R+BH^2}y\n$$\n$$\nx_a = x_b - \\frac{BH^2}{R+BH^2}x_b + \\frac{BH}{R+BH^2}y = x_b + \\frac{BH}{R+BH^2}(y - Hx_b)\n$$\nComparing this to $x_a = x_b + K(y - Hx_b)$, we identify the Kalman gain $K$ as:\n$$\nK = \\frac{BH}{R+BH^2}\n$$\nThe analysis error is $\\epsilon_a = x_a - x$. We substitute the expressions for $x_b = x + \\xi$ and $y = Hx + \\eta$:\n$$\nx_a - x = (x_b + K(y - Hx_b)) - x = (x+\\xi) + K(Hx+\\eta - H(x+\\xi)) - x = \\xi + K(\\eta - H\\xi)\n$$\n$$\nx_a - x = (1 - KH)\\xi + K\\eta\n$$\nThe analysis error variance $A$ is the expectation of the squared analysis error, $A = \\mathbb{E}[(x_a - x)^2]$. Since $\\xi$ and $\\eta$ are independent and have zero mean, their cross-correlation is zero.\n$$\nA = \\mathbb{E}[((1-KH)\\xi + K\\eta)^2] = (1-KH)^2\\mathbb{E}[\\xi^2] + K^2\\mathbb{E}[\\eta^2] = (1-KH)^2 B + K^2 R\n$$\nWe compute the term $(1 - KH)$:\n$$\n1-KH = 1 - \\frac{BH}{R+BH^2}H = 1 - \\frac{BH^2}{R+BH^2} = \\frac{R+BH^2-BH^2}{R+BH^2} = \\frac{R}{R+BH^2}\n$$\nSubstituting this into the expression for $A$:\n$$\nA = \\left(\\frac{R}{R+BH^2}\\right)^2 B + \\left(\\frac{BH}{R+BH^2}\\right)^2 R = \\frac{R^2B + B^2H^2R}{(R+BH^2)^2} = \\frac{RB(R+BH^2)}{(R+BH^2)^2} = \\frac{RB}{R+BH^2}\n$$\nAlternatively, $A = (1-KH)B = \\frac{R}{R+BH^2}B$. This is a well-known identity for the analysis error variance.\n\nThe Degrees of Freedom for Signal (DFS) measures the influence of the observations on the analysis. It is defined as the trace of the matrix derivative of the analysis in observation space ($Hx_a$) with respect to the observations ($y$).\n$$\nDFS = \\text{tr}\\left(\\frac{\\partial (Hx_a)}{\\partial y}\\right)\n$$\nFrom $x_a = (1-KH)x_b + Ky$, we get $\\frac{\\partial x_a}{\\partial y} = K$.\nTherefore, $\\frac{\\partial (Hx_a)}{\\partial y} = HK$. In the scalar case, the trace is simply the value itself.\n$$\nDFS = HK = H \\left(\\frac{BH}{R+BH^2}\\right) = \\frac{BH^2}{R+BH^2}\n$$\nNext, we compute the chi-square statistic of the observation-space residual at the analysis, defined as $\\chi^2 \\equiv \\mathbb{E}\\big[(y - H x_a)^{\\top} R^{-1} (y - H x_a)\\big]$. In scalar form:\n$$\n\\chi^2 = R^{-1} \\mathbb{E}[(y - Hx_a)^2]\n$$\nThe term $y-Hx_a$ is the analysis residual. We can express it in terms of the innovation, $d = y - Hx_b$.\n$$\ny - Hx_a = y - H(x_b + K(y-Hx_b)) = (y - Hx_b) - HK(y - Hx_b) = (1-HK)(y-Hx_b)\n$$\nNow, express the innovation in terms of the fundamental error terms:\n$$\ny - Hx_b = (Hx + \\eta) - H(x + \\xi) = \\eta - H\\xi\n$$\nThe expected variance of the innovation is $\\mathbb{E}[(\\eta - H\\xi)^2] = \\mathbb{E}[\\eta^2] - 2H\\mathbb{E}[\\eta\\xi] + H^2\\mathbb{E}[\\xi^2] = R + H^2B$.\nThe expected variance of the analysis residual is:\n$$\n\\mathbb{E}[(y - Hx_a)^2] = \\mathbb{E}[((1-HK)(\\eta - H\\xi))^2] = (1-HK)^2 \\mathbb{E}[(\\eta - H\\xi)^2] = (1-HK)^2(R+H^2B)\n$$\nSubstituting $1-KH = \\frac{R}{R+BH^2}$:\n$$\n\\mathbb{E}[(y - Hx_a)^2] = \\left(\\frac{R}{R+BH^2}\\right)^2(R+BH^2) = \\frac{R^2}{R+BH^2}\n$$\nFinally, we compute $\\chi^2$:\n$$\n\\chi^2 = R^{-1} \\left(\\frac{R^2}{R+BH^2}\\right) = \\frac{R}{R+BH^2}\n$$\nThe consistency relation involves the sum of DFS and $\\chi^2$.\n$$\nDFS + \\chi^2 = \\frac{BH^2}{R+BH^2} + \\frac{R}{R+BH^2} = \\frac{R+BH^2}{R+BH^2} = 1\n$$\nThe problem states the number of observations is $p=1$. Thus, the consistency relation $DFS + \\chi^2 = p$ holds true for this scalar case.\n\nFinally, we evaluate DFS and $\\chi^2$ for the given values: $B=4$, $R=1$, and $H=1$.\n$$\nDFS = \\frac{BH^2}{R+BH^2} = \\frac{4 \\cdot 1^2}{1 + 4 \\cdot 1^2} = \\frac{4}{5}\n$$\n$$\n\\chi^2 = \\frac{R}{R+BH^2} = \\frac{1}{1 + 4 \\cdot 1^2} = \\frac{1}{5}\n$$\nThe final answer requires reporting the numerical values for DFS and $\\chi^2$ in that order as a row matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{5} & \\frac{1}{5} \\end{pmatrix}}\n$$"
        }
    ]
}