{
    "hands_on_practices": [
        {
            "introduction": "数据同化的核心目标是将背景信息（如之前的预报）与新的观测相结合，以获得对大气状态更准确的“分析”估计。本练习将引导你完成一个简单的二维系统中这一过程的数学基础。我们将从贝叶斯定理出发，推导分析误差协方差矩阵 $A$，并为一个具体案例计算其数值。至关重要的是，我们将验证一个基本原则：分析的不确定性总是低于背景场的不确定性，这一概念可以通过矩阵的勒夫纳序（Loewner order）来严格表述。",
            "id": "4015013",
            "problem": "考虑一个用于数值天气预报的线性高斯数据同化场景，其状态向量 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 是二维的。设先验（背景）分布为高斯分布，其均值为 $\\mathbf{x}_{b}$，协方差矩阵为 $\\mathbf{B}$。并假设观测模型是线性的，$\\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon}$ 与 $\\mathbf{x}$ 独立，服从均值为零、协方差矩阵为 $\\mathbf{R}$ 的高斯分布。使用贝叶斯定理和多元高斯分布的性质，后验（分析）分布也是高斯分布，其均值为 $\\mathbf{x}_{a}$，协方差矩阵为 $\\mathbf{A}$。从这些基本定义和误差的独立性出发，推导分析协方差 $\\mathbf{A}$ 关于 $\\mathbf{B}$、$\\mathbf{H}$ 和 $\\mathbf{R}$ 的表达式。然后，对于以下特定情况\n$$\n\\mathbf{B} = \\begin{pmatrix} 1  0.5 \\\\ 0.5  2 \\end{pmatrix}, \\quad \\mathbf{H} = \\mathbf{I}, \\quad \\mathbf{R} = \\mathrm{diag}(0.25,\\,0.25),\n$$\n计算精确的分析协方差矩阵 $\\mathbf{A}$，并通过直接评估 $\\mathbf{B}-\\mathbf{A}$ 的半正定性，来验证在 Loewner 序下 $\\mathbf{A}$ 小于 $\\mathbf{B}$。将最终矩阵的元素精确地表示为有理数。不需要四舍五入。",
            "solution": "该问题是在线性高斯设定下推导分析协方差矩阵，针对一个具体案例进行计算，并验证一个与不确定性减少相关的性质。\n\n### 分析协方差 $\\mathbf{A}$ 的推导\n\n根据贝叶斯定理，后验概率密度函数 $p(\\mathbf{x}|\\mathbf{y})$ 正比于似然 $p(\\mathbf{y}|\\mathbf{x})$ 和先验 $p(\\mathbf{x})$ 的乘积：\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{x}) p(\\mathbf{x})$$\n在先验和似然均为高斯分布的假设下，后验分布也为高斯分布。通过比较后验分布的负对数指数项与先验和似然的负对数指数项之和，我们可以识别出后验协方差的逆。\n\n先验项的负对数指数为 $\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^T \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b)$。\n似然项的负对数指数为 $\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})$。\n\n将这两项相加并只关注与 $\\mathbf{x}$ 相关的二次项，我们得到 $\\frac{1}{2}\\mathbf{x}^T (\\mathbf{B}^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H}) \\mathbf{x}$。\n后验分布 $p(\\mathbf{x}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{x}; \\mathbf{x}_{a}, \\mathbf{A})$ 的负对数指数中，与 $\\mathbf{x}$ 相关的二次项为 $\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_a)^T \\mathbf{A}^{-1} (\\mathbf{x} - \\mathbf{x}_a) \\implies \\frac{1}{2}\\mathbf{x}^T \\mathbf{A}^{-1} \\mathbf{x}$。\n通过比较二次项，我们得到分析协方差矩阵 $\\mathbf{A}$ 的逆矩阵：\n$$\\mathbf{A}^{-1} = \\mathbf{B}^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H}$$\n因此，分析协方差矩阵为：\n$$\\mathbf{A} = (\\mathbf{B}^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H})^{-1}$$\n\n### 具体案例的计算\n\n我们已知：\n$$\\mathbf{B} = \\begin{pmatrix} 1  \\frac{1}{2} \\\\ \\frac{1}{2}  2 \\end{pmatrix}$$\n$$\\mathbf{H} = \\mathbf{I} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$$\n$$\\mathbf{R} = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{1}{4} \\end{pmatrix}$$\n\n首先，我们计算逆矩阵 $\\mathbf{B}^{-1}$ 和 $\\mathbf{R}^{-1}$。\n对于 $\\mathbf{B}$，其行列式为 $\\det(\\mathbf{B}) = (1)(2) - (\\frac{1}{2})(\\frac{1}{2}) = 2 - \\frac{1}{4} = \\frac{7}{4}$。\n$$\\mathbf{B}^{-1} = \\frac{1}{\\det(\\mathbf{B})} \\begin{pmatrix} 2  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 2  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{7}  -\\frac{2}{7} \\\\ -\\frac{2}{7}  \\frac{4}{7} \\end{pmatrix}$$\n对于 $\\mathbf{R}$，其逆矩阵为：\n$$\\mathbf{R}^{-1} = \\begin{pmatrix} 4  0 \\\\ 0  4 \\end{pmatrix}$$\n\n现在我们计算逆分析协方差 $\\mathbf{A}^{-1}$。由于 $\\mathbf{H} = \\mathbf{I}$，则 $\\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H} = \\mathbf{R}^{-1}$。\n$$\\mathbf{A}^{-1} = \\mathbf{B}^{-1} + \\mathbf{R}^{-1} = \\begin{pmatrix} \\frac{8}{7}  -\\frac{2}{7} \\\\ -\\frac{2}{7}  \\frac{4}{7} \\end{pmatrix} + \\begin{pmatrix} 4  0 \\\\ 0  4 \\end{pmatrix}$$\n$$\\mathbf{A}^{-1} = \\begin{pmatrix} \\frac{8}{7} + \\frac{28}{7}  -\\frac{2}{7} \\\\ -\\frac{2}{7}  \\frac{4}{7} + \\frac{28}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{36}{7}  -\\frac{2}{7} \\\\ -\\frac{2}{7}  \\frac{32}{7} \\end{pmatrix}$$\n\n最后，我们通过对 $\\mathbf{A}^{-1}$ 求逆来计算 $\\mathbf{A}$。$\\mathbf{A}^{-1}$ 的行列式为：\n$$\\det(\\mathbf{A}^{-1}) = \\left(\\frac{36}{7}\\right)\\left(\\frac{32}{7}\\right) - \\left(-\\frac{2}{7}\\right)\\left(-\\frac{2}{7}\\right) = \\frac{1152}{49} - \\frac{4}{49} = \\frac{1148}{49}$$\n分析协方差矩阵 $\\mathbf{A}$ 为：\n$$\\mathbf{A} = (\\mathbf{A}^{-1})^{-1} = \\frac{1}{\\det(\\mathbf{A}^{-1})} \\begin{pmatrix} \\frac{32}{7}  \\frac{2}{7} \\\\ \\frac{2}{7}  \\frac{36}{7} \\end{pmatrix} = \\frac{49}{1148} \\begin{pmatrix} \\frac{32}{7}  \\frac{2}{7} \\\\ \\frac{2}{7}  \\frac{36}{7} \\end{pmatrix}$$\n$$\\mathbf{A} = \\frac{7}{1148} \\begin{pmatrix} 32  2 \\\\ 2  36 \\end{pmatrix} = \\frac{14}{1148} \\begin{pmatrix} 16  1 \\\\ 1  18 \\end{pmatrix}$$\n化简分数 $\\frac{14}{1148} = \\frac{7}{574} = \\frac{1}{82}$。\n$$\\mathbf{A} = \\frac{1}{82} \\begin{pmatrix} 16  1 \\\\ 1  18 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{82}  \\frac{1}{82} \\\\ \\frac{1}{82}  \\frac{18}{82} \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{41}  \\frac{1}{82} \\\\ \\frac{1}{82}  \\frac{9}{41} \\end{pmatrix}$$\n\n### 不确定性减少的验证\n\n为了验证在 Loewner 序下 $\\mathbf{A}$ 小于 $\\mathbf{B}$ (即 $\\mathbf{B} \\ge \\mathbf{A}$)，我们必须证明差分矩阵 $\\mathbf{D} = \\mathbf{B} - \\mathbf{A}$ 是半正定的。\n$$\\mathbf{B} = \\begin{pmatrix} 1  \\frac{1}{2} \\\\ \\frac{1}{2}  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{82}{82}  \\frac{41}{82} \\\\ \\frac{41}{82}  \\frac{164}{82} \\end{pmatrix}$$\n$$\\mathbf{A} = \\begin{pmatrix} \\frac{16}{82}  \\frac{1}{82} \\\\ \\frac{1}{82}  \\frac{18}{82} \\end{pmatrix}$$\n$$\\mathbf{D} = \\mathbf{B} - \\mathbf{A} = \\begin{pmatrix} \\frac{82-16}{82}  \\frac{41-1}{82} \\\\ \\frac{41-1}{82}  \\frac{164-18}{82} \\end{pmatrix} = \\begin{pmatrix} \\frac{66}{82}  \\frac{40}{82} \\\\ \\frac{40}{82}  \\frac{146}{82} \\end{pmatrix} = \\frac{1}{41} \\begin{pmatrix} 33  20 \\\\ 20  73 \\end{pmatrix}$$\n一个对称矩阵是正定的当且仅当其所有主子式都为正。\n第一个主子式是左上角的元素：\n$$D_{11} = \\frac{33}{41} > 0$$\n第二个主子式是 $\\mathbf{D}$ 的行列式：\n$$\\det(\\mathbf{D}) = \\left(\\frac{1}{41}\\right)^2 \\left( (33)(73) - (20)(20) \\right) = \\frac{1}{1681} (2409 - 400) = \\frac{2009}{1681} > 0$$\n因为所有主子式都为正，所以矩阵 $\\mathbf{D} = \\mathbf{B} - \\mathbf{A}$ 是正定的。根据定义，正定矩阵也是半正定的。因此，我们验证了在 Loewner 序下 $\\mathbf{B} \\ge \\mathbf{A}$，这在物理上意味着数据同化过程减少了状态估计的不确定性。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{8}{41}  \\frac{1}{82} \\\\\n\\frac{1}{82}  \\frac{9}{41}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在理想化的模型之外，现实世界的观测误差往往存在相关性，例如由于卫星观测在空间和时间上的邻近性。本练习探讨了一个关键的实际挑战：当我们错误地假设观测误差是独立的时会发生什么？通过一个引导性的思想实验和推导，我们将揭示这种常见的模型设定错误如何导致“过度自信”的分析结果，并量化这种过度自信的程度。这个练习旨在提升我们对数据同化统计模型背后假设的批判性思维。",
            "id": "4015027",
            "problem": "在一个与数值天气预报（NWP）和气候再分析相关的一维同化情景中，考虑在给定有效时间的一个标量大气状态变量 $x$。$x$ 的背景（先验）是均值为 $x_b$、方差为 $B>0$ 的高斯分布，即 $x \\sim \\mathcal{N}(x_b, B)$。在时刻 $t_1$ 和 $t_2$ 有两个对同一有效时间状态 $x$ 的观测，模型为\n$$\ny_1 = x + \\varepsilon_1, \\qquad y_2 = x + \\varepsilon_2,\n$$\n其中 $(\\varepsilon_1, \\varepsilon_2)$ 是一个零均值的二元高斯观测误差向量，其方差 $\\operatorname{Var}(\\varepsilon_1)=\\operatorname{Var}(\\varepsilon_2)=R>0$，相关系数为 $\\rho \\in (-1,1)$，因此真实的观测误差协方差矩阵是\n$$\n\\mathbf{R}_{\\text{true}} = R \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}.\n$$\n然而，一位分析员错误地假设观测误差是时间白噪声（即在时间上不相关），因此使用了错误指定的协方差\n$$\n\\mathbf{R}_{\\text{assumed}} = R \\mathbf{I}_2.\n$$\n\n使用线性高斯模型的贝叶斯推断第一性原理，解决以下问题。\n\n1) 从信息含量和独立信息有效数量的角度解释，为什么当 $\\rho>0$ 时，使用 $\\mathbf{R}_{\\text{assumed}}$ 会导致过度自信的分析，即在独立性假设下计算的后验方差小于正确考虑真实相关性时获得的后验方差。\n\n2) 令 $a \\equiv B/R$ 表示背景方差与观测方差之比。推导比率\n$$\nU(a,\\rho) \\equiv \\frac{\\text{posterior variance computed under } \\mathbf{R}_{\\text{assumed}}}{\\text{posterior variance computed under } \\mathbf{R}_{\\text{true}}},\n$$\n的闭式解析表达式，仅用 $a$ 和 $\\rho$ 表示，并尽可能地简化它。\n\n将 $U(a,\\rho)$ 的简化解析表达式作为你的最终答案。不需要进行数值计算，也无需四舍五入。最终答案是无量纲的，报告时无需单位。",
            "solution": "该问题的核心在于线性高斯情景下后验（或分析）方差的贝叶斯公式。对于一个具有先验（背景）分布 $\\mathcal{N}(x_b, B)$ 的标量状态变量 $x$，以及一个通过线性观测算子 $\\mathbf{H}$ 与状态相关的观测向量 $\\mathbf{y}$，其高斯误差为 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$，后验方差（我们记为 $V_a$）由精度（方差的倒数）的更新公式给出：\n$$\nV_a^{-1} = B^{-1} + \\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H}\n$$\n这里，$B^{-1}$ 是背景精度，项 $\\mathbf{H}^T \\mathbf{R}^{-1} \\mathbf{H}$ 代表由观测贡献的信息（精度）。在我们的具体问题中，观测算子是 $\\mathbf{H} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n我们将根据对观测误差协方差矩阵 $\\mathbf{R}$ 的两种不同假设来计算后验方差：错误指定的 $\\mathbf{R}_{\\text{assumed}}$ 和正确的 $\\mathbf{R}_{\\text{true}}$。\n\n在独立性假设下，$\\mathbf{R}_{\\text{assumed}}^{-1} = \\frac{1}{R} \\mathbf{I}_2$。来自观测的信息是 $\\mathbf{H}^T \\mathbf{R}_{\\text{assumed}}^{-1} \\mathbf{H} = \\frac{2}{R}$。后验精度为 $ (V_{a, \\text{assumed}})^{-1} = B^{-1} + \\frac{2}{R} $。\n\n使用真实的相关误差，$\\mathbf{R}_{\\text{true}}^{-1} = \\frac{1}{R(1-\\rho^2)} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$。来自观测的真实信息是 $\\mathbf{H}^T \\mathbf{R}_{\\text{true}}^{-1} \\mathbf{H} = \\frac{2}{R(1+\\rho)}$。真实的后验精度为 $ (V_{a, \\text{true}})^{-1} = B^{-1} + \\frac{2}{R(1+\\rho)} $。\n\n### 1) 对 $\\rho > 0$ 时过度自信的解释：\n\n过度自信意味着计算出的后验方差小于真实的后验方差，即 $V_{a, \\text{assumed}}  V_{a, \\text{true}}$。这等价于证明假设的后验精度大于真实的后验精度。\n\n在独立性假设下，来自观测的信息含量为 $2/R$。真实的信息含量为 $2/(R(1+\\rho))$。\n当相关性为正（$\\rho > 0$）时，我们有 $1+\\rho > 1$，这意味着：\n$$\n\\frac{2}{R(1+\\rho)}  \\frac{2}{R}\n$$\n这个不等式表明，分析员通过错误地假设误差是独立的，高估了观测所提供的信息量。这两个观测（$y_1, y_2$）不是独立的；由于正相关，它们携带了冗余信息。分析员实际上“重复计算”了两者共有的信息。项 $1/(1+\\rho)  1$ 充当了对来自第二个观测的信息的折扣因子，从而正确地考虑了这种冗余。独立观测的有效数量是 $2/(1+\\rho)$，对于 $\\rho > 0$ 时，这个值小于2。\n\n通过高估来自观测的信息含量，总的后验精度也被高估了：\n$$\n(V_{a, \\text{assumed}})^{-1} = B^{-1} + \\frac{2}{R} > B^{-1} + \\frac{2}{R(1+\\rho)} = (V_{a, \\text{true}})^{-1}\n$$\n由于后验方差是后验精度的倒数，这种对精度的过高估计导致了对方差的过低估计：$V_{a, \\text{assumed}}  V_{a, \\text{true}}$。这就是“过度自信”的数学基础：分析模型认为其估计比实际情况更确定（具有更小的误差方差）。\n\n### 2) 比率 $U(a, \\rho)$ 的推导：\n\n该比率定义为 $U(a,\\rho) = V_{a, \\text{assumed}} / V_{a, \\text{true}}$。将其表示为相应逆方差（精度）的比率：\n$$\nU(a,\\rho) = \\frac{(V_{a, \\text{true}})^{-1}}{(V_{a, \\text{assumed}})^{-1}} = \\frac{B^{-1} + \\frac{2}{R(1+\\rho)}}{B^{-1} + \\frac{2}{R}}\n$$\n现在，我们引入定义的比率 $a = B/R$，这意味着 $B^{-1} = 1/B = 1/(aR)$。\n$$\nU(a,\\rho) = \\frac{\\frac{1}{aR} + \\frac{2}{R(1+\\rho)}}{\\frac{1}{aR} + \\frac{2}{R}}\n$$\n从分子和分母中提出因子 $1/R$并相消：\n$$\nU(a,\\rho) = \\frac{\\frac{1}{a} + \\frac{2}{1+\\rho}}{\\frac{1}{a} + 2}\n$$\n为简化此繁分数，我们将分子和分母中的项通分：\n分子：$\\frac{1}{a} + \\frac{2}{1+\\rho} = \\frac{1+\\rho+2a}{a(1+\\rho)}$\n分母：$\\frac{1}{a} + 2 = \\frac{1+2a}{a}$\n因此，该比率变为：\n$$\nU(a,\\rho) = \\frac{\\frac{1+\\rho+2a}{a(1+\\rho)}}{\\frac{1+2a}{a}} = \\frac{1+\\rho+2a}{a(1+\\rho)} \\cdot \\frac{a}{1+2a}\n$$\n因子 $a$ 相消，得到简化后的表达式：\n$$\nU(a,\\rho) = \\frac{1+2a+\\rho}{(1+2a)(1+\\rho)}\n$$\n展开分母得到所求的最终形式：\n$$\nU(a,\\rho) = \\frac{1+2a+\\rho}{1+2a+\\rho+2a\\rho}\n$$\n作为检验，对于 $\\rho > 0$ 和 $a > 0$，分母大于分子，因此 $U(a,\\rho)  1$，这证实了在第1部分中建立的过度自信（$V_{a, \\text{assumed}}  V_{a, \\text{true}}$）。",
            "answer": "$$\\boxed{\\frac{1 + 2a + \\rho}{1 + 2a + \\rho + 2a\\rho}}$$"
        },
        {
            "introduction": "在大型业务化数值天气预报系统中，寻找最优分析解等价于求解一个高维代价函数的最小值问题，这通常需要借助迭代数值方法。这些方法的计算效率在很大程度上取决于问题的数学“性态”（conditioning）。本练习旨在搭建理论与计算之间的桥梁，介绍一种称为“白化”的变量变换，这是一种强大的预处理技术。我们将分析这种变换如何改变优化问题的几何景观，并量化它对共轭梯度求解器预期收敛速度的显著影响。",
            "id": "4015052",
            "problem": "考虑一个针对线性观测算子的二次变分资料同化问题，其中背景误差和观测误差均为高斯分布。分析状态是组合了背景项和观测失配项的成本函数的最小化子。令控制变量为 $x \\in \\mathbb{R}^n$，背景场（或初猜场）为 $x_b \\in \\mathbb{R}^n$，观测向量为 $y \\in \\mathbb{R}^m$，线性观测算子为 $H \\in \\mathbb{R}^{m \\times n}$。背景误差协方差矩阵为 $B \\in \\mathbb{R}^{n \\times n}$，观测误差协方差矩阵为 $R \\in \\mathbb{R}^{m \\times m}$。假设 $B$ 和 $R$ 是对称正定矩阵。成本函数为\n$$\nJ(x) = \\tfrac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2} (y - H x)^\\top R^{-1} (y - H x).\n$$\n你将实现一个变量变换，通过引入新的控制变量 $v \\in \\mathbb{R}^n$ 来白化背景项。该变换通过因子分解 $B = L L^\\top$（其中 $L \\in \\mathbb{R}^{n \\times n}$ 是非奇异下三角矩阵）和映射 $x = x_b + L v$ 实现。仅使用高斯最小二乘法、二次型梯度以及对称正定系统共轭梯度法收敛界的定义和基本性质，完成以下任务：\n\n1) 实现变量变换 $v = L^{-1} (x - x_b)$，其中 $L$ 由 $B$ 的 Cholesky 分解定义。推导 $x$ 空间和 $v$ 空间中的梯度，并计算它们在初始猜测点 $x_0 = x_b$（等价于 $v_0 = 0$）处的欧几里得范数。你必须从 $J(x)$ 的定义出发，并使用链式法则来表示 $v$ 空间梯度。报告这两个范数。\n\n2) 推导两个空间中的对称正定 Hessian 矩阵：$x$ 空间中的 $A_x$ 和 $v$ 空间中的 $A_v$。计算 Hessian 矩阵的谱条件数，定义为 $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别表示最大和最小特征值。\n\n3) 使用对称正定矩阵的标准最坏情况共轭梯度界，该界表明 $k$ 次迭代后误差的 $A$-范数有界：\n$$\n\\| e_k \\|_A \\le 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\| e_0 \\|_A,\n$$\n计算最小整数 $k$，使得该界保证误差的 $A$-范数至少减小因子 $\\tau$，即找到满足以下条件的最小 $k$：\n$$\n2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\le \\tau.\n$$\n对 $A_x$ 和 $A_v$ 使用相同的容差 $\\tau$ 进行此计算。\n\n你的程序必须对以下测试套件中的每一组参数执行这些计算。所有矩阵和向量都明确给出或通过构造给出。对于任何对角矩阵 $\\operatorname{diag}([\\cdot])$，元素都放置在主对角线上。所有数字都是实数。\n\n测试套件：\n\n- 案例 1：\n  - 维度：$n = 3$, $m = 3$.\n  - $B_1 = \\begin{bmatrix} $1.0$  $0.8$  $0.3$ \\\\ $0.8$  $1.0$  $0.2$ \\\\ $0.3$  $0.2$  $1.0$ \\end{bmatrix}$.\n  - $R_1 = \\operatorname{diag}([ $0.25$, $0.25$, $0.25$ ])$.\n  - $H_1 = I_{3}$（$3 \\times 3$ 单位矩阵）。\n  - $x_{b,1} = \\begin{bmatrix} $0.0$ \\\\ $0.0$ \\\\ $0.0$ \\end{bmatrix}$, $y_1 = \\begin{bmatrix} $1.0$ \\\\ -$1.0$ \\\\ $0.5$ \\end{bmatrix}$.\n\n- 案例 2：\n  - 维度：$n = 4$, $m = 2$.\n  - $B_2 = \\operatorname{diag}([ $1.0$, $4.0$, $9.0$, $16.0$ ])$.\n  - $R_2 = \\operatorname{diag}([ $0.25$, $4.0$ ])$.\n  - $H_2 = \\begin{bmatrix} $1.0$  $0.0$  $0.5$  $0.0$ \\\\ $0.0$  $1.0$  $0.0$  -$1.0$ \\end{bmatrix}$.\n  - $x_{b,2} = \\begin{bmatrix} $0.0$ \\\\ $0.0$ \\\\ $0.0$ \\\\ $0.0$ \\end{bmatrix}$, $y_2 = \\begin{bmatrix} $2.0$ \\\\ -$1.0$ \\end{bmatrix}$.\n\n- 案例 3：\n  - 维度：$n = 3$, $m = 3$.\n  - 构造 $Q = \\begin{bmatrix} $0.8$  -$0.6$  $0.0$ \\\\ $0.6$  $0.8$  $0.0$ \\\\ $0.0$  $0.0$  $1.0$ \\end{bmatrix}$, $D = \\operatorname{diag}([ $0.01$, $1.0$, $100.0$ ])$，并设置 $B_3 = Q D Q^\\top$。\n  - $R_3 = \\operatorname{diag}([ $0.04$, $1.0$, $25.0$ ])$.\n  - $H_3 = \\begin{bmatrix} $1.0$  $0.2$  $0.0$ \\\\ $0.0$  $1.0$  -$0.3$ \\\\ $0.1$  $0.0$  $1.0$ \\end{bmatrix}$.\n  - $x_{b,3} = \\begin{bmatrix} $0.0$ \\\\ $0.0$ \\\\ $0.0$ \\end{bmatrix}$, $y_3 = \\begin{bmatrix} $0.5$ \\\\ -$0.5$ \\\\ $2.0$ \\end{bmatrix}$.\n\n使用容差 $\\tau = 10^{-6}$。\n\n对于每个案例，按顺序计算并返回以下六个量：\n- $x$ 空间中的条件数，$\\kappa(A_x)$。\n- $v$ 空间中的条件数，$\\kappa(A_v)$。\n- $x$ 空间中的最小整数迭代次数，$k_x$。\n- $v$ 空间中的最小整数迭代次数，$k_v$。\n- $x$ 空间中梯度在 $x_0 = x_b$ 处的欧几里得范数。\n- $v$ 空间中梯度在 $v_0 = 0$ 处的欧几里得范数。\n\n最终输出格式：\n你的程序应生成单行输出，包含一个含三个列表的列表，每个案例一个，条目按 $[\\kappa(A_x), \\kappa(A_v), k_x, k_v, \\|\\nabla_x J(x_b)\\|_2, \\|\\nabla_v J(0)\\|_2]$ 顺序排列。\n将浮点数 $\\kappa(A_x)$, $\\kappa(A_v)$, $\\|\\nabla_x J(x_b)\\|_2$, 和 $\\|\\nabla_v J(0)\\|_2$ 四舍五入到 $6$ 位小数，并将迭代次数 $k_x$ 和 $k_v$ 格式化为整数。输出中不要包含任何空格。例如，外层列表应如下所示\n$[[a_1,b_1,c_1,d_1,e_1,f_1],[a_2,b_2,c_2,d_2,e_2,f_2],[a_3,b_3,c_3,d_3,e_3,f_3]]$，\n其中每个 $a_i$, $b_i$, $e_i$, $f_i$ 是四舍五入到 $6$ 位小数的浮点数，而 $c_i$, $d_i$ 是整数。",
            "solution": "该问题要求分析一个二次变分资料同化成本函数，以及变量变换对优化问题条件数的影响。我们将首先在原始状态空间（$x$ 空间）和变换后的控制变量空间（$v$ 空间）中推导梯度和 Hessian 矩阵的表达式。\n\n### 1. 状态空间（$x$ 空间）分析\n成本函数 $J(x)$ 的梯度 $\\nabla_x J$ 和 Hessian 矩阵 $A_x = \\nabla^2_{xx} J$ 分别为：\n$$\n\\nabla_x J(x) = B^{-1}(x - x_b) + H^\\top R^{-1}(Hx - y)\n$$\n$$\nA_x = B^{-1} + H^\\top R^{-1} H\n$$\n\n### 2. 控制变量空间（$v$ 空间）分析\n通过变换 $x = x_b + Lv$（其中 $B = LL^\\top$）引入控制变量 $v$。成本函数 $J(v)$ 变为：\n$$\nJ(v) = \\frac{1}{2} v^\\top v + \\frac{1}{2} (y - H(x_b + Lv))^\\top R^{-1} (y - H(x_b + Lv))\n$$\n其梯度 $\\nabla_v J$ 和 Hessian 矩阵 $A_v = \\nabla^2_{vv} J$ 分别为：\n$$\n\\nabla_v J(v) = v + L^\\top H^\\top R^{-1} (HLv - (y - Hx_b))\n$$\n$$\nA_v = I + L^\\top H^\\top R^{-1} H L\n$$\n\n### 3. 初始猜测点的梯度\n在初始猜测点 $x_0 = x_b$（对应 $v_0=0$），梯度为：\n$$\n\\nabla_x J(x_b) = -H^\\top R^{-1}(y - Hx_b)\n$$\n$$\n\\nabla_v J(0) = -L^\\top H^\\top R^{-1}(y - Hx_b) = L^\\top \\nabla_x J(x_b)\n$$\n\n### 4. 共轭梯度迭代次数\n找到最小整数 $k$，满足：\n$$\nk \\ge \\frac{\\ln(\\tau/2)}{\\ln\\left(\\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1}\\right)}\n$$\n其中 $\\kappa(A)$ 是相关 Hessian 矩阵的条件数，$\\tau=10^{-6}$。\n\n基于这些公式，我们对每个测试案例进行计算。",
            "answer": "[[1.647754,3.905660,10,17,6.000000,3.328283],[65.000000,17.000000,65,34,8.077747,10.024969],[10001.000000,1421.311709,708,268,5.048020,20.372551]]"
        }
    ]
}