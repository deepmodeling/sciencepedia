{
    "hands_on_practices": [
        {
            "introduction": "Before an adjoint model can be trusted to compute the gradient of a cost function, its correctness must be rigorously verified. The dot-product test is the gold standard for this verification, providing a necessary condition that the discrete adjoint operator is the true transpose of the tangent linear operator. This practice will guide you through the fundamental workflow of deriving the tangent linear and adjoint models for a simplified radiative transfer operator and implementing this crucial diagnostic test to ensure their consistency .",
            "id": "4103767",
            "problem": "Construct a self-contained program that performs a dot-product test for a discrete Tangent Linear (TL) and Adjoint (ADJ) pair derived from a simplified $2$-layer, plane-parallel, non-scattering, upwelling radiative transfer operator under the Beer–Lambert law. Work under the Euclidean inner product, and verify discrete adjoint consistency. The program must implement the following from first principles.\n\nFundamental base and forward operator: Use the Beer–Lambert law for transmittance and linear superposition of layer emission. Consider $2$ homogeneous layers stacked above an emitting surface. Let the state vector be $x = [\\tau_1,\\tau_2,S_1,S_2]^\\top$, where $\\tau_1$ and $\\tau_2$ are nonnegative optical depths of the lower and upper layer respectively, and $S_1$ and $S_2$ are their corresponding layer source terms (radiances). Let $E$ denote the surface emission (radiance). For a set of viewing direction cosines $\\mu_k \\in (0,1]$ indexed by $k$, define the transmittances $T_{1,k} = \\exp(-\\tau_1/\\mu_k)$ and $T_{2,k} = \\exp(-\\tau_2/\\mu_k)$. The forward operator $F: \\mathbb{R}^4 \\to \\mathbb{R}^K$ maps the state to the top-of-atmosphere upwelling radiance vector $L \\in \\mathbb{R}^K$ with components\n$$\nL_k(x) = T_{1,k} T_{2,k} E + T_{1,k} \\left(1 - T_{2,k}\\right) S_2 + \\left(1 - T_{1,k}\\right) S_1,\\quad k=1,\\dots,K.\n$$\n\nTangent Linear (TL) and Adjoint (ADJ): The Tangent Linear operator at a base state $x_0$ is the Jacobian action $J(x_0)\\,\\delta x$ for a perturbation $\\delta x \\in \\mathbb{R}^4$. The Adjoint operator is the transpose action $J(x_0)^\\top\\,y$ for $y \\in \\mathbb{R}^K$ under the Euclidean inner product. Discrete adjoint consistency requires\n$$\n\\langle J(x_0)\\,\\delta x,\\; y \\rangle = \\langle \\delta x,\\; J(x_0)^\\top\\,y \\rangle\n$$\nfor all $\\delta x \\in \\mathbb{R}^4$ and $y \\in \\mathbb{R}^K$, where $\\langle \\cdot,\\cdot\\rangle$ denotes the standard Euclidean inner product. You must derive and implement the analytic TL and ADJ pair from the forward operator defined above.\n\nNumerical dot-product test: For each test case, compute the scalar quantities\n$$\n\\text{lhs} = \\langle J(x_0)\\,\\delta x,\\; y \\rangle,\\quad\n\\text{rhs} = \\langle \\delta x,\\; J(x_0)^\\top\\,y \\rangle,\n$$\nthen report the relative discrepancy\n$$\n\\varepsilon = \\frac{\\left|\\text{lhs} - \\text{rhs}\\right|}{\\max\\left(1,\\left|\\text{lhs}\\right|,\\left|\\text{rhs}\\right|\\right)}.\n$$\nEach reported $\\varepsilon$ is unitless and must be expressed as a floating-point value.\n\nAngle and unit conventions: Use direction cosines $\\mu_k$ directly (no angles are provided, so no angle unit specification is required). Radiances $\\left(S_1,S_2,E\\right)$ and the forward operator outputs $L_k$ have consistent radiance units, but only the unitless relative discrepancies $\\varepsilon$ must be output.\n\nTest suite: Implement the dot-product test for the following parameter sets, each defining the base state $x_0$, surface emission $E$, viewing direction cosines $\\{\\mu_k\\}_{k=1}^K$, a perturbation $\\delta x$, and a weighting vector $y$ in observation space.\n\n- Case $1$ (happy path):\n  - $x_0 = [\\tau_1,\\tau_2,S_1,S_2] = [0.7, 1.3, 260.0, 280.0]$\n  - $E = 300.0$\n  - $\\mu = [1.0, 0.8, 0.5]$\n  - $\\delta x = [0.01, -0.02, 1.5, -0.5]$\n  - $y = [0.7, -1.2, 0.3]$\n- Case $2$ (nearly transparent layers):\n  - $x_0 = [1\\times 10^{-4}, 2\\times 10^{-4}, 250.0, 250.0]$\n  - $E = 250.0$\n  - $\\mu = [1.0]$\n  - $\\delta x = [1\\times 10^{-6}, -2\\times 10^{-6}, 0.1, -0.1]$\n  - $y = [2.0]$\n- Case $3$ (opaque layers and slant views):\n  - $x_0 = [10.0, 12.0, 280.0, 290.0]$\n  - $E = 300.0$\n  - $\\mu = [0.2, 0.1]$\n  - $\\delta x = [-0.05, 0.04, -1.0, 2.0]$\n  - $y = [1.0, -0.5]$\n- Case $4$ (zero perturbation edge case):\n  - $x_0 = [0.7, 1.3, 260.0, 280.0]$\n  - $E = 290.0$\n  - $\\mu = [0.3, 0.6, 1.0]$\n  - $\\delta x = [0.0, 0.0, 0.0, 0.0]$\n  - $y = [0.3, 0.4, -0.1]$\n- Case $5$ (zero observation weights edge case):\n  - $x_0 = [0.9, 0.4, 265.0, 275.0]$\n  - $E = 285.0$\n  - $\\mu = [0.9, 0.7, 0.4, 0.2]$\n  - $\\delta x = [0.03, 0.02, -3.0, 1.0]$\n  - $y = [0.0, 0.0, 0.0, 0.0]$\n\nProgram requirements and output specification:\n- Implement the TL and ADJ operators analytically from the forward operator at each test case’s base state $x_0$.\n- For each case, compute and collect the unitless relative discrepancy $\\varepsilon$ defined above as a floating-point value.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4,r_5]$, where $r_i$ corresponds to case $i$ in the order listed.\n- The program must be self-contained, require no input, and use deterministic values exactly as specified above.",
            "solution": "We derive the Tangent Linear (TL) and Adjoint (ADJ) operators for the given $2$-layer upwelling radiative transfer operator starting from the Beer–Lambert law and linear emission superposition. Let the state be $x = [\\tau_1,\\tau_2,S_1,S_2]^\\top$, the surface emission be $E$, and the viewing direction cosines be $\\{\\mu_k\\}_{k=1}^K$ with $\\mu_k \\in (0,1]$. Define transmittances $T_{1,k} = \\exp(-\\tau_1/\\mu_k)$ and $T_{2,k} = \\exp(-\\tau_2/\\mu_k)$. The forward map $F:\\mathbb{R}^4 \\to \\mathbb{R}^K$ is component-wise\n$$\nL_k(x) = T_{1,k} T_{2,k} E + T_{1,k} \\left(1 - T_{2,k}\\right) S_2 + \\left(1 - T_{1,k}\\right) S_1.\n$$\nThis construction uses the Beer–Lambert law, which states that along a path with direction cosine $\\mu_k$, transmittance through a layer of optical depth $\\tau$ is $\\exp(-\\tau/\\mu_k)$, and assumes linear addition of isotropic emission from each layer, attenuated by overlying transmittances.\n\nTo construct the Tangent Linear operator, we compute the Jacobian $\\partial L_k/\\partial x_j$ at a base state $x_0$. Let $a_k \\equiv 1/\\mu_k$ and abbreviate $T_{1,k} \\equiv \\exp(-\\tau_1 a_k)$ and $T_{2,k} \\equiv \\exp(-\\tau_2 a_k)$ evaluated at $x_0$. The partial derivatives of $L_k$ with respect to the state components are\n$$\n\\frac{\\partial L_k}{\\partial S_1} = 1 - T_{1,k},\n\\quad\n\\frac{\\partial L_k}{\\partial S_2} = T_{1,k}\\left(1 - T_{2,k}\\right),\n$$\nand using $\\frac{\\partial T_{1,k}}{\\partial \\tau_1} = -a_k T_{1,k}$ and $\\frac{\\partial T_{2,k}}{\\partial \\tau_2} = -a_k T_{2,k}$,\n$$\n\\frac{\\partial L_k}{\\partial \\tau_1}\n= \\frac{\\partial T_{1,k}}{\\partial \\tau_1}\\,T_{2,k} E + \\frac{\\partial T_{1,k}}{\\partial \\tau_1}\\,\\left(1 - T_{2,k}\\right) S_2 - \\frac{\\partial T_{1,k}}{\\partial \\tau_1}\\,S_1\n= -a_k T_{1,k} T_{2,k} E - a_k T_{1,k}\\left(1 - T_{2,k}\\right) S_2 + a_k T_{1,k} S_1,\n$$\n$$\n\\frac{\\partial L_k}{\\partial \\tau_2}\n= T_{1,k}\\frac{\\partial T_{2,k}}{\\partial \\tau_2} E + T_{1,k}\\left(-\\frac{\\partial T_{2,k}}{\\partial \\tau_2}\\right) S_2\n= -a_k T_{1,k} T_{2,k} E + a_k T_{1,k} T_{2,k} S_2\n= a_k T_{1,k} T_{2,k}\\left(-E + S_2\\right).\n$$\nCollect these into the Jacobian matrix $J \\in \\mathbb{R}^{K \\times 4}$ with columns corresponding to $[\\tau_1,\\tau_2,S_1,S_2]$. For a perturbation $\\delta x = [\\delta \\tau_1,\\delta \\tau_2,\\delta S_1,\\delta S_2]^\\top$, the TL action is the matrix-vector product\n$$\n\\left(J\\,\\delta x\\right)_k\n= \\frac{\\partial L_k}{\\partial \\tau_1}\\,\\delta \\tau_1\n+ \\frac{\\partial L_k}{\\partial \\tau_2}\\,\\delta \\tau_2\n+ \\frac{\\partial L_k}{\\partial S_1}\\,\\delta S_1\n+ \\frac{\\partial L_k}{\\partial S_2}\\,\\delta S_2.\n$$\n\nUnder the Euclidean inner product $\\langle u,v\\rangle = \\sum_{i} u_i v_i$, the Adjoint operator is the transpose $J^\\top$. For any $y \\in \\mathbb{R}^K$,\n$$\n\\left(J^\\top y\\right)_1 = \\sum_{k=1}^K y_k \\frac{\\partial L_k}{\\partial \\tau_1},\\quad\n\\left(J^\\top y\\right)_2 = \\sum_{k=1}^K y_k \\frac{\\partial L_k}{\\partial \\tau_2},\\quad\n\\left(J^\\top y\\right)_3 = \\sum_{k=1}^K y_k \\frac{\\partial L_k}{\\partial S_1},\\quad\n\\left(J^\\top y\\right)_4 = \\sum_{k=1}^K y_k \\frac{\\partial L_k}{\\partial S_2}.\n$$\nBy construction of matrix transpose under the Euclidean inner product, we have the discrete adjoint identity\n$$\n\\langle J\\,\\delta x,\\; y\\rangle = \\langle \\delta x,\\; J^\\top y\\rangle\n$$\nfor all $\\delta x$ and $y$. Our numerical dot-product test evaluates this identity by computing\n$$\n\\text{lhs} = \\sum_{k=1}^K \\left(J\\,\\delta x\\right)_k\\, y_k,\\quad\n\\text{rhs} = \\sum_{j=1}^4 \\delta x_j \\left(J^\\top y\\right)_j,\n$$\nand the unitless relative discrepancy\n$$\n\\varepsilon = \\frac{\\left|\\text{lhs} - \\text{rhs}\\right|}{\\max\\left(1,\\left|\\text{lhs}\\right|,\\left|\\text{rhs}\\right|\\right)}.\n$$\n\nAlgorithmic design:\n- For each test case, evaluate $T_{1,k}$ and $T_{2,k}$ at $x_0$ and compute the partial derivatives $\\partial L_k/\\partial x_j$ for $j \\in \\{1,2,3,4\\}$ as above.\n- Compute the TL action $J\\,\\delta x$ via the weighted sum over columns using the components of $\\delta x$.\n- Compute the ADJ action $J^\\top y$ via inner products of each derivative column with $y$.\n- Form $\\text{lhs}$ and $\\text{rhs}$ using Euclidean inner products and report $\\varepsilon$ using the provided normalization to remain robust across scales, including the edge cases where either vector is the zero vector.\n- Repeat for all specified cases and output the list $[\\varepsilon_1,\\varepsilon_2,\\varepsilon_3,\\varepsilon_4,\\varepsilon_5]$ on a single line.\n\nBecause the TL and ADJ are implemented from exact analytic derivatives of a smooth finite-dimensional operator in double-precision arithmetic, the discrepancies $\\varepsilon$ should be on the order of floating-point round-off (typically near $10^{-15}$ to $10^{-16}$), with exactly zero for the degenerate cases where either the perturbation or the observation weights are the zero vector by construction of the normalization.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef tl_action_and_adjoint_rel_error(tau1, tau2, S1, S2, E, mu, dx, y):\n    \"\"\"\n    Compute the relative discrepancy epsilon between <J dx, y> and <dx, J^T y>\n    for the two-layer radiative transfer operator defined in the problem.\n    \"\"\"\n    mu = np.asarray(mu, dtype=np.float64)\n    dx = np.asarray(dx, dtype=np.float64)  # shape (4,)\n    y = np.asarray(y, dtype=np.float64)    # shape (K,)\n\n    # Avoid division by zero; mu values are guaranteed > 0 in test cases.\n    a = 1.0 / mu  # shape (K,)\n\n    # Transmittances at base state\n    T1 = np.exp(-tau1 * a)\n    T2 = np.exp(-tau2 * a)\n\n    # Partial derivatives for each k\n    dL_dS1 = 1.0 - T1                          # shape (K,)\n    dL_dS2 = T1 * (1.0 - T2)                   # shape (K,)\n    dL_dtau1 = -a * T1 * T2 * E - a * T1 * (1.0 - T2) * S2 + a * T1 * S1\n    dL_dtau2 = -a * T1 * T2 * E + a * T1 * T2 * S2\n\n    # TL action: J dx\n    Jdx = dL_dtau1 * dx[0] + dL_dtau2 * dx[1] + dL_dS1 * dx[2] + dL_dS2 * dx[3]\n\n    # Adjoint action: J^T y\n    JT_y = np.array([\n        np.dot(dL_dtau1, y),\n        np.dot(dL_dtau2, y),\n        np.dot(dL_dS1, y),\n        np.dot(dL_dS2, y)\n    ], dtype=np.float64)\n\n    lhs = float(np.dot(Jdx, y))\n    rhs = float(np.dot(dx, JT_y))\n\n    denom = max(1.0, abs(lhs), abs(rhs))\n    rel_err = abs(lhs - rhs) / denom\n    return rel_err\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (tau1, tau2, S1, S2, E, mu_list, dx, y)\n    test_cases = [\n        (0.7, 1.3, 260.0, 280.0, 300.0, [1.0, 0.8, 0.5], [0.01, -0.02, 1.5, -0.5], [0.7, -1.2, 0.3]),\n        (1e-4, 2e-4, 250.0, 250.0, 250.0, [1.0], [1e-6, -2e-6, 0.1, -0.1], [2.0]),\n        (10.0, 12.0, 280.0, 290.0, 300.0, [0.2, 0.1], [-0.05, 0.04, -1.0, 2.0], [1.0, -0.5]),\n        (0.7, 1.3, 260.0, 280.0, 290.0, [0.3, 0.6, 1.0], [0.0, 0.0, 0.0, 0.0], [0.3, 0.4, -0.1]),\n        (0.9, 0.4, 265.0, 275.0, 285.0, [0.9, 0.7, 0.4, 0.2], [0.03, 0.02, -3.0, 1.0], [0.0, 0.0, 0.0, 0.0]),\n    ]\n\n    results = []\n    for case in test_cases:\n        tau1, tau2, S1, S2, E, mu, dx, y = case\n        err = tl_action_and_adjoint_rel_error(tau1, tau2, S1, S2, E, mu, dx, y)\n        results.append(err)\n\n    # Final print statement in the exact required format (no spaces).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With verified tangent linear (TL) and adjoint (ADJ) operators, we can assemble the key components of the variational minimization problem. This exercise demonstrates how these operators are used to construct the Gauss-Newton approximation of the Hessian matrix, a critical piece of the optimization puzzle. Understanding the Hessian provides insight into the curvature of the cost function landscape, which governs the convergence properties of minimization algorithms and forms the basis for powerful preconditioning techniques .",
            "id": "4103712",
            "problem": "Consider the standard variational data assimilation cost function used in Four-Dimensional Variational (4D-Var) problems in Numerical Weather Prediction (NWP) and climate modeling, defined on a state vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ and an observation vector $\\mathbf{y} \\in \\mathbb{R}^{m}$, with positive definite background-error covariance matrix $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$, positive definite observation-error covariance matrix $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$, and a differentiable observation operator $\\mathbf{H} : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$. Let the iterate be $\\mathbf{x}^{k}$. The tangent-linear operator at $\\mathbf{x}^{k}$ is the Jacobian $\\mathbf{H}'(\\mathbf{x}^{k})$, and its adjoint is $\\mathbf{H}'(\\mathbf{x}^{k})^{T}$.\n\nStarting from the definitions above and basic multivariable calculus (Taylor expansion and the chain rule), derive the Gauss–Newton approximation to the Hessian of the cost function evaluated at $\\mathbf{x}^{k}$ and express it in terms of the background term and the observation term involving the tangent-linear and its adjoint.\n\nThen, evaluate this expression for the specific, dimensionally consistent case where:\n- The state dimension is $n=2$ and the observation dimension is $m=2$.\n- The background-error covariance is $\\mathbf{B} = \\mathrm{diag}(2,\\,1)$.\n- The observation-error covariance is $\\mathbf{R} = \\mathrm{diag}\\!\\left(\\frac{1}{4},\\,1\\right)$.\n- The observation operator is $\\mathbf{H}(\\mathbf{x}) = \\begin{pmatrix} x_{1}^{2} + x_{2} \\\\ \\sin(x_{1}) + x_{2} \\end{pmatrix}$.\n- The iterate is $\\mathbf{x}^{k} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nCompute the Gauss–Newton approximation to the Hessian matrix at $\\mathbf{x}^{k}$ for this case and provide the explicit matrix. If trigonometric terms arise, leave them in exact form. No rounding is required. Express the final answer as the matrix without units.",
            "solution": "The Four-Dimensional Variational (4D-Var) cost function is defined as\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_{b})^{T}\\,\\mathbf{B}^{-1}\\,(\\mathbf{x} - \\mathbf{x}_{b}) + \\frac{1}{2}\\,\\big(\\mathbf{H}(\\mathbf{x}) - \\mathbf{y}\\big)^{T}\\,\\mathbf{R}^{-1}\\,\\big(\\mathbf{H}(\\mathbf{x}) - \\mathbf{y}\\big),\n$$\nwhere $\\mathbf{x}_{b}$ is the background (prior) state.\n\nFrom multivariable calculus, the gradient of $J$ with respect to $\\mathbf{x}$ is obtained by differentiating each term. The background term is quadratic, so it contributes\n$$\n\\nabla_{\\mathbf{x}} \\left[ \\frac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_{b})^{T}\\,\\mathbf{B}^{-1}\\,(\\mathbf{x} - \\mathbf{x}_{b}) \\right] = \\mathbf{B}^{-1}\\,(\\mathbf{x} - \\mathbf{x}_{b}).\n$$\nFor the observation term, apply the chain rule. Let $\\mathbf{d}(\\mathbf{x}) \\equiv \\mathbf{H}(\\mathbf{x}) - \\mathbf{y}$. Then\n$$\n\\nabla_{\\mathbf{x}} \\left[ \\frac{1}{2}\\,\\mathbf{d}(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{d}(\\mathbf{x}) \\right] = \\mathbf{H}'(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{d}(\\mathbf{x}),\n$$\nwhere $\\mathbf{H}'(\\mathbf{x})$ is the Jacobian (tangent-linear operator) of $\\mathbf{H}$ evaluated at $\\mathbf{x}$, and $\\mathbf{H}'(\\mathbf{x})^{T}$ is its adjoint.\n\nTherefore, the gradient is\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}\\,(\\mathbf{x} - \\mathbf{x}_{b}) + \\mathbf{H}'(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\big(\\mathbf{H}(\\mathbf{x}) - \\mathbf{y}\\big).\n$$\n\nTo obtain the Hessian, differentiate the gradient with respect to $\\mathbf{x}$. The Hessian of the background term is simply\n$$\n\\nabla^{2}\\left[ \\frac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_{b})^{T}\\,\\mathbf{B}^{-1}\\,(\\mathbf{x} - \\mathbf{x}_{b}) \\right] = \\mathbf{B}^{-1}.\n$$\nFor the observation term, differentiate $\\mathbf{H}'(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\big(\\mathbf{H}(\\mathbf{x}) - \\mathbf{y}\\big)$. Using product and chain rules for Jacobians,\n$$\n\\nabla_{\\mathbf{x}}\\left[ \\mathbf{H}'(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\big(\\mathbf{H}(\\mathbf{x}) - \\mathbf{y}\\big) \\right]\n= \\mathbf{H}'(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}) + \\sum_{i=1}^{m} \\left[ \\nabla^{2} H_{i}(\\mathbf{x}) \\right]\\,\\big(\\mathbf{R}^{-1}\\,\\mathbf{d}(\\mathbf{x})\\big)_{i},\n$$\nwhere $\\nabla^{2} H_{i}(\\mathbf{x})$ denotes the Hessian of the scalar function $H_{i}(\\mathbf{x})$, and $(\\cdot)_{i}$ denotes the $i$-th component. The first term,\n$$\n\\mathbf{H}'(\\mathbf{x})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}),\n$$\narises from differentiating $\\mathbf{H}(\\mathbf{x})$ inside the residual, while the second term collects second derivatives of $\\mathbf{H}$ weighted by the residuals and $\\mathbf{R}^{-1}$.\n\nThe Gauss–Newton approximation to the Hessian neglects the second-order term involving $\\nabla^{2} H_{i}(\\mathbf{x})$ and the residual. This is justified near a solution where the residuals are small, and it is a standard approximation in nonlinear least-squares and variational data assimilation. Thus, at $\\mathbf{x}^{k}$, the Gauss–Newton Hessian is\n$$\n\\mathbf{G}(\\mathbf{x}^{k}) \\approx \\mathbf{B}^{-1} + \\mathbf{H}'(\\mathbf{x}^{k})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}^{k}).\n$$\n\nWe now evaluate this expression for the specified case. The given matrices are\n$$\n\\mathbf{B} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad \\mathbf{R} = \\begin{pmatrix} \\frac{1}{4} & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\mathbf{B}^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad \\mathbf{R}^{-1} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\n\nThe observation operator is\n$$\n\\mathbf{H}(\\mathbf{x}) = \\begin{pmatrix} H_{1}(\\mathbf{x}) \\\\ H_{2}(\\mathbf{x}) \\end{pmatrix} = \\begin{pmatrix} x_{1}^{2} + x_{2} \\\\ \\sin(x_{1}) + x_{2} \\end{pmatrix}.\n$$\nIts Jacobian (tangent-linear) is\n$$\n\\mathbf{H}'(\\mathbf{x}) = \\begin{pmatrix}\n\\frac{\\partial H_{1}}{\\partial x_{1}} & \\frac{\\partial H_{1}}{\\partial x_{2}} \\\\\n\\frac{\\partial H_{2}}{\\partial x_{1}} & \\frac{\\partial H_{2}}{\\partial x_{2}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 x_{1} & 1 \\\\\n\\cos(x_{1}) & 1\n\\end{pmatrix}.\n$$\nAt the iterate $\\mathbf{x}^{k} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, this becomes\n$$\n\\mathbf{H}'(\\mathbf{x}^{k}) = \\begin{pmatrix} 2 & 1 \\\\ \\cos(1) & 1 \\end{pmatrix}.\n$$\n\nCompute $\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}^{k})$:\n$$\n\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}^{k}) = \n\\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 2 & 1 \\\\ \\cos(1) & 1 \\end{pmatrix}\n=\n\\begin{pmatrix} 8 & 4 \\\\ \\cos(1) & 1 \\end{pmatrix}.\n$$\nThen\n$$\n\\mathbf{H}'(\\mathbf{x}^{k})^{T}\\,\\big(\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}^{k})\\big) =\n\\begin{pmatrix} 2 & \\cos(1) \\\\ 1 & 1 \\end{pmatrix}\n\\begin{pmatrix} 8 & 4 \\\\ \\cos(1) & 1 \\end{pmatrix}\n=\n\\begin{pmatrix}\n16 + \\cos^{2}(1) & 8 + \\cos(1) \\\\\n8 + \\cos(1) & 5\n\\end{pmatrix}.\n$$\n\nFinally, add $\\mathbf{B}^{-1}$:\n$$\n\\mathbf{G}(\\mathbf{x}^{k}) \\approx \\mathbf{B}^{-1} + \\mathbf{H}'(\\mathbf{x}^{k})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}^{k})\n=\n\\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 1 \\end{pmatrix}\n+\n\\begin{pmatrix}\n16 + \\cos^{2}(1) & 8 + \\cos(1) \\\\\n8 + \\cos(1) & 5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{33}{2} + \\cos^{2}(1) & 8 + \\cos(1) \\\\\n8 + \\cos(1) & 6\n\\end{pmatrix}.\n$$\n\nThis matrix is the Gauss–Newton approximation to the Hessian of the variational cost function at the given iterate, explicitly exhibiting the decomposition into the background term $\\mathbf{B}^{-1}$ and the observation term involving the tangent-linear operator and its adjoint, $\\mathbf{H}'(\\mathbf{x}^{k})^{T}\\,\\mathbf{R}^{-1}\\,\\mathbf{H}'(\\mathbf{x}^{k})$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{33}{2}+\\cos^{2}(1) & 8+\\cos(1) \\\\ 8+\\cos(1) & 6\\end{pmatrix}}$$"
        },
        {
            "introduction": "The utility of tangent linear models extends beyond calculating gradients for optimization; they are also powerful tools for sensitivity and uncertainty analysis. This practice explores how the TL operator connects the model's parameters to the observations, allowing for an assessment of parameter identifiability. You will construct the Fisher Information Matrix to diagnose which parameters can be effectively constrained by the observing system and quantify the posterior uncertainty of your parameter estimates .",
            "id": "4103682",
            "problem": "You are given a linearized data assimilation setting suitable for Numerical Weather Prediction and Climate Modeling, in which unknown model parameters are inferred from noisy observations using a variational framework. Let the parameter vector be $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$, the observation operator be $\\boldsymbol{h}(\\cdot)$ mapping a model state to observation space, the available observations be $\\boldsymbol{y} \\in \\mathbb{R}^m$, the observation error covariance be $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$, the background (prior) mean be $\\boldsymbol{\\theta}_b \\in \\mathbb{R}^n$, and the prior covariance be $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$. The standard variational cost function is\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\left(\\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta})) - \\boldsymbol{y}\\right)^\\top \\boldsymbol{R}^{-1} \\left(\\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta})) - \\boldsymbol{y}\\right) + \\frac{1}{2}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_b\\right)^\\top \\boldsymbol{B}^{-1}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_b\\right),\n$$\nwhere $\\boldsymbol{s}(\\boldsymbol{\\theta})$ denotes the model state induced by $\\boldsymbol{\\theta}$. Around a linearization point $\\boldsymbol{\\theta}_0$, the tangent linear (TL) operator of the map $\\boldsymbol{\\theta} \\mapsto \\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta}))$ is the matrix $\\boldsymbol{G} \\in \\mathbb{R}^{m \\times n}$ satisfying\n$$\n\\delta \\boldsymbol{y} \\approx \\boldsymbol{G}\\,\\delta \\boldsymbol{\\theta},\n$$\nwhere $\\delta \\boldsymbol{\\theta}$ and $\\delta \\boldsymbol{y}$ are small perturbations. The adjoint operator $\\boldsymbol{G}^\\star$ is defined with respect to the Euclidean inner product by the property\n$$\n\\langle \\boldsymbol{G}\\,\\delta \\boldsymbol{\\theta},\\,\\delta \\boldsymbol{y} \\rangle = \\langle \\delta \\boldsymbol{\\theta},\\, \\boldsymbol{G}^\\star\\,\\delta \\boldsymbol{y} \\rangle\n$$\nfor all $\\delta \\boldsymbol{\\theta} \\in \\mathbb{R}^n$ and $\\delta \\boldsymbol{y} \\in \\mathbb{R}^m$.\n\nStarting from the definitions above, and using the linearization and adjoint relationships, derive an algorithm to:\n- validate the adjoint consistency of the provided TL and adjoint outputs via the inner-product identity;\n- compute the Fisher information matrix for the parameters from the TL/adjoint outputs and the observation error covariance;\n- assess parameter identifiability by the rank of the Fisher information matrix relative to the parameter dimension;\n- quantify parameter uncertainty by the diagonal of the posterior covariance obtained from the prior and the Fisher information.\n\nNo physical units are involved in this problem; all quantities are dimensionless. All angles are irrelevant. You should express all floating-point results rounded to six decimal places. The program must produce boolean, integer, and floating-point outputs only.\n\nImplement your algorithm for the following test suite of cases. In each case, you are given the tangent linear matrix $\\boldsymbol{G}^{(k)}$, the adjoint matrix $(\\boldsymbol{G}^{(k)})^\\star$ (which, under the Euclidean inner product, coincides with the transpose), the observation error covariance diagonal entries (so that $\\boldsymbol{R}^{(k)} = \\mathrm{diag}(\\cdot)$), and the prior covariance diagonal entries (so that $\\boldsymbol{B}^{(k)} = \\mathrm{diag}(\\cdot)$). Use the specified random seeds for the adjoint consistency tests.\n\nCase $1$:\n- $\\boldsymbol{G}^{(1)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$,\n- $(\\boldsymbol{G}^{(1)})^\\star = \\left(\\boldsymbol{G}^{(1)}\\right)^\\top$,\n- $\\boldsymbol{R}^{(1)} = \\mathrm{diag}(0.25, 0.25, 1.0)$,\n- $\\boldsymbol{B}^{(1)} = \\mathrm{diag}(1.0, 1.0)$,\n- random seed $0$.\n\nCase $2$:\n- $\\boldsymbol{G}^{(2)} = \\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix}$,\n- $(\\boldsymbol{G}^{(2)})^\\star = \\left(\\boldsymbol{G}^{(2)}\\right)^\\top$,\n- $\\boldsymbol{R}^{(2)} = \\mathrm{diag}(1.0, 1.0)$,\n- $\\boldsymbol{B}^{(2)} = \\mathrm{diag}(100.0, 100.0, 100.0)$,\n- random seed $1$.\n\nCase $3$:\n- $\\boldsymbol{G}^{(3)} = \\begin{bmatrix} 1.0 & 0.999 & 0.001 \\\\ 0.999 & 0.998 & 0.002 \\\\ 0.001 & 0.002 & 0.999 \\end{bmatrix}$,\n- $(\\boldsymbol{G}^{(3)})^\\star = \\left(\\boldsymbol{G}^{(3)}\\right)^\\top$,\n- $\\boldsymbol{R}^{(3)} = \\mathrm{diag}(0.1, 0.1, 10.0)$,\n- $\\boldsymbol{B}^{(3)} = \\mathrm{diag}(1.0, 1.0, 1.0)$,\n- random seed $2$.\n\nFor each case $k \\in \\{1,2,3\\}$, perform the following computations:\n- Validate the adjoint identity by checking whether\n$$\n\\left|\\langle \\boldsymbol{G}^{(k)}\\,\\delta \\boldsymbol{\\theta},\\,\\delta \\boldsymbol{y} \\rangle - \\langle \\delta \\boldsymbol{\\theta},\\,(\\boldsymbol{G}^{(k)})^\\star\\,\\delta \\boldsymbol{y} \\rangle\\right| \\le \\varepsilon\n$$\nholds with tolerance $\\varepsilon = 10^{-10}$ for random vectors $\\delta \\boldsymbol{\\theta}$ and $\\delta \\boldsymbol{y}$ generated from a standard normal distribution using the specified seed. Report a boolean indicating pass or fail.\n- Compute the Fisher information matrix from the TL/adjoint outputs and $\\boldsymbol{R}^{(k)}$, then compute its matrix rank and the corresponding identifiability boolean indicating whether all parameters are identifiable (i.e., rank equals $n$).\n- Compute the spectral condition number (in the $2$-norm) of the Fisher information matrix; if the matrix is singular, report an infinite value.\n- Compute the posterior covariance $\\boldsymbol{\\Sigma}_{\\text{post}}^{(k)}$ formed by combining the prior and the Fisher information, and return the diagonal entries of $\\boldsymbol{\\Sigma}_{\\text{post}}^{(k)}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the entry for each case is itself a list of the form\n$$\n[\\text{adjoint\\_ok},\\ \\text{rank},\\ \\text{cond},\\ [\\text{var}_1,\\ \\ldots,\\ \\text{var}_n]],\n$$\nwith all floating-point values rounded to six decimal places (for infinite values, use the language default float infinity). For example, the output should look like\n$$\n[[\\text{True},\\ 2,\\ 1.500000,\\ [0.187500,\\ 0.187500]],\\ \\ldots].\n$$",
            "solution": "The user has provided a problem statement that is scientifically grounded, well-posed, objective, and self-contained. It is rooted in the established principles of variational data assimilation, Bayesian inference, and linear algebra, which are fundamental to numerical weather prediction and geophysical modeling. All definitions, data, and tasks are clear, precise, and internally consistent. There are no violations of scientific principles, logical contradictions, or ambiguities. The problem is therefore deemed **valid**.\n\nThe task is to perform a series of analyses for three distinct data assimilation scenarios. This involves validating the adjoint relationship, assessing parameter identifiability and numerical conditioning via the Fisher Information Matrix (FIM), and quantifying parameter uncertainty using the posterior covariance. The logical flow of the algorithm is as follows:\n\n_Step 1: Adjoint Consistency Validation_\nThe definition of the adjoint operator $\\boldsymbol{G}^\\star$ with respect to the Euclidean inner product is given by the relation $\\langle \\boldsymbol{G}\\,\\delta \\boldsymbol{\\theta},\\,\\delta \\boldsymbol{y} \\rangle = \\langle \\delta \\boldsymbol{\\theta},\\, \\boldsymbol{G}^\\star\\,\\delta \\boldsymbol{y} \\rangle$. For real-valued vectors and matrices, the Euclidean inner product is the dot product, $\\langle \\boldsymbol{a}, \\boldsymbol{b} \\rangle = \\boldsymbol{a}^\\top \\boldsymbol{b}$. This implies that the adjoint $\\boldsymbol{G}^\\star$ must be the transpose $\\boldsymbol{G}^\\top$. The identity can be written as $(\\boldsymbol{G}\\,\\delta \\boldsymbol{\\theta})^\\top \\delta \\boldsymbol{y} = (\\delta \\boldsymbol{\\theta})^\\top (\\boldsymbol{G}^\\top\\,\\delta \\boldsymbol{y})$. By the properties of matrix transposition, $(AB)^\\top = B^\\top A^\\top$, the left side is $(\\delta\\boldsymbol{\\theta})^\\top \\boldsymbol{G}^\\top \\delta \\boldsymbol{y}$, which is identical to the right side. This is a fundamental property of matrix algebra. The numerical test, therefore, serves as a \"sanity check\" on the implementation of the matrix-vector products. We test the relationship by computing the absolute difference of the two inner products for randomly generated perturbation vectors $\\delta \\boldsymbol{\\theta} \\in \\mathbb{R}^n$ and $\\delta \\boldsymbol{y} \\in \\mathbb{R}^m$, and checking if it is below a small tolerance $\\varepsilon = 10^{-10}$:\n$$\n\\left|\\langle \\boldsymbol{G}\\,\\delta \\boldsymbol{\\theta},\\,\\delta \\boldsymbol{y} \\rangle - \\langle \\delta \\boldsymbol{\\theta},\\, \\boldsymbol{G}^\\star\\,\\delta \\boldsymbol{y} \\rangle\\right| \\le \\varepsilon\n$$\nThe problem specifies that $\\boldsymbol{G}^\\star$ is indeed $\\boldsymbol{G}^\\top$, so this check is expected to pass, confirming the numerical integrity of the operations.\n\n_Step 2: Fisher Information Matrix (FIM)_\nThe Fisher Information Matrix, $\\mathcal{I}$, quantifies the amount of information that the observations $\\boldsymbol{y}$ carry about the parameters $\\boldsymbol{\\theta}$. It is derived from the observation term of the cost function, $J_o(\\boldsymbol{\\theta}) = \\frac{1}{2}\\left(\\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta})) - \\boldsymbol{y}\\right)^\\top \\boldsymbol{R}^{-1} \\left(\\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta})) - \\boldsymbol{y}\\right)$. In a linearized setting where the model response is approximated by $\\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta})) \\approx \\boldsymbol{h}(\\boldsymbol{s}(\\boldsymbol{\\theta}_0)) + \\boldsymbol{G}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)$, the Hessian of $J_o$ with respect to $\\boldsymbol{\\theta}$ is constant and given by:\n$$\n\\mathcal{I} = \\nabla^2_{\\boldsymbol{\\theta}} J_o = \\boldsymbol{G}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{G}\n$$\nThis $n \\times n$ symmetric positive semi-definite matrix is the FIM. For each case, we compute $\\mathcal{I}^{(k)} = (\\boldsymbol{G}^{(k)})^\\top (\\boldsymbol{R}^{(k)})^{-1} \\boldsymbol{G}^{(k)}$. Since $\\boldsymbol{R}^{(k)}$ is given as a diagonal matrix, its inverse $(\\boldsymbol{R}^{(k)})^{-1}$ is trivially found by taking the reciprocal of each diagonal element.\n\n_Step 3: Parameter Identifiability and Conditioning_\nThe parameters $\\boldsymbol{\\theta}$ are locally identifiable from the observations if the FIM is full rank, i.e., $\\mathrm{rank}(\\mathcal{I}) = n$, where $n$ is the number of parameters. A rank-deficient FIM indicates that some parameters or combinations of parameters have no influence on the observations and thus cannot be determined from them. We compute the rank of $\\mathcal{I}$ for each case. The identifiability is determined by comparing this rank to $n$.\n\nThe spectral condition number, $\\kappa_2(\\mathcal{I})$, of the FIM is the ratio of its largest to its smallest eigenvalue, $\\kappa_2(\\mathcal{I}) = \\lambda_{\\max} / \\lambda_{\\min}$. It measures the sensitivity of the parameter estimation problem to small changes in the data. A large condition number signifies an ill-conditioned problem, where small observational errors can lead to large errors in the estimated parameters. If the FIM is singular (rank-deficient), $\\lambda_{\\min} = 0$, and the condition number is infinite, confirming non-identifiability.\n\n_Step 4: Posterior Covariance and Uncertainty_\nThe full variational cost function $J(\\boldsymbol{\\theta})$ combines the observation term $J_o(\\boldsymbol{\\theta})$ and a background (prior) term $J_b(\\boldsymbol{\\theta}) = \\frac{1}{2}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_b\\right)^\\top \\boldsymbol{B}^{-1}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_b\\right)$. In a Bayesian framework, minimizing $J(\\boldsymbol{\\theta})$ is equivalent to finding the maximum a posteriori (MAP) estimate, assuming Gaussian error statistics. The posterior probability distribution for $\\boldsymbol{\\theta}$ is proportional to $\\exp(-J(\\boldsymbol{\\theta}))$. The inverse of the posterior covariance matrix, $\\boldsymbol{\\Sigma}_{\\text{post}}^{-1}$, is the Hessian of the full cost function:\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\nabla^2_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\nabla^2_{\\boldsymbol{\\theta}} J_o(\\boldsymbol{\\theta}) + \\nabla^2_{\\boldsymbol{\\theta}} J_b(\\boldsymbol{\\theta}) = \\boldsymbol{G}^\\top \\boldsymbol{R}^{-1} \\boldsymbol{G} + \\boldsymbol{B}^{-1} = \\mathcal{I} + \\boldsymbol{B}^{-1}\n$$\nThe posterior covariance matrix is thus given by the inverse of this \"posterior precision\" matrix:\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}} = (\\mathcal{I} + \\boldsymbol{B}^{-1})^{-1}\n$$\nThe diagonal entries of $\\boldsymbol{\\Sigma}_{\\text{post}}$ are the posterior variances of each parameter, $(\\sigma_i)^2$. These values quantify the uncertainty in the estimated parameters after combining the prior information (from $\\boldsymbol{B}$) with the information from the observations (from $\\mathcal{I}$). We compute these diagonal elements for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the data assimilation problem for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"G\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"R_diag\": np.array([0.25, 0.25, 1.0]),\n            \"B_diag\": np.array([1.0, 1.0]),\n            \"seed\": 0\n        },\n        {\n            \"G\": np.array([[1, 1, 0], [0, 1, 1]]),\n            \"R_diag\": np.array([1.0, 1.0]),\n            \"B_diag\": np.array([100.0, 100.0, 100.0]),\n            \"seed\": 1\n        },\n        {\n            \"G\": np.array([[1.0, 0.999, 0.001], [0.999, 0.998, 0.002], [0.001, 0.002, 0.999]]),\n            \"R_diag\": np.array([0.1, 0.1, 10.0]),\n            \"B_diag\": np.array([1.0, 1.0, 1.0]),\n            \"seed\": 2\n        }\n    ]\n\n    results_as_strings = []\n    for case in test_cases:\n        G = case[\"G\"]\n        R_diag = case[\"R_diag\"]\n        B_diag = case[\"B_diag\"]\n        seed = case[\"seed\"]\n\n        m, n = G.shape\n        G_star = G.T\n        \n        # Invert diagonal matrices\n        R_inv = np.diag(1.0 / R_diag)\n        B_inv = np.diag(1.0 / B_diag)\n\n        # Step 1: Adjoint consistency validation\n        rng = np.random.RandomState(seed)\n        d_theta = rng.randn(n)\n        d_y = rng.randn(m)\n\n        lhs = (G @ d_theta).T @ d_y\n        rhs = d_theta.T @ (G_star @ d_y)\n        \n        # The problem statement requires boolean output, so we explicitly cast.\n        adjoint_ok = bool(np.isclose(lhs, rhs, atol=1e-10, rtol=0))\n\n        # Step 2: Fisher Information Matrix (FIM)\n        fim = G.T @ R_inv @ G\n\n        # Step 3: Parameter identifiability and conditioning\n        rank = np.linalg.matrix_rank(fim)\n        # Note: p=2 is the default for np.linalg.cond but we specify for clarity\n        cond = np.linalg.cond(fim, p=2) \n        \n        # Step 4: Posterior Covariance and Uncertainty\n        posterior_precision = fim + B_inv\n        posterior_cov = np.linalg.inv(posterior_precision)\n        variances = np.diag(posterior_cov)\n\n        # Format results for this case\n        cond_str = f\"{cond:.6f}\" if np.isfinite(cond) else 'inf'\n        vars_list_str = ', '.join([f'{v:.6f}' for v in variances])\n        vars_str = f\"[{vars_list_str}]\"\n        \n        case_result_str = f\"[{str(adjoint_ok)}, {rank}, {cond_str}, {vars_str}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{', '.join(results_as_strings)}]\")\n\nsolve()\n```"
        }
    ]
}