{
    "hands_on_practices": [
        {
            "introduction": "The foundation of forecast verification lies in objectively comparing predictions to outcomes for discrete events. This first practice establishes this groundwork by having you construct a $2 \\times 2$ contingency table for a precipitation event, a common task in operational meteorology. By calculating fundamental metrics like the hit rate and false alarm rate, you will develop a hands-on understanding of how to quantify different aspects of forecast performance and synthesize them into a single measure of skill .",
            "id": "4044107",
            "problem": "A single-station daily precipitation verification study is conducted for a deterministic Numerical Weather Prediction (NWP) model over $N=24$ consecutive days. For each day $i$, a model forecast of $24$-hour accumulated precipitation (in millimeters) and a verifying observation (in millimeters) are available. Consider the binary event defined by a precipitation threshold of $10$ millimeters: the event occurs if and only if the $24$-hour accumulation is greater than or equal to $10$ millimeters.\n\nThe forecast–observation pairs $(f_i, o_i)$ for days $i=1,\\dots,24$ are given as follows (all values are in millimeters):\n- Day $1$: $f_1=12$, $o_1=13$\n- Day $2$: $f_2=8$, $o_2=0$\n- Day $3$: $f_3=15$, $o_3=5$\n- Day $4$: $f_4=4$, $o_4=11$\n- Day $5$: $f_5=20$, $o_5=22$\n- Day $6$: $f_6=9$, $o_6=7$\n- Day $7$: $f_7=11$, $o_7=9$\n- Day $8$: $f_8=6$, $o_8=12$\n- Day $9$: $f_9=14$, $o_9=16$\n- Day $10$: $f_{10}=7$, $o_{10}=0$\n- Day $11$: $f_{11}=10$, $o_{11}=9$\n- Day $12$: $f_{12}=3$, $o_{12}=2$\n- Day $13$: $f_{13}=13$, $o_{13}=4$\n- Day $14$: $f_{14}=2$, $o_{14}=0$\n- Day $15$: $f_{15}=18$, $o_{15}=19$\n- Day $16$: $f_{16}=0$, $o_{16}=0$\n- Day $17$: $f_{17}=16$, $o_{17}=14$\n- Day $18$: $f_{18}=5$, $o_{18}=6$\n- Day $19$: $f_{19}=9$, $o_{19}=15$\n- Day $20$: $f_{20}=10$, $o_{20}=10$\n- Day $21$: $f_{21}=11$, $o_{21}=8$\n- Day $22$: $f_{22}=1$, $o_{22}=0$\n- Day $23$: $f_{23}=7$, $o_{23}=13$\n- Day $24$: $f_{24}=13$, $o_{24}=11$\n\nTasks:\n1. Using first principles of conditional probability applied to binary forecast and observation indicators, provide precise definitions for the hit rate, false alarm rate, base rate, and precision in terms of the counts of the $2\\times 2$ contingency table and the sample size $N$.\n2. Threshold the forecasts and observations at $10$ millimeters to construct the $2\\times 2$ contingency table of counts for hits, false alarms, misses, and correct negatives.\n3. From the quantities you have defined, compute the Peirce Skill Score (also known as the True Skill Statistic) for this forecast system.\n\nExpress your final numeric answer for the Peirce Skill Score as a decimal rounded to four significant figures.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It represents a standard exercise in meteorological forecast verification. We proceed with the solution.\n\nThe solution is structured into three parts as requested by the problem statement.\n\n### Step 1: Definitions of Verification Metrics\n\nLet the binary event be denoted by $E$. The event occurs if the $24$-hour precipitation accumulation is greater than or equal to the threshold of $10$ millimeters. For each forecast-observation pair $(f_i, o_i)$, we define binary indicator variables:\n- $I_{f,i} = 1$ if the event is forecast ($f_i \\ge 10$), and $I_{f,i} = 0$ otherwise ($f_i  10$).\n- $I_{o,i} = 1$ if the event is observed ($o_i \\ge 10$), and $I_{o,i} = 0$ otherwise ($o_i  10$).\n\nThe verification is based on the four possible outcomes for each pair over the sample of $N$ days, which are summarized in a $2 \\times 2$ contingency table. The counts for each category are:\n- **Hits ($a$)**: The number of times the event was correctly forecast. $a = \\sum_{i=1}^{N} I_{f,i} I_{o,i}$.\n- **False Alarms ($b$)**: The number of times the event was forecast but did not occur. $b = \\sum_{i=1}^{N} I_{f,i} (1 - I_{o,i})$.\n- **Misses ($c$)**: The number of times the event occurred but was not forecast. $c = \\sum_{i=1}^{N} (1 - I_{f,i}) I_{o,i}$.\n- **Correct Negatives ($d$)**: The number of times the event was correctly not forecast. $d = \\sum_{i=1}^{N} (1 - I_{f,i}) (1 - I_{o,i})$.\n\nThe total number of cases is $N = a + b + c + d$.\n\nUsing these fundamental counts, we can define the requested verification metrics from the principles of conditional probability:\n\n1.  **Hit Rate ($H$)**: Also known as the Probability of Detection (POD), it is the conditional probability of a \"yes\" forecast given that the event occurred. It is the fraction of observed events that were correctly forecast.\n    $$H = P(\\text{forecast yes} | \\text{observed yes}) = \\frac{P(\\text{forecast yes} \\cap \\text{observed yes})}{P(\\text{observed yes})} = \\frac{a/N}{(a+c)/N} = \\frac{a}{a+c}$$\n\n2.  **False Alarm Rate ($F$)**: Also known as the Probability of False Detection (POFD), it is the conditional probability of a \"yes\" forecast given that the event did not occur. It is the fraction of non-events that were incorrectly forecast as events.\n    $$F = P(\\text{forecast yes} | \\text{observed no}) = \\frac{P(\\text{forecast yes} \\cap \\text{observed no})}{P(\\text{observed no})} = \\frac{b/N}{(b+d)/N} = \\frac{b}{b+d}$$\n\n3.  **Base Rate ($S$)**: Also known as the sample climatology, it is the unconditional probability of the event occurring in the sample. It is the fraction of all cases where the event was observed.\n    $$S = P(\\text{observed yes}) = \\frac{a+c}{N}$$\n\n4.  **Precision**: Also known as the Success Ratio, it is the conditional probability that the event occurred given that it was forecast. It is the fraction of \"yes\" forecasts that were correct.\n    $$\\text{Precision} = P(\\text{observed yes} | \\text{forecast yes}) = \\frac{P(\\text{observed yes} \\cap \\text{forecast yes})}{P(\\text{forecast yes})} = \\frac{a/N}{(a+b)/N} = \\frac{a}{a+b}$$\n\n### Step 2: Construction of the Contingency Table\n\nThe precipitation threshold is $10$ mm. We classify each of the $N=24$ forecast-observation pairs $(f_i, o_i)$ into one of the four categories: hit, false alarm, miss, or correct negative.\n\n- Day $1$: ($12, 13$) $\\implies f_1 \\ge 10, o_1 \\ge 10 \\implies$ **Hit**\n- Day $2$: ($8, 0$) $\\implies f_2  10, o_2  10 \\implies$ **Correct Negative**\n- Day $3$: ($15, 5$) $\\implies f_3 \\ge 10, o_3  10 \\implies$ **False Alarm**\n- Day $4$: ($4, 11$) $\\implies f_4  10, o_4 \\ge 10 \\implies$ **Miss**\n- Day $5$: ($20, 22$) $\\implies f_5 \\ge 10, o_5 \\ge 10 \\implies$ **Hit**\n- Day $6$: ($9, 7$) $\\implies f_6  10, o_6  10 \\implies$ **Correct Negative**\n- Day $7$: ($11, 9$) $\\implies f_7 \\ge 10, o_7  10 \\implies$ **False Alarm**\n- Day $8$: ($6, 12$) $\\implies f_8  10, o_8 \\ge 10 \\implies$ **Miss**\n- Day $9$: ($14, 16$) $\\implies f_9 \\ge 10, o_9 \\ge 10 \\implies$ **Hit**\n- Day $10$: ($7, 0$) $\\implies f_{10}  10, o_{10}  10 \\implies$ **Correct Negative**\n- Day $11$: ($10, 9$) $\\implies f_{11} \\ge 10, o_{11}  10 \\implies$ **False Alarm**\n- Day $12$: ($3, 2$) $\\implies f_{12}  10, o_{12}  10 \\implies$ **Correct Negative**\n- Day $13$: ($13, 4$) $\\implies f_{13} \\ge 10, o_{13}  10 \\implies$ **False Alarm**\n- Day $14$: ($2, 0$) $\\implies f_{14}  10, o_{14}  10 \\implies$ **Correct Negative**\n- Day $15$: ($18, 19$) $\\implies f_{15} \\ge 10, o_{15} \\ge 10 \\implies$ **Hit**\n- Day $16$: ($0, 0$) $\\implies f_{16}  10, o_{16}  10 \\implies$ **Correct Negative**\n- Day $17$: ($16, 14$) $\\implies f_{17} \\ge 10, o_{17} \\ge 10 \\implies$ **Hit**\n- Day $18$: ($5, 6$) $\\implies f_{18}  10, o_{18}  10 \\implies$ **Correct Negative**\n- Day $19$: ($9, 15$) $\\implies f_{19}  10, o_{19} \\ge 10 \\implies$ **Miss**\n- Day $20$: ($10, 10$) $\\implies f_{20} \\ge 10, o_{20} \\ge 10 \\implies$ **Hit**\n- Day $21$: ($11, 8$) $\\implies f_{21} \\ge 10, o_{21}  10 \\implies$ **False Alarm**\n- Day $22$: ($1, 0$) $\\implies f_{22}  10, o_{22}  10 \\implies$ **Correct Negative**\n- Day $23$: ($7, 13$) $\\implies f_{23}  10, o_{23} \\ge 10 \\implies$ **Miss**\n- Day $24$: ($13, 11$) $\\implies f_{24} \\ge 10, o_{24} \\ge 10 \\implies$ **Hit**\n\nTallying these results:\n- Hits ($a$) = $7$\n- False Alarms ($b$) = $5$\n- Misses ($c$) = $4$\n- Correct Negatives ($d$) = $8$\n\nThe total count is $a+b+c+d = 7+5+4+8 = 24$, which matches the sample size $N$. The resulting $2 \\times 2$ contingency table is:\n$$\n\\begin{array}{c|cc|c}\n\\multicolumn{2}{c}{}  \\multicolumn{2}{c}{\\text{Observed}} \\\\\n\\multicolumn{2}{c}{}  \\text{Yes (Event)}  \\text{No (Non-event)} \\\\\n\\cline{2-4}\n\\text{Forecast}  \\text{Yes}  a=7  b=5 \\\\\n \\text{No}  c=4  d=8 \\\\\n\\hline\n\\end{array}\n$$\n\n### Step 3: Computation of the Peirce Skill Score\n\nThe Peirce Skill Score (PSS), also known as the True Skill Statistic (TSS), is defined as the difference between the Hit Rate ($H$) and the False Alarm Rate ($F$):\n$$ \\text{PSS} = H - F $$\nUsing the definitions from Step 1 and the contingency table counts from Step 2:\n$$ \\text{PSS} = \\frac{a}{a+c} - \\frac{b}{b+d} $$\nSubstituting the values $a=7$, $b=5$, $c=4$, and $d=8$:\n$$ H = \\frac{7}{7+4} = \\frac{7}{11} $$\n$$ F = \\frac{5}{5+8} = \\frac{5}{13} $$\nTherefore, the Peirce Skill Score is:\n$$ \\text{PSS} = \\frac{7}{11} - \\frac{5}{13} $$\nTo calculate the exact value, we find a common denominator, which is $11 \\times 13 = 143$:\n$$ \\text{PSS} = \\frac{7 \\times 13}{143} - \\frac{5 \\times 11}{143} = \\frac{91 - 55}{143} = \\frac{36}{143} $$\nTo obtain the final numerical answer, we compute the decimal value of this fraction:\n$$ \\text{PSS} = \\frac{36}{143} \\approx 0.25174825... $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $2, 5, 1, 7$. The fifth digit is $4$, so we do not round up.\n$$ \\text{PSS} \\approx 0.2517 $$",
            "answer": "$$\\boxed{0.2517}$$"
        },
        {
            "introduction": "Modern numerical weather prediction relies heavily on ensemble systems, which produce a range of possible outcomes rather than a single deterministic forecast. To evaluate such probabilistic forecasts, we need scores that assess the entire forecast distribution, like the Continuous Ranked Probability Score (CRPS). This exercise challenges you to move from the integral definition of the CRPS to a computationally efficient algorithm, providing deep insight into how this powerful score measures the error of a probabilistic forecast in the same units as the variable itself .",
            "id": "4044072",
            "problem": "You are given an ensemble forecast for a single continuous meteorological variable, modeled as a finite set of $M$ ensemble members $\\{x_1,\\ldots,x_M\\}$, all equally likely, and a verifying observation $y \\in \\mathbb{R}$. The ensemble forecast induces an Empirical Distribution Function (EDF) $F_M(z)$ defined by $F_M(z) = \\frac{1}{M}\\sum_{i=1}^M \\mathbf{1}\\{x_i \\le z\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. The Continuous Ranked Probability Score (CRPS), for a forecast distribution with cumulative distribution function $F$ and an observation $y$, is defined from first principles by\n$$\n\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{\\infty} \\left(F(z) - \\mathbf{1}\\{z \\ge y\\}\\right)^2 \\, \\mathrm{d}z.\n$$\nStarting only from the above definitions and fundamental facts about indicator functions, integration, and summation, derive an algorithm to compute $\\mathrm{CRPS}(F_M,y)$ that runs in time $O(M)$ once the ensemble members are ordered. You may, as a preprocessing step, sort the ensemble members $\\{x_i\\}$ in ascending order, and you should count only the post-sorting computation toward the $O(M)$ complexity requirement. Your algorithm must accept any real-valued ensemble and any real-valued observation, including edge cases such as $M=1$, repeated ensemble values, and observations outside the range of the ensemble.\n\nYour program must implement the derived algorithm and compute the CRPS for each of the following test cases. The variable represents wind speed, so all CRPS outputs must be expressed in meters per second (m/s) as decimal floats. Round each CRPS to six decimal places for the final output.\n\nTest suite:\n- Case A (general, unsorted input): $M=7$, ensemble $\\{12.3, 10.1, 15.7, 9.8, 14.2, 13.0, 11.5\\}$, observation $y=12.0$.\n- Case B (degenerate ensemble): $M=1$, ensemble $\\{10.0\\}$, observation $y=12.0$.\n- Case C (observation below ensemble range): $M=4$, ensemble $\\{3.0, 4.5, 5.1, 6.2\\}$, observation $y=1.0$.\n- Case D (observation above ensemble range): $M=4$, ensemble $\\{3.0, 4.5, 5.1, 6.2\\}$, observation $y=8.0$.\n- Case E (duplicates in ensemble): $M=5$, ensemble $\\{5.0, 5.0, 5.0, 7.0, 9.0\\}$, observation $y=6.0$.\n- Case F (larger ensemble, explicit deterministic construction): $M=100$, ensemble members defined by $x_i = 0.5\\,i + \\sin(i)$ for $i=1,2,\\ldots,100$, observation $y=30.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the six CRPS values for Cases A–F, rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., \"[v_A,v_B,v_C,v_D,v_E,v_F]\"). The values must be in meters per second (m/s) and represented as decimal floats.",
            "solution": "We begin from the foundational definition of the Empirical Distribution Function (EDF) for an ensemble forecast and the Continuous Ranked Probability Score (CRPS). Let the ensemble members be $\\{x_1,\\ldots,x_M\\}$ and the observation be $y$. The EDF is defined as $F_M(z) = \\frac{1}{M} \\sum_{i=1}^M \\mathbf{1}\\{x_i \\le z\\}$. The Continuous Ranked Probability Score (CRPS) for a cumulative distribution function $F$ and an observation $y$ is defined by\n$$\n\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{\\infty} \\left(F(z) - \\mathbf{1}\\{z \\ge y\\}\\right)^2 \\, \\mathrm{d}z.\n$$\nOur goals are: derive a computationally efficient formula for $\\mathrm{CRPS}(F_M,y)$ and design an algorithm that executes in $O(M)$ time after sorting the ensemble.\n\nPrinciple-based derivation:\n1. We use only the definitions, linearity of integration and summation, and basic properties of indicator functions. Note that $F_M$ is a right-continuous, non-decreasing step function with jumps of size $1/M$ at the ensemble values. The indicator function $\\mathbf{1}\\{z \\ge y\\}$ is a unit step at $y$.\n\n2. Expand the square inside the integral:\n$$\n\\left(F_M(z) - \\mathbf{1}\\{z \\ge y\\}\\right)^2 = F_M(z)^2 - 2 F_M(z) \\mathbf{1}\\{z \\ge y\\} + \\mathbf{1}\\{z \\ge y\\}.\n$$\nIntegrating term by term and using the fact that both $F_M(z)$ and $\\mathbf{1}\\{z \\ge y\\}$ are bounded and piecewise constant yields:\n$$\n\\mathrm{CRPS}(F_M,y) = \\int_{-\\infty}^{\\infty} F_M(z)^2 \\,\\mathrm{d}z \\;-\\; 2\\int_{-\\infty}^{\\infty} F_M(z) \\mathbf{1}\\{z \\ge y\\} \\,\\mathrm{d}z \\;+\\; \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{z \\ge y\\} \\,\\mathrm{d}z.\n$$\n\n3. The last integral, $\\int_{-\\infty}^{\\infty} \\mathbf{1}\\{z \\ge y\\} \\,\\mathrm{d}z$, diverges if taken naively. However, the combination of terms is finite for distribution functions because the integrals must be interpreted as Lebesgue integrals over the real line in which the stepwise structure and cancellations are exploited. A more tractable route (and standard in verification theory) is to re-express the squared difference in terms of expectations of absolute differences. Specifically, it can be shown using indicator-layer representations and Fubini’s theorem (integrating over $z$ and then expectations over random variables) that for any cumulative distribution function $F$:\n$$\n\\mathrm{CRPS}(F,y) = \\mathbb{E}|X - y| - \\tfrac{1}{2}\\,\\mathbb{E}|X - X'|,\n$$\nwhere $X$ and $X'$ are independent random variables having cumulative distribution function $F$. This identity follows by expressing the $L^2$ norm of the difference of two cumulative distribution functions as the expected absolute difference between draws from these distributions and is derived rigorously by representing $F(z)$ and $\\mathbf{1}\\{z \\ge y\\}$ as expectations of indicator functions, expanding the square, and applying Fubini’s theorem to convert integrals of indicator differences into expectations of absolute differences.\n\n4. Applying this identity to the EDF $F_M$ yields a discrete distribution supported on $\\{x_1,\\ldots,x_M\\}$ with equal mass $1/M$ at each point. Therefore:\n$$\n\\mathbb{E}|X - y| = \\frac{1}{M}\\sum_{i=1}^M |x_i - y|,\n$$\nand\n$$\n\\mathbb{E}|X - X'| = \\frac{1}{M^2}\\sum_{i=1}^M\\sum_{j=1}^M |x_i - x_j|.\n$$\nThus,\n$$\n\\mathrm{CRPS}(F_M,y) = \\frac{1}{M}\\sum_{i=1}^M |x_i - y| - \\frac{1}{2M^2}\\sum_{i=1}^M\\sum_{j=1}^M |x_i - x_j|.\n$$\nThis formula is exact and depends only on the ensemble members and the observation. A naive implementation computes the double sum in $O(M^2)$ time, which is too slow for large $M$. The key to an $O(M)$ algorithm, after sorting, is to compute $\\sum_{i,j} |x_i - x_j|$ in $O(M)$ using properties of order statistics.\n\n5. Let $x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(M)}$ denote the sorted ensemble. Observe that\n$$\n\\sum_{ij} (x_{(j)} - x_{(i)}) = \\sum_{j=1}^{M} (j-1)\\, x_{(j)} - \\sum_{i=1}^{M} (M-i)\\, x_{(i)} = \\sum_{k=1}^{M} \\left[(k-1) - (M-k)\\right] x_{(k)} = \\sum_{k=1}^{M} (2k - M - 1)\\, x_{(k)}.\n$$\nBecause $|x_i - x_j| = x_{(j)} - x_{(i)}$ for $ij$ in sorted order and equals zero for $i=j$, the total pairwise absolute sum satisfies\n$$\n\\sum_{i=1}^{M}\\sum_{j=1}^{M} |x_i - x_j| = 2 \\sum_{ij} (x_{(j)} - x_{(i)}) = 2 \\sum_{k=1}^{M} (2k - M - 1)\\, x_{(k)}.\n$$\nTherefore,\n$$\n\\frac{1}{2M^2}\\sum_{i=1}^{M}\\sum_{j=1}^{M} |x_i - x_j| = \\frac{1}{M^2} \\sum_{k=1}^{M} (2k - M - 1)\\, x_{(k)}.\n$$\nCombining terms, we obtain the $O(M)$ post-sorting formula:\n$$\n\\mathrm{CRPS}(F_M,y) = \\frac{1}{M}\\sum_{i=1}^{M} |x_i - y| \\;-\\; \\frac{1}{M^2} \\sum_{k=1}^{M} (2k - M - 1)\\, x_{(k)}.\n$$\nThe first sum does not require sorting; the second sum requires the sorted ensemble $\\{x_{(k)}\\}$ but is computed in a single pass using precomputed coefficients $(2k - M - 1)$.\n\nAlgorithmic design:\n- Input: an array of real values $\\{x_i\\}_{i=1}^{M}$ and a real observation $y$.\n- Step 1 (preprocessing): sort the ensemble ascending to obtain $\\{x_{(k)}\\}_{k=1}^{M}$; this costs $O(M \\log M)$ but is excluded from the $O(M)$ requirement as per the problem statement.\n- Step 2: compute $S_1 = \\sum_{i=1}^{M} |x_i - y|$ in $O(M)$ time.\n- Step 3: compute $S_2 = \\sum_{k=1}^{M} (2k - M - 1)\\, x_{(k)}$ in $O(M)$ time using a single pass over the sorted list and the integer coefficients $(2k - M - 1)$.\n- Step 4: return $\\mathrm{CRPS}(F_M,y) = \\frac{S_1}{M} - \\frac{S_2}{M^2}$.\n- Complexity: after sorting, each of Steps 2–4 is $O(M)$, so total post-sorting complexity is $O(M)$ per case.\n\nScientific realism and edge cases:\n- For $M=1$, the formula yields $\\mathrm{CRPS}(F_1,y) = |x_1 - y|$, which matches the deterministic absolute error, consistent with the definition of the CRPS.\n- For repeated values, the sorted order statistics and coefficients properly account for zero differences; no special handling is required.\n- For observations outside the ensemble range, the absolute differences in $S_1$ properly reflect the distance to the support of the EDF.\n\nImplementation details:\n- The program will implement the above algorithm using vectorized operations where appropriate.\n- The test suite includes a general case with unsorted input, degenerate $M=1$, observation below and above the ensemble range, duplicates, and a larger deterministic ensemble defined by $x_i = 0.5\\,i + \\sin(i)$ for $i=1,\\ldots,100$.\n- Outputs will be rounded to six decimal places and expressed in meters per second (m/s) as decimal floats.\n- The final output will be a single line with a bracketed, comma-separated list of six values corresponding to Cases A–F.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef crps_ensemble(x, y):\n    \"\"\"\n    Compute CRPS for an equally weighted ensemble forecast with members x and observation y.\n    The algorithm is O(M) after sorting:\n    CRPS = (1/M) * sum |x_i - y| - (1/M^2) * sum_{k=1}^M (2k - M - 1) * x_(k),\n    where x_(k) are the sorted ensemble members.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    M = x.size\n    if M == 0:\n        raise ValueError(\"Ensemble must contain at least one member.\")\n    # Sort the ensemble for the second term\n    x_sorted = np.sort(x)\n    # First term: average absolute difference to the observation (no need to be sorted)\n    sum_abs_y = np.sum(np.abs(x - y))\n    term1 = sum_abs_y / M\n    # Second term: use sorted order statistics coefficients\n    k = np.arange(1, M + 1, dtype=float)\n    coeffs = 2 * k - M - 1  # (2k - M - 1)\n    sum_weighted_sorted = np.dot(coeffs, x_sorted)\n    term2 = sum_weighted_sorted / (M ** 2)\n    crps = term1 - term2\n    return crps\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # All outputs are in meters per second (m/s), rounded to six decimals.\n    test_cases = [\n        # Case A: general, unsorted input\n        ([12.3, 10.1, 15.7, 9.8, 14.2, 13.0, 11.5], 12.0),\n        # Case B: degenerate ensemble M=1\n        ([10.0], 12.0),\n        # Case C: observation below ensemble range\n        ([3.0, 4.5, 5.1, 6.2], 1.0),\n        # Case D: observation above ensemble range\n        ([3.0, 4.5, 5.1, 6.2], 8.0),\n        # Case E: duplicates in ensemble\n        ([5.0, 5.0, 5.0, 7.0, 9.0], 6.0),\n        # Case F: larger deterministic ensemble\n        ([(0.5 * i) + np.sin(i) for i in range(1, 101)], 30.0),\n    ]\n\n    results = []\n    for x, y in test_cases:\n        val = crps_ensemble(x, y)\n        # Round to six decimal places\n        results.append(f\"{val:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Traditional pixel-wise metrics like Root Mean Square Error (RMSE) often penalize spatially displaced forecasts unfairly, an issue known as the \"double penalty.\" To gain more meaningful insight into a model's performance for fields like precipitation, we turn to object-based verification methods. This advanced practice introduces the Structure-Amplitude-Location (SAL) metric, allowing you to diagnose forecast errors not as a single number, but in terms of physically interpretable attributes of the predicted weather systems .",
            "id": "4044117",
            "problem": "Consider two-dimensional gridded precipitation fields representing a numerical weather prediction forecast and a verifying analysis. Let the forecast field be denoted by $F:\\Omega \\to \\mathbb{R}_{\\ge 0}$ and the observation/analysis field by $O:\\Omega \\to \\mathbb{R}_{\\ge 0}$, where $\\Omega = \\{0,1,\\ldots,n_x-1\\} \\times \\{0,1,\\ldots,n_y-1\\}$ indexes grid-cell centers. All precipitation intensities must be treated as nonnegative scalars in millimeters per hour (mm/h). The goal is to compute two verification metrics and use them to reason about the nature of error:\n\n1. A pixel-wise Root Mean Square Error (RMSE), defined from first principles of Euclidean distance in function space.\n2. An object-based Structure–Amplitude–Location (SAL) triplet, designed to evaluate spatial and structural characteristics of the precipitation fields through mass and moment constructions.\n\nFundamental base and definitions:\n\n- The empirical mean of a field $X$ over $\\Omega$ is $\\bar{X} = \\frac{1}{|\\Omega|} \\sum_{(i,j)\\in\\Omega} X_{i,j}$.\n- The pixel-wise Root Mean Square Error is derived from the $L^2$ distance: $\\mathrm{RMSE}(F,O) = \\sqrt{\\frac{1}{|\\Omega|} \\sum_{(i,j)\\in\\Omega} \\left(F_{i,j} - O_{i,j}\\right)^2}$. The RMSE must be reported in millimeters per hour (mm/h), rounded to three decimals.\n- The center of mass of a field $X$ with respect to $\\Omega$ is $\\mathbf{x}_X = \\left(\\frac{\\sum_{(i,j)\\in\\Omega} X_{i,j} \\, i}{\\sum_{(i,j)\\in\\Omega} X_{i,j}}, \\frac{\\sum_{(i,j)\\in\\Omega} X_{i,j} \\, j}{\\sum_{(i,j)\\in\\Omega} X_{i,j}}\\right)$, provided $\\sum_{(i,j)\\in\\Omega} X_{i,j} > 0$. If $\\sum_{(i,j)\\in\\Omega} X_{i,j} = 0$, the center of mass is undefined and must be handled explicitly as described below.\n- The domain diameter used to normalize location distances is $D = \\sqrt{(n_x - 1)^2 + (n_y - 1)^2}$.\n\nObject identification and scaled volumes:\n\n- To identify contiguous precipitation objects in a field $X$, define a threshold $t$ as $t = \\max\\left(0.1, \\, 0.15 \\cdot q_{95}\\right)$, where $q_{95}$ is the $95$th percentile of the union of all intensity values from both $F$ and $O$. Then construct the binary mask $M_X = \\{(i,j)\\in\\Omega : X_{i,j} \\ge t\\}$.\n- A precipitation object in $X$ is a maximally connected component in $M_X$, using $4$-neighbor connectivity (north, south, east, west adjacency).\n- For each object $j$ in $X$, define its maximum intensity $R_{j,\\max} = \\max_{(i,j)\\in\\text{obj}_j} X_{i,j}$ and its object sum $V_j = \\sum_{(i,j)\\in\\text{obj}_j} X_{i,j}$. The scaled volume of object $j$ is $v_j = \\frac{V_j}{R_{j,\\max}}$. The scaled volume summary for the field is $S_X^\\star = \\sum_j v_j$. If there are no objects (i.e., $M_X$ is empty), then $S_X^\\star = 0$.\n\nSAL components to compute:\n\n- Amplitude $A$ is the normalized mean difference constructed from the empirical means: $A = \\frac{\\bar{F} - \\bar{O}}{\\frac{1}{2}(\\bar{F} + \\bar{O})}$ if $\\bar{F} + \\bar{O}  0$, and $A = 0$ if $\\bar{F} = \\bar{O} = 0$.\n- Location $L$ is the sum of two terms $L = L_1 + L_2$:\n  - $L_1 = \\frac{\\|\\mathbf{x}_F - \\mathbf{x}_O\\|_2}{D}$ if both centers of mass exist; if both fields are empty (both have zero total mass), then $L_1 = 0$; if only one field is empty, set $L_1 = 1$.\n  - $L_2 = \\left| r_F - r_O \\right|$, where $r_X = \\left(\\frac{\\sum_{(i,j)\\in\\Omega} X_{i,j} \\, \\|\\mathbf{x}_{i,j} - \\mathbf{x}_X\\|_2}{\\sum_{(i,j)\\in\\Omega} X_{i,j}}\\right)\\big/ D$ is the normalized first radial moment (dispersion) of $X$ around its center of mass, and $\\mathbf{x}_{i,j} = (i,j)$. If both fields are empty, set $r_F = r_O = 0$. If only one field is empty, set the empty field’s dispersion to $0$ and the non-empty field’s dispersion per the formula above.\n- Structure $S$ is the normalized difference of the scaled volume summaries: $S = \\frac{S_F^\\star - S_O^\\star}{\\frac{1}{2}(S_F^\\star + S_O^\\star)}$ if $S_F^\\star + S_O^\\star  0$, and $S = 0$ if $S_F^\\star = S_O^\\star = 0$.\n\nEpistemic classification based on SAL versus RMSE:\n\n- Define thresholds $a_{\\mathrm{th}} = 0.5$, $s_{\\mathrm{th}} = 0.5$, and $l_{\\mathrm{th}} = 0.3$ (all dimensionless).\n- For each test case, compute three boolean indicators:\n  - Displacement-dominated error: $\\mathrm{disp} = \\left(|A|  a_{\\mathrm{th}}\\right) \\wedge \\left(|S|  s_{\\mathrm{th}}\\right) \\wedge \\left(L  l_{\\mathrm{th}}\\right)$.\n  - Amplitude-bias-dominated error: $\\mathrm{amp} = \\left(|A|  a_{\\mathrm{th}}\\right) \\wedge \\left(L  l_{\\mathrm{th}}\\right)$.\n  - Structure-difference-dominated error: $\\mathrm{struc} = \\left(|S|  s_{\\mathrm{th}}\\right) \\wedge \\left(L  l_{\\mathrm{th}}\\right) \\wedge \\left(|A|  a_{\\mathrm{th}}\\right)$.\n\nTest suite and required outputs:\n\nUse the following test suite of $(F,O)$ pairs on $\\Omega$ with $n_x = n_y = 5$. All entries are in millimeters per hour (mm/h).\n\n- Test case 1 (happy path, near-perfect match):\n  - $O_1 =$ \n    $$\\begin{bmatrix}\n    0  0  1  0  0 \\\\\n    0  2  3  2  0 \\\\\n    1  3  5  3  1 \\\\\n    0  2  3  2  0 \\\\\n    0  0  1  0  0\n    \\end{bmatrix}$$\n  - $F_1 = O_1$.\n- Test case 2 (uniform amplitude bias, same structure and location):\n  - $O_2 = O_1$,\n  - $F_2 = 1.5 \\cdot O_1$.\n- Test case 3 (pure displacement, similar amplitude and structure):\n  - $O_3 = O_1$,\n  - $F_3$ is $O_1$ shifted one grid cell to the right with zero padding:\n    $$\\begin{bmatrix}\n    0  0  0  1  0 \\\\\n    0  0  2  3  2 \\\\\n    0  1  3  5  3 \\\\\n    0  0  2  3  2 \\\\\n    0  0  0  1  0\n    \\end{bmatrix}$$\n- Test case 4 (structure difference: one elongated object versus two compact objects; equal amplitude):\n  - $O_4 =$ \n    $$\\begin{bmatrix}\n    0  0  0  0  0 \\\\\n    0  0  4  0  0 \\\\\n    0  0  4  0  0 \\\\\n    0  0  4  0  0 \\\\\n    0  0  0  0  0\n    \\end{bmatrix}$$\n  - $F_4 =$ \n    $$\\begin{bmatrix}\n    0  0  0  0  0 \\\\\n    0  3  0  3  0 \\\\\n    0  3  0  3  0 \\\\\n    0  0  0  0  0 \\\\\n    0  0  0  0  0\n    \\end{bmatrix}$$\n- Test case 5 (boundary condition: both empty):\n  - $O_5 =$ zero $5\\times 5$ matrix,\n  - $F_5 =$ zero $5\\times 5$ matrix.\n- Test case 6 (boundary condition: forecast non-empty, observation empty):\n  - $O_6 =$ zero $5\\times 5$ matrix,\n  - $F_6 = O_1$.\n\nFinal output format:\n\nYour program should produce a single line of output containing, for each test case in order, a list with seven entries: $[A,S,L,\\mathrm{RMSE},\\mathrm{disp},\\mathrm{amp},\\mathrm{struc}]$, where $A$, $S$, and $L$ are unitless floats rounded to three decimals, $\\mathrm{RMSE}$ is a float in millimeters per hour (mm/h) rounded to three decimals, and the three booleans are the epistemic classifications defined above. Aggregate the six test case results into a single top-level list printed as a comma-separated list enclosed in square brackets; for example, in the exact format $[[A_1,S_1,L_1,\\mathrm{RMSE}_1,\\mathrm{disp}_1,\\mathrm{amp}_1,\\mathrm{struc}_1],[A_2,\\ldots],\\ldots]$ with no additional text.",
            "solution": "The problem statement is a well-defined computational task based on established principles of numerical weather prediction forecast verification. It requires the implementation of the Root Mean Square Error (RMSE) and the Structure-Amplitude-Location (SAL) verification metrics. All definitions, formulas, and boundary conditions are provided explicitly and are mathematically and scientifically sound. The test cases are constructed to probe different aspects of forecast error and test the implementation against a range of conditions, including edge cases like empty fields. The problem is self-contained, objective, and scientifically grounded.\n\n### Step 1: Extract Givens\n- **Fields**: Forecast $F:\\Omega \\to \\mathbb{R}_{\\ge 0}$ and Observation $O:\\Omega \\to \\mathbb{R}_{\\ge 0}$.\n- **Domain**: $\\Omega = \\{0,1,\\ldots,n_x-1\\} \\times \\{0,1,\\ldots,n_y-1\\}$ with $n_x=n_y=5$.\n- **Empirical Mean**: $\\bar{X} = \\frac{1}{|\\Omega|} \\sum_{(i,j)\\in\\Omega} X_{i,j}$.\n- **RMSE**: $\\mathrm{RMSE}(F,O) = \\sqrt{\\frac{1}{|\\Omega|} \\sum_{(i,j)\\in\\Omega} \\left(F_{i,j} - O_{i,j}\\right)^2}$.\n- **Center of Mass**: $\\mathbf{x}_X = \\left(\\frac{\\sum X_{i,j} \\, i}{\\sum X_{i,j}}, \\frac{\\sum X_{i,j} \\, j}{\\sum X_{i,j}}\\right)$ if $\\sum X_{i,j}  0$.\n- **Domain Diameter**: $D = \\sqrt{(n_x - 1)^2 + (n_y - 1)^2}$.\n- **Object Threshold**: $t = \\max\\left(0.1, \\, 0.15 \\cdot q_{95}\\right)$, where $q_{95}$ is the $95$th percentile of all values in $F \\cup O$.\n- **Object Definition**: Maximally connected component in $M_X = \\{(i,j)\\in\\Omega : X_{i,j} \\ge t\\}$ using $4$-neighbor connectivity.\n- **Scaled Volume Summary**: $S_X^\\star = \\sum_j v_j = \\sum_j \\frac{V_j}{R_{j,\\max}}$, where $V_j$ is the sum over object $j$ and $R_{j,\\max}$ is the maximum intensity in object $j$. If no objects, $S_X^\\star=0$.\n- **Amplitude (A)**: $A = \\frac{\\bar{F} - \\bar{O}}{\\frac{1}{2}(\\bar{F} + \\bar{O})}$ if $\\bar{F} + \\bar{O}  0$; else $A=0$.\n- **Location (L)**: $L = L_1 + L_2$.\n  - $L_1$: $\\frac{\\|\\mathbf{x}_F - \\mathbf{x}_O\\|_2}{D}$ if both CoMs exist. $L_1=0$ if both fields empty. $L_1=1$ if one field empty.\n  - $L_2$: $|r_F - r_O|$, where $r_X = \\left(\\frac{\\sum X_{i,j} \\, \\|\\mathbf{x}_{i,j} - \\mathbf{x}_X\\|_2}{\\sum X_{i,j}}\\right)\\big/ D$ is normalized dispersion. If field is empty, its dispersion is $0$.\n- **Structure (S)**: $S = \\frac{S_F^\\star - S_O^\\star}{\\frac{1}{2}(S_F^\\star + S_O^\\star)}$ if $S_F^\\star + S_O^\\star  0$; else $S=0$.\n- **Epistemic Classification Thresholds**: $a_{\\mathrm{th}} = 0.5$, $s_{\\mathrm{th}} = 0.5$, $l_{\\mathrm{th}} = 0.3$.\n- **Classification Rules**:\n  - $\\mathrm{disp} = \\left(|A|  a_{\\mathrm{th}}\\right) \\wedge \\left(|S|  s_{\\mathrm{th}}\\right) \\wedge \\left(L  l_{\\mathrm{th}}\\right)$\n  - $\\mathrm{amp} = \\left(|A|  a_{\\mathrm{th}}\\right) \\wedge \\left(L  l_{\\mathrm{th}}\\right)$\n  - $\\mathrm{struc} = \\left(|S|  s_{\\mathrm{th}}\\right) \\wedge \\left(L  l_{\\mathrm{th}}\\right) \\wedge \\left(|A|  a_{\\mathrm{th}}\\right)$\n- **Test Cases**: Six pairs of $(F, O)$ matrices are provided.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, drawing from established forecast verification literature (SAL metric). It is well-posed, with all terms, constants, and edge cases defined, ensuring a unique solution exists for each test case. The language is objective and precise. The problem is self-contained and internally consistent; there are no contradictions in the definitions. The numerical nature of the task makes it formalizable and verifiable. A manual check of simplified cases confirms that the logic holds, even if the classification outcomes for the named examples (e.g., \"pure displacement\") do not match intuition due to the specific thresholds, which is a valid and instructive result. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete solution will be provided.\n\n### Principle-Based Design\nThe solution will be structured into modular functions, each responsible for a distinct component of the overall calculation. This approach mirrors the hierarchical definition of the metrics themselves.\n\n1.  **Main Loop**: A primary function will orchestrate the process, iterating through each test case provided. For each $(F, O)$ pair, it will call a unified calculation function.\n\n2.  **RMSE Calculation**: A simple function implementing the standard Root Mean Square Error formula, leveraging `numpy` for efficient array operations. RMSE is a measure of the average magnitude of pixel-wise errors.\n\n3.  **Amplitude (A) Calculation**: A function to compute the normalized difference of the domain-averaged precipitation. Special care is taken for the edge case where both fields have a mean of zero, as specified. `A` isolates the systematic bias in the total volume of precipitation.\n\n4.  **Location (L) Calculation**: This is the most complex component and will be broken down further.\n    *   A helper function computes the center of mass (first moment of the field) for a given precipitation field. It returns a special value if the field's total mass is zero.\n    *   Another helper calculates the normalized dispersion (first radial moment) around the center of mass. It correctly handles empty fields.\n    *   The main `L` function integrates these parts to compute $L_1$ (distance between centers of mass) and $L_2$ (difference in dispersion), correctly applying the specific rules for cases where one or both fields are empty. `L` quantifies errors in the overall position and spatial extent of the precipitation pattern.\n\n5.  **Structure (S) Calculation**: This component evaluates the shape and size of precipitation features.\n    *   First, the object identification threshold $t$ is calculated based on the $95$th percentile of the combined precipitation data from both fields.\n    *   A helper function identifies contiguous objects above this threshold using `scipy.ndimage.label` with a 4-connectivity rule. For each object, it calculates the \"scaled volume\" ($V_j / R_{j,\\max}$), which captures the object's \"peakedness\". An object that is large and flat will have a larger scaled volume than a small, peaked object with the same total precipitation sum.\n    *   The `S` function then computes the normalized difference of the sum of these scaled volumes ($S_F^\\star$ and $S_O^\\star$). It correctly handles the zero-denominator case. `S` quantifies whether the forecast produces precipitation objects that are, on average, too large/flat or too small/peaked compared to observations.\n\n6.  **Epistemic Classification**: A final function takes the computed $A$, $S$, and $L$ values and applies the boolean logic with the given thresholds to classify the dominant error type.\n\nThis modular design ensures that each physical concept (Amplitude, Location, Structure) is encapsulated in its own computational block, improving code clarity, verifiability, and adherence to the problem's definitions. All numerical results are rounded to three decimal places as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.ndimage import label, find_objects, maximum, sum as nd_sum\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Test case definitions\n    O1 = np.array([\n        [0, 0, 1, 0, 0],\n        [0, 2, 3, 2, 0],\n        [1, 3, 5, 3, 1],\n        [0, 2, 3, 2, 0],\n        [0, 0, 1, 0, 0]\n    ], dtype=float)\n    \n    F1 = O1.copy()\n    \n    O2 = O1.copy()\n    F2 = 1.5 * O1\n    \n    O3 = O1.copy()\n    F3 = np.array([\n        [0, 0, 0, 1, 0],\n        [0, 0, 2, 3, 2],\n        [0, 1, 3, 5, 3],\n        [0, 0, 2, 3, 2],\n        [0, 0, 0, 1, 0]\n    ], dtype=float)\n    \n    O4 = np.array([\n        [0, 0, 0, 0, 0],\n        [0, 0, 4, 0, 0],\n        [0, 0, 4, 0, 0],\n        [0, 0, 4, 0, 0],\n        [0, 0, 0, 0, 0]\n    ], dtype=float)\n    F4 = np.array([\n        [0, 0, 0, 0, 0],\n        [0, 3, 0, 3, 0],\n        [0, 3, 0, 3, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]\n    ], dtype=float)\n    \n    O5 = np.zeros((5, 5), dtype=float)\n    F5 = np.zeros((5, 5), dtype=float)\n    \n    O6 = np.zeros((5, 5), dtype=float)\n    F6 = O1.copy()\n\n    test_cases = [\n        (F1, O1),\n        (F2, O2),\n        (F3, O3),\n        (F4, O4),\n        (F5, O5),\n        (F6, O6),\n    ]\n\n    results = []\n    for F, O in test_cases:\n        case_result = compute_all_metrics(F, O)\n        results.append(case_result)\n\n    # Formatting the output string\n    output_str = \"[\" + \",\".join([str(res) for res in results]) + \"]\"\n    print(output_str)\n\ndef compute_all_metrics(F, O):\n    \"\"\"\n    Computes all specified metrics for a given Forecast (F) and Observation (O) pair.\n    \"\"\"\n    nx, ny = F.shape\n    D = np.sqrt((nx - 1)**2 + (ny - 1)**2)\n    \n    # Grid coordinates for moment calculations\n    j_grid, i_grid = np.meshgrid(np.arange(ny), np.arange(nx))\n\n    # Calculate individual metrics\n    rmse = np.sqrt(np.mean((F - O)**2))\n    A = compute_A(F, O)\n    L = compute_L(F, O, i_grid, j_grid, D)\n    S = compute_S(F, O)\n    \n    # Epistemic classification\n    a_th, s_th, l_th = 0.5, 0.5, 0.3\n    disp = (abs(A)  a_th) and (abs(S)  s_th) and (L  l_th)\n    amp = (abs(A)  a_th) and (L  l_th)\n    struc = (abs(S)  s_th) and (L  l_th) and (abs(A)  a_th)\n    \n    return [\n        round(A, 3), \n        round(S, 3), \n        round(L, 3), \n        round(rmse, 3), \n        disp, \n        amp, \n        struc\n    ]\n\ndef compute_A(F, O):\n    \"\"\"Computes the Amplitude component A.\"\"\"\n    mean_F = np.mean(F)\n    mean_O = np.mean(O)\n    denominator = 0.5 * (mean_F + mean_O)\n    if denominator  0:\n        return (mean_F - mean_O) / denominator\n    return 0.0\n\ndef get_com(X, i_grid, j_grid):\n    \"\"\"Computes the center of mass for a field.\"\"\"\n    total_mass = np.sum(X)\n    if total_mass == 0:\n        return None\n    com_i = np.sum(X * i_grid) / total_mass\n    com_j = np.sum(X * j_grid) / total_mass\n    return np.array([com_i, com_j])\n\ndef get_dispersion(X, com, i_grid, j_grid, D):\n    \"\"\"Computes the normalized dispersion r_X.\"\"\"\n    total_mass = np.sum(X)\n    if total_mass == 0:\n        return 0.0\n    distances = np.sqrt((i_grid - com[0])**2 + (j_grid - com[1])**2)\n    radial_moment = np.sum(X * distances)\n    return (radial_moment / total_mass) / D\n\ndef compute_L(F, O, i_grid, j_grid, D):\n    \"\"\"Computes the Location component L.\"\"\"\n    total_mass_F = np.sum(F)\n    total_mass_O = np.sum(O)\n    \n    com_F = get_com(F, i_grid, j_grid)\n    com_O = get_com(O, i_grid, j_grid)\n    \n    # Calculate L1\n    if total_mass_F == 0 and total_mass_O == 0:\n        L1 = 0.0\n    elif total_mass_F == 0 or total_mass_O == 0:\n        L1 = 1.0\n    else:\n        L1 = np.linalg.norm(com_F - com_O) / D\n        \n    # Calculate L2\n    # The problem specifies that an empty field's dispersion is 0.\n    # get_dispersion already handles this.\n    # If a field is non-empty but the other is, we need to compute dispersion\n    # relative to its own CoM.\n    \n    # Effective CoM for dispersion calculation: if a field is empty,\n    # its CoM doesn't matter as its dispersion is 0.\n    r_F = get_dispersion(F, com_F if com_F is not None else np.array([0,0]), i_grid, j_grid, D)\n    r_O = get_dispersion(O, com_O if com_O is not None else np.array([0,0]), i_grid, j_grid, D)\n    L2 = abs(r_F - r_O)\n    \n    return L1 + L2\n\ndef compute_S_star(X, t):\n    \"\"\"Computes the scaled volume summary S* for a field.\"\"\"\n    mask = X = t\n    if not np.any(mask):\n        return 0.0\n        \n    # 4-connectivity structure\n    connectivity_structure = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n    labeled_array, num_features = label(mask, structure=connectivity_structure)\n    \n    if num_features == 0:\n        return 0.0\n        \n    object_slices = find_objects(labeled_array)\n    object_sums = nd_sum(X, labeled_array, index=np.arange(1, num_features + 1))\n    object_maxima = maximum(X, labeled_array, index=np.arange(1, num_features + 1))\n\n    # Avoid division by zero if an object's max is somehow zero (should not happen with t  0)\n    scaled_volumes = np.divide(object_sums, object_maxima, \n                              out=np.zeros_like(object_sums, dtype=float), \n                              where=object_maxima!=0)\n\n    return np.sum(scaled_volumes)\n\ndef compute_S(F, O):\n    \"\"\"Computes the Structure component S.\"\"\"\n    combined_values = np.concatenate((F.flatten(), O.flatten()))\n    # Handle empty fields for percentile calculation\n    if combined_values.size == 0 or np.all(combined_values == 0):\n        q95 = 0.0\n    else:\n        q95 = np.percentile(combined_values[combined_values  0], 95) if np.any(combined_values  0) else 0.0\n\n    t = max(0.1, 0.15 * q95)\n\n    S_F_star = compute_S_star(F, t)\n    S_O_star = compute_S_star(O, t)\n\n    denominator = 0.5 * (S_F_star + S_O_star)\n    if denominator  0:\n        return (S_F_star - S_O_star) / denominator\n    return 0.0\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}