## 引言
[预报检验](@entry_id:1125232)是数值天气预报（NWP）工作流程中至关重要的一环，它为客观评估预报系统的性能、诊断误差来源以及指导模式改进提供了不可或缺的科学依据。一个预报系统的好坏，并不能简单地用“准确”或“不准确”来概括；我们需要一个精细化的框架，来定量地衡量预报在不同方面的表现，理解其优势与缺陷，并最终提升其在现实决策中的应用价值。本文旨在系统性地梳理[预报检验](@entry_id:1125232)的核心理论、关键方法及其在多学科背景下的广泛应用。

本文将引导您穿越[预报检验](@entry_id:1125232)的多个层面。在“原理与机制”一章中，我们将首先建立[预报检验](@entry_id:1125232)的理论基石，厘清检验、验证与校准等核心概念，探讨误差的来源，并详细介绍用于确定性预报和[概率预报](@entry_id:183505)的经典与现代检验指标。接着，在“应用与交叉学科联系”一章中，我们将展示这些理论如何在实践中被灵活运用，从检验特定的气象现象（如风场和气旋），到通过诊断性检验将误差与模式的物理过程联系起来，再到评估预报对高影响事件的经济价值。最后，“动手实践”部分将为您提供亲手实现关键检验算法的机会，将理论知识转化为实践技能。

通过学习本章内容，您将能够构建一个关于如何评估数值天气预报的完整知识体系，为后续深入研究和应用打下坚实的基础。让我们从[预报检验](@entry_id:1125232)最基本的原理与机制开始。

## 原理与机制

### [预报检验](@entry_id:1125232)的基本框架

[预报检验](@entry_id:1125232)是[数值天气预报](@entry_id:191656)（NWP）流程中不可或缺的一环，它为我们评估和改进预报系统提供了定量的科学依据。本章节将深入探讨支撑现代[预报检验](@entry_id:1125232)的若干核心原理与关键机制，从基本概念的辨析，到检验指标的设计，再到对预报技巧的深刻诠释。

#### 检验、验证与校准

在评估一个复杂的NWP系统时，我们通常会涉及三个既有区别又相互关联的活动：**检验 (verification)**、**验证 (validation)** 和 **校准 (calibration)**。清晰地界定这三者的内涵、作用与 epistemic role (即它们在构建关于大气预测的知识声明的可信度中所扮演的角色) 是至关重要的。

**检验**直接回答了这样一个问题：“预报有多好？” 它指的是将预报输出与作为“事实”基准的观测数据进行定量比较的过程。对于确定性预报，这可能涉及计算[均方根误差](@entry_id:170440)或偏差；对于[概率预报](@entry_id:183505)，则可能采用诸如Brier评分或连续分级概率评分（CRPS）等恰当的评分规则。检验的目的是利用一套标准化的指标来刻画预报在特定条件下（如特定的预报时效 $\tau$、地理区域 $D$、观测系统等）的准确性、可靠性和锐度等多个方面的表现。因此，检验的认知作用在于为关于预报系统在特定应用场景下性能的声明提供经验证据和量化支持。

**验证**则提出了一个更深层次、更宽泛的问题：“从科学角度看，这个模式是否可信？” 它不仅仅是比较最终的预报产品与观测结果，而是对模型作为一个大气系统的生成性表征的全面评估。验证活动包括检查模型是否遵循基本的物理定律（如质量、[能量和动量守恒](@entry_id:193044)）、其动力学核心与[物理参数化](@entry_id:1129649)方案是否合理、能否再现已知的多尺度大气现象和过程（如遥相关、日循环、风暴结构等），以及模型对参数 $\boldsymbol{\theta}$ 和初值的[敏感性分析](@entry_id:147555)。验证的认知作用在于确立模型作为一种科学工具的可信度边界和[适用范围](@entry_id:636189)。一个经过充分验证的模型，被认为是现实大气的一个良好科学近似，即便其原始输出可能仍存在需要校准的系统性误差。

**校准**是一项更为具体的技术操作，其核心目标是：“如何修正已知的系统性误差？” 它通常指应用统计后处理技术（如偏差订正、可靠性调整）来调整原始预报输出，使其统计特征（如均值、方差）与观测序列的统计特征更加吻合。例如，通过优化某个恰当评分规则的[期望值](@entry_id:150961)，可以对预报的概率分布进行调整。校准的认知作用主要是实用主义的：它通过提升预报的[统计一致性](@entry_id:162814)来改善最终预报产品的质量和应用价值。然而，必须强调的是，校准本身并不修复模型内部的物理或动力学缺陷，它更像是在“掩盖”这些缺陷导致的系统性错误。一次成功的校准，并不意味着其背后的动力模型是经过充分验证的或结构上是完全正确的。

#### 真值、观测与误差

[预报检验](@entry_id:1125232)的核心是比较，而任何比较都需要一个基准。在理想情况下，我们希望将预报 $F$ 与一个潜在的、无法被完全观测的“**[真值](@entry_id:636547)**”过程 $X$ 进行比较。例如，一个检验指标的目标可能是评估 $F$ 相对于 $X$ 的某个泛函，如期望平方误差 $\mathrm{MSE}^{\star} = \mathbb{E}[(\mathcal{H}\{F\} - \mathcal{H}\{X\})^2]$，其中 $\mathcal{H}$ 是一个算子，用于将场变量映射到特定的观测空间 。

然而，在现实世界中，我们无法直接获取[真值](@entry_id:636547) $X$。我们所拥有的是**观测** $Y_k$，它们本身是真值通过复杂的物理过程和测量仪器得到的“有损”代理。一个观测值可以概念化地表示为 $Y_k = \mathcal{H}_k\{X(\cdot, t_k)\} + \epsilon_{obs, k}$，其中 $\mathcal{H}_k$ 是**观测算子 (observation operator)**，$\epsilon_{obs, k}$ 是总的**[观测误差](@entry_id:752871) (observation error)**。

[观测算子](@entry_id:752875) $\mathcal{H}$ 是一个至关重要的概念，它将模型[状态变量](@entry_id:138790)（通常是定义在离散网格上的场）映射到与观测相对应的量。例如，一个模型的网格点温度输出是瞬时值，代表了边长为 $\Delta = 9\,\mathrm{km}$ 的网格区域的平均状态。而一个地面气象站的温度观测可能是对一个半径 $r_o = 1\,\mathrm{km}$ 的小范围进行5分钟时间平均后的结果。为了进行公平比较，[观测算子](@entry_id:752875) $\mathcal{H}$ 必须模拟这一过程：它需要对模型的时空场进行插值，并进行与观测相一致的空间和[时间聚合](@entry_id:1132908)。只有这样，我们才能比较两个具有可比物理意义的量：实际观测值 $Y_k$ 和“观测的模式等价物” $H_k\{F(\cdot, t_k)\}$ 。

总[观测误差](@entry_id:752871) $\epsilon_{obs}$ 通常包含两个主要组成部分：

1.  **测量误差 (Measurement Error)**：这源于仪器本身的不完美，包括传感器噪声、校准偏差等。
2.  **代表性误差 (Representativeness Error)**：这源于模型和观测之间的“[尺度不匹配](@entry_id:1131268)”。模型由于其有限的分辨率，无法解析所有尺度的真实大气变化。而一个点观测或小范围观测则会受到这些模型无法解析的局地、小尺度过程的影响。这种由于被观测的物理过程与模型所能代表的物理过程之间存在差异而产生的误差，就是[代表性误差](@entry_id:754253)。它不是仪器本身的错误，而是表征能力的差异所致。

因此，为了使基于观测的检验能够有效地评估预报相对于[真值](@entry_id:636547)的表现，必须满足一系列关键假设。其中最核心的假设是，[观测误差](@entry_id:752871)（包括测量误差和[代表性误差](@entry_id:754253)）的期望为零，并且与预报本身是统计独立的。在这些假设下，诸如均方误差之类的经验评分的[期望值](@entry_id:150961)，会收敛于预报相对于[真值](@entry_id:636547)的目标评分加上一个与预报无关的常数（即[观测误差](@entry_id:752871)的方差）。例如，$\mathbb{E}[(Y_k - \mathcal{H}_k\{F\})^2] = \mathbb{E}[(\mathcal{H}_k\{F\} - \mathcal{H}_k\{X\})^2] + \mathbb{E}[\epsilon_{obs,k}^2]$。这意味着，尽管我们永远无法消除[观测误差](@entry_id:752871)，但只要它对所有待比较的预报系统是一视同仁的，那么基于观测的评分排序就能正确反映预报系统相对于潜[真值](@entry_id:636547)的优劣排序。这就是观测数据能够在检验中充当“[真值](@entry_id:636547)”角色的统计学基础 。

### 确定性[预报检验](@entry_id:1125232)指标

对于温度、气压等连续标量变量的确定性预报，一系列基础统计量被广泛用于量化预报性能。假设我们有一个包含 $N$ 个个例的预报-观测对序列 $\{(f_t, o_t)\}_{t=1}^N$，我们可以定义以下几个核心指标 。

**偏差 (Bias)**，或称为平均误差 (Mean Error, ME)，定义为预报误差 $e_t = f_t - o_t$ 的样本均值：
$$
\text{Bias} = \overline{e} = \frac{1}{N} \sum_{t=1}^N (f_t - o_t)
$$
偏差衡量了预报的系统性偏移。正偏差表示系统性高估，负偏差表示系统性低估。由于是算术平均，数值相近、符号相反的误差会相互抵消，这可能掩盖预报中存在的较大但方向不一的误差。它是一个关于平均偏移的度量，而非误差大小的度量。

**平均[绝对误差](@entry_id:139354) (Mean Absolute Error, MAE)** 定义为预报误差绝对值的样本均值：
$$
\text{MAE} = \frac{1}{N} \sum_{t=1}^N |f_t - o_t|
$$
MAE衡量了误差的平均大小。通过使用绝对值，它避免了正负误差的抵消。MAE是基于 $L_1$ 范数的度量，每个误差对总体的贡献与其量值成线性关系。这使得MAE相比于RMSE对少数极端大误差（异常值）不那么敏感，因此被认为是一个更稳健的平均误差大小的衡量标准。

**均方根误差 (Root Mean Squared Error, RMSE)** 定义为预报误差平方的均值再开方：
$$
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{t=1}^N (f_t - o_t)^2}
$$
RMSE是基于 $L_2$ 范数的度量。由于对误差进行了平方处理，它给予了大误差远高于小误差的权重。一个大误差对[均方误差](@entry_id:175403)(MSE)的贡献是二次方的，这使得RMSE对异常值非常敏感。MSE可以被分解为误差方差和偏差平方的和：
$$
\text{MSE} = \frac{1}{N} \sum_{t=1}^N e_t^2 = \sigma_e^2 + (\text{Bias})^2
$$
其中 $\sigma_e^2$ 是误差序列的方差。因此，RMSE是一个混合度量，同时反映了预报的系统性误差（Bias）和随机性误差（误差的离散程度 $\sigma_e$）。

**皮尔逊相关系数 (Pearson Correlation Coefficient, PCC)** 衡量了预报和观测之间线性关系的强度：
$$
\text{PCC} = \frac{\sum_{t=1}^N (f_t - \overline{f})(o_t - \overline{o})}{\sqrt{\sum_{t=1}^N (f_t - \overline{f})^2} \sqrt{\sum_{t=1}^N (o_t - \overline{o})^2}}
$$
其中 $\overline{f}$ 和 $\overline{o}$ 分别是预报和观测序列的样本均值。PCC的值域为 $[-1, 1]$。它对于预报或观测序列的线性[仿射变换](@entry_id:144885)（即加上一个常数或乘以一个正常数）是不变的。这意味着PCC对预报的平均偏差和振幅大小不敏感。它主要衡量预报能否准确地捕捉观测序列的变化趋势或位相，即两者围绕各自均值协同变化的程度。一个高的相关系数表明预报很好地模拟了事件的发生和演变节奏，但并不保证预报的量值是准确的。

### 概率和集合预报检验

现代NWP系统越来越多地采用[集合预报](@entry_id:1124525)的形式，为预报事件提供一个概率分布，而不仅仅是一个单一的确定性值。检验这类预报需要一套不同的工具。

#### 评估集合离散度和校准度：等级[直方图](@entry_id:178776)与PIT直方图

对于一个[集合预报系统](@entry_id:1124526)，最基本的检验问题是：预报的概率分布是否可靠地表征了真实的不确定性？**等级[直方图](@entry_id:178776) (rank histogram)** 和 **[概率积分变换](@entry_id:262799) (PIT) [直方图](@entry_id:178776)** 是回答这个问题的首要诊断工具 。

假设我们有一个包含 $m$ 个成员的集合预报 $\{X_1, \dots, X_m\}$，以及一个对应的真实观测值 $Y$。如果集合预报是“完美”的，即集合成员和真实观测值可以被看作是从同一个真实概率分布中抽取的 $m+1$ 个[独立同分布](@entry_id:169067)的样本，那么将 $Y$ 和这 $m$ 个集合成员放在一起排序时，$Y$ 的位次（rank）出现在第1位到第 $m+1$ 位的概率应该是均等的，即 $1/(m+1)$。

**等级直方图**正是基于这一原理。其构建步骤如下：
1.  将 $m$ 个集合成员按升序排列：$X_{(1)} \le X_{(2)} \le \dots \le X_{(m)}$。
2.  这 $m$ 个有序的成员构成了 $m+1$ 个区间（或等级），观测值 $Y$ 必然落入其中之一。例如，等级1表示 $Y  X_{(1)}$，等级 $i$ 表示 $X_{(i-1)} \le Y  X_{(i)}$，等级 $m+1$ 表示 $Y \ge X_{(m)}$。
3.  对大量的预报-观测样本重复此过程，统计观测值落在每个等级中的频率，绘制成直方图。

**[概率积分变换](@entry_id:262799) (PIT) [直方图](@entry_id:178776)**是等级直方图在[连续概率分布](@entry_id:636595)下的理论对应物。其理论基础是：如果一个[连续随机变量](@entry_id:166541) $Y$ 的[累积分布函数 (CDF)](@entry_id:264700) 是 $F(y)$，那么经过变换后的新[随机变量](@entry_id:195330) $U = F(Y)$ 将服从 $[0,1]$ 上的均匀分布。对于[集合预报](@entry_id:1124525)，我们可以用其[经验累积分布函数](@entry_id:167083) (ECDF) $\widehat{F}(y)$ 来近似真实的 $F(y)$。

对于一个理想的、经过良好校准的[集合预报系统](@entry_id:1124526)，其等级直方图或PIT直方图应该是平坦的。任何系统性的偏离都揭示了预报系统存在的问题：
*   **U形直方图**：观测值频繁地落在集合预报范围之外（即等级1和等级 $m+1$ 的频数过高）。这表明集合的**[离散度](@entry_id:168823)不足 (under-dispersive)**，预报系统过于自信，未能捕捉到真实的不确定性。
*   **拱形（或驼峰形）直方图**：观测值过于集中地落在[集合预报](@entry_id:1124525)的中间等级。这表明集合的**离散度过大 (over-dispersive)**，预报系统过于保守或不确定。
*   **倾斜的[直方图](@entry_id:178776)**：[直方图](@entry_id:178776)不对称，频数集中在某一端。这表明预报存在**系统性偏差 (bias)**。例如，如果频数集中在右侧（高等级），意味着观测值常常大于大多数集合成员，即预报系统性地**偏低**。反之，如果频数集中在左侧（低等级），则意味着预报系统性地**偏高**。

#### 评分规则与Brier评分分解

除了图形化的诊断工具，我们还需要定量的评分规则来评估[概率预报](@entry_id:183505)的总体质量。对于二元事件（例如，是否降水），**Brier评分 (Brier Score, BS)** 是一个经典的指标。它本质上是概率预报的[均方误差](@entry_id:175403)。若预报概率为 $f_i$，观测结果为 $X_i \in \{0, 1\}$，则BS定义为：
$$
BS = \frac{1}{N} \sum_{i=1}^{N} (f_i - X_i)^2
$$
Brier评分越低，预报质量越好。完美预报的BS为0。

Brier评分的一个强大之处在于它可以被分解，从而揭示预报质量的不同方面。对于按预报概率 $f_k$ [分箱](@entry_id:264748)的数据，BS可以被分解为三个部分，即**Murphy分解** ：
$$
BS = \mathrm{REL} - \mathrm{RES} + \mathrm{UNC}
$$
这三个组成部分分别是：
1.  **可靠性 (Reliability, REL)**：$\mathrm{REL} = \sum_{k=1}^{K} w_k (f_k - o_k)^2$。其中 $w_k$ 是第 $k$ 箱的样本权重，$f_k$ 是该箱的预报概率，而 $o_k$ 是该箱内事件发生的观测频率。可靠性衡量的是预报概率的“诚实度”。例如，当预报30%的降水概率时，实际降水频率是否也接近30%？一个完美可靠的预报系统，其 $f_k$ 应等于 $o_k$，此时REL为0。因此，REL是一个惩罚项，越小越好。
2.  **分辨率 (Resolution, RES)**：$\mathrm{RES} = \sum_{k=1}^{K} w_k (o_k - \bar{o})^2$。其中 $\bar{o}$ 是所有样本的平均气候频率。分辨率衡量的是预报系统区分不同天气情景的能力。一个有用的预报应该能将事件（如降水）和非事件（无降水）分离到具有不同观测频率的箱中。如果一个预报系统有分辨率，那么其 $o_k$ 值将围绕着气候平均值 $\bar{o}$ 变化。分辨率是一个奖励项，越大越好，因为它在BS的分解式中被减去。
3.  **不确定性 (Uncertainty, UNC)**：$\mathrm{UNC} = \bar{o}(1-\bar{o})$。不确定性衡量的是被预报事件本身固有的变率，它只与观测的气候频率有关，与任何预报系统都无关。对于一个给定的[检验数](@entry_id:173345)据集，它是一个常数，代表了预报所面临的“挑战难度”。

通过一个具体的数值算例 ，我们可以看到，一个预报系统的总Brier评分可能是 $0.1634$。这个数字本身意义有限。但通过分解，我们可能发现其可靠性项很小（如 $0.00232$），说明预报概率很准；分辨率项较大（如 $0.08284$），说明它能有效地区分高风险和低风险情景；而不确定性项（如 $0.24392$）则定义了该预报问题的基线难度。这种分解使得对预报系统优劣的诊断更加深刻。

#### 真正评分规则

在设计或选择评分规则时，一个至关重要的理论性质是**“真正性” (propriety)**。一个评分规则被称为**真正评分规则 (proper scoring rule)**，如果一个理性的预报员为了使自己的期望得分最优，会选择汇报其内心真实的最佳判断概率，而不是出于策略考虑而汇报一个不同的值 。如果只有在汇报真实概率时才能取得唯一最优的期望得分，则该规则是**严格真正的 (strictly proper)**。

这个性质为何重要？因为它确保了[预报检验](@entry_id:1125232)的公平性和诚实性。一个真正的评分规则奖励诚实的、反映预报员真实知识水平的预报，而惩罚“投机取巧”或“博弈”行为。Brier评分和连续分级概率评分 (CRPS) 都是严格真正的评分规则，这也是它们被广泛使用的原因。从理论上看，真正评分规则与[凸分析](@entry_id:273238)理论紧密相关。每一个真正评分规则都对应一个凸[势函数](@entry_id:176105)，预报的期望得分与该函数所诱导的Bregman散度有关，从而保证了诚实预报的期望得分最高。

### 技巧评分：与基准预报的比较

一个检验指标的原始分值（如RMSE为 $2.5^\circ\text{C}$）本身往往难以解释其好坏。一个预报“好”或“坏”的判断，通常是相对的。因此，我们需要将预报与一些简单的**基准预报 (baseline forecasts)** 进行比较，以评估其“技巧”所在 。

#### 基准预报

常用的基准预报包括：
*   **气候预报 (Climatology Forecast)**：使用长期的平均状态作为预报。例如，对温度距平的预报，气候预报就是0；对降水事件的概率预报，气候预报就是其气候概率 $p_c$。气候预报代表了没有任何关于当天天气形势信息的预报水平。
*   **持续性预报 (Persistence Forecast)**：假设未来的天气与当前（或最近）的天气相同。例如，预报 $\tau$ 小时后的温度就是当前温度。对于具有强自相关性的变量（如温度），在短临预报中，持续性预报往往是一个非常难以击败的强大基准。其[均方误差](@entry_id:175403)对于一个[平稳过程](@entry_id:196130)可以表示为 $\mathrm{MSE}_{pers} = 2\sigma^2(1 - \rho(\tau))$，其中 $\rho(\tau)$ 是滞后 $\tau$ 的自相关系数。当 $\rho(\tau)$ 接近1时，$\mathrm{MSE}_{pers}$ 趋近于0。
*   **随机预报 (Random Chance)**：指预报与观测在统计上完全独立。在分类预报中，它用于计算一些旨在剔除随机一致性的评分，如Equitable Threat Score (ETS)。

#### 技巧评分的定义与解释

基于基准预报，我们可以定义**技巧评分 (Skill Score)**。对于一个“越低越好”的评分 $S$（如MSE或BS），其相对于参考预报评分 $S_{\mathrm{ref}}$ 的技巧评分 $SS$ 定义为：
$$
SS = 1 - \frac{S}{S_{\mathrm{ref}}} = \frac{S_{\mathrm{ref}} - S}{S_{\mathrm{ref}}}
$$

这个公式的含义非常直观：它衡量了预报相对于参考预报的表现改进的百分比。
*   $SS=1$：完美预报（当 $S=0$ 时）。
*   $SS=0$：预报水平与参考预报相当（当 $S = S_{\mathrm{ref}}$ 时）。
*   $SS  0$：预报水平劣于参考预报（当 $S  S_{\mathrm{ref}}$ 时）。

例如，**[Brier技巧评分](@entry_id:1121884) (BSS)** 就是以气候预报的Brier评分 $BS_{clim} = \bar{o}(1-\bar{o})$ 作为参考来计算的。同样，**[距平相关系数](@entry_id:1121047) (ACC)** 本质上也是一种技巧评分，因为它衡量的是预报与观测距平（即剔除了气候平均值后）的[线性关联](@entry_id:912650)，ACC接近0就意味着相对于气候预报没有展现出线性预报技巧 。

使用技巧评分时必须注意，**为了公平地比较不同预报系统的技巧，必须使用相同的数据集和相同的参考预报**。如果两个系统使用了不同的参考基准（例如，一个系统与持续性预报比，另一个与气候预报比），它们的技巧评分将失去可比性 。此外，当参考预报本身非常完美（即 $S_{\mathrm{ref}}$ 趋近于0）时，技巧评分会变得极不稳定，即使一个相当好的预报也可能得到一个巨大的负分，这会影响评分的[可解释性](@entry_id:637759)。

### 高分辨率[预报检验](@entry_id:1125232)中的挑战

随着NWP模式分辨率的不断提高，传统的点对点或格点对格点的检验方法面临着新的挑战，其中最著名的是**[双重惩罚问题](@entry_id:1123950) (Double-Penalty Problem)**。

#### [双重惩罚问题](@entry_id:1123950)

这个问题在检验高分辨率的降水等空间不连续、形态多样的场时尤为突出 。假设观测显示一个降水雨带位于位置A，而一个高分辨率模式准确地预报了雨带的强度、形状和大小，但位置上出现了轻微的偏移，预报在了邻近的位置B。

如果我们采用传统的、逐个像素比较的二元检验方法（例如，计算命中、漏报、空报），会发生什么呢？
1.  在观测雨带的位置A，观测有雨，但预报无雨，这被记为一次**漏报 (miss)**。
2.  在预报雨带的位置B，预报有雨，但观测无雨，这被记为一次**空报 (false alarm)**。

尽管从[气象学](@entry_id:264031)家的角度看，这是一个相当不错的预报（仅仅是“差之毫厘”），但传统的检验指标却对这一个单一的空间位移事件给予了两次惩罚。这导致诸如[临界成功指数](@entry_id:1123210) (CSI) 或[公平威胁评分](@entry_id:1124616) (ETS) 等指标得分极低，无法客观反映高分辨率模式在刻画中小尺度系统方面的真实能力。这种现象就是“双重惩罚”。不仅是二元评分，像[均方根误差 (RMSE)](@entry_id:1131101) 这样的连续评分也同样存在此问题，因为空间上的错位会在两个区域都产生巨大的点对点误差。

为了应对[双重惩罚问题](@entry_id:1123950)，研究者们开发了多种新型的空间检验方法。这些方法的核心思想是引入某种形式的“空间容差”，不再要求预报和观测在空间上精确匹配。例如，**邻域法 (neighborhood methods)**，如**[分数技巧评分](@entry_id:1125282) (Fractions Skill Score, FSS)**，通过在不同大小的邻域窗口内比较预报和观测的事件频率来评估技巧。如果预报的雨带虽然位置稍有偏移，但仍落在同一个较大的邻域窗口内，那么它依然可以为FSS贡献正技巧，从而有效缓解了[双重惩罚问题](@entry_id:1123950)，更公平地评估了高分辨率预报的价值 。