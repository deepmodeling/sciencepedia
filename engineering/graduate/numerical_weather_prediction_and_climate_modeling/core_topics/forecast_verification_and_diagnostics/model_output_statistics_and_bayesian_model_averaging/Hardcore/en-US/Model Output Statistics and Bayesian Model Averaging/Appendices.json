{
    "hands_on_practices": [
        {
            "introduction": "Evaluating the quality of a probabilistic forecast requires a metric that goes beyond simple error measures. The Continuous Ranked Probability Score (CRPS) is a widely used proper scoring rule that generalizes the mean absolute error, holistically assessing both the reliability and sharpness of a forecast distribution. This practice provides a deep dive into the mathematical structure of the CRPS, demonstrating through calculation how it quantifies the improvement gained from statistical post-processing techniques like Model Output Statistics (MOS) .",
            "id": "4065230",
            "problem": "A site-specific $2\\,\\mathrm{m}$ air temperature forecast from a Numerical Weather Prediction (NWP) system is verified against an observation. The raw ensemble is approximated by a Gaussian predictive distribution with mean $\\mu_{\\mathrm{r}} = 289\\,\\mathrm{K}$ and standard deviation $\\sigma_{\\mathrm{r}} = 2\\,\\mathrm{K}$. A calibrated predictive distribution obtained via Model Output Statistics (MOS) and consistent with Bayesian Model Averaging (BMA) principles is represented by a Gaussian with mean $\\mu_{\\mathrm{c}} = 291\\,\\mathrm{K}$ and standard deviation $\\sigma_{\\mathrm{c}} = 2.5\\,\\mathrm{K}$. The verifying observation is $y = 291\\,\\mathrm{K}$. \n\nStarting from the formal definition of the Continuous Ranked Probability Score (CRPS) as a functional of a predictive cumulative distribution function and the observation, derive a closed-form expression for the CRPS of a normal predictive distribution in terms of the standardized error $z = (y - \\mu)/\\sigma$, the standard normal probability density function, and the standard normal cumulative distribution function. Then, use this expression to compute the CRPS for both the raw ensemble ($\\mu_{\\mathrm{r}}, \\sigma_{\\mathrm{r}}$) and the calibrated MOS/BMA distribution ($\\mu_{\\mathrm{c}}, \\sigma_{\\mathrm{c}}$) at the verifying observation $y$. \n\nDefine the improvement due to calibration as $\\Delta = \\mathrm{CRPS}_{\\mathrm{raw}} - \\mathrm{CRPS}_{\\mathrm{calibrated}}$. Compute $\\Delta$ and express your final numerical answer in $\\mathrm{K}$. Round your final answer to four significant figures. In addition, explain qualitatively, using the derived expression and scientifically supported reasoning, how calibration of the ensemble (bias correction and spread adjustment) affects the CRPS relative to the raw ensemble.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Extract Givens\n- Verifying observation: $y = 291\\,\\mathrm{K}$\n- Raw ensemble predictive distribution: A Gaussian distribution with mean $\\mu_{\\mathrm{r}} = 289\\,\\mathrm{K}$ and standard deviation $\\sigma_{\\mathrm{r}} = 2\\,\\mathrm{K}$.\n- Calibrated MOS/BMA predictive distribution: A Gaussian distribution with mean $\\mu_{\\mathrm{c}} = 291\\,\\mathrm{K}$ and standard deviation $\\sigma_{\\mathrm{c}} = 2.5\\,\\mathrm{K}$.\n- Definition of improvement: $\\Delta = \\mathrm{CRPS}_{\\mathrm{raw}} - \\mathrm{CRPS}_{\\mathrm{calibrated}}$.\n- Task 1: Derive the closed-form expression for the Continuous Ranked Probability Score (CRPS) for a normal predictive distribution $N(\\mu, \\sigma^2)$ and an observation $y$, in terms of the standardized error $z = (y - \\mu)/\\sigma$, the standard normal probability density function $\\phi(z)$, and the standard normal cumulative distribution function $\\Phi(z)$.\n- Task 2: Compute $\\mathrm{CRPS}_{\\mathrm{raw}}$ and $\\mathrm{CRPS}_{\\mathrm{calibrated}}$.\n- Task 3: Compute $\\Delta$ to four significant figures.\n- Task 4: Provide a qualitative explanation for how calibration affects the CRPS.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-grounded in the field of numerical weather prediction and forecast verification. The Continuous Ranked Probability Score (CRPS), Model Output Statistics (MOS), and Bayesian Model Averaging (BMA) are all standard and well-established concepts. The physical quantities and their values (air temperature in Kelvin, standard deviations) are realistic. The problem is self-contained, with all necessary data and definitions provided. The tasks are clearly stated and lead to a unique, meaningful solution. There are no scientific or logical contradictions, ambiguities, or unrealistic assumptions.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. The solution process will now proceed.\n\n### Derivation of the CRPS for a Normal Distribution\nThe CRPS for a predictive cumulative distribution function (CDF) $F(x)$ and a single scalar observation $y$ can be defined using an alternative, computationally convenient form:\n$$\n\\mathrm{CRPS}(F, y) = E_F[|X - y|] - \\frac{1}{2} E_F[|X - X'|]\n$$\nwhere $X$ and $X'$ are independent random variables drawn from the predictive distribution with CDF $F$.\n\nIn this problem, the predictive distribution is Gaussian, $X \\sim N(\\mu, \\sigma^2)$.\n\nFirst, we evaluate the second term, $\\frac{1}{2} E_F[|X - X'|]$.\nLet $Z = X - X'$. Since $X$ and $X'$ are independent and identically distributed as $N(\\mu, \\sigma^2)$, their difference $Z$ follows a normal distribution with mean $E[Z] = E[X] - E[X'] = \\mu - \\mu = 0$ and variance $\\mathrm{Var}(Z) = \\mathrm{Var}(X) + \\mathrm{Var}(X') = \\sigma^2 + \\sigma^2 = 2\\sigma^2$. So, $Z \\sim N(0, 2\\sigma^2)$. The standard deviation of $Z$ is $\\sigma_Z = \\sqrt{2}\\sigma$.\nThe expectation of the absolute value of a zero-mean normal variable $Z$ is its mean absolute deviation, given by $\\sigma_Z \\sqrt{2/\\pi}$.\n$$\nE[|X - X'|] = E[|Z|] = (\\sqrt{2}\\sigma) \\sqrt{\\frac{2}{\\pi}} = \\frac{2\\sigma}{\\sqrt{\\pi}}\n$$\nThus, the second term is:\n$$\n\\frac{1}{2} E[|X - X'|] = \\frac{1}{2} \\left( \\frac{2\\sigma}{\\sqrt{\\pi}} \\right) = \\frac{\\sigma}{\\sqrt{\\pi}}\n$$\n\nNext, we evaluate the first term, $E_F[|X - y|]$.\nLet $W = X - y$. The random variable $W$ follows a normal distribution with mean $E[W] = E[X] - y = \\mu - y$ and variance $\\mathrm{Var}(W) = \\mathrm{Var}(X) = \\sigma^2$. So, $W \\sim N(\\mu - y, \\sigma^2)$.\nWe need to compute $E[|W|]$. Let $\\mu_W = \\mu - y$.\n$$\nE[|W|] = \\int_{-\\infty}^{\\infty} |w| \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(w - \\mu_W)^2}{2\\sigma^2}\\right) dw\n$$\nLet's perform a change of variables to the standard normal space: $u = (w - \\mu_W)/\\sigma$, so $w = \\sigma u + \\mu_W$ and $dw = \\sigma du$. The standard normal PDF is $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-u^2/2)$.\n$$\nE[|W|] = \\int_{-\\infty}^{\\infty} |\\sigma u + \\mu_W| \\phi(u) du = \\sigma \\int_{-\\infty}^{\\infty} |u + \\mu_W/\\sigma| \\phi(u) du\n$$\nWe recognize the standardized error $z = (y - \\mu)/\\sigma$, so $\\mu_W/\\sigma = (\\mu-y)/\\sigma = -z$.\n$$\nE[|X - y|] = \\sigma \\int_{-\\infty}^{\\infty} |u - z| \\phi(u) du\n$$\nWe split the integral at $u=z$:\n$$\nE[|X - y|] = \\sigma \\left[ \\int_{-\\infty}^{z} (z-u)\\phi(u)du + \\int_{z}^{\\infty} (u-z)\\phi(u)du \\right]\n$$\n$$\n= \\sigma \\left[ z\\int_{-\\infty}^{z}\\phi(u)du - \\int_{-\\infty}^{z}u\\phi(u)du + \\int_{z}^{\\infty}u\\phi(u)du - z\\int_{z}^{\\infty}\\phi(u)du \\right]\n$$\nWe use the properties of the standard normal distribution:\n- $\\int_{-\\infty}^{z}\\phi(u)du = \\Phi(z)$\n- $\\int_{z}^{\\infty}\\phi(u)du = 1 - \\Phi(z)$\n- $\\int u\\phi(u)du = -\\phi(u)$ (since $d\\phi(u)/du = -u\\phi(u)$), so $\\int_{-\\infty}^{z}u\\phi(u)du = [-\\phi(u)]_{-\\infty}^{z} = -\\phi(z)$, and $\\int_{z}^{\\infty}u\\phi(u)du = [-\\phi(u)]_{z}^{\\infty} = \\phi(z)$.\n\nSubstituting these results:\n$$\nE[|X - y|] = \\sigma \\left[ z\\Phi(z) - (-\\phi(z)) + (\\phi(z)) - z(1 - \\Phi(z)) \\right]\n$$\n$$\n= \\sigma \\left[ z\\Phi(z) + \\phi(z) + \\phi(z) - z + z\\Phi(z) \\right]\n$$\n$$\n= \\sigma \\left[ 2z\\Phi(z) - z + 2\\phi(z) \\right] = \\sigma \\left[ z(2\\Phi(z) - 1) + 2\\phi(z) \\right]\n$$\nCombining both terms, we get the final expression for the CRPS of a normal distribution:\n$$\n\\mathrm{CRPS}(N(\\mu, \\sigma^2), y) = \\sigma \\left[ z(2\\Phi(z) - 1) + 2\\phi(z) \\right] - \\frac{\\sigma}{\\sqrt{\\pi}}\n$$\n$$\n\\mathrm{CRPS} = \\sigma \\left( z(2\\Phi(z) - 1) + 2\\phi(z) - \\frac{1}{\\sqrt{\\pi}} \\right)\n$$\nThis is the required closed-form expression.\n\n### Computation of CRPS for Raw and Calibrated Ensembles\nWe will use the following numerical values for the constants:\n- $\\Phi(0) = 0.5$\n- $\\phi(0) = 1/\\sqrt{2\\pi} \\approx 0.398942$\n- For $z=1$: $\\Phi(1) \\approx 0.841345$, $\\phi(1) = \\exp(-0.5)/\\sqrt{2\\pi} \\approx 0.241971$\n- $1/\\sqrt{\\pi} \\approx 0.564190$\n\n**1. CRPS for the raw ensemble ($\\mathrm{CRPS}_{\\mathrm{raw}}$)**\n- Given: $\\mu_{\\mathrm{r}} = 289\\,\\mathrm{K}$, $\\sigma_{\\mathrm{r}} = 2\\,\\mathrm{K}$, $y = 291\\,\\mathrm{K}$.\n- Standardized error: $z_{\\mathrm{r}} = (291 - 289)/2 = 1$.\n- $\\mathrm{CRPS}_{\\mathrm{raw}} = \\sigma_{\\mathrm{r}} \\left( z_{\\mathrm{r}}(2\\Phi(z_{\\mathrm{r}}) - 1) + 2\\phi(z_{\\mathrm{r}}) - \\frac{1}{\\sqrt{\\pi}} \\right)$\n- $\\mathrm{CRPS}_{\\mathrm{raw}} = 2 \\left( 1(2\\Phi(1) - 1) + 2\\phi(1) - \\frac{1}{\\sqrt{\\pi}} \\right)$\n- $\\mathrm{CRPS}_{\\mathrm{raw}} \\approx 2 \\left( 1(2 \\times 0.841345 - 1) + 2 \\times 0.241971 - 0.564190 \\right)$\n- $\\mathrm{CRPS}_{\\mathrm{raw}} \\approx 2 \\left( (1.68269 - 1) + 0.483942 - 0.564190 \\right)$\n- $\\mathrm{CRPS}_{\\mathrm{raw}} \\approx 2 \\left( 0.68269 + 0.483942 - 0.564190 \\right) = 2(0.602442) \\approx 1.20488\\,\\mathrm{K}$.\n\n**2. CRPS for the calibrated ensemble ($\\mathrm{CRPS}_{\\mathrm{calibrated}}$)**\n- Given: $\\mu_{\\mathrm{c}} = 291\\,\\mathrm{K}$, $\\sigma_{\\mathrm{c}} = 2.5\\,\\mathrm{K}$, $y = 291\\,\\mathrm{K}$.\n- Standardized error: $z_{\\mathrm{c}} = (291 - 291)/2.5 = 0$.\n- $\\mathrm{CRPS}_{\\mathrm{calibrated}} = \\sigma_{\\mathrm{c}} \\left( z_{\\mathrm{c}}(2\\Phi(z_{\\mathrm{c}}) - 1) + 2\\phi(z_{\\mathrm{c}}) - \\frac{1}{\\sqrt{\\pi}} \\right)$\n- With $z_c = 0$, the first term becomes $0$.\n- $\\mathrm{CRPS}_{\\mathrm{calibrated}} = 2.5 \\left( 2\\phi(0) - \\frac{1}{\\sqrt{\\pi}} \\right)$\n- $\\mathrm{CRPS}_{\\mathrm{calibrated}} \\approx 2.5 \\left( 2 \\times 0.398942 - 0.564190 \\right)$\n- $\\mathrm{CRPS}_{\\mathrm{calibrated}} \\approx 2.5 \\left( 0.797884 - 0.564190 \\right) = 2.5(0.233694) \\approx 0.584235\\,\\mathrm{K}$.\n\n### Computation of Improvement ($\\Delta$)\nThe improvement is the reduction in CRPS due to calibration.\n$$\n\\Delta = \\mathrm{CRPS}_{\\mathrm{raw}} - \\mathrm{CRPS}_{\\mathrm{calibrated}}\n$$\n$$\n\\Delta \\approx 1.20488\\,\\mathrm{K} - 0.584235\\,\\mathrm{K} \\approx 0.620645\\,\\mathrm{K}\n$$\nRounding to four significant figures, we get $\\Delta \\approx 0.6206\\,\\mathrm{K}$.\n\n### Qualitative Explanation\nThe derived formula, $\\mathrm{CRPS} = \\sigma ( z(2\\Phi(z) - 1) + 2\\phi(z) - 1/\\sqrt{\\pi} )$, reveals how the two main components of calibration—bias correction and spread adjustment—affect the score. The CRPS is a proper scoring rule that assesses both the sharpness (concentration) and reliability (statistical consistency) of a probabilistic forecast. A lower CRPS indicates a better forecast.\n\n1.  **Bias Correction (affecting $z$):** Bias correction adjusts the mean $\\mu$ of the forecast distribution to be closer, on average, to the observed outcome $y$. This directly reduces the magnitude of the standardized error, $|z| = |y-\\mu|/\\sigma$. The term in the parentheses, $g(z) = z(2\\Phi(z) - 1) + 2\\phi(z) - 1/\\sqrt{\\pi}$, is a non-negative function of $z$ that has a global minimum at $z=0$ (i.e., when the forecast mean equals the observation). Therefore, reducing bias by bringing $\\mu$ closer to $y$ (and thus $z$ closer to $0$) will always reduce the value of $g(z)$, leading to a lower (better) CRPS, provided $\\sigma$ is held constant. In this problem, the calibration was perfect in its bias correction for this specific instance, moving $\\mu$ from $289\\,\\mathrm{K}$ to $291\\,\\mathrm{K}$, which coincided with the observation. This changed $z$ from $1$ to $0$, substantially lowering the value of $g(z)$ and thus providing a large improvement to the score.\n\n2.  **Spread Adjustment (affecting $\\sigma$):** Spread adjustment modifies the standard deviation $\\sigma$ of the forecast distribution. Raw ensembles are often under-dispersive (overconfident), meaning their spread is too small to accurately represent the true forecast uncertainty. Calibration, particularly using methods like BMA, often increases the spread to a more reliable value. The impact of changing $\\sigma$ on the CRPS is twofold:\n    - There is a direct linear scaling factor $\\sigma$ in front of the expression. Increasing $\\sigma$ directly increases the CRPS, penalizing forecasts that are not sharp.\n    - The value of $z$ also depends on $\\sigma$.\n    In our specific case, the spread was increased from $\\sigma_{\\mathrm{r}} = 2\\,\\mathrm{K}$ to $\\sigma_{\\mathrm{c}} = 2.5\\,\\mathrm{K}$. For the calibrated case, $z=0$, so $\\mathrm{CRPS}_{\\mathrm{calibrated}} \\propto \\sigma_{\\mathrm{c}}$. The increase in spread from $2\\,\\mathrm{K}$ to $2.5\\,\\mathrm{K}$ acted to worsen the score. However, this penalty was much smaller than the substantial gain from eliminating the bias (reducing $z$ from $1$ to $0$). The net effect was a significant improvement ($\\Delta > 0$). This illustrates the fundamental trade-off that CRPS evaluates: the calibrated forecast is less sharp (larger $\\sigma$) but more reliable (unbiased), and in this case, the gain in reliability far outweighed the loss in sharpness. For a deterministic forecast ($\\sigma \\to 0$), the CRPS converges to the absolute error, $|y-\\mu|$. Calibration ensures the probabilistic forecast is penalized for being overconfident when it is wrong.\n\nIn summary, calibration improved the overall forecast quality by correcting the significant bias of the raw ensemble, even though it required increasing the forecast spread. The CRPS metric correctly identifies this as a net improvement.",
            "answer": "$$\\boxed{0.6206}$$"
        },
        {
            "introduction": "A key benefit of probabilistic forecasting is the ability to generate prediction intervals that quantify uncertainty. For these intervals to be trustworthy, the forecast must be well-calibrated, meaning a nominal $(1-\\alpha)$ interval should contain the true outcome with probability $(1-\\alpha)$. This exercise explores the consequences of miscalibration, specifically how errors in the estimated forecast variance affect the actual coverage probability of a prediction interval .",
            "id": "4065264",
            "problem": "In numerical weather prediction, postprocessing using Model Output Statistics (MOS) aims to correct systematic errors in raw model output and produce a predictive distribution for a weather quantity (for example, near-surface temperature) at a given site and lead time. A common, well-tested approach in MOS is to assume that conditional on predictors (e.g., ensemble mean and spread), the verifying observation $Y$ is Gaussian with a mean corrected by linear regression and a variance estimated from past errors. Suppose the MOS predictive distribution for $Y$ given predictors is Gaussian with mean $\\mu_{\\text{MOS}}$ and variance $\\sigma_{\\text{MOS}}^{2}$, and prediction intervals are constructed by inverting this predictive cumulative distribution function. The intended nominal two-sided prediction interval at level $1-\\alpha$ uses symmetric quantiles around the MOS mean. However, in practice, variance estimates can be miscalibrated relative to the true verifying distribution due to sampling variability, regime shifts, or imperfect spread-skill relationships, even when the predictive mean is unbiased.\n\nAssume the following scientifically realistic and self-consistent setting:\n- The MOS predictive distribution is $Y \\mid \\text{predictors} \\sim \\mathcal{N}\\!\\left(\\mu_{\\text{MOS}},\\,\\sigma_{\\text{MOS}}^{2}\\right)$.\n- The actual verifying distribution is unbiased in mean but has a multiplicative variance miscalibration factor $\\lambda>0$: $Y \\mid \\text{predictors} \\sim \\mathcal{N}\\!\\left(\\mu_{\\text{MOS}},\\,\\lambda\\,\\sigma_{\\text{MOS}}^{2}\\right)$.\n- Let $\\Phi$ denote the cumulative distribution function of the standard normal distribution, and let $\\Phi^{-1}$ denote its quantile function. Let $z_{p}=\\Phi^{-1}(p)$ for $p\\in(0,1)$.\n\nTask:\n1. Starting from the definition of a prediction interval as the inverse image of the predictive cumulative distribution function, construct the nominal two-sided $(1-\\alpha)$ prediction interval for $Y$ implied by the MOS predictive distribution.\n2. Using the frequentist definition of coverage probability and the properties of the Gaussian distribution, derive the actual coverage probability of this nominal $(1-\\alpha)$ prediction interval under the true verifying distribution with variance factor $\\lambda$.\n\nExpress your final answer as a single closed-form analytic expression for the actual coverage probability in terms of $\\alpha$ and $\\lambda$. No numerical evaluation is required. If you introduce any additional functions, clearly define them in your derivation. The final answer must be a single expression and must not include units or inequalities.",
            "solution": "The problem statement has been validated and is deemed scientifically sound, well-posed, and free of contradictions. The task is to derive the actual coverage probability of a nominal prediction interval when the variance of the underlying Gaussian process is miscalibrated.\n\nThe derivation proceeds in two parts as requested.\n\n**Part 1: Construction of the Nominal Prediction Interval**\n\nThe nominal prediction interval is constructed based on the assumed Model Output Statistics (MOS) predictive distribution. This distribution is given as a normal distribution for the verifying observation $Y$, conditional on the predictors:\n$$ Y \\mid \\text{predictors} \\sim \\mathcal{N}\\!\\left(\\mu_{\\text{MOS}},\\,\\sigma_{\\text{MOS}}^{2}\\right) $$\nA two-sided prediction interval with a nominal coverage level of $1-\\alpha$ is defined by the quantiles of this distribution. Specifically, a symmetric interval is constructed using the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles.\n\nLet $F_{\\text{MOS}}$ be the cumulative distribution function (CDF) of this nominal distribution. The interval $[L, U]$ is constructed such that $F_{\\text{MOS}}(L) = \\frac{\\alpha}{2}$ and $F_{\\text{MOS}}(U) = 1 - \\frac{\\alpha}{2}$. To find these quantiles, we standardize the random variable $Y$ according to the nominal distribution. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$.\n$$ Z = \\frac{Y - \\mu_{\\text{MOS}}}{\\sigma_{\\text{MOS}}} $$\nThe quantiles of the standard normal distribution are denoted by $z_{p} = \\Phi^{-1}(p)$, where $\\Phi$ is the CDF of the standard normal distribution. The desired quantiles for $Z$ are $z_{\\alpha/2}$ and $z_{1-\\alpha/2}$.\nThe nominal interval is constructed to satisfy:\n$$ P(z_{\\alpha/2} \\le Z \\le z_{1-\\alpha/2}) = 1 - \\alpha $$\nSubstituting the expression for $Z$:\n$$ P\\left(z_{\\alpha/2} \\le \\frac{Y - \\mu_{\\text{MOS}}}{\\sigma_{\\text{MOS}}} \\le z_{1-\\alpha/2}\\right) = 1 - \\alpha $$\nWe can solve for $Y$ by rearranging the inequality:\n$$ \\mu_{\\text{MOS}} + \\sigma_{\\text{MOS}}z_{\\alpha/2} \\le Y \\le \\mu_{\\text{MOS}} + \\sigma_{\\text{MOS}}z_{1-\\alpha/2} $$\nThe nominal $(1-\\alpha)$ prediction interval, denoted as $I_{\\text{nom}}$, is therefore:\n$$ I_{\\text{nom}} = [\\mu_{\\text{MOS}} + \\sigma_{\\text{MOS}}z_{\\alpha/2}, \\mu_{\\text{MOS}} + \\sigma_{\\text{MOS}}z_{1-\\alpha/2}] $$\nDue to the symmetry of the standard normal distribution, we have $z_{\\alpha/2} = -z_{1-\\alpha/2}$. The interval can be expressed more compactly as:\n$$ I_{\\text{nom}} = [\\mu_{\\text{MOS}} - \\sigma_{\\text{MOS}}z_{1-\\alpha/2}, \\mu_{\\text{MOS}} + \\sigma_{\\text{MOS}}z_{1-\\alpha/2}] $$\n\n**Part 2: Derivation of the Actual Coverage Probability**\n\nThe actual coverage probability is the probability that a random draw from the *true* verifying distribution falls within the *nominal* prediction interval $I_{\\text{nom}}$.\nThe true distribution is given as:\n$$ Y \\mid \\text{predictors} \\sim \\mathcal{N}\\!\\left(\\mu_{\\text{MOS}},\\,\\lambda\\,\\sigma_{\\text{MOS}}^{2}\\right) $$\nwhere $\\lambda > 0$ is the multiplicative variance miscalibration factor. The key features are that the mean is still $\\mu_{\\text{MOS}}$ (unbiased), but the true variance is $\\lambda\\sigma_{\\text{MOS}}^2$, and thus the true standard deviation is $\\sqrt{\\lambda}\\sigma_{\\text{MOS}}$.\n\nWe need to calculate the probability $P(Y \\in I_{\\text{nom}})$, where $Y$ follows this true distribution.\n$$ P_{\\text{actual}} = P(\\mu_{\\text{MOS}} - \\sigma_{\\text{MOS}}z_{1-\\alpha/2} \\le Y \\le \\mu_{\\text{MOS}} + \\sigma_{\\text{MOS}}z_{1-\\alpha/2}) $$\nTo evaluate this probability, we must standardize $Y$ using its true mean and true standard deviation. Let $Z'$ be a standard normal random variable defined as:\n$$ Z' = \\frac{Y - \\mu_{\\text{MOS}}}{\\sqrt{\\lambda}\\sigma_{\\text{MOS}}} \\sim \\mathcal{N}(0, 1) $$\nWe now transform the inequality for $Y$ into an inequality for $Z'$.\n\nFirst, subtract the mean $\\mu_{\\text{MOS}}$ from all parts of the inequality:\n$$ -\\sigma_{\\text{MOS}}z_{1-\\alpha/2} \\le Y - \\mu_{\\text{MOS}} \\le \\sigma_{\\text{MOS}}z_{1-\\alpha/2} $$\nNext, divide all parts by the true standard deviation, $\\sqrt{\\lambda}\\sigma_{\\text{MOS}}$ (since $\\lambda > 0$ and $\\sigma_{\\text{MOS}} > 0$, the inequality direction is preserved):\n$$ \\frac{-\\sigma_{\\text{MOS}}z_{1-\\alpha/2}}{\\sqrt{\\lambda}\\sigma_{\\text{MOS}}} \\le \\frac{Y - \\mu_{\\text{MOS}}}{\\sqrt{\\lambda}\\sigma_{\\text{MOS}}} \\le \\frac{\\sigma_{\\text{MOS}}z_{1-\\alpha/2}}{\\sqrt{\\lambda}\\sigma_{\\text{MOS}}} $$\nSimplifying the expressions on the left and right sides yields:\n$$ -\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}} \\le Z' \\le \\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}} $$\nThe actual coverage probability is the probability that the standard normal variable $Z'$ lies within this new interval:\n$$ P_{\\text{actual}} = P\\left(-\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}} \\le Z' \\le \\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}}\\right) $$\nThis probability can be expressed using the standard normal CDF, $\\Phi(x)$:\n$$ P_{\\text{actual}} = \\Phi\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}}\\right) - \\Phi\\left(-\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}}\\right) $$\nUsing the symmetry property of the standard normal CDF, $\\Phi(-x) = 1 - \\Phi(x)$, we can simplify this expression:\n$$ P_{\\text{actual}} = \\Phi\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}}\\right) - \\left[1 - \\Phi\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}}\\right)\\right] $$\n$$ P_{\\text{actual}} = 2\\Phi\\left(\\frac{z_{1-\\alpha/2}}{\\sqrt{\\lambda}}\\right) - 1 $$\nFinally, we substitute the definition $z_{1-\\alpha/2} = \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)$ to obtain the final expression for the actual coverage probability in terms of $\\alpha$ and $\\lambda$:\n$$ P_{\\text{actual}} = 2\\Phi\\left(\\frac{\\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)}{\\sqrt{\\lambda}}\\right) - 1 $$\nThis is the required closed-form analytic expression.",
            "answer": "$$\n\\boxed{2\\Phi\\left(\\frac{\\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right)}{\\sqrt{\\lambda}}\\right) - 1}\n$$"
        },
        {
            "introduction": "Estimating the true out-of-sample performance of a forecasting model is a critical step in its development. Standard cross-validation techniques often rely on an assumption of data exchangeability, which is typically violated by the complex temporal dependencies present in meteorological data. This exercise challenges you to design a robust validation strategy that accounts for these dependencies, thereby preventing data leakage and ensuring an unbiased estimate of real-world model skill .",
            "id": "4065253",
            "problem": "You are given a historical archive from a global ensemble Numerical Weather Prediction (NWP) system consisting of $T = 730$ consecutive daily forecast initializations at $00$ UTC, each providing $K = 6$ lead times $L \\in \\{12, 24, 36, 48, 60, 72\\}$ hours, for a single station’s $2$-meter temperature. For each initialization day $i \\in \\{1,\\dots,T\\}$ and lead time $\\ell \\in \\{1,\\dots,K\\}$, you have the ensemble mean forecast $x_{i\\ell}$ and the verifying observation $y_{i\\ell}$. You wish to assess the out-of-sample performance of a post-processing scheme that combines Model Output Statistics (MOS) and Bayesian Model Averaging (BMA). In particular, you will fit, for each lead time, a MOS linear calibration of the form $y_{i\\ell} = \\beta_\\ell x_{i\\ell} + \\epsilon_{i\\ell}$, and then apply Bayesian Model Averaging (BMA) across a set of $J$ candidate MOS models to yield a calibrated predictive distribution for $y_{i\\ell}$ at each lead time.\n\nAssume the following well-tested facts and conditions:\n- The MOS linear model is an accepted baseline, with least-squares estimation under Gaussian errors used for point calibration. Bayesian Model Averaging (BMA) uses likelihood-based weights across models, and proper scoring rules such as the logarithmic score and the Continuous Ranked Probability Score (CRPS) are standard for evaluating probabilistic forecasts.\n- For each initialization $i$, the MOS residual vector across lead times, $\\epsilon_{i} = (\\epsilon_{i1},\\dots,\\epsilon_{iK})^\\top$, is mean-zero Gaussian with covariance matrix having diagonal elements $\\sigma^2$ and off-diagonal elements $\\rho \\sigma^2$ for $\\ell \\neq \\ell'$, with $\\rho \\in (0,1)$ fixed. Residuals from different initializations are uncorrelated beyond a small temporal lag. Moreover, the joint distribution of $\\{(x_{i\\ell}, y_{i\\ell})\\}$ is approximately stationary over the $T$ days after seasonally detrending.\n- The conditional distribution $p(y_{i\\ell}\\,|\\,x_{i\\ell})$ depends on the lead time $\\ell$; that is, the data are not exchangeable across lead times. Therefore, mixing lead times in modeling or validation must be justified by an invariance argument you can defend.\n\nYour task is to select the cross-validation design and rationale that yields a statistically valid, approximately unbiased estimate of out-of-sample skill for both MOS and BMA at each lead time, under the stationarity and dependence structure stated above. In your reasoning, start from first principles of cross-validation consistency under exchangeability, and then account for the temporal dependence and lead-time non-exchangeability in this NWP setting. In particular, justify why mixing lead times into the same folds can invalidate performance estimates due to temporal dependence, and indicate the expected direction of bias induced by such mixing under $\\rho > 0$.\n\nWhich option best satisfies these requirements?\n\nA. For each lead time $\\ell$, perform a rolling-origin, blocked cross-validation across initializations: partition the $T$ days into $K_{\\mathrm{CV}}$ contiguous validation blocks of length $B$ days; for block $b$, fit the MOS and BMA models using only days strictly before the block and at least a gap of $g$ days to reduce residual temporal dependence, all at the same lead time $\\ell$; then evaluate proper scores (e.g., log score and CRPS) on the held-out block, and aggregate over blocks to estimate skill for that lead time. This respects the fact that $p(y_{i\\ell}\\,|\\,x_{i\\ell})$ is lead-time specific and avoids placing, in the same fold, training and test cases sharing the same initialization. Mixing lead times within folds would couple training parameter estimates to test residuals through the positive cross-lead residual correlation $\\rho$, which induces optimistic bias in estimated error metrics because training-time statistics partially “see” the test-time noise via the shared initialization.\n\nB. Randomly shuffle all pairs $\\{(x_{i\\ell}, y_{i\\ell})\\}$ across all initializations and lead times, then perform standard $K$-fold cross-validation without gaps, fitting a single MOS and BMA model that shares parameters across lead times, and compute aggregate scores. The large sample size ensures the law of large numbers eliminates concerns about serial correlation and non-exchangeability, so mixing lead times cannot bias estimated skill.\n\nC. For each initialization $i$, leave one lead time $\\ell$ out as test and use the remaining lead times from the same $i$ as training data, repeated over all initializations and leads. Because the target times differ, test and training cases are conditionally independent given $i$, so the cross-validation error is unbiased. Pool scores over all $\\ell$ to estimate overall skill.\n\nD. Use contiguous temporal blocks as folds but pool all lead times together in fitting a single MOS-BMA model that shares parameters across $\\ell$. Evaluate proper scores by averaging over all $\\ell$ in the held-out block. Since each fold is a clean temporal hold-out, pooling lead times cannot distort per-lead-time skill estimates, and any temporal dependence within an initialization is irrelevant once entire blocks are held out.\n\nChoose one option.",
            "solution": "The problem statement is valid. It presents a well-posed, scientifically grounded, and objective question in the field of numerical weather prediction (NWP) post-processing. The provided assumptions regarding the data structure, including stationarity, temporal dependence, and lead-time-specific distributions, are realistic and form a coherent basis for statistical reasoning. The task is to identify a cross-validation scheme that respects this structure to produce an unbiased estimate of out-of-sample predictive skill.\n\nThe fundamental principle underpinning standard cross-validation (CV) is data exchangeability. A dataset is exchangeable if its joint probability distribution is invariant to any permutation of its indices. This condition is met, for instance, by independent and identically distributed (i.i.d.) data. When data are not exchangeable, standard CV, which relies on random partitioning, can produce biased estimates of generalization error. The problem correctly details two violations of exchangeability:\n\n1.  **Temporal Dependence**: The statement \"Residuals from different initializations are uncorrelated beyond a small temporal lag\" implies that data points $(x_{i\\ell}, y_{i\\ell})$ and $(x_{j\\ell}, y_{j\\ell})$ are correlated if the initialization days $i$ and $j$ are close. This is a classic time series structure. A random partitioning of data into training and validation folds would place correlated data points in both sets, leading to information leakage and an optimistically-biased (i.e., underestimated) error. The correct procedure for time series is a blocked or rolling-origin cross-validation that preserves the temporal order, using past data to train and future data to test.\n\n2.  **Lead-Time Non-Exchangeability**: The problem explicitly states that \"the conditional distribution $p(y_{i\\ell}\\,|\\,x_{i\\ell})$ depends on the lead time $\\ell$\". This means that data are not identically distributed across lead times. Forecast bias and variance characteristically change with the forecast horizon. Therefore, pooling data from different lead times to fit a single model, as if they were drawn from the same distribution, constitutes a model misspecification. The task is to estimate skill \"at each lead time,\" which further mandates that models should be fitted and evaluated separately for each lead time $\\ell$.\n\nA third crucial detail is the intra-initialization correlation structure: for a fixed initialization day $i$, the forecast errors $\\epsilon_{i\\ell}$ and $\\epsilon_{i\\ell'}$ for different lead times $\\ell$ and $\\ell'$ are correlated, with $\\text{Cov}(\\epsilon_{i\\ell}, \\epsilon_{i\\ell'}) = \\rho \\sigma^2$ where $\\rho > 0$. This positive correlation means that if the model has a large positive error at one lead time for a given day's forecast, it is likely to have a positive error at other lead times for the same day's forecast. Any validation scheme that uses data from one lead time of initialization $i$ to train a model and data from another lead time of the *same* initialization $i$ to test it will suffer from severe data leakage. The training process will inadvertently learn from the noise statistics of the test set, leading to a substantial optimistic bias in performance metrics.\n\nWith these principles established, we evaluate the options.\n\n#### Option-by-Option Analysis\n\n**A. For each lead time $\\ell$, perform a rolling-origin, blocked cross-validation across initializations: partition the $T$ days into $K_{\\mathrm{CV}}$ contiguous validation blocks of length $B$ days; for block $b$, fit the MOS and BMA models using only days strictly before the block and at least a gap of $g$ days to reduce residual temporal dependence, all at the same lead time $\\ell$; then evaluate proper scores (e.g., log score and CRPS) on the held-out block, and aggregate over blocks to estimate skill for that lead time. This respects the fact that $p(y_{i\\ell}\\,|\\,x_{i\\ell})$ is lead-time specific and avoids placing, in the same fold, training and test cases sharing the same initialization. Mixing lead times within folds would couple training parameter estimates to test residuals through the positive cross-lead residual correlation $\\rho$, which induces optimistic bias in estimated error metrics because training-time statistics partially “see” the test-time noise via the shared initialization.**\n\nThis option describes a correct and rigorous methodology.\n1.  **Modeling**: It treats each lead time $\\ell$ separately (\"For each lead time $\\ell$ ...\"), which respects the stated non-exchangeability across lead times and addresses the task of estimating skill \"at each lead time.\"\n2.  **Temporal Dependence**: It employs a \"rolling-origin, blocked cross-validation\" with a \"gap of $g$ days,\" which is the state-of-the-art method for handling temporal dependence in time series forecasting, ensuring that the test set is always in the future relative to the training set and sufficiently separated to mitigate correlation.\n3.  **Intra-initialization Dependence**: By design, because a separate model is built for each lead time $\\ell$ using only data from that lead time, there is no possibility of using a data point $(x_{i\\ell}, y_{i\\ell})$ for training and $(x_{i\\ell'}, y_{i\\ell'})$ for testing. It correctly avoids this form of data leakage.\n4.  **Rationale**: The provided reasoning is sound. It correctly identifies that mixing lead times would cause data leakage due to the positive correlation $\\rho > 0$, and correctly deduces that this leakage would lead to an optimistic bias (underestimation of true error).\nThe entire description is statistically robust and addresses all the complexities of the problem.\n\n**Verdict: Correct.**\n\n**B. Randomly shuffle all pairs $\\{(x_{i\\ell}, y_{i\\ell})\\}$ across all initializations and lead times, then perform standard $K$-fold cross-validation without gaps, fitting a single MOS and BMA model that shares parameters across lead times, and compute aggregate scores. The large sample size ensures the law of large numbers eliminates concerns about serial correlation and non-exchangeability, so mixing lead times cannot bias estimated skill.**\n\nThis option is fundamentally flawed.\n1.  **Exchangeability Violation**: It completely ignores both the temporal dependence and the lead-time non-exchangeability. Randomly shuffling breaks the time series structure, guaranteeing that correlated data points will be in both training and test sets.\n2.  **Model Misspecification**: It fits a single model across all lead times, contradicting the known fact that the data-generating process depends on $\\ell$.\n3.  **Faulty Rationale**: The appeal to the \"law of large numbers\" is a gross misunderstanding of statistical theory. The law of large numbers does not negate the effect of data dependence on the bias of estimators; it only describes the convergence of sample averages. The bias problem from the invalid CV procedure remains, regardless of sample size.\n\n**Verdict: Incorrect.**\n\n**C. For each initialization $i$, leave one lead time $\\ell$ out as test and use the remaining lead times from the same $i$ as training data, repeated over all initializations and leads. Because the target times differ, test and training cases are conditionally independent given $i$, so the cross-validation error is unbiased. Pool scores over all $\\ell$ to estimate overall skill.**\n\nThis option proposes a procedure that directly falls into the trap of intra-initialization dependence.\n1.  **Data Leakage**: It explicitly trains on data from initialization $i$ (e.g., at leads $\\{12, 24, 48, 60, 72\\}$ hours) and tests on a different lead from the *same* initialization $i$ (e.g., at $36$ hours). Given $\\text{Cov}(\\epsilon_{i\\ell}, \\epsilon_{i\\ell'}) = \\rho \\sigma^2 > 0$, the training and test sets are statistically dependent. This is a severe form of data leakage.\n2.  **Faulty Rationale**: The claim of conditional independence is false. The error terms $\\epsilon_{i\\ell}$ are correlated for a given $i$, so the observations $y_{i\\ell}$ are not conditionally independent. This procedure would result in a massively optimistic bias.\n\n**Verdict: Incorrect.**\n\n**D. Use contiguous temporal blocks as folds but pool all lead times together in fitting a single MOS-BMA model that shares parameters across $\\ell$. Evaluate proper scores by averaging over all $\\ell$ in the held-out block. Since each fold is a clean temporal hold-out, pooling lead times cannot distort per-lead-time skill estimates, and any temporal dependence within an initialization is irrelevant once entire blocks are held out.**\n\nThis option correctly handles one issue but fails on another.\n1.  **Temporal Dependence**: It correctly uses \"contiguous temporal blocks as folds,\" which respects the time-series nature of the initializations.\n2.  **Model Misspecification**: It incorrectly \"pool[s] all lead times together in fitting a single ... model.\" This is a misspecification, as the relationship $p(y_{i\\ell}\\,|\\,x_{i\\ell})$ is known to be lead-time-dependent. The resulting model is suboptimal.\n3.  **Failed Goal**: Because it fits and evaluates a single pooled model, it cannot provide a skill estimate \"at each lead time,\" which was a requirement of the task. It only provides an aggregate skill for a misspecified model. The claim that this \"cannot distort per-lead-time skill estimates\" is false; the evaluation is of a pooled model, not a lead-time-specific one.\n\n**Verdict: Incorrect.**\n\nIn conclusion, Option A is the only choice that presents a statistically sound validation framework that correctly navigates the complex dependence structure of the NWP output, adhering to first principles of cross-validation for structured data. It addresses temporal dependence, lead-time non-exchangeability, and intra-initialization correlation with appropriate, standard techniques.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}