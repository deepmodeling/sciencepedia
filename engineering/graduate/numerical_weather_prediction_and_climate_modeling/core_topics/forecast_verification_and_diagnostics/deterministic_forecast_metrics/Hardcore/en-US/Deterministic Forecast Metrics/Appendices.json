{
    "hands_on_practices": [
        {
            "introduction": "A common first step in forecast post-processing is to correct for systematic biases. This exercise demonstrates the theoretical basis for this practice, tasking you with proving that the Root-Mean-Square Error ($RMSE$) is minimized when a constant additive correction is precisely equal to the forecast's mean bias . Completing this derivation provides a rigorous understanding of the fundamental relationship between a forecast's systematic error and its overall squared error.",
            "id": "4030885",
            "problem": "Consider a univariate verification of a deterministic numerical weather prediction forecast, consisting of $n$ forecast-observation pairs $\\{(f_i,o_i)\\}_{i=1}^{n}$ for a scalar atmospheric variable (e.g., daily area-mean $2$-meter temperature) over a fixed region. Define the constant-additive corrected forecast family $f_i^{*}(c) = f_i - c$, where $c \\in \\mathbb{R}$ is a constant applied identically to all forecast instances. Let the error of the corrected forecast be $e_i(c) = f_i^{*}(c) - o_i$, the sample bias be $B(c) = \\frac{1}{n}\\sum_{i=1}^{n} e_i(c)$, and the Root-Mean-Square Error (RMSE) be $R(c) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} e_i(c)^{2}}$. Assume that only an additive bias is present, in the sense that the centered second moment of the errors, $\\frac{1}{n}\\sum_{i=1}^{n}\\left(e_i(c) - B(c)\\right)^{2}$, does not depend on $c$. Denote the uncorrected error as $e_i = f_i - o_i$, and the sample means $\\bar{f} = \\frac{1}{n}\\sum_{i=1}^{n} f_i$ and $\\bar{o} = \\frac{1}{n}\\sum_{i=1}^{n} o_i$.\n\nUsing only the definitions above and standard algebraic manipulations grounded in sample moment decompositions, do the following:\n- Derive $B(c)$ as a function of $c$ and the uncorrected bias $B(0)$, and compute $\\frac{dB}{dc}$.\n- Express $R(c)^{2}$ in a form that separates a term depending on $B(c)$ from a nonnegative term that is independent of $c$, and use this to determine the value $c^{*}$ that minimizes $R(c)$ under the stated assumption.\n- Finally, express $c^{*}$ solely in terms of $\\bar{f}$ and $\\bar{o}$.\n\nProvide your final answer as a single closed-form symbolic expression for $c^{*}$ in terms of $\\bar{f}$ and $\\bar{o}$. No numerical approximation is required, and no units are needed for the final expression. Do not include any intermediate steps in the final answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Forecast-observation pairs: $\\{(f_i, o_i)\\}_{i=1}^{n}$\n- Corrected forecast: $f_i^{*}(c) = f_i - c$, for $c \\in \\mathbb{R}$\n- Corrected error: $e_i(c) = f_i^{*}(c) - o_i$\n- Sample bias: $B(c) = \\frac{1}{n}\\sum_{i=1}^{n} e_i(c)$\n- Root-Mean-Square Error (RMSE): $R(c) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} e_i(c)^{2}}$\n- Assumption: The centered second moment of the errors, $\\frac{1}{n}\\sum_{i=1}^{n}\\left(e_i(c) - B(c)\\right)^{2}$, is independent of $c$.\n- Uncorrected error: $e_i = f_i - o_i$\n- Sample means: $\\bar{f} = \\frac{1}{n}\\sum_{i=1}^{n} f_i$ and $\\bar{o} = \\frac{1}{n}\\sum_{i=1}^{n} o_i$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in the statistical verification of deterministic forecasts, specifically relating to bias correction and the decomposition of the Mean Square Error (MSE). The definitions provided are standard in meteorology and climate science. The assumption about the centered second moment of the error being independent of the correction factor $c$ is not just an arbitrary condition, but a direct consequence of the definitions, which ensures the problem's internal consistency. Let us verify this:\n$e_i(c) = (f_i - c) - o_i = (f_i - o_i) - c = e_i - c$.\n$B(c) = \\frac{1}{n}\\sum_{i=1}^{n} e_i(c) = \\frac{1}{n}\\sum_{i=1}^{n} (e_i - c) = \\left(\\frac{1}{n}\\sum_{i=1}^{n} e_i\\right) - c = B(0) - c$.\nThen, $e_i(c) - B(c) = (e_i - c) - (B(0) - c) = e_i - B(0)$.\nThe term $e_i - B(0)$ is independent of $c$. Therefore, its sample variance, $\\frac{1}{n}\\sum_{i=1}^{n}\\left(e_i(c) - B(c)\\right)^{2} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(e_i - B(0)\\right)^{2}$, is also independent of $c$. The problem is thus self-consistent, well-posed, and scientifically grounded.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be derived.\n\n**Derivation**\n\nThe first task is to derive an expression for the corrected bias $B(c)$ and its derivative with respect to $c$.\nUsing the definition of $B(c)$:\n$$B(c) = \\frac{1}{n}\\sum_{i=1}^{n} e_i(c)$$\nSubstitute the definition of the corrected error, $e_i(c) = f_i^{*}(c) - o_i = (f_i - c) - o_i$:\n$$B(c) = \\frac{1}{n}\\sum_{i=1}^{n} (f_i - c - o_i) = \\frac{1}{n}\\sum_{i=1}^{n} ((f_i - o_i) - c)$$\nThe term $(f_i - o_i)$ is the uncorrected error $e_i$. The uncorrected bias is $B(0) = \\frac{1}{n}\\sum_{i=1}^{n} (f_i - o_i)$. We can separate the summation:\n$$B(c) = \\frac{1}{n}\\sum_{i=1}^{n} (f_i - o_i) - \\frac{1}{n}\\sum_{i=1}^{n} c$$\nThe first term is $B(0)$. The second term is $\\frac{1}{n}(nc) = c$. Thus, we have:\n$$B(c) = B(0) - c$$\nThis expresses $B(c)$ as a function of the constant $c$ and the uncorrected bias $B(0)$.\nThe derivative of $B(c)$ with respect to $c$ is:\n$$\\frac{dB}{dc} = \\frac{d}{dc}(B(0) - c) = -1$$\nsince $B(0)$ is a constant with respect to $c$.\n\nThe second task is to express the squared RMSE, $R(c)^2$, in a form that separates the bias contribution. $R(c)^2$ is the Mean Square Error (MSE) of the corrected forecast.\n$$R(c)^2 = \\frac{1}{n}\\sum_{i=1}^{n} e_i(c)^2$$\nWe use the standard identity for the second moment of a sample $\\{x_i\\}$ with mean $\\bar{x}$, which is $\\frac{1}{n}\\sum x_i^2 = \\bar{x}^2 + \\frac{1}{n}\\sum (x_i - \\bar{x})^2$. In our case, the variable is $e_i(c)$ and its sample mean is $B(c)$. Applying this identity:\n$$R(c)^2 = \\left(\\frac{1}{n}\\sum_{i=1}^{n} e_i(c)\\right)^2 + \\frac{1}{n}\\sum_{i=1}^{n} \\left(e_i(c) - B(c)\\right)^2$$\n$$R(c)^2 = B(c)^2 + \\frac{1}{n}\\sum_{i=1}^{n} \\left(e_i(c) - B(c)\\right)^2$$\nThis is the required decomposition. The first term, $B(c)^2$, depends on $c$. The second term is the centered second moment of the errors, which was shown in the validation step to be independent of $c$. Let's denote this constant non-negative term as $V_e = \\frac{1}{n}\\sum_{i=1}^{n} \\left(e_i - B(0)\\right)^2$.\nThus, $R(c)^2 = B(c)^2 + V_e$.\n\nThe third task is to find the value $c = c^*$ that minimizes $R(c)$. Since $R(c)$ is non-negative, minimizing $R(c)$ is equivalent to minimizing $R(c)^2$. We need to find the minimum of the expression:\n$$R(c)^2 = (B(0) - c)^2 + V_e$$\nThe term $V_e$ is constant with respect to $c$. Therefore, minimizing $R(c)^2$ is equivalent to minimizing the term $(B(0) - c)^2$. As this is a squared real quantity, its minimum value is $0$, which is achieved when the term inside the square is zero. The minimum occurs at a value $c^*$ such that:\n$$B(0) - c^* = 0$$\nThis yields the optimal correction:\n$$c^* = B(0)$$\nThis result shows that the MSE (and thus RMSE) is minimized when the additive correction $c$ is chosen to be equal to the mean bias of the original uncorrected forecast, thereby making the corrected forecast unbiased.\n\nThe final task is to express $c^*$ in terms of the sample means $\\bar{f}$ and $\\bar{o}$.\nWe have $c^* = B(0)$. By definition, the uncorrected bias is:\n$$B(0) = \\frac{1}{n}\\sum_{i=1}^{n} (f_i - o_i)$$\nUsing the distributive property of summation:\n$$B(0) = \\frac{1}{n}\\sum_{i=1}^{n} f_i - \\frac{1}{n}\\sum_{i=1}^{n} o_i$$\nBy the definitions of the sample means $\\bar{f}$ and $\\bar{o}$, this becomes:\n$$B(0) = \\bar{f} - \\bar{o}$$\nTherefore, the value of $c$ that minimizes the RMSE is:\n$$c^* = \\bar{f} - \\bar{o}$$",
            "answer": "$$\\boxed{\\bar{f} - \\bar{o}}$$"
        },
        {
            "introduction": "Forecast skill is not static; it inherently decays with increasing lead time due to the chaotic nature of the atmosphere. This exercise bridges the gap between abstract metrics and physical theory by modeling the evolution of the Root-Mean-Square Error ($RMSE$) and Anomaly Correlation Coefficient ($ACC$) under exponential error growth . By deriving how these metrics change as a function of lead time, you will develop a quantitative intuition for the finite limits of predictability.",
            "id": "4030858",
            "problem": "Consider a deterministic forecast of a single large-scale atmospheric anomaly, represented as a scalar random variable $f(t)$ at lead time $t$, intended to predict the verifying anomaly $s(t)$ of the atmosphere. Assume a chaotic regime in which infinitesimal perturbations grow exponentially with an $e$-folding time $\\tau_{e}$, so that for sufficiently small initial errors the forecast error amplitude grows proportionally to $\\exp(t/\\tau_e)$. Let the forecast error be $e(t) = f(t) - s(t)$, and assume the following physically standard properties over the scales of interest: the error is unbiased, $\\mathbb{E}[e(t)] = 0$, the error is uncorrelated with the true anomaly, $\\operatorname{cov}(e(t), s(t)) = 0$, the variance of the true anomaly is stationary, $\\operatorname{var}(s(t)) = \\sigma_{s}^{2}$, and the initial error variance is $\\operatorname{var}(e(0)) = \\sigma_{e0}^{2}$. Under exponential growth of small errors, assume $\\operatorname{var}(e(t)) = \\sigma_{e0}^{2} \\exp\\!\\big(2 t/\\tau_{e}\\big)$.\n\nDefine the root-mean-square error (RMSE) as $\\mathrm{RMSE}(t) = \\sqrt{\\mathbb{E}\\big[(f(t) - s(t))^{2}\\big]}$ and the anomaly correlation coefficient (ACC) as $\\mathrm{ACC}(t) = \\dfrac{\\operatorname{cov}\\big(f(t), s(t)\\big)}{\\sqrt{\\operatorname{var}\\big(f(t)\\big)\\,\\operatorname{var}\\big(s(t)\\big)}}$, with anomalies taken relative to climatology (assume zero climatological mean so anomalies equal the raw values). Starting from these definitions and the exponential error growth premise, derive how increasing lead time $t$ reduces $\\mathrm{ACC}(t)$ and increases $\\mathrm{RMSE}(t)$, expressing both in closed form in terms of $\\sigma_{s}$, $\\sigma_{e0}$, and $\\tau_{e}$.\n\nThen, evaluate the following scenario: let $\\tau_{e} = 2$ days, $\\sigma_{s} = 60$ meters, and $\\sigma_{e0} = 6$ meters. Compute the value of $\\mathrm{RMSE}(t)$ at the lead time $t$ at which $\\mathrm{ACC}(t)$ declines to $0.8$. Round your numerical answer to four significant figures. Express the $\\mathrm{RMSE}$ in meters.",
            "solution": "The problem is well-posed and scientifically grounded in the principles of atmospheric predictability and forecast verification. All necessary data and definitions are provided, and there are no contradictions.\n\nThe givens are:\n- Forecast anomaly: $f(t)$\n- Verifying anomaly: $s(t)$\n- Forecast error: $e(t) = f(t) - s(t)$\n- $e$-folding time for error growth: $\\tau_{e}$\n- Unbiased error: $\\mathbb{E}[e(t)] = 0$\n- Error is uncorrelated with the true anomaly: $\\operatorname{cov}(e(t), s(t)) = 0$\n- Stationary variance of the true anomaly: $\\operatorname{var}(s(t)) = \\sigma_{s}^{2}$\n- Initial error variance: $\\operatorname{var}(e(0)) = \\sigma_{e0}^{2}$\n- Exponential error variance growth: $\\operatorname{var}(e(t)) = \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})$\n- Root-mean-square error definition: $\\mathrm{RMSE}(t) = \\sqrt{\\mathbb{E}\\big[(f(t) - s(t))^{2}\\big]}$\n- Anomaly correlation coefficient definition: $\\mathrm{ACC}(t) = \\dfrac{\\operatorname{cov}\\big(f(t), s(t)\\big)}{\\sqrt{\\operatorname{var}\\big(f(t)\\big)\\,\\operatorname{var}\\big(s(t)\\big)}}$\n\nFirst, we derive the closed-form expression for the root-mean-square error, $\\mathrm{RMSE}(t)$.\nBy definition, $\\mathrm{RMSE}(t) = \\sqrt{\\mathbb{E}\\big[(f(t) - s(t))^{2}\\big]}$.\nThe term inside the expectation is the squared forecast error, $e(t)^{2}$.\n$$\n\\mathrm{RMSE}(t) = \\sqrt{\\mathbb{E}\\big[e(t)^{2}\\big]}\n$$\nThe variance of the error is defined as $\\operatorname{var}(e(t)) = \\mathbb{E}[e(t)^2] - (\\mathbb{E}[e(t)])^2$.\nWe are given that the error is unbiased, meaning $\\mathbb{E}[e(t)] = 0$.\nTherefore, the variance of the error simplifies to $\\operatorname{var}(e(t)) = \\mathbb{E}[e(t)^2]$.\nThis allows us to express the $\\mathrm{RMSE}$ in terms of the error variance:\n$$\n\\mathrm{RMSE}(t) = \\sqrt{\\operatorname{var}(e(t))}\n$$\nThe problem states that the error variance grows exponentially: $\\operatorname{var}(e(t)) = \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})$. Substituting this into the expression for $\\mathrm{RMSE}(t)$:\n$$\n\\mathrm{RMSE}(t) = \\sqrt{\\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})} = \\sigma_{e0} \\exp(t/\\tau_{e})\n$$\nThis is the closed-form expression for $\\mathrm{RMSE}(t)$. It shows that the $\\mathrm{RMSE}$ grows exponentially with lead time $t$.\n\nNext, we derive the closed-form expression for the anomaly correlation coefficient, $\\mathrm{ACC}(t)$.\nThe definition is $\\mathrm{ACC}(t) = \\dfrac{\\operatorname{cov}\\big(f(t), s(t)\\big)}{\\sqrt{\\operatorname{var}\\big(f(t)\\big)\\,\\operatorname{var}\\big(s(t)\\big)}}$.\nWe need to find expressions for the three terms in this formula.\nThe variance of the true anomaly is given as $\\operatorname{var}(s(t)) = \\sigma_{s}^{2}$.\n\nLet's evaluate the covariance in the numerator, $\\operatorname{cov}(f(t), s(t))$. Using $f(t) = s(t) + e(t)$:\n$$\n\\operatorname{cov}(f(t), s(t)) = \\operatorname{cov}(s(t) + e(t), s(t))\n$$\nUsing the linearity of the covariance operator:\n$$\n\\operatorname{cov}(s(t) + e(t), s(t)) = \\operatorname{cov}(s(t), s(t)) + \\operatorname{cov}(e(t), s(t))\n$$\nWe know that $\\operatorname{cov}(s(t), s(t)) = \\operatorname{var}(s(t)) = \\sigma_{s}^{2}$. We are also given that the error is uncorrelated with the true anomaly, so $\\operatorname{cov}(e(t), s(t)) = 0$.\nThus, the numerator is:\n$$\n\\operatorname{cov}(f(t), s(t)) = \\sigma_{s}^{2}\n$$\n\nNow, let's evaluate the variance of the forecast in the denominator, $\\operatorname{var}(f(t))$. Using $f(t) = s(t) + e(t)$:\n$$\n\\operatorname{var}(f(t)) = \\operatorname{var}(s(t) + e(t))\n$$\nThe variance of a sum of two random variables is given by $\\operatorname{var}(X+Y) = \\operatorname{var}(X) + \\operatorname{var}(Y) + 2\\operatorname{cov}(X,Y)$.\n$$\n\\operatorname{var}(s(t) + e(t)) = \\operatorname{var}(s(t)) + \\operatorname{var}(e(t)) + 2\\operatorname{cov}(s(t), e(t))\n$$\nSubstituting the known quantities: $\\operatorname{var}(s(t)) = \\sigma_{s}^{2}$, $\\operatorname{var}(e(t)) = \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})$, and $\\operatorname{cov}(s(t), e(t)) = 0$.\n$$\n\\operatorname{var}(f(t)) = \\sigma_{s}^{2} + \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})\n$$\n\nNow we assemble the complete expression for $\\mathrm{ACC}(t)$:\n$$\n\\mathrm{ACC}(t) = \\frac{\\sigma_{s}^{2}}{\\sqrt{\\left(\\sigma_{s}^{2} + \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})\\right) \\sigma_{s}^{2}}}\n$$\n$$\n\\mathrm{ACC}(t) = \\frac{\\sigma_{s}^{2}}{\\sigma_{s} \\sqrt{\\sigma_{s}^{2} + \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})}} = \\frac{\\sigma_{s}}{\\sqrt{\\sigma_{s}^{2} + \\sigma_{e0}^{2} \\exp(2 t/\\tau_{e})}}\n$$\nThis expression can be further simplified by dividing the numerator and the term inside the square root by $\\sigma_{s}$:\n$$\n\\mathrm{ACC}(t) = \\frac{1}{\\sqrt{1 + \\frac{\\sigma_{e0}^{2}}{\\sigma_{s}^{2}} \\exp(2 t/\\tau_{e})}}\n$$\nThis is the closed-form expression for $\\mathrm{ACC}(t)$. It shows that $\\mathrm{ACC}(t)$ starts at $t=0$ from a value less than $1$ (due to initial error) and decreases towards $0$ as the lead time $t$ increases.\n\nFinally, we evaluate the scenario with the given parameters: $\\tau_{e} = 2$ days, $\\sigma_{s} = 60$ meters, and $\\sigma_{e0} = 6$ meters. We need to find the value of $\\mathrm{RMSE}(t)$ at the lead time $t$ where $\\mathrm{ACC}(t) = 0.8$.\n\nFirst, we solve for the lead time $t$ using the expression for $\\mathrm{ACC}(t)$:\n$$\n0.8 = \\frac{1}{\\sqrt{1 + \\left(\\frac{6}{60}\\right)^{2} \\exp\\left(\\frac{2t}{2}\\right)}}\n$$\n$$\n0.8 = \\frac{1}{\\sqrt{1 + (0.1)^{2} \\exp(t)}} = \\frac{1}{\\sqrt{1 + 0.01 \\exp(t)}}\n$$\nTo solve for $t$, we rearrange the equation:\n$$\n\\sqrt{1 + 0.01 \\exp(t)} = \\frac{1}{0.8} = 1.25\n$$\nSquaring both sides:\n$$\n1 + 0.01 \\exp(t) = (1.25)^{2} = 1.5625\n$$\n$$\n0.01 \\exp(t) = 1.5625 - 1 = 0.5625\n$$\n$$\n\\exp(t) = \\frac{0.5625}{0.01} = 56.25\n$$\nThis gives the value of $\\exp(t)$ at the specified lead time. We do not need to solve for $t$ explicitly.\n\nNow, we compute $\\mathrm{RMSE}(t)$ at this lead time using its derived formula:\n$$\n\\mathrm{RMSE}(t) = \\sigma_{e0} \\exp(t/\\tau_{e}) = 6 \\exp(t/2)\n$$\nWe can write $\\exp(t/2)$ as $\\sqrt{\\exp(t)}$. Since we found $\\exp(t) = 56.25$:\n$$\n\\exp(t/2) = \\sqrt{56.25} = 7.5\n$$\nSubstituting this back into the $\\mathrm{RMSE}(t)$ expression:\n$$\n\\mathrm{RMSE}(t) = 6 \\times 7.5 = 45\n$$\nThe value of the $\\mathrm{RMSE}$ at this lead time is exactly $45$ meters. The problem asks for the answer to be rounded to four significant figures, which is $45.00$.",
            "answer": "$$\n\\boxed{45.00}\n$$"
        },
        {
            "introduction": "The choice between verification metrics like the Root-Mean-Square Error ($RMSE$) and the Mean Absolute Error ($MAE$) is not arbitrary; it reflects a judgment about which aspects of forecast error are most important. This thought experiment guides you to quantify how these two metrics respond to an extreme outlier, revealing the mathematical reason for the $RMSE$'s heightened sensitivity to large forecast busts . This analysis is crucial for interpreting verification scores correctly and understanding the implicit penalty function of each metric.",
            "id": "4030890",
            "problem": "Consider a deterministic forecast of daily $500$ hPa geopotential height over a rectangular regional grid of $20 \\times 20$ points, so the number of verification samples is $n = 400$. Let the deterministic forecast errors at these $n$ samples be $\\{e_i\\}_{i=1}^{n}$, where $e_i$ is the difference between the forecast and the verifying analysis at sample $i$. The Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE) are defined by the fundamental aggregation definitions\n$$\\text{RMSE} = \\left(\\frac{1}{n}\\sum_{i=1}^{n} e_i^2\\right)^{1/2}, \\quad \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |e_i|.$$\nSuppose an extreme outlier is introduced by a single-sample perturbation at index $k$, replacing $e_k$ by $e_k + D$, where $D$ is a scalar perturbation whose magnitude is very large compared to all existing errors, i.e., $|D| \\gg \\max_i |e_i|$. Denote the perturbed metrics by $\\text{RMSE}'$ and $\\text{MAE}'$ computed from the modified set $\\{e_1,\\dots,e_k + D,\\dots,e_n\\}$.\n\nStarting from the above definitions and without appealing to any pre-derived sensitivity formulas, derive the leading-order asymptotic expressions, as $|D| \\to \\infty$, for the changes $\\Delta_{\\text{RMSE}} = \\text{RMSE}' - \\text{RMSE}$ and $\\Delta_{\\text{MAE}} = \\text{MAE}' - \\text{MAE}}$ produced by this single-sample perturbation. Then, using those asymptotic expressions, determine the asymptotic ratio\n$$\\rho = \\lim_{|D| \\to \\infty} \\frac{\\Delta_{\\text{RMSE}}}{\\Delta_{\\text{MAE}}}$$\nin terms of $n$. Finally, evaluate this ratio for $n = 400$. Provide the final answer as a single number. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and self-contained. It is a valid problem of applied mathematics in the context of numerical weather prediction verification.\n\nLet the initial set of forecast errors be $\\{e_i\\}_{i=1}^{n}$. The number of samples is given as $n = 20 \\times 20 = 400$. The Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) are defined as:\n$$ \\text{RMSE} = \\left(\\frac{1}{n}\\sum_{i=1}^{n} e_i^2\\right)^{1/2} $$\n$$ \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |e_i| $$\n\nA single-sample perturbation is introduced at an index $k$, such that the new error is $e_k' = e_k + D$. All other errors remain unchanged, $e_i' = e_i$ for $i \\neq k$. The magnitude of the perturbation is large, i.e., $|D| \\gg \\max_i |e_i|$. The perturbed metrics are denoted $\\text{RMSE}'$ and $\\text{MAE}'$. We need to find the leading-order asymptotic expressions for the changes $\\Delta_{\\text{RMSE}} = \\text{RMSE}' - \\text{RMSE}$ and $\\Delta_{\\text{MAE}} = \\text{MAE}' - \\text{MAE}}$ as $|D| \\to \\infty$.\n\nFirst, we analyze the change in RMSE, $\\Delta_{\\text{RMSE}}$.\nThe original sum of squares is $S_2 = \\sum_{i=1}^{n} e_i^2 = n (\\text{RMSE})^2$.\nThe new sum of squares, $S_2'$, is:\n$$ S_2' = \\sum_{i \\neq k} e_i^2 + (e_k+D)^2 = \\left(\\sum_{i=1}^{n} e_i^2 - e_k^2\\right) + (e_k^2 + 2e_k D + D^2) $$\n$$ S_2' = \\sum_{i=1}^{n} e_i^2 + 2e_k D + D^2 = S_2 + 2e_k D + D^2 $$\nThe perturbed RMSE is:\n$$ \\text{RMSE}' = \\left(\\frac{S_2'}{n}\\right)^{1/2} = \\left(\\frac{S_2 + 2e_k D + D^2}{n}\\right)^{1/2} $$\nTo find the asymptotic behavior for large $|D|$, we factor out $D^2$:\n$$ \\text{RMSE}' = \\left(\\frac{D^2}{n} \\left(1 + \\frac{2e_k}{D} + \\frac{S_2}{D^2}\\right)\\right)^{1/2} = \\frac{|D|}{\\sqrt{n}} \\left(1 + \\frac{2e_k}{D} + \\frac{S_2}{D^2}\\right)^{1/2} $$\nWe use the binomial expansion $(1+x)^{1/2} \\approx 1 + \\frac{1}{2}x$ for small $x$. Here, $x = \\frac{2e_k}{D} + \\frac{S_2}{D^2}$, which approaches $0$ as $|D| \\to \\infty$.\n$$ \\text{RMSE}' \\approx \\frac{|D|}{\\sqrt{n}} \\left(1 + \\frac{1}{2} \\left(\\frac{2e_k}{D} + \\frac{S_2}{D^2}\\right)\\right) = \\frac{|D|}{\\sqrt{n}} \\left(1 + \\frac{e_k}{D} + O\\left(\\frac{1}{D^2}\\right)\\right) $$\n$$ \\text{RMSE}' \\approx \\frac{|D|}{\\sqrt{n}} + \\frac{e_k |D|}{D\\sqrt{n}} = \\frac{|D|}{\\sqrt{n}} + \\frac{e_k \\text{sgn}(D)}{\\sqrt{n}} $$\nThe change in RMSE is $\\Delta_{\\text{RMSE}} = \\text{RMSE}' - \\text{RMSE}$. Asymptotically,\n$$ \\Delta_{\\text{RMSE}} \\approx \\frac{|D|}{\\sqrt{n}} + \\frac{e_k \\text{sgn}(D)}{\\sqrt{n}} - \\text{RMSE} $$\nThe leading-order term is the one that grows with $|D|$. Thus, the leading-order asymptotic expression for the change is:\n$$ \\Delta_{\\text{RMSE}} \\sim \\frac{|D|}{\\sqrt{n}} \\quad \\text{as } |D| \\to \\infty $$\n\nNext, we analyze the change in MAE, $\\Delta_{\\text{MAE}}$.\nThe original sum of absolute values is $S_1 = \\sum_{i=1}^{n} |e_i| = n \\cdot \\text{MAE}$.\nThe new sum of absolute values, $S_1'$, is:\n$$ S_1' = \\sum_{i \\neq k} |e_i| + |e_k+D| = \\left(\\sum_{i=1}^{n} |e_i| - |e_k|\\right) + |e_k+D| = S_1 - |e_k| + |e_k+D| $$\nThe perturbed MAE is:\n$$ \\text{MAE}' = \\frac{S_1'}{n} = \\frac{S_1 - |e_k| + |e_k+D|}{n} $$\nThe change in MAE is:\n$$ \\Delta_{\\text{MAE}} = \\text{MAE}' - \\text{MAE} = \\frac{S_1 - |e_k| + |e_k+D|}{n} - \\frac{S_1}{n} = \\frac{|e_k+D| - |e_k|}{n} $$\nFor the asymptotic behavior as $|D| \\to \\infty$, we analyze the term $|e_k+D|$. Since $|D| \\gg |e_k|$, the sign of $e_k+D$ is the same as the sign of $D$. Thus, $\\text{sgn}(e_k+D) = \\text{sgn}(D)$.\n$$ |e_k+D| = (e_k+D)\\text{sgn}(e_k+D) = (e_k+D)\\text{sgn}(D) = e_k\\text{sgn}(D) + D\\text{sgn}(D) = e_k\\text{sgn}(D) + |D| $$\nSubstituting this into the expression for $\\Delta_{\\text{MAE}}$:\n$$ \\Delta_{\\text{MAE}} = \\frac{1}{n} \\left( |D| + e_k\\text{sgn}(D) - |e_k| \\right) $$\nThe leading-order term is the one that grows with $|D|$. Thus, the leading-order asymptotic expression for the change is:\n$$ \\Delta_{\\text{MAE}} \\sim \\frac{|D|}{n} \\quad \\text{as } |D| \\to \\infty $$\n\nNow, we determine the asymptotic ratio $\\rho$.\n$$ \\rho = \\lim_{|D| \\to \\infty} \\frac{\\Delta_{\\text{RMSE}}}{\\Delta_{\\text{MAE}}} $$\nUsing the leading-order asymptotic expressions derived above:\n$$ \\rho = \\lim_{|D| \\to \\infty} \\frac{|D|/\\sqrt{n}}{|D|/n} = \\frac{1/\\sqrt{n}}{1/n} = \\frac{n}{\\sqrt{n}} = \\sqrt{n} $$\nThis result can be confirmed by taking the limit of the full expressions for the changes:\n$$ \\rho = \\lim_{|D| \\to \\infty} \\frac{\\left(\\frac{S_2 + 2e_k D + D^2}{n}\\right)^{1/2} - \\text{RMSE}}{\\frac{|e_k+D| - |e_k|}{n}} $$\nDivide the numerator and denominator by $|D|$:\n$$ \\rho = \\lim_{|D| \\to \\infty} \\frac{\\frac{1}{|D|}\\left(\\frac{|D|}{\\sqrt{n}}\\left(1 + \\frac{2e_k}{D} + \\frac{S_2}{D^2}\\right)^{1/2} - \\text{RMSE}\\right)}{\\frac{1}{|D|}\\left(\\frac{|D| + e_k\\text{sgn}(D) - |e_k|}{n}\\right)} $$\n$$ \\rho = \\lim_{|D| \\to \\infty} \\frac{\\frac{1}{\\sqrt{n}}\\left(1 + \\frac{2e_k}{D} + \\frac{S_2}{D^2}\\right)^{1/2} - \\frac{\\text{RMSE}}{|D|}}{\\frac{1}{n}\\left(1 + \\frac{e_k\\text{sgn}(D)}{|D|} - \\frac{|e_k|}{|D|}\\right)} $$\nAs $|D| \\to \\infty$, terms like $1/D$ and $1/|D|$ go to $0$.\n$$ \\rho = \\frac{\\frac{1}{\\sqrt{n}}(1)^{1/2} - 0}{\\frac{1}{n}(1 + 0 - 0)} = \\frac{1/\\sqrt{n}}{1/n} = \\sqrt{n} $$\nThe problem specifies that the number of verification samples is $n = 400$.\nSubstituting this value into the expression for $\\rho$:\n$$ \\rho = \\sqrt{400} = 20 $$",
            "answer": "$$\\boxed{20}$$"
        }
    ]
}