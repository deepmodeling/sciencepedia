{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of a multi-model ensemble is to produce a single, improved forecast. This practice moves beyond a simple average by deriving the optimal linear weights to combine model outputs, a technique central to statistical forecasting. By minimizing the mean squared error subject to an unbiasedness constraint, you will engage with the core principles of how to account for both individual model error variance and the covariance between model errors .",
            "id": "3895017",
            "problem": "In a model intercomparison study of a Multi-Model Ensemble (MME), $3$ independent modeling centers provide annual mean predictions $\\{Y_{k}\\}_{k=1}^{3}$ for the same geophysical quantity $Y$ (for example, a global-mean surface temperature anomaly). Assume each model is individually unbiased for $Y$, in the sense that the model error $e_{k} \\equiv Y_{k}-Y$ satisfies $\\mathbb{E}[e_{k}] = 0$ for each $k$, and let the model error vector be $e \\equiv (e_{1},e_{2},e_{3})^\\top$ with covariance matrix $\\Sigma \\equiv \\mathrm{Cov}(e)$ known from a long, quasi-stationary verification dataset. You wish to construct a linear ensemble estimator $\\hat{Y} \\equiv \\sum_{k=1}^{3} w_{k} Y_{k}$ with weights $w \\equiv (w_{1},w_{2},w_{3})^\\top$ that minimizes the expected squared error $\\mathbb{E}\\!\\left[\\left(\\sum_{k=1}^{3} w_{k} Y_{k} - Y\\right)^{2}\\right]$, subject to the constraint $\\sum_{k=1}^{3} w_{k} = 1$.\n\nStarting only from the definitions of expectation, variance, and covariance, and using the method of Lagrange multipliers from multivariate calculus, derive the constrained minimizer $w$ in symbolic form. Then, for the scientifically plausible error covariance matrix\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 0 \\\\\n1 & 0 & 2\n\\end{pmatrix},\n$$\ncompute the exact ensemble weights $w$. Express the final weight vector using exact rational numbers. No rounding is required. The weights are dimensionless.",
            "solution": "The user wants to find the optimal weights for a linear ensemble estimator by minimizing the expected squared error under a normalization constraint.\n\n### Step 1: Problem Validation\nThe problem statement is analyzed against the specified criteria.\n\n**Extracted Givens:**\n- A set of $3$ model predictions $\\{Y_k\\}_{k=1}^3$ for a true quantity $Y$.\n- Model error definition: $e_k \\equiv Y_k - Y$.\n- Unbiased model assumption: $\\mathbb{E}[e_k] = 0$ for $k \\in \\{1, 2, 3\\}$.\n- Error vector: $e = (e_1, e_2, e_3)^\\top$.\n- Error covariance matrix: $\\Sigma \\equiv \\mathrm{Cov}(e)$.\n- Linear ensemble estimator: $\\hat{Y} \\equiv \\sum_{k=1}^3 w_k Y_k$.\n- Weight vector: $w = (w_1, w_2, w_3)^\\top$.\n- Objective function to minimize: $J(w) = \\mathbb{E}[(\\hat{Y} - Y)^2]$.\n- Constraint: $\\sum_{k=1}^3 w_k = 1$.\n- Specific covariance matrix:\n$$\n\\Sigma =\n\\begin{pmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 0 \\\\\n1 & 0 & 2\n\\end{pmatrix}\n$$\n- The task is to first derive the symbolic expression for the optimal weights $w$ using the method of Lagrange multipliers and then compute their exact numerical values for the given $\\Sigma$.\n\n**Validation Using Extracted Givens:**\n1.  **Scientifically Grounded:** The problem is a standard, fundamental task in statistical forecasting and data assimilation, particularly in climate science and meteorology. The minimization of the mean squared error (MSE) for a weighted ensemble is a well-established method for improving forecast skill. The given covariance matrix $\\Sigma$ is symmetric. To be a valid covariance matrix, it must also be positive semi-definite. We test for positive definiteness using Sylvester's criterion on the leading principal minors:\n    - $\\det(M_1) = 4 > 0$.\n    - $\\det(M_2) = \\det\\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} = (4)(3) - (1)(1) = 11 > 0$.\n    - $\\det(M_3) = \\det(\\Sigma) = 4(3 \\cdot 2 - 0 \\cdot 0) - 1(1 \\cdot 2 - 0 \\cdot 1) + 1(1 \\cdot 0 - 3 \\cdot 1) = 24 - 2 - 3 = 19 > 0$.\n    Since all leading principal minors are positive, the matrix $\\Sigma$ is positive definite, which is a valid property for a covariance matrix of non-degenerate random variables. The problem is scientifically sound.\n2.  **Well-Posed:** The problem is well-posed. It involves minimizing a convex quadratic function ($w^\\top \\Sigma w$, with $\\Sigma$ being positive definite) subject to a linear constraint. This structure guarantees the existence of a unique minimum.\n3.  **Objective:** The problem is stated using precise, unambiguous mathematical language.\n4.  **Complete and Consistent:** The problem provides all necessary information (objective function, constraint, definitions, specific matrix) and contains no internal contradictions.\n\n**Verdict:** The problem is valid.\n\n### Step 2: Symbolic Derivation of Optimal Weights\n\nFirst, we express the objective function, the expected squared error $J(w)$, in terms of the weight vector $w$ and the covariance matrix $\\Sigma$.\nThe error of the ensemble estimator is $\\hat{Y} - Y$.\n$$\n\\hat{Y} - Y = \\sum_{k=1}^{3} w_k Y_k - Y\n$$\nUsing the constraint $\\sum_{k=1}^{3} w_k = 1$, we can write $Y$ as $Y \\left(\\sum_{k=1}^{3} w_k\\right) = \\sum_{k=1}^{3} w_k Y$.\nSubstituting this into the expression for the ensemble error:\n$$\n\\hat{Y} - Y = \\sum_{k=1}^{3} w_k Y_k - \\sum_{k=1}^{3} w_k Y = \\sum_{k=1}^{3} w_k (Y_k - Y) = \\sum_{k=1}^{3} w_k e_k\n$$\nIn vector notation, this is $w^\\top e$, where $w = (w_1, w_2, w_3)^\\top$ and $e = (e_1, e_2, e_3)^\\top$.\n\nThe objective function to minimize is the expected squared error, which is the variance of the ensemble error since the expected error is zero:\n$$\nJ(w) = \\mathbb{E}[(\\hat{Y} - Y)^2] = \\mathbb{E}[(w^\\top e)^2]\n$$\nSince $w^\\top e$ is a scalar, its square is $(w^\\top e)(w^\\top e) = (w^\\top e)(e^\\top w) = w^\\top e e^\\top w$.\nThe weights $w$ are deterministic, so we can move them outside the expectation:\n$$\nJ(w) = w^\\top \\mathbb{E}[e e^\\top] w\n$$\nThe covariance matrix of the error vector $e$ is defined as $\\Sigma = \\mathrm{Cov}(e) = \\mathbb{E}[(e - \\mathbb{E}[e])(e - \\mathbb{E}[e])^\\top]$. Given that the models are unbiased, $\\mathbb{E}[e_k] = 0$ for all $k$, which implies $\\mathbb{E}[e] = \\mathbf{0}$. Therefore, the covariance matrix simplifies to $\\Sigma = \\mathbb{E}[e e^\\top]$.\nThus, the objective function is a quadratic form in $w$:\n$$\nJ(w) = w^\\top \\Sigma w\n$$\nWe need to minimize $J(w)$ subject to the constraint $\\sum_{k=1}^3 w_k = 1$. In vector form, the constraint is $\\mathbf{1}^\\top w = 1$, where $\\mathbf{1}$ is a column vector of ones, $\\mathbf{1} = (1, 1, 1)^\\top$.\n\nWe use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(w, \\lambda) = J(w) - \\lambda (\\mathbf{1}^\\top w - 1) = w^\\top \\Sigma w - \\lambda (\\mathbf{1}^\\top w - 1)\n$$\nTo find the minimum, we set the gradient of $\\mathcal{L}$ with respect to $w$ to zero:\n$$\n\\nabla_w \\mathcal{L} = \\nabla_w (w^\\top \\Sigma w) - \\nabla_w (\\lambda \\mathbf{1}^\\top w) = \\mathbf{0}\n$$\nUsing standard matrix calculus identities, and noting that $\\Sigma$ is symmetric, $\\nabla_w (w^\\top \\Sigma w) = 2 \\Sigma w$. The derivative $\\nabla_w (\\lambda \\mathbf{1}^\\top w) = \\lambda \\mathbf{1}$.\nThe condition becomes:\n$$\n2 \\Sigma w - \\lambda \\mathbf{1} = \\mathbf{0} \\quad \\implies \\quad \\Sigma w = \\frac{\\lambda}{2} \\mathbf{1}\n$$\nSince $\\Sigma$ is positive definite, it is invertible. We can solve for $w$:\n$$\nw = \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\n$$\nTo find the Lagrange multiplier $\\lambda$, we apply the constraint $\\mathbf{1}^\\top w = 1$:\n$$\n\\mathbf{1}^\\top \\left( \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1} \\right) = 1 \\quad \\implies \\quad \\frac{\\lambda}{2} (\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}) = 1\n$$\nSolving for the term $\\frac{\\lambda}{2}$:\n$$\n\\frac{\\lambda}{2} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\n$$\nSubstituting this back into the expression for $w$ gives the symbolic solution for the optimal weights:\n$$\nw = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\n$$\n\n### Step 3: Numerical Calculation for the Given Covariance Matrix\n\nWe are given the covariance matrix:\n$$\n\\Sigma =\n\\begin{pmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 0 \\\\\n1 & 0 & 2\n\\end{pmatrix}\n$$\nFirst, we compute the inverse of $\\Sigma$. The determinant was calculated during validation as $\\det(\\Sigma) = 19$. The inverse is $\\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)} \\text{adj}(\\Sigma)$. The adjugate matrix, $\\text{adj}(\\Sigma)$, is the transpose of the cofactor matrix.\n\nThe cofactors are:\n$C_{11} = (3)(2) - (0)(0) = 6$\n$C_{12} = -((1)(2) - (0)(1)) = -2$\n$C_{13} = (1)(0) - (3)(1) = -3$\n$C_{21} = -((1)(2) - (1)(0)) = -2$\n$C_{22} = (4)(2) - (1)(1) = 7$\n$C_{23} = -((4)(0) - (1)(1)) = 1$\n$C_{31} = (1)(0) - (1)(3) = -3$\n$C_{32} = -((4)(0) - (1)(1)) = 1$\n$C_{33} = (4)(3) - (1)(1) = 11$\n\nThe cofactor matrix is $\\text{C} = \\begin{pmatrix} 6 & -2 & -3 \\\\ -2 & 7 & 1 \\\\ -3 & 1 & 11 \\end{pmatrix}$. Since $\\Sigma$ is symmetric, the cofactor matrix is also symmetric, so $\\text{adj}(\\Sigma) = C^\\top = C$.\nThe inverse matrix is:\n$$\n\\Sigma^{-1} = \\frac{1}{19}\n\\begin{pmatrix}\n6 & -2 & -3 \\\\\n-2 & 7 & 1 \\\\\n-3 & 1 & 11\n\\end{pmatrix}\n$$\nNext, we calculate the numerator of the weight formula, $\\Sigma^{-1} \\mathbf{1}$:\n$$\n\\Sigma^{-1} \\mathbf{1} = \\frac{1}{19}\n\\begin{pmatrix}\n6 & -2 & -3 \\\\\n-2 & 7 & 1 \\\\\n-3 & 1 & 11\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\frac{1}{19}\n\\begin{pmatrix}\n6 - 2 - 3 \\\\\n-2 + 7 + 1 \\\\\n-3 + 1 + 11\n\\end{pmatrix}\n= \\frac{1}{19}\n\\begin{pmatrix}\n1 \\\\\n6 \\\\\n9\n\\end{pmatrix}\n$$\nNext, we calculate the denominator, $\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}$. This is the sum of the components of the vector we just calculated:\n$$\n\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1} = \\frac{1}{19} (1 + 6 + 9) = \\frac{16}{19}\n$$\nFinally, we compute the weight vector $w$:\n$$\nw = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}} = \\frac{\\frac{1}{19} \\begin{pmatrix} 1 \\\\ 6 \\\\ 9 \\end{pmatrix}}{\\frac{16}{19}} = \\frac{1}{16} \\begin{pmatrix} 1 \\\\ 6 \\\\ 9 \\end{pmatrix}\n$$\nSo the individual weights are:\n$$\nw_1 = \\frac{1}{16}\n$$\n$$\nw_2 = \\frac{6}{16} = \\frac{3}{8}\n$$\n$$\nw_3 = \\frac{9}{16}\n$$\nThe final weight vector is $w = (1/16, 3/8, 9/16)^\\top$. We can verify that the weights sum to $1$: $\\frac{1}{16} + \\frac{6}{16} + \\frac{9}{16} = \\frac{16}{16} = 1$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{16} \\\\ \\frac{3}{8} \\\\ \\frac{9}{16} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While large ensembles are common in climate science, the adage 'more is better' is not always true if the models are not independent. This exercise introduces the crucial concept of effective sample size, $N_{\\text{eff}}$, which quantifies the true informational content of a correlated ensemble. You will derive this metric from first principles and apply it to a realistic scenario, revealing how model inter-dependencies can dramatically reduce the benefits of a large ensemble .",
            "id": "3895076",
            "problem": "Consider a multi-model ensemble in the Coupled Model Intercomparison Project (CMIP), consisting of $N=6$ climate models grouped into two structural families of size $3$ each, based on shared dynamical cores and parameterization choices. Let $X_i$ denote the standardized annual-mean near-surface temperature anomaly for model $i$, where standardization ensures $\\operatorname{Var}(X_i)=1$ for all $i$. Assume the following scientifically motivated factor structure: for models in family $A$,\n$$\nX_i = \\sqrt{0.3}\\,C + \\sqrt{0.5}\\,G_A + \\sqrt{0.2}\\,\\varepsilon_i,\n$$\nand for models in family $B$,\n$$\nX_j = \\sqrt{0.3}\\,C + \\sqrt{0.5}\\,G_B + \\sqrt{0.2}\\,\\varepsilon_j,\n$$\nwhere $C$, $G_A$, $G_B$, and all $\\varepsilon_i$ are mutually independent, each with zero mean and unit variance, and families $A$ and $B$ are disjoint index sets of size $3$ each. This construction implies the correlation matrix $\\boldsymbol{R}=\\{\\rho_{ij}\\}$ across models,\n$$\n\\boldsymbol{R} = \\begin{pmatrix}\n1 & 0.8 & 0.8 & 0.3 & 0.3 & 0.3 \\\\\n0.8 & 1 & 0.8 & 0.3 & 0.3 & 0.3 \\\\\n0.8 & 0.8 & 1 & 0.3 & 0.3 & 0.3 \\\\\n0.3 & 0.3 & 0.3 & 1 & 0.8 & 0.8 \\\\\n0.3 & 0.3 & 0.3 & 0.8 & 1 & 0.8 \\\\\n0.3 & 0.3 & 0.3 & 0.8 & 0.8 & 1\n\\end{pmatrix}.\n$$\nStarting from first principles—namely, the definition of the variance of a mean of correlated random variables in terms of covariances and correlations—define the effective sample size $N_{\\text{eff}}$ as the unique number satisfying that the variance of the ensemble mean under correlation equals the variance of the mean of $N_{\\text{eff}}$ independent, unit-variance draws. Derive a closed-form analytic expression for $N_{\\text{eff}}$ in terms of $\\{\\rho_{ij}\\}$ and $N$, and then compute $N_{\\text{eff}}$ for the given $\\boldsymbol{R}$. Round your final numerical answer to four significant figures. Report a dimensionless number. In your reasoning, interpret the value of $N_{\\text{eff}}$ relative to the actual ensemble size $N$ in the context of model intercomparison studies and ensemble averaging.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and complete.\n\n### Step 1: Extract Givens\n- Ensemble size: $N=6$.\n- Number of families: $2$.\n- Size of each family (family $A$ and family $B$): $3$.\n- Model output variable: $X_i$, standardized such that its variance is $\\operatorname{Var}(X_i)=1$.\n- Factor model for family $A$ ($i \\in A$): $X_i = \\sqrt{0.3}\\,C + \\sqrt{0.5}\\,G_A + \\sqrt{0.2}\\,\\varepsilon_i$.\n- Factor model for family $B$ ($j \\in B$): $X_j = \\sqrt{0.3}\\,C + \\sqrt{0.5}\\,G_B + \\sqrt{0.2}\\,\\varepsilon_j$.\n- Latent factors $C, G_A, G_B, \\varepsilon_k$ are mutually independent, with zero mean and unit variance.\n- The correlation matrix $\\boldsymbol{R}=\\{\\rho_{ij}\\}$ is given as:\n$$\n\\boldsymbol{R} = \\begin{pmatrix}\n1 & 0.8 & 0.8 & 0.3 & 0.3 & 0.3 \\\\\n0.8 & 1 & 0.8 & 0.3 & 0.3 & 0.3 \\\\\n0.8 & 0.8 & 1 & 0.3 & 0.3 & 0.3 \\\\\n0.3 & 0.3 & 0.3 & 1 & 0.8 & 0.8 \\\\\n0.3 & 0.3 & 0.3 & 0.8 & 1 & 0.8 \\\\\n0.3 & 0.3 & 0.3 & 0.8 & 0.8 & 1\n\\end{pmatrix}\n$$\n- Definition of effective sample size $N_{\\text{eff}}$: The variance of the ensemble mean of the $N$ correlated variables is equated to the variance of the mean of $N_{\\text{eff}}$ hypothetical independent, unit-variance variables.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, using a standard factor model to represent dependencies in a multi-model ensemble, a common practice in climate science. The concept of effective sample size is a well-established statistical tool for handling correlated data. The problem is well-posed, providing a clear definition and all necessary data. The mathematical consistency of the provided factor model and the correlation matrix is verified. For $i \\neq j$ both in family $A$, $\\rho_{ij} = \\operatorname{Cov}(X_i, X_j) = (\\sqrt{0.3})^2 \\operatorname{Var}(C) + (\\sqrt{0.5})^2 \\operatorname{Var}(G_A) = 0.3 + 0.5 = 0.8$. For $i \\in A$ and $j \\in B$, $\\rho_{ij} = \\operatorname{Cov}(X_i, X_j) = (\\sqrt{0.3})^2 \\operatorname{Var}(C) = 0.3$. The variance is $\\operatorname{Var}(X_i) = (\\sqrt{0.3})^2 + (\\sqrt{0.5})^2 + (\\sqrt{0.2})^2 = 0.3 + 0.5 + 0.2 = 1.0$. All values are consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation of the Effective Sample Size Formula\nLet $\\bar{X}$ be the ensemble mean of the $N$ model outputs $X_i$:\n$$\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i\n$$\nThe variance of the ensemble mean, $\\operatorname{Var}(\\bar{X})$, is given by:\n$$\n\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_i\\right) = \\frac{1}{N^2} \\operatorname{Var}\\left(\\sum_{i=1}^{N} X_i\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{N} X_i\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\operatorname{Cov}(X_i, X_j)\n$$\nSince each $X_i$ is standardized to have unit variance, $\\operatorname{Var}(X_i) = 1$. The covariance is related to the correlation $\\rho_{ij}$ by $\\operatorname{Cov}(X_i, X_j) = \\rho_{ij} \\sqrt{\\operatorname{Var}(X_i)\\operatorname{Var}(X_j)}$. Given unit variances, we have $\\operatorname{Cov}(X_i, X_j) = \\rho_{ij}$.\nSubstituting this into the expression for the variance of the mean:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho_{ij}\n$$\nThe problem defines the effective sample size $N_{\\text{eff}}$ by equating this variance to the variance of the mean of $N_{\\text{eff}}$ independent, unit-variance draws. For a sample of size $N_{\\text{eff}}$ of i.i.d. variables $Y_k$ with $\\operatorname{Var}(Y_k)=1$, the variance of their mean $\\bar{Y}$ is $\\operatorname{Var}(\\bar{Y}) = \\frac{1}{N_{\\text{eff}}}$.\nEquating the two expressions for the variance of the mean:\n$$\n\\frac{1}{N_{\\text{eff}}} = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho_{ij}\n$$\nSolving for $N_{\\text{eff}}$ yields the general closed-form expression:\n$$\nN_{\\text{eff}} = \\frac{N^2}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho_{ij}}\n$$\nThis expression relates the effective sample size to the nominal sample size $N$ and the sum of all elements in the correlation matrix $\\boldsymbol{R}$.\n\n### Calculation for the Given Ensemble\nWe are given $N=6$ and the correlation matrix $\\boldsymbol{R}$. The first step is to compute the sum of all its elements, $\\sum_{i=1}^{6} \\sum_{j=1}^{6} \\rho_{ij}$.\nThe matrix has a block structure. Let's sum the elements in each block.\nThe top-left $3 \\times 3$ block corresponding to family $A$ has $3$ diagonal elements equal to $1$ and $3 \\times 2 = 6$ off-diagonal elements equal to $0.8$. The sum of its elements is:\n$$\n\\text{Sum}_A = 3 \\times 1 + 6 \\times 0.8 = 3 + 4.8 = 7.8\n$$\nThe bottom-right $3 \\times 3$ block for family $B$ is identical in structure and sum:\n$$\n\\text{Sum}_B = 3 \\times 1 + 6 \\times 0.8 = 3 + 4.8 = 7.8\n$$\nThe top-right $3 \\times 3$ off-diagonal block represents the correlations between family $A$ and family $B$. All $3 \\times 3 = 9$ elements are equal to $0.3$. The sum of its elements is:\n$$\n\\text{Sum}_{AB} = 9 \\times 0.3 = 2.7\n$$\nThe bottom-left block is the transpose and has the same sum:\n$$\n\\text{Sum}_{BA} = 9 \\times 0.3 = 2.7\n$$\nThe total sum of all elements in $\\boldsymbol{R}$ is the sum of the sums of these four blocks:\n$$\n\\sum_{i=1}^{6} \\sum_{j=1}^{6} \\rho_{ij} = \\text{Sum}_A + \\text{Sum}_B + \\text{Sum}_{AB} + \\text{Sum}_{BA} = 7.8 + 7.8 + 2.7 + 2.7 = 15.6 + 5.4 = 21.0\n$$\nNow we can compute $N_{\\text{eff}}$ using the derived formula with $N=6$:\n$$\nN_{\\text{eff}} = \\frac{N^2}{\\sum_{i,j} \\rho_{ij}} = \\frac{6^2}{21.0} = \\frac{36}{21} = \\frac{12}{7}\n$$\nTo obtain the numerical value rounded to four significant figures:\n$$\nN_{\\text{eff}} = \\frac{12}{7} \\approx 1.7142857... \\approx 1.714\n$$\n\n### Interpretation\nThe actual size of the model ensemble is $N=6$. The calculated effective sample size is $N_{\\text{eff}} \\approx 1.714$. The fact that $N_{\\text{eff}} < N$ is a direct consequence of the positive correlations among the model outputs. The interpretation is that, in terms of reducing the variance of the ensemble mean, this ensemble of $6$ correlated models provides the same amount of information as a much smaller hypothetical ensemble of approximately $1.714$ independent models.\nThis substantial reduction from $6$ to $1.714$ highlights the strong redundancy within the ensemble, driven by the high intra-family correlations ($\\rho=0.8$) and the non-zero inter-family correlation ($\\rho=0.3$). In the context of model intercomparison studies like CMIP, this result demonstrates that simply increasing the number of models in an ensemble does not linearly increase the confidence in the ensemble mean if the models are not independent. Shared development history, code, and physical parameterizations lead to correlated errors and biases, which the concept of effective sample size quantifies. It underscores the importance of model diversity over mere model numerosity for robust ensemble forecasting and uncertainty quantification.",
            "answer": "$$\n\\boxed{1.714}\n$$"
        },
        {
            "introduction": "To truly advance modeling, we must move beyond quantifying total uncertainty to understanding its origins. This practice introduces the powerful technique of Analysis of Variance (ANOVA) to decompose prediction error into distinct sources, such as model structure, parameterization, and internal variability. By working through a nested random-effects model, you will learn how to attribute uncertainty within a complex modeling experiment, a key skill for designing and interpreting model intercomparison projects .",
            "id": "3895089",
            "problem": "In a model intercomparison study for annual mean near-surface air temperature bias, each structural model (distinct dynamical core and physics suite) is paired with multiple calibration parameter sets, and each parameter set is run as an ensemble over multiple initial conditions. Let $Y_{ijk}$ denote the bias (in kelvin) from structure (model) $i \\in \\{1,\\dots,a\\}$, parameter set $j \\in \\{1,\\dots,b\\}$ nested in structure $i$, and ensemble member $k \\in \\{1,\\dots,r\\}$. Assume the hierarchical linear mixed-effects model\n$$\nY_{ijk} \\;=\\; \\mu \\;+\\; S_{i} \\;+\\; P_{j(i)} \\;+\\; \\varepsilon_{k(ij)},\n$$\nwith independent random effects $S_{i} \\sim \\mathcal{N}(0,\\sigma_{\\text{struct}}^{2})$, $P_{j(i)} \\sim \\mathcal{N}(0,\\sigma_{\\text{param}}^{2})$, and $\\varepsilon_{k(ij)} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})$. The design is balanced with $a$ structures, each with $b$ parameter sets and $r$ ensemble members per parameter set. The Analysis of Variance (ANOVA; Analysis of Variance) decomposition is nested: structures, then parameters within structures, then residual (internal variability across initial conditions).\n\nStarting only from the model definition, the additivity and independence of the random effects, and the definition of ANOVA sums of squares and mean squares for balanced nested designs, derive expressions for the expectations of the mean squares at each level and use the method of moments to obtain unbiased estimators for $\\sigma_{\\text{struct}}^{2}$ and $\\sigma_{\\text{param}}^{2}$ in terms of observed mean squares.\n\nThen, for a balanced experiment with $a=4$, $b=3$, and $r=5$, the observed ANOVA sums of squares are:\n- Between-structures sum of squares $SS_{\\text{struct}} = 67.5$ (kelvin squared, K$^{2}$),\n- Between-parameters-within-structure sum of squares $SS_{\\text{param}(\\text{struct})} = 36.0$ (K$^{2}$),\n- Residual (within-parameter-set) sum of squares $SS_{\\varepsilon} = 96.0$ (K$^{2}$).\n\nCompute the unbiased estimates $\\hat{\\sigma}_{\\text{struct}}^{2}$ and $\\hat{\\sigma}_{\\text{param}}^{2}$ using your derived expressions. Round each estimate to three significant figures. Express your final estimates in kelvin squared (K$^{2}$). Report your final answer as the ordered pair $\\left(\\hat{\\sigma}_{\\text{struct}}^{2}, \\hat{\\sigma}_{\\text{param}}^{2}\\right)$.",
            "solution": "The problem is valid as it is scientifically grounded in standard statistical theory (linear mixed-effects models and ANOVA), is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language.\n\nThe problem requires a two-part solution: first, the derivation of unbiased estimators for variance components in a nested random-effects model, and second, the application of these estimators to a given dataset.\n\n**Part 1: Derivation of Estimators**\n\nThe specified hierarchical linear mixed-effects model is:\n$$\nY_{ijk} \\;=\\; \\mu \\;+\\; S_{i} \\;+\\; P_{j(i)} \\;+\\; \\varepsilon_{k(ij)}\n$$\nfor structure $i \\in \\{1,\\dots,a\\}$, parameter set $j \\in \\{1,\\dots,b\\}$ nested in structure $i$, and ensemble member $k \\in \\{1,\\dots,r\\}$. The terms are:\n- $\\mu$: a fixed overall mean.\n- $S_{i}$: a random effect for structure $i$, with $S_{i} \\sim \\mathcal{N}(0, \\sigma_{\\text{struct}}^{2})$.\n- $P_{j(i)}$: a random effect for parameter set $j$ within structure $i$, with $P_{j(i)} \\sim \\mathcal{N}(0, \\sigma_{\\text{param}}^{2})$.\n- $\\varepsilon_{k(ij)}$: a random residual error for member $k$ of parameter set $j$ within structure $i$, with $\\varepsilon_{k(ij)} \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$.\nAll random effects are assumed to be mutually independent. The design is balanced.\n\nWe define the means at different levels of aggregation:\n- Mean for parameter set $j$ within structure $i$: $\\bar{Y}_{ij\\bullet} = \\frac{1}{r} \\sum_{k=1}^{r} Y_{ijk}$\n- Mean for structure $i$: $\\bar{Y}_{i\\bullet\\bullet} = \\frac{1}{b} \\sum_{j=1}^{b} \\bar{Y}_{ij\\bullet} = \\frac{1}{br} \\sum_{j=1}^{b} \\sum_{k=1}^{r} Y_{ijk}$\n- Grand mean: $\\bar{Y}_{\\bullet\\bullet\\bullet} = \\frac{1}{a} \\sum_{i=1}^{a} \\bar{Y}_{i\\bullet\\bullet} = \\frac{1}{abr} \\sum_{i=1}^{a} \\sum_{j=1}^{b} \\sum_{k=1}^{r} Y_{ijk}$\n\nThe sums of squares (SS) for a balanced nested ANOVA are:\n- Between-structures: $SS_{\\text{struct}} = br \\sum_{i=1}^{a} (\\bar{Y}_{i\\bullet\\bullet} - \\bar{Y}_{\\bullet\\bullet\\bullet})^2$\n- Between-parameters-within-structures: $SS_{\\text{param}(\\text{struct})} = r \\sum_{i=1}^{a} \\sum_{j=1}^{b} (\\bar{Y}_{ij\\bullet} - \\bar{Y}_{i\\bullet\\bullet})^2$\n- Residual (within-parameter-sets): $SS_{\\varepsilon} = \\sum_{i=1}^{a} \\sum_{j=1}^{b} \\sum_{k=1}^{r} (Y_{ijk} - \\bar{Y}_{ij\\bullet})^2$\n\nThe corresponding degrees of freedom (df) are:\n- $df_{\\text{struct}} = a - 1$\n- $df_{\\text{param}(\\text{struct})} = a(b - 1)$\n- $df_{\\varepsilon} = ab(r - 1)$\n\nThe mean squares (MS) are the sums of squares divided by their degrees of freedom:\n- $MS_{\\text{struct}} = \\frac{SS_{\\text{struct}}}{a - 1}$\n- $MS_{\\text{param}(\\text{struct})} = \\frac{SS_{\\text{param}(\\text{struct})}}{a(b - 1)}$\n- $MS_{\\varepsilon} = \\frac{SS_{\\varepsilon}}{ab(r - 1)}$\n\nTo find the method-of-moments estimators, we first derive the expectation of each mean square (EMS). We use the property that for any random variable $X$ with mean $\\mu_X$ and variance $\\sigma_X^2$, $E[X^2] = \\text{Var}(X) + (E[X])^2 = \\sigma_X^2 + \\mu_X^2$. Since all our random effects have a mean of $0$, $E[S_i^2] = \\sigma_{\\text{struct}}^2$, $E[P_{j(i)}^2] = \\sigma_{\\text{param}}^2$, and $E[\\varepsilon_{k(ij)}^2] = \\sigma_{\\varepsilon}^2$.\n\n**1. Expectation of $MS_{\\varepsilon}$**\n$$\nE[MS_{\\varepsilon}] = E\\left[\\frac{SS_{\\varepsilon}}{ab(r - 1)}\\right] = \\frac{1}{ab(r - 1)} E\\left[\\sum_{i,j,k} (Y_{ijk} - \\bar{Y}_{ij\\bullet})^2\\right]\n$$\nSubstituting the model into the terms:\n$Y_{ijk} - \\bar{Y}_{ij\\bullet} = (\\mu + S_i + P_{j(i)} + \\varepsilon_{k(ij)}) - (\\mu + S_i + P_{j(i)} + \\bar{\\varepsilon}_{ij\\bullet}) = \\varepsilon_{k(ij)} - \\bar{\\varepsilon}_{ij\\bullet}$.\nThe sum $\\sum_{k=1}^{r} (\\varepsilon_{k(ij)} - \\bar{\\varepsilon}_{ij\\bullet})^2$ is $(r-1)$ times the sample variance of the $\\varepsilon_{k(ij)}$ for a fixed $(i,j)$. The expectation of the sample variance is the population variance $\\sigma_{\\varepsilon}^2$.\n$$\nE\\left[\\sum_{k=1}^{r} (\\varepsilon_{k(ij)} - \\bar{\\varepsilon}_{ij\\bullet})^2\\right] = (r-1)\\sigma_{\\varepsilon}^2\n$$\nSumming over all $i$ and $j$:\n$$\nE[SS_{\\varepsilon}] = \\sum_{i=1}^{a} \\sum_{j=1}^{b} (r-1)\\sigma_{\\varepsilon}^2 = ab(r-1)\\sigma_{\\varepsilon}^2\n$$\nTherefore,\n$$\nE[MS_{\\varepsilon}] = \\frac{ab(r-1)\\sigma_{\\varepsilon}^2}{ab(r-1)} = \\sigma_{\\varepsilon}^2\n$$\n\n**2. Expectation of $MS_{\\text{param}(\\text{struct})}$**\n$$\nE[MS_{\\text{param}(\\text{struct})}] = \\frac{1}{a(b-1)} E\\left[r \\sum_{i,j} (\\bar{Y}_{ij\\bullet} - \\bar{Y}_{i\\bullet\\bullet})^2\\right]\n$$\nThe difference in means is:\n$\\bar{Y}_{ij\\bullet} - \\bar{Y}_{i\\bullet\\bullet} = (P_{j(i)} - \\bar{P}_{i\\bullet}) + (\\bar{\\varepsilon}_{ij\\bullet} - \\bar{\\varepsilon}_{i\\bullet\\bullet})$, where $\\bar{P}_{i\\bullet} = \\frac{1}{b}\\sum_j P_{j(i)}$ and $\\bar{\\varepsilon}_{ij\\bullet}, \\bar{\\varepsilon}_{i\\bullet\\bullet}$ are the corresponding error means. Due to independence, the expectation of the cross-product is $0$.\n$$\nE[(\\bar{Y}_{ij\\bullet} - \\bar{Y}_{i\\bullet\\bullet})^2] = E[(P_{j(i)} - \\bar{P}_{i\\bullet})^2] + E[(\\bar{\\varepsilon}_{ij\\bullet} - \\bar{\\varepsilon}_{i\\bullet\\bullet})^2]\n$$\nThe sum $\\sum_{j=1}^{b} (P_{j(i)} - \\bar{P}_{i\\bullet})^2$ has expectation $(b-1)\\sigma_{\\text{param}}^2$.\nThe sum $\\sum_{j=1}^{b} (\\bar{\\varepsilon}_{ij\\bullet} - \\bar{\\varepsilon}_{i\\bullet\\bullet})^2$ has expectation $(b-1)\\text{Var}(\\bar{\\varepsilon}_{ij\\bullet}) = (b-1)\\frac{\\sigma_{\\varepsilon}^2}{r}$.\nSumming over $i$:\n$$\nE\\left[SS_{\\text{param}(\\text{struct})}\\right] = r \\sum_{i=1}^{a} \\left( (b-1)\\sigma_{\\text{param}}^2 + (b-1)\\frac{\\sigma_{\\varepsilon}^2}{r} \\right) = a(b-1)(r\\sigma_{\\text{param}}^2 + \\sigma_{\\varepsilon}^2)\n$$\nTherefore,\n$$\nE[MS_{\\text{param}(\\text{struct})}] = \\frac{a(b-1)(r\\sigma_{\\text{param}}^2 + \\sigma_{\\varepsilon}^2)}{a(b-1)} = r\\sigma_{\\text{param}}^2 + \\sigma_{\\varepsilon}^2\n$$\n\n**3. Expectation of $MS_{\\text{struct}}$**\n$$\nE[MS_{\\text{struct}}] = \\frac{1}{a-1} E\\left[br \\sum_{i} (\\bar{Y}_{i\\bullet\\bullet} - \\bar{Y}_{\\bullet\\bullet\\bullet})^2\\right]\n$$\nThe difference in means is:\n$\\bar{Y}_{i\\bullet\\bullet} - \\bar{Y}_{\\bullet\\bullet\\bullet} = (S_i - \\bar{S}_{\\bullet}) + (\\bar{P}_{i\\bullet} - \\bar{P}_{\\bullet\\bullet}) + (\\bar{\\varepsilon}_{i\\bullet\\bullet} - \\bar{\\varepsilon}_{\\bullet\\bullet\\bullet})$.\nAgain using independence, the expectation of the sum of squares is the sum of expectations of each squared term.\nThe sum $\\sum_{i=1}^{a} (S_i - \\bar{S}_{\\bullet})^2$ has expectation $(a-1)\\sigma_{\\text{struct}}^2$.\nThe sum $\\sum_{i=1}^{a} (\\bar{P}_{i\\bullet} - \\bar{P}_{\\bullet\\bullet})^2$ has expectation $(a-1)\\text{Var}(\\bar{P}_{i\\bullet}) = (a-1)\\frac{\\sigma_{\\text{param}}^2}{b}$.\nThe sum $\\sum_{i=1}^{a} (\\bar{\\varepsilon}_{i\\bullet\\bullet} - \\bar{\\varepsilon}_{\\bullet\\bullet\\bullet})^2$ has expectation $(a-1)\\text{Var}(\\bar{\\varepsilon}_{i\\bullet\\bullet}) = (a-1)\\frac{\\sigma_{\\varepsilon}^2}{br}$.\nSo,\n$$\nE[SS_{\\text{struct}}] = br \\left( (a-1)\\sigma_{\\text{struct}}^2 + (a-1)\\frac{\\sigma_{\\text{param}}^2}{b} + (a-1)\\frac{\\sigma_{\\varepsilon}^2}{br} \\right) = (a-1)(br\\sigma_{\\text{struct}}^2 + r\\sigma_{\\text{param}}^2 + \\sigma_{\\varepsilon}^2)\n$$\nTherefore,\n$$\nE[MS_{\\text{struct}}] = br\\sigma_{\\text{struct}}^2 + r\\sigma_{\\text{param}}^2 + \\sigma_{\\varepsilon}^2\n$$\n\n**Method of Moments Estimators**\nWe equate the observed mean squares to their expectations:\n1) $MS_{\\text{struct}} = br\\hat{\\sigma}_{\\text{struct}}^2 + r\\hat{\\sigma}_{\\text{param}}^2 + \\hat{\\sigma}_{\\varepsilon}^2$\n2) $MS_{\\text{param}(\\text{struct})} = r\\hat{\\sigma}_{\\text{param}}^2 + \\hat{\\sigma}_{\\varepsilon}^2$\n3) $MS_{\\varepsilon} = \\hat{\\sigma}_{\\varepsilon}^2$\n\nSolving this system by back-substitution:\nFrom (3), the estimator for $\\sigma_{\\varepsilon}^2$ is $\\hat{\\sigma}_{\\varepsilon}^2 = MS_{\\varepsilon}$.\nSubstitute into (2) to find $\\hat{\\sigma}_{\\text{param}}^2$:\n$$\nMS_{\\text{param}(\\text{struct})} = r\\hat{\\sigma}_{\\text{param}}^2 + MS_{\\varepsilon} \\implies \\hat{\\sigma}_{\\text{param}}^2 = \\frac{MS_{\\text{param}(\\text{struct})} - MS_{\\varepsilon}}{r}\n$$\nSubstitute (2) into (1) to find $\\hat{\\sigma}_{\\text{struct}}^2$:\n$$\nMS_{\\text{struct}} = br\\hat{\\sigma}_{\\text{struct}}^2 + MS_{\\text{param}(\\text{struct})} \\implies \\hat{\\sigma}_{\\text{struct}}^2 = \\frac{MS_{\\text{struct}} - MS_{\\text{param}(\\text{struct})}}{br}\n$$\nThese are the unbiased estimators for the variance components.\n\n**Part 2: Numerical Calculation**\n\nWe are given the following values for a balanced experiment:\n- Number of structures: $a=4$\n- Number of parameter sets per structure: $b=3$\n- Number of ensemble members per parameter set: $r=5$\n- Between-structures sum of squares: $SS_{\\text{struct}} = 67.5 \\, \\text{K}^2$\n- Between-parameters-within-structure sum of squares: $SS_{\\text{param}(\\text{struct})} = 36.0 \\, \\text{K}^2$\n- Residual sum of squares: $SS_{\\varepsilon} = 96.0 \\, \\text{K}^2$\n\nFirst, calculate the degrees of freedom:\n- $df_{\\text{struct}} = a - 1 = 4 - 1 = 3$\n- $df_{\\text{param}(\\text{struct})} = a(b - 1) = 4(3 - 1) = 8$\n- $df_{\\varepsilon} = ab(r - 1) = 4 \\times 3 \\times (5 - 1) = 12 \\times 4 = 48$\n\nNext, calculate the observed mean squares:\n- $MS_{\\text{struct}} = \\frac{SS_{\\text{struct}}}{df_{\\text{struct}}} = \\frac{67.5}{3} = 22.5 \\, \\text{K}^2$\n- $MS_{\\text{param}(\\text{struct})} = \\frac{SS_{\\text{param}(\\text{struct})}}{df_{\\text{param}(\\text{struct})}} = \\frac{36.0}{8} = 4.5 \\, \\text{K}^2$\n- $MS_{\\varepsilon} = \\frac{SS_{\\varepsilon}}{df_{\\varepsilon}} = \\frac{96.0}{48} = 2.0 \\, \\text{K}^2$\n\nNow, use the derived estimator formulas to compute the variance components:\n- For $\\sigma_{\\text{struct}}^{2}$:\n$$\n\\hat{\\sigma}_{\\text{struct}}^{2} = \\frac{MS_{\\text{struct}} - MS_{\\text{param}(\\text{struct})}}{br} = \\frac{22.5 - 4.5}{3 \\times 5} = \\frac{18.0}{15} = 1.2 \\, \\text{K}^2\n$$\n- For $\\sigma_{\\text{param}}^{2}$:\n$$\n\\hat{\\sigma}_{\\text{param}}^{2} = \\frac{MS_{\\text{param}(\\text{struct})} - MS_{\\varepsilon}}{r} = \\frac{4.5 - 2.0}{5} = \\frac{2.5}{5} = 0.5 \\, \\text{K}^2\n$$\nThe problem requires rounding each estimate to three significant figures.\n- $\\hat{\\sigma}_{\\text{struct}}^{2} = 1.2$ becomes $1.20$.\n- $\\hat{\\sigma}_{\\text{param}}^{2} = 0.5$ becomes $0.500$.\n\nThe final answer is the ordered pair $(\\hat{\\sigma}_{\\text{struct}}^{2}, \\hat{\\sigma}_{\\text{param}}^{2})$.",
            "answer": "$$\n\\boxed{\n(1.20, 0.500)\n}\n$$"
        }
    ]
}