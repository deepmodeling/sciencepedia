{
    "hands_on_practices": [
        {
            "introduction": "Understanding parameter identifiability begins with recognizing when a model's structure fundamentally prevents the unique determination of its parameters. This first exercise provides a clear, foundational example of structural non-identifiability where two parameters appear as a product. By working through this problem , you will learn to use both the definition of injectivity and the tools of sensitivity analysis to diagnose this issue and resolve it through a simple yet powerful reparameterization.",
            "id": "3904624",
            "problem": "An environmental monitoring station measures streamflow response to rainfall using a lumped linear input–output relationship. Let the measured output be $y(t)$ and the known exogenous input be $u(t)$, where $t$ denotes time. The model used by the analyst is\n$$\ny(t) \\;=\\; \\theta_{1}\\,\\theta_{2}\\,u(t),\n$$\nwhere $\\theta_{1}$ represents an effective runoff coefficient and $\\theta_{2}$ represents a calibration gain from instrumentation, both assumed constant in time. Measurements are taken at $N$ distinct times $\\{t_{i}\\}_{i=1}^{N}$, and the recorded data obey\n$$\ny_{i} \\;=\\; \\theta_{1}\\,\\theta_{2}\\,u(t_{i}) \\;+\\; \\varepsilon_{i}, \\quad i \\in \\{1,\\dots,N\\},\n$$\nwith $\\varepsilon_{i}$ being independent, identically distributed zero-mean Gaussian noise with variance $\\sigma^{2}$.\n\nStarting from first principles of structural identifiability in parameter estimation, and using the definition that a parameter (or function of parameters) is structurally identifiable if the input–output mapping is one-to-one with respect to that parameter under ideal noise-free conditions and known input, do the following:\n\n- Explain whether $\\theta_{1}$ and $\\theta_{2}$ are individually structurally identifiable under this model and input, and justify your conclusion using the injectivity of the parameter-to-output map.\n- Use the sensitivity matrix and the Fisher Information Matrix (Fisher Information Matrix (FIM)) under the Gaussian assumption to assess the rank properties of the information content for $(\\theta_{1},\\theta_{2})$ versus a scalar reparameterization. Explicitly construct the sensitivity vectors and show the resulting rank.\n- Propose a reparameterization that expresses the model in terms of a single structurally identifiable parameter $\\phi$ and write the reparameterized model. Justify why $\\phi$ is structurally identifiable by appealing to the one-to-one mapping from $\\phi$ to the output for nontrivial input.\n- Provide the single identifiable parameter $\\phi$ explicitly as a closed-form expression in terms of $\\theta_{1}$ and $\\theta_{2}$.\n\nYour final answer must be the closed-form expression for $\\phi$ only. Do not include any units in your final answer. Do not provide any inequalities or equations as your final answer beyond the single expression for $\\phi$.",
            "solution": "The validity of the problem statement is confirmed. It is scientifically grounded, well-posed, and objective, presenting a standard problem in system identification and parameter estimation. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThe problem asks for an analysis of the structural identifiability of the parameters in the linear model $y(t) = \\theta_{1}\\theta_{2}u(t)$. Structural identifiability concerns whether the model parameters can be uniquely determined from perfect, noise-free input-output data.\n\nThe analysis will proceed in three stages as requested: first, assessing the individual identifiability of $\\theta_{1}$ and $\\theta_{2}$ using the definition of injectivity; second, using the sensitivity and Fisher Information Matrix (FIM) to analyze the information content; and third, proposing and justifying a reparameterization into a single identifiable parameter.\n\n**1. Individual Structural Identifiability of $\\theta_{1}$ and $\\theta_{2}$**\n\nStructural identifiability is assessed under ideal conditions of noise-free data, i.e., $\\varepsilon_{i} = 0$ for all $i$. The model is given by the input-output mapping $y(t) = f(\\boldsymbol{\\theta}, u(t))$, where the parameter vector is $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2})$. The parameters are structurally identifiable if the mapping from the parameters to the output is one-to-one (injective) for a given known, non-trivial input $u(t)$.\n\nLet's test for injectivity. Suppose we have two distinct parameter vectors, $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2})$ and $\\boldsymbol{\\theta}' = (\\theta'_{1}, \\theta'_{2})$. We check if observing the same output, $y(t, \\boldsymbol{\\theta}) = y(t, \\boldsymbol{\\theta}')$ for all $t$, necessarily implies that $\\boldsymbol{\\theta} = \\boldsymbol{\\theta}'$.\nThe condition $y(t, \\boldsymbol{\\theta}) = y(t, \\boldsymbol{\\theta}')$ translates to:\n$$\n\\theta_{1}\\theta_{2}u(t) = \\theta'_{1}\\theta'_{2}u(t)\n$$\nFor a non-trivial input, there exists at least one time $t$ for which $u(t) \\neq 0$. At such a time, we can divide by $u(t)$ to obtain:\n$$\n\\theta_{1}\\theta_{2} = \\theta'_{1}\\theta'_{2}\n$$\nThis equation shows that the model output depends only on the product of the parameters, $\\theta_{1}\\theta_{2}$. It does not require $\\theta_{1} = \\theta'_{1}$ and $\\theta_{2} = \\theta'_{2}$. For any given pair $(\\theta_{1}, \\theta_{2})$, one can find infinitely many other pairs $(\\theta'_{1}, \\theta'_{2})$ that yield the same product and thus the same model output. For example, if $\\boldsymbol{\\theta} = (2, 3)$, the product is $6$. Another parameter vector, $\\boldsymbol{\\theta}' = (1.5, 4)$, also has a product of $6$. Since $\\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}'$ but $y(t, \\boldsymbol{\\theta}) = y(t, \\boldsymbol{\\theta}')$, the mapping from the parameter vector $\\boldsymbol{\\theta}$ to the output $y(t)$ is not injective. Therefore, the parameters $\\theta_{1}$ and $\\theta_{2}$ are not individually structurally identifiable. Only the functional combination $\\theta_{1}\\theta_{2}$ is identifiable.\n\n**2. Sensitivity Matrix and Fisher Information Matrix (FIM) Analysis**\n\nThis lack of identifiability can also be demonstrated by analyzing the rank of the sensitivity matrix and the Fisher Information Matrix (FIM). The sensitivity of the noise-free output $y(t)$ with respect to the parameters indicates how the output changes with an infinitesimal change in each parameter. The sensitivity vector at time $t$ for the parameter vector $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2})$ is:\n$$\n\\mathbf{s}(t) = \\begin{pmatrix} \\frac{\\partial y(t)}{\\partial \\theta_{1}} & \\frac{\\partial y(t)}{\\partial \\theta_{2}} \\end{pmatrix}\n$$\nCalculating the partial derivatives of $y(t) = \\theta_{1}\\theta_{2}u(t)$:\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\theta_{2}u(t)\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{2}} = \\theta_{1}u(t)\n$$\nSo, the sensitivity vector is $\\mathbf{s}(t) = \\begin{pmatrix} \\theta_{2}u(t) & \\theta_{1}u(t) \\end{pmatrix}$.\n\nThe sensitivity matrix, $\\mathbf{J}$, is constructed by stacking these sensitivity vectors for each of the $N$ measurement times $\\{t_{i}\\}_{i=1}^{N}$:\n$$\n\\mathbf{J} = \\begin{pmatrix} \\frac{\\partial y_{1}}{\\partial \\theta_{1}} & \\frac{\\partial y_{1}}{\\partial \\theta_{2}} \\\\ \\vdots & \\vdots \\\\ \\frac{\\partial y_{N}}{\\partial \\theta_{1}} & \\frac{\\partial y_{N}}{\\partial \\theta_{2}} \\end{pmatrix} = \\begin{pmatrix} \\theta_{2}u(t_{1}) & \\theta_{1}u(t_{1}) \\\\ \\vdots & \\vdots \\\\ \\theta_{2}u(t_{N}) & \\theta_{1}u(t_{N}) \\end{pmatrix}\n$$\nThe first column of $\\mathbf{J}$ is the vector $\\theta_{2}\\mathbf{u}$, and the second column is $\\theta_{1}\\mathbf{u}$, where $\\mathbf{u} = (u(t_{1}), \\dots, u(t_{N}))^{T}$. Assuming $\\theta_{1}$ and $\\theta_{2}$ are non-zero, the second column is a scalar multiple of the first column: $(\\text{column } 2) = \\frac{\\theta_{1}}{\\theta_{2}} (\\text{column } 1)$. The columns of $\\mathbf{J}$ are linearly dependent. Therefore, the rank of the sensitivity matrix $\\mathbf{J}$ is $1$, provided that the input vector $\\mathbf{u}$ is not the zero vector. A rank of $1$ is less than the number of parameters ($2$), which indicates structural non-identifiability.\n\nUnder the assumption of i.i.d. Gaussian noise, the Fisher Information Matrix (FIM) is given by $\\mathbf{F} = \\frac{1}{\\sigma^{2}}\\mathbf{J}^{T}\\mathbf{J}$. The FIM is a $2 \\times 2$ matrix:\n$$\n\\mathbf{F} = \\frac{1}{\\sigma^{2}} \\begin{pmatrix} \\theta_{2}u(t_{1}) & \\dots & \\theta_{2}u(t_{N}) \\\\ \\theta_{1}u(t_{1}) & \\dots & \\theta_{1}u(t_{N}) \\end{pmatrix} \\begin{pmatrix} \\theta_{2}u(t_{1}) & \\theta_{1}u(t_{1}) \\\\ \\vdots & \\vdots \\\\ \\theta_{2}u(t_{N}) & \\theta_{1}u(t_{N}) \\end{pmatrix}\n$$\n$$\n\\mathbf{F} = \\frac{1}{\\sigma^{2}} \\begin{pmatrix} \\sum_{i=1}^{N} (\\theta_{2}u(t_{i}))^{2} & \\sum_{i=1}^{N} (\\theta_{2}u(t_{i}))(\\theta_{1}u(t_{i})) \\\\ \\sum_{i=1}^{N} (\\theta_{1}u(t_{i}))(\\theta_{2}u(t_{i})) & \\sum_{i=1}^{N} (\\theta_{1}u(t_{i}))^{2} \\end{pmatrix}\n$$\nFactoring out the parameters and the sum over inputs:\n$$\n\\mathbf{F} = \\frac{\\sum_{i=1}^{N} u(t_{i})^{2}}{\\sigma^{2}} \\begin{pmatrix} \\theta_{2}^{2} & \\theta_{1}\\theta_{2} \\\\ \\theta_{1}\\theta_{2} & \\theta_{1}^{2} \\end{pmatrix}\n$$\nThe rank of the FIM is determined by the rank of the matrix component. The determinant of this matrix is $\\det(\\begin{pmatrix} \\theta_{2}^{2} & \\theta_{1}\\theta_{2} \\\\ \\theta_{1}\\theta_{2} & \\theta_{1}^{2} \\end{pmatrix}) = (\\theta_{2}^{2})(\\theta_{1}^{2}) - (\\theta_{1}\\theta_{2})(\\theta_{1}\\theta_{2}) = 0$. A matrix with a determinant of zero is singular. Since the FIM is not the zero matrix (assuming non-trivial input and non-zero parameters), its rank is $1$. A rank-deficient FIM confirms that the parameters $\\theta_1$ and $\\theta_2$ cannot be uniquely estimated.\n\n**3. Reparameterization and Justification**\n\nThe analysis above shows that while $\\theta_{1}$ and $\\theta_{2}$ are not individually identifiable, their product is. This suggests a reparameterization of the model in terms of a single, identifiable parameter. Let's define a new parameter $\\phi$ as the product of the original parameters:\n$$\n\\phi = \\theta_{1}\\theta_{2}\n$$\nThe reparameterized model is:\n$$\ny(t) = \\phi u(t)\n$$\nTo justify that $\\phi$ is structurally identifiable, we again appeal to the definition of a one-to-one mapping. Let us consider two different parameter values, $\\phi$ and $\\phi'$. If they produce the same output for a given non-trivial input $u(t)$, we have:\n$$\n\\phi u(t) = \\phi' u(t)\n$$\nFor a non-trivial input, there exists a time $t$ such that $u(t) \\neq 0$. At this time, we can divide by $u(t)$ to find:\n$$\n\\phi = \\phi'\n$$\nThis demonstrates that if two parameter values produce the same output, the parameter values must be identical. The mapping from the parameter $\\phi$ to the output $y(t)$ is injective. Therefore, $\\phi$ is structurally identifiable.\n\nThe single identifiable parameter $\\phi$ is explicitly given as the closed-form expression in terms of $\\theta_{1}$ and $\\theta_{2}$ by the definition used for the reparameterization.",
            "answer": "$$\\boxed{\\theta_{1}\\theta_{2}}$$"
        },
        {
            "introduction": "In many complex environmental models, parameters are not simply identifiable or unidentifiable; they exist on a spectrum of \"sloppiness,\" where some parameter combinations are well-constrained by data (\"stiff\") while others are not (\"sloppy\"). This practice introduces a powerful technique to navigate such models by analyzing the geometry of the parameter space through the Fisher Information Matrix (FIM). By performing an eigenanalysis of a given FIM , you will learn to distinguish between stiff and sloppy parameter directions and propose a more robust, reduced model based only on the identifiable parameter combinations.",
            "id": "3904551",
            "problem": "Consider a linearized single-box carbon cycle model used in environmental and earth system modeling to interpret atmospheric carbon dioxide time series. The box represents the atmospheric carbon stock, with fluxes parameterized by three unknown parameters: photosynthetic uptake sensitivity $a$, heterotrophic respiration rate coefficient $b$, and lateral export coefficient $c$. Under small perturbations around a nominal trajectory, the model-predicted observation vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ can be approximated by a first-order Taylor expansion in the parameter vector $\\boldsymbol{\\theta} = (a, b, c)$, as $\\mathbf{y}(\\boldsymbol{\\theta}) \\approx \\mathbf{y}(\\boldsymbol{\\theta}_{0}) + \\mathbf{S}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{0})$, where $\\mathbf{S} \\in \\mathbb{R}^{n \\times 3}$ is the sensitivity matrix whose $i$-th column contains the partial derivatives of the model outputs with respect to the $i$-th parameter. Assume additive independent Gaussian observation noise with variance $\\sigma^{2}$ in each component. In this setting, the Fisher Information Matrix (FIM) $\\mathbf{F}$ associated with $\\boldsymbol{\\theta}$ under Gaussian errors is given by $\\mathbf{F} = \\sigma^{-2} \\mathbf{S}^{\\top} \\mathbf{S}$.\n\nA data assimilation experiment yields the following symmetric positive semidefinite Fisher Information Matrix for the ordered parameter vector $(a, b, c)$:\n$$\n\\mathbf{F} = \n\\begin{pmatrix}\n100 & 95 & 0 \\\\\n95 & 100 & 0 \\\\\n0 & 0 & 0.01\n\\end{pmatrix}.\n$$\nAssume the noise variance is $\\sigma^{2} = 1$, so the matrix above is already scaled by $\\sigma^{-2}$. Define an identifiability threshold based on a curvature criterion: retain only those parameter directions whose corresponding FIM eigenvalues are at least $\\tau = 10$. Using first principles of likelihood-based inference and eigenanalysis, determine a reduced parameter set by projecting onto the identifiable subspace of the FIM. Specifically, compute the unit-norm linear combination vector of $(a, b, c)$ associated with the identifiable direction(s) as indicated by the threshold $\\tau$. Express your final answer as a single row vector of the coefficients multiplying $(a, b, c)$, written exactly with no rounding. No physical units are required for this vector. The final answer must be a single analytical expression.",
            "solution": "We begin from the standard assumptions for parameter estimation in environmental and earth system modeling under Gaussian observation errors. The model is linearized around a nominal parameter vector $\\boldsymbol{\\theta}_{0}$ so that the observation vector $\\mathbf{y}(\\boldsymbol{\\theta})$ depends approximately linearly on $\\boldsymbol{\\theta}$ via the sensitivity matrix $\\mathbf{S}$. Under independent Gaussian errors with variance $\\sigma^{2}$, the log-likelihood for $\\boldsymbol{\\theta}$ is, up to an additive constant,\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = -\\frac{1}{2 \\sigma^{2}} \\|\\mathbf{y}_{\\text{obs}} - \\mathbf{y}(\\boldsymbol{\\theta})\\|^{2},\n$$\nand the curvature of the negative log-likelihood at $\\boldsymbol{\\theta}_{0}$ is governed by the Fisher Information Matrix, which for the linearized model is\n$$\n\\mathbf{F} = \\sigma^{-2} \\mathbf{S}^{\\top} \\mathbf{S}.\n$$\nThe Fisher Information Matrix $\\mathbf{F}$ is symmetric positive semidefinite. Its eigenvalues and eigenvectors encode local identifiability: large eigenvalues correspond to directions in parameter space with high curvature of the log-likelihood (stiff directions), and small eigenvalues correspond to directions with low curvature (sloppy directions). In practice, a threshold $\\tau$ can be used to separate identifiable and non-identifiable directions: retain eigenvectors whose eigenvalues satisfy $\\lambda \\ge \\tau$.\n\nWe are given\n$$\n\\mathbf{F} =\n\\begin{pmatrix}\n100 & 95 & 0 \\\\\n95 & 100 & 0 \\\\\n0 & 0 & 0.01\n\\end{pmatrix},\n$$\nwith $\\sigma^{2} = 1$, so no further scaling is necessary. To perform eigenanalysis, note that $\\mathbf{F}$ is block diagonal with respect to the third parameter $c$; the top-left $2 \\times 2$ block couples $a$ and $b$, and the third parameter $c$ is decoupled with a small diagonal entry $0.01$.\n\nFirst, analyze the $2 \\times 2$ block:\n$$\n\\mathbf{B} = \n\\begin{pmatrix}\n100 & 95 \\\\\n95 & 100\n\\end{pmatrix}.\n$$\nThe eigenvalues of $\\mathbf{B}$ are obtained by solving\n$$\n\\det(\\mathbf{B} - \\lambda \\mathbf{I}) = 0,\n$$\nwhich yields\n$$\n\\det\\!\\begin{pmatrix}\n100 - \\lambda & 95 \\\\\n95 & 100 - \\lambda\n\\end{pmatrix}\n= (100 - \\lambda)^{2} - 95^{2} = 0.\n$$\nThus,\n$$\n(100 - \\lambda)^{2} = 95^{2} \\quad \\Rightarrow \\quad 100 - \\lambda = \\pm 95,\n$$\nleading to\n$$\n\\lambda_{1} = 100 + 95 = 195, \\quad \\lambda_{2} = 100 - 95 = 5.\n$$\nThe corresponding eigenvectors for $\\mathbf{B}$ can be found as follows. For $\\lambda_{1} = 195$,\n$$\n(\\mathbf{B} - 195 \\mathbf{I}) \\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{pmatrix}\n-95 & 95 \\\\\n95 & -95\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{a} \\\\\nv_{b}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThis yields $-95 v_{a} + 95 v_{b} = 0$, so $v_{a} = v_{b}$. A representative eigenvector is $(1, 1)$; the corresponding unit-norm vector in the $(a, b)$ subspace is $(1/\\sqrt{2}, 1/\\sqrt{2})$.\n\nFor $\\lambda_{2} = 5$,\n$$\n(\\mathbf{B} - 5 \\mathbf{I}) \\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{pmatrix}\n95 & 95 \\\\\n95 & 95\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{a} \\\\\nv_{b}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThis yields $95 v_{a} + 95 v_{b} = 0$, so $v_{a} = -v_{b}$. A representative eigenvector is $(1, -1)$; the corresponding unit-norm vector in the $(a, b)$ subspace is $(1/\\sqrt{2}, -1/\\sqrt{2})$.\n\nThe full $3 \\times 3$ matrix $\\mathbf{F}$ augments these with the third parameter $c$. Because the off-diagonal entries involving $c$ are $0$, the eigenvalues and eigenvectors extend to three dimensions by appending $0$ for the $c$ component to the first two eigenvectors, and using the standard basis vector for $c$. Therefore, the eigenvalues of $\\mathbf{F}$ are\n$$\n\\lambda_{1} = 195, \\quad \\lambda_{2} = 5, \\quad \\lambda_{3} = 0.01,\n$$\nwith associated unit-norm eigenvectors\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{v}_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nApply the identifiability threshold $\\tau = 10$. We assess each eigenvalue:\n- $\\lambda_{1} = 195 \\ge 10$: the direction spanned by $\\mathbf{v}_{1}$ is identifiable (stiff).\n- $\\lambda_{2} = 5 < 10$: the direction spanned by $\\mathbf{v}_{2}$ is sloppy and not retained.\n- $\\lambda_{3} = 0.01 < 10$: the direction spanned by $\\mathbf{v}_{3}$ is sloppy and not retained.\n\nTherefore, the identifiable subspace is one-dimensional and is spanned by $\\mathbf{v}_{1}$. A reduced parameter set is obtained by projecting $(a, b, c)$ onto this identifiable direction, yielding a single identifiable linear combination of parameters. The requested output is the unit-norm linear combination vector of $(a, b, c)$ associated with the retained direction, expressed as a row vector of coefficients. This is:\n$$\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}\n$$\n\nJustification for the selection is as follows. Under Gaussian errors, the Fisher Information Matrix encodes the local curvature of the negative log-likelihood in parameter space. Eigenvalues larger than the threshold $\\tau$ indicate directions with sufficiently high curvature relative to noise, implying estimability with finite precision and meaningful constraint from the data. Here, only the symmetric combination of $a$ and $b$ along $\\mathbf{v}_{1}$ is identifiable, while the antisymmetric combination of $a$ and $b$ and the parameter $c$ are sloppy, consistent with a sloppily parameterized model in which many directions are poorly constrained. The reduced parameter set retains the single stiff combination, improving robustness by removing unidentifiable directions.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0\\end{pmatrix}}$$"
        },
        {
            "introduction": "Theoretical identifiability is a crucial first step, but the ability to estimate parameters in practice also depends heavily on the quality and nature of the data collected. This final, computational exercise bridges theory and practice by exploring the critical role of experimental design in determining practical identifiability. By numerically calculating the Fisher Information Matrix and its associated metrics for different sampling schemes , you will gain hands-on experience in how the choice of measurement times can dramatically impact parameter uncertainty, correlation, and the overall success of a modeling endeavor.",
            "id": "3904602",
            "problem": "Consider a deterministic, noise-free mean response model for a single-output environmental process given by $y(t;\\boldsymbol{\\theta}) = \\theta_1 \\exp(-\\theta_2 t)$, where $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)$ are unknown, dimensionless parameters, and $t$ is a dimensionless, nonnegative sampling time. Suppose that measurements at sampling times $t_i$ are corrupted by independent, identically distributed additive Gaussian noise with zero mean and variance $\\sigma^2$. Assume the following foundational base: under independent Gaussian noise with variance $\\sigma^2$ and mean response $\\mu_i(\\boldsymbol{\\theta}) = y(t_i;\\boldsymbol{\\theta})$, the Fisher Information Matrix (FIM) for $\\boldsymbol{\\theta}$ equals the expected negative Hessian of the log-likelihood, which is equivalently the expectation of the outer product of the score. This can be computed from the Jacobian of the mean response with respect to $\\boldsymbol{\\theta}$. Identifiability is assessed locally at a nominal $\\boldsymbol{\\theta}$ using the FIM: local structural identifiability at $\\boldsymbol{\\theta}$ requires that the FIM is full rank. Numerical identifiability and precision can be further assessed by the spectral condition number and the parameter correlation induced by the Cramér–Rao Lower Bound (CRLB), which is the inverse of the FIM when it exists.\n\nTask: Write a program that, for each specified sampling design $\\{t_i\\}$, computes numerically at the nominal parameter $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)$ and noise level $\\sigma^2$:\n- The $2 \\times 2$ Fisher Information Matrix.\n- Its numerical rank using Singular Value Decomposition (SVD) with singular value threshold $\\tau = 10^{-10}$.\n- Its spectral condition number $\\kappa_2$, defined as the ratio of the largest to the smallest singular value when the matrix is full rank; if the matrix is singular (numerical rank less than $2$), report $+\\infty$.\n- A boolean $b_{\\text{ident}}$ indicating local identifiability, defined as $\\text{rank}(\\text{FIM}) = 2$.\n- The absolute parameter correlation between $\\theta_1$ and $\\theta_2$ obtained from the CRLB, i.e., if $\\mathbf{C} = \\text{FIM}^{-1}$ exists, then $\\rho = \\left|\\mathbf{C}_{12}\\right|/\\sqrt{\\mathbf{C}_{11}\\mathbf{C}_{22}}$; if $\\text{FIM}$ is singular, report $\\rho = \\text{NaN}$.\n\nYour program must use the following fixed nominal values and test suite. All quantities are dimensionless.\n- Nominal parameters: $\\theta_1 = 1.5$, $\\theta_2 = 0.4$.\n- Noise variance: $\\sigma^2 = 0.01$.\n- Singular value threshold: $\\tau = 10^{-10}$.\n- Test suite of sampling time designs:\n  1. $\\{t_i\\} = (0, 0, 0, 0, 0)$.\n  2. $\\{t_i\\} = (0, 1, 2, 3, 4, 5)$.\n  3. $\\{t_i\\} = (5, 5, 5, 5)$.\n  4. $\\{t_i\\} = (0, 0.01, 0.02, 0.03, 0.04)$.\n  5. $\\{t_i\\} = (0, 5, 10, 15, 20)$.\n\nNumerical and formatting requirements:\n- Compute the FIM using the Jacobian of the mean response with respect to $\\boldsymbol{\\theta}$ evaluated at the specified $\\boldsymbol{\\theta}$ and the given sampling times.\n- Determine the numerical rank using SVD and the threshold $\\tau = 10^{-10}$.\n- Report the spectral condition number as $+\\infty$ if the FIM is singular; otherwise report the finite value.\n- Report the absolute correlation as $\\text{NaN}$ if the FIM is singular; otherwise report the finite value computed from the inverse of the FIM.\n- Round all finite floating-point outputs (condition number and absolute correlation) to $6$ decimal places before printing. The boolean must be exactly either True or False.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form $[\\text{rank}, \\kappa_2, b_{\\text{ident}}, \\rho]$. For example: $[[2,123.456789,True,0.123456],[\\dots],\\dots]$. If a condition number is infinite, it must appear as $inf$. If a correlation is undefined, it must appear as $nan$.\n\nThe program must be self-contained and must not read any input. It must compute the results for the test suite above and print the single required output line. The expected outputs are numerical and boolean values only; no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded, self-contained, and well-posed. It presents a standard task in parameter estimation and identifiability analysis for a common exponential decay model. All required data, constants, and definitions are provided, and there are no internal contradictions or ambiguities.\n\nThe solution proceeds by first deriving the necessary mathematical expressions and then outlining the algorithm to compute the requested quantities for each sampling design.\n\n**1. Model, Parameters, and the Jacobian Matrix**\n\nThe deterministic mean response model is given by:\n$$\ny(t; \\boldsymbol{\\theta}) = \\theta_1 \\exp(-\\theta_2 t)\n$$\nwhere the parameter vector is $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)^T$.\n\nParameter identifiability is assessed by examining how sensitive the model output $y$ is to changes in the parameters $\\boldsymbol{\\theta}$. This sensitivity is captured by the Jacobian matrix of the model response, $\\mathbf{J}$, whose elements are the partial derivatives of the model function with respect to each parameter, evaluated at each sampling time $t_i$.\n\nThe partial derivatives are:\n$$\n\\frac{\\partial y}{\\partial \\theta_1} = \\exp(-\\theta_2 t)\n$$\n$$\n\\frac{\\partial y}{\\partial \\theta_2} = \\theta_1 \\frac{\\partial}{\\partial \\theta_2}(\\exp(-\\theta_2 t)) = \\theta_1 (\\exp(-\\theta_2 t))(-t) = -t \\theta_1 \\exp(-\\theta_2 t)\n$$\n\nFor a set of $N$ sampling times $\\{t_1, t_2, \\ldots, t_N\\}$, the Jacobian is an $N \\times 2$ matrix where the $i$-th row is the gradient of the response at time $t_i$:\n$$\n\\mathbf{J}(\\boldsymbol{\\theta}) =\n\\begin{pmatrix}\n\\frac{\\partial y(t_1; \\boldsymbol{\\theta})}{\\partial \\theta_1} & \\frac{\\partial y(t_1; \\boldsymbol{\\theta})}{\\partial \\theta_2} \\\\\n\\frac{\\partial y(t_2; \\boldsymbol{\\theta})}{\\partial \\theta_1} & \\frac{\\partial y(t_2; \\boldsymbol{\\theta})}{\\partial \\theta_2} \\\\\n\\vdots & \\vdots \\\\\n\\frac{\\partial y(t_N; \\boldsymbol{\\theta})}{\\partial \\theta_1} & \\frac{\\partial y(t_N; \\boldsymbol{\\theta})}{\\partial \\theta_2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\exp(-\\theta_2 t_1) & -t_1 \\theta_1 \\exp(-\\theta_2 t_1) \\\\\n\\exp(-\\theta_2 t_2) & -t_2 \\theta_1 \\exp(-\\theta_2 t_2) \\\\\n\\vdots & \\vdots \\\\\n\\exp(-\\theta_2 t_N) & -t_N \\theta_1 \\exp(-\\theta_2 t_N)\n\\end{pmatrix}\n$$\n\n**2. The Fisher Information Matrix (FIM)**\n\nFor independent, identically distributed additive Gaussian noise with variance $\\sigma^2$, the Fisher Information Matrix (FIM) is given by:\n$$\n\\text{FIM}(\\boldsymbol{\\theta}) = \\frac{1}{\\sigma^2} \\mathbf{J}(\\boldsymbol{\\theta})^T \\mathbf{J}(\\boldsymbol{\\theta})\n$$\nThe FIM is a $2 \\times 2$ symmetric, positive semi-definite matrix. The product $\\mathbf{J}^T \\mathbf{J}$ is:\n$$\n\\mathbf{J}^T \\mathbf{J} = \\begin{pmatrix}\n\\sum_{i=1}^N \\left(\\frac{\\partial y_i}{\\partial \\theta_1}\\right)^2 & \\sum_{i=1}^N \\frac{\\partial y_i}{\\partial \\theta_1}\\frac{\\partial y_i}{\\partial \\theta_2} \\\\\n\\sum_{i=1}^N \\frac{\\partial y_i}{\\partial \\theta_1}\\frac{\\partial y_i}{\\partial \\theta_2} & \\sum_{i=1}^N \\left(\\frac{\\partial y_i}{\\partial \\theta_2}\\right)^2\n\\end{pmatrix}\n$$\nSubstituting the partial derivatives, we obtain the elements of $\\mathbf{J}^T \\mathbf{J}$:\n$$\n(\\mathbf{J}^T \\mathbf{J})_{11} = \\sum_{i=1}^N \\exp(-2\\theta_2 t_i)\n$$\n$$\n(\\mathbf{J}^T \\mathbf{J})_{12} = (\\mathbf{J}^T \\mathbf{J})_{21} = \\sum_{i=1}^N (\\exp(-\\theta_2 t_i))(-t_i \\theta_1 \\exp(-\\theta_2 t_i)) = -\\theta_1 \\sum_{i=1}^N t_i \\exp(-2\\theta_2 t_i)\n$$\n$$\n(\\mathbf{J}^T \\mathbf{J})_{22} = \\sum_{i=1}^N (-t_i \\theta_1 \\exp(-\\theta_2 t_i))^2 = \\theta_1^2 \\sum_{i=1}^N t_i^2 \\exp(-2\\theta_2 t_i)\n$$\n\n**3. Identifiability Analysis**\n\nThe properties of the FIM, evaluated at the nominal parameter values $\\boldsymbol{\\theta} = (1.5, 0.4)$ and noise variance $\\sigma^2 = 0.01$, allow for a comprehensive assessment of identifiability.\n\n*   **Local Structural Identifiability and Rank**: The parameters $\\theta_1$ and $\\theta_2$ are locally structurally identifiable if and only if the FIM is nonsingular, i.e., it has full rank. For this $2 \\times 2$ matrix, full rank is $\\text{rank}(\\text{FIM}) = 2$. A rank less than $2$ implies that the columns of the Jacobian are linearly dependent, meaning that a change in one parameter can be perfectly counteracted by a change in the other, making them indistinguishable from the data. The boolean value $b_{\\text{ident}}$ is `True` if $\\text{rank}(\\text{FIM}) = 2$ and `False` otherwise. The numerical rank is determined by counting the number of singular values of the FIM that are greater than a small threshold $\\tau = 10^{-10}$.\n\n*   **Numerical Identifiability and Condition Number**: The spectral condition number, $\\kappa_2$, measures the sensitivity of the solution of a linear system to perturbations in the data. For the FIM, a large condition number indicates that the matrix is close to singular and the parameter estimation problem is ill-conditioned. It is defined as the ratio of the largest to the smallest singular value ($s_{max}/s_{min}$). If the FIM is determined to be singular ($s_{min} \\le \\tau$), the condition number is taken to be infinite.\n\n*   **Parameter Correlation and the Cramér-Rao Lower Bound**: If the FIM is invertible, its inverse $\\mathbf{C} = \\text{FIM}^{-1}$ is the Cramér-Rao Lower Bound (CRLB) matrix. This matrix provides a lower bound on the variance of any unbiased estimator for $\\boldsymbol{\\theta}$. The diagonal elements, $\\mathbf{C}_{11}$ and $\\mathbf{C}_{22}$, are the minimum possible variances for the estimates of $\\theta_1$ and $\\theta_2$, respectively. The off-diagonal element $\\mathbf{C}_{12}$ represents the covariance between the estimates. The absolute parameter correlation, $\\rho$, is calculated as:\n    $$\n    \\rho = \\frac{|\\mathbf{C}_{12}|}{\\sqrt{\\mathbf{C}_{11}\\mathbf{C}_{22}}}\n    $$\n    A value of $\\rho$ close to $1$ indicates high correlation between the parameter estimates, making it difficult to estimate them independently. If the FIM is singular, its inverse does not exist, and the correlation is undefined (reported as NaN).\n\n**4. Algorithm Implementation**\n\nFor each test case of sampling times $\\{t_i\\}$:\n1.  Define the nominal parameters $\\theta_1 = 1.5$, $\\theta_2 = 0.4$, and noise variance $\\sigma^2 = 0.01$.\n2.  Construct the $N \\times 2$ Jacobian matrix $\\mathbf{J}$ by evaluating its columns at each $t_i$.\n3.  Compute the FIM using $\\text{FIM} = (\\mathbf{J}^T \\mathbf{J}) / \\sigma^2$.\n4.  Perform Singular Value Decomposition (SVD) on the FIM to obtain its singular values, $s_1 \\ge s_2$.\n5.  Determine the numerical rank as the count of singular values $s_j > \\tau = 10^{-10}$.\n6.  Set $b_{\\text{ident}}$ to `True` if rank is $2$, else `False`.\n7.  If $b_{\\text{ident}}$ is `True`:\n    a. Calculate the condition number $\\kappa_2 = s_1 / s_2$.\n    b. Invert the FIM to get the CRLB matrix, $\\mathbf{C} = \\text{FIM}^{-1}$.\n    c. Calculate the absolute correlation $\\rho = |\\mathbf{C}_{0,1}| / \\sqrt{\\mathbf{C}_{0,0} \\mathbf{C}_{1,1}}$ (using 0-based indexing).\n8.  If $b_{\\text{ident}}$ is `False`:\n    a. Set $\\kappa_2$ to infinity.\n    b. Set $\\rho$ to NaN.\n9.  Store the quartet of results $[\\text{rank}, \\kappa_2, b_{\\text{ident}}, \\rho]$ for the current test case.\n10. After processing all test cases, format the collected results into the specified string format, rounding floating-point numbers to 6 decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes identifiability metrics for an exponential decay model\n    for a suite of sampling time designs.\n    \"\"\"\n    # Fixed nominal values and constants\n    THETA_NOM = (1.5, 0.4)    # (theta1, theta2)\n    SIGMA2 = 0.01             # Noise variance\n    TAU = 1e-10               # Singular value threshold for rank\n\n    # Test suite of sampling time designs\n    test_cases = [\n        (0, 0, 0, 0, 0),\n        (0, 1, 2, 3, 4, 5),\n        (5, 5, 5, 5),\n        (0, 0.01, 0.02, 0.03, 0.04),\n        (0, 5, 10, 15, 20),\n    ]\n\n    # List to store formatted string results for each case\n    results_str_list = []\n\n    for t_samples in test_cases:\n        t = np.array(t_samples, dtype=float)\n        theta1, theta2 = THETA_NOM\n\n        # 1. Construct the Jacobian matrix J\n        # Column 1: partial derivative with respect to theta1\n        j_col1 = np.exp(-theta2 * t)\n        # Column 2: partial derivative with respect to theta2\n        j_col2 = -t * theta1 * np.exp(-theta2 * t)\n\n        J = np.stack((j_col1, j_col2), axis=-1)\n\n        # 2. Compute the Fisher Information Matrix (FIM)\n        FIM = (1 / SIGMA2) * (J.T @ J)\n\n        # 3. Perform SVD to get singular values\n        # svd returns sorted singular values by default\n        try:\n            singular_values = np.linalg.svd(FIM, compute_uv=False)\n        except np.linalg.LinAlgError:\n            singular_values = np.array([0.0, 0.0]) # Should not happen for J^T J\n\n        s_max = singular_values[0] if len(singular_values) > 0 else 0.0\n        s_min = singular_values[-1] if len(singular_values) > 1 else 0.0\n\n        # 4. Compute rank, identifiability, condition number, and correlation\n        rank = np.sum(singular_values > TAU)\n        b_ident = (rank == 2)\n\n        if b_ident:\n            kappa = s_max / s_min\n            # Compute CRLB = FIM inverse\n            try:\n                CRLB = np.linalg.inv(FIM)\n                c11, c22 = CRLB[0, 0], CRLB[1, 1]\n                c12 = CRLB[0, 1]\n                \n                # Handle potential floating point issues where c11 or c22 are non-positive\n                if c11 = 0 or c22 = 0:\n                    rho = np.nan\n                else:\n                    rho = np.abs(c12) / np.sqrt(c11 * c22)\n\n            except np.linalg.LinAlgError:\n                # Should not be reached if b_ident is True, but for robustness\n                kappa = np.inf\n                rho = np.nan\n                b_ident = False\n                rank = np.sum(singular_values > TAU) # re-calculate rank based on SVD\n        else:\n            kappa = np.inf\n            rho = np.nan\n\n        # 5. Format the results for the current test case\n        formatted_res = []\n        formatted_res.append(str(rank))\n\n        if kappa == np.inf:\n            formatted_res.append(\"inf\")\n        else:\n            formatted_res.append(f\"{kappa:.6f}\")\n\n        formatted_res.append(str(b_ident))\n        \n        if np.isnan(rho):\n            formatted_res.append(\"nan\")\n        else:\n            formatted_res.append(f\"{rho:.6f}\")\n        \n        results_str_list.append(f\"[{','.join(formatted_res)}]\")\n\n    # 6. Final print statement in the exact required format\n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}