{
    "hands_on_practices": [
        {
            "introduction": "涌现约束的核心在于将来自模型集合的先验知识与来自现实世界的不完美观测相结合。这个练习将引导我们从第一性原理出发，在一个考虑了变量误差的贝叶斯分层模型中，推导约束后预测的后验均值。通过完成这个推导，你将深入理解信息是如何在这个统计框架下被融合的，为更复杂的应用打下坚实的数学基础。",
            "id": "3878124",
            "problem": "考虑一个环境和地球系统建模中的涌现约束问题。一组地球系统模型（ESM）揭示了未来预测值 $Y$（相对于历史基准的世纪末全球平均地表气温变化）与一个现今预测因子 $X^{\\ast}$（一个衡量年际变率幅度的无量纲指标）之间存在一个有物理动机的线性关系。该关系被建模为跨 ESM 集合的线性回归，并带有加性高斯散布，\n$$\nY \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(a + b\\,X^{\\ast},\\,\\sigma^{2}\\right),\n$$\n其中 $a$ 和 $b$ 是从该集合估计的回归参数，$\\sigma^{2}$ 量化了由于结构差异和内部变率导致模型间围绕该直线的散布程度。预测因子的观测估计存在变量误差：由于测量误差和抽样变率，测量值 $x_{\\mathrm{obs}}$ 偏离了潜变量 $X^{\\ast}$。观测数据模型为\n$$\nx_{\\mathrm{obs}} \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(X^{\\ast},\\,s_{x}^{2}\\right).\n$$\n为了构建完整的贝叶斯层级模型，潜预测因子 $X^{\\ast}$ 被赋予一个基于该预测因子集合分布的先验，\n$$\nX^{\\ast} \\sim \\mathcal{N}\\!\\left(\\mu_{x},\\,\\tau_{x}^{2}\\right).\n$$\n假设以下从 ESM 集合和观测特征中获得的科学上合理的参数值：\n- $a = 1.0$ (开尔文),\n- $b = 1.5$ (开尔文每单位无量纲预测因子),\n- $\\sigma = 0.2$ (开尔文),\n- $\\mu_{x} = 0.1$ (无量纲),\n- $\\tau_{x}^{2} = 0.36$,\n- $s_{x}^{2} = 0.04$,\n- $x_{\\mathrm{obs}} = 0.8$ (无量纲).\n\n从贝叶斯定理和正态分布的性质出发，推导此层级变量误差回归模型下后验均值 $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$ 的解析表达式，并明确所有条件化和边缘化步骤。然后使用给定的参数值对该表达式进行数值计算。将最终数值答案四舍五入至四位有效数字，并以开尔文表示。",
            "solution": "目标是推导给定观测数据 $x_{\\mathrm{obs}}$ 的未来预测值 $Y$ 的后验均值（记为 $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$）的解析表达式，然后对该表达式进行数值计算。推导从概率论的基本原理开始，特别是全期望定律和贝叶斯定理。\n\n我们感兴趣的量是后验期望 $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$。我们可以通过对潜（未观测到的）预测因子 $X^{\\ast}$ 进行边缘化来表示它。使用全期望定律，我们写出：\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = \\mathbb{E}_{X^{\\ast} \\mid x_{\\mathrm{obs}}}\\!\\left[\\mathbb{E}\\!\\left[Y \\mid X^{\\ast}, x_{\\mathrm{obs}}\\right]\\right]\n$$\n内部期望是 $Y$ 在给定潜变量 $X^{\\ast}$ 和观测值 $x_{\\mathrm{obs}}$ 条件下的期望值。根据问题陈述，$Y$ 的数据生成过程由 $Y \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(a + b\\,X^{\\ast},\\,\\sigma^{2}\\right)$ 给出。此表达式表明 $Y$ 的分布仅依赖于 $X^{\\ast}$。变量 $Y$ 和 $x_{\\mathrm{obs}}$ 在给定 $X^{\\ast}$ 的条件下是条件独立的。因此，一旦 $X^{\\ast}$ 已知，对 $x_{\\mathrm{obs}}$ 的条件化不会提供关于 $Y$ 的额外信息。所以：\n$$\n\\mathbb{E}\\!\\left[Y \\mid X^{\\ast}, x_{\\mathrm{obs}}\\right] = \\mathbb{E}\\!\\left[Y \\mid X^{\\ast}\\right] = a + b\\,X^{\\ast}\n$$\n将此结果代回全期望定律，得到：\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = \\mathbb{E}_{X^{\\ast} \\mid x_{\\mathrm{obs}}}\\!\\left[a + b\\,X^{\\ast}\\right]\n$$\n根据期望的线性性质，这可以简化为：\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = a + b\\,\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right]\n$$\n因此，问题简化为求给定观测值 $x_{\\mathrm{obs}}$ 时潜预测因子 $X^{\\ast}$ 的后验均值。这需要确定后验概率分布 $p(X^{\\ast} \\mid x_{\\mathrm{obs}})$。\n\n我们应用贝叶斯定理来求 $X^{\\ast}$ 的后验分布：\n$$\np(X^{\\ast} \\mid x_{\\mathrm{obs}}) = \\frac{p(x_{\\mathrm{obs}} \\mid X^{\\ast}) \\, p(X^{\\ast})}{p(x_{\\mathrm{obs}})}\n$$\n项 $p(x_{\\mathrm{obs}} \\mid X^{\\ast})$ 是给定潜变量时的观测似然，而 $p(X^{\\ast})$ 是潜变量的先验分布。分母 $p(x_{\\mathrm{obs}})$ 是边缘似然，它作为一个归一化常数。因此，后验分布正比于似然和先验的乘积：\n$$\np(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto p(x_{\\mathrm{obs}} \\mid X^{\\ast}) \\, p(X^{\\ast})\n$$\n问题给出了似然和先验的函数形式：\n\\begin{itemize}\n    \\item 似然：$x_{\\mathrm{obs}} \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(X^{\\ast},\\,s_{x}^{2}\\right)$，所以 $p(x_{\\mathrm{obs}} \\mid X^{\\ast}) \\propto \\exp\\left(-\\frac{1}{2s_{x}^{2}}(x_{\\mathrm{obs}} - X^{\\ast})^2\\right)$\n    \\item 先验：$X^{\\ast} \\sim \\mathcal{N}\\!\\left(\\mu_{x},\\,\\tau_{x}^{2}\\right)$，所以 $p(X^{\\ast}) \\propto \\exp\\left(-\\frac{1}{2\\tau_{x}^{2}}(X^{\\ast} - \\mu_x)^2\\right)$\n\\end{itemize}\n后验分布正比于这两个高斯函数的乘积。两个高斯概率密度函数的乘积正比于另一个高斯 PDF。我们可以通过分析乘积的指数来找到其参数：\n$$\n\\ln p(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2s_{x}^{2}}(X^{\\ast} - x_{\\mathrm{obs}})^2 - \\frac{1}{2\\tau_{x}^{2}}(X^{\\ast} - \\mu_{x})^2\n$$\n展开 $X^{\\ast}$ 的二次项：\n$$\n\\ln p(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2} \\left[ \\frac{(X^{\\ast})^2 - 2X^{\\ast}x_{\\mathrm{obs}} + x_{\\mathrm{obs}}^2}{s_{x}^{2}} + \\frac{(X^{\\ast})^2 - 2X^{\\ast}\\mu_x + \\mu_x^2}{\\tau_{x}^{2}} \\right]\n$$\n合并 $X^{\\ast}$ 的幂次项：\n$$\n\\ln p(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2} \\left[ (X^{\\ast})^2 \\left(\\frac{1}{s_{x}^{2}} + \\frac{1}{\\tau_{x}^{2}}\\right) - 2X^{\\ast} \\left(\\frac{x_{\\mathrm{obs}}}{s_{x}^{2}} + \\frac{\\mu_x}{\\tau_{x}^{2}}\\right) \\right] + C\n$$\n其中 $C$ 包含不涉及 $X^{\\ast}$ 的项。这个表达式是关于 $X^{\\ast}$ 的二次式，证实了后验分布是高斯分布，记为 $X^{\\ast} \\mid x_{\\mathrm{obs}} \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$。其 PDF 的指数为 $-\\frac{1}{2\\sigma_{\\text{post}}^2}(X^{\\ast} - \\mu_{\\text{post}})^2 \\propto -\\frac{1}{2}\\left[\\frac{(X^{\\ast})^2}{\\sigma_{\\text{post}}^2} - \\frac{2X^{\\ast}\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2}\\right]$。\n通过比较 $(X^{\\ast})^2$ 项的系数，我们确定了后验方差的倒数（精度）：\n$$\n\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{s_{x}^{2}} + \\frac{1}{\\tau_{x}^{2}} = \\frac{s_{x}^{2} + \\tau_{x}^{2}}{s_{x}^{2}\\tau_{x}^{2}}\n$$\n通过比较 $X^{\\ast}$ 项的系数，我们求得后验均值 $\\mu_{\\text{post}} = \\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right]$：\n$$\n\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = \\frac{x_{\\mathrm{obs}}}{s_{x}^{2}} + \\frac{\\mu_x}{\\tau_{x}^{2}}\n$$\n$$\n\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left(\\frac{x_{\\mathrm{obs}}}{s_{x}^{2}} + \\frac{\\mu_x}{\\tau_{x}^{2}}\\right) = \\left(\\frac{s_{x}^{2}\\tau_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}\\right) \\left(\\frac{x_{\\mathrm{obs}}\\tau_{x}^{2} + \\mu_x s_{x}^{2}}{s_{x}^{2}\\tau_{x}^{2}}\\right) = \\frac{x_{\\mathrm{obs}}\\tau_{x}^{2} + \\mu_x s_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}\n$$\n这个后验均值是观测值 $x_{\\mathrm{obs}}$ 和先验均值 $\\mu_x$ 的一个精度加权平均。\n\n因此，$Y$ 的后验均值的最终解析表达式为：\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = a + b\\,\\left(\\frac{\\tau_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}x_{\\mathrm{obs}} + \\frac{s_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}\\mu_x\\right)\n$$\n我们现在使用提供的参数值来计算这个表达式：$a = 1.0$, $b = 1.5$, $\\sigma = 0.2$ (意味着 $\\sigma^2 = 0.04$), $\\mu_{x} = 0.1$, $\\tau_{x}^{2} = 0.36$, $s_{x}^{2} = 0.04$, 以及 $x_{\\mathrm{obs}} = 0.8$。注意，计算后验均值不需要 $\\sigma^2$。\n\n首先，我们计算 $X^{\\ast}$ 的后验均值：\n$$\n\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right] = \\frac{0.36}{0.04 + 0.36} (0.8) + \\frac{0.04}{0.04 + 0.36} (0.1)\n$$\n$$\n\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right] = \\frac{0.36}{0.40} (0.8) + \\frac{0.04}{0.40} (0.1)\n$$\n$$\n\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right] = (0.9)(0.8) + (0.1)(0.1) = 0.72 + 0.01 = 0.73\n$$\n接下来，我们将这个值代入 $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$ 的表达式中：\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = 1.0 + 1.5 \\times 0.73\n$$\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = 1.0 + 1.095 = 2.095\n$$\n问题要求最终数值答案四舍五入到四位有效数字。计算出的值 $2.095$ 恰好有四位有效数字。单位是开尔文。",
            "answer": "$$\\boxed{2.095}$$"
        },
        {
            "introduction": "在建立了理论基础之后，下一个关键步骤是将其应用于实际的模型集合数据，并评估约束的有效性。本练习将使用一种常见的方法——普通最小二乘法（OLS）——来估计模型间的线性关系。更重要的是，它引入了“不确定性缩减因子”这一关键指标，使我们能够量化涌现约束在多大程度上减少了对未来预测的不确定性，从而判断其科学价值。",
            "id": "4047342",
            "problem": "给定一个气候模拟中涌现约束的概念性设置：在多个全球气候模式（GCMs）中，一个现今可观测量 $X$（无量纲，例如与反照率相关的度量）和一个未来响应变量 $Y$（例如雪-反照率反馈强度）被配对为 $(X_i, Y_i)$，其中 $i = 1, \\dots, n$。涌现约束假设，$X$ 的模式间系统性差异通过一个具有物理动机且可进行统计检验的关系，解释了 $Y$ 的模式间差异。假设一个跨模式的线性模型 $Y_i = a + b\\,X_i + \\varepsilon_i$，其中 $\\varepsilon_i$ 是零均值和共同方差的高斯噪声，而 $a$ 和 $b$ 是未知系数。你观测到带有测量不确定性的真实世界现今量，$X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$，其中 $x_{\\mathrm{obs}}$ 是观测值，$s_x$ 是其不确定性的标准差。任务是利用模型集合和观测所隐含的涌现约束，来量化真实世界未来量 $Y^\\star$ 的受约束预估不确定性。\n\n从统计推断的第一性原理（适用于高斯似然和无信息先验的贝叶斯定理）、作为线性高斯模型最大似然估计量的普通最小二乘（OLS）估计，以及全期望定律和全方差定律出发，推导并实现一个方法来：\n- 估计跨模式的 $X$ 和 $Y$ 之间的线性关系。\n- 针对不确定的真实世界量 $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$，构建 $Y^\\star$ 的后验预测分布。\n- 通过 $Y^\\star$ 的预测均值和预测标准差来量化受约束的预估不确定性。\n- 通过跨模式的 $Y_i$ 的样本标准差来量化无约束的预估不确定性，并报告定义为受约束的预测标准差与无约束的集合标准差之比的缩减因子。\n\n所有涉及 $Y$ 的输出都必须以 $\\mathrm{W\\,m^{-2}\\,K^{-1}}$ 为单位，并且是浮点数。缩减因子必须表示为浮点小数（无单位）。本问题不涉及角度。\n\n实现一个完整的 Python 程序，为每个测试用例计算 $Y^\\star$ 的受约束预测均值、$Y^\\star$ 的受约束预测标准差以及缩减因子（受约束的标准差除以无约束的集合标准差）。该程序不应读取任何输入，且必须能直接运行。\n\n使用以下涵盖一系列场景的参数值测试套件，包括高相关性案例、弱相关性案例、零测量不确定性的边界案例以及大测量不确定性案例。在每个案例中，$X$ 是无量纲的，$Y$ 的单位是 $\\mathrm{W\\,m^{-2}\\,K^{-1}}$：\n\n- 测试用例 1（高相关性，小测量不确定性）：\n  - $X = [\\,0.2,\\,0.4,\\,0.6,\\,0.8,\\,1.0,\\,0.3,\\,0.7,\\,0.9\\,]$\n  - $Y = [\\,0.98,\\,1.25,\\,1.73,\\,2.08,\\,2.51,\\,1.10,\\,1.86,\\,2.32\\,]$，单位为 $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.65$\n  - $s_x = 0.05$\n\n- 测试用例 2（弱相关性，可比噪声）：\n  - $X = [\\,0.1,\\,0.2,\\,0.5,\\,0.6,\\,0.8,\\,0.3,\\,0.4,\\,0.7,\\,0.9,\\,0.55\\,]$\n  - $Y = [\\,0.71,\\,1.27,\\,0.95,\\,1.26,\\,0.93,\\,1.08,\\,0.99,\\,1.17,\\,0.89,\\,1.205\\,]$，单位为 $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.50$\n  - $s_x = 0.05$\n\n- 测试用例 3（边界：零测量不确定性且观测值位于集合均值处）：\n  - $X = [\\,0.2,\\,0.3,\\,0.4,\\,0.5,\\,0.6\\,]$\n  - $Y = [\\,0.0,\\,0.05,\\,0.22,\\,0.27,\\,0.41\\,]$，单位为 $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.40$\n  - $s_x = 0.00$\n\n- 测试用例 4（中等相关性，大测量不确定性）：\n  - $X = [\\,0.2,\\,0.5,\\,0.7,\\,1.0,\\,0.3,\\,0.9\\,]$\n  - $Y = [\\,0.70,\\,0.85,\\,1.40,\\,1.65,\\,0.65,\\,1.65\\,]$，单位为 $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.60$\n  - $s_x = 0.20$\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个测试用例的结果本身就是一个三元素浮点数列表，顺序为 $[\\,Y^\\star \\text{的预测均值},\\,Y^\\star \\text{的预测标准差},\\,\\text{缩减因子}\\,]$，所有数值格式化为六位小数。例如，按要求的格式：$[[\\mu_1,\\sigma_1,r_1],[\\mu_2,\\sigma_2,r_2],[\\mu_3,\\sigma_3,r_3],[\\mu_4,\\sigma_4,r_4]]$。",
            "solution": "该问题要求推导并实现一个方法，以利用从模式模拟集合中导出的涌现约束，来计算未来气候变量 $Y^\\star$ 的受约束预估不确定性。该约束是一个现今可观测量 $X$ 与未来变量 $Y$ 之间的线性关系。对现今量 $X^\\star$ 的真实世界观测是不确定的，并由一个高斯分布描述。\n\n推导过程分为三个主要阶段：首先，使用普通最小二乘法（OLS）从模式集合中估计线性关系；其次，通过线性模型传播 $X^\\star$ 观测的不确定性，推导 $Y^\\star$ 的预测均值和方差；第三，量化不确定性的缩减程度。\n\n**1. 通过普通最小二乘法（OLS）估计线性关系**\n\n给定来自气候模式集合的 $n$ 对数据点 $(X_i, Y_i)$。假设的线性模型为：\n$$\nY_i = a + b\\,X_i + \\varepsilon_i\n$$\n其中 $a$ 和 $b$ 分别是截距和斜率系数，$\\varepsilon_i$ 是来自均值为零、方差为 $\\sigma^2$ 的高斯分布的独立同分布随机误差，即 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\nOLS 方法找到最小化残差平方和的系数估计值 $\\hat{a}$ 和 $\\hat{b}$。在高斯误差假设下，这些估计值也是最大似然估计值。它们由以下公式给出：\n$$\n\\hat{b} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{S_{XY}}{S_{XX}}\n$$\n$$\n\\hat{a} = \\bar{Y} - \\hat{b}\\,\\bar{X}\n$$\n其中 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ 和 $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ 是样本均值。\n\n残差的方差 $\\sigma^2$ 代表了各模式围绕回归线的离散程度（一种模式结构不确定性的形式），其由无偏估计量 $\\hat{\\sigma}^2$ 估计：\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i - (\\hat{a} + \\hat{b}\\,X_i))^2\n$$\n分母是 $n-2$，因为估计两个参数 $\\hat{a}$ 和 $\\hat{b}$ 用掉了两个自由度。\n\n**2. 受约束预测分布的推导**\n\n我们的任务是基于对真实世界现今量 $X^\\star$ 的不确定观测来预测真实世界未来量 $Y^\\star$。观测以分布形式给出：$X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$。量 $Y^\\star$ 通过模型 $Y^\\star = \\hat{a} + \\hat{b}\\,X^\\star$ 与 $X^\\star$ 相关，但这个预测本身也是不确定的。\n\n$Y^\\star$ 预测的总不确定性来自三个来源：（1）模型关系内在的“结构性”不确定性，由 $\\hat{\\sigma}^2$ 捕捉；（2）估计的回归参数 $\\hat{a}$ 和 $\\hat{b}$ 的不确定性；以及（3）预测变量 $X^\\star$ 的不确定性，由 $s_x^2$ 捕捉。\n\n为了找到 $Y^\\star$ 的后验预测分布的均值和方差，我们使用全期望定律和全方差定律。\n\n**$Y^\\star$ 的预测均值**\n全期望定律表明 $\\mathbb{E}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$。\n内部期望是在给定 $X^\\star$ 值的情况下 $Y^\\star$ 的预测值：\n$$\n\\mathbb{E}[Y^\\star | X^\\star = x] = \\hat{a} + \\hat{b}\\,x\n$$\n对 $X^\\star$ 的分布取外部期望：\n$$\n\\mu_{Y^\\star} = \\mathbb{E}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\hat{a} + \\hat{b}\\,X^\\star] = \\hat{a} + \\hat{b}\\,\\mathbb{E}[X^\\star]\n$$\n由于 $\\mathbb{E}[X^\\star] = x_{\\mathrm{obs}}$，预测均值为：\n$$\n\\mu_{Y^\\star} = \\hat{a} + \\hat{b}\\,x_{\\mathrm{obs}}\n$$\n\n**$Y^\\star$ 的预测方差**\n全方差定律表明 $\\mathrm{Var}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] + \\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$。\n\n第一项，$\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]]$，代表平均预测不确定性。在一个*已知*点 $x$ 处对*单个新观测值*进行预测的方差由以下公式给出：\n$$\n\\mathrm{Var}[Y^\\star | X^\\star=x] = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(x - \\bar{X})^2}{S_{XX}} \\right)\n$$\n该表达式同时考虑了残差方差（$\\hat{\\sigma}^2$）和估计的回归线本身的不确定性（带有 $1/n$ 和 $(x - \\bar{X})^2/S_{XX}$ 的项）。我们现在对 $X^\\star$ 的分布对此进行平均：\n$$\n\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] = \\mathbb{E}_{X^\\star} \\left[ \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(X^\\star - \\bar{X})^2}{S_{XX}} \\right) \\right]\n$$\n$$\n= \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} \\right) + \\frac{\\hat{\\sigma}^2}{S_{XX}} \\mathbb{E}_{X^\\star}[(X^\\star - \\bar{X})^2]\n$$\n期望 $\\mathbb{E}[(X^\\star - \\bar{X})^2]$ 可以展开为 $\\mathbb{E}[(X^\\star - x_{\\mathrm{obs}} + x_{\\mathrm{obs}} - \\bar{X})^2] = \\mathbb{E}[(X^\\star-x_{\\mathrm{obs}})^2] + (x_{\\mathrm{obs}}-\\bar{X})^2 = s_x^2 + (x_{\\mathrm{obs}}-\\bar{X})^2$。\n因此，总方差的第一项是：\n$$\n\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{s_x^2 + (x_{\\mathrm{obs}} - \\bar{X})^2}{S_{XX}} \\right)\n$$\n\n第二项，$\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$，代表由于预测变量 $X^\\star$ 的不确定性而导致的平均预测的不确定性。\n$$\n\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]] = \\mathrm{Var}_{X^\\star}[\\hat{a} + \\hat{b}\\,X^\\star] = \\hat{b}^2\\,\\mathrm{Var}[X^\\star]\n$$\n由于 $\\mathrm{Var}[X^\\star] = s_x^2$，该项为：\n$$\n\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]] = \\hat{b}^2 s_x^2\n$$\n\n合并两项，总预测方差 $\\sigma^2_{Y^\\star}$ 为：\n$$\n\\sigma^2_{Y^\\star} = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(x_{\\mathrm{obs}} - \\bar{X})^2 + s_x^2}{S_{XX}} \\right) + \\hat{b}^2 s_x^2\n$$\n受约束的预测标准差是 $\\sigma_{Y^\\star} = \\sqrt{\\sigma^2_{Y^\\star}}$。\n\n**3. 无约束不确定性与缩减因子**\n\n无约束预估不确定性定义为原始模式集合中未来响应变量 $Y$ 的离散程度，通过样本标准差进行量化：\n$$\ns_Y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (Y_i - \\bar{Y})^2}\n$$\n缩减因子 $R$ 量化了涌现约束带来的好处。它是受约束的预测标准差与无约束的集合标准差之比：\n$$\nR = \\frac{\\sigma_{Y^\\star}}{s_Y}\n$$\n$R < 1$ 的值表明该约束已成功减少了预估不确定性。\n\n实现部分将遵循这些推导出的公式，为每个给定的测试用例计算 $(\\mu_{Y^\\star}, \\sigma_{Y^\\star}, R)$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes constrained projections and uncertainty reduction for a series of test cases\n    based on the emergent constraint methodology.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": [0.2, 0.4, 0.6, 0.8, 1.0, 0.3, 0.7, 0.9],\n            \"Y\": [0.98, 1.25, 1.73, 2.08, 2.51, 1.10, 1.86, 2.32],\n            \"x_obs\": 0.65,\n            \"s_x\": 0.05,\n        },\n        {\n            \"X\": [0.1, 0.2, 0.5, 0.6, 0.8, 0.3, 0.4, 0.7, 0.9, 0.55],\n            \"Y\": [0.71, 1.27, 0.95, 1.26, 0.93, 1.08, 0.99, 1.17, 0.89, 1.205],\n            \"x_obs\": 0.50,\n            \"s_x\": 0.05,\n        },\n        {\n            \"X\": [0.2, 0.3, 0.4, 0.5, 0.6],\n            \"Y\": [0.0, 0.05, 0.22, 0.27, 0.41],\n            \"x_obs\": 0.40,\n            \"s_x\": 0.00,\n        },\n        {\n            \"X\": [0.2, 0.5, 0.7, 1.0, 0.3, 0.9],\n            \"Y\": [0.70, 0.85, 1.40, 1.65, 0.65, 1.65],\n            \"x_obs\": 0.60,\n            \"s_x\": 0.20,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        X_data = np.array(case[\"X\"])\n        Y_data = np.array(case[\"Y\"])\n        x_obs = case[\"x_obs\"]\n        s_x = case[\"s_x\"]\n\n        n = len(X_data)\n        \n        # Step 1: OLS Estimation\n        X_mean = np.mean(X_data)\n        Y_mean = np.mean(Y_data)\n\n        S_xy = np.sum((X_data - X_mean) * (Y_data - Y_mean))\n        S_xx = np.sum((X_data - X_mean)**2)\n        \n        if S_xx == 0:\n            # Handle degenerate case where all X values are the same.\n            # This should not occur in the given test cases.\n            # In this scenario, b_hat is undefined and correlation is meaningless.\n            # For a valid solution, we would need to flag this as problematic.\n            # This simple handling prevents division by zero.\n            b_hat = 0\n            a_hat = Y_mean\n            sigma_hat_sq = np.var(Y_data, ddof=1) if n > 1 else 0\n        else:\n            b_hat = S_xy / S_xx\n            a_hat = Y_mean - b_hat * X_mean\n\n            Y_pred_model = a_hat + b_hat * X_data\n            residuals = Y_data - Y_pred_model\n            SSR = np.sum(residuals**2)\n            # Degrees of freedom are n-2 for simple linear regression\n            sigma_hat_sq = SSR / (n - 2)\n            \n        # Step 2: Calculate constrained predictive mean and variance\n        pred_mean_Y = a_hat + b_hat * x_obs\n        \n        # The total predictive variance calculation requires S_xx > 0\n        if S_xx > 0:\n            term1 = sigma_hat_sq * (1 + 1/n + ((x_obs - X_mean)**2 + s_x**2) / S_xx)\n            term2 = b_hat**2 * s_x**2\n            pred_var_Y = term1 + term2\n        else: # If S_xx=0, we cannot use the constraint. Uncertainty is just the sample variance.\n            pred_var_Y = np.var(Y_data, ddof=1) if n > 1 else 0\n\n        pred_std_Y = np.sqrt(pred_var_Y)\n        \n        # Step 3: Quantify uncertainty reduction\n        unconstrained_std_Y = np.std(Y_data, ddof=1)\n        \n        if unconstrained_std_Y > 0:\n            reduction_factor = pred_std_Y / unconstrained_std_Y\n        else:\n            # If original data has no spread, the concept of reduction is ill-defined.\n            reduction_factor = 1.0 if pred_std_Y == 0 else float('inf')\n\n        results.append([pred_mean_Y, pred_std_Y, reduction_factor])\n\n    # Final print statement in the exact required format.\n    # Format each float to 6 decimal places and then join.\n    formatted_results = []\n    for res_list in results:\n        formatted_list_str = f\"[{','.join([f'{val:.6f}' for val in res_list])}]\"\n        formatted_results.append(formatted_list_str)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的物理关系很少是完美的线性关系。这个练习将我们带入一个更高级也更现实的场景：如何处理非线性涌现约束。我们将学习使用赤池信息量准则（AIC）在多个候选函数形式（线性、二次、指数）之间进行选择，并计算选定非线性模型下的预测不确定性。此练习还强调了检查外推风险的重要性，这是应用任何统计模型时都必须具备的审慎思维。",
            "id": "3878138",
            "problem": "设一涌现约束 (Emergent Constraint, EC) 构建于一个无量纲预测变量 $X$（例如，历史变率度量）与一个无量纲目标变量 $Y$（例如，未来变化度量）之间，数据来自一个包含 $N$ 个地球系统模型的集合。给定一个成对样本的集合 $\\{(x_i,y_i)\\}_{i=1}^N$ 和预测变量的观测分布 $X_{\\text{obs}} \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$，其中 $\\mathcal{N}$ 表示均值为 $\\mu_x$、方差为 $\\sigma_x^2$ 的正态分布。假设存在加性高斯残差 $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$，且 $X$ 与 $\\epsilon$ 相互独立。\n\n候选的 EC 函数形式为：\n- 线性：$f_1(x) = a_1 + b_1 x$。\n- 二次：$f_2(x) = a_2 + b_2 x + c_2 x^2$。\n- 饱和指数：$f_3(x) = \\alpha_3 + \\beta_3 \\left(1 - e^{-\\gamma_3 x}\\right)$。\n\n对于每种函数形式 $f_j$，在高斯残差假设下通过最大似然估计其参数。在此假设下，$\\sigma_\\epsilon^2$ 的最大似然估计为 $\\hat{\\sigma}_\\epsilon^2 = \\frac{1}{N}\\sum_{i=1}^N r_i^2$，其中 $r_i = y_i - f_j(x_i)$ 是残差。最大化对数似然为 $\\ln(\\hat{L}) = -\\frac{N}{2}\\left(\\ln\\left(2\\pi\\hat{\\sigma}_\\epsilon^2\\right) + 1\\right)$。使用 Akaike 信息准则 (AIC)，其定义为 $\\mathrm{AIC} = 2k - 2\\ln(\\hat{L})$，其中 $k$ 是包括 $\\sigma_\\epsilon^2$ 在内的估计参数的数量。\n\n选择 AIC 最低的函数形式。然后，对于所选形式，通过对 $X_{\\text{obs}} \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$ 和加性残差 $\\epsilon \\sim \\mathcal{N}(0,\\hat{\\sigma}_\\epsilon^2)$ 进行边缘化，计算 EC 约束的预测均值 $m = \\mathbb{E}[Y \\mid X_{\\text{obs}}]$ 和标准差 $s = \\sqrt{\\operatorname{Var}(Y \\mid X_{\\text{obs}})}$。定义外推指标 $E$，如果 $\\mu_x$ 位于闭区间 $[\\min_i x_i,\\max_i x_i]$ 之外，则为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n\n您的程序必须实现上述过程，并为每个测试用例返回一个列表 $[m, s, i, E]$，其中 $m$ 是浮点数， $s$ 是浮点数， $i$ 是所选函数形式的整数索引（线性为 $i=0$，二次为 $i=1$，饱和指数为 $i=2$），$E$ 是布尔值外推标志。所有量都是无量纲的。\n\n使用以下测试套件。在每种情况下，都明确给出了 $\\{x_i\\}$ 和 $\\{y_i\\}$，以及 $(\\mu_x,\\sigma_x)$：\n\n- 测试用例 1（近似线性 EC）：\n  - 预测变量集合：$x = [\\,0,\\;0.5,\\;1.0,\\;1.5,\\;2.0\\,]$。\n  - 目标变量集合：$y = [\\,2.1,\\;3.45,\\;5.08,\\;6.48,\\;8.05\\,]$。\n  - 观测值：$\\mu_x = 1.2$, $\\sigma_x = 0.1$。\n\n- 测试用例 2（近似二次 EC）：\n  - 预测变量集合：$x = [\\,0,\\;0.5,\\;1.0,\\;1.5,\\;2.0,\\;2.5\\,]$。\n  - 目标变量集合：$y = [\\,1.02,\\;0.595,\\;0.51,\\;0.675,\\;0.98,\\;1.665\\,]$。\n  - 观测值：$\\mu_x = 1.8$, $\\sigma_x = 0.2$。\n\n- 测试用例 3（近似饱和指数 EC）：\n  - 预测变量集合：$x = [\\,0,\\;0.5,\\;1.0,\\;1.5,\\;2.0,\\;3.0\\,]$。\n  - 目标变量集合：$y = [\\,0.1,\\;2.45594182,\\;3.5440289405,\\;4.353505559,\\;4.6264102335,\\;4.99338139\\,]$。\n  - 观测值：$\\mu_x = 1.7$, $\\sigma_x = 0.3$。\n\n- 测试用例 4（外推检查）：\n  - 预测变量集合：$x = [\\,0.0,\\;0.2,\\;0.4,\\;0.6,\\;0.8,\\;1.0\\,]$。\n  - 目标变量集合：$y = [\\,0.98,\\;1.41,\\;1.8,\\;2.19,\\;2.62,\\;2.98\\,]$。\n  - 观测值：$\\mu_x = 2.0$, $\\sigma_x = 0.1$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，即 $[\\,[m_1,s_1,i_1,E_1],\\,[m_2,s_2,i_2,E_2],\\,[m_3,s_3,i_3,E_3],\\,[m_4,s_4,i_4,E_4]\\,]$。所有值 $m_j$ 和 $s_j$ 都必须是浮点数，$i_j$ 必须是整数，而 $E_j$ 必须是布尔值。由于所有量都是无量纲的，因此不需要进行单位转换。",
            "solution": "我们通过假设一个函数关系 $Y = f(X) + \\epsilon$ 来形式化涌现约束，其中 $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$，并且对于模型输出，$X$ 被视为已知；对于观测值，$X$ 被视为随机变量。集合 $\\{(x_i,y_i)\\}_{i=1}^N$ 提供了样本，用于在高斯残差下通过最大似然来估计 $f$ 和 $\\sigma_\\epsilon^2$。\n\n给定一个参数形式为 $f_\\theta(x)$，其参数向量为 $\\theta$，高斯似然为\n$$\nL(\\theta,\\sigma_\\epsilon^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma_\\epsilon^2}} \\exp\\left(-\\frac{\\left(y_i - f_\\theta(x_i)\\right)^2}{2\\sigma_\\epsilon^2}\\right).\n$$\n对数似然为\n$$\n\\ln L(\\theta,\\sigma_\\epsilon^2) = -\\frac{N}{2}\\ln(2\\pi\\sigma_\\epsilon^2) - \\frac{1}{2\\sigma_\\epsilon^2}\\sum_{i=1}^N \\left(y_i - f_\\theta(x_i)\\right)^2.\n$$\n在固定 $\\theta$ 的情况下，对 $\\sigma_\\epsilon^2$ 进行最大化，得到最大似然估计\n$$\n\\hat{\\sigma}_\\epsilon^2 = \\frac{1}{N}\\sum_{i=1}^N r_i^2,\\quad r_i = y_i - f_{\\hat{\\theta}}(x_i),\n$$\n最大化的对数似然计算结果为\n$$\n\\ln(\\hat{L}) = -\\frac{N}{2}\\left(\\ln\\left(2\\pi\\hat{\\sigma}_\\epsilon^2\\right) + 1\\right).\n$$\n为了平衡拟合优度和简约性，通过最小化 Akaike 信息准则来选择函数形式，\n$$\n\\mathrm{AIC} = 2k - 2\\ln(\\hat{L}),\n$$\n其中 $k$ 计算了包括 $\\sigma_\\epsilon^2$ 在内的自由参数数量。对于候选形式，线性模型的 $k$ 等于 3（$a_1$, $b_1$, $\\sigma_\\epsilon^2$），二次模型的 $k$ 等于 4（$a_2$, $b_2$, $c_2$, $\\sigma_\\epsilon^2$），饱和指数模型的 $k$ 等于 4（$\\alpha_3$, $\\beta_3$, $\\gamma_3$, $\\sigma_\\epsilon^2$）。\n\n一旦选定形式，我们在 $X$ 和残差噪声 $\\epsilon$ 相互独立的假设下，推导观测预测变量 $X_{\\text{obs}} \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$ 的预测矩：\n$$\nY = f(X) + \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0,\\hat{\\sigma}_\\epsilon^2).\n$$\n因此，\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = \\mathbb{E}[f(X)],\\quad \\operatorname{Var}(Y \\mid X_{\\text{obs}}) = \\operatorname{Var}(f(X)) + \\hat{\\sigma}_\\epsilon^2.\n$$\n我们使用正态分布的已知性质和矩生成函数来计算这些矩。\n\n对于线性模型 $f_1(x) = a_1 + b_1 x$，\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = a_1 + b_1 \\mu_x,\\quad \\operatorname{Var}(Y \\mid X_{\\text{obs}}) = b_1^2 \\sigma_x^2 + \\hat{\\sigma}_\\epsilon^2.\n$$\n\n对于二次模型 $f_2(x) = a_2 + b_2 x + c_2 x^2$，当 $X \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$ 时，使用 $\\mathbb{E}[X^2] = \\mu_x^2 + \\sigma_x^2$，$\\operatorname{Var}(X) = \\sigma_x^2$，$\\operatorname{Var}(X^2) = 2\\sigma_x^4 + 4\\mu_x^2\\sigma_x^2$ 和 $\\operatorname{Cov}(X,X^2) = 2\\mu_x\\sigma_x^2$，\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = a_2 + b_2 \\mu_x + c_2\\left(\\mu_x^2 + \\sigma_x^2\\right),\n$$\n$$\n\\operatorname{Var}(Y \\mid X_{\\text{obs}}) = b_2^2 \\sigma_x^2 + c_2^2\\left(2\\sigma_x^4 + 4\\mu_x^2\\sigma_x^2\\right) + 2b_2c_2\\left(2\\mu_x\\sigma_x^2\\right) + \\hat{\\sigma}_\\epsilon^2.\n$$\n\n对于饱和指数模型 $f_3(x) = \\alpha_3 + \\beta_3 \\left(1 - e^{-\\gamma_3 x}\\right)$，记 $Z = e^{-\\gamma_3 X}$。对于 $X \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$，使用正态随机变量的矩生成函数，我们有\n$$\n\\mathbb{E}[Z] = \\exp\\left(-\\gamma_3 \\mu_x + \\frac{1}{2}\\gamma_3^2 \\sigma_x^2\\right),\n$$\n和\n$$\n\\operatorname{Var}(Z) = \\exp\\left(-2\\gamma_3 \\mu_x\\right)\\left[\\exp\\left(2\\gamma_3^2 \\sigma_x^2\\right) - \\exp\\left(\\gamma_3^2 \\sigma_x^2\\right)\\right].\n$$\n因此，\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = \\alpha_3 + \\beta_3\\left(1 - \\mathbb{E}[Z]\\right),\n$$\n$$\n\\operatorname{Var}(Y \\mid X_{\\text{obs}}) = \\beta_3^2 \\operatorname{Var}(Z) + \\hat{\\sigma}_\\epsilon^2.\n$$\n\n外推指标通过将观测的预测变量均值与集合支持区间进行比较来定义：\n$$\nE = \\begin{cases}\n\\mathrm{True},  \\text{如果 } \\mu_x < \\min_i x_i \\text{ 或 } \\mu_x > \\max_i x_i,\\\\\n\\mathrm{False},  \\text{否则}.\n\\end{cases}\n$$\n\n每个测试用例的算法步骤：\n$1.$ 通过最小化 $\\sum_i r_i^2$（等效于最大化高斯似然），将所有三个候选模型拟合到 $\\{(x_i,y_i)\\}_{i=1}^N$，获得参数估计值，并计算 $\\hat{\\sigma}_\\epsilon^2 = \\frac{1}{N}\\sum_i r_i^2$ 和 $\\ln(\\hat{L}) = -\\frac{N}{2}\\left(\\ln\\left(2\\pi\\hat{\\sigma}_\\epsilon^2\\right) + 1\\right)$。\n$2.$ 为每个候选模型计算 $\\mathrm{AIC} = 2k - 2\\ln(\\hat{L})$，其中线性的 $k$ 等于 3，二次的 $k$ 等于 4，饱和指数的 $k$ 等于 4。\n$3.$ 选择 AIC 最小的候选模型，并使用上述公式计算 $m = \\mathbb{E}[Y \\mid X_{\\text{obs}}]$ 和 $s = \\sqrt{\\operatorname{Var}(Y \\mid X_{\\text{obs}})}$。\n$4.$ 按规定计算 $E$。\n$5.$ 返回 $[m,s,i,E]$，其中 $i \\in \\{0,1,2\\}$ 分别对应线性、二次和饱和指数模型。\n\n最终程序将此过程应用于提供的四个测试用例，并打印一行包含聚合结果列表的输出 $[\\,[m_1,s_1,i_1,E_1],\\,[m_2,s_2,i_2,E_2],\\,[m_3,s_3,i_3,E_3],\\,[m_4,s_4,i_4,E_4]\\,]$。所有量都是无量纲的，不需要进行单位转换。该方法利用了正态分布和最大似然估计的基本性质，而没有依赖于标准推导之外的快捷公式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef fit_linear(x, y):\n    # Fit y = a + b x\n    # np.polyfit returns [b, a] for deg=1\n    b, a = np.polyfit(x, y, deg=1)\n    y_hat = a + b * x\n    residuals = y - y_hat\n    N = len(y)\n    sigma_eps2 = float(np.sum(residuals**2) / N)\n    if sigma_eps2 <= 0: sigma_eps2 = 1e-12\n    logL = -0.5 * N * (np.log(2 * np.pi * sigma_eps2) + 1.0)\n    k = 3  # a, b, sigma^2\n    AIC = 2 * k - 2 * logL\n    return {'name': 'linear', 'params': (a, b), 'sigma_eps2': sigma_eps2, 'AIC': AIC}\n\ndef fit_quadratic(x, y):\n    # Fit y = a + b x + c x^2\n    # np.polyfit returns [c, b, a] for deg=2\n    c, b, a = np.polyfit(x, y, deg=2)\n    y_hat = a + b * x + c * x**2\n    residuals = y - y_hat\n    N = len(y)\n    sigma_eps2 = float(np.sum(residuals**2) / N)\n    if sigma_eps2 <= 0: sigma_eps2 = 1e-12\n    logL = -0.5 * N * (np.log(2 * np.pi * sigma_eps2) + 1.0)\n    k = 4  # a, b, c, sigma^2\n    AIC = 2 * k - 2 * logL\n    return {'name': 'quadratic', 'params': (a, b, c), 'sigma_eps2': sigma_eps2, 'AIC': AIC}\n\ndef exp_saturating(x, alpha, beta, gamma):\n    return alpha + beta * (1.0 - np.exp(-gamma * x))\n\ndef fit_exponential(x, y):\n    # Fit y = alpha + beta (1 - exp(-gamma x))\n    # Provide reasonable initial guesses and bounds\n    alpha0 = float(np.min(y))\n    beta0 = float(np.max(y) - alpha0)\n    x_range = float(np.max(x) - np.min(x))\n    gamma0 = 1.0 / (x_range + 1e-6)\n    p0 = (alpha0, beta0 if beta0 > 0 else 1.0, gamma0)\n    bounds = ([-1e3, 0.0, 1e-6], [1e3, 1e3, 1e3])\n    try:\n        popt, _ = curve_fit(exp_saturating, x, y, p0=p0, bounds=bounds, maxfev=10000)\n    except Exception:\n        # Fallback to initial guesses if optimization fails\n        popt = p0\n    alpha, beta, gamma = popt\n    y_hat = exp_saturating(x, alpha, beta, gamma)\n    residuals = y - y_hat\n    N = len(y)\n    sigma_eps2 = float(np.sum(residuals**2) / N)\n    if sigma_eps2 <= 0: sigma_eps2 = 1e-12\n    logL = -0.5 * N * (np.log(2 * np.pi * sigma_eps2) + 1.0)\n    k = 4  # alpha, beta, gamma, sigma^2\n    AIC = 2 * k - 2 * logL\n    return {'name': 'exponential', 'params': (alpha, beta, gamma), 'sigma_eps2': sigma_eps2, 'AIC': AIC}\n\ndef moments_linear(params, mu_x, sigma_x, sigma_eps2):\n    a, b = params\n    mean = a + b * mu_x\n    var = (b ** 2) * (sigma_x ** 2) + sigma_eps2\n    return mean, var\n\ndef moments_quadratic(params, mu_x, sigma_x, sigma_eps2):\n    a, b, c = params\n    sigma_x2 = sigma_x**2\n    Ex2 = mu_x**2 + sigma_x2\n    VarX = sigma_x2\n    VarX2 = 2.0 * (sigma_x2**2) + 4.0 * (mu_x**2) * sigma_x2\n    CovXX2 = 2.0 * mu_x * sigma_x2\n    mean = a + b * mu_x + c * Ex2\n    var = (b**2) * VarX + (c**2) * VarX2 + 2.0 * b * c * CovXX2 + sigma_eps2\n    return mean, var\n\ndef moments_exponential(params, mu_x, sigma_x, sigma_eps2):\n    alpha, beta, gamma = params\n    sigma_x2 = sigma_x**2\n    Ez = np.exp(-gamma * mu_x + 0.5 * (gamma**2) * sigma_x2)\n    var_z = np.exp(-2.0 * gamma * mu_x) * (np.exp(2.0 * (gamma**2) * sigma_x2) - np.exp((gamma**2) * sigma_x2))\n    mean = alpha + beta * (1.0 - Ez)\n    var = (beta**2) * var_z + sigma_eps2\n    return mean, var\n\ndef select_model_and_predict(x, y, mu_x, sigma_x):\n    # Fit all models\n    models = [\n        ('linear', fit_linear(x, y)),\n        ('quadratic', fit_quadratic(x, y)),\n        ('exponential', fit_exponential(x, y))\n    ]\n    # Select by lowest AIC\n    AICs = [m[1]['AIC'] for m in models]\n    best_idx = int(np.argmin(AICs))\n    best_name, best = models[best_idx]\n    # Compute predictive moments based on selected model\n    if best_name == 'linear':\n        mean, var = moments_linear(best['params'], mu_x, sigma_x, best['sigma_eps2'])\n        model_index = 0\n    elif best_name == 'quadratic':\n        mean, var = moments_quadratic(best['params'], mu_x, sigma_x, best['sigma_eps2'])\n        model_index = 1\n    else: # exponential\n        mean, var = moments_exponential(best['params'], mu_x, sigma_x, best['sigma_eps2'])\n        model_index = 2\n    std = float(np.sqrt(var))\n    # Extrapolation flag\n    extrap = bool((mu_x < float(np.min(x))) or (mu_x > float(np.max(x))))\n    return [float(mean), float(std), int(model_index), extrap]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0]),\n            'y': np.array([2.1, 3.45, 5.08, 6.48, 8.05]),\n            'mu_x': 1.2,\n            'sigma_x': 0.1\n        },\n        # Test Case 2\n        {\n            'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0, 2.5]),\n            'y': np.array([1.02, 0.595, 0.51, 0.675, 0.98, 1.665]),\n            'mu_x': 1.8,\n            'sigma_x': 0.2\n        },\n        # Test Case 3\n        {\n            'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0, 3.0]),\n            'y': np.array([0.1, 2.45594182, 3.5440289405, 4.353505559, 4.6264102335, 4.99338139]),\n            'mu_x': 1.7,\n            'sigma_x': 0.3\n        },\n        # Test Case 4\n        {\n            'x': np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n            'y': np.array([0.98, 1.41, 1.8, 2.19, 2.62, 2.98]),\n            'mu_x': 2.0,\n            'sigma_x': 0.1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x = case['x']\n        y = case['y']\n        mu_x = case['mu_x']\n        sigma_x = case['sigma_x']\n        res = select_model_and_predict(x, y, mu_x, sigma_x)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}