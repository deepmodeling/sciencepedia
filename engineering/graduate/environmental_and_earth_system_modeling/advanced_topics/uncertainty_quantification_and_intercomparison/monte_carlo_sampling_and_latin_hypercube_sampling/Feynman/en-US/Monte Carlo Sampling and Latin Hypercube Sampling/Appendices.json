{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Latin Hypercube Sampling (LHS), it is essential to see how it works from the ground up. This first exercise provides a concrete, hands-on opportunity to build a small LHS design and verify its defining feature: perfect marginal stratification . By manually applying random permutations and jittering, you will gain an intuitive feel for how LHS ensures a balanced and efficient exploration of the input parameter space.",
            "id": "3897076",
            "problem": "Consider a three-parameter environmental model in which three uncertain inputs are modeled as independent standard uniform random variables on the unit interval: precipitation scaling factor $p \\sim \\mathrm{Uniform}(0,1)$, soil hydraulic conductivity scaling factor $k \\sim \\mathrm{Uniform}(0,1)$, and surface albedo scaling factor $a \\sim \\mathrm{Uniform}(0,1)$. As part of a Monte Carlo sampling (MCS) design, you are to construct a Latin hypercube sampling (LHS) of size $n=5$ in dimension $d=3$ using the canonical construction built from independent random permutations and independent uniform jitters inside equal-probability strata.\n\nUse the following independent permutations of $\\{1,2,3,4,5\\}$ for the three dimensions,\n$$\n\\sigma_{p} = (3,\\,1,\\,5,\\,2,\\,4),\\quad\n\\sigma_{k} = (5,\\,2,\\,4,\\,1,\\,3),\\quad\n\\sigma_{a} = (2,\\,3,\\,1,\\,5,\\,4),\n$$\nand the following independent jitter values $u_{i,j} \\in (0,1)$, where $i \\in \\{1,2,3,4,5\\}$ indexes samples and $j \\in \\{p,k,a\\}$ indexes dimensions,\n$$\n\\begin{aligned}\n&u_{1,p}=0.37,\\quad u_{1,k}=0.82,\\quad u_{1,a}=0.15,\\\\\n&u_{2,p}=0.58,\\quad u_{2,k}=0.29,\\quad u_{2,a}=0.64,\\\\\n&u_{3,p}=0.09,\\quad u_{3,k}=0.53,\\quad u_{3,a}=0.41,\\\\\n&u_{4,p}=0.76,\\quad u_{4,k}=0.12,\\quad u_{4,a}=0.87,\\\\\n&u_{5,p}=0.24,\\quad u_{5,k}=0.67,\\quad u_{5,a}=0.33.\n\\end{aligned}\n$$\nConstruct the explicit LHS sample matrix $X \\in [0,1]^{5 \\times 3}$ using the standard LHS mapping\n$$\nx_{i,j} \\;=\\; \\frac{\\sigma_{j}(i) - u_{i,j}}{n},\n$$\nfor each sample index $i \\in \\{1,2,3,4,5\\}$ and each dimension $j \\in \\{p,k,a\\}$. Then, for each marginal dimension $j$, partition $[0,1]$ into $n=5$ equal-width bins\n$$\nB_{k} \\;=\\; \\left(\\frac{k-1}{5},\\,\\frac{k}{5}\\right),\\quad k=1,2,3,4,5,\n$$\nand compute the marginal bin counts $c_{j,k}$, defined as the number of samples $x_{i,j}$ that fall in bin $B_{k}$ for dimension $j$. Finally, define the stratification deviation metric\n$$\nD_{\\max} \\;=\\; \\max_{j \\in \\{p,k,a\\}} \\;\\max_{k \\in \\{1,2,3,4,5\\}} \\; \\left|\\,c_{j,k} - 1\\,\\right|.\n$$\n\nStarting from the foundational definition of Latin hypercube sampling as stratified sampling in each marginal with independent permutations and jittering, construct $X$, compute all bin counts $c_{j,k}$, and evaluate $D_{\\max}$. Express the final answer as an exact real number. No rounding is necessary, and no physical units apply to $D_{\\max}$.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Model parameters**: Three independent inputs, $p$, $k$, $a$, modeled as $\\mathrm{Uniform}(0,1)$ random variables.\n- **Sampling method**: Latin hypercube sampling (LHS).\n- **Sample size**: $n=5$.\n- **Dimension**: $d=3$.\n- **Permutations**:\n  - $\\sigma_{p} = (3, 1, 5, 2, 4)$\n  - $\\sigma_{k} = (5, 2, 4, 1, 3)$\n  - $\\sigma_{a} = (2, 3, 1, 5, 4)$\n- **Jitter values**: $u_{i,j} \\in (0,1)$, provided in a $5 \\times 3$ table.\n  - $u_{1,p}=0.37$, $u_{1,k}=0.82$, $u_{1,a}=0.15$\n  - $u_{2,p}=0.58$, $u_{2,k}=0.29$, $u_{2,a}=0.64$\n  - $u_{3,p}=0.09$, $u_{3,k}=0.53$, $u_{3,a}=0.41$\n  - $u_{4,p}=0.76$, $u_{4,k}=0.12$, $u_{4,a}=0.87$\n  - $u_{5,p}=0.24$, $u_{5,k}=0.67$, $u_{5,a}=0.33$\n- **LHS mapping formula**: $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{n}$ for $i \\in \\{1,2,3,4,5\\}$ and $j \\in \\{p,k,a\\}$.\n- **Bins**: $B_{k} = \\left(\\frac{k-1}{5}, \\frac{k}{5}\\right)$ for $k=1,2,3,4,5$.\n- **Bin counts**: $c_{j,k}$ is the number of samples $x_{i,j}$ that fall in bin $B_{k}$ for dimension $j$.\n- **Metric**: $D_{\\max} = \\max_{j \\in \\{p,k,a\\}} \\max_{k \\in \\{1,2,3,4,5\\}} |c_{j,k} - 1|$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard, albeit specific, procedure for generating a Latin hypercube sample. The concepts—uniform distributions, permutations, stratification, and jittering—are fundamental to Monte Carlo methods in computational science and statistics. The provided LHS mapping formula, $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{n}$, is a valid (though less common than $x_{i,j} = \\frac{\\sigma_{j}(i) - 1 + u_{i,j}}{n}$) construction. It guarantees the key stratification property of LHS, which is that each marginal axis is divided into $n$ equal-probability intervals, and each interval contains exactly one sample point. The problem is self-contained, with all necessary data and formulas provided. It is scientifically grounded, well-posed, objective, and does not violate any of the invalidity criteria. The problem requires explicit construction and verification of a property that is known from theory, which is a valid pedagogical exercise.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be constructed.\n\nThe core of the problem is to construct a specific Latin hypercube sample and then verify its stratification property using the given metric $D_{\\max}$. The defining characteristic of an LHS of size $n$ is that for each dimension (or parameter), the $n$ samples are placed such that there is exactly one sample in each of the $n$ equal-probability marginal strata. In this case, with inputs from $\\mathrm{Uniform}(0,1)$, these strata are the bins $B_k$.\n\nThe prescribed mapping is $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{n}$. Here $n=5$, and for each dimension $j$, $\\sigma_j$ is a permutation of $\\{1, 2, 3, 4, 5\\}$. The jitter $u_{i,j}$ is in the interval $(0,1)$. For a given sample $i$ and dimension $j$, let's determine which bin $B_k$ the value $x_{i,j}$ falls into. A value $x$ is in bin $B_k = \\left(\\frac{k-1}{n}, \\frac{k}{n}\\right)$ if $\\frac{k-1}{n} < x < \\frac{k}{n}$.\nSubstituting the formula for $x_{i,j}$:\n$$\n\\frac{k-1}{n} < \\frac{\\sigma_{j}(i) - u_{i,j}}{n} < \\frac{k}{n}\n$$\nMultiplying by $n$ gives:\n$$\nk-1 < \\sigma_{j}(i) - u_{i,j} < k\n$$\nSince $u_{i,j} \\in (0,1)$, we know that $0 < u_{i,j} < 1$. This implies:\n$$\n\\sigma_{j}(i) - 1 < \\sigma_{j}(i) - u_{i,j} < \\sigma_{j}(i)\n$$\nBy comparing the two inequalities, we see that they are satisfied if and only if $k-1 = \\sigma_{j}(i) - 1$ and $k = \\sigma_{j}(i)$. This means $k = \\sigma_{j}(i)$.\nThis theoretical result establishes that the sample $x_{i,j}$ will always fall into the bin indexed by the permutation value $\\sigma_{j}(i)$.\nFor any given dimension $j$, the collection of permutation values for $i=1, \\dots, 5$ is $\\{\\sigma_j(1), \\sigma_j(2), \\sigma_j(3), \\sigma_j(4), \\sigma_j(5)\\}$. Since $\\sigma_j$ is a permutation of $\\{1, 2, 3, 4, 5\\}$, this set of values is simply $\\{1, 2, 3, 4, 5\\}$ in a different order. Consequently, for each dimension $j$, the five samples will fall into bins $\\{1, 2, 3, 4, 5\\}$ respectively, with each bin being occupied by exactly one sample. Therefore, the bin count $c_{j,k}$ must be $1$ for all $j \\in \\{p,k,a\\}$ and $k \\in \\{1,2,3,4,5\\}$. This directly implies that $D_{\\max}=0$.\n\nAlthough the final result is fixed by the definition of LHS, the problem demands the explicit construction of the sample matrix $X$ and the computation of the bin counts. We proceed with these calculations to verify the theoretical result.\n\n**1. Construct Sample Matrix $X$**\nThe matrix $X$ has elements $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{5}$. The columns correspond to $j \\in \\{p,k,a\\}$ and rows to $i \\in \\{1,2,3,4,5\\}$.\n\nFor $i=1$:\n$x_{1,p} = \\frac{\\sigma_p(1) - u_{1,p}}{5} = \\frac{3 - 0.37}{5} = \\frac{2.63}{5} = 0.526$\n$x_{1,k} = \\frac{\\sigma_k(1) - u_{1,k}}{5} = \\frac{5 - 0.82}{5} = \\frac{4.18}{5} = 0.836$\n$x_{1,a} = \\frac{\\sigma_a(1) - u_{1,a}}{5} = \\frac{2 - 0.15}{5} = \\frac{1.85}{5} = 0.370$\n\nFor $i=2$:\n$x_{2,p} = \\frac{\\sigma_p(2) - u_{2,p}}{5} = \\frac{1 - 0.58}{5} = \\frac{0.42}{5} = 0.084$\n$x_{2,k} = \\frac{\\sigma_k(2) - u_{2,k}}{5} = \\frac{2 - 0.29}{5} = \\frac{1.71}{5} = 0.342$\n$x_{2,a} = \\frac{\\sigma_a(2) - u_{2,a}}{5} = \\frac{3 - 0.64}{5} = \\frac{2.36}{5} = 0.472$\n\nFor $i=3$:\n$x_{3,p} = \\frac{\\sigma_p(3) - u_{3,p}}{5} = \\frac{5 - 0.09}{5} = \\frac{4.91}{5} = 0.982$\n$x_{3,k} = \\frac{\\sigma_k(3) - u_{3,k}}{5} = \\frac{4 - 0.53}{5} = \\frac{3.47}{5} = 0.694$\n$x_{3,a} = \\frac{\\sigma_a(3) - u_{3,a}}{5} = \\frac{1 - 0.41}{5} = \\frac{0.59}{5} = 0.118$\n\nFor $i=4$:\n$x_{4,p} = \\frac{\\sigma_p(4) - u_{4,p}}{5} = \\frac{2 - 0.76}{5} = \\frac{1.24}{5} = 0.248$\n$x_{4,k} = \\frac{\\sigma_k(4) - u_{4,k}}{5} = \\frac{1 - 0.12}{5} = \\frac{0.88}{5} = 0.176$\n$x_{4,a} = \\frac{\\sigma_a(4) - u_{4,a}}{5} = \\frac{5 - 0.87}{5} = \\frac{4.13}{5} = 0.826$\n\nFor $i=5$:\n$x_{5,p} = \\frac{\\sigma_p(5) - u_{5,p}}{5} = \\frac{4 - 0.24}{5} = \\frac{3.76}{5} = 0.752$\n$x_{5,k} = \\frac{\\sigma_k(5) - u_{5,k}}{5} = \\frac{3 - 0.67}{5} = \\frac{2.33}{5} = 0.466$\n$x_{5,a} = \\frac{\\sigma_a(5) - u_{5,a}}{5} = \\frac{4 - 0.33}{5} = \\frac{3.67}{5} = 0.734$\n\nThe complete sample matrix is:\n$$\nX = \\begin{pmatrix}\n0.526 & 0.836 & 0.370 \\\\\n0.084 & 0.342 & 0.472 \\\\\n0.982 & 0.694 & 0.118 \\\\\n0.248 & 0.176 & 0.826 \\\\\n0.752 & 0.466 & 0.734\n\\end{pmatrix}\n$$\n\n**2. Compute Bin Counts $c_{j,k}$**\nThe bins are $B_1=(0.0, 0.2)$, $B_2=(0.2, 0.4)$, $B_3=(0.4, 0.6)$, $B_4=(0.6, 0.8)$, $B_5=(0.8, 1.0)$.\n\nDimension $p$: $\\{0.526, 0.084, 0.982, 0.248, 0.752\\}$\n- $x_{1,p}=0.526 \\in B_3$ ($\\sigma_p(1)=3$)\n- $x_{2,p}=0.084 \\in B_1$ ($\\sigma_p(2)=1$)\n- $x_{3,p}=0.982 \\in B_5$ ($\\sigma_p(3)=5$)\n- $x_{4,p}=0.248 \\in B_2$ ($\\sigma_p(4)=2$)\n- $x_{5,p}=0.752 \\in B_4$ ($\\sigma_p(5)=4$)\nThe counts are: $c_{p,1}=1$, $c_{p,2}=1$, $c_{p,3}=1$, $c_{p,4}=1$, $c_{p,5}=1$.\n\nDimension $k$: $\\{0.836, 0.342, 0.694, 0.176, 0.466\\}$\n- $x_{1,k}=0.836 \\in B_5$ ($\\sigma_k(1)=5$)\n- $x_{2,k}=0.342 \\in B_2$ ($\\sigma_k(2)=2$)\n- $x_{3,k}=0.694 \\in B_4$ ($\\sigma_k(3)=4$)\n- $x_{4,k}=0.176 \\in B_1$ ($\\sigma_k(4)=1$)\n- $x_{5,k}=0.466 \\in B_3$ ($\\sigma_k(5)=3$)\nThe counts are: $c_{k,1}=1$, $c_{k,2}=1$, $c_{k,3}=1$, $c_{k,4}=1$, $c_{k,5}=1$.\n\nDimension $a$: $\\{0.370, 0.472, 0.118, 0.826, 0.734\\}$\n- $x_{1,a}=0.370 \\in B_2$ ($\\sigma_a(1)=2$)\n- $x_{2,a}=0.472 \\in B_3$ ($\\sigma_a(2)=3$)\n- $x_{3,a}=0.118 \\in B_1$ ($\\sigma_a(3)=1$)\n- $x_{4,a}=0.826 \\in B_5$ ($\\sigma_a(4)=5$)\n- $x_{5,a}=0.734 \\in B_4$ ($\\sigma_a(5)=4$)\nThe counts are: $c_{a,1}=1$, $c_{a,2}=1$, $c_{a,3}=1$, $c_{a,4}=1$, $c_{a,5}=1$.\n\nThe explicit calculation confirms that for all $j \\in \\{p,k,a\\}$ and $k \\in \\{1,2,3,4,5\\}$, the bin count is $c_{j,k}=1$.\n\n**3. Evaluate $D_{\\max}$**\nThe stratification deviation metric is given by:\n$$\nD_{\\max} = \\max_{j \\in \\{p,k,a\\}} \\max_{k \\in \\{1,2,3,4,5\\}} |c_{j,k} - 1|\n$$\nSince every count $c_{j,k}$ is equal to $1$, the term inside the absolute value is $|1-1|=0$ for all $j$ and $k$. The maximum of a set of zeros is zero.\n$$\nD_{\\max} = \\max_{j,k} |1 - 1| = \\max_{j,k} 0 = 0\n$$\nThe result demonstrates the perfect marginal stratification property of Latin hypercube sampling by construction.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "A key motivation for using advanced sampling techniques is to gain efficiency, reducing the number of computationally expensive model runs needed to achieve a certain goal. This practice puts that principle into action by asking you to plan a sampling campaign to estimate a model output with a target precision . By comparing the required sample sizes for simple Monte Carlo and Latin Hypercube Sampling, you will directly quantify the practical benefits of the LHS variance reduction property.",
            "id": "3897042",
            "problem": "Consider an environmental catchment model where the random vector of annual drivers $X$ comprises basin-mean precipitation $P$, potential evapotranspiration $E_{p}$, and soil hydraulic conductivity $K_{s}$. Let the annual basin-averaged runoff be $Y = g(X)$, where $g$ is a deterministic hydrological model mapping inputs to output. Your task is to estimate the expectation $\\mathbb{E}[Y]$ under the joint distribution of $X$ using sampling-based estimators.\n\nYou will consider two estimators of $\\mathbb{E}[Y]$ based on $n$ model evaluations:\n- A simple Monte Carlo (MC) estimator that draws $X_{1},\\dots,X_{n}$ independently and identically distributed from the joint law of $X$ and computes the sample mean $\\bar{Y}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} g(X_{i})$.\n- A Latin Hypercube Sampling (LHS) estimator that draws $X_{1},\\dots,X_{n}$ using stratified one-dimensional marginals with the same joint marginals as MC, and also computes a sample mean of model outputs.\n\nAssume:\n- $Y$ has finite mean $\\mu = \\mathbb{E}[Y]$ and finite variance $\\sigma^{2} = \\mathrm{Var}(Y)$.\n- For MC, the Central Limit Theorem (CLT) applies: $\\sqrt{n}\\,(\\bar{Y}_{n} - \\mu) \\Rightarrow \\mathcal{N}(0,\\sigma^{2})$ as $n \\to \\infty$.\n- For LHS, the sample mean is unbiased and admits an asymptotic normal approximation with variance not exceeding that of MC; you are given pilot-run variance estimates to plan the sample sizes.\n\nFrom first principles, derive the asymptotic $(1-\\alpha)$ confidence interval for $\\mu$ based on the normal approximation and obtain the scaling of the half-width with the sample size $n$. Then, using the derived scaling, compute the minimum integer sample sizes needed for the following design specifications:\n- Confidence level $(1-\\alpha) = 0.95$.\n- Target half-width $h^{\\star} = 3$ in the same units as $Y$.\n- Pilot estimates of output standard deviation from $n_{\\mathrm{pilot}}$ runs:\n  - MC pilot standard deviation $s_{\\mathrm{MC}} = 60$.\n  - LHS pilot standard deviation $s_{\\mathrm{LHS}} = 40$.\n\nFor planning, treat $s_{\\mathrm{MC}}$ and $s_{\\mathrm{LHS}}$ as proxies for $\\sigma$ under each sampling scheme. Use the standard normal quantile $z_{\\alpha/2}$ associated with $(1-\\alpha)=0.95$ in your calculations. Compute the minimum integer sample sizes $n_{\\mathrm{MC}}$ and $n_{\\mathrm{LHS}}$ such that the asymptotic half-width is at most $h^{\\star}$ for each scheme, and then compute the ratio $r = \\frac{n_{\\mathrm{LHS}}}{n_{\\mathrm{MC}}}$. Round your final answer for $r$ to four significant figures. No physical units are required for $r$.",
            "solution": "The goal is to determine the sample size $n$ required to achieve a target confidence interval half-width for the estimate of the mean $\\mu = \\mathbb{E}[Y]$.\n\n**Step 1: Derive the sample size formula.**\nThe Central Limit Theorem (CLT) states that for a large sample size $n$, the distribution of the sample mean $\\bar{Y}_n$ is approximately normal:\n$$\n\\bar{Y}_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\nwhere $\\mu$ is the true mean and $\\sigma^2$ is the true variance of the output $Y$.\nA $(1-\\alpha)$ confidence interval for $\\mu$ is constructed as:\n$$\n\\bar{Y}_n \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\n$$\nHere, $z_{\\alpha/2}$ is the quantile of the standard normal distribution such that $P(Z > z_{\\alpha/2}) = \\alpha/2$. The half-width of this interval is $h = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$.\nTo find the required sample size $n$ to achieve a target half-width $h^{\\star}$, we solve for $n$:\n$$\nh^{\\star} = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\implies \\sqrt{n} = \\frac{z_{\\alpha/2} \\sigma}{h^{\\star}} \\implies n = \\left(\\frac{z_{\\alpha/2} \\sigma}{h^{\\star}}\\right)^2\n$$\nIn practice, the true standard deviation $\\sigma$ is unknown and is replaced by a sample estimate $s$ from a pilot study.\n\n**Step 2: Calculate the required sample sizes for MC and LHS.**\nWe are given:\n- Confidence level: $(1-\\alpha) = 0.95$, so $\\alpha=0.05$ and $\\alpha/2 = 0.025$. The corresponding quantile is $z_{0.025} \\approx 1.96$.\n- Target half-width: $h^{\\star} = 3$.\n- Pilot standard deviation for Monte Carlo: $s_{\\mathrm{MC}} = 60$.\n- Pilot standard deviation for Latin Hypercube Sampling: $s_{\\mathrm{LHS}} = 40$.\n\nFor Monte Carlo (MC):\n$$\nn_{\\mathrm{MC}} = \\left(\\frac{z_{\\alpha/2} s_{\\mathrm{MC}}}{h^{\\star}}\\right)^2 = \\left(\\frac{1.96 \\times 60}{3}\\right)^2 = (1.96 \\times 20)^2 = (39.2)^2 = 1536.64\n$$\nSince the sample size must be an integer, the minimum required sample size is $n_{\\mathrm{MC}} = \\lceil 1536.64 \\rceil = 1537$.\n\nFor Latin Hypercube Sampling (LHS):\n$$\nn_{\\mathrm{LHS}} = \\left(\\frac{z_{\\alpha/2} s_{\\mathrm{LHS}}}{h^{\\star}}\\right)^2 = \\left(\\frac{1.96 \\times 40}{3}\\right)^2 \\approx (26.1333)^2 \\approx 682.95\n$$\nThe minimum required integer sample size is $n_{\\mathrm{LHS}} = \\lceil 682.95 \\rceil = 683$.\n\n**Step 3: Compute the ratio $r$.**\nThe problem asks for the ratio $r = \\frac{n_{\\mathrm{LHS}}}{n_{\\mathrm{MC}}}$.\nIn sample size planning, it is common to compare the *relative efficiency* of the methods, which is independent of the target half-width and confidence level. This is given by the ratio of the continuous (non-integer) sample size requirements, which simplifies to the ratio of the variances.\n$$\nr = \\frac{n_{\\mathrm{LHS}}}{n_{\\mathrm{MC}}} = \\frac{\\left(\\frac{z_{\\alpha/2} s_{\\mathrm{LHS}}}{h^{\\star}}\\right)^2}{\\left(\\frac{z_{\\alpha/2} s_{\\mathrm{MC}}}{h^{\\star}}\\right)^2} = \\left(\\frac{s_{\\mathrm{LHS}}}{s_{\\mathrm{MC}}}\\right)^2\n$$\nPlugging in the given values:\n$$\nr = \\left(\\frac{40}{60}\\right)^2 = \\left(\\frac{2}{3}\\right)^2 = \\frac{4}{9}\n$$\nConverting this fraction to a decimal gives:\n$$\nr = \\frac{4}{9} \\approx 0.444444\\ldots\n$$\nRounding to four significant figures gives $0.4444$. This result shows that LHS is more than twice as efficient as MC for this particular model, requiring only about 44.4% of the samples to achieve the same precision.",
            "answer": "$$\\boxed{0.4444}$$"
        },
        {
            "introduction": "The assumptions underlying our statistical analyses are just as important as the analyses themselves, and in practice, these assumptions are sometimes violated. This advanced exercise tackles a common issue in large-scale simulations where model outputs exhibit serial correlation, violating the assumption of independence . You will derive the correct variance for a sample mean under autocorrelation and compute the 'effective sample size', a crucial concept for accurately quantifying uncertainty from dependent model runs.",
            "id": "3897094",
            "problem": "A research team uses Monte Carlo (MC) sampling and Latin Hypercube Sampling (LHS) to propagate parametric uncertainty in an Earth system model. Let the uncertain input vector be $X$, and let the scalar quantity of interest be the model output $g(X)$ (for example, a global annual-mean radiative forcing diagnostic). The team runs the model at $N$ input settings $\\{X_{i}\\}_{i=1}^{N}$, obtained by MC or LHS. Due to shared boundary conditions and spin-up reuse in the execution schedule, the outputs $Y_{i} \\equiv g(X_{i})$ exhibit serial dependence across the run index $i$. Assume $\\{Y_{i}\\}$ is a weakly stationary sequence with mean $\\mu$, variance $\\sigma^{2}$, and autocorrelation function $\\rho_{k} = \\mathrm{Corr}(Y_{i}, Y_{i+k})$ for integer lag $k \\ge 0$.\n\nThe team wishes to estimate $\\mathbb{E}[g(X)] = \\mu$ by the sample mean $\\bar{Y}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} Y_{i}$, and to quantify its uncertainty. However, a colleague computes the variance of $\\bar{Y}_{N}$ as if the $Y_{i}$ were independent, i.e., uses $\\sigma^{2}/N$.\n\nTasks:\n1) Starting from the definitions of variance, covariance, and weak stationarity, derive an expression for $\\mathrm{Var}(\\bar{Y}_{N})$ in terms of $\\sigma^{2}$, $N$, and the autocorrelation sequence $\\{\\rho_{k}\\}$. Your derivation should explicitly display the finite-$N$ averaging weights on the autocovariances and should make no independence assumption. Use only standard properties of variance and covariance for sums of random variables and the definition of weak stationarity.\n2) Define the effective sample size (ESS), $n_{\\mathrm{eff}}$, by the identity $\\mathrm{Var}(\\bar{Y}_{N}) = \\sigma^{2}/n_{\\mathrm{eff}}$. Express $n_{\\mathrm{eff}}$ in terms of $N$ and $\\{\\rho_{k}\\}$. Explain how positive autocorrelation leads to a downward bias if one naively uses $\\sigma^{2}/N$.\n3) In a concrete study, suppose the output autocorrelation follows a first-order autoregressive pattern, $\\rho_{k} = \\rho^{k}$ with $\\rho \\in (0,1)$, induced by the queue ordering of runs, and that $\\rho = 0.3$. With $N = 100$ runs (obtained by either MC or LHS, but executed in a sequence that induces this autocorrelation), compute $n_{\\mathrm{eff}}$. Round your answer to four significant figures. Express your final answer as a pure number (no units).",
            "solution": "We begin with the definitions and properties relevant to a weakly stationary sequence $\\{Y_{i}\\}$. Weak stationarity means that $\\mathbb{E}[Y_{i}] = \\mu$ is constant, $\\mathrm{Var}(Y_{i}) = \\sigma^{2}$ is constant, and the autocovariance depends only on lag: $\\gamma_{k} \\equiv \\mathrm{Cov}(Y_{i}, Y_{i+k}) = \\sigma^{2} \\rho_{k}$ with $\\rho_{0} = 1$.\n\nDefine the sample mean\n$$\n\\bar{Y}_{N} \\equiv \\frac{1}{N} \\sum_{i=1}^{N} Y_{i}.\n$$\nUsing the bilinearity of covariance and the variance of a sum, we have\n$$\n\\mathrm{Var}(\\bar{Y}_{N}) = \\mathrm{Var}\\!\\left(\\frac{1}{N} \\sum_{i=1}^{N} Y_{i}\\right) = \\frac{1}{N^{2}} \\mathrm{Var}\\!\\left(\\sum_{i=1}^{N} Y_{i}\\right) = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(Y_{i}, Y_{j}).\n$$\nBy weak stationarity, $\\mathrm{Cov}(Y_{i}, Y_{j}) = \\gamma_{|i-j|}$. Grouping terms by lag $k = |i-j|$, there are $N$ terms with $k=0$ (the variances) and for each $k \\in \\{1,\\dots,N-1\\}$ there are $2(N-k)$ off-diagonal terms with lag $k$. Therefore,\n$$\n\\mathrm{Var}(\\bar{Y}_{N}) = \\frac{1}{N^{2}} \\left[ N \\gamma_{0} + 2 \\sum_{k=1}^{N-1} (N-k) \\gamma_{k} \\right].\n$$\nSince $\\gamma_{0} = \\sigma^{2}$ and $\\gamma_{k} = \\sigma^{2} \\rho_{k}$, factor out $\\sigma^{2}$ and divide by $N$:\n$$\n\\mathrm{Var}(\\bar{Y}_{N}) = \\frac{\\sigma^{2}}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho_{k} \\right].\n$$\nThis is the exact finite-$N$ variance of the sample mean under weak stationarity. The colleague’s independence calculation corresponds to dropping all $\\rho_{k}$ for $k \\ge 1$, i.e., using only the leading term $\\sigma^{2}/N$. When the $\\rho_{k}$ are positive, the bracketed term exceeds $1$, so the independence-based variance $\\sigma^{2}/N$ is biased downward relative to the true variance.\n\nDefine the effective sample size (ESS), $n_{\\mathrm{eff}}$, by equating the true variance of $\\bar{Y}_{N}$ under correlation to that of an independent sample of size $n_{\\mathrm{eff}}$:\n$$\n\\mathrm{Var}(\\bar{Y}_{N}) \\equiv \\frac{\\sigma^{2}}{n_{\\mathrm{eff}}}.\n$$\nComparing with the expression just derived yields\n$$\n\\frac{\\sigma^{2}}{n_{\\mathrm{eff}}} = \\frac{\\sigma^{2}}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho_{k} \\right].\n$$\nCancel $\\sigma^{2}$ and solve for $n_{\\mathrm{eff}}$:\n$$\nn_{\\mathrm{eff}} = \\frac{N}{\\,1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho_{k}\\,}.\n$$\nBecause $\\rho_{k} \\ge 0$ for positively correlated outputs, the denominator exceeds $1$, implying $n_{\\mathrm{eff}} \\le N$. Thus, using $\\sigma^{2}/N$ under positive autocorrelation underestimates $\\mathrm{Var}(\\bar{Y}_{N})$ by the variance inflation factor\n$$\n\\mathcal{V}_{N} \\equiv 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho_{k}.\n$$\n\nWe now apply this to the autoregressive-of-order-one pattern $\\rho_{k} = \\rho^{k}$ with $\\rho \\in (0,1)$, here $\\rho = 0.3$, and $N = 100$. Compute\n$$\n\\mathcal{V}_{N} = 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho^{k} = 1 + 2 \\left[ \\sum_{k=1}^{N-1} \\rho^{k} - \\frac{1}{N} \\sum_{k=1}^{N-1} k \\rho^{k} \\right].\n$$\nThe two finite sums have closed forms. For $m = N-1$,\n$$\n\\sum_{k=1}^{m} \\rho^{k} = \\frac{\\rho \\left(1 - \\rho^{m}\\right)}{1 - \\rho}, \\quad \\sum_{k=1}^{m} k \\rho^{k} = \\frac{\\rho \\left(1 - (m+1) \\rho^{m} + m \\rho^{m+1}\\right)}{(1 - \\rho)^{2}}.\n$$\nSubstituting $m = N-1$ and simplifying,\n$$\n\\mathcal{V}_{N} = 1 + 2 \\left[ \\frac{\\rho \\left(1 - \\rho^{N-1}\\right)}{1 - \\rho} - \\frac{1}{N} \\cdot \\frac{\\rho \\left(1 - N \\rho^{N-1} + (N-1) \\rho^{N}\\right)}{(1 - \\rho)^{2}} \\right].\n$$\nWith $N = 100$ and $\\rho = 0.3$, note that $\\rho^{N-1} = 0.3^{99}$ and $\\rho^{N} = 0.3^{100}$ are astronomically small and negligible at any practical precision. Using exact rational values for the leading terms, $\\rho = \\frac{3}{10}$ and $1 - \\rho = \\frac{7}{10}$, gives\n$$\n\\sum_{k=1}^{N-1} \\rho^{k} \\approx \\frac{\\rho}{1 - \\rho} = \\frac{3/10}{7/10} = \\frac{3}{7}, \\quad \\sum_{k=1}^{N-1} k \\rho^{k} \\approx \\frac{\\rho}{(1 - \\rho)^{2}} = \\frac{3/10}{(7/10)^{2}} = \\frac{30}{49}.\n$$\nHence\n$$\n\\mathcal{V}_{N} \\approx 1 + 2 \\left[ \\frac{3}{7} - \\frac{1}{100} \\cdot \\frac{30}{49} \\right] = 1 + 2 \\left[ \\frac{3}{7} - \\frac{3}{490} \\right] = 1 + 2 \\cdot \\frac{207}{490} = 1 + \\frac{207}{245} = \\frac{452}{245}.\n$$\nTherefore,\n$$\nn_{\\mathrm{eff}} = \\frac{N}{\\mathcal{V}_{N}} \\approx \\frac{100}{452/245} = \\frac{24500}{452} \\approx 54.203539823\\ldots\n$$\nRounding to four significant figures yields $54.20$.\n\nThis calculation applies equally whether the input design is generated by Monte Carlo sampling or Latin Hypercube Sampling, because the correlation addressed here arises from the execution-induced dependence in the outputs $Y_{i} = g(X_{i})$ rather than the input-space stratification. Positive serial dependence inflates the variance of the sample mean and reduces the effective sample size relative to $N$.",
            "answer": "$$\\boxed{54.20}$$"
        }
    ]
}