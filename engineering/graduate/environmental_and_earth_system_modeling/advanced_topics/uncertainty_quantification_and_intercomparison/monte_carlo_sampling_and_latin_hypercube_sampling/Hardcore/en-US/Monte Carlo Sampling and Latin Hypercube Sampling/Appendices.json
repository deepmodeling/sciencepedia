{
    "hands_on_practices": [
        {
            "introduction": "Before applying advanced sampling methods, it is essential to grasp their fundamental construction. This first exercise demystifies Latin Hypercube Sampling (LHS) by guiding you through the manual creation of a small sample set. By working with explicit permutations and random 'jitter', you will build an LHS matrix from scratch and verify its defining property: perfect one-dimensional stratification . This foundational practice illuminates how LHS ensures a more uniform exploration of the parameter space compared to simple random sampling.",
            "id": "3897076",
            "problem": "Consider a three-parameter environmental model in which three uncertain inputs are modeled as independent standard uniform random variables on the unit interval: precipitation scaling factor $p \\sim \\mathrm{Uniform}(0,1)$, soil hydraulic conductivity scaling factor $k \\sim \\mathrm{Uniform}(0,1)$, and surface albedo scaling factor $a \\sim \\mathrm{Uniform}(0,1)$. As part of a Monte Carlo sampling (MCS) design, you are to construct a Latin hypercube sampling (LHS) of size $n=5$ in dimension $d=3$ using the canonical construction built from independent random permutations and independent uniform jitters inside equal-probability strata.\n\nUse the following independent permutations of $\\{1,2,3,4,5\\}$ for the three dimensions,\n$$\n\\sigma_{p} = (3,\\,1,\\,5,\\,2,\\,4),\\quad\n\\sigma_{k} = (5,\\,2,\\,4,\\,1,\\,3),\\quad\n\\sigma_{a} = (2,\\,3,\\,1,\\,5,\\,4),\n$$\nand the following independent jitter values $u_{i,j} \\in (0,1)$, where $i \\in \\{1,2,3,4,5\\}$ indexes samples and $j \\in \\{p,k,a\\}$ indexes dimensions,\n$$\n\\begin{aligned}\nu_{1,p}=0.37,\\quad u_{1,k}=0.82,\\quad u_{1,a}=0.15,\\\\\nu_{2,p}=0.58,\\quad u_{2,k}=0.29,\\quad u_{2,a}=0.64,\\\\\nu_{3,p}=0.09,\\quad u_{3,k}=0.53,\\quad u_{3,a}=0.41,\\\\\nu_{4,p}=0.76,\\quad u_{4,k}=0.12,\\quad u_{4,a}=0.87,\\\\\nu_{5,p}=0.24,\\quad u_{5,k}=0.67,\\quad u_{5,a}=0.33.\n\\end{aligned}\n$$\nConstruct the explicit LHS sample matrix $X \\in [0,1]^{5 \\times 3}$ using the standard LHS mapping\n$$\nx_{i,j} \\;=\\; \\frac{\\sigma_{j}(i) - u_{i,j}}{n},\n$$\nfor each sample index $i \\in \\{1,2,3,4,5\\}$ and each dimension $j \\in \\{p,k,a\\}$. Then, for each marginal dimension $j$, partition $[0,1]$ into $n=5$ equal-width bins\n$$\nB_{k} \\;=\\; \\left(\\frac{k-1}{5}, \\frac{k}{5}\\right),\\quad k=1,2,3,4,5,\n$$\nand compute the marginal bin counts $c_{j,k}$, defined as the number of samples $x_{i,j}$ that fall in bin $B_{k}$ for dimension $j$. Finally, define the stratification deviation metric\n$$\nD_{\\max} \\;=\\; \\max_{j \\in \\{p,k,a\\}} \\;\\max_{k \\in \\{1,2,3,4,5\\}} \\; \\left|\\,c_{j,k} - 1\\,\\right|.\n$$\n\nStarting from the foundational definition of Latin hypercube sampling as stratified sampling in each marginal with independent permutations and jittering, construct $X$, compute all bin counts $c_{j,k}$, and evaluate $D_{\\max}$. Express the final answer as an exact real number. No rounding is necessary, and no physical units apply to $D_{\\max}$.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Model parameters**: Three independent inputs, $p$, $k$, $a$, modeled as $\\mathrm{Uniform}(0,1)$ random variables.\n- **Sampling method**: Latin hypercube sampling (LHS).\n- **Sample size**: $n=5$.\n- **Dimension**: $d=3$.\n- **Permutations**:\n  - $\\sigma_{p} = (3, 1, 5, 2, 4)$\n  - $\\sigma_{k} = (5, 2, 4, 1, 3)$\n  - $\\sigma_{a} = (2, 3, 1, 5, 4)$\n- **Jitter values**: $u_{i,j} \\in (0,1)$, provided in a $5 \\times 3$ table.\n  - $u_{1,p}=0.37$, $u_{1,k}=0.82$, $u_{1,a}=0.15$\n  - $u_{2,p}=0.58$, $u_{2,k}=0.29$, $u_{2,a}=0.64$\n  - $u_{3,p}=0.09$, $u_{3,k}=0.53$, $u_{3,a}=0.41$\n  - $u_{4,p}=0.76$, $u_{4,k}=0.12$, $u_{4,a}=0.87$\n  - $u_{5,p}=0.24$, $u_{5,k}=0.67$, $u_{5,a}=0.33$\n- **LHS mapping formula**: $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{n}$ for $i \\in \\{1,2,3,4,5\\}$ and $j \\in \\{p,k,a\\}$.\n- **Bins**: $B_{k} = \\left(\\frac{k-1}{5}, \\frac{k}{5}\\right)$ for $k=1,2,3,4,5$.\n- **Bin counts**: $c_{j,k}$ is the number of samples $x_{i,j}$ that fall in bin $B_{k}$ for dimension $j$.\n- **Metric**: $D_{\\max} = \\max_{j \\in \\{p,k,a\\}} \\max_{k \\in \\{1,2,3,4,5\\}} |c_{j,k} - 1|$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard, albeit specific, procedure for generating a Latin hypercube sample. The concepts—uniform distributions, permutations, stratification, and jittering—are fundamental to Monte Carlo methods in computational science and statistics. The provided LHS mapping formula, $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{n}$, is a valid (though less common than $x_{i,j} = \\frac{\\sigma_{j}(i) - 1 + u_{i,j}}{n}$) construction. It guarantees the key stratification property of LHS, which is that each marginal axis is divided into $n$ equal-probability intervals, and each interval contains exactly one sample point. The problem is self-contained, with all necessary data and formulas provided. It is scientifically grounded, well-posed, objective, and does not violate any of the invalidity criteria. The problem requires explicit construction and verification of a property that is known from theory, which is a valid pedagogical exercise.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be constructed.\n\nThe core of the problem is to construct a specific Latin hypercube sample and then verify its stratification property using the given metric $D_{\\max}$. The defining characteristic of an LHS of size $n$ is that for each dimension (or parameter), the $n$ samples are placed such that there is exactly one sample in each of the $n$ equal-probability marginal strata. In this case, with inputs from $\\mathrm{Uniform}(0,1)$, these strata are the bins $B_k$.\n\nThe prescribed mapping is $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{n}$. Here $n=5$, and for each dimension $j$, $\\sigma_j$ is a permutation of $\\{1, 2, 3, 4, 5\\}$. The jitter $u_{i,j}$ is in the interval $(0,1)$. For a given sample $i$ and dimension $j$, let's determine which bin $B_k$ the value $x_{i,j}$ falls into. A value $x$ is in bin $B_k = \\left(\\frac{k-1}{n}, \\frac{k}{n}\\right)$ if $\\frac{k-1}{n}  x  \\frac{k}{n}$.\nSubstituting the formula for $x_{i,j}$:\n$$\n\\frac{k-1}{n}  \\frac{\\sigma_{j}(i) - u_{i,j}}{n}  \\frac{k}{n}\n$$\nMultiplying by $n$ gives:\n$$\nk-1  \\sigma_{j}(i) - u_{i,j}  k\n$$\nSince $u_{i,j} \\in (0,1)$, we know that $0  u_{i,j}  1$. This implies:\n$$\n\\sigma_{j}(i) - 1  \\sigma_{j}(i) - u_{i,j}  \\sigma_{j}(i)\n$$\nBy comparing the two inequalities, we see that they are satisfied if and only if $k-1 = \\sigma_{j}(i) - 1$ and $k = \\sigma_{j}(i)$. This means $k = \\sigma_{j}(i)$.\nThis theoretical result establishes that the sample $x_{i,j}$ will always fall into the bin indexed by the permutation value $\\sigma_{j}(i)$.\nFor any given dimension $j$, the collection of permutation values for $i=1, \\dots, 5$ is $\\{\\sigma_j(1), \\sigma_j(2), \\sigma_j(3), \\sigma_j(4), \\sigma_j(5)\\}$. Since $\\sigma_j$ is a permutation of $\\{1, 2, 3, 4, 5\\}$, this set of values is simply $\\{1, 2, 3, 4, 5\\}$ in a different order. Consequently, for each dimension $j$, the five samples will fall into bins $\\{1, 2, 3, 4, 5\\}$ respectively, with each bin being occupied by exactly one sample. Therefore, the bin count $c_{j,k}$ must be $1$ for all $j \\in \\{p,k,a\\}$ and $k \\in \\{1,2,3,4,5\\}$. This directly implies that $D_{\\max}=0$.\n\nAlthough the final result is fixed by the definition of LHS, the problem demands the explicit construction of the sample matrix $X$ and the computation of the bin counts. We proceed with these calculations to verify the theoretical result.\n\n**1. Construct Sample Matrix $X$**\nThe matrix $X$ has elements $x_{i,j} = \\frac{\\sigma_{j}(i) - u_{i,j}}{5}$. The columns correspond to $j \\in \\{p,k,a\\}$ and rows to $i \\in \\{1,2,3,4,5\\}$.\n\nFor $i=1$:\n$x_{1,p} = \\frac{\\sigma_p(1) - u_{1,p}}{5} = \\frac{3 - 0.37}{5} = \\frac{2.63}{5} = 0.526$\n$x_{1,k} = \\frac{\\sigma_k(1) - u_{1,k}}{5} = \\frac{5 - 0.82}{5} = \\frac{4.18}{5} = 0.836$\n$x_{1,a} = \\frac{\\sigma_a(1) - u_{1,a}}{5} = \\frac{2 - 0.15}{5} = \\frac{1.85}{5} = 0.370$\n\nFor $i=2$:\n$x_{2,p} = \\frac{\\sigma_p(2) - u_{2,p}}{5} = \\frac{1 - 0.58}{5} = \\frac{0.42}{5} = 0.084$\n$x_{2,k} = \\frac{\\sigma_k(2) - u_{2,k}}{5} = \\frac{2 - 0.29}{5} = \\frac{1.71}{5} = 0.342$\n$x_{2,a} = \\frac{\\sigma_a(2) - u_{2,a}}{5} = \\frac{3 - 0.64}{5} = \\frac{2.36}{5} = 0.472$\n\nFor $i=3$:\n$x_{3,p} = \\frac{\\sigma_p(3) - u_{3,p}}{5} = \\frac{5 - 0.09}{5} = \\frac{4.91}{5} = 0.982$\n$x_{3,k} = \\frac{\\sigma_k(3) - u_{3,k}}{5} = \\frac{4 - 0.53}{5} = \\frac{3.47}{5} = 0.694$\n$x_{3,a} = \\frac{\\sigma_a(3) - u_{3,a}}{5} = \\frac{1 - 0.41}{5} = \\frac{0.59}{5} = 0.118$\n\nFor $i=4$:\n$x_{4,p} = \\frac{\\sigma_p(4) - u_{4,p}}{5} = \\frac{2 - 0.76}{5} = \\frac{1.24}{5} = 0.248$\n$x_{4,k} = \\frac{\\sigma_k(4) - u_{4,k}}{5} = \\frac{1 - 0.12}{5} = \\frac{0.88}{5} = 0.176$\n$x_{4,a} = \\frac{\\sigma_a(4) - u_{4,a}}{5} = \\frac{5 - 0.87}{5} = \\frac{4.13}{5} = 0.826$\n\nFor $i=5$:\n$x_{5,p} = \\frac{\\sigma_p(5) - u_{5,p}}{5} = \\frac{4 - 0.24}{5} = \\frac{3.76}{5} = 0.752$\n$x_{5,k} = \\frac{\\sigma_k(5) - u_{5,k}}{5} = \\frac{3 - 0.67}{5} = \\frac{2.33}{5} = 0.466$\n$x_{5,a} = \\frac{\\sigma_a(5) - u_{5,a}}{5} = \\frac{4 - 0.33}{5} = \\frac{3.67}{5} = 0.734$\n\nThe complete sample matrix is:\n$$\nX = \\begin{pmatrix}\n0.526  0.836  0.370 \\\\\n0.084  0.342  0.472 \\\\\n0.982  0.694  0.118 \\\\\n0.248  0.176  0.826 \\\\\n0.752  0.466  0.734\n\\end{pmatrix}\n$$\n\n**2. Compute Bin Counts $c_{j,k}$**\nThe bins are $B_1=(0.0, 0.2)$, $B_2=(0.2, 0.4)$, $B_3=(0.4, 0.6)$, $B_4=(0.6, 0.8)$, $B_5=(0.8, 1.0)$.\n\nDimension $p$: $\\{0.526, 0.084, 0.982, 0.248, 0.752\\}$\n- $x_{1,p}=0.526 \\in B_3$ ($\\sigma_p(1)=3$)\n- $x_{2,p}=0.084 \\in B_1$ ($\\sigma_p(2)=1$)\n- $x_{3,p}=0.982 \\in B_5$ ($\\sigma_p(3)=5$)\n- $x_{4,p}=0.248 \\in B_2$ ($\\sigma_p(4)=2$)\n- $x_{5,p}=0.752 \\in B_4$ ($\\sigma_p(5)=4$)\nThe counts are: $c_{p,1}=1$, $c_{p,2}=1$, $c_{p,3}=1$, $c_{p,4}=1$, $c_{p,5}=1$.\n\nDimension $k$: $\\{0.836, 0.342, 0.694, 0.176, 0.466\\}$\n- $x_{1,k}=0.836 \\in B_5$ ($\\sigma_k(1)=5$)\n- $x_{2,k}=0.342 \\in B_2$ ($\\sigma_k(2)=2$)\n- $x_{3,k}=0.694 \\in B_4$ ($\\sigma_k(3)=4$)\n- $x_{4,k}=0.176 \\in B_1$ ($\\sigma_k(4)=1$)\n- $x_{5,k}=0.466 \\in B_3$ ($\\sigma_k(5)=3$)\nThe counts are: $c_{k,1}=1$, $c_{k,2}=1$, $c_{k,3}=1$, $c_{k,4}=1$, $c_{k,5}=1$.\n\nDimension $a$: $\\{0.370, 0.472, 0.118, 0.826, 0.734\\}$\n- $x_{1,a}=0.370 \\in B_2$ ($\\sigma_a(1)=2$)\n- $x_{2,a}=0.472 \\in B_3$ ($\\sigma_a(2)=3$)\n- $x_{3,a}=0.118 \\in B_1$ ($\\sigma_a(3)=1$)\n- $x_{4,a}=0.826 \\in B_5$ ($\\sigma_a(4)=5$)\n- $x_{5,a}=0.734 \\in B_4$ ($\\sigma_a(5)=4$)\nThe counts are: $c_{a,1}=1$, $c_{a,2}=1$, $c_{a,3}=1$, $c_{a,4}=1$, $c_{a,5}=1$.\n\nThe explicit calculation confirms that for all $j \\in \\{p,k,a\\}$ and $k \\in \\{1,2,3,4,5\\}$, the bin count is $c_{j,k}=1$.\n\n**3. Evaluate $D_{\\max}$**\nThe stratification deviation metric is given by:\n$$\nD_{\\max} = \\max_{j \\in \\{p,k,a\\}} \\max_{k \\in \\{1,2,3,4,5\\}} |c_{j,k} - 1|\n$$\nSince every count $c_{j,k}$ is equal to $1$, the term inside the absolute value is $|1-1|=0$ for all $j$ and $k$. The maximum of a set of zeros is zero.\n$$\nD_{\\max} = \\max_{j,k} |1 - 1| = \\max_{j,k} 0 = 0\n$$\nThe result demonstrates the perfect marginal stratification property of Latin hypercube sampling by construction.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Theoretical sampling methods are defined on a uniform hypercube, but real-world model parameters follow diverse and often complex distributions. This practice addresses the critical step of transforming a standard LHS sample into one that accurately represents physical parameters like temperature, conductivity, or albedo, which might be normally, log-normally, or otherwise distributed . You will implement the inverse transform sampling method, a cornerstone technique that bridges abstract sampling design with practical application in environmental and earth system modeling.",
            "id": "3897056",
            "problem": "An advanced graduate-level environmental and earth system modeling study requires sampling environmental parameters with prescribed marginal distributions while preserving per-dimension stratification inherent in Latin Hypercube Sampling (LHS). Latin Hypercube Sampling (LHS) is defined as a stratified sampling design in the unit hypercube $[0,1]^d$ for $N$ samples such that, for each dimension $j \\in \\{1,\\dots,d\\}$, exactly one sample falls in each of the $N$ equal-width strata $[k/N,(k+1)/N)$ for $k \\in \\{0,\\dots,N-1\\}$. The Cumulative Distribution Function (CDF) is a function $F(x)$ that maps a real-valued variable $x$ to a probability in $[0,1]$, and the Probability Density Function (PDF) is its derivative when it exists. The probability integral transform states that if $U$ is uniform on $[0,1]$, then $X = F^{-1}(U)$ has CDF $F$. The goal is to design a method that transforms an LHS in $[0,1]^d$ into physical parameter distributions via dimension-wise inverse cumulative distribution functions, and then verify the preservation of stratification in probability space.\n\nYou must implement the following, relying only on first principles and well-tested facts:\n- Generate an $N \\times d$ Latin hypercube sample in $[0,1]^d$, using independent random permutations to assign strata in each dimension and independent uniform jitter within strata.\n- For each dimension $j$, transform the uniform sample $U_{ij} \\in [0,1]$ to a physical parameter $X_{ij}$ by applying the inverse cumulative distribution function $F_j^{-1}$, where $F_j$ is the prescribed marginal CDF for dimension $j$. The transformation must be done independently per dimension.\n- Verify preservation of stratification by checking that, for each dimension $j$, when transforming back to probability space via $P_{ij} = F_j(X_{ij})$, the set $\\{P_{ij}\\}_{i=1}^N$ contains exactly one point per stratum $[k/N,(k+1)/N)$, for all $k \\in \\{0,\\dots,N-1\\}$. This check must return a boolean for each test case, where the boolean is $true$ if and only if all dimensions satisfy the one-per-stratum property.\n\nYour program must implement the method and perform verification for the following test suite of parameter values and distributions, chosen to represent typical environmental and earth system modeling marginals while ensuring scientific realism:\n\n- Test case $1$ (general case): $N=50$, $d=3$. Marginals per dimension:\n  - Dimension $1$: standard normal distribution with mean $0$ and standard deviation $1$.\n  - Dimension $2$: lognormal distribution with underlying normal mean $0$ and standard deviation $0.6$.\n  - Dimension $3$: beta distribution with shape parameters $2$ and $5$.\n\n- Test case $2$ (skewed marginal, small $N$): $N=10$, $d=1$. Marginal:\n  - Dimension $1$: gamma distribution with shape $0.5$ and scale $2$.\n\n- Test case $3$ (boundary case $N=1$): $N=1$, $d=5$. Marginals per dimension:\n  - Dimension $1$: normal distribution with mean $5$ and standard deviation $2$.\n  - Dimension $2$: lognormal distribution with underlying normal mean $0$ and standard deviation $1$.\n  - Dimension $3$: beta distribution with shape parameters $3$ and $3$.\n  - Dimension $4$: gamma distribution with shape $2$ and scale $1.5$.\n  - Dimension $5$: Weibull distribution with shape $1.5$ and scale $2$.\n\n- Test case $4$ (heterogeneous marginals, larger $N$): $N=100$, $d=4$. Marginals per dimension:\n  - Dimension $1$: truncated normal distribution with lower bound $-1$, upper bound $2$, base mean $0$, and base standard deviation $1$.\n  - Dimension $2$: beta distribution with shape parameters $0.8$ and $0.8$.\n  - Dimension $3$: Weibull distribution with shape $1.5$ and scale $2$.\n  - Dimension $4$: Generalized Extreme Value distribution with shape $0.2$, location $0$, and scale $1$.\n\nAll randomization must be reproducible by using a fixed seed. No physical units are required in the output as the verification is purely probabilistic. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false]\"), where each element corresponds to the boolean verification result for each of the four test cases in order.",
            "solution": "The problem requires the implementation and verification of a standard technique in environmental and earth system modeling: generating parameter samples that follow prescribed marginal distributions while preserving the per-dimension stratification characteristic of Latin Hypercube Sampling (LHS). This process involves three main stages: generating an LHS in the unit hypercube, transforming these uniform samples to the physical parameter space using the inverse transform method, and verifying that the stratification property is preserved in the probability domain.\n\n**1. Theoretical Foundation**\n\n**Latin Hypercube Sampling (LHS)**: LHS is a stratified sampling method for generating $N$ sample points in a $d$-dimensional space, specifically the unit hypercube $[0,1]^d$. Its defining property is that for each dimension $j \\in \\{1, \\dots, d\\}$, the marginal projection of the $N$ samples is stratified. This means that if we divide the range $[0,1]$ into $N$ non-overlapping intervals of equal width, $[k/N, (k+1)/N)$ for $k \\in \\{0, \\dots, N-1\\}$, each interval contains exactly one sample point along that dimension.\n\n**Inverse Transform Sampling**: This method is based on the probability integral transform, a fundamental result in probability theory. It states that if a random variable $U$ is uniformly distributed on $[0,1]$, then the random variable $X = F^{-1}(U)$ has a cumulative distribution function (CDF) $F(x)$. The function $F^{-1}$ is the inverse CDF, also known as the quantile function or the percent-point function (PPF).\n\n**Combined Method**: By applying the inverse transform method to an LHS sample, we can generate a set of parameter vectors with desired marginal distributions. If $U$ is an $N \\times d$ matrix representing an LHS, then the transformed matrix $X$, where each element is given by $X_{ij} = F_j^{-1}(U_{ij})$, constitutes a sample where the $j$-th column vector $\\{X_{ij}\\}_{i=1}^N$ is a sample from the distribution with CDF $F_j$. The key advantage is that this method preserves the beneficial stratification properties of the original LHS in the probability space of each marginal distribution.\n\n**2. Algorithmic Design and Verification**\n\nThe solution is structured into three computational steps followed by a verification check.\n\n**Step A: Generation of the Latin Hypercube Sample**\nAn $N \\times d$ matrix $U$ is generated, representing $N$ points in $[0,1]^d$. The algorithm proceeds as follows:\n1. For each dimension $j \\in \\{0, \\dots, d-1\\}$:\n2. A random permutation $\\pi_j$ of the integers $\\{0, \\dots, N-1\\}$ is created. This assigns each of the $N$ samples to a unique stratum index for dimension $j$.\n3. A vector of $N$ random \"jitter\" values, $\\Delta_j$, is drawn from the uniform distribution $U(0,1)$.\n4. The samples for dimension $j$ are constructed as $U_{ij} = \\frac{\\pi_j(i) + \\Delta_{ij}}{N}$ for $i \\in \\{0, \\dots, N-1\\}$.\nBy construction, for any given dimension $j$, the set $\\{U_{ij}\\}_{i=1}^N$ contains exactly one point in each probability stratum $[k/N, (k+1)/N)$.\n\n**Step B: Transformation to Physical Parameter Space**\nFor each dimension $j$, the corresponding column of the LHS matrix $U$ is transformed using the specified inverse CDF, $F_j^{-1}$. This yields the matrix of physical parameters $X$:\n$$ X_{ij} = F_j^{-1}(U_{ij}) $$\nThe functions $F_j^{-1}$ correspond to the `.ppf` methods of the distribution objects provided by the `scipy.stats` library.\n\n**Step C: Verification of Stratification Preservation**\nThe core task is to verify that the stratification is preserved in probability space after the transformation. This is accomplished by transforming the physical parameters $X_{ij}$ back to the unit probability space using their respective CDFs, $F_j$:\n$$ P_{ij} = F_j(X_{ij}) $$\nFor continuous distributions and perfect numerical precision, the identity $F_j(F_j^{-1}(u)) = u$ holds. Therefore, we expect the resulting probability matrix $P$ to be identical to the original LHS matrix $U$, i.e., $P_{ij} = U_{ij}$. The verification step computationally checks if the stratification property of the original $U$ matrix is retained in the computed $P$ matrix.\n\nThe verification check proceeds for each dimension $j$:\n1. For each point $P_{ij}$ in the $j$-th dimension, its stratum index $s_{ij}$ is calculated as $s_{ij} = \\lfloor N \\cdot P_{ij} \\rfloor$. Since the input uniform samples $U_{ij}$ are strictly less than $1$, the transformed values $P_{ij}$ are also expected to be less than $1$, so $s_{ij}$ will range from $0$ to $N-1$.\n2. It is then verified that the set of computed stratum indices $\\{s_{ij}\\}_{i=1}^N$ is a complete set of the integers from $0$ to $N-1$. A robust way to perform this check is to sort the array of indices and compare it for equality with an array containing the integers from $0$ to $N-1$.\n3. A test case is considered valid (returns `true`) if and only if this property holds for all $d$ dimensions. The high-quality numerical routines in `scipy` are expected to preserve this identity with sufficient precision, so the verification should pass for all test cases. The case where $N=1$ is a trivial boundary condition; with one stratum $[0,1)$, the single sample must fall within it, guaranteeing a `true` result.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, lognorm, beta, gamma, weibull_min, truncnorm, genextreme\n\ndef generate_lhs(n, d, rng):\n    \"\"\"\n    Generates a Latin Hypercube Sample of size n x d.\n\n    Args:\n        n (int): The number of samples.\n        d (int): The number of dimensions (parameters).\n        rng (numpy.random.Generator): The random number generator for reproducibility.\n\n    Returns:\n        numpy.ndarray: An n x d array representing the LHS in [0,1]^d.\n    \"\"\"\n    lhs_samples = np.zeros((n, d))\n    for j in range(d):\n        permutation = rng.permutation(n)\n        jitter = rng.uniform(0, 1, size=n)\n        lhs_samples[:, j] = (permutation + jitter) / n\n    return lhs_samples\n\ndef verify_stratification_for_case(n, d, distributions, rng):\n    \"\"\"\n    Generates an LHS, transforms it to physical parameters, transforms it back to\n    probability space, and verifies if the stratification is preserved.\n\n    Args:\n        n (int): The number of samples.\n        d (int): The number of dimensions.\n        distributions (list): A list of scipy.stats frozen distribution objects.\n        rng (numpy.random.Generator): The random number generator.\n\n    Returns:\n        bool: True if stratification is preserved for all dimensions, False otherwise.\n    \"\"\"\n    if n == 0:\n        return True # Vacuously true\n\n    # Step 1: Generate an N x d Latin hypercube sample in [0,1]^d\n    u_samples = generate_lhs(n, d, rng)\n\n    # Step 2: Transform to physical parameters using inverse CDFs (ppf)\n    x_samples = np.zeros_like(u_samples)\n    for j in range(d):\n        x_samples[:, j] = distributions[j].ppf(u_samples[:, j])\n\n    # Step 3: Transform back to probability space using CDFs\n    p_samples = np.zeros_like(x_samples)\n    for j in range(d):\n        p_samples[:, j] = distributions[j].cdf(x_samples[:, j])\n\n    # Step 4: Verify that for each dimension, there is exactly one sample per stratum\n    for j in range(d):\n        p_column = p_samples[:, j]\n        \n        # Calculate the stratum index for each point in the current dimension.\n        # The strata are [k/n, (k+1)/n), so the index is floor(p * n).\n        stratum_indices = np.floor(p_column * n).astype(int)\n\n        # Handle the edge case where a value due to floating point arithmetic\n        # becomes exactly 1.0, which would map to stratum index n.\n        # This point belongs to the last stratum, n-1.\n        stratum_indices[stratum_indices == n] = n - 1\n\n        # The set of stratum indices must be a permutation of {0, 1, ..., n-1}.\n        # A robust check is to sort the indices and compare to a reference range.\n        if not np.array_equal(np.sort(stratum_indices), np.arange(n)):\n            return False  # Stratification failed for this dimension\n\n    return True  # Stratification holds for all dimensions\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases.\n    \"\"\"\n    # Fixed seed for reproducibility of the entire run\n    rng = np.random.default_rng(seed=42)\n\n    # Define the test suite\n    test_cases = [\n        # Test case 1: general case\n        {\n            \"n\": 50, \"d\": 3,\n            \"dists\": [\n                norm(loc=0, scale=1),\n                lognorm(s=0.6, scale=np.exp(0)),\n                beta(a=2, b=5)\n            ]\n        },\n        # Test case 2: skewed marginal, small N\n        {\n            \"n\": 10, \"d\": 1,\n            \"dists\": [\n                gamma(a=0.5, scale=2)\n            ]\n        },\n        # Test case 3: boundary case N=1\n        {\n            \"n\": 1, \"d\": 5,\n            \"dists\": [\n                norm(loc=5, scale=2),\n                lognorm(s=1, scale=np.exp(0)),\n                beta(a=3, b=3),\n                gamma(a=2, scale=1.5),\n                weibull_min(c=1.5, scale=2)\n            ]\n        },\n        # Test case 4: heterogeneous marginals, larger N\n        {\n            \"n\": 100, \"d\": 4,\n            \"dists\": [\n                truncnorm(a=(-1-0)/1, b=(2-0)/1, loc=0, scale=1),\n                beta(a=0.8, b=0.8),\n                weibull_min(c=1.5, scale=2),\n                genextreme(c=0.2, loc=0, scale=1)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        is_valid = verify_stratification_for_case(\n            case[\"n\"], case[\"d\"], case[\"dists\"], rng\n        )\n        results.append(str(is_valid).lower())\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A crucial question in any simulation-based study is determining an adequate sample size to achieve a desired level of precision without wasting computational resources. This final exercise provides a framework for planning your sampling campaign by estimating the number of runs needed to constrain the uncertainty in a model's output to a target level . By comparing the sample size requirements for simple Monte Carlo (MC) versus Latin Hypercube Sampling (LHS), you will gain a quantitative appreciation for the variance reduction and enhanced efficiency that LHS offers.",
            "id": "3897042",
            "problem": "Consider an environmental catchment model where the random vector of annual drivers $X$ comprises basin-mean precipitation $P$, potential evapotranspiration $E_{p}$, and soil hydraulic conductivity $K_{s}$. Let the annual basin-averaged runoff be $Y = g(X)$, where $g$ is a deterministic hydrological model mapping inputs to output. Your task is to estimate the expectation $\\mathbb{E}[Y]$ under the joint distribution of $X$ using sampling-based estimators.\n\nYou will consider two estimators of $\\mathbb{E}[Y]$ based on $n$ model evaluations:\n- A simple Monte Carlo (MC) estimator that draws $X_{1},\\dots,X_{n}$ independently and identically distributed from the joint law of $X$ and computes the sample mean $\\bar{Y}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} g(X_{i})$.\n- A Latin Hypercube Sampling (LHS) estimator that draws $X_{1},\\dots,X_{n}$ using stratified one-dimensional marginals with the same joint marginals as MC, and also computes a sample mean of model outputs.\n\nAssume:\n- $Y$ has finite mean $\\mu = \\mathbb{E}[Y]$ and finite variance $\\sigma^{2} = \\mathrm{Var}(Y)$.\n- For MC, the Central Limit Theorem (CLT) applies: $\\sqrt{n}\\,(\\bar{Y}_{n} - \\mu) \\xrightarrow{d} \\mathcal{N}(0,\\sigma^{2})$ as $n \\to \\infty$.\n- For LHS, the sample mean is unbiased and admits an asymptotic normal approximation with variance not exceeding that of MC; you are given pilot-run variance estimates to plan the sample sizes.\n\nFrom first principles, derive the asymptotic $(1-\\alpha)$ confidence interval for $\\mu$ based on the normal approximation and obtain the scaling of the half-width with the sample size $n$. Then, using the derived scaling, compute the minimum integer sample sizes needed for the following design specifications:\n- Confidence level $(1-\\alpha) = 0.95$.\n- Target half-width $h^{\\star} = 3$ in the same units as $Y$.\n- Pilot estimates of output standard deviation from $n_{\\mathrm{pilot}}$ runs:\n  - MC pilot standard deviation $s_{\\mathrm{MC}} = 60$.\n  - LHS pilot standard deviation $s_{\\mathrm{LHS}} = 40$.\n\nFor planning, treat $s_{\\mathrm{MC}}$ and $s_{\\mathrm{LHS}}$ as proxies for $\\sigma$ under each sampling scheme. Use the standard normal quantile $z_{\\alpha/2}$ associated with $(1-\\alpha)=0.95$ in your calculations. Compute the minimum integer sample sizes $n_{\\mathrm{MC}}$ and $n_{\\mathrm{LHS}}$ such that the asymptotic half-width is at most $h^{\\star}$ for each scheme, and then compute the ratio $r = \\frac{n_{\\mathrm{LHS}}}{n_{\\mathrm{MC}}}$. Round your final answer for $r$ to four significant figures. No physical units are required for $r$.",
            "solution": "This problem requires us to determine the minimum sample sizes for Monte Carlo (MC) and Latin Hypercube Sampling (LHS) estimators to achieve a specified precision for the estimate of a mean. We then compute the ratio of these sample sizes.\n\n### 1. Derivation of Sample Size Formula\n\nThe Central Limit Theorem (CLT) states that for a sample mean $\\bar{Y}_n$ from a population with mean $\\mu$ and variance $\\sigma^2$, the following holds for large $n$:\n$$ \\frac{\\bar{Y}_n - \\mu}{\\sigma/\\sqrt{n}} \\approx \\mathcal{N}(0,1) $$\nAn approximate $(1-\\alpha)$ confidence interval for $\\mu$ is constructed as:\n$$ \\bar{Y}_n \\pm z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} $$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution (e.g., for a 95% confidence level, $\\alpha=0.05$ and $z_{0.975} \\approx 1.96$).\n\nThe half-width of this confidence interval is:\n$$ h = z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} $$\nThis shows that the half-width scales with the sample size as $O(n^{-1/2})$.\n\nTo find the required sample size $n$ to achieve a target half-width $h^{\\star}$, we set $h \\le h^{\\star}$ and solve for $n$:\n$$ z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\le h^{\\star} $$\nRearranging the inequality gives:\n$$ \\sqrt{n} \\ge z_{1-\\alpha/2} \\frac{\\sigma}{h^{\\star}} $$\n$$ n \\ge \\left( \\frac{z_{1-\\alpha/2} \\sigma}{h^{\\star}} \\right)^2 $$\nSince the sample size $n$ must be an integer, the minimum required sample size is the smallest integer satisfying this condition, which is obtained by taking the ceiling of the right-hand side.\n\n### 2. Calculation of Sample Sizes\n\nWe are given the following design specifications:\n- Confidence level: $1-\\alpha = 0.95$, which implies $\\alpha = 0.05$. The corresponding quantile is $z_{1-\\alpha/2} = z_{0.975} \\approx 1.96$.\n- Target half-width: $h^{\\star} = 3$.\n- Pilot standard deviations: $\\sigma_{\\mathrm{MC}} \\approx s_{\\mathrm{MC}} = 60$ and $\\sigma_{\\mathrm{LHS}} \\approx s_{\\mathrm{LHS}} = 40$.\n\n**For the Monte Carlo (MC) estimator:**\nUsing the derived formula with $\\sigma = s_{\\mathrm{MC}}$:\n$$ n_{\\mathrm{MC}} \\ge \\left( \\frac{1.96 \\times 60}{3} \\right)^2 $$\n$$ n_{\\mathrm{MC}} \\ge (1.96 \\times 20)^2 = (39.2)^2 = 1536.64 $$\nThe minimum integer sample size is the ceiling of this value:\n$$ n_{\\mathrm{MC}} = \\lceil 1536.64 \\rceil = 1537 $$\n\n**For the Latin Hypercube Sampling (LHS) estimator:**\nUsing the derived formula with $\\sigma = s_{\\mathrm{LHS}}$:\n$$ n_{\\mathrm{LHS}} \\ge \\left( \\frac{1.96 \\times 40}{3} \\right)^2 $$\n$$ n_{\\mathrm{LHS}} \\ge \\left( \\frac{78.4}{3} \\right)^2 \\approx (26.1333...)^2 \\approx 682.9511... $$\nThe minimum integer sample size is the ceiling of this value:\n$$ n_{\\mathrm{LHS}} = \\lceil 682.9511... \\rceil = 683 $$\n\nThe results show that due to its lower variance (variance reduction), LHS requires significantly fewer samples than MC to achieve the same level of precision.\n\n### 3. Calculation of the Ratio\n\nFinally, we compute the ratio $r$ of the required sample sizes:\n$$ r = \\frac{n_{\\mathrm{LHS}}}{n_{\\mathrm{MC}}} = \\frac{683}{1537} \\approx 0.44437215... $$\nRounding to four significant figures, we get:\n$$ r \\approx 0.4444 $$",
            "answer": "$$\\boxed{0.4444}$$"
        }
    ]
}