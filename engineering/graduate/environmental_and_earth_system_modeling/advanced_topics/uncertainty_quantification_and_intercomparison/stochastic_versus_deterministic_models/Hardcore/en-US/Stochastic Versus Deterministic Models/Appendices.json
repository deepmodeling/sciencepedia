{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation, we begin by analyzing the dynamics of the simplest yet most fundamental continuous-time stochastic model: the Ornstein-Uhlenbeck process. This exercise asks you to derive the time evolution of the system's mean and variance directly from its governing Stochastic Differential Equation (SDE). By comparing the evolution of the stochastic system's mean to the trajectory of its deterministic counterpart, you will gain a first-principles understanding of how random forcing generates and propagates uncertainty .",
            "id": "3916822",
            "problem": "In a one-box linearized model of a spatially averaged environmental tracer anomaly subjected to unresolved stochastic fluxes, the anomaly $X_t$ is modeled by the linear Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_t = a\\,X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\n$$\nwhere $a$ and $\\sigma$ are real constants with $a \\neq 0$ and $\\sigma  0$, and $W_t$ is a standard Brownian motion (Wiener process). The initial condition $X_0$ is independent of $W_t$ and has mean $\\mathbb{E}[X_0] = m_0$ and variance $\\mathrm{Var}(X_0) = v_0$, with $v_0 \\ge 0$. Interpretively, $a$ captures linearized relaxation or growth, and $\\sigma\\,\\mathrm{d}W_t$ represents unresolved stochastic forcing.\n\nStarting from first principles for SDEs, including Itô’s formula and the definition of variance $\\mathrm{Var}(X_t) = \\mathbb{E}[X_t^2] - (\\mathbb{E}[X_t])^2$, perform the following:\n\n- Derive closed ordinary differential equations (ODEs) for the mean $m(t) = \\mathbb{E}[X_t]$ and for the second moment $q(t) = \\mathbb{E}[X_t^2]$, and from these obtain the variance $s(t) = \\mathrm{Var}(X_t)$, with initial conditions $m(0) = m_0$ and $s(0) = v_0$.\n\n- For the corresponding deterministic model $\\dot{x}(t) = a\\,x(t)$ with initial condition $x(0) = m_0$, define the deterministic solution $x_{\\mathrm{det}}(t) = m_0 \\exp(a t)$.\n\n- At a fixed time horizon $T  0$, compute the mean-square deviation between the stochastic and deterministic solutions,\n$$\n\\mathbb{E}\\!\\left[\\big(X_T - x_{\\mathrm{det}}(T)\\big)^2\\right],\n$$\nand express your result in closed form in terms of $a$, $\\sigma$, $v_0$, and $T$.\n\nExpress the final answer as an exact analytic expression. No rounding is required. Do not include units in your final expression.",
            "solution": "The user has requested a solution to a problem involving a linear stochastic differential equation. The first step is to validate the problem statement.\n\n### Step 1: Extract Givens\nThe givens are extracted verbatim from the problem statement:\n- The stochastic differential equation (SDE) for the anomaly $X_t$:\n$$\n\\mathrm{d}X_t = a\\,X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t\n$$\n- The constants $a$ and $\\sigma$ are real, with $a \\neq 0$ and $\\sigma  0$.\n- $W_t$ is a standard Brownian motion (Wiener process).\n- The initial condition $X_0$ is independent of $W_t$.\n- The initial mean is $\\mathbb{E}[X_0] = m_0$.\n- The initial variance is $\\mathrm{Var}(X_0) = v_0$, with $v_0 \\ge 0$.\n- The corresponding deterministic model is $\\dot{x}(t) = a\\,x(t)$ with initial condition $x(0) = m_0$.\n- The solution to the deterministic model is $x_{\\mathrm{det}}(t) = m_0 \\exp(a t)$.\n- The target quantity is the mean-square deviation at a fixed time $T  0$:\n$$\n\\mathbb{E}\\!\\left[\\big(X_T - x_{\\mathrm{det}}(T)\\big)^2\\right]\n$$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity based on the provided givens.\n- **Scientifically Grounded:** The problem describes the Ornstein-Uhlenbeck process, a canonical and extensively studied model in stochastic calculus with wide applications in physics, finance, and environmental science. It is scientifically and mathematically sound.\n- **Well-Posed:** The problem is clearly defined. It requests the derivation of ODEs for the moments of the process and the computation of a specific expected value. All necessary parameters ($a$, $\\sigma$, $m_0$, $v_0$, $T$) are provided, making the problem self-contained and admitting a unique solution.\n- **Objective:** The problem is stated using formal mathematical language, free from ambiguity or subjective content.\n\nThe problem does not exhibit any of the flaws listed in the problem validation section, such as scientific unsoundness, missing information, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution process will now proceed.\n\n### Derivation of the Mean $m(t)$\nLet $m(t) = \\mathbb{E}[X_t]$. We take the expectation of the SDE:\n$$\n\\mathbb{E}[\\mathrm{d}X_t] = \\mathbb{E}[a\\,X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t]\n$$\nUsing the linearity of the expectation operator and the fact that expectation commutes with the differential operator, we get:\n$$\n\\mathrm{d}\\mathbb{E}[X_t] = a\\,\\mathbb{E}[X_t]\\,\\mathrm{d}t + \\sigma\\,\\mathbb{E}[\\mathrm{d}W_t]\n$$\nA fundamental property of the Wiener process is that the expected value of its increment is zero, i.e., $\\mathbb{E}[\\mathrm{d}W_t] = 0$. Substituting the definition of $m(t)$ yields the ordinary differential equation (ODE) for the mean:\n$$\n\\mathrm{d}m(t) = a\\,m(t)\\,\\mathrm{d}t \\implies \\frac{\\mathrm{d}m}{\\mathrm{d}t} = a\\,m(t)\n$$\nThe initial condition is $m(0) = \\mathbb{E}[X_0] = m_0$. The solution to this linear ODE is:\n$$\nm(t) = m_0 \\exp(at)\n$$\n\n### Derivation of the Second Moment $q(t)$\nLet $q(t) = \\mathbb{E}[X_t^2]$. To find the dynamics of $X_t^2$, we apply Itô's formula to the function $f(X_t) = X_t^2$. The derivatives are $f'(x) = 2x$ and $f''(x) = 2$. Itô's formula is:\n$$\n\\mathrm{d}f(X_t) = f'(X_t)\\,\\mathrm{d}X_t + \\frac{1}{2} f''(X_t) (\\mathrm{d}X_t)^2\n$$\nSubstituting $f(X_t)=X_t^2$ and the SDE for $\\mathrm{d}X_t$:\n$$\n\\mathrm{d}(X_t^2) = (2X_t)\\,(a\\,X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t) + \\frac{1}{2}\\,(2)\\, (a\\,X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t)^2\n$$\nWe expand the quadratic term $(\\mathrm{d}X_t)^2$ using the Itô multiplication rules: $(\\mathrm{d}t)^2 = 0$, $\\mathrm{d}t\\,\\mathrm{d}W_t = 0$, and $(\\mathrm{d}W_t)^2 = \\mathrm{d}t$.\n$$\n(a\\,X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t)^2 = a^2 X_t^2 (\\mathrm{d}t)^2 + 2a\\sigma X_t \\,\\mathrm{d}t\\,\\mathrm{d}W_t + \\sigma^2 (\\mathrm{d}W_t)^2 = \\sigma^2 \\mathrm{d}t\n$$\nSubstituting this back into the expression for $\\mathrm{d}(X_t^2)$:\n$$\n\\mathrm{d}(X_t^2) = 2a X_t^2 \\,\\mathrm{d}t + 2\\sigma X_t \\,\\mathrm{d}W_t + \\sigma^2 \\mathrm{d}t = (2a X_t^2 + \\sigma^2)\\,\\mathrm{d}t + 2\\sigma X_t \\,\\mathrm{d}W_t\n$$\nNow, we take the expectation of this expression:\n$$\n\\mathrm{d}\\mathbb{E}[X_t^2] = \\mathbb{E}[(2a X_t^2 + \\sigma^2)\\,\\mathrm{d}t] + \\mathbb{E}[2\\sigma X_t \\,\\mathrm{d}W_t]\n$$\nThis simplifies to:\n$$\n\\mathrm{d}q(t) = (2a\\,\\mathbb{E}[X_t^2] + \\sigma^2)\\,\\mathrm{d}t + 2\\sigma\\,\\mathbb{E}[X_t\\,\\mathrm{d}W_t]\n$$\nThe term $\\mathbb{E}[X_t\\,\\mathrm{d}W_t]$ is zero because $X_t$ is adapted to the filtration at time $t$ (it is non-anticipating) and $\\mathrm{d}W_t$ is a future increment. Thus, we obtain the ODE for the second moment:\n$$\n\\frac{\\mathrm{d}q}{\\mathrm{d}t} = 2a\\,q(t) + \\sigma^2\n$$\nThe initial condition is $q(0) = \\mathbb{E}[X_0^2]$. From the definition of variance, $\\mathrm{Var}(X_0) = \\mathbb{E}[X_0^2] - (\\mathbb{E}[X_0])^2$, we have $v_0 = q(0) - m_0^2$. Therefore, $q(0) = v_0 + m_0^2$.\nThis is a linear first-order non-homogeneous ODE. Its solution is:\n$$\nq(t) = \\left(q(0) + \\frac{\\sigma^2}{2a}\\right) \\exp(2at) - \\frac{\\sigma^2}{2a}\n$$\nSubstituting $q(0) = v_0 + m_0^2$:\n$$\nq(t) = \\left(v_0 + m_0^2 + \\frac{\\sigma^2}{2a}\\right) \\exp(2at) - \\frac{\\sigma^2}{2a}\n$$\nThis can be rewritten as:\n$$\nq(t) = (v_0 + m_0^2)\\exp(2at) + \\frac{\\sigma^2}{2a}(\\exp(2at) - 1)\n$$\n\n### Derivation of the Variance $s(t)$\nThe variance $s(t) = \\mathrm{Var}(X_t)$ is given by $s(t) = q(t) - m(t)^2$.\n$$\ns(t) = \\left[ (v_0 + m_0^2)\\exp(2at) + \\frac{\\sigma^2}{2a}(\\exp(2at) - 1) \\right] - \\left[ m_0 \\exp(at) \\right]^2\n$$\n$$\ns(t) = (v_0 + m_0^2)\\exp(2at) + \\frac{\\sigma^2}{2a}(\\exp(2at) - 1) - m_0^2 \\exp(2a t)\n$$\nThe terms involving $m_0^2$ cancel out:\n$$\ns(t) = v_0 \\exp(2at) + \\frac{\\sigma^2}{2a}(\\exp(2at) - 1)\n$$\nThe initial condition is $s(0) = v_0 \\exp(0) + \\frac{\\sigma^2}{2a}(\\exp(0) - 1) = v_0$, which is consistent with the problem statement.\n\n### Computation of the Mean-Square Deviation\nThe quantity to compute is the mean-square deviation at time $T  0$:\n$$\n\\mathbb{E}\\!\\left[\\big(X_T - x_{\\mathrm{det}}(T)\\big)^2\\right]\n$$\nThe deterministic solution is given as $x_{\\mathrm{det}}(t) = m_0 \\exp(at)$. We note that this is identical to the mean of the stochastic process, $m(t) = \\mathbb{E}[X_t]$. Therefore, $x_{\\mathrm{det}}(T) = m(T) = \\mathbb{E}[X_T]$.\nSubstituting this into the expression for the mean-square deviation:\n$$\n\\mathbb{E}\\!\\left[\\big(X_T - \\mathbb{E}[X_T]\\big)^2\\right]\n$$\nThis expression is the definition of the variance of the random variable $X_T$.\n$$\n\\mathbb{E}\\!\\left[\\big(X_T - x_{\\mathrm{det}}(T)\\big)^2\\right] = \\mathrm{Var}(X_T) = s(T)\n$$\nUsing the previously derived expression for $s(t)$ and evaluating it at $t=T$:\n$$\ns(T) = v_0 \\exp(2aT) + \\frac{\\sigma^2}{2a}(\\exp(2aT) - 1)\n$$\nThis is the final closed-form expression for the mean-square deviation.",
            "answer": "$$\n\\boxed{v_0 \\exp(2aT) + \\frac{\\sigma^2}{2a}\\left(\\exp(2aT) - 1\\right)}\n$$"
        },
        {
            "introduction": "Having explored how the moments of a stochastic system evolve over time, a natural next question is whether the system settles into a statistical equilibrium. This practice introduces the Fokker-Planck equation, the master equation governing the evolution of the system's probability density function. By solving for the time-independent (stationary) solution in the case of the Ornstein-Uhlenbeck process, you will see how a balance between deterministic restoring forces and stochastic forcing gives rise to a stable, non-trivial probability distribution .",
            "id": "3916813",
            "problem": "Consider a one-dimensional Itō diffusion for a scalar environmental state variable $X_t$ evolving under a stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = f(X_t)\\,\\mathrm{d}t + g(X_t)\\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Wiener process, $f(\\cdot)$ and $g(\\cdot)$ are sufficiently smooth functions, and the law of $X_t$ admits a probability density function $p(x,t)$ that decays sufficiently rapidly as $|x|\\to\\infty$. \n\n(a) Starting from conservation of probability and the Itō formula applied to smooth test functions, derive the evolution equation governing $p(x,t)$ (also known as the Kolmogorov forward or Fokker–Planck equation) and express it in divergence form. Then define a time-independent stationary density $p_s(x)$ and derive the ordinary differential equation and associated probability flux condition that $p_s(x)$ must satisfy on the real line under the assumption of normalizability and vanishing flux at infinity.\n\n(b) Specialize to the Ornstein–Uhlenbeck case with linear restoring drift and additive noise, $f(x)=-\\lambda x$ and $g(x)=\\sigma$, where $\\lambda0$ and $\\sigma0$ are constants. Solve explicitly for the normalized stationary density $p_s(x)$ on the real line. Your final answer must be a single closed-form analytic expression for $p_s(x)$ as a function of $x$. No numerical approximation is required. Do not include units.\n\nExplain each step from first principles. The final boxed answer must be the explicit analytical expression for $p_s(x)$ only.",
            "solution": "The problem is valid as it is scientifically grounded in the established theory of stochastic processes, well-posed with sufficient and consistent conditions, and stated objectively. We shall proceed with the derivation as requested.\n\n(a) Derivation of the Fokker–Planck Equation and Stationary Condition\n\nLet $X_t$ be a stochastic process governed by the Itō diffusion\n$$\n\\mathrm{d}X_t = f(X_t)\\,\\mathrm{d}t + g(X_t)\\,\\mathrm{d}W_t\n$$\nwhere $W_t$ is a standard Wiener process. The law of $X_t$ is described by a probability density function (PDF) $p(x,t)$. To derive the evolution equation for $p(x,t)$, we consider an arbitrary, twice continuously differentiable test function $\\phi(x)$ with compact support (or one that vanishes sufficiently rapidly at infinity along with its derivatives).\n\nThe expectation of $\\phi(X_t)$ is given by\n$$\n\\mathbb{E}[\\phi(X_t)] = \\int_{-\\infty}^{\\infty} \\phi(x) p(x,t) \\,\\mathrm{d}x\n$$\nThe time evolution of this expectation is\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}[\\phi(X_t)] = \\int_{-\\infty}^{\\infty} \\phi(x) \\frac{\\partial p(x,t)}{\\partial t} \\,\\mathrm{d}x\n$$\nThis is based on the assumption that we can differentiate under the integral sign.\n\nAlternatively, we can compute the differential of $\\phi(X_t)$ using Itō's formula. For a function $\\phi(x)$ applied to the Itō process $X_t$, the differential is\n$$\n\\mathrm{d}\\phi(X_t) = \\phi'(X_t)\\,\\mathrm{d}X_t + \\frac{1}{2}\\phi''(X_t)\\,(\\mathrm{d}X_t)^2\n$$\nUsing the Itō multiplication rules $\\mathrm{d}t \\cdot \\mathrm{d}t = 0$, $\\mathrm{d}t \\cdot \\mathrm{d}W_t = 0$, and $\\mathrm{d}W_t \\cdot \\mathrm{d}W_t = \\mathrm{d}t$, we find $(\\mathrm{d}X_t)^2 = (g(X_t)\\,\\mathrm{d}W_t)^2 = g(X_t)^2\\,\\mathrm{d}t$. Substituting this and the expression for $\\mathrm{d}X_t$ gives\n$$\n\\mathrm{d}\\phi(X_t) = \\phi'(X_t) \\left( f(X_t)\\,\\mathrm{d}t + g(X_t)\\,\\mathrm{d}W_t \\right) + \\frac{1}{2}\\phi''(X_t) g(X_t)^2\\,\\mathrm{d}t\n$$\nRearranging terms yields\n$$\n\\mathrm{d}\\phi(X_t) = \\left( f(X_t)\\phi'(X_t) + \\frac{1}{2}g(X_t)^2\\phi''(X_t) \\right)\\mathrm{d}t + g(X_t)\\phi'(X_t)\\,\\mathrm{d}W_t\n$$\nTaking the expectation of this differential, we use the property that the expectation of an Itō integral is zero, i.e., $\\mathbb{E}\\left[\\int_0^t g(X_s)\\phi'(X_s)\\,\\mathrm{d}W_s\\right] = 0$. Thus,\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}[\\phi(X_t)] = \\mathbb{E}\\left[ f(X_t)\\phi'(X_t) + \\frac{1}{2}g(X_t)^2\\phi''(X_t) \\right]\n$$\nExpressing the expectation on the right-hand side as an integral over the PDF $p(x,t)$, we have\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}[\\phi(X_t)] = \\int_{-\\infty}^{\\infty} \\left( f(x)\\phi'(x) + \\frac{1}{2}g(x)^2\\phi''(x) \\right) p(x,t) \\,\\mathrm{d}x\n$$\nEquating our two expressions for $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}[\\phi(X_t)]$ results in\n$$\n\\int_{-\\infty}^{\\infty} \\phi(x) \\frac{\\partial p}{\\partial t} \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\left( f(x)p(x,t)\\phi'(x) + \\frac{1}{2}g(x)^2p(x,t)\\phi''(x) \\right) \\,\\mathrm{d}x\n$$\nWe now use integration by parts on the right-hand side to transfer the derivatives from $\\phi$ to the other terms. The boundary terms vanish due to the assumptions on $\\phi(x)$ and $p(x,t)$ at infinity.\nFor the first term:\n$$\n\\int_{-\\infty}^{\\infty} f(x)p(x,t)\\phi'(x) \\,\\mathrm{d}x = \\left[f(x)p(x,t)\\phi(x)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x}(f(x)p(x,t)) \\phi(x) \\,\\mathrm{d}x = - \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x}(f(x)p(x,t)) \\phi(x) \\,\\mathrm{d}x\n$$\nFor the second term, we integrate by parts twice:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{1}{2}g(x)^2p(x,t)\\phi''(x) \\,\\mathrm{d}x = \\left[\\frac{1}{2}g(x)^2p(x,t)\\phi'(x)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x}\\left(\\frac{1}{2}g(x)^2p(x,t)\\right) \\phi'(x) \\,\\mathrm{d}x\n$$\nThe boundary term is zero. Integrating the remaining integral by parts again:\n$$\n- \\left[\\frac{\\partial}{\\partial x}\\left(\\frac{1}{2}g(x)^2p(x,t)\\right)\\phi(x)\\right]_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial x^2}\\left(\\frac{1}{2}g(x)^2p(x,t)\\right) \\phi(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial x^2}\\left(\\frac{1}{2}g(x)^2p(x,t)\\right) \\phi(x) \\,\\mathrm{d}x\n$$\nSubstituting these back into the integral equality, we get\n$$\n\\int_{-\\infty}^{\\infty} \\phi(x) \\frac{\\partial p}{\\partial t} \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\left[ - \\frac{\\partial}{\\partial x}(f(x)p) + \\frac{\\partial^2}{\\partial x^2}\\left(\\frac{1}{2}g(x)^2 p\\right) \\right] \\phi(x) \\,\\mathrm{d}x\n$$\nSince this equality must hold for any arbitrary test function $\\phi(x)$, the integrands must be identical. This yields the Kolmogorov forward equation, or Fokker–Planck equation:\n$$\n\\frac{\\partial p(x,t)}{\\partial t} = -\\frac{\\partial}{\\partial x}\\left[f(x)p(x,t)\\right] + \\frac{\\partial^2}{\\partial x^2}\\left[\\frac{1}{2}g(x)^2p(x,t)\\right]\n$$\nThis equation can be written in the form of a continuity equation, $\\frac{\\partial p}{\\partial t} + \\frac{\\partial J}{\\partial x} = 0$, which is its divergence form. The probability flux $J(x,t)$ is defined as\n$$\nJ(x,t) = f(x)p(x,t) - \\frac{\\partial}{\\partial x}\\left[\\frac{1}{2}g(x)^2p(x,t)\\right]\n$$\nA time-independent stationary density, $p_s(x)$, is defined by the condition $\\frac{\\partial p_s}{\\partial t} = 0$. In this case, the Fokker–Planck equation reduces to $\\frac{\\partial J_s}{\\partial x} = 0$, where $J_s$ is the stationary flux. This implies that $J_s(x)$ must be a constant, say $J_s(x)=C$.\n$$\nf(x)p_s(x) - \\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(g(x)^2 p_s(x)\\right) = C\n$$\nFor a system on the entire real line, a normalizable stationary probability density requires that the probability flux vanishes at infinity, i.e., $\\lim_{|x|\\to\\infty} J_s(x) = 0$. Since $J_s(x)$ is a constant, this boundary condition implies that $C=0$. Therefore, the stationary density $p_s(x)$ must satisfy the first-order ordinary differential equation\n$$\nf(x)p_s(x) - \\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(g(x)^2 p_s(x)\\right) = 0\n$$\n\n(b) Stationary Density for the Ornstein–Uhlenbeck Process\n\nWe now specialize to the Ornstein–Uhlenbeck case, where the drift is $f(x) = -\\lambda x$ and the diffusion is constant, $g(x) = \\sigma$, with $\\lambda0$ and $\\sigma0$.\nSubstituting these functions into the stationary ODE from part (a) gives:\n$$\n(-\\lambda x)p_s(x) - \\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\sigma^2 p_s(x)\\right) = 0\n$$\nSince $\\sigma$ is a constant, we have\n$$\n-\\lambda x p_s(x) - \\frac{\\sigma^2}{2}\\frac{\\mathrm{d}p_s(x)}{\\mathrm{d}x} = 0\n$$\nThis is a separable first-order linear ordinary differential equation. We can rearrange it as\n$$\n\\frac{\\sigma^2}{2}\\frac{\\mathrm{d}p_s}{dx} = -\\lambda x p_s\n$$\nSeparating variables, we obtain\n$$\n\\frac{1}{p_s}\\mathrm{d}p_s = -\\frac{2\\lambda x}{\\sigma^2}\\mathrm{d}x\n$$\nIntegrating both sides yields\n$$\n\\int \\frac{1}{p_s}\\mathrm{d}p_s = \\int -\\frac{2\\lambda x}{\\sigma^2}\\mathrm{d}x\n$$\n$$\n\\ln(p_s) = -\\frac{2\\lambda}{\\sigma^2}\\frac{x^2}{2} + C' = -\\frac{\\lambda x^2}{\\sigma^2} + C'\n$$\nwhere $C'$ is the constant of integration. Exponentiating both sides gives the unnormalized solution:\n$$\np_s(x) = \\exp\\left(-\\frac{\\lambda x^2}{\\sigma^2} + C'\\right) = A \\exp\\left(-\\frac{\\lambda x^2}{\\sigma^2}\\right)\n$$\nwhere $A = \\exp(C')$ is a positive constant. To find $A$, we enforce the normalization condition $\\int_{-\\infty}^{\\infty} p_s(x)\\,\\mathrm{d}x = 1$.\n$$\nA \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\lambda x^2}{\\sigma^2}\\right) \\,\\mathrm{d}x = 1\n$$\nThe integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)\\,\\mathrm{d}x = \\sqrt{\\frac{\\pi}{a}}$. In our case, the parameter $a = \\frac{\\lambda}{\\sigma^2}$. The value of the integral is\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\lambda x^2}{\\sigma^2}\\right) \\,\\mathrm{d}x = \\sqrt{\\frac{\\pi}{\\lambda/\\sigma^2}} = \\sqrt{\\frac{\\pi\\sigma^2}{\\lambda}} = \\sigma\\sqrt{\\frac{\\pi}{\\lambda}}\n$$\nSubstituting this back into the normalization equation:\n$$\nA \\left(\\sigma\\sqrt{\\frac{\\pi}{\\lambda}}\\right) = 1\n$$\nSolving for the normalization constant $A$:\n$$\nA = \\frac{1}{\\sigma\\sqrt{\\pi/\\lambda}} = \\sqrt{\\frac{\\lambda}{\\pi\\sigma^2}}\n$$\nThus, the normalized stationary probability density for the Ornstein–Uhlenbeck process is\n$$\np_s(x) = \\sqrt{\\frac{\\lambda}{\\pi\\sigma^2}} \\exp\\left(-\\frac{\\lambda x^2}{\\sigma^2}\\right)\n$$\nThis is a Gaussian distribution with mean $0$ and variance $\\frac{\\sigma^2}{2\\lambda}$.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\lambda}{\\pi\\sigma^2}} \\exp\\left(-\\frac{\\lambda x^2}{\\sigma^2}\\right)}\n$$"
        },
        {
            "introduction": "While analytical derivations of moments and distributions provide deep insight, most real-world environmental models are too complex for such solutions. This is where computational methods become essential. This hands-on practice bridges theory and application by introducing the Monte Carlo method to estimate statistical properties from a set of model simulations. You will derive the standard estimators for the mean and variance and then apply the Central Limit Theorem to calculate the required sample size for a desired level of accuracy, a crucial skill for any computational modeler .",
            "id": "3916769",
            "problem": "Consider a stochastically forced environmental model for a basin-averaged temperature anomaly, represented as a stationary solution of the Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation $dX(t)=-k\\,X(t)\\,dt+\\sigma\\,dW(t)$, where $W(t)$ is a standard Wiener process. Under stationarity, the marginal distribution of $X$ is Gaussian with mean $0$ and variance $\\sigma^{2}/(2k)$. You wish to estimate a diagnostic $f(X)$ of the model output under stochastic forcing and to quantify uncertainty using the Central Limit Theorem (CLT), where CLT stands for Central Limit Theorem and Monte Carlo (MC) refers to Monte Carlo sampling.\n\nUsing $n$ independent and identically distributed (i.i.d., independent and identically distributed) draws $X_{1},\\dots,X_{n}$ from the stationary distribution of $X$, define $Y_{i}=f(X_{i})$. Starting from the definitions of expectation and variance, and the statement of the CLT for i.i.d. random variables with finite variance, perform the following:\n\n- Derive the MC estimator $\\hat{\\mu}_{n}$ for $\\mathbb{E}[f(X)]$ and the MC estimator $\\hat{v}_{n}$ for $\\mathrm{Var}(f(X))$, expressed in terms of $Y_{1},\\dots,Y_{n}$ only.\n- Using the CLT, derive the asymptotic sampling distribution of $\\hat{\\mu}_{n}$ and express the two-sided $(1-\\alpha)$ confidence interval half-width $\\varepsilon_{\\mathrm{CLT}}$ in terms of the unknown variance $\\mathrm{Var}(f(X))$, the sample size $n$, and the standard normal quantile $z_{1-\\alpha/2}$.\n- Specialize to the identity diagnostic $f(x)=x$ for the OU model, and use the stationary variance to eliminate unknowns in the half-width formula.\n\nFinally, for the specific parameter values $k=0.5\\,\\mathrm{day}^{-1}$, $\\sigma=2\\,\\mathrm{K}\\,\\mathrm{day}^{-1/2}$, $\\alpha=0.05$, and target half-width $\\varepsilon=0.1\\,\\mathrm{K}$ for the two-sided confidence interval of $\\mathbb{E}[f(X)]$ under the CLT approximation, compute the minimal integer sample size $n$ such that the half-width does not exceed $\\varepsilon$. Express your final answer as a dimensionless count and provide the minimal integer $n$ that satisfies the CLT-based bound. Do not report any intermediate quantities. The final answer must be a single real number. No rounding instruction is necessary beyond the requirement to provide the minimal integer $n$ meeting the bound.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n-   Stochastic model: Ornstein–Uhlenbeck (OU) process for temperature anomaly $X(t)$, defined by the stochastic differential equation (SDE) $dX(t)=-k\\,X(t)\\,dt+\\sigma\\,dW(t)$.\n-   $W(t)$ is a standard Wiener process.\n-   The stationary solution $X$ follows a Gaussian distribution with mean $\\mathbb{E}[X]=0$ and variance $\\mathrm{Var}(X) = \\frac{\\sigma^{2}}{2k}$.\n-   A diagnostic function $f(X)$ is considered.\n-   A sample of $n$ independent and identically distributed (i.i.d.) draws $X_{1},\\dots,X_{n}$ is taken from the stationary distribution of $X$.\n-   A new set of random variables is defined as $Y_{i}=f(X_{i})$ for $i=1,\\dots,n$.\n-   The goal is to estimate $\\mathbb{E}[f(X)]$ and its uncertainty.\n-   Specific tasks:\n    1.  Derive the Monte Carlo (MC) estimator $\\hat{\\mu}_{n}$ for $\\mathbb{E}[f(X)]$ and $\\hat{v}_{n}$ for $\\mathrm{Var}(f(X))$.\n    2.  Derive the asymptotic sampling distribution of $\\hat{\\mu}_{n}$ via the Central Limit Theorem (CLT).\n    3.  Derive the two-sided $(1-\\alpha)$ confidence interval half-width $\\varepsilon_{\\mathrm{CLT}}$.\n    4.  Specialize to the case $f(x)=x$ and use the known stationary variance of the OU process.\n    5.  Compute the minimal integer sample size $n$ required to achieve a target half-width $\\varepsilon$.\n-   Parameter values for the final calculation:\n    -   $k=0.5\\,\\mathrm{day}^{-1}$\n    -   $\\sigma=2\\,\\mathrm{K}\\,\\mathrm{day}^{-1/2}$\n    -   $\\alpha=0.05$\n    -   $\\varepsilon=0.1\\,\\mathrm{K}$\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The OU process is a cornerstone of stochastic modeling in physics, finance, and environmental science. The stated properties of its stationary solution are correct. Monte Carlo estimation and the Central Limit Theorem are fundamental principles of statistics. The problem is scientifically and mathematically sound.\n-   **Well-Posedness**: The problem provides a clear sequence of tasks, from general derivation to a specific numerical calculation. All necessary information (model, parameters, statistical framework) is provided. The question asks for a minimal integer sample size, which is a well-defined quantity that can be uniquely determined from the given constraints.\n-   **Objectivity and Consistency**: The problem is stated in precise mathematical and statistical language. The physical units of the parameters $k$, $\\sigma$, and $\\varepsilon$ are consistent with the SDE and the definition of a confidence interval. The problem is free of ambiguity and subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We may proceed with the solution.\n\n**Derivations and Solution**\n\nLet $Y_i = f(X_i)$ for $i=1, \\dots, n$ be a set of i.i.d. samples, since the $X_i$ are i.i.d. We denote the true mean and variance of $Y$ as $\\mu = \\mathbb{E}[Y] = \\mathbb{E}[f(X)]$ and $v = \\mathrm{Var}(Y) = \\mathrm{Var}(f(X))$.\n\nThe Monte Carlo estimator for the expectation $\\mu$ is the sample mean of the observations $Y_i$. By the Law of Large Numbers, the sample mean is a consistent estimator of the true mean.\n$$\n\\hat{\\mu}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThe standard unbiased estimator for the variance $v$ is the sample variance, calculated using the sample mean $\\hat{\\mu}_{n}$.\n$$\n\\hat{v}_{n} = \\frac{1}{n-1} \\sum_{i=1}^{n} (Y_i - \\hat{\\mu}_{n})^2\n$$\n\nThe Central Limit Theorem (CLT) states that for i.i.d. random variables $Y_i$ with finite mean $\\mu$ and finite variance $v$, the distribution of the sample mean $\\hat{\\mu}_n$ approaches a normal distribution as $n \\to \\infty$. More formally,\n$$\n\\sqrt{n}(\\hat{\\mu}_{n} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, v)\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. This implies that for a sufficiently large sample size $n$, the sampling distribution of $\\hat{\\mu}_{n}$ can be approximated as:\n$$\n\\hat{\\mu}_{n} \\sim \\mathcal{N}\\left(\\mu, \\frac{v}{n}\\right)\n$$\nThis is the asymptotic sampling distribution of $\\hat{\\mu}_{n}$. To construct a confidence interval for $\\mu$, we can standardize the estimator $\\hat{\\mu}_n$:\n$$\n\\frac{\\hat{\\mu}_{n} - \\mu}{\\sqrt{v/n}} \\sim \\mathcal{N}(0, 1)\n$$\nA two-sided $(1-\\alpha)$ confidence interval for $\\mu$ is defined by the interval where the probability that $\\mu$ lies within it is $1-\\alpha$. Using the standard normal distribution, this is given by:\n$$\nP\\left( -z_{1-\\alpha/2} \\le \\frac{\\hat{\\mu}_{n} - \\mu}{\\sqrt{v/n}} \\le z_{1-\\alpha/2} \\right) \\approx 1-\\alpha\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution (i.e., $\\Phi(z_{1-\\alpha/2}) = 1-\\alpha/2$, where $\\Phi$ is the standard normal cumulative distribution function).\n\nRearranging the inequality to isolate $\\mu$ gives the confidence interval:\n$$\n\\left[ \\hat{\\mu}_{n} - z_{1-\\alpha/2} \\sqrt{\\frac{v}{n}}, \\quad \\hat{\\mu}_{n} + z_{1-\\alpha/2} \\sqrt{\\frac{v}{n}} \\right]\n$$\nThe half-width of this confidence interval, denoted $\\varepsilon_{\\mathrm{CLT}}$, is:\n$$\n\\varepsilon_{\\mathrm{CLT}} = z_{1-\\alpha/2} \\sqrt{\\frac{v}{n}} = z_{1-\\alpha/2} \\frac{\\sqrt{\\mathrm{Var}(f(X))}}{\\sqrt{n}}\n$$\n\nNow, we specialize to the identity diagnostic, $f(x)=x$. In this case, $Y_i = X_i$. The quantity to be estimated is $\\mu = \\mathbb{E}[X]$, and the variance required for the CLT is $v = \\mathrm{Var}(X)$.\nFrom the problem statement, the stationary distribution of the OU process is Gaussian with mean $\\mathbb{E}[X]=0$ and variance $\\mathrm{Var}(X) = \\frac{\\sigma^2}{2k}$.\nSubstituting this known variance $v$ into the half-width formula, we eliminate the unknown $\\mathrm{Var}(f(X))$:\n$$\n\\varepsilon_{\\mathrm{CLT}} = z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma^2/(2k)}{n}} = z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{2kn}}\n$$\n\nThe final task is to find the minimal integer sample size $n$ such that this half-width does not exceed a target value $\\varepsilon$. We set up the inequality:\n$$\n\\varepsilon_{\\mathrm{CLT}} \\le \\varepsilon\n$$\n$$\nz_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{2kn}} \\le \\varepsilon\n$$\nSolving for $n$:\n$$\n\\sqrt{n} \\ge z_{1-\\alpha/2} \\frac{\\sigma}{\\varepsilon\\sqrt{2k}}\n$$\n$$\nn \\ge \\left( z_{1-\\alpha/2} \\frac{\\sigma}{\\varepsilon\\sqrt{2k}} \\right)^2 = z_{1-\\alpha/2}^2 \\frac{\\sigma^2}{2k\\varepsilon^2}\n$$\nNow, we substitute the given numerical values:\n-   $k=0.5\\,\\mathrm{day}^{-1}$\n-   $\\sigma=2\\,\\mathrm{K}\\,\\mathrm{day}^{-1/2}$\n-   $\\alpha=0.05$\n-   $\\varepsilon=0.1\\,\\mathrm{K}$\n\nFor $\\alpha=0.05$, the required quantile is $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975}$. This is a standard value from normal distribution tables, $z_{0.975} \\approx 1.96$.\n\nPlugging these values into the inequality for $n$:\n$$\nn \\ge (1.96)^2 \\frac{(2\\,\\mathrm{K}\\,\\mathrm{day}^{-1/2})^2}{2 \\times (0.5\\,\\mathrm{day}^{-1}) \\times (0.1\\,\\mathrm{K})^2}\n$$\n$$\nn \\ge (3.8416) \\frac{4\\,\\mathrm{K}^2\\,\\mathrm{day}^{-1}}{1\\,\\mathrm{day}^{-1} \\times 0.01\\,\\mathrm{K}^2}\n$$\n$$\nn \\ge (3.8416) \\frac{4}{0.01} = 3.8416 \\times 400\n$$\n$$\nn \\ge 1536.64\n$$\nSince the sample size $n$ must be an integer, the minimal integer value that satisfies this condition is the ceiling of $1536.64$.\n$$\nn = \\lceil 1536.64 \\rceil = 1537\n$$\nTherefore, a minimum of $1537$ independent samples are required to achieve the desired confidence interval half-width.",
            "answer": "$$\\boxed{1537}$$"
        }
    ]
}