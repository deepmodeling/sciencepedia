{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in model structure identification is to determine if the available data is sufficiently informative to distinguish between competing structural terms. This exercise  explores this fundamental concept of structural identifiability by examining how different boundary conditions and data 'excitations' affect our ability to uniquely identify terms in a partial differential equation. By investigating the linear independence of columns in a regression matrix, you will gain a practical understanding of why the design of experiments or the nature of observations is paramount for successful model discovery.",
            "id": "3895645",
            "problem": "You are given a one-dimensional, dimensionless environmental tracer field $u(x,t)$ on a spatial interval $x \\in [0,1]$ and a time interval $t \\in [0,1]$. The task is to assess which candidate terms in a structural model of a Partial Differential Equation (PDE) are uniquely identifiable under different Boundary Conditions (BC) and excitation choices. The fundamental base is the mass conservation statement in one spatial dimension, which implies that any physically realistic evolution of a scalar tracer can be written as a combination of spatial flux divergence and local sources or sinks. The candidate structure is restricted to terms that are ubiquitous in environmental and earth system modeling: the second spatial derivative $u_{xx}$, the first spatial derivative $u_{x}$, and the zero-order term $u$. You must determine structural identifiability of these candidate terms using only the data $u(x,t)$ and finite-difference approximations to its derivatives.\n\nDefinitions and constraints:\n- Define the PDE candidate library $\\mathcal{L} = \\{u, u_{x}, u_{xx}\\}$.\n- Structural identifiability in this context means: for a linear-in-terms PDE model with coefficients multiplying the terms in $\\mathcal{L}$, a term is identifiable if and only if its associated column in the regression matrix constructed from $\\mathcal{L}$ is not a linear combination of the remaining columns. Equivalently, if the regression matrix has full column rank, all coefficients are identifiable; if the matrix is rank-deficient, at least one coefficient is not identifiable. You must decide identifiability term-by-term.\n- Derivative approximations must be computed using central differences on interior grid points. Boundary conditions must be obeyed by the excitation functions $u(x,t)$; angles in trigonometric functions must be in radians.\n- Do not estimate any physical parameters. Only determine identifiability based on the linear independence of columns constructed from $u$, $u_{x}$, and $u_{xx}$.\n\nComputational procedure:\n1. Discretize the domain with a uniform spatial grid of $N_x$ points on $[0,1]$ and a uniform time grid of $N_t$ points on $[0,1]$.\n2. For each test case, construct $u(x,t)$ on the grid such that it respects the specified boundary conditions. Use interior central differences to approximate $u_{x}$, $u_{xx}$, and $u_{t}$, and restrict all quantities to interior points in both space and time (i.e., exclude boundary indices where one-sided differences would be required).\n3. Construct the regression matrix $\\Theta$ by stacking the columns corresponding to $u_{xx}$, $u_{x}$, and $u$ evaluated at the interior spacetime points. Let $\\Theta \\in \\mathbb{R}^{M \\times 3}$, where $M$ is the number of interior spacetime samples.\n4. Determine structural identifiability for each term in $\\mathcal{L}$ by checking if its column is in the span of the other two columns. A column is considered non-identifiable if it is numerically zero or if its least-squares projection onto the span of the other columns has a relative residual smaller than a prescribed numerical tolerance. Otherwise, it is identifiable. Use a tolerance on the relative residual of $10^{-8}$ and treat any column with relative norm smaller than $10^{-12}$ as numerically zero.\n5. Your program must output, for each test case, a list of three booleans $[I_{xx}, I_{x}, I_{0}]$ indicating identifiability of the $u_{xx}$, $u_{x}$, and $u$ terms, respectively.\n\nTest suite:\nUse the following four scientifically realistic and self-consistent test cases that cover different boundary conditions and excitation structures. All angles in trigonometric functions must be in radians. There are no physical units in this problem; all quantities are dimensionless.\n\n- Test Case 1 (General “happy path” under periodic BC): $N_x = 128$, $N_t = 64$, periodic BC, and excitation\n  $$u(x,t) = e^{-t}\\left(\\sin(2\\pi x) + \\tfrac{1}{2}\\sin(4\\pi x)\\right).$$\n  This combines two spatial Fourier modes so that $u$ and $u_{xx}$ are not collinear, and $u_{x}$ is distinct, promoting full identifiability.\n\n- Test Case 2 (Boundary-induced collinearity under periodic BC): $N_x = 128$, $N_t = 64$, periodic BC, and excitation\n  $$u(x,t) = e^{-t}\\sin(2\\pi x).$$\n  Here $u_{xx} = -(2\\pi)^2 u$ for the spatial mode, making $u$ and $u_{xx}$ collinear across spacetime and preventing separate identification of reaction and diffusion structures.\n\n- Test Case 3 (Dirichlet zero value BC with polynomial excitation): $N_x = 129$, $N_t = 64$, Dirichlet BC with $u(0,t)=0$ and $u(1,t)=0$, and excitation\n  $$u(x,t) = e^{-t}x(1-x).$$\n  This satisfies the boundary condition and yields linearly independent samples for $u$, $u_{x}$, and $u_{xx}$ across the interior.\n\n- Test Case 4 (Neumann zero-flux BC with spatially constant excitation): $N_x = 129$, $N_t = 64$, Neumann BC with $u_{x}(0,t)=0$ and $u_{x}(1,t)=0$, and excitation\n  $$u(x,t) = e^{-t}.$$\n  This yields $u_{x} \\equiv 0$ and $u_{xx} \\equiv 0$, making advection and diffusion non-identifiable while leaving the zero-order term identifiable.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is the three-boolean list for the corresponding test case. For example: \n$$[\\,[I_{xx}^{(1)},I_{x}^{(1)},I_{0}^{(1)}],\\,[I_{xx}^{(2)},I_{x}^{(2)},I_{0}^{(2)}],\\,[I_{xx}^{(3)},I_{x}^{(3)},I_{0}^{(3)}],\\,[I_{xx}^{(4)},I_{x}^{(4)},I_{0}^{(4)}]\\,]$$\nprinted as a single Python list literal in one line.",
            "solution": "The starting point is the one-dimensional mass conservation for a scalar tracer, which states that the rate of change of the tracer density $u(x,t)$ within a small control volume equals the net inflow due to flux divergence plus local sources or sinks. Denoting the flux as $J(x,t)$ and source as $S(x,t)$, the conservation statement is\n$$\\frac{\\partial u}{\\partial t} = -\\frac{\\partial J}{\\partial x} + S.$$\nIn environmental modeling, a widely accepted and fundamental decomposition of the flux in the absence of external forcing is the sum of a Fickian diffusive component and an advective component:\n$$J = -D\\frac{\\partial u}{\\partial x} + v\\,u,$$\nwith $D$ a diffusion parameter and $v$ an advection velocity. A linear reaction source is commonly represented as\n$$S = r\\,u,$$\nwith $r$ a reaction parameter. Substituting these into the conservation law yields\n$$\\frac{\\partial u}{\\partial t} = D\\,\\frac{\\partial^2 u}{\\partial x^2} - v\\,\\frac{\\partial u}{\\partial x} + r\\,u.$$\nWe do not estimate the parameters $D$, $v$, and $r$ here; rather, we assess structural identifiability of the terms $u_{xx}$, $u_{x}$, and $u$ from data. The reason this is a principled approach is that, under linear-in-terms representation, interpretability and uniqueness of term coefficients reduce to the linear independence of the columns of a regression matrix built from the library of candidate terms evaluated at observed spacetime samples.\n\nGiven sampled fields $u(x_i,t_j)$ on a two-dimensional grid, finite differences provide approximations of the required derivatives. We select central differences for both space and time to minimize bias and increase accuracy:\n- For interior spatial points $x_i$ with $i=1,\\dots,N_x-2$, the first derivative is approximated by\n$$u_{x}(x_i,t_j) \\approx \\frac{u(x_{i+1},t_j) - u(x_{i-1},t_j)}{2\\Delta x},$$\nand the second derivative by\n$$u_{xx}(x_i,t_j) \\approx \\frac{u(x_{i+1},t_j) - 2u(x_i,t_j) + u(x_{i-1},t_j)}{\\Delta x^2},$$\nwhere $\\Delta x$ is the spatial grid spacing.\n- For interior temporal points $t_j$ with $j=1,\\dots,N_t-2$, the time derivative is approximated by\n$$u_{t}(x_i,t_j) \\approx \\frac{u(x_i,t_{j+1}) - u(x_i,t_{j-1})}{2\\Delta t},$$\nwhere $\\Delta t$ is the time step.\n\nThese approximations are consistent with boundary conditions when we restrict attention to interior points; the excitation functions $u(x,t)$ are constructed to satisfy the stated boundary conditions exactly, which ensures physical realism without having to use one-sided differences at the boundaries.\n\nTo build the regression matrix, we stack the interior spacetime samples of $u_{xx}$, $u_{x}$, and $u$ into columns:\n$$\\Theta = \\begin{bmatrix}\n\\vdots & \\vdots & \\vdots \\\\\nu_{xx}(x_i,t_j) & u_{x}(x_i,t_j) & u(x_i,t_j) \\\\\n\\vdots & \\vdots & \\vdots\n\\end{bmatrix} \\in \\mathbb{R}^{M \\times 3},$$\nwhere $M$ is the number of interior spacetime samples.\n\nStructural identifiability criteria follow from linear algebra:\n- If $\\mathrm{rank}(\\Theta) = 3$, all terms are identifiable; each coefficient is uniquely determined in principle from the data via least squares because the columns are linearly independent.\n- If $\\mathrm{rank}(\\Theta) < 3$, at least one column is a linear combination of the others (or numerically zero), which implies non-identifiability for the corresponding coefficient(s). More finely, a given term is non-identifiable if its column lies in the span of the remaining columns. Conversely, a term is identifiable if its column is not in the span of the others.\n\nTo robustly assess identifiability term-by-term in finite precision arithmetic:\n1. Compute the relative norm of each column. If the relative norm is below a tiny threshold (e.g., $10^{-12}$), treat the column as numerically zero and declare the term non-identifiable.\n2. For each column $j$, solve the least squares problem\n$$\\min_{\\beta}\\left\\| \\Theta_{\\cdot j} - \\Theta_{\\cdot,-j}\\,\\beta \\right\\|_2,$$\nwhere $\\Theta_{\\cdot j}$ denotes column $j$ and $\\Theta_{\\cdot,-j}$ denotes the matrix of the other two columns. Compute the relative residual\n$$\\rho_j = \\frac{\\left\\| \\Theta_{\\cdot j} - \\Theta_{\\cdot,-j}\\,\\hat{\\beta} \\right\\|_2}{\\left\\| \\Theta_{\\cdot j} \\right\\|_2}.$$\nIf $\\rho_j < 10^{-8}$, declare column $j$ dependent and the corresponding term non-identifiable; otherwise, declare it identifiable.\n\nWhy boundary conditions matter:\n- Under periodic or Dirichlet boundary conditions, spatial eigenfunctions of the Laplacian, such as $\\sin(k\\pi x)$, satisfy $u_{xx} = -\\lambda u$ with $\\lambda = (k\\pi)^2$ for Dirichlet or $\\lambda = (2\\pi k)^2$ for periodic modes on $[0,1]$. If the excitation $u$ is a single eigenfunction modulated in time, the columns corresponding to $u$ and $u_{xx}$ are collinear, making diffusion and reaction inseparable structurally. Adding multiple distinct eigenmodes breaks this proportionality and restores independence.\n- Under Neumann zero-flux boundary conditions, a spatially constant excitation yields $u_{x} \\equiv 0$ and $u_{xx} \\equiv 0$, immediately removing advection and diffusion from the identifiable set while leaving the zero-order term identifiable.\n- Polynomial excitations satisfying Dirichlet boundary conditions (e.g., $u = e^{-t}x(1-x)$) produce distinct spatial patterns for $u$, $u_{x}$, and $u_{xx}$, promoting full identifiability.\n\nApplying the procedure to the test suite:\n- Test Case 1 combines two periodic Fourier modes, making $u$ and $u_{xx}$ non-collinear and $u_{x}$ distinct. The algorithm will find column-wise independence for all three terms and return $[{\\rm True},{\\rm True},{\\rm True}]$.\n- Test Case 2 uses a single periodic Fourier mode, so $u_{xx}$ is a scalar multiple of $u$. The least squares projection will yield near-zero residual for both directions of dependence, declaring $u_{xx}$ and $u$ non-identifiable, while $u_{x}$ remains identifiable. The result is $[{\\rm False},{\\rm True},{\\rm False}]$.\n- Test Case 3 uses a polynomial under Dirichlet boundary conditions; the spatial patterns for $u$, $u_{x}$, and $u_{xx}$ are distinct across interior points, leading to $[{\\rm True},{\\rm True},{\\rm True}]$.\n- Test Case 4 uses a spatially constant excitation under Neumann boundary conditions; $u_{x}$ and $u_{xx}$ columns are numerically zero and therefore non-identifiable, while the $u$ column remains independent, leading to $[{\\rm False},{\\rm False},{\\rm True}]$.\n\nThe program implements the above finite-difference approximations, constructs $\\Theta$, and performs the identifiability checks with the specified numerical tolerances. It outputs the list of three-booleans per test case as a single Python list literal on one line, in the order of the four test cases.",
            "answer": "```python\n# Python 3.12\n# Libraries: numpy 1.23.5, scipy 1.11.4 (not used)\nimport numpy as np\n\ndef generate_u(case, x, t):\n    \"\"\"\n    Generate u(x,t) for a given test case definition.\n    case: dict with keys 'bc' and 'excitation'\n    x: 1D array of spatial points\n    t: 1D array of time points\n    Returns u as a 2D array of shape (Nx, Nt) where Nx=len(x), Nt=len(t).\n    \"\"\"\n    X, T = np.meshgrid(x, t, indexing='ij')\n    exc = case['excitation']\n    if exc == 'periodic_mixture':\n        u = np.exp(-T) * (np.sin(2*np.pi*X) + 0.5*np.sin(4*np.pi*X))\n        # Periodic BC is satisfied by construction.\n    elif exc == 'periodic_single':\n        u = np.exp(-T) * np.sin(2*np.pi*X)\n        # Periodic BC satisfied.\n    elif exc == 'dirichlet_quadratic':\n        u = np.exp(-T) * X * (1.0 - X)\n        # Dirichlet BC: u(0,t)=0 and u(1,t)=0 satisfied.\n    elif exc == 'neumann_constant':\n        u = np.exp(-T) * np.ones_like(X)\n        # Neumann BC: ux=0 at boundaries satisfied since u is constant in space.\n    else:\n        raise ValueError(\"Unknown excitation type\")\n    return u\n\ndef finite_differences(u, x, t):\n    \"\"\"\n    Compute central-difference approximations of ux, uxx, ut on interior points.\n    Returns flattened vectors for the interior grid: ux_vec, uxx_vec, u_vec\n    \"\"\"\n    dx = x[1] - x[0]\n    dt = t[1] - t[0]\n\n    # Interior indices\n    ix0, ix1 = 1, len(x) - 2\n    it0, it1 = 1, len(t) - 2\n\n    # Slicing interior\n    u_int = u[ix0:ix1+1, it0:it1+1]\n\n    # Spatial derivatives using central differences on interior spatial points\n    ux_int = (u[ix0+1:ix1+2, it0:it1+1] - u[ix0-1:ix1, it0:it1+1]) / (2.0 * dx)\n    uxx_int = (u[ix0+1:ix1+2, it0:it1+1] - 2.0*u[ix0:ix1+1, it0:it1+1] + u[ix0-1:ix1, it0:it1+1]) / (dx*dx)\n\n    # Time derivative using central differences on interior time points\n    ut_int = (u[ix0:ix1+1, it0+1:it1+2] - u[ix0:ix1+1, it0-1:it1]) / (2.0 * dt)\n\n    # To align shapes, restrict u_int, ux_int, uxx_int and ut_int to common interior\n    # u_int is already (ix1-ix0+1, it1-it0+1)\n    # ux_int and uxx_int have same shape as u_int\n    # ut_int has same shape as u_int\n\n    # Flatten vectors\n    u_vec = u_int.reshape(-1)\n    ux_vec = ux_int.reshape(-1)\n    uxx_vec = uxx_int.reshape(-1)\n    ut_vec = ut_int.reshape(-1)\n\n    return ux_vec, uxx_vec, u_vec, ut_vec\n\ndef term_identifiability(theta, tol_zero=1e-12, tol_dep=1e-8):\n    \"\"\"\n    Determine identifiability of each column in theta by checking if the column is\n    numerically zero or lies in the span of the other columns.\n    Returns a list of booleans [I_xx, I_x, I_0] for columns in order [uxx, ux, u].\n    \"\"\"\n    M, K = theta.shape\n    assert K == 3, \"Theta must have 3 columns for [uxx, ux, u]\"\n    id_flags = []\n\n    # Pre-compute norms\n    col_norms = np.linalg.norm(theta, axis=0)\n    # Relative norms (scaled by overall matrix norm to guard)\n    mat_norm = np.linalg.norm(theta)\n    rel_norms = col_norms / (mat_norm + 1e-30)\n\n    for j in range(K):\n        # Zero column check\n        if rel_norms[j] < tol_zero:\n            id_flags.append(False)\n            continue\n        # Build matrix of other columns\n        other_indices = [i for i in range(K) if i != j]\n        Theta_others = theta[:, other_indices]\n        col_j = theta[:, j]\n\n        # If other columns are all near-zero, then col_j is independent\n        if np.linalg.matrix_rank(Theta_others) == 0 and np.linalg.norm(Theta_others) < tol_zero:\n            id_flags.append(True)\n            continue\n\n        # Least squares projection of col_j onto span of others\n        beta, *_ = np.linalg.lstsq(Theta_others, col_j, rcond=None)\n        residual = col_j - Theta_others @ beta\n        r_rel = np.linalg.norm(residual) / (np.linalg.norm(col_j) + 1e-30)\n        # If residual is tiny, column is dependent -> not identifiable\n        id_flags.append(r_rel >= tol_dep)\n    return id_flags\n\ndef run_case(case):\n    Nx = case['Nx']\n    Nt = case['Nt']\n    # Uniform grids on [0,1]\n    x = np.linspace(0.0, 1.0, Nx)\n    t = np.linspace(0.0, 1.0, Nt)\n    u = generate_u(case, x, t)\n    ux_vec, uxx_vec, u_vec, ut_vec = finite_differences(u, x, t)\n    # Build Theta with columns [uxx, ux, u]\n    Theta = np.column_stack([uxx_vec, ux_vec, u_vec])\n    # Determine identifiability\n    id_flags = term_identifiability(Theta, tol_zero=1e-12, tol_dep=1e-8)\n    return id_flags\n\ndef solve():\n    # Define test cases from the problem statement\n    test_cases = [\n        # Test Case 1: Periodic BC, mixture of Fourier modes\n        {'bc': 'periodic', 'excitation': 'periodic_mixture', 'Nx': 128, 'Nt': 64},\n        # Test Case 2: Periodic BC, single Fourier mode\n        {'bc': 'periodic', 'excitation': 'periodic_single', 'Nx': 128, 'Nt': 64},\n        # Test Case 3: Dirichlet zero value BC, polynomial excitation\n        {'bc': 'dirichlet', 'excitation': 'dirichlet_quadratic', 'Nx': 129, 'Nt': 64},\n        # Test Case 4: Neumann zero-flux BC, spatially constant excitation\n        {'bc': 'neumann', 'excitation': 'neumann_constant', 'Nx': 129, 'Nt': 64},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(case)\n        results.append(res)\n\n    # Print single-line output in the exact required format\n    # e.g., [[True,True,True],[False,True,False],...]\n    def list_to_str(lst):\n        return \"[\" + \",\".join([\"True\" if v else \"False\" for v in lst]) + \"]\"\n    print(\"[\" + \",\".join([list_to_str(r) for r in results]) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having established the importance of data quality for identifiability, we now turn to a powerful computational method for performing model selection from a large library of potential physical processes. This practice  introduces the group-sparse least absolute shrinkage and selection operator (LASSO) as a framework for discovering governing equations from data. You will derive and apply the selection criteria that allow this method to 'switch off' entire groups of operators—such as advection or reaction—that are not supported by the evidence, thereby promoting parsimonious and physically interpretable models.",
            "id": "3895649",
            "problem": "Consider a two-dimensional advection–diffusion–reaction process for a conserved scalar field $c(\\boldsymbol{x}, t)$, governed by the conservation of mass and constitutive relationships that lead to the canonical transport form\n$$\n\\frac{\\partial c}{\\partial t} + \\nabla \\cdot (\\boldsymbol{u} c) = \\nabla \\cdot \\left(D \\nabla c \\right) + R(c),\n$$\nwhere $\\boldsymbol{u}$ is the velocity field, $D$ is the diffusivity, and $R(c)$ is a reaction term. In data-driven model structure identification for environmental and earth system modeling, one constructs an operator library and fits a parameterized model that is linear in the operator coefficients. Aggregating spatiotemporal samples yields a regression of the form\n$$\n\\boldsymbol{y} = X_{\\mathrm{adv}} \\boldsymbol{\\beta}_{\\mathrm{adv}} + X_{\\mathrm{diff}} \\boldsymbol{\\beta}_{\\mathrm{diff}} + X_{\\mathrm{react}} \\boldsymbol{\\beta}_{\\mathrm{react}} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\boldsymbol{y} \\in \\mathbb{R}^{n}$ are the measured time tendencies of $c$, and $X_{\\mathrm{adv}} \\in \\mathbb{R}^{n \\times d_{\\mathrm{adv}}}$, $X_{\\mathrm{diff}} \\in \\mathbb{R}^{n \\times d_{\\mathrm{diff}}}$, and $X_{\\mathrm{react}} \\in \\mathbb{R}^{n \\times d_{\\mathrm{react}}}$ collect the evaluated operator templates for advection, diffusion, and reaction, respectively. To identify which operators are structurally present, we pose a group-sparse estimator using the Least Absolute Shrinkage and Selection Operator (LASSO) in its group form:\n$$\n\\min_{\\boldsymbol{\\beta}_{\\mathrm{adv}},\\,\\boldsymbol{\\beta}_{\\mathrm{diff}},\\,\\boldsymbol{\\beta}_{\\mathrm{react}}} \\;\\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - X_{\\mathrm{adv}} \\boldsymbol{\\beta}_{\\mathrm{adv}} - X_{\\mathrm{diff}} \\boldsymbol{\\beta}_{\\mathrm{diff}} - X_{\\mathrm{react}} \\boldsymbol{\\beta}_{\\mathrm{react}} \\right\\|_{2}^{2} + \\lambda \\left( w_{\\mathrm{adv}} \\left\\|\\boldsymbol{\\beta}_{\\mathrm{adv}}\\right\\|_{2} + w_{\\mathrm{diff}} \\left\\|\\boldsymbol{\\beta}_{\\mathrm{diff}}\\right\\|_{2} + w_{\\mathrm{react}} \\left\\|\\boldsymbol{\\beta}_{\\mathrm{react}}\\right\\|_{2} \\right),\n$$\nwhere $\\lambda \\geq 0$ is a regularization level, and $w_{\\mathrm{adv}}$, $w_{\\mathrm{diff}}$, and $w_{\\mathrm{react}}$ are positive group weights. Suppose the following scientifically consistent pre-processing and data conditions hold:\n- The number of aggregated samples is $n = 50$.\n- Column sets of $X_{\\mathrm{adv}}$, $X_{\\mathrm{diff}}$, and $X_{\\mathrm{react}}$ have been orthonormalized and mutually orthogonalized in the sense that $(1/n) X_{g}^{\\top} X_{g} = I_{d_{g}}$ for each group $g \\in \\{\\mathrm{adv}, \\mathrm{diff}, \\mathrm{react}\\}$, and $X_{g}^{\\top} X_{h} = 0$ for $g \\neq h$.\n- The group dimensions are $d_{\\mathrm{adv}} = 2$, $d_{\\mathrm{diff}} = 1$, and $d_{\\mathrm{react}} = 3$.\n- The group weights are chosen as $w_{g} = \\sqrt{d_{g}}$ to balance the penalty across groups of different sizes.\n- The empirically computed operator–response inner products (from the data and the orthonormalized design) are $X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y} = \\begin{pmatrix} 9 \\\\ -3 \\end{pmatrix}$, $X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y} = \\begin{pmatrix} 22 \\end{pmatrix}$, and $X_{\\mathrm{react}}^{\\top} \\boldsymbol{y} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n\nStarting from the conservation law and least-squares data fitting principle, derive the necessary and sufficient selection conditions for a group to be excluded by the group penalty, and use them to determine the minimal regularization level $\\lambda_{\\min}$ (dimensionless) such that, at the optimizer of the above problem, only the diffusion operator group remains active while the advection and reaction groups are excluded. Express the final answer as an exact analytic expression and in dimensionless units. No rounding is required. The final answer must be a single real-valued expression.",
            "solution": "The goal is to find the minimum regularization parameter $\\lambda$ that makes the coefficient groups $\\boldsymbol{\\beta}_{\\mathrm{adv}}$ and $\\boldsymbol{\\beta}_{\\mathrm{react}}$ zero, while leaving $\\boldsymbol{\\beta}_{\\mathrm{diff}}$ non-zero. The solution is found by analyzing the Karush-Kuhn-Tucker (KKT) optimality conditions for the group LASSO objective function:\n$$ J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{g} X_g \\boldsymbol{\\beta}_g \\right\\|_{2}^{2} + \\lambda \\sum_{g} w_g \\|\\boldsymbol{\\beta}_g\\|_2 $$\nwhere $g \\in \\{\\mathrm{adv}, \\mathrm{diff}, \\mathrm{react}\\}$. The stationarity condition with respect to a group's coefficients $\\boldsymbol{\\beta}_g$ is $\\boldsymbol{0} \\in \\nabla_{\\boldsymbol{\\beta}_g} J(\\boldsymbol{\\beta})$. Due to the stated orthogonality conditions ($X_g^\\top X_h = 0$ for $g \\neq h$ and $\\frac{1}{n}X_g^\\top X_g = I_{d_g}$), the gradient of the least-squares term simplifies considerably for each group:\n$$ \\nabla_{\\boldsymbol{\\beta}_g} \\left( \\frac{1}{2n} \\| \\boldsymbol{y} - \\sum_h X_h \\boldsymbol{\\beta}_h \\|_2^2 \\right) = -\\frac{1}{n} X_g^\\top \\boldsymbol{y} + \\boldsymbol{\\beta}_g $$\nThe subgradient of the group LASSO penalty term for group $g$ is:\n$$ \\partial \\left( \\lambda w_g \\|\\boldsymbol{\\beta}_g\\|_2 \\right) = \\begin{cases} \\{ \\lambda w_g \\frac{\\boldsymbol{\\beta}_g}{\\|\\boldsymbol{\\beta}_g\\|_2} \\} & \\text{if } \\boldsymbol{\\beta}_g \\neq \\boldsymbol{0} \\\\ \\{ \\boldsymbol{v} \\in \\mathbb{R}^{d_g} : \\|\\boldsymbol{v}\\|_2 \\le \\lambda w_g \\} & \\text{if } \\boldsymbol{\\beta}_g = \\boldsymbol{0} \\end{cases} $$\nCombining these gives the KKT conditions. A group $g$ is excluded from the model ($\\boldsymbol{\\beta}_g = \\boldsymbol{0}$) if and only if the following condition holds at the optimum:\n$$ \\left\\| \\frac{1}{n} X_g^\\top \\boldsymbol{y} \\right\\|_2 \\le \\lambda w_g $$\nConversely, a group is included ($\\boldsymbol{\\beta}_g \\neq \\boldsymbol{0}$) if and only if:\n$$ \\left\\| \\frac{1}{n} X_g^\\top \\boldsymbol{y} \\right\\|_2 > \\lambda w_g $$\nWe require the advection and reaction groups to be excluded, and the diffusion group to be included. This translates to a system of inequalities for $\\lambda$:\n1.  Advection excluded: $\\lambda \\ge \\frac{1}{n w_{\\mathrm{adv}}} \\|X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y}\\|_2$\n2.  Reaction excluded: $\\lambda \\ge \\frac{1}{n w_{\\mathrm{react}}} \\|X_{\\mathrm{react}}^{\\top} \\boldsymbol{y}\\|_2$\n3.  Diffusion included: $\\lambda  \\frac{1}{n w_{\\mathrm{diff}}} \\|X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y}\\|_2$\n\nThe minimal value of $\\lambda$ that ensures the exclusion of the advection and reaction groups is the maximum of their individual thresholds:\n$$ \\lambda_{\\min} = \\max\\left(\\frac{\\|X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y}\\|_2}{n w_{\\mathrm{adv}}}, \\frac{\\|X_{\\mathrm{react}}^{\\top} \\boldsymbol{y}\\|_2}{n w_{\\mathrm{react}}}\\right) $$\nUsing the given data:\n- $n = 50$\n- $w_{\\mathrm{adv}} = \\sqrt{d_{\\mathrm{adv}}} = \\sqrt{2}$\n- $w_{\\mathrm{react}} = \\sqrt{d_{\\mathrm{react}}} = \\sqrt{3}$\n- $\\|X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y}\\|_2 = \\left\\| \\begin{pmatrix} 9 \\\\ -3 \\end{pmatrix} \\right\\|_2 = \\sqrt{9^2 + (-3)^2} = \\sqrt{90} = 3\\sqrt{10}$\n- $\\|X_{\\mathrm{react}}^{\\top} \\boldsymbol{y}\\|_2 = \\left\\| \\begin{pmatrix} 4 \\\\ -2 \\\\ 1 \\end{pmatrix} \\right\\|_2 = \\sqrt{4^2 + (-2)^2 + 1^2} = \\sqrt{21}$\n\nThe thresholds are:\n$$ \\lambda_{\\mathrm{adv}}^{\\text{thresh}} = \\frac{3\\sqrt{10}}{50\\sqrt{2}} = \\frac{3\\sqrt{5}}{50} $$\n$$ \\lambda_{\\mathrm{react}}^{\\text{thresh}} = \\frac{\\sqrt{21}}{50\\sqrt{3}} = \\frac{\\sqrt{7}}{50} $$\nTo find the maximum, we compare $3\\sqrt{5}$ and $\\sqrt{7}$. Squaring them gives $(3\\sqrt{5})^2 = 45$ and $(\\sqrt{7})^2 = 7$. Since $45 > 7$, the advection threshold is larger.\nTherefore, the minimal regularization level is:\n$$ \\lambda_{\\min} = \\frac{3\\sqrt{5}}{50} $$\nWe must also verify that this value is less than the diffusion threshold: $\\lambda_{\\min}  \\frac{\\|X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y}\\|_2}{n w_{\\mathrm{diff}}} = \\frac{22}{50 \\cdot 1} = \\frac{22}{50}$. This is true, since $(3\\sqrt{5})^2 = 45$ and $22^2 = 484$. Thus, at $\\lambda = \\lambda_{\\min}$, the diffusion group remains active, satisfying all conditions.",
            "answer": "$$\\boxed{\\frac{3\\sqrt{5}}{50}}$$"
        },
        {
            "introduction": "Environmental models often include unmeasured, or latent, processes that are critical to system dynamics but pose a significant challenge for identification. This practice  delves into this problem by framing model identifiability in the context of a linear state-space model with a latent component. By applying the fundamental concept of observability from linear systems theory, you will determine the conditions under which the complete, augmented state of the system can be uniquely reconstructed from the available measurements.",
            "id": "3895655",
            "problem": "Consider an environmental single-compartment mass balance model for a dissolved nutrient in a lake, where the compartment stores mass $x_t$ at discrete time $t$ with fixed time step $\\Delta t$. The baseline conservation law is derived from mass balance: the rate of change equals inflow minus first-order removal. After linearization and time discretization, the baseline discrete-time dynamics are represented as $x_{t+1} = a x_t + b u_t$, where $u_t$ is the known external mass input sequence and $a \\in (0,1)$ encodes removal over one time step. Suppose an unmodeled seasonal driver is hypothesized, represented as a latent process $z_t$ that is linearly coupled into the compartment dynamics and evolves as a first-order autoregression. The augmented state vector is $s_t = [x_t, z_t]^\\top$ and follows the linear time-invariant (LTI) dynamics\n$$\n\\begin{bmatrix}\nx_{t+1} \\\\\nz_{t+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na  \\gamma \\\\\n0  \\phi\n\\end{bmatrix}\n\\begin{bmatrix}\nx_t \\\\\nz_t\n\\end{bmatrix}\n+\n\\begin{bmatrix}\nb \\\\\n0\n\\end{bmatrix}\nu_t,\n$$\nwhere $\\gamma$ quantifies the coupling of the latent driver into the compartment and $\\phi \\in (-1,1)$ is the autoregressive coefficient of the latent process. The measurement of the system is a single scalar output\n$$\ny_t = [1,\\ \\lambda] \\begin{bmatrix}x_t \\\\ z_t\\end{bmatrix},\n$$\nwhere $\\lambda$ captures any linear cross-sensitivity of the sensor to the latent process. In this setting, model structure identification requires determining whether augmenting the state with the latent process yields a structure in which the augmented state $[x_t, z_t]^\\top$ is reconstructible from the output sequence $y_t$ and the known input sequence $u_t$.\n\nStarting only from the conservation law (mass balance) and the definition of structural identifiability for linear time-invariant systems (the ability to uniquely infer the initial augmented state from a finite sequence of outputs and inputs), derive a criterion that decides whether the augmented state is structurally identifiable for the given parameters $(a,\\phi,\\gamma,\\lambda)$. Based on that criterion, implement an algorithm that, for each parameter set, returns a boolean that is $True$ if the augmented state is structurally identifiable and $False$ otherwise. Use a numerical rank test with a tolerance $\\epsilon = 10^{-12}$, such that matrices with singular values less than or equal to $\\epsilon$ are treated as rank-deficient.\n\nNo physical units are required in the final answers since the analysis pertains to structural properties of linear mappings after nondimensionalization. Angles are not involved, and no percentages are required.\n\nTest suite parameters to evaluate in the program:\n- Case 1 (general coupling present): $(a,\\phi,\\gamma,\\lambda) = (0.85, 0.70, 0.10, 0.00)$.\n- Case 2 (no coupling, no cross-sensitivity): $(a,\\phi,\\gamma,\\lambda) = (0.85, 0.70, 0.00, 0.00)$.\n- Case 3 (no coupling, cross-sensitivity present, distinct dynamics): $(a,\\phi,\\gamma,\\lambda) = (0.85, 0.70, 0.00, 0.50)$.\n- Case 4 (no coupling, cross-sensitivity present, identical dynamics): $(a,\\phi,\\gamma,\\lambda) = (0.80, 0.80, 0.00, 0.50)$.\n- Case 5 (nontrivial cancellation in identifiability): $(a,\\phi,\\gamma,\\lambda) = (0.60, 0.90, -0.30, 1.00)$.\n- Case 6 (near-zero coupling to test numerical robustness): $(a,\\phi,\\gamma,\\lambda) = (0.90, 0.50, 1\\times 10^{-14}, 0.00)$.\n- Case 7 (mixed signs, identifiable): $(a,\\phi,\\gamma,\\lambda) = (0.50, 0.80, -0.10, 0.40)$.\n\nYour program should produce a single line of output containing the results for these seven cases as a comma-separated list enclosed in square brackets (for example, $[result_1,result_2,\\dots,result_7]$), where each $result_i$ is a boolean as defined above.",
            "solution": "The problem asks for a criterion to determine if the augmented state vector of a linear time-invariant (LTI) system is structurally identifiable. The system is described by the state-space equations:\n$$\ns_{t+1} = A s_t + B u_t\n$$\n$$\ny_t = C s_t\n$$\nwhere $s_t = [x_t, z_t]^\\top$ is the augmented state vector, $u_t$ is the known scalar input, and $y_t$ is the scalar output. The matrices are defined as:\n$$\nA = \\begin{bmatrix} a  \\gamma \\\\ 0  \\phi \\end{bmatrix}, \\quad B = \\begin{bmatrix} b \\\\ 0 \\end{bmatrix}, \\quad C = [1, \\lambda]\n$$\nThe parameters $(a, \\phi, \\gamma, \\lambda)$ define the model structure. Structural identifiability is defined as the ability to uniquely determine the initial state $s_0$ from a finite sequence of outputs $\\{y_t\\}$ and inputs $\\{u_t\\}$. This property is known in linear systems theory as observability or reconstructibility.\n\nTo derive the criterion, we express the outputs in terms of the initial state $s_0$ and the input sequence. By recursively substituting the state equation, we can write the state at time $t$ as:\n$$\ns_t = A^t s_0 + \\sum_{j=0}^{t-1} A^{t-1-j} B u_j\n$$\nThe output at time $t$ is therefore:\n$$\ny_t = C s_t = C A^t s_0 + C \\sum_{j=0}^{t-1} A^{t-1-j} B u_j\n$$\nSince the input sequence $\\{u_t\\}$ and the system matrices $A$, $B$, and $C$ are known, the second term, which represents the forced response, can be computed and subtracted from the measured output $y_t$. Let's define a modified output $\\tilde{y}_t$ which represents the system's free response (the response due to the initial state $s_0$ alone):\n$$\n\\tilde{y}_t = y_t - C \\sum_{j=0}^{t-1} A^{t-1-j} B u_j = C A^t s_0\n$$\nThe problem of determining $s_0$ is now reduced to solving a system of linear equations using the known values of $\\tilde{y}_t$. For a state vector of dimension $n=2$, we need at least two such equations, collected over two time steps, $t=0$ and $t=1$:\n$$\n\\tilde{y}_0 = C A^0 s_0 = C s_0\n$$\n$$\n\\tilde{y}_1 = C A^1 s_0 = C A s_0\n$$\nWe can write this in matrix form:\n$$\n\\begin{bmatrix} \\tilde{y}_0 \\\\ \\tilde{y}_1 \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} s_0\n$$\nThe initial state $s_0$ can be uniquely determined if and only if the matrix multiplying it is invertible. This matrix is the observability matrix, denoted $\\mathcal{O}$, for the pair $(A, C)$.\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}\n$$\nFor an $n$-dimensional state space, the system is observable if and only if the observability matrix $\\mathcal{O}$ has full rank, which is $n$. In our case, $n=2$, so the condition for identifiability is $\\text{rank}(\\mathcal{O}) = 2$.\n\nLet's construct the matrix $\\mathcal{O}$ for the given system.\nThe first row is the matrix $C$:\n$$\nC = [1, \\lambda]\n$$\nThe second row is the product $CA$:\n$$\nCA = [1, \\lambda] \\begin{bmatrix} a  \\gamma \\\\ 0  \\phi \\end{bmatrix} = [1 \\cdot a + \\lambda \\cdot 0, \\quad 1 \\cdot \\gamma + \\lambda \\cdot \\phi] = [a, \\gamma + \\lambda\\phi]\n$$\nCombining these rows, the observability matrix is:\n$$\n\\mathcal{O} = \\begin{bmatrix} 1  \\lambda \\\\ a  \\gamma + \\lambda\\phi \\end{bmatrix}\n$$\nThe system is structurally identifiable if this $2 \\times 2$ matrix has rank $2$. A matrix has full rank if and only if its determinant is non-zero.\n$$\n\\det(\\mathcal{O}) = (1)(\\gamma + \\lambda\\phi) - (\\lambda)(a) = \\gamma + \\lambda\\phi - \\lambda a = \\gamma + \\lambda(\\phi - a)\n$$\nThus, the analytical criterion for structural identifiability is:\n$$\n\\gamma + \\lambda(\\phi - a) \\neq 0\n$$\nThis condition has a clear physical interpretation. The state $z_t$ can be reconstructed if it either affects the state $x_t$ (i.e., $\\gamma \\neq 0$), which is directly part of the measurement, or if it is directly sensed (i.e., $\\lambda \\neq 0$). In the latter case ($\\gamma=0, \\lambda \\neq 0$), its dynamics must be distinct from those of $x_t$ (i.e., $\\phi \\neq a$) to avoid being perfectly confounded with the signal from $x_t$. If $\\phi=a$, the two modes are aliased and cannot be distinguished.\n\nFor the numerical implementation, the problem requires a rank test based on singular values. The rank of a matrix is the number of its non-zero singular values. In numerical computation, we check if the singular values are greater than a small tolerance $\\epsilon$. A matrix is considered rank-deficient if any of its singular values are less than or equal to this tolerance. The condition $\\text{rank}(\\mathcal{O}) = 2$ is numerically equivalent to checking if the smallest singular value of $\\mathcal{O}$, denoted $\\sigma_{\\min}(\\mathcal{O})$, is greater than the given tolerance $\\epsilon = 10^{-12}$.\n\nThe algorithm to be implemented is as follows:\nFor each given set of parameters $(a, \\phi, \\gamma, \\lambda)$:\n1.  Construct the observability matrix $\\mathcal{O} = \\begin{bmatrix} 1  \\lambda \\\\ a  \\gamma + \\lambda\\phi \\end{bmatrix}$.\n2.  Compute the singular values of $\\mathcal{O}$.\n3.  Find the minimum singular value, $\\sigma_{\\min}$.\n4.  The system is deemed identifiable if $\\sigma_{\\min}  10^{-12}$. The result is $True$ in this case, and $False$ otherwise.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies the structural identifiability criterion for a 2D LTI system.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, phi, gamma, lambda)\n        (0.85, 0.70, 0.10, 0.00),         # Case 1: general coupling present\n        (0.85, 0.70, 0.00, 0.00),         # Case 2: no coupling, no cross-sensitivity\n        (0.85, 0.70, 0.00, 0.50),         # Case 3: no coupling, cross-sensitivity, distinct dynamics\n        (0.80, 0.80, 0.00, 0.50),         # Case 4: no coupling, cross-sensitivity, identical dynamics\n        (0.60, 0.90, -0.30, 1.00),        # Case 5: nontrivial cancellation\n        (0.90, 0.50, 1e-14, 0.00),        # Case 6: near-zero coupling\n        (0.50, 0.80, -0.10, 0.40)         # Case 7: mixed signs, identifiable\n    ]\n\n    # Numerical tolerance for rank test\n    epsilon = 1e-12\n    \n    results = []\n    for case in test_cases:\n        a, phi, gamma, lambda_ = case\n        \n        # Construct the observability matrix O = [[C], [CA]]\n        # C = [1, lambda]\n        # CA = [a, gamma + lambda * phi]\n        observability_matrix = np.array([\n            [1.0, lambda_],\n            [a, gamma + lambda_ * phi]\n        ])\n        \n        # Compute the singular values of the observability matrix.\n        # We only need the singular values, not the full SVD.\n        singular_values = np.linalg.svd(observability_matrix, compute_uv=False)\n        \n        # The rank of the matrix is numerically determined by the number of singular\n        # values greater than the tolerance. For a 2x2 matrix to have full rank (rank 2),\n        # its smallest singular value must be greater than the tolerance.\n        min_singular_value = np.min(singular_values)\n        \n        # The state is structurally identifiable if the matrix has full rank.\n        is_identifiable = min_singular_value  epsilon\n        \n        results.append(is_identifiable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}