{
    "hands_on_practices": [
        {
            "introduction": "本练习提供了高斯过程（GP）模拟器的基础实践。您将实现 GP 预测的核心方程，并探索如何通过添加一个经过策略性选择的新数据点来降低模型的预测不确定性——这是主动学习中的一个关键概念。这项实践旨在为您建立关于 GP 如何平衡数据和先验信念的深入直观理解。",
            "id": "3891179",
            "problem": "考虑一个用作平滑环境响应函数代理模型的一维高斯过程（GP）模拟器。高斯过程（GP）由一个均值函数和一个协方差函数定义。假设先验均值为零，并使用平方指数协方差函数，其形式为$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right),$$ 其中 $\\sigma_f^2$ 是信号方差，$\\ell$ 是特征长度尺度。观测值服从方差为 $\\sigma_n^2$ 的独立同分布高斯噪声。对于给定的训练数据集 $(\\mathbf{X}, \\mathbf{y})$（其中 $\\mathbf{X} \\in \\mathbb{R}^n$ 且 $\\mathbf{y} \\in \\mathbb{R}^n$），训练输出与测试点 $x^\\star$ 处函数值的联合分布是多变量正态分布。因此，可以通过多变量正态分布的条件性质得到在 $x^\\star$ 处的预测分布。\n\n您的任务是实现一个程序，该程序能够：\n- 对于给定的小型数据集和超参数，计算在指定测试位置 $x^\\star$ 处的高斯过程预测均值和方差。\n- 通过在一个从有限候选集中策略性选择的位置 $x_{\\text{new}}$ 添加一个额外观测值，以最小化增广后在 $x^\\star$ 处的预测方差，并量化在 $x^\\star$ 处的不确定性减少量。新观测值假定与训练数据具有相同的噪声方差 $\\sigma_n^2$。不确定性减少量定义为添加新观测值前后 $x^\\star$ 处预测方差的非负差值。\n\n仅使用多变量正态条件作用、平方指数协方差函数和独立高斯观测噪声的基础知识。所有量均为无量纲。\n\n对于下方的每个测试用例，计算：\n- 在 $x^\\star$ 处的预测均值，记为 $\\mu(x^\\star)$，为一个实数。\n- 在 $x^\\star$ 处的预测方差，记为 $\\sigma^2(x^\\star)$，为一个非负实数。\n- 通过添加从提供的候选集中选择的最优单个新观测值而在 $x^\\star$ 处实现的不确定性减少量，记为 $\\Delta\\sigma^2(x^\\star) = \\sigma^2(x^\\star) - \\sigma^2_{\\text{aug}}(x^\\star)$，其中 $\\sigma^2_{\\text{aug}}(x^\\star)$ 是用该单个观测值增广数据集后在 $x^\\star$ 处的预测方差。\n\n测试套件（每个测试用例指定为 $(\\mathbf{X}, \\mathbf{y}, \\sigma_f, \\ell, \\sigma_n, x^\\star, \\mathcal{C})$）：\n1. $(\\mathbf{X}=[0.1,0.4,0.7],\\,\\mathbf{y}=[0.2,0.6,0.1],\\,\\sigma_f=1.0,\\,\\ell=0.2,\\,\\sigma_n=0.05,\\,x^\\star=0.5,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n2. $(\\mathbf{X}=[0.1,0.4,0.7],\\,\\mathbf{y}=[0.2,0.6,0.1],\\,\\sigma_f=1.0,\\,\\ell=0.05,\\,\\sigma_n=0.05,\\,x^\\star=0.5,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n3. $(\\mathbf{X}=[0.0,0.3,0.9],\\,\\mathbf{y}=[0.0,0.5,-0.2],\\,\\sigma_f=1.5,\\,\\ell=0.5,\\,\\sigma_n=0.01,\\,x^\\star=0.25,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n4. $(\\mathbf{X}=[0.1,0.4,0.7],\\,\\mathbf{y}=[0.2,0.6,0.1],\\,\\sigma_f=1.0,\\,\\ell=0.2,\\,\\sigma_n=0.10,\\,x^\\star=0.4,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n\n您的程序必须：\n- 对于每个测试用例，计算 $\\mu(x^\\star)$、$\\sigma^2(x^\\star)$ 和 $\\Delta\\sigma^2(x^\\star)$，其中 $x_{\\text{new}}$ 从 $\\mathcal{C}$ 中选择，以最小化在 $x^\\star$ 处的增广预测方差。\n- 使用数值稳定的线性代数运算，并确保在数值舍入导致负值时，将预测方差裁剪为非负值。\n- 生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素是对应每个测试用例的一个包含三个十进制数的内部列表 $[\\mu(x^\\star),\\sigma^2(x^\\star),\\Delta\\sigma^2(x^\\star)]$，小数点后保留六位数字，且无空格。例如：“[[0.123456,0.234567,0.001000],[...],...]”。",
            "solution": "该问题定义明确，具有科学依据，并包含进行求解所需的所有信息。\n\n目标是计算高斯过程（GP）模拟器在特定点 $x^\\star$ 的预测均值和方差，然后通过添加一个新观测值来量化在该点上的最大不确定性减少量。这个新观测值的位置 $x_{\\text{new}}$ 从一个有限候选集 $\\mathcal{C}$ 中选择，以贪心法最小化在 $x^\\star$ 处的后验预测方差。\n\n高斯过程定义了函数上的先验分布。从 GP 中抽取的函数 $f(x)$ 由一个均值函数 $m(x)$ 和一个协方差函数（或核函数）$k(x, x')$ 指定。在此问题中，给定先验均值为零，即 $m(x) = 0$，以及一个平方指数协方差函数：\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)$$\n此处，$\\sigma_f^2$ 是信号方差，控制函数的整体振幅；$\\ell$ 是特征长度尺度，决定函数的光滑度或“摆动性”。\n\n我们给定一组 $n$ 个训练观测值 $(\\mathbf{X}, \\mathbf{y})$，其中 $\\mathbf{X} = \\{x_1, \\dots, x_n\\}$ 是输入位置，$\\mathbf{y} = \\{y_1, \\dots, y_n\\}$ 是对应的带噪声输出。观测模型为 $y_i = f(x_i) + \\epsilon_i$，其中噪声 $\\epsilon_i$ 是独立同分布的，服从 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$。\n\n高斯过程回归的核心在于对一个联合多变量正态分布进行条件化。在训练点上的潜在函数值 $\\mathbf{f} = [f(x_1), \\dots, f(x_n)]^T$ 与新测试点 $x^\\star$ 处的潜在函数值（记为 $f^\\star = f(x^\\star)$）的联合分布为：\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N}\n\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK(\\mathbf{X}, \\mathbf{X}) & K(\\mathbf{X}, x^\\star) \\\\\nK(x^\\star, \\mathbf{X}) & k(x^\\star, x^\\star)\n\\end{pmatrix}\n\\right)\n$$\n其中 $K(\\mathbf{X}, \\mathbf{X})$ 是 $n \\times n$ 的协方差矩阵，其元素为 $[K(\\mathbf{X}, \\mathbf{X})]_{ij} = k(x_i, x_j)$；$K(\\mathbf{X}, x^\\star)$ 是 $n \\times 1$ 的协方差向量，其元素为 $[K(\\mathbf{X}, x^\\star)]_i = k(x_i, x^\\star)$；$k(x^\\star, x^\\star)$ 是测试点处的先验方差。\n\n观测到的训练输出 $\\mathbf{y}$ 通过 $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{f}, \\sigma_n^2 \\mathbf{I})$ 与潜在值 $\\mathbf{f}$ 相关。通过对潜在变量 $\\mathbf{f}$ 进行积分，我们得到观测输出 $\\mathbf{y}$ 和潜在测试值 $f^\\star$ 的联合分布：\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N}\n\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I} & K(\\mathbf{X}, x^\\star) \\\\\nK(x^\\star, \\mathbf{X}) & k(x^\\star, x^\\star)\n\\end{pmatrix}\n\\right)\n$$\n其中 $\\mathbf{I}$ 是 $n \\times n$ 的单位矩阵。\n\n使用多变量正态分布的标准化条件规则，条件（后验）分布 $p(f^\\star | \\mathbf{X}, \\mathbf{y}, x^\\star)$ 也是一个高斯分布，即 $p(f^\\star | \\mathbf{X}, \\mathbf{y}, x^\\star) = \\mathcal{N}(\\mu(x^\\star), \\sigma^2(x^\\star))$，其均值和方差由以下公式给出：\n$$ \\mu(x^\\star) = K(x^\\star, \\mathbf{X}) [K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1} \\mathbf{y} $$\n$$ \\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, \\mathbf{X}) [K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1} K(\\mathbf{X}, x^\\star) $$\n为了数值稳定性，不直接计算矩阵的逆 $[K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1}$。而是求解一个线性方程组。令 $K_y = K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}$。该矩阵是对称正定的。我们可以使用 Cholesky 分解，$K_y = L L^T$，其中 $L$ 是一个下三角矩阵。计算过程变为：\n1. 使用前向替换求解 $L \\boldsymbol{\\alpha} = \\mathbf{y}$ 以得到 $\\boldsymbol{\\alpha}$。\n2. 使用后向替换求解 $L^T \\mathbf{w} = \\boldsymbol{\\alpha}$ 以得到 $\\mathbf{w}$。此时 $\\mathbf{w} = K_y^{-1} \\mathbf{y}$。\n3. 均值为 $\\mu(x^\\star) = K(x^\\star, \\mathbf{X}) \\mathbf{w}$。\n4. 使用前向替换求解 $L \\mathbf{v} = K(\\mathbf{X}, x^\\star)$ 以得到 $\\mathbf{v}$。\n5. 方差为 $\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - \\mathbf{v}^T \\mathbf{v}$。\n\n每个测试用例的程序如下：\n\n1.  **计算初始预测**：给定初始训练集 $(\\mathbf{X}, \\mathbf{y})$ 和超参数 $(\\sigma_f, \\ell, \\sigma_n)$，使用上述公式计算测试点 $x^\\star$ 处的初始预测均值 $\\mu(x^\\star)$ 和方差 $\\sigma^2(x^\\star)$。\n\n2.  **寻找最优增广**：为从候选集 $\\mathcal{C}$ 中找到最优的新观测点 $x_{\\text{new}}$，我们遍历每个候选点 $x_c \\in \\mathcal{C}$。对于每个 $x_c$，我们形成一个增广输入集 $\\mathbf{X}_{\\text{aug}} = \\mathbf{X} \\cup \\{x_c\\}$。预测方差 $\\sigma^2(x^\\star)$ 仅取决于输入位置，而不取决于观测到的输出值 $\\mathbf{y}$。因此，我们可以计算在 $x_c$ 处增广数据集后在 $x^\\star$ 处的假设后验方差，记为 $\\sigma^2_{\\text{aug}}(x^\\star; x_c)$。这使用相同的方差公式计算，但用 $\\mathbf{X}_{\\text{aug}}$ 代替 $\\mathbf{X}$：\n    $$ \\sigma^2_{\\text{aug}}(x^\\star; x_c) = k(x^\\star, x^\\star) - K(x^\\star, \\mathbf{X}_{\\text{aug}}) [K(\\mathbf{X}_{\\text{aug}}, \\mathbf{X}_{\\text{aug}}) + \\sigma_n^2 \\mathbf{I}]^{-1} K(\\mathbf{X}_{\\text{aug}}, x^\\star) $$\n    然后我们确定最小可能增广方差 $\\sigma^2_{\\text{min,aug}}(x^\\star) = \\min_{x_c \\in \\mathcal{C}} \\sigma^2_{\\text{aug}}(x^\\star; x_c)$。\n\n3.  **计算不确定性减少量**：不确定性减少量是初始方差与增广后可达到的最小方差之差：\n    $$ \\Delta\\sigma^2(x^\\star) = \\sigma^2(x^\\star) - \\sigma^2_{\\text{min,aug}}(x^\\star) $$\n    根据构造，添加数据只能减少（或保持）后验方差，因此 $\\Delta\\sigma^2(x^\\star) \\ge 0$。由数值浮点不精确性产生的任何小的负值将被裁剪为 $0$。\n\n对每个测试用例重复这整个过程，以产生所需的三元组值：$[\\mu(x^\\star), \\sigma^2(x^\\star), \\Delta\\sigma^2(x^\\star)]$。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian Process regression problem for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # (X, y, sigma_f, l, sigma_n, x_star, C)\n        ([0.1, 0.4, 0.7], [0.2, 0.6, 0.1], 1.0, 0.2, 0.05, 0.5, np.linspace(0.0, 1.0, 11)),\n        ([0.1, 0.4, 0.7], [0.2, 0.6, 0.1], 1.0, 0.05, 0.05, 0.5, np.linspace(0.0, 1.0, 11)),\n        ([0.0, 0.3, 0.9], [0.0, 0.5, -0.2], 1.5, 0.5, 0.01, 0.25, np.linspace(0.0, 1.0, 11)),\n        ([0.1, 0.4, 0.7], [0.2, 0.6, 0.1], 1.0, 0.2, 0.10, 0.4, np.linspace(0.0, 1.0, 11)),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y, sigma_f, l, sigma_n, x_star, C = case\n        \n        # Convert inputs to numpy arrays\n        X = np.asarray(X)\n        y = np.asarray(y)\n        C = np.asarray(C)\n\n        def squared_exp_kernel(x1, x2, sf, sl):\n            \"\"\"\n            Computes the squared exponential kernel between two sets of 1D points.\n            \"\"\"\n            x1 = x1.reshape(-1, 1)\n            x2 = x2.reshape(-1, 1)\n            # Use broadcasting to compute squared Euclidean distance matrix\n            sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n            return (sf**2) * np.exp(-0.5 * sqdist / (sl**2))\n\n        def get_pred_mean_and_var(x_train, y_train, x_test, sf, sl, sn):\n            \"\"\"\n            Computes the GP predictive mean and variance at x_test.\n            \"\"\"\n            n = len(x_train)\n            K = squared_exp_kernel(x_train, x_train, sf, sl)\n            Ky = K + (sn**2) * np.eye(n)\n            \n            k_star = squared_exp_kernel(x_train, x_test, sf, sl)\n            k_star_star = squared_exp_kernel(x_test, x_test, sf, sl)[0, 0]\n\n            try:\n                # Use Cholesky decomposition for stable computation\n                L, lower = linalg.cho_factor(Ky, lower=True, overwrite_a=False)\n                \n                # Compute mean: k_star.T @ inv(Ky) @ y\n                alpha = linalg.cho_solve((L, lower), y_train)\n                mean = k_star.T @ alpha\n                \n                # Compute variance: k_star_star - k_star.T @ inv(Ky) @ k_star\n                v = linalg.cho_solve((L, lower), k_star)\n                var = k_star_star - k_star.T @ v\n\n            except linalg.LinAlgError:\n                # Fallback in case of numerical instability, though unlikely here\n                inv_Ky = linalg.inv(Ky)\n                mean = k_star.T @ inv_Ky @ y_train\n                var = k_star_star - k_star.T @ inv_Ky @ k_star\n\n            # Ensure variance is non-negative due to potential floating point errors\n            var = np.maximum(0, var.item())\n            \n            return mean.item(), var\n\n        def get_pred_var(x_train, x_test, sf, sl, sn):\n            \"\"\"\n            Computes only the GP predictive variance.\n            \"\"\"\n            n = len(x_train)\n            K = squared_exp_kernel(x_train, x_train, sf, sl)\n            Ky = K + (sn**2) * np.eye(n)\n            \n            k_star = squared_exp_kernel(x_train, x_test, sf, sl)\n            k_star_star = squared_exp_kernel(x_test, x_test, sf, sl)[0, 0]\n\n            try:\n                L, lower = linalg.cho_factor(Ky, lower=True, overwrite_a=False)\n                v = linalg.cho_solve((L, lower), k_star)\n                var = k_star_star - k_star.T @ v\n            except linalg.LinAlgError:\n                inv_Ky = linalg.inv(Ky)\n                var = k_star_star - k_star.T @ inv_Ky @ k_star\n\n            var = np.maximum(0, var.item())\n            return var\n\n        # Step 1: Compute initial predictive mean and variance\n        x_star_np = np.array([x_star])\n        initial_mean, initial_var = get_pred_mean_and_var(X, y, x_star_np, sigma_f, l, sigma_n)\n\n        # Step 2: Find optimal new point and minimal augmented variance\n        min_aug_var = float('inf')\n        for x_new in C:\n            X_aug = np.append(X, x_new)\n            # Sorting might help stability but is not strictly necessary\n            # X_aug = np.unique(np.sort(X_aug))\n            var_aug = get_pred_var(X_aug, x_star_np, sigma_f, l, sigma_n)\n            if var_aug  min_aug_var:\n                min_aug_var = var_aug\n        \n        # Step 3: Compute uncertainty reduction\n        uncertainty_reduction = initial_var - min_aug_var\n        uncertainty_reduction = max(0.0, uncertainty_reduction)\n\n        results.append([initial_mean, initial_var, uncertainty_reduction])\n\n    # Format output string\n    formatted_results = []\n    for res in results:\n        formatted_results.append(f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\")\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "接下来，我们将转向另一类代理模型，本练习聚焦于多项式混沌展开（PCE）。您将使用最小二乘回归方法，在赫米特多项式基上构建一个 PCE 模型，并分析系数估计的质量如何依赖于实验设计。这项练习突出了代理建模与经典统计回归理论之间的联系。",
            "id": "3891168",
            "problem": "要求您实现一个多项式混沌展开（PCE）仿真器，并在与环境和地球系统建模相关的受控环境中量化估计量的不确定性。考虑一个不确定的、标准化的环境驱动因子向量 $\\mathbf{Z} = (Z_1,\\dots,Z_d)$，其中每个 $Z_i$ 独立同分布于标准正态分布，$Z_i \\sim \\mathcal{N}(0,1)$。代理响应代表一个无量纲的环境量，由一个确定性映射 $g:\\mathbb{R}^d \\to \\mathbb{R}$ 定义，具体如下\n$$\ng(\\mathbf{z}) = a_0 + a_1 z_1 + a_2 z_2 + a_3 z_3 + a_{12} z_1 z_2 + a_{23} z_2 z_3 + b_1 \\tanh(c_1 z_1 + c_2 z_2) + d_1 z_1^2 + d_2 z_2^3 + e_1 z_3 z_4 + e_2 z_4^2 + e_3 z_5^3,\n$$\n其中系数固定为 $a_0=1$, $a_1=0.5$, $a_2=0.25$, $a_3=-0.15$, $a_{12}=0.2$, $a_{23}=-0.1$, $b_1=0.2$, $c_1=0.5$, $c_2=0.3$, $d_1=0.1$, $d_2=0.05$, $e_1=0.07$, $e_2=0.03$, $e_3=-0.02$。函数 $g(\\mathbf{z})$ 的定义涉及变量 $z_1$ 至 $z_5$。对于维度为 $d$ 的问题，任何下标 $i > d$ 的变量 $z_i$ 都被设为零。\n\n您的任务是使用相对于标准正态测度标准正交的多元 Hermite 基函数，为 $g(\\mathbf{Z})$ 构建一个多项式混沌展开（PCE）代理。令 $\\mathrm{He}_k(x)$ 表示 $k$ 次的概率论者 Hermite 多项式，使得 $\\mathrm{He}_0(x)=1$, $\\mathrm{He}_1(x)=x$，且递推关系 $\\,\\mathrm{He}_{k+1}(x)=x\\,\\mathrm{He}_k(x)-k\\,\\mathrm{He}_{k-1}(x)\\,$ 成立。单变量标准正交基函数为\n$$\n\\phi_k(x) = \\frac{\\mathrm{He}_k(x)}{\\sqrt{k!}},\n$$\n多元基由多重索引 $\\boldsymbol{\\alpha}=(\\alpha_1,\\dots,\\alpha_d)$ 索引，其形式为\n$$\n\\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z}) = \\prod_{i=1}^d \\phi_{\\alpha_i}(z_i).\n$$\n令全阶截断集为\n$$\n\\mathcal{A}_{d,p} = \\left\\{ \\boldsymbol{\\alpha}\\in\\mathbb{N}_0^d : \\sum_{i=1}^d \\alpha_i \\le p \\right\\},\n$$\n其基数为 $M=\\binom{d+p}{p}$。\n\n给定从 $\\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)$ 独立抽取的训练样本 $\\{\\mathbf{z}^{(j)}\\}_{j=1}^n$ 及其对应的响应 $y^{(j)}=g(\\mathbf{z}^{(j)})$，通过在基 $\\{\\Phi_{\\boldsymbol{\\alpha}}\\}_{\\boldsymbol{\\alpha}\\in\\mathcal{A}_{d,p}}$ 上进行普通最小二乘回归来拟合 PCE 系数 $\\mathbf{c}\\in\\mathbb{R}^M$。构建设计矩阵 $\\mathbf{X}\\in\\mathbb{R}^{n\\times M}$，其元素为\n$$\nX_{j,m} = \\Phi_{\\boldsymbol{\\alpha}^{(m)}}\\!\\left(\\mathbf{z}^{(j)}\\right),\n$$\n其中 $\\boldsymbol{\\alpha}^{(m)}$ 枚举了 $\\mathcal{A}_{d,p}$ 中的元素。普通最小二乘估计量求解以下问题\n$$\n\\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^M} \\left\\| \\mathbf{X}\\mathbf{c} - \\mathbf{y}\\right\\|_2^2,\n$$\n其中 $\\mathbf{y}=(y^{(1)},\\dots,y^{(n)})^\\top$。在线性回归的 Gauss–Markov 框架下，对于随机设计和零均值残差，残差方差的估计量为\n$$\n\\widehat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{\\max(n - M, 1)}, \\quad \\mathrm{RSS} = \\left\\| \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{c}}\\right\\|_2^2,\n$$\n系数协方差矩阵的估计量为\n$$\n\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) = \\widehat{\\sigma}^2\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger},\n$$\n其中 $\\left(\\cdot\\right)^{\\dagger}$ 表示 Moore–Penrose 伪逆。定义平均估计量方差为\n$$\n\\overline{v} = \\frac{1}{M}\\sum_{m=1}^M \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m},\n$$\n最大估计量方差为\n$$\nv_{\\max} = \\max_{1\\le m \\le M} \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m},\n$$\n以及 $\\mathbf{X}^\\top \\mathbf{X}$ 的谱条件数为\n$$\n\\kappa = \\|\\mathbf{X}^\\top \\mathbf{X}\\|_2 \\cdot \\left\\| \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\right\\|_2,\n$$\n该条件数通过 $2$-范数计算。\n\n实现一个完整的程序，该程序：\n- 使用固定的种子 $s=314159$ 生成训练样本 $\\mathbf{z}^{(j)}$ 以保证可复现性（所有测试用例使用相同的种子，但您可以为每个测试用例确定性地通过一个偏移量来改变生成器状态）。\n- 构建最高达到全阶 $p$ 的多元标准正交 Hermite 基。\n- 构建设计矩阵 $\\mathbf{X}$ 并使用普通最小二乘法拟合 $\\widehat{\\mathbf{c}}$。\n- 使用上述公式计算 $\\overline{v}$、$v_{\\max}$ 和 $\\kappa$。\n\n测试集：\n- 情况 1：$(d,p,n)=(3,2,10)$，其中 $M=\\binom{3+2}{2}=10$ 等于 $n$（一个方形设计）。\n- 情况 2：$(d,p,n)=(3,2,50)$，其中 $n=5M$（过采样）。\n- 情况 3：$(d,p,n)=(3,3,40)$，其中 $M=\\binom{3+3}{3}=20$ 且 $n=2M$。\n- 情况 4：$(d,p,n)=(5,2,63)$，其中 $M=\\binom{5+2}{2}=21$ 且 $n=3M$。\n- 情况 5：$(d,p,n)=(5,3,60)$，其中 $M=\\binom{5+3}{3}=56$ 且 $n$ 略高于 $M$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例贡献一个按 $[\\overline{v}, v_{\\max}, \\kappa]$ 顺序排列的、包含三个浮点数的内部列表。例如，输出格式必须与 $[ [\\overline{v}_1, v_{\\max,1}, \\kappa_1], [\\overline{v}_2, v_{\\max,2}, \\kappa_2], \\dots ]$ 完全一样，不得包含任何附加文本。",
            "solution": "该问题要求为给定的确定性函数 $g(\\mathbf{z})$ 构建一个多项式混沌展开（PCE）代理。该函数依赖于不确定性输入 $\\mathbf{Z} = (Z_1, \\dots, Z_d)$，其中每个 $Z_i$ 是一个独立的标准正态随机变量，$Z_i \\sim \\mathcal{N}(0,1)$。该任务还涉及量化 PCE 系数估计量的不确定性。该问题是适定的、科学上合理的，并为获得唯一的、可复现的解提供了所有必要信息。我们将通过逐步实现指定的流程来解决此问题。\n\n任务的核心是使用一个更简单的多项式模型来近似复杂的模型 $g(\\mathbf{Z})$，该模型代表某个环境量。这个代理模型，即 PCE，由一个截断级数展开给出：\n$$\n\\widehat{g}(\\mathbf{Z}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_{d,p}} c_{\\boldsymbol{\\alpha}} \\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{Z})\n$$\n其中 $\\{c_{\\boldsymbol{\\alpha}}\\}$ 是待定系数，$\\{\\Phi_{\\boldsymbol{\\alpha}}\\}$ 是多元多项式基函数，而 $\\mathcal{A}_{d,p}$ 是一个多重索引的截断集。\n\n程序步骤如下：\n\n1.  **基函数定义**：输入 $Z_i$ 是标准正态变量。对于标准正态概率测度，其正交多项式的自然选择是概率论者的 Hermite 多项式，记为 $\\mathrm{He}_k(x)$。问题指明了它们的递推定义：\n    $$\n    \\mathrm{He}_0(x) = 1 \\\\\n    \\mathrm{He}_1(x) = x \\\\\n    \\mathrm{He}_{k+1}(x) = x\\,\\mathrm{He}_k(x) - k\\,\\mathrm{He}_{k-1}(x)\n    $$\n    这些多项式是正交的，但不是标准正交的。相应的标准正交基函数 $\\phi_k(x)$ 定义为：\n    $$\n    \\phi_k(x) = \\frac{\\mathrm{He}_k(x)}{\\sqrt{k!}}\n    $$\n    使得当 $Z \\sim \\mathcal{N}(0,1)$ 时，有 $\\mathbb{E}[\\phi_j(Z) \\phi_k(Z)] = \\delta_{jk}$。对于一个 $d$ 维输入向量 $\\mathbf{z}=(z_1, \\dots, z_d)$，多元基函数 $\\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z})$ 由单变量函数的张量积构成，并由多重索引 $\\boldsymbol{\\alpha}=(\\alpha_1, \\dots, \\alpha_d) \\in \\mathbb{N}_0^d$ 索引：\n    $$\n    \\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z}) = \\prod_{i=1}^d \\phi_{\\alpha_i}(z_i)\n    $$\n    我们将实现一个函数，用于同时为多个 $x$ 值和多个次数 $k$ 计算 $\\mathrm{He}_k(x)$ 的值，并实现另一个函数来生成归一化所需的阶乘 $\\sqrt{k!}$。\n\n2.  **截断集生成**：为了进行实际计算，必须截断无限的 PCE 求和。问题指定了全阶截断方案，其中活性多重索引集 $\\mathcal{A}_{d,p}$ 包括所有其分量之和最多为总次数 $p$ 的 $\\boldsymbol{\\alpha}$：\n    $$\n    \\mathcal{A}_{d,p} = \\left\\{ \\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d : \\sum_{i=1}^d \\alpha_i \\le p \\right\\}\n    $$\n    基函数的数量 $M$ 是该集合的基数，由 $M = |\\mathcal{A}_{d,p}| = \\binom{d+p}{p}$ 给出。我们将实现一个递归算法，为给定的参数 $d$ 和 $p$ 生成这些多重索引。\n\n3.  **数据生成**：为了拟合 PCE 系数，我们需要一个训练数据集。该数据集包含从指定分布 $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ 中抽取的 $n$ 个输入向量样本 $\\{\\mathbf{z}^{(j)}\\}_{j=1}^n$ 和相应的模型响应 $y^{(j)} = g(\\mathbf{z}^{(j)})$。函数 $g(\\mathbf{z})$ 的形式如下：\n    $$\n    g(\\mathbf{z}) = 1 + 0.5 z_1 + 0.25 z_2 - 0.15 z_3 + 0.2 z_1 z_2 - 0.1 z_2 z_3 + 0.2 \\tanh(0.5 z_1 + 0.3 z_2) + 0.1 z_1^2 + 0.05 z_2^3 + 0.07 z_3 z_4 + 0.03 z_4^2 - 0.02 z_5^3\n    $$\n    并遵循规则：当 $i  d$ 时 $z_i=0$。根据问题描述，我们将使用带种子的伪随机数生成器以确保可复现性，并为每个测试用例设置一个确定性偏移。\n\n4.  **最小二乘回归**：通过最小化 PCE 代理与训练集上真实模型输出之间的平方误差来估计系数 $\\mathbf{c} = (c_{\\boldsymbol{\\alpha}^{(1)}}, \\dots, c_{\\boldsymbol{\\alpha}^{(M)}})^\\top$。这是一个普通最小二乘（OLS）问题：\n    $$\n    \\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^M} \\left\\| \\mathbf{X}\\mathbf{c} - \\mathbf{y}\\right\\|_2^2\n    $$\n    此处，$\\mathbf{y} = (y^{(1)}, \\dots, y^{(n)})^\\top$ 是模型响应向量，$\\mathbf{X}$ 是 $n \\times M$ 的设计矩阵。设计矩阵的每个元素 $X_{j,m}$ 是第 $m$ 个基函数 $\\Phi_{\\boldsymbol{\\alpha}^{(m)}}$ 在第 $j$ 个输入样本 $\\mathbf{z}^{(j)}$ 处的评价值：\n    $$\n    X_{j,m} = \\Phi_{\\boldsymbol{\\alpha}^{(m)}}\\!\\left(\\mathbf{z}^{(j)}\\right)\n    $$\n    我们将构建此矩阵，并使用标准的数值线性代数求解器 `numpy.linalg.lstsq` 来求解 $\\widehat{\\mathbf{c}}$，该求解器对于此任务是稳健且合适的。\n\n5.  **估计量不确定性分析**：最后一步是量化所估计系数 $\\widehat{\\mathbf{c}}$ 的统计不确定性。这种不确定性源于有限的样本量 $n$ 和模型的不充分性（即 PCE 的截断误差）。所需的度量指标从估计量的协方差矩阵 $\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}})$ 导出。\n    -   首先，我们计算残差平方和：$\\mathrm{RSS} = \\left\\| \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{c}}\\right\\|_2^2$。\n    -   接着，我们估计残差方差：$\\widehat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{\\max(n - M, 1)}$。分母 $\\max(n - M, 1)$ 正确地处理了自由度，包括 $n \\le M$ 的边界情况。\n    -   系数协方差矩阵的估计量为 $\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) = \\widehat{\\sigma}^2\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger}$，其中 $(\\cdot)^{\\dagger}$ 表示 Moore-Penrose 伪逆。我们使用伪逆来确保数值稳定性和正确性，特别是当 $\\mathbf{X}^\\top \\mathbf{X}$ 是奇异或病态的时候。\n    -   从这个协方差矩阵中，我们计算两个度量指标：\n        -   平均估计量方差：$\\overline{v} = \\frac{1}{M}\\mathrm{Tr}\\left(\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}})\\right) = \\frac{1}{M}\\sum_{m=1}^M \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m}$。\n        -   最大估计量方差：$v_{\\max} = \\max_{1\\le m \\le M} \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m}$。\n    -   最后，我们通过计算 Gram 矩阵的谱条件数来评估回归问题的条件：$\\kappa = \\mathrm{cond}(\\mathbf{X}^\\top \\mathbf{X}) = \\|\\mathbf{X}^\\top \\mathbf{X}\\|_2 \\cdot \\left\\| \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger} \\right\\|_2$。一个高的 $\\kappa$ 值表明设计矩阵近似共线，这会增大系数估计的方差。\n\n这些步骤将被封装在一个程序中，该程序将遍历五个指定的测试用例，为每个用例计算 $[\\overline{v}, v_{\\max}, \\kappa]$，并将输出格式化为单行的列表之列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import factorial\n\ndef solve():\n    \"\"\"\n    Main function to execute the Polynomial Chaos Expansion analysis for all test cases.\n    \"\"\"\n\n    # Coefficients for the g-function as defined in the problem.\n    COEFFS = {\n        'a0': 1.0, 'a1': 0.5, 'a2': 0.25, 'a3': -0.15,\n        'a12': 0.2, 'a23': -0.1,\n        'b1': 0.2, 'c1': 0.5, 'c2': 0.3,\n        'd1': 0.1, 'd2': 0.05,\n        'e1': 0.07, 'e2': 0.03, 'e3': -0.02\n    }\n    SEED = 314159\n\n    def g_model(z):\n        \"\"\"\n        Computes the deterministic response function g(z).\n        z is expected to be an array of shape (n_samples, d).\n        \"\"\"\n        n_samples, d = z.shape\n        # Pad with zeros to handle dimensions up to 5 consistently.\n        z_pad = np.zeros((n_samples, 5))\n        z_pad[:, :d] = z\n\n        z1, z2, z3, z4, z5 = z_pad.T\n        c = COEFFS\n\n        term_a = c['a0'] + c['a1'] * z1 + c['a2'] * z2 + c['a3'] * z3\n        term_a12_23 = c['a12'] * z1 * z2 + c['a23'] * z2 * z3\n        term_b = c['b1'] * np.tanh(c['c1'] * z1 + c['c2'] * z2)\n        term_d = c['d1'] * z1**2 + c['d2'] * z2**3\n        term_e = c['e1'] * z3 * z4 + c['e2'] * z4**2 + c['e3'] * z5**3\n\n        return term_a + term_a12_23 + term_b + term_d + term_e\n\n    def generate_multi_indices(d, p):\n        \"\"\"\n        Generates total-order multi-indices recursively.\n        \"\"\"\n        if d == 1:\n            return [[i] for i in range(p + 1)]\n        indices = []\n        for i in range(p + 1):\n            sub_indices = generate_multi_indices(d - 1, p - i)\n            for sub_index in sub_indices:\n                indices.append([i] + sub_index)\n        return indices\n\n    def hermite_poly_val(x, k_max):\n        \"\"\"\n        Evaluates probabilists' Hermite polynomials He_k(x) up to degree k_max.\n        x is a 1D array. Returns a matrix of shape (len(x), k_max+1).\n        \"\"\"\n        vals = np.zeros((len(x), k_max + 1))\n        if k_max >= 0:\n            vals[:, 0] = 1.0\n        if k_max >= 1:\n            vals[:, 1] = x\n        for k in range(1, k_max):\n            vals[:, k + 1] = x * vals[:, k] - k * vals[:, k - 1]\n        return vals\n\n    test_cases = [\n        (3, 2, 10),\n        (3, 2, 50),\n        (3, 3, 40),\n        (5, 2, 63),\n        (5, 3, 60),\n    ]\n\n    all_results = []\n    for i, (d, p, n) in enumerate(test_cases):\n        # Use seed with deterministic offset for each case for reproducibility.\n        rng = np.random.default_rng(SEED + i)\n\n        # 1. Generate multi-indices for the basis.\n        alphas = generate_multi_indices(d, p)\n        M = len(alphas)\n\n        # 2. Generate training data.\n        z_train = rng.standard_normal(size=(n, d))\n        y_train = g_model(z_train)\n\n        # 3. Construct the design matrix X.\n        X = np.ones((n, M))\n        \n        # Pre-compute univariate orthonormal basis function values.\n        max_deg = p\n        phi_vals = np.zeros((n, d, max_deg + 1))\n        fact_sqrt = np.sqrt(factorial(np.arange(max_deg + 1)))\n\n        for i_dim in range(d):\n            # hermite_poly_val returns shape (n, max_deg+1) for each dimension.\n            he_vals_dim = hermite_poly_val(z_train[:, i_dim], max_deg)\n            phi_vals[:, i_dim, :] = he_vals_dim / fact_sqrt\n\n        # Populate X using the tensor product structure.\n        for m, alpha in enumerate(alphas):\n            term_prod = np.ones(n)\n            for i_dim in range(d):\n                term_prod *= phi_vals[:, i_dim, alpha[i_dim]]\n            X[:, m] = term_prod\n            \n        # 4. Fit coefficients using Ordinary Least Squares.\n        c_hat, _, _, _ = np.linalg.lstsq(X, y_train, rcond=None)\n\n        # 5. Compute the required uncertainty quantification metrics.\n        y_pred = X @ c_hat\n        rss = np.sum((y_train - y_pred)**2)\n        \n        sigma2_hat = rss / max(n - M, 1)\n        \n        XtX = X.T @ X\n        XtX_pinv = np.linalg.pinv(XtX)\n        \n        cov_c_hat = sigma2_hat * XtX_pinv\n        \n        variances = np.diag(cov_c_hat)\n        v_bar = np.mean(variances)\n        v_max = np.max(variances)\n        \n        kappa = np.linalg.cond(XtX)\n        \n        all_results.append([v_bar, v_max, kappa])\n\n    # Format output as a single string line\n    outer_parts = []\n    for res in all_results:\n        inner_str = f\"[{','.join(map(str, res))}]\"\n        outer_parts.append(inner_str)\n    final_output = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这项练习综合了前面的概念，旨在解决不确定性分析中的一个核心挑战：同时传递来自模型输入和模拟器自身的不确定性。通过应用全方差定律，您将推导并计算某个目标量的总体预测不确定性。这个练习揭示了考虑模拟器不确定性的至关重要性，以避免得出过于自信的预测结论。",
            "id": "3891130",
            "problem": "您正在使用一个统计代理模型来为一个计算成本高昂的模拟器建模，该模型用于模拟环境和地球系统建模中一个感兴趣的标量，例如流域平均降水变化。该代理仿真器是基于高斯过程 (GP) 仿真器构建的，用于在给定不确定输入向量的情况下预测感兴趣的量。您必须计算一个 $0.95$ 的可信区间，该区间需要同时考虑输入不确定性和仿真器预测不确定性，并将其与忽略仿真器不确定性的区间进行比较。\n\n假设标量输出 $Y$ 具有以下分层预测模型：\n1. 不确定输入由一个随机向量 $X \\in \\mathbb{R}^d$ 表示，该向量服从多元正态分布 $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。\n2. 仿真器提供一个关于 $X$ 的线性预测均值，由 $m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$ 给出，其中 $\\beta_0 \\in \\mathbb{R}$ 且 $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$。\n3. 仿真器预测方差被建模为一个加性项 $v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2$，其中 $\\gamma_0 \\ge 0$ 且对于 $i=1,\\dots,d$，有 $\\gamma_i \\ge 0$。\n\n令 $Y \\mid X \\sim \\mathcal{N}(m(X), v_{\\mathrm{em}}(X))$。$Y$ 的边际预测分布对 $X$ 的不确定性进行积分，并结合了来自 $X$ 和仿真器的不确定性。通过设置 $v_{\\mathrm{em}}(X) = 0$，可以得到一个忽略仿真器不确定性的 $Y$ 的 $0.95$ 可信区间。\n\n仅从全期望定律、全方差定律和正态分布的标准性质出发，推导一个算法来计算 $Y$ 的 $0.95$ 可信区间端点，单位为毫米/天 (mm/day)，保留六位小数，并考虑两种情况：(a) 包括仿真器不确定性 和 (b) 忽略仿真器不确定性。\n\n您的程序必须实现该推导过程，并为以下测试套件计算区间。在所有情况下，$Y$ 的单位都是 毫米/天 (mm/day)，并且所有区间端点都必须以 毫米/天 (mm/day) 表示。\n\n- 测试用例 A（二维输入，相关，非零仿真器不确定性）：\n  - $d = 2$，\n  - $\\boldsymbol{\\mu} = [1.0, 0.5]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.09  0.03 \\\\ 0.03  0.04\\end{bmatrix}$，\n  - $\\beta_0 = 0.0$, $\\boldsymbol{\\beta} = [2.0, -1.5]$,\n  - $\\gamma_0 = 0.05$, $\\boldsymbol{\\gamma} = [0.02, 0.01]$.\n\n- 测试用例 B（相同输入，仿真器不确定性设为零；如果推导正确，区间将在此边界条件下重合）：\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [1.0, 0.5]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.09  0.03 \\\\ 0.03  0.04\\end{bmatrix}$，\n  - $\\beta_0 = 0.0$, $\\boldsymbol{\\beta} = [2.0, -1.5]$,\n  - $\\gamma_0 = 0.0$, $\\boldsymbol{\\gamma} = [0.0, 0.0]$.\n\n- 测试用例 C（确定性输入，仿真器方差恒定；在此边界条件下，忽略仿真器不确定性将产生一个退化区间）：\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [0.8, -0.4]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.0  0.0 \\\\ 0.0  0.0\\end{bmatrix}$，\n  - $\\beta_0 = 0.3$, $\\boldsymbol{\\beta} = [1.2, 0.5]$,\n  - $\\gamma_0 = 0.2$, $\\boldsymbol{\\gamma} = [0.0, 0.0]$.\n\n- 测试用例 D（一维输入，较大的仿真器方差随 $X^2$ 增加）：\n  - $d = 1$,\n  - $\\boldsymbol{\\mu} = [-0.2]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.25\\end{bmatrix}$，\n  - $\\beta_0 = 1.0$, $\\boldsymbol{\\beta} = [1.0]$,\n  - $\\gamma_0 = 0.1$, $\\boldsymbol{\\gamma} = [0.5]$.\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果是一个包含四个浮点数的子列表，顺序为 $[L_{\\mathrm{with}}, U_{\\mathrm{with}}, L_{\\mathrm{ignore}}, U_{\\mathrm{ignore}}]$，所有值均以 毫米/天 (mm/day) 为单位，并四舍五入到六位小数。例如，输出应类似于 $[[a,b,c,d],[e,f,g,h],\\dots]$，不含空格。",
            "solution": "该问题是有效的，因为它在科学上基于统计理论，问题定义良好、客观，并为推导解决方案提供了一套完整且一致的信息。任务是基于一个分层预测模型，计算一个感兴趣的标量 $Y$ 的 $0.95$ 可信区间。$Y$ 的边际分布必须考虑模型输入 $X$ 的不确定性以及用于预测 $Y$ 的统计仿真器的不确定性。我们将首先推导 $Y$ 的边际均值和方差，然后假设 $Y$ 的边际分布可以用正态分布近似，利用这些矩来构建可信区间。\n\n该模型由以下层次结构定义：\n1.  输入不确定性：输入向量 $X \\in \\mathbb{R}^d$ 服从多元正态分布，$X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。\n2.  仿真器模型：给定输入 $X$ 时，输出 $Y$ 的条件分布是正态的，$Y \\mid X \\sim \\mathcal{N}(m(X), v_{\\mathrm{em}}(X))$，其中预测均值为 $m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$，预测方差为 $v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2$。\n\n我们的目标是找到 $Y$ 的边际分布的参数，这可以通过对输入向量 $X$ 进行积分得到。然后，基于边际均值 $\\mathbb{E}[Y]$ 和边际方差 $\\mathrm{Var}(Y)$ 构建可信区间。\n\n**边际均值 $\\mathbb{E}[Y]$ 的推导**\n\n我们应用全期望定律，该定律指出 $\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]$。\n内部期望是给定 $X$ 时 $Y$ 的条件分布的均值，也就是仿真器的均值函数：\n$$\n\\mathbb{E}[Y \\mid X] = m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X\n$$\n外部期望是关于 $X$ 的分布求得的。由于期望是线性算子：\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\beta_0 + \\boldsymbol{\\beta}^\\top X] = \\beta_0 + \\boldsymbol{\\beta}^\\top \\mathbb{E}_X[X]\n$$\n已知 $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$，我们有 $\\mathbb{E}_X[X] = \\boldsymbol{\\mu}$。因此，$Y$ 的边际均值为：\n$$\n\\mu_Y = \\mathbb{E}[Y] = \\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\mu}\n$$\n这个结果与仿真器方差 $v_{\\mathrm{em}}(X)$ 无关，因此对问题中考虑的两种情况都成立。\n\n**边际方差 $\\mathrm{Var}(Y)$ 的推导**\n\n我们应用全方差定律，它将 $Y$ 的总方差分解为两个部分：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])\n$$\n我们分别计算每一项。\n\n1.  **第一项：$\\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)]$**\n    该项表示来自仿真器预测不确定性的贡献，它在输入 $X$ 的分布上取平均。给定 $X$ 时 $Y$ 的条件方差由仿真器的预测方差函数给出：\n    $$\n    \\mathrm{Var}(Y \\mid X) = v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2\n    $$\n    我们对这个表达式关于 $X$ 求期望：\n    $$\n    \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] = \\mathbb{E}_X\\left[\\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2\\right] = \\gamma_0 + \\sum_{i=1}^d \\gamma_i \\mathbb{E}_X[X_i^2]\n    $$\n    对于任何随机变量 $Z$，我们知道 $\\mathbb{E}[Z^2] = \\mathrm{Var}(Z) + (\\mathbb{E}[Z])^2$。对于随机向量 $X$ 的第 $i$ 个分量，我们有 $\\mathbb{E}[X_i] = \\mu_i$（$\\boldsymbol{\\mu}$ 的第 $i$ 个元素）和 $\\mathrm{Var}(X_i) = \\Sigma_{ii}$（$\\boldsymbol{\\Sigma}$ 的第 $i$ 个对角元素）。\n    将此代入我们的表达式中，得到：\n    $$\n    \\mathbb{E}_X[X_i^2] = \\Sigma_{ii} + \\mu_i^2\n    $$\n    因此，总方差的第一项是：\n    $$\n    \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] = \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2)\n    $$\n\n2.  **第二项：$\\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$**\n    该项表示通过仿真器的均值函数 $m(X)$ 从输入 $X$ 传播的不确定性。我们需要计算 $\\mathbb{E}[Y \\mid X] = m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$ 关于 $X$ 的方差：\n    $$\n    \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X]) = \\mathrm{Var}_X(\\beta_0 + \\boldsymbol{\\beta}^\\top X) = \\mathrm{Var}_X(\\boldsymbol{\\beta}^\\top X)\n    $$\n    对于协方差矩阵为 $\\boldsymbol{\\Sigma}$ 的随机向量 $X$，线性变换 $\\boldsymbol{\\beta}^\\top X$ 的方差由二次型 $\\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}$ 给出。\n    因此，第二项是：\n    $$\n    \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X]) = \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n    $$\n\n**总边际方差和区间构建**\n\n结合这两项，$Y$ 的总边际方差为：\n$$\n\\sigma_Y^2 = \\mathrm{Var}(Y) = \\left( \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2) \\right) + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n我们现在为所需的两种情况定义方差：\n\n**(a) 包括仿真器不确定性：**\n方差是上面推导出的总方差：\n$$\n\\sigma_{\\mathrm{with}}^2 = \\left( \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2) \\right) + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n**(b) 忽略仿真器不确定性：**\n这种情况对应于将仿真器方差 $v_{\\mathrm{em}}(X)$ 设置为 $0$。这等同于设置 $\\gamma_0=0$ 以及对于 $i=1,\\dots,d$ 的所有 $\\gamma_i=0$。总方差的第一项消失，只剩下通过均值函数传播的方差：\n$$\n\\sigma_{\\mathrm{ignore}}^2 = \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n**可信区间计算**\n将 $Y$ 的边际分布近似为正态分布 $\\mathcal{N}(\\mu_Y, \\sigma_Y^2)$，则 $Y$ 的一个 $0.95$ 可信区间由 $[\\mu_Y - z_{0.975} \\sigma_Y, \\mu_Y + z_{0.975} \\sigma_Y]$ 给出，其中 $z_{0.975}$ 是标准正态分布的 $0.975$ 分位数，约为 $1.959964$。\n\n两种情况的端点是：\n*   **包含不确定性：** $L_{\\mathrm{with}} = \\mu_Y - z_{0.975} \\sigma_{\\mathrm{with}}$ 和 $U_{\\mathrm{with}} = \\mu_Y + z_{0.975} \\sigma_{\\mathrm{with}}$。\n*   **忽略不确定性：** $L_{\\mathrm{ignore}} = \\mu_Y - z_{0.975} \\sigma_{\\mathrm{ignore}}$ 和 $U_{\\mathrm{ignore}} = \\mu_Y + z_{0.975} \\sigma_{\\mathrm{ignore}}$。\n\n以下程序实现了这个推导出的算法。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes 0.95 credible intervals for a scalar output from a GP emulator,\n    accounting for input and emulator uncertainty.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\n            \"d\": 2, \"mu\": [1.0, 0.5], \"Sigma\": [[0.09, 0.03], [0.03, 0.04]],\n            \"beta0\": 0.0, \"beta\": [2.0, -1.5], \"gamma0\": 0.05, \"gamma\": [0.02, 0.01]\n        },\n        # Test Case B\n        {\n            \"d\": 2, \"mu\": [1.0, 0.5], \"Sigma\": [[0.09, 0.03], [0.03, 0.04]],\n            \"beta0\": 0.0, \"beta\": [2.0, -1.5], \"gamma0\": 0.0, \"gamma\": [0.0, 0.0]\n        },\n        # Test Case C\n        {\n            \"d\": 2, \"mu\": [0.8, -0.4], \"Sigma\": [[0.0, 0.0], [0.0, 0.0]],\n            \"beta0\": 0.3, \"beta\": [1.2, 0.5], \"gamma0\": 0.2, \"gamma\": [0.0, 0.0]\n        },\n        # Test Case D\n        {\n            \"d\": 1, \"mu\": [-0.2], \"Sigma\": [[0.25]],\n            \"beta0\": 1.0, \"beta\": [1.0], \"gamma0\": 0.1, \"gamma\": [0.5]\n        }\n    ]\n\n    results = []\n    # Standard normal quantile for a 95% interval\n    z_quantile = norm.ppf(1 - (1 - 0.95) / 2)\n\n    for case in test_cases:\n        # Extract and convert parameters to numpy arrays\n        mu = np.array(case[\"mu\"])\n        Sigma = np.array(case[\"Sigma\"])\n        beta0 = case[\"beta0\"]\n        beta = np.array(case[\"beta\"])\n        gamma0 = case[\"gamma0\"]\n        gamma = np.array(case[\"gamma\"])\n\n        # --- DERIVATION IMPLEMENTATION ---\n\n        # 1. Calculate the marginal mean of Y, which is common to both cases.\n        # E[Y] = beta0 + beta.T @ E[X] = beta0 + beta.T @ mu\n        mu_Y = beta0 + beta.T @ mu\n\n        # 2. Calculate the variance for the case IGNORING emulator uncertainty.\n        # This is the variance of the emulator mean function, Var(m(X)).\n        # Var_X(E[Y|X]) = Var_X(beta0 + beta.T @ X) = beta.T @ Var(X) @ beta\n        var_ignore = beta.T @ Sigma @ beta\n\n        # 3. Calculate the variance for the case INCLUDING emulator uncertainty.\n        # This adds the average emulator variance, E[Var(Y|X)].\n        # E_X[v_em(X)] = E_X[gamma0 + sum(gamma_i * X_i^2)]\n        #             = gamma0 + sum(gamma_i * E[X_i^2])\n        # E[X_i^2] = Var(X_i) + (E[X_i])^2 = Sigma_ii + mu_i^2\n        E_v_em = gamma0 + gamma.T @ (np.diag(Sigma) + mu**2)\n        var_with = E_v_em + var_ignore\n\n        # 4. Calculate standard deviations\n        std_dev_with = np.sqrt(var_with)\n        std_dev_ignore = np.sqrt(var_ignore)\n\n        # 5. Compute interval endpoints for both cases\n        L_with = mu_Y - z_quantile * std_dev_with\n        U_with = mu_Y + z_quantile * std_dev_with\n        L_ignore = mu_Y - z_quantile * std_dev_ignore\n        U_ignore = mu_Y + z_quantile * std_dev_ignore\n\n        # 6. Format the results for the current case\n        case_result = [\n            round(L_with, 6),\n            round(U_with, 6),\n            round(L_ignore, 6),\n            round(U_ignore, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists creates the desired format\n    # with spaces, which are then removed.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}