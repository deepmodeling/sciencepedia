{
    "hands_on_practices": [
        {
            "introduction": "The power of Polynomial Chaos Expansions stems from using basis functions tailored to the probability distribution of the uncertain inputs. For the ubiquitous Gaussian distribution, the appropriate basis is the family of Hermite polynomials. This exercise  provides fundamental practice in constructing these essential building blocks and verifying their orthogonality by calculating their norms, a property that is critical for the entire PCE framework to function.",
            "id": "2589427",
            "problem": "In stochastic finite element analysis using generalized polynomial chaos expansions, Gaussian input fields are represented with polynomial bases orthogonal to an associated Gaussian weight. Let $\\xi$ be a standard normal random variable. To align with the physicists’ convention for Hermite polynomials and a Gaussian weight of the form $\\exp(-y^{2})$, consider the rescaled random variable $y=\\xi/\\sqrt{2}$, whose probability density is $w(y)=\\frac{1}{\\sqrt{\\pi}}\\exp(-y^{2})$. \n\nUsing the defining properties of the physicists’ Hermite polynomials $H_{n}(y)$ and the weighted $L^{2}$ norm $\\|p\\|_{w}=\\left(\\int_{-\\infty}^{\\infty}p(y)^{2}w(y)\\,\\mathrm{d}y\\right)^{1/2}$, perform the following:\n\n- Construct explicitly the first $4$ physicists’ Hermite polynomials $H_{0}(y)$, $H_{1}(y)$, $H_{2}(y)$, and $H_{3}(y)$ from their defining relation.\n- Compute their norms $\\|H_{n}\\|_{w}$ under the weight $w(y)=\\frac{1}{\\sqrt{\\pi}}\\exp(-y^{2})$ for $n\\in\\{0,1,2,3\\}$.\n\nReport as your final answer the row vector containing the four norms $\\big(\\|H_{0}\\|_{w},\\|H_{1}\\|_{w},\\|H_{2}\\|_{w},\\|H_{3}\\|_{w}\\big)$ in exact form. No rounding is required. The answer is dimensionless.",
            "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded in the theory of orthogonal polynomials and their application in uncertainty quantification, specifically generalized polynomial chaos (gPC) expansions. The problem is well-posed, objective, and contains all necessary information to derive a unique solution without ambiguity. We may proceed.\n\nThe task is to construct the first four physicists' Hermite polynomials, $H_n(y)$, and compute their norms with respect to the weight function $w(y)=\\frac{1}{\\sqrt{\\pi}}\\exp(-y^{2})$.\n\nFirst, we construct the polynomials $H_n(y)$ for $n \\in \\{0, 1, 2, 3\\}$. The physicists' Hermite polynomials are defined by several equivalent relations. The most direct for sequential construction is the recurrence relation:\n$$H_{n+1}(y) = 2y H_n(y) - 2n H_{n-1}(y)$$\nwith the initial polynomials given by:\n$$H_0(y) = 1$$\n$$H_1(y) = 2y$$\n\nUsing this recurrence, we compute $H_2(y)$ and $H_3(y)$.\n\nFor $n=1$:\n$$H_2(y) = 2y H_1(y) - 2(1) H_0(y) = 2y(2y) - 2(1) = 4y^2 - 2$$\n\nFor $n=2$:\n$$H_3(y) = 2y H_2(y) - 2(2) H_1(y) = 2y(4y^2 - 2) - 4(2y) = 8y^3 - 4y - 8y = 8y^3 - 12y$$\n\nThe first four physicists' Hermite polynomials are therefore:\n$H_0(y) = 1$\n$H_1(y) = 2y$\n$H_2(y) = 4y^2 - 2$\n$H_3(y) = 8y^3 - 12y$\n\nNext, we compute the norm $\\|H_n\\|_w$ for $n \\in \\{0, 1, 2, 3\\}$. The norm is defined as:\n$$\\|H_n\\|_w = \\left( \\int_{-\\infty}^{\\infty} [H_n(y)]^2 w(y) \\, \\mathrm{d}y \\right)^{1/2}$$\nSubstituting the given weight function $w(y) = \\frac{1}{\\sqrt{\\pi}}\\exp(-y^2)$, the square of the norm is:\n$$\\|H_n\\|_w^2 = \\int_{-\\infty}^{\\infty} [H_n(y)]^2 \\frac{1}{\\sqrt{\\pi}}\\exp(-y^2) \\, \\mathrm{d}y = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} [H_n(y)]^2 \\exp(-y^2) \\, \\mathrm{d}y$$\n\nThe physicists' Hermite polynomials satisfy the orthogonality relation with respect to the weight function $\\exp(-y^2)$:\n$$\\int_{-\\infty}^{\\infty} H_m(y) H_n(y) \\exp(-y^2) \\, \\mathrm{d}y = \\sqrt{\\pi} \\, 2^n n! \\, \\delta_{mn}$$\nwhere $\\delta_{mn}$ is the Kronecker delta.\n\nFor the case $m=n$, relevant to the norm calculation, this simplifies to:\n$$\\int_{-\\infty}^{\\infty} [H_n(y)]^2 \\exp(-y^2) \\, \\mathrm{d}y = \\sqrt{\\pi} \\, 2^n n!$$\n\nSubstituting this result into our expression for the squared norm, we find a remarkably simple form:\n$$\\|H_n\\|_w^2 = \\frac{1}{\\sqrt{\\pi}} \\left( \\sqrt{\\pi} \\, 2^n n! \\right) = 2^n n!$$\n\nThe norm is therefore given by the general formula:\n$$\\|H_n\\|_w = \\sqrt{2^n n!}$$\n\nWe now apply this formula for $n = 0, 1, 2, 3$:\n\nFor $n=0$:\n$$\\|H_0\\|_w = \\sqrt{2^0 \\cdot 0!} = \\sqrt{1 \\cdot 1} = 1$$\n\nFor $n=1$:\n$$\\|H_1\\|_w = \\sqrt{2^1 \\cdot 1!} = \\sqrt{2 \\cdot 1} = \\sqrt{2}$$\n\nFor $n=2$:\n$$\\|H_2\\|_w = \\sqrt{2^2 \\cdot 2!} = \\sqrt{4 \\cdot 2} = \\sqrt{8} = 2\\sqrt{2}$$\n\nFor $n=3$:\n$$\\|H_3\\|_w = \\sqrt{2^3 \\cdot 3!} = \\sqrt{8 \\cdot 6} = \\sqrt{48} = \\sqrt{16 \\cdot 3} = 4\\sqrt{3}$$\n\nThe requested row vector containing the four norms is $(\\|H_0\\|_w, \\|H_1\\|_w, \\|H_2\\|_w, \\|H_3\\|_w)$. This evaluates to $(1, \\sqrt{2}, 2\\sqrt{2}, 4\\sqrt{3})$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 & \\sqrt{2} & 2\\sqrt{2} & 4\\sqrt{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Once a model's uncertain output is represented as a Polynomial Chaos Expansion, one of the most significant advantages is the ability to compute its statistical moments with remarkable ease. This practice problem  guides you through deriving and applying the simple yet powerful formulas that link the PCE coefficients directly to the mean and variance of the output. This skill is central to the \"post-processing\" phase of any uncertainty quantification analysis using PCE.",
            "id": "2671650",
            "problem": "A straight prismatic bar of length $L$ is loaded in uniaxial tension by a deterministic force $P$ at its free end. The bar is linear elastic, and its uncertain material and geometric properties are modeled by a finite set of independent standardized random inputs collected in the vector $\\boldsymbol{\\xi}$. The tip displacement $Y$ at the loaded end is a square-integrable random variable. A Polynomial Chaos Expansion (PCE) representation is adopted for $Y$, written in an orthonormal basis $\\{\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\}$ with respect to the joint probability density of $\\boldsymbol{\\xi}$, so that\n$$\nY=\\sum_{\\alpha \\in \\mathcal{A}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}),\n$$\nwhere $\\mathcal{A}$ is a finite multi-index set, $c_{\\alpha} \\in \\mathbb{R}$ are deterministic coefficients, and $\\Psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi}) \\equiv 1$ is the constant polynomial. The orthonormality means that for any multi-indices $\\alpha$ and $\\beta$,\n$$\n\\mathbb{E}\\!\\left[\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\,\\Psi_{\\beta}(\\boldsymbol{\\xi})\\right]=\\delta_{\\alpha\\beta},\n$$\nwhere $\\delta_{\\alpha\\beta}$ is the Kronecker delta and $\\mathbb{E}[\\cdot]$ denotes expectation with respect to the law of $\\boldsymbol{\\xi}$.\n\nUsing only the definitions of expectation $\\mathbb{E}[\\cdot]$ and variance $\\mathrm{Var}[Y]=\\mathbb{E}\\!\\left[(Y-\\mathbb{E}[Y])^{2}\\right]$, the linearity of expectation, and the orthonormality property stated above, first derive general expressions for $\\mathbb{E}[Y]$ and $\\mathrm{Var}[Y]$ in terms of the coefficients $\\{c_{\\alpha}\\}$.\n\nThen, consider a concrete second-order PCE of the bar’s tip displacement $Y$ in an orthonormal basis constructed from multivariate Hermite polynomials in two independent standard normal inputs $\\boldsymbol{\\xi}=(\\xi_{1},\\xi_{2})$. The retained basis functions are the constant $\\Psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi})=1$ and four non-constant orthonormal polynomials $\\Psi_{1}(\\boldsymbol{\\xi}),\\Psi_{2}(\\boldsymbol{\\xi}),\\Psi_{3}(\\boldsymbol{\\xi}),\\Psi_{4}(\\boldsymbol{\\xi})$, with the corresponding coefficients\n$$\nc_{\\boldsymbol{0}}=2.50\\times 10^{-3},\\quad\nc_{1}=1.20\\times 10^{-4},\\quad\nc_{2}=-8.00\\times 10^{-5},\\quad\nc_{3}=5.00\\times 10^{-5},\\quad\nc_{4}=1.50\\times 10^{-5}.\n$$\nCompute the numerical values of the mean $\\mathbb{E}[Y]$ (in meters) and the variance $\\mathrm{Var}[Y]$ (in meters squared) of the tip displacement using your derived expressions. Round both results to four significant figures. Provide your final answer as two numbers in the order $\\mathbb{E}[Y]$, $\\mathrm{Var}[Y]$.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the theory of Polynomial Chaos Expansion (PCE) as applied to solid mechanics, is well-posed with all necessary data and definitions provided, and is formulated in precise, objective language. There are no logical contradictions, factual inaccuracies, or ambiguities that would prevent a rigorous, unique solution. We proceed with the derivation and computation.\n\nFirst, we derive the general expressions for the expectation and variance of the random variable $Y$.\n\nThe Polynomial Chaos Expansion of the tip displacement $Y$ is given as:\n$$\nY = \\sum_{\\alpha \\in \\mathcal{A}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nwhere $\\{c_{\\alpha}\\}$ are deterministic coefficients and $\\{\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\}$ is a set of orthonormal basis polynomials.\n\nTo find the expectation $\\mathbb{E}[Y]$, we apply the expectation operator to the expansion. By the linearity of the expectation operator, we can write:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\sum_{\\alpha \\in \\mathcal{A}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\right] = \\sum_{\\alpha \\in \\mathcal{A}} c_{\\alpha}\\,\\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi})]\n$$\nThe expectation of any basis polynomial $\\Psi_{\\alpha}(\\boldsymbol{\\xi})$ can be determined using the provided orthonormality property, $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}] = \\delta_{\\alpha\\beta}$, and the fact that the zeroth-order polynomial is constant, $\\Psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi}) \\equiv 1$. We compute the expectation of $\\Psi_{\\alpha}$ as its projection onto $\\Psi_{\\boldsymbol{0}}$:\n$$\n\\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi})] = \\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi}) \\cdot 1] = \\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\,\\Psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi})] = \\delta_{\\alpha\\boldsymbol{0}}\n$$\nThis result indicates that the expectation is $1$ for $\\alpha = \\boldsymbol{0}$ and $0$ for all other multi-indices $\\alpha \\neq \\boldsymbol{0}$. Substituting this into the expression for $\\mathbb{E}[Y]$ yields:\n$$\n\\mathbb{E}[Y] = \\sum_{\\alpha \\in \\mathcal{A}} c_{\\alpha}\\,\\delta_{\\alpha\\boldsymbol{0}} = c_{\\boldsymbol{0}} \\cdot 1 + \\sum_{\\alpha \\in \\mathcal{A}, \\alpha \\neq \\boldsymbol{0}} c_{\\alpha} \\cdot 0 = c_{\\boldsymbol{0}}\n$$\nThus, the mean of the random variable $Y$ is simply the coefficient of the constant basis polynomial.\n\nNext, we derive the expression for the variance, $\\mathrm{Var}[Y]$, defined as $\\mathrm{Var}[Y] = \\mathbb{E}[(Y - \\mathbb{E}[Y])^2]$. Substituting the expressions for $Y$ and $\\mathbb{E}[Y]$:\n$$\nY - \\mathbb{E}[Y] = \\sum_{\\alpha \\in \\mathcal{A}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi}) - c_{\\boldsymbol{0}} = \\left(c_{\\boldsymbol{0}}\\,\\Psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi}) + \\sum_{\\alpha \\in \\mathcal{A}, \\alpha \\neq \\boldsymbol{0}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\right) - c_{\\boldsymbol{0}}\n$$\nSince $\\Psi_{\\boldsymbol{0}}(\\boldsymbol{\\xi}) = 1$, this simplifies to:\n$$\nY - \\mathbb{E}[Y] = \\sum_{\\alpha \\in \\mathcal{A}, \\alpha \\neq \\boldsymbol{0}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\n$$\nSquaring this expression gives:\n$$\n(Y - \\mathbb{E}[Y])^2 = \\left(\\sum_{\\alpha \\in \\mathcal{A}, \\alpha \\neq \\boldsymbol{0}} c_{\\alpha}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\right) \\left(\\sum_{\\beta \\in \\mathcal{A}, \\beta \\neq \\boldsymbol{0}} c_{\\beta}\\,\\Psi_{\\beta}(\\boldsymbol{\\xi})\\right) = \\sum_{\\alpha \\neq \\boldsymbol{0}} \\sum_{\\beta \\neq \\boldsymbol{0}} c_{\\alpha}c_{\\beta}\\,\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\Psi_{\\beta}(\\boldsymbol{\\xi})\n$$\nWe now take the expectation of this quantity. Using the linearity of expectation:\n$$\n\\mathrm{Var}[Y] = \\mathbb{E}[(Y - \\mathbb{E}[Y])^2] = \\sum_{\\alpha \\neq \\boldsymbol{0}} \\sum_{\\beta \\neq \\boldsymbol{0}} c_{\\alpha}c_{\\beta}\\,\\mathbb{E}[\\Psi_{\\alpha}(\\boldsymbol{\\xi})\\Psi_{\\beta}(\\boldsymbol{\\xi})]\n$$\nApplying the orthonormality property $\\mathbb{E}[\\Psi_{\\alpha}\\Psi_{\\beta}] = \\delta_{\\alpha\\beta}$, the expression simplifies. The double summation collapses because the terms are non-zero only when $\\alpha = \\beta$:\n$$\n\\mathrm{Var}[Y] = \\sum_{\\alpha \\neq \\boldsymbol{0}} \\sum_{\\beta \\neq \\boldsymbol{0}} c_{\\alpha}c_{\\beta}\\,\\delta_{\\alpha\\beta} = \\sum_{\\alpha \\in \\mathcal{A}, \\alpha \\neq \\boldsymbol{0}} c_{\\alpha}^2\n$$\nThe variance is therefore the sum of the squares of all PCE coefficients except the one corresponding to the constant term.\n\nNow we apply these derived formulas to the specific numerical case.\nThe given PCE coefficients are:\n$c_{\\boldsymbol{0}} = 2.50 \\times 10^{-3}$\n$c_{1} = 1.20 \\times 10^{-4}$\n$c_{2} = -8.00 \\times 10^{-5}$\n$c_{3} = 5.00 \\times 10^{-5}$\n$c_{4} = 1.50 \\times 10^{-5}$\n\nThe mean displacement $\\mathbb{E}[Y]$ is simply $c_{\\boldsymbol{0}}$. The value given has three significant figures. To conform to the requirement of four significant figures, we write:\n$$\n\\mathbb{E}[Y] = c_{\\boldsymbol{0}} = 2.500 \\times 10^{-3} \\text{ meters}\n$$\nThe variance $\\mathrm{Var}[Y]$ is the sum of the squares of the coefficients of the non-constant basis functions:\n$$\n\\mathrm{Var}[Y] = c_{1}^2 + c_{2}^2 + c_{3}^2 + c_{4}^2\n$$\nWe compute the square of each coefficient:\n$$\nc_{1}^2 = (1.20 \\times 10^{-4})^2 = 1.44 \\times 10^{-8}\n$$\n$$\nc_{2}^2 = (-8.00 \\times 10^{-5})^2 = 64.0 \\times 10^{-10} = 6.40 \\times 10^{-9}\n$$\n$$\nc_{3}^2 = (5.00 \\times 10^{-5})^2 = 25.0 \\times 10^{-10} = 2.50 \\times 10^{-9}\n$$\n$$\nc_{4}^2 = (1.50 \\times 10^{-5})^2 = 2.25 \\times 10^{-10} = 0.225 \\times 10^{-9}\n$$\nSumming these values:\n$$\n\\mathrm{Var}[Y] = (1.44 \\times 10^{-8}) + (0.640 \\times 10^{-8}) + (0.250 \\times 10^{-8}) + (0.0225 \\times 10^{-8})\n$$\n$$\n\\mathrm{Var}[Y] = (1.44 + 0.64 + 0.25 + 0.0225) \\times 10^{-8} = 2.3525 \\times 10^{-8} \\text{ meters}^2\n$$\nRounding this result to four significant figures gives:\n$$\n\\mathrm{Var}[Y] = 2.353 \\times 10^{-8} \\text{ meters}^2\n$$\nThe requested numerical values are the mean $\\mathbb{E}[Y]$ and the variance $\\mathrm{Var}[Y]$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2.500 \\times 10^{-3} & 2.353 \\times 10^{-8} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world models in science and engineering often contain non-smooth behaviors, such as thresholds, switches, or hard cut-offs. Approximating such discontinuous functions with a global basis of smooth polynomials can lead to spurious oscillations known as the Gibbs phenomenon. This advanced exercise  delves into this important limitation of standard PCE and challenges you to critically evaluate modern strategies, like piecewise expansions, designed to handle such complexities and ensure accurate uncertainty propagation.",
            "id": "3941459",
            "problem": "An automated battery discharge test uses a simple Thevenin-equivalent single-resistor model with voltage $V(t;\\xi)=V_{\\mathrm{oc}}(z(t)) - I R(\\xi)$, where $V_{\\mathrm{oc}}$ is the open-circuit voltage as a function of state-of-charge $z(t)$, $I$ is a prescribed constant discharge current, and $R(\\xi)$ is the internal resistance parameterized by a scalar random input $\\xi$ that captures manufacturing variability. The test includes a hard cut-off at voltage $V_{\\mathrm{cut}}$: discharge is terminated at the stopping time $\\tau(\\xi)$ defined by the event $V(\\tau(\\xi);\\xi)=V_{\\mathrm{cut}}$, or at a fixed horizon $T$ if the cut-off is not reached earlier. Consider the quantity of interest $Y(\\xi)$ defined as the safety trip indicator up to time $T$, \n$$\nY(\\xi)=\\mathbf{1}\\{\\tau(\\xi)\\le T\\}.\n$$\nAssume the input $\\xi\\sim \\mathcal{U}(-1,1)$ with probability density $\\rho(\\xi)=\\tfrac{1}{2}$, and that $R(\\xi)$ is monotone in $\\xi$ so that the event surface $\\{\\xi: \\tau(\\xi)=T\\}$ is a single threshold $\\xi_0\\in(-1,1)$, hence \n$$\nY(\\xi)=H(\\xi-\\xi_0),\n$$\nwhere $H$ is the Heaviside step. To propagate uncertainty, one constructs a Polynomial Chaos Expansion (PCE) of $Y(\\xi)$ using the Legendre basis $\\{P_n(\\xi)\\}_{n\\ge 0}$ orthonormal on $[-1,1]$ with respect to $\\rho(\\xi)$. The truncated $L^2$ projection of degree $p$ is\n$$\nY_p(\\xi)=\\sum_{n=0}^p c_n \\,\\phi_n(\\xi), \\quad \\phi_n(\\xi)=\\sqrt{2n+1}\\,P_n(\\xi), \\quad c_n=\\int_{-1}^1 Y(\\zeta)\\,\\phi_n(\\zeta)\\,\\rho(\\zeta)\\,d\\zeta.\n$$\n\nStarting from the definitions of $L^2$ projection and orthogonality of Legendre polynomials, explain why the truncation $Y_p(\\xi)$ exhibits non-vanishing oscillatory overshoots and undershoots localized near the threshold $\\xi_0$ as $p\\to\\infty$ (a Gibbs-like phenomenon), even though $Y\\in L^2([-1,1],\\rho)$. Your explanation should use only properties of orthogonal projections and kernels (for example, the Christoffel–Darboux identity) and should not assume any special Fourier-series facts. Then, based on this analysis and the battery switching mechanism, identify which of the following statements are correct about mitigation strategies and their implications in this context.\n\nA. Simply increasing the global polynomial degree $p$ in the Legendre PCE will eliminate the oscillations and recover exponential convergence of $Y_p$ to $Y$ in the $L^\\infty$ norm, even in the presence of the cut-off switching.\n\nB. Constructing a piecewise PCE by partitioning the input domain at $\\xi_0$ into two elements $[-1,\\xi_0)$ and $(\\xi_0,1]$ and fitting separate local Legendre expansions on each element can suppress Gibbs-like ringing and restore fast convergence consistent with the smoothness of $Y$ restricted to each element.\n\nC. When estimating a PCE from data via regression, augmenting the least-squares fit with a Total Variation (TV) penalty in the input domain, i.e., solving $\\min_{u\\in\\mathcal{P}_p}\\sum_{j=1}^m(u(\\xi^{(j)})-Y^{(j)})^2+\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ with $\\lambda>0$, can reduce oscillations near $\\xi_0$ but may introduce bias in estimates of moments such as the mean and variance.\n\nD. Pre-smoothing the output with a Gaussian kernel, i.e., replacing $Y(\\xi)$ by $(G_\\sigma\\ast Y)(\\xi)$ with a small standard deviation $\\sigma>0$, yields a PCE whose mean and variance remain unbiased estimators of those of $Y$ while removing Gibbs oscillations.\n\nE. An affine reparameterization of the input, $\\eta=a\\xi+b$ with constants $a\\ne 0$ and $b$, always removes Gibbs-like oscillations for step-type quantities like $Y(\\xi)$, regardless of the orientation of the discontinuity surface in higher-dimensional inputs.\n\nSelect all that apply. Provide a rigorous, principle-based argument to justify the Gibbs-like phenomenon and to evaluate each option. All mathematical symbols, variables, functions, operators, and numbers must appear in LaTeX. Assume $V_{\\mathrm{oc}}$ is smooth and strictly decreasing in $z$ over the relevant range, $R(\\xi)$ is smooth and strictly increasing in $\\xi$, and $I$, $V_{\\mathrm{cut}}$, and $T$ are fixed positive constants.",
            "solution": "The problem statement poses a valid and well-posed question in the field of uncertainty quantification, grounded in a realistic (though simplified) model of battery discharge. The core of the problem lies in the mathematical properties of Polynomial Chaos Expansions (PCE) when approximating discontinuous functions, which is a standard and important topic. All assumptions are clearly stated and are physically and mathematically consistent.\n\nThe first task is to explain the Gibbs-like phenomenon for the Legendre PCE of the step function $Y(\\xi) = H(\\xi - \\xi_0)$.\n\nThe truncated PCE of degree $p$, denoted $Y_p(\\xi)$, is the orthogonal projection of the function $Y(\\xi)$ onto the subspace $\\mathcal{P}_p$ of polynomials of degree at most $p$. The projection is defined with respect to the $L^2$ inner product $\\langle f, g \\rangle = \\int_{-1}^1 f(\\xi)g(\\xi)\\rho(\\xi)d\\xi$. The projection $Y_p$ is the unique polynomial in $\\mathcal{P}_p$ that minimizes the squared error in the $L^2$ norm:\n$$\nY_p = \\arg\\min_{u \\in \\mathcal{P}_p} \\|Y - u\\|_{L^2}^2 = \\arg\\min_{u \\in \\mathcal{P}_p} \\int_{-1}^1 (Y(\\xi) - u(\\xi))^2 \\rho(\\xi) d\\xi\n$$\nThe formal expression for this projection is given by\n$$\nY_p(\\xi) = \\sum_{n=0}^p c_n \\phi_n(\\xi) = \\int_{-1}^1 Y(\\zeta) \\left( \\sum_{n=0}^p \\phi_n(\\xi) \\phi_n(\\zeta) \\right) \\rho(\\zeta) d\\zeta\n$$\nLet's define the kernel of the projection operator as $K_p(\\xi, \\zeta) = \\sum_{n=0}^p \\phi_n(\\xi) \\phi_n(\\zeta)$. The projection can then be written as a convolution-like integral:\n$$\nY_p(\\xi) = \\int_{-1}^1 Y(\\zeta) K_p(\\xi, \\zeta) \\rho(\\zeta) d\\zeta\n$$\nSince $Y(\\xi)$ is the Heaviside function $H(\\xi-\\xi_0)$, the integral becomes:\n$$\nY_p(\\xi) = \\int_{\\xi_0}^1 K_p(\\xi, \\zeta) \\rho(\\zeta) d\\zeta\n$$\nThe Christoffel-Darboux formula for the orthonormal Legendre polynomials $\\phi_n(\\xi)$ provides an expression for the kernel:\n$$\nK_p(\\xi, \\zeta) = \\sqrt{(p+1)(2p+3)} \\frac{\\phi_{p+1}(\\xi)\\phi_p(\\zeta) - \\phi_p(\\xi)\\phi_{p+1}(\\zeta)}{\\xi-\\zeta}\n$$\nThe key to understanding the Gibbs phenomenon lies in the properties of this projection.\n1.  **Nature of Convergence**: Since $Y(\\xi)$ is square-integrable on $[-1, 1]$ (i.e., $Y \\in L^2([-1,1], \\rho)$), the theory of orthogonal expansions guarantees that the sequence of projections $Y_p$ converges to $Y$ in the $L^2$ norm. That is, $\\|Y_p - Y\\|_{L^2} \\to 0$ as $p \\to \\infty$.\n2.  **Lack of Uniform Convergence**: However, $L^2$ convergence does not imply uniform convergence (convergence in the $L^\\infty$ norm, $\\|f\\|_\\infty = \\sup_\\xi |f(\\xi)|$). For uniform convergence to hold, the limit function must be continuous, as the uniform limit of a sequence of continuous functions (the polynomials $Y_p$) must be continuous. Our target function $Y(\\xi)$ has a jump discontinuity at $\\xi = \\xi_0$. Therefore, the convergence of $Y_p$ to $Y$ cannot be uniform.\n3.  **Mechanism of Oscillation**: The failure to converge uniformly is precisely the Gibbs phenomenon. The polynomial $Y_p(\\xi)$, being infinitely smooth, cannot reproduce a sharp jump. To minimize the integrated squared error, the polynomial must rise very steeply near $\\xi_0$. A steep polynomial rise inevitably leads to an \"overshoot\" past the target value of $1$. As the degree $p$ increases, the approximation gets better in the $L^2$ sense: the oscillations are squeezed into a narrower and narrower region around the discontinuity $\\xi_0$. However, the maximum height of the overshoot does not decrease to zero. Instead, it converges to a fixed value greater than $1$ (approximately $1.09$ for a normalized jump of $1$ in the classic Fourier series case; the value is different but the principle is the same for Legendre series). This non-vanishing amplitude of the oscillation is the hallmark of the Gibbs phenomenon. The integral nature of the $L^2$ projection means it is insensitive to localized, large pointwise errors as long as the region they occupy has a measure that shrinks sufficiently fast.\n\nNow, we evaluate each of the proposed statements.\n\n**A. Simply increasing the global polynomial degree $p$ in the Legendre PCE will eliminate the oscillations and recover exponential convergence of $Y_p$ to $Y$ in the $L^\\infty$ norm, even in the presence of the cut-off switching.**\nThis statement is fundamentally incorrect. As explained above, the Gibbs phenomenon is characterized by non-vanishing oscillations for PCEs of discontinuous functions. Increasing the polynomial degree $p$ localizes the oscillations closer to the discontinuity but does not eliminate them. Therefore, convergence in the $L^\\infty$ norm (uniform convergence) is not achieved. Furthermore, exponential convergence of PCEs is typically associated with functions that are analytic within an ellipse in the complex plane containing the interval $[-1, 1]$. The function $Y(\\xi)$ is a step function, which is not even continuous, let alone analytic. The convergence rate in $L^2$ is algebraic, not exponential.\n**Verdict: Incorrect**\n\n**B. Constructing a piecewise PCE by partitioning the input domain at $\\xi_0$ into two elements $[-1,\\xi_0)$ and $(\\xi_0,1]$ and fitting separate local Legendre expansions on each element can suppress Gibbs-like ringing and restore fast convergence consistent with the smoothness of $Y$ restricted to each element.**\nThis statement is correct. The Gibbs phenomenon arises from approximating a non-smooth function with a single global basis of smooth functions. By partitioning the domain at the point of discontinuity $\\xi_0$, this strategy treats the problem as two separate approximation problems. On the subdomain $[-1, \\xi_0)$, the function $Y(\\xi)$ is identically zero. On the subdomain $(\\xi_0, 1]$, the function is identically one. Both $f(\\xi)=0$ and $f(\\xi)=1$ are constant functions, hence they are infinitely smooth ($C^\\infty$). A polynomial expansion can represent a constant exactly with a single, degree-$0$ term. Therefore, a piecewise PCE (with degree $p \\ge 0$ on each piece) will represent $Y(\\xi)$ exactly, with no error and consequently no Gibbs oscillations. This restores the convergence properties to be consistent with the smoothness of the function *within each subdomain*.\n**Verdict: Correct**\n\n**C. When estimating a PCE from data via regression, augmenting the least-squares fit with a Total Variation (TV) penalty in the input domain, i.e., solving $\\min_{u\\in\\mathcal{P}_p}\\sum_{j=1}^m(u(\\xi^{(j)})-Y^{(j)})^2+\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ with $\\lambda>0$, can reduce oscillations near $\\xi_0$ but may introduce bias in estimates of moments such as the mean and variance.**\nThis statement is correct. The standard least-squares fit is a discrete analogue of the $L^2$ projection and thus exhibits Gibbs oscillations. The term $\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ is a Total Variation (TV) regularization penalty. This penalty term discourages solutions $u(\\xi)$ with large variations, such as the rapid oscillations of the Gibbs phenomenon. Maximizing this penalty favors smoother, less oscillatory functions. Thus, including it in the minimization objective will suppress the ringing artifacts. However, this comes at a cost. The solution to the regularized problem is no longer the true projection; it is a biased approximation. The regularization systematically alters the coefficients of the polynomial away from their unbiased least-squares values. Since the moments of the random variable (e.g., mean $\\mathbb{E}[Y]$ and variance $\\mathrm{Var}[Y]$) are computed directly from these coefficients, using the biased coefficients from the TV-regularized fit will result in biased estimates of the moments. This is a classic example of the bias-variance trade-off in statistics and machine learning.\n**Verdict: Correct**\n\n**D. Pre-smoothing the output with a Gaussian kernel, i.e., replacing $Y(\\xi)$ by $(G_\\sigma\\ast Y)(\\xi)$ with a small standard deviation $\\sigma>0$, yields a PCE whose mean and variance remain unbiased estimators of those of $Y$ while removing Gibbs oscillations.**\nThis statement is incorrect. Convolving the step function $Y(\\xi)$ with a Gaussian kernel $G_\\sigma$ produces a smooth function $Y_\\sigma(\\xi)$. A PCE of this new, smooth function $Y_\\sigma$ will indeed converge rapidly and without Gibbs oscillations. So the first part of the claim is correct. However, the second part is false. The PCE of $Y_\\sigma$ gives us approximations of the moments of $Y_\\sigma$, not $Y$. The process of smoothing changes the function, and therefore changes its moments. For example, the mean is $\\mathbb{E}[Y] = \\int_{-1}^1 Y(\\xi)\\rho(\\xi)d\\xi$. The mean of the smoothed function is $\\mathbb{E}[Y_\\sigma] = \\int_{-1}^1 Y_\\sigma(\\xi)\\rho(\\xi)d\\xi$. Since $Y_\\sigma \\neq Y$, there is no reason to expect $\\mathbb{E}[Y_\\sigma] = \\mathbb{E}[Y]$. The smoothing operation introduces a systematic error, or bias, into the moments. Thus, the mean and variance of the PCE of $Y_\\sigma$ are biased estimators of the mean and variance of $Y$.\n**Verdict: Incorrect**\n\n**E. An affine reparameterization of the input, $\\eta=a\\xi+b$ with constants $a\\ne 0$ and $b$, always removes Gibbs-like oscillations for step-type quantities like $Y(\\xi)$, regardless of the orientation of the discontinuity surface in higher-dimensional inputs.**\nThis statement is incorrect. An affine reparameterization $\\eta=a\\xi+b$ simply maps the interval $[-1, 1]$ to another interval and shifts/scales the coordinate system. The function $Y(\\xi) = H(\\xi-\\xi_0)$ becomes $\\tilde{Y}(\\eta) = H((\\eta-b)/a - \\xi_0)$, which is still a step function with a discontinuity at $\\eta_0 = a\\xi_0+b$. Approximating this new step function $\\tilde{Y}(\\eta)$ with polynomials in $\\eta$ will still produce the Gibbs phenomenon for the exact same reasons as before. An affine map does not alter the fundamental nature of the problem: approximating a discontinuous function with smooth global basis functions. The claim is patently false, and its generalization to higher dimensions is equally invalid.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}