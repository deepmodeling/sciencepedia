{
    "hands_on_practices": [
        {
            "introduction": "Dynamic downscaling models solve the governing equations of atmospheric motion on a fine-resolution grid. The stability of these numerical solutions is paramount, and it is governed by fundamental principles like the Courant–Friedrichs–Lewy (CFL) condition. This practice provides a concrete exercise in calculating the maximum permissible time step for a model, connecting the abstract theory of numerical stability to the practical choices modelers make to ensure a simulation is both efficient and reliable .",
            "id": "3875707",
            "problem": "A regional climate model performing dynamic downscaling of a General Circulation Model (GCM) boundary forcing advances a passive tracer governed by the one-dimensional linear advection equation on a uniform grid. The model uses an explicit one-step, monotone spatial discretization consistent with upwind information propagation. The governing partial differential equation is the linear advection equation, and the wind is constant and aligned with the grid. The downscaled field is subsequently intended for statistical downscaling and bias correction, making preservation of variance and extremes relevant to method choice. The model grid spacing is $\\Delta x = 3$ km and the wind speed is $u = 20$ m s$^{-1}$. Using the Courant–Friedrichs–Lewy (CFL) condition and the requirement that the numerical domain of dependence include the physical domain of dependence, derive the maximum stable time step $\\Delta t_{\\max}$ for this explicit advection update. Provide the final numerical value of $\\Delta t_{\\max}$ in seconds. Round your answer to four significant figures. Express the final time step in seconds.\n\nIn addition, briefly articulate the damping trade-offs that arise from choosing a time step close to the stability limit in the context of dynamic downscaling feeding into statistical downscaling, focusing on how explicit diffusion or filtering choices interact with stability and fidelity. Your discussion does not affect the computed value of $\\Delta t_{\\max}$.",
            "solution": "The fundamental base is the one-dimensional linear advection equation for a passive tracer $\\phi(x,t)$,\n$$\n\\frac{\\partial \\phi}{\\partial t} + u \\frac{\\partial \\phi}{\\partial x} = 0,\n$$\nwith constant velocity $u$. The physical domain of dependence for this hyperbolic equation follows characteristics $x - u t = \\text{constant}$, implying that information propagates at speed $u$ in the direction of the flow.\n\nFor explicit time-stepping on a uniform grid with spacing $\\Delta x$, a necessary stability requirement is given by the Courant–Friedrichs–Lewy (CFL) condition, which states that the numerical domain of dependence must contain the physical domain of dependence. For monotone upwind-like schemes consistent with the direction of characteristic propagation, this requirement translates to a bound on the advective Courant number,\n$$\nC \\equiv \\frac{u \\Delta t}{\\Delta x}.\n$$\nTo see this from first principles, consider a conservative first-order upwind semi-discrete form aligned with the sign of $u$. A von Neumann stability analysis expands a mode $\\phi_j^n = \\hat{\\phi}^n \\exp(i k j \\Delta x)$, with amplification factor $G(k)$ over one time step $\\Delta t$. For the explicit forward Euler time discretization and upwind spatial flux, the discrete update for $u>0$ can be written as\n$$\n\\phi_j^{n+1} = \\phi_j^n - C \\left(\\phi_j^n - \\phi_{j-1}^n\\right),\n$$\nwhich yields\n$$\nG(k) = 1 - C \\left(1 - \\exp(- i k \\Delta x)\\right).\n$$\nThe magnitude of $G(k)$ is bounded by\n$$\n|G(k)|^{2} = \\left[1 - C (1 - \\cos(k \\Delta x))\\right]^2 + \\left[C \\sin(k \\Delta x)\\right]^2 = 1 - 2C(1-\\cos(k \\Delta x)) + C^{2} \\left[2 - 2\\cos(k \\Delta x)\\right].\n$$\nThe most restrictive case occurs at the highest wavenumber $k \\Delta x = \\pi$, for which $\\cos(k \\Delta x) = -1$. Substituting gives\n$$\n|G|^{2} = 1 - 2C(1 - (-1)) + C^{2} (2 - 2(-1)) = 1 - 4C + 4 C^{2} = (1 - 2C)^{2}.\n$$\nTo avoid unbounded growth, the amplification must satisfy $|G| \\le 1$ for all admissible $k$. The monotonicity and positivity constraints further require $0 \\le C \\le 1$. Thus, the CFL stability criterion for explicit upwind advection is\n$$\n0 \\le \\frac{u \\Delta t}{\\Delta x} \\le 1,\n$$\ngiving the maximum stable time step\n$$\n\\Delta t_{\\max} = \\frac{\\Delta x}{|u|}.\n$$\n\nWe now evaluate $\\Delta t_{\\max}$ with the given parameters. The grid spacing is $\\Delta x = 3$ km, which is $\\Delta x = 3000$ m, and the wind speed is $u = 20$ m s$^{-1}$. Therefore,\n$$\n\\Delta t_{\\max} = \\frac{3000}{20} = 150 \\text{ s}.\n$$\nRounding to four significant figures yields\n$$\n\\Delta t_{\\max} = 150.0 \\text{ s}.\n$$\n\nDiscussion of damping trade-offs: In dynamic downscaling, choosing $\\Delta t$ close to $\\Delta t_{\\max}$ maximizes computational efficiency but increases susceptibility to dispersive and diffusive numerical errors inherent in explicit advection schemes. First-order upwind introduces numerical diffusion with an effective diffusivity scaling as\n$$\n\\nu_{\\text{num}} \\sim \\frac{1}{2} u \\Delta x (1 - C),\n$$\nso as $C \\to 1$ numerical diffusion diminishes, better preserving variance and extremes important for subsequent statistical downscaling. However, operating very near $C=1$ leaves little margin against local increases in $|u|$ (e.g., gusts or model variability), risking violation of the CFL bound and instability. Conversely, reducing $\\Delta t$ (smaller $C$) increases numerical damping, which stabilizes high-wavenumber content and can suppress grid-scale noise, but at the cost of attenuating peaks and altering temporal autocorrelation structures. If explicit diffusion (e.g., a Laplacian with coefficient $\\nu$) or time filters such as the Robert–Asselin filter are added, additional stability constraints arise, for example\n$$\n\\Delta t \\le \\frac{\\Delta x^{2}}{2 \\nu}\n$$\nfor explicit diffusion, and filter-induced damping scales with the filter parameter. These mechanisms can allow acceptable stability margins with larger $\\Delta t$, but they may degrade fidelity of extremes and cross-scale variance that statistical downscaling seeks to correct. Thus, selecting $\\Delta t$ involves balancing stability, numerical damping, and the preservation of physically meaningful variability that will feed into statistical post-processing.",
            "answer": "$$\\boxed{150.0}$$"
        },
        {
            "introduction": "After generating high-resolution data, statistical downscaling is often essential to correct systematic biases present in the model output. Quantile mapping is a powerful technique that adjusts the entire probability distribution of a modeled variable to better match observations, proving especially crucial for the study of extremes. This exercise explores the mathematical foundation of quantile mapping, demonstrating how it transforms return levels and periods, and providing hands-on experience in quantifying its impact .",
            "id": "3875621",
            "problem": "Consider a statistical downscaling problem in environmental and earth system modeling where bias correction is performed using quantile mapping. Let $X$ denote a daily precipitation amount simulated by a regional climate model. Assume $X$ has a continuous distribution with cumulative distribution function (CDF) $F_{X}$ and strictly increasing quantile function $Q_{X}(p)$ defined by $Q_{X}(p) = \\inf\\{x : F_{X}(x) \\geq p\\}$ for $p \\in (0,1)$. The return period $N$ (in days) associated with a return level $z$ is defined by the exceedance probability $p_{N} = 1/N$, so that $z$ satisfies $1 - F_{X}(z) = p_{N}$ and hence $z = Q_{X}(1 - p_{N}) = Q_{X}(1 - 1/N)$ for independent daily realizations.\n\nSuppose a bias-correction transformation is applied via a strictly monotone increasing function $g$, producing $Y = g(X)$ as the downscaled series. In particular, for quantile mapping (QM), $g$ is constructed as $g(x) = F_{o}^{-1}(F_{m}(x))$, where $F_{m}$ is the model CDF (the CDF of $X$) and $F_{o}$ is the observational reference CDF. Let the observational reference quantile function be $Q_{o}(p) = F_{o}^{-1}(p)$.\n\n1. Starting from the fundamental definitions of the cumulative distribution function and quantile function, and the definition of return period as $N = 1/p_{N}$ with $p_{N}$ the exceedance probability per day, derive an expression for the post-transformation return level $z_{Y}(N)$ associated with return period $N$ in terms of $g$ and $Q_{X}$.\n\n2. Specialize to quantile mapping with $g(x) = F_{o}^{-1}(F_{m}(x))$, and show how $z_{Y}(N)$ relates to $Q_{o}(p)$. Then, assume both $X$ (model) and the observational reference are lognormal: $\\ln X \\sim \\mathcal{N}(\\mu_{m}, \\sigma_{m}^{2})$ and $\\ln Y_{\\mathrm{ref}} \\sim \\mathcal{N}(\\mu_{o}, \\sigma_{o}^{2})$. Express $z_{X}(N)$ and $z_{Y}(N)$ in closed form using the standard normal cumulative distribution function $\\Phi$ and its inverse $\\Phi^{-1}$, and derive the ratio\n$$\nR(N) = \\frac{z_{Y}(N)}{z_{X}(N)}\n$$\nas a closed-form function of $\\mu_{m}$, $\\sigma_{m}$, $\\mu_{o}$, $\\sigma_{o}$, and $N$.\n\n3. For parameters $\\mu_{m} = 2.5$, $\\sigma_{m} = 0.7$, $\\mu_{o} = 2.6$, and $\\sigma_{o} = 0.6$, compute the numerical value of $R(N)$ for $N = 100$ days. Express the ratio as a unitless decimal and round your answer to four significant figures.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It contains sufficient information for a unique solution and is free of contradictions or fallacies. We may proceed with the solution.\n\nThe problem is divided into three parts. We will address them sequentially.\n\n***Part 1: Derivation of the post-transformation return level $z_{Y}(N)$***\n\nThe return level $z$ for a random variable is defined by its exceedance probability. For a return period of $N$ days, the exceedance probability is $p_{N} = 1/N$. The return level $z_{Y}(N)$ for the transformed variable $Y$ is the value that is expected to be exceeded, on average, once every $N$ days. Mathematically, this is expressed as:\n$$\nP(Y > z_{Y}(N)) = p_{N} = \\frac{1}{N}\n$$\nLet $F_{Y}$ be the cumulative distribution function (CDF) of $Y$. The exceedance probability is $1 - F_{Y}(z_{Y}(N))$. Therefore:\n$$\n1 - F_{Y}(z_{Y}(N)) = \\frac{1}{N} \\implies F_{Y}(z_{Y}(N)) = 1 - \\frac{1}{N}\n$$\nBy the definition of the quantile function, $Q_{Y}(p) = \\inf\\{y : F_{Y}(y) \\geq p\\}$, we can express the return level as:\n$$\nz_{Y}(N) = Q_{Y}\\left(1 - \\frac{1}{N}\\right)\n$$\nWe need to relate $Q_{Y}$ to the properties of the original variable $X$ and the transformation $g$. The variable $Y$ is defined as $Y = g(X)$, where $g$ is a strictly monotone increasing function. Let us find the CDF of $Y$:\n$$\nF_{Y}(y) = P(Y \\leq y) = P(g(X) \\leq y)\n$$\nSince $g$ is strictly increasing, its inverse $g^{-1}$ exists and is also strictly increasing. Applying $g^{-1}$ to both sides of the inequality preserves its direction:\n$$\nF_{Y}(y) = P(X \\leq g^{-1}(y))\n$$\nBy the definition of the CDF of $X$, which is $F_{X}$, we have:\n$$\nF_{Y}(y) = F_{X}(g^{-1}(y))\n$$\nTo find the quantile function $Q_{Y}(p)$, we set $F_{Y}(y) = p$ and solve for $y$:\n$$\np = F_{X}(g^{-1}(y))\n$$\nApplying the quantile function of $X$, $Q_{X}$, to both sides:\n$$\nQ_{X}(p) = g^{-1}(y)\n$$\nSolving for $y$ gives the expression for the quantile function of $Y$:\n$$\ny = g(Q_{X}(p)) \\implies Q_{Y}(p) = g(Q_{X}(p))\n$$\nNow we can substitute this expression into the equation for the return level $z_{Y}(N)$:\n$$\nz_{Y}(N) = g\\left(Q_{X}\\left(1 - \\frac{1}{N}\\right)\\right)\n$$\nThis is the desired expression for the post-transformation return level $z_{Y}(N)$ in terms of $g$ and $Q_{X}$.\n\n***Part 2: Specialization to Quantile Mapping and Lognormal Distributions***\n\nFor quantile mapping (QM), the transformation function is given by $g(x) = F_{o}^{-1}(F_{m}(x))$, where $F_{m}$ is the model CDF (i.e., $F_{X}$) and $F_{o}$ is the observational reference CDF.\nThe quantile function of the transformed variable $Y$ is $Q_{Y}(p) = g(Q_{X}(p))$. Substituting the specific form of $g$ for QM:\n$$\nQ_{Y}(p) = F_{o}^{-1}(F_{m}(Q_{X}(p)))\n$$\nBy definition, $Q_{X}$ is the inverse of $F_{X}$ (which is $F_{m}$), so $F_{m}(Q_{X}(p)) = p$. Therefore:\n$$\nQ_{Y}(p) = F_{o}^{-1}(p) = Q_{o}(p)\n$$\nThis demonstrates that the quantile function of the bias-corrected variable $Y$ is identical to the quantile function of the observations. Consequently, the return level $z_{Y}(N)$ is given by the observational quantile function evaluated at the non-exceedance probability:\n$$\nz_{Y}(N) = Q_{Y}\\left(1 - \\frac{1}{N}\\right) = Q_{o}\\left(1 - \\frac{1}{N}\\right)\n$$\nNext, we assume both the model and observational data are lognormally distributed.\n- Model: $\\ln X \\sim \\mathcal{N}(\\mu_{m}, \\sigma_{m}^{2})$.\n- Observations: $\\ln Y_{\\mathrm{ref}} \\sim \\mathcal{N}(\\mu_{o}, \\sigma_{o}^{2})$.\n\nThe quantile function for a lognormal variable whose logarithm has mean $\\mu$ and standard deviation $\\sigma$ is $Q(p) = \\exp(\\mu + \\sigma \\Phi^{-1}(p))$, where $\\Phi$ is the standard normal CDF and $\\Phi^{-1}$ is its inverse (the probit function).\n\nFor the model variable $X$, the quantile function is $Q_{X}(p) = \\exp(\\mu_{m} + \\sigma_{m} \\Phi^{-1}(p))$. The model return level $z_{X}(N)$ is:\n$$\nz_{X}(N) = Q_{X}\\left(1 - \\frac{1}{N}\\right) = \\exp\\left(\\mu_{m} + \\sigma_{m} \\Phi^{-1}\\left(1 - \\frac{1}{N}\\right)\\right)\n$$\nFor the observational reference, the quantile function is $Q_{o}(p) = \\exp(\\mu_{o} + \\sigma_{o} \\Phi^{-1}(p))$. As shown above, $z_{Y}(N) = Q_{o}(1 - 1/N)$, so the post-transformation return level $z_{Y}(N)$ is:\n$$\nz_{Y}(N) = Q_{o}\\left(1 - \\frac{1}{N}\\right) = \\exp\\left(\\mu_{o} + \\sigma_{o} \\Phi^{-1}\\left(1 - \\frac{1}{N}\\right)\\right)\n$$\nThe ratio $R(N) = z_{Y}(N) / z_{X}(N)$ is then:\n$$\nR(N) = \\frac{\\exp\\left(\\mu_{o} + \\sigma_{o} \\Phi^{-1}\\left(1 - \\frac{1}{N}\\right)\\right)}{\\exp\\left(\\mu_{m} + \\sigma_{m} \\Phi^{-1}\\left(1 - \\frac{1}{N}\\right)\\right)}\n$$\nUsing the property $\\exp(a)/\\exp(b) = \\exp(a-b)$, we get the closed-form expression:\n$$\nR(N) = \\exp\\left[ \\left(\\mu_{o} - \\mu_{m}\\right) + \\left(\\sigma_{o} - \\sigma_{m}\\right) \\Phi^{-1}\\left(1 - \\frac{1}{N}\\right) \\right]\n$$\n\n***Part 3: Numerical Calculation***\n\nWe are given the parameters $\\mu_{m} = 2.5$, $\\sigma_{m} = 0.7$, $\\mu_{o} = 2.6$, and $\\sigma_{o} = 0.6$. We need to compute $R(N)$ for $N = 100$.\n\nFirst, we calculate the differences in the parameters:\n$$\n\\mu_{o} - \\mu_{m} = 2.6 - 2.5 = 0.1\n$$\n$$\n\\sigma_{o} - \\sigma_{m} = 0.6 - 0.7 = -0.1\n$$\nNext, we evaluate the non-exceedance probability for $N=100$:\n$$\n1 - \\frac{1}{N} = 1 - \\frac{1}{100} = 0.99\n$$\nWe need the value of the inverse standard normal CDF at $0.99$:\n$$\n\\Phi^{-1}(0.99) \\approx 2.32634787\n$$\nNow, we substitute these values into the expression for $R(100)$:\n$$\nR(100) = \\exp\\left[ 0.1 + (-0.1) \\times \\Phi^{-1}(0.99) \\right]\n$$\n$$\nR(100) \\approx \\exp[0.1 - (0.1 \\times 2.32634787)]\n$$\n$$\nR(100) \\approx \\exp[0.1 - 0.232634787]\n$$\n$$\nR(100) \\approx \\exp[-0.132634787]\n$$\n$$\nR(100) \\approx 0.8757870\n$$\nRounding the result to four significant figures gives $0.8758$.",
            "answer": "$$\\boxed{0.8758}$$"
        },
        {
            "introduction": "The ultimate value of downscaling is often realized when its outputs are used to drive impact assessments. This comprehensive practice simulates a full end-to-end workflow, from processing coarse model outputs to estimating the potential yield of a wind turbine. You will integrate concepts representing both dynamic downscaling (modifying mean and variance) and statistical downscaling (fitting extreme value distributions to the tails) to see how these refinements directly affect a practical, real-world outcome .",
            "id": "3875597",
            "problem": "A wind energy analyst aims to quantify how dynamic and statistical downscaling of wind speed fields modifies the expected power yield of a single horizontal-axis wind turbine. The turbine is characterized by a simplified piecewise power curve and operates under site-specific wind speed distributions produced by a two-step downscaling process. The analyst must implement a program that, for a set of specified parameter combinations, computes the expected power output as the mathematical expectation of the turbine power under the downscaled wind speed probability density function.\n\nThe fundamental base to be used is the definition of mathematical expectation, the definition of the Lognormal distribution as a model for wind speeds, and the Generalized Pareto Distribution (GPD) for the upper tail in a peaks-over-threshold framework. The analyst must leverage the following well-tested definitions:\n\n1. The expectation of a nonnegative measurable function $P(u)$ with respect to a wind speed probability density function $f(u)$ is\n$$\n\\mathbb{E}[P] = \\int_{0}^{\\infty} P(u) f(u) \\, du.\n$$\n\n2. A wind speed $U$ modeled as Lognormal with parameters $(\\mu, \\sigma)$ has probability density function\n$$\nf_{\\mathrm{LN}}(u; \\mu, \\sigma) = \\frac{1}{u \\sigma \\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\ln u - \\mu)^2}{2 \\sigma^2} \\right), \\quad u > 0,\n$$\nand cumulative distribution function\n$$\nF_{\\mathrm{LN}}(u; \\mu, \\sigma) = \\Phi\\left( \\frac{\\ln u - \\mu}{\\sigma} \\right),\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. If coarse-grid wind speeds have mean $\\bar{u}_c$ and variance $\\mathrm{Var}_c$ for $U_c$, then a Lognormal representation of $U_c$ in logarithmic space has parameters\n$$\n\\sigma_c^2 = \\ln\\left(1 + \\frac{\\mathrm{Var}_c}{\\bar{u}_c^2}\\right), \\quad \\mu_c = \\ln(\\bar{u}_c) - \\frac{1}{2}\\sigma_c^2.\n$$\n\n3. Dynamic downscaling is represented as a physically motivated transformation that increases subgrid variability and adjusts mean wind speed due to local speed-up. In log space, this is modeled by\n$$\n\\mu_{\\ell} = \\ln(k) + \\mu_c, \\quad \\sigma_{\\ell} = s \\, \\sigma_c,\n$$\nwhere $k > 0$ is a multiplicative mean speed-up factor and $s > 0$ is a variability inflation factor. The baseline local distribution is therefore $U_{\\ell} \\sim \\mathrm{Lognormal}(\\mu_{\\ell}, \\sigma_{\\ell})$.\n\n4. Statistical downscaling for tail behavior is implemented by replacing the upper tail beyond a threshold $u_t > 0$ using a peaks-over-threshold approach with the Generalized Pareto Distribution (GPD). The Generalized Pareto Distribution (GPD) with shape parameter $\\xi$ and scale parameter $\\beta > 0$ for exceedances $Y = U - u_t$ ($U \\ge u_t$) has cumulative distribution function\n$$\nH(y; \\xi, \\beta) =\n\\begin{cases}\n1 - \\left(1 + \\dfrac{\\xi y}{\\beta}\\right)^{-1/\\xi}, & \\xi \\ne 0, \\\\\n1 - \\exp\\left(-\\dfrac{y}{\\beta}\\right), & \\xi = 0,\n\\end{cases}\n$$\nwith support $y \\ge 0$ and $1 + \\dfrac{\\xi y}{\\beta} > 0$, and probability density function\n$$\nh(y; \\xi, \\beta) =\n\\begin{cases}\n\\dfrac{1}{\\beta}\\left(1 + \\dfrac{\\xi y}{\\beta}\\right)^{-1/\\xi - 1}, & \\xi \\ne 0, \\\\\n\\dfrac{1}{\\beta}\\exp\\left(-\\dfrac{y}{\\beta}\\right), & \\xi = 0.\n\\end{cases}\n$$\nDefine the baseline exceedance probability\n$$\np_t = 1 - F_{\\mathrm{LN}}(u_t; \\mu_{\\ell}, \\sigma_{\\ell}).\n$$\nConstruct the composite local cumulative distribution function $F(u)$ and probability density function $f(u)$ by splicing the tail:\n$$\nF(u) =\n\\begin{cases}\nF_{\\mathrm{LN}}(u; \\mu_{\\ell}, \\sigma_{\\ell}), & 0 < u < u_t, \\\\\nF_{\\mathrm{LN}}(u_t; \\mu_{\\ell}, \\sigma_{\\ell}) + p_t \\, H(u - u_t; \\xi, \\beta), & u \\ge u_t,\n\\end{cases}\n$$\n$$\nf(u) =\n\\begin{cases}\nf_{\\mathrm{LN}}(u; \\mu_{\\ell}, \\sigma_{\\ell}), & 0 < u < u_t, \\\\\np_t \\, h(u - u_t; \\xi, \\beta), & u \\ge u_t,\n\\end{cases}\n$$\nwith the GPD support constraint enforced for $u \\ge u_t$ when $\\xi < 0$ by $u \\le u_{\\max}$ where\n$$\nu_{\\max} = u_t - \\frac{\\beta}{\\xi}.\n$$\nIf tail replacement is not applied, set the composite distribution to the baseline Lognormal for all $u > 0$.\n\n5. The turbine power curve $P(u)$ is represented by a cubic ramp to rated power and a plateau, with cut-in speed $v_{\\mathrm{ci}} > 0$, rated speed $v_r > v_{\\mathrm{ci}}$, cut-out speed $v_{\\mathrm{co}} > v_r$, and rated power $P_r > 0$:\n$$\nP(u) =\n\\begin{cases}\n0, & u < v_{\\mathrm{ci}}, \\\\\nP_r \\left( \\dfrac{u - v_{\\mathrm{ci}}}{v_r - v_{\\mathrm{ci}}} \\right)^3, & v_{\\mathrm{ci}} \\le u < v_r, \\\\\nP_r, & v_r \\le u < v_{\\mathrm{co}}, \\\\\n0, & u \\ge v_{\\mathrm{co}}.\n\\end{cases}\n$$\n\nThe analyst must implement a numerical integration that respects the piecewise definition of $P(u)$ and the composite $f(u)$, including the tail support constraint when $\\xi < 0$. The integral must be computed over $u \\in [0, v_{\\mathrm{co}}]$ since $P(u) = 0$ for $u \\ge v_{\\mathrm{co}}$. Any robust quadrature method may be used, and care must be taken to split the integral across the relevant breakpoints $u \\in \\{ v_{\\mathrm{ci}}, v_r, v_{\\mathrm{co}}, u_t, u_{\\max} \\}$ when applicable.\n\nPhysical units must be respected: wind speeds are in meters per second (m/s), and power must be returned in watts (W). Express the final expected power for each test case in W as an integer rounded to the nearest whole number.\n\nImplement the program to compute the expected power for the following test suite. Each test case provides coarse-grid wind statistics and downscaling parameters, and turbine characteristics:\n\n- Test case $1$ (general happy path):\n  - Coarse mean wind speed $\\bar{u}_c = 7.0$ m/s, coarse variance $\\mathrm{Var}_c = 8.0$ $(\\mathrm{m/s})^2$.\n  - Dynamic downscaling: variability inflation $s = 1.3$, speed-up factor $k = 1.1$.\n  - Statistical tail replacement: threshold $u_t = 15.0$ m/s, shape $\\xi = 0.1$, scale $\\beta = 2.0$ m/s.\n  - Turbine: $v_{\\mathrm{ci}} = 3.0$ m/s, $v_r = 12.0$ m/s, $v_{\\mathrm{co}} = 25.0$ m/s, $P_r = 3000000.0$ W.\n\n- Test case $2$ (stronger variability and heavier tail):\n  - Coarse mean wind speed $\\bar{u}_c = 7.0$ m/s, coarse variance $\\mathrm{Var}_c = 8.0$ $(\\mathrm{m/s})^2$.\n  - Dynamic downscaling: variability inflation $s = 1.5$, speed-up factor $k = 1.0$.\n  - Statistical tail replacement: threshold $u_t = 10.0$ m/s, shape $\\xi = 0.35$, scale $\\beta = 3.0$ m/s.\n  - Turbine: $v_{\\mathrm{ci}} = 3.0$ m/s, $v_r = 12.0$ m/s, $v_{\\mathrm{co}} = 25.0$ m/s, $P_r = 3000000.0$ W.\n\n- Test case $3$ (bounded thin tail, mean reduction):\n  - Coarse mean wind speed $\\bar{u}_c = 8.0$ m/s, coarse variance $\\mathrm{Var}_c = 6.0$ $(\\mathrm{m/s})^2$.\n  - Dynamic downscaling: variability inflation $s = 1.0$, speed-up factor $k = 0.9$.\n  - Statistical tail replacement: threshold $u_t = 13.0$ m/s, shape $\\xi = -0.1$, scale $\\beta = 5.0$ m/s.\n  - Turbine: $v_{\\mathrm{ci}} = 3.0$ m/s, $v_r = 12.0$ m/s, $v_{\\mathrm{co}} = 25.0$ m/s, $P_r = 3000000.0$ W.\n\n- Test case $4$ (baseline with no tail replacement):\n  - Coarse mean wind speed $\\bar{u}_c = 6.0$ m/s, coarse variance $\\mathrm{Var}_c = 5.0$ $(\\mathrm{m/s})^2$.\n  - Dynamic downscaling: variability inflation $s = 1.2$, speed-up factor $k = 1.05$.\n  - Statistical tail replacement: none (use Lognormal only).\n  - Turbine: $v_{\\mathrm{ci}} = 3.0$ m/s, $v_r = 12.0$ m/s, $v_{\\mathrm{co}} = 25.0$ m/s, $P_r = 3000000.0$ W.\n\n- Test case $5$ (lower cut-out, altered turbine, threshold above rated):\n  - Coarse mean wind speed $\\bar{u}_c = 5.0$ m/s, coarse variance $\\mathrm{Var}_c = 4.0$ $(\\mathrm{m/s})^2$.\n  - Dynamic downscaling: variability inflation $s = 1.1$, speed-up factor $k = 1.2$.\n  - Statistical tail replacement: threshold $u_t = 18.0$ m/s, shape $\\xi = 0.2$, scale $\\beta = 2.5$ m/s.\n  - Turbine: $v_{\\mathrm{ci}} = 5.0$ m/s, $v_r = 11.0$ m/s, $v_{\\mathrm{co}} = 20.0$ m/s, $P_r = 2000000.0$ W.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[result1,result2,result3]$`). Each result must be an integer representing the expected power in watts (W), rounded to the nearest whole number, in the order of the test cases $1$ through $5$.",
            "solution": "The problem requires the computation of the expected power output, $\\mathbb{E}[P]$, of a wind turbine under a downscaled wind speed regime. The solution involves defining the mathematical models for the wind speed distribution and the turbine power curve, and then evaluating the expectation integral numerically.\n\nThe expected power $\\mathbb{E}[P]$ is given by the integral of the power curve $P(u)$ weighted by the wind speed probability density function (PDF) $f(u)$:\n$$\n\\mathbb{E}[P] = \\int_{0}^{\\infty} P(u) f(u) \\, du\n$$\nThe functions $P(u)$ and $f(u)$ are piecewise, which necessitates a careful, piecewise approach to the integration.\n\nFirst, we define the composite wind speed PDF, $f(u)$, which is constructed through a two-step downscaling process.\n\n**Step 1: Baseline Lognormal Distribution from Coarse-Grid Data and Dynamic Downscaling**\n\nThe coarse-grid wind speed $U_c$ is characterized by its mean $\\bar{u}_c$ and variance $\\mathrm{Var}_c$. These are used to parameterize a Lognormal distribution. The parameters of the underlying normal distribution in logarithmic space, $\\mu_c$ and $\\sigma_c$, are found using the method of moments:\n$$\n\\sigma_c^2 = \\ln\\left(1 + \\frac{\\mathrm{Var}_c}{\\bar{u}_c^2}\\right)\n$$\n$$\n\\mu_c = \\ln(\\bar{u}_c) - \\frac{1}{2}\\sigma_c^2\n$$\nDynamic downscaling adjusts these parameters to reflect local topographical effects. A speed-up factor $k$ modifies the mean, and a variability inflation factor $s$ modifies the standard deviation in log space. The resulting local Lognormal parameters, $\\mu_{\\ell}$ and $\\sigma_{\\ell}$, are:\n$$\n\\mu_{\\ell} = \\ln(k) + \\mu_c\n$$\n$$\n\\sigma_{\\ell} = s \\sqrt{\\sigma_c^2}\n$$\nThe baseline local wind speed distribution is thus Lognormal, $U_{\\ell} \\sim \\mathrm{Lognormal}(\\mu_{\\ell}, \\sigma_{\\ell})$, with the PDF:\n$$\nf_{\\mathrm{LN}}(u; \\mu_{\\ell}, \\sigma_{\\ell}) = \\frac{1}{u \\sigma_{\\ell} \\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\ln u - \\mu_{\\ell})^2}{2 \\sigma_{\\ell}^2} \\right), \\quad u > 0\n$$\n\n**Step 2: Composite PDF with Statistical Tail Replacement**\n\nFor cases involving statistical downscaling, the upper tail of the baseline Lognormal distribution is replaced with a Generalized Pareto Distribution (GPD) above a certain threshold $u_t$. This is a peaks-over-threshold (POT) model.\n\nThe probability of exceeding the threshold $u_t$ under the baseline distribution is:\n$$\np_t = 1 - F_{\\mathrm{LN}}(u_t; \\mu_{\\ell}, \\sigma_{\\ell}) = 1 - \\Phi\\left( \\frac{\\ln u_t - \\mu_{\\ell}}{\\sigma_{\\ell}} \\right)\n$$\nwhere $F_{\\mathrm{LN}}$ is the cumulative distribution function (CDF) of the Lognormal distribution and $\\Phi$ is the standard normal CDF.\n\nThe distribution of exceedances $Y = U - u_t$ for $U \\ge u_t$ is modeled by the GPD with shape parameter $\\xi$ and scale parameter $\\beta$. The GPD's PDF is given by:\n$$\nh(y; \\xi, \\beta) =\n\\begin{cases}\n\\dfrac{1}{\\beta}\\left(1 + \\dfrac{\\xi y}{\\beta}\\right)^{-1/\\xi - 1} & \\text{for } \\xi \\ne 0 \\\\\n\\dfrac{1}{\\beta}\\exp\\left(-\\dfrac{y}{\\beta}\\right) & \\text{for } \\xi = 0\n\\end{cases}\n$$\nThis is valid for $y \\ge 0$ and, if $\\xi < 0$, for $y \\le -\\beta/\\xi$.\n\nThe final composite PDF $f(u)$ is formed by splicing the Lognormal PDF and the scaled GPD PDF at the threshold $u_t$:\n$$\nf(u) =\n\\begin{cases}\nf_{\\mathrm{LN}}(u; \\mu_{\\ell}, \\sigma_{\\ell}), & 0 < u < u_t \\\\\np_t \\, h(u - u_t; \\xi, \\beta), & u \\ge u_t\n\\end{cases}\n$$\nIf $\\xi < 0$, the GPD tail is bounded, and $f(u) = 0$ for $u > u_{\\max} = u_t - \\beta/\\xi$. If tail replacement is not applied, $f(u) = f_{\\mathrm{LN}}(u; \\mu_{\\ell}, \\sigma_{\\ell})$ for all $u > 0$.\n\n**Turbine Power Curve**\n\nThe turbine's power output $P(u)$ is defined as a piecewise function of the wind speed $u$:\n$$\nP(u) =\n\\begin{cases}\n0, & u < v_{\\mathrm{ci}} \\text{ or } u \\ge v_{\\mathrm{co}} \\\\\nP_r \\left( \\dfrac{u - v_{\\mathrm{ci}}}{v_r - v_{\\mathrm{ci}}} \\right)^3, & v_{\\mathrm{ci}} \\le u < v_r \\\\\nP_r, & v_r \\le u < v_{\\mathrm{co}}\n\\end{cases}\n$$\nwhere $v_{\\mathrm{ci}}$, $v_r$, and $v_{\\mathrm{co}}$ are the cut-in, rated, and cut-out speeds, respectively, and $P_r$ is the rated power. Since $P(u) = 0$ for $u \\ge v_{\\mathrm{co}}$, the expectation integral is non-zero only over the interval $[0, v_{\\mathrm{co}}]$.\n\n**Numerical Integration Strategy**\n\nThe integrand $P(u)f(u)$ is a piecewise function. The points where its functional form changes are derived from the definitions of $P(u)$ and $f(u)$. These breakpoints are $\\{0, v_{\\mathrm{ci}}, v_r, v_{\\mathrm{co}}\\}$, and if applicable, the tail threshold $u_t$. If the GPD tail is bounded ($\\xi < 0$), its upper limit $u_{\\max}$ is also a potential breakpoint.\n\nTo compute the total expectation, the integral is split into a sum of integrals over subintervals defined by these breakpoints.\n$$\n\\mathbb{E}[P] = \\int_{0}^{v_{\\mathrm{co}}} P(u) f(u) \\, du = \\sum_{i=0}^{N-1} \\int_{z_i}^{z_{i+1}} P(u) f(u) \\, du\n$$\nwhere $\\{z_0, z_1, \\dots, z_N\\}$ is the sorted set of unique breakpoints within the range $[0, v_{\\mathrm{co}}]$. On each subinterval $(z_i, z_{i+1})$, the integrand has a single, continuous analytical form, allowing for accurate numerical evaluation using a standard quadrature method, such as `scipy.integrate.quad`.\n\nThe overall algorithm for each test case is as follows:\n1.  Calculate the local Lognormal parameters $\\mu_{\\ell}$ and $\\sigma_{\\ell}$ from the coarse-grid statistics and downscaling factors.\n2.  Define the functions for the Lognormal PDF $f_{\\mathrm{LN}}(u)$, the GPD PDF $h(y)$, and the power curve $P(u)$.\n3.  Construct the composite PDF $f(u)$, including the calculation of the exceedance probability $p_t$ if tail replacement is active.\n4.  Determine the set of unique, sorted integration breakpoints by combining $\\{0, v_{\\mathrm{ci}}, v_r, v_{\\mathrm{co}}\\}$ and, if applicable, $\\{u_t, u_{\\max}\\}$, considering only points within $[0, v_{\\mathrm{co}}]$.\n5.  Iterate through the adjacent pairs of breakpoints, defining the integration intervals. For each interval, calculate $\\int P(u)f(u) \\, du$ using numerical quadrature.\n6.  Sum the results from all intervals to obtain the total expected power $\\mathbb{E}[P]$.\n7.  Round the final value to the nearest integer, as required.\n\nThis structured approach ensures that all piecewise components of the model are handled correctly, yielding an accurate value for the expected power output.",
            "answer": "$$\\boxed{\\text{[975878,895531,617255,417121,304417]}}$$"
        }
    ]
}