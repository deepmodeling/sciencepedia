{
    "hands_on_practices": [
        {
            "introduction": "The timing of developmental stages, or phenology, forms the biological clock of any crop model, governing the duration of growth phases and the plant's response to seasonal cues. This practice guides you through the implementation of a daily-time-step phenology model, integrating the combined effects of temperature, photoperiod, and vernalization on winter wheat development. By building this model from its fundamental biophysical principles, you will gain a concrete understanding of how environmental drivers are translated into developmental progress, a core skill in process-based agricultural modeling .",
            "id": "3860938",
            "problem": "Implement a program that computes sowing-to-flowering duration for a winter wheat cultivar under daily forcing by temperature, photoperiod, and vernalization temperature. The cultivar is characterized by cardinal temperatures, photoperiod sensitivity, vernalization requirement, and an intrinsic development time scale. The computation must adhere to the following physically and biologically grounded principles and definitions, and all outputs must be returned as integer day counts in days.\n\nUse the following fundamental bases:\n\n1. Daily development rate definition: The instantaneous rate of progress toward flowering is proportional to multiplicative environmental response scalars and an intrinsic time scale. Let $r(d)$ be the day-indexed rate on day $d$. Then\n$$\nr(d) = k \\cdot f_T(T(d)) \\cdot f_P(P(d)) \\cdot f_V(V(d)),\n$$\nwhere $k$ is the intrinsic rate constant with units $\\text{day}^{-1}$, $f_T$ is a temperature response scalar in $[0,1]$, $f_P$ is a photoperiod response scalar in $[0,1]$, and $f_V$ is a vernalization completion scalar in $[0,1]$.\n\n2. Event criterion: Flowering occurs on the earliest day $D$ such that the cumulative development sum reaches a cultivar-specific threshold $D_{\\text{req}}$:\n$$\n\\sum_{i=1}^{D} r(i) \\ge D_{\\text{req}}.\n$$\nIf no such $D$ exists within the provided daily series, the output for that case must be $-1$.\n\n3. Temperature response using cardinal temperatures: Let the cardinal temperatures be base $T_b$, optimum $T_o$, and ceiling $T_c$ in $\\degree \\mathrm{C}$. Define $f_T(T)$ by a piecewise linear function:\n- If $T \\le T_b$, $f_T(T) = 0$.\n- If $T_b < T \\le T_o$, $f_T(T) = \\dfrac{T - T_b}{T_o - T_b}$.\n- If $T_o < T < T_c$, $f_T(T) = \\dfrac{T_c - T}{T_c - T_o}$.\n- If $T \\ge T_c$, $f_T(T) = 0$.\n\n4. Photoperiod response for a long-day cereal: Let the critical photoperiod be $P_{\\text{crit}}$ and the saturating photoperiod be $P_{\\text{opt}}$ in $\\mathrm{hours}$. Introduce a minimum photoperiod factor $s_{P,\\min}$, representing residual development under very short days. Define $f_P(P)$ as a bounded linear scalar:\n- If $P \\le P_{\\text{crit}}$, $f_P(P) = s_{P,\\min}$.\n- If $P_{\\text{crit}} < P < P_{\\text{opt}}$, \n$$\nf_P(P) = s_{P,\\min} + \\left( \\frac{P - P_{\\text{crit}}}{P_{\\text{opt}} - P_{\\text{crit}}} \\right) \\cdot (1 - s_{P,\\min}).\n$$\n- If $P \\ge P_{\\text{opt}}$, $f_P(P) = 1$.\n\n5. Vernalization accumulation and effect: The daily vernalization unit accrual $a_V(T_v)$ is a triangular function of vernalization temperature $T_v$ in $\\degree \\mathrm{C}$ with parameters $T_{v,\\min}$, $T_{v,\\opt}$, and $T_{v,\\max}$:\n- If $T_v \\le T_{v,\\min}$, $a_V(T_v) = 0$.\n- If $T_{v,\\min} < T_v \\le T_{v,\\opt}$,\n$$\na_V(T_v) = \\frac{T_v - T_{v,\\min}}{T_{v,\\opt} - T_{v,\\min}}.\n$$\n- If $T_{v,\\opt} < T_v < T_{v,\\max}$,\n$$\na_V(T_v) = \\frac{T_{v,\\max} - T_v}{T_{v,\\max} - T_{v,\\opt}}.\n$$\n- If $T_v \\ge T_{v,\\max}$, $a_V(T_v) = 0$.\n\nCumulative vernalization is\n$$\nV(d) = \\min\\left(V_{\\text{req}}, \\sum_{i=1}^{d} a_V(T_v(i))\\right),\n$$\nand the vernalization effect scalar is\n$$\nf_V(V(d)) = \\frac{V(d)}{V_{\\text{req}}}.\n$$\n\nCultivar parameters (constant unless otherwise specified per test case):\n- Cardinal temperatures: $T_b = 0 \\ \\degree \\mathrm{C}$, $T_o = 18 \\ \\degree \\mathrm{C}$, $T_c = 35 \\ \\degree \\mathrm{C}$.\n- Photoperiod sensitivity: $P_{\\text{crit}} = 8 \\ \\mathrm{hours}$, $P_{\\text{opt}} = 16 \\ \\mathrm{hours}$, $s_{P,\\min} = 0.5$.\n- Vernalization parameters: $T_{v,\\min} = 0 \\ \\degree \\mathrm{C}$, $T_{v,\\opt} = 6 \\ \\degree \\mathrm{C}$, $T_{v,\\max} = 15 \\ \\degree \\mathrm{C}$, $V_{\\text{req}} = 40$ (dimensionless units).\n- Development threshold: $D_{\\text{req}} = 1.0$ (dimensionless).\n\nIntrinsic development rate constant $k$ (specified per test case): see Test Suite below. Units of $k$ are $\\text{day}^{-1}$.\n\nDaily series generation and units:\n- All temperatures $T(d)$ and $T_v(d)$ are in $\\degree \\mathrm{C}$.\n- All photoperiods $P(d)$ are in $\\mathrm{hours}$.\n- All day counts must be reported in $\\mathrm{days}$ as integers.\n\nTest Suite (four cases; each case defines $N$ days and daily series):\n\n- Case $1$ (temperate seasonal progression, typical winter wheat, happy path):\n    - $N = 220$.\n    - $T(d) = 6 + 7 \\cdot \\sin\\left(\\dfrac{2\\pi (d - 20)}{365}\\right)$ for $d = 1,2,\\dots,N$.\n    - $P(d) = 12 + 4 \\cdot \\sin\\left(\\dfrac{2\\pi (d - 20)}{365}\\right)$.\n    - $T_v(d) = T(d)$.\n    - $k = 0.012$.\n\n- Case $2$ (boundary photoperiod at the critical threshold; same thermal forcing):\n    - $N = 220$.\n    - $T(d) = 6 + 7 \\cdot \\sin\\left(\\dfrac{2\\pi (d - 20)}{365}\\right)$.\n    - $P(d) = P_{\\text{crit}} = 8$ for all $d$.\n    - $T_v(d) = T(d)$.\n    - $k = 0.012$.\n\n- Case $3$ (insufficient vernalization due to warm temperatures):\n    - $N = 200$.\n    - $T(d) = 20 + 4 \\cdot \\sin\\left(\\dfrac{2\\pi d}{365}\\right)$.\n    - $P(d) = 12 + 2 \\cdot \\sin\\left(\\dfrac{2\\pi d}{365}\\right)$.\n    - $T_v(d) = T(d)$.\n    - $k = 0.012$.\n\n- Case $4$ (slower cultivar under supportive temperatures):\n    - $N = 250$.\n    - $T(d) = 14 + 6 \\cdot \\sin\\left(\\dfrac{2\\pi (d - 60)}{365}\\right)$.\n    - $P(d) = 11 + 3 \\cdot \\sin\\left(\\dfrac{2\\pi (d - 60)}{365}\\right)$.\n    - $T_v(d) = T(d)$.\n    - $k = 0.008$.\n\nAlgorithmic task:\n- For each case, compute $f_T(T(d))$, $f_P(P(d))$, $a_V(T_v(d))$, $V(d)$, $f_V(V(d))$, and $r(d)$ for all $d$, then determine the smallest day $D$ such that $\\sum_{i=1}^{D} r(i) \\ge D_{\\text{req}}$. If such $D$ does not exist for $d \\in \\{1,\\dots,N\\}$, report $-1$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the integer number of days from sowing to flowering in $\\mathrm{days}$, or $-1$ if flowering does not occur within the provided $N$ days.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of environmental and earth system modeling, specifically crop phenology, and is mathematically well-posed, objective, and self-contained. All necessary equations, parameters, and environmental forcing data are provided to permit a unique and verifiable solution.\n\nThe solution is found by implementing a discrete-time daily simulation model for each of the four test cases. The model calculates the progress towards flowering for a winter wheat cultivar based on daily environmental inputs. The core of the model is the calculation of a daily development rate, $r(d)$, which is then integrated over time until a specified development threshold is met.\n\nThe daily development rate on day $d$ is given by the equation:\n$$\nr(d) = k \\cdot f_T(T(d)) \\cdot f_P(P(d)) \\cdot f_V(V(d))\n$$\nHere, $k$ is the cultivar's intrinsic development rate constant, given in units of $\\text{day}^{-1}$. The terms $f_T$, $f_P$, and $f_V$ are dimensionless scalars between $0$ and $1$ that represent the limiting effects of temperature, photoperiod, and vernalization, respectively.\n\nFor each day $d$ in the simulation, from $d=1$ up to the maximum number of days $N$ specified for the case, the following steps are executed:\n\n1.  **Environmental Forcing Calculation**: The daily mean temperature $T(d)$, photoperiod $P(d)$, and vernalization temperature $T_v(d)$ are calculated according to the sinusoidal functions provided for each test case. For all cases, it is specified that $T_v(d) = T(d)$.\n\n2.  **Temperature Response Scalar, $f_T(T(d))$**: This scalar is calculated using a piecewise linear (triangular) function of the daily temperature $T(d)$ and the cultivar's cardinal temperatures: base $T_b = 0 \\ \\degree \\mathrm{C}$, optimum $T_o = 18 \\ \\degree \\mathrm{C}$, and ceiling $T_c = 35 \\ \\degree \\mathrm{C}$. The function is defined as:\n    -   $f_T(T) = 0$ if $T \\le T_b$ or $T \\ge T_c$.\n    -   $f_T(T) = (T - T_b) / (T_o - T_b)$ if $T_b < T \\le T_o$.\n    -   $f_T(T) = (T_c - T) / (T_c - T_o)$ if $T_o < T < T_c$.\n    This function evaluates to $0$ outside the growth-permissive temperature range and scales linearly to a maximum of $1$ at the optimum temperature.\n\n3.  **Photoperiod Response Scalar, $f_P(P(d))$**: This scalar quantifies the plant's response to day length, $P(d)$. For a long-day plant like winter wheat, development accelerates with longer days. The function is a bounded linear model based on a critical photoperiod $P_{\\text{crit}} = 8 \\ \\mathrm{hours}$, a saturating photoperiod $P_{\\text{opt}} = 16 \\ \\mathrm{hours}$, and a minimum response factor $s_{P,\\min} = 0.5$.\n    -   If $P \\le P_{\\text{crit}}$, development proceeds at a baseline rate, so $f_P(P) = s_{P,\\min}$.\n    -   If $P \\ge P_{\\text{opt}}$, the photoperiod requirement is fully met, so $f_P(P) = 1$.\n    -   If $P_{\\text{crit}} < P < P_{\\text{opt}}$, the response interpolates linearly: $f_P(P) = s_{P,\\min} + ( (P - P_{\\text{crit}}) / (P_{\\text{opt}} - P_{\\text{crit}}) ) \\cdot (1 - s_{P,\\min})$.\n\n4.  **Vernalization Response Scalar, $f_V(V(d))$**: This component models the fulfillment of the chilling requirement, which is essential for winter wheat to become competent to flower. The calculation is a two-step process involving state accumulation.\n    -   First, the daily vernalization unit accrual, $a_V(T_v(d))$, is computed based on the vernalization temperature $T_v(d)$. This also follows a triangular response function with cardinal vernalization temperatures $T_{v,\\min} = 0 \\ \\degree \\mathrm{C}$, $T_{v,\\opt} = 6 \\ \\degree \\mathrm{C}$, and $T_{v,\\max} = 15 \\ \\degree \\mathrm{C}$. No vernalization occurs if the temperature is below $T_{v,\\min}$ or above $T_{v,\\max}$, and the maximum rate of accrual ($1$ unit per day) occurs at $T_{v,\\opt}$.\n    -   These daily units are accumulated over time: $V_{\\text{accum}}(d) = \\sum_{i=1}^{d} a_V(T_v(i))$.\n    -   The effective cumulative vernalization, $V(d)$, is the lesser of the accumulated total and the full requirement, $V_{\\text{req}} = 40$ units: $V(d) = \\min(V_{\\text{req}}, V_{\\text{accum}}(d))$.\n    -   Finally, the vernalization scalar $f_V(V(d))$ is the proportion of the vernalization requirement that has been satisfied: $f_V(V(d)) = V(d) / V_{\\text{req}}$. This value increases from $0$ to $1$ as the chilling requirement is met.\n\n5.  **Cumulative Development and Event Criterion**: The daily rate $r(d)$ is calculated by multiplying the intrinsic rate $k$ by the three environmental scalars. This rate is added to a running total, `cumulative_development_sum`. The simulation checks on each day $d$ if this sum has reached or exceeded the total development requirement, $D_{\\text{req}} = 1.0$. The day $D$ on which this condition is first met, $\\sum_{i=1}^{D} r(i) \\ge D_{\\text{req}}$, is the predicted day of flowering.\n\n6.  **Termination**: The simulation for a given case terminates in one of two ways. If the flowering criterion is met on day $D$, the integer $D$ is recorded as the result. If the simulation completes all $N$ days without the criterion being met, it signifies that flowering did not occur, and the result is recorded as $-1$.\n\nThis entire procedure is independently applied to each of the four test cases defined in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the sowing-to-flowering duration for a winter wheat cultivar\n    based on a daily-time-step phenology model.\n    \"\"\"\n\n    # Cultivar parameters (constant across all cases)\n    T_b, T_o, T_c = 0.0, 18.0, 35.0  # Cardinal temperatures for growth\n    P_crit, P_opt, s_P_min = 8.0, 16.0, 0.5  # Photoperiod parameters\n    T_v_min, T_v_opt, T_v_max = 0.0, 6.0, 15.0  # Cardinal vernalization temperatures\n    V_req = 40.0  # Vernalization requirement\n    D_req = 1.0  # Development threshold\n\n    def calculate_f_T(T):\n        \"\"\"Calculates the temperature response scalar f_T.\"\"\"\n        if T <= T_b or T >= T_c:\n            return 0.0\n        elif T_b < T <= T_o:\n            return (T - T_b) / (T_o - T_b)\n        else:  # T_o < T < T_c\n            return (T_c - T) / (T_c - T_o)\n\n    def calculate_f_P(P):\n        \"\"\"Calculates the photoperiod response scalar f_P.\"\"\"\n        if P <= P_crit:\n            return s_P_min\n        elif P >= P_opt:\n            return 1.0\n        else:  # P_crit < P < P_opt\n            return s_P_min + ((P - P_crit) / (P_opt - P_crit)) * (1.0 - s_P_min)\n            \n    def calculate_a_V(T_v):\n        \"\"\"Calculates the daily vernalization unit accrual a_V.\"\"\"\n        if T_v <= T_v_min or T_v >= T_v_max:\n            return 0.0\n        elif T_v_min < T_v <= T_v_opt:\n            return (T_v - T_v_min) / (T_v_opt - T_v_min)\n        else: # T_v_opt < T_v < T_v_max\n            return (T_v_max - T_v) / (T_v_max - T_v_opt)\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"N\": 220, \"k\": 0.012,\n            \"T_func\": lambda d: 6.0 + 7.0 * np.sin(2 * np.pi * (d - 20) / 365.0),\n            \"P_func\": lambda d: 12.0 + 4.0 * np.sin(2 * np.pi * (d - 20) / 365.0),\n        },\n        {\n            \"N\": 220, \"k\": 0.012,\n            \"T_func\": lambda d: 6.0 + 7.0 * np.sin(2 * np.pi * (d - 20) / 365.0),\n            \"P_func\": lambda d: P_crit,\n        },\n        {\n            \"N\": 200, \"k\": 0.012,\n            \"T_func\": lambda d: 20.0 + 4.0 * np.sin(2 * np.pi * d / 365.0),\n            \"P_func\": lambda d: 12.0 + 2.0 * np.sin(2 * np.pi * d / 365.0),\n        },\n        {\n            \"N\": 250, \"k\": 0.008,\n            \"T_func\": lambda d: 14.0 + 6.0 * np.sin(2 * np.pi * (d - 60) / 365.0),\n            \"P_func\": lambda d: 11.0 + 3.0 * np.sin(2 * np.pi * (d - 60) / 365.0),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        k = case[\"k\"]\n        T_func = case[\"T_func\"]\n        P_func = case[\"P_func\"]\n\n        cumulative_development_sum = 0.0\n        cumulative_vernalization_units = 0.0\n        flowering_day = -1\n\n        for d in range(1, N + 1):\n            # Calculate daily environmental drivers\n            T_d = T_func(d)\n            P_d = P_func(d)\n            T_v_d = T_d\n\n            # Calculate response scalars\n            f_T_d = calculate_f_T(T_d)\n            f_P_d = calculate_f_P(P_d)\n\n            # Update vernalization status and calculate f_V\n            a_V_d = calculate_a_V(T_v_d)\n            cumulative_vernalization_units += a_V_d\n            V_d = min(V_req, cumulative_vernalization_units)\n            f_V_d = V_d / V_req if V_req > 0 else 1.0\n\n            # Calculate daily development rate\n            r_d = k * f_T_d * f_P_d * f_V_d\n\n            # Update cumulative development sum\n            cumulative_development_sum += r_d\n\n            # Check for flowering\n            if cumulative_development_sum >= D_req:\n                flowering_day = d\n                break\n        \n        results.append(flowering_day)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The foundation of crop yield is biomass accumulation, which is primarily driven by photosynthesis. Accurately quantifying the amount of light intercepted by the crop canopy is therefore a critical first step in simulating growth. This exercise applies the Beer-Lambert law to calculate daily intercepted photosynthetically active radiation (PAR), contrasting a simple constant light extinction coefficient with a more physically realistic one that varies with the solar zenith angle . This practice demonstrates how seemingly small assumptions in model structure can significantly influence predictions of resource capture and potential yield.",
            "id": "3860944",
            "problem": "You are to develop a complete and runnable program that computes daily intercepted photosynthetically active radiation (PAR) for a horizontally homogeneous crop canopy with a measured Leaf Area Index (LAI), using two alternative formulations of the shortwave extinction coefficient: a constant value and a value that depends on solar zenith angle. The scientific context is agricultural yield modeling within environmental and earth system modeling. The derivation and computation must begin from well-tested physical principles of radiative transfer, specifically exponential attenuation of beam radiation in absorbing media. Your implementation must be universal and purely mathematical, relying on the specified input parameters without any external input. All angles must be treated in degrees in the input, with any internal trigonometric functions applied to angles converted to radians. Energy must be expressed in megajoules per square meter ($\\text{MJ m}^{-2}$). The final result for each test case must be the daily intercepted PAR in $\\text{MJ m}^{-2} \\text{ day}^{-1}$, rounded to $3$ decimals.\n\nScientific base and assumptions to be used:\n- Photosynthetically Active Radiation (PAR) is the portion of shortwave radiation relevant for photosynthesis and is treated as beam radiation at the canopy top for the purpose of this task.\n- Exponential attenuation in a homogeneous absorbing medium (Beer–Lambert law) applies to a horizontally homogeneous canopy with leaf elements characterized by a leaf angle distribution.\n- The extinction coefficient for a random (spherical) leaf angle distribution depends on the projection function and the solar zenith angle. For the constant-coefficient case, use a single diffuse-light extinction value. For the solar-zenith-dependent case, use the spherical-leaf projection function normalization.\n- Ignore nighttime periods with zero PAR. When PAR is positive and the solar zenith angle approaches the horizon, physically consistent limits should be respected to avoid singularities in computation.\n\nYour program must:\n- For each provided test case, compute the daily intercepted PAR using two models:\n  1. A constant extinction coefficient model with a specified constant $k_c$.\n  2. A solar-zenith-dependent extinction coefficient model $k(\\theta)$ consistent with a spherical leaf angle distribution.\n- For each hour $h$ with hourly PAR $I_{h}$ in $\\text{MJ m}^{-2} \\text{ h}^{-1}$ and solar zenith angle $\\theta_h$ in degrees, compute the intercepted fraction based on Beer–Lambert attenuation across the canopy, aggregate the intercepted hourly PAR across all daylight hours, and return the daily total intercepted PAR in $\\text{MJ m}^{-2} \\text{ day}^{-1}$ for both models, rounded to $3$ decimals.\n\nTest suite and parameters:\n- Use the following four test cases. Each case provides an array of daylight-hour PAR values $[I_{1}, I_{2}, \\dots]$ in $\\text{MJ m}^{-2} \\text{ h}^{-1}$ and corresponding solar zenith angles $[\\theta_{1}, \\theta_{2}, \\dots]$ in degrees, along with a measured LAI and a constant extinction coefficient $k_c$. In all cases, adopt the spherical leaf angle distribution normalization $G = 0.5$ for the solar-zenith-dependent formulation.\n\n- Test Case $1$ (happy path, mid-latitude summer-like):\n  - LAI $= 3.0$, $k_c = 0.5$.\n  - Hourly PAR ($\\text{MJ m}^{-2} \\text{ h}^{-1}$): $[0.12, 0.28, 0.55, 0.82, 1.05, 1.15, 1.12, 1.00, 0.78, 0.50, 0.28, 0.12]$.\n  - Solar zenith angles (degrees): $[80, 70, 60, 50, 40, 30, 25, 30, 40, 50, 60, 70]$.\n\n- Test Case $2$ (boundary high-latitude with large solar zenith):\n  - LAI $= 2.0$, $k_c = 0.5$.\n  - Hourly PAR ($\\text{MJ m}^{-2} \\text{ h}^{-1}$): $[0.05, 0.08, 0.12, 0.18, 0.22, 0.22, 0.18, 0.12, 0.08, 0.05]$.\n  - Solar zenith angles (degrees): $[85, 80, 75, 70, 65, 65, 70, 75, 80, 85]$.\n\n- Test Case $3$ (edge case, leafless canopy):\n  - LAI $= 0.0$, $k_c = 0.5$.\n  - Hourly PAR ($\\text{MJ m}^{-2} \\text{ h}^{-1}$): $[0.12, 0.28, 0.55, 0.82, 1.05, 1.15, 1.12, 1.00, 0.78, 0.50, 0.28, 0.12]$.\n  - Solar zenith angles (degrees): $[80, 70, 60, 50, 40, 30, 25, 30, 40, 50, 60, 70]$.\n\n- Test Case $4$ (edge case, dense canopy):\n  - LAI $= 6.0$, $k_c = 0.5$.\n  - Hourly PAR ($\\text{MJ m}^{-2} \\text{ h}^{-1}$): $[0.10, 0.20, 0.40, 0.80, 1.20, 1.40, 1.30, 1.10, 0.90, 0.60, 0.30, 0.10]$.\n  - Solar zenith angles (degrees): $[80, 70, 60, 50, 40, 30, 25, 30, 40, 50, 60, 70]$.\n\nOutput specification:\n- For each test case, produce a two-element list $[D_c, D_v]$ where $D_c$ is the daily intercepted PAR in $\\text{MJ m}^{-2} \\text{ day}^{-1}$ using the constant extinction model and $D_v$ is the daily intercepted PAR in $\\text{MJ m}^{-2} \\text{ day}^{-1}$ using the solar-zenith-dependent extinction model. Each value must be rounded to $3$ decimals.\n- Aggregate the results from all test cases into a single line of output as a comma-separated list enclosed in square brackets, with each test case represented as its own bracketed pair, for example: $[[D_{c1},D_{v1}],[D_{c2},D_{v2}],\\dots]$.\n\nYour program must produce exactly one line of output in this format and must not read any input from the user or external sources.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and all necessary parameters for a unique solution are provided. The solution is derived from the fundamental principles of radiative transfer in plant canopies.\n\nThe core principle governing the absorption of light in a crop canopy is the Beer-Lambert law, which describes the exponential decay of radiation traversing an absorbing medium. For a horizontally homogeneous canopy, the photosynthetically active radiation (PAR) flux, $I$, at a cumulative leaf area index $L$ from the top of the canopy is given by:\n$$\nI(L) = I_0 \\exp(-k \\cdot L)\n$$\nwhere $I_0$ is the incident PAR flux at the canopy top ($L=0$) and $k$ is the light extinction coefficient.\n\nThe amount of PAR intercepted by the entire canopy, with a total Leaf Area Index (LAI), is the difference between the incident radiation and the radiation that penetrates to the ground ($L = \\text{LAI}$). The fraction of intercepted radiation, $f_{\\text{int}}$, is:\n$$\nf_{\\text{int}} = \\frac{I_0 - I(\\text{LAI})}{I_0} = 1 - \\exp(-k \\cdot \\text{LAI})\n$$\nThe problem requires calculating the total daily intercepted PAR by summing up the hourly intercepted PAR over all daylight hours. For each hour $h$, the intercepted PAR, $I_{\\text{int}, h}$, is:\n$$\nI_{\\text{int}, h} = I_h \\cdot f_{\\text{int}, h} = I_h \\cdot (1 - \\exp(-k_h \\cdot \\text{LAI}))\n$$\nwhere $I_h$ is the incident PAR for hour $h$, and $k_h$ is the corresponding hourly extinction coefficient. The total daily intercepted PAR, $D$, is the sum:\n$$\nD = \\sum_{h=1}^{N} I_{\\text{int}, h}\n$$\nwhere $N$ is the number of daylight hours. The problem asks for this calculation to be performed using two different models for the extinction coefficient $k$.\n\n**Model 1: Constant Extinction Coefficient**\n\nThis model assumes a single, constant extinction coefficient, $k_c$, for the entire day. This is a common simplification, often representative of diffuse light conditions or a daily average. The daily intercepted PAR, $D_c$, is given by:\n$$\nD_c = \\sum_{h=1}^{N} I_h \\cdot (1 - \\exp(-k_c \\cdot \\text{LAI}))\n$$\nSince the term $(1 - \\exp(-k_c \\cdot \\text{LAI}))$ is constant for all hours, it can be factored out of the summation:\n$$\nD_c = \\left(1 - \\exp(-k_c \\cdot \\text{LAI})\\right) \\sum_{h=1}^{N} I_h\n$$\n\n**Model 2: Solar-Zenith-Dependent Extinction Coefficient**\n\nThis model uses an extinction coefficient that varies with the solar zenith angle, $\\theta$. For beam radiation, the extinction coefficient is a function of the path length of the light through the canopy, given by:\n$$\nk(\\theta) = \\frac{G(\\theta)}{\\cos(\\theta)}\n$$\nHere, $G(\\theta)$ is the leaf orientation function, which represents the fraction of leaf area projected towards the sun. The problem specifies a spherical leaf angle distribution, for which the projection function is constant, $G(\\theta) = 0.5$. Therefore, the variable extinction coefficient $k_v$ for hour $h$ with zenith angle $\\theta_h$ is:\n$$\nk_v(\\theta_h) = \\frac{0.5}{\\cos(\\theta_h)}\n$$\nNote that the solar zenith angles $\\theta_h$ are provided in degrees and must be converted to radians for use in trigonometric functions like $\\cos()$.\nThe daily intercepted PAR for this model, $D_v$, is calculated by summing the hourly interceptions, where the extinction coefficient changes each hour:\n$$\nD_v = \\sum_{h=1}^{N} \\left[ I_h \\cdot \\left(1 - \\exp\\left(-\\frac{0.5}{\\cos(\\theta_h)} \\cdot \\text{LAI}\\right)\\right) \\right]\n$$\nUnlike the constant coefficient model, the interception fraction term cannot be factored out of the summation.\n\n**Computational Implementation**\n\nFor each test case, we are given the LAI, the constant extinction coefficient $k_c$, an array of hourly PAR values $\\{I_h\\}$, and an array of corresponding solar zenith angles $\\{\\theta_h\\}$.\n\n1.  For the constant model, we first sum all hourly PAR values to get the total daily incident PAR, $\\sum I_h$. Then, we calculate $D_c$ using its simplified formula.\n\n2.  For the variable model, we iterate through each hour. In each iteration, we convert the zenith angle $\\theta_h$ to radians, calculate $k_v(\\theta_h)$, compute the hourly intercepted PAR $I_{\\text{int}, h}$, and add it to a running total for $D_v$.\n\n3.  A special case is when LAI $= 0$, which represents a leafless canopy or bare ground. In this case, the term $\\exp(-k \\cdot \\text{LAI})$ becomes $\\exp(0) = 1$, making the interception fraction $1-1=0$. Therefore, for LAI $= 0$, both $D_c$ and $D_v$ are correctly computed as $0$.\n\nThe final results, $D_c$ and $D_v$, for each test case are rounded to $3$ decimals as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes daily intercepted PAR for a crop canopy using two extinction coefficient models.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (happy path, mid-latitude summer-like)\n        {\n            \"LAI\": 3.0, \"k_c\": 0.5,\n            \"par\": np.array([0.12, 0.28, 0.55, 0.82, 1.05, 1.15, 1.12, 1.00, 0.78, 0.50, 0.28, 0.12]),\n            \"zenith_angles\": np.array([80, 70, 60, 50, 40, 30, 25, 30, 40, 50, 60, 70])\n        },\n        # Test Case 2 (boundary high-latitude with large solar zenith)\n        {\n            \"LAI\": 2.0, \"k_c\": 0.5,\n            \"par\": np.array([0.05, 0.08, 0.12, 0.18, 0.22, 0.22, 0.18, 0.12, 0.08, 0.05]),\n            \"zenith_angles\": np.array([85, 80, 75, 70, 65, 65, 70, 75, 80, 85])\n        },\n        # Test Case 3 (edge case, leafless canopy)\n        {\n            \"LAI\": 0.0, \"k_c\": 0.5,\n            \"par\": np.array([0.12, 0.28, 0.55, 0.82, 1.05, 1.15, 1.12, 1.00, 0.78, 0.50, 0.28, 0.12]),\n            \"zenith_angles\": np.array([80, 70, 60, 50, 40, 30, 25, 30, 40, 50, 60, 70])\n        },\n        # Test Case 4 (edge case, dense canopy)\n        {\n            \"LAI\": 6.0, \"k_c\": 0.5,\n            \"par\": np.array([0.10, 0.20, 0.40, 0.80, 1.20, 1.40, 1.30, 1.10, 0.90, 0.60, 0.30, 0.10]),\n            \"zenith_angles\": np.array([80, 70, 60, 50, 40, 30, 25, 30, 40, 50, 60, 70])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        lai = case[\"LAI\"]\n        k_c = case[\"k_c\"]\n        hourly_par = case[\"par\"]\n        zenith_deg = case[\"zenith_angles\"]\n\n        # Handle the edge case of LAI = 0 explicitly for clarity,\n        # though the formulas would yield 0 anyway.\n        if lai == 0.0:\n            d_c = 0.0\n            d_v = 0.0\n        else:\n            # Model 1: Constant extinction coefficient (k_c)\n            # D_c = (1 - exp(-k_c * LAI)) * sum(I_h)\n            total_daily_par = np.sum(hourly_par)\n            interception_fraction_c = 1.0 - np.exp(-k_c * lai)\n            d_c = interception_fraction_c * total_daily_par\n\n            # Model 2: Solar-zenith-dependent extinction coefficient k(theta)\n            # D_v = sum [ I_h * (1 - exp(-k_v(theta_h) * LAI)) ]\n            # k_v(theta) = G / cos(theta) with G = 0.5\n            zenith_rad = np.deg2rad(zenith_deg)\n            cos_zenith = np.cos(zenith_rad)\n            \n            # The problem guarantees zenith angles < 90, so cos_zenith > 0.\n            # No need to check for division by zero.\n            k_v = 0.5 / cos_zenith\n            \n            interception_fraction_v = 1.0 - np.exp(-k_v * lai)\n            hourly_intercepted_par_v = hourly_par * interception_fraction_v\n            d_v = np.sum(hourly_intercepted_par_v)\n            \n        # Round results to 3 decimal places\n        d_c_rounded = round(d_c, 3)\n        d_v_rounded = round(d_v, 3)\n        \n        all_results.append(f\"[{d_c_rounded:.3f},{d_v_rounded:.3f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A model's value is ultimately determined by its ability to skillfully predict real-world outcomes. Rigorous evaluation against observational data is therefore an indispensable part of the modeling workflow, ensuring that our simulations are not just theoretically sound but practically useful. This practice introduces k-fold cross-validation, a standard and powerful technique for assessing how well a model generalizes to unseen data . By implementing this procedure for a common bias-correction model, you will develop essential skills for quantifying model performance and building confidence in your predictions.",
            "id": "3860914",
            "problem": "You are provided with two datasets of agricultural yields across multiple years and sites. In each dataset, there are paired observations of simulated yields and observed yields. Let the observed yields be denoted by $o_i$ (in tonnes per hectare, t/ha) and the simulated yields by $s_i$ (in tonnes per hectare, t/ha) for sample index $i$. To assess the generalization ability of a simple bias-correction model and evaluate its predictive performance, you must implement a deterministic $k$-fold cross-validation across all samples in each dataset and compute specified evaluation metrics on the pooled test predictions. The cross-validation procedure must satisfy the following constraints:\n- Samples are ordered by site identifier (ascending) and then by year (ascending).\n- The fold index for sample $i$ is $f_i = i \\bmod k$, where $i$ starts from $0$ after ordering, and $k$ is the number of folds.\n- For each fold $j \\in \\{0,1,\\dots,k-1\\}$, the training set consists of all samples with $f_i \\neq j$, and the test set consists of samples with $f_i = j$.\n\nThe predictive model to be fit on each training set is a linear bias-correction mapping from simulated to observed yield,\n$$\n\\hat{o}_i = a\\, s_i + b,\n$$\nwith scalar parameters $a$ and $b$. These parameters must be estimated by ordinary least squares that minimizes the sum of squared residuals on the training set. After fitting, you must generate predictions $\\hat{o}_i$ on the corresponding test set, and finally pool all test predictions across folds to compute the requested evaluation metric on the pooled test set.\n\nThe evaluation metrics are defined from first principles based on the error $e_i = \\hat{o}_i - o_i$:\n- Root Mean Squared Error (RMSE, in t/ha): $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} e_i^2}.$$\n- Mean Absolute Error (MAE, in t/ha): $$\\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |e_i|.$$\n- Bias (in t/ha): $$\\mathrm{Bias} = \\frac{1}{n}\\sum_{i=1}^{n} e_i.$$\n- Nash–Sutcliffe efficiency (dimensionless): $$\\mathrm{NSE} = 1 - \\frac{\\sum_{i=1}^{n} e_i^2}{\\sum_{i=1}^{n} (o_i - \\bar{o})^2},$$ where $\\bar{o}$ is the mean of the observed yields over the pooled test set.\n\nAll yields are in tonnes per hectare (t/ha). Any intermediate or final values must be computed consistently with these units, and any metric that is in physical units must be expressed in t/ha. The Nash–Sutcliffe efficiency is unitless.\n\nYour program must implement the above procedure for the following test suite. Each dataset is fully specified below as $(\\text{site}, \\text{year}, s, o)$ tuples where $s$ and $o$ are in t/ha:\n\nDataset A (multi-site, multi-year):\n- Site $0$: years $2015$ to $2018$\n  - $(0, 2015, 4.9, 5.2)$, $(0, 2016, 5.7, 5.6)$, $(0, 2017, 6.0, 6.1)$, $(0, 2018, 5.5, 5.8)$\n- Site $1$: years $2015$ to $2018$\n  - $(1, 2015, 6.5, 7.1)$, $(1, 2016, 7.0, 6.8)$, $(1, 2017, 7.3, 7.4)$, $(1, 2018, 7.2, 7.0)$\n- Site $2$: years $2015$ to $2018$\n  - $(2, 2015, 3.5, 3.8)$, $(2, 2016, 4.0, 4.2)$, $(2, 2017, 4.7, 4.5)$, $(2, 2018, 3.9, 4.0)$\n\nDataset B (single-site, multi-year):\n- Site $3$: years $2015$ to $2019$\n  - $(3, 2015, 3.5, 4.0)$, $(3, 2016, 4.2, 4.0)$, $(3, 2017, 3.8, 4.0)$, $(3, 2018, 4.5, 4.0)$, $(3, 2019, 4.0, 4.0)$\n\nFor each of the following test cases, implement the deterministic $k$-fold cross-validation procedure described above and compute the metric on the pooled test predictions:\n- Case $1$: Dataset A, $k=3$, metric $\\mathrm{NSE}$ (dimensionless). Report the result as a decimal rounded to six digits after the decimal point.\n- Case $2$: Dataset A, $k=4$, metric $\\mathrm{RMSE}$ (in t/ha). Report the result as a decimal rounded to six digits after the decimal point.\n- Case $3$: Dataset B, $k=5$, metric $\\mathrm{Bias}$ (in t/ha). Report the result as a decimal rounded to six digits after the decimal point.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Case $1$, Case $2$, Case $3$, with each number formatted to exactly six digits after the decimal point (for example, `[0.123456,0.654321,-0.000100]`).",
            "solution": "We begin from fundamental definitions used in environmental and earth system modeling for deterministic evaluation of predictive models. Yield prediction is assessed by comparing simulated yields to observed yields using error-based metrics. Let the observed yields be $o_i \\in \\mathbb{R}$ and the simulated yields be $s_i \\in \\mathbb{R}$, both measured in tonnes per hectare (t/ha). We define the prediction model as a linear bias-correction mapping,\n$$\n\\hat{o}_i = a\\, s_i + b,\n$$\nwhere $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are parameters to be estimated from data. The modeling assumption is that the relationship between simulated and observed yields can be approximated by an affine transformation, and random errors are captured by the residuals $\\epsilon_i = o_i - \\hat{o}_i$.\n\nTo estimate $a$ and $b$ from a training set, we invoke ordinary least squares based on the well-tested principle of minimizing the sum of squared residuals. Let the training set consist of $m$ samples indexed by $i \\in \\{1,\\dots,m\\}$. Define the design matrix\n$$\nX = \n\\begin{bmatrix}\ns_1 & 1 \\\\\ns_2 & 1 \\\\\n\\vdots & \\vdots \\\\\ns_m & 1\n\\end{bmatrix},\n$$\nthe parameter vector\n$$\n\\theta = \n\\begin{bmatrix}\na \\\\ b\n\\end{bmatrix},\n$$\nand the response vector\n$$\ny = \n\\begin{bmatrix}\no_1 \\\\ o_2 \\\\ \\vdots \\\\ o_m\n\\end{bmatrix}.\n$$\nThe residual vector is $r = y - X \\theta$. The ordinary least squares estimate $\\hat{\\theta}$ minimizes the objective\n$$\nJ(\\theta) = \\| y - X \\theta \\|_2^2 = \\sum_{i=1}^{m} \\left(o_i - a s_i - b\\right)^2.\n$$\nA well-tested solution is given by the normal equations\n$$\nX^\\top X \\hat{\\theta} = X^\\top y,\n$$\nwhose solution is\n$$\n\\hat{\\theta} = (X^\\top X)^{-1} X^\\top y,\n$$\nprovided $X^\\top X$ is invertible. Numerically, to ensure stability, we compute $\\hat{\\theta}$ using the least squares solver, which finds the $\\hat{\\theta}$ minimizing $J(\\theta)$ in the Euclidean norm sense, even if $X^\\top X$ is ill-conditioned.\n\nOnce $\\hat{\\theta} = (\\hat{a}, \\hat{b})^\\top$ is obtained, predictions on any test sample with simulated yield $s$ are computed as $\\hat{o} = \\hat{a} s + \\hat{b}$. The error for a test sample is\n$$\ne = \\hat{o} - o.\n$$\nTo assess performance over a pooled test set of $n$ samples, we compute the following metrics from first principles:\n- Root Mean Squared Error,\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} e_i^2},\n$$\nwhich has units of t/ha because it is derived from squared differences in t/ha and then square-rooted.\n- Mean Absolute Error,\n$$\n\\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |e_i|,\n$$\nwhich is also in t/ha.\n- Bias,\n$$\n\\mathrm{Bias} = \\frac{1}{n}\\sum_{i=1}^{n} e_i,\n$$\nin t/ha.\n- Nash–Sutcliffe efficiency,\n$$\n\\mathrm{NSE} = 1 - \\frac{\\sum_{i=1}^{n} e_i^2}{\\sum_{i=1}^{n} (o_i - \\bar{o})^2},\n$$\nwith $\\bar{o} = \\frac{1}{n}\\sum_{i=1}^{n} o_i$. This metric is dimensionless because it is a ratio of sums of squared quantities in the same units.\n\nFor cross-validation, we use a deterministic partition rule to ensure reproducibility across years and sites. Let the samples be sorted by site identifier (ascending) and then by year (ascending). Assign each sample a fold index\n$$\nf_i = i \\bmod k,\n$$\nwhere $i$ is the $0$-based index in the sorted order and $k$ is the number of folds. For each fold $j \\in \\{0,1,\\dots,k-1\\}$, define the training set $\\mathcal{T}_j = \\{ i \\mid f_i \\neq j \\}$ and the test set $\\mathcal{S}_j = \\{ i \\mid f_i = j \\}$. On $\\mathcal{T}_j$, fit $\\hat{a}_j$ and $\\hat{b}_j$ by ordinary least squares. Then, generate predictions on $\\mathcal{S}_j$ as $\\hat{o}_i = \\hat{a}_j s_i + \\hat{b}_j$. Pooled test predictions across all folds are collected into a single set $\\{(\\hat{o}_i, o_i)\\}_{i=1}^{n}$ for metric computation, where $n$ is the total number of samples in the dataset.\n\nAlgorithmic design for each test case:\n1. Load the dataset and sort samples by site and year to establish a deterministic index $i$.\n2. Compute fold indices $f_i = i \\bmod k$ for the specified $k$.\n3. For each fold $j$, assemble training matrix $X$ and vector $y$ from $\\mathcal{T}_j$, solve for $\\hat{\\theta}_j$ via least squares, and compute test predictions on $\\mathcal{S}_j$.\n4. Concatenate all test predictions to obtain arrays $(\\hat{o}_i)$ and $(o_i)$ for the pooled test set.\n5. Compute the requested metric using its fundamental definition as above.\n6. Format the result with exactly six digits after the decimal point.\n\nFor the test suite:\n- Case $1$ uses Dataset A, $k=3$, and $\\mathrm{NSE}$. The Nash–Sutcliffe efficiency assesses how well the bias-corrected predictions reproduce the variability of observed yields relative to the mean of observed yields.\n- Case $2$ uses Dataset A, $k=4$, and $\\mathrm{RMSE}$ in t/ha. This evaluates the typical magnitude of prediction errors after bias correction.\n- Case $3$ uses Dataset B, $k=5$, and $\\mathrm{Bias}$ in t/ha. With constant observed yields across years, the bias quantifies the mean signed deviation of predictions from observations in t/ha and is robust under zero variance in $o_i$.\n\nFinally, the program outputs the three metric values in the order of Case $1$, Case $2$, Case $3$ as a single line in the format `[v1,v2,v3]`, where $v1$, $v2$, and $v3$ are decimals rounded to six digits after the decimal point.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assign_folds(sorted_samples, k):\n    \"\"\"\n    Assign fold indices deterministically using i % k, where i is the index\n    after sorting by (site, year).\n    \"\"\"\n    folds = []\n    for i, _ in enumerate(sorted_samples):\n        folds.append(i % k)\n    return np.array(folds, dtype=int)\n\ndef fit_linear_least_squares(s_train, o_train):\n    \"\"\"\n    Fit a linear model o_hat = a*s + b using ordinary least squares.\n    Returns (a, b).\n    \"\"\"\n    # Design matrix with columns [s, 1]\n    X = np.column_stack([s_train, np.ones_like(s_train)])\n    # Solve least squares: minimize ||X*[a,b] - o||\n    theta, _, _, _ = np.linalg.lstsq(X, o_train, rcond=None)\n    a, b = theta[0], theta[1]\n    return a, b\n\ndef predict_linear(a, b, s):\n    \"\"\"\n    Predict observed yield from simulated yield using o_hat = a*s + b.\n    \"\"\"\n    return a * s + b\n\ndef metric_rmse(o_hat, o_true):\n    \"\"\"\n    Root Mean Squared Error (t/ha).\n    \"\"\"\n    e = o_hat - o_true\n    return float(np.sqrt(np.mean(e**2)))\n\ndef metric_bias(o_hat, o_true):\n    \"\"\"\n    Bias (t/ha).\n    \"\"\"\n    e = o_hat - o_true\n    return float(np.mean(e))\n\ndef metric_nse(o_hat, o_true):\n    \"\"\"\n    Nash–Sutcliffe efficiency (dimensionless).\n    \"\"\"\n    e = o_hat - o_true\n    denom = np.sum((o_true - np.mean(o_true))**2)\n    num = np.sum(e**2)\n    # If denom is zero (no variability), NSE is undefined; set to 1 - num/0 -> -inf.\n    # For robustness, we return -inf in that case.\n    if denom == 0.0:\n        return float(\"-inf\")\n    return float(1.0 - num / denom)\n\ndef cross_validate(samples, k, metric_name):\n    \"\"\"\n    Perform deterministic k-fold cross-validation on given samples.\n    samples: list of tuples (site, year, s, o)\n    k: number of folds\n    metric_name: one of 'NSE', 'RMSE', 'Bias'\n    Returns computed metric on pooled test predictions.\n    \"\"\"\n    # Sort samples by (site, year)\n    sorted_samples = sorted(samples, key=lambda x: (x[0], x[1]))\n    folds = assign_folds(sorted_samples, k)\n    # Collect pooled predictions and observations\n    o_hat_pooled = []\n    o_true_pooled = []\n\n    for j in range(k):\n        # Indices for training and test\n        test_idx = np.where(folds == j)[0]\n        train_idx = np.where(folds != j)[0]\n\n        # Extract training data\n        s_train = np.array([sorted_samples[idx][2] for idx in train_idx], dtype=float)\n        o_train = np.array([sorted_samples[idx][3] for idx in train_idx], dtype=float)\n\n        # Fit linear model\n        a, b = fit_linear_least_squares(s_train, o_train)\n\n        # Predict on test data\n        s_test = np.array([sorted_samples[idx][2] for idx in test_idx], dtype=float)\n        o_test = np.array([sorted_samples[idx][3] for idx in test_idx], dtype=float)\n\n        o_hat = predict_linear(a, b, s_test)\n\n        o_hat_pooled.append(o_hat)\n        o_true_pooled.append(o_test)\n\n    # Concatenate pooled arrays\n    o_hat_pooled = np.concatenate(o_hat_pooled) if len(o_hat_pooled) > 0 else np.array([], dtype=float)\n    o_true_pooled = np.concatenate(o_true_pooled) if len(o_true_pooled) > 0 else np.array([], dtype=float)\n\n    # Compute requested metric\n    if metric_name == 'NSE':\n        return metric_nse(o_hat_pooled, o_true_pooled)\n    elif metric_name == 'RMSE':\n        return metric_rmse(o_hat_pooled, o_true_pooled)\n    elif metric_name == 'Bias':\n        return metric_bias(o_hat_pooled, o_true_pooled)\n    else:\n        raise ValueError(\"Unsupported metric_name\")\n\ndef solve():\n    # Define Dataset A (multi-site, multi-year): (site, year, s, o) with yields in t/ha.\n    dataset_a = [\n        (0, 2015, 4.9, 5.2), (0, 2016, 5.7, 5.6), (0, 2017, 6.0, 6.1), (0, 2018, 5.5, 5.8),\n        (1, 2015, 6.5, 7.1), (1, 2016, 7.0, 6.8), (1, 2017, 7.3, 7.4), (1, 2018, 7.2, 7.0),\n        (2, 2015, 3.5, 3.8), (2, 2016, 4.0, 4.2), (2, 2017, 4.7, 4.5), (2, 2018, 3.9, 4.0),\n    ]\n\n    # Define Dataset B (single-site, multi-year): (site, year, s, o) with yields in t/ha.\n    dataset_b = [\n        (3, 2015, 3.5, 4.0), (3, 2016, 4.2, 4.0), (3, 2017, 3.8, 4.0), (3, 2018, 4.5, 4.0), (3, 2019, 4.0, 4.0),\n    ]\n\n    # Test cases: (dataset, k, metric_name)\n    test_cases = [\n        (dataset_a, 3, 'NSE'),   # Case 1\n        (dataset_a, 4, 'RMSE'),  # Case 2\n        (dataset_b, 5, 'Bias'),  # Case 3\n    ]\n\n    results = []\n    for dataset, k, metric_name in test_cases:\n        value = cross_validate(dataset, k, metric_name)\n        # Format to six decimal places; handle infinities explicitly\n        if np.isneginf(value):\n            formatted = \"-inf\"\n        elif np.isposinf(value):\n            formatted = \"inf\"\n        elif np.isnan(value):\n            formatted = \"nan\"\n        else:\n            formatted = f\"{value:.6f}\"\n        results.append(formatted)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}