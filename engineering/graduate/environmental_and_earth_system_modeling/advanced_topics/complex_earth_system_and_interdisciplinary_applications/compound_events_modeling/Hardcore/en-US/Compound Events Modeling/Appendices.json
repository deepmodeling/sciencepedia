{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex models, we must master the fundamental language of probability. This first exercise reinforces the inclusion-exclusion principle, a cornerstone for calculating the probability of compound events. By working through this problem, you will solidify your understanding of the relationship between the probabilities of individual events, their union, and their intersection, and learn to diagnose statistical dependence—the very essence of a compound event .",
            "id": "3868819",
            "problem": "In a coastal compound-hazard assessment for a monsoon season, let $P$ denote the daily total precipitation (in millimeters) and let $S$ denote the daily storm surge anomaly (in meters) relative to mean sea level. Define the two events $A=\\{P50\\}$ and $B=\\{S1.0\\}$. Suppose that, for a particular location and stationary seasonal climatology, you are provided with the following probabilities computed from a long homogeneous record: $P(A)=0.12$, $P(B)=0.04$, and $P(A\\cup B)=0.145$. Starting only from the axioms of probability and basic set identities, derive an expression for $P(A\\cap B)$ in terms of $P(A)$, $P(B)$, and $P(A\\cup B)$, and then evaluate it numerically for the given values. Further, discuss concisely why the case of statistical independence corresponds to $P(A\\cap B)=P(A)P(B)$ and how this condition constrains $P(A\\cup B)$ in that special case. Express the numerical value of $P(A\\cap B)$ as a decimal and round your answer to four significant figures.",
            "solution": "The problem statement is subjected to validation before proceeding with a solution.\n\n**Step 1: Extract Givens**\n- Let $P$ be the daily total precipitation in millimeters.\n- Let $S$ be the daily storm surge anomaly in meters relative to mean sea level.\n- Event $A = \\{P  50\\}$.\n- Event $B = \\{S  1.0\\}$.\n- $P(A) = 0.12$.\n- $P(B) = 0.04$.\n- $P(A \\cup B) = 0.145$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific or Factual Soundness**: The problem is grounded in the valid scientific context of compound-event analysis in hydrology and coastal engineering. The variables $P$ and $S$ represent physically meaningful quantities. The specified thresholds for events $A$ and $B$, $50$ mm/day precipitation and $1.0$ m storm surge, are realistic values for significant meteorological and oceanographic events. The provided probabilities are all valid, i.e., they lie within the interval $[0, 1]$. Basic consistency checks hold: $P(A \\cup B) = 0.145 \\geq \\max(P(A), P(B)) = \\max(0.12, 0.04) = 0.12$, and $P(A \\cup B) = 0.145 \\leq P(A) + P(B) = 0.12 + 0.04 = 0.16$. The premises are scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary data to derive and compute the required probability. The requested derivation and discussion are based on standard, unambiguous principles of probability theory. A unique solution exists.\n- **Objective**: The problem is stated using clear, precise, and objective language, free of any subjective or non-formalizable claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\nThe foundation of this problem rests on the axioms of probability and elementary set theory. We are asked to find the probability of the intersection of two events, $A$ and $B$, denoted as $P(A \\cap B)$. This represents the compound event where both high precipitation and high storm surge occur on the same day.\n\nThe third axiom of probability, for a finite number of mutually exclusive events, states that the probability of their union is the sum of their individual probabilities. While events $A$ and $B$ are not necessarily mutually exclusive, we can use this principle to derive the general addition rule for probabilities. The set $A \\cup B$ can be partitioned into three disjoint sets: $A \\cap B^c$, $B \\cap A^c$, and $A \\cap B$, where $A^c$ and $B^c$ are the complements of $A$ and $B$, respectively.\nTherefore, the probability of the union is:\n$$P(A \\cup B) = P(A \\cap B^c) + P(B \\cap A^c) + P(A \\cap B)$$\nWe also know that $P(A) = P(A \\cap B) + P(A \\cap B^c)$ and $P(B) = P(A \\cap B) + P(B \\cap A^c)$.\nSumming these two gives:\n$$P(A) + P(B) = P(A \\cap B^c) + P(B \\cap A^c) + 2 P(A \\cap B)$$\nSubstituting the expression for $P(A \\cup B)$ into this equation yields:\n$$P(A) + P(B) = P(A \\cup B) + P(A \\cap B)$$\nThis identity is the inclusion-exclusion principle for two events. It is the fundamental relationship connecting the probabilities of the union, the intersection, and the individual events.\n\nThe problem asks for an expression for $P(A \\cap B)$ in terms of $P(A)$, $P(B)$, and $P(A \\cup B)$. By rearranging the inclusion-exclusion principle, we directly obtain the desired expression:\n$$P(A \\cap B) = P(A) + P(B) - P(A \\cup B)$$\nThis is the formal derivation requested.\n\nNow, we can evaluate this expression numerically using the given values:\n$P(A) = 0.12$\n$P(B) = 0.04$\n$P(A \\cup B) = 0.145$\nSubstituting these values into the derived formula:\n$$P(A \\cap B) = 0.12 + 0.04 - 0.145$$\n$$P(A \\cap B) = 0.16 - 0.145$$\n$$P(A \\cap B) = 0.015$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.015$, which has two significant figures. To express this with four significant figures, we add trailing zeros: $0.01500$.\n\nNext, we address the discussion of statistical independence. Two events $A$ and $B$ are defined as statistically independent if and only if the probability of their joint occurrence is the product of their individual probabilities. Mathematically, this condition is:\n$$P(A \\cap B) = P(A) P(B)$$\nThis definition implies that the occurrence of one event provides no information about the likelihood of the other event occurring.\nIf independence holds, this condition provides a specific value for $P(A \\cap B)$, which in turn constrains the value of $P(A \\cup B)$ through the inclusion-exclusion principle. Specifically, if $A$ and $B$ are independent, we can substitute $P(A)P(B)$ for $P(A \\cap B)$ in the general addition rule:\n$$P(A \\cup B) = P(A) + P(B) - P(A)P(B)$$\nThis is the expression for the probability of the union of two independent events.\n\nFor the given data, let's check if the independence assumption holds. The probability of the intersection under the assumption of independence would be:\n$$P(A)P(B) = (0.12)(0.04) = 0.0048$$\nOur calculated value for the actual probability of intersection is $P(A \\cap B) = 0.015$. Since $0.015 \\neq 0.0048$, the events $A$ and $B$ are not statistically independent. In fact, because $P(A \\cap B)  P(A)P(B)$, the events are positively correlated. This is physically intuitive; the large-scale weather systems that cause extreme precipitation (event $A$) often involve strong winds that drive high storm surges (event $B$), so their occurrences are linked.",
            "answer": "$$\\boxed{0.01500}$$"
        },
        {
            "introduction": "Environmental data rarely consist of independent events; instead, phenomena like storms and heatwaves exhibit temporal clustering. This exercise introduces the crucial concept of the extremal index, $\\theta$, which quantifies this clustering in a time series of extreme events. You will learn the 'runs method' for declustering, a practical technique to isolate statistically independent compound events from a raw, dependent sequence of observations, which is an essential prerequisite for accurate risk assessment .",
            "id": "3868877",
            "problem": "Consider a daily bivariate stationary time series $\\{(P_t,S_t)\\}_{t=1}^T$ where $P_t$ is precipitation (in millimeters) and $S_t$ is storm surge (in meters). A compound event is defined as a joint threshold exceedance, namely the event that $P_t  u_P$ and $S_t  u_S$, where $u_P$ and $u_S$ are high quantile thresholds chosen so that $P(P_t  u_P) \\approx 0.05$ and $P(S_t  u_S) \\approx 0.05$. Let $Z_t = \\mathbf{1}\\{P_t  u_P,\\, S_t  u_S\\}$ denote the indicator of joint exceedance, and suppose the time series is $\\alpha$-mixing with short-range dependence so that a Poisson cluster process limit for exceedances is appropriate under high thresholds.\n\nExplain declustering of the joint exceedance time series $\\{Z_t\\}$ using the runs method with gap $r$: articulate from first principles how clusters are defined and terminated with $r$ consecutive non-exceedances, and derive an estimator for the extremal index $\\theta$ in terms of observed cluster counts and exceedances, including the theoretical justification linking $\\theta$ to the mean cluster size. Then, justify the choice of the gap $r$ by combining the stabilization of the extremal index estimator with a short-range tail dependence assessment based on the conditional probability $P(Z_{t+\\ell}=1 \\mid Z_t=1)$ decaying to the unconditional exceedance probability $p = P(Z_t=1)$ at lag $\\ell \\ge r$.\n\nYou are given a data set of length $T = 20000$ days for which $N = 240$ joint exceedances were observed over the study period. Using the runs declustering applied to $\\{Z_t\\}$ for several candidate gaps $r$, the number of clusters $J(r)$ was obtained as:\n$J(1) = 110$, $J(3) = 92$, $J(5) = 85$, $J(7) = 85$.\nAdditionally, the empirical conditional exceedance probabilities $\\hat{\\rho}(\\ell) = \\widehat{P}(Z_{t+\\ell}=1 \\mid Z_t=1)$ and the unconditional probability $\\hat{p} = \\widehat{P}(Z_t=1)$ were computed as:\n$\\hat{\\rho}(1) = 0.18$, $\\hat{\\rho}(2) = 0.09$, $\\hat{\\rho}(3) = 0.04$, $\\hat{\\rho}(4) = 0.02$, $\\hat{\\rho}(5) = 0.013$, $\\hat{\\rho}(6) = 0.0125$, $\\hat{\\rho}(7) = 0.012$, $\\hat{p} = 0.012$.\nChoose the option that correctly describes the declustering setup for the compound event, derives a principled extremal index estimator from first principles, and justifies an appropriate choice of $r$ for this data, including a numerical estimate of $\\theta$ consistent with the provided information.\n\nA. Define $Z_t = \\mathbf{1}\\{P_t  u_P,\\, S_t  u_S\\}$ and use runs declustering with gap $r$ so that a cluster terminates only after $r$ consecutive days with $Z_t = 0$. Under a Poisson cluster process limit for exceedances, the extremal index $\\theta$ equals the reciprocal of the limiting mean cluster size, motivating the estimator $\\hat{\\theta}(r) = J(r)/N$ from the ratio of the number of clusters to the number of exceedances. Choose $r$ as the smallest lag for which both $\\hat{\\theta}(r)$ stabilizes across larger $r$ and $\\hat{\\rho}(\\ell)$ is indistinguishable from $\\hat{p}$ for $\\ell \\ge r$. For the given data, $J(5) = 85$ and $J(7) = 85$ yield the same $\\hat{\\theta}$, and $\\hat{\\rho}(\\ell)$ has essentially reached $\\hat{p}$ by $\\ell = 5$, so $r = 5$ is appropriate with $\\hat{\\theta}(5) = 85/240 \\approx 0.354$.\n\nB. Declustering can be performed on $P_t$ alone because precipitation is the driver of compound events, and the gap $r$ should be chosen to maximize the extremal index estimator. Using the given counts, $r = 7$ maximizes $\\hat{\\theta}(r)$, so $\\hat{\\theta}(7) = 85/240 \\approx 0.354$ is appropriate.\n\nC. Clusters should be defined as contiguous runs of joint exceedances with no zeros allowed inside, corresponding to $r = 0$. This choice ensures independence across clusters and implies $\\theta = 1$ for high thresholds, so declustering is unnecessary.\n\nD. Define the compound event indicator as $Z_t = \\mathbf{1}\\{P_t  u_P \\text{ or } S_t  u_S\\}$ so that either component exceeding is treated as a compound event. Choose $r$ by the largest drop in $\\hat{\\rho}(\\ell)$, which occurs by $\\ell = 3$, and estimate $\\theta$ as $\\hat{\\theta}(3) = 92/240 \\approx 0.383$ because fewer dependencies remain at this lag.\n\nSelect the single best option.",
            "solution": "The user wants me to solve a problem related to the declustering of compound extreme events and the estimation of the extremal index.\n\n### Step 1: Extract Givens\n\n-   **Time Series**: A daily bivariate stationary time series $\\{(P_t, S_t)\\}_{t=1}^T$, where $P_t$ is precipitation and $S_t$ is storm surge.\n-   **Compound Event Definition**: A joint threshold exceedance, $P_t  u_P$ and $S_t  u_S$.\n-   **Thresholds**: $u_P$ and $u_S$ are high quantile thresholds such that the marginal exceedance probabilities are $P(P_t  u_P) \\approx 0.05$ and $P(S_t  u_S) \\approx 0.05$.\n-   **Indicator Variable**: $Z_t = \\mathbf{1}\\{P_t  u_P,\\, S_t  u_S\\}$.\n-   **Dependence Structure**: The time series $\\{Z_t\\}$ is $\\alpha$-mixing with short-range dependence.\n-   **Statistical Model**: A Poisson cluster process limit for exceedances is appropriate.\n-   **Declustering Method**: Runs method with gap $r$, where clusters are terminated by $r$ consecutive non-exceedances.\n-   **Parameter to Estimate**: The extremal index, $\\theta$.\n-   **Data**:\n    -   Time series length: $T = 20000$ days.\n    -   Total number of joint exceedances: $N = 240$.\n    -   Number of clusters $J(r)$ for different gaps $r$: $J(1) = 110$, $J(3) = 92$, $J(5) = 85$, $J(7) = 85$.\n    -   Empirical conditional exceedance probabilities $\\hat{\\rho}(\\ell) = \\widehat{P}(Z_{t+\\ell}=1 \\mid Z_t=1)$ for lag $\\ell$: $\\hat{\\rho}(1) = 0.18$, $\\hat{\\rho}(2) = 0.09$, $\\hat{\\rho}(3) = 0.04$, $\\hat{\\rho}(4) = 0.02$, $\\hat{\\rho}(5) = 0.013$, $\\hat{\\rho}(6) = 0.0125$, $\\hat{\\rho}(7) = 0.012$.\n    -   Empirical unconditional exceedance probability: $\\hat{p} = \\widehat{P}(Z_t=1) = 0.012$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is well-grounded in the statistical theory of extreme values for dependent time series. The concepts of compound events, joint exceedances, declustering, the runs method, the extremal index, and Poisson cluster processes are standard in climatology, hydrology, and financial econometrics. The application to precipitation and storm surge is a canonical example of a compound environmental hazard. The premise is scientifically sound.\n-   **Well-Posed**: The problem is well-posed. It asks for an explanation of a methodology and its application to a given dataset. The data provided is sufficient to apply the methodology and evaluate the options. A unique, meaningful solution can be derived.\n-   **Objective**: The language is precise and objective. All terms are either standard in the field or explicitly defined. The quantities are numerical. There are no subjective or opinion-based statements.\n-   **Completeness**: The problem statement is self-contained and complete. It provides all necessary definitions and data to derive the answer. The data is internally consistent (e.g., $\\hat{p} = N/T = 240/20000 = 0.012$, which matches the provided value).\n-   **No other flaws**: The problem does not violate any of the other criteria for invalidity. It is not trivial, unrealistic, or non-formalizable.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with deriving the solution and evaluating the options.\n\n### Solution Derivation\n\nThe problem requires an explanation of the runs declustering method, the derivation of an estimator for the extremal index $\\theta$, and the justification for choosing the gap parameter $r$, followed by an application to the given data.\n\n**1. Declustering and the Extremal Index $\\theta$**\n\nIn a stationary time series with short-range dependence, extreme events (exceedances of a high threshold) tend to occur in clusters. The extremal index, $\\theta \\in (0, 1]$, quantifies this clustering. A value of $\\theta=1$ corresponds to the case of independent exceedances (like in an i.i.d. sequence), while $\\theta  1$ indicates clustering. The extremal index can be interpreted as the reciprocal of the limiting mean size of clusters of exceedances. That is,\n$$ \\theta = \\frac{1}{\\text{mean cluster size}} $$\nThe sequence of clusters of exceedances can be treated as a Poisson process with a rate of occurrence that is $\\theta$ times the rate of exceedances in a corresponding independent series.\n\n**2. Runs Declustering and the Estimator for $\\theta$**\n\nThe \"runs method\" is a common technique for identifying clusters. Let $Z_t = \\mathbf{1}\\{P_t  u_P, S_t  u_S\\}$ be the indicator of a joint exceedance on day $t$. A cluster of exceedances is defined as a sequence of observations that begins with an exceedance ($Z_t=1$) and is terminated by a \"gap\" of $r$ consecutive non-exceedances (i.e., $Z_{t+1}=0, Z_{t+2}=0, \\dots, Z_{t+r}=0$). All exceedances occurring between two such gaps are considered part of the same cluster. The parameter $r$ is the declustering gap.\n\nBased on the theoretical relationship between $\\theta$ and the mean cluster size, a natural estimator for $\\theta$ using the runs method is the reciprocal of the empirical mean cluster size. Given $N$ total exceedances and $J(r)$ clusters identified with a gap of size $r$, the empirical mean cluster size is $\\frac{N}{J(r)}$. Therefore, the estimator for the extremal index is:\n$$ \\hat{\\theta}(r) = \\frac{1}{N/J(r)} = \\frac{J(r)}{N} $$\nThis estimator gives the ratio of the number of clusters (independent extreme events) to the total number of individual exceedances.\n\n**3. Choosing the Gap Parameter $r$**\n\nThe choice of $r$ is a crucial step involving a bias-variance trade-off.\n-   If $r$ is too small, a single physical cluster might be split into multiple smaller ones. This artificially inflates the number of clusters $J(r)$, leading to a positively biased estimate of $\\theta$.\n-   If $r$ is too large, physically distinct and independent clusters might be merged into one. This artificially deflates the number of clusters $J(r)$, leading to a negatively biased estimate of $\\theta$.\n\nA principled approach to choosing $r$ relies on two complementary diagnostics:\n1.  **Stabilization of $\\hat{\\theta}(r)$**: We compute $\\hat{\\theta}(r)$ for a range of candidate values of $r$. We look for a value of $r$ after which the estimate $\\hat{\\theta}(r)$ becomes stable (i.e., does not change significantly for larger $r$). This region of stability suggests that $r$ is large enough to separate most of the independent clusters.\n2.  **Decay of Serial Dependence**: The purpose of declustering is to render the resulting cluster events approximately independent. This implies that the gap $r$ should be large enough to break the short-range temporal dependence in the exceedance series $\\{Z_t\\}$. We can assess this by examining the conditional probability $P(Z_{t+\\ell}=1 \\mid Z_t=1)$. This probability should decay to the unconditional probability $p = P(Z_t=1)$ for lags $\\ell$ greater than or equal to the chosen gap $r$. That is, we choose $r$ such that for all $\\ell \\ge r$, $\\hat{\\rho}(\\ell) \\approx \\hat{p}$.\n\n**4. Application to the Given Data**\n\nWe apply these principles to the provided data.\n-   $N=240$\n-   $J(1)=110, J(3)=92, J(5)=85, J(7)=85$\n-   $\\hat{p}=0.012$\n-   $\\hat{\\rho}(1)=0.18, \\hat{\\rho}(2)=0.09, \\hat{\\rho}(3)=0.04, \\hat{\\rho}(4)=0.02, \\hat{\\rho}(5)=0.013, \\hat{\\rho}(6)=0.0125, \\hat{\\rho}(7)=0.012$\n\nFirst, let's compute the extremal index estimates:\n-   $\\hat{\\theta}(1) = \\frac{J(1)}{N} = \\frac{110}{240} \\approx 0.458$\n-   $\\hat{\\theta}(3) = \\frac{J(3)}{N} = \\frac{92}{240} \\approx 0.383$\n-   $\\hat{\\theta}(5) = \\frac{J(5)}{N} = \\frac{85}{240} \\approx 0.354$\n-   $\\hat{\\theta}(7) = \\frac{J(7)}{N} = \\frac{85}{240} \\approx 0.354$\n\nNow, we check for stabilization. The estimates are $0.458, 0.383, 0.354, 0.354$. The estimator $\\hat{\\theta}(r)$ stabilizes at $r=5$, since $\\hat{\\theta}(5) = \\hat{\\theta}(7)$. This suggests a choice of $r \\ge 5$.\n\nNext, we check for the decay of serial dependence. We compare $\\hat{\\rho}(\\ell)$ to $\\hat{p} = 0.012$:\n-   $\\hat{\\rho}(1) = 0.18 \\gg 0.012$\n-   $\\hat{\\rho}(2) = 0.09 \\gg 0.012$\n-   $\\hat{\\rho}(3) = 0.04  0.012$\n-   $\\hat{\\rho}(4) = 0.02  0.012$\n-   $\\hat{\\rho}(5) = 0.013 \\approx 0.012$\n-   $\\hat{\\rho}(6) = 0.0125 \\approx 0.012$\n-   $\\hat{\\rho}(7) = 0.012 = 0.012$\n\nThe conditional probability $\\hat{\\rho}(\\ell)$ has essentially decayed to the unconditional probability $\\hat{p}$ at lag $\\ell=5$. For lags $\\ell \\ge 5$, the two probabilities are practically indistinguishable. This suggests that a gap of $r=5$ days is sufficient to ensure approximate independence between clusters.\n\nCombining both criteria, the choice of $r=5$ is well-justified. It is the smallest gap size for which both the estimate $\\hat{\\theta}(r)$ stabilizes and the temporal dependence becomes negligible. With $r=5$, the corresponding estimate for the extremal index is $\\hat{\\theta}(5) = 85/240 \\approx 0.354$.\n\n### Option-by-Option Analysis\n\n**A. Define $Z_t = \\mathbf{1}\\{P_t  u_P,\\, S_t  u_S\\}$ and use runs declustering with gap $r$ so that a cluster terminates only after $r$ consecutive days with $Z_t = 0$. Under a Poisson cluster process limit for exceedances, the extremal index $\\theta$ equals the reciprocal of the limiting mean cluster size, motivating the estimator $\\hat{\\theta}(r) = J(r)/N$ from the ratio of the number of clusters to the number of exceedances. Choose $r$ as the smallest lag for which both $\\hat{\\theta}(r)$ stabilizes across larger $r$ and $\\hat{\\rho}(\\ell)$ is indistinguishable from $\\hat{p}$ for $\\ell \\ge r$. For the given data, $J(5) = 85$ and $J(7) = 85$ yield the same $\\hat{\\theta}$, and $\\hat{\\rho}(\\ell)$ has essentially reached $\\hat{p}$ by $\\ell = 5$, so $r = 5$ is appropriate with $\\hat{\\theta}(5) = 85/240 \\approx 0.354$.**\n\nThis-option correctly states the definition of the joint exceedance indicator $Z_t$.\n-   It correctly describes the runs declustering method.\n-   It correctly links the extremal index $\\theta$ to the mean cluster size and presents the correct estimator $\\hat{\\theta}(r) = J(r)/N$.\n-   It correctly states the dual criteria for choosing $r$: stabilization of the estimator and decay of serial correlation.\n-   It correctly applies these criteria to the data, noting the stabilization of $\\hat{\\theta}(r)$ at $r=5$ (since $\\hat{\\theta}(5) = \\hat{\\theta}(7)$) and the decay of $\\hat{\\rho}(\\ell)$ to $\\hat{p}$ at $\\ell=5$.\n-   The choice of $r=5$ and the resulting calculation $\\hat{\\theta}(5) = 85/240 \\approx 0.354$ are both correct.\n**Verdict: Correct.**\n\n**B. Declustering can be performed on $P_t$ alone because precipitation is the driver of compound events, and the gap $r$ should be chosen to maximize the extremal index estimator. Using the given counts, $r = 7$ maximizes $\\hat{\\theta}(r)$, so $\\hat{\\theta}(7) = 85/240 \\approx 0.354$ is appropriate.**\n\n-   The first statement, \"Declustering can be performed on $P_t$ alone...\", is incorrect. The problem is explicitly about the **compound event** $Z_t$, which depends on the joint behavior of $P_t$ and $S_t$. The clustering properties of $Z_t$ may be very different from those of $P_t$ alone.\n-   The second statement, \"...the gap $r$ should be chosen to maximize the extremal index estimator,\" is methodologically incorrect. The goal is to find a stable estimate, not to maximize it. Maximizing $\\hat{\\theta}(r)$ would typically mean choosing a small $r$ (here $r=1$) and would lead to a biased estimate.\n-   The third statement, \"...$r = 7$ maximizes $\\hat{\\theta}(r)$...\", is factually incorrect based on the data. $\\hat{\\theta}(1) \\approx 0.458$ is the maximum, while $\\hat{\\theta}(7) \\approx 0.354$ is the minimum of the calculated values.\n**Verdict: Incorrect.**\n\n**C. Clusters should be defined as contiguous runs of joint exceedances with no zeros allowed inside, corresponding to $r = 0$. This choice ensures independence across clusters and implies $\\theta = 1$ for high thresholds, so declustering is unnecessary.**\n\n-   The definition of clusters as \"contiguous runs of joint exceedances\" with no zeros allowed corresponds to the runs method with $r=1$, not $r=0$. A gap of one zero is sufficient to terminate the cluster. The term $r=0$ is not standard and conceptually ill-defined in this context.\n-   The claim that this choice \"ensures independence\" is false. Two exceedances separated by a short gap of non-exceedances (e.g., sequence $1,0,1$) can be strongly dependent, but this method would split them.\n-   The claim that this implies $\\theta=1$ is a severe theoretical error. $\\theta=1$ signifies no clustering (i.e., independent exceedances), which would make declustering unnecessary. However, the data clearly show strong clustering: $\\hat{\\rho}(1) = 0.18$ is much larger than $\\hat{p} = 0.012$, and all estimates of $\\theta$ are substantially less than $1$.\n**Verdict: Incorrect.**\n\n**D. Define the compound event indicator as $Z_t = \\mathbf{1}\\{P_t  u_P \\text{ or } S_t  u_S\\}$ so that either component exceeding is treated as a compound event. Choose $r$ by the largest drop in $\\hat{\\rho}(\\ell)$, which occurs by $\\ell = 3$, and estimate $\\theta$ as $\\hat{\\theta}(3) = 92/240 \\approx 0.383$ because fewer dependencies remain at this lag.**\n\n-   The definition of the compound event as an \"or\" condition contradicts the problem statement, which explicitly uses an \"and\" condition ($P_t  u_P$ and $S_t  u_S$). This changes the fundamental nature of the event being analyzed.\n-   The heuristic to \"choose $r$ by the largest drop in $\\hat{\\rho}(\\ell)$\" is not a standard or robust method. The preferred method is to wait for $\\hat{\\rho}(\\ell)$ to decay to the baseline unconditional probability $\\hat{p}$.\n-   At lag $\\ell=3$, $\\hat{\\rho}(3) = 0.04$, which is still over three times the unconditional probability $\\hat{p}=0.012$. Significant dependence remains, so the justification \"fewer dependencies remain at this lag\" is weak and insufficient. The choice of $r=3$ is not well-supported and would likely lead to a biased (high) estimate of $\\theta$.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "This final practice is a capstone exercise that integrates the concepts of non-stationarity, marginal distributions, and dependence into a unified, state-of-the-art modeling framework. You will develop a program to model compound events using covariate-driven Generalized Extreme Value (GEV) distributions and an extreme value copula. The goal is not only to predict risk but to attribute changes in that risk to its underlying drivers—changes in the individual extremes versus changes in their dependence structure—a powerful diagnostic for understanding the impacts of climate change .",
            "id": "3868831",
            "problem": "Consider two environmental variables $X_1$ and $X_2$ representing block maxima of distinct, physically relevant processes in an Earth system (for example, extreme precipitation and coastal surge). Assume both variables follow the Generalized Extreme Value (GEV) distribution conditionally on a climate covariate $Z$. The GEV marginal cumulative distribution function for variable $i \\in \\{1,2\\}$ at threshold $x$ and covariate value $Z=z$ is denoted $F_i(x \\mid Z=z)$ with parameters $\\mu_i(z)$ (location), $\\sigma_i(z)$ (scale), and $\\xi_i(z)$ (shape). The GEV distribution is defined as follows for any real $x$ with $\\sigma_i(z)0$:\n- When $\\xi_i(z) \\neq 0$, the support is given by $1+\\xi_i(z)\\,(x-\\mu_i(z))/\\sigma_i(z)  0$, and the cumulative distribution function is $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\left[1+\\xi_i(z)\\,\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right]^{-1/\\xi_i(z)}\\right)$.\n- When $\\xi_i(z) = 0$, the cumulative distribution function is $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\exp\\!\\left(-\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right)\\right)$.\n\nAssume further that the joint distribution of the maxima $(X_1,X_2)$ is max-stable and characterized by an extreme value copula driven by a logistic stable tail dependence function. Specifically, denote by $C(p_1,p_2 \\mid Z=z)$ the extreme value copula applied to marginal probabilities $p_1=F_1(x_1 \\mid Z=z)$ and $p_2=F_2(x_2 \\mid Z=z)$. The copula is determined by a stable tail dependence function $l(t_1,t_2 \\mid Z=z)$ where $t_i=-\\log(p_i)$, and by a dependence parameter $\\alpha(z)\\in(0,1]$. The logistic form of the stable tail dependence function is given by $l(t_1,t_2 \\mid Z=z) = \\left(t_1^{1/\\alpha(z)} + t_2^{1/\\alpha(z)}\\right)^{\\alpha(z)}$. The resulting extreme value copula is $C(p_1,p_2 \\mid Z=z) = \\exp\\!\\left(-\\,l(-\\log p_1,-\\log p_2 \\mid Z=z)\\right)$, which encodes the dependence structure between the maxima while preserving the GEV margins.\n\nDefine the compound exceedance probability at thresholds $(u_1,u_2)$ for covariate value $Z=z$ as the probability that both variables exceed their thresholds simultaneously, namely $\\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z)$. This probability must be derived using first principles, beginning from the definitions of the GEV marginal distributions, the extreme value copula induced by the logistic stable tail dependence function, and the inclusion-exclusion principle for joint probabilities. You must then attribute changes in the compound exceedance probability between two scenarios $Z=z_1$ and $Z=z_2$ to changes in margins versus changes in dependence using a symmetric, path-independent decomposition based on averaging contributions along two transformation paths: margins-then-dependence and dependence-then-margins.\n\nAdopt covariate-driven parameterizations for the GEV margins and the logistic dependence parameter. For each variable $i \\in \\{1,2\\}$, specify\n$\\mu_i(z) = \\mu_{i0} + \\mu_{i1}\\,z$, $\\sigma_i(z) = \\sigma_{i0}\\,\\exp(\\sigma_{i1}\\,z)$, and $\\xi_i(z) = \\xi_{i0} + \\xi_{i1}\\,z$, with $\\sigma_{i0}0$. For dependence, specify $\\alpha(z) = \\alpha_0 + \\alpha_1\\,z$ with $\\alpha(z)\\in(0,1]$ for the scenarios considered.\n\nYour task is to write a complete program that, for each test case listed below, does the following:\n- Computes the compound exceedance probability $\\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z_1)$ and $\\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z_2)$.\n- Computes the total change $\\Delta = \\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z_2) - \\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z_1)$.\n- Performs a symmetric attribution (averaged over two paths) of $\\Delta$ into a margins contribution and a dependence contribution, as defined by:\n  Path A (margins then dependence): the margins-only hybrid probability $\\mathbb{P}_{\\text{M}}$ uses $F_1(\\cdot \\mid Z=z_2)$ and $F_2(\\cdot \\mid Z=z_2)$ with $C(\\cdot,\\cdot \\mid Z=z_1)$, and the dependence contribution along Path A is $\\mathbb{P}(X_1u_1,X_2u_2 \\mid Z=z_2) - \\mathbb{P}_{\\text{M}}$ while the margins contribution along Path A is $\\mathbb{P}_{\\text{M}} - \\mathbb{P}(X_1u_1,X_2u_2 \\mid Z=z_1)$.\n  Path B (dependence then margins): the dependence-only hybrid probability $\\mathbb{P}_{\\text{D}}$ uses $F_1(\\cdot \\mid Z=z_1)$ and $F_2(\\cdot \\mid Z=z_1)$ with $C(\\cdot,\\cdot \\mid Z=z_2)$, and the margins contribution along Path B is $\\mathbb{P}(X_1u_1,X_2u_2 \\mid Z=z_2) - \\mathbb{P}_{\\text{D}}$ while the dependence contribution along Path B is $\\mathbb{P}_{\\text{D}} - \\mathbb{P}(X_1u_1,X_2u_2 \\mid Z=z_1)$. The final margins contribution is the average of the margins contributions along the two paths, and the final dependence contribution is the average of the dependence contributions along the two paths.\n\nYou must express all derived probabilities as real numbers (floats). There are no physical units involved in the answer.\n\nTest Suite:\n- Case $1$ (general case with moderate positive dependence and mixed-tail margins):\n  - Scenarios: $z_1=0$, $z_2=1$.\n  - Variable $1$: $\\mu_{10}=30$, $\\mu_{11}=2$, $\\sigma_{10}=10$, $\\sigma_{11}=0.1$, $\\xi_{10}=0.1$, $\\xi_{11}=0.0$, threshold $u_1=70$.\n  - Variable $2$: $\\mu_{20}=1.5$, $\\mu_{21}=0.2$, $\\sigma_{20}=0.5$, $\\sigma_{21}=0.05$, $\\xi_{20}=-0.1$, $\\xi_{21}=0.0$, threshold $u_2=5.5$.\n  - Dependence: $\\alpha_0=0.6$, $\\alpha_1=-0.1$.\n- Case $2$ (independence boundary with Gumbel margins and changing location/scale):\n  - Scenarios: $z_1=0$, $z_2=2$.\n  - Variable $1$: $\\mu_{10}=0$, $\\mu_{11}=0$, $\\sigma_{10}=1$, $\\sigma_{11}=0$, $\\xi_{10}=0$, $\\xi_{11}=0$, threshold $u_1=2$.\n  - Variable $2$: $\\mu_{20}=0$, $\\mu_{21}=1$, $\\sigma_{20}=1$, $\\sigma_{21}=0.2$, $\\xi_{20}=0$, $\\xi_{21}=0$, threshold $u_2=3$.\n  - Dependence: $\\alpha_0=1$, $\\alpha_1=0$.\n- Case $3$ (dependence-only change with fixed margins):\n  - Scenarios: $z_1=0$, $z_2=1$.\n  - Variable $1$: $\\mu_{10}=10$, $\\mu_{11}=0$, $\\sigma_{10}=2$, $\\sigma_{11}=0$, $\\xi_{10}=0.05$, $\\xi_{11}=0$, threshold $u_1=20$.\n  - Variable $2$: $\\mu_{20}=5$, $\\mu_{21}=0$, $\\sigma_{20}=1$, $\\sigma_{21}=0$, $\\xi_{20}=0$, $\\xi_{21}=0$, threshold $u_2=8$.\n  - Dependence: $\\alpha_0=0.9$, $\\alpha_1=-0.6$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of five floats in the order $[\\mathbb{P}(X_1u_1,X_2u_2 \\mid Z=z_1), \\mathbb{P}(X_1u_1,X_2u_2 \\mid Z=z_2), \\Delta, \\text{margins\\_contribution}, \\text{dependence\\_contribution}]$. For example, the final output should look like $[[p_{1,1},p_{1,2},\\Delta_1,m_1,d_1],[p_{2,1},p_{2,2},\\Delta_2,m_2,d_2],[p_{3,1},p_{3,2},\\Delta_3,m_3,d_3]]$.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Marginal Distributions**: Two environmental variables $X_1$ and $X_2$ follow the Generalized Extreme Value (GEV) distribution conditionally on a climate covariate $Z$.\n- **GEV CDF**: For variable $i \\in \\{1,2\\}$ at threshold $x$ and covariate value $Z=z$, the CDF is $F_i(x \\mid Z=z)$ with parameters $\\mu_i(z)$, $\\sigma_i(z)0$, and $\\xi_i(z)$.\n  - If $\\xi_i(z) \\neq 0$: $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\left[1+\\xi_i(z)\\,\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right]^{-1/\\xi_i(z)}\\right)$, with support $1+\\xi_i(z)\\,(x-\\mu_i(z))/\\sigma_i(z)  0$.\n  - If $\\xi_i(z) = 0$: $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\exp\\!\\left(-\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right)\\right)$.\n- **Dependence Structure**: The joint distribution of $(X_1, X_2)$ is modeled by an extreme value copula $C(p_1, p_2 \\mid Z=z)$ with a logistic stable tail dependence function $l(t_1,t_2 \\mid Z=z) = \\left(t_1^{1/\\alpha(z)} + t_2^{1/\\alpha(z)}\\right)^{\\alpha(z)}$, where $t_i = -\\log(p_i)$ and $p_i = F_i(x_i \\mid Z=z)$. The dependence parameter is $\\alpha(z) \\in (0,1]$.\n- **Copula Function**: $C(p_1,p_2 \\mid Z=z) = \\exp\\!\\left(-\\,l(-\\log p_1,-\\log p_2 \\mid Z=z)\\right)$.\n- **Target Quantity**: The compound exceedance probability $\\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z)$.\n- **Parameterization**:\n  - GEV: $\\mu_i(z) = \\mu_{i0} + \\mu_{i1}\\,z$, $\\sigma_i(z) = \\sigma_{i0}\\,\\exp(\\sigma_{i1}\\,z)$, $\\xi_i(z) = \\xi_{i0} + \\xi_{i1}\\,z$.\n  - Dependence: $\\alpha(z) = \\alpha_0 + \\alpha_1\\,z$.\n- **Attribution Task**: Decompose the total change $\\Delta = \\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z_2) - \\mathbb{P}(X_1u_1, X_2u_2 \\mid Z=z_1)$ into contributions from margins and dependence using a symmetric, path-independent averaging method.\n  - Path A (margins-then-dependence): Hybrid probability $\\mathbb{P}_{\\text{M}}$ uses margins from $z_2$ and dependence from $z_1$. Margins contribution is $\\mathbb{P}_{\\text{M}} - \\mathbb{P}(\\dots|z_1)$, dependence contribution is $\\mathbb{P}(\\dots|z_2) - \\mathbb{P}_{\\text{M}}$.\n  - Path B (dependence-then-margins): Hybrid probability $\\mathbb{P}_{\\text{D}}$ uses margins from $z_1$ and dependence from $z_2$. Dependence contribution is $\\mathbb{P}_{\\text{D}} - \\mathbb{P}(\\dots|z_1)$, margins contribution is $\\mathbb{P}(\\dots|z_2) - \\mathbb{P}_{\\text{D}}$.\n  - Final contributions are averages over paths A and B.\n- **Test Cases**: Three test cases with specific parameter values are provided.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The problem uses standard, well-established models from extreme value theory (GEV distribution) and copula theory (extreme value copulas, logistic model), which are widely applied in environmental and earth system modeling. The attribution method is a standard technique for sensitivity analysis. The problem does not violate any scientific or mathematical principles.\n- **Well-Posedness**: All necessary parameters, functional forms, and definitions are provided. The parameter values for the test cases are checked to ensure they fall within valid domains (e.g., $\\sigma  0$, $\\alpha \\in (0,1]$, and thresholds within the GEV support), which they do. The task is clearly defined and leads to a unique, meaningful solution.\n- **Objectivity**: The language is precise, formal, and free of subjective claims. The problem is a formal mathematical exercise based on given models and data.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\nThe core of the problem is to calculate the compound exceedance probability $\\mathbb{P}(X_1  u_1, X_2  u_2 \\mid Z=z)$ and attribute its change between two scenarios, $Z=z_1$ and $Z=z_2$, to changes in marginal distributions versus changes in the dependence structure.\n\nFirst, we derive the expression for the compound exceedance probability. Let $p_1 = F_1(u_1 \\mid Z=z)$ and $p_2 = F_2(u_2 \\mid Z=z)$ be the marginal cumulative probabilities at the given thresholds $u_1$ and $u_2$. The joint cumulative distribution function is given by the copula: $\\mathbb{P}(X_1 \\le u_1, X_2 \\le u_2 \\mid Z=z) = C(p_1, p_2 \\mid Z=z)$. The probability of interest, $\\mathbb{P}(X_1  u_1, X_2  u_2 \\mid Z=z)$, refers to the joint survival function. Using the principle of inclusion-exclusion for probabilities, we relate the joint survival probability to the copula and marginal probabilities:\n$$\n\\mathbb{P}(X_1  u_1, X_2  u_2) = 1 - \\mathbb{P}(X_1 \\le u_1) - \\mathbb{P}(X_2 \\le u_2) + \\mathbb{P}(X_1 \\le u_1, X_2 \\le u_2)\n$$\nSubstituting the notation for the marginal and joint CDFs, we have:\n$$\nP_{exc}(u_1, u_2 \\mid z) = 1 - p_1 - p_2 + C(p_1, p_2 \\mid Z=z)\n$$\nThe problem specifies the GEV CDF, $F_i(x \\mid Z=z)$, for the margins, and an extreme value copula for the dependence. The parameters of these distributions, $\\mu_i(z)$, $\\sigma_i(z)$, $\\xi_i(z)$, and $\\alpha(z)$, are functions of the covariate $Z=z$. The GEV CDF $F_i(x \\mid z)$ is calculated as:\n$$\nF_i(x \\mid z) = \\begin{cases} \\exp\\left(-\\exp\\left(-\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right)\\right)  \\text{if } \\xi_i(z)=0 \\\\ \\exp\\left(-\\left[1+\\xi_i(z)\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right]^{-1/\\xi_i(z)}\\right)  \\text{if } \\xi_i(z)\\neq0 \\end{cases}\n$$\nThe copula $C(p_1, p_2 \\mid Z=z)$ is constructed from the logistic stable tail dependence function $l(t_1, t_2 \\mid Z=z) = (t_1^{1/\\alpha(z)} + t_2^{1/\\alpha(z)})^{\\alpha(z)}$, where $t_i = -\\log p_i$. The copula is then:\n$$\nC(p_1, p_2 \\mid Z=z) = \\exp(-l(-\\log p_1, -\\log p_2 \\mid Z=z)) = \\exp\\left(-\\left((-\\log p_1)^{1/\\alpha(z)} + (-\\log p_2)^{1/\\alpha(z)}\\right)^{\\alpha(z)}\\right)\n$$\nWith these components, we can compute the compound exceedance probability for any given scenario $z$.\n\nLet the set of marginal parameters for both variables be denoted by $M(z) = \\{\\mu_1(z), \\sigma_1(z), \\xi_1(z), \\mu_2(z), \\sigma_2(z), \\xi_2(z)\\}$ and the dependence parameter by $D(z) = \\alpha(z)$. The exceedance probability can be written as a function of these parameter sets: $P(M(z), D(z))$. We consider two scenarios, $z_1$ and $z_2$.\nThe baseline probability is $P_1 = P(M(z_1), D(z_1))$.\nThe final probability is $P_2 = P(M(z_2), D(z_2))$.\nThe total change is $\\Delta = P_2 - P_1$.\n\nThe attribution analysis requires calculating probabilities for two hybrid scenarios:\n1. Margins from $z_2$ and dependence from $z_1$: $\\mathbb{P}_{\\text{M}} = P(M(z_2), D(z_1))$.\n2. Margins from $z_1$ and dependence from $z_2$: $\\mathbb{P}_{\\text{D}} = P(M(z_1), D(z_2))$.\n\nThe change is decomposed along two paths:\nPath A (margins, then dependence): The change is split into a part due to margins, $\\Delta_{M,A} = \\mathbb{P}_{\\text{M}} - P_1$, and a part due to dependence, $\\Delta_{D,A} = P_2 - \\mathbb{P}_{\\text{M}}$.\nPath B (dependence, then margins): The change is split into a part due to dependence, $\\Delta_{D,B} = \\mathbb{P}_{\\text{D}} - P_1$, and a part due to margins, $\\Delta_{M,B} = P_2 - \\mathbb{P}_{\\text{D}}$.\n\nThe final symmetric, path-independent contributions are the averages of the contributions from each path:\n- Margins contribution: $\\Delta_M = (\\Delta_{M,A} + \\Delta_{M,B}) / 2$.\n- Dependence contribution: $\\Delta_D = (\\Delta_{D,A} + \\Delta_{D,B}) / 2$.\nBy construction, $\\Delta_M + \\Delta_D = \\Delta$.\n\nThe solution involves implementing functions to compute the GEV parameters, the GEV CDF, the copula, and the compound exceedance probability. These functions are then orchestrated to calculate $P_1$, $P_2$, $\\mathbb{P}_{\\text{M}}$, and $\\mathbb{P}_{\\text{D}}$, from which the final attribution components are derived for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the compound event attribution problem for the provided test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"z1\": 0.0, \"z2\": 1.0, \"u1\": 70.0, \"u2\": 5.5,\n            \"mu10\": 30.0, \"mu11\": 2.0, \"sigma10\": 10.0, \"sigma11\": 0.1, \"xi10\": 0.1, \"xi11\": 0.0,\n            \"mu20\": 1.5, \"mu21\": 0.2, \"sigma20\": 0.5, \"sigma21\": 0.05, \"xi20\": -0.1, \"xi21\": 0.0,\n            \"alpha0\": 0.6, \"alpha1\": -0.1\n        },\n        # Case 2 (independence boundary)\n        {\n            \"z1\": 0.0, \"z2\": 2.0, \"u1\": 2.0, \"u2\": 3.0,\n            \"mu10\": 0.0, \"mu11\": 0.0, \"sigma10\": 1.0, \"sigma11\": 0.0, \"xi10\": 0.0, \"xi11\": 0.0,\n            \"mu20\": 0.0, \"mu21\": 1.0, \"sigma20\": 1.0, \"sigma21\": 0.2, \"xi20\": 0.0, \"xi21\": 0.0,\n            \"alpha0\": 1.0, \"alpha1\": 0.0\n        },\n        # Case 3 (dependence-only change)\n        {\n            \"z1\": 0.0, \"z2\": 1.0, \"u1\": 20.0, \"u2\": 8.0,\n            \"mu10\": 10.0, \"mu11\": 0.0, \"sigma10\": 2.0, \"sigma11\": 0.0, \"xi10\": 0.05, \"xi11\": 0.0,\n            \"mu20\": 5.0, \"mu21\": 0.0, \"sigma20\": 1.0, \"sigma21\": 0.0, \"xi20\": 0.0, \"xi21\": 0.0,\n            \"alpha0\": 0.9, \"alpha1\": -0.6\n        }\n    ]\n\n    def get_gev_cdf(x, mu, sigma, xi):\n        \"\"\"Computes the GEV cumulative distribution function.\"\"\"\n        if sigma = 0:\n            raise ValueError(\"Scale parameter sigma must be positive.\")\n        \n        # Handle the Gumbel case (xi = 0)\n        if xi == 0.0:\n            return np.exp(-np.exp(-(x - mu) / sigma))\n\n        # Handle the Frechet/Weibull cases (xi != 0)\n        support_term = 1 + xi * (x - mu) / sigma\n        if support_term = 0:\n            # If x is outside the support of the distribution\n            if xi  0: # Lower bound at mu - sigma/xi\n                return 0.0\n            else: # Upper bound at mu - sigma/xi\n                return 1.0\n        \n        return np.exp(-np.power(support_term, -1.0 / xi))\n\n    def get_compound_exceedance_prob(u1, u2, m1_params, m2_params, alpha):\n        \"\"\"\n        Computes the compound exceedance probability P(X1  u1, X2  u2).\n        \"\"\"\n        mu1, sigma1, xi1 = m1_params\n        mu2, sigma2, xi2 = m2_params\n\n        p1 = get_gev_cdf(u1, mu1, sigma1, xi1)\n        p2 = get_gev_cdf(u2, mu2, sigma2, xi2)\n\n        # Handle independence case alpha=1 separately to avoid potential precision issues\n        if alpha == 1.0:\n            return (1.0 - p1) * (1.0 - p2)\n\n        # Avoid log(0) for probabilities at the boundary\n        if p1 == 0.0 or p2 == 0.0:\n            return 0.0\n        if p1 == 1.0:\n            return 1.0 - p2\n        if p2 == 1.0:\n            return 1.0 - p1\n\n        t1 = -np.log(p1)\n        t2 = -np.log(p2)\n        \n        l = np.power(np.power(t1, 1.0/alpha) + np.power(t2, 1.0/alpha), alpha)\n        copula_val = np.exp(-l)\n        \n        prob = 1.0 - p1 - p2 + copula_val\n        return prob\n\n    results = []\n    for case in test_cases:\n        # Define functions to get parameters based on covariate z\n        def get_params_at_z(z):\n            mu1 = case[\"mu10\"] + case[\"mu11\"] * z\n            sigma1 = case[\"sigma10\"] * np.exp(case[\"sigma11\"] * z)\n            xi1 = case[\"xi10\"] + case[\"xi11\"] * z\n            m1_params = (mu1, sigma1, xi1)\n\n            mu2 = case[\"mu20\"] + case[\"mu21\"] * z\n            sigma2 = case[\"sigma20\"] * np.exp(case[\"sigma21\"] * z)\n            xi2 = case[\"xi20\"] + case[\"xi21\"] * z\n            m2_params = (mu2, sigma2, xi2)\n\n            alpha = case[\"alpha0\"] + case[\"alpha1\"] * z\n            \n            return m1_params, m2_params, alpha\n\n        u1, u2 = case[\"u1\"], case[\"u2\"]\n        z1, z2 = case[\"z1\"], case[\"z2\"]\n\n        # Get parameter sets for the two scenarios\n        m1_params_z1, m2_params_z1, alpha_z1 = get_params_at_z(z1)\n        m1_params_z2, m2_params_z2, alpha_z2 = get_params_at_z(z2)\n\n        # Calculate probabilities for the four scenarios (P1, P2, PM, PD)\n        # P1: Margins and dependence from z1\n        p1 = get_compound_exceedance_prob(u1, u2, m1_params_z1, m2_params_z1, alpha_z1)\n        \n        # P2: Margins and dependence from z2\n        p2 = get_compound_exceedance_prob(u1, u2, m1_params_z2, m2_params_z2, alpha_z2)\n        \n        # PM: Margins from z2, dependence from z1\n        p_m = get_compound_exceedance_prob(u1, u2, m1_params_z2, m2_params_z2, alpha_z1)\n        \n        # PD: Margins from z1, dependence from z2\n        p_d = get_compound_exceedance_prob(u1, u2, m1_params_z1, m2_params_z1, alpha_z2)\n\n        # Total change\n        delta = p2 - p1\n\n        # Path A contributions\n        delta_m_a = p_m - p1\n        delta_d_a = p2 - p_m\n\n        # Path B contributions\n        delta_d_b = p_d - p1\n        delta_m_b = p2 - p_d\n\n        # Averaged contributions\n        margins_contribution = (delta_m_a + delta_m_b) / 2.0\n        dependence_contribution = (delta_d_a + delta_d_b) / 2.0\n        \n        case_results = [p1, p2, delta, margins_contribution, dependence_contribution]\n        results.append(f\"[{','.join(f'{x:.8f}' for x in case_results)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}