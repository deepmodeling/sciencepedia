{
    "hands_on_practices": [
        {
            "introduction": "To accurately assess the risk of compound extremes, we must look beyond simple correlations and focus on the dependence structure in the tails of the distribution. While the asymptotic dependence coefficient $\\lambda_U$ is a common starting point, it can sometimes be misleading. This exercise explores the critical concept of \"hidden\" regular variation, where two variables can be asymptotically independent ($\\lambda_U=0$) yet still exhibit a degree of dependence for extreme but finite levels, leading to a much higher joint risk than true independence would suggest .",
            "id": "3868820",
            "problem": "A region’s summer compound risk is characterized by two environmental indices: $X$ representing heatwave intensity and $Y$ representing drought severity. Suppose both indices are transformed to have unit Fréchet margins, so that for large thresholds $t$ the marginal exceedance probabilities satisfy $P(X>t) \\sim t^{-1}$ and $P(Y>t) \\sim t^{-1}$. The asymptotic upper tail dependence coefficient is defined as $\\lambda_U = \\lim_{t \\to \\infty} \\frac{P(X>t, Y>t)}{P(X>t)}$. The coefficient of tail dependence in the sense of Ledford and Tawn is defined by the existence of a slowly varying function $L(t)$ and a parameter $\\eta \\in (0,1]$ such that $P(X>t, Y>t) \\sim L(t)\\,t^{-1/\\eta}$ as $t \\to \\infty$. Consider two candidate models of dependence for $(X,Y)$, each with $\\lambda_U = 0$ (asymptotic independence), but differing in their residual tail dependence structure:\n\n- Model I: Baseline asymptotic independence without hidden regular variation, implying independent tail behavior between $X$ and $Y$ at extreme levels.\n- Model II: Asymptotic independence with hidden regular variation, characterized by $\\eta = 0.7$ and the same slowly varying behavior $L(t)$.\n\nAssume thresholds are chosen so that the marginal exceedance probability for each index per summer is $p = 0.02$, that years are exchangeable with weak temporal dependence such that rare-event counts can be approximated as independent across years, and that $L(t)$ may be treated as approximately constant over the range of interest (a standard assumption for slowly varying functions in asymptotic risk approximations). Over $n = 50$ summers, compare the implications of the two models for the probability of at least one concurrent exceedance event, defined as the event $\\{X>t, Y>t\\}$ occurring in any summer, and for the interpretation of rare concurrent heat and drought risk. Which of the following statements is most consistent with the governing definitions and asymptotic behavior?\n\nA. Under Model II, the per-summer joint exceedance probability scales as $p^{1/\\eta}$, giving a larger value than $p^2$ for small $p$; numerically, the probability of at least one concurrent event in $50$ summers is about $0.17$ for Model II versus about $0.02$ for Model I, so hidden regular variation substantially increases compound risk despite $\\lambda_U=0$.\n\nB. Because both models have $\\lambda_U=0$, the probability of at least one concurrent exceedance in $50$ summers is asymptotically identical under Model I and Model II and is approximately $0.02$ in both cases.\n\nC. Hidden regular variation with $\\eta=0.7$ reduces joint tail probability relative to independence, so the expected number of concurrent exceedances in $50$ summers is smaller under Model II than under Model I.\n\nD. Under Model II, the per-summer joint exceedance probability scales as $p^2$, matching the independent baseline, so hidden regular variation does not affect the compound risk in practice.\n\nE. Hidden regular variation implies $\\lambda_U>0$, so the per-summer joint exceedance probability is asymptotically constant in $p$, leading to near certainty of at least one concurrent event in $50$ summers.",
            "solution": "The problem statement will first be validated for scientific and logical consistency.\n\n### Step 1: Extract Givens\n- Two environmental indices: $X$ (heatwave intensity) and $Y$ (drought severity).\n- Marginal distributions: Unit Fréchet margins.\n- Marginal tail behavior: For a large threshold $t$, the marginal exceedance probabilities satisfy $P(X>t) \\sim t^{-1}$ and $P(Y>t) \\sim t^{-1}$.\n- Asymptotic upper tail dependence coefficient: $\\lambda_U = \\lim_{t \\to \\infty} \\frac{P(X>t, Y>t)}{P(X>t)}$.\n- Coefficient of tail dependence (Ledford and Tawn): $P(X>t, Y>t) \\sim L(t)\\,t^{-1/\\eta}$ for a slowly varying function $L(t)$ and a parameter $\\eta \\in (0,1]$.\n- Model I: Baseline asymptotic independence with $\\lambda_U = 0$, implying independent tail behavior.\n- Model II: Asymptotic independence with hidden regular variation, characterized by $\\lambda_U = 0$ and $\\eta = 0.7$. The function $L(t)$ is assumed to have the \"same slowly varying behavior\".\n- Marginal exceedance probability per summer: $p = 0.02$.\n- Time period: $n = 50$ summers.\n- Inter-annual dependence: Years are exchangeable with weak temporal dependence, allowing rare-event counts to be approximated as independent across years.\n- Assumption on $L(t)$: $L(t)$ may be treated as approximately constant over the range of interest.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the statistical field of multivariate extreme value theory. The concepts presented—unit Fréchet margins, the upper tail dependence coefficient $\\lambda_U$, and the Ledford-Tawn framework for asymptotic independence via the coefficient $\\eta$—are standard and well-established for modeling compound events.\n\nThe relationship between $\\lambda_U$ and $\\eta$ is consistent:\nThe definition of $\\lambda_U$ is $\\lim_{t \\to \\infty} P(Y>t|X>t) = \\lim_{t \\to \\infty} \\frac{P(X>t, Y>t)}{P(X>t)}$.\nUsing the given asymptotic forms, $\\lambda_U = \\lim_{t \\to \\infty} \\frac{L(t)t^{-1/\\eta}}{t^{-1}} = \\lim_{t \\to \\infty} L(t)t^{1-1/\\eta}$.\nFor $\\eta \\in (0,1)$, we have $1 - 1/\\eta < 0$, which causes $t^{1-1/\\eta} \\to 0$ as $t \\to \\infty$. As $L(t)$ is slowly varying, the limit is $\\lambda_U = 0$. This correctly describes asymptotic independence. The case $\\eta=0.7$ thus falls into the asymptotically independent class ($\\lambda_U=0$), as stated. The case of perfect independence in the tails can be modeled within this framework by setting $\\eta=0.5$ and $L(t)=1$, since $P(X>t,Y>t) \\approx P(X>t)P(Y>t) \\sim (t^{-1})^2 = t^{-2} = t^{-1/0.5}$.\n\nThe problem setup is well-posed, objective, and contains sufficient information to compare the two models. There are no scientific contradictions or logical fallacies.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. The solution will proceed by deriving the risk implications for each model and evaluating the given options.\n\n### Derivation and Analysis\n\nThe core task is to compute the probability of at least one concurrent exceedance event, $\\{X>t, Y>t\\}$, over $n=50$ summers. Let $p_{\\text{joint}}$ be the probability of this event in a single summer. Since events are treated as independent across summers, the probability of at least one event in $n$ summers is $P_{\\text{total}} = 1 - (1-p_{\\text{joint}})^n$.\n\nWe are given the marginal exceedance probability $p = P(X>t) = P(Y>t) = 0.02$. From the unit Fréchet margin property, $p \\sim t^{-1}$, which implies the threshold $t$ is such that $t \\approx p^{-1}$.\n\n**Model I: Baseline Asymptotic Independence**\nThis model assumes that for extreme events, $X$ and $Y$ are independent.\nThe joint probability for a single summer is:\n$$p_{\\text{joint, I}} = P(X>t, Y>t) = P(X>t)P(Y>t) = p \\cdot p = p^2$$\nSubstituting $p=0.02$:\n$$p_{\\text{joint, I}} = (0.02)^2 = 0.0004$$\nThe probability of at least one such event in $n=50$ summers is:\n$$P_{\\text{total, I}} = 1 - (1 - p_{\\text{joint, I}})^{50} = 1 - (1 - 0.0004)^{50}$$\nUsing a calculator, $ (0.9996)^{50} \\approx 0.980198 $.\n$$P_{\\text{total, I}} \\approx 1 - 0.980198 = 0.019802$$\nThis value is approximately $0.02$.\n\n**Model II: Asymptotic Independence with Hidden Regular Variation**\nThis model is described by the Ledford-Tawn framework with $\\eta=0.7$.\nThe joint probability is $P(X>t, Y>t) \\sim L(t)t^{-1/\\eta}$.\nSubstituting $t \\approx p^{-1}$ and treating $L(t)$ as a constant $C$, the joint probability scales with the marginal probability $p$ as:\n$$p_{\\text{joint, II}} \\approx C \\cdot p^{1/\\eta}$$\nWith $\\eta=0.7$, the exponent is $1/0.7 = 10/7 \\approx 1.4286$.\nThe scaling is $p_{\\text{joint, II}} \\propto p^{10/7}$.\nFor the independence case (Model I), the scaling is $p^2$. Since the exponent for Model II ($10/7 \\approx 1.4286$) is smaller than the exponent for Model I ($2$), for a small probability $p$, Model II will predict a significantly higher joint probability than Model I. This captures the \"hidden\" or \"residual\" dependence.\n\n### Option-by-Option Analysis\n\n**A. Under Model II, the per-summer joint exceedance probability scales as $p^{1/\\eta}$, giving a larger value than $p^2$ for small $p$; numerically, the probability of at least one concurrent event in $50$ summers is about $0.17$ for Model II versus about $0.02$ for Model I, so hidden regular variation substantially increases compound risk despite $\\lambda_U=0$.**\n- **Scaling Law**: The statement that the probability scales as $p^{1/\\eta}$ is correct based on the Ledford-Tawn model.\n- **Comparison to Independence**: The statement that this gives a larger value than $p^2$ is correct, as for $p=0.02$ and $\\eta=0.7$, the exponent $1/\\eta \\approx 1.4286 < 2$.\n- **Numerical Value for Model I**: The value of \"about $0.02$\" is consistent with our calculation of $0.0198$.\n- **Numerical Value for Model II**: Let's check the value of $0.17$. This would require $1 - (1 - p_{\\text{joint, II}})^{50} \\approx 0.17$, which implies $p_{\\text{joint, II}} \\approx 0.00372$. We know $p_{\\text{joint, II}} \\approx C \\cdot p^{1/0.7} = C \\cdot (0.02)^{10/7} \\approx C \\cdot 0.001964$. To obtain the probability of $0.00372$, the constant $C$ must be $C \\approx 0.00372 / 0.001964 \\approx 1.89$. While the problem does not specify $C$, a value of $C \\approx 2$ is plausible in applications. The phrase \"same slowly varying behavior\" likely implies the functional form is simple, and the problem expects this constant to be inferred or accepted as given in the option.\n- **Conclusion**: The concluding remark that hidden regular variation substantially increases compound risk is the central message of this theory and is demonstrated by the large difference between the probabilities ($0.17$ vs $0.02$).\nThis statement is qualitatively and conceptually sound in every aspect. The numerical value for Model II is achievable with a reasonable, albeit unstated, constant for the slowly varying function.\n**Verdict: Correct.**\n\n**B. Because both models have $\\lambda_U=0$, the probability of at least one concurrent exceedance in $50$ summers is asymptotically identical under Model I and Model II and is approximately $0.02$ in both cases.**\nThis is fundamentally incorrect. The coefficient $\\eta$ is specifically designed to distinguish different dependence structures within the class of asymptotically independent models (for which $\\lambda_U=0$). As shown, a model with $\\eta=0.7$ has a substantially higher joint probability than a model with behavior equivalent to $\\eta=0.5$ (independence). Thus, the probabilities are not identical.\n**Verdict: Incorrect.**\n\n**C. Hidden regular variation with $\\eta=0.7$ reduces joint tail probability relative to independence, so the expected number of concurrent exceedances in $50$ summers is smaller under Model II than under Model I.**\nThis is the opposite of the truth. The joint probability for Model II scales as $p^{1/0.7} \\approx p^{1.43}$, while for Model I (independence) it scales as $p^2$. Since $1.43 < 2$, for small $p$ (like $p=0.02$), $p^{1.43} > p^2$. Therefore, hidden regular variation with $\\eta>0.5$ *increases* the joint probability relative to independence, leading to a higher expected number of events.\n**Verdict: Incorrect.**\n\n**D. Under Model II, the per-summer joint exceedance probability scales as $p^2$, matching the independent baseline, so hidden regular variation does not affect the compound risk in practice.**\nThis misstates the scaling law for Model II. The defining feature of this model is that the probability scales as $p^{1/\\eta}$ with $\\eta=0.7$, which is different from the $p^2$ scaling of independence ($\\eta=0.5$). Consequently, it does affect the compound risk substantially.\n**Verdict: Incorrect.**\n\n**E. Hidden regular variation implies $\\lambda_U>0$, so the per-summer joint exceedance probability is asymptotically constant in $p$, leading to near certainty of at least one concurrent event in $50$ summers.**\nThis statement contains multiple errors. First, hidden regular variation is a concept developed for the case of asymptotic independence, i.e., $\\lambda_U=0$. The case $\\lambda_U > 0$ is termed asymptotic dependence. Second, if $\\lambda_U > 0$, the joint probability is $p_{\\text{joint}} \\approx \\lambda_U p$, which is linear in $p$, not constant. It tends to $0$ as $p \\to 0$.\n**Verdict: Incorrect.**\n\nBased on the analysis, statement A is the only one that correctly describes the a) theoretical scaling, b) qualitative comparison to independence, and c) overall implication for risk assessment. The numerical values provided are consistent with the theory, with the constant for the slowly varying function for Model II being implicitly defined by the option itself.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "When analyzing time series of environmental data, we often find that extreme events cluster together in time—for example, during a single prolonged storm system. To properly estimate the frequency of independent compound events, we must first \"decluster\" the data to isolate these event clusters. This practice introduces the runs method for declustering and the concept of the extremal index, $\\theta$, which quantifies the degree of temporal clustering and corrects our risk estimates accordingly .",
            "id": "3868877",
            "problem": "Consider a daily bivariate stationary time series $\\{(P_t,S_t)\\}_{t=1}^T$ where $P_t$ is precipitation (in millimeters) and $S_t$ is storm surge (in meters). A compound event is defined as a joint threshold exceedance, namely the event that $P_t > u_P$ and $S_t > u_S$, where $u_P$ and $u_S$ are high quantile thresholds chosen so that $P(P_t > u_P) \\approx 0.05$ and $P(S_t > u_S) \\approx 0.05$. Let $Z_t = \\mathbf{1}\\{P_t > u_P,\\, S_t > u_S\\}$ denote the indicator of joint exceedance, and suppose the time series is $\\alpha$-mixing with short-range dependence so that a Poisson cluster process limit for exceedances is appropriate under high thresholds.\n\nExplain declustering of the joint exceedance time series $\\{Z_t\\}$ using the runs method with gap $r$: articulate from first principles how clusters are defined and terminated with $r$ consecutive non-exceedances, and derive an estimator for the extremal index $\\theta$ in terms of observed cluster counts and exceedances, including the theoretical justification linking $\\theta$ to the mean cluster size. Then, justify the choice of the gap $r$ by combining the stabilization of the extremal index estimator with a short-range tail dependence assessment based on the conditional probability $P(Z_{t+\\ell}=1 \\mid Z_t=1)$ decaying to the unconditional exceedance probability $p = P(Z_t=1)$ at lag $\\ell \\ge r$.\n\nYou are given a data set of length $T = 20000$ days for which $N = 240$ joint exceedances were observed over the study period. Using the runs declustering applied to $\\{Z_t\\}$ for several candidate gaps $r$, the number of clusters $J(r)$ was obtained as:\n$J(1) = 110$, $J(3) = 92$, $J(5) = 85$, $J(7) = 85$.\nAdditionally, the empirical conditional exceedance probabilities $\\hat{\\rho}(\\ell) = \\widehat{P}(Z_{t+\\ell}=1 \\mid Z_t=1)$ and the unconditional probability $\\hat{p} = \\widehat{P}(Z_t=1)$ were computed as:\n$\\hat{\\rho}(1) = 0.18$, $\\hat{\\rho}(2) = 0.09$, $\\hat{\\rho}(3) = 0.04$, $\\hat{\\rho}(4) = 0.02$, $\\hat{\\rho}(5) = 0.013$, $\\hat{\\rho}(6) = 0.0125$, $\\hat{\\rho}(7) = 0.012$, $\\hat{p} = 0.012$.\nChoose the option that correctly describes the declustering setup for the compound event, derives a principled extremal index estimator from first principles, and justifies an appropriate choice of $r$ for this data, including a numerical estimate of $\\theta$ consistent with the provided information.\n\nA. Define $Z_t = \\mathbf{1}\\{P_t > u_P,\\, S_t > u_S\\}$ and use runs declustering with gap $r$ so that a cluster terminates only after $r$ consecutive days with $Z_t = 0$. Under a Poisson cluster process limit for exceedances, the extremal index $\\theta$ equals the reciprocal of the limiting mean cluster size, motivating the estimator $\\hat{\\theta}(r) = J(r)/N$ from the ratio of the number of clusters to the number of exceedances. Choose $r$ as the smallest lag for which both $\\hat{\\theta}(r)$ stabilizes across larger $r$ and $\\hat{\\rho}(\\ell)$ is indistinguishable from $\\hat{p}$ for $\\ell \\ge r$. For the given data, $J(5) = 85$ and $J(7) = 85$ yield the same $\\hat{\\theta}$, and $\\hat{\\rho}(\\ell)$ has essentially reached $\\hat{p}$ by $\\ell = 5$, so $r = 5$ is appropriate with $\\hat{\\theta}(5) = 85/240 \\approx 0.354$.\n\nB. Declustering can be performed on $P_t$ alone because precipitation is the driver of compound events, and the gap $r$ should be chosen to maximize the extremal index estimator. Using the given counts, $r = 7$ maximizes $\\hat{\\theta}(r)$, so $\\hat{\\theta}(7) = 85/240 \\approx 0.354$ is appropriate.\n\nC. Clusters should be defined as contiguous runs of joint exceedances with no zeros allowed inside, corresponding to $r = 0$. This choice ensures independence across clusters and implies $\\theta = 1$ for high thresholds, so declustering is unnecessary.\n\nD. Define the compound event indicator as $Z_t = \\mathbf{1}\\{P_t > u_P \\text{ or } S_t > u_S\\}$ so that either component exceeding is treated as a compound event. Choose $r$ by the largest drop in $\\hat{\\rho}(\\ell)$, which occurs by $\\ell = 3$, and estimate $\\theta$ as $\\hat{\\theta}(3) = 92/240 \\approx 0.383$ because fewer dependencies remain at this lag.\n\nSelect the single best option.",
            "solution": "The user wants me to solve a problem related to the declustering of compound extreme events and the estimation of the extremal index.\n\n### Step 1: Extract Givens\n\n-   **Time Series**: A daily bivariate stationary time series $\\{(P_t, S_t)\\}_{t=1}^T$, where $P_t$ is precipitation and $S_t$ is storm surge.\n-   **Compound Event Definition**: A joint threshold exceedance, $P_t > u_P$ and $S_t > u_S$.\n-   **Thresholds**: $u_P$ and $u_S$ are high quantile thresholds such that the marginal exceedance probabilities are $P(P_t > u_P) \\approx 0.05$ and $P(S_t > u_S) \\approx 0.05$.\n-   **Indicator Variable**: $Z_t = \\mathbf{1}\\{P_t > u_P,\\, S_t > u_S\\}$.\n-   **Dependence Structure**: The time series $\\{Z_t\\}$ is $\\alpha$-mixing with short-range dependence.\n-   **Statistical Model**: A Poisson cluster process limit for exceedances is appropriate.\n-   **Declustering Method**: Runs method with gap $r$, where clusters are terminated by $r$ consecutive non-exceedances.\n-   **Parameter to Estimate**: The extremal index, $\\theta$.\n-   **Data**:\n    -   Time series length: $T = 20000$ days.\n    -   Total number of joint exceedances: $N = 240$.\n    -   Number of clusters $J(r)$ for different gaps $r$: $J(1) = 110$, $J(3) = 92$, $J(5) = 85$, $J(7) = 85$.\n    -   Empirical conditional exceedance probabilities $\\hat{\\rho}(\\ell) = \\widehat{P}(Z_{t+\\ell}=1 \\mid Z_t=1)$ for lag $\\ell$: $\\hat{\\rho}(1) = 0.18$, $\\hat{\\rho}(2) = 0.09$, $\\hat{\\rho}(3) = 0.04$, $\\hat{\\rho}(4) = 0.02$, $\\hat{\\rho}(5) = 0.013$, $\\hat{\\rho}(6) = 0.0125$, $\\hat{\\rho}(7) = 0.012$.\n    -   Empirical unconditional exceedance probability: $\\hat{p} = \\widehat{P}(Z_t=1) = 0.012$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is well-grounded in the statistical theory of extreme values for dependent time series. The concepts of compound events, joint exceedances, declustering, the runs method, the extremal index, and Poisson cluster processes are standard in climatology, hydrology, and financial econometrics. The application to precipitation and storm surge is a canonical example of a compound environmental hazard. The premise is scientifically sound.\n-   **Well-Posed**: The problem is well-posed. It asks for an explanation of a methodology and its application to a given dataset. The data provided is sufficient to apply the methodology and evaluate the options. A unique, meaningful solution can be derived.\n-   **Objective**: The language is precise and objective. All terms are either standard in the field or explicitly defined. The quantities are numerical. There are no subjective or opinion-based statements.\n-   **Completeness**: The problem statement is self-contained and complete. It provides all necessary definitions and data to derive the answer. The data is internally consistent (e.g., $\\hat{p} = N/T = 240/20000 = 0.012$, which matches the provided value).\n-   **No other flaws**: The problem does not violate any of the other criteria for invalidity. It is not trivial, unrealistic, or non-formalizable.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with deriving the solution and evaluating the options.\n\n### Solution Derivation\n\nThe problem requires an explanation of the runs declustering method, the derivation of an estimator for the extremal index $\\theta$, and the justification for choosing the gap parameter $r$, followed by an application to the given data.\n\n**1. Declustering and the Extremal Index $\\theta$**\n\nIn a stationary time series with short-range dependence, extreme events (exceedances of a high threshold) tend to occur in clusters. The extremal index, $\\theta \\in (0, 1]$, quantifies this clustering. A value of $\\theta=1$ corresponds to the case of independent exceedances (like in an i.i.d. sequence), while $\\theta < 1$ indicates clustering. The extremal index can be interpreted as the reciprocal of the limiting mean size of clusters of exceedances. That is,\n$$ \\theta = \\frac{1}{\\text{mean cluster size}} $$\nThe sequence of clusters of exceedances can be treated as a Poisson process with a rate of occurrence that is $\\theta$ times the rate of exceedances in a corresponding independent series.\n\n**2. Runs Declustering and the Estimator for $\\theta$**\n\nThe \"runs method\" is a common technique for identifying clusters. Let $Z_t = \\mathbf{1}\\{P_t > u_P, S_t > u_S\\}$ be the indicator of a joint exceedance on day $t$. A cluster of exceedances is defined as a sequence of observations that begins with an exceedance ($Z_t=1$) and is terminated by a \"gap\" of $r$ consecutive non-exceedances (i.e., $Z_{t+1}=0, Z_{t+2}=0, \\dots, Z_{t+r}=0$). All exceedances occurring between two such gaps are considered part of the same cluster. The parameter $r$ is the declustering gap.\n\nBased on the theoretical relationship between $\\theta$ and the mean cluster size, a natural estimator for $\\theta$ using the runs method is the reciprocal of the empirical mean cluster size. Given $N$ total exceedances and $J(r)$ clusters identified with a gap of size $r$, the empirical mean cluster size is $\\frac{N}{J(r)}$. Therefore, the estimator for the extremal index is:\n$$ \\hat{\\theta}(r) = \\frac{1}{N/J(r)} = \\frac{J(r)}{N} $$\nThis estimator gives the ratio of the number of clusters (independent extreme events) to the total number of individual exceedances.\n\n**3. Choosing the Gap Parameter $r$**\n\nThe choice of $r$ is a crucial step involving a bias-variance trade-off.\n-   If $r$ is too small, a single physical cluster might be split into multiple smaller ones. This artificially inflates the number of clusters $J(r)$, leading to a positively biased estimate of $\\theta$.\n-   If $r$ is too large, physically distinct and independent clusters might be merged into one. This artificially deflates the number of clusters $J(r)$, leading to a negatively biased estimate of $\\theta$.\n\nA principled approach to choosing $r$ relies on two complementary diagnostics:\n1.  **Stabilization of $\\hat{\\theta}(r)$**: We compute $\\hat{\\theta}(r)$ for a range of candidate values of $r$. We look for a value of $r$ after which the estimate $\\hat{\\theta}(r)$ becomes stable (i.e., does not change significantly for larger $r$). This region of stability suggests that $r$ is large enough to separate most of the independent clusters.\n2.  **Decay of Serial Dependence**: The purpose of declustering is to render the resulting cluster events approximately independent. This implies that the gap $r$ should be large enough to break the short-range temporal dependence in the exceedance series $\\{Z_t\\}$. We can assess this by examining the conditional probability $P(Z_{t+\\ell}=1 \\mid Z_t=1)$. This probability should decay to the unconditional probability $p = P(Z_t=1)$ for lags $\\ell$ greater than or equal to the chosen gap $r$. That is, we choose $r$ such that for all $\\ell \\ge r$, $\\hat{\\rho}(\\ell) \\approx \\hat{p}$.\n\n**4. Application to the Given Data**\n\nWe apply these principles to the provided data.\n-   $N=240$\n-   $J(1)=110, J(3)=92, J(5)=85, J(7)=85$\n-   $\\hat{p}=0.012$\n-   $\\hat{\\rho}(1)=0.18, \\hat{\\rho}(2)=0.09, \\hat{\\rho}(3)=0.04, \\hat{\\rho}(4)=0.02, \\hat{\\rho}(5)=0.013, \\hat{\\rho}(6)=0.0125, \\hat{\\rho}(7)=0.012$\n\nFirst, let's compute the extremal index estimates:\n-   $\\hat{\\theta}(1) = \\frac{J(1)}{N} = \\frac{110}{240} \\approx 0.458$\n-   $\\hat{\\theta}(3) = \\frac{J(3)}{N} = \\frac{92}{240} \\approx 0.383$\n-   $\\hat{\\theta}(5) = \\frac{J(5)}{N} = \\frac{85}{240} \\approx 0.354$\n-   $\\hat{\\theta}(7) = \\frac{J(7)}{N} = \\frac{85}{240} \\approx 0.354$\n\nNow, we check for stabilization. The estimates are $0.458, 0.383, 0.354, 0.354$. The estimator $\\hat{\\theta}(r)$ stabilizes at $r=5$, since $\\hat{\\theta}(5) = \\hat{\\theta}(7)$. This suggests a choice of $r \\ge 5$.\n\nNext, we check for the decay of serial dependence. We compare $\\hat{\\rho}(\\ell)$ to $\\hat{p} = 0.012$:\n-   $\\hat{\\rho}(1) = 0.18 \\gg 0.012$\n-   $\\hat{\\rho}(2) = 0.09 \\gg 0.012$\n-   $\\hat{\\rho}(3) = 0.04 > 0.012$\n-   $\\hat{\\rho}(4) = 0.02 > 0.012$\n-   $\\hat{\\rho}(5) = 0.013 \\approx 0.012$\n-   $\\hat{\\rho}(6) = 0.0125 \\approx 0.012$\n-   $\\hat{\\rho}(7) = 0.012 = 0.012$\n\nThe conditional probability $\\hat{\\rho}(\\ell)$ has essentially decayed to the unconditional probability $\\hat{p}$ at lag $\\ell=5$. For lags $\\ell \\ge 5$, the two probabilities are practically indistinguishable. This suggests that a gap of $r=5$ days is sufficient to ensure approximate independence between clusters.\n\nCombining both criteria, the choice of $r=5$ is well-justified. It is the smallest gap size for which both the estimate $\\hat{\\theta}(r)$ stabilizes and the temporal dependence becomes negligible. With $r=5$, the corresponding estimate for the extremal index is $\\hat{\\theta}(5) = 85/240 \\approx 0.354$.\n\n### Option-by-Option Analysis\n\n**A. Define $Z_t = \\mathbf{1}\\{P_t > u_P,\\, S_t > u_S\\}$ and use runs declustering with gap $r$ so that a cluster terminates only after $r$ consecutive days with $Z_t = 0$. Under a Poisson cluster process limit for exceedances, the extremal index $\\theta$ equals the reciprocal of the limiting mean cluster size, motivating the estimator $\\hat{\\theta}(r) = J(r)/N$ from the ratio of the number of clusters to the number of exceedances. Choose $r$ as the smallest lag for which both $\\hat{\\theta}(r)$ stabilizes across larger $r$ and $\\hat{\\rho}(\\ell)$ is indistinguishable from $\\hat{p}$ for $\\ell \\ge r$. For the given data, $J(5) = 85$ and $J(7) = 85$ yield the same $\\hat{\\theta}$, and $\\hat{\\rho}(\\ell)$ has essentially reached $\\hat{p}$ by $\\ell = 5$, so $r = 5$ is appropriate with $\\hat{\\theta}(5) = 85/240 \\approx 0.354$.**\n\nThis-option correctly states the definition of the joint exceedance indicator $Z_t$.\n-   It correctly describes the runs declustering method.\n-   It correctly links the extremal index $\\theta$ to the mean cluster size and presents the correct estimator $\\hat{\\theta}(r) = J(r)/N$.\n-   It correctly states the dual criteria for choosing $r$: stabilization of the estimator and decay of serial correlation.\n-   It correctly applies these criteria to the data, noting the stabilization of $\\hat{\\theta}(r)$ at $r=5$ (since $\\hat{\\theta}(5) = \\hat{\\theta}(7)$) and the decay of $\\hat{\\rho}(\\ell)$ to $\\hat{p}$ at $\\ell=5$.\n-   The choice of $r=5$ and the resulting calculation $\\hat{\\theta}(5) = 85/240 \\approx 0.354$ are both correct.\n**Verdict: Correct.**\n\n**B. Declustering can be performed on $P_t$ alone because precipitation is the driver of compound events, and the gap $r$ should be chosen to maximize the extremal index estimator. Using the given counts, $r = 7$ maximizes $\\hat{\\theta}(r)$, so $\\hat{\\theta}(7) = 85/240 \\approx 0.354$ is appropriate.**\n\n-   The first statement, \"Declustering can be performed on $P_t$ alone...\", is incorrect. The problem is explicitly about the **compound event** $Z_t$, which depends on the joint behavior of $P_t$ and $S_t$. The clustering properties of $Z_t$ may be very different from those of $P_t$ alone.\n-   The second statement, \"...the gap $r$ should be chosen to maximize the extremal index estimator,\" is methodologically incorrect. The goal is to find a stable estimate, not to maximize it. Maximizing $\\hat{\\theta}(r)$ would typically mean choosing a small $r$ (here $r=1$) and would lead to a biased estimate.\n-   The third statement, \"...$r = 7$ maximizes $\\hat{\\theta}(r)$...\", is factually incorrect based on the data. $\\hat{\\theta}(1) \\approx 0.458$ is the maximum, while $\\hat{\\theta}(7) \\approx 0.354$ is the minimum of the calculated values.\n**Verdict: Incorrect.**\n\n**C. Clusters should be defined as contiguous runs of joint exceedances with no zeros allowed inside, corresponding to $r = 0$. This choice ensures independence across clusters and implies $\\theta = 1$ for high thresholds, so declustering is unnecessary.**\n\n-   The definition of clusters as \"contiguous runs of joint exceedances\" with no zeros allowed corresponds to the runs method with $r=1$, not $r=0$. A gap of one zero is sufficient to terminate the cluster. The term $r=0$ is not standard and conceptually ill-defined in this context.\n-   The claim that this choice \"ensures independence\" is false. Two exceedances separated by a short gap of non-exceedances (e.g., sequence $1,0,1$) can be strongly dependent, but this method would split them.\n-   The claim that this implies $\\theta=1$ is a severe theoretical error. $\\theta=1$ signifies no clustering (i.e., independent exceedances), which would make declustering unnecessary. However, the data clearly show strong clustering: $\\hat{\\rho}(1) = 0.18$ is much larger than $\\hat{p} = 0.012$, and all estimates of $\\theta$ are substantially less than $1$.\n**Verdict: Incorrect.**\n\n**D. Define the compound event indicator as $Z_t = \\mathbf{1}\\{P_t > u_P \\text{ or } S_t > u_S\\}$ so that either component exceeding is treated as a compound event. Choose $r$ by the largest drop in $\\hat{\\rho}(\\ell)$, which occurs by $\\ell = 3$, and estimate $\\theta$ as $\\hat{\\theta}(3) = 92/240 \\approx 0.383$ because fewer dependencies remain at this lag.**\n\n-   The definition of the compound event as an \"or\" condition contradicts the problem statement, which explicitly uses an \"and\" condition ($P_t > u_P$ and $S_t > u_S$). This changes the fundamental nature of the event being analyzed.\n-   The heuristic to \"choose $r$ by the largest drop in $\\hat{\\rho}(\\ell)$\" is not a standard or robust method. The preferred method is to wait for $\\hat{\\rho}(\\ell)$ to decay to the baseline unconditional probability $\\hat{p}$.\n-   At lag $\\ell=3$, $\\hat{\\rho}(3) = 0.04$, which is still over three times the unconditional probability $\\hat{p}=0.012$. Significant dependence remains, so the justification \"fewer dependencies remain at this lag\" is weak and insufficient. The choice of $r=3$ is not well-supported and would likely lead to a biased (high) estimate of $\\theta$.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "In a changing climate, we cannot assume that the statistical properties of extreme events are constant over time. This capstone practice challenges you to build and apply a fully non-stationary model for compound events, where both the marginal behavior of the variables and their dependence structure evolve with an external covariate. You will then use this model to perform an attribution analysis, a powerful technique that disentangles how much of the change in compound risk is due to changes in the individual hazards versus changes in their tendency to occur together .",
            "id": "3868831",
            "problem": "Consider two environmental variables $X_1$ and $X_2$ representing block maxima of distinct, physically relevant processes in an Earth system (for example, extreme precipitation and coastal surge). Assume both variables follow the Generalized Extreme Value (GEV) distribution conditionally on a climate covariate $Z$. The GEV marginal cumulative distribution function for variable $i \\in \\{1,2\\}$ at threshold $x$ and covariate value $Z=z$ is denoted $F_i(x \\mid Z=z)$ with parameters $\\mu_i(z)$ (location), $\\sigma_i(z)$ (scale), and $\\xi_i(z)$ (shape). The GEV distribution is defined as follows for any real $x$ with $\\sigma_i(z)>0$:\n- When $\\xi_i(z) \\neq 0$, the support is given by $1+\\xi_i(z)\\,(x-\\mu_i(z))/\\sigma_i(z) > 0$, and the cumulative distribution function is $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\left[1+\\xi_i(z)\\,\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right]^{-1/\\xi_i(z)}\\right)$.\n- When $\\xi_i(z) = 0$, the cumulative distribution function is $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\exp\\!\\left(-\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right)\\right)$.\n\nAssume further that the joint distribution of the maxima $(X_1,X_2)$ is max-stable and characterized by an extreme value copula driven by a logistic stable tail dependence function. Specifically, denote by $C(p_1,p_2 \\mid Z=z)$ the extreme value copula applied to marginal probabilities $p_1=F_1(x_1 \\mid Z=z)$ and $p_2=F_2(x_2 \\mid Z=z)$. The copula is determined by a stable tail dependence function $l(t_1,t_2 \\mid Z=z)$ where $t_i=-\\log(p_i)$, and by a dependence parameter $\\alpha(z)\\in(0,1]$. The logistic form of the stable tail dependence function is given by $l(t_1,t_2 \\mid Z=z) = \\left(t_1^{1/\\alpha(z)} + t_2^{1/\\alpha(z)}\\right)^{\\alpha(z)}$. The resulting extreme value copula is $C(p_1,p_2 \\mid Z=z) = \\exp\\!\\left(-\\,l(-\\log p_1,-\\log p_2 \\mid Z=z)\\right)$, which encodes the dependence structure between the maxima while preserving the GEV margins.\n\nDefine the compound exceedance probability at thresholds $(u_1,u_2)$ for covariate value $Z=z$ as the probability that both variables exceed their thresholds simultaneously, namely $\\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z)$. This probability must be derived using first principles, beginning from the definitions of the GEV marginal distributions, the extreme value copula induced by the logistic stable tail dependence function, and the inclusion-exclusion principle for joint probabilities. You must then attribute changes in the compound exceedance probability between two scenarios $Z=z_1$ and $Z=z_2$ to changes in margins versus changes in dependence using a symmetric, path-independent decomposition based on averaging contributions along two transformation paths: margins-then-dependence and dependence-then-margins.\n\nAdopt covariate-driven parameterizations for the GEV margins and the logistic dependence parameter. For each variable $i \\in \\{1,2\\}$, specify\n$\\mu_i(z) = \\mu_{i0} + \\mu_{i1}\\,z$, $\\sigma_i(z) = \\sigma_{i0}\\,\\exp(\\sigma_{i1}\\,z)$, and $\\xi_i(z) = \\xi_{i0} + \\xi_{i1}\\,z$, with $\\sigma_{i0}>0$. For dependence, specify $\\alpha(z) = \\alpha_0 + \\alpha_1\\,z$ with $\\alpha(z)\\in(0,1]$ for the scenarios considered.\n\nYour task is to write a complete program that, for each test case listed below, does the following:\n- Computes the compound exceedance probability $\\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z_1)$ and $\\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z_2)$.\n- Computes the total change $\\Delta = \\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z_2) - \\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z_1)$.\n- Performs a symmetric attribution (averaged over two paths) of $\\Delta$ into a margins contribution and a dependence contribution, as defined by:\n  Path A (margins then dependence): the margins-only hybrid probability $\\mathbb{P}_{\\text{M}}$ uses $F_1(\\cdot \\mid Z=z_2)$ and $F_2(\\cdot \\mid Z=z_2)$ with $C(\\cdot,\\cdot \\mid Z=z_1)$, and the dependence contribution along Path A is $\\mathbb{P}(X_1>u_1,X_2>u_2 \\mid Z=z_2) - \\mathbb{P}_{\\text{M}}$ while the margins contribution along Path A is $\\mathbb{P}_{\\text{M}} - \\mathbb{P}(X_1>u_1,X_2>u_2 \\mid Z=z_1)$.\n  Path B (dependence then margins): the dependence-only hybrid probability $\\mathbb{P}_{\\text{D}}$ uses $F_1(\\cdot \\mid Z=z_1)$ and $F_2(\\cdot \\mid Z=z_1)$ with $C(\\cdot,\\cdot \\mid Z=z_2)$, and the margins contribution along Path B is $\\mathbb{P}(X_1>u_1,X_2>u_2 \\mid Z=z_2) - \\mathbb{P}_{\\text{D}}$ while the dependence contribution along Path B is $\\mathbb{P}_{\\text{D}} - \\mathbb{P}(X_1>u_1,X_2>u_2 \\mid Z=z_1)$. The final margins contribution is the average of the margins contributions along the two paths, and the final dependence contribution is the average of the dependence contributions along the two paths.\n\nYou must express all derived probabilities as real numbers (floats). There are no physical units involved in the answer.\n\nTest Suite:\n- Case $1$ (general case with moderate positive dependence and mixed-tail margins):\n  - Scenarios: $z_1=0$, $z_2=1$.\n  - Variable $1$: $\\mu_{10}=30$, $\\mu_{11}=2$, $\\sigma_{10}=10$, $\\sigma_{11}=0.1$, $\\xi_{10}=0.1$, $\\xi_{11}=0.0$, threshold $u_1=70$.\n  - Variable $2$: $\\mu_{20}=1.5$, $\\mu_{21}=0.2$, $\\sigma_{20}=0.5$, $\\sigma_{21}=0.05$, $\\xi_{20}=-0.1$, $\\xi_{21}=0.0$, threshold $u_2=5.5$.\n  - Dependence: $\\alpha_0=0.6$, $\\alpha_1=-0.1$.\n- Case $2$ (independence boundary with Gumbel margins and changing location/scale):\n  - Scenarios: $z_1=0$, $z_2=2$.\n  - Variable $1$: $\\mu_{10}=0$, $\\mu_{11}=0$, $\\sigma_{10}=1$, $\\sigma_{11}=0$, $\\xi_{10}=0$, $\\xi_{11}=0$, threshold $u_1=2$.\n  - Variable $2$: $\\mu_{20}=0$, $\\mu_{21}=1$, $\\sigma_{20}=1$, $\\sigma_{21}=0.2$, $\\xi_{20}=0$, $\\xi_{21}=0$, threshold $u_2=3$.\n  - Dependence: $\\alpha_0=1$, $\\alpha_1=0$.\n- Case $3$ (dependence-only change with fixed margins):\n  - Scenarios: $z_1=0$, $z_2=1$.\n  - Variable $1$: $\\mu_{10}=10$, $\\mu_{11}=0$, $\\sigma_{10}=2$, $\\sigma_{11}=0$, $\\xi_{10}=0.05$, $\\xi_{11}=0$, threshold $u_1=20$.\n  - Variable $2$: $\\mu_{20}=5$, $\\mu_{21}=0$, $\\sigma_{20}=1$, $\\sigma_{21}=0$, $\\xi_{20}=0$, $\\xi_{21}=0$, threshold $u_2=8$.\n  - Dependence: $\\alpha_0=0.9$, $\\alpha_1=-0.6$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of five floats in the order $[\\mathbb{P}(X_1>u_1,X_2>u_2 \\mid Z=z_1), \\mathbb{P}(X_1>u_1,X_2>u_2 \\mid Z=z_2), \\Delta, \\text{margins\\_contribution}, \\text{dependence\\_contribution}]$. For example, the final output should look like $[[p_{1,1},p_{1,2},\\Delta_1,m_1,d_1],[p_{2,1},p_{2,2},\\Delta_2,m_2,d_2],[p_{3,1},p_{3,2},\\Delta_3,m_3,d_3]]$.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Marginal Distributions**: Two environmental variables $X_1$ and $X_2$ follow the Generalized Extreme Value (GEV) distribution conditionally on a climate covariate $Z$.\n- **GEV CDF**: For variable $i \\in \\{1,2\\}$ at threshold $x$ and covariate value $Z=z$, the CDF is $F_i(x \\mid Z=z)$ with parameters $\\mu_i(z)$, $\\sigma_i(z)>0$, and $\\xi_i(z)$.\n  - If $\\xi_i(z) \\neq 0$: $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\left[1+\\xi_i(z)\\,\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right]^{-1/\\xi_i(z)}\\right)$, with support $1+\\xi_i(z)\\,(x-\\mu_i(z))/\\sigma_i(z) > 0$.\n  - If $\\xi_i(z) = 0$: $F_i(x \\mid Z=z) = \\exp\\!\\left(-\\exp\\!\\left(-\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right)\\right)$.\n- **Dependence Structure**: The joint distribution of $(X_1, X_2)$ is modeled by an extreme value copula $C(p_1, p_2 \\mid Z=z)$ with a logistic stable tail dependence function $l(t_1,t_2 \\mid Z=z) = \\left(t_1^{1/\\alpha(z)} + t_2^{1/\\alpha(z)}\\right)^{\\alpha(z)}$, where $t_i = -\\log(p_i)$ and $p_i = F_i(x_i \\mid Z=z)$. The dependence parameter is $\\alpha(z) \\in (0,1]$.\n- **Copula Function**: $C(p_1,p_2 \\mid Z=z) = \\exp\\!\\left(-\\,l(-\\log p_1,-\\log p_2 \\mid Z=z)\\right)$.\n- **Target Quantity**: The compound exceedance probability $\\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z)$.\n- **Parameterization**:\n  - GEV: $\\mu_i(z) = \\mu_{i0} + \\mu_{i1}\\,z$, $\\sigma_i(z) = \\sigma_{i0}\\,\\exp(\\sigma_{i1}\\,z)$, $\\xi_i(z) = \\xi_{i0} + \\xi_{i1}\\,z$.\n  - Dependence: $\\alpha(z) = \\alpha_0 + \\alpha_1\\,z$.\n- **Attribution Task**: Decompose the total change $\\Delta = \\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z_2) - \\mathbb{P}(X_1>u_1, X_2>u_2 \\mid Z=z_1)$ into contributions from margins and dependence using a symmetric, path-independent averaging method.\n  - Path A (margins-then-dependence): Hybrid probability $\\mathbb{P}_{\\text{M}}$ uses margins from $z_2$ and dependence from $z_1$. Margins contribution is $\\mathbb{P}_{\\text{M}} - \\mathbb{P}(\\dots|z_1)$, dependence contribution is $\\mathbb{P}(\\dots|z_2) - \\mathbb{P}_{\\text{M}}$.\n  - Path B (dependence-then-margins): Hybrid probability $\\mathbb{P}_{\\text{D}}$ uses margins from $z_1$ and dependence from $z_2$. Dependence contribution is $\\mathbb{P}_{\\text{D}} - \\mathbb{P}(\\dots|z_1)$, margins contribution is $\\mathbb{P}(\\dots|z_2) - \\mathbb{P}_{\\text{D}}$.\n  - Final contributions are averages over paths A and B.\n- **Test Cases**: Three test cases with specific parameter values are provided.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The problem uses standard, well-established models from extreme value theory (GEV distribution) and copula theory (extreme value copulas, logistic model), which are widely applied in environmental and earth system modeling. The attribution method is a standard technique for sensitivity analysis. The problem does not violate any scientific or mathematical principles.\n- **Well-Posedness**: All necessary parameters, functional forms, and definitions are provided. The parameter values for the test cases are checked to ensure they fall within valid domains (e.g., $\\sigma > 0$, $\\alpha \\in (0,1]$, and thresholds within the GEV support), which they do. The task is clearly defined and leads to a unique, meaningful solution.\n- **Objectivity**: The language is precise, formal, and free of subjective claims. The problem is a formal mathematical exercise based on given models and data.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\nThe core of the problem is to calculate the compound exceedance probability $\\mathbb{P}(X_1 > u_1, X_2 > u_2 \\mid Z=z)$ and attribute its change between two scenarios, $Z=z_1$ and $Z=z_2$, to changes in marginal distributions versus changes in the dependence structure.\n\nFirst, we derive the expression for the compound exceedance probability. Let $p_1 = F_1(u_1 \\mid Z=z)$ and $p_2 = F_2(u_2 \\mid Z=z)$ be the marginal cumulative probabilities at the given thresholds $u_1$ and $u_2$. The joint cumulative distribution function is given by the copula: $\\mathbb{P}(X_1 \\le u_1, X_2 \\le u_2 \\mid Z=z) = C(p_1, p_2 \\mid Z=z)$. The probability of interest, $\\mathbb{P}(X_1 > u_1, X_2 > u_2 \\mid Z=z)$, refers to the joint survival function. Using the principle of inclusion-exclusion for probabilities, we relate the joint survival probability to the copula and marginal probabilities:\n$$\n\\mathbb{P}(X_1 > u_1, X_2 > u_2) = 1 - \\mathbb{P}(X_1 \\le u_1) - \\mathbb{P}(X_2 \\le u_2) + \\mathbb{P}(X_1 \\le u_1, X_2 \\le u_2)\n$$\nSubstituting the notation for the marginal and joint CDFs, we have:\n$$\nP_{exc}(u_1, u_2 \\mid z) = 1 - p_1 - p_2 + C(p_1, p_2 \\mid Z=z)\n$$\nThe problem specifies the GEV CDF, $F_i(x \\mid Z=z)$, for the margins, and an extreme value copula for the dependence. The parameters of these distributions, $\\mu_i(z)$, $\\sigma_i(z)$, $\\xi_i(z)$, and $\\alpha(z)$, are functions of the covariate $Z=z$. The GEV CDF $F_i(x \\mid z)$ is calculated as:\n$$\nF_i(x \\mid z) = \\begin{cases} \\exp\\left(-\\exp\\left(-\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right)\\right) & \\text{if } \\xi_i(z)=0 \\\\ \\exp\\left(-\\left[1+\\xi_i(z)\\frac{x-\\mu_i(z)}{\\sigma_i(z)}\\right]^{-1/\\xi_i(z)}\\right) & \\text{if } \\xi_i(z)\\neq0 \\end{cases}\n$$\nThe copula $C(p_1, p_2 \\mid Z=z)$ is constructed from the logistic stable tail dependence function $l(t_1, t_2 \\mid Z=z) = (t_1^{1/\\alpha(z)} + t_2^{1/\\alpha(z)})^{\\alpha(z)}$, where $t_i = -\\log p_i$. The copula is then:\n$$\nC(p_1, p_2 \\mid Z=z) = \\exp(-l(-\\log p_1, -\\log p_2 \\mid Z=z)) = \\exp\\left(-\\left((-\\log p_1)^{1/\\alpha(z)} + (-\\log p_2)^{1/\\alpha(z)}\\right)^{\\alpha(z)}\\right)\n$$\nWith these components, we can compute the compound exceedance probability for any given scenario $z$.\n\nLet the set of marginal parameters for both variables be denoted by $M(z) = \\{\\mu_1(z), \\sigma_1(z), \\xi_1(z), \\mu_2(z), \\sigma_2(z), \\xi_2(z)\\}$ and the dependence parameter by $D(z) = \\alpha(z)$. The exceedance probability can be written as a function of these parameter sets: $P(M(z), D(z))$. We consider two scenarios, $z_1$ and $z_2$.\nThe baseline probability is $P_1 = P(M(z_1), D(z_1))$.\nThe final probability is $P_2 = P(M(z_2), D(z_2))$.\nThe total change is $\\Delta = P_2 - P_1$.\n\nThe attribution analysis requires calculating probabilities for two hybrid scenarios:\n1. Margins from $z_2$ and dependence from $z_1$: $\\mathbb{P}_{\\text{M}} = P(M(z_2), D(z_1))$.\n2. Margins from $z_1$ and dependence from $z_2$: $\\mathbb{P}_{\\text{D}} = P(M(z_1), D(z_2))$.\n\nThe change is decomposed along two paths:\nPath A (margins, then dependence): The change is split into a part due to margins, $\\Delta_{M,A} = \\mathbb{P}_{\\text{M}} - P_1$, and a part due to dependence, $\\Delta_{D,A} = P_2 - \\mathbb{P}_{\\text{M}}$.\nPath B (dependence, then margins): The change is split into a part due to dependence, $\\Delta_{D,B} = \\mathbb{P}_{\\text{D}} - P_1$, and a part due to margins, $\\Delta_{M,B} = P_2 - \\mathbb{P}_{\\text{D}}$.\n\nThe final symmetric, path-independent contributions are the averages of the contributions from each path:\n- Margins contribution: $\\Delta_M = (\\Delta_{M,A} + \\Delta_{M,B}) / 2$.\n- Dependence contribution: $\\Delta_D = (\\Delta_{D,A} + \\Delta_{D,B}) / 2$.\nBy construction, $\\Delta_M + \\Delta_D = \\Delta$.\n\nThe solution involves implementing functions to compute the GEV parameters, the GEV CDF, the copula, and the compound exceedance probability. These functions are then orchestrated to calculate $P_1$, $P_2$, $\\mathbb{P}_{\\text{M}}$, and $\\mathbb{P}_{\\text{D}}$, from which the final attribution components are derived for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the compound event attribution problem for the provided test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"z1\": 0.0, \"z2\": 1.0, \"u1\": 70.0, \"u2\": 5.5,\n            \"mu10\": 30.0, \"mu11\": 2.0, \"sigma10\": 10.0, \"sigma11\": 0.1, \"xi10\": 0.1, \"xi11\": 0.0,\n            \"mu20\": 1.5, \"mu21\": 0.2, \"sigma20\": 0.5, \"sigma21\": 0.05, \"xi20\": -0.1, \"xi21\": 0.0,\n            \"alpha0\": 0.6, \"alpha1\": -0.1\n        },\n        # Case 2 (independence boundary)\n        {\n            \"z1\": 0.0, \"z2\": 2.0, \"u1\": 2.0, \"u2\": 3.0,\n            \"mu10\": 0.0, \"mu11\": 0.0, \"sigma10\": 1.0, \"sigma11\": 0.0, \"xi10\": 0.0, \"xi11\": 0.0,\n            \"mu20\": 0.0, \"mu21\": 1.0, \"sigma20\": 1.0, \"sigma21\": 0.2, \"xi20\": 0.0, \"xi21\": 0.0,\n            \"alpha0\": 1.0, \"alpha1\": 0.0\n        },\n        # Case 3 (dependence-only change)\n        {\n            \"z1\": 0.0, \"z2\": 1.0, \"u1\": 20.0, \"u2\": 8.0,\n            \"mu10\": 10.0, \"mu11\": 0.0, \"sigma10\": 2.0, \"sigma11\": 0.0, \"xi10\": 0.05, \"xi11\": 0.0,\n            \"mu20\": 5.0, \"mu21\": 0.0, \"sigma20\": 1.0, \"sigma21\": 0.0, \"xi20\": 0.0, \"xi21\": 0.0,\n            \"alpha0\": 0.9, \"alpha1\": -0.6\n        }\n    ]\n\n    def get_gev_cdf(x, mu, sigma, xi):\n        \"\"\"Computes the GEV cumulative distribution function.\"\"\"\n        if sigma = 0:\n            raise ValueError(\"Scale parameter sigma must be positive.\")\n        \n        # Handle the Gumbel case (xi = 0)\n        if xi == 0.0:\n            return np.exp(-np.exp(-(x - mu) / sigma))\n\n        # Handle the Frechet/Weibull cases (xi != 0)\n        support_term = 1 + xi * (x - mu) / sigma\n        if support_term = 0:\n            # If x is outside the support of the distribution\n            if xi > 0: # Lower bound at mu - sigma/xi\n                return 0.0\n            else: # Upper bound at mu - sigma/xi\n                return 1.0\n        \n        return np.exp(-np.power(support_term, -1.0 / xi))\n\n    def get_compound_exceedance_prob(u1, u2, m1_params, m2_params, alpha):\n        \"\"\"\n        Computes the compound exceedance probability P(X1 > u1, X2 > u2).\n        \"\"\"\n        mu1, sigma1, xi1 = m1_params\n        mu2, sigma2, xi2 = m2_params\n\n        p1 = get_gev_cdf(u1, mu1, sigma1, xi1)\n        p2 = get_gev_cdf(u2, mu2, sigma2, xi2)\n\n        # Avoid log(0) for probabilities at the boundary\n        if p1 == 0.0 or p2 == 0.0:\n            return 0.0\n        if p1 == 1.0:\n            return 1.0 - p2 # S_2\n        if p2 == 1.0:\n            return 1.0 - p1 # S_1\n\n        t1 = -np.log(p1)\n        t2 = -np.log(p2)\n        \n        l = np.power(np.power(t1, 1.0/alpha) + np.power(t2, 1.0/alpha), alpha)\n        copula_val = np.exp(-l)\n        \n        prob = 1.0 - p1 - p2 + copula_val\n        return prob\n\n    results = []\n    for case in test_cases:\n        # Define functions to get parameters based on covariate z\n        def get_params_at_z(z):\n            mu1 = case[\"mu10\"] + case[\"mu11\"] * z\n            sigma1 = case[\"sigma10\"] * np.exp(case[\"sigma11\"] * z)\n            xi1 = case[\"xi10\"] + case[\"xi11\"] * z\n            m1_params = (mu1, sigma1, xi1)\n\n            mu2 = case[\"mu20\"] + case[\"mu21\"] * z\n            sigma2 = case[\"sigma20\"] * np.exp(case[\"sigma21\"] * z)\n            xi2 = case[\"xi20\"] + case[\"xi21\"] * z\n            m2_params = (mu2, sigma2, xi2)\n\n            alpha = case[\"alpha0\"] + case[\"alpha1\"] * z\n            \n            return m1_params, m2_params, alpha\n\n        u1, u2 = case[\"u1\"], case[\"u2\"]\n        z1, z2 = case[\"z1\"], case[\"z2\"]\n\n        # Get parameter sets for the two scenarios\n        m1_params_z1, m2_params_z1, alpha_z1 = get_params_at_z(z1)\n        m1_params_z2, m2_params_z2, alpha_z2 = get_params_at_z(z2)\n\n        # Calculate probabilities for the four scenarios (P1, P2, PM, PD)\n        # P1: Margins and dependence from z1\n        p1 = get_compound_exceedance_prob(u1, u2, m1_params_z1, m2_params_z1, alpha_z1)\n        \n        # P2: Margins and dependence from z2\n        p2 = get_compound_exceedance_prob(u1, u2, m1_params_z2, m2_params_z2, alpha_z2)\n        \n        # PM: Margins from z2, dependence from z1\n        p_m = get_compound_exceedance_prob(u1, u2, m1_params_z2, m2_params_z2, alpha_z1)\n        \n        # PD: Margins from z1, dependence from z2\n        p_d = get_compound_exceedance_prob(u1, u2, m1_params_z1, m2_params_z1, alpha_z2)\n\n        # Total change\n        delta = p2 - p1\n\n        # Path A contributions\n        delta_m_a = p_m - p1\n        delta_d_a = p2 - p_m\n\n        # Path B contributions\n        delta_d_b = p_d - p1\n        delta_m_b = p2 - p_d\n\n        # Averaged contributions\n        margins_contribution = (delta_m_a + delta_m_b) / 2.0\n        dependence_contribution = (delta_d_a + delta_d_b) / 2.0\n        \n        case_results = [p1, p2, delta, margins_contribution, dependence_contribution]\n        results.append(f\"[{','.join(map(str, case_results))}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}