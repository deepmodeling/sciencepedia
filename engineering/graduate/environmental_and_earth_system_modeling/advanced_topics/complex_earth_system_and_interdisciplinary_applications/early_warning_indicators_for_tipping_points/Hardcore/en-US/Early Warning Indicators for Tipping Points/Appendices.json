{
    "hands_on_practices": [
        {
            "introduction": "The theory of early warning indicators is rooted in the mathematical field of dynamical systems. This exercise explores the canonical model for a tipping point, the saddle-node bifurcation, to build intuition from first principles. By analyzing this simple, one-dimensional system, you will see precisely how the loss of a stable state—the tipping point itself—is preceded by a 'critical slowing down,' and how this slowing manifests as an increase in the system's autocorrelation, a key statistical signature. ",
            "id": "3876409",
            "problem": "Consider the canonical saddle-node (fold) normal form from dynamical systems theory applied to environmental tipping models,\n$$\\frac{dx}{dt}=\\mu - x^{2},$$\nwhere $x$ is a scalar system state and $\\mu$ is a slowly varying control parameter representing external forcing. This one-dimensional deterministic system is widely used as an archetype for abrupt transitions in Earth system components when a stable state disappears under parameter change.\n\n(a) Using fundamental linear stability analysis, identify all equilibria of the deterministic system as a function of $\\mu$ and determine their stability.\n\n(b) Suppose $\\mu(t)$ varies slowly and monotonically in time due to external forcing, with $\\mu(t)=\\mu_{0}-\\varepsilon t$, where $\\mu_{0}0$ and $0\\varepsilon\\ll 1$ is a small ramp rate. Starting from the stable equilibrium at $t\\ll 0$, explain, from first principles, why a hysteresis-like delayed loss of stability (dynamic bifurcation delay) occurs as $\\mu(t)$ passes through $0$ and why this is relevant to early warning indicators in environmental and Earth system modeling.\n\n(c) To connect with early warning indicators, include weak environmental noise and consider the Stochastic Differential Equation (SDE)\n$$dx=\\left(\\mu - x^{2}\\right)dt+\\sigma\\,dW_{t},$$\nwhere $\\sigma0$ is the noise amplitude and $W_{t}$ is a standard Wiener process (Brownian motion), so that $dW_{t}$ represents Gaussian white noise with independent stationary increments. For fixed $\\mu0$, linearize this SDE about the stable equilibrium $x^{\\star}(\\mu)$ and obtain the stationary lag-$\\Delta t$ autocorrelation of the fluctuations, defined as\n$$\\rho(\\Delta t)=\\frac{\\mathbb{E}\\left[(x(t)-\\mathbb{E}x(t))\\,(x(t+\\Delta t)-\\mathbb{E}x(t))\\right]}{\\operatorname{Var}(x(t))},$$\nunder the linearized dynamics. Express the final answer as an exact analytic expression in terms of $\\mu$ and $\\Delta t$. No rounding is required. The autocorrelation is dimensionless; do not include units.",
            "solution": "The problem is scientifically grounded, well-posed, and all necessary information is provided. It represents a standard analysis of a canonical model for tipping points. Therefore, the problem is deemed valid.\n\n(a) To identify the equilibria of the system $\\frac{dx}{dt}=\\mu - x^{2}$, we set the rate of change to zero:\n$$\n\\frac{dx}{dt} = 0 \\implies \\mu - x^{2} = 0\n$$\nThis gives $x^{2} = \\mu$. Let the equilibrium points be denoted by $x^{\\star}$.\n\n-   If $\\mu  0$, there are two distinct real equilibria: $x^{\\star}_{1} = \\sqrt{\\mu}$ and $x^{\\star}_{2} = -\\sqrt{\\mu}$.\n-   If $\\mu = 0$, there is a single equilibrium at $x^{\\star} = 0$.\n-   If $\\mu  0$, there are no real equilibria, as $x^{2}$ cannot be negative for real $x$.\n\nTo determine the stability of these equilibria, we use linear stability analysis. Let $f(x) = \\mu - x^{2}$. The stability is determined by the sign of the eigenvalue $\\lambda = \\frac{df}{dx}$ evaluated at the equilibrium point $x^{\\star}$. The derivative is:\n$$\n\\frac{df}{dx} = -2x\n$$\nWe evaluate this at each equilibrium for the case $\\mu0$:\n\n-   For the equilibrium $x^{\\star}_{1} = \\sqrt{\\mu}$:\n    $$\n    \\lambda_{1} = \\left.\\frac{df}{dx}\\right|_{x=\\sqrt{\\mu}} = -2\\sqrt{\\mu}\n    $$\n    Since $\\mu  0$, $\\sqrt{\\mu}$ is real and positive, so $\\lambda_{1}  0$. An equilibrium with a negative real eigenvalue is stable. Thus, $x^{\\star}_{1} = \\sqrt{\\mu}$ is a stable equilibrium.\n\n-   For the equilibrium $x^{\\star}_{2} = -\\sqrt{\\mu}$:\n    $$\n    \\lambda_{2} = \\left.\\frac{df}{dx}\\right|_{x=-\\sqrt{\\mu}} = -2(-\\sqrt{\\mu}) = 2\\sqrt{\\mu}\n    $$\n    Since $\\mu  0$, $\\lambda_{2}  0$. An equilibrium with a positive real eigenvalue is unstable. Thus, $x^{\\star}_{2} = -\\sqrt{\\mu}$ is an unstable equilibrium.\n\nAt $\\mu=0$, the two equilibria merge. Here, $\\lambda = -2(0) = 0$. This is a non-hyperbolic equilibrium, and linear stability analysis is inconclusive. This point, $\\mu=0$, is the location of a saddle-node (or fold) bifurcation, where the stable and unstable equilibria collide and annihilate.\n\n(b) Hysteresis-like delayed loss of stability, also known as dynamic bifurcation delay, occurs when a control parameter changes at a finite rate through a bifurcation point.\n\nConsider the system starting at $t \\ll 0$ in the stable equilibrium $x(t) = \\sqrt{\\mu(t)} = \\sqrt{\\mu_{0}-\\varepsilon t}$ for $\\mu(t)0$. As time $t$ increases, the parameter $\\mu(t)$ slowly decreases. At $t_{c} = \\mu_{0}/\\varepsilon$, the parameter $\\mu$ crosses zero. At this exact moment, the static bifurcation occurs: the stable equilibrium at $x^{\\star}=0$ ceases to exist. For any $t  t_{c}$, $\\mu(t)$ becomes negative. In this regime, the governing equation is $\\frac{dx}{dt} = \\mu(t) - x^{2}$. Since $\\mu(t)  0$ and $-x^{2} \\le 0$, the right-hand side is strictly negative for all $x$. This means $\\frac{dx}{dt}  0$, and the state $x(t)$ must decrease indefinitely (in this simple model, towards $-\\infty$).\n\nThe delay arises because the rate of change is not instantaneous. Just after the bifurcation at $t_{c}$, the state $x(t)$ is close to $0$, and $\\mu(t)$ is a small negative number. For example, at a time $t = t_{c} + \\delta t$ where $\\delta t$ is small and positive, $\\mu(t) = -\\varepsilon \\delta t$. The equation of motion is $\\frac{dx}{dt} = -\\varepsilon\\delta t - x^{2}$. Since $x$ is near $0$, $\\frac{dx}{dt}$ is a small negative number. The state $x(t)$ therefore begins to drift away from $x=0$ very slowly. It does not \"jump\" to a new state. This lingering of the trajectory near the \"ghost\" of the now-vanished equilibrium is the dynamic bifurcation delay. The system's state lags behind the change in the underlying stability landscape. The smaller the ramp rate $\\varepsilon$, the slower the drift and the longer the delay.\n\nThis phenomenon is critically relevant to early warning indicators (EWS). EWS, such as rising variance or autocorrelation, are based on \"critical slowing down\"—the fact that the system's recovery rate from perturbations slows as it approaches a bifurcation. The recovery rate is given by $|\\lambda_{1}| = 2\\sqrt{\\mu}$, which approaches $0$ as $\\mu \\to 0^{+}$. The dynamic bifurcation delay is a direct consequence of this critical slowing down. The system becomes so sluggish that it cannot keep up with the changing environment (the changing $\\mu$). The danger this reveals is that the actual tipping point ($\\mu=0$) can be crossed before any dramatic, observable shift in the system's state occurs. An observer might believe the system is still in a safe regime, when in fact the underlying stability has already been lost, and a large, irreversible transition is merely delayed, but now inevitable.\n\n(c) We are given the Stochastic Differential Equation (SDE):\n$$\ndx = (\\mu - x^{2})dt + \\sigma dW_{t}\n$$\nFor a fixed $\\mu  0$, the stable equilibrium is $x^{\\star} = \\sqrt{\\mu}$. We linearize the system around this equilibrium by considering small fluctuations $y(t) = x(t) - x^{\\star}$. This also means $dx = dy$. The drift term $f(x) = \\mu - x^{2}$ is approximated using a first-order Taylor expansion around $x^{\\star}$:\n$$\nf(x) \\approx f(x^{\\star}) + f'(x^{\\star})(x - x^{\\star})\n$$\nWe know $f(x^{\\star}) = \\mu - (x^{\\star})^{2} = \\mu - \\mu = 0$. The derivative is $f'(x) = -2x$, so $f'(x^{\\star}) = -2x^{\\star} = -2\\sqrt{\\mu}$. Substituting these into the expansion gives:\n$$\nf(x) \\approx 0 + (-2\\sqrt{\\mu})y(t) = -2\\sqrt{\\mu}y(t)\n$$\nThe linearized SDE for the fluctuations $y(t)$ is therefore:\n$$\ndy = -2\\sqrt{\\mu} y dt + \\sigma dW_{t}\n$$\nThis is the equation for an Ornstein-Uhlenbeck (OU) process of the general form $dy = -\\theta y dt + \\sigma dW_{t}$, with a relaxation rate $\\theta = 2\\sqrt{\\mu}$.\n\nThe stationary solution for an OU process is a Gaussian process with zero mean, $\\mathbb{E}[y(t)]=0$. This implies $\\mathbb{E}[x(t)] = x^{\\star}$. The fluctuations are defined as deviations from the mean, so $x(t) - \\mathbb{E}[x(t)] = x(t) - x^{\\star} = y(t)$.\n\nThe stationary lag-$\\Delta t$ autocorrelation function $\\rho(\\Delta t)$ is defined as:\n$$\n\\rho(\\Delta t) = \\frac{\\mathbb{E}\\left[(x(t)-\\mathbb{E}x(t))\\,(x(t+\\Delta t)-\\mathbb{E}x(t))\\right]}{\\operatorname{Var}(x(t))} = \\frac{\\mathbb{E}[y(t)y(t+\\Delta t)]}{\\operatorname{Var}(y(t))}\n$$\nThe numerator is the autocovariance of $y(t)$. For a stationary OU process, the autocovariance function is given by $\\operatorname{Cov}(y(t), y(t+\\Delta t)) = \\operatorname{Var}(y(t))\\exp(-\\theta |\\Delta t|)$. Assuming $\\Delta t \\ge 0$, this is $\\operatorname{Var}(y(t))\\exp(-\\theta \\Delta t)$.\n\nSubstituting this into the expression for $\\rho(\\Delta t)$:\n$$\n\\rho(\\Delta t) = \\frac{\\operatorname{Var}(y(t))\\exp(-\\theta \\Delta t)}{\\operatorname{Var}(y(t))} = \\exp(-\\theta \\Delta t)\n$$\nFinally, we substitute the value of $\\theta = 2\\sqrt{\\mu}$ back into the expression:\n$$\n\\rho(\\Delta t) = \\exp(-2\\sqrt{\\mu} \\Delta t)\n$$\nThis expression shows that as the system approaches the tipping point ($\\mu \\to 0^{+}$), the exponent approaches $0$, and the autocorrelation $\\rho(\\Delta t)$ approaches $1$ for any fixed lag $\\Delta t$. This increasing autocorrelation is a key early warning signal.",
            "answer": "$$\\boxed{\\exp(-2\\sqrt{\\mu}\\Delta t)}$$"
        },
        {
            "introduction": "Moving from theory to application, this practice focuses on the computational implementation of early warning indicators. The theoretical concept of 'critical slowing down' predicts that both the variance and autocorrelation of a system's fluctuations should increase as it approaches a tipping point. This exercise guides you through writing a program to detect these signals in a time series by calculating these statistics within a moving window, a foundational technique in EWS analysis. ",
            "id": "3876395",
            "problem": "You are given detrended scalar time series data that is hypothesized to approach a bifurcation-induced regime shift. An early warning indicator is the progressive increase in lag-$1$ autocorrelation and variance, which can be assessed via a moving-window fit of an autoregressive order-$1$ model and inference on indicator trends. Starting from the definition of an Autoregressive (AR) model, consider the AR order-$1$ process defined by $x_t = \\phi x_{t-1} + \\epsilon_t$ with $t \\in \\{1,2,\\ldots,N-1\\}$, where $x_t$ is the detrended scalar state, $\\phi$ is the AR coefficient, and $\\epsilon_t$ is a zero-mean noise sequence. Assume the noise is independent and identically distributed and the time series is mean-zero after detrending. Using Ordinary Least Squares (OLS) as the principle for parameter estimation, and unbiased sample variance within each window as the measure of variability, your task is to implement a complete program that:\n\n- Segments the series into overlapping windows of fixed length, advancing by a fixed step, and for each window:\n  - Computes the AR order-$1$ coefficient $\\phi$ by minimizing the sum of squared one-step prediction errors on the window segment.\n  - Computes the unbiased sample variance within the window segment.\n- Defines the window time coordinate as the center index of each window and computes the linear trend slope of the windowed estimates of $\\phi$ versus window center time, and the linear trend slope of the windowed variance versus window center time, using a least-squares fit.\n- Returns, for each test case, a list containing two floating-point numbers: the slope of $\\phi$ and the slope of the variance across window centers.\n\nFundamental base and assumptions:\n- The Autoregressive (AR) definition: $x_t = \\phi x_{t-1} + \\epsilon_t$ for a detrended series with zero intercept, where $\\epsilon_t$ is zero-mean noise.\n- Ordinary Least Squares (OLS): The parameter estimate minimizes $\\sum_t (x_t - \\phi x_{t-1})^2$ over the window.\n- Unbiased sample variance: For a window of length $W$, $\\mathrm{Var}(x) = \\frac{1}{W-1} \\sum_{i=1}^W (x_i - \\bar{x})^2$.\n- Linear trend slope: For paired data $(t_i, y_i)$, the OLS slope is the coefficient multiplying $t$ in the best linear fit $y = a + b t$, equivalently $b = \\frac{\\sum_i (t_i - \\bar{t})(y_i - \\bar{y})}{\\sum_i (t_i - \\bar{t})^2}$.\n\nRequirements:\n- Implement the procedure exactly as described.\n- Use linear slope as the quantification of trend for both $\\phi$ and variance.\n- If a window yields a degenerate AR fit (for example, if $\\sum x_{t-1}^2 = 0$ within the window), define the window’s $\\phi$ estimate to be $0$.\n- The input time series are internally generated by the program using reproducible pseudorandom noise with specified seeds.\n- No physical units are involved; all outputs are dimensionless real numbers.\n- Angles are not used.\n- Express all outputs as raw floating-point numbers with no rounding requirement beyond standard double precision.\n\nTest suite:\nImplement four test cases with the following parameters. In all cases, set the initial state to $x_0 = 0$ and use independent Gaussian noise $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ with standard deviation $\\sigma$. When $\\phi$ varies over time, set $\\phi_t = \\phi_{\\mathrm{start}} + (\\phi_{\\mathrm{end}} - \\phi_{\\mathrm{start}})\\cdot \\frac{t}{N-1}$ for $t \\in \\{0,1,\\ldots,N-1\\}$, and simulate $x_t = \\phi_t x_{t-1} + \\epsilon_t$.\n\n- Case $1$ (stable baseline):\n  - Series length $N = 500$\n  - Constant $\\phi = 0.2$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 10$\n  - Random seed $s = 42$\n- Case $2$ (approaching tipping with increasing persistence):\n  - Series length $N = 500$\n  - Linearly increasing $\\phi$ from $\\phi_{\\mathrm{start}} = 0.2$ to $\\phi_{\\mathrm{end}} = 0.95$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 10$\n  - Random seed $s = 43$\n- Case $3$ (boundary condition with few windows):\n  - Series length $N = 60$\n  - Linearly increasing $\\phi$ from $\\phi_{\\mathrm{start}} = 0.5$ to $\\phi_{\\mathrm{end}} = 0.9$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 5$\n  - Random seed $s = 44$\n- Case $4$ (near white noise):\n  - Series length $N = 500$\n  - Constant $\\phi = 0.0$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 10$\n  - Random seed $s = 45$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list of two floating-point numbers, ordered as $[\\text{slope of } \\phi, \\text{slope of variance}]$. For example, the output format must be exactly like $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$, where each $a_i$ and $b_i$ are floating-point numbers.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique, verifiable solution. We will proceed with a step-by-step derivation and implementation of the specified procedure. The objective is to compute trend indicators for a potential bifurcation-induced regime shift in a scalar time series by analyzing the behavior of the lag-$1$ autocorrelation coefficient and the variance within a moving window.\n\nThe core of the analysis rests upon an autoregressive model of order-$1$ ($\\text{AR}(1)$). For a detrended, zero-mean time series $x_t$, the model is defined as:\n$$x_t = \\phi x_{t-1} + \\epsilon_t$$\nwhere $t \\in \\{1, 2, \\ldots, N-1\\}$ is the time index, $\\phi$ is the lag-$1$ autoregressive coefficient, and $\\epsilon_t$ is a sequence of independent and identically distributed zero-mean noise terms.\n\nThe analysis consists of three main stages: segmenting the time series into windows; estimating $\\phi$ and the variance within each window; and quantifying the temporal trend of these estimates.\n\n**1. Time Series Segmentation and Windowing**\n\nThe given time series, of total length $N$, is segmented into overlapping windows of a fixed length $W$. The windows advance along the series with a fixed step size $S$. The starting index of the first window is $0$. Subsequent windows start at indices $S, 2S, 3S, \\ldots$. The process continues as long as the window fits entirely within the series. Thus, the starting index $i$ of any window must satisfy the condition $i + W \\le N$. The sequence of starting indices is therefore $i_k = k \\cdot S$ for $k = 0, 1, 2, \\ldots$ such that $kS \\le N - W$. For each window, we will compute two statistics: the $\\text{AR}(1)$ coefficient and the unbiased sample variance.\n\n**2. Per-Window Parameter Estimation**\n\nLet us consider a single window of data points $\\{x_i, x_{i+1}, \\ldots, x_{i+W-1}\\}$.\n\n**2.1. AR(1) Coefficient ($\\phi$) Estimation**\n\nThe coefficient $\\phi$ is estimated using the Ordinary Least Squares (OLS) principle. We seek the value of $\\phi$ that minimizes the sum of squared residuals, $L(\\phi)$, for the one-step predictions within the window. The data available for fitting the model $x_t = \\phi x_{t-1} + \\epsilon_t$ within this window consists of $W-1$ pairs of the form $(x_{t-1}, x_t)$, for $t$ ranging from $i+1$ to $i+W-1$.\n\nThe sum of squared residuals is:\n$$L(\\phi) = \\sum_{t=i+1}^{i+W-1} (x_t - \\phi x_{t-1})^2$$\nTo find the minimum, we compute the derivative of $L(\\phi)$ with respect to $\\phi$ and set it to zero:\n$$\\frac{d L}{d \\phi} = \\frac{d}{d \\phi} \\sum_{t=i+1}^{i+W-1} (x_t^2 - 2\\phi x_t x_{t-1} + \\phi^2 x_{t-1}^2) = \\sum_{t=i+1}^{i+W-1} (-2 x_t x_{t-1} + 2\\phi x_{t-1}^2) = 0$$\nSolving for $\\phi$:\n$$2 \\phi \\sum_{t=i+1}^{i+W-1} x_{t-1}^2 = 2 \\sum_{t=i+1}^{i+W-1} x_t x_{t-1}$$\nThis yields the OLS estimate $\\hat{\\phi}$ for the window:\n$$\\hat{\\phi} = \\frac{\\sum_{t=i+1}^{i+W-1} x_t x_{t-1}}{\\sum_{t=i+1}^{i+W-1} x_{t-1}^2}$$\nAs specified, if the denominator $\\sum x_{t-1}^2$ is zero, which indicates no variation in the regressor within the window segment used for fitting, the estimate $\\hat{\\phi}$ is defined to be $0$.\n\n**2.2. Unbiased Sample Variance Estimation**\n\nThe unbiased sample variance for the data points $\\{x_i, x_{i+1}, \\ldots, x_{i+W-1}\\}$ within the window is calculated. First, the sample mean, $\\bar{x}_{\\text{win}}$, of the $W$ data points is computed:\n$$\\bar{x}_{\\text{win}} = \\frac{1}{W} \\sum_{j=i}^{i+W-1} x_j$$\nThe unbiased sample variance, $\\widehat{\\mathrm{Var}}(x)$, is then given by:\n$$\\widehat{\\mathrm{Var}}(x) = \\frac{1}{W-1} \\sum_{j=i}^{i+W-1} (x_j - \\bar{x}_{\\text{win}})^2$$\nThe denominator $W-1$ reflects Bessel's correction for an unbiased estimate of the population variance from a sample.\n\n**3. Trend Analysis of Indicator Time Series**\n\nAfter processing all windows, we obtain two new time series: one for the estimates of $\\phi$ and one for the estimates of variance. To analyze their trends, we first assign a time coordinate to each estimate. The problem specifies using the center index of the window as its time coordinate. For a window starting at index $i$ with length $W$, the time coordinate $t_{\\text{center}}$ is:\n$$t_{\\text{center}} = i + \\frac{W-1}{2}$$\nLet the series of window time centers be $\\{t_1, t_2, \\ldots, t_M\\}$ and the corresponding indicator estimates (either $\\hat{\\phi}$ or $\\widehat{\\mathrm{Var}}$) be $\\{y_1, y_2, \\ldots, y_M\\}$, where $M$ is the total number of windows.\n\nTo quantify the trend, we perform a linear regression of the indicator $y$ on time $t$, fitting the model $y_k = a + b t_k$. The slope $b$ represents the linear trend. According to OLS, the formula for the slope is:\n$$b = \\frac{\\sum_{k=1}^M (t_k - \\bar{t})(y_k - \\bar{y})}{\\sum_{k=1}^M (t_k - \\bar{t})^2}$$\nwhere $\\bar{t} = \\frac{1}{M}\\sum_{k=1}^M t_k$ and $\\bar{y} = \\frac{1}{M}\\sum_{k=1}^M y_k$ are the sample means of the time coordinates and the indicator values, respectively. This formula is equivalent to the ratio of the sample covariance of $t$ and $y$ to the sample variance of $t$. This slope $b$ is computed for both the $\\hat{\\phi}$ series and the $\\widehat{\\mathrm{Var}}(x)$ series.\n\nThis complete procedure provides the two required outputs for each test case: the slope of the lag-$1$ autocorrelation coefficient and the slope of the variance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_slope(t, y):\n    \"\"\"\n    Computes the slope of a linear regression of y on t using OLS.\n    \"\"\"\n    n = len(t)\n    if n  2:\n        return 0.0\n    \n    t_mean = np.mean(t)\n    y_mean = np.mean(y)\n    \n    numerator = np.sum((t - t_mean) * (y - y_mean))\n    denominator = np.sum((t - t_mean)**2)\n    \n    if denominator == 0:\n        return 0.0\n        \n    return numerator / denominator\n\ndef analyze_time_series(N, phi_params, sigma, W, S, seed):\n    \"\"\"\n    Generates an AR(1) time series and computes the slopes of EWI trends.\n    \n    Args:\n        N (int): Series length.\n        phi_params (float or tuple): Constant phi or (start, end) for varying phi.\n        sigma (float): Noise standard deviation.\n        W (int): Window length.\n        S (int): Step size.\n        seed (int): Random seed.\n\n    Returns:\n        list: A list containing [slope_phi, slope_var].\n    \"\"\"\n    # 1. Generate the time series\n    rng = np.random.default_rng(seed)\n    x = np.zeros(N)\n    noise = rng.normal(loc=0.0, scale=sigma, size=N)\n    \n    if isinstance(phi_params, tuple):\n        phi_start, phi_end = phi_params\n        # phi_t is used to evolve x from t-1 to t\n        phi_vec = np.linspace(phi_start, phi_end, N)\n    else:\n        phi_vec = np.full(N, phi_params)\n\n    # Initial state x_0 = 0 is set by np.zeros\n    for t in range(1, N):\n        x[t] = phi_vec[t] * x[t-1] + noise[t]\n\n    # 2. Compute windowed statistics\n    t_centers = []\n    phi_estimates = []\n    var_estimates = []\n\n    for i in range(0, N - W + 1, S):\n        window = x[i : i + W]\n        \n        # Estimate phi using OLS for x_t = phi * x_{t-1}\n        # The regressors are x_0, ..., x_{W-2} and responses x_1, ..., x_{W-1}\n        # within the window.\n        X_reg = window[:-1]\n        y_reg = window[1:]\n        \n        sum_X_sq = np.dot(X_reg, X_reg)\n        if sum_X_sq == 0:\n            phi_hat = 0.0\n        else:\n            phi_hat = np.dot(X_reg, y_reg) / sum_X_sq\n        \n        # Estimate unbiased sample variance for the entire window\n        var_hat = np.var(window, ddof=1)\n        \n        # Define window time coordinate as the center index\n        t_center = i + (W - 1) / 2.0\n        \n        phi_estimates.append(phi_hat)\n        var_estimates.append(var_hat)\n        t_centers.append(t_center)\n\n    # 3. Calculate trend slopes\n    t_centers_arr = np.array(t_centers)\n    phi_estimates_arr = np.array(phi_estimates)\n    var_estimates_arr = np.array(var_estimates)\n\n    slope_phi = calculate_slope(t_centers_arr, phi_estimates_arr)\n    slope_var = calculate_slope(t_centers_arr, var_estimates_arr)\n    \n    return [slope_phi, slope_var]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (stable baseline)\n        {'N': 500, 'phi_params': 0.2, 'sigma': 1.0, 'W': 50, 'S': 10, 'seed': 42},\n        # Case 2 (approaching tipping)\n        {'N': 500, 'phi_params': (0.2, 0.95), 'sigma': 1.0, 'W': 50, 'S': 10, 'seed': 43},\n        # Case 3 (boundary condition)\n        {'N': 60, 'phi_params': (0.5, 0.9), 'sigma': 1.0, 'W': 50, 'S': 5, 'seed': 44},\n        # Case 4 (near white noise)\n        {'N': 500, 'phi_params': 0.0, 'sigma': 1.0, 'W': 50, 'S': 10, 'seed': 45},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_time_series(\n            N=case['N'],\n            phi_params=case['phi_params'],\n            sigma=case['sigma'],\n            W=case['W'],\n            S=case['S'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Format the output string exactly as required\n    inner_parts = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A crucial challenge in applying early warning indicators to real-world data is the risk of false positives, where an indicator rises for reasons unrelated to an approaching bifurcation. This advanced practice tackles this problem head-on by developing a robust diagnostic test to distinguish a genuine signal from a spurious one caused by underlying trends. By implementing a sophisticated pipeline involving wavelet detrending and residual analysis, you will learn to build more reliable and credible early warning systems. ",
            "id": "3876387",
            "problem": "You are tasked with designing and implementing a rigorous statistical test to rule out spurious increases in lag-$1$ autocorrelation that can arise from smoother trend components in environmental time series, a key challenge when computing Early Warning Indicators (EWI) for tipping points in environmental and earth system modeling. The goal is to combine wavelet-based detrending with residual diagnostics to determine whether an observed increase in autocorrelation is genuine (consistent with critical slowing down) or spurious (caused by smooth trends contaminating autocorrelation estimates).\n\nThe fundamental base for this problem comprises: the definition of lag-$k$ autocorrelation for a discrete-time process, the decomposition $x_t = s_t + e_t$ into a smooth trend component $s_t$ and a residual component $e_t$, the stationary wavelet transform smoothing via the \"à trous\" algorithm using a degree-$3$ B-spline scaling filter, Kendall's $\\tau$ rank correlation to assess monotonic trends across windows, and the Ljung–Box (LB) portmanteau test to assess residual whiteness. These are well-established components for formal early warning system diagnostics, when correctly integrated.\n\nYour program must implement the following, starting from the provided fundamental base and definitions, and must not assume shortcut formulas:\n\n- Model of autocorrelation: For a discrete-time series $\\{x_t\\}_{t=1}^n$, the lag-$k$ sample autocorrelation is\n$$\n\\hat{\\rho}_k = \\frac{\\sum_{t=k+1}^{n} (x_t - \\bar{x})(x_{t-k} - \\bar{x})}{\\sum_{t=1}^{n} (x_t - \\bar{x})^2},\n$$\nwhere $\\bar{x}$ is the sample mean. The lag-$1$ autocorrelation $\\hat{\\rho}_1$ is the indicator of interest.\n\n- Trend-plus-residual decomposition: Assume $x_t = s_t + e_t$, where $s_t$ is a smooth component and $e_t$ is a stationary residual. A smooth $s_t$ can inflate $\\hat{\\rho}_1$ even if $e_t$ is white noise. To mitigate this, detrend $x_t$ using wavelet-based smoothing.\n\n- Wavelet-based detrending (stationary wavelet transform via the \"à trous\" algorithm): Use the degree-$3$ B-spline scaling filter with coefficients $h = [1,4,6,4,1]/16$. For scale $j \\in \\{1,\\dots,J\\}$, construct a dilated filter by inserting $2^{j-1}-1$ zeros (\"holes\") between the coefficients of $h$, and convolve the current smooth approximation $c_{j-1}$ with this dilated filter (using symmetric boundary handling) to obtain $c_j$. Set $c_0 = x$ and take the final smooth component $c_J$ as the trend estimate. The residual is $r = x - c_J$.\n\n- Moving-window diagnostics: Partition $x_t$ into overlapping windows of length $W$ and step $S$, indexed by increasing time. For each window, compute $\\hat{\\rho}_1$ for both the original series and the residual series. Assess monotonic increase of windowed $\\hat{\\rho}_1$ values across time using Kendall's $\\tau$ rank correlation test, i.e., compute $\\tau$ and its two-sided $p$-value for the association between window index and the sequence of windowed lag-$1$ autocorrelations.\n\n- Residual whiteness diagnostics: For each residual window, compute the Ljung–Box statistic\n$$\nQ = n(n+2)\\sum_{k=1}^{h} \\frac{\\hat{\\rho}_k^2}{n-k},\n$$\nwhere $n$ is the window length and $h$ is the maximum lag tested. Under the null hypothesis of white noise, $Q$ is approximately chi-square distributed with $h$ degrees of freedom. Compute the $p$-value $p_{\\mathrm{LB}} = 1 - F_{\\chi^2(h)}(Q)$. A small $p_{\\mathrm{LB}}$ indicates rejection of whiteness.\n\n- Decision rule: Define significance levels $\\alpha_{\\tau}$ and $\\alpha_{\\mathrm{LB}}$ and a threshold fraction $q \\in [0,1]$. Conclude that the increase in autocorrelation is genuine if:\n    - the original series exhibits significant monotonic increase in windowed lag-$1$ autocorrelation, i.e., $p_{\\tau,\\mathrm{orig}}  \\alpha_{\\tau}$ with $\\tau_{\\mathrm{orig}}  0$,\n    - the residual series also exhibits significant monotonic increase, i.e., $p_{\\tau,\\mathrm{res}}  \\alpha_{\\tau}$ with $\\tau_{\\mathrm{res}}  0$, and\n    - at least a fraction $q$ of residual windows reject whiteness at level $\\alpha_{\\mathrm{LB}}$.\n  Otherwise, rule out the increase as spurious (return a boolean value indicating the test outcome).\n\nYour program must implement the above algorithm, using the following test suite consisting of internally generated time series, covered by varied scenarios:\n\n- Test case $1$ (spurious trend-induced autocorrelation): length $n = 1024$, deterministic logistic trend $s_t = A/(1 + \\exp(-k(t-t_0)))$ with $A = 1.0$, $k = 0.01$, $t_0 = 512$, and additive independent and identically distributed Gaussian noise $e_t \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.1$. Expectation: original windowed lag-$1$ autocorrelations exhibit increasing values due to $s_t$; after wavelet detrending with sufficiently large $J$, the residual windows do not exhibit significant monotonic increase, and the residual passes whiteness in most windows. The decision should be spurious (boolean false).\n\n- Test case $2$ (genuine increase due to critical slowing down): length $n = 1024$, autoregressive process with time-varying coefficient $\\phi_t$ increasing linearly from $\\phi_{\\min} = 0.2$ to $\\phi_{\\max} = 0.95$ (inclusive), i.e., $x_t = \\phi_t x_{t-1} + \\epsilon_t$ with $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$, $\\sigma = 0.5$, and $x_0 = 0$. No deterministic trend is present. Expectation: genuine monotonic increase in windowed lag-$1$ autocorrelations that persists after detrending, and residual windows reject whiteness frequently. The decision should be genuine (boolean true).\n\n- Test case $3$ (baseline white noise): length $n = 1024$, independent and identically distributed Gaussian noise $x_t \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.5$. Expectation: no significant monotonic increase in windowed lag-$1$ autocorrelations either before or after detrending; residual windows do not systematically reject whiteness. The decision should be spurious/non-genuine (boolean false).\n\nUse the following analysis parameters uniformly across tests: window length $W = 128$, step size $S = 64$, wavelet scales $J = 4$, Ljung–Box maximum lag $h = 5$, Kendall significance level $\\alpha_{\\tau} = 0.05$, Ljung–Box significance level $\\alpha_{\\mathrm{LB}} = 0.05$, and whiteness-rejection fraction threshold $q = 0.6$. All random draws must be reproducible via a fixed pseudorandom seed.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each entry is a boolean for the corresponding test case, in the order listed above. No physical units are involved in this problem; all computations are dimensionless. No angles are involved.\n\nYour implementation must be a complete, runnable program with no external inputs or files, strictly using Python and the specified libraries.",
            "solution": "The task is to construct a rigorous statistical procedure to differentiate a genuine increase in lag-$1$ autocorrelation, an early warning indicator (EWI) for tipping points, from a spurious increase induced by smooth, non-stationarity trends in a time series. This is a critical problem in the analysis of complex systems, as false positives can undermine the credibility of early warning systems. The solution requires the careful integration of signal processing and statistical hypothesis testing into a single, coherent algorithmic framework.\n\nThe core principle of the methodology is the decomposition of the observed time series, $x_t$, into a smooth trend component, $s_t$, and a residual fluctuation component, $e_t$, such that $x_t = s_t + e_t$. A smoothly varying $s_t$ can artificially inflate estimates of autocorrelation, even if $e_t$ is purely random (white noise). Therefore, to probe the intrinsic dynamics of the system, we must first estimate and remove this trend. The chosen method for this detrending is a wavelet-based smoother implementing the \"à trous\" (with holes) algorithm. This is an undecimated, stationary wavelet transform that iteratively applies a low-pass filter to the signal. For this problem, a degree-$3$ B-spline scaling filter with coefficients $h = [1, 4, 6, 4, 1]/16$ is specified. At each scale level $j$ from $1$ to a maximum of $J=4$, the filter $h$ is dilated by inserting $2^{j-1}-1$ zeros between its coefficients. The smoothed series from the previous scale, $c_{j-1}$ (where $c_0 = x$), is then convolved with this dilated filter to produce the next-level smoothed series, $c_j$. We employ symmetric boundary handling for the convolution to minimize edge artifacts. After $J=4$ iterations, the final smoothed series $c_J$ serves as our estimate for the trend $s_t$. The residual series, which should contain the system's intrinsic dynamics, is then simply $r_t = x_t - c_J$.\n\nTo capture the temporal evolution of the system's properties, the analysis is conducted on overlapping moving windows across the time series. We use a window of length $W=128$ and a step size of $S=64$. Within each window, we compute the primary EWI, the lag-$1$ sample autocorrelation, $\\hat{\\rho}_1$. This is done for both the original time series data and the derived residual data. The sample autocorrelation for a generic series $y$ of length $n$ at lag $k$ is calculated precisely according to the provided formula:\n$$\n\\hat{\\rho}_k = \\frac{\\sum_{t=k+1}^{n} (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^{n} (y_t - \\bar{y})^2}\n$$\nThis calculation yields a sequence of $\\hat{\\rho}_1$ values for the original series and a corresponding sequence for the residual series, with each value representing the local autocorrelation at a point in time.\n\nThe diagnostic power of the method comes from comparing the behavior of $\\hat{\\rho}_1$ before and after detrending, and from examining the statistical properties of the residuals. First, we test for a monotonic rise in autocorrelation over time. This is accomplished using the Kendall's $\\tau$ rank correlation test, which assesses the association between the window index (representing time) and the sequence of calculated $\\hat{\\rho}_1$ values. A positive $\\tau$ statistic with a two-sided $p$-value below the significance level $\\alpha_{\\tau}=0.05$ is taken as evidence of a significant increasing trend. This test is performed independently on the $\\hat{\\rho}_1$ sequences from both the original and residual series. Second, we must validate the nature of the residual series. If the original signal's autocorrelation was due only to a simple trend added to white noise, the residuals $r_t$ should themselves be white noise. To test this, we apply the Ljung-Box (LB) portmanteau test to the residual data in each window. The LB statistic,\n$$\nQ = n(n+2)\\sum_{k=1}^{h} \\frac{\\hat{\\rho}_k^2}{n-k}\n$$\nis computed using the window length $n=W=128$ and a maximum lag of $h=5$. Under the null hypothesis that the series is white noise, $Q$ is approximately chi-square distributed with $h=5$ degrees of freedom. A resulting $p$-value, $p_{\\mathrm{LB}}$, below the significance level $\\alpha_{\\mathrm{LB}}=0.05$ constitutes a rejection of the whiteness hypothesis, indicating that significant autocorrelation structure remains in the residuals.\n\nFinally, these individual tests are synthesized into a single, strict decision rule. An observed increase in autocorrelation is classified as \"genuine,\" and thus a valid EWI, if and only if all three of the following conditions are met:\n1.  The original series exhibits a significant positive trend in its windowed lag-$1$ autocorrelation values, defined as $\\tau_{\\mathrm{orig}}  0$ and $p_{\\tau,\\mathrm{orig}}  \\alpha_{\\tau}$.\n2.  The residual series also exhibits a significant positive trend in its windowed lag-$1$ autocorrelation values, defined as $\\tau_{\\mathrm{res}}  0$ and $p_{\\tau,\\mathrm{res}}  \\alpha_{\\tau}$. The persistence of the trend after detrending is crucial evidence that the increasing memory is an intrinsic property of the system's dynamics.\n3.  The residuals are not consistently white noise. Specifically, the fraction of windows for which the Ljung-Box test rejects the null hypothesis of whiteness must be at least $q=0.6$. This confirms that the residual series contains evolving, non-random dynamic structure consistent with an approaching critical transition.\n\nIf any one of these conditions is not satisfied, the observed increase in autocorrelation is ruled as spurious or non-genuine. This composite, multi-faceted test provides a robust defense against the common pitfalls of EWI analysis. The entire procedure is implemented algorithmically and verified against a suite of synthetic time series with known ground truths, ensuring its correctness and utility for the specified scenarios. The use of a fixed seed for all random number generation ensures the full analysis is deterministic and reproducible.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import convolve1d\nfrom scipy.stats import kendalltau, chi2\n\ndef sample_autocorr(y: np.ndarray, k: int) - float:\n    \"\"\"\n    Computes the sample lag-k autocorrelation for a time series y.\n    Uses the definition provided in the problem statement.\n    \"\"\"\n    n = len(y)\n    if n = k:\n        return 0.0\n    \n    mean_y = np.mean(y)\n    y_demeaned = y - mean_y\n    \n    numerator = np.sum(y_demeaned[k:] * y_demeaned[:-k])\n    denominator = np.sum(y_demeaned**2)\n    \n    if denominator == 0.0:\n        return 0.0\n        \n    return numerator / denominator\n\ndef ljung_box_test(y: np.ndarray, h: int) - tuple[float, float]:\n    \"\"\"\n    Computes the Ljung-Box Q statistic and its p-value.\n    \"\"\"\n    n = len(y)\n    q_stat = 0.0\n    for k in range(1, h + 1):\n        if n - k = 0:\n            continue\n        rho_k = sample_autocorr(y, k)\n        q_stat += (rho_k**2) / (n - k)\n    \n    q_stat *= n * (n + 2)\n    p_value = chi2.sf(q_stat, df=h)\n    \n    return q_stat, p_value\n\ndef wavelet_smooth(x: np.ndarray, J: int) - np.ndarray:\n    \"\"\"\n    Performs wavelet-based smoothing using the 'à trous' algorithm.\n    \"\"\"\n    base_filter = np.array([1.0, 4.0, 6.0, 4.0, 1.0]) / 16.0\n    c_j = x.copy()\n    \n    for j in range(1, J + 1):\n        num_zeros = 2**(j - 1) - 1\n        if num_zeros  0:\n            dilated_filter = np.zeros(len(base_filter) + (len(base_filter) - 1) * num_zeros)\n            dilated_filter[::num_zeros + 1] = base_filter\n        else:\n            dilated_filter = base_filter\n            \n        c_j = convolve1d(c_j, dilated_filter, mode='reflect')\n        \n    return c_j\n\ndef analyze_series(x: np.ndarray, W: int, S: int, J: int, h_lb: int, alpha_tau: float, alpha_lb: float, q_thresh: float) - bool:\n    \"\"\"\n    Implements the full diagnostic pipeline for one time series.\n    \"\"\"\n    # 1. Wavelet detrending\n    s_t = wavelet_smooth(x, J)\n    r_t = x - s_t\n\n    # 2. Moving window analysis\n    num_windows = (len(x) - W) // S + 1\n    rhos_orig = []\n    rhos_res = []\n    lb_p_values = []\n\n    for i in range(num_windows):\n        start = i * S\n        end = start + W\n        \n        window_orig = x[start:end]\n        window_res = r_t[start:end]\n\n        rho1_orig = sample_autocorr(window_orig, k=1)\n        rhos_orig.append(rho1_orig)\n\n        rho1_res = sample_autocorr(window_res, k=1)\n        rhos_res.append(rho1_res)\n\n        _, p_lb = ljung_box_test(window_res, h=h_lb)\n        lb_p_values.append(p_lb)\n\n    # 3. Kendall's Tau test for monotonic trends in rho1\n    window_indices = np.arange(len(rhos_orig))\n    # Check for constant series which gives NaN in Kendall's Tau\n    if np.all(rhos_orig == rhos_orig[0]):\n        tau_orig, p_tau_orig = 0.0, 1.0\n    else:\n        tau_orig, p_tau_orig = kendalltau(window_indices, rhos_orig)\n\n    if np.all(rhos_res == rhos_res[0]):\n        tau_res, p_tau_res = 0.0, 1.0\n    else:\n        tau_res, p_tau_res = kendalltau(window_indices, rhos_res)\n\n    # 4. Fraction of windows rejecting whiteness\n    rejected_whiteness_frac = np.sum(np.array(lb_p_values)  alpha_lb) / len(lb_p_values)\n\n    # 5. Apply the decision rule\n    cond1_orig_trend = (p_tau_orig  alpha_tau) and (tau_orig  0)\n    cond2_res_trend = (p_tau_res  alpha_tau) and (tau_res  0)\n    cond3_res_nonwhite = (rejected_whiteness_frac = q_thresh)\n    \n    is_genuine = cond1_orig_trend and cond2_res_trend and cond3_res_nonwhite\n    \n    return is_genuine\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print results.\n    \"\"\"\n    # Global parameters\n    SEED = 12345\n    rng = np.random.default_rng(SEED)\n    \n    # Analysis parameters\n    W = 128\n    S = 64\n    J = 4\n    h_lb = 5\n    alpha_tau = 0.05\n    alpha_lb = 0.05\n    q_thresh = 0.6\n    \n    # --- Test Case Generation ---\n    def generate_case1():\n        n = 1024\n        A = 1.0\n        k = 0.01\n        t0 = 512.0\n        sigma = 0.1\n        t = np.arange(n)\n        s_t = A / (1.0 + np.exp(-k * (t - t0)))\n        e_t = rng.normal(loc=0.0, scale=sigma, size=n)\n        return s_t + e_t\n\n    def generate_case2():\n        n = 1024\n        phi_min = 0.2\n        phi_max = 0.95\n        sigma = 0.5\n        x0 = 0.0\n        \n        phi_t = np.linspace(phi_min, phi_max, n)\n        epsilon_t = rng.normal(loc=0.0, scale=sigma, size=n)\n        x = np.zeros(n)\n        x_prev = x0\n        for i in range(n):\n            x[i] = phi_t[i] * x_prev + epsilon_t[i]\n            x_prev = x[i]\n        return x\n\n    def generate_case3():\n        n = 1024\n        sigma = 0.5\n        return rng.normal(loc=0.0, scale=sigma, size=n)\n\n    test_data = [generate_case1(), generate_case2(), generate_case3()]\n    \n    results = []\n    for x_t in test_data:\n        is_genuine = analyze_series(x_t, W, S, J, h_lb, alpha_tau, alpha_lb, q_thresh)\n        results.append(is_genuine)\n\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        }
    ]
}