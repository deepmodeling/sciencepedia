{
    "hands_on_practices": [
        {
            "introduction": "The journey into early warning indicators begins with understanding their theoretical origin in the mathematics of dynamical systems. By analyzing the canonical \"saddle-node bifurcation\" model, a simple yet powerful archetype for tipping points, we can derive the fundamental link between an approaching threshold and \"critical slowing down.\" This foundational exercise  demonstrates from first principles how the system's recovery rate diminishes near a bifurcation, leading directly to a rise in autocorrelation, one of the most important statistical warnings.",
            "id": "3876409",
            "problem": "Consider the canonical saddle-node (fold) normal form from dynamical systems theory applied to environmental tipping models,\n$$\\frac{dx}{dt}=\\mu - x^{2},$$\nwhere $x$ is a scalar system state and $\\mu$ is a slowly varying control parameter representing external forcing. This one-dimensional deterministic system is widely used as an archetype for abrupt transitions in Earth system components when a stable state disappears under parameter change.\n\n(a) Using fundamental linear stability analysis, identify all equilibria of the deterministic system as a function of $\\mu$ and determine their stability.\n\n(b) Suppose $\\mu(t)$ varies slowly and monotonically in time due to external forcing, with $\\mu(t)=\\mu_{0}-\\varepsilon t$, where $\\mu_{0}>0$ and $0<\\varepsilon\\ll 1$ is a small ramp rate. Starting from the stable equilibrium at $t\\ll 0$, explain, from first principles, why a hysteresis-like delayed loss of stability (dynamic bifurcation delay) occurs as $\\mu(t)$ passes through $0$ and why this is relevant to early warning indicators in environmental and Earth system modeling.\n\n(c) To connect with early warning indicators, include weak environmental noise and consider the Stochastic Differential Equation (SDE)\n$$dx=\\left(\\mu - x^{2}\\right)dt+\\sigma\\,dW_{t},$$\nwhere $\\sigma>0$ is the noise amplitude and $W_{t}$ is a standard Wiener process (Brownian motion), so that $dW_{t}$ represents Gaussian white noise with independent stationary increments. For fixed $\\mu>0$, linearize this SDE about the stable equilibrium $x^{\\star}(\\mu)$ and obtain the stationary lag-$\\Delta t$ autocorrelation of the fluctuations, defined as\n$$\\rho(\\Delta t)=\\frac{\\mathbb{E}\\left[(x(t)-\\mathbb{E}x(t))\\,(x(t+\\Delta t)-\\mathbb{E}x(t))\\right]}{\\operatorname{Var}(x(t))},$$\nunder the linearized dynamics. Express the final answer as an exact analytic expression in terms of $\\mu$ and $\\Delta t$. No rounding is required. The autocorrelation is dimensionless; do not include units.",
            "solution": "The problem is scientifically grounded, well-posed, and all necessary information is provided. It represents a standard analysis of a canonical model for tipping points. Therefore, the problem is deemed valid.\n\n(a) To identify the equilibria of the system $\\frac{dx}{dt}=\\mu - x^{2}$, we set the rate of change to zero:\n$$\n\\frac{dx}{dt} = 0 \\implies \\mu - x^{2} = 0\n$$\nThis gives $x^{2} = \\mu$. Let the equilibrium points be denoted by $x^{\\star}$.\n\n-   If $\\mu > 0$, there are two distinct real equilibria: $x^{\\star}_{1} = \\sqrt{\\mu}$ and $x^{\\star}_{2} = -\\sqrt{\\mu}$.\n-   If $\\mu = 0$, there is a single equilibrium at $x^{\\star} = 0$.\n-   If $\\mu < 0$, there are no real equilibria, as $x^{2}$ cannot be negative for real $x$.\n\nTo determine the stability of these equilibria, we use linear stability analysis. Let $f(x) = \\mu - x^{2}$. The stability is determined by the sign of the eigenvalue $\\lambda = \\frac{df}{dx}$ evaluated at the equilibrium point $x^{\\star}$. The derivative is:\n$$\n\\frac{df}{dx} = -2x\n$$\nWe evaluate this at each equilibrium for the case $\\mu>0$:\n\n-   For the equilibrium $x^{\\star}_{1} = \\sqrt{\\mu}$:\n    $$\n    \\lambda_{1} = \\left.\\frac{df}{dx}\\right|_{x=\\sqrt{\\mu}} = -2\\sqrt{\\mu}\n    $$\n    Since $\\mu > 0$, $\\sqrt{\\mu}$ is real and positive, so $\\lambda_{1} < 0$. An equilibrium with a negative real eigenvalue is stable. Thus, $x^{\\star}_{1} = \\sqrt{\\mu}$ is a stable equilibrium.\n\n-   For the equilibrium $x^{\\star}_{2} = -\\sqrt{\\mu}$:\n    $$\n    \\lambda_{2} = \\left.\\frac{df}{dx}\\right|_{x=-\\sqrt{\\mu}} = -2(-\\sqrt{\\mu}) = 2\\sqrt{\\mu}\n    $$\n    Since $\\mu > 0$, $\\lambda_{2} > 0$. An equilibrium with a positive real eigenvalue is unstable. Thus, $x^{\\star}_{2} = -\\sqrt{\\mu}$ is an unstable equilibrium.\n\nAt $\\mu=0$, the two equilibria merge. Here, $\\lambda = -2(0) = 0$. This is a non-hyperbolic equilibrium, and linear stability analysis is inconclusive. This point, $\\mu=0$, is the location of a saddle-node (or fold) bifurcation, where the stable and unstable equilibria collide and annihilate.\n\n(b) Hysteresis-like delayed loss of stability, also known as dynamic bifurcation delay, occurs when a control parameter changes at a finite rate through a bifurcation point.\n\nConsider the system starting at $t \\ll 0$ in the stable equilibrium $x(t) = \\sqrt{\\mu(t)} = \\sqrt{\\mu_{0}-\\varepsilon t}$ for $\\mu(t)>0$. As time $t$ increases, the parameter $\\mu(t)$ slowly decreases. At $t_{c} = \\mu_{0}/\\varepsilon$, the parameter $\\mu$ crosses zero. At this exact moment, the static bifurcation occurs: the stable equilibrium at $x^{\\star}=0$ ceases to exist. For any $t > t_{c}$, $\\mu(t)$ becomes negative. In this regime, the governing equation is $\\frac{dx}{dt} = \\mu(t) - x^{2}$. Since $\\mu(t) < 0$ and $-x^{2} \\le 0$, the right-hand side is strictly negative for all $x$. This means $\\frac{dx}{dt} < 0$, and the state $x(t)$ must decrease indefinitely (in this simple model, towards $-\\infty$).\n\nThe delay arises because the rate of change is not instantaneous. Just after the bifurcation at $t_{c}$, the state $x(t)$ is close to $0$, and $\\mu(t)$ is a small negative number. For example, at a time $t = t_{c} + \\delta t$ where $\\delta t$ is small and positive, $\\mu(t) = -\\varepsilon \\delta t$. The equation of motion is $\\frac{dx}{dt} = -\\varepsilon\\delta t - x^{2}$. Since $x$ is near $0$, $\\frac{dx}{dt}$ is a small negative number. The state $x(t)$ therefore begins to drift away from $x=0$ very slowly. It does not \"jump\" to a new state. This lingering of the trajectory near the \"ghost\" of the now-vanished equilibrium is the dynamic bifurcation delay. The system's state lags behind the change in the underlying stability landscape. The smaller the ramp rate $\\varepsilon$, the slower the drift and the longer the delay.\n\nThis phenomenon is critically relevant to early warning indicators (EWS). EWS, such as rising variance or autocorrelation, are based on \"critical slowing down\"—the fact that the system's recovery rate from perturbations slows as it approaches a bifurcation. The recovery rate is given by $|\\lambda_{1}| = 2\\sqrt{\\mu}$, which approaches $0$ as $\\mu \\to 0^{+}$. The dynamic bifurcation delay is a direct consequence of this critical slowing down. The system becomes so sluggish that it cannot keep up with the changing environment (the changing $\\mu$). The danger this reveals is that the actual tipping point ($\\mu=0$) can be crossed before any dramatic, observable shift in the system's state occurs. An observer might believe the system is still in a safe regime, when in fact the underlying stability has already been lost, and a large, irreversible transition is merely delayed, but now inevitable.\n\n(c) We are given the Stochastic Differential Equation (SDE):\n$$\ndx = (\\mu - x^{2})dt + \\sigma dW_{t}\n$$\nFor a fixed $\\mu > 0$, the stable equilibrium is $x^{\\star} = \\sqrt{\\mu}$. We linearize the system around this equilibrium by considering small fluctuations $y(t) = x(t) - x^{\\star}$. This also means $dx = dy$. The drift term $f(x) = \\mu - x^{2}$ is approximated using a first-order Taylor expansion around $x^{\\star}$:\n$$\nf(x) \\approx f(x^{\\star}) + f'(x^{\\star})(x - x^{\\star})\n$$\nWe know $f(x^{\\star}) = \\mu - (x^{\\star})^{2} = \\mu - \\mu = 0$. The derivative is $f'(x) = -2x$, so $f'(x^{\\star}) = -2x^{\\star} = -2\\sqrt{\\mu}$. Substituting these into the expansion gives:\n$$\nf(x) \\approx 0 + (-2\\sqrt{\\mu})y(t) = -2\\sqrt{\\mu}y(t)\n$$\nThe linearized SDE for the fluctuations $y(t)$ is therefore:\n$$\ndy = -2\\sqrt{\\mu} y dt + \\sigma dW_{t}\n$$\nThis is the equation for an Ornstein-Uhlenbeck (OU) process of the general form $dy = -\\theta y dt + \\sigma dW_{t}$, with a relaxation rate $\\theta = 2\\sqrt{\\mu}$.\n\nThe stationary solution for an OU process is a Gaussian process with zero mean, $\\mathbb{E}[y(t)]=0$. This implies $\\mathbb{E}[x(t)] = x^{\\star}$. The fluctuations are defined as deviations from the mean, so $x(t) - \\mathbb{E}[x(t)] = x(t) - x^{\\star} = y(t)$.\n\nThe stationary lag-$\\Delta t$ autocorrelation function $\\rho(\\Delta t)$ is defined as:\n$$\n\\rho(\\Delta t) = \\frac{\\mathbb{E}\\left[(x(t)-\\mathbb{E}x(t))\\,(x(t+\\Delta t)-\\mathbb{E}x(t))\\right]}{\\operatorname{Var}(x(t))} = \\frac{\\mathbb{E}[y(t)y(t+\\Delta t)]}{\\operatorname{Var}(y(t))}\n$$\nThe numerator is the autocovariance of $y(t)$. For a stationary OU process, the autocovariance function is given by $\\operatorname{Cov}(y(t), y(t+\\Delta t)) = \\operatorname{Var}(y(t))\\exp(-\\theta |\\Delta t|)$. Assuming $\\Delta t \\ge 0$, this is $\\operatorname{Var}(y(t))\\exp(-\\theta \\Delta t)$.\n\nSubstituting this into the expression for $\\rho(\\Delta t)$:\n$$\n\\rho(\\Delta t) = \\frac{\\operatorname{Var}(y(t))\\exp(-\\theta \\Delta t)}{\\operatorname{Var}(y(t))} = \\exp(-\\theta \\Delta t)\n$$\nFinally, we substitute the value of $\\theta = 2\\sqrt{\\mu}$ back into the expression:\n$$\n\\rho(\\Delta t) = \\exp(-2\\sqrt{\\mu} \\Delta t)\n$$\nThis expression shows that as the system approaches the tipping point ($\\mu \\to 0^{+}$), the exponent approaches $0$, and the autocorrelation $\\rho(\\Delta t)$ approaches $1$ for any fixed lag $\\Delta t$. This increasing autocorrelation is a key early warning signal.",
            "answer": "$$\\boxed{\\exp(-2\\sqrt{\\mu}\\Delta t)}$$"
        },
        {
            "introduction": "Moving from theory to practical application, we now address how to detect the signatures of critical slowing down in real-world time series data. The increasing system memory and larger fluctuations predicted by theory manifest as measurable increases in lag-1 autocorrelation and variance. This hands-on coding practice  guides you through the implementation of the standard moving-window technique, where you will fit an autoregressive model to segments of a time series to extract and trend these key early warning indicators.",
            "id": "3876395",
            "problem": "You are given detrended scalar time series data that is hypothesized to approach a bifurcation-induced regime shift. An early warning indicator is the progressive increase in lag-$1$ autocorrelation and variance, which can be assessed via a moving-window fit of an autoregressive order-$1$ model and inference on indicator trends. Starting from the definition of an Autoregressive (AR) model, consider the AR order-$1$ process defined by $x_t = \\phi x_{t-1} + \\epsilon_t$ with $t \\in \\{1,2,\\ldots,N-1\\}$, where $x_t$ is the detrended scalar state, $\\phi$ is the AR coefficient, and $\\epsilon_t$ is a zero-mean noise sequence. Assume the noise is independent and identically distributed and the time series is mean-zero after detrending. Using Ordinary Least Squares (OLS) as the principle for parameter estimation, and unbiased sample variance within each window as the measure of variability, your task is to implement a complete program that:\n\n- Segments the series into overlapping windows of fixed length, advancing by a fixed step, and for each window:\n  - Computes the AR order-$1$ coefficient $\\phi$ by minimizing the sum of squared one-step prediction errors on the window segment.\n  - Computes the unbiased sample variance within the window segment.\n- Defines the window time coordinate as the center index of each window and computes the linear trend slope of the windowed estimates of $\\phi$ versus window center time, and the linear trend slope of the windowed variance versus window center time, using a least-squares fit.\n- Returns, for each test case, a list containing two floating-point numbers: the slope of $\\phi$ and the slope of the variance across window centers.\n\nFundamental base and assumptions:\n- The Autoregressive (AR) definition: $x_t = \\phi x_{t-1} + \\epsilon_t$ for a detrended series with zero intercept, where $\\epsilon_t$ is zero-mean noise.\n- Ordinary Least Squares (OLS): The parameter estimate minimizes $\\sum_t (x_t - \\phi x_{t-1})^2$ over the window.\n- Unbiased sample variance: For a window of length $W$, $\\mathrm{Var}(x) = \\frac{1}{W-1} \\sum_{i=1}^W (x_i - \\bar{x})^2$.\n- Linear trend slope: For paired data $(t_i, y_i)$, the OLS slope is the coefficient multiplying $t$ in the best linear fit $y = a + b t$, equivalently $b = \\frac{\\sum_i (t_i - \\bar{t})(y_i - \\bar{y})}{\\sum_i (t_i - \\bar{t})^2}$.\n\nRequirements:\n- Implement the procedure exactly as described.\n- Use linear slope as the quantification of trend for both $\\phi$ and variance.\n- If a window yields a degenerate AR fit (for example, if $\\sum x_{t-1}^2 = 0$ within the window), define the window’s $\\phi$ estimate to be $0$.\n- The input time series are internally generated by the program using reproducible pseudorandom noise with specified seeds.\n- No physical units are involved; all outputs are dimensionless real numbers.\n- Angles are not used.\n- Express all outputs as raw floating-point numbers with no rounding requirement beyond standard double precision.\n\nTest suite:\nImplement four test cases with the following parameters. In all cases, set the initial state to $x_0 = 0$ and use independent Gaussian noise $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ with standard deviation $\\sigma$. When $\\phi$ varies over time, set $\\phi_t = \\phi_{\\mathrm{start}} + (\\phi_{\\mathrm{end}} - \\phi_{\\mathrm{start}})\\cdot \\frac{t}{N-1}$ for $t \\in \\{0,1,\\ldots,N-1\\}$, and simulate $x_t = \\phi_t x_{t-1} + \\epsilon_t$.\n\n- Case $1$ (stable baseline):\n  - Series length $N = 500$\n  - Constant $\\phi = 0.2$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 10$\n  - Random seed $s = 42$\n- Case $2$ (approaching tipping with increasing persistence):\n  - Series length $N = 500$\n  - Linearly increasing $\\phi$ from $\\phi_{\\mathrm{start}} = 0.2$ to $\\phi_{\\mathrm{end}} = 0.95$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 10$\n  - Random seed $s = 43$\n- Case $3$ (boundary condition with few windows):\n  - Series length $N = 60$\n  - Linearly increasing $\\phi$ from $\\phi_{\\mathrm{start}} = 0.5$ to $\\phi_{\\mathrm{end}} = 0.9$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 5$\n  - Random seed $s = 44$\n- Case $4$ (near white noise):\n  - Series length $N = 500$\n  - Constant $\\phi = 0.0$\n  - Noise standard deviation $\\sigma = 1.0$\n  - Window length $W = 50$\n  - Step size $S = 10$\n  - Random seed $s = 45$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list of two floating-point numbers, ordered as $[\\text{slope of } \\phi, \\text{slope of variance}]$. For example, the output format must be exactly like $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$, where each $a_i$ and $b_i$ are floating-point numbers.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique, verifiable solution. We will proceed with a step-by-step derivation and implementation of the specified procedure. The objective is to compute trend indicators for a potential bifurcation-induced regime shift in a scalar time series by analyzing the behavior of the lag-$1$ autocorrelation coefficient and the variance within a moving window.\n\nThe core of the analysis rests upon an autoregressive model of order-$1$ ($\\text{AR}(1)$). For a detrended, zero-mean time series $x_t$, the model is defined as:\n$$x_t = \\phi x_{t-1} + \\epsilon_t$$\nwhere $t \\in \\{1, 2, \\ldots, N-1\\}$ is the time index, $\\phi$ is the lag-$1$ autoregressive coefficient, and $\\epsilon_t$ is a sequence of independent and identically distributed zero-mean noise terms.\n\nThe analysis consists of three main stages: segmenting the time series into windows; estimating $\\phi$ and the variance within each window; and quantifying the temporal trend of these estimates.\n\n**1. Time Series Segmentation and Windowing**\n\nThe given time series, of total length $N$, is segmented into overlapping windows of a fixed length $W$. The windows advance along the series with a fixed step size $S$. The starting index of the first window is $0$. Subsequent windows start at indices $S, 2S, 3S, \\ldots$. The process continues as long as the window fits entirely within the series. Thus, the starting index $i$ of any window must satisfy the condition $i + W \\le N$. The sequence of starting indices is therefore $i_k = k \\cdot S$ for $k = 0, 1, 2, \\ldots$ such that $kS \\le N - W$. For each window, we will compute two statistics: the $\\text{AR}(1)$ coefficient and the unbiased sample variance.\n\n**2. Per-Window Parameter Estimation**\n\nLet us consider a single window of data points $\\{x_i, x_{i+1}, \\ldots, x_{i+W-1}\\}$.\n\n**2.1. AR(1) Coefficient ($\\phi$) Estimation**\n\nThe coefficient $\\phi$ is estimated using the Ordinary Least Squares (OLS) principle. We seek the value of $\\phi$ that minimizes the sum of squared residuals, $L(\\phi)$, for the one-step predictions within the window. The data available for fitting the model $x_t = \\phi x_{t-1} + \\epsilon_t$ within this window consists of $W-1$ pairs of the form $(x_{t-1}, x_t)$, for $t$ ranging from $i+1$ to $i+W-1$.\n\nThe sum of squared residuals is:\n$$L(\\phi) = \\sum_{t=i+1}^{i+W-1} (x_t - \\phi x_{t-1})^2$$\nTo find the minimum, we compute the derivative of $L(\\phi)$ with respect to $\\phi$ and set it to zero:\n$$\\frac{d L}{d \\phi} = \\frac{d}{d \\phi} \\sum_{t=i+1}^{i+W-1} (x_t^2 - 2\\phi x_t x_{t-1} + \\phi^2 x_{t-1}^2) = \\sum_{t=i+1}^{i+W-1} (-2 x_t x_{t-1} + 2\\phi x_{t-1}^2) = 0$$\nSolving for $\\phi$:\n$$2 \\phi \\sum_{t=i+1}^{i+W-1} x_{t-1}^2 = 2 \\sum_{t=i+1}^{i+W-1} x_t x_{t-1}$$\nThis yields the OLS estimate $\\hat{\\phi}$ for the window:\n$$\\hat{\\phi} = \\frac{\\sum_{t=i+1}^{i+W-1} x_t x_{t-1}}{\\sum_{t=i+1}^{i+W-1} x_{t-1}^2}$$\nAs specified, if the denominator $\\sum x_{t-1}^2$ is zero, which indicates no variation in the regressor within the window segment used for fitting, the estimate $\\hat{\\phi}$ is defined to be $0$.\n\n**2.2. Unbiased Sample Variance Estimation**\n\nThe unbiased sample variance for the data points $\\{x_i, x_{i+1}, \\ldots, x_{i+W-1}\\}$ within the window is calculated. First, the sample mean, $\\bar{x}_{\\text{win}}$, of the $W$ data points is computed:\n$$\\bar{x}_{\\text{win}} = \\frac{1}{W} \\sum_{j=i}^{i+W-1} x_j$$\nThe unbiased sample variance, $\\widehat{\\mathrm{Var}}(x)$, is then given by:\n$$\\widehat{\\mathrm{Var}}(x) = \\frac{1}{W-1} \\sum_{j=i}^{i+W-1} (x_j - \\bar{x}_{\\text{win}})^2$$\nThe denominator $W-1$ reflects Bessel's correction for an unbiased estimate of the population variance from a sample.\n\n**3. Trend Analysis of Indicator Time Series**\n\nAfter processing all windows, we obtain two new time series: one for the estimates of $\\phi$ and one for the estimates of variance. To analyze their trends, we first assign a time coordinate to each estimate. The problem specifies using the center index of the window as its time coordinate. For a window starting at index $i$ with length $W$, the time coordinate $t_{\\text{center}}$ is:\n$$t_{\\text{center}} = i + \\frac{W-1}{2}$$\nLet the series of window time centers be $\\{t_1, t_2, \\ldots, t_M\\}$ and the corresponding indicator estimates (either $\\hat{\\phi}$ or $\\widehat{\\mathrm{Var}}$) be $\\{y_1, y_2, \\ldots, y_M\\}$, where $M$ is the total number of windows.\n\nTo quantify the trend, we perform a linear regression of the indicator $y$ on time $t$, fitting the model $y_k = a + b t_k$. The slope $b$ represents the linear trend. According to OLS, the formula for the slope is:\n$$b = \\frac{\\sum_{k=1}^M (t_k - \\bar{t})(y_k - \\bar{y})}{\\sum_{k=1}^M (t_k - \\bar{t})^2}$$\nwhere $\\bar{t} = \\frac{1}{M}\\sum_{k=1}^M t_k$ and $\\bar{y} = \\frac{1}{M}\\sum_{k=1}^M y_k$ are the sample means of the time coordinates and the indicator values, respectively. This formula is equivalent to the ratio of the sample covariance of $t$ and $y$ to the sample variance of $t$. This slope $b$ is computed for both the $\\hat{\\phi}$ series and the $\\widehat{\\mathrm{Var}}(x)$ series.\n\nThis complete procedure provides the two required outputs for each test case: the slope of the lag-$1$ autocorrelation coefficient and the slope of the variance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_slope(t, y):\n    \"\"\"\n    Computes the slope of a linear regression of y on t using OLS.\n    \"\"\"\n    n = len(t)\n    if n < 2:\n        return 0.0\n    \n    t_mean = np.mean(t)\n    y_mean = np.mean(y)\n    \n    numerator = np.sum((t - t_mean) * (y - y_mean))\n    denominator = np.sum((t - t_mean)**2)\n    \n    if denominator == 0:\n        return 0.0\n        \n    return numerator / denominator\n\ndef analyze_time_series(N, phi_params, sigma, W, S, seed):\n    \"\"\"\n    Generates an AR(1) time series and computes the slopes of EWI trends.\n    \n    Args:\n        N (int): Series length.\n        phi_params (float or tuple): Constant phi or (start, end) for varying phi.\n        sigma (float): Noise standard deviation.\n        W (int): Window length.\n        S (int): Step size.\n        seed (int): Random seed.\n\n    Returns:\n        list: A list containing [slope_phi, slope_var].\n    \"\"\"\n    # 1. Generate the time series\n    rng = np.random.default_rng(seed)\n    x = np.zeros(N)\n    noise = rng.normal(loc=0.0, scale=sigma, size=N)\n    \n    if isinstance(phi_params, tuple):\n        phi_start, phi_end = phi_params\n        # phi_t is used to evolve x from t-1 to t\n        phi_vec = np.linspace(phi_start, phi_end, N)\n    else:\n        phi_vec = np.full(N, phi_params)\n\n    # Initial state x_0 = 0 is set by np.zeros\n    for t in range(1, N):\n        x[t] = phi_vec[t] * x[t-1] + noise[t]\n\n    # 2. Compute windowed statistics\n    t_centers = []\n    phi_estimates = []\n    var_estimates = []\n\n    for i in range(0, N - W + 1, S):\n        window = x[i : i + W]\n        \n        # Estimate phi using OLS for x_t = phi * x_{t-1}\n        # The regressors are x_0, ..., x_{W-2} and responses x_1, ..., x_{W-1}\n        # within the window.\n        X_reg = window[:-1]\n        y_reg = window[1:]\n        \n        sum_X_sq = np.dot(X_reg, X_reg)\n        if sum_X_sq == 0:\n            phi_hat = 0.0\n        else:\n            phi_hat = np.dot(X_reg, y_reg) / sum_X_sq\n        \n        # Estimate unbiased sample variance for the entire window\n        var_hat = np.var(window, ddof=1)\n        \n        # Define window time coordinate as the center index\n        t_center = i + (W - 1) / 2.0\n        \n        phi_estimates.append(phi_hat)\n        var_estimates.append(var_hat)\n        t_centers.append(t_center)\n\n    # 3. Calculate trend slopes\n    t_centers_arr = np.array(t_centers)\n    phi_estimates_arr = np.array(phi_estimates)\n    var_estimates_arr = np.array(var_estimates)\n\n    slope_phi = calculate_slope(t_centers_arr, phi_estimates_arr)\n    slope_var = calculate_slope(t_centers_arr, var_estimates_arr)\n    \n    return [slope_phi, slope_var]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (stable baseline)\n        {'N': 500, 'phi_params': 0.2, 'sigma': 1.0, 'W': 50, 'S': 10, 'seed': 42},\n        # Case 2 (approaching tipping)\n        {'N': 500, 'phi_params': (0.2, 0.95), 'sigma': 1.0, 'W': 50, 'S': 10, 'seed': 43},\n        # Case 3 (boundary condition)\n        {'N': 60, 'phi_params': (0.5, 0.9), 'sigma': 1.0, 'W': 50, 'S': 5, 'seed': 44},\n        # Case 4 (near white noise)\n        {'N': 500, 'phi_params': 0.0, 'sigma': 1.0, 'W': 50, 'S': 10, 'seed': 45},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_time_series(\n            N=case['N'],\n            phi_params=case['phi_params'],\n            sigma=case['sigma'],\n            W=case['W'],\n            S=case['S'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Format the output string exactly as required\n    inner_parts = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The practical application of moving-window analysis introduces a critical methodological challenge: the choice of window size. A window that is too short will produce noisy, high-variance estimates, while a window that is too long will average out the very signal you are trying to detect, introducing bias. This practice  delves into this fundamental bias-variance tradeoff, challenging you to mathematically derive an optimal window length that minimizes the total estimation error for a system with a known underlying trend.",
            "id": "3876413",
            "problem": "An environmental time series is observed at unit time intervals, modeled as the sum of a slowly drifting mean and a weakly stationary noise process approaching a tipping point with critical slowing down. Specifically, let the observations be $X_{t} = \\mu(t) + Y_{t}$, where the mean drifts linearly as $\\mu(t) = \\mu_{0} + \\gamma t$ with constant drift rate $\\gamma$, and the noise is a zero-mean Gaussian first-order autoregressive process (AR(1)) satisfying $Y_{t} = \\phi Y_{t-1} + \\varepsilon_{t}$ with $|\\phi| < 1$ and innovations $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})$ independent and identically distributed over time. Let $\\sigma_{Y}^{2} = \\operatorname{Var}(Y_{t}) = \\sigma_{\\varepsilon}^{2}/(1-\\phi^{2})$ denote the stationary variance of $Y_{t}$.\n\nA common early warning indicator for tipping points is the rolling-window variance estimator. At a given time index $\\tau$, define the windowed variance estimator computed about its within-window mean as\n$$\nW_{w}(\\tau) \\equiv \\frac{1}{w} \\sum_{j=1}^{w} \\left( X_{\\tau-j+1} - \\bar{X}_{w}(\\tau) \\right)^{2}, \\quad \\bar{X}_{w}(\\tau) \\equiv \\frac{1}{w} \\sum_{j=1}^{w} X_{\\tau-j+1}.\n$$\nAssume $w \\ll \\tau$ so that edge effects may be neglected and the AR(1) process is in its stationary regime. You may treat $w$ as a large integer and, where appropriate, approximate sums by their infinite counterparts.\n\nUsing only fundamental definitions of bias, variance, and the properties of Gaussian AR(1) processes, do the following:\n\n1. Derive the deterministic bias of $W_{w}(\\tau)$ relative to the target stationary variance $\\sigma_{Y}^{2}$ induced by the linear drift $\\mu(t) = \\mu_{0} + \\gamma t$, expressing the bias as an explicit function of $w$ and $\\gamma$. State clearly any indexing convention you adopt within the window.\n\n2. Derive an asymptotically valid (large $w$) leading-order expression for $\\operatorname{Var}\\!\\left[ W_{w}(\\tau) \\right]$ in terms of $\\phi$ and $\\sigma_{\\varepsilon}^{2}$. Base your derivation on the covariance structure of the squared Gaussian AR(1) process and standard asymptotics for variances of windowed averages, and express your final result in the form $c(\\phi,\\sigma_{\\varepsilon})/w$ for an explicit function $c(\\cdot)$.\n\n3. Combine your results to write the large-$w$ mean squared error as $\\operatorname{MSE}(w) \\approx \\operatorname{Bias}(w)^{2} + \\operatorname{Var}\\!\\left[ W_{w}(\\tau) \\right]$, and minimize this approximation over $w > 0$ treated as a continuous variable to obtain a closed-form analytic expression for the asymptotically optimal window length $w^{\\star}$.\n\nProvide your final answer as a single closed-form symbolic expression for $w^{\\star}$ in terms of $\\phi$, $\\sigma_{\\varepsilon}$, and $\\gamma$. No numerical evaluation is required, and no units are to be reported in the final answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Time series model: $X_{t} = \\mu(t) + Y_{t}$\n-   Mean drift: $\\mu(t) = \\mu_{0} + \\gamma t$, where $\\gamma$ is a constant.\n-   Noise process: $Y_{t}$ is a zero-mean Gaussian first-order autoregressive (AR(1)) process.\n-   AR(1) definition: $Y_{t} = \\phi Y_{t-1} + \\varepsilon_{t}$ with $|\\phi| < 1$.\n-   Innovations: $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})$ and are independent and identically distributed (i.i.d.).\n-   Stationary variance of $Y_{t}$: $\\sigma_{Y}^{2} = \\operatorname{Var}(Y_{t}) = \\sigma_{\\varepsilon}^{2}/(1-\\phi^{2})$.\n-   Rolling-window variance estimator: $W_{w}(\\tau) \\equiv \\frac{1}{w} \\sum_{j=1}^{w} \\left( X_{\\tau-j+1} - \\bar{X}_{w}(\\tau) \\right)^{2}$.\n-   Rolling-window mean: $\\bar{X}_{w}(\\tau) \\equiv \\frac{1}{w} \\sum_{j=1}^{w} X_{\\tau-j+1}$.\n-   Assumptions: $w \\ll \\tau$ (stationarity within window, edge effects negligible), $w$ is a large integer. Approximations for large $w$ (e.g., infinite sums) are permitted.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is based on standard time series analysis, specifically the study of AR(1) processes, which is a cornerstone of modeling stochastic dynamics. The concept of using statistical indicators like variance to detect \"critical slowing down\" (the increase in autocorrelation, $\\phi \\to 1$, and variance) near a tipping point is a major topic in climate science, ecology, and complex systems theory. All principles are well-established.\n-   **Well-Posed:** The problem provides a complete set of definitions and asks for specific, derivable quantities (bias, variance) and a subsequent optimization (minimization of Mean Squared Error). The assumptions about large $w$ guide the necessary approximations, ensuring that a unique, meaningful solution can be found.\n-   **Objective:** The problem is stated using formal mathematical language and definitions, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is consistent, scientifically sound, and well-posed. The solution process may proceed.\n\n---\n\nThe solution is derived in three parts as requested.\n\n### 1. Derivation of the Deterministic Bias\nThe rolling-window variance estimator is $W_{w}(\\tau)$. We seek its expectation, $E[W_{w}(\\tau)]$. Let the window of observation indices be $t \\in \\{\\tau-w+1, \\dots, \\tau\\}$.\n$$\nX_{t} - \\bar{X}_{w}(\\tau) = \\left(\\mu(t) + Y_{t}\\right) - \\left(\\bar{\\mu}_{w}(\\tau) + \\bar{Y}_{w}(\\tau)\\right) = \\left(\\mu(t) - \\bar{\\mu}_{w}(\\tau)\\right) + \\left(Y_{t} - \\bar{Y}_{w}(\\tau)\\right)\n$$\nwhere $\\bar{\\mu}_{w}(\\tau)$ and $\\bar{Y}_{w}(\\tau)$ are the windowed averages of the mean and noise components, respectively. Squaring this expression and taking the expectation:\n$$\nE\\left[\\left(X_{t} - \\bar{X}_{w}(\\tau)\\right)^2\\right] = E\\left[\\left(\\mu(t) - \\bar{\\mu}_{w}(\\tau)\\right)^2 + 2\\left(\\mu(t) - \\bar{\\mu}_{w}(\\tau)\\right)\\left(Y_{t} - \\bar{Y}_{w}(\\tau)\\right) + \\left(Y_{t} - \\bar{Y}_{w}(\\tau)\\right)^2\\right]\n$$\nSince $\\mu(t)$ is deterministic and $Y_t$ is a zero-mean process ($E[Y_t] = 0$, thus $E[\\bar{Y}_{w}(\\tau)]=0$), the expectation of the cross-term is zero.\n$$\nE\\left[\\left(X_{t} - \\bar{X}_{w}(\\tau)\\right)^2\\right] = \\left(\\mu(t) - \\bar{\\mu}_{w}(\\tau)\\right)^2 + E\\left[\\left(Y_{t} - \\bar{Y}_{w}(\\tau)\\right)^2\\right]\n$$\nThe expectation of the estimator is the average of this quantity over the window:\n$$\nE[W_{w}(\\tau)] = \\frac{1}{w}\\sum_{t=\\tau-w+1}^{\\tau} E\\left[\\left(X_{t} - \\bar{X}_{w}(\\tau)\\right)^2\\right] = \\frac{1}{w}\\sum_{t=\\tau-w+1}^{\\tau} \\left(\\mu(t) - \\bar{\\mu}_{w}(\\tau)\\right)^2 + E\\left[\\frac{1}{w}\\sum_{t=\\tau-w+1}^{\\tau}\\left(Y_{t} - \\bar{Y}_{w}(\\tau)\\right)^2\\right]\n$$\nThe problem asks for the deterministic bias induced by the linear drift. This corresponds to the first term, which is the sample variance of the deterministic mean component within the window. Let's calculate this term. Let's re-index the window from $k=1$ to $w$, where $t_k = \\tau - w + k$.\nThe mean within the window is $\\mu(t_k) = \\mu_0 + \\gamma(\\tau-w+k)$.\nThe average mean is $\\bar{\\mu}_{w}(\\tau) = \\frac{1}{w}\\sum_{k=1}^{w} \\mu(t_k) = \\mu_0 + \\frac{\\gamma}{w}\\sum_{k=1}^{w}(\\tau-w+k) = \\mu_0 + \\gamma(\\tau-w) + \\frac{\\gamma}{w}\\frac{w(w+1)}{2} = \\mu_0 + \\gamma(\\tau - \\frac{w-1}{2})$.\nThe deviation from the average mean is $\\mu(t_k) - \\bar{\\mu}_{w} = \\gamma(\\tau-w+k) - \\gamma(\\tau - \\frac{w-1}{2}) = \\gamma(k - w + \\frac{w-1}{2}) = \\gamma(k - \\frac{w+1}{2})$.\nThe sum of squares of these deviations is:\n$$\n\\sum_{k=1}^{w} \\left(\\mu(t_k) - \\bar{\\mu}_{w}\\right)^2 = \\sum_{k=1}^{w} \\gamma^2 \\left(k - \\frac{w+1}{2}\\right)^2 = \\gamma^2 \\sum_{k=1}^{w} \\left(k - \\bar{k}\\right)^2\n$$\nwhere $\\bar{k} = (w+1)/2$ is the mean of the indices $1, ..., w$. The sum of squared deviations from the mean for the first $w$ integers is a standard result: $\\sum_{k=1}^{w} (k - \\bar{k})^2 = \\frac{w(w^2-1)}{12}$.\nThe bias induced by the drift is therefore:\n$$\n\\operatorname{Bias}_{\\text{drift}}(w) = \\frac{1}{w} \\sum_{k=1}^{w} \\left(\\mu(t_k) - \\bar{\\mu}_{w}\\right)^2 = \\frac{1}{w} \\gamma^2 \\frac{w(w^2-1)}{12} = \\frac{\\gamma^2 (w^2-1)}{12}\n$$\nFor large $w$, this bias is approximated by its leading term:\n$$\n\\operatorname{Bias}_{\\text{drift}}(w) \\approx \\frac{\\gamma^2 w^2}{12}\n$$\n\n### 2. Derivation of the Estimator Variance\nWe need to find $\\operatorname{Var}[W_{w}(\\tau)]$. For large $w$, the estimator's variance is dominated by the fluctuations of the noise term $Y_t$. The contribution from the trend $\\mu(t)$ is deterministic and primarily adds to the bias, not the variance. Furthermore, for large $w$, the sample mean $\\bar{Y}_w(\\tau)$ converges to the true mean $E[Y_t]=0$, so we can approximate the estimator by ignoring the mean subtraction for the variance calculation:\n$$\nW_{w}(\\tau) \\approx \\frac{1}{w} \\sum_{t} Y_t^2\n$$\nThe variance is then given by the variance of the sample mean of the squared process $Z_t = Y_t^2$. For a stationary process, the variance of the sample mean over a large window $w$ is asymptotically:\n$$\n\\operatorname{Var}\\left[\\frac{1}{w}\\sum_{t=1}^{w} Z_t\\right] \\approx \\frac{1}{w} \\sum_{k=-\\infty}^{\\infty} \\operatorname{Cov}(Z_t, Z_{t+k})\n$$\nWe need the autocovariance of the squared Gaussian process, $\\operatorname{Cov}(Y_t^2, Y_{t+k}^2)$. Since $Y_t$ is a zero-mean Gaussian process, we use Isserlis' theorem for the fourth moment:\n$$\nE[Y_t^2 Y_{t+k}^2] = E[Y_t Y_t Y_{t+k} Y_{t+k}] = E[Y_t^2]E[Y_{t+k}^2] + 2 \\left(E[Y_t Y_{t+k}]\\right)^2\n$$\nThe covariance is then:\n$$\n\\operatorname{Cov}(Y_t^2, Y_{t+k}^2) = E[Y_t^2 Y_{t+k}^2] - E[Y_t^2]E[Y_{t+k}^2] = 2 \\left(E[Y_t Y_{t+k}]\\right)^2 = 2 \\left(\\operatorname{Cov}(Y_t, Y_{t+k})\\right)^2\n$$\nFor a stationary AR(1) process, the autocovariance is $\\operatorname{Cov}(Y_t, Y_{t+k}) = \\sigma_Y^2 \\phi^{|k|}$.\nThus, $\\operatorname{Cov}(Y_t^2, Y_{t+k}^2) = 2 (\\sigma_Y^2 \\phi^{|k|})^2 = 2 \\sigma_Y^4 \\phi^{2|k|}$.\nNow, we sum this over all integer lags $k$:\n$$\n\\sum_{k=-\\infty}^{\\infty} \\operatorname{Cov}(Y_t^2, Y_{t+k}^2) = \\sum_{k=-\\infty}^{\\infty} 2 \\sigma_Y^4 \\phi^{2|k|} = 2 \\sigma_Y^4 \\left(1 + 2\\sum_{k=1}^{\\infty} (\\phi^2)^k\\right)\n$$\nThe geometric series sum is $\\sum_{k=1}^{\\infty} (\\phi^2)^k = \\frac{\\phi^2}{1-\\phi^2}$, since $|\\phi|<1 \\implies \\phi^2<1$.\n$$\n\\sum_{k=-\\infty}^{\\infty} \\operatorname{Cov}(Y_t^2, Y_{t+k}^2) = 2 \\sigma_Y^4 \\left(1 + \\frac{2\\phi^2}{1-\\phi^2}\\right) = 2 \\sigma_Y^4 \\left(\\frac{1-\\phi^2+2\\phi^2}{1-\\phi^2}\\right) = 2 \\sigma_Y^4 \\frac{1+\\phi^2}{1-\\phi^2}\n$$\nThe variance of the estimator is then:\n$$\n\\operatorname{Var}[W_w(\\tau)] \\approx \\frac{1}{w} \\left(2 \\sigma_Y^4 \\frac{1+\\phi^2}{1-\\phi^2}\\right)\n$$\nFinally, we substitute $\\sigma_Y^2 = \\frac{\\sigma_\\varepsilon^2}{1-\\phi^2}$:\n$$\n\\operatorname{Var}[W_w(\\tau)] \\approx \\frac{2}{w} \\left(\\frac{\\sigma_\\varepsilon^2}{1-\\phi^2}\\right)^2 \\frac{1+\\phi^2}{1-\\phi^2} = \\frac{1}{w} \\frac{2 \\sigma_\\varepsilon^4 (1+\\phi^2)}{(1-\\phi^2)^3}\n$$\n\n### 3. Mean Squared Error Minimization\nThe Mean Squared Error (MSE) is the sum of the variance and the squared bias. Using the large-$w$ approximations for both terms:\n$$\n\\operatorname{MSE}(w) \\approx \\operatorname{Bias}(w)^2 + \\operatorname{Var}[W_w(\\tau)] \\approx \\left(\\frac{\\gamma^2 w^2}{12}\\right)^2 + \\frac{1}{w} \\frac{2 \\sigma_\\varepsilon^4 (1+\\phi^2)}{(1-\\phi^2)^3}\n$$\nLet's\n$$\n\\operatorname{MSE}(w) \\approx \\frac{\\gamma^4}{144} w^4 + \\frac{C}{w}\n$$\nwhere $C = \\frac{2 \\sigma_\\varepsilon^4 (1+\\phi^2)}{(1-\\phi^2)^3}$ is a constant with respect to $w$. To find the optimal window length $w^\\star$ that minimizes the MSE, we differentiate with respect to $w$ and set the result to zero. We treat $w$ as a continuous variable for this optimization.\n$$\n\\frac{d}{dw}\\operatorname{MSE}(w) = \\frac{\\gamma^4}{144} (4 w^3) - \\frac{C}{w^2} = \\frac{\\gamma^4}{36} w^3 - \\frac{C}{w^2}\n$$\nSetting the derivative to zero:\n$$\n\\frac{\\gamma^4}{36} (w^\\star)^3 = \\frac{C}{(w^\\star)^2} \\implies (w^\\star)^5 = \\frac{36 C}{\\gamma^4}\n$$\nSubstituting the expression for $C$:\n$$\n(w^\\star)^5 = \\frac{36}{\\gamma^4} \\left(\\frac{2 \\sigma_\\varepsilon^4 (1+\\phi^2)}{(1-\\phi^2)^3}\\right) = \\frac{72 \\sigma_\\varepsilon^4 (1+\\phi^2)}{\\gamma^4 (1-\\phi^2)^3}\n$$\nSolving for $w^\\star$ by taking the fifth root gives the asymptotically optimal window length:\n$$\nw^\\star = \\left(\\frac{72 \\sigma_\\varepsilon^4 (1+\\phi^2)}{\\gamma^4 (1-\\phi^2)^3}\\right)^{1/5}\n$$\nThis expression can be rearranged as:\n$$\nw^\\star = \\left(\\frac{72(1+\\phi^2)}{(1-\\phi^2)^3}\\right)^{1/5} \\left(\\frac{\\sigma_\\varepsilon}{\\gamma}\\right)^{4/5}\n$$\nThe gamma term appears as $\\gamma^4$, so its sign does not affect the result.",
            "answer": "$$\\boxed{\\left(\\frac{72 \\sigma_{\\varepsilon}^4 (1+\\phi^2)}{\\gamma^4 (1-\\phi^2)^3}\\right)^{\\frac{1}{5}}}$$"
        }
    ]
}