{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of four-dimensional variational data assimilation hinges on the adjoint method for gradient computation. This foundational exercise guides you through the derivation of the first-order optimality conditions for weak-constraint 4D-Var using the method of Lagrange multipliers . By completing this derivation, you will uncover the structure of the adjoint model, its backward-in-time integration, and the explicit relationship between the model error and the adjoint variables, which are central to the entire framework.",
            "id": "3426006",
            "problem": "Consider a discrete-time, finite-horizon data assimilation problem formulated as weak-constraint four-dimensional variational (4D-Var) data assimilation. Let the state be $x_k \\in \\mathbb{R}^{n}$ at discrete times $k = 0, 1, \\dots, N$, and suppose the dynamics are given by a possibly nonlinear model with additive model error,\n$$\nx_{k+1} = m_k(x_k) + \\eta_k,\n$$\nwhere $m_k: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is continuously differentiable and $\\eta_k \\in \\mathbb{R}^{n}$ is the model error at time $k$. Observations $y_k \\in \\mathbb{R}^{p_k}$ are related to the state through an observation operator $H_k: \\mathbb{R}^{n} \\to \\mathbb{R}^{p_k}$, assumed differentiable, with additive zero-mean Gaussian observational errors having covariance $R_k \\in \\mathbb{R}^{p_k \\times p_k}$, symmetric positive definite. The model error is assumed zero-mean Gaussian with covariance $Q_k \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite. The prior (background) state $x_b \\in \\mathbb{R}^{n}$ has error covariance $B \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite.\n\nDefine the weak-constraint 4D-Var cost function\n$$\nJ(x_0, \\{\\eta_k\\}_{k=0}^{N-1}) = \\tfrac{1}{2} (x_0 - x_b)^{\\top} B^{-1} (x_0 - x_b) + \\tfrac{1}{2} \\sum_{k=0}^{N} \\left(H_k(x_k) - y_k\\right)^{\\top} R_k^{-1} \\left(H_k(x_k) - y_k\\right) + \\tfrac{1}{2} \\sum_{k=0}^{N-1} \\eta_k^{\\top} Q_k^{-1} \\eta_k,\n$$\nwith the state sequence $\\{x_k\\}$ constrained by the model $x_{k+1} = m_k(x_k) + \\eta_k$.\n\nStarting from these definitions and using first principles of constrained optimization via Lagrange multipliers and calculus of variations, derive the first-order optimality conditions of weak-constraint 4D-Var with respect to both $x_0$ and $\\{\\eta_k\\}$, including:\n- the adjoint recursion in terms of the Jacobians of the model and observation operators,\n- the boundary (terminal) adjoint condition,\n- and the stationarity condition with respect to the model error controls $\\{\\eta_k\\}$.\n\nExpress the Jacobian of the model as $M_k(x_k) = \\nabla m_k(x_k) \\in \\mathbb{R}^{n \\times n}$ and the Jacobian of the observation operator as $H_k'(x_k) = \\nabla H_k(x_k) \\in \\mathbb{R}^{p_k \\times n}$, and define the innovation $d_k = H_k(x_k) - y_k$. Conclude by providing the closed-form analytic expression for the optimal control update $\\eta_k$ in terms of $Q_k$ and the adjoint variable at time $k+1$.\n\nYour final output must be a single symbolic mathematical expression for the control update. No numerical computation or rounding is required.",
            "solution": "The problem requires the derivation of the first-order optimality conditions for a weak-constraint four-dimensional variational (4D-Var) data assimilation problem using the method of Lagrange multipliers. The goal is to find the initial state $x_0$ and the sequence of model errors $\\{\\eta_k\\}_{k=0}^{N-1}$ that minimize a given cost function $J$, subject to the constraints imposed by the model dynamics.\n\nThe cost function to be minimized is:\n$$\nJ(x_0, \\{\\eta_k\\}_{k=0}^{N-1}) = \\tfrac{1}{2} (x_0 - x_b)^{\\top} B^{-1} (x_0 - x_b) + \\tfrac{1}{2} \\sum_{k=0}^{N} \\left(H_k(x_k) - y_k\\right)^{\\top} R_k^{-1} \\left(H_k(x_k) - y_k\\right) + \\tfrac{1}{2} \\sum_{k=0}^{N-1} \\eta_k^{\\top} Q_k^{-1} \\eta_k\n$$\nThis minimization is subject to the model dynamics acting as constraints:\n$$\nx_{k+1} = m_k(x_k) + \\eta_k, \\quad \\text{for } k = 0, 1, \\dots, N-1\n$$\nTo solve this constrained optimization problem, we form the Lagrangian $\\mathcal{L}$ by augmenting the cost function with the constraints, weighted by a sequence of Lagrange multipliers (or adjoint variables) $\\lambda_{k+1} \\in \\mathbb{R}^n$ for $k = 0, \\dots, N-1$. We treat the state trajectory $\\{x_k\\}_{k=0}^N$ and the model errors $\\{\\eta_k\\}_{k=0}^{N-1}$ as independent variables in the formulation of the Lagrangian. The choice of sign for the Lagrange multiplier term is a convention; we adopt the one prevalent in control theory and data assimilation.\n$$\n\\mathcal{L} \\left(\\{x_k\\}_{k=0}^N, \\{\\eta_k\\}_{k=0}^{N-1}, \\{\\lambda_k\\}_{k=1}^N\\right) = J - \\sum_{k=0}^{N-1} \\lambda_{k+1}^{\\top} \\left( x_{k+1} - m_k(x_k) - \\eta_k \\right)\n$$\nThe first-order necessary conditions for an extremum are that the partial derivatives of the Lagrangian with respect to all its arguments must be zero. We proceed by computing these derivatives.\n\n1.  **Derivative with respect to the Lagrange multipliers $\\lambda_{k+1}$ (for $k=0, \\dots, N-1$):**\n    Taking the gradient of $\\mathcal{L}$ with respect to $\\lambda_{k+1}$ and setting it to zero recovers the model constraint equations:\n    $$\n    \\nabla_{\\lambda_{k+1}} \\mathcal{L} = -\\left( x_{k+1} - m_k(x_k) - \\eta_k \\right) = 0\n    $$\n    $$\n    \\implies x_{k+1} = m_k(x_k) + \\eta_k\n    $$\n    This confirms that at the optimum, the state trajectory must satisfy the model dynamics.\n\n2.  **Derivative with respect to the control variables $\\eta_k$ (for $k=0, \\dots, N-1$):**\n    We take the gradient of $\\mathcal{L}$ with respect to each model error vector $\\eta_k$. The terms in $\\mathcal{L}$ depending on a specific $\\eta_k$ are $\\tfrac{1}{2}\\eta_k^{\\top} Q_k^{-1} \\eta_k$ and $\\lambda_{k+1}^{\\top}\\eta_k$.\n    $$\n    \\nabla_{\\eta_k} \\mathcal{L} = \\nabla_{\\eta_k} \\left(\\tfrac{1}{2}\\eta_k^{\\top} Q_k^{-1} \\eta_k\\right) + \\nabla_{\\eta_k} \\left(\\lambda_{k+1}^{\\top}\\eta_k\\right) = Q_k^{-1} \\eta_k + \\lambda_{k+1} = 0\n    $$\n    This gives the **stationarity condition with respect to the model error controls $\\{\\eta_k\\}$**. Solving for $\\eta_k$ yields a closed-form expression for the optimal model error in terms of the adjoint variable:\n    $$\n    \\eta_k = -Q_k \\lambda_{k+1}\n    $$\n\n3.  **Derivative with respect to the terminal state $x_N$:**\n    The terms in $\\mathcal{L}$ that depend on $x_N$ are the final observation term $\\tfrac{1}{2}(H_N(x_N) - y_N)^{\\top} R_N^{-1} (H_N(x_N) - y_N)$ and the final constraint term $-\\lambda_N^{\\top}x_N$. Using the notation $d_k = H_k(x_k) - y_k$ and $H_k' = \\nabla H_k(x_k)$:\n    $$\n    \\nabla_{x_N} \\mathcal{L} = \\nabla_{x_N} \\left(\\tfrac{1}{2}d_N^{\\top}R_N^{-1}d_N\\right) - \\nabla_{x_N}\\left(\\lambda_N^{\\top}x_N\\right) = (H_N'(x_N))^{\\top} R_N^{-1} d_N - \\lambda_N = 0\n    $$\n    This yields the **boundary (terminal) adjoint condition**:\n    $$\n    \\lambda_N = (H_N'(x_N))^{\\top} R_N^{-1} d_N\n    $$\n\n4.  **Derivative with respect to the intermediate states $x_k$ (for $k=1, \\dots, N-1$):**\n    For a state $x_k$ in the middle of the trajectory, the relevant terms in $\\mathcal{L}$ are the observation term at time $k$, the constraint term linking $x_k$ to $x_{k-1}$ (i.e., $-\\lambda_k^{\\top}x_k$), and the constraint term linking $x_{k+1}$ to $x_k$ (i.e., $\\lambda_{k+1}^{\\top}m_k(x_k)$).\n    $$\n    \\nabla_{x_k} \\mathcal{L} = \\nabla_{x_k}\\left(\\tfrac{1}{2}d_k^{\\top}R_k^{-1}d_k\\right) - \\nabla_{x_k}\\left(\\lambda_k^{\\top}x_k\\right) + \\nabla_{x_k}\\left(\\lambda_{k+1}^{\\top}m_k(x_k)\\right) = 0\n    $$\n    Using the notation $M_k = \\nabla m_k(x_k)$:\n    $$\n    (H_k'(x_k))^{\\top} R_k^{-1} d_k - \\lambda_k + (M_k(x_k))^{\\top} \\lambda_{k+1} = 0\n    $$\n    Rearranging this equation provides the **adjoint recursion**:\n    $$\n    \\lambda_k = (M_k(x_k))^{\\top} \\lambda_{k+1} + (H_k'(x_k))^{\\top} R_k^{-1} d_k, \\quad \\text{for } k=N-1, \\dots, 1\n    $$\n    This relation demonstrates that the adjoint variable $\\lambda_k$ is propagated backward in time from the terminal condition at $k=N$.\n\n5.  **Derivative with respect to the initial state $x_0$**:\n    Finally, we compute the gradient with respect to the control variable $x_0$. The relevant terms are the background term, the observation term at $k=0$, and the first constraint term involving $m_0(x_0)$.\n    $$\n    \\nabla_{x_0} \\mathcal{L} = \\nabla_{x_0}\\left(\\tfrac{1}{2}(x_0-x_b)^{\\top}B^{-1}(x_0-x_b)\\right) + \\nabla_{x_0}\\left(\\tfrac{1}{2}d_0^{\\top}R_0^{-1}d_0\\right) + \\nabla_{x_0}\\left(\\lambda_1^{\\top}m_0(x_0)\\right) = 0\n    $$\n    $$\n    B^{-1}(x_0 - x_b) + (H_0'(x_0))^{\\top} R_0^{-1} d_0 + (M_0(x_0))^{\\top} \\lambda_1 = 0\n    $$\n    This equation is the stationarity condition for $x_0$, which represents the gradient of the cost function $J$ with respect to $x_0$. If we define an initial adjoint variable $\\lambda_0$ by extending the recursion, $\\lambda_0 = (M_0(x_0))^{\\top} \\lambda_1 + (H_0'(x_0))^{\\top} R_0^{-1} d_0$, the stationarity condition simplifies to $B^{-1}(x_0 - x_b) + \\lambda_0 = 0$.\n\nIn summary, the first-order optimality conditions comprise the forward model integration, the backward adjoint model integration, and the analytic expressions for the controls. The problem specifically asks for the expression for the optimal control update $\\eta_k$. As derived in step 2, this is given by the stationarity condition with respect to model error.\n\nThe required expression for the optimal control update $\\eta_k$ is a direct consequence of the stationarity condition $\\nabla_{\\eta_k} \\mathcal{L} = 0$. This condition, $Q_k^{-1} \\eta_k + \\lambda_{k+1} = 0$, is solved for $\\eta_k$ by premultiplying by the model error covariance matrix $Q_k$.",
            "answer": "$$\n\\boxed{\\eta_k = -Q_k \\lambda_{k+1}}\n$$"
        },
        {
            "introduction": "With the theoretical machinery in place, we now apply it to a minimalist system to build intuition. This problem presents a simple scenario where a perfect-model (strong-constraint) assumption is demonstrably inadequate to fit observations due to a systematic model bias . You will then solve the weak-constraint problem analytically, calculating the optimal constant model error that allows the system to reconcile the model dynamics with the observed data, illustrating the core strength of the weak-constraint approach.",
            "id": "3431076",
            "problem": "Consider a one-dimensional, discrete-time dynamical system over the window indexed by $k \\in \\{0,1,2\\}$ with state $x_k \\in \\mathbb{R}$ and observation operator $H$ given by $H x_k = x_k$. The data available to assimilation are the observations $y_0 = 0$, $y_1 = 1$, and $y_2 = 2$. The forecast model used within assimilation is the persistence model $M_k(x_k) = x_k$ so that the forecast constraint reads $x_{k+1} = x_k$ if no model error is allowed. The background (prior) for the initial condition is $x_0 \\sim \\mathcal{N}(x_b, \\sigma_b^2)$ with $x_b = 0$ and $\\sigma_b^2 = 1$. Observational errors at each time are independent and Gaussian with variance $\\sigma_o^2 = 1$.\n\n1. Explain why, under the strong-constraint four-dimensional variational assimilation (4D-Var), which assumes a perfect model $x_{k+1} = x_k$ and only estimates $x_0$, there exists no choice of $x_0$ that can fit all the observations $y_0, y_1, y_2$ exactly without violating the model constraint.\n\n2. Now adopt weak-constraint four-dimensional variational assimilation (4D-Var) by introducing an additive model-error sequence with the simplified parametric form $w_k \\equiv b$ for $k \\in \\{0,1\\}$, where $b \\in \\mathbb{R}$ is a constant bias over the window. Assume a zero-mean Gaussian prior for each $w_k$ with variance $\\sigma_q^2 = 1$, independent across time. Using the assumptions of Gaussian priors and likelihoods and independence across error sources, formulate the corresponding maximum a posteriori estimation problem for $(x_0, b)$, reduce it to a quadratic minimization in $(x_0, b)$, and solve for the unique minimizer. Compute the value of the optimal constant model-error bias $b^{\\star}$ for this specific dataset. Do not round your result; provide it in exact form.\n\nYour final answer should be the single value of $b^{\\star}$ with no units and no additional commentary.",
            "solution": "The problem asks for an analysis of a simple data assimilation scenario under two different frameworks: strong-constraint and weak-constraint four-dimensional variational assimilation (4D-Var).\n\nPart 1: Strong-Constraint 4D-Var\n\nIn the strong-constraint 4D-Var formulation, the forecast model is assumed to be perfect. The given forecast model is the persistence model, $M_k(x_k) = x_k$, which implies the model constraint $x_{k+1} = x_k$ for $k \\in \\{0, 1\\}$. The control variable in this setup is solely the initial state, $x_0$.\n\nUnder this perfect model assumption, the state of the system over the entire assimilation window is determined by the initial state:\n$$x_1 = M_0(x_0) = x_0$$\n$$x_2 = M_1(x_1) = x_1 = x_0$$\nTherefore, the model dynamics impose the strict constraint $x_0 = x_1 = x_2$.\n\nThe problem states that an exact fit to the observations would require the model state at each time $k$ to be equal to the corresponding observation $y_k$. The given observations are $y_0 = 0$, $y_1 = 1$, and $y_2 = 2$. An exact fit would thus imply:\n$$x_0 = y_0 = 0$$\n$$x_1 = y_1 = 1$$\n$$x_2 = y_2 = 2$$\n\nThese three conditions, $x_0 = 0$, $x_1 = 1$, and $x_2 = 2$, are in direct contradiction with the model constraint $x_0 = x_1 = x_2$. It is impossible for a single value of $x_0$ to simultaneously satisfy $x_0 = 0$, $x_0 = 1$, and $x_0 = 2$. Consequently, no choice of the initial state $x_0$ can perfectly fit all three observations without violating the perfect model constraint. Strong-constraint 4D-Var will find an optimal $x_0$ that minimizes a cost function balancing the misfit to observations and the misfit to the background, but it can never achieve zero misfit to all observations in this case.\n\nPart 2: Weak-Constraint 4D-Var\n\nIn the weak-constraint 4D-Var formulation, the model is allowed to be imperfect. This is achieved by introducing a model error term, $w_k$. The model dynamics are now given by:\n$$x_{k+1} = M_k(x_k) + w_k = x_k + w_k$$\nThe problem specifies a simplified parameterization for the model error, where it is a constant bias over the window: $w_k \\equiv b$ for $k \\in \\{0, 1\\}$. The control variables for the assimilation are now the initial state $x_0$ and the constant model error bias $b$.\n\nThe goal is to find the values of $(x_0, b)$ that maximize the posterior probability density, which, under the assumption of Gaussian errors, is equivalent to minimizing a quadratic cost function $J(x_0, b)$. The cost function is the sum of three terms: a background term ($J_b$), a model error term ($J_q$), and an observation term ($J_o$).\n$$J(x_0, b) = J_b(x_0) + J_q(b) + J_o(x_0, b)$$\n\n1.  The background term penalizes deviations of the initial state $x_0$ from the background estimate $x_b$:\n    $$J_b(x_0) = \\frac{1}{2\\sigma_b^2}(x_0 - x_b)^2$$\n    With $x_b = 0$ and $\\sigma_b^2 = 1$, this becomes $J_b(x_0) = \\frac{1}{2}x_0^2$.\n\n2.  The model error term penalizes the departure of the model error from its prior estimate (which is zero mean). Since the model error sequence is $w_0 = b$ and $w_1 = b$, and the prior for each $w_k$ is independent with variance $\\sigma_q^2$, this term is:\n    $$J_q(b) = \\sum_{k=0}^{1} \\frac{1}{2\\sigma_q^2}w_k^2 = \\frac{1}{2\\sigma_q^2}b^2 + \\frac{1}{2\\sigma_q^2}b^2 = \\frac{b^2}{\\sigma_q^2}$$\n    With $\\sigma_q^2 = 1$, this becomes $J_q(b) = b^2$.\n\n3.  The observation term penalizes the misfit between the model trajectory and the observations. First, we express the trajectory $x_1, x_2$ in terms of the control variables $x_0, b$:\n    $$x_1 = x_0 + w_0 = x_0 + b$$\n    $$x_2 = x_1 + w_1 = (x_0 + b) + b = x_0 + 2b$$\n    The observation term is:\n    $$J_o(x_0, b) = \\sum_{k=0}^{2} \\frac{1}{2\\sigma_o^2}(y_k - Hx_k)^2$$\n    With observation operator $H x_k = x_k$, observations $y_0 = 0$, $y_1 = 1$, $y_2 = 2$, and variance $\\sigma_o^2 = 1$, this becomes:\n    $$J_o(x_0, b) = \\frac{1}{2} \\left[ (y_0 - x_0)^2 + (y_1 - x_1)^2 + (y_2 - x_2)^2 \\right]$$\n    $$J_o(x_0, b) = \\frac{1}{2} \\left[ (0 - x_0)^2 + (1 - (x_0 + b))^2 + (2 - (x_0 + 2b))^2 \\right]$$\n\nCombining these terms, the total cost function is:\n$$J(x_0, b) = \\frac{1}{2}x_0^2 + b^2 + \\frac{1}{2} \\left[ x_0^2 + (1 - x_0 - b)^2 + (2 - x_0 - 2b)^2 \\right]$$\nTo find the optimal values $(x_0^\\star, b^\\star)$ that minimize $J$, we compute the gradient of $J$ with respect to $x_0$ and $b$ and set it to zero.\n\nPartial derivative with respect to $x_0$:\n$$\\frac{\\partial J}{\\partial x_0} = x_0 + \\frac{1}{2} \\left[ 2x_0 + 2(1 - x_0 - b)(-1) + 2(2 - x_0 - 2b)(-1) \\right]$$\n$$\\frac{\\partial J}{\\partial x_0} = x_0 + x_0 - (1 - x_0 - b) - (2 - x_0 - 2b)$$\n$$\\frac{\\partial J}{\\partial x_0} = 2x_0 - 1 + x_0 + b - 2 + x_0 + 2b = 4x_0 + 3b - 3$$\n\nPartial derivative with respect to $b$:\n$$\\frac{\\partial J}{\\partial b} = 2b + \\frac{1}{2} \\left[ 2(1 - x_0 - b)(-1) + 2(2 - x_0 - 2b)(-2) \\right]$$\n$$\\frac{\\partial J}{\\partial b} = 2b - (1 - x_0 - b) - 2(2 - x_0 - 2b)$$\n$$\\frac{\\partial J}{\\partial b} = 2b - 1 + x_0 + b - 4 + 2x_0 + 4b = 3x_0 + 7b - 5$$\n\nSetting the partial derivatives to zero yields a system of linear equations for $(x_0, b)$:\n1) $4x_0 + 3b = 3$\n2) $3x_0 + 7b = 5$\n\nWe can solve this system. From equation (1), we express $x_0$ in terms of $b$:\n$$4x_0 = 3 - 3b \\implies x_0 = \\frac{3 - 3b}{4}$$\nSubstitute this expression for $x_0$ into equation (2):\n$$3\\left(\\frac{3 - 3b}{4}\\right) + 7b = 5$$\nMultiply the entire equation by $4$ to eliminate the fraction:\n$$3(3 - 3b) + 28b = 20$$\n$$9 - 9b + 28b = 20$$\n$$19b = 11$$\n$$b^\\star = \\frac{11}{19}$$\nThis is the optimal constant model-error bias. The problem asks for this value. The positive definite nature of the Hessian matrix guarantees this is a unique minimum.",
            "answer": "$$\\boxed{\\frac{11}{19}}$$"
        },
        {
            "introduction": "From theory to practice, the final step is implementation. A common and critical source of error in any variational data assimilation system is an incorrect implementation of the gradient, often derived from a complex adjoint model. This hands-on coding exercise directs you to perform a \"gradient check,\" a fundamental verification technique where the analytically derived gradient is compared against a numerical finite-difference approximation . Successfully implementing this test is a crucial milestone in developing robust and reliable data assimilation software.",
            "id": "3931478",
            "problem": "Consider weak-constraint Four-Dimensional Variational Data Assimilation (4D-Var), where the model dynamics include a time-dependent additive model-error forcing. In a one-dimensional setting, the discrete-time prognostic model is given by the linear recursion $x_{k+1} = a \\, x_k + w_k$ for $k \\in \\{0,1,\\dots,N-1\\}$, where $x_k \\in \\mathbb{R}$ is the model state, $a \\in \\mathbb{R}$ is a known scalar model operator, and $w_k \\in \\mathbb{R}$ is the model-error forcing at time index $k$. The observation model is $y_k = H \\, x_k + \\eta_k$, with $H \\in \\mathbb{R}$ a known scalar observation operator, $y_k \\in \\mathbb{R}$ observed data, and $\\eta_k \\in \\mathbb{R}$ observation noise. Assume Gaussian error models with known positive variances and independent errors, so that the standard variational cost functional is built from a background penalty on the initial condition, an observation misfit penalty over the window, and a model-error penalty.\n\nStarting from these fundamental bases:\n- Linear discrete model evolution law $x_{k+1} = a \\, x_k + w_k$.\n- Linear observation model $y_k = H \\, x_k + \\eta_k$.\n- Gaussian error models with independent components, implying quadratic penalties in the variational cost.\n\nDefine the weak-constraint 4D-Var cost function for the control vector $v = (x_0, w_0, w_1, \\dots, w_{N-1}) \\in \\mathbb{R}^{N+1}$ as\n$$\nJ(v) = \\tfrac{1}{2}\\,\\frac{(x_0 - x_b)^2}{B} + \\tfrac{1}{2}\\,\\sum_{k=0}^{N} \\frac{(H\\,x_k(v) - y_k)^2}{R} + \\tfrac{1}{2}\\,\\sum_{k=0}^{N-1} \\frac{w_k^2}{Q},\n$$\nwhere $x_b \\in \\mathbb{R}$ is the background initial condition, $B > 0$ is the background-variance scalar, $R > 0$ is the observation-variance scalar, $Q > 0$ is the model-error variance scalar, and $x_k(v)$ denotes the state at time $k$ obtained by forward iteration of the model from $x_0$ under the sequence $\\{w_k\\}$.\n\nYour task is to implement a numerical gradient-correctness test for $J(v)$ using random perturbations and central finite differences. The test must compare the directional derivative given by the inner product of the analytically derived gradient $\\nabla J(v)$ with a random direction $p$ against the central finite-difference approximation:\n$$\nD_{\\text{FD}}(v;p,\\varepsilon) = \\frac{J(v+\\varepsilon p) - J(v-\\varepsilon p)}{2\\,\\varepsilon}.\n$$\nThe relative error for a given $\\varepsilon$ is\n$$\nE(v;p,\\varepsilon) = \\frac{\\left|\\langle \\nabla J(v), p \\rangle - D_{\\text{FD}}(v;p,\\varepsilon)\\right|}{\\max\\left(1,\\left|\\langle \\nabla J(v), p \\rangle\\right|,\\left|D_{\\text{FD}}(v;p,\\varepsilon)\\right|\\right)}.\n$$\n\nYou must construct the analytical gradient $\\nabla J(v)$ by deriving and implementing the discrete adjoint of the one-dimensional linear model from first principles. Do not use automatic differentiation or external algorithmic differentiation tools. Use random perturbation directions $p$ normalized to unit Euclidean norm. For each test case, compute $E(v;p,\\varepsilon)$ over a prescribed set of $\\varepsilon$ values and report the maximum relative error over that set.\n\nThe program must be self-contained and generate synthetic observations $y_k$ by first constructing a reference truth trajectory $x_k^{\\text{true}}$ under the same model $x_{k+1}^{\\text{true}} = a\\,x_k^{\\text{true}} + w_k^{\\text{true}}$, with $w_k^{\\text{true}} = 0$ for all $k$ and a specified true initial state $x_0^{\\text{true}}$, then drawing independent Gaussian observation noise $\\eta_k$ with variance $R$, so $y_k = H\\,x_k^{\\text{true}} + \\eta_k$. For reproducibility, random draws for $v$ and $p$ as well as observation noise must use the provided seeds.\n\nImplement the following test suite, each item specifying $(a, N, H, B, R, Q, x_b, x_0^{\\text{true}}, \\text{seed}, \\text{epsilons})$:\n- Case $1$: $(a = 0.9, N = 20, H = 1.0, B = 1.0, R = 0.5, Q = 0.1, x_b = 0.0, x_0^{\\text{true}} = 1.5, \\text{seed} = 42, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$.\n- Case $2$: $(a = 1.0, N = 30, H = 1.0, B = 0.2, R = 0.8, Q = 0.05, x_b = 0.0, x_0^{\\text{true}} = 1.0, \\text{seed} = 7, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$.\n- Case $3$: $(a = 0.5, N = 10, H = 1.0, B = 2.0, R = 0.3, Q = 10^{-3}, x_b = 0.0, x_0^{\\text{true}} = 0.5, \\text{seed} = 99, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$.\n- Case $4$: $(a = 0.9, N = 15, H = 0.0, B = 1.5, R = 1.0, Q = 0.2, x_b = 0.0, x_0^{\\text{true}} = 2.0, \\text{seed} = 5, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$.\n- Case $5$: $(a = 0.99, N = 40, H = 1.0, B = 0.5, R = 10^{-4}, Q = 0.2, x_b = 0.0, x_0^{\\text{true}} = 1.2, \\text{seed} = 123, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$.\n\nFor each case, construct a baseline control $v$ and a random direction $p$ as independent draws from a standard normal distribution using the specified seed, and normalize $p$ to have unit Euclidean norm. Compute and return the maximum relative error $\\max_{\\varepsilon \\in \\text{epsilons}} E(v;p,\\varepsilon)$ as a floating-point number.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases, for example $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_5]$. No physical units or angles are involved, so no unit conversions are required. The answer for each test case must be a floating-point number.",
            "solution": "The core of the task is to derive the analytical gradient $\\nabla J(v)$ of the cost function $J(v)$ with respect to the control vector $v = (x_0, w_0, \\dots, w_{N-1})$ and then implement it. The most efficient method for this derivation is the adjoint method.\n\nThe cost function is:\n$$\nJ(v) = \\underbrace{\\tfrac{1}{2}\\,\\frac{(x_0 - x_b)^2}{B}}_{J_b} + \\underbrace{\\tfrac{1}{2}\\,\\sum_{k=0}^{N} \\frac{(H\\,x_k - y_k)^2}{R}}_{J_o} + \\underbrace{\\tfrac{1}{2}\\,\\sum_{k=0}^{N-1} \\frac{w_k^2}{Q}}_{J_q}\n$$\nThe state trajectory $x_1, \\dots, x_N$ depends on the control vector $v$ through the linear model dynamics $x_{k+1} = a x_k + w_k$.\n\n**Adjoint Model Derivation**\n\nWe derive the adjoint model to efficiently compute the gradient of the cost function with respect to the state trajectory. The adjoint state variable, $\\lambda_k$, propagates gradient information backward in time. The recurrence relation for the adjoint is derived from the chain rule.\n\nThe forcing for the adjoint model at each time step $k$ comes from the gradient of the observation cost term $J_o$ with respect to the state $x_k$:\n$$\nf_k = \\frac{\\partial J_o}{\\partial x_k} = \\frac{H}{R} (H x_k - y_k)\n$$\nThe adjoint equations are then:\n1.  **Terminal Condition (at $k=N$)**: The adjoint variable at the end of the window is equal to the forcing at that time.\n    $$\n    \\lambda_N = f_N = \\frac{H}{R} (H x_N - y_N)\n    $$\n2.  **Backward Recurrence (for $k = N-1, \\dots, 0$)**: The adjoint variable $\\lambda_k$ is calculated from the next step's adjoint, $\\lambda_{k+1}$, propagated backward by the transpose of the tangent linear model (which is just $a$ in this scalar case), and added to the local forcing $f_k$.\n    $$\n    \\lambda_k = a^T \\lambda_{k+1} + f_k = a \\lambda_{k+1} + \\frac{H}{R} (H x_k - y_k)\n    $$\n\n**Gradient Calculation**\n\nWith the adjoint variables computed, the components of the gradient $\\nabla J(v)$ can be assembled. Each component is the sum of the partial derivative of $J$ with respect to the control variable (direct dependence) and the indirect dependence mediated by the adjoint variables.\n\n1.  **Gradient with respect to initial condition $x_0$**: This combines the derivative of the background term $J_b$ and the adjoint variable $\\lambda_0$, which carries the influence of $x_0$ on the entire trajectory's cost.\n    $$\n    \\frac{\\partial J}{\\partial x_0} = \\frac{\\partial J_b}{\\partial x_0} + \\lambda_0 = \\frac{x_0 - x_b}{B} + \\lambda_0\n    $$\n2.  **Gradient with respect to model error $w_k$**: This combines the derivative of the model error penalty term $J_q$ and the adjoint variable $\\lambda_{k+1}$, which represents the sensitivity of the cost from time $k+1$ onwards to a perturbation in $w_k$.\n    $$\n    \\frac{\\partial J}{\\partial w_k} = \\frac{\\partial J_q}{\\partial w_k} + \\lambda_{k+1} = \\frac{w_k}{Q} + \\lambda_{k+1} \\quad \\text{for } k=0, \\dots, N-1\n    $$\n\nThe full gradient vector $\\nabla J(v) \\in \\mathbb{R}^{N+1}$ is therefore:\n$$\n\\nabla J(v) = \\begin{pmatrix}\n\\frac{x_0 - x_b}{B} + \\lambda_0 \\\\\n\\frac{w_0}{Q} + \\lambda_1 \\\\\n\\vdots \\\\\n\\frac{w_{N-1}}{Q} + \\lambda_N\n\\end{pmatrix}\n$$\n\n**Implementation**\nThe numerical implementation involves a two-pass procedure:\n1.  **Forward Pass**: Given a control vector $v$, run the prognostic model $x_{k+1} = a x_k + w_k$ forward to compute and store the state trajectory $x_0, \\dots, x_N$.\n2.  **Backward Pass**: Integrate the adjoint model backward from $k=N$ to $k=0$ to compute the adjoint variables $\\lambda_0, \\dots, \\lambda_N$.\n3.  **Gradient Assembly**: Use the states, adjoints, and control variables to compute the gradient components as derived above.\n\nThis analytically derived gradient is then compared against a central finite-difference approximation to verify its correctness. The Python code below implements this entire procedure, including data generation, the cost and gradient functions, and the verification test for all specified cases.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient correctness test for all specified cases.\n    \"\"\"\n\n    test_cases = [\n        # (a, N, H, B, R, Q, x_b, x_0_true, seed, epsilons)\n        (0.9, 20, 1.0, 1.0, 0.5, 0.1, 0.0, 1.5, 42, [1e-2, 1e-4, 1e-6]),\n        (1.0, 30, 1.0, 0.2, 0.8, 0.05, 0.0, 1.0, 7, [1e-2, 1e-4, 1e-6]),\n        (0.5, 10, 1.0, 2.0, 0.3, 1e-3, 0.0, 0.5, 99, [1e-2, 1e-4, 1e-6]),\n        (0.9, 15, 0.0, 1.5, 1.0, 0.2, 0.0, 2.0, 5, [1e-2, 1e-4, 1e-6]),\n        (0.99, 40, 1.0, 0.5, 1e-4, 0.2, 0.0, 1.2, 123, [1e-2, 1e-4, 1e-6]),\n    ]\n\n    results = []\n    for case in test_cases:\n        max_error = run_gradient_test(*case)\n        results.append(max_error)\n\n    # This part is for local execution to generate the answer.\n    # The final output is just the string.\n    # print(f\"[{','.join(map(str, results))}]\")\n\ndef run_forward_model(v, a, N):\n    \"\"\"\n    Runs the forward prognostic model to get the state trajectory.\n\n    Args:\n        v (np.ndarray): Control vector (x_0, w_0, ..., w_{N-1}).\n        a (float): Model operator.\n        N (int): Number of time steps.\n\n    Returns:\n        np.ndarray: State trajectory x_0, ..., x_N.\n    \"\"\"\n    x = np.zeros(N + 1)\n    x0 = v[0]\n    w = v[1:]\n    x[0] = x0\n    for k in range(N):\n        x[k + 1] = a * x[k] + w[k]\n    return x\n\ndef compute_J(v, a, N, H, B, R, Q, x_b, y):\n    \"\"\"\n    Computes the weak-constraint 4D-Var cost function J(v).\n\n    Args:\n        v, a, N, H, B, R, Q, x_b: As defined in the problem.\n        y (np.ndarray): Observation vector.\n\n    Returns:\n        float: The value of the cost function J(v).\n    \"\"\"\n    x0 = v[0]\n    w = v[1:]\n\n    # Forward model run\n    x = run_forward_model(v, a, N)\n\n    # Background penalty\n    J_b = 0.5 * ((x0 - x_b)**2) / B\n\n    # Observation penalty\n    J_o = 0.5 * np.sum((H * x - y)**2) / R\n    \n    # Model error penalty\n    J_q = 0.5 * np.sum(w**2) / Q\n\n    return J_b + J_o + J_q\n\ndef compute_grad_J(v, a, N, H, B, R, Q, x_b, y):\n    \"\"\"\n    Computes the gradient of the cost function, grad J(v), using the adjoint method.\n\n    Args:\n        v, a, N, H, B, R, Q, x_b: As defined in the problem.\n        y (np.ndarray): Observation vector.\n\n    Returns:\n        np.ndarray: The gradient vector nabla J(v).\n    \"\"\"\n    x0 = v[0]\n    w = v[1:]\n    grad_v = np.zeros(N + 1)\n\n    # 1. Forward pass: Get state trajectory x\n    x = run_forward_model(v, a, N)\n\n    # 2. Backward pass: Compute adjoint variables lambda\n    lambdas = np.zeros(N + 1)\n    \n    # Terminal condition for adjoint\n    lambdas[N] = (H / R) * (H * x[N] - y[N])\n\n    # Backward recurrence\n    for k in range(N - 1, -1, -1):\n        forcing = (H / R) * (H * x[k] - y[k])\n        lambdas[k] = a * lambdas[k + 1] + forcing\n\n    # 3. Gradient assembly\n    # Gradient component for x_0\n    grad_v[0] = (x0 - x_b) / B + lambdas[0]\n\n    # Gradient components for w_k\n    grad_v[1:] = w / Q + lambdas[1:]\n\n    return grad_v\n\ndef run_gradient_test(a, N, H, B, R, Q, x_b, x0_true, seed, epsilons):\n    \"\"\"\n    Performs the gradient correctness test for a single case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate synthetic observations\n    x_true = np.zeros(N + 1)\n    x_true[0] = x0_true\n    for k in range(N):\n        x_true[k+1] = a * x_true[k] # w_true is zero\n    \n    noise = rng.normal(loc=0.0, scale=np.sqrt(R), size=N + 1)\n    y = H * x_true + noise\n\n    # Generate random control vector v and perturbation p\n    v = rng.normal(loc=0.0, scale=1.0, size=N + 1)\n    p = rng.normal(loc=0.0, scale=1.0, size=N + 1)\n    p /= np.linalg.norm(p) # Normalize perturbation\n\n    # Compute analytical directional derivative\n    grad_J = compute_grad_J(v, a, N, H, B, R, Q, x_b, y)\n    dir_deriv_analytic = np.dot(grad_J, p)\n\n    errors = []\n    for eps in epsilons:\n        # Compute finite difference approximation\n        v_plus = v + eps * p\n        v_minus = v - eps * p\n        \n        J_plus = compute_J(v_plus, a, N, H, B, R, Q, x_b, y)\n        J_minus = compute_J(v_minus, a, N, H, B, R, Q, x_b, y)\n        \n        dir_deriv_fd = (J_plus - J_minus) / (2 * eps)\n\n        # Compute relative error\n        numerator = np.abs(dir_deriv_analytic - dir_deriv_fd)\n        denominator = np.max([1.0, np.abs(dir_deriv_analytic), np.abs(dir_deriv_fd)])\n        error = numerator / denominator\n        errors.append(error)\n        \n    return np.max(errors)\n\n# The following is left commented out as per instructions for final output.\n# if __name__ == '__main__':\n#     solve()\n```",
            "answer": "[2.3533810243163353e-09,2.4593458319530266e-09,3.424424888060867e-09,0.0,1.037373307525359e-08]"
        }
    ]
}