{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any source attribution problem is the forward model, the physical and mathematical link between an emission and its resulting concentration at a receptor. This first practice guides you through the process of constructing this link from fundamental principles. By deriving and implementing the Green's function for an advection-diffusion equation, you will build the sensitivity matrix $H$, often called a \"footprint\" matrix, which is the cornerstone of the linear inverse problem $y = Hx + \\varepsilon$ .",
            "id": "3886147",
            "problem": "Consider a one-dimensional linear transport of a scalar concentration along the horizontal coordinate governed by a canonical Partial Differential Equation (PDE) with constant coefficients. The scalar concentration is denoted by $c(x,t)$, the wind speed by $u$ in $\\mathrm{m/s}$, the diffusivity by $K$ in $\\mathrm{m^2/s}$, and the first-order decay rate by $\\lambda$ in $\\mathrm{s^{-1}}$. A source term $s(x,t)$ injects mass instantaneously at specified coordinates and times. Assume the line is laterally and vertically integrated such that the concentration $c(x,t)$ has units of $\\mathrm{kg/m}$.\n\nYour task is to derive, implement, and use the Green’s function (GF) for the PDE to build a receptor-source sensitivity matrix $H$ (often called the footprint matrix). The matrix $H$ maps a set of discrete unit pulse emissions (each with mass in $\\mathrm{kg}$) located at specified source positions and emission times to concentrations at specified receptor positions and receptor times via linear superposition. Specifically:\n\n1. Start from the linear advection–diffusion–decay PDE with a source term,\n   $$\n   \\frac{\\partial c}{\\partial t} + u \\frac{\\partial c}{\\partial x} = K \\frac{\\partial^2 c}{\\partial x^2} - \\lambda c + s(x,t),\n   $$\n   where all variables and parameters are as defined above. You must use this PDE as the fundamental base, derive the Green’s function $G(\\xi,\\tau)$ for an instantaneous unit mass pulse released at position $x_s$ and time $\\tau$, and then use linear superposition to express the receptor concentrations in terms of $H$ and an emission vector $q$.\n\n2. Construct the footprint matrix $H$ whose entries $H_{ij}$ equal the receptor concentration response at receptor $i$ due to a unit pulse emission $j$. Each emission $j$ is characterized by its source position $x_{s_j}$ and emission time $\\tau_j$, and each receptor $i$ is characterized by its position $x_{r_i}$ and measurement time $t_{r_i}$. Causality must be enforced so that for $t_{r_i} \\le \\tau_j$, $H_{ij} = 0$.\n\n3. Use the derived expressions to numerically compute $H$, and then compute the receptor concentration vector $y$ from the emission mass vector $q$ via\n   $$\n   y = H q,\n   $$\n   expressed in $\\mathrm{kg/m}$.\n\nNumerical and physical setup:\n- Wind speed: $u = 1.0$ $\\mathrm{m/s}$.\n- Diffusivity: $K = 50.0$ $\\mathrm{m^2/s}$.\n- First-order decay rate: $\\lambda = 1.0 \\times 10^{-3}$ $\\mathrm{s^{-1}}$.\n- Source positions: $x_s \\in \\{0.0, 500.0, 1000.0\\}$ $\\mathrm{m}$.\n- Emission times: $\\tau \\in \\{0.0, 100.0\\}$ $\\mathrm{s}$.\n- Receptors: $(x_r,t_r) \\in \\{(1500.0, 600.0), (2000.0, 1200.0), (300.0, 300.0)\\}$ with $x_r$ in $\\mathrm{m}$ and $t_r$ in $\\mathrm{s}$.\n- There are $6$ unit emission pulses formed by the Cartesian product of the $3$ source positions and $2$ emission times, ordered lexicographically by source position then time.\n\nTest suite:\n- Test Case A (general case): $q_A = [2.0, 0.5, 1.0, 0.0, 3.0, 0.1]$ in $\\mathrm{kg}$.\n- Test Case B (zero emissions): $q_B = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$ in $\\mathrm{kg}$.\n- Test Case C (single early pulse): $q_C = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]$ in $\\mathrm{kg}$.\n- Test Case D (single later pulse at upstream source): $q_D = [0.0, 0.0, 0.0, 4.0, 0.0, 0.0]$ in $\\mathrm{kg}$.\n- Test Case E (single later pulse at far source): $q_E = [0.0, 0.0, 0.0, 0.0, 0.0, 5.0]$ in $\\mathrm{kg}$.\n- Test Case F (superposition check): Evaluate whether the superposition-based $H q_A$ equals the direct sum of Green’s function responses for $q_A$ within tolerance $10^{-12}$; output should be a boolean.\n\nRequired outputs and units:\n- For Test Cases A–E, output the resulting receptor concentration vectors $y$ in $\\mathrm{kg/m}$ as floating-point numbers.\n- For Test Case F, output a boolean indicating whether the two computations agree within the stated tolerance.\n\nFinal output format:\n- Your program should produce a single line of output containing all test results as a comma-separated list enclosed in square brackets. Order the results as $[y_A, y_B, y_C, y_D, y_E, \\text{bool}_F]$ with each $y_\\cdot$ expanded into its three receptor entries in the order of receptors given above. For example, the output should have the form\n  $$\n  [y_{A,1}, y_{A,2}, y_{A,3}, y_{B,1}, y_{B,2}, y_{B,3}, y_{C,1}, y_{C,2}, y_{C,3}, y_{D,1}, y_{D,2}, y_{D,3}, y_{E,1}, y_{E,2}, y_{E,3}, \\text{bool}_F],\n  $$\n  where each $y_{\\cdot,i}$ is a float in $\\mathrm{kg/m}$ and $\\text{bool}_F$ is a boolean. Angles do not appear; if any trigonometric functions are used internally, angles must be in radians. No percentages should be printed.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of transport physics, is well-posed, objective, and self-contained. All necessary parameters and conditions are provided, and there are no internal contradictions.\n\nThe solution proceeds by first deriving the Green's function for the governing partial differential equation (PDE), then using this function to construct the source-receptor sensitivity matrix $H$, and finally applying this matrix to compute receptor concentrations for various emission scenarios.\n\nThe governing one-dimensional linear advection–diffusion–decay PDE is given by:\n$$\n\\frac{\\partial c}{\\partial t} + u \\frac{\\partial c}{\\partial x} = K \\frac{\\partial^2 c}{\\partial x^2} - \\lambda c + s(x,t)\n$$\nwhere $c(x,t)$ is the concentration in $\\mathrm{kg/m}$, $u$ is the constant wind speed, $K$ is the constant diffusivity, $\\lambda$ is the first-order decay rate, and $s(x,t)$ is the source term.\n\nThe Green's function, $G(x,t; x_s, \\tau)$, represents the concentration field resulting from an instantaneous point source of unit mass, i.e., $s(x,t) = \\delta(x-x_s)\\delta(t-\\tau)$, where $\\delta(\\cdot)$ is the Dirac delta function. Hence, $G$ is the solution to:\n$$\n\\frac{\\partial G}{\\partial t} + u \\frac{\\partial G}{\\partial x} = K \\frac{\\partial^2 G}{\\partial x^2} - \\lambda G + \\delta(x-x_s)\\delta(t-\\tau)\n$$\nwith the initial condition $G(x, t; x_s, \\tau) = 0$ for $t < \\tau$.\n\nTo solve this PDE, we employ a series of transformations.\n\nFirst, to handle the decay term, we introduce a new variable $c'(x,t)$ defined by the transformation $c(x,t) = c'(x,t) e^{-\\lambda t}$. Substituting this into the homogeneous version of the PDE reveals that the decay term is eliminated if $c'$ satisfies the advection-diffusion equation. A more direct approach for the Green's function, where the process starts at time $\\tau$, is realizing that the decay acts over the elapsed time $\\Delta t = t-\\tau$. The solution can thus be written as the solution to the advection-diffusion equation multiplied by a decay factor $e^{-\\lambda(t-\\tau)}$.\n\nThe advection-diffusion equation is:\n$$\n\\frac{\\partial c'}{\\partial t} + u \\frac{\\partial c'}{\\partial x} = K \\frac{\\partial^2 c'}{\\partial x^2}\n$$\nThe solution to this equation is found by transforming to a coordinate system moving with the mean flow. Let $x' = x - u(t-\\tau)$ be the coordinate in a frame moving with speed $u$, which has its origin at the source location $x_s$ at time $\\tau$. In this frame, the PDE for a substance released at $x' = 0$ simplifies to the pure diffusion (or heat) equation:\n$$\n\\frac{\\partial c'}{\\partial t} = K \\frac{\\partial^2 c'}{\\partial x'^2}\n$$\nFor a unit mass released at $x'=0$ at time $\\tau$, the solution in the moving frame for $t > \\tau$ is a Gaussian distribution:\n$$\nc'(x', t) = \\frac{1}{\\sqrt{4 \\pi K (t-\\tau)}} \\exp\\left(-\\frac{(x')^2}{4 K (t-\\tau)}\\right)\n$$\nTransforming back to the original spatial coordinate $x$ by substituting $x' = x - x_s - u(t-\\tau)$:\n$$\nG_{AD}(x,t; x_s, \\tau) = \\frac{1}{\\sqrt{4 \\pi K (t-\\tau)}} \\exp\\left(-\\frac{(x - x_s - u(t-\\tau))^2}{4 K (t-\\tau)}\\right)\n$$\nThis is the Green's function for the advection-diffusion equation.\n\nFinally, reintroducing the decay factor $e^{-\\lambda(t-\\tau)}$ gives the complete Green's function for the original PDE. Causality requires the concentration to be zero for any time before the emission, so we define:\n$$\nG(x, t; x_s, \\tau) =\n\\begin{cases}\n    \\frac{1}{\\sqrt{4 \\pi K (t-\\tau)}} \\exp\\left(-\\frac{(x - x_s - u(t-\\tau))^2}{4 K (t-\\tau)}\\right) e^{-\\lambda(t-\\tau)} & \\text{if } t > \\tau \\\\\n    0 & \\text{if } t \\le \\tau\n\\end{cases}\n$$\n\nThe footprint matrix $H$ is constructed such that its element $H_{ij}$ is the concentration at receptor $i$ (at position $x_{r_i}$ and time $t_{r_i}$) due to a unit pulse emission from source $j$ (at position $x_{s_j}$ and time $\\tau_j$). By definition, this is given by the Green's function:\n$$\nH_{ij} = G(x_{r_i}, t_{r_i}; x_{s_j}, \\tau_j)\n$$\nLetting $\\Delta t_{ij} = t_{r_i} - \\tau_j$ and $\\Delta x_{ij} = x_{r_i} - x_{s_j}$, the expression for $H_{ij}$ (for $\\Delta t_{ij} > 0$) is:\n$$\nH_{ij} = \\frac{1}{\\sqrt{4 \\pi K \\Delta t_{ij}}} \\exp\\left(-\\frac{(\\Delta x_{ij} - u \\Delta t_{ij})^2}{4 K \\Delta t_{ij}}\\right) e^{-\\lambda \\Delta t_{ij}}\n$$\nThe problem specifies $3$ receptors and $6$ source events, resulting in a $3 \\times 6$ matrix $H$. The source events are the Cartesian product of positions $\\{0.0, 500.0, 1000.0\\}$ $\\mathrm{m}$ and times $\\{0.0, 100.0\\}$ $\\mathrm{s}$, ordered lexicographically. The receptors are ordered as given.\n\nOnce $H$ is computed, the receptor concentration vector $y$ for an arbitrary emission mass vector $q$ is obtained through linear superposition, which is expressed by the matrix-vector product:\n$$\ny = Hq\n$$\nEach element $y_i$ of the vector $y$ is the total concentration at receptor $i$, calculated as $y_i = \\sum_j H_{ij} q_j$.\n\nFor Test Case F, the problem asks to check the equivalence of this matrix-based calculation ($Hq_A$) with the direct summation of individual source contributions ($\\sum_j q_{A,j} G(x_{r_i}, t_{r_i}; x_{s_j}, \\tau_j)$ for each receptor $i$). This serves as a numerical verification of the implemented linearity principle.\n\nThe numerical implementation involves first populating the matrix $H$ by applying the derived formula for each receptor-source pair. Then, for each test case, the corresponding emission vector $q$ is multiplied by $H$ to find the resulting concentration vector $y$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef green_function(xr, tr, xs, tau, u, K, lambd):\n    \"\"\"\n    Computes the Green's function for the 1D advection-diffusion-decay equation.\n    This gives the concentration at (xr, tr) from a unit impulse at (xs, tau).\n    \"\"\"\n    delta_t = tr - tau\n    \n    # Enforce causality\n    if delta_t <= 0:\n        return 0.0\n\n    delta_x = xr - xs\n    \n    # Denominator of the argument of the exponential\n    denom_exp = 4.0 * K * delta_t\n    \n    # Argument of the exponential\n    arg_exp = -((delta_x - u * delta_t)**2) / denom_exp\n    \n    # Pre-factor (1 / sqrt(4*pi*K*t))\n    prefactor_denom = np.sqrt(4.0 * np.pi * K * delta_t)\n    prefactor = 1.0 / prefactor_denom\n    \n    # Decay factor\n    decay_factor = np.exp(-lambd * delta_t)\n    \n    return prefactor * np.exp(arg_exp) * decay_factor\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as described.\n    \"\"\"\n    # Numerical and physical setup\n    u = 1.0  # m/s\n    K = 50.0  # m^2/s\n    lambd = 1.0e-3  # s^-1\n\n    # Source positions and emission times\n    source_positions = [0.0, 500.0, 1000.0]  # m\n    emission_times = [0.0, 100.0]  # s\n    \n    # Generate the list of 6 sources, ordered lexicographically\n    sources = []\n    for xs in source_positions:\n        for tau in emission_times:\n            sources.append((xs, tau))\n\n    # Receptor positions and measurement times\n    receptors = [\n        (1500.0, 600.0),   # (m, s)\n        (2000.0, 1200.0),  # (m, s)\n        (300.0, 300.0)     # (m, s)\n    ]\n    \n    num_receptors = len(receptors)\n    num_sources = len(sources)\n    \n    # Construct the footprint matrix H\n    H = np.zeros((num_receptors, num_sources))\n    for i in range(num_receptors):\n        for j in range(num_sources):\n            xr_i, tr_i = receptors[i]\n            xs_j, tau_j = sources[j]\n            H[i, j] = green_function(xr_i, tr_i, xs_j, tau_j, u, K, lambd)\n\n    # Test suite emission vectors\n    q_A = np.array([2.0, 0.5, 1.0, 0.0, 3.0, 0.1])\n    q_B = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n    q_C = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 0.0])\n    q_D = np.array([0.0, 0.0, 0.0, 4.0, 0.0, 0.0])\n    q_E = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 5.0])\n    \n    # Calculate receptor concentration vectors y = Hq\n    y_A = H @ q_A\n    y_B = H @ q_B\n    y_C = H @ q_C\n    y_D = H @ q_D\n    y_E = H @ q_E\n    \n    # Test Case F: Superposition check\n    # Calculate y_A again via direct summation\n    y_A_direct = np.zeros(num_receptors)\n    for i in range(num_receptors):\n        conc_sum = 0.0\n        xr_i, tr_i = receptors[i]\n        for j in range(num_sources):\n            xs_j, tau_j = sources[j]\n            conc_sum += q_A[j] * green_function(xr_i, tr_i, xs_j, tau_j, u, K, lambd)\n        y_A_direct[i] = conc_sum\n        \n    tolerance = 1e-12\n    bool_F = np.allclose(y_A, y_A_direct, rtol=0, atol=tolerance)\n\n    # Compile results into a single list for output\n    final_results = []\n    final_results.extend(list(y_A))\n    final_results.extend(list(y_B))\n    final_results.extend(list(y_C))\n    final_results.extend(list(y_D))\n    final_results.extend(list(y_E))\n    final_results.append(bool_F)\n\n    # Format the final output string as per requirements\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust inversion requires a realistic characterization of errors. A common pitfall is to assume that the observation error, the $\\varepsilon$ in $y = Hx + \\varepsilon$, is solely due to instrument noise. This exercise delves into the critical concept of \"representation error,\" which arises when our model cannot resolve the same level of detail as reality. You will derive how sub-grid variability in emissions leads to a quantifiable error, which must be incorporated into an augmented observation error covariance matrix for a scientifically sound inversion .",
            "id": "3886202",
            "problem": "In inverse modeling and source attribution for environmental and earth system modeling, consider a single in-situ receptor that measures a linear response to sub-grid emissions of a tracer. The domain consists of one model grid cell subdivided into $N = 4$ sub-grid patches indexed by $i \\in \\{1,2,3,4\\}$. Let the sub-grid emission field be represented by the random vector $\\mathbf{x} = (x_1, x_2, x_3, x_4)^{\\top}$ with zero-mean anomaly at the sub-grid scale relative to the grid-cell mean. The receptor measurement is modeled by the linear mapping $y = \\sum_{i=1}^{N} w_i x_i + \\epsilon$, where $\\mathbf{w} = (w_1, w_2, w_3, w_4)^{\\top}$ are fixed transport weights (a footprint), and $\\epsilon$ is a zero-mean instrument error with variance $R$. The atmospheric model represents the grid cell by its grid-mean state $\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$ and maps it to a modeled observation $\\hat{y} = \\left(\\sum_{i=1}^{N} w_i\\right) \\bar{x}$, thereby neglecting sub-grid variability.  \n\nStarting only from the definitions of linear observation mapping, grid averaging, and the variance of linear combinations, do the following:  \n1) Explain how representation error arises from sub-grid variability in this setting and define the representation error $\\delta$ as the mismatch between the true observation operator applied to $\\mathbf{x}$ and the model’s grid-mean mapping.  \n2) Derive a formula for the representation error variance $R_{\\mathrm{rep}}$ in terms of the sub-grid covariance matrix $\\Sigma = \\mathrm{Cov}(\\mathbf{x})$ and the weights $\\mathbf{w}$. Use this to obtain the augmented observation error covariance $R' = R + R_{\\mathrm{rep}}$.  \n3) For a scientifically plausible case, take $\\mathbf{w} = (0.4, 0.3, 0.2, 0.1)^{\\top}$, an instrument error variance $R = 2.25$, and a stationary sub-grid covariance structure $\\Sigma_{ij} = \\sigma^2 \\rho^{|i-j|}$ with $\\sigma^2 = 25$ and $\\rho = 0.5$. Compute the exact numerical value of the augmented observation error variance $R'$. Express the final $R'$ in $(\\mathrm{ppb})^2$. Do not round your final number.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and self-contained. It addresses a fundamental concept in environmental data assimilation known as representation error. We may proceed with the solution.\n\nThe solution is organized into three parts as requested by the problem statement.\n\n1) **Definition and Explanation of Representation Error**\n\nRepresentation error, in the context of environmental modeling, arises from the discrepancy between the spatial or temporal resolution of the physical reality and the resolution of the computational model. Here, the \"true\" state is described by a sub-grid emission field $\\mathbf{x} = (x_1, x_2, x_3, x_4)^{\\top}$, which has variability within a single model grid cell. The true observation operator, which maps the high-resolution state to a noise-free measurement, is a linear combination given by:\n$$ H(\\mathbf{x}) = \\sum_{i=1}^{N} w_i x_i = \\mathbf{w}^{\\top}\\mathbf{x} $$\nwhere $\\mathbf{w}$ is the vector of transport weights, also known as a footprint.\n\nThe atmospheric model, however, is too coarse to resolve this sub-grid variability. It operates on the grid-mean state, $\\bar{x}$, defined as:\n$$ \\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\nThe model's observation operator, $\\hat{H}$, maps this coarse grid-mean state to a modeled observation. The problem states this mapping is $\\hat{y} = \\left(\\sum_{i=1}^{N} w_i\\right) \\bar{x}$. Therefore, the model's observation operator applied to the true state $\\mathbf{x}$ consists of first averaging $\\mathbf{x}$ to get $\\bar{x}$, and then applying the coarse mapping:\n$$ \\hat{H}(\\mathbf{x}) = \\left(\\sum_{j=1}^{N} w_j\\right) \\left(\\frac{1}{N} \\sum_{k=1}^{N} x_k\\right) $$\nThe representation error, $\\delta$, is defined as the mismatch between the true observation operator and the model's grid-mean mapping when both are applied to the true sub-grid state $\\mathbf{x}$:\n$$ \\delta \\equiv H(\\mathbf{x}) - \\hat{H}(\\mathbf{x}) = \\sum_{i=1}^{N} w_i x_i - \\left(\\sum_{j=1}^{N} w_j\\right) \\left(\\frac{1}{N} \\sum_{k=1}^{N} x_k\\right) $$\nThis error $\\delta$ quantifies the part of the observation signal that is generated by sub-grid scale correlations between the emissions $\\mathbf{x}$ and the transport weights $\\mathbf{w}$, which is lost when the model pre-averages the emissions.\n\n2) **Derivation of Representation Error Variance and Augmented Error Covariance**\n\nTo derive the representation error variance, $R_{\\mathrm{rep}} = \\mathrm{Var}(\\delta)$, we first compute the expectation of $\\delta$. The emission field $\\mathbf{x}$ is given as a zero-mean anomaly, so its expectation is $\\mathrm{E}[\\mathbf{x}] = \\mathbf{0}$.\n$$ \\mathrm{E}[\\delta] = \\mathrm{E}\\left[ \\sum_{i=1}^{N} w_i x_i - \\left(\\sum_{j=1}^{N} w_j\\right) \\left(\\frac{1}{N} \\sum_{k=1}^{N} x_k\\right) \\right] $$\nBy linearity of expectation:\n$$ \\mathrm{E}[\\delta] = \\sum_{i=1}^{N} w_i \\mathrm{E}[x_i] - \\left(\\frac{1}{N} \\sum_{j=1}^{N} w_j\\right) \\left(\\sum_{k=1}^{N} \\mathrm{E}[x_k]\\right) = 0 - 0 = 0 $$\nSince $\\delta$ has zero mean, its variance is the expectation of its square, $R_{\\mathrm{rep}} = \\mathrm{E}[\\delta^2]$. We can express $\\delta$ more compactly using vector notation. Let $\\mathbf{1}$ be a column vector of $N$ ones. Then $\\bar{x} = \\frac{1}{N}\\mathbf{1}^{\\top}\\mathbf{x}$ and $\\sum_i w_i = \\mathbf{w}^{\\top}\\mathbf{1}$.\n$$ \\delta = \\mathbf{w}^{\\top}\\mathbf{x} - (\\mathbf{w}^{\\top}\\mathbf{1})\\left(\\frac{1}{N}\\mathbf{1}^{\\top}\\mathbf{x}\\right) = \\left(\\mathbf{w}^{\\top} - \\frac{\\mathbf{w}^{\\top}\\mathbf{1}}{N}\\mathbf{1}^{\\top}\\right)\\mathbf{x} $$\nLet us define an adjusted weight vector $\\mathbf{w}'^{\\top} = \\mathbf{w}^{\\top} - \\frac{\\mathbf{w}^{\\top}\\mathbf{1}}{N}\\mathbf{1}^{\\top}$. Then $\\delta = \\mathbf{w}'^{\\top}\\mathbf{x}$. The variance is:\n$$ R_{\\mathrm{rep}} = \\mathrm{Var}(\\delta) = \\mathrm{Var}(\\mathbf{w}'^{\\top}\\mathbf{x}) = \\mathrm{E}[(\\mathbf{w}'^{\\top}\\mathbf{x})(\\mathbf{w}'^{\\top}\\mathbf{x})^{\\top}] $$\nUsing the property that for a scalar $s$, $s = s^{\\top}$:\n$$ R_{\\mathrm{rep}} = \\mathrm{E}[(\\mathbf{w}'^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{w}')] = \\mathbf{w}'^{\\top}\\mathrm{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w}' $$\nSince $\\mathrm{E}[\\mathbf{x}] = \\mathbf{0}$, the covariance matrix of $\\mathbf{x}$ is $\\Sigma = \\mathrm{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$. Therefore, the representation error variance is:\n$$ R_{\\mathrm{rep}} = \\mathbf{w}'^{\\top} \\Sigma \\mathbf{w}' = \\left(\\mathbf{w} - \\frac{\\mathbf{w}^{\\top}\\mathbf{1}}{N}\\mathbf{1}\\right)^{\\top} \\Sigma \\left(\\mathbf{w} - \\frac{\\mathbf{w}^{\\top}\\mathbf{1}}{N}\\mathbf{1}\\right) $$\nThe total error in an observation assimilation system is the discrepancy between the actual measurement, $y$, and the modeled prediction, $\\hat{y}$. This total error is:\n$$ y - \\hat{y} = (\\mathbf{w}^{\\top}\\mathbf{x} + \\epsilon) - (\\mathbf{w}^{\\top}\\mathbf{1})\\bar{x} = (\\mathbf{w}^{\\top}\\mathbf{x} - (\\mathbf{w}^{\\top}\\mathbf{1})\\bar{x}) + \\epsilon = \\delta + \\epsilon $$\nThe variance of this total error is the augmented observation error variance, $R'$. Assuming the instrument error $\\epsilon$ is uncorrelated with the state $\\mathbf{x}$ (and hence with $\\delta$), the variance of the sum is the sum of the variances:\n$$ R' = \\mathrm{Var}(\\delta + \\epsilon) = \\mathrm{Var}(\\delta) + \\mathrm{Var}(\\epsilon) = R_{\\mathrm{rep}} + R $$\nThis gives the augmented observation error variance $R' = R + R_{\\mathrm{rep}}$.\n\n3) **Numerical Computation of the Augmented Observation Error Variance**\n\nWe are given the following values: $N=4$, $R = 2.25$, $\\mathbf{w} = (0.4, 0.3, 0.2, 0.1)^{\\top}$, and $\\Sigma_{ij} = \\sigma^2 \\rho^{|i-j|}$ with $\\sigma^2 = 25$ and $\\rho = 0.5$.\n\nFirst, we compute the adjusted weight vector $\\mathbf{w}'$. We need the sum of the weights:\n$$ \\sum_{i=1}^{4} w_i = 0.4 + 0.3 + 0.2 + 0.1 = 1.0 $$\nThe mean of the weights is $\\frac{1}{N}\\sum_i w_i = \\frac{1.0}{4} = 0.25$.\n$$ \\mathbf{w}' = \\mathbf{w} - 0.25 \\cdot \\mathbf{1} = \\begin{pmatrix} 0.4 \\\\ 0.3 \\\\ 0.2 \\\\ 0.1 \\end{pmatrix} - \\begin{pmatrix} 0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} 0.15 \\\\ 0.05 \\\\ -0.05 \\\\ -0.15 \\end{pmatrix} $$\nNext, we construct the sub-grid covariance matrix $\\Sigma$ with components $\\Sigma_{ij} = 25 \\cdot (0.5)^{|i-j|}$:\n$$ \\Sigma = 25 \\begin{pmatrix}\n(0.5)^0 & (0.5)^1 & (0.5)^2 & (0.5)^3 \\\\\n(0.5)^1 & (0.5)^0 & (0.5)^1 & (0.5)^2 \\\\\n(0.5)^2 & (0.5)^1 & (0.5)^0 & (0.5)^1 \\\\\n(0.5)^3 & (0.5)^2 & (0.5)^1 & (0.5)^0\n\\end{pmatrix} = \\begin{pmatrix}\n25.0 & 12.5 & 6.25 & 3.125 \\\\\n12.5 & 25.0 & 12.5 & 6.25 \\\\\n6.25 & 12.5 & 25.0 & 12.5 \\\\\n3.125 & 6.25 & 12.5 & 25.0\n\\end{pmatrix} $$\nNow we compute the representation error variance $R_{\\mathrm{rep}} = \\mathbf{w}'^{\\top} \\Sigma \\mathbf{w}'$. First, we compute the vector-matrix product $\\mathbf{v}^{\\top} = \\mathbf{w}'^{\\top} \\Sigma$:\n$$ \\mathbf{w}'^{\\top} \\Sigma = \\begin{pmatrix} 0.15 & 0.05 & -0.05 & -0.15 \\end{pmatrix} \\begin{pmatrix}\n25.0 & 12.5 & 6.25 & 3.125 \\\\\n12.5 & 25.0 & 12.5 & 6.25 \\\\\n6.25 & 12.5 & 25.0 & 12.5 \\\\\n3.125 & 6.25 & 12.5 & 25.0\n\\end{pmatrix} $$\nThe resulting row vector is $\\mathbf{v}^{\\top} = (v_1, v_2, v_3, v_4)$:\n$v_1 = 0.15(25) + 0.05(12.5) - 0.05(6.25) - 0.15(3.125) = 3.75 + 0.625 - 0.3125 - 0.46875 = 3.59375$\n$v_2 = 0.15(12.5) + 0.05(25) - 0.05(12.5) - 0.15(6.25) = 1.875 + 1.25 - 0.625 - 0.9375 = 1.5625$\n$v_3 = 0.15(6.25) + 0.05(12.5) - 0.05(25) - 0.15(12.5) = 0.9375 + 0.625 - 1.25 - 1.875 = -1.5625$\n$v_4 = 0.15(3.125) + 0.05(6.25) - 0.05(12.5) - 0.15(25) = 0.46875 + 0.3125 - 0.625 - 3.75 = -3.59375$\nSo, $\\mathbf{v}^{\\top} = \\begin{pmatrix} 3.59375 & 1.5625 & -1.5625 & -3.59375 \\end{pmatrix}$.\nNow we compute the final dot product $R_{\\mathrm{rep}} = \\mathbf{v}^{\\top} \\mathbf{w}'$:\n$$ R_{\\mathrm{rep}} = 3.59375(0.15) + 1.5625(0.05) + (-1.5625)(-0.05) + (-3.59375)(-0.15) $$\n$$ R_{\\mathrm{rep}} = 0.5390625 + 0.078125 + 0.078125 + 0.5390625 = 2 \\times (0.5390625 + 0.078125) = 2 \\times 0.6171875 = 1.234375 $$\nFinally, we compute the augmented observation error variance $R'$:\n$$ R' = R + R_{\\mathrm{rep}} = 2.25 + 1.234375 = 3.484375 $$\nThe units of variance are the square of the units of the measured quantity. Since the final answer is requested in $(\\mathrm{ppb})^2$, the numerical value is $3.484375$.",
            "answer": "$$ \\boxed{3.484375} $$"
        },
        {
            "introduction": "With the forward operator and error statistics defined, we can now tackle the inverse problem itself: estimating the unknown sources $x$. Because inverse problems are often ill-posed, simply fitting the data is not enough; we need to introduce constraints and regularization to obtain a stable and physically meaningful solution. This practice guides you through formulating a regularized inversion with a non-negativity constraint and using the Karush-Kuhn-Tucker (KKT) conditions to derive the analytical solution, providing a clear window into the mechanics of constrained optimization in source attribution .",
            "id": "3886219",
            "problem": "A single receptor located downwind of a source region measures a pollutant concentration at quasi-steady state. Under a first-order linear response derived from mass conservation and transport, the measured concentration increment is modeled as $y = a x + \\varepsilon$, where $y$ is the observed receptor concentration increment, $x$ is the unknown source emission rate to be inferred, $a > 0$ is the receptor sensitivity determined by transport and chemistry, and $\\varepsilon$ is a zero-mean stochastic error representing unresolved processes. To stabilize the inversion and encode a preference for smaller emissions in the absence of strong evidence, adopt a Tikhonov (ridge) regularization penalty. Impose the physically motivated nonnegativity constraint $x \\ge 0$ to reflect that emissions cannot be negative.\n\nFormulate the constrained regularized inverse problem that estimates $x$ by minimizing a quadratic loss comprising a squared data misfit and a squared regularization term, subject to the nonnegativity constraint. Starting from the definitions of the Karush–Kuhn–Tucker (KKT) conditions (stationarity, primal feasibility, dual feasibility, and complementary slackness) for inequality-constrained convex optimization, derive the KKT optimality conditions for this problem and solve them analytically to obtain the unique optimal emission estimate $x^{\\star}$ in terms of $a$, $y$, and the regularization weight $\\lambda > 0$. Express your final answer as a single closed-form analytic expression for $x^{\\star}$. No numerical evaluation or rounding is required.",
            "solution": "The problem requires the derivation of the optimal estimate for a source emission rate, $x$, by solving a constrained, regularized inverse problem. The solution must be found analytically by applying the Karush–Kuhn–Tucker (KKT) conditions.\n\nFirst, we formulate the optimization problem. The goal is to estimate the unknown source emission rate $x$ based on an observation $y$. The relationship is given by the linear model $y = a x + \\varepsilon$. We seek to minimize a cost function $J(x)$ that balances data fidelity with a preference for smaller emission values, subject to a physical constraint.\n\nThe cost function consists of two terms:\n$1$. The data misfit term, which is the squared difference between the observation $y$ and the model prediction $ax$. This term is $(y - ax)^2$.\n$2$. The Tikhonov regularization term, which penalizes the magnitude of the solution. For a scalar variable $x$, this is the squared value of $x$ multiplied by a regularization weight $\\lambda > 0$. This term is $\\lambda x^2$.\n\nThe total cost function to be minimized is the sum of these two terms:\n$$J(x) = (y - ax)^2 + \\lambda x^2$$\n\nFurthermore, the problem imposes the nonnegativity constraint $x \\ge 0$, which is physically necessary as emission rates cannot be negative.\n\nThus, the complete constrained optimization problem is:\n$$\\text{minimize} \\quad J(x) = (y - ax)^2 + \\lambda x^2$$\n$$\\text{subject to} \\quad x \\ge 0$$\n\nThis is a convex optimization problem because the objective function $J(x)$ is a sum of two quadratic (and therefore convex) functions, making it strictly convex for $a^2+\\lambda > 0$, which is guaranteed since $\\lambda > 0$. The constraint set $x \\ge 0$ is a convex set. Consequently, a unique global minimum exists.\n\nTo solve this problem, we use the KKT conditions. First, we rewrite the inequality constraint in the standard form $g(x) \\le 0$:\n$$g(x) = -x \\le 0$$\nThe Lagrangian function $\\mathcal{L}(x, \\mu)$ is constructed by adding the constraint multiplied by a Lagrange multiplier (or KKT multiplier) $\\mu$ to the objective function:\n$$\\mathcal{L}(x, \\mu) = J(x) + \\mu g(x) = (y - ax)^2 + \\lambda x^2 - \\mu x$$\n\nThe KKT conditions for optimality at a point $x^{\\star}$ are:\n$1$. **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero at the optimal point $x^{\\star}$.\n$$\\nabla_x \\mathcal{L}(x^{\\star}, \\mu) = \\frac{d}{dx} \\left[ (y - ax)^2 + \\lambda x^2 - \\mu x \\right] \\bigg|_{x=x^{\\star}} = 0$$\n$2$. **Primal Feasibility**: The solution $x^{\\star}$ must satisfy the original constraint.\n$$x^{\\star} \\ge 0$$\n$3$. **Dual Feasibility**: The Lagrange multiplier $\\mu$ must be non-negative.\n$$\\mu \\ge 0$$\n$4$. **Complementary Slackness**: The product of the Lagrange multiplier and the constraint function at the solution must be zero.\n$$\\mu x^{\\star} = 0$$\n\nWe now solve this system of conditions. First, we compute the derivative for the stationarity condition:\n$$\\frac{d\\mathcal{L}}{dx} = 2(y - ax)(-a) + 2\\lambda x - \\mu = 2(a^2 + \\lambda)x - 2ay - \\mu$$\nSetting the derivative to zero gives the stationarity equation:\n$$2(a^2 + \\lambda)x^{\\star} - 2ay - \\mu = 0$$\n$$x^{\\star} = \\frac{2ay + \\mu}{2(a^2 + \\lambda)} = \\frac{ay + \\mu/2}{a^2 + \\lambda}$$\n\nThe complementary slackness condition, $\\mu x^{\\star} = 0$, implies that either $\\mu = 0$ or $x^{\\star} = 0$. This provides a basis for a case-by-case analysis.\n\n**Case 1: The constraint is inactive ($x^{\\star} > 0$).**\nIf $x^{\\star} > 0$, the complementary slackness condition requires that $\\mu = 0$. We substitute $\\mu = 0$ into the stationarity equation for $x^{\\star}$:\n$$x^{\\star} = \\frac{ay}{a^2 + \\lambda}$$\nFor this solution to be valid, it must be consistent with the assumption of this case, i.e., $x^{\\star} > 0$. Since we are given $a > 0$ and $\\lambda > 0$, the denominator $a^2 + \\lambda$ is always positive. Therefore, the sign of $x^{\\star}$ is determined by the sign of $y$. The condition $x^{\\star} > 0$ holds if and only if $ay > 0$, which simplifies to $y > 0$. The dual feasibility condition $\\mu \\ge 0$ is also satisfied, as $\\mu = 0$.\nThus, if $y > 0$, the optimal solution is $x^{\\star} = \\frac{ay}{a^2 + \\lambda}$.\n\n**Case 2: The constraint is active ($x^{\\star} = 0$).**\nIf the optimal solution is $x^{\\star}=0$, the primal feasibility condition $x^{\\star} \\ge 0$ is satisfied. We must now check if there exists a Lagrange multiplier $\\mu \\ge 0$ (dual feasibility) that satisfies the stationarity and complementary slackness conditions.\nSubstituting $x^{\\star} = 0$ into the stationarity equation:\n$$2(a^2 + \\lambda)(0) - 2ay - \\mu = 0$$\n$$\\mu = -2ay$$\nThe dual feasibility condition requires $\\mu \\ge 0$. Therefore, we must have:\n$$-2ay \\ge 0$$\nSince $a > 0$, this inequality simplifies to $-y \\ge 0$, or $y \\le 0$.\nThus, if $y \\le 0$, the optimal solution is $x^{\\star} = 0$, and this solution is KKT-optimal with the valid Lagrange multiplier $\\mu = -2ay \\ge 0$.\n\nCombining the results from both cases, we can express the optimal solution $x^{\\star}$ as a piecewise function of $y$:\n$$x^{\\star} = \\begin{cases} \\frac{ay}{a^2+\\lambda} & \\text{if } y > 0 \\\\ 0 & \\text{if } y \\le 0 \\end{cases}$$\n\nThis piecewise expression can be written more compactly using the maximum function. Notice that if $y > 0$, then $\\frac{ay}{a^2+\\lambda} > 0$, and if $y \\le 0$, then $\\frac{ay}{a^2+\\lambda} \\le 0$. The solution is therefore the positive part of the unconstrained solution $\\frac{ay}{a^2+\\lambda}$.\nThis is equivalent to:\n$$x^{\\star} = \\max\\left(0, \\frac{ay}{a^2+\\lambda}\\right)$$\nThis single closed-form expression represents the unique, optimal, non-negative emission estimate derived from the constrained regularized inversion.",
            "answer": "$$\\boxed{\\max\\left(0, \\frac{ay}{a^2+\\lambda}\\right)}$$"
        }
    ]
}