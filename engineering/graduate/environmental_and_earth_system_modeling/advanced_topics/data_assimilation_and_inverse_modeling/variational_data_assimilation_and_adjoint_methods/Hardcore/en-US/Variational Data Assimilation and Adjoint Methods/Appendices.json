{
    "hands_on_practices": [
        {
            "introduction": "At its heart, variational data assimilation is a mathematical framework for blending information from a model forecast, known as the background state $\\mathbf{x}_b$, with new observations $\\mathbf{y}$. This fundamental exercise  strips the problem down to its essence, asking you to derive the optimal analysis state $\\mathbf{x}_a$ for a simplified system where the error covariances are diagonal. By minimizing the cost function, you will see how the analysis emerges as an intuitive weighted average of the background and observation, with weights determined by the inverse of their respective error variances.",
            "id": "3929900",
            "problem": "Consider a single analysis step in a variational data assimilation for an Earth system state. Let the state vector be $\\mathbf{x} \\in \\mathbb{R}^n$, with a background (prior) state $\\mathbf{x}_b \\in \\mathbb{R}^n$ and an observation $\\mathbf{y} \\in \\mathbb{R}^n$. Assume a linear observation operator $\\mathbf{H} = \\mathbf{I}$, background error covariance $\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$, and observation error covariance $\\mathbf{R} = \\sigma_o^2 \\mathbf{I}$, where $\\sigma_b^2 > 0$ and $\\sigma_o^2 > 0$ and $\\mathbf{I}$ denotes the identity matrix of appropriate dimension. Assume the background and observation errors are independent, zero-mean Gaussian with the stated covariances. Using Maximum A Posteriori (MAP) estimation, derive the closed-form expression for the analysis state $\\mathbf{x}_a$ that minimizes the negative log posterior (equivalently, the standard quadratic variational data assimilation objective), expressed in terms of $\\mathbf{x}_b$, $\\mathbf{y}$, $\\sigma_b^2$, and $\\sigma_o^2$. Additionally, in your derivation, identify the role of the adjoint operator of $\\mathbf{H}$ in the first-order optimality condition, and explain the limiting behaviors as $\\sigma_b^2 \\to 0$ and as $\\sigma_o^2 \\to 0$. Provide the final MAP analysis $\\mathbf{x}_a$ as a single closed-form analytic expression. No numerical rounding is required, and no physical units are associated with $\\mathbf{x}$, $\\mathbf{x}_b$, or $\\mathbf{y}$.",
            "solution": "The problem asks for the derivation of the analysis state $\\mathbf{x}_a$ using Maximum A Posteriori (MAP) estimation for a simplified variational data assimilation scenario. The state vector is $\\mathbf{x} \\in \\mathbb{R}^n$. We are given a background (prior) state $\\mathbf{x}_b \\in \\mathbb{R}^n$, an observation $\\mathbf{y} \\in \\mathbb{R}^n$, a linear observation operator $\\mathbf{H} = \\mathbf{I}$, a background error covariance matrix $\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$, and an observation error covariance matrix $\\mathbf{R} = \\sigma_o^2 \\mathbf{I}$. The variances $\\sigma_b^2$ and $\\sigma_o^2$ are positive scalars, and $\\mathbf{I}$ is the identity matrix.\n\nThe MAP estimate of the state $\\mathbf{x}$ is the state that maximizes the posterior probability density function $p(\\mathbf{x}|\\mathbf{y})$. By Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{x}) p(\\mathbf{x})$$\nThe problem states that the background and observation errors are independent, zero-mean Gaussian.\nThe prior distribution for the state $\\mathbf{x}$, based on the background information, is given by $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}_b, \\mathbf{B})$. Its probability density function is:\n$$p(\\mathbf{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^T \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b)\\right)$$\nThe likelihood, or the probability of the observations $\\mathbf{y}$ given a state $\\mathbf{x}$, is based on the model $\\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\boldsymbol{\\epsilon}$, where the error $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$. With $\\mathbf{H}=\\mathbf{I}$, this becomes $\\mathbf{y} = \\mathbf{x} + \\boldsymbol{\\epsilon}$, so $\\mathbf{y}|\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}, \\mathbf{R})$. The probability density function is:\n$$p(\\mathbf{y}|\\mathbf{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})\\right)$$\nMaximizing the posterior $p(\\mathbf{x}|\\mathbf{y})$ is equivalent to maximizing its logarithm, $\\ln(p(\\mathbf{x}|\\mathbf{y}))$, which in turn is equivalent to minimizing its negative logarithm. This gives rise to the variational cost function, often denoted by $J(\\mathbf{x})$.\n$$J(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^T \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^T \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})$$\nThe analysis state $\\mathbf{x}_a$ is the value of $\\mathbf{x}$ that minimizes this cost function.\n\nWe are given $\\mathbf{H}=\\mathbf{I}$, $\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$, and $\\mathbf{R} = \\sigma_o^2 \\mathbf{I}$. The inverses of the covariance matrices are $\\mathbf{B}^{-1} = (\\sigma_b^2 \\mathbf{I})^{-1} = \\frac{1}{\\sigma_b^2}\\mathbf{I}$ and $\\mathbf{R}^{-1} = (\\sigma_o^2 \\mathbf{I})^{-1} = \\frac{1}{\\sigma_o^2}\\mathbf{I}$. Substituting these into the cost function yields:\n$$J(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^T \\left(\\frac{1}{\\sigma_b^2}\\mathbf{I}\\right) (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{y} - \\mathbf{I}\\mathbf{x})^T \\left(\\frac{1}{\\sigma_o^2}\\mathbf{I}\\right) (\\mathbf{y} - \\mathbf{I}\\mathbf{x})$$\n$$J(\\mathbf{x}) = \\frac{1}{2\\sigma_b^2}(\\mathbf{x} - \\mathbf{x}_b)^T(\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2\\sigma_o^2}(\\mathbf{y} - \\mathbf{x})^T(\\mathbf{y} - \\mathbf{x})$$\nTo find the minimum of this quadratic and convex cost function, we compute its gradient with respect to $\\mathbf{x}$ and set it to zero. This is the first-order optimality condition. The gradient of the general cost function is:\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) - \\mathbf{H}^T \\mathbf{R}^{-1}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})$$\nHere, the operator $\\mathbf{H}^T$ is the adjoint of the observation operator $\\mathbf{H}$. Its role is crucial as it maps the observation-space residual vector $(\\mathbf{y}-\\mathbf{H}\\mathbf{x})$ back to the state space, where it can be combined with the state-space background residual $(\\mathbf{x}-\\mathbf{x}_b)$. For this specific problem, $\\mathbf{H}=\\mathbf{I}$, so its adjoint $\\mathbf{H}^T$ is also the identity matrix $\\mathbf{I}$. Thus, the gradient becomes:\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\left(\\frac{1}{\\sigma_b^2}\\mathbf{I}\\right)(\\mathbf{x} - \\mathbf{x}_b) - \\mathbf{I}^T \\left(\\frac{1}{\\sigma_o^2}\\mathbf{I}\\right)(\\mathbf{y} - \\mathbf{I}\\mathbf{x})$$\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\frac{1}{\\sigma_b^2}(\\mathbf{x} - \\mathbf{x}_b) - \\frac{1}{\\sigma_o^2}(\\mathbf{y} - \\mathbf{x}) = \\frac{1}{\\sigma_b^2}(\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{\\sigma_o^2}(\\mathbf{x} - \\mathbf{y})$$\nSetting the gradient to zero at the analysis state $\\mathbf{x} = \\mathbf{x}_a$:\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}_a) = \\frac{1}{\\sigma_b^2}(\\mathbf{x}_a - \\mathbf{x}_b) + \\frac{1}{\\sigma_o^2}(\\mathbf{x}_a - \\mathbf{y}) = \\mathbf{0}$$\nWe now solve for $\\mathbf{x}_a$:\n$$\\mathbf{x}_a \\left(\\frac{1}{\\sigma_b^2}\\right) - \\frac{\\mathbf{x}_b}{\\sigma_b^2} + \\mathbf{x}_a \\left(\\frac{1}{\\sigma_o^2}\\right) - \\frac{\\mathbf{y}}{\\sigma_o^2} = \\mathbf{0}$$\n$$\\mathbf{x}_a \\left(\\frac{1}{\\sigma_b^2} + \\frac{1}{\\sigma_o^2}\\right) = \\frac{\\mathbf{x}_b}{\\sigma_b^2} + \\frac{\\mathbf{y}}{\\sigma_o^2}$$\n$$\\mathbf{x}_a \\left(\\frac{\\sigma_o^2 + \\sigma_b^2}{\\sigma_b^2 \\sigma_o^2}\\right) = \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_b^2 \\sigma_o^2}$$\nMultiplying both sides by $\\sigma_b^2 \\sigma_o^2$ and dividing by $(\\sigma_o^2 + \\sigma_b^2)$ gives the closed-form expression for the analysis state $\\mathbf{x}_a$:\n$$\\mathbf{x}_a = \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_o^2 + \\sigma_b^2}$$\nThis expression shows that the analysis is a weighted average of the background state $\\mathbf{x}_b$ and the observation $\\mathbf{y}$. The weights are inversely proportional to their respective error variances.\n\nNext, we analyze the limiting behaviors.\n1.  As the confidence in the background becomes perfect, its error variance approaches zero: $\\sigma_b^2 \\to 0$.\n    $$\\lim_{\\sigma_b^2 \\to 0} \\mathbf{x}_a = \\lim_{\\sigma_b^2 \\to 0} \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_o^2 + \\sigma_b^2} = \\frac{\\sigma_o^2 \\mathbf{x}_b + (0) \\mathbf{y}}{\\sigma_o^2 + 0} = \\frac{\\sigma_o^2 \\mathbf{x}_b}{\\sigma_o^2} = \\mathbf{x}_b$$\n    In this limit, the analysis state converges to the background state. The observations are disregarded because the background is considered to be perfectly accurate.\n\n2.  As the confidence in the observation becomes perfect, its error variance approaches zero: $\\sigma_o^2 \\to 0$.\n    $$\\lim_{\\sigma_o^2 \\to 0} \\mathbf{x}_a = \\lim_{\\sigma_o^2 \\to 0} \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_o^2 + \\sigma_b^2} = \\frac{(0) \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{0 + \\sigma_b^2} = \\frac{\\sigma_b^2 \\mathbf{y}}{\\sigma_b^2} = \\mathbf{y}$$\n    In this limit, the analysis state converges to the observation. The background is disregarded because the observation is considered to be perfectly accurate.\n\nThese limiting behaviors are consistent with the principles of optimal estimation, where the final estimate is dominated by the source of information with the lowest uncertainty.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_b^2 + \\sigma_o^2}}\n$$"
        },
        {
            "introduction": "To find the optimal analysis in realistic Earth system models, we must minimize a high-dimensional, nonlinear cost function, a task that requires computing its gradient. The adjoint method is the workhorse algorithm for this task, but it can often seem like a \"black box.\" This practice  demystifies the process by guiding you through a manual calculation using reverse-mode automatic differentiation, revealing the adjoint method as a clever and systematic application of the chain rule of calculus.",
            "id": "3929915",
            "problem": "In a variational data assimilation setting for environmental and earth system modeling, consider a scalar observation operator $f(x,y)$ that maps two nondimensionalized control variables $x$ and $y$ (e.g., standardized temperature and humidity) to a synthetic observed quantity used in the Four-Dimensional Variational assimilation (4D-Var) cost function. The operator is defined by $f(x,y) = \\exp(xy + \\sin x)$. The adjoint method and Automatic Differentiation (AD) are used to obtain gradients required by the variational algorithm.\n\nStarting from fundamental principles, namely the chain rule of differential calculus and the reverse-mode adjoint paradigm (seed the output adjoint with $1$ and propagate backward along the computational graph), compute the reverse-mode adjoint derivatives of $f$ with respect to $x$ and $y$, that is, the components $\\partial f / \\partial x$ and $\\partial f / \\partial y$, as analytic functions of $x$ and $y$. Interpret the reverse-mode adjoints of the inputs as the gradient of $f$ evaluated with an output adjoint seed of $1$.\n\nProvide the final expressions for $\\partial f / \\partial x$ and $\\partial f / \\partial y$ in closed form. Express your final answer as a single row matrix containing the two analytic expressions. No numerical evaluation is required. No units are required.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of differential calculus and reverse-mode automatic differentiation (AD), which is the mathematical foundation of the adjoint method used in variational data assimilation. The problem is well-posed, objective, and contains all necessary information to derive a unique analytical solution.\n\nThe objective is to compute the partial derivatives of the scalar function $f(x, y)$ with respect to its input variables $x$ and $y$. The function is given by:\n$$f(x, y) = \\exp(xy + \\sin x)$$\nThe reverse-mode adjoint method provides an efficient way to compute the gradient of a function. The core principle is the chain rule of calculus, applied backward through the sequence of operations that define the function. In the context of AD, this sequence is represented by a computational graph. The \"adjoint\" of a variable is its partial derivative with respect to the final output. For the inputs $x$ and $y$, their adjoints $\\bar{x}$ and $\\bar{y}$ are defined as:\n$$\\bar{x} \\equiv \\frac{\\partial f}{\\partial x} \\quad \\text{and} \\quad \\bar{y} \\equiv \\frac{\\partial f}{\\partial y}$$\nThe reverse-mode process starts by seeding the adjoint of the output variable with $1$, i.e., $\\bar{f} = \\frac{\\partial f}{\\partial f} = 1$, and then propagates sensitivities backward.\n\nFirst, we decompose the function $f(x, y)$ into a sequence of elementary operations (the \"forward pass\"):\n1. $v_1 = \\sin x$\n2. $v_2 = x \\cdot y$\n3. $v_3 = v_1 + v_2$\n4. $f = \\exp(v_3)$\n\nNext, we perform the \"reverse pass\" to compute the adjoints. We start from the output $f$ and work our way back to the inputs $x$ and $y$. The adjoint of a variable $u$ is denoted by $\\bar{u}$. The fundamental rule for an operation $w = g(u_1, u_2, \\dots)$ is that the adjoints of the inputs are computed as $\\bar{u}_i = \\bar{u}_i + \\bar{w} \\frac{\\partial g}{\\partial u_i}$. The accumulation (addition) is crucial for variables that appear in multiple operations.\n\n**Step a: Initialize the output adjoint.**\nThe process begins by setting the adjoint of the final output to $1$.\n$$\\bar{f} = 1$$\n\n**Step b: Compute the adjoint for $v_3$.**\nThe operation is $f = \\exp(v_3)$. Applying the chain rule:\n$$\\bar{v}_3 = \\frac{\\partial f}{\\partial v_3} \\bar{f} = \\frac{d}{dv_3}(\\exp(v_3)) \\cdot \\bar{f} = \\exp(v_3) \\cdot 1 = \\exp(v_3)$$\nSubstituting the expression for $v_3$:\n$$\\bar{v}_3 = \\exp(xy + \\sin x)$$\n\n**Step c: Compute the adjoints for $v_1$ and $v_2$.**\nThe operation is $v_3 = v_1 + v_2$. The adjoint $\\bar{v}_3$ propagates back to $\\bar{v}_1$ and $\\bar{v}_2$.\n$$\\bar{v}_1 = \\frac{\\partial v_3}{\\partial v_1} \\bar{v}_3 = 1 \\cdot \\bar{v}_3 = \\exp(xy + \\sin x)$$\n$$\\bar{v}_2 = \\frac{\\partial v_3}{\\partial v_2} \\bar{v}_3 = 1 \\cdot \\bar{v}_3 = \\exp(xy + \\sin x)$$\n\n**Step d: Compute the adjoint for $y$.**\nThe variable $y$ is an input to the operation $v_2 = x \\cdot y$. Its adjoint $\\bar{y}$ is computed from $\\bar{v}_2$.\n$$\\bar{y} = \\frac{\\partial v_2}{\\partial y} \\bar{v}_2 = x \\cdot \\bar{v}_2 = x \\exp(xy + \\sin x)$$\nThis is the partial derivative $\\frac{\\partial f}{\\partial y}$.\n\n**Step e: Compute the adjoint for $x$.**\nThe variable $x$ is an input to two operations: $v_1 = \\sin x$ and $v_2 = x \\cdot y$. Its total adjoint, $\\bar{x}$, is the sum of the contributions from both paths.\nContribution from $v_1$:\n$$ \\left( \\frac{\\partial v_1}{\\partial x} \\right) \\bar{v}_1 = \\left( \\frac{d}{dx}(\\sin x) \\right) \\bar{v}_1 = (\\cos x) \\cdot \\bar{v}_1 = (\\cos x) \\exp(xy + \\sin x) $$\nContribution from $v_2$:\n$$ \\left( \\frac{\\partial v_2}{\\partial x} \\right) \\bar{v}_2 = y \\cdot \\bar{v}_2 = y \\exp(xy + \\sin x) $$\nThe total adjoint for $x$ is the sum of these two contributions:\n$$\\bar{x} = (\\cos x) \\exp(xy + \\sin x) + y \\exp(xy + \\sin x)$$\nFactoring out the exponential term:\n$$\\bar{x} = (y + \\cos x) \\exp(xy + \\sin x)$$\nThis is the partial derivative $\\frac{\\partial f}{\\partial x}$.\n\nThe resulting adjoints of the inputs are the components of the gradient of $f$:\n$$\\frac{\\partial f}{\\partial x} = (y + \\cos x) \\exp(xy + \\sin x)$$\n$$\\frac{\\partial f}{\\partial y} = x \\exp(xy + \\sin x)$$\nThese expressions are the required reverse-mode adjoint derivatives, presented in closed analytical form. They represent the sensitivity of the output $f$ to infinitesimal changes in the inputs $x$ and $y$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(y + \\cos x) \\exp(xy + \\sin x) & x \\exp(xy + \\sin x)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Obtaining an analysis state is not the final step; we must also assess its quality and quantify the impact of the observations. This exercise  delves into post-assimilation diagnostics, first by showing that the analysis error covariance matrix $\\mathbf{A}$ is the inverse of the Hessian of the cost function, $\\mathcal{H}^{-1}$. You will then apply this framework to compute the Degrees of Freedom for Signal (DFS), a crucial metric that measures the amount of information the analysis has drawn from the observations.",
            "id": "3929919",
            "problem": "Consider a linear-Gaussian variational data assimilation (VDA) problem for an environmental state vector of dimension $n$, where the background state $\\mathbf{x}_b \\in \\mathbb{R}^n$ has error covariance $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$, and observations $\\mathbf{y} \\in \\mathbb{R}^m$ are related to the state through a linear observation operator $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ with observation error covariance $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$. The analysis is obtained by minimizing the quadratic cost function \n$$\nJ(\\mathbf{x}) = \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2} (\\mathbf{y} - \\mathbf{H} \\mathbf{x})^{\\top} \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H} \\mathbf{x}),\n$$\nand the solution can be expressed in terms of the Kalman gain $\\mathbf{K} \\in \\mathbb{R}^{n \\times m}$ as $\\mathbf{x}_a = \\mathbf{x}_b + \\mathbf{K} (\\mathbf{y} - \\mathbf{H} \\mathbf{x}_b)$. The gradient of $J$ involves the adjoint of the observation operator through $\\mathbf{H}^{\\top}$. \n\nTasks:\n1. Starting from first principles appropriate to a linear-Gaussian VDA framework (i.e., Gaussian prior and likelihood, and the quadratic form of $J(\\mathbf{x})$), derive the expression for the analysis error covariance matrix in the linear case, making explicit the role of the Hessian of $J(\\mathbf{x})$ and the adjoint operator $\\mathbf{H}^{\\top}$.\n2. For a concrete, scientifically plausible example with $n=2$ and $m=2$, let\n$$\n\\mathbf{B} = \\begin{pmatrix}\n1 & 0.2 \\\\\n0.2 & 1.5\n\\end{pmatrix}, \n\\quad\n\\mathbf{H} = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1\n\\end{pmatrix},\n\\quad\n\\mathbf{R} = \\begin{pmatrix}\n0.4 & 0 \\\\\n0 & 0.6\n\\end{pmatrix}.\n$$\nCompute the Degrees of Freedom for Signal (DFS), defined here as \n$$\n\\mathrm{DFS} = \\mathrm{tr}(\\mathbf{H} \\mathbf{K}),\n$$\nwhere $\\mathbf{K}$ is the optimal linear gain for this problem. Express the final DFS value as a dimensionless decimal number and round your answer to four significant figures.",
            "solution": "The problem is divided into two parts. The first part is a theoretical derivation of the analysis error covariance matrix in a linear-Gaussian variational data assimilation (VDA) framework. The second part is a numerical computation of the Degrees of Freedom for Signal (DFS) for a specific low-dimensional system.\n\n### Part 1: Derivation of the Analysis Error Covariance\n\nThe analysis state, $\\mathbf{x}_a$, is the state vector $\\mathbf{x}$ that minimizes the cost function $J(\\mathbf{x})$. The cost function is given by:\n$$\nJ(\\mathbf{x}) = \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2} (\\mathbf{y} - \\mathbf{H} \\mathbf{x})^{\\top} \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H} \\mathbf{x})\n$$\nHere, $\\mathbf{x}_b$ is the background state with error covariance $\\mathbf{B}$, and $\\mathbf{y}$ is the observation vector with error covariance $\\mathbf{R}$. The operator $\\mathbf{H}$ maps the state space to the observation space. The minimization of this quadratic cost function is equivalent to finding the maximum likelihood estimate of the state given the background and observation information, assuming Gaussian error statistics.\n\nTo find the minimum, we compute the gradient of $J(\\mathbf{x})$ with respect to $\\mathbf{x}$ and set it to zero. The gradient, $\\nabla_{\\mathbf{x}} J(\\mathbf{x})$, is:\n$$\n\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\frac{\\partial J(\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\nIn this expression, $\\mathbf{H}^{\\top}$ is the adjoint of the observation operator $\\mathbf{H}$. Setting the gradient to zero at the analysis state $\\mathbf{x} = \\mathbf{x}_a$ yields:\n$$\n\\mathbf{B}^{-1}(\\mathbf{x}_a - \\mathbf{x}_b) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{x}_a - \\mathbf{y}) = \\mathbf{0}\n$$\nRearranging the terms to solve for $\\mathbf{x}_a$:\n$$\n(\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})\\mathbf{x}_a = \\mathbf{B}^{-1}\\mathbf{x}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{y}\n$$\nThe matrix pre-multiplying $\\mathbf{x}_a$ is the Hessian of the cost function, $\\mathcal{H} = \\nabla_{\\mathbf{x}}^2 J(\\mathbf{x})$, which is constant for this linear problem:\n$$\n\\mathcal{H} = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\n$$\nThus, the analysis state is given by:\n$$\n\\mathbf{x}_a = \\mathcal{H}^{-1} (\\mathbf{B}^{-1}\\mathbf{x}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{y})\n$$\nTo find the analysis error covariance, we define the errors. Let $\\mathbf{x}_t$ be the true state. The background error is $\\boldsymbol{\\epsilon}_b = \\mathbf{x}_b - \\mathbf{x}_t$ and the observation error is $\\boldsymbol{\\epsilon}_o = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_t$. The errors are assumed to be unbiased, $\\mathbb{E}[\\boldsymbol{\\epsilon}_b] = \\mathbf{0}$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_o] = \\mathbf{0}$, with covariances $\\mathbb{E}[\\boldsymbol{\\epsilon}_b \\boldsymbol{\\epsilon}_b^{\\top}] = \\mathbf{B}$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_o \\boldsymbol{\\epsilon}_o^{\\top}] = \\mathbf{R}$. They are also assumed to be uncorrelated, $\\mathbb{E}[\\boldsymbol{\\epsilon}_b \\boldsymbol{\\epsilon}_o^{\\top}] = \\mathbf{0}$.\n\nWe substitute $\\mathbf{x}_b = \\mathbf{x}_t + \\boldsymbol{\\epsilon}_b$ and $\\mathbf{y} = \\mathbf{H}\\mathbf{x}_t + \\boldsymbol{\\epsilon}_o$ into the expression for $\\mathbf{x}_a$:\n$$\n\\mathbf{x}_a = \\mathcal{H}^{-1} [\\mathbf{B}^{-1}(\\mathbf{x}_t + \\boldsymbol{\\epsilon}_b) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{x}_t + \\boldsymbol{\\epsilon}_o)]\n$$\n$$\n\\mathbf{x}_a = \\mathcal{H}^{-1} [(\\mathbf{B}^{-1}\\mathbf{x}_t + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H}\\mathbf{x}_t) + \\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o]\n$$\n$$\n\\mathbf{x}_a = \\mathcal{H}^{-1} [(\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})\\mathbf{x}_t + \\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o]\n$$\n$$\n\\mathbf{x}_a = \\mathcal{H}^{-1} \\mathcal{H} \\mathbf{x}_t + \\mathcal{H}^{-1} (\\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o) = \\mathbf{x}_t + \\mathcal{H}^{-1} (\\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o)\n$$\nThe analysis error is $\\boldsymbol{\\epsilon}_a = \\mathbf{x}_a - \\mathbf{x}_t$:\n$$\n\\boldsymbol{\\epsilon}_a = \\mathcal{H}^{-1} (\\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o)\n$$\nThe analysis error covariance matrix, $\\mathbf{A}$, is defined as the expected value of the outer product of the analysis error with itself, $\\mathbf{A} = \\mathbb{E}[\\boldsymbol{\\epsilon}_a \\boldsymbol{\\epsilon}_a^{\\top}]$.\n$$\n\\mathbf{A} = \\mathbb{E} \\left[ \\left( \\mathcal{H}^{-1} (\\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o) \\right) \\left( \\mathcal{H}^{-1} (\\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o) \\right)^{\\top} \\right]\n$$\n$$\n\\mathbf{A} = \\mathcal{H}^{-1} \\mathbb{E} \\left[ (\\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o) (\\boldsymbol{\\epsilon}_b^{\\top}\\mathbf{B}^{-1} + \\boldsymbol{\\epsilon}_o^{\\top}\\mathbf{R}^{-1}\\mathbf{H}) \\right] (\\mathcal{H}^{-1})^{\\top}\n$$\nThe Hessian $\\mathcal{H}$ is symmetric, so $\\mathcal{H}^{-1}$ is also symmetric, meaning $(\\mathcal{H}^{-1})^{\\top} = \\mathcal{H}^{-1}$. Expanding the expectation term:\n\\begin{align*} \\mathbb{E} [ \\dots ] &= \\mathbb{E} [ \\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b\\boldsymbol{\\epsilon}_b^{\\top}\\mathbf{B}^{-1} + \\mathbf{B}^{-1}\\boldsymbol{\\epsilon}_b\\boldsymbol{\\epsilon}_o^{\\top}\\mathbf{R}^{-1}\\mathbf{H} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o\\boldsymbol{\\epsilon}_b^{\\top}\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\boldsymbol{\\epsilon}_o\\boldsymbol{\\epsilon}_o^{\\top}\\mathbf{R}^{-1}\\mathbf{H} ] \\\\ &= \\mathbf{B}^{-1}\\mathbb{E}[\\boldsymbol{\\epsilon}_b\\boldsymbol{\\epsilon}_b^{\\top}]\\mathbf{B}^{-1} + \\mathbf{B}^{-1}\\mathbb{E}[\\boldsymbol{\\epsilon}_b\\boldsymbol{\\epsilon}_o^{\\top}]\\mathbf{R}^{-1}\\mathbf{H} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbb{E}[\\boldsymbol{\\epsilon}_o\\boldsymbol{\\epsilon}_b^{\\top}]\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbb{E}[\\boldsymbol{\\epsilon}_o\\boldsymbol{\\epsilon}_o^{\\top}]\\mathbf{R}^{-1}\\mathbf{H} \\end{align*}\nUsing the error covariance definitions and the uncorrelation assumption ($\\mathbb{E}[\\boldsymbol{\\epsilon}_b\\boldsymbol{\\epsilon}_o^{\\top}]=\\mathbf{0}$), this simplifies to:\n$$\n\\mathbb{E} [ \\dots ] = \\mathbf{B}^{-1}\\mathbf{B}\\mathbf{B}^{-1} + \\mathbf{0} + \\mathbf{0} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{R}\\mathbf{R}^{-1}\\mathbf{H} = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H} = \\mathcal{H}\n$$\nSubstituting this back into the expression for $\\mathbf{A}$:\n$$\n\\mathbf{A} = \\mathcal{H}^{-1} (\\mathcal{H}) \\mathcal{H}^{-1} = \\mathcal{H}^{-1}\n$$\nTherefore, the analysis error covariance matrix is the inverse of the Hessian of the cost function:\n$$\n\\mathbf{A} = (\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})^{-1}\n$$\nThis expression explicitly shows the roles of the Hessian and the adjoint operator $\\mathbf{H}^{\\top}$ in determining the uncertainty of the analysis.\n\n### Part 2: Computation of Degrees of Freedom for Signal\n\nWe are asked to compute the Degrees of Freedom for Signal, $\\mathrm{DFS} = \\mathrm{tr}(\\mathbf{H}\\mathbf{K})$, for the given matrices:\n$$\n\\mathbf{B} = \\begin{pmatrix} 1 & 0.2 \\\\ 0.2 & 1.5 \\end{pmatrix}, \\quad \\mathbf{H} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}, \\quad \\mathbf{R} = \\begin{pmatrix} 0.4 & 0 \\\\ 0 & 0.6 \\end{pmatrix}\n$$\nThe Kalman gain $\\mathbf{K}$ can be computed using the formula $\\mathbf{K} = \\mathbf{B} \\mathbf{H}^{\\top} (\\mathbf{H} \\mathbf{B} \\mathbf{H}^{\\top} + \\mathbf{R})^{-1}$. This form is often numerically convenient as it requires inverting a matrix of size $m \\times m$.\n\nFirst, we compute the components required for this formula.\nThe transpose of $\\mathbf{H}$ is:\n$$\n\\mathbf{H}^{\\top} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nNext, we compute the product $\\mathbf{B} \\mathbf{H}^{\\top}$:\n$$\n\\mathbf{B} \\mathbf{H}^{\\top} = \\begin{pmatrix} 1 & 0.2 \\\\ 0.2 & 1.5 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0.2(0) & 1(1) + 0.2(1) \\\\ 0.2(1) + 1.5(0) & 0.2(1) + 1.5(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 1.2 \\\\ 0.2 & 1.7 \\end{pmatrix}\n$$\nNow, we compute $\\mathbf{H} \\mathbf{B} \\mathbf{H}^{\\top}$:\n$$\n\\mathbf{H} \\mathbf{B} \\mathbf{H}^{\\top} = \\mathbf{H} (\\mathbf{B} \\mathbf{H}^{\\top}) = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1.2 \\\\ 0.2 & 1.7 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0.2) & 1(1.2) + 0(1.7) \\\\ 1(1) + 1(0.2) & 1(1.2) + 1(1.7) \\end{pmatrix} = \\begin{pmatrix} 1 & 1.2 \\\\ 1.2 & 2.9 \\end{pmatrix}\n$$\nLet's define the innovation covariance matrix $\\mathbf{S} = \\mathbf{H} \\mathbf{B} \\mathbf{H}^{\\top} + \\mathbf{R}$:\n$$\n\\mathbf{S} = \\begin{pmatrix} 1 & 1.2 \\\\ 1.2 & 2.9 \\end{pmatrix} + \\begin{pmatrix} 0.4 & 0 \\\\ 0 & 0.6 \\end{pmatrix} = \\begin{pmatrix} 1.4 & 1.2 \\\\ 1.2 & 3.5 \\end{pmatrix}\n$$\nTo compute $\\mathbf{K}$, we need the inverse of $\\mathbf{S}$. The determinant of $\\mathbf{S}$ is:\n$$\n\\det(\\mathbf{S}) = (1.4)(3.5) - (1.2)(1.2) = 4.9 - 1.44 = 3.46\n$$\nThe inverse $\\mathbf{S}^{-1}$ is:\n$$\n\\mathbf{S}^{-1} = \\frac{1}{3.46} \\begin{pmatrix} 3.5 & -1.2 \\\\ -1.2 & 1.4 \\end{pmatrix}\n$$\nNow we can compute the Kalman gain matrix $\\mathbf{K}$:\n$$\n\\mathbf{K} = (\\mathbf{B} \\mathbf{H}^{\\top}) \\mathbf{S}^{-1} = \\begin{pmatrix} 1 & 1.2 \\\\ 0.2 & 1.7 \\end{pmatrix} \\frac{1}{3.46} \\begin{pmatrix} 3.5 & -1.2 \\\\ -1.2 & 1.4 \\end{pmatrix}\n$$\n$$\n\\mathbf{K} = \\frac{1}{3.46} \\begin{pmatrix} 1(3.5) + 1.2(-1.2) & 1(-1.2) + 1.2(1.4) \\\\ 0.2(3.5) + 1.7(-1.2) & 0.2(-1.2) + 1.7(1.4) \\end{pmatrix}\n$$\n$$\n\\mathbf{K} = \\frac{1}{3.46} \\begin{pmatrix} 3.5 - 1.44 & -1.2 + 1.68 \\\\ 0.7 - 2.04 & -0.24 + 2.38 \\end{pmatrix} = \\frac{1}{3.46} \\begin{pmatrix} 2.06 & 0.48 \\\\ -1.34 & 2.14 \\end{pmatrix}\n$$\nFinally, we compute $\\mathrm{DFS} = \\mathrm{tr}(\\mathbf{H}\\mathbf{K})$. First, we find the matrix product $\\mathbf{H}\\mathbf{K}$:\n$$\n\\mathbf{H}\\mathbf{K} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\frac{1}{3.46} \\begin{pmatrix} 2.06 & 0.48 \\\\ -1.34 & 2.14 \\end{pmatrix} = \\frac{1}{3.46} \\begin{pmatrix} 1(2.06) + 0(-1.34) & 1(0.48) + 0(2.14) \\\\ 1(2.06) + 1(-1.34) & 1(0.48) + 1(2.14) \\end{pmatrix}\n$$\n$$\n\\mathbf{H}\\mathbf{K} = \\frac{1}{3.46} \\begin{pmatrix} 2.06 & 0.48 \\\\ 0.72 & 2.62 \\end{pmatrix}\n$$\nThe trace of this matrix is the sum of its diagonal elements:\n$$\n\\mathrm{DFS} = \\mathrm{tr}(\\mathbf{H}\\mathbf{K}) = \\frac{1}{3.46} (2.06 + 2.62) = \\frac{4.68}{3.46}\n$$\nComputing the numerical value:\n$$\n\\frac{4.68}{3.46} \\approx 1.352601156...\n$$\nRounding to four significant figures, we get $1.353$.",
            "answer": "$$\n\\boxed{1.353}\n$$"
        }
    ]
}