{
    "hands_on_practices": [
        {
            "introduction": "为了掌握变分数据同化，我们从最核心的概念——最优估计——开始。这个练习将问题简化到其本质，即如何将一个先验估计（背景场）与新的信息（观测）相结合。通过推导，您将亲身体会到分析场如何自然地成为背景场和观测值的一个加权平均，其权重由我们对各自信息来源的置信度（即误差方差）决定，这为所有更复杂的变分方法建立了最基本的直觉 。",
            "id": "3929900",
            "problem": "考虑地球系统状态变分数据同化中的一个分析步骤。设状态向量为 $x \\in \\mathbb{R}^n$，背景（先验）状态为 $x_b \\in \\mathbb{R}^n$，观测值为 $y \\in \\mathbb{R}^n$。假设线性观测算子 $H = I$，背景误差协方差 $B = \\sigma_b^2 I$，观测误差协方差 $R = \\sigma_o^2 I$，其中 $\\sigma_b^2 > 0$ 和 $\\sigma_o^2 > 0$，并且 $I$ 表示适当维度的单位矩阵。假设背景误差和观测误差是独立的、零均值的高斯分布，其协方差如上所述。使用最大后验（MAP）估计，推导分析状态 $x_a$ 的闭式表达式，该表达式最小化负对数后验（等价于标准的二次变分数据同化目标函数），并用 $x_b$、$y$、$\\sigma_b^2$ 和 $\\sigma_o^2$ 表示。此外，在您的推导中，请指明 $H$ 的伴随算子在一阶最优性条件中的作用，并解释当 $\\sigma_b^2 \\to 0$ 和 $\\sigma_o^2 \\to 0$ 时的极限行为。请提供最终的 MAP 分析 $x_a$ 的单一闭式解析表达式。不需要进行数值舍入，且 $x$、$x_b$ 或 $y$ 不关联任何物理单位。",
            "solution": "该问题要求在一个简化的变分数据同化场景中，使用最大后验（MAP）估计来推导分析状态 $x_a$。状态向量为 $x \\in \\mathbb{R}^n$。我们已知背景（先验）状态 $x_b \\in \\mathbb{R}^n$，观测值 $y \\in \\mathbb{R}^n$，线性观测算子 $H = I$，背景误差协方差矩阵 $B = \\sigma_b^2 I$ 和观测误差协方差矩阵 $R = \\sigma_o^2 I$。方差 $\\sigma_b^2$ 和 $\\sigma_o^2$ 是正标量，$I$ 是单位矩阵。\n\n状态 $x$ 的 MAP 估计是使后验概率密度函数 $p(x|y)$ 最大化的状态。根据贝叶斯定理，后验概率与似然和先验的乘积成正比：\n$$p(x|y) \\propto p(y|x) p(x)$$\n问题陈述背景误差和观测误差是独立的、零均值的高斯分布。\n基于背景信息的状态 $x$ 的先验分布由 $x \\sim \\mathcal{N}(x_b, B)$ 给出。其概率密度函数为：\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b)\\right)$$\n似然，即在给定状态 $x$ 的情况下观测值 $y$ 的概率，基于模型 $y = Hx + \\epsilon$，其中误差 $\\epsilon \\sim \\mathcal{N}(0, R)$。当 $H=I$ 时，模型变为 $y = x + \\epsilon$，因此 $y|x \\sim \\mathcal{N}(x, R)$。其概率密度函数为：\n$$p(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)$$\n最大化后验概率 $p(x|y)$ 等价于最大化其对数 $\\ln(p(x|y))$，而这又等价于最小化其负对数。这就引出了变分代价函数，通常表示为 $J(x)$。\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\n分析状态 $x_a$ 是使该代价函数最小化的 $x$ 的值。\n\n我们已知 $H=I$、$B = \\sigma_b^2 I$ 和 $R = \\sigma_o^2 I$。协方差矩阵的逆为 $B^{-1} = (\\sigma_b^2 I)^{-1} = \\frac{1}{\\sigma_b^2}I$ 和 $R^{-1} = (\\sigma_o^2 I)^{-1} = \\frac{1}{\\sigma_o^2}I$。将这些代入代价函数可得：\n$$J(x) = \\frac{1}{2}(x - x_b)^T \\left(\\frac{1}{\\sigma_b^2}I\\right) (x - x_b) + \\frac{1}{2}(y - Ix)^T \\left(\\frac{1}{\\sigma_o^2}I\\right) (y - Ix)$$\n$$J(x) = \\frac{1}{2\\sigma_b^2}(x - x_b)^T(x - x_b) + \\frac{1}{2\\sigma_o^2}(y - x)^T(y - x)$$\n为求此二次凸代价函数的最小值，我们计算其关于 $x$ 的梯度并令其为零。这是一阶最优性条件。一般代价函数的梯度为：\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1}(y - Hx)$$\n在这里，算子 $H^T$ 是观测算子 $H$ 的伴随算子。它的作用至关重要，因为它将观测空间残差向量 $(y-Hx)$ 映射回状态空间，从而可以与状态空间背景残差 $(x-x_b)$ 相结合。对于这个特定问题，$H=I$，因此其伴随算子 $H^T$ 也是单位矩阵 $I$。于是，梯度变为：\n$$\\nabla_x J(x) = \\left(\\frac{1}{\\sigma_b^2}I\\right)(x - x_b) - I^T \\left(\\frac{1}{\\sigma_o^2}I\\right)(y - Ix)$$\n$$\\nabla_x J(x) = \\frac{1}{\\sigma_b^2}(x - x_b) - \\frac{1}{\\sigma_o^2}(y - x) = \\frac{1}{\\sigma_b^2}(x - x_b) + \\frac{1}{\\sigma_o^2}(x - y)$$\n在分析状态 $x = x_a$ 处将梯度设为零：\n$$\\nabla_x J(x_a) = \\frac{1}{\\sigma_b^2}(x_a - x_b) + \\frac{1}{\\sigma_o^2}(x_a - y) = 0$$\n我们现在求解 $x_a$：\n$$x_a \\left(\\frac{1}{\\sigma_b^2}\\right) - \\frac{x_b}{\\sigma_b^2} + x_a \\left(\\frac{1}{\\sigma_o^2}\\right) - \\frac{y}{\\sigma_o^2} = 0$$\n$$x_a \\left(\\frac{1}{\\sigma_b^2} + \\frac{1}{\\sigma_o^2}\\right) = \\frac{x_b}{\\sigma_b^2} + \\frac{y}{\\sigma_o^2}$$\n$$x_a \\left(\\frac{\\sigma_o^2 + \\sigma_b^2}{\\sigma_b^2 \\sigma_o^2}\\right) = \\frac{\\sigma_o^2 x_b + \\sigma_b^2 y}{\\sigma_b^2 \\sigma_o^2}$$\n两边同乘以 $\\sigma_b^2 \\sigma_o^2$ 再除以 $(\\sigma_o^2 + \\sigma_b^2)$，得到分析状态 $x_a$ 的闭式表达式：\n$$x_a = \\frac{\\sigma_o^2 x_b + \\sigma_b^2 y}{\\sigma_o^2 + \\sigma_b^2}$$\n该表达式表明，分析是背景状态 $x_b$ 和观测值 $y$ 的加权平均。权重与其各自的误差方差成反比。\n\n接下来，我们分析其极限行为。\n1.  当对背景的置信度趋于完美时，其误差方差趋于零：$\\sigma_b^2 \\to 0$。\n    $$\\lim_{\\sigma_b^2 \\to 0} x_a = \\lim_{\\sigma_b^2 \\to 0} \\frac{\\sigma_o^2 x_b + \\sigma_b^2 y}{\\sigma_o^2 + \\sigma_b^2} = \\frac{\\sigma_o^2 x_b + (0) y}{\\sigma_o^2 + 0} = \\frac{\\sigma_o^2 x_b}{\\sigma_o^2} = x_b$$\n    在此极限下，分析状态收敛于背景状态。观测值被忽略，因为背景被认为是完全准确的。\n\n2.  当对观测的置信度趋于完美时，其误差方差趋于零：$\\sigma_o^2 \\to 0$。\n    $$\\lim_{\\sigma_o^2 \\to 0} x_a = \\lim_{\\sigma_o^2 \\to 0} \\frac{\\sigma_o^2 x_b + \\sigma_b^2 y}{\\sigma_o^2 + \\sigma_b^2} = \\frac{(0) x_b + \\sigma_b^2 y}{0 + \\sigma_b^2} = \\frac{\\sigma_b^2 y}{\\sigma_b^2} = y$$\n    在此极限下，分析状态收敛于观测值。背景被忽略，因为观测被认为是完全准确的。\n\n这些极限行为与最优估计的原理一致，即最终的估计值由不确定性最低的信息源主导。",
            "answer": "$$\n\\boxed{\\frac{\\sigma_o^2 x_b + \\sigma_b^2 y}{\\sigma_b^2 + \\sigma_o^2}}\n$$"
        },
        {
            "introduction": "在解决了简单的线性问题后，我们转向在地球系统科学中无处不在的复杂非线性模型。为了最小化这些模型的代价函数以找到最优分析，我们需要计算其梯度，而伴随方法正是实现这一目标的关键计算工具。这个练习提供了一个具体的、分步的计算过程，让您手动推导一个非线性函数的伴随，从而揭示其内部机制——它仅仅是微积分中链式法则的一种系统性应用 。",
            "id": "3929915",
            "problem": "在用于环境和地球系统建模的变分资料同化设置中，考虑一个标量观测算子 $f(x,y)$，它将两个无量纲控制变量 $x$ 和 $y$（例如，标准化的温度和湿度）映射到一个用于四维变分同化 (4D-Var) 代价函数的合成观测量。该算子定义为 $f(x,y) = \\exp(xy + \\sin x)$。伴随方法和自动微分 (AD) 用于获得变分算法所需的梯度。\n\n从基本原理出发，即微分学的链式法则和反向模式伴随范式（将输出伴随量设为 $1$ 并沿计算图向后传播），计算 $f$ 相对于 $x$ 和 $y$ 的反向模式伴随导数，即分量 $\\partial f / \\partial x$ 和 $\\partial f / \\partial y$，作为 $x$ 和 $y$ 的解析函数。将输入的反向模式伴随量解释为使用输出伴随种子 $1$ 计算的 $f$ 的梯度。\n\n以闭式形式给出 $\\partial f / \\partial x$ 和 $\\partial f / \\partial y$ 的最终表达式。将您的最终答案表示为包含两个解析表达式的单行矩阵。不需要进行数值计算。不需要单位。",
            "solution": "目标是计算标量函数 $f(x, y)$ 相对于其输入变量 $x$ 和 $y$ 的偏导数。该函数由下式给出：\n$$f(x, y) = \\exp(xy + \\sin x)$$\n反向模式伴随方法提供了一种计算函数梯度的有效方法。其核心原理是微积分的链式法则，通过定义函数的一系列运算反向应用。在AD的背景下，这个序列由一个计算图表示。变量的“伴随”是它相对于最终输出的偏导数。对于输入 $x$ 和 $y$，它们的伴随量 $\\bar{x}$ 和 $\\bar{y}$ 定义为：\n$$\\bar{x} \\equiv \\frac{\\partial f}{\\partial x} \\quad \\text{和} \\quad \\bar{y} \\equiv \\frac{\\partial f}{\\partial y}$$\n反向模式过程从将输出变量的伴随量设为种子 $1$ 开始，即 $\\bar{f} = \\frac{\\partial f}{\\partial f} = 1$，然后向后传播敏感性。\n\n首先，我们将函数 $f(x, y)$ 分解为一系列基本运算（“前向传播”）：\n1. $v_1 = \\sin x$\n2. $v_2 = x \\cdot y$\n3. $v_3 = v_1 + v_2$\n4. $f = \\exp(v_3)$\n\n接下来，我们执行“反向传播”来计算伴随量。我们从输出 $f$ 开始，反向追溯到输入 $x$ 和 $y$。变量 $u$ 的伴随量用 $\\bar{u}$ 表示。对于运算 $w = g(u_1, u_2, \\dots)$，其基本法则是输入的伴随量计算为 $\\bar{u}_i = \\bar{u}_i + \\bar{w} \\frac{\\partial g}{\\partial u_i}$。对于出现在多个运算中的变量，累加（加法）至关重要。\n\n**步骤a：初始化输出伴随量。**\n该过程从将最终输出的伴随量设为 $1$ 开始。\n$$\\bar{f} = 1$$\n\n**步骤b：计算 $v_3$ 的伴随量。**\n运算为 $f = \\exp(v_3)$。应用链式法则：\n$$\\bar{v}_3 = \\frac{\\partial f}{\\partial v_3} \\bar{f} = \\frac{d}{dv_3}(\\exp(v_3)) \\cdot \\bar{f} = \\exp(v_3) \\cdot 1 = \\exp(v_3)$$\n代入 $v_3$ 的表达式：\n$$\\bar{v}_3 = \\exp(xy + \\sin x)$$\n\n**步骤c：计算 $v_1$ 和 $v_2$ 的伴随量。**\n运算为 $v_3 = v_1 + v_2$。伴随量 $\\bar{v}_3$ 反向传播到 $\\bar{v}_1$ 和 $\\bar{v}_2$。\n$$\\bar{v}_1 = \\frac{\\partial v_3}{\\partial v_1} \\bar{v}_3 = 1 \\cdot \\bar{v}_3 = \\exp(xy + \\sin x)$$\n$$\\bar{v}_2 = \\frac{\\partial v_3}{\\partial v_2} \\bar{v}_3 = 1 \\cdot \\bar{v}_3 = \\exp(xy + \\sin x)$$\n\n**步骤d：计算 $y$ 的伴随量。**\n变量 $y$ 是运算 $v_2 = x \\cdot y$ 的一个输入。其伴随量 $\\bar{y}$ 由 $\\bar{v}_2$ 计算得出。\n$$\\bar{y} = \\frac{\\partial v_2}{\\partial y} \\bar{v}_2 = x \\cdot \\bar{v}_2 = x \\exp(xy + \\sin x)$$\n这就是偏导数 $\\frac{\\partial f}{\\partial y}$。\n\n**步骤e：计算 $x$ 的伴随量。**\n变量 $x$ 是两个运算的输入：$v_1 = \\sin x$ 和 $v_2 = x \\cdot y$。它的总伴随量 $\\bar{x}$ 是来自两条路径贡献的总和。\n来自 $v_1$ 的贡献：\n$$ \\left( \\frac{\\partial v_1}{\\partial x} \\right) \\bar{v}_1 = \\left( \\frac{d}{dx}(\\sin x) \\right) \\bar{v}_1 = (\\cos x) \\cdot \\bar{v}_1 = (\\cos x) \\exp(xy + \\sin x) $$\n来自 $v_2$ 的贡献：\n$$ \\left( \\frac{\\partial v_2}{\\partial x} \\right) \\bar{v}_2 = y \\cdot \\bar{v}_2 = y \\exp(xy + \\sin x) $$\n$x$ 的总伴随量是这两个贡献的总和：\n$$\\bar{x} = (\\cos x) \\exp(xy + \\sin x) + y \\exp(xy + \\sin x)$$\n提出指数项：\n$$\\bar{x} = (y + \\cos x) \\exp(xy + \\sin x)$$\n这就是偏导数 $\\frac{\\partial f}{\\partial x}$。\n\n最终得到的输入伴随量即为 $f$ 的梯度分量：\n$$\\frac{\\partial f}{\\partial x} = (y + \\cos x) \\exp(xy + \\sin x)$$\n$$\\frac{\\partial f}{\\partial y} = x \\exp(xy + \\sin x)$$\n这些表达式就是所求的反向模式伴随导数，以闭式解析形式给出。它们表示输出 $f$ 对输入 $x$ 和 $y$ 的无穷小变化的敏感性。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(y + \\cos x) \\exp(xy + \\sin x)  x \\exp(xy + \\sin x)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "得到分析场之后，一个自然而然的问题是：“我们的估计有多好？观测数据在多大程度上改进了我们的状态估计？” 本练习将指导您从代价函数的曲率（Hessian矩阵）出发，推导分析误差协方差，它量化了分析结果的不确定性。然后，您将运用这一理论框架计算信号自由度（Degrees of Freedom for Signal, DFS），这是一个衡量观测信息被有效传递到分析场中的强大诊断工具 。",
            "id": "3929919",
            "problem": "考虑一个维度为 $n$ 的环境状态向量的线性高斯变分资料同化（VDA）问题，其中背景场状态 $x_b \\in \\mathbb{R}^n$ 的误差协方差为 $B \\in \\mathbb{R}^{n \\times n}$，观测 $y \\in \\mathbb{R}^m$ 通过线性观测算子 $H \\in \\mathbb{R}^{m \\times n}$ 与状态相关联，其观测误差协方差为 $R \\in \\mathbb{R}^{m \\times m}$。分析场是通过最小化二次代价函数得到的\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\top} B^{-1} (x - x_b) + \\frac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x),\n$$\n其解可以用卡尔曼增益 $K \\in \\mathbb{R}^{n \\times m}$ 表示为 $x_a = x_b + K (y - H x_b)$。$J$ 的梯度通过 $H^{\\top}$ 涉及到观测算子的伴随算子。\n\n任务：\n1. 从适用于线性高斯VDA框架的第一性原理出发（即高斯先验和似然，以及 $J(x)$ 的二次形式），推导线性情况下分析误差协方差矩阵的表达式，并明确 $J(x)$ 的Hessian矩阵和伴随算子 $H^{\\top}$ 的作用。\n2. 对于一个 $n=2$ 和 $m=2$ 的具体的、科学上合理的例子，令\n$$\nB = \\begin{pmatrix}\n1  0.2 \\\\\n0.2  1.5\n\\end{pmatrix}, \n\\quad\nH = \\begin{pmatrix}\n1  0 \\\\\n1  1\n\\end{pmatrix},\n\\quad\nR = \\begin{pmatrix}\n0.4  0 \\\\\n0  0.6\n\\end{pmatrix}.\n$$\n计算信号自由度 (DFS)，此处定义为\n$$\n\\mathrm{DFS} = \\mathrm{tr}(H K),\n$$\n其中 $K$ 是此问题的最优线性增益。将最终的DFS值表示为一个无量纲的十进制数，并将答案四舍五入到四位有效数字。",
            "solution": "该问题分为两部分。第一部分是在线性高斯变分资料同化（VDA）框架下对分析误差协方差矩阵进行理论推导。第二部分是针对一个特定的低维系统，对信号自由度（DFS）进行数值计算。\n\n### 第1部分：分析误差协方差的推导\n\n分析状态 $x_a$ 是使代价函数 $J(x)$ 最小化的状态向量 $x$。代价函数由下式给出：\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\top} B^{-1} (x - x_b) + \\frac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x)\n$$\n这里，$x_b$ 是背景场状态，其误差协方差为 $B$，$y$ 是观测向量，其误差协方差为 $R$。算子 $H$ 将状态空间映射到观测空间。在假设误差服从高斯统计的情况下，最小化这个二次代价函数等价于在给定背景场和观测信息的情况下，寻找状态的最大似然估计。\n\n为求最小值，我们计算 $J(x)$ 关于 $x$ 的梯度，并令其为零。梯度 $\\nabla_x J(x)$ 为：\n$$\n\\nabla_x J(x) = \\frac{\\partial J(x)}{\\partial x} = B^{-1}(x - x_b) + H^{\\top}R^{-1}(Hx - y)\n$$\n在此表达式中，$H^{\\top}$ 是观测算子 $H$ 的伴随算子。在分析状态 $x = x_a$ 处将梯度设为零，可得：\n$$\nB^{-1}(x_a - x_b) + H^{\\top}R^{-1}(Hx_a - y) = 0\n$$\n整理各项以求解 $x_a$：\n$$\n(B^{-1} + H^{\\top}R^{-1}H)x_a = B^{-1}x_b + H^{\\top}R^{-1}y\n$$\n前乘 $x_a$ 的矩阵是代价函数的Hessian矩阵 $\\mathcal{H} = \\nabla_x^2 J(x)$，对于这个线性问题，它是一个常数：\n$$\n\\mathcal{H} = B^{-1} + H^{\\top}R^{-1}H\n$$\n因此，分析状态由下式给出：\n$$\nx_a = \\mathcal{H}^{-1} (B^{-1}x_b + H^{\\top}R^{-1}y)\n$$\n为了求分析误差协方差，我们定义误差。令 $x_t$ 为真实状态。背景场误差为 $\\epsilon_b = x_b - x_t$，观测误差为 $\\epsilon_o = y - Hx_t$。假设误差是无偏的，即 $\\mathbb{E}[\\epsilon_b] = 0$ 和 $\\mathbb{E}[\\epsilon_o] = 0$，其协方差分别为 $\\mathbb{E}[\\epsilon_b \\epsilon_b^{\\top}] = B$ 和 $\\mathbb{E}[\\epsilon_o \\epsilon_o^{\\top}] = R$。同时假设它们是不相关的，即 $\\mathbb{E}[\\epsilon_b \\epsilon_o^{\\top}] = 0$。\n\n我们将 $x_b = x_t + \\epsilon_b$ 和 $y = Hx_t + \\epsilon_o$ 代入 $x_a$ 的表达式中：\n$$\nx_a = \\mathcal{H}^{-1} [B^{-1}(x_t + \\epsilon_b) + H^{\\top}R^{-1}(Hx_t + \\epsilon_o)]\n$$\n$$\nx_a = \\mathcal{H}^{-1} [(B^{-1}x_t + H^{\\top}R^{-1}Hx_t) + B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o]\n$$\n$$\nx_a = \\mathcal{H}^{-1} [(B^{-1} + H^{\\top}R^{-1}H)x_t + B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o]\n$$\n$$\nx_a = \\mathcal{H}^{-1} \\mathcal{H} x_t + \\mathcal{H}^{-1} (B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o) = x_t + \\mathcal{H}^{-1} (B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o)\n$$\n分析误差为 $\\epsilon_a = x_a - x_t$：\n$$\n\\epsilon_a = \\mathcal{H}^{-1} (B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o)\n$$\n分析误差协方差矩阵 $A$ 定义为分析误差与其自身外积的期望值，即 $A = \\mathbb{E}[\\epsilon_a \\epsilon_a^{\\top}]$。\n$$\nA = \\mathbb{E} \\left[ \\left( \\mathcal{H}^{-1} (B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o) \\right) \\left( \\mathcal{H}^{-1} (B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o) \\right)^{\\top} \\right]\n$$\n$$\nA = \\mathcal{H}^{-1} \\mathbb{E} \\left[ (B^{-1}\\epsilon_b + H^{\\top}R^{-1}\\epsilon_o) (\\epsilon_b^{\\top}B^{-1} + \\epsilon_o^{\\top}R^{-1}H) \\right] (\\mathcal{H}^{-1})^{\\top}\n$$\nHessian矩阵 $\\mathcal{H}$ 是对称的，所以 $\\mathcal{H}^{-1}$ 也是对称的，即 $(\\mathcal{H}^{-1})^{\\top} = \\mathcal{H}^{-1}$。展开期望项：\n\\begin{align*} \\mathbb{E} [ \\dots ] = \\mathbb{E} [ B^{-1}\\epsilon_b\\epsilon_b^{\\top}B^{-1} + B^{-1}\\epsilon_b\\epsilon_o^{\\top}R^{-1}H + H^{\\top}R^{-1}\\epsilon_o\\epsilon_b^{\\top}B^{-1} + H^{\\top}R^{-1}\\epsilon_o\\epsilon_o^{\\top}R^{-1}H ] \\\\ = B^{-1}\\mathbb{E}[\\epsilon_b\\epsilon_b^{\\top}]B^{-1} + B^{-1}\\mathbb{E}[\\epsilon_b\\epsilon_o^{\\top}]R^{-1}H + H^{\\top}R^{-1}\\mathbb{E}[\\epsilon_o\\epsilon_b^{\\top}]B^{-1} + H^{\\top}R^{-1}\\mathbb{E}[\\epsilon_o\\epsilon_o^{\\top}]R^{-1}H \\end{align*}\n使用误差协方差的定义和不相关性假设（$\\mathbb{E}[\\epsilon_b\\epsilon_o^{\\top}]=0$），上式简化为：\n$$\n\\mathbb{E} [ \\dots ] = B^{-1}BB^{-1} + 0 + 0 + H^{\\top}R^{-1}RR^{-1}H = B^{-1} + H^{\\top}R^{-1}H = \\mathcal{H}\n$$\n将此结果代回 $A$ 的表达式中：\n$$\nA = \\mathcal{H}^{-1} (\\mathcal{H}) \\mathcal{H}^{-1} = \\mathcal{H}^{-1}\n$$\n因此，分析误差协方差矩阵是代价函数Hessian矩阵的逆：\n$$\nA = (B^{-1} + H^{\\top}R^{-1}H)^{-1}\n$$\n这个表达式明确地显示了Hessian矩阵和伴随算子 $H^{\\top}$ 在确定分析场不确定性中的作用。\n\n### 第2部分：信号自由度的计算\n\n我们需要为给定的矩阵计算信号自由度 $\\mathrm{DFS} = \\mathrm{tr}(HK)$：\n$$\nB = \\begin{pmatrix} 1  0.2 \\\\ 0.2  1.5 \\end{pmatrix}, \\quad H = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix}, \\quad R = \\begin{pmatrix} 0.4  0 \\\\ 0  0.6 \\end{pmatrix}\n$$\n卡尔曼增益 $K$ 可以使用公式 $K = B H^{\\top} (H B H^{\\top} + R)^{-1}$ 计算。这种形式在数值计算上通常很方便，因为它需要对一个 $m \\times m$ 大小的矩阵求逆。\n\n首先，我们计算该公式所需的各个部分。\n$H$ 的转置是：\n$$\nH^{\\top} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}\n$$\n接下来，我们计算乘积 $B H^{\\top}$：\n$$\nB H^{\\top} = \\begin{pmatrix} 1  0.2 \\\\ 0.2  1.5 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0.2(0)  1(1) + 0.2(1) \\\\ 0.2(1) + 1.5(0)  0.2(1) + 1.5(1) \\end{pmatrix} = \\begin{pmatrix} 1  1.2 \\\\ 0.2  1.7 \\end{pmatrix}\n$$\n现在，我们计算 $H B H^{\\top}$：\n$$\nH B H^{\\top} = H (B H^{\\top}) = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1.2 \\\\ 0.2  1.7 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0.2)  1(1.2) + 0(1.7) \\\\ 1(1) + 1(0.2)  1(1.2) + 1(1.7) \\end{pmatrix} = \\begin{pmatrix} 1  1.2 \\\\ 1.2  2.9 \\end{pmatrix}\n$$\n我们定义新息协方差矩阵 $S = H B H^{\\top} + R$：\n$$\nS = \\begin{pmatrix} 1  1.2 \\\\ 1.2  2.9 \\end{pmatrix} + \\begin{pmatrix} 0.4  0 \\\\ 0  0.6 \\end{pmatrix} = \\begin{pmatrix} 1.4  1.2 \\\\ 1.2  3.5 \\end{pmatrix}\n$$\n为了计算 $K$，我们需要 $S$ 的逆。$S$ 的行列式是：\n$$\n\\det(S) = (1.4)(3.5) - (1.2)(1.2) = 4.9 - 1.44 = 3.46\n$$\n$S$ 的逆 $S^{-1}$ 是：\n$$\nS^{-1} = \\frac{1}{3.46} \\begin{pmatrix} 3.5  -1.2 \\\\ -1.2  1.4 \\end{pmatrix}\n$$\n现在我们可以计算卡尔曼增益矩阵 $K$：\n$$\nK = (B H^{\\top}) S^{-1} = \\begin{pmatrix} 1  1.2 \\\\ 0.2  1.7 \\end{pmatrix} \\frac{1}{3.46} \\begin{pmatrix} 3.5  -1.2 \\\\ -1.2  1.4 \\end{pmatrix}\n$$\n$$\nK = \\frac{1}{3.46} \\begin{pmatrix} 1(3.5) + 1.2(-1.2)  1(-1.2) + 1.2(1.4) \\\\ 0.2(3.5) + 1.7(-1.2)  0.2(-1.2) + 1.7(1.4) \\end{pmatrix}\n$$\n$$\nK = \\frac{1}{3.46} \\begin{pmatrix} 3.5 - 1.44  -1.2 + 1.68 \\\\ 0.7 - 2.04  -0.24 + 2.38 \\end{pmatrix} = \\frac{1}{3.46} \\begin{pmatrix} 2.06  0.48 \\\\ -1.34  2.14 \\end{pmatrix}\n$$\n最后，我们计算 $\\mathrm{DFS} = \\mathrm{tr}(HK)$。首先，我们求矩阵乘积 $HK$：\n$$\nHK = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\frac{1}{3.46} \\begin{pmatrix} 2.06  0.48 \\\\ -1.34  2.14 \\end{pmatrix} = \\frac{1}{3.46} \\begin{pmatrix} 1(2.06) + 0(-1.34)  1(0.48) + 0(2.14) \\\\ 1(2.06) + 1(-1.34)  1(0.48) + 1(2.14) \\end{pmatrix}\n$$\n$$\nHK = \\frac{1}{3.46} \\begin{pmatrix} 2.06  0.48 \\\\ 0.72  2.62 \\end{pmatrix}\n$$\n该矩阵的迹是其对角元素之和：\n$$\n\\mathrm{DFS} = \\mathrm{tr}(HK) = \\frac{1}{3.46} (2.06 + 2.62) = \\frac{4.68}{3.46}\n$$\n计算数值：\n$$\n\\frac{4.68}{3.46} \\approx 1.352601156...\n$$\n四舍五入到四位有效数字，我们得到 $1.353$。",
            "answer": "$$\n\\boxed{1.353}\n$$"
        }
    ]
}