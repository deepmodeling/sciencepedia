{
    "hands_on_practices": [
        {
            "introduction": "Before constructing a data-driven model, it is crucial to formulate the problem in a way that simplifies the learning task. This practice leverages the principle of dimensional homogeneity through the Buckingham $\\Pi$ theorem to systematically reduce the dimensionality of the input space. By transforming dimensional variables into a minimal set of dimensionless groups, you will see how this not only reduces model complexity but also builds in physical scaling laws, enhancing the model's ability to extrapolate .",
            "id": "3884451",
            "problem": "A hybrid physics–data model is being built to learn a sediment transport law for suspended load over rough beds in an open-channel flow. The learned mapping is to be constructed from dimensional predictor variables $(\\rho,\\nu,U,D,g)$, where $\\rho$ is the fluid density, $\\nu$ is the kinematic viscosity, $U$ is a characteristic mean flow speed, $D$ is a characteristic sediment grain diameter, and $g$ is gravitational acceleration. The model will subsequently be trained on laboratory and field datasets that span a wide range of $U$, $D$, $\\nu$, and $g$ values. To improve extrapolation and reduce the hypothesis space, the inputs to the learned component must be nondimensional.\n\nUsing only the fundamental principles of dimensional homogeneity and the Buckingham $\\Pi$ theorem, derive a minimal set of independent nondimensional input groups $\\Pi_i$ that can be formed from $(\\rho,\\nu,U,D,g)$. Explicitly construct these groups by solving for exponents that make each $\\Pi_i$ dimensionless, and justify whether $\\rho$ can enter any dimensionless input formed from the given set. Then, explain, without invoking any empirical sediment transport formulas, why replacing $(\\rho,\\nu,U,D,g)$ by the derived $\\Pi_i$ reduces hypothesis complexity in a hybrid physics–data model, and quantify this reduction by comparing the number of free coefficients in a generic total-degree $p$ polynomial model in the original dimensional inputs versus a total-degree $p$ polynomial model in the $\\Pi_i$.\n\nProvide your final answer as the explicit analytical expressions for the independent $\\Pi_i$ collected in a single row matrix. No numerical evaluation is required, and no rounding is needed. Do not include units in the final answer.",
            "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in the principles of dimensional analysis, is well-posed, objective, and internally consistent. It presents a standard but rigorous application of the Buckingham $\\Pi$ theorem to a relevant problem in environmental modeling.\n\nThe task is to derive a minimal set of independent nondimensional groups from a set of $n=5$ physical variables relevant to suspended sediment transport: fluid density $\\rho$, kinematic viscosity $\\nu$, mean flow speed $U$, sediment grain diameter $D$, and gravitational acceleration $g$. The fundamental dimensions involved are Mass ($M$), Length ($L$), and Time ($T$).\n\nFirst, we list the variables and their corresponding dimensions:\n1.  Fluid density, $\\rho$: $[\\rho] = M L^{-3}$\n2.  Kinematic viscosity, $\\nu$: $[\\nu] = L^2 T^{-1}$\n3.  Flow speed, $U$: $[U] = L T^{-1}$\n4.  Grain diameter, $D$: $[D] = L$\n5.  Gravitational acceleration, $g$: $[g] = L T^{-2}$\n\nThe Buckingham $\\Pi$ theorem states that a physically meaningful relationship between $n$ variables can be expressed as a relationship between $n-k$ independent dimensionless groups, where $k$ is the number of fundamental dimensions required to describe the variables (i.e., the rank of the dimensional matrix).\n\nLet us construct the dimensional matrix, where the rows correspond to the fundamental dimensions ($M, L, T$) and the columns correspond to the variables ($\\rho, \\nu, U, D, g$):\n$$\n\\begin{pmatrix}\n & \\rho & \\nu & U & D & g \\\\\nM & 1 & 0 & 0 & 0 & 0 \\\\\nL & -3 & 2 & 1 & 1 & 1 \\\\\nT & 0 & -1 & -1 & 0 & -2\n\\end{pmatrix}\n$$\nThe rank of this matrix, $k$, is the number of linearly independent rows (or columns). The first row, corresponding to Mass ($M$), is clearly independent of the others. The submatrix for dimensions $L, T$ and variables $\\nu, U$ is $\\begin{pmatrix} 2 & 1 \\\\ -1 & -1 \\end{pmatrix}$, which has a determinant of $(2)(-1) - (1)(-1) = -1 \\neq 0$. Thus, the last two rows are also linearly independent. The rank of the matrix is $k=3$.\n\nAccording to the Buckingham $\\Pi$ theorem, the number of independent dimensionless groups is $n - k = 5 - 3 = 2$. We will denote these as $\\Pi_1$ and $\\Pi_2$.\n\nThe problem asks whether the fluid density, $\\rho$, can be a component of any dimensionless group formed from the given set of variables. Let a generic dimensionless group $\\Pi$ be formed by the product of the variables raised to some exponents:\n$$ \\Pi = \\rho^{e_1} \\nu^{e_2} U^{e_3} D^{e_4} g^{e_5} $$\nFor $\\Pi$ to be dimensionless, its total dimensions must be $M^0 L^0 T^0$. We can write the dimensional equation as:\n$$ [\\Pi] = (M L^{-3})^{e_1} (L^2 T^{-1})^{e_2} (L T^{-1})^{e_3} (L)^{e_4} (L T^{-2})^{e_5} = M^{e_1} L^{-3e_1+2e_2+e_3+e_4+e_5} T^{-e_2-e_3-2e_5} $$\nFor this to be dimensionless, the exponent of each fundamental dimension must be zero. For the Mass dimension ($M$), we have:\n$$ e_1 = 0 $$\nThis result proves that the exponent of $\\rho$ must be zero in any dimensionless group formed from this set of variables. Therefore, $\\rho$ cannot enter into any of the dimensionless inputs. This is a direct consequence of $\\rho$ being the only variable in the set that carries the mass dimension, leaving no other variable to cancel it out.\n\nSince $\\rho$ is excluded, the problem reduces to finding $2$ dimensionless groups from the remaining $4$ variables ($\\nu, U, D, g$) which are described by $2$ fundamental dimensions ($L, T$). We select $k=2$ repeating variables from this set. A valid choice must contain both $L$ and $T$ and be dimensionally independent. Let us choose $U$ and $D$.\n- $[U] = L T^{-1}$\n- $[D] = L$\nThe dimensional matrix for $\\{U, D\\}$ is $\\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix}$, which has determinant $1 \\neq 0$, confirming they are a valid choice.\n\nWe now form the two $\\Pi$ groups using the remaining variables, $\\nu$ and $g$.\n\nFirst, we form $\\Pi_1$ with $\\nu$:\n$$ \\Pi_1 = U^a D^b \\nu^1 $$\nThe dimensional equation is:\n$$ [\\Pi_1] = (L T^{-1})^a (L)^b (L^2 T^{-1})^1 = L^{a+b+2} T^{-a-1} = L^0 T^0 $$\nEquating the exponents:\n- For $T$: $-a-1 = 0 \\implies a = -1$\n- For $L$: $a+b+2 = 0 \\implies -1+b+2 = 0 \\implies b = -1$\nThis gives $\\Pi_1 = U^{-1} D^{-1} \\nu = \\frac{\\nu}{UD}$. For convention, we use its reciprocal, the particle Reynolds number:\n$$ \\Pi_1 = \\frac{UD}{\\nu} $$\nSecond, we form $\\Pi_2$ with $g$:\n$$ \\Pi_2 = U^a D^b g^1 $$\nThe dimensional equation is:\n$$ [\\Pi_2] = (L T^{-1})^a (L)^b (L T^{-2})^1 = L^{a+b+1} T^{-a-2} = L^0 T^0 $$\nEquating the exponents:\n- For $T$: $-a-2 = 0 \\implies a = -2$\n- For $L$: $a+b+1 = 0 \\implies -2+b+1 = 0 \\implies b = 1$\nThis gives $\\Pi_2 = U^{-2} D^{1} g = \\frac{gD}{U^2}$. For convention, we use its reciprocal, the square of the particle Froude number:\n$$ \\Pi_2 = \\frac{U^2}{gD} $$\nThus, a minimal set of independent nondimensional input groups is $\\{\\frac{UD}{\\nu}, \\frac{U^2}{gD}\\}$.\n\nReplacing the original dimensional inputs $(\\rho, \\nu, U, D, g)$ with the derived $\\Pi_i$ groups reduces hypothesis complexity in a hybrid physics–data model for several reasons. The core principle is that dimensional analysis embeds physical law (the principle of dimensional homogeneity) into the structure of the problem.\n1.  **Dimensionality Reduction**: The search space for the unknown functional relationship is reduced from a $5$-dimensional space of inputs $(\\rho, \\nu, U, D, g)$ to a $2$-dimensional space of inputs $(\\Pi_1, \\Pi_2)$. This is a drastic simplification. A data-driven model working in the lower-dimensional space is less susceptible to the \"curse of dimensionality\", requires less data to train robustly, and is computationally more efficient.\n2.  **Imposition of Physical Constraints**: By reformulating the problem in terms of dimensionless groups, we are forcing the learned model to conform to the principle of dimensional homogeneity. Any function of $\\Pi_1$ and $\\Pi_2$ will automatically yield a dimensionally correct result. This prunes the hypothesis space by eliminating all physically impossible functional forms, which a naive model operating on dimensional inputs would otherwise have to learn to avoid.\n3.  **Enhanced Generalization (Similitude)**: The dimensionless groups represent ratios of physical effects (e.g., $Re$ is a ratio of inertial to viscous forces). The principle of dynamic similarity states that two systems with different dimensional parameters ($\\nu$, $U$, $D$, etc.) will behave identically if their corresponding dimensionless groups are identical. A model trained on $(\\Pi_1, \\Pi_2)$ inherently learns this principle. It can accurately predict the behavior of a large-scale river flow based on data from a small-scale lab flume, provided they share the same $(\\Pi_1, \\Pi_2)$ values, even if the individual dimensional variables are vastly different. A model trained on the raw dimensional variables would treat these as entirely separate data points and would not be guaranteed to generalize correctly.\n\nTo quantify this complexity reduction, we compare the number of free coefficients in a generic total-degree $p$ polynomial model. The number of coefficients $N$ for such a model in $d$ variables is given by the formula for combinations with repetition: $N(d, p) = \\binom{p+d}{d}$.\n- For the model in the original dimensional inputs, we have $d_1 = 5$ variables $(\\rho, \\nu, U, D, g)$. The number of coefficients is:\n  $$ N_1 = \\binom{p+5}{5} = \\frac{(p+5)(p+4)(p+3)(p+2)(p+1)}{120} $$\n  This model is physically naive as it does not enforce dimensional consistency among its terms, but it represents the complexity a purely data-driven model would face.\n- For the model in the nondimensional inputs, we have $d_2 = 2$ variables $(\\Pi_1, \\Pi_2)$. The number of coefficients is:\n  $$ N_2 = \\binom{p+2}{2} = \\frac{(p+2)(p+1)}{2} $$\nThe number of coefficients grows as $O(p^5)$ in the dimensional case versus $O(p^2)$ in the nondimensional case. For example, for a quadratic model ($p=2$), the dimensional model has $N_1 = \\binom{7}{5} = 21$ coefficients, while the nondimensional model has only $N_2 = \\binom{4}{2} = 6$ coefficients. This dramatic reduction in the number of free parameters makes the model far simpler to specify, easier to constrain with available data, and less prone to overfitting.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{UD}{\\nu} & \\frac{U^2}{gD}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key challenge in modeling physical systems is ensuring that model predictions adhere to fundamental constraints, such as the non-negativity of concentrations or densities. This exercise demonstrates a powerful method for enforcing such properties: designing the constraint directly into the model architecture. By using a link function like the exponential, $c = \\exp(g)$, we can guarantee a positive output, exploring how this choice impacts the training dynamics and loss landscape .",
            "id": "3884463",
            "problem": "Consider a passive tracer concentration field $c(x,t)$ evolving in a one-dimensional periodic channel $x \\in [0,2\\pi]$ under Fickian diffusion with constant diffusivity $\\kappa>0$ and no sources or sinks. The governing mass-conservation law and Fick’s law combine to yield the partial differential equation (PDE) $\\partial_t c - \\kappa \\,\\partial_{xx} c = 0$. To enforce positivity in a hybrid physics–data parameterization, use the link function $c(x,t) = \\exp\\!\\big(g_{\\phi}(x,t)\\big)$, where $g_{\\phi}$ is a data-driven model and $\\phi$ denotes the trainable parameters.\n\nFor this problem, restrict $g_{\\phi}$ to a single scalar parameter $a \\in \\mathbb{R}$ multiplying a fixed basis function $\\psi(x,t)$, namely $g_{\\phi}(x,t) = a\\,\\psi(x,t)$ with $\\psi(x,t) = \\sin(kx)\\cos(\\omega t)$, where $k \\in \\mathbb{N}$ and $\\omega>0$ are fixed. Define the physics-informed training objective as the space–time integral of the squared PDE residual over one spatio-temporal period,\n$$\nL(a) = \\frac{1}{2}\\int_{0}^{2\\pi}\\!\\int_{0}^{\\frac{2\\pi}{\\omega}}\\!\\left(\\partial_t c(x,t) - \\kappa\\,\\partial_{xx} c(x,t)\\right)^{2}\\, dt\\, dx,\n$$\nwith $c(x,t) = \\exp\\!\\big(a\\,\\psi(x,t)\\big)$.\n\nTasks:\n- Starting from the PDE and the definition of $L(a)$, derive the exact expression for the gradient $\\frac{dL}{da}$ in terms of $\\psi$, its derivatives, and $a$, using the chain rule.\n- Linearize the gradient flow near $a=0$ and show explicitly how the positivity link $c=\\exp(g_{\\phi})$ scales the gradient. Compute the Hessian $\\frac{d^{2}L}{da^{2}}\\big|_{a=0}$ as a closed-form integral in terms of $\\kappa$, $k$, and $\\omega$.\n- Considering gradient descent on $a$ with a constant learning rate $\\eta$ and updates $a_{n+1} = a_n - \\eta\\,\\frac{dL}{da}(a_n)$, use the linearized dynamics near $a=0$ to determine the largest learning rate $\\eta_{\\max}$ that ensures linear stability of the training iterates. Provide your final answer as a single analytic expression for $\\eta_{\\max}$ in terms of $\\kappa$, $k$, and $\\omega$. No numerical substitution or rounding is required, and no units are needed for the final answer.",
            "solution": "The problem is first validated against the given criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Tracer concentration field: $c(x,t)$.\n-   Domain: $x \\in [0, 2\\pi]$, 1D periodic channel.\n-   Governing PDE: $\\partial_t c - \\kappa \\partial_{xx} c = 0$, with constant diffusivity $\\kappa>0$.\n-   Positivity-enforcing link function: $c(x,t) = \\exp(g_{\\phi}(x,t))$.\n-   Parameterized model: $g_{\\phi}(x,t) = a\\,\\psi(x,t)$, where $a \\in \\mathbb{R}$ is the trainable parameter.\n-   Fixed basis function: $\\psi(x,t) = \\sin(kx)\\cos(\\omega t)$, with $k \\in \\mathbb{N}$ and $\\omega>0$.\n-   Physics-informed loss function: $L(a) = \\frac{1}{2}\\int_{0}^{2\\pi}\\int_{0}^{\\frac{2\\pi}{\\omega}}\\!\\left(\\partial_t c(x,t) - \\kappa\\,\\partial_{xx} c(x,t)\\right)^{2}\\, dt\\, dx$.\n-   Gradient descent update rule: $a_{n+1} = a_n - \\eta\\,\\frac{dL}{da}(a_n)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is firmly based on the fundamental principles of diffusion (Fick's laws, mass conservation) and on established methods in scientific machine learning (physics-informed learning). The use of a loss function based on the PDE residual is a standard technique. The exponential link function is a common method for enforcing positivity constraints.\n-   **Well-Posed**: The problem is mathematically well-defined. It provides all necessary functions, parameters, and integration domains to perform the requested derivations and calculations. The tasks are specific and lead to a unique analytical solution.\n-   **Objective**: The problem is stated using precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is found to be valid as it is scientifically sound, well-posed, objective, and complete. We may proceed with the solution.\n\n### derivation\n\n**Task 1: Derive the gradient $\\frac{dL}{da}$**\n\nThe loss function is given by\n$$L(a) = \\frac{1}{2}\\int_{0}^{2\\pi}\\int_{0}^{2\\pi/\\omega} R(a, x, t)^2 \\, dt \\, dx$$\nwhere the residual $R(a, x, t)$ is defined as\n$$R = \\partial_t c - \\kappa \\partial_{xx} c$$\nwith $c(x,t) = \\exp(a\\psi(x,t))$.\n\nTo find the gradient $\\frac{dL}{da}$, we differentiate $L(a)$ with respect to $a$. Using the chain rule and Leibniz's rule for differentiating under the integral sign, we get:\n$$\\frac{dL}{da} = \\frac{d}{da} \\left( \\frac{1}{2}\\int\\int R^2 \\,dt\\,dx \\right) = \\int\\int R \\frac{\\partial R}{\\partial a} \\,dt\\,dx$$\nLet us define the linear differential operator $\\mathcal{L} = \\partial_t - \\kappa \\partial_{xx}$. The residual is $R = \\mathcal{L}c$.\nThe partial derivative of the residual with respect to $a$ is:\n$$\\frac{\\partial R}{\\partial a} = \\frac{\\partial}{\\partial a} (\\mathcal{L}c) = \\mathcal{L}\\left(\\frac{\\partial c}{\\partial a}\\right)$$\nWe compute $\\frac{\\partial c}{\\partial a}$:\n$$\\frac{\\partial c}{\\partial a} = \\frac{\\partial}{\\partial a} \\exp(a\\psi) = \\psi \\exp(a\\psi) = \\psi c$$\nSubstituting this into the expression for $\\frac{\\partial R}{\\partial a}$:\n$$\\frac{\\partial R}{\\partial a} = \\mathcal{L}(\\psi c) = (\\partial_t - \\kappa \\partial_{xx})(\\psi c)$$\nWe expand the terms:\n$$\\partial_t(\\psi c) = (\\partial_t \\psi)c + \\psi(\\partial_t c)$$\n$$\\partial_x(\\psi c) = (\\partial_x \\psi)c + \\psi(\\partial_x c)$$\n$$\\partial_{xx}(\\psi c) = \\partial_x((\\partial_x \\psi)c + \\psi(\\partial_x c)) = (\\partial_{xx}\\psi)c + (\\partial_x \\psi)(\\partial_x c) + (\\partial_x \\psi)(\\partial_x c) + \\psi(\\partial_{xx}c)$$\n$$\\partial_{xx}(\\psi c) = (\\partial_{xx}\\psi)c + 2(\\partial_x \\psi)(\\partial_x c) + \\psi(\\partial_{xx}c)$$\nCombining these gives:\n$$\\mathcal{L}(\\psi c) = [(\\partial_t \\psi)c + \\psi(\\partial_t c)] - \\kappa[(\\partial_{xx}\\psi)c + 2(\\partial_x \\psi)(\\partial_x c) + \\psi(\\partial_{xx}c)]$$\nRearranging terms to group by $\\psi$ and $c$:\n$$\\mathcal{L}(\\psi c) = \\psi(\\partial_t c - \\kappa \\partial_{xx}c) + c(\\partial_t \\psi - \\kappa \\partial_{xx}\\psi) - 2\\kappa(\\partial_x c)(\\partial_x \\psi)$$\nRecognizing $R = \\partial_t c - \\kappa \\partial_{xx}c$ and $\\mathcal{L}\\psi = \\partial_t\\psi - \\kappa\\partial_{xx}\\psi$, we have:\n$$\\frac{\\partial R}{\\partial a} = \\psi R + c(\\mathcal{L}\\psi) - 2\\kappa(\\partial_x c)(\\partial_x \\psi)$$\nFinally, the expression for the gradient $\\frac{dL}{da}$ is:\n$$\\frac{dL}{da} = \\int\\int R \\left( \\psi R + c(\\mathcal{L}\\psi) - 2\\kappa(\\partial_x c)(\\partial_x \\psi) \\right) \\,dt\\,dx$$\nwhere $c = \\exp(a\\psi)$, $R = \\mathcal{L}c$, and $\\partial_x c = a(\\partial_x \\psi)c$. This is the exact expression for the gradient.\n\n**Task 2: Linearize the gradient flow and compute the Hessian at $a=0$**\n\nThe gradient flow is given by $\\frac{da}{d\\tau} = -\\frac{dL}{da}(a)$, for a continuous \"training time\" $\\tau$. To linearize this flow near $a=0$, we need the first-order Taylor expansion of $\\frac{dL}{da}(a)$ around $a=0$:\n$$\\frac{dL}{da}(a) \\approx \\frac{dL}{da}\\bigg|_{a=0} + a \\left(\\frac{d^2L}{da^2}\\bigg|_{a=0}\\right)$$\nFirst, we evaluate the gradient at $a=0$. At $a=0$, we have $c(x,t) = \\exp(0) = 1$. The residual is:\n$$R(a=0) = \\partial_t(1) - \\kappa\\partial_{xx}(1) = 0 - 0 = 0$$\nSince $R(a=0) = 0$, the gradient is also zero:\n$$\\frac{dL}{da}\\bigg|_{a=0} = \\int\\int R(0) \\frac{\\partial R}{\\partial a}\\bigg|_{a=0} \\,dt\\,dx = 0$$\nThis confirms that $a=0$ is a critical point of the loss function. The linearized gradient is thus:\n$$\\frac{dL}{da}(a) \\approx a \\left(\\frac{d^2L}{da^2}\\bigg|_{a=0}\\right)$$\nThe term $\\frac{d^2L}{da^2}|_{a=0}$ is the Hessian $H$ of the loss function at $a=0$.\nThe use of the link function $c=\\exp(a\\psi)$ results in a non-quadratic loss landscape. For small $a$, however, $c \\approx 1+a\\psi$. The residual becomes $R=\\mathcal{L}c \\approx \\mathcal{L}(1+a\\psi) = a\\mathcal{L}\\psi$. The loss is $L(a) \\approx \\frac{1}{2}\\int\\int (a\\mathcal{L}\\psi)^2 dtdx$, making the gradient $\\frac{dL}{da} \\approx a \\int\\int (\\mathcal{L}\\psi)^2 dtdx$. The non-linear link function's effect is that this linear gradient scaling is an approximation, whereas for a simple linear model $c=a\\psi$, it would be exact.\n\nTo compute the Hessian $H$, we differentiate the gradient expression:\n$$\\frac{d^2L}{da^2} = \\frac{d}{da} \\int\\int R \\frac{\\partial R}{\\partial a} \\,dt\\,dx = \\int\\int \\left( \\left(\\frac{\\partial R}{\\partial a}\\right)^2 + R \\frac{\\partial^2 R}{\\partial a^2} \\right) \\,dt\\,dx$$\nEvaluating at $a=0$, since $R(0)=0$, the second term vanishes:\n$$H = \\frac{d^2L}{da^2}\\bigg|_{a=0} = \\int\\int \\left(\\frac{\\partial R}{\\partial a}\\bigg|_{a=0}\\right)^2 \\,dt\\,dx$$\nWe need $\\frac{\\partial R}{\\partial a}$ at $a=0$. We found $\\frac{\\partial R}{\\partial a} = \\mathcal{L}(\\frac{\\partial c}{\\partial a})$.\nAt $a=0$:\n$$\\frac{\\partial c}{\\partial a}\\bigg|_{a=0} = (\\psi\\exp(a\\psi))\\big|_{a=0} = \\psi \\exp(0) = \\psi$$\nTherefore,\n$$\\frac{\\partial R}{\\partial a}\\bigg|_{a=0} = \\mathcal{L}(\\psi) = \\partial_t\\psi - \\kappa \\partial_{xx}\\psi$$\nThe Hessian is the integral of the square of this term:\n$$H = \\int_{0}^{2\\pi}\\int_{0}^{\\frac{2\\pi}{\\omega}} (\\partial_t\\psi - \\kappa \\partial_{xx}\\psi)^2 \\,dt\\,dx$$\nNow, we substitute $\\psi(x,t) = \\sin(kx)\\cos(\\omega t)$:\n$$\\partial_t \\psi = -\\omega \\sin(kx)\\sin(\\omega t)$$\n$$\\partial_x \\psi = k \\cos(kx)\\cos(\\omega t) \\quad \\implies \\quad \\partial_{xx} \\psi = -k^2 \\sin(kx)\\cos(\\omega t)$$\nSo,\n$$\\mathcal{L}\\psi = -\\omega \\sin(kx)\\sin(\\omega t) - \\kappa(-k^2 \\sin(kx)\\cos(\\omega t)) = \\sin(kx) \\left( \\kappa k^2 \\cos(\\omega t) - \\omega \\sin(\\omega t) \\right)$$\nThe integrand is:\n$$(\\mathcal{L}\\psi)^2 = \\sin^2(kx) \\left( \\kappa k^2 \\cos(\\omega t) - \\omega \\sin(\\omega t) \\right)^2$$\nThe integral for $H$ is separable:\n$$H = \\left(\\int_{0}^{2\\pi} \\sin^2(kx) \\,dx\\right) \\left(\\int_{0}^{\\frac{2\\pi}{\\omega}} \\left( \\kappa k^2 \\cos(\\omega t) - \\omega \\sin(\\omega t) \\right)^2 \\,dt\\right)$$\nLet's evaluate the integrals. Since $k \\in \\mathbb{N}$:\n$$\\int_{0}^{2\\pi} \\sin^2(kx) \\,dx = \\int_{0}^{2\\pi} \\frac{1 - \\cos(2kx)}{2} \\,dx = \\left[\\frac{x}{2} - \\frac{\\sin(2kx)}{4k}\\right]_{0}^{2\\pi} = \\pi$$\nFor the time integral, we expand the square:\n$$(\\dots)^2 = \\kappa^2 k^4 \\cos^2(\\omega t) - 2\\kappa k^2 \\omega \\cos(\\omega t)\\sin(\\omega t) + \\omega^2 \\sin^2(\\omega t)$$\nIntegrating over one period $T = \\frac{2\\pi}{\\omega}$:\n$\\int_0^T \\cos^2(\\omega t) \\,dt = \\frac{T}{2} = \\frac{\\pi}{\\omega}$\n$\\int_0^T \\sin^2(\\omega t) \\,dt = \\frac{T}{2} = \\frac{\\pi}{\\omega}$\n$\\int_0^T \\cos(\\omega t)\\sin(\\omega t) \\,dt = \\frac{1}{2}\\int_0^T \\sin(2\\omega t) \\,dt = 0$\nSo, the time integral evaluates to:\n$$\\kappa^2 k^4 \\left(\\frac{\\pi}{\\omega}\\right) - 0 + \\omega^2 \\left(\\frac{\\pi}{\\omega}\\right) = \\frac{\\pi}{\\omega}(\\kappa^2 k^4 + \\omega^2)$$\nMultiplying the space and time integrals gives the Hessian:\n$$H = \\pi \\cdot \\frac{\\pi}{\\omega}(\\kappa^2 k^4 + \\omega^2) = \\frac{\\pi^2}{\\omega}(\\kappa^2 k^4 + \\omega^2)$$\n\n**Task 3: Determine the largest learning rate $\\eta_{\\max}$**\n\nThe gradient descent update is $a_{n+1} = a_n - \\eta \\frac{dL}{da}(a_n)$. Near the minimum $a=0$, we use the linearized gradient $\\frac{dL}{da}(a_n) \\approx H a_n$. The linearized update rule becomes:\n$$a_{n+1} \\approx a_n - \\eta H a_n = (1 - \\eta H) a_n$$\nThis is a linear iterative map. For the iterates $a_n$ to converge to $0$, the amplification factor must have a magnitude less than $1$:\n$$|1 - \\eta H| < 1$$\nThis inequality is equivalent to $-1 < 1 - \\eta H < 1$.\nThe right side, $1 - \\eta H < 1$, implies $-\\eta H < 0$. Since the learning rate $\\eta > 0$ and the Hessian $H = \\frac{\\pi^2}{\\omega}(\\kappa^2 k^4 + \\omega^2)$ is strictly positive (given $\\kappa>0, \\omega>0, k\\in\\mathbb{N}$), this condition is always satisfied.\nThe left side, $-1 < 1 - \\eta H$, implies $\\eta H < 2$.\nThis gives the stability condition on the learning rate:\n$$\\eta < \\frac{2}{H}$$\nThe largest learning rate $\\eta_{\\max}$ that guarantees stability is therefore the upper bound of this interval.\n$$\\eta_{\\max} = \\frac{2}{H}$$\nSubstituting the expression for $H$:\n$$\\eta_{\\max} = \\frac{2}{\\frac{\\pi^2}{\\omega}(\\kappa^2 k^4 + \\omega^2)}$$\nSimplifying the expression gives the final answer:\n$$\\eta_{\\max} = \\frac{2\\omega}{\\pi^2(\\kappa^2 k^4 + \\omega^2)}$$",
            "answer": "$$\\boxed{\\frac{2\\omega}{\\pi^{2}(\\kappa^{2}k^{4} + \\omega^{2})}}$$"
        },
        {
            "introduction": "One of the most powerful paradigms in hybrid modeling is to embed governing equations directly into the training objective. For complex systems governed by partial differential equations, like the Navier-Stokes equations, the \"weak formulation\" provides a robust and mathematically elegant framework for this task. This practice guides you through the process of constructing a weak-form loss function, projecting the PDE residuals onto carefully chosen test functions to enforce physical laws in a flexible, integral sense .",
            "id": "3884404",
            "problem": "Consider the incompressible, steady, nondimensional Navier–Stokes equations in two spatial dimensions on the unit square domain $\\Omega = [0,1]\\times[0,1]$ with homogeneous Dirichlet conditions on velocity test functions, treated in the sense of the Finite Element Method (FEM). Let $\\boldsymbol{u}(\\boldsymbol{x})$ denote the velocity field, $p(\\boldsymbol{x})$ denote the pressure, $\\nu$ denote the kinematic viscosity (dimensionless here), and $\\boldsymbol{f}(\\boldsymbol{x})$ denote a body force (set to $\\boldsymbol{0}$ for this task). The governing laws are the balance of linear momentum (Newton’s second law of motion for a continuum) and mass conservation for an incompressible fluid:\n$$\n\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u} + \\nabla p - \\nu\\,\\Delta\\boldsymbol{u} = \\boldsymbol{f} = \\boldsymbol{0},\\quad \\nabla\\cdot\\boldsymbol{u} = 0,\n$$\nwhere every operator and quantity is in standard mathematical notation, angles are in radians, and all variables are dimensionless.\n\nYou are asked to construct a hybrid physics–data training objective in weak form by projecting the residuals of the momentum and continuity equations onto specified test function spaces, consistent with the FEM approach. Specifically, use divergence-free vector-valued test functions $\\boldsymbol{w}_i$ that vanish on the boundary $\\partial\\Omega$, so that the pressure term is eliminated from the weak momentum balance, and use scalar-valued test functions $q_j$ for the continuity equation. The training objective should be the sum of squares of the residual projections:\n- The momentum residual projection integrals $\\int_{\\Omega} \\boldsymbol{w}_i \\cdot \\left(\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u} - \\nu\\,\\Delta\\boldsymbol{u}\\right)\\,\\mathrm{d}\\boldsymbol{x}$, and\n- The continuity residual projection integrals $\\int_{\\Omega} q_j\\,(\\nabla\\cdot\\boldsymbol{u})\\,\\mathrm{d}\\boldsymbol{x}$,\naggregated as a single scalar objective. All integrals must be computed numerically by a composite rectangle rule on a uniform grid.\n\nUse the following scientifically consistent, fixed choices for basis functions and parameterization to make the problem fully specified and testable:\n- Parameterize the velocity field $\\boldsymbol{u}(\\boldsymbol{x})$ as a linear combination of four non-divergence-free, smooth, boundary-vanishing basis fields on $\\Omega$:\n  $$\n  \\boldsymbol{u}(\\boldsymbol{x};\\boldsymbol{c}) = c_1\\,\\begin{bmatrix}\\sin^2(\\pi x)\\,\\sin^2(\\pi y)\\\\ 0\\end{bmatrix} + c_2\\,\\begin{bmatrix}0\\\\ \\sin^2(\\pi x)\\,\\sin^2(\\pi y)\\end{bmatrix} + c_3\\,\\begin{bmatrix}\\sin^2(2\\pi x)\\,\\sin^2(\\pi y)\\\\ 0\\end{bmatrix} + c_4\\,\\begin{bmatrix}0\\\\ \\sin^2(\\pi x)\\,\\sin^2(2\\pi y)\\end{bmatrix},\n  $$\n  where $\\boldsymbol{x}=[x,y]^\\top$, $\\boldsymbol{c}=[c_1,c_2,c_3,c_4]^\\top$, and trigonometric arguments are in radians. All quantities are dimensionless.\n- Choose three divergence-free, boundary-vanishing vector test functions $\\boldsymbol{w}_i(\\boldsymbol{x}) = [\\partial_y\\phi_i(\\boldsymbol{x}),\\, -\\partial_x\\phi_i(\\boldsymbol{x})]^\\top$ constructed from streamfunctions $\\phi_i$ as:\n  $$\n  \\phi_1(x,y) = \\sin^2(\\pi x)\\,\\sin^2(\\pi y),\\quad \\phi_2(x,y) = \\sin^2(2\\pi x)\\,\\sin^2(\\pi y),\\quad \\phi_3(x,y) = \\sin^2(\\pi x)\\,\\sin^2(2\\pi y).\n  $$\n- Choose three scalar continuity test functions $q_j(x,y)$ identical to the above $\\phi_j(x,y)$.\n- Set $\\boldsymbol{f}(\\boldsymbol{x})=\\boldsymbol{0}$.\n- Discretize $\\Omega$ with a uniform tensor-product grid of $N\\times N$ points, where $N$ is a given positive integer, with grid spacings $\\Delta x = 1/(N-1)$ and $\\Delta y = 1/(N-1)$. Approximate each integral $\\int_{\\Omega} g(x,y)\\,\\mathrm{d}\\boldsymbol{x}$ by the composite rectangle rule $I \\approx \\sum_{i=1}^{N}\\sum_{j=1}^{N} g(x_i,y_j)\\,\\Delta x\\,\\Delta y$.\n\nYour program must:\n- Derive from first principles the weak-form projections consistent with the above setup and implement the numerical evaluation of all terms using the exact analytic spatial derivatives of the basis functions.\n- Construct the training objective\n  $$\n  J(\\boldsymbol{c};\\nu,\\lambda) = \\sum_{i=1}^{3}\\left(\\int_{\\Omega} \\boldsymbol{w}_i \\cdot \\left(\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u} - \\nu\\,\\Delta\\boldsymbol{u}\\right)\\,\\mathrm{d}\\boldsymbol{x}\\right)^2 + \\lambda\\sum_{j=1}^{3}\\left(\\int_{\\Omega} q_j \\,(\\nabla\\cdot\\boldsymbol{u})\\,\\mathrm{d}\\boldsymbol{x}\\right)^2,\n  $$\n  where $\\lambda$ is a nonnegative scalar weight (dimensionless), and every integral is approximated as specified. Angles in all trigonometric functions must be interpreted in radians.\n\nTest Suite:\nEvaluate $J(\\boldsymbol{c};\\nu,\\lambda)$ for the following four parameter sets to test different regimes and edge cases. All quantities are dimensionless.\n- Case $1$: $\\nu = 0.1$, $\\lambda = 1.0$, $N=128$, $\\boldsymbol{c}=[0.0,0.0,0.0,0.0]^\\top$.\n- Case $2$: $\\nu = 0.1$, $\\lambda = 1.0$, $N=128$, $\\boldsymbol{c}=[0.2,0.0,0.0,0.0]^\\top$.\n- Case $3$: $\\nu = 1.0$, $\\lambda = 1.0$, $N=64$, $\\boldsymbol{c}=[0.2,0.2,0.0,0.0]^\\top$.\n- Case $4$: $\\nu = 0.01$, $\\lambda = 1.0$, $N=96$, $\\boldsymbol{c}=[0.1,0.05,0.08,0.02]^\\top$.\n\nAnswer Specification:\n- Compute and return the objective values $J$ for all four cases as dimensionless floats.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$. Express each objective value as a floating-point number. No other text should be printed.",
            "solution": "The problem requires the construction and evaluation of a hybrid physics–data training objective, denoted $J(\\boldsymbol{c};\\nu,\\lambda)$, for the steady, incompressible Navier-Stokes equations on a two-dimensional unit square domain $\\Omega = [0,1]\\times[0,1]$. The process is validated as scientifically and mathematically sound, and a step-by-step derivation and numerical implementation plan is formulated.\n\nThe objective function is defined as the sum of squared, projected residuals of the momentum and continuity equations:\n$$\nJ(\\boldsymbol{c};\\nu,\\lambda) = \\sum_{i=1}^{3}\\left(\\int_{\\Omega} \\boldsymbol{w}_i \\cdot \\left(\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u} - \\nu\\,\\Delta\\boldsymbol{u}\\right)\\,\\mathrm{d}\\boldsymbol{x}\\right)^2 + \\lambda\\sum_{j=1}^{3}\\left(\\int_{\\Omega} q_j \\,(\\nabla\\cdot\\boldsymbol{u})\\,\\mathrm{d}\\boldsymbol{x}\\right)^2\n$$\nHere, $\\boldsymbol{u}$ is the velocity field parameterized by coefficients $\\boldsymbol{c}$, $\\nu$ is the kinematic viscosity, $\\lambda$ is a weighting factor, $\\boldsymbol{w}_i$ are vector test functions, and $q_j$ are scalar test functions. The body force $\\boldsymbol{f}$ is $\\boldsymbol{0}$. The pressure term $\\nabla p$ is eliminated from the momentum equation's weak form by choosing divergence-free test functions $\\boldsymbol{w}_i$ that satisfy $\\nabla \\cdot \\boldsymbol{w}_i = 0$ and vanish on the boundary $\\partial\\Omega$, which allows the use of the identity $\\int_{\\Omega} \\boldsymbol{w}_i \\cdot \\nabla p \\, \\mathrm{d}\\boldsymbol{x} = -\\int_{\\Omega} p (\\nabla \\cdot \\boldsymbol{w}_i) \\, \\mathrm{d}\\boldsymbol{x} = 0$.\n\nThe evaluation of $J$ requires three main components: the parameterized velocity field $\\boldsymbol{u}$ and its derivatives, the defined test functions $\\boldsymbol{w}_i$ and $q_j$, and the numerical integration scheme.\n\nFirst, we define the velocity field $\\boldsymbol{u}(\\boldsymbol{x};\\boldsymbol{c}) = [u(\\boldsymbol{x}), v(\\boldsymbol{x})]^\\top$ with $\\boldsymbol{x}=[x,y]^\\top$ and coefficients $\\boldsymbol{c}=[c_1,c_2,c_3,c_4]^\\top$. It is given as a linear combination of four basis fields:\n$$\n\\boldsymbol{u}(\\boldsymbol{x};\\boldsymbol{c}) = c_1\\,\\begin{bmatrix}\\sin^2(\\pi x)\\sin^2(\\pi y)\\\\ 0\\end{bmatrix} + c_2\\,\\begin{bmatrix}0\\\\ \\sin^2(\\pi x)\\sin^2(\\pi y)\\end{bmatrix} + c_3\\,\\begin{bmatrix}\\sin^2(2\\pi x)\\sin^2(\\pi y)\\\\ 0\\end{bmatrix} + c_4\\,\\begin{bmatrix}0\\\\ \\sin^2(\\pi x)\\sin^2(2\\pi y)\\end{bmatrix}\n$$\nThe components $u$ and $v$ are:\n$$\nu(x,y) = \\left(c_1\\sin^2(\\pi x) + c_3\\sin^2(2\\pi x)\\right)\\sin^2(\\pi y)\n$$\n$$\nv(x,y) = \\sin^2(\\pi x)\\left(c_2\\sin^2(\\pi y) + c_4\\sin^2(2\\pi y)\\right)\n$$\nTo construct the residuals, we need the divergence $\\nabla \\cdot \\boldsymbol{u}$, the Laplacian $\\Delta\\boldsymbol{u}$, and the convective term $\\boldsymbol{u} \\cdot \\nabla\\boldsymbol{u}$. The necessary partial derivatives are found using the rules $(\\sin^2(k\\pi z))' = k\\pi\\sin(2k\\pi z)$ and $(\\sin^2(k\\pi z))'' = 2(k\\pi)^2\\cos(2k\\pi z)$.\n\nThe divergence is $\\nabla \\cdot \\boldsymbol{u} = \\partial_x u + \\partial_y v$:\n$$\n\\nabla \\cdot \\boldsymbol{u} = \\left(c_1\\pi\\sin(2\\pi x) + c_3 2\\pi\\sin(4\\pi x)\\right)\\sin^2(\\pi y) + \\sin^2(\\pi x)\\left(c_2\\pi\\sin(2\\pi y) + c_4 2\\pi\\sin(4\\pi y)\\right)\n$$\nThis forms the continuity equation residual, $R_c = \\nabla \\cdot \\boldsymbol{u}$.\n\nThe momentum equation residual is $\\boldsymbol{R}_m = \\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u} - \\nu\\,\\Delta\\boldsymbol{u}$.\nThe convective term $\\boldsymbol{u} \\cdot \\nabla \\boldsymbol{u}$ has components:\n$$\n(\\boldsymbol{u} \\cdot \\nabla \\boldsymbol{u})_x = u\\,\\partial_x u + v\\,\\partial_y u\n$$\n$$\n(\\boldsymbol{u} \\cdot \\nabla \\boldsymbol{u})_y = u\\,\\partial_x v + v\\,\\partial_y v\n$$\nThe Laplacian $\\Delta \\boldsymbol{u} = [\\Delta u, \\Delta v]^\\top$ has components:\n$$\n\\Delta u = \\partial_{xx}u + \\partial_{yy}u\n$$\n$$\n\\Delta v = \\partial_{xx}v + \\partial_{yy}v\n$$\nAll these terms are derived by applying the differentiation rules to the expressions for $u$ and $v$.\n\nSecond, we define the test functions. The scalar test functions $q_j$ are identical to the streamfunctions $\\phi_j$:\n$$\nq_1(x,y) = \\phi_1 = \\sin^2(\\pi x)\\sin^2(\\pi y)\n$$\n$$\nq_2(x,y) = \\phi_2 = \\sin^2(2\\pi x)\\sin^2(\\pi y)\n$$\n$$\nq_3(x,y) = \\phi_3 = \\sin^2(\\pi x)\\sin^2(2\\pi y)\n$$\nThe vector test functions $\\boldsymbol{w}_i=[\\partial_y \\phi_i, -\\partial_x \\phi_i]^\\top$ are constructed to be divergence-free:\n$$\n\\boldsymbol{w}_1(x,y) = \\begin{bmatrix} \\pi\\sin^2(\\pi x)\\sin(2\\pi y) \\\\ -\\pi\\sin(2\\pi x)\\sin^2(\\pi y) \\end{bmatrix}\n$$\n$$\n\\boldsymbol{w}_2(x,y) = \\begin{bmatrix} \\pi\\sin^2(2\\pi x)\\sin(2\\pi y) \\\\ -2\\pi\\sin(4\\pi x)\\sin^2(\\pi y) \\end{bmatrix}\n$$\n$$\n\\boldsymbol{w}_3(x,y) = \\begin{bmatrix} 2\\pi\\sin^2(\\pi x)\\sin(4\\pi y) \\\\ -\\pi\\sin(2\\pi x)\\sin^2(2\\pi y) \\end{bmatrix}\n$$\nThird, we define the numerical integration. The domain $\\Omega$ is discretized into a uniform grid of $N \\times N$ points $(x_i, y_j)$ with $x_i = (i-1)/(N-1)$ and $y_j = (j-1)/(N-1)$ for $i,j \\in \\{1,...,N\\}$. Any integral $\\int_{\\Omega} g(\\boldsymbol{x})\\,\\mathrm{d}\\boldsymbol{x}$ is approximated by a composite rectangle rule:\n$$\nI \\approx \\sum_{i=1}^{N}\\sum_{j=1}^{N} g(x_i,y_j)\\,\\Delta x\\,\\Delta y\n$$\nwhere $\\Delta x = \\Delta y = 1/(N-1)$.\n\nThe computational procedure for each test case $(\\boldsymbol{c}, \\nu, \\lambda, N)$ is as follows:\n1.  Generate the $N \\times N$ grid of coordinates $(x_i, y_j)$.\n2.  Evaluate the basic trigonometric functions and their derivatives on the entire grid.\n3.  Combine these to compute the fields for $\\boldsymbol{u}$ and its derivatives ($\\nabla\\cdot\\boldsymbol{u}$, $\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u}$, $\\Delta\\boldsymbol{u}$) for the given $\\boldsymbol{c}$.\n4.  Compute the values of the test functions $\\boldsymbol{w}_i$ and $q_j$ on the grid.\n5.  For each $i \\in \\{1,2,3\\}$, form the momentum residual integrand $I_{m,i} = \\boldsymbol{w}_i \\cdot (\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u} - \\nu\\Delta\\boldsymbol{u})$ and compute its integral $M_i$ using the rectangle rule.\n6.  For each $j \\in \\{1,2,3\\}$, form the continuity residual integrand $I_{c,j} = q_j(\\nabla\\cdot\\boldsymbol{u})$ and compute its integral $C_j$ using the rectangle rule.\n7.  Calculate the final objective value as $J = \\sum_{i=1}^{3} (M_i)^2 + \\lambda \\sum_{j=1}^{3} (C_j)^2$.\n\nThis procedure is implemented for each of the four test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_objective(nu, lam, N, c):\n    \"\"\"\n    Constructs and evaluates the training objective J(c; nu, lambda).\n    \"\"\"\n    # 1. Set up the grid\n    x = np.linspace(0.0, 1.0, N)\n    y = np.linspace(0.0, 1.0, N)\n    xv, yv = np.meshgrid(x, y, indexing='ij')\n    dx = 1.0 / (N - 1)\n    dy = 1.0 / (N - 1)\n    dA = dx * dy\n\n    # 2. Evaluate basis functions and their derivatives on the grid\n    pi = np.pi\n    \n    # Trigonometric components for basis functions\n    s1x = np.sin(pi * xv)**2\n    s1y = np.sin(pi * yv)**2\n    s2x = np.sin(2 * pi * xv)**2\n    s2y = np.sin(2 * pi * yv)**2\n\n    # First derivatives\n    ds1dx = pi * np.sin(2 * pi * xv)\n    ds1dy = pi * np.sin(2 * pi * yv)\n    ds2dx = 2 * pi * np.sin(4 * pi * xv)\n    ds2dy = 2 * pi * np.sin(4 * pi * yv)\n\n    # Second derivatives\n    d2s1dx2 = 2 * pi**2 * np.cos(2 * pi * xv)\n    d2s1dy2 = 2 * pi**2 * np.cos(2 * pi * yv)\n    d2s2dx2 = 8 * pi**2 * np.cos(4 * pi * xv)\n    d2s2dy2 = 8 * pi**2 * np.cos(4 * pi * yv)\n    \n    c1, c2, c3, c4 = c\n\n    # 3. Construct velocity field and its derivatives\n    # Velocity components (u, v)\n    u = c1 * s1x * s1y + c3 * s2x * s1y\n    v = c2 * s1x * s1y + c4 * s1x * s2y\n\n    # Velocity gradient components\n    du_dx = c1 * ds1dx * s1y + c3 * ds2dx * s1y\n    du_dy = c1 * s1x * ds1dy + c3 * s2x * ds1dy\n    dv_dx = c2 * ds1dx * s1y + c4 * ds1dx * s2y\n    dv_dy = c2 * s1x * ds1dy + c4 * s1x * ds2dy\n    \n    # Convective term: u * grad(u)\n    conv_x = u * du_dx + v * du_dy\n    conv_y = u * dv_dx + v * dv_dy\n\n    # Laplacian of velocity: lap(u)\n    lap_u = (c1 * (d2s1dx2 * s1y + s1x * d2s1dy2) + \n             c3 * (d2s2dx2 * s1y + s2x * d2s1dy2))\n    lap_v = (c2 * (d2s1dx2 * s1y + s1x * d2s1dy2) + \n             c4 * (d2s1dx2 * s2y + s1x * d2s2dy2))\n\n    # Divergence of velocity: div(u)\n    div_u = du_dx + dv_dy\n\n    # 4. Construct test functions\n    # Scalar test functions q_j\n    q = [\n        s1x * s1y,       # q1\n        s2x * s1y,       # q2\n        s1x * s2y,       # q3\n    ]\n\n    # Vector test functions w_i = [d(phi_i)/dy, -d(phi_i)/dx]\n    w = [\n        (s1x * ds1dy, -ds1dx * s1y),       # w1\n        (s2x * ds1dy, -ds2dx * s1y),       # w2\n        (s1x * ds2dy, -ds1dx * s2y),       # w3\n    ]\n\n    # 5. Compute residual projections\n    # Momentum residual: R_m = u.grad(u) - nu * lap(u)\n    res_mom_x = conv_x - nu * lap_u\n    res_mom_y = conv_y - nu * lap_v\n    \n    sum_sq_mom = 0.0\n    for w_ix, w_iy in w:\n        integrand = w_ix * res_mom_x + w_iy * res_mom_y\n        integral = np.sum(integrand) * dA\n        sum_sq_mom += integral**2\n        \n    # Continuity residual: R_c = div(u)\n    res_cont = div_u\n    \n    sum_sq_cont = 0.0\n    for q_j in q:\n        integrand = q_j * res_cont\n        integral = np.sum(integrand) * dA\n        sum_sq_cont += integral**2\n\n    # 6. Calculate final objective J\n    J = sum_sq_mom + lam * sum_sq_cont\n    return J\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'nu': 0.1, 'lam': 1.0, 'N': 128, 'c': np.array([0.0, 0.0, 0.0, 0.0])},\n        {'nu': 0.1, 'lam': 1.0, 'N': 128, 'c': np.array([0.2, 0.0, 0.0, 0.0])},\n        {'nu': 1.0, 'lam': 1.0, 'N': 64,  'c': np.array([0.2, 0.2, 0.0, 0.0])},\n        {'nu': 0.01, 'lam': 1.0, 'N': 96, 'c': np.array([0.1, 0.05, 0.08, 0.02])},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        result = calculate_objective(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}