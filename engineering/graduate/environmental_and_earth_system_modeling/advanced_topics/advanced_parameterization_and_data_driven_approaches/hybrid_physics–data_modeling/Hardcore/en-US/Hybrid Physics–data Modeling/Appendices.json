{
    "hands_on_practices": [
        {
            "introduction": "Our first practice explores one of the most powerful and elegant ways to incorporate physics into a data-driven model: dimensional analysis. Before we even design a neural network or write a loss function, we can simplify the problem and enforce physical consistency by reformulating our inputs into dimensionless groups . This exercise will guide you through using the Buckingham $\\Pi$ theorem to reduce a model's hypothesis space, enhancing its ability to generalize from limited data.",
            "id": "3884451",
            "problem": "A hybrid physics–data model is being built to learn a sediment transport law for suspended load over rough beds in an open-channel flow. The learned mapping is to be constructed from dimensional predictor variables $(\\rho,\\nu,U,D,g)$, where $\\rho$ is the fluid density, $\\nu$ is the kinematic viscosity, $U$ is a characteristic mean flow speed, $D$ is a characteristic sediment grain diameter, and $g$ is gravitational acceleration. The model will subsequently be trained on laboratory and field datasets that span a wide range of $U$, $D$, $\\nu$, and $g$ values. To improve extrapolation and reduce the hypothesis space, the inputs to the learned component must be nondimensional.\n\nUsing only the fundamental principles of dimensional homogeneity and the Buckingham $\\Pi$ theorem, derive a minimal set of independent nondimensional input groups $\\Pi_i$ that can be formed from $(\\rho,\\nu,U,D,g)$. Explicitly construct these groups by solving for exponents that make each $\\Pi_i$ dimensionless, and justify whether $\\rho$ can enter any dimensionless input formed from the given set. Then, explain, without invoking any empirical sediment transport formulas, why replacing $(\\rho,\\nu,U,D,g)$ by the derived $\\Pi_i$ reduces hypothesis complexity in a hybrid physics–data model, and quantify this reduction by comparing the number of free coefficients in a generic total-degree $p$ polynomial model in the original dimensional inputs versus a total-degree $p$ polynomial model in the $\\Pi_i$.\n\nProvide your final answer as the explicit analytical expressions for the independent $\\Pi_i$ collected in a single row matrix. No numerical evaluation is required, and no rounding is needed. Do not include units in the final answer.",
            "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in the principles of dimensional analysis, is well-posed, objective, and internally consistent. It presents a standard but rigorous application of the Buckingham $\\Pi$ theorem to a relevant problem in environmental modeling.\n\nThe task is to derive a minimal set of independent nondimensional groups from a set of $n=5$ physical variables relevant to suspended sediment transport: fluid density $\\rho$, kinematic viscosity $\\nu$, mean flow speed $U$, sediment grain diameter $D$, and gravitational acceleration $g$. The fundamental dimensions involved are Mass ($M$), Length ($L$), and Time ($T$).\n\nFirst, we list the variables and their corresponding dimensions:\n1.  Fluid density, $\\rho$: $[\\rho] = M L^{-3}$\n2.  Kinematic viscosity, $\\nu$: $[\\nu] = L^2 T^{-1}$\n3.  Flow speed, $U$: $[U] = L T^{-1}$\n4.  Grain diameter, $D$: $[D] = L$\n5.  Gravitational acceleration, $g$: $[g] = L T^{-2}$\n\nThe Buckingham $\\Pi$ theorem states that a physically meaningful relationship between $n$ variables can be expressed as a relationship between $n-k$ independent dimensionless groups, where $k$ is the number of fundamental dimensions required to describe the variables (i.e., the rank of the dimensional matrix).\n\nLet us construct the dimensional matrix, where the rows correspond to the fundamental dimensions ($M, L, T$) and the columns correspond to the variables ($\\rho, \\nu, U, D, g$):\n$$\n\\begin{pmatrix}\n & \\rho & \\nu & U & D & g \\\\\nM & 1 & 0 & 0 & 0 & 0 \\\\\nL & -3 & 2 & 1 & 1 & 1 \\\\\nT & 0 & -1 & -1 & 0 & -2\n\\end{pmatrix}\n$$\nThe rank of this matrix, $k$, is the number of linearly independent rows (or columns). The first row, corresponding to Mass ($M$), is clearly independent of the others. The submatrix for dimensions $L, T$ and variables $\\nu, U$ is $\\begin{pmatrix} 2 & 1 \\\\ -1 & -1 \\end{pmatrix}$, which has a determinant of $(2)(-1) - (1)(-1) = -1 \\neq 0$. Thus, the last two rows are also linearly independent. The rank of the matrix is $k=3$.\n\nAccording to the Buckingham $\\Pi$ theorem, the number of independent dimensionless groups is $n - k = 5 - 3 = 2$. We will denote these as $\\Pi_1$ and $\\Pi_2$.\n\nThe problem asks whether the fluid density, $\\rho$, can be a component of any dimensionless group formed from the given set of variables. Let a generic dimensionless group $\\Pi$ be formed by the product of the variables raised to some exponents:\n$$ \\Pi = \\rho^{e_1} \\nu^{e_2} U^{e_3} D^{e_4} g^{e_5} $$\nFor $\\Pi$ to be dimensionless, its total dimensions must be $M^0 L^0 T^0$. We can write the dimensional equation as:\n$$ [\\Pi] = (M L^{-3})^{e_1} (L^2 T^{-1})^{e_2} (L T^{-1})^{e_3} (L)^{e_4} (L T^{-2})^{e_5} = M^{e_1} L^{-3e_1+2e_2+e_3+e_4+e_5} T^{-e_2-e_3-2e_5} $$\nFor this to be dimensionless, the exponent of each fundamental dimension must be zero. For the Mass dimension ($M$), we have:\n$$ e_1 = 0 $$\nThis result proves that the exponent of $\\rho$ must be zero in any dimensionless group formed from this set of variables. Therefore, $\\rho$ cannot enter into any of the dimensionless inputs. This is a direct consequence of $\\rho$ being the only variable in the set that carries the mass dimension, leaving no other variable to cancel it out.\n\nSince $\\rho$ is excluded, the problem reduces to finding $2$ dimensionless groups from the remaining $4$ variables ($\\nu, U, D, g$) which are described by $2$ fundamental dimensions ($L, T$). We select $k=2$ repeating variables from this set. A valid choice must contain both $L$ and $T$ and be dimensionally independent. Let us choose $U$ and $D$.\n- $[U] = L T^{-1}$\n- $[D] = L$\nThe dimensional matrix for $\\{U, D\\}$ is $\\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix}$, which has determinant $1 \\neq 0$, confirming they are a valid choice.\n\nWe now form the two $\\Pi$ groups using the remaining variables, $\\nu$ and $g$.\n\nFirst, we form $\\Pi_1$ with $\\nu$:\n$$ \\Pi_1 = U^a D^b \\nu^1 $$\nThe dimensional equation is:\n$$ [\\Pi_1] = (L T^{-1})^a (L)^b (L^2 T^{-1})^1 = L^{a+b+2} T^{-a-1} = L^0 T^0 $$\nEquating the exponents:\n- For $T$: $-a-1 = 0 \\implies a = -1$\n- For $L$: $a+b+2 = 0 \\implies -1+b+2 = 0 \\implies b = -1$\nThis gives $\\Pi_1 = U^{-1} D^{-1} \\nu = \\frac{\\nu}{UD}$. For convention, we use its reciprocal, the particle Reynolds number:\n$$ \\Pi_1 = \\frac{UD}{\\nu} $$\nSecond, we form $\\Pi_2$ with $g$:\n$$ \\Pi_2 = U^a D^b g^1 $$\nThe dimensional equation is:\n$$ [\\Pi_2] = (L T^{-1})^a (L)^b (L T^{-2})^1 = L^{a+b+1} T^{-a-2} = L^0 T^0 $$\nEquating the exponents:\n- For $T$: $-a-2 = 0 \\implies a = -2$\n- For $L$: $a+b+1 = 0 \\implies -2+b+1 = 0 \\implies b = 1$\nThis gives $\\Pi_2 = U^{-2} D^{1} g = \\frac{gD}{U^2}$. For convention, we use its reciprocal, the square of the particle Froude number:\n$$ \\Pi_2 = \\frac{U^2}{gD} $$\nThus, a minimal set of independent nondimensional input groups is $\\{\\frac{UD}{\\nu}, \\frac{U^2}{gD}\\}$.\n\nReplacing the original dimensional inputs $(\\rho, \\nu, U, D, g)$ with the derived $\\Pi_i$ groups reduces hypothesis complexity in a hybrid physics–data model for several reasons. The core principle is that dimensional analysis embeds physical law (the principle of dimensional homogeneity) into the structure of the problem.\n1.  **Dimensionality Reduction**: The search space for the unknown functional relationship is reduced from a $5$-dimensional space of inputs $(\\rho, \\nu, U, D, g)$ to a $2$-dimensional space of inputs $(\\Pi_1, \\Pi_2)$. This is a drastic simplification. A data-driven model working in the lower-dimensional space is less susceptible to the \"curse of dimensionality\", requires less data to train robustly, and is computationally more efficient.\n2.  **Imposition of Physical Constraints**: By reformulating the problem in terms of dimensionless groups, we are forcing the learned model to conform to the principle of dimensional homogeneity. Any function of $\\Pi_1$ and $\\Pi_2$ will automatically yield a dimensionally correct result. This prunes the hypothesis space by eliminating all physically impossible functional forms, which a naive model operating on dimensional inputs would otherwise have to learn to avoid.\n3.  **Enhanced Generalization (Similitude)**: The dimensionless groups represent ratios of physical effects (e.g., $Re$ is a ratio of inertial to viscous forces). The principle of dynamic similarity states that two systems with different dimensional parameters ($\\nu$, $U$, $D$, etc.) will behave identically if their corresponding dimensionless groups are identical. A model trained on $(\\Pi_1, \\Pi_2)$ inherently learns this principle. It can accurately predict the behavior of a large-scale river flow based on data from a small-scale lab flume, provided they share the same $(\\Pi_1, \\Pi_2)$ values, even if the individual dimensional variables are vastly different. A model trained on the raw dimensional variables would treat these as entirely separate data points and would not be guaranteed to generalize correctly.\n\nTo quantify this complexity reduction, we compare the number of free coefficients in a generic total-degree $p$ polynomial model. The number of coefficients $N$ for such a model in $d$ variables is given by the formula for combinations with repetition: $N(d, p) = \\binom{p+d}{d}$.\n- For the model in the original dimensional inputs, we have $d_1 = 5$ variables $(\\rho, \\nu, U, D, g)$. The number of coefficients is:\n  $$ N_1 = \\binom{p+5}{5} = \\frac{(p+5)(p+4)(p+3)(p+2)(p+1)}{120} $$\n  This model is physically naive as it does not enforce dimensional consistency among its terms, but it represents the complexity a purely data-driven model would face.\n- For the model in the nondimensional inputs, we have $d_2 = 2$ variables $(\\Pi_1, \\Pi_2)$. The number of coefficients is:\n  $$ N_2 = \\binom{p+2}{2} = \\frac{(p+2)(p+1)}{2} $$\nThe number of coefficients grows as $O(p^5)$ in the dimensional case versus $O(p^2)$ in the nondimensional case. For example, for a quadratic model ($p=2$), the dimensional model has $N_1 = \\binom{7}{5} = 21$ coefficients, while the nondimensional model has only $N_2 = \\binom{4}{2} = 6$ coefficients. This dramatic reduction in the number of free parameters makes the model far simpler to specify, easier to constrain with available data, and less prone to overfitting.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{UD}{\\nu} & \\frac{U^2}{gD}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Many physical systems are governed by strict conservation laws, such as the conservation of mass, energy, or momentum. This practice demonstrates how to enforce these inviolable principles as hard constraints within a hybrid model's training process . You will construct an objective function using an augmented Lagrangian to enforce a global conservation law and derive a post-hoc correction that guarantees the model's predictions are perfectly conservative.",
            "id": "3884400",
            "problem": "Consider a passive tracer field $c(\\mathbf{x},t)$ transported by a known velocity field $\\mathbf{u}(\\mathbf{x},t)$ with a volumetric source term $S(\\mathbf{x},t)$ on a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$ with outward unit normal $\\mathbf{n}$ on its boundary $\\partial\\Omega$. The governing global conservation law for the tracer is that the rate of change of total mass within $\\Omega$ plus the net outward flux across $\\partial\\Omega$ equals the total production inside $\\Omega$. This can be expressed as the integral balance\n$$\n\\partial_{t}\\int_{\\Omega} c\\,d\\mathbf{x} \\;+\\; \\int_{\\partial\\Omega} c\\,\\mathbf{u}\\cdot\\mathbf{n}\\,ds \\;=\\; \\int_{\\Omega} S\\,d\\mathbf{x}.\n$$\nA hybrid physics–data model uses a parameterized surrogate $c_{\\theta}(\\mathbf{x},t)$ to predict the tracer, and must enforce the above conservation law as a hard constraint during training.\n\nYou are given discrete time levels $t_{k}$ and $t_{k+1}$ with uniform time step $\\Delta t = t_{k+1}-t_{k}$. At each time $t_{k}$ and $t_{k+1}$, the domain integral is approximated by quadrature with interior points $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ and positive weights $\\{w_{i}\\}_{i=1}^{N}$, and the boundary integral at $t_{k+1}$ is approximated by boundary quadrature points $\\{\\mathbf{x}^{b}_{j}\\}_{j=1}^{M}$ with positive weights $\\{w^{b}_{j}\\}_{j=1}^{M}$. Denote $u_{n,j} \\equiv \\mathbf{u}(\\mathbf{x}^{b}_{j},t_{k+1})\\cdot\\mathbf{n}(\\mathbf{x}^{b}_{j})$. Define the forward-Euler time discretization of the domain-total mass and a boundary flux evaluated at $t_{k+1}$.\n\nTasks:\n- Starting from the global conservation statement above, derive a discrete residual $g_{k+1}(\\theta)$ that measures violation of the global conservation law at time $t_{k+1}$ using the given quadrature rules and a forward-Euler approximation for $\\partial_{t}\\int_{\\Omega} c\\,d\\mathbf{x}$.\n- Construct a training objective $\\mathcal{J}(\\theta,\\lambda)$ that enforces the constraint $g_{k+1}(\\theta)=0$ as a hard constraint via a Lagrange multiplier $\\lambda$ and an augmented penalty with coefficient $\\rho>0$, in addition to a generic data-fidelity term $\\mathcal{L}_{\\text{data}}(\\theta)$ and an optional regularizer $\\mathcal{R}(\\theta)$.\n- Discuss a numerically stable strategy to enforce the hard constraint during training in a stochastic setting where the quadrature points may be subsampled, addressing how to update the Lagrange multiplier and the penalty parameter, and how to mitigate bias from subsampling.\n- To enable exact satisfaction of the conservation law at $t_{k+1}$ without altering $\\theta$ at that step, suppose you apply an additive, spatially uniform correction $\\beta_{k+1}$ to the surrogate prediction at $t_{k+1}$ only, so that the corrected field is $c(\\mathbf{x},t_{k+1}) = c_{\\theta}(\\mathbf{x},t_{k+1}) + \\beta_{k+1}$, while at the previous time $t_{k}$ there is no correction, i.e., $c(\\mathbf{x},t_{k}) = c_{\\theta}(\\mathbf{x},t_{k})$.\n  \n  Introduce the shorthands\n  $$\n  \\tilde{M}_{k} \\equiv \\sum_{i=1}^{N} w_{i}\\,c_{\\theta}(\\mathbf{x}_{i},t_{k}),\\quad\n  \\tilde{M}_{k+1} \\equiv \\sum_{i=1}^{N} w_{i}\\,c_{\\theta}(\\mathbf{x}_{i},t_{k+1}),\\quad\n  \\tilde{F}_{k+1} \\equiv \\sum_{j=1}^{M} w^{b}_{j}\\,c_{\\theta}(\\mathbf{x}^{b}_{j},t_{k+1})\\,u_{n,j},\\quad\n  Q_{k+1} \\equiv \\sum_{i=1}^{N} w_{i}\\,S(\\mathbf{x}_{i},t_{k+1}),\n  $$\n  and the aggregate quadrature factors\n  $$\n  W \\equiv \\sum_{i=1}^{N} w_{i},\\qquad B \\equiv \\sum_{j=1}^{M} w^{b}_{j}\\,u_{n,j}.\n  $$\n  Using these, write the corrected residual $g_{k+1}^{\\text{corr}}(\\beta_{k+1})$ and determine the unique value $\\beta_{k+1}$ that enforces $g_{k+1}^{\\text{corr}}(\\beta_{k+1}) = 0$ provided $W/\\Delta t + B \\neq 0$. Provide your final result as a single, simplified symbolic expression in terms of $\\tilde{M}_{k}$, $\\tilde{M}_{k+1}$, $\\tilde{F}_{k+1}$, $Q_{k+1}$, $W$, $B$, and $\\Delta t$. Do not evaluate numerically, and do not include units in your final expression.",
            "solution": "The problem statement is a valid exercise in the application of numerical methods and constrained optimization to hybrid physics–data modeling. It is scientifically sound, well-posed, and objective. We shall proceed by addressing each task in the order presented.\n\nThe foundation of the problem is the integral form of the conservation law for a passive tracer $c(\\mathbf{x}, t)$ in a domain $\\Omega$:\n$$\n\\partial_{t}\\int_{\\Omega} c\\,d\\mathbf{x} \\;+\\; \\int_{\\partial\\Omega} c\\,\\mathbf{u}\\cdot\\mathbf{n}\\,ds \\;=\\; \\int_{\\Omega} S\\,d\\mathbf{x}\n$$\nThis equation states that the rate of change of the total mass of the tracer in $\\Omega$, plus the net mass flux out of the boundary $\\partial\\Omega$, equals the total rate of mass production from the source $S$ within $\\Omega$.\n\nFirst, we derive the discrete residual $g_{k+1}(\\theta)$. We begin by discretizing the time derivative term using a forward-Euler approximation centered at time $t_{k+1}$:\n$$\n\\partial_{t}\\int_{\\Omega} c(\\mathbf{x}, t)\\,d\\mathbf{x} \\bigg|_{t=t_{k+1}} \\approx \\frac{1}{\\Delta t} \\left( \\int_{\\Omega} c(\\mathbf{x}, t_{k+1})\\,d\\mathbf{x} - \\int_{\\Omega} c(\\mathbf{x}, t_{k})\\,d\\mathbf{x} \\right)\n$$\nSubstituting this into the conservation law and evaluating all other terms at $t_{k+1}$, we obtain the semi-discrete equation:\n$$\n\\frac{1}{\\Delta t} \\left( \\int_{\\Omega} c(\\mathbf{x}, t_{k+1})\\,d\\mathbf{x} - \\int_{\\Omega} c(\\mathbf{x}, t_{k})\\,d\\mathbf{x} \\right) + \\int_{\\partial\\Omega} c(\\mathbf{x},t_{k+1})\\,\\mathbf{u}(\\mathbf{x},t_{k+1})\\cdot\\mathbf{n}\\,ds = \\int_{\\Omega} S(\\mathbf{x},t_{k+1})\\,d\\mathbf{x}\n$$\nNext, we introduce the parameterized surrogate $c_{\\theta}(\\mathbf{x}, t)$ and apply the specified quadrature rules to approximate the integrals. The domain integrals over $\\Omega$ use interior points $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ and weights $\\{w_{i}\\}_{i=1}^{N}$, and the boundary integral over $\\partial\\Omega$ uses boundary points $\\{\\mathbf{x}^{b}_{j}\\}_{j=1}^{M}$ and weights $\\{w^{b}_{j}\\}_{j=1}^{M}$. This yields the fully discrete equation:\n$$\n\\frac{1}{\\Delta t} \\left( \\sum_{i=1}^{N} w_{i}c_{\\theta}(\\mathbf{x}_{i}, t_{k+1}) - \\sum_{i=1}^{N} w_{i}c_{\\theta}(\\mathbf{x}_{i}, t_{k}) \\right) + \\sum_{j=1}^{M} w^{b}_{j}c_{\\theta}(\\mathbf{x}^{b}_{j}, t_{k+1})u_{n,j} = \\sum_{i=1}^{N} w_{i}S(\\mathbf{x}_{i}, t_{k+1})\n$$\nThe discrete residual, $g_{k+1}(\\theta)$, is defined as the amount by which this equation is violated. We obtain it by moving all terms to one side:\n$$\ng_{k+1}(\\theta) \\equiv \\frac{1}{\\Delta t} \\left( \\sum_{i=1}^{N} w_{i}c_{\\theta}(\\mathbf{x}_{i}, t_{k+1}) - \\sum_{i=1}^{N} w_{i}c_{\\theta}(\\mathbf{x}_{i}, t_{k}) \\right) + \\sum_{j=1}^{M} w^{b}_{j}c_{\\theta}(\\mathbf{x}^{b}_{j}, t_{k+1})u_{n,j} - \\sum_{i=1}^{N} w_{i}S(\\mathbf{x}_{i}, t_{k+1})\n$$\nThe goal is to enforce the constraint $g_{k+1}(\\theta)=0$.\n\nSecond, we construct the training objective $\\mathcal{J}(\\theta,\\lambda)$ using the augmented Lagrangian method. This method combines a Lagrange multiplier term to enforce the constraint and a quadratic penalty term to improve numerical stability. The objective function is the sum of the data-fidelity loss $\\mathcal{L}_{\\text{data}}(\\theta)$, an optional regularization term $\\mathcal{R}(\\theta)$, and the augmented Lagrangian terms for the constraint $g_{k+1}(\\theta)=0$. The complete objective is:\n$$\n\\mathcal{J}(\\theta,\\lambda) = \\mathcal{L}_{\\text{data}}(\\theta) + \\mathcal{R}(\\theta) + \\lambda \\, g_{k+1}(\\theta) + \\frac{\\rho}{2} [g_{k+1}(\\theta)]^{2}\n$$\nHere, $\\lambda$ is the Lagrange multiplier associated with the conservation constraint, and $\\rho > 0$ is the penalty parameter. Minimizing this objective with respect to $\\theta$ while updating $\\lambda$ and $\\rho$ appropriately drives the residual $g_{k+1}(\\theta)$ towards zero.\n\nThird, we discuss a numerically stable strategy for enforcing this hard constraint in a stochastic training setting. Subsampling the quadrature points $\\{\\mathbf{x}_i\\}$ and $\\{\\mathbf{x}_j^b\\}$ to form mini-batches for estimating the data loss $\\mathcal{L}_{\\text{data}}(\\theta)$ is standard practice. However, using these same mini-batches to estimate the global residual $g_{k+1}(\\theta)$ introduces significant challenges. A mini-batch estimate of the residual will be a noisy, biased estimator of the true global residual. Enforcing the mini-batch residual to be zero does not guarantee that the global conservation law is satisfied. To mitigate this, a robust strategy is to decouple the evaluation of the data loss and the constraint residual. The data loss $\\mathcal{L}_{\\text{data}}(\\theta)$ and its gradients can be estimated stochastically on mini-batches. However, the conservation residual $g_{k+1}(\\theta)$, which is an inexpensive global sum, should be computed using the full set of $N$ and $M$ quadrature points in every training step or at a lower frequency. This ensures that the constraint being enforced is the true, unbiased global conservation law.\nThe optimization proceeds via the method of multipliers. After a certain number of optimization steps for $\\theta$ using the objective $\\mathcal{J}(\\theta, \\lambda_m, \\rho_m)$, the Lagrange multiplier $\\lambda$ and penalty parameter $\\rho$ are updated. Standard update rules are:\n1. Update of the Lagrange multiplier: $\\lambda_{m+1} \\leftarrow \\lambda_m + \\rho_m \\, g_{k+1}(\\theta)$, where $\\theta$ are the parameters after the inner optimization steps. This is a dual ascent step on the Lagrangian.\n2. Update of the penalty parameter: $\\rho$ is adaptively increased to enforce the constraint more strongly if convergence is slow. For example, if the constraint violation $|g_{k+1}(\\theta)|$ does not decrease by a target factor, increase $\\rho$: $\\rho_{m+1} \\leftarrow \\alpha \\rho_m$ for some factor $\\alpha > 1$. Otherwise, $\\rho_{m+1} \\leftarrow \\rho_m$. This improves the conditioning of the problem and accelerates convergence to a feasible solution.\n\nFourth, we derive the exact, additive, spatially uniform correction $\\beta_{k+1}$ that enforces the discrete conservation law at time $t_{k+1}$. The corrected tracer field at $t_{k+1}$ is $c(\\mathbf{x}, t_{k+1}) = c_{\\theta}(\\mathbf{x}, t_{k+1}) + \\beta_{k+1}$, while the field at $t_k$ remains $c(\\mathbf{x}, t_{k}) = c_{\\theta}(\\mathbf{x}, t_k)$. We must find the $\\beta_{k+1}$ such that the corrected residual, $g_{k+1}^{\\text{corr}}(\\beta_{k+1})$, is exactly zero.\n\nThe uncorrected residual can be written using the provided shorthands:\n$$\ng_{k+1}(\\theta) = \\frac{\\tilde{M}_{k+1} - \\tilde{M}_{k}}{\\Delta t} + \\tilde{F}_{k+1} - Q_{k+1}\n$$\nNow, we compute the terms for the corrected residual.\nThe corrected total mass at $t_{k+1}$ is:\n$$\n\\sum_{i=1}^{N} w_{i} (c_{\\theta}(\\mathbf{x}_{i},t_{k+1}) + \\beta_{k+1}) = \\left(\\sum_{i=1}^{N} w_{i} c_{\\theta}(\\mathbf{x}_{i},t_{k+1})\\right) + \\beta_{k+1} \\left(\\sum_{i=1}^{N} w_{i}\\right) = \\tilde{M}_{k+1} + \\beta_{k+1}W\n$$\nThe total mass at $t_k$ is unchanged: $\\tilde{M}_{k}$.\nThe corrected total flux at $t_{k+1}$ is:\n$$\n\\sum_{j=1}^{M} w^{b}_{j} (c_{\\theta}(\\mathbf{x}^{b}_{j},t_{k+1})+\\beta_{k+1}) u_{n,j} = \\left(\\sum_{j=1}^{M} w^{b}_{j}c_{\\theta}(\\mathbf{x}^{b}_{j},t_{k+1})u_{n,j}\\right) + \\beta_{k+1}\\left(\\sum_{j=1}^{M} w^{b}_{j}u_{n,j}\\right) = \\tilde{F}_{k+1} + \\beta_{k+1}B\n$$\nThe total source term $Q_{k+1}$ is independent of $c$ and thus remains unchanged.\nThe corrected residual is therefore:\n$$\ng_{k+1}^{\\text{corr}}(\\beta_{k+1}) = \\frac{(\\tilde{M}_{k+1} + \\beta_{k+1}W) - \\tilde{M}_{k}}{\\Delta t} + (\\tilde{F}_{k+1} + \\beta_{k+1}B) - Q_{k+1}\n$$\nWe enforce $g_{k+1}^{\\text{corr}}(\\beta_{k+1}) = 0$ and solve for $\\beta_{k+1}$:\n$$\n\\frac{\\tilde{M}_{k+1} - \\tilde{M}_{k}}{\\Delta t} + \\frac{\\beta_{k+1}W}{\\Delta t} + \\tilde{F}_{k+1} + \\beta_{k+1}B - Q_{k+1} = 0\n$$\nGrouping terms containing $\\beta_{k+1}$:\n$$\n\\beta_{k+1} \\left( \\frac{W}{\\Delta t} + B \\right) = - \\left( \\frac{\\tilde{M}_{k+1} - \\tilde{M}_{k}}{\\Delta t} + \\tilde{F}_{k+1} - Q_{k+1} \\right)\n$$\nThe expression in the parenthesis on the right-hand side is the uncorrected residual $g_{k+1}(\\theta)$.\n$$\n\\beta_{k+1} \\left( \\frac{W + \\Delta t B}{\\Delta t} \\right) = - \\left( \\frac{(\\tilde{M}_{k+1} - \\tilde{M}_{k}) + \\Delta t \\tilde{F}_{k+1} - \\Delta t Q_{k+1}}{\\Delta t} \\right)\n$$\nAssuming $\\Delta t \\neq 0$, we can simplify:\n$$\n\\beta_{k+1} (W + \\Delta t B) = - ((\\tilde{M}_{k+1} - \\tilde{M}_{k}) + \\Delta t \\tilde{F}_{k+1} - \\Delta t Q_{k+1})\n$$\nProvided that the denominator is non-zero, as stated in the condition $W/\\Delta t + B \\neq 0$ which implies $W + \\Delta t B \\neq 0$, we can isolate $\\beta_{k+1}$:\n$$\n\\beta_{k+1} = - \\frac{(\\tilde{M}_{k+1} - \\tilde{M}_{k}) + \\Delta t \\tilde{F}_{k+1} - \\Delta t Q_{k+1}}{W + \\Delta t B}\n$$\nRearranging the numerator for the final expression:\n$$\n\\beta_{k+1} = \\frac{\\tilde{M}_{k} - \\tilde{M}_{k+1} - \\Delta t \\tilde{F}_{k+1} + \\Delta t Q_{k+1}}{W + \\Delta t B}\n$$\nThis expression provides the unique, spatially uniform correction that must be added to the surrogate prediction at time $t_{k+1}$ to exactly satisfy the discrete global conservation law.",
            "answer": "$$\n\\boxed{\\frac{\\tilde{M}_{k} - \\tilde{M}_{k+1} - \\Delta t \\tilde{F}_{k+1} + \\Delta t Q_{k+1}}{W + \\Delta t B}}\n$$"
        },
        {
            "introduction": "Physical quantities like concentration or density must always be non-negative, a constraint that standard data-driven models do not inherently respect. This exercise introduces an architectural solution: using a link function to guarantee positivity by design . Beyond simply implementing this technique, you will analyze how such a reparameterization reshapes the loss landscape and affects the stability of the training process, a critical consideration for any practitioner.",
            "id": "3884463",
            "problem": "Consider a passive tracer concentration field $c(x,t)$ evolving in a one-dimensional periodic channel $x \\in [0,2\\pi]$ under Fickian diffusion with constant diffusivity $\\kappa>0$ and no sources or sinks. The governing mass-conservation law and Fick’s law combine to yield the partial differential equation (PDE) $\\partial_t c - \\kappa \\,\\partial_{xx} c = 0$. To enforce positivity in a hybrid physics–data parameterization, use the link function $c(x,t) = \\exp\\!\\big(g_{\\phi}(x,t)\\big)$, where $g_{\\phi}$ is a data-driven model and $\\phi$ denotes the trainable parameters.\n\nFor this problem, restrict $g_{\\phi}$ to a single scalar parameter $a \\in \\mathbb{R}$ multiplying a fixed basis function $\\psi(x,t)$, namely $g_{\\phi}(x,t) = a\\,\\psi(x,t)$ with $\\psi(x,t) = \\sin(kx)\\cos(\\omega t)$, where $k \\in \\mathbb{N}$ and $\\omega>0$ are fixed. Define the physics-informed training objective as the space–time integral of the squared PDE residual over one spatio-temporal period,\n$$\nL(a) = \\frac{1}{2}\\int_{0}^{2\\pi}\\!\\int_{0}^{\\frac{2\\pi}{\\omega}}\\!\\left(\\partial_t c(x,t) - \\kappa\\,\\partial_{xx} c(x,t)\\right)^{2}\\, dt\\, dx,\n$$\nwith $c(x,t) = \\exp\\!\\big(a\\,\\psi(x,t)\\big)$.\n\nTasks:\n- Starting from the PDE and the definition of $L(a)$, derive the exact expression for the gradient $\\frac{dL}{da}$ in terms of $\\psi$, its derivatives, and $a$, using the chain rule.\n- Linearize the gradient flow near $a=0$ and show explicitly how the positivity link $c=\\exp(g_{\\phi})$ scales the gradient. Compute the Hessian $\\frac{d^{2}L}{da^{2}}\\big|_{a=0}$ as a closed-form integral in terms of $\\kappa$, $k$, and $\\omega$.\n- Considering gradient descent on $a$ with a constant learning rate $\\eta$ and updates $a_{n+1} = a_n - \\eta\\,\\frac{dL}{da}(a_n)$, use the linearized dynamics near $a=0$ to determine the largest learning rate $\\eta_{\\max}$ that ensures linear stability of the training iterates. Provide your final answer as a single analytic expression for $\\eta_{\\max}$ in terms of $\\kappa$, $k$, and $\\omega$. No numerical substitution or rounding is required, and no units are needed for the final answer.",
            "solution": "The problem is first validated against the given criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Tracer concentration field: $c(x,t)$.\n-   Domain: $x \\in [0, 2\\pi]$, 1D periodic channel.\n-   Governing PDE: $\\partial_t c - \\kappa \\partial_{xx} c = 0$, with constant diffusivity $\\kappa>0$.\n-   Positivity-enforcing link function: $c(x,t) = \\exp(g_{\\phi}(x,t))$.\n-   Parameterized model: $g_{\\phi}(x,t) = a\\,\\psi(x,t)$, where $a \\in \\mathbb{R}$ is the trainable parameter.\n-   Fixed basis function: $\\psi(x,t) = \\sin(kx)\\cos(\\omega t)$, with $k \\in \\mathbb{N}$ and $\\omega>0$.\n-   Physics-informed loss function: $L(a) = \\frac{1}{2}\\int_{0}^{2\\pi}\\int_{0}^{\\frac{2\\pi}{\\omega}}\\!\\left(\\partial_t c(x,t) - \\kappa\\,\\partial_{xx} c(x,t)\\right)^{2}\\, dt\\, dx$.\n-   Gradient descent update rule: $a_{n+1} = a_n - \\eta\\,\\frac{dL}{da}(a_n)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is firmly based on the fundamental principles of diffusion (Fick's laws, mass conservation) and on established methods in scientific machine learning (physics-informed learning). The use of a loss function based on the PDE residual is a standard technique. The exponential link function is a common method for enforcing positivity constraints.\n-   **Well-Posed**: The problem is mathematically well-defined. It provides all necessary functions, parameters, and integration domains to perform the requested derivations and calculations. The tasks are specific and lead to a unique analytical solution.\n-   **Objective**: The problem is stated using precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is found to be valid as it is scientifically sound, well-posed, objective, and complete. We may proceed with the solution.\n\n### Derivation\n\n**Task 1: Derive the gradient $\\frac{dL}{da}$**\n\nThe loss function is given by\n$$L(a) = \\frac{1}{2}\\int_{0}^{2\\pi}\\int_{0}^{2\\pi/\\omega} R(a, x, t)^2 \\, dt \\, dx$$\nwhere the residual $R(a, x, t)$ is defined as\n$$R = \\partial_t c - \\kappa \\partial_{xx} c$$\nwith $c(x,t) = \\exp(a\\psi(x,t))$.\n\nTo find the gradient $\\frac{dL}{da}$, we differentiate $L(a)$ with respect to $a$. Using the chain rule and Leibniz's rule for differentiating under the integral sign, we get:\n$$\\frac{dL}{da} = \\frac{d}{da} \\left( \\frac{1}{2}\\int\\int R^2 \\,dt\\,dx \\right) = \\int\\int R \\frac{\\partial R}{\\partial a} \\,dt\\,dx$$\nLet us define the linear differential operator $\\mathcal{L} = \\partial_t - \\kappa \\partial_{xx}$. The residual is $R = \\mathcal{L}c$.\nThe partial derivative of the residual with respect to $a$ is:\n$$\\frac{\\partial R}{\\partial a} = \\frac{\\partial}{\\partial a} (\\mathcal{L}c) = \\mathcal{L}\\left(\\frac{\\partial c}{\\partial a}\\right)$$\nWe compute $\\frac{\\partial c}{\\partial a}$:\n$$\\frac{\\partial c}{\\partial a} = \\frac{\\partial}{\\partial a} \\exp(a\\psi) = \\psi \\exp(a\\psi) = \\psi c$$\nSubstituting this into the expression for $\\frac{\\partial R}{\\partial a}$:\n$$\\frac{\\partial R}{\\partial a} = \\mathcal{L}(\\psi c) = (\\partial_t - \\kappa \\partial_{xx})(\\psi c)$$\nWe expand the terms:\n$$\\partial_t(\\psi c) = (\\partial_t \\psi)c + \\psi(\\partial_t c)$$\n$$\\partial_x(\\psi c) = (\\partial_x \\psi)c + \\psi(\\partial_x c)$$\n$$\\partial_{xx}(\\psi c) = \\partial_x((\\partial_x \\psi)c + \\psi(\\partial_x c)) = (\\partial_{xx}\\psi)c + (\\partial_x \\psi)(\\partial_x c) + (\\partial_x \\psi)(\\partial_x c) + \\psi(\\partial_{xx}c)$$\n$$\\partial_{xx}(\\psi c) = (\\partial_{xx}\\psi)c + 2(\\partial_x \\psi)(\\partial_x c) + \\psi(\\partial_{xx}c)$$\nCombining these gives:\n$$\\mathcal{L}(\\psi c) = [(\\partial_t \\psi)c + \\psi(\\partial_t c)] - \\kappa[(\\partial_{xx}\\psi)c + 2(\\partial_x \\psi)(\\partial_x c) + \\psi(\\partial_{xx}c)]$$\nRearranging terms to group by $\\psi$ and $c$:\n$$\\mathcal{L}(\\psi c) = \\psi(\\partial_t c - \\kappa \\partial_{xx}c) + c(\\partial_t \\psi - \\kappa \\partial_{xx}\\psi) - 2\\kappa(\\partial_x c)(\\partial_x \\psi)$$\nRecognizing $R = \\partial_t c - \\kappa \\partial_{xx}c$ and $\\mathcal{L}\\psi = \\partial_t\\psi - \\kappa\\partial_{xx}\\psi$, we have:\n$$\\frac{\\partial R}{\\partial a} = \\psi R + c(\\mathcal{L}\\psi) - 2\\kappa(\\partial_x c)(\\partial_x \\psi)$$\nFinally, the expression for the gradient $\\frac{dL}{da}$ is:\n$$\\frac{dL}{da} = \\int\\int R \\left( \\psi R + c(\\mathcal{L}\\psi) - 2\\kappa(\\partial_x c)(\\partial_x \\psi) \\right) \\,dt\\,dx$$\nwhere $c = \\exp(a\\psi)$, $R = \\mathcal{L}c$, and $\\partial_x c = a(\\partial_x \\psi)c$. This is the exact expression for the gradient.\n\n**Task 2: Linearize the gradient flow and compute the Hessian at $a=0$**\n\nThe gradient flow is given by $\\frac{da}{d\\tau} = -\\frac{dL}{da}(a)$, for a continuous \"training time\" $\\tau$. To linearize this flow near $a=0$, we need the first-order Taylor expansion of $\\frac{dL}{da}(a)$ around $a=0$:\n$$\\frac{dL}{da}(a) \\approx \\frac{dL}{da}\\bigg|_{a=0} + a \\left(\\frac{d^2L}{da^2}\\bigg|_{a=0}\\right)$$\nFirst, we evaluate the gradient at $a=0$. At $a=0$, we have $c(x,t) = \\exp(0) = 1$. The residual is:\n$$R(a=0) = \\partial_t(1) - \\kappa\\partial_{xx}(1) = 0 - 0 = 0$$\nSince $R(a=0) = 0$, the gradient is also zero:\n$$\\frac{dL}{da}\\bigg|_{a=0} = \\int\\int R(0) \\frac{\\partial R}{\\partial a}\\bigg|_{a=0} \\,dt\\,dx = 0$$\nThis confirms that $a=0$ is a critical point of the loss function. The linearized gradient is thus:\n$$\\frac{dL}{da}(a) \\approx a \\left(\\frac{d^2L}{da^2}\\bigg|_{a=0}\\right)$$\nThe term $\\frac{d^2L}{da^2}|_{a=0}$ is the Hessian $H$ of the loss function at $a=0$.\nThe use of the link function $c=\\exp(a\\psi)$ results in a non-quadratic loss landscape. For small $a$, however, $c \\approx 1+a\\psi$. The residual becomes $R=\\mathcal{L}c \\approx \\mathcal{L}(1+a\\psi) = a\\mathcal{L}\\psi$. The loss is $L(a) \\approx \\frac{1}{2}\\int\\int (a\\mathcal{L}\\psi)^2 dtdx$, making the gradient $\\frac{dL}{da} \\approx a \\int\\int (\\mathcal{L}\\psi)^2 dtdx$. The non-linear link function's effect is that this linear gradient scaling is an approximation, whereas for a simple linear model $c=a\\psi$, it would be exact.\n\nTo compute the Hessian $H$, we differentiate the gradient expression:\n$$\\frac{d^2L}{da^2} = \\frac{d}{da} \\int\\int R \\frac{\\partial R}{\\partial a} \\,dt\\,dx = \\int\\int \\left( \\left(\\frac{\\partial R}{\\partial a}\\right)^2 + R \\frac{\\partial^2 R}{\\partial a^2} \\right) \\,dt\\,dx$$\nEvaluating at $a=0$, since $R(0)=0$, the second term vanishes:\n$$H = \\frac{d^2L}{da^2}\\bigg|_{a=0} = \\int\\int \\left(\\frac{\\partial R}{\\partial a}\\bigg|_{a=0}\\right)^2 \\,dt\\,dx$$\nWe need $\\frac{\\partial R}{\\partial a}$ at $a=0$. We found $\\frac{\\partial R}{\\partial a} = \\mathcal{L}(\\frac{\\partial c}{\\partial a})$.\nAt $a=0$:\n$$\\frac{\\partial c}{\\partial a}\\bigg|_{a=0} = (\\psi\\exp(a\\psi))\\big|_{a=0} = \\psi \\exp(0) = \\psi$$\nTherefore,\n$$\\frac{\\partial R}{\\partial a}\\bigg|_{a=0} = \\mathcal{L}(\\psi) = \\partial_t\\psi - \\kappa \\partial_{xx}\\psi$$\nThe Hessian is the integral of the square of this term:\n$$H = \\int_{0}^{2\\pi}\\int_{0}^{\\frac{2\\pi}{\\omega}} (\\partial_t\\psi - \\kappa \\partial_{xx}\\psi)^2 \\,dt\\,dx$$\nNow, we substitute $\\psi(x,t) = \\sin(kx)\\cos(\\omega t)$:\n$$\\partial_t \\psi = -\\omega \\sin(kx)\\sin(\\omega t)$$\n$$\\partial_x \\psi = k \\cos(kx)\\cos(\\omega t) \\quad \\implies \\quad \\partial_{xx} \\psi = -k^2 \\sin(kx)\\cos(\\omega t)$$\nSo,\n$$\\mathcal{L}\\psi = -\\omega \\sin(kx)\\sin(\\omega t) - \\kappa(-k^2 \\sin(kx)\\cos(\\omega t)) = \\sin(kx) \\left( \\kappa k^2 \\cos(\\omega t) - \\omega \\sin(\\omega t) \\right)$$\nThe integrand is:\n$$(\\mathcal{L}\\psi)^2 = \\sin^2(kx) \\left( \\kappa k^2 \\cos(\\omega t) - \\omega \\sin(\\omega t) \\right)^2$$\nThe integral for $H$ is separable:\n$$H = \\left(\\int_{0}^{2\\pi} \\sin^2(kx) \\,dx\\right) \\left(\\int_{0}^{\\frac{2\\pi}{\\omega}} \\left( \\kappa k^2 \\cos(\\omega t) - \\omega \\sin(\\omega t) \\right)^2 \\,dt\\right)$$\nLet's evaluate the integrals. Since $k \\in \\mathbb{N}$:\n$$\\int_{0}^{2\\pi} \\sin^2(kx) \\,dx = \\int_{0}^{2\\pi} \\frac{1 - \\cos(2kx)}{2} \\,dx = \\left[\\frac{x}{2} - \\frac{\\sin(2kx)}{4k}\\right]_{0}^{2\\pi} = \\pi$$\nFor the time integral, we expand the square:\n$$(\\dots)^2 = \\kappa^2 k^4 \\cos^2(\\omega t) - 2\\kappa k^2 \\omega \\cos(\\omega t)\\sin(\\omega t) + \\omega^2 \\sin^2(\\omega t)$$\nIntegrating over one period $T = \\frac{2\\pi}{\\omega}$:\n$\\int_0^T \\cos^2(\\omega t) \\,dt = \\frac{T}{2} = \\frac{\\pi}{\\omega}$\n$\\int_0^T \\sin^2(\\omega t) \\,dt = \\frac{T}{2} = \\frac{\\pi}{\\omega}$\n$\\int_0^T \\cos(\\omega t)\\sin(\\omega t) \\,dt = \\frac{1}{2}\\int_0^T \\sin(2\\omega t) \\,dt = 0$\nSo, the time integral evaluates to:\n$$\\kappa^2 k^4 \\left(\\frac{\\pi}{\\omega}\\right) - 0 + \\omega^2 \\left(\\frac{\\pi}{\\omega}\\right) = \\frac{\\pi}{\\omega}(\\kappa^2 k^4 + \\omega^2)$$\nMultiplying the space and time integrals gives the Hessian:\n$$H = \\pi \\cdot \\frac{\\pi}{\\omega}(\\kappa^2 k^4 + \\omega^2) = \\frac{\\pi^2}{\\omega}(\\kappa^2 k^4 + \\omega^2)$$\n\n**Task 3: Determine the largest learning rate $\\eta_{\\max}$**\n\nThe gradient descent update is $a_{n+1} = a_n - \\eta \\frac{dL}{da}(a_n)$. Near the minimum $a=0$, we use the linearized gradient $\\frac{dL}{da}(a_n) \\approx H a_n$. The linearized update rule becomes:\n$$a_{n+1} \\approx a_n - \\eta H a_n = (1 - \\eta H) a_n$$\nThis is a linear iterative map. For the iterates $a_n$ to converge to $0$, the amplification factor must have a magnitude less than $1$:\n$$|1 - \\eta H| < 1$$\nThis inequality is equivalent to $-1 < 1 - \\eta H < 1$.\nThe right side, $1 - \\eta H < 1$, implies $-\\eta H < 0$. Since the learning rate $\\eta > 0$ and the Hessian $H = \\frac{\\pi^2}{\\omega}(\\kappa^2 k^4 + \\omega^2)$ is strictly positive (given $\\kappa>0, \\omega>0, k\\in\\mathbb{N}$), this condition is always satisfied.\nThe left side, $-1 < 1 - \\eta H$, implies $\\eta H < 2$.\nThis gives the stability condition on the learning rate:\n$$\\eta < \\frac{2}{H}$$\nThe largest learning rate $\\eta_{\\max}$ that guarantees stability is therefore the upper bound of this interval.\n$$\\eta_{\\max} = \\frac{2}{H}$$\nSubstituting the expression for $H$:\n$$\\eta_{\\max} = \\frac{2}{\\frac{\\pi^2}{\\omega}(\\kappa^2 k^4 + \\omega^2)}$$\nSimplifying the expression gives the final answer:\n$$\\eta_{\\max} = \\frac{2\\omega}{\\pi^2(\\kappa^2 k^4 + \\omega^2)}$$",
            "answer": "$$\\boxed{\\frac{2\\omega}{\\pi^{2}(\\kappa^{2}k^{4} + \\omega^{2})}}$$"
        }
    ]
}