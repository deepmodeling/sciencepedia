{
    "hands_on_practices": [
        {
            "introduction": "The heart of a Physics-Informed Neural Network (PINN) is its loss function, which translates a differential equation into a language a neural network can understand: optimization. This exercise walks you through the fundamental process of constructing a composite loss function from its constituent parts. By defining terms for the PDE residual, initial conditions, and boundary conditions, you will see how PINNs are trained to find a solution that respects both the governing physical laws and the specific constraints of the problem .",
            "id": "2126319",
            "problem": "A team of computational scientists is developing a Physics-Informed Neural Network (PINN) to solve a partial differential equation. A PINN is a neural network whose loss function includes a term that penalizes deviations from the governing physical laws.\n\nThe specific problem is to model the one-dimensional advection equation, which describes the transport of a quantity. The governing equation is:\n$$\n\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} = 0\n$$\nwhere $u(x, t)$ is the quantity of interest, and $c$ is a constant positive wave speed. The solution is sought on the spatio-temporal domain defined by $x \\in [X_0, X_1]$ and $t \\in [0, T]$.\n\nThe neural network, denoted by $\\hat{u}(x, t; \\theta)$, approximates the true solution $u(x, t)$. Here, $\\theta$ represents all the trainable parameters (weights and biases) of the network. The goal is to find the optimal parameters $\\theta$ by minimizing a total loss function, $\\mathcal{L}(\\theta)$.\n\nThe total loss function is a weighted sum of three components:\n1.  **PDE Loss ($\\mathcal{L}_{PDE}$)**: Enforces the advection equation inside the domain.\n2.  **Initial Condition Loss ($\\mathcal{L}_{IC}$)**: Enforces the state of the system at $t=0$.\n3.  **Boundary Condition Loss ($\\mathcal{L}_{BC}$)**: Enforces the behavior at the spatial boundaries $x=X_0$ and $x=X_1$.\n\nThe specific conditions are:\n-   **Initial Condition (IC)**: At $t=0$, the profile is a Gaussian pulse, given by $u(x, 0) = f(x) = A \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$.\n-   **Boundary Condition (BC)**: The system has periodic boundaries, meaning $u(X_0, t) = u(X_1, t)$ for all $t \\in [0, T]$.\n\nTo compute the loss, the domain is sampled at a finite number of points:\n-   A set of $N_r$ \"residual\" or \"collocation\" points $\\{ (x_i^r, t_i^r) \\}_{i=1}^{N_r}$ are sampled from the interior of the domain, $(X_0, X_1) \\times (0, T]$.\n-   A set of $N_{ic}$ initial points $\\{ (x_i^{ic}, 0) \\}_{i=1}^{N_{ic}}$ are sampled along the initial time line.\n-   A set of $N_{bc}$ boundary time-points $\\{ t_i^{bc} \\}_{i=1}^{N_{bc}}$ are sampled along the temporal domain, for which the boundary condition is evaluated at $x=X_0$ and $x=X_1$.\n\nThe loss for each component is defined as the mean squared error. The total loss function is given by the weighted sum:\n$$\n\\mathcal{L}(\\theta) = w_{PDE} \\mathcal{L}_{PDE} + w_{IC} \\mathcal{L}_{IC} + w_{BC} \\mathcal{L}_{BC}\n$$\nwhere $w_{PDE}$, $w_{IC}$, and $w_{BC}$ are positive constant weights.\n\nYour task is to write down the complete mathematical expression for the total loss function $\\mathcal{L}(\\theta)$. Your expression should be in terms of the neural network approximation $\\hat{u}$ and its partial derivatives, the given parameters and functions ($c, A, \\mu, \\sigma, X_0, X_1$), the sample points, and the weights.",
            "solution": "The goal is to construct the total loss function $\\mathcal{L}(\\theta)$ by defining its three components: the PDE loss $\\mathcal{L}_{PDE}$, the initial condition loss $\\mathcal{L}_{IC}$, and the boundary condition loss $\\mathcal{L}_{BC}$.\n\nFirst, we define the PDE loss, $\\mathcal{L}_{PDE}$. This term measures how well the neural network's output $\\hat{u}(x, t; \\theta)$ satisfies the governing advection equation. We start by defining the PDE residual, $R(x, t; \\theta)$, which is what the equation equals when the approximate solution $\\hat{u}$ is substituted into it:\n$$\nR(x, t; \\theta) = \\frac{\\partial \\hat{u}}{\\partial t}(x, t; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x, t; \\theta)\n$$\nThe PDE loss is the mean squared error of this residual, evaluated over the $N_r$ collocation points $\\{ (x_i^r, t_i^r) \\}_{i=1}^{N_r}$.\n$$\n\\mathcal{L}_{PDE} = \\frac{1}{N_r} \\sum_{i=1}^{N_r} \\left( R(x_i^r, t_i^r; \\theta) \\right)^2 = \\frac{1}{N_r} \\sum_{i=1}^{N_r} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(x_i^r, t_i^r; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x_i^r, t_i^r; \\theta) \\right)^2\n$$\n\nSecond, we define the initial condition loss, $\\mathcal{L}_{IC}$. This term measures the discrepancy between the network's prediction at time $t=0$ and the true initial condition, $u(x, 0) = f(x)$. The initial condition is given as $f(x) = A \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. The loss is the mean squared error between $\\hat{u}(x_i^{ic}, 0; \\theta)$ and $f(x_i^{ic})$ over the $N_{ic}$ initial points.\n$$\n\\mathcal{L}_{IC} = \\frac{1}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - f(x_i^{ic}) \\right)^2 = \\frac{1}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - A \\exp\\left(-\\frac{(x_i^{ic} - \\mu)^2}{2\\sigma^2}\\right) \\right)^2\n$$\n\nThird, we define the boundary condition loss, $\\mathcal{L}_{BC}$. This term enforces the periodic boundary condition, $u(X_0, t) = u(X_1, t)$. The loss is the mean squared error of the difference between the network's predictions at the two boundaries, evaluated at the $N_{bc}$ sample time points $\\{ t_i^{bc} \\}_{i=1}^{N_{bc}}$.\n$$\n\\mathcal{L}_{BC} = \\frac{1}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( \\hat{u}(X_0, t_i^{bc}; \\theta) - \\hat{u}(X_1, t_i^{bc}; \\theta) \\right)^2\n$$\n\nFinally, we construct the total loss function $\\mathcal{L}(\\theta)$ by taking the weighted sum of these three components:\n$$\n\\mathcal{L}(\\theta) = w_{PDE} \\mathcal{L}_{PDE} + w_{IC} \\mathcal{L}_{IC} + w_{BC} \\mathcal{L}_{BC}\n$$\nSubstituting the expressions for each component gives the final expression for the total loss function:\n$$\n\\mathcal{L}(\\theta) = \\frac{w_{PDE}}{N_r} \\sum_{i=1}^{N_r} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(x_i^r, t_i^r; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x_i^r, t_i^r; \\theta) \\right)^2 + \\frac{w_{IC}}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - A \\exp\\left(-\\frac{(x_i^{ic} - \\mu)^2}{2\\sigma^2}\\right) \\right)^2 + \\frac{w_{BC}}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( \\hat{u}(X_0, t_i^{bc}; \\theta) - \\hat{u}(X_1, t_i^{bc}; \\theta) \\right)^2\n$$",
            "answer": "$$\\boxed{\\mathcal{L}(\\theta) = \\frac{w_{PDE}}{N_r} \\sum_{i=1}^{N_r} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(x_i^r, t_i^r; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x_i^r, t_i^r; \\theta) \\right)^2 + \\frac{w_{IC}}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - A \\exp\\left(-\\frac{(x_i^{ic} - \\mu)^2}{2\\sigma^2}\\right) \\right)^2 + \\frac{w_{BC}}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( \\hat{u}(X_0, t_i^{bc}; \\theta) - \\hat{u}(X_1, t_i^{bc}; \\theta) \\right)^2}$$"
        },
        {
            "introduction": "While adding boundary conditions to the loss function is a common practice, it can sometimes be challenging to balance this term with the PDE residual. A more robust alternative is to enforce these conditions by construction, directly within the network's architecture . This practice challenges you to devise a transformation that guarantees the network's output will exactly match the required Dirichlet boundary values, an elegant technique that can improve training stability and accuracy.",
            "id": "2126300",
            "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.",
            "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand a simple vanishing factor such as\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, one possible solution is:\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, we evaluate the expression at $x=0$ and $x=L$:\nAt $x=0$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot (L-0)\\,\\hat{u}_{NN}(0)=A\n$$\nAt $x=L$:\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        },
        {
            "introduction": "One of the most critical and non-intuitive challenges in training PINNs is the phenomenon of spectral bias, where the network preferentially learns low-frequency components of a solution before high-frequency ones. This advanced practice provides a complete, hands-on coding exercise to demonstrate this effect empirically . By setting up and training a PINN to solve an ODE with a known multi-frequency solution, you will quantitatively observe how the model captures the simple, slow-varying patterns first, providing a crucial insight into the training dynamics of PINNs.",
            "id": "2427229",
            "problem": "You will implement a complete, runnable program that demonstrates the spectral bias of a Physics-Informed Neural Network (PINN). The central idea is to train a PINN to solve a one-dimensional boundary value problem whose known solution is the superposition of a low-frequency and a high-frequency sine, namely $u(x) = \\sin(x) + \\sin(25x)$, and to quantitatively observe which frequency component is learned first during training. Angles must be in radians throughout.\n\nStart from the following physically consistent ordinary differential equation (ODE) with periodic boundary conditions:\nGiven the domain $x \\in [0, 2\\pi]$, consider\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\nwith periodic boundary conditions\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\nIt is a well-tested fact that if $u(x) = \\sin(x) + \\sin(25x)$ then $u''(x) + u(x) = -624 \\sin(25x)$ and the periodic boundary conditions hold. You must not use any labeled training data for $u(x)$ except the boundary conditions; instead, use the ODE residual and boundary residuals in the loss, as is standard for a Physics-Informed Neural Network (PINN).\n\nConstruct a single-hidden-layer neural network $u_{\\theta}(x)$ with $H$ hidden units and hyperbolic tangent activation as the trial solution. Define the hidden pre-activations as $z_i(x) = w_i x + b_i$ for $i \\in \\{1,\\dots,H\\}$, the hidden activations as $h_i(x) = \\tanh(z_i(x))$, and the output as\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\nUse the chain rule and the product rule to compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$ in closed form. Recall the standard identities for the hyperbolic tangent and its derivatives:\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\nDefine the pointwise physics residual for collocation points $\\{x_n\\}_{n=1}^{N}$ as\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\nand the periodic boundary residuals as\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\nUse the mean-squared residual loss with a boundary weight $\\lambda_{\\text{bc}}$:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\nTrain the parameters $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$ by gradient-based optimization from random initialization. To quantitatively assess spectral bias, at the end of a short training budget, project the learned function $u_{\\theta}(x)$ onto the two basis functions $\\sin(x)$ and $\\sin(25x)$ over a dense uniform grid on $[0, 2\\pi)$ by least squares. That is, find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ minimizing\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\nwith $x_m$ uniformly spaced in $[0, 2\\pi)$. Define the learned amplitudes as $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. Spectral bias is deemed present at early training if $A_1 > A_{25}$.\n\nImplement the program with a fully vectorized training loop and closed-form gradients with respect to all network parameters using only the ODE residual and boundary residuals. Do not use any external automatic differentiation library.\n\nTest Suite and Output Specification:\n- Use the following three test cases to exercise different regimes. Each case specifies $(H, N, K, \\eta)$ where $H$ is the number of hidden units, $N$ is the number of collocation points, $K$ is the number of gradient steps, and $\\eta$ is the learning rate. Use $\\lambda_{\\text{bc}} = 1$ in all cases. Angles are in radians.\n  1. Case $1$: $(H, N, K, \\eta) = (20, 128, 60, 0.01)$.\n  2. Case $2$: $(H, N, K, \\eta) = (10, 64, 80, 0.01)$.\n  3. Case $3$: $(H, N, K, \\eta) = (5, 128, 120, 0.01)$.\n- For each case, initialize parameters with a fixed seed so that results are deterministic. After training for $K$ steps, compute $A_1$ and $A_{25}$ by least squares projection over a dense grid of $M$ points with $M = 4096$. Record a boolean result for the case defined as\n$$\n\\text{result} = \\begin{cases}\n\\text{True}, & \\text{if } A_1 > A_{25},\\\\\n\\text{False}, & \\text{otherwise.}\n\\end{cases}\n$$\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,True,False]\").\n\nYour program must be self-contained, receive no input, and run as-is. Angles must be in radians. All numerical answers are dimensionless, and the final outputs are booleans. The training and projections must be implemented in pure linear algebra using the formulas above, without any external machine learning framework. The goal is to demonstrate, via these test cases, that the low-frequency component $\\sin(x)$ is learned earlier than the high-frequency component $\\sin(25x)$ by a Physics-Informed Neural Network (PINN), consistent with spectral bias.",
            "solution": "The objective is to train a neural network $u_{\\theta}(x)$ to approximate the solution of the one-dimensional ordinary differential equation (ODE)\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\non the domain $x \\in [0, 2\\pi]$ with periodic boundary conditions $u(0) = u(2\\pi)$ and $u'(0) = u'(2\\pi)$. The analytical solution, $u(x) = \\sin(x) + \\sin(25x)$, is a superposition of a low-frequency component and a high-frequency component. We will demonstrate that gradient-based optimization of the PINN loss causes the network to learn the low-frequency component, $\\sin(x)$, faster than the high-frequency component, $\\sin(25x)$.\n\nFirst, we define the neural network ansatz, a single-hidden-layer perceptron with $H$ neurons and $\\tanh$ activation function:\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\nThe parameters of the network are $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$. To enforce the ODE, we must compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$. Using the chain rule and the identities $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ and $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$, we obtain:\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\nThe network is trained by minimizing a loss function composed of the mean squared error of the ODE residual and the boundary condition residuals. The physics residual at a set of $N$ collocation points $\\{x_n\\}$ is:\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\nThe periodic boundary condition residuals are:\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\nThe total loss function is a weighted sum:\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\nwhere $\\lambda_{\\text{bc}}$ is a hyperparameter to balance the terms, given as $\\lambda_{\\text{bc}} = 1$.\n\nTraining is performed using gradient descent. The parameters are updated according to $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$, where $\\eta$ is the learning rate. The analytical gradients $\\nabla_{\\theta} \\mathcal{L}(\\theta)$ are required. The gradient of the loss with respect to any parameter $p \\in \\theta$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\nThe derivatives of the network output and its spatial derivatives with respect to the parameters $\\{a_k, w_k, b_k, c\\}$ are computed via the chain rule. For example, the gradient with respect to an output weight $a_k$ involves terms like $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$. Complete expressions for all gradients must be derived and implemented for a fully vectorized training loop.\n\nAfter training for a specified number of steps, we quantify the learned frequency components. We evaluate the trained network $u_{\\theta}(x)$ over a dense grid of $M$ points $\\{x_m\\}$ in $[0, 2\\pi)$. We then project this learned function onto the basis functions $\\sin(x)$ and $\\sin(25x)$ by solving a linear least-squares problem to find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ that minimize:\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\nThe solution to this problem is given by $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$, where $\\mathbf{y}$ is the vector of network predictions $u_{\\theta}(x_m)$ and $\\mathbf{B}$ is the design matrix with columns $\\sin(x_m)$ and $\\sin(25x_m)$. The learned amplitudes are $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. Spectral bias is observed if $A_1 > A_{25}$.",
            "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}