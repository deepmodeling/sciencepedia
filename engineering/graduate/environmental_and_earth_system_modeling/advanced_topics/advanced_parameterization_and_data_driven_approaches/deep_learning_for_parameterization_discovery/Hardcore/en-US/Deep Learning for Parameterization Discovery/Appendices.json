{
    "hands_on_practices": [
        {
            "introduction": "When developing deep learning models for physical systems, we often encounter input and target variables with vastly different physical units and numerical scales, such as temperature in Kelvin and specific humidity in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}$. This exercise tackles the foundational issue of data preprocessing, guiding you to derive the optimal normalization that ensures the learning process is stable and treats all variables equitably. By analyzing a local linearization of the network, you will discover the specific transformation that balances gradient contributions at initialization, a crucial step for effective training. ",
            "id": "3873665",
            "problem": "A research group is developing a deep-learning-based parameterization for subgrid tendencies in a general circulation model for the atmosphere. The parameterization takes a state vector of resolved variables and predicts a vector of subgrid source terms. Let the input be the resolved state vector $x \\in \\mathbb{R}^{n}$, composed of physically heterogeneous quantities (for example, temperature in $\\mathrm{K}$, specific humidity in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}$, and wind components in $\\mathrm{m}\\,\\mathrm{s}^{-1}$), and let the target be the subgrid tendency vector $y \\in \\mathbb{R}^{m}$ (for example, heating rate in $\\mathrm{K}\\,\\mathrm{s}^{-1}$ and moistening rate in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}\\,\\mathrm{s}^{-1}$). Assume $x$ and $y$ are jointly zero-mean random vectors, and define their covariance matrices by $\\Sigma_{x} = \\mathbb{E}[x x^{\\top}] \\in \\mathbb{R}^{n \\times n}$ and $\\Sigma_{y} = \\mathbb{E}[y y^{\\top}] \\in \\mathbb{R}^{m \\times m}$, both symmetric positive definite.\n\nTo mitigate unit and scale disparities and obtain balanced gradient magnitudes across all learnable parameters and across variables with different physical units, the team decides to normalize both inputs and targets before training. Consider a local linearization of the network around initialization that maps normalized inputs to normalized outputs: the model output in normalized space is $\\hat{\\tilde{y}} = W \\tilde{x}$, where $\\tilde{x} = A x$ and $\\tilde{y} = B y$ for some invertible normalization matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$ to be chosen. The training objective is the dimensionless Mean Squared Error (MSE) in normalized space,\n$$\nL(W) = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\,\\|\\hat{\\tilde{y}} - \\tilde{y}\\|_{2}^{2}\\,\\right] = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\,\\|W A x - B y\\|_{2}^{2}\\,\\right].\n$$\n\nAssume that, at initialization, $W = 0$, and that $x$ and $y$ are independent (consistent with an uninformative initial model). Define balanced gradients precisely as the requirement that the covariance of the per-sample gradient with respect to $W$ is isotropic at initialization; that is, if $g(x,y) = \\nabla_{W} L(W)\\big|_{W=0}$ denotes the gradient for a single sample, then\n$$\n\\mathrm{Cov}\\!\\left(\\mathrm{vec}\\!\\left(g(x,y)\\right)\\right) = c\\,I_{mn}\n$$\nfor some scalar $c > 0$, where $\\mathrm{vec}(\\cdot)$ vectorizes a matrix by stacking its columns and $I_{mn}$ is the $mn \\times mn$ identity matrix.\n\nStarting only from the definitions above, derive the normalization matrices $A$ and $B$ that achieve balanced gradients at initialization. Express your final answer as a closed-form analytic expression in terms of $\\Sigma_{x}$ and $\\Sigma_{y}$. No numerical evaluation or rounding is required, and no physical units are to be included in the final expression.",
            "solution": "The user wants me to derive the normalization matrices $A$ and $B$ that ensure the covariance of the per-sample gradient is isotropic at initialization.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- Input state vector: $x \\in \\mathbb{R}^{n}$.\n- Target tendency vector: $y \\in \\mathbb{R}^{m}$.\n- Mean of inputs and targets: $\\mathbb{E}[x] = 0$, $\\mathbb{E}[y] = 0$.\n- Input covariance matrix: $\\Sigma_{x} = \\mathbb{E}[x x^{\\top}] \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite.\n- Target covariance matrix: $\\Sigma_{y} = \\mathbb{E}[y y^{\\top}] \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite.\n- Normalization matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$ are invertible.\n- Normalized input: $\\tilde{x} = A x$.\n- Normalized target: $\\tilde{y} = B y$.\n- Linearized model prediction: $\\hat{\\tilde{y}} = W \\tilde{x}$.\n- Training objective (expected loss): $L(W) = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\,\\|W A x - B y\\|_{2}^{2}\\,\\right]$.\n- Initialization of weights: $W = 0$.\n- Statistical independence at initialization: $x$ and $y$ are independent.\n- Per-sample gradient at initialization: $g(x,y) = \\nabla_{W} l(W;x,y)\\big|_{W=0}$, where $l(W;x,y) = \\frac{1}{2}\\|W A x - B y\\|_{2}^{2}$ is the loss for a single sample pair $(x,y)$.\n- Balanced gradients condition: $\\mathrm{Cov}\\!\\left(\\mathrm{vec}\\!\\left(g(x,y)\\right)\\right) = c\\,I_{mn}$ for some scalar constant $c > 0$, where $\\mathrm{vec}(\\cdot)$ vectorizes a matrix and $I_{mn}$ is the identity matrix of size $mn \\times mn$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the field of machine learning for Earth system modeling. The setup is a mathematically precise and well-posed problem involving matrix calculus and statistics. All terms are clearly defined, and the assumptions (zero-mean data, positive definite covariance, independence at initialization) are standard in such analyses. The goal is to derive expressions for $A$ and $B$, which is a solvable task based on the givens. There are no contradictions, ambiguities, or factual errors. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the derivation.\n\nThe core of the problem is to find the matrices $A$ and $B$ that satisfy the balanced gradients condition. The procedure is as follows:\n$1$. Compute the per-sample gradient $g(x,y)$.\n$2$. Compute the covariance matrix of the vectorized gradient, $\\mathrm{Cov}(\\mathrm{vec}(g(x,y)))$.\n$3$. Apply the balanced gradients condition to derive equations for $A$ and $B$.\n$4$. Solve these equations to find the expressions for $A$ and $B$.\n\n**1. Compute the per-sample gradient**\nThe loss for a single sample $(x, y)$ is $l(W; x, y) = \\frac{1}{2}\\|W A x - B y\\|_{2}^{2}$. We can write the squared norm as a trace:\n$$\nl(W; x, y) = \\frac{1}{2}\\mathrm{Tr}\\left( (WAx - By)^{\\top} (WAx - By) \\right)\n$$\nTo find the gradient with respect to the matrix $W$, we compute the differential $dl$:\n$$\ndl = \\mathrm{Tr}\\left( (WAx - By)^{\\top} d(WAx - By) \\right) = \\mathrm{Tr}\\left( (WAx - By)^{\\top} (dW A x) \\right)\n$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(XYZ) = \\mathrm{Tr}(ZXY)$:\n$$\ndl = \\mathrm{Tr}\\left( (Ax) (WAx - By)^{\\top} dW \\right)\n$$\nFrom the definition of the matrix derivative, $dl = \\mathrm{Tr}( (\\nabla_W l)^{\\top} dW )$, we can identify the gradient:\n$$\n\\nabla_W l(W; x, y) = \\left((Ax) (WAx - By)^{\\top}\\right)^{\\top} = (WAx - By)(Ax)^{\\top}\n$$\nThe problem specifies the gradient $g(x,y)$ at initialization, where $W=0$:\n$$\ng(x,y) = \\nabla_W l(W; x, y)\\big|_{W=0} = (0 \\cdot Ax - By)(Ax)^{\\top} = -By(Ax)^{\\top} = -B y x^{\\top} A^{\\top}\n$$\nThis is an $m \\times n$ matrix.\n\n**2. Compute the covariance of the gradient**\nWe need to find $\\mathrm{Cov}(\\mathrm{vec}(g(x,y)))$. First, we compute the mean of the gradient. Since $x$ and $y$ are independent and zero-mean:\n$$\n\\mathbb{E}[g(x,y)] = \\mathbb{E}[-B y x^{\\top} A^{\\top}] = -B \\, \\mathbb{E}[y] \\, \\mathbb{E}[x^{\\top}] \\, A^{\\top} = -B \\cdot 0 \\cdot 0^{\\top} \\cdot A^{\\top} = 0\n$$\nThe mean of the gradient is the zero matrix. Therefore, the covariance of its vectorized form is given by:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}[\\mathrm{vec}(g) \\mathrm{vec}(g)^{\\top}]\n$$\nTo vectorize $g(x,y)$, we use the identity for the outer product of two vectors $u$ and $v$: $\\mathrm{vec}(uv^{\\top}) = v \\otimes u$. In our case, $g(x,y) = (-By)(A x)^{\\top}$. Let $u = -By$ and $v = Ax$. Then:\n$$\n\\mathrm{vec}(g(x,y)) = (Ax) \\otimes (-By) = -(Ax \\otimes By)\n$$\nNow we compute the covariance matrix:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}\\left[ \\left(-(Ax \\otimes By)\\right) \\left(-(Ax \\otimes By)\\right)^{\\top} \\right] \\\\\n= \\mathbb{E}\\left[ (Ax \\otimes By) (Ax \\otimes By)^{\\top} \\right]\n$$\nUsing the identity $(a \\otimes b)^{\\top} = a^{\\top} \\otimes b^{\\top}$, we have:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}\\left[ (Ax \\otimes By) ((Ax)^{\\top} \\otimes (By)^{\\top}) \\right]\n$$\nUsing the mixed-product property $(a \\otimes b)(c \\otimes d) = (ac) \\otimes (bd)$:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}\\left[ ((Ax)(Ax)^{\\top}) \\otimes ((By)(By)^{\\top}) \\right] \\\\\n= \\mathbb{E}\\left[ (Axx^{\\top}A^{\\top}) \\otimes (Byy^{\\top}B^{\\top}) \\right]\n$$\nSince $x$ and $y$ are independent, the expectation of the Kronecker product is the Kronecker product of the expectations:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}[Axx^{\\top}A^{\\top}] \\otimes \\mathbb{E}[Byy^{\\top}B^{\\top}] \\\\\n= A\\,\\mathbb{E}[xx^{\\top}]A^{\\top} \\otimes B\\,\\mathbb{E}[yy^{\\top}]B^{\\top}\n$$\nSubstituting the definitions of the covariance matrices $\\Sigma_x = \\mathbb{E}[xx^{\\top}]$ and $\\Sigma_y = \\mathbb{E}[yy^{\\top}]$:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = (A \\Sigma_x A^{\\top}) \\otimes (B \\Sigma_y B^{\\top})\n$$\n\n**3. Apply the balanced gradients condition**\nThe problem requires this covariance to be isotropic:\n$$\n(A \\Sigma_x A^{\\top}) \\otimes (B \\Sigma_y B^{\\top}) = c I_{mn}\n$$\nfor some scalar $c > 0$. The identity matrix can be written as a Kronecker product $I_{mn} = I_n \\otimes I_m$.\n$$\n(A \\Sigma_x A^{\\top}) \\otimes (B \\Sigma_y B^{\\top}) = c (I_n \\otimes I_m)\n$$\nFor the Kronecker product of two matrices to equal a scaled Kronecker product of two other matrices of the same dimensions, the corresponding matrices must be proportional. Thus, there must exist a non-zero scalar $\\lambda$ such that:\n$$\nA \\Sigma_x A^{\\top} = \\lambda I_n \\\\\nB \\Sigma_y B^{\\top} = \\frac{c}{\\lambda} I_m\n$$\nSince $\\Sigma_x$ and $\\Sigma_y$ are positive definite and $A, B$ are invertible, the matrices on the left are positive definite. This implies that $\\lambda > 0$ and $c/\\lambda > 0$.\n\n**4. Solve for A and B**\nThe constants $\\lambda$ and $c$ affect the overall magnitude of the gradients, which can be absorbed into the learning rate of the optimization algorithm. A canonical choice, corresponding to whitening the variables, is to set these scaling factors to $1$. We choose $\\lambda = 1$ and $c=1$, which simplifies the conditions to:\n$$\n(1) \\quad A \\Sigma_x A^{\\top} = I_n \\\\\n(2) \\quad B \\Sigma_y B^{\\top} = I_m\n$$\nLet's solve equation (1) for $A$. Since $\\Sigma_x$ is symmetric positive definite (SPD), it has a unique SPD square root $\\Sigma_x^{1/2}$ and a unique SPD inverse square root $\\Sigma_x^{-1/2} = (\\Sigma_x^{1/2})^{-1}$. Let's propose $A = \\Sigma_x^{-1/2}$ as a solution. This matrix is also SPD.\nSubstituting this into the left side of equation (1):\n$$\nA \\Sigma_x A^{\\top} = \\Sigma_x^{-1/2} \\Sigma_x (\\Sigma_x^{-1/2})^{\\top}\n$$\nSince $\\Sigma_x^{-1/2}$ is symmetric, $(\\Sigma_x^{-1/2})^{\\top} = \\Sigma_x^{-1/2}$.\n$$\n\\Sigma_x^{-1/2} \\Sigma_x \\Sigma_x^{-1/2} = \\Sigma_x^{-1/2} (\\Sigma_x^{1/2} \\Sigma_x^{1/2}) \\Sigma_x^{-1/2} = (\\Sigma_x^{-1/2} \\Sigma_x^{1/2}) (\\Sigma_x^{1/2} \\Sigma_x^{-1/2}) = I_n \\cdot I_n = I_n\n$$\nThis matches the right side of equation (1). Thus, $A = \\Sigma_x^{-1/2}$ is a valid solution. This is the unique symmetric positive definite solution for $A$. Any other solution would be of the form $O_A \\Sigma_x^{-1/2}$ for an arbitrary orthogonal matrix $O_A$, which corresponds to an arbitrary rotation of the whitened data. The problem asks for \"the\" matrices, implying a canonical choice, which is the symmetric positive definite one.\n\nBy the exact same reasoning for equation (2), the canonical solution for $B$ is:\n$$\nB = \\Sigma_y^{-1/2}\n$$\nThese matrices $A$ and $B$ are the inverse principal square roots of their respective covariance matrices. They are well-defined because $\\Sigma_x$ and $\\Sigma_y$ are symmetric positive definite. They are also invertible, as required.\n\nThe derived matrices $A$ and $B$ correspond to a standard whitening transformation, which decorrelates the components of the input and target vectors and scales them to have unit variance.",
            "answer": "$$\n\\boxed{A = \\Sigma_x^{-1/2}, \\quad B = \\Sigma_y^{-1/2}}\n$$"
        },
        {
            "introduction": "A central challenge in discovering parameterizations is ensuring the learned model respects fundamental physical laws, something not guaranteed by a simple Mean Squared Error loss. This practice moves to the heart of physics-informed machine learning by tasking you with the construction of a sophisticated composite loss function from first principles. You will learn how to combine penalties for local prediction errors, violations of conserved quantities like energy and enstrophy, and deviations from theoretical spectral slopes into a single, cohesive negative log-likelihood framework. ",
            "id": "3873751",
            "problem": "Consider a barotropic Quasi-Geostrophic (QG) closure on a doubly periodic domain $\\Omega \\subset \\mathbb{R}^{2}$ with area $|\\Omega|$, where the barotropic vorticity is $\\zeta = \\nabla^{2} \\psi$, $\\psi$ is the streamfunction, and the Jacobian is $J(\\psi,\\zeta) = \\partial_{x} \\psi \\, \\partial_{y} \\zeta - \\partial_{y} \\psi \\, \\partial_{x} \\zeta$. The coarse-grained barotropic vorticity equation with a parameterized subgrid closure $\\mathcal{C}_{\\boldsymbol{\\theta}}$ reads\n$$\n\\partial_{t} \\zeta + J(\\psi,\\zeta) = f + d + \\mathcal{C}_{\\boldsymbol{\\theta}}(q),\n$$\nwhere $f$ and $d$ are the resolved large-scale forcing and dissipation, respectively, and $q$ denotes the coarse-grained inputs to the closure. You are given a training set of $N$ samples $\\{(q^{(n)},\\psi^{(n)},T^{\\mathrm{ref}(n)},P_{E}^{*(n)},P_{Z}^{*(n)},E_{k}^{\\mathrm{obs}(n)}(\\kappa))\\}_{n=1}^{N}$, where $T^{\\mathrm{ref}(n)}$ is a coarse-grained “reference” subgrid tendency derived from a high-resolution truth, $P_{E}^{*(n)}$ and $P_{Z}^{*(n)}$ are target energy and enstrophy production rates to be matched by the closure, and $E_{k}^{\\mathrm{obs}(n)}(\\kappa)$ is an observed isotropic kinetic energy spectrum used to define an inertial-range slope target. Assume the following fundamental bases and definitions:\n\n- The domain-integrated kinetic energy density is $E = \\frac{1}{2|\\Omega|} \\int_{\\Omega} |\\nabla \\psi|^{2} \\, \\mathrm{d}\\mathbf{x}$, and the enstrophy density is $Z = \\frac{1}{2|\\Omega|} \\int_{\\Omega} \\zeta^{2} \\, \\mathrm{d}\\mathbf{x}$.\n\n- The closure contribution to the energy and enstrophy production rates at a given sample $n$ is defined by the QG energetics and periodic boundary conditions as\n$$\nP_{E}^{(n)}(\\boldsymbol{\\theta}) = -\\frac{1}{|\\Omega|} \\int_{\\Omega} \\psi^{(n)}(\\mathbf{x}) \\, \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) \\, \\mathrm{d}\\mathbf{x}, \\quad\nP_{Z}^{(n)}(\\boldsymbol{\\theta}) = \\frac{1}{|\\Omega|} \\int_{\\Omega} \\zeta^{(n)}(\\mathbf{x}) \\, \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) \\, \\mathrm{d}\\mathbf{x}.\n$$\n\n- Let $\\kappa$ denote the isotropic wavenumber magnitude. Over an inertial subrange $[\\kappa_{1},\\kappa_{2}]$, a physically consistent kinetic energy spectrum obeys a power-law $E_{k}(\\kappa) \\propto \\kappa^{s^{*}}$ with a target slope $s^{*}$ informed by the observed $E_{k}^{\\mathrm{obs}(n)}(\\kappa)$; spectral slope accuracy will be penalized via the deviation of the log-log derivative $\\frac{\\mathrm{d}\\ln E_{k}}{\\mathrm{d}\\ln \\kappa}$ from $s^{*}$.\n\nYou seek a dimensionless composite training loss for deep learning-based parameterization discovery that simultaneously penalizes: (i) local tendency error, (ii) conserved budget residuals, and (iii) spectral slope inaccuracy. Assume each residual is independently Gaussian-distributed with unknown scale parameters $\\sigma_{\\mathrm{t}}$, $\\sigma_{\\mathrm{E}}$, $\\sigma_{\\mathrm{Z}}$, and $\\sigma_{\\mathrm{s}}$, respectively. Define the components as follows:\n\n- The local tendency error averaged over the dataset is\n$$\n\\mathcal{E}_{\\mathrm{tend}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{|\\Omega|} \\int_{\\Omega} \\left( \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) - T^{\\mathrm{ref}(n)}(\\mathbf{x}) \\right)^{2} \\, \\mathrm{d}\\mathbf{x}.\n$$\n\n- The energy and enstrophy budget residuals averaged over the dataset are\n$$\n\\mathcal{E}_{\\mathrm{bud,E}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\left( P_{E}^{(n)}(\\boldsymbol{\\theta}) - P_{E}^{*(n)} \\right)^{2}, \\quad\n\\mathcal{E}_{\\mathrm{bud,Z}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\left( P_{Z}^{(n)}(\\boldsymbol{\\theta}) - P_{Z}^{*(n)} \\right)^{2}.\n$$\n\n- The spectral slope inaccuracy averaged over the dataset is\n$$\n\\mathcal{E}_{\\mathrm{spec}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} W(\\kappa) \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\ln \\kappa} \\ln E_{k}^{(n)}(\\kappa;\\boldsymbol{\\theta}) - s^{*} \\right)^{2} \\, \\mathrm{d}(\\ln \\kappa),\n$$\nwhere $W(\\kappa)$ is a nonnegative weight normalized such that $\\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} W(\\kappa) \\, \\mathrm{d}(\\ln \\kappa) = 1$ to ensure a dimensionless penalty, and $E_{k}^{(n)}(\\kappa;\\boldsymbol{\\theta})$ denotes the isotropic kinetic energy spectrum associated with the coarse state and closure for sample $n$.\n\nConstruct, from first principles and the Gaussian residual model, the single closed-form analytic expression for the dimensionless composite negative log-likelihood loss $L(\\boldsymbol{\\theta},\\sigma_{\\mathrm{t}},\\sigma_{\\mathrm{E}},\\sigma_{\\mathrm{Z}},\\sigma_{\\mathrm{s}})$ that combines the three components above. The final answer must be a single closed-form analytic expression. No units are required in the final answer. Do not round; an exact symbolic expression is required.",
            "solution": "The problem asks for the construction of a dimensionless composite negative log-likelihood loss function, denoted as $L(\\boldsymbol{\\theta},\\sigma_{\\mathrm{t}},\\sigma_{\\mathrm{E}},\\sigma_{\\mathrm{Z}},\\sigma_{\\mathrm{s}})$, for a deep learning-based parameterization in a quasi-geostrophic model. The process begins with the validation of the problem statement.\n\n### Step 1: Extract Givens\n- **Governing Equation:** The coarse-grained barotropic vorticity equation is $\\partial_{t} \\zeta + J(\\psi,\\zeta) = f + d + \\mathcal{C}_{\\boldsymbol{\\theta}}(q)$.\n- **Definitions:** Vorticity $\\zeta = \\nabla^{2} \\psi$; Jacobian $J(\\psi,\\zeta) = \\partial_{x} \\psi \\, \\partial_{y} \\zeta - \\partial_{y} \\psi \\, \\partial_{x} \\zeta$. Domain is $\\Omega \\subset \\mathbb{R}^{2}$ with area $|\\Omega|$.\n- **Training Data:** A set of $N$ samples $\\{(q^{(n)},\\psi^{(n)},T^{\\mathrm{ref}(n)},P_{E}^{*(n)},P_{Z}^{*(n)},E_{k}^{\\mathrm{obs}(n)}(\\kappa))\\}_{n=1}^{N}$.\n- **Energy and Enstrophy Production Rates by Closure:** For a sample $n$,\n$$\nP_{E}^{(n)}(\\boldsymbol{\\theta}) = -\\frac{1}{|\\Omega|} \\int_{\\Omega} \\psi^{(n)}(\\mathbf{x}) \\, \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) \\, \\mathrm{d}\\mathbf{x}\n$$\n$$\nP_{Z}^{(n)}(\\boldsymbol{\\theta}) = \\frac{1}{|\\Omega|} \\int_{\\Omega} \\zeta^{(n)}(\\mathbf{x}) \\, \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) \\, \\mathrm{d}\\mathbf{x}\n$$\n- **Mean-Squared Error Components:**\n  - Local tendency error: $\\mathcal{E}_{\\mathrm{tend}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{|\\Omega|} \\int_{\\Omega} \\left( \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) - T^{\\mathrm{ref}(n)}(\\mathbf{x}) \\right)^{2} \\, \\mathrm{d}\\mathbf{x}$\n  - Energy budget residual: $\\mathcal{E}_{\\mathrm{bud,E}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\left( P_{E}^{(n)}(\\boldsymbol{\\theta}) - P_{E}^{*(n)} \\right)^{2}$\n  - Enstrophy budget residual: $\\mathcal{E}_{\\mathrm{bud,Z}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\left( P_{Z}^{(n)}(\\boldsymbol{\\theta}) - P_{Z}^{*(n)} \\right)^{2}$\n  - Spectral slope inaccuracy: $\\mathcal{E}_{\\mathrm{spec}}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^{N} \\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} W(\\kappa) \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\ln \\kappa} \\ln E_{k}^{(n)}(\\kappa;\\boldsymbol{\\theta}) - s^{*} \\right)^{2} \\, \\mathrm{d}(\\ln \\kappa)$, with $\\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} W(\\kappa) \\, \\mathrm{d}(\\ln \\kappa) = 1$.\n- **Probabilistic Assumption:** The residuals for the four components (tendency, energy budget, enstrophy budget, spectral slope) are independently Gaussian-distributed with zero mean and unknown scale parameters (standard deviations) $\\sigma_{\\mathrm{t}}$, $\\sigma_{\\mathrm{E}}$, $\\sigma_{\\mathrm{Z}}$, and $\\sigma_{\\mathrm{s}}$, respectively.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in geophysical fluid dynamics and machine learning. It is well-posed, as it asks for the derivation of a function from clear definitions and a standard probabilistic model. The language is objective and mathematically precise. All necessary components are defined, and there are no contradictions. The problem does not exhibit any of the flaws listed in the validation criteria. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to derive the solution.\n\n### Derivation of the Composite Loss Function\nThe foundation of the loss function is the principle of maximum likelihood estimation. We are tasked with finding the negative log-likelihood of the observations, given the model parameters $\\boldsymbol{\\theta}$ and the noise parameters $\\sigma_{\\mathrm{t}}, \\sigma_{\\mathrm{E}}, \\sigma_{\\mathrm{Z}}, \\sigma_{\\mathrm{s}}$.\n\nLet's consider a generic task with residuals $r_i$ assumed to be drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. The probability density function for a single residual $r$ is:\n$$\np(r|\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right)\n$$\nThe negative log-likelihood for this single observation is:\n$$\n-\\ln p(r|\\sigma) = \\frac{r^2}{2\\sigma^2} + \\frac{1}{2}\\ln(2\\pi\\sigma^2) = \\frac{r^2}{2\\sigma^2} + \\ln\\sigma + \\frac{1}{2}\\ln(2\\pi)\n$$\nFor optimization purposes, the constant term $\\frac{1}{2}\\ln(2\\pi)$ can be dropped. The loss for a single observation is proportional to $\\frac{r^2}{2\\sigma^2} + \\ln\\sigma$.\n\nWhen dealing with a set of observations, the total negative log-likelihood is the sum over all observations. If we average this over the number of observations, and define the mean squared error as $\\mathcal{E} = \\langle r^2 \\rangle$, the per-observation negative log-likelihood loss becomes $\\frac{\\mathcal{E}}{2\\sigma^2} + \\ln\\sigma$. This form is standard for multi-task learning with homoscedastic uncertainty for each task.\n\nThe problem states that the residuals for the four components are independent. Therefore, the total likelihood is the product of the individual likelihoods, and the total negative log-likelihood is the sum of the individual negative log-likelihoods. We apply the derived form to each of the four components.\n\n1.  **Tendency Loss ($L_{\\mathrm{tend}}$):** The mean squared error is given by $\\mathcal{E}_{\\mathrm{tend}}(\\boldsymbol{\\theta})$. The associated noise parameter is $\\sigma_{\\mathrm{t}}$. The negative log-likelihood component is:\n    $$\n    L_{\\mathrm{tend}}(\\boldsymbol{\\theta}, \\sigma_{\\mathrm{t}}) = \\frac{1}{2\\sigma_{\\mathrm{t}}^2} \\mathcal{E}_{\\mathrm{tend}}(\\boldsymbol{\\theta}) + \\ln \\sigma_{\\mathrm{t}}\n    $$\n\n2.  **Energy Budget Loss ($L_{\\mathrm{bud,E}}$):** The mean squared error is $\\mathcal{E}_{\\mathrm{bud,E}}(\\boldsymbol{\\theta})$. The noise parameter is $\\sigma_{\\mathrm{E}}$. The negative log-likelihood component is:\n    $$\n    L_{\\mathrm{bud,E}}(\\boldsymbol{\\theta}, \\sigma_{\\mathrm{E}}) = \\frac{1}{2\\sigma_{\\mathrm{E}}^2} \\mathcal{E}_{\\mathrm{bud,E}}(\\boldsymbol{\\theta}) + \\ln \\sigma_{\\mathrm{E}}\n    $$\n\n3.  **Enstrophy Budget Loss ($L_{\\mathrm{bud,Z}}$):** The mean squared error is $\\mathcal{E}_{\\mathrm{bud,Z}}(\\boldsymbol{\\theta})$. The noise parameter is $\\sigma_{\\mathrm{Z}}$. The negative log-likelihood component is:\n    $$\n    L_{\\mathrm{bud,Z}}(\\boldsymbol{\\theta}, \\sigma_{\\mathrm{Z}}) = \\frac{1}{2\\sigma_{\\mathrm{Z}}^2} \\mathcal{E}_{\\mathrm{bud,Z}}(\\boldsymbol{\\theta}) + \\ln \\sigma_{\\mathrm{Z}}\n    $$\n\n4.  **Spectral Slope Loss ($L_{\\mathrm{spec}}$):** The mean squared error is $\\mathcal{E}_{\\mathrm{spec}}(\\boldsymbol{\\theta})$. The noise parameter is $\\sigma_{\\mathrm{s}}$. The negative log-likelihood component is:\n    $$\n    L_{\\mathrm{spec}}(\\boldsymbol{\\theta}, \\sigma_{\\mathrm{s}}) = \\frac{1}{2\\sigma_{\\mathrms}^2} \\mathcal{E}_{\\mathrm{spec}}(\\boldsymbol{\\theta}) + \\ln \\sigma_{\\mathrm{s}}\n    $$\n\nThe composite negative log-likelihood loss $L(\\boldsymbol{\\theta},\\sigma_{\\mathrm{t}},\\sigma_{\\mathrm{E}},\\sigma_{\\mathrm{Z}},\\sigma_{\\mathrm{s}})$ is the sum of these four components:\n$$\nL = L_{\\mathrm{tend}} + L_{\\mathrm{bud,E}} + L_{\\mathrm{bud,Z}} + L_{\\mathrm{spec}}\n$$\nSubstituting the full expressions for the error terms $\\mathcal{E}$ gives the final closed-form expression. The problem is posed to find the single analytic expression for the loss, which combines these elements.\n\nThe complete expression is therefore:\n$L(\\boldsymbol{\\theta},\\sigma_{\\mathrm{t}},\\sigma_{\\mathrm{E}},\\sigma_{\\mathrm{Z}},\\sigma_{\\mathrm{s}}) = \\left( \\frac{1}{2\\sigma_{\\mathrm{t}}^2} \\left[ \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{|\\Omega|} \\int_{\\Omega} \\left( \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) - T^{\\mathrm{ref}(n)}(\\mathbf{x}) \\right)^{2} \\, \\mathrm{d}\\mathbf{x} \\right] + \\ln \\sigma_{\\mathrm{t}} \\right) + \\left( \\frac{1}{2\\sigma_{\\mathrm{E}}^2} \\left[ \\frac{1}{N} \\sum_{n=1}^{N} \\left( P_{E}^{(n)}(\\boldsymbol{\\theta}) - P_{E}^{*(n)} \\right)^{2} \\right] + \\ln \\sigma_{\\mathrm{E}} \\right) + \\left( \\frac{1}{2\\sigma_{\\mathrm{Z}}^2} \\left[ \\frac{1}{N} \\sum_{n=1}^{N} \\left( P_{Z}^{(n)}(\\boldsymbol{\\theta}) - P_{Z}^{*(n)} \\right)^{2} \\right] + \\ln \\sigma_{\\mathrm{Z}} \\right) + \\left( \\frac{1}{2\\sigma_{\\mathrms}^2} \\left[ \\frac{1}{N} \\sum_{n=1}^{N} \\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} W(\\kappa) \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\ln \\kappa} \\ln E_{k}^{(n)}(\\kappa;\\boldsymbol{\\theta}) - s^{*} \\right)^{2} \\, \\mathrm{d}(\\ln \\kappa) \\right] + \\ln \\sigma_{\\mathrm{s}} \\right)$.\n\nThis expression can be slightly simplified by factoring out the common $1/N$ term from the summations.\n$L = \\frac{1}{2\\sigma_{\\mathrm{t}}^2 N |\\Omega|} \\sum_{n=1}^{N} \\int_{\\Omega} \\left( \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) - T^{\\mathrm{ref}(n)}(\\mathbf{x}) \\right)^{2} \\, \\mathrm{d}\\mathbf{x} + \\frac{1}{2\\sigma_{\\mathrm{E}}^2 N} \\sum_{n=1}^{N} \\left( P_{E}^{(n)}(\\boldsymbol{\\theta}) - P_{E}^{*(n)} \\right)^{2} + \\frac{1}{2\\sigma_{\\mathrm{Z}}^2 N} \\sum_{n=1}^{N} \\left( P_{Z}^{(n)}(\\boldsymbol{\\theta}) - P_{Z}^{*(n)} \\right)^{2} + \\frac{1}{2\\sigma_{\\mathrms}^2 N} \\sum_{n=1}^{N} \\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} W(\\kappa) \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\ln \\kappa} \\ln E_{k}^{(n)}(\\kappa;\\boldsymbol{\\theta}) - s^{*} \\right)^{2} \\, \\mathrm{d}(\\ln \\kappa) + \\ln(\\sigma_{\\mathrm{t}}\\sigma_{\\mathrm{E}}\\sigma_{\\mathrm{Z}}\\sigma_{\\mathrm{s}})$.\n\nThis final expression represents the total composite negative log-likelihood, which is to be minimized with respect to both the model parameters $\\boldsymbol{\\theta}$ and the noise scale parameters $\\sigma_{\\mathrm{t}}, \\sigma_{\\mathrm{E}}, \\sigma_{\\mathrm{Z}}, \\sigma_{\\mathrm{s}}$. The logarithmic terms act as regularizers that prevent the scale parameters from growing infinitely large (which would drive the MSE-related terms to zero).",
            "answer": "$$\n\\boxed{\\frac{1}{2\\sigma_{\\mathrm{t}}^2 N} \\sum_{n=1}^{N} \\frac{1}{|\\Omega|} \\int_{\\Omega} \\! \\left( \\mathcal{C}_{\\boldsymbol{\\theta}}(q^{(n)})(\\mathbf{x}) - T^{\\mathrm{ref}(n)}(\\mathbf{x}) \\right)^{2} \\mathrm{d}\\mathbf{x} + \\frac{1}{2\\sigma_{\\mathrm{E}}^2 N} \\sum_{n=1}^{N} \\left( P_{E}^{(n)}(\\boldsymbol{\\theta}) - P_{E}^{*(n)} \\right)^{2} + \\frac{1}{2\\sigma_{\\mathrm{Z}}^2 N} \\sum_{n=1}^{N} \\left( P_{Z}^{(n)}(\\boldsymbol{\\theta}) - P_{Z}^{*(n)} \\right)^{2} + \\frac{1}{2\\sigma_{\\mathrm{s}}^2 N} \\sum_{n=1}^{N} \\int_{\\ln \\kappa_{1}}^{\\ln \\kappa_{2}} \\! W(\\kappa) \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\ln \\kappa} \\ln E_{k}^{(n)}(\\kappa;\\boldsymbol{\\theta}) - s^{*} \\right)^{2} \\mathrm{d}(\\ln \\kappa) + \\ln(\\sigma_{\\mathrm{t}}) + \\ln(\\sigma_{\\mathrm{E}}) + \\ln(\\sigma_{\\mathrm{Z}}) + \\ln(\\sigma_{\\mathrm{s}})}\n$$"
        },
        {
            "introduction": "The ultimate validation for a parameterization is its performance when integrated within a larger dynamical system, a process known as *a posteriori* testing. This hands-on coding exercise provides a practical framework for this crucial evaluation, focusing on the non-negotiable principle of energy conservation. You will implement a simplified atmospheric energy budget to quantify the unphysical energy sources or sinks introduced by a hypothetical learned convective closure and determine if this imbalance is statistically significant. ",
            "id": "3873646",
            "problem": "Consider a single-column moist static energy budget for an atmospheric column, where the column-integrated moist static energy is defined by $h = c_p T + L_v q + g z$ and the column integral is $H = \\int_0^{z_t} \\rho h \\, dz$, with $c_p$ the specific heat of air at constant pressure, $T$ the temperature, $L_v$ the latent heat of vaporization, $q$ the specific humidity, $g$ the gravitational acceleration, $\\rho$ the air density, and $z_t$ the top of the column. The conservation of energy for $H$ at the column scale obeys the budget equation\n$$\n\\frac{\\partial H}{\\partial t} = R + S + A + C + \\varepsilon,\n$$\nwhere $R$ is net radiative heating into the column, $S$ is net surface enthalpy flux into the column, $A$ is net horizontal advective import of moist static energy, $C$ is the subgrid convective moist static energy tendency, and $\\varepsilon$ is the residual imbalance due to diagnostics, discretization, or parameterization error. All flux terms $R$, $S$, $A$, and $C$ as well as $\\partial_t H$ are to be treated as column-integrated tendencies in units of $\\mathrm{W\\,m^{-2}}$.\n\nYou are given a proposed convective closure $C_\\theta(t)$ produced by a deep learning parameterization discovery model. The closure is approximated as a nonlinear mapping of available resolved-column diagnostics,\n$$\nC_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh\\!\\Big(\\beta_1 \\frac{S(t) - S_0}{S_{\\mathrm{amp}}}\\Big) + \\alpha_2 \\tanh\\!\\Big(\\beta_2 \\frac{R(t) - R_0}{R_{\\mathrm{amp}}}\\Big) + \\alpha_3 \\tanh\\!\\Big(\\beta_3 \\frac{A(t) - A_0}{A_{\\mathrm{amp}}}\\Big) - \\chi \\big(R(t) + S(t) + A(t)\\big),\n$$\nwhere $\\alpha_0$, $\\alpha_1$, $\\alpha_2$, $\\alpha_3$, $\\beta_1$, $\\beta_2$, $\\beta_3$, and $\\chi$ are closure parameters. You must compute $\\varepsilon(t)$ implied by this proposed closure and assess its statistical significance under explicit measurement and modeling uncertainties.\n\nFor the purposes of this problem, the resolved diagnostics follow deterministic periodic trajectories that are scientifically plausible for a diurnal cycle. For a given number of time steps $N$ and angular frequency $\\omega = 2\\pi/N$ (in radians), define for $t = 0, 1, \\dots, N-1$,\n- $R(t) = R_0 + R_{\\mathrm{amp}} \\sin(\\omega t)$,\n- $S(t) = S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)$,\n- $A(t) = A_0 + A_{\\mathrm{amp}} \\sin(2\\omega t)$.\n\nA physically consistent reference convective tendency is constructed to yield a column storage that is small compared to the fluxes, mimicking a near-steady column with mild variability:\n$$\nC_{\\mathrm{true}}(t) = -\\big(R(t) + S(t) + A(t)\\big) + D_0 + D_{\\mathrm{amp}} \\sin\\!\\big(\\omega t + \\phi\\big),\n$$\nwith a prescribed phase shift $\\phi$ (in radians). The diagnosed storage tendency is then\n$$\n\\frac{\\partial H}{\\partial t}(t) = R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t).\n$$\n\nGiven the uncertainties $\\sigma_R$, $\\sigma_S$, $\\sigma_A$, $\\sigma_{\\partial H}$, and $\\sigma_C$ (each in $\\mathrm{W\\,m^{-2}}$) for the respective diagnostics $R$, $S$, $A$, $\\partial_t H$, and the closure $C_\\theta$, assume independent, zero-mean Gaussian errors across time and terms. Under these assumptions, the standard error of the time-mean imbalance $\\bar{\\varepsilon} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\varepsilon(t)$ is\n$$\n\\mathrm{SE}(\\bar{\\varepsilon}) = \\sqrt{\\frac{\\sigma_R^2 + \\sigma_S^2 + \\sigma_A^2 + \\sigma_{\\partial H}^2 + \\sigma_C^2}{N}}.\n$$\nDefine the $z$-score $z = |\\bar{\\varepsilon}| / \\mathrm{SE}(\\bar{\\varepsilon})$ and assess statistical significance at the two-sided $95\\%$ confidence level using the Gaussian critical value $z_{0.95} = 1.96$. Report whether $z > 1.96$.\n\nYour task is to implement a program that, for each test case, computes the time series $\\varepsilon(t) = \\frac{\\partial H}{\\partial t}(t) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)$, its time-mean $\\bar{\\varepsilon}$, the standard error $\\mathrm{SE}(\\bar{\\varepsilon})$, and determines the significance boolean $z > 1.96$. Express the mean residual imbalance $\\bar{\\varepsilon}$ in $\\mathrm{W\\,m^{-2}}$, rounded to three decimal places. Angles (the phase $\\phi$ and trigonometric arguments) must be treated in radians.\n\nTest Suite:\nProvide results for the following four parameter sets. For each case, the program must use the values exactly as given.\n\n- Case $1$ (general diurnal variability, moderate uncertainties):\n  - $N = 24$, $R_0 = -50$, $R_{\\mathrm{amp}} = 15$, $S_0 = 100$, $S_{\\mathrm{amp}} = 25$, $A_0 = 10$, $A_{\\mathrm{amp}} = 8$, $D_0 = 3$, $D_{\\mathrm{amp}} = 2$, $\\phi = \\pi/6$, closure parameters: $\\alpha_0 = 0$, $\\alpha_1 = 8$, $\\beta_1 = 1.0$, $\\alpha_2 = -5$, $\\beta_2 = 1.2$, $\\alpha_3 = 2$, $\\beta_3 = 0.8$, $\\chi = 0.92$, uncertainties: $\\sigma_R = 3.0$, $\\sigma_S = 5.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 4.0$.\n- Case $2$ (large uncertainties, near-balanced closure):\n  - $N = 12$, $R_0 = -30$, $R_{\\mathrm{amp}} = 12$, $S_0 = 80$, $S_{\\mathrm{amp}} = 20$, $A_0 = 5$, $A_{\\mathrm{amp}} = 4$, $D_0 = 0.5$, $D_{\\mathrm{amp}} = 1.0$, $\\phi = \\pi/3$, closure parameters: $\\alpha_0 = 0$, $\\alpha_1 = 5$, $\\beta_1 = 0.7$, $\\alpha_2 = -3$, $\\beta_2 = 1.1$, $\\alpha_3 = 1$, $\\beta_3 = 0.5$, $\\chi = 0.99$, uncertainties: $\\sigma_R = 10.0$, $\\sigma_S = 12.0$, $\\sigma_A = 6.0$, $\\sigma_{\\partial H} = 8.0$, $\\sigma_C = 10.0$.\n- Case $3$ (strong closure bias, small uncertainties):\n  - $N = 60$, $R_0 = -40$, $R_{\\mathrm{amp}} = 20$, $S_0 = 120$, $S_{\\mathrm{amp}} = 30$, $A_0 = 15$, $A_{\\mathrm{amp}} = 10$, $D_0 = 2$, $D_{\\mathrm{amp}} = 3$, $\\phi = \\pi/4$, closure parameters: $\\alpha_0 = 10$, $\\alpha_1 = -2$, $\\beta_1 = 1.5$, $\\alpha_2 = 4$, $\\beta_2 = 0.9$, $\\alpha_3 = -1$, $\\beta_3 = 1.2$, $\\chi = 0.75$, uncertainties: $\\sigma_R = 2.0$, $\\sigma_S = 3.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 2.0$.\n- Case $4$ (small sample, moderate uncertainties):\n  - $N = 10$, $R_0 = -35$, $R_{\\mathrm{amp}} = 10$, $S_0 = 90$, $S_{\\mathrm{amp}} = 15$, $A_0 = 8$, $A_{\\mathrm{amp}} = 6$, $D_0 = 1$, $D_{\\mathrm{amp}} = 1.5$, $\\phi = \\pi/8$, closure parameters: $\\alpha_0 = 0$, $\\alpha_1 = 1$, $\\beta_1 = 1.0$, $\\alpha_2 = 1$, $\\beta_2 = 1.0$, $\\alpha_3 = 1$, $\\beta_3 = 1.0$, $\\chi = 0.98$, uncertainties: $\\sigma_R = 4.0$, $\\sigma_S = 6.0$, $\\sigma_A = 3.0$, $\\sigma_{\\partial H} = 2.0$, $\\sigma_C = 5.0$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the pair $[\\bar{\\varepsilon}, \\text{is\\_significant}]$, where $\\bar{\\varepsilon}$ is the mean residual imbalance in $\\mathrm{W\\,m^{-2}}$ rounded to three decimal places, and $\\text{is\\_significant}$ is a boolean indicating whether $z > 1.96$. The final output must look like\n$$\n\\big[[\\bar{\\varepsilon}_1,\\text{is\\_significant}_1],[\\bar{\\varepsilon}_2,\\text{is\\_significant}_2],[\\bar{\\varepsilon}_3,\\text{is\\_significant}_3],[\\bar{\\varepsilon}_4,\\text{is\\_significant}_4]\\big].\n$$",
            "solution": "The user has provided a problem that requires the evaluation of a deep learning-based convective parameterization within a simplified atmospheric moist static energy budget. The core task is to compute the time-mean residual energy imbalance, $\\bar{\\varepsilon}$, and assess its statistical significance against measurement and modeling uncertainties.\n\n### Step 1: Extract Givens\n\n**Equations and Definitions:**\n- Column-integrated moist static energy budget: $\\frac{\\partial H}{\\partial t} = R + S + A + C + \\varepsilon$\n- Proposed convective closure ($C_\\theta$): $C_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh\\!\\Big(\\beta_1 \\frac{S(t) - S_0}{S_{\\mathrm{amp}}}\\Big) + \\alpha_2 \\tanh\\!\\Big(\\beta_2 \\frac{R(t) - R_0}{R_{\\mathrm{amp}}}\\Big) + \\alpha_3 \\tanh\\!\\Big(\\beta_3 \\frac{A(t) - A_0}{A_{\\mathrm{amp}}}\\Big) - \\chi \\big(R(t) + S(t) + A(t)\\big)$\n- Resolved diagnostic trajectories for $t = 0, 1, \\dots, N-1$ with $\\omega = 2\\pi/N$:\n  - $R(t) = R_0 + R_{\\mathrm{amp}} \\sin(\\omega t)$\n  - $S(t) = S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)$\n  - $A(t) = A_0 + A_{\\mathrm{amp}} \\sin(2\\omega t)$\n- Physically consistent reference convective tendency ($C_{\\mathrm{true}}$): $C_{\\mathrm{true}}(t) = -\\big(R(t) + S(t) + A(t)\\big) + D_0 + D_{\\mathrm{amp}} \\sin\\!\\big(\\omega t + \\phi\\big)$\n- Diagnosed storage tendency: $\\frac{\\partial H}{\\partial t}(t) = R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t)$\n- Residual imbalance: $\\varepsilon(t) = \\frac{\\partial H}{\\partial t}(t) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)$\n- Time-mean imbalance: $\\bar{\\varepsilon} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\varepsilon(t)$\n- Standard error of the mean imbalance: $\\mathrm{SE}(\\bar{\\varepsilon}) = \\sqrt{\\frac{\\sigma_R^2 + \\sigma_S^2 + \\sigma_A^2 + \\sigma_{\\partial H}^2 + \\sigma_C^2}{N}}$\n- $z$-score: $z = |\\bar{\\varepsilon}| / \\mathrm{SE}(\\bar{\\varepsilon})$\n- Significance criterion: $z > 1.96$ (two-sided $95\\%$ confidence level)\n\n**Test Cases:**\n- **Case 1**: $N = 24$, $R_0 = -50$, $R_{\\mathrm{amp}} = 15$, $S_0 = 100$, $S_{\\mathrm{amp}} = 25$, $A_0 = 10$, $A_{\\mathrm{amp}} = 8$, $D_0 = 3$, $D_{\\mathrm{amp}} = 2$, $\\phi = \\pi/6$, $\\alpha_0 = 0$, $\\alpha_1 = 8$, $\\beta_1 = 1.0$, $\\alpha_2 = -5$, $\\beta_2 = 1.2$, $\\alpha_3 = 2$, $\\beta_3 = 0.8$, $\\chi = 0.92$, $\\sigma_R = 3.0$, $\\sigma_S = 5.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 4.0$.\n- **Case 2**: $N = 12$, $R_0 = -30$, $R_{\\mathrm{amp}} = 12$, $S_0 = 80$, $S_{\\mathrm{amp}} = 20$, $A_0 = 5$, $A_{\\mathrm{amp}} = 4$, $D_0 = 0.5$, $D_{\\mathrm{amp}} = 1.0$, $\\phi = \\pi/3$, $\\alpha_0 = 0$, $\\alpha_1 = 5$, $\\beta_1 = 0.7$, $\\alpha_2 = -3$, $\\beta_2 = 1.1$, $\\alpha_3 = 1$, $\\beta_3 = 0.5$, $\\chi = 0.99$, $\\sigma_R = 10.0$, $\\sigma_S = 12.0$, $\\sigma_A = 6.0$, $\\sigma_{\\partial H} = 8.0$, $\\sigma_C = 10.0$.\n- **Case 3**: $N = 60$, $R_0 = -40$, $R_{\\mathrm{amp}} = 20$, $S_0 = 120$, $S_{\\mathrm{amp}} = 30$, $A_0 = 15$, $A_{\\mathrm{amp}} = 10$, $D_0 = 2$, $D_{\\mathrm{amp}} = 3$, $\\phi = \\pi/4$, $\\alpha_0 = 10$, $\\alpha_1 = -2$, $\\beta_1 = 1.5$, $\\alpha_2 = 4$, $\\beta_2 = 0.9$, $\\alpha_3 = -1$, $\\beta_3 = 1.2$, $\\chi = 0.75$, $\\sigma_R = 2.0$, $\\sigma_S = 3.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 2.0$.\n- **Case 4**: $N = 10$, $R_0 = -35$, $R_{\\mathrm{amp}} = 10$, $S_0 = 90$, $S_{\\mathrm{amp}} = 15$, $A_0 = 8$, $A_{\\mathrm{amp}} = 6$, $D_0 = 1$, $D_{\\mathrm{amp}} = 1.5$, $\\phi = \\pi/8$, $\\alpha_0 = 0$, $\\alpha_1 = 1$, $\\beta_1 = 1.0$, $\\alpha_2 = 1$, $\\beta_2 = 1.0$, $\\alpha_3 = 1$, $\\beta_3 = 1.0$, $\\chi = 0.98$, $\\sigma_R = 4.0$, $\\sigma_S = 6.0$, $\\sigma_A = 3.0$, $\\sigma_{\\partial H} = 2.0$, $\\sigma_C = 5.0$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Soundness**: The problem uses a simplified, but conceptually valid, model of an atmospheric column's energy budget. Such simplifications are standard practice in theoretical meteorology and for testing numerical schemes. The functional forms for the parameterization and forcings are plausible representations for a modeling exercise. All equations are mathematically and physically consistent within the context of this model.\n2.  **Well-Posedness**: All necessary definitions, equations, and numerical parameters are provided for each test case. The problem is self-contained and free of contradictions. The computational task is clearly defined, leading to a unique solution for each case.\n3.  **Objectivity**: The problem is stated in precise mathematical and scientific language. It does not contain subjective claims or ambiguities.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe primary objective is to calculate the time-mean residual $\\bar{\\varepsilon}$ and determine if it is statistically significant. The process involves several computational steps for each test case.\n\n**1. Simplification of the Residual Calculation**\n\nThe residual imbalance $\\varepsilon(t)$ is defined as the difference between the diagnosed storage tendency $\\frac{\\partial H}{\\partial t}$ and the sum of all parameterized and resolved flux tendencies:\n$$\n\\varepsilon(t) = \\frac{\\partial H}{\\partial t}(t) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)\n$$\nThe problem provides an expression for the \"true\" or diagnosed storage tendency, which is based on a reference convective tendency $C_{\\mathrm{true}}(t)$:\n$$\n\\frac{\\partial H}{\\partial t}(t) = R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t)\n$$\nSubstituting this expression into the equation for $\\varepsilon(t)$ yields a crucial simplification:\n$$\n\\varepsilon(t) = \\big(R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t)\\big) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)\n$$\nThe terms $R(t)$, $S(t)$, and $A(t)$ cancel out, leaving:\n$$\n\\varepsilon(t) = C_{\\mathrm{true}}(t) - C_\\theta(t)\n$$\nThis result shows that the residual imbalance is simply the difference between the true convective tendency and the value predicted by the deep learning closure. This simplifies the computation significantly.\n\n**2. Algorithmic Procedure**\n\nFor each test case, the following steps are executed:\n\n- **Generate Time Series**: For a given number of time steps $N$, define a time vector for $t = 0, 1, \\dots, N-1$. The angular frequency is $\\omega = 2\\pi/N$.\n\n- **Compute Forcing Terms**: Calculate the time series for the resolved diagnostics $R(t)$, $S(t)$, and $A(t)$ using the provided sinusoidal functions and case-specific parameters:\n  - $R(t) = R_0 + R_{\\mathrm{amp}} \\sin(\\omega t)$\n  - $S(t) = S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)$\n  - $A(t) = A_0 + A_{\\mathrm{amp}} \\sin(2\\omega t)$\n\n- **Compute True Convective Tendency**: Use the computed forcing terms to find the true convective tendency $C_{\\mathrm{true}}(t)$:\n  $$\n  C_{\\mathrm{true}}(t) = -\\big(R(t) + S(t) + A(t)\\big) + D_0 + D_{\\mathrm{amp}} \\sin\\!\\big(\\omega t + \\phi\\big)\n  $$\n\n- **Compute Parameterized Convective Tendency**: Calculate the proposed closure's prediction $C_\\theta(t)$ using its definition:\n  $$\n  C_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh\\!\\Big(\\beta_1 \\frac{S(t) - S_0}{S_{\\mathrm{amp}}}\\Big) + \\alpha_2 \\tanh\\!\\Big(\\beta_2 \\frac{R(t) - R_0}{R_{\\mathrm{amp}}}\\Big) + \\alpha_3 \\tanh\\!\\Big(\\beta_3 \\frac{A(t) - A_0}{A_{\\mathrm{amp}}}\\Big) - \\chi \\big(R(t) + S(t) + A(t)\\big)\n  $$\n  Note that the arguments of the $\\tanh$ functions simplify. For instance, $\\frac{S(t) - S_0}{S_{\\mathrm{amp}}} = \\frac{(S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)) - S_0}{S_{\\mathrm{amp}}} = \\cos(\\omega t)$. The full expression for $C_\\theta(t)$ becomes:\n  $$\n  C_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh(\\beta_1 \\cos(\\omega t)) + \\alpha_2 \\tanh(\\beta_2 \\sin(\\omega t)) + \\alpha_3 \\tanh(\\beta_3 \\sin(2\\omega t)) - \\chi \\big(R(t) + S(t) + A(t)\\big)\n  $$\n\n- **Compute Residual**: Compute the time series of the residual $\\varepsilon(t) = C_{\\mathrm{true}}(t) - C_\\theta(t)$.\n\n- **Calculate Mean Residual**: Compute the time-average of the residual series:\n  $$\n  \\bar{\\varepsilon} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\varepsilon(t)\n  $$\n  This value will be rounded to three decimal places.\n\n- **Calculate Statistical Significance**:\n  1.  Compute the standard error of the mean, $\\mathrm{SE}(\\bar{\\varepsilon})$, using the given formula and uncertainty values ($\\sigma_R, \\sigma_S, \\sigma_A, \\sigma_{\\partial H}, \\sigma_C$):\n      $$\n      \\mathrm{SE}(\\bar{\\varepsilon}) = \\sqrt{\\frac{\\sigma_R^2 + \\sigma_S^2 + \\sigma_A^2 + \\sigma_{\\partial H}^2 + \\sigma_C^2}{N}}\n      $$\n  2.  Calculate the $z$-score:\n      $$\n      z = \\frac{|\\bar{\\varepsilon}|}{\\mathrm{SE}(\\bar{\\varepsilon})}\n      $$\n  3.  Determine if the result is statistically significant by comparing the $z$-score to the critical value for a two-sided $95\\%$ confidence interval, which is $z_{0.95} = 1.96$:\n      $$\n      \\text{is\\_significant} = (z > 1.96)\n      $$\n\nThis procedure is applied to each of the four test cases specified in the problem statement to generate the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the residual energy imbalance and its statistical significance\n    for a proposed convective parameterization across multiple test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general diurnal variability, moderate uncertainties)\n        {\n            \"N\": 24, \"R_0\": -50, \"R_amp\": 15, \"S_0\": 100, \"S_amp\": 25, \"A_0\": 10, \"A_amp\": 8,\n            \"D_0\": 3, \"D_amp\": 2, \"phi\": np.pi/6,\n            \"params\": {\"alpha_0\": 0, \"alpha_1\": 8, \"beta_1\": 1.0, \"alpha_2\": -5, \"beta_2\": 1.2,\n                       \"alpha_3\": 2, \"beta_3\": 0.8, \"chi\": 0.92},\n            \"sigmas\": {\"R\": 3.0, \"S\": 5.0, \"A\": 2.0, \"dH\": 1.0, \"C\": 4.0}\n        },\n        # Case 2 (large uncertainties, near-balanced closure)\n        {\n            \"N\": 12, \"R_0\": -30, \"R_amp\": 12, \"S_0\": 80, \"S_amp\": 20, \"A_0\": 5, \"A_amp\": 4,\n            \"D_0\": 0.5, \"D_amp\": 1.0, \"phi\": np.pi/3,\n            \"params\": {\"alpha_0\": 0, \"alpha_1\": 5, \"beta_1\": 0.7, \"alpha_2\": -3, \"beta_2\": 1.1,\n                       \"alpha_3\": 1, \"beta_3\": 0.5, \"chi\": 0.99},\n            \"sigmas\": {\"R\": 10.0, \"S\": 12.0, \"A\": 6.0, \"dH\": 8.0, \"C\": 10.0}\n        },\n        # Case 3 (strong closure bias, small uncertainties)\n        {\n            \"N\": 60, \"R_0\": -40, \"R_amp\": 20, \"S_0\": 120, \"S_amp\": 30, \"A_0\": 15, \"A_amp\": 10,\n            \"D_0\": 2, \"D_amp\": 3, \"phi\": np.pi/4,\n            \"params\": {\"alpha_0\": 10, \"alpha_1\": -2, \"beta_1\": 1.5, \"alpha_2\": 4, \"beta_2\": 0.9,\n                       \"alpha_3\": -1, \"beta_3\": 1.2, \"chi\": 0.75},\n            \"sigmas\": {\"R\": 2.0, \"S\": 3.0, \"A\": 2.0, \"dH\": 1.0, \"C\": 2.0}\n        },\n        # Case 4 (small sample, moderate uncertainties)\n        {\n            \"N\": 10, \"R_0\": -35, \"R_amp\": 10, \"S_0\": 90, \"S_amp\": 15, \"A_0\": 8, \"A_amp\": 6,\n            \"D_0\": 1, \"D_amp\": 1.5, \"phi\": np.pi/8,\n            \"params\": {\"alpha_0\": 0, \"alpha_1\": 1, \"beta_1\": 1.0, \"alpha_2\": 1, \"beta_2\": 1.0,\n                       \"alpha_3\": 1, \"beta_3\": 1.0, \"chi\": 0.98},\n            \"sigmas\": {\"R\": 4.0, \"S\": 6.0, \"A\": 3.0, \"dH\": 2.0, \"C\": 5.0}\n        }\n    ]\n\n    results = []\n    z_critical = 1.96\n\n    for case in test_cases:\n        # Unpack parameters\n        N = case[\"N\"]\n        R_0, R_amp = case[\"R_0\"], case[\"R_amp\"]\n        S_0, S_amp = case[\"S_0\"], case[\"S_amp\"]\n        A_0, A_amp = case[\"A_0\"], case[\"A_amp\"]\n        D_0, D_amp, phi = case[\"D_0\"], case[\"D_amp\"], case[\"phi\"]\n        \n        p = case[\"params\"]\n        alpha_0, alpha_1, beta_1 = p[\"alpha_0\"], p[\"alpha_1\"], p[\"beta_1\"]\n        alpha_2, beta_2 = p[\"alpha_2\"], p[\"beta_2\"]\n        alpha_3, beta_3 = p[\"alpha_3\"], p[\"beta_3\"]\n        chi = p[\"chi\"]\n        \n        s = case[\"sigmas\"]\n        sigma_R, sigma_S, sigma_A, sigma_dH, sigma_C = s[\"R\"], s[\"S\"], s[\"A\"], s[\"dH\"], s[\"C\"]\n\n        # Time vector and angular frequency\n        t = np.arange(N)\n        omega = 2 * np.pi / N\n\n        # 1. Generate forcing time series\n        R_t = R_0 + R_amp * np.sin(omega * t)\n        S_t = S_0 + S_amp * np.cos(omega * t)\n        A_t = A_0 + A_amp * np.sin(2 * omega * t)\n        \n        # 2. Compute C_true(t)\n        total_forcing = R_t + S_t + A_t\n        C_true_t = -total_forcing + D_0 + D_amp * np.sin(omega * t + phi)\n\n        # 3. Compute C_theta(t)\n        # Arguments for tanh simplify based on definitions of R(t), S(t), A(t)\n        term_S = alpha_1 * np.tanh(beta_1 * np.cos(omega * t))\n        term_R = alpha_2 * np.tanh(beta_2 * np.sin(omega * t))\n        term_A = alpha_3 * np.tanh(beta_3 * np.sin(2 * omega * t))\n        \n        C_theta_t = alpha_0 + term_S + term_R + term_A - chi * total_forcing\n        \n        # 4. Compute residual epsilon(t) and its mean\n        epsilon_t = C_true_t - C_theta_t\n        epsilon_bar = np.mean(epsilon_t)\n\n        # 5. Calculate statistical significance\n        # Sum of variances\n        variance_sum = sigma_R**2 + sigma_S**2 + sigma_A**2 + sigma_dH**2 + sigma_C**2\n        # Standard error of the mean\n        se_epsilon_bar = np.sqrt(variance_sum / N)\n        \n        # z-score\n        if se_epsilon_bar > 0:\n            z_score = np.abs(epsilon_bar) / se_epsilon_bar\n        else: # Avoid division by zero, though unlikely with given inputs\n            z_score = np.inf if np.abs(epsilon_bar) > 0 else 0\n            \n        # Significance check\n        is_significant = z_score > z_critical\n\n        # Append formatted result\n        results.append([round(epsilon_bar, 3), is_significant])\n        \n    # Format final output string\n    print(str(results).replace(\" \", \"\").replace(\"'\", \"\"))\n\nsolve()\n```"
        }
    ]
}