{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any deep learning workflow is preparing the data. This practice addresses the challenge of handling input and output variables with different physical units and scales, a common scenario in Earth system modeling. By deriving the normalization matrices that ensure balanced gradients, you will gain a fundamental understanding of how to set up a neural network for stable and efficient training, preventing certain variables from dominating the learning process .",
            "id": "3873665",
            "problem": "A research group is developing a deep-learning-based parameterization for subgrid tendencies in a general circulation model for the atmosphere. The parameterization takes a state vector of resolved variables and predicts a vector of subgrid source terms. Let the input be the resolved state vector $x \\in \\mathbb{R}^{n}$, composed of physically heterogeneous quantities (for example, temperature in $\\mathrm{K}$, specific humidity in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}$, and wind components in $\\mathrm{m}\\,\\mathrm{s}^{-1}$), and let the target be the subgrid tendency vector $y \\in \\mathbb{R}^{m}$ (for example, heating rate in $\\mathrm{K}\\,\\mathrm{s}^{-1}$ and moistening rate in $\\mathrm{kg}\\,\\mathrm{kg}^{-1}\\,\\mathrm{s}^{-1}$). Assume $x$ and $y$ are jointly zero-mean random vectors, and define their covariance matrices by $\\Sigma_{x} = \\mathbb{E}[x x^{\\top}] \\in \\mathbb{R}^{n \\times n}$ and $\\Sigma_{y} = \\mathbb{E}[y y^{\\top}] \\in \\mathbb{R}^{m \\times m}$, both symmetric positive definite.\n\nTo mitigate unit and scale disparities and obtain balanced gradient magnitudes across all learnable parameters and across variables with different physical units, the team decides to normalize both inputs and targets before training. Consider a local linearization of the network around initialization that maps normalized inputs to normalized outputs: the model output in normalized space is $\\hat{\\tilde{y}} = W \\tilde{x}$, where $\\tilde{x} = A x$ and $\\tilde{y} = B y$ for some invertible normalization matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$ to be chosen. The training objective is the dimensionless Mean Squared Error (MSE) in normalized space,\n$$\nL(W) = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\,\\|\\hat{\\tilde{y}} - \\tilde{y}\\|_{2}^{2}\\,\\right] = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\,\\|W A x - B y\\|_{2}^{2}\\,\\right].\n$$\n\nAssume that, at initialization, $W = 0$, and that $x$ and $y$ are independent (consistent with an uninformative initial model). Define balanced gradients precisely as the requirement that the covariance of the per-sample gradient with respect to $W$ is isotropic at initialization; that is, if $g(x,y) = \\nabla_{W} L(W)\\big|_{W=0}$ denotes the gradient for a single sample, then\n$$\n\\mathrm{Cov}\\!\\left(\\mathrm{vec}\\!\\left(g(x,y)\\right)\\right) = c\\,I_{mn}\n$$\nfor some scalar $c > 0$, where $\\mathrm{vec}(\\cdot)$ vectorizes a matrix by stacking its columns and $I_{mn}$ is the $mn \\times mn$ identity matrix.\n\nStarting only from the definitions above, derive the normalization matrices $A$ and $B$ that achieve balanced gradients at initialization. Express your final answer as a closed-form analytic expression in terms of $\\Sigma_{x}$ and $\\Sigma_{y}$. No numerical evaluation or rounding is required, and no physical units are to be included in the final expression.",
            "solution": "The user wants me to derive the normalization matrices $A$ and $B$ that ensure the covariance of the per-sample gradient is isotropic at initialization.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- Input state vector: $x \\in \\mathbb{R}^{n}$.\n- Target tendency vector: $y \\in \\mathbb{R}^{m}$.\n- Mean of inputs and targets: $\\mathbb{E}[x] = 0$, $\\mathbb{E}[y] = 0$.\n- Input covariance matrix: $\\Sigma_{x} = \\mathbb{E}[x x^{\\top}] \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite.\n- Target covariance matrix: $\\Sigma_{y} = \\mathbb{E}[y y^{\\top}] \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite.\n- Normalization matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$ are invertible.\n- Normalized input: $\\tilde{x} = A x$.\n- Normalized target: $\\tilde{y} = B y$.\n- Linearized model prediction: $\\hat{\\tilde{y}} = W \\tilde{x}$.\n- Training objective (expected loss): $L(W) = \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\,\\|W A x - B y\\|_{2}^{2}\\,\\right]$.\n- Initialization of weights: $W = 0$.\n- Statistical independence at initialization: $x$ and $y$ are independent.\n- Per-sample gradient at initialization: $g(x,y) = \\nabla_{W} l(W;x,y)\\big|_{W=0}$, where $l(W;x,y) = \\frac{1}{2}\\|W A x - B y\\|_{2}^{2}$ is the loss for a single sample pair $(x,y)$.\n- Balanced gradients condition: $\\mathrm{Cov}\\!\\left(\\mathrm{vec}\\!\\left(g(x,y)\\right)\\right) = c\\,I_{mn}$ for some scalar constant $c > 0$, where $\\mathrm{vec}(\\cdot)$ vectorizes a matrix and $I_{mn}$ is the identity matrix of size $mn \\times mn$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the field of machine learning for Earth system modeling. The setup is a mathematically precise and well-posed problem involving matrix calculus and statistics. All terms are clearly defined, and the assumptions (zero-mean data, positive definite covariance, independence at initialization) are standard in such analyses. The goal is to derive expressions for $A$ and $B$, which is a solvable task based on the givens. There are no contradictions, ambiguities, or factual errors. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the derivation.\n\nThe core of the problem is to find the matrices $A$ and $B$ that satisfy the balanced gradients condition. The procedure is as follows:\n$1$. Compute the per-sample gradient $g(x,y)$.\n$2$. Compute the covariance matrix of the vectorized gradient, $\\mathrm{Cov}(\\mathrm{vec}(g(x,y)))$.\n$3$. Apply the balanced gradients condition to derive equations for $A$ and $B$.\n$4$. Solve these equations to find the expressions for $A$ and $B$.\n\n**1. Compute the per-sample gradient**\nThe loss for a single sample $(x, y)$ is $l(W; x, y) = \\frac{1}{2}\\|W A x - B y\\|_{2}^{2}$. We can write the squared norm as a trace:\n$$\nl(W; x, y) = \\frac{1}{2}\\mathrm{Tr}\\left( (WAx - By)^{\\top} (WAx - By) \\right)\n$$\nTo find the gradient with respect to the matrix $W$, we compute the differential $dl$:\n$$\ndl = \\mathrm{Tr}\\left( (WAx - By)^{\\top} d(WAx - By) \\right) = \\mathrm{Tr}\\left( (WAx - By)^{\\top} (dW A x) \\right)\n$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(XYZ) = \\mathrm{Tr}(ZXY)$:\n$$\ndl = \\mathrm{Tr}\\left( (Ax) (WAx - By)^{\\top} dW \\right)\n$$\nFrom the definition of the matrix derivative, $dl = \\mathrm{Tr}( (\\nabla_W l)^{\\top} dW )$, we can identify the gradient:\n$$\n\\nabla_W l(W; x, y) = \\left((Ax) (WAx - By)^{\\top}\\right)^{\\top} = (WAx - By)(Ax)^{\\top}\n$$\nThe problem specifies the gradient $g(x,y)$ at initialization, where $W=0$:\n$$\ng(x,y) = \\nabla_W l(W; x, y)\\big|_{W=0} = (0 \\cdot Ax - By)(Ax)^{\\top} = -By(Ax)^{\\top} = -B y x^{\\top} A^{\\top}\n$$\nThis is an $m \\times n$ matrix.\n\n**2. Compute the covariance of the gradient**\nWe need to find $\\mathrm{Cov}(\\mathrm{vec}(g(x,y)))$. First, we compute the mean of the gradient. Since $x$ and $y$ are independent and zero-mean:\n$$\n\\mathbb{E}[g(x,y)] = \\mathbb{E}[-B y x^{\\top} A^{\\top}] = -B \\, \\mathbb{E}[y] \\, \\mathbb{E}[x^{\\top}] \\, A^{\\top} = -B \\cdot 0 \\cdot 0^{\\top} \\cdot A^{\\top} = 0\n$$\nThe mean of the gradient is the zero matrix. Therefore, the covariance of its vectorized form is given by:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}[\\mathrm{vec}(g) \\mathrm{vec}(g)^{\\top}]\n$$\nTo vectorize $g(x,y)$, we use the identity for the outer product of two vectors $u$ and $v$: $\\mathrm{vec}(uv^{\\top}) = v \\otimes u$. In our case, $g(x,y) = (-By)(A x)^{\\top}$. Let $u = -By$ and $v = Ax$. Then:\n$$\n\\mathrm{vec}(g(x,y)) = (Ax) \\otimes (-By) = -(Ax \\otimes By)\n$$\nNow we compute the covariance matrix:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}\\left[ \\left(-(Ax \\otimes By)\\right) \\left(-(Ax \\otimes By)\\right)^{\\top} \\right] \\\\\n= \\mathbb{E}\\left[ (Ax \\otimes By) (Ax \\otimes By)^{\\top} \\right]\n$$\nUsing the identity $(a \\otimes b)^{\\top} = a^{\\top} \\otimes b^{\\top}$, we have:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}\\left[ (Ax \\otimes By) ((Ax)^{\\top} \\otimes (By)^{\\top}) \\right]\n$$\nUsing the mixed-product property $(a \\otimes b)(c \\otimes d) = (ac) \\otimes (bd)$:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}\\left[ ((Ax)(Ax)^{\\top}) \\otimes ((By)(By)^{\\top}) \\right] \\\\\n= \\mathbb{E}\\left[ (Axx^{\\top}A^{\\top}) \\otimes (Byy^{\\top}B^{\\top}) \\right]\n$$\nSince $x$ and $y$ are independent, the expectation of the Kronecker product is the Kronecker product of the expectations:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = \\mathbb{E}[Axx^{\\top}A^{\\top}] \\otimes \\mathbb{E}[Byy^{\\top}B^{\\top}] \\\\\n= A\\,\\mathbb{E}[xx^{\\top}]A^{\\top} \\otimes B\\,\\mathbb{E}[yy^{\\top}]B^{\\top}\n$$\nSubstituting the definitions of the covariance matrices $\\Sigma_x = \\mathbb{E}[xx^{\\top}]$ and $\\Sigma_y = \\mathbb{E}[yy^{\\top}]$:\n$$\n\\mathrm{Cov}(\\mathrm{vec}(g)) = (A \\Sigma_x A^{\\top}) \\otimes (B \\Sigma_y B^{\\top})\n$$\n\n**3. Apply the balanced gradients condition**\nThe problem requires this covariance to be isotropic:\n$$\n(A \\Sigma_x A^{\\top}) \\otimes (B \\Sigma_y B^{\\top}) = c I_{mn}\n$$\nfor some scalar $c > 0$. The identity matrix can be written as a Kronecker product $I_{mn} = I_n \\otimes I_m$.\n$$\n(A \\Sigma_x A^{\\top}) \\otimes (B \\Sigma_y B^{\\top}) = c (I_n \\otimes I_m)\n$$\nFor the Kronecker product of two matrices to equal a scaled Kronecker product of two other matrices of the same dimensions, the corresponding matrices must be proportional. Thus, there must exist a non-zero scalar $\\lambda$ such that:\n$$\nA \\Sigma_x A^{\\top} = \\lambda I_n \\\\\nB \\Sigma_y B^{\\top} = \\frac{c}{\\lambda} I_m\n$$\nSince $\\Sigma_x$ and $\\Sigma_y$ are positive definite and $A, B$ are invertible, the matrices on the left are positive definite. This implies that $\\lambda > 0$ and $c/\\lambda > 0$.\n\n**4. Solve for A and B**\nThe constants $\\lambda$ and $c$ affect the overall magnitude of the gradients, which can be absorbed into the learning rate of the optimization algorithm. A canonical choice, corresponding to whitening the variables, is to set these scaling factors to $1$. We choose $\\lambda = 1$ and $c=1$, which simplifies the conditions to:\n$$\n(1) \\quad A \\Sigma_x A^{\\top} = I_n \\\\\n(2) \\quad B \\Sigma_y B^{\\top} = I_m\n$$\nLet's solve equation (1) for $A$. Since $\\Sigma_x$ is symmetric positive definite (SPD), it has a unique SPD square root $\\Sigma_x^{1/2}$ and a unique SPD inverse square root $\\Sigma_x^{-1/2} = (\\Sigma_x^{1/2})^{-1}$. Let's propose $A = \\Sigma_x^{-1/2}$ as a solution. This matrix is also SPD.\nSubstituting this into the left side of equation (1):\n$$\nA \\Sigma_x A^{\\top} = \\Sigma_x^{-1/2} \\Sigma_x (\\Sigma_x^{-1/2})^{\\top}\n$$\nSince $\\Sigma_x^{-1/2}$ is symmetric, $(\\Sigma_x^{-1/2})^{\\top} = \\Sigma_x^{-1/2}$.\n$$\n\\Sigma_x^{-1/2} \\Sigma_x \\Sigma_x^{-1/2} = \\Sigma_x^{-1/2} (\\Sigma_x^{1/2} \\Sigma_x^{1/2}) \\Sigma_x^{-1/2} = (\\Sigma_x^{-1/2} \\Sigma_x^{1/2}) (\\Sigma_x^{1/2} \\Sigma_x^{-1/2}) = I_n \\cdot I_n = I_n\n$$\nThis matches the right side of equation (1). Thus, $A = \\Sigma_x^{-1/2}$ is a valid solution. This is the unique symmetric positive definite solution for $A$. Any other solution would be of the form $O_A \\Sigma_x^{-1/2}$ for an arbitrary orthogonal matrix $O_A$, which corresponds to an arbitrary rotation of the whitened data. The problem asks for \"the\" matrices, implying a canonical choice, which is the symmetric positive definite one.\n\nBy the exact same reasoning for equation (2), the canonical solution for $B$ is:\n$$\nB = \\Sigma_y^{-1/2}\n$$\nThese matrices $A$ and $B$ are the inverse principal square roots of their respective covariance matrices. They are well-defined because $\\Sigma_x$ and $\\Sigma_y$ are symmetric positive definite. They are also invertible, as required.\n\nThe derived matrices $A$ and $B$ correspond to a standard whitening transformation, which decorrelates the components of the input and target vectors and scales them to have unit variance.",
            "answer": "$$ \\boxed{A = \\Sigma_{x}^{-1/2}, \\quad B = \\Sigma_{y}^{-1/2}} $$"
        },
        {
            "introduction": "Supervised learning requires a \"ground truth\" target, but in parameterization discovery, this target is often not directly observable and must be inferred. This exercise delves into a practical method for constructing a surrogate subgrid tendency from data assimilation increments. You will derive and analyze the bias and noise introduced by the observation system, providing critical insight into the quality and limitations of the data used to train your models .",
            "id": "3873624",
            "problem": "Consider a linear-Gaussian Data Assimilation (DA) setting for an environmental dynamical system with state vector $x \\in \\mathbb{R}^n$, where the forecast model omits an unknown subgrid tendency $s \\in \\mathbb{R}^n$ that is approximately constant over a short assimilation window of length $\\Delta t$. Observations $y \\in \\mathbb{R}^m$ are related to the state by a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ and additive, zero-mean observation error $\\varepsilon \\sim \\mathcal{N}(0, R)$ with covariance $R \\in \\mathbb{R}^{m \\times m}$. The Kalman Filter (KF) analysis update is given by the well-tested formula $x^a = x^b + K\\left(y - H x^b\\right)$, where the Kalman gain $K \\in \\mathbb{R}^{n \\times m}$ equals $K = P_b H^\\top \\left( H P_b H^\\top + R \\right)^{-1}$ with $P_b \\in \\mathbb{R}^{n \\times n}$ the background-error covariance. The assimilation increment is $\\delta x = x^a - x^b$. A common surrogate target for subgrid tendency discovery in deep learning is the increment-based tendency $\\widehat{q} = \\delta x / \\Delta t$.\n\nStarting only from the preceding linear-Gaussian DA definitions, from the definition of the innovation $d = y - H x^b$, the assumption that the true model state misfit over one window is dominated by the missing constant subgrid tendency $s$ so that $x^{\\text{true}} - x^b \\approx \\Delta t \\, s$ in expectation, and from the properties of multivariate normal random variables, derive principled expressions for:\n- the expected surrogate mean $\\mathbb{E}[\\widehat{q}]$ in model space,\n- the bias vector $b = \\mathbb{E}[\\widehat{q}] - s$ induced by the observation operator and gain,\n- the surrogate noise covariance $\\Sigma_{\\widehat{q}} = \\mathrm{Cov}(\\widehat{q})$ implied by observation error,\nexpressed only in terms of $H$, $R$, $P_b$, $\\Delta t$, and $s$.\n\nThen implement these expressions in a program to compute, for each of the following test cases, the tuple consisting of:\n- the expected surrogate mean vector components in model space, listed as three floats $[\\widehat{q}_1,\\widehat{q}_2,\\widehat{q}_3]$ in the base units of the state per second,\n- the Euclidean norm of the bias vector $\\lVert b \\rVert_2$ in the base units of the state per second,\n- the trace of $\\Sigma_{\\widehat{q}}$ (that is, $\\mathrm{tr}(\\Sigma_{\\widehat{q}})$) in the squared base units of the state per second squared,\n- a boolean indicating whether the bias norm is effectively zero under a strict tolerance $\\tau = 10^{-9}$ (true if $\\lVert b \\rVert_2 \\le \\tau$, false otherwise).\n\nAll computations must be performed in exact matrix form, with all inverses well-defined for the given cases. Angles are not involved. Express all rate quantities in units of the state per second, and all covariance traces in squared state units per second squared. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list with no spaces inside the brackets, for example, $[[\\widehat{q}_1,\\widehat{q}_2,\\widehat{q}_3,\\lVert b \\rVert_2,\\mathrm{tr}(\\Sigma_{\\widehat{q}}),\\mathrm{boolean}],\\ldots]$.\n\nUse the following test suite with state dimension $n = 3$ and the same true subgrid tendency vector for all cases:\n- True subgrid tendency $s \\in \\mathbb{R}^3$: \n$$\ns = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.05 \\end{bmatrix}\\ \\text{(state units per second)}.\n$$\n- Case $1$ (happy path, full observation, heterogeneous observation noise):\n$$\nP_b = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix},\\quad\nH = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix},\\quad\nR = \\begin{bmatrix} 0.01 & 0 & 0 \\\\ 0 & 0.5 & 0 \\\\ 0 & 0 & 10.0 \\end{bmatrix},\\quad\n\\Delta t = 600\\ \\text{s}.\n$$\n- Case $2$ (boundary: partial observation of components $1$ and $3$ only):\n$$\nP_b = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix},\\quad\nH = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix},\\quad\nR = \\begin{bmatrix} 0.04 & 0 \\\\ 0 & 0.04 \\end{bmatrix},\\quad\n\\Delta t = 600\\ \\text{s}.\n$$\n- Case $3$ (edge: smoothing observation operator and short window):\n$$\nP_b = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix},\\quad\nH = \\begin{bmatrix} 0.5 & 0.5 & 0.0 \\\\ 0.0 & 0.5 & 0.5 \\end{bmatrix},\\quad\nR = \\begin{bmatrix} 0.09 & 0 \\\\ 0 & 0.09 \\end{bmatrix},\\quad\n\\Delta t = 60\\ \\text{s}.\n$$\n- Case $4$ (near-perfect full observation as a bias-baseline):\n$$\nP_b = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 4 \\end{bmatrix},\\quad\nH = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix},\\quad\nR = \\begin{bmatrix} 10^{-8} & 0 & 0 \\\\ 0 & 10^{-8} & 0 \\\\ 0 & 0 & 10^{-8} \\end{bmatrix},\\quad\n\\Delta t = 600\\ \\text{s}.\n$$\n\nYour task:\n- Derive from first principles the model-space surrogate expressions requested above.\n- Implement a program that, for each test case, computes and returns the list $[\\widehat{q}_1,\\widehat{q}_2,\\widehat{q}_3,\\lVert b \\rVert_2,\\mathrm{tr}(\\Sigma_{\\widehat{q}}),\\mathrm{boolean}]$, and finally prints a single line containing the aggregate list of all four test cases in the exact format $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$ with commas and no spaces.",
            "solution": "The problem requires the derivation of key statistical properties of a surrogate model error tendency, $\\widehat{q}$, derived from a linear-Gaussian data assimilation framework. We will first validate the problem statement and then proceed with the derivations and implementation.\n\n### Step 1: Extract Givens\n\n- **State and Tendency:** State vector $x \\in \\mathbb{R}^n$, unknown constant subgrid tendency $s \\in \\mathbb{R}^n$.\n- **Assimilation Window:** Length $\\Delta t$.\n- **Observations:** Observation vector $y \\in \\mathbb{R}^m$, linear observation operator $H \\in \\mathbb{R}^{m \\times n}$.\n- **Observation Error:** Additive, zero-mean Gaussian error $\\varepsilon \\sim \\mathcal{N}(0, R)$ with covariance $R \\in \\mathbb{R}^{m \\times m}$.\n- **Data Assimilation (DA) Formulas:**\n    - Background-error covariance: $P_b \\in \\mathbb{R}^{n \\times n}$.\n    - Kalman gain: $K = P_b H^\\top \\left( H P_b H^\\top + R \\right)^{-1}$.\n    - Analysis update: $x^a = x^b + K\\left(y - H x^b\\right)$.\n    - Assimilation increment: $\\delta x = x^a - x^b$.\n    - Innovation: $d = y - H x^b$.\n- **Surrogate Tendency:** $\\widehat{q} = \\delta x / \\Delta t$.\n- **Core Assumption:** The expected background state error is attributed to the missing tendency: $\\mathbb{E}[x^{\\text{true}} - x^b] \\approx \\Delta t \\, s$. For the derivation, this is treated as an equality.\n- **Derivation Objectives:** Expressions for the expected surrogate mean $\\mathbb{E}[\\widehat{q}]$, the bias $b = \\mathbb{E}[\\widehat{q}] - s$, and the surrogate noise covariance $\\Sigma_{\\widehat{q}} = \\mathrm{Cov}(\\widehat{q})$ due to observation error, all in terms of $H, R, P_b, \\Delta t, s$.\n- **Numerical Cases:** Four test cases are provided with specific matrices $P_b, H, R$ and scalars $\\Delta t$. The state dimension is $n=3$, and the true tendency is $s = [0.2, -0.1, 0.05]^\\top$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is **valid**.\n- **Scientifically Grounded:** The entire problem is formulated within the standard and well-established mathematical framework of the Kalman Filter, a cornerstone of data assimilation in the environmental sciences. The concept of using assimilation increments to infer model error is a recognized technique.\n- **Well-Posed:** The problem provides all necessary definitions, equations, and data to derive the requested expressions and compute the numerical results. It specifies that all matrix inverses are well-defined, ensuring a unique solution exists for each case. The dimensions of all matrices and vectors are consistent across all test cases.\n- **Objective:** The problem is stated using precise, unambiguous mathematical and technical language, free from any subjectivity.\n\n### Step 3: Derivation from First Principles\n\nWe proceed with the derivation of the required expressions based on the provided definitions and properties of multivariate random variables.\n\n**1. Expected Surrogate Mean $\\mathbb{E}[\\widehat{q}]$**\n\nThe surrogate tendency is defined as $\\widehat{q} = \\delta x / \\Delta t$. The assimilation increment is $\\delta x = x^a - x^b$. Using the analysis update equation, we have:\n$$\n\\delta x = K(y - Hx^b) = K d\n$$\nTherefore, the surrogate tendency can be written in terms of the innovation $d$:\n$$\n\\widehat{q} = \\frac{K d}{\\Delta t}\n$$\nTo find its expectation, we take the expectation of this expression. The Kalman gain $K$ and $\\Delta t$ are deterministic quantities.\n$$\n\\mathbb{E}[\\widehat{q}] = \\mathbb{E}\\left[\\frac{K d}{\\Delta t}\\right] = \\frac{K}{\\Delta t} \\mathbb{E}[d]\n$$\nNext, we find the expectation of the innovation, $d = y - Hx^b$. The observation $y$ is related to the true state $x^{\\text{true}}$ by $y = H x^{\\text{true}} + \\varepsilon$.\n$$\nd = (H x^{\\text{true}} + \\varepsilon) - Hx^b = H(x^{\\text{true}} - x^b) + \\varepsilon\n$$\nTaking the expectation and using the linearity of the expectation operator:\n$$\n\\mathbb{E}[d] = \\mathbb{E}[H(x^{\\text{true}} - x^b) + \\varepsilon] = H \\mathbb{E}[x^{\\text{true}} - x^b] + \\mathbb{E}[\\varepsilon]\n$$\nFrom the problem statement, we use the key assumption $\\mathbb{E}[x^{\\text{true}} - x^b] = \\Delta t \\, s$ and the fact that observation error is zero-mean, $\\mathbb{E}[\\varepsilon] = 0$.\n$$\n\\mathbb{E}[d] = H(\\Delta t \\, s) + 0 = \\Delta t \\, H s\n$$\nSubstituting this back into the expression for $\\mathbb{E}[\\widehat{q}]$:\n$$\n\\mathbb{E}[\\widehat{q}] = \\frac{K}{\\Delta t} (\\Delta t \\, H s) = K H s\n$$\nThus, the expected surrogate mean is:\n$$\n\\mathbb{E}[\\widehat{q}] = K H s = \\left( P_b H^\\top \\left( H P_b H^\\top + R \\right)^{-1} \\right) H s\n$$\n\n**2. Bias Vector $b$**\n\nThe bias is defined as the difference between the expected surrogate tendency and the true tendency $s$.\n$$\nb = \\mathbb{E}[\\widehat{q}] - s\n$$\nSubstituting our result for $\\mathbb{E}[\\widehat{q}]$:\n$$\nb = K H s - s = (K H - I) s\n$$\nwhere $I$ is the $n \\times n$ identity matrix. This expression reveals that the surrogate is an unbiased estimator of $s$ (i.e., $b=0$) if and only if $K H = I$ for any $s \\neq 0$. This condition is generally not met, especially when observations are incomplete ($H$ is not the identity) or noisy ($R$ is not the zero matrix).\n\n**3. Surrogate Noise Covariance $\\Sigma_{\\widehat{q}}$**\n\nThe surrogate noise covariance $\\Sigma_{\\widehat{q}}$ is the covariance of $\\widehat{q}$, defined as $\\Sigma_{\\widehat{q}} = \\mathrm{Cov}(\\widehat{q})$. The problem specifically asks for the covariance \"implied by observation error.\" This instructs us to treat the randomness as originating solely from the observation error term $\\varepsilon$, while other quantities are considered fixed at their expected values for this part of the analysis.\nWe start with the expression for $\\widehat{q}$:\n$$\n\\widehat{q} = \\frac{K}{\\Delta t} d = \\frac{K}{\\Delta t} (H(x^{\\text{true}} - x^b) + \\varepsilon)\n$$\nDistributing the terms:\n$$\n\\widehat{q} = \\frac{K H (x^{\\text{true}} - x^b)}{\\Delta t} + \\frac{K \\varepsilon}{\\Delta t}\n$$\nFollowing the problem's constraint, we find the covariance by treating the first term as a constant (since it does not depend on $\\varepsilon$). The covariance of a random variable plus a constant is just the covariance of the random variable.\n$$\n\\Sigma_{\\widehat{q}} = \\mathrm{Cov}(\\widehat{q}) = \\mathrm{Cov}\\left( \\frac{K H (x^{\\text{true}} - x^b)}{\\Delta t} + \\frac{K \\varepsilon}{\\Delta t} \\right) = \\mathrm{Cov}\\left( \\frac{K \\varepsilon}{\\Delta t} \\right)\n$$\nUsing the property of covariance, $\\mathrm{Cov}(A X) = A \\mathrm{Cov}(X) A^\\top$, with $A = K / \\Delta t$ and $X = \\varepsilon$:\n$$\n\\Sigma_{\\widehat{q}} = \\left(\\frac{K}{\\Delta t}\\right) \\mathrm{Cov}(\\varepsilon) \\left(\\frac{K}{\\Delta t}\\right)^\\top\n$$\nSince $\\mathrm{Cov}(\\varepsilon) = R$, we arrive at the final expression for the surrogate noise covariance:\n$$\n\\Sigma_{\\widehat{q}} = \\frac{1}{\\Delta t^2} K R K^\\top\n$$\n\nThese derived expressions will now be implemented to solve the numerical test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes properties of a surrogate subgrid tendency from a \n    linear-Gaussian data assimilation framework for several test cases.\n    \"\"\"\n\n    # True subgrid tendency vector s (n=3)\n    s = np.array([0.2, -0.1, 0.05])\n    \n    # Bias norm tolerance\n    tau = 1e-9\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Full observation, heterogeneous noise\n        {\n            \"Pb\": np.array([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 4.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R\": np.array([[0.01, 0.0, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 10.0]]),\n            \"dt\": 600.0\n        },\n        # Case 2: Partial observation (components 1 and 3)\n        {\n            \"Pb\": np.array([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 4.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R\": np.array([[0.04, 0.0], [0.0, 0.04]]),\n            \"dt\": 600.0\n        },\n        # Case 3: Smoothing observation operator, short window\n        {\n            \"Pb\": np.array([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 4.0]]),\n            \"H\": np.array([[0.5, 0.5, 0.0], [0.0, 0.5, 0.5]]),\n            \"R\": np.array([[0.09, 0.0], [0.0, 0.09]]),\n            \"dt\": 60.0\n        },\n        # Case 4: Near-perfect full observation\n        {\n            \"Pb\": np.array([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 4.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R\": np.array([[1e-8, 0.0, 0.0], [0.0, 1e-8, 0.0], [0.0, 0.0, 1e-8]]),\n            \"dt\": 600.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Pb, H, R, dt = case[\"Pb\"], case[\"H\"], case[\"R\"], case[\"dt\"]\n        \n        n = Pb.shape[0]\n        I = np.identity(n)\n\n        # 1. Compute Kalman Gain K = Pb H^T (H Pb H^T + R)^{-1}\n        H_Pb_Ht = H @ Pb @ H.T\n        S_inv = np.linalg.inv(H_Pb_Ht + R)\n        K = Pb @ H.T @ S_inv\n        \n        # 2. Compute expected surrogate mean E[q_hat] = K H s\n        q_hat_mean = K @ H @ s\n        \n        # 3. Compute bias vector b = (K H - I) s\n        b = (K @ H - I) @ s\n        \n        # 4. Compute Euclidean norm of the bias vector\n        norm_b = np.linalg.norm(b)\n        \n        # 5. Compute surrogate noise covariance Sigma_q_hat = (1/dt^2) K R K^T\n        Sigma_q_hat = (1 / (dt**2)) * (K @ R @ K.T)\n        \n        # 6. Compute trace of the surrogate noise covariance\n        trace_Sigma = np.trace(Sigma_q_hat)\n        \n        # 7. Check if bias norm is effectively zero\n        is_zero_bias = norm_b = tau\n        \n        # Assemble the results for the current case\n        case_results = [\n            *q_hat_mean.tolist(),\n            norm_b,\n            trace_Sigma,\n            is_zero_bias\n        ]\n        results.append(case_results)\n\n    # Format the final output string precisely as required\n    # e.g., [[q1,q2,q3,norm,trace,bool],[...]]\n    output_str_list = []\n    for res_list in results:\n        # Convert each item to string, joining with a comma\n        inner_str = \",\".join(map(str, res_list))\n        output_str_list.append(f\"[{inner_str}]\")\n    \n    final_output = f\"[{','.join(output_str_list)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A successful parameterization must not only be accurate in its local predictions but also respect global physical laws, such as the conservation of energy. This practice moves beyond simple error metrics to the crucial step of model evaluation within a physical context. By computing the energy imbalance introduced by a learned closure in a simplified atmospheric column, you will learn how to assess a model's physical consistency and determine if its residual errors are statistically significant .",
            "id": "3873646",
            "problem": "Consider a single-column moist static energy budget for an atmospheric column, where the column-integrated moist static energy is defined by $h = c_p T + L_v q + g z$ and the column integral is $H = \\int_0^{z_t} \\rho h \\, dz$, with $c_p$ the specific heat of air at constant pressure, $T$ the temperature, $L_v$ the latent heat of vaporization, $q$ the specific humidity, $g$ the gravitational acceleration, $\\rho$ the air density, and $z_t$ the top of the column. The conservation of energy for $H$ at the column scale obeys the budget equation\n$$\n\\frac{\\partial H}{\\partial t} = R + S + A + C + \\varepsilon,\n$$\nwhere $R$ is net radiative heating into the column, $S$ is net surface enthalpy flux into the column, $A$ is net horizontal advective import of moist static energy, $C$ is the subgrid convective moist static energy tendency, and $\\varepsilon$ is the residual imbalance due to diagnostics, discretization, or parameterization error. All flux terms $R$, $S$, $A$, and $C$ as well as $\\partial_t H$ are to be treated as column-integrated tendencies in units of $\\mathrm{W\\,m^{-2}}$.\n\nYou are given a proposed convective closure $C_\\theta(t)$ produced by a deep learning parameterization discovery model. The closure is approximated as a nonlinear mapping of available resolved-column diagnostics,\n$$\nC_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh\\!\\Big(\\beta_1 \\frac{S(t) - S_0}{S_{\\mathrm{amp}}}\\Big) + \\alpha_2 \\tanh\\!\\Big(\\beta_2 \\frac{R(t) - R_0}{R_{\\mathrm{amp}}}\\Big) + \\alpha_3 \\tanh\\!\\Big(\\beta_3 \\frac{A(t) - A_0}{A_{\\mathrm{amp}}}\\Big) - \\chi \\big(R(t) + S(t) + A(t)\\big),\n$$\nwhere $\\alpha_0$, $\\alpha_1$, $\\alpha_2$, $\\alpha_3$, $\\beta_1$, $\\beta_2$, $\\beta_3$, and $\\chi$ are closure parameters. You must compute $\\varepsilon(t)$ implied by this proposed closure and assess its statistical significance under explicit measurement and modeling uncertainties.\n\nFor the purposes of this problem, the resolved diagnostics follow deterministic periodic trajectories that are scientifically plausible for a diurnal cycle. For a given number of time steps $N$ and angular frequency $\\omega = 2\\pi/N$ (in radians), define for $t = 0, 1, \\dots, N-1$,\n- $R(t) = R_0 + R_{\\mathrm{amp}} \\sin(\\omega t)$,\n- $S(t) = S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)$,\n- $A(t) = A_0 + A_{\\mathrm{amp}} \\sin(2\\omega t)$.\n\nA physically consistent reference convective tendency is constructed to yield a column storage that is small compared to the fluxes, mimicking a near-steady column with mild variability:\n$$\nC_{\\mathrm{true}}(t) = -\\big(R(t) + S(t) + A(t)\\big) + D_0 + D_{\\mathrm{amp}} \\sin\\!\\big(\\omega t + \\phi\\big),\n$$\nwith a prescribed phase shift $\\phi$ (in radians). The diagnosed storage tendency is then\n$$\n\\frac{\\partial H}{\\partial t}(t) = R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t).\n$$\n\nGiven the uncertainties $\\sigma_R$, $\\sigma_S$, $\\sigma_A$, $\\sigma_{\\partial H}$, and $\\sigma_C$ (each in $\\mathrm{W\\,m^{-2}}$) for the respective diagnostics $R$, $S$, $A$, $\\partial_t H$, and the closure $C_\\theta$, assume independent, zero-mean Gaussian errors across time and terms. Under these assumptions, the standard error of the time-mean imbalance $\\bar{\\varepsilon} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\varepsilon(t)$ is\n$$\n\\mathrm{SE}(\\bar{\\varepsilon}) = \\sqrt{\\frac{\\sigma_R^2 + \\sigma_S^2 + \\sigma_A^2 + \\sigma_{\\partial H}^2 + \\sigma_C^2}{N}}.\n$$\nDefine the $z$-score $z = |\\bar{\\varepsilon}| / \\mathrm{SE}(\\bar{\\varepsilon})$ and assess statistical significance at the two-sided $95\\%$ confidence level using the Gaussian critical value $z_{0.95} = 1.96$. Report whether $z  1.96$.\n\nYour task is to implement a program that, for each test case, computes the time series $\\varepsilon(t) = \\frac{\\partial H}{\\partial t}(t) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)$, its time-mean $\\bar{\\varepsilon}$, the standard error $\\mathrm{SE}(\\bar{\\varepsilon})$, and determines the significance boolean $z  1.96$. Express the mean residual imbalance $\\bar{\\varepsilon}$ in $\\mathrm{W\\,m^{-2}}$, rounded to three decimal places. Angles (the phase $\\phi$ and trigonometric arguments) must be treated in radians.\n\nTest Suite:\nProvide results for the following four parameter sets. For each case, the program must use the values exactly as given.\n\n- Case $1$ (general diurnal variability, moderate uncertainties):\n  - $N = 24$, $R_0 = -50$, $R_{\\mathrm{amp}} = 15$, $S_0 = 100$, $S_{\\mathrm{amp}} = 25$, $A_0 = 10$, $A_{\\mathrm{amp}} = 8$, $D_0 = 3$, $D_{\\mathrm{amp}} = 2$, $\\phi = \\pi/6$, closure parameters: $\\alpha_0 = 0$, $\\alpha_1 = 8$, $\\beta_1 = 1.0$, $\\alpha_2 = -5$, $\\beta_2 = 1.2$, $\\alpha_3 = 2$, $\\beta_3 = 0.8$, $\\chi = 0.92$, uncertainties: $\\sigma_R = 3.0$, $\\sigma_S = 5.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 4.0$.\n- Case $2$ (large uncertainties, near-balanced closure):\n  - $N = 12$, $R_0 = -30$, $R_{\\mathrm{amp}} = 12$, $S_0 = 80$, $S_{\\mathrm{amp}} = 20$, $A_0 = 5$, $A_{\\mathrm{amp}} = 4$, $D_0 = 0.5$, $D_{\\mathrm{amp}} = 1.0$, $\\phi = \\pi/3$, closure parameters: $\\alpha_0 = 0$, $\\alpha_1 = 5$, $\\beta_1 = 0.7$, $\\alpha_2 = -3$, $\\beta_2 = 1.1$, $\\alpha_3 = 1$, $\\beta_3 = 0.5$, $\\chi = 0.99$, uncertainties: $\\sigma_R = 10.0$, $\\sigma_S = 12.0$, $\\sigma_A = 6.0$, $\\sigma_{\\partial H} = 8.0$, $\\sigma_C = 10.0$.\n- Case $3$ (strong closure bias, small uncertainties):\n  - $N = 60$, $R_0 = -40$, $R_{\\mathrm{amp}} = 20$, $S_0 = 120$, $S_{\\mathrm{amp}} = 30$, $A_0 = 15$, $A_{\\mathrm{amp}} = 10$, $D_0 = 2$, $D_{\\mathrm{amp}} = 3$, $\\phi = \\pi/4$, closure parameters: $\\alpha_0 = 10$, $\\alpha_1 = -2$, $\\beta_1 = 1.5$, $\\alpha_2 = 4$, $\\beta_2 = 0.9$, $\\alpha_3 = -1$, $\\beta_3 = 1.2$, $\\chi = 0.75$, uncertainties: $\\sigma_R = 2.0$, $\\sigma_S = 3.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 2.0$.\n- Case $4$ (small sample, moderate uncertainties):\n  - $N = 10$, $R_0 = -35$, $R_{\\mathrm{amp}} = 10$, $S_0 = 90$, $S_{\\mathrm{amp}} = 15$, $A_0 = 8$, $A_{\\mathrm{amp}} = 6$, $D_0 = 1$, $D_{\\mathrm{amp}} = 1.5$, $\\phi = \\pi/8$, closure parameters: $\\alpha_0 = 0$, $\\alpha_1 = 1$, $\\beta_1 = 1.0$, $\\alpha_2 = 1$, $\\beta_2 = 1.0$, $\\alpha_3 = 1$, $\\beta_3 = 1.0$, $\\chi = 0.98$, uncertainties: $\\sigma_R = 4.0$, $\\sigma_S = 6.0$, $\\sigma_A = 3.0$, $\\sigma_{\\partial H} = 2.0$, $\\sigma_C = 5.0$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the pair $[\\bar{\\varepsilon}, \\text{is\\_significant}]$, where $\\bar{\\varepsilon}$ is the mean residual imbalance in $\\mathrm{W\\,m^{-2}}$ rounded to three decimal places, and $\\text{is\\_significant}$ is a boolean indicating whether $z  1.96$. The final output must look like\n$$\n\\big[[\\bar{\\varepsilon}_1,\\text{is\\_significant}_1],[\\bar{\\varepsilon}_2,\\text{is\\_significant}_2],[\\bar{\\varepsilon}_3,\\text{is\\_significant}_3],[\\bar{\\varepsilon}_4,\\text{is\\_significant}_4]\\big].\n$$",
            "solution": "The user has provided a problem that requires the evaluation of a deep learning-based convective parameterization within a simplified atmospheric moist static energy budget. The core task is to compute the time-mean residual energy imbalance, $\\bar{\\varepsilon}$, and assess its statistical significance against measurement and modeling uncertainties.\n\n### Step 1: Extract Givens\n\n**Equations and Definitions:**\n- Column-integrated moist static energy budget: $\\frac{\\partial H}{\\partial t} = R + S + A + C + \\varepsilon$\n- Proposed convective closure ($C_\\theta$): $C_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh\\!\\Big(\\beta_1 \\frac{S(t) - S_0}{S_{\\mathrm{amp}}}\\Big) + \\alpha_2 \\tanh\\!\\Big(\\beta_2 \\frac{R(t) - R_0}{R_{\\mathrm{amp}}}\\Big) + \\alpha_3 \\tanh\\!\\Big(\\beta_3 \\frac{A(t) - A_0}{A_{\\mathrm{amp}}}\\Big) - \\chi \\big(R(t) + S(t) + A(t)\\big)$\n- Resolved diagnostic trajectories for $t = 0, 1, \\dots, N-1$ with $\\omega = 2\\pi/N$:\n  - $R(t) = R_0 + R_{\\mathrm{amp}} \\sin(\\omega t)$\n  - $S(t) = S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)$\n  - $A(t) = A_0 + A_{\\mathrm{amp}} \\sin(2\\omega t)$\n- Physically consistent reference convective tendency ($C_{\\mathrm{true}}$): $C_{\\mathrm{true}}(t) = -\\big(R(t) + S(t) + A(t)\\big) + D_0 + D_{\\mathrm{amp}} \\sin\\!\\big(\\omega t + \\phi\\big)$\n- Diagnosed storage tendency: $\\frac{\\partial H}{\\partial t}(t) = R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t)$\n- Residual imbalance: $\\varepsilon(t) = \\frac{\\partial H}{\\partial t}(t) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)$\n- Time-mean imbalance: $\\bar{\\varepsilon} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\varepsilon(t)$\n- Standard error of the mean imbalance: $\\mathrm{SE}(\\bar{\\varepsilon}) = \\sqrt{\\frac{\\sigma_R^2 + \\sigma_S^2 + \\sigma_A^2 + \\sigma_{\\partial H}^2 + \\sigma_C^2}{N}}$\n- $z$-score: $z = |\\bar{\\varepsilon}| / \\mathrm{SE}(\\bar{\\varepsilon})$\n- Significance criterion: $z > 1.96$ (two-sided $95\\%$ confidence level)\n\n**Test Cases:**\n- **Case 1**: $N = 24$, $R_0 = -50$, $R_{\\mathrm{amp}} = 15$, $S_0 = 100$, $S_{\\mathrm{amp}} = 25$, $A_0 = 10$, $A_{\\mathrm{amp}} = 8$, $D_0 = 3$, $D_{\\mathrm{amp}} = 2$, $\\phi = \\pi/6$, $\\alpha_0 = 0$, $\\alpha_1 = 8$, $\\beta_1 = 1.0$, $\\alpha_2 = -5$, $\\beta_2 = 1.2$, $\\alpha_3 = 2$, $\\beta_3 = 0.8$, $\\chi = 0.92$, $\\sigma_R = 3.0$, $\\sigma_S = 5.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 4.0$.\n- **Case 2**: $N = 12$, $R_0 = -30$, $R_{\\mathrm{amp}} = 12$, $S_0 = 80$, $S_{\\mathrm{amp}} = 20$, $A_0 = 5$, $A_{\\mathrm{amp}} = 4$, $D_0 = 0.5$, $D_{\\mathrm{amp}} = 1.0$, $\\phi = \\pi/3$, $\\alpha_0 = 0$, $\\alpha_1 = 5$, $\\beta_1 = 0.7$, $\\alpha_2 = -3$, $\\beta_2 = 1.1$, $\\alpha_3 = 1$, $\\beta_3 = 0.5$, $\\chi = 0.99$, $\\sigma_R = 10.0$, $\\sigma_S = 12.0$, $\\sigma_A = 6.0$, $\\sigma_{\\partial H} = 8.0$, $\\sigma_C = 10.0$.\n- **Case 3**: $N = 60$, $R_0 = -40$, $R_{\\mathrm{amp}} = 20$, $S_0 = 120$, $S_{\\mathrm{amp}} = 30$, $A_0 = 15$, $A_{\\mathrm{amp}} = 10$, $D_0 = 2$, $D_{\\mathrm{amp}} = 3$, $\\phi = \\pi/4$, $\\alpha_0 = 10$, $\\alpha_1 = -2$, $\\beta_1 = 1.5$, $\\alpha_2 = 4$, $\\beta_2 = 0.9$, $\\alpha_3 = -1$, $\\beta_3 = 1.2$, $\\chi = 0.75$, $\\sigma_R = 2.0$, $\\sigma_S = 3.0$, $\\sigma_A = 2.0$, $\\sigma_{\\partial H} = 1.0$, $\\sigma_C = 2.0$.\n- **Case 4**: $N = 10$, $R_0 = -35$, $R_{\\mathrm{amp}} = 10$, $S_0 = 90$, $S_{\\mathrm{amp}} = 15$, $A_0 = 8$, $A_{\\mathrm{amp}} = 6$, $D_0 = 1$, $D_{\\mathrm{amp}} = 1.5$, $\\phi = \\pi/8$, $\\alpha_0 = 0$, $\\alpha_1 = 1$, $\\beta_1 = 1.0$, $\\alpha_2 = 1$, $\\beta_2 = 1.0$, $\\alpha_3 = 1$, $\\beta_3 = 1.0$, $\\chi = 0.98$, $\\sigma_R = 4.0$, $\\sigma_S = 6.0$, $\\sigma_A = 3.0$, $\\sigma_{\\partial H} = 2.0$, $\\sigma_C = 5.0$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Soundness**: The problem uses a simplified, but conceptually valid, model of an atmospheric column's energy budget. Such simplifications are standard practice in theoretical meteorology and for testing numerical schemes. The functional forms for the parameterization and forcings are plausible representations for a modeling exercise. All equations are mathematically and physically consistent within the context of this model.\n2.  **Well-Posedness**: All necessary definitions, equations, and numerical parameters are provided for each test case. The problem is self-contained and free of contradictions. The computational task is clearly defined, leading to a unique solution for each case.\n3.  **Objectivity**: The problem is stated in precise mathematical and scientific language. It does not contain subjective claims or ambiguities.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe primary objective is to calculate the time-mean residual $\\bar{\\varepsilon}$ and determine if it is statistically significant. The process involves several computational steps for each test case.\n\n**1. Simplification of the Residual Calculation**\n\nThe residual imbalance $\\varepsilon(t)$ is defined as the difference between the diagnosed storage tendency $\\frac{\\partial H}{\\partial t}$ and the sum of all parameterized and resolved flux tendencies:\n$$\n\\varepsilon(t) = \\frac{\\partial H}{\\partial t}(t) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)\n$$\nThe problem provides an expression for the \"true\" or diagnosed storage tendency, which is based on a reference convective tendency $C_{\\mathrm{true}}(t)$:\n$$\n\\frac{\\partial H}{\\partial t}(t) = R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t)\n$$\nSubstituting this expression into the equation for $\\varepsilon(t)$ yields a crucial simplification:\n$$\n\\varepsilon(t) = \\big(R(t) + S(t) + A(t) + C_{\\mathrm{true}}(t)\\big) - \\big(R(t) + S(t) + A(t) + C_\\theta(t)\\big)\n$$\nThe terms $R(t)$, $S(t)$, and $A(t)$ cancel out, leaving:\n$$\n\\varepsilon(t) = C_{\\mathrm{true}}(t) - C_\\theta(t)\n$$\nThis result shows that the residual imbalance is simply the difference between the true convective tendency and the value predicted by the deep learning closure. This simplifies the computation significantly.\n\n**2. Algorithmic Procedure**\n\nFor each test case, the following steps are executed:\n\n- **Generate Time Series**: For a given number of time steps $N$, define a time vector for $t = 0, 1, \\dots, N-1$. The angular frequency is $\\omega = 2\\pi/N$.\n\n- **Compute Forcing Terms**: Calculate the time series for the resolved diagnostics $R(t)$, $S(t)$, and $A(t)$ using the provided sinusoidal functions and case-specific parameters:\n  - $R(t) = R_0 + R_{\\mathrm{amp}} \\sin(\\omega t)$\n  - $S(t) = S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)$\n  - $A(t) = A_0 + A_{\\mathrm{amp}} \\sin(2\\omega t)$\n\n- **Compute True Convective Tendency**: Use the computed forcing terms to find the true convective tendency $C_{\\mathrm{true}}(t)$:\n  $$\n  C_{\\mathrm{true}}(t) = -\\big(R(t) + S(t) + A(t)\\big) + D_0 + D_{\\mathrm{amp}} \\sin\\!\\big(\\omega t + \\phi\\big)\n  $$\n\n- **Compute Parameterized Convective Tendency**: Calculate the proposed closure's prediction $C_\\theta(t)$ using its definition:\n  $$\n  C_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh\\!\\Big(\\beta_1 \\frac{S(t) - S_0}{S_{\\mathrm{amp}}}\\Big) + \\alpha_2 \\tanh\\!\\Big(\\beta_2 \\frac{R(t) - R_0}{R_{\\mathrm{amp}}}\\Big) + \\alpha_3 \\tanh\\!\\Big(\\beta_3 \\frac{A(t) - A_0}{A_{\\mathrm{amp}}}\\Big) - \\chi \\big(R(t) + S(t) + A(t)\\big)\n  $$\n  Note that the arguments of the $\\tanh$ functions simplify. For instance, $\\frac{S(t) - S_0}{S_{\\mathrm{amp}}} = \\frac{(S_0 + S_{\\mathrm{amp}} \\cos(\\omega t)) - S_0}{S_{\\mathrm{amp}}} = \\cos(\\omega t)$. The full expression for $C_\\theta(t)$ becomes:\n  $$\n  C_\\theta(t) = \\alpha_0 + \\alpha_1 \\tanh(\\beta_1 \\cos(\\omega t)) + \\alpha_2 \\tanh(\\beta_2 \\sin(\\omega t)) + \\alpha_3 \\tanh(\\beta_3 \\sin(2\\omega t)) - \\chi \\big(R(t) + S(t) + A(t)\\big)\n  $$\n\n- **Compute Residual**: Compute the time series of the residual $\\varepsilon(t) = C_{\\mathrm{true}}(t) - C_\\theta(t)$.\n\n- **Calculate Mean Residual**: Compute the time-average of the residual series:\n  $$\n  \\bar{\\varepsilon} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\varepsilon(t)\n  $$\n  This value will be rounded to three decimal places.\n\n- **Calculate Statistical Significance**:\n  1.  Compute the standard error of the mean, $\\mathrm{SE}(\\bar{\\varepsilon})$, using the given formula and uncertainty values ($\\sigma_R, \\sigma_S, \\sigma_A, \\sigma_{\\partial H}, \\sigma_C$):\n      $$\n      \\mathrm{SE}(\\bar{\\varepsilon}) = \\sqrt{\\frac{\\sigma_R^2 + \\sigma_S^2 + \\sigma_A^2 + \\sigma_{\\partial H}^2 + \\sigma_C^2}{N}}\n      $$\n  2.  Calculate the $z$-score:\n      $$\n      z = \\frac{|\\bar{\\varepsilon}|}{\\mathrm{SE}(\\bar{\\varepsilon})}\n      $$\n  3.  Determine if the result is statistically significant by comparing the $z$-score to the critical value for a two-sided $95\\%$ confidence interval, which is $z_{0.95} = 1.96$:\n      $$\n      \\text{is\\_significant} = (z > 1.96)\n      $$\n\nThis procedure is applied to each of the four test cases specified in the problem statement to generate the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the residual energy imbalance and its statistical significance\n    for a proposed convective parameterization across multiple test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general diurnal variability, moderate uncertainties)\n        {\n            \"N\": 24, \"R_0\": -50, \"R_amp\": 15, \"S_0\": 100, \"S_amp\": 25, \"A_0\": 10, \"A_amp\": 8,\n            \"D_0\": 3, \"D_amp\": 2, \"phi\": np.pi/6,\n            \"params\": {\"alpha_0\": 0, \"alpha_1\": 8, \"beta_1\": 1.0, \"alpha_2\": -5, \"beta_2\": 1.2,\n                       \"alpha_3\": 2, \"beta_3\": 0.8, \"chi\": 0.92},\n            \"sigmas\": {\"R\": 3.0, \"S\": 5.0, \"A\": 2.0, \"dH\": 1.0, \"C\": 4.0}\n        },\n        # Case 2 (large uncertainties, near-balanced closure)\n        {\n            \"N\": 12, \"R_0\": -30, \"R_amp\": 12, \"S_0\": 80, \"S_amp\": 20, \"A_0\": 5, \"A_amp\": 4,\n            \"D_0\": 0.5, \"D_amp\": 1.0, \"phi\": np.pi/3,\n            \"params\": {\"alpha_0\": 0, \"alpha_1\": 5, \"beta_1\": 0.7, \"alpha_2\": -3, \"beta_2\": 1.1,\n                       \"alpha_3\": 1, \"beta_3\": 0.5, \"chi\": 0.99},\n            \"sigmas\": {\"R\": 10.0, \"S\": 12.0, \"A\": 6.0, \"dH\": 8.0, \"C\": 10.0}\n        },\n        # Case 3 (strong closure bias, small uncertainties)\n        {\n            \"N\": 60, \"R_0\": -40, \"R_amp\": 20, \"S_0\": 120, \"S_amp\": 30, \"A_0\": 15, \"A_amp\": 10,\n            \"D_0\": 2, \"D_amp\": 3, \"phi\": np.pi/4,\n            \"params\": {\"alpha_0\": 10, \"alpha_1\": -2, \"beta_1\": 1.5, \"alpha_2\": 4, \"beta_2\": 0.9,\n                       \"alpha_3\": -1, \"beta_3\": 1.2, \"chi\": 0.75},\n            \"sigmas\": {\"R\": 2.0, \"S\": 3.0, \"A\": 2.0, \"dH\": 1.0, \"C\": 2.0}\n        },\n        # Case 4 (small sample, moderate uncertainties)\n        {\n            \"N\": 10, \"R_0\": -35, \"R_amp\": 10, \"S_0\": 90, \"S_amp\": 15, \"A_0\": 8, \"A_amp\": 6,\n            \"D_0\": 1, \"D_amp\": 1.5, \"phi\": np.pi/8,\n            \"params\": {\"alpha_0\": 0, \"alpha_1\": 1, \"beta_1\": 1.0, \"alpha_2\": 1, \"beta_2\": 1.0,\n                       \"alpha_3\": 1, \"beta_3\": 1.0, \"chi\": 0.98},\n            \"sigmas\": {\"R\": 4.0, \"S\": 6.0, \"A\": 3.0, \"dH\": 2.0, \"C\": 5.0}\n        }\n    ]\n\n    results = []\n    z_critical = 1.96\n\n    for case in test_cases:\n        # Unpack parameters\n        N = case[\"N\"]\n        R_0, R_amp = case[\"R_0\"], case[\"R_amp\"]\n        S_0, S_amp = case[\"S_0\"], case[\"S_amp\"]\n        A_0, A_amp = case[\"A_0\"], case[\"A_amp\"]\n        D_0, D_amp, phi = case[\"D_0\"], case[\"D_amp\"], case[\"phi\"]\n        \n        p = case[\"params\"]\n        alpha_0, alpha_1, beta_1 = p[\"alpha_0\"], p[\"alpha_1\"], p[\"beta_1\"]\n        alpha_2, beta_2 = p[\"alpha_2\"], p[\"beta_2\"]\n        alpha_3, beta_3 = p[\"alpha_3\"], p[\"beta_3\"]\n        chi = p[\"chi\"]\n        \n        s = case[\"sigmas\"]\n        sigma_R, sigma_S, sigma_A, sigma_dH, sigma_C = s[\"R\"], s[\"S\"], s[\"A\"], s[\"dH\"], s[\"C\"]\n\n        # Time vector and angular frequency\n        t = np.arange(N)\n        omega = 2 * np.pi / N\n\n        # 1. Generate forcing time series\n        R_t = R_0 + R_amp * np.sin(omega * t)\n        S_t = S_0 + S_amp * np.cos(omega * t)\n        A_t = A_0 + A_amp * np.sin(2 * omega * t)\n        \n        # 2. Compute C_true(t)\n        total_forcing = R_t + S_t + A_t\n        C_true_t = -total_forcing + D_0 + D_amp * np.sin(omega * t + phi)\n\n        # 3. Compute C_theta(t)\n        # Arguments for tanh simplify based on definitions of R(t), S(t), A(t)\n        term_S = alpha_1 * np.tanh(beta_1 * np.cos(omega * t))\n        term_R = alpha_2 * np.tanh(beta_2 * np.sin(omega * t))\n        term_A = alpha_3 * np.tanh(beta_3 * np.sin(2 * omega * t))\n        \n        C_theta_t = alpha_0 + term_S + term_R + term_A - chi * total_forcing\n        \n        # 4. Compute residual epsilon(t) and its mean\n        epsilon_t = C_true_t - C_theta_t\n        epsilon_bar = np.mean(epsilon_t)\n\n        # 5. Calculate statistical significance\n        # Sum of variances\n        variance_sum = sigma_R**2 + sigma_S**2 + sigma_A**2 + sigma_dH**2 + sigma_C**2\n        # Standard error of the mean\n        se_epsilon_bar = np.sqrt(variance_sum / N)\n        \n        # z-score\n        if se_epsilon_bar > 0:\n            z_score = np.abs(epsilon_bar) / se_epsilon_bar\n        else: # Avoid division by zero, though unlikely with given inputs\n            z_score = np.inf if np.abs(epsilon_bar) > 0 else 0\n            \n        # Significance check\n        is_significant = z_score > z_critical\n\n        # Append formatted result\n        results.append([round(epsilon_bar, 3), is_significant])\n        \n    # Format final output string\n    print(str(results).replace(\" \", \"\").replace(\"'\", \"\"))\n\nsolve()\n```"
        }
    ]
}