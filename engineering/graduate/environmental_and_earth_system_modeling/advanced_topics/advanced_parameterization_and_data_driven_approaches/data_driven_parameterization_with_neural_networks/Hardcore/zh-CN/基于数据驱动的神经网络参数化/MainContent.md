## 引言
在地球系统科学等领域，复杂的自然现象由我们无法在有限计算资源下完全求解的控制方程所描述。这种尺度上的局限性导致了“闭合问题”——即必须近似表达那些无法被显式解析的微小尺度过程（如[湍流](@entry_id:151300)、云微物理）对宏观系统的累积效应。传统的[参数化](@entry_id:265163)方案虽在历史上扮演了重要角色，但其基于简化假设的函数形式在面对“灰色地带”和极端条件下往往表现不佳。数据驱动[参数化](@entry_id:265163)，特别是利用神经网络，正成为一个充满前景的替代方案，它能够直接从高保真模拟或观测数据中学习这些复杂且[非线性](@entry_id:637147)的次网格效应。

然而，将神经网络这一强大的“黑箱”工具应用于严谨的科学建模并非易事。一个未经深思熟虑训练出的模型可能违反基本的物理守恒定律，或是在与动态[模式耦合](@entry_id:752088)时引发数值爆炸，其预测在未知情景下也并不可信。本文旨在系统性地解决这一知识鸿沟，为研究人员和实践者提供一个从理论到实践的全面指南，阐明如何构建和应用物理上一致、数值上稳定且可靠的数据驱动[参数化](@entry_id:265163)方案。

为了实现这一目标，我们将通过以下三个章节展开深入探讨。首先，在“**原理与机制**”一章中，我们将深入剖析闭合问题的物理根源，阐明神经网络作为[通用函数逼近器](@entry_id:637737)的理论依据及其局限性，并系统地介绍确保物理守恒、数值稳定性和量化不确定性的核心技术。接着，在“**应用与交叉学科联系**”一章中，我们将展示这些原理如何在地球[系统建模](@entry_id:197208)（大气与海洋）、[计算燃烧学](@entry_id:1122776)、材料科学等多个领域中得到应用，凸显其作为一种通用方法论的广泛潜力。最后，“**动手实践**”部分将通过具体练习，指导您应对构建稳健[参数化](@entry_id:265163)方案时遇到的关键挑战。

## 原理与机制

在上一章中，我们介绍了在地球系统模式中使用数据驱动方法进行[参数化](@entry_id:265163)的动机和背景。本章将深入探讨其核心的科学原理与技术机制。我们将从问题的根源——即控制方程在粗分辨率网格上产生的闭合问题——出发，系统地阐明为何需要[参数化](@entry_id:265163)、神经网络如何作为一种强大的工具来解决这一问题，以及在实践中将这些数据驱动方案与复杂的地球系统[模式耦合](@entry_id:752088)时必须满足的关键物理和数值约束。

### 地球系统模式中的闭合问题

地球系统模式（ESM）的核心是描述大气、海洋、陆地和冰雪圈中流体运动与物理化学过程的[偏微分方程组](@entry_id:172573)。这些方程，例如[纳维-斯托克斯方程](@entry_id:142275)或[标量守恒律](@entry_id:754532)，是在连续介质假设下建立的。然而，为了进行数值求解，我们必须将这些连续方程离散化到有限的[计算网格](@entry_id:168560)上。一个典型的网格单元大小（或称模式分辨率）可能从几公里到数百公里不等。这种离散化操作从根本上限制了模式能够直接解析的物理过程的尺度。任何尺度小于网格分辨率的现象，如单个的积云、[湍流](@entry_id:151300)涡旋或微物理过程，都无法被显式表达，它们被统称为**[次网格尺度](@entry_id:1132591)（subgrid-scale, SGS）**过程。

尽管这些次网格过程本身无法被解析，但它们通过与可解析的大尺度环流相互作用，对其产生不可忽略的累积效应。例如，成千上万个微小的云滴共同决定了云的宏观辐射特性；无数个[湍流](@entry_id:151300)涡旋集体完成了动量与热量的垂直输送。忽略这些次网格效应将导致模式对大尺度气候的模拟产生严重偏差。因此，我们必须找到一种方法，用模式能够解析的变量（如网格平均的温度、湿度）来表达次网格过程的净效应。这个过程就是**[参数化](@entry_id:265163)（parameterization）**，而由此引发的问题被称为**闭合问题（closure problem）**。

#### 滤波、平均与未闭合项的产生

为了更严谨地理解闭合问题的起源，我们可以引入**空间滤波（spatial filtering）**或**平均（averaging）**算子的概念。这是一种数学上的理想化操作，它将一个连续的物理场分解为**可解尺度（resolved-scale）**[部分和](@entry_id:162077)**次网格尺度（subgrid-scale）**部分。例如，对于一个[标量场](@entry_id:151443) $\phi(\mathbf{x}, t)$（如污染物浓度或水汽含量），我们可以定义其在尺度 $\Delta$ 上的滤波场（或称平均场）为 $\overline{\phi}$。

让我们考虑一个一维的[标量输运方程](@entry_id:1131253)，它描述了浓度为 $c(x,t)$ 的示踪剂在速度场 $u(x,t)$ 平流和[分子扩散](@entry_id:154595)（扩散系数为 $\kappa$）共同作用下的演变：
$$
\frac{\partial c}{\partial t} + \frac{\partial J}{\partial x} = S(x,t), \quad \text{其中 } J(x,t) = u(x,t)c(x,t) - \kappa \frac{\partial c}{\partial x}
$$
这里，$J$ 是总通量，$S$ 是源汇项。当我们对该方程进行滤波操作，得到可解尺度场的控制方程时，会发现一个关键问题。假设滤波算子与微分算子可交换，我们得到：
$$
\frac{\partial \overline{c}}{\partial t} + \frac{\partial \overline{J}}{\partial x} = \overline{S}
$$
问题出在对[非线性](@entry_id:637147)项的滤波上，特别是平流通量项 $\overline{uc}$。一般而言，**滤波后的乘积不等于滤波后变量的乘积**，即 $\overline{uc} \neq \overline{u}\overline{c}$。为了处理这一项，我们将其分解为可解部分和次网格部分：
$$
\overline{uc} = \overline{u}\,\overline{c} + (\overline{uc} - \overline{u}\,\overline{c})
$$
于是，总的滤波后通量 $\overline{J}$ 可以写为：
$$
\overline{J} = \underbrace{\left(\overline{u}\,\overline{c} - \kappa \frac{\partial \overline{c}}{\partial x}\right)}_{\text{可解尺度通量}} + \underbrace{\left(\overline{uc} - \overline{u}\,\overline{c}\right)}_{\text{次网格尺度通量}}
$$
右侧第一项仅依赖于可解尺度变量，可以在粗分辨率模式中直接计算。而第二项，即**[次网格尺度](@entry_id:1132591)通量 (subgrid-scale flux)** $J_{\mathrm{sgs}} = \overline{uc} - \overline{u}\,\overline{c}$，则依赖于速度场和浓度场的次网格脉动之间的关联，它无法仅从可解尺度变量 $\overline{u}$ 和 $\overline{c}$ 中直接得到。这一项就是所谓的**未闭合项（unclosed term）**。[参数化](@entry_id:265163)的核心任务，就是为这个未闭合项建立一个模型，即 $J_{\mathrm{sgs}} \approx P(\overline{u}, \overline{c}, \dots)$。

这个概念可以推广到更复杂的情形。例如，在[雷诺平均](@entry_id:754341)（Reynolds averaging）的框架下，将物理量分解为平均部分和脉动部分（如 $u = \overline{u} + u'$），对一个包含[非线性](@entry_id:637147)源汇项 $S(q) = -\lambda q + \alpha q^2$ 的守恒律进行平均，会产生两类未闭合项：
1.  **[湍流通量](@entry_id:1133513)散度（Eddy-flux divergence）**: 来自平流项的 $\partial_x(\overline{u'q'})$。
2.  **[非线性源项](@entry_id:1128871)修正（Nonlinear source correction）**: 来自[非线性源项](@entry_id:1128871)的 $\alpha \overline{q'^2}$。
这两项共同构成了模式需要[参数化](@entry_id:265163)的总的次网格倾向 $\mathcal{T}(x) = -\partial_{x}(\overline{u'q'}) + \alpha\overline{q'^2}$。数据驱动[参数化](@entry_id:265163)的目标，正是利用数据学习一个从可解变量到 $\mathcal{T}(x)$ 的映射。

#### [参数化](@entry_id:265163)、数值误差与结构误差

在建立和评估[参数化](@entry_id:265163)方案时，至关重要的是要将其与模式中其他两类误差明确区分开来：

1.  **[次网格尺度参数化](@entry_id:1132601)（Subgrid-scale Parameterization）**：这是一个**物理建模**问题。它源于我们通过滤波或平均来简化控制方程时，必然产生的未闭合项。[参数化](@entry_id:265163)的目标是为这些代表了真实物理过程（如[湍流混合](@entry_id:202591)）的未闭合项提供一个近似的函数表达。这个问题在概念上独立于任何特定的数值解法。

2.  **数值离散误差（Numerical Discretization Error）**：这是一个**数学近似**问题。它源于我们将（已经滤波后的）连续[偏微分](@entry_id:194612)方程中的[微分算子](@entry_id:140145)（如 $\partial/\partial x$）替换为离散的代数运算（如[有限差分](@entry_id:167874)）。这种误差的大小取决于网格间距 $\Delta x$ 和所用数值格式的阶数。即使我们求解一个完全没有[次网格物理](@entry_id:755602)的[线性方程](@entry_id:151487)，离散误差依然存在。数据驱动[参数化](@entry_id:265163)的目标不是消除数值离散误差。

3.  **模式结构误差（Model Structural Error）**：这是一个**物理知识**的局限性问题。它源于我们赖以出发的“完整”控制方程本身就不完整或不正确。例如，我们可能遗漏了某个重要的化学反应，或者对辐射传输过程的描述有误。

一个设计良好的数据驱动[参数化](@entry_id:265163)，其训练目标应该是真实物理世界中的次网格效应（即未闭合项），而不是去拟合特定数值格式产生的离散误差，或是模式本身的结构误差。在实际操作中，将这三者清晰地分离开来是一个巨大的挑战。

### [参数化](@entry_id:265163)的理据：[尺度分离](@entry_id:270204)与灰色地带

我们之所以能够并需要对某些过程进行[参数化](@entry_id:265163)，其背后有一个核心假设：**[尺度分离](@entry_id:270204)（scale separation）**。这个假设认为，我们感兴趣的可解尺度现象与需要被[参数化](@entry_id:265163)的次网格过程之间，在空间和时间尺度上存在明显的鸿沟。

以热带[深对流](@entry_id:1123472)为例，这是驱动[全球大气环流](@entry_id:189520)的关键过程。单个的对流云（尤其是其核心上升气流）的特征水平尺度 $L_c$ 大约在 2 km 量级。对于一个典型的全球气候模式，其水平分辨率 $\Delta x$ 可能为 25 km。在这种情况下，$L_c \ll \Delta x$，[尺度分离假设](@entry_id:1131494)成立。模式网格完全无法解析单个对流云的结构，只能“感受”到它们对整个网格单元平均温湿场的集体加热和加湿效应。因此，深对流过程必须被[参数化](@entry_id:265163)。一个传统的对流[参数化](@entry_id:265163)方案，本质上就是一个根据网格的平均状态（如温度、湿度、抬升触发条件）来诊断出是否存在对流，并计算其对网格整体的反馈的子程序。

然而，随着计算能力的提升，模式分辨率越来越高。当我们使用一个“对流许可（convection-permitting）”的区域模式，其分辨率 $\Delta x$ 达到 3 km 时，情况就变得复杂了。此时，$L_c \approx \Delta x$，单个对流云的尺度与网格尺度相当。尺度分离的假设被打破，我们进入了所谓的**灰色地带（gray zone）**。

在灰色地带，模式的[动力核心](@entry_id:1124042)开始能够部分地、笨拙地解析出对流运动。此时，如果继续使用为粗分辨率设计的传统[参数化](@entry_id:265163)方案，就会出现严重问题。该方案仍然假设所有对流都是次网格的，并会据此计算出一个完整的对流加热/加湿倾向。与此同时，模式的[动力核心](@entry_id:1124042)也在自行产生对流。这两者叠加，会导致对能量和水汽的**“双重计算（double-counting）”**，造成虚假的强降水和不稳定的数值行为。

灰色地带问题是传统[参数化](@entry_id:265163)方案面临的重大挑战，也正是数据驱动方法可以大放异彩的领域。一个设计良好的神经网络[参数化](@entry_id:265163)可以被训练成**尺度感知（scale-aware）**的。通过将模式分辨率 $\Delta x$ 作为神经网络的一个输入特征，它可以学会在不同分辨率下扮演不同的角色：在粗分辨率下，它提供完整的次网格效应；而在灰色地带，它能学会“退让”，只提供动力核心未能解析的那部分“残余”的次网格效应，从而平滑地跨越不同尺度，避免双重计算。

### 作为[通用函数逼近器](@entry_id:637737)的神经网络

选择神经网络（NN）作为[参数化](@entry_id:265163)工具，其理论基础是**通用逼近定理（Universal Approximation Theorem, UAT）**。该定理指出，一个包含足够多神经元和单个隐藏层的[前馈神经网络](@entry_id:635871)，只要其激活函数是连续且非多项式的（例如 Sigmoid 或 [tanh](@entry_id:636446)），就能够以任意精度逼近一个[紧集](@entry_id:147575)（compact set）上的任意连续函数。

在[参数化](@entry_id:265163)问题的语境下，这意味着，如果从可解尺度状态（如 $\overline{\mathbf{x}}$）到次网格倾向（如 $\mathcal{T}$）的真实物理映射 $\mathbf{f}^{\star}(\overline{\mathbf{x}})$ 是连续的，并且我们关心的物理[状态空间](@entry_id:160914)是有限（有界闭合，即[紧集](@entry_id:147575)）的，那么理论上**存在**一个神经网络可以完美地学习这个映射。这为我们使用神经网络作为[参数化](@entry_id:265163)工具提供了根本的理论信心。

然而，通用逼近定理的承诺是有严格边界和重要警示的，理解其局限性对于科学应用至关重要：

1.  **存在性 vs. 构造性**：UAT 仅仅是一个**[存在性定理](@entry_id:261096)**。它保证了“理想”网络的存在，但并未告诉我们如何找到它。实际的训练过程（如使用[梯度下降法](@entry_id:637322)）是否能找到这个理想网络，或者需要多大的网络、多少数据，定理本身没有回答。

2.  **物理约束的缺失**：UAT 是一个纯粹的数学定理，它对物理定律一无所知。一个通过最小化均方误差等统计损失函数训练出来的神经网络，不会自动满足**能量守恒、[质量守恒](@entry_id:204015)**等基本物理约束。除非在网络结构设计或损失函数中明确地加入这些约束，否则学到的[参数化](@entry_id:265163)方案很可能在与[模式耦合](@entry_id:752088)后导致物理量的虚假产生或消失。

3.  **[动力学稳定性](@entry_id:150175)的缺失**：UAT 保证的是对一个静态函数的逼近能力。它完全不保证当这个学到的函数作为一个组件被放入一个随时间演化的动力系统中（即耦合到 ESM 中进行时间积分）时，整个系统是**数值稳定**的。一个在离线测试中误差极小的网络，在线耦合后完全可能引发数值爆炸。

4.  **对函数形式的假设**：经典的 UAT 适用于从输入到输出的确定性、无记忆（马尔可夫）映射。然而，许多真实的次网格过程具有**随机性**（stochasticity）和**历史依赖性**（non-Markovianity）。为了应用 UAT，我们必须对问题进行重新表述。一种常见的做法是将训练目标定义为次网格倾向的**[条件期望](@entry_id:159140)** $\mathbb{E}[\mathcal{T} | \overline{\mathbf{x}}]$，这是一个确定性的映射，从而回避了随机性问题。

### 实践与实现：将神经网络与物理[模式耦合](@entry_id:752088)

将一个离线训练好的神经网络成功地耦合进一个在线运行的地球系统模式，需要克服两大核心挑战：确保物理守恒性和数值稳定性。

#### 确保物理一致性：守恒律

在地球系统中，质量、能量、水分等都是严格守恒的。一个用于[参数化](@entry_id:265163)[输运过程](@entry_id:177992)的方案，绝不能在模式中凭空创造或销毁这些[守恒量](@entry_id:161475)。在广泛使用的**有限体积（Finite Volume, FV）**数值方法中，实现[离散守恒](@entry_id:1123819)的关键在于将所有改变单元内[守恒量](@entry_id:161475)的过程都表达为通过单元边界的**通量（flux）**的散度。

对于一个守恒标量 $q$ 的输运，其在单元 $i$ 中的变化率由进出该单元的通量决定。如果[次网格参数化](@entry_id:1132597)的作用是输运 $q$，那么它也必须以通量形式贡献。这引出了神经网络[参数化](@entry_id:265163)输出格式的关键设计选择：

1.  **预测界面通量（Face Fluxes）**：神经网络的输出是每个单元界面 $f$ 上的次网格通量 $\Delta F_f^{\mathrm{sg}}$。在更新单元 $i$ 时，我们将这些通量贡献加到总通量散度中。只要保证对于任意一对相邻单元共享的内部界面，流出一个单元的通量精确等于流入另一个单元的通量（即通量是反对称的），那么在整个周期性区域内，总的[守恒量](@entry_id:161475)积分将**精确守恒**。这是实现守恒的最直接、最稳健的方法。

2.  **预测单元倾向（Cell Tendencies）**：神经网络直接输出每个单元 $i$ 的总倾向（变化率）$T_i^{\mathrm{sg}}$，然后直接加到单元状态上。这种方法**不保证守恒**。因为神经网络在预测每个 $T_i^{\mathrm{sg}}$ 时是局部的，没有全局约束来保证所有单元倾向的质量加权总和为零（$\sum_i V_i T_i^{\mathrm{sg}} = 0$）。这很容易导致模式在长期积分中出现系统性的物理量漂移。

3.  **预测闭合系数（Closure Coefficients）**：神经网络输出物理上有意义的系数，例如涡动扩散系数 $\kappa$。然后，这些系数被用来在一个物理上合理的闭合关系（如 $\Delta F_f^{\mathrm{sg}} = - \kappa_f A_f \frac{q_j - q_i}{d_f}$）中计算界面通量。这种方法既能保证守恒（因为它最终被构造成[通量形式](@entry_id:273811)），又能提供一定的物理解释性。

此外，采用通量形式的输出还有一个数值上的优势。它可以将次网格通量与模式动力核心计算的可解尺度通量在界面上直接相加，然后统一计算一次散度。这避免了将动力学倾向和物理过程倾向分开计算再相加所带来的**[算子分裂](@entry_id:634210)误差（operator-splitting error）**，从而提高了数值精度和稳定性。

#### 确保[数值稳定性](@entry_id:175146)

即使[参数化](@entry_id:265163)方案是守恒的，它也可能导致数值不稳定，尤其是在与[显式时间积分](@entry_id:165797)格式耦合时。一个微小的扰动可能被[参数化](@entry_id:265163)方案和时间步进过程不断放大，最终导致[模式崩溃](@entry_id:636761)。

##### 局部线性稳定性分析

我们可以通过一个简单的例子来理解这个问题。考虑一个由物理弛豫过程 $g(q)$ 和神经网络倾向 $f_{\theta}(q)$ 共同驱动的单变量系统 $\frac{dq}{dt} = g(q) + f_{\theta}(q)$。假设系统在 $q^{\star}$ 处有一个稳定不动点，即 $g(q^{\star})=0$ 且 $f_{\theta}(q^{\star})=0$。物理过程本身是稳定的，其在不动点附近的线性化斜率为 $g'(q^{\star}) = -\lambda$（其中 $\lambda > 0$）。神经网络在该点的斜率为 $k = f'_{\theta}(q^{\star})$。

当使用前向欧拉法 $q_{n+1} = q_{n} + h\,[g(q_{n}) + f_{\theta}(q_{n})]$ 进行时间积分时，对不动点附近的小扰动 $\epsilon_n$ 的演化进行线性化，可以得到 $\epsilon_{n+1} = [1 + h(k - \lambda)] \epsilon_n$。为了使扰动衰减，放大因子的绝对值必须小于1，即 $|1 + h(k - \lambda)|  1$。这导出了对神经网络局部斜率 $k$ 的严格约束：$\lambda - 2/h  k  \lambda$ 。

这个分析揭示了一个深刻的道理：神经网络的**局部梯度**直接影响着耦合系统的稳定性。一个梯度过大（无论是正或负）的神经网络都可能严重限制模型的最大稳定时间步长 $h_{\max}$。例如，如果物理[弛豫时间尺度](@entry_id:1130826) $\tau=1/\lambda$ 为 $7200$ s，而一个神经网络的局部斜率 $k$ 被学习为 $0.7\lambda$（即神经网络在一定程度上抵消了物理阻尼），那么最大稳定时间步长将被限制在 $h_{\max} = 2/(0.3\lambda) \approx 4.8 \times 10^4$ s 以内。

##### 通过[利普希茨连续性](@entry_id:142246)实现全局稳定性

[局部稳定性分析](@entry_id:178725)只在不动点附近有效。一个更强大、更具全局性的控制稳定性的工具是**[利普希茨连续性](@entry_id:142246)（Lipschitz continuity）**。一个函数 $P_{\theta}(x)$ 是[利普希茨连续的](@entry_id:267396)，如果存在一个常数 $L_P$（称为[利普希茨常数](@entry_id:146583)），使得对于任意两个输入 $x_1, x_2$，其输出的差异都有[上界](@entry_id:274738)：$\|P_{\theta}(x_1) - P_{\theta}(x_2)\| \le L_P \|x_1 - x_2\|$。这本质上是对函数梯度的全局上界进行了约束。

对于一个由[显式欧拉法](@entry_id:1124769)驱动的耦合系统 $x_{n+1} = x_n + \Delta t(F(x_n) + P_{\theta}(x_n))$，可以证明，一步映射的[利普希茨常数](@entry_id:146583) $L_G$ 满足 $L_G \le 1 + \Delta t (L_F + L_P)$，其中 $L_F$ 和 $L_P$ 分别是[动力核心](@entry_id:1124042)和神经网络[参数化](@entry_id:265163)的[利普希茨常数](@entry_id:146583)。这意味着扰动的增长率直接受控于 $L_P$。通过约束神经网络的[利普希茨常数](@entry_id:146583)，我们可以从根本上抑制由[参数化](@entry_id:265163)引入的虚假扰动增长。

对于一个多层感知机，其[利普希茨常数](@entry_id:146583)的一个[上界](@entry_id:274738)可以通过其各层权重矩阵的[谱范数](@entry_id:143091)（最大奇异值）的乘积来估计。例如，一个由三层权重矩阵 $W_1, W_2, W_3$ 和 1-利普希茨激活函数组成的网络，其[利普希茨常数](@entry_id:146583) $L_P \le \|W_3\|_2 \|W_2\|_2 \|W_1\|_2$。若已知 $\|W_1\|_2=1.8$, $\|W_2\|_2=1.2$, $\|W_3\|_2=0.7 \text{ s}^{-1}$，则可以估算出 $L_P$ 的一个[上界](@entry_id:274738)为 $1.512 \text{ s}^{-1}$ 。在训练过程中，可以通过对权重矩阵进行[谱归一化](@entry_id:637347)等技术来直接控制 $L_P$，从而保证在线耦合的稳定性。

### 前沿课题：随机性与[不确定性量化](@entry_id:138597)

传统的[参数化](@entry_id:265163)方案通常提供一个确定性的输出。然而，次网格过程具有内在的随机性。此外，我们用有限数据训练出的神经[网络模型](@entry_id:136956)本身也存在不确定性。一个成熟的数据驱动[参数化](@entry_id:265163)框架必须能够表达和量化这些不确定性。

#### [随机参数化](@entry_id:1132435)的理由

次网格过程对可解尺度的影响并不仅仅是一个平均的倾向，还包含着围绕这个平均值的涨落。忽略这些涨落可能导致模式无法正确模拟出气候系统的变率，例如极端天气事件的频率和强度。

**Mori-Zwanzig 形式理论**为随机参数化的必要性提供了坚实的理论基础。这个理论框架旨在从一个高维、确定性的完整动力系统中，推导出少数几个我们关心的“慢”变量（即可解尺度变量）的有效演化方程。理论表明，当我们“积分掉”那些我们不关心的“快”变量（即次网格变量）后，它们对慢变量的有效影响包含三个部分：一个修正后的确定性漂移项（modified drift）、一个依赖于慢变量历史的记忆项（memory term），以及一个**随机力项（random forcing term）**。

在一个简化的理想模型中，我们可以看到这一点。考虑一个慢变量 $x$ 和一个快变量 $y$ 通过乘性相互作用的系统。在快变量弛豫时间 $\lambda^{-1}$ 远小于慢变量演化时间的极限下，记忆效应可以忽略，慢变量 $x$ 的有效动力学遵循一个**[随机微分方程](@entry_id:146618)（SDE）**：
$$
dx = \left(-\alpha\,x\right)\,dt \;+\; \sqrt{2\,D(x)} \circ dW_{t}
$$
这里的随机项 $\sqrt{2D(x)} \circ dW_t$ 正是快变量随机性的体现。其强度，即状态依赖的扩散系数 $D(x)$，由快变量影响的[自相关函数](@entry_id:138327)的[时间积分](@entry_id:267413)决定。对于上述模型，可以推导出 $D(x) = \frac{\beta^{2} \sigma_{y}^{2}}{\lambda} x^{2}$。这个结果清晰地表明，从第一性原理出发，一个完备的[参数化](@entry_id:265163)不仅应该包含确定性倾向，还应该包含一个依赖于可解尺度状态的随机噪声项。

#### 量化神经网络[参数化](@entry_id:265163)的不确定性

当我们用神经网络预测次网格效应时，其预测的不确定性主要有两个来源：

1.  **[偶然不确定性](@entry_id:634772)（Aleatoric Uncertainty）**：这是**数据本身固有的、不可约减的随机性**。它反映了次网格过程的内在变率。即使我们拥有无限多的数据和完美的模型，这种不确定性依然存在。它对应于前文提到的随机力项。

2.  **认知不确定性（Epistemic Uncertainty）**：这是**由于我们知识的局限而产生的模型不确定性**。它源于训练数据有限、模型结构不完美等。理论上，随着数据量的增加和模型能力的提升，这种不确定性可以被减小。

在实践中，一种强大的区分并量化这两种不确定性的方法是使用**[深度集成](@entry_id:636362)（deep ensemble）**。我们训练多个（例如 $M$ 个）神经网络，每个网络使用不同的随机[权重初始化](@entry_id:636952)和/或不同的数据子集（通过 bootstrap 抽样）进行训练。此外，每个网络不仅预测一个均值 $\mu_i(\mathbf{x})$，还预测一个方差 $\sigma_i^2(\mathbf{x})$（通过优化异方差高斯[负对数似然](@entry_id:637801)损失函数）。

基于概率论中的**[全方差公式](@entry_id:177482)**，总的预测方差可以分解为：
$$
\operatorname{Var}(y | \mathbf{x}) = \underbrace{\mathbb{E}[\operatorname{Var}(y|\mathbf{x}, \mathbf{w})]}_{\text{偶然不确定性}} + \underbrace{\operatorname{Var}[\mathbb{E}(y|\mathbf{x}, \mathbf{w})]}_{\text{认知不确定性}}
$$
在[深度集成](@entry_id:636362)框架下，这可以近似为：

-   **[偶然不确定性](@entry_id:634772)** ≈ 集成成员预测方差的平均值： $\frac{1}{M}\sum_{i=1}^M \sigma_i^2(\mathbf{x})$。
-   **认知不确定性** ≈ 集成成员预测均值的方差： $\operatorname{Var}_{i}(\mu_i(\mathbf{x})) = \frac{1}{M}\sum_{i=1}^M (\mu_i(\mathbf{x}) - \overline{\mu}(\mathbf{x}))^2$。

这种分解具有极高的实用价值。认知不确定性对于**分布外（Out-of-Distribution, OOD）**检测特别有用。当模型遇到一个与训练数据截然不同的输入状态时，由于每个集成成员学到的函数在未知区域会以不同的方式外插，它们预测的均值会产生很大的[分歧](@entry_id:193119)，导致认知不确定性急剧增加。这可以作为一个可靠的“警报”，提示我们模型在该状态下的预测是不可信的。相比之下，[偶然不确定性](@entry_id:634772)通常不能提供这种警示。

除了[深度集成](@entry_id:636362)，**[分位数回归](@entry_id:169107)（quantile regression）**是另一种可以表征[偶然不确定性](@entry_id:634772)的有效方法。通过训练神经网络直接预测[条件分布](@entry_id:138367)的多个分位数（如第10、50、90百分位数），我们可以获得对次网格过程随机分布范围的非[参数化](@entry_id:265163)描述，而这仅需单个网络即可实现。

本章系统地阐述了数据驱动[参数化](@entry_id:265163)的核心原理与关键机制，从闭合问题的物理起源，到神经网络作为逼近工具的理论基础，再到耦合实践中守恒性、稳定性与不确定性量化等一系列核心挑战及其解决方案。掌握这些原理，是开发和应用可靠、鲁棒的下一代地球系统模式[参数化](@entry_id:265163)方案的基础。