{
    "hands_on_practices": [
        {
            "introduction": "物理建模中的一个基本挑战是确保模型学习到普适的规律，而不仅仅是训练数据特有的相关性。本练习将指导您使用白金汉 $\\Pi$ 定理，将有量纲的物理变量转换为无量纲数组，这是构建稳健且尺度不变的神经网络参数化方案的关键一步。通过处理无量纲输入，您可以显著提高模型对新情景的泛化能力。",
            "id": "3873146",
            "problem": "您正在使用人工神经网络（ANN）在​​大气柱模型中设计一种数据驱动的对流参数化方案。为了提高跨尺度的泛化能力，您决定将ANN的输入限制为从白金汉 $\\Pi$ 定理推导出的无量纲特征。考虑与稳定分层大气中的干对流动力学相关的以下可测量变量：垂直速度 $w$、绝对温度 $T$、水汽混合比 $q_v$、布伦特-维萨拉频率 $N$ 以及水平网格间距 $\\Delta$。假设 $w$ 的量纲为长度除以时间，$T$ 的量纲为热力学温度，$q_v$ 是质量之比因此是无量纲的，$N$ 的量纲为时间的倒数，$\\Delta$ 的量纲为长度。除了在 $N$ 中隐含的量纲外，没有其他量纲常数（例如，重力加速度或参考热力学量）可用于无量纲化。仅使用基本量纲长度 $L$、时间 $T$ 和热力学温度 $\\Theta$，应用白金汉 $\\Pi$ 定理，从 $\\{w, T, q_v, N, \\Delta\\}$ 中构建一个最小的独立无量纲输入特征集。以最简乘幂形式，将您的最终答案表示为一个包含独立 $\\Pi$ 群组的行向量。不要引入额外的参考尺度、经验常数或外部参数。以封闭形式的符号表达式提供这些特征。无需四舍五入，答案应为无单位的。",
            "solution": "在进行解答之前，需对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n-   变量：垂直速度 ($w$)、绝对温度 ($T$)、水汽混合比 ($q_v$)、布伦特-维萨拉频率 ($N$) 和水平网格间距 ($\\Delta$)。\n-   变量数量，$n=5$。\n-   基本量纲：长度 ($L$)、时间 ($T_{dim}$) 和热力学温度 ($\\Theta$)。我们使用 $T_{dim}$ 表示时间量纲，以避免与温度变量 $T$ 混淆。\n-   变量的量纲：\n    -   $[w] = L T_{dim}^{-1}$\n    -   $[T] = \\Theta$\n    -   $[q_v] = 1$ (无量纲)\n    -   $[N] = T_{dim}^{-1}$\n    -   $[\\Delta] = L$\n-   基本量纲数量，$k=3$。\n-   约束：没有其他量纲常数或参考量可用。\n-   目标：使用白金汉 $\\Pi$ 定理，从给定变量中找到一个最小的独立无量纲群组（$\\Pi$ 群组）集合。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，提法明确且客观。它提出了一个根植于大气物理学的标准量纲分析问题。在干对流动力学的背景下，这些变量及其量纲在物理上是正确的。该问题是自洽的，并提供了应用白金汉 $\\Pi$ 定理所需的所有必要信息。不存在矛盾、歧义或事实上的不健全之处。仅使用给定变量的约束是明确的，并且对推导至关重要。\n\n### 步骤3：结论与行动\n问题有效。将提供完整的解答。\n\n### 解答推导\n白金汉 $\\Pi$ 定理指出，一个涉及 $n$ 个变量且可以用 $k$ 个基本量纲表示的物理上有意义的方程，可以重写为一个包含 $p = n - r$ 个无量纲参数 ($\\Pi_i$) 的方程，其中 $r$ 是量纲矩阵的秩。\n\n首先，我们确定变量的数量为 $n=5$，即 $\\{w, T, q_v, N, \\Delta\\}$。\n基本量纲是长度 ($L$)、时间 ($T_{dim}$) 和热力学温度 ($\\Theta$)，因此有 $k=3$ 个量纲。\n\n每个变量的量纲可以表示为基本量纲 $(L, T_{dim}, \\Theta)$ 的指数向量：\n-   $w \\rightarrow (1, -1, 0)$\n-   $T \\rightarrow (0, 0, 1)$\n-   $q_v \\rightarrow (0, 0, 0)$\n-   $N \\rightarrow (0, -1, 0)$\n-   $\\Delta \\rightarrow (1, 0, 0)$\n\n我们通过将这些向量排列为列来构建量纲矩阵：\n$$ M = \\begin{pmatrix} 1  0  0  0  1 \\\\ -1  0  0  -1  0 \\\\ 0  1  0  0  0 \\end{pmatrix} $$\n行分别对应于 $L$、$T_{dim}$ 和 $\\Theta$，列对应于 $w、T、q_v、N、\\Delta$。\n\n该矩阵的秩 $r$ 是列空间的维数。我们可以找到一个行列式不为零的 $3 \\times 3$ 子矩阵。例如，选择对应于 $w$、$T$ 和 $\\Delta$ 的列：\n$$ \\det \\begin{pmatrix} 1  0  1 \\\\ -1  0  0 \\\\ 0  1  0 \\end{pmatrix} = 1(0) - 0(0) + 1(-1) = -1 \\neq 0 $$\n因此，矩阵的秩为 $r=3$。\n\n独立无量纲群组的数量为 $p = n - r = 5 - 3 = 2$。因此，我们必须找到两个独立的 $\\Pi$ 群组。\n\n设一个通用的无量纲群组 $\\Pi$ 由变量的特定次幂的乘积形成：\n$$ \\Pi = w^{a_1} T^{a_2} q_v^{a_3} N^{a_4} \\Delta^{a_5} $$\n要使 $\\Pi$ 无量纲，其量纲表示必须为 $L^0 T_{dim}^0 \\Theta^0$。\n$$ [\\Pi] = [w]^{a_1} [T]^{a_2} [q_v]^{a_3} [N]^{a_4} [\\Delta]^{a_5} $$\n$$ [\\Pi] = (L T_{dim}^{-1})^{a_1} (\\Theta)^{a_2} (1)^{a_3} (T_{dim}^{-1})^{a_4} (L)^{a_5} $$\n$$ [\\Pi] = L^{a_1+a_5} T_{dim}^{-a_1-a_4} \\Theta^{a_2} $$\n为了满足 $[\\Pi] = L^0 T_{dim}^0 \\Theta^0$，我们必须解以下线性方程组：\n1.  $a_1 + a_5 = 0$\n2.  $-a_1 - a_4 = 0$\n3.  $a_2 = 0$\n\n第三个方程 $a_2=0$ 是一个直接的推论，因为绝对温度 $T$ 是唯一具有热力学温度 $\\Theta$ 量纲的变量。由于没有其他变量或常数可以用来抵消这个量纲，所以 $T$ 不能成为任何无量纲群组的一部分。\n\n变量 $q_v$ 已被指定为无量纲。因此，我们可以选择它作为我们的第一个 $\\Pi$ 群组。\n$$ \\Pi_1 = q_v $$\n这对应于将 $a_3=1$ 且所有其他指数设为零 ($a_1=a_2=a_4=a_5=0$)，这满足该方程组。\n\n对于第二个 $\\Pi$ 群组，我们必须找到该方程组的第二个独立解。我们寻求剩余的有量纲变量 $\\{w, N, \\Delta\\}$ 的组合。该方程组简化为：\n1.  $a_1 + a_5 = 0$\n2.  $-a_1 - a_4 = 0$\n\n我们有两个方程和三个未知数 ($a_1, a_4, a_5$)。我们可以选择一个指数，然后解出其他指数。按照惯例，我们将我们希望无量纲化的变量（在本例中为 $w$）的指数选为 $a_1=1$。\n-   将 $a_1=1$ 代入第一个方程：$1 + a_5 = 0 \\implies a_5 = -1$。\n-   将 $a_1=1$ 代入第二个方程：$-1 - a_4 = 0 \\implies a_4 = -1$。\n该群组的指数为 $a_1=1, a_2=0, a_3=0, a_4=-1, a_5=-1$。\n这给出了第二个无量纲群组：\n$$ \\Pi_2 = w^{1} T^{0} q_v^{0} N^{-1} \\Delta^{-1} = \\frac{w}{N\\Delta} $$\n这个群组在物理上是有意义的，它代表一个无量纲速度，类似于分层流体环境中的弗劳德数。\n\n这两个独立的无量纲群组，以其最简乘幂形式表示，是 $\\Pi_1=q_v$ 和 $\\Pi_2=\\frac{w}{N\\Delta}$。问题要求以行向量的形式给出答案。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} q_v  \\frac{w}{N\\Delta} \\end{pmatrix} } $$"
        },
        {
            "introduction": "神经网络产生无约束的输出，但许多物理量（如扩散率或混合分数）必须遵守严格的界限。本练习深入探讨了如何选择适当的函数，将原始网络输出映射到物理上合理的范围内，这是一个关键的设计选择。您将分析和比较常用方法，权衡它们在物理可容许性、数值稳定性和训练便利性方面的影响。",
            "id": "3873105",
            "problem": "在一个环境大气环流模型中，标量示踪剂 $\\theta(\\mathbf{x}, t)$ 在平流和亚网格扩散的作用下演变，其方程为 $\\partial_t \\theta = \\nabla \\cdot \\left(K(\\mathbf{x}, t)\\nabla \\theta\\right) + S(\\mathbf{x}, t)$，其中 $K(\\mathbf{x}, t)$ 是涡动扩散系数，$S(\\mathbf{x}, t)$ 是源项。菲克扩散意味着通量为 $\\mathbf{J} = -K\\nabla \\theta$。抛物型算子的适定性和热力学第二定律要求 $K \\ge 0$，以避免反扩散行为。在许多闭合方案中，一个分数混合或夹带系数 $\\alpha(\\mathbf{x}, t)$ 将两个趋势项 $T_1(\\mathbf{x}, t)$ 和 $T_2(\\mathbf{x}, t)$ 组合为 $T = \\alpha T_1 + (1-\\alpha)T_2$。为了保证凸性和稳定性，物理上的可容许性要求 $\\alpha \\in [0, 1]$。\n\n假设一个人工神经网络 (ANN) 产生无约束的预激活值 $z_K \\in \\mathbb{R}$ 和 $z_\\alpha \\in \\mathbb{R}$，分别用于参数化 $K$ 和 $\\alpha$。设计必须将 $z_K$ 和 $z_\\alpha$ 映射到物理上允许的范围内，同时平衡优化和数值方面的考量。具体而言，该设计应：\n- 强制 $K \\ge 0$ 且 $\\alpha \\in [0,1]$，\n- 避免产生可能在时间积分中引入数值刚度的极大 $K$ 值，\n- 减少梯度饱和，以支持在 $z_K$ 和 $z_\\alpha$ 的典型范围内进行稳定训练，\n- 保持基于梯度的学习所需的可微性。\n\n令 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ 表示 logistic sigmoid 函数，$\\operatorname{softplus}(u) = \\log\\!\\big(1 + e^{u}\\big)$ 表示 softplus 函数。考虑以下候选参数化方案及其相关的权衡。选择最能满足上述所有四个设计目标的选项。\n\nA. $K = \\exp(z_K)$ 且 $\\alpha = \\sigma(z_\\alpha)$。这保证了 $K \\ge 0$ 和 $\\alpha \\in [0, 1]$。然而，对于类高斯分布的 $z_K$，$K$ 近似呈对数正态分布，可能产生重尾和非常大的 $K$ 值。梯度 $\\partial K / \\partial z_K = \\exp(z_K)$ 和 $\\partial \\alpha / \\partial z_\\alpha = \\sigma(z_\\alpha)\\big(1 - \\sigma(z_\\alpha)\\big)$ 在 $|z_\\alpha|$ 较大时会饱和，在 $z_K$ 为较大正数时会爆炸。\n\nB. $K = \\operatorname{softplus}(z_K) + K_{\\min}$，其中 $K_{\\min}  0$，且 $\\alpha = \\operatorname{clip}(z_\\alpha, 0, 1)$，这里的 $\\operatorname{clip}$ 函数将值硬性限制在区间 $[0,1]$ 内。这强制了 $K \\ge K_{\\min}  0$ 和 $\\alpha \\in [0, 1]$。softplus 的增长比指数函数缓和，但裁剪操作使得在 $[0,1]$ 之外 $\\partial \\alpha / \\partial z_\\alpha = 0$，并且在边界处无定义，这破坏了基于梯度的学习，并可能使训练产生偏差。\n\nC. $K = \\exp(z_K) + K_{\\min}$，其中 $K_{\\min}  0$，且 $\\alpha = \\tanh(z_\\alpha)$。虽然 $K \\ge K_{\\min}  0$ 成立，但 $\\alpha = \\tanh(z_\\alpha) \\in (-1, 1)$ 不能保证 $\\alpha \\in [0, 1]$，因此违反了凸性混合的要求，除非引入一个额外的到 $[0,1]$ 的仿射映射，而这里没有。\n\nD. $K = \\epsilon + \\operatorname{softplus}(z_K)$，其中 $\\epsilon  0$ 是一个小数，且 $\\alpha = \\sigma\\!\\big(z_\\alpha / \\tau\\big)$，其中温度参数 $\\tau  0$ 与 ANN 联合学习。这强制了 $K \\ge \\epsilon  0$ 和 $\\alpha \\in [0, 1]$。对于较大的正数 $z_K$，softplus 产生近线性的增长，相对于 $\\exp$ 减轻了极端的 $K$ 值和刚度问题，同时保留了平滑的梯度 $\\partial K / \\partial z_K = \\sigma(z_K)$。一个可学习的 $\\tau$ 控制 sigmoid 函数的锐度，以减少在典型 $z_\\alpha$ 范围内的饱和现象，从而改善优化过程。",
            "solution": "问题要求从无约束的神经网络输出 $z_K \\in \\mathbb{R}$ 和 $z_\\alpha \\in \\mathbb{R}$ 中，为涡动扩散系数 $K$ 和混合系数 $\\alpha$ 选择最佳的参数化方案。该选择必须基于四个设计目标：\n1.  **物理可容许性**：强制约束 $K \\ge 0$ 和 $\\alpha \\in [0, 1]$。\n2.  **数值稳定性**：避免可能导致数值刚度的极大 $K$ 值。\n3.  **优化稳定性**：减少梯度饱和或爆炸，以促进稳定的训练。\n4.  **可微性**：确保参数化函数对于基于梯度的优化方法是可微的。\n\n让我们根据这四个标准来分析每个选项。\n\n### 逐项分析\n\n**A. $K = \\exp(z_K)$ 和 $\\alpha = \\sigma(z_\\alpha)$**\n\n1.  **物理可容许性**：\n    -   对于任何 $z_K \\in \\mathbb{R}$，$K = \\exp(z_K)$ 的值域总是 $(0, \\infty)$，因此满足条件 $K \\ge 0$。\n    -   logistic sigmoid 函数 $\\sigma(z_\\alpha) = \\frac{1}{1 + e^{-z_\\alpha}}$ 将任何 $z_\\alpha \\in \\mathbb{R}$ 映射到开区间 $(0, 1)$。这个范围是所需闭区间 $[0, 1]$ 的子集，因此满足条件 $\\alpha \\in [0, 1]$。\n    -   此选项在物理上是可容许的。\n\n2.  **数值稳定性**：\n    -   指数函数 $\\exp(z_K)$ 在 $z_K$ 为正时增长极快。即使神经网络输出中等大小的 $z_K$，也可能导致 $K$ 值大到天文数字。例如，如果 $z_K = 10$，$K \\approx 2.2 \\times 10^4$。如此大的扩散系数值会急剧减小显式时间积分方案的稳定时间步长（$\\Delta t \\propto 1/K$），导致严重的数值刚度。这未能满足第二个设计目标。\n\n3.  **优化稳定性**：\n    -   $K$ 的梯度为 $\\frac{\\partial K}{\\partial z_K} = \\frac{\\partial}{\\partial z_K} \\exp(z_K) = \\exp(z_K)$。对于较大的正数 $z_K$，该梯度可能变得非常大，导致训练过程中的“梯度爆炸”问题。\n    -   $\\alpha$ 的梯度为 $\\frac{\\partial \\alpha}{\\partial z_\\alpha} = \\sigma(z_\\alpha)(1 - \\sigma(z_\\alpha))$。对于较大的 $|z_\\alpha|$，该梯度趋近于 $0$，这就是众所周知的 sigmoid 函数的“梯度消失”或“梯度饱和”问题，它会阻碍学习。\n    -   此选项带来了显著的优化挑战。\n\n4.  **可微性**：\n    -   $\\exp(u)$ 和 $\\sigma(u)$ 都是无穷次可微（$C^\\infty$）函数。此标准得到满足。\n\n*A 选项的结论*：**不正确**。虽然在物理上可容许且可微，但这种参数化方案容易因大的 $K$ 值而导致数值不稳定，并因梯度爆炸和梯度消失而导致优化不稳定。\n\n**B. $K = \\operatorname{softplus}(z_K) + K_{\\min}$，其中 $K_{\\min}  0$，且 $\\alpha = \\operatorname{clip}(z_\\alpha, 0, 1)$**\n\n1.  **物理可容许性**：\n    -   softplus 函数 $\\operatorname{softplus}(u) = \\log(1+e^u)$ 将 $\\mathbb{R}$ 映射到 $(0, \\infty)$。加上一个正常数 $K_{\\min}  0$ 后，$K = \\operatorname{softplus}(z_K) + K_{\\min}$ 总是大于 $K_{\\min}$，因此满足 $K \\ge 0$。\n    -   根据定义，$\\operatorname{clip}(u, 0, 1)$ 函数将其输出限制在闭区间 $[0, 1]$ 内。因此，满足 $\\alpha \\in [0, 1]$。\n    -   此选项在物理上是可容许的。\n\n2.  **数值稳定性**：\n    -   对于较大的正数 $u$，$\\operatorname{softplus}(u) \\approx u$。因此，$K$ 随 $z_K$ 线性增长，而非指数增长。与选项 A 相比，这是一种更受控制的行为，大大降低了产生极大 $K$ 值的风险，从而提高了数值稳定性。\n\n3.  **优化稳定性**：\n    -   $K$ 的梯度为 $\\frac{\\partial K}{\\partial z_K} = \\frac{\\partial}{\\partial z_K} \\log(1+e^{z_K}) = \\frac{e^{z_K}}{1+e^{z_K}} = \\sigma(z_K)$。该梯度被限制在 $(0, 1)$ 内，防止了 $K$ 参数化中的梯度爆炸和梯度消失问题。这对优化非常有利。\n    -   然而，$\\alpha$ 的梯度存在问题。$\\operatorname{clip}(z_\\alpha, 0, 1)$ 相对于 $z_\\alpha$ 的梯度在 $z_\\alpha  0$ 和 $z_\\alpha > 1$ 时为 $0$。如果网络输出 $z_\\alpha$ 落入这些区域，梯度信息将无法回传到网络，负责该输出的权重也无法更新。这个“死亡梯度”问题严重损害了学习过程。\n\n4.  **可微性**：\n    -   裁剪函数在点 $z_\\alpha = 0$ 和 $z_\\alpha = 1$ 处不可微。在这些点上缺少明确定义的梯度，违反了许多基于梯度的优化器对可微性的要求。\n\n*B 选项的结论*：**不正确**。使用 `clip` 函数引入了不可微性，并在目标区间外导致灾难性的梯度饱和（梯度为零），使其不适用于稳健的基于梯度的学习。\n\n**C. $K = \\exp(z_K) + K_{\\min}$，其中 $K_{\\min}  0$，且 $\\alpha = \\tanh(z_\\alpha)$**\n\n1.  **物理可容许性**：\n    -   对于 $K$，将 $K_{\\min}0$ 加到 $\\exp(z_K)$ 上确保了 $K  K_{\\min}  0$。满足条件 $K \\ge 0$。\n    -   双曲正切函数 $\\tanh(z_\\alpha)$ 的值域是 $(-1, 1)$。这个范围不包含在物理上要求的区间 $[0, 1]$ 内。对于凸性混合方案，$\\alpha$ 的负值在物理上是无效的。\n    -   此选项在 $\\alpha$ 的物理可容许性测试上存在致命缺陷。\n\n2.  **数值稳定性**：\n    -   使用 $\\exp(z_K)$，因此它与选项 A 存在相同的数值刚度问题。\n\n3.  **优化稳定性**：\n    -   使用 $\\exp(z_K)$，因此它与选项 A 有相同的梯度爆炸可能性。$\\tanh(z_\\alpha)$ 的梯度是 $1 - \\tanh^2(z_\\alpha)$，在 $|z_\\alpha|$ 较大时会饱和，类似于 sigmoid 函数。\n\n4.  **可微性**：\n    -   $\\exp(u)$ 和 $\\tanh(u)$ 都是 $C^\\infty$ 函数。\n\n*C 选项的结论*：**不正确**。此选项根本上是无效的，因为它不能保证物理约束 $\\alpha \\in [0, 1]$。\n\n**D. $K = \\epsilon + \\operatorname{softplus}(z_K)$，其中 $\\epsilon  0$ 是一个小数，且 $\\alpha = \\sigma\\!\\big(z_\\alpha / \\tau\\big)$，其中温度参数 $\\tau  0$**\n\n1.  **物理可容许性**：\n    -   正如在选项 B 的分析中确立的，函数 $\\epsilon + \\operatorname{softplus}(z_K)$ 确保 $K  \\epsilon  0$，因此满足 $K \\ge 0$。\n    -   正如在选项 A 的分析中确立的，sigmoid 函数 $\\sigma(\\cdot)$ 确保其输出在 $(0, 1)$ 内，因此满足 $\\alpha \\in [0, 1]$。\n    -   此选项在物理上是可容许的。\n\n2.  **数值稳定性**：\n    -   与选项 B 一样，使用 $\\operatorname{softplus}(z_K)$ 确保了对于较大的正数 $z_K$，$K$ 呈近线性增长。这有效地减轻了极大 $K$ 值及其相关的数值刚度风险，满足了第二个设计目标。\n\n3.  **优化稳定性**：\n    -   $K$ 的梯度为 $\\frac{\\partial K}{\\partial z_K} = \\sigma(z_K)$，其值被限制在 $(0, 1)$ 内，提供了极佳的优化稳定性。\n    -   $\\alpha$ 的梯度为 $\\frac{\\partial \\alpha}{\\partial z_\\alpha} = \\frac{1}{\\tau} \\sigma(z_\\alpha / \\tau)(1 - \\sigma(z_\\alpha / \\tau))$。温度参数 $\\tau$ 缩放了 sigmoid 函数的输入。较大的 $\\tau$ 会“拉伸”函数，使从 $0$ 到 $1$ 的过渡更加平缓。这种对活跃区域（梯度不容忽略的区域）的拓宽有助于在更宽的 $z_\\alpha$ 值范围内防止梯度饱和。将 $\\tau$ 设为可学习参数，允许模型在训练期间动态调整过渡的锐度以找到最佳平衡点。这直接解决了梯度饱和问题。\n    -   此选项为两个参数的优化稳定性提供了稳健的解决方案。\n\n4.  **可微性**：\n    -   $\\operatorname{softplus}(u)$ 和 $\\sigma(u)$ 都是 $C^\\infty$ 函数。整个参数化方案是平滑且可微的。\n\n*D 选项的结论*：**正确**。此选项成功满足了所有四个设计目标。它使用平滑、可微的函数来确保物理约束。它通过对 $K$ 使用 softplus 函数来防止数值刚度。它通过对 $K$ 使用有界梯度的 softplus 和对 $\\alpha$ 使用温度缩放的 sigmoid 来缓解梯度饱和，从而促进了稳定的优化。\n\n### 结论\n\n比较四个选项，选项 D 无疑是最佳的。它采用了先进的现代技术，在神经网络的背景下对有物理约束的变量进行参数化，以一种有原则的方式解决了所有指定的设计挑战。选项 A、B 和 C 各自至少有一个致命缺陷，使它们不适用。",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "物理系统受制于关联多个变量的基本守恒定律。本练习以地表能量平衡为例，探讨了多种迫使神经网络遵守此类定律的方法。您将评估不同的策略，从在训练目标中添加惩罚项到修改网络架构，以理解如何将物理守恒原理直接嵌入到学习过程中。",
            "id": "3873150",
            "problem": "您正在使用神经网络 $f_{\\theta}$ 训练一个数据驱动的参数化方案，以根据气象特征 $x \\in \\mathbb{R}^{p}$ 和陆面状态变量来预测地表感热通量 $H$、潜热通量 $LE$ 和地表热通量 $G$。训练数据提供了净辐射 $R_{n}$ 和通量 $H, LE, G$，根据陆面能量守恒定律，地表能量收支闭合成立：$R_{n} = H + LE + G$。目标是确保学习到的模型在训练和推理过程中不违反此收支平衡，同时与带噪声的观测数据保持一致，并能进行可微优化。\n\n使用能量守恒这一基本物理定律 $R_{n} = H + LE + G$ 和标准的约束优化原理，选择所有正确指定了约束设计并描述了在训练 $f_{\\theta}$ 期间强制执行该约束的有效方法的选项：\n\nA. 通过向数据拟合目标函数中添加一个二次的收支违背惩罚项来引入软约束。对于预测 $(\\hat{H}, \\hat{LE}, \\hat{G}) = f_{\\theta}(x)$，最小化损失函数\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\left[\\left(\\hat{H}_{i}-H_{i}\\right)^{2}+\\left(\\hat{LE}_{i}-LE_{i}\\right)^{2}+\\left(\\hat{G}_{i}-G_{i}\\right)^{2}\\right] + \\lambda\\,\\frac{1}{N}\\sum_{i=1}^{N}\\left(R_{n,i}-\\hat{H}_{i}-\\hat{LE}_{i}-\\hat{G}_{i}\\right)^{2},\n$$\n其中 $\\lambda0$ 用于平衡数据拟合和收支闭合，并通过随机梯度下降 (SGD) 优化 $\\theta$。这鼓励模型在期望意义上满足收支平衡，同时允许测量噪声的存在。\n\nB. 通过对输出进行重参数化，使其位于由收支平衡定义的仿射子空间上，从而施加硬约束。让网络生成一个无约束向量 $\\tilde{y}_{i} = [\\tilde{H}_{i},\\tilde{LE}_{i},\\tilde{G}_{i}]^{\\top}$，并定义有约束的预测\n$$\n\\begin{aligned}\ny_{i} = [\\hat{H}_{i},\\hat{LE}_{i},\\hat{G}_{i}]^{\\top} \\\\\n= \\tilde{y}_{i} - \\frac{1}{3}\\left(\\mathbf{1}^{\\top}\\tilde{y}_{i}-R_{n,i}\\right)\\mathbf{1}, \\quad \\text{其中 } \\mathbf{1} = [1,1,1]^{\\top}.\n\\end{aligned}\n$$\n通过使用 SGD 最小化 $y_{i}$ 和 $[H_{i},LE_{i},G_{i}]^{\\top}$ 之间的均方误差 (MSE) 来进行训练。这保证了对于每一次前向传播，都有 $R_{n,i} = \\hat{H}_{i}+\\hat{LE}_{i}+\\hat{G}_{i}$ 精确成立。\n\nC. 使用增广拉格朗日 (AL) 方法将问题表述为等式约束学习。定义约束 $c_{i}(\\theta) = R_{n,i} - \\hat{H}_{i} - \\hat{LE}_{i} - \\hat{G}_{i} = 0$ 和增广拉格朗日函数\n$$\n\\mathcal{L}_{\\text{AL}}(\\theta,\\mu) = \\frac{1}{N}\\sum_{i=1}^{N}\\left[\\left(\\hat{H}_{i}-H_{i}\\right)^{2}+\\left(\\hat{LE}_{i}-LE_{i}\\right)^{2}+\\left(\\hat{G}_{i}-G_{i}\\right)^{2}\\right] + \\frac{1}{N}\\sum_{i=1}^{N}\\left[\\mu\\,c_{i}(\\theta) + \\frac{\\rho}{2}\\,c_{i}(\\theta)^{2}\\right],\n$$\n其中 $\\mu$ 是对偶变量，$\\rho0$ 是惩罚参数。交替进行对 $\\theta$ 的梯度下降步骤以减小 $\\mathcal{L}_{\\text{AL}}$，并更新 $\\mu \\leftarrow \\mu + \\rho\\,\\frac{1}{N}\\sum_{i=1}^{N}c_{i}(\\theta)$，直到约束残差 $\\{c_{i}\\}$ 消失。\n\nD. 仅在训练后应用后处理重缩放，方法是在验证集上将 $(\\hat{H},\\hat{LE},\\hat{G})$ 替换为 $R_{n}\\cdot(\\hat{H},\\hat{LE},\\hat{G})/\\left(\\hat{H}+\\hat{LE}+\\hat{G}\\right)$。这在不改变训练目标或梯度的情况下强制执行了收支平衡，因此保留了学习到的参数。\n\nE. 在输出层插入批量归一化 (Batch Normalization)，以在小批量 (mini-batch) 数据上对 $(\\hat{H},\\hat{LE},\\hat{G})$ 进行归一化。因为归一化改变了输出的尺度，其和将在期望上匹配 $R_{n}$，从而在训练期间强制执行收支平衡。\n\n选择所有适用的选项。",
            "solution": "### 第一步：提取已知信息\n- 神经网络模型：$f_{\\theta}$\n- 模型输入：气象特征 $x \\in \\mathbb{R}^{p}$ 和陆面状态变量。\n- 模型输出（预测）：地表感热通量 $\\hat{H}$、潜热通量 $\\hat{LE}$ 和地表热通量 $\\hat{G}$。\n- 训练数据提供：净辐射 $R_{n}$ 和观测通量 $H, LE, G$。\n- 物理定律（约束）：地表能量收支闭合，$R_{n} = H + LE + G$。这对数据成立。\n- 目标：确保学习到的模型的预测在训练和推理期间满足收支平衡，即 $\\hat{H} + \\hat{LE} + \\hat{G} = R_{n}$。\n- 方法论约束：解决方案必须与带噪声的观测数据一致，并适用于可微优化（例如，使用随机梯度下降，SGD）。\n- 问题：选择所有正确指定了约束设计和在训练期间强制执行该约束的有效方法的选项。\n\n### 第二步：使用提取的已知信息进行验证\n- **科学依据：** 该问题基于应用于地球表面的能量守恒原理，这是环境物理学和气候科学的基石。方程 $R_{n} = H + LE + G$ 是地表能量平衡的标准表示。将此类物理定律嵌入机器学习模型的任务是物理信息机器学习中一个关键且活跃的研究领域。该问题在科学上是合理的。\n- **适定性：** 问题定义清晰。它提出了一个特定的、可形式化的线性等式约束（$\\hat{H} + \\hat{LE} + \\hat{G} = R_{n}$），需要施加在神经网络的输出上。它要求在基于梯度的优化背景下评估实现这一目标的标准方法。这种结构允许对所提出的技术进行唯一且有意义的分析。\n- **客观性：** 问题陈述使用精确的数学和技术语言表达，没有歧义或主观论断。所有变量和目标都得到了明确定义。\n\n### 第三步：结论与行动\n问题陈述是有效的。它具有科学依据、适定性、客观性，并描述了数据驱动的地球系统建模中的一个现实挑战。我将继续分析每个选项。\n\n### 基于原理的推导与选项分析\n核心任务是在训练神经网络 $f_{\\theta}$（该网络预测 $(\\hat{H}_i, \\hat{LE}_i, \\hat{G}_i) = f_{\\theta}(x_i)$）的过程中，对每个数据样本 $i$ 强制执行线性等式约束 $\\hat{H}_i + \\hat{LE}_i + \\hat{G}_i = R_{n,i}$。这些方法必须与基于梯度的优化兼容。\n\n**A. 通过向数据拟合目标函数中添加一个二次的收支违背惩罚项来引入软约束...**\n该选项提出了惩罚法，这是约束优化中的一种经典技术。损失函数由两项组成：数据拟合项（均方误差，MSE）和约束违背项。\n$$\n\\mathcal{L}(\\theta) = \\underbrace{\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\left(\\hat{H}_{i}-H_{i}\\right)^{2}+\\left(\\hat{LE}_{i}-LE_{i}\\right)^{2}+\\left(\\hat{G}_{i}-G_{i}\\right)^{2}\\right]}_{\\text{数据拟合 (MSE)}} + \\underbrace{\\lambda\\,\\frac{1}{N}\\sum_{i=1}^{N}\\left(R_{n,i}-\\hat{H}_{i}-\\hat{LE}_{i}-\\hat{G}_{i}\\right)^{2}}_{\\text{约束惩罚}}\n$$\n只要 $f_{\\theta}$ 是可微的，这个复合损失函数对模型参数 $\\theta$ 就是可微的。因此，它可以使用 SGD 或其变体进行优化。超参数 $\\lambda  0$ 控制惩罚的强度。较大的 $\\lambda$ 会迫使模型优先满足能量收支。这种方法不能保证每次预测都精确满足约束，但它“鼓励”模型学习接近约束曲面的解。在处理收支可能无法完美闭合的带噪数据时，这种灵活性可能是有利的。所给的描述准确地刻画了该方法的行为和效用。\n**结论：正确**\n\n**B. 通过对输出进行重参数化，使其位于由收支平衡定义的仿射子空间上，从而施加硬约束...**\n该选项描述了一种通过结构设计来强制执行约束的方法。网络首先产生一个无约束的中间输出 $\\tilde{y}_{i} = [\\tilde{H}_{i},\\tilde{LE}_{i},\\tilde{G}_{i}]^{\\top}$。然后将此向量正交投影到由约束 $\\hat{H}_{i}+\\hat{LE}_{i}+\\hat{G}_{i} = R_{n,i}$ 定义的仿射子空间上。所提供的公式 $y_{i} = \\tilde{y}_{i} - \\frac{1}{3}\\left(\\mathbf{1}^{\\top}\\tilde{y}_{i}-R_{n,i}\\right)\\mathbf{1}$ 正是该投影的公式。我们来验证一下：\n最终输出 $y_i$ 的分量之和为：\n$$\n\\mathbf{1}^{\\top}y_{i} = \\mathbf{1}^{\\top}\\left( \\tilde{y}_{i} - \\frac{1}{3}\\left(\\mathbf{1}^{\\top}\\tilde{y}_{i}-R_{n,i}\\right)\\mathbf{1} \\right)\n$$\n$$\n= \\mathbf{1}^{\\top}\\tilde{y}_{i} - \\frac{1}{3}\\left(\\mathbf{1}^{\\top}\\tilde{y}_{i}-R_{n,i}\\right)(\\mathbf{1}^{\\top}\\mathbf{1})\n$$\n由于 $\\mathbf{1} = [1,1,1]^{\\top}$，所以 $\\mathbf{1}^{\\top}\\mathbf{1} = 1^2 + 1^2 + 1^2 = 3$。\n$$\n= \\mathbf{1}^{\\top}\\tilde{y}_{i} - \\frac{1}{3}\\left(\\mathbf{1}^{\\top}\\tilde{y}_{i}-R_{n,i}\\right)(3)\n$$\n$$\n= \\mathbf{1}^{\\top}\\tilde{y}_{i} - \\left(\\mathbf{1}^{\\top}\\tilde{y}_{i}-R_{n,i}\\right) = R_{n,i}\n$$\n对于任何 $\\tilde{y}_{i}$，约束都得到精确满足。这个投影是一个可微操作，因此从输入 $x_i$ 到最终有约束输出 $y_i$ 的整个模型保持端到端可微。然后可以通过最小化有约束预测 $y_i$ 和观测通量之间的均方误差来训练模型。如其所述，该方法强制执行了一个在每次前向传播中都成立的“硬”约束。\n**结论：正确**\n\n**C. 使用增广拉格朗日 (AL) 方法将问题表述为等式约束学习...**\n该选项建议使用增广拉格朗日方法，这是约束优化文献中一种复杂而强大的算法。AL 函数结合了数据拟合目标、拉格朗日乘子项和二次惩罚项。\n$$\n\\mathcal{L}_{\\text{AL}}(\\theta,\\mu) = \\text{MSE} + \\frac{1}{N}\\sum_{i=1}^{N}\\left[\\mu\\,c_{i}(\\theta) + \\frac{\\rho}{2}\\,c_{i}(\\theta)^{2}\\right]\n$$\n其中 $c_{i}(\\theta) = R_{n,i} - \\hat{H}_{i} - \\hat{LE}_{i} - \\hat{G}_{i}$ 是约束残差。训练过程涉及一个迭代的两步过程：\n1. 对于固定的对偶变量 $\\mu$ 和惩罚参数 $\\rho  0$，关于模型参数 $\\theta$ 最小化 $\\mathcal{L}_{\\text{AL}}$。在深度学习中，这通常通过一定数量的 SGD 步骤完成。\n2. 使用对偶上升步骤更新对偶变量（拉格朗日乘子）$\\mu$：$\\mu \\leftarrow \\mu + \\rho\\,\\langle c_i(\\theta) \\rangle$。\n重复此过程直至收敛。与选项 A 中的简单惩罚法相比，AL 方法可以在不要求惩罚参数 $\\rho$ 趋于无穷大的情况下实现精确的约束满足。所描述的公式和更新规则是该方法的标准和正确形式。这是在训练期间强制执行约束的一种有效且稳健的方法。\n**结论：正确**\n\n**D. 仅在训练后应用后处理重缩放...**\n该选项建议在没有任何物理约束的情况下训练模型（即仅最小化 MSE），然后在训练完成后应用校正。校正步骤涉及将预测通量 $(\\hat{H}, \\hat{LE}, \\hat{G})$ 乘以因子 $R_{n}/\\left(\\hat{H}+\\hat{LE}+\\hat{G}\\right)$。虽然此过程确实在最终输出上强制执行了收支平衡，但它违反了问题陈述的一个关键要求：“确保学习到的模型在**训练期间**不违反此收支平衡...”。此方法明确地将训练与约束执行分离开来。模型参数 $\\theta$ 的学习没有受到物理约束的任何指导。这可能导致模型学习到物理上不一致的关系，并且最终的后处理校正可能是一个巨大的、扭曲性的调整。由于约束不是在*训练期间*强制执行的，因此该方法不是所提问题的正确答案。\n**结论：错误**\n\n**E. 在输出层插入批量归一化...**\n该选项建议在输出层使用批量归一化 (BN) 来强制执行约束。这暴露了对 BN 工作原理的根本误解。BN 是在*小批量样本之间*，对每个特征进行操作的。例如，它会归一化一个批次中所有 $\\hat{H}$ 值的向量，所有 $\\hat{LE}$ 值的向量，以及所有 $\\hat{G}$ 值的向量，每个都是独立进行的。它不会对单个样本的特征之间进行操作。标准 BN 算法内部没有机制来为每个样本 $i$ 强制执行像 $\\hat{H}_{i}+\\hat{LE}_{i}+\\hat{G}_{i} = R_{n,i}$ 这样的线性关系。“其和将在期望上匹配 $R_{n}$”的说法是毫无根据的。BN 归一化的是一个批次上每个特征分布的一阶矩和二阶矩，这与满足每个单独预测的代数和约束无关。\n**结论：错误**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}