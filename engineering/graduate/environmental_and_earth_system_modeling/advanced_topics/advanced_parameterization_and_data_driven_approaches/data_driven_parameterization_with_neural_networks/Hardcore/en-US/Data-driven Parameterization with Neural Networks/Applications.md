## Applications and Interdisciplinary Connections

The preceding chapters have established the core principles and mechanisms of data-driven parameterization, focusing on the architecture and training of neural networks to represent unresolved physical processes. This chapter shifts the focus from theory to practice, exploring how these foundational concepts are applied to solve concrete problems across a spectrum of scientific and engineering disciplines. Our goal is not to re-teach the fundamentals, but to demonstrate their utility, versatility, and integration into the complex workflows of modern computational science. Through a curated set of examples, we will see how the abstract principles of physics-informed machine learning are translated into tangible solutions for modeling the atmosphere, oceans, land surface, and beyond. This journey will illuminate not only the power of data-driven methods but also the critical importance of domain knowledge and physical reasoning in their successful application.

### Core Applications in Earth System Modeling

Earth System Models (ESMs) are a natural and highly active domain for data-driven parameterization due to their multiscale nature and the computational expense of their traditional closure schemes. Neural networks offer a promising path to both accelerate these models and improve their fidelity by learning from high-resolution data.

#### Atmospheric Processes

The atmosphere is a chaotic, turbulent fluid where critical processes like cloud formation and [energy transport](@entry_id:183081) occur at scales far smaller than a global model grid can resolve. Parameterizing these subgrid processes is a central challenge in weather and [climate prediction](@entry_id:184747).

A canonical example is the parameterization of turbulent heat fluxes in the atmospheric boundary layer. High-fidelity Large-Eddy Simulations (LES) can resolve these fluxes, providing a "ground truth" for training a neural network. However, a naive application of machine learning can fail spectacularly. A successful parameterization must be designed with careful consideration of the physics. The inputs to the neural network must be variables available in the coarse-resolution host model at the time of inference, such as resolved-scale vertical gradients of temperature and wind, surface properties, and radiative fluxes. Crucially, one must avoid "target leakage" or circular reasoning, where inputs are used that would not be known without first knowing the quantity being predicted. For instance, using the true [friction velocity](@entry_id:267882) $u_{*}$ or Obukhov length $L$, which are calculated from the turbulent fluxes themselves, as inputs to predict those same fluxes is a critical design flaw. A robust data-driven model should learn the mapping from the mean state to the subgrid fluxes, and can even be trained to predict the entire vertical profile of the flux, providing a more physically complete closure than traditional schemes that only predict the surface value .

Another computationally demanding component of [atmospheric models](@entry_id:1121200) is the radiative transfer code, which calculates the heating and cooling of the atmosphere by solar and thermal radiation. These codes solve the complex, spectrally-dependent [radiative transfer equation](@entry_id:155344). Neural networks can be trained to emulate these codes, acting as fast and accurate surrogates. This is possible because, according to the Universal Approximation Theorem, a neural network can approximate any continuous functional. The mapping from the atmospheric state to the layerwise radiative heating rates is such a functional. To succeed, the network must be provided with a physically complete set of input features that fully determine the radiative transfer. This includes the vertical profiles of temperature, pressure, all radiatively significant gases (e.g., water vapor, CO$_2$, ozone), cloud properties (including fraction, condensate path, and particle size), and relevant aerosol properties, as well as the surface temperature and emissivity. Omitting any of these first-order physical drivers would render the learning problem ill-posed and lead to an inaccurate emulator .

#### Oceanic Dynamics

Similar challenges exist in ocean modeling. Submesoscale motions, such as [oceanic fronts](@entry_id:1129041) and filaments with scales of $1$ to $10$ kilometers, are rich in energy and play a key role in the transport and mixing of heat, carbon, and nutrients. As these scales are unresolved in global ocean models, their effects must be parameterized. A data-driven parameterization for submesoscale mixing must be built upon physically meaningful and invariant features. The laws of fluid dynamics are independent of the observer's reference frame, a property known as Galilean invariance. Therefore, inputs to the parameterization should not be raw velocities, but rather their [spatial derivatives](@entry_id:1132036) or quantities derived from them. For submesoscale instabilities, the key controlling variables are related to buoyancy and rotation. Suitable scalar, Galilean-invariant features include the magnitude of the horizontal buoyancy gradient, $M^2 = |\nabla_h b|$, which measures frontal strength; the vertical stratification, measured by the [buoyancy frequency](@entry_id:1121933) squared, $N^2 = \partial b / \partial z$; and the Ertel Potential Vorticity (PV), $q$. A particularly insightful feature is a dimensionless form of PV, such as $\tilde{q} = q/(f N^2)$, which is directly related to the criteria for [symmetric instability](@entry_id:1132736), a potent mixing process in frontal regions .

#### Land Surface Processes

The interface between the land and the atmosphere is governed by complex biological and physical processes. One such process is the regulation of water vapor and carbon dioxide exchange by [plant stomata](@entry_id:153552). The stomatal conductance, $g_s$, is known from decades of ecophysiological research to have specific qualitative behaviors: it is a positive quantity that increases with incoming photosynthetically active radiation ($I$) and decreases as the air becomes drier (i.e., as vapor pressure deficit, $D$, increases). A data-driven parameterization for $g_s$ must respect these known physical constraints to be scientifically realistic. This can be achieved by building the constraints directly into the neural network's architecture and feature engineering. For example, to enforce the correct monotonic responses, one can transform the raw physical inputs into features that are non-decreasing with respect to the desired output. A feature like $x_R = I/(I + I_0)$ is monotonically increasing with $I$, while $x_D = -\ln(D + D_\epsilon)$ is monotonically decreasing with $D$. If a neural network is then constructed using only non-decreasing [activation functions](@entry_id:141784) (like ReLU or softplus) and all its weights are constrained to be non-negative, the entire network becomes a composition of non-decreasing functions. When fed with the engineered features, the network output is guaranteed to be non-decreasing with respect to $I$ and non-increasing with respect to $D$, satisfying the physical constraints by construction .

### Broadening the Scope: Interdisciplinary Frontiers

The principles of data-driven parameterization extend far beyond Earth system science. The strategy of using neural networks to learn unresolved physics from data is being applied across a vast range of disciplines, from [engineering mechanics](@entry_id:178422) to biology.

#### Computational Mechanics and Materials Science

In [computational combustion](@entry_id:1122776), modeling the Reynolds stress tensor, $\tau_{ij}$, is a central challenge for Reynolds-Averaged Navier-Stokes (RANS) simulations. A fundamental physical principle is objectivity, or frame invariance, which dictates that the constitutive law for a material should not depend on the coordinate system used to describe it. A standard neural network that takes the components of the mean velocity gradient tensor as input will violate this principle. A more sophisticated architecture, the Tensor Basis Neural Network (TBNN), is designed to enforce it. The theory of [isotropic tensor](@entry_id:189108) functions states that any frame-invariant relationship between two tensors can be expressed as a [linear combination](@entry_id:155091) of a standardized set of basis tensors, where the coefficients are scalar functions of the [scalar invariants](@entry_id:193787) of the input tensor. A TBNN operationalizes this by having a neural network learn the scalar coefficient functions from a set of [scalar invariants](@entry_id:193787) of the mean strain-rate and rotation-rate tensors. This structure guarantees, by construction, that the resulting [turbulence model](@entry_id:203176) is frame-invariant. Furthermore, physical [realizability](@entry_id:193701)—the constraint that the Reynolds stress tensor must be positive semi-definite—can be enforced through penalty terms in the loss function during training .

This paradigm of learning physics-constrained constitutive models is also prevalent in solid mechanics. For instance, in modeling the viscoplastic behavior of metals, the material model must be consistent with the second law of thermodynamics, which requires that the rate of dissipation be non-negative. In the framework of generalized standard materials, this is ensured if the dissipation potential is a convex function of its arguments. A neural network can be trained to represent the mapping from overstress (the driving force for plastic flow) to the plastic strain rate. By constraining the network's architecture—for example, by using non-negative weights and ReLU activations—one can guarantee that the learned mapping is a monotone [non-decreasing function](@entry_id:202520). The integral of a [monotone function](@entry_id:637414) is always convex. Therefore, by enforcing [monotonicity](@entry_id:143760) on the learned [rate law](@entry_id:141492), we automatically guarantee the convexity of the corresponding dual dissipation potential, ensuring [thermodynamic consistency](@entry_id:138886) of the data-driven material model .

#### Systems Biology and Chemical Kinetics

In [systems biology](@entry_id:148549), a primary goal is to understand the dynamics of complex [biochemical networks](@entry_id:746811), such as metabolic pathways. Often, the precise mathematical forms of the enzyme [kinetic rate laws](@entry_id:1126935) are unknown. Neural Ordinary Differential Equations (Neural ODEs) offer a powerful framework for this "gray-box" modeling problem. Instead of assuming a specific functional form for the reaction rates (e.g., Michaelis-Menten kinetics), a Neural ODE parameterizes the entire right-hand-side of the system of differential equations with a neural network: $\dot{\mathbf{y}} = f_{NN}(\mathbf{y}, t; \theta)$, where $\mathbf{y}$ is the vector of metabolite concentrations. By training the network to minimize the discrepancy between the ODE's solution and observed [time-series data](@entry_id:262935), the model can discover the underlying vector field of the dynamical system from data, without a priori specification of the kinetic laws .

A related but distinct task is the inverse problem of inferring unknown physical *constants* within a known model structure. In [combustion chemistry](@entry_id:202796), for example, the Arrhenius law describes the [temperature dependence of reaction rates](@entry_id:142636), but its parameters (pre-exponential factor $A$, temperature exponent $n$, and activation energy $E$) are often uncertain. A Physics-Informed Neural Network (PINN) can be configured to solve this inverse problem. Here, the state trajectories (temperature and species concentrations) are represented by neural networks, and the unknown physical parameters $(A, n, E)$ are treated as trainable variables. The network is trained by minimizing a composite loss function that penalizes both the misfit with sparse experimental data and the residual of the known governing ODEs. This approach allows for the discovery of [fundamental physical constants](@entry_id:272808) from limited data by leveraging knowledge of the underlying physical laws. Success in such problems often hinges on proper [numerical conditioning](@entry_id:136760), such as non-dimensionalizing the equations and enforcing physical constraints (e.g., $A > 0, E > 0$) through [reparameterization](@entry_id:270587) .

### Advanced Methodological Frameworks

The successful application of data-driven parameterization often requires moving beyond standard neural network architectures and embracing more sophisticated frameworks that are tailored to the structure of physical problems.

#### Hybrid Physics-ML Modeling

A guiding philosophy in [scientific machine learning](@entry_id:145555) is to "learn what you don't know." Instead of replacing an entire physics-based model with a neural network, a more robust strategy is to create a hybrid model. In this approach, the components of a model that represent fundamental, known physical laws—such as the conservation of mass, momentum, and energy—are retained in their original, physics-based form. The neural network is then used to represent only the uncertain or computationally intractable parts of the model, such as the subgrid-scale closure terms. For a system described by $\partial_t \mathbf{q} = \mathcal{F}(\mathbf{q}) + \text{(unresolved terms)}$, where $\mathcal{F}$ is the operator encoding the resolved conservation laws, the hybrid model becomes $\partial_t \mathbf{q} = \mathcal{F}(\mathbf{q}) + \mathcal{M}_{NN}(\mathbf{q})$. This strategy guarantees that the overall model respects the fundamental invariances of the system, leading to greater stability and physical realism .

The practical coupling of the neural network component, $\mathcal{M}_{NN}$, to the physical solver, $\mathcal{F}$, also has critical implications for stability. Subgrid processes are often "stiff," meaning they operate on time scales much faster than the resolved dynamics. An explicit time-stepping scheme for the neural network component, such as $x_{n+1} = x_n + \Delta t f(x_n) + \Delta t \mathcal{M}_{NN}(x_n)$, can be severely limited by the small time step required for stability. A more stable approach is to treat the stiff neural network term implicitly, solving an algebraic equation at each time step, as in an IMEX (Implicit-Explicit) scheme: $x_{n+1} = x_n + \Delta t f(x_n) + \Delta t \mathcal{M}_{NN}(x_{n+1})$. This mirrors the established practice of using implicit solvers for stiff components in traditional numerical methods and can dramatically improve the stability of the coupled hybrid model .

#### Harnessing Geometric and Graph Structures

Many physical systems are simulated on unstructured computational grids, which pose a challenge for standard [convolutional neural networks](@entry_id:178973) that expect regular, grid-like data. Graph Neural Networks (GNNs) provide a natural and powerful framework for learning on such domains. By representing the [computational mesh](@entry_id:168560) as a graph—where simulation cells are nodes and their interfaces are edges—a GNN can learn physics-aware mappings. The core GNN operation of message-passing, where information is exchanged between neighboring nodes, can be designed to directly mimic physical processes. For example, the [diffusive flux](@entry_id:748422) of a tracer between two cells in a finite-volume model is proportional to the difference in their concentrations. A GNN message function can be constructed to have this [exact form](@entry_id:273346), $ \text{message}_{ij} \propto \kappa_{ij} (h_j - h_i) $, where $h_i$ is the state at node $i$ and $\kappa_{ij}$ is a learned diffusivity. The GNN architecture thus provides a strong inductive bias that is perfectly aligned with the underlying physics of local transport on an irregular grid .

#### From Black-Box Emulation to Interpretable Discovery

While neural networks are powerful function approximators, they are often criticized for being "black boxes" that lack [interpretability](@entry_id:637759). In scientific applications, an interpretable model is often more valuable than a slightly more accurate but opaque one. This has motivated the development of methods that discover governing equations in a symbolic, human-readable form. One such method is Sparse Identification of Nonlinear Dynamics (SINDy). SINDy operates on the assumption that the dynamics of many physical systems are sparse in a large library of candidate functions (e.g., polynomials, [trigonometric functions](@entry_id:178918)). The method first constructs a large matrix $\Theta$ of these candidate functions evaluated on time-series data, and then solves a [sparse regression](@entry_id:276495) problem, $\dot{\mathbf{X}} \approx \Theta \boldsymbol{\Xi}$, to find the few non-zero coefficients in $\boldsymbol{\Xi}$ that best describe the dynamics. This yields a parsimonious differential equation, trading the universal approximation power of a neural network for interpretability and simplicity . This contrasts with the PINN framework, where the physics is not discovered but used as a constraint to help infer an unknown function, such as an eddy viscosity, from sparse data .

#### The Problem of Extrapolation and the Role of Physics

Perhaps the greatest challenge for any data-driven model is extrapolation—making predictions for inputs outside the distribution of the training data. This is a common requirement in scientific modeling, for example, when predicting the climate's response to unprecedented forcing. A simple parameter calibration approach, which fits the coefficients of a pre-supposed model structure (e.g., a power law), is highly vulnerable to *[structural error](@entry_id:1132551)*. If the assumed structure is incorrect, the model will extrapolate poorly, regardless of how well it fits the training data. A more flexible parameterization discovery approach using a neural network can, in principle, learn a more correct functional form. However, without guidance, its [extrapolation](@entry_id:175955) is unreliable. The key to improving [extrapolation](@entry_id:175955) is to embed known physical principles as inductive biases. Constraints such as conservation laws, symmetries (like frame invariance), and [thermodynamic principles](@entry_id:142232) (like non-negative dissipation or [monotonicity](@entry_id:143760)) are universally valid. By building these constraints into the model architecture and training process, we reduce the [hypothesis space](@entry_id:635539) to physically plausible functions, making the model more robust and its extrapolations more credible .

#### Theoretical Underpinnings: The Mori-Zwanzig Formalism

The challenges encountered in data-driven parameterization—such as the need for stochasticity or memory—are not mere artifacts of machine learning but can be rigorously derived from the first principles of statistical mechanics. The Mori-Zwanzig formalism provides a mathematical framework for exactly eliminating unresolved ("fast") variables from a high-dimensional [deterministic system](@entry_id:174558). The resulting exact evolution equation for the resolved ("slow") variables, known as the Generalized Langevin Equation, is composed of three distinct terms:
1.  A **Markovian term**, representing the instantaneous [mean force](@entry_id:751818) exerted on the slow variables.
2.  A **memory term**, expressed as a time-[convolution integral](@entry_id:155865), which accounts for the delayed feedback from the fast variables as they are influenced by and, in turn, influence the slow variables.
3.  A **noise term**, which represents the influence of the specific initial state of the fast variables and is effectively stochastic from the perspective of the resolved model.

This formalism reveals that memory (non-Markovian effects) and noise are not ad-hoc additions but are emergent and intrinsic properties of any reduced-order model derived from a more complex deterministic system. This provides a deep theoretical justification for the use of advanced architectures. When time-scale separation is weak, as is common in geophysical flows, the memory and noise terms are significant. This motivates the use of sequence models like Recurrent Neural Networks (RNNs) to capture the [memory kernel](@entry_id:155089) and stochastic models to capture the statistics of the noise term, leading to more physically complete and accurate data-driven parameterizations .