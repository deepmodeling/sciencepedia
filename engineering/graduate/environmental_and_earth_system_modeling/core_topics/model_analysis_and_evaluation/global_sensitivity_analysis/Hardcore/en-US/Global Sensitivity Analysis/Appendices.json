{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation in Global Sensitivity Analysis, it is essential to work through the mechanics of variance decomposition from first principles. This exercise asks you to compute the first-order and total-effect Sobol indices for a simple multiplicative model, a classic case where interactions between inputs are crucial. By performing these calculations , you will gain a concrete understanding of how total output variance is apportioned to the main effects of individual inputs and their synergistic interactions.",
            "id": "4348293",
            "problem": "In a simplified systems biomedicine model of a phenotype regulated by two independent pathway activities, suppose the output is modeled as the multiplicative response $Y = X_{1} X_{2}$, where $X_{1}$ and $X_{2}$ are independent and identically distributed as $\\mathrm{Unif}(0,1)$. Using first-principles definitions from variance-based global sensitivity analysis for independent inputs, compute the first-order Sobol indices $S_{1}$ and $S_{2}$ and the total-effect Sobol indices $S_{T_1}$ and $S_{T_2}$ for this model. Then, based on your variance decomposition, state whether the variance due to interaction between $X_{1}$ and $X_{2}$ exceeds the variance due to either main effect. Report $S_{1}$, $S_{2}$, $S_{T_1}$, and $S_{T_2}$ as exact values (no rounding), and express the final answer as a row vector in the order $S_{1}$, $S_{2}$, $S_{T_1}$, $S_{T_2}$. No units are required.",
            "solution": "We start from the foundational definitions of variance-based global sensitivity analysis for independent inputs, derived from the Hoeffding–Sobol analysis of variance (ANOVA) decomposition. For a square-integrable model output $Y=f(X_{1},X_{2})$ with independent inputs, the variance decomposes orthogonally as\n$$\n\\operatorname{Var}(Y) = V_{1} + V_{2} + V_{12},\n$$\nwhere $V_{i} = \\operatorname{Var}(\\mathbb{E}[Y \\mid X_{i}])$ are the first-order variance components and $V_{12}$ is the second-order (interaction) variance component. The first-order Sobol indices are defined by\n$$\nS_{i} = \\frac{V_{i}}{\\operatorname{Var}(Y)},\n$$\nand the total-effect indices are defined by\n$$\nS_{T_i} = \\frac{\\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid X_{-i})\\right]}{\\operatorname{Var}(Y)} = 1 - \\frac{\\operatorname{Var}(\\mathbb{E}[Y \\mid X_{-i}])}{\\operatorname{Var}(Y)},\n$$\nwhere $X_{-i}$ denotes all inputs except $X_{i}$. For two inputs, $S_{T_1} = S_{1} + S_{12}$ and $S_{T_2} = S_{2} + S_{12}$, where $S_{12} = V_{12}/\\operatorname{Var}(Y)$.\n\nFor the given model $Y = X_{1} X_{2}$ with $X_{1}, X_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathrm{Unif}(0,1)$ and independent, compute the necessary expectations and variances.\n\nFirst, compute $\\mathbb{E}[Y]$ and $\\operatorname{Var}(Y)$:\n- Since $X_{1}$ and $X_{2}$ are independent and identically distributed with $\\mathbb{E}[X_{i}] = \\frac{1}{2}$, we have\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[X_{1} X_{2}] = \\mathbb{E}[X_{1}] \\, \\mathbb{E}[X_{2}] = \\frac{1}{4}.\n$$\n- Also, $\\mathbb{E}[X_{i}^{2}] = \\frac{1}{3}$, hence\n$$\n\\mathbb{E}[Y^{2}] = \\mathbb{E}[X_{1}^{2} X_{2}^{2}] = \\mathbb{E}[X_{1}^{2}] \\, \\mathbb{E}[X_{2}^{2}] = \\frac{1}{9}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^{2}] - \\left(\\mathbb{E}[Y]\\right)^{2} = \\frac{1}{9} - \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{9} - \\frac{1}{16} = \\frac{16 - 9}{144} = \\frac{7}{144}.\n$$\n\nNext, compute the first-order variance components $V_{1}$ and $V_{2}$:\n- Compute the conditional expectation\n$$\n\\mathbb{E}[Y \\mid X_{1}] = \\mathbb{E}[X_{1} X_{2} \\mid X_{1}] = X_{1} \\, \\mathbb{E}[X_{2}] = \\frac{X_{1}}{2}.\n$$\nHence\n$$\nV_{1} = \\operatorname{Var}(\\mathbb{E}[Y \\mid X_{1}]) = \\operatorname{Var}\\!\\left(\\frac{X_{1}}{2}\\right) = \\frac{1}{4} \\operatorname{Var}(X_{1}) = \\frac{1}{4} \\cdot \\frac{1}{12} = \\frac{1}{48}.\n$$\nBy symmetry, $V_{2} = \\frac{1}{48}$.\n\nTherefore, the first-order Sobol indices are\n$$\nS_{1} = \\frac{V_{1}}{\\operatorname{Var}(Y)} = \\frac{\\frac{1}{48}}{\\frac{7}{144}} = \\frac{1}{48} \\cdot \\frac{144}{7} = \\frac{3}{7},\n\\qquad\nS_{2} = \\frac{V_{2}}{\\operatorname{Var}(Y)} = \\frac{3}{7}.\n$$\n\nNow compute the total-effect indices. We use two equivalent routes to verify consistency.\n\nRoute $1$ (complement of main effect of the complement set):\n- Since $\\mathbb{E}[Y \\mid X_{2}] = \\frac{X_{2}}{2}$, we have\n$$\n\\operatorname{Var}(\\mathbb{E}[Y \\mid X_{2}]) = \\operatorname{Var}\\!\\left(\\frac{X_{2}}{2}\\right) = \\frac{1}{48}.\n$$\nThus\n$$\nS_{T_1} = 1 - \\frac{\\operatorname{Var}(\\mathbb{E}[Y \\mid X_{2}])}{\\operatorname{Var}(Y)} = 1 - \\frac{\\frac{1}{48}}{\\frac{7}{144}} = 1 - \\frac{3}{7} = \\frac{4}{7}.\n$$\nBy symmetry,\n$$\nS_{T_2} = \\frac{4}{7}.\n$$\n\nRoute $2$ (expected conditional variance):\n- Compute\n$$\n\\operatorname{Var}(Y \\mid X_{1}) = \\operatorname{Var}(X_{1} X_{2} \\mid X_{1}) = X_{1}^{2} \\operatorname{Var}(X_{2}) = X_{1}^{2} \\cdot \\frac{1}{12}.\n$$\nHence\n$$\n\\mathbb{E}[\\operatorname{Var}(Y \\mid X_{1})] = \\frac{1}{12} \\mathbb{E}[X_{1}^{2}] = \\frac{1}{12} \\cdot \\frac{1}{3} = \\frac{1}{36}.\n$$\nTherefore\n$$\nS_{T_1} = \\frac{\\mathbb{E}[\\operatorname{Var}(Y \\mid X_{1})]}{\\operatorname{Var}(Y)} = \\frac{\\frac{1}{36}}{\\frac{7}{144}} = \\frac{1}{36} \\cdot \\frac{144}{7} = \\frac{4}{7},\n$$\nand likewise $S_{T_2} = \\frac{4}{7}$, consistent with the previous route.\n\nInteraction assessment. The interaction variance fraction is\n$$\nS_{12} = 1 - S_{1} - S_{2} = 1 - \\frac{3}{7} - \\frac{3}{7} = \\frac{1}{7}.\n$$\nThus, in this model, the interaction contribution $\\frac{1}{7}$ does not exceed either main effect contribution $\\frac{3}{7}$. Each main effect dominates the interaction, although the interaction is non-negligible, as reflected by the gap between $S_{T_i}$ and $S_{i}$ of size $\\frac{1}{7}$ for each input.\n\nCollecting the requested indices in the order $S_{1}$, $S_{2}$, $S_{T_1}$, $S_{T_2}$ yields the exact values\n$$\n\\left(\\frac{3}{7}, \\frac{3}{7}, \\frac{4}{7}, \\frac{4}{7}\\right).\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{7}  \\frac{3}{7}  \\frac{4}{7}  \\frac{4}{7}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Global Sensitivity Analysis often reveals insights that simpler methods miss, especially for the complex, non-linear relationships common in environmental models. This problem presents a scenario where different sensitivity metrics appear to give contradictory results, a situation that requires deeper conceptual understanding . By analyzing a non-monotonic, bell-shaped response function, you will learn to distinguish between variance-based indices, which capture any functional relationship, and correlation-based metrics that are limited to monotonic trends.",
            "id": "1436446",
            "problem": "A systems biologist is analyzing a metabolic pathway where the flux $J$ is controlled by the concentration of an external signaling molecule, denoted by the parameter $X$. A global sensitivity analysis of the computational model yields two seemingly contradictory results: the total-effect Sobol index for $X$ is high ($S_{TX} \\approx 0.75$), while the Partial Rank Correlation Coefficient (PRCC) between $X$ and $J$ is close to zero ($\\rho_{XJ} \\approx 0.01$).\n\nThis suggests that $X$ strongly influences the flux $J$, but not in a monotonic fashion. The biologist hypothesizes a model where the relationship is bell-shaped, representing an optimal concentration for pathway activation, with inhibitory effects at lower and higher concentrations. The proposed functional form relating the flux $J$ to the modulator concentration $X$ is:\n$$J(X) = \\frac{J_{max}}{1 + \\left(\\frac{X - X_{opt}}{W}\\right)^2}$$\nHere, $J_{max}$ is the maximum possible flux, $X_{opt}$ is the optimal concentration of the modulator, and $W$ is a parameter that determines the width of the peak. All three ($J_{max}$, $X_{opt}$, $W$) are positive constants.\n\nTo further investigate the properties of this model, calculate the expected value (mean) of the flux, $E[J]$, under the assumption that the modulator concentration $X$ is a random variable sampled from a uniform distribution over the interval $[X_{opt} - W, X_{opt} + W]$. Express your answer as a symbolic expression in terms of $J_{max}$ and $\\pi$.",
            "solution": "We are given $J(X) = \\dfrac{J_{max}}{1 + \\left(\\dfrac{X - X_{opt}}{W}\\right)^{2}}$ and $X \\sim \\text{Uniform}[X_{opt} - W, X_{opt} + W]$. The probability density function of $X$ on this interval is $f_{X}(x) = \\dfrac{1}{2W}$ and zero otherwise. The expected value is\n$$\nE[J] = \\int_{X_{opt} - W}^{X_{opt} + W} J(x) f_{X}(x)\\,dx = \\frac{1}{2W} \\int_{X_{opt} - W}^{X_{opt} + W} \\frac{J_{max}}{1 + \\left(\\frac{x - X_{opt}}{W}\\right)^{2}}\\,dx.\n$$\nMake the change of variables $y = \\dfrac{x - X_{opt}}{W}$, so $dx = W\\,dy$ and the limits become $y = -1$ to $y = 1$. Then\n$$\nE[J] = \\frac{J_{max}}{2W} \\int_{X_{opt} - W}^{X_{opt} + W} \\frac{dx}{1 + \\left(\\frac{x - X_{opt}}{W}\\right)^{2}}\n= \\frac{J_{max}}{2W} \\int_{-1}^{1} \\frac{W\\,dy}{1 + y^{2}}\n= \\frac{J_{max}}{2} \\int_{-1}^{1} \\frac{dy}{1 + y^{2}}.\n$$\nUsing $\\int \\dfrac{dy}{1 + y^{2}} = \\arctan(y)$, evaluate\n$$\n\\int_{-1}^{1} \\frac{dy}{1 + y^{2}} = \\arctan(1) - \\arctan(-1) = \\frac{\\pi}{4} - \\left(-\\frac{\\pi}{4}\\right) = \\frac{\\pi}{2}.\n$$\nTherefore,\n$$\nE[J] = \\frac{J_{max}}{2} \\cdot \\frac{\\pi}{2} = \\frac{\\pi J_{max}}{4}.\n$$\nThis mean is independent of $X_{opt}$ and $W$ due to symmetry and scaling.",
            "answer": "$$\\boxed{\\frac{\\pi J_{max}}{4}}$$"
        },
        {
            "introduction": "Beyond comprehensive variance-based methods, computationally efficient screening techniques like the Morris method are invaluable for exploring high-dimensional models. However, their efficiency comes with subtleties that a practitioner must understand to avoid misinterpretation . This exercise challenges you to analyze a pure interaction model using Morris's elementary effects, demonstrating why relying on the mean effect alone is misleading and highlighting the importance of using the full set of Morris statistics to correctly identify influential interactions.",
            "id": "3883347",
            "problem": "Consider a dimensionless, stylized interaction index for cloud–aerosol radiative impacts in an Earth system component model, defined on a standardized unit hypercube. Let the model output be $y = f(x_{1}, x_{2}) = x_{1} x_{2}$, where $x_{1}$ and $x_{2}$ are independent inputs representing standardized anomalies of aerosol burden and cloud fraction, respectively. Assume $x_{1}$ and $x_{2}$ are independent and identically distributed as $\\mathrm{Uniform}(-1,1)$.\n\nIn the One-At-a-Time screening method of Global Sensitivity Analysis (GSA) due to Morris, the first-order elementary effect for factor $i$ at a baseline point $\\boldsymbol{x}$ with step size $\\Delta \\in (0,2)$ is defined by\n$$\nEE_{i}(\\boldsymbol{x}) \\equiv \\frac{f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{i}) - f(\\boldsymbol{x})}{\\Delta},\n$$\nwhere $\\boldsymbol{e}_{i}$ is the $i$-th standard basis vector. The second-order elementary effect for a factor pair $(i,j)$ is defined by\n$$\nEE_{ij}^{(2)}(\\boldsymbol{x}) \\equiv \\frac{f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{i} + \\Delta \\boldsymbol{e}_{j}) - f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{i}) - f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{j}) + f(\\boldsymbol{x})}{\\Delta^{2}}.\n$$\nConsider Morris trajectories whose starting points $\\boldsymbol{x}$ are distributed uniformly over the admissible interior of the grid consistent with the step $\\Delta$, and for each first-order effect of $x_{1}$ treat $x_{2}$ as an independent draw from $\\mathrm{Uniform}(-1,1)$ at the baseline.\n\nStarting from these core definitions and the assumed input distributions, derive closed-form expressions for the following expected summary statistics over random trajectories:\n- the mean of first-order elementary effects for $x_{1}$, denoted $\\mu_{1} \\equiv \\mathbb{E}[EE_{1}(\\boldsymbol{x})]$,\n- the standard deviation of first-order elementary effects for $x_{1}$, denoted $\\sigma_{1} \\equiv \\sqrt{\\mathrm{Var}(EE_{1}(\\boldsymbol{x}))}$,\n- the mean absolute first-order elementary effect for $x_{1}$, denoted $\\mu_{1}^{\\star} \\equiv \\mathbb{E}[|EE_{1}(\\boldsymbol{x})|]$,\n- and the second-order elementary effect for the pair $(1,2)$, denoted $EE_{12}^{(2)}(\\boldsymbol{x})$, evaluated in expectation over $\\boldsymbol{x}$.\n\nExplain, using these results, why reliance solely on $\\mu_{1}$ can miss pure interaction effects that have zero-mean first-order elementary effects. Propose at least two principled strategies within GSA to robustly detect such cases in environmental and Earth system modeling.\n\nExpress your four requested quantities in the order $\\mu_{1}$, $\\sigma_{1}$, $\\mu_{1}^{\\star}$, $EE_{12}^{(2)}$ as a single row matrix. All quantities are dimensionless. Provide exact analytic values and do not round.",
            "solution": "The problem asks for the calculation of four statistical measures related to the Morris elementary effects for the model $y = f(x_{1}, x_{2}) = x_{1} x_{2}$. The inputs $x_{1}$ and $x_{2}$ are independent and identically distributed according to a Uniform distribution on the interval $[-1, 1]$, denoted as $x_{i} \\sim \\mathrm{Uniform}(-1, 1)$.\n\nFirst, we establish the key properties of this distribution. The probability density function (PDF) is $p(x) = \\frac{1}{2}$ for $x \\in [-1, 1]$ and $p(x) = 0$ otherwise.\nThe expected value of an input $x_{i}$ is:\n$$ \\mathbb{E}[x_{i}] = \\int_{-1}^{1} x \\frac{1}{2} dx = \\frac{1}{2} \\left[ \\frac{x^{2}}{2} \\right]_{-1}^{1} = \\frac{1}{4} (1^{2} - (-1)^{2}) = 0 $$\nThe variance of an input $x_{i}$ is:\n$$ \\mathrm{Var}(x_{i}) = \\mathbb{E}[x_{i}^{2}] - (\\mathbb{E}[x_{i}])^{2} = \\int_{-1}^{1} x^{2} \\frac{1}{2} dx - 0^2 = \\frac{1}{2} \\left[ \\frac{x^{3}}{3} \\right]_{-1}^{1} = \\frac{1}{6} (1^{3} - (-1)^{3}) = \\frac{1}{3} $$\n\nNow we proceed to calculate the four requested quantities.\n\n1.  **The mean of first-order elementary effects for $x_{1}$, $\\mu_{1} \\equiv \\mathbb{E}[EE_{1}(\\boldsymbol{x})]$**\n    The first-order elementary effect for factor $x_{1}$ is:\n    $$ EE_{1}(\\boldsymbol{x}) = \\frac{f(x_{1} + \\Delta, x_{2}) - f(x_{1}, x_{2})}{\\Delta} = \\frac{(x_{1} + \\Delta) x_{2} - x_{1} x_{2}}{\\Delta} = \\frac{\\Delta x_{2}}{\\Delta} = x_{2} $$\n    The mean, $\\mu_{1}$, is the expectation of $EE_{1}(\\boldsymbol{x})$ over the distribution of the inputs, which is simply the expectation of $x_{2}$.\n    $$ \\mu_{1} = \\mathbb{E}[EE_{1}(\\boldsymbol{x})] = \\mathbb{E}[x_{2}] = 0 $$\n\n2.  **The standard deviation of first-order elementary effects for $x_{1}$, $\\sigma_{1} \\equiv \\sqrt{\\mathrm{Var}(EE_{1}(\\boldsymbol{x}))}$**\n    The variance of the elementary effect $EE_{1}(\\boldsymbol{x})$ is:\n    $$ \\mathrm{Var}(EE_{1}(\\boldsymbol{x})) = \\mathrm{Var}(x_{2}) = \\frac{1}{3} $$\n    The standard deviation $\\sigma_{1}$ is the square root of the variance:\n    $$ \\sigma_{1} = \\sqrt{\\mathrm{Var}(x_{2})} = \\sqrt{\\frac{1}{3}} = \\frac{1}{\\sqrt{3}} = \\frac{\\sqrt{3}}{3} $$\n\n3.  **The mean absolute first-order elementary effect for $x_{1}$, $\\mu_{1}^{\\star} \\equiv \\mathbb{E}[|EE_{1}(\\boldsymbol{x})|]$**\n    This requires calculating the expectation of the absolute value of $EE_{1}(\\boldsymbol{x}) = x_{2}$.\n    $$ \\mu_{1}^{\\star} = \\mathbb{E}[|EE_{1}(\\boldsymbol{x})|] = \\mathbb{E}[|x_{2}|] = \\int_{-1}^{1} |x| \\frac{1}{2} dx $$\n    Since the integrand is an even function, we can simplify the integral:\n    $$ \\mu_{1}^{\\star} = 2 \\int_{0}^{1} x \\frac{1}{2} dx = \\int_{0}^{1} x dx = \\left[ \\frac{x^{2}}{2} \\right]_{0}^{1} = \\frac{1}{2} $$\n\n4.  **The expected second-order elementary effect for the pair $(1,2)$, $\\mathbb{E}[EE_{12}^{(2)}(\\boldsymbol{x})]$**\n    The second-order elementary effect is:\n    $$ EE_{12}^{(2)}(\\boldsymbol{x}) = \\frac{f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{1} + \\Delta \\boldsymbol{e}_{2}) - f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{1}) - f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{2}) + f(\\boldsymbol{x})}{\\Delta^{2}} $$\n    We evaluate each term in the numerator for $f(x_{1}, x_{2}) = x_{1} x_{2}$:\n    -   $f(\\boldsymbol{x}) = x_{1} x_{2}$\n    -   $f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{1}) = (x_{1} + \\Delta) x_{2} = x_{1} x_{2} + \\Delta x_{2}$\n    -   $f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{2}) = x_{1} (x_{2} + \\Delta) = x_{1} x_{2} + \\Delta x_{1}$\n    -   $f(\\boldsymbol{x} + \\Delta \\boldsymbol{e}_{1} + \\Delta \\boldsymbol{e}_{2}) = (x_{1} + \\Delta)(x_{2} + \\Delta) = x_{1} x_{2} + \\Delta x_{1} + \\Delta x_{2} + \\Delta^{2}$\n    Substituting these into the numerator gives $\\Delta^2$. Therefore, the second-order elementary effect is:\n    $$ EE_{12}^{(2)}(\\boldsymbol{x}) = \\frac{\\Delta^{2}}{\\Delta^{2}} = 1 $$\n    Since $EE_{12}^{(2)}(\\boldsymbol{x})$ is a constant, its expectation is the constant itself.\n    $$ \\mathbb{E}[EE_{12}^{(2)}(\\boldsymbol{x})] = \\mathbb{E}[1] = 1 $$\n\n**Explanation of Interaction Effects**\nThe model $y = x_{1} x_{2}$ represents a pure interaction effect. Our analysis shows that $\\mu_{1} = 0$, while $\\mu_{1}^{\\star} = \\frac{1}{2}$ and $\\sigma_{1} = \\frac{\\sqrt{3}}{3}$ are non-zero. The mean elementary effect $\\mu_{1}$ measures the average linear effect of $x_{1}$ on $y$ across the input space. For this model, the effect of $x_{1}$ is to scale $x_{2}$. Since $x_{2}$ is symmetrically distributed around $0$, the positive and negative effects of $x_{1}$ cancel out perfectly on average, leading to $\\mu_{1} = 0$. An analyst relying solely on $\\mu_{1}$ would incorrectly conclude that $x_{1}$ is a non-influential parameter.\n\nHowever, the non-zero value of $\\mu_{1}^{\\star}$ correctly identifies $x_{1}$ as influential by measuring the average magnitude of its effect. Furthermore, the large value of $\\sigma_{1}$ (the standard deviation of effects) relative to $\\mu_{1}$ is a key indicator. A high $\\sigma_{1}$ signifies that the effect of $x_{1}$ is not constant but varies depending on the values of other parameters, which is the definition of an interaction effect. In this case, the entire variance of $EE_{1}$ is due to its interaction with $x_{2}$. Finally, the direct calculation of the second-order effect $\\mathbb{E}[EE_{12}^{(2)}] = 1$ unambiguously quantifies this strong interaction.\n\n**Principled GSA Strategies for Detecting Interactions**\nTo robustly detect parameters involved in pure interaction effects, which might be missed by examining only the mean elementary effect, more comprehensive GSA methods should be employed. Two such principled strategies are:\n\n1.  **Variance-Based Sensitivity Analysis (e.g., Sobol' Method):** This method formally decomposes the total variance of the model output, $\\mathrm{Var}(Y)$, into contributions from individual input factors (first-order effects, $S_{i}$) and their interactions (higher-order effects, $S_{ij}, S_{ijk}, \\dots$). A key diagnostic is the comparison of the first-order index $S_{i}$ with the total-effect index $S_{Ti}$. For a parameter $x_{i}$ involved only in interactions, its first-order index $S_{i}$ will be zero or close to zero, while its total-effect index $S_{Ti}$ (which includes all interaction terms involving $x_{i}$) will be significantly larger. For the model $y = x_{1} x_{2}$, one can show that $S_{1} = 0$ while $S_{T1} > 0$, definitively identifying $x_{1}$ as an important parameter whose influence comes from interactions.\n\n2.  **Moment-Independent Sensitivity Analysis (e.g., PAWN, DELSA):** These methods do not rely on the variance or other moments of the output distribution. Instead, they assess sensitivity by comparing the entire probability distribution of the output. The PAWN method, for example, compares the cumulative distribution function (CDF) of the unconditional output with conditional CDFs obtained by fixing an input factor. The sensitivity index is derived from the maximum distance (e.g., using the Kolmogorov-Smirnov statistic) between these distributions. A parameter that dramatically alters the shape of the output's distribution, even if it does not change its mean or variance, will be assigned a high sensitivity index. For $y = x_{1} x_{2}$, fixing $x_{1}$ at different values produces scaled uniform distributions, whose CDFs are markedly different from the unconditional CDF of the product of two uniforms. This approach robustly detects influence regardless of whether it manifests as a simple shift in the mean or as a more complex change in the output distribution.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\frac{\\sqrt{3}}{3}  \\frac{1}{2}  1 \\end{pmatrix}}\n$$"
        }
    ]
}