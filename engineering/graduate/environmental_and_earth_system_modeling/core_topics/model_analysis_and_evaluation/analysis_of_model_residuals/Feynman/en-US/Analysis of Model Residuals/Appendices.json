{
    "hands_on_practices": [
        {
            "introduction": "The first step in any model evaluation is to examine the residuals—the differences between observed and predicted values. This practice grounds you in the fundamentals by having you compute the three most important summary statistics for a residual time series: the mean (to check for systematic bias), the variance (to quantify overall error magnitude), and the lag-1 autocorrelation (to detect memory or persistence in the errors). By implementing these from first principles, you will gain a robust understanding of what these metrics reveal about your model's behavior .",
            "id": "3862150",
            "problem": "Consider a monthly hydrologic time series context in which observed streamflow $Q_t$ (in cubic meters per second, written as $\\mathrm{m^3/s}$) is paired with a model-estimated streamflow $\\hat{Q}_t$ for $t = 1, \\dots, n$. Define the model residual at time $t$ as $r_t = Q_t - \\hat{Q}_t$, with $r_t$ in $\\mathrm{m^3/s}$. From a first-principles perspective, use the classical sample definitions from descriptive statistics and time series analysis (Autocorrelation Function (ACF)) to compute three summaries of the residual sequence: the sample mean, the unbiased sample variance, and the sample autocorrelation at lag $1$. The lag-$1$ autocorrelation must be computed using the correlation of consecutive residual anomalies about their sample mean. If the sample variance of $r_t$ is zero for a given case, define the lag-$1$ autocorrelation as $0$ by convention. Express the mean residual in $\\mathrm{m^3/s}$, the variance in $(\\mathrm{m^3/s})^2$, and the lag-$1$ autocorrelation as a pure number without units. Angles are not involved in this task.\n\nStarting from foundational definitions only (sample mean, unbiased sample variance, and sample autocorrelation via sample covariance), implement a program to produce the three residual summaries for each of the provided test cases. No shortcut formulas are permitted in the problem statement; the solution must be derived from core definitions.\n\nUse the following test suite. Each test case provides sequences $(Q_t)_{t=1}^n$ and $(\\hat{Q}_t)_{t=1}^n$:\n\n- Test Case $1$ (seasonal model bias, $n = 12$):\n  - $Q_t = \\{42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50\\}$.\n  - $\\hat{Q}_t = \\{40, 50, 58, 80, 120, 150, 180, 170, 130, 95, 75, 55\\}$.\n\n- Test Case $2$ (perfect model, $n = 12$):\n  - $Q_t = \\{60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60\\}$.\n  - $\\hat{Q}_t = \\{60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60\\}$.\n\n- Test Case $3$ (constant bias, $n = 12$):\n  - $Q_t = \\{42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50\\}$.\n  - $\\hat{Q}_t = \\{22, 35, 40, 55, 90, 140, 170, 155, 120, 80, 50, 30\\}$.\n\n- Test Case $4$ (strong positive serial dependence in residuals, $n = 12$):\n  - $Q_t = \\{50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60\\}$.\n  - $\\hat{Q}_t = \\{45, 52, 58, 75, 103, 141, 170, 162, 134, 96, 70, 53\\}$.\n\n- Test Case $5$ (alternating residuals, negative serial dependence, $n = 12$):\n  - $Q_t = \\{50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60\\}$.\n  - $\\hat{Q}_t = \\{40, 70, 61, 99, 112, 168, 183, 187, 156, 104, 85, 55\\}$.\n\n- Test Case $6$ (boundary case with minimal length, $n = 2$):\n  - $Q_t = \\{100, 120\\}$.\n  - $\\hat{Q}_t = \\{95, 130\\}$.\n\nFor each test case, compute:\n- The sample mean $\\bar{r}$ (in $\\mathrm{m^3/s}$).\n- The unbiased sample variance $s_r^2$ (in $(\\mathrm{m^3/s})^2$).\n- The lag-$1$ autocorrelation $\\rho_1$ (unitless).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of three decimal values $\\left[\\bar{r}, s_r^2, \\rho_1\\right]$ rounded to $6$ decimal places, and the overall output must be a list of these lists in order of the test cases, for example: $\\left[[x_1,y_1,z_1],[x_2,y_2,z_2],\\dots\\right]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of time series analysis and descriptive statistics as applied to environmental model evaluation. It is well-posed, with all necessary data and definitions provided to compute a unique and meaningful solution. The language is objective and precise. I will proceed with a solution derived from first principles as requested.\n\nThe task is to compute three statistical summaries for a time series of model residuals, $r_t = Q_t - \\hat{Q}_t$, where $Q_t$ is the observed streamflow and $\\hat{Q}_t$ is the model-estimated streamflow at time $t$, for $t = 1, \\dots, n$. The required summaries are the sample mean ($\\bar{r}$), the unbiased sample variance ($s_r^2$), and the lag-$1$ sample autocorrelation ($\\rho_1$).\n\nLet the sequence of residuals be $r = \\{r_1, r_2, \\dots, r_n\\}$.\n\n**1. Sample Mean ($\\bar{r}$)**\n\nThe sample mean is the arithmetic average of the residual values. This is the first moment of the sample distribution. Its classical definition is the sum of all observations divided by the number of observations, $n$.\n\n$$ \\bar{r} = \\frac{1}{n} \\sum_{t=1}^{n} r_t $$\n\nThe units of $\\bar{r}$ are the same as the units of $r_t$, which is $\\mathrm{m^3/s}$.\n\n**2. Unbiased Sample Variance ($s_r^2$)**\n\nThe unbiased sample variance measures the dispersion of the data around the sample mean. The term \"unbiased\" signifies that the denominator is $n-1$, which corrects for the bias introduced by using the sample mean instead of the true (but unknown) population mean. It is defined as the sum of the squared deviations from the sample mean, divided by $n-1$.\n\n$$ s_r^2 = \\frac{1}{n-1} \\sum_{t=1}^{n} (r_t - \\bar{r})^2 $$\n\nThis definition is valid for $n > 1$. All test cases provided satisfy this condition. The units of $s_r^2$ are the square of the units of $r_t$, which is $(\\mathrm{m^3/s})^2$.\n\n**3. Lag-1 Sample Autocorrelation ($\\rho_1$)**\n\nThe lag-$1$ sample autocorrelation measures the linear relationship between a residual value, $r_t$, and the subsequent residual value, $r_{t+1}$. The problem requires its computation \"using the correlation of consecutive residual anomalies about their sample mean.\" This corresponds to the standard definition of the sample autocorrelation function (ACF) at lag $1$, which is the sample autocovariance at lag $1$ divided by the sample variance (or autocovariance at lag $0$).\n\nThe sample autocovariance at lag $k$, denoted $\\hat{\\gamma}(k)$, is defined as:\n$$ \\hat{\\gamma}(k) = \\frac{1}{n} \\sum_{t=1}^{n-k} (r_t - \\bar{r})(r_{t+k} - \\bar{r}) $$\n\nFor lag $k=1$, the autocovariance is:\n$$ \\hat{\\gamma}(1) = \\frac{1}{n} \\sum_{t=1}^{n-1} (r_t - \\bar{r})(r_{t+1} - \\bar{r}) $$\n\nThe sample variance, which is the autocovariance at lag $k=0$, is:\n$$ \\hat{\\gamma}(0) = \\frac{1}{n} \\sum_{t=1}^{n} (r_t - \\bar{r})^2 $$\n\nThe lag-$1$ sample autocorrelation, $\\rho_1$, is the ratio of $\\hat{\\gamma}(1)$ to $\\hat{\\gamma}(0)$. The factor of $1/n$ in both numerator and denominator cancels.\n\n$$ \\rho_1 = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)} = \\frac{\\frac{1}{n} \\sum_{t=1}^{n-1} (r_t - \\bar{r})(r_{t+1} - \\bar{r})}{\\frac{1}{n} \\sum_{t=1}^{n} (r_t - \\bar{r})^2} = \\frac{\\sum_{t=1}^{n-1} (r_t - \\bar{r})(r_{t+1} - \\bar{r})}{\\sum_{t=1}^{n} (r_t - \\bar{r})^2} $$\n\nThe quantity $\\rho_1$ is a dimensionless pure number, as the units in the numerator, $(\\mathrm{m^3/s})^2$, cancel with the units in the denominator. Per the problem statement, a special case must be handled: if the sample variance is zero, which occurs when the denominator $\\sum_{t=1}^{n} (r_t - \\bar{r})^2$ is zero, then $\\rho_1$ is defined to be $0$. This convention avoids division by zero. This occurs if and only if all residuals $r_t$ are identical, resulting in $s_r^2=0$.\n\nThe implementation will follow these three derived formulas for each provided test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes residual summaries for multiple test cases and prints the results.\n    The summaries are: sample mean, unbiased sample variance, and lag-1 autocorrelation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (seasonal model bias, n = 12)\n        (\n            np.array([42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50], dtype=np.float64),\n            np.array([40, 50, 58, 80, 120, 150, 180, 170, 130, 95, 75, 55], dtype=np.float64)\n        ),\n        # Test Case 2 (perfect model, n = 12)\n        (\n            np.array([60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60], dtype=np.float64),\n            np.array([60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60], dtype=np.float64)\n        ),\n        # Test Case 3 (constant bias, n = 12)\n        (\n            np.array([42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50], dtype=np.float64),\n            np.array([22, 35, 40, 55, 90, 140, 170, 155, 120, 80, 50, 30], dtype=np.float64)\n        ),\n        # Test Case 4 (strong positive serial dependence in residuals, n = 12)\n        (\n            np.array([50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60], dtype=np.float64),\n            np.array([45, 52, 58, 75, 103, 141, 170, 162, 134, 96, 70, 53], dtype=np.float64)\n        ),\n        # Test Case 5 (alternating residuals, negative serial dependence, n = 12)\n        (\n            np.array([50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60], dtype=np.float64),\n            np.array([40, 70, 61, 99, 112, 168, 183, 187, 156, 104, 85, 55], dtype=np.float64)\n        ),\n        # Test Case 6 (boundary case with minimal length, n = 2)\n        (\n            np.array([100, 120], dtype=np.float64),\n            np.array([95, 130], dtype=np.float64)\n        )\n    ]\n    \n    results_list = []\n    \n    for Q_t, Q_hat_t in test_cases:\n        \n        # 0. Calculate the residual series r_t = Q_t - Q_hat_t\n        r_t = Q_t - Q_hat_t\n        n = len(r_t)\n\n        # 1. Compute the sample mean of residuals, r_bar\n        # Formula: r_bar = (1/n) * sum(r_t) for t=1 to n\n        r_bar = np.mean(r_t)\n\n        # 2. Compute the unbiased sample variance, s_r^2\n        # Formula: s_r^2 = (1/(n-1)) * sum((r_t - r_bar)^2) for t=1 to n\n        # numpy.var with ddof=1 calculates the unbiased sample variance.\n        # This check is only for n<=1, but all test cases have n>1.\n        if n > 1:\n            s_r_sq = np.var(r_t, ddof=1)\n        else:\n            s_r_sq = 0.0\n\n        # 3. Compute the lag-1 autocorrelation, rho_1\n        # Formula: rho_1 = sum((r_t - r_bar)(r_{t+1} - r_bar)) / sum((r_t - r_bar)^2)\n        # The sum in the numerator is from t=1 to n-1.\n        \n        # Calculate deviations from the mean (anomalies)\n        anomalies = r_t - r_bar\n        \n        # Calculate the denominator: sum of squared anomalies\n        sum_sq_anomalies = np.sum(anomalies**2)\n\n        # Handle the case where variance is zero, as specified in the problem\n        if sum_sq_anomalies  1e-12: # Use a small tolerance for float comparison\n            rho_1 = 0.0\n        else:\n            # Numerator: sum of products of consecutive anomalies\n            # anomalies[:-1] is {r_1-r_bar, ..., r_{n-1}-r_bar}\n            # anomalies[1:] is {r_2-r_bar, ..., r_n-r_bar}\n            numerator = np.sum(anomalies[:-1] * anomalies[1:])\n            rho_1 = numerator / sum_sq_anomalies\n        \n        # Store results for this case, formatted to 6 decimal places\n        case_result = [r_bar, s_r_sq, rho_1]\n        results_list.append(case_result)\n\n    # Format the final output string as a list of lists.\n    # Each sublist contains the three computed values rounded to 6 decimal places.\n    output_parts = []\n    for res in results_list:\n        # Format each number to 6 decimal places (f-string with :.6f)\n        # and create the string for one sub-list, e.g., \"[val1,val2,val3]\"\n        formatted_res = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_parts.append(formatted_res)\n\n    # Join the sub-list strings with commas and enclose in brackets,\n    # e.g., \"[[...],[...]]\"\n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "While aggregate statistics provide a big-picture view, some of the most critical modeling errors are driven by just a few influential data points. This exercise introduces you to the essential regression diagnostics of leverage, studentized residuals, and Cook's distance, which are designed to pinpoint these problematic observations. Learning to compute and interpret these diagnostics is a crucial skill for building robust and reliable environmental models .",
            "id": "3862109",
            "problem": "You are analyzing residuals from a linear Ordinary Least Squares (OLS) regression used in environmental and earth system modeling to estimate daily river nutrient load as a function of river discharge and water temperature. Consider the linear model with intercept, where the response is nutrient load in kilograms per day and the predictors are discharge in cubic meters per second and water temperature in degrees Celsius. Use the following model specification:\nGiven $n$ observations, define the design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $p = 3$ columns (an intercept, discharge, and temperature), the response vector $y \\in \\mathbb{R}^n$, and assume the linear model $y = X \\beta + \\varepsilon$, where $\\beta \\in \\mathbb{R}^p$ is unknown and $\\varepsilon$ are independent errors with zero mean and constant variance.\n\nStarting from the OLS estimator and residual definitions, compute for each observation $i \\in \\{1,\\dots,n\\}$:\n- The leverage $h_{ii}$ (the $i$-th diagonal element of the hat matrix).\n- The externally studentized residual $t_i$.\n- Cook’s distance $D_i$.\n\nThen, identify influential observations affecting slope estimates using the following decision rule:\nAn observation $i$ is flagged as influential if either\n1) $D_i > \\frac{4}{n}$, or\n2) $h_{ii} > \\frac{2p}{n}$ and $|t_i| > t_{0.975}(n - p - 1)$, where $t_{0.975}(\\cdot)$ denotes the upper $0.975$ quantile of the Student’s $t$ distribution with the specified degrees of freedom.\n\nAll quantities $h_{ii}$, $t_i$, and $D_i$ are dimensionless and must be reported as floating-point numbers rounded to six decimal places. Indices of influential observations must be reported using zero-based indexing.\n\nUse the following three test cases. For each case, construct $X$ with a column of ones (intercept), the discharge vector, and the temperature vector, and use the provided nutrient load vector $y$. Units are specified for scientific realism, but outputs are dimensionless.\n\nTest Case 1 (happy path, moderate variability, $n=8$):\n- Discharge vector (in $\\mathrm{m}^3/\\mathrm{s}$): $[80, 95, 110, 125, 140, 155, 170, 185]$.\n- Temperature vector (in $^{\\circ}\\mathrm{C}$): $[10, 12, 11, 13, 15, 14, 16, 12]$.\n- Nutrient load vector (in $\\mathrm{kg/day}$): $[255, 292, 327, 370, 418, 450, 491, 520]$.\n\nTest Case 2 (boundary condition with high leverage candidate, $n=9$):\n- Discharge vector (in $\\mathrm{m}^3/\\mathrm{s}$): $[80, 90, 100, 110, 120, 130, 140, 150, 350]$.\n- Temperature vector (in $^{\\circ}\\mathrm{C}$): $[10, 11, 12, 13, 14, 12, 11, 13, 10]$.\n- Nutrient load vector (in $\\mathrm{kg/day}$): $[218, 241, 266, 290, 315, 332, 351, 378, 812]$.\n\nTest Case 3 (edge case with an outlier in the response, $n=10$):\n- Discharge vector (in $\\mathrm{m}^3/\\mathrm{s}$): $[100, 110, 120, 130, 140, 150, 160, 170, 180, 140]$.\n- Temperature vector (in $^{\\circ}\\mathrm{C}$): $[12, 13, 14, 12, 13, 15, 14, 16, 15, 13]$.\n- Nutrient load vector (in $\\mathrm{kg/day}$): $[262, 284, 307, 322, 345, 370, 388, 415, 431, 420]$ (note the last value is an intentional outlier relative to typical patterns).\n\nAlgorithmic requirements:\n- Compute the OLS estimator $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$.\n- Compute residuals $r = y - X \\hat{\\beta}$.\n- Compute the hat matrix $H = X (X^\\top X)^{-1} X^\\top$ and its diagonal $h_{ii}$.\n- Compute the residual variance estimate $s^2 = \\frac{\\sum_{i=1}^{n} r_i^2}{n - p}$.\n- Compute the leave-one-out mean squared error $s_{(i)}^2$ via the identity $$s_{(i)}^2 = \\frac{(n - p) s^2 - \\frac{r_i^2}{1 - h_{ii}}}{n - p - 1},$$ and the externally studentized residual $$t_i = \\frac{r_i}{s_{(i)} \\sqrt{1 - h_{ii}}}.$$\n- Compute Cook’s distance $$D_i = \\frac{r_i^2}{p s^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}.$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a nested list containing four elements: the list of $h_{ii}$ values, the list of $t_i$ values, the list of $D_i$ values, and the list of influential indices determined by the decision rule. All floating-point numbers must be rounded to six decimal places. The overall output must be a single top-level list of the three per-case lists, for example: `[[h_1,t_1,D_1,I_1],[h_2,t_2,D_2,I_2],[h_3,t_3,D_3,I_3]]` where each `h_k`, `t_k`, and `D_k` is itself a list of floats and `I_k` is a list of integers.",
            "solution": "The problem requires the calculation of several key diagnostic statistics for Ordinary Least Squares (OLS) linear regression and the subsequent identification of influential observations based on established criteria. The analysis is to be performed for three distinct test cases within an environmental modeling context. The solution proceeds by first defining the theoretical framework and then applying it algorithmically to the provided data.\n\nThe general form of the multiple linear regression model is given by:\n$$ y = X \\beta + \\varepsilon $$\nwhere $y \\in \\mathbb{R}^n$ is the vector of observed responses (nutrient load), $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of unknown model coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the vector of random errors. The design matrix $X$ is constructed from the predictor variables (river discharge and water temperature) and a leading column of ones representing the model intercept. Thus, for this problem, the number of parameters $p$ is $3$.\n\nThe step-by-step procedure for computing the required diagnostics is as follows:\n\n1.  **OLS Coefficient Estimation**: The OLS estimator $\\hat{\\beta}$ minimizes the sum of squared residuals. It is computed using the normal equations:\n    $$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n    This requires that the matrix $X^\\top X$ be invertible, which is true if the columns of $X$ are linearly independent.\n\n2.  **Fitted Values and Residuals**: Once $\\hat{\\beta}$ is found, the vector of fitted (or predicted) values, $\\hat{y}$, is given by:\n    $$ \\hat{y} = X \\hat{\\beta} $$\n    The vector of residuals, $r$, which represents the difference between observed and fitted values, is then:\n    $$ r = y - \\hat{y} = y - X \\hat{\\beta} $$\n\n3.  **Hat Matrix and Leverage**: The hat matrix, $H$, maps the observed response vector $y$ to the fitted value vector $\\hat{y}$:\n    $$ \\hat{y} = X (X^\\top X)^{-1} X^\\top y = H y $$\n    The hat matrix is defined as $H = X (X^\\top X)^{-1} X^\\top$. It is a symmetric and idempotent ($H^2=H$) projection matrix. The diagonal elements of the hat matrix, $h_{ii}$, are the leverages of each observation $i$. A leverage $h_{ii}$ measures the influence of the response value $y_i$ on its own fitted value $\\hat{y}_i$, as $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_{ii}$. It also reflects the distance of the predictor vector for observation $i$ from the center of the predictor space. The values of $h_{ii}$ are bounded such that $1/n \\le h_{ii} \\le 1$.\n\n4.  **Residual Variance Estimation**: An unbiased estimator for the variance of the errors, $\\sigma^2$, is the Mean Squared Error (MSE), also denoted by $s^2$:\n    $$ s^2 = \\frac{r^\\top r}{n - p} = \\frac{\\sum_{i=1}^{n} r_i^2}{n - p} $$\n    where $n-p$ are the residual degrees of freedom.\n\n5.  **Externally Studentized Residuals**: The externally studentized residual, $t_i$, for observation $i$ is calculated by comparing the residual $r_i$ to its standard error, where the standard error is estimated from a model fitted to all data points *except* observation $i$. Let $s_{(i)}^2$ be the mean squared error calculated without observation $i$. The studentized residual is:\n    $$ t_i = \\frac{r_i}{s_{(i)} \\sqrt{1 - h_{ii}}} $$\n    A computationally efficient formula for $s_{(i)}^2$ avoids re-fitting the model $n$ times:\n    $$ s_{(i)}^2 = \\frac{(n - p) s^2 - \\frac{r_i^2}{1 - h_{ii}}}{n - p - 1} $$\n    These residuals follow a Student's $t$-distribution with $n - p - 1$ degrees of freedom under the standard OLS assumptions.\n\n6.  **Cook's Distance**: Cook's distance, $D_i$, measures the aggregate change in all estimated coefficients when observation $i$ is removed from the dataset. It is a measure of the overall influence of an observation on the model's fitted values. It is calculated as:\n    $$ D_i = \\frac{r_i^2}{p s^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2} $$\n    This formula combines the residual magnitude (through $r_i^2$) and the leverage ($h_{ii}$) to quantify influence.\n\n7.  **Identification of Influential Observations**: An observation is flagged as influential if it satisfies either of two common diagnostic criteria:\n    a.  Its Cook's distance is large. A common threshold is $D_i > \\frac{4}{n}$.\n    b.  It has both high leverage and a large residual. A common threshold for high leverage is $h_{ii} > \\frac{2p}{n}$. When this is the case, the magnitude of its externally studentized residual is also checked against a critical value from the Student's $t$-distribution. The specific rule is $|t_i| > t_{\\alpha/2}(n - p - 1)$, where $\\alpha=0.05$ for a $95\\%$ confidence level, so we use the $0.975$ quantile, $t_{0.975}(n - p - 1)$.\n\nThe algorithm proceeds by applying these steps to each of the three test cases, constructing the appropriate $X$ and $y$ from the provided data, computing the vectors of $h_{ii}$, $t_i$, and $D_i$, and then applying the decision rules to identify the indices of influential points. All floating-point results are rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"discharge\": [80, 95, 110, 125, 140, 155, 170, 185],\n            \"temperature\": [10, 12, 11, 13, 15, 14, 16, 12],\n            \"load\": [255, 292, 327, 370, 418, 450, 491, 520],\n        },\n        {\n            \"discharge\": [80, 90, 100, 110, 120, 130, 140, 150, 350],\n            \"temperature\": [10, 11, 12, 13, 14, 12, 11, 13, 10],\n            \"load\": [218, 241, 266, 290, 315, 332, 351, 378, 812],\n        },\n        {\n            \"discharge\": [100, 110, 120, 130, 140, 150, 160, 170, 180, 140],\n            \"temperature\": [12, 13, 14, 12, 13, 15, 14, 16, 15, 13],\n            \"load\": [262, 284, 307, 322, 345, 370, 388, 415, 431, 420],\n        }\n    ]\n\n    all_results = []\n    \n    for case_data in test_cases:\n        discharge_vec = np.array(case_data[\"discharge\"])\n        temp_vec = np.array(case_data[\"temperature\"])\n        y = np.array(case_data[\"load\"])\n        n = len(y)\n        p = 3  # Intercept, discharge, temperature\n\n        # Construct the design matrix X\n        X = np.ones((n, p))\n        X[:, 1] = discharge_vec\n        X[:, 2] = temp_vec\n\n        # Calculate diagnostics\n        h_values, t_values, D_values, influential_indices = _calculate_diagnostics(X, y, n, p)\n        \n        # Format results for output\n        case_result = [\n            list(np.round(h_values, 6)),\n            list(np.round(t_values, 6)),\n            list(np.round(D_values, 6)),\n            influential_indices\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # str() on a list creates a string like '[1, 2, 3]'.\n    # Joining these with commas and enclosing in brackets creates the desired format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef _calculate_diagnostics(X, y, n, p):\n    \"\"\"\n    Calculate regression diagnostics for a given design matrix X and response y.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n        n (int): Number of observations.\n        p (int): Number of parameters.\n        \n    Returns:\n        tuple: A tuple containing:\n            - h_values (np.ndarray): Leverage values.\n            - t_values (np.ndarray): Externally studentized residuals.\n            - D_values (np.ndarray): Cook's distances.\n            - influential_indices (list): List of zero-based indices of influential points.\n    \"\"\"\n    # OLS estimator: beta_hat = (X.T @ X)^-1 @ X.T @ y\n    try:\n        XTX_inv = np.linalg.inv(X.T @ X)\n    except np.linalg.LinAlgError:\n        # This case should not happen with the provided test data\n        return [], [], [], []\n\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Residuals: r = y - X @ beta_hat\n    residuals = y - X @ beta_hat\n    \n    # Hat matrix diagonal (leverages): H = X @ (X.T @ X)^-1 @ X.T\n    # Efficient calculation of the diagonal h_ii\n    h_values = np.sum((X @ XTX_inv) * X, axis=1)\n\n    # Residual variance estimate: s^2\n    df_residual = n - p\n    ssr = np.sum(residuals**2)\n    s2 = ssr / df_residual\n    \n    # Externally studentized residuals (t_i) and Cook's distance (D_i)\n    t_values = np.zeros(n)\n    D_values = np.zeros(n)\n    \n    # Small value to prevent division by zero in 1 - h_ii\n    epsilon = 1e-12 \n\n    for i in range(n):\n        # Leave-one-out mean squared error: s_(i)^2\n        s2_i = (df_residual * s2 - residuals[i]**2 / (1 - h_values[i] + epsilon)) / (df_residual - 1)\n        # Avoid sqrt of negative number if s2_i is slightly negative due to precision\n        if s2_i  0: s2_i = 0\n        \n        # Externally studentized residual: t_i\n        t_values[i] = residuals[i] / (np.sqrt(s2_i) * np.sqrt(1 - h_values[i] + epsilon))\n        \n        # Cook's distance: D_i\n        D_values[i] = (residuals[i]**2 / (p * s2)) * (h_values[i] / (1 - h_values[i] + epsilon)**2)\n\n    # Identify influential points\n    influential_indices = []\n    \n    # Rule 1 threshold\n    cook_threshold = 4 / n\n    \n    # Rule 2 thresholds\n    leverage_threshold = 2 * p / n\n    df_t = n - p - 1\n    if df_t > 0:\n        t_critical = t.ppf(0.975, df=df_t)\n    else: # Should not happen for valid n, p\n        t_critical = float('inf')\n\n    for i in range(n):\n        # Check rule 1\n        rule1_triggered = D_values[i] > cook_threshold\n        \n        # Check rule 2\n        rule2_triggered = (h_values[i] > leverage_threshold) and (np.abs(t_values[i]) > t_critical)\n        \n        if rule1_triggered or rule2_triggered:\n            influential_indices.append(i)\n            \n    return h_values, t_values, D_values, influential_indices\n\nsolve()\n```"
        },
        {
            "introduction": "Residual analysis extends beyond static regression models into the dynamic world of data assimilation, such as in atmospheric transport modeling with Kalman filters. In this context, residuals are called 'innovations,' and their statistical properties are critical for judging filter consistency. This advanced practice will guide you through computing standardized innovations and performing a $\\chi^2$ consistency test, a core validation technique in state-space modeling .",
            "id": "3862130",
            "problem": "Consider a linear Gaussian state-space model for atmospheric transport with a Kalman filtering framework. The state evolution is $x_{t+1} = F_t x_t + w_t$ and the observation model is $y_t = H_t x_t + v_t$, where $w_t \\sim \\mathcal{N}(0, Q_t)$ and $v_t \\sim \\mathcal{N}(0, R_t)$. Let the Kalman filter produce a prior (forecast) state estimate $x_t^{-}$ and its prior covariance $P_t^{-}$ before assimilating $y_t$. Define the innovation (observation residual) at time $t$ as $\\nu_t = y_t - H_t x_t^{-}$ and the innovation covariance as $S_t = H_t P_t^{-} H_t^\\top + R_t$. Under the standard linear Gaussian assumptions, $\\nu_t \\sim \\mathcal{N}(0, S_t)$. The Normalized Innovation Squared (NIS) is $n_t = \\nu_t^\\top S_t^{-1} \\nu_t$, which follows a chi-squared distribution with degrees of freedom equal to the observation dimension $p$ when the filter is consistent.\n\nYou are asked to compute standardized innovations and perform a chi-squared consistency test over a time window. Use the following definitions and requirements:\n\n- The standardized innovation is $z_t = S_t^{-1/2} \\nu_t$, where $S_t^{-1/2}$ is defined via the Cholesky factorization. Specifically, find a lower-triangular matrix $L_t$ with $L_t L_t^\\top = S_t$ and define $z_t$ as the solution of $L_t z_t = \\nu_t$. This yields $n_t = z_t^\\top z_t$ and $z_t$ is dimensionless.\n- Over a window of length $m$, define the window statistic $W = \\sum_{t=1}^{m} n_t$ and degrees of freedom (DoF) $k = m p$. Under filter consistency, $W \\sim \\chi^2_k$. Use the upper-tail p-value $p_{\\text{val}} = 1 - F_{\\chi^2_k}(W)$, where $F_{\\chi^2_k}$ is the cumulative distribution function of the chi-squared distribution with $k$ degrees of freedom. Declare the test passed if $p_{\\text{val}} \\ge \\alpha$, where $\\alpha$ is the significance level.\n- For numerical robustness, if $S_t$ is ill-conditioned, regularize by adding a jitter $\\delta I_p$, where $I_p$ is the $p \\times p$ identity matrix, with $\\delta = 10^{-9}$. Use $S_t^{\\text{reg}} = S_t + \\delta I_p$ for both the Cholesky factorization and all derived quantities.\n\nImplementation requirements:\n\n- For each time $t$ in each test case, compute $v_t$, $S_t$, $z_t$, $n_t$, and accumulate $W$. Compute $k = m p$, the p-value $p_{\\text{val}}$, and a boolean test outcome indicating whether $p_{\\text{val}} \\ge \\alpha$. Also compute the maximum absolute component across all standardized innovations in the window, $\\max_{t,i} |z_{t,i}|$.\n- For each test case, your program must output the list $[W_{\\text{rounded}}, k, p_{\\text{val,rounded}}, \\text{pass}, \\max|z|_{\\text{rounded}}]$, where $W_{\\text{rounded}}$ and $p_{\\text{val,rounded}}$ and $\\max|z|_{\\text{rounded}}$ are rounded to six decimal places. The boolean $\\text{pass}$ indicates whether the window consistency test is not rejected at level $\\alpha$.\n- Final output format: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list. For example, \"[[...],[...],...]\".\n\nTest suite (all matrices and vectors are provided as exact numeric values, and should be used as-is):\n\n- Case 1 (happy path, two-dimensional observations):\n    - $m = 3$, $p = 2$, $n = 2$, $\\alpha = 0.05$.\n    - $H_t = \\begin{bmatrix} 1  0.5 \\\\ 0  1 \\end{bmatrix}$ for all $t$.\n    - $P_t^{-} = \\begin{bmatrix} 0.2  0 \\\\ 0  0.1 \\end{bmatrix}$ for all $t$.\n    - $R_t = \\begin{bmatrix} 0.05  0 \\\\ 0  0.08 \\end{bmatrix}$ for all $t$.\n    - $x_1^{-} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$, $x_2^{-} = \\begin{bmatrix} 0.8 \\\\ -0.3 \\end{bmatrix}$, $x_3^{-} = \\begin{bmatrix} 1.2 \\\\ -0.4 \\end{bmatrix}$.\n    - Observations:\n      $y_1 = \\begin{bmatrix} 0.80 \\\\ -0.52 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0.64 \\\\ -0.27 \\end{bmatrix}$, $y_3 = \\begin{bmatrix} 1.02 \\\\ -0.39 \\end{bmatrix}$.\n- Case 2 (inconsistent residuals, two-dimensional observations, small covariances):\n    - $m = 2$, $p = 2$, $n = 2$, $\\alpha = 0.05$.\n    - $H_t = \\begin{bmatrix} 1  0.5 \\\\ 0  1 \\end{bmatrix}$ for all $t$.\n    - $P_t^{-} = \\begin{bmatrix} 0.02  0 \\\\ 0  0.02 \\end{bmatrix}$ for all $t$.\n    - $R_t = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}$ for all $t$.\n    - $x_1^{-} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$, $x_2^{-} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n    - Observations:\n      $y_1 = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} -0.75 \\\\ 0.6 \\end{bmatrix}$.\n- Case 3 (boundary condition, scalar observation):\n    - $m = 1$, $p = 1$, $n = 1$, $\\alpha = 0.1$.\n    - $H_1 = \\begin{bmatrix} 1 \\end{bmatrix}$.\n    - $P_1^{-} = \\begin{bmatrix} 0.5 \\end{bmatrix}$.\n    - $R_1 = \\begin{bmatrix} 0.2 \\end{bmatrix}$.\n    - $x_1^{-} = \\begin{bmatrix} 2.0 \\end{bmatrix}$.\n    - Observation:\n      $y_1 = \\begin{bmatrix} 2.3 \\end{bmatrix}$.\n- Case 4 (near-singular innovation covariance, two-dimensional observations):\n    - $m = 2$, $p = 2$, $n = 2$, $\\alpha = 0.05$.\n    - $H_t = \\begin{bmatrix} 1  10^{-4} \\\\ 10^{-4}  1 \\end{bmatrix}$ for all $t$.\n    - $P_t^{-} = \\begin{bmatrix} 10^{-6}  0 \\\\ 0  10^{-6} \\end{bmatrix}$ for all $t$.\n    - $R_t = \\begin{bmatrix} 10^{-8}  0 \\\\ 0  10^{-8} \\end{bmatrix}$ for all $t$.\n    - $x_1^{-} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$, $x_2^{-} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n    - Observations:\n      $y_1 = \\begin{bmatrix} 10^{-4} \\\\ -10^{-4} \\end{bmatrix}$, $y_2 = \\begin{bmatrix} -2 \\cdot 10^{-4} \\\\ 10^{-4} \\end{bmatrix}$.\n\nNumerical and output specifications:\n\n- Use $\\delta = 10^{-9}$ for regularization, applied as $S_t^{\\text{reg}} = S_t + \\delta I_p$.\n- Angles do not appear in this problem; no angle unit is required.\n- All returned values are dimensionless.\n- The final program output must be a single line with a comma-separated list of per-case results enclosed in square brackets. Each per-case result is a list $[W_{\\text{rounded}}, k, p_{\\text{val,rounded}}, \\text{pass}, \\max|z|_{\\text{rounded}}]$.",
            "solution": "The user has provided a problem centered on the analysis of residuals in a linear Gaussian state-space model, specifically within the context of a Kalman filter. The task requires the computation of standardized innovations and the execution of a chi-squared consistency test over a defined time window.\n\nThe problem is scientifically sound, well-posed, and provides all necessary information to proceed with a solution. The definitions and methodologies described—such as the innovation, innovation covariance, Normalized Innovation Squared (NIS), and the chi-squared test for filter consistency—are standard and fundamental concepts in statistical signal processing and data assimilation. The use of Cholesky factorization for calculating standardized innovations is a numerically stable and efficient approach.\n\nThe solution will be implemented by creating a function that processes each test case. This function will iteratively compute the required quantities for each time step within the specified window, accumulate the test statistic, and then perform the final statistical evaluation.\n\nThe overall algorithm for each test case is as follows:\n\n1.  Initialize the total window statistic $W$ to $0$ and create a list to store all components of the standardized innovations, $z_{t,i}$.\n2.  The innovation covariance matrix, $S_t = H_t P_t^{-} H_t^\\top + R_t$, and its Cholesky factorization are key to the process. Since the matrices $H_t$, $P_t^{-}$, and $R_t$ are constant within each test case's time window, we can pre-compute the innovation covariance $S$ and its regularized form $S^{\\text{reg}} = S + \\delta I_p$.\n3.  Perform a Cholesky decomposition on $S^{\\text{reg}}$ to obtain a lower-triangular matrix $L$ such that $L L^\\top = S^{\\text{reg}}$. This decomposition is performed once per test case for efficiency.\n4.  Iterate through each time step $t$ from $1$ to the window length $m$:\n    a. Calculate the innovation (or observation residual) vector: $\\nu_t = y_t - H_t x_t^{-}$. This measures the discrepancy between the actual observation $y_t$ and the observation predicted from the prior state estimate $x_t^{-}$.\n    b. Compute the standardized innovation vector $z_t$. This is defined by the relation $L z_t = \\nu_t$. This linear system is solved efficiently for $z_t$ using forward substitution, as $L$ is lower-triangular. The vector $z_t$ represents the innovation normalized by its covariance, resulting in a quantity that, under ideal conditions, is drawn from a standard normal distribution $\\mathcal{N}(0, I_p)$.\n    c. Store the components of the computed $z_t$ vector for later analysis.\n    d. Calculate the Normalized Innovation Squared (NIS) for the current time step: $n_t = z_t^\\top z_t$. This scalar value follows a $\\chi^2_p$ distribution if the filter is consistent.\n    e. Accumulate the window statistic by adding the current NIS value: $W = W + n_t$.\n5.  After the loop completes, finalize the window-based analysis:\n    a. The total degrees of freedom for the window test statistic $W$ is $k = m \\times p$, where $p$ is the dimension of the observation vector.\n    b. The statistic $W$ is expected to follow a chi-squared distribution with $k$ degrees of freedom, $W \\sim \\chi^2_k$.\n    c. Calculate the upper-tail p-value for the observed statistic $W$: $p_{\\text{val}} = P(\\chi^2_k > W) = 1 - F_{\\chi^2_k}(W)$, where $F_{\\chi^2_k}$ is the cumulative distribution function (CDF) of the chi-squared distribution. This p-value represents the probability of observing a total squared innovation as large as or larger than $W$ if the model and filter are correct.\n    d. Compare the p-value to the given significance level $\\alpha$. The consistency test is considered passed if $p_{\\text{val}} \\ge \\alpha$.\n    e. Determine the maximum absolute value among all components of all standardized innovation vectors $z_t$ computed within the window, $\\max_{t,i} |z_{t,i}|$. This provides a measure of the single largest normalized deviation observed.\n6.  The final results for the test case—$[W, k, p_{\\text{val}}, \\text{pass}, \\max|z|]$—are assembled, with floating-point values rounded to six decimal places as specified. This process is repeated for all provided test cases.\n\nThe implementation will use the `numpy` library for numerical linear algebra operations (matrix and vector manipulations) and the `scipy` library for the Cholesky decomposition (`scipy.linalg.cholesky`), solving triangular systems (`scipy.linalg.solve_triangular`), and the chi-squared CDF (`scipy.stats.chi2.sf`, which directly computes the survival function $1 - \\text{CDF}$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef analyze_innovations(m, p, alpha, delta, H, P_minus, R, x_minus_series, y_series):\n    \"\"\"\n    Computes standardized innovations and performs a chi-squared consistency test.\n\n    Args:\n        m (int): Window length.\n        p (int): Observation dimension.\n        alpha (float): Significance level.\n        delta (float): Regularization jitter.\n        H (np.ndarray): Observation operator matrix (p x n).\n        P_minus (np.ndarray): Prior state error covariance matrix (n x n).\n        R (np.ndarray): Observation error covariance matrix (p x p).\n        x_minus_series (list[np.ndarray]): List of prior state estimates.\n        y_series (list[np.ndarray]): List of observations.\n    \n    Returns:\n        list: A list containing [W_rounded, k, p_val_rounded, pass_test, max_z_rounded].\n    \"\"\"\n    W_total = 0.0\n    all_z_components = []\n\n    # Calculate the innovation covariance S_t. Since H, P_minus, R are constant\n    # for all t in a given test case, S_t is also constant.\n    S = H @ P_minus @ H.T + R\n    S_reg = S + delta * np.identity(p)\n    \n    try:\n        # Cholesky decomposition of the regularized innovation covariance\n        # L is a lower-triangular matrix such that L @ L.T = S_reg\n        L = cholesky(S_reg, lower=True)\n    except np.linalg.LinAlgError:\n        # This case should be handled if S_reg is not positive definite,\n        # but the regularization makes this highly unlikely. The problem\n        # assumes this step is successful.\n        # For this problem, we'll assume it doesn't fail.\n        # A robust implementation might return an error indicator.\n        return [np.nan, m * p, np.nan, False, np.nan]\n\n    for t in range(m):\n        x_minus = x_minus_series[t]\n        y = y_series[t]\n        \n        # 1. Calculate innovation (residual)\n        # v_t = y_t - H_t * x_t^-\n        v = y - H @ x_minus\n        \n        # 2. Calculate standardized innovation\n        # Solve L * z_t = v_t for z_t using forward substitution\n        z = solve_triangular(L, v, lower=True)\n        all_z_components.extend(z)\n        \n        # 3. Calculate Normalized Innovation Squared (NIS) for this time step\n        # n_t = z_t^T * z_t\n        n_t = np.dot(z, z)\n        \n        # 4. Accumulate the window statistic W\n        W_total += n_t\n        \n    # 5. Calculate total degrees of freedom\n    k = m * p\n    \n    # 6. Calculate the upper-tail p-value for the chi-squared test\n    # p_val = 1 - CDF(W | k) = SurvivalFunction(W | k)\n    p_val = chi2.sf(W_total, k)\n    \n    # 7. Determine if the test is passed\n    pass_test = p_val >= alpha\n    \n    # 8. Find the maximum absolute component of all standardized innovations\n    max_abs_z = 0.0\n    if all_z_components:\n        max_abs_z = np.max(np.abs(all_z_components))\n        \n    # 9. Round results to six decimal places\n    W_rounded = round(W_total, 6)\n    p_val_rounded = round(p_val, 6)\n    max_z_rounded = round(max_abs_z, 6)\n    \n    return [W_rounded, k, p_val_rounded, pass_test, max_z_rounded]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, two-dimensional observations):\n        {\n            \"m\": 3, \"p\": 2, \"n\": 2, \"alpha\": 0.05,\n            \"H\": np.array([[1, 0.5], [0, 1]]),\n            \"P_minus\": np.array([[0.2, 0], [0, 0.1]]),\n            \"R\": np.array([[0.05, 0], [0, 0.08]]),\n            \"x_minus_series\": [np.array([1.0, -0.5]), np.array([0.8, -0.3]), np.array([1.2, -0.4])],\n            \"y_series\": [np.array([0.80, -0.52]), np.array([0.64, -0.27]), np.array([1.02, -0.39])]\n        },\n        # Case 2 (inconsistent residuals, two-dimensional observations, small covariances):\n        {\n            \"m\": 2, \"p\": 2, \"n\": 2, \"alpha\": 0.05,\n            \"H\": np.array([[1, 0.5], [0, 1]]),\n            \"P_minus\": np.array([[0.02, 0], [0, 0.02]]),\n            \"R\": np.array([[0.01, 0], [0, 0.01]]),\n            \"x_minus_series\": [np.array([0.0, 0.0]), np.array([0.1, -0.1])],\n            \"y_series\": [np.array([1.0, -0.5]), np.array([-0.75, 0.6])]\n        },\n        # Case 3 (boundary condition, scalar observation):\n        {\n            \"m\": 1, \"p\": 1, \"n\": 1, \"alpha\": 0.1,\n            \"H\": np.array([[1]]),\n            \"P_minus\": np.array([[0.5]]),\n            \"R\": np.array([[0.2]]),\n            \"x_minus_series\": [np.array([2.0])],\n            \"y_series\": [np.array([2.3])]\n        },\n        # Case 4 (near-singular innovation covariance, two-dimensional observations):\n        {\n            \"m\": 2, \"p\": 2, \"n\": 2, \"alpha\": 0.05,\n            \"H\": np.array([[1, 1e-4], [1e-4, 1]]),\n            \"P_minus\": np.array([[1e-6, 0], [0, 1e-6]]),\n            \"R\": np.array([[1e-8, 0], [0, 1e-8]]),\n            \"x_minus_series\": [np.array([0.0, 0.0]), np.array([0.0, 0.0])],\n            \"y_series\": [np.array([1e-4, -1e-4]), np.array([-2e-4, 1e-4])]\n        }\n    ]\n\n    results = []\n    delta = 1e-9  # Regularization jitter is constant for all cases\n\n    for case in test_cases:\n        result = analyze_innovations(\n            case[\"m\"], case[\"p\"], case[\"alpha\"], delta,\n            case[\"H\"], case[\"P_minus\"], case[\"R\"],\n            case[\"x_minus_series\"], case[\"y_series\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation and join with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}