{
    "hands_on_practices": [
        {
            "introduction": "The first step in evaluating any environmental model is to examine its residuals, the differences between observed and predicted values. These residuals are not just random noise; they contain critical information about your model's performance. This practice guides you through computing three fundamental statistical summaries: the mean, variance, and lag-1 autocorrelation. By analyzing these properties, you can diagnose systematic model bias, quantify the overall error magnitude, and detect uncaptured temporal patterns in your model's predictions .",
            "id": "3862150",
            "problem": "Consider a monthly hydrologic time series context in which observed streamflow $Q_t$ (in cubic meters per second, written as $\\mathrm{m^3/s}$) is paired with a model-estimated streamflow $\\hat{Q}_t$ for $t = 1, \\dots, n$. Define the model residual at time $t$ as $r_t = Q_t - \\hat{Q}_t$, with $r_t$ in $\\mathrm{m^3/s}$. From a first-principles perspective, use the classical sample definitions from descriptive statistics and time series analysis (Autocorrelation Function (ACF)) to compute three summaries of the residual sequence: the sample mean, the unbiased sample variance, and the sample autocorrelation at lag $1$. The lag-$1$ autocorrelation must be computed using the correlation of consecutive residual anomalies about their sample mean. If the sample variance of $r_t$ is zero for a given case, define the lag-$1$ autocorrelation as $0$ by convention. Express the mean residual in $\\mathrm{m^3/s}$, the variance in $(\\mathrm{m^3/s})^2$, and the lag-$1$ autocorrelation as a pure number without units. Angles are not involved in this task.\n\nStarting from foundational definitions only (sample mean, unbiased sample variance, and sample autocorrelation via sample covariance), implement a program to produce the three residual summaries for each of the provided test cases. No shortcut formulas are permitted in the problem statement; the solution must be derived from core definitions.\n\nUse the following test suite. Each test case provides sequences $(Q_t)_{t=1}^n$ and $(\\hat{Q}_t)_{t=1}^n$:\n\n- Test Case $1$ (seasonal model bias, $n = 12$):\n  - $Q_t = \\{42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50\\}$.\n  - $\\hat{Q}_t = \\{40, 50, 58, 80, 120, 150, 180, 170, 130, 95, 75, 55\\}$.\n\n- Test Case $2$ (perfect model, $n = 12$):\n  - $Q_t = \\{60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60\\}$.\n  - $\\hat{Q}_t = \\{60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60\\}$.\n\n- Test Case $3$ (constant bias, $n = 12$):\n  - $Q_t = \\{42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50\\}$.\n  - $\\hat{Q}_t = \\{22, 35, 40, 55, 90, 140, 170, 155, 120, 80, 50, 30\\}$.\n\n- Test Case $4$ (strong positive serial dependence in residuals, $n = 12$):\n  - $Q_t = \\{50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60\\}$.\n  - $\\hat{Q}_t = \\{45, 52, 58, 75, 103, 141, 170, 162, 134, 96, 70, 53\\}$.\n\n- Test Case $5$ (alternating residuals, negative serial dependence, $n = 12$):\n  - $Q_t = \\{50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60\\}$.\n  - $\\hat{Q}_t = \\{40, 70, 61, 99, 112, 168, 183, 187, 156, 104, 85, 55\\}$.\n\n- Test Case $6$ (boundary case with minimal length, $n = 2$):\n  - $Q_t = \\{100, 120\\}$.\n  - $\\hat{Q}_t = \\{95, 130\\}$.\n\nFor each test case, compute:\n- The sample mean $\\bar{r}$ (in $\\mathrm{m^3/s}$).\n- The unbiased sample variance $s_r^2$ (in $(\\mathrm{m^3/s})^2$).\n- The lag-$1$ autocorrelation $\\rho_1$ (unitless).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of three decimal values $\\left[\\bar{r}, s_r^2, \\rho_1\\right]$ rounded to $6$ decimal places, and the overall output must be a list of these lists in order of the test cases, for example: $\\left[[x_1,y_1,z_1],[x_2,y_2,z_2],\\dots\\right]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of time series analysis and descriptive statistics as applied to environmental model evaluation. It is well-posed, with all necessary data and definitions provided to compute a unique and meaningful solution. The language is objective and precise. I will proceed with a solution derived from first principles as requested.\n\nThe task is to compute three statistical summaries for a time series of model residuals, $r_t = Q_t - \\hat{Q}_t$, where $Q_t$ is the observed streamflow and $\\hat{Q}_t$ is the model-estimated streamflow at time $t$, for $t = 1, \\dots, n$. The required summaries are the sample mean ($\\bar{r}$), the unbiased sample variance ($s_r^2$), and the lag-$1$ sample autocorrelation ($\\rho_1$).\n\nLet the sequence of residuals be $r = \\{r_1, r_2, \\dots, r_n\\}$.\n\n**1. Sample Mean ($\\bar{r}$)**\n\nThe sample mean is the arithmetic average of the residual values. This is the first moment of the sample distribution. Its classical definition is the sum of all observations divided by the number of observations, $n$.\n\n$$ \\bar{r} = \\frac{1}{n} \\sum_{t=1}^{n} r_t $$\n\nThe units of $\\bar{r}$ are the same as the units of $r_t$, which is $\\mathrm{m^3/s}$.\n\n**2. Unbiased Sample Variance ($s_r^2$)**\n\nThe unbiased sample variance measures the dispersion of the data around the sample mean. The term \"unbiased\" signifies that the denominator is $n-1$, which corrects for the bias introduced by using the sample mean instead of the true (but unknown) population mean. It is defined as the sum of the squared deviations from the sample mean, divided by $n-1$.\n\n$$ s_r^2 = \\frac{1}{n-1} \\sum_{t=1}^{n} (r_t - \\bar{r})^2 $$\n\nThis definition is valid for $n > 1$. All test cases provided satisfy this condition. The units of $s_r^2$ are the square of the units of $r_t$, which is $(\\mathrm{m^3/s})^2$.\n\n**3. Lag-1 Sample Autocorrelation ($\\rho_1$)**\n\nThe lag-$1$ sample autocorrelation measures the linear relationship between a residual value, $r_t$, and the subsequent residual value, $r_{t+1}$. The problem requires its computation \"using the correlation of consecutive residual anomalies about their sample mean.\" This corresponds to the standard definition of the sample autocorrelation function (ACF) at lag $1$, which is the sample autocovariance at lag $1$ divided by the sample variance (or autocovariance at lag $0$).\n\nThe sample autocovariance at lag $k$, denoted $\\hat{\\gamma}(k)$, is defined as:\n$$ \\hat{\\gamma}(k) = \\frac{1}{n} \\sum_{t=1}^{n-k} (r_t - \\bar{r})(r_{t+k} - \\bar{r}) $$\n\nFor lag $k=1$, the autocovariance is:\n$$ \\hat{\\gamma}(1) = \\frac{1}{n} \\sum_{t=1}^{n-1} (r_t - \\bar{r})(r_{t+1} - \\bar{r}) $$\n\nThe sample variance, which is the autocovariance at lag $k=0$, is:\n$$ \\hat{\\gamma}(0) = \\frac{1}{n} \\sum_{t=1}^{n} (r_t - \\bar{r})^2 $$\n\nThe lag-$1$ sample autocorrelation, $\\rho_1$, is the ratio of $\\hat{\\gamma}(1)$ to $\\hat{\\gamma}(0)$. The factor of $1/n$ in both numerator and denominator cancels.\n\n$$ \\rho_1 = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)} = \\frac{\\frac{1}{n} \\sum_{t=1}^{n-1} (r_t - \\bar{r})(r_{t+1} - \\bar{r})}{\\frac{1}{n} \\sum_{t=1}^{n} (r_t - \\bar{r})^2} = \\frac{\\sum_{t=1}^{n-1} (r_t - \\bar{r})(r_{t+1} - \\bar{r})}{\\sum_{t=1}^{n} (r_t - \\bar{r})^2} $$\n\nThe quantity $\\rho_1$ is a dimensionless pure number, as the units in the numerator, $(\\mathrm{m^3/s})^2$, cancel with the units in the denominator. Per the problem statement, a special case must be handled: if the sample variance is zero, which occurs when the denominator $\\sum_{t=1}^{n} (r_t - \\bar{r})^2$ is zero, then $\\rho_1$ is defined to be $0$. This convention avoids division by zero. This occurs if and only if all residuals $r_t$ are identical, resulting in $s_r^2=0$.\n\nThe implementation will follow these three derived formulas for each provided test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes residual summaries for multiple test cases and prints the results.\n    The summaries are: sample mean, unbiased sample variance, and lag-1 autocorrelation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (seasonal model bias, n = 12)\n        (\n            np.array([42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50], dtype=np.float64),\n            np.array([40, 50, 58, 80, 120, 150, 180, 170, 130, 95, 75, 55], dtype=np.float64)\n        ),\n        # Test Case 2 (perfect model, n = 12)\n        (\n            np.array([60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60], dtype=np.float64),\n            np.array([60, 65, 70, 90, 130, 170, 200, 190, 150, 110, 80, 60], dtype=np.float64)\n        ),\n        # Test Case 3 (constant bias, n = 12)\n        (\n            np.array([42, 55, 60, 75, 110, 160, 190, 175, 140, 100, 70, 50], dtype=np.float64),\n            np.array([22, 35, 40, 55, 90, 140, 170, 155, 120, 80, 50, 30], dtype=np.float64)\n        ),\n        # Test Case 4 (strong positive serial dependence in residuals, n = 12)\n        (\n            np.array([50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60], dtype=np.float64),\n            np.array([45, 52, 58, 75, 103, 141, 170, 162, 134, 96, 70, 53], dtype=np.float64)\n        ),\n        # Test Case 5 (alternating residuals, negative serial dependence, n = 12)\n        (\n            np.array([50, 60, 70, 90, 120, 160, 190, 180, 150, 110, 80, 60], dtype=np.float64),\n            np.array([40, 70, 61, 99, 112, 168, 183, 187, 156, 104, 85, 55], dtype=np.float64)\n        ),\n        # Test Case 6 (boundary case with minimal length, n = 2)\n        (\n            np.array([100, 120], dtype=np.float64),\n            np.array([95, 130], dtype=np.float64)\n        )\n    ]\n    \n    results_list = []\n    \n    for Q_t, Q_hat_t in test_cases:\n        \n        # 0. Calculate the residual series r_t = Q_t - Q_hat_t\n        r_t = Q_t - Q_hat_t\n        n = len(r_t)\n\n        # 1. Compute the sample mean of residuals, r_bar\n        # Formula: r_bar = (1/n) * sum(r_t) for t=1 to n\n        r_bar = np.mean(r_t)\n\n        # 2. Compute the unbiased sample variance, s_r^2\n        # Formula: s_r^2 = (1/(n-1)) * sum((r_t - r_bar)^2) for t=1 to n\n        # numpy.var with ddof=1 calculates the unbiased sample variance.\n        # This check is only for n=1, but all test cases have n>1.\n        if n > 1:\n            s_r_sq = np.var(r_t, ddof=1)\n        else:\n            s_r_sq = 0.0\n\n        # 3. Compute the lag-1 autocorrelation, rho_1\n        # Formula: rho_1 = sum((r_t - r_bar)(r_{t+1} - r_bar)) / sum((r_t - r_bar)^2)\n        # The sum in the numerator is from t=1 to n-1.\n        \n        # Calculate deviations from the mean (anomalies)\n        anomalies = r_t - r_bar\n        \n        # Calculate the denominator: sum of squared anomalies\n        sum_sq_anomalies = np.sum(anomalies**2)\n\n        # Handle the case where variance is zero, as specified in the problem\n        if sum_sq_anomalies  1e-12: # Use a small tolerance for float comparison\n            rho_1 = 0.0\n        else:\n            # Numerator: sum of products of consecutive anomalies\n            # anomalies[:-1] is {r_1-r_bar, ..., r_{n-1}-r_bar}\n            # anomalies[1:] is {r_2-r_bar, ..., r_n-r_bar}\n            numerator = np.sum(anomalies[:-1] * anomalies[1:])\n            rho_1 = numerator / sum_sq_anomalies\n        \n        # Store results for this case, formatted to 6 decimal places\n        case_result = [r_bar, s_r_sq, rho_1]\n        results_list.append(case_result)\n\n    # Format the final output string as a list of lists.\n    # Each sublist contains the three computed values rounded to 6 decimal places.\n    output_parts = []\n    for res in results_list:\n        # Format each number to 6 decimal places (f-string with :.6f)\n        # and create the string for one sub-list, e.g., \"[val1,val2,val3]\"\n        formatted_res = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_parts.append(formatted_res)\n\n    # Join the sub-list strings with commas and enclose in brackets,\n    # e.g., \"[[...],[...]]\"\n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "While a full analysis of the residual series is invaluable, we often need single-number metrics like Root Mean Square Error ($RMSE$) and Mean Absolute Error ($MAE$) to summarize and compare model performance. However, these metrics are not interchangeable, as their mathematical definitions give them different sensitivities to outliers. This exercise provides a hands-on numerical experiment to explore this critical difference . By systematically perturbing the largest errors, you will quantify how the squared-error basis of $RMSE$ makes it far more sensitive to outliers than the absolute-error basis of $MAE$, a crucial insight for robust model evaluation.",
            "id": "3862083",
            "problem": "You are given a deterministic experiment to quantify the sensitivity of error metrics to outliers in the context of residual analysis for environmental and Earth system models. Let a residual vector be denoted by $\\mathbf{r} = (r_1, r_2, \\dots, r_n) \\in \\mathbb{R}^n$, with $n \\in \\mathbb{N}$. The Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE) are defined by\n$$\n\\mathrm{RMSE}(\\mathbf{r}) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} r_i^2}, \\quad \\mathrm{MAE}(\\mathbf{r}) = \\frac{1}{n}\\sum_{i=1}^{n} |r_i|.\n$$\nTo mimic the effect of outliers, you will perturb a subset of the residuals by a nonnegative scaling factor. Given a fraction $f \\in [0,1]$, define $m = \\lfloor f n \\rfloor$. Construct the index set $\\mathcal{S} \\subset \\{1,2,\\dots,n\\}$ of size $m$ by selecting the $m$ indices corresponding to the largest values of $|r_i|$, breaking ties by choosing the smaller index first (that is, sort by descending $|r_i|$ and then by ascending index). For a given scaling factor $c \\ge 0$, define the perturbed residual vector $\\mathbf{r}'$ by\n$$\nr'_i = \\begin{cases}\nc \\, r_i,  \\text{if } i \\in \\mathcal{S},\\\\\nr_i,  \\text{if } i \\notin \\mathcal{S}.\n\\end{cases}\n$$\nFor each test case, compute:\n- the changes $\\Delta \\mathrm{RMSE} = \\mathrm{RMSE}(\\mathbf{r}') - \\mathrm{RMSE}(\\mathbf{r})$ and $\\Delta \\mathrm{MAE} = \\mathrm{MAE}(\\mathbf{r}') - \\mathrm{MAE}(\\mathbf{r})$,\n- the ratios $\\rho_{\\mathrm{RMSE}} = \\mathrm{RMSE}(\\mathbf{r}') / \\mathrm{RMSE}(\\mathbf{r})$ and $\\rho_{\\mathrm{MAE}} = \\mathrm{MAE}(\\mathbf{r}') / \\mathrm{MAE}(\\mathbf{r})$.\n\nIf a baseline denominator is zero, define the ratio to be $+\\infty$ when the numerator is positive and to be $1$ when both numerator and denominator are zero.\n\nYour program must compute these quantities for the following test suite. In each case, the residual vector $\\mathbf{r}$, the fraction $f$, and the scaling factor $c$ are specified:\n\n- Test case $1$: $\\mathbf{r} = [\\, $0.2$, $-0.1$, $0.3$, $-1.5$, $0.7$, $0.0$, $1.2$, $-0.4$, $0.6$, $-0.3$, $2.5$, $-0.2$, $0.1$, $-0.9$, $0.8$, $-0.7$, $0.4$, $-0.6$, $0.05$, $-0.05$ \\,]$, $f = $0.1$, $c = $3.0$.\n- Test case $2$: $\\mathbf{r} = [\\, $-0.2$, $0.2$, $-0.2$, $0.2$, $0.0$, $0.0$, $0.1$, $-0.1$, $0.15$, $-0.15$ \\,]$, $f = $0.0$, $c = $10.0$.\n- Test case $3$: $\\mathbf{r} = [\\, $1.0$, $-1.0$, $1.0$, $-1.0$, $1.0$, $-1.0$, $1.0$, $-1.0$ \\,]$, $f = $1.0$, $c = $2.0$.\n- Test case $4$: $\\mathbf{r} = [\\, $0.05$, $-0.04$, $0.06$, $-0.07$, $0.08$, $-0.09$, $0.1$, $-0.11$, $0.12$, $-0.13$, $5.0$, $-0.14$ \\,]$, $f = $0.1$, $c = $5.0$.\n- Test case $5$: $\\mathbf{r} = [\\, $0.9$, $-1.1$, $0.8$, $-0.7$, $0.6$, $-0.5$, $0.4$, $-0.3$, $0.2$, $-0.1$, $0.05$, $-0.05$, $0.0$, $1.5$, $-1.4$ \\,]$, $f = $0.2$, $c = $0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a nested list of four floating-point numbers in the order $[\\, \\Delta \\mathrm{RMSE}, \\Delta \\mathrm{MAE}, \\rho_{\\mathrm{RMSE}}, \\rho_{\\mathrm{MAE}} \\,]$. Each floating-point number must be rounded to exactly six decimal places. For example, a valid output format for three hypothetical cases would be\n[[$0.123456$,$-0.000001$,$1.234567$,$0.999999$],[$0.000000$,$0.000000$,$1.000000$,$1.000000$],[$0.010000$,$0.005000$,$1.010000$,$1.005000$]].",
            "solution": "The problem statement is a well-defined exercise in numerical sensitivity analysis, grounded in standard statistical metrics. It is scientifically sound, self-contained, and algorithmically specified. Therefore, the problem is valid and a solution can be derived.\n\nThe task is to compute the change and ratio of two error metrics, the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE), after perturbing a subset of a given residual vector. Let the original residual vector be $\\mathbf{r} = (r_1, r_2, \\dots, r_n) \\in \\mathbb{R}^n$. The definitions for RMSE and MAE are provided as:\n$$\n\\mathrm{RMSE}(\\mathbf{r}) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} r_i^2}\n$$\n$$\n\\mathrm{MAE}(\\mathbf{r}) = \\frac{1}{n}\\sum_{i=1}^{n} |r_i|\n$$\nA perturbed vector $\\mathbf{r}'$ is constructed based on a fraction $f \\in [0,1]$ and a scaling factor $c \\ge 0$. The number of elements to be perturbed is $m = \\lfloor f n \\rfloor$. The set of indices of these elements, denoted $\\mathcal{S}$, corresponds to the $m$ largest values of $|r_i|$, with ties broken by selecting the smaller index first. The perturbation is defined as:\n$$\nr'_i = \\begin{cases}\nc \\, r_i,  \\text{if } i \\in \\mathcal{S},\\\\\nr_i,  \\text{if } i \\notin \\mathcal{S}.\n\\end{cases}\n$$\nWe need to compute $\\Delta \\mathrm{RMSE} = \\mathrm{RMSE}(\\mathbf{r}') - \\mathrm{RMSE}(\\mathbf{r})$, $\\Delta \\mathrm{MAE} = \\mathrm{MAE}(\\mathbf{r}') - \\mathrm{MAE}(\\mathbf{r})$, $\\rho_{\\mathrm{RMSE}} = \\mathrm{RMSE}(\\mathbf{r}') / \\mathrm{RMSE}(\\mathbf{r})$, and $\\rho_{\\mathrm{MAE}} = \\mathrm{MAE}(\\mathbf{r}') / \\mathrm{MAE}(\\mathbf{r})$.\n\nFirst, let us establish the expressions for $\\mathrm{RMSE}(\\mathbf{r}')$ and $\\mathrm{MAE}(\\mathbf{r}')$. To do this efficiently, we can analyze the change in the sum of squares and the sum of absolute values.\n\nLet $S_2(\\mathbf{r}) = \\sum_{i=1}^{n} r_i^2$ and $S_1(\\mathbf{r}) = \\sum_{i=1}^{n} |r_i|$.\nThen, $\\mathrm{RMSE}(\\mathbf{r}) = \\sqrt{S_2(\\mathbf{r})/n}$ and $\\mathrm{MAE}(\\mathbf{r}) = S_1(\\mathbf{r})/n$.\n\nThe sum of squares for the perturbed vector $\\mathbf{r}'$ is:\n$$\nS_2(\\mathbf{r}') = \\sum_{i=1}^{n} (r'_i)^2 = \\sum_{i \\in \\mathcal{S}} (c r_i)^2 + \\sum_{i \\notin \\mathcal{S}} (r_i)^2\n$$\n$$\nS_2(\\mathbf{r}') = c^2 \\sum_{i \\in \\mathcal{S}} r_i^2 + \\left( \\sum_{i=1}^{n} r_i^2 - \\sum_{i \\in \\mathcal{S}} r_i^2 \\right) = \\sum_{i=1}^{n} r_i^2 + (c^2 - 1) \\sum_{i \\in \\mathcal{S}} r_i^2\n$$\nThus, $S_2(\\mathbf{r}') = S_2(\\mathbf{r}) + (c^2 - 1) \\sum_{i \\in \\mathcal{S}} r_i^2$.\nThe perturbed RMSE is:\n$$\n\\mathrm{RMSE}(\\mathbf{r}') = \\sqrt{\\frac{S_2(\\mathbf{r}')}{n}} = \\sqrt{\\frac{S_2(\\mathbf{r}) + (c^2 - 1) \\sum_{i \\in \\mathcal{S}} r_i^2}{n}}\n$$\n\nSimilarly, the sum of absolute values for $\\mathbf{r}'$ is (noting that $c \\ge 0$):\n$$\nS_1(\\mathbf{r}') = \\sum_{i=1}^{n} |r'_i| = \\sum_{i \\in \\mathcal{S}} |c r_i| + \\sum_{i \\notin \\mathcal{S}} |r_i| = c \\sum_{i \\in \\mathcal{S}} |r_i| + \\sum_{i \\notin \\mathcal{S}} |r_i|\n$$\n$$\nS_1(\\mathbf{r}') = c \\sum_{i \\in \\mathcal{S}} |r_i| + \\left( \\sum_{i=1}^{n} |r_i| - \\sum_{i \\in \\mathcal{S}} |r_i| \\right) = \\sum_{i=1}^{n} |r_i| + (c - 1) \\sum_{i \\in \\mathcal{S}} |r_i|\n$$\nThus, $S_1(\\mathbf{r}') = S_1(\\mathbf{r}) + (c - 1) \\sum_{i \\in \\mathcal{S}} |r_i|$.\nThe perturbed MAE is:\n$$\n\\mathrm{MAE}(\\mathbf{r}') = \\frac{S_1(\\mathbf{r}')}{n} = \\frac{S_1(\\mathbf{r}) + (c - 1) \\sum_{i \\in \\mathcal{S}} |r_i|}{n}\n$$\n\nThe algorithmic procedure for each test case is as follows:\n1.  For a given residual vector $\\mathbf{r}$, fraction $f$, and scaling factor $c$:\n2.  Determine the size of the vector, $n$.\n3.  Calculate the number of residuals to perturb, $m = \\lfloor f n \\rfloor$.\n4.  If $m = 0$, no perturbation occurs. $\\mathbf{r}' = \\mathbf{r}$, so $\\Delta \\mathrm{RMSE}=0$, $\\Delta \\mathrm{MAE}=0$, $\\rho_{\\mathrm{RMSE}}=1$, and $\\rho_{\\mathrm{MAE}}=1$.\n5.  If $m > 0$, identify the set of indices $\\mathcal{S}$. This requires sorting the indices based on $|r_i|$ in descending order, with the original index value used as a tie-breaker in ascending order. The first $m$ indices from this sorted list form $\\mathcal{S}$.\n6.  Calculate the baseline metrics: $\\mathrm{RMSE}(\\mathbf{r})$ and $\\mathrm{MAE}(\\mathbf{r})$.\n7.  Calculate the sums over the subset $\\mathcal{S}$: $\\sum_{i \\in \\mathcal{S}} r_i^2$ and $\\sum_{i \\in \\mathcal{S}} |r_i|$.\n8.  Use the derived formulae to compute the perturbed metrics: $\\mathrm{RMSE}(\\mathbf{r}')$ and $\\mathrm{MAE}(\\mathbf{r}')$.\n9.  Calculate the differences $\\Delta \\mathrm{RMSE}$ and $\\Delta \\mathrm{MAE}$.\n10. Calculate the ratios $\\rho_{\\mathrm{RMSE}}$ and $\\rho_{\\mathrm{MAE}}$. The rules for division by zero must be respected: if the denominator is $0$, the ratio is $1$ if the numerator is also $0$, and $+\\infty$ if the numerator is positive. For the given problem, a zero denominator implies the original vector $\\mathbf{r}$ is a zero vector, which in turn means the perturbed vector $\\mathbf{r}'$ is also a zero vector. Thus, the case of a zero denominator and non-zero numerator does not occur, and the ratio is $1$.\n\nThis procedure will be implemented for each test case provided. The final numerical results will be rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes sensitivity of RMSE and MAE to outliers for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (r, f, c)\n        (np.array([0.2, -0.1, 0.3, -1.5, 0.7, 0.0, 1.2, -0.4, 0.6, -0.3, 2.5, -0.2, 0.1, -0.9, 0.8, -0.7, 0.4, -0.6, 0.05, -0.05]), 0.1, 3.0),\n        (np.array([-0.2, 0.2, -0.2, 0.2, 0.0, 0.0, 0.1, -0.1, 0.15, -0.15]), 0.0, 10.0),\n        (np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0]), 1.0, 2.0),\n        (np.array([0.05, -0.04, 0.06, -0.07, 0.08, -0.09, 0.1, -0.11, 0.12, -0.13, 5.0, -0.14]), 0.1, 5.0),\n        (np.array([0.9, -1.1, 0.8, -0.7, 0.6, -0.5, 0.4, -0.3, 0.2, -0.1, 0.05, -0.05, 0.0, 1.5, -1.4]), 0.2, 0.5),\n    ]\n\n    all_results = []\n    \n    for r, f, c in test_cases:\n        n = len(r)\n        \n        # Calculate baseline metrics\n        sum_sq_r = np.sum(r**2)\n        sum_abs_r = np.sum(np.abs(r))\n        \n        rmse_r = np.sqrt(sum_sq_r / n)\n        mae_r = sum_abs_r / n\n\n        # Determine the number of residuals to perturb\n        m = int(np.floor(f * n))\n        \n        if m == 0:\n            # No perturbation, so changes are 0 and ratios are 1\n            delta_rmse = 0.0\n            delta_mae = 0.0\n            rho_rmse = 1.0\n            rho_mae = 1.0\n        else:\n            # Identify the index set S\n            # Sort indices by descending |r_i| and then by ascending index for tie-breaking\n            indices = sorted(range(n), key=lambda i: (-np.abs(r[i]), i))\n            s_indices = indices[:m]\n            \n            # Get the residuals in S\n            r_s = r[s_indices]\n            \n            # Calculate sums over the subset S\n            sum_sq_s = np.sum(r_s**2)\n            sum_abs_s = np.sum(np.abs(r_s))\n            \n            # Calculate perturbed sums\n            sum_sq_r_prime = sum_sq_r + (c**2 - 1) * sum_sq_s\n            sum_abs_r_prime = sum_abs_r + (c - 1) * sum_abs_s\n            \n            # Calculate perturbed metrics\n            rmse_r_prime = np.sqrt(sum_sq_r_prime / n)\n            mae_r_prime = sum_abs_r_prime / n\n            \n            # Calculate changes\n            delta_rmse = rmse_r_prime - rmse_r\n            delta_mae = mae_r_prime - mae_r\n\n            # Calculate ratios, handling zero denominator case\n            if rmse_r == 0:\n                # If rmse_r is 0, r is a zero vector, so r' is also zero vector, and rmse_r_prime is 0.\n                # Per problem spec, ratio is 1 if both are 0.\n                rho_rmse = 1.0\n            else:\n                rho_rmse = rmse_r_prime / rmse_r\n\n            if mae_r == 0:\n                # Same logic for MAE\n                rho_mae = 1.0\n            else:\n                rho_mae = mae_r_prime / mae_r\n\n        # Store results for this case, formatted to 6 decimal places\n        case_results = [\n            f\"{delta_rmse:.6f}\",\n            f\"{delta_mae:.6f}\",\n            f\"{rho_rmse:.6f}\",\n            f\"{rho_mae:.6f}\"\n        ]\n        all_results.append(f\"[{','.join(case_results)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A residual analysis can reveal not just general model failings but also the problematic nature of individual data points. An observation's impact on a regression model is a function of both its \"outlyingness\" in the response variable and its \"leverage\" in the predictor space. This advanced practice introduces a suite of powerful regression diagnostics designed to untangle these effects, including leverage values ($h_{ii}$), studentized residuals ($t_i$), and Cook's distance ($D_i$) . By implementing these calculations, you will learn a systematic method to identify which specific observations exert a disproportionate influence on your model's parameters, a critical step in building a robust and reliable environmental model.",
            "id": "3862109",
            "problem": "You are analyzing residuals from a linear Ordinary Least Squares (OLS) regression used in environmental and earth system modeling to estimate daily river nutrient load as a function of river discharge and water temperature. Consider the linear model with intercept, where the response is nutrient load in kilograms per day and the predictors are discharge in cubic meters per second and water temperature in degrees Celsius. Use the following model specification:\nGiven $n$ observations, define the design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $p = 3$ columns (an intercept, discharge, and temperature), the response vector $y \\in \\mathbb{R}^n$, and assume the linear model $y = X \\beta + \\varepsilon$, where $\\beta \\in \\mathbb{R}^p$ is unknown and $\\varepsilon$ are independent errors with zero mean and constant variance.\n\nStarting from the OLS estimator and residual definitions, compute for each observation $i \\in \\{1,\\dots,n\\}$:\n- The leverage $h_{ii}$ (the $i$-th diagonal element of the hat matrix).\n- The externally studentized residual $t_i$.\n- Cook’s distance $D_i$.\n\nThen, identify influential observations affecting slope estimates using the following decision rule:\nAn observation $i$ is flagged as influential if either\n1) $D_i > \\frac{4}{n}$, or\n2) $h_{ii} > \\frac{2p}{n}$ and $|t_i| > t_{0.975}(n - p - 1)$, where $t_{0.975}(\\cdot)$ denotes the upper $0.975$ quantile of the Student’s $t$ distribution with the specified degrees of freedom.\n\nAll quantities $h_{ii}$, $t_i$, and $D_i$ are dimensionless and must be reported as floating-point numbers rounded to six decimal places. Indices of influential observations must be reported using zero-based indexing.\n\nUse the following three test cases. For each case, construct $X$ with a column of ones (intercept), the discharge vector, and the temperature vector, and use the provided nutrient load vector $y$. Units are specified for scientific realism, but outputs are dimensionless.\n\nTest Case 1 (happy path, moderate variability, $n=8$):\n- Discharge vector (in $\\mathrm{m}^3/\\mathrm{s}$): $[80, 95, 110, 125, 140, 155, 170, 185]$.\n- Temperature vector (in $^{\\circ}\\mathrm{C}$): $[10, 12, 11, 13, 15, 14, 16, 12]$.\n- Nutrient load vector (in $\\mathrm{kg/day}$): $[255, 292, 327, 370, 418, 450, 491, 520]$.\n\nTest Case 2 (boundary condition with high leverage candidate, $n=9$):\n- Discharge vector (in $\\mathrm{m}^3/\\mathrm{s}$): $[80, 90, 100, 110, 120, 130, 140, 150, 350]$.\n- Temperature vector (in $^{\\circ}\\mathrm{C}$): $[10, 11, 12, 13, 14, 12, 11, 13, 10]$.\n- Nutrient load vector (in $\\mathrm{kg/day}$): $[218, 241, 266, 290, 315, 332, 351, 378, 812]$.\n\nTest Case 3 (edge case with an outlier in the response, $n=10$):\n- Discharge vector (in $\\mathrm{m}^3/\\mathrm{s}$): $[100, 110, 120, 130, 140, 150, 160, 170, 180, 140]$.\n- Temperature vector (in $^{\\circ}\\mathrm{C}$): $[12, 13, 14, 12, 13, 15, 14, 16, 15, 13]$.\n- Nutrient load vector (in $\\mathrm{kg/day}$): $[262, 284, 307, 322, 345, 370, 388, 415, 431, 420]$ (note the last value is an intentional outlier relative to typical patterns).\n\nAlgorithmic requirements:\n- Compute the OLS estimator $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$.\n- Compute residuals $r = y - X \\hat{\\beta}$.\n- Compute the hat matrix $H = X (X^\\top X)^{-1} X^\\top$ and its diagonal $h_{ii}$.\n- Compute the residual variance estimate $s^2 = \\frac{\\sum_{i=1}^{n} r_i^2}{n - p}$.\n- Compute the leave-one-out mean squared error $s_{(i)}^2$ via the identity $$s_{(i)}^2 = \\frac{(n - p) s^2 - \\frac{r_i^2}{1 - h_{ii}}}{n - p - 1},$$ and the externally studentized residual $$t_i = \\frac{r_i}{s_{(i)} \\sqrt{1 - h_{ii}}}.$$\n- Compute Cook’s distance $$D_i = \\frac{r_i^2}{p s^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}.$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a nested list containing four elements: the list of $h_{ii}$ values, the list of $t_i$ values, the list of $D_i$ values, and the list of influential indices determined by the decision rule. All floating-point numbers must be rounded to six decimal places. The overall output must be a single top-level list of the three per-case lists, for example: $[[h\\_1,t\\_1,D\\_1,I\\_1],[h\\_2,t\\_2,D\\_2,I\\_2],[h\\_3,t\\_3,D\\_3,I\\_3]]$ where each $h\\_k$, $t\\_k$, and $D\\_k$ is itself a list of floats and each $I\\_k$ is a list of integers.",
            "solution": "The problem requires the calculation of several key diagnostic statistics for Ordinary Least Squares (OLS) linear regression and the subsequent identification of influential observations based on established criteria. The analysis is to be performed for three distinct test cases within an environmental modeling context. The solution proceeds by first defining the theoretical framework and then applying it algorithmically to the provided data.\n\nThe general form of the multiple linear regression model is given by:\n$$ y = X \\beta + \\varepsilon $$\nwhere $y \\in \\mathbb{R}^n$ is the vector of observed responses (nutrient load), $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of unknown model coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the vector of random errors. The design matrix $X$ is constructed from the predictor variables (river discharge and water temperature) and a leading column of ones representing the model intercept. Thus, for this problem, the number of parameters $p$ is $3$.\n\nThe step-by-step procedure for computing the required diagnostics is as follows:\n\n1.  **OLS Coefficient Estimation**: The OLS estimator $\\hat{\\beta}$ minimizes the sum of squared residuals. It is computed using the normal equations:\n    $$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n    This requires that the matrix $X^\\top X$ be invertible, which is true if the columns of $X$ are linearly independent.\n\n2.  **Fitted Values and Residuals**: Once $\\hat{\\beta}$ is found, the vector of fitted (or predicted) values, $\\hat{y}$, is given by:\n    $$ \\hat{y} = X \\hat{\\beta} $$\n    The vector of residuals, $r$, which represents the difference between observed and fitted values, is then:\n    $$ r = y - \\hat{y} = y - X \\hat{\\beta} $$\n\n3.  **Hat Matrix and Leverage**: The hat matrix, $H$, maps the observed response vector $y$ to the fitted value vector $\\hat{y}$:\n    $$ \\hat{y} = X (X^\\top X)^{-1} X^\\top y = H y $$\n    The hat matrix is defined as $H = X (X^\\top X)^{-1} X^\\top$. It is a symmetric and idempotent ($H^2=H$) projection matrix. The diagonal elements of the hat matrix, $h_{ii}$, are the leverages of each observation $i$. A leverage $h_{ii}$ measures the influence of the response value $y_i$ on its own fitted value $\\hat{y}_i$, as $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_{ii}$. It also reflects the distance of the predictor vector for observation $i$ from the center of the predictor space. The values of $h_{ii}$ are bounded such that $1/n \\le h_{ii} \\le 1$.\n\n4.  **Residual Variance Estimation**: An unbiased estimator for the variance of the errors, $\\sigma^2$, is the Mean Squared Error (MSE), also denoted by $s^2$:\n    $$ s^2 = \\frac{r^\\top r}{n - p} = \\frac{\\sum_{i=1}^{n} r_i^2}{n - p} $$\n    where $n-p$ are the residual degrees of freedom.\n\n5.  **Externally Studentized Residuals**: The externally studentized residual, $t_i$, for observation $i$ is calculated by comparing the residual $r_i$ to its standard error, where the standard error is estimated from a model fitted to all data points *except* observation $i$. Let $s_{(i)}^2$ be the mean squared error calculated without observation $i$. The studentized residual is:\n    $$ t_i = \\frac{r_i}{s_{(i)} \\sqrt{1 - h_{ii}}} $$\n    A computationally efficient formula for $s_{(i)}^2$ avoids re-fitting the model $n$ times:\n    $$ s_{(i)}^2 = \\frac{(n - p) s^2 - \\frac{r_i^2}{1 - h_{ii}}}{n - p - 1} $$\n    These residuals follow a Student's $t$-distribution with $n - p - 1$ degrees of freedom under the standard OLS assumptions.\n\n6.  **Cook's Distance**: Cook's distance, $D_i$, measures the aggregate change in all estimated coefficients when observation $i$ is removed from the dataset. It is a measure of the overall influence of an observation on the model's fitted values. It is calculated as:\n    $$ D_i = \\frac{r_i^2}{p s^2} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2} $$\n    This formula combines the residual magnitude (through $r_i^2$) and the leverage ($h_{ii}$) to quantify influence.\n\n7.  **Identification of Influential Observations**: An observation is flagged as influential if it satisfies either of two common diagnostic criteria:\n    a.  Its Cook's distance is large. A common threshold is $D_i > \\frac{4}{n}$.\n    b.  It has both high leverage and a large residual. A common threshold for high leverage is $h_{ii} > \\frac{2p}{n}$. When this is the case, the magnitude of its externally studentized residual is also checked against a critical value from the Student's $t$-distribution. The specific rule is $|t_i| > t_{\\alpha/2}(n - p - 1)$, where $\\alpha=0.05$ for a $95\\%$ confidence level, so we use the $0.975$ quantile, $t_{0.975}(n - p - 1)$.\n\nThe algorithm proceeds by applying these steps to each of the three test cases, constructing the appropriate $X$ and $y$ from the provided data, computing the vectors of $h_{ii}$, $t_i$, and $D_i$, and then applying the decision rules to identify the indices of influential points. All floating-point results are rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"discharge\": [80, 95, 110, 125, 140, 155, 170, 185],\n            \"temperature\": [10, 12, 11, 13, 15, 14, 16, 12],\n            \"load\": [255, 292, 327, 370, 418, 450, 491, 520],\n        },\n        {\n            \"discharge\": [80, 90, 100, 110, 120, 130, 140, 150, 350],\n            \"temperature\": [10, 11, 12, 13, 14, 12, 11, 13, 10],\n            \"load\": [218, 241, 266, 290, 315, 332, 351, 378, 812],\n        },\n        {\n            \"discharge\": [100, 110, 120, 130, 140, 150, 160, 170, 180, 140],\n            \"temperature\": [12, 13, 14, 12, 13, 15, 14, 16, 15, 13],\n            \"load\": [262, 284, 307, 322, 345, 370, 388, 415, 431, 420],\n        }\n    ]\n\n    all_results = []\n    \n    for case_data in test_cases:\n        discharge_vec = np.array(case_data[\"discharge\"])\n        temp_vec = np.array(case_data[\"temperature\"])\n        y = np.array(case_data[\"load\"])\n        n = len(y)\n        p = 3  # Intercept, discharge, temperature\n\n        # Construct the design matrix X\n        X = np.ones((n, p))\n        X[:, 1] = discharge_vec\n        X[:, 2] = temp_vec\n\n        # Calculate diagnostics\n        h_values, t_values, D_values, influential_indices = _calculate_diagnostics(X, y, n, p)\n        \n        # Format results for output\n        case_result = [\n            list(np.round(h_values, 6)),\n            list(np.round(t_values, 6)),\n            list(np.round(D_values, 6)),\n            influential_indices\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # str() on a list creates a string like '[1, 2, 3]'.\n    # Joining these with commas and enclosing in brackets creates the desired format.\n    final_output_str = str(all_results).replace(\"'\", \"\")\n    print(final_output_str)\n\ndef _calculate_diagnostics(X, y, n, p):\n    \"\"\"\n    Calculate regression diagnostics for a given design matrix X and response y.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n        n (int): Number of observations.\n        p (int): Number of parameters.\n        \n    Returns:\n        tuple: A tuple containing:\n            - h_values (np.ndarray): Leverage values.\n            - t_values (np.ndarray): Externally studentized residuals.\n            - D_values (np.ndarray): Cook's distances.\n            - influential_indices (list): List of zero-based indices of influential points.\n    \"\"\"\n    # OLS estimator: beta_hat = (X.T @ X)^-1 @ X.T @ y\n    try:\n        XTX_inv = np.linalg.inv(X.T @ X)\n    except np.linalg.LinAlgError:\n        # This case should not happen with the provided test data\n        return [], [], [], []\n\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Residuals: r = y - X @ beta_hat\n    residuals = y - X @ beta_hat\n    \n    # Hat matrix diagonal (leverages): H = X @ (X.T @ X)^-1 @ X.T\n    # Efficient calculation of the diagonal h_ii\n    h_values = np.sum((X @ XTX_inv) * X, axis=1)\n\n    # Residual variance estimate: s^2\n    df_residual = n - p\n    ssr = np.sum(residuals**2)\n    s2 = ssr / df_residual if df_residual > 0 else 0\n    \n    # Externally studentized residuals (t_i) and Cook's distance (D_i)\n    t_values = np.zeros(n)\n    D_values = np.zeros(n)\n    \n    # Small value to prevent division by zero in 1 - h_ii\n    epsilon = 1e-12 \n\n    for i in range(n):\n        # Leave-one-out mean squared error: s_(i)^2\n        df_loocv_residual = df_residual - 1\n        if df_loocv_residual > 0:\n            s2_i = (df_residual * s2 - residuals[i]**2 / (1 - h_values[i] + epsilon)) / df_loocv_residual\n        else:\n            s2_i = 0\n        \n        # Avoid sqrt of negative number if s2_i is slightly negative due to precision\n        if s2_i  0: s2_i = 0\n        \n        # Externally studentized residual: t_i\n        denominator_t = np.sqrt(s2_i * (1 - h_values[i] + epsilon))\n        t_values[i] = residuals[i] / denominator_t if denominator_t > 0 else 0\n        \n        # Cook's distance: D_i\n        denominator_d = p * s2 * (1 - h_values[i] + epsilon)**2\n        D_values[i] = (residuals[i]**2 * h_values[i]) / denominator_d if denominator_d > 0 else 0\n\n    # Identify influential points\n    influential_indices = []\n    \n    # Rule 1 threshold\n    cook_threshold = 4 / n\n    \n    # Rule 2 thresholds\n    leverage_threshold = 2 * p / n\n    df_t = n - p - 1\n    if df_t > 0:\n        t_critical = t.ppf(0.975, df=df_t)\n    else: # Should not happen for valid n, p\n        t_critical = float('inf')\n\n    for i in range(n):\n        # Check rule 1\n        rule1_triggered = D_values[i] > cook_threshold\n        \n        # Check rule 2\n        rule2_triggered = (h_values[i] > leverage_threshold) and (np.abs(t_values[i]) > t_critical)\n        \n        if rule1_triggered or rule2_triggered:\n            influential_indices.append(i)\n            \n    return h_values, t_values, D_values, influential_indices\n\n# A direct call to `str(all_results).replace(\"'\", \"\")` in the final print statement\n# ensures the exact string format without extra spaces that `json.dumps` might add.\n# I'll modify the final print line to handle this better.\ndef solve_and_print():\n    # This is a wrapper to get the final list and format it correctly\n    # ... code from solve() up to calculating all_results ...\n    # The logic inside solve() is mostly correct, but the final print needs adjustment for exact formatting.\n    # The provided `solve()` function has a slight formatting issue at the end.\n    # I will correct it by building the string manually.\n    \n    # (Replicating the logic from the original `solve` function)\n    test_cases = [\n        {\"discharge\": [80, 95, 110, 125, 140, 155, 170, 185], \"temperature\": [10, 12, 11, 13, 15, 14, 16, 12], \"load\": [255, 292, 327, 370, 418, 450, 491, 520]},\n        {\"discharge\": [80, 90, 100, 110, 120, 130, 140, 150, 350], \"temperature\": [10, 11, 12, 13, 14, 12, 11, 13, 10], \"load\": [218, 241, 266, 290, 315, 332, 351, 378, 812]},\n        {\"discharge\": [100, 110, 120, 130, 140, 150, 160, 170, 180, 140], \"temperature\": [12, 13, 14, 12, 13, 15, 14, 16, 15, 13], \"load\": [262, 284, 307, 322, 345, 370, 388, 415, 431, 420]},\n    ]\n    all_results_str_list = []\n    for case_data in test_cases:\n        y = np.array(case_data[\"load\"])\n        n = len(y)\n        p = 3\n        X = np.column_stack([np.ones(n), case_data[\"discharge\"], case_data[\"temperature\"]])\n        h, t, D, I = _calculate_diagnostics(X, y, n, p)\n        \n        h_str = str(list(np.round(h, 6)))\n        t_str = str(list(np.round(t, 6)))\n        D_str = str(list(np.round(D, 6)))\n        I_str = str(I)\n        \n        # Remove spaces after commas that numpy/python's default str() adds to lists\n        h_str = h_str.replace(', ', ',')\n        t_str = t_str.replace(', ', ',')\n        D_str = D_str.replace(', ', ',')\n        I_str = I_str.replace(', ', ',')\n\n        all_results_str_list.append(f\"[{h_str},{t_str},{D_str},{I_str}]\")\n    \n    final_output = f\"[{','.join(all_results_str_list)}]\"\n    print(final_output)\n\n# Since I must provide a single runnable block, I'll modify the initial `solve` function.\n# I'll combine the logic.\n\ndef final_solve():\n    test_cases = [\n        {\"discharge\": [80, 95, 110, 125, 140, 155, 170, 185], \"temperature\": [10, 12, 11, 13, 15, 14, 16, 12], \"load\": [255, 292, 327, 370, 418, 450, 491, 520]},\n        {\"discharge\": [80, 90, 100, 110, 120, 130, 140, 150, 350], \"temperature\": [10, 11, 12, 13, 14, 12, 11, 13, 10], \"load\": [218, 241, 266, 290, 315, 332, 351, 378, 812]},\n        {\"discharge\": [100, 110, 120, 130, 140, 150, 160, 170, 180, 140], \"temperature\": [12, 13, 14, 12, 13, 15, 14, 16, 15, 13], \"load\": [262, 284, 307, 322, 345, 370, 388, 415, 431, 420]},\n    ]\n    all_results_str_list = []\n    for case_data in test_cases:\n        y = np.array(case_data[\"load\"])\n        n = len(y)\n        p = 3\n        X = np.column_stack([np.ones(n), case_data[\"discharge\"], case_data[\"temperature\"]])\n        h, t_vals, D, I = _calculate_diagnostics(X, y, n, p)\n        \n        # Build string representations without extra spaces\n        h_str = f\"[{','.join(f'{x:.6f}' for x in h)}]\"\n        t_str = f\"[{','.join(f'{x:.6f}' for x in t_vals)}]\"\n        D_str = f\"[{','.join(f'{x:.6f}' for x in D)}]\"\n        I_str = f\"[{','.join(map(str, I))}]\" if I else \"[]\"\n        \n        all_results_str_list.append(f\"[{h_str},{t_str},{D_str},{I_str}]\")\n    \n    final_output = f\"[{','.join(all_results_str_list)}]\"\n    print(final_output)\n\nfinal_solve()\n# The original code provided in the problem had formatting issues, this is a corrected version that will be placed in the answer.\n# The `_calculate_diagnostics` function remains the same. The wrapper `final_solve` produces the correct output format.\n# I will replace the provided code with this corrected implementation.\n```\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef _calculate_diagnostics(X, y, n, p):\n    \"\"\"\n    Calculate regression diagnostics for a given design matrix X and response y.\n    \"\"\"\n    # OLS estimator: beta_hat = (X.T @ X)^-1 @ X.T @ y\n    try:\n        XTX_inv = np.linalg.inv(X.T @ X)\n    except np.linalg.LinAlgError:\n        return np.array([]), np.array([]), np.array([]), []\n\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Residuals: r = y - X @ beta_hat\n    residuals = y - X @ beta_hat\n    \n    # Hat matrix diagonal (leverages): H = X @ (X.T @ X)^-1 @ X.T\n    h_values = np.sum((X @ XTX_inv) * X, axis=1)\n\n    # Residual variance estimate: s^2\n    df_residual = n - p\n    ssr = np.sum(residuals**2)\n    s2 = ssr / df_residual if df_residual > 0 else 0\n    \n    # Externally studentized residuals (t_i) and Cook's distance (D_i)\n    t_values = np.zeros(n)\n    D_values = np.zeros(n)\n    \n    epsilon = 1e-12 \n\n    for i in range(n):\n        df_loocv_residual = df_residual - 1\n        s2_i = 0\n        if df_loocv_residual > 0:\n            s2_i = (df_residual * s2 - residuals[i]**2 / (1 - h_values[i] + epsilon)) / df_loocv_residual\n        \n        if s2_i  0: s2_i = 0\n        \n        denominator_t = np.sqrt(s2_i * (1 - h_values[i] + epsilon))\n        t_values[i] = residuals[i] / denominator_t if denominator_t > 0 else 0\n        \n        denominator_d = p * s2 * (1 - h_values[i] + epsilon)**2 if p * s2 > 0 else 1\n        D_values[i] = (residuals[i]**2 * h_values[i]) / denominator_d if denominator_d > 0 else 0\n\n    # Identify influential points\n    influential_indices = []\n    \n    cook_threshold = 4 / n\n    leverage_threshold = 2 * p / n\n    df_t = n - p - 1\n    t_critical = float('inf')\n    if df_t > 0:\n        t_critical = t.ppf(0.975, df=df_t)\n\n    for i in range(n):\n        rule1_triggered = D_values[i] > cook_threshold\n        rule2_triggered = (h_values[i] > leverage_threshold) and (np.abs(t_values[i]) > t_critical)\n        \n        if rule1_triggered or rule2_triggered:\n            influential_indices.append(i)\n            \n    return h_values, t_values, D_values, influential_indices\n\ndef solve():\n    test_cases = [\n        {\"discharge\": [80, 95, 110, 125, 140, 155, 170, 185], \"temperature\": [10, 12, 11, 13, 15, 14, 16, 12], \"load\": [255, 292, 327, 370, 418, 450, 491, 520]},\n        {\"discharge\": [80, 90, 100, 110, 120, 130, 140, 150, 350], \"temperature\": [10, 11, 12, 13, 14, 12, 11, 13, 10], \"load\": [218, 241, 266, 290, 315, 332, 351, 378, 812]},\n        {\"discharge\": [100, 110, 120, 130, 140, 150, 160, 170, 180, 140], \"temperature\": [12, 13, 14, 12, 13, 15, 14, 16, 15, 13], \"load\": [262, 284, 307, 322, 345, 370, 388, 415, 431, 420]},\n    ]\n    all_results_str_list = []\n    for case_data in test_cases:\n        y = np.array(case_data[\"load\"])\n        n = len(y)\n        p = 3\n        X = np.column_stack([np.ones(n), case_data[\"discharge\"], case_data[\"temperature\"]])\n        h, t_vals, D, I = _calculate_diagnostics(X, y, n, p)\n        \n        h_str = f\"[{','.join([f'{x:.6f}' for x in h])}]\"\n        t_str = f\"[{','.join([f'{x:.6f}' for x in t_vals])}]\"\n        D_str = f\"[{','.join([f'{x:.6f}' for x in D])}]\"\n        I_str = f\"[{','.join(map(str, I))}]\" if I else \"[]\"\n        \n        all_results_str_list.append(f\"[{h_str},{t_str},{D_str},{I_str}]\")\n    \n    final_output = f\"[{','.join(all_results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}