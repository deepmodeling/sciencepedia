{
    "hands_on_practices": [
        {
            "introduction": "The choice of an error metric is a critical first step in any model calibration process. This exercise contrasts two of the most common metrics: the Root Mean Square Error (RMSE), which is based on an $L_2$ norm, and the Mean Absolute Error (MAE), based on an $L_1$ norm. By working through a simple calibration problem, you will see firsthand how the mathematical properties of each metric lead to different optimal parameters and how they uniquely respond to outlier data points .",
            "id": "3899972",
            "problem": "A catchment-scale hydrological model is being calibrated for daily precipitation using an additive bias parameter. Let the raw model precipitation for day $i$ be $P_{i}^{\\mathrm{raw}}$, the observed precipitation be $P_{i}^{\\mathrm{obs}}$, and the model with an additive bias parameter $b$ be $P_{i}^{\\mathrm{mod}}(b) = P_{i}^{\\mathrm{raw}} + b$. Define the residuals as $e_{i}(b) = P_{i}^{\\mathrm{mod}}(b) - P_{i}^{\\mathrm{obs}}$. For a $12$-day training period, the residuals at $b=0$ are given (in millimeters per day) by the set\n$$\n\\{ -6.2,\\ 3.9,\\ -1.1,\\ 0.0,\\ 5.3,\\ -14.8,\\ 7.5,\\ 1.6,\\ -2.9,\\ 2.1,\\ -0.7,\\ 4.0 \\}.\n$$\nStarting only from the definitions of Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), perform the following:\n\n1) Using the provided residuals at $b=0$, compute the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE). Express both in millimeters per day.\n\n2) Treating the additive bias $b$ as the sole calibration parameter, write down the objective functions $J_{\\mathrm{RMSE}}(b)$ and $J_{\\mathrm{MAE}}(b)$ that correspond to the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE), respectively, in terms of the residuals $e_{i}(b)$. From first principles, derive the values of $b$ that minimize each objective, and evaluate these minimizing values numerically for the provided data.\n\n3) Using your derivations, discuss qualitatively how each metric shapes the calibration landscape for $b$ (for example, smoothness, curvature, and sensitivity to outliers), and relate these properties to the minimizers you obtained.\n\nFor grading, report as a single scalar the ratio of RMSE to MAE computed in part $1$ at $b=0$. Round your reported ratio to four significant figures. Report the ratio as a pure number with no units. If intermediate quantities are stated, express them in millimeters per day.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It presents a standard calibration task from environmental modeling that is solvable with the provided information.\n\nThe total number of data points for the training period is $N=12$. The set of residuals at a bias of $b=0$ is given as $E_0 = \\{ -6.2,\\ 3.9,\\ -1.1,\\ 0.0,\\ 5.3,\\ -14.8,\\ 7.5,\\ 1.6,\\ -2.9,\\ 2.1,\\ -0.7,\\ 4.0 \\}$. The units are millimeters per day (mm/day).\n\nFirst, we compute the Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) for the case where the additive bias $b=0$.\nThe definition of RMSE for a set of $N$ residuals $e_i$ is:\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} e_{i}^{2}}\n$$\nFor the given residuals $e_i(0)$, we first calculate the sum of squares:\n$$\n\\sum_{i=1}^{12} (e_{i}(0))^{2} = (-6.2)^2 + (3.9)^2 + (-1.1)^2 + (0.0)^2 + (5.3)^2 + (-14.8)^2 + (7.5)^2 + (1.6)^2 + (-2.9)^2 + (2.1)^2 + (-0.7)^2 + (4.0)^2\n$$\n$$\n\\sum_{i=1}^{12} (e_{i}(0))^{2} = 38.44 + 15.21 + 1.21 + 0.0 + 28.09 + 219.04 + 56.25 + 2.56 + 8.41 + 4.41 + 0.49 + 16.0 = 390.11\n$$\nThe RMSE at $b=0$ is therefore:\n$$\n\\mathrm{RMSE}(0) = \\sqrt{\\frac{390.11}{12}} \\approx 5.7017 \\text{ mm/day}\n$$\nThe definition of MAE for a set of $N$ residuals $e_i$ is:\n$$\n\\mathrm{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |e_i|\n$$\nFor the given residuals, we calculate the sum of absolute values:\n$$\n\\sum_{i=1}^{12} |e_{i}(0)| = |-6.2| + |3.9| + |-1.1| + |0.0| + |5.3| + |-14.8| + |7.5| + |1.6| + |-2.9| + |2.1| + |-0.7| + |4.0|\n$$\n$$\n\\sum_{i=1}^{12} |e_{i}(0)| = 6.2 + 3.9 + 1.1 + 0.0 + 5.3 + 14.8 + 7.5 + 1.6 + 2.9 + 2.1 + 0.7 + 4.0 = 50.1\n$$\nThe MAE at $b=0$ is therefore:\n$$\n\\mathrm{MAE}(0) = \\frac{50.1}{12} = 4.175 \\text{ mm/day}\n$$\n\nSecond, we determine the values of the additive bias parameter $b$ that minimize the RMSE and MAE. The residuals as a function of $b$ are $e_i(b) = P_{i}^{\\mathrm{mod}}(b) - P_{i}^{\\mathrm{obs}} = (P_{i}^{\\mathrm{raw}} + b) - P_{i}^{\\mathrm{obs}} = (P_{i}^{\\mathrm{raw}} - P_{i}^{\\mathrm{obs}}) + b = e_{i}(0) + b$.\n\nTo minimize RMSE, we define the objective function $J_{\\mathrm{RMSE}}(b)$:\n$$\nJ_{\\mathrm{RMSE}}(b) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (e_{i}(b))^{2}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (e_{i}(0) + b)^{2}}\n$$\nMinimizing $J_{\\mathrm{RMSE}}(b)$ is equivalent to minimizing its square, the Mean Squared Error (MSE), which we denote $L(b) = (J_{\\mathrm{RMSE}}(b))^2$. To find the minimum, we compute the derivative of $L(b)$ with respect to $b$ and set it to $0$:\n$$\n\\frac{dL}{db} = \\frac{d}{db} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} (e_{i}(0) + b)^{2} \\right] = \\frac{1}{N} \\sum_{i=1}^{N} 2(e_{i}(0) + b) = \\frac{2}{N} \\left( \\sum_{i=1}^{N} e_{i}(0) + Nb \\right)\n$$\nSetting the derivative to zero gives:\n$$\n\\sum_{i=1}^{N} e_{i}(0) + Nb = 0 \\implies b = -\\frac{1}{N} \\sum_{i=1}^{N} e_{i}(0) = -\\overline{e(0)}\n$$\nThe optimal bias $b_{\\mathrm{RMSE}}$ is the negative of the mean of the initial residuals. The second derivative $\\frac{d^{2}L}{db^2} = 2 > 0$, confirming this is a minimum.\nWe calculate the sum of the initial residuals:\n$$\n\\sum_{i=1}^{12} e_{i}(0) = -6.2 + 3.9 - 1.1 + 0.0 + 5.3 - 14.8 + 7.5 + 1.6 - 2.9 + 2.1 - 0.7 + 4.0 = -1.3\n$$\nThe optimal bias for RMSE is:\n$$\nb_{\\mathrm{RMSE}} = -\\frac{-1.3}{12} = \\frac{1.3}{12} \\approx 0.1083 \\text{ mm/day}\n$$\n\nTo minimize MAE, we define the objective function $J_{\\mathrm{MAE}}(b)$:\n$$\nJ_{\\mathrm{MAE}}(b) = \\frac{1}{N} \\sum_{i=1}^{N} |e_{i}(b)| = \\frac{1}{N} \\sum_{i=1}^{N} |e_{i}(0) + b|\n$$\nMinimizing $J_{\\mathrm{MAE}}(b)$ is equivalent to minimizing the sum of absolute deviations $\\sum_{i=1}^{N} |e_{i}(0) - (-b)|$. From first principles of statistics, this sum is minimized when the point $-b$ is the median of the set of values $\\{e_{i}(0)\\}$. Therefore, the optimal bias is $b_{\\mathrm{MAE}} = -\\mathrm{median}\\{e_{i}(0)\\}$.\nTo find the median, we sort the initial residuals:\n$$\n\\{ -14.8, -6.2, -2.9, -1.1, -0.7, 0.0, 1.6, 2.1, 3.9, 4.0, 5.3, 7.5 \\}\n$$\nSince $N=12$ is an even number, the median is the average of the two central values, the $6$-th and $7$-th elements:\n$$\n\\mathrm{median}\\{e_{i}(0)\\} = \\frac{0.0 + 1.6}{2} = 0.8\n$$\nThe optimal bias for MAE is:\n$$\nb_{\\mathrm{MAE}} = -0.8 \\text{ mm/day}\n$$\n\nThird, we provide a qualitative discussion of the calibration landscapes.\nThe RMSE objective function, being based on squared errors, results in a calibration landscape for $b$ that is a smooth, convex parabola. Its objective function, $L(b) = (J_{\\mathrm{RMSE}}(b))^2$, is quadratic in $b$, guaranteeing a unique, well-defined minimum that can be found analytically. The key property of the squared term is its sensitivity to outliers. Large residuals are weighted much more heavily than small residuals. The value $b_{\\mathrm{RMSE}} \\approx 0.1083$ is a small positive bias. It is influenced significantly by the large negative outlier $e_6(0) = -14.8$. This single large error pulls the mean of the residuals down to $\\overline{e(0)} \\approx -0.1083$, so the calibration adjusts with a positive bias to counteract this.\n\nThe MAE objective function, based on absolute errors, results in a calibration landscape that is also convex but is not smooth. It is piecewise linear, with points of non-differentiability (\"kinks\") at each value of $b$ such that $b = -e_i(0)$. Its minimizer, the median, is robust to outliers. The magnitude of an extreme residual does not influence the median's position, only its rank. For our data, the median of the residuals is $0.8$. This indicates that half of the time the model over-predicts and half the time it under-predicts relative to this value. The large outlier $-14.8$ has no more influence on the median than the value $-6.2$ does. The resulting optimal bias, $b_{\\mathrm{MAE}} = -0.8$, suggests a correction based on the central tendency of the bulk of the data, effectively ignoring the extreme outlier and concluding that the model generally over-predicts precipitation. The substantial difference between $b_{\\mathrm{RMSE}} \\approx 0.1083$ and $b_{\\mathrm{MAE}} = -0.8$ is a direct consequence of how each metric handles the outlier.\n\nFinally, for the purpose of grading, we report the ratio of the RMSE to the MAE at $b=0$.\n$$\n\\text{Ratio} = \\frac{\\mathrm{RMSE}(0)}{\\mathrm{MAE}(0)} = \\frac{\\sqrt{390.11/12}}{4.175} \\approx \\frac{5.70168}{4.175} \\approx 1.36567\n$$\nRounding to four significant figures, the ratio is $1.366$.",
            "answer": "$$\n\\boxed{1.366}\n$$"
        },
        {
            "introduction": "While RMSE and MAE are foundational, their respective properties regarding outliers can be suboptimal for certain applications. Robust objective functions, such as the Huber loss, offer a sophisticated alternative by blending quadratic penalties for small errors with linear penalties for large ones. This practice guides you through the implementation of a Huber-type objective, demonstrating how to derive an optimal parameter and how tuning its threshold, $\\delta$, alters the balance between robustness and efficiency .",
            "id": "3899944",
            "problem": "A simplified calibration scenario in environmental and earth system modeling considers tuning a scalar additive bias parameter $b$ in a precipitation model to align modeled daily precipitation intensities with observed values over a short period. Let observed minus modeled daily precipitation residuals be given as $e_i = o_i - m_i$ for $i = 1, \\dots, 7$, with units $\\mathrm{mm\\,day^{-1}}$. Assume the residuals are\n$$\ne_1 = -4,\\quad e_2 = -1,\\quad e_3 = 0,\\quad e_4 = 1,\\quad e_5 = 2,\\quad e_6 = 3,\\quad e_7 = 8.\n$$\nThe calibration parameter $b$ adjusts the model additively, producing residuals $r_i(b) = e_i - b$. Robust calibration seeks to reduce the undue influence of outliers by penalizing small residuals quadratically and large residuals linearly, with a tunable threshold $\\delta$ that controls the transition between these regimes. The Huber-type objective $J_{\\delta}(b)$ is the sum over daily contributions $J_{\\delta}(b) = \\sum_{i=1}^{7} \\rho_{\\delta}\\big(r_i(b)\\big)$, where the penalty is quadratic for $|r_i(b)|$ below the threshold $\\delta$ and linear for $|r_i(b)|$ above the threshold.\n\nStarting from the principles of residual-based misfit minimization and robust penalty design that is quadratic in a core region and linear in the tails, derive the explicit mathematical form of the Huber penalty $\\rho_{\\delta}(r)$ and its first derivative. Then, using the first-order optimality condition for convex objectives, compute the bias $b^{\\star}$ that minimizes $J_{\\delta}(b)$ for $\\delta = 2\\,\\mathrm{mm\\,day^{-1}}$ and for $\\delta = 10\\,\\mathrm{mm\\,day^{-1}}$. For each $\\delta$, also compute the minimized objective $J_{\\delta}\\big(b^{\\star}\\big)$.\n\nExpress the final calibrated bias values $b^{\\star}$ in $\\mathrm{mm\\,day^{-1}}$ and the minimized objective values $J_{\\delta}\\big(b^{\\star}\\big)$ in $\\big(\\mathrm{mm\\,day^{-1}}\\big)^{2}$. Round all reported numerical quantities to four significant figures. Your final answer must be a single row matrix containing, in order, $b^{\\star}$ for $\\delta=2$, $J_{2}\\big(b^{\\star}\\big)$, $b^{\\star}$ for $\\delta=10$, and $J_{10}\\big(b^{\\star}\\big)$.",
            "solution": "### Step 1: Derivation of the Huber Penalty Function and its Derivative\n\nThe problem describes a robust penalty function $\\rho_{\\delta}(r)$ that is quadratic for small residuals ($|r| \\le \\delta$) and linear for large residuals ($|r| > \\delta$). To ensure the overall objective function is convex and differentiable, we must construct $\\rho_{\\delta}(r)$ to be continuously differentiable ($C^1$) everywhere.\n\nFor the quadratic region, we define the penalty as:\n$$\n\\rho_{\\delta}(r) = \\frac{1}{2} r^2 \\quad \\text{for } |r| \\le \\delta\n$$\nThe factor of $\\frac{1}{2}$ is a standard convention that simplifies the derivative. The derivative in this region is:\n$$\n\\rho'_{\\delta}(r) = r \\quad \\text{for } |r| < \\delta\n$$\nAt the transition points $r = \\pm\\delta$, the function value and its derivative are:\n$$\n\\rho_{\\delta}(\\pm\\delta) = \\frac{1}{2}\\delta^2\n$$\n$$\n\\rho'_{\\delta}(\\pm\\delta) = \\pm\\delta\n$$\nFor the linear region $|r| > \\delta$, the function must match these values and slopes. For $r > \\delta$, we need a line with slope $m = \\delta$ that passes through the point $(\\delta, \\frac{1}{2}\\delta^2)$. Using the point-slope form, $y - y_1 = m(x-x_1)$:\n$$\n\\rho_{\\delta}(r) - \\frac{1}{2}\\delta^2 = \\delta(r - \\delta)\n$$\n$$\n\\rho_{\\delta}(r) = \\delta r - \\delta^2 + \\frac{1}{2}\\delta^2 = \\delta r - \\frac{1}{2}\\delta^2 \\quad \\text{for } r > \\delta\n$$\nSince the penalty must be an even function of $r$ (i.e., $\\rho_{\\delta}(r) = \\rho_{\\delta}(-r)$), the expression for $r < -\\delta$ is found by replacing $r$ with $|r|$:\n$$\n\\rho_{\\delta}(r) = \\delta|r| - \\frac{1}{2}\\delta^2 \\quad \\text{for } |r| > \\delta\n$$\nCombining these, the Huber penalty function is:\n$$\n\\rho_{\\delta}(r) = \\begin{cases}\n\\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n\\delta |r| - \\frac{1}{2} \\delta^2 & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThe derivative, $\\rho'_{\\delta}(r)$, is found by differentiating each piece:\n$$\n\\rho'_{\\delta}(r) = \\begin{cases}\n-\\delta & \\text{if } r < -\\delta \\\\\nr & \\text{if } -\\delta \\le r \\le \\delta \\\\\n\\delta & \\text{if } r > \\delta\n\\end{cases}\n$$\nThis derivative is continuous and can be compactly written as $\\rho'_{\\delta}(r) = \\text{median}(-\\delta, r, \\delta)$.\n\n### Step 2: First-Order Optimality Condition\n\nThe objective function to minimize is $J_{\\delta}(b) = \\sum_{i=1}^{7} \\rho_{\\delta}(r_i(b))$, where $r_i(b) = e_i - b$. The Huber loss $\\rho_{\\delta}$ is a convex function. Since the sum of convex functions is convex, $J_{\\delta}(b)$ is also convex. The minimum $b^{\\star}$ of a convex, differentiable function is found by setting its first derivative to zero (the first-order optimality condition).\n$$\n\\frac{dJ_{\\delta}}{db}(b) = \\sum_{i=1}^{7} \\frac{d}{db} \\rho_{\\delta}(e_i - b) = 0\n$$\nUsing the chain rule, $\\frac{d}{db}\\rho_{\\delta}(e_i - b) = \\rho'_{\\delta}(e_i - b) \\cdot (-1)$. The condition becomes:\n$$\n\\sum_{i=1}^{7} \\left(-\\rho'_{\\delta}(e_i - b)\\right) = 0 \\implies \\sum_{i=1}^{7} \\rho'_{\\delta}(e_i - b) = 0\n$$\nWe need to solve this non-linear equation for $b^{\\star}$ for the two given values of $\\delta$. The data for the residuals are $e = \\{-4, -1, 0, 1, 2, 3, 8\\}$.\n\n### Step 3: Calculation for $\\delta = 2\\,\\mathrm{mm\\,day^{-1}}$\n\nFor $\\delta=2$, the optimality condition is:\n$$\n\\sum_{i=1}^{7} \\rho'_{2}(e_i - b) = \\sum_{i=1}^{7} \\text{median}(-2, e_i - b, 2) = 0\n$$\nWe seek a value of $b$ that satisfies this equation. Let's test integer values of $b$ near the mean of the central data points. Consider $b^{\\star}=1$. The adjusted residuals $r_i(1) = e_i - 1$ are:\n$$\n\\{-4-1, -1-1, 0-1, 1-1, 2-1, 3-1, 8-1\\} = \\{-5, -2, -1, 0, 1, 2, 7\\}\n$$\nNow we compute the terms $\\rho'_{2}(r_i(1))$:\n\\begin{itemize}\n    \\item $\\rho'_{2}(-5) = \\text{median}(-2, -5, 2) = -2$ (since $-5 < -2$)\n    \\item $\\rho'_{2}(-2) = \\text{median}(-2, -2, 2) = -2$ (since $-2 \\le -2 \\le 2$)\n    \\item $\\rho'_{2}(-1) = \\text{median}(-2, -1, 2) = -1$ (since $-2 \\le -1 \\le 2$)\n    \\item $\\rho'_{2}(0) = \\text{median}(-2, 0, 2) = 0$ (since $-2 \\le 0 \\le 2$)\n    \\item $\\rho'_{2}(1) = \\text{median}(-2, 1, 2) = 1$ (since $-2 \\le 1 \\le 2$)\n    \\item $\\rho'_{2}(2) = \\text{median}(-2, 2, 2) = 2$ (since $-2 \\le 2 \\le 2$)\n    \\item $\\rho'_{2}(7) = \\text{median}(-2, 7, 2) = 2$ (since $7 > 2$)\n\\end{itemize}\nThe sum is $\\sum_{i=1}^{7} \\rho'_{2}(e_i - 1) = -2 - 2 - 1 + 0 + 1 + 2 + 2 = 0$.\nThe condition is satisfied, so the optimal bias is $b^{\\star} = 1\\,\\mathrm{mm\\,day^{-1}}$.\n\nNext, we compute the minimized objective value $J_{2}(b^{\\star})$ using the adjusted residuals $r_i = \\{-5, -2, -1, 0, 1, 2, 7\\}$.\n$$\n\\rho_{2}(r) = \\begin{cases}\n\\frac{1}{2} r^2 & \\text{if } |r| \\le 2 \\\\\n2|r| - 2 & \\text{if } |r| > 2\n\\end{cases}\n$$\n\\begin{itemize}\n    \\item $\\rho_{2}(-5) = 2|-5| - 2 = 10 - 2 = 8$\n    \\item $\\rho_{2}(-2) = \\frac{1}{2}(-2)^2 = 2$\n    \\item $\\rho_{2}(-1) = \\frac{1}{2}(-1)^2 = 0.5$\n    \\item $\\rho_{2}(0) = \\frac{1}{2}(0)^2 = 0$\n    \\item $\\rho_{2}(1) = \\frac{1}{2}(1)^2 = 0.5$\n    \\item $\\rho_{2}(2) = \\frac{1}{2}(2)^2 = 2$\n    \\item $\\rho_{2}(7) = 2|7| - 2 = 14 - 2 = 12$\n\\end{itemize}\n$J_{2}(b^{\\star}=1) = 8 + 2 + 0.5 + 0 + 0.5 + 2 + 12 = 25$.\nSo, $J_{2}(b^{\\star}) = 25\\,\\big(\\mathrm{mm\\,day^{-1}}\\big)^{2}$.\n\n### Step 4: Calculation for $\\delta = 10\\,\\mathrm{mm\\,day^{-1}}$\n\nFor $\\delta=10$, the optimality condition is $\\sum_{i=1}^{7} \\rho'_{10}(e_i - b) = 0$.\nThe mean of the residuals is $\\bar{e} = \\frac{1}{7}\\sum e_i = \\frac{-4-1+0+1+2+3+8}{7} = \\frac{9}{7}$.\nLet's check if all residuals fall within the quadratic region ($|r_i| \\le \\delta$) if we use this mean as the bias. Let $b = 9/7$. The largest deviation from this bias will be for $e_{min}=-4$ or $e_{max}=8$.\n$e_{min} - b = -4 - \\frac{9}{7} = -\\frac{28}{7} - \\frac{9}{7} = -\\frac{37}{7} \\approx -5.29$.\n$e_{max} - b = 8 - \\frac{9}{7} = \\frac{56}{7} - \\frac{9}{7} = \\frac{47}{7} \\approx 6.71$.\nSince $|-5.29| < 10$ and $|6.71| < 10$, all adjusted residuals $|e_i - b|$ will be less than $\\delta=10$.\nTherefore, for all $i$, we are in the quadratic regime where $\\rho'_{10}(r) = r$. The optimality condition simplifies to:\n$$\n\\sum_{i=1}^{7} (e_i - b) = 0\n$$\nThis is the standard least squares problem, and its solution is the sample mean of the residuals:\n$$\nb^{\\star} = \\frac{1}{7} \\sum_{i=1}^{7} e_i = \\frac{9}{7}\\,\\mathrm{mm\\,day^{-1}}\n$$\nNext, we compute the minimized objective $J_{10}(b^{\\star})$. Since all $|r_i| \\le 10$, we use $\\rho_{10}(r) = \\frac{1}{2}r^2$.\n$$\nJ_{10}(b^{\\star}) = \\sum_{i=1}^{7} \\frac{1}{2} (e_i - b^{\\star})^2 = \\frac{1}{2}\\sum_{i=1}^{7} (e_i - \\bar{e})^2\n$$\nThe sum of squared residuals is related to the sample variance. We can compute it as $\\sum e_i^2 - n(\\bar{e})^2$:\n$$\n\\sum_{i=1}^{7} e_i^2 = (-4)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 3^2 + 8^2 = 16 + 1 + 0 + 1 + 4 + 9 + 64 = 95\n$$\n$$\nn(\\bar{e})^2 = 7 \\left(\\frac{9}{7}\\right)^2 = 7 \\frac{81}{49} = \\frac{81}{7}\n$$\n$$\n\\sum_{i=1}^{7} (e_i - \\bar{e})^2 = 95 - \\frac{81}{7} = \\frac{665 - 81}{7} = \\frac{584}{7}\n$$\nSo, the objective value is:\n$$\nJ_{10}(b^{\\star}) = \\frac{1}{2} \\left(\\frac{584}{7}\\right) = \\frac{292}{7}\\,\\big(\\mathrm{mm\\,day^{-1}}\\big)^{2}\n$$\n\n### Step 5: Final Numerical Values\n\nThe problem requires reporting numerical values to four significant figures.\nFor $\\delta=2$:\n$b^{\\star} = 1.000\\,\\mathrm{mm\\,day^{-1}}$\n$J_{2}(b^{\\star}) = 25.00\\,\\big(\\mathrm{mm\\,day^{-1}}\\big)^{2}$\n\nFor $\\delta=10$:\n$b^{\\star} = \\frac{9}{7} \\approx 1.285714...\\,\\mathrm{mm\\,day^{-1}}$, which is $1.286$ to four significant figures.\n$J_{10}(b^{\\star}) = \\frac{292}{7} \\approx 41.714285...\\,\\big(\\mathrm{mm\\,day^{-1}}\\big)^{2}$, which is $41.71$ to four significant figures.\n\nThe final answer is presented as a row matrix containing these four values in order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.000 & 25.00 & 1.286 & 41.71 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Objective functions find one of their most powerful applications in data assimilation, the science of optimally merging model forecasts with observations to produce the best possible estimate of a system's state. This exercise introduces the core of Three-Dimensional Variational (3D-Var) assimilation through its objective function, which is a form of weighted least squares. You will derive the optimal \"analysis\" state that balances the information from a model's background forecast and an observation, with the weights determined by their respective error variances, $B$ and $R$ .",
            "id": "3899992",
            "problem": "Consider a single-state, single-observation Three-Dimensional Variational (3D-Var) data assimilation for near-surface air temperature at a single grid point. Let the state be the scalar temperature $x$ expressed in $\\mathrm{K}$, with a prior (background) estimate $x_b$ and a single observation $y$. Assume that background and observation errors are unbiased, Gaussian, and uncorrelated, with background error variance $B$ and observation error variance $R$. The observation operator is linear and, for this scalar case, equal to the identity, so $H = 1$. The 3D-Var objective function is defined as\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)\\,B^{-1}\\,(x - x_b) + \\tfrac{1}{2}\\,(y - Hx)\\,R^{-1}\\,(y - Hx).\n$$\nStarting from this definition and first principles of least-squares optimality, derive the stationarity condition by taking the derivative of $J(x)$ with respect to $x$ and setting it to zero, then solve for the analysis $x_a$ and the analysis increment $\\delta x \\equiv x_a - x_b$ in closed form. Use your derivation to explicitly show how $B$ and $R$ determine the balance between the background and the observation in the scalar increment formula.\n\nFinally, evaluate the analysis increment for the following physically plausible case: $x_b = 290\\,\\mathrm{K}$, $y = 293.7\\,\\mathrm{K}$, $B = 5\\,\\mathrm{K}^2$, $R = 2\\,\\mathrm{K}^2$, and $H = 1$. Express the analysis increment in $\\mathrm{K}$ and round your answer to $4$ significant figures. The final reported quantity must be the single real value of $\\delta x$.",
            "solution": "### Derivation\nThe objective function to be minimized is given as:\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)\\,B^{-1}\\,(x - x_b) + \\tfrac{1}{2}\\,(y - Hx)\\,R^{-1}\\,(y - Hx)\n$$\nIn this scalar case, the variables $x$, $x_b$, and $y$ are scalars. The error variances $B$ and $R$ are also scalars, so their inverses are simply $B^{-1} = \\frac{1}{B}$ and $R^{-1} = \\frac{1}{R}$. The observation operator is the identity, $H = 1$. The objective function simplifies to:\n$$\nJ(x) = \\frac{1}{2B}(x - x_b)^2 + \\frac{1}{2R}(y - x)^2\n$$\nThe minimum of the objective function corresponds to the optimal state estimate, known as the analysis, $x_a$. This minimum is found at the stationary point where the first derivative of $J(x)$ with respect to $x$ is zero. We compute the derivative:\n$$\n\\frac{dJ}{dx} = \\frac{d}{dx} \\left( \\frac{1}{2B}(x - x_b)^2 \\right) + \\frac{d}{dx} \\left( \\frac{1}{2R}(y - x)^2 \\right)\n$$\nUsing the chain rule, we have:\n$$\n\\frac{dJ}{dx} = \\frac{1}{2B} \\cdot 2(x - x_b) \\cdot 1 + \\frac{1}{2R} \\cdot 2(y - x) \\cdot (-1)\n$$\n$$\n\\frac{dJ}{dx} = \\frac{1}{B}(x - x_b) - \\frac{1}{R}(y - x)\n$$\nSetting the derivative to zero to find the analysis $x_a$:\n$$\n\\frac{1}{B}(x_a - x_b) - \\frac{1}{R}(y - x_a) = 0\n$$\nNow, we solve for $x_a$:\n$$\n\\frac{x_a}{B} - \\frac{x_b}{B} = \\frac{y}{R} - \\frac{x_a}{R}\n$$\n$$\nx_a \\left( \\frac{1}{B} + \\frac{1}{R} \\right) = \\frac{x_b}{B} + \\frac{y}{R}\n$$\n$$\nx_a \\left( \\frac{R + B}{BR} \\right) = \\frac{x_b R + y B}{BR}\n$$\nMultiplying both sides by $BR$ yields the expression for the analysis $x_a$:\n$$\nx_a(R+B) = x_b R + y B\n$$\n$$\nx_a = \\frac{R}{B+R}x_b + \\frac{B}{B+R}y\n$$\nThis equation shows that the analysis $x_a$ is a weighted average of the background $x_b$ and the observation $y$. The weights are determined by the respective error variances.\n\nNext, we derive the analysis increment, $\\delta x$, defined as $\\delta x = x_a - x_b$:\n$$\n\\delta x = \\left( \\frac{R}{B+R}x_b + \\frac{B}{B+R}y \\right) - x_b\n$$\n$$\n\\delta x = \\left( \\frac{R}{B+R} - 1 \\right)x_b + \\frac{B}{B+R}y\n$$\n$$\n\\delta x = \\left( \\frac{R - (B+R)}{B+R} \\right)x_b + \\frac{B}{B+R}y\n$$\n$$\n\\delta x = \\left( \\frac{-B}{B+R} \\right)x_b + \\frac{B}{B+R}y\n$$\nFactoring out the common term $\\frac{B}{B+R}$:\n$$\n\\delta x = \\frac{B}{B+R} (y - x_b)\n$$\nThis is the closed-form expression for the analysis increment. The term $(y - x_b)$ is the innovation, and the coefficient $K = \\frac{B}{B+R}$ is the Kalman gain for this scalar case.\n\nThe balance between the background and the observation is explicitly determined by the ratio of their error variances, $B$ and $R$, as captured by the gain $K$.\n-   If the background is considered highly accurate (i.e., its error variance $B$ is small, $B \\to 0$), then the gain $K \\to 0$. The increment $\\delta x \\to 0$, and the analysis $x_a = x_b + \\delta x \\approx x_b$. The system trusts the background.\n-   If the observation is considered highly accurate (i.e., its error variance $R$ is small, $R \\to 0$), then the gain $K = \\frac{B}{B+R} \\to \\frac{B}{B} = 1$. The increment $\\delta x \\to y - x_b$, and the analysis $x_a = x_b + (y - x_b) = y$. The system trusts the observation.\nThus, the increment corrects the background by a fraction of the innovation, where that fraction is determined by the relative uncertainty of the background compared to the total uncertainty ($B+R$).\n\n### Numerical Evaluation\nWe are given the following values:\n-   $x_b = 290\\,\\mathrm{K}$\n-   $y = 293.7\\,\\mathrm{K}$\n-   $B = 5\\,\\mathrm{K}^2$\n-   $R = 2\\,\\mathrm{K}^2$\n\nFirst, calculate the innovation:\n$$\ny - x_b = 293.7 - 290 = 3.7\\,\\mathrm{K}\n$$\nNext, calculate the analysis increment $\\delta x$ using the derived formula:\n$$\n\\delta x = \\frac{B}{B+R} (y - x_b)\n$$\n$$\n\\delta x = \\frac{5}{5+2} (3.7) = \\frac{5}{7} (3.7)\n$$\n$$\n\\delta x = \\frac{18.5}{7} \\approx 2.642857... \\,\\mathrm{K}\n$$\nThe problem requires rounding the answer to $4$ significant figures.\n$$\n\\delta x \\approx 2.643\\,\\mathrm{K}\n$$\nThe final requested quantity is the single real value of $\\delta x$.",
            "answer": "$$\n\\boxed{2.643}\n$$"
        }
    ]
}