{
    "hands_on_practices": [
        {
            "introduction": "Understanding system stability begins with the fundamental question: what happens when a system at equilibrium is slightly perturbed? This exercise () walks you through the foundational technique of linear stability analysis. By applying a first-order Taylor expansion to the classic logistic growth model, you will learn how to approximate the behavior of a nonlinear system near its equilibrium, a core skill that forms the bedrock for analyzing the stability of more complex environmental models.",
            "id": "3919783",
            "problem": "Consider a well-mixed single-compartment environmental box model for biomass concentration $x(t)$ (for example, phytoplankton biomass in a homogeneous mixed layer), governed by the Ordinary Differential Equation (ODE) \n$$\n\\frac{dx}{dt} = a x - b x^{2},\n$$\nwhere $a>0$ is the intrinsic per-capita growth rate and $b>0$ is a crowding or self-limitation coefficient arising from resource constraints and self-shading. Assume that $a$ and $b$ are constant in time and that the compartment is spatially homogeneous. \n\nUsing only the fundamental definitions of equilibrium and first-order Taylor expansion, derive the linearized perturbation equation about the nontrivial equilibrium $x^{\\ast}$, and compute the single eigenvalue of this linearized system at $x^{\\ast}$ to assess local stability. \n\nYou must:\n- Identify the nontrivial equilibrium $x^{\\ast}$ by solving $f(x^{\\ast})=0$ for $f(x)=a x - b x^{2}$.\n- Introduce a perturbation $\\eta(t)$ defined by $x(t)=x^{\\ast}+\\eta(t)$ and derive the first-order linearized equation for $\\eta(t)$ from the Taylor expansion of $f(x)$ around $x^{\\ast}$, retaining only terms up to first order in $\\eta$.\n- Compute the eigenvalue of the resulting linearized system explicitly as a symbolic expression in terms of $a$ and $b$.\n\nExpress the eigenvalue in $\\mathrm{s}^{-1}$. The final answer must be a single closed-form symbolic expression. No numerical rounding is required.",
            "solution": "The problem provides a differential equation describing the evolution of biomass concentration $x(t)$, which is the logistic growth model:\n$$\n\\frac{dx}{dt} = f(x) = a x - b x^{2}\n$$\nwhere the parameters $a>0$ and $b>0$ represent the intrinsic growth rate and self-limitation coefficient, respectively. The task is to analyze the local stability of the nontrivial equilibrium point of this system.\n\nFirst, we identify the equilibrium points by setting the rate of change of the biomass concentration to zero. An equilibrium point, denoted as $x_{eq}$, is a value of $x$ for which the system does not change over time. Mathematically, this corresponds to solving $\\frac{dx}{dt} = 0$.\n$$\nf(x_{eq}) = a x_{eq} - b x_{eq}^{2} = 0\n$$\nFactoring the expression gives:\n$$\nx_{eq} (a - b x_{eq}) = 0\n$$\nThis equation yields two solutions for the equilibrium points:\n1. The trivial equilibrium: $x_{eq,1} = 0$. This corresponds to the extinction of the biomass.\n2. The nontrivial equilibrium: $a - b x_{eq,2} = 0$, which gives $x_{eq,2} = \\frac{a}{b}$.\n\nThe problem statement asks for the analysis of the nontrivial equilibrium, which we denote as $x^{\\ast}$.\n$$\nx^{\\ast} = \\frac{a}{b}\n$$\nSince $a>0$ and $b>0$, this equilibrium concentration $x^{\\ast}$ is positive, which is physically and biologically meaningful.\n\nNext, we investigate the local stability of this equilibrium point $x^{\\ast}$. This is done by analyzing the behavior of small perturbations around the equilibrium. Let the state of the system at time $t$ be represented as the sum of the equilibrium state and a small perturbation $\\eta(t)$:\n$$\nx(t) = x^{\\ast} + \\eta(t)\n$$\nWe assume that $|\\eta(t)|$ is small. We substitute this expression into the original differential equation:\n$$\n\\frac{d}{dt}(x^{\\ast} + \\eta(t)) = f(x^{\\ast} + \\eta(t))\n$$\nSince $x^{\\ast}$ is a constant, its time derivative is zero, so the left-hand side simplifies to:\n$$\n\\frac{d\\eta}{dt}\n$$\nFor the right-hand side, we perform a first-order Taylor series expansion of the function $f(x)$ around the point $x = x^{\\ast}$:\n$$\nf(x) \\approx f(x^{\\ast}) + f'(x^{\\ast})(x - x^{\\ast})\n$$\nSubstituting $x = x^{\\ast} + \\eta(t)$, we get:\n$$\nf(x^{\\ast} + \\eta(t)) \\approx f(x^{\\ast}) + f'(x^{\\ast})((x^{\\ast} + \\eta(t)) - x^{\\ast}) = f(x^{\\ast}) + f'(x^{\\ast})\\eta(t)\n$$\nBy definition of an equilibrium point, $f(x^{\\ast}) = 0$. Therefore, retaining only the first-order term in $\\eta(t)$, the right-hand side becomes:\n$$\nf(x^{\\ast} + \\eta(t)) \\approx f'(x^{\\ast})\\eta(t)\n$$\nEquating the left-hand and right-hand sides gives the linearized perturbation equation:\n$$\n\\frac{d\\eta}{dt} = f'(x^{\\ast})\\eta(t)\n$$\nTo find the explicit form of this equation, we must compute the derivative of $f(x)$ and evaluate it at $x^{\\ast}$. The function is $f(x) = ax - bx^2$. Its first derivative with respect to $x$ is:\n$$\nf'(x) = \\frac{df}{dx} = \\frac{d}{dx}(ax - bx^2) = a - 2bx\n$$\nNow, we evaluate this derivative at the nontrivial equilibrium $x^{\\ast} = \\frac{a}{b}$:\n$$\nf'(x^{\\ast}) = f'\\left(\\frac{a}{b}\\right) = a - 2b\\left(\\frac{a}{b}\\right) = a - 2a = -a\n$$\nThe coefficient $f'(x^{\\ast})$ is the single eigenvalue of this one-dimensional linearized system. The eigenvalue, denoted by $\\lambda$, determines the local stability of the equilibrium. For a one-dimensional system of the form $\\frac{d\\eta}{dt} = \\lambda \\eta$, the coefficient $\\lambda$ is the eigenvalue.\nTherefore, the linearized equation is:\n$$\n\\frac{d\\eta}{dt} = -a \\eta(t)\n$$\nThe eigenvalue of the linearized system at the equilibrium $x^{\\ast}$ is the coefficient of $\\eta(t)$:\n$$\n\\lambda = -a\n$$\nThe parameter $a$ has units of a rate, or time$^{-1}$. If time is in seconds, the units are $\\mathrm{s}^{-1}$. Since $a > 0$ is given, the eigenvalue $\\lambda = -a$ is strictly negative. A negative eigenvalue indicates that small perturbations $\\eta(t)$ will decay exponentially over time, meaning the system will return to the equilibrium state $x^{\\ast}$. Thus, the nontrivial equilibrium is locally asymptotically stable.",
            "answer": "$$\\boxed{-a}$$"
        },
        {
            "introduction": "Real-world environmental systems are rarely isolated; they are webs of interacting components. This practice () extends the concept of linearization to a two-dimensional system representing a canonical predator-prey interaction. You will construct the Jacobian matrix and analyze its eigenvalues to uncover how the coupling between populations can give rise to oscillatory dynamicsâ€”a rich behavior not possible in single-variable systems and a common feature in many ecological and climate cycles.",
            "id": "3919758",
            "problem": "Consider a canonical predator-prey interaction used in environmental and earth system modeling to represent, for example, phytoplankton and zooplankton dynamics. The model is an autonomous system of ordinary differential equations (ODEs):\n$$\n\\frac{dx}{dt} = x(\\alpha - \\beta y), \\qquad \\frac{dy}{dt} = y(\\delta x - \\gamma),\n$$\nwhere $x$ and $y$ are the prey and predator densities, respectively, and the parameters $\\alpha$, $\\beta$, $\\delta$, and $\\gamma$ are positive constants with ecological interpretations (prey intrinsic growth rate $\\alpha$, predation rate coefficient $\\beta$, conversion efficiency $\\delta$, and predator mortality rate $\\gamma$). \n\nStarting from the foundational principle of linearization in dynamical systems, which states that the local behavior near an equilibrium of a nonlinear autonomous system can be characterized by the eigenvalues of its Jacobian matrix evaluated at that equilibrium, perform the following:\n\n1. Determine the coexistence equilibrium $(x^{\\ast}, y^{\\ast})$ with strictly positive coordinates.\n2. Construct the Jacobian matrix $J(x,y)$ of the system and evaluate it at $(x^{\\ast}, y^{\\ast})$.\n3. Using the trace and determinant of the Jacobian at $(x^{\\ast}, y^{\\ast})$, derive the condition under which the linearized dynamics are oscillatory, by establishing when the eigenvalues are complex.\n4. Under that condition, identify the angular frequency $\\omega$ (in radians per unit time) of small-amplitude oscillations predicted by the linearization, expressed purely in terms of the parameters $\\alpha$, $\\beta$, $\\delta$, and $\\gamma$.\n\nProvide your final answer as a single closed-form analytic expression for $\\omega$. No numerical evaluation or rounding is required, and no units should be included inside your final boxed expression.",
            "solution": "The problem requires an analysis of the local stability of the coexistence equilibrium of a canonical predator-prey model. The analysis will proceed by linearizing the system of ordinary differential equations (ODEs) at this equilibrium point.\n\nThe model is given by the following system of autonomous ODEs:\n$$\n\\frac{dx}{dt} = x(\\alpha - \\beta y)\n$$\n$$\n\\frac{dy}{dt} = y(\\delta x - \\gamma)\n$$\nHere, $x(t)$ and $y(t)$ represent the prey and predator population densities, respectively, at time $t$. The parameters $\\alpha$, $\\beta$, $\\delta$, and $\\gamma$ are all positive real constants.\n\n**Step 1: Determine the Coexistence Equilibrium**\n\nAn equilibrium point $(x^{\\ast}, y^{\\ast})$ of the system is a state where the population densities do not change over time. Mathematically, this corresponds to the condition where the time derivatives are zero:\n$$\n\\frac{dx}{dt} = 0 \\quad \\text{and} \\quad \\frac{dy}{dt} = 0\n$$\nSubstituting the given equations, we obtain the following algebraic system:\n$$\nx^{\\ast}(\\alpha - \\beta y^{\\ast}) = 0\n$$\n$$\ny^{\\ast}(\\delta x^{\\ast} - \\gamma) = 0\n$$\nThe problem asks for the coexistence equilibrium, which is defined as the equilibrium point $(x^{\\ast}, y^{\\ast})$ where both populations are present, meaning $x^{\\ast} > 0$ and $y^{\\ast} > 0$. For the above equations to hold under these conditions, the terms in the parentheses must be equal to zero:\n$$\n\\alpha - \\beta y^{\\ast} = 0 \\implies y^{\\ast} = \\frac{\\alpha}{\\beta}\n$$\n$$\n\\delta x^{\\ast} - \\gamma = 0 \\implies x^{\\ast} = \\frac{\\gamma}{\\delta}\n$$\nSince all parameters $\\alpha, \\beta, \\delta, \\gamma$ are positive, both $x^{\\ast}$ and $y^{\\ast}$ are strictly positive. Thus, the coexistence equilibrium point is $(x^{\\ast}, y^{\\ast}) = (\\frac{\\gamma}{\\delta}, \\frac{\\alpha}{\\beta})$.\n\n**Step 2: Construct and Evaluate the Jacobian Matrix**\n\nTo analyze the behavior of the nonlinear system near the equilibrium point, we employ the principle of linearization. The dynamics of small perturbations from the equilibrium are governed by the Jacobian matrix of the system, evaluated at that equilibrium point. Let the right-hand sides of the ODEs be denoted by $f(x,y) = \\alpha x - \\beta xy$ and $g(x,y) = \\delta xy - \\gamma y$. The Jacobian matrix $J(x,y)$ is given by:\n$$\nJ(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\\\ \\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y} \\end{pmatrix}\n$$\nThe required partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x} = \\alpha - \\beta y\n$$\n$$\n\\frac{\\partial f}{\\partial y} = -\\beta x\n$$\n$$\n\\frac{\\partial g}{\\partial x} = \\delta y\n$$\n$$\n\\frac{\\partial g}{\\partial y} = \\delta x - \\gamma\n$$\nThis gives the Jacobian matrix as a function of $x$ and $y$:\n$$\nJ(x,y) = \\begin{pmatrix} \\alpha - \\beta y & -\\beta x \\\\ \\delta y & \\delta x - \\gamma \\end{pmatrix}\n$$\nNext, we evaluate this matrix at the coexistence equilibrium $(x^{\\ast}, y^{\\ast}) = (\\frac{\\gamma}{\\delta}, \\frac{\\alpha}{\\beta})$. Let this be $J^{\\ast}$:\n$$\nJ^{\\ast} = J\\left(\\frac{\\gamma}{\\delta}, \\frac{\\alpha}{\\beta}\\right) = \\begin{pmatrix} \\alpha - \\beta\\left(\\frac{\\alpha}{\\beta}\\right) & -\\beta\\left(\\frac{\\gamma}{\\delta}\\right) \\\\ \\delta\\left(\\frac{\\alpha}{\\beta}\\right) & \\delta\\left(\\frac{\\gamma}{\\delta}\\right) - \\gamma \\end{pmatrix}\n$$\nSimplifying each element of the matrix yields:\n$$\nJ^{\\ast} = \\begin{pmatrix} 0 & -\\frac{\\beta\\gamma}{\\delta} \\\\ \\frac{\\alpha\\delta}{\\beta} & 0 \\end{pmatrix}\n$$\n\n**Step 3 & 4: Determine Condition for Oscillations and Find Angular Frequency**\n\nThe local dynamics near the equilibrium are determined by the eigenvalues $\\lambda$ of the Jacobian matrix $J^{\\ast}$. The eigenvalues are the roots of the characteristic equation $\\det(J^{\\ast} - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix. For a general $2 \\times 2$ matrix, this equation can be written as:\n$$\n\\lambda^2 - \\text{Tr}(J^{\\ast})\\lambda + \\text{Det}(J^{\\ast}) = 0\n$$\nLet's compute the trace and determinant of $J^{\\ast}$:\nThe trace is the sum of the diagonal elements:\n$$\n\\text{Tr}(J^{\\ast}) = 0 + 0 = 0\n$$\nThe determinant is given by:\n$$\n\\text{Det}(J^{\\ast}) = (0)(0) - \\left(-\\frac{\\beta\\gamma}{\\delta}\\right)\\left(\\frac{\\alpha\\delta}{\\beta}\\right) = \\frac{\\alpha\\beta\\gamma\\delta}{\\beta\\delta} = \\alpha\\gamma\n$$\nSubstituting these values into the characteristic equation:\n$$\n\\lambda^2 - (0)\\lambda + \\alpha\\gamma = 0 \\implies \\lambda^2 + \\alpha\\gamma = 0\n$$\nThe eigenvalues are the solutions to this equation:\n$$\n\\lambda^2 = -\\alpha\\gamma \\implies \\lambda = \\pm\\sqrt{-\\alpha\\gamma}\n$$\nThe dynamics are oscillatory when the eigenvalues are complex. The eigenvalues are given by the quadratic formula:\n$$\n\\lambda = \\frac{\\text{Tr}(J^{\\ast}) \\pm \\sqrt{\\text{Tr}(J^{\\ast})^2 - 4\\text{Det}(J^{\\ast})}}{2}\n$$\nFor the eigenvalues to be complex, the discriminant must be negative:\n$$\n\\text{Tr}(J^{\\ast})^2 - 4\\text{Det}(J^{\\ast}) < 0\n$$\nSubstituting our results:\n$$\n(0)^2 - 4(\\alpha\\gamma) < 0 \\implies -4\\alpha\\gamma < 0\n$$\nSince the problem states that $\\alpha > 0$ and $\\gamma > 0$, their product $\\alpha\\gamma$ is strictly positive. Therefore, the condition $-4\\alpha\\gamma < 0$ is always satisfied. This means the linearized dynamics around the coexistence equilibrium are always oscillatory under the given model assumptions.\n\nAn eigenvalue pair of the form $\\lambda = \\mu \\pm i\\omega$ corresponds to dynamics with stability determined by the real part $\\mu$ and an angular frequency of oscillation given by the imaginary part $\\omega$.\nOur eigenvalues are:\n$$\n\\lambda = \\pm i\\sqrt{\\alpha\\gamma}\n$$\nComparing this to the general form $\\lambda = \\mu \\pm i\\omega$, we can identify $\\mu=0$ and $\\omega=\\sqrt{\\alpha\\gamma}$. The zero real part indicates that the equilibrium is a center in the linearized approximation, corresponding to neutrally stable oscillations. The angular frequency of these small-amplitude oscillations around the equilibrium point is:\n$$\n\\omega = \\sqrt{\\alpha\\gamma}\n$$\nThis expression depends only on the prey's intrinsic growth rate $\\alpha$ and the predator's mortality rate $\\gamma$.",
            "answer": "$$\n\\boxed{\\sqrt{\\alpha\\gamma}}\n$$"
        },
        {
            "introduction": "While eigenvalue analysis reveals a system's long-term fate, it doesn't tell the whole story. In many critical Earth systems, like atmospheric and oceanic circulation, perturbations can experience massive short-term growth even when the system is asymptotically stable. This advanced practice () introduces this concept of transient growth in non-normal systems, guiding you to use singular value decomposition (SVD) to identify the \"optimal\" initial perturbations that trigger maximum amplification, a crucial concept for understanding the limits of predictability in weather and climate.",
            "id": "3919718",
            "problem": "Consider a linear time-invariant system that results from linearizing an environmental or Earth system model around an equilibrium state. Let the state vector be $x(t) \\in \\mathbb{R}^n$ and the constant Jacobian matrix at the equilibrium be $J \\in \\mathbb{R}^{n \\times n}$. The linearized dynamics are given by the ordinary differential equation $ \\frac{dx}{dt} = J x $, whose solution is $ x(t) = e^{J t} x(0) $. The transient amplification of an initial perturbation $x(0)$ at a given time $t$ (in seconds) under the Euclidean norm is defined as the dimensionless quantity $ A(x(0), t) = \\frac{\\|x(t)\\|_2}{\\|x(0)\\|_2} = \\frac{\\|e^{J t} x(0)\\|_2}{\\|x(0)\\|_2} $. Starting only from the given linear dynamics $ \\frac{dx}{dt} = J x $, the definition of the matrix exponential $e^{J t}$, and core properties of the Euclidean norm, derive the characterization of the optimal initial perturbation direction $x^\\star(0)$ that maximizes $ A(x(0), t) $ over all nonzero $x(0)$, and the associated maximum amplification at time $t$. Translate your derivation into an algorithm that computes, for specified $J$ and $t$: (i) the maximum amplification factor and (ii) the angle, in radians, between the optimal initial perturbation direction and the dominant eigenvector of $J$ (the eigenvector corresponding to the eigenvalue with the largest real part), both expressed as real-valued floats.\n\nYour program must implement this algorithm to solve the following test suite, where each case specifies a $2 \\times 2$ real matrix $J$ and a time $t$ in seconds:\n\n- Test case $1$ (normal, stable): $ J = \\begin{bmatrix} -0.5 & 0 \\\\ 0 & -0.1 \\end{bmatrix} $, $ t = 10.0 $.\n- Test case $2$ (non-normal, stable with transient growth): $ J = \\begin{bmatrix} -1.0 & 10.0 \\\\ 0.0 & -1.0 \\end{bmatrix} $, $ t = 0.5 $.\n- Test case $3$ (boundary case): $ J = \\begin{bmatrix} -0.3 & 2.0 \\\\ 0.0 & -0.3 \\end{bmatrix} $, $ t = 0.0 $.\n- Test case $4$ (normal, unstable): $ J = \\begin{bmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix} $, $ t = 20.0 $.\n- Test case $5$ (non-normal, stable, long-time): $ J = \\begin{bmatrix} -0.01 & 0.5 \\\\ 0.0 & -0.01 \\end{bmatrix} $, $ t = 500.0 $.\n\nFor each test case, compute $e^{J t}$, identify the optimal initial perturbation direction via an appropriate singular value problem based on $e^{J t}$, and determine the angle between this direction and the dominant eigenvector of $J$. The angle must be expressed in radians. The amplification is dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[ \\text{amplification}, \\text{angle} ]$ corresponding to one test case (for example, $[[\\text{float},\\text{float}],[\\text{float},\\text{float}],\\dots]$).",
            "solution": "The problem requires the derivation and implementation of an algorithm to find the maximum transient amplification of a linear dynamical system and the orientation of the optimal initial perturbation relative to the system's dominant eigenvector.\n\nThe system dynamics are given by the linear ordinary differential equation:\n$$\n\\frac{d x}{dt} = J x\n$$\nwhere $x(t) \\in \\mathbb{R}^n$ is the state vector and $J \\in \\mathbb{R}^{n \\times n}$ is the constant Jacobian matrix. The solution to this equation is $x(t) = e^{J t} x(0)$, where $x(0)$ is the initial state at time $t=0$ and $e^{Jt}$ is the matrix exponential.\n\nThe transient amplification at a time $t$ for an initial perturbation $x(0)$ is defined as the ratio of the Euclidean norms:\n$$\nA(x(0), t) = \\frac{\\|x(t)\\|_2}{\\|x(0)\\|_2} = \\frac{\\|e^{J t} x(0)\\|_2}{\\|x(0)\\|_2}\n$$\nOur first objective is to find the maximum possible amplification at a fixed time $t$ by choosing the optimal initial perturbation direction. This corresponds to solving the following maximization problem for all non-zero initial conditions $x(0) \\in \\mathbb{R}^n$:\n$$\n\\max_{x(0) \\neq 0} A(x(0), t) = \\max_{x(0) \\neq 0} \\frac{\\|e^{J t} x(0)\\|_2}{\\|x(0)\\|_2}\n$$\nLet us define the matrix $M = e^{Jt}$. The expression to be maximized is the definition of the induced $2$-norm (or spectral norm) of the matrix $M$.\n$$\n\\|M\\|_2 = \\max_{x \\neq 0} \\frac{\\|M x\\|_2}{\\|x\\|_2}\n$$\nThe induced $2$-norm of any matrix is equal to its largest singular value. To demonstrate this, we utilize the Singular Value Decomposition (SVD) of $M$. Any real matrix $M \\in \\mathbb{R}^{n \\times n}$ can be decomposed as $M = U \\Sigma V^T$, where $U$ and $V$ are orthogonal matrices ($U^T U = I$, $V^T V = I$) and $\\Sigma$ is a diagonal matrix containing the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$.\n\nLet's analyze the square of the norm of $Mx$:\n$$\n\\|M x\\|_2^2 = (Mx)^T (Mx) = x^T M^T M x\n$$\nSubstituting the SVD of $M$:\n$$\nM^T M = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma V^T = V \\Sigma^2 V^T\n$$\nsince $\\Sigma$ is diagonal. So, we have:\n$$\n\\|M x\\|_2^2 = x^T V \\Sigma^2 V^T x\n$$\nThe maximization problem for the amplification squared is:\n$$\n\\max_{x \\neq 0} \\frac{x^T V \\Sigma^2 V^T x}{x^T x}\n$$\nLet's define a new vector $y = V^T x$. Since $V$ is an orthogonal matrix, it preserves the norm, i.e., $\\|y\\|_2 = \\|V^T x\\|_2 = \\|x\\|_2$. Therefore, $y^T y = x^T x$. Substituting $y$ into the expression gives:\n$$\n\\max_{y \\neq 0} \\frac{(Vy)^T V \\Sigma^2 y}{y^T y} = \\max_{y \\neq 0} \\frac{y^T V^T V \\Sigma^2 y}{y^T y} = \\max_{y \\neq 0} \\frac{y^T \\Sigma^2 y}{y^T y}\n$$\nWriting this in terms of the components of $y = [y_1, y_2, \\dots, y_n]^T$ and the singular values $\\sigma_i$:\n$$\n\\frac{y^T \\Sigma^2 y}{y^T y} = \\frac{\\sum_{i=1}^n \\sigma_i^2 y_i^2}{\\sum_{i=1}^n y_i^2}\n$$\nThis expression is a weighted average of the values $\\sigma_i^2$. It is maximized when all the \"weight\" is placed on the largest value, which is $\\sigma_1^2$ (since we ordered $\\sigma_1 \\ge \\sigma_2 \\ge \\dots$). This occurs when $y_1 \\neq 0$ and $y_2 = y_3 = \\dots = y_n = 0$. For instance, we can choose $y$ to be the first standard basis vector, $y = e_1 = [1, 0, \\dots, 0]^T$.\n\nWith this choice, the maximum value of the ratio is $\\sigma_1^2$. The maximum amplification $A_{max}$ is the square root of this value:\n$$\nA_{max}(t) = \\sqrt{\\sigma_1^2} = \\sigma_1 = \\sigma_{max}(e^{Jt})\n$$\nThe maximum amplification factor is the largest singular value of the matrix propagator $e^{Jt}$.\n\nThe optimal initial perturbation, $x^\\star(0)$, is the vector $x$ that corresponds to $y = e_1$. From the substitution $y = V^T x$, we can find $x$:\n$$\nx = V y\n$$\nFor the optimal case, $y = e_1$, so the optimal initial perturbation is:\n$$\nx^\\star(0) = V e_1\n$$\nThis vector, $V e_1$, is simply the first column of the matrix $V$. The columns of $V$ are the right singular vectors of $M = e^{Jt}$. Thus, the optimal initial perturbation $x^\\star(0)$ is the right singular vector of $e^{Jt}$ corresponding to the largest singular value $\\sigma_1$.\n\nThe second part of the problem is to compute the angle between this optimal perturbation direction $x^\\star(0)$ and the dominant eigenvector of the Jacobian matrix $J$. The dominant eigenvector of $J$ is defined as the eigenvector $v_{dom}$ corresponding to the eigenvalue $\\lambda_{dom}$ with the largest real part:\n$$\n\\text{Re}(\\lambda_{dom}) = \\max_{i} \\{\\text{Re}(\\lambda_i)\\}\n$$\nwhere $\\lambda_i$ are the eigenvalues of $J$.\n\nOnce we have the two vectors, $x^\\star(0)$ (the first right-singular vector of $e^{Jt}$) and $v_{dom}$ (the dominant eigenvector of $J$), the angle $\\theta$ in radians between them can be computed using the dot product formula:\n$$\n\\theta = \\arccos\\left( \\frac{|x^\\star(0) \\cdot v_{dom}|}{\\|x^\\star(0)\\|_2 \\|v_{dom}\\|_2} \\right)\n$$\nThe absolute value of the dot product is used because the direction of an eigenvector or singular vector is defined only up to a sign. We seek the smallest angle between the lines spanned by the two vectors, which lies in the interval $[0, \\pi/2]$.\n\nThe algorithm to be implemented is as follows:\nFor each test case with a given matrix $J$ and time $t$:\n1.  Compute the matrix $M = e^{Jt}$.\n2.  Perform the Singular Value Decomposition of $M$ to obtain its singular values and right singular vectors.\n3.  The maximum amplification is the largest singular value, $\\sigma_1$.\n4.  The optimal initial perturbation direction, $x^\\star(0)$, is the right singular vector corresponding to $\\sigma_1$.\n5.  Compute the eigenvalues and eigenvectors of the matrix $J$.\n6.  Identify the dominant eigenvector, $v_{dom}$, by finding the eigenvector corresponding to the eigenvalue with the largest real part. For degenerate cases (repeated eigenvalues with maximum real part), any vector in the corresponding eigenspace can be chosen; a standard numerical library will provide one such vector.\n7.  Calculate the angle in radians between $x^\\star(0)$ and $v_{dom}$ using the arccosine of their normalized absolute dot product.\n8.  Store the pair $[\\text{amplification}, \\text{angle}]$.\n\nThis procedure will be applied to each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the problem for a given set of test cases.\n    For each case (J, t), it calculates the maximum transient amplification\n    and the angle between the optimal initial perturbation and the dominant\n    eigenvector of J.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[-0.5, 0.0], [0.0, -0.1]]), 10.0),  # Case 1\n        (np.array([[-1.0, 10.0], [0.0, -1.0]]), 0.5), # Case 2\n        (np.array([[-0.3, 2.0], [0.0, -0.3]]), 0.0),  # Case 3\n        (np.array([[0.1, 0.0], [0.0, 0.1]]), 20.0),   # Case 4\n        (np.array([[-0.01, 0.5], [0.0, -0.01]]), 500.0), # Case 5\n    ]\n\n    results = []\n    for J, t in test_cases:\n        # Step 1: Compute the matrix exponential M = exp(J*t)\n        # The matrix Jt is computed first.\n        Jt = J * t\n        M = expm(Jt)\n\n        # Step 2: Compute the SVD of M = U * S * Vh\n        # U, S, Vh = np.linalg.svd(M)\n        # S contains the singular values in descending order.\n        # Vh is the conjugate transpose of the matrix of right singular vectors.\n        # The rows of Vh are the right singular vectors.\n        try:\n            _, s, vh = np.linalg.svd(M)\n        except np.linalg.LinAlgError:\n            # Handle cases where SVD might fail, though unlikely for these inputs.\n            results.append([float('nan'), float('nan')])\n            continue\n\n        # Step 3: The maximum amplification is the largest singular value.\n        max_amplification = s[0]\n\n        # Step 4: The optimal initial perturbation is the first right singular vector.\n        # This corresponds to the first row of Vh.\n        optimal_perturbation_vec = vh[0, :]\n\n        # Step 5: Compute eigenvalues and eigenvectors of J.\n        # w contains eigenvalues, v contains corresponding eigenvectors as columns.\n        try:\n            eigenvalues, eigenvectors = np.linalg.eig(J)\n        except np.linalg.LinAlgError:\n            results.append([max_amplification, float('nan')])\n            continue\n\n        # Step 6: Identify the dominant eigenvector.\n        # This is the eigenvector corresponding to the eigenvalue with the largest real part.\n        dominant_eigenvalue_idx = np.argmax(np.real(eigenvalues))\n        dominant_eigenvector = eigenvectors[:, dominant_eigenvalue_idx]\n\n        # Step 7: Calculate the angle between the two vectors.\n        # The vectors from np.linalg.svd and np.linalg.eig are normalized.\n        # We take the absolute value of the dot product because the sign of\n        # eigenvectors/singular vectors is arbitrary. This gives the acute angle.\n        dot_product = np.dot(optimal_perturbation_vec.conj(), dominant_eigenvector)\n        # Ensure the argument to arccos is within [-1, 1] to avoid numerical errors.\n        cosine_angle = np.clip(np.abs(dot_product), -1.0, 1.0)\n        angle_rad = np.arccos(cosine_angle)\n\n        results.append([max_amplification, angle_rad])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # e.g., [[float1, float2], [float3, float4]]\n    # Using map(str, results) and join correctly formats inner lists as strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}