{
    "hands_on_practices": [
        {
            "introduction": "Before we attempt to solve a system of equations numerically, we must ask a more fundamental question: does a unique solution even exist? This exercise explores the concept of well-posedness for a boundary value problem common in fields like groundwater hydrology. By investigating the Poisson equation with pure Neumann (flux) boundary conditions, you will discover why such problems can have infinitely many solutions and learn how to apply a physically meaningful constraint to restore uniqueness .",
            "id": "3920554",
            "problem": "Consider a two-dimensional, confined aquifer modeled over the nondimensional unit disk $\\Omega = \\{(x,y) \\in \\mathbb{R}^{2} : x^{2} + y^{2} \\leq 1\\}$. Let the hydraulic head $h(x,y)$ be nondimensionalized such that the saturated hydraulic conductivity is unity, and the recharge is encoded as a volumetric source term. The governing Partial Differential Equation (PDE) for steady flow is the Poisson equation, which in this nondimensional form reads $\\Delta h = f$ in $\\Omega$. On the boundary $\\partial \\Omega$, assume a pure Neumann boundary condition, representing prescribed normal flux, $\\frac{\\partial h}{\\partial n} = g$, where $\\mathbf{n}$ is the outward unit normal vector.\n\nUsing mass conservation for saturated groundwater flow as the fundamental base, where the Darcy flux $\\mathbf{q} = - \\nabla h$ and $\\nabla \\cdot \\mathbf{q} = f$, construct an explicit counterexample that demonstrates non-uniqueness of solutions under pure Neumann conditions. Specifically, consider the particular choice $f(x,y) \\equiv 2$ and define $g$ by $g(\\mathbf{s}) = \\nabla u_{0}(\\mathbf{s}) \\cdot \\mathbf{n}(\\mathbf{s})$ for $\\mathbf{s} \\in \\partial \\Omega$, where $u_{0}(x,y) = x^{2}$. \n\nTasks:\n- Show that $u_{0}$ solves the given PDE and boundary condition and that for any constant $C \\in \\mathbb{R}$, $u_{0} + C$ also solves the same PDE and boundary condition on $\\Omega$ with the given $f$ and $g$.\n- Identify the nullspace of the homogeneous operator associated with the pure Neumann problem and explain why it leads to non-uniqueness.\n- Impose the physically meaningful integral constraint that restores uniqueness by requiring the solution to have zero spatial mean, $\\int_{\\Omega} h \\,\\mathrm{d}\\Omega = 0$, and compute the unique constant $C$ that enforces this constraint for the family $h(x,y) = u_{0}(x,y) + C$.\n\nYour final answer must be the value of the constant $C$ as a single real number, expressed without units. If any approximation is required, round to four significant figures; however, in this problem, provide the exact value.",
            "solution": "The problem is validated as scientifically grounded, well-posed (within the context of demonstrating and then resolving non-uniqueness), and free of any contradictions or ambiguities. The premises are consistent with the fundamental principles of potential theory and its application to groundwater flow. The required compatibility condition for the existence of a solution to a pure Neumann problem is satisfied. We may proceed with the solution.\n\nThe governing system is a Poisson problem on the unit disk $\\Omega$ with pure Neumann boundary conditions:\n$$\n\\begin{cases}\n\\Delta h = f  \\text{in } \\Omega = \\{(x,y) \\in \\mathbb{R}^{2} : x^{2} + y^{2} \\leq 1\\} \\\\\n\\frac{\\partial h}{\\partial n} = g  \\text{on } \\partial\\Omega\n\\end{cases}\n$$\nwhere $f(x,y) = 2$ and $g$ is defined via the function $u_{0}(x,y) = x^{2}$.\n\nFirst, we address the task of demonstrating non-uniqueness. Let us consider the function $u_{0}(x,y) = x^{2}$ and verify that it is a solution.\nThe Laplacian of $u_{0}$ is:\n$$\n\\Delta u_{0} = \\frac{\\partial^{2}}{\\partial x^{2}}(x^{2}) + \\frac{\\partial^{2}}{\\partial y^{2}}(x^{2}) = 2 + 0 = 2\n$$\nThis matches the given source term $f(x,y) = 2$. Thus, $u_{0}$ satisfies the PDE $\\Delta h = f$ in $\\Omega$.\n\nNext, we examine the boundary condition. The outward unit normal vector on the boundary $\\partial\\Omega$ (the unit circle $x^2+y^2=1$) is $\\mathbf{n} = (x, y)$. The normal derivative is given by the dot product of the gradient with the normal vector:\n$$\n\\frac{\\partial u_{0}}{\\partial n} = \\nabla u_{0} \\cdot \\mathbf{n}\n$$\nThe gradient of $u_{0}(x,y) = x^{2}$ is $\\nabla u_{0} = (2x, 0)$. The boundary condition $g$ is defined as $g(\\mathbf{s}) = \\nabla u_{0}(\\mathbf{s}) \\cdot \\mathbf{n}(\\mathbf{s})$ for any point $\\mathbf{s} \\in \\partial\\Omega$. Therefore, by its very definition, $u_{0}$ satisfies the boundary condition $\\frac{\\partial u_{0}}{\\partial n} = g$. Consequently, $u_{0}(x,y) = x^{2}$ is a solution to the given boundary value problem.\n\nNow, consider the family of functions $h(x,y) = u_{0}(x,y) + C = x^{2} + C$ for any constant $C \\in \\mathbb{R}$. We check if this general form also constitutes a solution.\nFor the PDE, we use the linearity of the Laplacian operator:\n$$\n\\Delta h = \\Delta(u_{0} + C) = \\Delta u_{0} + \\Delta C = 2 + 0 = 2\n$$\nThe PDE is satisfied for any constant $C$.\nFor the boundary condition, we use the linearity of the gradient operator:\n$$\n\\frac{\\partial h}{\\partial n} = \\nabla h \\cdot \\mathbf{n} = \\nabla(u_{0} + C) \\cdot \\mathbf{n} = (\\nabla u_{0} + \\nabla C) \\cdot \\mathbf{n}\n$$\nSince $C$ is a constant, its gradient is the zero vector, $\\nabla C = (0,0)$. Thus,\n$$\n\\frac{\\partial h}{\\partial n} = (\\nabla u_{0} + \\mathbf{0}) \\cdot \\mathbf{n} = \\nabla u_{0} \\cdot \\mathbf{n} = g\n$$\nThe boundary condition is also satisfied for any constant $C$. This demonstrates that if a solution exists, there is an infinite family of solutions of the form $u_{0}(x,y) + C$, which confirms the non-uniqueness of the solution to the pure Neumann problem.\n\nThe non-uniqueness arises from the nullspace of the differential operator under the given boundary conditions. Consider the associated homogeneous problem:\n$$\n\\begin{cases}\n\\Delta u = 0  \\text{in } \\Omega \\\\\n\\frac{\\partial u}{\\partial n} = 0  \\text{on } \\partial\\Omega\n\\end{cases}\n$$\nThe set of solutions to this problem forms the nullspace. If $u_{p}$ is a particular solution to the inhomogeneous problem, and $u_{n}$ is any function in the nullspace, then $u_{p} + u_{n}$ is also a solution to the inhomogeneous problem.\nTo find the nullspace, we use Green's first identity: $\\int_{\\Omega} (v \\Delta u + \\nabla v \\cdot \\nabla u) \\,d\\Omega = \\int_{\\partial\\Omega} v \\frac{\\partial u}{\\partial n} \\,dS$. Let $v=u$.\n$$\n\\int_{\\Omega} (u \\Delta u + |\\nabla u|^{2}) \\,d\\Omega = \\int_{\\partial\\Omega} u \\frac{\\partial u}{\\partial n} \\,dS\n$$\nFor a function $u$ in the nullspace, $\\Delta u = 0$ and $\\frac{\\partial u}{\\partial n} = 0$. The identity simplifies to:\n$$\n\\int_{\\Omega} |\\nabla u|^{2} \\,d\\Omega = 0\n$$\nSince the integrand $|\\nabla u|^{2}$ is non-negative, the integral can only be zero if the integrand is identically zero throughout the domain $\\Omega$. This means $|\\nabla u|^{2} = 0$, which implies $\\nabla u = \\mathbf{0}$. A function whose gradient is zero everywhere in a connected domain must be a constant. Therefore, the nullspace consists of all constant functions, $u(x,y) = C$. Since the nullspace is non-trivial (it is the one-dimensional space of constants), the solution to the original problem is unique only up to an additive constant.\n\nFinally, we impose the physically meaningful integral constraint $\\int_{\\Omega} h \\,d\\Omega = 0$ to restore uniqueness. We apply this constraint to our family of solutions $h(x,y) = x^{2} + C$ to find the specific value of $C$.\n$$\n\\int_{\\Omega} (x^{2} + C) \\,d\\Omega = 0\n$$\nBy linearity of the integral:\n$$\n\\int_{\\Omega} x^{2} \\,d\\Omega + \\int_{\\Omega} C \\,d\\Omega = 0\n$$\nThe second integral is $C$ times the area of the unit disk: $\\int_{\\Omega} C \\,d\\Omega = C \\cdot (\\pi \\cdot 1^{2}) = \\pi C$.\nTo evaluate the first integral, we switch to polar coordinates, where $x = r\\cos\\theta$ and the area element is $d\\Omega = r dr d\\theta$. The domain $\\Omega$ is described by $0 \\leq r \\leq 1$ and $0 \\leq \\theta \\leq 2\\pi$.\n$$\n\\int_{\\Omega} x^{2} \\,d\\Omega = \\int_{0}^{2\\pi} \\int_{0}^{1} (r\\cos\\theta)^{2} \\,r dr d\\theta = \\int_{0}^{2\\pi} \\int_{0}^{1} r^{3} \\cos^{2}\\theta \\,dr d\\theta\n$$\nThe integral is separable:\n$$\n\\left( \\int_{0}^{1} r^{3} \\,dr \\right) \\left( \\int_{0}^{2\\pi} \\cos^{2}\\theta \\,d\\theta \\right)\n$$\nThe radial integral is $\\int_{0}^{1} r^{3} \\,dr = \\left[\\frac{r^{4}}{4}\\right]_{0}^{1} = \\frac{1}{4}$.\nThe angular integral is evaluated using the identity $\\cos^{2}\\theta = \\frac{1 + \\cos(2\\theta)}{2}$:\n$$\n\\int_{0}^{2\\pi} \\frac{1 + \\cos(2\\theta)}{2} \\,d\\theta = \\frac{1}{2} \\left[ \\theta + \\frac{\\sin(2\\theta)}{2} \\right]_{0}^{2\\pi} = \\frac{1}{2} \\left( (2\\pi + 0) - (0+0) \\right) = \\pi\n$$\nThus, $\\int_{\\Omega} x^{2} \\,d\\Omega = \\frac{1}{4} \\cdot \\pi = \\frac{\\pi}{4}$.\nSubstituting these results back into the constraint equation:\n$$\n\\frac{\\pi}{4} + \\pi C = 0\n$$\nSolving for $C$:\n$$\n\\pi C = -\\frac{\\pi}{4} \\implies C = -\\frac{1}{4}\n$$\nThis is the unique constant that makes the solution have a zero spatial mean.",
            "answer": "$$\n\\boxed{-\\frac{1}{4}}\n$$"
        },
        {
            "introduction": "Once we have a well-posed linear system, the next challenge is to solve it efficiently, especially on fine computational grids. This practice delves into the convergence properties of the Gauss-Seidel method, a classical iterative solver, when applied to the 1D Poisson equation. You will analytically derive the spectral radius $\\rho$ of the iteration matrix and see how it quantitatively predicts the method's slow convergence as the grid resolution increases—a critical insight for modelers choosing numerical schemes .",
            "id": "3920583",
            "problem": "Consider a steady one-dimensional (1D) diffusive balance for a scalar field $\\phi(x)$ along a transect of length $L$, representative of a homogeneous groundwater head profile under steady sources, where conservation of mass and Fickian diffusion imply the second-order ordinary differential equation $-\\frac{d^{2}\\phi}{dx^{2}} = s(x)$ with Dirichlet boundary conditions $\\phi(0)=\\phi(L)=0$. A standard centered finite-difference approximation on a uniform grid with $n$ interior nodes and spacing $h = \\frac{L}{n+1}$ yields a linear system $A \\phi = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix\n$$\nA = \\frac{1}{h^{2}}\\begin{pmatrix}\n2  -1  0  \\cdots  0 \\\\\n-1  2  -1  \\ddots  \\vdots \\\\\n0  -1  2  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1 \\\\\n0  \\cdots  0  -1  2\n\\end{pmatrix}.\n$$\nTo solve $A \\phi = b$, consider the classical Gauss–Seidel (GS) iteration, defined by the matrix splitting $A = D - L - U$ with $D$ the diagonal of $A$, $L$ its strictly lower-triangular part, and $U$ its strictly upper-triangular part, and the GS error-propagation operator\n$$\nM_{\\mathrm{GS}} = (D - L)^{-1} U.\n$$\nUsing only the foundational properties of the discrete 1D Poisson operator and the GS splitting, derive the eigenvalues of $M_{\\mathrm{GS}}$ by analyzing its action on the discrete sine modes associated with the grid, identify the spectral radius $\\rho(M_{\\mathrm{GS}})$ in terms of $n$, and compute its numerical value for $n = 127$. Assume lexicographic ordering of the unknowns and that all angles introduced in your derivation are in radians. Finally, explain how this spectral radius quantitatively relates to observed asymptotic convergence rates of the GS method on this problem class.\n\nReport the spectral radius $\\rho(M_{\\mathrm{GS}})$ for $n=127$ as a single real number, rounded to six significant figures. No units are required for the spectral radius; express angles in radians wherever applicable.",
            "solution": "The problem requires the derivation of the eigenvalues and spectral radius of the Gauss-Seidel (GS) iteration matrix for the finite-difference discretization of the 1D Poisson equation, followed by a numerical calculation and an interpretation of the result in the context of convergence analysis.\n\nThe system of linear equations is $A \\phi = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by\n$$\nA = \\frac{1}{h^{2}}\\begin{pmatrix}\n2  -1  0  \\cdots  0 \\\\\n-1  2  -1  \\ddots  \\vdots \\\\\n0  -1  2  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1 \\\\\n0  \\cdots  0  -1  2\n\\end{pmatrix}\n$$\nwith $h = \\frac{L}{n+1}$. The Gauss-Seidel iteration is based on the splitting $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strictly lower-triangular part, and $-U$ is the strictly upper-triangular part. The iteration matrix is $M_{\\mathrm{GS}} = (D - L)^{-1} U$.\n\nTo find the eigenvalues of $M_{\\mathrm{GS}}$, we first analyze the related Jacobi iteration, whose matrix is $M_{\\mathrm{J}} = D^{-1}(L+U)$. From the splitting $A = D - L - U$, we can write $L+U = D-A$. Thus, $M_{\\mathrm{J}} = D^{-1}(D-A) = I - D^{-1}A$.\n\nThe eigenvalues of $M_{\\mathrm{J}}$ can be found from the eigenvalues of $A$. The matrix $A$ is a scaled version of the standard discrete 1D Laplacian. Its eigenvectors, the discrete sine modes, are given by the vectors $v^{(k)}$ with components\n$$\n(v^{(k)})_{j} = \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad \\text{for } j, k \\in \\{1, 2, \\dots, n\\}.\n$$\nThe corresponding eigenvalues $\\lambda_k(A)$ of $A$ are\n$$\n\\lambda_k(A) = \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) = \\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right).\n$$\nThe matrix $D$ is the diagonal part of $A$, so $D = \\frac{2}{h^2}I$, where $I$ is the identity matrix. The eigenvalues of $D$ are all $\\frac{2}{h^2}$.\nSince $M_{\\mathrm{J}} = I - D^{-1}A$, and $A$ and $D^{-1}$ commute (as $D$ is a multiple of $I$), the eigenvalues of $M_{\\mathrm{J}}$, denoted $\\mu_{\\mathrm{J},k}$, are related to the eigenvalues of $A$ by\n$$\n\\mu_{\\mathrm{J},k} = 1 - \\frac{\\lambda_k(A)}{\\lambda(D)} = 1 - \\frac{\\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right)}{\\frac{2}{h^2}} = 1 - 2\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right).\n$$\nUsing the trigonometric identity $\\cos(2\\theta) = 1 - 2\\sin^2(\\theta)$, we find the eigenvalues of the Jacobi matrix:\n$$\n\\mu_{\\mathrm{J},k} = \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad \\text{for } k \\in \\{1, 2, \\dots, n\\}.\n$$\nThe matrix $A$ is a tridiagonal matrix with non-zero diagonal entries. Such matrices belong to the class of \"consistently ordered\" matrices. For any consistently ordered matrix, there is a direct relationship between the eigenvalues of the Jacobi and Gauss-Seidel iterations. If $\\mu_{\\mathrm{J}}$ is an eigenvalue of $M_{\\mathrm{J}}$, then $\\mu_{\\mathrm{GS}} = \\mu_{\\mathrm{J}}^2$ is an eigenvalue of $M_{\\mathrm{GS}}$. This is a foundational result from the theory of iterative methods.\n\nUsing this property, we can derive the eigenvalues of the Gauss-Seidel matrix $M_{\\mathrm{GS}}$, denoted $\\mu_{\\mathrm{GS},k}$, from the eigenvalues of $M_{\\mathrm{J}}$:\n$$\n\\mu_{\\mathrm{GS},k} = (\\mu_{\\mathrm{J},k})^2 = \\left(\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right)^2 = \\cos^2\\left(\\frac{k\\pi}{n+1}\\right), \\quad \\text{for } k \\in \\{1, 2, \\dots, n\\}.\n$$\nThe spectral radius of a matrix is the maximum of the absolute values of its eigenvalues. For $M_{\\mathrm{GS}}$, the spectral radius $\\rho(M_{\\mathrm{GS}})$ is\n$$\n\\rho(M_{\\mathrm{GS}}) = \\max_{k \\in \\{1, \\dots, n\\}} |\\mu_{\\mathrm{GS},k}| = \\max_{k \\in \\{1, \\dots, n\\}} \\left| \\cos^2\\left(\\frac{k\\pi}{n+1}\\right) \\right|.\n$$\nSince $\\cos^2(x) \\ge 0$, the absolute value is redundant. The function $\\cos^2(x)$ is maximized when its argument is closest to an integer multiple of $\\pi$. For $k \\in \\{1, \\dots, n\\}$, the argument $\\frac{k\\pi}{n+1}$ lies in the interval $(0, \\pi)$. The values closest to $0$ and $\\pi$ occur at $k=1$ and $k=n$, respectively.\nFor $k=1$, the eigenvalue is $\\cos^2\\left(\\frac{\\pi}{n+1}\\right)$.\nFor $k=n$, the eigenvalue is $\\cos^2\\left(\\frac{n\\pi}{n+1}\\right) = \\cos^2\\left(\\pi - \\frac{\\pi}{n+1}\\right) = \\left(-\\cos\\left(\\frac{\\pi}{n+1}\\right)\\right)^2 = \\cos^2\\left(\\frac{\\pi}{n+1}\\right)$.\nThe maximum value is thus achieved at both $k=1$ and $k=n$. The spectral radius is\n$$\n\\rho(M_{\\mathrm{GS}}) = \\cos^2\\left(\\frac{\\pi}{n+1}\\right).\n$$\nWe are asked to compute this value for $n = 127$.\n$$\n\\rho(M_{\\mathrm{GS}}) = \\cos^2\\left(\\frac{\\pi}{127+1}\\right) = \\cos^2\\left(\\frac{\\pi}{128}\\right).\n$$\nNumerically evaluating this expression with the angle in radians:\n$$\n\\frac{\\pi}{128} \\approx 0.0245436926 \\text{ rad}\n$$\n$$\n\\cos\\left(\\frac{\\pi}{128}\\right) \\approx 0.99969886\n$$\n$$\n\\rho(M_{\\mathrm{GS}}) = \\left(\\cos\\left(\\frac{\\pi}{128}\\right)\\right)^2 \\approx (0.99969886)^2 \\approx 0.99939781\n$$\nRounding to six significant figures, we get $0.999398$.\n\nFinally, we explain the relation of the spectral radius to the convergence rate. The error in an iterative method of the form $\\phi^{(k+1)} = M \\phi^{(k)} + c$ is reduced at each step, on average, by a factor of $\\rho(M)$. The asymptotic rate of convergence is given by $R = -\\log_{10}(\\rho(M))$. For the Gauss-Seidel method on this problem, a spectral radius very close to $1$ implies very slow convergence.\n\nFor large $n$, we can use the Taylor expansion $\\cos(x) \\approx 1 - x^2/2$ for small $x$.\n$$\n\\rho(M_{\\mathrm{GS}}) = \\cos^2\\left(\\frac{\\pi}{n+1}\\right) \\approx \\left(1 - \\frac{1}{2}\\left(\\frac{\\pi}{n+1}\\right)^2\\right)^2 \\approx 1 - \\left(\\frac{\\pi}{n+1}\\right)^2.\n$$\nSince $h = L/(n+1)$, we have $n+1 = L/h$, so $\\rho(M_{\\mathrm{GS}}) \\approx 1 - (\\pi h/L)^2$. The convergence rate $R$ is approximately\n$$\nR = -\\log_{10}\\left(1 - \\left(\\frac{\\pi}{n+1}\\right)^2\\right) \\approx -\\frac{\\ln\\left(1 - \\left(\\frac{\\pi}{n+1}\\right)^2\\right)}{\\ln(10)} \\approx \\frac{1}{\\ln(10)}\\left(\\frac{\\pi}{n+1}\\right)^2 \\propto \\frac{1}{n^2} \\propto h^2.\n$$\nThe number of iterations required to achieve a certain error reduction is proportional to $1/R$, which is proportional to $(n+1)^2 \\propto h^{-2}$. This means that if the grid spacing $h$ is halved, the number of iterations required for the same accuracy increases by a factor of four. Our numerical result for $n=127$, $\\rho(M_{\\mathrm{GS}}) \\approx 0.999398$, is very close to $1$, quantitatively confirming that the GS method exhibits slow asymptotic convergence on fine grids for this problem.",
            "answer": "$$\\boxed{0.999398}$$"
        },
        {
            "introduction": "Real-world environmental systems are inherently non-linear, and solving the resulting equations often requires iterative methods that depend on the model's Jacobian. This exercise addresses a key practical challenge: how to approximate Jacobian-vector products efficiently without forming the massive Jacobian matrix itself. You will determine the optimal finite-difference step size $\\epsilon$ by balancing the trade-off between mathematical truncation error and computational roundoff error, a fundamental skill in developing robust numerical models .",
            "id": "3920541",
            "problem": "In atmospheric data assimilation for a nonlinear advection–diffusion model of a passive tracer, the forward operator is represented by a mapping $F:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$ that predicts the tracer concentration at $m$ observation locations from a nondimensional parameter vector $x\\in\\mathbb{R}^{n}$. A Gauss–Newton method (GN) requires Jacobian–vector products $J(x)v$, where $J(x)$ is the Jacobian of $F$ at $x$ and $v\\in\\mathbb{R}^{n}$ is a search direction. To avoid forming $J(x)$ explicitly, consider the forward finite-difference approximation\n$$\nJ(x)v\\approx \\frac{F(x+\\epsilon v)-F(x)}{\\epsilon},\n$$\nwith a nondimensional step size $\\epsilon0$.\n\nAssume the following scientifically grounded conditions:\n\n- The second Fréchet derivative (the Hessian) of $F$ at $x$ is bounded in operator norm by a constant $M0$ along unit directions; specifically, for any $w\\in\\mathbb{R}^{n}$ with $\\|w\\|_{2}=1$ and any $t\\in[0,\\epsilon]$, the Jacobian varies with Lipschitz constant $M$ as $\\|J(x+tw)-J(x)\\|_{2}\\leq Mt$.\n- The function evaluations $F(x)$ and $F(x+\\epsilon v)$ are performed in double-precision arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) 754 standard, with unit roundoff $u=1.1\\times 10^{-16}$. The algorithmic evaluation of $F$ yields a worst-case componentwise absolute error bounded by $\\delta=\\gamma\\,u\\,S$, where $\\gamma0$ is an algorithm-dependent amplification factor and $S0$ is a representative scale for the magnitude of $F(x)$.\n- At the current iterate $x$ in the GN method, the quantities are measured or estimated as follows: $M=8.0$, $\\gamma=20$, $S=\\|F(x)\\|_{2}=2.5\\times 10^{3}$, and the search direction is normalized with $\\|v\\|_{2}=1$.\n\nStarting only from the Taylor expansion with remainder for $F(x+\\epsilon v)$ and the stated floating-point error model, derive a worst-case upper bound on the total error in the finite-difference approximation, resolved into truncation (modeling) and roundoff (numerical) contributions as a function of $\\epsilon$. Then, determine the single value of the nondimensional step size $\\epsilon$ that minimizes this worst-case upper bound under the given conditions. Round your final numerical answer for $\\epsilon$ to three significant figures. Because $\\epsilon$ is nondimensional by construction, report it without any physical unit. The answer must be a single real number.",
            "solution": "The objective is to find the optimal nondimensional step size $\\epsilon$ for a forward finite-difference approximation of a Jacobian-vector product $J(x)v$. This involves deriving an upper bound on the total error, which is composed of a truncation error and a roundoff error, and then minimizing this bound with respect to $\\epsilon$. The problem is deemed valid as it is scientifically grounded in numerical analysis, well-posed, and all necessary parameters for the derivation are provided.\n\nThe finite-difference approximation is given by\n$$\nJ(x)v \\approx \\frac{F(x+\\epsilon v) - F(x)}{\\epsilon}\n$$\nLet $\\tilde{F}(y)$ denote the result of the algorithmic evaluation of $F(y)$ in floating-point arithmetic. The computed approximation is therefore\n$$\nA_{computed} = \\frac{\\tilde{F}(x+\\epsilon v) - \\tilde{F}(x)}{\\epsilon}\n$$\nThe total error, $E_{total}$, is the $2$-norm of the difference between the true Jacobian-vector product and the computed approximation:\n$$\nE_{total} = \\left\\| J(x)v - \\frac{\\tilde{F}(x+\\epsilon v) - \\tilde{F}(x)}{\\epsilon} \\right\\|_2\n$$\nWe can decompose this error by adding and subtracting the exact finite-difference term $\\frac{F(x+\\epsilon v) - F(x)}{\\epsilon}$ and applying the triangle inequality:\n$$\nE_{total} \\le \\left\\| J(x)v - \\frac{F(x+\\epsilon v) - F(x)}{\\epsilon} \\right\\|_2 + \\left\\| \\frac{F(x+\\epsilon v) - F(x)}{\\epsilon} - \\frac{\\tilde{F}(x+\\epsilon v) - \\tilde{F}(x)}{\\epsilon} \\right\\|_2\n$$\nThe first term is the truncation error, $E_{trunc}$, which arises from approximating the derivative with a finite difference. The second term is the roundoff error, $E_{round}$, which arises from floating-point computations.\n\nFirst, we derive an upper bound for the truncation error, $E_{trunc}$. We use the Taylor expansion of $F(x+\\epsilon v)$ around $x$ with an integral remainder. For a vector-valued function, the mean value theorem gives:\n$$\nF(x+\\epsilon v) - F(x) = \\int_0^1 \\frac{d}{dt} F(x+t\\epsilon v) \\, dt = \\int_0^1 J(x+t\\epsilon v)(\\epsilon v) \\, dt = \\epsilon \\int_0^1 J(x+t\\epsilon v)v \\, dt\n$$\nSubstituting this into the expression for the truncation error:\n$$\nE_{trunc} = \\left\\| J(x)v - \\int_0^1 J(x+t\\epsilon v)v \\, dt \\right\\|_2 = \\left\\| \\int_0^1 (J(x) - J(x+t\\epsilon v))v \\, dt \\right\\|_2\n$$\nUsing the triangle inequality for integrals and properties of operator norms:\n$$\nE_{trunc} \\le \\int_0^1 \\|(J(x) - J(x+t\\epsilon v))v\\|_2 \\, dt \\le \\int_0^1 \\|J(x+t\\epsilon v) - J(x)\\|_2 \\|v\\|_2 \\, dt\n$$\nThe problem provides the Lipschitz condition on the Jacobian: $\\|J(x+\\tau w) - J(x)\\|_2 \\le M\\tau$ for any unit vector $w$ and $\\tau \\in [0, \\epsilon]$. In our case, the displacement vector is $t\\epsilon v$. Since $\\|v\\|_2=1$, the direction is a unit vector. The parameter $\\tau$ corresponds to $t\\epsilon$. Thus, $\\|J(x+t\\epsilon v) - J(x)\\|_2 \\le M(t\\epsilon)$. Given $\\|v\\|_2=1$:\n$$\nE_{trunc} \\le \\int_0^1 M(t\\epsilon) \\cdot 1 \\, dt = M\\epsilon \\int_0^1 t \\, dt = M\\epsilon \\left[ \\frac{t^2}{2} \\right]_0^1 = \\frac{M\\epsilon}{2}\n$$\n\nNext, we derive an upper bound for the roundoff error, $E_{round}$. Let $e(y) = \\tilde{F}(y) - F(y)$ be the error vector from the function evaluation.\n$$\nE_{round} = \\left\\| \\frac{(F(x+\\epsilon v) - \\tilde{F}(x+\\epsilon v)) - (F(x) - \\tilde{F}(x))}{\\epsilon} \\right\\|_2 = \\frac{1}{\\epsilon} \\| -e(x+\\epsilon v) + e(x) \\|_2 \\le \\frac{1}{\\epsilon} (\\|e(x+\\epsilon v)\\|_2 + \\|e(x)\\|_2)\n$$\nThe problem states a \"worst-case componentwise absolute error bounded by $\\delta=\\gamma u S$\". This means $\\|e(y)\\|_\\infty \\le \\delta$. A direct conversion to the $2$-norm, $\\|e(y)\\|_2 \\le \\sqrt{m}\\|e(y)\\|_\\infty$, would introduce a dependency on the unstated dimension $m$. However, the problem defines the scale $S$ as the $2$-norm of the vector $F(x)$, i.e., $S = \\|F(x)\\|_2$. This suggests that the intended error model for the vector-valued function is one where the norm of the error vector is bounded in relation to the norm of the output vector. A standard and consistent interpretation is that the algorithmic evaluation produces an error whose norm is bounded by $\\|\\tilde{F}(y) - F(y)\\|_2 \\le \\gamma u \\|F(y)\\|_2$.\nAt the point $x$, this gives $\\|e(x)\\|_2 \\le \\gamma u \\|F(x)\\|_2 = \\gamma u S$.\nFor the evaluation at $x+\\epsilon v$, we assume $\\epsilon$ is small enough that $\\|F(x+\\epsilon v)\\|_2 \\approx \\|F(x)\\|_2 = S$. This is a standard assumption in this type of analysis. Therefore, we can bound the error norm as $\\|e(x+\\epsilon v)\\|_2 \\le \\gamma u S$.\nLet's define an effective error bound $\\delta_{eff} = \\gamma u S$. The roundoff error is then bounded by:\n$$\nE_{round} \\le \\frac{\\delta_{eff} + \\delta_{eff}}{\\epsilon} = \\frac{2\\gamma u S}{\\epsilon}\n$$\n\nCombining the two error bounds, we get an upper bound for the total error, $B(\\epsilon)$:\n$$\nB(\\epsilon) = E_{trunc} + E_{round} \\le \\frac{M\\epsilon}{2} + \\frac{2\\gamma u S}{\\epsilon}\n$$\nTo find the value of $\\epsilon$ that minimizes this upper bound, we differentiate $B(\\epsilon)$ with respect to $\\epsilon$ and set the derivative to zero:\n$$\n\\frac{dB}{d\\epsilon} = \\frac{M}{2} - \\frac{2\\gamma u S}{\\epsilon^2}\n$$\nSetting $\\frac{dB}{d\\epsilon} = 0$:\n$$\n\\frac{M}{2} = \\frac{2\\gamma u S}{\\epsilon^2} \\implies \\epsilon^2 = \\frac{4\\gamma u S}{M}\n$$\nThe optimal step size $\\epsilon_{opt}$ is the positive root:\n$$\n\\epsilon_{opt} = \\sqrt{\\frac{4\\gamma u S}{M}} = 2\\sqrt{\\frac{\\gamma u S}{M}}\n$$\nThe second derivative, $\\frac{d^2B}{d\\epsilon^2} = \\frac{4\\gamma u S}{\\epsilon^3}$, is positive for $\\epsilon  0$, confirming that this value minimizes the bound.\n\nNow, we substitute the given numerical values: $M=8.0$, $\\gamma=20$, $u=1.1 \\times 10^{-16}$, and $S=2.5 \\times 10^{3}$.\n$$\n\\epsilon_{opt} = 2\\sqrt{\\frac{(20)(1.1 \\times 10^{-16})(2.5 \\times 10^{3})}{8.0}}\n$$\n$$\n\\epsilon_{opt} = 2\\sqrt{\\frac{55 \\times 10^{-13}}{8.0}} = 2\\sqrt{6.875 \\times 10^{-13}} = 2\\sqrt{0.6875 \\times 10^{-12}}\n$$\n$$\n\\epsilon_{opt} = 2 \\times 10^{-6} \\sqrt{0.6875} \\approx 2 \\times 10^{-6} \\times 0.829156\n$$\n$$\n\\epsilon_{opt} \\approx 1.658312 \\times 10^{-6}\n$$\nRounding the result to three significant figures as required:\n$$\n\\epsilon_{opt} \\approx 1.66 \\times 10^{-6}\n$$",
            "answer": "$$\\boxed{1.66 \\times 10^{-6}}$$"
        }
    ]
}