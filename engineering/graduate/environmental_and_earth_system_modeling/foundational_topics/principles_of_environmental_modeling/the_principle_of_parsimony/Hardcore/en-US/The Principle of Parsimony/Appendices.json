{
    "hands_on_practices": [
        {
            "introduction": "The principle of parsimony, while intuitive, requires a quantitative framework to be applied in practice. Information criteria provide a powerful way to operationalize Ockham's razor by creating a formal trade-off between a model's goodness-of-fit and its complexity. This first exercise provides direct practice in calculating two of the most widely used metrics, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), for a realistic environmental modeling scenario. By working through this problem, you will see firsthand how their different penalty structures for model complexity can lead to different conclusions, prompting a deeper consideration of the assumptions underlying each criterion .",
            "id": "3925501",
            "problem": "In a regional carbon flux inversion for a temperate forest, two alternative forward models are considered for the atmospheric transport component within an Earth system data assimilation framework. Model A assumes a spatially homogeneous turbulence parameterization, while Model B introduces a spatially varying turbulence field resolved on a coarser grid. The data consist of $n=500$ independent daily observations of column-averaged carbon dioxide concentration, and each model is fit by Maximum Likelihood Estimation (MLE) to the same dataset. Let the number of identifiable free parameters be $k=12$ for Model A and $k=20$ for Model B. The maximized log-likelihoods are $\\ell(\\hat{\\theta}_A)=-1400$ and $\\ell(\\hat{\\theta}_B)=-1390$, respectively. Under the principle of parsimony (Ockham's razor), models are compared using two standard information criteria grounded in well-tested statistical theory: the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC).\n\nStarting from the fundamental definitions of expected information loss and marginal likelihood with appropriate asymptotic approximations, compute the AIC and BIC for both models, using the natural logarithm for the sample-size penalty in BIC. Explain how each criterion operationalizes parsimony and determine which model each criterion prefers. Express AIC values exactly as integers. Express BIC values rounded to four significant figures. Provide your final answer as a row matrix in the form $(\\text{AIC}_A, \\text{BIC}_A, \\text{AIC}_B, \\text{BIC}_B, p_{\\mathrm{AIC}}, p_{\\mathrm{BIC}})$, where $p_{\\mathrm{AIC}}=0$ if Model A is preferred by AIC and $p_{\\mathrm{AIC}}=1$ if Model B is preferred by AIC; similarly, $p_{\\mathrm{BIC}}=0$ if Model A is preferred by BIC and $p_{\\mathrm{BIC}}=1$ if Model B is preferred by BIC. No physical units are required for the criteria.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It requires the application of standard statistical model selection criteria (AIC and BIC) to a well-defined scenario in environmental modeling, with all necessary data provided.\n\nThe principle of parsimony, or Ockham's razor, suggests that among competing hypotheses, the one with the fewest assumptions should be selected. In statistical modeling, this translates to a preference for simpler models that can adequately explain the data. Information criteria provide a formal framework for balancing model fit (how well the model explains the data) against model complexity (the number of parameters).\n\nThe Akaike Information Criterion (AIC) is derived from information theory. It estimates the relative amount of information lost when a given model is used to represent the process that generates the data. This is equivalent to estimating the expected Kullback-Leibler divergence between the model and the true data-generating process. For a given model with $k$ free parameters fit to a dataset, resulting in a maximized log-likelihood value of $\\ell$, the AIC is defined as:\n$$\n\\text{AIC} = -2\\ell + 2k\n$$\nA lower AIC value indicates a better model, meaning it is estimated to lose less information. The term $-2\\ell$ represents the goodness-of-fit, while the term $2k$ serves as a penalty for complexity. Parsimony is operationalized by penalizing each additional parameter by a constant amount, forcing a more complex model to achieve a substantially better fit to be preferred.\n\nThe Bayesian Information Criterion (BIC), also known as the Schwarz Criterion, is derived from a Bayesian framework. It is an asymptotic approximation (for large sample sizes) of a function of the marginal likelihood of the data given the model, $p(\\text{data}|M)$. Choosing the model with the highest posterior probability (assuming equal prior probabilities for the models) is equivalent to choosing the model with the lowest BIC. The BIC is defined as:\n$$\n\\text{BIC} = -2\\ell + k \\ln(n)\n$$\nHere, $k$ is the number of parameters, $\\ell$ is the maximized log-likelihood, and $n$ is the number of observations. Similar to AIC, a lower BIC value is preferred. The BIC's penalty term, $k \\ln(n)$, depends on both the number of parameters $k$ and the sample size $n$. For any sample size $n > e^2 \\approx 7.4$, the BIC penalty per parameter, $\\ln(n)$, is larger than the AIC penalty of $2$. In this problem, $n=500$, so $\\ln(500) \\approx 6.21$. This means the BIC imposes a much stronger penalty for model complexity than AIC, thus more strongly favoring parsimonious models, particularly with large datasets.\n\nWe now compute these criteria for the two given models.\n\nFor Model A:\nThe number of parameters is $k_A = 12$.\nThe maximized log-likelihood is $\\ell(\\hat{\\theta}_A) = -1400$.\nThe number of observations is $n = 500$.\n\nThe AIC for Model A is:\n$$\n\\text{AIC}_A = -2\\ell(\\hat{\\theta}_A) + 2k_A = -2(-1400) + 2(12) = 2800 + 24 = 2824\n$$\n\nThe BIC for Model A is:\n$$\n\\text{BIC}_A = -2\\ell(\\hat{\\theta}_A) + k_A \\ln(n) = -2(-1400) + 12 \\ln(500) = 2800 + 12 \\ln(500)\n$$\nUsing the value $\\ln(500) \\approx 6.214608$, we have:\n$$\n\\text{BIC}_A \\approx 2800 + 12(6.214608) \\approx 2800 + 74.5753 = 2874.5753\n$$\nRounding to four significant figures, we get $\\text{BIC}_A = 2875$.\n\nFor Model B:\nThe number of parameters is $k_B = 20$.\nThe maximized log-likelihood is $\\ell(\\hat{\\theta}_B) = -1390$.\nThe number of observations is $n = 500$.\n\nThe AIC for Model B is:\n$$\n\\text{AIC}_B = -2\\ell(\\hat{\\theta}_B) + 2k_B = -2(-1390) + 2(20) = 2780 + 40 = 2820\n$$\n\nThe BIC for Model B is:\n$$\n\\text{BIC}_B = -2\\ell(\\hat{\\theta}_B) + k_B \\ln(n) = -2(-1390) + 20 \\ln(500) = 2780 + 20 \\ln(500)\n$$\nUsing $\\ln(500) \\approx 6.214608$:\n$$\n\\text{BIC}_B \\approx 2780 + 20(6.214608) \\approx 2780 + 124.2922 = 2904.2922\n$$\nRounding to four significant figures, we get $\\text{BIC}_B = 2904$.\n\nComparison and Model Preference:\nThe model with the lower information criterion value is preferred.\n\nAIC Comparison:\n$\\text{AIC}_A = 2824$\n$\\text{AIC}_B = 2820$\nSince $\\text{AIC}_B  \\text{AIC}_A$, the Akaike Information Criterion prefers Model B. The improved fit of Model B (increase of $10$ in log-likelihood) outweighs its penalty for $8$ additional parameters ($2 \\times 8 = 16$). The change in AIC is $2820 - 2824 = -4$.\nThus, $p_{\\mathrm{AIC}} = 1$.\n\nBIC Comparison:\n$\\text{BIC}_A \\approx 2875$\n$\\text{BIC}_B \\approx 2904$\nSince $\\text{BIC}_A  \\text{BIC}_B$, the Bayesian Information Criterion prefers Model A. Under BIC's stricter penalty, the improved fit of Model B is not sufficient to justify the added complexity. The penalty for $8$ additional parameters is $8 \\ln(500) \\approx 8 \\times 6.21 = 49.68$, which is much larger than the gain from the improved fit ($2 \\times 10 = 20$).\nThus, $p_{\\mathrm{BIC}} = 0$.\n\nThe final results are: $\\text{AIC}_A = 2824$, $\\text{BIC}_A \\approx 2875$, $\\text{AIC}_B = 2820$, $\\text{BIC}_B \\approx 2904$, $p_{\\mathrm{AIC}}=1$, and $p_{\\mathrm{BIC}}=0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2824  2875  2820  2904  1  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond frequentist-inspired information criteria, the Bayesian paradigm offers its own intrinsic and deeply principled mechanism for enforcing parsimony. Instead of adding an explicit penalty term for the number of parameters, the Bayesian framework evaluates models based on their marginal likelihood, or evidence. This practice demonstrates how this \"Bayesian Ockham's razor\" works by having you compute a Bayes factor, which quantifies the evidence from the data in favor of one model over another. By calculating the Bayes factor and interpreting its magnitude, you will gain insight into how a more complex model must provide a substantially better explanation of the data to overcome the inherent penalty for its additional complexity .",
            "id": "3925554",
            "problem": "Two candidate process-based aerosol forcing models are compared for their ability to explain a global top-of-atmosphere radiative imbalance dataset. Model $M_1$ is a parsimonious linear mapping from aerosol optical depth to effective radiative forcing with $2$ parameters, and model $M_2$ is a more complex regime-switching nonlinear mapping with $4$ parameters. Both models use proper, scientifically defensible priors over parameters that place nonzero mass only on physically plausible parameter ranges. The data $y$ consist of annually averaged observations spanning $1970$–$2020$, and each model’s marginal likelihood $p(y \\mid M_i)$ has been computed by integrating the likelihood over its parameter prior.\n\nYou are given the log-evidences $\\log p(y \\mid M_1) = -520$ and $\\log p(y \\mid M_2) = -515$. Assume equal prior odds for $M_1$ and $M_2$, consistent with the principle of parsimony (Ockham’s razor), which prefers simpler models unless the data provide sufficiently strong support to overcome the implicit complexity penalty in the marginal likelihood.\n\nUsing only these inputs and first principles of Bayesian model comparison, compute the Bayes factor favoring $M_2$ over $M_1$, and interpret the strength of evidence for choosing between the two aerosol forcing models in terms of the qualitative categories commonly used in the literature for Bayes factors. Report the numerical value of the Bayes factor in standard scientific notation rounded to four significant figures. No units are required because the Bayes factor is dimensionless.",
            "solution": "The primary tool for Bayesian model comparison is the Bayes factor. The Bayes factor, $K_{ij}$, which quantifies the evidence from the data $y$ in favor of model $M_i$ over model $M_j$, is defined as the ratio of their marginal likelihoods (also known as evidences):\n$$\nK_{ij} = \\frac{p(y \\mid M_i)}{p(y \\mid M_j)}\n$$\nIn this problem, we are asked to compute the Bayes factor favoring model $M_2$ over model $M_1$. This corresponds to $K_{21}$:\n$$\nK_{21} = \\frac{p(y \\mid M_2)}{p(y \\mid M_1)}\n$$\nThe problem provides the natural logarithms of the marginal likelihoods, which are the log-evidences:\n$$\n\\log p(y \\mid M_1) = -520\n$$\n$$\n\\log p(y \\mid M_2) = -515\n$$\nTo compute $K_{21}$, it is numerically stable to first compute its logarithm. The logarithm of the Bayes factor is the difference between the log-evidences:\n$$\n\\log K_{21} = \\log\\left(\\frac{p(y \\mid M_2)}{p(y \\mid M_1)}\\right) = \\log p(y \\mid M_2) - \\log p(y \\mid M_1)\n$$\nSubstituting the given values:\n$$\n\\log K_{21} = (-515) - (-520) = 5\n$$\nTo find the Bayes factor $K_{21}$, we exponentiate this result:\n$$\nK_{21} = \\exp(\\log K_{21}) = \\exp(5)\n$$\nNow, we compute the numerical value and round it to four significant figures as required:\n$$\nK_{21} \\approx 148.413159...\n$$\n$$\nK_{21} \\approx 1.484 \\times 10^{2}\n$$\nThe second part of the task is to interpret this value. Bayes factors are interpreted using qualitative scales to describe the strength of evidence. A commonly used scale, such as the one proposed by Kass and Raftery (1995), categorizes the evidence as follows:\n- $1  K  3$: Not worth more than a bare mention (or \"anecdotal\").\n- $3  K  20$: Positive (or \"substantial\").\n- $20  K  150$: Strong.\n- $K  150$: Very strong (or \"decisive\").\n\nOur calculated value $K_{21} \\approx 148.4$ falls at the upper end of the \"strong\" evidence category, bordering on \"very strong\" evidence in favor of model $M_2$. This means that the observed data $y$ are approximately $148.4$ times more probable under model $M_2$ than under model $M_1$.\n\nThe problem states that $M_2$ is a more complex model with $4$ parameters, while $M_1$ is a simpler model with $2$ parameters. The marginal likelihood $p(y \\mid M)$ inherently penalizes complexity; a more complex model must provide a substantially better fit to the data to justify its extra parameters. This is the Bayesian quantification of the principle of parsimony (Ockham's razor). In this case, the data provide strong to very strong evidence that overcomes this implicit penalty, favoring the more complex nonlinear model $M_2$ over the parsimonious linear model $M_1$.",
            "answer": "$$\\boxed{1.484 \\times 10^{2}}$$"
        },
        {
            "introduction": "The reliability of any model selection process hinges on the integrity of the validation experiment used to estimate model performance. This is especially true in Earth system science, where data are rarely independent and identically distributed. This final practice shifts the focus from calculating metrics to the critical task of designing a robust validation procedure for a time series with strong autocorrelation and periodicity. Answering this question requires you to diagnose the potential for information \"leakage\" in standard cross-validation methods and design a procedure that provides an unbiased estimate of generalization error. This exercise underscores that applying the principle of parsimony is not just about a final calculation, but about ensuring the entire methodological pipeline is sound and fair .",
            "id": "3925495",
            "problem": "Consider a daily environmental time series $\\{y_t\\}_{t=1}^N$ with $N$ sufficiently large to contain multiple years (e.g., $N \\approx 365 \\times 10$), where $y_t$ exhibits a strong annual cycle attributable to periodic forcing. Suppose you aim to compare a family of harmonic regression models indexed by the number of included harmonics $P \\in \\{1,2,3,4\\}$, where the candidate models are of the form\n$$\ny_t = \\sum_{p=1}^{P} \\left[a_p \\cos(p \\omega_1 t) + b_p \\sin(p \\omega_1 t)\\right] + \\varepsilon_t,\n$$\nwith $\\omega_1 = 2\\pi / 365$ and $\\varepsilon_t$ a stochastic residual term. The residual process $\\{\\varepsilon_t\\}$ is weakly stationary and autocorrelated with autocorrelation function (ACF) $\\rho(\\tau)$, and there is interannual variability in the cycle amplitude due to large-scale climate modes. Define the decorrelation time $\\tau_c$ as the smallest lag $\\tau$ such that $|\\rho(\\tau)| \\le \\epsilon$ for a small threshold $\\epsilon$ (e.g., $\\epsilon = 0.05$). Assume $\\tau_c$ is on the order of $10$ to $20$ days.\n\nYou intend to use Cross-Validation (CV) to estimate out-of-sample performance via Mean Squared Error (MSE), defined as \n$$\n\\mathrm{MSE} = \\frac{1}{M}\\sum_{i=1}^{M} (\\hat{y}_{t_i} - y_{t_i})^2\n$$\nover a test set of size $M$, and to decide whether adding harmonics (increasing $P$) materially improves generalization according to the Principle of Parsimony (Ockham’s razor), which favors simpler models unless a more complex model achieves a demonstrable and robust reduction in generalization error.\n\nFrom first principles, unbiased CV requires that training and test sets be approximately independent. In time series with strong periodic components and non-negligible autocorrelation, independence can be approximated by separating train and test observations by a gap $g$ satisfying $g \\ge \\tau_c$ to mitigate leakage via temporal dependence, and by grouping to avoid phase leakage from repeated annual structure.\n\nWhich option best outlines a stratified cross-validation procedure that avoids leakage and thereby prevents overestimating the benefit of added harmonics, while ensuring a fair, parsimonious comparison across $P$?\n\nA. Perform random $k$-fold CV on individual daily observations, stratifying by astronomical season so each fold contains an equal fraction of winter, spring, summer, and fall days. Use the same folds to tune $P$ and to estimate $\\mathrm{MSE}$, without any temporal gaps.\n\nB. Use grouped, blocked leave-one-year-out CV in an outer loop: define groups by calendar year, and for each outer fold hold out one full year as the test set. Impose symmetric temporal gaps of $g$ days around the test year boundaries with $g \\ge \\tau_c$ (e.g., $g = \\lceil 2\\tau_c \\rceil$) to limit dependence. Stratify the outer folds by interannual cycle amplitude (e.g., quartiles of annual amplitude) so that performance estimates aggregate across regimes. In an inner nested CV loop, tune $P$ using the same grouped-by-year blocking and gap strategy on the training years only. Aggregate outer-fold $\\mathrm{MSE}$ across held-out years to compare $P$, selecting the smallest $P$ that achieves a statistically significant and robust reduction in $\\mathrm{MSE}$.\n\nC. Partition the data by harmonic phase: compute $\\phi_t = \\omega_1 t \\bmod 2\\pi$ and assign each day to one of $K$ equal-width phase bins. Perform $K$-fold CV by holding out one phase bin across all years per fold, training on the complement, and use the same phase folds to tune $P$ and estimate $\\mathrm{MSE}$.\n\nD. Use rolling-origin CV with an expanding training window: for fold $j$, train on all data up to day $t_j$, impose a gap of $g$ days, and test on the next $h$ days, choosing $t_j$ so that each fold lands in a different season. Tune $P$ on the same rolling folds and aggregate $\\mathrm{MSE}$ across all folds to select $P$.\n\nSelect the option that most rigorously avoids leakage and yields an unbiased assessment of whether adding harmonics genuinely improves generalization in this environmental time series, consistent with the Principle of Parsimony.",
            "solution": "The problem statement is a valid exercise in statistical methodology for environmental time series analysis. It is scientifically grounded, well-posed, objective, and contains sufficient information to determine the most appropriate cross-validation strategy.\n\nThe core task is to select a model complexity, represented by the number of harmonics $P$, for a harmonic regression model applied to a daily environmental time series. The data presents three critical statistical features that a robust cross-validation (CV) procedure must address to provide an unbiased estimate of generalization error and facilitate a parsimonious model choice:\n\n1.  **Autocorrelation:** The residual process $\\{\\varepsilon_t\\}$ is autocorrelated. This means that observations close in time are not independent. Standard random $k$-fold CV, which assumes independence, would be invalid as it would place highly correlated data points in both the training and test sets. This \"leakage\" of information leads to an overly optimistic (downwardly biased) estimate of the true generalization error, which in turn encourages the selection of overly complex models (larger $P$). To mitigate this, the train and test sets must be separated by a temporal gap $g$ that is larger than the decorrelation time $\\tau_c$. The problem states $\\tau_c$ is on the order of $10$ to $20$ days, so a gap $g \\ge \\tau_c$ is required.\n\n2.  **Strong Periodicity (Annual Cycle):** The model's purpose is to capture a strong annual cycle with fundamental frequency $\\omega_1 = 2\\pi/365$. If the CV procedure allows the model to see data from the same phase (e.g., the same day of the year) in both training and testing (e.g., `train` on Jan 1, Year 1; `test` on Jan 1, Year 2), it can \"memorize\" the value for that phase rather than learning the generalizable shape of the periodic function. This is a form of information leakage specific to periodic data. The most robust way to prevent this is to hold out entire, contiguous blocks of time that span one or more full cycles. For an annual cycle, this means holding out one or more full years of data.\n\n3.  **Interannual Variability:** The problem specifies that the amplitude of the annual cycle varies from year to year. A good estimate of generalization performance must be robust to this variability. This means the model should be trained and tested across years with different characteristics (e.g., high-amplitude years, low-amplitude years). A stratified CV scheme, where the folds are constructed to be representative of this year-to-year variability, will yield a more reliable estimate of expected performance on future, unseen years.\n\n4.  **Hyperparameter Tuning vs. Performance Estimation:** The goal is to both tune the hyperparameter $P$ and estimate the final model's generalization performance. Using the same data splits for both tasks introduces an optimistic bias, as the hyperparameter $P$ would be selected to perform best on those specific test sets. The correct approach is **nested cross-validation**. An outer loop splits the data for performance estimation, and for each training set in the outer loop, an inner loop is run to select the optimal $P$. The model with this chosen $P$ is then evaluated on the outer loop's test set.\n\nWe will now evaluate each option against these four requirements.\n\n**A. Perform random $k$-fold CV on individual daily observations, stratifying by astronomical season so each fold contains an equal fraction of winter, spring, summer, and fall days. Use the same folds to tune $P$ and to estimate $\\mathrm{MSE}$, without any temporal gaps.**\n\nThis procedure is fundamentally flawed. By performing random sampling on individual observations, it completely destroys the temporal structure of the data. This maximizes information leakage due to autocorrelation, as a test point $y_t$ will almost certainly be adjacent to training points $y_{t-1}$ and $y_{t+1}$. The explicit lack of temporal gaps (`without any temporal gaps`) confirms this fatal flaw. Stratifying by season is insufficient to correct this. This method will severely underestimate the true generalization error. Furthermore, using the same folds to tune $P$ and estimate final performance induces selection bias.\n\n**Verdict: Incorrect.**\n\n**B. Use grouped, blocked leave-one-year-out CV in an outer loop: define groups by calendar year, and for each outer fold hold out one full year as the test set. Impose symmetric temporal gaps of $g$ days around the test year boundaries with $g \\ge \\tau_c$ (e.g., $g = \\lceil 2\\tau_c \\rceil$) to limit dependence. Stratify the outer folds by interannual cycle amplitude (e.g., quartiles of annual amplitude) so that performance estimates aggregate across regimes. In an inner nested CV loop, tune $P$ using the same grouped-by-year blocking and gap strategy on the training years only. Aggregate outer-fold $\\mathrm{MSE}$ across held-out years to compare $P$, selecting the smallest $P$ that achieves a statistically significant and robust reduction in $\\mathrm{MSE}$.**\n\nThis procedure is exceptionally rigorous and a textbook example of best practice for this problem.\n- **Grouped, blocked leave-one-year-out CV:** This perfectly addresses the issue of phase leakage from the strong annual cycle by forcing the model to predict an entire year it has never seen.\n- **Symmetric temporal gaps:** The use of gaps $g \\ge \\tau_c$ correctly mitigates information leakage from autocorrelation at the boundaries of the held-out year.\n- **Stratification by interannual cycle amplitude:** This accounts for the specified interannual variability, ensuring the performance estimate is robust and representative.\n- **Nested CV:** It correctly uses a nested loop structure to separate hyperparameter tuning ($P$) from final performance estimation, thus avoiding selection bias.\n- **Parsimony:** The final selection criterion explicitly incorporates the Principle of Parsimony, aligning with the problem's objective.\n\nThis option systematically addresses all four of the critical requirements identified.\n\n**Verdict: Correct.**\n\n**C. Partition the data by harmonic phase: compute $\\phi_t = \\omega_1 t \\bmod 2\\pi$ and assign each day to one of $K$ equal-width phase bins. Perform $K$-fold CV by holding out one phase bin across all years per fold, training on the complement, and use the same phase folds to tune $P$ and estimate $\\mathrm{MSE}$.**\n\nThis approach, sometimes called \"leave-one-phase-out\" CV, is not suitable here. While it prevents the model from seeing the exact same phase in training and testing, it fails to address autocorrelation. If the phase bin for \"January 15\" is held out, the model is still trained on \"January 14\" and \"January 16\" from all years. The high correlation between adjacent days' residuals allows for significant information leakage. This method tests the model's ability to interpolate between phases, not to generalize its learned cycle to a completely unseen time block. It also uses the same folds for tuning and estimation, which is improper.\n\n**Verdict: Incorrect.**\n\n**D. Use rolling-origin CV with an expanding training window: for fold $j$, train on all data up to day $t_j$, impose a gap of $g$ days, and test on the next $h$ days, choosing $t_j$ so that each fold lands in a different season. Tune $P$ on the same rolling folds and aggregate $\\mathrm{MSE}$ across all folds to select $P$.**\n\nRolling-origin (or walk-forward) validation is a valid time series CV method that respects temporal order and, with the inclusion of a gap, can mitigate autocorrelation. However, it is suboptimal for this specific problem for several reasons. First, testing on a short block of $h$ days is a less stringent test of generalization for a periodic model than holding out a full cycle (one year). The model is never required to extrapolate the periodic function far into the future or over a full cycle it has not seen. Second, the procedure described uses the same folds for tuning $P$ and estimating performance, which introduces bias. Third, the expanding window is inefficient, with early folds having very small training sets, potentially leading to unstable results. Leave-one-year-out CV (Option B) makes much more efficient use of the data, as each training set is large.\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}