{
    "hands_on_practices": [
        {
            "introduction": "A foundational skill in environmental modeling is the ability to derive governing equations from fundamental physical laws. This exercise challenges you to apply the principles of mass conservation and Darcy's law to a classic hydrogeology scenario, demonstrating how to construct and analytically solve a boundary value problem for groundwater flow .",
            "id": "3883626",
            "problem": "Consider an unconfined aquifer extending along a horizontal coordinate $x$ on the interval $[0,L]$ under steady flow. The system is one-dimensional ($1$D) and receives uniform areal recharge (accumulation) at a constant rate $R0$. Assume the Dupuit–Forchheimer (DF) approximation (vertical hydraulic gradients are negligible and flow is predominantly horizontal) and Darcy’s law. Let $H(x)$ denote the saturated thickness of the aquifer measured from an impermeable base, and let $q(x)$ denote the specific discharge (volume flux per unit width) in the $+x$ direction. The hydraulic conductivity $K$ is constant. Two rivers fix the water table at the boundaries, imposing the Dirichlet conditions $H(0)=H_{0}0$ and $H(L)=H_{L}0$.\n\nStarting from the local mass balance in steady state and Darcy’s law under the DF approximation only, derive the governing equation and solve for the water table profile $H(x)$ that is consistent with these boundary conditions. Express your final answer as a single closed-form analytic expression for $H(x)$ in terms of $x$, $L$, $K$, $R$, $H_{0}$, and $H_{L}$. No numerical evaluation is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of hydrogeology, mathematically well-posed, and an objective statement of a standard boundary value problem. We can proceed with the derivation of the solution.\n\nThe derivation begins with the principle of mass conservation in a steady state. Consider a one-dimensional control volume of the aquifer of infinitesimal length $dx$ along the $x$-axis and of unit width. The flow is governed by the specific discharge per unit width, $q(x)$, which represents the volume of water flowing through the cross-section at $x$ per unit time. The aquifer receives a uniform areal recharge at a rate $R$, which has units of volume per unit area per unit time. For our one-dimensional system of unit width, this recharge contributes a source term of $R \\cdot dx$ to the control volume.\n\nThe steady-state mass balance equation for the control volume is:\n$$ \\text{Flow in} + \\text{Source} = \\text{Flow out} $$\n$$ q(x) + R \\, dx = q(x+dx) $$\nRearranging the terms, we get:\n$$ q(x+dx) - q(x) = R \\, dx $$\nDividing by $dx$ and taking the limit as $dx \\to 0$ yields the differential form of the mass conservation equation:\n$$ \\frac{dq}{dx} = R $$\nThis equation states that the rate of change of specific discharge along the flow path is equal to the recharge rate.\n\nNext, we apply Darcy's law as adapted by the Dupuit-Forchheimer (DF) approximation. The DF approximation assumes that the flow is essentially horizontal, so the hydraulic head, $h$, at any point $x$ can be taken as equal to the saturated thickness of the aquifer, $H(x)$. Thus, $h(x) = H(x)$.\n\nAccording to Darcy's law, the specific discharge (flux per unit area of aquifer), let's call it $q'$, is given by:\n$$ q'(x) = -K \\frac{dh}{dx} = -K \\frac{dH}{dx} $$\nwhere $K$ is the constant hydraulic conductivity. The quantity $q(x)$ in the problem statement is the specific discharge per unit width, which is the Darcy flux $q'(x)$ integrated over the saturated thickness $H(x)$:\n$$ q(x) = q'(x) \\cdot H(x) = \\left(-K \\frac{dH}{dx}\\right) H(x) = -K H(x) \\frac{dH}{dx} $$\nThis expression can be written more conveniently using the chain rule identity $\\frac{d}{dx}(H^2) = 2H \\frac{dH}{dx}$:\n$$ q(x) = -\\frac{K}{2} \\frac{d}{dx}\\left(H(x)^2\\right) $$\n\nNow, we substitute this expression for $q(x)$ into the mass conservation equation $\\frac{dq}{dx} = R$:\n$$ \\frac{d}{dx} \\left(-\\frac{K}{2} \\frac{d}{dx}\\left(H(x)^2\\right)\\right) = R $$\nSince $K$ is a constant, we can move it outside the derivative:\n$$ -\\frac{K}{2} \\frac{d^2}{dx^2}\\left(H(x)^2\\right) = R $$\nThis is the governing second-order ordinary differential equation for $H(x)^2$. Let us define a new variable $Y(x) = H(x)^2$. The equation becomes:\n$$ \\frac{d^2Y}{dx^2} = -\\frac{2R}{K} $$\nThis is a simple linear ODE which can be solved by direct integration. Integrating once with respect to $x$ gives:\n$$ \\frac{dY}{dx} = -\\frac{2R}{K}x + C_1 $$\nwhere $C_1$ is a constant of integration. Integrating a second time gives the general solution for $Y(x)$:\n$$ Y(x) = -\\frac{R}{K}x^2 + C_1 x + C_2 $$\nwhere $C_2$ is the second constant of integration. Substituting back $Y(x) = H(x)^2$:\n$$ H(x)^2 = -\\frac{R}{K}x^2 + C_1 x + C_2 $$\n\nThe constants $C_1$ and $C_2$ are determined by applying the given Dirichlet boundary conditions: $H(0) = H_0$ and $H(L) = H_L$. In terms of $Y(x)$, these are $Y(0) = H_0^2$ and $Y(L) = H_L^2$.\n\nApplying the first boundary condition at $x=0$:\n$$ Y(0) = H_0^2 = -\\frac{R}{K}(0)^2 + C_1(0) + C_2 $$\nThis immediately yields $C_2 = H_0^2$. The solution now takes the form:\n$$ H(x)^2 = -\\frac{R}{K}x^2 + C_1 x + H_0^2 $$\n\nApplying the second boundary condition at $x=L$:\n$$ Y(L) = H_L^2 = -\\frac{R}{K}L^2 + C_1 L + H_0^2 $$\nWe solve this equation for $C_1$:\n$$ C_1 L = H_L^2 - H_0^2 + \\frac{R}{K}L^2 $$\n$$ C_1 = \\frac{H_L^2 - H_0^2}{L} + \\frac{RL}{K} $$\n\nFinally, we substitute the expressions for $C_1$ and $C_2$ back into the general solution for $H(x)^2$:\n$$ H(x)^2 = -\\frac{R}{K}x^2 + \\left(\\frac{H_L^2 - H_0^2}{L} + \\frac{RL}{K}\\right)x + H_0^2 $$\nTo obtain a more organized expression, we can group the terms:\n$$ H(x)^2 = H_0^2 + \\frac{H_L^2 - H_0^2}{L}x + \\frac{RL}{K}x - \\frac{R}{K}x^2 $$\n$$ H(x)^2 = H_0^2 + \\left(\\frac{H_L^2 - H_0^2}{L}\\right)x + \\frac{R}{K}(Lx - x^2) $$\nSince $H(x)$ represents a physical saturated thickness, it must be non-negative. We therefore take the positive square root to obtain the final expression for the water table profile:\n$$ H(x) = \\sqrt{H_0^2 + \\frac{H_L^2 - H_0^2}{L}x + \\frac{R}{K}(Lx - x^2)} $$\nThis solution describes a parabolic water table profile superimposed on a linear profile that would exist in the absence of recharge ($R=0$).",
            "answer": "$$ \\boxed{H(x) = \\sqrt{H_0^2 + \\frac{H_L^2 - H_0^2}{L}x + \\frac{R}{K}(Lx - x^2)}} $$"
        },
        {
            "introduction": "While analytical solutions provide deep insight, most real-world problems require numerical methods. This practice guides you through the implementation and analysis of the Crank-Nicolson scheme, a cornerstone of computational physics, for the diffusion equation . By exploring its stability and damping properties through Fourier analysis, you will gain practical skills in evaluating the performance and reliability of numerical models.",
            "id": "3883646",
            "problem": "Consider one-dimensional diffusion of a passive tracer concentration governed by Fick’s First Law and conservation of mass. The flux is given by $J(x,t)=-D\\,\\partial_x c(x,t)$, where $D$ is a constant molecular diffusivity, and the conservation law is $\\partial_t c(x,t)+\\partial_x J(x,t)=0$. Combining these yields the parabolic partial differential equation (PDE) $\\partial_t c(x,t)=D\\,\\partial_{xx}c(x,t)$.\n\nYou will implement the Crank–Nicolson time-stepping scheme for the spatially discrete form of this PDE on a uniform periodic grid, and then use your implementation to numerically assess unconditional stability and numerical damping characteristics. The domain is $[0,L]$ with periodic boundary conditions and $N$ uniformly spaced grid points with spacing $\\Delta x=L/N$. Use the standard second-order centered difference for the discrete Laplacian operator and the trapezoidal (Crank–Nicolson) rule in time for the semi-implicit update between time levels $n$ and $n+1$.\n\nStarting from the fundamental base of Fick’s First Law and conservation of mass, derive the scheme you implement. Do not use any shortcut formulas in the problem statement. Your program should then apply the scheme in a way that allows you to:\n- Compute the per-step amplification factor of any discrete Fourier mode on the periodic grid.\n- Compare the Crank–Nicolson amplification with the exact semi-discrete time evolution of the spatially discretized ordinary differential equation.\n\nDefinitions to be used in your reasoning and implementation:\n- A discrete Fourier mode on a periodic grid with $N$ points is indexed by an integer $m\\in\\{0,1,\\dots,N-1\\}$ and has grid value proportional to $\\cos\\!\\big(2\\pi m\\, j/N\\big)$ or $\\sin\\!\\big(2\\pi m\\, j/N\\big)$ at grid index $j$.\n- The semi-discrete exact time evolution refers to analytically integrating the system of ordinary differential equations obtained after spatial discretization, yielding $c(t)=\\exp\\!\\big(\\lambda_m\\, t\\big)\\,c(0)$ for each discrete eigenmode, where $\\lambda_m$ is the eigenvalue of the discrete Laplacian corresponding to mode $m$.\n\nPhysical and numerical units:\n- Use $L$ in meters ($\\mathrm{m}$), $D$ in square meters per second ($\\mathrm{m}^2/\\mathrm{s}$), time $t$ in seconds ($\\mathrm{s}$), and wavenumbers in radians per meter ($\\mathrm{rad}/\\mathrm{m}$).\n- All requested outputs are dimensionless numbers and should be reported as plain floating-point values or booleans as specified.\n\nTest suite and outputs:\nImplement your program to evaluate the following three test cases. In each case, assume an initial condition $c(x,0)=A\\cos\\!\\big(2\\pi m\\,x/L\\big)$ with amplitude $A=1$ so that the dynamics are entirely captured by the amplification factor of mode $m$.\n\n- Test Case $1$ (accuracy in a non-stiff regime):\n    - Parameters: $L=1\\,\\mathrm{m}$, $N=128$, $D=10^{-4}\\,\\mathrm{m}^2/\\mathrm{s}$, $\\Delta t=10^{-3}\\,\\mathrm{s}$, final time $T=0.1\\,\\mathrm{s}$, mode index $m=4$.\n    - Task: Compute the absolute difference between the Crank–Nicolson-amplified magnitude after $T$ seconds and the semi-discrete exact magnitude after $T$ seconds for mode $m$. Return this absolute difference as a floating-point number.\n\n- Test Case $2$ (unconditional stability across all modes for a large time step):\n    - Parameters: $L=1\\,\\mathrm{m}$, $N=128$, $D=10^{-4}\\,\\mathrm{m}^2/\\mathrm{s}$, $\\Delta t=5\\,\\mathrm{s}$.\n    - Task: For all discrete modes $m\\in\\{0,\\dots,N-1\\}$, compute the per-step Crank–Nicolson amplification factor magnitude. Return a boolean that is $True$ if and only if every mode satisfies $|G_m|\\le 1$ (unconditional stability), otherwise return $False$.\n\n- Test Case $3$ (numerical damping deficiency for stiff high-frequency modes and temporal oscillations):\n    - Parameters: $L=1\\,\\mathrm{m}$, $N=128$, $D=1\\,\\mathrm{m}^2/\\mathrm{s}$, $\\Delta t=0.1\\,\\mathrm{s}$, final time $T=10\\,\\mathrm{s}$, mode index $m=N/2$ (Nyquist mode).\n    - Task: Compute the base-$10$ logarithm of the ratio of the Crank–Nicolson-amplified magnitude at $T$ seconds to the semi-discrete exact magnitude at $T$ seconds for mode $m$. Return this value as a floating-point number. A large positive value indicates severe under-damping by the Crank–Nicolson scheme relative to the semi-discrete exact decay.\n\nFinal output format:\nYour program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, in the order of Test Case $1$, Test Case $2$, Test Case $3$. For example, the output should look like $[x_1,x_2,x_3]$, where $x_1$ and $x_3$ are floating-point values and $x_2$ is a boolean.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the principles of mass conservation and Fickian diffusion, mathematically well-posed, and presented with objective, precise language. All necessary data and definitions for the three specified test cases are provided, and the problem is a standard, non-trivial exercise in the numerical analysis of partial differential equations.\n\n### 1. Derivation of the Governing Equation and Numerical Scheme\n\nThe problem starts with two fundamental principles: the conservation of mass for a tracer concentration $c(x,t)$ and Fick's First Law for diffusive flux $J(x,t)$.\n\nThe conservation law in one dimension is expressed as a continuity equation:\n$$\n\\partial_t c(x,t) + \\partial_x J(x,t) = 0\n$$\nwhere $\\partial_t$ is the partial derivative with respect to time $t$ and $\\partial_x$ is the partial derivative with respect to space $x$.\n\nFick's First Law states that the flux is proportional to the negative of the concentration gradient, describing a down-gradient transport process:\n$$\nJ(x,t) = -D \\, \\partial_x c(x,t)\n$$\nwhere $D$ is the constant molecular diffusivity.\n\nSubstituting the expression for $J(x,t)$ into the conservation law yields:\n$$\n\\partial_t c(x,t) + \\partial_x \\left(-D \\, \\partial_x c(x,t)\\right) = 0\n$$\nAs the diffusivity $D$ is given as a constant, it can be factored out of the spatial derivative, leading to the one-dimensional diffusion equation, a parabolic partial differential equation (PDE):\n$$\n\\partial_t c(x,t) = D \\, \\partial_{xx} c(x,t)\n$$\n\n### 2. Spatial and Temporal Discretization\n\nTo solve this PDE numerically, we first perform a spatial discretization (method of lines). The domain $[0, L]$ is divided into $N$ uniform intervals of width $\\Delta x = L/N$, with grid points at $x_j = j \\Delta x$ for $j = 0, 1, \\dots, N-1$. Let $c_j(t)$ denote the numerical approximation of the concentration $c(x_j, t)$.\n\nThe second spatial derivative, $\\partial_{xx} c$, at grid point $x_j$ is approximated using the second-order central difference formula. This is appropriate for the periodic boundary conditions specified:\n$$\n\\partial_{xx} c(x_j, t) \\approx \\frac{c_{j+1}(t) - 2c_j(t) + c_{j-1}(t)}{\\Delta x^2}\n$$\nwhere indices are taken modulo $N$ (e.g., $c_N = c_0$ and $c_{-1} = c_{N-1}$). Substituting this into the PDE results in a system of coupled ordinary differential equations (ODEs), the semi-discrete form:\n$$\n\\frac{d c_j(t)}{dt} = \\frac{D}{\\Delta x^2} \\left( c_{j+1}(t) - 2c_j(t) + c_{j-1}(t) \\right)\n$$\nThis system can be written in vector form as $\\frac{d\\vec{c}}{dt} = \\mathbf{L}\\vec{c}$, where $\\vec{c} = [c_0, c_1, \\dots, c_{N-1}]^T$ and $\\mathbf{L}$ is the matrix representing the discrete Laplacian operator multiplied by $D$.\n\nFor the time integration, we apply the Crank-Nicolson method, which is equivalent to the trapezoidal rule. For a time step $\\Delta t$, with $\\vec{c}^n \\approx \\vec{c}(n\\Delta t)$, the scheme is:\n$$\n\\frac{\\vec{c}^{n+1} - \\vec{c}^n}{\\Delta t} = \\frac{1}{2} \\left( \\mathbf{L}\\vec{c}^{n+1} + \\mathbf{L}\\vec{c}^n \\right)\n$$\nThis is an implicit method. To facilitate computation, we rearrange the equation to isolate the unknown state $\\vec{c}^{n+1}$ on one side:\n$$\n\\vec{c}^{n+1} - \\frac{\\Delta t}{2} \\mathbf{L}\\vec{c}^{n+1} = \\vec{c}^n + \\frac{\\Delta t}{2} \\mathbf{L}\\vec{c}^n\n$$\nFactoring out the state vectors yields the standard form of the Crank-Nicolson scheme, which requires solving a linear system at each time step:\n$$\n\\left(\\mathbf{I} - \\frac{\\Delta t}{2} \\mathbf{L}\\right) \\vec{c}^{n+1} = \\left(\\mathbf{I} + \\frac{\\Delta t}{2} \\mathbf{L}\\right) \\vec{c}^n\n$$\nwhere $\\mathbf{I}$ is the identity matrix.\n\n### 3. Fourier Mode Analysis (von Neumann Stability Analysis)\n\nTo analyze the accuracy and stability of the scheme, we study its behavior on discrete Fourier modes, which are the eigenvectors of the circulant matrix $\\mathbf{L}$. A complex Fourier mode with index $m$ on the grid is given by $c_j = \\exp(i k x_j) = \\exp(i 2\\pi m j / N)$, where $k = 2\\pi m / L$ is the wavenumber.\n\nApplying the discrete spatial operator to this mode reveals the corresponding eigenvalue $\\lambda_m$:\n$$\n(\\mathbf{L} \\vec{c})_j = \\frac{D}{\\Delta x^2} \\left( e^{i 2\\pi m (j+1)/N} - 2e^{i 2\\pi m j/N} + e^{i 2\\pi m (j-1)/N} \\right) = \\frac{D}{\\Delta x^2} c_j \\left( e^{i 2\\pi m/N} - 2 + e^{-i 2\\pi m/N} \\right)\n$$\nUsing Euler's formula, $e^{i\\theta} + e^{-i\\theta} = 2\\cos(\\theta)$, this simplifies to:\n$$\n(\\mathbf{L} \\vec{c})_j = \\frac{2D}{\\Delta x^2} c_j \\left( \\cos\\left(\\frac{2\\pi m}{N}\\right) - 1 \\right)\n$$\nWith the identity $1 - \\cos(2\\theta) = 2\\sin^2(\\theta)$, the eigenvalue $\\lambda_m$ is:\n$$\n\\lambda_m = \\frac{-4D}{\\Delta x^2} \\sin^2\\left(\\frac{\\pi m}{N}\\right)\n$$\nThese eigenvalues are real and non-positive ($\\lambda_m \\le 0$), consistent with a purely dissipative physical process.\n\nThe \"semi-discrete exact\" evolution of mode $m$ follows the ODE $\\frac{dc_m(t)}{dt} = \\lambda_m c_m(t)$, whose solution is $c_m(t) = c_m(0) \\exp(\\lambda_m t)$. The amplification of this mode over a time $T$ is $\\exp(\\lambda_m T)$.\n\nThe numerical amplification factor, $G_m$, for the Crank-Nicolson scheme is found by substituting $\\vec{c}^{n+1} = G_m \\vec{c}^n$ and $\\mathbf{L}\\vec{c} = \\lambda_m \\vec{c}$ into the scheme's equation:\n$$\n\\left(1 - \\frac{\\Delta t}{2} \\lambda_m\\right) G_m = \\left(1 + \\frac{\\Delta t}{2} \\lambda_m\\right) \\implies G_m = \\frac{1 + \\lambda_m \\Delta t / 2}{1 - \\lambda_m \\Delta t / 2}\n$$\n\n### 4. Implementation Strategy for Test Cases\n\nThe derived formulas for $\\lambda_m$ and $G_m$ are used to evaluate the test cases.\n\n**Test Case 1:** The goal is to compute the accuracy of the scheme. We calculate the absolute difference between the total numerical amplification over $N_{steps} = T / \\Delta t$ steps and the total semi-discrete exact amplification over time $T$. The quantity is $\\left| |G_m|^{N_{steps}} - \\exp(\\lambda_m T) \\right|$.\n\n**Test Case 2:** This test verifies the unconditional stability property of the Crank-Nicolson scheme for the diffusion equation. We must check if $|G_m| \\le 1$ for all modes $m \\in \\{0, \\dots, N-1\\}$. Since $\\lambda_m \\le 0$ and $\\Delta t  0$, the term $z_m = \\lambda_m \\Delta t / 2$ is non-positive. Let $z_m = -a_m$ where $a_m \\ge 0$. Then $|G_m| = |(1-a_m)/(1+a_m)|$. As shown in elementary analysis, this quantity is always less than or equal to $1$ for $a_m \\ge 0$. Therefore, the scheme is A-stable, and the test must evaluate to `True`.\n\n**Test Case 3:** This case highlights a known deficiency of the Crank-Nicolson scheme: its lack of strong damping for high-frequency (stiff) modes. We analyze the Nyquist mode, $m=N/2$, which has the largest magnitude eigenvalue and thus the fastest physical decay rate. We compute the base-$10$ logarithm of the ratio of the numerical amplification magnitude to the exact amplification magnitude: $\\log_{10}\\left( |G_m|^{N_{steps}} / \\exp(\\lambda_m T) \\right)$. For large $\\Delta t$, the value of $G_{N/2}$ approaches $-1$, meaning the numerical solution barely decays and oscillates in sign, whereas the exact solution decays almost instantaneously. This results in a very large positive value for the log-ratio. For numerical stability, this calculation is performed in log-space.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the Crank-Nicolson scheme analysis and print the results.\n    \"\"\"\n\n    # --- Helper function for core physics/numerics calculation ---\n    def get_modal_properties(L, N, D, dt, m):\n        \"\"\"\n        Computes the eigenvalue lambda_m of the discrete Laplacian and the \n        Crank-Nicolson amplification factor G_m for a given mode m.\n        \n        Args:\n            L (float): Domain length.\n            N (int): Number of grid points.\n            D (float): Diffusivity.\n            dt (float): Time step.\n            m (int): Fourier mode index.\n            \n        Returns:\n            tuple: (lambda_m, G_m)\n        \"\"\"\n        dx = L / N\n        \n        # Eigenvalue lambda_m. Handle the m=0 (constant) mode explicitly\n        # to ensure it is exactly zero.\n        if m % N == 0:\n            lambda_m = 0.0\n        else:\n            # Formula derived from centered difference operator on a periodic grid.\n            lambda_m = (-4.0 * D / dx**2) * np.sin(np.pi * m / N)**2\n\n        # Crank-Nicolson per-step amplification factor G_m.\n        numerator = 1.0 + 0.5 * dt * lambda_m\n        denominator = 1.0 - 0.5 * dt * lambda_m\n        # Denominator is guaranteed to be positive as D>=0, dt>0, and lambda_m=0.\n        G_m = numerator / denominator\n        \n        return lambda_m, G_m\n\n    # --- Container for results ---\n    results = []\n\n    # =========================================================================\n    # Test Case 1: Accuracy in a non-stiff regime\n    # =========================================================================\n    params1 = {'L': 1.0, 'N': 128, 'D': 1e-4, 'dt': 1e-3, 'T': 0.1, 'm': 4}\n    \n    lambda_m1, G_m1 = get_modal_properties(params1['L'], params1['N'], params1['D'], params1['dt'], params1['m'])\n    \n    num_steps1 = int(round(params1['T'] / params1['dt']))\n    \n    # Final magnitude from Crank-Nicolson scheme after T seconds.\n    cn_magnitude1 = np.abs(G_m1)**num_steps1\n    \n    # Final magnitude from semi-discrete exact solution after T seconds.\n    exact_magnitude1 = np.exp(lambda_m1 * params1['T'])\n    \n    result1 = np.abs(cn_magnitude1 - exact_magnitude1)\n    results.append(result1)\n\n    # =========================================================================\n    # Test Case 2: Unconditional stability\n    # =========================================================================\n    params2 = {'L': 1.0, 'N': 128, 'D': 1e-4, 'dt': 5.0}\n\n    is_stable = True\n    # Theory guarantees stability (|G_m| = 1). This is a numerical verification.\n    for m_idx in range(params2['N']):\n        _, G_m_val = get_modal_properties(params2['L'], params2['N'], params2['D'], params2['dt'], m_idx)\n        # Allow a small floating-point tolerance for the check.\n        if np.abs(G_m_val) > 1.0 + 1e-12:\n            is_stable = False\n            break\n    \n    result2 = is_stable\n    results.append(result2)\n\n    # =========================================================================\n    # Test Case 3: Numerical damping deficiency for stiff modes\n    # =========================================================================\n    params3 = {'L': 1.0, 'N': 128, 'D': 1.0, 'dt': 0.1, 'T': 10.0, 'm': 128 // 2}\n\n    lambda_m3, G_m3 = get_modal_properties(params3['L'], params3['N'], params3['D'], params3['dt'], params3['m'])\n    \n    num_steps3 = int(round(params3['T'] / params3['dt']))\n\n    # To avoid potential underflow/overflow from direct ratio computation,\n    # the log of the ratio is computed in log space: log10(a/b) = log10(a) - log10(b).\n    # log10(|G|**N_steps) = N_steps * log10(|G|)\n    # log10(exp(lambda*T)) = (lambda*T) / ln(10)\n    \n    log10_cn_mag = num_steps3 * np.log10(np.abs(G_m3))\n    log10_exact_mag = (lambda_m3 * params3['T']) / np.log(10)\n    \n    result3 = log10_cn_mag - log10_exact_mag\n    results.append(result3)\n\n    # =========================================================================\n    # Final Output Formatting\n    # =========================================================================\n    # The format must be exactly [val1,val2,val3] with no extra spaces or text.\n    output_str = f\"[{results[0]},{results[1]},{results[2]}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Environmental models are most powerful when constrained by real-world data. This hands-on practice introduces inverse modeling, a critical technique for inferring unknown sources or parameters from observations . You will derive and implement a solution using Tikhonov regularization, learning how to stabilize ill-posed problems and extract physically meaningful information from noisy data.",
            "id": "3883625",
            "problem": "Consider an environmental inverse problem in a steady-state setting where a scalar source field $S(\\mathbf{x})$ produces observed concentrations at a finite set of monitors via a linear forward response. Under discretization on $n$ grid cells, the unknown source vector is $\\mathbf{s} \\in \\mathbb{R}^n$ with units kilograms per second (kg/s). Observations are collected at $m$ monitors and represented as a concentration vector $\\mathbf{c} \\in \\mathbb{R}^m$ with units milligrams per cubic meter (mg/m$^3$). The linear forward operator is the matrix $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ whose entries encode the response of each monitor to unit source emissions from each grid cell; physically, entries of $\\mathbf{G}$ have units mg m$^{-3}$ per (kg s$^{-1}$). Measurement errors are modeled as additive, independent, zero-mean Gaussian noise with known variances, which defines a diagonal weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times m}$ with entries $w_i = 1/\\sigma_i^2$, where $\\sigma_i$ is the standard deviation of monitor $i$ in mg/m$^3$. To mitigate ill-posedness and encode smoothness or other constraints, Tikhonov regularization is introduced through a penalty operator $\\mathbf{L} \\in \\mathbb{R}^{p \\times n}$ and a regularization strength $\\lambda \\ge 0$.\n\nStarting from the following fundamental base:\n- Mass conservation and linearity of the steady-state response imply the discrete relation $\\mathbf{c} = \\mathbf{G}\\mathbf{s} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon}$ is the measurement error vector.\n- Under the Gaussian error model, minimizing a weighted least squares objective is equivalent to Maximum Likelihood Estimation (MLE).\n- Tikhonov regularization adds a quadratic penalty on $\\mathbf{L}\\mathbf{s}$ to stabilize the inversion.\n\nDerive, from first principles, the normal equations that determine the Tikhonov-regularized estimator for $\\mathbf{s}$ using the objective that combines the weighted data misfit and the quadratic regularization penalty. Clearly state each mathematical step and any assumptions used.\n\nThen, implement a program that, for each test case specified below, computes the Tikhonov-regularized estimate $\\widehat{\\mathbf{s}}$ by solving the derived normal equations. All final source strengths must be reported in kilograms per second (kg/s). Each entry must be rounded to six decimal places.\n\nUse the following test suite. In every case, $\\mathbf{L}$ is either the identity matrix of appropriate size or the discrete first-difference operator on a one-dimensional grid defined by\n$$\n\\mathbf{L}_{\\text{diff}} = \\begin{bmatrix}\n-1  1  0  \\cdots  0 \\\\\n0  -1  1  \\cdots  0 \\\\\n\\vdots    \\ddots  \\vdots \\\\\n0  \\cdots  0  -1  1\n\\end{bmatrix} \\in \\mathbb{R}^{(n-1) \\times n}.\n$$\nFor each case, the weight matrix is $\\mathbf{W} = \\mathrm{diag}(w_1,\\ldots,w_m)$.\n\n- Case 1 (general well-posed with smoothness):\n  - $n = 3$, $m = 3$.\n  - $\\mathbf{G} = \\begin{bmatrix} 1.0  0.5  0.0 \\\\ 0.2  1.0  0.3 \\\\ 0.0  0.3  0.8 \\end{bmatrix}$.\n  - $\\mathbf{c} = \\begin{bmatrix} 1.5 \\\\ 2.0 \\\\ 1.0 \\end{bmatrix}$ mg/m$^3$.\n  - Measurement uncertainties: $\\boldsymbol{\\sigma} = \\begin{bmatrix} 0.10 \\\\ 0.20 \\\\ 0.15 \\end{bmatrix}$ mg/m$^3$, so $\\mathbf{W} = \\mathrm{diag}(100.0, 25.0, 44.\\overline{4})$.\n  - $\\mathbf{L} = \\mathbf{L}_{\\text{diff}}$ for $n=3$.\n  - $\\lambda = 0.1$.\n\n- Case 2 (boundary case: no regularization, invertible forward):\n  - $n = 4$, $m = 4$.\n  - $\\mathbf{G} = \\mathrm{diag}(2.0, 2.0, 2.0, 2.0)$ mg m$^{-3}$ per (kg s$^{-1}$).\n  - $\\mathbf{c} = \\begin{bmatrix} 0.5 \\\\ 0.1 \\\\ 0.0 \\\\ 0.9 \\end{bmatrix}$ mg/m$^3$.\n  - $\\mathbf{W} = \\mathbf{I}_4$.\n  - $\\mathbf{L} = \\mathbf{L}_{\\text{diff}}$ for $n=4$ (it is ignored because $\\lambda=0$).\n  - $\\lambda = 0$.\n\n- Case 3 (rank-deficient forward, identity penalty):\n  - $n = 3$, $m = 2$.\n  - $\\mathbf{G} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\end{bmatrix}$.\n  - $\\mathbf{c} = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}$ mg/m$^3$.\n  - $\\mathbf{W} = \\mathbf{I}_2$.\n  - $\\mathbf{L} = \\mathbf{I}_3$.\n  - $\\lambda = 0.5$.\n\n- Case 4 (heteroskedastic weighting, smoothness penalty):\n  - $n = 3$, $m = 3$.\n  - $\\mathbf{G} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.0  1.5  0.1 \\\\ 0.1  0.0  0.8 \\end{bmatrix}$.\n  - $\\mathbf{c} = \\begin{bmatrix} 0.8 \\\\ 1.2 \\\\ 0.6 \\end{bmatrix}$ mg/m$^3$.\n  - Measurement uncertainties: $\\boldsymbol{\\sigma} = \\begin{bmatrix} 0.05 \\\\ 0.20 \\\\ 0.10 \\end{bmatrix}$ mg/m$^3$, so $\\mathbf{W} = \\mathrm{diag}(400.0, 25.0, 100.0)$.\n  - $\\mathbf{L} = \\mathbf{L}_{\\text{diff}}$ for $n=3$.\n  - $\\lambda = 0.01$.\n\n- Case 5 (strong smoothing):\n  - Same as Case 1 but with $\\lambda = 100.0$.\n\nYour program should produce a single line of output containing the results for all five test cases, aggregated as a comma-separated list enclosed in square brackets, where each element is itself a bracketed comma-separated list of the estimated source strengths in kg/s, rounded to six decimal places. For example, the overall format should be like $[ [\\cdots], [\\cdots], [\\cdots], [\\cdots], [\\cdots] ]$ with no spaces after commas. The program must not read input and must not print any additional text.",
            "solution": "The problem requires the derivation of the normal equations for a Tikhonov-regularized inverse problem and their subsequent implementation to solve for an unknown source vector $\\mathbf{s}$. The derivation begins from the fundamental principles outlined in the problem description.\n\nThe physical system is described by the linear model $\\mathbf{c} = \\mathbf{G}\\mathbf{s} + \\boldsymbol{\\epsilon}$, where $\\mathbf{s} \\in \\mathbb{R}^n$ is the unknown source vector, $\\mathbf{c} \\in \\mathbb{R}^m$ is the observation vector, $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ is the linear forward operator, and $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^m$ is the measurement error.\n\nThe measurement errors are assumed to be independent, zero-mean Gaussian variables with known variances $\\sigma_i^2$. Based on this, the principle of Maximum Likelihood Estimation (MLE) calls for maximizing the likelihood function $p(\\mathbf{c}|\\mathbf{s})$. For Gaussian noise, this is equivalent to minimizing the negative log-likelihood, which is proportional to the weighted sum of squared residuals. This gives the data misfit term $J_{\\text{misfit}}(\\mathbf{s})$:\n$$ J_{\\text{misfit}}(\\mathbf{s}) = (\\mathbf{G}\\mathbf{s} - \\mathbf{c})^T \\mathbf{W} (\\mathbf{G}\\mathbf{s} - \\mathbf{c}) $$\nHere, $\\mathbf{W} \\in \\mathbb{R}^{m \\times m}$ is the weight matrix, which is diagonal with entries $w_{ii} = 1/\\sigma_i^2$. This matrix is the inverse of the error covariance matrix.\n\nTo regularize the inverse problem, a penalty term, $J_{\\text{reg}}(\\mathbf{s})$, is added to the objective function. This term penalizes solutions that are not physically plausible (e.g., are not smooth). The Tikhonov regularization penalty is quadratic in form:\n$$ J_{\\text{reg}}(\\mathbf{s}) = \\lambda (\\mathbf{L}\\mathbf{s})^T (\\mathbf{L}\\mathbf{s}) $$\nwhere $\\mathbf{L} \\in \\mathbb{R}^{p \\times n}$ is the regularization operator and $\\lambda \\ge 0$ is a scalar regularization parameter that controls the trade-off between fitting the data and the penalty.\n\nThe total objective function $J(\\mathbf{s})$ to be minimized is the sum of the data misfit and the regularization penalty:\n$$ J(\\mathbf{s}) = J_{\\text{misfit}}(\\mathbf{s}) + J_{\\text{reg}}(\\mathbf{s}) = (\\mathbf{G}\\mathbf{s} - \\mathbf{c})^T \\mathbf{W} (\\mathbf{G}\\mathbf{s} - \\mathbf{c}) + \\lambda (\\mathbf{L}\\mathbf{s})^T (\\mathbf{L}\\mathbf{s}) $$\n\nTo find the source vector estimate $\\widehat{\\mathbf{s}}$ that minimizes $J(\\mathbf{s})$, we must find the point where the gradient of $J(\\mathbf{s})$ with respect to $\\mathbf{s}$ is the zero vector, i.e., $\\nabla_{\\mathbf{s}} J(\\mathbf{s}) = \\mathbf{0}$. We start by expanding the terms of $J(\\mathbf{s})$:\n$$ (\\mathbf{G}\\mathbf{s} - \\mathbf{c})^T \\mathbf{W} (\\mathbf{G}\\mathbf{s} - \\mathbf{c}) = (\\mathbf{s}^T\\mathbf{G}^T - \\mathbf{c}^T)\\mathbf{W}(\\mathbf{G}\\mathbf{s} - \\mathbf{c}) = \\mathbf{s}^T\\mathbf{G}^T\\mathbf{W}\\mathbf{G}\\mathbf{s} - \\mathbf{s}^T\\mathbf{G}^T\\mathbf{W}\\mathbf{c} - \\mathbf{c}^T\\mathbf{W}\\mathbf{G}\\mathbf{s} + \\mathbf{c}^T\\mathbf{W}\\mathbf{c} $$\nThe term $\\mathbf{c}^T\\mathbf{W}\\mathbf{G}\\mathbf{s}$ is a scalar and is therefore equal to its transpose. Since $\\mathbf{W}$ is diagonal, it is a symmetric matrix, meaning $\\mathbf{W}^T = \\mathbf{W}$. This allows us to write $(\\mathbf{c}^T\\mathbf{W}\\mathbf{G}\\mathbf{s})^T = \\mathbf{s}^T\\mathbf{G}^T\\mathbf{W}^T\\mathbf{c} = \\mathbf{s}^T\\mathbf{G}^T\\mathbf{W}\\mathbf{c}$. The misfit term thus simplifies to:\n$$ J_{\\text{misfit}}(\\mathbf{s}) = \\mathbf{s}^T(\\mathbf{G}^T\\mathbf{W}\\mathbf{G})\\mathbf{s} - 2\\mathbf{s}^T(\\mathbf{G}^T\\mathbf{W}\\mathbf{c}) + \\mathbf{c}^T\\mathbf{W}\\mathbf{c} $$\n\nThe regularization term can be written as:\n$$ J_{\\text{reg}}(\\mathbf{s}) = \\lambda \\mathbf{s}^T\\mathbf{L}^T\\mathbf{L}\\mathbf{s} $$\n\nCombining these expressions, the full objective function is:\n$$ J(\\mathbf{s}) = \\mathbf{s}^T(\\mathbf{G}^T\\mathbf{W}\\mathbf{G} + \\lambda\\mathbf{L}^T\\mathbf{L})\\mathbf{s} - 2\\mathbf{s}^T(\\mathbf{G}^T\\mathbf{W}\\mathbf{c}) + \\mathbf{c}^T\\mathbf{W}\\mathbf{c} $$\nThis is a quadratic function of $\\mathbf{s}$. We compute its gradient using the standard vector calculus identities $\\nabla_{\\mathbf{x}}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}$ for a symmetric matrix $\\mathbf{A}$, and $\\nabla_{\\mathbf{x}}(\\mathbf{b}^T\\mathbf{x}) = \\mathbf{b}$. The matrix $(\\mathbf{G}^T\\mathbf{W}\\mathbf{G} + \\lambda\\mathbf{L}^T\\mathbf{L})$ is symmetric. The term $\\mathbf{c}^T\\mathbf{W}\\mathbf{c}$ is constant with respect to $\\mathbf{s}$, so its gradient is zero. The gradient of $J(\\mathbf{s})$ is:\n$$ \\nabla_{\\mathbf{s}} J(\\mathbf{s}) = 2(\\mathbf{G}^T\\mathbf{W}\\mathbf{G} + \\lambda\\mathbf{L}^T\\mathbf{L})\\mathbf{s} - 2(\\mathbf{G}^T\\mathbf{W}\\mathbf{c}) $$\n\nSetting the gradient to zero, $\\nabla_{\\mathbf{s}} J(\\widehat{\\mathbf{s}}) = \\mathbf{0}$, yields the optimal estimate $\\widehat{\\mathbf{s}}$:\n$$ 2(\\mathbf{G}^T\\mathbf{W}\\mathbf{G} + \\lambda\\mathbf{L}^T\\mathbf{L})\\widehat{\\mathbf{s}} - 2(\\mathbf{G}^T\\mathbf{W}\\mathbf{c}) = \\mathbf{0} $$\n$$ (\\mathbf{G}^T\\mathbf{W}\\mathbf{G} + \\lambda\\mathbf{L}^T\\mathbf{L})\\widehat{\\mathbf{s}} = \\mathbf{G}^T\\mathbf{W}\\mathbf{c} $$\n\nThis final equation is the system of linear equations known as the **normal equations** for the Tikhonov-regularized weighted least-squares problem. The solution for the estimated source vector $\\widehat{\\mathbf{s}}$ is obtained by solving this system. This system has a unique solution if the matrix $(\\mathbf{G}^T\\mathbf{W}\\mathbf{G} + \\lambda\\mathbf{L}^T\\mathbf{L})$ is invertible. For $\\lambda  0$, this is generally true if the nullspaces of $\\mathbf{G}$ and $\\mathbf{L}$ intersect only at the origin ($\\text{ker}(\\mathbf{G}) \\cap \\text{ker}(\\mathbf{L}) = \\{\\mathbf{0}\\}$), a condition met by the provided test cases.\n\nThe provided Python code implements this solution. For each test case, it constructs the matrices and vectors of the normal equations and solves the system for $\\widehat{\\mathbf{s}}$ using `numpy.linalg.solve`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the Tikhonov regularization problem for all test cases.\n    \"\"\"\n\n    def create_L_diff(n):\n        \"\"\"\n        Creates the discrete first-difference operator L_diff of size (n-1) x n.\n        \"\"\"\n        if n  2:\n            return np.zeros((0, n))\n        p = n - 1\n        L = np.zeros((p, n))\n        rows = np.arange(p)\n        cols1 = np.arange(p)\n        cols2 = np.arange(1, n)\n        L[rows, cols1] = -1.0\n        L[rows, cols2] = 1.0\n        return L\n\n    def solve_tikhonov(G, c, W, L, lam):\n        \"\"\"\n        Solves the Tikhonov-regularized normal equations:\n        (G.T @ W @ G + lam * L.T @ L) @ s = G.T @ W @ c\n        \"\"\"\n        # Ensure c is a column vector (m x 1)\n        c = c.reshape(-1, 1)\n\n        # G is m x n, W is m x m, L is p x n\n        GT = G.T\n        LT = L.T\n\n        # Left-hand side of the normal equations (n x n)\n        LHS = GT @ W @ G + lam * (LT @ L)\n        \n        # Right-hand side of the normal equations (n x 1)\n        RHS = GT @ W @ c\n        \n        # Solve the linear system for s_hat\n        s_hat = np.linalg.solve(LHS, RHS)\n        \n        # Return as a 1D array\n        return s_hat.flatten()\n\n    # Define the test cases from the problem statement.\n    test_cases_data = [\n        {\n            # Case 1\n            \"G\": np.array([[1.0, 0.5, 0.0], [0.2, 1.0, 0.3], [0.0, 0.3, 0.8]]),\n            \"c\": np.array([1.5, 2.0, 1.0]),\n            \"sigma\": np.array([0.10, 0.20, 0.15]),\n            \"L\": create_L_diff(3),\n            \"lam\": 0.1,\n        },\n        {\n            # Case 2\n            \"G\": np.diag([2.0, 2.0, 2.0, 2.0]),\n            \"c\": np.array([0.5, 0.1, 0.0, 0.9]),\n            \"sigma\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"L\": create_L_diff(4),\n            \"lam\": 0.0,\n        },\n        {\n            # Case 3\n            \"G\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"c\": np.array([1.0, 0.5]),\n            \"sigma\": np.array([1.0, 1.0]),\n            \"L\": np.eye(3),\n            \"lam\": 0.5,\n        },\n        {\n            # Case 4\n            \"G\": np.array([[1.0, 0.2, 0.0], [0.0, 1.5, 0.1], [0.1, 0.0, 0.8]]),\n            \"c\": np.array([0.8, 1.2, 0.6]),\n            \"sigma\": np.array([0.05, 0.20, 0.10]),\n            \"L\": create_L_diff(3),\n            \"lam\": 0.01,\n        },\n        {\n            # Case 5 (Same as Case 1 but with lambda = 100.0)\n            \"G\": np.array([[1.0, 0.5, 0.0], [0.2, 1.0, 0.3], [0.0, 0.3, 0.8]]),\n            \"c\": np.array([1.5, 2.0, 1.0]),\n            \"sigma\": np.array([0.10, 0.20, 0.15]),\n            \"L\": create_L_diff(3),\n            \"lam\": 100.0,\n        },\n    ]\n\n    results = []\n    for case_data in test_cases_data:\n        G = case_data[\"G\"]\n        c = case_data[\"c\"]\n        sigma = case_data[\"sigma\"]\n        L = case_data[\"L\"]\n        lam = case_data[\"lam\"]\n\n        # Construct the weight matrix W\n        W = np.diag(1.0 / sigma**2)\n\n        # Solve for the source strengths s_hat\n        s_hat = solve_tikhonov(G, c, W, L, lam)\n        \n        # Format the result vector for this case\n        s_hat_formatted = [f\"{x:.6f}\" for x in s_hat]\n        case_result_str = f\"[{','.join(s_hat_formatted)}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}