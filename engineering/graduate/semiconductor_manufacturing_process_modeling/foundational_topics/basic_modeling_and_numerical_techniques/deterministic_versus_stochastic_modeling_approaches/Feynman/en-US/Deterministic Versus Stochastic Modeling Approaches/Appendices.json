{
    "hands_on_practices": [
        {
            "introduction": "Even when a physical process like chemical etching is governed by a deterministic law, our ability to measure and model it is affected by random noise. This exercise bridges the deterministic world of differential equations with the stochastic reality of measurement . By deriving an estimator and its variance, you will learn how to quantify the uncertainty of parameters in deterministic models when faced with real-world, noisy data.",
            "id": "4119772",
            "problem": "In a wet etch step of a semiconductor manufacturing process, the concentration of a reactive etchant species is described deterministically by the ordinary differential equation (ODE) $\\dot{c}(t) = -k\\,c(t)$, where $k$ is the (unknown) reaction rate constant and $c(t)$ is the etchant concentration at time $t$. The deterministic model solution, given an exactly known initial concentration $c(0) = c_0$, is $c(t) = c_0 \\exp(-k t)$.\n\nYou collect $N$ scalar measurements of concentration at known times $\\{t_i\\}_{i=1}^{N}$, modeled as $y_i = c(t_i) + \\varepsilon_i$, where the measurement noise $\\varepsilon_i$ is stochastic, independent and identically distributed with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, and $\\sigma^2$ is known. Treat the ODE as the deterministic process model and the measurement noise as the sole source of randomness.\n\nTasks:\n- Formulate the deterministic Nonlinear Least Squares (NLS) estimator $\\hat{k}$ for $k$ based on minimizing the sum of squared residuals between the data $\\{y_i\\}$ and the model $c_0 \\exp(-k t_i)$.\n- Using a first-order local linearization argument about the true $k$ under the stated noise model, derive a closed-form analytic expression for the asymptotic variance of $\\hat{k}$ as a function of $k$, $c_0$, $\\{t_i\\}$, and $\\sigma^2$.\n\nProvide as your final answer the derived analytic expression for the asymptotic variance of $\\hat{k}$ in terms of $k$, $c_0$, $\\{t_i\\}$, and $\\sigma^2$. The final answer must be a single closed-form expression. Do not include units in the final answer box. Do not round or approximate.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n-   The deterministic process model is the ordinary differential equation (ODE): $\\dot{c}(t) = -k\\,c(t)$.\n-   $k$ is the unknown reaction rate constant.\n-   $c(t)$ is the etchant concentration at time $t$.\n-   The initial concentration is exactly known: $c(0) = c_0$.\n-   The solution to the ODE is given as $c(t) = c_0 \\exp(-k t)$.\n-   $N$ scalar measurements, $\\{y_i\\}_{i=1}^{N}$, are collected at known times $\\{t_i\\}_{i=1}^{N}$.\n-   The measurement model is $y_i = c(t_i) + \\varepsilon_i$.\n-   The measurement noise terms, $\\varepsilon_i$, are independent and identically distributed (i.i.d.) random variables following a normal distribution with mean $0$ and known variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes first-order reaction kinetics, a fundamental and widely used model in chemistry and chemical engineering, including for processes like wet etching. The use of an additive Gaussian noise model for measurements is a standard assumption in statistical signal processing and parameter estimation. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides all necessary information (the model function, the statistical properties of the noise, and the known parameters) to formulate an estimator and derive its asymptotic properties. The tasks are specific and lead to a unique analytical expression. The problem is well-posed.\n-   **Objective:** The problem is stated using precise mathematical and technical language, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically sound, self-contained, and well-posed. A solution will be derived.\n\n**Part 1: Formulation of the Nonlinear Least Squares (NLS) Estimator**\nThe deterministic model for the concentration at time $t_i$ is a function of the unknown parameter $k$, which we denote as $f(t_i; k) = c_0 \\exp(-k t_i)$.\nThe Nonlinear Least Squares (NLS) method seeks to find the value of $k$ that minimizes the sum of the squared differences (residuals) between the observed measurements $y_i$ and the model predictions $f(t_i; k)$. The sum of squared residuals, $S(k)$, is given by:\n$$S(k) = \\sum_{i=1}^{N} [y_i - f(t_i; k)]^2 = \\sum_{i=1}^{N} [y_i - c_0 \\exp(-k t_i)]^2$$\nThe NLS estimator for $k$, denoted as $\\hat{k}$, is the value of $k$ that minimizes this function:\n$$\\hat{k} = \\arg\\min_k S(k) = \\arg\\min_k \\sum_{i=1}^{N} [y_i - c_0 \\exp(-k t_i)]^2$$\nThis formulation fulfills the first task. To find $\\hat{k}$ in practice, one would solve the first-order necessary condition $\\frac{dS(k)}{dk} = 0$, which yields a nonlinear equation in $k$.\n\n**Part 2: Derivation of the Asymptotic Variance of $\\hat{k}$**\nWe derive the asymptotic variance of $\\hat{k}$ using a first-order local linearization of the model around the true, unknown value of the parameter, which we denote as $k_{true}$. The measurement model is $y_i = f(t_i; k_{true}) + \\varepsilon_i$.\n\nThe estimator $\\hat{k}$ satisfies the first-order condition $\\frac{dS(k)}{dk}\\bigg|_{k=\\hat{k}} = 0$.\n$$\\frac{dS(k)}{dk} = \\sum_{i=1}^{N} 2[y_i - f(t_i; k)] \\left(-\\frac{\\partial f(t_i; k)}{\\partial k}\\right) = 0$$\nLet $J_i(k) = \\frac{\\partial f(t_i; k)}{\\partial k}$ be the partial derivative of the model function with respect to $k$ for the $i$-th data point.\n$$J_i(k) = \\frac{\\partial}{\\partial k} [c_0 \\exp(-k t_i)] = -c_0 t_i \\exp(-k t_i)$$\nThe condition for $\\hat{k}$ becomes:\n$$\\sum_{i=1}^{N} [y_i - f(t_i; \\hat{k})] J_i(\\hat{k}) = 0$$\nFor an estimator based on a large number of data points or small noise, $\\hat{k}$ is expected to be close to $k_{true}$. We can therefore linearize the model function $f(t_i; \\hat{k})$ and the Jacobian term $J_i(\\hat{k})$ around $k_{true}$. A first-order Taylor expansion of $f(t_i; \\hat{k})$ around $k_{true}$ gives:\n$$f(t_i; \\hat{k}) \\approx f(t_i; k_{true}) + J_i(k_{true}) (\\hat{k} - k_{true})$$\nWe also approximate the Jacobian term in the first-order condition as $J_i(\\hat{k}) \\approx J_i(k_{true})$. Substituting these approximations and the measurement model $y_i = f(t_i; k_{true}) + \\varepsilon_i$ into the condition at $\\hat{k}$:\n$$\\sum_{i=1}^{N} \\left[ (f(t_i; k_{true}) + \\varepsilon_i) - \\left( f(t_i; k_{true}) + J_i(k_{true}) (\\hat{k} - k_{true}) \\right) \\right] J_i(k_{true}) \\approx 0$$\nSimplifying the term inside the brackets:\n$$\\sum_{i=1}^{N} \\left[ \\varepsilon_i - J_i(k_{true}) (\\hat{k} - k_{true}) \\right] J_i(k_{true}) \\approx 0$$\nDistributing the summation and rearranging to solve for the estimation error, $\\hat{k} - k_{true}$:\n$$\\sum_{i=1}^{N} \\varepsilon_i J_i(k_{true}) \\approx \\sum_{i=1}^{N} J_i(k_{true})^2 (\\hat{k} - k_{true})$$\n$$(\\hat{k} - k_{true}) \\sum_{i=1}^{N} J_i(k_{true})^2 \\approx \\sum_{i=1}^{N} \\varepsilon_i J_i(k_{true})$$\nThis leads to an approximate expression for the estimation error:\n$$\\hat{k} - k_{true} \\approx \\frac{\\sum_{i=1}^{N} \\varepsilon_i J_i(k_{true})}{\\sum_{i=1}^{N} J_i(k_{true})^2}$$\nThe asymptotic variance of $\\hat{k}$ is the variance of this expression. Since $k_{true}$ is a constant, $\\text{Var}(\\hat{k}) = \\text{Var}(\\hat{k} - k_{true})$. The denominator, $\\sum_{i=1}^{N} J_i(k_{true})^2$, is a constant with respect to the random noise variables $\\varepsilon_i$.\n$$\\text{Var}(\\hat{k}) \\approx \\text{Var}\\left( \\frac{\\sum_{i=1}^{N} \\varepsilon_i J_i(k_{true})}{\\sum_{i=1}^{N} J_i(k_{true})^2} \\right) = \\frac{1}{\\left(\\sum_{i=1}^{N} J_i(k_{true})^2\\right)^2} \\text{Var}\\left(\\sum_{i=1}^{N} \\varepsilon_i J_i(k_{true})\\right)$$\nSince the noise terms $\\varepsilon_i$ are independent, the variance of the sum is the sum of the variances:\n$$\\text{Var}\\left(\\sum_{i=1}^{N} \\varepsilon_i J_i(k_{true})\\right) = \\sum_{i=1}^{N} \\text{Var}(\\varepsilon_i J_i(k_{true}))$$\nUsing the property $\\text{Var}(aX) = a^2\\text{Var}(X)$, where $J_i(k_{true})$ is a constant factor for each term:\n$$\\sum_{i=1}^{N} \\text{Var}(\\varepsilon_i J_i(k_{true})) = \\sum_{i=1}^{N} J_i(k_{true})^2 \\text{Var}(\\varepsilon_i)$$\nWe are given that $\\text{Var}(\\varepsilon_i) = \\sigma^2$ for all $i$.\n$$\\sum_{i=1}^{N} J_i(k_{true})^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^{N} J_i(k_{true})^2$$\nSubstituting this back into the expression for $\\text{Var}(\\hat{k})$:\n$$\\text{Var}(\\hat{k}) \\approx \\frac{\\sigma^2 \\sum_{i=1}^{N} J_i(k_{true})^2}{\\left(\\sum_{i=1}^{N} J_i(k_{true})^2\\right)^2} = \\frac{\\sigma^2}{\\sum_{i=1}^{N} J_i(k_{true})^2}$$\nFinally, we substitute the expression for $J_i(k) = -c_0 t_i \\exp(-k t_i)$ and represent the true value $k_{true}$ simply as $k$, as requested for the final expression.\n$$J_i(k)^2 = (-c_0 t_i \\exp(-k t_i))^2 = c_0^2 t_i^2 \\exp(-2 k t_i)$$\nThe asymptotic variance of the estimator $\\hat{k}$ is therefore:\n$$\\text{Var}(\\hat{k}) \\approx \\frac{\\sigma^2}{\\sum_{i=1}^{N} c_0^2 t_i^2 \\exp(-2 k t_i)}$$\nThis is the required closed-form analytic expression for the asymptotic variance as a function of $k$, $c_0$, $\\{t_i\\}$, and $\\sigma^2$.",
            "answer": "$$\\boxed{\\frac{\\sigma^2}{\\sum_{i=1}^{N} c_0^2 t_i^2 \\exp(-2 k t_i)}}$$"
        },
        {
            "introduction": "A critical step in process modeling is deciding on the right level of complexity; for example, whether a key process parameter like gain should be treated as a fixed constant or a randomly varying quantity. This practice introduces the Likelihood Ratio Test, a powerful statistical tool for formally comparing the goodness-of-fit of a simpler (deterministic) model nested within a more complex (stochastic) one . You will gain hands-on experience in using data to make principled decisions between competing model structures, a fundamental skill in advanced process modeling.",
            "id": "4119783",
            "problem": "Consider a discrete run-to-run process model for a single-input single-output semiconductor manufacturing step. Let the measured output sequence be $\\{y_t\\}_{t=1}^T$ and the known recipe/input sequence be $\\{u_t\\}_{t=1}^T$, where $T$ is a positive integer. The process is modeled as $y_t = g_t u_t + e_t$, where $g_t$ is a (possibly time-varying) gain and $e_t$ is measurement noise. Assume Gaussian noise with $e_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma_e^2)$, and that $\\sigma_e^2$ is known and positive.\n\nTwo rival models are to be compared:\n- Deterministic constant gain (null hypothesis): $\\mathcal{H}_0: g_t = g$ for all $t$, where $g$ is an unknown constant parameter.\n- Stochastic time-varying gain (alternative hypothesis): $\\mathcal{H}_1: g_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(g,\\sigma_g^2)$ with $\\sigma_g^2 \\ge 0$, independent of $\\{e_t\\}$, and $g$ is an unknown constant mean gain.\n\nStarting only from the following fundamental bases:\n- The Gaussian probability density function for a scalar random variable and its logarithm.\n- Independence implying factorization of joint densities and additivity of log-likelihoods.\n- Basic rules of differentiation and optimization for maximum likelihood estimation.\n\nPerform the following tasks:\n1. Derive the marginal distribution of $y_t$ conditioned on $u_t$ under $\\mathcal{H}_1$ by integrating out the latent $g_t$, and express the log-likelihood under $\\mathcal{H}_1$ in terms of $g$ and $\\sigma_g^2$.\n2. Show that, for a fixed $\\sigma_g^2$, maximizing the log-likelihood under $\\mathcal{H}_1$ with respect to $g$ yields a weighted least squares estimator $\\hat{g}(\\sigma_g^2)$, and give its explicit form in terms of $\\{u_t\\}$, $\\{y_t\\}$, $\\sigma_e^2$, and $\\sigma_g^2$.\n3. Express the profile log-likelihood under $\\mathcal{H}_1$ as a function of $\\sigma_g^2$ only by substituting $\\hat{g}(\\sigma_g^2)$ and explain why a one-dimensional numerical maximization over $\\sigma_g^2 \\ge 0$ is appropriate.\n4. Derive the maximum likelihood estimates under $\\mathcal{H}_0$ and write the log-likelihood at the maximizer.\n5. Define the Likelihood Ratio Test (LRT) statistic\n   $$\\Lambda = -2\\left(\\ell_{\\mathcal{H}_0}(\\hat{g}_0) - \\max_{\\sigma_g^2 \\ge 0}\\ \\ell_{\\mathcal{H}_1}(\\hat{g}(\\sigma_g^2),\\sigma_g^2)\\right),$$\n   where $\\ell_{\\mathcal{H}_0}$ and $\\ell_{\\mathcal{H}_1}$ denote the log-likelihood functions under $\\mathcal{H}_0$ and $\\mathcal{H}_1$, respectively, and $\\hat{g}_0$ is the maximizer under $\\mathcal{H}_0$. Using the theory of parameters on the boundary, specify the asymptotic null distribution of $\\Lambda$ and a correct $p$-value formula for testing $\\mathcal{H}_0$ versus $\\mathcal{H}_1$ at level $\\alpha \\in (0,1)$.\n6. Implement a program that, for each test case in the test suite below, computes $\\Lambda$ and returns a boolean decision to reject $\\mathcal{H}_0$ if the $p$-value is strictly less than $\\alpha$.\n\nAll computations are dimensionless; no physical units are required.\n\nUse the following test suite. Each test case provides $(\\{u_t\\}_{t=1}^T, \\{y_t\\}_{t=1}^T, \\sigma_e, \\alpha)$:\n- Case A (clear time-variation expected): $T = 12$, $u = [1.00, 0.80, 1.20, 1.10, 0.90, 1.30, 0.70, 1.50, 1.00, 1.40, 0.95, 1.05]$, $y = [1.56, 1.555, 2.04, 2.153, 1.481, 2.667, 1.047, 3.306, 1.793, 2.803, 1.518, 1.999]$, $\\sigma_e = 0.05$, $\\alpha = 0.05$.\n- Case B (approximately constant gain): $T = 8$, $u = [1.0, 0.9, 1.1, 1.2, 0.8, 1.3, 0.7, 1.4]$, $y = [1.805, 1.616, 1.983, 2.154, 1.442, 2.337, 1.261, 2.524]$, $\\sigma_e = 0.05$, $\\alpha = 0.05$.\n- Case C (small sample with a zero input): $T = 4$, $u = [1.0, 0.0, 1.5, 0.5]$, $y = [1.71, -0.02, 2.865, 0.795]$, $\\sigma_e = 0.05$, $\\alpha = 0.05$.\n\nRequirements for the final output:\n- For each test case, compute the LRT statistic $\\Lambda$ and the boolean decision to reject $\\mathcal{H}_0$ using the asymptotic null distribution appropriate for a parameter on the boundary.\n- The program must output a single line containing a JSON-like list of length equal to the number of test cases. Each element must be a two-element list $[\\Lambda,\\text{decision}]$, where $\\Lambda$ is a floating-point number rounded to exactly six digits after the decimal point, and $\\text{decision}$ is a boolean.\n- The exact output format is: a single line with no additional text, for example $[[2.000000,True],[0.000000,False],[1.234567,False]]$.",
            "solution": "The problem requires the derivation and application of a likelihood ratio test (LRT) to distinguish between a deterministic process model ($\\mathcal{H}_0$) and a stochastic one ($\\mathcal{H}_1$) for a semiconductor manufacturing step. The validation confirms that the problem is well-posed, scientifically grounded, and provides all necessary information. We proceed with the derivation as requested.\n\n### 1. Marginal Distribution and Log-Likelihood under $\\mathcal{H}_1$\n\nUnder the alternative hypothesis $\\mathcal{H}_1$, the process is described by the equations:\n$$y_t = g_t u_t + e_t$$\nwhere $g_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(g, \\sigma_g^2)$ and $e_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma_e^2)$. The random variables $g_t$ and $e_t$ are independent. The values of $u_t$ are known constants for each run $t$.\n\nTo find the marginal distribution of $y_t$ conditioned on $u_t$, we integrate out the latent variable $g_t$. Since $g_t$ is Gaussian, the term $g_t u_t$ is also a Gaussian random variable: $g_t u_t \\sim \\mathcal{N}(g u_t, u_t^2 \\sigma_g^2)$. The output $y_t$ is the sum of two independent Gaussian random variables, $g_t u_t$ and $e_t$. The sum of independent Gaussian random variables is itself Gaussian.\n\nThe mean of $y_t$ is the sum of the means:\n$$E[y_t | u_t] = E[g_t u_t] + E[e_t] = g u_t + 0 = g u_t$$\nThe variance of $y_t$ is the sum of the variances due to independence:\n$$\\text{Var}(y_t | u_t) = \\text{Var}(g_t u_t) + \\text{Var}(e_t) = u_t^2 \\text{Var}(g_t) + \\sigma_e^2 = u_t^2 \\sigma_g^2 + \\sigma_e^2$$\nThus, the marginal distribution of $y_t$ conditioned on $u_t$ is:\n$$y_t | u_t \\sim \\mathcal{N}(g u_t, u_t^2 \\sigma_g^2 + \\sigma_e^2)$$\n\nThe probability density function (PDF) for a single observation $y_t$ is:\n$$p(y_t | u_t; g, \\sigma_g^2) = \\frac{1}{\\sqrt{2\\pi(u_t^2 \\sigma_g^2 + \\sigma_e^2)}} \\exp\\left(-\\frac{(y_t - g u_t)^2}{2(u_t^2 \\sigma_g^2 + \\sigma_e^2)}\\right)$$\nThe corresponding log-likelihood for observation $y_t$ is:\n$$\\ell_t(g, \\sigma_g^2) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(u_t^2 \\sigma_g^2 + \\sigma_e^2) - \\frac{(y_t - g u_t)^2}{2(u_t^2 \\sigma_g^2 + \\sigma_e^2)}$$\nSince the observations $(y_1, \\dots, y_T)$ are independent, the total log-likelihood for the sequence is the sum of individual log-likelihoods:\n$$\\ell_{\\mathcal{H}_1}(g, \\sigma_g^2) = \\sum_{t=1}^T \\ell_t(g, \\sigma_g^2) = -\\frac{T}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{t=1}^T \\log(u_t^2 \\sigma_g^2 + \\sigma_e^2) - \\frac{1}{2}\\sum_{t=1}^T \\frac{(y_t - g u_t)^2}{u_t^2 \\sigma_g^2 + \\sigma_e^2}$$\n\n### 2. Maximum Likelihood Estimator for $g$ under $\\mathcal{H}_1$\n\nTo find the maximum likelihood estimator (MLE) for $g$ for a fixed $\\sigma_g^2$, we differentiate $\\ell_{\\mathcal{H}_1}(g, \\sigma_g^2)$ with respect to $g$ and set the result to zero. The terms not involving $g$ can be ignored.\n$$\\frac{\\partial \\ell_{\\mathcal{H}_1}}{\\partial g} = \\frac{\\partial}{\\partial g} \\left( -\\frac{1}{2}\\sum_{t=1}^T \\frac{(y_t - g u_t)^2}{u_t^2 \\sigma_g^2 + \\sigma_e^2} \\right) = -\\frac{1}{2}\\sum_{t=1}^T \\frac{2(y_t - g u_t)(-u_t)}{u_t^2 \\sigma_g^2 + \\sigma_e^2} = \\sum_{t=1}^T \\frac{u_t y_t - g u_t^2}{u_t^2 \\sigma_g^2 + \\sigma_e^2}$$\nSetting the derivative to zero yields:\n$$\\sum_{t=1}^T \\frac{u_t y_t}{u_t^2 \\sigma_g^2 + \\sigma_e^2} = g \\sum_{t=1}^T \\frac{u_t^2}{u_t^2 \\sigma_g^2 + \\sigma_e^2}$$\nSolving for $g$ gives the estimator $\\hat{g}(\\sigma_g^2)$:\n$$\\hat{g}(\\sigma_g^2) = \\frac{\\sum_{t=1}^T \\frac{u_t y_t}{u_t^2 \\sigma_g^2 + \\sigma_e^2}}{\\sum_{t=1}^T \\frac{u_t^2}{u_t^2 \\sigma_g^2 + \\sigma_e^2}}$$\nThis is a weighted least squares (WLS) estimator. Maximizing the log-likelihood with respect to $g$ is equivalent to minimizing the weighted sum of squared residuals $\\sum_{t=1}^T w_t (y_t - g u_t)^2$, where the weights $w_t$ are the inverse of the variances of the observations: $w_t = (u_t^2 \\sigma_g^2 + \\sigma_e^2)^{-1}$. The derived expression for $\\hat{g}(\\sigma_g^2)$ is precisely the solution to this WLS problem. Note that if any $u_t=0$, the corresponding term is zero in both the numerator and denominator sums, indicating that such points correctly provide no information for estimating $g$.\n\n### 3. Profile Log-Likelihood for $\\sigma_g^2$\n\nThe profile log-likelihood for $\\sigma_g^2$, denoted $\\ell_P(\\sigma_g^2)$, is obtained by substituting the estimator $\\hat{g}(\\sigma_g^2)$ back into the full log-likelihood function $\\ell_{\\mathcal{H}_1}$:\n$$\\ell_P(\\sigma_g^2) = \\ell_{\\mathcal{H}_1}(\\hat{g}(\\sigma_g^2), \\sigma_g^2)$$\n$$\\ell_P(\\sigma_g^2) = C - \\frac{1}{2}\\sum_{t=1}^T \\log(u_t^2 \\sigma_g^2 + \\sigma_e^2) - \\frac{1}{2}\\sum_{t=1}^T \\frac{(y_t - \\hat{g}(\\sigma_g^2) u_t)^2}{u_t^2 \\sigma_g^2 + \\sigma_e^2}$$\nwhere $C = -T/2 \\log(2\\pi)$. The analytic maximization of this function with respect to $\\sigma_g^2$ is intractable because $\\sigma_g^2$ appears in a complex, non-linear fashion. The standard and appropriate method is to find the maximum likelihood estimate for $\\sigma_g^2$ numerically. This reduces the two-dimensional optimization over $(g, \\sigma_g^2)$ to a more manageable one-dimensional numerical search over the valid parameter range for the variance, which is $\\sigma_g^2 \\ge 0$.\n\n### 4. Maximum Likelihood Estimation under $\\mathcal{H}_0$\n\nUnder the null hypothesis $\\mathcal{H}_0$, the gain is constant, $g_t = g$, which is equivalent to setting $\\sigma_g^2 = 0$ in the $\\mathcal{H}_1$ model. The model becomes $y_t = g u_t + e_t$, where $e_t \\sim \\mathcal{N}(0, \\sigma_e^2)$. This is a standard linear regression model through the origin with known error variance.\nThe log-likelihood function is:\n$$\\ell_{\\mathcal{H}_0}(g) = -\\frac{T}{2}\\log(2\\pi\\sigma_e^2) - \\frac{1}{2\\sigma_e^2}\\sum_{t=1}^T(y_t - g u_t)^2$$\nMaximizing this with respect to $g$ is equivalent to minimizing the sum of squared residuals, $\\sum(y_t - g u_t)^2$. This is an ordinary least squares (OLS) problem. The derivative is:\n$$\\frac{\\partial \\ell_{\\mathcal{H}_0}}{\\partial g} = \\frac{1}{\\sigma_e^2} \\sum_{t=1}^T (u_t y_t - g u_t^2)$$\nSetting this to zero and solving for $g$ gives the MLE $\\hat{g}_0$:\n$$\\hat{g}_0 = \\frac{\\sum_{t=1}^T u_t y_t}{\\sum_{t=1}^T u_t^2}$$\nThe maximized log-likelihood under $\\mathcal{H}_0$ is found by substituting $\\hat{g}_0$ back into $\\ell_{\\mathcal{H}_0}(g)$:\n$$\\ell_{\\mathcal{H}_0}(\\hat{g}_0) = -\\frac{T}{2}\\log(2\\pi\\sigma_e^2) - \\frac{1}{2\\sigma_e^2}\\sum_{t=1}^T(y_t - \\hat{g}_0 u_t)^2$$\n\n### 5. Likelihood Ratio Test Statistic and Asymptotic Distribution\n\nThe LRT statistic $\\Lambda$ is defined as twice the difference between the maximized log-likelihood under the alternative hypothesis $\\mathcal{H}_1$ and the null hypothesis $\\mathcal{H}_0$:\n$$\\Lambda = -2\\left(\\ell_{\\mathcal{H}_0}(\\hat{g}_0) - \\max_{\\sigma_g^2 \\ge 0}\\ \\ell_{\\mathcal{H}_1}(\\hat{g}(\\sigma_g^2),\\sigma_g^2)\\right) = 2\\left(\\max_{\\sigma_g^2 \\ge 0} \\ell_P(\\sigma_g^2) - \\ell_{\\mathcal{H}_0}(\\hat{g}_0) \\right)$$\nThe null hypothesis $\\mathcal{H}_0$ corresponds to setting the parameter $\\sigma_g^2 = 0$ in the model $\\mathcal{H}_1$. Since variance cannot be negative, the parameter space for $\\sigma_g^2$ is $[0, \\infty)$, and the null value lies on the boundary of this space.\n\nAccording to the theory of hypothesis testing for parameters on the boundary (e.g., Chernoff, 1954; Self & Liang, 1987), Wilks' theorem must be adapted. For testing a single variance component, the asymptotic null distribution of $\\Lambda$ is a mixture of a point mass at zero and a chi-squared distribution with one degree of freedom:\n$$\\Lambda \\sim \\frac{1}{2}\\chi^2_0 + \\frac{1}{2}\\chi^2_1$$\nwhere $\\chi^2_0$ is a degenerate distribution with all its mass at $0$.\n\nThe $p$-value for an observed statistic $\\Lambda_{obs}$ is the probability of obtaining a test statistic at least as extreme as $\\Lambda_{obs}$ under the null hypothesis. For $\\Lambda_{obs} > 0$:\n$$p = P(\\Lambda \\ge \\Lambda_{obs}) = P(\\Lambda \\ge \\Lambda_{obs} | \\Lambda > 0) P(\\Lambda > 0) = P(\\chi^2_1 \\ge \\Lambda_{obs}) \\times \\frac{1}{2}$$\nIf $\\Lambda_{obs} = 0$, the MLE for $\\sigma_g^2$ is $0$, and the likelihoods are equal. The p-value is $0.5$. A more general formula for the p-value for $\\Lambda_{obs} \\ge 0$ is:\n$$p = \\frac{1}{2} P(\\chi^2_1 > \\Lambda_{obs})$$\nThe survival function of a $\\chi^2_1$ distribution can be expressed using the standard normal CDF, $\\Phi(z)$, or the error function, $\\text{erf}(z)$. The decision rule is to reject $\\mathcal{H}_0$ if $p < \\alpha$.\n\nNote that for any data points $(u_t, y_t)$ where $u_t=0$, the model under both hypotheses simplifies to $y_t=e_t$. The log-likelihood contribution from such a point is $-\\frac{1}{2}\\log(2\\pi\\sigma_e^2) - y_t^2/(2\\sigma_e^2)$, which is a constant that does not depend on $g$ or $\\sigma_g^2$. Therefore, these points add the same constant to $\\ell_{\\mathcal{H}_0}(\\hat{g}_0)$ and $\\ell_{\\mathcal{H}_1}(\\hat{g}(\\sigma_g^2), \\sigma_g^2)$, and cancel out in the LRT statistic $\\Lambda$. Consequently, it is valid to perform the analysis using only the subset of data for which $u_t \\neq 0$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"u\": np.array([1.00, 0.80, 1.20, 1.10, 0.90, 1.30, 0.70, 1.50, 1.00, 1.40, 0.95, 1.05]),\n            \"y\": np.array([1.56, 1.555, 2.04, 2.153, 1.481, 2.667, 1.047, 3.306, 1.793, 2.803, 1.518, 1.999]),\n            \"sigma_e\": 0.05,\n            \"alpha\": 0.05\n        },\n        {\n            \"u\": np.array([1.0, 0.9, 1.1, 1.2, 0.8, 1.3, 0.7, 1.4]),\n            \"y\": np.array([1.805, 1.616, 1.983, 2.154, 1.442, 2.337, 1.261, 2.524]),\n            \"sigma_e\": 0.05,\n            \"alpha\": 0.05\n        },\n        {\n            \"u\": np.array([1.0, 0.0, 1.5, 0.5]),\n            \"y\": np.array([1.71, -0.02, 2.865, 0.795]),\n            \"sigma_e\": 0.05,\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        u, y, sigma_e, alpha = case[\"u\"], case[\"y\"], case[\"sigma_e\"], case[\"alpha\"]\n\n        # Filter out data points where u_t = 0, as they don't contribute to the LRT statistic\n        nonzero_idx = u != 0\n        u_nz = u[nonzero_idx]\n        y_nz = y[nonzero_idx]\n        T_nz = len(u_nz)\n        sigma_e_sq = sigma_e**2\n\n        # 1. Calculation under H0 (null hypothesis: g_t = g)\n        g_hat_0 = np.sum(u_nz * y_nz) / np.sum(u_nz**2)\n        ssr_0 = np.sum((y_nz - g_hat_0 * u_nz)**2)\n        log_lik_h0 = -T_nz / 2.0 * np.log(2 * np.pi * sigma_e_sq) - ssr_0 / (2 * sigma_e_sq)\n\n        # 2. Calculation under H1 (alternative hypothesis: g_t ~ N(g, sigma_g^2))\n        # We need to maximize the profile log-likelihood w.r.t. sigma_g^2\n        def profile_log_likelihood(sigma_g_sq):\n            # Ensure sigma_g_sq is non-negative\n            sigma_g_sq = max(0.0, sigma_g_sq)\n            \n            # Variance and weights for each observation under H1\n            var_y = u_nz**2 * sigma_g_sq + sigma_e_sq\n            weights = 1.0 / var_y\n\n            # WLS estimate for g given sigma_g^2\n            g_hat = np.sum(weights * u_nz * y_nz) / np.sum(weights * u_nz**2)\n            \n            # Profile log-likelihood\n            ssr_1 = np.sum(weights * (y_nz - g_hat * u_nz)**2)\n            log_det_term = np.sum(np.log(var_y))\n            \n            log_lik = -T_nz / 2.0 * np.log(2 * np.pi) - 0.5 * log_det_term - 0.5 * ssr_1\n            return log_lik\n\n        # Minimize the negative profile log-likelihood to find its maximum\n        # Use a bounded optimizer for sigma_g^2 >= 0\n        opt_result = minimize_scalar(lambda s: -profile_log_likelihood(s),\n                                       bounds=(0, 100.0), method='bounded')\n        \n        # Max log-likelihood under H1 is the negative of the minimized value\n        max_log_lik_h1 = -opt_result.fun\n\n        # 3. Compute LRT statistic Lambda\n        lrt_statistic = 2 * (max_log_lik_h1 - log_lik_h0)\n        # Clamp to zero to handle potential small negative values from numerical precision\n        lrt_statistic = max(0.0, lrt_statistic)\n\n        # 4. Compute p-value and make decision\n        # The null distribution of Lambda is a 0.5*chi_0^2 + 0.5*chi_1^2 mixture.\n        # p_val = 0.5 * P(chi_1^2 > Lambda)\n        # Using scipy.stats.chi2.sf (survival function) is safer than erf for numerical stability\n        from scipy.stats import chi2\n        p_value = 0.5 * chi2.sf(lrt_statistic, 1)\n        \n        decision = bool(p_value  alpha)\n        \n        results.append((lrt_statistic, decision))\n    \n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        lambda_val, decision_val = res\n        formatted_results.append(f\"[{lambda_val:.6f},{'True' if decision_val else 'False'}]\")\n    \n    return f\"[{','.join(formatted_results)}]\"\n\nprint(solve())\n```"
        },
        {
            "introduction": "For real-time process control, we often need to estimate unmeasured variables like the instantaneous etch rate. Estimators for this task can be designed based on purely deterministic principles or by explicitly accounting for stochastic disturbances . This exercise contrasts the design philosophy of a Luenberger observer with that of a Kalman filter, allowing you to quantitatively grasp the performance benefits of using a stochastic framework for state estimation in noisy environments.",
            "id": "4119816",
            "problem": "A single-wafer plasma etch step is monitored in real time to estimate the instantaneous etch rate. Let the true etch rate be modeled as a slowly varying scalar state in discrete time with sampling period $T_{s}$, governed by the random-walk model\n$$\nx_{k+1} = x_{k} + w_{k}, \\quad y_{k} = x_{k} + v_{k},\n$$\nwhere $x_{k}$ is the etch rate at sample $k$, $y_{k}$ is an in situ metrology measurement, $w_{k} \\sim \\mathcal{N}(0,Q)$ is zero-mean independent process noise modeling unmodeled disturbances and drift, and $v_{k} \\sim \\mathcal{N}(0,R)$ is zero-mean independent measurement noise. Assume $\\{w_{k}\\}$ and $\\{v_{k}\\}$ are mutually independent, independent across time, and independent of the initial condition $x_{0}$. The sampling period is $T_{s} = 1\\,\\mathrm{min}$, and the variances are $Q = (0.5\\,\\mathrm{nm/min})^{2}$ and $R = (2.0\\,\\mathrm{nm/min})^{2}$.\n\nTwo estimators are designed for $x_{k}$:\n- A deterministic Luenberger observer (LO) that does not explicitly model noise, with update\n$$\n\\hat{x}_{k+1} = \\hat{x}_{k} + L\\left(y_{k} - \\hat{x}_{k}\\right),\n$$\nwhere the observer gain $L$ is chosen to place the noise-free error dynamics pole to match a desired discrete-time pole $p_{d} = \\exp\\!\\left(-T_{s}/\\tau_{d}\\right)$ with design time constant $\\tau_{d} = 2\\,\\mathrm{min}$.\n- A stochastic Kalman Filter (KF) designed under the given $(Q,R)$.\n\nUsing first principles of linear state estimation and covariance propagation, derive the steady-state estimation error variance of the LO under the true noisy dynamics, and the steady-state error variance of the KF. Then compute the ratio of these steady-state variances,\n$$\n\\rho \\equiv \\frac{P_{\\mathrm{LO}}}{P_{\\mathrm{KF}}},\n$$\nas a dimensionless number. Round your final answer for $\\rho$ to four significant figures.",
            "solution": "We begin from the discrete-time state-space model\n$$\nx_{k+1} = x_{k} + w_{k}, \\quad y_{k} = x_{k} + v_{k},\n$$\nwith $w_{k}\\sim \\mathcal{N}(0,Q)$, $v_{k}\\sim \\mathcal{N}(0,R)$, mutually independent, and independent across time. The sampling period is $T_{s} = 1\\,\\mathrm{min}$, the process noise variance is $Q = (0.5\\,\\mathrm{nm/min})^{2} = 0.25\\,(\\mathrm{nm/min})^{2}$, and the measurement noise variance is $R = (2.0\\,\\mathrm{nm/min})^{2} = 4.0\\,(\\mathrm{nm/min})^{2}$.\n\n**Luenberger Observer (LO)**\nThe observer update is\n$$\n\\hat{x}_{k+1} = \\hat{x}_{k} + L\\left(y_{k} - \\hat{x}_{k}\\right).\n$$\nDefine the estimation error $e_{k} \\equiv x_{k} - \\hat{x}_{k}$. The true dynamics and observer imply\n$$\ne_{k+1} = x_{k+1} - \\hat{x}_{k+1} = (x_{k} + w_{k}) - \\left[\\hat{x}_{k} + L\\left(y_{k} - \\hat{x}_{k}\\right)\\right]\n= (1 - L)(x_{k} - \\hat{x}_{k}) + w_{k} - L v_{k} = (1 - L)e_{k} + w_{k} - L v_{k}.\n$$\nAssuming $\\{e_{k}\\}$, $\\{w_{k}\\}$, and $\\{v_{k}\\}$ are jointly wide-sense stationary in steady state and that $w_{k}$ and $v_{k}$ are independent of $e_{k}$, the error variance $P_{k} \\equiv \\mathbb{E}[e_{k}^{2}]$ obeys the discrete Lyapunov equation:\n$$\nP_{k+1} = (1 - L)^{2} P_{k} + \\mathbb{E}[w_{k}^{2}] + L^{2} \\mathbb{E}[v_{k}^{2}] = (1 - L)^{2} P_{k} + Q + L^{2} R.\n$$\nAt steady state $P_{k+1} = P_{k} = P_{\\mathrm{LO}}$, which yields\n$$\nP_{\\mathrm{LO}} = (1 - L)^{2} P_{\\mathrm{LO}} + Q + L^{2} R\n\\quad \\Longrightarrow \\quad\nP_{\\mathrm{LO}} = \\frac{Q + L^{2} R}{1 - (1 - L)^{2}},\n$$\nfor $|1 - L|  1$ so that the denominator is positive, which holds for $0  L  2$.\n\nThe gain $L$ is chosen deterministically to place the noise-free error dynamics pole. In the absence of noise, $e_{k+1} = (1 - L)e_{k}$. The design requirement is a target pole\n$$\np_{d} = \\exp\\!\\left(-\\frac{T_{s}}{\\tau_{d}}\\right),\n$$\nhence\n$$\n1 - L = p_{d} \\quad \\Longrightarrow \\quad L = 1 - \\exp\\!\\left(-\\frac{T_{s}}{\\tau_{d}}\\right).\n$$\nWith $T_{s} = 1$ and $\\tau_{d} = 2$, we have\n$$\nL = 1 - \\exp\\!\\left(-\\frac{1}{2}\\right) \\approx 0.393469.\n$$\nTherefore,\n$$\nP_{\\mathrm{LO}} = \\frac{Q + L^{2} R}{1 - (1 - L)^{2}} = \\frac{Q + (1 - e^{-1/2})^{2} R}{1 - e^{-1}}.\n$$\nSubstituting $Q = 0.25$ and $R = 4.0$,\n$$\nP_{\\mathrm{LO}} = \\frac{0.25 + (1 - e^{-1/2})^{2} \\times 4.0}{1 - e^{-1}} \\approx \\frac{0.25 + (0.393469)^2 \\times 4.0}{1 - 0.367879}\n\\approx \\frac{0.25 + 0.61927}{0.632121} \\approx 1.37517.\n$$\n\n**Stochastic Kalman Filter (KF)**\nFor the scalar system with state transition matrix $F = 1$ and measurement matrix $H = 1$, the steady-state a priori covariance satisfies $P^{-} = P + Q$, the steady-state Kalman gain is\n$$\nK = \\frac{P^{-}}{P^{-} + R} = \\frac{P + Q}{P + Q + R},\n$$\nand the steady-state a posteriori covariance is\n$$\nP = (1 - K) P^{-} = \\left(1 - \\frac{P + Q}{P + Q + R}\\right) (P + Q) = \\frac{R}{P + Q + R} (P + Q).\n$$\nThis yields the algebraic Riccati equation\n$$\nP(P + Q + R) = R(P + Q) \\quad \\Longrightarrow \\quad P^{2} + Q P - Q R = 0.\n$$\nSolving the quadratic for $P \\ge 0$ gives\n$$\nP_{\\mathrm{KF}} = \\frac{-Q + \\sqrt{Q^{2} + 4 Q R}}{2}.\n$$\nSubstituting $Q = 0.25$ and $R = 4.0$,\n$$\nP_{\\mathrm{KF}} = \\frac{-0.25 + \\sqrt{0.25^{2} + 4(0.25)(4.0)}}{2}\n= \\frac{-0.25 + \\sqrt{0.0625 + 4}}{2}\n= \\frac{-0.25 + \\sqrt{4.0625}}{2}\n\\approx \\frac{-0.25 + 2.01556}{2} \\approx 0.88278.\n$$\nThe exact form is $P_{\\mathrm{KF}} = \\frac{-0.25 + \\sqrt{65/16}}{2} = \\frac{-0.25 + \\sqrt{65}/4}{2} = \\frac{\\sqrt{65}-1}{8}$.\n\n**Ratio of Variances**\nFinally, the requested ratio is\n$$\n\\rho \\equiv \\frac{P_{\\mathrm{LO}}}{P_{\\mathrm{KF}}} \\approx \\frac{1.37517}{0.88278} \\approx 1.55776\\ldots\n$$\nRounded to four significant figures,\n$$\n\\rho \\approx 1.558.\n$$\nThus, under the specified $Q$ and $R$ and the chosen deterministic pole, the deterministic Luenberger observerâ€™s steady-state error variance is about 56% larger than that of the optimal stochastic Kalman Filter.",
            "answer": "$$\\boxed{1.558}$$"
        }
    ]
}