{
    "hands_on_practices": [
        {
            "introduction": "Before embarking on any large-scale Monte Carlo simulation, a critical first step is to estimate the computational resources required. This exercise provides a foundational practice in simulation planning: deriving the minimum sample size $N$ needed to achieve a desired level of statistical precision. By connecting the confidence level, estimator variance, and target mean to the required number of samples, you will gain insight into how to budget for and design effective Monte Carlo studies in semiconductor process modeling .",
            "id": "4143286",
            "problem": "In a semiconductor manufacturing process modeling task for stochastic defect prediction during extreme ultraviolet lithography, let a high-dimensional random vector $\\mathbf{X} \\in \\mathbb{R}^{d}$ encode wafer-level process variations (line-edge roughness parameters, resist blur, stochastic photon shot noise surrogates, etch bias parameters), and let the scalar response $Y = g(\\mathbf{X})$ denote the predicted count of stochastic microbridges per unit area. You use plain Monte Carlo (MC) to estimate the mean $\\mu = \\mathbb{E}[Y]$ by the sample mean $\\bar{Y}_{N}$ from $N$ independent and identically distributed realizations. Assume $Y$ has finite variance $\\sigma^{2} = \\operatorname{Var}(Y)$, and that a preliminary pilot study yields a reliable planning value for $\\sigma^{2}$. You require a two-sided confidence interval for $\\mu$ with confidence level $1-\\alpha$ whose half-width is at most a prescribed relative tolerance $\\varepsilon \\mu$, where $\\varepsilon \\in (0,1)$ and $\\alpha \\in (0,1)$ are given.\n\nStarting from the Central Limit Theorem (CLT) and the normal-quantile-based confidence interval for the sample mean, derive an explicit closed-form expression for the minimal $N$ (ignoring integer constraints) in terms of the upper $\\alpha/2$ standard normal quantile $z_{\\alpha/2}$, the variance proxy $\\sigma^{2}$, the relative tolerance $\\varepsilon$, and the true mean $\\mu$. Express your final answer as a single simplified analytical expression. No units are required.\n\nFinally, discuss, without numerical computation, the practicality of this requirement when calibrating $g$ in high-dimensional parameter spaces (large $d$) common in semiconductor process model calibration, including the implications when $|\\mu|$ is small, and how variance reduction could alter the required $N$.\n\nYour final reported result must be the single closed-form expression for $N$. Do not round or approximate this expression.",
            "solution": "The problem requires the derivation of a closed-form expression for the minimum sample size $N$ needed for a plain Monte Carlo estimation of a mean $\\mu = \\mathbb{E}[Y]$ to achieve a specified relative precision.\n\nThe validation of the problem statement is performed first.\nStep 1: Extract Givens.\n- The random vector is $\\mathbf{X} \\in \\mathbb{R}^{d}$.\n- The scalar response is $Y = g(\\mathbf{X})$.\n- The quantity to be estimated is the mean $\\mu = \\mathbb{E}[Y]$.\n- The estimator is the sample mean $\\bar{Y}_{N}$ from $N$ independent and identically distributed realizations.\n- The variance of the response is $\\sigma^{2} = \\operatorname{Var}(Y)$, which is finite and has a known planning value.\n- A two-sided confidence interval for $\\mu$ is required.\n- The confidence level is $1-\\alpha$, with $\\alpha \\in (0,1)$.\n- The half-width of the confidence interval, denoted $H$, must be at most $\\varepsilon \\mu$.\n- The relative tolerance is $\\varepsilon \\in (0,1)$.\n- The derivation is to start from the Central Limit Theorem (CLT) and the normal-quantile-based confidence interval.\n- The derived expression for $N$ should be in terms of the upper $\\alpha/2$ standard normal quantile $z_{\\alpha/2}$, $\\sigma^{2}$, $\\varepsilon$, and $\\mu$.\n- The integer constraint on $N$ is to be ignored.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, as it is a standard application of statistical theory (Central Limit Theorem, confidence intervals, sample size calculation) to a well-defined problem in computational science and engineering (Monte Carlo simulation for process modeling). The problem is well-posed, providing all necessary information to derive the required expression. It is stated objectively and contains no contradictory or incomplete information. The problem is a formalizable and relevant exercise in the specified domain. Therefore, the problem is deemed valid.\n\nStep 3: Verdict and Action.\nThe problem is valid. The solution will now be derived.\n\nAccording to the Central Limit Theorem, for a sufficiently large sample size $N$, the distribution of the sample mean $\\bar{Y}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$ approximates a normal distribution. Specifically, given that the individual samples $Y_i$ are independent and identically distributed with mean $\\mu$ and variance $\\sigma^2$, the sample mean $\\bar{Y}_{N}$ is approximately distributed as:\n$$\n\\bar{Y}_{N} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{N}\\right)\n$$\nTo construct a confidence interval for $\\mu$, we standardize the random variable $\\bar{Y}_{N}$ to obtain a standard normal variable $Z$:\n$$\nZ = \\frac{\\bar{Y}_{N} - \\mu}{\\sigma / \\sqrt{N}} \\sim \\mathcal{N}(0, 1)\n$$\nA two-sided confidence interval with confidence level $1-\\alpha$ is defined by the interval that contains the true mean $\\mu$ with probability $1-\\alpha$. Using the standard normal distribution, this is expressed as:\n$$\nP\\left(-z_{\\alpha/2} \\le Z \\le z_{\\alpha/2}\\right) = 1-\\alpha\n$$\nwhere $z_{\\alpha/2}$ is the critical value from the standard normal distribution such that $P(Z > z_{\\alpha/2}) = \\alpha/2$. Substituting the expression for $Z$:\n$$\nP\\left(-z_{\\alpha/2} \\le \\frac{\\bar{Y}_{N} - \\mu}{\\sigma / \\sqrt{N}} \\le z_{\\alpha/2}\\right) = 1-\\alpha\n$$\nRearranging the inequality to isolate $\\mu$:\n$$\nP\\left(\\bar{Y}_{N} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}} \\le \\mu \\le \\bar{Y}_{N} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}}\\right) = 1-\\alpha\n$$\nThis gives the approximate $100(1-\\alpha)\\%$ confidence interval for $\\mu$ as $[\\bar{Y}_{N} - H, \\bar{Y}_{N} + H]$, where the half-width $H$ is:\n$$\nH = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}}\n$$\nThe problem specifies that this half-width must be at most a prescribed relative tolerance, $\\varepsilon \\mu$. This imposes the constraint:\n$$\nH \\le \\varepsilon \\mu\n$$\nSubstituting the expression for $H$ into the constraint gives:\n$$\nz_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}} \\le \\varepsilon \\mu\n$$\nTo find the minimal sample size $N$ that satisfies this condition, we solve the corresponding equality for $N$:\n$$\nz_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}} = \\varepsilon \\mu\n$$\nWe rearrange the equation to solve for $N$. First, we isolate $\\sqrt{N}$:\n$$\n\\sqrt{N} = \\frac{z_{\\alpha/2} \\sigma}{\\varepsilon \\mu}\n$$\nSquaring both sides yields the expression for the minimal sample size $N$, ignoring the integer constraint as requested:\n$$\nN = \\left(\\frac{z_{\\alpha/2} \\sigma}{\\varepsilon \\mu}\\right)^2\n$$\nThis can be written in its final simplified form as:\n$$\nN = \\frac{z_{\\alpha/2}^2 \\sigma^2}{\\varepsilon^2 \\mu^2}\n$$\nThis is the required closed-form expression for $N$.\n\nAs for the discussion on practicality:\nThe derived formula reveals a practical difficulty: $N$ depends on $\\mu$ and $\\sigma^2$, which are the unknown population parameters we wish to estimate. In practice, $\\sigma^2$ is often replaced with a planning value from a pilot study or a sample variance estimate $s^2_N$ from the data, and $\\mu$ is replaced by the sample mean $\\bar{Y}_N$. The latter introduces a circular dependency, but for sample size planning, a rough estimate of $\\mu$ from a pilot run is typically used.\n\nIn high-dimensional parameter spaces (large $d$), the function $g(\\mathbf{X})$ can be complex and computationally expensive to evaluate. More importantly, high dimensionality often leads to functions with large variance $\\sigma^2$, a phenomenon related to the curse of dimensionality. Since $N \\propto \\sigma^2$, a large variance necessitates a very large number of Monte Carlo samples, which, coupled with the high cost of each evaluation, can make the estimation computationally intractable.\n\nThe dependence $N \\propto 1/\\mu^2$ highlights a critical issue when $|\\mu|$ is small. For a fixed relative tolerance $\\varepsilon$, as the mean approaches zero, the required sample size $N$ tends to infinity. This makes estimating a near-zero quantity with high *relative* precision extremely difficult. For example, if predicting a very low defect count, the relative error criterion is often impractical. In such a scenario, an *absolute* error tolerance, $H \\le \\delta$, would be more appropriate, leading to $N = (z_{\\alpha/2} \\sigma / \\delta)^2$, a requirement independent of $\\mu$.\n\nVariance reduction techniques directly address the inefficiency of plain Monte Carlo. If a more advanced estimator (e.g., using control variates, importance sampling) has a variance $\\sigma_{\\text{new}}^2 < \\sigma^2$, the required sample size becomes $N_{\\text{new}} = N \\cdot (\\sigma_{\\text{new}}^2 / \\sigma^2)$. The reduction in sample size is proportional to the reduction in variance. For high-dimensional problems where $\\sigma^2$ is large, achieving a significant variance reduction is often the only feasible way to obtain a reliable estimate within a reasonable computational budget.",
            "answer": "$$\n\\boxed{\\frac{z_{\\alpha/2}^2 \\sigma^2}{\\varepsilon^2 \\mu^2}}\n$$"
        },
        {
            "introduction": "Beyond the sheer number of samples, the quality of their distribution significantly impacts the accuracy of Monte Carlo integration. This practice introduces star-discrepancy, a rigorous metric for quantifying how uniformly a set of points covers a given space. By implementing a grid-based calculation, you will empirically compare the uniformity of standard pseudorandom points against that of a low-discrepancy Sobol sequence, providing a tangible understanding of why Quasi-Monte Carlo (QMC) methods are often superior for high-dimensional integration tasks .",
            "id": "4143268",
            "problem": "Consider a lithography dose uniformity model in a semiconductor manufacturing process, where equal-weight exposure shots are planned over the unit square $[0,1]^2$ and the cumulative dose in any origin-anchored axis-aligned rectangle $[0,t_1)\\times[0,t_2)$ is assessed against the ideal uniform target. The uniformity of point sets is quantified by the star-discrepancy, which for a set of $N$ points $\\{x_i\\}_{i=1}^N \\subset [0,1]^2$ is defined as the supremum over $t\\in[0,1]^2$ of the deviation between the empirical distribution function and the uniform distribution: \n$$\nD_N^* \\equiv \\sup_{t=(t_1,t_2)\\in[0,1]^2} \\left| \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{x_i^{(1)} < t_1,\\, x_i^{(2)} < t_2\\} - t_1 t_2 \\right|.\n$$\nThis quantity is dimensionless and directly interpretable as the worst-case relative cumulative dose error over origin-anchored rectangles, assuming equal dose per shot and a uniform target dose proportional to area. In Monte Carlo and Quasi-Monte Carlo (QMC) sampling for process modeling, low-discrepancy sequences such as Sobol points are known to improve uniformity relative to pseudorandom points.\n\nYour task is to implement a grid-based empirical approximation to the star-discrepancy by replacing the supremum over $[0,1]^2$ with a supremum over a finite grid $\\mathcal{G}_m \\equiv \\{(i/m,j/m) : i,j \\in \\{1,2,\\dots,m\\}\\}$, for a given integer resolution $m\\ge 1$. For each grid node $(i/m,j/m)$, compute the empirical cumulative count in $[0,i/m)\\times[0,j/m)$ and its deviation from the area $(i/m)\\cdot(j/m)$, normalized by $N$, and take the maximum absolute deviation over $\\mathcal{G}_m$. This defines the empirical grid-based star-discrepancy approximation\n$$\n\\widehat{D}_{N,m}^* \\equiv \\max_{1\\le i,j\\le m} \\left| \\frac{1}{N}\\sum_{k=1}^N \\mathbf{1}\\left\\{x_k^{(1)}<\\frac{i}{m},\\, x_k^{(2)}<\\frac{j}{m}\\right\\} - \\frac{i}{m}\\cdot\\frac{j}{m} \\right|.\n$$\n\nStarting from the foundations of Monte Carlo sampling and the definition of star-discrepancy, and without using any shortcut formulas, implement the following:\n\n- Generate $N=256$ two-dimensional Sobol points (unscrambled) in $[0,1)^2$.\n- Generate $N=256$ two-dimensional pseudorandom points in $[0,1)^2$ for each of three independent seeds, and aggregate the comparison by taking the arithmetic mean of the three empirical grid-based star-discrepancies.\n- For each specified grid resolution $m$, compute the empirical grid-based star-discrepancy $\\widehat{D}_{N,m}^*$ for the Sobol set and the mean empirical grid-based star-discrepancy over the three pseudorandom sets.\n- Interpret each discrepancy as a dimensionless worst-case relative cumulative dose error under the equal-shot-dose lithography model described above.\n\nPhysical units are not required because the discrepancy and its interpretation in this task are dimensionless. Do not express any result using a percentage sign; all outputs must be pure decimals.\n\nUse the following test suite to ensure coverage of different facets:\n- A coarse grid resolution $m=4$ (boundary condition for grid-based supremum).\n- A moderately fine resolution $m=16$ (happy path).\n- A fine resolution $m=64$ (increased fidelity).\n- A very fine resolution $m=128$ (edge case stressing computational efficiency).\n\nFor all test cases, use the same three pseudorandom seeds $s\\in\\{101,202,303\\}$. The dimension is $2$ and the number of points is $N=256$ in every case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the four grid resolutions in the order $m\\in[4,16,64,128]$, output eight floats in the following order:\n$$\n[\\widehat{D}_{N,4}^{*,\\mathrm{Sobol}},\\, \\overline{\\widehat{D}_{N,4}^{*,\\mathrm{PRNG}}},\\, \\widehat{D}_{N,16}^{*,\\mathrm{Sobol}},\\, \\overline{\\widehat{D}_{N,16}^{*,\\mathrm{PRNG}}},\\, \\widehat{D}_{N,64}^{*,\\mathrm{Sobol}},\\, \\overline{\\widehat{D}_{N,64}^{*,\\mathrm{PRNG}}},\\, \\widehat{D}_{N,128}^{*,\\mathrm{Sobol}},\\, \\overline{\\widehat{D}_{N,128}^{*,\\mathrm{PRNG}}}],\n$$\nwhere $\\overline{\\widehat{D}_{N,m}^{*,\\mathrm{PRNG}}}$ denotes the arithmetic mean over the three pseudorandom seeds. The final outputs are dimensionless decimals representing the empirical grid-based star-discrepancy values for the Sobol points and the mean for the pseudorandom points at each $m$.",
            "solution": "The posed problem is valid. It is scientifically grounded in the fields of numerical analysis and statistical computing, specifically concerning Monte Carlo and Quasi-Monte Carlo methods. The problem is well-posed, with all necessary parameters, constants, and definitions provided for a unique, verifiable solution. The context of semiconductor lithography provides a realistic and formalizable application for the concept of star-discrepancy.\n\nThe task is to compute a grid-based empirical approximation of the star-discrepancy, $\\widehat{D}_{N,m}^*$, for two-dimensional point sets. This metric is a cornerstone in the theory of uniform distribution and its applications, quantifying the uniformity of a point distribution within the unit hypercube. For a set of $N$ points $\\{x_k\\}_{k=1}^N$ in $[0,1]^2$, the star-discrepancy $D_N^*$ is the worst-case deviation between the empirical distribution and the uniform distribution over all origin-anchored rectangular test sets. Its definition is:\n$$\nD_N^* \\equiv \\sup_{t=(t_1,t_2)\\in[0,1]^2} \\left| \\frac{N([0,t_1)\\times[0,t_2))}{N} - t_1 t_2 \\right|\n$$\nwhere $N([0,t_1)\\times[0,t_2))$ is the number of points $x_k$ falling into the rectangle $[0,t_1)\\times[0,t_2)$, and the term $t_1 t_2$ is the area of this rectangle, representing the expected fraction of points under a uniform distribution.\n\nThe problem requires implementing not the true supremum, which is computationally difficult, but a tractable approximation $\\widehat{D}_{N,m}^*$. This is achieved by restricting the test rectangles to a finite grid $\\mathcal{G}_m = \\{(i/m, j/m) : i,j \\in \\{1, 2, \\dots, m\\}\\}$ for a given resolution $m$. The formula to be computed is:\n$$\n\\widehat{D}_{N,m}^* \\equiv \\max_{1\\le i,j\\le m} \\left| \\frac{1}{N}\\sum_{k=1}^N \\mathbf{1}\\left\\{x_k^{(1)}<\\frac{i}{m},\\, x_k^{(2)}<\\frac{j}{m}\\right\\} - \\frac{i}{m}\\cdot\\frac{j}{m} \\right|\n$$\nHere, $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The term $\\frac{1}{N}\\sum_{k=1}^N \\mathbf{1}\\{\\cdot\\}$ is the empirical cumulative distribution function (ECDF) evaluated at the grid point $(\\frac{i}{m}, \\frac{j}{m})$, while $\\frac{i}{m}\\cdot\\frac{j}{m}$ is the theoretical cumulative distribution function (CDF) of a uniform distribution.\n\nA direct, naive computation iterating through all $m^2$ grid nodes and, for each, all $N$ points would have a time complexity of $O(N \\cdot m^2)$. For the largest specified resolution $m=128$, this is computationally feasible but inefficient. A more efficient algorithm with complexity $O(N + m^2)$ is employed. This algorithm proceeds in the following steps:\n\n1.  **Point Set Generation**: First, the required point sets are generated. For the Quasi-Monte Carlo (QMC) case, $N=256$ unscrambled two-dimensional Sobol points are generated. For the Monte Carlo (MC) case, three independent sets of $N=256$ pseudorandom points are generated using a standard generator seeded with $101$, $202$, and $303$ for reproducibility.\n\n2.  **Binning**: For a given point set and grid resolution $m$, the $N$ points in $[0,1)^2$ are binned into an $m \\times m$ grid of cells. A two-dimensional histogram is constructed, where each entry $(i,j)$ stores the number of points falling into the cell $[\\frac{i}{m}, \\frac{i+1}{m}) \\times [\\frac{j}{m}, \\frac{j+1}{m})$. This step has a complexity of $O(N)$.\n\n3.  **Cumulative Sum**: A two-dimensional prefix sum (or cumulative sum) is computed over the histogram. The resulting $m \\times m$ cumulative count matrix, let's call it $\\mathbf{C}$, is such that its element $\\mathbf{C}_{i-1,j-1}$ contains the total count of points within the rectangle $[0, \\frac{i}{m}) \\times [0, \\frac{j}{m})$. This operation has a complexity of $O(m^2)$.\n\n4.  **Discrepancy Calculation**: The matrix $\\mathbf{C}$ is normalized by $N$ to obtain the ECDF values, $\\mathbf{F}_N = \\mathbf{C}/N$. A corresponding $m \\times m$ matrix of theoretical areas, $\\mathbf{A}$, where $\\mathbf{A}_{i-1,j-1} = (\\frac{i}{m}) \\cdot (\\frac{j}{m})$, is also constructed. The grid-based discrepancy is then found by taking the maximum of the element-wise absolute difference: $\\widehat{D}_{N,m}^* = \\max |\\mathbf{F}_N - \\mathbf{A}|$.\n\nThis procedure is repeated for each grid resolution $m \\in \\{4, 16, 64, 128\\}$. For the pseudorandom points, the final reported value for each $m$ is the arithmetic mean of the discrepancies calculated from the three independent sets. The results for Sobol and mean pseudorandom discrepancies are then collated. As expected from the theory of low-discrepancy sequences, the Sobol points should exhibit a systematically lower discrepancy, and thus better uniformity, than the pseudorandom points.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import qmc\n\ndef calculate_grid_discrepancy(points: np.ndarray, m: int) -> float:\n    \"\"\"\n    Calculates the grid-based empirical star-discrepancy for a set of 2D points.\n\n    Args:\n        points: An array of shape (N, 2) of points in the unit square [0,1)^2.\n        m: The resolution of the grid.\n\n    Returns:\n        The empirical grid-based star-discrepancy.\n    \"\"\"\n    N = points.shape[0]\n    if N == 0:\n        # If there are no points, the empirical measure is 0. The max discrepancy\n        # will be at i=m, j=m, where the area is 1.\n        return 1.0\n\n    # Step 1: Create a 2D histogram of the points.\n    # `hist[i, j]` counts points in [i/m, (i+1)/m) x [j/m, (j+1)/m).\n    # The range is [0,1) for both axes.\n    # Note: numpy.histogram2d returns a histogram where the first dimension corresponds\n    # to the first input array (x-coordinates) and the second to the y-coordinates.\n    # We treat x as dimension 0 and y as dimension 1.\n    hist, _, _ = np.histogram2d(\n        points[:, 0], points[:, 1], bins=m, range=[[0, 1], [0, 1]]\n    )\n\n    # Step 2: Compute the 2D cumulative sum to get counts for rectangles [0, i/m)x[0,j/m).\n    # `cumulative_counts[i-1, j-1]` will hold the count of points in the\n    # rectangle [0, i/m) x [0, j/m).\n    cumulative_counts = hist.cumsum(axis=0).cumsum(axis=1)\n\n    # Step 3: Normalize by N to get the empirical CDF values on the grid.\n    empirical_cdf = cumulative_counts / N\n\n    # Step 4: Create a grid of theoretical probabilities (areas).\n    # The test points are (i/m, j/m) for i,j in {1, ..., m}.\n    i_indices = np.arange(1, m + 1)\n    # Create m x m grids for i and j indices, with 'ij' indexing to match\n    # matrix row/column conventions.\n    grid_i, grid_j = np.meshgrid(i_indices, i_indices, indexing='ij')\n    areas = (grid_i / m) * (grid_j / m)\n\n    # Step 5: Calculate the element-wise absolute difference and find the maximum.\n    # The indices of `empirical_cdf` are 0..m-1, corresponding to i,j = 1..m.\n    # `empirical_cdf[i-1, j-1]` corresponds to the test point (i/m, j/m).\n    # `areas[i-1, j-1]` corresponds to the area (i/m * j/m).\n    discrepancy_matrix = np.abs(empirical_cdf - areas)\n    \n    return np.max(discrepancy_matrix)\n\ndef solve():\n    # Define the problem parameters.\n    N_POINTS = 256\n    DIMENSION = 2\n    PRNG_SEEDS = [101, 202, 303]\n    test_cases_m = [4, 16, 64, 128]\n\n    # Generate Sobol points once, as they are deterministic.\n    # `scramble=False` for unscrambled points as specified.\n    sobol_sampler = qmc.Sobol(d=DIMENSION, scramble=False)\n    # For N=256, which is 2^8, the sequence is optimal.\n    sobol_points = sobol_sampler.random(n=N_POINTS)\n\n    results = []\n    # Iterate through each grid resolution m.\n    for m in test_cases_m:\n        # Calculate and store the discrepancy for the Sobol set.\n        d_sobol = calculate_grid_discrepancy(sobol_points, m)\n        results.append(d_sobol)\n\n        # Calculate discrepancy for each pseudorandom set and average the results.\n        prng_discrepancies = []\n        for seed in PRNG_SEEDS:\n            rng = np.random.default_rng(seed=seed)\n            prng_points = rng.random(size=(N_POINTS, DIMENSION))\n            d_prng = calculate_grid_discrepancy(prng_points, m)\n            prng_discrepancies.append(d_prng)\n        \n        d_prng_mean = np.mean(prng_discrepancies)\n        results.append(d_prng_mean)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "When exploring complex, high-dimensional probability distributions, such as those in Bayesian model calibration, Markov Chain Monte Carlo (MCMC) methods are indispensable. However, MCMC samplers do not produce independent draws, and their convergence to the target distribution must be carefully verified. This exercise guides you through the derivation and implementation of the Gelman-Rubin diagnostic ($\\hat{R}$), an essential tool for assessing whether multiple MCMC chains have converged to the same stationary distribution, thereby ensuring the reliability of your simulation results .",
            "id": "4143285",
            "problem": "A semiconductor fabrication team is modeling the posterior distribution of a single latent process parameter $\\theta$ representing an effective plasma etch rate in nanometers per minute (nm/min). The posterior is explored via multiple independent Markov Chain Monte Carlo (MCMC) chains, each producing samples $\\{\\theta_{ij}\\}$, where $i$ indexes the chain and $j$ indexes the iteration within a chain. The goal is to construct a Gelman–Rubin potential scale reduction diagnostic $\\hat{R}$ to assess whether the chains have converged to the same stationary distribution.\n\nStarting from fundamental definitions and laws, you must derive a computable expression for the Gelman–Rubin potential scale reduction factor $\\hat{R}$:\n- Use the definition of sample mean $\\bar{x} = \\frac{1}{n}\\sum_{j=1}^{n} x_{j}$ and unbiased sample variance $s^{2} = \\frac{1}{n-1}\\sum_{j=1}^{n} (x_{j}-\\bar{x})^{2}$ for any finite sample $\\{x_{j}\\}$.\n- Use the law of total variance, which states that for any random variables $X$ and $Y$, $\\mathrm{Var}(X) = \\mathrm{E}[\\mathrm{Var}(X|Y)] + \\mathrm{Var}(\\mathrm{E}[X|Y])$.\n- Define all quantities needed to estimate the target ratio between a marginal posterior variance estimate across chains and the average within-chain variance, and from that ratio define a potential scale reduction factor $\\hat{R}$ that quantifies how much scale reduction one could expect if sampling continued indefinitely.\n\nOnce you have derived a correct expression for $\\hat{R}$, implement a program that:\n1. Generates multiple synthetic chains of posterior samples for $\\theta$ under specified normal distributions to emulate MCMC outputs. Use independent pseudorandom number generators with fixed seeds for reproducibility.\n2. Computes $\\hat{R}$ for each test case using your derived expression.\n3. Compares $\\hat{R}$ against a chosen threshold $T$ to produce a boolean flag indicating convergence. For interpretation, smaller values of $\\hat{R}$ indicate better cross-chain agreement; you must decide convergence by checking whether $\\hat{R}<T$.\n4. Outputs, for each test case, a two-element list $[\\hat{R}, \\text{converged}]$ where $\\hat{R}$ is rounded to $4$ decimal places and $\\text{converged}$ is a boolean. The potential scale reduction factor $\\hat{R}$ is dimensionless (no physical unit), and the boolean flag should be computed using the threshold $T$ provided in each test case.\n5. Produces a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example $[[r_{1},b_{1}],[r_{2},b_{2}],[r_{3},b_{3}]]$ where each $r_{k}$ is a float and each $b_{k}$ is a boolean.\n\nTest suite:\n- Test case $1$ (happy path): $m=4$ chains, each of length $n=1000$, independent normal draws with means $[50,50,50,50]$ (nm/min) and common standard deviation $2$ (nm/min), seeds $[101,102,103,104]$, threshold $T=1.1$.\n- Test case $2$ (non-converged means): $m=3$ chains, $n=800$, means $[50,50,53]$ (nm/min), standard deviation $2$ (nm/min), seeds $[201,202,203]$, threshold $T=1.1$.\n- Test case $3$ (short chains as an edge case): $m=4$ chains, $n=25$, means $[50,50,50,50]$ (nm/min), standard deviation $2$ (nm/min), seeds $[301,302,303,304]$, threshold $T=1.1$.\n- Test case $4$ (near-boundary sensitivity): $m=4$ chains, $n=500$, means $[50,50,50,51]$ (nm/min), standard deviation $2$ (nm/min), seeds $[401,402,403,404]$, threshold $T=1.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is a two-element list $[\\hat{R}, \\text{converged}]$, for example $[[1.0023,True],[1.3567,False],[1.0421,True],[1.0876,True]]$.",
            "solution": "The objective is to derive and implement the Gelman-Rubin potential scale reduction factor, denoted as $\\hat{R}$, to assess the convergence of multiple Markov Chain Monte Carlo (MCMC) chains. The derivation will be based on fundamental statistical principles, including the law of total variance and definitions of sample moments.\n\nLet there be $m$ independent MCMC chains, each of length $n$. Let $\\theta_{ij}$ denote the $j$-th sample from the $i$-th chain, where $i=1, \\dots, m$ and $j=1, \\dots, n$. These are samples from the posterior distribution of a parameter $\\theta$.\n\nThe derivation proceeds in several steps:\n\n1.  **Calculate Within-Chain and Between-Chain Variances**\n\n    First, for each individual chain $i$, we compute its sample mean, $\\bar{\\theta}_{i.}$, and its unbiased sample variance, $s_i^2$.\n    \n    The sample mean of chain $i$ is:\n    $$ \\bar{\\theta}_{i.} = \\frac{1}{n} \\sum_{j=1}^{n} \\theta_{ij} $$\n    \n    The unbiased sample variance of chain $i$ is:\n    $$ s_i^2 = \\frac{1}{n-1} \\sum_{j=1}^{n} (\\theta_{ij} - \\bar{\\theta}_{i.})^2 $$\n\n    The **average within-chain variance**, $W$, is the mean of these individual chain variances. $W$ serves as an estimate of the expected variance within any given chain, assuming the chains have reached stationarity.\n    $$ W = \\frac{1}{m} \\sum_{i=1}^{m} s_i^2 $$\n\n    Next, we quantify the variation *between* the chains. This is captured by the variance of the chain means around the overall mean of all samples. The overall mean, $\\bar{\\theta}_{..}$, is the average of the chain means:\n    $$ \\bar{\\theta}_{..} = \\frac{1}{m} \\sum_{i=1}^{m} \\bar{\\theta}_{i.} $$\n\n    The **between-chain variance**, $B$, is defined as the sample variance of the chain means, scaled by the chain length $n$.\n    $$ B = \\frac{n}{m-1} \\sum_{i=1}^{m} (\\bar{\\theta}_{i.} - \\bar{\\theta}_{..})^2 $$\n    The factor $n$ is crucial. The quantity $\\frac{B}{n}$ is the variance of the chain means. Since the variance of a sample mean of size $n$ is approximately $\\frac{1}{n}$ of the underlying distribution's variance, we scale by $n$ so that $B$ estimates the same variance that $W$ does.\n\n2.  **Estimate the Marginal Posterior Variance**\n\n    The law of total variance states that the total variance of a random variable can be decomposed into the expectation of the conditional variance and the variance of the conditional expectation. For our parameter $\\theta$, conditional on the chain index $I$, we have:\n    $$ \\mathrm{Var}(\\theta) = \\mathrm{E}[\\mathrm{Var}(\\theta|I)] + \\mathrm{Var}(\\mathrm{E}[\\theta|I]) $$\n    If the chains have converged, they all sample from the same stationary posterior distribution. In this ideal case, $W$ is an estimator for $\\mathrm{Var}(\\theta)$. However, for finite-length chains, $W$ tends to underestimate the true posterior variance because each individual chain may not have fully explored the entire parameter space.\n\n    Gelman and Rubin proposed an estimator for the marginal posterior variance, $\\widehat{\\mathrm{Var}}^+(\\theta|y)$, that combines the within-chain and between-chain information. This estimator is designed to overestimate the true variance, making it a conservative measure for diagnostics. It is a weighted average of $W$ and $B$:\n    $$ \\widehat{\\mathrm{Var}}^+(\\theta|y) = \\frac{n-1}{n} W + \\frac{1}{n} B $$\n    This pooled variance estimate accounts for the fact that for finite $n$, $W$ is an underestimate, and it incorporates the between-chain variation $B$ as a correction. If the chains are stationary, this estimator is unbiased for the posterior variance. If they are not (i.e., they started far apart and have not fully mixed), $B$ will be inflated, causing $\\widehat{\\mathrm{Var}}^+(\\theta|y)$ to be large.\n\n3.  **Define the Potential Scale Reduction Factor ($\\hat{R}$)**\n\n    The core idea of the diagnostic is to compare the pooled variance estimate, $\\widehat{\\mathrm{Var}}^+(\\theta|y)$, with the average within-chain variance, $W$. If the chains have converged to the target distribution, these two quantities should be close to each other. A large discrepancy indicates that the between-chain variance is substantial compared to the within-chain variance, signaling a lack of convergence.\n\n    The potential scale reduction factor, $\\hat{R}$, is defined as the square root of the ratio of these two variance estimates:\n    $$ \\hat{R} = \\sqrt{\\frac{\\widehat{\\mathrm{Var}}^+(\\theta|y)}{W}} $$\n    Substituting the expression for $\\widehat{\\mathrm{Var}}^+(\\theta|y)$, we obtain the final computable formula:\n    $$ \\hat{R} = \\sqrt{\\frac{\\frac{n-1}{n} W + \\frac{1}{n} B}{W}} = \\sqrt{\\frac{n-1}{n} + \\frac{B}{nW}} $$\n    As the chains run longer ($n \\to \\infty$) and converge, the chain means $\\bar{\\theta}_{i.}$ will approach a common value, causing $B$ to become small relative to $nW$. Consequently, $\\hat{R}$ will approach $1$. A value of $\\hat{R}$ significantly greater than $1$ indicates that the variance could be substantially reduced by running the chains longer to improve mixing, and thus suggests non-convergence. A commonly used heuristic is to require $\\hat{R} < 1.1$ for all parameters as a necessary (but not sufficient) condition for convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gelman-Rubin diagnostic problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"m\": 4, \"n\": 1000, \"means\": [50, 50, 50, 50],\n            \"std_dev\": 2, \"seeds\": [101, 102, 103, 104], \"T\": 1.1\n        },\n        {\n            \"m\": 3, \"n\": 800, \"means\": [50, 50, 53],\n            \"std_dev\": 2, \"seeds\": [201, 202, 203], \"T\": 1.1\n        },\n        {\n            \"m\": 4, \"n\": 25, \"means\": [50, 50, 50, 50],\n            \"std_dev\": 2, \"seeds\": [301, 302, 303, 304], \"T\": 1.1\n        },\n        {\n            \"m\": 4, \"n\": 500, \"means\": [50, 50, 50, 51],\n            \"std_dev\": 2, \"seeds\": [401, 402, 403, 404], \"T\": 1.1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        n = case[\"n\"]\n        means = case[\"means\"]\n        std_dev = case[\"std_dev\"]\n        seeds = case[\"seeds\"]\n        T = case[\"T\"]\n\n        # 1. Generate synthetic chains\n        chains = []\n        for i in range(m):\n            rng = np.random.default_rng(seeds[i])\n            chain = rng.normal(loc=means[i], scale=std_dev, size=n)\n            chains.append(chain)\n        \n        # Convert list of arrays to a 2D numpy array for efficient computation\n        chains_array = np.array(chains)\n\n        # 2. Compute R-hat\n        r_hat = calculate_r_hat(chains_array, m, n)\n        \n        # 3. Round R-hat and determine convergence\n        r_hat_rounded = round(r_hat, 4)\n        converged = r_hat_rounded  T\n        \n        results.append([r_hat_rounded, converged])\n\n    # 4. Format and print the final output\n    # The output format [[r1,b1],[r2,b2]] requires custom string formatting\n    # to avoid spaces and use capital True/False.\n    output_parts = [f\"[{r},{str(c)}]\" for r, c in results]\n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\ndef calculate_r_hat(chains: np.ndarray, m: int, n: int) -> float:\n    \"\"\"\n    Computes the Gelman-Rubin potential scale reduction factor (R-hat).\n\n    Args:\n        chains: A numpy array of shape (m, n) containing the MCMC samples.\n        m: The number of chains.\n        n: The length of each chain.\n\n    Returns:\n        The calculated R-hat value.\n    \"\"\"\n    if m = 1:\n        raise ValueError(\"R-hat requires at least 2 chains.\")\n    if n = 1:\n        raise ValueError(\"Chains must have length > 1 to compute variance.\")\n\n    # Calculate chain means (shape: (m,))\n    chain_means = np.mean(chains, axis=1)\n\n    # Calculate unbiased chain variances (shape: (m,))\n    # ddof=1 for unbiased sample variance (n-1 denominator)\n    chain_variances = np.var(chains, axis=1, ddof=1)\n\n    # Calculate W: average of the within-chain variances\n    W = np.mean(chain_variances)\n\n    # If W is zero, all chains are constant.\n    # If means are same, B=0, converged, R=sqrt((n-1)/n)  1.\n    # If means are different, B>0, not converged, R=inf.\n    if W == 0:\n        is_converged = np.all(np.isclose(chain_means, chain_means[0]))\n        return np.sqrt((n - 1) / n) if is_converged else np.inf\n\n    # Calculate B: scaled variance of the chain means\n    overall_mean = np.mean(chain_means)\n    B = (n / (m - 1)) * np.sum((chain_means - overall_mean)**2)\n\n    # Calculate R-hat using the derived formula\n    r_hat = np.sqrt(((n - 1) / n) + (B / (n * W)))\n\n    return r_hat\n\nsolve()\n```"
        }
    ]
}