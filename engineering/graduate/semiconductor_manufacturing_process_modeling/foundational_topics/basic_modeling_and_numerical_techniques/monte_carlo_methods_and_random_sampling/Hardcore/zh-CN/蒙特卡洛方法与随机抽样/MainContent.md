## 引言
在尖端的[半导体制造](@entry_id:187383)领域，随机性是无处不在的敌人，也是必须被精确量化的现实。从原子尺度的材料波动到宏观的工艺设备漂移，无数不确定性因素共同决定了最终芯片的性能与良率。面对如此高维、复杂的[随机系统](@entry_id:187663)，传统的解析模型往往束手无策。因此，亟需一种强大而灵活的计算范式，以在设计和生产阶段有效地进行预测、评估和优化。蒙特卡洛方法正是应对这一挑战的关键工具。

本文旨在系统性地剖析[蒙特卡洛方法](@entry_id:136978)及其在半导体过程建模中的应用。我们不仅仅是介绍算法，更是要建立一个从理论到实践的完整认知体系。文章将分为三个核心部分：

首先，在“原理与机制”一章中，我们将深入探讨蒙特卡洛方法的数学基石，从大数定律和[中心极限定理](@entry_id:143108)出发，理解其为何有效，并学习如何生成随机样本以及通过[方差缩减技术](@entry_id:141433)提升模拟效率。

接着，在“应用与交叉学科联系”一章中，我们将理论付诸实践，展示如何利用这些方法对工艺可变性进行建模、精确估计良率、追踪时空动态过程，并探索其与全局敏感性分析、机器学习等前沿领域的深刻联系。

最后，“动手实践”部分将提供精选的练习，帮助您巩固所学知识，并培养解决实际问题的能力。

现在，让我们从[蒙特卡洛方法](@entry_id:136978)最核心的“原理与机制”开始，揭示其通过随机性征服不确定性的奥秘。

## 原理与机制

[蒙特卡洛](@entry_id:144354)（[Monte Carlo](@entry_id:144354), MC）方法是一大类计算算法，其核心思想是利用随机抽样来获得数值结果。在[半导体制造](@entry_id:187383)过程建模这一复杂领域，物理过程的随机性与参数空间的高维度使得解析方法往往难以奏效，而蒙特卡洛方法则提供了一个强大而灵活的框架，用于[量化不确定性](@entry_id:272064)、评估良率和校准模型。本章将深入探讨蒙特卡洛方法的基本原理，从[随机抽样](@entry_id:175193)的数学基础到高级的[方差缩减技术](@entry_id:141433)，阐明其在[过程建模](@entry_id:183557)中的应用机制。

### 估算的基石：通过随机抽样进行估算

[蒙特卡洛方法](@entry_id:136978)最核心的应用是计算积分，特别是[高维积分](@entry_id:143557)。在概率论的语言中，一个[期望值](@entry_id:150961)本质上就是一个积分。假设我们关心一个与工艺相关的性能指标，该指标是[随机过程](@entry_id:268487)参数 $X$ 的函数 $f(X)$。这里的 $X$ 可能是一个代表线宽、阈值电压等变化的随机向量，其遵循一个概率律 $\mathbb{P}_X$。我们希望估算的目标量是该性能指标的均值 $\mu$，即：
$$
\mu = \mathbb{E}[f(X)] = \int f(x) d\mathbb{P}_X(x)
$$
当 $X$ 的分布和函数 $f$ 足够复杂时，这个积分可能没有解析解。

**朴素[蒙特卡洛](@entry_id:144354)（Crude Monte Carlo）** 估算器提供了一个直观的解决方案：通过从分布 $\mathbb{P}_X$ 中抽取 $N$ 个独立的样本 $\{X_i\}_{i=1}^N$，然后计算这些样本上函数值的[算术平均值](@entry_id:165355)，来近似真实的[期望值](@entry_id:150961)：
$$
\hat{\mu}_N = \frac{1}{N} \sum_{i=1}^{N} f(X_i)
$$
这个简单的平均过程背后隐藏着深刻的数学原理。一个自然而然的问题是：为什么这个样本均值会收敛到我们想要的目标值 $\mu$？答案在于**大数定律（Law of Large Numbers, LLN）**。

#### 收敛性保证：大数定律

大数定律为蒙特卡洛方法的有效性提供了理论基石。它描述了在何种条件下，样本均值会随着[样本量](@entry_id:910360)的增加而收敛于[期望值](@entry_id:150961)。这种收敛性对于保证我们模拟结果的可靠性至关重要。

最广为人知的版本是**柯尔莫哥洛夫强大数定律（Kolmogorov's Strong Law of Large Numbers）**。该定律指出，如果样本序列 $\{X_i\}$ 是**[独立同分布](@entry_id:169067)（independent and identically distributed, i.i.d.）**的，那么样本均值 $\hat{\mu}_N$ **[几乎必然收敛](@entry_id:265812)（almost surely converges）**到[期望值](@entry_id:150961) $\mu$ 的充分必要条件是[期望值](@entry_id:150961)存在，即 $\mathbb{E}[|f(X)|] \lt \infty$。这意味着只要我们的性能函数是可积的，通过 i.i.d. 抽样得到的[蒙特卡洛](@entry_id:144354)估算值几乎肯定会随着模拟次数的增加而趋近于[真值](@entry_id:636547) 。

在实践中，获得完全独立的样本并非总是可行。幸运的是，[大数定律](@entry_id:140915)的成立条件可以放宽。例如，**埃特马迪强[大数定律](@entry_id:140915)（Etemadi's Strong Law of Large Numbers）** 表明，即使样本序列仅满足较弱的**成对独立（pairwise independent）**和同分布条件，只要期望存在，强[大数定律](@entry_id:140915)依然成立 。

在更复杂的场景中，如使用[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）进行模型校准时，样本之间存在固有的依赖性。在这种情况下，[大数定律](@entry_id:140915)依然有其对应的形式。**伯克霍夫-钦钦[遍历定理](@entry_id:261967)（Birkhoff-Khinchin Ergodic Theorem）**指出，如果样本序列 $\{X_i\}$ 是由一个相对于其[平稳分布](@entry_id:194199) $\mathbb{P}_X$ 保持不变的**遍历（ergodic）**[马尔可夫链](@entry_id:150828)生成的，那么只要 $\mathbb{E}[|f(X)|] \lt \infty$，样本均值仍然[几乎必然收敛](@entry_id:265812)到 $\mu$ 。这为 MCMC 方法的应用提供了坚实的理论基础。

需要强调的是，某些看似合理的条件并不足以保证收敛。例如，对于一组[相互独立](@entry_id:273670)但非同分布的[随机变量](@entry_id:195330)，仅仅假设它们的方差有限，并不足以保证强大数定律成立。还需要对这些方差的增长施加额外的约束（例如，柯尔莫哥洛夫的SLLN要求方差序列满足 $\sum_{i=1}^\infty \frac{\operatorname{Var}(Y_i)}{i^2} \lt \infty$）。

### [量化不确定性](@entry_id:272064)：中心极限定理与[置信区间](@entry_id:142297)

[大数定律](@entry_id:140915)告诉我们估算值会收敛到真值，但它没有说明收敛的速度有多快，也没有量化在有限样本量 $N$ 下估算值 $\hat{\mu}_N$ 的不确定性。为了回答“我的估算有多精确？”这个问题，我们需要借助**[中心极限定理](@entry_id:143108)（Central Limit Theorem, CLT）**。

[中心极限定理](@entry_id:143108)描述了样本均值的分布形态。对于 i.i.d. 样本，只要 $f(X)$ 的方差 $\sigma^2 = \operatorname{Var}(f(X))$ 是有限的，那么当样本量 $N$ 足够大时，样本均值 $\hat{\mu}_N$ 的分布将近似于一个正态分布。更准确地说，标准化后的估算器收敛于[标准正态分布](@entry_id:184509)：
$$
\sqrt{N} \frac{\hat{\mu}_N - \mu}{\sigma} \xrightarrow{d} \mathcal{N}(0,1)
$$
其中 $\xrightarrow{d}$ 表示[依分布收敛](@entry_id:275544)。

这个定理非常实用，因为它允许我们为估算值 $\mu$ 构建一个**置信区间（confidence interval）**。然而，在实际应用中，真实的方差 $\sigma^2$ 通常是未知的。一个自然的想法是用**样本方差** $s_N^2 = \frac{1}{N-1}\sum_{i=1}^{N}(f(X_i)-\hat{\mu}_N)^2$ 来替代 $\sigma^2$。由于样本方差是真实方差的一致性估计（即当 $N \to \infty$ 时，$s_N^2 \xrightarrow{p} \sigma^2$），根据**[斯卢茨基定理](@entry_id:181685)（Slutsky's Theorem）**，用 $s_N$ 替换 $\sigma$ 后，上述收敛关系依然成立：
$$
\sqrt{N} \frac{\hat{\mu}_N - \mu}{s_N} \xrightarrow{d} \mathcal{N}(0,1)
$$
基于此，我们可以构建一个近似的 $(1-\alpha)$ 水平的双侧置信区间：
$$
\mu \in \left[ \hat{\mu}_N - z_{1-\alpha/2} \frac{s_N}{\sqrt{N}}, \hat{\mu}_N + z_{1-\alpha/2} \frac{s_N}{\sqrt{N}} \right]
$$
其中 $z_{1-\alpha/2}$ 是[标准正态分布](@entry_id:184509)的 $(1-\alpha/2)$ [分位数](@entry_id:178417)（例如，对于 $95\%$ 的置信水平，$\alpha=0.05$，$z_{0.975} \approx 1.96$）。

置信区间的**半宽（half-width）** $h = z_{1-\alpha/2} \frac{s_N}{\sqrt{N}}$ 是衡量估算精度的直接指标。这个关系式也揭示了蒙特卡洛方法的一个基本特征：估算误差的衰减速度为 $O(N^{-1/2})$。

**应用示例：确定样本量**
[中心极限定理](@entry_id:143108)不仅用于评估已有结果的精度，还可用于指导[实验设计](@entry_id:142447)。假设在一次[等离子体刻蚀](@entry_id:192173)工艺的初步模拟中，我们运行了 $N_0=400$ 次，得到平均刻蚀速率为 $\bar{R}_0 = 525$ nm/min，样本方差为 $s_0^2 = 900$ (nm/min)$^2$。由此可计算出 $95\%$ [置信区间](@entry_id:142297)的半宽为 $h_0 \approx 1.96 \times \frac{\sqrt{900}}{\sqrt{400}} = 2.94$ nm/min 。

如果我们希望在后续更大规模的模拟中，将置信区间的半宽控制在 $h=2$ nm/min 以内，我们需要多大的样本量 $N$？利用半宽公式 $h = z_{1-\alpha/2} \frac{\sigma}{\sqrt{N}}$，并使用初步模拟得到的 $s_0=30$ 作为对 $\sigma$ 的插件估计，我们可以求解 $N$：
$$
N \ge \left( \frac{z_{1-\alpha/2} s_0}{h} \right)^2 = \left( \frac{1.96 \times 30}{2} \right)^2 \approx 864.36
$$
由于样本量必须是整数，我们需要至少 $N=865$ 次模拟才能达到预期的精度 。这个过程展示了如何从理论出发，结合初步数据来规划计算资源，这是过程建模中一个非常实际的问题。

### 生成随机样本

[蒙特卡洛方法](@entry_id:136978)的基础是能够生成满足特定分布的随机数。这个过程分为两个主要步骤：首先，生成看似随机的均匀分布序列；然后，将这些均匀分布的样本转换为我们[目标分布](@entry_id:634522)的样本。

#### [伪随机数生成器](@entry_id:145648)（PRNG）

计算机无法生成真正意义上的随机数，而是通过确定性算法产生**[伪随机数](@entry_id:196427)（pseudo-random numbers）**。这些序列虽然是确定性的，但如果设计得当，其统计特性可以很好地模拟真实的随机序列。一个高质量的[伪随机数生成器](@entry_id:145648)（PRNG）对于保证[蒙特卡洛模拟](@entry_id:193493)的有效性至关重要，尤其是在处理半导体建模中常见的高维问题时。

一个好的 PRNG 应具备以下关键特性 ：
1.  **长周期（Long Period）**：PRNG 生成的序列最终会重复。周期长度必须远大于模拟所需的随机数总量，以避免在模拟过程中出现序列循环，导致样本失去独立性。
2.  **高维均匀性（High-Dimensional Uniformity）**：由连续的 PRNG 输出构成的 $d$ 维向量应均匀地分布在 $d$ 维单位[超立方体](@entry_id:273913)中。如果 PRNG 在高维空间中表现出结构性偏差（例如，点都落在少数几个超平面上），模拟结果将会产生系统性误差。这个特性也称为**高维均布性（equidistribution）**。
3.  **[统计独立性](@entry_id:150300)（Statistical Independence）**：序列中的数应通过各种统计检验，表现得像独立的[随机变量](@entry_id:195330)一样，没有明显的[自相关](@entry_id:138991)性。

经典的**[线性同余生成器](@entry_id:143094)（Linear Congruential Generator, LCG）**，其形式为 $X_{n+1} = (a X_n + c) \pmod m$，结构简单，计算速度快。然而，LCG 的一个致命弱点是其输出的 $d$ 维向量会落在至多 $(d! m)^{1/d}$ 个平行的超平面上，这就是所谓的“晶格结构”。在维度 $d$ 较高时（例如，在一个包含 $d=48$ 个工艺参数的模型中），这种晶格结构会变得非常稀疏，导致样本无法有效覆盖整个参数空间，从而引入严重偏误 。

相比之下，现代的 PRNG，如**[梅森旋转算法](@entry_id:145337)（[Mersenne Twister](@entry_id:145337), [MT19937](@entry_id:752216)）**，则表现出优越得多的性能。[MT19937](@entry_id:752216) 拥有长达 $2^{19937}-1$ 的惊人周期，并且其设计保证了在高达 623 维的空间里，其输出向量都能达到 32 位精度的均布性。对于一个需要 $d=48$ 维随机数的半导体工艺仿真任务，[MT19937](@entry_id:752216) 能够提供高质量的、无结构性偏差的[伪随机数](@entry_id:196427)源，是远比 LCG 更可靠的选择 。

#### 生成非均匀分布的[随机变量](@entry_id:195330)

一旦我们有了高质量的 $U \sim \text{Uniform}(0,1)$ 随机数源，下一步就是将其转换为服从特定[目标分布](@entry_id:634522)（如正态分布、对数正态分布、Beta分布等）的[随机变量](@entry_id:195330)。

##### [逆变换采样](@entry_id:139050)（Inverse Transform Sampling）

该方法基于**[概率积分变换](@entry_id:262799)（probability integral transform）**原理：若[随机变量](@entry_id:195330) $X$ 的[累积分布函数](@entry_id:143135)（CDF）为 $F_X(x)$，则[随机变量](@entry_id:195330) $Y=F_X(X)$ 服从 $U(0,1)$ 分布。反之，若 $U \sim U(0,1)$，则 $X = F_X^{-1}(U)$ 就服从由 $F_X$ 定义的分布。因此，只要我们能够得到[目标分布](@entry_id:634522) CDF 的[反函数](@entry_id:141256) $F_X^{-1}$，就可以直接通过此法生成样本。

例如，在半导体工艺中，许多参数（如刻蚀速率、线边缘粗糙度）表现出乘性变化，常被建模为**对数正态分布（lognormal distribution）**。若 $\ln(X) \sim \mathcal{N}(\mu, \sigma^2)$，则 $X$ 为对数正态分布。我们可以先生成一个[标准正态分布](@entry_id:184509)的样本 $Z \sim \mathcal{N}(0,1)$，然后通过线性变换得到 $Y = \mu + \sigma Z \sim \mathcal{N}(\mu, \sigma^2)$，最后取指数 $X = \exp(Y)$。生成 $Z$ 的一步就可以使用[逆变换法](@entry_id:141695)：$Z = \Phi^{-1}(U)$，其中 $U \sim U(0,1)$，$\Phi^{-1}$ 是[标准正态分布](@entry_id:184509)CDF的[反函数](@entry_id:141256)。综上，生成[对数正态分布](@entry_id:261888)样本的完整公式为：
$$
X = \exp\{\mu + \sigma \Phi^{-1}(U)\}
$$
该方法生成的[随机变量](@entry_id:195330) $X$ 的均值和方差分别为 $\mathbb{E}[X] = \exp\{\mu + \frac{1}{2}\sigma^2\}$ 和 $\mathrm{Var}(X) = (\exp\{\sigma^2\}-1)\exp\{2\mu + \sigma^2\}$ 。

##### 接受-[拒绝采样](@entry_id:142084)（Acceptance-Rejection Sampling）

当[目标分布](@entry_id:634522)的 CDF [反函数](@entry_id:141256)难以求得时，[逆变换法](@entry_id:141695)便不再适用。**接受-[拒绝采样](@entry_id:142084)**提供了一种替代方案。其思想是：选择一个容易抽样的**[提议分布](@entry_id:144814)（proposal distribution）** $g(x)$，并找到一个常数 $M$，使得目标[概率密度函数](@entry_id:140610)（PDF）$f(x)$ 满足 $f(x) \le M g(x)$ 对所有 $x$ 成立。算法步骤如下：
1. 从[提议分布](@entry_id:144814) $g(x)$ 中抽取一个候选样本 $Y$。
2. 从 $U(0,1)$ 中抽取一个随机数 $U$。
3. 如果 $U \le \frac{f(Y)}{M g(Y)}$，则接受样本 $Y$；否则，拒绝它。

该算法的正确性可以被严格证明。其效率由**[接受概率](@entry_id:138494)**决定。单次尝试的[接受概率](@entry_id:138494)为 $\mathbb{P}(\text{accept}) = 1/M$。为了提高效率，我们需要选择一个尽可能“贴近” $f(x)$ 的 $g(x)$，并找到最小的常数 $M$，即 $M = \sup_x \frac{f(x)}{g(x)}$。

一个重要的前提是，必须存在一个有限的 $M$。如果对于某个区域，$f(x)$ 相对于 $g(x)$ 是无界的，那么该方法就不可行。例如，我们尝试使用均匀分布 $g(x) = U(0,1)$ 来对 Beta 分布 $f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$ 进行采样。只有当 $\alpha \ge 1$ 且 $\beta \ge 1$ 时，$f(x)$ 在 $(0,1)$ 上才是有界的，此时可以找到有限的 $M$，[接受-拒绝法](@entry_id:263903)可行。若 $0 \lt \alpha \lt 1$ 或 $0 \lt \beta \lt 1$，则 $f(x)$ 在端点处趋于无穷，无法用均匀分布作为[提议分布](@entry_id:144814)，此时算法的[接受概率](@entry_id:138494)为 0 。

### [方差缩减技术](@entry_id:141433)

蒙特卡洛估算的误差率是 $O(N^{-1/2})$，这意味着要将误差减半，需要将样本量增加四倍。在仿真成本高昂的半导体建模中，这种[收敛速度](@entry_id:636873)可能过慢。**[方差缩减技术](@entry_id:141433)（Variance Reduction Techniques）**旨在通过更“聪明”的抽样方式，在不增加（甚至减少）样本量的情况下，获得更精确的估算，即减小估算器 $\hat{\mu}_N$ 的方差。

#### 重要性采样（Importance Sampling）

在[半导体制造](@entry_id:187383)中，我们常常关心**稀有事件（rare events）**的发生概率，例如器件失效或工艺参数超出规格。假设栅极长度 $L \sim \mathcal{N}(\mu, \sigma^2)$，我们想估算其超过某个规格上限 $\tau$ 的概率 $p = \mathbb{P}(L > \tau)$。朴素蒙特卡洛估算器是 $\hat{p}_n = \frac{1}{n}\sum \mathbf{1}\{L_i > \tau\}$。其**相对误差** $\frac{\sqrt{\operatorname{Var}(\hat{p}_n)}}{p} \approx \frac{1}{\sqrt{np}}$。当 $p$ 非常小时（例如 $p \approx 10^{-5}$），为了达到一个合理的[相对误差](@entry_id:147538)（如 $10\%$），所需的样本量 $n$ 可能会达到千万甚至上亿级别，这在计算上是不可行的 。

**重要性采样（Importance Sampling, IS）** 正是为解决此类问题而设计的。其核心思想是，与其在原始分布 $p(x)$ 下被动地等待稀有事件发生，不如从另一个**[提议分布](@entry_id:144814)** $q(x)$ 中主动抽样，这个 $q(x)$ 会更频繁地生成我们感兴趣的“重要”区域（例如，接近 $\tau$ 的值）的样本。

为了修正因改变[抽样分布](@entry_id:269683)而引入的偏差，我们需要对每个样本的贡献进行加权。原始期望可以重写为：
$$
\mu = \mathbb{E}_p[f(X)] = \int f(x) p(x) dx = \int \left[f(x) \frac{p(x)}{q(x)}\right] q(x) dx = \mathbb{E}_q\left[f(X) \frac{p(X)}{q(X)}\right]
$$
这个变换引出了**重要性权重** $w(x) = p(x)/q(x)$。于是，[重要性采样](@entry_id:145704)估算器为：
$$
\hat{\mu}_{IS} = \frac{1}{N} \sum_{i=1}^{N} f(X_i) \frac{p(X_i)}{q(X_i)}, \quad \text{其中 } X_i \sim q(x)
$$
可以证明，只要 $q(x)$ 的支撑集包含了 $p(x)$ 的支撑集（即，任何 $p(x) \gt 0$ 的地方都有 $q(x) \gt 0$），这个估算器就是无偏的 。

一个好的[提议分布](@entry_id:144814) $q(x)$ 能使得被积函数 $f(x)w(x)$ 的方差远小于原始函数 $f(x)$ 的方差。在[稀有事件模拟](@entry_id:142769)中，通过精心设计的 IS 方案（例如，对高斯分布进行均值漂移），可以实现**有界相对误差**，即无论事件多么稀有，估算器的[相对误差](@entry_id:147538)都保持在一个有界的常数水平，这相对于朴素蒙特卡洛的爆炸性误差增长是一个巨大的改进 。

#### 分层采样（Stratified Sampling）

当被积函数在定义域的不同区域表现出不同变异性时，**分层采样（Stratified Sampling）** 是一种非常有效的[方差缩减技术](@entry_id:141433)。其思想是将总体的抽样[域划分](@entry_id:748628)为若干个互不重叠的**层（strata）**，然后在每一层内独立进行简单随机抽样。

在[半导体制造](@entry_id:187383)中，晶圆上的薄膜沉积厚度常因设备几何结构和气流而呈现区域性差异。我们可以将晶圆划分为中心、边缘等几个区域（层）。设第 $k$ 层的权重（面积占比）为 $w_k$，该层内厚度的真实均值和方差分别为 $\mu_k$ 和 $\sigma_k^2$。总体的晶圆平均厚度为 $\mu = \sum_k w_k \mu_k$。分层采样的估算器为：
$$
\hat{\mu}_{st} = \sum_{k=1}^{K} w_k \bar{Y}_k
$$
其中 $\bar{Y}_k$ 是从第 $k$ 层抽取的样本的均值。由于各层抽样独立，该估算器的方差为：
$$
\mathrm{Var}(\hat{\mu}_{st}) = \sum_{k=1}^{K} w_k^2 \mathrm{Var}(\bar{Y}_k)
$$
分层采样的优势在于，它消除了**层间方差**对总估算方差的贡献，只留下了**层内方差**的加权和。如果分层得当，使得层内方差远小于[总体方差](@entry_id:901078)，那么方差缩减的效果将非常显著。

在**[按比例分配](@entry_id:634725)（proportional allocation）**的方案下，分配到第 $k$ 层的样本量 $n_k$ 与该层的权重成正比，即 $n_k = n w_k$。在这种情况下，估算器的方差可以简化为 ：
$$
\mathrm{Var}(\hat{\mu}_{st}) = \frac{1 - f}{n} \sum_{k=1}^{K} w_k \sigma_k^2
$$
其中 $f=n/N$ 是总抽样比，$n$ 是总[样本量](@entry_id:910360)。这个公式清晰地显示了方差是如何由各层的加权平均方差决定的。

### 高级主题与扩展

#### 生成[相关随机变量](@entry_id:200386)与 Copula 方法

在许多工艺模型中，不同的输入参数之间并非相互独立，而是存在相关性。例如，刻蚀过程中的温度和压力可能相关。为了准确建模，我们需要能够生成服从特定[联合分布](@entry_id:263960)的随机向量。

对于多元对数正态分布，一个常用方法是先生成其对应的[多元正态分布](@entry_id:175229)向量 $Z \sim \mathcal{N}(\mu, \Sigma)$，然后逐分量取指数 $X_i = \exp(Z_i)$。生成 $Z$ 的标准方法是利用**[乔列斯基分解](@entry_id:166031)（Cholesky factorization）**。将对数尺度上的[协方差矩阵](@entry_id:139155)分解为 $\Sigma = LL^\top$，然后生成一个标准多元正态向量 $\varepsilon \sim \mathcal{N}(0, I)$，最后通过线性变换 $Z = \mu + L\varepsilon$ 得到目标向量 。

值得注意的是，对数尺度上的协方差 $\Sigma_{ij}$ 与原始尺度上的皮尔逊相关系数 $\mathrm{Corr}(X_i, X_j)$ 之间存在一个[非线性](@entry_id:637147)关系 ：
$$
\mathrm{Corr}(X_i,X_j) = \frac{\exp\{\Sigma_{ij}\}-1}{\sqrt{(\exp\{\Sigma_{ii}\}-1)(\exp\{\Sigma_{jj}\}-1)}}
$$
这意味着我们不能直接将目标相关性作为对数尺度上的协方差。

**Copula 方法**为构建多变量[联合分布](@entry_id:263960)提供了一个更为通用的框架。其核心思想是，任何一个[联合分布](@entry_id:263960)都可以分解为各自的边缘分布和一个描述它们之间依赖结构的 Copula 函数。例如，通过**高斯 Copula**，我们可以将任意给定的边缘分布（例如对数正态、威布尔等）与一个高斯依赖结构（由一个[相关矩阵](@entry_id:262631) $R$ 定义）结合起来。具体操作是：先生成一个多元正态向量 $Z \sim \mathcal{N}(0,R)$，然后将其每个分量通过标准正态CDF $\Phi$ 映射到 $(0,1)$ 区间得到 $U_i = \Phi(Z_i)$，最后将这些相关的 $U_i$ 通过各自边缘分布的逆CDF变换，即可得到具有指定边缘和高斯依赖结构的随机向量。可以证明，对于对数正态分布，此方法与[乔列斯基分解](@entry_id:166031)法是等价的 。

#### 拟[蒙特卡洛方法](@entry_id:136978)（Quasi-Monte Carlo, QMC）

拟蒙特卡洛方法是标准蒙特卡洛方法的一种确定性替代方案。它不使用[伪随机数](@entry_id:196427)，而是使用精心构造的**[低差异序列](@entry_id:139452)（low-discrepancy sequences）**（如 Sobol 序列或 Halton 序列）来填充积分空间。这些序列被设计得尽可能均匀，避免了随机抽样中可能出现的“聚集”和“空洞”。

一个点集的均匀性可以通过其**星差异（star-discrepancy）** $D_N^*$ 来量化。星差异衡量的是，在所有以原点为锚点的轴对齐的超矩形中，点集落入该矩形的比例与该矩形体积之间的最大偏差 。

**[科克斯马-赫劳卡不等式](@entry_id:146879)（Koksma-Hlawka inequality）**揭示了差异度与[积分误差](@entry_id:171351)之间的关系：
$$
|\text{Error}| \le V_{HK}(f) \cdot D_N^*
$$
其中 $V_{HK}(f)$ 是被积函数 $f$ 的 Hardy-Krause 总变差，衡量了函数的“粗糙度”。这个不等式表明，对于足够光滑的函数（即 $V_{HK}(f)$ 有限），[积分误差](@entry_id:171351)直接由点集的星差异控制。

[低差异序列](@entry_id:139452)被设计为具有极低的星差异，其收敛速度为 $D_N^* = O(\frac{(\log N)^s}{N})$，其中 $s$ 是积分的维度。这意味着 QMC 的[积分误差界](@entry_id:750705)为确定性的 $O(\frac{(\log N)^s}{N})$，在维度 $s$ 不太高时，其[收敛速度](@entry_id:636873)渐进地优于标准[蒙特卡洛](@entry_id:144354)的概率性[误差界](@entry_id:139888) $O(N^{-1/2})$。因此，对于光滑的响应函数（如掺杂剂量模型），QMC 是一种极具吸[引力](@entry_id:189550)的高效积分方法 。

#### 使用依赖样本的[蒙特卡洛](@entry_id:144354)（MCMC）

在贝叶斯框架下校准工艺模型时，我们常使用[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）方法从复杂的[后验分布](@entry_id:145605) $\pi(\theta | \mathcal{D})$ 中抽样。MCMC 生成的是一个马尔可夫链 $\{X_t\}$，其中的样本是相关的，而非独立的。

幸运的是，如前所述，只要马尔可夫链是遍历的，[大数定律](@entry_id:140915)（[遍历定理](@entry_id:261967)）依然保证样本均值会收敛到真实的后验[期望值](@entry_id:150961) 。同样，在一定混合条件下，中心极限定理也对 MCMC 估算器成立，但其形式有所调整：
$$
\sqrt{n}(\bar{g}_n - \mu) \Rightarrow \mathcal{N}(0, \sigma_{\text{as}}^2)
$$
这里的**[渐近方差](@entry_id:269933)** $\sigma_{\text{as}}^2$ 不再是单个样本的方差 $\sigma_g^2$，而是包含了样本之间[自相关](@entry_id:138991)性的影响。其表达式为：
$$
\sigma_{\text{as}}^2 = \sigma_g^2 \left(1 + 2\sum_{k=1}^{\infty} \rho_k\right)
$$
其中 $\rho_k$ 是滞后 $k$ 阶的自[相关系数](@entry_id:147037)。这个[渐近方差](@entry_id:269933)也等于[平稳过程](@entry_id:196130) $\{g(X_t)\}$ 在频率 0 处的**谱密度** 。

正的自相关性（$\rho_k \gt 0$）意味着样本之间存在信息冗余，这会导致 $\sigma_{\text{as}}^2 \gt \sigma_g^2$，从而增大了估算器的方差，减慢了[收敛速度](@entry_id:636873)（相对于同样数量的[独立样本](@entry_id:177139)）。我们可以定义**有效样本量（effective sample size, ESS）** $n_{\text{eff}} = n / (1 + 2\sum_{k=1}^{\infty} \rho_k)$，它表示 $n$ 个[相关样本](@entry_id:904545)所包含的等效[独立样本](@entry_id:177139)信息量。正相关性越强，$n_{\text{eff}}$ 就越小于 $n$ 。

一个常见的误区是认为通过**稀疏化（thinning）**（即每隔 $m$ 个样本保留一个）可以降低估算器 $\bar{g}_n$ 的方差。这是错误的。虽然稀疏化降低了保留样本之间的相关性，但它也极大地减少了用于计算均值的样本总数。对于估算[期望值](@entry_id:150961)而言，使用所有 MCMC 样本总是能得到方差最小的估算器 。稀疏化的主要目的通常是减少存储和后续处理的计算负担，而不是提高均值估算的精度。