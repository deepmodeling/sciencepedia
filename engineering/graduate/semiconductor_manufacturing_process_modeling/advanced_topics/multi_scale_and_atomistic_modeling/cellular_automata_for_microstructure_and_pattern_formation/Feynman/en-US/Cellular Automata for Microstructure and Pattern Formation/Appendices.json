{
    "hands_on_practices": [
        {
            "introduction": "Cellular automata are powerful tools for modeling how local interactions produce global order. A fundamental aspect of this process is the emergence of stable microstructures, or fixed points, which represent equilibrium or steady-state configurations. This exercise explores these core concepts using a deterministic, Ising-like cellular automaton where spins align based on a local field derived from neighbor interactions and an external bias. By analyzing the stability of specific patterns like uniform or checkerboard configurations, you will gain a tangible understanding of concepts from dynamical systems—such as fixed points and basins of attraction—within the concrete context of microstructure evolution. This practice `` provides foundational skills for characterizing the long-term behavior and stability of patterns generated by local rules.",
            "id": "4113048",
            "problem": "You will implement a deterministic two-dimensional binary cellular automaton to model microstructure evolution in a simplified setting relevant to semiconductor manufacturing process modeling. The automaton is defined on a periodic square lattice and uses an update rule derived from minimizing a local interaction energy that favors alignment with nearest neighbors and a uniform bias field. You will analyze stability of specified fixed microstructural configurations under perturbations to the local rule and characterize basins of attraction in terms of Hamming distance neighborhoods.\n\nFundamental Basis and Definitions:\n- Let the lattice be $\\Omega = \\{0,1,\\dots,N-1\\} \\times \\{0,1,\\dots,N-1\\}$ with periodic boundary conditions. Each site $(i,j) \\in \\Omega$ carries a spin $s_{i,j} \\in \\{-1,+1\\}$ representing two microstructural phases.\n- The neighborhood is the eight-neighbor set $\\mathcal{N}_{8}(i,j)$ consisting of the sites at offsets $(\\Delta i,\\Delta j) \\in \\{(-1,0),(+1,0),(0,-1),(0,+1),(-1,-1),(-1,+1),(+1,-1),(+1,+1)\\}$, interpreted with periodic wrapping.\n- The local field at site $(i,j)$ is\n$$\nF_{i,j} = J \\sum_{(p,q)\\in\\mathcal{N}_{8}(i,j)} s_{p,q} + h,\n$$\nwhere $J \\in \\mathbb{R}$ is the nearest-neighbor coupling strength and $h \\in \\mathbb{R}$ is a uniform bias field representing an external driving preference.\n- The deterministic synchronous update rule for time $t \\mapsto t+1$ is\n$$\ns_{i,j}^{t+1} =\n\\begin{cases}\ns_{i,j}^{t}, & \\text{if } F_{i,j}^{t} = 0,\\\\\n\\mathrm{sign}\\!\\left(F_{i,j}^{t}\\right), & \\text{if } F_{i,j}^{t} \\neq 0,\n\\end{cases}\n$$\nwhere $\\mathrm{sign}(x)=+1$ if $x>0$ and $\\mathrm{sign}(x)=-1$ if $x<0$. Ties ($F_{i,j}^{t}=0$) keep the current spin.\n- A configuration $s^\\star = \\{s_{i,j}^\\star\\}$ is a fixed configuration under parameters $(J,h)$ if and only if applying the synchronous update produces no changes, i.e., $s_{i,j}^{t+1}=s_{i,j}^{t}=s_{i,j}^\\star$ for all $(i,j)$.\n- Stability under perturbations: Given a finite set of parameter perturbations $\\{(\\Delta J_k,\\Delta h_k)\\}_{k=1}^{K}$, the configuration $s^\\star$ is said to be stable under these perturbations if it remains a fixed configuration for each perturbed parameter pair $(J+\\Delta J_k,h+\\Delta h_k)$.\n- Basin of attraction within Hamming radius: For a target fixed configuration $s^\\star$, define the Hamming distance $d_H(x,s^\\star)$ between configurations $x$ and $s^\\star$ as the number of sites on which they disagree. For a radius $R \\in \\mathbb{N}$ and a maximum iteration budget $M \\in \\mathbb{N}$, the radius-$R$ basin $\\mathcal{B}_R$ is the set of all configurations $x$ with $d_H(x,s^\\star) \\le R$ such that iterative application of the synchronous update under baseline parameters $(J,h)$ reaches $s^\\star$ within at most $M$ steps. If a trajectory enters a cycle that does not include $s^\\star$ or does not reach $s^\\star$ within $M$ steps, it does not belong to $\\mathcal{B}_R$.\n\nTasks to implement:\n1. For each test case below, compute the boolean $b_0$ indicating whether $s^\\star$ is a fixed configuration under the baseline $(J,h)$.\n2. For each test case, compute the boolean $b_1$ indicating whether $s^\\star$ is a fixed configuration under all perturbations $(J+\\Delta J_k,h+\\Delta h_k)$ in the provided list.\n3. For each test case, compute the integer $B = |\\mathcal{B}_R|$, the size of the basin of attraction within Hamming radius $R$ under the baseline parameters.\n4. For each test case, compute the float $\\bar{T}$, the mean number of update steps required for configurations in $\\mathcal{B}_R$ to reach $s^\\star$, rounded to three decimals. If $|\\mathcal{B}_R|=0$, return $\\bar{T}=0.0$.\n\nScientific basis:\n- Start from the local interaction energy for the Ising-like model $E = -J \\sum_{\\langle (i,j),(p,q)\\rangle} s_{i,j} s_{p,q} - h \\sum_{(i,j)} s_{i,j}$, where $\\langle \\cdot,\\cdot \\rangle$ denotes nearest neighbors. In the zero-temperature deterministic limit, local updates align spins with the sign of the local field $F_{i,j}$, yielding the rule above. Ties correspond to locally neutral energy differences and are kept unchanged, consistent with non-increasing energy dynamics.\n\nBoundary conditions and computational details:\n- Use periodic boundary conditions on the $N \\times N$ lattice.\n- The neighborhood sum is over the eight nearest neighbors as specified.\n- Equality to zero in the tie condition should be treated robustly in floating-point arithmetic, considering a small tolerance, but since all sums here are integer multiples of $J$ plus $h$, exact comparisons are acceptable if implemented carefully.\n- The Hamming neighborhood enumeration must include all configurations differing from $s^\\star$ by exactly $k$ spins for $k \\in \\{0,1,\\dots,R\\}$.\n\nTest suite:\n- Use lattice size $N=4$, radius $R=3$, and maximum iteration budget $M=50$ for all cases. For each case, define $s^\\star$, $(J,h)$, and the perturbations $\\{(\\Delta J_k,\\Delta h_k)\\}$ as follows.\n  - Case $1$ (uniform positive phase):\n    - $s^\\star$ is the uniform configuration with $s_{i,j}^\\star=+1$ for all $(i,j)$.\n    - Baseline parameters $(J,h)=(1.0,0.0)$.\n    - Perturbations: $\\{(-0.1,0.0),(0.0,0.2),(0.05,-0.3)\\}$.\n  - Case $2$ (checkerboard):\n    - $s^\\star$ is the checkerboard configuration $s_{i,j}^\\star = (-1)^{i+j}$ mapped to $\\{-1,+1\\}$, i.e., $s_{i,j}^\\star=+1$ if $i+j$ is even and $s_{i,j}^\\star=-1$ if $i+j$ is odd.\n    - Baseline parameters $(J,h)=(1.0,0.0)$.\n    - Perturbations: $\\{(0.0,0.1),(0.0,-0.1),(-0.1,0.05)\\}$.\n  - Case $3$ (uniform negative phase):\n    - $s^\\star$ is the uniform configuration with $s_{i,j}^\\star=-1$ for all $(i,j)$.\n    - Baseline parameters $(J,h)=(1.0,0.0)$.\n    - Perturbations: $\\{(0.0,-0.2),(-0.5,3.9),(-0.95,0.5)\\}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a bracketed list $[b_0,b_1,B,\\bar{T}]$ in that order, where $b_0$ and $b_1$ are booleans, $B$ is an integer, and $\\bar{T}$ is a float rounded to three decimals. For example, a valid output for three test cases would look like $[[\\mathrm{True},\\mathrm{False},12,3.000],[\\mathrm{True},\\mathrm{True},8,1.250],[\\mathrm{False},\\mathrm{False},0,0.000]]$.",
            "solution": "The user's request is to implement and analyze a deterministic two-dimensional binary cellular automaton. The problem is scientifically grounded in the principles of statistical mechanics, specifically the Ising model at zero temperature, and is well-posed with all necessary parameters and definitions provided. The tasks are computationally feasible and stated with objectivity and clarity. Therefore, the problem is deemed valid.\n\nThe solution proceeds by first formalizing the model and then detailing the algorithms designed to solve the specified tasks.\n\n### Model Formalization\n\nThe system is defined on a two-dimensional square lattice $\\Omega$ of size $N \\times N$ with periodic boundary conditions. Each site $(i,j)$ on the lattice is in one of two states, represented by a spin variable $s_{i,j} \\in \\{-1, +1\\}$. The evolution of the system is governed by a synchronous update rule that deterministically minimizes a local energy functional. This rule depends on the local field, $F_{i,j}$, at each site, which is defined as:\n$$\nF_{i,j} = J \\sum_{(p,q)\\in\\mathcal{N}_{8}(i,j)} s_{p,q} + h\n$$\nHere, $J$ is the coupling constant that dictates the strength of interaction between a spin and its neighbors, $h$ is an external bias field that favors one spin state over the other, and $\\mathcal{N}_{8}(i,j)$ is the set of the $8$ nearest neighbors of site $(i,j)$. This formulation is analogous to the zero-temperature dynamics of an Ising model where spins align to lower the local energy.\n\nThe synchronous update rule for all sites $(i,j)$ from time $t$ to $t+1$ is given by:\n$$\ns_{i,j}^{t+1} =\n\\begin{cases}\ns_{i,j}^{t}, & \\text{if } F_{i,j}^{t} = 0,\\\\\n\\mathrm{sign}\\!\\left(F_{i,j}^{t}\\right), & \\text{if } F_{i,j}^{t} \\neq 0,\n\\end{cases}\n$$\nwhere the sign function is defined as $\\mathrm{sign}(x) = +1$ for $x > 0$ and $\\mathrm{sign}(x) = -1$ for $x < 0$. If the local field $F_{i,j}^{t}$ is exactly zero, the spin at that site remains unchanged, representing a state of neutral stability.\n\n### Algorithmic Implementation\n\nAn efficient implementation requires careful design, particularly for the neighborhood sum calculation and the basin of attraction analysis.\n\n**1. Fixed Point and Stability Analysis (Tasks 1 & 2)**\n\nA configuration $s^\\star = \\{s_{i,j}^\\star\\}$ is a fixed point if, for every site $(i,j)$, the update rule yields no change: $s_{i,j}^{t+1} = s_{i,j}^\\star$. This is equivalent to the condition that for every site, either the local field $F_{i,j}$ is zero, or its sign matches the spin at that site, i.e., $s_{i,j}^\\star \\cdot F_{i,j} > 0$.\n\nTo implement the check for $b_0$ (whether $s^\\star$ is a fixed point under baseline parameters $(J,h)$), we perform one full synchronous update on $s^\\star$ and verify if the resulting configuration is identical to $s^\\star$.\n\nThe stability analysis for $b_1$ extends this procedure. The configuration $s^\\star$ is stable under a set of perturbations $\\{(\\Delta J_k, \\Delta h_k)\\}$ if it remains a fixed point for all perturbed parameters $(J_k', h_k') = (J+\\Delta J_k, h+\\Delta h_k)$. The algorithm iterates through each perturbation, applies the fixed-point check, and returns `True` only if the condition holds for all of them.\n\nThe crucial calculation of the neighbor sum, $\\sum s_{p,q}$, for all sites simultaneously is efficiently performed using a 2D convolution. The lattice configuration is convolved with a $3 \\times 3$ kernel of ones with a zero at the center. The use of periodic boundary conditions (`wrap` mode in `scipy.signal.convolve2d`) correctly handles the lattice topology.\n\n**2. Basin of Attraction Analysis (Tasks 3 & 4)**\n\nThis analysis is performed only if the target configuration $s^\\star$ is a valid fixed point under the baseline parameters $(J,h)$. The goal is to find the size $B = |\\mathcal{B}_R|$ of the basin of attraction and the mean convergence time $\\bar{T}$.\n\nThe algorithm proceeds as follows:\n- **Enumerate Hamming Neighborhood**: We must test every configuration $s$ whose Hamming distance from $s^\\star$ is less than or equal to the radius $R$. This is done by generating all combinations of $k$ distinct sites to flip from their state in $s^\\star$, for each $k \\in \\{0, 1, \\dots, R\\}$. The `itertools.combinations` function is used for this combinatorial generation.\n- **Simulate Trajectories**: For each initial configuration generated:\n    1. A simulation is run for a maximum of $M$ steps.\n    2. At each step, the configuration is updated using the synchronous rule.\n    3. The trajectory is checked for convergence, which occurs if the configuration becomes identical to $s^\\star$. If it converges at step $\\tau$, this configuration is added to the basin $\\mathcal{B}_R$, and the step count $\\tau$ is recorded.\n    4. To detect non-converging cycles, a history of all visited configurations in the current trajectory is maintained. If a configuration repeats before reaching $s^\\star$, a cycle has been found, and the trajectory is marked as non-converging.\n    5. If a trajectory does not converge to $s^\\star$ within the budget of $M$ steps, it is also considered non-converging.\n- **Compute Statistics**: After testing all initial configurations in the Hamming ball, the basin size $B$ is the total count of converging trajectories. The total number of steps for all converging trajectories is summed, and the mean convergence time $\\bar{T}$ is calculated as this sum divided by $B$. If $B=0$, $\\bar{T}$ is defined as $0.0$. The initial state $s^\\star$ itself contributes to the basin with a convergence time of $0$ steps.\n\nThis comprehensive approach ensures that the properties of the automaton's dynamics near the specified fixed points are correctly characterized according to the problem statement. The use of `numpy` for array operations, `scipy` for convolution, and standard library tools for combinatorics provides an efficient and robust solution.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    \n    # Define a helper function to create the s_star configurations.\n    def create_s_star(n_size, config_type):\n        if config_type == 'uniform_pos':\n            # All spins are +1\n            return np.ones((n_size, n_size), dtype=np.int8)\n        elif config_type == 'uniform_neg':\n            # All spins are -1\n            return -np.ones((n_size, n_size), dtype=np.int8)\n        elif config_type == 'checkerboard':\n            # s_ij = (-1)^(i+j)\n            # Create a grid where value is (i+j)\n            idx_grid = np.fromfunction(lambda i, j: i + j, (n_size, n_size), dtype=np.int8)\n            # Map even to +1, odd to -1\n            return 1 - 2 * (idx_grid % 2)\n        return None\n\n    # Define test cases as specified in the problem statement.\n    N_val = 4\n    test_cases = [\n        {\n            \"s_star\": create_s_star(N_val, 'uniform_pos'),\n            \"J\": 1.0, \"h\": 0.0,\n            \"perturbations\": [(-0.1, 0.0), (0.0, 0.2), (0.05, -0.3)],\n            \"N\": N_val, \"R\": 3, \"M\": 50\n        },\n        {\n            \"s_star\": create_s_star(N_val, 'checkerboard'),\n            \"J\": 1.0, \"h\": 0.0,\n            \"perturbations\": [(0.0, 0.1), (0.0, -0.1), (-0.1, 0.05)],\n            \"N\": N_val, \"R\": 3, \"M\": 50\n        },\n        {\n            \"s_star\": create_s_star(N_val, 'uniform_neg'),\n            \"J\": 1.0, \"h\": 0.0,\n            \"perturbations\": [(0.0, -0.2), (-0.5, 3.9), (-0.95, 0.5)],\n            \"N\": N_val, \"R\": 3, \"M\": 50\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(\n            case[\"s_star\"], case[\"J\"], case[\"h\"], case[\"perturbations\"],\n            case[\"N\"], case[\"R\"], case[\"M\"]\n        )\n        results.append(result)\n\n    # Format the results into the required single-line string format.\n    str_results = []\n    for res in results:\n        b0_str = str(res[0])\n        b1_str = str(res[1])\n        b_str = str(res[2])\n        t_bar_str = f\"{res[3]:.3f}\"\n        str_results.append(f\"[{b0_str},{b1_str},{b_str},{t_bar_str}]\")\n        \n    print(f\"[{','.join(str_results)}]\")\n\ndef update_step(config, J, h):\n    \"\"\"\n    Performs a single synchronous update of the entire lattice.\n    \"\"\"\n    # Kernel for summing the 8-neighbor Moore neighborhood.\n    kernel = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=np.int8)\n    # Use 2D convolution with periodic boundary conditions.\n    neighbor_sum = convolve2d(config, kernel, mode='same', boundary='wrap')\n    \n    # Calculate the local field F for all sites.\n    F = J * neighbor_sum + h\n    \n    # Apply the update rule.\n    next_config = config.copy()\n    next_config[F > 0] = 1\n    next_config[F < 0] = -1\n    # Where F=0, the spin remains unchanged from the copied config.\n    \n    return next_config\n\ndef is_fixed_point(config, J, h):\n    \"\"\"\n    Checks if a given configuration is a fixed point for the given parameters.\n    \"\"\"\n    next_config = update_step(config, J, h)\n    return np.array_equal(config, next_config)\n\ndef analyze_basin(s_star, J, h, N, R, M):\n    \"\"\"\n    Analyzes the basin of attraction within Hamming radius R.\n    Returns the size of the basin (B) and the mean convergence time (T_bar).\n    \"\"\"\n    basin_size = 0\n    total_steps = 0\n    \n    num_sites = N * N\n    site_indices = np.arange(num_sites)\n\n    # Iterate through all Hamming distances from 0 to R.\n    for k in range(R + 1):\n        # Generate all combinations of k sites to flip.\n        for flip_indices_1d in combinations(site_indices, k):\n            # Create the initial configuration by flipping spins in s_star.\n            initial_config = s_star.copy()\n            if k > 0:\n                # Convert 1D indices to 2D for array indexing.\n                flip_indices_2d = np.unravel_index(list(flip_indices_1d), (N, N))\n                initial_config[flip_indices_2d] *= -1\n\n            # Simulate the trajectory for this initial configuration.\n            current_config = initial_config\n            # Use a set of byte-representations for efficient history tracking (cycle detection).\n            history = {current_config.tobytes()}\n            \n            # Check for convergence within M steps.\n            for step in range(M + 1):\n                if np.array_equal(current_config, s_star):\n                    basin_size += 1\n                    total_steps += step\n                    break\n                \n                # If we've reached the step limit M without converging, stop.\n                if step == M:\n                    break\n\n                current_config = update_step(current_config, J, h)\n                config_bytes = current_config.tobytes()\n                if config_bytes in history:\n                    # Cycle detected, trajectory does not converge to s_star.\n                    break\n                history.add(config_bytes)\n    \n    if basin_size == 0:\n        return 0, 0.0\n    \n    mean_steps = total_steps / basin_size\n    return basin_size, mean_steps\n\ndef process_case(s_star, J, h, perturbations, N, R, M):\n    \"\"\"\n    Processes a single test case to compute b0, b1, B, and T_bar.\n    \"\"\"\n    # Task 1: Check if s_star is a fixed configuration under baseline parameters.\n    b0 = is_fixed_point(s_star, J, h)\n    \n    # Task 2: Check if s_star is stable under all perturbations.\n    if not b0:\n        b1 = False\n    else:\n        b1 = True\n        for dJ, dh in perturbations:\n            if not is_fixed_point(s_star, J + dJ, h + dh):\n                b1 = False\n                break\n    \n    # Tasks 3 & 4: Analyze basin of attraction.\n    # If s_star is not a fixed point, its basin of attraction is considered empty.\n    if not b0:\n        B = 0\n        T_bar = 0.0\n    else:\n        B, T_bar = analyze_basin(s_star, J, h, N, R, M)\n        \n    return [b0, b1, B, T_bar]\n\n# Run the solver.\nsolve()\n```"
        },
        {
            "introduction": "While detailed simulations provide high-fidelity results, their computational cost can be prohibitive for large-scale systems, a common challenge in process modeling. This practice introduces coarse-graining, a vital technique for balancing accuracy and efficiency. You will implement a simulation of domain coarsening and then apply a block-averaging method to run the simulation on a reduced-resolution grid. The core scientific principle is the trade-off between computational savings and information loss, which you will quantify using a formal objective function $\\Phi(b) = \\lambda L(b) + (1 - \\lambda) / b^2$, where $L(b)$ is the error and $b$ is the coarse-graining level. This exercise `` offers critical, hands-on experience in model reduction and optimization, skills essential for developing practical and scalable computational models in any scientific field.",
            "id": "4113070",
            "problem": "Consider a binary Cellular Automaton (CA) on a two-dimensional periodic lattice modeling microstructure domain coarsening in semiconductor manufacturing process modeling. Each lattice site at position $(i,j)$ holds a spin $s_{ij}(t) \\in \\{-1, +1\\}$ at discrete time $t$. The evolution rule is a deterministic majority rule over the Moore neighborhood: at each time step $t \\to t+1$, the spin $s_{ij}(t+1)$ is set to $+1$ if the sum of the $8$ neighboring spins at time $t$ is positive, $-1$ if the sum is negative, and remains unchanged if the sum is exactly zero. The boundary condition is periodic in both directions. This rule approximates local energy descent in an Ising-like interaction and is a standard abstraction for curvature-driven coarsening.\n\nYou will evaluate coarse-graining by block averaging and quantify the trade-off between computational savings and accuracy loss. Let the fine lattice size be $N \\times N$ and the update horizon be $T$ time steps. The fine initial condition $s_{ij}(0)$ is specified deterministically by the sign of a superposition of cosines:\n$$\ns_{ij}(0) = \\begin{cases}\n+1, & \\cos\\left(\\frac{2\\pi k_x i}{N}\\right) + \\cos\\left(\\frac{2\\pi k_y j}{N}\\right) \\ge 0, \\\\\n-1, & \\text{otherwise},\n\\end{cases}\n$$\nfor integer wavenumbers $k_x$ and $k_y$. Define the fine-reference result $s^{\\mathrm{ref}}(T)$ by evolving this fine lattice with the above CA rule for $T$ steps.\n\nFor a given block size $b \\in \\mathbb{N}$ that divides $N$, define coarse-graining as follows:\n- Downsample the fine initial state $s(0)$ to a coarse initial state $S(0)$ of size $(N/b) \\times (N/b)$ by taking the majority spin over each non-overlapping $b \\times b$ block. In case of a tie (sum equal to zero), break the tie by choosing the top-left spin of the block.\n- Evolve $S(t)$ for $T$ steps using the same CA majority rule with periodic boundary conditions.\n- Upsample $S(T)$ back to the fine resolution by repeating each coarse spin into its corresponding $b \\times b$ block, yielding $s^{\\mathrm{cg}\\uparrow}(T)$ of size $N \\times N$.\n\nDefine the normalized Hamming loss $L(b)$ as the fraction of mismatched sites between $s^{\\mathrm{ref}}(T)$ and $s^{\\mathrm{cg}\\uparrow}(T)$:\n$$\nL(b) = \\frac{1}{N^2} \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\mathbb{1}\\left[ s^{\\mathrm{ref}}_{ij}(T) \\ne s^{\\mathrm{cg}\\uparrow}_{ij}(T) \\right].\n$$\nAssume a computational cost proportional to the number of site updates, so the relative cost of block size $b$ is\n$$\nC(b) = \\frac{1}{b^2},\n$$\nand the computational savings is\n$$\nS(b) = 1 - C(b) = 1 - \\frac{1}{b^2}.\n$$\n\nGiven a trade-off parameter $\\lambda \\in [0,1]$, define the scalar objective\n$$\n\\Phi(b) = \\lambda \\, L(b) + (1 - \\lambda) \\, \\big(1 - S(b)\\big) = \\lambda \\, L(b) + (1 - \\lambda) \\, \\frac{1}{b^2}.\n$$\nFor a fixed set of candidate block sizes\n$$\nB = \\{1, 2, 4, 8\\},\n$$\nidentify the optimal resolution\n$$\nb^\\star = \\arg\\min_{b \\in B} \\Phi(b),\n$$\nwith the tie-breaking rule that in case of equal objective values, the smallest $b$ is preferred.\n\nYour program must implement this procedure and evaluate the following test suite. All lattice sizes $N$ are guaranteed to be divisible by all $b \\in B$:\n- Test case $1$: $(N = 128, T = 50, k_x = 4, k_y = 6, \\lambda = 0.5)$.\n- Test case $2$: $(N = 64, T = 5, k_x = 7, k_y = 9, \\lambda = 0.7)$.\n- Test case $3$: $(N = 128, T = 100, k_x = 3, k_y = 5, \\lambda = 0.3)$.\n- Test case $4$: $(N = 128, T = 30, k_x = 31, k_y = 33, \\lambda = 0.6)$.\n- Test case $5$: $(N = 128, T = 0, k_x = 10, k_y = 12, \\lambda = 0.8)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case contributes an inner list of the form $[b^\\star, L(b^\\star), S(b^\\star)]$. Express $L(b^\\star)$ and $S(b^\\star)$ as floating-point values rounded to six decimal places. For example, the output must look like\n$$\n[[b_1,L_1,S_1],[b_2,L_2,S_2],\\ldots,[b_5,L_5,S_5]],\n$$\nwhere $b_m$ is an integer and $L_m$, $S_m$ are floats rounded to six decimals for test case $m$.",
            "solution": "The user has provided a valid problem statement. The task is to evaluate a coarse-graining strategy for a two-dimensional cellular automaton (CA) model of domain coarsening, a process relevant to semiconductor manufacturing process modeling. The quality of the coarse-graining is quantified by an objective function that balances computational savings against accuracy loss. The goal is to find the optimal coarse-graining block size from a predefined set of candidates.\n\n### 1. Model Formulation and Simulation\n\nThe physical system is modeled by a CA on an $N \\times N$ lattice with periodic boundary conditions. Each site $(i,j)$ has a spin $s_{ij}(t) \\in \\{-1, +1\\}$ at discrete time $t$.\n\n**Initial State Generation**:\nThe initial state of the fine-grained lattice, $s(0)$, is deterministically set based on a superposition of sinusoidal waves. For each site $(i,j)$, the spin $s_{ij}(0)$ is determined by the sign of a function:\n$$\ns_{ij}(0) = \\text{sgn}\\left(\\cos\\left(\\frac{2\\pi k_x i}{N}\\right) + \\cos\\left(\\frac{2\\pi k_y j}{N}\\right)\\right)\n$$\nwhere $\\text{sgn}(x)$ is defined to be $+1$ for $x \\ge 0$ and $-1$ for $x < 0$. This is implemented by creating a meshgrid of indices for $i$ and $j$ from $0$ to $N-1$ and applying the formula in a vectorized manner using NumPy.\n\n**Cellular Automaton Evolution Rule**:\nThe system evolves in discrete time steps $t \\to t+1$. The spin at each site is updated based on a majority rule over its Moore neighborhood, which consists of the $8$ adjacent sites. The new spin $s_{ij}(t+1)$ is determined as follows:\n$$\ns_{ij}(t+1) = \\begin{cases}\n+1, & \\text{if } \\sum_{\\text{neighbors}} s(t) > 0 \\\\\n-1, & \\text{if } \\sum_{\\text{neighbors}} s(t) < 0 \\\\\ns_{ij}(t), & \\text{if } \\sum_{\\text{neighbors}} s(t) = 0\n\\end{cases}\n$$\nThis rule is applied simultaneously to all sites. The sum over neighbors can be efficiently computed as a 2D convolution. We define a kernel representing the Moore neighborhood:\n$$\nK = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n$$\nThe sum of neighbors at each site is then the result of convolving the lattice $s(t)$ with the kernel $K$. The `scipy.signal.convolve2d` function with `mode='same'` and `boundary='wrap'` is perfectly suited for this, as it naturally handles the periodic boundary conditions. For each of the $T$ time steps, we compute this convolution and update the lattice according to the rule.\n\n### 2. Coarse-Graining Methodology\n\nThe core of the problem is to assess a coarse-graining strategy. For a given integer block size $b$ (that divides $N$), the process involves three steps:\n\n**1. Downsampling**: The initial $N \\times N$ fine lattice $s(0)$ is downsampled to a coarse lattice $S(0)$ of size $(N/b) \\times (N/b)$. For each non-overlapping $b \\times b$ block in $s(0)$, the corresponding coarse spin in $S(0)$ is determined by the majority spin within that block. If the sum of spins in a block is $0$, a tie-breaking rule is applied: the coarse spin is set to the value of the top-left spin of the block. This operation is implemented efficiently by reshaping the $N \\times N$ array into an $(N/b) \\times (N/b) \\times b \\times b$ array, summing over the last two dimensions to get block sums, and then applying the sign and tie-breaking rules.\n\n**2. Coarse Evolution**: The coarse lattice $S(0)$ is evolved for $T$ time steps using the exact same CA evolution rule as the fine lattice. This yields the final coarse state $S(T)$. Since the lattice is smaller, this step is computationally cheaper.\n\n**3. Upsampling**: The final coarse state $S(T)$ is upsampled back to an $N \\times N$ lattice, $s^{\\mathrm{cg}\\uparrow}(T)$, by repeating each coarse spin value across its corresponding $b \\times b$ block. This is efficiently achieved using the Kronecker product, `numpy.kron`, between the coarse lattice and a $b \\times b$ block of ones.\n\nThe evolution of the original fine lattice for $T$ steps produces the reference result, $s^{\\mathrm{ref}}(T)$.\n\n### 3. Optimization and Evaluation\n\nThe trade-off between accuracy and computational cost is formalized through an objective function $\\Phi(b)$.\n\n**Metrics**:\n- The **normalized Hamming loss**, $L(b)$, measures the discrepancy between the reference and the coarse-grained results. It is the fraction of sites where the spins differ:\n$$\nL(b) = \\frac{1}{N^2} \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\mathbb{1}\\left[ s^{\\mathrm{ref}}_{ij}(T) \\ne s^{\\mathrm{cg}\\uparrow}_{ij}(T) \\right]\n$$\n- The **computational savings**, $S(b)$, is defined based on the reduction in the number of site updates. For a $b \\times b$ block, we perform one update instead of $b^2$, so the relative cost is $C(b) = 1/b^2$ and the savings is $S(b) = 1 - C(b) = 1 - 1/b^2$. For $b=1$, there is no coarse-graining, so the loss $L(1)$ is $0$ and the savings $S(1)$ is $0$.\n\n**Objective Function**:\nThe scalar objective $\\Phi(b)$ is a weighted sum of the loss and the computational cost (which is $1-S(b)$):\n$$\n\\Phi(b) = \\lambda \\, L(b) + (1 - \\lambda) \\, (1 - S(b)) = \\lambda \\, L(b) + (1 - \\lambda) \\frac{1}{b^2}\n$$\nwhere $\\lambda \\in [0,1]$ is a trade-off parameter. A high $\\lambda$ prioritizes accuracy (low $L(b)$), while a low $\\lambda$ prioritizes low cost (high $b$).\n\n**Optimization**:\nThe goal is to find the optimal block size $b^\\star$ from the candidate set $B = \\{1, 2, 4, 8\\}$ that minimizes the objective function:\n$$\nb^\\star = \\arg\\min_{b \\in B} \\Phi(b)\n$$\nIn cases where multiple block sizes yield the same minimum $\\Phi$ value, the smallest block size is chosen.\n\nFor each test case, the algorithm iterates through each $b \\in B$, performs the full fine-grained simulation to get $s^{\\mathrm{ref}}(T)$, performs the coarse-graining procedure to get $s^{\\mathrm{cg}\\uparrow}(T)$, calculates $L(b)$ and $S(b)$, evaluates $\\Phi(b)$, and finally identifies the optimal $b^\\star$ based on the minimum $\\Phi$ value and the tie-breaking rule. The final output is a list containing the results for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef generate_initial_state(N, kx, ky):\n    \"\"\"Generates the initial N x N lattice based on the cosine formula.\"\"\"\n    i = np.arange(N)\n    j = np.arange(N)\n    I, J = np.meshgrid(i, j, indexing='ij')\n    vals = np.cos(2 * np.pi * kx * I / N) + np.cos(2 * np.pi * ky * J / N)\n    lattice = np.ones((N, N), dtype=np.int8)\n    lattice[vals < 0] = -1\n    return lattice\n\ndef evolve(lattice, T):\n    \"\"\"Evolves a lattice for T steps using the specified CA rule.\"\"\"\n    if T == 0:\n        return lattice\n\n    current_lattice = lattice.copy()\n    KERNEL = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=np.int8)\n\n    for _ in range(T):\n        neighbor_sum = convolve2d(current_lattice, KERNEL, mode='same', boundary='wrap')\n        \n        # The rule is deterministic, so we can prepare the next state\n        # A copy is needed because some cells might not change (sum=0)\n        next_lattice = current_lattice.copy()\n        next_lattice[neighbor_sum > 0] = 1\n        next_lattice[neighbor_sum < 0] = -1\n        current_lattice = next_lattice\n\n    return current_lattice\n\ndef solve_case(N, T, kx, ky, lambda_val):\n    \"\"\"\n    Solves a single test case: finds the optimal block size b_star and\n    the corresponding L(b_star) and S(b_star).\n    \"\"\"\n    B = [1, 2, 4, 8]\n    \n    # Generate fine initial state\n    s0_fine = generate_initial_state(N, kx, ky)\n    \n    # Get reference result by evolving the fine grid\n    sT_ref = evolve(s0_fine, T)\n    \n    results_by_b = {}\n    \n    for b in B:\n        S_b = 1.0 - 1.0 / (b**2)\n        \n        if b == 1:\n            # No coarse-graining, so loss is 0\n            L_b = 0.0\n            sT_cg_up = sT_ref\n        else:\n            # 1. Downsample\n            Nc = N // b\n            \n            # Reshape for efficient block processing\n            fine_reshaped = s0_fine.reshape(Nc, b, Nc, b).transpose(0, 2, 1, 3)\n            block_sums = fine_reshaped.sum(axis=(2, 3))\n            \n            # Majority rule\n            S0_coarse = np.ones((Nc, Nc), dtype=np.int8)\n            S0_coarse[block_sums < 0] = -1\n            \n            # Tie-breaking rule\n            tie_indices = np.where(block_sums == 0)\n            if tie_indices[0].size > 0:\n                top_lefts = fine_reshaped[:, :, 0, 0]\n                S0_coarse[tie_indices] = top_lefts[tie_indices]\n\n            # 2. Evolve coarse grid\n            ST_coarse = evolve(S0_coarse, T)\n            \n            # 3. Upsample\n            ones_block = np.ones((b, b), dtype=np.int8)\n            sT_cg_up = np.kron(ST_coarse, ones_block)\n            \n            # Calculate loss\n            L_b = np.sum(sT_ref != sT_cg_up) / (N * N)\n            \n        # Calculate objective function\n        phi = lambda_val * L_b + (1.0 - lambda_val) * (1.0 - S_b)\n        results_by_b[b] = {'phi': phi, 'L': L_b, 'S': S_b}\n\n    # Find optimal b by sorting on (phi, b) tuple\n    # This automatically handles the tie-breaking rule (prefer smaller b)\n    best_b = min(results_by_b, key=lambda b_key: (results_by_b[b_key]['phi'], b_key))\n\n    L_star = results_by_b[best_b]['L']\n    S_star = results_by_b[best_b]['S']\n    \n    return [best_b, L_star, S_star]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (128, 50, 4, 6, 0.5),\n        (64, 5, 7, 9, 0.7),\n        (128, 100, 3, 5, 0.3),\n        (128, 30, 31, 33, 0.6),\n        (128, 0, 10, 12, 0.8),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        res = solve_case(*case)\n        all_results.append(res)\n    \n    # Format the output string without spaces as required.\n    inner_parts = []\n    for res in all_results:\n        b_star, L_star, S_star = res\n        inner_parts.append(f\"[{b_star},{L_star:.6f},{S_star:.6f}]\")\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond abstract pattern formation, cellular automata can be designed to quantitatively model complex physical phenomena. This advanced practice challenges you to build a CA that simulates directional solidification, a key process in materials science and semiconductor crystal growth. The model you will develop captures the essential physics of the Mullins-Sekerka instability, where a stable, planar solidification front breaks down into a cellular or dendritic pattern due to the interplay between a destabilizing thermal or concentration gradient and stabilizing forces like diffusion and interface curvature. By comparing the stability threshold predicted by your CA simulation with results from a formal linear stability analysis ``, you will learn how to bridge the conceptual gap between discrete, rule-based models and continuum field theories, a cornerstone of modern computational materials science.",
            "id": "4113123",
            "problem": "Design and implement a two-dimensional Cellular Automaton (CA) model for directional solidification relevant to semiconductor manufacturing process modeling that captures the competition between a planar solidification front and the emergence of cellular patterns. The model must be grounded on the following foundational base and assumptions, and all quantities are to be dimensionless:\n\n1. Base physical law: use Fick's second law of diffusion for a scalar field interpreted as an undercooling or supersaturation field, expressed as the partial differential equation $$\\frac{\\partial u}{\\partial t} = D \\nabla^2 u,$$ where $u$ is the scalar field, $t$ is time, $D$ is a positive diffusion coefficient, and $\\nabla^2$ is the Laplacian operator. Implement a discrete version on a uniform square lattice with spacing $\\Delta x = 1$ and explicit time stepping $\\Delta t = 1$ using the standard five-point stencil for the Laplacian.\n\n2. Interface kinetics: approximate normal interface velocity by a linear kinetic law combining local drive and curvature penalty, $$v_n \\propto \\alpha u_{\\mathrm{int}} - \\beta \\kappa,$$ where $\\alpha > 0$ is an interface mobility coefficient, $\\beta \\ge 0$ is an effective capillarity coefficient, $u_{\\mathrm{int}}$ is the local scalar field immediately ahead of the interface, and $\\kappa$ is the local interface curvature. Model the interface as the top boundary of a solid region; curvature may be approximated by a discrete second difference in the interface height.\n\n3. Boundary conditions: impose a fixed scalar field at the top and bottom boundaries that drives a linear background gradient with magnitude proportional to a control parameter $G$. Use periodic boundary conditions in the horizontal direction.\n\n4. Initialization: initialize a planar interface with a small sinusoidal height perturbation of amplitude $A_0$ and wavenumber $k = \\frac{2\\pi m}{N_x}$, where $m$ is an integer mode index and $N_x$ is the horizontal grid size. The solid occupies the region below the interface. Initialize the scalar field to a linear profile consistent with the boundary conditions.\n\n5. Empirical stability assessment: numerically estimate the temporal growth rate of the sinusoidal amplitude by $$\\sigma_{\\mathrm{emp}}(k) = \\frac{1}{T_{\\mathrm{eff}}} \\ln\\left(\\frac{A(T_{\\mathrm{eff}})}{A(0)}\\right),$$ where $A(t)$ is the amplitude of the selected Fourier mode of the interface height at time $t$, extracted from a discrete Fourier transform of the height field, and $T_{\\mathrm{eff}}$ is the number of time steps actually simulated (to account for early termination if the interface reaches the domain boundary). The empirical critical gradient $G_{\\mathrm{c,CA}}(k)$ is the value of $G$ at which $\\sigma_{\\mathrm{emp}}(k)$ changes sign, estimated by scanning $G$ over a range and selecting the value closest to zero growth rate.\n\n6. Analytical comparison: starting from Fick's second law and the linear kinetic law for the interface, perform a linear stability analysis in the discrete setting to obtain a dispersion relation of the form $$\\sigma(k;G) = \\alpha G - \\mathcal{D}(k) - \\beta k^2,$$ where $\\mathcal{D}(k)$ is the discrete diffusion contribution. For the five-point stencil on a square lattice with spacing $\\Delta x = 1$, the plane-wave response yields $$\\mathcal{D}(k) = 2D\\left(1 - \\cos(k)\\right),$$ for perturbations periodic in the horizontal direction. The analytical critical gradient is then $$G_{\\mathrm{c,an}}(k) = \\frac{\\mathcal{D}(k) + \\beta k^2}{\\alpha}.$$\n\n7. Output quantity: for each test case, compute the difference $$\\Delta G(k) = G_{\\mathrm{c,CA}}(k) - G_{\\mathrm{c,an}}(k).$$\n\nThe CA update must adhere to the following steps per time step:\n- Diffuse the scalar field $u$ using the discrete Laplacian with diffusion coefficient $D$ and enforce Dirichlet boundary conditions at the top $(y=0)$ and bottom $(y=N_y-1)$ consistent with a vertical gradient controlled by $G$. Use periodic boundary conditions in $x$.\n- Compute the discrete interface curvature $\\kappa$ using second differences of the interface height.\n- Advance the interface upward by one lattice site at columns where the interface drive is positive according to the rule: a liquid cell just above the interface solidifies if $\\alpha u_{\\mathrm{int}} - \\beta \\kappa > 0$. Reduce $u$ locally at newly solidified sites by a small consumption amount to mimic latent heat or solute uptake. The solid region is updated accordingly.\n- Measure the interface amplitude $A(t)$ for the specified mode $m$ using the magnitude of the corresponding discrete Fourier coefficient of the interface height deviation from its mean.\n\nTest suite and specifications:\n- Use a uniform grid with $N_x = 64$ and $N_y = 64$, spacing $\\Delta x = 1$ and time step $\\Delta t = 1$.\n- Use the following three test cases, each specified by $(m, D, \\alpha, \\beta, A_0, T)$:\n    1. $(m=2, D=0.2, \\alpha=0.7, \\beta=0.05, A_0=2.0, T=180)$\n    2. $(m=2, D=0.01, \\alpha=0.5, \\beta=0.0, A_0=2.0, T=180)$\n    3. $(m=4, D=0.15, \\alpha=0.9, \\beta=0.25, A_0=2.0, T=180)$\n- For each test case, determine $G_{\\mathrm{c,CA}}(k)$ by scanning $G$ over a suitable range centered around $G_{\\mathrm{c,an}}(k)$ and selecting the $G$ giving $\\sigma_{\\mathrm{emp}}(k)$ closest to zero.\n- The final program output must be a single line containing a comma-separated list enclosed in square brackets with the three values $[\\Delta G_1,\\Delta G_2,\\Delta G_3]$ for the test cases in the given order. Values must be decimals in dimensionless units.\n\nThe program must be self-contained and must not require any user input. It must implement the CA, perform the empirical stability analysis, compute the analytical predictions, and print the final single-line output in the specified format. No other output is permitted.",
            "solution": "The repository of scientific knowledge establishes that the problem is valid. It presents a well-posed computational physics task grounded in the established principles of pattern formation during directional solidification, specifically the Mullins-Sekerka instability. The problem asks for a comparison between a direct numerical simulation using a Cellular Automaton (CA) and a linearized analytical stability analysis. The objective is to quantify the difference between the critical thermal gradient for instability predicted by these two methods.\n\nThe solution proceeds in two main stages for each test case: first, calculating the analytical critical gradient $G_{\\mathrm{c,an}}(k)$, and second, numerically determining the empirical critical gradient $G_{\\mathrm{c,CA}}(k)$ via direct simulation. The final result is the difference $\\Delta G(k) = G_{\\mathrm{c,CA}}(k) - G_{\\mathrm{c,an}}(k)$.\n\n### Analytical Critical Gradient, $G_{\\mathrm{c,an}}(k)$\n\nThe problem provides the analytical dispersion relation for the growth rate $\\sigma$ of a sinusoidal perturbation with wavenumber $k$ under an imposed gradient $G$:\n$$\n\\sigma(k;G) = \\alpha G - \\mathcal{D}(k) - \\beta k^2\n$$\nHere, $\\alpha G$ is the destabilizing term from the constitutional undercooling/supersaturation gradient. $\\mathcal{D}(k)$ is a stabilizing term due to diffusion, and $\\beta k^2$ is a stabilizing term due to capillarity (interface energy). The critical condition for stability is when the growth rate is zero, $\\sigma(k;G) = 0$. Solving for the gradient $G$ at this threshold gives the analytical critical gradient, $G_{\\mathrm{c,an}}(k)$.\n\nGiven the discrete diffusion term for a five-point stencil on a lattice with spacing $\\Delta x = 1$:\n$$\n\\mathcal{D}(k) = 2D(1 - \\cos(k))\n$$\nThe analytical critical gradient is found by setting $\\sigma(k;G) = 0$:\n$$\n\\alpha G_{\\mathrm{c,an}}(k) - 2D(1 - \\cos(k)) - \\beta k^2 = 0\n$$\n$$\nG_{\\mathrm{c,an}}(k) = \\frac{2D(1 - \\cos(k)) + \\beta k^2}{\\alpha}\n$$\nFor each test case, the wavenumber $k$ is determined by the mode index $m$ and the horizontal grid size $N_x = 64$:\n$$\nk = \\frac{2\\pi m}{N_x}\n$$\nThis formula is used to compute the reference critical gradient for each set of parameters $(m, D, \\alpha, \\beta)$.\n\n### Numerical Critical Gradient, $G_{\\mathrm{c,CA}}(k)$\n\nThe empirical critical gradient, $G_{\\mathrm{c,CA}}(k)$, is determined by performing a series of CA simulations over a range of candidate gradients $G$ and identifying the value for which the perturbation's growth rate is closest to zero.\n\n**1. Simulation Setup**\n\nFor each simulation, a two-dimensional grid of size $N_x = 64 \\times N_y = 64$ is initialized. The coordinate system is defined with $y=0$ at the top and $y=N_y-1$ at the bottom.\n\n- **Scalar Field $u$**: The scalar field $u$, representing supersaturation, is initialized with a linear profile consistent with the imposed gradient $G$. Boundary conditions are fixed (Dirichlet) at the top and bottom: $u(x, y=0) = G(N_y-1)$ and $u(x, y=N_y-1) = 0$. The initial field is thus $u(x, y) = G \\cdot (N_y - 1 - y)$.\n- **Interface**: The interface is represented by an integer array of heights $h(x)$, where $h(x)$ is the y-coordinate of the solid-liquid boundary in column $x$. The solid phase exists for $y \\ge h(x)$. The interface is initialized near the bottom of the domain (at a mean height of $N_y - 20$) with a sinusoidal perturbation of amplitude $A_0$ and wavenumber $k$:\n  $$\n  h(x, t=0) = \\text{round}\\left( (N_y - 20) - A_0 \\cos(kx) \\right)\n  $$\n- **Phase Field**: A phase field $\\phi(x,y)$ is used to distinguish between liquid ($\\phi=0$) and solid ($\\phi=1$) cells.\n\n**2. CA Time Evolution**\n\nThe system evolves over a maximum of $T$ time steps, with $\\Delta t=1$. Each step consists of the following sequential updates:\n\n- **Diffusion of $u$**: The scalar field $u$ evolves according to the explicit finite-difference form of Fick's second law, $\\frac{\\partial u}{\\partial t} = D \\nabla^2 u$. The Laplacian $\\nabla^2 u$ is computed using a five-point stencil with periodic boundary conditions in the $x$-direction.\n  $$\n  u(x,y,t+\\Delta t) = u(x,y,t) + D \\Delta t \\left[ u(x+1,y,t) + u(x-1,y,t) + u(x,y+1,t) + u(x,y-1,t) - 4u(x,y,t) \\right]\n  $$\n  After the diffusion update, the fixed boundary conditions at $y=0$ and $y=N_y-1$ are re-imposed. Furthermore, the supersaturation within the solid region is set to zero, $u(x,y) = 0$ for all cells where $\\phi(x,y)=1$.\n\n- **Interface Advancement**: The interface motion is governed by a discrete rule based on the local driving force.\n    - The local supersaturation at the interface is $u_{\\mathrm{int}}(x) = u(x, h(x)-1, t+\\Delta t)$.\n    - The local interface curvature $\\kappa(x)$ is approximated by the central second difference of the height profile: $\\kappa(x) = h(x+1) + h(x-1) - 2h(x)$.\n    - The driving force is $F_{\\text{drive}}(x) = \\alpha u_{\\mathrm{int}}(x) - \\beta \\kappa(x)$.\n    - For each column $x$ where $F_{\\text{drive}}(x) > 0$, the interface advances upward by one lattice site: $h(x) \\to h(x) - 1$. This threshold-based, discrete advancement is a key distinction from the continuous velocity law assumed in the analytical model.\n\n- **Post-advancement Update**: Following interface advancement, any newly solidified cells have their local supersaturation consumed by setting $u=0$ at their locations.\n\n**3. Growth Rate Measurement**\n\nThe amplitude of the perturbation, $A(t)$, is measured at each time step. The mean height is subtracted from the interface profile, $h'(x,t) = h(x,t) - \\langle h \\rangle_x$, and a Discrete Fourier Transform (DFT) is performed on $h'(x,t)$. The amplitude of the mode $m$ is extracted using the standard definition for real signals:\n$$\nA(t) = \\frac{2}{N_x} |\\hat{h}'(m,t)|\n$$\nwhere $\\hat{h}'(m,t)$ is the $m$-th complex coefficient from the DFT. The initial amplitude $A(0)$ is measured from the discretized initial interface, and the final amplitude $A(T_{\\mathrm{eff}})$ is measured at the end of the simulation. The effective simulation time, $T_{\\mathrm{eff}}$, is the number of steps completed before the simulation terminates, which may be less than the maximum time $T$ if the interface reaches a domain boundary.\n\nThe empirical growth rate is then computed as:\n$$\n\\sigma_{\\mathrm{emp}}(k) = \\frac{1}{T_{\\mathrm{eff}}} \\ln\\left(\\frac{A(T_{\\mathrm{eff}})}{A(0)}\\right)\n$$\n\n**4. Finding $G_{\\mathrm{c,CA}}(k)$**\n\nFor each test case, a one-dimensional search is conducted over a range of candidate $G$ values centered on the analytical prediction $G_{\\mathrm{c,an}}(k)$. For each $G_{\\text{test}}$, a full CA simulation is run to compute $\\sigma_{\\mathrm{emp}}(k)$. The value of $G_{\\text{test}}$ that yields a growth rate closest to zero is selected as the numerical critical gradient $G_{\\mathrm{c,CA}}(k)$.\n\nThis procedure systematically determines the numerical stability threshold, allowing for a direct comparison with the analytical prediction and the calculation of $\\Delta G(k)$. The primary sources of discrepancy $\\Delta G(k)$ include the CA's discrete, thresholded interface advancement rule versus the continuous velocity law of the analytical model, and the use of the continuous curvature approximation $\\beta k^2$ in the analytical formula instead of the discrete representation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the complete analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (m, D, alpha, beta, A0, T)\n        (2, 0.2, 0.7, 0.05, 2.0, 180),\n        (2, 0.01, 0.5, 0.0, 2.0, 180),\n        (4, 0.15, 0.9, 0.25, 2.0, 180),\n    ]\n\n    delta_g_results = []\n\n    for m, D, alpha, beta, A0, T in test_cases:\n        params = {\n            'm': m, 'D': D, 'alpha': alpha, 'beta': beta,\n            'A0': A0, 'T': T,\n            'Nx': 64, 'Ny': 64\n        }\n        \n        # 1. Calculate analytical critical gradient\n        k = 2 * np.pi * m / params['Nx']\n        D_k = 2 * D * (1 - np.cos(k))\n        g_c_analytical = (D_k + beta * k**2) / alpha\n        \n        # 2. Find numerical critical gradient by scanning G\n        g_c_numerical = find_g_crit_ca(params, g_c_analytical)\n        \n        # 3. Compute and store the difference\n        delta_g = g_c_numerical - g_c_analytical\n        delta_g_results.append(delta_g)\n\n    print(f\"[{','.join(f'{g:.6f}' for g in delta_g_results)}]\")\n\ndef find_g_crit_ca(params, g_an_estimate):\n    \"\"\"\n    Scans a range of G values around an estimate to find the numerical critical gradient.\n    \"\"\"\n    if g_an_estimate == 0:\n        # Handle cases where analytical G is zero or very small\n        g_range = np.linspace(-0.01, 0.01, 21)\n    else:\n        # Scan a range from 50% to 150% of the analytical value\n        g_range = np.linspace(g_an_estimate * 0.5, g_an_estimate * 1.5, 21)\n    \n    sigmas = []\n    for g_test in g_range:\n        current_params = params.copy()\n        current_params['G'] = g_test\n        sigma = run_ca_simulation(current_params)\n        sigmas.append(sigma)\n    \n    sigmas = np.array(sigmas)\n    # Find the G that results in sigma closest to 0\n    closest_idx = np.argmin(np.abs(sigmas))\n    \n    return g_range[closest_idx]\n\ndef run_ca_simulation(params):\n    \"\"\"\n    Runs a single Cellular Automaton simulation for a given set of parameters.\n    \"\"\"\n    # Unpack parameters\n    m, D, alpha, beta, A0, T = params['m'], params['D'], params['alpha'], params['beta'], params['A0'], params['T']\n    Nx, Ny, G = params['Nx'], params['Ny'], params['G']\n\n    k = 2 * np.pi * m / Nx\n    x_coords = np.arange(Nx)\n    \n    # Initialization\n    # Scalar field u\n    y_coords = np.arange(Ny).reshape(-1, 1)\n    u = G * (Ny - 1 - y_coords)\n    u = np.broadcast_to(u, (Ny, Nx)).copy()\n    \n    # Interface height h\n    h_mean = Ny - 20\n    h_initial = np.round(h_mean - A0 * np.cos(k * x_coords)).astype(int)\n    h = h_initial.copy()\n    \n    # Phase field (1 for solid, 0 for liquid)\n    y_grid, _ = np.mgrid[0:Ny, 0:Nx]\n    phase_mask = y_grid >= h\n    u[phase_mask] = 0.0\n\n    # Measure initial amplitude\n    h_dev_initial = h_initial - np.mean(h_initial)\n    fft_initial = np.fft.fft(h_dev_initial)\n    A_initial = (2 / Nx) * np.abs(fft_initial[m])\n\n    if A_initial == 0:\n        return np.inf  # Undefined growth\n\n    T_eff = T\n    for t in range(1, T + 1):\n        # Diffusion Step\n        lap_u = (np.roll(u, 1, axis=1) + np.roll(u, -1, axis=1) +\n                 np.roll(u, 1, axis=0) + np.roll(u, -1, axis=0) - 4.0 * u)\n        u += D * lap_u\n\n        # Set u=0 in the solid region\n        solid_mask = y_grid >= h\n        u[solid_mask] = 0.0\n\n        # Enforce Dirichlet BCs\n        u[0, :] = G * (Ny - 1)\n        u[Ny-1, :] = 0.0\n        \n        # Interface Advancement Step\n        # Check for boundary collision\n        if np.any(h <= 1) or np.any(h >= Ny - 2):\n            T_eff = t - 1\n            break\n\n        # Get u just above the interface\n        u_interface = u[h - 1, x_coords]\n        \n        # Calculate curvature\n        kappa = np.roll(h, 1) + np.roll(h, -1) - 2 * h\n        \n        # Calculate drive and advance interface\n        drive = alpha * u_interface - beta * kappa\n        to_advance = drive > 0\n        \n        h_old = h.copy()\n        h[to_advance] -= 1\n        \n        # Consume supersaturation at newly solidified cells\n        newly_solidified_mask_x = h < h_old\n        if np.any(newly_solidified_mask_x):\n            new_solid_y_coords = h[newly_solidified_mask_x]\n            new_solid_x_coords = x_coords[newly_solidified_mask_x]\n            u[new_solid_y_coords, new_solid_x_coords] = 0.0\n    \n    if T_eff <= 0:\n        return np.inf # Simulation ended immediately\n\n    # Measure final amplitude\n    h_dev_final = h - np.mean(h)\n    fft_final = np.fft.fft(h_dev_final)\n    A_final = (2 / Nx) * np.abs(fft_final[m])\n\n    if A_final == 0:\n        return -np.inf # Perturbation died out completely\n\n    # Calculate empirical growth rate\n    sigma_emp = (1 / T_eff) * np.log(A_final / A_initial)\n    \n    return sigma_emp\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}