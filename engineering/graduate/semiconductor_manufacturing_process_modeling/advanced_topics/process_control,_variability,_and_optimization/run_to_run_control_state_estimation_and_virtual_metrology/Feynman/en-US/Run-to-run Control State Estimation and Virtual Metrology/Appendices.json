{
    "hands_on_practices": [
        {
            "introduction": "Before implementing any control or estimation scheme, it is crucial to first characterize the natural variability of the process. This exercise guides you through the fundamental analysis of an open-loop system, using the autocovariance function to dissect the measured output variance into its two primary components: temporally correlated process drift and uncorrelated measurement noise. Mastering this decomposition  is the first step toward designing effective estimators and controllers, as it clarifies what part of the variation can be predicted and controlled.",
            "id": "4162423",
            "problem": "A single-input single-output wafer-to-wafer model for a semiconductor process under open-loop operation is described by a linear time-invariant state evolution and measurement relation. The scalar tool state $x_k$ evolves according to $x_{k+1} = \\phi x_k + b u + w_k$ and the measured output (metrology) is $y_k = c x_k + v_k$, where $k \\in \\mathbb{Z}$ indexes wafer runs, $|\\phi| < 1$, and $u$ is a constant recipe setting held fixed for all runs. The process disturbance $w_k$ is independent and identically distributed (IID) zero-mean white noise with variance $\\sigma_w^2$, the measurement noise $v_k$ is IID zero-mean white noise with variance $\\sigma_v^2$, and $\\{w_k\\}$, $\\{v_k\\}$, and $x_0$ are mutually independent. Assume the system has reached wide-sense steady state so that the first and second moments are time-invariant.\n\nStarting only from fundamental definitions of wide-sense stationarity and autocovariance, and the properties of linear time-invariant systems driven by independent white noise, derive a closed-form expression for the autocovariance function of the measured output $y_k$ at integer lag $\\ell \\in \\mathbb{Z}$,\n$$\\Gamma_y(\\ell) = \\mathbb{E}\\!\\left[(y_{k+\\ell} - \\mu_y)(y_k - \\mu_y)\\right],$$\nin terms of $\\phi$, $b$, $u$, $c$, $\\sigma_w^2$, and $\\sigma_v^2$, where $\\mu_y = \\mathbb{E}[y_k]$ is the steady-state mean. Your derivation should clearly identify how the process disturbance variance $\\sigma_w^2$ and the measurement noise variance $\\sigma_v^2$ shape the observed run-to-run variability at zero and nonzero lags. You may use the Kronecker delta $\\delta_{\\ell,0}$, defined by $\\delta_{\\ell,0} = 1$ if $\\ell = 0$ and $\\delta_{\\ell,0} = 0$ otherwise, to concisely express your result.\n\nProvide the final answer as a single closed-form analytical expression for $\\Gamma_y(\\ell)$. No numerical evaluation or rounding is required.",
            "solution": "The problem provides a single-input single-output (SISO) linear time-invariant (LTI) state-space model describing a semiconductor process. The state evolution and measurement equations are given by:\n$$x_{k+1} = \\phi x_k + b u + w_k$$\n$$y_k = c x_k + v_k$$\nwhere $k \\in \\mathbb{Z}$ is the run index, $x_k$ is the scalar state, $y_k$ is the scalar output, and $u$ is a constant input. The model parameters $\\phi$, $b$, and $c$ are constants, with the stability condition $|\\phi| < 1$. The process disturbance $w_k$ and measurement noise $v_k$ are independent and identically distributed (IID) zero-mean white noise processes with variances $\\sigma_w^2$ and $\\sigma_v^2$, respectively. This implies $\\mathbb{E}[w_k] = 0$, $\\mathbb{E}[v_k] = 0$, $\\mathbb{E}[w_k w_j] = \\sigma_w^2 \\delta_{k,j}$, and $\\mathbb{E}[v_k v_j] = \\sigma_v^2 \\delta_{k,j}$, where $\\delta_{k,j}$ is the Kronecker delta. The sequences $\\{w_k\\}$, $\\{v_k\\}$, and the initial state $x_0$ are mutually independent. The system is assumed to be in a wide-sense steady state (WSS), meaning the first and second moments of the stochastic processes are time-invariant.\n\nOur objective is to derive the autocovariance function of the output, $\\Gamma_y(\\ell) = \\mathbb{E}[(y_{k+\\ell} - \\mu_y)(y_k - \\mu_y)]$, where $\\mu_y = \\mathbb{E}[y_k]$ is the steady-state mean of the output.\n\nFirst, we determine the steady-state mean of the state, $\\mu_x$, and the output, $\\mu_y$. In WSS, $\\mathbb{E}[x_{k+1}] = \\mathbb{E}[x_k] = \\mu_x$. Taking the expectation of the state equation:\n$$\\mathbb{E}[x_{k+1}] = \\mathbb{E}[\\phi x_k + b u + w_k]$$\n$$\\mu_x = \\phi \\mathbb{E}[x_k] + b u + \\mathbb{E}[w_k]$$\nSubstituting $\\mathbb{E}[x_k] = \\mu_x$ and $\\mathbb{E}[w_k] = 0$:\n$$\\mu_x = \\phi \\mu_x + b u$$\n$$\\mu_x(1 - \\phi) = b u$$\nSince $|\\phi| < 1$, we have $1 - \\phi \\neq 0$, so the mean of the state is:\n$$\\mu_x = \\frac{b u}{1 - \\phi}$$\nThe mean of the output, $\\mu_y$, is found by taking the expectation of the output equation:\n$$\\mu_y = \\mathbb{E}[y_k] = \\mathbb{E}[c x_k + v_k] = c \\mathbb{E}[x_k] + \\mathbb{E}[v_k]$$\nSubstituting $\\mathbb{E}[x_k] = \\mu_x$ and $\\mathbb{E}[v_k] = 0$:\n$$\\mu_y = c \\mu_x = \\frac{c b u}{1 - \\phi}$$\n\nNext, we define the centered (zero-mean) state and output variables as $\\tilde{x}_k = x_k - \\mu_x$ and $\\tilde{y}_k = y_k - \\mu_y$. The dynamics of the centered state are found by subtracting the mean dynamics:\n$$(x_{k+1} - \\mu_x) = (\\phi x_k + b u + w_k) - (\\phi \\mu_x + b u)$$\n$$\\tilde{x}_{k+1} = \\phi (x_k - \\mu_x) + w_k = \\phi \\tilde{x}_k + w_k$$\nThe autocovariance function of the state is $\\Gamma_x(\\ell) = \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k]$. For lag $\\ell = 0$, this gives the state variance, $\\sigma_x^2 = \\Gamma_x(0) = \\mathbb{E}[\\tilde{x}_k^2]$. In WSS, $\\mathbb{E}[\\tilde{x}_{k+1}^2] = \\mathbb{E}[\\tilde{x}_k^2] = \\sigma_x^2$.\n$$\\sigma_x^2 = \\mathbb{E}[(\\phi \\tilde{x}_k + w_k)^2] = \\mathbb{E}[\\phi^2 \\tilde{x}_k^2 + 2\\phi \\tilde{x}_k w_k + w_k^2]$$\n$$\\sigma_x^2 = \\phi^2 \\mathbb{E}[\\tilde{x}_k^2] + 2\\phi \\mathbb{E}[\\tilde{x}_k w_k] + \\mathbb{E}[w_k^2]$$\nThe state $\\tilde{x}_k$ is a function of past disturbances $\\{w_j\\}_{j<k}$. Since $\\{w_k\\}$ is an IID sequence, $w_k$ is independent of $\\tilde{x}_k$, so $\\mathbb{E}[\\tilde{x}_k w_k] = \\mathbb{E}[\\tilde{x}_k]\\mathbb{E}[w_k] = 0$. With $\\mathbb{E}[w_k^2] = \\sigma_w^2$, we have:\n$$\\sigma_x^2 = \\phi^2 \\sigma_x^2 + \\sigma_w^2$$\n$$\\sigma_x^2(1 - \\phi^2) = \\sigma_w^2$$\nSince $|\\phi|<1$, $1-\\phi^2 > 0$, and the steady-state variance is:\n$$\\sigma_x^2 = \\frac{\\sigma_w^2}{1 - \\phi^2}$$\nFor a general lag $\\ell > 0$, we iterate the centered state equation: $\\tilde{x}_{k+\\ell} = \\phi^\\ell \\tilde{x}_k + \\sum_{i=0}^{\\ell-1} \\phi^i w_{k+\\ell-1-i}$.\nThen, the autocovariance is:\n$$\\Gamma_x(\\ell) = \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k] = \\mathbb{E}\\left[\\left(\\phi^\\ell \\tilde{x}_k + \\sum_{i=0}^{\\ell-1} \\phi^i w_{k+\\ell-1-i}\\right)\\tilde{x}_k\\right]$$\n$$\\Gamma_x(\\ell) = \\phi^\\ell \\mathbb{E}[\\tilde{x}_k^2] + \\sum_{i=0}^{\\ell-1} \\phi^i \\mathbb{E}[w_{k+\\ell-1-i}\\tilde{x}_k]$$\nFor $\\ell > 0$, the sum involves disturbances $w_j$ for $j \\ge k$. As argued before, $\\tilde{x}_k$ is uncorrelated with any $w_j$ where $j \\ge k$. Thus, $\\mathbb{E}[w_{k+\\ell-1-i}\\tilde{x}_k]=0$ for all terms in the sum.\n$$\\Gamma_x(\\ell) = \\phi^\\ell \\sigma_x^2 = \\phi^\\ell \\frac{\\sigma_w^2}{1 - \\phi^2} \\quad \\text{for } \\ell > 0$$\nFor WSS processes, the autocovariance function is symmetric, $\\Gamma_x(\\ell) = \\Gamma_x(-\\ell)$. For $\\ell < 0$, we have $\\Gamma_x(\\ell) = \\Gamma_x(-\\ell) = \\phi^{-\\ell}\\sigma_x^2 = \\phi^{|\\ell|}\\sigma_x^2$. Combining all cases for $\\ell \\in \\mathbb{Z}$:\n$$\\Gamma_x(\\ell) = \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$$\n\nNow we derive the output autocovariance, $\\Gamma_y(\\ell)$. The centered output is $\\tilde{y}_k = y_k - \\mu_y = (c x_k + v_k) - c \\mu_x = c(x_k - \\mu_x) + v_k = c \\tilde{x}_k + v_k$.\n$$\\Gamma_y(\\ell) = \\mathbb{E}[\\tilde{y}_{k+\\ell}\\tilde{y}_k] = \\mathbb{E}[(c \\tilde{x}_{k+\\ell} + v_{k+\\ell})(c \\tilde{x}_k + v_k)]$$\nExpanding the product and using linearity of expectation:\n$$\\Gamma_y(\\ell) = c^2 \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k] + c \\mathbb{E}[\\tilde{x}_{k+\\ell}v_k] + c \\mathbb{E}[v_{k+\\ell}\\tilde{x}_k] + \\mathbb{E}[v_{k+\\ell}v_k]$$\nWe analyze each term:\n1.  $c^2 \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k] = c^2 \\Gamma_x(\\ell) = c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$.\n2.  $\\mathbb{E}[v_{k+\\ell}v_k]$ is the autocovariance of the measurement noise. Since $\\{v_k\\}$ is white noise, $\\mathbb{E}[v_{k+\\ell}v_k] = \\sigma_v^2 \\delta_{\\ell,0}$.\n3.  The cross-terms involve $\\mathbb{E}[\\tilde{x}_{j}v_{i}]$. The state $x_j$ (and thus $\\tilde{x}_j$) is a function only of the initial state $x_0$ and the process disturbances $\\{w_m\\}_{m<j}$. Given that $\\{w_k\\}$, $\\{v_k\\}$, and $x_0$ are mutually independent, the state $\\tilde{x}_j$ is independent of the measurement noise sequence $\\{v_k\\}$ for all $j$ and $k$. Therefore, the cross-correlations are zero: $\\mathbb{E}[\\tilde{x}_{j}v_{i}] = \\mathbb{E}[\\tilde{x}_{j}]\\mathbb{E}[v_{i}] = 0 \\times 0 = 0$.\n\nCombining the non-zero terms, we obtain the final expression for the output autocovariance:\n$$\\Gamma_y(\\ell) = c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2} + \\sigma_v^2 \\delta_{\\ell,0}$$\n\nThis result clearly separates the contributions of the process and measurement noise to the output variability. The first term, $c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$, originates from the process disturbance $w_k$. This disturbance is integrated by the system dynamics, represented by $\\phi$, creating a state process $x_k$ that is correlated over time. The variance of this disturbance, $\\sigma_w^2$, scales the magnitude of this correlated variability, which decays exponentially with lag $|\\ell|$. This term explains why measurements at different runs are statistically dependent. The second term, $\\sigma_v^2 \\delta_{\\ell,0}$, originates from the measurement noise $v_k$. Because this noise is white (IID), it is uncorrelated from run to run. Therefore, its variance $\\sigma_v^2$ contributes only to the total variance of the output, $\\Gamma_y(0)$, and does not create any correlation between measurements at different runs (i.e., for $\\ell \\neq 0$). At lag zero ($\\ell = 0$), the total variance is $\\Gamma_y(0) = \\frac{c^2 \\sigma_w^2}{1 - \\phi^2} + \\sigma_v^2$, the sum of propagated process variance and measurement variance. At any non-zero lag ($\\ell \\neq 0$), the autocovariance $\\Gamma_y(\\ell) = c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$ is determined solely by the system's memory of past process disturbances.",
            "answer": "$$ \\boxed{c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2} + \\sigma_v^2 \\delta_{\\ell,0}} $$"
        },
        {
            "introduction": "The Exponentially Weighted Moving Average (EWMA) is a ubiquitous tool in statistical process control, but its tuning parameter, $\\lambda$, is often chosen heuristically. This practice reveals the profound connection between the EWMA and the optimal state estimator, the Kalman filter, for a fundamental process model. By deriving the conditions under which these two estimators are equivalent , you will learn to interpret $\\lambda$ in terms of physical process and measurement noise covariances, transforming filter tuning from an art into a science.",
            "id": "4162389",
            "problem": "A single-input, single-output run-to-run semiconductor manufacturing step is modeled as a scalar random-walk state with additive measurement noise. Let the true process state be $x_k \\in \\mathbb{R}$ at run $k$, with evolution $x_{k+1} = x_k + w_k$, where $w_k$ is zero-mean, independent and identically distributed Gaussian process noise with covariance $Q > 0$. The metrology (or virtual metrology) measurement is $y_k = x_k + v_k$, where $v_k$ is zero-mean, independent and identically distributed Gaussian measurement noise with covariance $R > 0$, and $\\{w_k\\}$ and $\\{v_k\\}$ are mutually independent. Consider two state estimators:\n\n- The optimal linear estimator derived from the Kalman filter for this model, operating in steady state.\n- The exponentially weighted moving average (EWMA, Exponentially Weighted Moving Average) estimator with update $\\hat{x}_k = (1 - \\lambda)\\,\\hat{x}_{k-1} + \\lambda\\,y_k$, where $0 < \\lambda < 1$ is a constant weight.\n\nStarting from the fundamental definitions of the discrete-time linear Gaussian state-space model and the optimal mean-square-error properties that characterize the Kalman filter, and from the EWMA recursion, derive the steady-state error dynamics of both estimators and equate them to interpret the EWMA weight $\\lambda$ in terms of the process and measurement noise covariances $Q$ and $R$.\n\nProvide a single closed-form analytic expression for $\\lambda$ as a function of $Q$ and $R$. The final answer must be a single expression without units. Do not use any numerical values; no rounding is required.",
            "solution": "The problem asks for an analytic expression for the exponentially weighted moving average (EWMA) weight $\\lambda$ in terms of the process and measurement noise covariances, $Q$ and $R$, by equating the steady-state behavior of an EWMA estimator to that of an optimal steady-state Kalman filter.\n\nThe system is described by a linear discrete-time state-space model.\nThe state equation is a random walk:\n$$x_{k+1} = x_k + w_k$$\nThe measurement equation is:\n$$y_k = x_k + v_k$$\nHere, $x_k \\in \\mathbb{R}$ is the true process state at run $k$. The process noise $w_k$ is drawn from a zero-mean Gaussian distribution with covariance $Q > 0$, i.e., $w_k \\sim \\mathcal{N}(0, Q)$. The measurement noise $v_k$ is drawn from a zero-mean Gaussian distribution with covariance $R > 0$, i.e., $v_k \\sim \\mathcal{N}(0, R)$. The noise sequences $\\{w_k\\}$ and $\\{v_k\\}$ are independent and identically distributed, and are mutually independent.\n\nThis model can be written in the standard state-space form $x_{k+1} = A x_k + B u_k + w_k$ and $y_k = C x_k + v_k$. For this specific problem, the state transition matrix is $A=1$, the input matrix is $B=0$ (as there is no control input mentioned for the estimation problem), and the observation matrix is C=1.\n\nFirst, we derive the equations for the optimal linear estimator, the Kalman filter, for this system. Let $\\hat{x}_{k|k-1}$ be the state prediction at time $k$ given measurements up to $k-1$, and let $\\hat{x}_k$ be the updated state estimate at time $k$ given the measurement $y_k$. Let $P_{k|k-1}$ and $P_k$ be the corresponding error covariances.\n\nThe Kalman filter consists of a prediction-update cycle:\n$1$. Prediction:\nThe state prediction is $\\hat{x}_{k|k-1} = A\\hat{x}_{k-1} = 1 \\cdot \\hat{x}_{k-1} = \\hat{x}_{k-1}$.\nThe prediction error covariance is $P_{k|k-1} = A_k P_{k-1} A_k^T + Q = 1 \\cdot P_{k-1} \\cdot 1^T + Q = P_{k-1} + Q$.\n\n$2$. Update:\nThe Kalman gain is $K_k = P_{k|k-1} C^T (C P_{k|k-1} C^T + R)^{-1} = P_{k|k-1} \\cdot 1 \\cdot (1 \\cdot P_{k|k-1} \\cdot 1^T + R)^{-1} = \\frac{P_{k|k-1}}{P_{k|k-1} + R}$.\nThe state estimate is updated using the measurement $y_k$: $\\hat{x}_k = \\hat{x}_{k|k-1} + K_k (y_k - C\\hat{x}_{k|k-1}) = \\hat{x}_{k-1} + K_k(y_k - \\hat{x}_{k-1})$. This can be rewritten as $\\hat{x}_k = (1-K_k)\\hat{x}_{k-1} + K_k y_k$.\nThe update error covariance is $P_k = (1 - K_k C) P_{k|k-1} = (1 - K_k) P_{k|k-1}$.\n\nIn steady state, the error covariance converges to a constant value, $P_k \\to P_{ss}$ and $P_{k-1} \\to P_{ss}$. Consequently, the Kalman gain also converges to a steady-state value, $K_k \\to K_{ss}$. The steady-state covariance $P_{ss}$ is the positive solution to the Discrete Algebraic Riccati Equation (DARE).\nFrom the update step, we have $P_{ss} = (1 - K_{ss}) P_{ss|ss-1}$.\nFrom the prediction step, we have $P_{ss|ss-1} = P_{ss} + Q$.\nSubstituting the second into the first gives $P_{ss} = (1 - K_{ss})(P_{ss} + Q)$. Rearranging this yields $K_{ss}(P_{ss}+Q) = Q$.\nAlso from the update step, $K_{ss} = \\frac{P_{ss|ss-1}}{P_{ss|ss-1} + R} = \\frac{P_{ss}+Q}{P_{ss}+Q+R}$.\nWe now have a system of equations for $K_{ss}$ and $P_{ss}$. A direct relation between them is found by substituting $P_{ss}+Q = Q/K_{ss}$ into the expression for $K_{ss}$:\n$K_{ss} = \\frac{Q/K_{ss}}{Q/K_{ss} + R}$.\nMultiplying the numerator and denominator by $K_{ss}$ gives $K_{ss} = \\frac{Q}{Q + R K_{ss}}$.\nThis leads to a quadratic equation for $K_{ss}$:\n$K_{ss} (Q + R K_{ss}) = Q$\n$R K_{ss}^2 + Q K_{ss} - Q = 0$.\n\nSolving this quadratic equation for $K_{ss}$ using the quadratic formula:\n$$K_{ss} = \\frac{-Q \\pm \\sqrt{Q^2 - 4(R)(-Q)}}{2R} = \\frac{-Q \\pm \\sqrt{Q^2 + 4RQ}}{2R}$$\nSince $Q > 0$ and $R > 0$, the term under the square root is positive. The Kalman gain $K_{ss}$ must be positive, as $P_{ss|ss-1} > 0$ and $R > 0$. The term $\\sqrt{Q^2 + 4RQ} > \\sqrt{Q^2} = Q$. Thus, we must take the positive root to ensure $K_{ss} > 0$.\nThe optimal steady-state Kalman gain is:\n$$K_{ss} = \\frac{-Q + \\sqrt{Q^2 + 4RQ}}{2R}$$\nThe steady-state Kalman filter recursion is given by $\\hat{x}_k = (1-K_{ss})\\hat{x}_{k-1} + K_{ss} y_k$.\n\nNext, we analyze the EWMA estimator: $\\hat{x}_k = (1-\\lambda)\\hat{x}_{k-1} + \\lambda y_k$, where $0 < \\lambda < 1$.\nThe problem requires deriving and equating the steady-state error dynamics. Let the estimation error be $e_k = x_k - \\hat{x}_k$.\nFor the EWMA estimator:\n$e_k = x_k - \\left( (1-\\lambda)\\hat{x}_{k-1} + \\lambda y_k \\right)$\nSubstitute $y_k = x_k + v_k$:\n$e_k = x_k - (1-\\lambda)\\hat{x}_{k-1} - \\lambda(x_k+v_k) = (1-\\lambda)x_k - (1-\\lambda)\\hat{x}_{k-1} - \\lambda v_k$\n$e_k = (1-\\lambda)(x_k - \\hat{x}_{k-1})$\nSubstitute $x_k = x_{k-1} + w_{k-1}$ and add and subtract $(1-\\lambda)x_{k-1}$:\n$e_k = (1-\\lambda)(x_{k-1} + w_{k-1} - \\hat{x}_{k-1}) - \\lambda v_k$\n$e_k = (1-\\lambda)(e_{k-1} + w_{k-1}) - \\lambda v_k$\nSo, the error dynamics for the EWMA estimator is $e_k = (1-\\lambda)e_{k-1} + (1-\\lambda)w_{k-1} - \\lambda v_k$.\n\nFor the steady-state Kalman filter, the derivation is identical, with $K_{ss}$ replacing $\\lambda$. The error dynamics for the optimal filter is $e_k = (1-K_{ss})e_{k-1} + (1-K_{ss})w_{k-1} - K_{ss}v_k$.\n\nThe Kalman filter is, by definition, the optimal linear estimator in the mean-square-error sense. The EWMA is a linear estimator. For the EWMA estimator to be optimal, its performance must match that of the Kalman filter. Equating their error dynamics reveals that this requires their parameters to be identical. By comparing the coefficients of the two error dynamics equations, or more simply, by comparing the estimator update forms:\nKalman filter: $\\hat{x}_k = (1-K_{ss})\\hat{x}_{k-1} + K_{ss} y_k$\nEWMA filter:  $\\hat{x}_k = (1-\\lambda)\\hat{x}_{k-1} + \\lambda y_k$\nIt is evident that the EWMA filter is structurally identical to the steady-state Kalman filter for this model. The EWMA becomes the optimal linear filter when its weight $\\lambda$ is chosen to be equal to the steady-state Kalman gain $K_{ss}$.\n\nTherefore, we interpret the EWMA weight $\\lambda$ as the optimal gain by setting $\\lambda = K_{ss}$.\n$$\\lambda = \\frac{-Q + \\sqrt{Q^2 + 4RQ}}{2R}$$\nThis expression provides the value of $\\lambda$ that makes the EWMA filter the optimal linear state estimator for the given random-walk process model.",
            "answer": "$$\\boxed{\\frac{-Q + \\sqrt{Q^2 + 4RQ}}{2R}}$$"
        },
        {
            "introduction": "Real-world manufacturing processes often exhibit nonlinear behavior that cannot be captured by simple linear models. This hands-on exercise challenges you to move beyond linear theory by implementing an Extended Kalman Filter (EKF) to estimate a latent process drift from a nonlinear measurement model, a common scenario in virtual metrology. By translating the EKF's recursive predict-correct algorithm into code and handling practical issues like missing data , you will gain invaluable experience in building and deploying advanced state estimation systems.",
            "id": "4162414",
            "problem": "Consider a run-to-run drift estimation problem in semiconductor manufacturing where the process output is modeled as a discrete-time nonlinear measurement with a single latent drift state. Let the latent drift be $x_k \\in \\mathbb{R}$ evolving according to a random-walk model and the measured output be $y_k \\in \\mathbb{R}$ given by a known nonlinear function of the recipe setting $u_k \\in \\mathbb{R}$ and the drift. The discrete-time model is:\n- State evolution: $x_{k+1} = x_k + w_k$, where $w_k$ is zero-mean process noise with covariance $Q \\ge 0$.\n- Measurement: $y_k = h(u_k) + x_k + v_k$, where $v_k$ is zero-mean measurement noise with covariance $R \\ge 0$, $u_k$ is known at run $k$, and $h(\\cdot)$ is a known differentiable function.\n\nThis setting is typical of Virtual Metrology (VM), where $y_k$ may be inferred from secondary signals and the drift $x_k$ represents process bias requiring estimation for run-to-run control. The Extended Kalman Filter (EKF) is used because the measurement is nonlinear in $u_k$. You must derive the EKF specialized to the above model, starting from the foundational discrete-time Bayesian filtering principles: prediction via state evolution and correction via linearization of the measurement function around the prior estimate. Explicitly derive the Jacobian(s) required by the EKF updates, and explain why a first-order Taylor expansion is justified for the correction step under small linearization error.\n\nThen, implement the EKF for drift estimation that:\n- Uses the prior $x_k$ and covariance $P_k$ to predict $x_{k+1}$ and $P_{k+1}$,\n- Linearizes the measurement function $g(x_k, u_k) = h(u_k) + x_k$ about the predicted state to obtain the measurement Jacobian,\n- Updates the state estimate and covariance using the Kalman gain, and\n- Handles missing measurements by skipping the correction step when a measurement is unavailable.\n\nAngles, where applicable, must be in radians.\n\nYour program must:\n- Construct the true state trajectory using the provided deterministic sequences for $w_k$ and $v_k$ and the given initial true state $x_0$,\n- Generate the measurement sequence $y_k$ using the true trajectory and $h(u_k)$,\n- Run the EKF on this data using the specified $Q$, $R$, initial estimate $x_0^{\\text{est}}$, and initial covariance $P_0$,\n- Compute and use the required Jacobian(s) in the EKF update,\n- Skip updates at times where the measurement is indicated missing, and\n- Output the final estimated drift $\\hat{x}_N$ for each test case as floats.\n\nTest suite and parameters:\n- Test Case $1$ (happy path, smooth drift and moderate noise):\n  - Nonlinearity: $h(u) = \\alpha \\tanh(\\beta u)$ with $\\alpha = 0.8$, $\\beta = 0.6$.\n  - Input sequence $u_k$ for $k = 0,1,\\dots,9$: $[0.0, 0.25, 0.5, 0.75, 1.0, 0.5, 0.0, -0.5, -1.0, 0.0]$.\n  - True state initialization $x_0 = 0.0$.\n  - Process noise increments $w_k$ for $k = 0,1,\\dots,8$: $[0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]$.\n  - Measurement noise sequence $v_k$ for $k = 0,1,\\dots,9$: $[0.01, -0.005, 0.0, 0.02, -0.015, 0.0, 0.005, -0.01, 0.0, 0.005]$.\n  - EKF parameters: $x_0^{\\text{est}}=0.0$, $P_0=0.05$, $Q=0.0004$, $R=0.0004$.\n  - Measurements are all available.\n- Test Case $2$ (boundary condition, zero process noise and nearly noiseless metrology):\n  - Nonlinearity: $h(u) = \\gamma u^3$ with $\\gamma = 0.5$.\n  - Input sequence $u_k$ for $k = 0,1,\\dots,9$: $[-1.0, -0.5, 0.0, 0.5, 1.0, 0.0, -1.0, 1.0, 0.5, -0.5]$.\n  - True state initialization $x_0 = 0.1$.\n  - Process noise increments $w_k$ for $k = 0,1,\\dots,8$: $[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Measurement noise sequence $v_k$ for $k = 0,1,\\dots,9$: $[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - EKF parameters: $x_0^{\\text{est}}=0.0$, $P_0=0.1$, $Q=0.0$, $R=1.0\\times 10^{-8}$.\n  - Measurements are all available.\n- Test Case $3$ (edge case, nonstationary drift, high noise, and missing measurements):\n  - Nonlinearity: $h(u) = \\delta \\sin(\\varepsilon u)$ with $\\delta = 0.7$, $\\varepsilon = 1.2$ (use radians).\n  - Input sequence $u_k$ for $k = 0,1,\\dots,9$: $[0.0, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7]$.\n  - True state initialization $x_0 = 0.05$.\n  - Process noise increments $w_k$ for $k = 0,1,\\dots,8$: $[0.03, -0.01, 0.02, 0.0, 0.01, -0.02, 0.03, -0.01, 0.0]$.\n  - Measurement noise sequence $v_k$ for $k = 0,1,\\dots,9$: $[0.05, -0.04, 0.03, -0.03, 0.02, -0.02, 0.01, -0.01, 0.0, 0.0]$.\n  - EKF parameters: $x_0^{\\text{est}}=0.0$, $P_0=0.2$, $Q=0.0009$, $R=0.01$.\n  - Measurement availability mask for $k = 0,1,\\dots,9$: $[1, 1, 1, 0, 1, 1, 1, 0, 1, 1]$, where $1$ indicates available and $0$ indicates missing.\n\nYour program should produce a single line of output containing the final estimated drift values $\\hat{x}_{N}$ for the three test cases as a comma-separated list enclosed in square brackets, in the order of Test Case $1$, Test Case $2$, Test Case $3$. For example, it should print $\\texttt{[result1,result2,result3]}$, where each result is a float.",
            "solution": "The problem requires the derivation and implementation of an Extended Kalman Filter (EKF) for a specific discrete-time state-space model relevant to semiconductor process control. The first step, as per the principles of rigorous scientific inquiry, is a thorough validation of the problem statement. The problem provides a well-defined model, complete sets of parameters and data for three distinct test cases, and a clear objective. The model equations, though a special case, are standard in control and estimation theory. The premise is scientifically grounded in the context of run-to-run control and virtual metrology. The problem is self-contained, and all data are present and consistent. The phrasing concerning the reason for using an EKF—\"because the measurement is nonlinear in $u_k$\"—is mechanistically imprecise, as the EKF linearizes with respect to the state, not the input. However, this imprecision does not invalidate the problem, as the instructions to linearize the measurement function $g(x_k, u_k)$ are unambiguous and lead to a correct and well-defined filter. The problem is thus deemed valid, and we may proceed with a formal solution.\n\nThe problem defines a discrete-time system with a single latent state $x_k \\in \\mathbb{R}$ and a a single measurement $y_k \\in \\mathbb{R}$. The system is described by the following equations:\n\nState Evolution:\n$$x_{k+1} = f(x_k, u_k) + w_k = x_k + w_k$$\nwhere $w_k$ is a zero-mean Gaussian process noise with covariance $Q_k$, i.e., $w_k \\sim \\mathcal{N}(0, Q)$.\n\nMeasurement Model:\n$$y_k = g(x_k, u_k) + v_k = h(u_k) + x_k + v_k$$\nwhere $v_k$ is a zero-mean Gaussian measurement noise with covariance $R_k$, i.e., $v_k \\sim \\mathcal{N}(0, R)$. The input $u_k$ and the function $h(\\cdot)$ are known.\n\nThe Extended Kalman Filter is a recursive Bayesian estimator for nonlinear systems. It operates in two steps: prediction and correction (or update). Let $\\hat{x}_{k|k-1}$ and $P_{k|k-1}$ be the prior (a priori) state estimate and covariance at time $k$ given information up to time $k-1$. Let $\\hat{x}_{k|k}$ and $P_{k|k}$ be the posterior (a posteriori) state estimate and covariance at time $k$ after incorporating the measurement $y_k$.\n\nThe filter is initialized with a prior estimate for the first state, $\\hat{x}_{0|-1} = x_0^{\\text{est}}$, and its associated covariance, $P_{0|-1} = P_0$. The EKF recursion proceeds for $k = 0, 1, 2, \\ldots$.\n\n**EKF Correction (Update) Step**\n\nAt time $k$, upon receiving a new measurement $y_k$, the prior estimate $\\hat{x}_{k|k-1}$ is corrected. The EKF linearizes the measurement function $g(x_k, u_k)$ around the prior estimate $\\hat{x}_{k|k-1}$ using a first-order Taylor expansion:\n$$g(x_k, u_k) \\approx g(\\hat{x}_{k|k-1}, u_k) + \\frac{\\partial g}{\\partial x_k}\\bigg|_{x_k=\\hat{x}_{k|k-1}} (x_k - \\hat{x}_{k|k-1})$$\nThe measurement Jacobian, $H_k$, is this partial derivative:\n$$H_k = \\frac{\\partial g}{\\partial x_k}\\bigg|_{x_k=\\hat{x}_{k|k-1}} = \\frac{\\partial}{\\partial x_k} (h(u_k) + x_k) = 1$$\nIt is a critical observation that for this specific model, the measurement function $g(x_k, u_k)$ is linear in the state variable $x_k$. Consequently, the Jacobian $H_k$ is a constant, $H_k = 1$, and does not depend on the state estimate. The first-order Taylor expansion is not an approximation but is exact. The linearization error is zero. This simplifies the EKF to the standard Kalman Filter. The nonlinearity in $h(u_k)$ with respect to the input $u_k$ is handled by direct evaluation, as $u_k$ is a known quantity, not an estimated state.\n\nThe correction equations are:\n1.  Predicted Measurement: $\\hat{y}_{k|k-1} = g(\\hat{x}_{k|k-1}, u_k) = h(u_k) + \\hat{x}_{k|k-1}$.\n2.  Innovation (or Residual): $\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}$.\n3.  Innovation Covariance: $S_k = H_k P_{k|k-1} H_k^T + R = P_{k|k-1} + R$.\n4.  Kalman Gain: $K_k = P_{k|k-1} H_k^T S_k^{-1} = P_{k|k-1} (P_{k|k-1} + R)^{-1}$.\n5.  Posterior State Estimate: $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k \\tilde{y}_k$.\n6.  Posterior Covariance Estimate: $P_{k|k} = (I - K_k H_k) P_{k|k-1} = (1 - K_k) P_{k|k-1}$.\n\nIf a measurement $y_k$ is missing, the correction step is skipped. The posterior estimates are set equal to the prior estimates: $\\hat{x}_{k|k} = \\hat{x}_{k|k-1}$ and $P_{k|k} = P_{k|k-1}$.\n\n**EKF Prediction (Time Evolution) Step**\n\nThis step projects the state and covariance estimates forward from time $k$ to $k+1$. The state evolution function $f(x_k, u_k)$ is linearized around the posterior estimate $\\hat{x}_{k|k}$.\n$$f(x_k, u_k) \\approx f(\\hat{x}_{k|k}, u_k) + \\frac{\\partial f}{\\partial x_k}\\bigg|_{x_k=\\hat{x}_{k|k}} (x_k - \\hat{x}_{k|k})$$\nThe state-transition Jacobian, $F_k$, is this partial derivative:\n$$F_k = \\frac{\\partial f}{\\partial x_k}\\bigg|_{x_k=\\hat{x}_{k|k}} = \\frac{\\partial}{\\partial x_k} (x_k) = 1$$\nSimilar to the measurement model, the state evolution model is linear in $x_k$, making the linearization exact.\n\nThe prediction equations are:\n1.  Prior State Estimate for step $k+1$: $\\hat{x}_{k+1|k} = f(\\hat{x}_{k|k}, u_k) = \\hat{x}_{k|k}$.\n2.  Prior Covariance Estimate for step $k+1$: $P_{k+1|k} = F_k P_{k|k} F_k^T + Q = P_{k|k} + Q$.\n\nThe recursive estimation proceeds by alternating between the correction and prediction steps. The final estimate for each test case will be the posterior estimate after the last measurement is processed, which is $\\hat{x}_{N-1|N-1}$ for $N=10$ measurements (i.e., $\\hat{x}_{9|9}$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the EKF-based drift estimation problem for the given test cases.\n    \"\"\"\n\n    # Define the nonlinear functions h(u) for each test case.\n    h_funcs = [\n        lambda u, alpha=0.8, beta=0.6: alpha * np.tanh(beta * u),\n        lambda u, gamma=0.5: gamma * u**3,\n        lambda u, delta=0.7, epsilon=1.2: delta * np.sin(epsilon * u)\n    ]\n\n    test_cases = [\n        # Test Case 1: happy path, smooth drift and moderate noise\n        {\n            \"h_func\": h_funcs[0],\n            \"u_seq\": np.array([0.0, 0.25, 0.5, 0.75, 1.0, 0.5, 0.0, -0.5, -1.0, 0.0]),\n            \"x0_true\": 0.0,\n            \"w_seq\": np.array([0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]),\n            \"v_seq\": np.array([0.01, -0.005, 0.0, 0.02, -0.015, 0.0, 0.005, -0.01, 0.0, 0.005]),\n            \"x0_est\": 0.0,\n            \"P0_est\": 0.05,\n            \"Q\": 0.0004,\n            \"R\": 0.0004,\n            \"meas_mask\": np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n        },\n        # Test Case 2: boundary condition, zero process noise and nearly noiseless metrology\n        {\n            \"h_func\": h_funcs[1],\n            \"u_seq\": np.array([-1.0, -0.5, 0.0, 0.5, 1.0, 0.0, -1.0, 1.0, 0.5, -0.5]),\n            \"x0_true\": 0.1,\n            \"w_seq\": np.zeros(9),\n            \"v_seq\": np.zeros(10),\n            \"x0_est\": 0.0,\n            \"P0_est\": 0.1,\n            \"Q\": 0.0,\n            \"R\": 1.0e-8,\n            \"meas_mask\": np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n        },\n        # Test Case 3: edge case, nonstationary drift, high noise, and missing measurements\n        {\n            \"h_func\": h_funcs[2],\n            \"u_seq\": np.array([0.0, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7]),\n            \"x0_true\": 0.05,\n            \"w_seq\": np.array([0.03, -0.01, 0.02, 0.0, 0.01, -0.02, 0.03, -0.01, 0.0]),\n            \"v_seq\": np.array([0.05, -0.04, 0.03, -0.03, 0.02, -0.02, 0.01, -0.01, 0.0, 0.0]),\n            \"x0_est\": 0.0,\n            \"P0_est\": 0.2,\n            \"Q\": 0.0009,\n            \"R\": 0.01,\n            \"meas_mask\": np.array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1])\n        }\n    ]\n\n    final_estimates = []\n\n    for case in test_cases:\n        h = case[\"h_func\"]\n        u_seq = case[\"u_seq\"]\n        x0_true = case[\"x0_true\"]\n        w_seq = case[\"w_seq\"]\n        v_seq = case[\"v_seq\"]\n        x0_est = case[\"x0_est\"]\n        P0_est = case[\"P0_est\"]\n        Q = case[\"Q\"]\n        R = case[\"R\"]\n        meas_mask = case[\"meas_mask\"]\n        \n        N = len(u_seq)\n\n        # 1. Construct the true state trajectory and measurement sequence\n        x_true = np.zeros(N)\n        x_true[0] = x0_true\n        for k in range(N - 1):\n            x_true[k+1] = x_true[k] + w_seq[k]\n\n        y_meas = np.zeros(N)\n        for k in range(N):\n            y_meas[k] = h(u_seq[k]) + x_true[k] + v_seq[k]\n\n        # 2. Run the Kalman Filter (EKF simplifies to KF for this model)\n        \n        # Initialize with prior for k=0\n        x_hat_prior = float(x0_est)\n        P_prior = float(P0_est)\n        \n        x_hat_posterior = 0.0\n\n        for k in range(N):\n            # --- Correction (Update) Step ---\n            if meas_mask[k] == 1:\n                # Measurement Jacobian H_k is 1\n                # Since all variables are scalars, no matrix operations are needed.\n                H_k = 1.0\n                \n                # Predicted measurement\n                y_hat = h(u_seq[k]) + x_hat_prior\n                \n                # Innovation and its covariance\n                residual = y_meas[k] - y_hat\n                S_k = P_prior + R  # H*P*H' + R\n                \n                # Kalman Gain\n                K_k = P_prior / S_k # P*H' * S^-1\n                \n                # Update state and covariance\n                x_hat_posterior = x_hat_prior + K_k * residual\n                P_posterior = (1.0 - K_k * H_k) * P_prior\n            else:\n                # No measurement, so posterior is the same as prior\n                x_hat_posterior = x_hat_prior\n                P_posterior = P_prior\n\n            # --- Prediction Step ---\n            # State transition Jacobian F_k is 1\n            # Predict next prior state and covariance\n            # x_{k+1|k} = x_{k|k}\n            # P_{k+1|k} = P_{k|k} + Q\n            x_hat_prior = x_hat_posterior\n            P_prior = P_posterior + Q\n\n        # The final estimate is the posterior after the last measurement at k=N-1\n        final_estimates.append(x_hat_posterior)\n\n    # Format and print the final results\n    print(f\"[{','.join(map(str, final_estimates))}]\")\n\n\nsolve()\n```"
        }
    ]
}