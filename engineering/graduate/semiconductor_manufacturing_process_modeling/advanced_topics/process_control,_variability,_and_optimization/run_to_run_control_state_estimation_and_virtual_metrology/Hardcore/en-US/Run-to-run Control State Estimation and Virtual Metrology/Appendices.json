{
    "hands_on_practices": [
        {
            "introduction": "Before implementing any control scheme, it is essential to first understand the inherent variability of the process itself. This exercise  explores the fundamental concept of autocovariance within a linear state-space model, a common representation for dynamic behavior in semiconductor manufacturing. By deriving the autocovariance function, you will learn to mathematically distinguish how persistent process disturbances create time-correlated variations, while random measurement noise contributes to the overall, yet uncorrelated, output variance.",
            "id": "4162423",
            "problem": "A single-input single-output wafer-to-wafer model for a semiconductor process under open-loop operation is described by a linear time-invariant state evolution and measurement relation. The scalar tool state $x_k$ evolves according to $x_{k+1} = \\phi x_k + b u + w_k$ and the measured output (metrology) is $y_k = c x_k + v_k$, where $k \\in \\mathbb{Z}$ indexes wafer runs, $|\\phi|  1$, and $u$ is a constant recipe setting held fixed for all runs. The process disturbance $w_k$ is independent and identically distributed (IID) zero-mean white noise with variance $\\sigma_w^2$, the measurement noise $v_k$ is IID zero-mean white noise with variance $\\sigma_v^2$, and $\\{w_k\\}$, $\\{v_k\\}$, and $x_0$ are mutually independent. Assume the system has reached wide-sense steady state so that the first and second moments are time-invariant.\n\nStarting only from fundamental definitions of wide-sense stationarity and autocovariance, and the properties of linear time-invariant systems driven by independent white noise, derive a closed-form expression for the autocovariance function of the measured output $y_k$ at integer lag $\\ell \\in \\mathbb{Z}$,\n$$\\Gamma_y(\\ell) = \\mathbb{E}\\!\\left[(y_{k+\\ell} - \\mu_y)(y_k - \\mu_y)\\right],$$\nin terms of $\\phi$, $b$, $u$, $c$, $\\sigma_w^2$, and $\\sigma_v^2$, where $\\mu_y = \\mathbb{E}[y_k]$ is the steady-state mean. Your derivation should clearly identify how the process disturbance variance $\\sigma_w^2$ and the measurement noise variance $\\sigma_v^2$ shape the observed run-to-run variability at zero and nonzero lags. You may use the Kronecker delta $\\delta_{\\ell,0}$, defined by $\\delta_{\\ell,0} = 1$ if $\\ell = 0$ and $\\delta_{\\ell,0} = 0$ otherwise, to concisely express your result.\n\nProvide the final answer as a single closed-form analytical expression for $\\Gamma_y(\\ell)$. No numerical evaluation or rounding is required.",
            "solution": "The problem provides a single-input single-output (SISO) linear time-invariant (LTI) state-space model describing a semiconductor process. The state evolution and measurement equations are given by:\n$$x_{k+1} = \\phi x_k + b u + w_k$$\n$$y_k = c x_k + v_k$$\nwhere $k \\in \\mathbb{Z}$ is the run index, $x_k$ is the scalar state, $y_k$ is the scalar output, and $u$ is a constant input. The model parameters $\\phi$, $b$, and $c$ are constants, with the stability condition $|\\phi|  1$. The process disturbance $w_k$ and measurement noise $v_k$ are independent and identically distributed (IID) zero-mean white noise processes with variances $\\sigma_w^2$ and $\\sigma_v^2$, respectively. This implies $\\mathbb{E}[w_k] = 0$, $\\mathbb{E}[v_k] = 0$, $\\mathbb{E}[w_k w_j] = \\sigma_w^2 \\delta_{k,j}$, and $\\mathbb{E}[v_k v_j] = \\sigma_v^2 \\delta_{k,j}$, where $\\delta_{k,j}$ is the Kronecker delta. The sequences $\\{w_k\\}$, $\\{v_k\\}$, and the initial state $x_0$ are mutually independent. The system is assumed to be in a wide-sense steady state (WSS), meaning the first and second moments of the stochastic processes are time-invariant.\n\nOur objective is to derive the autocovariance function of the output, $\\Gamma_y(\\ell) = \\mathbb{E}[(y_{k+\\ell} - \\mu_y)(y_k - \\mu_y)]$, where $\\mu_y = \\mathbb{E}[y_k]$ is the steady-state mean of the output.\n\nFirst, we determine the steady-state mean of the state, $\\mu_x$, and the output, $\\mu_y$. In WSS, $\\mathbb{E}[x_{k+1}] = \\mathbb{E}[x_k] = \\mu_x$. Taking the expectation of the state equation:\n$$\\mathbb{E}[x_{k+1}] = \\mathbb{E}[\\phi x_k + b u + w_k]$$\n$$\\mu_x = \\phi \\mathbb{E}[x_k] + b u + \\mathbb{E}[w_k]$$\nSubstituting $\\mathbb{E}[x_k] = \\mu_x$ and $\\mathbb{E}[w_k] = 0$:\n$$\\mu_x = \\phi \\mu_x + b u$$\n$$\\mu_x(1 - \\phi) = b u$$\nSince $|\\phi|  1$, we have $1 - \\phi \\neq 0$, so the mean of the state is:\n$$\\mu_x = \\frac{b u}{1 - \\phi}$$\nThe mean of the output, $\\mu_y$, is found by taking the expectation of the output equation:\n$$\\mu_y = \\mathbb{E}[y_k] = \\mathbb{E}[c x_k + v_k] = c \\mathbb{E}[x_k] + \\mathbb{E}[v_k]$$\nSubstituting $\\mathbb{E}[x_k] = \\mu_x$ and $\\mathbb{E}[v_k] = 0$:\n$$\\mu_y = c \\mu_x = \\frac{c b u}{1 - \\phi}$$\n\nNext, we define the centered (zero-mean) state and output variables as $\\tilde{x}_k = x_k - \\mu_x$ and $\\tilde{y}_k = y_k - \\mu_y$. The dynamics of the centered state are found by subtracting the mean dynamics:\n$$(x_{k+1} - \\mu_x) = (\\phi x_k + b u + w_k) - (\\phi \\mu_x + b u)$$\n$$\\tilde{x}_{k+1} = \\phi (x_k - \\mu_x) + w_k = \\phi \\tilde{x}_k + w_k$$\nThe autocovariance function of the state is $\\Gamma_x(\\ell) = \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k]$. For lag $\\ell = 0$, this gives the state variance, $\\sigma_x^2 = \\Gamma_x(0) = \\mathbb{E}[\\tilde{x}_k^2]$. In WSS, $\\mathbb{E}[\\tilde{x}_{k+1}^2] = \\mathbb{E}[\\tilde{x}_k^2] = \\sigma_x^2$.\n$$\\sigma_x^2 = \\mathbb{E}[(\\phi \\tilde{x}_k + w_k)^2] = \\mathbb{E}[\\phi^2 \\tilde{x}_k^2 + 2\\phi \\tilde{x}_k w_k + w_k^2]$$\n$$\\sigma_x^2 = \\phi^2 \\mathbb{E}[\\tilde{x}_k^2] + 2\\phi \\mathbb{E}[\\tilde{x}_k w_k] + \\mathbb{E}[w_k^2]$$\nThe state $\\tilde{x}_k$ is a function of past disturbances $\\{w_j\\}_{jk}$. Since $\\{w_k\\}$ is an IID sequence, $w_k$ is independent of $\\tilde{x}_k$, so $\\mathbb{E}[\\tilde{x}_k w_k] = \\mathbb{E}[\\tilde{x}_k]\\mathbb{E}[w_k] = 0$. With $\\mathbb{E}[w_k^2] = \\sigma_w^2$, we have:\n$$\\sigma_x^2 = \\phi^2 \\sigma_x^2 + \\sigma_w^2$$\n$$\\sigma_x^2(1 - \\phi^2) = \\sigma_w^2$$\nSince $|\\phi|1$, $1-\\phi^2  0$, and the steady-state variance is:\n$$\\sigma_x^2 = \\frac{\\sigma_w^2}{1 - \\phi^2}$$\nFor a general lag $\\ell  0$, we iterate the centered state equation: $\\tilde{x}_{k+\\ell} = \\phi^\\ell \\tilde{x}_k + \\sum_{i=0}^{\\ell-1} \\phi^i w_{k+\\ell-1-i}$.\nThen, the autocovariance is:\n$$\\Gamma_x(\\ell) = \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k] = \\mathbb{E}\\left[\\left(\\phi^\\ell \\tilde{x}_k + \\sum_{i=0}^{\\ell-1} \\phi^i w_{k+\\ell-1-i}\\right)\\tilde{x}_k\\right]$$\n$$\\Gamma_x(\\ell) = \\phi^\\ell \\mathbb{E}[\\tilde{x}_k^2] + \\sum_{i=0}^{\\ell-1} \\phi^i \\mathbb{E}[w_{k+\\ell-1-i}\\tilde{x}_k]$$\nFor $\\ell  0$, the sum involves disturbances $w_j$ for $j \\ge k$. As argued before, $\\tilde{x}_k$ is uncorrelated with any $w_j$ where $j \\ge k$. Thus, $\\mathbb{E}[w_{k+\\ell-1-i}\\tilde{x}_k]=0$ for all terms in the sum.\n$$\\Gamma_x(\\ell) = \\phi^\\ell \\sigma_x^2 = \\phi^\\ell \\frac{\\sigma_w^2}{1 - \\phi^2} \\quad \\text{for } \\ell  0$$\nFor WSS processes, the autocovariance function is symmetric, $\\Gamma_x(\\ell) = \\Gamma_x(-\\ell)$. For $\\ell  0$, we have $\\Gamma_x(\\ell) = \\Gamma_x(-\\ell) = \\phi^{-\\ell}\\sigma_x^2 = \\phi^{|\\ell|}\\sigma_x^2$. Combining all cases for $\\ell \\in \\mathbb{Z}$:\n$$\\Gamma_x(\\ell) = \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$$\n\nNow we derive the output autocovariance, $\\Gamma_y(\\ell)$. The centered output is $\\tilde{y}_k = y_k - \\mu_y = (c x_k + v_k) - c \\mu_x = c(x_k - \\mu_x) + v_k = c \\tilde{x}_k + v_k$.\n$$\\Gamma_y(\\ell) = \\mathbb{E}[\\tilde{y}_{k+\\ell}\\tilde{y}_k] = \\mathbb{E}[(c \\tilde{x}_{k+\\ell} + v_{k+\\ell})(c \\tilde{x}_k + v_k)]$$\nExpanding the product and using linearity of expectation:\n$$\\Gamma_y(\\ell) = c^2 \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k] + c \\mathbb{E}[\\tilde{x}_{k+\\ell}v_k] + c \\mathbb{E}[v_{k+\\ell}\\tilde{x}_k] + \\mathbb{E}[v_{k+\\ell}v_k]$$\nWe analyze each term:\n1.  $c^2 \\mathbb{E}[\\tilde{x}_{k+\\ell}\\tilde{x}_k] = c^2 \\Gamma_x(\\ell) = c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$.\n2.  $\\mathbb{E}[v_{k+\\ell}v_k]$ is the autocovariance of the measurement noise. Since $\\{v_k\\}$ is white noise, $\\mathbb{E}[v_{k+\\ell}v_k] = \\sigma_v^2 \\delta_{\\ell,0}$.\n3.  The cross-terms involve $\\mathbb{E}[\\tilde{x}_{j}v_{i}]$. The state $x_j$ (and thus $\\tilde{x}_j$) is a function only of the initial state $x_0$ and the process disturbances $\\{w_m\\}_{mj}$. Given that $\\{w_k\\}$, $\\{v_k\\}$, and $x_0$ are mutually independent, the state $\\tilde{x}_j$ is independent of the measurement noise sequence $\\{v_k\\}$ for all $j$ and $k$. Therefore, the cross-correlations are zero: $\\mathbb{E}[\\tilde{x}_{j}v_{i}] = \\mathbb{E}[\\tilde{x}_{j}]\\mathbb{E}[v_{i}] = 0 \\times 0 = 0$.\n\nCombining the non-zero terms, we obtain the final expression for the output autocovariance:\n$$\\Gamma_y(\\ell) = c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2} + \\sigma_v^2 \\delta_{\\ell,0}$$\n\nThis result clearly separates the contributions of the process and measurement noise to the output variability. The first term, $c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$, originates from the process disturbance $w_k$. This disturbance is integrated by the system dynamics, represented by $\\phi$, creating a state process $x_k$ that is correlated over time. The variance of this disturbance, $\\sigma_w^2$, scales the magnitude of this correlated variability, which decays exponentially with lag $|\\ell|$. This term explains why measurements at different runs are statistically dependent. The second term, $\\sigma_v^2 \\delta_{\\ell,0}$, originates from the measurement noise $v_k$. Because this noise is white (IID), it is uncorrelated from run to run. Therefore, its variance $\\sigma_v^2$ contributes only to the total variance of the output, $\\Gamma_y(0)$, and does not create any correlation between measurements at different runs (i.e., for $\\ell \\neq 0$). At lag zero ($\\ell = 0$), the total variance is $\\Gamma_y(0) = \\frac{c^2 \\sigma_w^2}{1 - \\phi^2} + \\sigma_v^2$, the sum of propagated process variance and measurement variance. At any non-zero lag ($\\ell \\neq 0$), the autocovariance $\\Gamma_y(\\ell) = c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2}$ is determined solely by the system's memory of past process disturbances.",
            "answer": "$$ \\boxed{c^2 \\phi^{|\\ell|} \\frac{\\sigma_w^2}{1 - \\phi^2} + \\sigma_v^2 \\delta_{\\ell,0}} $$"
        },
        {
            "introduction": "After characterizing an open-loop process, the logical next step is to design a controller to compensate for its inherent drift and disturbances. This practice  challenges you to analyze the performance of a closed-loop system under the widely used Exponentially Weighted Moving Average (EWMA) control strategy. Deriving the steady-state variance of the tracking error, $e_k$, provides a quantitative metric for controller effectiveness and reveals the fundamental trade-offs in tuning the EWMA parameter $\\lambda$ to balance system responsiveness against sensitivity to measurement noise.",
            "id": "4162430",
            "problem": "A single-input single-output run-to-run process in semiconductor manufacturing is actuated by a recipe input $u_k$ to achieve a target output $y^{\\star}$. The process output $y_k$ at run $k$ can be modeled as $y_k = G u_k + d_k$, where $G$ is the (constant) process gain and $d_k$ is an unmeasured additive disturbance that evolves as a random walk $d_k = d_{k-1} + w_k$. The random increment $w_k$ is assumed to be independent and identically distributed with zero mean and variance $\\sigma_w^2$. A Virtual Metrology (VM) system provides a noisy measurement $z_k$ of $y_k$ as $z_k = y_k + v_k$, where $v_k$ is independent and identically distributed with zero mean and variance $\\sigma_v^2$, and $\\{w_k\\}$, $\\{v_k\\}$, and all past states and inputs are mutually independent.\n\nAn Exponentially Weighted Moving Average (EWMA) estimator updates the estimate $\\hat{d}_k$ of the disturbance according to\n$$\n\\hat{d}_k = (1 - \\lambda)\\hat{d}_{k-1} + \\lambda\\left(z_k - G u_k\\right),\n$$\nwhere $0  \\lambda  2$ is the EWMA smoothing parameter. The controller sets the next runâ€™s recipe using the disturbance estimate as\n$$\nu_{k+1} = \\frac{y^{\\star} - \\hat{d}_k}{G}.\n$$\nDefine the tracking error $e_k = y_k - y^{\\star}$. Assume transients have died out and the closed-loop error process is stationary for the chosen $\\lambda$.\n\nStarting from the given process and estimator definitions, derive the closed-loop error dynamics and compute the steady-state variance of the tracking error $e_k$ as an analytical expression in terms of $\\sigma_w^2$, $\\sigma_v^2$, $G$, and $\\lambda$. Assume all variables are dimensionless. Express your final answer as a single closed-form expression; no rounding is required.",
            "solution": "The user wants to find the steady-state variance of the tracking error for a run-to-run control system.\n\n### Step 1: Extract Givens\n- Process model: $y_k = G u_k + d_k$\n- Disturbance model: $d_k = d_{k-1} + w_k$\n- Disturbance increment properties: $w_k$ are independent and identically distributed (i.i.d.) with $E[w_k] = 0$ and $Var(w_k) = \\sigma_w^2$.\n- Virtual Metrology (VM) model: $z_k = y_k + v_k$\n- Measurement noise properties: $v_k$ are i.i.d. with $E[v_k] = 0$ and $Var(v_k) = \\sigma_v^2$.\n- Independence: $\\{w_k\\}$, $\\{v_k\\}$, and all past states/inputs are mutually independent.\n- Estimator (EWMA): $\\hat{d}_k = (1 - \\lambda)\\hat{d}_{k-1} + \\lambda\\left(z_k - G u_k\\right)$\n- EWMA parameter range: $0  \\lambda  2$\n- Controller law: $u_{k+1} = \\frac{y^{\\star} - \\hat{d}_k}{G}$\n- Tracking error definition: $e_k = y_k - y^{\\star}$\n- Assumption: The closed-loop error process is stationary.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing standard linear models for process dynamics, disturbance evolution (a random walk or IMA(0,1,1) process), and control (an EWMA controller, a form of integral control). This framework is a cornerstone of statistical process control and run-to-run control in semiconductor manufacturing. The problem is well-posed, providing all necessary definitions, relationships, and statistical properties to derive a unique solution for the steady-state variance. The provided condition $0  \\lambda  2$ is the standard requirement for the stability of this closed-loop system, which justifies the assumption of stationarity. The language is objective and unambiguous. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution follows.\n\nThe objective is to derive an analytical expression for the steady-state variance of the tracking error, $Var(e_k)$, where $e_k = y_k - y^{\\star}$.\n\nFirst, we express the tracking error $e_k$ in terms of the underlying system variables. We start with the definition of $e_k$ and substitute the process model $y_k = G u_k + d_k$:\n$$\ne_k = y_k - y^{\\star} = (G u_k + d_k) - y^{\\star}\n$$\nThe controller law is given for the input at run $k+1$, so we must re-index it to express the input at run $k$:\n$$\nu_k = \\frac{y^{\\star} - \\hat{d}_{k-1}}{G}\n$$\nSubstituting this expression for $u_k$ into the equation for $e_k$:\n$$\ne_k = G \\left( \\frac{y^{\\star} - \\hat{d}_{k-1}}{G} \\right) + d_k - y^{\\star} = (y^{\\star} - \\hat{d}_{k-1}) + d_k - y^{\\star}\n$$\nThis simplifies to a relationship between the tracking error and the disturbance terms:\n$$\ne_k = d_k - \\hat{d}_{k-1}\n$$\nLet's define the disturbance estimation error at run $k$ as $\\tilde{d}_k = d_k - \\hat{d}_k$. Using the disturbance model $d_k = d_{k-1} + w_k$, we can rewrite the tracking error as:\n$$\ne_k = (d_{k-1} + w_k) - \\hat{d}_{k-1} = (d_{k-1} - \\hat{d}_{k-1}) + w_k = \\tilde{d}_{k-1} + w_k\n$$\nThe variance of the tracking error, $Var(e_k)$, can now be calculated. Since the estimation error at run $k-1$, $\\tilde{d}_{k-1}$, depends only on random variables up to run $k-1$, it is independent of the disturbance increment $w_k$. Therefore, the variance of their sum is the sum of their variances:\n$$\nVar(e_k) = Var(\\tilde{d}_{k-1} + w_k) = Var(\\tilde{d}_{k-1}) + Var(w_k) = Var(\\tilde{d}_{k-1}) + \\sigma_w^2\n$$\nUnder the assumption of stationarity, the variance of the estimation error is constant in steady state, i.e., $Var(\\tilde{d}_k) = Var(\\tilde{d}_{k-1}) = \\sigma_{\\tilde{d}}^2$. Thus, we have:\n$$\nVar(e_k) = \\sigma_{\\tilde{d}}^2 + \\sigma_w^2\n$$\nTo find $Var(e_k)$, we must first determine $\\sigma_{\\tilde{d}}^2$. We need to derive the dynamics of the estimation error $\\tilde{d}_k$. We start with the EWMA estimator equation:\n$$\n\\hat{d}_k = (1 - \\lambda)\\hat{d}_{k-1} + \\lambda\\left(z_k - G u_k\\right)\n$$\nThe term in the parenthesis can be simplified by substituting the VM and process models:\n$$\nz_k - G u_k = (y_k + v_k) - G u_k = (G u_k + d_k + v_k) - G u_k = d_k + v_k\n$$\nSubstituting this back into the estimator equation gives:\n$$\n\\hat{d}_k = (1 - \\lambda)\\hat{d}_{k-1} + \\lambda(d_k + v_k)\n$$\nNow, we find the recursive expression for the estimation error $\\tilde{d}_k = d_k - \\hat{d}_k$:\n$$\n\\tilde{d}_k = d_k - \\left[ (1 - \\lambda)\\hat{d}_{k-1} + \\lambda(d_k + v_k) \\right]\n$$\n$$\n\\tilde{d}_k = (1 - \\lambda)d_k - (1 - \\lambda)\\hat{d}_{k-1} - \\lambda v_k\n$$\nUsing the disturbance model $d_k = d_{k-1} + w_k$:\n$$\n\\tilde{d}_k = (1 - \\lambda)(d_{k-1} + w_k) - (1 - \\lambda)\\hat{d}_{k-1} - \\lambda v_k\n$$\nGrouping terms involving variables from run $k-1$:\n$$\n\\tilde{d}_k = (1 - \\lambda)(d_{k-1} - \\hat{d}_{k-1}) + (1 - \\lambda)w_k - \\lambda v_k\n$$\nThis yields the desired first-order dynamics for the estimation error:\n$$\n\\tilde{d}_k = (1 - \\lambda)\\tilde{d}_{k-1} + (1 - \\lambda)w_k - \\lambda v_k\n$$\nNow we can compute the steady-state variance $\\sigma_{\\tilde{d}}^2 = Var(\\tilde{d}_k)$. The terms on the right-hand side, $\\tilde{d}_{k-1}$, $w_k$, and $v_k$, are mutually independent. Thus, the variance of the sum is the sum of the variances:\n$$\nVar(\\tilde{d}_k) = Var((1 - \\lambda)\\tilde{d}_{k-1}) + Var((1 - \\lambda)w_k) + Var(-\\lambda v_k)\n$$\n$$\nVar(\\tilde{d}_k) = (1 - \\lambda)^2 Var(\\tilde{d}_{k-1}) + (1 - \\lambda)^2 Var(w_k) + (-\\lambda)^2 Var(v_k)\n$$\nIn steady state, $Var(\\tilde{d}_k) = Var(\\tilde{d}_{k-1}) = \\sigma_{\\tilde{d}}^2$. Substituting this and the known variances of the noise terms:\n$$\n\\sigma_{\\tilde{d}}^2 = (1 - \\lambda)^2 \\sigma_{\\tilde{d}}^2 + (1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2\n$$\nWe now solve for $\\sigma_{\\tilde{d}}^2$:\n$$\n\\sigma_{\\tilde{d}}^2 - (1 - \\lambda)^2 \\sigma_{\\tilde{d}}^2 = (1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2\n$$\n$$\n\\sigma_{\\tilde{d}}^2 \\left(1 - (1 - 2\\lambda + \\lambda^2)\\right) = (1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2\n$$\n$$\n\\sigma_{\\tilde{d}}^2 (2\\lambda - \\lambda^2) = (1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2\n$$\n$$\n\\sigma_{\\tilde{d}}^2 = \\frac{(1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2}{\\lambda(2 - \\lambda)}\n$$\nFinally, we substitute this expression for $\\sigma_{\\tilde{d}}^2$ into our equation for the tracking error variance, $Var(e_k) = \\sigma_{\\tilde{d}}^2 + \\sigma_w^2$:\n$$\nVar(e_k) = \\frac{(1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2}{\\lambda(2 - \\lambda)} + \\sigma_w^2\n$$\nTo simplify, we find a common denominator:\n$$\nVar(e_k) = \\frac{(1 - \\lambda)^2 \\sigma_w^2 + \\lambda^2 \\sigma_v^2 + \\lambda(2 - \\lambda)\\sigma_w^2}{\\lambda(2 - \\lambda)}\n$$\n$$\nVar(e_k) = \\frac{\\left[(1 - 2\\lambda + \\lambda^2) + (2\\lambda - \\lambda^2)\\right]\\sigma_w^2 + \\lambda^2 \\sigma_v^2}{\\lambda(2 - \\lambda)}\n$$\nThe coefficient of $\\sigma_w^2$ in the numerator simplifies to $1$:\n$$\nVar(e_k) = \\frac{1 \\cdot \\sigma_w^2 + \\lambda^2 \\sigma_v^2}{\\lambda(2 - \\lambda)}\n$$\nThis is the final analytical expression for the steady-state variance of the tracking error.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_w^2 + \\lambda^2 \\sigma_v^2}{\\lambda(2 - \\lambda)}}\n$$"
        },
        {
            "introduction": "In practice, the models we use for controller design are never perfect representations of reality. A truly effective controller must therefore be robust, performing predictably even when the actual process deviates from its assumed model. This exercise  delves into this critical topic by analyzing how a mismatch between the controller's nominal process gain, $G$, and the true plant gain, $G(1+\\epsilon)$, affects closed-loop stability. By determining the maximum tolerable gain mismatch, you will develop a practical understanding of how to design and validate control systems that are resilient to model uncertainty.",
            "id": "4162401",
            "problem": "A single-input single-output run-to-run controller is used to regulate critical dimension in semiconductor lithography. The nominal process model is $y_k = G u_k + \\mu_k$, where $y_k$ is the measured output at run $k$, $u_k$ is the recipe, $G  0$ is the nominal process gain, and $\\mu_k$ is an unknown process bias that is constant across runs. Due to plant-model mismatch, the true gain is $G(1+\\epsilon)$, where $\\epsilon$ is an unknown dimensionless relative gain error.\n\nThe controller uses the standard set-point recipe based on the nominal model and an Exponentially Weighted Moving Average (EWMA) bias estimator:\n- Control law: $u_{k+1} = \\frac{y^{*} - \\hat{\\mu}_k}{G}$, where $y^{*}$ is the target output and $\\hat{\\mu}_k$ is the bias estimate at run $k$.\n- EWMA estimator update: $\\hat{\\mu}_{k+1} = \\lambda \\hat{\\mu}_{k} + (1-\\lambda)\\left(y_{k+1} - G u_{k+1}\\right)$ with $0  \\lambda  1$.\n\nAssume negligible metrology noise and no bias drift so that $\\mu_k = \\mu$ is constant. Define the estimation error $\\tilde{\\mu}_k = \\hat{\\mu}_k - \\mu$. Working from first principles of discrete-time linear systems stability (all closed-loop eigenvalues must lie strictly inside the unit disk) and contraction rate (spectral radius bounded by a prescribed value), derive the closed-loop estimation error dynamics as a function of $\\epsilon$, and use it to determine the largest symmetric gain-mismatch magnitude $\\epsilon_{\\max}$ such that for all $\\epsilon$ satisfying $|\\epsilon| \\leq \\epsilon_{\\max}$, both:\n- Stability: all closed-loop eigenvalues lie inside the unit disk.\n- Performance: the magnitude of the closed-loop contraction factor does not exceed a specified bound.\n\nTake $\\lambda = 0.6$ and require the contraction-factor magnitude be at most $0.7$. Express $\\epsilon_{\\max}$ as a dimensionless number. Round your answer to four significant figures.",
            "solution": "The objective is to derive the closed-loop dynamics for the estimation error and subsequently determine the maximum symmetric gain-mismatch magnitude, $\\epsilon_{\\max}$, that guarantees both stability and a specified performance level for the controller.\n\nThe system is described by the following equations:\nTrue process model: $y_k = G(1+\\epsilon) u_k + \\mu$\nControl law: $u_{k+1} = \\frac{y^{*} - \\hat{\\mu}_k}{G}$\nEWMA estimator update: $\\hat{\\mu}_{k+1} = \\lambda \\hat{\\mu}_{k} + (1-\\lambda)\\left(y_{k+1} - G u_{k+1}\\right)$\n\nHere, $y_k$ is the output, $u_k$ is the control recipe, $G  0$ is the nominal gain, $\\epsilon$ is the relative gain error, $\\mu$ is the constant process bias, $y^{*}$ is the target output, $\\hat{\\mu}_k$ is the bias estimate, and $\\lambda$ is the EWMA smoothing constant where $0  \\lambda  1$. The estimation error is defined as $\\tilde{\\mu}_k = \\hat{\\mu}_k - \\mu$.\n\nOur first step is to derive the closed-loop dynamics of the estimation error, $\\tilde{\\mu}_k$. We seek an expression of the form $\\tilde{\\mu}_{k+1} = f(\\tilde{\\mu}_k, \\epsilon)$.\n\nWe begin by substituting the control law for run $k+1$ into the true process model to express the output $y_{k+1}$ in terms of the bias estimate $\\hat{\\mu}_k$:\n$$y_{k+1} = G(1+\\epsilon)u_{k+1} + \\mu = G(1+\\epsilon)\\left(\\frac{y^{*} - \\hat{\\mu}_k}{G}\\right) + \\mu$$\n$$y_{k+1} = (1+\\epsilon)(y^{*} - \\hat{\\mu}_k) + \\mu$$\n\nNext, we analyze the term $\\left(y_{k+1} - G u_{k+1}\\right)$ that appears in the estimator update equation. This term represents the estimated bias at run $k+1$ before the EWMA filter is applied.\n$$y_{k+1} - G u_{k+1} = \\left[(1+\\epsilon)(y^{*} - \\hat{\\mu}_k) + \\mu\\right] - G\\left(\\frac{y^{*} - \\hat{\\mu}_k}{G}\\right)$$\nSimplifying this expression:\n$$y_{k+1} - G u_{k+1} = (1+\\epsilon)(y^{*} - \\hat{\\mu}_k) + \\mu - (y^{*} - \\hat{\\mu}_k)$$\n$$y_{k+1} - G u_{k+1} = (y^{*} - \\hat{\\mu}_k) + \\epsilon(y^{*} - \\hat{\\mu}_k) + \\mu - (y^{*} - \\hat{\\mu}_k)$$\n$$y_{k+1} - G u_{k+1} = \\epsilon(y^{*} - \\hat{\\mu}_k) + \\mu$$\n\nNow, substitute this result into the EWMA estimator update equation:\n$$\\hat{\\mu}_{k+1} = \\lambda \\hat{\\mu}_{k} + (1-\\lambda)\\left[\\epsilon(y^{*} - \\hat{\\mu}_k) + \\mu\\right]$$\nExpanding and rearranging terms to group $\\hat{\\mu}_k$:\n$$\\hat{\\mu}_{k+1} = \\lambda \\hat{\\mu}_{k} - (1-\\lambda)\\epsilon \\hat{\\mu}_k + (1-\\lambda)\\epsilon y^{*} + (1-\\lambda)\\mu$$\n$$\\hat{\\mu}_{k+1} = \\left[\\lambda - (1-\\lambda)\\epsilon\\right] \\hat{\\mu}_k + (1-\\lambda)(\\epsilon y^{*} + \\mu)$$\n\nTo obtain the dynamics of the estimation error $\\tilde{\\mu}_k = \\hat{\\mu}_k - \\mu$, we substitute $\\hat{\\mu}_k = \\tilde{\\mu}_k + \\mu$ and $\\hat{\\mu}_{k+1} = \\tilde{\\mu}_{k+1} + \\mu$ into the equation above:\n$$\\tilde{\\mu}_{k+1} + \\mu = \\left[\\lambda - (1-\\lambda)\\epsilon\\right](\\tilde{\\mu}_k + \\mu) + (1-\\lambda)(\\epsilon y^{*} + \\mu)$$\n$$\\tilde{\\mu}_{k+1} + \\mu = \\left[\\lambda - (1-\\lambda)\\epsilon\\right]\\tilde{\\mu}_k + \\left[\\lambda - (1-\\lambda)\\epsilon\\right]\\mu + (1-\\lambda)\\epsilon y^{*} + (1-\\lambda)\\mu$$\nIsolating $\\tilde{\\mu}_{k+1}$:\n$$\\tilde{\\mu}_{k+1} = \\left[\\lambda - (1-\\lambda)\\epsilon\\right]\\tilde{\\mu}_k + \\lambda\\mu - (1-\\lambda)\\epsilon\\mu + (1-\\lambda)\\epsilon y^{*} + (1-\\lambda)\\mu - \\mu$$\nThe constant terms can be simplified:\n$$\\lambda\\mu - \\mu + (1-\\lambda)\\mu = (\\lambda - 1 + 1 - \\lambda)\\mu = 0$$\nThe remaining terms involving $\\epsilon$ are:\n$$(1-\\lambda)\\epsilon y^{*} - (1-\\lambda)\\epsilon\\mu = (1-\\lambda)\\epsilon(y^{*} - \\mu)$$\nThus, the closed-loop estimation error dynamics are given by the first-order affine difference equation:\n$$\\tilde{\\mu}_{k+1} = \\left[\\lambda - (1-\\lambda)\\epsilon\\right]\\tilde{\\mu}_k + (1-\\lambda)\\epsilon(y^{*} - \\mu)$$\n\nThe stability and convergence rate of this discrete-time system are determined by the magnitude of the eigenvalue of the homogeneous part. Since this is a scalar system, the eigenvalue is the coefficient of $\\tilde{\\mu}_k$. We define this as the closed-loop contraction factor, $\\rho$:\n$$\\rho(\\epsilon) = \\lambda - (1-\\lambda)\\epsilon$$\n\nThe problem imposes two conditions:\n$1$. Stability: For a discrete-time linear system, all eigenvalues must lie strictly inside the unit disk. This means $|\\rho(\\epsilon)|  1$.\n$2$. Performance: The magnitude of the closed-loop contraction factor must not exceed a specified bound of $0.7$. This means $|\\rho(\\epsilon)| \\leq 0.7$.\n\nThe performance condition is stricter than the stability condition ($0.7  1$), so satisfying it automatically ensures stability. We must therefore solve the inequality:\n$$|\\lambda - (1-\\lambda)\\epsilon| \\leq 0.7$$\n\nWe are given $\\lambda = 0.6$, which implies $1-\\lambda = 0.4$. Substituting these values:\n$$|0.6 - 0.4\\epsilon| \\leq 0.7$$\nThis inequality is equivalent to:\n$$-0.7 \\leq 0.6 - 0.4\\epsilon \\leq 0.7$$\nWe solve this two-sided inequality for $\\epsilon$. First, subtract $0.6$ from all parts:\n$$-0.7 - 0.6 \\leq -0.4\\epsilon \\leq 0.7 - 0.6$$\n$$-1.3 \\leq -0.4\\epsilon \\leq 0.1$$\nNext, divide all parts by $-0.4$. This operation reverses the direction of the inequality signs:\n$$\\frac{-1.3}{-0.4} \\geq \\epsilon \\geq \\frac{0.1}{-0.4}$$\n$$\\frac{1.3}{0.4} \\geq \\epsilon \\geq -0.25$$\n$$3.25 \\geq \\epsilon \\geq -0.25$$\nSo, the range of gain mismatch $\\epsilon$ that satisfies the performance requirement is $[-0.25, 3.25]$.\n\nThe problem asks for the largest symmetric gain-mismatch magnitude, $\\epsilon_{\\max}$, such that for all $\\epsilon$ satisfying $|\\epsilon| \\leq \\epsilon_{\\max}$, the conditions hold. This means we must find the largest value $\\epsilon_{\\max} \\geq 0$ for which the symmetric interval $[-\\epsilon_{\\max}, \\epsilon_{\\max}]$ is a subset of the valid interval $[-0.25, 3.25]$.\nFor the interval $[-\\epsilon_{\\max}, \\epsilon_{\\max}]$ to be contained within $[-0.25, 3.25]$, we must satisfy both:\n$$-\\epsilon_{\\max} \\geq -0.25 \\implies \\epsilon_{\\max} \\leq 0.25$$\n$$\\epsilon_{\\max} \\leq 3.25$$\nTo satisfy both conditions simultaneously, $\\epsilon_{\\max}$ must be less than or equal to the minimum of the two bounds:\n$$\\epsilon_{\\max} \\leq \\min(0.25, 3.25) = 0.25$$\nThe largest value for $\\epsilon_{\\max}$ is therefore $0.25$.\n\nThe problem requires the answer to be rounded to four significant figures.\n$$\\epsilon_{\\max} = 0.2500$$",
            "answer": "$$\\boxed{0.2500}$$"
        }
    ]
}