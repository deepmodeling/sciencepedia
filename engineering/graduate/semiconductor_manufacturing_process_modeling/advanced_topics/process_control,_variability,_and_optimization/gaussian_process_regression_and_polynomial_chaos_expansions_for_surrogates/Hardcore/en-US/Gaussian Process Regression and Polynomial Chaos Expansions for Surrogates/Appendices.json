{
    "hands_on_practices": [
        {
            "introduction": "When constructing a Polynomial Chaos Expansion (PCE) surrogate, a primary decision is the choice of the polynomial basis. The size of this basis, which depends on the input dimension $d$ and the maximum polynomial degree $p$, directly impacts the model's flexibility and computational cost. This exercise guides you through a fundamental derivation that quantifies this relationship, connecting an abstract combinatorial problem to the practical task of defining a PCE model's complexity . Understanding this scaling is the first step toward building efficient and effective surrogate models.",
            "id": "4128836",
            "problem": "A computational lithography surrogate is constructed for the line-edge roughness in a photoresist, modeled as a function of $d$ statistically independent, standardized process inputs: exposure dose, defocus, mask bias, numerical aperture, resist thickness, and post-exposure bake time, so $d=6$. Two surrogate modeling approaches are under consideration: Gaussian Process Regression (GPR) and Polynomial Chaos Expansion (PCE). For the PCE, an orthonormal basis consistent with the marginal input distributions is chosen, and a total-degree truncation of order $p$ is imposed, meaning that all multivariate polynomial basis functions whose total degree is less than or equal to $p$ are retained.\n\nFrom first principles in combinatorial counting, starting only from the definition that a multivariate monomial in $d$ variables has the form $x^{\\boldsymbol{\\alpha}}=\\prod_{i=1}^{d}x_{i}^{\\alpha_{i}}$ with a multi-index $\\boldsymbol{\\alpha}\\in\\mathbb{N}_{0}^{d}$ and total degree $\\lvert\\boldsymbol{\\alpha}\\rvert=\\sum_{i=1}^{d}\\alpha_{i}$, derive a closed-form expression for the number of distinct multivariate monomials in $d$ variables whose total degree is at most $p$. Then, evaluate this count for the lithography model with $d=6$ inputs and a total-degree truncation order $p=3$.\n\nYour final answer must be a single integer. No rounding is required, and no units are needed. Express the final answer as a single number only.",
            "solution": "The problem statement is critically validated and deemed valid. It is scientifically grounded in the fields of surrogate modeling and combinatorial mathematics, well-posed with all necessary information provided, and objective in its formulation. No flaws were found. We may proceed with the solution.\n\nThe problem asks for two tasks. First, to derive a closed-form expression for the number of distinct multivariate monomials in $d$ variables whose total degree is at most $p$. Second, to evaluate this count for a specific application in semiconductor manufacturing process modeling where the number of input variables is $d=6$ and the total-degree truncation order is $p=3$.\n\nThe number of basis functions in a Polynomial Chaos Expansion (PCE) with a total-degree truncation of order $p$ is equivalent to the number of distinct multivariate monomials of the form $x^{\\boldsymbol{\\alpha}} = \\prod_{i=1}^{d}x_{i}^{\\alpha_{i}}$ where the total degree $\\lvert\\boldsymbol{\\alpha}\\rvert = \\sum_{i=1}^{d}\\alpha_{i}$ is less than or equal to $p$. The multi-index $\\boldsymbol{\\alpha}$ is a vector of non-negative integers, $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\dots, \\alpha_d) \\in \\mathbb{N}_{0}^{d}$.\n\nOur objective is to find the cardinality of the set of all such valid multi-indices, which we denote by $N$. This is the number of integer solutions to the inequality:\n$$ \\sum_{i=1}^{d} \\alpha_i \\leq p, \\quad \\text{where } \\alpha_i \\in \\mathbb{N}_0 \\text{ for } i=1, 2, \\dots, d $$\nThis is a problem in combinatorial enumeration. To solve it from first principles, we can convert the inequality into an equality by introducing a non-negative integer slack variable, let's call it $\\alpha_{d+1}$. The original inequality is equivalent to the equation:\n$$ \\alpha_1 + \\alpha_2 + \\dots + \\alpha_d + \\alpha_{d+1} = p $$\nwhere $\\alpha_i \\geq 0$ for all $i \\in \\{1, 2, \\dots, d+1\\}$. The slack variable $\\alpha_{d+1}$ represents the difference $p - \\sum_{i=1}^{d} \\alpha_i$, which must be non-negative according to the original inequality. There is a one-to-one correspondence between the solution sets of the inequality in $d$ variables and the equality in $d+1$ variables.\n\nWe now need to count the number of non-negative integer solutions to this equation. This is a classic combinatorial problem that can be solved using the \"stars and bars\" method. Imagine we have $p$ indistinguishable items (the \"stars,\" representing the units of the sum) that we want to place into $d+1$ distinguishable bins (representing the variables $\\alpha_1, \\dots, \\alpha_{d+1}$). To partition the stars into $d+1$ bins, we need $d$ separators (the \"bars\").\n\nConsider a sequence of $p$ stars and $d$ bars arranged in a line. For example, if $p=3$ and $d=2$ (meaning $d+1=3$ bins), a possible arrangement is `*|**|`. This corresponds to the solution $\\alpha_1=1$, $\\alpha_2=2$, $\\alpha_3=0$. The number of stars before the first bar is $\\alpha_1$, the number of stars between the first and second bar is $\\alpha_2$, and so on. The number of stars after the last bar is $\\alpha_{d+1}$.\n\nThe total number of positions in this arrangement is the sum of the number of stars and the number of bars, which is $p+d$. The total number of distinct solutions is equal to the number of ways we can arrange these $p$ stars and $d$ bars. This is equivalent to choosing the positions for the $d$ bars from the $p+d$ available positions. The remaining positions will be filled by stars.\n\nThe number of ways to do this is given by the binomial coefficient \"p+d choose d\":\n$$ N = \\binom{p+d}{d} $$\nBy the symmetry of binomial coefficients, $\\binom{n}{k} = \\binom{n}{n-k}$, this is also equal to:\n$$ N = \\binom{p+d}{p} $$\nThis is the closed-form expression for the number of distinct multivariate monomials in $d$ variables with a total degree of at most $p$.\n\nNow, we must evaluate this expression for the specific case given in the problem: a lithography model with $d=6$ inputs and a total-degree truncation order of $p=3$.\nSubstituting these values into our derived formula:\n$$ N = \\binom{p+d}{d} = \\binom{3+6}{6} = \\binom{9}{6} $$\nWe can compute this binomial coefficient. It is often simpler to use the symmetric form:\n$$ \\binom{9}{6} = \\binom{9}{9-6} = \\binom{9}{3} $$\nNow we expand the binomial coefficient using its definition in terms of factorials:\n$$ \\binom{9}{3} = \\frac{9!}{3!(9-3)!} = \\frac{9!}{3!6!} $$\n$$ \\binom{9}{3} = \\frac{9 \\times 8 \\times 7 \\times 6!}{ (3 \\times 2 \\times 1) \\times 6!} $$\nCanceling the $6!$ term:\n$$ \\binom{9}{3} = \\frac{9 \\times 8 \\times 7}{3 \\times 2 \\times 1} = \\frac{504}{6} $$\nPerforming the division:\n$$ \\binom{9}{3} = 3 \\times 4 \\times 7 = 12 \\times 7 = 84 $$\nTherefore, for a PCE model with $d=6$ inputs and a total-degree truncation of order $p=3$, there are $84$ basis functions.",
            "answer": "$$\\boxed{84}$$"
        },
        {
            "introduction": "The standard Gaussian Process (GP) regression framework assumes a constant noise variance across all observations, an assumption known as homoscedasticity. However, in many physical experiments and metrology applications, measurement error is input-dependent, or heteroscedastic. This practice tackles this realistic scenario, forcing a departure from standard formulas and a return to the first principles of GP modeling . By deriving the posterior predictive equations for a GP with input-dependent noise, you will learn how to build more faithful models that correctly weight the information from each data point according to its specific uncertainty.",
            "id": "4128838",
            "problem": "In advanced semiconductor manufacturing, spectroscopic ellipsometry and reflectometry are commonly used for metrology of thin film thickness. These measurement systems often exhibit variance that increases with film thickness due to reduced sensitivity and model-form uncertainty at higher optical path lengths. Consider modeling the latent mapping from a controllable deposition setting to measured film thickness as a Gaussian Process (GP), and assume that the reported metrology error variance is input-dependent. Let the latent function be $f(x)$, where $x$ denotes the deposition outcome (film thickness) in nanometers, and the measurement model be $y_{i} = f(x_{i}) + \\epsilon_{i}$ with $\\epsilon_{i} \\sim \\mathcal{N}(0, g(x_{i}))$ independently across $i$, where $g(x)$ is the input-dependent noise variance provided by the metrology system. The GP prior on $f$ is zero-mean with a squared exponential kernel,\n$$\nk(x, x') = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{(x - x')^{2}}{2 \\ell^{2}}\\right).\n$$\nYou are given two measurements at $x_{1} = 50$ and $x_{2} = 150$ nanometers, and you will predict at $x_{\\star} = 100$ nanometers. The hyperparameters are $\\sigma_{f} = 20$ and $\\ell = 60$ nanometers. The metrology variance model is\n$$\ng(x) = \\sigma_{0}^{2} + \\beta x^{2},\n$$\nwith $\\sigma_{0} = 5$ nanometers and $\\beta = 1.0 \\times 10^{-3}$.\n\nTask:\n1. Using fundamental properties of multivariate normal distributions and the definition of a Gaussian Process (GP), explain why a homoscedastic noise assumption (constant variance across all inputs) is inadequate for film thickness metrology when the measurement variance depends on $x$, and show how such an assumption misweights the likelihood contributions of data points at different thicknesses.\n2. Derive the posterior predictive mean and variance for a heteroscedastic GP with input-dependent noise variance $g(x)$, starting explicitly from the joint Gaussian structure and conditioning rules without invoking shortcut formulas.\n3. Specializing your general result to the provided two-point dataset and hyperparameters, write the exact analytical expression for the posterior predictive variance at $x_{\\star} = 100$ nanometers. Express the final variance in $\\mathrm{nm}^{2}$ and give it as a closed-form symbolic expression in terms of exponentials and the given numeric hyperparameters. Do not approximate or round; provide the exact expression.\n\nNote: Gaussian Process (GP) and Polynomial Chaos Expansion (PCE) are both surrogate modeling approaches; while PCE typically assumes a known input distribution and often noise-free observations (or explicitly modeled noise channels), metrology-driven input-dependent noise necessitates a heteroscedastic likelihood treatment when using GP surrogates. Your derivation must begin from the multivariate normal laws and the GP definition, not from ready-made GP regression formulas. Your final answer must be a single closed-form analytic expression for the predictive variance at $x_{\\star}$ as specified above.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The scenario is a realistic application of advanced statistical modeling in semiconductor manufacturing metrology. All parameters and models are clearly defined.\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n### Part 1: Inadequacy of the Homoscedastic Noise Assumption\n\nA Gaussian Process (GP) models a latent function $f(x)$ as a draw from a distribution over functions. The observation model relates the measured data $y_i$ to the latent function value $f(x_i)$ at input $x_i$. In this problem, the model is $y_i = f(x_i) + \\epsilon_i$, where the noise term $\\epsilon_i$ is distributed as $\\epsilon_i \\sim \\mathcal{N}(0, g(x_i))$. The key feature is that the noise variance, $g(x_i)$, depends on the input $x_i$. This is known as heteroscedasticity.\n\nThe likelihood of a set of observations $\\mathbf{y} = \\{y_1, \\dots, y_N\\}$ given the corresponding latent function values $\\mathbf{f} = \\{f(x_1), \\dots, f(x_N)\\}$ is given by the product of the individual probability densities, due to the independence of the noise terms $\\epsilon_i$:\n$$\np(\\mathbf{y}|\\mathbf{f}) = \\prod_{i=1}^{N} p(y_i|f(x_i)) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi g(x_i)}} \\exp\\left(-\\frac{(y_i - f(x_i))^2}{2 g(x_i)}\\right)\n$$\nThe log-likelihood, which is typically maximized (or forms a part of the posterior probability) during model fitting, is:\n$$\n\\ln p(\\mathbf{y}|\\mathbf{f}) = \\sum_{i=1}^{N} \\left( -\\frac{1}{2} \\ln(2\\pi) - \\frac{1}{2}\\ln(g(x_i)) - \\frac{(y_i - f(x_i))^2}{2 g(x_i)} \\right)\n$$\nIgnoring constants, the part of the log-likelihood that depends on the fit is $\\sum_{i=1}^{N} \\left( -\\frac{1}{2}\\ln(g(x_i)) - \\frac{(y_i - f(x_i))^2}{2 g(x_i)} \\right)$. The term $(y_i - f(x_i))^2$ represents the squared residual for the $i$-th data point. Its contribution to the log-likelihood is weighted by $1/(2g(x_i))$.\n\nA homoscedastic noise assumption posits that the noise variance is constant for all inputs, i.e., $g(x_i) = \\sigma_n^2$ for all $i$. Under this incorrect assumption, the log-likelihood term becomes:\n$$\n\\sum_{i=1}^{N} \\left( -\\frac{1}{2}\\ln(\\sigma_n^2) - \\frac{(y_i - f(x_i))^2}{2 \\sigma_n^2} \\right) \\propto -\\frac{1}{2\\sigma_n^2} \\sum_{i=1}^{N} (y_i - f(x_i))^2\n$$\nIn this homoscedastic case, each squared residual $(y_i - f(x_i))^2$ is weighted equally.\n\nThe problem arises when $g(x)$ is not constant. In the given semiconductor metrology context, variance increases with film thickness $x$. This means a measurement at a large thickness $x_{\\text{high}}$ will have a large variance $g(x_{\\text{high}})$, while a measurement at a small thickness $x_{\\text{low}}$ will have a small variance $g(x_{\\text{low}})$.\n- For the precise measurement at $x_{\\text{low}}$, $g(x_{\\text{low}})$ is small, so its likelihood weight $1/g(x_{\\text{low}})$ is large. The GP model is thus heavily penalized for deviations from this data point, forcing the posterior to pass closer to $y_{\\text{low}}$.\n- For the noisy measurement at $x_{\\text{high}}$, $g(x_{\\text{high}})$ is large, so its weight $1/g(x_{\\text{high}})$ is small. The model is less penalized for deviating from $y_{\\text{high}}$, correctly acknowledging its lower certainty.\n\nA homoscedastic assumption, perhaps using an average variance, would assign the same weight to both the precise and the noisy measurements. This misweights their contributions: it underweights the informative, low-noise data (treating it as noisier than it is) and overweights the uninformative, high-noise data (treating it as more precise than it is). This leads to a suboptimal and biased surrogate model that is \"distracted\" by noise at high-thickness points and fails to properly anchor to the high-quality data at low-thickness points.\n\n### Part 2: Derivation of Posterior Predictive Distribution\n\nWe start from the fundamental definition of a GP. The set of latent function values at the training points, $\\mathbf{f} = [f(x_1), f(x_2)]^T$, and at the test point, $f_{\\star} = f(x_{\\star})$, have a joint Gaussian prior distribution. Since the GP is zero-mean, this is:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_{\\star} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK(\\mathbf{x}, \\mathbf{x}) & K(\\mathbf{x}, x_{\\star}) \\\\\nK(x_{\\star}, \\mathbf{x}) & K(x_{\\star}, x_{\\star})\n\\end{pmatrix}\n\\right)\n$$\nwhere $\\mathbf{x} = [x_1, x_2]^T$. We use the shorthand $K = K(\\mathbf{x}, \\mathbf{x})$, $\\mathbf{k}_{\\star} = K(\\mathbf{x}, x_{\\star})$, and $k_{\\star\\star} = K(x_{\\star}, x_{\\star})$.\n\nThe observations are $\\mathbf{y} = \\mathbf{f} + \\mathbf{\\epsilon}$, with noise $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_n)$. For heteroscedastic noise, $\\Sigma_n$ is a diagonal matrix of input-dependent variances: $\\Sigma_n = \\mathrm{diag}(g(x_1), g(x_2))$.\n\nWe need the posterior distribution of $f_{\\star}$ given the observations $\\mathbf{y}$, which is $p(f_{\\star}|\\mathbf{y})$. To find this, we first establish the joint distribution of the observables $\\mathbf{y}$ and the quantity to be predicted, $f_{\\star}$. Since $\\mathbf{y}$ and $f_{\\star}$ are linear combinations of jointly Gaussian variables, they are themselves jointly Gaussian.\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_{\\star} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} \\mathbb{E}[\\mathbf{y}] \\\\ \\mathbb{E}[f_{\\star}] \\end{pmatrix},\n\\begin{pmatrix}\n\\mathrm{Cov}(\\mathbf{y}, \\mathbf{y}) & \\mathrm{Cov}(\\mathbf{y}, f_{\\star}) \\\\\n\\mathrm{Cov}(f_{\\star}, \\mathbf{y}) & \\mathrm{Cov}(f_{\\star}, f_{\\star})\n\\end{pmatrix}\n\\right)\n$$\nThe expectations are zero due to the zero-mean GP prior and zero-mean noise. The covariance terms are:\n- $\\mathrm{Cov}(\\mathbf{y}, \\mathbf{y}) = \\mathrm{Cov}(\\mathbf{f} + \\mathbf{\\epsilon}, \\mathbf{f} + \\mathbf{\\epsilon}) = \\mathrm{Cov}(\\mathbf{f}, \\mathbf{f}) + \\mathrm{Cov}(\\mathbf{\\epsilon}, \\mathbf{\\epsilon}) = K + \\Sigma_n$, since $\\mathbf{f}$ and $\\mathbf{\\epsilon}$ are independent.\n- $\\mathrm{Cov}(\\mathbf{y}, f_{\\star}) = \\mathrm{Cov}(\\mathbf{f} + \\mathbf{\\epsilon}, f_{\\star}) = \\mathrm{Cov}(\\mathbf{f}, f_{\\star}) = \\mathbf{k}_{\\star}$.\n- $\\mathrm{Cov}(f_{\\star}, f_{\\star}) = k_{\\star\\star}$.\n\nThus, the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_{\\star} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK + \\Sigma_n & \\mathbf{k}_{\\star} \\\\\n\\mathbf{k}_{\\star}^T & k_{\\star\\star}\n\\end{pmatrix}\n\\right)\n$$\nUsing the standard formula for conditioning a multivariate normal distribution, if $\\begin{pmatrix} \\mathbf{z}_1 \\\\ \\mathbf{z}_2 \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} \\mathbf{\\mu}_1 \\\\ \\mathbf{\\mu}_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix} \\right)$, then the conditional distribution $p(\\mathbf{z}_2|\\mathbf{z}_1)$ is Gaussian with mean $\\bar{\\mathbf{\\mu}}_2 = \\mathbf{\\mu}_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{z}_1 - \\mathbf{\\mu}_1)$ and covariance $\\bar{\\Sigma}_{22} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$.\n\nApplying this to our case with $\\mathbf{z}_1 = \\mathbf{y}$ and $\\mathbf{z}_2 = f_{\\star}$ (and zero means), we obtain the posterior predictive distribution $p(f_{\\star}|\\mathbf{y}) = \\mathcal{N}(\\bar{f}_{\\star}, \\mathbb{V}[f_{\\star}])$ where:\n- Posterior predictive mean: $\\bar{f}_{\\star} = \\mathbf{k}_{\\star}^T (K + \\Sigma_n)^{-1} \\mathbf{y}$\n- Posterior predictive variance: $\\mathbb{V}[f_{\\star}] = k_{\\star\\star} - \\mathbf{k}_{\\star}^T (K + \\Sigma_n)^{-1} \\mathbf{k}_{\\star}$\n\nThese are the general formulas for a GP with heteroscedastic noise.\n\n### Part 3: Analytical Expression for the Posterior Predictive Variance\n\nWe now specialize the variance formula for the given data. The units of variance will be $\\mathrm{nm}^2$.\n**Given data:**\n- Training points: $x_1 = 50$, $x_2 = 150$\n- Test point: $x_{\\star} = 100$\n- Hyperparameters: $\\sigma_f = 20$, $\\ell = 60$\n- Noise parameters: $\\sigma_0 = 5$, $\\beta = 1.0 \\times 10^{-3}$\n\n**1. Calculate kernel matrix $K$:**\nThe kernel is $k(x, x') = 20^2 \\exp\\left(-\\frac{(x - x')^2}{2 \\cdot 60^2}\\right) = 400 \\exp\\left(-\\frac{(x - x')^2}{7200}\\right)$.\n$K_{11} = k(50, 50) = 400$.\n$K_{22} = k(150, 150) = 400$.\n$K_{12} = K_{21} = k(50, 150) = 400 \\exp\\left(-\\frac{(50-150)^2}{7200}\\right) = 400 \\exp\\left(-\\frac{10000}{7200}\\right) = 400 \\exp\\left(-\\frac{25}{18}\\right)$.\n$$ K = \\begin{pmatrix} 400 & 400 \\exp(-\\frac{25}{18}) \\\\ 400 \\exp(-\\frac{25}{18}) & 400 \\end{pmatrix} $$\n\n**2. Calculate noise covariance matrix $\\Sigma_n$:**\nThe noise variance is $g(x) = 5^2 + (1.0 \\times 10^{-3}) x^2 = 25 + 0.001 x^2$.\n$g(x_1) = g(50) = 25 + 0.001(50^2) = 25 + 2.5 = 27.5$.\n$g(x_2) = g(150) = 25 + 0.001(150^2) = 25 + 22.5 = 47.5$.\n$$ \\Sigma_n = \\begin{pmatrix} 27.5 & 0 \\\\ 0 & 47.5 \\end{pmatrix} $$\n\n**3. Calculate kernel vector $\\mathbf{k}_{\\star}$ and scalar $k_{\\star\\star}$:**\n$k(x_1, x_{\\star}) = k(50, 100) = 400 \\exp\\left(-\\frac{(50-100)^2}{7200}\\right) = 400 \\exp\\left(-\\frac{2500}{7200}\\right) = 400 \\exp\\left(-\\frac{25}{72}\\right)$.\n$k(x_2, x_{\\star}) = k(150, 100) = 400 \\exp\\left(-\\frac{(150-100)^2}{7200}\\right) = 400 \\exp\\left(-\\frac{2500}{7200}\\right) = 400 \\exp\\left(-\\frac{25}{72}\\right)$.\n$$ \\mathbf{k}_{\\star} = \\begin{pmatrix} 400 \\exp(-\\frac{25}{72}) \\\\ 400 \\exp(-\\frac{25}{72}) \\end{pmatrix} $$\n$k_{\\star\\star} = k(100, 100) = 400$.\n\n**4. Assemble and compute the variance:**\nWe need to compute $\\mathbb{V}[f_{\\star}] = k_{\\star\\star} - \\mathbf{k}_{\\star}^T (K + \\Sigma_n)^{-1} \\mathbf{k}_{\\star}$.\nFirst, find $M = K + \\Sigma_n$:\n$$ M = \\begin{pmatrix} 400+27.5 & 400 \\exp(-\\frac{25}{18}) \\\\ 400 \\exp(-\\frac{25}{18}) & 400+47.5 \\end{pmatrix} = \\begin{pmatrix} 427.5 & 400 \\exp(-\\frac{25}{18}) \\\\ 400 \\exp(-\\frac{25}{18}) & 447.5 \\end{pmatrix} $$\nLet's invert this $2 \\times 2$ matrix $M = \\begin{pmatrix} a & c \\\\ c & b \\end{pmatrix}$: $M^{-1} = \\frac{1}{ab-c^2} \\begin{pmatrix} b & -c \\\\ -c & a \\end{pmatrix}$.\nThe quadratic form $\\mathbf{k}_{\\star}^T M^{-1} \\mathbf{k}_{\\star}$ with $\\mathbf{k}_{\\star} = [k_s, k_s]^T$ is:\n$$ \\mathbf{k}_{\\star}^T M^{-1} \\mathbf{k}_{\\star} = \\frac{1}{ab-c^2} \\begin{pmatrix} k_s & k_s \\end{pmatrix} \\begin{pmatrix} b & -c \\\\ -c & a \\end{pmatrix} \\begin{pmatrix} k_s \\\\ k_s \\end{pmatrix} = \\frac{k_s^2(a+b-2c)}{ab-c^2} $$\nSubstituting the values:\n- $a = 427.5$\n- $b = 447.5$\n- $c = 400 \\exp(-\\frac{25}{18})$\n- $k_s = 400 \\exp(-\\frac{25}{72})$\nThe quadratic term is:\n$$ \\frac{(400 \\exp(-\\frac{25}{72}))^2 (427.5 + 447.5 - 2 \\cdot 400 \\exp(-\\frac{25}{18}))}{(427.5)(447.5) - (400 \\exp(-\\frac{25}{18}))^2} $$\nSimplifying the numerical parts:\n- $(400 \\exp(-\\frac{25}{72}))^2 = 160000 \\exp(-\\frac{50}{72}) = 160000 \\exp(-\\frac{25}{36})$\n- $427.5 + 447.5 = 875$\n- $(427.5)(447.5) = 191306.25$\n- $(400 \\exp(-\\frac{25}{18}))^2 = 160000 \\exp(-\\frac{50}{18}) = 160000 \\exp(-\\frac{25}{9})$\nThe quadratic term becomes:\n$$ \\frac{160000 \\exp(-\\frac{25}{36}) (875 - 800 \\exp(-\\frac{25}{18}))}{191306.25 - 160000 \\exp(-\\frac{25}{9})} $$\nFinally, the posterior predictive variance is $\\mathbb{V}[f_{\\star}] = k_{\\star\\star} - (\\text{quadratic term})$.\n$$ \\mathbb{V}[f_{\\star}] = 400 - \\frac{160000 \\exp(-\\frac{25}{36}) (875 - 800 \\exp(-\\frac{25}{18}))}{191306.25 - 160000 \\exp(-\\frac{25}{9})} $$\nThis is the exact analytical expression for the posterior predictive variance in units of $\\mathrm{nm}^2$.",
            "answer": "$$\n\\boxed{400 - \\frac{160000 \\exp(-\\frac{25}{36}) \\left(875 - 800 \\exp(-\\frac{25}{18})\\right)}{191306.25 - 160000 \\exp(-\\frac{25}{9})}}\n$$"
        },
        {
            "introduction": "Beyond treating a complex simulation as a \"black box,\" surrogate modeling can be integrated directly into the governing physics of a system. This \"intrusive\" approach is particularly powerful for propagating uncertainty through dynamic models described by differential equations. This exercise demonstrates how to apply an intrusive Polynomial Chaos Expansion to a stochastic partial differential equation (SPDE) for dopant diffusion . By using a Galerkin projection, you will transform the SPDE into a larger, coupled system of deterministic PDEs, a complete description of how the uncertainty, captured by the PCE coefficients, evolves in space and time.",
            "id": "4128810",
            "problem": "Consider dopant diffusion during thermal processing in a semiconductor wafer, modeled by the partial differential equation for dopant concentration $C(\\mathbf{x}, t, \\xi)$,\n$$\n\\partial_{t} C(\\mathbf{x}, t, \\xi) \\;=\\; D(\\xi)\\,\\nabla^{2} C(\\mathbf{x}, t, \\xi),\n$$\nwhere $\\mathbf{x} \\in \\Omega \\subset \\mathbb{R}^{d}$ denotes spatial position in the wafer, $t \\geq 0$ is time, and $\\xi$ is a scalar uncertain process parameter (for example, an uncertain activation energy or process temperature-related parameter) with a known probability density function $\\rho(\\xi)$ supported on a measurable set $\\mathcal{S} \\subseteq \\mathbb{R}$. Assume $\\Omega$ is a bounded domain with sufficiently regular boundary so that standard diffusion operators are well-defined under physically appropriate boundary conditions.\n\nConstruct an intrusive Polynomial Chaos Expansion (PCE) discretization of this stochastic partial differential equation by expanding both the dopant concentration $C(\\mathbf{x}, t, \\xi)$ and the diffusivity $D(\\xi)$ in the orthonormal polynomial basis $\\{\\Psi_{k}(\\xi)\\}_{k=0}^{\\infty}$ associated with the weight $\\rho(\\xi)$, where orthonormality is defined by the inner product\n$$\n\\langle f, g \\rangle \\;\\equiv\\; \\int_{\\mathcal{S}} f(\\xi)\\,g(\\xi)\\,\\rho(\\xi)\\,d\\xi, \\quad \\langle \\Psi_{i}, \\Psi_{j} \\rangle \\;=\\; \\delta_{ij}.\n$$\nSpecifically, suppose $C(\\mathbf{x}, t, \\xi)$ is truncated at total order $P$ and $D(\\xi)$ is truncated at total order $Q$, with\n$$\nC(\\mathbf{x}, t, \\xi) \\;=\\; \\sum_{k=0}^{P} c_{k}(\\mathbf{x}, t)\\,\\Psi_{k}(\\xi), \n\\qquad \nD(\\xi) \\;=\\; \\sum_{n=0}^{Q} d_{n}\\,\\Psi_{n}(\\xi),\n$$\nwhere the coefficient functions $c_{k}(\\mathbf{x}, t)$ are deterministic in $\\mathbf{x}$ and $t$, and the scalars $d_{n}$ are deterministic expansion coefficients of the uncertain diffusivity.\n\nStarting from the diffusion equation above, perform a weighted residual projection onto each basis element $\\Psi_{j}(\\xi)$ using the inner product with respect to $\\rho(\\xi)$ and derive the coupled deterministic system governing the coefficient fields $\\{c_{k}(\\mathbf{x}, t)\\}_{k=0}^{P}$. Express the coupling in terms of the diffusivity expansion coefficients $\\{d_{n}\\}_{n=0}^{Q}$ and the triple products of the orthonormal basis $\\langle \\Psi_{j}\\Psi_{n}\\Psi_{k} \\rangle$.\n\nYour final answer should be a single closed-form analytic expression for the entries of the coupling matrix that multiplies $\\nabla^{2}\\mathbf{c}(\\mathbf{x}, t)$ in the coefficient system. No rounding is required. Do not include any units in the final answer.",
            "solution": "The user wants to derive the entries of the coupling matrix in an intrusive Polynomial Chaos Expansion (PCE) formulation of a stochastic diffusion equation.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n\n*   **Governing Equation:** The stochastic partial differential equation (SPDE) for dopant concentration $C(\\mathbf{x}, t, \\xi)$ is given by $\\partial_{t} C(\\mathbf{x}, t, \\xi) \\;=\\; D(\\xi)\\,\\nabla^{2} C(\\mathbf{x}, t, \\xi)$.\n*   **Variables and Parameters:** $\\mathbf{x} \\in \\Omega \\subset \\mathbb{R}^{d}$ is the spatial position, $t \\geq 0$ is time, and $\\xi$ is a scalar uncertain parameter with probability density function $\\rho(\\xi)$ on the support $\\mathcal{S} \\subseteq \\mathbb{R}$.\n*   **Orthonormal Basis:** An orthonormal polynomial basis $\\{\\Psi_{k}(\\xi)\\}_{k=0}^{\\infty}$ is defined with respect to the weight function $\\rho(\\xi)$.\n*   **Inner Product:** The inner product is defined as $\\langle f, g \\rangle \\;\\equiv\\; \\int_{\\mathcal{S}} f(\\xi)\\,g(\\xi)\\,\\rho(\\xi)\\,d\\xi$.\n*   **Orthonormality Condition:** The basis functions satisfy $\\langle \\Psi_{i}, \\Psi_{j} \\rangle \\;=\\; \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n*   **Polynomial Chaos Expansions:** The concentration $C(\\mathbf{x}, t, \\xi)$ and diffusivity $D(\\xi)$ are expanded in this basis up to finite orders $P$ and $Q$, respectively:\n    $$\n    C(\\mathbf{x}, t, \\xi) \\;=\\; \\sum_{k=0}^{P} c_{k}(\\mathbf{x}, t)\\,\\Psi_{k}(\\xi)\n    $$\n    $$\n    D(\\xi) \\;=\\; \\sum_{n=0}^{Q} d_{n}\\,\\Psi_{n}(\\xi)\n    $$\n*   **Coefficients:** The coefficients $c_{k}(\\mathbf{x}, t)$ are deterministic functions of space and time, and the coefficients $d_{n}$ are deterministic constants.\n*   **Objective:** Derive the coupled deterministic system for the coefficients $\\{c_{k}(\\mathbf{x}, t)\\}_{k=0}^{P}$ by performing a Galerkin projection (weighted residual) of the SPDE onto each basis function $\\Psi_{j}(\\xi)$. The goal is to find the expression for the entries of the coupling matrix that results from this procedure.\n\n**1.2. Validation Using Extracted Givens**\n\n*   **Scientific Grounding:** The problem is based on Fick's second law of diffusion, a fundamental principle in materials science and physics. The use of Polynomial Chaos Expansions to handle uncertainty in the diffusivity is a standard, well-established technique in the field of Uncertainty Quantification (UQ) and computational engineering. The setup is scientifically and mathematically sound.\n*   **Well-Posedness:** The problem provides a clear governing equation, expansions for the stochastic quantities, and a defined mathematical procedure (Galerkin projection) to follow. The objective is to derive a specific analytical expression, which is a well-defined task. A unique solution exists based on the provided framework.\n*   **Objectivity:** The problem statement is precise, formal, and free of any subjective or ambiguous language.\n\nAll criteria for a valid problem are met. There are no scientific flaws, inconsistencies, or ambiguities.\n\n**1.3. Verdict and Action**\n\nThe problem is **valid**. The solution process will now proceed.\n\n### Step 2: Solution Derivation\n\nThe derivation begins by substituting the given PCEs for $C(\\mathbf{x}, t, \\xi)$ and $D(\\xi)$ into the governing SPDE.\n\nThe SPDE is:\n$$\n\\partial_{t} C(\\mathbf{x}, t, \\xi) \\;=\\; D(\\xi)\\,\\nabla^{2} C(\\mathbf{x}, t, \\xi)\n$$\nSubstituting the expansions yields:\n$$\n\\partial_{t} \\left( \\sum_{k=0}^{P} c_{k}(\\mathbf{x}, t)\\,\\Psi_{k}(\\xi) \\right) \\;=\\; \\left( \\sum_{n=0}^{Q} d_{n}\\,\\Psi_{n}(\\xi) \\right) \\nabla^{2} \\left( \\sum_{k=0}^{P} c_{k}(\\mathbf{x}, t)\\,\\Psi_{k}(\\xi) \\right)\n$$\nThe differential operators $\\partial_{t}$ and $\\nabla^{2}$ act on the deterministic coefficients $c_{k}(\\mathbf{x}, t)$, not on the basis functions $\\Psi_{k}(\\xi)$. Applying the operators, we get:\n$$\n\\sum_{k=0}^{P} \\left( \\partial_{t} c_{k}(\\mathbf{x}, t) \\right) \\Psi_{k}(\\xi) \\;=\\; \\left( \\sum_{n=0}^{Q} d_{n}\\,\\Psi_{n}(\\xi) \\right) \\left( \\sum_{k=0}^{P} \\left( \\nabla^{2} c_{k}(\\mathbf{x}, t) \\right) \\Psi_{k}(\\xi) \\right)\n$$\nWe expand the product on the right-hand side:\n$$\n\\sum_{k=0}^{P} \\left( \\partial_{t} c_{k}(\\mathbf{x}, t) \\right) \\Psi_{k}(\\xi) \\;=\\; \\sum_{n=0}^{Q} \\sum_{k=0}^{P} d_{n} \\left(\\nabla^{2} c_{k}(\\mathbf{x}, t)\\right) \\Psi_{n}(\\xi)\\Psi_{k}(\\xi)\n$$\nThe next step is to perform a Galerkin projection. This involves taking the inner product of the entire equation with a basis function $\\Psi_{j}(\\xi)$ for each $j \\in \\{0, 1, \\dots, P\\}$. This process makes the residual of the approximation orthogonal to the approximation space.\n$$\n\\left\\langle \\sum_{k=0}^{P} \\left( \\partial_{t} c_{k}(\\mathbf{x}, t) \\right) \\Psi_{k}(\\xi), \\Psi_{j}(\\xi) \\right\\rangle \\;=\\; \\left\\langle \\sum_{n=0}^{Q} \\sum_{k=0}^{P} d_{n} \\left( \\nabla^{2} c_{k}(\\mathbf{x}, t) \\right) \\Psi_{n}(\\xi)\\Psi_{k}(\\xi), \\Psi_{j}(\\xi) \\right\\rangle\n$$\nDue to the linearity of the inner product, we can move the summations and the deterministic coefficients outside the brackets.\n\nFor the left-hand side (LHS):\n$$\n\\text{LHS} \\;=\\; \\sum_{k=0}^{P} \\left( \\partial_{t} c_{k}(\\mathbf{x}, t) \\right) \\langle \\Psi_{k}(\\xi), \\Psi_{j}(\\xi) \\rangle\n$$\nUsing the orthonormality property $\\langle \\Psi_{k}, \\Psi_{j} \\rangle = \\delta_{kj}$, the summation over $k$ collapses, leaving only the term where $k=j$:\n$$\n\\text{LHS} \\;=\\; \\partial_{t} c_{j}(\\mathbf{x}, t)\n$$\nFor the right-hand side (RHS):\n$$\n\\text{RHS} \\;=\\; \\sum_{n=0}^{Q} \\sum_{k=0}^{P} d_{n} \\left( \\nabla^{2} c_{k}(\\mathbf{x}, t) \\right) \\langle \\Psi_{n}(\\xi)\\Psi_{k}(\\xi), \\Psi_{j}(\\xi) \\rangle\n$$\nThe inner product term $\\langle \\Psi_{j}(\\xi)\\Psi_{n}(\\xi)\\Psi_{k}(\\xi), 1 \\rangle$ can be written using the notation provided in the problem statement as $\\langle \\Psi_{j}\\Psi_{n}\\Psi_{k} \\rangle$. These terms are often called \"triple product\" or \"interaction\" coefficients. The RHS becomes:\n$$\n\\text{RHS} \\;=\\; \\sum_{k=0}^{P} \\left( \\sum_{n=0}^{Q} d_{n} \\langle \\Psi_{j}\\Psi_{n}\\Psi_{k} \\rangle \\right) \\nabla^{2} c_{k}(\\mathbf{x}, t)\n$$\nEquating the LHS and RHS gives the system of coupled deterministic PDEs for the coefficients $c_{j}(\\mathbf{x}, t)$, where $j = 0, 1, \\dots, P$:\n$$\n\\partial_{t} c_{j}(\\mathbf{x}, t) \\;=\\; \\sum_{k=0}^{P} \\left( \\sum_{n=0}^{Q} d_{n} \\langle \\Psi_{j}\\Psi_{n}\\Psi_{k} \\rangle \\right) \\nabla^{2} c_{k}(\\mathbf{x}, t)\n$$\nThis system can be written in matrix-vector form. Let $\\mathbf{c}(\\mathbf{x}, t)$ be the column vector of coefficients $[c_0(\\mathbf{x}, t), c_1(\\mathbf{x}, t), \\dots, c_P(\\mathbf{x}, t)]^T$. The system is:\n$$\n\\partial_{t} \\mathbf{c}(\\mathbf{x}, t) \\;=\\; \\mathbf{M} \\nabla^{2} \\mathbf{c}(\\mathbf{x}, t)\n$$\nHere, $\\nabla^{2}\\mathbf{c}(\\mathbf{x},t)$ is understood as applying the Laplacian operator element-wise to the vector $\\mathbf{c}(\\mathbf{x},t)$. The matrix $\\mathbf{M}$ is the coupling matrix, a $(P+1) \\times (P+1)$ matrix whose entries are constants. The problem asks for the entries of this matrix. By comparing the general form of the $j$-th row of the matrix equation, $\\partial_{t} c_{j} = \\sum_{k=0}^{P} M_{jk} \\nabla^{2} c_{k}$, with our derived equation, we can identify the entry $M_{jk}$ (the element in the $j$-th row and $k$-th column) of the matrix $\\mathbf{M}$.\n\nThe entry $M_{jk}$ is the coefficient that multiplies $\\nabla^{2} c_{k}(\\mathbf{x}, t)$ in the equation for $\\partial_{t} c_{j}(\\mathbf{x}, t)$. From our derivation, this coefficient is:\n$$\nM_{jk} = \\sum_{n=0}^{Q} d_{n} \\langle \\Psi_{j}\\Psi_{n}\\Psi_{k} \\rangle\n$$\nThis is the final expression for the entries of the coupling matrix. The indices $j$ and $k$ range from $0$ to $P$.",
            "answer": "$$\n\\boxed{\\sum_{n=0}^{Q} d_{n} \\langle \\Psi_{j}\\Psi_{n}\\Psi_{k} \\rangle}\n$$"
        }
    ]
}