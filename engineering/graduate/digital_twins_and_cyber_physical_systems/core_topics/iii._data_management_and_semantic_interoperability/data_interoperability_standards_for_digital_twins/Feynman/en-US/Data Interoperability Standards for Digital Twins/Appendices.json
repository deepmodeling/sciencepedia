{
    "hands_on_practices": [
        {
            "introduction": "This first practice focuses on the foundational task of ensuring data consistency at the source. You will design a validation system for telemetry data using JSON Schema concepts, enforcing not just the structure of the data but also its semantic correctness, including physical units and valid value ranges. This exercise highlights the critical role of schema-driven validation in building robust and interoperable data pipelines .",
            "id": "4212061",
            "problem": "You are designing validation for telemetry interoperability in a digital twin. Use JavaScript Object Notation Schema (JSON Schema) concepts to enforce required structure and constraints that guarantee consistent semantics across producers and consumers. You must formulate the constraints as a schema design and implement an equivalent programmatic validator that enforces the same rules. The schema must capture three invariants: unit annotations, value ranges in their physical meaning, and timestamp format. The goal is to derive a correct and efficient validator from fundamental definitions of typing and set membership, not by relying on a pre-existing validator library.\n\nFundamental base and setting: Treat a telemetry document as a finite mapping from keys to values. A key either maps to a scalar string or to a finite map containing scalar keys. Enforce membership constraints by sets specified as regular languages and value intervals. Use the following well-tested facts as the foundation: (i) timestamps must be valid Coordinated Universal Time (UTC) instants in the International Organization for Standardization (ISO) $8601$ basic extended syntax with suffix $Z$; (ii) unit conversions are linear mappings, for example temperature in Kelvin $K$ and degree Celsius ${}^\\circ\\text{C}$ satisfy $T_{K} = T_{{}^\\circ\\text{C}} + 273.15$, speed satisfies $v_{\\mathrm{m/s}} = v_{\\mathrm{km/h}} \\cdot \\frac{1000}{3600}$, and pressure satisfies $p_{\\mathrm{Pa}} = 10^{5}\\,p_{\\mathrm{bar}} = 10^{3}\\,p_{\\mathrm{kPa}}$; (iii) range validation compares values in a single canonical unit using closed intervals $[a,b]$.\n\nRequired schema constraints to encode and enforce:\n- Top-level is an object with required keys \"twinId\", \"ts\", and \"measurements\". No additional top-level keys are allowed.\n- \"twinId\" is a string over the alphabet $\\{A\\text{-}Z,a\\text{-}z,0\\text{-}9,.,\\_,\\text{-}\\}$ with length in $[1,64]$.\n- \"ts\" is a string that represents a valid UTC instant exactly in the lexical form \"YYYY-MM-DDThh:mm:ssZ\", where \"Z\" is literal and the date-time fields form a valid calendar instant. Angle units are not involved; no fractional seconds are allowed.\n- \"measurements\" is an object with exactly the keys \"temperature\", \"pressure\", and \"speed\". No additional keys are allowed.\n- Each measurement is an object with exactly the keys \"value\" and \"unit\". No additional keys are allowed.\n- Units and ranges are constrained as follows, with validation performed by converting to canonical base units and then checking inclusive bounds:\n  - Temperature: unit in {\"K\",\"degC\"}. Canonical unit is $K$. Allowed canonical range is $[0, 4000]$ $K$. Conversion is $T_{K} = T_{{}^\\circ\\text{C}} + 273.15$ or identity for $K$.\n  - Pressure: unit in {\"Pa\",\"kPa\",\"bar\"}. Canonical unit is $\\mathrm{Pa}$. Allowed canonical range is $[0, 10^{8}]$ $\\mathrm{Pa}$. Conversions: $p_{\\mathrm{Pa}} = p_{\\mathrm{Pa}}$; $p_{\\mathrm{Pa}} = 10^{3} \\cdot p_{\\mathrm{kPa}}$; $p_{\\mathrm{Pa}} = 10^{5} \\cdot p_{\\mathrm{bar}}$.\n  - Speed: unit in {\"m/s\",\"km/h\"}. Canonical unit is $\\mathrm{m/s}$. Allowed canonical range is $[0, 3000]$ $\\mathrm{m/s}$. Conversion: $v_{\\mathrm{m/s}} = v_{\\mathrm{km/h}} \\cdot \\frac{1000}{3600}$ or identity for $\\mathrm{m/s}$.\n- All numeric \"value\" fields must be finite real numbers. The inclusive bounds mean that boundary values exactly equal to the endpoints pass validation.\n\nDesign task:\n1. Construct a JSON Schema that encodes the structural and lexical constraints above in a way that a generic JSON Schema validator could check, including:\n   - Regular-expression-based constraints where applicable (for \"twinId\"),\n   - Enumeration constraints for units,\n   - Disallowing additional properties,\n   - A timestamp pattern consistent with \"YYYY-MM-DDThh:mm:ssZ\".\n   You are not required to encode cross-field unit conversion arithmetic in the JSON Schema; instead, you must implement the unit conversion and range checks programmatically in step $2$. Your JSON Schema should be minimal and deterministic to reduce backtracking and enable linear-time checks per field.\n2. Implement a program that:\n   - Defines the JSON Schema as a data structure,\n   - Validates a set of telemetry payloads by enforcing all of the constraints above, including the unit conversion and canonical range checks,\n   - Produces the final results.\n\nTest suite:\nYou must validate the following six payloads. Each payload is a JSON-like object literal. All physical values are to be interpreted and validated in their stated units. There is no angle involved and no percentage quantities.\n\n- Case A (happy path, interior values):\n  {\n    \"twinId\": \"plantA.line-1.pump_42\",\n    \"ts\": \"2024-01-01T00:00:00Z\",\n    \"measurements\": {\n      \"temperature\": {\"value\": 300, \"unit\": \"K\"},\n      \"pressure\": {\"value\": 101325, \"unit\": \"Pa\"},\n      \"speed\": {\"value\": 10, \"unit\": \"m/s\"}\n    }\n  }\n- Case B (invalid unit spelling for temperature):\n  {\n    \"twinId\": \"plantA.line-1.pump_42\",\n    \"ts\": \"2024-01-01T00:00:00Z\",\n    \"measurements\": {\n      \"temperature\": {\"value\": 20, \"unit\": \"C\"},\n      \"pressure\": {\"value\": 101.325, \"unit\": \"kPa\"},\n      \"speed\": {\"value\": 36, \"unit\": \"km/h\"}\n    }\n  }\n- Case C (boundary values valid in all three channels):\n  {\n    \"twinId\": \"A\",\n    \"ts\": \"2025-02-28T23:59:59Z\",\n    \"measurements\": {\n      \"temperature\": {\"value\": -273.15, \"unit\": \"degC\"},\n      \"pressure\": {\"value\": 1000, \"unit\": \"bar\"},\n      \"speed\": {\"value\": 10800, \"unit\": \"km/h\"}\n    }\n  }\n- Case D (invalid timestamp because it lacks literal Z and uses an offset instead):\n  {\n    \"twinId\": \"plantA.line-1.pump_42\",\n    \"ts\": \"2024-01-01T00:00:00+00:00\",\n    \"measurements\": {\n      \"temperature\": {\"value\": 300, \"unit\": \"K\"},\n      \"pressure\": {\"value\": 101325, \"unit\": \"Pa\"},\n      \"speed\": {\"value\": 10, \"unit\": \"m/s\"}\n    }\n  }\n- Case E (disallowed additional property inside a measurement object):\n  {\n    \"twinId\": \"plantA.line-1.pump_42\",\n    \"ts\": \"2024-01-01T00:00:00Z\",\n    \"measurements\": {\n      \"temperature\": {\"value\": 4000, \"unit\": \"K\", \"note\": \"x\"},\n      \"pressure\": {\"value\": 500000, \"unit\": \"Pa\"},\n      \"speed\": {\"value\": 0, \"unit\": \"m/s\"}\n    }\n  }\n- Case F (physically invalid negative pressure):\n  {\n    \"twinId\": \"plantA.line-1.pump_42\",\n    \"ts\": \"2024-01-01T00:00:00Z\",\n    \"measurements\": {\n      \"temperature\": {\"value\": 290, \"unit\": \"K\"},\n      \"pressure\": {\"value\": -1, \"unit\": \"Pa\"},\n      \"speed\": {\"value\": 1, \"unit\": \"m/s\"}\n    }\n  }\n\nAnswer specification and output format:\n- For each case, compute a boolean indicating whether the payload is valid according to the constraints. The output is the aggregation of these booleans in the order [A, B, C, D, E, F].\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False,True,False]\"). There are no physical units to print in the output.",
            "solution": "We model validation as set membership over a product space of structural and semantic constraints. A document $d$ is valid if and only if it lies in the intersection of the language recognized by the structural schema $S$ and the semantic set $M$ induced by unit conversions and canonical ranges. Formally, let $D$ be the set of all finite maps over strings. Define $S \\subseteq D$ to enforce shape and lexical well-formedness, and define $M \\subseteq D$ to enforce numerical semantics. We require $d \\in S \\cap M$.\n\nPrinciples:\n1. Regular-language constraints: Deterministic regular expressions and fixed-format parsing provide $O(n)$ lexical validation for a string of length $n$. We use this for \"twinId\" and for timestamps.\n2. Linear unit conversions: All unit conversions given are affine or linear maps of the form $y = a x + b$ with constants $a$ and $b$. To compare values against canonical ranges $[L,U]$, we first compute the canonical value $c(x) = a x + b$ and test $L \\le c(x) \\le U$. For temperature, $a=1$ and $b=273.15$ when converting from ${}^\\circ\\text{C}$ to $K$, and $a=1$, $b=0$ from $K$ to $K$. For pressure, $a \\in \\{10^{5}, 10^{3}, 1\\}$ with $b=0$. For speed, $a=\\frac{1000}{3600}$ or $a=1$, $b=0$. These conversions are stable and do not require iterative methods.\n3. Determinism and efficiency: To minimize backtracking and branch explosion, we avoid ambiguous schema constructs such as nested \"anyOf\". We restrict patterns to anchored expressions and use precompiled deterministic finite automata. Structural checks are short-circuited as soon as a violation is observed, and allowed units are represented by hash sets yielding expected $O(1)$ average-time membership tests.\n\nSchema construction:\n- \"twinId\": The permitted alphabet is $\\Sigma = \\{A\\text{-}Z,a\\text{-}z,0\\text{-}9,.,\\_,\\text{-}\\}$ and the length must be in $[1,64]$. A deterministic regular expression is $^[A-Za-z0-9._-]{1,64}$$. The anchor ensures linear-time scanning without catastrophic backtracking.\n- \"ts\": We require the exact format \"YYYY-MM-DDThh:mm:ssZ\" with literal $Z$ indicating Coordinated Universal Time (UTC). We validate via a format-specific parser using the format string corresponding to the ISO $8601$ extended representation, ensuring that calendar constraints (months in $[1,12]$, hours in $[0,23]$, etc.) are enforced by parsing rather than by a complex regular expression. This reduces the risk of erroneously accepting invalid dates while remaining $O(1)$ in the number of fields and $O(n)$ in string length $n$.\n- \"measurements\": The object keys must be exactly the set $\\{\\text{temperature},\\text{pressure},\\text{speed}\\}$, with no extras. Each measurement object must have exactly the keys $\\{\\text{value},\\text{unit}\\}$. We ensure $|\\text{keys}|=2$ and membership in the allowed set.\n- Units: We encode allowed units via enumerations per measurement. This establishes a finite choice set and reduces validation to membership queries.\n- Ranges: We define canonical ranges as closed intervals: temperature $T_{K} \\in [0,4000]$, pressure $p_{\\mathrm{Pa}} \\in [0,10^{8}]$, speed $v_{\\mathrm{m/s}} \\in [0,3000]$. For any input value $x$ with unit $u$, we compute $c(x,u)$ and test $L \\le c(x,u) \\le U$.\n\nAlgorithm:\n- Structural validation: Check the top-level type is a map, required keys present, no additional keys, and that \"twinId\" matches the deterministic pattern and length constraints.\n- Timestamp validation: Parse \"ts\" via a fixed format parser for \"YYYY-MM-DDThh:mm:ssZ\". This simultaneously ensures lexical structure and calendar validity in $O(1)$ field checks.\n- Measurement validation: For each required key, ensure no additional properties. Enforce \"unit\" membership in the allowed set. Verify \"value\" is a finite real number. Convert to canonical using $c(x,u)$ and test against the closed interval $[L,U]$.\n\nCorrectness argument:\n- If the structural checks hold, then the document is in $S$ by construction because we have verified membership in the regular languages for each lexical component and enforced the exact key sets.\n- If the semantic checks hold, then by the linearity of $c(x,u)$ and the monotonicity of affine maps with positive $a$, $x$ lies within the preimage of $[L,U]$ for its declared unit $u$, guaranteeing the physical quantity lies in the intended canonical range. The use of closed intervals ensures that boundary points such as $T_{{}^\\circ\\text{C}} = -273.15$ map to $T_{K} = 0$ and are included, as $0 \\in [0,4000]$.\n- The conjunction $S \\cap M$ is enforced by short-circuit evaluation, so the algorithm returns true if and only if all constraints hold.\n\nEfficiency justification:\n- Each check uses constant-time operations per field: regular expression match for \"twinId\" is linear in the identifier length and compiled once; timestamp parsing is linear in string length and bounded; unit membership is a hash lookup with expected $O(1)$ time; conversions and comparisons are $O(1)$. The total time per document is $O(n)$ where $n$ is the combined length of string fields plus a constant for numeric conversions, and memory overhead is $O(1)$ beyond the input because we do not allocate proportional to input size.\n- By avoiding disjunctive schema constructs with overlapping alternatives and by anchoring regexes, we remove backtracking paths, making validation deterministic and predictable.\n\nApplication to the test suite:\n- Case A: $T_{K}=300$ lies in $[0,4000]$; $p_{\\mathrm{Pa}}=101325$ lies in $[0,10^{8}]$; $v_{\\mathrm{m/s}}=10$ lies in $[0,3000]$; timestamp parses with $Z$; structure is exact. Valid.\n- Case B: Temperature unit \"C\" is not in $\\{\\text{K},\\text{degC}\\}$; invalid regardless of other fields. Invalid.\n- Case C: $T_{{}^\\circ\\text{C}}=-273.15 \\Rightarrow T_{K}=0 \\in [0,4000]$; $p_{\\mathrm{bar}}=1000 \\Rightarrow p_{\\mathrm{Pa}}=10^{5}\\cdot 1000=10^{8} \\in [0,10^{8}]$; $v_{\\mathrm{km/h}}=10800 \\Rightarrow v_{\\mathrm{m/s}} = 10800\\cdot \\frac{1000}{3600}=3000 \\in [0,3000]$; timestamp is valid with literal $Z$. Valid.\n- Case D: Timestamp has \"+00:00\" and lacks literal $Z$, so it fails the exact-format requirement even though it is an ISO $8601$ instant. Invalid.\n- Case E: \"temperature\" contains an extra key \"note\", violating the exact-keys constraint. Invalid.\n- Case F: $p_{\\mathrm{Pa}}=-1$ is below $0$; invalid.\n\nThe program defines the schema as a Python structure and implements the described validation algorithm. It applies the algorithm to the six cases and prints a single line list of booleans in the required order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport re\nfrom datetime import datetime\nimport math\n\n# Precompile regex for twinId: ^[A-Za-z0-9._-]{1,64}$\nTWIN_ID_REGEX = re.compile(r'^[A-Za-z0-9._-]{1,64}$')\n\ndef is_finite_number(x):\n    return isinstance(x, (int, float)) and math.isfinite(x)\n\ndef validate_timestamp_utc_z(ts: str) -> bool:\n    # Exact format: YYYY-MM-DDThh:mm:ssZ (literal 'Z', no fractional seconds)\n    if not isinstance(ts, str):\n        return False\n    try:\n        # This enforces valid calendar fields and exact 'Z' suffix\n        dt = datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%SZ\")\n    except Exception:\n        return False\n    # strptime above already enforces 'Z' literal placement and ranges\n    # No need to check timezone further as we require literal 'Z'\n    return True\n\n# Define a JSON-Schema-like structure (for documentation and limited structural checks)\nSCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"twinId\", \"ts\", \"measurements\"],\n    \"additionalProperties\": False,\n    \"properties\": {\n        \"twinId\": {\n            \"type\": \"string\",\n            \"pattern\": r\"^[A-Za-z0-9._-]{1,64}$\"\n        },\n        \"ts\": {\n            \"type\": \"string\",\n            \"pattern\": r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$\"\n        },\n        \"measurements\": {\n            \"type\": \"object\",\n            \"required\": [\"temperature\", \"pressure\", \"speed\"],\n            \"additionalProperties\": False,\n            \"properties\": {\n                \"temperature\": {\n                    \"type\": \"object\",\n                    \"required\": [\"value\", \"unit\"],\n                    \"additionalProperties\": False,\n                    \"properties\": {\n                        \"value\": {\"type\": \"number\"},\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"K\", \"degC\"]},\n                    },\n                },\n                \"pressure\": {\n                    \"type\": \"object\",\n                    \"required\": [\"value\", \"unit\"],\n                    \"additionalProperties\": False,\n                    \"properties\": {\n                        \"value\": {\"type\": \"number\"},\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"Pa\", \"kPa\", \"bar\"]},\n                    },\n                },\n                \"speed\": {\n                    \"type\": \"object\",\n                    \"required\": [\"value\", \"unit\"],\n                    \"additionalProperties\": False,\n                    \"properties\": {\n                        \"value\": {\"type\": \"number\"},\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"m/s\", \"km/h\"]},\n                    },\n                },\n            },\n        },\n    },\n}\n\n# Canonical ranges in base units\nCANONICAL_RANGES = {\n    \"temperature\": (\"K\", 0.0, 4000.0),\n    \"pressure\": (\"Pa\", 0.0, 1e8),\n    \"speed\": (\"m/s\", 0.0, 3000.0),\n}\n\n# Unit conversion functions to canonical base units\ndef to_canonical_temperature(value: float, unit: str) -> float:\n    if unit == \"K\":\n        return value\n    elif unit == \"degC\":\n        return value + 273.15\n    else:\n        raise ValueError(\"Unsupported temperature unit\")\n\ndef to_canonical_pressure(value: float, unit: str) -> float:\n    if unit == \"Pa\":\n        return value\n    elif unit == \"kPa\":\n        return value * 1_000.0\n    elif unit == \"bar\":\n        return value * 100_000.0\n    else:\n        raise ValueError(\"Unsupported pressure unit\")\n\ndef to_canonical_speed(value: float, unit: str) -> float:\n    if unit == \"m/s\":\n        return value\n    elif unit == \"km/h\":\n        return value * (1000.0 / 3600.0)\n    else:\n        raise ValueError(\"Unsupported speed unit\")\n\nUNIT_CONVERTERS = {\n    \"temperature\": to_canonical_temperature,\n    \"pressure\": to_canonical_pressure,\n    \"speed\": to_canonical_speed,\n}\n\nALLOWED_UNITS = {\n    \"temperature\": {\"K\", \"degC\"},\n    \"pressure\": {\"Pa\", \"kPa\", \"bar\"},\n    \"speed\": {\"m/s\", \"km/h\"},\n}\n\ndef validate_structure(payload: dict) -> bool:\n    # Top-level object\n    if not isinstance(payload, dict):\n        return False\n    # Required top-level keys\n    required_top = {\"twinId\", \"ts\", \"measurements\"}\n    if set(payload.keys()) != required_top:\n        return False\n    # twinId\n    twin_id = payload.get(\"twinId\")\n    if not isinstance(twin_id, str) or TWIN_ID_REGEX.fullmatch(twin_id) is None:\n        return False\n    # ts\n    if not validate_timestamp_utc_z(payload.get(\"ts\")):\n        return False\n    # measurements\n    measurements = payload.get(\"measurements\")\n    if not isinstance(measurements, dict):\n        return False\n    if set(measurements.keys()) != {\"temperature\", \"pressure\", \"speed\"}:\n        return False\n    # per measurement object: exact keys \"value\" and \"unit\"\n    for key, m in measurements.items():\n        if not isinstance(m, dict):\n            return False\n        if set(m.keys()) != {\"value\", \"unit\"}:\n            return False\n        if not isinstance(m.get(\"unit\"), str):\n            return False\n        if m.get(\"unit\") not in ALLOWED_UNITS[key]:\n            return False\n        if not is_finite_number(m.get(\"value\")):\n            return False\n    return True\n\ndef validate_semantics(payload: dict) -> bool:\n    # Assumes structure is valid\n    measurements = payload[\"measurements\"]\n    for name in [\"temperature\", \"pressure\", \"speed\"]:\n        unit = measurements[name][\"unit\"]\n        value = float(measurements[name][\"value\"])\n        converter = UNIT_CONVERTERS[name]\n        canonical_value = converter(value, unit)\n        base_unit, lo, hi = CANONICAL_RANGES[name]\n        # Inclusive range\n        if not (lo = canonical_value = hi):\n            return False\n    return True\n\ndef validate_payload(payload: dict) -> bool:\n    # Structural validation\n    if not validate_structure(payload):\n        return False\n    # Semantic validation\n    if not validate_semantics(payload):\n        return False\n    return True\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"twinId\": \"plantA.line-1.pump_42\",\n            \"ts\": \"2024-01-01T00:00:00Z\",\n            \"measurements\": {\n                \"temperature\": {\"value\": 300, \"unit\": \"K\"},\n                \"pressure\": {\"value\": 101325, \"unit\": \"Pa\"},\n                \"speed\": {\"value\": 10, \"unit\": \"m/s\"},\n            },\n        },\n        # Case B\n        {\n            \"twinId\": \"plantA.line-1.pump_42\",\n            \"ts\": \"2024-01-01T00:00:00Z\",\n            \"measurements\": {\n                \"temperature\": {\"value\": 20, \"unit\": \"C\"},\n                \"pressure\": {\"value\": 101.325, \"unit\": \"kPa\"},\n                \"speed\": {\"value\": 36, \"unit\": \"km/h\"},\n            },\n        },\n        # Case C\n        {\n            \"twinId\": \"A\",\n            \"ts\": \"2025-02-28T23:59:59Z\",\n            \"measurements\": {\n                \"temperature\": {\"value\": -273.15, \"unit\": \"degC\"},\n                \"pressure\": {\"value\": 1000, \"unit\": \"bar\"},\n                \"speed\": {\"value\": 10800, \"unit\": \"km/h\"},\n            },\n        },\n        # Case D\n        {\n            \"twinId\": \"plantA.line-1.pump_42\",\n            \"ts\": \"2024-01-01T00:00:00+00:00\",\n            \"measurements\": {\n                \"temperature\": {\"value\": 300, \"unit\": \"K\"},\n                \"pressure\": {\"value\": 101325, \"unit\": \"Pa\"},\n                \"speed\": {\"value\": 10, \"unit\": \"m/s\"},\n            },\n        },\n        # Case E\n        {\n            \"twinId\": \"plantA.line-1.pump_42\",\n            \"ts\": \"2024-01-01T00:00:00Z\",\n            \"measurements\": {\n                \"temperature\": {\"value\": 4000, \"unit\": \"K\", \"note\": \"x\"},\n                \"pressure\": {\"value\": 500000, \"unit\": \"Pa\"},\n                \"speed\": {\"value\": 0, \"unit\": \"m/s\"},\n            },\n        },\n        # Case F\n        {\n            \"twinId\": \"plantA.line-1.pump_42\",\n            \"ts\": \"2024-01-01T00:00:00Z\",\n            \"measurements\": {\n                \"temperature\": {\"value\": 290, \"unit\": \"K\"},\n                \"pressure\": {\"value\": -1, \"unit\": \"Pa\"},\n                \"speed\": {\"value\": 1, \"unit\": \"m/s\"},\n            },\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = validate_payload(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Interoperability is not just about correct formats, but also about understanding data quality. This exercise delves into the propagation of error, a crucial aspect when data from different sources are integrated. You will calculate the maximum absolute error introduced when a sensor reading with a specific quantization resolution is converted from one unit system to another, providing a quantitative basis for assessing data uncertainty .",
            "id": "4212018",
            "problem": "A water distribution digital twin ingests flow rate observations from a legacy sensor via Open Platform Communications Unified Architecture (OPC UA) and Open Geospatial Consortium (OGC) SensorThings standardized endpoints. The sensor publishes volumetric flow in $\\,\\mathrm{L/min}\\,$ with a quantization resolution of $\\,0.1\\ \\mathrm{L/min}\\,$; each reported value is formed by rounding the true flow to the nearest $\\,0.1\\ \\mathrm{L/min}\\,$ increment. The twinâ€™s state estimator expects inputs in $\\,\\mathrm{m^3/s}\\,$, and the data integration pipeline performs only unit conversion without any de-quantization or smoothing.\n\nStarting from fundamental unit definitions $1\\ \\mathrm{L} = 10^{-3}\\ \\mathrm{m^3}$ and $1\\ \\mathrm{min} = 60\\ \\mathrm{s}$, and from the basic model of rounding to the nearest quantization level, derive the maximum absolute error bound introduced solely by the rounding when the reported value is converted from $\\,\\mathrm{L/min}\\,$ to $\\,\\mathrm{m^3/s}\\,$. Express the final numeric bound as an exact closed-form expression in $\\,\\mathrm{m^3/s}\\,$. Do not approximate or round the final expression.\n\nThen, briefly explain in words (no numerical output required) how this bound informs the design of interoperable data exchange (for example, within OPC UA and OGC SensorThings) in a multi-source digital twin where unit semantics and resolution metadata must be preserved across conversions.\n\nYour final answer must be the single exact value of the maximum absolute error bound in $\\,\\mathrm{m^3/s}\\,$. No units should be included inside the final boxed answer.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique and meaningful solution. The problem addresses a practical and fundamental issue in engineering and data science, namely the propagation of quantization error through unit conversions, which is a critical consideration in designing interoperable systems like digital twins.\n\nLet $Q_{true}$ be the true volumetric flow rate in units of liters per minute ($\\mathrm{L/min}$) and $Q_{reported}$ be the value reported by the sensor in the same units. The problem states that the sensor has a quantization resolution, which we denote as $\\Delta_Q$, of $0.1\\ \\mathrm{L/min}$. The reported value, $Q_{reported}$, is obtained by rounding the true value, $Q_{true}$, to the nearest multiple of this resolution.\n\nThe quantization error, $\\epsilon_Q$, is the difference between the true and reported values:\n$$ \\epsilon_Q = Q_{true} - Q_{reported} $$\nFor a rounding-to-the-nearest-value scheme, the magnitude of the error is at most half the quantization step size. Therefore, the maximum absolute error in the original units is given by:\n$$ \\max(|\\epsilon_Q|) = \\frac{\\Delta_Q}{2} $$\nSubstituting the given value for the resolution:\n$$ \\max(|\\epsilon_Q|) = \\frac{0.1\\ \\mathrm{L/min}}{2} = 0.05\\ \\mathrm{L/min} $$\nThis value represents the maximum uncertainty of the flow rate measurement in its original units.\n\nThe digital twin's state estimator requires the input to be in units of cubic meters per second ($\\mathrm{m^3/s}$). The data integration pipeline performs a direct unit conversion. We must determine the conversion factor, $C$, from $\\mathrm{L/min}$ to $\\mathrm{m^3/s}$. Using the provided fundamental definitions:\n$$ 1\\ \\mathrm{L} = 10^{-3}\\ \\mathrm{m^3} $$\n$$ 1\\ \\mathrm{min} = 60\\ \\mathrm{s} $$\nFrom these, we can construct the conversion factor:\n$$ C = \\frac{1\\ \\mathrm{L}}{1\\ \\mathrm{min}} = \\frac{10^{-3}\\ \\mathrm{m^3}}{60\\ \\mathrm{s}} = \\frac{10^{-3}}{60}\\ \\frac{\\mathrm{m^3}}{\\mathrm{s}} = \\frac{1}{60000}\\ \\frac{\\mathrm{m^3}}{\\mathrm{s}} $$\nLet $S_{true}$ and $S_{reported}$ be the true and reported flow rates in the target units of $\\mathrm{m^3/s}$, respectively. The conversion is a linear transformation:\n$$ S_{true} = Q_{true} \\cdot C $$\n$$ S_{reported} = Q_{reported} \\cdot C $$\nThe absolute error in the new unit system, $\\epsilon_S$, is the difference between these two values:\n$$ \\epsilon_S = S_{true} - S_{reported} = (Q_{true} \\cdot C) - (Q_{reported} \\cdot C) = (Q_{true} - Q_{reported}) \\cdot C = \\epsilon_Q \\cdot C $$\nThe maximum absolute error bound in the target units is therefore the maximum absolute error bound in the original units multiplied by the conversion factor:\n$$ \\max(|\\epsilon_S|) = \\max(|\\epsilon_Q|) \\cdot C $$\nSubstituting the known values:\n$$ \\max(|\\epsilon_S|) = (0.05\\ \\mathrm{L/min}) \\cdot \\left(\\frac{1}{60000}\\ \\frac{\\mathrm{m^3/s}}{\\mathrm{L/min}}\\right) $$\nTo express this as an exact closed-form expression, we convert the decimal to a fraction:\n$$ 0.05 = \\frac{5}{100} = \\frac{1}{20} $$\nNow, we compute the final value for the error bound:\n$$ \\max(|\\epsilon_S|) = \\frac{1}{20} \\cdot \\frac{1}{60000}\\ \\mathrm{m^3/s} = \\frac{1}{1200000}\\ \\mathrm{m^3/s} $$\nThis is the maximum absolute error introduced by the sensor's quantization after the value is converted to the units required by the digital twin.\n\nRegarding the design of interoperable data exchange, this calculated error bound is a quantitative measure of data quality, specifically the uncertainty introduced by the sensor's finite resolution. For true interoperability in a multi-source digital twin, it is insufficient to simply exchange data values and their primary units. Data quality metadata, such as sensor resolution, accuracy, and the resulting uncertainty bounds, must also be communicated. Standards like OPC UA and OGC SensorThings provide mechanisms to include such metadata within their information models (e.g., as properties or attributes of a variable or observation). By propagating resolution and uncertainty information, a downstream system like a state estimator (e.g., a Kalman filter) can properly weight inputs from various sensors. A sensor with a larger error bound (lower precision) would be given less weight in data fusion algorithms, preventing low-quality data from unduly corrupting the overall state estimate. Therefore, designing interoperable data exchange requires defining a common semantic model not just for the physical quantity being measured, but also for its associated uncertainty, ensuring that the confidence in a measurement is preserved and correctly interpreted across system boundaries and unit conversions.",
            "answer": "$$\\boxed{\\frac{1}{1200000}}$$"
        },
        {
            "introduction": "In large-scale digital twin ecosystems, multiple systems may refer to the same real-world asset using different identifiers. This advanced practice challenges you to design an algorithm that reconciles these local identifiers with global URIs, using a combination of semantic rules and evidence-based matching. Successfully completing this exercise demonstrates an understanding of how to establish a coherent and unified view of assets in a complex, federated environment .",
            "id": "4212038",
            "problem": "A federated data space contains many digital twin instances, each maintained by a local authority with its own identifier scheme. Interoperability requires reconciliation between locally scoped identifiers and globally accessible Uniform Resource Identifiers (URI). The design must adhere to open standards semantics such as the Resource Description Framework (RDF) and the Web Ontology Language (OWL), in particular the equivalence relation semantics of owl:sameAs and the uniqueness semantics of inverse-functional properties that encode globally unique keys. Build a principled algorithm and implement it as a complete, runnable program that, given partial and potentially conflicting information, computes a reconciliation function and evaluates its correctness against a ground truth.\n\nFoundational base and formalization:\n- Let $L$ be a finite set of local identifiers and let $U$ be a finite set of external Uniform Resource Identifiers (URI).\n- Let $R \\subseteq U \\times U$ be an equivalence relation induced by OWL sameAs semantics, satisfying reflexivity, symmetry, and transitivity. The equivalence classes form a partition $P = \\{C_1, C_2, \\dots, C_m\\}$ of $U$.\n- Let $w: L \\times U \\to [0,1]$ be a weight function encoding evidence of correspondence between a local identifier and an external URI, with higher values indicating stronger evidence.\n- Let $K$ denote a set of key attributes that are declared inverse-functional in OWL. Let $k_L: L \\to V \\cup \\{\\bot\\}$ and $k_U: U \\to V \\cup \\{\\bot\\}$ give, respectively, the key value for a local identifier or a URI, with $\\bot$ denoting missing information. Assume that if $k_U(u) = v \\in V$ and $u \\in C$, then the class $C$ is keyed by $v$ and that $v$ appears for at most one class, consistent with inverse-functional semantics.\n- Define the cluster-level weight $W: L \\times P \\to [0,1]$ by $W(\\ell, C) = \\max\\{w(\\ell, u) \\mid u \\in C\\}$.\n- A reconciliation mapping is a partial injective function $\\phi: L \\rightharpoonup P$ such that:\n  1. If $k_L(\\ell) = v \\in V$ and there exists exactly one class $C \\in P$ with key $v$, then either $\\phi(\\ell) = C$ or $\\phi(\\ell)$ is undefined in case of key conflict described below.\n  2. If multiple $\\ell \\in L$ share the same $v$ and there exists a unique keyed class $C$ with $v$, matching any of them to a different $C' \\neq C$ violates inverse-functional semantics. The algorithm must resolve such conflicts conservatively under partial information to preserve soundness.\n  3. The function $\\phi$ must be injective, that is, distinct $\\ell_1 \\neq \\ell_2$ cannot map to the same class $C$.\n- Introduce a confidence threshold $\\tau \\in [0,1]$ and only accept matches $\\phi(\\ell) = C$ if $W(\\ell, C) \\ge \\tau$.\n\nAlgorithmic task:\n- Given $L$, $U$, the equivalence classes $P$ as sets of URIs, the evidence weights $w(\\ell,u)$, the key functions $k_L$, $k_U$, and the threshold $\\tau$, construct a mapping $\\phi$ with the following two-phase design:\n  1. Hard key phase: Use inverse-functional keys to assign any $\\ell$ with a unique keyed class $C$ to $C$ provided there is no key conflict. If a key conflict arises (two or more $\\ell$ share the same key value $v$ that identifies one class $C$), leave all conflicting $\\ell$ unmatched to preserve soundness and do not consider them in the soft phase.\n  2. Soft matching phase: On the remaining unmatched $\\ell$ and unused classes, compute an injective assignment that maximizes $\\sum_{\\ell \\in \\mathrm{dom}(\\phi)} W(\\ell, \\phi(\\ell))$ and then filter by the threshold $\\tau$ to retain only assignments with $W(\\ell, \\phi(\\ell)) \\ge \\tau$. Formally, this is a maximum-weight bipartite matching between the remaining locals and classes.\n- For evaluation, define the ground truth mapping $\\phi^\\star: L \\rightharpoonup P$ as input and compute:\n  1. The number of matches $n = |\\mathrm{dom}(\\phi)|$.\n  2. The soundness $s = \\frac{\\mathrm{tp}}{\\mathrm{pred}}$, where $\\mathrm{tp}$ is the number of $\\ell$ such that $\\phi(\\ell) = \\phi^\\star(\\ell)$ and $\\mathrm{pred} = |\\mathrm{dom}(\\phi)|$. If $\\mathrm{pred} = 0$, define $s = 1$.\n  3. The completeness $c = \\frac{\\mathrm{tp}}{\\mathrm{truth}}$, where $\\mathrm{truth} = |\\mathrm{dom}(\\phi^\\star)|$. If $\\mathrm{truth} = 0$, define $c = 1$.\n  4. A boolean consistency flag indicating whether $\\phi$ is injective.\n- Canonical representative of a class $C$: For output and ground truth comparison, represent $C$ by the lexicographically smallest URI in $C$.\n\nInput test suite:\n- Test case $1$ (happy path with a mix of hard keys and soft weights):\n  - Locals $L = \\{\\text{\"L1\"}, \\text{\"L2\"}, \\text{\"L3\"}\\}$.\n  - URIs $U = \\{\\text{\"U1\"}, \\text{\"U1a\"}, \\text{\"U2\"}, \\text{\"U3\"}, \\text{\"U4\"}\\}$.\n  - Equivalence classes $P = \\{\\{\\text{\"U1\"}, \\text{\"U1a\"}\\}, \\{\\text{\"U2\"}\\}, \\{\\text{\"U3\"}\\}, \\{\\text{\"U4\"}\\}\\}$ with canonical representatives $\\text{\"U1\"}$, $\\text{\"U2\"}$, $\\text{\"U3\"}$, $\\text{\"U4\"}$ respectively.\n  - Keys: $k_L(\\text{\"L1\"}) = \\text{\"GTIN-893-ABC\"}$, $k_L(\\text{\"L2\"}) = \\bot$, $k_L(\\text{\"L3\"}) = \\bot$. For URIs, $k_U(\\text{\"U1\"}) = \\text{\"GTIN-893-ABC\"}$, $k_U(\\text{\"U1a\"}) = \\text{\"GTIN-893-ABC\"}$, $k_U(\\text{\"U2\"}) = \\bot$, $k_U(\\text{\"U3\"}) = \\bot$, $k_U(\\text{\"U4\"}) = \\bot$.\n  - Weights $w$: $w(\\text{\"L1\"}, \\text{\"U1\"}) = 0.9$, $w(\\text{\"L1\"}, \\text{\"U2\"}) = 0.2$, $w(\\text{\"L2\"}, \\text{\"U2\"}) = 0.85$, $w(\\text{\"L3\"}, \\text{\"U4\"}) = 0.8$, $w(\\text{\"L3\"}, \\text{\"U3\"}) = 0.3$; all unspecified pairs have weight $0.0$.\n  - Threshold $\\tau = 0.5$.\n  - Ground truth $\\phi^\\star$: $\\phi^\\star(\\text{\"L1\"}) = \\{\\text{\"U1\"}, \\text{\"U1a\"}\\}$ with canonical representative $\\text{\"U1\"}$, $\\phi^\\star(\\text{\"L2\"}) = \\{\\text{\"U2\"}\\}$ with canonical $\\text{\"U2\"}$, $\\phi^\\star(\\text{\"L3\"}) = \\{\\text{\"U4\"}\\}$ with canonical $\\text{\"U4\"}$.\n- Test case $2$ (inverse-functional key conflict under partial information):\n  - Locals $L = \\{\\text{\"A\"}, \\text{\"B\"}\\}$.\n  - URIs $U = \\{\\text{\"U5\"}, \\text{\"U6\"}\\}$.\n  - Equivalence classes $P = \\{\\{\\text{\"U5\"}\\}, \\{\\text{\"U6\"}\\}\\}$ with canonical $\\text{\"U5\"}$, $\\text{\"U6\"}$.\n  - Keys: $k_L(\\text{\"A\"}) = \\text{\"EPC-XYZ\"}$, $k_L(\\text{\"B\"}) = \\text{\"EPC-XYZ\"}$. For URIs, $k_U(\\text{\"U5\"}) = \\text{\"EPC-XYZ\"}$, $k_U(\\text{\"U6\"}) = \\bot$.\n  - Weights $w$: $w(\\text{\"A\"}, \\text{\"U5\"}) = 0.9$, $w(\\text{\"B\"}, \\text{\"U6\"}) = 0.6$; all unspecified pairs $0.0$.\n  - Threshold $\\tau = 0.5$.\n  - Ground truth $\\phi^\\star$: $\\phi^\\star(\\text{\"A\"}) = \\{\\text{\"U5\"}\\}$ with canonical $\\text{\"U5\"}$, $\\phi^\\star(\\text{\"B\"})$ undefined.\n- Test case $3$ (soft matching only with overlapping evidence and class merging):\n  - Locals $L = \\{\\text{\"X\"}, \\text{\"Y\"}, \\text{\"Z\"}\\}$.\n  - URIs $U = \\{\\text{\"V1\"}, \\text{\"V2\"}, \\text{\"V2a\"}, \\text{\"V3\"}\\}$.\n  - Equivalence classes $P = \\{\\{\\text{\"V1\"}\\}, \\{\\text{\"V2\"}, \\text{\"V2a\"}\\}, \\{\\text{\"V3\"}\\}\\}$ with canonical $\\text{\"V1\"}$, $\\text{\"V2\"}$, $\\text{\"V3\"}$.\n  - Keys: all $\\bot$.\n  - Weights $w$: $w(\\text{\"X\"}, \\text{\"V1\"}) = 0.55$, $w(\\text{\"X\"}, \\text{\"V2\"}) = 0.50$, $w(\\text{\"X\"}, \\text{\"V3\"}) = 0.10$, $w(\\text{\"Y\"}, \\text{\"V2a\"}) = 0.60$, $w(\\text{\"Y\"}, \\text{\"V1\"}) = 0.40$, $w(\\text{\"Z\"}, \\text{\"V3\"}) = 0.49$, $w(\\text{\"Z\"}, \\text{\"V2\"}) = 0.51$; all unspecified pairs $0.0$.\n  - Threshold $\\tau = 0.5$.\n  - Ground truth $\\phi^\\star$: $\\phi^\\star(\\text{\"X\"}) = \\{\\text{\"V1\"}\\}$ with canonical $\\text{\"V1\"}$, $\\phi^\\star(\\text{\"Y\"}) = \\{\\text{\"V2\"}, \\text{\"V2a\"}\\}$ with canonical $\\text{\"V2\"}$, $\\phi^\\star(\\text{\"Z\"})$ undefined.\n\nYour program must implement the algorithm described, compute the mapping $\\phi$ for each test case, and then compute the metrics $n$, $s$, $c$, and the consistency flag as defined. Final output format: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is a list in the order $[n, s, c, \\mathrm{is\\_consistent}]$. For example, an output of the form $[[n_1, s_1, c_1, b_1],[n_2, s_2, c_2, b_2],[n_3, s_3, c_3, b_3]]$ must be printed exactly as a single line. No additional text must be printed.",
            "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n- **Sets**: A finite set of local identifiers $L$ and a finite set of external Uniform Resource Identifiers (URIs) $U$.\n- **Equivalence Relation**: $R \\subseteq U \\times U$ is an equivalence relation (reflexive, symmetric, transitive), induced by `owl:sameAs`. It partitions $U$ into equivalence classes $P = \\{C_1, C_2, \\dots, C_m\\}$.\n- **Evidence Weights**: A weight function $w: L \\times U \\to [0,1]$ represents evidence of correspondence.\n- **Key Attributes**: A set of inverse-functional key attributes $K$. Key functions $k_L: L \\to V \\cup \\{\\bot\\}$ and $k_U: U \\to V \\cup \\{\\bot\\}$ provide key values from a value space $V$ or a null value $\\bot$.\n- **Key Semantics**: If $k_U(u) = v \\in V$ for $u \\in C$, then class $C$ is keyed by $v$, and $v$ keys at most one class.\n- **Cluster-Level Weight**: $W: L \\times P \\to [0,1]$ is defined as $W(\\ell, C) = \\max\\{w(\\ell, u) \\mid u \\in C\\}$.\n- **Reconciliation Mapping**: A partial injective function $\\phi: L \\rightharpoonup P$.\n- **Confidence Threshold**: $\\tau \\in [0,1]$. Matches are accepted only if $W(\\ell, C) \\ge \\tau$.\n- **Algorithmic Task**:\n  1. **Hard Key Phase**: For an identifier $\\ell$ with key $k_L(\\ell) = v$, if $v$ uniquely identifies a class $C$ and $\\ell$ is the only local identifier with key $v$, assign $\\phi(\\ell) = C$. If multiple locals share key $v$, a conflict occurs: all conflicting locals are left unmatched and are excluded from the soft phase.\n  2. **Soft Matching Phase**: For remaining unmatched locals and unused classes, find an injective assignment that maximizes the total cluster-level weight $\\sum_{\\ell \\in \\mathrm{dom}(\\phi)} W(\\ell, \\phi(\\ell))$. This is a maximum-weight bipartite matching problem. After finding the optimal matching, filter out any assignments $(\\ell, C)$ where $W(\\ell, C)  \\tau$.\n- **Evaluation Metrics**: Given a ground truth mapping $\\phi^\\star$, compute:\n  - Number of matches: $n = |\\mathrm{dom}(\\phi)|$.\n  - Soundness: $s = \\frac{\\mathrm{tp}}{\\mathrm{pred}}$, where $\\mathrm{tp}$ is the number of correct matches $(\\phi(\\ell) = \\phi^\\star(\\ell))$ and $\\mathrm{pred} = n$. If $\\mathrm{pred} = 0$, $s=1$.\n  - Completeness: $c = \\frac{\\mathrm{tp}}{\\mathrm{truth}}$, where $\\mathrm{truth} = |\\mathrm{dom}(\\phi^\\star)|$. If $\\mathrm{truth} = 0$, $c=1$.\n  - Consistency: A boolean flag for the injectivity of $\\phi$.\n- **Class Representation**: An equivalence class $C$ is represented by its lexicographically smallest URI.\n- **Inputs**: Three complete test cases are provided with all necessary data ($L$, $U$, $P$, $k_L$, $k_U$, $w$, $\\tau$, $\\phi^\\star$).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in established computer science principles, particularly those from the Semantic Web (RDF, OWL, `owl:sameAs`, inverse-functional properties) and combinatorial optimization (maximum-weight bipartite matching). These concepts are standard for data integration and reconciliation tasks. The formulation is scientifically sound.\n- **Well-Posed**: The problem is structured to have a unique and meaningful solution. The two-phase algorithm is explicitly defined. Phase 1 provides deterministic rules for handling key-based matches and conflicts. Phase 2 formulates a maximum-weight bipartite matching problem, which is a standard, solvable optimization problem, admitting a unique optimal solution (up to tie-breaking, which does not affect the maximum weight). The subsequent filtering step is also well-defined. The evaluation metrics are precise.\n- **Objective**: The problem statement uses formal mathematical notation and unambiguous language, free of subjective or opinion-based claims.\n- **Completeness and Consistency**: The problem is self-contained. Each test case provides all necessary data to execute the algorithm and perform the evaluation. The rules are internally consistent. For example, the conservative handling of key conflicts prevents logical contradictions that could arise from violating inverse-functional property semantics.\n- **Realism and Feasibility**: The scenario of reconciling identifiers across federated data spaces using semantic web standards is a realistic and important problem in digital twin and cyber-physical systems engineering. The provided data are consistent and feasible.\n- **No Other Flaws**: The problem is not metaphorical, trivial, poorly structured, or unverifiable. It presents a clear, formal, and non-trivial algorithmic challenge.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is scientifically grounded, well-posed, objective, complete, consistent, and feasible. I will proceed to provide a full solution.\n\n### Algorithmic Solution Design\n\nThe solution is a direct implementation of the two-phase algorithm described in the problem statement. The core steps are as follows:\n\n**1. Pre-computation and Data Structuring:**\nFirst, we process the input data into more convenient structures.\n- **Canonical Representatives**: For each equivalence class $C \\in P$, which is a set of URIs, we determine its canonical representative by finding the lexicographically smallest URI within it. We create a mapping from each class to its canonical representative and a reverse mapping from the canonical representative to the class itself.\n- **URI-to-Class Mapping**: We create a mapping from each individual URI $u \\in U$ to the equivalence class $C$ it belongs to.\n- **Key-to-Class Mapping**: We process the URI key function $k_U$ to establish a mapping from a key value $v \\in V$ to the canonical representative of the class $C$ it identifies. The problem guarantees that a key value $v$ identifies at most one class.\n- **Cluster-Level Weights ($W$)**: We compute the matrix of cluster-level weights $W(\\ell, C) = \\max\\{w(\\ell, u) \\mid u \\in C\\}$ for all pairs of local identifiers $\\ell \\in L$ and classes $C \\in P$. This is done by iterating through all specified $w(\\ell, u)$ pairs and updating the maximum weight for the corresponding $(\\ell, C)$ pair, where $C$ is the class containing $u$.\n\n**2. Phase 1: Hard Key Matching**\nThis phase leverages the unique-key semantics of inverse-functional properties to make high-confidence matches.\n- **Group Locals by Key**: We first group local identifiers $\\ell \\in L$ by their key value $k_L(\\ell)$, ignoring those with a null key value ($\\bot$).\n- **Process Key Groups**: For each key value $v$ and its associated group of local identifiers:\n  - We check if $v$ corresponds to a keyed class in our pre-computed key-to-class map.\n  - **No Conflict**: If exactly one local identifier $\\ell$ has key $v$, and $v$ maps to a class $C$, we establish a hard match: $\\phi(\\ell) = C$. We then mark both $\\ell$ and $C$ as \"matched\" to exclude them from the next phase.\n  - **Key Conflict**: If two or more local identifiers share the same key $v$, a conflict arises. As per the problem's soundness requirement, all these conflicting locals are left unmatched. Both the conflicting local identifiers and the class $C$ they contend for are removed from consideration for the soft matching phase.\n\n**3. Phase 2: Soft Matching**\nThis phase resolves the remaining ambiguities by finding an optimal assignment based on evidence weights.\n- **Construct Bipartite Graph**: We form a bipartite graph between the set of remaining (unmatched and non-conflicted) local identifiers and the set of remaining (unused) classes. The weight of an edge between a local $\\ell$ and a class $C$ is given by the cluster-level weight $W(\\ell, C)$.\n- **Solve for Maximum-Weight Matching**: The task is to find a one-to-one matching that maximizes the sum of weights of the selected edges. This is a classic assignment problem. We use the `scipy.optimize.linear_sum_assignment` function. Since this function finds a minimum-cost assignment, we construct a cost matrix by negating the weight matrix (i.e., `cost = -weight`). The function returns the indices of the optimal assignment.\n- **Apply Threshold Filter**: The optimal matching from the previous step is provisional. We iterate through each proposed match $(\\ell, C)$ and finalize it only if its weight meets the confidence threshold: $W(\\ell, C) \\ge \\tau$. Matches below the threshold are discarded.\n\n**4. Evaluation**\nFinally, we evaluate the computed mapping $\\phi$ against the provided ground truth $\\phi^\\star$.\n- **Injectivity Check**: We verify that the final mapping $\\phi$ is injective, i.e., no two distinct local identifiers map to the same class. This is an explicit requirement and serves as a sanity check on the algorithm's implementation.\n- **Metrics Calculation**: We compute the number of matches ($n$), soundness ($s$), and completeness ($c$) precisely according to the definitions provided, including handling the edge cases where the number of predictions or the size of the ground truth is zero.\n\nThis principled, phased approach ensures that high-certainty information (keys) is used first to constrain the problem, followed by an optimal but evidence-driven resolution for the remainder, respecting all semantic constraints and rules defined in the problem.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom collections import defaultdict\n\ndef solve_reconciliation(L, U, P_sets, k_L, k_U, w_raw, tau, phi_star_raw):\n    \"\"\"\n    Computes and evaluates a reconciliation mapping phi based on the two-phase algorithm.\n    \"\"\"\n    # 1. Pre-computation and Data Structuring\n    \n    # Use frozensets for classes to make them hashable dictionary keys\n    P = [frozenset(s) for s in P_sets]\n\n    # Canonical representatives and class lookups\n    canonical_reps = {c: min(c) for c in P}\n    rep_to_class = {v: k for k, v in canonical_reps.items()}\n    uri_to_class = {uri: c for c in P for uri in c}\n\n    # Key-to-class mapping\n    key_to_class_rep = {}\n    for uri, key_val in k_U.items():\n        if key_val is not None:\n            if uri in uri_to_class:\n                cls = uri_to_class[uri]\n                key_to_class_rep[key_val] = canonical_reps[cls]\n\n    # Cluster-level weights W(l, C)\n    W = defaultdict(float)\n    for (local, uri), weight in w_raw.items():\n        if uri in uri_to_class:\n            cls = uri_to_class[uri]\n            W[(local, cls)] = max(W[(local, cls)], weight)\n            \n    # Ground truth mapping using canonical representatives\n    phi_star = {\n        local: rep\n        for local, rep in phi_star_raw.items()\n        if rep is not None\n    }\n\n    phi = {}\n    matched_locals = set()\n    matched_classes = set()\n\n    # 2. Phase 1: Hard Key Matching\n    \n    # Group locals by their key value\n    locals_by_key = defaultdict(list)\n    for local in L:\n        key_val = k_L.get(local)\n        if key_val is not None:\n            locals_by_key[key_val].append(local)\n\n    # Process each key group for matches or conflicts\n    for key_val, locals_with_key in locals_by_key.items():\n        if key_val in key_to_class_rep:\n            class_rep = key_to_class_rep[key_val]\n            cls = rep_to_class[class_rep]\n            \n            if len(locals_with_key) == 1: # Unambiguous match\n                local = locals_with_key[0]\n                if local not in matched_locals and cls not in matched_classes:\n                    phi[local] = class_rep\n                    matched_locals.add(local)\n                    matched_classes.add(cls)\n            else: # Key conflict\n                # Per problem, conflicting locals and the class are out of scope for soft matching\n                for local in locals_with_key:\n                    matched_locals.add(local)\n                matched_classes.add(cls)\n\n    # 3. Phase 2: Soft Matching\n    \n    # Identify remaining entities for soft matching\n    remaining_locals = sorted([l for l in L if l not in matched_locals])\n    remaining_classes = sorted([c for c in P if c not in matched_classes], key=lambda c: canonical_reps[c])\n    \n    if remaining_locals and remaining_classes:\n        # Construct weight matrix for the assignment problem\n        weight_matrix = np.zeros((len(remaining_locals), len(remaining_classes)))\n        for i, local in enumerate(remaining_locals):\n            for j, cls in enumerate(remaining_classes):\n                weight_matrix[i, j] = W.get((local, cls), 0.0)\n\n        # Solve for max-weight matching (min-cost on negative weights)\n        cost_matrix = -weight_matrix\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n\n        # Process and filter optimal assignments\n        for r, c in zip(row_ind, col_ind):\n            local, cls = remaining_locals[r], remaining_classes[c]\n            match_weight = weight_matrix[r, c]\n\n            # Apply confidence threshold\n            if match_weight >= tau:\n                phi[local] = canonical_reps[cls]\n\n    # 4. Evaluation\n    \n    # Number of matches\n    n = len(phi)\n    \n    # True positives\n    tp = sum(1 for l, c_rep in phi.items() if l in phi_star and phi_star[l] == c_rep)\n    \n    # Soundness\n    pred = n\n    s = 1.0 if pred == 0 else tp / pred\n    \n    # Completeness\n    truth = len(phi_star)\n    c = 1.0 if truth == 0 else tp / truth\n    \n    # Consistency (Injectivity)\n    is_consistent = len(set(phi.values())) == len(phi)\n    \n    return [n, s, c, is_consistent]\n\n\ndef solve():\n    test_cases = [\n        # Test case 1: happy path\n        {\n            \"L\": [\"L1\", \"L2\", \"L3\"],\n            \"U\": [\"U1\", \"U1a\", \"U2\", \"U3\", \"U4\"],\n            \"P\": [[\"U1\", \"U1a\"], [\"U2\"], [\"U3\"], [\"U4\"]],\n            \"k_L\": {\"L1\": \"GTIN-893-ABC\"},\n            \"k_U\": {\"U1\": \"GTIN-893-ABC\", \"U1a\": \"GTIN-893-ABC\"},\n            \"w\": {(\"L1\", \"U1\"): 0.9, (\"L1\", \"U2\"): 0.2, (\"L2\", \"U2\"): 0.85, (\"L3\", \"U4\"): 0.8, (\"L3\", \"U3\"): 0.3},\n            \"tau\": 0.5,\n            \"phi_star\": {\"L1\": \"U1\", \"L2\": \"U2\", \"L3\": \"U4\"}\n        },\n        # Test case 2: key conflict\n        {\n            \"L\": [\"A\", \"B\"],\n            \"U\": [\"U5\", \"U6\"],\n            \"P\": [[\"U5\"], [\"U6\"]],\n            \"k_L\": {\"A\": \"EPC-XYZ\", \"B\": \"EPC-XYZ\"},\n            \"k_U\": {\"U5\": \"EPC-XYZ\"},\n            \"w\": {(\"A\", \"U5\"): 0.9, (\"B\", \"U6\"): 0.6},\n            \"tau\": 0.5,\n            \"phi_star\": {\"A\": \"U5\", \"B\": None}\n        },\n        # Test case 3: soft matching only\n        {\n            \"L\": [\"X\", \"Y\", \"Z\"],\n            \"U\": [\"V1\", \"V2\", \"V2a\", \"V3\"],\n            \"P\": [[\"V1\"], [\"V2\", \"V2a\"], [\"V3\"]],\n            \"k_L\": {},\n            \"k_U\": {},\n            \"w\": {(\"X\", \"V1\"): 0.55, (\"X\", \"V2\"): 0.50, (\"X\", \"V3\"): 0.10, (\"Y\", \"V2a\"): 0.60, (\"Y\", \"V1\"): 0.40, (\"Z\", \"V3\"): 0.49, (\"Z\", \"V2\"): 0.51},\n            \"tau\": 0.5,\n            \"phi_star\": {\"X\": \"V1\", \"Y\": \"V2\", \"Z\": None}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_reconciliation(\n            case[\"L\"], case[\"U\"], case[\"P\"],\n            case[\"k_L\"], case[\"k_U\"], case[\"w\"],\n            case[\"tau\"], case[\"phi_star\"]\n        )\n        results.append(result)\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}