{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of multi-fidelity modeling is the creation of efficient, low-fidelity surrogates from complex, high-fidelity physical models. This practice guides you through a fundamental technique, Model Order Reduction (MOR) via modal analysis, applied to a structural dynamics system. By constructing a reduced-order model and quantifying the resulting truncation error, you will gain hands-on experience with the foundational trade-off between computational cost and model accuracy .",
            "id": "4232599",
            "problem": "Consider a three-degree-of-freedom undamped structural dynamics model relevant to Digital Twins (DT) of Cyber-Physical Systems (CPS), where the high-fidelity second-order dynamics obey the fundamental principle of linear momentum balance and elastic energy storage in the form\n$$\n\\mathbf{M}\\,\\ddot{\\mathbf{x}}(t) + \\mathbf{K}\\,\\mathbf{x}(t) = \\mathbf{0},\n$$\nwith symmetric positive-definite mass and stiffness matrices $\\mathbf{M}$ and $\\mathbf{K}$. Let the mass matrix be the identity $\\mathbf{M} = \\mathbf{I}_3$ and the stiffness matrix be the standard discrete second-difference operator\n$$\n\\mathbf{K} = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nYou will construct a reduced-order model via a multi-fidelity modeling strategy that preserves the symmetry of $\\mathbf{M}$ and $\\mathbf{K}$ by using a congruence transformation grounded in modal analysis.\n\nTasks:\n1. Starting from the core definition of undamped normal modes for second-order linear time-invariant systems, formulate the generalized eigenvalue problem that diagonalizes the elastic energy with respect to the kinetic energy inner product. Use this to construct a congruence transformation $\\mathbf{T}$ based on an $\\mathbf{M}$-orthonormal modal basis that preserves symmetry of the transformed mass and stiffness. Explicitly choose the two lowest-frequency modes to define a reduced-order model (Model Order Reduction (MOR)) and write down the reduced mass and stiffness obtained by congruence with $\\mathbf{T}$ restricted to these modes.\n\n2. To quantify multi-fidelity error, let the initial displacement be $\\mathbf{x}(0) = (1,0,0)^{\\top}$ and the initial velocity be $\\dot{\\mathbf{x}}(0) = \\mathbf{0}$. Define the modal truncation error as the fraction (expressed as a decimal or fraction, without a percentage sign) of potential energy at $t=0$ that is lost when truncating to the two-mode reduced basis, i.e.,\n$$\n\\varepsilon \\equiv \\frac{\\text{omitted modal potential energy at } t=0}{\\text{total potential energy at } t=0}.\n$$\nCompute $\\varepsilon$ exactly. Express your final answer as a single closed-form analytic expression. No units are required. If you choose to present a numerical approximation, round your answer to six significant figures.",
            "solution": "The problem is assessed as valid as it is scientifically grounded in linear structural dynamics, well-posed with all necessary information provided, and objective in its formulation. The problem is a standard exercise in model order reduction and is directly relevant to multi-fidelity modeling in the context of Digital Twins.\n\nThe solution is developed in two parts as requested.\n\nPart 1: Construction of the Reduced-Order Model\n\nThe governing equation for the undamped system is given by\n$$\n\\mathbf{M}\\,\\ddot{\\mathbf{x}}(t) + \\mathbf{K}\\,\\mathbf{x}(t) = \\mathbf{0}\n$$\nWe seek normal mode solutions of the form $\\mathbf{x}(t) = \\boldsymbol{\\phi} e^{i\\omega t}$, where $\\boldsymbol{\\phi}$ is the mode shape vector and $\\omega$ is the natural frequency. Substituting this into the equation of motion yields:\n$$\n\\mathbf{M}\\,(-\\omega^2 \\boldsymbol{\\phi} e^{i\\omega t}) + \\mathbf{K}\\,(\\boldsymbol{\\phi} e^{i\\omega t}) = \\mathbf{0}\n$$\n$$\n(\\mathbf{K} - \\omega^2 \\mathbf{M}) \\boldsymbol{\\phi} = \\mathbf{0}\n$$\nLetting $\\lambda = \\omega^2$, we arrive at the generalized eigenvalue problem (GEP):\n$$\n\\mathbf{K}\\boldsymbol{\\phi} = \\lambda \\mathbf{M}\\boldsymbol{\\phi}\n$$\nGiven the mass matrix $\\mathbf{M} = \\mathbf{I}_3$, the GEP simplifies to the standard eigenvalue problem:\n$$\n\\mathbf{K}\\boldsymbol{\\phi} = \\lambda \\boldsymbol{\\phi}\n$$\nWe need to find the eigenvalues $\\lambda_i$ and eigenvectors $\\boldsymbol{\\phi}_i$ of the stiffness matrix $\\mathbf{K}$:\n$$\n\\mathbf{K} = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nThe characteristic equation is $\\det(\\mathbf{K} - \\lambda \\mathbf{I}) = 0$:\n$$\n\\det\\begin{pmatrix}\n2-\\lambda & -1 & 0 \\\\\n-1 & 2-\\lambda & -1 \\\\\n0 & -1 & 2-\\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(2-\\lambda)\\left((2-\\lambda)^2 - (-1)(-1)\\right) - (-1)\\left((-1)(2-\\lambda) - 0\\right) = 0\n$$\n$$\n(2-\\lambda)((2-\\lambda)^2 - 1) - (2-\\lambda) = 0\n$$\n$$\n(2-\\lambda)\\left( (2-\\lambda)^2 - 1 - 1 \\right) = 0\n$$\n$$\n(2-\\lambda)(\\lambda^2 - 4\\lambda + 4 - 2) = 0\n$$\n$$\n(2-\\lambda)(\\lambda^2 - 4\\lambda + 2) = 0\n$$\nThe solutions for $\\lambda$ are $\\lambda = 2$ and the roots of $\\lambda^2 - 4\\lambda + 2 = 0$, which are $\\lambda = \\frac{4 \\pm \\sqrt{16-8}}{2} = 2 \\pm \\sqrt{2}$. The eigenvalues, representing the squared natural frequencies $\\omega_i^2$, are, in ascending order:\n$$\n\\lambda_1 = 2 - \\sqrt{2} \\quad, \\quad \\lambda_2 = 2 \\quad, \\quad \\lambda_3 = 2 + \\sqrt{2}\n$$\nNext, we find the corresponding eigenvectors $\\boldsymbol{\\phi}_i$.\n\nFor $\\lambda_1 = 2 - \\sqrt{2}$: $(\\mathbf{K} - \\lambda_1\\mathbf{I})\\boldsymbol{\\phi}_1 = \\mathbf{0}$\n$$\n\\begin{pmatrix} \\sqrt{2} & -1 & 0 \\\\ -1 & \\sqrt{2} & -1 \\\\ 0 & -1 & \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} \\phi_{11} \\\\ \\phi_{12} \\\\ \\phi_{13} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies \\boldsymbol{\\phi}_1 \\propto \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix}\n$$\nFor $\\lambda_2 = 2$: $(\\mathbf{K} - \\lambda_2\\mathbf{I})\\boldsymbol{\\phi}_2 = \\mathbf{0}$\n$$\n\\begin{pmatrix} 0 & -1 & 0 \\\\ -1 & 0 & -1 \\\\ 0 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} \\phi_{21} \\\\ \\phi_{22} \\\\ \\phi_{23} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies \\boldsymbol{\\phi}_2 \\propto \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nThe problem asks for a model based on the two lowest-frequency modes, which correspond to $\\lambda_1$ and $\\lambda_2$.\nTo construct the congruence transformation, we use an $\\mathbf{M}$-orthonormal basis. Since $\\mathbf{M}=\\mathbf{I}$, this is equivalent to a standard orthonormal basis. We normalize the eigenvectors $\\boldsymbol{\\phi}_1$ and $\\boldsymbol{\\phi}_2$ to obtain the basis vectors $\\mathbf{u}_1$ and $\\mathbf{u}_2$.\n$$\n\\|\\boldsymbol{\\phi}_1\\|^2 = 1^2 + (\\sqrt{2})^2 + 1^2 = 4 \\implies \\|\\boldsymbol{\\phi}_1\\| = 2\n$$\n$$\n\\mathbf{u}_1 = \\frac{\\boldsymbol{\\phi}_1}{\\|\\boldsymbol{\\phi}_1\\|} = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\|\\boldsymbol{\\phi}_2\\|^2 = 1^2 + 0^2 + (-1)^2 = 2 \\implies \\|\\boldsymbol{\\phi}_2\\| = \\sqrt{2}\n$$\n$$\n\\mathbf{u}_2 = \\frac{\\boldsymbol{\\phi}_2}{\\|\\boldsymbol{\\phi}_2\\|} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nThe transformation matrix $\\mathbf{T}$ for the reduced-order model is formed by these two modal vectors:\n$$\n\\mathbf{T} = \\begin{pmatrix} \\mathbf{u}_1 & \\mathbf{u}_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{\\sqrt{2}}{2} & 0 \\\\ \\frac{1}{2} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nThe reduced-order model is obtained via the congruence transformation $\\mathbf{x} = \\mathbf{T}\\mathbf{q}$, which yields the reduced system $\\mathbf{M}_r \\ddot{\\mathbf{q}} + \\mathbf{K}_r \\mathbf{q} = \\mathbf{0}$. The reduced mass and stiffness matrices are:\n$$\n\\mathbf{M}_r = \\mathbf{T}^{\\top} \\mathbf{M} \\mathbf{T} = \\mathbf{T}^{\\top} \\mathbf{I} \\mathbf{T} = \\mathbf{T}^{\\top} \\mathbf{T}\n$$\n$$\n\\mathbf{K}_r = \\mathbf{T}^{\\top} \\mathbf{K} \\mathbf{T}\n$$\nBy the $\\mathbf{M}$-orthonormality property of the modal vectors, $\\mathbf{u}_i^{\\top}\\mathbf{M}\\mathbf{u}_j = \\delta_{ij}$, the reduced mass matrix is the identity matrix:\n$$\n\\mathbf{M}_r = \\begin{pmatrix} \\mathbf{u}_1^{\\top}\\mathbf{u}_1 & \\mathbf{u}_1^{\\top}\\mathbf{u}_2 \\\\ \\mathbf{u}_2^{\\top}\\mathbf{u}_1 & \\mathbf{u}_2^{\\top}\\mathbf{u}_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\mathbf{I}_2\n$$\nThe reduced stiffness matrix becomes a diagonal matrix of the corresponding eigenvalues, due to the property $\\mathbf{u}_i^{\\top}\\mathbf{K}\\mathbf{u}_j = \\lambda_j \\delta_{ij}$:\n$$\n\\mathbf{K}_r = \\begin{pmatrix} \\mathbf{u}_1^{\\top}\\mathbf{K}\\mathbf{u}_1 & \\mathbf{u}_1^{\\top}\\mathbf{K}\\mathbf{u}_2 \\\\ \\mathbf{u}_2^{\\top}\\mathbf{K}\\mathbf{u}_1 & \\mathbf{u}_2^{\\top}\\mathbf{K}\\mathbf{u}_2 \\end{pmatrix} = \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 2 - \\sqrt{2} & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\n\nPart 2: Computation of Modal Truncation Error\n\nThe potential energy of the system is $V = \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{K}\\mathbf{x}$. The total energy at $t=0$ is purely potential energy, since $\\dot{\\mathbf{x}}(0) = \\mathbf{0}$.\n$$\nE_{\\text{total}} = V(0) = \\frac{1}{2}\\mathbf{x}(0)^{\\top}\\mathbf{K}\\mathbf{x}(0)\n$$\nWith $\\mathbf{x}(0) = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}^{\\top}$,\n$$\nE_{\\text{total}} = \\frac{1}{2}\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{2}(2) = 1\n$$\nTo find the modal energy distribution, we express the initial displacement $\\mathbf{x}(0)$ in the full modal basis $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\}$. First, we find $\\mathbf{u}_3$:\n$$\n\\boldsymbol{\\phi}_3 \\propto \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\\\ 1 \\end{pmatrix} \\implies \\mathbf{u}_3 = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\\\ 1 \\end{pmatrix}\n$$\nThe modal coordinates $c_i(0)$ are found by projection: $c_i(0) = \\mathbf{u}_i^{\\top}\\mathbf{x}(0)$.\n$$\nc_1(0) = \\mathbf{u}_1^{\\top}\\mathbf{x}(0) = \\frac{1}{2}(1) + \\frac{\\sqrt{2}}{2}(0) + \\frac{1}{2}(0) = \\frac{1}{2}\n$$\n$$\nc_2(0) = \\mathbf{u}_2^{\\top}\\mathbf{x}(0) = \\frac{1}{\\sqrt{2}}(1) + 0(0) - \\frac{1}{\\sqrt{2}}(0) = \\frac{1}{\\sqrt{2}}\n$$\n$$\nc_3(0) = \\mathbf{u}_3^{\\top}\\mathbf{x}(0) = \\frac{1}{2}(1) - \\frac{\\sqrt{2}}{2}(0) + \\frac{1}{2}(0) = \\frac{1}{2}\n$$\nThe potential energy can be written as a sum of modal potential energies: $V = \\sum_{i=1}^{3} V_i = \\sum_{i=1}^{3} \\frac{1}{2}c_i^2 \\lambda_i$.\nThe total potential energy at $t=0$ is:\n$$\nV(0) = \\frac{1}{2}\\left( c_1(0)^2 \\lambda_1 + c_2(0)^2 \\lambda_2 + c_3(0)^2 \\lambda_3 \\right)\n$$\n$$\nV(0) = \\frac{1}{2}\\left( \\left(\\frac{1}{2}\\right)^2(2-\\sqrt{2}) + \\left(\\frac{1}{\\sqrt{2}}\\right)^2(2) + \\left(\\frac{1}{2}\\right)^2(2+\\sqrt{2}) \\right)\n$$\n$$\nV(0) = \\frac{1}{2}\\left( \\frac{1}{4}(2-\\sqrt{2}) + \\frac{1}{2}(2) + \\frac{1}{4}(2+\\sqrt{2}) \\right) = \\frac{1}{2}\\left( \\frac{1}{2} - \\frac{\\sqrt{2}}{4} + 1 + \\frac{1}{2} + \\frac{\\sqrt{2}}{4} \\right) = \\frac{1}{2}(2) = 1\n$$\nThis confirms the direct calculation.\nThe reduced-order model retains modes $1$ and $2$. The omitted mode is mode $3$. The omitted potential energy is the energy in mode $3$:\n$$\nV_{\\text{omitted}} = V_3(0) = \\frac{1}{2}c_3(0)^2 \\lambda_3 = \\frac{1}{2}\\left(\\frac{1}{2}\\right)^2 (2+\\sqrt{2}) = \\frac{1}{8}(2+\\sqrt{2})\n$$\nThe modal truncation error $\\varepsilon$ is defined as the ratio of omitted potential energy to total potential energy at $t=0$:\n$$\n\\varepsilon = \\frac{V_{\\text{omitted}}}{V(0)} = \\frac{\\frac{1}{8}(2+\\sqrt{2})}{1} = \\frac{2+\\sqrt{2}}{8}\n$$\nSimplifying the expression gives:\n$$\n\\varepsilon = \\frac{2}{8} + \\frac{\\sqrt{2}}{8} = \\frac{1}{4} + \\frac{\\sqrt{2}}{8}\n$$\nThis is the exact analytical expression for the modal truncation error.",
            "answer": "$$\n\\boxed{\\frac{2+\\sqrt{2}}{8}}\n$$"
        },
        {
            "introduction": "Beyond physics-based reduction, data-driven techniques offer powerful ways to fuse information from multiple sources. This exercise introduces the auto-regressive Gaussian Process model, a key method for combining sparse, expensive high-fidelity data with abundant, cheaper low-fidelity data. You will compute the posterior distribution for a high-fidelity function, directly applying the principles of Bayesian inference to create a surrogate model that is more accurate than one built from either data source alone .",
            "id": "4232572",
            "problem": "In a digital twin for a cyber-physical system, consider a two-level multi-fidelity auto-regressive model where the high-fidelity function $f_H(x)$ is related to the low-fidelity function $f_L(x)$ by $f_H(x) = \\rho f_L(x) + \\delta(x)$. The low-fidelity component $f_L(x)$ is modeled as a zero-mean Gaussian Process (GP) with covariance $k_L(x,x')$, and the discrepancy $\\delta(x)$ is modeled as an independent zero-mean Gaussian Process with covariance $k_{\\Delta}(x,x')$. These Gaussian Process components are independent. You are given one co-located pair of observations and hyperparameters as follows:\n- Observed input-output pair: $(x_1, f_L(x_1), f_H(x_1)) = (0, 0, 1)$.\n- Auto-regressive coefficient: $\\rho = \\frac{1}{2}$.\n- Low-fidelity covariance: $k_L(x,x') = \\sigma_L^2 \\exp\\!\\big(-\\frac{(x-x')^2}{2\\ell_L^2}\\big)$ with $\\sigma_L^2 = 1$ and $\\ell_L = 1$.\n- Discrepancy covariance: $k_{\\Delta}(x,x') = \\sigma_{\\Delta}^2 \\exp\\!\\big(-\\frac{(x-x')^2}{2\\ell_{\\Delta}^2}\\big)$ with $\\sigma_{\\Delta}^2 = 1$ and $\\ell_{\\Delta} = 1$.\n- Observation noise variances: $\\sigma_{n,L}^2 = 0$ and $\\sigma_{n,H}^2 = 0$.\n\nStarting from the core definitions of Gaussian Processes and properties of multivariate normal distributions, and using the above modeling assumptions and hyperparameters, compute the posterior mean $\\mu^{\\star}$ and posterior variance $\\sigma_{\\star}^2$ of the high-fidelity function $f_H(x)$ at the test input $x^{\\star} = 1$. Express your final answer as a single closed-form analytic expression for the row matrix $\\big(\\mu^{\\star},\\;\\sigma_{\\star}^2\\big)$, and do not round.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard application of multi-fidelity Gaussian Process (GP) regression. All necessary information, including the model structure, hyperparameters, and observation data, is provided. Therefore, the problem is valid, and a solution can be derived.\n\nThe high-fidelity model is given by the auto-regressive relationship:\n$$f_H(x) = \\rho f_L(x) + \\delta(x)$$\nwhere $f_L(x)$ and $\\delta(x)$ are independent, zero-mean Gaussian Processes.\n$f_L(x) \\sim \\mathcal{GP}(0, k_L(x,x'))$\n$\\delta(x) \\sim \\mathcal{GP}(0, k_{\\Delta}(x,x'))$\n\nWe are tasked with finding the posterior mean $\\mu^{\\star}$ and posterior variance $\\sigma_{\\star}^2$ of $f_H(x^{\\star})$ at the test point $x^{\\star} = 1$, conditioned on the provided observations.\n\nThe posterior distribution of $f_H(x^{\\star})$ is Gaussian because it is a linear combination of the random variables $f_L(x^{\\star})$ and $\\delta(x^{\\star})$, whose posterior distributions are also Gaussian. The posterior mean and variance of $f_H(x^{\\star})$ are determined by the properties of linear combinations of random variables.\n\nBy the linearity of expectation, the posterior mean is:\n$$\\mu^{\\star} = \\mathbb{E}[f_H(x^{\\star}) | \\text{data}] = \\mathbb{E}[\\rho f_L(x^{\\star}) + \\delta(x^{\\star}) | \\text{data}] = \\rho \\mathbb{E}[f_L(x^{\\star}) | \\text{data}] + \\mathbb{E}[\\delta(x^{\\star}) | \\text{data}]$$\nLet's denote $\\mu_{L}^{\\star} = \\mathbb{E}[f_L(x^{\\star}) | \\text{data}]$ and $\\mu_{\\delta}^{\\star} = \\mathbb{E}[\\delta(x^{\\star}) | \\text{data}]$. Then, $\\mu^{\\star} = \\rho \\mu_{L}^{\\star} + \\mu_{\\delta}^{\\star}$.\n\nGiven that the GPs $f_L(x)$ and $\\delta(x)$ are independent, and the data for each can be separated, their posterior distributions are also independent. Thus, the posterior variance is the sum of the scaled variances:\n$$\\sigma_{\\star}^2 = \\text{Var}[f_H(x^{\\star}) | \\text{data}] = \\text{Var}[\\rho f_L(x^{\\star}) + \\delta(x^{\\star}) | \\text{data}] = \\rho^2 \\text{Var}[f_L(x^{\\star}) | \\text{data}] + \\text{Var}[\\delta(x^{\\star}) | \\text{data}]$$\nLet's denote $\\sigma_{L, \\star}^2 = \\text{Var}[f_L(x^{\\star}) | \\text{data}]$ and $\\sigma_{\\delta, \\star}^2 = \\text{Var}[\\delta(x^{\\star}) | \\text{data}]$. Then, $\\sigma_{\\star}^2 = \\rho^2 \\sigma_{L, \\star}^2 + \\sigma_{\\delta, \\star}^2$.\n\nThe problem is thus decomposed into two separate GP regression tasks.\n\nFirst, we must identify the observation data for each process. We are given the co-located observation $(x_1, f_L(x_1), f_H(x_1)) = (0, 0, 1)$ with zero noise, so these are direct observations of the function values.\n- For the low-fidelity process $f_L(x)$, we have the data point $D_L = \\{(x_1, y_L)\\}$, where $x_1 = 0$ and $y_L = f_L(0) = 0$.\n- For the discrepancy process $\\delta(x)$, we can infer its value at $x_1$ from the model equation:\n$$\\delta(x_1) = f_H(x_1) - \\rho f_L(x_1)$$\nSubstituting the known values $\\rho = 1/2$, $f_H(0) = 1$, and $f_L(0) = 0$:\n$$y_{\\delta} = \\delta(0) = 1 - \\frac{1}{2}(0) = 1$$\nSo, for the discrepancy process $\\delta(x)$, we have the data point $D_{\\delta} = \\{(x_1, y_{\\delta})\\}$, where $x_1 = 0$ and $y_{\\delta} = 1$.\n\nNow, we apply the standard GP conditioning formulas. For a GP with prior mean $0$ and covariance $k(x,x')$, given observations $\\mathbf{y}$ at inputs $X$, the posterior distribution at a test point $x^{\\star}$ is a Gaussian with mean $\\mu_{\\star}$ and variance $\\sigma_{\\star}^2$ given by:\n$$\\mu_{\\star} = \\mathbf{k}_{\\star}^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}$$\n$$\\sigma_{\\star}^2 = k(x^{\\star}, x^{\\star}) - \\mathbf{k}_{\\star}^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_{\\star}$$\nwhere $K = K(X, X)$ is the covariance matrix of training inputs, $\\mathbf{k}_{\\star} = K(X, x^{\\star})$ is the vector of covariances between training and test inputs, and $\\sigma_n^2$ is the observation noise variance, which is $0$ in this problem.\n\n**1. Posterior for $f_L(x^{\\star})$**\nWe have one training point $x_1 = 0$ with output $y_L=0$. The test point is $x^{\\star}=1$.\nThe covariance function is $k_L(x,x') = \\sigma_L^2 \\exp(-\\frac{(x-x')^2}{2\\ell_L^2}) = 1 \\cdot \\exp(-\\frac{(x-x')^2}{2})$.\n- $K = k_L(x_1, x_1) = k_L(0,0) = \\exp(0) = 1$.\n- $\\mathbf{k}_{\\star} = k_L(x_1, x^{\\star}) = k_L(0,1) = \\exp(-\\frac{(0-1)^2}{2}) = \\exp(-\\frac{1}{2})$.\n- $k(x^{\\star}, x^{\\star}) = k_L(1,1) = \\exp(0) = 1$.\nThe posterior mean for $f_L$ is:\n$$\\mu_{L}^{\\star} = k_L(1,0) [k_L(0,0)]^{-1} y_L = \\exp(-\\frac{1}{2}) \\cdot (1)^{-1} \\cdot 0 = 0$$\nThe posterior variance for $f_L$ is:\n$$\\sigma_{L, \\star}^2 = k_L(1,1) - k_L(1,0) [k_L(0,0)]^{-1} k_L(0,1) = 1 - \\exp(-\\frac{1}{2}) \\cdot (1)^{-1} \\cdot \\exp(-\\frac{1}{2}) = 1 - \\exp(-1)$$\n\n**2. Posterior for $\\delta(x^{\\star})$**\nWe have one training point $x_1 = 0$ with output $y_{\\delta}=1$. The test point is $x^{\\star}=1$.\nThe covariance function is $k_{\\Delta}(x,x') = \\sigma_{\\Delta}^2 \\exp(-\\frac{(x-x')^2}{2\\ell_{\\Delta}^2}) = 1 \\cdot \\exp(-\\frac{(x-x')^2}{2})$. This is identical to $k_L$.\n- $K = k_{\\Delta}(0,0) = 1$.\n- $\\mathbf{k}_{\\star} = k_{\\Delta}(0,1) = \\exp(-\\frac{1}{2})$.\n- $k(x^{\\star}, x^{\\star}) = k_{\\Delta}(1,1) = 1$.\nThe posterior mean for $\\delta$ is:\n$$\\mu_{\\delta}^{\\star} = k_{\\Delta}(1,0) [k_{\\Delta}(0,0)]^{-1} y_{\\delta} = \\exp(-\\frac{1}{2}) \\cdot (1)^{-1} \\cdot 1 = \\exp(-\\frac{1}{2})$$\nThe posterior variance for $\\delta$ is:\n$$\\sigma_{\\delta, \\star}^2 = k_{\\Delta}(1,1) - k_{\\Delta}(1,0) [k_{\\Delta}(0,0)]^{-1} k_{\\Delta}(0,1) = 1 - \\exp(-\\frac{1}{2}) \\cdot (1)^{-1} \\cdot \\exp(-\\frac{1}{2}) = 1 - \\exp(-1)$$\n\n**3. Combine for $f_H(x^{\\star})$**\nUsing the results from the two independent regressions and the auto-regressive coefficient $\\rho = 1/2$:\nThe posterior mean of $f_H(x^{\\star})$ is:\n$$\\mu^{\\star} = \\rho \\mu_{L}^{\\star} + \\mu_{\\delta}^{\\star} = \\frac{1}{2} \\cdot 0 + \\exp(-\\frac{1}{2}) = \\exp(-\\frac{1}{2})$$\nThe posterior variance of $f_H(x^{\\star})$ is:\n$$\\sigma_{\\star}^2 = \\rho^2 \\sigma_{L, \\star}^2 + \\sigma_{\\delta, \\star}^2 = \\left(\\frac{1}{2}\\right)^2 (1 - \\exp(-1)) + (1 - \\exp(-1))$$\n$$\\sigma_{\\star}^2 = \\frac{1}{4}(1 - \\exp(-1)) + 1 \\cdot (1 - \\exp(-1)) = \\left(\\frac{1}{4} + 1\\right)(1 - \\exp(-1)) = \\frac{5}{4}(1 - \\exp(-1))$$\nThe posterior mean and variance of $f_H(x^{\\star})$ at $x^{\\star} = 1$ are $\\mu^{\\star} = \\exp(-\\frac{1}{2})$ and $\\sigma_{\\star}^2 = \\frac{5}{4}(1 - \\exp(-1))$. The final answer is the row matrix $(\\mu^{\\star}, \\sigma_{\\star}^2)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\exp\\left(-\\frac{1}{2}\\right) & \\frac{5}{4}\\left(1 - \\exp(-1)\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The ultimate goal of many multi-fidelity systems is to make optimal decisions under resource constraints. This final practice moves from static model creation to dynamic, intelligent decision-making by having you implement an adaptive fidelity selection policy. You will design a strategy that, at each step, chooses the most informative action by maximizing the expected reduction in uncertainty per unit cost, a core concept in efficient experimental design and the operation of smart digital twins .",
            "id": "4232543",
            "problem": "You are tasked with designing and implementing an adaptive fidelity selection policy for identifying the thermal diffusivity parameter in a one-dimensional conduction setting within the discipline of Digital Twins and Cyber-Physical Systems (CPS). The objective is to minimize the posterior uncertainty in the estimated thermal diffusivity under a fixed total budget by choosing between a low-fidelity and a high-fidelity sensor model at each step, and to compare the efficiency of this adaptive strategy to fixed-fidelity strategies under equal total cost.\n\nFoundational base: Consider the one-dimensional heat equation on a bar of length $L$ with zero Dirichlet boundary conditions,\n$$\n\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2},\n$$\nwhere $u(x,t)$ is the temperature field, $\\alpha$ is the thermal diffusivity (with units $\\mathrm{m}^2/\\mathrm{s}$), $x \\in [0,L]$ is the spatial coordinate, and $t \\ge 0$ is time. Suppose that the initial condition is a single Fourier sine mode,\n$$\nu(x,0) = \\sin\\left(\\frac{\\pi x}{L}\\right),\n$$\nwhich yields the well-tested analytical solution\n$$\nu(x,t;\\alpha) = \\sin\\left(\\frac{\\pi x}{L}\\right) \\exp\\left(-\\alpha \\left(\\frac{\\pi}{L}\\right)^2 t\\right).\n$$\nThe unknown parameter to be identified is $\\alpha$. Assume a Gaussian prior for $\\alpha$, $\\alpha \\sim \\mathcal{N}(\\mu_0, s_0^2)$, where $\\mu_0$ and $s_0^2$ denote the prior mean and variance, respectively.\n\nDefine two sensor fidelity models:\n\n- Low fidelity ($L$): A structurally damped model with scaling factor $\\chi_L = 1 - \\delta$ and additive Gaussian noise of standard deviation $\\sigma_L$. The noiseless mean function is\n$$\ng_L(\\alpha) = A \\exp\\left(-\\alpha \\chi_L k t\\right),\n$$\nwhere $A = \\sin\\left(\\frac{\\pi x}{L}\\right)$ and $k = \\left(\\frac{\\pi}{L}\\right)^2$. Each low-fidelity measurement incurs cost $c_L$.\n\n- High fidelity ($H$): An accurate model with $\\chi_H = 1$ and additive Gaussian noise of standard deviation $\\sigma_H$,\n$$\ng_H(\\alpha) = A \\exp\\left(-\\alpha \\chi_H k t\\right) = A \\exp\\left(-\\alpha k t\\right),\n$$\nand each high-fidelity measurement incurs cost $c_H$.\n\nAdaptive selection policy requirement: Implement an adaptive fidelity selection policy that, at each step, chooses between $L$ and $H$ to maximize the expected reduction in posterior variance per unit cost under a local linearization about the current posterior mean. Use a one-step linear-Gaussian update approximation with Jacobian evaluated at the current posterior mean $\\mu$,\n$$\nJ_f(\\mu) \\equiv \\frac{\\partial g_f}{\\partial \\alpha}\\Big|_{\\alpha=\\mu} = -A \\chi_f k t \\exp\\left(-\\mu \\chi_f k t\\right),\n$$\nwhere $f \\in \\{L,H\\}$. The posterior variance update for a single measurement of fidelity $f$ under the linearized model with Gaussian noise $\\mathcal{N}(0,\\sigma_f^2)$ is\n$$\ns_{\\text{new}}^2 = s^2 - \\frac{s^4 J_f(\\mu)^2}{\\sigma_f^2 + s^2 J_f(\\mu)^2}.\n$$\nDesign the adaptive policy to select the fidelity $f$ that maximizes the efficiency ratio defined as expected variance reduction per unit cost,\n$$\n\\rho_f \\equiv \\frac{s^2 - s_{\\text{new}}^2}{c_f} = \\frac{s^4 J_f(\\mu)^2}{\\left(\\sigma_f^2 + s^2 J_f(\\mu)^2\\right) c_f}.\n$$\nProceed greedily until the remaining budget cannot afford any measurement. For deterministic evaluation of expected information, treat the posterior mean as fixed at $\\mu$ for Jacobian evaluation throughout the procedure, and update only the posterior variance $s^2$ after each step.\n\nFixed-fidelity baselines: Implement two baselines that spend the same total budget exclusively on one fidelity:\n- Fixed-low: repeatedly choose $L$ until the budget is exhausted.\n- Fixed-high: repeatedly choose $H$ until the budget is exhausted.\n\nOutput requirement: For each test case, compute the three final posterior variances, expressed in $(\\mathrm{m}^2/\\mathrm{s})^2$: $v_{\\text{adapt}}$, $v_{\\text{fixed-low}}$, and $v_{\\text{fixed-high}}$. Your program should produce a single line of output containing the results as a comma-separated list of lists, one per test case, each inner list in the order $[v_{\\text{adapt}}, v_{\\text{fixed-low}}, v_{\\text{fixed-high}}]$.\n\nTest suite: Use the following four scientifically realistic parameter sets. Each test case is specified as $(L, x, t, \\mu_0, s_0, \\delta, \\sigma_L, \\sigma_H, c_L, c_H, B)$, with $L$ in meters, $x$ in meters, $t$ in seconds, $\\mu_0$ and $s_0$ in $\\mathrm{m}^2/\\mathrm{s}$ (note $s_0$ is the prior standard deviation), $\\delta$ dimensionless, $\\sigma_L$ and $\\sigma_H$ are temperature amplitudes in the same dimensionless units as $u$, costs $c_L$ and $c_H$ are in arbitrary cost units, and $B$ is the total budget in the same cost units. The final posterior variances must be expressed in $(\\mathrm{m}^2/\\mathrm{s})^2$.\n\n- Case $1$ (happy path): $(L,x,t,\\mu_0,s_0,\\delta,\\sigma_L,\\sigma_H,c_L,c_H,B) = (1.0, 0.5, 2.0, 0.2, 0.05, 0.1, 0.05, 0.01, 1.0, 5.0, 20.0)$.\n- Case $2$ (budget boundary: insufficient for any high-fidelity step): $(1.0, 0.5, 2.0, 0.2, 0.05, 0.1, 0.05, 0.01, 1.0, 5.0, 1.0)$.\n- Case $3$ (low-fidelity relatively cheap but moderately noisy): $(1.0, 0.5, 2.0, 0.2, 0.05, 0.0, 0.02, 0.01, 1.0, 10.0, 15.0)$.\n- Case $4$ (strong low-fidelity structural bias and longer observation time): $(1.0, 0.5, 5.0, 0.2, 0.05, 0.3, 0.05, 0.01, 1.0, 5.0, 20.0)$.\n\nImplementation details:\n- Compute $A = \\sin\\left(\\frac{\\pi x}{L}\\right)$ and $k = \\left(\\frac{\\pi}{L}\\right)^2$.\n- Use $\\chi_L = 1 - \\delta$ and $\\chi_H = 1$.\n- Start with variance $s^2 = s_0^2$; update $s^2$ using the local linearization formula upon each selected measurement.\n- The adaptive policy must select the fidelity $f \\in \\{L,H\\}$ that maximizes $\\rho_f$ among those affordable with the remaining budget; break if neither is affordable.\n- For fixed-low and fixed-high, repeat selection until the remaining budget cannot afford the next measurement.\n- No random sampling is permitted; all calculations must be deterministic and based on the local linearization and variance update only.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, for example, $[[\\text{case1}], [\\text{case2}], [\\text{case3}], [\\text{case4}]]$, where each inner list is $[v_{\\text{adapt}}, v_{\\text{fixed-low}}, v_{\\text{fixed-high}}]$ with each $v$ a floating-point number in $(\\mathrm{m}^2/\\mathrm{s})^2$.",
            "solution": "The user-provided problem statement has been analyzed and found to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique, verifiable solution.\n\nThe problem requires the design and implementation of three distinct strategies for allocating a fixed computational budget to estimate the thermal diffusivity parameter, $\\alpha$, in a one-dimensional heat conduction problem. The goal is to compare the effectiveness of an adaptive multi-fidelity sampling strategy against two fixed-fidelity baseline strategies in reducing the posterior variance of the estimate for $\\alpha$.\n\nThe physical system is a one-dimensional bar of length $L$ governed by the heat equation:\n$$\n\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}\n$$\nWith the specified initial condition $u(x,0) = \\sin\\left(\\frac{\\pi x}{L}\\right)$ and zero Dirichlet boundary conditions, the analytical solution for the temperature field $u(x,t)$ is given by:\n$$\nu(x,t;\\alpha) = \\sin\\left(\\frac{\\pi x}{L}\\right) \\exp\\left(-\\alpha \\left(\\frac{\\pi}{L}\\right)^2 t\\right)\n$$\nThe parameter to be inferred, $\\alpha$, is modeled with a Gaussian prior distribution, $\\alpha \\sim \\mathcal{N}(\\mu_0, s_0^2)$, where $\\mu_0$ is the prior mean and $s_0^2$ is the prior variance.\n\nMeasurements are obtained from two sensor models of differing fidelity and cost:\n1.  A low-fidelity ($L$) model, which is structurally biased and noisy. The noiseless mean function is $g_L(\\alpha) = A \\exp(-\\alpha \\chi_L k t)$, where $A = \\sin\\left(\\frac{\\pi x}{L}\\right)$, $k = \\left(\\frac{\\pi}{L}\\right)^2$, and $\\chi_L = 1 - \\delta$ is a structural bias factor. Measurements from this model have an associated cost $c_L$ and are corrupted by additive Gaussian noise with variance $\\sigma_L^2$.\n2.  A high-fidelity ($H$) model, which is structurally accurate. The noiseless mean function is $g_H(\\alpha) = A \\exp(-\\alpha \\chi_H k t)$ with $\\chi_H = 1$. Measurements have a cost $c_H$ and are corrupted by additive Gaussian noise with variance $\\sigma_H^2$.\n\nThe inference process uses a simplified Bayesian update based on a local linearization of the sensor models. The Jacobian of a model $g_f$ (where $f \\in \\{L, H\\}$) is evaluated at the prior mean $\\mu_0$:\n$$\nJ_f(\\mu_0) \\equiv \\frac{\\partial g_f}{\\partial \\alpha}\\Big|_{\\alpha=\\mu_0} = -A \\chi_f k t \\exp\\left(-\\mu_0 \\chi_f k t\\right)\n$$\nAs per the problem specification, this Jacobian remains constant throughout the simulation for a given test case. Given a current posterior variance $s^2$ for $\\alpha$, a single measurement from model $f$ updates the variance to $s_{\\text{new}}^2$ according to the formula:\n$$\ns_{\\text{new}}^2 = s^2 - \\frac{s^4 J_f(\\mu_0)^2}{\\sigma_f^2 + s^2 J_f(\\mu_0)^2}\n$$\nFor numerical stability and efficiency, this update is implemented using precisions (inverse variances). If $p = 1/s^2$ is the current precision, the updated precision $p_{\\text{new}}$ after a measurement is $p_{\\text{new}} = p + J_f(\\mu_0)^2 / \\sigma_f^2$. The new variance is then $s_{\\text{new}}^2 = 1/p_{\\text{new}}$.\n\nThree strategies are implemented to spend the total budget $B$:\n\n1.  **Adaptive Strategy**: At each step, this policy greedily selects the measurement that maximizes the expected posterior variance reduction per unit cost. The efficiency metric, $\\rho_f$, for each fidelity $f$ is:\n    $$\n    \\rho_f = \\frac{s^2 - s_{\\text{new}}^2}{c_f} = \\frac{s^4 J_f(\\mu_0)^2}{\\left(\\sigma_f^2 + s^2 J_f(\\mu_0)^2\\right) c_f}\n    $$\n    The policy computes $\\rho_L$ and $\\rho_H$ based on the current posterior variance $s^2$ and chooses the fidelity with the higher $\\rho$ value, provided it is affordable within the remaining budget. The simulation terminates when neither measurement type is affordable.\n\n2.  **Fixed-Low Strategy**: This baseline strategy exclusively uses the low-fidelity model. It repeatedly performs measurements with model $L$, updating the posterior variance and decrementing the budget by $c_L$ at each step, until the remaining budget is less than $c_L$.\n\n3.  **Fixed-High Strategy**: This baseline exclusively uses the high-fidelity model. It repeatedly performs measurements with model $H$, costing $c_H$ per step, until the budget is exhausted.\n\nFor each test case, the three strategies are run independently, each starting with the same initial variance $s_0^2$ and total budget $B$. The final posterior variance for each strategy ($v_{\\text{adapt}}$, $v_{\\text{fixed-low}}$, $v_{\\text{fixed-high}}$) is then reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the adaptive fidelity selection problem for all test cases.\n    \"\"\"\n    # Test suite as specified in the problem statement.\n    # Each case: (L, x, t, mu_0, s_0, delta, sigma_L, sigma_H, c_L, c_H, B)\n    test_cases = [\n        (1.0, 0.5, 2.0, 0.2, 0.05, 0.1, 0.05, 0.01, 1.0, 5.0, 20.0), # Case 1\n        (1.0, 0.5, 2.0, 0.2, 0.05, 0.1, 0.05, 0.01, 1.0, 5.0, 1.0),   # Case 2\n        (1.0, 0.5, 2.0, 0.2, 0.05, 0.0, 0.02, 0.01, 1.0, 10.0, 15.0), # Case 3\n        (1.0, 0.5, 5.0, 0.2, 0.05, 0.3, 0.05, 0.01, 1.0, 5.0, 20.0),  # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        variances = run_simulation_for_case(case)\n        results.append(variances)\n    \n    # Format the final output as a comma-separated list of lists.\n    # The str() function on a list correctly produces the bracketed format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation_for_case(case):\n    \"\"\"\n    Runs the simulations for the adaptive, fixed-low, and fixed-high strategies for a single case.\n    Returns a list of final posterior variances [v_adapt, v_fixed_low, v_fixed_high].\n    \"\"\"\n    # Unpack parameters from the test case tuple\n    L, x, t, mu_0, s_0, delta, sigma_L, sigma_H, c_L, c_H, B = case\n\n    # Initial variance and noise variances\n    s2_initial = s_0 ** 2\n    sigma_L_sq = sigma_L ** 2\n    sigma_H_sq = sigma_H ** 2\n\n    # Pre-compute constants\n    A = np.sin(np.pi * x / L)\n    k = (np.pi / L) ** 2\n    chi_L = 1.0 - delta\n    chi_H = 1.0\n\n    # Pre-compute Jacobians (evaluated at mu_0 and held constant) and their squares\n    J_L = -A * chi_L * k * t * np.exp(-mu_0 * chi_L * k * t)\n    J_H = -A * chi_H * k * t * np.exp(-mu_0 * chi_H * k * t)\n    J_L_sq = J_L ** 2\n    J_H_sq = J_H ** 2\n\n    # --- Strategy 1: Adaptive Fidelity Selection ---\n    s2_adapt = s2_initial\n    budget_adapt = B\n    while True:\n        can_afford_L = budget_adapt >= c_L\n        can_afford_H = budget_adapt >= c_H\n\n        rho_L = -1\n        if can_afford_L:\n            # Efficiency for low-fidelity model\n            numerator_L = (s2_adapt ** 2) * J_L_sq\n            denominator_L = (sigma_L_sq + s2_adapt * J_L_sq) * c_L\n            rho_L = numerator_L / denominator_L\n\n        rho_H = -1\n        if can_afford_H:\n            # Efficiency for high-fidelity model\n            numerator_H = (s2_adapt ** 2) * J_H_sq\n            denominator_H = (sigma_H_sq + s2_adapt * J_H_sq) * c_H\n            rho_H = numerator_H / denominator_H\n        \n        # Decision logic\n        if not can_afford_L and not can_afford_H:\n            break\n        \n        # Use precision update for numerical stability: p_new = p_old + J^2/sigma^2\n        p_adapt = 1.0 / s2_adapt\n        if (can_afford_L and rho_L > rho_H) or (can_afford_L and not can_afford_H):\n            p_adapt += J_L_sq / sigma_L_sq\n            budget_adapt -= c_L\n        elif can_afford_H:\n            p_adapt += J_H_sq / sigma_H_sq\n            budget_adapt -= c_H\n        \n        s2_adapt = 1.0 / p_adapt\n            \n    v_adapt = s2_adapt\n\n    # --- Strategy 2: Fixed-Low Fidelity ---\n    s2_low = s2_initial\n    budget_low = B\n    p_low = 1.0 / s2_low\n    info_gain_low = J_L_sq / sigma_L_sq\n    while budget_low >= c_L:\n        p_low += info_gain_low\n        budget_low -= c_L\n    v_fixed_low = 1.0 / p_low if (1.0/s2_initial) != p_low else s2_initial\n\n    # --- Strategy 3: Fixed-High Fidelity ---\n    s2_high = s2_initial\n    budget_high = B\n    p_high = 1.0 / s2_high\n    info_gain_high = J_H_sq / sigma_H_sq\n    while budget_high >= c_H:\n        p_high += info_gain_high\n        budget_high -= c_H\n    v_fixed_high = 1.0 / p_high if (1.0/s2_initial) != p_high else s2_initial\n\n    return [v_adapt, v_fixed_low, v_fixed_high]\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}