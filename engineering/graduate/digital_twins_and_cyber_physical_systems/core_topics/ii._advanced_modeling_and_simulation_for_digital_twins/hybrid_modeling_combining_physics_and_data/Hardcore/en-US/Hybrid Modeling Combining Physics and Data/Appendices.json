{
    "hands_on_practices": [
        {
            "introduction": "This first practice focuses on a common scenario: a well-understood physical structure with an unknown, complex component. We will build a hybrid model for a simple mechanical system where the friction force is learned from data. The key challenge is to ensure the learned model respects a fundamental law of physics—that friction always dissipates energy—which we will enforce using a constrained optimization technique .",
            "id": "4226933",
            "problem": "Consider a single-degree-of-freedom cyber-physical system with a physical plant and its digital twin sharing the hybrid model coupling physics-based structure with data-driven friction. Let the plant be a point mass on a linear spring with unknown speed-dependent damping. The longitudinal motion is described by Newton's second law of motion and Hooke's law. The governing physics is that the sum of forces equals mass times acceleration. Denote position by $x(t)$ in meters, velocity by $v(t)$ in meters per second, acceleration by $a(t)$ in meters per second squared, external actuation force by $u(t)$ in newtons, mass by $m$ in kilograms, linear stiffness by $k$ in newtons per meter, and friction force by $F_{\\mathrm{fric}}(t)$ in newtons. The fundamental equations are: \n$$ m \\, a(t) = u(t) - k \\, x(t) - F_{\\mathrm{fric}}(t). $$\nTo ensure physically consistent non-negative energy dissipation, we represent the friction as a viscous-like term with an effective friction coefficient that depends on the speed magnitude, so that\n$$ F_{\\mathrm{fric}}(t) = c\\big(|v(t)|\\big)\\, v(t), \\quad c\\big(|v(t)|\\big) \\ge 0, $$\nwhich yields an instantaneous power dissipation \n$$ P_{\\mathrm{diss}}(t) = F_{\\mathrm{fric}}(t)\\, v(t) \\ge 0. $$\nWe adopt the following hybrid modeling scheme: the structural model ($m$ and $k$) is known from physics, while the effective friction coefficient $c\\big(|v|\\big)$ is learned from data using a minimal parametric basis that preserves positivity. Specifically, let\n$$ c\\big(|v|\\big) = \\alpha_0 + \\alpha_1 |v|, $$\nwith unknown coefficients $\\alpha_0 \\ge 0$ and $\\alpha_1 \\ge 0$ to be inferred from time-series data and the physics equation. The learning objective is to find $\\alpha_0$ and $\\alpha_1$ that minimize the mismatch between the measured external force $u(t)$ and the force predicted by the hybrid model, under the non-negativity constraints on $\\alpha_0$ and $\\alpha_1$, thereby guaranteeing $P_{\\mathrm{diss}}(t) \\ge 0$. The modeling assumption is scientifically realistic for lubricated contacts in which viscous friction increases with speed.\n\nYour task is to implement a program that:\n- Generates synthetic data $(x(t), v(t), a(t), u(t))$ for specified parameters by exciting the system with a known motion profile, computes the design signals from the physics, and estimates $(\\alpha_0,\\alpha_1)$ using Non-Negative Least Squares (NNLS).\n- Computes the root-mean-square speed \n$$ v_{\\mathrm{rms}} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} v(t_i)^2}, $$\nand reports the scalar effective friction coefficient at this speed,\n$$ c_{\\mathrm{eff}} = \\alpha_0 + \\alpha_1 \\, v_{\\mathrm{rms}}, $$\nexpressed in $\\mathrm{N\\cdot s/m}$.\n- Verifies the positivity of dissipation across the dataset by checking \n$$ P_{\\mathrm{diss}}(t_i) = \\big(\\alpha_0 v(t_i) + \\alpha_1 |v(t_i)| v(t_i)\\big)\\, v(t_i) \\ge 0 $$\nfor all samples $t_i$. The boolean should be true if every sample satisfies the inequality (which is guaranteed to hold true by the problem's construction since NNLS ensures non-negative coefficients), and false otherwise.\n\nThe program should implement the following data generation protocol for each test case:\n- Time samples $t_i = i \\, \\Delta t$ for $i = 0,1,\\dots,N-1$, with specified $N$ and $\\Delta t$ in seconds.\n- Motion profile $x(t) = X_0 \\sin(\\omega t)$, with $X_0$ in meters and angular frequency $\\omega$ in radians per second. Then $v(t) = X_0 \\omega \\cos(\\omega t)$ and $a(t) = -X_0 \\omega^2 \\sin(\\omega t)$.\n- True friction coefficients $(\\alpha_0^{\\mathrm{true}}, \\alpha_1^{\\mathrm{true}})$ produce a friction force $F_{\\mathrm{fric}}^{\\mathrm{true}}(t) = \\alpha_0^{\\mathrm{true}} v(t) + \\alpha_1^{\\mathrm{true}} |v(t)| v(t)$.\n- Measured actuation is $u(t) = m a(t) + k x(t) + F_{\\mathrm{fric}}^{\\mathrm{true}}(t) + \\eta(t)$, with additive zero-mean Gaussian noise $\\eta(t)$ in newtons, generated with a given standard deviation and a fixed random seed per test case for reproducibility.\n\nEstimation protocol:\n- Using the physics equation, rearrange the measurement equation to isolate the friction contribution: \n$$ b(t) = u(t) - m a(t) - k x(t) = \\alpha_0 v(t) + \\alpha_1 |v(t)| v(t). $$\n- Estimate $(\\alpha_0,\\alpha_1)$ from the samples $\\{b(t_i), v(t_i)\\}_{i=1}^N$ by solving the NNLS problem that minimizes the sum of squared residuals subject to $\\alpha_0 \\ge 0$ and $\\alpha_1 \\ge 0$.\n\nUnits and output specification:\n- Mass $m$ in $\\mathrm{kg}$, stiffness $k$ in $\\mathrm{N/m}$, position $x$ in $\\mathrm{m}$, velocity $v$ in $\\mathrm{m/s}$, acceleration $a$ in $\\mathrm{m/s^2}$, force $u$ in $\\mathrm{N}$, friction coefficients $\\alpha_0$ in $\\mathrm{N\\cdot s/m}$ and $\\alpha_1$ in $\\mathrm{N\\cdot s^2/m^2}$, and $c_{\\mathrm{eff}}$ in $\\mathrm{N\\cdot s/m}$. Angles are in radians. No percentages appear in the output.\n- For each test case, the program should output a list $[c_{\\mathrm{eff}}, \\mathrm{flag}]$ where $c_{\\mathrm{eff}}$ is a float in $\\mathrm{N\\cdot s/m}$ and $\\mathrm{flag}$ is a boolean indicating whether $P_{\\mathrm{diss}}(t_i) \\ge 0$ for all samples.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[[c_1,\\mathrm{flag}_1],[c_2,\\mathrm{flag}_2],[c_3,\\mathrm{flag}_3]]$.\n\nTest suite:\n- Case $1$ (happy path): $m = 2.0$, $k = 5.0$, $X_0 = 0.15$, $\\omega = 3.0$, $\\Delta t = 0.01$, $N = 1000$, $\\alpha_0^{\\mathrm{true}} = 0.8$, $\\alpha_1^{\\mathrm{true}} = 0.3$, noise standard deviation $= 0.05$ newtons, random seed $= 42$.\n- Case $2$ (boundary condition: purely viscous): $m = 1.2$, $k = 8.0$, $X_0 = 0.08$, $\\omega = 2.5$, $\\Delta t = 0.01$, $N = 800$, $\\alpha_0^{\\mathrm{true}} = 1.0$, $\\alpha_1^{\\mathrm{true}} = 0.0$, noise standard deviation $= 0.02$ newtons, random seed $= 123$.\n- Case $3$ (edge case: near-zero speeds): $m = 3.5$, $k = 4.0$, $X_0 = 0.02$, $\\omega = 1.5$, $\\Delta t = 0.01$, $N = 600$, $\\alpha_0^{\\mathrm{true}} = 0.5$, $\\alpha_1^{\\mathrm{true}} = 0.1$, noise standard deviation $= 0.01$ newtons, random seed $= 7$.\n\nYour implementation must be deterministic, use the specified seeds, and adhere strictly to the final output format.",
            "solution": "The problem requires the estimation of friction parameters for a single-degree-of-freedom cyber-physical system. The system consists of a physical plant, represented as a point mass $m$ on a linear spring with stiffness $k$, and its digital twin. The solution approach involves a hybrid model that combines a known physics-based structure with a data-driven component for the friction force.\n\nThe governing equation of motion for the mass is derived from Newton's second law:\n$$ m \\, a(t) = u(t) - k \\, x(t) - F_{\\mathrm{fric}}(t) $$\nwhere $x(t)$, $v(t)$, and $a(t)$ are the position, velocity, and acceleration of the mass, respectively. The term $u(t)$ represents an external actuation force, and $F_{\\mathrm{fric}}(t)$ is the friction force.\n\nThe friction is modeled using a hybrid approach. The model structure is physically inspired, ensuring that the instantaneous power dissipated by friction, $P_{\\mathrm{diss}}(t) = F_{\\mathrm{fric}}(t)v(t)$, is always non-negative. This is achieved by defining the friction force as:\n$$ F_{\\mathrm{fric}}(t) = c\\big(|v(t)|\\big)\\, v(t) $$\nwhere the effective friction coefficient $c\\big(|v(t)|\\big)$ is a non-negative function of the speed $|v(t)|$. The data-driven component is the parametric form of this coefficient, which is to be learned from data:\n$$ c\\big(|v(t)|\\big) = \\alpha_0 + \\alpha_1 |v(t)| $$\nThe parameters $\\alpha_0$ and $\\alpha_1$ are unknown and must be estimated. To ensure $P_{\\mathrm{diss}}(t) = (\\alpha_0 + \\alpha_1 |v(t)|)v(t)^2 \\ge 0$, we must enforce the constraints $\\alpha_0 \\ge 0$ and $\\alpha_1 \\ge 0$.\n\nTo estimate $\\alpha_0$ and $\\alpha_1$, we first rearrange the governing equation to isolate the terms containing the unknowns. This establishes a linear relationship between a measurable quantity and the parameters:\n$$ u(t) - m \\, a(t) - k \\, x(t) = \\alpha_0 v(t) + \\alpha_1 |v(t)| v(t) $$\nThis equation forms the basis for a linear regression problem. We define a target variable $b(t) = u(t) - m \\, a(t) - k \\, x(t)$. Given a set of $N$ time-series measurements $(x(t_i), v(t_i), a(t_i), u(t_i))$ for $i=0, \\dots, N-1$, we can construct a system of linear equations:\n$$ \\mathbf{b} \\approx \\mathbf{A} \\boldsymbol{\\alpha} $$\nwhere $\\mathbf{b}$ is a column vector of size $N$ with elements $b(t_i)$, $\\boldsymbol{\\alpha}$ is the column vector of parameters $[\\alpha_0, \\alpha_1]^T$, and $\\mathbf{A}$ is the $N \\times 2$ design matrix whose rows are $[v(t_i), |v(t_i)|v(t_i)]$. The approximation symbol $\\approx$ acknowledges the presence of measurement noise in $u(t)$.\n\nThe estimation task is to find the vector $\\boldsymbol{\\alpha}$ that minimizes the sum of squared residuals, $||\\mathbf{A}\\boldsymbol{\\alpha} - \\mathbf{b}||_2^2$, subject to the physical constraints $\\alpha_0 \\ge 0$ and $\\alpha_1 \\ge 0$. This is a Non-Negative Least Squares (NNLS) problem. It can be solved efficiently using standard optimization routines, such as `scipy.optimize.nnls`.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  **Data Generation**: A synthetic dataset is created.\n    -   A time vector $t$ is generated with $N$ samples and a time step of $\\Delta t$.\n    -   A sinusoidal motion profile $x(t) = X_0 \\sin(\\omega t)$ is prescribed. The velocity $v(t)$ and acceleration $a(t)$ are obtained by analytical differentiation.\n    -   The true friction force $F_{\\mathrm{fric}}^{\\mathrm{true}}(t)$ is calculated using the given true parameters $(\\alpha_0^{\\mathrm{true}}, \\alpha_1^{\\mathrm{true}})$.\n    -   The \"measured\" actuation force $u(t)$ is synthesized by evaluating $m a(t) + k x(t) + F_{\\mathrm{fric}}^{\\mathrm{true}}(t)$ and adding zero-mean Gaussian noise with a specified standard deviation and a fixed random seed for reproducibility.\n\n2.  **Parameter Estimation**:\n    -   The target vector $\\mathbf{b}$ and design matrix $\\mathbf{A}$ are constructed from the generated data $(x, v, a, u)$ and the known system parameters $(m, k)$.\n    -   The NNLS problem is solved to obtain the estimated parameters $\\hat{\\alpha}_0$ and $\\hat{\\alpha}_1$.\n\n3.  **Output Calculation**:\n    -   The root-mean-square speed is calculated over the time series: $v_{\\mathrm{rms}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} v(t_i)^2}$.\n    -   The effective friction coefficient at this speed is computed: $c_{\\mathrm{eff}} = \\hat{\\alpha}_0 + \\hat{\\alpha}_1 v_{\\mathrm{rms}}$.\n    -   The non-negativity of the dissipated power, $P_{\\mathrm{diss}}(t_i) = (\\hat{\\alpha}_0 + \\hat{\\alpha}_1 |v(t_i)|) v(t_i)^2 \\ge 0$, is verified for all time samples. Since NNLS guarantees $\\hat{\\alpha}_0 \\ge 0$ and $\\hat{\\alpha}_1 \\ge 0$, and since $v(t_i)^2 \\ge 0$ and $|v(t_i)| \\ge 0$, this condition is mathematically guaranteed to be satisfied. The boolean flag serves as a confirmation of this theoretical property.\n\n4.  **Formatting**: The final result for each case, a list containing $c_{\\mathrm{eff}}$ and the boolean flag, is formatted into a string. These strings are then aggregated into a final list format as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Solves the hybrid modeling problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path\n        (2.0, 5.0, 0.15, 3.0, 0.01, 1000, 0.8, 0.3, 0.05, 42),\n        # Case 2: boundary condition (purely viscous)\n        (1.2, 8.0, 0.08, 2.5, 0.01, 800, 1.0, 0.0, 0.02, 123),\n        # Case 3: edge case (near-zero speeds)\n        (3.5, 4.0, 0.02, 1.5, 0.01, 600, 0.5, 0.1, 0.01, 7),\n    ]\n\n    results_list = []\n    \n    for case in test_cases:\n        m, k, X0, omega, dt, N, alpha0_true, alpha1_true, noise_std, seed = case\n\n        # 1. Generate synthetic data\n        # Time vector\n        t = np.arange(N) * dt\n        \n        # Motion profile\n        x = X0 * np.sin(omega * t)\n        v = X0 * omega * np.cos(omega * t)\n        a = -X0 * omega**2 * np.sin(omega * t)\n        \n        # True friction force\n        F_fric_true = alpha0_true * v + alpha1_true * np.abs(v) * v\n        \n        # Generate reproducible noise\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(loc=0.0, scale=noise_std, size=N)\n        \n        # Synthesize measured actuation force\n        u = m * a + k * x + F_fric_true + noise\n\n        # 2. Set up and solve the NNLS problem\n        # Target vector b(t) = u(t) - m*a(t) - k*x(t)\n        b = u - m * a - k * x\n        \n        # Design matrix A(t) = [v(t), |v(t)|v(t)]\n        A = np.vstack([v, np.abs(v) * v]).T\n        \n        # Solve for alpha = [alpha0, alpha1] using Non-Negative Least Squares\n        alpha_est, _ = nnls(A, b)\n        alpha0_est, alpha1_est = alpha_est\n\n        # 3. Calculate specified outputs\n        # Root-mean-square speed\n        v_rms = np.sqrt(np.mean(v**2))\n        \n        # Effective friction coefficient at v_rms\n        c_eff = alpha0_est + alpha1_est * v_rms\n        \n        # Verify positivity of dissipated power\n        # P_diss = (alpha0*v + alpha1*|v|*v)*v = alpha0*v^2 + alpha1*|v|*v^2\n        # Since alpha0_est, alpha1_est from NNLS are = 0, and v^2, |v| =0,\n        # P_diss is guaranteed to be non-negative.\n        dissipated_power = (alpha0_est * v + alpha1_est * np.abs(v) * v) * v\n        dissipation_positive_flag = np.all(dissipated_power = 0.0)\n\n        # Append result for the current case\n        results_list.append([c_eff, dissipation_positive_flag])\n\n    # Final print statement in the exact required format.\n    # The format [[c1,flag1],[c2,flag2]] is achieved by printing the\n    # string representation of the list of lists and removing spaces.\n    print(str(results_list).replace(\" \", \"\"))\n\nsolve()\n\n```"
        },
        {
            "introduction": "This exercise explores how to bake physical laws, specifically boundary conditions, into a neural network-based model for a steady-state physical field. We will compare two powerful techniques: architecturally enforcing the conditions (\"hard\" constraints) versus penalizing violations in the training loss (\"soft\" constraints) . This practice highlights the fundamental design choices and trade-offs involved in creating reliable physics-informed models.",
            "id": "4226906",
            "problem": "A Digital Twin (DT) of a one-dimensional heat-conducting bar, a representative Cyber-Physical System (CPS), models the steady-state temperature field $u(x)$ on the spatial domain $x \\in [0,1]$. The field must satisfy Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 1$, along with a physics residual that is not needed for this question. You are comparing two hybrid modeling strategies that combine physics and data via an Artificial Neural Network (ANN) to represent unknown components of the solution.\n\nStrategy A (hard enforcement via constrained architecture): Use a trial solution of the form $u_{\\mathrm{c}}(x;\\theta) = g(x) + s(x)\\,N(x;\\theta)$, where $g(x)$ is a fixed function that satisfies the boundary conditions, $s(x)$ is a fixed function such that $s(0) = 0$ and $s(1) = 0$, and $N(x;\\theta)$ is an ANN with parameters $\\theta$.\n\nStrategy B (soft enforcement via loss penalties): Use an unconstrained ANN $u_{\\mathrm{u}}(x;\\theta)$ and penalize boundary violations in the loss with a boundary penalty\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda \\left[ \\left(u_{\\mathrm{u}}(0;\\theta) - 0\\right)^{2} + \\left(u_{\\mathrm{u}}(1;\\theta) - 1\\right)^{2} \\right],\n$$\nwhere $\\lambda > 0$ is a fixed scalar weight.\n\nFor Strategy A, let $g(x) = x$ and $s(x) = x(1-x)$. For Strategy B, take an explicit unconstrained ANN ansatz $u_{\\mathrm{u}}(x;\\theta) = \\theta_{1}\\tanh(\\theta_{2}x)$ with parameter vector $\\theta = (\\theta_{1},\\theta_{2})$.\n\nTasks:\n- Using fundamental definitions of boundary conditions and the property $s(0)=0$ and $s(1)=0$, explain why Strategy A enforces the boundary conditions for all values of $\\theta$.\n- Starting from the definition of $L_{\\mathrm{bc}}(\\theta)$ and the chain rule for gradients, derive the exact analytic gradient $\\nabla_{\\theta} L_{\\mathrm{bc}}(\\theta)$ for Strategy B with the given ansatz $u_{\\mathrm{u}}(x;\\theta) = \\theta_{1}\\tanh(\\theta_{2}x)$. Express your final answer as a single closed-form row vector in terms of $\\lambda$, $\\theta_{1}$, and $\\theta_{2}$. Do not substitute numerical values.\n- Briefly, based on first principles of constrained optimization and smooth penalty methods, discuss how the optimization landscape differs between Strategies A and B near feasibility.\n\nYour final answer must be the gradient vector for Strategy B in a single closed-form analytic expression. Provide the gradient as a row matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required, and no units are needed.",
            "solution": "The problem as stated is scientifically sound, well-posed, and self-contained. It describes a standard comparison between two common techniques for enforcing boundary conditions in physics-informed machine learning: hard enforcement by architectural construction and soft enforcement via penalty terms in the loss function. All necessary definitions and functions are provided, and the tasks are direct applications of mathematical principles. Therefore, the problem is valid, and a solution can be provided.\n\nThe problem asks for three distinct tasks: an explanation of why Strategy A satisfies the boundary conditions, a derivation of the gradient of the boundary loss for Strategy B, and a brief discussion on the resulting optimization landscapes.\n\nFirst, let us analyze Strategy A, which uses the trial solution $u_{\\mathrm{c}}(x;\\theta) = g(x) + s(x)\\,N(x;\\theta)$. The problem specifies that the function $g(x)$ must satisfy the Dirichlet boundary conditions, and the function $s(x)$ must be zero at the boundaries. The given boundary conditions are $u(0)=0$ and $u(1)=1$.\nFor Strategy A, we are given $g(x)=x$ and $s(x)=x(1-x)$. Let us verify these auxiliary functions meet their requirements.\nFor $g(x)$, we have $g(0)=0$ and $g(1)=1$, so it correctly satisfies the boundary conditions.\nFor $s(x)$, we have $s(0)=0(1-0)=0$ and $s(1)=1(1-1)=0$, so it correctly vanishes at the boundaries.\n\nNow, we evaluate the trial solution $u_{\\mathrm{c}}(x;\\theta)$ at the boundaries $x=0$ and $x=1$:\nAt $x=0$:\n$$u_{\\mathrm{c}}(0;\\theta) = g(0) + s(0)\\,N(0;\\theta)$$\nSubstituting the values $g(0)=0$ and $s(0)=0$:\n$$u_{\\mathrm{c}}(0;\\theta) = 0 + (0) \\cdot N(0;\\theta) = 0$$\nThis result is independent of the value of the neural network output $N(0;\\theta)$ and hence holds for any parameter vector $\\theta$.\n\nAt $x=1$:\n$$u_{\\mathrm{c}}(1;\\theta) = g(1) + s(1)\\,N(1;\\theta)$$\nSubstituting the values $g(1)=1$ and $s(1)=0$:\n$$u_{\\mathrm{c}}(1;\\theta) = 1 + (0) \\cdot N(1;\\theta) = 1$$\nThis result is also independent of the value of $N(1;\\theta)$ and holds for any parameter vector $\\theta$.\nThus, by its very construction, the trial solution $u_{\\mathrm{c}}(x;\\theta)$ in Strategy A rigorously enforces the Dirichlet boundary conditions $u(0)=0$ and $u(1)=1$ for all possible choices of the neural network parameters $\\theta$.\n\nNext, we address Strategy B and derive the gradient of the boundary loss function $L_{\\mathrm{bc}}(\\theta)$. The parameter vector is $\\theta = (\\theta_1, \\theta_2)$. The gradient is a row vector given by $\\nabla_{\\theta} L_{\\mathrm{bc}}(\\theta) = \\begin{pmatrix} \\frac{\\partial L_{\\mathrm{bc}}}{\\partial \\theta_1}  \\frac{\\partial L_{\\mathrm{bc}}}{\\partial \\theta_2} \\end{pmatrix}$.\nThe boundary loss is defined as:\n$$L_{\\mathrm{bc}}(\\theta) = \\lambda \\left[ \\left(u_{\\mathrm{u}}(0;\\theta) - 0\\right)^{2} + \\left(u_{\\mathrm{u}}(1;\\theta) - 1\\right)^{2} \\right]$$\nThe specific ansatz for the unconstrained ANN is $u_{\\mathrm{u}}(x;\\theta) = \\theta_{1}\\tanh(\\theta_{2}x)$.\n\nFirst, let's evaluate the ansatz at the boundaries $x=0$ and $x=1$:\n$$u_{\\mathrm{u}}(0;\\theta) = \\theta_{1}\\tanh(\\theta_{2} \\cdot 0) = \\theta_{1}\\tanh(0) = 0$$\n$$u_{\\mathrm{u}}(1;\\theta) = \\theta_{1}\\tanh(\\theta_{2} \\cdot 1) = \\theta_{1}\\tanh(\\theta_{2})$$\nSubstituting these into the loss function simplifies the expression:\n$$L_{\\mathrm{bc}}(\\theta) = \\lambda \\left[ (0)^{2} + (\\theta_{1}\\tanh(\\theta_{2}) - 1)^{2} \\right] = \\lambda (\\theta_{1}\\tanh(\\theta_{2}) - 1)^{2}$$\n\nNow, we compute the partial derivatives with respect to $\\theta_1$ and $\\theta_2$ using the chain rule.\nFor $\\theta_1$:\n$$\\frac{\\partial L_{\\mathrm{bc}}}{\\partial \\theta_1} = \\lambda \\cdot 2 (\\theta_{1}\\tanh(\\theta_{2}) - 1) \\cdot \\frac{\\partial}{\\partial \\theta_1}(\\theta_{1}\\tanh(\\theta_{2}) - 1)$$\n$$\\frac{\\partial L_{\\mathrm{bc}}}{\\partial \\theta_1} = 2\\lambda (\\theta_{1}\\tanh(\\theta_{2}) - 1) \\cdot \\tanh(\\theta_{2})$$\n\nFor $\\theta_2$:\n$$\\frac{\\partial L_{\\mathrm{bc}}}{\\partial \\theta_2} = \\lambda \\cdot 2 (\\theta_{1}\\tanh(\\theta_{2}) - 1) \\cdot \\frac{\\partial}{\\partial \\theta_2}(\\theta_{1}\\tanh(\\theta_{2}) - 1)$$\nWe need the derivative of $\\tanh(z)$, which is $\\frac{d}{dz}\\tanh(z) = \\frac{1}{\\cosh^2(z)}$. Applying the chain rule:\n$$\\frac{\\partial}{\\partial \\theta_2}(\\theta_{1}\\tanh(\\theta_{2})) = \\theta_1 \\cdot \\frac{1}{\\cosh^2(\\theta_2)}$$\nSubstituting this back into the partial derivative for $L_{\\mathrm{bc}}$:\n$$\\frac{\\partial L_{\\mathrm{bc}}}{\\partial \\theta_2} = 2\\lambda (\\theta_{1}\\tanh(\\theta_{2}) - 1) \\cdot \\frac{\\theta_1}{\\cosh^2(\\theta_2)}$$\n\nCombining these two components, we obtain the gradient vector $\\nabla_{\\theta} L_{\\mathrm{bc}}(\\theta)$:\n$$\\nabla_{\\theta} L_{\\mathrm{bc}}(\\theta) = \\begin{pmatrix} 2\\lambda (\\theta_{1}\\tanh(\\theta_{2}) - 1)\\tanh(\\theta_{2})  2\\lambda (\\theta_{1}\\tanh(\\theta_{2}) - 1)\\frac{\\theta_1}{\\cosh^2(\\theta_2)} \\end{pmatrix}$$\n\nFinally, we briefly discuss the difference in optimization landscapes between the two strategies.\nIn Strategy A, the satisfaction of boundary conditions is a hard constraint, enforced architecturally. The entire parameter space for $\\theta$ maps to a function space where all functions are feasible with respect to the boundary conditions. The optimization algorithm, driven by the physics residual loss (not specified, but assumed to exist), searches within this feasible subspace. The landscape is determined solely by the physics loss, and the optimizer is never directed to correct for boundary violations, as none can occur.\n\nIn contrast, Strategy B employs a soft constraint via a penalty method. The parameter space for $\\theta$ maps to a larger function space, most of which is infeasible (i.e., violates the boundary conditions). The boundary loss $L_{\\mathrm{bc}}(\\theta)$ creates a penalty \"well\" or \"valley\" in the total loss landscape. The minimum of this penalty term (at $L_{\\mathrm{bc}}=0$) defines the feasible region in the parameter space. The total loss landscape is a superposition of the physics loss and this penalty term. For any finite penalty weight $\\lambda$, the global minimum of the total loss will likely be a point where the boundary conditions are only approximately satisfied, representing a compromise between satisfying the physics and satisfying the boundaries. Near the feasible region, the gradient of the penalty term, $\\nabla_{\\theta} L_{\\mathrm{bc}}(\\theta)$, dominates the search direction, pulling the parameters towards boundary satisfaction. This can introduce steep gradients and potential numerical stiffness, making the optimization problem more challenging compared to Strategy A, where the search is intrinsically confined to the feasible set.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2\\lambda (\\theta_1 \\tanh(\\theta_2) - 1) \\tanh(\\theta_2)  2\\lambda (\\theta_1 \\tanh(\\theta_2) - 1) \\frac{\\theta_1}{\\cosh^2(\\theta_2)} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Many real-world systems are \"stiff,\" meaning they have dynamics occurring on vastly different timescales, posing a challenge for numerical simulation. This final practice delves into the crucial issue of ensuring that a hybrid model remains numerically stable when a learned component is added to a stiff physical core . You will derive the conditions under which an implicit numerical method retains its desirable stability properties, linking the physics to requirements on the learned function.",
            "id": "4226985",
            "problem": "A digital twin of a stiff cyber-physical thermal system is modeled by a hybrid ordinary differential equation (ODE) that combines a first-principles linear physics core with a data-driven correction. The state is $x \\in \\mathbb{R}^{n}$ and the dynamics are\n$$\n\\dot{x}(t) \\;=\\; A\\,x(t) \\;+\\; \\alpha\\, g_{\\phi}\\big(x(t)\\big),\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is the physics-based linearized operator and $g_{\\phi}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ is a learned term. The system is stiff in the sense that the symmetric part of $A$, defined as $S \\coloneqq \\frac{1}{2}(A + A^{\\top})$, is negative definite with a known dissipativity margin $\\lambda0$, i.e.,\n$$\nS \\;\\preceq\\; -\\lambda\\, I,\n$$\nand the eigenvalues of $A$ have large negative real parts. The learned term is globally one-sided Lipschitz with constant $m0$ in the Euclidean norm, meaning\n$$\n\\big(g_{\\phi}(x) - g_{\\phi}(y)\\big)^{\\top} (x-y) \\;\\le\\; m\\,\\|x-y\\|^{2} \\quad \\text{for all } x,y\\in\\mathbb{R}^{n}.\n$$\nTo simulate the digital twin with arbitrary sampling periods in a cyber-physical loop, you adopt the fully implicit backward Euler one-step method applied to the full hybrid vector field:\n$$\nx_{k+1} \\;=\\; x_{k} \\;+\\; h\\Big(A\\,x_{k+1} \\;+\\; \\alpha\\, g_{\\phi}(x_{k+1})\\Big),\n$$\nwhere $h0$ is the time step.\n\nStarting from the definitions of stiffness via dissipativity, the symmetric part of a matrix, and the one-sided Lipschitz property, derive the largest scalar $\\alpha_{\\max}$ as a closed-form expression in terms of $\\lambda$ and $m$ such that, for every step size $h0$, the backward Euler update map is strongly monotone in the Euclidean inner product. Conclude that under this bound on $\\alpha$, the implicit integrator is unconditionally stable in the sense of A-stability for the hybrid system’s stiff regime. State your final answer as the single expression for $\\alpha_{\\max}$. Assume $m0$ and $\\lambda0$.",
            "solution": "The user wants me to solve the following problem.\nA digital twin of a stiff cyber-physical thermal system is modeled by a hybrid ordinary differential equation (ODE) that combines a first-principles linear physics core with a data-driven correction. The state is $x \\in \\mathbb{R}^{n}$ and the dynamics are\n$$\n\\dot{x}(t) \\;=\\; A\\,x(t) \\;+\\; \\alpha\\, g_{\\phi}\\big(x(t)\\big),\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is the physics-based linearized operator and $g_{\\phi}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ is a learned term. The system is stiff in the sense that the symmetric part of $A$, defined as $S \\coloneqq \\frac{1}{2}(A + A^{\\top})$, is negative definite with a known dissipativity margin $\\lambda0$, i.e.,\n$$\nS \\;\\preceq\\; -\\lambda\\, I,\n$$\nand the eigenvalues of $A$ have large negative real parts. The learned term is globally one-sided Lipschitz with constant $m0$ in the Euclidean norm, meaning\n$$\n\\big(g_{\\phi}(x) - g_{\\phi}(y)\\big)^{\\top} (x-y) \\;\\le\\; m\\,\\|x-y\\|^{2} \\quad \\text{for all } x,y\\in\\mathbb{R}^{n}.\n$$\nTo simulate the digital twin with arbitrary sampling periods in a cyber-physical loop, you adopt the fully implicit backward Euler one-step method applied to the full hybrid vector field:\n$$\nx_{k+1} \\;=\\; x_{k} \\;+\\; h\\Big(A\\,x_{k+1} \\;+\\; \\alpha\\, g_{\\phi}(x_{k+1})\\Big),\n$$\nwhere $h0$ is the time step.\n\nStarting from the definitions of stiffness via dissipativity, the symmetric part of a matrix, and the one-sided Lipschitz property, derive the largest scalar $\\alpha_{\\max}$ as a closed-form expression in terms of $\\lambda$ and $m$ such that, for every step size $h0$, the backward Euler update map is strongly monotone in the Euclidean inner product. Conclude that under this bound on $\\alpha$, the implicit integrator is unconditionally stable in the sense of A-stability for the hybrid system’s stiff regime. State your final answer as the single expression for $\\alpha_{\\max}$. Assume $m0$ and $\\lambda0$.\n\nThe problem is scientifically and mathematically well-posed, providing all necessary definitions and constraints to derive the requested quantity. The concepts of stiffness, dissipativity, one-sided Lipschitz continuity, and the backward Euler method are standard in the fields of numerical analysis and dynamical systems. The problem is directly relevant to the stability analysis of hybrid models in digital twins. The problem is valid.\n\nThe problem asks for the condition on the scalar $\\alpha$ that ensures a stability-related property for the backward Euler integration scheme, applied to the hybrid ODE $\\dot{x} = F(x)$, where $F(x) = A\\,x + \\alpha\\, g_{\\phi}(x)$. The backward Euler scheme is given by:\n$$\nx_{k+1} = x_{k} + h F(x_{k+1})\n$$\nThis equation implicitly defines the next state $x_{k+1}$ as a function of the current state $x_k$. To analyze the properties of this update, we rearrange the equation to define a resolvent-style operator, $P$, which must be inverted at each step.\n$$\nx_k = x_{k+1} - h F(x_{k+1}) = x_{k+1} - h \\Big( A\\,x_{k+1} + \\alpha\\, g_{\\phi}(x_{k+1}) \\Big)\n$$\nLet us define the operator $P: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ as:\n$$\nP(z) = z - h \\Big( A\\,z + \\alpha\\, g_{\\phi}(z) \\Big)\n$$\nThe backward Euler update is then expressed as solving $P(x_{k+1}) = x_k$ for $x_{k+1}$. The update map is therefore $G(x_k) = P^{-1}(x_k)$.\n\nThe problem phrasing \"the backward Euler update map is strongly monotone\" is best interpreted in the context of numerical stability analysis. The standard approach is to analyze the monotonicity of the operator $P(z)$ that is being inverted. If $P(z)$ is strongly monotone, it is invertible, and its inverse $P^{-1}(z)$ (the update map) is guaranteed to be contractive, which implies numerical stability. An operator $P$ is strongly monotone if there exists a constant $\\beta  0$ such that for any $x, y \\in \\mathbb{R}^n$:\n$$\n\\big(P(x) - P(y)\\big)^{\\top}(x - y) \\ge \\beta \\|x-y\\|^2\n$$\nLet us compute the inner product for our operator $P(z)$. Let $\\Delta z = x - y$.\n$$\nP(x) - P(y) = (x - y) - h \\Big( A(x - y) + \\alpha (g_{\\phi}(x) - g_{\\phi}(y)) \\Big)\n$$\nThen the inner product is:\n$$\n\\big(P(x) - P(y)\\big)^{\\top}(x-y) = \\Big( \\Delta z - h A \\Delta z - h \\alpha (g_{\\phi}(x) - g_{\\phi}(y)) \\Big)^{\\top} \\Delta z\n$$\n$$\n= \\Delta z^{\\top} \\Delta z - h (\\Delta z^{\\top} A \\Delta z) - h \\alpha \\Big( (g_{\\phi}(x) - g_{\\phi}(y))^{\\top} \\Delta z \\Big)\n$$\nWe now use the properties given in the problem statement to establish a lower bound for this expression.\n\nFirst, we analyze the term involving the matrix $A$. For any vector $v \\in \\mathbb{R}^n$, the quadratic form $v^{\\top}Av$ can be related to the symmetric part of $A$, $S = \\frac{1}{2}(A + A^{\\top})$, as follows:\n$$\nv^{\\top}Av = v^{\\top} \\Big( \\frac{A+A^{\\top}}{2} + \\frac{A-A^{\\top}}{2} \\Big) v = v^{\\top}Sv + v^{\\top}\\Big(\\frac{A-A^{\\top}}{2}\\Big)v\n$$\nThe second term is zero because $\\frac{A-A^{\\top}}{2}$ is a skew-symmetric matrix. The problem states that $S \\preceq -\\lambda I$, which by definition means $v^{\\top}Sv \\le -\\lambda \\|v\\|^2$ for any $v$. Applying this to our expression with $v = \\Delta z$:\n$$\n\\Delta z^{\\top} A \\Delta z = \\Delta z^{\\top} S \\Delta z \\le -\\lambda \\|\\Delta z\\|^2\n$$\nTherefore, the term $-h(\\Delta z^{\\top} A \\Delta z)$ has the lower bound:\n$$\n-h(\\Delta z^{\\top} A \\Delta z) \\ge -h(-\\lambda \\|\\Delta z\\|^2) = h\\lambda \\|\\Delta z\\|^2\n$$\nSecond, we use the one-sided Lipschitz property of $g_{\\phi}$:\n$$\n(g_{\\phi}(x) - g_{\\phi}(y))^{\\top} (x-y) \\le m \\|x-y\\|^2\n$$\nWith $\\Delta z = x-y$, this is $(g_{\\phi}(x) - g_{\\phi}(y))^{\\top} \\Delta z \\le m \\|\\Delta z\\|^2$. The problem asks for the largest $\\alpha_{\\max}$, implying we are seeking an upper bound on $\\alpha$. We can assume $\\alpha \\ge 0$. Multiplying the inequality by $-h\\alpha$ (where $h0, \\alpha\\ge0$):\n$$\n-h\\alpha \\Big( (g_{\\phi}(x) - g_{\\phi}(y))^{\\top} \\Delta z \\Big) \\ge -h\\alpha (m \\|\\Delta z\\|^2)\n$$\nSubstituting these two lower bounds back into the expression for the inner product:\n$$\n\\big(P(x) - P(y)\\big)^{\\top}(x-y) \\ge \\|\\Delta z\\|^2 + h\\lambda \\|\\Delta z\\|^2 - h\\alpha m \\|\\Delta z\\|^2\n$$\nFactoring out $\\|\\Delta z\\|^2 = \\|x-y\\|^2$:\n$$\n\\big(P(x) - P(y)\\big)^{\\top}(x-y) \\ge \\Big( 1 + h(\\lambda - \\alpha m) \\Big) \\|x-y\\|^2\n$$\nFor $P(z)$ to be strongly monotone, the coefficient on the right-hand side must be strictly positive. This condition must hold for all step sizes $h  0$:\n$$\n1 + h(\\lambda - \\alpha m)  0 \\quad \\text{for all } h  0\n$$\nLet $C = \\lambda - \\alpha m$. The inequality is $1 + hC  0$.\nIf $C  0$, then for a sufficiently large $h$ (specifically, $h  -1/C$), the term $hC$ will be less than $-1$, and the inequality will fail. Since the property must hold for every $h  0$, we cannot have $C  0$.\nThus, we must have $C \\ge 0$.\n$$\n\\lambda - \\alpha m \\ge 0\n$$\nSince $m  0$ is given, we can solve for $\\alpha$:\n$$\n\\alpha m \\le \\lambda \\implies \\alpha \\le \\frac{\\lambda}{m}\n$$\nThe largest scalar $\\alpha$ for which this condition holds is $\\alpha_{\\max}$. Therefore:\n$$\n\\alpha_{\\max} = \\frac{\\lambda}{m}\n$$\nUnder this condition, $\\alpha \\le \\alpha_{\\max}$, the monotonicity constant for $P(z)$ is $\\beta = 1 + h(\\lambda - \\alpha m)$. Since $\\lambda - \\alpha m \\ge 0$ and $h  0$, we have $\\beta \\ge 1$. If an operator $P$ is strongly monotone with constant $\\beta \\ge 1$, its inverse $P^{-1}$ is a contraction mapping: $\\|P^{-1}(u) - P^{-1}(v)\\| \\le (1/\\beta)\\|u-v\\| \\le \\|u-v\\|$. For the backward Euler scheme, this means $\\|x_{k+1} - y_{k+1}\\| \\le \\|x_k - y_k\\|$, where $x_k$ and $y_k$ are two distinct initial conditions. This contractivity property, holding for any step size $h0$, is a strong form of unconditional stability, consistent with the notion of A-stability for stiff nonlinear systems. The condition $\\alpha \\le \\lambda/m$ ensures the continuous-time dynamics are dissipative, and the A-stability of the backward Euler method preserves this dissipativity discretely for any step size.",
            "answer": "$$\n\\boxed{\\frac{\\lambda}{m}}\n$$"
        }
    ]
}