{
    "hands_on_practices": [
        {
            "introduction": "Before we can build reduced-order models from data, it is crucial to understand the mathematical foundation of how they are constructed. This exercise guides you through the fundamental principles of projection-based model reduction for a linear system. By deriving the Petrov-Galerkin projection from first principles, you will gain a clear understanding of how a high-dimensional system can be approximated in a low-dimensional subspace, forming the theoretical bedrock for many advanced surrogate modeling techniques. ",
            "id": "4249015",
            "problem": "A digital twin of a linear time-invariant (LTI) subsystem is described by the full-order model $ \\dot{x}(t) = A x(t) $, where $ x(t) \\in \\mathbb{R}^{n} $ and $ A \\in \\mathbb{R}^{n \\times n} $. To construct a reduced-order surrogate, the state is approximated in a trial subspace as $ x(t) \\approx V a(t) $, where $ V \\in \\mathbb{R}^{n \\times r} $ has full column rank and $ a(t) \\in \\mathbb{R}^{r} $ are reduced coordinates. Using a Petrov–Galerkin projection with a test basis $ W \\in \\mathbb{R}^{n \\times r} $ of full column rank, start from first principles by enforcing residual orthogonality to derive the reduced dynamical system for $ a(t) $. Explicitly show how the reduced operator can be written in terms of $ A $, $ V $, and $ W $ under a biorthonormality condition between $ V $ and $ W $. Then, specialize your result to the Galerkin case and identify the reduced operator when the columns of $ V $ are orthonormal.\n\nFinally, for the specific data\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & -1\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n1 & -1 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix},\n$$\nverify that the biorthonormality condition holds, construct the reduced operator obtained from the Petrov–Galerkin projection, and compute its trace. Report only the scalar trace as your final answer. No rounding is required, and no units are involved. The final answer must be a single real number.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of linear algebra and model order reduction, is well-posed, and uses objective, formal language. It is self-contained and free of contradictions or ambiguities.\n\nWe begin by deriving the reduced-order model from first principles. The full-order linear time-invariant (LTI) system is given by:\n$$ \\dot{x}(t) = A x(t) $$\nwhere $x(t) \\in \\mathbb{R}^{n}$ is the state vector and $A \\in \\mathbb{R}^{n \\times n}$ is the state matrix.\n\nA reduced-order approximation is sought in a trial subspace spanned by the columns of a matrix $V \\in \\mathbb{R}^{n \\times r}$, where $r \\ll n$. The state is approximated as:\n$$ x(t) \\approx V a(t) $$\nHere, $V$ is the trial basis matrix with full column rank, and $a(t) \\in \\mathbb{R}^{r}$ is the vector of reduced coordinates.\n\nSubstituting this approximation into the full-order model, we obtain a residual, $R(t)$, which represents the error of the approximation in satisfying the original differential equation:\n$$ V \\dot{a}(t) = A (V a(t)) - R(t) $$\n$$ R(t) = A V a(t) - V \\dot{a}(t) $$\nThe Petrov–Galerkin projection method aims to minimize this residual by enforcing that it is orthogonal to a chosen test subspace. The test subspace is spanned by the columns of a test basis matrix $W \\in \\mathbb{R}^{n \\times r}$, which also has full column rank. The orthogonality condition is expressed as:\n$$ W^T R(t) = 0 $$\nSubstituting the expression for the residual $R(t)$ into the orthogonality condition gives:\n$$ W^T (A V a(t) - V \\dot{a}(t)) = 0 $$\nUsing the linearity of the transpose operation, we can distribute $W^T$:\n$$ W^T A V a(t) - W^T V \\dot{a}(t) = 0 $$\nRearranging the terms, we get the reduced dynamical system for the coordinates $a(t)$:\n$$ W^T V \\dot{a}(t) = (W^T A V) a(t) $$\nTo obtain an explicit form for $\\dot{a}(t)$, we must invert the matrix $W^T V \\in \\mathbb{R}^{r \\times r}$. Since both $V$ and $W$ have full column rank, the matrix $W^T V$ is generally invertible for well-chosen bases. The reduced system is then:\n$$ \\dot{a}(t) = (W^T V)^{-1} (W^T A V) a(t) $$\nThis equation is of the form $\\dot{a}(t) = \\hat{A} a(t)$, where the reduced operator is $\\hat{A} = (W^T V)^{-1} (W^T A V)$.\n\nThe problem specifies a biorthonormality condition between the trial basis $V$ and the test basis $W$. This condition is mathematically stated as:\n$$ W^T V = I_r $$\nwhere $I_r$ is the $r \\times r$ identity matrix.\nUnder this condition, the matrix $(W^T V)^{-1}$ becomes $I_r^{-1} = I_r$. The reduced dynamical system simplifies significantly:\n$$ I_r \\dot{a}(t) = (W^T A V) a(t) $$\n$$ \\dot{a}(t) = (W^T A V) a(t) $$\nThus, with biorthonormal bases, the reduced operator is simply $\\hat{A} = W^T A V$.\n\nNext, we specialize this result to the Galerkin case. In a Galerkin projection, the test basis is chosen to be identical to the trial basis, so $W = V$. The general reduced operator becomes:\n$$ \\hat{A}_{\\text{Gal}} = (V^T V)^{-1} (V^T A V) $$\nThe matrix $V^T V$ is the Gramian matrix of the basis vectors in $V$. Since $V$ has full column rank, $V^T V$ is symmetric positive definite and therefore invertible.\n\nIf we further specify that the columns of $V$ are orthonormal, this implies by definition that $V^T V = I_r$. In this situation, the Galerkin projection's reduced operator simplifies to:\n$$ \\hat{A}_{\\text{ortho-Gal}} = (I_r)^{-1} (V^T A V) = V^T A V $$\nThis is consistent with the biorthonormal case where $W=V$ satisfies $W^TV=V^TV = I_r$.\n\nNow, we apply these findings to the specific data provided:\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & -1\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n1 & -1 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\n$$\nFirst, we verify the biorthonormality condition $W^T V = I_2$. The transpose of $W$ is:\n$$ W^T = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\end{pmatrix} $$\nWe compute the product $W^T V$:\n$$ W^T V = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(1) + (0)(0) & (1)(0) + (0)(1) + (0)(1) \\\\ (-1)(1) + (1)(1) + (0)(0) & (-1)(0) + (1)(1) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nThe biorthonormality condition holds.\n\nSince the condition holds, the reduced operator $\\hat{A}$ is given by the simplified Petrov–Galerkin formula $\\hat{A} = W^T A V$. We compute this product in two steps. First, we calculate the product $AV$:\n$$ AV = \\begin{pmatrix} 2 & -1 & 0 \\\\ 0 & 3 & 4 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (-1)(1) + (0)(0) & (2)(0) + (-1)(1) + (0)(1) \\\\ (0)(1) + (3)(1) + (4)(0) & (0)(0) + (3)(1) + (4)(1) \\\\ (0)(1) + (0)(1) + (-1)(0) & (0)(0) + (0)(1) + (-1)(1) \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ 3 & 7 \\\\ 0 & -1 \\end{pmatrix} $$\nNext, we pre-multiply this result by $W^T$:\n$$ \\hat{A} = W^T (AV) = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 3 & 7 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(3) + (0)(0) & (1)(-1) + (0)(7) + (0)(-1) \\\\ (-1)(1) + (1)(3) + (0)(0) & (-1)(-1) + (1)(7) + (0)(-1) \\end{pmatrix} $$\n$$ \\hat{A} = \\begin{pmatrix} 1 & -1 \\\\ 2 & 8 \\end{pmatrix} $$\nThe final task is to compute the trace of the reduced operator $\\hat{A}$. The trace of a square matrix is the sum of its diagonal elements.\n$$ \\mathrm{tr}(\\hat{A}) = \\mathrm{tr} \\begin{pmatrix} 1 & -1 \\\\ 2 & 8 \\end{pmatrix} = 1 + 8 = 9 $$",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "The projection methods derived in the previous exercise rely on a well-chosen low-dimensional basis. This hands-on coding practice introduces Proper Orthogonal Decomposition (POD), a powerful technique for extracting an optimal basis directly from simulation or experimental data. You will implement the POD algorithm using Singular Value Decomposition (SVD) and explore how truncating this basis affects the accuracy of data reconstruction, a key step in building effective data-driven surrogate models. ",
            "id": "4249036",
            "problem": "Consider a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ that collects $m$ state snapshots of a cyber-physical system, arranged as columns, where each state is $n$-dimensional. Proper Orthogonal Decomposition (POD) is used in surrogate and reduced-order modeling to find a low-dimensional subspace that captures the dominant coherent structures in the data. The POD basis can be obtained from the Singular Value Decomposition (SVD) of $X$. The goal is to compute the first $r$ POD modes and the corresponding rank-$r$ reconstruction, then report the relative reconstruction error.\n\nStarting from the fundamental base that any real matrix admits an SVD, your program must:\n\n- Compute the Singular Value Decomposition (SVD) $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times p}$, $V \\in \\mathbb{R}^{m \\times p}$, and $\\Sigma \\in \\mathbb{R}^{p \\times p}$ with $p = \\min(n,m)$, $U$ and $V$ have orthonormal columns, and $\\Sigma$ is diagonal with nonnegative entries (the singular values).\n- Extract the first $r$ POD modes as the first $r$ columns of $U$, denoted $U_r \\in \\mathbb{R}^{n \\times r}$, and the corresponding truncated diagonal matrix $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ whose diagonal entries are the top $r$ singular values, along with $V_r \\in \\mathbb{R}^{m \\times r}$ formed by the first $r$ columns of $V$.\n- Form the rank-$r$ approximation $X_r = U_r \\Sigma_r V_r^\\top$ and compute the relative Frobenius norm error\n$$\ne_r = \\frac{\\|X - X_r\\|_F}{\\|X\\|_F},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n- If $\\|X\\|_F = 0$, define $e_r = 0$.\n\nYour program must implement the above steps for each test case in the following test suite and produce the results as specified. No external input is required; the matrices and values of $r$ are given below.\n\nTest Suite:\n\n- Case $1$ (general non-square matrix, happy path): $X_1 \\in \\mathbb{R}^{4 \\times 3}$ with\n$$\nX_1 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n0 & 1 & 3 \\\\\n4 & -2 & 1 \\\\\n1 & 0 & -1\n\\end{bmatrix},\n$$\n$r_1 = 2$.\n\n- Case $2$ (rank-deficient matrix where columns are linearly dependent): $X_2 \\in \\mathbb{R}^{3 \\times 3}$ with\n$$\nX_2 = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n1 & 2 & 3\n\\end{bmatrix},\n$$\n$r_2 = 1$.\n\n- Case $3$ (boundary case $r=0$): $X_3 \\in \\mathbb{R}^{2 \\times 2}$ with\n$$\nX_3 = \\begin{bmatrix}\n3 & -1 \\\\\n0 & 2\n\\end{bmatrix},\n$$\n$r_3 = 0$.\n\n- Case $4$ (full-rank reconstruction with $r = \\min(n,m)$): $X_4 \\in \\mathbb{R}^{5 \\times 3}$ with\n$$\nX_4 = \\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n2 & -1 & 0 \\\\\n1 & 3 & 1 \\\\\n-2 & 0 & 1\n\\end{bmatrix},\n$$\n$r_4 = 3$.\n\n- Case $5$ (zero matrix edge case): $X_5 \\in \\mathbb{R}^{3 \\times 4}$ with\n$$\nX_5 = \\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix},\n$$\n$r_5 = 2$.\n\n- Case $6$ (ill-conditioned diagonal matrix with decaying scales): $X_6 \\in \\mathbb{R}^{3 \\times 3}$ with\n$$\nX_6 = \\begin{bmatrix}\n10 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0.1\n\\end{bmatrix},\n$$\n$r_6 = 2$.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $e_r$ rounded to $8$ decimal places, in the order of the cases from $1$ to $6$, for example: $[e_1,e_2,e_3,e_4,e_5,e_6]$.\n- Each entry must be a decimal number.",
            "solution": "The problem requires the computation of the relative Frobenius norm error for a rank-$r$ approximation of a given matrix $X$, obtained via Proper Orthogonal Decomposition (POD). This is a fundamental task in reduced-order modeling, where the goal is to capture the most significant features of a high-dimensional system with a low-dimensional representation. The mathematical tool for achieving this is the Singular Value Decomposition (SVD).\n\nA given data matrix $X \\in \\mathbb{R}^{n \\times m}$ aggregates $m$ snapshots of an $n$-dimensional state vector. The SVD of $X$ is a factorization of the form:\n$$\nX = U \\Sigma V^\\top\n$$\nThe problem specifies the use of the \"thin\" or \"economy\" SVD. For a matrix $X$ with dimensions $n \\times m$, let $p = \\min(n, m)$. The components of the thin SVD are:\n- $U \\in \\mathbb{R}^{n \\times p}$: A matrix whose columns are the first $p$ left-singular vectors of $X$. These columns are orthonormal and are known as the POD modes. They form an optimal basis for the data in a least-squares sense.\n- $\\Sigma \\in \\mathbb{R}^{p \\times p}$: A diagonal matrix containing the $p$ singular values of $X$, denoted $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$, in descending order. The magnitude of each singular value $\\sigma_i$ corresponds to the importance of the $i$-th POD mode.\n- $V \\in \\mathbb{R}^{m \\times p}$: A matrix whose columns are the first $p$ right-singular vectors of $X$. These columns are also orthonormal.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of a matrix $X$ in the Frobenius norm (and the spectral norm) is obtained by truncating the SVD. This rank-$r$ approximation, denoted $X_r$, is constructed using the first $r$ singular values and their corresponding singular vectors:\n$$\nX_r = U_r \\Sigma_r V_r^\\top\n$$\nwhere:\n- $U_r \\in \\mathbb{R}^{n \\times r}$ is the matrix containing the first $r$ columns of $U$.\n- $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ is the diagonal matrix containing the first $r$ singular values, $\\sigma_1, \\dots, \\sigma_r$.\n- $V_r \\in \\mathbb{R}^{m \\times r}$ is the matrix containing the first $r$ columns of $V$, so $V_r^\\top \\in \\mathbb{R}^{r \\times m}$.\n\nThe problem requires calculating the relative reconstruction error, defined as:\n$$\ne_r = \\frac{\\|X - X_r\\|_F}{\\|X\\|_F}\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm of a matrix $A \\in \\mathbb{R}^{n \\times m}$, calculated as $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^m A_{ij}^2}$.\n\nA key property of the SVD is its relationship with the Frobenius norm. The squared Frobenius norm of a matrix is equal to the sum of its squared singular values:\n$$\n\\|X\\|_F^2 = \\sum_{i=1}^p \\sigma_i^2\n$$\nThe approximation error matrix, $X - X_r$, has singular values $\\sigma_{r+1}, \\sigma_{r+2}, \\dots, \\sigma_p$. Therefore, the squared Frobenius norm of the error is:\n$$\n\\|X - X_r\\|_F^2 = \\sum_{i=r+1}^p \\sigma_i^2\n$$\nThis provides a highly efficient way to compute the relative error $e_r$ directly from the singular values without explicitly constructing the matrix $X_r$:\n$$\ne_r = \\sqrt{\\frac{\\sum_{i=r+1}^p \\sigma_i^2}{\\sum_{i=1}^p \\sigma_i^2}}\n$$\nThis formula is valid provided that $\\|X\\|_F > 0$. If $X$ is the zero matrix, its norm is $0$ and all its singular values are $0$. In this case, the problem specifies that the error $e_r$ should be defined as $0$.\n\nThe algorithm for each test case $(X, r)$ is as follows:\n$1$. Calculate the Frobenius norm of $X$, $\\|X\\|_F$. If $\\|X\\|_F = 0$, the error $e_r$ is $0$.\n$2$. If $\\|X\\|_F > 0$, compute the singular values of $X$. Let them be the array $s = [\\sigma_1, \\sigma_2, \\dots, \\sigma_p]$.\n$3$. The number of singular values is $p = \\min(n, m)$. The index for slicing corresponds to 0-based programming indices. The sum for the error norm involves singular values from index $r$ to $p-1$.\n$4$. Calculate the numerator, the error norm: $\\|X - X_r\\|_F = \\sqrt{\\sum_{i=r}^{p-1} s_i^2}$.\n$5$. The denominator, $\\|X\\|_F$, is already computed. It can also be calculated as $\\sqrt{\\sum_{i=0}^{p-1} s_i^2}$.\n$6$. Compute the relative error $e_r = \\|X - X_r\\|_F / \\|X\\|_F$.\n\nThis procedure is applied to each test case:\n- For Case $1$ ($r_1=2$), the error will be determined by the smallest singular value, $\\sigma_3$.\n- For Case $2$ (rank-deficient), the matrix rank is $1$. Thus, only one singular value, $\\sigma_1$, will be non-zero. For $r_2=1$, we keep the entire non-zero part of the spectrum, so the error $\\|X - X_1\\|_F$ should be $0$, yielding $e_1=0$.\n- For Case $3$ ($r_3=0$), the approximation $X_0$ is the zero matrix. The error is $\\|X - \\mathbf{0}\\|_F = \\|X\\|_F$. The relative error is $e_0 = \\|X\\|_F / \\|X\\|_F = 1$, since $X_3$ is not the zero matrix.\n- For Case $4$ ($r_4=3$), we have $p = \\min(5, 3) = 3$. Since $r_4=p$, we are performing a full reconstruction. Thus, $X_3 = X$, and the error $e_3$ will be $0$ (within machine precision).\n- For Case $5$ ($X_5=\\mathbf{0}$), the matrix norm is $0$. By the problem's definition, the error $e_2$ is $0$.\n- For Case $6$ (diagonal matrix), the singular values are the absolute values of the diagonal entries: $10, 1, 0.1$. For $r_6=2$, the error is determined by the smallest singular value, $\\sigma_3=0.1$. The relative error will be $\\sigma_3 / \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative Frobenius norm error for rank-r POD approximations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [2., -1., 0.],\n            [0., 1., 3.],\n            [4., -2., 1.],\n            [1., 0., -1.]\n        ]), 2),\n        (np.array([\n            [1., 2., 3.],\n            [2., 4., 6.],\n            [1., 2., 3.]\n        ]), 1),\n        (np.array([\n            [3., -1.],\n            [0., 2.]\n        ]), 0),\n        (np.array([\n            [1., 0., 2.],\n            [0., 1., -1.],\n            [2., -1., 0.],\n            [1., 3., 1.],\n            [-2., 0., 1.]\n        ]), 3),\n        (np.array([\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]\n        ]), 2),\n        (np.array([\n            [10., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 0.1]\n        ]), 2)\n    ]\n\n    results = []\n    for X, r in test_cases:\n        # Calculate the Frobenius norm of the original matrix X.\n        norm_X = np.linalg.norm(X, 'fro')\n\n        # Handle the special case where the matrix is the zero matrix.\n        # If norm_X is 0, the relative error is defined as 0.\n        if np.isclose(norm_X, 0.0):\n            relative_error = 0.0\n            results.append(relative_error)\n            continue\n\n        # Compute the singular values of X.\n        # We only need the singular values, so we don't compute U and Vh for efficiency.\n        # singular values are returned in descending order.\n        s = np.linalg.svd(X, compute_uv=False)\n        \n        # The squared Frobenius norm of the error matrix (X - X_r) is the sum\n        # of the squares of the truncated singular values (from index r onwards).\n        # s[r:] gives all singular values from index r to the end.\n        squared_error_norm = np.sum(s[r:]**2)\n        \n        # The error norm is the square root of this sum.\n        error_norm = np.sqrt(squared_error_norm)\n        \n        # The relative error is the ratio of the error norm to the original matrix norm.\n        relative_error = error_norm / norm_X\n        \n        results.append(relative_error)\n\n    # Format the results to 8 decimal places and print in the specified format.\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building a surrogate model involves a critical trade-off: a model that is too simple may be inaccurate, while one that is too complex may overfit the training data and fail to generalize. This comprehensive exercise integrates the concepts of basis construction and model reduction into a realistic workflow for model selection. You will use held-out validation data to select the optimal order for a reduced-order model, while also enforcing the critical real-world constraint of dynamic stability. ",
            "id": "4249030",
            "problem": "You are given a discrete-time linear time-invariant state-space model intended to represent a component within a Digital Twin for Cyber-Physical Systems (CPS). The true plant evolves according to the equations\n$$\nx_{k+1} = A x_k + B u_k + w_k, \\quad y_k = C x_k + v_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state, $u_k \\in \\mathbb{R}^m$ is the control input, $y_k \\in \\mathbb{R}^n$ is the measured output, $A \\in \\mathbb{R}^{n \\times n}$ is the system matrix, $B \\in \\mathbb{R}^{n \\times m}$ is the input matrix, $C \\in \\mathbb{R}^{n \\times n}$ is the output matrix, $w_k \\in \\mathbb{R}^n$ is process noise, and $v_k \\in \\mathbb{R}^n$ is measurement noise. In this problem, assume $C = I_n$, $w_k = 0$, and that $A$ is stable in the discrete-time sense. The control inputs $u_k$ are independently and identically distributed Gaussian random vectors with zero mean and variance $\\sigma_u^2$ in each component, and the measurement noise $v_k$ is independently and identically distributed Gaussian noise with zero mean and specified standard deviation.\n\nA surrogate reduced-order model is constructed by projecting the dynamics onto a subspace computed from training data via Proper Orthogonal Decomposition derived from Singular Value Decomposition (SVD). Let the training outputs be stacked into the snapshot matrix\n$$\nX_{\\text{train}} = \\begin{bmatrix} y_0 & y_1 & \\cdots & y_T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (T+1)}.\n$$\nCompute the SVD $X_{\\text{train}} = U \\Sigma V^\\top$ and form the reduced basis $U_r \\in \\mathbb{R}^{n \\times r}$ by taking the first $r$ columns of $U$. Define the reduced coordinates $z_k = U_r^\\top y_k \\in \\mathbb{R}^r$. Fit the reduced-order linear model with an affine term from the training data by solving the least-squares problem\n$$\n\\min_{A_r, B_r, c_r} \\sum_{k=0}^{T-1} \\left\\| z_{k+1} - A_r z_k - B_r u_k - c_r \\right\\|_2^2,\n$$\nwhere $A_r \\in \\mathbb{R}^{r \\times r}$, $B_r \\in \\mathbb{R}^{r \\times m}$, and $c_r \\in \\mathbb{R}^r$.\n\nTo ensure scientific realism, enforce a stability constraint on the reduced surrogate dynamics, requiring that the spectral radius of $A_r$, denoted by $\\rho(A_r)$, satisfies\n$$\n\\rho(A_r) < 1 - \\varepsilon,\n$$\nfor a small margin $\\varepsilon > 0$. For each candidate order $r$ that satisfies the stability constraint, perform held-out validation by simulating the reduced surrogate on the validation inputs and initial reduced state $z_0 = U_r^\\top y_0^{\\text{val}}$:\n$$\nz_{k+1} = A_r z_k + B_r u_k^{\\text{val}} + c_r, \\quad \\hat{y}_k = U_r z_k,\n$$\nand compute the Mean Squared Error (MSE) over the validation horizon,\n$$\nE_r = \\frac{1}{N_{\\text{val}} \\cdot n} \\sum_{k=0}^{N_{\\text{val}}-1} \\left\\| \\hat{y}_k - y_k^{\\text{val}} \\right\\|_2^2,\n$$\nwhere $N_{\\text{val}}$ is the number of validation time steps and $n$ is the full state dimension. Select the order $r^\\star$ that minimizes the held-out loss $E_r$ subject to the stability constraint. In case of ties within a small numerical tolerance, choose the smallest $r$ among the minimizers. If no candidate order yields a stable reduced dynamics, return $-1$ for that test case.\n\nImplement the above procedure and apply it to the following test suite. For each test case, generate a stable matrix $A$ and an input matrix $B$ of compatible dimensions as follows: draw a random matrix $M \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries, compute its spectral radius $\\rho(M)$, and set\n$$\nA = \\alpha M, \\quad \\alpha = \\frac{0.9}{\\rho(M)},\n$$\nto ensure $\\rho(A) \\leq 0.9$. Draw $B$ with independent normal entries with standard deviation $0.5$. Use $x_0 = 0_n$ and simulate the system for $N_{\\text{train}} + N_{\\text{val}}$ steps. For all cases, use $\\sigma_u = 0.5$ and $w_k = 0$. The measurement noise standard deviation is specified per test case. The random number generator must be seeded with the given seed value for reproducibility.\n\nTest suite specifications:\n- Case $1$: $n = 5$, $m = 2$, $N_{\\text{train}} = 200$, $N_{\\text{val}} = 100$, measurement noise standard deviation $0.01$, seed $2025$, candidate orders $\\{1,2,3,4,5\\}$.\n- Case $2$: $n = 5$, $m = 2$, $N_{\\text{train}} = 60$, $N_{\\text{val}} = 150$, measurement noise standard deviation $0.05$, seed $7$, candidate orders $\\{1,2,3,4\\}$.\n- Case $3$: $n = 5$, $m = 2$, $N_{\\text{train}} = 120$, $N_{\\text{val}} = 200$, measurement noise standard deviation $0.02$, seed $123$, candidate orders $\\{1,2,3,4,5\\}$.\n- Case $4$: $n = 5$, $m = 2$, $N_{\\text{train}} = 80$, $N_{\\text{val}} = 50$, measurement noise standard deviation $0.2$, seed $999$, candidate orders $\\{1,2,3\\}$.\n\nUse $\\varepsilon = 10^{-6}$ for the stability margin. For each case, compute the selected order $r^\\star$ according to the rule described. The final program output must be a single line containing the selected orders for the four cases as a comma-separated list enclosed in square brackets, for example, $[r_1^\\star,r_2^\\star,r_3^\\star,r_4^\\star]$. Angles are not involved and no physical units are required in the output. All numerical values should be computed in floating point where appropriate, and the program must produce the output in the exact specified format.",
            "solution": "The problem requires selecting the surrogate reduced-order model dimension by minimizing held-out loss while enforcing stability of the reduced dynamics. The approach is grounded in standard discrete-time linear system theory and projection-based model reduction.\n\nBegin with the discrete-time linear time-invariant model\n$$\nx_{k+1} = A x_k + B u_k + w_k, \\quad y_k = C x_k + v_k,\n$$\nwith the assumptions $C = I_n$, $w_k = 0$, and a stable $A$. Discrete-time stability is characterized by the spectral radius $\\rho(A)$, defined by\n$$\n\\rho(A) = \\max_i \\left| \\lambda_i(A) \\right|,\n$$\nwhere $\\lambda_i(A)$ are the eigenvalues of $A$. A matrix $A$ is stable if $\\rho(A) < 1$. To ensure the ground-truth system is stable, we construct $A$ by scaling a random matrix $M$:\n$$\nA = \\alpha M, \\quad \\alpha = \\frac{0.9}{\\rho(M)}.\n$$\nThis guarantees $\\rho(A) \\leq 0.9$, and thus stability.\n\nThe surrogate reduced-order model is built via Proper Orthogonal Decomposition using Singular Value Decomposition (SVD). From the training outputs,\n$$\nX_{\\text{train}} = \\begin{bmatrix} y_0 & y_1 & \\cdots & y_T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (T+1)},\n$$\ncompute the SVD\n$$\nX_{\\text{train}} = U \\Sigma V^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ has orthonormal columns, and $\\Sigma$ contains singular values. The reduced basis for order $r$ is $U_r \\in \\mathbb{R}^{n \\times r}$ formed by the first $r$ columns of $U$. The reduced coordinates are defined by\n$$\nz_k = U_r^\\top y_k \\in \\mathbb{R}^r.\n$$\n\nTo estimate reduced dynamics, we posit an affine linear model in the reduced coordinates,\n$$\nz_{k+1} = A_r z_k + B_r u_k + c_r,\n$$\nand fit $A_r \\in \\mathbb{R}^{r \\times r}$, $B_r \\in \\mathbb{R}^{r \\times m}$, and $c_r \\in \\mathbb{R}^r$ by minimizing the sum of squared prediction errors over the training sequence,\n$$\n\\min_{A_r, B_r, c_r} \\sum_{k=0}^{T-1} \\left\\| z_{k+1} - A_r z_k - B_r u_k - c_r \\right\\|_2^2.\n$$\nThis least-squares problem can be cast in matrix form. Let $L = T$ be the number of transitions, and define the matrices\n$$\nZ_0 = \\begin{bmatrix} z_0 & z_1 & \\cdots & z_{L-1} \\end{bmatrix} \\in \\mathbb{R}^{r \\times L}, \\quad Z_1 = \\begin{bmatrix} z_1 & z_2 & \\cdots & z_{L} \\end{bmatrix} \\in \\mathbb{R}^{r \\times L},\n$$\n$$\nU_0 = \\begin{bmatrix} u_0 & u_1 & \\cdots & u_{L-1} \\end{bmatrix} \\in \\mathbb{R}^{m \\times L}, \\quad \\mathbf{1}_L = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times L}.\n$$\nConstruct the regressor\n$$\n\\Phi = \\begin{bmatrix} Z_0 \\\\ U_0 \\\\ \\mathbf{1}_L \\end{bmatrix} \\in \\mathbb{R}^{(r + m + 1) \\times L}.\n$$\nThe least-squares solution minimizing the Frobenius norm of residuals is obtained via the right pseudoinverse,\n$$\n\\Theta^\\star = Z_1 \\, \\Phi^\\dagger \\in \\mathbb{R}^{r \\times (r + m + 1)},\n$$\nwhere $\\Theta^\\star$ is partitioned as\n$$\n\\Theta^\\star = \\begin{bmatrix} A_r & B_r & c_r \\end{bmatrix}.\n$$\n\nWe must enforce stability in the reduced dynamics. The discrete-time stability condition for the reduced state-transition matrix is\n$$\n\\rho(A_r) < 1 - \\varepsilon,\n$$\nwith a small numerical margin $\\varepsilon > 0$. Orders $r$ that do not satisfy this condition are excluded from consideration.\n\nFor held-out validation, simulate the reduced model on the validation inputs and initial reduced state $z_0^{\\text{val}} = U_r^\\top y_0^{\\text{val}}$:\n$$\nz_{k+1}^{\\text{val}} = A_r z_k^{\\text{val}} + B_r u_k^{\\text{val}} + c_r,\n$$\nthen reconstruct the outputs\n$$\n\\hat{y}_k = U_r z_k^{\\text{val}}.\n$$\nCompute the Mean Squared Error (MSE) across the validation horizon,\n$$\nE_r = \\frac{1}{N_{\\text{val}} \\cdot n} \\sum_{k=0}^{N_{\\text{val}}-1} \\left\\| \\hat{y}_k - y_k^{\\text{val}} \\right\\|_2^2.\n$$\nThe selected order is\n$$\nr^\\star = \\arg\\min_{r \\in \\mathcal{R}} E_r \\quad \\text{subject to} \\quad \\rho(A_r) < 1 - \\varepsilon,\n$$\nwhere $\\mathcal{R}$ is the candidate order set. If there is a tie in $E_r$ within numerical tolerance, select the smallest $r$ to favor parsimony. If no candidate order yields a stable reduced dynamics, return $-1$.\n\nAlgorithmic implementation details:\n- Use a fixed random number generator seed per test case for reproducibility.\n- Generate $A$ by scaling a random matrix $M$ using $\\alpha = 0.9 / \\rho(M)$ to ensure $\\rho(A) \\leq 0.9$.\n- Draw $B$ with independent normal entries with standard deviation $0.5$.\n- Simulate the system for $N_{\\text{train}} + N_{\\text{val}}$ steps from $x_0 = 0_n$, with $u_k \\sim \\mathcal{N}(0, \\sigma_u^2 I_m)$ and measurement noise $v_k \\sim \\mathcal{N}(0, \\sigma_v^2 I_n)$, where $\\sigma_u = 0.5$ and $\\sigma_v$ is specified per test case.\n- Build $U_r$ using SVD of $X_{\\text{train}}$, then project to reduced coordinates and fit $(A_r, B_r, c_r)$ by least squares.\n- Enforce the stability constraint via spectral radius check on $A_r$.\n- Perform validation rollout and compute $E_r$.\n- Select $r^\\star$ based on minimal $E_r$ among stable candidates, with tie-breaking on $r$.\n\nTest suite definition:\n- Case $1$: $n = 5$, $m = 2$, $N_{\\text{train}} = 200$, $N_{\\text{val}} = 100$, $\\sigma_v = 0.01$, seed $2025$, $\\mathcal{R} = \\{1,2,3,4,5\\}$.\n- Case $2$: $n = 5$, $m = 2$, $N_{\\text{train}} = 60$, $N_{\\text{val}} = 150$, $\\sigma_v = 0.05$, seed $7$, $\\mathcal{R} = \\{1,2,3,4\\}$.\n- Case $3$: $n = 5$, $m = 2$, $N_{\\text{train}} = 120$, $N_{\\text{val}} = 200$, $\\sigma_v = 0.02$, seed $123$, $\\mathcal{R} = \\{1,2,3,4,5\\}$.\n- Case $4$: $n = 5$, $m = 2$, $N_{\\text{train}} = 80$, $N_{\\text{val}} = 50$, $\\sigma_v = 0.2$, seed $999$, $\\mathcal{R} = \\{1,2,3\\}$.\n\nUse $\\varepsilon = 10^{-6}$. The program must output a single line containing the selected orders for the four cases in the form $[r_1^\\star,r_2^\\star,r_3^\\star,r_4^\\star]$. This format ensures that the solution is quantifiable and testable, with each element being an integer result per test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_stable_system(n, m, rng):\n    \"\"\"\n    Generate a stable discrete-time linear system (A, B).\n    A is constructed by scaling a random matrix M so that its spectral radius is <= 0.9.\n    B is drawn with normal entries with std 0.5.\n    \"\"\"\n    M = rng.normal(size=(n, n))\n    # Compute spectral radius of M\n    eigvals = np.linalg.eigvals(M)\n    rho_M = np.max(np.abs(eigvals))\n    # Scale to ensure stability: spectral radius <= 0.9\n    alpha = 0.9 / rho_M if rho_M > 0 else 0.0\n    A = alpha * M\n    B = rng.normal(scale=0.5, size=(n, m))\n    return A, B\n\ndef simulate_system(A, B, u, noise_std, rng):\n    \"\"\"\n    Simulate the system x_{k+1} = A x_k + B u_k, y_k = x_k + v_k.\n    Inputs:\n        A: (n,n) system matrix\n        B: (n,m) input matrix\n        u: (T, m) input sequence\n        noise_std: float, standard deviation of measurement noise v_k\n    Returns:\n        y: (T, n) measured outputs\n        x: (T, n) true states\n    \"\"\"\n    n, m = B.shape\n    T = u.shape[0]\n    x = np.zeros((T, n))\n    y = np.zeros((T, n))\n    xk = np.zeros(n)\n    for k in range(T):\n        # Output with measurement noise\n        vk = rng.normal(scale=noise_std, size=n) if noise_std > 0 else np.zeros(n)\n        y[k] = xk + vk\n        # State update\n        xkp1 = A @ xk + B @ u[k]\n        x[k] = xk\n        xk = xkp1\n    return y, x\n\ndef build_pod_basis(y_train, r):\n    \"\"\"\n    Build POD basis U_r using SVD of training data snapshots.\n    y_train: (N_train, n)\n    r: reduced order\n    Returns:\n        U_r: (n, r)\n    \"\"\"\n    # Snapshots as (n, T+1) matrix; here use all y_train columns\n    X = y_train.T  # shape (n, N_train)\n    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n    U_r = U[:, :r]\n    return U_r\n\ndef estimate_reduced_dynamics(U_r, y_train, u_train):\n    \"\"\"\n    Estimate (A_r, B_r, c_r) via least squares in reduced coordinates.\n    y_train: (N_train, n)\n    u_train: (N_train, m)\n    Returns:\n        A_r: (r, r)\n        B_r: (r, m)\n        c_r: (r,)\n    \"\"\"\n    # Project training outputs to reduced coordinates\n    Z = (U_r.T @ y_train.T)  # shape (r, N_train)\n    # Use transitions from k=0..N_train-2 to k+1\n    L = Z.shape[1] - 1\n    Z0 = Z[:, :L]            # (r, L)\n    Z1 = Z[:, 1:L+1]         # (r, L)\n    U0 = u_train[:L, :].T    # (m, L)\n    ones = np.ones((1, L))   # (1, L)\n    Phi = np.vstack([Z0, U0, ones])  # (r+m+1, L)\n    # Least squares via pseudoinverse\n    Theta = Z1 @ np.linalg.pinv(Phi)  # (r, r+m+1)\n    r = U_r.shape[1]\n    m = u_train.shape[1]\n    A_r = Theta[:, :r]\n    B_r = Theta[:, r:r+m]\n    c_r = Theta[:, -1]\n    return A_r, B_r, c_r\n\ndef spectral_radius(A):\n    \"\"\"\n    Compute spectral radius of matrix A.\n    \"\"\"\n    eigvals = np.linalg.eigvals(A)\n    return np.max(np.abs(eigvals))\n\ndef validate_model(U_r, A_r, B_r, c_r, y_val, u_val):\n    \"\"\"\n    Validate reduced model by rollout on validation data and compute MSE.\n    y_val: (N_val, n)\n    u_val: (N_val, m)\n    Returns:\n        mse: float\n    \"\"\"\n    n = U_r.shape[0]\n    r = U_r.shape[1]\n    N_val = y_val.shape[0]\n    # Initialize reduced state from first validation measurement\n    z = np.zeros((N_val, r))\n    z[0] = U_r.T @ y_val[0]\n    # Rollout\n    for k in range(N_val - 1):\n        z[k+1] = A_r @ z[k] + B_r @ u_val[k] + c_r\n    # Reconstruct outputs\n    y_hat = (U_r @ z.T).T  # (N_val, n)\n    # Compute mean squared error across all times and dimensions\n    err = y_hat - y_val\n    mse = np.mean(err**2)\n    return mse\n\ndef select_order(orders, A, B, u_train, y_train, u_val, y_val, epsilon):\n    \"\"\"\n    Cross-validate across candidate orders, enforcing stability.\n    Returns:\n        selected_r: int (best order), or -1 if no stable candidate\n    \"\"\"\n    best_r = None\n    best_mse = np.inf\n    mse_tie_tol = 1e-12\n    n = A.shape[0]\n    for r in orders:\n        if r <= 0 or r > n:\n            continue\n        # Build POD basis\n        U_r = build_pod_basis(y_train, r)\n        # Estimate reduced dynamics\n        A_r, B_r, c_r = estimate_reduced_dynamics(U_r, y_train, u_train)\n        # Stability check\n        rho = spectral_radius(A_r)\n        if not (rho < 1.0 - epsilon):\n            continue\n        # Validate\n        mse = validate_model(U_r, A_r, B_r, c_r, y_val, u_val)\n        # Select by minimal MSE, tie-break by smaller r\n        if (mse + mse_tie_tol) < best_mse or (abs(mse - best_mse) <= mse_tie_tol and (best_r is None or r < best_r)):\n            best_mse = mse\n            best_r = r\n    return best_r if best_r is not None else -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (n, m, N_train, N_val, noise_std, seed, orders)\n    test_cases = [\n        (5, 2, 200, 100, 0.01, 2025, [1, 2, 3, 4, 5]),\n        (5, 2, 60, 150, 0.05, 7,    [1, 2, 3, 4]),\n        (5, 2, 120, 200, 0.02, 123, [1, 2, 3, 4, 5]),\n        (5, 2, 80, 50,  0.2,  999,  [1, 2, 3]),\n    ]\n    epsilon = 1e-6\n    sigma_u = 0.5\n\n    results = []\n    for case in test_cases:\n        n, m, N_train, N_val, noise_std, seed, orders = case\n        rng = np.random.default_rng(seed)\n        # Generate system\n        A, B = generate_stable_system(n, m, rng)\n        # Generate inputs for full horizon\n        T_total = N_train + N_val\n        u = rng.normal(scale=sigma_u, size=(T_total, m))\n        # Simulate system\n        y, x = simulate_system(A, B, u, noise_std, rng)\n        # Split into train and validation\n        y_train = y[:N_train]\n        y_val = y[N_train:N_train+N_val]\n        u_train = u[:N_train]\n        u_val = u[N_train:N_train+N_val]\n        # Select order\n        selected_r = select_order(orders, A, B, u_train, y_train, u_val, y_val, epsilon)\n        results.append(selected_r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}