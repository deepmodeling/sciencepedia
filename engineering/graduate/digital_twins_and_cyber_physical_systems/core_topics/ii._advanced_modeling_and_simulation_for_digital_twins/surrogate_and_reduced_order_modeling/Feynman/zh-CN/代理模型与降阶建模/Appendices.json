{
    "hands_on_practices": [
        {
            "introduction": "我们从核心的数学基础开始。本练习将引导你通过第一性原理，推导基于投影方法的降阶模型。通过该练习，你将阐明试验基和测试基在彼得罗夫-伽辽金（Petrov-Galerkin）方法中的作用，及其如何特殊化为常见的伽辽金（Galerkin）方法。掌握这一推导对于理解高维动力学系统如何被压缩成计算上易于处理的形式至关重要。",
            "id": "4249015",
            "problem": "一个线性时不变(LTI)子系统的数字孪生由全阶模型 $ \\dot{x}(t) = A x(t) $ 描述，其中 $ x(t) \\in \\mathbb{R}^{n} $ 且 $ A \\in \\mathbb{R}^{n \\times n} $。为了构建一个降阶代理模型，状态在试探子空间中被近似为 $ x(t) \\approx V a(t) $，其中 $ V \\in \\mathbb{R}^{n \\times r} $ 是满列秩矩阵，$ a(t) \\in \\mathbb{R}^{r} $ 是降阶坐标。使用一个测试基为满列秩矩阵 $ W \\in \\mathbb{R}^{n \\times r} $ 的 Petrov–Galerkin 投影，从第一性原理出发，通过强制残差正交性来推导 $ a(t) $ 的降阶动力系统。明确展示在 $ V $ 和 $ W $ 之间满足双正交条件下，降阶算子如何用 $ A $、$ V $ 和 $ W $ 表示。然后，将您的结果特化到 Galerkin 情况，并确定当 $ V $ 的列是标准正交时降阶算子是什么。\n\n最后，对于给定的数据\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n0  3  4 \\\\\n0  0  -1\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  1\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n1  -1 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix},\n$$\n验证双正交条件成立，构建通过 Petrov–Galerkin 投影得到的降阶算子，并计算其迹。只需报告标量迹作为最终答案。无需四舍五入，不涉及单位。最终答案必须是单个实数。",
            "solution": "该问题是有效的，因为它在科学上基于线性代数和模型降阶的原理，是适定的，并使用了客观、正式的语言。它是自洽的，没有矛盾或歧义。\n\n我们从第一性原理开始推导降阶模型。全阶线性时不变 (LTI) 系统由下式给出：\n$$ \\dot{x}(t) = A x(t) $$\n其中 $x(t) \\in \\mathbb{R}^{n}$ 是状态向量，$A \\in \\mathbb{R}^{n \\times n}$ 是状态矩阵。\n\n在一个由矩阵 $V \\in \\mathbb{R}^{n \\times r}$（其中 $r \\ll n$）的列向量张成的试探子空间中寻求一个降阶近似。状态被近似为：\n$$ x(t) \\approx V a(t) $$\n这里，$V$ 是具有满列秩的试探基矩阵，$a(t) \\in \\mathbb{R}^{r}$ 是降阶坐标向量。\n\n将此近似代入全阶模型，我们得到一个残差 $R(t)$，它表示该近似在满足原微分方程时的误差：\n$$ V \\dot{a}(t) = A (V a(t)) - R(t) $$\n$$ R(t) = A V a(t) - V \\dot{a}(t) $$\nPetrov–Galerkin 投影方法旨在通过强制残差与一个选定的测试子空间正交来最小化该残差。测试子空间由一个测试基矩阵 $W \\in \\mathbb{R}^{n \\times r}$ 的列向量张成，该矩阵也具有满列秩。正交性条件表示为：\n$$ W^T R(t) = 0 $$\n将残差 $R(t)$ 的表达式代入正交性条件，得到：\n$$ W^T (A V a(t) - V \\dot{a}(t)) = 0 $$\n利用转置运算的线性性质，我们可以分配 $W^T$：\n$$ W^T A V a(t) - W^T V \\dot{a}(t) = 0 $$\n重新排列各项，我们得到坐标 $a(t)$ 的降阶动力系统：\n$$ W^T V \\dot{a}(t) = (W^T A V) a(t) $$\n为了得到 $\\dot{a}(t)$ 的显式形式，我们必须对矩阵 $W^T V \\in \\mathbb{R}^{r \\times r}$ 求逆。由于 $V$ 和 $W$ 都是满列秩的，对于恰当选择的基，$W^T V$ 矩阵通常是可逆的。降阶系统则为：\n$$ \\dot{a}(t) = (W^T V)^{-1} (W^T A V) a(t) $$\n该方程的形式为 $\\dot{a}(t) = \\hat{A} a(t)$，其中降阶算子是 $\\hat{A} = (W^T V)^{-1} (W^T A V)$。\n\n题目指定了试探基 $V$ 和测试基 $W$ 之间的双正交条件。该条件在数学上表述为：\n$$ W^T V = I_r $$\n其中 $I_r$ 是 $r \\times r$ 的单位矩阵。\n在此条件下，矩阵 $(W^T V)^{-1}$ 变为 $I_r^{-1} = I_r$。降阶动力系统显著简化为：\n$$ I_r \\dot{a}(t) = (W^T A V) a(t) $$\n$$ \\dot{a}(t) = (W^T A V) a(t) $$\n因此，对于双正交基，降阶算子就是 $\\hat{A} = W^T A V$。\n\n接下来，我们将此结果特化到 Galerkin 情况。在 Galerkin 投影中，选择的测试基与试探基相同，即 $W = V$。一般的降阶算子变为：\n$$ \\hat{A}_{\\text{Gal}} = (V^T V)^{-1} (V^T A V) $$\n矩阵 $V^T V$ 是 $V$ 中基向量的格拉姆矩阵。由于 $V$ 是满列秩的，$V^T V$ 是对称正定矩阵，因此是可逆的。\n\n如果我们进一步指定 $V$ 的列是标准正交的，根据定义，这意味着 $V^T V = I_r$。在这种情况下，Galerkin 投影的降阶算子简化为：\n$$ \\hat{A}_{\\text{ortho-Gal}} = (I_r)^{-1} (V^T A V) = V^T A V $$\n这与双正交情况中 $W=V$ 满足 $W^TV=V^TV = I_r$ 的情况是一致的。\n\n现在，我们将这些结论应用于给定的具体数据：\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n0  3  4 \\\\\n0  0  -1\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  1\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n1  -1 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}\n$$\n首先，我们验证双正交条件 $W^T V = I_2$。$W$ 的转置是：\n$$ W^T = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\end{pmatrix} $$\n我们计算乘积 $W^T V$：\n$$ W^T V = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(1) + (0)(0)  (1)(0) + (0)(1) + (0)(1) \\\\ (-1)(1) + (1)(1) + (0)(0)  (-1)(0) + (1)(1) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\n双正交条件成立。\n\n由于该条件成立，降阶算子 $\\hat{A}$ 由简化的 Petrov–Galerkin 公式 $\\hat{A} = W^T A V$ 给出。我们分两步计算这个乘积。首先，我们计算乘积 $AV$：\n$$ AV = \\begin{pmatrix} 2  -1  0 \\\\ 0  3  4 \\\\ 0  0  -1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (-1)(1) + (0)(0)  (2)(0) + (-1)(1) + (0)(1) \\\\ (0)(1) + (3)(1) + (4)(0)  (0)(0) + (3)(1) + (4)(1) \\\\ (0)(1) + (0)(1) + (-1)(0)  (0)(0) + (0)(1) + (-1)(1) \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ 3  7 \\\\ 0  -1 \\end{pmatrix} $$\n接下来，我们将此结果左乘 $W^T$：\n$$ \\hat{A} = W^T (AV) = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 3  7 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(3) + (0)(0)  (1)(-1) + (0)(7) + (0)(-1) \\\\ (-1)(1) + (1)(3) + (0)(0)  (-1)(-1) + (1)(7) + (0)(-1) \\end{pmatrix} $$\n$$ \\hat{A} = \\begin{pmatrix} 1  -1 \\\\ 2  8 \\end{pmatrix} $$\n最后的任务是计算降阶算子 $\\hat{A}$ 的迹。方阵的迹是其对角线元素之和。\n$$ \\mathrm{tr}(\\hat{A}) = \\mathrm{tr} \\begin{pmatrix} 1  -1 \\\\ 2  8 \\end{pmatrix} = 1 + 8 = 9 $$",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "在理解了投影机制之后，下一步是从数据中获取一个合适的低维基。本练习聚焦于本征正交分解（Proper Orthogonal Decomposition, POD），这是一种从仿真或实验快照中提取主导模式的基石技术。你将通过编程实现基于奇异值分解（Singular Value Decomposition, SVD）的算法来计算POD模态，并量化近似误差，从而在数据和降阶基之间建立直接的联系。",
            "id": "4249036",
            "problem": "考虑一个快照矩阵 $X \\in \\mathbb{R}^{n \\times m}$，它收集了一个信息物理系统的 $m$ 个状态快照，这些快照以列的形式排列，每个状态是 $n$ 维的。本征正交分解（POD）用于代理模型和降阶模型中，以找到一个能够捕捉数据中主导相干结构的低维子空间。POD基可以通过对 $X$ 进行奇异值分解（SVD）来获得。目标是计算前 $r$ 个POD模态和相应的秩-$r$重构，然后报告相对重构误差。\n\n从任何实数矩阵都存在SVD分解这一基本前提开始，您的程序必须：\n\n- 计算奇异值分解（SVD）$X = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times p}$，$V \\in \\mathbb{R}^{m \\times p}$，$\\Sigma \\in \\mathbb{R}^{p \\times p}$，$p = \\min(n,m)$，$U$ 和 $V$ 的列是标准正交的，$\\Sigma$ 是对角矩阵，其对角线上的项为非负的（即奇异值）。\n- 提取 $U$ 的前 $r$ 列作为前 $r$ 个POD模态，记作 $U_r \\in \\mathbb{R}^{n \\times r}$，以及相应的截断对角矩阵 $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$，其对角线项为前 $r$ 个最大的奇异值，同时提取 $V$ 的前 $r$ 列构成 $V_r \\in \\mathbb{R}^{m \\times r}$。\n- 构建秩-$r$近似 $X_r = U_r \\Sigma_r V_r^\\top$ 并计算相对Frobenius范数误差\n$$\ne_r = \\frac{\\|X - X_r\\|_F}{\\|X\\|_F},\n$$\n其中 $\\|\\cdot\\|_F$ 表示Frobenius范数。\n- 如果 $\\|X\\|_F = 0$，则定义 $e_r = 0$。\n\n您的程序必须为以下测试套件中的每个测试用例实现上述步骤，并按规定格式生成结果。无需外部输入；矩阵和 $r$ 的值如下所示。\n\n测试套件：\n\n- 案例 $1$（通用非方阵，理想路径）：$X_1 \\in \\mathbb{R}^{4 \\times 3}$\n$$\nX_1 = \\begin{bmatrix}\n2  -1  0 \\\\\n0  1  3 \\\\\n4  -2  1 \\\\\n1  0  -1\n\\end{bmatrix},\n$$\n$r_1 = 2$。\n\n- 案例 $2$（列线性相关的秩亏矩阵）：$X_2 \\in \\mathbb{R}^{3 \\times 3}$\n$$\nX_2 = \\begin{bmatrix}\n1  2  3 \\\\\n2  4  6 \\\\\n1  2  3\n\\end{bmatrix},\n$$\n$r_2 = 1$。\n\n- 案例 $3$（边界情况 r=0）：$X_3 \\in \\mathbb{R}^{2 \\times 2}$\n$$\nX_3 = \\begin{bmatrix}\n3  -1 \\\\\n0  2\n\\end{bmatrix},\n$$\n$r_3 = 0$。\n\n- 案例 $4$（使用 r = min(n,m) 的全秩重构）：$X_4 \\in \\mathbb{R}^{5 \\times 3}$\n$$\nX_4 = \\begin{bmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n2  -1  0 \\\\\n1  3  1 \\\\\n-2  0  1\n\\end{bmatrix},\n$$\n$r_4 = 3$。\n\n- 案例 $5$（零矩阵边缘情况）：$X_5 \\in \\mathbb{R}^{3 \\times 4}$\n$$\nX_5 = \\begin{bmatrix}\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{bmatrix},\n$$\n$r_5 = 2$。\n\n- 案例 $6$（尺度衰减的病态对角矩阵）：$X_6 \\in \\mathbb{R}^{3 \\times 3}$\n$$\nX_6 = \\begin{bmatrix}\n10  0  0 \\\\\n0  1  0 \\\\\n0  0  0.1\n\\end{bmatrix},\n$$\n$r_6 = 2$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个 $e_r$ 值都四舍五入到8位小数，并按照案例1到6的顺序排列，例如：$[e_1,e_2,e_3,e_4,e_5,e_6]$。\n- 每个条目必须是十进制数。",
            "solution": "该问题要求计算给定矩阵 $X$ 的秩-$r$近似的相对Frobenius范数误差，该近似通过本征正交分解（POD）获得。这是降阶模型中的一项基本任务，其目标是用低维表示来捕捉高维系统的最显著特征。实现这一目标的数学工具是奇异值分解（SVD）。\n\n给定的数据矩阵 $X \\in \\mathbb{R}^{n \\times m}$ 聚合了一个 $n$ 维状态向量的 $m$ 个快照。 $X$ 的SVD是一种形式如下的分解：\n$$\nX = U \\Sigma V^\\top\n$$\n问题指定使用“薄”SVD或“经济”SVD。对于一个维度为 $n \\times m$ 的矩阵 $X$，令 $p = \\min(n, m)$。薄SVD的组成部分是：\n- $U \\in \\mathbb{R}^{n \\times p}$：一个矩阵，其列是 $X$ 的前 $p$ 个左奇异向量。这些列是标准正交的，被称为POD模态。它们在最小二乘意义上构成了数据的最优基。\n- $\\Sigma \\in \\mathbb{R}^{p \\times p}$：一个对角矩阵，包含 $X$ 的 $p$ 个奇异值，按降序排列，记为 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$。每个奇异值 $\\sigma_i$ 的大小对应于第 $i$ 个POD模态的重要性。\n- $V \\in \\mathbb{R}^{m \\times p}$：一个矩阵，其列是 $X$ 的前 $p$ 个右奇异向量。这些列也是标准正交的。\n\nEckart-Young-Mirsky 定理指出，在Frobenius范数（以及谱范数）下，矩阵 $X$ 的最佳秩-$r$近似是通过截断SVD得到的。这个秩-$r$近似，记作 $X_r$，是使用前 $r$ 个奇异值及其对应的奇异向量构建的：\n$$\nX_r = U_r \\Sigma_r V_r^\\top\n$$\n其中：\n- $U_r \\in \\mathbb{R}^{n \\times r}$ 是包含 $U$ 的前 $r$ 列的矩阵。\n- $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ 是包含前 $r$ 个奇异值 $\\sigma_1, \\dots, \\sigma_r$ 的对角矩阵。\n- $V_r \\in \\mathbb{R}^{m \\times r}$ 是包含 $V$ 的前 $r$ 列的矩阵，因此 $V_r^\\top \\in \\mathbb{R}^{r \\times m}$。\n\n问题要求计算相对重构误差，定义为：\n$$\ne_r = \\frac{\\|X - X_r\\|_F}{\\|X\\|_F}\n$$\n其中 $\\|\\cdot\\|_F$ 是矩阵 $A \\in \\mathbb{R}^{n \\times m}$ 的Frobenius范数，计算公式为 $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^m A_{ij}^2}$。\n\nSVD的一个关键性质是它与Frobenius范数的关系。一个矩阵的Frobenius范数的平方等于其奇异值平方和：\n$$\n\\|X\\|_F^2 = \\sum_{i=1}^p \\sigma_i^2\n$$\n近似误差矩阵 $X - X_r$ 的奇异值为 $\\sigma_{r+1}, \\sigma_{r+2}, \\dots, \\sigma_p$。因此，误差的Frobenius范数的平方是：\n$$\n\\|X - X_r\\|_F^2 = \\sum_{i=r+1}^p \\sigma_i^2\n$$\n这提供了一种高效的方法，可以直接从奇异值计算相对误差 $e_r$，而无需显式地构造矩阵 $X_r$：\n$$\ne_r = \\sqrt{\\frac{\\sum_{i=r+1}^p \\sigma_i^2}{\\sum_{i=1}^p \\sigma_i^2}}\n$$\n只要 $\\|X\\|_F > 0$，此公式就有效。如果 $X$ 是零矩阵，其范数为 $0$，且其所有奇异值也为 $0$。在这种情况下，问题规定误差 $e_r$ 应定义为 $0$。\n\n对于每个测试用例 $(X, r)$，算法如下：\n$1$. 计算 $X$ 的Frobenius范数 $\\|X\\|_F$。如果 $\\|X\\|_F = 0$，则误差 $e_r$ 为 $0$。\n$2$. 如果 $\\|X\\|_F > 0$，计算 $X$ 的奇异值。设它们为数组 $s = [\\sigma_1, \\sigma_2, \\dots, \\sigma_p]$。\n$3$. 奇异值的数量为 $p = \\min(n, m)$。用于切片的索引对应于从0开始的编程索引。误差范数的求和涉及从索引 $r$ 到 $p-1$ 的奇异值。\n$4$. 计算分子，即误差范数：$\\|X - X_r\\|_F = \\sqrt{\\sum_{i=r}^{p-1} s_i^2}$。\n$5$. 分母 $\\|X\\|_F$ 已经计算过。它也可以计算为 $\\sqrt{\\sum_{i=0}^{p-1} s_i^2}$。\n$6$. 计算相对误差 $e_r = \\|X - X_r\\|_F / \\|X\\|_F$。\n\n将此过程应用于每个测试用例：\n- 对于案例1（$r_1=2$），误差将由最小的奇异值 $\\sigma_3$ 决定。\n- 对于案例2（秩亏），矩阵的秩为1。因此，只有一个奇异值 $\\sigma_1$ 是非零的。对于 $r_2=1$，我们保留了谱的整个非零部分，所以误差 $\\|X - X_1\\|_F$ 应为 $0$，得到 $e_1=0$。\n- 对于案例3（$r_3=0$），近似 $X_0$ 是零矩阵。误差为 $\\|X - \\mathbf{0}\\|_F = \\|X\\|_F$。由于 $X_3$ 不是零矩阵，相对误差为 $e_0 = \\|X\\|_F / \\|X\\|_F = 1$。\n- 对于案例4（$r_4=3$），我们有 $p = \\min(5, 3) = 3$。由于 $r_4=p$，我们执行的是完全重构。因此，$X_3 = X$，误差 $e_3$ 将为 $0$（在机器精度范围内）。\n- 对于案例5（$X_5=\\mathbf{0}$），矩阵范数为 $0$。根据问题的定义，误差 $e_5$ 为 $0$。\n- 对于案例6（对角矩阵），奇异值是对角元素的绝对值：$10, 1, 0.1$。对于 $r_6=2$，误差由最小的奇异值 $\\sigma_3=0.1$ 决定。相对误差将是 $\\sigma_3 / \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative Frobenius norm error for rank-r POD approximations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [2., -1., 0.],\n            [0., 1., 3.],\n            [4., -2., 1.],\n            [1., 0., -1.]\n        ]), 2),\n        (np.array([\n            [1., 2., 3.],\n            [2., 4., 6.],\n            [1., 2., 3.]\n        ]), 1),\n        (np.array([\n            [3., -1.],\n            [0., 2.]\n        ]), 0),\n        (np.array([\n            [1., 0., 2.],\n            [0., 1., -1.],\n            [2., -1., 0.],\n            [1., 3., 1.],\n            [-2., 0., 1.]\n        ]), 3),\n        (np.array([\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]\n        ]), 2),\n        (np.array([\n            [10., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 0.1]\n        ]), 2)\n    ]\n\n    results = []\n    for X, r in test_cases:\n        # Calculate the Frobenius norm of the original matrix X.\n        norm_X = np.linalg.norm(X, 'fro')\n\n        # Handle the special case where the matrix is the zero matrix.\n        # If norm_X is 0, the relative error is defined as 0.\n        if np.isclose(norm_X, 0.0):\n            relative_error = 0.0\n            results.append(relative_error)\n            continue\n\n        # Compute the singular values of X.\n        # We only need the singular values, so we don't compute U and Vh for efficiency.\n        # singular values are returned in descending order.\n        s = np.linalg.svd(X, compute_uv=False)\n        \n        # The squared Frobenius norm of the error matrix (X - X_r) is the sum\n        # of the squares of the truncated singular values (from index r onwards).\n        # s[r:] gives all singular values from index r to the end.\n        squared_error_norm = np.sum(s[r:]**2)\n        \n        # The error norm is the square root of this sum.\n        error_norm = np.sqrt(squared_error_norm)\n        \n        # The relative error is the ratio of the error norm to the original matrix norm.\n        relative_error = error_norm / norm_X\n        \n        results.append(relative_error)\n\n    # Format the results to 8 decimal places and print in the specified format.\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这项练习将前面的概念集成到一个完整且真实的流程中，用于构建和验证一个数据驱动的降阶模型。你不仅要构建模型，还要解决一个关键任务：通过在保留验证集上的预测精度与动力学稳定性这一基本要求之间进行权衡，来选择模型的阶数（$r$）。该练习模拟了工程师在开发数字孪生组件时所遵循的端到端过程。",
            "id": "4249030",
            "problem": "给定一个离散时间线性时不变状态空间模型，该模型用于表示信息物理系统 (CPS) 中数字孪生的一个组件。真实对象的演化遵循以下方程\n$$\nx_{k+1} = A x_k + B u_k + w_k, \\quad y_k = C x_k + v_k,\n$$\n其中 $x_k \\in \\mathbb{R}^n$ 是状态，$u_k \\in \\mathbb{R}^m$ 是控制输入，$y_k \\in \\mathbb{R}^n$ 是测量输出，$A \\in \\mathbb{R}^{n \\times n}$ 是系统矩阵，$B \\in \\mathbb{R}^{n \\times m}$ 是输入矩阵，$C \\in \\mathbb{R}^{n \\times n}$ 是输出矩阵，$w_k \\in \\mathbb{R}^n$ 是过程噪声，以及 $v_k \\in \\mathbb{R}^n$ 是测量噪声。在本问题中，假设 $C = I_n$，$w_k = 0$，并且 $A$ 在离散时间意义上是稳定的。控制输入 $u_k$ 是独立同分布的高斯随机向量，其每个分量的均值为零，方差为 $\\sigma_u^2$；测量噪声 $v_k$ 是独立同分布的高斯噪声，其均值为零，标准差已指定。\n\n通过将动态投影到由训练数据经奇异值分解 (SVD) 得到的本征正交分解计算出的子空间上，构建了一个代理降阶模型。将训练输出堆叠成快照矩阵\n$$\nX_{\\text{train}} = \\begin{bmatrix} y_0  y_1  \\cdots  y_T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (T+1)}.\n$$\n计算 SVD $X_{\\text{train}} = U \\Sigma V^\\top$ 并通过取 $U$ 的前 $r$ 列来形成降阶基 $U_r \\in \\mathbb{R}^{n \\times r}$。定义降阶坐标 $z_k = U_r^\\top y_k \\in \\mathbb{R}^r$。通过求解最小二乘问题，从训练数据中拟合带有仿射项的降阶线性模型\n$$\n\\min_{A_r, B_r, c_r} \\sum_{k=0}^{T-1} \\left\\| z_{k+1} - A_r z_k - B_r u_k - c_r \\right\\|_2^2,\n$$\n其中 $A_r \\in \\mathbb{R}^{r \\times r}$，$B_r \\in \\mathbb{R}^{r \\times m}$，以及 $c_r \\in \\mathbb{R}^r$。\n\n为确保科学真实性，对降阶代理动态施加稳定性约束，要求 $A_r$ 的谱半径（记为 $\\rho(A_r)$）满足\n$$\n\\rho(A_r)  1 - \\varepsilon,\n$$\n其中 $\\varepsilon > 0$ 是一个小的裕度。对于每个满足稳定性约束的候选阶数 $r$，通过在验证输入和初始降阶状态 $z_0 = U_r^\\top y_0^{\\text{val}}$ 上模拟降阶代理来进行留出验证：\n$$\nz_{k+1} = A_r z_k + B_r u_k^{\\text{val}} + c_r, \\quad \\hat{y}_k = U_r z_k,\n$$\n并计算验证时域上的均方误差 (MSE)，\n$$\nE_r = \\frac{1}{N_{\\text{val}} \\cdot n} \\sum_{k=0}^{N_{\\text{val}}-1} \\left\\| \\hat{y}_k - y_k^{\\text{val}} \\right\\|_2^2,\n$$\n其中 $N_{\\text{val}}$ 是验证时间步数，$n$ 是全状态维度。选择在稳定性约束下最小化留出损失 $E_r$ 的阶数 $r^\\star$。如果在小的数值容差内出现平局，则在最小化器中选择最小的 $r$。如果没有任何候选阶数能够产生稳定的降阶动态，则对该测试用例返回 $-1$。\n\n实现上述过程，并将其应用于以下测试套件。对于每个测试用例，按如下方式生成一个稳定的矩阵 $A$ 和一个兼容维度的输入矩阵 $B$：抽取一个随机矩阵 $M \\in \\mathbb{R}^{n \\times n}$，其元素为独立的标准正态分布，计算其谱半径 $\\rho(M)$，并设置\n$$\nA = \\alpha M, \\quad \\alpha = \\frac{0.9}{\\rho(M)},\n$$\n以确保 $\\rho(A) \\leq 0.9$。抽取 $B$，其元素为独立的标准差为 $0.5$ 的正态分布。使用 $x_0 = 0_n$ 并模拟系统 $N_{\\text{train}} + N_{\\text{val}}$ 步。对于所有情况，使用 $\\sigma_u = 0.5$ 和 $w_k = 0$。每个测试用例的测量噪声标准差已指定。为保证可复现性，必须使用给定的种子值初始化随机数生成器。\n\n测试套件规格：\n- 案例 1：$n = 5$，$m = 2$，$N_{\\text{train}} = 200$，$N_{\\text{val}} = 100$，测量噪声标准差 $0.01$，种子 $2025$，候选阶数 $\\{1,2,3,4,5\\}$。\n- 案例 2：$n = 5$，$m = 2$，$N_{\\text{train}} = 60$，$N_{\\text{val}} = 150$，测量噪声标准差 $0.05$，种子 $7$，候选阶数 $\\{1,2,3,4\\}$。\n- 案例 3：$n = 5$，$m = 2$，$N_{\\text{train}} = 120$，$N_{\\text{val}} = 200$，测量噪声标准差 $0.02$，种子 $123$，候选阶数 $\\{1,2,3,4,5\\}$。\n- 案例 4：$n = 5$，$m = 2$，$N_{\\text{train}} = 80$，$N_{\\text{val}} = 50$，测量噪声标准差 $0.2$，种子 $999$，候选阶数 $\\{1,2,3\\}$。\n\n稳定性裕度使用 $\\varepsilon = 10^{-6}$。对于每种情况，根据所述规则计算选定的阶数 $r^\\star$。最终程序输出必须是包含四个案例的选定阶数的单行，格式为逗号分隔的列表，并用方括号括起来，例如 $[r_1^\\star,r_2^\\star,r_3^\\star,r_4^\\star]$。输出中不涉及角度，也不需要物理单位。所有数值应酌情使用浮点数计算，并且程序必须以确切指定的格式生成输出。",
            "solution": "该问题要求通过最小化留出损失来选择代理降阶模型的维度，同时强制要求降阶动态的稳定性。该方法基于标准的离散时间线性系统理论和基于投影的模型降阶。\n\n从离散时间线性时不变模型开始\n$$\nx_{k+1} = A x_k + B u_k + w_k, \\quad y_k = C x_k + v_k,\n$$\n并假设 $C = I_n$，$w_k = 0$，且 $A$ 是稳定的。离散时间稳定性由谱半径 $\\rho(A)$ 表征，其定义为\n$$\n\\rho(A) = \\max_i \\left| \\lambda_i(A) \\right|,\n$$\n其中 $\\lambda_i(A)$ 是 $A$ 的特征值。如果 $\\rho(A)  1$，则矩阵 $A$ 是稳定的。为确保真实系统是稳定的，我们通过缩放一个随机矩阵 $M$ 来构造 $A$：\n$$\nA = \\alpha M, \\quad \\alpha = \\frac{0.9}{\\rho(M)}.\n$$\n这保证了 $\\rho(A) \\leq 0.9$，从而保证了稳定性。\n\n代理降阶模型是通过使用奇异值分解 (SVD) 的本征正交分解构建的。从训练输出\n$$\nX_{\\text{train}} = \\begin{bmatrix} y_0  y_1  \\cdots  y_T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (T+1)},\n$$\n计算 SVD\n$$\nX_{\\text{train}} = U \\Sigma V^\\top,\n$$\n其中 $U \\in \\mathbb{R}^{n \\times n}$ 具有标准正交列，$\\Sigma$ 包含奇异值。阶数为 $r$ 的降阶基 $U_r \\in \\mathbb{R}^{n \\times r}$ 由 $U$ 的前 $r$ 列构成。降阶坐标定义为\n$$\nz_k = U_r^\\top y_k \\in \\mathbb{R}^r.\n$$\n\n为了估计降阶动态，我们在降阶坐标中假设一个仿射线性模型，\n$$\nz_{k+1} = A_r z_k + B_r u_k + c_r,\n$$\n并通过最小化训练序列上的预测误差平方和来拟合 $A_r \\in \\mathbb{R}^{r \\times r}$，$B_r \\in \\mathbb{R}^{r \\times m}$ 和 $c_r \\in \\mathbb{R}^r$，\n$$\n\\min_{A_r, B_r, c_r} \\sum_{k=0}^{T-1} \\left\\| z_{k+1} - A_r z_k - B_r u_k - c_r \\right\\|_2^2.\n$$\n这个最小二乘问题可以表示为矩阵形式。设 $L = T$ 为转换次数，并定义矩阵\n$$\nZ_0 = \\begin{bmatrix} z_0  z_1  \\cdots  z_{L-1} \\end{bmatrix} \\in \\mathbb{R}^{r \\times L}, \\quad Z_1 = \\begin{bmatrix} z_1  z_2  \\cdots  z_{L} \\end{bmatrix} \\in \\mathbb{R}^{r \\times L},\n$$\n$$\nU_0 = \\begin{bmatrix} u_0  u_1  \\cdots  u_{L-1} \\end{bmatrix} \\in \\mathbb{R}^{m \\times L}, \\quad \\mathbf{1}_L = \\begin{bmatrix} 1  1  \\cdots  1 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times L}.\n$$\n构建回归量\n$$\n\\Phi = \\begin{bmatrix} Z_0 \\\\ U_0 \\\\ \\mathbf{1}_L \\end{bmatrix} \\in \\mathbb{R}^{(r + m + 1) \\times L}.\n$$\n最小化残差的 Frobenius 范数的最小二乘解通过右伪逆获得，\n$$\n\\Theta^\\star = Z_1 \\, \\Phi^\\dagger \\in \\mathbb{R}^{r \\times (r + m + 1)},\n$$\n其中 $\\Theta^\\star$ 被划分为\n$$\n\\Theta^\\star = \\begin{bmatrix} A_r  B_r  c_r \\end{bmatrix}.\n$$\n\n我们必须在降阶动态中强制执行稳定性。降阶状态转换矩阵的离散时间稳定性条件是\n$$\n\\rho(A_r)  1 - \\varepsilon,\n$$\n其中 $\\varepsilon > 0$ 是一个小的数值裕度。不满足此条件的阶数 $r$ 将不予考虑。\n\n对于留出验证，在验证输入和初始降阶状态 $z_0^{\\text{val}} = U_r^\\top y_0^{\\text{val}}$ 上模拟降阶模型：\n$$\nz_{k+1}^{\\text{val}} = A_r z_k^{\\text{val}} + B_r u_k^{\\text{val}} + c_r,\n$$\n然后重构输出\n$$\n\\hat{y}_k = U_r z_k^{\\text{val}}.\n$$\n计算整个验证时域的均方误差 (MSE)，\n$$\nE_r = \\frac{1}{N_{\\text{val}} \\cdot n} \\sum_{k=0}^{N_{\\text{val}}-1} \\left\\| \\hat{y}_k - y_k^{\\text{val}} \\right\\|_2^2.\n$$\n选定的阶数是\n$$\nr^\\star = \\arg\\min_{r \\in \\mathcal{R}} E_r \\quad \\text{subject to} \\quad \\rho(A_r)  1 - \\varepsilon,\n$$\n其中 $\\mathcal{R}$ 是候选阶数集。如果在数值容差内 $E_r$ 出现平局，选择最小的 $r$ 以偏好简约性。如果没有候选阶数能产生稳定的降阶动态，则返回 -1。\n\n算法实现细节：\n- 每个测试用例使用固定的随机数生成器种子以保证可复现性。\n- 通过使用 $\\alpha = 0.9 / \\rho(M)$ 缩放随机矩阵 $M$ 来生成 $A$，以确保 $\\rho(A) \\leq 0.9$。\n- 抽取 $B$，其元素为标准差为 $0.5$ 的独立正态分布。\n- 从 $x_0 = 0_n$ 开始，模拟系统 $N_{\\text{train}} + N_{\\text{val}}$ 步，其中 $u_k \\sim \\mathcal{N}(0, \\sigma_u^2 I_m)$ 和测量噪声 $v_k \\sim \\mathcal{N}(0, \\sigma_v^2 I_n)$，$\\sigma_u = 0.5$，$\\sigma_v$ 由每个测试用例指定。\n- 使用 $X_{\\text{train}}$ 的 SVD 构建 $U_r$，然后投影到降阶坐标并通过最小二乘法拟合 $(A_r, B_r, c_r)$。\n- 通过对 $A_r$ 进行谱半径检查来强制执行稳定性约束。\n- 执行验证推演并计算 $E_r$。\n- 在稳定候选者中基于最小的 $E_r$ 选择 $r^\\star$，并以较小的 $r$ 打破平局。\n\n测试套件定义：\n- 案例 1：$n = 5$，$m = 2$，$N_{\\text{train}} = 200$，$N_{\\text{val}} = 100$，$\\sigma_v = 0.01$，种子 $2025$，$\\mathcal{R} = \\{1,2,3,4,5\\}$。\n- 案例 2：$n = 5$，$m = 2$，$N_{\\text{train}} = 60$，$N_{\\text{val}} = 150$，$\\sigma_v = 0.05$，种子 $7$，$\\mathcal{R} = \\{1,2,3,4\\}$。\n- 案例 3：$n = 5$，$m = 2$，$N_{\\text{train}} = 120$，$N_{\\text{val}} = 200$，$\\sigma_v = 0.02$，种子 $123$，$\\mathcal{R} = \\{1,2,3,4,5\\}$。\n- 案例 4：$n = 5$，$m = 2$，$N_{\\text{train}} = 80$，$N_{\\text{val}} = 50$，$\\sigma_v = 0.2$，种子 $999$，$\\mathcal{R} = \\{1,2,3\\}$。\n\n使用 $\\varepsilon = 10^{-6}$。程序必须输出包含四个案例的选定阶数的单行，格式为 $[r_1^\\star,r_2^\\star,r_3^\\star,r_4^\\star]$。这种格式确保解决方案是可量化和可测试的，每个元素是每个测试用例的整数结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_stable_system(n, m, rng):\n    \"\"\"\n    Generate a stable discrete-time linear system (A, B).\n    A is constructed by scaling a random matrix M so that its spectral radius is = 0.9.\n    B is drawn with normal entries with std 0.5.\n    \"\"\"\n    M = rng.normal(size=(n, n))\n    # Compute spectral radius of M\n    eigvals = np.linalg.eigvals(M)\n    rho_M = np.max(np.abs(eigvals))\n    # Scale to ensure stability: spectral radius = 0.9\n    alpha = 0.9 / rho_M if rho_M  0 else 0.0\n    A = alpha * M\n    B = rng.normal(scale=0.5, size=(n, m))\n    return A, B\n\ndef simulate_system(A, B, u, noise_std, rng):\n    \"\"\"\n    Simulate the system x_{k+1} = A x_k + B u_k, y_k = x_k + v_k.\n    Inputs:\n        A: (n,n) system matrix\n        B: (n,m) input matrix\n        u: (T, m) input sequence\n        noise_std: float, standard deviation of measurement noise v_k\n    Returns:\n        y: (T, n) measured outputs\n        x: (T, n) true states\n    \"\"\"\n    n, m = B.shape\n    T = u.shape[0]\n    x = np.zeros((T, n))\n    y = np.zeros((T, n))\n    xk = np.zeros(n)\n    for k in range(T):\n        # Output with measurement noise\n        vk = rng.normal(scale=noise_std, size=n) if noise_std  0 else np.zeros(n)\n        y[k] = xk + vk\n        # State update\n        xkp1 = A @ xk + B @ u[k]\n        x[k] = xk\n        xk = xkp1\n    return y, x\n\ndef build_pod_basis(y_train, r):\n    \"\"\"\n    Build POD basis U_r using SVD of training data snapshots.\n    y_train: (N_train, n)\n    r: reduced order\n    Returns:\n        U_r: (n, r)\n    \"\"\"\n    # Snapshots as (n, T+1) matrix; here use all y_train columns\n    X = y_train.T  # shape (n, N_train)\n    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n    U_r = U[:, :r]\n    return U_r\n\ndef estimate_reduced_dynamics(U_r, y_train, u_train):\n    \"\"\"\n    Estimate (A_r, B_r, c_r) via least squares in reduced coordinates.\n    y_train: (N_train, n)\n    u_train: (N_train, m)\n    Returns:\n        A_r: (r, r)\n        B_r: (r, m)\n        c_r: (r,)\n    \"\"\"\n    # Project training outputs to reduced coordinates\n    Z = (U_r.T @ y_train.T)  # shape (r, N_train)\n    # Use transitions from k=0..N_train-2 to k+1\n    L = Z.shape[1] - 1\n    Z0 = Z[:, :L]            # (r, L)\n    Z1 = Z[:, 1:L+1]         # (r, L)\n    U0 = u_train[:L, :].T    # (m, L)\n    ones = np.ones((1, L))   # (1, L)\n    Phi = np.vstack([Z0, U0, ones])  # (r+m+1, L)\n    # Least squares via pseudoinverse\n    Theta = Z1 @ np.linalg.pinv(Phi)  # (r, r+m+1)\n    r = U_r.shape[1]\n    m = u_train.shape[1]\n    A_r = Theta[:, :r]\n    B_r = Theta[:, r:r+m]\n    c_r = Theta[:, -1]\n    return A_r, B_r, c_r\n\ndef spectral_radius(A):\n    \"\"\"\n    Compute spectral radius of matrix A.\n    \"\"\"\n    eigvals = np.linalg.eigvals(A)\n    return np.max(np.abs(eigvals))\n\ndef validate_model(U_r, A_r, B_r, c_r, y_val, u_val):\n    \"\"\"\n    Validate reduced model by rollout on validation data and compute MSE.\n    y_val: (N_val, n)\n    u_val: (N_val, m)\n    Returns:\n        mse: float\n    \"\"\"\n    n = U_r.shape[0]\n    r = U_r.shape[1]\n    N_val = y_val.shape[0]\n    # Initialize reduced state from first validation measurement\n    z = np.zeros((N_val, r))\n    z[0] = U_r.T @ y_val[0]\n    # Rollout\n    for k in range(N_val - 1):\n        z[k+1] = A_r @ z[k] + B_r @ u_val[k] + c_r\n    # Reconstruct outputs\n    y_hat = (U_r @ z.T).T  # (N_val, n)\n    # Compute mean squared error across all times and dimensions\n    err = y_hat - y_val\n    mse = np.mean(err**2)\n    return mse\n\ndef select_order(orders, A, B, u_train, y_train, u_val, y_val, epsilon):\n    \"\"\"\n    Cross-validate across candidate orders, enforcing stability.\n    Returns:\n        selected_r: int (best order), or -1 if no stable candidate\n    \"\"\"\n    best_r = None\n    best_mse = np.inf\n    mse_tie_tol = 1e-12\n    n = A.shape[0]\n    for r in orders:\n        if r == 0 or r  n:\n            continue\n        # Build POD basis\n        U_r = build_pod_basis(y_train, r)\n        # Estimate reduced dynamics\n        A_r, B_r, c_r = estimate_reduced_dynamics(U_r, y_train, u_train)\n        # Stability check\n        rho = spectral_radius(A_r)\n        if not (rho  1.0 - epsilon):\n            continue\n        # Validate\n        mse = validate_model(U_r, A_r, B_r, c_r, y_val, u_val)\n        # Select by minimal MSE, tie-break by smaller r\n        if (mse + mse_tie_tol)  best_mse or (abs(mse - best_mse) = mse_tie_tol and (best_r is None or r  best_r)):\n            best_mse = mse\n            best_r = r\n    return best_r if best_r is not None else -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (n, m, N_train, N_val, noise_std, seed, orders)\n    test_cases = [\n        (5, 2, 200, 100, 0.01, 2025, [1, 2, 3, 4, 5]),\n        (5, 2, 60, 150, 0.05, 7,    [1, 2, 3, 4]),\n        (5, 2, 120, 200, 0.02, 123, [1, 2, 3, 4, 5]),\n        (5, 2, 80, 50,  0.2,  999,  [1, 2, 3]),\n    ]\n    epsilon = 1e-6\n    sigma_u = 0.5\n\n    results = []\n    for case in test_cases:\n        n, m, N_train, N_val, noise_std, seed, orders = case\n        rng = np.random.default_rng(seed)\n        # Generate system\n        A, B = generate_stable_system(n, m, rng)\n        # Generate inputs for full horizon\n        T_total = N_train + N_val\n        u = rng.normal(scale=sigma_u, size=(T_total, m))\n        # Simulate system\n        y, x = simulate_system(A, B, u, noise_std, rng)\n        # Split into train and validation\n        y_train = y[:N_train]\n        y_val = y[N_train:N_train+N_val]\n        u_train = u[:N_train]\n        u_val = u[N_train:N_train+N_val]\n        # Select order\n        selected_r = select_order(orders, A, B, u_train, y_train, u_val, y_val, epsilon)\n        results.append(selected_r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}