{
    "hands_on_practices": [
        {
            "introduction": "许多降阶建模技术的核心是将高维动力学投影到一个低维子空间上。本练习将引导您完成使用 Petrov-Galerkin 投影推导降阶模型的全过程，阐明一个大型系统矩阵 $A$ 如何被压缩成一个更小的矩阵 $A_r$。通过这项实践，您将巩固支撑多种降阶模型技术的数学基础，并理解试探基和测试基之间的关系。",
            "id": "4249015",
            "problem": "一个线性时不变 (LTI) 子系统的数字孪生由全阶模型 $ \\dot{x}(t) = A x(t) $ 描述，其中 $ x(t) \\in \\mathbb{R}^{n} $ 且 $ A \\in \\mathbb{R}^{n \\times n} $。为构建降阶代理模型，状态在试探子空间中被近似为 $ x(t) \\approx V a(t) $，其中 $ V \\in \\mathbb{R}^{n \\times r} $ 是列满秩的，且 $ a(t) \\in \\mathbb{R}^{r} $ 是降阶坐标。使用具有列满秩测试基 $ W \\in \\mathbb{R}^{n \\times r} $ 的 Petrov-Galerkin 投影，从第一性原理出发，通过强制残差正交性来推导 $ a(t) $ 的降阶动力学系统。明确展示在 $ V $ 和 $ W $ 之间满足双正交条件下，降阶算子如何用 $ A $、$ V $ 和 $ W $ 表示。然后，将您的结果特化到 Galerkin 情况，并确定当 $ V $ 的列是标准正交时降阶算子的形式。\n\n最后，对于给定的数据\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n0  3  4 \\\\\n0  0  -1\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  1\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n1  -1 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix},\n$$\n验证双正交条件成立，构建通过 Petrov-Galerkin 投影得到的降阶算子，并计算其迹。仅报告标量迹作为您的最终答案。无需四舍五入，不涉及单位。最终答案必须是单个实数。",
            "solution": "该问题是有效的，因为它科学地基于线性代数和模型降阶的原理，是适定的，并使用客观、正式的语言。它是自洽的，没有矛盾或歧义。\n\n我们从第一性原理开始推导降阶模型。全阶线性时不变 (LTI) 系统由以下公式给出：\n$$ \\dot{x}(t) = A x(t) $$\n其中 $x(t) \\in \\mathbb{R}^{n}$ 是状态向量，$A \\in \\mathbb{R}^{n \\times n}$ 是状态矩阵。\n\n在由矩阵 $V \\in \\mathbb{R}^{n \\times r}$（其中 $r \\ll n$）的列向量张成的试探子空间中寻求降阶近似。状态近似为：\n$$ x(t) \\approx V a(t) $$\n此处，$V$ 是列满秩的试探基矩阵，$a(t) \\in \\mathbb{R}^{r}$ 是降阶坐标向量。\n\n将此近似代入全阶模型，我们得到一个残差 $R(t)$，它表示近似在满足原微分方程时的误差：\n$$ V \\dot{a}(t) = A (V a(t)) - R(t) $$\n$$ R(t) = A V a(t) - V \\dot{a}(t) $$\nPetrov-Galerkin 投影方法旨在通过强制残差与一个选定的测试子空间正交来最小化该残差。测试子空间由一个同样是列满秩的测试基矩阵 $W \\in \\mathbb{R}^{n \\times r}$ 的列向量张成。正交条件表示为：\n$$ W^T R(t) = 0 $$\n将残差 $R(t)$ 的表达式代入正交条件，得到：\n$$ W^T (A V a(t) - V \\dot{a}(t)) = 0 $$\n利用转置运算的线性性质，我们可以分配 $W^T$：\n$$ W^T A V a(t) - W^T V \\dot{a}(t) = 0 $$\n重新整理各项，我们得到坐标 $a(t)$ 的降阶动力学系统：\n$$ W^T V \\dot{a}(t) = (W^T A V) a(t) $$\n为得到 $\\dot{a}(t)$ 的显式形式，我们必须对矩阵 $W^T V \\in \\mathbb{R}^{r \\times r}$ 求逆。由于 $V$ 和 $W$ 都是列满秩的，对于恰当选择的基，$W^T V$ 矩阵通常是可逆的。降阶系统则为：\n$$ \\dot{a}(t) = (W^T V)^{-1} (W^T A V) a(t) $$\n该方程的形式为 $\\dot{a}(t) = \\hat{A} a(t)$，其中降阶算子为 $\\hat{A} = (W^T V)^{-1} (W^T A V)$。\n\n问题指定了试探基 $V$ 和测试基 $W$ 之间的双正交条件。该条件在数学上表述为：\n$$ W^T V = I_r $$\n其中 $I_r$ 是 $r \\times r$ 的单位矩阵。\n在此条件下，矩阵 $(W^T V)^{-1}$ 变为 $I_r^{-1} = I_r$。降阶动力学系统显著简化为：\n$$ I_r \\dot{a}(t) = (W^T A V) a(t) $$\n$$ \\dot{a}(t) = (W^T A V) a(t) $$\n因此，对于双正交基，降阶算子简化为 $\\hat{A} = W^T A V$。\n\n接下来，我们将此结果特化到 Galerkin 情况。在 Galerkin 投影中，选择测试基与试探基相同，即 $W = V$。一般的降阶算子变为：\n$$ \\hat{A}_{\\text{Gal}} = (V^T V)^{-1} (V^T A V) $$\n矩阵 $V^T V$ 是 $V$ 中基向量的格拉姆矩阵。由于 $V$ 是列满秩的，$V^T V$ 是对称正定矩阵，因此是可逆的。\n\n如果我们进一步指定 $V$ 的列是标准正交的，根据定义，这意味着 $V^T V = I_r$。在这种情况下，Galerkin 投影的降阶算子简化为：\n$$ \\hat{A}_{\\text{ortho-Gal}} = (I_r)^{-1} (V^T A V) = V^T A V $$\n这与双正交情况中 $W=V$ 满足 $W^TV=V^TV = I_r$ 的情况是一致的。\n\n现在，我们将这些发现应用于给定的具体数据：\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n0  3  4 \\\\\n0  0  -1\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  1\n\\end{pmatrix}, \\quad\nW = \\begin{pmatrix}\n1  -1 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}\n$$\n首先，我们验证双正交条件 $W^T V = I_2$。$W$ 的转置是：\n$$ W^T = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\end{pmatrix} $$\n我们计算乘积 $W^T V$：\n$$ W^T V = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(1) + (0)(0)  (1)(0) + (0)(1) + (0)(1) \\\\ (-1)(1) + (1)(1) + (0)(0)  (-1)(0) + (1)(1) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\n双正交条件成立。\n\n由于该条件成立，降阶算子 $\\hat{A}$ 由简化的 Petrov-Galerkin 公式 $\\hat{A} = W^T A V$ 给出。我们分两步计算这个乘积。首先，我们计算乘积 $AV$：\n$$ AV = \\begin{pmatrix} 2  -1  0 \\\\ 0  3  4 \\\\ 0  0  -1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (-1)(1) + (0)(0)  (2)(0) + (-1)(1) + (0)(1) \\\\ (0)(1) + (3)(1) + (4)(0)  (0)(0) + (3)(1) + (4)(1) \\\\ (0)(1) + (0)(1) + (-1)(0)  (0)(0) + (0)(1) + (-1)(1) \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ 3  7 \\\\ 0  -1 \\end{pmatrix} $$\n接下来，我们用 $W^T$ 左乘这个结果：\n$$ \\hat{A} = W^T (AV) = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 3  7 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(3) + (0)(0)  (1)(-1) + (0)(7) + (0)(-1) \\\\ (-1)(1) + (1)(3) + (0)(0)  (-1)(-1) + (1)(7) + (0)(-1) \\end{pmatrix} $$\n$$ \\hat{A} = \\begin{pmatrix} 1  -1 \\\\ 2  8 \\end{pmatrix} $$\n最后的任务是计算降阶算子 $\\hat{A}$ 的迹。一个方阵的迹是其对角线元素之和。\n$$ \\mathrm{tr}(\\hat{A}) = \\mathrm{tr} \\begin{pmatrix} 1  -1 \\\\ 2  8 \\end{pmatrix} = 1 + 8 = 9 $$",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "在掌握了投影的理论之后，我们将转向一个真实世界中更常见的场景：投影基底是未知的，必须从数据中学习。本练习模拟了一项典型的工程任务，即使用系统快照数据通过本征正交分解（POD）来构建降阶模型。您将学习如何通过交叉验证来选择最佳模型阶数，从而在预测精度、模型复杂性和动态稳定性之间取得平衡。",
            "id": "4249030",
            "problem": "给定一个离散时间线性时不变状态空间模型，旨在表示信息物理系统（CPS）数字孪生中的一个组件。真实系统根据以下方程演化\n$$\nx_{k+1} = A x_k + B u_k + w_k, \\quad y_k = C x_k + v_k,\n$$\n其中 $x_k \\in \\mathbb{R}^n$ 是状态，$u_k \\in \\mathbb{R}^m$ 是控制输入，$y_k \\in \\mathbb{R}^n$ 是测量输出，$A \\in \\mathbb{R}^{n \\times n}$ 是系统矩阵，$B \\in \\mathbb{R}^{n \\times m}$ 是输入矩阵，$C \\in \\mathbb{R}^{n \\times n}$ 是输出矩阵，$w_k \\in \\mathbb{R}^n$ 是过程噪声，$v_k \\in \\mathbb{R}^n$ 是测量噪声。在本问题中，假设 $C = I_n$，$w_k = 0$，并且 $A$ 在离散时间意义下是稳定的。控制输入 $u_k$ 是独立同分布的高斯随机向量，其每个分量的均值为零，方差为 $\\sigma_u^2$。测量噪声 $v_k$ 是独立同分布的高斯噪声，其均值为零，标准差为指定值。\n\n通过将动力学投影到由奇异值分解（SVD）推导的本征正交分解从训练数据计算出的子空间上，构建一个代理降阶模型。将训练输出堆叠成快照矩阵\n$$\nX_{\\text{train}} = \\begin{bmatrix} y_0  y_1  \\cdots  y_T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (T+1)}.\n$$\n计算 SVD $X_{\\text{train}} = U \\Sigma V^\\top$，并通过取 $U$ 的前 $r$ 列来形成降阶基 $U_r \\in \\mathbb{R}^{n \\times r}$。定义降阶坐标 $z_k = U_r^\\top y_k \\in \\mathbb{R}^r$。通过求解以下最小二乘问题，从训练数据中拟合带有仿射项的降阶线性模型\n$$\n\\min_{A_r, B_r, c_r} \\sum_{k=0}^{T-1} \\left\\| z_{k+1} - A_r z_k - B_r u_k - c_r \\right\\|_2^2,\n$$\n其中 $A_r \\in \\mathbb{R}^{r \\times r}$，$B_r \\in \\mathbb{R}^{r \\times m}$，$c_r \\in \\mathbb{R}^r$。\n\n为确保科学真实性，对降阶代理动力学施加稳定性约束，要求 $A_r$ 的谱半径（记为 $\\rho(A_r)$）满足\n$$\n\\rho(A_r)  1 - \\varepsilon,\n$$\n其中 $\\varepsilon > 0$ 为一个小的裕度。对于每个满足稳定性约束的候选阶数 $r$，通过在验证输入和初始降阶状态 $z_0 = U_r^\\top y_0^{\\text{val}}$ 上模拟降阶代理模型来进行留出验证：\n$$\nz_{k+1} = A_r z_k + B_r u_k^{\\text{val}} + c_r, \\quad \\hat{y}_k = U_r z_k,\n$$\n并计算在验证时域内的均方误差（MSE）：\n$$\nE_r = \\frac{1}{N_{\\text{val}} \\cdot n} \\sum_{k=0}^{N_{\\text{val}}-1} \\left\\| \\hat{y}_k - y_k^{\\text{val}} \\right\\|_2^2,\n$$\n其中 $N_{\\text{val}}$ 是验证时间步数，$n$ 是全状态维度。选择在满足稳定性约束的条件下使留出损失 $E_r$ 最小化的阶数 $r^\\star$。如果在某个小的数值容差范围内出现相同结果，则在最小化器中选择最小的 $r$。如果没有候选阶数能产生稳定的降阶动力学，则该测试用例返回 $-1$。\n\n实现上述过程，并将其应用于以下测试套件。对于每个测试用例，按如下方式生成一个稳定矩阵 $A$ 和一个维度兼容的输入矩阵 $B$：抽取一个具有独立标准正态分布元素的随机矩阵 $M \\in \\mathbb{R}^{n \\times n}$，计算其谱半径 $\\rho(M)$，并设置\n$$\nA = \\alpha M, \\quad \\alpha = \\frac{0.9}{\\rho(M)},\n$$\n以确保 $\\rho(A) \\leq 0.9$。抽取 $B$，其元素为标准差为 $0.5$ 的独立正态分布。使用 $x_0 = 0_n$ 并模拟系统 $N_{\\text{train}} + N_{\\text{val}}$ 步。对于所有情况，使用 $\\sigma_u = 0.5$ 和 $w_k = 0$。测量噪声的标准差由每个测试用例指定。为了保证可复现性，必须使用给定的种子值来初始化随机数生成器。\n\n测试套件规格：\n- 用例1：$n = 5$, $m = 2$, $N_{\\text{train}} = 200$, $N_{\\text{val}} = 100$, 测量噪声标准差 $0.01$, 种子 $2025$, 候选阶数 $\\{1,2,3,4,5\\}$。\n- 用例2：$n = 5$, $m = 2$, $N_{\\text{train}} = 60$, $N_{\\text{val}} = 150$, 测量噪声标准差 $0.05$, 种子 $7$, 候选阶数 $\\{1,2,3,4\\}$。\n- 用例3：$n = 5$, $m = 2$, $N_{\\text{train}} = 120$, $N_{\\text{val}} = 200$, 测量噪声标准差 $0.02$, 种子 $123$, 候选阶数 $\\{1,2,3,4,5\\}$。\n- 用例4：$n = 5$, $m = 2$, $N_{\\text{train}} = 80$, $N_{\\text{val}} = 50$, 测量噪声标准差 $0.2$, 种子 $999$, 候选阶数 $\\{1,2,3\\}$。\n\n稳定性裕度使用 $\\varepsilon = 10^{-6}$。对于每个用例，根据所述规则计算选定的阶数 $r^\\star$。最终的程序输出必须是单行文本，包含四个用例的选定阶数，以逗号分隔并用方括号括起来，例如 $[r_1^\\star,r_2^\\star,r_3^\\star,r_4^\\star]$。输出不涉及角度，也不需要物理单位。所有数值应酌情使用浮点数计算，程序必须以确切指定的格式生成输出。",
            "solution": "该问题要求通过最小化留出损失来选择代理降阶模型的维度，同时强制降阶动力学的稳定性。该方法基于标准的离散时间线性系统理论和基于投影的模型降阶。\n\n从离散时间线性时不变模型开始\n$$\nx_{k+1} = A x_k + B u_k + w_k, \\quad y_k = C x_k + v_k,\n$$\n假设 $C = I_n$，$w_k = 0$，且 $A$ 是稳定的。离散时间稳定性由谱半径 $\\rho(A)$ 表征，其定义为\n$$\n\\rho(A) = \\max_i \\left| \\lambda_i(A) \\right|,\n$$\n其中 $\\lambda_i(A)$ 是 $A$ 的特征值。如果 $\\rho(A)  1$，则矩阵 $A$ 是稳定的。为确保真实系统是稳定的，我们通过缩放一个随机矩阵 $M$ 来构造 $A$：\n$$\nA = \\alpha M, \\quad \\alpha = \\frac{0.9}{\\rho(M)}.\n$$\n这保证了 $\\rho(A) \\leq 0.9$，从而保证了稳定性。\n\n代理降阶模型通过使用奇异值分解（SVD）的本征正交分解（POD）来构建。从训练输出中，\n$$\nX_{\\text{train}} = \\begin{bmatrix} y_0  y_1  \\cdots  y_T \\end{bmatrix} \\in \\mathbb{R}^{n \\times (T+1)},\n$$\n计算SVD\n$$\nX_{\\text{train}} = U \\Sigma V^\\top,\n$$\n其中 $U \\in \\mathbb{R}^{n \\times n}$ 具有标准正交列，$\\Sigma$ 包含奇异值。阶数为 $r$ 的降阶基是由 $U$ 的前 $r$ 列构成的 $U_r \\in \\mathbb{R}^{n \\times r}$。降阶坐标定义为\n$$\nz_k = U_r^\\top y_k \\in \\mathbb{R}^r.\n$$\n\n为了估计降阶动力学，我们在降阶坐标中假定一个仿射线性模型，\n$$\nz_{k+1} = A_r z_k + B_r u_k + c_r,\n$$\n并通过最小化训练序列上的预测误差平方和来拟合 $A_r \\in \\mathbb{R}^{r \\times r}$、$B_r \\in \\mathbb{R}^{r \\times m}$ 和 $c_r \\in \\mathbb{R}^r$，\n$$\n\\min_{A_r, B_r, c_r} \\sum_{k=0}^{T-1} \\left\\| z_{k+1} - A_r z_k - B_r u_k - c_r \\right\\|_2^2.\n$$\n这个最小二乘问题可以表示为矩阵形式。令 $L = T$ 为转换次数，并定义以下矩阵\n$$\nZ_0 = \\begin{bmatrix} z_0  z_1  \\cdots  z_{L-1} \\end{bmatrix} \\in \\mathbb{R}^{r \\times L}, \\quad Z_1 = \\begin{bmatrix} z_1  z_2  \\cdots  z_{L} \\end{bmatrix} \\in \\mathbb{R}^{r \\times L},\n$$\n$$\nU_0 = \\begin{bmatrix} u_0  u_1  \\cdots  u_{L-1} \\end{bmatrix} \\in \\mathbb{R}^{m \\times L}, \\quad \\mathbf{1}_L = \\begin{bmatrix} 1  1  \\cdots  1 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times L}.\n$$\n构造回归量矩阵\n$$\n\\Phi = \\begin{bmatrix} Z_0 \\\\ U_0 \\\\ \\mathbf{1}_L \\end{bmatrix} \\in \\mathbb{R}^{(r + m + 1) \\times L}.\n$$\n通过右伪逆可以获得使残差的弗罗贝尼乌斯范数最小化的最小二乘解，\n$$\n\\Theta^\\star = Z_1 \\, \\Phi^\\dagger \\in \\mathbb{R}^{r \\times (r + m + 1)},\n$$\n其中 $\\Theta^\\star$ 分块为\n$$\n\\Theta^\\star = \\begin{bmatrix} A_r  B_r  c_r \\end{bmatrix}.\n$$\n\n我们必须在降阶动力学中强制执行稳定性。降阶状态转移矩阵的离散时间稳定性条件是\n$$\n\\rho(A_r)  1 - \\varepsilon,\n$$\n其中 $\\varepsilon > 0$ 是一个小的数值裕度。不满足此条件的阶数 $r$ 将被排除在考虑范围之外。\n\n对于留出验证，在验证输入和初始降阶状态 $z_0^{\\text{val}} = U_r^\\top y_0^{\\text{val}}$ 上模拟降阶模型：\n$$\nz_{k+1}^{\\text{val}} = A_r z_k^{\\text{val}} + B_r u_k^{\\text{val}} + c_r,\n$$\n然后重构输出\n$$\n\\hat{y}_k = U_r z_k^{\\text{val}}.\n$$\n计算在整个验证时域内的均方误差（MSE），\n$$\nE_r = \\frac{1}{N_{\\text{val}} \\cdot n} \\sum_{k=0}^{N_{\\text{val}}-1} \\left\\| \\hat{y}_k - y_k^{\\text{val}} \\right\\|_2^2.\n$$\n选定的阶数是\n$$\nr^\\star = \\arg\\min_{r \\in \\mathcal{R}} E_r \\quad \\text{subject to} \\quad \\rho(A_r)  1 - \\varepsilon,\n$$\n其中 $\\mathcal{R}$ 是候选阶数集。如果在数值容差范围内 $E_r$ 出现相同结果，则选择最小的 $r$ 以支持简约性。如果没有候选阶数能产生稳定的降阶动力学，则返回 $-1$。\n\n算法实现细节：\n- 每个测试用例使用固定的随机数生成器种子以保证可复现性。\n- 使用 $\\alpha = 0.9 / \\rho(M)$ 缩放随机矩阵 $M$ 来生成 $A$，以确保 $\\rho(A) \\leq 0.9$。\n- 抽取 $B$，其元素为标准差为 $0.5$ 的独立正态分布。\n- 从 $x_0 = 0_n$ 开始，模拟系统 $N_{\\text{train}} + N_{\\text{val}}$ 步，其中 $u_k \\sim \\mathcal{N}(0, \\sigma_u^2 I_m)$，测量噪声 $v_k \\sim \\mathcal{N}(0, \\sigma_v^2 I_n)$，且 $\\sigma_u = 0.5$，$\\sigma_v$ 由每个测试用例指定。\n- 使用 $X_{\\text{train}}$ 的SVD构建 $U_r$，然后投影到降阶坐标并通过最小二乘法拟合 $(A_r, B_r, c_r)$。\n- 通过检查 $A_r$ 的谱半径来强制执行稳定性约束。\n- 执行验证展开并计算 $E_r$。\n- 在稳定的候选者中，基于最小的 $E_r$ 选择 $r^\\star$，并以 $r$ 的大小作为决胜标准。\n\n测试套件定义：\n- 用例1：$n = 5$, $m = 2$, $N_{\\text{train}} = 200$, $N_{\\text{val}} = 100$, $\\sigma_v = 0.01$, 种子 $2025$, $\\mathcal{R} = \\{1,2,3,4,5\\}$。\n- 用例2：$n = 5$, $m = 2$, $N_{\\text{train}} = 60$, $N_{\\text{val}} = 150$, $\\sigma_v = 0.05$, 种子 $7$, $\\mathcal{R} = \\{1,2,3,4\\}$。\n- 用例3：$n = 5$, $m = 2$, $N_{\\text{train}} = 120$, $N_{\\text{val}} = 200$, $\\sigma_v = 0.02$, 种子 $123$, $\\mathcal{R} = \\{1,2,3,4,5\\}$。\n- 用例4：$n = 5$, $m = 2$, $N_{\\text{train}} = 80$, $N_{\\text{val}} = 50$, $\\sigma_v = 0.2$, 种子 $999$, $\\mathcal{R} = \\{1,2,3\\}$。\n\n使用 $\\varepsilon = 10^{-6}$。程序必须输出单行文本，包含四个用例的选定阶数，格式为 $[r_1^\\star,r_2^\\star,r_3^\\star,r_4^\\star]$。这种格式确保解决方案是可量化和可测试的，每个元素是对应测试用例的整数结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_stable_system(n, m, rng):\n    \"\"\"\n    Generate a stable discrete-time linear system (A, B).\n    A is constructed by scaling a random matrix M so that its spectral radius is = 0.9.\n    B is drawn with normal entries with std 0.5.\n    \"\"\"\n    M = rng.normal(size=(n, n))\n    # Compute spectral radius of M\n    eigvals = np.linalg.eigvals(M)\n    rho_M = np.max(np.abs(eigvals))\n    # Scale to ensure stability: spectral radius = 0.9\n    alpha = 0.9 / rho_M if rho_M > 0 else 0.0\n    A = alpha * M\n    B = rng.normal(scale=0.5, size=(n, m))\n    return A, B\n\ndef simulate_system(A, B, u, noise_std, rng):\n    \"\"\"\n    Simulate the system x_{k+1} = A x_k + B u_k, y_k = x_k + v_k.\n    Inputs:\n        A: (n,n) system matrix\n        B: (n,m) input matrix\n        u: (T, m) input sequence\n        noise_std: float, standard deviation of measurement noise v_k\n    Returns:\n        y: (T, n) measured outputs\n        x: (T, n) true states\n    \"\"\"\n    n, m = B.shape\n    T = u.shape[0]\n    x = np.zeros((T + 1, n))\n    y = np.zeros((T, n))\n    xk = np.zeros(n)\n    for k in range(T):\n        # Output with measurement noise\n        vk = rng.normal(scale=noise_std, size=n) if noise_std > 0 else np.zeros(n)\n        y[k] = xk + vk\n        # State update\n        xkp1 = A @ xk + B @ u[k]\n        x[k] = xk\n        xk = xkp1\n    x[T] = xk\n    return y, x[:-1]\n\ndef build_pod_basis(y_train, r):\n    \"\"\"\n    Build POD basis U_r using SVD of training data snapshots.\n    y_train: (N_train, n)\n    r: reduced order\n    Returns:\n        U_r: (n, r)\n    \"\"\"\n    # Snapshots as (n, T+1) matrix; here use all y_train columns\n    X = y_train.T  # shape (n, N_train)\n    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n    U_r = U[:, :r]\n    return U_r\n\ndef estimate_reduced_dynamics(U_r, y_train, u_train):\n    \"\"\"\n    Estimate (A_r, B_r, c_r) via least squares in reduced coordinates.\n    y_train: (N_train, n)\n    u_train: (N_train, m)\n    Returns:\n        A_r: (r, r)\n        B_r: (r, m)\n        c_r: (r,)\n    \"\"\"\n    # Project training outputs to reduced coordinates\n    Z = (U_r.T @ y_train.T)  # shape (r, N_train)\n    # Use transitions from k=0..N_train-2 to k+1\n    L = Z.shape[1] - 1\n    Z0 = Z[:, :L]            # (r, L)\n    Z1 = Z[:, 1:L+1]         # (r, L)\n    U0 = u_train[:L, :].T    # (m, L)\n    ones = np.ones((1, L))   # (1, L)\n    Phi = np.vstack([Z0, U0, ones])  # (r+m+1, L)\n    # Least squares via pseudoinverse\n    Theta = Z1 @ np.linalg.pinv(Phi)  # (r, r+m+1)\n    r = U_r.shape[1]\n    m = u_train.shape[1]\n    A_r = Theta[:, :r]\n    B_r = Theta[:, r:r+m]\n    c_r = Theta[:, -1]\n    return A_r, B_r, c_r\n\ndef spectral_radius(A):\n    \"\"\"\n    Compute spectral radius of matrix A.\n    \"\"\"\n    if A.size == 0:\n        return 0.0\n    eigvals = np.linalg.eigvals(A)\n    return np.max(np.abs(eigvals))\n\ndef validate_model(U_r, A_r, B_r, c_r, y_val, u_val):\n    \"\"\"\n    Validate reduced model by rollout on validation data and compute MSE.\n    y_val: (N_val, n)\n    u_val: (N_val, m)\n    Returns:\n        mse: float\n    \"\"\"\n    n = U_r.shape[0]\n    r = U_r.shape[1]\n    N_val = y_val.shape[0]\n    # Initialize reduced state from first validation measurement\n    z = np.zeros((N_val, r))\n    z[0] = U_r.T @ y_val[0]\n    # Rollout\n    for k in range(N_val - 1):\n        z[k+1] = A_r @ z[k] + B_r @ u_val[k] + c_r\n    # Reconstruct outputs\n    y_hat = (U_r @ z.T).T  # (N_val, n)\n    # Compute mean squared error across all times and dimensions\n    err = y_hat - y_val\n    mse = np.mean(err**2)\n    return mse\n\ndef select_order(orders, y_train, u_train, y_val, u_val, epsilon):\n    \"\"\"\n    Cross-validate across candidate orders, enforcing stability.\n    Returns:\n        selected_r: int (best order), or -1 if no stable candidate\n    \"\"\"\n    best_r = None\n    best_mse = np.inf\n    mse_tie_tol = 1e-12\n    n = y_train.shape[1]\n    for r in orders:\n        if r = 0 or r > n:\n            continue\n        # Build POD basis\n        U_r = build_pod_basis(y_train, r)\n        # Estimate reduced dynamics\n        A_r, B_r, c_r = estimate_reduced_dynamics(U_r, y_train, u_train)\n        # Stability check\n        rho = spectral_radius(A_r)\n        if not (rho  1.0 - epsilon):\n            continue\n        # Validate\n        mse = validate_model(U_r, A_r, B_r, c_r, y_val, u_val)\n        # Select by minimal MSE, tie-break by smaller r\n        if (mse  best_mse - mse_tie_tol) or (abs(mse - best_mse) = mse_tie_tol and (best_r is None or r  best_r)):\n            best_mse = mse\n            best_r = r\n    return best_r if best_r is not None else -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (n, m, N_train, N_val, noise_std, seed, orders)\n    test_cases = [\n        (5, 2, 200, 100, 0.01, 2025, [1, 2, 3, 4, 5]),\n        (5, 2, 60, 150, 0.05, 7,    [1, 2, 3, 4]),\n        (5, 2, 120, 200, 0.02, 123, [1, 2, 3, 4, 5]),\n        (5, 2, 80, 50,  0.2,  999,  [1, 2, 3]),\n    ]\n    epsilon = 1e-6\n    sigma_u = 0.5\n\n    results = []\n    for case in test_cases:\n        n, m, N_train, N_val, noise_std, seed, orders = case\n        rng = np.random.default_rng(seed)\n        # Generate system\n        A, B = generate_stable_system(n, m, rng)\n        # Generate inputs for full horizon\n        T_total = N_train + N_val\n        u = rng.normal(scale=sigma_u, size=(T_total, m))\n        # Simulate system\n        y, _ = simulate_system(A, B, u, noise_std, rng)\n        # Split into train and validation\n        y_train = y[:N_train]\n        y_val = y[N_train:]\n        u_train = u[:N_train]\n        u_val = u[N_train:]\n        # Select order\n        selected_r = select_order(orders, y_train, u_train, y_val, u_val, epsilon)\n        results.append(selected_r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "现在，我们将视野从线性降阶模型扩展到非线性的概率代理模型，例如高斯过程（GP），这对于行为复杂或需要量化不确定性的系统至关重要。本练习旨在解决创建“活的”数字孪生中的一个关键挑战：当新数据流式传入时，如何高效地更新模型。您将亲手实践高斯过程序贯更新背后的数值线性代数技巧，这对于实现实时模型自适应至关重要。",
            "id": "4249065",
            "problem": "一个信息物理系统的数字孪生需要一个能够同化流式测量数据同时保持概率上一致预测的实时代理模型。考虑一个用于标量无量纲函数的高斯过程代理模型，该模型基于一个零均值高斯先验，其平稳协方差由一个平方指数核函数指定。观测值受到方差已知的独立零均值高斯噪声的污染。当追加单个新样本时，代理模型必须进行顺序更新。目标是通过更新正则化协方差矩阵的 Cholesky 因子并仅使用三角求解，来高效地计算查询输入处的更新后预测均值，而不是从头重新计算矩阵分解。\n\n从联合高斯随机变量的基本性质（有限维多元高斯分布的存在性及多元高斯分布的条件化法则）和对称正定矩阵的 Cholesky 分解的定义出发，推导出一个当新样本被追加时，对下三角 Cholesky 因子和预测均值进行更新的数值稳定且计算高效的顺序更新方法。您的推导必须遵循以下约束：\n- 第 $k$ 步的正则化协方差矩阵，记为 $\\mathbf{C}_k$，等于 $\\mathbf{K}_k + \\sigma^2 \\mathbf{I}$，其中 $\\mathbf{K}_k$ 是在当前训练输入上求值的核函数的 Gram 矩阵，$\\sigma^2$ 是观测噪声方差。\n- 该更新必须使用一个有效的下三角追加操作，该操作与添加单个新输入-输出对 $(x_{k+1}, y_{k+1})$ 后的增广正则化协方差 $\\mathbf{C}_{k+1}$ 的块 Cholesky 分解相一致。\n- 查询输入处的预测均值必须仅通过使用更新后的下三角 Cholesky 因子的三角求解来计算，不进行任何矩阵求逆，也不从头重新计算分解。\n\n您必须在单个可运行的程序中实现以下内容：\n- 使用平方指数核函数 $k(x, x') = \\sigma_f^2 \\exp\\!\\big(-\\frac{(x - x')^2}{2 \\ell^2}\\big)$，其超参数 $\\sigma_f$ 和 $\\ell$ 在每个测试用例中提供。\n- 使用零先验均值和方差为 $\\sigma^2$ 的独立高斯观测噪声。\n- 维护 $\\mathbf{C}_k$ 的下三角 Cholesky 因子 $\\mathbf{L}_k$，并在追加一个新样本后，通过执行一个从第一性原理（多元高斯条件化和 Cholesky 分解的性质）推导出的单块追加操作，将其更新为 $\\mathbf{L}_{k+1}$。此更新必须依赖于三角求解和一个标量平方根，并且不得从头重新计算完整的分解。\n- 使用 $\\mathbf{L}_{k+1}$ 和与训练输入的核交叉协方差，仅通过三角求解来计算查询输入 $\\mathbf{X}_*$ 处的预测均值，该方法需与多元高斯分布的条件化法则保持一致。\n- 为进行验证，同时使用对完整的增广正则化协方差进行朴素的从头重算分解的方法来计算预测均值。\n- 对于每个测试用例，返回顺序更新方法得到的预测均值与朴素从头计算方法得到的预测均值之间的最大绝对差，作为一个浮点数。\n\n所有量均为无量纲。您必须在用于分解的每个正则化协方差矩阵的对角线上添加一个小的正对角正则化项（数值抖动），以确保数值稳定性。\n\n测试套件：\n- 用例 A（正常路径）：核函数超参数 $\\sigma_f = 1.2$, $\\ell = 0.7$，噪声标准差 $\\sigma = 0.05$（因此噪声方差 $\\sigma^2 = 0.0025$），抖动 $10^{-12}$。初始训练输入 $\\mathbf{X}_0 = [-1.0, 0.0, 1.0]$，初始输出 $\\mathbf{y}_0$ 由 $y = \\sin(1.5 x)$ 在 $\\mathbf{X}_0$ 处求值定义。新样本 $(x_{1}, y_{1})$，其中 $x_{1} = 0.5$ 且 $y_{1} = \\sin(1.5 \\cdot 0.5)$。查询输入 $\\mathbf{X}_* = [-0.75, 0.25, 1.25]$。\n- 用例 B（边界情况，初始集为空）：超参数与用例 A 相同。初始训练集为空：$\\mathbf{X}_0 = [\\,]$, $\\mathbf{y}_0 = [\\,]$。新样本 $(x_{1}, y_{1})$，其中 $x_{1} = 0.0$ 且 $y_{1} = \\sin(1.5 \\cdot 0.0)$。查询输入 $\\mathbf{X}_* = [-1.0, 1.0]$。\n- 用例 C（边缘情况，近似重复的输入）：超参数与用例 A 相同。初始训练输入 $\\mathbf{X}_0 = [0.0, 1.0]$，初始输出 $\\mathbf{y}_0$ 由 $y = \\cos(2.0 x)$ 在 $\\mathbf{X}_0$ 处求值定义。新样本 $(x_{1}, y_{1})$，其中 $x_{1} = 1.0 + 10^{-8}$ 且 $y_{1} = \\cos(2.0 \\cdot (1.0 + 10^{-8}))$。查询输入 $\\mathbf{X}_* = [0.5, 1.0]$。\n\n输出规格：\n- 对于每个测试用例，计算一个浮点数，其值等于在所有提供的查询输入上，顺序更新方法和朴素从头计算方法所得预测均值之间的最大绝对差。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_1, r_2, r_3]$，其中每个 $r_i$ 是对应测试用例（按 A、B、C 顺序）的浮点结果。\n\n该程序必须是自包含的，无需用户输入，并且必须严格遵守指定的输出格式。",
            "solution": "该问题要求推导并实现一种用于高斯过程（GP）预测均值的顺序更新算法。这是数字孪生等实时应用的常见需求，在这些应用中数据是顺序到达的。其核心思想是高效地更新正则化协方差矩阵的 Cholesky 分解，从而避免完全的重新计算。\n\n### 高斯过程预测基础\n\n高斯过程定义了函数上的一个分布。我们假设函数 $f(x)$ 服从一个零均值高斯过程先验：\n$$\nf(x) \\sim \\mathcal{GP}(0, k(x, x'))\n$$\n其中 $k(x, x')$ 是平方指数核函数：\n$$\nk(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right)\n$$\n此处，$\\sigma_f^2$ 是信号方差，$\\ell$ 是特征长度尺度。\n\n假设在训练输入 $\\mathbf{X}$ 处的观测值 $\\mathbf{y}$ 是含噪声的，其噪声为独立同分布的高斯噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$：\n$$\ny = f(x) + \\epsilon\n$$\n给定 $k$ 个训练点 $(\\mathbf{X}_k, \\mathbf{y}_k)$ 和一组查询输入 $\\mathbf{X}_*$, 训练输出 $\\mathbf{y}_k$ 和在查询点处的函数值 $\\mathbf{f}_*$ 的联合分布是一个多元高斯分布：\n$$\n\\begin{pmatrix} \\mathbf{y}_k \\\\ \\mathbf{f}_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} \\mathbf{K}_k + \\sigma^2\\mathbf{I}  \\mathbf{K}_{k,*} \\\\ \\mathbf{K}_{*,k}  \\mathbf{K}_{*,*} \\end{pmatrix} \\right)\n$$\n其中 $\\mathbf{K}_k = k(\\mathbf{X}_k, \\mathbf{X}_k)$，$\\mathbf{K}_{*,*} = k(\\mathbf{X}_*, \\mathbf{X}_*)$，以及 $\\mathbf{K}_{*,k} = \\mathbf{K}_{k,*}^T = k(\\mathbf{X}_*, \\mathbf{X}_k)$。令 $\\mathbf{C}_k = \\mathbf{K}_k + \\sigma^2\\mathbf{I}$ 为观测值的正则化协方差矩阵。\n\n利用多元高斯分布的条件化法则，给定数据时 $\\mathbf{f}_*$ 的后验分布也是高斯分布，$p(\\mathbf{f}_*|\\mathbf{X}_*, \\mathbf{X}_k, \\mathbf{y}_k) = \\mathcal{N}(\\bar{\\mathbf{f}}_*, \\text{cov}(\\mathbf{f}_*))$。我们关心的是预测均值 $\\bar{\\mathbf{f}}_*$：\n$$\n\\bar{\\mathbf{f}}_* = \\mathbf{K}_{*,k} \\mathbf{C}_k^{-1} \\mathbf{y}_k\n$$\n直接对 $\\mathbf{C}_k$ 求逆在计算上是昂贵的（复杂度为 $k$ 的三次方）且数值不稳定。一种标准的、更稳定的方法是使用 $\\mathbf{C}_k$ 的 Cholesky 分解，$\\mathbf{C}_k$ 是对称正定的（由噪声方差 $\\sigma^2 > 0$ 和使用半正定核保证）。令 $\\mathbf{C}_k = \\mathbf{L}_k \\mathbf{L}_k^T$，其中 $\\mathbf{L}_k$ 是一个下三角矩阵。预测均值可以计算为：\n$$\n\\bar{\\mathbf{f}}_* = \\mathbf{K}_{*,k} (\\mathbf{L}_k \\mathbf{L}_k^T)^{-1} \\mathbf{y}_k = \\mathbf{K}_{*,k} (\\mathbf{L}_k^T)^{-1} \\mathbf{L}_k^{-1} \\mathbf{y}_k\n$$\n这个问题可以通过定义一个中间向量 $\\boldsymbol{\\beta} = \\mathbf{C}_k^{-1} \\mathbf{y}_k$ 并通过两次三角求解来高效地解决：\n1.  求解 $\\mathbf{L}_k \\boldsymbol{\\alpha} = \\mathbf{y}_k$ 以获得 $\\boldsymbol{\\alpha}$（使用前向代入）。\n2.  求解 $\\mathbf{L}_k^T \\boldsymbol{\\beta} = \\boldsymbol{\\alpha}$ 以获得 $\\boldsymbol{\\beta}$（使用反向代入）。\n那么预测均值为 $\\bar{\\mathbf{f}}_* = \\mathbf{K}_{*,k} \\boldsymbol{\\beta}$。\n\n### Cholesky 因子的顺序更新\n\n当观测到一个新的数据点 $(x_{k+1}, y_{k+1})$ 时，训练集变为 $(\\mathbf{X}_{k+1}, \\mathbf{y}_{k+1})$，其中 $\\mathbf{X}_{k+1} = [\\mathbf{X}_k^T, x_{k+1}]^T$ 且 $\\mathbf{y}_{k+1} = [\\mathbf{y}_k^T, y_{k+1}]^T$。新的正则化协方差矩阵 $\\mathbf{C}_{k+1}$ 具有块结构：\n$$\n\\mathbf{C}_{k+1} = k(\\mathbf{X}_{k+1}, \\mathbf{X}_{k+1}) + \\sigma^2\\mathbf{I} =\n\\begin{pmatrix}\nk(\\mathbf{X}_k, \\mathbf{X}_k) + \\sigma^2\\mathbf{I}  k(\\mathbf{X}_k, x_{k+1}) \\\\\nk(x_{k+1}, \\mathbf{X}_k)  k(x_{k+1}, x_{k+1}) + \\sigma^2\n\\end{pmatrix}\n$$\n令 $\\mathbf{k}_{k+1} = k(\\mathbf{X}_k, x_{k+1})$ 且 $c_{k+1,k+1} = k(x_{k+1}, x_{k+1}) + \\sigma^2$。为了增强数值稳定性，一个小的正抖动项 $\\delta_{jitter}$ 被加到对角线上，因此 $c_{k+1,k+1} \\to c_{k+1,k+1} + \\delta_{jitter}$ 且 $\\mathbf{C}_k \\to \\mathbf{K}_k + (\\sigma^2 + \\delta_{jitter})\\mathbf{I}$。隐式地包含这个抖动项后，我们有：\n$$\n\\mathbf{C}_{k+1} = \\begin{pmatrix} \\mathbf{C}_k  \\mathbf{k}_{k+1} \\\\ \\mathbf{k}_{k+1}^T  c_{k+1,k+1} \\end{pmatrix}\n$$\n我们寻求新的 Cholesky 因子 $\\mathbf{L}_{k+1}$ 使得 $\\mathbf{C}_{k+1} = \\mathbf{L}_{k+1} \\mathbf{L}_{k+1}^T$。我们假设 $\\mathbf{L}_{k+1}$ 也具有一个块下三角形式，该形式建立在已有因子 $\\mathbf{L}_k$ 的基础上：\n$$\n\\mathbf{L}_{k+1} = \\begin{pmatrix} \\mathbf{L}_k  \\mathbf{0} \\\\ \\mathbf{l}_{k+1}^T  l_{k+1,k+1} \\end{pmatrix}\n$$\n其中 $\\mathbf{l}_{k+1}$ 是一个待定的列向量，$l_{k+1,k+1}$ 是一个待定的标量。展开乘积 $\\mathbf{L}_{k+1} \\mathbf{L}_{k+1}^T$：\n$$\n\\mathbf{L}_{k+1} \\mathbf{L}_{k+1}^T = \\begin{pmatrix} \\mathbf{L}_k  \\mathbf{0} \\\\ \\mathbf{l}_{k+1}^T  l_{k+1,k+1} \\end{pmatrix} \\begin{pmatrix} \\mathbf{L}_k^T  \\mathbf{l}_{k+1} \\\\ \\mathbf{0}^T  l_{k+1,k+1} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{L}_k\\mathbf{L}_k^T  \\mathbf{L}_k\\mathbf{l}_{k+1} \\\\ \\mathbf{l}_{k+1}^T\\mathbf{L}_k^T  \\mathbf{l}_{k+1}^T\\mathbf{l}_{k+1} + l_{k+1,k+1}^2 \\end{pmatrix}\n$$\n通过令 $\\mathbf{C}_{k+1}$ 和 $\\mathbf{L}_{k+1}\\mathbf{L}_{k+1}^T$ 的各个块相等，我们推导出更新规则：\n1.  左上角的块给出 $\\mathbf{L}_k\\mathbf{L}_k^T = \\mathbf{C}_k$，根据我们的初始前提，这是成立的。\n2.  非对角块给出 $\\mathbf{L}_k\\mathbf{l}_{k+1} = \\mathbf{k}_{k+1}$。由于 $\\mathbf{L}_k$ 是已知的下三角矩阵，我们可以通过一次前向代入高效地解出未知向量 $\\mathbf{l}_{k+1}$。这在计算上比完整的矩阵分解要便宜得多。\n3.  右下角元素给出 $\\mathbf{l}_{k+1}^T\\mathbf{l}_{k+1} + l_{k+1,k+1}^2 = c_{k+1,k+1}$。在求得 $\\mathbf{l}_{k+1}$ 后，我们可以解出 $l_{k+1,k+1}$：\n    $$\n    l_{k+1,k+1} = \\sqrt{c_{k+1,k+1} - \\mathbf{l}_{k+1}^T\\mathbf{l}_{k+1}}\n    $$\n    平方根下的项是 $\\mathbf{C}_k$ 在 $\\mathbf{C}_{k+1}$ 中的 Schur 补，为了使 $\\mathbf{C}_{k+1}$ 是正定的，该项必须为正。抖动和噪声项有助于确保这一点。\n\n推导至此完成。从 $\\mathbf{L}_k$ 到 $\\mathbf{L}_{k+1}$ 的更新仅需要计算一个新的核向量 $\\mathbf{k}_{k+1}$、一次三角求解、一次向量点积和一次标量平方根。一旦构造出 $\\mathbf{L}_{k+1}$，就可以通过构建新的交叉协方差矩阵 $\\mathbf{K}_{*,k+1} = k(\\mathbf{X}_*, \\mathbf{X}_{k+1})$ 并使用先前描述的两次三角求解（现在使用 $\\mathbf{L}_{k+1}$ 和 $\\mathbf{y}_{k+1}$）来找到查询点 $\\mathbf{X}_*$ 处的预测均值。这种顺序方法避免了从头重新计算完整的 $(k+1) \\times (k+1)$ 矩阵的 Cholesky 分解。\n\n作为比较，朴素的从头计算方法涉及重新构建整个矩阵 $\\mathbf{C}_{k+1}$ 并执行完整的 Cholesky 分解。在数学上，两种方法产生相同的结果，但顺序更新提供了显著的计算节省，尤其是在 $k$ 很大时。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and comparing sequential and naive\n    Gaussian Process regression updates for three test cases.\n    \"\"\"\n\n    def squared_exponential_kernel(X1, X2, sigma_f, length_scale):\n        \"\"\"\n        Computes the squared exponential kernel matrix between two sets of inputs.\n        Inputs X1 and X2 are expected to be column vectors (n_points, 1).\n        \"\"\"\n        if X1.size == 0 or X2.size == 0:\n            return np.empty((X1.shape[0], X2.shape[0]))\n        \n        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n        return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n\n    def compute_predictive_mean(X_query, X_train, y_train, L, sigma_f, length_scale):\n        \"\"\"\n        Computes the GP predictive mean using a pre-computed Cholesky factor L.\n        \"\"\"\n        if X_train.size == 0:\n            return np.zeros(X_query.shape[0])\n            \n        K_star = squared_exponential_kernel(X_query, X_train, sigma_f, length_scale)\n        \n        # Solve L * alpha = y for alpha (forward substitution)\n        alpha = solve_triangular(L, y_train, lower=True)\n        \n        # Solve L.T * beta = alpha for beta (backward substitution)\n        beta = solve_triangular(L.T, alpha, lower=False)\n        \n        # Predictive mean is K_star @ beta\n        return K_star @ beta\n\n    test_cases = [\n        {\n            \"name\": \"Case A (happy path)\",\n            \"params\": {\"sigma_f\": 1.2, \"l\": 0.7, \"sigma\": 0.05, \"jitter\": 1e-12},\n            \"initial_X\": np.array([-1.0, 0.0, 1.0]),\n            \"y_func\": lambda x: np.sin(1.5 * x),\n            \"new_x\": 0.5,\n            \"query_X\": np.array([-0.75, 0.25, 1.25]),\n        },\n        {\n            \"name\": \"Case B (boundary, empty initial set)\",\n            \"params\": {\"sigma_f\": 1.2, \"l\": 0.7, \"sigma\": 0.05, \"jitter\": 1e-12},\n            \"initial_X\": np.array([]),\n            \"y_func\": lambda x: np.sin(1.5 * x),\n            \"new_x\": 0.0,\n            \"query_X\": np.array([-1.0, 1.0]),\n        },\n        {\n            \"name\": \"Case C (edge, near-duplicate input)\",\n            \"params\": {\"sigma_f\": 1.2, \"l\": 0.7, \"sigma\": 0.05, \"jitter\": 1e-12},\n            \"initial_X\": np.array([0.0, 1.0]),\n            \"y_func\": lambda x: np.cos(2.0 * x),\n            \"new_x\": 1.0 + 1e-8,\n            \"query_X\": np.array([0.5, 1.0]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        params = case[\"params\"]\n        sigma_f = params[\"sigma_f\"]\n        length_scale = params[\"l\"]\n        noise_var = params[\"sigma\"]**2\n        jitter = params[\"jitter\"]\n\n        # Prepare initial data\n        X_k = case[\"initial_X\"].reshape(-1, 1)\n        y_k = case[\"y_func\"](X_k.ravel())\n\n        # Prepare new data point\n        x_new = np.array([[case[\"new_x\"]]])\n        y_new = case[\"y_func\"](x_new.ravel())\n\n        # Prepare query points\n        X_query = case[\"query_X\"].reshape(-1, 1)\n\n        # --------- Sequential Update Method ---------\n        \n        # 1. Initial Cholesky factor L_k\n        k = X_k.shape[0]\n        if k > 0:\n            K_k = squared_exponential_kernel(X_k, X_k, sigma_f, length_scale)\n            C_k = K_k + np.eye(k) * (noise_var + jitter)\n            L_k = cholesky(C_k, lower=True)\n        else:\n            L_k = np.empty((0, 0))\n\n        # 2. Update L_k to L_{k+1}\n        # Compute vector k_{k+1} = k(X_k, x_{k+1})\n        k_vec = squared_exponential_kernel(X_k, x_new, sigma_f, length_scale)\n        \n        # Solve L_k * l_{k+1} = k_{k+1}\n        if k > 0:\n            l_vec = solve_triangular(L_k, k_vec, lower=True)\n        else:\n            l_vec = np.empty((0, 1))\n\n        # Compute scalar l_{k+1, k+1}\n        c_scalar = squared_exponential_kernel(x_new, x_new, sigma_f, length_scale)[0, 0] + noise_var + jitter\n        l_scalar_sq = c_scalar - np.dot(l_vec.T, l_vec)\n        l_scalar = np.sqrt(l_scalar_sq)\n        \n        # 3. Assemble new Cholesky factor L_{k+1}\n        L_k_plus_1_seq = np.zeros((k + 1, k + 1))\n        if k > 0:\n            L_k_plus_1_seq[:k, :k] = L_k\n            L_k_plus_1_seq[k, :k] = l_vec.T\n        L_k_plus_1_seq[k, k] = l_scalar\n\n        # 4. Compute predictive mean using updated factor\n        X_k_plus_1 = np.vstack((X_k, x_new))\n        y_k_plus_1 = np.concatenate((y_k, y_new))\n        mean_seq = compute_predictive_mean(X_query, X_k_plus_1, y_k_plus_1, L_k_plus_1_seq, sigma_f, length_scale)\n\n        # --------- Naive From-Scratch Method ---------\n        \n        # 1. Form full augmented covariance matrix C_{k+1}\n        K_k_plus_1_naive = squared_exponential_kernel(X_k_plus_1, X_k_plus_1, sigma_f, length_scale)\n        C_k_plus_1_naive = K_k_plus_1_naive + np.eye(k + 1) * (noise_var + jitter)\n        \n        # 2. Compute Cholesky factor from scratch\n        L_k_plus_1_naive = cholesky(C_k_plus_1_naive, lower=True)\n        \n        # 3. Compute predictive mean\n        mean_naive = compute_predictive_mean(X_query, X_k_plus_1, y_k_plus_1, L_k_plus_1_naive, sigma_f, length_scale)\n\n        # --------- Comparison ---------\n        max_abs_diff = np.max(np.abs(mean_seq - mean_naive))\n        results.append(max_abs_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}