{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a first-principles derivation of the Koopman spectrum for a canonical dynamical system. This exercise demystifies the abstract definitions of Koopman operators by applying them to a simple rotation on a circle, a system whose behavior is intuitively clear. By connecting the operator's spectral properties to the familiar framework of Fourier analysis, this practice builds a concrete understanding of Koopman modes and eigenvalues and their relationship to the underlying dynamics .",
            "id": "4219100",
            "problem": "A digital twin of a synchronous oscillator in a cyber-physical system represents the oscillatorâ€™s phase by the discrete-time map on the circle given by $x_{k+1} = x_{k} + \\omega \\pmod{2\\pi}$, where $x_{k} \\in [0,2\\pi)$ is the phase at discrete time $k \\in \\mathbb{Z}_{\\ge 0}$ and $\\omega \\in \\mathbb{R}$ is a constant angular increment. Angles are measured in radians. The digital twin uses the Koopman operator framework to propagate observables $f$ of the phase. The Koopman operator $\\mathcal{U}$ acts on an observable $f$ by composition with the dynamics, i.e., $(\\mathcal{U}f)(x) = f(x+\\omega \\pmod{2\\pi})$. Consider the Hilbert space $L^{2}([0,2\\pi),\\mathrm{d}x/(2\\pi))$ of square-integrable complex-valued observables with respect to the normalized Lebesgue (invariant) measure on the circle. In practice, Dynamic Mode Decomposition (DMD) is used to estimate spectral properties of $\\mathcal{U}$ from data, but here you are to work from first principles.\n\nStarting only from the definition of the Koopman operator and standard facts about $L^{2}$ spaces and Fourier series, do the following:\n\n- Construct a complete set of Koopman eigenfunctions in $L^{2}([0,2\\pi))$ and derive their associated eigenvalues in closed form as functions of $\\omega$.\n- Explain, in terms of basic properties of rotations on the circle and orthogonality of complex exponentials, how these eigenfunctions span $L^{2}([0,2\\pi))$ and why the Koopman operator is unitary on this space.\n- Interpret the Koopman spectrum for the cases where $\\omega/(2\\pi)$ is rational versus irrational, addressing whether the point spectrum is finite, countable, and whether it is dense on the unit circle. Briefly relate the spectral nature to what an idealized, noiseless Dynamic Mode Decomposition (DMD) would recover from uniformly sampled data along an orbit.\n\nFinally, let $f_{n}(x) = \\exp(\\mathrm{i} n x)$ for an arbitrary but fixed integer $n \\in \\mathbb{Z}$. Report, as your final answer, the Koopman eigenvalue $\\lambda_{n}$ associated with $f_{n}$, expressed in exact symbolic form as a function of $n$ and $\\omega$. No numerical approximation is required, and no units are to be included in the final answer.",
            "solution": "The problem statement presented is a well-posed and scientifically grounded exercise in the application of Koopman operator theory to a fundamental dynamical system, the rigid rotation on a circle. It is a standard model used to introduce the core concepts of Koopman spectral analysis. The problem is self-contained, mathematically precise, and free of any factual or logical inconsistencies. All terms are standard within the fields of dynamical systems and functional analysis. Therefore, the problem is valid and a solution will be provided.\n\nThe system dynamics are given by the map $T: [0, 2\\pi) \\to [0, 2\\pi)$, defined as $T(x) = x + \\omega \\pmod{2\\pi}$. The Koopman operator $\\mathcal{U}$ associated with this map acts on a function $f$ in the Hilbert space $\\mathcal{H} = L^{2}([0,2\\pi), \\frac{\\mathrm{d}x}{2\\pi})$ by composition with the dynamics: $(\\mathcal{U}f)(x) = f(T(x)) = f(x+\\omega \\pmod{2\\pi})$. The inner product on this space is $\\langle g, h \\rangle = \\int_0^{2\\pi} g(x) \\overline{h(x)} \\frac{\\mathrm{d}x}{2\\pi}$.\n\nAn eigenfunction of the Koopman operator, also known as a Koopman mode, is a non-zero function $f \\in \\mathcal{H}$ that satisfies the eigenvalue equation:\n$$\n\\mathcal{U}f = \\lambda f\n$$\nfor some complex scalar $\\lambda$, the corresponding Koopman eigenvalue. Substituting the definition of the operator, this equation becomes:\n$$\nf(x+\\omega \\pmod{2\\pi}) = \\lambda f(x)\n$$\n\nTo find the eigenfunctions and eigenvalues, we leverage the structure of the Hilbert space $L^{2}([0,2\\pi))$. A fundamental result in Fourier analysis states that the set of complex exponential functions $\\{ \\exp(\\mathrm{i} n x) \\}_{n \\in \\mathbb{Z}}$ forms a complete orthonormal basis for this space. We test these basis functions as candidates for Koopman eigenfunctions.\n\nLet us define $f_n(x) = \\exp(\\mathrm{i} n x)$ for any integer $n \\in \\mathbb{Z}$. These functions are members of $L^{2}([0,2\\pi))$ as their squared modulus is $|\\exp(\\mathrm{i} n x)|^2 = 1$, which is clearly integrable over $[0, 2\\pi)$. We now apply the Koopman operator $\\mathcal{U}$ to $f_n(x)$:\n$$\n(\\mathcal{U}f_n)(x) = f_n(x+\\omega \\pmod{2\\pi}) = \\exp(\\mathrm{i} n (x+\\omega \\pmod{2\\pi}))\n$$\nSince $\\exp(\\mathrm{i} z)$ is $2\\pi$-periodic in its argument $z$, and $n$ is an integer, the function $\\exp(\\mathrm{i} n z)$ is periodic with period $2\\pi/|n|$ (or it is constant if $n=0$). This means that for any integer $k$, $\\exp(\\mathrm{i} n (z + 2\\pi k)) = \\exp(\\mathrm{i} n z)$. Because $(x+\\omega \\pmod{2\\pi})$ differs from $(x+\\omega)$ by an integer multiple of $2\\pi$, we can simplify the expression:\n$$\n(\\mathcal{U}f_n)(x) = \\exp(\\mathrm{i} n (x+\\omega)) = \\exp(\\mathrm{i} n x) \\exp(\\mathrm{i} n \\omega)\n$$\nBy substituting back $f_n(x) = \\exp(\\mathrm{i} n x)$, we obtain:\n$$\n(\\mathcal{U}f_n)(x) = (\\exp(\\mathrm{i} n \\omega)) f_n(x)\n$$\nThis is precisely the eigenvalue equation $\\mathcal{U}f_n = \\lambda_n f_n$, where the eigenfunction is $f_n(x) = \\exp(\\mathrm{i} n x)$ and the associated eigenvalue is $\\lambda_n = \\exp(\\mathrm{i} n \\omega)$.\nThis holds for all integers $n \\in \\mathbb{Z}$. Thus, the set of functions $\\{ \\exp(\\mathrm{i} n x) \\}_{n \\in \\mathbb{Z}}$ constitutes a complete set of Koopman eigenfunctions.\n\nNext, we address the spanning property and the unitarity of the operator.\nThe fact that these eigenfunctions $\\{f_n(x)\\}_{n \\in \\mathbb{Z}}$ span the space $L^{2}([0,2\\pi))$ is a cornerstone of Fourier analysis (related to the Riesz-Fischer theorem). Any function $g \\in L^{2}([0,2\\pi))$ can be uniquely represented by its Fourier series, $g(x) = \\sum_{n \\in \\mathbb{Z}} c_n \\exp(\\mathrm{i} n x)$, where the equality is understood in the $L^2$ sense. This confirms that our set of eigenfunctions is complete. The orthogonality of these eigenfunctions for $n \\neq m$ arises from the fundamental integral property of complex exponentials:\n$$\n\\langle f_n, f_m \\rangle = \\int_0^{2\\pi} \\exp(\\mathrm{i} n x) \\overline{\\exp(\\mathrm{i} m x)} \\frac{\\mathrm{d}x}{2\\pi} = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\exp(\\mathrm{i} (n-m) x) \\mathrm{d}x = \\delta_{nm}\n$$\nwhere $\\delta_{nm}$ is the Kronecker delta. This shows they form an orthonormal basis.\n\nThe Koopman operator $\\mathcal{U}$ is unitary on this space. An operator is unitary if it preserves the inner product. Let $g, h \\in L^{2}([0,2\\pi))$. We compute the inner product of their images under $\\mathcal{U}$:\n$$\n\\langle \\mathcal{U}g, \\mathcal{U}h \\rangle = \\int_0^{2\\pi} (\\mathcal{U}g)(x) \\overline{(\\mathcal{U}h)(x)} \\frac{\\mathrm{d}x}{2\\pi} = \\int_0^{2\\pi} g(x+\\omega) \\overline{h(x+\\omega)} \\frac{\\mathrm{d}x}{2\\pi}\n$$\nPerforming a change of variable $y = x+\\omega$, we have $\\mathrm{d}y = \\mathrm{d}x$. The integration interval $[0, 2\\pi)$ becomes $[\\omega, 2\\pi+\\omega)$. Since the integrand $g(y)\\overline{h(y)}$ is a $2\\pi$-periodic function (as its constituents are functions on the circle), integrating over any interval of length $2\\pi$ yields the same result.\n$$\n\\langle \\mathcal{U}g, \\mathcal{U}h \\rangle = \\int_{\\omega}^{2\\pi+\\omega} g(y) \\overline{h(y)} \\frac{\\mathrm{d}y}{2\\pi} = \\int_{0}^{2\\pi} g(y) \\overline{h(y)} \\frac{\\mathrm{d}y}{2\\pi} = \\langle g, h \\rangle\n$$\nSince $\\mathcal{U}$ preserves the inner product and is surjective (its inverse is the Koopman operator for the inverse map $T^{-1}(x) = x-\\omega$), it is unitary. This is also confirmed by the fact that all its eigenvalues $\\lambda_n = \\exp(\\mathrm{i} n \\omega)$ lie on the unit circle in the complex plane, as $|\\lambda_n| = |\\exp(\\mathrm{i} n \\omega)| = 1$ for real $n$ and $\\omega$.\n\nFinally, we interpret the spectrum. The point spectrum is the set of all eigenvalues, $\\{\\lambda_n = \\exp(\\mathrm{i} n \\omega) \\mid n \\in \\mathbb{Z}\\}$.\n- If $\\omega/(2\\pi)$ is rational, i.e., $\\omega/(2\\pi) = p/q$ for coprime integers $p, q$ with $q > 0$, then $\\omega = 2\\pi p/q$. The eigenvalues are $\\lambda_n = \\exp(\\mathrm{i} n (2\\pi p/q))$. These values are the $q$-th roots of unity, and the set of distinct eigenvalues is $\\{\\exp(2\\pi \\mathrm{i} k/q)\\}_{k=0}^{q-1}$. The point spectrum is therefore **finite**, consisting of $q$ points on the unit circle. The dynamics are periodic with period $q$.\n- If $\\omega/(2\\pi)$ is irrational, then for any two distinct integers $n \\neq m$, the eigenvalues $\\lambda_n$ and $\\lambda_m$ are distinct. If they were not, $(n-m)\\omega$ would be a multiple of $2\\pi$, which would imply $\\omega/(2\\pi)$ is rational. Thus, the point spectrum is **countably infinite**. Furthermore, by the Jacobi-Kronecker theorem on Diophantine approximation, the set of points $\\{n\\omega \\pmod{2\\pi}\\}_{n \\in \\mathbb{Z}}$ is dense in $[0, 2\\pi)$. This implies that the set of eigenvalues $\\{\\lambda_n\\}_{n \\in \\mathbb{Z}}$ is **dense** on the unit circle. The dynamics are quasi-periodic.\n\nAn idealized, noiseless Dynamic Mode Decomposition (DMD) algorithm, when applied to a sufficiently long trajectory of observables, aims to approximate the eigenvalues and eigenfunctions of the Koopman operator.\n- In the rational case, the system's orbit is periodic, and DMD would correctly identify the finite set of $q$ eigenvalues corresponding to the fundamental frequency and its harmonics.\n- In the irrational case, the orbit is quasi-periodic and never repeats. DMD applied to a finite dataset can only ever produce a finite number of eigenvalues. These DMD eigenvalues would approximate a subset of the true, dense spectrum, corresponding to the most dominant frequencies (i.e., eigenfunctions with large coefficients in the expansion of the measured observable) present in the data. With more data, DMD can resolve more frequencies and provide a better approximation of the dense spectrum.\n\nThe problem asks for the specific eigenvalue $\\lambda_n$ associated with the eigenfunction $f_n(x) = \\exp(\\mathrm{i} n x)$. Based on our derivation, this is:\n$\\lambda_n = \\exp(\\mathrm{i} n \\omega)$.",
            "answer": "$$\n\\boxed{\\exp(\\mathrm{i} n \\omega)}\n$$"
        },
        {
            "introduction": "While some systems have neatly structured Koopman operators, most nonlinear systems present significant challenges. This exercise explores a crucial concept: how the Koopman operator acts on observables in a simple but genuinely nonlinear system, governed by the map $f(x) = x^{2}$. You will see how the operator acting on a simple polynomial observable generates a polynomial of higher degree, implying that the space of all polynomials is not a finite-dimensional invariant subspace . This result illuminates why finite-dimensional approximations like Dynamic Mode Decomposition (DMD) are necessary and are, in fact, projections of an infinite-dimensional reality.",
            "id": "4219108",
            "problem": "Consider a digital twin of a scalar Cyber-Physical System (CPS) whose discrete-time state $x_{k} \\in \\mathbb{R}$ evolves according to the deterministic nonlinear map $f(x) = x^{2}$. Observables are functions $g : \\mathbb{R} \\to \\mathbb{R}$ used to extract measurable quantities from the state. In the context of operator-theoretic modeling relevant to Dynamic Mode Decomposition (DMD), the Koopman operator acts on observables induced by the system dynamics. For the specific observable $g(x) = x$, compute the analytic expression for $(\\mathcal{K}g)(x)$. Then, using only foundational definitions of discrete-time dynamical systems and the Koopman operator, explain why this computation implies that the space of polynomial observables is not finite-dimensional invariant under $\\mathcal{K}$ for this $f$. Provide your reasoning, but express your final answer as the analytic expression for $(\\mathcal{K}g)(x)$. No rounding is required.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a discrete-time dynamical system with evolution map $f(x) = x^2$, a state $x_k \\in \\mathbb{R}$, and an observable function $g(x) = x$. The tasks are to compute the action of the Koopman operator on this specific observable and to use this result to explain a property of the operator's action on the space of polynomial observables. All definitions and conditions are standard within the fields of dynamical systems and operator theory. The problem is thus deemed valid.\n\nThe solution proceeds first by computing the required expression and then by providing the subsequent reasoning.\n\nThe Koopman operator, denoted by $\\mathcal{K}$, describes the evolution of observable functions defined on the state space of a dynamical system. For a discrete-time system governed by the map $x_{k+1} = f(x_k)$, the action of the Koopman operator on an observable function $g$ is defined as:\n$$(\\mathcal{K}g)(x) = g(f(x))$$\n\nIn this problem, the state evolution is given by the nonlinear map $f(x) = x^2$. The specific observable under consideration is the identity function, $g(x) = x$.\n\nTo compute the analytic expression for $(\\mathcal{K}g)(x)$, we substitute the given functions $f$ and $g$ into the definition of the Koopman operator:\n$$(\\mathcal{K}g)(x) = g(f(x)) = g(x^2)$$\n\nSince the function $g$ is defined as $g(z) = z$ for any input $z$, we evaluate $g(x^2)$ as:\n$$g(x^2) = x^2$$\n\nTherefore, the analytic expression for the action of the Koopman operator $\\mathcal{K}$ on the observable $g(x)=x$ is:\n$$(\\mathcal{K}g)(x) = x^2$$\n\nNext, we must explain why this result implies that the space of polynomial observables is not a finite-dimensional invariant subspace under $\\mathcal{K}$ for the given dynamics $f(x) = x^2$.\n\nA vector space $\\mathcal{V}$ is said to be an invariant subspace under a linear operator $\\mathcal{L}$ if for every vector $v \\in \\mathcal{V}$, the vector $\\mathcal{L}v$ is also in $\\mathcal{V}$. In our context, the vectors are observable functions and the operator is the Koopman operator $\\mathcal{K}$.\n\nLet $\\mathcal{P}$ be the space of all polynomial observables on $\\mathbb{R}$. A function $p \\in \\mathcal{P}$ can be written as $p(x) = \\sum_{j=0}^{d} c_j x^j$ for some finite degree $d \\in \\mathbb{N}_0$ and coefficients $c_j \\in \\mathbb{R}$.\n\nLet us analyze the action of $\\mathcal{K}$ on a general monomial observable $h_n(x) = x^n$ for some non-negative integer $n$.\n$$(\\mathcal{K}h_n)(x) = h_n(f(x)) = h_n(x^2) = (x^2)^n = x^{2n}$$\nThis shows that the Koopman operator for this system maps a monomial of degree $n$ to a monomial of degree $2n$.\n\nNow, let us assume for the sake of contradiction that there exists a finite-dimensional subspace of polynomials, $\\mathcal{G}$, which is invariant under $\\mathcal{K}$ and is non-trivial (i.e., it contains more than just constant functions).\nSince $\\mathcal{G}$ is a finite-dimensional space of polynomials, there must exist a maximum degree $N$ for any polynomial in $\\mathcal{G}$, where $N > 0$. Let $p(x) \\in \\mathcal{G}$ be a polynomial of this maximum degree $N$. We can write $p(x) = a_N x^N + \\dots + a_1 x + a_0$, with $a_N \\neq 0$.\n\nApplying the Koopman operator to $p(x)$:\n$$(\\mathcal{K}p)(x) = p(f(x)) = p(x^2) = a_N (x^2)^N + \\dots + a_1 (x^2) + a_0 = a_N x^{2N} + \\dots + a_1 x^2 + a_0$$\nThe resulting function, $(\\mathcal{K}p)(x)$, is a polynomial. Its degree is determined by the highest power of $x$, which is $2N$.\nSince we assumed $N>0$, it follows that $2N > N$.\n\nFor $\\mathcal{G}$ to be an invariant subspace, the function $(\\mathcal{K}p)(x)$ must also be an element of $\\mathcal{G}$. However, the degree of $(\\mathcal{K}p)(x)$ is $2N$, which is strictly greater than the maximum degree $N$ of any polynomial in $\\mathcal{G}$. This means $(\\mathcal{K}p)(x) \\notin \\mathcal{G}$, which contradicts our assumption that $\\mathcal{G}$ is an invariant subspace.\n\nThe only way for the contradiction to be avoided is if $N \\le 0$. If $N=0$, the subspace consists only of constant functions $g(x)=c$. For such a function, $(\\mathcal{K}g)(x) = g(x^2) = c$, which is still a constant. So, the space of constant functions is a trivial, one-dimensional invariant subspace. However, any finite-dimensional space of polynomials with maximum degree $N > 0$ cannot be invariant.\n\nThe initial computation, $(\\mathcal{K}\\{x\\})(x) = x^2$, serves as the base case for this argument. It demonstrates that an observable of degree $1$ is mapped to an observable of degree $2$. Applying the operator repeatedly generates a sequence of observables with unboundedly increasing degrees:\n$$g_0(x) = x$$\n$$(\\mathcal{K}g_0)(x) = x^2$$\n$$(\\mathcal{K}^2 g_0)(x) = \\mathcal{K}(x^2) = (x^2)^2 = x^4$$\n$$(\\mathcal{K}^n g_0)(x) = x^{2^n}$$\nAny finite-dimensional space of polynomials would have a maximum degree, and a sufficient number of applications of $\\mathcal{K}$ to the simple observable $g(x)=x$ will produce a polynomial that exceeds this maximum degree, thus leaving the space. Consequently, the space of all polynomial observables cannot be represented by a finite-dimensional invariant subspace under this Koopman operator. This is a key reason why finite-dimensional approximations like DMD can be challenging for certain nonlinear systems.",
            "answer": "$$\\boxed{x^2}$$"
        },
        {
            "introduction": "Bridging theory and practice requires confronting the challenges of applying DMD to real-world, noisy data. Since DMD produces a finite-rank approximation of the Koopman operator, the most critical modeling decision is the choice of truncation rank, $r$. This problem delves into the practical art of model selection, tasking you with evaluating different strategies for choosing $r$ based on their statistical rigor . By analyzing methods like cross-validation and information criteria, you will develop a principled approach to managing the bias-variance tradeoff, ensuring your data-driven model is not just accurate on training data but also generalizable and predictive.",
            "id": "4219089",
            "problem": "A digital twin for a feedback-stabilized cyber-physical system (CPS) is built from time-series of state snapshots using Dynamic Mode Decomposition (DMD). Dynamic Mode Decomposition (DMD) seeks a low-dimensional linear model that advances snapshots forward in time and can be interpreted as a finite-dimensional approximation of the Koopman operator acting on selected observables. The standard pipeline applies Singular Value Decomposition (SVD) to the snapshot matrix and truncates to rank $r$ before estimating the reduced linear map. In practice, the truncation rank $r$ controls model complexity: larger $r$ allows the model to fit finer-scale structures (including noise), whereas smaller $r$ enforces smoother, lower-variance models that may miss important dynamics. Let the data be contaminated with sensor noise of approximately Gaussian statistics and let the validation objective be multi-step prediction error on held-out trajectories representative of the intended operating envelope.\n\nFrom a first-principles perspective, the selection of $r$ should balance bias and variance, reflect the physics of the CPS, and be justified by generalization performance rather than training fit. Consider the following candidate procedures for selecting $r$:\n\nA. Select $r$ as the smallest value for which the cumulative energy captured by the first $r$ singular values of the snapshot matrix exceeds a threshold $\\tau$ derived from an estimate of the sensor noise level, then confirm $r$ by $k$-fold cross-validation of multi-step prediction error on held-out data. For the closed-loop stable CPS, discard modes whose discrete-time eigenvalues have magnitude exceeding $1$ as a post-processing stability filter.\n\nB. Select $r$ as the maximum possible rank permitted by the data matrix dimensions to minimize approximation bias, because the DMD model is linear and therefore is unlikely to overfit.\n\nC. Select $r$ by minimizing the one-step reconstruction error on the training snapshots only, stopping when the marginal decrease in error upon increasing $r$ falls below a fixed tolerance (for example, a relative decrease smaller than $1\\%$), without any cross-validation or model complexity penalty.\n\nD. Select $r$ by minimizing a Minimum Description Length (MDL) objective that trades off data-fit code length under a Gaussian noise model, evaluated on validation data, with a model code length that increases with the number of free parameters induced by rank $r$; then, to reduce estimator variance, apply the $1$-standard-error rule to choose the smallest $r$ whose validation score lies within one estimated standard error of the optimum.\n\nWhich of the options above are principled criteria for selecting the DMD truncation rank $r$ that explicitly or implicitly balance bias and variance while respecting the CPS context and aiming for out-of-sample predictive performance? Select all that apply.\n\nImportant definitions: Singular Value Decomposition (SVD); Dynamic Mode Decomposition (DMD); Minimum Description Length (MDL); Koopman operator; cross-validation (CV).",
            "solution": "The problem requires an evaluation of four distinct procedures for selecting the truncation rank $r$ in a Dynamic Mode Decomposition (DMD) model of a feedback-stabilized cyber-physical system (CPS). The data for the model are noisy time-series snapshots. The stated objective is to achieve good generalization performance, measured by multi-step prediction error on held-out data. This requires a procedure that properly balances the bias-variance trade-off.\n\n### Theoretical Foundation\n\nDynamic Mode Decomposition provides a linear operator $\\mathbf{A}$ that best approximates the evolution of state snapshots $\\mathbf{x}_k \\in \\mathbb{R}^n$ such that $\\mathbf{x}_{k+1} \\approx \\mathbf{A} \\mathbf{x}_k$. The standard algorithm operates on two matrices formed by stacking $m$ snapshots:\n$$\n\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_{m-1}] \\quad \\text{and} \\quad \\mathbf{X}' = [\\mathbf{x}_2, \\mathbf{x}_3, \\ldots, \\mathbf{x}_m]\n$$\nThe goal is to find $\\mathbf{A}$ that minimizes $\\|\\mathbf{X}' - \\mathbf{A}\\mathbf{X}\\|_F$. To make the problem well-conditioned and to reduce the model's complexity, the data matrix $\\mathbf{X}$ is first approximated by its rank-$r$ Singular Value Decomposition (SVD), $\\mathbf{X} \\approx \\mathbf{U}_r \\mathbf{\\Sigma}_r \\mathbf{V}_r^*$. The DMD operator is then constructed in the reduced-dimensional space of the first $r$ POD (Proper Orthogonal Decomposition) modes, yielding an $r \\times r$ matrix $\\tilde{\\mathbf{A}} = \\mathbf{U}_r^* \\mathbf{X}' \\mathbf{V}_r \\mathbf{\\Sigma}_r^{-1}$.\n\nThe choice of the truncation rank $r$ is critical.\n- A small $r$ results in a low-complexity model. This leads to high bias (the model may be too simple to capture the true dynamics) but low variance (the model is less sensitive to the specific noise realization in the training data).\n- A large $r$ results in a high-complexity model. This leads to low bias (the model can fit the training data very well) but high variance (the model is likely to fit the noise, leading to poor performance on new data, i.e., overfitting).\n\nA \"principled criterion\" for selecting $r$ must therefore navigate this trade-off to optimize for out-of-sample performance, not just in-sample (training) fit. It should also incorporate knowledge about the data (noise characteristics) and the system (physical properties like stability).\n\n### Evaluation of Candidate Procedures\n\n**Option A: Select $r$ as the smallest value for which the cumulative energy captured by the first $r$ singular values of the snapshot matrix exceeds a threshold $\\tau$ derived from an estimate of the sensor noise level, then confirm $r$ by $k$-fold cross-validation of multi-step prediction error on held-out data. For the closed-loop stable CPS, discard modes whose discrete-time eigenvalues have magnitude exceeding $1$ as a post-processing stability filter.**\n\nThis procedure is composed of three methodologically sound steps:\n1.  **Noise-based Thresholding:** The premise states the data are contaminated with Gaussian noise. Random Matrix Theory provides methods (e.g., related to the Marcenko-Pastur distribution) to estimate the portion of the singular value spectrum attributable to noise. Setting a threshold $\\tau$ based on the estimated noise variance is a principled way to perform \"optimal hard thresholding\" of the singular values. This directly addresses the problem of separating signal from noise, aiming to select a rank $r$ that retains the dynamically significant modes while discarding those dominated by noise. This is a direct method for variance control.\n2.  **Cross-Validation (CV):** $k$-fold cross-validation is a robust, empirical standard for estimating a model's generalization performance. By evaluating the multi-step prediction error on held-out data, this step directly addresses the stated validation objective. It provides a definitive check on whether the rank $r$ (perhaps chosen from a range of candidates suggested by the first step) yields a model that predicts well on unseen data. This is the canonical way to manage the bias-variance trade-off empirically.\n3.  **Stability Post-Processing:** The problem states the CPS is feedback-stabilized, implying its dynamics are stable. Any unstable eigenvalues (modulus $> 1$) in the discrete-time DMD model are likely artifacts of noise and overfitting. Removing these spurious modes or projecting the dynamics onto the stable subspace is a valid and common technique to incorporate prior physical knowledge into the model, improving its long-term predictive accuracy and physical realism.\n\nThis option combines a theoretically-grounded heuristic for noise removal, the empirical gold standard for model selection (CV), and the use of prior physical knowledge. This multi-pronged strategy is highly principled.\n\n**Verdict: Correct**\n\n**Option B: Select $r$ as the maximum possible rank permitted by the data matrix dimensions to minimize approximation bias, because the DMD model is linear and therefore is unlikely to overfit.**\n\nThis procedure is fundamentally flawed for two reasons:\n1.  **Ignoring Variance:** Selecting the maximum possible rank $r$ is equivalent to minimizing the reconstruction error on the training data. This focuses exclusively on minimizing bias *with respect to the training data*, which includes noise. It completely ignores the variance component of the error. Such a model will have maximum complexity and is almost certain to overfit the noise, leading to poor generalization performance.\n2.  **Incorrect Justification:** The claim that a linear model is \"unlikely to overfit\" is false. A linear model with a large number of parameters can easily overfit. The number of parameters in the reduced DMD operator $\\tilde{\\mathbf{A}}$ is $r^2$. As $r$ increases, the model's capacity to fit noise increases quadratically. The linearity of the dynamics $\\mathbf{x}_{k+1} = \\mathbf{A} \\mathbf{x}_k$ in no way prevents the overfitting of the matrix $\\mathbf{A}$ itself during the estimation phase.\n\nThis approach violates the core principle of balancing bias and variance.\n\n**Verdict: Incorrect**\n\n**Option C: Select $r$ by minimizing the one-step reconstruction error on the training snapshots only, stopping when the marginal decrease in error upon increasing $r$ falls below a fixed tolerance (for example, a relative decrease smaller than $1\\%$), without any cross-validation or model complexity penalty.**\n\nThis procedure suffers from the same fundamental issue as option B, though it appears more nuanced.\n1.  **Focus on Training Error:** The criterion is based entirely on the one-step reconstruction error on the training data ($\\|\\mathbf{X}' - \\mathbf{A}_r\\mathbf{X}\\|_F$). This error metric is a monotonically decreasing function of $r$. A model that performs well on this metric is not guaranteed to generalize well.\n2.  **Heuristic Stopping Rule:** The \"elbow\" method (stopping when the marginal gain in performance becomes small) is a common but weak heuristic. It does not have a strong theoretical justification for finding the optimal trade-off point for generalization. The point where the training error curve flattens might occur long after the model has begun to fit noise significantly.\n3.  **Explicit Rejection of Principled Methods:** The procedure explicitly states it is performed \"without any cross-validation or model complexity penalty\". This highlights its primary weakness: it lacks a mechanism to estimate or control out-of-sample error.\n\nThis method is prone to selecting an overly complex model that overfits the training data.\n\n**Verdict: Incorrect**\n\n**Option D: Select $r$ by minimizing a Minimum Description Length (MDL) objective that trades off data-fit code length under a Gaussian noise model, evaluated on validation data, with a model code length that increases with the number of free parameters induced by rank $r$; then, to reduce estimator variance, apply the $1$-standard-error rule to choose the smallest $r$ whose validation score lies within one estimated standard error of the optimum.**\n\nThis procedure describes a sophisticated and principled approach rooted in information theory and statistics.\n1.  **Minimum Description Length (MDL):** MDL is a formal implementation of Occam's Razor. It seeks the model that provides the most compact description of the data. This involves minimizing a cost function that combines the cost of encoding the model's prediction errors (data-fit) and the cost of encoding the model itself (model complexity). The term for model complexity acts as a regularization penalty. Since the number of parameters in the DMD model scales with $r$ (as $r^2$), MDL provides a theoretically sound way to balance model fit against model complexity, directly addressing the bias-variance trade-off. Evaluating this on validation data further ensures the focus is on generalization.\n2.  **The $1$-Standard-Error Rule:** This is a standard and robust statistical heuristic used in conjunction with cross-validation or validation-set evaluation. After finding the model with the minimum validation error (the \"optimal\" model), this rule selects the simplest model (smallest $r$) whose performance is statistically indistinguishable from the optimum (i.e., within one standard error of the minimum error). This explicitly favors simpler, lower-variance models and helps prevent \"chasing noise\" in the validation set, leading to more robust model selection.\n\nThis option describes a formal, theoretically justified method for managing the bias-variance trade-off, augmented by a sound statistical rule for robustness.\n\n**Verdict: Correct**\n\n### Conclusion\n\nOptions A and D both describe principled, state-of-the-art approaches to selecting the DMD rank $r$. Option A is a pragmatic, empirically-driven approach combining noise estimation, cross-validation, and physics-based constraints. Option D is a formal, information-theoretic approach combined with a robust statistical selection rule. Both correctly address the need to balance bias and variance to optimize for out-of-sample predictive performance. Options B and C are flawed because they focus on training set performance and ignore or misunderstand the problem of overfitting.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}