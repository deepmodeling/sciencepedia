{
    "hands_on_practices": [
        {
            "introduction": "Before applying data-driven methods, it is essential to understand the Koopman operator from first principles. This exercise grounds your understanding by tasking you with analytically deriving the complete spectral properties of the Koopman operator for a canonical dynamical system: a simple rotation on a circle. By directly calculating the eigenfunctions and eigenvalues, you will build a foundational intuition for how system dynamics translate into the spectral domain of the operator .",
            "id": "4219100",
            "problem": "A digital twin of a synchronous oscillator in a cyber-physical system represents the oscillatorâ€™s phase by the discrete-time map on the circle given by $x_{k+1} = x_{k} + \\omega \\; \\mathrm{mod}\\; 2\\pi$, where $x_{k} \\in [0,2\\pi)$ is the phase at discrete time $k \\in \\mathbb{Z}_{\\ge 0}$ and $\\omega \\in \\mathbb{R}$ is a constant angular increment. Angles are measured in radians. The digital twin uses the Koopman operator framework to propagate observables $f$ of the phase. The Koopman operator $\\mathcal{U}$ acts on an observable $f$ by composition with the dynamics, i.e., $(\\mathcal{U}f)(x) = f(x+\\omega \\; \\mathrm{mod}\\; 2\\pi)$. Consider the Hilbert space $L^{2}([0,2\\pi),\\mathrm{d}x/(2\\pi))$ of square-integrable complex-valued observables with respect to the normalized Lebesgue (invariant) measure on the circle. In practice, Dynamic Mode Decomposition (DMD) is used to estimate spectral properties of $\\mathcal{U}$ from data, but here you are to work from first principles.\n\nStarting only from the definition of the Koopman operator and standard facts about $L^{2}$ spaces and Fourier series, do the following:\n\n- Construct a complete set of Koopman eigenfunctions in $L^{2}([0,2\\pi))$ and derive their associated eigenvalues in closed form as functions of $\\omega$.\n- Explain, in terms of basic properties of rotations on the circle and orthogonality of complex exponentials, how these eigenfunctions span $L^{2}([0,2\\pi))$ and why the Koopman operator is unitary on this space.\n- Interpret the Koopman spectrum for the cases where $\\omega/(2\\pi)$ is rational versus irrational, addressing whether the point spectrum is finite, countable, and whether it is dense on the unit circle. Briefly relate the spectral nature to what an idealized, noiseless Dynamic Mode Decomposition (DMD) would recover from uniformly sampled data along an orbit.\n\nFinally, let $f_{n}(x) = \\exp(\\mathrm{i} n x)$ for an arbitrary but fixed integer $n \\in \\mathbb{Z}$. Report, as your final answer, the Koopman eigenvalue $\\lambda_{n}$ associated with $f_{n}$, expressed in exact symbolic form as a function of $n$ and $\\omega$. No numerical approximation is required, and no units are to be included in the final answer.",
            "solution": "The problem statement presented is a well-posed and scientifically grounded exercise in the application of Koopman operator theory to a fundamental dynamical system, the rigid rotation on a circle. It is a standard model used to introduce the core concepts of Koopman spectral analysis. The problem is self-contained, mathematically precise, and free of any factual or logical inconsistencies. All terms are standard within the fields of dynamical systems and functional analysis. Therefore, the problem is valid and a solution will be provided.\n\nThe system dynamics are given by the map $T: [0, 2\\pi) \\to [0, 2\\pi)$, defined as $T(x) = x + \\omega \\pmod{2\\pi}$. The Koopman operator $\\mathcal{U}$ associated with this map acts on a function $f$ in the Hilbert space $\\mathcal{H} = L^{2}([0,2\\pi), \\frac{\\mathrm{d}x}{2\\pi})$ by composition with the dynamics: $(\\mathcal{U}f)(x) = f(T(x)) = f(x+\\omega \\pmod{2\\pi})$. The inner product on this space is $\\langle g, h \\rangle = \\int_0^{2\\pi} g(x) \\overline{h(x)} \\frac{\\mathrm{d}x}{2\\pi}$.\n\nAn eigenfunction of the Koopman operator is a non-zero function $f \\in \\mathcal{H}$ that satisfies the eigenvalue equation:\n$$\n\\mathcal{U}f = \\lambda f\n$$\nfor some complex scalar $\\lambda$, the corresponding Koopman eigenvalue. Substituting the definition of the operator, this equation becomes:\n$$\nf(x+\\omega \\pmod{2\\pi}) = \\lambda f(x)\n$$\n\nTo find the eigenfunctions and eigenvalues, we leverage the structure of the Hilbert space $L^{2}([0,2\\pi))$. A fundamental result in Fourier analysis states that the set of complex exponential functions $\\{ \\exp(\\mathrm{i} n x) \\}_{n \\in \\mathbb{Z}}$ forms a complete orthonormal basis for this space. We test these basis functions as candidates for Koopman eigenfunctions.\n\nLet us define $f_n(x) = \\exp(\\mathrm{i} n x)$ for any integer $n \\in \\mathbb{Z}$. These functions are members of $L^{2}([0,2\\pi))$ as their squared modulus is $|\\exp(\\mathrm{i} n x)|^2 = 1$, which is clearly integrable over $[0, 2\\pi)$. We now apply the Koopman operator $\\mathcal{U}$ to $f_n(x)$:\n$$\n(\\mathcal{U}f_n)(x) = f_n(x+\\omega \\pmod{2\\pi}) = \\exp(\\mathrm{i} n (x+\\omega \\pmod{2\\pi}))\n$$\nSince $\\exp(\\mathrm{i} z)$ is $2\\pi$-periodic in its argument $z$, and $n$ is an integer, the function $\\exp(\\mathrm{i} n z)$ is periodic with period $2\\pi/|n|$ (or it is constant if $n=0$). This means that for any integer $k$, $\\exp(\\mathrm{i} n (z + 2\\pi k)) = \\exp(\\mathrm{i} n z)$. Because $(x+\\omega \\pmod{2\\pi})$ differs from $(x+\\omega)$ by an integer multiple of $2\\pi$, we can simplify the expression:\n$$\n(\\mathcal{U}f_n)(x) = \\exp(\\mathrm{i} n (x+\\omega)) = \\exp(\\mathrm{i} n x) \\exp(\\mathrm{i} n \\omega)\n$$\nBy substituting back $f_n(x) = \\exp(\\mathrm{i} n x)$, we obtain:\n$$\n(\\mathcal{U}f_n)(x) = (\\exp(\\mathrm{i} n \\omega)) f_n(x)\n$$\nThis is precisely the eigenvalue equation $\\mathcal{U}f_n = \\lambda_n f_n$, where the eigenfunction is $f_n(x) = \\exp(\\mathrm{i} n x)$ and the associated eigenvalue is $\\lambda_n = \\exp(\\mathrm{i} n \\omega)$.\nThis holds for all integers $n \\in \\mathbb{Z}$. Thus, the set of functions $\\{ \\exp(\\mathrm{i} n x) \\}_{n \\in \\mathbb{Z}}$ constitutes a complete set of Koopman eigenfunctions.\n\nNext, we address the spanning property and the unitarity of the operator.\nThe fact that these eigenfunctions $\\{f_n(x)\\}_{n \\in \\mathbb{Z}}$ span the space $L^{2}([0,2\\pi))$ is a cornerstone of Fourier analysis (related to the Riesz-Fischer theorem). Any function $g \\in L^{2}([0,2\\pi))$ can be uniquely represented by its Fourier series, $g(x) = \\sum_{n \\in \\mathbb{Z}} c_n \\exp(\\mathrm{i} n x)$, where the equality is understood in the $L^2$ sense. This confirms that our set of eigenfunctions is complete. The orthogonality of these eigenfunctions for $n \\neq m$ arises from the fundamental integral property of complex exponentials:\n$$\n\\langle f_n, f_m \\rangle = \\int_0^{2\\pi} \\exp(\\mathrm{i} n x) \\overline{\\exp(\\mathrm{i} m x)} \\frac{\\mathrm{d}x}{2\\pi} = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\exp(\\mathrm{i} (n-m) x) \\mathrm{d}x = \\delta_{nm}\n$$\nwhere $\\delta_{nm}$ is the Kronecker delta. This shows they form an orthonormal basis.\n\nThe Koopman operator $\\mathcal{U}$ is unitary on this space. An operator is unitary if it preserves the inner product. Let $g, h \\in L^{2}([0,2\\pi))$. We compute the inner product of their images under $\\mathcal{U}$:\n$$\n\\langle \\mathcal{U}g, \\mathcal{U}h \\rangle = \\int_0^{2\\pi} (\\mathcal{U}g)(x) \\overline{(\\mathcal{U}h)(x)} \\frac{\\mathrm{d}x}{2\\pi} = \\int_0^{2\\pi} g(x+\\omega) \\overline{h(x+\\omega)} \\frac{\\mathrm{d}x}{2\\pi}\n$$\nPerforming a change of variable $y = x+\\omega$, we have $\\mathrm{d}y = \\mathrm{d}x$. The integration interval $[0, 2\\pi)$ becomes $[\\omega, 2\\pi+\\omega)$. Since the integrand $g(y)\\overline{h(y)}$ is a $2\\pi$-periodic function (as its constituents are functions on the circle), integrating over any interval of length $2\\pi$ yields the same result.\n$$\n\\langle \\mathcal{U}g, \\mathcal{U}h \\rangle = \\int_{\\omega}^{2\\pi+\\omega} g(y) \\overline{h(y)} \\frac{\\mathrm{d}y}{2\\pi} = \\int_{0}^{2\\pi} g(y) \\overline{h(y)} \\frac{\\mathrm{d}y}{2\\pi} = \\langle g, h \\rangle\n$$\nSince $\\mathcal{U}$ preserves the inner product and is surjective (its inverse is the Koopman operator for the inverse map $T^{-1}(x) = x-\\omega$), it is unitary. This is also confirmed by the fact that all its eigenvalues $\\lambda_n = \\exp(\\mathrm{i} n \\omega)$ lie on the unit circle in the complex plane, as $|\\lambda_n| = |\\exp(\\mathrm{i} n \\omega)| = 1$ for real $n$ and $\\omega$.\n\nFinally, we interpret the spectrum. The point spectrum is the set of all eigenvalues, $\\{\\lambda_n = \\exp(\\mathrm{i} n \\omega) \\mid n \\in \\mathbb{Z}\\}$.\n- If $\\omega/(2\\pi)$ is rational, i.e., $\\omega/(2\\pi) = p/q$ for coprime integers $p, q$ with $q > 0$, then $\\omega = 2\\pi p/q$. The eigenvalues are $\\lambda_n = \\exp(\\mathrm{i} n (2\\pi p/q))$. These values are the $q$-th roots of unity, and the set of distinct eigenvalues is $\\{\\exp(2\\pi \\mathrm{i} k/q)\\}_{k=0}^{q-1}$. The point spectrum is therefore **finite**, consisting of $q$ points on the unit circle. The dynamics are periodic with period $q$.\n- If $\\omega/(2\\pi)$ is irrational, then for any two distinct integers $n \\neq m$, the eigenvalues $\\lambda_n$ and $\\lambda_m$ are distinct. If they were not, $(n-m)\\omega$ would be a multiple of $2\\pi$, which would imply $\\omega/(2\\pi)$ is rational. Thus, the point spectrum is **countably infinite**. Furthermore, by the Jacobi-Kronecker theorem on Diophantine approximation, the set of points $\\{n\\omega \\pmod{2\\pi}\\}_{n \\in \\mathbb{Z}}$ is dense in $[0, 2\\pi)$. This implies that the set of eigenvalues $\\{\\lambda_n\\}_{n \\in \\mathbb{Z}}$ is **dense** on the unit circle. The dynamics are quasi-periodic.\n\nAn idealized, noiseless Dynamic Mode Decomposition (DMD) algorithm, when applied to a sufficiently long trajectory of observables, aims to approximate the eigenvalues and eigenfunctions of the Koopman operator.\n- In the rational case, the system's orbit is periodic, and DMD would correctly identify the finite set of $q$ eigenvalues corresponding to the fundamental frequency and its harmonics.\n- In the irrational case, the orbit is quasi-periodic and never repeats. DMD applied to a finite dataset can only ever produce a finite number of eigenvalues. These DMD eigenvalues would approximate a subset of the true, dense spectrum, corresponding to the most dominant frequencies (i.e., eigenfunctions with large coefficients in the expansion of the measured observable) present in the data. With more data, DMD can resolve more frequencies and provide a better approximation of the dense spectrum.\n\nThe problem asks for the specific eigenvalue $\\lambda_n$ associated with the eigenfunction $f_n(x) = \\exp(\\mathrm{i} n x)$. Based on our derivation, this is:\n$\\lambda_n = \\exp(\\mathrm{i} n \\omega)$.",
            "answer": "$$\n\\boxed{\\exp(\\mathrm{i} n \\omega)}\n$$"
        },
        {
            "introduction": "While some systems have a neat, analytically tractable Koopman spectrum, many do not. This practice demonstrates a crucial feature of Koopman analysis for nonlinear systems: the generation of infinite-dimensional dynamics from simple observables. By examining the action of the Koopman operator for the map $f(x)=x^2$, you will discover why finite-dimensional closures are often impossible, motivating the need for approximation methods like Dynamic Mode Decomposition (DMD) .",
            "id": "4219108",
            "problem": "Consider a digital twin of a scalar Cyber-Physical System (CPS) whose discrete-time state $x_{k} \\in \\mathbb{R}$ evolves according to the deterministic nonlinear map $f(x) = x^{2}$. Observables are functions $g : \\mathbb{R} \\to \\mathbb{R}$ used to extract measurable quantities from the state. In the context of operator-theoretic modeling relevant to Dynamic Mode Decomposition (DMD), the Koopman operator acts on observables induced by the system dynamics. For the specific observable $g(x) = x$, compute the analytic expression for $(\\mathcal{K}g)(x)$. Then, using only foundational definitions of discrete-time dynamical systems and the Koopman operator, explain why this computation implies that the space of polynomial observables is not finite-dimensional invariant under $\\mathcal{K}$ for this $f$. Provide your reasoning, but express your final answer as the analytic expression for $(\\mathcal{K}g)(x)$. No rounding is required.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a discrete-time dynamical system with evolution map $f(x) = x^2$, a state $x_k \\in \\mathbb{R}$, and an observable function $g(x) = x$. The tasks are to compute the action of the Koopman operator on this specific observable and to use this result to explain a property of the operator's action on the space of polynomial observables. All definitions and conditions are standard within the fields of dynamical systems and operator theory. The problem is thus deemed valid.\n\nThe solution proceeds first by computing the required expression and then by providing the subsequent reasoning.\n\nThe Koopman operator, denoted by $\\mathcal{K}$, describes the evolution of observable functions defined on the state space of a dynamical system. For a discrete-time system governed by the map $x_{k+1} = f(x_k)$, the action of the Koopman operator on an observable function $g$ is defined as:\n$$(\\mathcal{K}g)(x) = g(f(x))$$\n\nIn this problem, the state evolution is given by the nonlinear map $f(x) = x^2$. The specific observable under consideration is the identity function, $g(x) = x$.\n\nTo compute the analytic expression for $(\\mathcal{K}g)(x)$, we substitute the given functions $f$ and $g$ into the definition of the Koopman operator:\n$$(\\mathcal{K}g)(x) = g(f(x)) = g(x^2)$$\n\nSince the function $g$ is defined as $g(z) = z$ for any input $z$, we evaluate $g(x^2)$ as:\n$$g(x^2) = x^2$$\n\nTherefore, the analytic expression for the action of the Koopman operator $\\mathcal{K}$ on the observable $g(x)=x$ is:\n$$(\\mathcal{K}g)(x) = x^2$$\n\nNext, we must explain why this result implies that the space of polynomial observables is not a finite-dimensional invariant subspace under $\\mathcal{K}$ for the given dynamics $f(x) = x^2$.\n\nA vector space $\\mathcal{V}$ is said to be an invariant subspace under a linear operator $\\mathcal{L}$ if for every vector $v \\in \\mathcal{V}$, the vector $\\mathcal{L}v$ is also in $\\mathcal{V}$. In our context, the vectors are observable functions and the operator is the Koopman operator $\\mathcal{K}$.\n\nLet $\\mathcal{P}$ be the space of all polynomial observables on $\\mathbb{R}$. A function $p \\in \\mathcal{P}$ can be written as $p(x) = \\sum_{j=0}^{d} c_j x^j$ for some finite degree $d \\in \\mathbb{N}_0$ and coefficients $c_j \\in \\mathbb{R}$.\n\nLet us analyze the action of $\\mathcal{K}$ on a general monomial observable $h_n(x) = x^n$ for some non-negative integer $n$.\n$$(\\mathcal{K}h_n)(x) = h_n(f(x)) = h_n(x^2) = (x^2)^n = x^{2n}$$\nThis shows that the Koopman operator for this system maps a monomial of degree $n$ to a monomial of degree $2n$.\n\nNow, let us assume for the sake of contradiction that there exists a finite-dimensional subspace of polynomials, $\\mathcal{G}$, which is invariant under $\\mathcal{K}$ and is non-trivial (i.e., it contains more than just constant functions).\nSince $\\mathcal{G}$ is a finite-dimensional space of polynomials, there must exist a maximum degree $N$ for any polynomial in $\\mathcal{G}$, where $N  0$. Let $p(x) \\in \\mathcal{G}$ be a polynomial of this maximum degree $N$. We can write $p(x) = a_N x^N + \\dots + a_1 x + a_0$, with $a_N \\neq 0$.\n\nApplying the Koopman operator to $p(x)$:\n$$(\\mathcal{K}p)(x) = p(f(x)) = p(x^2) = a_N (x^2)^N + \\dots + a_1 (x^2) + a_0 = a_N x^{2N} + \\dots + a_1 x^2 + a_0$$\nThe resulting function, $(\\mathcal{K}p)(x)$, is a polynomial. Its degree is determined by the highest power of $x$, which is $2N$.\nSince we assumed $N0$, it follows that $2N  N$.\n\nFor $\\mathcal{G}$ to be an invariant subspace, the function $(\\mathcal{K}p)(x)$ must also be an element of $\\mathcal{G}$. However, the degree of $(\\mathcal{K}p)(x)$ is $2N$, which is strictly greater than the maximum degree $N$ of any polynomial in $\\mathcal{G}$. This means $(\\mathcal{K}p)(x) \\notin \\mathcal{G}$, which contradicts our assumption that $\\mathcal{G}$ is an invariant subspace.\n\nThe only way for the contradiction to be avoided is if $N \\le 0$. If $N=0$, the subspace consists only of constant functions $g(x)=c$. For such a function, $(\\mathcal{K}g)(x) = g(x^2) = c$, which is still a constant. So, the space of constant functions is a trivial, one-dimensional invariant subspace. However, any finite-dimensional space of polynomials with maximum degree $N  0$ cannot be invariant.\n\nThe initial computation, $(\\mathcal{K}\\{x\\})(x) = x^2$, serves as the base case for this argument. It demonstrates that an observable of degree $1$ is mapped to an observable of degree $2$. Applying the operator repeatedly generates a sequence of observables with unboundedly increasing degrees:\n$$g_0(x) = x$$\n$$(\\mathcal{K}g_0)(x) = x^2$$\n$$(\\mathcal{K}^2 g_0)(x) = \\mathcal{K}(x^2) = (x^2)^2 = x^4$$\n$$(\\mathcal{K}^n g_0)(x) = x^{2^n}$$\nAny finite-dimensional space of polynomials would have a maximum degree, and a sufficient number of applications of $\\mathcal{K}$ to the simple observable $g(x)=x$ will produce a polynomial that exceeds this maximum degree, thus leaving the space. Consequently, the space of all polynomial observables cannot be represented by a finite-dimensional invariant subspace under this Koopman operator. This is a key reason why finite-dimensional approximations like DMD can be challenging for certain nonlinear systems.",
            "answer": "$$\\boxed{x^2}$$"
        },
        {
            "introduction": "Data from real-world cyber-physical systems are invariably corrupted by noise. This exercise moves from deterministic theory to the practical realities of data-driven modeling by exploring how different types of stochastic disturbances affect DMD results. By distinguishing between process noise and measurement noise, you will learn to critically assess the source of errors and biases in your estimated Koopman operator, a vital skill for any practitioner .",
            "id": "4219056",
            "problem": "A cyber-physical system (CPS) digital twin monitors a discrete-time plant whose true state evolution is governed by unknown nonlinear dynamics. Let the true state be denoted by $x_k \\in \\mathbb{R}^n$ and suppose the true evolution admits a local linearization around an operating point so that for sufficiently small deviations the evolution can be approximated by $x_{k+1} \\approx A x_k$, where $A \\in \\mathbb{R}^{n \\times n}$ is an unknown linear operator. In the presence of stochastic disturbances, the plant evolves according to $x_{k+1} = A x_k + w_k$, with process noise $w_k$ modeled as $w_k \\sim \\mathcal{N}(0, Q)$ that is independent of $x_k$. Measurements available to the digital twin are $y_k = x_k + v_k$, with measurement noise $v_k \\sim \\mathcal{N}(0, R)$ independent of $x_k$ and $w_k$. The digital twin performs Dynamic Mode Decomposition (DMD) to estimate a finite-dimensional approximation of the Koopman operator, using snapshot matrices constructed from the measured data. Specifically, it forms $X = [y_0, y_1, \\dots, y_{m-1}]$ and $X' = [y_1, y_2, \\dots, y_m]$ and computes the least-squares estimator $A_{\\mathrm{DMD}} = X' X^{\\dagger}$, where $X^{\\dagger}$ denotes the Moore-Penrose pseudoinverse. Assume ergodicity and stationarity so that empirical covariances converge to their expectations as $m \\to \\infty$, and the snapshot matrix $X$ has full row rank with high probability. The Koopman operator for deterministic dynamics maps an observable $g$ as $U g(x) = g(A x)$ and its eigenvalues characterize linear evolution of observables; under stochastic dynamics with independent, zero-mean process noise, the Markov (stochastic) Koopman operator is defined by $U g(x) = \\mathbb{E}[g(x_{k+1}) \\mid x_k = x]$.\n\nUsing only the above base definitions and assumptions, reason about how process noise and measurement noise differently affect the estimates produced by data-driven Koopman analysis via DMD. Focus on the direction of bias in the estimated eigenvalues and the robustness of estimated modes. Which of the following statements is most consistent with first-principles derivations under the stated assumptions?\n\nA. When only process noise $w_k$ is present and measurements are noise-free ($v_k = 0$), $A_{\\mathrm{DMD}}$ is an unbiased estimator of the underlying linear operator $A$ in expectation, so Koopman/DMD eigenvalues are correct on average but exhibit increased variance; when only measurement noise $v_k$ is present and the true dynamics is noise-free ($w_k = 0$), $A_{\\mathrm{DMD}}$ suffers errors-in-variables bias that shrinks eigenvalue magnitudes toward the origin (inside the unit circle for discrete-time), and DMD modes are biased toward sensor-noise directions unless specialized corrections are used.\n\nB. Measurement noise $v_k$ does not bias $A_{\\mathrm{DMD}}$ and only inflates variance, so DMD eigenvalues remain correct on average; process noise $w_k$ systematically biases $A_{\\mathrm{DMD}}$ to have larger eigenvalue magnitudes than the truth (pushing eigenvalues outward), because $w_k$ adds energy to $X'$ but not to $X$.\n\nC. Process noise $w_k$ and measurement noise $v_k$ have equivalent effects on $A_{\\mathrm{DMD}}$, both causing eigenvalues to shift outward and DMD modes to be unbiased but more variable; their effects cannot be distinguished asymptotically from snapshot covariances.\n\nD. Process noise $w_k$ generally makes DMD infer instability by moving eigenvalues outside the unit circle, while measurement noise $v_k$ only adds random perturbations with no systematic bias in eigenvalues or modes.\n\nE. Measurement noise $v_k$ mainly rotates DMD eigenvectors (modes) without affecting eigenvalue magnitudes, whereas process noise $w_k$ mainly reduces eigenvalue magnitudes, both with negligible impact on variance in large samples.\n\nChoose the single best option.",
            "solution": "The problem statement has been validated and is found to be scientifically grounded, well-posed, objective, and self-contained. The analysis can proceed.\n\nThe core of the problem is to analyze the statistical properties of the Dynamic Mode Decomposition (DMD) estimator for a linear system operator $A$ under two distinct noise scenarios: process noise and measurement noise.\n\nThe system is described by the state evolution and measurement equations:\n$$x_{k+1} = A x_k + w_k$$\n$$y_k = x_k + v_k$$\nwhere $w_k \\sim \\mathcal{N}(0, Q)$ is the process noise and $v_k \\sim \\mathcal{N}(0, R)$ is the measurement noise. The DMD estimator for the system matrix is given by $A_{\\mathrm{DMD}} = X' X^{\\dagger}$, where $X = [y_0, y_1, \\dots, y_{m-1}]$ and $X' = [y_1, y_2, \\dots, y_m]$. $X^{\\dagger}$ is the Moore-Penrose pseudoinverse.\n\nThe problem states to assume ergodicity and stationarity, allowing us to analyze the estimator in the limit of a large number of snapshots ($m \\to \\infty$). In this limit, the estimator converges to:\n$$A_{\\mathrm{DMD}} \\to \\left(\\lim_{m\\to\\infty} \\frac{1}{m} X' X^T\\right) \\left(\\lim_{m\\to\\infty} \\frac{1}{m} X X^T\\right)^{-1} = \\mathbb{E}[y_{k+1} y_k^T] (\\mathbb{E}[y_k y_k^T])^{-1}$$\n\nLet's compute the necessary expectation terms.\nLet $\\Sigma_x = \\mathbb{E}[x_k x_k^T]$ be the state covariance matrix. Under the stationarity assumption, $\\Sigma_x$ is constant and satisfies the discrete-time Lyapunov equation:\n$\\Sigma_x = \\mathbb{E}[(A x_{k-1} + w_{k-1})(A x_{k-1} + w_{k-1})^T] = A \\mathbb{E}[x_{k-1}x_{k-1}^T] A^T + \\mathbb{E}[w_{k-1}w_{k-1}^T] = A \\Sigma_x A^T + Q$.\n\nThe data covariance matrix is:\n$C_{yy} = \\mathbb{E}[y_k y_k^T] = \\mathbb{E}[(x_k + v_k)(x_k + v_k)^T] = \\mathbb{E}[x_k x_k^T] + \\mathbb{E}[x_k v_k^T] + \\mathbb{E}[v_k x_k^T] + \\mathbb{E}[v_k v_k^T]$.\nSince $v_k$ is independent of $x_k$, the cross-terms are zero. Thus,\n$$C_{yy} = \\Sigma_x + R$$\n\nThe cross-covariance matrix is:\n$C_{y'y} = \\mathbb{E}[y_{k+1} y_k^T] = \\mathbb{E}[(x_{k+1} + v_{k+1})(x_k + v_k)^T] = \\mathbb{E}[x_{k+1} x_k^T] + \\mathbb{E}[x_{k+1} v_k^T] + \\mathbb{E}[v_{k+1} x_k^T] + \\mathbb{E}[v_{k+1} v_k^T]$.\nThe noise terms are independent of the state and are assumed to be temporally uncorrelated (white noise), so all cross-terms involving noise are zero.\n$C_{y'y} = \\mathbb{E}[x_{k+1} x_k^T] = \\mathbb{E}[(A x_k + w_k) x_k^T] = A \\mathbb{E}[x_k x_k^T] + \\mathbb{E}[w_k x_k^T]$.\nSince $w_k$ is independent of $x_k$ (which depends only on past noise $w_{jk}$), $\\mathbb{E}[w_k x_k^T] = 0$. So,\n$$C_{y'y} = A \\Sigma_x$$\n\nCombining these results, the asymptotic estimator is:\n$$A_{\\mathrm{DMD}} \\to (A \\Sigma_x) (\\Sigma_x + R)^{-1}$$\n\nNow, we analyze the two specific cases.\n\n**Case 1: Process Noise Only ($w_k \\neq 0, v_k = 0$)**\nIn this case, the measurement noise covariance is $R=0$. The measurements are perfect: $y_k = x_k$.\nThe asymptotic estimator becomes:\n$$A_{\\mathrm{DMD}} \\to (A \\Sigma_x) (\\Sigma_x + 0)^{-1} = A \\Sigma_x \\Sigma_x^{-1} = A$$\nIn the limit of infinite data, the estimator converges to the true system matrix $A$.\nFor a finite number of samples $m$, the estimator is $A_{\\mathrm{DMD}} = X'X^{\\dagger}$. Here $X = [x_0, \\dots, x_{m-1}]$ and $X' = [x_1, \\dots, x_m]$.\nWe can write $X' = AX + W$, where $W = [w_0, \\dots, w_{m-1}]$.\nSo, $A_{\\mathrm{DMD}} = (AX + W)X^{\\dagger} = AXX^{\\dagger} + WX^{\\dagger}$.\nAssuming $X$ has full row rank, $XX^{\\dagger} = I$.\n$A_{\\mathrm{DMD}} = A + WX^{\\dagger}$.\nTo check for bias, we take the expectation, conditional on the state trajectory $X$:\n$\\mathbb{E}[A_{\\mathrm{DMD}} \\mid X] = A + \\mathbb{E}[W \\mid X] X^{\\dagger}$.\nSince $w_k$ is independent of past states $x_j$ for $j \\le k$, it is independent of $X$. Thus, $\\mathbb{E}[W \\mid X] = \\mathbb{E}[W] = 0$.\nSo, $\\mathbb{E}[A_{\\mathrm{DMD}} \\mid X] = A$, which implies $\\mathbb{E}[A_{\\mathrm{DMD}}] = A$.\nThe DMD estimator is unbiased in the presence of only process noise. The eigenvalues of $A_{\\mathrm{DMD}}$ will be centered around the true eigenvalues of $A$. However, the estimation error $A_{\\mathrm{DMD}} - A = WX^{\\dagger}$ is non-zero, and its covariance depends on $Q$. This means the estimate is noisy, and the estimated eigenvalues will exhibit variance around their true mean values.\n\n**Case 2: Measurement Noise Only ($w_k = 0, v_k \\neq 0$)**\nIn this case, the process noise covariance is $Q=0$. The dynamics are deterministic: $x_{k+1} = A x_k$.\nThe noise-free state covariance $\\Sigma_x$ is assumed to be non-zero (e.g., through averaging over initial conditions or motion on a stable attractor).\nThe measurement noise covariance is $R \\neq 0$.\nThe asymptotic estimator is:\n$$A_{\\mathrm{DMD}} \\to (A \\Sigma_x) (\\Sigma_x + R)^{-1}$$\nThis is clearly not equal to $A$ in general. The estimator is biased. Let's analyze the bias:\n$A_{\\mathrm{DMD}} \\to A \\Sigma_x (\\Sigma_x + R)^{-1} = A (\\Sigma_x + R - R) (\\Sigma_x + R)^{-1} = A(I - R(\\Sigma_x + R)^{-1})$.\nThis is a classic result from errors-in-variables regression. The bias matrix is $- A R(\\Sigma_x + R)^{-1}$.\nThe term $R(\\Sigma_x + R)^{-1}$ is related to the noise-to-signal ratio. The factor $(I - R(\\Sigma_x + R)^{-1})$ acts as an attenuation matrix.\nConsider a simple scalar case where $A, \\Sigma_x, R$ are numbers $a, \\sigma_x^2, r^2$.\n$a_{\\mathrm{DMD}} \\to a \\frac{\\sigma_x^2}{\\sigma_x^2 + r^2}$.\nSince $\\frac{\\sigma_x^2}{\\sigma_x^2 + r^2}  1$, the estimated eigenvalue $a_{\\mathrm{DMD}}$ has a smaller magnitude than the true eigenvalue $a$, i.e., $|a_{\\mathrm{DMD}}|  |a|$. This bias shrinks the eigenvalues toward the origin.\nIn the general matrix case, the estimated operator $A_{\\mathrm{DMD}}$ is a \"shrunken\" version of $A$. This causes its eigenvalues to be biased toward the origin (i.e., their magnitudes are attenuated).\nFurthermore, the eigenvectors of $A_{\\mathrm{DMD}}$ will not, in general, be the same as the eigenvectors of $A$. The bias depends on the covariance structure of the measurement noise, $R$. If noise is particularly strong in certain sensor channels (large diagonal entries in $R$), the estimated modes (eigenvectors) will be distorted in ways that reflect this noise structure.\n\n**Evaluation of the Options**\n\n*   **A. When only process noise $w_k$ is present and measurements are noise-free ($v_k = 0$), $A_{\\mathrm{DMD}}$ is an unbiased estimator of the underlying linear operator $A$ in expectation, so Koopman/DMD eigenvalues are correct on average but exhibit increased variance; when only measurement noise $v_k$ is present and the true dynamics is noise-free ($w_k = 0$), $A_{\\mathrm{DMD}}$ suffers errors-in-variables bias that shrinks eigenvalue magnitudes toward the origin (inside the unit circle for discrete-time), and DMD modes are biased toward sensor-noise directions unless specialized corrections are used.**\n    This statement is perfectly aligned with our derivations. The first part correctly identifies that process noise leads to an unbiased estimate with increased variance. The second part correctly identifies the errors-in-variables problem, the resulting attenuation bias (shrinking eigenvalues), and the bias in the modes due to measurement noise. **Correct.**\n\n*   **B. Measurement noise $v_k$ does not bias $A_{\\mathrm{DMD}}$ and only inflates variance, so DMD eigenvalues remain correct on average; process noise $w_k$ systematically biases $A_{\\mathrm{DMD}}$ to have larger eigenvalue magnitudes than the truth (pushing eigenvalues outward), because $w_k$ adds energy to $X'$ but not to $X$.**\n    This is incorrect. It reverses the roles of the two noise types regarding bias. Measurement noise causes bias, whereas process noise does not. The reasoning about energy is also flawed; the crucial property is the correlation between noise and data, not just the energy. **Incorrect.**\n\n*   **C. Process noise $w_k$ and measurement noise $v_k$ have equivalent effects on $A_{\\mathrm{DMD}}$, both causing eigenvalues to shift outward and DMD modes to be unbiased but more variable; their effects cannot be distinguished asymptotically from snapshot covariances.**\n    This is incorrect. The effects are fundamentally different, as shown by our analysis (no bias vs. attenuation bias). The claim that both cause eigenvalues to shift outward is false. The claim that modes are unbiased under measurement noise is false. Their effects are distinguishable. **Incorrect.**\n\n*   **D. Process noise $w_k$ generally makes DMD infer instability by moving eigenvalues outside the unit circle, while measurement noise $v_k$ only adds random perturbations with no systematic bias in eigenvalues or modes.**\n    This is incorrect. It mischaracterizes both effects. Process noise does not create a systematic bias towards instability. Measurement noise creates a systematic attenuation bias. **Incorrect.**\n\n*   **E. Measurement noise $v_k$ mainly rotates DMD eigenvectors (modes) without affecting eigenvalue magnitudes, whereas process noise $w_k$ mainly reduces eigenvalue magnitudes, both with negligible impact on variance in large samples.**\n    This is incorrect. Measurement noise is well-known to attenuate eigenvalue magnitudes. Process noise does not systematically reduce eigenvalue magnitudes (it leads to an unbiased estimate). Both noise sources contribute to estimation variance. **Incorrect.**\n\nThe analysis confirms that statement A provides the most accurate and consistent description of the effects of process and measurement noise on DMD estimates.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}