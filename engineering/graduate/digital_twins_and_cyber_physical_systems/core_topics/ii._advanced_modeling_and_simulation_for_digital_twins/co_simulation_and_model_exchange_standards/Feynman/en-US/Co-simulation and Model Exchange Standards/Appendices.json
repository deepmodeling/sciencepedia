{
    "hands_on_practices": [
        {
            "introduction": "At the heart of co-simulation lies the challenge of coupling continuous-time models through discrete communication steps. The way a receiving model reconstructs an input signal from discrete data points directly impacts the simulation's accuracy. This practice guides you through a fundamental analysis of the local truncation error introduced by different signal extrapolation and interpolation schemes, providing a crucial, quantitative understanding of the trade-offs between computational simplicity and numerical fidelity. ",
            "id": "4208494",
            "problem": "A co-simulation between two Functional Mock-up Units (FMUs) exchanges an input signal $u(t)$ at communication times $t=0$ and $t=h$, where $h>0$ is the macro-step size. The receiving FMU implements a single-input single-output linear time-invariant ordinary differential equation (ODE) whose continuous-time model is\n$$\n\\frac{d x(t)}{d t} = a\\, x(t) + b\\, u(t),\n$$\nwith constant parameters $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$, and initial state $x(0)$ given. Between communication points, the master algorithm reconstructs the input $u(t)$ by one of the following time-domain schemes on the interval $t \\in [0,h]$:\n- Zero-order hold (ZOH): $u_{\\mathrm{ZOH}}(t) = u(0)$,\n- First-order hold (FOH): $u_{\\mathrm{FOH}}(t)$ is the unique affine function such that $u_{\\mathrm{FOH}}(0) = u(0)$ and $u_{\\mathrm{FOH}}(h) = u(h)$,\n- Quadratic polynomial interpolation: $u_{\\mathrm{QP}}(t)$ is the unique polynomial of degree at most $2$ interpolating $u(0)$, $u(h/2)$, and $u(h)$.\n\nAssume $u(t)$ is three times continuously differentiable on a neighborhood of $t=0$, and write its Taylor expansion about $t=0$ as\n$$\nu(t) = u(0) + u^{(1)}(0)\\, t + \\frac{u^{(2)}(0)}{2}\\, t^{2} + \\frac{u^{(3)}(0)}{6}\\, t^{3} + \\mathcal{O}(t^{4}),\n$$\nwhere $u^{(k)}(0)$ denotes the $k$-th time derivative of $u(t)$ evaluated at $t=0$. Suppose the FMU’s internal integrator is exact given the supplied input reconstruction, so that the only source of local truncation error over $[0,h]$ is the input approximation.\n\nStarting from fundamental definitions and well-tested formulas for linear ODEs, derive and compare the leading-order behavior in $h$ of the input-induced local truncation error in the state at time $t=h$, denoted $\\Delta x(h) = x_{\\mathrm{approx}}(h) - x_{\\mathrm{exact}}(h)$, for the three schemes. Then, explicitly compute the leading-order term of $\\Delta x(h)$ for the first-order hold scheme, expressed in terms of $a$, $b$, $h$, and $u^{(k)}(0)$.\n\nYour final answer must be a single closed-form analytic expression for the leading-order term of $\\Delta x(h)$ under first-order hold. No rounding is required, and no units should be included in the final boxed expression.",
            "solution": "The system is governed by the linear time-invariant ordinary differential equation\n$$\n\\frac{d x(t)}{d t} = a\\, x(t) + b\\, u(t).\n$$\nFor any input $v(t)$ and initial condition $x(0)$, the exact solution at $t=h$ can be written using the variation of constants formula:\n$$\nx(h) = \\exp(a h)\\, x(0) + b \\int_{0}^{h} \\exp\\!\\big(a(h - \\tau)\\big)\\, v(\\tau)\\, d\\tau.\n$$\nIn our setting, the internal integrator is assumed exact for the supplied input reconstruction, so the state computed by the FMU under an approximate input $u_{\\mathrm{approx}}(t)$ will be\n$$\nx_{\\mathrm{approx}}(h) = \\exp(a h)\\, x(0) + b \\int_{0}^{h} \\exp\\!\\big(a(h - \\tau)\\big)\\, u_{\\mathrm{approx}}(\\tau)\\, d\\tau.\n$$\nThe exact state using the true input $u(t)$ is\n$$\nx_{\\mathrm{exact}}(h) = \\exp(a h)\\, x(0) + b \\int_{0}^{h} \\exp\\!\\big(a(h - \\tau)\\big)\\, u(\\tau)\\, d\\tau.\n$$\nTherefore, the input-induced local truncation error at $t=h$ is\n$$\n\\Delta x(h) \\equiv x_{\\mathrm{approx}}(h) - x_{\\mathrm{exact}}(h) = b \\int_{0}^{h} \\exp\\!\\big(a(h - \\tau)\\big)\\, \\big(u_{\\mathrm{approx}}(\\tau) - u(\\tau)\\big)\\, d\\tau.\n$$\n\nWe now analyze the leading-order behavior in $h$ for the three schemes. Throughout, we use the Taylor expansion for $u(\\tau)$ around $\\tau=0$:\n$$\nu(\\tau) = u(0) + u^{(1)}(0)\\, \\tau + \\frac{u^{(2)}(0)}{2}\\, \\tau^{2} + \\frac{u^{(3)}(0)}{6}\\, \\tau^{3} + \\mathcal{O}(\\tau^{4}).\n$$\nWe also expand the exponential factor for small $h$ as\n$$\n\\exp\\!\\big(a(h - \\tau)\\big) = 1 + a(h - \\tau) + \\frac{a^{2}}{2}(h - \\tau)^{2} + \\mathcal{O}(h^{3}),\n$$\nand note that, because we are targeting the leading-order term in $h$ of $\\Delta x(h)$, higher-order corrections in the exponential contribute terms of higher order in $h$ than the leading term induced by the input reconstruction difference and can be neglected when identifying the first nonzero term.\n\nZero-order hold (ZOH): $u_{\\mathrm{ZOH}}(\\tau) = u(0)$. The difference is\n$$\nu_{\\mathrm{ZOH}}(\\tau) - u(\\tau) = - u^{(1)}(0)\\, \\tau - \\frac{u^{(2)}(0)}{2}\\, \\tau^{2} - \\frac{u^{(3)}(0)}{6}\\, \\tau^{3} + \\mathcal{O}(\\tau^{4}).\n$$\nTaking the exponential factor as $1$ to extract the leading term, we have\n$$\n\\Delta x_{\\mathrm{ZOH}}(h) \\approx b \\int_{0}^{h} \\big(- u^{(1)}(0)\\, \\tau - \\frac{u^{(2)}(0)}{2}\\, \\tau^{2}\\big)\\, d\\tau = b \\left( - u^{(1)}(0)\\, \\frac{h^{2}}{2} - \\frac{u^{(2)}(0)}{2}\\, \\frac{h^{3}}{3} \\right) + \\mathcal{O}(h^{4}),\n$$\nso the leading-order term is of order $h^{2}$ and equals $- b\\, u^{(1)}(0)\\, h^{2}/2$.\n\nFirst-order hold (FOH): Let us define $u_{\\mathrm{FOH}}(\\tau)$ as the affine function matching $u(0)$ and $u(h)$. Denoting $u(h)$ by its Taylor expansion,\n$$\nu(h) = u(0) + u^{(1)}(0)\\, h + \\frac{u^{(2)}(0)}{2}\\, h^{2} + \\frac{u^{(3)}(0)}{6}\\, h^{3} + \\mathcal{O}(h^{4}),\n$$\nthe slope of the linear interpolant is\n$$\ns(h) = \\frac{u(h) - u(0)}{h} = u^{(1)}(0) + \\frac{u^{(2)}(0)}{2}\\, h + \\frac{u^{(3)}(0)}{6}\\, h^{2} + \\mathcal{O}(h^{3}).\n$$\nThus,\n$$\nu_{\\mathrm{FOH}}(\\tau) = u(0) + s(h)\\, \\tau = u(0) + u^{(1)}(0)\\, \\tau + \\frac{u^{(2)}(0)}{2}\\, h\\, \\tau + \\frac{u^{(3)}(0)}{6}\\, h^{2}\\, \\tau + \\mathcal{O}(h^{3}\\tau).\n$$\nSubtracting the true input,\n$$\nu_{\\mathrm{FOH}}(\\tau) - u(\\tau) = \\frac{u^{(2)}(0)}{2}\\, \\big(h\\, \\tau - \\tau^{2}\\big) + \\frac{u^{(3)}(0)}{6}\\, \\big(h^{2}\\, \\tau - \\tau^{3}\\big) + \\mathcal{O}(h^{3}\\tau, \\tau^{4}).\n$$\nUsing $\\exp\\!\\big(a(h - \\tau)\\big) = 1 + \\mathcal{O}(h)$ for the leading term, we obtain\n$$\n\\Delta x_{\\mathrm{FOH}}(h) \\approx b \\int_{0}^{h} \\left[ \\frac{u^{(2)}(0)}{2}\\, \\big(h\\, \\tau - \\tau^{2}\\big) \\right] d\\tau + \\text{higher-order terms}.\n$$\nThe integral evaluates to\n$$\n\\int_{0}^{h} \\big(h\\, \\tau - \\tau^{2}\\big)\\, d\\tau = \\left[ \\frac{h\\, \\tau^{2}}{2} - \\frac{\\tau^{3}}{3} \\right]_{0}^{h} = \\frac{h^{3}}{2} - \\frac{h^{3}}{3} = \\frac{h^{3}}{6}.\n$$\nTherefore,\n$$\n\\Delta x_{\\mathrm{FOH}}(h) = b\\, \\frac{u^{(2)}(0)}{2}\\, \\frac{h^{3}}{6} + \\mathcal{O}(h^{4}) = b\\, \\frac{u^{(2)}(0)}{12}\\, h^{3} + \\mathcal{O}(h^{4}).\n$$\nWe must verify that the neglected contribution from the exponential factor does not alter the leading-order term. Including the first correction,\n$$\n\\exp\\!\\big(a(h - \\tau)\\big) = 1 + a(h - \\tau) + \\mathcal{O}(h^{2}),\n$$\nand thus\n$$\n\\int_{0}^{h} a(h - \\tau)\\, \\left[ \\frac{u^{(2)}(0)}{2}\\, \\big(h\\, \\tau - \\tau^{2}\\big) \\right] d\\tau = a\\, \\frac{u^{(2)}(0)}{2} \\left[ h \\int_{0}^{h} \\big(h\\, \\tau - \\tau^{2}\\big) d\\tau - \\int_{0}^{h} \\tau \\big(h\\, \\tau - \\tau^{2}\\big) d\\tau \\right].\n$$\nWe have already computed $\\int_{0}^{h} \\big(h\\, \\tau - \\tau^{2}\\big) d\\tau = h^{3}/6$. Also,\n$$\n\\int_{0}^{h} \\tau \\big(h\\, \\tau - \\tau^{2}\\big)\\, d\\tau = \\int_{0}^{h} \\big(h\\, \\tau^{2} - \\tau^{3}\\big)\\, d\\tau = h\\, \\frac{h^{3}}{3} - \\frac{h^{4}}{4} = \\frac{h^{4}}{3} - \\frac{h^{4}}{4} = \\frac{h^{4}}{12}.\n$$\nThus the correction is\n$$\na\\, \\frac{u^{(2)}(0)}{2} \\left[ h \\cdot \\frac{h^{3}}{6} - \\frac{h^{4}}{12} \\right] = a\\, \\frac{u^{(2)}(0)}{2} \\left[ \\frac{h^{4}}{6} - \\frac{h^{4}}{12} \\right] = a\\, \\frac{u^{(2)}(0)}{2} \\cdot \\frac{h^{4}}{12} = \\frac{a\\, u^{(2)}(0)}{24}\\, h^{4},\n$$\nwhich is of order $h^{4}$ and therefore does not affect the leading-order term. Hence the leading-order term for FOH is conclusively\n$$\n\\Delta x_{\\mathrm{FOH}}(h) = b\\, \\frac{u^{(2)}(0)}{12}\\, h^{3} + \\mathcal{O}(h^{4}).\n$$\n\nQuadratic polynomial interpolation (QP) with nodes $0$, $h/2$, and $h$: The interpolation error for a degree-$2$ Lagrange interpolant at a point $\\tau \\in [0,h]$ is given by the standard remainder formula\n$$\nu_{\\mathrm{QP}}(\\tau) - u(\\tau) = \\frac{u^{(3)}(\\xi_{\\tau})}{3!}\\, (\\tau - 0)\\, \\left(\\tau - \\frac{h}{2}\\right)\\, (\\tau - h),\n$$\nfor some $\\xi_{\\tau} \\in (0,h)$. Therefore $u_{\\mathrm{QP}}(\\tau) - u(\\tau) = \\mathcal{O}(h^{3})$ uniformly on $[0,h]$. Integrating against the exponential factor, the leading contribution to $\\Delta x_{\\mathrm{QP}}(h)$ scales like\n$$\n\\Delta x_{\\mathrm{QP}}(h) = b \\int_{0}^{h} \\exp\\!\\big(a(h - \\tau)\\big)\\, \\mathcal{O}(h^{3})\\, d\\tau = \\mathcal{O}(h^{4}),\n$$\nwith the explicit leading term proportional to $u^{(3)}(0)\\, h^{4}$ when expanding $u^{(3)}(\\xi_{\\tau})$ near $0$.\n\nSummary of orders:\n- Zero-order hold yields $\\Delta x_{\\mathrm{ZOH}}(h) = - b\\, \\frac{u^{(1)}(0)}{2}\\, h^{2} + \\mathcal{O}(h^{3})$; leading order is $h^{2}$.\n- First-order hold yields $\\Delta x_{\\mathrm{FOH}}(h) = b\\, \\frac{u^{(2)}(0)}{12}\\, h^{3} + \\mathcal{O}(h^{4})$; leading order is $h^{3}$.\n- Quadratic polynomial interpolation yields $\\Delta x_{\\mathrm{QP}}(h) = \\mathcal{O}(h^{4})$ with leading term proportional to $u^{(3)}(0)\\, h^{4}$.\n\nThe requested explicit leading-order term for the first-order hold scheme is\n$$\nb\\, \\frac{u^{(2)}(0)}{12}\\, h^{3}.\n$$",
            "answer": "$$\\boxed{b\\,\\frac{u^{(2)}(0)}{12}\\,h^{3}}$$"
        },
        {
            "introduction": "A key responsibility of a co-simulation master is to orchestrate multiple, potentially event-driven, components without missing critical discrete events. The FMI standard facilitates this through features like time-of-next-event, which allows a model to inform the master about its internal state trajectory. This hands-on problem challenges you to design a safe and adaptive step-sizing algorithm by synthesizing event horizon data from multiple FMUs, while also accounting for real-world uncertainties like clock offsets. ",
            "id": "4208538",
            "problem": "A digital twin co-simulation master orchestrates a multi-rate coupling of four Functional Mock-up Units (FMUs) using the Functional Mock-up Interface (FMI). In multi-rate settings, discrete events from different FMUs occur at distinct times and can be missed if a macro-step advances past an FMU’s earliest possible event occurrence. The FMI standard provides two relevant quantities per FMU to avoid missed events: (i) time stamps for scheduled activations (periodic or externally scheduled), and (ii) time-of-next-event signals reporting the earliest time under a constant-input assumption at which the FMU may require a discrete update. Assume clock synchronization errors are bounded and accounted for conservatively by subtracting the maximum offset from each FMU’s reported time.\n\nStarting from the core definitions that a discrete event is a discontinuity that must be synchronized at a macro-step boundary, that time stamps are monotone in the master time, and that a time-of-next-event is a lower bound on the next event under constant inputs, derive the condition on the macro-step $h$ chosen at current master time $t_0$ that ensures no discrete event is missed across all FMUs. Then, apply your derivation to the following co-simulation snapshot.\n\nThe current master time is $t_0 = 12.500\\ \\mathrm{s}$. The FMUs report:\n\n- FMU A (continuous plant with a guarded zero-crossing): time-of-next-event $t_{\\mathrm{NE},A} = 12.533\\ \\mathrm{s}$; maximum clock offset magnitude $\\Delta_A = 0.0015\\ \\mathrm{s}$.\n\n- FMU B (periodic sampled controller): sample period $T_B = 0.008\\ \\mathrm{s}$; the last sample occurred at $12.496\\ \\mathrm{s}$, so the next local time stamp is $t_{\\mathrm{stamp},B} = 12.504\\ \\mathrm{s}$; maximum clock offset magnitude $\\Delta_B = 0.0005\\ \\mathrm{s}$.\n\n- FMU C (event-driven sensor with predictive bounds): next event will occur within the local interval $[12.512\\ \\mathrm{s}, 12.514\\ \\mathrm{s}]$; the reported time-of-next-event is the lower bound $t_{\\mathrm{NE},C} = 12.512\\ \\mathrm{s}$; maximum clock offset magnitude $\\Delta_C = 0.0010\\ \\mathrm{s}$.\n\n- FMU D (network scheduler with jitter): next local release is nominally $12.520\\ \\mathrm{s}$ with bounded jitter $\\pm 0.003\\ \\mathrm{s}$; thus the earliest local release is $t_{\\mathrm{stamp},D} = 12.517\\ \\mathrm{s}$; maximum clock offset magnitude $\\Delta_D = 0.0020\\ \\mathrm{s}$.\n\nUsing your derived condition, compute the maximal safe macro-step $h_{\\max}$ that avoids missing any discrete event in this snapshot. Express the final step size in seconds ($\\mathrm{s}$) and round your answer to four significant figures.",
            "solution": "The objective is to ensure that a macro-step selected by the co-simulation master does not advance the simulation past any time at which an FMU may trigger a discrete event. The fundamental basis for the derivation is:\n\n1. A discrete event is a discontinuity that must be synchronized at a macro-step boundary, meaning that the co-simulation master must end a macro-step at or before the earliest time a discrete event can occur across all FMUs.\n\n2. Time stamps produced by FMUs for scheduled activations are monotone in master time and indicate specific instants at which a discrete interaction is required.\n\n3. A time-of-next-event provided by an FMU is a lower bound on the earliest next event time under constant inputs. To be conservative in multi-rate settings, the co-simulation master must assume that this event can occur at any time at or after the lower bound, not later.\n\n4. With bounded clock synchronization error, if an FMU reports a local time $t_i$ for the earliest event or time stamp, and the maximum offset magnitude with respect to the master time is $\\Delta_i$, then the earliest possible time in the master’s frame at which the event might occur is $t_i - \\Delta_i$, because the FMU’s local clock may be ahead of the master by up to $\\Delta_i$.\n\nLet the co-simulation master be at current time $t_0$. For each FMU $i$, define the conservative earliest event horizon in the master time frame as\n$$\nt_{\\mathrm{safe}, i} = t_i - \\Delta_i,\n$$\nwhere $t_i$ is either the FMU’s next time stamp $t_{\\mathrm{stamp}, i}$ (for scheduled periodic or externally driven events) or its time-of-next-event $t_{\\mathrm{NE}, i}$ (for internally predicted events), whichever is applicable to the earliest discrete interaction requirement.\n\nTo avoid missing any discrete event, the macro-step $h$ must satisfy\n$$\nt_0 + h \\leq \\min_i t_{\\mathrm{safe}, i}.\n$$\nTherefore, the maximal safe macro-step is\n$$\nh_{\\max} = \\min_i t_{\\mathrm{safe}, i} - t_0.\n$$\n\nWe now apply this to the given snapshot.\n\n- FMU A: $t_{\\mathrm{NE}, A} = 12.533\\ \\mathrm{s}$, $\\Delta_A = 0.0015\\ \\mathrm{s}$. Hence,\n$$\nt_{\\mathrm{safe}, A} = 12.533 - 0.0015 = 12.5315\\ \\mathrm{s}.\n$$\n\n- FMU B: periodic sample period $T_B = 0.008\\ \\mathrm{s}$; last sample at $12.496\\ \\mathrm{s}$, so\n$$\nt_{\\mathrm{stamp}, B} = 12.496 + 0.008 = 12.504\\ \\mathrm{s}.\n$$\nWith $\\Delta_B = 0.0005\\ \\mathrm{s}$,\n$$\nt_{\\mathrm{safe}, B} = 12.504 - 0.0005 = 12.5035\\ \\mathrm{s}.\n$$\n\n- FMU C: reported lower bound $t_{\\mathrm{NE}, C} = 12.512\\ \\mathrm{s}$, $\\Delta_C = 0.0010\\ \\mathrm{s}$. Hence,\n$$\nt_{\\mathrm{safe}, C} = 12.512 - 0.0010 = 12.511\\ \\mathrm{s}.\n$$\n\n- FMU D: earliest local release $t_{\\mathrm{stamp}, D} = 12.517\\ \\mathrm{s}$ (accounting for jitter), $\\Delta_D = 0.0020\\ \\mathrm{s}$. Hence,\n$$\nt_{\\mathrm{safe}, D} = 12.517 - 0.0020 = 12.515\\ \\mathrm{s}.\n$$\n\nCompute the minimum of these conservative horizons:\n$$\n\\min_i t_{\\mathrm{safe}, i} = \\min\\{12.5315,\\ 12.5035,\\ 12.511,\\ 12.515\\} = 12.5035\\ \\mathrm{s}.\n$$\n\nTherefore,\n$$\nh_{\\max} = 12.5035 - 12.500 = 0.0035\\ \\mathrm{s}.\n$$\n\nRounding $0.0035\\ \\mathrm{s}$ to four significant figures, the step size becomes\n$$\n3.500 \\times 10^{-3}.\n$$",
            "answer": "$$\\boxed{3.500 \\times 10^{-3}}$$"
        },
        {
            "introduction": "When subsystems in a digital twin are tightly coupled, their interactions can form instantaneous algebraic loops, posing a significant numerical challenge. A powerful technique to resolve these is \"tearing,\" where a strategic subset of variables is chosen to break the dependency cycle, allowing for an iterative solution. This exercise guides you to implement and analyze tearing strategies, connecting the abstract concept of an iteration matrix's spectral radius to the practical goal of ensuring fast and stable convergence for the coupled system. ",
            "id": "4208462",
            "problem": "You are given a coupled algebraic system that arises from the linearization of a co-simulation interconnection under the Functional Mock-up Interface (FMI) standard. In this context, multiple Functional Mock-up Units (FMUs) exchange inputs and outputs within a single macro-step, resulting in algebraic loops. A classical approach is to perform a tearing: select a subset of variables to be solved implicitly, breaking the loop and enabling block-iterative solution. Consider a linearized residual system $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$. A tearing strategy chooses an index set $T \\subset \\{0,1,\\dots,n-1\\}$ of size $k$ (the torn variables), with the retained set $R = \\{0,1,\\dots,n-1\\} \\setminus T$. Let $A$ be permuted into block form according to $T$ first and $R$ second:\n$$\nA =\n\\begin{bmatrix}\nA_{TT} & A_{TR} \\\\\nA_{RT} & A_{RR}\n\\end{bmatrix},\n\\quad\nx =\n\\begin{bmatrix}\nx_T \\\\\nx_R\n\\end{bmatrix},\n\\quad\nb =\n\\begin{bmatrix}\nb_T \\\\\nb_R\n\\end{bmatrix}.\n$$\nA block Gauss-Seidel update computes\n$$x_T^{(k+1)} = A_{TT}^{-1} \\left( b_T - A_{TR} x_R^{(k)} \\right),$$\nfollowed by\n$$x_R^{(k+1)} = A_{RR}^{-1} \\left( b_R - A_{RT} x_T^{(k+1)} \\right).$$\nThis induces an affine iteration $x^{(k+1)} = M x^{(k)} + c$, with a block iteration matrix whose nonzero block is $S_R = A_{RR}^{-1} A_{RT} A_{TT}^{-1} A_{TR}$ acting on $x_R^{(k)}$. The numerical stability and local convergence rate of this iteration are governed by the spectral radius $\\rho(S_R)$: by the Banach fixed-point theorem, if there exists a norm such that the induced operator norm is less than $1$, then the iteration converges; for a linear map, a necessary condition is $\\rho(S_R) < 1$, and a smaller $\\rho(S_R)$ implies faster asymptotic error decay.\n\nYour task is to design and implement a program that, for each test case specified below, enumerates all tearing sets $T$ of a given size $k$, filters out those for which either $A_{TT}$ or $A_{RR}$ is numerically singular or severely ill-conditioned, computes $\\rho(S_R)$ for each valid $T$, selects the tearing set with the minimal $\\rho(S_R)$, and then simulates the block Gauss-Seidel iteration from the initial condition $x^{(0)} = \\mathbf{1}$ (the vector of all ones) to compute the number of iterations required to achieve $$\\| x^{(k+1)} - x^{(k)} \\|_2 < \\text{tol}$$ with a specified tolerance. For the purposes of this task, treat the condition number as $\\kappa(M) = \\|M\\|_2 \\|M^{-1}\\|_2$ approximated numerically, and reject blocks whose condition number exceeds $10^{10}$. All computations must be performed in double precision.\n\nTest suite definition:\n\n- Test case $1$: $n = 3$,\n$$\nA_1 =\n\\begin{bmatrix}\n1.0 & -0.2 & 0.0 \\\\\n-0.3 & 1.0 & -0.1 \\\\\n0.0 & -0.4 & 1.0\n\\end{bmatrix},\n$$\n$$\nb_1 =\n\\begin{bmatrix}\n1.0 \\\\\n1.0 \\\\\n1.0\n\\end{bmatrix},\n$$\n$k_1 = 1$, $\\text{tol}_1 = 10^{-8}$, $\\text{max\\_iter}_1 = 10000$.\n\n- Test case $2$: $n = 4$,\n$$\nA_2 =\n\\begin{bmatrix}\n1.0 & -0.9 & 0.0 & 0.0 \\\\\n-0.9 & 1.0 & -0.9 & 0.0 \\\\\n0.0 & -0.9 & 1.0 & -0.9 \\\\\n0.0 & 0.0 & -0.9 & 1.0\n\\end{bmatrix},\n$$\n$$\nb_2 =\n\\begin{bmatrix}\n1.0 \\\\\n1.0 \\\\\n1.0 \\\\\n1.0\n\\end{bmatrix},\n$$\n$k_2 = 2$, $\\text{tol}_2 = 10^{-8}$, $\\text{max\\_iter}_2 = 10000$.\n\n- Test case $3$: $n = 5$,\n$$\nA_3 =\n\\begin{bmatrix}\n1.0 & -0.95 & 0.0 & 0.0 & 0.0 \\\\\n-0.95 & 1.0 & -0.5 & 0.0 & 0.0 \\\\\n0.0 & -0.5 & 1.0 & -0.5 & 0.0 \\\\\n0.0 & 0.0 & -0.5 & 1.0 & -0.95 \\\\\n0.0 & 0.0 & 0.0 & -0.95 & 1.0\n\\end{bmatrix},\n$$\n$$\nb_3 =\n\\begin{bmatrix}\n1.0 \\\\\n1.0 \\\\\n1.0 \\\\\n1.0 \\\\\n1.0\n\\end{bmatrix},\n$$\n$k_3 = 2$, $\\text{tol}_3 = 10^{-8}$, $\\text{max\\_iter}_3 = 10000$.\n\nAlgorithmic requirements:\n\n- For each test case, enumerate all subsets $T$ of indices of size $k$.\n- For each $T$, form $A_{TT}$, $A_{TR}$, $A_{RT}$, $A_{RR}$ and compute $S_R = A_{RR}^{-1} A_{RT} A_{TT}^{-1} A_{TR}$. Compute the spectral radius $\\rho(S_R)$ as the maximum absolute value of the eigenvalues of $S_R$.\n- Reject $T$ if either $A_{TT}$ or $A_{RR}$ has condition number exceeding $10^{10}$.\n- Select the $T$ with minimal $\\rho(S_R)$; if multiple $T$ tie, choose the one with lexicographically smallest index list.\n- With the selected $T$, perform block Gauss-Seidel iteration starting from $x^{(0)} = \\mathbf{1}$ and using the given $b$, updating $x_T$ and $x_R$ as defined above in each iteration, until $\\| x^{(k+1)} - x^{(k)} \\|_2 < \\text{tol}$ or until $\\text{max\\_iter}$ iterations are reached. Record the number of iterations performed.\n- Define a stability indicator as a boolean $\\text{stable} = (\\rho(S_R) < 1)$.\n\nFinal output format specification:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the list corresponds to a test case and must itself be a list with the following elements in order: the selected tearing set indices as a list of integers $T$ in ascending order, the spectral radius $\\rho(S_R)$ rounded to $6$ decimal places, the integer iteration count required to meet the tolerance (or the maximum iterations if not converged), and the stability indicator as a boolean. For example, the format for three test cases is $[[T_1,\\rho_1,\\text{iter}_1,\\text{stable}_1],[T_2,\\rho_2,\\text{iter}_2,\\text{stable}_2],[T_3,\\rho_3,\\text{iter}_3,\\text{stable}_3]]$.",
            "solution": "The user-provided problem is assessed to be valid. It is a well-posed problem in numerical linear algebra, specifically concerning iterative methods for solving linear systems that arise in co-simulation frameworks like the Functional Mock-up Interface (FMI). The problem is scientifically grounded, internally consistent, and provides all necessary data and definitions for a unique solution.\n\nThe task is to determine an optimal tearing strategy for a coupled linear system $A x = b$ by selecting a set of tearing variables, indexed by a set $T$, which minimizes the spectral radius of the associated block Gauss-Seidel iteration matrix. This optimal strategy is then used to solve the system iteratively, and the number of iterations required for convergence is reported.\n\nThe theoretical foundation of this problem lies in the analysis of iterative methods. For a linear system permuted into a $2 \\times 2$ block structure based on a tearing set $T$ and its complement $R$,\n$$\n\\begin{bmatrix}\nA_{TT} & A_{TR} \\\\\nA_{RT} & A_{RR}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_T \\\\\nx_R\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nb_T \\\\\nb_R\n\\end{bmatrix}\n$$\nthe block Gauss-Seidel iteration is defined by the successive updates:\n$$\nx_T^{(k+1)} = A_{TT}^{-1} \\left( b_T - A_{TR} x_R^{(k)} \\right)\n$$\n$$\nx_R^{(k+1)} = A_{RR}^{-1} \\left( b_R - A_{RT} x_T^{(k+1)} \\right)\n$$\nSubstituting the first equation into the second yields an affine iteration for the retained variables $x_R$:\n$$\nx_R^{(k+1)} = (A_{RR}^{-1} A_{RT} A_{TT}^{-1} A_{TR}) x_R^{(k)} + A_{RR}^{-1}(b_R - A_{RT} A_{TT}^{-1} b_T)\n$$\nThis is of the form $x_R^{(k+1)} = S_R x_R^{(k)} + c_R$, where $S_R = A_{RR}^{-1} A_{RT} A_{TT}^{-1} A_{TR}$ is the iteration matrix. The convergence of this method is guaranteed if the spectral radius of $S_R$, denoted $\\rho(S_R)$, is less than $1$. The value of $\\rho(S_R)$ also dictates the asymptotic rate of convergence; a smaller spectral radius implies faster convergence.\n\nThe algorithmic procedure to solve this problem is as follows:\n\n1.  **Enumerate Tearing Sets**: For a given system size $n$ and tearing size $k$, all possible tearing sets $T$ are generated. Each $T$ is a subset of $\\{0, 1, \\dots, n-1\\}$ of size $k$. The number of such sets is given by the binomial coefficient $\\binom{n}{k}$.\n\n2.  **Evaluate Each Tearing Set**: For each candidate set $T$:\n    a.  The retained set $R = \\{0, 1, \\dots, n-1\\} \\setminus T$ is determined.\n    b.  The matrix $A$ and vector $b$ are partitioned into blocks $A_{TT}$, $A_{TR}$, $A_{RT}$, $A_{RR}$, $b_T$, and $b_R$ based on the index sets $T$ and $R$.\n    c.  **Stability Check**: The numerical stability of the block partitioning is assessed by computing the condition numbers of the diagonal blocks, $\\kappa(A_{TT})$ and $\\kappa(A_{RR})$. The 2-norm condition number is used. If either $\\kappa(A_{TT})$ or $\\kappa(A_{RR})$ exceeds the specified threshold of $10^{10}$, the tearing set $T$ is considered invalid and is discarded. This step prevents numerical issues arising from inverting nearly singular matrices.\n    d.  **Spectral Radius Calculation**: For a valid tearing set, the iteration matrix $S_R = A_{RR}^{-1} A_{RT} A_{TT}^{-1} A_{TR}$ is computed. To avoid explicit matrix inversion, which is computationally more expensive and less stable, this is calculated by solving linear systems. First, we solve $A_{TT} X = A_{TR}$ for $X$. Then, we compute $S_R$ by solving $A_{RR} S_R = A_{RT} X$. The spectral radius $\\rho(S_R)$ is then computed as the maximum absolute value of the eigenvalues of $S_R$.\n\n3.  **Optimal Set Selection**: After evaluating all valid tearing sets, the set $T_{\\text{opt}}$ that yields the minimum spectral radius $\\rho_{\\text{min}}$ is selected. In case of a tie in $\\rho$, the set whose indices are lexicographically smallest is chosen.\n\n4.  **Iterative Solution**: Using the optimal tearing set $T_{\\text{opt}}$, the block Gauss-Seidel iteration is performed.\n    a.  The iteration starts with an initial guess $x^{(0)} = \\mathbf{1}$ (a vector of all ones).\n    b.  In each step $k$, the vectors $x_T^{(k+1)}$ and $x_R^{(k+1)}$ are computed by solving the linear systems $A_{TT} x_T^{(k+1)} = b_T - A_{TR} x_R^{(k)}$ and $A_{RR} x_R^{(k+1)} = b_R - A_{RT} x_T^{(k+1)}$.\n    c.  The full solution vector $x^{(k+1)}$ is assembled from its components.\n    d.  The iteration continues until the L2-norm of the difference between successive iterates, $\\| x^{(k+1)} - x^{(k)} \\|_2$, falls below a given tolerance $\\text{tol}$, or a maximum number of iterations, $\\text{max\\_iter}$, is reached.\n\n5.  **Result Formulation**: For each test case, the final result is a list containing the optimal tearing set $T_{\\text{opt}}$ (as a list of integers), the minimal spectral radius $\\rho_{\\text{min}}$, the number of iterations performed, and a boolean indicator for stability ($\\rho_{\\text{min}} < 1$). This entire process is repeated for all provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It processes each case, finds the optimal tearing set, runs the\n    block Gauss-Seidel iteration, and formats the results for output.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, -0.2, 0.0],\n                [-0.3, 1.0, -0.1],\n                [0.0, -0.4, 1.0]\n            ]),\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"k\": 1,\n            \"tol\": 1e-8,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([\n                [1.0, -0.9, 0.0, 0.0],\n                [-0.9, 1.0, -0.9, 0.0],\n                [0.0, -0.9, 1.0, -0.9],\n                [0.0, 0.0, -0.9, 1.0]\n            ]),\n            \"b\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"k\": 2,\n            \"tol\": 1e-8,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([\n                [1.0, -0.95, 0.0, 0.0, 0.0],\n                [-0.95, 1.0, -0.5, 0.0, 0.0],\n                [0.0, -0.5, 1.0, -0.5, 0.0],\n                [0.0, 0.0, -0.5, 1.0, -0.95],\n                [0.0, 0.0, 0.0, -0.95, 1.0]\n            ]),\n            \"b\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"k\": 2,\n            \"tol\": 1e-8,\n            \"max_iter\": 10000,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(\n            case[\"A\"], case[\"b\"], case[\"k\"], case[\"tol\"], case[\"max_iter\"]\n        )\n        results.append(result)\n\n    # Manual string formatting to match the output specification precisely.\n    # Avoids spaces introduced by standard list-to-string conversion.\n    result_strings = []\n    for T, rho, iters, stable in results:\n        T_str = f'[{\",\".join(map(str, T))}]'\n        # Format rho to 6 decimal places and bool as capitalized True/False\n        item_str = f'[{T_str},{rho:.6f},{iters},{str(stable)}]'\n        result_strings.append(item_str)\n    \n    final_output = f'[{\",\".join(result_strings)}]'\n    print(final_output)\n\ndef process_case(A, b, k, tol, max_iter):\n    \"\"\"\n    Handles the logic for a single test case.\n    \"\"\"\n    n = A.shape[0]\n    indices = list(range(n))\n    \n    best_rho = float('inf')\n    best_T = None\n    \n    cond_threshold = 1e10\n\n    # 1. Enumerate all tearing sets T and find the optimal one\n    for T_tuple in itertools.combinations(indices, k):\n        T = list(T_tuple)\n        R = sorted(list(set(indices) - set(T)))\n        \n        try:\n            # Form block matrices\n            A_TT = A[np.ix_(T, T)]\n            A_TR = A[np.ix_(T, R)]\n            A_RT = A[np.ix_(R, T)]\n            A_RR = A[np.ix_(R, R)]\n            \n            # 2. Check condition numbers\n            cond_A_TT = np.linalg.cond(A_TT) if k > 0 else 1.0\n            cond_A_RR = np.linalg.cond(A_RR) if k < n else 1.0\n            \n            if cond_A_TT > cond_threshold or cond_A_RR > cond_threshold:\n                continue\n\n            # 3. Compute spectral radius of S_R\n            # S_R = inv(A_RR) * A_RT * inv(A_TT) * A_TR\n            # To avoid explicit inverses, solve linear systems\n            if k == 0 or k == n:\n                # If no tearing or full tearing, rho is 0 as there's no coupling term\n                rho = 0.0\n            else:\n                # X = inv(A_TT) * A_TR\n                X = linalg.solve(A_TT, A_TR)\n                # S_R = inv(A_RR) * (A_RT * X)\n                S_R = linalg.solve(A_RR, A_RT @ X)\n                \n                eigenvalues = linalg.eigvals(S_R)\n                rho = np.max(np.abs(eigenvalues))\n\n            # 4. Update best T based on minimal rho (with lexicographical tie-breaking)\n            if rho < best_rho:\n                best_rho = rho\n                best_T = T\n            elif rho == best_rho:\n                # For tie-breaking, current T_tuple is already lexicographically sorted\n                if best_T is None or list(T_tuple) < best_T:\n                    best_T = T\n        \n        except np.linalg.LinAlgError:\n            # This tearing set leads to a singular block, so it's invalid.\n            continue\n            \n    # 5. Perform Block Gauss-Seidel iteration with the optimal T\n    T = best_T\n    R = sorted(list(set(indices) - set(T)))\n    \n    A_TT = A[np.ix_(T, T)]\n    A_TR = A[np.ix_(T, R)]\n    A_RT = A[np.ix_(R, T)]\n    A_RR = A[np.ix_(R, R)]\n    \n    b_T = b[T]\n    b_R = b[R]\n    \n    x = np.ones(n, dtype=float)\n    \n    iter_count = 0\n    for i in range(max_iter):\n        iter_count = i + 1\n        x_prev = x.copy()\n        \n        # Get components from previous iteration\n        x_R_prev = x_prev[R]\n        \n        # Update x_T\n        rhs_T = b_T - A_TR @ x_R_prev\n        x_T_new = linalg.solve(A_TT, rhs_T)\n        \n        # Update x_R using the newly computed x_T\n        rhs_R = b_R - A_RT @ x_T_new\n        x_R_new = linalg.solve(A_RR, rhs_R)\n        \n        # Reconstruct full x vector\n        x[T] = x_T_new\n        x[R] = x_R_new\n        \n        # Check for convergence\n        if np.linalg.norm(x - x_prev, 2) < tol:\n            break\n            \n    is_stable = best_rho < 1.0\n\n    return [best_T, best_rho, iter_count, is_stable]\n\nsolve()\n```"
        }
    ]
}