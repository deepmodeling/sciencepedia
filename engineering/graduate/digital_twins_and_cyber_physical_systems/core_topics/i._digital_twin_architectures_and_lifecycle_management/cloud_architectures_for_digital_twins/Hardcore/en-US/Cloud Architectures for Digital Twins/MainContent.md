## Introduction
As Digital Twins (DTs) evolve from conceptual models to core components of modern Cyber-Physical Systems (CPS), their successful implementation increasingly depends on the sophisticated cloud platforms that host them. Building such platforms, however, presents a formidable challenge that goes beyond simply deploying services to the cloud. It demands a rigorous, first-principles approach to architecture to ensure the resulting systems are scalable, secure, performant, and economically viable. This article addresses the knowledge gap between using cloud services and architecting robust DT platforms from the ground up, providing a comprehensive guide for graduate-level engineers and architects.

This journey is structured into three distinct but interconnected chapters. We begin with **Principles and Mechanisms**, where we will dissect the foundational building blocks of a DT cloud platform. This includes the logical separation of concerns using the three-plane architecture, the physical distribution of compute across the edge-fog-cloud continuum, and the critical models for state management and [data consistency](@entry_id:748190). Next, in **Applications and Interdisciplinary Connections**, we will bring these principles to life by examining their application in diverse industries like aerospace and [smart manufacturing](@entry_id:1131785). We will analyze the real-world trade-offs, advanced security strategies like Zero Trust, and the integration with industry standards that define successful deployments. Finally, the **Hands-On Practices** chapter will provide an opportunity to apply this theoretical knowledge to solve concrete engineering problems, reinforcing key concepts in availability, cost modeling, and distributed system design.

## Principles and Mechanisms

This chapter delves into the core architectural principles and foundational mechanisms that underpin the design and operation of robust, scalable, and secure cloud platforms for Digital Twins (DTs). We will dissect the system into its constituent layers, explore the trade-offs inherent in distributed computation and data management, and establish a formal basis for evaluating system qualities such as consistency, availability, and [scalability](@entry_id:636611). The principles discussed herein are not merely theoretical constructs; they are the essential tools for an architect to reason about and build complex Cyber-Physical Systems (CPS) in the cloud.

### Macro-Architecture: Structuring the Cloud Platform

A well-designed cloud architecture for digital twins begins with a coherent high-level structure that enforces discipline, security, and maintainability. This structure is typically defined across two dimensions: a logical separation of functional responsibilities and a physical distribution of computational resources.

#### The Three-Plane Architecture: Separation of Concerns

Large-scale distributed systems, from network infrastructure to cloud platforms, benefit from a logical partitioning into three distinct planes: the **Data Plane**, the **Control Plane**, and the **Management Plane**. This separation of concerns is a foundational principle that enhances security, improves resilience, and reduces software coupling, making the system easier to manage and evolve.

The **Data Plane** is responsible for the primary, real-time function of the system. In a digital twin context, this is the "fast path" that handles the high-throughput flow of data to and from the physical assets. Its responsibilities include ingesting [telemetry](@entry_id:199548), processing data streams, maintaining the synchronized state of the twin in low-latency caches, and dispatching actuation commands back to the physical world. Components in the data plane are optimized for performance and are expected to be the most numerous and dynamic.

The **Control Plane** acts as the "brain" of the system, responsible for orchestrating and configuring the data plane. It ensures that the data plane operates as intended. Its tasks are less frequent but more privileged with respect to resource management. These include deploying new twin instances, scaling services up or down, distributing configuration updates, and performing reconciliation—the process of comparing the desired state of the system with its observed state and taking corrective action.

The **Management Plane** provides the administrative interface and governance for the entire platform. It handles functions that are cross-cutting, require the highest level of trust, and are not part of the real-time control loop. These include Identity and Access Management (IAM), Role-Based Access Control (RBAC), policy definition and auditing, billing and metering, and the platform's own deployment and update mechanisms, often managed through a Continuous Integration/Continuous Delivery (CI/CD) pipeline.

To understand the benefits of this separation, consider a cloud-hosted DT platform that must serve a fleet of cyber-physical assets . A critical goal is to reduce the **blast radius**—the extent of cascading failure or security compromise from a single faulty component. By placing these planes in separate network segments with strict trust boundaries, we can enforce the **principle of least privilege**. For example, a compromised telemetry ingestion gateway in the data plane should not have the network reachability or credentials to reconfigure the system (a control plane function) or alter user permissions (a management plane function). In an attack graph model, this separation prunes countless edges that would otherwise connect low-privilege data components to high-privilege control and management components, thereby strictly limiting the potential scope of a breach. Similarly, this layered dependency reduces software **coupling**. The data plane depends on the control plane's API for its configuration, but the control plane does not depend on the data plane's internal implementation. This allows architects to evolve or replace data plane technologies without impacting the control plane, as long as the contractual interface is maintained.

#### The Edge-Fog-Cloud Continuum

While the three-plane model provides a logical structure, the physical deployment of a digital twin often spans multiple tiers of a distributed infrastructure, commonly known as the **Edge-Fog-Cloud continuum**. Each tier presents a different trade-off profile in terms of latency, bandwidth, computational power, and trust.

*   **Edge:** This tier consists of compute resources co-located with the physical asset, such as an on-device controller or a nearby gateway. It is characterized by extremely low latency to the asset, high bandwidth for local data, and typically resides within a high-trust physical security perimeter. However, edge devices are often resource-constrained in terms of compute and storage.

*   **Fog:** This tier represents an intermediate layer, such as an on-premises datacenter or a factory-level server cluster. It offers more substantial compute and storage resources than the edge, with low-latency connectivity to multiple edge devices over a Local Area Network (LAN). It is often considered part of the trusted, on-premises environment.

*   **Cloud:** This is the centralized, off-premises public or private cloud infrastructure. It provides virtually limitless scalability for computation and storage but at the cost of higher and more variable latency (due to Wide Area Network (WAN) traversal) and potentially different trust boundaries for data.

The placement of digital twin functions—sensing, preprocessing, [model inference](@entry_id:636556), and actuation—across this continuum is a critical architectural decision dictated by the system's constraints. Consider a manufacturing CPS that requires a [closed-loop control](@entry_id:271649) response to an anomaly within a hard real-time deadline of $L_{\text{deadline}} = 15\,\text{ms}$ . The pipeline involves sensing, preprocessing data (e.g., [feature extraction](@entry_id:164394)), running an inference model to detect the anomaly, and issuing an actuation command. Let's analyze the placement of the inference model based on representative latencies: edge-to-fog round trip of $2\,\text{ms}$, fog-to-cloud round trip of $28\,\text{ms}$, and processing times for preprocessing ($t_{\text{prep}} = 3\,\text{ms}$) and inference ($t_{\text{inf}}^{\text{fog}} = 6\,\text{ms}$, $t_{\text{inf}}^{\text{cloud}} = 4\,\text{ms}$).

If we place inference in the cloud, the total latency for the loop would be $L = t_{\text{prep}} + L_{E \leftrightarrow C} + t_{\text{inf}}^{\text{cloud}} \approx 3 + (1+14) \times 2 + 4 = 37\,\text{ms}$. This clearly violates the deadline. If we place inference in the fog, the latency is $L = t_{\text{prep}} + L_{E \leftrightarrow F} + t_{\text{inf}}^{\text{fog}} = 3 + (1 \times 2) + 6 = 11\,\text{ms}$. This meets the $15\,\text{ms}$ deadline. This simple analysis demonstrates a key principle: **time-critical control loops must be closed at the lowest possible tier (edge or fog) that meets the computational requirements**. The cloud is best reserved for less time-sensitive tasks like fleet-wide analytics, model training, and long-term data archival, which can tolerate higher latency. Furthermore, trust policies often mandate that raw, proprietary sensor data remains on-premises (edge or fog), making preprocessing at these tiers a necessity before sending derived, less sensitive features to the cloud.

### Core Compute and State Management Models

Within the architectural planes, [microservices](@entry_id:751978) that implement the digital twin's functionality must be hosted on appropriate compute abstractions. The choice of abstraction is primarily dictated by the service's requirements for statefulness, lifecycle, and identity.

#### Compute Abstractions for Twin Microservices

Modern cloud platforms offer a spectrum of compute abstractions, most notably containers and serverless functions, which are orchestrated within a broader networking fabric often provided by a service mesh.

*   **Containers** provide operating system-level [virtualization](@entry_id:756508), packaging an application and its dependencies into an isolated environment using primitives like kernel namespaces and control groups ([cgroups](@entry_id:747258)). A container runs as a long-lived process with a stable network identity. This model is ideally suited for **stateful services** that must maintain in-memory state, manage durable connections, or require a continuous, uninterrupted lifecycle. Critically, containers can be attached to persistent storage volumes, allowing their state to survive restarts and failures.

*   **Serverless Functions** (or Functions-as-a-Service, FaaS) offer a higher level of abstraction. The platform manages the underlying servers, and the developer provides code that is executed in response to an event trigger. Each invocation runs in an ephemeral, sandboxed environment. This model is perfect for **stateless services** that perform a discrete task, such as processing an event or transforming data. The platform automatically scales the number of function instances based on the rate of incoming events, even scaling to zero when there is no traffic. However, this model introduces the "cold start" problem: if a function has not been invoked recently, the platform must provision a new environment, which can add significant latency (hundreds of milliseconds or more) to the first request.

*   A **Service Mesh** is an infrastructure layer that handles service-to-service communication. It is typically implemented by injecting a lightweight network proxy (a "sidecar") alongside each service instance. The mesh provides features like mutual TLS (mTLS) for security, intelligent routing, retries, circuit breaking for resilience, and detailed [observability](@entry_id:152062) (metrics, logs, traces) for all network traffic. It is crucial to understand that a service mesh is part of the networking and data plane infrastructure; it does not provide application compute or durable state itself.

Consider a digital twin that requires two [microservices](@entry_id:751978): a long-lived state synchronizer that must apply sensor updates in strict order, and a stateless event processor for handling alarms . The state synchronizer is stateful, requires a continuous identity to be an endpoint for ordered updates, and must maintain its state across failures using persistent logs and [checkpoints](@entry_id:747314). This maps perfectly to the **container** model. Using a serverless function would be disastrous; the ephemeral lifecycle would break the continuous state, and a cold start latency of, say, $t_{cs,99} = 300\,\text{ms}$ would violate any reasonable latency budget for real-time updates (e.g., $L_{\max}=80\,\text{ms}$). Conversely, the stateless event processor, which handles independent, idempotent alarm events, is a canonical use case for **serverless functions**. The platform can automatically scale out instances to handle a burst of alarms in parallel, and the at-least-once delivery semantics of event triggers are safely handled by the idempotent nature of the function.

### Data Flow and Consistency in a Distributed Twin

The lifeblood of a digital twin is data. The architectural choices for how data is ingested, persisted, and aligned with the physical world are central to the twin's fidelity and usefulness.

#### Ingestion Protocols for Telemetry

The journey of data from a physical asset to the cloud begins with an ingestion protocol. For the massive-scale, event-driven communication typical of IoT and CPS, publish-subscribe (pub-sub) messaging is the dominant paradigm. Several protocols are common, each with different trade-offs regarding reliability, performance, and features.

*   **MQTT (Message Queuing Telemetry Transport)** is a lightweight, brokered protocol widely used in IoT. It defines three Quality of Service (QoS) levels: QoS 0 (at-most-once), QoS 1 (at-least-once), and QoS 2 (exactly-once). The central broker architecture simplifies connectivity for devices behind firewalls but introduces a single point of serialization and an extra network hop, which adds latency. While MQTT ensures ordered delivery from a single publisher, it does not provide a built-in mechanism for total ordering of messages from multiple publishers based on source timestamps.

*   **AMQP (Advanced Message Queuing Protocol)** is a more feature-rich, brokered protocol that provides robust queuing semantics, transactions, and flexible routing topologies. Like MQTT, its reliance on a broker can be a latency bottleneck for time-critical applications.

*   **DDS (Data Distribution Service)** is a standard designed for high-performance, real-time, and reliable data exchange in distributed systems. A key [differentiator](@entry_id:272992) is that it is typically **brokerless**, with publishers and subscribers discovering each other and communicating directly (e.g., via UDP multicast or unicast). DDS provides a rich set of configurable QoS policies, including `RELIABILITY`, `DEADLINE` (specifying the maximum expected interval between updates), `LATENCY_BUDGET`, and `DESTINATION_ORDER` (which can enforce ordering by source timestamp at the subscriber).

For a time-critical digital twin, such as one for a manufacturing line requiring sub-5ms latency and strict ordering across multiple sensors , a brokerless protocol like DDS with its Real-Time Publish-Subscribe (RTPS) wire protocol is often the most appropriate choice. The absence of a central broker minimizes network path length and jitter. The extensive QoS policies allow architects to precisely specify the application's real-time requirements, and features like `DESTINATION_ORDER BY_SOURCE_TIMESTAMP` provide the exact mechanism needed to enforce a [total order](@entry_id:146781) based on when events occurred at the physical source, assuming synchronized clocks. Brokered protocols like MQTT and AMQP, while excellent for many IoT use cases, typically cannot meet such stringent, multi-publisher, low-latency ordering requirements.

#### State Persistence and Recovery: Event Sourcing and Snapshotting

Once ingested, the twin's state must be persisted durably. Two primary patterns for this are **snapshotting** and **event sourcing**.

**Snapshotting** is the more traditional approach, where the system periodically saves the entire current state of the digital twin to durable storage. Recovery from failure is simple and fast: load the most recent snapshot. The major drawback is that all history between snapshots is lost. One cannot ask "what was the state of the twin five minutes before the failure?" if the last snapshot was taken an hour ago.

**Event Sourcing** takes a different approach. Instead of storing the current state, the system stores a complete, append-only log of all the events that have ever modified the state. The state itself is a derived artifact, materialized by replaying the events from the beginning of time. This pattern provides a complete, immutable audit trail of the system's history. This is invaluable for debugging, as any past state can be perfectly reconstructed. It also enables powerful analytics and "what-if" scenarios by replaying events under different logic. The main challenge is that recovery can be slow if the event log is very long.

In practice, a hybrid approach is often best . The system uses event sourcing as the primary record but also periodically creates snapshots. To recover, it loads the most recent snapshot and then replays only the events that have occurred since that snapshot was taken. This provides a balance between the fast recovery of snapshotting and the complete auditability of event sourcing. The frequency of snapshotting presents a trade-off: more frequent snapshots reduce the average number of events that need to be replayed (and thus shorten recovery time), but at the cost of higher storage and computational overhead for creating the snapshots. For a system where events arrive at rate $\lambda$ and snapshots are taken every $\Delta$ time units, the expected number of events to replay after a failure is proportional to $\lambda \Delta$, illustrating this direct trade-off. In regulated contexts requiring non-repudiable histories, the immutable log provided by event sourcing is often a mandatory architectural choice .

#### State Alignment: From Physical to Digital

A digital twin is only as good as its fidelity to the physical asset. Due to sensor noise, unmeasured state variables, and network delays, the raw [telemetry](@entry_id:199548) received by the cloud is an imperfect representation of reality. Ensuring the twin's state, $\hat{x}_k$, is accurately aligned with the true physical state, $x_k$, requires more than simple [data forwarding](@entry_id:169799).

An **open-loop reporting** strategy, which simply displays the incoming sensor data $y_k$, is insufficient. It is susceptible to noise and provides no way to estimate parts of the state that are not directly measured. It cannot compensate for the effects of disturbances or latency .

A far more robust approach is **closed-loop state estimation**. This involves implementing a **[state observer](@entry_id:268642)** in the cloud. An observer, such as a Luenberger observer or a Kalman filter, is a dynamical system that runs a predictive model of the physical plant in parallel. It uses the known system dynamics (e.g., a [state-space model](@entry_id:273798) $x_{k+1} = Ax_k + Bu_k$) to predict the next state. When a new measurement $y_k$ arrives, the observer compares it to its own predicted measurement, $\hat{y}_k = C\hat{x}_k$. The difference, known as the **residual** or **innovation**, is used as a correction signal, fed back to adjust the state estimate. This is a closed-loop process within the digital twin itself, constantly correcting the twin's state to keep it aligned with the physical reality, even in the presence of noise and disturbances.

This mechanism is powerful. Provided the system is **observable** (a property of the matrices $A$ and $C$, which means the internal state can be inferred from the outputs), the observer's gain can be designed to guarantee that the estimation error $e_k = x_k - \hat{x}_k$ converges toward zero. Even with network delay, an observer can compensate by predicting the state forward to align with the current time before applying the delayed measurement correction. This model-based fusion of prediction and measurement provides a mathematically rigorous foundation for maintaining a high-fidelity digital twin .

### Synchronization and Ordering in Distributed Systems

Digital twins are inherently [distributed systems](@entry_id:268208). Events originate from many physical sources, and state is often replicated across multiple cloud regions. Establishing a consistent understanding of time and the order of events is a fundamental and non-trivial challenge.

#### Time Synchronization: The Foundation of Coordination

When events from different sources are time-stamped, the utility of those timestamps depends entirely on the synchronization of the clocks that generated them. Two primary protocols are used for this purpose:

*   **NTP (Network Time Protocol)** is the de facto standard for time synchronization over the internet. It works via software-based message exchanges and uses statistical filtering to mitigate the effects of variable network delay. However, its accuracy is fundamentally limited by the non-deterministic latencies of the OS software stack and asymmetry in network paths. In a typical cloud environment, NTP can achieve accuracy on the order of single-digit to tens of milliseconds .

*   **PTP (Precision Time Protocol, IEEE 1588)** is designed for much higher accuracy, typically in controlled network environments like [industrial automation](@entry_id:276005) or financial trading. Its key advantage is **hardware timestamping**, where timestamps are captured at the network interface controller, bypassing the software stack. PTP-aware network switches can act as **transparent clocks** or **boundary clocks**, correcting for the time packets spend within them. This allows PTP to achieve sub-microsecond accuracy.

For a digital twin application requiring event alignment within, say, $500\,\mu\text{s}$, standard NTP is unlikely to be sufficient due to the variability of multi-tenant cloud networks. In such cases, PTP with hardware support becomes necessary to provide the required temporal precision for event correlation and ordering .

#### Logical Time and Causal Ordering

When precisely synchronized physical clocks are unavailable or insufficient, [distributed systems](@entry_id:268208) rely on **[logical clocks](@entry_id:751443)** to reason about the order of events. The foundational concept is the **happened-before** relation ($\rightarrow$), which captures causality. An event $a$ happened-before an event $b$ ($a \rightarrow b$) if $b$ could have been influenced by $a$.

*   **Scalar Lamport Clocks** provide a simple mechanism to assign a monotonically increasing integer to events such that if $a \rightarrow b$, then the clock value of $a$ is less than the clock value of $b$. By combining Lamport timestamps with a deterministic tie-breaker (e.g., a writer ID), one can create a total ordering of all events that is consistent with causality. However, Lamport clocks are not sufficient to detect [concurrency](@entry_id:747654). If two events are concurrent (neither happened-before the other), a Lamport clock will still assign them an order, making them appear sequential.

*   **Vector Clocks** are more powerful. A vector clock for a system with $N$ processes is a vector of $N$ integers. It allows for determining the exact causal relationship between any two events. Specifically, $a \rightarrow b$ if and only if the vector clock of $a$ is component-wise less than or equal to the vector clock of $b$, and they are not equal. Crucially, [vector clocks](@entry_id:756458) can definitively detect concurrency: two events $a$ and $b$ are concurrent if neither's vector clock is less than or equal to the other's.

The choice between them depends on application semantics . If the application only needs a consistent [total order](@entry_id:146781) and can treat all updates as a sequential log (e.g., in a single-writer system or one with a central sequencer), then Lamport clocks suffice. But if the application needs to detect when updates from multiple writers are truly concurrent in order to apply special conflict resolution or merge logic, then [vector clocks](@entry_id:756458) are necessary. This power comes at a cost: for $N$ writers, the [metadata](@entry_id:275500) overhead of a vector clock is $N$ times that of a scalar clock, a significant trade-off in high-throughput systems.

#### Data Consistency Models

When a digital twin's state is replicated across multiple geographic regions for low-latency access or resilience, we must define the rules governing how updates propagate and are seen by observers. This is the role of a **consistency model**.

*   **Strong Consistency (Linearizability)** is the most intuitive model. It guarantees that all operations appear to have taken place atomically at some single point in time, and their global order is consistent with the real-time ordering of non-overlapping operations. It behaves as if there is only a single copy of the data.

*   **Causal Consistency** is a weaker model that guarantees that if one operation causally happened-before another, all replicas will see them in that order. However, it makes no guarantees about the ordering of concurrent operations; different replicas may see concurrent updates in different orders.

*   **Eventual Consistency** is the weakest model. It only guarantees that if no new updates are made, all replicas will eventually converge to the same state. During the convergence process, replicas can be out of sync and may see updates in different orders.

The choice of consistency model has profound implications for safety . Consider an actuator controlled by multiple operators in different regions. If the actuation commands are non-commutative (e.g., "rotate 30 degrees" followed by "extend arm" is different from the reverse), then eventual or causal consistency is dangerously insufficient. Concurrent commands could be applied in different orders, leading to divergent and unsafe physical states. In such a scenario, a model that establishes a [total order](@entry_id:146781) on all commands is required. This can be achieved with strong consistency ([linearizability](@entry_id:751297)), or by building a total ordering mechanism (e.g., using a [consensus algorithm](@entry_id:1122892) like Paxos or Raft) on top of a weaker model. However, if the architecture can guarantee a **single writer** for each actuator, the problem simplifies dramatically. Since all commands for an actuator originate from a single source, they are already causally ordered, and a FIFO communication channel is sufficient to ensure they are applied correctly without needing expensive global consensus.

### Operational Qualities: Scalability, Availability, and Durability

Beyond functional correctness, a cloud architecture for digital twins must be designed to meet non-functional requirements for performance and robustness as the system grows.

#### Scalability and Elasticity

**Scalability** refers to the ability of a system to handle a growing amount of work by adding resources. It is an architectural property, often measured by speedup $S(n) = T(n)/T(1)$, where $T(n)$ is the throughput with $n$ resources. A system that scales linearly has $S(n) \approx n$. **Elasticity**, on the other hand, is a dynamic property. It refers to the ability of the system to *automatically* provision and de-provision resources to match the workload demand in real time. A system can be scalable but not elastic if it requires manual intervention to add resources.

These concepts manifest differently for stateless and stateful services .
*   **Stateless services**, like an event processing tier, are easy to scale. Since each request is independent, one can simply add more identical instances behind a load balancer (horizontal scaling). The performance of such a system can often be modeled by an $M/M/c$ queue, where adding more servers ($c$) reduces utilization and queueing delays, leading to near-linear throughput gains until a shared bottleneck (like a database) is hit.
*   **Stateful services**, like a twin state store, are much harder to scale. The common technique is **sharding** (or partitioning), where the dataset is split across multiple nodes. While this distributes the load, the act of scaling—adding a new shard and rebalancing the data—is complex. It involves migrating large amounts of data, which consumes network bandwidth and CPU, temporarily reducing the effective service capacity and potentially causing a transient degradation in performance. This distinction is critical: architects must plan for the significantly higher operational complexity and transient performance hits associated with scaling stateful persistence layers.

#### Availability, Reliability, and Durability

Finally, we must formally define and measure the system's robustness. These terms are often used interchangeably, but have precise meanings in [systems engineering](@entry_id:180583) .

*   **Availability** is the proportion of time that a service is operational and capable of fulfilling its function. It is often expressed as a percentage, such as "five nines" ($99.999\%$). In a system with a [failure rate](@entry_id:264373) $\lambda$ and a repair rate $\mu$, the steady-state availability is $A = \mu / (\lambda + \mu)$.

*   **Reliability** is the probability of a system operating continuously without failure over a specified interval of time. For a system with a constant [failure rate](@entry_id:264373) $\lambda$, the reliability over an interval $t$ is $R(t) = \exp(-\lambda t)$. A system can have high availability but low reliability if it fails often but recovers very quickly.

*   **Durability** refers to the persistence of data. It is the probability that stored data is not lost or corrupted over a specified time horizon. For a storage system that uses $n$ independent replicas, each with an annual data-loss probability of $p$, the probability of losing all $n$ replicas (and thus the data) is $p^n$, making the durability $1 - p^n$.

In modern practice, these qualities are managed using the Site Reliability Engineering (SRE) framework of **Service Level Indicators (SLIs)**, **Service Level Objectives (SLOs)**, and **Error Budgets**.
*   An **SLI** is a direct, quantitative measure of a service's performance (e.g., the fraction of successful HTTP requests).
*   An **SLO** is the target value for that SLI over a period (e.g., "the 28-day request success SLI will be $\ge 99.9\%"$).
*   The **Error Budget** is the complement of the SLO ($1 - \text{SLO}$), representing the quantity of "badness" (e.g., downtime minutes, failed requests) that is permissible within the measurement window. The error budget is a powerful tool for data-driven decision making: if the budget is being consumed too quickly, engineering must focus on reliability; if there is ample budget remaining, the team is empowered to take risks, such as shipping new features more aggressively. This framework transforms abstract goals like "high availability" into concrete, measurable targets that guide engineering priorities.