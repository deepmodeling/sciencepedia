{
    "hands_on_practices": [
        {
            "introduction": "For a Digital Twin to effectively interact with its physical counterpart, especially in real-time control scenarios, its end-to-end latency is a critical performance characteristic. This exercise  provides a practical method for calculating a latency budget across a layered reference architecture, accounting for sequential processing, parallel execution, and batching delays. By connecting this total latency to the fundamental stability limits of a feedback control loop, you will gain insight into how architectural design directly constrains the twin's ability to influence the physical world.",
            "id": "4215319",
            "problem": "In the context of a layered Digital Twin reference architecture consistent with the Industrial Internet Reference Architecture (IIRA) and the Reference Architecture Model for Industrie 4.0 (RAMI 4.0), consider an end-to-end closed-loop path that spans the following layers: physical device sensing, communication uplink, edge pre-processing, platform ingestion, analytics, fusion, application decision logic, communication downlink, and actuation. The analytics layer comprises two branches that execute concurrently: a physics-based model and a data-driven model accelerated by a Graphics Processing Unit (GPU). The platform layer employs an event-stream micro-batcher that releases frames at fixed intervals. Assume that each layer exhibits deterministic service time and that the event stream is always backlogged such that the worst-case waiting time due to micro-batching must be included in the end-to-end latency budget. The fusion stage waits for both analytics branches to complete before proceeding.\n\nThe following per-layer deterministic delays are given, expressed as service or enforced waiting times in milliseconds: device sensing delay $t_{\\text{sense}} = 2$, uplink communication delay $t_{\\text{up}} = 5$, edge pre-processing delay $t_{\\text{edge}} = 3$, platform micro-batch interval (worst-case waiting time) $T_{\\text{batch}} = 10$, platform ingestion delay $t_{\\text{plat}} = 4$, physics-based analytics branch delay $t_{\\text{phys}} = 7$, data-driven analytics branch delay $t_{\\text{ml}} = 11$, fusion delay $t_{\\text{fuse}} = 2$, application decision logic delay $t_{\\text{app}} = 1.5$, downlink communication delay $t_{\\text{down}} = 5$, and actuation delay $t_{\\text{act}} = 2.5$.\n\nStarting from core definitions in real-time systems and feedback control, treat the end-to-end latency budget $L$ as the worst-case time from sensing to actuation along the critical path, accounting for pipeline parallelism by modeling the analytics stage as a fork-join where its effective service time equals the maximum of the branch service times. Then, derive how the computed $L$ constrains the achievable closed-loop bandwidth when the twin is used for real-time feedback. Specifically, for a required phase margin $\\phi_{m} = \\pi/4$ and a loop where the dominant additional phase lag is the transport-delay induced by $L$, determine the maximal crossover angular frequency $\\omega_{c}^{\\max}$ in radians per second that is conservatively admissible such that the delay does not consume the entire phase margin.\n\nCompute the worst-case end-to-end latency $L$ in milliseconds, and the maximal admissible crossover angular frequency $\\omega_{c}^{\\max}$ in radians per second. Express $L$ in milliseconds. Express $\\omega_{c}^{\\max}$ in radians per second and round your answer for $\\omega_{c}^{\\max}$ to four significant figures. Provide your final answer as a two-entry row, with the first entry being $L$ (in milliseconds) and the second entry being $\\omega_{c}^{\\max}$ (in radians per second).",
            "solution": "The user wants to solve a two-part problem related to the real-time performance of a Digital Twin.\n\n**Part 1: Calculation of Worst-Case End-to-End Latency ($L$)**\n\nThe problem defines the end-to-end closed-loop path as a series of stages with deterministic delays. The total latency, $L$, is the worst-case time from sensing to actuation. This corresponds to the sum of delays along the critical path. The architecture specifies that the analytics layer consists of two parallel branches, and the subsequent fusion stage must wait for both to complete. This is a classic \"fork-join\" parallelism model. The effective delay of such a parallel stage is determined by the longest-running branch (the critical path).\n\nThe delays of the two analytics branches are given as:\n- Physics-based analytics branch delay: $t_{\\text{phys}} = 7$ ms\n- Data-driven analytics branch delay: $t_{\\text{ml}} = 11$ ms\n\nThe effective service time of the analytics stage, denoted as $t_{\\text{analytics}}$, is the maximum of these two values:\n$$t_{\\text{analytics}} = \\max(t_{\\text{phys}}, t_{\\text{ml}}) = \\max(7, 11) = 11 \\text{ ms}$$\n\nThe total end-to-end latency $L$ is the sum of the delays of all stages in sequence. The critical path includes the longer of the two analytics branches. The stages are: device sensing ($t_{\\text{sense}}$), uplink communication ($t_{\\text{up}}$), edge pre-processing ($t_{\\text{edge}}$), platform micro-batch waiting time ($T_{\\text{batch}}$), platform ingestion ($t_{\\text{plat}}$), analytics ($t_{\\text{analytics}}$), fusion ($t_{\\text{fuse}}$), application decision logic ($t_{\\text{app}}$), downlink communication ($t_{\\text{down}}$), and actuation ($t_{\\text{act}}$).\n\nThe total latency $L$ is therefore:\n$$L = t_{\\text{sense}} + t_{\\text{up}} + t_{\\text{edge}} + T_{\\text{batch}} + t_{\\text{plat}} + t_{\\text{analytics}} + t_{\\text{fuse}} + t_{\\text{app}} + t_{\\text{down}} + t_{\\text{act}}$$\n\nSubstituting the given values in milliseconds:\n$$L = 2 + 5 + 3 + 10 + 4 + 11 + 2 + 1.5 + 5 + 2.5$$\n$$L = 46 \\text{ ms}$$\n\n**Part 2: Calculation of Maximal Admissible Crossover Angular Frequency ($\\omega_{c}^{\\max}$)**\n\nThe second part of the problem relates the total latency $L$ to the stability of a real-time feedback control loop. The latency $L$ is modeled as a pure transport delay. In the Laplace domain, a transport delay has the transfer function $G_d(s) = \\exp(-sL)$.\n\nTo analyze the effect on stability in the frequency domain, we substitute $s = j\\omega$, where $j$ is the imaginary unit and $\\omega$ is the angular frequency.\n$$G_d(j\\omega) = \\exp(-j\\omega L)$$\n\nAccording to Euler's formula, $\\exp(-j\\theta) = \\cos(\\theta) - j\\sin(\\theta)$, which has a magnitude of $1$ and a phase angle of $-\\theta$. Therefore, the transport delay introduces a frequency-dependent phase lag, $\\Delta\\phi(\\omega)$, given by:\n$$\\Delta\\phi(\\omega) = -\\omega L$$\n\nThe phase margin, $\\phi_m$, is a measure of a system's stability, defined at the gain crossover frequency, $\\omega_c$ (where the open-loop gain is unity). The problem requires a phase margin of at least $\\phi_m = \\pi/4$ radians. The problem states that the phase lag from the transport delay is the dominant additional lag and should not consume the entire phase margin. This establishes a conservative limit for stable operation. The phase lag induced by the delay at the crossover frequency, $|\\Delta\\phi(\\omega_c)| = \\omega_c L$, must be less than or equal to the required phase margin $\\phi_m$.\n$$\\omega_c L \\le \\phi_m$$\n\nTo find the maximal admissible crossover frequency, $\\omega_{c}^{\\max}$, we solve for the equality condition:\n$$\\omega_{c}^{\\max} L = \\phi_m$$\n$$\\omega_{c}^{\\max} = \\frac{\\phi_m}{L}$$\n\nWe must use consistent units. The latency $L$ was calculated in milliseconds, so it must be converted to seconds for this calculation. The phase margin $\\phi_m$ is already given in radians, and the resulting frequency $\\omega_c^{\\max}$ will be in radians per second.\n- $L = 46 \\text{ ms} = 46 \\times 10^{-3} \\text{ s}$\n- $\\phi_m = \\frac{\\pi}{4}$ radians\n\nSubstituting these values into the equation:\n$$\\omega_{c}^{\\max} = \\frac{\\frac{\\pi}{4}}{46 \\times 10^{-3}} = \\frac{\\pi}{4 \\times 46 \\times 10^{-3}} = \\frac{\\pi}{184 \\times 10^{-3}} = \\frac{1000\\pi}{184}$$\n$$\\omega_{c}^{\\max} = \\frac{125\\pi}{23} \\text{ rad/s}$$\n\nNow, we compute the numerical value and round to four significant figures as requested.\n$$\\omega_{c}^{\\max} \\approx \\frac{125 \\times 3.14159265}{23} \\approx 17.07387... \\text{ rad/s}$$\n\nRounding to four significant figures, we get:\n$$\\omega_{c}^{\\max} \\approx 17.07 \\text{ rad/s}$$\n\nThe two computed values are $L=46$ ms and $\\omega_{c}^{\\max} \\approx 17.07$ rad/s.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n46 & 17.07\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond performance, a Digital Twin's power lies in its ability to process and fuse information from disparate sources into a coherent whole. This requires a robust and unambiguous data architecture, a challenge effectively addressed by semantic technologies like ontologies. This practice  guides you through the formal evaluation of a DT's data model, assessing its maturity based on structural integrity, alignment with standard vocabularies, and the plausibility of its data, which are all crucial for achieving true interoperability.",
            "id": "4215334",
            "problem": "You are to construct and validate an ontology fragment for a digital twin of an industrial pump, formalized as a finite directed, labeled multigraph. The fragment must include classes, object properties, and datatype properties, and be aligned to recognized external vocabularies in order to evaluate semantic interoperability maturity. Use a model-theoretic approach grounded in Resource Description Framework (RDF) and Web Ontology Language (OWL) semantics. The ontology fragment must satisfy the following core specifications and constraints and be evaluated against a maturity model.\n\nFundamental base definitions:\n- Resource Description Framework (RDF) uses triples of the form $\\langle s, p, o \\rangle$ where $s$ is a subject, $p$ is a predicate, and $o$ is an object. A triple asserts a binary relation $p(s,o)$ between resources.\n- Web Ontology Language (OWL) and RDF Schema (RDFS) support axioms such as $rdfs:subClassOf$, $owl:equivalentClass$, $rdfs:domain$, and $rdfs:range$ for reasoning about classes and properties.\n- The ontology is a directed labeled multigraph $G=(V,E)$ where $V$ is the set of Internationalized Resource Identifiers (IRIs) and literals, and $E \\subseteq V \\times V \\times V$ is the set of triples. Classes and properties are elements of $V$; individuals are also elements of $V$; typed literals are represented by value-unit pairs.\n- Units must be normalized to International System of Units (SI). For pressure: Pascal (Pa). For volumetric flow rate: cubic meter per second ($\\mathrm{m}^3/\\mathrm{s}$). Conversion facts include $1\\,\\mathrm{bar} = 10^5\\,\\mathrm{Pa}$, $1\\,\\mathrm{kPa} = 10^3\\,\\mathrm{Pa}$, $1\\,\\mathrm{psi} \\approx 6894.757\\,\\mathrm{Pa}$, $1\\,\\mathrm{L} = 10^{-3}\\,\\mathrm{m}^3$, and $1\\,\\mathrm{L/min} = \\frac{10^{-3}}{60}\\,\\mathrm{m}^3/\\mathrm{s}$.\n\nOntology fragment requirements:\n- Classes: $Pump$, $Sensor$, $MaintenanceTask$ (all IRIs in the base namespace $http://example.org/twin\\#$).\n- Object properties: $hasSensor$, $scheduledFor$.\n- Datatype properties: $pressure$, $flowRate$.\n- Domain and range constraints:\n  - $hasSensor$ has domain $Pump$ and range $Sensor$.\n  - $scheduledFor$ has domain $Pump$ and range $MaintenanceTask$.\n  - $pressure$ and $flowRate$ attach to $Sensor$ and take real-valued measurements with explicit units.\n- Alignment to existing vocabularies:\n  - Align $Pump$ to Brick Schema (Brick) $brick:Pump$ and declare it a subclass of Sensor, Observation, Sample, and Actuator (SOSA) $sosa:Platform$.\n  - Align $Sensor$ to Sensor, Observation, Sample, and Actuator (SOSA) $sosa:Sensor$.\n  - Align $MaintenanceTask$ to W3C Provenance Ontology (PROV-O) $prov:Activity$.\n  - Align $hasSensor$ to SOSA $sosa:hosts$.\n  - Align $scheduledFor$ as an $rdfs:subPropertyOf$ $prov:wasInfluencedBy$.\n  - Align $pressure$ and $flowRate$ as $rdfs:subPropertyOf$ Smart Appliances REFerence (SAREF) $saref:hasValue$.\n- Instances: At least one instance $p_1$ of $Pump$, one instance $s_1$ of $Sensor$, and one instance $m_1$ of $MaintenanceTask$ must exist, with triples asserting $(p_1,hasSensor,s_1)$ and $(p_1,scheduledFor,m_1)$.\n- Measurement normalization and plausibility: A sensor $s_1$ must have $pressure$ and $flowRate$ values that can be normalized to SI units. After normalization, the pressure must lie in the interval $[10^3,5\\times 10^6]\\,\\mathrm{Pa}$ and the flow rate in $[10^{-4},2]\\,\\mathrm{m}^3/\\mathrm{s}$.\n\nMaturity levels:\n- Define maturity levels $L_0,L_1,L_2,L_3$ as follows:\n  - $L_0$: Structural constraints fail (no valid instances or required triples).\n  - $L_1$: Structural constraints pass.\n  - $L_2$: Structural constraints pass and all specified alignments are present.\n  - $L_3$: Structural constraints and alignments pass, and SI normalization succeeds with values within the given plausible ranges.\nA candidate ontology’s maturity level is the highest $L_k$ satisfied, where $k \\in \\{0,1,2,3\\}$.\n\nInput representation for testing:\n- Each test case is an embedded parameter set defining:\n  - A set of local IRIs for classes $\\{Pump,Sensor,MaintenanceTask\\}$, object properties $\\{hasSensor,scheduledFor\\}$, and datatype properties $\\{pressure,flowRate\\}$ under $http://example.org/twin\\#$.\n  - Individuals $p_1,s_1,m_1$ with $rdf:type$ assertions.\n  - Triples for $hasSensor$ and $scheduledFor$.\n  - Datatype assertions for $pressure$ and $flowRate$ as $(\\text{value},\\text{unit})$ pairs attached to $s_1$.\n  - Alignment assertions using $owl:equivalentClass$, $owl:equivalentProperty$, $rdfs:subClassOf$, and $rdfs:subPropertyOf$ to external IRIs in Brick, SOSA, PROV-O, and SAREF.\n\nTest suite:\n- Case $1$ (happy path):\n  - $pressure(s_1) = (3.2,\\text{\"bar\"})$, $flowRate(s_1) = (120,\\text{\"L/min\"})$.\n  - All alignments present: $Pump \\equiv brick:Pump$, $Pump \\sqsubseteq sosa:Platform$, $Sensor \\equiv sosa:Sensor$, $MaintenanceTask \\equiv prov:Activity$, $hasSensor \\equiv sosa:hosts$, $scheduledFor \\sqsubseteq prov:wasInfluencedBy$, $pressure \\sqsubseteq saref:hasValue$, $flowRate \\sqsubseteq saref:hasValue$.\n- Case $2$ (alignment boundary):\n  - $pressure(s_1) = (2.0,\\text{\"bar\"})$, $flowRate(s_1) = (2,\\text{\"L/s\"})$.\n  - Missing alignment for $scheduledFor$; all other alignments present.\n- Case $3$ (domain/range edge):\n  - $pressure(s_1) = (100,\\text{\"kPa\"})$, $flowRate(s_1) = (0.5,\\text{\"L/s\"})$.\n  - Triple $(s_1,hasSensor,p_1)$ instead of $(p_1,hasSensor,s_1)$; all alignments present.\n- Case $4$ (unit edge):\n  - $pressure(s_1) = (50,\\text{\"psi\"})$, $flowRate(s_1) = (1,\\text{\"unknown\\_unit\"})$.\n  - All alignments present.\n\nOutput specification:\n- Your program must construct each test case internally and evaluate the maturity level $L_k$ for each case as an integer $k \\in \\{0,1,2,3\\}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $\\text{\"[2,1,0,3]\"}$.\n\nNo external input is required. No angles are involved. All physical measurements must be normalized to SI units: pressure in Pascal (Pa) and flow rate in cubic meter per second ($\\mathrm{m}^3/\\mathrm{s}$). The program must adhere to logical consistency and the specified constraints to determine the correct maturity level for each test case.",
            "solution": "### Step 1: Extract Givens\nThe problem provides the following specifications for constructing and validating an ontology fragment for a digital twin of an industrial pump.\n\n**Formal Foundations:**\n- **Model:** A finite directed, labeled multigraph $G=(V,E)$ where $V$ is a set of IRIs and literals, and $E \\subseteq V \\times V \\times V$ is a set of RDF triples $\\langle s, p, o \\rangle$.\n- **Semantics:** Grounded in Resource Description Framework (RDF) and Web Ontology Language (OWL), utilizing axioms such as $rdfs:subClassOf$ ($\\sqsubseteq$), $owl:equivalentClass$ ($\\equiv$), $rdfs:domain$, and $rdfs:range$.\n- **Base Namespace:** $http://example.org/twin\\#$ for local resources.\n- **Unit Conversions:**\n  - Pressure: $1\\,\\mathrm{bar} = 10^5\\,\\mathrm{Pa}$; $1\\,\\mathrm{kPa} = 10^3\\,\\mathrm{Pa}$; $1\\,\\mathrm{psi} \\approx 6894.757\\,\\mathrm{Pa}$.\n  - Volumetric Flow Rate: $1\\,\\mathrm{L} = 10^{-3}\\,\\mathrm{m}^3$; $1\\,\\mathrm{L/min} = \\frac{10^{-3}}{60}\\,\\mathrm{m}^3/\\mathrm{s}$.\n  - SI Units: Pressure in Pascal ($\\mathrm{Pa}$), volumetric flow rate in cubic meters per second ($\\mathrm{m}^3/\\mathrm{s}$).\n\n**Ontology Fragment Requirements:**\n- **Classes:** $Pump$, $Sensor$, $MaintenanceTask$.\n- **Object Properties:** $hasSensor$, $scheduledFor$.\n- **Datatype Properties:** $pressure$, $flowRate$.\n- **Domain/Range Constraints:**\n  - $domain(hasSensor) = Pump$, $range(hasSensor) = Sensor$.\n  - $domain(scheduledFor) = Pump$, $range(scheduledFor) = MaintenanceTask$.\n  - $pressure$ and $flowRate$ properties attach to $Sensor$.\n- **Alignments to External Vocabularies:**\n  1. $Pump \\equiv brick:Pump$\n  2. $Pump \\sqsubseteq sosa:Platform$\n  3. $Sensor \\equiv sosa:Sensor$\n  4. $MaintenanceTask \\equiv prov:Activity$\n  5. $hasSensor \\equiv sosa:hosts$\n  6. $scheduledFor \\sqsubseteq prov:wasInfluencedBy$\n  7. $pressure \\sqsubseteq saref:hasValue$\n  8. $flowRate \\sqsubseteq saref:hasValue$\n- **Instance and Structural Requirements:**\n  - One instance $p_1$ of type $Pump$.\n  - One instance $s_1$ of type $Sensor$.\n  - One instance $m_1$ of type $MaintenanceTask$.\n  - Required triples: $(p_1, hasSensor, s_1)$ and $(p_1, scheduledFor, m_1)$.\n- **Measurement Plausibility Constraints:**\n  - Normalized pressure must be in $[10^3, 5 \\times 10^6]\\,\\mathrm{Pa}$.\n  - Normalized flow rate must be in $[10^{-4}, 2]\\,\\mathrm{m}^3/\\mathrm{s}$.\n\n**Maturity Model:**\n- $L_0$: Structural constraints fail.\n- $L_1$: Structural constraints pass.\n- $L_2$: Structural constraints pass and all specified alignments are present.\n- $L_3$: Structural constraints, alignments pass, and SI normalization succeeds with values within plausible ranges.\n- The assigned level is the highest $L_k$ for which all conditions are met.\n\n**Test Suite:**\n- **Case 1:** All structural and alignment constraints are met. Measurements: $pressure(s_1) = (3.2, \\text{\"bar\"})$, $flowRate(s_1) = (120, \\text{\"L/min\"})$.\n- **Case 2:** A single alignment ($scheduledFor$) is missing. Measurements: $pressure(s_1) = (2.0, \\text{\"bar\"})$, $flowRate(s_1) = (2, \\text{\"L/s\"})$.\n- **Case 3:** A structural constraint is violated: the triple $(s_1, hasSensor, p_1)$ exists instead of $(p_1, hasSensor, s_1)$. Measurements: $pressure(s_1) = (100, \\text{\"kPa\"})$, $flowRate(s_1) = (0.5, \\text{\"L/s\"})$.\n- **Case 4:** A measurement has an unrecognized unit. Measurements: $pressure(s_1) = (50, \\text{\"psi\"})$, $flowRate(s_1) = (1, \\text{\"unknown\\_unit\"})$.\n\n**Output Specification:**\n- A single line of output with a comma-separated list of integer maturity levels $[k_1, k_2, k_3, k_4]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the established criteria.\n\n- **Scientifically Grounded:** The problem is firmly located within computer science (semantic web technologies, ontologies) and engineering (digital twins, industrial systems). The use of RDF, OWL, and specific vocabularies (Brick, SOSA, PROV-O, SAREF) is standard practice. The physical units, conversions, and plausible value ranges for an industrial pump are realistic and factually correct.\n- **Well-Posed:** The problem provides a clear, hierarchical set of rules for evaluation (maturity levels $L_0, L_1, L_2, L_3$). The inputs for each test case are explicitly defined, and the desired output format is unambiguous. A unique maturity level can be determined for each case by systematically applying the rules.\n- **Objective:** The evaluation criteria are objective and quantitative (e.g., presence of specific triples, number of alignments met, numerical ranges for data). There are no subjective or ambiguous terms.\n- **Flaw Checklist:**\n  1.  **Scientific/Factual Unsoundness:** None. The semantic web concepts and physical quantities are correctly represented.\n  2.  **Non-Formalizable/Irrelevant:** None. The problem is directly relevant to digital twin reference architectures and is highly formalizable.\n  3.  **Incomplete/Contradictory Setup:** None. All information required to evaluate each test case against the maturity model is provided.\n  4.  **Unrealistic/Infeasible:** None. The scenario is a simplified but plausible representation of an engineering application of semantic technologies.\n  5.  **Ill-Posed/Poorly Structured:** None. The hierarchical maturity model ensures a unique solution for each case.\n  6.  **Pseudo-Profound/Trivial:** None. The problem requires a multi-step logical evaluation combining structural, semantic, and data-level checks, which is a non-trivial task.\n  7.  **Outside Scientific Verifiability:** None. The evaluation is a deterministic process based on the given rules and data.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is self-contained, scientifically sound, and well-posed. A solution will be provided.\n\n### Principle-Based Design and Solution\n\nThe evaluation of each test case's maturity level will proceed through a hierarchical sequence of checks, corresponding to the levels $L_1$, $L_2$, and $L_3$. The final level assigned is the highest level for which all criteria are satisfied. If the foundational criteria for $L_1$ are not met, the level is determined to be $L_0$.\n\n**Methodology for Maturity Evaluation**\n\n1.  **Level $L_1$ (Structural Integrity):**\n    - Verify the existence of required instances: an individual of type $Pump$, one of type $Sensor$, and one of type $MaintenanceTask$.\n    - Verify the existence of the triple $(p_1, hasSensor, s_1)$, where $p_1$ is the pump and $s_1$ is the sensor.\n    - Verify the existence of the triple $(p_1, scheduledFor, m_1)$, where $m_1$ is the maintenance task.\n    - Verify adherence to domain and range constraints for all properties. For an object property predicate $p$ in a triple $(s, p, o)$, the type of $s$ must match $domain(p)$ and the type of $o$ must match $range(p)$.\n    - If any of these checks fail, the maturity is $L_0$. Otherwise, the ontology meets the $L_1$ criteria, and we proceed to the next level.\n\n2.  **Level $L_2$ (Semantic Alignment):**\n    - Assuming $L_1$ criteria are met, verify the presence of all $8$ specified alignment axioms to external vocabularies (Brick, SOSA, PROV-O, SAREF). These include $owl:equivalentClass$, $rdfs:subClassOf$, $owl:equivalentProperty$, and $rdfs:subPropertyOf$ assertions.\n    - If all $8$ alignments are present, the ontology meets the $L_2$ criteria. Otherwise, the maturity level is $L_1$.\n\n3.  **Level $L_3$ (Data Normalization and Plausibility):**\n    - Assuming $L_2$ criteria are met, evaluate the sensor measurement data.\n    - For each datatype property ($pressure$, $flowRate$), attempt to normalize the value to its SI unit. This requires the provided unit to be recognized.\n    - **Pressure:** The normalized value must be within the interval $[10^3, 5 \\times 10^6]\\,\\mathrm{Pa}$.\n    - **Flow Rate:** The normalized value must be within the interval $[10^{-4}, 2]\\,\\mathrm{m}^3/\\mathrm{s}$.\n    - If normalization is possible for all measurements and all normalized values fall within their specified plausible ranges, the ontology meets the $L_3$ criteria. Otherwise, the maturity level is $L_2$.\n\n**Unit Conversion Factors:**\nThe analysis uses the following conversion factors to transform given units into SI units:\n- **Pressure to Pascal ($\\mathrm{Pa}$):**\n  - from $\\mathrm{bar}$: multiply by $10^5$\n  - from $\\mathrm{kPa}$: multiply by $10^3$\n  - from $\\mathrm{psi}$: multiply by $6894.757$\n- **Flow Rate to $\\mathrm{m}^3/\\mathrm{s}$:**\n  - from $\\mathrm{L/min}$: multiply by $\\frac{10^{-3}}{60} \\approx 1.6667 \\times 10^{-5}$\n  - from $\\mathrm{L/s}$: multiply by $10^{-3}$\n\n**Case-by-Case Analysis**\n\n**Case 1: Happy Path**\n- **$L_1$ Check:** The problem states all structural constraints are met. **Pass.**\n- **$L_2$ Check:** The problem states all $8$ alignments are present. **Pass.**\n- **$L_3$ Check:**\n  - Pressure: $3.2\\,\\mathrm{bar} \\times 10^5\\,\\mathrm{Pa}/\\mathrm{bar} = 320,000\\,\\mathrm{Pa}$. This value is in the range $[10^3, 5 \\times 10^6]$. **Pass.**\n  - Flow Rate: $120\\,\\mathrm{L/min} \\times \\frac{10^{-3}}{60}\\,\\mathrm{m^3/s}/\\mathrm{L/min} = 0.002\\,\\mathrm{m^3/s}$. This value is in the range $[10^{-4}, 2]$. **Pass.**\n- **Conclusion for Case 1:** All checks up to $L_3$ pass. The maturity level is **$3$**.\n\n**Case 2: Alignment Boundary**\n- **$L_1$ Check:** Structural constraints are met. **Pass.**\n- **$L_2$ Check:** The problem states the alignment for $scheduledFor$ is missing. Since not all $8$ alignments are present, this check fails.\n- **Conclusion for Case 2:** The ontology passes $L_1$ but fails $L_2$. The maturity level is **$1$**.\n\n**Case 3: Domain/Range Edge**\n- **$L_1$ Check:** The problem specifies the triple $(s_1, hasSensor, p_1)$. The property $hasSensor$ has a defined domain of $Pump$ and a range of $Sensor$. The subject of this triple, $s_1$, is of type $Sensor$, which violates the domain constraint. This constitutes a structural failure. **Fail.**\n- **Conclusion for Case 3:** The structural integrity check fails. The maturity level is **$0$**.\n\n**Case 4: Unit Edge**\n- **$L_1$ Check:** Structural constraints are met. **Pass.**\n- **$L_2$ Check:** The problem states all $8$ alignments are present. **Pass.**\n- **$L_3$ Check:**\n  - Pressure: $50\\,\\mathrm{psi} \\times 6894.757\\,\\mathrm{Pa}/\\mathrm{psi} \\approx 344,737.85\\,\\mathrm{Pa}$. This value is in the range $[10^3, 5 \\times 10^6]$. **Pass.**\n  - Flow Rate: The unit is specified as \"unknown\\_unit\". Normalization to the SI unit $\\mathrm{m}^3/\\mathrm{s}$ is not possible. This check fails.\n- **Conclusion for Case 4:** The ontology passes $L_2$ but fails $L_3$. The maturity level is **$2$**.\n\n**Summary of Results**\n- Case 1: Level $3$\n- Case 2: Level $1$\n- Case 3: Level $0$\n- Case 4: Level $2$\n\nThe final output is the sequence of these integer levels.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef evaluate_maturity(case):\n    \"\"\"\n    Evaluates the maturity level of a digital twin ontology fragment based on a test case.\n    Maturity Levels:\n    L0: Structural failure.\n    L1: Structural constraints pass.\n    L2: L1 + all semantic alignments are present.\n    L3: L2 + all measurement data is normalizable and plausible.\n    \"\"\"\n    \n    # Level 1 Check: Structural Integrity\n    # This checks for correct instance typing, required triples, and domain/range validity.\n    if not case[\"structural_integrity\"]:\n        return 0\n    \n    # If structural integrity holds, maturity is at least L1.\n    level = 1\n\n    # Level 2 Check: Semantic Alignment\n    # This checks if all required alignment axioms to external ontologies are present.\n    if not all(case[\"alignments\"].values()):\n        return level  # Fails L2, so maturity remains L1.\n    \n    # If all alignments are present, maturity is at least L2.\n    level = 2\n\n    # Level 3 Check: Data Normalization and Plausibility\n    # This checks if sensor data can be converted to SI units and falls within plausible ranges.\n    \n    pressure_conversions = {\n        \"bar\": 1e5,\n        \"kPa\": 1e3,\n        \"psi\": 6894.757\n    }\n    flow_rate_conversions = {\n        \"L/min\": 1e-3 / 60.0,\n        \"L/s\": 1e-3\n    }\n    \n    # Check pressure\n    p_val, p_unit = case[\"measurements\"][\"pressure\"]\n    if p_unit not in pressure_conversions:\n        return level # Fails L3 due to unknown unit, maturity is L2.\n    \n    p_si = p_val * pressure_conversions[p_unit]\n    if not (1e3 <= p_si <= 5e6):\n        return level # Fails L3 due to out-of-range value, maturity is L2.\n\n    # Check flow rate\n    f_val, f_unit = case[\"measurements\"][\"flow_rate\"]\n    if f_unit not in flow_rate_conversions:\n        return level # Fails L3 due to unknown unit, maturity is L2.\n        \n    f_si = f_val * flow_rate_conversions[f_unit]\n    if not (1e-4 <= f_si <= 2.0):\n        return level # Fails L3 due to out-of-range value, maturity is L2.\n    \n    # If all data checks pass, maturity is L3.\n    level = 3\n    return level\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates the test cases for the ontology maturity model.\n    \"\"\"\n    # Each test case is represented as a dictionary encoding its properties.\n    # The 'alignments' dict represents the 8 required alignment axioms.\n    test_cases = [\n        # Case 1: Happy path\n        {\n            \"structural_integrity\": True,\n            \"alignments\": {\n                \"Pump_equiv_brick_Pump\": True, \"Pump_subclass_sosa_Platform\": True,\n                \"Sensor_equiv_sosa_Sensor\": True, \"MaintenanceTask_equiv_prov_Activity\": True,\n                \"hasSensor_equiv_sosa_hosts\": True, \"scheduledFor_subprop_prov_wasInfluencedBy\": True,\n                \"pressure_subprop_saref_hasValue\": True, \"flowRate_subprop_saref_hasValue\": True\n            },\n            \"measurements\": {\"pressure\": (3.2, \"bar\"), \"flow_rate\": (120, \"L/min\")}\n        },\n        # Case 2: Alignment boundary\n        {\n            \"structural_integrity\": True,\n            \"alignments\": {\n                \"Pump_equiv_brick_Pump\": True, \"Pump_subclass_sosa_Platform\": True,\n                \"Sensor_equiv_sosa_Sensor\": True, \"MaintenanceTask_equiv_prov_Activity\": True,\n                \"hasSensor_equiv_sosa_hosts\": True, \"scheduledFor_subprop_prov_wasInfluencedBy\": False, # Missing alignment\n                \"pressure_subprop_saref_hasValue\": True, \"flowRate_subprop_saref_hasValue\": True\n            },\n            \"measurements\": {\"pressure\": (2.0, \"bar\"), \"flow_rate\": (2, \"L/s\")}\n        },\n        # Case 3: Domain/range edge\n        {\n            \"structural_integrity\": False, # Violated by the (s1, hasSensor, p1) triple\n            \"alignments\": {\n                \"Pump_equiv_brick_Pump\": True, \"Pump_subclass_sosa_Platform\": True,\n                \"Sensor_equiv_sosa_Sensor\": True, \"MaintenanceTask_equiv_prov_Activity\": True,\n                \"hasSensor_equiv_sosa_hosts\": True, \"scheduledFor_subprop_prov_wasInfluencedBy\": True,\n                \"pressure_subprop_saref_hasValue\": True, \"flowRate_subprop_saref_hasValue\": True\n            },\n            \"measurements\": {\"pressure\": (100, \"kPa\"), \"flow_rate\": (0.5, \"L/s\")}\n        },\n        # Case 4: Unit edge\n        {\n            \"structural_integrity\": True,\n            \"alignments\": {\n                \"Pump_equiv_brick_Pump\": True, \"Pump_subclass_sosa_Platform\": True,\n                \"Sensor_equiv_sosa_Sensor\": True, \"MaintenanceTask_equiv_prov_Activity\": True,\n                \"hasSensor_equiv_sosa_hosts\": True, \"scheduledFor_subprop_prov_wasInfluencedBy\": True,\n                \"pressure_subprop_saref_hasValue\": True, \"flowRate_subprop_saref_hasValue\": True\n            },\n            \"measurements\": {\"pressure\": (50, \"psi\"), \"flow_rate\": (1, \"unknown_unit\")}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = evaluate_maturity(case)\n        results.append(result)\n\n    # The final print statement must follow the exact specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Developing a Digital Twin is an evolutionary process, and tracking its progress requires a holistic assessment framework. A maturity model allows stakeholders to quantitatively evaluate the DT's capabilities across diverse and often heterogeneous dimensions, from model fidelity to security compliance. In this final exercise , you will construct a weighted scoring rubric, a powerful tool for synthesizing various performance metrics into a single, comparable maturity score, thereby enabling informed decision-making and strategic planning.",
            "id": "4215324",
            "problem": "A systems engineering team is evaluating the maturity of a Digital Twin (DT) program for a complex Cyber-Physical System (CPS) against a reference architecture that partitions capability into eight dimensions. To construct a quantitative maturity model, the team must use an additive, weighted rubric that is scientifically defensible from the standpoint of multi-attribute measurement. The team collects raw architectural metrics for each dimension, each with physically meaningful units or bounded scores, and will convert these raw metrics into dimensionless, normalized scores before aggregation. The evaluation context and data are as follows.\n\nDimensions and raw metrics:\n- Data interoperability coverage: fraction of canonical model mappings implemented, raw value $x_{1}=0.78$, bounded on $[0,1]$, larger is better.\n- Model fidelity: root-mean-square error (RMSE) of state estimates in millimeters, raw value $x_{2}=3.2$ with target range $[0,10]$ millimeters, where smaller is better.\n- Synchronization latency: median DT-to-physical synchronization latency in milliseconds, raw value $x_{3}=55$ with target range $[0,200]$ milliseconds, where smaller is better.\n- Security and governance: compliance score on a $0$ to $5$ audit scale, raw value $x_{4}=4.0$, larger is better.\n- Lifecycle integration: number of supported lifecycle stages out of $8$ defined stages, raw value $x_{5}=6$, larger is better; the range is $[0,8]$ stages.\n- Analytics and decision support: predictive model quality measured as an $F_1$ score on $[0,1]$, raw value $x_{6}=0.67$, larger is better.\n- Scalability: throughput ratio at $10\\times$ load relative to baseline, raw value $x_{7}=7.5$, bounded on $[0,10]$, larger is better.\n- Standards compliance coverage: ratio of applicable standards requirements satisfied, raw value $x_{8}=0.62$, bounded on $[0,1]$, larger is better.\n\nWeights for the rubric are assigned by architecture-criticality and sum to one:\n$$\nw_{1}=0.18,\\quad\nw_{2}=0.14,\\quad\nw_{3}=0.12,\\quad\nw_{4}=0.16,\\quad\nw_{5}=0.12,\\quad\nw_{6}=0.10,\\quad\nw_{7}=0.10,\\quad\nw_{8}=0.08.\n$$\n\nTasks:\n1. Using a scientifically grounded normalization that produces dimensionless scores $s_{i}\\in[0,1]$ and preserves the monotonicity of each metric with respect to maturity (that is, larger raw values should map to larger $s_{i}$ when larger is better, and larger raw values should map to smaller $s_{i}$ when smaller is better), construct $s_{i}$ for all eight dimensions from the given $x_{i}$ and their ranges.\n2. Aggregate the normalized scores using a principled weighted additive model consistent with the requirements of multi-attribute utility for commensurate scales to produce a single overall maturity score $M$.\n3. Briefly explain, from first principles of measurement, how normalization affects comparability across heterogeneous units and scales in this maturity assessment, and why the chosen normalization is appropriate in this context.\n\nRound the final overall maturity score to four significant figures. Express the final answer as a unitless decimal without a percentage sign.",
            "solution": "The problem is deemed valid as it is scientifically grounded in multi-attribute measurement theory, well-posed with all necessary data provided, and objective in its formulation. All data and constraints are self-contained and consistent.\n\nThe solution is divided into three parts as requested by the problem statement.\n\n### 1. Construction of Normalized Scores\n\nTo create a scientifically defensible rubric, the raw metrics $x_i$, which possess heterogeneous units and scales, must be transformed into dimensionless scores $s_i$ on a commensurate scale. The most direct and objective method, given the problem's constraints, is linear min-max normalization. This method maps each raw value to a score on the interval $[0, 1]$, where $s_i = 0$ represents the worst possible outcome and $s_i = 1$ represents the best possible outcome.\n\nThere are two cases for this transformation, depending on whether a larger or smaller raw value is preferable.\n\nCase A: For \"larger is better\" metrics, the normalized score $s_i$ is calculated as:\n$$s_i = \\frac{x_i - x_{i, \\text{min}}}{x_{i, \\text{max}} - x_{i, \\text{min}}}$$\nwhere $x_i$ is the raw value, and $x_{i, \\text{min}}$ and $x_{i, \\text{max}}$ are the minimum and maximum values of the metric's defined range, respectively.\n\nCase B: For \"smaller is better\" metrics, the relationship is inverted to ensure $s_i=1$ corresponds to the best outcome (the minimum raw value). The normalized score $s_i$ is calculated as:\n$$s_i = \\frac{x_{i, \\text{max}} - x_i}{x_{i, \\text{max}} - x_{i, \\text{min}}}$$\n\nWe apply these formulae to each of the eight dimensions:\n\n- **Dimension 1 (Data interoperability):** $x_1 = 0.78$, range $[0, 1]$, larger is better.\n$$s_1 = \\frac{0.78 - 0}{1 - 0} = 0.78$$\n\n- **Dimension 2 (Model fidelity):** $x_2 = 3.2$ mm, range $[0, 10]$ mm, smaller is better.\n$$s_2 = \\frac{10 - 3.2}{10 - 0} = \\frac{6.8}{10} = 0.68$$\n\n- **Dimension 3 (Synchronization latency):** $x_3 = 55$ ms, range $[0, 200]$ ms, smaller is better.\n$$s_3 = \\frac{200 - 55}{200 - 0} = \\frac{145}{200} = 0.725$$\n\n- **Dimension 4 (Security and governance):** $x_4 = 4.0$, range $[0, 5]$, larger is better.\n$$s_4 = \\frac{4.0 - 0}{5.0 - 0} = 0.8$$\n\n- **Dimension 5 (Lifecycle integration):** $x_5 = 6$, range $[0, 8]$, larger is better.\n$$s_5 = \\frac{6 - 0}{8 - 0} = 0.75$$\n\n- **Dimension 6 (Analytics and decision support):** $x_6 = 0.67$, range $[0, 1]$, larger is better.\n$$s_6 = \\frac{0.67 - 0}{1 - 0} = 0.67$$\n\n- **Dimension 7 (Scalability):** $x_7 = 7.5$, range $[0, 10]$, larger is better.\n$$s_7 = \\frac{7.5 - 0}{10 - 0} = 0.75$$\n\n- **Dimension 8 (Standards compliance):** $x_8 = 0.62$, range $[0, 1]$, larger is better.\n$$s_8 = \\frac{0.62 - 0}{1 - 0} = 0.62$$\n\n### 2. Aggregation into a Single Maturity Score\n\nThe problem specifies a principled weighted additive model, which is a standard method in Multi-Attribute Utility Theory (MAUT). The overall maturity score, $M$, is the weighted sum of the individual normalized scores $s_i$. The weights $w_i$ represent the relative importance of each dimension, and their sum is $\\sum_{i=1}^{8} w_i = 1$.\n\nThe formula for the overall maturity score $M$ is:\n$$M = \\sum_{i=1}^{8} w_i s_i$$\n\nSubstituting the given weights and the calculated normalized scores:\n$$M = w_1 s_1 + w_2 s_2 + w_3 s_3 + w_4 s_4 + w_5 s_5 + w_6 s_6 + w_7 s_7 + w_8 s_8$$\n$$M = (0.18)(0.78) + (0.14)(0.68) + (0.12)(0.725) + (0.16)(0.8) + (0.12)(0.75) + (0.10)(0.67) + (0.10)(0.75) + (0.08)(0.62)$$\n$$M = 0.1404 + 0.0952 + 0.0870 + 0.1280 + 0.0900 + 0.0670 + 0.0750 + 0.0496$$\n$$M = 0.7322$$\n\nThe problem requires the final score $M$ to be rounded to four significant figures. The calculated value $0.7322$ already has four significant figures.\n\n### 3. Explanation of Normalization in Measurement\n\nFrom the first principles of measurement, aggregating quantities requires them to be dimensionally homogeneous. The raw architectural metrics ($x_i$) are dimensionally heterogeneous, possessing different units (e.g., millimeters, milliseconds, audit points) and scales (e.g., $[0, 10]$ vs. $[0, 200]$). Directly summing these quantities, even with weighting, is mathematically and physically meaningless—it is akin to adding length and time. The result would be numerically dominated by metrics with a large range magnitude (e.g., latency) regardless of their assigned importance, violating the principle of a weighted rubric.\n\nNormalization addresses this fundamental problem by transforming each raw measurement into a dimensionless value on a common, commensurate scale, here $[0, 1]$. This new scale represents the degree of achievement or \"utility\" for each dimension, where $0$ is the worst performance and $1$ is the ideal outcome. By making the scales commensurate, the weighted sum becomes a valid mathematical operation. The weights $w_i$ can now be correctly interpreted as the relative importance of achieving maximal utility in each dimension.\n\nThe chosen linear min-max normalization is appropriate in this context for several reasons. First, it is a simple, transparent, and deterministic transformation. Second, it correctly preserves the monotonicity of each metric with respect to maturity, handling both \"larger is better\" and \"smaller is better\" cases. Third, it assumes a linear relationship between the raw metric and its utility. In the absence of evidence for non-linear preferences (such as diminishing returns), assuming linearity is the most parsimonious and objective approach, consistent with the requirements for a scientifically defensible model. This normalization step is the essential precondition that makes the additive aggregation of scores a valid and meaningful procedure.",
            "answer": "$$ \\boxed{0.7322} $$"
        }
    ]
}