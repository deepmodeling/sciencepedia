{
    "hands_on_practices": [
        {
            "introduction": "In the verification and validation of cyber-physical systems, it is often not enough to know *if* a safety requirement is met; we also need to know *by how much*. This practice introduces the concept of quantitative semantics through Signal Temporal Logic (STL), a formal language for specifying properties of real-valued signals over time. By calculating the robustness of an STL formula, you will learn to move beyond a simple true/false verdict and quantify the margin of safety or the severity of a violation, which is crucial for system analysis and design .",
            "id": "4253595",
            "problem": "A digital twin of an autonomous ground vehicle must verify the safety requirement that its longitudinal speed remains below a threshold during a fixed horizon of interest. Consider the Signal Temporal Logic (STL) formula $\\varphi = \\mathbf{G}_{[0,10]}(v < 30)$, where $v(t)$ is the vehicle speed signal in meters per second, and $\\mathbf{G}$ denotes the globally (always) operator over the time interval $[0,10]$.\n\nTask:\n1) Starting from the core definitions of Boolean and quantitative (robust) semantics of Signal Temporal Logic (STL), specify the formal Boolean semantics and the quantitative robustness semantics of the formula $\\varphi$ evaluated at time $t=0$ for a continuous-time signal $v : \\mathbb{R}_{\\ge 0} \\to \\mathbb{R}$.\n\n2) You are given a measured time series from the digital twin as timestamped samples $\\{(t_i, v_i)\\}_{i=0}^{6}$ with $t_0 = 0$, $t_6 = 10$, and strictly increasing times:\n$(t_0, v_0) = (0, 28.2)$,\n$(t_1, v_1) = (1.7, 29.6)$,\n$(t_2, v_2) = (3.1, 30.9)$,\n$(t_3, v_3) = (5.0, 31.2)$,\n$(t_4, v_4) = (6.8, 29.7)$,\n$(t_5, v_5) = (8.9, 29.9)$,\n$(t_6, v_6) = (10.0, 27.5)$.\nAssume the continuous-time signal $v(t)$ over $[0,10]$ is obtained by continuous piecewise-linear interpolation between successive samples. Using your semantics from part $1$, compute the quantitative robustness value $\\rho_{\\varphi}(v,0)$.\n\nExpress the final robustness as a single real number in meters per second. No rounding is required. If the value is negative, include the negative sign. Do not include units inside the final boxed answer.",
            "solution": "The problem is well-posed and scientifically grounded in the formal methods of Signal Temporal Logic (STL) as applied to the verification of cyber-physical systems. All necessary data and definitions are provided. We may proceed with the solution.\n\nThe problem asks for two tasks. First, to define the Boolean and quantitative semantics for the given STL formula. Second, to compute the quantitative robustness value for a given piecewise-linear signal.\n\n**Part 1: Formal Semantics of the STL Formula**\n\nThe given STL formula is $\\varphi = \\mathbf{G}_{[0,10]}(v < 30)$. Let the continuous-time signal be $v: \\mathbb{R}_{\\ge 0} \\to \\mathbb{R}$. The formula is evaluated at time $t=0$. The core of the formula $\\varphi$ is the atomic predicate $\\mu := (v < 30)$. This predicate is a statement about the value of the signal $v$ at a single point in time.\n\n**Boolean Semantics**\nThe Boolean semantics of an STL formula determine whether the formula is true (satisfied) or false (violated) for a given signal at a given time.\n\n1.  **Atomic Predicate:** The Boolean semantics of the atomic predicate $\\mu$ for a signal $v$ at time $t'$ are defined as:\n    $$ (v, t') \\models \\mu \\iff v(t') < 30 $$\n    This means the signal $v$ satisfies $\\mu$ at time $t'$ if and only if the value of $v$ at $t'$ is less than $30$.\n\n2.  **Globally Operator:** The formula $\\varphi$ uses the \"globally\" or \"always\" operator $\\mathbf{G}$ over a time interval $I = [0,10]$. The Boolean semantics for $\\varphi = \\mathbf{G}_{I} \\mu$ evaluated at time $t$ are:\n    $$ (v, t) \\models \\mathbf{G}_{I} \\mu \\iff \\forall t' \\in t+I, (v, t') \\models \\mu $$\n    where $t+I = \\{t+\\tau \\mid \\tau \\in I\\}$.\n\n    For our specific problem, we evaluate $\\varphi$ at $t=0$ over the interval $I=[0,10]$. Thus, $t+I = [0,10]$. The Boolean semantics are:\n    $$ (v, 0) \\models \\varphi \\iff \\forall t' \\in [0,10], (v, t') \\models (v < 30) $$\n    In words, the signal $v$ satisfies the formula $\\varphi$ at time $t=0$ if and only if the speed $v(t')$ is less than $30$ for all time points $t'$ in the interval $[0, 10]$.\n\n**Quantitative (Robustness) Semantics**\nThe quantitative semantics, also known as robustness, measure the degree to which a formula is satisfied or violated. A positive robustness value indicates satisfaction, a negative value indicates violation, and a value of $0$ indicates satisfaction at the boundary. The magnitude of the value quantifies the margin of satisfaction or violation.\n\n1.  **Atomic Predicate:** The standard quantitative semantics for an atomic predicate of the form $f(x) > c$ is $\\rho = f(x) - c$. Our predicate is $v < 30$, which can be rewritten as $30 - v > 0$. Therefore, the robustness of $\\mu$ for a signal $v$ at time $t'$ is:\n    $$ \\rho_{\\mu}(v, t') = 30 - v(t') $$\n    The units of this robustness value are the same as the units of $v$, which are meters per second.\n\n2.  **Globally Operator:** The robustness for the formula $\\varphi = \\mathbf{G}_{I} \\mu$ evaluated at time $t$ is defined as the infimum (greatest lower bound) of the robustness of its subformula $\\mu$ over the specified time interval:\n    $$ \\rho_{\\mathbf{G}_{I} \\mu}(v, t) = \\inf_{t' \\in t+I} \\rho_{\\mu}(v, t') $$\n\n    For our specific problem, evaluating $\\varphi$ at $t=0$ over the interval $I=[0,10]$ gives:\n    $$ \\rho_{\\varphi}(v, 0) = \\inf_{t' \\in [0,10]} \\rho_{\\mu}(v, t') = \\inf_{t' \\in [0,10]} (30 - v(t')) $$\n    Using the properties of the infimum, this can be expressed as:\n    $$ \\rho_{\\varphi}(v, 0) = 30 - \\sup_{t' \\in [0,10]} v(t') $$\n    This expression states that the robustness of the \"always\" property is determined by the \"weakest link\" in the signal trace. This corresponds to the point in time where the speed $v(t')$ is at its maximum, as this is where the constraint $v < 30$ is most likely to be violated.\n\n**Part 2: Computation of the Robustness Value**\n\nWe are tasked with computing $\\rho_{\\varphi}(v,0)$ for a signal $v(t)$ constructed by continuous piecewise-linear interpolation of the given sample points $\\{(t_i, v_i)\\}_{i=0}^{6}$:\n$(t_0, v_0) = (0, 28.2)$\n$(t_1, v_1) = (1.7, 29.6)$\n$(t_2, v_2) = (3.1, 30.9)$\n$(t_3, v_3) = (5.0, 31.2)$\n$(t_4, v_4) = (6.8, 29.7)$\n$(t_5, v_5) = (8.9, 29.9)$\n$(t_6, v_6) = (10.0, 27.5)$\n\nThe robustness is given by the formula derived in Part 1:\n$$ \\rho_{\\varphi}(v, 0) = 30 - \\sup_{t' \\in [0,10]} v(t') $$\n\nFor a continuous piecewise-linear function defined over a set of points, the supremum (or maximum value) of the function over the entire domain must occur at one of the given sample points (the \"knots\" of the spline). This is because on each linear segment between two points $(t_i, v_i)$ and $(t_{i+1}, v_{i+1})$, the function is monotonic. Therefore, the maximum or minimum on that segment must be at one of the endpoints, $v_i$ or $v_{i+1}$. Consequently, the global maximum over the entire interval $[0,10]$ must be one of the sample values $v_i$.\n\nWe need to find the maximum value among the set of speed measurements $\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}$.\nThe values are:\n$v_0 = 28.2$\n$v_1 = 29.6$\n$v_2 = 30.9$\n$v_3 = 31.2$\n$v_4 = 29.7$\n$v_5 = 29.9$\n$v_6 = 27.5$\n\nBy inspection, the maximum value is $v_3 = 31.2$.\nTherefore,\n$$ \\sup_{t' \\in [0,10]} v(t') = \\max_{i \\in \\{0, \\dots, 6\\}} \\{v_i\\} = v_3 = 31.2 $$\n\nNow, we substitute this value back into the robustness formula:\n$$ \\rho_{\\varphi}(v, 0) = 30 - 31.2 $$\n$$ \\rho_{\\varphi}(v, 0) = -1.2 $$\n\nThe robustness value is $-1.2$ meters per second. The negative sign indicates that the safety requirement $\\varphi$ is violated by the signal $v$. The magnitude $1.2$ indicates that at its worst point (at $t_3=5.0$), the speed exceeded the threshold of $30$ m/s by $1.2$ m/s.",
            "answer": "$$\n\\boxed{-1.2}\n$$"
        },
        {
            "introduction": "While monitoring a single system trajectory is useful, formal verification aims to provide guarantees for *all possible* behaviors. This exercise delves into the fundamentals of reachability analysis, a cornerstone of verifying continuous and hybrid systems. You will practice computing the set of all reachable states for a linear system by propagating an initial set through the system dynamics, using fundamental operations like linear maps and Minkowski sums to provide a provable bound on system performance .",
            "id": "4253596",
            "problem": "A linear time-invariant (LTI) cyber-physical system is modeled by the continuous-time dynamics $x'(t) = A x(t) + B u(t)$, where $x(t) \\in \\mathbb{R}^{2}$ is the state and $u(t) \\in \\mathbb{R}^{2}$ is an external input constrained by a known bounded set. In a digital twin–based verification and validation workflow, you are tasked with computing a tight bound on a safety-relevant linear performance index at a fixed time $T$ by performing set-based reachability using linear maps and Minkowski sums. The following data are given:\n- $A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}$,\n- $B = I_{2}$,\n- Initial set $X_{0} = \\{ x \\in \\mathbb{R}^{2} \\mid x_{1} \\in [0,2],\\ x_{2} \\in [-1,1] \\}$,\n- Input set $U = \\{ u \\in \\mathbb{R}^{2} \\mid u_{1} \\in [-1,1],\\ u_{2} \\in [-2,2] \\}$,\n- Evaluation time $T = \\ln(3)$,\n- Direction vector $c = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n\nStarting from first principles—namely, the variation-of-constants formula for linear systems, the definition of the Minkowski sum $S \\oplus K = \\{ s + k \\mid s \\in S,\\ k \\in K \\}$, and the definition of the support function $h_{S}(c) = \\sup_{x \\in S} c^{\\top} x$—derive the one-step reachability construction that maps $X_{0}$ and $U$ to the reachable set $R(T)$ at time $T$, expressed via linear images and Minkowski sums. Then, compute the exact value of the maximal performance index $J^{\\star} = \\sup_{x \\in R(T)} c^{\\top} x$.\n\nReport the final answer as a single real number. No rounding is required. No units are required.",
            "solution": "The user wants to find the maximum value of a linear performance index for a continuous-time linear system by using reachability analysis.\n\n### Step 1: Extract Givens\n- System dynamics: $x'(t) = A x(t) + B u(t)$, where $x(t) \\in \\mathbb{R}^{2}$ and $u(t) \\in \\mathbb{R}^{2}$.\n- System matrix: $A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}$.\n- Input matrix: $B = I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n- Initial set of states: $X_{0} = \\{ x \\in \\mathbb{R}^{2} \\mid x_{1} \\in [0,2],\\ x_{2} \\in [-1,1] \\}$.\n- Set of allowed inputs: $U = \\{ u \\in \\mathbb{R}^{2} \\mid u_{1} \\in [-1,1],\\ u_{2} \\in [-2,2] \\}$.\n- Time horizon: $T = \\ln(3)$.\n- Direction vector for performance index: $c = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n- Performance index to maximize: $J^{\\star} = \\sup_{x \\in R(T)} c^{\\top} x$, where $R(T)$ is the reachable set at time $T$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in standard control theory and the verification of cyber-physical systems. The formulation involves a linear time-invariant system, reachability analysis, Minkowski sums, and support functions, all of which are well-established mathematical concepts in this field. The problem is well-posed: all necessary data (matrices, sets, time horizon, and objective direction) are provided and are mutually consistent. The language is objective and precise. The sets $X_0$ and $U$ are compact and convex (hyperrectangles), and the dynamics are linear, which guarantees that the reachable set is also compact and convex, and thus a maximum for the linear functional exists. The matrix $A$ is stable (eigenvalues $-1$ and $-2$), which is physically meaningful. The problem is self-contained and solvable using the principles cited in the prompt.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe solution to the linear time-invariant system $x'(t) = A x(t) + B u(t)$ with an initial state $x(0) = x_0$ is given by the variation-of-constants formula:\n$$\nx(t) = e^{At} x_0 + \\int_0^t e^{A(t-\\tau)} B u(\\tau) d\\tau\n$$\nThe reachable set $R(T)$ at time $T$ is the set of all possible states $x(T)$ that can be reached starting from any $x_0 \\in X_0$ and applying any valid input function $u(\\tau) \\in U$ for $\\tau \\in [0, T]$. This can be expressed using set-based operations as the Minkowski sum of two sets:\n$$\nR(T) = e^{AT} X_0 \\oplus \\int_0^T e^{A(T-\\tau)} B U d\\tau\n$$\nThe first term, $e^{AT} X_0 = \\{ e^{AT} x_0 \\mid x_0 \\in X_0 \\}$, represents the evolution of the initial set. The second term, which we will denote $R_U(T) = \\int_0^T e^{A(T-\\tau)} B U d\\tau$, represents the set of states reachable from the origin due to the input.\n\nThe goal is to compute $J^{\\star} = \\sup_{x \\in R(T)} c^{\\top} x$, which is the definition of the support function of the set $R(T)$ in the direction $c$, denoted $h_{R(T)}(c)$.\n\nSupport functions have a useful property regarding Minkowski sums: $h_{S_1 \\oplus S_2}(l) = h_{S_1}(l) + h_{S_2}(l)$. Applying this property, we get:\n$$\nJ^{\\star} = h_{R(T)}(c) = h_{e^{AT} X_0}(c) + h_{R_U(T)}(c)\n$$\nWe compute each term separately.\n\n**Term 1: $h_{e^{AT} X_0}(c)$**\nFor a linear transformation $\\mathcal{L}$, the support function of the transformed set is $h_{\\mathcal{L}(S)}(l) = h_S(\\mathcal{L}^{\\top} l)$. Here, the linear map is multiplication by the matrix $e^{AT}$.\n$$\nh_{e^{AT} X_0}(c) = h_{X_0}((e^{AT})^{\\top} c)\n$$\nFirst, we compute the state transition matrix $e^{AT}$. Since $A$ is a diagonal matrix, $A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}$, its matrix exponential is:\n$$\ne^{At} = \\begin{pmatrix} e^{-t} & 0 \\\\ 0 & e^{-2t} \\end{pmatrix}\n$$\nAt time $T=\\ln(3)$:\n$$\ne^{AT} = \\begin{pmatrix} e^{-\\ln(3)} & 0 \\\\ 0 & e^{-2\\ln(3)} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{9} \\end{pmatrix}\n$$\nSince $e^{AT}$ is a symmetric matrix, $(e^{AT})^{\\top} = e^{AT}$. We compute the new direction vector $c' = e^{AT} c$:\n$$\nc' = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ 0 & \\frac{1}{9} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{4}{9} \\end{pmatrix}\n$$\nNow we find the support function of $X_0 = [0,2] \\times [-1,1]$ in the direction $c'$:\n$$\nh_{X_0}(c') = \\sup_{x \\in X_0} (c')^{\\top} x = \\sup_{x_1 \\in [0,2], x_2 \\in [-1,1]} \\left(1 \\cdot x_1 + \\frac{4}{9} \\cdot x_2\\right)\n$$\nSince the objective is separable, we can maximize each term independently:\n$$\nh_{X_0}(c') = \\left(\\sup_{x_1 \\in [0,2]} 1 \\cdot x_1\\right) + \\left(\\sup_{x_2 \\in [-1,1]} \\frac{4}{9} \\cdot x_2\\right) = (1 \\cdot 2) + \\left(\\frac{4}{9} \\cdot 1\\right) = 2 + \\frac{4}{9} = \\frac{22}{9}\n$$\n\n**Term 2: $h_{R_U(T)}(c)$**\nThe second term is the support function of an integral of a set. A key result states that we can interchange the integral and the support function operation:\n$$\nh_{R_U(T)}(c) = h_{\\int_0^T e^{A(T-\\tau)} B U d\\tau}(c) = \\int_0^T h_{e^{A(T-\\tau)} B U}(c) d\\tau\n$$\nUsing the property for linear transformations again, with $\\mathcal{L}_{\\tau} = e^{A(T-\\tau)}B$:\n$$\nh_{R_U(T)}(c) = \\int_0^T h_U((e^{A(T-\\tau)}B)^{\\top} c) d\\tau\n$$\nGiven $B=I_2$ and $A$ is symmetric, $(e^{A(T-\\tau)}B)^{\\top} = B^{\\top}(e^{A(T-\\tau)})^{\\top} = I_2 e^{A(T-\\tau)} = e^{A(T-\\tau)}$. The direction vector for $h_U$ at time $\\tau$ is $d(\\tau) = e^{A(T-\\tau)}c$:\n$$\nd(\\tau) = \\begin{pmatrix} e^{-(T-\\tau)} & 0 \\\\ 0 & e^{-2(T-\\tau)} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 3 e^{-(T-\\tau)} \\\\ 4 e^{-2(T-\\tau)} \\end{pmatrix}\n$$\nThe support function of the input set $U = [-1,1] \\times [-2,2]$ in this direction is:\n$$\nh_U(d(\\tau)) = \\sup_{u \\in U} d(\\tau)^{\\top}u = \\sup_{u_1 \\in [-1,1], u_2 \\in [-2,2]} \\left( 3 e^{-(T-\\tau)}u_1 + 4 e^{-2(T-\\tau)}u_2 \\right)\n$$\nFor any $\\tau \\in [0,T]$, the exponential terms are positive. To maximize the expression, we must choose the largest possible values for $u_1$ and $u_2$, which are $1$ and $2$ respectively.\n$$\nh_U(d(\\tau)) = \\left( 3 e^{-(T-\\tau)} \\cdot 1 \\right) + \\left( 4 e^{-2(T-\\tau)} \\cdot 2 \\right) = 3 e^{-(T-\\tau)} + 8 e^{-2(T-\\tau)}\n$$\nNow we integrate this expression from $\\tau=0$ to $\\tau=T$:\n$$\nh_{R_U(T)}(c) = \\int_0^T \\left( 3 e^{-(T-\\tau)} + 8 e^{-2(T-\\tau)} \\right) d\\tau\n$$\nLet's use the substitution $s=T-\\tau$, which implies $ds = -d\\tau$. The limits of integration change from $\\tau=0$ to $s=T$ and $\\tau=T$ to $s=0$:\n$$\nh_{R_U(T)}(c) = \\int_T^0 (3 e^{-s} + 8 e^{-2s})(-ds) = \\int_0^T (3 e^{-s} + 8 e^{-2s}) ds\n$$\nEvaluating the integral:\n$$\n\\int_0^T (3 e^{-s} + 8 e^{-2s}) ds = \\left[ -3 e^{-s} - \\frac{8}{2} e^{-2s} \\right]_0^T = \\left[ -3 e^{-s} - 4 e^{-2s} \\right]_0^T\n$$\n$$\n= (-3 e^{-T} - 4 e^{-2T}) - (-3 e^0 - 4 e^0) = (-3 e^{-T} - 4 e^{-2T}) - (-7) = 7 - 3 e^{-T} - 4 e^{-2T}\n$$\nSubstitute $T=\\ln(3)$, so $e^{-T}=1/3$ and $e^{-2T}=1/9$:\n$$\nh_{R_U(T)}(c) = 7 - 3\\left(\\frac{1}{3}\\right) - 4\\left(\\frac{1}{9}\\right) = 7 - 1 - \\frac{4}{9} = 6 - \\frac{4}{9} = \\frac{54-4}{9} = \\frac{50}{9}\n$$\n\n**Final Calculation**\nFinally, we sum the two terms to find the total value of the performance index:\n$$\nJ^{\\star} = h_{e^{AT} X_0}(c) + h_{R_U(T)}(c) = \\frac{22}{9} + \\frac{50}{9} = \\frac{72}{9} = 8\n$$",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "Modern CPS increasingly rely on complex, learning-based components like neural networks, which pose a significant verification challenge due to their non-linear and high-dimensional nature. This advanced practice demonstrates how to formally verify a neural network controller by encoding its behavior, along with the set of possible inputs, into a Mixed-Integer Linear Program (MILP). By formulating and solving this optimization problem, you can find the exact worst-case output of the network, providing rigorous safety guarantees for an AI-driven system .",
            "id": "4253557",
            "problem": "A digital twin of a cyber-physical system (CPS) includes a neural-network-based safety-critical controller that maps a measured state vector $x \\in \\mathbb{R}^{2}$ to a scalar control action $u \\in \\mathbb{R}$. The system-level safety requirement stipulates that, over all admissible states $x$ in a known input polytope, the controller output must satisfy $u \\in [u_{\\min}, u_{\\max}]$.\n\nConsider a single-hidden-layer feedforward network with a hidden layer of $2$ units using the Rectified Linear Unit (ReLU). The network parameters are as follows: the hidden layer pre-activation is $z = W_{1} x + b_{1}$ with \n$$\nW_{1} = \n\\begin{pmatrix}\n1 & -2 \\\\\n-3 & 1\n\\end{pmatrix},\n\\quad\nb_{1} = \n\\begin{pmatrix}\n0.5 \\\\\n-0.2\n\\end{pmatrix},\n$$\nand the hidden layer output is $y = \\max(0, z)$ applied elementwise. The scalar output is $u = c^{\\top} y + d$ with\n$$\nc = \n\\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix},\n\\quad\nd = 0.3.\n$$\nThe admissible state space is the polytope \n$$\n\\{ x \\in \\mathbb{R}^{2} \\mid -1 \\le x_{1} \\le 2, \\; 0 \\le x_{2} \\le 1 \\},\n$$\nwhich can be equivalently written in the form $A x \\le b$ with appropriate $A \\in \\mathbb{R}^{m \\times 2}$ and $b \\in \\mathbb{R}^{m}$.\n\nYour task is to encode the exact verification problem “ensure $u \\in [u_{\\min}, u_{\\max}]$ for all $x$ in the admissible state polytope” as a Mixed Integer Linear Program (MILP), starting from fundamental definitions (piecewise-linear ReLU and polyhedral sets) and without using any pre-given encoding shortcuts. In particular:\n\n- Derive pre-activation bounds for each hidden unit using only the given $W_{1}$, $b_{1}$, and the input polytope, and then encode each ReLU exactly by introducing binary variables and big-$M$ constants that are tight with respect to those bounds.\n- Formulate the overall MILP whose objective maximizes the controller output $u$ over the input polytope, subject to the exact ReLU encoding constraints and all network relations.\n- Based on your MILP and the derived bounds, compute the worst-case upper bound $u_{\\max}^{\\star}$ of $u$ over the admissible polytope. Round your final numeric result to four significant figures and express it as a dimensionless quantity.\n- Briefly discuss solver guarantees in this setting, including what conditions are required to claim soundness and completeness of the MILP-based verification with respect to the ReLU network and the given input polytope.\n\nLet the safety specification be $u \\in [u_{\\min}, u_{\\max}]$ with $u_{\\min} = -4$ and $u_{\\max} = 5$. Your final answer must be the single rounded value of $u_{\\max}^{\\star}$, following the rounding instruction above.",
            "solution": "The user-provided problem is critically validated against the established criteria.\n\n### Step 1: Extract Givens\n- **System**: A cyber-physical system (CPS) with a neural-network controller.\n- **Controller Input**: State vector $x \\in \\mathbb{R}^{2}$.\n- **Controller Output**: Scalar control action $u \\in \\mathbb{R}$.\n- **Safety Requirement**: $u \\in [u_{\\min}, u_{\\max}]$ over an admissible input-state polytope.\n- **Network Architecture**: Single hidden layer with $2$ ReLU units.\n- **Hidden Layer Pre-activation**: $z = W_{1} x + b_{1}$, where\n$$\nW_{1} = \n\\begin{pmatrix}\n1 & -2 \\\\\n-3 & 1\n\\end{pmatrix},\n\\quad\nb_{1} = \n\\begin{pmatrix}\n0.5 \\\\\n-0.2\n\\end{pmatrix}\n$$\n- **Hidden Layer Activation**: $y = \\max(0, z)$ (element-wise ReLU).\n- **Output Layer**: $u = c^{\\top} y + d$, where\n$$\nc = \n\\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix},\n\\quad\nd = 0.3\n$$\n- **Admissible State Space (Input Polytope)**: $\\{ x \\in \\mathbb{R}^{2} \\mid -1 \\le x_{1} \\le 2, \\; 0 \\le x_{2} \\le 1 \\}$.\n- **Safety Specification**: $u_{\\min} = -4$, $u_{\\max} = 5$.\n- **Task**:\n  1. Encode the verification problem as a Mixed Integer Linear Program (MILP).\n  2. Derive tight pre-activation bounds and use them in a big-M encoding.\n  3. Formulate the MILP to maximize $u$.\n  4. Compute the worst-case upper bound $u_{\\max}^{\\star}$ and round to four significant figures.\n  5. Discuss solver guarantees for soundness and completeness.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a canonical example of neural network verification using Mixed Integer Linear Programming. This is a well-established and scientifically sound method in the field of formal verification for cyber-physical systems. The network structure (feedforward with ReLUs) and the use of a polyhedral input set are standard for this type of analysis.\n- **Well-Posed**: The problem is well-posed. The function to be maximized, $u(x)$, is a continuous and piecewise-linear function of $x$. The domain of $x$ is a compact set (a closed and bounded polytope). By the Extreme Value Theorem, the function $u(x)$ must attain its maximum and minimum values on this domain. The problem asks for the maximum value, which is guaranteed to exist and be unique.\n- **Objective**: The problem is stated using precise mathematical notation and terminology. There are no subjective or ambiguous statements.\n- **Completeness and Consistency**: All parameters of the neural network ($W_1, b_1, c, d$) and the definition of the input set are fully specified. The dimensions of all matrices and vectors are consistent for valid matrix-vector operations. For example, $W_1$ is $2 \\times 2$, $x$ is $2 \\times 1$, $b_1$ is $2 \\times 1$, so $z$ is $2 \\times 1$. $c^T$ is $1 \\times 2$, $y$ is $2 \\times 1$, so $c^T y$ is a scalar. All aspects are consistent.\n- **Other Flaws**: The problem is not trivial, metaphorical, or outside the scope of scientific verification. It represents a concrete, solvable instance of a relevant problem in the specified domain.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous solution will be provided.\n\n### Solution Derivation\n\nThe neural network controller is defined by the equations:\n$$ z = W_{1} x + b_{1} $$\n$$ y = \\text{ReLU}(z) = \\max(0, z) $$\n$$ u = c^{\\top} y + d $$\n\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, $z = \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}$, and $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$. The network equations are:\n$$ z_1 = x_1 - 2x_2 + 0.5 $$\n$$ z_2 = -3x_1 + x_2 - 0.2 $$\n$$ y_1 = \\max(0, z_1) $$\n$$ y_2 = \\max(0, z_2) $$\n$$ u = 2y_1 - y_2 + 0.3 $$\n\nThe problem is to find the maximum value of $u$, denoted $u_{\\max}^{\\star}$, for all $x$ in the input polytope defined by $-1 \\le x_1 \\le 2$ and $0 \\le x_2 \\le 1$.\n\n**1. Derivation of Pre-activation Bounds**\nTo formulate the MILP, we first need to find the lower and upper bounds ($L_j, U_j$) for each pre-activation variable $z_j$ over the input polytope. Since $z_j$ are linear functions of $x_1$ and $x_2$, and the input domain is a convex polytope (a rectangle), the extrema will occur at the vertices of the domain: $(-1,0)$, $(2,0)$, $(-1,1)$, and $(2,1)$.\n\nFor $z_1 = x_1 - 2x_2 + 0.5$:\nThe bounds can be found by separately minimizing and maximizing the terms:\n$L_1 = (\\min x_1) - 2(\\max x_2) + 0.5 = (-1) - 2(1) + 0.5 = -2.5$.\n$U_1 = (\\max x_1) - 2(\\min x_2) + 0.5 = (2) - 2(0) + 0.5 = 2.5$.\nSo, $z_1 \\in [-2.5, 2.5]$.\n\nFor $z_2 = -3x_1 + x_2 - 0.2$:\n$L_2 = -3(\\max x_1) + (\\min x_2) - 0.2 = -3(2) + 0 - 0.2 = -6.2$.\n$U_2 = -3(\\min x_1) + (\\max x_2) - 0.2 = -3(-1) + 1 - 0.2 = 3.8$.\nSo, $z_2 \\in [-6.2, 3.8]$.\n\nBoth neurons are 'unstable' as their pre-activation bounds span across $0$. This means they can be either active or inactive depending on the input $x$.\n\n**2. MILP Formulation for ReLU**\nEach ReLU neuron $y_j = \\max(0, z_j)$ with pre-activation bounds $[L_j, U_j]$ can be exactly modeled using a binary variable $\\delta_j \\in \\{0, 1\\}$ and the big-M method. The tightest big-M constants are $M_j^{\\text{upper}} = U_j$ and $M_j^{\\text{lower}} = L_j$. The set of linear constraints is:\n\n1. $y_j \\ge z_j$\n2. $y_j \\ge 0$\n3. $z_j \\le U_j \\delta_j$\n4. $y_j \\le U_j \\delta_j$\n5. $y_j \\le z_j - L_j(1 - \\delta_j)$\n\nIf $\\delta_j=1$, $z_j \\in (0, U_j]$, and the constraints force $y_j = z_j$. If $\\delta_j=0$, $z_j \\in [L_j, 0]$, and the constraints force $y_j = 0$.\n\nFor neuron 1 ($L_1=-2.5, U_1=2.5$):\n- $y_1 \\ge z_1$\n- $y_1 \\ge 0$\n- $z_1 \\le 2.5 \\delta_1$\n- $y_1 \\le 2.5 \\delta_1$\n- $y_1 \\le z_1 - (-2.5)(1-\\delta_1) \\Rightarrow y_1 \\le z_1 + 2.5(1-\\delta_1)$\n\nFor neuron 2 ($L_2=-6.2, U_2=3.8$):\n- $y_2 \\ge z_2$\n- $y_2 \\ge 0$\n- $z_2 \\le 3.8 \\delta_2$\n- $y_2 \\le 3.8 \\delta_2$\n- $y_2 \\le z_2 - (-6.2)(1-\\delta_2) \\Rightarrow y_2 \\le z_2 + 6.2(1-\\delta_2)$\n\n**3. Complete MILP Formulation to Maximize $u$**\nThe overall problem of finding $u_{\\max}^{\\star}$ is expressed as the following MILP:\n- **Objective function**: Maximize $u = 2y_1 - y_2 + 0.3$.\n- **Variables**:\n  - Continuous: $x_1, x_2, z_1, z_2, y_1, y_2$\n  - Binary: $\\delta_1, \\delta_2$\n- **Constraints**:\n  - Input polytope:\n    - $-1 \\le x_1 \\le 2$\n    - $0 \\le x_2 \\le 1$\n  - Network definition:\n    - $z_1 = x_1 - 2x_2 + 0.5$\n    - $z_2 = -3x_1 + x_2 - 0.2$\n  - ReLU constraints for neuron 1:\n    - $y_1 \\ge z_1$, $y_1 \\ge 0$, $z_1 \\le 2.5 \\delta_1$, $y_1 \\le 2.5 \\delta_1$, $y_1 \\le z_1 + 2.5(1-\\delta_1)$\n  - ReLU constraints for neuron 2:\n    - $y_2 \\ge z_2$, $y_2 \\ge 0$, $z_2 \\le 3.8 \\delta_2$, $y_2 \\le 3.8 \\delta_2$, $y_2 \\le z_2 + 6.2(1-\\delta_2)$\n\n**4. Computation of $u_{\\max}^{\\star}$**\nSolving the MILP is equivalent to analyzing the $2^2=4$ possible linear programs (LPs) corresponding to the four discrete activation patterns $(\\delta_1, \\delta_2) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}$. The global maximum is the maximum over all feasible LPs.\n\n- **Case 1: $(\\delta_1, \\delta_2) = (0,0)$**\n  - Conditions: $z_1 \\le 0, z_2 \\le 0$.\n  - Network output: $y_1 = 0, y_2 = 0$. So, $u = 2(0) - 0 + 0.3 = 0.3$.\n  - This activation pattern is feasible for some $x$ in the polytope (e.g., $x=(0.5, 0.5)$ gives $z_1=0, z_2=-1.2$).\n  - The maximum $u$ in this region is $u_{\\max}^{(1)} = 0.3$.\n\n- **Case 2: $(\\delta_1, \\delta_2) = (0,1)$**\n  - Conditions: $z_1 \\le 0, z_2 > 0$.\n  - Network output: $y_1 = 0, y_2 = z_2 = -3x_1 + x_2 - 0.2$.\n  - $u = 2(0) - (-3x_1 + x_2 - 0.2) + 0.3 = 3x_1 - x_2 + 0.5$.\n  - We maximize $3x_1 - x_2 + 0.5$ over the region $\\{x \\mid -1\\le x_1 \\le 2, 0\\le x_2 \\le 1, x_1 - 2x_2 + 0.5 \\le 0, -3x_1 + x_2 - 0.2 > 0\\}$.\n  - The maximum of this linear objective is found at a vertex of the feasible region. By inspection, none of the vertices of this sub-polytope yield a value of $u$ greater than $0.3$ (e.g., at $x=(-1,1)$, $u=-3.5$; at $x=(0.8/3, 1)$, $u=0.3$).\n  - The maximum $u$ in this region is $u_{\\max}^{(2)} = 0.3$.\n\n- **Case 3: $(\\delta_1, \\delta_2) = (1,0)$**\n  - Conditions: $z_1 > 0, z_2 \\le 0$.\n  - Network output: $y_1 = z_1 = x_1 - 2x_2 + 0.5$, and $y_2 = 0$.\n  - $u = 2(x_1 - 2x_2 + 0.5) - 0 + 0.3 = 2x_1 - 4x_2 + 1.3$.\n  - We maximize $2x_1 - 4x_2 + 1.3$ over the region $\\{x \\mid -1\\le x_1 \\le 2, 0\\le x_2 \\le 1, x_1 - 2x_2 + 0.5 > 0, -3x_1 + x_2 - 0.2 \\le 0\\}$.\n  - The objective function gradient $(2, -4)$ favors large $x_1$ and small $x_2$. We test the corresponding vertex of the input box, $x=(2,0)$.\n    - For $x=(2,0)$: $z_1 = 2-0+0.5 = 2.5 > 0$. $z_2 = -3(2)+0-0.2 = -6.2 \\le 0$.\n  - The point $(2,0)$ is feasible for this case. Since the objective is linear, the maximum over the sub-polytope is attained at one of its vertices, and $(2,0)$ is a vertex of this sub-polytope that also maximizes the objective over the entire input box. Thus, the maximum is attained here.\n  - $u(2,0) = 2(2) - 4(0) + 1.3 = 5.3$.\n  - The maximum $u$ in this region is $u_{\\max}^{(3)} = 5.3$.\n\n- **Case 4: $(\\delta_1, \\delta_2) = (1,1)$**\n  - Conditions: $z_1 > 0, z_2 > 0$.\n  - Network output: $y_1 = z_1, y_2 = z_2$.\n  - $u = 2z_1 - z_2 + 0.3 = 2(x_1 - 2x_2 + 0.5) - (-3x_1 + x_2 - 0.2) + 0.3 = 5x_1 - 5x_2 + 1.5$.\n  - We maximize $5x_1 - 5x_2 + 1.5$ over the region $\\{x \\mid -1\\le x_1 \\le 2, 0\\le x_2 \\le 1, x_1 - 2x_2 + 0.5 > 0, -3x_1 + x_2 - 0.2 > 0\\}$.\n  - The objective function gradient $(5, -5)$ favors large $x_1$ and small $x_2$. The point $(2,0)$ is not in this region ($z_2(2,0) < 0$).\n  - The feasible region is a triangle with vertices $(-0.5, 0)$, $(-1/15, 0)$, and $(0.02, 0.26)$.\n  - Evaluating $u$ at these vertices: $u(-0.5, 0) = -1.0$, $u(-1/15, 0) \\approx 1.167$, $u(0.02, 0.26) = 0.3$.\n  - The maximum $u$ in this region is $u_{\\max}^{(4)} = 7/6 \\approx 1.167$.\n\n**Conclusion on $u_{\\max}^{\\star}$**\nThe global maximum output is the maximum of the values found in each case:\n$u_{\\max}^{\\star} = \\max\\{0.3, 0.3, 5.3, 7/6\\} = 5.3$.\nRounding to four significant figures gives $5.300$. This value is greater than the specified safety limit $u_{\\max}=5$, indicating a safety violation.\n\n**5. Discussion of Solver Guarantees**\nThe MILP-based verification approach provides strong guarantees of soundness and completeness under specific conditions.\n- **Soundness**: The encoding is sound if any feasible solution to the MILP corresponds to a true behavior of the neural network. The big-M formulation for ReLUs is an exact, not an approximate, representation. Therefore, if the pre-activation bounds $[L_j, U_j]$ are computed correctly (i.e., they are valid lower and upper bounds, as was done here), the MILP constraints are a completely faithful model of the network's input-output function over the specified polytope. Any optimal solution found by the solver corresponds to a real input $x$ and output $u$.\n- **Completeness**: The encoding is complete if for any input $x$ in the polytope, there is a corresponding feasible solution in the MILP. Since the constraints exactly mirror the network's mathematical definition for all inputs, this property holds. The MILP formulation covers the entire input space without gaps.\n- **Solver Guarantees**: A correct MILP solver (e.g., using a branch-and-bound algorithm) is guaranteed to find the globally optimal solution, provided it runs to completion. This means the $u_{\\max}^{\\star}$ it returns is the true maximum value of the network's output over the entire input polytope. The method is therefore exhaustive.\n\nIn summary, for piecewise-linear networks like ReLU FNNs and polyhedral input sets, MILP-based verification is a sound and complete verification method. The main requirements are the correct derivation of pre-activation bounds and the use of a correct solver that is run to termination.",
            "answer": "$$\n\\boxed{5.300}\n$$"
        }
    ]
}