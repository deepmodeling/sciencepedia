{
    "hands_on_practices": [
        {
            "introduction": "在验证系统属性时，一个核心任务是确定需要对系统输入施加何种约束，才能确保其输出满足安全规范。本练习  将引导您应用最弱前置条件（Weakest Precondition, WP）演算，这是一种强大的形式化方法，用于从期望的输出属性（后置条件）逆向推导输入所需满足的最宽松条件。通过对一个典型的网络物理传感流水线进行分析，您将实践如何形式化地保证系统在连续操作下的行为边界。",
            "id": "4250414",
            "problem": "考虑一个数字孪生中的离散时间网络物理感知管道。设输入传感器序列为一个实值序列 $\\{y[k]\\}_{k \\in \\mathbb{Z}}$，且 $\\|y\\|_{\\infty} = \\sup_{k \\in \\mathbb{Z}} |y[k]|$。该管道应用以下操作序列以产生输出 $\\{r[k]\\}_{k \\in \\mathbb{Z}}$：\n1) 一个具有三个抽头 $h_{0}, h_{1}, h_{2} \\in \\mathbb{R}$ 的因果有限冲激响应滤波器：\n$$\nv[k] = h_{0} y[k] + h_{1} y[k-1] + h_{2} y[k-2].\n$$\n2) 一个由 $\\gamma > 0$ 参数化的非线性归一化映射，逐元素地应用于：\n$$\nw[k] = g(v[k]) \\quad \\text{其中} \\quad g(u) = \\frac{u}{1 + \\gamma |u|}.\n$$\n3) 一个仿射缩放和偏移：\n$$\nr[k] = \\alpha w[k] + \\beta,\n$$\n其中 $\\alpha, \\beta \\in \\mathbb{R}$。\n\n数字孪生输出流所需的安全属性是一致有界后置条件\n$$\n\\forall k \\in \\mathbb{Z}:\\ |r[k]| \\leq B,\n$$\n其中给定 $B > 0$。假设参数满足 $0  B - |\\beta|  |\\alpha|/\\gamma$。\n\n使用顺序组合的最弱前置条件 (WP) 语义以及关于线性算子诱导范数和单调利普希茨非线性的基本事实，推导输入 $\\{y[k]\\}$ 上的最弱前置条件谓词，以确保上述后置条件对所有时间索引均成立。然后，计算最小界 $C$，使得最弱前置条件可以写成规范形式\n$$\n\\|y\\|_{\\infty} \\leq C.\n$$\n\n将最终的 $C$ 表示为关于 $\\alpha, \\beta, \\gamma, B,$ 以及滤波器系数 $h_{0}, h_{1}, h_{2}$ 的单个闭式解析表达式。不需要进行数值评估，也无需指定单位。最终答案必须是单个表达式。",
            "solution": "该问题要求推导输入序列 $\\{y[k]\\}$ 上的最弱前置条件，以保证安全属性 $|r[k]| \\leq B$ 对所有 $k \\in \\mathbb{Z}$ 成立。所寻求的最弱前置条件的形式为 $\\|y\\|_{\\infty} \\leq C$。推导过程是通过将后置条件从输出 $r[k]$ 到输入 $y[k]$，沿着操作序列向后传播来进行的。\n\n该管道包含三个顺序操作：\n1. FIR 滤波器：$v[k] = h_{0} y[k] + h_{1} y[k-1] + h_{2} y[k-2]$\n2. 非线性映射：$w[k] = g(v[k]) = \\frac{v[k]}{1 + \\gamma |v[k]|}$\n3. 仿射变换：$r[k] = \\alpha w[k] + \\beta$\n\n后置条件是输出上的安全属性：\n$$\n\\forall k \\in \\mathbb{Z}: |r[k]| \\leq B\n$$\n\n我们将从最后一个阶段开始，为每个阶段找到最弱前置条件。\n\n**步骤1：关于 $\\{w[k]\\}$ 的前置条件**\n我们从 $r[k]$ 上的条件开始，并对仿射变换求逆。\n$$\n|r[k]| \\leq B \\iff |\\alpha w[k] + \\beta| \\leq B\n$$\n这个双边不等式等价于：\n$$\n-B \\leq \\alpha w[k] + \\beta \\leq B\n$$\n从所有部分减去 $\\beta$ 得到：\n$$\n-B - \\beta \\leq \\alpha w[k] \\leq B - \\beta\n$$\n问题陈述提供了约束条件 $0  B - |\\beta|  |\\alpha|/\\gamma$，这意味着 $B > |\\beta|$ 且 $|\\alpha| \\neq 0$。我们必须考虑 $\\alpha$ 的符号。\n\n情况1：$\\alpha > 0$。则 $|\\alpha| = \\alpha$。\n$$\n\\frac{-B - \\beta}{\\alpha} \\leq w[k] \\leq \\frac{B - \\beta}{\\alpha}\n$$\n这等价于 $|w[k]| \\leq \\min\\left(\\frac{B - \\beta}{\\alpha}, \\frac{B + \\beta}{\\alpha}\\right)$。由于 $B > |\\beta|$，两个分子都为正。最小值为 $\\frac{B - |\\beta|}{\\alpha} = \\frac{B - |\\beta|}{|\\alpha|}$。\n\n情况2：$\\alpha  0$。则 $|\\alpha| = -\\alpha$。\n$$\n\\frac{B - \\beta}{\\alpha} \\leq w[k] \\leq \\frac{-B - \\beta}{\\alpha} \\implies \\frac{-(B - \\beta)}{|\\alpha|} \\leq w[k] \\leq \\frac{-(-B-\\beta)}{|\\alpha|} \\implies \\frac{-(B - \\beta)}{|\\alpha|} \\leq w[k] \\leq \\frac{B + \\beta}{|\\alpha|}\n$$\n这等价于 $|w[k]| \\leq \\min\\left(\\frac{B + \\beta}{|\\alpha|}, \\frac{B - \\beta}{|\\alpha|}\\right) = \\frac{B - |\\beta|}{|\\alpha|}$。\n\n在两种情况下，对 $w[k]$ 的最弱前置条件是：\n$$\n|w[k]| \\leq \\frac{B - |\\beta|}{|\\alpha|}\n$$\n我们将这个界定义为 $B' = \\frac{B - |\\beta|}{|\\alpha|}$。对 $\\{w[k]\\}$ 的前置条件是 $|w[k]| \\leq B'$ 对所有 $k \\in \\mathbb{Z}$ 成立。\n\n**步骤2：关于 $\\{v[k]\\}$ 的前置条件**\n接下来，我们通过非线性映射 $g(u)$ 将 $\\{w[k]\\}$ 上的条件向后传播。\n$$\n|w[k]| \\leq B' \\iff \\left| \\frac{v[k]}{1 + \\gamma |v[k]|} \\right| \\leq B'\n$$\n这可以简化为：\n$$\n\\frac{|v[k]|}{1 + \\gamma |v[k]|} \\leq B'\n$$\n令 $x = |v[k]|$。对于 $x \\geq 0$，函数 $f(x) = \\frac{x}{1 + \\gamma x}$ 是单调递增的，因为它的导数 $f'(x) = \\frac{1}{(1+\\gamma x)^2}$ 对 $\\gamma > 0$ 总是正的。因此，我们可以求解 $x$ 而不必改变不等号的方向。\n$$\nx \\leq B'(1 + \\gamma x) \\implies x \\leq B' + \\gamma B' x \\implies x(1 - \\gamma B') \\leq B'\n$$\n问题陈述 $0  B - |\\beta|  |\\alpha|/\\gamma$。除以 $|\\alpha| > 0$ 得到 $0  \\frac{B - |\\beta|}{|\\alpha|}  \\frac{1}{\\gamma}$，即 $0  B'  1/\\gamma$。这意味着 $1 - \\gamma B' > 0$。因此，我们可以除以 $(1 - \\gamma B')$：\n$$\nx \\leq \\frac{B'}{1 - \\gamma B'}\n$$\n代入 $x = |v[k]|$ 和 $B' = \\frac{B - |\\beta|}{|\\alpha|}$，对 $v[k]$ 的最弱前置条件是：\n$$\n|v[k]| \\leq \\frac{\\frac{B - |\\beta|}{|\\alpha|}}{1 - \\gamma \\frac{B - |\\beta|}{|\\alpha|}} = \\frac{B - |\\beta|}{|\\alpha| - \\gamma (B - |\\beta|)}\n$$\n我们将这个新的界定义为 $B'' = \\frac{B - |\\beta|}{|\\alpha| - \\gamma (B - |\\beta|)}$。条件是 $|v[k]| \\leq B''$ 对所有 $k \\in \\mathbb{Z}$ 成立。\n\n**步骤3：关于 $\\{y[k]\\}$ 的前置条件**\n最后，我们通过 FIR 滤波器将 $\\{v[k]\\}$ 上的条件向后传播。我们需要找到形式为 $\\|y\\|_{\\infty} \\leq C$ 的最弱前置条件，以确保 $|v[k]| \\leq B''$ 对所有 $k$ 成立。\n$$\n|v[k]| = |h_0 y[k] + h_1 y[k-1] + h_2 y[k-2]|\n$$\n使用三角不等式：\n$$\n|v[k]| \\leq |h_0||y[k]| + |h_1||y[k-1]| + |h_2||y[k-2]|\n$$\n由于对于任何索引 $j$ 都有 $|y[j]| \\leq \\sup_{i \\in \\mathbb{Z}} |y[i]| = \\|y\\|_{\\infty}$，我们可以写出：\n$$\n|v[k]| \\leq |h_0|\\|y\\|_{\\infty} + |h_1|\\|y\\|_{\\infty} + |h_2|\\|y\\|_{\\infty} = (|h_0| + |h_1| + |h_2|)\\|y\\|_{\\infty}\n$$\n项 $\\|h\\|_1 = |h_0| + |h_1| + |h_2|$ 是滤波器冲激响应的 $L_1$ 范数，也就是卷积算子的诱导 $L_{\\infty}$ 范数。这个界是紧的，意味着对于任何 $\\|y\\|_{\\infty}$，都存在一个序列 $\\{y[k]\\}$ 使得 $\\sup_k |v[k]| = \\|h\\|_1 \\|y\\|_{\\infty}$。\n为了保证 $|v[k]| \\leq B''$ 对所有 $k$ 和所有有效的输入序列成立，只需 $|v[k]|$ 的最大可能值受 $B''$ 的约束：\n$$\n\\|h\\|_1 \\|y\\|_{\\infty} \\leq B''\n$$\n这给出了一个关于 $\\|y\\|_{\\infty}$ 的条件：\n$$\n\\|y\\|_{\\infty} \\leq \\frac{B''}{\\|h\\|_1}\n$$\n问题要求找到最小界 $C$，使得 $\\|y\\|_{\\infty} \\leq C$ 是这种形式的最弱前置条件。这对应于 $\\|y\\|_{\\infty}$ 上界的可能最大值，恰好是 $\\frac{B''}{\\|h\\|_1}$ (假设 $\\|h\\|_1 \\neq 0$ )。如果 $\\|h\\|_1 = 0$，则 $v[k]=0$ 且任何输入都是允许的，因此不存在有限的最小界 $C$。我们假设 $\\|h\\|_1 > 0$。\n因此，$C$ 由以下公式给出：\n$$\nC = \\frac{B''}{|h_0| + |h_1| + |h_2|}\n$$\n代入 $B''$ 的表达式：\n$$\nC = \\frac{\\frac{B - |\\beta|}{|\\alpha| - \\gamma (B - |\\beta|)}}{|h_0| + |h_1| + |h_2|} = \\frac{B - |\\beta|}{\\left(|\\alpha| - \\gamma (B - |\\beta|)\\right)\\left(|h_0| + |h_1| + |h_2|\\right)}\n$$\n这就是最小界 $C$ 的最终闭式表达式。",
            "answer": "$$\n\\boxed{\\frac{B - |\\beta|}{(|\\alpha| - \\gamma (B - |\\beta|))(|h_{0}| + |h_{1}| + |h_{2}|)}}\n$$"
        },
        {
            "introduction": "许多网络物理系统本质上是随机的，其行为受信于概率而非确定性规则。本练习  引入马尔可夫决策过程（Markov Decision Process, MDP）作为模型，来捕捉这种不确定性，并应用概率模型检验技术来验证安全属性。您将通过求解贝尔曼方程（Bellman equations）来计算在最坏情况下的安全保证，这是评估和设计鲁棒系统时至关重要的一步。",
            "id": "4250421",
            "problem": "一个信息物理系统（CPS）的数字孪生（DT）被用于合成监督控制，以将物理资产保持在安全包络内。该DT被建模为一个马尔可夫决策过程（MDP），定义为一个元组 $(S, A, P, s_{\\mathrm{init}})$，其中 $S$ 是一个有限状态集，$A$ 是一个有限动作集，$P: S \\times A \\times S \\to [0,1]$ 是一个转移核，对于所有 $s \\in S$ 和 $a \\in A$ 满足 $\\sum_{s' \\in S} P(s,a,s')=1$，并且 $s_{\\mathrm{init}} \\in S$ 是初始状态。一个确定性无记忆调度器（策略）$\\sigma: S \\to A$ 解决非确定性，从而导出一个离散时间马尔可夫链。安全性质在概率计算树逻辑（PCTL）中表示为 $\\mathbb{P}_{\\ge 0.99}[\\mathbf{G}\\, \\mathrm{safe}]$，其中 $\\mathbf{G}$ 是“全局”（总是）时序算子，“safe”是表征安全集的一元状态谓词。\n\n考虑以下为DT监督器建模的具体MDP实例：\n\n- 状态：$S=\\{s_{0}, s_{1}, s_{2}, ok, fail\\}$。初始状态是 $s_{\\mathrm{init}}=s_{0}$。\n- 安全集：$\\mathrm{Safe}=\\{s_{0}, s_{1}, s_{2}, ok\\}$；唯一不安全的状态是 $fail$。\n- 可用动作和转移：\n  - 在 $s_{0}$ 处：动作 $a$ 和 $b$。\n    - $P(s_{0}, a, s_{1})=\\frac{1}{2}$, $P(s_{0}, a, s_{2})=\\frac{1}{2}$。\n    - $P(s_{0}, b, s_{1})=\\frac{1}{3}$, $P(s_{0}, b, s_{2})=\\frac{1}{3}$, $P(s_{0}, b, fail)=\\frac{1}{3}$。\n  - 在 $s_{1}$ 处：动作 $c$ 和 $d$。\n    - $P(s_{1}, c, ok)=\\frac{255}{256}$, $P(s_{1}, c, fail)=\\frac{1}{256}$。\n    - $P(s_{1}, d, ok)=\\frac{31}{32}$, $P(s_{1}, d, fail)=\\frac{1}{32}$。\n  - 在 $s_{2}$ 处：动作 $e$ 和 $f$。\n    - $P(s_{2}, e, ok)=\\frac{511}{512}$, $P(s_{2}, e, fail)=\\frac{1}{512}$。\n    - $P(s_{2}, f, ok)=\\frac{15}{16}$, $P(s_{2}, f, fail)=\\frac{1}{16}$。\n  - $ok$ 和 $fail$ 是吸收态：$P(ok, -, ok)=1$ 和 $P(fail, -, fail)=1$。\n- 指定的调度器：$\\sigma^{\\star}(s_{0})=a$, $\\sigma^{\\star}(s_{1})=c$, $\\sigma^{\\star}(s_{2})=e$。\n\n任务：\n1) 从PCTL的形式语义和调度器 $\\sigma^{\\star}$ 下导出的马尔可夫链出发，证明性质 $\\mathbb{P}_{\\ge 0.99}[\\mathbf{G}\\, \\mathrm{safe}]$ 在 $s_{0}$ 处成立。最后给出 $\\mathbb{P}^{\\sigma^{\\star}}_{s_{0}}[\\mathbf{G}\\, \\mathrm{safe}]$ 的精确值，并说明其为何满足阈值。\n2) 通过求解安全性的Bellman方程，计算在所有调度器上满足安全性质 $\\mathbf{G}\\, \\mathrm{safe}$ 的最小概率，即，计算\n$$\nv_{\\min}(s_{0}) \\;=\\; \\inf_{\\sigma}\\; \\mathbb{P}^{\\sigma}_{s_{0}}[\\mathbf{G}\\, \\mathrm{safe}],\n$$\n作为此MDP的相应方程组的最小不动点解。将你的最终答案表示为精确分数。需要提交的最终答案是单个数字 $v_{\\min}(s_{0})$（无单位）。",
            "solution": "我们首先回顾在马尔可夫决策过程（MDP）中进行概率安全性推理所需的核心定义。一个MDP $(S, A, P, s_{\\mathrm{init}})$ 由一个有限状态集 $S$、一个有限动作集 $A$ 以及转移概率 $P(s,a,s')$ 组成，对于每个 $s \\in S$ 和 $a \\in A$ 都满足 $\\sum_{s' \\in S} P(s,a,s')=1$。一个确定性无记忆调度器 $\\sigma: S \\to A$ 仅根据当前状态选择一个动作，从而导出一个离散时间、时齐的马尔可夫链，其单步转移概率为 $P_{\\sigma}(s,s') := P(s,\\sigma(s),s')$。\n\n概率计算树逻辑（PCTL）公式 $\\mathbb{P}_{\\ge p}[\\mathbf{G}\\, \\varphi]$ 的语义要求，在调度器 $\\sigma$ 下，从给定状态 $s$ 开始的无限路径上的概率测度，为那些满足 $\\mathbf{G}\\, \\varphi$ 的路径（即在所有时间索引上都保持在满足状态谓词 $\\varphi$ 的状态中的路径）赋予至少为 $p$ 的测度。在我们的设定中，$\\varphi$ 是谓词“safe”，恰好由安全集 $\\mathrm{Safe}$ 中的状态满足。\n\n对于一个固定的调度器 $\\sigma$，概率 $\\mathbb{P}^{\\sigma}_{s}[\\mathbf{G}\\, \\mathrm{safe}]$ 等于从未访问不安全状态 $fail$ 的无限路径集合的测度。通常，$\\mathbf{G}\\, \\mathrm{safe}$ 是 $\\mathbf{F}\\, \\mathrm{unsafe}$ 的补集，其中 $\\mathbf{F}$ 是“最终”算子；因此\n$$\n\\mathbb{P}^{\\sigma}_{s}[\\mathbf{G}\\, \\mathrm{safe}] \\;=\\; 1 \\;-\\; \\mathbb{P}^{\\sigma}_{s}[\\mathbf{F}\\, fail].\n$$\n在这里给出的有限吸收结构中，所有从 $s_0$ 开始的轨迹在最多2步内到达 $ok$ 或 $fail$，之后它们将永远停留在那里，因为 $ok$ 和 $fail$ 是吸收态。因此，“全局安全”事件等同于“到达 $ok$ 且从未访问过 $fail$”。\n\n第1部分：在指定的调度器 $\\sigma^{\\star}$ 下进行验证。\n\n在 $\\sigma^{\\star}$ 下，导出的马尔可夫链具有以下非平凡转移：\n- 从 $s_{0}$ 出发，动作 $a$ 以 $\\frac{1}{2}$ 的概率转移到 $s_{1}$，以 $\\frac{1}{2}$ 的概率转移到 $s_{2}$。\n- 从 $s_{1}$ 出发，动作 $c$ 以 $\\frac{255}{256}$ 的概率转移到 $ok$，以 $\\frac{1}{256}$ 的概率转移到 $fail$。\n- 从 $s_{2}$ 出发，动作 $e$ 以 $\\frac{511}{512}$ 的概率转移到 $ok$，以 $\\frac{1}{512}$ 的概率转移到 $fail$。\n\n因为 $ok$ 和 $fail$ 是吸收态，违反 $\\mathbf{G}\\, \\mathrm{safe}$ 的唯一方式是到达 $fail$。因此，从 $s_0$ 开始满足 $\\mathbf{G}\\, \\mathrm{safe}$ 的概率就是从 $s_0$ 到达 $ok$ 且不经过 $fail$ 的概率，在这里这恰好在两步内发生。以从 $s_{0}$ 出发的第一次转移为条件：\n$$\n\\mathbb{P}^{\\sigma^{\\star}}_{s_{0}}[\\mathbf{G}\\, \\mathrm{safe}]\n\\;=\\; \\frac{1}{2} \\cdot \\frac{255}{256} \\;+\\; \\frac{1}{2} \\cdot \\frac{511}{512}\n\\;=\\; \\frac{255}{512} \\;+\\; \\frac{511}{1024}\n\\;=\\; \\frac{510}{1024} \\;+\\; \\frac{511}{1024}\n\\;=\\; \\frac{1021}{1024}.\n$$\n由于 $\\frac{1021}{1024} = 0.9970703125$，我们有 $\\mathbb{P}^{\\sigma^{\\star}}_{s_{0}}[\\mathbf{G}\\, \\mathrm{safe}] \\ge 0.99$，因此PCTL性质 $\\mathbb{P}_{\\ge 0.99}[\\mathbf{G}\\, \\mathrm{safe}]$ 在 $\\sigma^{\\star}$ 下于 $s_{0}$ 处成立。\n\n第2部分：通过安全性的Bellman方程计算最小概率。\n\n我们现在计算在所有调度器上满足安全性质的最小概率。对于安全性质，最坏情况（对抗性）满足概率 $v_{\\min}(s)$ 是以下Bellman系统的最小不动点解，该系统由路径测度定义和调度器量化的单步展开推导得出：\n- 对于不安全状态 $fail$，$v_{\\min}(fail) = 0$。\n- 对于吸收安全状态 $ok$，$v_{\\min}(ok) = 1$。\n- 对于其他状态 $s \\in \\{s_{0}, s_{1}, s_{2}\\}$，\n$$\nv_{\\min}(s) \\;=\\; \\min_{a \\in A(s)} \\sum_{s' \\in S} P(s,a,s') \\, v_{\\min}(s').\n$$\n直观地，最小化反映了对抗性地选择动作，以最小化永远停留在安全集中的概率。\n\n我们通过反向代入法求解这些方程，利用其无环结构（所有非吸收状态在最多2步内到达 $ok$ 或 $fail$）。\n\n首先，在 $s_1$ 处我们有两个动作：\n- 在动作 $c$ 下，$v_{\\min}(s_{1})$ 将是 $\\frac{255}{256} \\cdot v_{\\min}(ok) + \\frac{1}{256} \\cdot v_{\\min}(fail) = \\frac{255}{256} \\cdot 1 + \\frac{1}{256} \\cdot 0 = \\frac{255}{256}$。\n- 在动作 $d$ 下，$v_{\\min}(s_{1})$ 将是 $\\frac{31}{32} \\cdot v_{\\min}(ok) + \\frac{1}{32} \\cdot v_{\\min}(fail) = \\frac{31}{32}$。\n因此，\n$$\nv_{\\min}(s_{1}) \\;=\\; \\min \\left\\{ \\frac{255}{256}, \\frac{31}{32} \\right\\} \\;=\\; \\frac{31}{32}.\n$$\n\n其次，在 $s_2$ 处：\n- 在动作 $e$ 下，$v_{\\min}(s_{2})$ 将是 $\\frac{511}{512}$。\n- 在动作 $f$ 下，$v_{\\min}(s_{2})$ 将是 $\\frac{15}{16}$。\n因此，\n$$\nv_{\\min}(s_{2}) \\;=\\; \\min \\left\\{ \\frac{511}{512}, \\frac{15}{16} \\right\\} \\;=\\; \\frac{15}{16}.\n$$\n\n最后，在 $s_0$ 处：\n- 在动作 $a$ 下，$v_{\\min}(s_{0})$ 将是\n$$\n\\frac{1}{2}\\, v_{\\min}(s_{1}) \\;+\\; \\frac{1}{2}\\, v_{\\min}(s_{2})\n\\;=\\; \\frac{1}{2} \\cdot \\frac{31}{32} \\;+\\; \\frac{1}{2} \\cdot \\frac{15}{16}\n\\;=\\; \\frac{1}{2} \\cdot \\left( \\frac{31}{32} + \\frac{30}{32} \\right)\n\\;=\\; \\frac{61}{64}.\n$$\n- 在动作 $b$ 下，$v_{\\min}(s_{0})$ 将是\n$$\n\\frac{1}{3}\\, v_{\\min}(s_{1}) \\;+\\; \\frac{1}{3}\\, v_{\\min}(s_{2}) \\;+\\; \\frac{1}{3}\\, v_{\\min}(fail)\n\\;=\\; \\frac{1}{3} \\cdot \\frac{31}{32} \\;+\\; \\frac{1}{3} \\cdot \\frac{15}{16} \\;+\\; \\frac{1}{3} \\cdot 0\n\\;=\\; \\frac{1}{3} \\cdot \\left( \\frac{31}{32} + \\frac{30}{32} \\right)\n\\;=\\; \\frac{61}{96}.\n$$\n因此，\n$$\nv_{\\min}(s_{0}) \\;=\\; \\min \\left\\{ \\frac{61}{64}, \\frac{61}{96} \\right\\} \\;=\\; \\frac{61}{96}.\n$$\n\n这个值是在所有调度器上，从初始状态 $s_0$ 开始满足安全性质 $\\mathbf{G}\\, \\mathrm{safe}$ 的最小概率。用精确分数形式表示，\n$$\nv_{\\min}(s_{0}) \\;=\\; \\frac{61}{96}.\n$$",
            "answer": "$$\\boxed{\\frac{61}{96}}$$"
        },
        {
            "introduction": "除了确保安全性，性能优化（如最小化能耗）对于网络物理系统的设计同样至关重要。本练习  再次使用马尔可夫决策过程（MDP）框架，但目标从验证转向综合：寻找一个最优控制策略。通过应用动态规划和贝尔曼最优性方程（Bellman optimality equation），您将学习如何系统地得出一个能最小化长期累积成本的策略，从而为能效管理等应用提供理论基础。",
            "id": "4250425",
            "problem": "考虑一个离散时间信息物理系统，该系统由一个有限状态、平稳、带折扣的马尔可夫决策过程 (MDP) 表示，用于对数字孪生中的一个能源管理执行器进行建模。状态空间为 $\\{H,L\\}$，其中 $H$ 代表高负载，$L$ 代表低负载。动作空间为 $\\{a_{r},a_{s}\\}$，其中 $a_{r}$ 表示资源分配动作，$a_{s}$ 表示休眠动作。每阶段的能耗成本是归一化且无量纲的。目标是最小化期望折扣累积能耗，折扣因子为 $\\gamma \\in (0,1)$。\n\n基本出发点：在任何平稳策略 $\\pi$ 下，价值函数 $V^{\\pi}(x)$ 定义为每阶段成本的期望折扣总和，\n$$\nV^{\\pi}(x) = \\mathbb{E}_{x}^{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} g(x_{t},a_{t})\\right],\n$$\n并满足特定于策略的贝尔曼方程，\n$$\nV^{\\pi}(x) = g^{\\pi}(x) + \\gamma \\sum_{y} P^{\\pi}(x,y) V^{\\pi}(y),\n$$\n而最优价值函数 $V^{\\star}(x)$ 满足贝尔曼最优性方程，\n$$\nV^{\\star}(x) = \\min_{a}\\left\\{g(x,a) + \\gamma \\sum_{y} P(x,a,y) V^{\\star}(y)\\right\\}。\n$$\n\n假设每阶段的能耗成本是有界的，即存在 $g_{\\max}  0$ 使得对于所有状态 $x$ 和动作 $a$ 都有 $0 \\leq g(x,a) \\leq g_{\\max}$。使用动态规划原理，建立一个关于 $g_{\\max}$ 和 $\\gamma$ 的最优价值函数的显式上界，任何最小化期望折扣能耗的策略都必须满足该上界。\n\n然后，对于以下具体的 MDP 实例，以闭式形式计算最优价值函数 $V^{\\star}$。使用折扣因子 $\\gamma = \\frac{1}{2}$。成本和转移概率如下：\n- 成本：$g(H,a_{r}) = 6$, $g(H,a_{s}) = 10$, $g(L,a_{r}) = 3$, $g(L,a_{s}) = 5$。\n- 转移：\n  - 在 $a_{r}$ 下：\n    - $P(H,a_{r},H) = \\frac{2}{3}$， $P(H,a_{r},L) = \\frac{1}{3}$，\n    - $P(L,a_{r},H) = \\frac{1}{2}$， $P(L,a_{r},L) = \\frac{1}{2}$。\n  - 在 $a_{s}$ 下：\n    - $P(H,a_{s},L) = 1$， $P(L,a_{s},L) = 1$。\n\n你的任务：\n- 证明最优价值函数的上界是由 $g_{\\max}$ 和 $\\gamma$ 的一个函数给出的，该函数通过动态规划论证得到。\n- 为给定的 MDP 实例精确计算最优价值函数 $V^{\\star}(H)$ 和 $V^{\\star}(L)$。\n\n以行矩阵的形式提供最终答案，顺序为 $(H,L)$，并将每个条目表示为精确的有理数（不要四舍五入）。所有能耗成本都已归一化且无量纲，因此最终答案中不需要物理单位。",
            "solution": "该问题将分两部分解决。首先，我们为折扣马尔可夫决策过程 (MDP) 的最优价值函数建立一个通用上界。其次，我们为所提供的具体 MDP 实例计算精确的最优价值函数。\n\n### 第 1 部分：最优价值函数的上界\n\n对于给定的平稳策略 $\\pi$ 和初始状态 $x$，价值函数定义为未来成本的期望折扣总和：\n$$\nV^{\\pi}(x) = \\mathbb{E}_{x}^{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} g(x_{t}, a_{t})\\right]\n$$\n其中 $a_t = \\pi(x_t)$。我们已知每阶段成本是一致有界的，$0 \\leq g(x, a) \\leq g_{\\max}$ 对于所有 $x \\in S$ 和 $a \\in A$。我们可以用这个来界定价值函数。\n\n根据期望的线性性质和给定的成本界限，我们有：\n$$\nV^{\\pi}(x) = \\sum_{t=0}^{\\infty} \\gamma^{t} \\mathbb{E}_{x}^{\\pi}[g(x_{t}, a_{t})] \\leq \\sum_{t=0}^{\\infty} \\gamma^{t} g_{\\max}\n$$\n右边是一个公比为 $\\gamma$ 的几何级数。由于 $\\gamma \\in (0,1)$，该级数收敛：\n$$\ng_{\\max} \\sum_{t=0}^{\\infty} \\gamma^{t} = g_{\\max} \\left(\\frac{1}{1-\\gamma}\\right)\n$$\n因此，对于任何策略 $\\pi$ 和任何状态 $x$，价值函数有上界：\n$$\nV^{\\pi}(x) \\leq \\frac{g_{\\max}}{1-\\gamma}\n$$\n最优价值函数 $V^{\\star}(x)$ 定义为在所有可能策略 $\\pi$ 上 $V^{\\pi}(x)$ 的下确界：\n$$\nV^{\\star}(x) = \\inf_{\\pi} V^{\\pi}(x)\n$$\n由于不等式 $V^{\\pi}(x) \\leq \\frac{g_{\\max}}{1-\\gamma}$ 对所有策略都成立，它也必须对下确界成立。因此，最优价值函数的上界也是同一个量：\n$$\nV^{\\star}(x) \\leq \\frac{g_{\\max}}{1-\\gamma}\n$$\n这就建立了任何最小化期望折扣能耗的策略都必须满足的所需上界。\n\n### 第 2 部分：最优价值函数的计算\n\n我们给定一个具体 MDP，其状态空间为 $S = \\{H, L\\}$，折扣因子为 $\\gamma = \\frac{1}{2}$。最优价值函数的分量，我们记为 $V^{\\star}(H)$ 和 $V^{\\star}(L)$，必须满足贝尔曼最优性方程。\n\n设 $V_H = V^{\\star}(H)$ 和 $V_L = V^{\\star}(L)$。方程组为：\n$$\nV_H = \\min \\left\\{ g(H,a_r) + \\gamma \\left( P(H,a_r,H)V_H + P(H,a_r,L)V_L \\right), \\quad g(H,a_s) + \\gamma \\left( P(H,a_s,H)V_H + P(H,a_s,L)V_L \\right) \\right\\}\n$$\n$$\nV_L = \\min \\left\\{ g(L,a_r) + \\gamma \\left( P(L,a_r,H)V_H + P(L,a_r,L)V_L \\right), \\quad g(L,a_s) + \\gamma \\left( P(L,a_s,H)V_H + P(L,a_s,L)V_L \\right) \\right\\}\n$$\n代入给定值：\n对于状态 $H$：\n动作 $a_r$：$6 + \\frac{1}{2} \\left( \\frac{2}{3} V_H + \\frac{1}{3} V_L \\right) = 6 + \\frac{1}{3} V_H + \\frac{1}{6} V_L$。\n动作 $a_s$：$10 + \\frac{1}{2} \\left( 0 \\cdot V_H + 1 \\cdot V_L \\right) = 10 + \\frac{1}{2} V_L$。\n所以，$V_H = \\min \\left( 6 + \\frac{1}{3} V_H + \\frac{1}{6} V_L, \\quad 10 + \\frac{1}{2} V_L \\right)$。\n\n对于状态 $L$：\n动作 $a_r$：$3 + \\frac{1}{2} \\left( \\frac{1}{2} V_H + \\frac{1}{2} V_L \\right) = 3 + \\frac{1}{4} V_H + \\frac{1}{4} V_L$。\n动作 $a_s$：$5 + \\frac{1}{2} \\left( 0 \\cdot V_H + 1 \\cdot V_L \\right) = 5 + \\frac{1}{2} V_L$。\n所以，$V_L = \\min \\left( 3 + \\frac{1}{4} V_H + \\frac{1}{4} V_L, \\quad 5 + \\frac{1}{2} V_L \\right)$。\n\n这个非线性方程组可以使用策略迭代法求解。我们假设一个最优策略，解相应的线性方程组，然后验证解是否与初始假设一致。我们假设最优策略为 $\\pi^{\\star}(H) = a_r$ 和 $\\pi^{\\star}(L) = a_r$。该策略将贝尔曼方程设定为：\n$$\nV_H = 6 + \\frac{1}{3} V_H + \\frac{1}{6} V_L\n$$\n$$\nV_L = 3 + \\frac{1}{4} V_H + \\frac{1}{4} V_L\n$$\n我们重新整理这些方程，形成一个标准的线性系统：\n$$\n\\left(1 - \\frac{1}{3}\\right) V_H - \\frac{1}{6} V_L = 6 \\quad \\implies \\quad \\frac{2}{3} V_H - \\frac{1}{6} V_L = 6\n$$\n$$\n-\\frac{1}{4} V_H + \\left(1 - \\frac{1}{4}\\right) V_L = 3 \\quad \\implies \\quad -\\frac{1}{4} V_H + \\frac{3}{4} V_L = 3\n$$\n将第一个方程乘以 $6$，第二个方程乘以 $4$ 来简化系统：\n$$\n4 V_H - V_L = 36\n$$\n$$\n-V_H + 3 V_L = 12\n$$\n从第一个方程，我们用 $V_H$ 表示 $V_L$：$V_L = 4 V_H - 36$。\n将此代入第二个方程：\n$$\n-V_H + 3(4 V_H - 36) = 12\n$$\n$$\n-V_H + 12 V_H - 108 = 12\n$$\n$$\n11 V_H = 120 \\quad \\implies \\quad V_H = \\frac{120}{11}\n$$\n现在我们求 $V_L$：\n$$\nV_L = 4 \\left( \\frac{120}{11} \\right) - 36 = \\frac{480}{11} - \\frac{396}{11} = \\frac{84}{11}\n$$\n在策略 $(a_r, a_r)$ 下的解是 $(V_H, V_L) = (\\frac{120}{11}, \\frac{84}{11})$。我们现在必须验证这个解是否满足原始的贝尔曼最优性方程，即我们为每个状态选择的动作是否确实是最优的。\n\n对于状态 $H$，我们必须检查是否 $6 + \\frac{1}{3} V_H + \\frac{1}{6} V_L \\leq 10 + \\frac{1}{2} V_L$。\n根据构造，左侧是 $V_H = \\frac{120}{11}$。\n右侧是 $10 + \\frac{1}{2} V_L = 10 + \\frac{1}{2} \\left(\\frac{84}{11}\\right) = 10 + \\frac{42}{11} = \\frac{110+42}{11} = \\frac{152}{11}$。\n不等式为 $\\frac{120}{11} \\leq \\frac{152}{11}$，这是成立的。因此，$a_r$ 是状态 $H$ 下的最优动作。\n\n对于状态 $L$，我们必须检查是否 $3 + \\frac{1}{4} V_H + \\frac{1}{4} V_L \\leq 5 + \\frac{1}{2} V_L$。\n根据构造，左侧是 $V_L = \\frac{84}{11}$。\n右侧是 $5 + \\frac{1}{2} V_L = 5 + \\frac{1}{2} \\left(\\frac{84}{11}\\right) = 5 + \\frac{42}{11} = \\frac{55+42}{11} = \\frac{97}{11}$。\n不等式为 $\\frac{84}{11} \\leq \\frac{97}{11}$，这是成立的。因此，$a_r$ 是状态 $L$ 下的最优动作。\n\n由于两个条件都满足，我们最初的假设是正确的。最优策略是 $\\pi^{\\star}(H) = a_r$ 和 $\\pi^{\\star}(L) = a_r$，相应的最优价值函数确实是 $V^{\\star}(H) = \\frac{120}{11}$ 和 $V^{\\star}(L) = \\frac{84}{11}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{120}{11}  \\frac{84}{11} \\end{pmatrix}}\n$$"
        }
    ]
}