{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of model-based anomaly detection is the concept of a \"residual\"—the difference between a physical system's measured output and the prediction from its digital twin. This exercise provides a practical foundation by exploring how a small, unmodeled physical disturbance in a DC motor creates a detectable residual. By linearizing the system's nonlinear dynamics around a nominal operating point, you will derive a simple yet powerful model that reveals how anomalies manifest in the residual signal, a fundamental skill in designing sensitive detection systems .",
            "id": "4215454",
            "problem": "A cyber-physical system (CPS) consisting of a Direct Current (DC) motor is monitored by a model-based Digital Twin. The physical motor dynamics include viscous and weak cubic friction, back electromotive force, and load torque. The nominal continuous-time nonlinear state-space model uses the state vector $x = [\\omega\\ i]^{\\top}$, the control input $u$, and the output $y$, with \n$$\n\\dot{\\omega} = \\frac{1}{J}\\left(K_{t} i - B \\omega - B_{\\text{nl}} \\omega^{3} - \\tau_{\\text{load}}\\right), \\quad\n\\dot{i} = \\frac{1}{L}\\left(u - R i - K_{e} \\omega\\right), \\quad\ny = \\omega,\n$$\nwhere $\\omega$ is the angular speed and $i$ is the armature current. The Digital Twin uses the same nominal model and the same commanded input $u$ to produce a predicted output $\\hat{y}$. An anomaly is modeled as a small additive disturbance $d(t)$ in the load torque acting on the physical plant only, so that the plant load torque becomes $\\tau_{\\text{load}} + d(t)$ while the Digital Twin uses the nominal $\\tau_{\\text{load}}$.\n\nThe parameters are $J = 0.01$, $B = 0.005$, $B_{\\text{nl}} = 1 \\times 10^{-7}$, $L = 0.05$, $R = 1$, $K_{t} = 0.5$, and $K_{e} = 0.5$. The nominal operating point is characterized by the steady values $\\omega_{0} = 100$, $i_{0} = 1.6$, $u_{0} = 51.6$, and $\\tau_{\\text{load},0} = 0.2$, which satisfy the equilibrium conditions of the nominal model. The residual $r(t)$ is defined as the measured-minus-predicted output difference $r(t) = y(t) - \\hat{y}(t)$.\n\nStarting from the definitions of equilibrium and first-order Taylor expansion (Jacobian linearization) around the operating point $(\\omega_{0}, i_{0}, u_{0})$, perform the following:\n\n1. Derive the linearized small-signal model of the plant and Digital Twin around $(\\omega_{0}, i_{0}, u_{0})$ and identify the matrices $A$, $B$, and $C$ for the nominal model, as well as the disturbance channel $E$ from the anomaly input $d(t)$ to the state dynamics.\n2. Using the linearized models, derive the residual generator dynamics for small anomalies, i.e., the linearized dynamics of the residual $r(t)$ driven by $d(t)$ when the plant and Digital Twin share the same initial condition at the operating point and the same commanded input $u(t)$.\n3. For a constant step anomaly $d(t) = d_{0}$ with $d_{0} = 0.02$ (units of torque), compute the steady-state residual value $r_{\\text{ss}}$ implied by the linearized dynamics. Express the final numerical answer in radians per second and round your answer to four significant figures.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective. The provided model is a standard representation of a DC motor, and the parameters are physically plausible. The task involves standard techniques from control theory: linearization and residual analysis. We first verify that the given operating point $(\\omega_{0}, i_{0}, u_{0}, \\tau_{\\text{load},0}) = (100, 1.6, 51.6, 0.2)$ is an equilibrium of the nominal model by setting the time derivatives to zero:\nFor $\\dot{\\omega} = 0$:\n$$K_{t} i_{0} - B \\omega_{0} - B_{\\text{nl}} \\omega_{0}^{3} - \\tau_{\\text{load},0} = (0.5)(1.6) - (0.005)(100) - (1 \\times 10^{-7})(100)^{3} - 0.2$$\n$$= 0.8 - 0.5 - (10^{-7})(10^{6}) - 0.2 = 0.8 - 0.5 - 0.1 - 0.2 = 0$$\nFor $\\dot{i} = 0$:\n$$u_{0} - R i_{0} - K_{e} \\omega_{0} = 51.6 - (1)(1.6) - (0.5)(100) = 51.6 - 1.6 - 50 = 0$$\nThe equilibrium conditions are satisfied. The problem is valid and we may proceed with the solution.\n\nThe nonlinear state-space model can be written in the form $\\dot{x} = f(x, u)$ with $x = [\\omega\\ i]^{\\top}$.\n$f(x, u) = \\begin{pmatrix} f_1 \\\\ f_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{J}\\left(K_{t} i - B \\omega - B_{\\text{nl}} \\omega^{3} - \\tau_{\\text{load}}\\right) \\\\ \\frac{1}{L}\\left(u - R i - K_{e} \\omega\\right) \\end{pmatrix}$.\nThe output is $y = h(x) = \\omega$.\n\n**1. Derivation of the Linearized Model**\n\nThe system is linearized around the operating point $(x_0, u_0)$, where $x_0 = [\\omega_0 \\ i_0]^{\\top} = [100 \\ 1.6]^{\\top}$ and $u_0 = 51.6$. The linearized model is given by $\\delta\\dot{x} = A \\delta x + B \\delta u$, where $\\delta x = x - x_0$ and $\\delta u = u - u_0$. The matrices $A$ and $B$ are the Jacobians of $f$ with respect to $x$ and $u$, evaluated at the operating point.\n\nThe Jacobian matrix $A$ is:\n$$A = \\frac{\\partial f}{\\partial x}\\bigg|_{x_0, u_0} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial \\omega} & \\frac{\\partial f_1}{\\partial i} \\\\ \\frac{\\partial f_2}{\\partial \\omega} & \\frac{\\partial f_2}{\\partial i} \\end{pmatrix}_{x_0, u_0}$$\nThe partial derivatives are:\n$\\frac{\\partial f_1}{\\partial \\omega} = \\frac{1}{J}(-B - 3B_{\\text{nl}}\\omega^2)$. Evaluating at $\\omega_0=100$:\n$\\frac{\\partial f_1}{\\partial \\omega}\\bigg|_{\\omega_0} = \\frac{1}{0.01}(-0.005 - 3(1 \\times 10^{-7})(100)^2) = 100(-0.005 - 0.003) = -0.8$.\n$\\frac{\\partial f_1}{\\partial i} = \\frac{K_t}{J} = \\frac{0.5}{0.01} = 50$.\n$\\frac{\\partial f_2}{\\partial \\omega} = -\\frac{K_e}{L} = -\\frac{0.5}{0.05} = -10$.\n$\\frac{\\partial f_2}{\\partial i} = -\\frac{R}{L} = -\\frac{1}{0.05} = -20$.\nThus, the state matrix $A$ is:\n$$A = \\begin{pmatrix} -0.8 & 50 \\\\ -10 & -20 \\end{pmatrix}$$\n\nThe input matrix $B$ is:\n$$B = \\frac{\\partial f}{\\partial u}\\bigg|_{x_0, u_0} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial u} \\\\ \\frac{\\partial f_2}{\\partial u} \\end{pmatrix}_{x_0, u_0} = \\begin{pmatrix} 0 \\\\ \\frac{1}{L} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{0.05} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 20 \\end{pmatrix}$$\n\nThe output equation is $y = \\omega$, which is already linear in the state variables. The output matrix $C$ is:\n$$C = \\frac{\\partial h}{\\partial x}\\bigg|_{x_0} = \\begin{pmatrix} \\frac{\\partial \\omega}{\\partial \\omega} & \\frac{\\partial \\omega}{\\partial i} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$$\n\nThe anomaly $d(t)$ enters the physical plant dynamics as an additive load torque. The plant's $\\dot{\\omega}$ equation is $\\dot{\\omega}_{\\text{plant}} = \\frac{1}{J}(K_{t} i - B \\omega - B_{\\text{nl}} \\omega^{3} - \\tau_{\\text{load},0} - d(t))$. The disturbance channel matrix $E$ maps the disturbance $d(t)$ to the state dynamics.\n$$E = \\frac{\\partial f}{\\partial d}\\bigg|_{d=0} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial d} \\\\ \\frac{\\partial f_2}{\\partial d} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{J} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{0.01} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -100 \\\\ 0 \\end{pmatrix}$$\n\n**2. Derivation of the Residual Generator Dynamics**\n\nLet $\\delta x_p(t)$ be the small-signal state deviation of the physical plant and $\\delta \\hat{x}(t)$ be that of the Digital Twin. The linearized models are:\nPlant: $\\delta \\dot{x}_p(t) = A \\delta x_p(t) + B \\delta u(t) + E d(t)$\nTwin: $\\delta \\dot{\\hat{x}}(t) = A \\delta \\hat{x}(t) + B \\delta u(t)$\nThe small-signal outputs are $\\delta y(t) = C \\delta x_p(t)$ and $\\delta \\hat{y}(t) = C \\delta \\hat{x}(t)$.\n\nThe residual is $r(t) = y(t) - \\hat{y}(t) = (y_0 + \\delta y(t)) - (\\hat{y}_0 + \\delta\\hat{y}(t))$. Since the operating points are the same ($y_0 = \\hat{y}_0$), $r(t) = \\delta y(t) - \\delta \\hat{y}(t) = C(\\delta x_p(t) - \\delta \\hat{x}(t))$.\nLet the state error be $e(t) = \\delta x_p(t) - \\delta \\hat{x}(t)$. The dynamics of the state error are found by subtracting the twin's state equation from the plant's state equation:\n$$\\dot{e}(t) = \\delta \\dot{x}_p(t) - \\delta \\dot{\\hat{x}}(t) = (A \\delta x_p + B \\delta u + E d) - (A \\delta \\hat{x} + B \\delta u)$$\n$$\\dot{e}(t) = A(\\delta x_p - \\delta \\hat{x}) + (B - B)\\delta u + E d = A e(t) + E d(t)$$\nThe residual is then given by $r(t) = C e(t)$.\nThe problem states that the initial conditions are the same, so $e(0) = \\delta x_p(0) - \\delta \\hat{x}(0) = 0$.\nThe residual generator is thus the linear time-invariant system:\n$$ \\dot{e}(t) = A e(t) + E d(t), \\quad e(0)=0 $$\n$$ r(t) = C e(t) $$\n\n**3. Computation of the Steady-State Residual**\n\nFor a constant step anomaly $d(t) = d_0 = 0.02$, the steady-state response is found by setting the time derivative of the state error to zero, $\\dot{e}(t) = 0$, assuming the system matrix $A$ is stable. The eigenvalues of $A$ have negative real parts (characteristic equation $\\lambda^2+20.8\\lambda+516=0$), so the system is stable.\nIn steady state, we have $0 = A e_{\\text{ss}} + E d_0$.\nSolving for the steady-state error $e_{\\text{ss}}$:\n$$e_{\\text{ss}} = -A^{-1}E d_0$$\nThe steady-state residual $r_{\\text{ss}}$ is then:\n$$r_{\\text{ss}} = C e_{\\text{ss}} = -C A^{-1} E d_0$$\nThis term, $-C A^{-1} E$, is the DC gain of the system from input $d$ to output $r$.\nFirst, we compute the inverse of $A$:\n$$\\det(A) = (-0.8)(-20) - (50)(-10) = 16 + 500 = 516$$\n$$A^{-1} = \\frac{1}{\\det(A)}\\begin{pmatrix} -20 & -50 \\\\ 10 & -0.8 \\end{pmatrix} = \\frac{1}{516}\\begin{pmatrix} -20 & -50 \\\\ 10 & -0.8 \\end{pmatrix}$$\nNow we compute the product $C A^{-1} E$:\n$$C A^{-1} E = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\left(\\frac{1}{516}\\begin{pmatrix} -20 & -50 \\\\ 10 & -0.8 \\end{pmatrix}\\right) \\begin{pmatrix} -100 \\\\ 0 \\end{pmatrix}$$\n$$= \\frac{1}{516} \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} (-20)(-100) + (-50)(0) \\\\ (10)(-100) + (-0.8)(0) \\end{pmatrix} = \\frac{1}{516} \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2000 \\\\ -1000 \\end{pmatrix} = \\frac{2000}{516}$$\nThe steady-state residual is:\n$$r_{\\text{ss}} = - \\frac{2000}{516} d_0$$\nSubstituting $d_0 = 0.02$:\n$$r_{\\text{ss}} = - \\frac{2000}{516} \\times 0.02 = - \\frac{40}{516} \\approx -0.0775193798...$$\nRounding to four significant figures, we get:\n$$r_{\\text{ss}} = -0.07752$$\nThe units are radians per second, the same as for $\\omega$.",
            "answer": "$$\\boxed{-0.07752}$$"
        },
        {
            "introduction": "Once a residual is generated, the critical next step is to decide whether its value indicates a genuine anomaly or is simply a product of inherent system noise. This practice moves from generating residuals to statistically interpreting them. You will design a chi-square anomaly detector, a standard and robust method that evaluates the magnitude of the residual vector relative to its expected covariance, allowing you to set a decision threshold based on a statistically meaningful false alarm probability .",
            "id": "4215503",
            "problem": "A digital twin used for anomaly detection in a Cyber-Physical System (CPS) predicts the plant state at discrete time $k$ as $\\hat{x}_{k} \\in \\mathbb{R}^{3}$ with prediction error $e_{k} = x_{k} - \\hat{x}_{k}$. The prediction error is modeled as zero-mean Gaussian with covariance $P_{k} \\in \\mathbb{R}^{3 \\times 3}$. Two physical sensors measure a subset of the state via a linear observation model $y_{k} = C x_{k} + v_{k}$, where $C \\in \\mathbb{R}^{2 \\times 3}$ and $v_{k}$ is zero-mean Gaussian with covariance $R \\in \\mathbb{R}^{2 \\times 2}$, independent of $e_{k}$. The residual is defined as $r_{k} = y_{k} - C \\hat{x}_{k}$. Assume the system is operating nominally (no anomaly), so that the above probabilistic models hold exactly.\n\nYou are given the following numerically specified matrices:\n$$\nP_{k} =\n\\begin{pmatrix}\n0.036 & 0.012 & -0.004 \\\\\n0.012 & 0.081 & \\;\\;0.003 \\\\\n-0.004 & 0.003 & \\;\\;0.25\n\\end{pmatrix},\n\\quad\nC =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix},\n\\quad\nR =\n\\begin{pmatrix}\n0.014 & 0 \\\\\n0 & 0.019\n\\end{pmatrix}.\n$$\n\nTasks:\n1) From first principles of linear Gaussian models and independence, compute the $2 \\times 2$ residual covariance matrix of $r_{k}$ under nominal operation. Provide the exact numeric matrix.\n\n2) Design a chi-square anomaly detector that flags an anomaly at time $k$ when the quadratic form $q_{k} = r_{k}^{\\top} S_{k}^{-1} r_{k}$ exceeds a threshold $\\gamma(\\alpha)$ chosen to achieve a fixed significance level $\\alpha \\in (0,1)$. Derive a closed-form analytic expression for $\\gamma(\\alpha)$ in terms of $\\alpha$.\n\nAnswer specification:\n- Your final boxed answer must be a single closed-form analytic expression for the chi-square threshold $\\gamma(\\alpha)$, in terms of $\\alpha$ only. Do not include any units in the boxed answer.\n- No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective. The task involves standard techniques from linear systems and statistical analysis.\n\n**1. Residual Covariance Matrix Calculation**\n\nThe residual at time $k$ is defined as $r_{k} = y_{k} - C \\hat{x}_{k}$.\nThe measurement model is $y_{k} = C x_{k} + v_{k}$.\nSubstituting the measurement model into the residual definition:\n$$r_{k} = (C x_{k} + v_{k}) - C \\hat{x}_{k} = C(x_{k} - \\hat{x}_{k}) + v_{k}$$\nThe term $x_{k} - \\hat{x}_{k}$ is the state prediction error, which is denoted as $e_k$ in the problem description. Thus:\n$$r_{k} = C e_{k} + v_{k}$$\nThe residual covariance matrix $S_k$ is the expectation of the outer product of the residual with itself:\n$$S_{k} = \\mathbb{E}[r_{k} r_{k}^{\\top}] = \\mathbb{E}[(C e_{k} + v_{k})(C e_{k} + v_{k})^{\\top}]$$\nExpanding the product:\n$$S_{k} = \\mathbb{E}[C e_{k} e_{k}^{\\top} C^{\\top} + C e_{k} v_{k}^{\\top} + v_{k} e_{k}^{\\top} C^{\\top} + v_{k} v_{k}^{\\top}]$$\nBy the linearity of expectation:\n$$S_{k} = C \\mathbb{E}[e_{k} e_{k}^{\\top}] C^{\\top} + C \\mathbb{E}[e_{k} v_{k}^{\\top}] + \\mathbb{E}[v_{k} e_{k}^{\\top}] C^{\\top} + \\mathbb{E}[v_{k} v_{k}^{\\top}]$$\nFrom the problem statement, the state prediction error $e_k$ and the measurement noise $v_k$ are independent. Since both are also zero-mean, their cross-correlation is zero: $\\mathbb{E}[e_{k} v_{k}^{\\top}] = \\mathbb{E}[e_k]\\mathbb{E}[v_k^\\top] = 0$. The cross-terms vanish.\nWe are given the covariances $P_k = \\mathbb{E}[e_k e_k^\\top]$ and $R = \\mathbb{E}[v_k v_k^\\top]$. The expression for the residual covariance simplifies to the well-known formula:\n$$S_{k} = C P_{k} C^{\\top} + R$$\nNow, we substitute the given numerical matrices:\n$$C P_{k} C^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0.036 & 0.012 & -0.004 \\\\ 0.012 & 0.081 & 0.003 \\\\ -0.004 & 0.003 & 0.25 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$$\n$$= \\begin{pmatrix} 0.036 & 0.012 & -0.004 \\\\ 0.012 & 0.081 & 0.003 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0.036 & 0.012 \\\\ 0.012 & 0.081 \\end{pmatrix}$$\nFinally, we add the measurement noise covariance $R$:\n$$S_{k} = \\begin{pmatrix} 0.036 & 0.012 \\\\ 0.012 & 0.081 \\end{pmatrix} + \\begin{pmatrix} 0.014 & 0 \\\\ 0 & 0.019 \\end{pmatrix} = \\begin{pmatrix} 0.050 & 0.012 \\\\ 0.012 & 0.100 \\end{pmatrix}$$\n\n**2. Derivation of the Chi-Square Threshold**\n\nUnder the null hypothesis (nominal operation), the state prediction error $e_k$ and measurement noise $v_k$ are zero-mean Gaussian random vectors. Since the residual $r_k = C e_k + v_k$ is a linear transformation of these vectors, it is also a zero-mean Gaussian random vector with covariance $S_k$. So, $r_k \\sim \\mathcal{N}(0, S_k)$.\n\nThe test statistic is the squared Mahalanobis distance of the residual: $q_{k} = r_{k}^{\\top} S_{k}^{-1} r_{k}$.\nA fundamental result in statistics states that if a random vector $z \\in \\mathbb{R}^m$ follows a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$, then the quadratic form $z^{\\top} \\Sigma^{-1} z$ follows a chi-square distribution with $m$ degrees of freedom, denoted $\\chi^2(m)$.\nIn our case, the residual vector $r_k$ is in $\\mathbb{R}^2$ (since $C \\in \\mathbb{R}^{2 \\times 3}$). Therefore, $m=2$, and the test statistic $q_k$ follows a chi-square distribution with 2 degrees of freedom: $q_k \\sim \\chi^2(2)$.\n\nThe significance level $\\alpha$ represents the desired false alarm rate, which is the probability that the test statistic $q_k$ exceeds the threshold $\\gamma(\\alpha)$ when the system is normal:\n$$\\alpha = P(q_k > \\gamma(\\alpha))$$\nThis is the survival function (or complementary cumulative distribution function) of the $\\chi^2(2)$ distribution, evaluated at $\\gamma(\\alpha)$. The cumulative distribution function (CDF) of a $\\chi^2(2)$ distribution is $F_{\\chi^2(2)}(x) = 1 - e^{-x/2}$ for $x \\ge 0$.\nTherefore,\n$$\\alpha = 1 - F_{\\chi^2(2)}(\\gamma(\\alpha)) = 1 - (1 - e^{-\\gamma(\\alpha)/2}) = e^{-\\gamma(\\alpha)/2}$$\nTo find the threshold $\\gamma(\\alpha)$, we solve for it by taking the natural logarithm of both sides:\n$$\\ln(\\alpha) = -\\frac{\\gamma(\\alpha)}{2}$$\nThis gives the closed-form expression for the threshold:\n$$\\gamma(\\alpha) = -2 \\ln(\\alpha)$$\nThis expression depends only on the desired false alarm probability $\\alpha$, as requested.",
            "answer": "$$ \\boxed{-2 \\ln(\\alpha)} $$"
        },
        {
            "introduction": "Digital twins operate in dynamic environments where uncertainty is not static. As a twin predicts the system's state forward in time, its uncertainty naturally grows due to unmodeled dynamics and process noise. This advanced exercise demonstrates how to formally model this time-varying uncertainty by deriving the covariance propagation equations, a core component of the Kalman filter. Understanding this dynamic evolution is crucial for creating adaptive anomaly detection systems that adjust their sensitivity over time, a concept known as innovation gating .",
            "id": "4215465",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) tracks a linearized discrete-time state with time index $k \\in \\mathbb{N}$. The true state evolves according to $x_{k+1} = A x_{k} + w_{k}$, where $x_{k} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{n \\times n}$ is constant, and the process noise $w_{k}$ is a zero-mean Gaussian random vector with covariance $Q \\in \\mathbb{R}^{n \\times n}$. The DT maintains an unbiased predictor $\\hat{x}_{k \\mid k-1}$ with associated state prediction error $e_{k \\mid k-1} = x_{k} - \\hat{x}_{k \\mid k-1}$ and covariance $P_{k} = \\mathbb{E}[e_{k \\mid k-1} e_{k \\mid k-1}^{\\top}]$. Assume $x_{0}$ is zero-mean with covariance $P_{0}$, and that the sequences $\\{w_{k}\\}$ and $\\{v_{k}\\}$ introduced below are mutually independent and independent of $x_{0}$.\n\nA sensor provides measurements $y_{k} = H x_{k} + v_{k}$ with $H \\in \\mathbb{R}^{m \\times n}$ and measurement noise $v_{k}$ zero-mean Gaussian with covariance $R \\in \\mathbb{R}^{m \\times m}$. The innovation (or residual) used for anomaly detection is $r_{k} = y_{k} - H \\hat{x}_{k \\mid k-1}$. The Digital Twin employs innovation gating via a Mahalanobis distance test to decide whether to flag an anomaly.\n\nStarting only from the definitions of covariance, independence, and the properties of linear transformations of Gaussian random variables, perform the following:\n1) Derive the state prediction covariance recursion that maps $P_{k}$ to $P_{k+1}$ under the given linear dynamics and noise assumptions.\n2) Unroll this recursion to obtain a closed-form expression for $P_{k}$ in terms of $A$, $P_{0}$, $Q$, and $k$, valid for any $k \\in \\mathbb{N}$.\n3) Derive the innovation covariance $S_{k} = \\mathbb{E}[r_{k} r_{k}^{\\top}]$ in terms of $H$, $P_{k}$, and $R$.\n4) Starting from the Gaussianity of $r_{k}$, show that the squared Mahalanobis distance $d_{k}^{2} = r_{k}^{\\top} S_{k}^{-1} r_{k}$ follows a chi-square law with $m$ degrees of freedom, and express a dynamic gating threshold $\\gamma_{k}$ that achieves a fixed false alarm probability $\\alpha \\in (0,1)$ as a symbolic function of $m$ and $\\alpha$. Explain why this threshold is termed “dynamic” in the context of time-varying uncertainty in $P_{k}$ and $S_{k}$.\n\nExpress your final answer as the closed-form expression for $P_{k}$ derived in item $2$. Do not provide any numerical evaluation. No rounding is required. Units are not applicable.",
            "solution": "The problem statement is validated and found to be scientifically grounded, well-posed, and objective. It describes a standard scenario in state estimation and anomaly detection for linear systems. The solution proceeds by deriving the requested quantities based on the provided model and definitions.\n\nWe address the four tasks in order.\n\n**1) Derivation of the State Prediction Covariance Recursion**\n\nThe state prediction error at time $k+1$ is defined as $e_{k+1 \\mid k} = x_{k+1} - \\hat{x}_{k+1 \\mid k}$. The problem describes a Digital Twin that maintains a predictor, but does not specify a measurement update step (e.g., a Kalman filter correction step). The context of innovation gating for anomaly detection implies that we are comparing incoming measurements against a pure model-based prediction. Therefore, the most logical and consistent interpretation is that the state prediction is propagated forward based on the previous prediction, without incorporating a measurement update. This implies that the predictor for time $k+1$ is derived from the predictor at time $k$, $\\hat{x}_{k \\mid k-1}$, propagated through the system dynamics:\n$$\n\\hat{x}_{k+1 \\mid k} = A \\hat{x}_{k \\mid k-1}\n$$\nThe state itself evolves according to $x_{k+1} = A x_{k} + w_{k}$. Substituting these into the definition of $e_{k+1 \\mid k}$:\n$$\ne_{k+1 \\mid k} = (A x_{k} + w_{k}) - (A \\hat{x}_{k \\mid k-1}) = A (x_{k} - \\hat{x}_{k \\mid k-1}) + w_{k}\n$$\nRecognizing the definition of the prediction error at time $k$, $e_{k \\mid k-1} = x_{k} - \\hat{x}_{k \\mid k-1}$, we have:\n$$\ne_{k+1 \\mid k} = A e_{k \\mid k-1} + w_{k}\n$$\nThe state prediction error covariance at time $k+1$, denoted as $P_{k+1}$ in the problem statement (where $P_{k+1} = P_{k+1 \\mid k}$), is given by the expectation $P_{k+1} = \\mathbb{E}[e_{k+1 \\mid k} e_{k+1 \\mid k}^{\\top}]$. Substituting the expression for $e_{k+1 \\mid k}$:\n$$\nP_{k+1} = \\mathbb{E}[(A e_{k \\mid k-1} + w_{k})(A e_{k \\mid k-1} + w_{k})^{\\top}]\n$$\nExpanding the product:\n$$\nP_{k+1} = \\mathbb{E}[A e_{k \\mid k-1} e_{k \\mid k-1}^{\\top} A^{\\top} + A e_{k \\mid k-1} w_{k}^{\\top} + w_{k} e_{k \\mid k-1}^{\\top} A^{\\top} + w_{k} w_{k}^{\\top}]\n$$\nBy linearity of expectation:\n$$\nP_{k+1} = A \\mathbb{E}[e_{k \\mid k-1} e_{k \\mid k-1}^{\\top}] A^{\\top} + A \\mathbb{E}[e_{k \\mid k-1} w_{k}^{\\top}] + \\mathbb{E}[w_{k} e_{k \\mid k-1}^{\\top}] A^{\\top} + \\mathbb{E}[w_{k} w_{k}^{\\top}]\n$$\nFrom the problem definitions, we have $\\mathbb{E}[e_{k \\mid k-1} e_{k \\mid k-1}^{\\top}] = P_{k}$ and $\\mathbb{E}[w_{k} w_{k}^{\\top}] = Q$. The state prediction error $e_{k \\mid k-1}$ is a function of the initial state $x_0$ and the process noise sequence up to time $k-1$, i.e., $\\{w_0, w_1, \\dots, w_{k-1}\\}$. The process noise $w_k$ is independent of this history. Therefore, $e_{k \\mid k-1}$ and $w_k$ are statistically independent. Since $w_k$ is zero-mean ($\\mathbb{E}[w_k] = 0$), they are uncorrelated:\n$$\n\\mathbb{E}[e_{k \\mid k-1} w_{k}^{\\top}] = \\mathbb{E}[e_{k \\mid k-1}] \\mathbb{E}[w_{k}^{\\top}] = 0\n$$\nThe cross-terms vanish. The recursion for the state prediction covariance is:\n$$\nP_{k+1} = A P_{k} A^{\\top} + Q\n$$\n\n**2) Closed-Form Expression for $P_{k}$**\n\nWe unroll the recursion $P_{k+1} = A P_{k} A^{\\top} + Q$ starting from the initial covariance $P_0$. The problem assumes $\\hat{x}_{0 \\mid -1} = \\mathbb{E}[x_0] = 0$, so $e_{0 \\mid -1} = x_0$ and $P_0 = \\mathbb{E}[x_0 x_0^\\top]$.\nFor $k=1$:\n$$\nP_{1} = A P_{0} A^{\\top} + Q\n$$\nFor $k=2$:\n$$\nP_{2} = A P_{1} A^{\\top} + Q = A (A P_{0} A^{\\top} + Q) A^{\\top} + Q = A^{2} P_{0} (A^{\\top})^{2} + A Q A^{\\top} + Q\n$$\nFor $k=3$:\n$$\nP_{3} = A P_{2} A^{\\top} + Q = A (A^{2} P_{0} (A^{\\top})^{2} + A Q A^{\\top} + Q) A^{\\top} + Q = A^{3} P_{0} (A^{\\top})^{3} + A^{2} Q (A^{\\top})^{2} + A Q A^{\\top} + Q\n$$\nBy inspection, a general pattern emerges. The closed-form expression for $P_k$ for any $k \\in \\mathbb{N}$ (assuming $\\mathbb{N}=\\{0, 1, 2, ...\\}$) is:\n$$\nP_{k} = A^{k} P_{0} (A^{\\top})^{k} + \\sum_{i=0}^{k-1} A^{i} Q (A^{\\top})^{i}\n$$\nThis is the solution to the discrete-time Lyapunov equation for $P_k$. For $k=0$, the sum is empty and evaluates to zero, yielding $P_0 = A^0 P_0 (A^\\top)^0 = P_0$, which is consistent.\n\n**3) Derivation of the Innovation Covariance $S_{k}$**\n\nThe innovation is defined as $r_{k} = y_{k} - H \\hat{x}_{k \\mid k-1}$. We are given the measurement model $y_{k} = H x_{k} + v_{k}$. Substituting this into the innovation equation:\n$$\nr_{k} = (H x_{k} + v_{k}) - H \\hat{x}_{k \\mid k-1} = H (x_{k} - \\hat{x}_{k \\mid k-1}) + v_{k}\n$$\nThis simplifies to $r_{k} = H e_{k \\mid k-1} + v_{k}$.\nThe innovation covariance is $S_{k} = \\mathbb{E}[r_{k} r_{k}^{\\top}]$. Substituting the expression for $r_k$:\n$$\nS_{k} = \\mathbb{E}[(H e_{k \\mid k-1} + v_{k})(H e_{k \\mid k-1} + v_{k})^{\\top}]\n$$\nExpanding the product:\n$$\nS_{k} = \\mathbb{E}[H e_{k \\mid k-1} e_{k \\mid k-1}^{\\top} H^{\\top} + H e_{k \\mid k-1} v_{k}^{\\top} + v_{k} e_{k \\mid k-1}^{\\top} H^{\\top} + v_{k} v_{k}^{\\top}]\n$$\nBy linearity of expectation:\n$$\nS_{k} = H \\mathbb{E}[e_{k \\mid k-1} e_{k \\mid k-1}^{\\top}] H^{\\top} + H \\mathbb{E}[e_{k \\mid k-1} v_{k}^{\\top}] + \\mathbb{E}[v_{k} e_{k \\mid k-1}^{\\top}] H^{\\top} + \\mathbb{E}[v_{k} v_{k}^{\\top}]\n$$\nWe have $\\mathbb{E}[e_{k \\mid k-1} e_{k \\mid k-1}^{\\top}] = P_{k}$ and $\\mathbb{E}[v_{k} v_{k}^{\\top}] = R$. The state prediction error $e_{k \\mid k-1}$ depends on $x_0$ and $\\{w_0, \\dots, w_{k-1}\\}$. The measurement noise $v_k$ is, by definition, independent of the initial state $x_0$ and the entire sequence of process noise $\\{w_j\\}$. Thus, $e_{k \\mid k-1}$ and $v_k$ are statistically independent. Since $v_k$ is zero-mean, they are uncorrelated. The cross-terms are zero:\n$$\n\\mathbb{E}[e_{k \\mid k-1} v_{k}^{\\top}] = 0\n$$\nTherefore, the innovation covariance is:\n$$\nS_{k} = H P_{k} H^{\\top} + R\n$$\n\n**4) Mahalanobis Distance Distribution and Gating Threshold**\n\nFirst, we establish the distribution of the innovation $r_k$. The initial state $x_0$, process noise $w_k$, and measurement noise $v_k$ are all specified as Gaussian random vectors. A linear transformation of a Gaussian vector is also Gaussian. The state $x_k$ and the prediction error $e_{k \\mid k-1}$ are generated through a sequence of linear operations on these initial Gaussian variables, and are therefore also Gaussian. The innovation $r_{k} = H e_{k \\mid k-1} + v_{k}$ is a linear combination of the Gaussian vectors $e_{k \\mid k-1}$ and $v_k$, so $r_k$ is also a Gaussian random vector. Its mean is $\\mathbb{E}[r_k] = H \\mathbb{E}[e_{k \\mid k-1}] + \\mathbb{E}[v_k]$. Since the predictor is unbiased ($\\mathbb{E}[\\hat{x}_{k \\mid k-1}] = \\mathbb{E}[x_k]$), the prediction error is zero-mean ($\\mathbb{E}[e_{k \\mid k-1}]=0$). With $\\mathbb{E}[v_k]=0$, we conclude that $\\mathbb{E}[r_k]=0$. Thus, $r_k \\sim \\mathcal{N}(0, S_k)$.\n\nTo analyze the squared Mahalanobis distance $d_{k}^{2} = r_{k}^{\\top} S_{k}^{-1} r_{k}$, we normalize the innovation vector. Let $S_k^{1/2}$ be a matrix square root of $S_k$ (e.g., from Cholesky decomposition), such that $S_k = S_k^{1/2} (S_k^{1/2})^{\\top}$. We define a normalized vector $z_k = (S_k^{1/2})^{-1} r_k$. The vector $z_k$ is a linear transformation of the Gaussian vector $r_k$, so it is also Gaussian with mean $\\mathbb{E}[z_k] = (S_k^{1/2})^{-1} \\mathbb{E}[r_k] = 0$. Its covariance is:\n$$\n\\mathbb{E}[z_k z_k^{\\top}] = \\mathbb{E}[(S_k^{1/2})^{-1} r_k r_k^{\\top} ((S_k^{1/2})^{-1})^{\\top}] = (S_k^{1/2})^{-1} \\mathbb{E}[r_k r_k^{\\top}] ((S_k^{1/2})^{-1})^{\\top} = (S_k^{1/2})^{-1} S_k ((S_k^{1/2})^{\\top})^{-1} = I_m\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix. Thus, $z_k$ is a standard multivariate normal random vector, $z_k \\sim \\mathcal{N}(0, I_m)$, whose $m$ components are independent standard normal variables.\nThe Mahalanobis distance can be rewritten as:\n$$\nd_k^2 = r_{k}^{\\top} S_{k}^{-1} r_{k} = r_{k}^{\\top} (((S_{k}^{1/2})^{\\top})^{-1} (S_{k}^{1/2})^{-1}) r_{k} = ((S_{k}^{1/2})^{-1}r_k)^{\\top} ((S_{k}^{1/2})^{-1}r_k) = z_k^{\\top} z_k\n$$\nThis is the sum of the squares of the $m$ components of $z_k$: $d_k^2 = \\sum_{i=1}^{m} z_{k,i}^2$. By definition, the sum of the squares of $m$ independent standard normal random variables follows a chi-square distribution with $m$ degrees of freedom. Therefore, $d_k^2 \\sim \\chi^2(m)$.\n\nAn anomaly is flagged if $d_k^2 > \\gamma_k$. A false alarm occurs if this condition is met even when the system is operating normally (i.e., under the derived null hypothesis $d_k^2 \\sim \\chi^2(m)$). The false alarm probability $\\alpha$ is given by:\n$$\n\\alpha = P(d_k^2 > \\gamma_k)\n$$\nLet $F_{\\chi^2(m)}$ be the cumulative distribution function (CDF) of the chi-square distribution with $m$ degrees of freedom. Then $P(d_k^2 \\le \\gamma_k) = F_{\\chi^2(m)}(\\gamma_k)$, and thus $\\alpha = 1 - F_{\\chi^2(m)}(\\gamma_k)$. The threshold $\\gamma_k$ is found by inverting the CDF:\n$$\n\\gamma_{k} = (F_{\\chi^2(m)})^{-1}(1 - \\alpha)\n$$\nThis threshold value depends only on the measurement dimension $m$ and the desired false alarm probability $\\alpha$, both of which are constant. Therefore, the numerical value of $\\gamma_k$ is constant for all $k$. The reason the gating process is described as \"dynamic\" is that the test itself, $r_{k}^{\\top} S_{k}^{-1} r_{k} > \\gamma_k$, involves the time-varying innovation covariance matrix $S_k$. The acceptance region for the innovation, defined by the ellipsoid $\\{ r \\in \\mathbb{R}^m \\mid r^{\\top} S_{k}^{-1} r \\le \\gamma_k \\}$, changes its size and orientation at each time step $k$ as the state prediction covariance $P_k$ evolves. The threshold is applied to this dynamically normalized statistic.",
            "answer": "$$\\boxed{A^{k} P_{0} (A^{\\top})^{k} + \\sum_{i=0}^{k-1} A^{i} Q (A^{\\top})^{i}}$$"
        }
    ]
}