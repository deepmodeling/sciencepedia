{
    "hands_on_practices": [
        {
            "introduction": "Understanding a causal graph begins with learning how to read the independence relationships it implies. This exercise challenges you to apply the formal rules of d-separation to a simple chain structure, $A \\rightarrow B \\rightarrow C$, a fundamental building block of more complex models. By connecting this graphical property to statistical independence via the Causal Markov property, you will solidify your understanding of how Directed Acyclic Graphs (DAGs) serve as a rigorous language for expressing causal assumptions .",
            "id": "4207427",
            "problem": "A digital twin of a cyber-physical production line is used for diagnosis and root cause analysis of a vibration anomaly. The twin’s structural causal diagram is a Directed Acyclic Graph with three nodes forming a single chain: actuator command $A$ (upstream motor torque setpoint), internal dynamic state $B$ (shaft angular acceleration), and downstream sensor $C$ (end-effector vibration amplitude), with arrows $A \\rightarrow B \\rightarrow C$. Starting only from the definitions of Directed Acyclic Graph, d-separation, and the Causal Markov property, do the following:\n- Enumerate the set $\\mathcal{S}$ of all distinct, non-trivial conditional independence relations among $\\{A,B,C\\}$ that are implied by d-separation. “Non-trivial” means that neither side of the independence contains the other, the conditioning set does not contain the variables being declared independent, and the statement is not tautological.\n- Let $I(A;C \\mid B)$ denote the conditional mutual information (in nats) of $A$ and $C$ given $B$ under any joint distribution $p(A,B,C)$ that is Markov with respect to the given graph. Compute the exact value of $I(A;C \\mid B)$.\n\nExpress the final answer as a real number. No rounding is required.",
            "solution": "The user-provided problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   **System**: A cyber-physical production line represented by a digital twin.\n-   **Causal Structure**: A Directed Acyclic Graph (DAG) with three nodes: $A$ (actuator command), $B$ (internal dynamic state), and $C$ (downstream sensor).\n-   **Graph Topology**: A single chain structure: $A \\rightarrow B \\rightarrow C$.\n-   **Assumptions**: The solution must start from the definitions of Directed Acyclic Graph (DAG), d-separation, and the Causal Markov property.\n-   **Task 1**: Enumerate the set $\\mathcal{S}$ of all distinct, non-trivial conditional independence relations among $\\{A,B,C\\}$ implied by d-separation.\n-   **Definition of \"Non-trivial\"**:\n    1.  Neither side of the independence contains the other.\n    2.  The conditioning set does not contain the variables being declared independent.\n    3.  The statement is not tautological.\n-   **Task 2**: Compute the exact value of the conditional mutual information $I(A;C \\mid B)$ in nats, for any joint distribution $p(A,B,C)$ that is Markov with respect to the given graph.\n-   **Output requirement for Task 2**: A real number.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Groundedness**: The problem is grounded in the established mathematical formalisms of causal inference (Structural Causal Models, DAGs, d-separation) and information theory (mutual information). These are standard tools used in statistics, computer science, and engineering for analyzing systems like digital twins. The problem is scientifically sound.\n-   **Well-Posedness**: The problem is well-posed. The causal graph is explicitly defined ($A \\rightarrow B \\rightarrow C$). The tasks are clearly stated and rely on standard, unambiguous definitions (d-separation, Causal Markov property, conditional mutual information). The constraints on what constitutes a \"non-trivial\" relation are provided, ensuring a unique answer for the first task. The second task asks for a value that is a direct and necessary consequence of the premises. A unique, stable, and meaningful solution exists.\n-   **Objectivity**: The language is formal, precise, and devoid of any subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. It contains no logical contradictions, missing information, or other fatal flaws. The solution process will now proceed.\n\n### Solution Derivation\n\nThe problem is addressed in two parts as requested.\n\n#### Part 1: Enumeration of Conditional Independence Relations\n\nThe given causal structure is a Directed Acyclic Graph (DAG) represented by the chain $A \\rightarrow B \\rightarrow C$. The conditional independence relations implied by this graph are determined by the criterion of d-separation. Two (sets of) nodes $X$ and $Y$ are d-separated by a set of nodes $Z$ (denoted $X \\perp_g Y \\mid Z$) if every path between a node in $X$ and a node in $Y$ is \"blocked\" by $Z$. A path is blocked if it contains:\n1.  A chain $I \\rightarrow M \\rightarrow J$ or a fork $I \\leftarrow M \\rightarrow J$ such that the middle node $M$ is in the conditioning set $Z$.\n2.  A collider $I \\rightarrow M \\leftarrow J$ such that the middle node $M$ and all of its descendants are **not** in the conditioning set $Z$.\n\nWe must test for independence between all pairs of distinct variables from the set $\\{A, B, C\\}$.\n\n-   **Pair $\\{A, B\\}$**: There is one path between $A$ and $B$, which is the direct edge $A \\rightarrow B$. A direct edge can never be blocked by any conditioning set $Z$ that does not contain $A$ or $B$. Therefore, $A$ and $B$ are never d-separated. $A \\not\\perp_g B \\mid \\emptyset$ and $A \\not\\perp_g B \\mid C$. No independence relation is implied.\n\n-   **Pair $\\{B, C\\}$**: Similar to the previous case, the single path is the direct edge $B \\rightarrow C$. It cannot be blocked by any valid conditioning set. Therefore, $B$ and $C$ are never d-separated. $B \\not\\perp_g C \\mid \\emptyset$ and $B \\not\\perp_g C \\mid A$. No independence relation is implied.\n\n-   **Pair $\\{A, C\\}$**: There is one path between $A$ and $C$: $A \\rightarrow B \\rightarrow C$. This path is a chain with the middle node $B$.\n    -   **Conditioning on the empty set ($\\emptyset$)**: The conditioning set is $Z = \\emptyset$. The middle node $B$ is not in $Z$. According to rule 1, the path is not blocked. Thus, $A$ and $C$ are **not** d-separated by the empty set: $A \\not\\perp_g C \\mid \\emptyset$.\n    -   **Conditioning on node $B$**: The conditioning set is $Z = \\{B\\}$. The path is $A \\rightarrow B \\rightarrow C$. The middle node $B$ is in the conditioning set $Z$. According to rule 1, the path is blocked. Since this is the only path between $A$ and $C$, we conclude that $A$ and $C$ are d-separated by $B$: $A \\perp_g C \\mid B$.\n\nWe have found one independence relation: $A \\perp_g C \\mid B$. Now we check if it is \"non-trivial\" according to the given criteria.\n1.  `Neither side of the independence contains the other`: The sets are $\\{A\\}$ and $\\{C\\}$. This condition is met.\n2.  `The conditioning set does not contain the variables being declared independent`: The conditioning set is $\\{B\\}$, which contains neither $A$ nor $C$. This condition is met.\n3.  `The statement is not tautological`: The statement asserts independence between two distinct variables, $A$ and $C$, which is a non-tautological claim about the system's structure. This condition is met.\n\nThe relation $A \\perp C \\mid B$ is the only non-trivial independence relation implied by the graph. Note that the relation $C \\perp A \\mid B$ is identical. Thus, the set $\\mathcal{S}$ of distinct, non-trivial conditional independence relations contains only one element.\n$\\mathcal{S} = \\{ (A \\perp C \\mid B) \\}$\n\n#### Part 2: Computation of Conditional Mutual Information\n\nThe second task is to compute the value of the conditional mutual information $I(A; C \\mid B)$ in nats. This requires invoking the Causal Markov property.\n\nThe **Causal Markov property** states that for a distribution $p$ that is Markov with respect to a DAG $G$, if two nodes $X$ and $Y$ are d-separated by a set of nodes $Z$ in $G$ ($X \\perp_g Y \\mid Z$), then $X$ and $Y$ are conditionally independent given $Z$ in the distribution $p$ ($X \\perp_p Y \\mid Z$).\n\nFrom Part 1, we established the d-separation $A \\perp_g C \\mid B$. Since the distribution $p(A,B,C)$ is given to be Markov with respect to the graph $A \\rightarrow B \\rightarrow C$, the Causal Markov property implies the statistical conditional independence:\n$$A \\perp C \\mid B$$\nThis statistical independence means that the joint conditional probability of $A$ and $C$ given $B$ factors into the product of the individual conditional probabilities:\n$$p(A, C \\mid B) = p(A \\mid B) p(C \\mid B)$$\n\nThe conditional mutual information $I(A; C \\mid B)$ is defined as the Kullback-Leibler divergence between the joint conditional distribution $p(A,C|B)$ and the product of the marginal conditional distributions $p(A|B)p(C|B)$. In the continuous case, the formula (in nats) is:\n$$ I(A; C \\mid B) = \\int p(a,b,c) \\ln\\left( \\frac{p(a, c \\mid b)}{p(a \\mid b) p(c \\mid b)} \\right) da \\, db \\, dc $$\nAlternatively, using the definition of conditional probability $p(a,b,c) = p(a,c|b)p(b)$:\n$$ I(A; C \\mid B) = \\int p(b) \\left[ \\int p(a,c|b) \\ln\\left( \\frac{p(a, c \\mid b)}{p(a \\mid b) p(c \\mid b)} \\right) da \\, dc \\right] db $$\nBecause of the conditional independence $A \\perp C \\mid B$, we have $p(a, c \\mid b) = p(a \\mid b) p(c \\mid b)$ for all values $a, b, c$. Substituting this into the fraction inside the logarithm gives:\n$$ \\frac{p(a, c \\mid b)}{p(a \\mid b) p(c \\mid b)} = \\frac{p(a \\mid b) p(c \\mid b)}{p(a \\mid b) p(c \\mid b)} = 1 $$\nThe natural logarithm of $1$ is $0$:\n$$ \\ln(1) = 0 $$\nTherefore, the expression for the conditional mutual information becomes:\n$$ I(A; C \\mid B) = \\int p(a,b,c) \\ln(1) da \\, db \\, dc = \\int p(a,b,c) \\cdot 0 \\, da \\, db \\, dc = 0 $$\nThis result holds for any distribution $p(A,B,C)$ that is Markov with respect to the given graph, regardless of whether the variables are discrete or continuous, or what the specific functional relationships are. The conditional independence itself is equivalent to zero conditional mutual information.\n\nThe exact value of $I(A; C \\mid B)$ is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "A central challenge in root cause analysis is separating correlation from causation, especially when a third variable—a confounder—influences both the potential cause and the effect. This practice problem provides a concrete scenario where you must compute the true causal effect of an actuator on system failure by adjusting for a known environmental confounder. By applying the backdoor adjustment formula, you will learn to distinguish the observational probability $P(Y=1 \\mid X=1)$ from the interventional probability $P(Y=1 \\mid do(X=1))$, a crucial skill for accurate diagnosis .",
            "id": "4207465",
            "problem": "A digital twin of a smart pump embedded in a Cyber-Physical System (CPS) monitors a binary actuator setting $X \\in \\{0,1\\}$ and a binary failure indicator $Y \\in \\{0,1\\}$. The ambient condition $Z$ takes three discrete levels $Z \\in \\{z_0, z_1, z_2\\}$ representing environmental regimes that influence both the chosen actuator setting $X$ and the likelihood of failure $Y$, making $Z$ a confounder of the $X \\rightarrow Y$ relationship. The causal structure is such that $Z$ has directed arrows into both $X$ and $Y$, and $X$ has a directed arrow into $Y$. Assume that $Z$ satisfies the backdoor criterion relative to the causal effect of $X$ on $Y$, and that the intervention $do(X=1)$ corresponds to setting $X$ to $1$ while leaving the mechanism generating $Z$ unchanged.\n\nThe observational model for the conditional failure probability at each ambient condition level is specified by a logistic link:\n$$\nP(Y=1 \\mid X=x, Z=z_k) \\;=\\; \\sigma\\!\\left(\\alpha_k + \\beta_k x\\right),\n$$\nwhere $\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$, with parameters\n$$\n\\alpha_0 = -2, \\quad \\beta_0 = 0.5; \\qquad \\alpha_1 = -1.5, \\quad \\beta_1 = 0.8; \\qquad \\alpha_2 = -1.0, \\quad \\beta_2 = 0.2.\n$$\nThe ambient condition distribution is\n$$\nP(Z=z_0) = 0.2, \\qquad P(Z=z_1) = 0.5, \\qquad P(Z=z_2) = 0.3.\n$$\n\nUsing foundational definitions of intervention and the backdoor criterion in causal inference, derive from first principles an expression for $P(Y=1 \\mid do(X=1))$ and evaluate it numerically under the given models. Round your final probability to four significant figures and express it as a decimal.",
            "solution": "The problem requires the calculation of an interventional probability, $P(Y=1 \\mid do(X=1))$, which represents the probability of system failure ($Y=1$) if the actuator setting is forced to be $X=1$. This quantity is distinct from the observational conditional probability $P(Y=1 \\mid X=1)$, as the latter can be influenced by the confounding variable $Z$, the ambient condition. The problem states that the causal structure is given by $Z \\rightarrow X$ and $Z \\rightarrow Y$ and $X \\rightarrow Y$, and that $Z$ satisfies the backdoor criterion relative to the causal effect of $X$ on $Y$. This allows us to compute the interventional probability from observational quantities using the backdoor adjustment formula.\n\nWe derive this formula from first principles. The intervention $do(X=x)$ corresponds to modifying the underlying causal system by removing all incoming arrows to $X$ and setting $X$ to the constant value $x$. The probability distribution in this modified system is denoted by $P_{do(X=x)}(\\cdot)$. The quantity of interest is $P(Y=1 \\mid do(X=1))$, which is defined as $P_{do(X=1)}(Y=1)$.\n\nTo calculate this, we can marginalize over the confounding variable $Z$:\n$$ P(Y=y \\mid do(X=x)) = \\sum_{z} P_{do(X=x)}(Y=y, Z=z) $$\nBy the chain rule of probability, this becomes:\n$$ P(Y=y \\mid do(X=x)) = \\sum_{z} P_{do(X=x)}(Y=y \\mid Z=z) P_{do(X=x)}(Z=z) $$\nThe intervention on $X$ does not affect the causal mechanism that generates $Z$, since $Z$ is not a descendant of $X$ in the causal graph. Therefore, the distribution of $Z$ remains unchanged:\n$$ P_{do(X=x)}(Z=z) = P(Z=z) $$\nFurthermore, the intervention $do(X=x)$ sets the value of $X$ to $x$. The mechanism determining $Y$ from its direct causes, $X$ and $Z$, remains intact. Therefore, the conditional probability of $Y$ given $X$ and $Z$ is the same in the interventional and observational regimes:\n$$ P_{do(X=x)}(Y=y \\mid Z=z) = P(Y=y \\mid X=x, Z=z) $$\nSubstituting these two equivalences back into the summation yields the backdoor adjustment formula:\n$$ P(Y=y \\mid do(X=x)) = \\sum_{z} P(Y=y \\mid X=x, Z=z) P(Z=z) $$\nThis formula allows the estimation of the causal effect of $X$ on $Y$ by adjusting for the confounding effects of $Z$.\n\nFor this specific problem, we are asked to find $P(Y=1 \\mid do(X=1))$. We apply the formula with $y=1$ and $x=1$, summing over the three discrete levels of $Z \\in \\{z_0, z_1, z_2\\}$:\n$$ P(Y=1 \\mid do(X=1)) = \\sum_{k=0}^{2} P(Y=1 \\mid X=1, Z=z_k) P(Z=z_k) $$\nExpanding the sum gives:\n\\begin{align*} P(Y=1 \\mid do(X=1)) = & P(Y=1 \\mid X=1, Z=z_0) P(Z=z_0) \\\\ & + P(Y=1 \\mid X=1, Z=z_1) P(Z=z_1) \\\\ & + P(Y=1 \\mid X=1, Z=z_2) P(Z=z_2) \\end{align*}\nThe problem provides the necessary model components:\nThe conditional probability model is $P(Y=1 \\mid X=x, Z=z_k) = \\sigma(\\alpha_k + \\beta_k x)$, where $\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$.\nThe parameters are:\n- For $z_0$: $\\alpha_0 = -2$, $\\beta_0 = 0.5$\n- For $z_1$: $\\alpha_1 = -1.5$, $\\beta_1 = 0.8$\n- For $z_2$: $\\alpha_2 = -1.0$, $\\beta_2 = 0.2$\nThe distribution of $Z$ is:\n- $P(Z=z_0) = 0.2$\n- $P(Z=z_1) = 0.5$\n- $P(Z=z_2) = 0.3$\n\nFirst, we compute the conditional probabilities for $X=1$:\n\\begin{itemize}\n    \\item For $Z=z_0$: $P(Y=1 \\mid X=1, Z=z_0) = \\sigma(\\alpha_0 + \\beta_0 \\cdot 1) = \\sigma(-2 + 0.5) = \\sigma(-1.5)$\n    \\item For $Z=z_1$: $P(Y=1 \\mid X=1, Z=z_1) = \\sigma(\\alpha_1 + \\beta_1 \\cdot 1) = \\sigma(-1.5 + 0.8) = \\sigma(-0.7)$\n    \\item For $Z=z_2$: $P(Y=1 \\mid X=1, Z=z_2) = \\sigma(\\alpha_2 + \\beta_2 \\cdot 1) = \\sigma(-1.0 + 0.2) = \\sigma(-0.8)$\n\\end{itemize}\nThe sigmoid function values are:\n\\begin{itemize}\n    \\item $\\sigma(-1.5) = \\frac{1}{1 + \\exp(1.5)}$\n    \\item $\\sigma(-0.7) = \\frac{1}{1 + \\exp(0.7)}$\n    \\item $\\sigma(-0.8) = \\frac{1}{1 + \\exp(0.8)}$\n\\end{itemize}\nNow, we substitute these expressions and the probabilities of $Z$ into the adjustment formula:\n$$ P(Y=1 \\mid do(X=1)) = \\sigma(-1.5) P(Z=z_0) + \\sigma(-0.7) P(Z=z_1) + \\sigma(-0.8) P(Z=z_2) $$\n$$ P(Y=1 \\mid do(X=1)) = \\left(\\frac{1}{1 + \\exp(1.5)}\\right) (0.2) + \\left(\\frac{1}{1 + \\exp(0.7)}\\right) (0.5) + \\left(\\frac{1}{1 + \\exp(0.8)}\\right) (0.3) $$\nWe proceed with the numerical evaluation:\n\\begin{itemize}\n    \\item $\\exp(1.5) \\approx 4.481689$, so $\\sigma(-1.5) \\approx \\frac{1}{1 + 4.481689} \\approx 0.1824255$\n    \\item $\\exp(0.7) \\approx 2.013753$, so $\\sigma(-0.7) \\approx \\frac{1}{1 + 2.013753} \\approx 0.3318122$\n    \\item $\\exp(0.8) \\approx 2.225541$, so $\\sigma(-0.8) \\approx \\frac{1}{1 + 2.225541} \\approx 0.3099904$\n\\end{itemize}\nSubstituting these values:\n$$ P(Y=1 \\mid do(X=1)) \\approx (0.1824255)(0.2) + (0.3318122)(0.5) + (0.3099904)(0.3) $$\n$$ P(Y=1 \\mid do(X=1)) \\approx 0.0364851 + 0.1659061 + 0.09299712 $$\n$$ P(Y=1 \\mid do(X=1)) \\approx 0.29538832 $$\nRounding the final result to four significant figures as requested gives $0.2954$.",
            "answer": "$$ \\boxed{0.2954} $$"
        },
        {
            "introduction": "In dynamic systems, it is tempting to interpret time-lagged correlations as causal relationships, a notion often associated with Granger causality. However, as this problem demonstrates, a strong predictive link does not necessarily imply a true causal one, especially in the presence of unobserved confounding processes. By deriving both the Granger-predictive coefficient and the structural causal effect from a `do`-intervention, you will uncover a scenario where prediction is possible but intervention is futile, a vital distinction for effective root cause analysis in cyber-physical systems .",
            "id": "4207416",
            "problem": "A digital twin of a Cyber-Physical System (CPS) monitors two observable signals: a control-side measurement $X_t$ and a performance-side measurement $Y_t$. Both are influenced by an unobserved ambient disturbance $U_t$ that evolves as a linear AutoRegressive (AR) process. Consider the following stationary linear dynamical model and noise assumptions:\n- The latent disturbance evolves as $U_t = \\rho U_{t-1} + \\nu_t$, where $\\nu_t$ is independent and identically distributed, zero-mean, Gaussian with variance $\\sigma_{\\nu}^{2}$, and $|\\rho| &lt; 1$.\n- The observed signals are $X_t = \\alpha U_t + \\epsilon_t$ and $Y_t = \\beta U_t + \\eta_t$, where $\\epsilon_t$ and $\\eta_t$ are independent, zero-mean, Gaussian with variances $\\sigma_{X}^{2}$ and $\\sigma_{Y}^{2}$, respectively. All innovations $\\nu_t$, $\\epsilon_t$, and $\\eta_t$ are mutually independent across time and among themselves. The system is stationary.\n\nA diagnostic team applies Granger causality testing to assess whether $X$ Granger-causes $Y$ by comparing the mean-square prediction error of $Y_t$ from its own past with and without $X$ lags. Under linear Gaussian stationarity, this reduces to estimating the linear least-squares predictor of $Y_t$ from $(Y_{t-1}, X_{t-1})$ and inspecting the coefficient multiplying $X_{t-1}$; denote this coefficient by $\\phi_X$. Separately, for root-cause analysis in the Structural Causal Model (SCM) sense, the team studies the interventional response using the do-operator $do(\\cdot)$ and defines the average causal effect slope as $\\frac{d}{dx}\\mathbb{E}[Y_t \\mid do(X_{t-1}=x)]$.\n\nStarting only from the model and stationarity assumptions above and fundamental definitions of linear least-squares prediction and the do-operator, derive:\n1. The exact closed-form expression for the Granger-predictive regression coefficient $\\phi_X$ multiplying $X_{t-1}$ in the best linear predictor of $Y_t$ from $(Y_{t-1}, X_{t-1})$, expressed in terms of $\\alpha$, $\\beta$, $\\rho$, $\\sigma_{\\nu}^{2}$, $\\sigma_{X}^{2}$, and $\\sigma_{Y}^{2}$.\n2. The exact value of the interventional average causal effect slope $\\frac{d}{dx}\\mathbb{E}[Y_t \\mid do(X_{t-1}=x)]$.\n\nReport your final answer as a two-entry row matrix containing first $\\phi_X$ and then the interventional slope. No numerical rounding is required, and no units are needed.",
            "solution": "The problem requires the derivation of two distinct quantities from a given stationary linear dynamical model: first, a predictive coefficient from a Granger causality analysis, and second, a causal effect slope from an interventional analysis. We will address each part in sequence.\n\nThe model is defined by the following equations:\nThe latent disturbance: $U_t = \\rho U_{t-1} + \\nu_t$, with $|\\rho| < 1$.\nThe observed signals: $X_t = \\alpha U_t + \\epsilon_t$ and $Y_t = \\beta U_t + \\eta_t$.\nThe noise terms $\\nu_t$, $\\epsilon_t$, $\\eta_t$ are mutually independent, zero-mean Gaussian processes with variances $\\sigma_{\\nu}^{2}$, $\\sigma_{X}^{2}$, and $\\sigma_{Y}^{2}$, respectively. All processes are stationary and have zero mean.\n\nFirst, we determine the variance of the latent process $U_t$. From the stationarity condition, $\\text{Var}(U_t) = \\text{Var}(U_{t-1})$. Let this be denoted by $\\sigma_U^2$.\nFrom the equation for $U_t$, we have:\n$$\n\\text{Var}(U_t) = \\text{Var}(\\rho U_{t-1} + \\nu_t)\n$$\nSince $U_{t-1}$ is determined by past values of $\\nu$ (i.e., $\\nu_{t-1}, \\nu_{t-2}, \\dots$), it is independent of $\\nu_t$. Thus:\n$$\n\\sigma_U^2 = \\rho^2 \\text{Var}(U_{t-1}) + \\text{Var}(\\nu_t) = \\rho^2 \\sigma_U^2 + \\sigma_{\\nu}^2\n$$\nSolving for $\\sigma_U^2$:\n$$\n\\sigma_U^2(1-\\rho^2) = \\sigma_{\\nu}^2 \\implies \\sigma_U^2 = \\frac{\\sigma_{\\nu}^2}{1-\\rho^2}\n$$\n\n**Part 1: Derivation of the Granger-predictive coefficient $\\phi_X$**\n\nWe seek the coefficient $\\phi_X$ in the best linear predictor of $Y_t$ from the variables $Y_{t-1}$ and $X_{t-1}$. The linear model is $\\hat{Y}_t = \\mathbb{E}[Y_t | Y_{t-1}, X_{t-1}] = \\phi_Y Y_{t-1} + \\phi_X X_{t-1}$. The coefficients $(\\phi_Y, \\phi_X)$ are given by the solution to the normal equations:\n$$\n\\begin{pmatrix} \\phi_Y \\\\ \\phi_X \\end{pmatrix} = \\begin{pmatrix} \\text{Var}(Y_{t-1}) & \\text{Cov}(Y_{t-1}, X_{t-1}) \\\\ \\text{Cov}(X_{t-1}, Y_{t-1}) & \\text{Var}(X_{t-1}) \\end{pmatrix}^{-1} \\begin{pmatrix} \\text{Cov}(Y_{t-1}, Y_t) \\\\ \\text{Cov}(X_{t-1}, Y_t) \\end{pmatrix}\n$$\nWe need to compute the variances and covariances involved. Since all variables are zero-mean, $\\text{Cov}(A, B) = \\mathbb{E}[AB]$.\n\n1.  **Variances of the predictors at time $t-1$**:\n    $\\text{Var}(X_{t-1}) = \\mathbb{E}[X_{t-1}^2] = \\mathbb{E}[(\\alpha U_{t-1} + \\epsilon_{t-1})^2] = \\alpha^2 \\mathbb{E}[U_{t-1}^2] + \\mathbb{E}[\\epsilon_{t-1}^2] = \\alpha^2 \\sigma_U^2 + \\sigma_X^2$.\n    $\\text{Var}(Y_{t-1}) = \\mathbb{E}[Y_{t-1}^2] = \\mathbb{E}[(\\beta U_{t-1} + \\eta_{t-1})^2] = \\beta^2 \\mathbb{E}[U_{t-1}^2] + \\mathbb{E}[\\eta_{t-1}^2] = \\beta^2 \\sigma_U^2 + \\sigma_Y^2$.\n\n2.  **Covariance of the predictors at time $t-1$**:\n    $\\text{Cov}(X_{t-1}, Y_{t-1}) = \\mathbb{E}[X_{t-1}Y_{t-1}] = \\mathbb{E}[(\\alpha U_{t-1} + \\epsilon_{t-1})(\\beta U_{t-1} + \\eta_{t-1})]$.\n    Due to mutual independence of $U_{t-1}$, $\\epsilon_{t-1}$, and $\\eta_{t-1}$, this simplifies to:\n    $\\text{Cov}(X_{t-1}, Y_{t-1}) = \\alpha \\beta \\mathbb{E}[U_{t-1}^2] = \\alpha \\beta \\sigma_U^2$.\n\n3.  **Covariances between predictors and the target $Y_t$**:\n    $\\text{Cov}(X_{t-1}, Y_t) = \\mathbb{E}[X_{t-1}Y_t] = \\mathbb{E}[(\\alpha U_{t-1} + \\epsilon_{t-1})(\\beta U_t + \\eta_t)]$.\n    Substitute $U_t = \\rho U_{t-1} + \\nu_t$:\n    $= \\mathbb{E}[(\\alpha U_{t-1} + \\epsilon_{t-1})(\\beta(\\rho U_{t-1} + \\nu_t) + \\eta_t)]$.\n    Expanding and using the independence of $\\epsilon_{t-1}$, $\\eta_t$, $\\nu_t$ from each other and from $U_{t-1}$:\n    $= \\mathbb{E}[\\alpha \\beta \\rho U_{t-1}^2] = \\alpha \\beta \\rho \\sigma_U^2$.\n\n    $\\text{Cov}(Y_{t-1}, Y_t) = \\mathbb{E}[Y_{t-1}Y_t] = \\mathbb{E}[(\\beta U_{t-1} + \\eta_{t-1})(\\beta U_t + \\eta_t)]$.\n    Similarly, this simplifies to:\n    $= \\mathbb{E}[\\beta^2 \\rho U_{t-1}^2] = \\beta^2 \\rho \\sigma_U^2$.\n\nNow, we solve for $\\phi_X$ using Cramer's rule on the matrix equation:\n$$\n\\phi_X = \\frac{\\det \\begin{pmatrix} \\text{Var}(Y_{t-1}) & \\text{Cov}(Y_{t-1}, Y_t) \\\\ \\text{Cov}(X_{t-1}, Y_{t-1}) & \\text{Cov}(X_{t-1}, Y_t) \\end{pmatrix}}{\\det \\begin{pmatrix} \\text{Var}(Y_{t-1}) & \\text{Cov}(X_{t-1}, Y_{t-1}) \\\\ \\text{Cov}(X_{t-1}, Y_{t-1}) & \\text{Var}(X_{t-1}) \\end{pmatrix}}\n$$\nLet's compute the numerator and denominator determinants.\nNumerator $N$:\n$N = \\text{Var}(Y_{t-1})\\text{Cov}(X_{t-1}, Y_t) - \\text{Cov}(Y_{t-1}, Y_t)\\text{Cov}(X_{t-1}, Y_{t-1})$\n$N = (\\beta^2 \\sigma_U^2 + \\sigma_Y^2)(\\alpha \\beta \\rho \\sigma_U^2) - (\\beta^2 \\rho \\sigma_U^2)(\\alpha \\beta \\sigma_U^2)$\n$N = \\alpha \\beta^3 \\rho (\\sigma_U^2)^2 + \\alpha \\beta \\rho \\sigma_U^2 \\sigma_Y^2 - \\alpha \\beta^3 \\rho (\\sigma_U^2)^2 = \\alpha \\beta \\rho \\sigma_U^2 \\sigma_Y^2$.\n\nDenominator $D$:\n$D = \\text{Var}(Y_{t-1})\\text{Var}(X_{t-1}) - (\\text{Cov}(X_{t-1}, Y_{t-1}))^2$\n$D = (\\beta^2 \\sigma_U^2 + \\sigma_Y^2)(\\alpha^2 \\sigma_U^2 + \\sigma_X^2) - (\\alpha \\beta \\sigma_U^2)^2$\n$D = \\alpha^2 \\beta^2 (\\sigma_U^2)^2 + \\beta^2 \\sigma_U^2 \\sigma_X^2 + \\alpha^2 \\sigma_U^2 \\sigma_Y^2 + \\sigma_X^2 \\sigma_Y^2 - \\alpha^2 \\beta^2 (\\sigma_U^2)^2$\n$D = \\sigma_U^2(\\beta^2 \\sigma_X^2 + \\alpha^2 \\sigma_Y^2) + \\sigma_X^2 \\sigma_Y^2$.\n\nSo, $\\phi_X = \\frac{N}{D} = \\frac{\\alpha \\beta \\rho \\sigma_U^2 \\sigma_Y^2}{\\sigma_U^2(\\beta^2 \\sigma_X^2 + \\alpha^2 \\sigma_Y^2) + \\sigma_X^2 \\sigma_Y^2}$.\nSubstituting $\\sigma_U^2 = \\frac{\\sigma_{\\nu}^2}{1-\\rho^2}$:\n$$\n\\phi_X = \\frac{\\alpha \\beta \\rho \\left(\\frac{\\sigma_{\\nu}^2}{1-\\rho^2}\\right) \\sigma_Y^2}{\\left(\\frac{\\sigma_{\\nu}^2}{1-\\rho^2}\\right)(\\beta^2 \\sigma_X^2 + \\alpha^2 \\sigma_Y^2) + \\sigma_X^2 \\sigma_Y^2}\n$$\nMultiplying the numerator and denominator by $(1-\\rho^2)$ to simplify:\n$$\n\\phi_X = \\frac{\\alpha \\beta \\rho \\sigma_{\\nu}^2 \\sigma_Y^2}{\\sigma_{\\nu}^2(\\beta^2 \\sigma_X^2 + \\alpha^2 \\sigma_Y^2) + (1-\\rho^2) \\sigma_X^2 \\sigma_Y^2}\n$$\nThis is the closed-form expression for the Granger-predictive coefficient. It is non-zero if $\\alpha$, $\\beta$, and $\\rho$ are all non-zero, indicating that $X_{t-1}$ has predictive power for $Y_t$ because it provides information about the latent state $U_{t-1}$, which influences the future state $U_t$ and thus $Y_t$.\n\n**Part 2: Derivation of the interventional average causal effect slope**\n\nWe need to compute the quantity $\\frac{d}{dx}\\mathbb{E}[Y_t \\mid do(X_{t-1}=x)]$. The do-operator, $do(V=v)$, represents an intervention that sets the variable $V$ to a value $v$ by breaking all causal arrows pointing into $V$.\n\nThe original Structural Causal Model (SCM) includes the equation $X_{t-1} = \\alpha U_{t-1} + \\epsilon_{t-1}$. The intervention $do(X_{t-1}=x)$ replaces this equation with $X_{t-1}=x$. This severs the causal link $U_{t-1} \\to X_{t-1}$.\nThe modified system of equations governing the variables under this intervention is:\n1.  $U_t = \\rho U_{t-1} + \\nu_t$\n2.  $X_{t-1} = x$\n3.  $Y_t = \\beta U_t + \\eta_t$\n\nThe other variables, particularly $U_{t-1}$, remain governed by their original causal mechanisms. In the causal graph, $U_{t-1}$ is a parent of $X_{t-1}$. Intervening on a variable does not affect its parents or other ancestors. Therefore, the distribution of $U_{t-1}$ is unaffected by the intervention on $X_{t-1}$. It remains described by its unconditional stationary distribution.\n\nWe now compute the expectation of $Y_t$ in this modified system:\n$$\n\\mathbb{E}[Y_t \\mid do(X_{t-1}=x)] = \\mathbb{E}[\\beta U_t + \\eta_t \\mid do(X_{t-1}=x)]\n$$\nUsing the linearity of expectation:\n$$\n= \\beta \\mathbb{E}[U_t \\mid do(X_{t-1}=x)] + \\mathbb{E}[\\eta_t \\mid do(X_{t-1}=x)]\n$$\nThe noise term $\\eta_t$ is exogenous and independent of all other variables and any intervention at a prior time step. Thus, $\\mathbb{E}[\\eta_t \\mid do(X_{t-1}=x)] = \\mathbb{E}[\\eta_t] = 0$.\nNow consider $\\mathbb{E}[U_t \\mid do(X_{t-1}=x)]$:\n$$\n\\mathbb{E}[U_t \\mid do(X_{t-1}=x)] = \\mathbb{E}[\\rho U_{t-1} + \\nu_t \\mid do(X_{t-1}=x)]\n$$\n$$\n= \\rho \\mathbb{E}[U_{t-1} \\mid do(X_{t-1}=x)] + \\mathbb{E}[\\nu_t \\mid do(X_{t-1}=x)]\n$$\nAs with $\\eta_t$, $\\nu_t$ is exogenous and $\\mathbb{E}[\\nu_t \\mid do(X_{t-1}=x)] = \\mathbb{E}[\\nu_t] = 0$.\nAs argued before, the intervention on $X_{t-1}$ does not affect its parent $U_{t-1}$. Thus, the expectation of $U_{t-1}$ is its unconditional mean:\n$$\n\\mathbb{E}[U_{t-1} \\mid do(X_{t-1}=x)] = \\mathbb{E}[U_{t-1}]\n$$\nSince the process is stationary and zero-mean, $\\mathbb{E}[U_{t-1}]=0$.\nPutting all pieces together:\n$$\n\\mathbb{E}[Y_t \\mid do(X_{t-1}=x)] = \\beta (\\rho \\cdot 0 + 0) + 0 = 0\n$$\nThe expected value of $Y_t$ under the intervention is $0$, regardless of the value $x$ to which $X_{t-1}$ is set. The interventional expectation is a constant function of $x$.\n\nThe average causal effect slope is the derivative of this expectation with respect to $x$:\n$$\n\\frac{d}{dx}\\mathbb{E}[Y_t \\mid do(X_{t-1}=x)] = \\frac{d}{dx}(0) = 0\n$$\nThe causal effect is zero because there is no direct or indirect causal path from $X_{t-1}$ to $Y_t$. The statistical association detected by Granger causality is due entirely to the confounding variable $U_{t-1}$, which is a common cause of both $X_{t-1}$ and the future value $Y_t$ (through $U_t$).\n\nThe final answers are the expression for $\\phi_X$ and the value $0$ for the interventional slope.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\alpha \\beta \\rho \\sigma_{\\nu}^2 \\sigma_Y^2}{\\sigma_{\\nu}^2(\\beta^2 \\sigma_X^2 + \\alpha^2 \\sigma_Y^2) + (1-\\rho^2) \\sigma_X^2 \\sigma_Y^2} & 0 \\end{pmatrix}}\n$$"
        }
    ]
}