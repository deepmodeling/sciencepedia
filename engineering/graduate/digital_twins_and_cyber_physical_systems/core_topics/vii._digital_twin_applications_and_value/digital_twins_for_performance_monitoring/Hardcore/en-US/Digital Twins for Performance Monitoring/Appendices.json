{
    "hands_on_practices": [
        {
            "introduction": "This practice lays the theoretical groundwork for building data-driven digital twins. Before we can accurately monitor performance, we need a robust model, and this exercise explores how to estimate its parameters from sensor data using the Maximum Likelihood principle. More importantly, it challenges us to determine the fundamental limit on the precision of any performance metric derived from this model by deriving the Cramér-Rao Lower Bound (CRLB), a cornerstone of statistical estimation theory. ",
            "id": "4215921",
            "problem": "A cyber-physical production cell is monitored by a digital twin whose measurement model for a vector of sensor outputs is given by $y_k = h_{\\theta}(x_k, u_k) + v_k$, where $y_k \\in \\mathbb{R}^{m}$ are the measured outputs at time index $k$, $x_k \\in \\mathbb{R}^{n}$ are the known states, $u_k \\in \\mathbb{R}^{p}$ are the known control inputs, and $h_{\\theta} : \\mathbb{R}^{n} \\times \\mathbb{R}^{p} \\to \\mathbb{R}^{m}$ is a differentiable output map parameterized by $\\theta \\in \\mathbb{R}^{d}$. The noise sequence $v_k$ is independent across $k$, with $v_k \\sim \\mathcal{N}(0, R)$ for a known positive-definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$, and $R$ does not depend on $\\theta$. The dataset $\\{(x_k, u_k, y_k)\\}_{k=1}^{K}$ is available.\n\nThe production cell’s performance is summarized by a scalar output defined at a particular operating condition $(x_{\\mathrm{op}}, u_{\\mathrm{op}})$ as $g(\\theta) = a^{\\top} h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})$, where $a \\in \\mathbb{R}^{m}$ is a known weighting vector. Assume standard regularity conditions hold for likelihood-based inference, and suppose that an unbiased estimator of $g(\\theta)$ is constructed via parameter estimation followed by evaluation at $(x_{\\mathrm{op}}, u_{\\mathrm{op}})$.\n\nStarting from the fundamental definition of the likelihood for independent Gaussian measurement noise and the definition of the Maximum Likelihood Estimator (MLE), derive the form of the log-likelihood and the corresponding estimator for $\\theta$. Then, using the definition of the Fisher Information Matrix (FIM) and the Cramér–Rao Lower Bound (CRLB), derive a closed-form analytic expression for the CRLB on the variance of any unbiased estimator of $g(\\theta)$ evaluated at the true parameter value $\\theta^{\\star}$. Your final answer must be a single closed-form analytic expression. Do not provide numerical values. Express your final answer explicitly in terms of the Jacobians $\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}$ and $\\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}$, the covariance $R$, and the weighting vector $a$.",
            "solution": "The problem asks for the Cramér–Rao Lower Bound (CRLB) on the variance of an unbiased estimator of the performance scalar $g(\\theta) = a^{\\top} h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})$. The derivation proceeds by first defining the statistical model and its log-likelihood, then calculating the Fisher Information Matrix (FIM), and finally applying the CRLB theorem for a function of the estimated parameters.\n\nThe measurement model is given by $y_k = h_{\\theta}(x_k, u_k) + v_k$, where the noise term $v_k$ follows an independent and identically distributed Gaussian distribution, $v_k \\sim \\mathcal{N}(0, R)$. This implies that the conditional probability distribution of a single measurement vector $y_k$ given the state $x_k$, input $u_k$, and parameter vector $\\theta$ is also Gaussian:\n$$y_k | x_k, u_k, \\theta \\sim \\mathcal{N}(h_{\\theta}(x_k, u_k), R)$$\nThe probability density function (PDF) for a single observation $y_k$ is therefore:\n$$p(y_k | \\theta) = \\frac{1}{(2\\pi)^{m/2} \\sqrt{\\det(R)}} \\exp\\left(-\\frac{1}{2} (y_k - h_{\\theta}(x_k, u_k))^{\\top} R^{-1} (y_k - h_{\\theta}(x_k, u_k))\\right)$$\nSince the noise terms $v_k$ are independent across the time index $k$, the likelihood of the entire dataset $Y = \\{y_k\\}_{k=1}^{K}$ is the product of the individual PDFs:\n$$L(\\theta | Y) = \\prod_{k=1}^{K} p(y_k | \\theta)$$\nThe log-likelihood function, $\\mathcal{L}(\\theta) = \\ln(L(\\theta|Y))$, is the sum of the individual log-PDFs:\n$$\\mathcal{L}(\\theta) = \\sum_{k=1}^{K} \\left[ -\\frac{m}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(R)) - \\frac{1}{2}(y_k - h_{\\theta}(x_k, u_k))^{\\top} R^{-1} (y_k - h_{\\theta}(x_k, u_k)) \\right]$$\nThe Maximum Likelihood Estimator (MLE) for $\\theta$, denoted $\\hat{\\theta}_{MLE}$, is the value of $\\theta$ that maximizes $\\mathcal{L}(\\theta)$. Since the first two terms in the sum are constant with respect to $\\theta$, this is equivalent to minimizing the sum of weighted squared residuals:\n$$\\hat{\\theta}_{MLE} = \\arg\\min_{\\theta} \\sum_{k=1}^{K} (y_k - h_{\\theta}(x_k, u_k))^{\\top} R^{-1} (y_k - h_{\\theta}(x_k, u_k))$$\n\nThe Fisher Information Matrix (FIM), $I(\\theta)$, provides a measure of the amount of information that the observable data $Y$ carries about the unknown parameter $\\theta$. Under standard regularity conditions, it is defined as the negative expectation of the Hessian of the log-likelihood function, evaluated at the true parameter value $\\theta^{\\star}$:\n$$I(\\theta^{\\star}) = -E\\left[ \\left. \\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial \\theta \\partial \\theta^{\\top}} \\right|_{\\theta=\\theta^{\\star}} \\right]$$\nLet us first compute the gradient of the log-likelihood (the score function). Let $J_k(\\theta) = \\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}$ be the $m \\times d$ Jacobian matrix of $h_{\\theta}$ with respect to $\\theta$. Using vector calculus identities, the gradient of $\\mathcal{L}(\\theta)$ is:\n$$\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} = \\sum_{k=1}^{K} J_k(\\theta)^{\\top} R^{-1} (y_k - h_{\\theta}(x_k, u_k))$$\nThe Hessian is the derivative of the score function with respect to $\\theta^{\\top}$:\n$$\\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial \\theta \\partial \\theta^{\\top}} = \\sum_{k=1}^{K} \\frac{\\partial}{\\partial \\theta^{\\top}} \\left[ J_k(\\theta)^{\\top} R^{-1} (y_k - h_{\\theta}(x_k, u_k)) \\right]$$\nApplying the product rule for differentiation yields:\n$$\\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial \\theta \\partial \\theta^{\\top}} = \\sum_{k=1}^{K} \\left[ \\left(\\frac{\\partial (J_k(\\theta)^{\\top} R^{-1})}{\\partial \\theta^{\\top}}\\right) (y_k - h_{\\theta}(x_k, u_k)) - J_k(\\theta)^{\\top} R^{-1} J_k(\\theta) \\right]$$\nThe expectation is taken with respect to the data distribution, where $E[y_k] = h_{\\theta^{\\star}}(x_k, u_k)$. Thus, at the true parameter value $\\theta^{\\star}$, we have $E[y_k - h_{\\theta^{\\star}}(x_k, u_k)] = E[v_k] = 0$. The expectation of the first term in the Hessian vanishes:\n$$E\\left[ \\left. \\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial \\theta \\partial \\theta^{\\top}} \\right|_{\\theta=\\theta^{\\star}} \\right] = - \\sum_{k=1}^{K} J_k(\\theta^{\\star})^{\\top} R^{-1} J_k(\\theta^{\\star})$$\nThe FIM at $\\theta^{\\star}$ is therefore:\n$$I(\\theta^{\\star}) = \\sum_{k=1}^{K} J_k(\\theta^{\\star})^{\\top} R^{-1} J_k(\\theta^{\\star}) = \\sum_{k=1}^{K} \\left(\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}\\right)_{\\theta=\\theta^{\\star}}^{\\top} R^{-1} \\left(\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}\\right)_{\\theta=\\theta^{\\star}}$$\nThe Cramér–Rao Lower Bound theorem states that for any unbiased estimator $\\hat{\\theta}$ of $\\theta$, its covariance matrix is bounded below by the inverse of the FIM: $\\mathrm{Cov}(\\hat{\\theta}) \\ge I(\\theta^{\\star})^{-1}$.\n\nWe need the CRLB for an estimator of the scalar function $g(\\theta) = a^{\\top} h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})$. For a function of the parameters, the CRLB is given by the delta method (or propagation of uncertainty):\n$$\\mathrm{CRLB}(g(\\theta^{\\star})) = (\\nabla_{\\theta} g(\\theta^{\\star}))^{\\top} I(\\theta^{\\star})^{-1} (\\nabla_{\\theta} g(\\theta^{\\star}))$$\nwhere $\\nabla_{\\theta} g(\\theta^{\\star})$ is the gradient of $g(\\theta)$ evaluated at $\\theta^{\\star}$. Let us compute this gradient. Let $J_{\\mathrm{op}}(\\theta) = \\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}$ be the $m \\times d$ Jacobian of the output map at the operating point.\n$$g(\\theta) = a^{\\top} h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})$$\nThe gradient with respect to $\\theta$ is a $d \\times 1$ column vector:\n$$\\nabla_{\\theta} g(\\theta) = \\left(\\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}\\right)^{\\top} a = J_{\\mathrm{op}}(\\theta)^{\\top} a$$\nThe transpose of this gradient is $(\\nabla_{\\theta} g(\\theta))^{\\top} = a^{\\top} J_{\\mathrm{op}}(\\theta)$.\nSubstituting the gradient and the FIM into the CRLB formula, we get the lower bound on the variance of any unbiased estimator of $g(\\theta)$, evaluated at the true parameter $\\theta^{\\star}$:\n$$\\mathrm{CRLB}(g(\\theta^{\\star})) = (a^{\\top} J_{\\mathrm{op}}(\\theta^{\\star})) I(\\theta^{\\star})^{-1} (J_{\\mathrm{op}}(\\theta^{\\star})^{\\top} a)$$\nSubstituting the full expression for $I(\\theta^{\\star})^{-1}$:\n$$\\mathrm{CRLB}(g(\\theta^{\\star})) = a^{\\top} J_{\\mathrm{op}}(\\theta^{\\star}) \\left[ \\sum_{k=1}^{K} J_k(\\theta^{\\star})^{\\top} R^{-1} J_k(\\theta^{\\star}) \\right]^{-1} J_{\\mathrm{op}}(\\theta^{\\star})^{\\top} a$$\nAs requested, we express the final answer in terms of the specified Jacobians, omitting the explicit evaluation at $\\theta^{\\star}$ for notational clarity as it is implied by the problem context.\n$$\\mathrm{CRLB}(g) = a^{\\top} \\left(\\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}\\right) \\left[ \\sum_{k=1}^{K} \\left(\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}\\right)^{\\top} R^{-1} \\left(\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}\\right) \\right]^{-1} \\left(\\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}\\right)^{\\top} a$$\nThis expression represents the minimum achievable variance for any unbiased estimator of the performance metric $g(\\theta)$, based on the provided data and model structure.",
            "answer": "$$\n\\boxed{\na^{\\top} \\left(\\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}\\right) \\left[ \\sum_{k=1}^{K} \\left(\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}\\right)^{\\top} R^{-1} \\left(\\frac{\\partial h_{\\theta}(x_k, u_k)}{\\partial \\theta}\\right) \\right]^{-1} \\left(\\frac{\\partial h_{\\theta}(x_{\\mathrm{op}}, u_{\\mathrm{op}})}{\\partial \\theta}\\right)^{\\top} a\n}\n$$"
        },
        {
            "introduction": "Once a model is established, a digital twin must contend with the inherent uncertainty of real-time data. This practice focuses on a critical skill: propagating the uncertainty from a system's state variables through a nonlinear performance model. By applying a first-order Taylor approximation, you will derive and compute the resulting variance of a Key Performance Indicator (KPI), providing a quantitative measure of the reliability of the digital twin's monitoring output. ",
            "id": "4215971",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) monitors a machining cell’s cycle time as a performance key performance indicator. The cycle time $K$ is modeled from the state vector $x = (u, b, T)^\\top$ via the function\n$$\nK = \\phi(x) = \\frac{\\eta}{1 - u} \\left( 1 + \\gamma b \\right) \\exp\\!\\left( \\beta \\left( T - T_{\\mathrm{ref}} \\right) \\right),\n$$\nwhere $u$ is utilization (dimensionless, expressed as a fraction), $b$ is normalized buffer occupancy (dimensionless, expressed as a fraction), $T$ is temperature in degrees Celsius, and $T_{\\mathrm{ref}}$ is a reference temperature in degrees Celsius. The constants are $\\eta = 0.8$ (seconds), $\\gamma = 0.1$ (dimensionless), $\\beta = 0.02$ (per degree Celsius), and $T_{\\mathrm{ref}} = 35$ (degrees Celsius). The DT maintains a Gaussian estimate of the state, with mean (operating point)\n$$\n\\mu_x = \\begin{pmatrix} u_0 \\\\ b_0 \\\\ T_0 \\end{pmatrix} = \\begin{pmatrix} 0.75 \\\\ 0.30 \\\\ 40 \\end{pmatrix},\n$$\nand covariance\n$$\n\\Sigma_x = \\begin{pmatrix}\n0.0004 & 0.0003 & 0.01 \\\\\n0.0003 & 0.0025 & 0.02 \\\\\n0.01 & 0.02 & 2.25\n\\end{pmatrix}.\n$$\nStarting from fundamental definitions of variance, covariance, and a first-order Taylor expansion, derive the first-order approximation to the variance of $K$ about $\\mu_x$. Then, using the provided numerical parameters, compute the approximate variance of $K$ in seconds squared. Assume the noise around $\\mu_x$ is sufficiently small for the first-order approximation to be valid. Round your final numerical answer to four significant figures. Express the final variance in $\\mathrm{s}^2$.",
            "solution": "The problem asks for a first-order approximation of the variance of the cycle time $K$, which is a function of the state vector $x = (u, b, T)^\\top$. The state vector $x$ is a random variable with a given mean $\\mu_x$ and covariance matrix $\\Sigma_x$.\n\nThe function is given by:\n$$\nK = \\phi(x) = \\frac{\\eta}{1 - u} \\left( 1 + \\gamma b \\right) \\exp\\!\\left( \\beta \\left( T - T_{\\mathrm{ref}} \\right) \\right)\n$$\nThe state is characterized by a Gaussian distribution with mean $\\mu_x$ and covariance $\\Sigma_x$.\nThe first-order Taylor series expansion of the function $K = \\phi(x)$ around the mean state $\\mu_x$ is:\n$$\n\\phi(x) \\approx \\phi(\\mu_x) + \\nabla \\phi(\\mu_x)^\\top (x - \\mu_x)\n$$\nwhere $\\nabla \\phi(\\mu_x)$ is the gradient of $\\phi$ evaluated at $\\mu_x$. Let us denote this gradient as $J$.\n\nThe expected value of $K$ is approximated by:\n$$\nE[K] = E[\\phi(x)] \\approx E[\\phi(\\mu_x) + J^\\top (x - \\mu_x)] = \\phi(\\mu_x) + J^\\top E[x - \\mu_x]\n$$\nSince $E[x - \\mu_x] = E[x] - \\mu_x = \\mu_x - \\mu_x = 0$, the first-order approximation of the mean is $E[K] \\approx \\phi(\\mu_x)$.\n\nThe variance of $K$ is defined as $\\mathrm{Var}(K) = E[(K - E[K])^2]$. Using the first-order approximation for both $K$ and $E[K]$:\n$$\n\\mathrm{Var}(K) \\approx E\\left[ \\left( (\\phi(\\mu_x) + J^\\top (x - \\mu_x)) - \\phi(\\mu_x) \\right)^2 \\right]\n$$\n$$\n\\mathrm{Var}(K) \\approx E\\left[ \\left( J^\\top (x - \\mu_x) \\right)^2 \\right]\n$$\nThe squared term can be written as a matrix product:\n$$\n\\mathrm{Var}(K) \\approx E\\left[ \\left( J^\\top (x - \\mu_x) \\right) \\left( (x - \\mu_x)^\\top J \\right) \\right]\n$$\nSince $J$ is a constant vector (evaluated at the constant mean $\\mu_x$), we can move it outside the expectation:\n$$\n\\mathrm{Var}(K) \\approx J^\\top E\\left[ (x - \\mu_x) (x - \\mu_x)^\\top \\right] J\n$$\nThe term inside the expectation is the definition of the covariance matrix of the state vector $x$, $\\Sigma_x = E\\left[ (x - \\mu_x) (x - \\mu_x)^\\top \\right]$.\nThus, the first-order approximation for the variance of $K$ is:\n$$\n\\mathrm{Var}(K) \\approx J^\\top \\Sigma_x J\n$$\nThis is the general formula for the propagation of uncertainty for a vector function, specialized here to a scalar output.\n\nTo apply this formula, we first need to compute the gradient vector $J = \\nabla \\phi(x)$ and evaluate it at the mean operating point $\\mu_x = (u_0, b_0, T_0)^\\top$. The components of the gradient are the partial derivatives with respect to $u$, $b$, and $T$.\n\n1.  Partial derivative with respect to $u$:\n    $$\n    \\frac{\\partial K}{\\partial u} = \\frac{\\partial}{\\partial u} \\left[ \\frac{\\eta}{1 - u} \\left( 1 + \\gamma b \\right) \\exp\\left( \\beta (T - T_{\\mathrm{ref}}) \\right) \\right] = \\frac{\\eta}{(1-u)^2} \\left( 1 + \\gamma b \\right) \\exp\\left( \\beta (T - T_{\\mathrm{ref}}) \\right)\n    $$\n    This can be expressed more simply in terms of $K$ itself:\n    $$\n    \\frac{\\partial K}{\\partial u} = K \\cdot \\frac{1}{1-u}\n    $$\n\n2.  Partial derivative with respect to $b$:\n    $$\n    \\frac{\\partial K}{\\partial b} = \\frac{\\eta}{1 - u} \\left[ \\frac{\\partial}{\\partial b} (1 + \\gamma b) \\right] \\exp\\left( \\beta (T - T_{\\mathrm{ref}}) \\right) = \\frac{\\eta}{1 - u} \\gamma \\exp\\left( \\beta (T - T_{\\mathrm{ref}}) \\right)\n    $$\n    In terms of $K$:\n    $$\n    \\frac{\\partial K}{\\partial b} = K \\cdot \\frac{\\gamma}{1+\\gamma b}\n    $$\n\n3.  Partial derivative with respect to $T$:\n    $$\n    \\frac{\\partial K}{\\partial T} = \\frac{\\eta}{1 - u} (1 + \\gamma b) \\left[ \\frac{\\partial}{\\partial T} \\exp\\left( \\beta (T - T_{\\mathrm{ref}}) \\right) \\right] = \\frac{\\eta}{1 - u} (1 + \\gamma b) \\exp\\left( \\beta (T - T_{\\mathrm{ref}}) \\right) \\cdot \\beta\n    $$\n    In terms of $K$:\n    $$\n    \\frac{\\partial K}{\\partial T} = K \\cdot \\beta\n    $$\n\nNext, we evaluate these derivatives at the mean operating point $\\mu_x$, where $u_0 = 0.75$, $b_0 = 0.30$, and $T_0 = 40$. We also use the given constants: $\\eta = 0.8$, $\\gamma = 0.1$, $\\beta = 0.02$, and $T_{\\mathrm{ref}} = 35$.\n\nFirst, let's compute the value of $K$ at the mean point, $K_0 = \\phi(\\mu_x)$:\n$$\nK_0 = \\frac{0.8}{1 - 0.75} (1 + 0.1 \\cdot 0.30) \\exp(0.02(40 - 35))\n$$\n$$\nK_0 = \\frac{0.8}{0.25} (1 + 0.03) \\exp(0.02 \\cdot 5) = 3.2 \\cdot 1.03 \\cdot \\exp(0.1) = 3.296 \\exp(0.1)\n$$\nUsing a numerical value for $\\exp(0.1)$, we get $K_0 \\approx 3.296 \\cdot 1.1051709 = 3.6426633$ s.\n\nNow we evaluate the components of the Jacobian (gradient) vector $J = (J_u, J_b, J_T)^\\top$ at $\\mu_x$:\n$$\nJ_u = \\frac{\\partial K}{\\partial u} \\bigg|_{\\mu_x} = \\frac{K_0}{1 - u_0} = \\frac{K_0}{1 - 0.75} = \\frac{K_0}{0.25} = 4 K_0 \\approx 4 \\cdot 3.6426633 = 14.570653\n$$\n$$\nJ_b = \\frac{\\partial K}{\\partial b} \\bigg|_{\\mu_x} = \\frac{K_0 \\gamma}{1 + \\gamma b_0} = \\frac{K_0 \\cdot 0.1}{1 + 0.1 \\cdot 0.30} = \\frac{0.1 K_0}{1.03} \\approx 0.097087 \\cdot K_0 \\approx 0.353686\n$$\n$$\nJ_T = \\frac{\\partial K}{\\partial T} \\bigg|_{\\mu_x} = K_0 \\beta = 0.02 K_0 \\approx 0.02 \\cdot 3.6426633 = 0.072853\n$$\nSo, the gradient vector is $J \\approx (14.570653, 0.353686, 0.072853)^\\top$.\n\nFinally, we compute the variance using $\\mathrm{Var}(K) \\approx J^\\top \\Sigma_x J$. The covariance matrix is given as:\n$$\n\\Sigma_x = \\begin{pmatrix}\n0.0004 & 0.0003 & 0.01 \\\\\n0.0003 & 0.0025 & 0.02 \\\\\n0.01 & 0.02 & 2.25\n\\end{pmatrix}\n$$\nThe variance is given by the quadratic form:\n$$\n\\mathrm{Var}(K) \\approx J_u^2 \\sigma_{uu}^2 + J_b^2 \\sigma_{bb}^2 + J_T^2 \\sigma_{TT}^2 + 2(J_u J_b \\sigma_{ub} + J_u J_T \\sigma_{uT} + J_b J_T \\sigma_{bT})\n$$\nwhere $\\sigma_{ij}$ are the elements of $\\Sigma_x$.\n\nLet's compute each term:\n-   Variance contribution from $u$: $J_u^2 \\sigma_{uu}^2 = (14.570653)^2 \\cdot 0.0004 \\approx 0.0849215$\n-   Variance contribution from $b$: $J_b^2 \\sigma_{bb}^2 = (0.353686)^2 \\cdot 0.0025 \\approx 0.0003127$\n-   Variance contribution from $T$: $J_T^2 \\sigma_{TT}^2 = (0.072853)^2 \\cdot 2.25 \\approx 0.0119421$\n-   Covariance contribution from $u, b$: $2 J_u J_b \\sigma_{ub} = 2 \\cdot 14.570653 \\cdot 0.353686 \\cdot 0.0003 \\approx 0.0030930$\n-   Covariance contribution from $u, T$: $2 J_u J_T \\sigma_{uT} = 2 \\cdot 14.570653 \\cdot 0.072853 \\cdot 0.01 \\approx 0.0212318$\n-   Covariance contribution from $b, T$: $2 J_b J_T \\sigma_{bT} = 2 \\cdot 0.353686 \\cdot 0.072853 \\cdot 0.02 \\approx 0.0010308$\n\nSumming these contributions:\n$$\n\\mathrm{Var}(K) \\approx 0.0849215 + 0.0003127 + 0.0119421 + 0.0030930 + 0.0212318 + 0.0010308\n$$\n$$\n\\mathrm{Var}(K) \\approx 0.1225319 \\;\\mathrm{s}^2\n$$\nThe problem requires rounding the final answer to four significant figures.\n$$\n\\mathrm{Var}(K) \\approx 0.1225 \\;\\mathrm{s}^2\n$$",
            "answer": "$$\n\\boxed{0.1225}\n$$"
        },
        {
            "introduction": "This final practice integrates theoretical concepts into a practical engineering design scenario. The performance of a digital twin is fundamentally limited by the quality and cost of its data sources. Here, you will tackle a sensor selection problem, learning how to use the Fisher information metric to choose a sensor suite that maximizes knowledge about a critical parameter while respecting real-world budget and bandwidth constraints. ",
            "id": "4215953",
            "problem": "A Digital Twin (DT) for a Cyber-Physical System (CPS) is deployed to monitor a dimensionless performance-related parameter $\\theta$ that encodes normalized throughput degradation (lower values of $\\theta$ indicate worse performance). The DT can subscribe to a finite set of sensor streams indexed by $i \\in \\{1,2,3,4\\}$, each providing measurements modeled by\n$$\ny_{i}[k] \\;=\\; h_{i}(\\theta) \\;+\\; v_{i}[k],\n$$\nwhere $v_{i}[k] \\sim \\mathcal{N}(0,\\sigma_{i}^{2})$ are independent and identically distributed across time and sensors, and the local sensitivity $s_{i} \\equiv \\left.\\frac{\\partial h_{i}(\\theta)}{\\partial \\theta}\\right|_{\\theta=\\theta_{0}}$ is constant within the operating neighborhood of interest. Each sensor $i$ streams at a fixed sampling rate $r_{i}$ (samples per second) and has a message size $m_{i}$ (kilobits per sample), incurring a bandwidth consumption $b_{i} = r_{i} m_{i}$ (kilobits per second). Each sensor also incurs a fixed operating cost $c_{i}$ (arbitrary units) when selected. The DT must choose a subset of sensors to maximize the Fisher information about $\\theta$ per second, subject to total bandwidth and total cost constraints.\n\nAssume the following sensor characteristics:\n- Sensor $1$: $s_{1} = 1.8$, $\\sigma_{1} = 0.6$, $r_{1} = 10$, $m_{1} = 5$ kilobits, $c_{1} = 2$.\n- Sensor $2$: $s_{2} = 3.0$, $\\sigma_{2} = 1.5$, $r_{2} = 30$, $m_{2} = 8$ kilobits, $c_{2} = 5$.\n- Sensor $3$: $s_{3} = 0.9$, $\\sigma_{3} = 0.3$, $r_{3} = 5$, $m_{3} = 3$ kilobits, $c_{3} = 1$.\n- Sensor $4$: $s_{4} = 2.0$, $\\sigma_{4} = 1.0$, $r_{4} = 50$, $m_{4} = 20$ kilobits, $c_{4} = 8$.\n\nThe Digital Twin operates under the constraints\n$$\n\\sum_{i \\in S} b_{i} \\;\\leq\\; B_{\\max} \\;=\\; 300 \\text{ kilobits per second}, \\quad\n\\sum_{i \\in S} c_{i} \\;\\leq\\; C_{\\max} \\;=\\; 7,\n$$\nwhere $S$ is the selected subset of sensors, and the selection must respect both constraints simultaneously. You may assume measurements across sensors and time are statistically independent, and that linearization around $\\theta_{0}$ is valid. Using only the definition of Fisher information and standard properties of Gaussian likelihoods, derive the expression for the per-second Fisher information contributed by a selected subset $S$ and compute the maximum achievable Fisher information per second under the given constraints. Express the final information value as a pure number with no units and no rounding required.",
            "solution": "The objective is to maximize the total Fisher information per second from a subset of sensors $S$, subject to bandwidth and cost constraints. The total Fisher information is the sum of the information contributions from each selected sensor, due to the assumption of statistical independence.\n\nFirst, we derive the Fisher information contributed by a single sample from sensor $i$. The measurement $y_{i}[k]$ follows a Gaussian distribution with mean $h_{i}(\\theta)$ and variance $\\sigma_{i}^{2}$. The probability density function (likelihood for a single sample) is:\n$$\np(y_{i}[k] | \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_{i}^{2}}} \\exp\\left( -\\frac{(y_{i}[k] - h_{i}(\\theta))^{2}}{2\\sigma_{i}^{2}} \\right)\n$$\nThe log-likelihood, $\\mathcal{L}(\\theta) = \\ln p(y_{i}[k] | \\theta)$, is:\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{i}^{2}) - \\frac{(y_{i}[k] - h_{i}(\\theta))^{2}}{2\\sigma_{i}^{2}}\n$$\nThe Fisher information for a single sample, $J_{i}^{\\text{sample}}$, is defined as the negative expected value of the second derivative of the log-likelihood with respect to the parameter $\\theta$:\n$$\nJ_{i}^{\\text{sample}} = -E\\left[ \\frac{\\partial^{2}\\mathcal{L}(\\theta)}{\\partial\\theta^{2}} \\right]\n$$\nWe compute the derivatives:\n$$\n\\frac{\\partial\\mathcal{L}(\\theta)}{\\partial\\theta} = -\\frac{1}{2\\sigma_{i}^{2}} \\cdot 2(y_{i}[k] - h_{i}(\\theta)) \\cdot \\left(-\\frac{\\partial h_{i}(\\theta)}{\\partial\\theta}\\right) = \\frac{y_{i}[k] - h_{i}(\\theta)}{\\sigma_{i}^{2}} \\frac{\\partial h_{i}(\\theta)}{\\partial\\theta}\n$$\n$$\n\\frac{\\partial^{2}\\mathcal{L}(\\theta)}{\\partial\\theta^{2}} = -\\frac{1}{\\sigma_{i}^{2}} \\left(\\frac{\\partial h_{i}(\\theta)}{\\partial\\theta}\\right)^{2} + \\frac{y_{i}[k] - h_{i}(\\theta)}{\\sigma_{i}^{2}} \\frac{\\partial^{2}h_{i}(\\theta)}{\\partial\\theta^{2}}\n$$\nTaking the expectation, we note that $E[y_{i}[k] - h_{i}(\\theta)] = E[v_{i}[k]] = 0$. Using the provided local sensitivity $s_{i} = \\frac{\\partial h_{i}(\\theta)}{\\partial\\theta}$, the expectation of the second derivative becomes:\n$$\nE\\left[ \\frac{\\partial^{2}\\mathcal{L}(\\theta)}{\\partial\\theta^{2}} \\right] = -\\frac{s_{i}^{2}}{\\sigma_{i}^{2}}\n$$\nTherefore, the Fisher information from a single sample is:\n$$\nJ_{i}^{\\text{sample}} = - \\left(-\\frac{s_{i}^{2}}{\\sigma_{i}^{2}}\\right) = \\frac{s_{i}^{2}}{\\sigma_{i}^{2}}\n$$\nSensor $i$ provides $r_{i}$ independent samples per second. The total Fisher information per second from sensor $i$, denoted $J_{i}$, is the sum of the information from each sample in that second:\n$$\nJ_{i} = r_{i} \\cdot J_{i}^{\\text{sample}} = r_{i} \\frac{s_{i}^{2}}{\\sigma_{i}^{2}}\n$$\nFor a subset of sensors $S$, the total Fisher information per second, $J(S)$, is the sum of contributions from each sensor in the set, due to independence across sensors:\n$$\nJ(S) = \\sum_{i \\in S} J_{i} = \\sum_{i \\in S} r_{i} \\frac{s_{i}^{2}}{\\sigma_{i}^{2}}\n$$\nThe problem is to find the subset $S \\subseteq \\{1, 2, 3, 4\\}$ that maximizes $J(S)$ subject to:\n$$\n\\sum_{i \\in S} b_{i} \\leq B_{\\max} \\quad \\text{and} \\quad \\sum_{i \\in S} c_{i} \\leq C_{\\max}\n$$\nwhere $b_i = r_i m_i$, $B_{\\max} = 300$, and $C_{\\max} = 7$.\n\nWe compute the values of $b_i$ and $J_i$ for each sensor:\n- **Sensor 1:**\n  $b_1 = 10 \\times 5 = 50$ kb/s\n  $J_1 = 10 \\cdot \\frac{1.8^2}{0.6^2} = 10 \\cdot \\left(\\frac{1.8}{0.6}\\right)^2 = 10 \\cdot 3^2 = 90$\n- **Sensor 2:**\n  $b_2 = 30 \\times 8 = 240$ kb/s\n  $J_2 = 30 \\cdot \\frac{3.0^2}{1.5^2} = 30 \\cdot \\left(\\frac{3.0}{1.5}\\right)^2 = 30 \\cdot 2^2 = 120$\n- **Sensor 3:**\n  $b_3 = 5 \\times 3 = 15$ kb/s\n  $J_3 = 5 \\cdot \\frac{0.9^2}{0.3^2} = 5 \\cdot \\left(\\frac{0.9}{0.3}\\right)^2 = 5 \\cdot 3^2 = 45$\n- **Sensor 4:**\n  $b_4 = 50 \\times 20 = 1000$ kb/s\n  $J_4 = 50 \\cdot \\frac{2.0^2}{1.0^2} = 50 \\cdot 2^2 = 200$\n\nThe optimization problem is to choose a subset $S$ from $\\{1,2,3,4\\}$ to maximize $\\sum_{i \\in S} J_i$ subject to $\\sum_{i \\in S} b_i \\le 300$ and $\\sum_{i \\in S} c_i \\le 7$. Since there are only $2^4 = 16$ possible subsets, we can find the solution by enumeration.\n\nLet's evaluate all possible subsets $S$:\n1.  $S = \\emptyset$: $b=0$, $c=0$. Valid. $J=0$.\n2.  $S = \\{1\\}$: $b=50 \\le 300$, $c=2 \\le 7$. Valid. $J=90$.\n3.  $S = \\{2\\}$: $b=240 \\le 300$, $c=5 \\le 7$. Valid. $J=120$.\n4.  $S = \\{3\\}$: $b=15 \\le 300$, $c=1 \\le 7$. Valid. $J=45$.\n5.  $S = \\{4\\}$: $b=1000 > 300$ (and $c=8 > 7$). Invalid.\n6.  $S = \\{1, 2\\}$: $b = 50+240=290 \\le 300$, $c = 2+5=7 \\le 7$. Valid. $J = 90+120=210$.\n7.  $S = \\{1, 3\\}$: $b = 50+15=65 \\le 300$, $c = 2+1=3 \\le 7$. Valid. $J = 90+45=135$.\n8.  $S = \\{1, 4\\}$: Contains sensor 4. Invalid.\n9.  $S = \\{2, 3\\}$: $b=240+15=255 \\le 300$, $c=5+1=6 \\le 7$. Valid. $J = 120+45=165$.\n10. $S = \\{2, 4\\}$: Contains sensor 4. Invalid.\n11. $S = \\{3, 4\\}$: Contains sensor 4. Invalid.\n12. $S = \\{1, 2, 3\\}$: $b=50+240+15=305 > 300$. Invalid.\n13. All other subsets of size 3 or 4 contain sensor 4 or the invalid subset $\\{1,2,3\\}$ and are therefore also invalid.\n\nComparing the Fisher information values for all valid subsets:\n- $J(\\emptyset) = 0$\n- $J(\\{1\\}) = 90$\n- $J(\\{2\\}) = 120$\n- $J(\\{3\\}) = 45$\n- $J(\\{1, 2\\}) = 210$\n- $J(\\{1, 3\\}) = 135$\n- $J(\\{2, 3\\}) = 165$\n\nThe maximum value is $210$, achieved with the sensor subset $S = \\{1, 2\\}$.",
            "answer": "$$\n\\boxed{210}\n$$"
        }
    ]
}