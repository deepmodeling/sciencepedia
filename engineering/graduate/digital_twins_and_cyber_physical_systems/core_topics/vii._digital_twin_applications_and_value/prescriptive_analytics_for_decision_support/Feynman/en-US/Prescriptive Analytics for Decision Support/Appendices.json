{
    "hands_on_practices": [
        {
            "introduction": "At the heart of prescriptive analytics lies the ability to optimize system behavior under a set of rules and limitations. This first exercise provides a foundational experience by formulating a resource allocation problem as a strictly convex Quadratic Program (QP), a common model for such tasks. By deriving the dual problem and interpreting the Lagrange multipliers, you will not only determine optimal operational setpoints but also uncover the economic value, or \"shadow price,\" of constrained resources within a cyber-physical system .",
            "id": "4235979",
            "problem": "A digital twin of a cyber-physical system (CPS) coordinates $2$ networked control subsystems that share a communication bandwidth resource. Let $x \\in \\mathbb{R}^n$ denote a vector of prescriptive setpoints (e.g., actuation rates) to be deployed by the CPS controller. The digital twin uses prescriptive analytics to choose $x$ by minimizing a strictly convex quadratic performance index of the form $f(x) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x$, where $Q \\in \\mathbb{R}^{n \\times n}$ satisfies $Q \\succ 0$ and $c \\in \\mathbb{R}^n$. The setpoints are subject to linear constraints representing CPS resource limits and coordination requirements.\n\nStarting from the definitions of the Lagrangian and the Lagrange dual function for convex programs, and without assuming any pre-derived formulas, perform the following:\n\n$1.$ Consider the strictly convex quadratic program with linear constraints\n$$\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^n} \\quad  \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\\\\n\\text{s.t.} \\quad  A x \\leq b, \\\\\n E x = h,\n\\end{aligned}\n$$\nwhere $Q \\succ 0$, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $E \\in \\mathbb{R}^{p \\times n}$, and $h \\in \\mathbb{R}^p$. Derive the Lagrangian, the Lagrange dual function, and the dual optimization problem, including the domain of the dual variables. Clearly state the sign or free-domain conditions for the dual variables. Interpret the economic meaning of the dual variables as shadow prices for CPS resources.\n\n$2.$ Now specialize to a $2$-subsystem CPS, with $n=2$, with a shared bandwidth resource represented by a single equality constraint $a^{\\top} x = b$ with $a = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $b \\in \\mathbb{R}$, and nonnegativity $x \\geq 0$. Let\n$$\nQ = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}, \n\\qquad\nc = \\begin{bmatrix} -8 \\\\ -5 \\end{bmatrix},\n\\qquad\nb = 5.\n$$\nUsing the Karush–Kuhn–Tucker (KKT) optimality conditions derived from first principles in part $1$, compute the optimal Lagrange multiplier associated with the equality constraint $a^{\\top} x = b$, and interpret it as a shadow price for the bandwidth resource in the CPS. Then, using the envelope interpretation of the dual variable, determine the marginal change in the optimal objective value with respect to $b$ at $b=5$. Express this marginal change in cost units per unit bandwidth. Provide the final answer as a single exact real number. No rounding is required. Do not include units in the final boxed answer.",
            "solution": "The user wants me to solve the problem by first validating it.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A digital twin coordinates $2$ networked control subsystems.\n- The control setpoints are a vector $x \\in \\mathbb{R}^n$.\n- The performance index to be minimized is a strictly convex quadratic function: $f(x) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x$, where $Q \\in \\mathbb{R}^{n \\times n}$ is a matrix satisfying $Q \\succ 0$ (strictly positive definite) and $c \\in \\mathbb{R}^n$ is a vector.\n- The constraints on $x$ are linear.\n- **Part 1**: The general problem is a strictly convex quadratic program (QP):\n$$\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^n} \\quad  \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\\\\n\\text{s.t.} \\quad  A x \\leq b, \\\\\n E x = h,\n\\end{aligned}\n$$\nwhere $Q \\succ 0$, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $E \\in \\mathbb{R}^{p \\times n}$, and $h \\in \\mathbb{R}^p$.\n- **Part 2**: A specialized problem for a $2$-subsystem CPS with $n=2$.\n- The constraints are a shared bandwidth resource $a^{\\top} x = b$ and non-negativity $x \\geq 0$.\n- The specific parameters are:\n$$\nQ = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}, \n\\qquad\nc = \\begin{bmatrix} -8 \\\\ -5 \\end{bmatrix},\n\\qquad\na = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\n\\qquad\nb = 5.\n$$\n- The task is to compute the optimal Lagrange multiplier for the equality constraint, interpret it as a shadow price, and determine the marginal change in the optimal objective value with respect to $b$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is grounded in the well-established field of convex optimization, specifically quadratic programming. The application to resource allocation in a cyber-physical system (CPS) is a standard and realistic use case for such models. The condition $Q \\succ 0$ ensures the objective function is strictly convex, which is a fundamental property for guaranteeing a unique minimizer. For the specific matrix in Part 2, $Q = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}$, the leading principal minors are $4  0$ and $\\det(Q) = (4)(3) - (2)(2) = 12 - 4 = 8  0$. Since all leading principal minors are positive, $Q$ is indeed positive definite, satisfying the stated condition.\n2.  **Well-Posed**: The problem is well-posed. A strictly convex objective function minimized over a non-empty, closed, convex set of feasible points has a unique global minimum. The constraints in both Part 1 and Part 2 are linear, defining a convex feasible set. The feasible set in Part 2, $\\{x \\in \\mathbb{R}^2 \\mid x_1+x_2=5, x_1 \\ge 0, x_2 \\ge 0 \\}$, is a non-empty, compact line segment, ensuring a solution exists and is unique.\n3.  **Objective**: The problem statement uses precise, unambiguous mathematical language. There are no subjective or opinion-based elements.\n4.  **Completeness**: The problem provides all necessary data and definitions to proceed with the derivations and calculations. It is self-contained.\n5.  **Other Flaws**: The problem does not suffer from any of the other listed invalidity criteria. It is not contradictory, unrealistic, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid and well-posed. I will proceed with the solution.\n\n### Part 1: Derivation of the Dual Problem\n\nThe primal problem is a convex quadratic program given by:\n$$\n\\begin{aligned}\np^* = \\min_{x \\in \\mathbb{R}^n} \\quad  \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\\\\n\\text{s.t.} \\quad  A x \\leq b, \\\\\n E x = h.\n\\end{aligned}\n$$\nTo form the Lagrangian, we introduce Lagrange multipliers (dual variables). Let $\\lambda \\in \\mathbb{R}^m$ be the vector of multipliers associated with the $m$ inequality constraints $A x - b \\leq 0$, and let $\\nu \\in \\mathbb{R}^p$ be the vector of multipliers for the $p$ equality constraints $E x - h = 0$. By convention, the multipliers for inequality constraints of the form $g(x) \\leq 0$ must be non-negative. Therefore, the domain for the dual variables is $\\lambda \\geq 0$ (element-wise) and $\\nu \\in \\mathbb{R}^p$.\n\nThe Lagrangian, $L(x, \\lambda, \\nu)$, is defined as:\n$$\nL(x, \\lambda, \\nu) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x + \\lambda^{\\top}(A x - b) + \\nu^{\\top}(E x - h)\n$$\nWe can rearrange the terms to group those involving $x$:\n$$\nL(x, \\lambda, \\nu) = \\frac{1}{2} x^{\\top} Q x + (c + A^{\\top}\\lambda + E^{\\top}\\nu)^{\\top} x - \\lambda^{\\top}b - \\nu^{\\top}h\n$$\nThe Lagrange dual function, $g(\\lambda, \\nu)$, is the infimum of the Lagrangian over the primal variable $x$:\n$$\ng(\\lambda, \\nu) = \\inf_{x \\in \\mathbb{R}^n} L(x, \\lambda, \\nu)\n$$\nFor a fixed $\\lambda$ and $\\nu$, the Lagrangian $L(x, \\lambda, \\nu)$ is a quadratic function of $x$. Since $Q \\succ 0$, this function is strictly convex in $x$. The infimum is attained at the unique point where the gradient with respect to $x$ is zero:\n$$\n\\nabla_x L(x, \\lambda, \\nu) = Q x + c + A^{\\top}\\lambda + E^{\\top}\\nu = 0\n$$\nSolving for $x$, we find the minimizer $x^*(\\lambda, \\nu)$:\n$$\nx^*(\\lambda, \\nu) = -Q^{-1}(c + A^{\\top}\\lambda + E^{\\top}\\nu)\n$$\nNote that $Q$ is invertible because it is positive definite.\n\nTo find the dual function $g(\\lambda, \\nu)$, we substitute $x^*$ back into the expression for $L(x, \\lambda, \\nu)$:\n$$\ng(\\lambda, \\nu) = \\frac{1}{2} (x^*)^{\\top} Q x^* + (c + A^{\\top}\\lambda + E^{\\top}\\nu)^{\\top} x^* - \\lambda^{\\top}b - \\nu^{\\top}h\n$$\nFrom the stationarity condition, we have $(c + A^{\\top}\\lambda + E^{\\top}\\nu) = -Qx^*$. Substituting this into the second term gives:\n$$\ng(\\lambda, \\nu) = \\frac{1}{2} (x^*)^{\\top} Q x^* + (-Qx^*)^{\\top} x^* - \\lambda^{\\top}b - \\nu^{\\top}h = \\frac{1}{2} (x^*)^{\\top} Q x^* - (x^*)^{\\top} Q x^* - \\lambda^{\\top}b - \\nu^{\\top}h\n$$\n$$\ng(\\lambda, \\nu) = -\\frac{1}{2} (x^*)^{\\top} Q x^* - \\lambda^{\\top}b - \\nu^{\\top}h\n$$\nNow, we substitute the expression for $x^*$:\n$$\n(x^*)^{\\top} Q x^* = \\left(-Q^{-1}(c + A^{\\top}\\lambda + E^{\\top}\\nu)\\right)^{\\top} Q \\left(-Q^{-1}(c + A^{\\top}\\lambda + E^{\\top}\\nu)\\right)\n$$\nSince $Q$ and $Q^{-1}$ are symmetric, this simplifies to:\n$$\n(x^*)^{\\top} Q x^* = (c + A^{\\top}\\lambda + E^{\\top}\\nu)^{\\top} (Q^{-1})^{\\top} Q Q^{-1} (c + A^{\\top}\\lambda + E^{\\top}\\nu) = (c + A^{\\top}\\lambda + E^{\\top}\\nu)^{\\top} Q^{-1} (c + A^{\\top}\\lambda + E^{\\top}\\nu)\n$$\nTherefore, the Lagrange dual function is:\n$$\ng(\\lambda, \\nu) = -\\frac{1}{2} (c + A^{\\top}\\lambda + E^{\\top}\\nu)^{\\top} Q^{-1} (c + A^{\\top}\\lambda + E^{\\top}\\nu) - b^{\\top}\\lambda - h^{\\top}\\nu\n$$\nThe dual problem is to maximize the dual function subject to the constraints on the dual variables:\n$$\n\\begin{aligned}\n\\max_{\\lambda, \\nu} \\quad  g(\\lambda, \\nu) \\\\\n\\text{s.t.} \\quad  \\lambda \\geq 0.\n\\end{aligned}\n$$\nThe domain of the dual variables requires $\\lambda_i \\geq 0$ for each $i=1, \\dots, m$, while the variables $\\nu_j$ for $j=1, \\dots, p$ are free (unconstrained in sign).\n\nThe economic interpretation of the dual variables is as shadow prices. For a problem where $b$ represents resource limits and $h$ represents performance targets, the optimal dual variables $(\\lambda^*, \\nu^*)$ quantify the sensitivity of the optimal objective value $p^*$ to perturbations in these constraints. By the envelope theorem, at the optimum, we have $\\frac{\\partial p^*}{\\partial b_i} = \\lambda_i^*$ and $\\frac{\\partial p^*}{\\partial h_j} = \\nu_j^*$ when the Lagrangian is formed as $L=f(x) - \\lambda^T(Ax-b) - \\nu^T(Ex-h)$. With the convention used here, $L=f(x) + \\lambda^T(Ax-b) + \\nu^T(Ex-h)$, the sensitivities are $\\frac{\\partial p^*}{\\partial b_i} = -\\lambda_i^*$ and $\\frac{\\partial p^*}{\\partial h_j} = -\\nu_j^*$. Thus, $\\lambda_i^*$ is the marginal rate at which the optimal cost *increases* as the $i$-th resource limit $b_i$ is tightened (decreased). A value of $\\lambda_i^*0$ indicates that the $i$-th constraint is active and scarce, and relaxing the constraint (increasing $b_i$) would improve the objective. Similarly, $-\\nu_j^*$ is the marginal rate at which the cost changes for an increase in the $j$-th target $h_j$.\n\n### Part 2: Specific Calculation and Interpretation\n\nWe are given the specific problem:\n$$\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^2} \\quad  f(x) = \\frac{1}{2} x^{\\top} \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix} x + \\begin{bmatrix} -8 \\\\ -5 \\end{bmatrix}^{\\top} x \\\\\n\\text{s.t.} \\quad  x_1 + x_2 = 5, \\\\\n x_1 \\geq 0, \\quad x_2 \\geq 0.\n\\end{aligned}\n$$\nThe constraints can be written as $x_1+x_2-5=0$, $-x_1 \\le 0$, and $-x_2 \\le 0$. We introduce a Lagrange multiplier $\\nu$ for the equality constraint and multipliers $\\mu_1, \\mu_2 \\ge 0$ for the non-negativity constraints. The Lagrangian is:\n$$\nL(x_1, x_2, \\mu_1, \\mu_2, \\nu) = \\frac{1}{2}(4x_1^2 + 4x_1x_2 + 3x_2^2) - 8x_1 - 5x_2 - \\mu_1 x_1 - \\mu_2 x_2 + \\nu(x_1+x_2-5)\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions for an optimal solution $(x^*, \\mu^*, \\nu^*)$ are:\n1.  **Stationarity**: $\\nabla_x L = 0$\n    $$ \\frac{\\partial L}{\\partial x_1} = 4x_1 + 2x_2 - 8 - \\mu_1 + \\nu = 0 $$\n    $$ \\frac{\\partial L}{\\partial x_2} = 2x_1 + 3x_2 - 5 - \\mu_2 + \\nu = 0 $$\n2.  **Primal Feasibility**:\n    $$ x_1 + x_2 = 5 $$\n    $$ x_1 \\geq 0, \\quad x_2 \\geq 0 $$\n3.  **Dual Feasibility**:\n    $$ \\mu_1 \\geq 0, \\quad \\mu_2 \\geq 0 $$\n4.  **Complementary Slackness**:\n    $$ \\mu_1 x_1 = 0, \\quad \\mu_2 x_2 = 0 $$\n\nWe solve this system. The complementary slackness conditions suggest analyzing cases based on which non-negativity constraints are active. Let's assume the optimal solution is in the interior of the feasible region for $x$, i.e., $x_1  0$ and $x_2  0$.\n\nCase 1: $x_1  0$ and $x_2  0$.\nFrom complementary slackness, we must have $\\mu_1 = 0$ and $\\mu_2 = 0$. The stationarity conditions simplify to:\n(1) $4x_1 + 2x_2 + \\nu = 8$\n(2) $2x_1 + 3x_2 + \\nu = 5$\nWe also have the primal feasibility condition:\n(3) $x_1 + x_2 = 5$\n\nSubtracting equation (2) from (1) eliminates $\\nu$:\n$(4x_1 - 2x_1) + (2x_2 - 3x_2) = 8 - 5 \\implies 2x_1 - x_2 = 3$.\nWe now have a system of two linear equations for $x_1$ and $x_2$:\n$$ 2x_1 - x_2 = 3 $$\n$$ x_1 + x_2 = 5 $$\nAdding these two equations gives $3x_1 = 8$, so $x_1 = \\frac{8}{3}$.\nSubstituting back into $x_1 + x_2 = 5$ gives $x_2 = 5 - \\frac{8}{3} = \\frac{15-8}{3} = \\frac{7}{3}$.\nWe must verify our initial assumption: $x_1 = \\frac{8}{3}  0$ and $x_2 = \\frac{7}{3}  0$. The assumption is consistent.\nNow we find the value of $\\nu$ using equation (2):\n$2(\\frac{8}{3}) + 3(\\frac{7}{3}) + \\nu = 5$\n$\\frac{16}{3} + \\frac{21}{3} + \\nu = 5$\n$\\frac{37}{3} + \\nu = 5 \\implies \\nu = 5 - \\frac{37}{3} = \\frac{15-37}{3} = -\\frac{22}{3}$.\n\nThe candidate solution is $x^* = (\\frac{8}{3}, \\frac{7}{3})$, $\\mu^* = (0, 0)$, $\\nu^* = -\\frac{22}{3}$. All KKT conditions are met: primal feasibility ($x_1+x_2 = \\frac{15}{3} = 5$, $x_1,x_2  0$), dual feasibility ($\\mu_1, \\mu_2 \\geq 0$), and complementary slackness hold. Since the problem is strictly convex, this is the unique global optimum.\n\nThe optimal Lagrange multiplier associated with the equality constraint $a^{\\top}x = b$ is $\\nu^* = -\\frac{22}{3}$.\nIn the context of the CPS, the constraint $x_1 + x_2 = 5$ represents the total shared bandwidth of $5$ units. The multiplier $\\nu^*$ is the shadow price associated with this resource constraint.\nAccording to the envelope theorem interpretation derived in Part 1, the derivative of the optimal cost $p^*(b)$ with respect to the resource limit $b$ is given by $\\frac{dp^*}{db} = -\\nu^*$.\nThe marginal change in the optimal objective value with respect to $b$ at $b=5$ is:\n$$ -\\nu^* = - \\left(-\\frac{22}{3}\\right) = \\frac{22}{3} $$\nThis value represents the marginal benefit of increasing the shared bandwidth. An increase in total bandwidth by one unit would lead to a decrease in the minimum achievable performance cost by $\\frac{22}{3}$ cost units. Therefore, the marginal value, or shadow price, of the bandwidth resource is $\\frac{22}{3}$ cost units per unit of bandwidth. The requested quantity is this marginal change.",
            "answer": "$$\n\\boxed{\\frac{22}{3}}\n$$"
        },
        {
            "introduction": "Decision-making rarely involves a single, clear-cut objective; more often, we face a landscape of competing goals, such as minimizing energy use while also minimizing production time. This practice moves beyond single-objective optimization to address the challenge of multi-objective decision support . You will explore the concept of Pareto optimality and use the weighted-sum scalarization method to navigate the trade-offs between conflicting performance metrics, learning a fundamental technique for making informed decisions when no single \"best\" solution exists.",
            "id": "4235973",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) is used to support prescriptive decision-making for a manufacturing cell that can operate under one of several control policies. The DT continuously estimates two competing operational performance outcomes for each policy: average energy per unit, denoted by $f_{1}(x)$ in $\\mathrm{kJ}$, and average cycle-time deviation from a target, denoted by $f_{2}(x)$ in $\\mathrm{s}$. The decision-maker seeks to minimize both objectives simultaneously.\n\nStarting from the core definitions of Pareto dominance and Pareto optimality for multiobjective minimization, and assuming the following base principles for the decision-maker’s scalarization of vector-valued costs:\n- Monotonicity with respect to each objective (if all objective values weakly improve and at least one strictly improves, the scalarized value strictly improves),\n- Positive homogeneity with respect to scaling of objective magnitudes,\n- Additivity for separable objectives,\n\nderive a scalar objective $g(x)$ that induces a total preorder over decisions consistent with Pareto improvements. Then, using convex analysis of the attainable objective set in objective space, establish conditions under which minimizing $g(x)$ yields Pareto optimal solutions when the Pareto front is convex, and explain the limitation that arises when the Pareto front is non-convex.\n\nThe DT has evaluated three discrete control policies, $x_{A}$, $x_{B}$, and $x_{C}$, yielding the following two-dimensional objective vectors:\n- $f(x_{A}) = (10\\,\\mathrm{kJ},\\, 6\\,\\mathrm{s})$,\n- $f(x_{B}) = (12\\,\\mathrm{kJ},\\, 5.3\\,\\mathrm{s})$,\n- $f(x_{C}) = (14\\,\\mathrm{kJ},\\, 3.8\\,\\mathrm{s})$.\n\nAssume a two-objective specialization of your derived scalarization where a single weight $w \\in (0,1)$ modulates the relative importance of $f_{1}(x)$ versus $f_{2}(x)$ in a way that is consistent with the principles above. Determine the unique value $w^{\\star} \\in (0,1)$ at which decisions $x_{A}$ and $x_{C}$ are exactly tied under the scalarization. Based on this analysis, and the geometric structure of the attainable objective set, argue whether $x_{B}$ can be selected by any $w \\in (0,1)$ when the Pareto front is non-convex.\n\nExpress the final value of $w^{\\star}$ as a reduced fraction. No rounding is required and no units should be included with $w^{\\star}$.",
            "solution": "The decision problem involves two objectives to be minimized, $f_{1}(x)$ and $f_{2}(x)$, with vector objective $\\mathbf{f}(x) = \\big(f_{1}(x), f_{2}(x)\\big)$. A decision $x$ Pareto dominates $y$ if $\\mathbf{f}(x) \\le \\mathbf{f}(y)$ componentwise and at least one inequality is strict. A decision is Pareto optimal if no other decision dominates it.\n\nWe seek a scalarization $g(x)$ that maps $\\mathbf{f}(x)$ to a scalar and is consistent with the decision-maker’s principles:\n- Monotonicity: If $\\mathbf{z}, \\mathbf{z}' \\in \\mathbb{R}^{m}$ satisfy $\\mathbf{z} \\le \\mathbf{z}'$ componentwise and $z_{i}  z'_{i}$ for some $i$, then $g(\\mathbf{z})  g(\\mathbf{z}')$.\n- Positive homogeneity: For $\\alpha \\ge 0$, $g(\\alpha \\mathbf{z}) = \\alpha g(\\mathbf{z})$.\n- Additivity for separable objectives: For $\\mathbf{z}, \\mathbf{z}' \\in \\mathbb{R}^{m}$, $g(\\mathbf{z} + \\mathbf{z}') = g(\\mathbf{z}) + g(\\mathbf{z}')$.\n\nAssuming separability over components, posit $g(\\mathbf{z}) = \\sum_{i=1}^{m} h_{i}(z_{i})$. Additivity together with positive homogeneity constrains each $h_{i}$ to be a linear function: for all $u, v \\in \\mathbb{R}$ and $\\alpha \\ge 0$, $h_{i}(u+v) = h_{i}(u) + h_{i}(v)$ and $h_{i}(\\alpha u) = \\alpha h_{i}(u)$, and monotonicity requires $h_{i}$ to be strictly increasing. The only functions satisfying these properties and continuity (a standard regularity assumption for decision support in engineering systems) are $h_{i}(z_{i}) = w_{i} z_{i}$ with weights $w_{i}  0$. Therefore,\n$$\ng(\\mathbf{f}(x)) \\;=\\; \\sum_{i=1}^{m} w_{i} f_{i}(x),\n$$\nwith $w_{i}  0$ encoding the decision-maker’s relative emphasis. This scalarization is order-preserving with respect to Pareto dominance due to positivity of $w_{i}$: if $\\mathbf{f}(x) \\le \\mathbf{f}(y)$ and at least one inequality is strict, then $g(\\mathbf{f}(x))  g(\\mathbf{f}(y))$.\n\nWhen the attainable objective set $\\mathcal{Y} = \\{\\mathbf{f}(x) : x \\in \\mathcal{X}\\} \\subset \\mathbb{R}^{m}$ is convex and we consider minimizing $g(\\mathbf{y})$ over $\\mathbf{y} \\in \\mathcal{Y}$ with $w_{i}  0$, the minimizer is Pareto optimal because any strict improvement in a component reduces the scalarized value. Moreover, by the supporting hyperplane theorem from convex analysis, each Pareto optimal point on a convex lower boundary of $\\mathcal{Y}$ admits a supporting hyperplane with positive normal $\\mathbf{w} \\in \\mathbb{R}^{m}_{++}$ such that it minimizes $\\mathbf{w}^{\\top} \\mathbf{y}$ over $\\mathcal{Y}$. Thus, in convex settings, weighted-sum scalarization recovers all Pareto optimal points.\n\nIn contrast, when the Pareto front is non-convex, there exist Pareto optimal points that are not supported by any hyperplane with positive normal; these are called unsupported Pareto points. Linear scalarization with $\\sum_{i} w_{i} f_{i}(x)$ cannot select such points for any choice of positive weights, which is a fundamental limitation.\n\nWe now specialize to two objectives and the given three policies. Let $w \\in (0,1)$ be the weight on $f_{1}(x)$ and $1-w$ be the weight on $f_{2}(x)$. The scalarized objective is\n$$\ng_{w}(x) \\;=\\; w f_{1}(x) + (1-w) f_{2}(x).\n$$\nCompute the scalarized values for each policy:\n- For $x_{A}$ with $f(x_{A}) = (10, 6)$:\n$$\ng_{w}(x_{A}) \\;=\\; w \\cdot 10 + (1-w) \\cdot 6 \\;=\\; 6 + 4w.\n$$\n- For $x_{B}$ with $f(x_{B}) = (12, 5.3)$:\n$$\ng_{w}(x_{B}) \\;=\\; w \\cdot 12 + (1-w) \\cdot 5.3 \\;=\\; 5.3 + 6.7w.\n$$\n- For $x_{C}$ with $f(x_{C}) = (14, 3.8)$:\n$$\ng_{w}(x_{C}) \\;=\\; w \\cdot 14 + (1-w) \\cdot 3.8 \\;=\\; 3.8 + 10.2w.\n$$\n\nDetermine the unique $w^{\\star} \\in (0,1)$ such that $g_{w^{\\star}}(x_{A}) = g_{w^{\\star}}(x_{C})$:\n$$\n6 + 4w^{\\star} \\;=\\; 3.8 + 10.2 w^{\\star}\n\\;\\;\\Longrightarrow\\;\\;\n6 - 3.8 \\;=\\; (10.2 - 4) w^{\\star}\n\\;\\;\\Longrightarrow\\;\\;\n2.2 \\;=\\; 6.2\\, w^{\\star}\n\\;\\;\\Longrightarrow\\;\\;\nw^{\\star} \\;=\\; \\frac{2.2}{6.2} \\;=\\; \\frac{11}{31}.\n$$\n\nTo analyze whether $x_B$ can be optimal for any $w \\in (0,1)$, we find the conditions under which it is preferred to (i.e., has a lower scalarized cost than) both $x_A$ and $x_C$.\n- $x_B$ is preferred to $x_A$ if $g_w(x_B)  g_w(x_A) \\iff 5.3 + 6.7w  6 + 4w \\iff 2.7w  0.7 \\iff w  7/27$.\n- $x_B$ is preferred to $x_C$ if $g_w(x_B)  g_w(x_C) \\iff 5.3 + 6.7w  3.8 + 10.2w \\iff 1.5  3.5w \\iff w > 3/7$.\nSince the conditions $w  7/27$ and $w > 3/7$ are mutually exclusive (as $7/27  3/7$), there is no weight $w \\in (0,1)$ for which $x_B$ is simultaneously preferred to both other options. Therefore, $x_B$ is never the optimal choice under this scalarization.\n\nGeometrically, the attainable objective points are $\\{(10,6), (12,5.3), (14,3.8)\\}$. The point $(12,5.3)$ lies strictly \"above\" the line segment connecting $(10,6)$ and $(14,3.8)$ in the objective space for minimization, so it is an unsupported Pareto point. Linear scalarization corresponds to finding the first point touched by a line with normal $(w, 1-w)$ as it sweeps over the attainable set; only the supported extreme points $(10,6)$ and $(14,3.8)$ can be selected. This illustrates the limitation of weighted-sum scalarization when the Pareto front is non-convex.\n\nThus, the unique tie weight is $w^{\\star} = \\frac{11}{31}$, and $x_{B}$ is never optimal for any $w \\in (0,1)$ under the derived scalarization.",
            "answer": "$$\\boxed{\\frac{11}{31}}$$"
        },
        {
            "introduction": "Effective prescriptive analytics must contend with the reality of an uncertain future, from unpredictable processing times to fluctuating market demands. This final practice introduces robust optimization, a powerful paradigm for creating decisions that are resilient to such uncertainty . You will formulate and solve a robust scheduling problem under a budgeted uncertainty model, and in doing so, quantify the \"Price of Robustness\"—the potential performance cost of ensuring a solution is resilient against worst-case scenarios.",
            "id": "4235993",
            "problem": "A single-machine scheduling module within a digital twin for a cyber-physical system must sequence jobs whose processing times are uncertain. The prescriptive analytics requirement is to compute a schedule that is robust to bounded uncertainty using a budgeted model of uncertainty and to quantify the trade-off between robustness and nominal performance. Consider a set of $n$ independent jobs to be processed non-preemptively on a single machine. Let the nominal processing time of job $i$ be $\\bar{p}_i \\ge 0$, and the processing time uncertainty bound be $\\hat{d}_i \\ge 0$. The realized processing time is $p_i = \\bar{p}_i + \\hat{d}_i \\xi_i$, where the uncertainty vector $\\xi \\in \\mathbb{R}^n$ satisfies $0 \\le \\xi_i \\le 1$ for all $i$ and $\\sum_{i=1}^n \\xi_i \\le \\Gamma$ for a given uncertainty budget $\\Gamma \\ge 0$. The objective is to minimize the worst-case total completion time over the uncertainty set, subject to valid assignment (each job appears exactly once, and each position is filled by exactly one job). All modeling assumptions must respect standard single-machine scheduling definitions (no preemption, deterministic sequencing, and completion times defined by cumulative processing along the sequence). You must proceed from the fundamental definitions of completion times and the budgeted uncertainty set to obtain a robust formulation that is solvable as a mixed-integer optimization problem.\n\nTasks:\n1) Starting from the fundamental definitions of completion time and the budgeted uncertainty set, formulate the min-max robust optimization problem for the total completion time objective under the specified uncertainty model. Then derive an equivalent robust mixed-integer linear optimization formulation without max operators by using standard dualization of the inner maximization.\n2) For any fixed sequence, express the worst-case total completion time under budgeted uncertainty in a way that can be computed in polynomial time without solving a mixed-integer optimization. Justify the correctness of your expression using the structure of the budgeted uncertainty set and linearity of the objective in the processing times.\n3) Implement an algorithm that, given a small number of jobs, enumerates all feasible sequences and computes the robust optimal value for a specified $\\Gamma$, along with the nominal optimal value for $\\Gamma = 0$. Define the price of robustness as $\\text{PoR}(\\Gamma) = V_{\\text{robust}}(\\Gamma) / V_{\\text{nominal}}(0)$, where $V_{\\text{robust}}(\\Gamma)$ is the optimal robust worst-case total completion time and $V_{\\text{nominal}}(0)$ is the optimal total completion time with no uncertainty. Use this algorithm to solve the test suite below.\n\nTest suite (each test case is specified by $(\\bar{p}, \\hat{d}, \\Gamma)$):\n- Test $1$: $n = 5$, $\\bar{p} = [3, 6, 2, 7, 4]$, $\\hat{d} = [1, 2, 1, 3, 2]$, $\\Gamma = 1.5$.\n- Test $2$: $n = 5$, $\\bar{p} = [3, 6, 2, 7, 4]$, $\\hat{d} = [1, 2, 1, 3, 2]$, $\\Gamma = 0$.\n- Test $3$: $n = 5$, $\\bar{p} = [3, 6, 2, 7, 4]$, $\\hat{d} = [1, 2, 1, 3, 2]$, $\\Gamma = 5$.\n- Test $4$: $n = 4$, $\\bar{p} = [5, 5, 5, 5]$, $\\hat{d} = [0, 1, 0, 2]$, $\\Gamma = 2$.\n- Test $5$: $n = 3$, $\\bar{p} = [2, 9, 4]$, $\\hat{d} = [5, 1, 0]$, $\\Gamma = 0.7$.\n\nOutput specification:\n- For each test case, compute the pair $[V_{\\star}, \\text{PoR}]$, where $V_{\\star}$ is the optimal robust worst-case total completion time for the specified $\\Gamma$, and $\\text{PoR}$ is the price of robustness as defined above. Both $V_{\\star}$ and $\\text{PoR}$ must be output as real numbers (floats).\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list. For example, an output with two test cases would look like $[[v_1, r_1],[v_2, r_2]]$.\n- Round all real numbers to six decimal places. No physical units are involved. Angles are not used. Do not express any number as a percentage; if a fraction is needed, output it in decimal form.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard problem in robust scheduling theory, specifically minimizing the worst-case total completion time on a single machine under budgeted uncertainty. The provided parameters and constraints are complete and sufficient for deriving a solution. The tasks guide a principled derivation from first principles to an algorithmic implementation, which is appropriate for a problem of this nature. Therefore, the problem is deemed valid.\n\nThe solution proceeds in three parts as requested. First, we derive the mixed-integer linear programming (MILP) formulation. Second, we develop a polynomial-time algorithm for evaluating the worst-case objective for a fixed sequence. Third, we describe the implementation strategy for solving the problem instances by enumeration.\n\n### Part 1: Robust Mixed-Integer Optimization Formulation\n\nLet the set of jobs be $\\mathcal{N} = \\{1, 2, \\dots, n\\}$. A schedule is a permutation of these jobs. We introduce binary decision variables $x_{ij}$ where $x_{ij} = 1$ if job $i \\in \\mathcal{N}$ is scheduled in position $j \\in \\{1, \\dots, n\\}$, and $x_{ij} = 0$ otherwise. The constraints that ensure $x$ represents a valid permutation are:\n$$ \\sum_{i=1}^n x_{ij} = 1 \\quad \\forall j \\in \\{1, \\dots, n\\} $$\n$$ \\sum_{j=1}^n x_{ij} = 1 \\quad \\forall i \\in \\mathcal{N} $$\n$$ x_{ij} \\in \\{0, 1\\} \\quad \\forall i \\in \\mathcal{N}, j \\in \\{1, \\dots, n\\} $$\n\nThe processing time of job $i$ is given by $p_i = \\bar{p}_i + \\hat{d}_i \\xi_i$, where the uncertainty vector $\\xi = (\\xi_1, \\dots, \\xi_n)$ belongs to the uncertainty set $U$:\n$$ U = \\left\\{ \\xi \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n \\xi_i \\le \\Gamma, 0 \\le \\xi_i \\le 1, \\forall i \\in \\mathcal{N} \\right\\} $$\n\nThe total completion time, $\\sum_{k=1}^n C_k$, for a given sequence can be expressed as a weighted sum of processing times. If job $i$ is in position $j$, its processing time $p_i$ contributes to its own completion time and the completion times of the $n-j$ jobs that follow it. Thus, $p_i$ is summed $n-j+1$ times. The position of job $i$ can be written as $\\sum_{j=1}^n j \\cdot x_{ij}$. Therefore, the total completion time is:\n$$ \\sum C_k = \\sum_{i=1}^n \\left(n - \\left(\\sum_{j=1}^n j \\cdot x_{ij}\\right) + 1\\right) p_i $$\n\nSubstituting the expression for $p_i$, we obtain:\n$$ \\sum C_k = \\sum_{i=1}^n \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) (\\bar{p}_i + \\hat{d}_i \\xi_i) $$\n$$ = \\sum_{i=1}^n \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) \\bar{p}_i + \\sum_{i=1}^n \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) \\hat{d}_i \\xi_i $$\n\nThe objective is to find a schedule (an assignment $x$) that minimizes the worst-case total completion time over all possible realizations of $\\xi \\in U$. This leads to the min-max formulation:\n$$ \\min_{x} \\left\\{ \\sum_{i=1}^n \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) \\bar{p}_i + \\max_{\\xi \\in U} \\left\\{ \\sum_{i=1}^n \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) \\hat{d}_i \\xi_i \\right\\} \\right\\} $$\n\nThe inner maximization problem is a linear program for a fixed $x$:\n$$\n\\begin{array}{lll}\n\\text{(Primal)}  \\max_{\\xi}  \\sum_{i=1}^n c_i(x) \\xi_i \\\\\n \\text{s.t.}  \\sum_{i=1}^n \\xi_i \\le \\Gamma \\\\\n \\xi_i \\le 1, \\quad \\forall i \\in \\mathcal{N} \\\\\n \\xi_i \\ge 0, \\quad \\forall i \\in \\mathcal{N}\n\\end{array}\n$$\nwhere $c_i(x) = (n+1 - \\sum_{j=1}^n j x_{ij}) \\hat{d}_i$. We can dualize this LP. Let $\\theta \\ge 0$ be the dual variable for the budget constraint $\\sum \\xi_i \\le \\Gamma$, and $\\lambda_i \\ge 0$ be the dual variables for the box constraints $\\xi_i \\le 1$. The dual problem is:\n$$\n\\begin{array}{lll}\n\\text{(Dual)}  \\min_{\\theta, \\lambda}  \\Gamma \\theta + \\sum_{i=1}^n \\lambda_i \\\\\n \\text{s.t.}  \\theta + \\lambda_i \\ge c_i(x), \\quad \\forall i \\in \\mathcal{N} \\\\\n \\theta \\ge 0, \\lambda_i \\ge 0, \\quad \\forall i \\in \\mathcal{N}\n\\end{array}\n$$\n\nBy strong duality, the optimal values of the primal and dual are equal. We can replace the inner maximization in the min-max formulation with this dual minimization problem. This results in a single-level minimization problem:\n$$ \\min_{x, \\theta, \\lambda} \\left\\{ \\sum_{i=1}^n \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) \\bar{p}_i + \\Gamma \\theta + \\sum_{i=1}^n \\lambda_i \\right\\} $$\nsubject to the dual constraints, assignment constraints on $x$, and non-negativity of dual variables. The constraint $\\theta + \\lambda_i \\ge c_i(x)$ can be written as:\n$$ \\theta + \\lambda_i \\ge \\left(n+1 - \\sum_{j=1}^n j x_{ij}\\right) \\hat{d}_i $$\nwhich is linear in $x_{ij}$. The objective function is also linear in all its variables. Thus, we have the following mixed-integer linear program:\n$$\n\\begin{array}{lll}\n\\min  \\sum_{i=1}^n \\sum_{j=1}^n (n+1-j) \\bar{p}_i x_{ij} + \\Gamma \\theta + \\sum_{i=1}^n \\lambda_i \\\\\n\\text{s.t.}  \\theta + \\lambda_i \\ge (n+1)\\hat{d}_i - \\sum_{j=1}^n j \\hat{d}_i x_{ij},  \\forall i \\in \\mathcal{N} \\\\\n \\sum_{i=1}^n x_{ij} = 1,  \\forall j \\in \\{1, \\dots, n\\} \\\\\n \\sum_{j=1}^n x_{ij} = 1,  \\forall i \\in \\mathcal{N} \\\\\n x_{ij} \\in \\{0, 1\\},  \\forall i \\in \\mathcal{N}, j \\in \\{1, \\dots, n\\} \\\\\n \\theta \\ge 0, \\lambda_i \\ge 0,  \\forall i \\in \\mathcal{N}\n\\end{array}\n$$\nIn the objective function, we used the identity $\\sum_{i=1}^n (n+1 - \\sum_j j x_{ij}) \\bar{p}_i = \\sum_{i=1}^n \\sum_{j=1}^n (n+1-j) \\bar{p}_i x_{ij}$. This is the required MILP formulation.\n\n### Part 2: Polynomial-Time Algorithm for a Fixed Sequence\n\nFor a fixed sequence $\\pi$, the position of each job $i$, denoted $pos_\\pi(i)$, is fixed. The worst-case total completion time is:\n$$ WCT(\\pi) = \\sum_{i=1}^n (n+1-pos_\\pi(i))\\bar{p}_i + \\max_{\\xi \\in U} \\left\\{ \\sum_{i=1}^n (n+1-pos_\\pi(i))\\hat{d}_i \\xi_i \\right\\} $$\nLet $w_i = (n+1-pos_\\pi(i))\\hat{d}_i$. These are fixed coefficients for a given $\\pi$. The inner maximization is a continuous knapsack problem:\n$$ \\max \\sum_{i=1}^n w_i \\xi_i \\quad \\text{s.t.} \\quad \\sum_{i=1}^n \\xi_i \\le \\Gamma, \\quad 0 \\le \\xi_i \\le 1 \\quad \\forall i $$\nThe optimal solution to this linear program can be found with a greedy algorithm. The \"value\" of increasing each $\\xi_i$ is $w_i$, while the \"cost\" in terms of the budget $\\Gamma$ is $1$. The algorithm sorts the jobs in descending order of their corresponding coefficient $w_i$. Let the sorted indices be $k_1, k_2, \\dots, k_n$ such that $w_{k_1} \\ge w_{k_2} \\ge \\dots \\ge w_{k_n}$. The algorithm then allocates the budget $\\Gamma$ to the $\\xi_i$ variables in this order.\nSpecifically, let $\\gamma = \\Gamma$ be the remaining budget. For $j = 1, \\dots, n$:\n1. Let $i = k_j$.\n2. The optimal value for $\\xi_i$ is $\\xi_i^* = \\min(1, \\gamma)$.\n3. Update the budget: $\\gamma \\leftarrow \\gamma - \\xi_i^*$.\n\nIf $\\Gamma$ is not an integer, say $\\Gamma = K + f$ where $K = \\lfloor \\Gamma \\rfloor$ and $f \\in (0,1)$, this corresponds to setting $\\xi_{k_j}^*=1$ for $j=1, \\dots, K$, $\\xi_{k_{K+1}}^* = f$, and $\\xi_{k_j}^*=0$ for $j  K+1$.\nThe total complexity of this procedure is dominated by the sorting step, which is $O(n \\log n)$, hence it is a polynomial-time algorithm.\n\n### Part 3: Algorithmic Implementation via Enumeration\n\nFor the small values of $n$ given in the test suite ($n \\le 5$), the total number of feasible sequences, $n!$, is small ($5! = 120$). We can find the exact robust optimal schedule by enumerating all possible sequences and, for each sequence, computing its worst-case total completion time using the polynomial-time algorithm from Part 2. The sequence with the minimum worst-case total completion time is the robust optimal solution.\n\nThe algorithm proceeds as follows:\n1.  **Compute Nominal Optimum $V_{\\text{nominal}}(0)$**: The optimal schedule for the nominal problem (where all $p_i = \\bar{p}_i$) is given by the Shortest Processing Time (SPT) rule. Sort the jobs in non-decreasing order of their nominal processing times $\\bar{p}_i$. Compute the total completion time for this sequence. This value is $V_{\\text{nominal}}(0)$.\n2.  **Compute Robust Optimum $V_{\\text{robust}}(\\Gamma)$**:\n    a. Initialize a variable `min_worst_case_tct` to a very large number.\n    b. Generate every permutation $\\pi$ of the jobs $\\{1, ..., n\\}$.\n    c. For each permutation $\\pi$:\n        i. Calculate the nominal component of the total completion time: $\\sum_{i=1}^n (n+1-pos_\\pi(i))\\bar{p}_i$.\n        ii. Calculate the coefficients for the uncertainty term: $w_i = (n+1-pos_\\pi(i))\\hat{d}_i$ for all $i \\in \\mathcal{N}$.\n        iii. Compute the maximum deviation due to uncertainty, $\\max \\sum w_i\\xi_i$, using the greedy algorithm described in Part 2.\n        iv. The worst-case total completion time for sequence $\\pi$ is the sum of the nominal component and the maximum deviation.\n        v. Update `min_worst_case_tct = min(min_worst_case_tct, current_worst_case_tct)`.\n    d. The final value of `min_worst_case_tct` is $V_{\\text{robust}}(\\Gamma)$.\n3.  **Compute Price of Robustness (PoR)**: Calculate $\\text{PoR}(\\Gamma) = V_{\\text{robust}}(\\Gamma) / V_{\\text{nominal}}(0)$. If $V_{\\text{nominal}}(0) = 0$, the PoR is taken to be $1.0$ if $V_{\\text{robust}}(\\Gamma)=0$ and infinity otherwise. For the given problems, $V_{\\text{nominal}}(0)0$.\n\nThis procedure is implemented to solve the provided test suite.",
            "answer": "[[66.5, 1.254717],[53.0, 1.0],[94.0, 1.773585],[54.0, 1.08],[33.5, 1.456522]]"
        }
    ]
}