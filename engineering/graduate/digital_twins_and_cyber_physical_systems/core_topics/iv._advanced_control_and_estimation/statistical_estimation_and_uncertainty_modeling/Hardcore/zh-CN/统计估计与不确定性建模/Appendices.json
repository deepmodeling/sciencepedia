{
    "hands_on_practices": [
        {
            "introduction": "在统计估计中，无偏性通常被认为是一个理想的属性，但它并非总是最佳选择。本练习旨在通过均方误差（MSE）这一更全面的性能指标，探讨有偏估计量与无偏估计量之间的权衡。通过一个具体的计算，我们将揭示在某些情况下，引入少量偏差可以显著降低估计量的方差，从而获得更低的整体误差，这正是偏差-方差权衡的核心思想。",
            "id": "4247291",
            "problem": "一个信息物理系统（CPS）的数字孪生（DT）的任务是估计一个影响测量通道的标量校准参数 $\\theta$。CPS根据数据生成过程 $y_{i} = \\theta + \\varepsilon_{i}$ 提供了 $n$ 个同步样本 $\\{y_{i}\\}_{i=1}^{n}$，其中 $\\varepsilon_{i}$ 是独立的、零均值的扰动，其方差为 $\\sigma^{2}$。该DT的建模流程假设样本间具有同方差性和独立性。\n\n为了补偿在系统辨识中识别出的模型失配，DT部署了一个有偏估计器 $\\hat{\\theta}_{b} = \\bar{y} + c$，其中 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$，$c$ 是一个已知的常数偏移量。参考的无偏估计器是 $\\hat{\\theta}_{u} = \\bar{y}$。仅使用期望、方差、偏差以及在欧几里得范数平方损失（通常称为 $L_{2}$ 损失）下的均方误差的核心定义，推导 $\\hat{\\theta}_{b}$ 和 $\\hat{\\theta}_{u}$ 的均方误差，并计算下面要求的比较值。\n\n给定 $n = 36$，$\\sigma^{2} = 0.09$ 和 $c = 0.05$，计算比率 $\\mathrm{MSE}(\\hat{\\theta}_{b}) / \\mathrm{MSE}(\\hat{\\theta}_{u})$。将最终答案表示为一个四舍五入到四位有效数字的纯数字。",
            "solution": "需对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- 数据生成过程：$y_{i} = \\theta + \\varepsilon_{i}$，其中 $i=1, \\dots, n$。\n- 扰动 $\\varepsilon_{i}$ 是独立同分布（i.i.d.）的，其期望 $\\mathbb{E}[\\varepsilon_{i}] = 0$，方差 $\\mathrm{Var}(\\varepsilon_{i}) = \\sigma^{2}$。\n- 样本数量：$n$。\n- 样本均值：$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$。\n- 无偏估计器：$\\hat{\\theta}_{u} = \\bar{y}$。\n- 有偏估计器：$\\hat{\\theta}_{b} = \\bar{y} + c$，其中 $c$ 是一个已知常数。\n- 损失函数：均方误差（MSE），定义为 $\\mathrm{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2]$。\n- 给定的数值：$n = 36$，$\\sigma^{2} = 0.09$，$c = 0.05$。\n- 任务：计算比率 $\\mathrm{MSE}(\\hat{\\theta}_{b}) / \\mathrm{MSE}(\\hat{\\theta}_{u})$ 并将结果四舍五入到四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，因为它基于统计估计理论的基本原理（偏差、方差、均方误差），这些是信息物理系统和数字孪生建模与分析的核心。该问题是适定的（well-posed），提供了所有必要的数据和定义以得出一个唯一的、有意义的解。语言客观且正式。设置完整且内部一致。该模型是常数信号被加性噪声污染的标准表示，且数值是合理的。\n\n### 步骤3：结论与行动\n该问题有效。将提供完整解法。\n\n解题过程首先推导两个估计器 $\\hat{\\theta}_{u}$ 和 $\\hat{\\theta}_{b}$ 的均方误差（MSE）表达式，然后使用给定的数值计算它们的比率。MSE的一个基本性质是它可以分解为估计器偏差的平方和方差的和：\n$$\n\\mathrm{MSE}(\\hat{\\theta}) = (\\mathrm{Bias}(\\hat{\\theta}))^2 + \\mathrm{Var}(\\hat{\\theta})\n$$\n其中偏差为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$，方差为 $\\mathrm{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2]$。\n\n首先，我们分析样本均值 $\\bar{y}$ 的性质，它是两个估计器的组成部分。\n样本均值的期望是：\n$$\n\\mathbb{E}[\\bar{y}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[y_{i}]\n$$\n给定 $y_i = \\theta + \\varepsilon_i$ 和 $\\mathbb{E}[\\varepsilon_i] = 0$，单个样本的期望是 $\\mathbb{E}[y_i] = \\mathbb{E}[\\theta + \\varepsilon_i] = \\theta + \\mathbb{E}[\\varepsilon_i] = \\theta$。\n因此，\n$$\n\\mathbb{E}[\\bar{y}] = \\frac{1}{n}\\sum_{i=1}^{n} \\theta = \\frac{1}{n}(n\\theta) = \\theta\n$$\n样本均值的方差是：\n$$\n\\mathrm{Var}(\\bar{y}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\right) = \\frac{1}{n^2}\\mathrm{Var}\\left(\\sum_{i=1}^{n} y_{i}\\right)\n$$\n由于扰动 $\\varepsilon_i$ 是独立的，样本 $y_i$ 也是独立的。因此，和的方差是方差的和：\n$$\n\\mathrm{Var}(\\bar{y}) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Var}(y_i)\n$$\n单个样本的方差是 $\\mathrm{Var}(y_i) = \\mathrm{Var}(\\theta + \\varepsilon_i) = \\mathrm{Var}(\\varepsilon_i) = \\sigma^2$，因为 $\\theta$ 是一个常数。\n因此，\n$$\n\\mathrm{Var}(\\bar{y}) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sigma^2 = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}\n$$\n\n现在，我们可以计算每个估计器的MSE。\n\n对于无偏估计器 $\\hat{\\theta}_{u} = \\bar{y}$：\n期望是 $\\mathbb{E}[\\hat{\\theta}_{u}] = \\mathbb{E}[\\bar{y}] = \\theta$。\n偏差是 $\\mathrm{Bias}(\\hat{\\theta}_{u}) = \\mathbb{E}[\\hat{\\theta}_{u}] - \\theta = \\theta - \\theta = 0$。\n方差是 $\\mathrm{Var}(\\hat{\\theta}_{u}) = \\mathrm{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}$。\n无偏估计器的MSE是：\n$$\n\\mathrm{MSE}(\\hat{\\theta}_{u}) = (\\mathrm{Bias}(\\hat{\\theta}_{u}))^2 + \\mathrm{Var}(\\hat{\\theta}_{u}) = 0^2 + \\frac{\\sigma^2}{n} = \\frac{\\sigma^2}{n}\n$$\n\n对于有偏估计器 $\\hat{\\theta}_{b} = \\bar{y} + c$：\n期望是 $\\mathbb{E}[\\hat{\\theta}_{b}] = \\mathbb{E}[\\bar{y} + c] = \\mathbb{E}[\\bar{y}] + c = \\theta + c$。\n偏差是 $\\mathrm{Bias}(\\hat{\\theta}_{b}) = \\mathbb{E}[\\hat{\\theta}_{b}] - \\theta = (\\theta + c) - \\theta = c$。\n方差是 $\\mathrm{Var}(\\hat{\\theta}_{b}) = \\mathrm{Var}(\\bar{y} + c)$。由于 $c$ 是一个常数，它不影响方差，所以 $\\mathrm{Var}(\\hat{\\theta}_{b}) = \\mathrm{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}$。\n有偏估计器的MSE是：\n$$\n\\mathrm{MSE}(\\hat{\\theta}_{b}) = (\\mathrm{Bias}(\\hat{\\theta}_{b}))^2 + \\mathrm{Var}(\\hat{\\theta}_{b}) = c^2 + \\frac{\\sigma^2}{n}\n$$\n\n最后，我们计算所要求的比率：\n$$\n\\frac{\\mathrm{MSE}(\\hat{\\theta}_{b})}{\\mathrm{MSE}(\\hat{\\theta}_{u})} = \\frac{c^2 + \\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n}} = \\frac{c^2}{\\frac{\\sigma^2}{n}} + \\frac{\\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n}} = \\frac{nc^2}{\\sigma^2} + 1\n$$\n我们代入给定的数值：$n = 36$，$\\sigma^{2} = 0.09$ 和 $c = 0.05$。\n$$\n\\frac{\\mathrm{MSE}(\\hat{\\theta}_{b})}{\\mathrm{MSE}(\\hat{\\theta}_{u})} = \\frac{36 \\times (0.05)^2}{0.09} + 1\n$$\n首先，计算分子中的项：$36 \\times (0.05)^2 = 36 \\times 0.0025 = 0.09$。\n现在将此代回表达式中：\n$$\n\\frac{\\mathrm{MSE}(\\hat{\\theta}_{b})}{\\mathrm{MSE}(\\hat{\\theta}_{u})} = \\frac{0.09}{0.09} + 1 = 1 + 1 = 2\n$$\n问题要求答案表示为四舍五入到四位有效数字的纯数字。由于结果恰好是 $2$，我们将其写为 $2.000$ 以明确表示所需的精度。",
            "answer": "$$\\boxed{2.000}$$"
        },
        {
            "introduction": "在许多数字孪生和信息物理系统的应用中，我们并非从零开始进行参数估计，而是拥有来自先前实验或物理模型的宝贵先验知识。本练习将引导你从频率派的最大似然估计转向贝叶斯方法，推导最大后验（MAP）估计量。你将看到，通过贝叶斯框架，我们可以将数据证据与先验信念相结合，产生向先验均值“收缩”的稳健估计。",
            "id": "4247342",
            "problem": "一个信息物理系统（CPS）数字孪生被用于持续校准一个旋转部件的潜在运行参数，即平均振动幅值，记为 $\\mu$。在校准周期 $t$，该数字孪生收集 $n$ 个传感器观测值 $\\{y_i\\}_{i=1}^{n}$。这些观测值受到加性传感器噪声的影响，该噪声被建模为独立同分布的高斯噪声，具有已知方差 $\\sigma^{2}$，因此 $y_i \\sim \\mathcal{N}(\\mu,\\sigma^{2})$，其中 $i=1,\\dots,n$。来自先前运行周期的先验工程知识表示为一个高斯先验分布 $\\mu \\sim \\mathcal{N}(\\mu_{0},\\tau^{2})$，其中超参数 $\\mu_{0}$ 和 $\\tau^{2}$ 已知。\n\n从贝叶斯推断的第一性原理和最大后验（MAP）的定义出发，推导在这些假设下 $\\mu$ 的闭式MAP估计量。您的推导必须从似然、先验和贝叶斯规则的定义开始，并通过对后验密度函数关于 $\\mu$ 进行优化来继续。在您的解释中，阐明该估计量表现出收缩效应的统计机制，包括估计量如何根据 $n$、$\\sigma^{2}$ 和 $\\tau^{2}$ 来平衡样本均值与先验均值，并讨论当 $\\tau^{2} \\to \\infty$ 和 $\\tau^{2} \\to 0$ 时的极限行为。\n\n请以 $n$、$\\sigma^{2}$、$\\tau^{2}$、$\\mu_{0}$ 和样本均值 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ 的形式，提供一个单一的闭式解析表达式作为最终的估计量。最终表达式中不需要进行数值四舍五入，也不需要包含任何单位。",
            "solution": "用户想要解决这个问题。\n问题是有效的。我将继续进行解答。\n\n目标是根据一组传感器观测值和关于参数的先验知识，推导旋转部件平均振动幅值 $\\mu$ 的最大后验（MAP）估计量。\n\n问题提供了以下信息：\n1.  一组 $n$ 个传感器观测值 $\\{y_i\\}_{i=1}^{n}$。\n2.  每个观测值的似然模型：$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其中观测值是独立同分布（i.i.d.）的，并且方差 $\\sigma^2$ 已知。\n3.  参数 $\\mu$ 的先验分布：$\\mu \\sim \\mathcal{N}(\\mu_0, \\tau^2)$，其中超参数 $\\mu_0$ 和 $\\tau^2$ 已知。\n\n推导将从贝叶斯推断的第一性原理开始。\n\n根据贝叶斯规则，在给定数据 $\\{y_i\\}$ 的条件下，参数 $\\mu$ 的后验概率密度与给定参数时数据的似然和参数的先验概率密度的乘积成正比。\n$$\np(\\mu | y_1, \\dots, y_n) \\propto p(y_1, \\dots, y_n | \\mu) \\, p(\\mu)\n$$\n其中 $p(\\mu | y_1, \\dots, y_n)$ 是后验，$p(y_1, \\dots, y_n | \\mu)$ 是似然，而 $p(\\mu)$ 是先验。\n\n首先，我们定义似然函数 $L(\\mu) = p(y_1, \\dots, y_n | \\mu)$。由于观测值 $\\{y_i\\}$ 是独立同分布的，联合似然是各个概率密度的乘积：\n$$\nL(\\mu) = \\prod_{i=1}^{n} p(y_i | \\mu)\n$$\n给定高斯模型 $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$，单个观测值的概率密度函数为：\n$$\np(y_i | \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\n因此，完整的似然函数为：\n$$\nL(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right)\n$$\n\n接下来，我们定义 $\\mu$ 的先验分布，给定为 $\\mu \\sim \\mathcal{N}(\\mu_0, \\tau^2)$：\n$$\np(\\mu) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(\\mu - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n\n现在，我们通过将似然和先验相乘来构建后验密度。我们可以忽略归一化常数，因为它们不依赖于 $\\mu$：\n$$\np(\\mu | \\{y_i\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) \\exp\\left( -\\frac{(\\mu - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n合并指数部分，我们得到：\n$$\np(\\mu | \\{y_i\\}) \\propto \\exp\\left( - \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 + \\frac{(\\mu - \\mu_0)^2}{2\\tau^2} \\right] \\right)\n$$\n\nMAP估计量，记为 $\\hat{\\mu}_{\\text{MAP}}$，是使该后验概率密度最大化的 $\\mu$ 值。计算上，最大化后验的自然对数更为方便，因为对数是单调函数。令 $\\mathcal{L}(\\mu) = \\ln(p(\\mu | \\{y_i\\}))$。\n$$\n\\mathcal{L}(\\mu) = C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 - \\frac{(\\mu - \\mu_0)^2}{2\\tau^2}\n$$\n其中 $C$ 是一个不依赖于 $\\mu$ 的常数项。为了找到最大值，我们对 $\\mathcal{L}(\\mu)$ 关于 $\\mu$ 求导，并令其为零：\n$$\n\\frac{d\\mathcal{L}}{d\\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) - \\frac{1}{2\\tau^2} 2(\\mu - \\mu_0)(1) = 0\n$$\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) - \\frac{1}{\\tau^2}(\\mu - \\mu_0) = 0\n$$\n注意到 $\\sum_{i=1}^{n} y_i = n\\bar{y}$，其中 $\\bar{y}$ 是样本均值，我们有：\n$$\n\\frac{1}{\\sigma^2} (n\\bar{y} - n\\mu) - \\frac{1}{\\tau^2}(\\mu - \\mu_0) = 0\n$$\n现在，我们求解 $\\mu$：\n$$\n\\frac{n\\bar{y}}{\\sigma^2} - \\frac{n\\mu}{\\sigma^2} - \\frac{\\mu}{\\tau^2} + \\frac{\\mu_0}{\\tau^2} = 0\n$$\n$$\n\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} = \\mu \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2} \\right)\n$$\n$$\n\\hat{\\mu}_{\\text{MAP}} = \\mu = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}\n$$\n为了得到所需形式的表达式，我们将分子和分母同乘以 $\\sigma^2\\tau^2$：\n$$\n\\hat{\\mu}_{\\text{MAP}} = \\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}\n$$\n\n这个 $\\hat{\\mu}_{\\text{MAP}}$ 的闭式解可以重写为样本均值 $\\bar{y}$ 和先验均值 $\\mu_0$ 的加权平均：\n$$\n\\hat{\\mu}_{\\text{MAP}} = \\left(\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\\right) \\bar{y} + \\left(\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2}\\right) \\mu_0\n$$\n\n这种形式清楚地说明了收缩的统计机制。MAP估计是最大似然估计（对于高斯分布，即样本均值 $\\bar{y}$）和先验均值 $\\mu_0$ 的凸组合。数据驱动的估计值 $\\bar{y}$ 被“收缩”到先验信念 $\\mu_0$。收缩的程度由权重决定，而权重取决于样本大小 $n$、传感器噪声方差 $\\sigma^2$ 和先验方差 $\\tau^2$。\n-   更大的样本大小 $n$ 会增加 $\\bar{y}$ 的权重，意味着估计更依赖于数据。\n-   更大的传感器噪声方差 $\\sigma^2$ 会增加 $\\mu_0$ 的权重，表明噪声大的数据不太可信，因此估计更依赖于先验。\n-   更大的先验方差 $\\tau^2$（一个不那么确定的先验）会增加 $\\bar{y}$ 的权重，因为先验信息被认为不太可靠。\n\n我们现在分析估计量的极限行为。\n\n情况1：无信息先验，$\\tau^2 \\to \\infty$。这代表了一个先验知识非常薄弱的场景。\n$$\n\\lim_{\\tau^2 \\to \\infty} \\hat{\\mu}_{\\text{MAP}} = \\lim_{\\tau^2 \\to \\infty} \\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}\n$$\n分子和分母同除以 $\\tau^2$：\n$$\n\\lim_{\\tau^2 \\to \\infty} \\frac{n\\bar{y} + \\frac{\\sigma^2}{\\tau^2}\\mu_0}{n + \\frac{\\sigma^2}{\\tau^2}} = \\frac{n\\bar{y} + 0}{n + 0} = \\bar{y}\n$$\n在此极限下，MAP估计量收敛于样本均值 $\\bar{y}$，即最大似然估计（MLE）。先验的影响消失了。\n\n情况2：无限强的先验，$\\tau^2 \\to 0$。这代表了对 $\\mu = \\mu_0$ 的绝对确定性。\n$$\n\\lim_{\\tau^2 \\to 0} \\hat{\\mu}_{\\text{MAP}} = \\lim_{\\tau^2 \\to 0} \\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}\n$$\n当 $\\tau^2 \\to 0$ 时，包含 $\\tau^2$ 的项消失：\n$$\n\\lim_{\\tau^2 \\to 0} \\frac{n(0)\\bar{y} + \\sigma^2\\mu_0}{n(0) + \\sigma^2} = \\frac{\\sigma^2\\mu_0}{\\sigma^2} = \\mu_0\n$$\n在此极限下，MAP估计量完全忽略数据，并固定在先验均值 $\\mu_0$。先验信念是如此之强，以至于任何数量的数据都无法改变它。",
            "answer": "$$\n\\boxed{\\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}}\n$$"
        },
        {
            "introduction": "真实世界的信息物理系统常常表现出非线性动力学特性，这使得传统的线性估计工具（如卡尔曼滤波器）失效。本练习将带你进入顺序蒙特卡洛方法的世界，通过编程实现一个自举粒子滤波器来解决非线性状态估计问题。这是一个极具实践意义的任务，它将帮助你理解如何使用一组带权重的“粒子”来近似后验概率分布，并有效处理非高斯和非线性系统中的不确定性。",
            "id": "4247459",
            "problem": "给定一个标量信息物理系统 (Cyber-Physical System, CPS)，该系统由一个具有过程不确定性和测量不确定性的非线性离散时间状态空间模型控制。潜状态在离散时间索引 $k \\in \\{0, 1, 2, \\dots\\}$ 时表示为 $x_k \\in \\mathbb{R}$，过程噪声表示为 $w_k \\in \\mathbb{R}$，测量噪声表示为 $v_k \\in \\mathbb{R}$。该模型为\n- 状态转移：$x_{k+1} = x_k^2 + w_k$，\n- 测量：$y_k = x_k + v_k$，\n其中具有独立的高斯不确定性 $w_k \\sim \\mathcal{N}(0, q)$ 和 $v_k \\sim \\mathcal{N}(0, r)$，以及一个高斯先验 $x_0 \\sim \\mathcal{N}(m_0, P_0)$，这里 $q > 0$，$r > 0$，$P_0 > 0$。\n\n任务：实现一个自助粒子滤波器，该滤波器使用 $N$ 个粒子进行序贯贝叶斯估计，其提议分布等于状态转移密度。在每个时间步 $k$，在进行任何重采样之前，计算有效样本大小 (Effective Sample Size, ESS)，其定义为\n$$\nN_{\\text{eff}}(k) = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_{k}^{(i)\\,2}},\n$$\n其中 $\\tilde{w}_{k}^{(i)}$ 是在时间 $k$ 的归一化重要性权重。仅当重采样前的 ESS 满足 $N_{\\text{eff}}(k)  N/2$ 时，才使用系统重采样。时间 $k$ 的粒子滤波递归应为：\n- 对于 $k = 0$：从 $\\mathcal{N}(m_0, P_0)$ 中抽取 $x_0^{(i)}$，计算与似然 $p(y_0 \\mid x_0^{(i)})$ 成比例的权重，归一化权重，计算 $N_{\\text{eff}}(0)$，如果 $N_{\\text{eff}}(0)  N/2$，则重采样并将权重重置为均匀分布。\n- 对于 $k \\ge 1$：通过 $x_k^{(i)} = \\left(x_{k-1}^{(i)}\\right)^2 + w_{k-1}^{(i)}$（其中 $w_{k-1}^{(i)} \\sim \\mathcal{N}(0, q)$）传播粒子，将权重更新为与 $p(y_k \\mid x_k^{(i)})$ 乘以先前权重的乘积成比例，归一化权重，计算 $N_{\\text{eff}}(k)$，如果 $N_{\\text{eff}}(k)  N/2$，则重采样并将权重重置为均匀分布。\n\n实现约束和要求：\n- 使用固定的伪随机数生成器种子 $s = 12345$ 以保证可复现性。如果在一个程序运行中评估多个测试用例，请按下面列出的顺序为连续的用例使用种子 $s, s+1, s+2, \\dots$，以确保跨测试用例的确定性和不同的随机性。\n- 通过在对数域中操作来避免下溢，从而使用数值稳定的权重计算，并通过 log-sum-exp 稳定化方法进行归一化。\n- 每个测试用例的输出是一个实数，等于在所有时间索引 $k \\in \\{0, \\dots, T-1\\}$ 上的最小重采样前 $N_{\\text{eff}}(k)$，其中 $T$ 是该测试用例中测量序列的长度。\n- 最终数值答案表示为精确到 $4$ 位小数的四舍五入值。\n\n测试套件：\n在以下三个测试用例上评估您的自助粒子滤波器。在每个案例中，测量序列的长度为 $T = 6$，观测值为 $y_0, y_1, y_2, y_3, y_4, y_5$。\n\n- 测试用例 $1$ (顺利路径):\n  - 粒子数：$N = 5000$。\n  - 过程噪声方差：$q = 0.05$。\n  - 测量噪声方差：$r = 0.2$。\n  - 先验均值和方差：$m_0 = 0.3$, $P_0 = 0.4$。\n  - 测量值：$y = [0.25, 0.11, 0.15, 0.05, 0.10, 0.18]$。\n\n- 测试用例 $2$ (小 $N$ 和较高噪声的边界情况):\n  - 粒子数：$N = 200$。\n  - 过程噪声方差：$q = 0.2$。\n  - 测量噪声方差：$r = 0.8$。\n  - 先验均值和方差：$m_0 = 0.0$, $P_0 = 1.0$。\n  - 测量值：$y = [0.2, -0.1, 0.0, 0.05, -0.2, 0.1]$。\n\n- 测试用例 $3$ (强非线性和低噪声的边缘情况):\n  - 粒子数：$N = 3000$。\n  - 过程噪声方差：$q = 0.01$。\n  - 测量噪声方差：$r = 0.05$。\n  - 先验均值和方差：$m_0 = -1.0$, $P_0 = 0.1$。\n  - 测量值：$y = [-0.90, 0.85, 0.70, 0.55, 0.36, 0.14]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。\n- 具体来说，输出必须是以下形式的单行：\n  $$\n  [\\text{res}_1,\\text{res}_2,\\text{res}_3],\n  $$\n  其中 $\\text{res}_j$ 是测试用例 $j$ 的最小重采样前有效样本大小，四舍五入到恰好 $4$ 位小数（无单位）。\n\n角度单位和物理单位：\n- 没有出现角度量；不需要角度单位。\n- 有效样本大小是无量纲的；不需要物理单位。\n\n您的实现必须是一个完整的、可运行的程序，仅使用标准库和指定的数值库，不读取任何输入，并严格按照指定格式打印一行。",
            "solution": "用户提供了一个在非线性动力系统统计估计领域中定义明确的问题。该问题具有科学依据、内容自洽且算法具体。它要求实现自助粒子滤波器，这是一种解决状态估计问题的标准蒙特卡洛方法。所有参数、系统动力学和评估标准都已明确提供。因此，该问题是有效的。\n\n问题的核心是为在离散时间 $k$ 的标量状态 $x_k$ 进行序贯贝叶斯估计，该状态由非线性状态空间模型控制：\n$$\n\\text{状态转移: } x_{k+1} = x_k^2 + w_k, \\quad w_k \\sim \\mathcal{N}(0, q)\n$$\n$$\n\\text{测量: } y_k = x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, r)\n$$\n估计是基于一系列测量值 $y_0, y_1, \\dots, y_{T-1}$。粒子滤波器使用一组 $N$ 个加权样本或“粒子” $\\{(x_k^{(i)}, \\tilde{w}_k^{(i)})\\}_{i=1}^N$ 来近似后验概率密度函数 $p(x_k | y_{0:k})$。解决方案通过实现指定的自助粒子滤波器算法来推进。\n\n自助滤波器是序贯重要性重采样 (Sequential Importance Resampling, SIR) 滤波器的一种特定类型，其中用于生成下一时间步粒子的提议分布被选为状态转移分布 $p(x_k | x_{k-1})$。这种选择简化了权重更新方程。该算法按如下方式递归进行：\n\n**第1步：初始化 ($k=0$)**\n我们首先从状态的先验分布中抽取 $N$ 个初始粒子。问题指定了一个高斯先验 $x_0 \\sim \\mathcal{N}(m_0, P_0)$。\n$$\nx_0^{(i)} \\sim \\mathcal{N}(m_0, P_0) \\quad \\text{对于 } i=1, \\dots, N\n$$\n初始权重是均匀的，这相当于假设在第一次测量之前没有先验信息。然后我们通过更新一组未归一化的权重 $w_0^{(i)}$ 来纳入第一次测量 $y_0$，使其与给定粒子状态 $x_0^{(i)}$ 下观测到 $y_0$ 的似然成比例。\n$$\nw_0^{(i)} \\propto p(y_0 | x_0^{(i)})\n$$\n由于测量噪声 $v_0 = y_0 - x_0$ 是方差为 $r$ 的高斯噪声，似然函数是在 $y_0$ 处评估的高斯分布 $\\mathcal{N}(x_0^{(i)}, r)$ 的概率密度函数 (PDF)。为了数值稳定性，我们使用对数权重。未归一化的对数权重为：\n$$\n\\log w_0^{(i)} = \\log p(y_0 | x_0^{(i)}) = -\\frac{1}{2}\\log(2\\pi r) - \\frac{(y_0 - x_0^{(i)})^2}{2r}\n$$\n\n**第2步：传播 (对于 $k \\ge 1$)**\n对于后续的时间步，我们首先使用状态转移模型预测每个粒子的下一个状态。这是“自助”步骤，我们从动力学模型中采样。上一步的粒子 $x_{k-1}^{(i)}$ 被传播到时间 $k$：\n$$\nx_k^{(i)} = (x_{k-1}^{(i)})^2 + w_{k-1}^{(i)}, \\quad \\text{其中 } w_{k-1}^{(i)} \\sim \\mathcal{N}(0, q)\n$$\n这将生成一组新的粒子 $\\{x_k^{(i)}\\}_{i=1}^N$，它代表了预测密度 $p(x_k|y_{0:k-1})$ 的经验近似。\n\n**第3步：权重更新 (对于 $k \\ge 1$)**\n在接收到新的测量值 $y_k$ 时，更新权重以反映每个传播的粒子 $x_k^{(i)}$ 与该测量值的一致程度。通用粒子滤波器的更新规则是 $w_k^{(i)} \\propto \\tilde{w}_{k-1}^{(i)} \\frac{p(y_k|x_k^{(i)})p(x_k^{(i)}|x_{k-1}^{(i)})}{q(x_k^{(i)}|x_{k-1}^{(i)})}$，其中 $\\tilde{w}_{k-1}^{(i)}$ 是上一步的归一化权重，$q(\\cdot)$ 是提议分布。在自助滤波器中，提议分布是先验分布，即 $q(x_k^{(i)}|x_{k-1}^{(i)}) = p(x_k^{(i)}|x_{k-1}^{(i)})$，因此分数项抵消了。更新简化为：\n$$\nw_k^{(i)} \\propto \\tilde{w}_{k-1}^{(i)} p(y_k | x_k^{(i)})\n$$\n在对数域中，这变成了一个加法更新：\n$$\n\\log w_k^{(i)} = \\log \\tilde{w}_{k-1}^{(i)} + \\log p(y_k | x_k^{(i)})\n$$\n\n**第4步：归一化与有效样本大小 (ESS) 计算**\n未归一化的对数权重 $\\log w_k^{(i)}$ 被归一化以产生总和为 $1$ 的权重 $\\tilde{w}_k^{(i)}$。为避免数值下溢/上溢，这通过使用 log-sum-exp 技巧来完成：\n$$\nS_k = \\log\\left(\\sum_{i=1}^N \\exp(\\log w_k^{(i)})\\right)\n$$\n归一化的对数权重为 $\\log \\tilde{w}_k^{(i)} = \\log w_k^{(i)} - S_k$，归一化的权重为 $\\tilde{w}_k^{(i)} = \\exp(\\log \\tilde{w}_k^{(i)})$。\n\n在任何重采样之前，我们计算有效样本大小 (ESS) 来衡量粒子退化程度。低的 ESS 表明少数粒子具有非常高的权重，使得对后验分布的近似很差。公式是：\n$$\nN_{\\text{eff}}(k) = \\frac{1}{\\sum_{i=1}^{N} (\\tilde{w}_{k}^{(i)})^2}\n$$\n对于每个时间步 $k \\in \\{0, \\dots, T-1\\}$，计算并存储该值，以找到问题所要求的最小值。\n\n**第5步：重采样**\n如果粒子集发生退化，如 $N_{\\text{eff}}(k)  N/2$ 所示，则执行重采样步骤。我们将实现系统重采样，这是一种低方差方法。此步骤通过从当前粒子集 $\\{x_k^{(i)}\\}$ 中以其权重 $\\{\\tilde{w}_k^{(i)}\\}$ 给出的概率进行 $N$ 次抽样，来创建一个新的粒子集。高权重的粒子可能会被复制，而低权重的粒子可能会被剔除。重采样后，粒子被新集合替换，其权重被重置为均匀分布：对所有 $i$，$\\tilde{w}_k^{(i)} = 1/N$。在对数域中，这意味着 $\\log \\tilde{w}_k^{(i)} = -\\log N$。如果没有发生重采样，归一化的权重 $\\tilde{w}_k^{(i)}$（或其对数）将被带到下一个时间步。\n\n这整个序列——传播、加权、归一化、ESS 计算和条件重采样——从时间步 $k=0$ 到 $k=T-1$ 为每个时间步重复进行。每个测试用例的最终输出是在此区间内记录的重采样前 $N_{\\text{eff}}(k)$ 值的最小值。实现将对每个测试用例使用固定的随机种子以确保可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_particle_filter(N, q, r, m0, P0, y_seq, seed):\n    \"\"\"\n    Implements a bootstrap particle filter for a nonlinear state-space model.\n\n    Args:\n        N (int): Number of particles.\n        q (float): Process noise variance.\n        r (float): Measurement noise variance.\n        m0 (float): Prior mean.\n        P0 (float): Prior variance.\n        y_seq (list or np.ndarray): Sequence of measurements.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        float: The minimum pre-resampling effective sample size (ESS).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    T = len(y_seq)\n    \n    # Standard deviations for noise generation\n    std_q = np.sqrt(q)\n    std_r = np.sqrt(r)\n\n    # --- Step 1: Initialization (k=0) ---\n    # Draw initial particles from the prior distribution\n    # These represent the particle set for time k=0, i.e., {x_0^(i)}\n    particles = rng.normal(loc=m0, scale=np.sqrt(P0), size=N)\n    \n    # Initialize log weights to be uniform. These represent log(w_tilde_{-1}),\n    # which are implicitly uniform before the first measurement.\n    log_weights = np.full(N, -np.log(N))\n    \n    all_ess_values = []\n\n    # --- Main Loop (k = 0 to T-1) ---\n    for k in range(T):\n        # At the start of loop k, `particles` holds {x_{k-1}^{(i)}} and `log_weights` holds log(w_tilde_{k-1}).\n        # For k=0, `particles` holds {x_0^{(i)}} and there is no propagation.\n        \n        # --- Step 2: Propagation (for k >= 1) ---\n        if k > 0:\n            process_noise = rng.normal(loc=0.0, scale=std_q, size=N)\n            particles = particles**2 + process_noise\n        \n        # `particles` now holds the set {x_k^(i)}\n\n        # --- Step 3: Weight Update ---\n        # Calculate log-likelihood log p(y_k | x_k^(i))\n        log_likelihood = norm.logpdf(y_seq[k], loc=particles, scale=std_r)\n        \n        # Update log-weights: log(w_k) = log(w_tilde_{k-1}) + log p(y_k | x_k)\n        unnormalized_log_weights = log_weights + log_likelihood\n\n        # --- Step 4: Normalization and ESS Calculation ---\n        # Normalize weights using log-sum-exp trick for numerical stability\n        max_log = np.max(unnormalized_log_weights)\n        if np.isneginf(max_log): # All weights are -inf, numerically unstable\n            # This can happen if all particles are extremely unlikely.\n            # We can reset to uniform weights to recover, but for this problem,\n            # this indicates a serious filter divergence.\n            # To avoid division by zero, we set weights to uniform.\n            normalized_weights = np.full(N, 1.0/N)\n            log_weights = np.log(normalized_weights)\n        else:\n            temp_weights = np.exp(unnormalized_log_weights - max_log)\n            sum_of_weights = np.sum(temp_weights)\n            log_sum = max_log + np.log(sum_of_weights)\n            normalized_weights = temp_weights / sum_of_weights\n            log_weights = unnormalized_log_weights - log_sum\n        \n        # Calculate pre-resampling Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n        all_ess_values.append(ess)\n\n        # --- Step 5: Resampling ---\n        if ess  N / 2.0:\n            # Systematic resampling\n            positions = (np.arange(N) + rng.random()) / N\n            cumulative_sum = np.cumsum(normalized_weights)\n            indices = np.searchsorted(cumulative_sum, positions, side='left')\n            \n            # Ensure indices are within bounds (can happen with numerical precision issues)\n            indices = np.clip(indices, 0, N - 1)\n\n            particles = particles[indices]\n            log_weights = np.full(N, -np.log(N)) # Reset weights to uniform\n\n    return min(all_ess_values)\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the particle filter, and print results.\n    \"\"\"\n    base_seed = 12345\n\n    test_cases = [\n        {\n            \"N\": 5000, \"q\": 0.05, \"r\": 0.2, \"m0\": 0.3, \"P0\": 0.4,\n            \"y\": [0.25, 0.11, 0.15, 0.05, 0.10, 0.18]\n        },\n        {\n            \"N\": 200, \"q\": 0.2, \"r\": 0.8, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": [0.2, -0.1, 0.0, 0.05, -0.2, 0.1]\n        },\n        {\n            \"N\": 3000, \"q\": 0.01, \"r\": 0.05, \"m0\": -1.0, \"P0\": 0.1,\n            \"y\": [-0.90, 0.85, 0.70, 0.55, 0.36, 0.14]\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        min_ess = run_particle_filter(\n            N=case[\"N\"],\n            q=case[\"q\"],\n            r=case[\"r\"],\n            m0=case[\"m0\"],\n            P0=case[\"P0\"],\n            y_seq=case[\"y\"],\n            seed=base_seed + i\n        )\n        results.append(f\"{min_ess:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}