{
    "hands_on_practices": [
        {
            "introduction": "数字孪生的一个核心任务是校准其模型参数，这通常需要结合历史工程知识（先验信息）与实时传感器读数（数据）。本练习将引导你运用最大后验（MAP）估计，这是一个强大的贝叶斯框架，用于形式化地融合这两类信息。通过推导一个高斯模型下的 MAP 估计器 ，你将亲身体会到先验信念如何将估计结果“收缩”到更可信的范围，从而在数据有限或噪声较强时获得更稳健的估计。",
            "id": "4247342",
            "problem": "一个信息物理系统（CPS）的数字孪生被用来持续校准一个旋转部件的潜在运行参数，即平均振动幅度，记为 $\\mu$。在校准周期 $t$，该数字孪生收集 $n$ 个传感器观测值 $\\{y_i\\}_{i=1}^{n}$。这些观测值受到加性传感器噪声的影响，该噪声被建模为独立同分布的高斯分布，其方差 $\\sigma^{2}$ 已知，因此 $y_i \\sim \\mathcal{N}(\\mu,\\sigma^{2})$，其中 $i=1,\\dots,n$。来自先前运行周期的先验工程知识被表示为一个高斯先验 $\\mu \\sim \\mathcal{N}(\\mu_{0},\\tau^{2})$，其超参数 $\\mu_{0}$ 和 $\\tau^{2}$ 是已知的。\n\n从贝叶斯推断的基本原理和最大后验（MAP）定义出发，在这些假设下推导 $\\mu$ 的闭式最大后验估计量。你的推导必须从似然、先验和贝叶斯法则的定义开始，然后通过对后验密度函数关于 $\\mu$ 进行优化来继续。在你的解释中，阐明该估计量表现出收缩效应的统计机制，包括估计量如何根据 $n$、$\\sigma^{2}$ 和 $\\tau^{2}$ 的值来平衡样本均值与先验均值，并讨论当 $\\tau^{2} \\to \\infty$ 和 $\\tau^{2} \\to 0$ 时的极限行为。\n\n以 $n$、$\\sigma^{2}$、$\\tau^{2}$、$\\mu_{0}$ 和样本均值 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ 的形式，提供最终的估计量作为一个单一的闭式解析表达式。不需要进行数值四舍五入，最终表达式中也不应包含任何单位。",
            "solution": "用户希望解决此问题。\n问题有效。我将开始解答。\n\n目标是推导旋转部件平均振动幅度 $\\mu$ 的最大后验（MAP）估计量，给定一组传感器观测值和关于该参数的先验知识。\n\n问题提供了以下信息：\n1.  一组 $n$ 个传感器观测值 $\\{y_i\\}_{i=1}^{n}$。\n2.  每个观测值的似然模型：$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其中观测值是独立同分布的（i.i.d.），并且方差 $\\sigma^2$ 是已知的。\n3.  参数 $\\mu$ 的先验分布：$\\mu \\sim \\mathcal{N}(\\mu_0, \\tau^2)$，其中超参数 $\\mu_0$ 和 $\\tau^2$ 是已知的。\n\n推导将从贝叶斯推断的基本原理开始。\n\n根据贝叶斯法则，在给定数据 $\\{y_i\\}$ 的条件下，参数 $\\mu$ 的后验概率密度与给定参数时数据的似然和参数的先验概率密度的乘积成正比。\n$$\np(\\mu | y_1, \\dots, y_n) \\propto p(y_1, \\dots, y_n | \\mu) \\, p(\\mu)\n$$\n其中 $p(\\mu | y_1, \\dots, y_n)$ 是后验，$p(y_1, \\dots, y_n | \\mu)$ 是似然，$p(\\mu)$ 是先验。\n\n首先，我们定义似然函数 $L(\\mu) = p(y_1, \\dots, y_n | \\mu)$。由于观测值 $\\{y_i\\}$ 是独立同分布的，联合似然是各个概率密度的乘积：\n$$\nL(\\mu) = \\prod_{i=1}^{n} p(y_i | \\mu)\n$$\n给定高斯模型 $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$，单个观测值的概率密度函数为：\n$$\np(y_i | \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\n因此，完整的似然函数为：\n$$\nL(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right)\n$$\n\n接下来，我们定义 $\\mu$ 的先验分布，给定为 $\\mu \\sim \\mathcal{N}(\\mu_0, \\tau^2)$：\n$$\np(\\mu) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(\\mu - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n\n现在，通过将似然和先验相乘来构建后验密度。我们可以忽略归一化常数，因为它们不依赖于 $\\mu$：\n$$\np(\\mu | \\{y_i\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) \\exp\\left( -\\frac{(\\mu - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n合并指数项，我们得到：\n$$\np(\\mu | \\{y_i\\}) \\propto \\exp\\left( - \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 + \\frac{(\\mu - \\mu_0)^2}{2\\tau^2} \\right] \\right)\n$$\n\nMAP 估计量，记作 $\\hat{\\mu}_{\\text{MAP}}$，是使该后验概率密度最大化的 $\\mu$ 值。在计算上，最大化后验的自然对数更为方便，因为对数是单调函数。令 $\\mathcal{L}(\\mu) = \\ln(p(\\mu | \\{y_i\\}))$。\n$$\n\\mathcal{L}(\\mu) = C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 - \\frac{(\\mu - \\mu_0)^2}{2\\tau^2}\n$$\n其中 $C$ 是一个不依赖于 $\\mu$ 的常数项。为了找到最大值，我们对 $\\mathcal{L}(\\mu)$ 关于 $\\mu$ 求导，并将其设为零：\n$$\n\\frac{d\\mathcal{L}}{d\\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) - \\frac{1}{2\\tau^2} 2(\\mu - \\mu_0)(1) = 0\n$$\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) - \\frac{1}{\\tau^2}(\\mu - \\mu_0) = 0\n$$\n认识到 $\\sum_{i=1}^{n} y_i = n\\bar{y}$，其中 $\\bar{y}$ 是样本均值，我们有：\n$$\n\\frac{1}{\\sigma^2} (n\\bar{y} - n\\mu) - \\frac{1}{\\tau^2}(\\mu - \\mu_0) = 0\n$$\n现在，我们求解 $\\mu$：\n$$\n\\frac{n\\bar{y}}{\\sigma^2} - \\frac{n\\mu}{\\sigma^2} - \\frac{\\mu}{\\tau^2} + \\frac{\\mu_0}{\\tau^2} = 0\n$$\n$$\n\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} = \\mu \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2} \\right)\n$$\n$$\n\\hat{\\mu}_{\\text{MAP}} = \\mu = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}\n$$\n为了得到所需形式的表达式，我们将分子和分母同乘以 $\\sigma^2\\tau^2$：\n$$\n\\hat{\\mu}_{\\text{MAP}} = \\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}\n$$\n\n这个 $\\hat{\\mu}_{\\text{MAP}}$ 的闭式解可以重写为样本均值 $\\bar{y}$ 和先验均值 $\\mu_0$ 的加权平均：\n$$\n\\hat{\\mu}_{\\text{MAP}} = \\left(\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\\right) \\bar{y} + \\left(\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2}\\right) \\mu_0\n$$\n\n这种形式清楚地说明了收缩的统计机制。MAP 估计是最大似然估计（对于高斯分布，即为样本均值 $\\bar{y}$）和先验均值 $\\mu_0$ 的凸组合。数据驱动的估计 $\\bar{y}$ 被“收缩”到先验信念 $\\mu_0$ 上。收缩的程度由权重决定，这些权重取决于样本量 $n$、传感器噪声方差 $\\sigma^2$ 和先验方差 $\\tau^2$。\n-   较大的样本量 $n$ 会增加 $\\bar{y}$ 的权重，意味着估计更多地依赖于数据。\n-   较大的传感器噪声方差 $\\sigma^2$ 会增加 $\\mu_0$ 的权重，表明噪声大的数据不太可信，因此估计更多地依赖于先验。\n-   较大的先验方差 $\\tau^2$（一个不那么确定的先验）会增加 $\\bar{y}$ 的权重，因为先验信息被认为不太可靠。\n\n我们现在分析估计量的极限行为。\n\n情况1：无信息先验，$\\tau^2 \\to \\infty$。这代表先验知识非常弱的情景。\n$$\n\\lim_{\\tau^2 \\to \\infty} \\hat{\\mu}_{\\text{MAP}} = \\lim_{\\tau^2 \\to \\infty} \\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}\n$$\n分子和分母同除以 $\\tau^2$：\n$$\n\\lim_{\\tau^2 \\to \\infty} \\frac{n\\bar{y} + \\frac{\\sigma^2}{\\tau^2}\\mu_0}{n + \\frac{\\sigma^2}{\\tau^2}} = \\frac{n\\bar{y} + 0}{n + 0} = \\bar{y}\n$$\n在此极限下，MAP 估计量收敛于样本均值 $\\bar{y}$，即最大似然估计（MLE）。先验的影响消失了。\n\n情况2：无限强的先验，$\\tau^2 \\to 0$。这代表对 $\\mu = \\mu_0$ 的绝对确定性。\n$$\n\\lim_{\\tau^2 \\to 0} \\hat{\\mu}_{\\text{MAP}} = \\lim_{\\tau^2 \\to 0} \\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}\n$$\n当 $\\tau^2 \\to 0$ 时，涉及 $\\tau^2$ 的项消失：\n$$\n\\lim_{\\tau^2 \\to 0} \\frac{n(0)\\bar{y} + \\sigma^2\\mu_0}{n(0) + \\sigma^2} = \\frac{\\sigma^2\\mu_0}{\\sigma^2} = \\mu_0\n$$\n在此极限下，MAP 估计量完全忽略数据，固定在先验均值 $\\mu_0$ 上。先验信念如此之强，以至于任何数量的数据都无法改变它。",
            "answer": "$$\n\\boxed{\\frac{n\\tau^2\\bar{y} + \\sigma^2\\mu_0}{n\\tau^2 + \\sigma^2}}\n$$"
        },
        {
            "introduction": "在选择了估计方法之后，我们如何评判一个估计器的好坏？本练习深入探讨了估计器性能评估的核心概念——均方误差（MSE），并揭示了其与偏差和方差的深刻联系。通过分析一个特意引入偏差的估计器 ，你将理解统计估计中最重要的权衡之一：偏差-方差权衡。这个练习将挑战你的直觉，展示在某些情况下，一个有偏估计器可以因为其方差显著降低而获得比无偏估计器更小的总误差。",
            "id": "4247291",
            "problem": "一个信息物理系统 (Cyber-Physical System, CPS) 的数字孪生 (Digital Twin, DT) 的任务是估计一个影响测量通道的标量校准参数 $\\theta$。该 CPS 提供了 $n$ 个同步样本 $\\{y_{i}\\}_{i=1}^{n}$，这些样本根据数据生成过程 $y_{i} = \\theta + \\varepsilon_{i}$ 生成，其中 $\\varepsilon_{i}$ 是独立的、零均值的扰动，其方差为 $\\sigma^{2}$，并且该 DT 的建模流程假设样本之间具有同方差性和独立性。\n\n为了补偿在系统辨识中发现的模型失配，该 DT 部署了一个有偏估计量 $\\hat{\\theta}_{b} = \\bar{y} + c$，其中 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$，$c$ 是一个已知的常数偏移量。参考的无偏估计量为 $\\hat{\\theta}_{u} = \\bar{y}$。仅使用期望、方差、偏差以及在欧几里得范数平方损失（通常称为 $L_{2}$ 损失）下的均方误差的核心定义，推导 $\\hat{\\theta}_{b}$ 和 $\\hat{\\theta}_{u}$ 的均方误差，并计算下面要求的比较。\n\n给定 $n = 36$，$\\sigma^{2} = 0.09$ 和 $c = 0.05$，计算比率 $\\mathrm{MSE}(\\hat{\\theta}_{b}) / \\mathrm{MSE}(\\hat{\\theta}_{u})$。将您的最终答案表示为一个纯数字，并四舍五入到四位有效数字。",
            "solution": "对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- 数据生成过程：$y_{i} = \\theta + \\varepsilon_{i}$，其中 $i=1, \\dots, n$。\n- 扰动 $\\varepsilon_{i}$ 是独立同分布 (i.i.d.) 的，满足 $\\mathbb{E}[\\varepsilon_{i}] = 0$ 和 $\\mathrm{Var}(\\varepsilon_{i}) = \\sigma^{2}$。\n- 样本数量：$n$。\n- 样本均值：$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$。\n- 无偏估计量：$\\hat{\\theta}_{u} = \\bar{y}$。\n- 有偏估计量：$\\hat{\\theta}_{b} = \\bar{y} + c$，其中 $c$ 是一个已知常数。\n- 损失函数：均方误差 (Mean Squared Error, MSE)，定义为 $\\mathrm{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2]$。\n- 给定的数值：$n = 36$，$\\sigma^{2} = 0.09$，$c = 0.05$。\n- 任务：计算比率 $\\mathrm{MSE}(\\hat{\\theta}_{b}) / \\mathrm{MSE}(\\hat{\\theta}_{u})$ 并将结果四舍五入到四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，因为它基于统计估计理论（偏差、方差、均方误差）的基本原理，而这些原理是信息物理系统和数字孪生建模与分析的核心。该问题是适定的，提供了所有必要的数据和定义，可以得出一个唯一且有意义的解。语言客观、正式。问题设定完整且内部一致。该模型是一个被加性噪声破坏的恒定信号的标准表示，并且数值是合理的。\n\n### 步骤3：结论与行动\n问题有效。将提供完整的解答。\n\n求解过程首先推导两个估计量 $\\hat{\\theta}_{u}$ 和 $\\hat{\\theta}_{b}$ 的均方误差 (MSE) 表达式，然后使用给定的数值计算它们的比率。MSE 的一个基本性质是它可以分解为估计量偏差的平方与估计量的方差之和：\n$$\n\\mathrm{MSE}(\\hat{\\theta}) = (\\mathrm{Bias}(\\hat{\\theta}))^2 + \\mathrm{Var}(\\hat{\\theta})\n$$\n其中偏差为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$，方差为 $\\mathrm{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2]$。\n\n首先，我们分析样本均值 $\\bar{y}$ 的性质，它是两个估计量的共同组成部分。\n样本均值的期望是：\n$$\n\\mathbb{E}[\\bar{y}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[y_{i}]\n$$\n给定 $y_i = \\theta + \\varepsilon_i$ 且 $\\mathbb{E}[\\varepsilon_i] = 0$，单个样本的期望是 $\\mathbb{E}[y_i] = \\mathbb{E}[\\theta + \\varepsilon_i] = \\theta + \\mathbb{E}[\\varepsilon_i] = \\theta$。\n因此，\n$$\n\\mathbb{E}[\\bar{y}] = \\frac{1}{n}\\sum_{i=1}^{n} \\theta = \\frac{1}{n}(n\\theta) = \\theta\n$$\n样本均值的方差是：\n$$\n\\mathrm{Var}(\\bar{y}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\right) = \\frac{1}{n^2}\\mathrm{Var}\\left(\\sum_{i=1}^{n} y_{i}\\right)\n$$\n因为扰动 $\\varepsilon_i$ 是独立的，所以样本 $y_i$ 也是独立的。因此，和的方差等于方差的和：\n$$\n\\mathrm{Var}(\\bar{y}) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Var}(y_i)\n$$\n单个样本的方差是 $\\mathrm{Var}(y_i) = \\mathrm{Var}(\\theta + \\varepsilon_i) = \\mathrm{Var}(\\varepsilon_i) = \\sigma^2$，因为 $\\theta$ 是一个常数。\n因此，\n$$\n\\mathrm{Var}(\\bar{y}) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sigma^2 = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}\n$$\n\n现在，我们可以计算每个估计量的 MSE。\n\n对于无偏估计量 $\\hat{\\theta}_{u} = \\bar{y}$：\n期望是 $\\mathbb{E}[\\hat{\\theta}_{u}] = \\mathbb{E}[\\bar{y}] = \\theta$。\n偏差是 $\\mathrm{Bias}(\\hat{\\theta}_{u}) = \\mathbb{E}[\\hat{\\theta}_{u}] - \\theta = \\theta - \\theta = 0$。\n方差是 $\\mathrm{Var}(\\hat{\\theta}_{u}) = \\mathrm{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}$。\n无偏估计量的 MSE 是：\n$$\n\\mathrm{MSE}(\\hat{\\theta}_{u}) = (\\mathrm{Bias}(\\hat{\\theta}_{u}))^2 + \\mathrm{Var}(\\hat{\\theta}_{u}) = 0^2 + \\frac{\\sigma^2}{n} = \\frac{\\sigma^2}{n}\n$$\n\n对于有偏估计量 $\\hat{\\theta}_{b} = \\bar{y} + c$：\n期望是 $\\mathbb{E}[\\hat{\\theta}_{b}] = \\mathbb{E}[\\bar{y} + c] = \\mathbb{E}[\\bar{y}] + c = \\theta + c$。\n偏差是 $\\mathrm{Bias}(\\hat{\\theta}_{b}) = \\mathbb{E}[\\hat{\\theta}_{b}] - \\theta = (\\theta + c) - \\theta = c$。\n方差是 $\\mathrm{Var}(\\hat{\\theta}_{b}) = \\mathrm{Var}(\\bar{y} + c)$。因为 $c$ 是一个常数，它不影响方差，所以 $\\mathrm{Var}(\\hat{\\theta}_{b}) = \\mathrm{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}$。\n有偏估计量的 MSE 是：\n$$\n\\mathrm{MSE}(\\hat{\\theta}_{b}) = (\\mathrm{Bias}(\\hat{\\theta}_{b}))^2 + \\mathrm{Var}(\\hat{\\theta}_{b}) = c^2 + \\frac{\\sigma^2}{n}\n$$\n\n最后，我们计算所要求的比率：\n$$\n\\frac{\\mathrm{MSE}(\\hat{\\theta}_{b})}{\\mathrm{MSE}(\\hat{\\theta}_{u})} = \\frac{c^2 + \\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n}} = \\frac{nc^2}{\\sigma^2} + 1\n$$\n我们代入给定的数值：$n = 36$，$\\sigma^{2} = 0.09$ 和 $c = 0.05$。\n$$\n\\frac{\\mathrm{MSE}(\\hat{\\theta}_{b})}{\\mathrm{MSE}(\\hat{\\theta}_{u})} = \\frac{36 \\times (0.05)^2}{0.09} + 1\n$$\n首先，计算分子中的项：$36 \\times (0.05)^2 = 36 \\times 0.0025 = 0.09$。\n现在将此结果代回表达式中：\n$$\n\\frac{\\mathrm{MSE}(\\hat{\\theta}_{b})}{\\mathrm{MSE}(\\hat{\\theta}_{u})} = \\frac{0.09}{0.09} + 1 = 1 + 1 = 2\n$$\n题目要求答案表示为一个纯数字，并四舍五入到四位有效数字。由于结果恰好是 $2$，我们将其写为 $2.000$ 以明确表示所需的精度。",
            "answer": "$$\\boxed{2.000}$$"
        },
        {
            "introduction": "真实世界的赛博物理系统（CPS）是动态且往往非线性的，对其状态的实时追踪是数字孪生的关键能力。本练习将带领你从估计静态参数迈向追踪动态系统状态，通过动手实现一个自举粒子滤波器来解决非线性状态空间模型的估计问题 。这个实践不仅让你掌握一种强大的蒙特卡洛模拟方法，还将锻炼你处理数值稳定性、评估滤波器性能（例如通过有效样本量 ESS）等关键的工程和编程技能。",
            "id": "4247459",
            "problem": "给定一个标量信息物理系统（Cyber-Physical System, CPS），该系统由一个包含过程不确定性和测量不确定性的非线性离散时间状态空间模型控制。在离散时间索引 $k \\in \\{0, 1, 2, \\dots\\}$ 处，潜在状态表示为 $x_k \\in \\mathbb{R}$，过程噪声表示为 $w_k \\in \\mathbb{R}$，测量噪声表示为 $v_k \\in \\mathbb{R}$。该模型为\n- 状态转移：$x_{k+1} = x_k^2 + w_k$，\n- 测量：$y_k = x_k + v_k$，\n其中不确定性是独立的高斯分布 $w_k \\sim \\mathcal{N}(0, q)$ 和 $v_k \\sim \\mathcal{N}(0, r)$，先验分布为高斯分布 $x_0 \\sim \\mathcal{N}(m_0, P_0)$，并且 $q > 0$，$r > 0$，$P_0 > 0$。\n\n任务：实现一个自举（bootstrap）粒子滤波器，该滤波器使用 $N$ 个粒子进行序贯贝叶斯估计，并以状态转移密度作为提议分布。在每个时间步 $k$，在进行任何重采样之前，计算有效样本量（Effective Sample Size, ESS），其定义为\n$$\nN_{\\text{eff}}(k) = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_{k}^{(i)\\,2}},\n$$\n其中 $\\tilde{w}_{k}^{(i)}$ 是在时间 $k$ 的归一化重要性权重。仅当重采样前的 ESS 满足 $N_{\\text{eff}}(k)  N/2$ 时，才使用系统重采样。在时间 $k$ 的粒子滤波递推过程应为：\n- 当 $k = 0$ 时：从 $\\mathcal{N}(m_0, P_0)$ 中抽取 $x_0^{(i)}$，计算与似然 $p(y_0 \\mid x_0^{(i)})$ 成正比的权重，对权重进行归一化，计算 $N_{\\text{eff}}(0)$，如果 $N_{\\text{eff}}(0)  N/2$，则进行重采样并将权重重置为均匀分布。\n- 当 $k \\ge 1$ 时：通过 $x_k^{(i)} = \\left(x_{k-1}^{(i)}\\right)^2 + w_{k-1}^{(i)}$（其中 $w_{k-1}^{(i)} \\sim \\mathcal{N}(0, q)$）传播粒子，将权重更新为与 $p(y_k \\mid x_k^{(i)})$ 乘以先前权重的乘积成正比，对权重进行归一化，计算 $N_{\\text{eff}}(k)$，如果 $N_{\\text{eff}}(k)  N/2$，则进行重采样并将权重重置为均匀分布。\n\n实现约束与要求：\n- 为保证可复现性，使用固定的伪随机数生成器种子 $s = 12345$。如果在一个程序运行中评估多个测试用例，请按下面列出的顺序为连续的用例使用种子 $s, s+1, s+2, \\dots$，以确保各测试用例的随机性是确定且不同的。\n- 通过在对数域中进行计算，以避免下溢，从而实现权重的数值稳定计算，并通过 log-sum-exp 稳定化方法进行归一化。\n- 每个测试用例的输出是一个实数，等于在所有时间索引 $k \\in \\{0, \\dots, T-1\\}$ 上的最小重采样前 $N_{\\text{eff}}(k)$，其中 $T$ 是该测试用例中测量序列的长度。\n- 最终数值答案表示为精确到 $4$ 位小数的四舍五入值。\n\n测试套件：\n在以下三个测试用例上评估您的自举粒子滤波器。在每个用例中，测量序列的长度为 $T = 6$，观测值为 $y_0, y_1, y_2, y_3, y_4, y_5$。\n\n- 测试用例 1 (理想路径):\n  - 粒子数：$N = 5000$。\n  - 过程噪声方差：$q = 0.05$。\n  - 测量噪声方差：$r = 0.2$。\n  - 先验均值和方差：$m_0 = 0.3$, $P_0 = 0.4$。\n  - 测量值：$y = [0.25, 0.11, 0.15, 0.05, 0.10, 0.18]$。\n\n- 测试用例 2 (小 $N$ 和较高噪声的边界情况):\n  - 粒子数：$N = 200$。\n  - 过程噪声方差：$q = 0.2$。\n  - 测量噪声方差：$r = 0.8$。\n  - 先验均值和方差：$m_0 = 0.0$, $P_0 = 1.0$。\n  - 测量值：$y = [0.2, -0.1, 0.0, 0.05, -0.2, 0.1]$。\n\n- 测试用例 3 (强非线性和低噪声的边缘情况):\n  - 粒子数：$N = 3000$。\n  - 过程噪声方差：$q = 0.01$。\n  - 测量噪声方差：$r = 0.05$。\n  - 先验均值和方差：$m_0 = -1.0$, $P_0 = 0.1$。\n  - 测量值：$y = [-0.90, 0.85, 0.70, 0.55, 0.36, 0.14]$。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔的结果列表。\n- 具体来说，输出必须是以下形式的单行\n  $$\n  [\\text{res}_1,\\text{res}_2,\\text{res}_3],\n  $$\n  其中 $\\text{res}_j$ 是测试用例 $j$ 的最小重采样前有效样本量，四舍五入到恰好 $4$ 位小数（无单位）。\n\n角度单位和物理单位：\n- 不涉及角度量；不需要角度单位。\n- 有效样本量是无量纲的；不需要物理单位。\n\n您的实现必须是一个完整的、可运行的程序，仅使用标准库和指定的数值库，不读取任何输入，并严格按照指定格式打印一行输出。",
            "solution": "用户提供了一个非线性动力学系统统计估计领域中定义明确的问题。该问题具有科学依据，内容自洽，且算法明确。它要求实现一个自举（bootstrap）粒子滤波器，这是一种用于解决状态估计问题的标准蒙特卡洛方法。所有参数、系统动力学和评估标准都已明确给出。因此，该问题是有效的。\n\n问题的核心是为一个在离散时间 $k$ 的标量状态 $x_k$ 执行序贯贝叶斯估计，该状态由非线性状态空间模型控制：\n$$\n\\text{状态转移：} x_{k+1} = x_k^2 + w_k, \\quad w_k \\sim \\mathcal{N}(0, q)\n$$\n$$\n\\text{测量：} y_k = x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, r)\n$$\n估计是基于一系列测量值 $y_0, y_1, \\dots, y_{T-1}$。粒子滤波器使用一组 $N$ 个加权样本或“粒子” $\\{(x_k^{(i)}, \\tilde{w}_k^{(i)})\\}_{i=1}^N$ 来近似后验概率密度函数 $p(x_k | y_{0:k})$。解决方案通过实现指定的自举粒子滤波器算法来进行。\n\n自举滤波器是序贯重要性重采样（Sequential Importance Resampling, SIR）滤波器的一种特殊类型，其中用于在下一时间步生成粒子的提议分布被选为状态转移分布 $p(x_k | x_{k-1})$。这种选择简化了权重更新方程。该算法按如下方式递归进行：\n\n**第 1 步：初始化 ($k=0$)**\n我们首先从状态的先验分布中抽取 $N$ 个初始粒子。问题指定了高斯先验 $x_0 \\sim \\mathcal{N}(m_0, P_0)$。\n$$\nx_0^{(i)} \\sim \\mathcal{N}(m_0, P_0) \\quad \\text{for } i=1, \\dots, N\n$$\n初始权重是均匀的，这等同于假设在第一次测量之前没有先验信息。然后我们引入第一次测量值 $y_0$，方法是更新一组未归一化的权重 $w_0^{(i)}$，使其与给定粒子状态 $x_0^{(i)}$ 时观测到 $y_0$ 的似然成正比。\n$$\nw_0^{(i)} \\propto p(y_0 | x_0^{(i)})\n$$\n由于测量噪声 $v_0 = y_0 - x_0$ 是方差为 $r$ 的高斯分布，似然函数是在 $y_0$ 处求值的高斯分布 $\\mathcal{N}(x_0^{(i)}, r)$ 的概率密度函数（PDF）。为保证数值稳定性，我们使用对数权重。未归一化的对数权重是：\n$$\n\\log w_0^{(i)} = \\log p(y_0 | x_0^{(i)}) = -\\frac{1}{2}\\log(2\\pi r) - \\frac{(y_0 - x_0^{(i)})^2}{2r}\n$$\n\n**第 2 步：传播 (对于 $k \\ge 1$)**\n对于后续的时间步，我们首先使用状态转移模型预测每个粒子的下一个状态。这就是“自举”步骤，我们从动力学模型中采样。上一步的粒子 $x_{k-1}^{(i)}$ 被传播到时间 $k$：\n$$\nx_k^{(i)} = (x_{k-1}^{(i)})^2 + w_{k-1}^{(i)}, \\quad \\text{where } w_{k-1}^{(i)} \\sim \\mathcal{N}(0, q)\n$$\n这将生成一组新的粒子 $\\{x_k^{(i)}\\}_{i=1}^N$，它代表了预测密度 $p(x_k|y_{0:k-1})$ 的一个经验近似。\n\n**第 3 步：权重更新 (对于 $k \\ge 1$)**\n在接收到新的测量值 $y_k$ 后，更新权重以反映每个传播后的粒子 $x_k^{(i)}$ 与该测量值的一致程度。通用粒子滤波器的更新规则是 $w_k^{(i)} \\propto \\tilde{w}_{k-1}^{(i)} \\frac{p(y_k|x_k^{(i)})p(x_k^{(i)}|x_{k-1}^{(i)})}{q(x_k^{(i)}|x_{k-1}^{(i)})}$，其中 $\\tilde{w}_{k-1}^{(i)}$ 是上一步的归一化权重，$q(\\cdot)$ 是提议分布。在自举滤波器中，提议分布是先验分布，$q(x_k^{(i)}|x_{k-1}^{(i)}) = p(x_k^{(i)}|x_{k-1}^{(i)})$，所以分数项被消去。更新简化为：\n$$\nw_k^{(i)} \\propto \\tilde{w}_{k-1}^{(i)} p(y_k | x_k^{(i)})\n$$\n在对数域中，这变成一个加法更新：\n$$\n\\log w_k^{(i)} = \\log \\tilde{w}_{k-1}^{(i)} + \\log p(y_k | x_k^{(i)})\n$$\n\n**第 4 步：归一化和有效样本量（ESS）计算**\n未归一化的对数权重 $\\log w_k^{(i)}$ 被归一化，以产生总和为 1 的权重 $\\tilde{w}_k^{(i)}$。为避免数值下溢/上溢，使用 log-sum-exp 技巧来完成此操作：\n$$\nS_k = \\log\\left(\\sum_{i=1}^N \\exp(\\log w_k^{(i)})\\right)\n$$\n归一化的对数权重为 $\\log \\tilde{w}_k^{(i)} = \\log w_k^{(i)} - S_k$，归一化的权重为 $\\tilde{w}_k^{(i)} = \\exp(\\log \\tilde{w}_k^{(i)})$。\n\n在进行任何重采样之前，我们计算有效样本量（ESS）来衡量粒子退化程度。低的 ESS 表示少数几个粒子具有非常高的权重，导致对后验分布的近似效果很差。公式为：\n$$\nN_{\\text{eff}}(k) = \\frac{1}{\\sum_{i=1}^{N} (\\tilde{w}_{k}^{(i)})^2}\n$$\n对于每个时间步 $k \\in \\{0, \\dots, T-1\\}$，计算并存储该值，以找到问题所要求的最小值。\n\n**第 5 步：重采样**\n如果粒子集发生退化，即 $N_{\\text{eff}}(k)  N/2$，则执行重采样步骤。我们将实现系统重采样，这是一种低方差的方法。此步骤通过根据当前粒子集 $\\{x_k^{(i)}\\}$ 的权重 $\\{\\tilde{w}_k^{(i)}\\}$ 从中抽取 $N$ 次来创建一组新的粒子。高权重的粒子很可能被复制，而低权重的粒子很可能被淘汰。重采样后，粒子被新集合替换，并且它们的权重被重置为均匀分布：对所有 $i$，$ \\tilde{w}_k^{(i)} = 1/N$。在对数域中，这意味着 $\\log \\tilde{w}_k^{(i)} = -\\log N$。如果不发生重采样，归一化的权重 $\\tilde{w}_k^{(i)}$（或其对数）将被带到下一个时间步。\n\n这整个序列——传播、加权、归一化、ESS 计算和条件重采样——在从 $k=0$ 到 $k=T-1$ 的每个时间步重复进行。每个测试用例的最终输出是在此区间内记录的重采样前 $N_{\\text{eff}}(k)$ 值的最小值。该实现将为每个测试用例使用固定的随机种子以确保可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_particle_filter(N, q, r, m0, P0, y_seq, seed):\n    \"\"\"\n    Implements a bootstrap particle filter for a nonlinear state-space model.\n\n    Args:\n        N (int): Number of particles.\n        q (float): Process noise variance.\n        r (float): Measurement noise variance.\n        m0 (float): Prior mean.\n        P0 (float): Prior variance.\n        y_seq (list or np.ndarray): Sequence of measurements.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        float: The minimum pre-resampling effective sample size (ESS).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    T = len(y_seq)\n    \n    # Standard deviations for noise generation\n    std_q = np.sqrt(q)\n    std_r = np.sqrt(r)\n\n    # --- Step 1: Initialization (k=0) ---\n    # Draw initial particles from the prior distribution\n    # These represent the particle set for time k=0, i.e., {x_0^(i)}\n    particles = rng.normal(loc=m0, scale=np.sqrt(P0), size=N)\n    \n    # Initialize log weights to be uniform. These represent log(w_tilde_{-1}),\n    # which are implicitly uniform before the first measurement.\n    log_weights = np.full(N, -np.log(N))\n    \n    all_ess_values = []\n\n    # --- Main Loop (k = 0 to T-1) ---\n    for k in range(T):\n        # At the start of loop k, `particles` holds {x_{k-1}^{(i)}} and `log_weights` holds log(w_tilde_{k-1}).\n        # For k=0, `particles` holds {x_0^{(i)}} and there is no propagation.\n        \n        # --- Step 2: Propagation (for k = 1) ---\n        if k  0:\n            process_noise = rng.normal(loc=0.0, scale=std_q, size=N)\n            particles = particles**2 + process_noise\n        \n        # `particles` now holds the set {x_k^(i)}\n\n        # --- Step 3: Weight Update ---\n        # Calculate log-likelihood log p(y_k | x_k^(i))\n        log_likelihood = norm.logpdf(y_seq[k], loc=particles, scale=std_r)\n        \n        # Update log-weights: log(w_k) = log(w_tilde_{k-1}) + log p(y_k | x_k)\n        unnormalized_log_weights = log_weights + log_likelihood\n\n        # --- Step 4: Normalization and ESS Calculation ---\n        # Normalize weights using log-sum-exp trick for numerical stability\n        max_log = np.max(unnormalized_log_weights)\n        if np.isneginf(max_log): # All weights are -inf, numerically unstable\n            # This can happen if all particles are extremely unlikely.\n            # We can reset to uniform weights to recover, but for this problem,\n            # this indicates a serious filter divergence.\n            # To avoid division by zero, we set weights to uniform.\n            normalized_weights = np.full(N, 1.0/N)\n            log_weights = np.log(normalized_weights)\n        else:\n            temp_weights = np.exp(unnormalized_log_weights - max_log)\n            sum_of_weights = np.sum(temp_weights)\n            log_sum = max_log + np.log(sum_of_weights)\n            normalized_weights = temp_weights / sum_of_weights\n            log_weights = unnormalized_log_weights - log_sum\n        \n        # Calculate pre-resampling Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n        all_ess_values.append(ess)\n\n        # --- Step 5: Resampling ---\n        if ess  N / 2.0:\n            # Systematic resampling\n            positions = (np.arange(N) + rng.random()) / N\n            cumulative_sum = np.cumsum(normalized_weights)\n            indices = np.searchsorted(cumulative_sum, positions, side='left')\n            \n            # Ensure indices are within bounds (can happen with numerical precision issues)\n            indices = np.clip(indices, 0, N - 1)\n\n            particles = particles[indices]\n            log_weights = np.full(N, -np.log(N)) # Reset weights to uniform\n\n    return min(all_ess_values)\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the particle filter, and print results.\n    \"\"\"\n    base_seed = 12345\n\n    test_cases = [\n        {\n            \"N\": 5000, \"q\": 0.05, \"r\": 0.2, \"m0\": 0.3, \"P0\": 0.4,\n            \"y\": [0.25, 0.11, 0.15, 0.05, 0.10, 0.18]\n        },\n        {\n            \"N\": 200, \"q\": 0.2, \"r\": 0.8, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": [0.2, -0.1, 0.0, 0.05, -0.2, 0.1]\n        },\n        {\n            \"N\": 3000, \"q\": 0.01, \"r\": 0.05, \"m0\": -1.0, \"P0\": 0.1,\n            \"y\": [-0.90, 0.85, 0.70, 0.55, 0.36, 0.14]\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        min_ess = run_particle_filter(\n            N=case[\"N\"],\n            q=case[\"q\"],\n            r=case[\"r\"],\n            m0=case[\"m0\"],\n            P0=case[\"P0\"],\n            y_seq=case[\"y\"],\n            seed=base_seed + i\n        )\n        results.append(f\"{min_ess:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}