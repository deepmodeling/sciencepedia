{
    "hands_on_practices": [
        {
            "introduction": "This first practice establishes the cornerstone of multi-sensor fusion: combining independent measurements in a statistically optimal way. By working through the fusion of two simple Gaussian-distributed temperature readings, you will derive the posterior distribution from first principles. This exercise demonstrates the fundamental concept of precision-weighted averaging, showing how a fused estimate becomes more certain than any individual measurement and how the algorithm naturally gives more weight to the more reliable sensor .",
            "id": "4233215",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) monitors an unobserved scalar state $x$ (the internal core temperature of a battery module) using a centralized multi-sensor fusion architecture that aggregates a physical temperature sensor and a software-based observer. At a fixed time instant, the measurement models conditioned on $x$ are independent and described by the following Gaussian likelihoods: the physical sensor produces $y_1$ with $y_1 \\mid x \\sim \\mathcal{N}(x,\\sigma_1^2)$, and the DT observer produces $y_2$ with $y_2 \\mid x \\sim \\mathcal{N}(x,\\sigma_2^2)$. Assume a noninformative flat prior $p(x) \\propto 1$ over the range of interest, and conditional independence of $y_1$ and $y_2$ given $x$. Starting from Bayes’ rule and the definition of the Gaussian likelihood, derive the posterior density $p(x \\mid y_1,y_2)$ by completing the square, and identify the posterior mean as the minimum mean-squared error estimator for $x$ under the flat prior.\n\nAt a particular time instant, the measurements and known noise levels are:\n- $y_1 = 335.6$ K with $\\sigma_1 = 0.4$ K,\n- $y_2 = 336.9$ K with $\\sigma_2 = 0.9$ K.\n\nCompute the fused posterior mean estimate of $x$ in Kelvin. Express your final numerical answer in K and round to four significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in Bayesian statistical inference, well-posed with sufficient information for a unique solution, and stated using objective, formal language. It represents a standard sensor fusion problem in engineering and is free from the specified flaws.\n\nThe objective is to derive the posterior probability density function $p(x \\mid y_1, y_2)$ for an unobserved state $x$ given two independent measurements $y_1$ and $y_2$, and then compute the posterior mean.\n\nAccording to Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior distribution:\n$$\np(x \\mid y_1, y_2) \\propto p(y_1, y_2 \\mid x) p(x)\n$$\nThe problem states that the measurements $y_1$ and $y_2$ are conditionally independent given the state $x$. Therefore, the joint likelihood $p(y_1, y_2 \\mid x)$ can be factored into the product of the individual likelihoods:\n$$\np(y_1, y_2 \\mid x) = p(y_1 \\mid x) p(y_2 \\mid x)\n$$\nThe prior distribution for $x$ is given as a noninformative flat prior, $p(x) \\propto 1$. Substituting these into the Bayes' rule expression yields:\n$$\np(x \\mid y_1, y_2) \\propto p(y_1 \\mid x) p(y_2 \\mid x)\n$$\nThe likelihoods are given as Gaussian distributions:\n$y_1 \\mid x \\sim \\mathcal{N}(x, \\sigma_1^2)$, so its probability density function (PDF) is\n$$\np(y_1 \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left( -\\frac{(y_1 - x)^2}{2\\sigma_1^2} \\right)\n$$\nand $y_2 \\mid x \\sim \\mathcal{N}(x, \\sigma_2^2)$, so its PDF is\n$$\np(y_2 \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left( -\\frac{(y_2 - x)^2}{2\\sigma_2^2} \\right)\n$$\nThe posterior distribution is proportional to the product of these two functions. We can ignore the normalization constants as we are working with proportionality:\n$$\np(x \\mid y_1, y_2) \\propto \\exp\\left( -\\frac{(x - y_1)^2}{2\\sigma_1^2} \\right) \\exp\\left( -\\frac{(x - y_2)^2}{2\\sigma_2^2} \\right)\n$$\nCombining the exponential terms:\n$$\np(x \\mid y_1, y_2) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x - y_1)^2}{\\sigma_1^2} + \\frac{(x - y_2)^2}{\\sigma_2^2} \\right] \\right)\n$$\nTo identify the form of this distribution, we analyze the argument of the exponential, which we denote as $L(x)$:\n$$\nL(x) = \\frac{(x - y_1)^2}{\\sigma_1^2} + \\frac{(x - y_2)^2}{\\sigma_2^2}\n$$\nWe expand the squared terms and group by powers of $x$:\n$$\nL(x) = \\frac{x^2 - 2xy_1 + y_1^2}{\\sigma_1^2} + \\frac{x^2 - 2xy_2 + y_2^2}{\\sigma_2^2}\n$$\n$$\nL(x) = \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} \\right)x^2 - 2\\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} \\right)x + \\left( \\frac{y_1^2}{\\sigma_1^2} + \\frac{y_2^2}{\\sigma_2^2} \\right)\n$$\nThis is a quadratic function of $x$. We can see that the posterior distribution is also Gaussian, since its log-PDF is a quadratic function of $x$. A generic Gaussian PDF for $x$ with mean $\\mu_{post}$ and variance $\\sigma_{post}^2$ has the form:\n$$\np(x) \\propto \\exp\\left( -\\frac{(x - \\mu_{post})^2}{2\\sigma_{post}^2} \\right)\n$$\nThe argument in the exponential is $-\\frac{1}{2} \\left( \\frac{x^2 - 2x\\mu_{post} + \\mu_{post}^2}{\\sigma_{post}^2} \\right)$. Comparing the coefficients of the $x^2$ and $x$ terms in our derived expression for $L(x)$ with the standard form, we can identify $\\mu_{post}$ and $\\sigma_{post}^2$.\nComparing the $x^2$ terms:\n$$\n\\frac{1}{\\sigma_{post}^2} = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\n$$\nThis shows that the precision of the posterior (inverse variance) is the sum of the precisions of the individual measurements.\n\nComparing the $x$ terms:\n$$\n\\frac{2\\mu_{post}}{\\sigma_{post}^2} = 2\\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} \\right)\n$$\nSolving for the posterior mean, $\\mu_{post}$:\n$$\n\\mu_{post} = \\sigma_{post}^2 \\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} \\right) = \\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\n$$\nThus, the posterior distribution $p(x \\mid y_1, y_2)$ is a Gaussian distribution $\\mathcal{N}(\\mu_{post}, \\sigma_{post}^2)$. The posterior mean, $\\mu_{post}$, is the precision-weighted average of the measurements $y_1$ and $y_2$.\n\nFor any posterior distribution, the Minimum Mean-Squared Error (MMSE) estimator of a parameter $x$ is defined as the expectation of $x$ under that posterior distribution, $\\hat{x}_{MMSE} = E[x \\mid y_1, y_2]$. For a Gaussian posterior distribution, the expected value is its mean. Therefore, the derived posterior mean $\\mu_{post}$ is the MMSE estimator for $x$.\n\nNow, we compute the numerical value of this estimate using the given data:\n- $y_1 = 335.6$ K\n- $\\sigma_1 = 0.4$ K $\\implies \\sigma_1^2 = 0.16$ K$^2$\n- $y_2 = 336.9$ K\n- $\\sigma_2 = 0.9$ K $\\implies \\sigma_2^2 = 0.81$ K$^2$\n\nFirst, calculate the precisions (inverse variances):\n$$\n\\frac{1}{\\sigma_1^2} = \\frac{1}{0.16} = 6.25 \\text{ K}^{-2}\n$$\n$$\n\\frac{1}{\\sigma_2^2} = \\frac{1}{0.81} \\text{ K}^{-2}\n$$\nNow, substitute these values into the formula for $\\mu_{post}$:\n$$\n\\mu_{post} = \\frac{y_1 \\left(\\frac{1}{\\sigma_1^2}\\right) + y_2 \\left(\\frac{1}{\\sigma_2^2}\\right)}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}} = \\frac{(335.6)(6.25) + (336.9)\\left(\\frac{1}{0.81}\\right)}{6.25 + \\frac{1}{0.81}}\n$$\n$$\n\\mu_{post} = \\frac{2097.5 + \\frac{336.9}{0.81}}{6.25 + \\frac{1}{0.81}} = \\frac{2097.5 + 415.9259...}{6.25 + 1.2345...}\n$$\n$$\n\\mu_{post} = \\frac{2513.4259...}{7.4845...} \\approx 335.8144329... \\text{ K}\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $3$, $3$, $5$, and $8$. The following digit is $1$, so we round down.\nThe fused posterior mean estimate is $335.8$ K.",
            "answer": "$$\\boxed{335.8}$$"
        },
        {
            "introduction": "Real-world estimation problems rarely involve simple linear models. This practice moves from foundational theory to practical application by addressing the challenge of nonlinear measurements, a common scenario in robotics and navigation. You will implement the measurement update step of an Extended Kalman Filter (EKF) for an underwater vehicle whose position is observed via range and bearing sensors. This exercise will solidify your understanding of how to linearize a system model using a Jacobian matrix to apply the principles of Bayesian updates to complex, nonlinear systems .",
            "id": "4233142",
            "problem": "A centralized multi-sensor fusion architecture is used in the Digital Twin (DT) of an underwater autonomous vehicle within a Cyber-Physical System (CPS). At a discrete time index $k$, the DT maintains a prior Gaussian state estimate for the vehicle’s planar position, modeled as $x \\in \\mathbb{R}^{2}$ with $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$, having prior mean $\\hat{x}_{k|k-1}$ and prior covariance $P_{k|k-1}$. Two heterogeneous sensors independently observe a stationary target at the origin: a forward-looking sonar provides a range measurement and a monocular camera provides a bearing measurement. The fused measurement vector is\n$$\ny = \\begin{pmatrix} y_{r} \\\\ y_{\\theta} \\end{pmatrix} = h(x) + v,\n$$\nwhere $h(x) = \\begin{pmatrix} \\sqrt{x_{1}^{2} + x_{2}^{2}} \\\\ \\operatorname{arctan2}(x_{2}, x_{1}) \\end{pmatrix}$ and $v \\sim \\mathcal{N}(0, R)$ with $R \\in \\mathbb{R}^{2 \\times 2}$ positive definite. Assume the sensors are conditionally independent given the state, so $R$ is diagonal. The Extended Kalman Filter (EKF) measurement update is to be derived from first principles by linearizing the nonlinear measurement model about the prior mean and applying the linear-Gaussian Bayesian update.\n\nGiven:\n- Prior mean $\\hat{x}_{k|k-1} = \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix}$ (meters),\n- Prior covariance $P_{k|k-1} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix}$ (meters squared),\n- Measurement noise covariance $R = \\begin{pmatrix} 4.0 & 0 \\\\ 0 & 1.0 \\times 10^{-4} \\end{pmatrix}$ with range variance in meters squared and bearing variance in radians squared,\n- Actual fused measurement $y = \\begin{pmatrix} 49.0 \\\\ 0.93 \\end{pmatrix}$ with range in meters and bearing in radians.\n\nTasks:\n1. Starting strictly from the first-order Taylor linearization of $h(x)$ about $\\hat{x}_{k|k-1}$ and the laws of linear-Gaussian Bayesian estimation, derive the EKF measurement update for the posterior mean $\\hat{x}_{k|k}$ in terms of the Jacobian matrix $H = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_{k|k-1}}$, the prior covariance $P_{k|k-1}$, and the measurement noise covariance $R$.\n2. Compute the Jacobian $H$ at $x = \\hat{x}_{k|k-1}$ for the given $h(x)$.\n3. Use the derived update to compute the posterior mean $\\hat{x}_{k|k}$ numerically for the provided data.\n\nRound your final posterior state components to four significant figures. Express each component in meters. Provide your final answer as a single row vector.",
            "solution": "The user wants me to solve an EKF problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   State vector: $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} \\in \\mathbb{R}^{2}$\n*   Prior mean at time $k$: $\\hat{x}_{k|k-1} = \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix}$ meters\n*   Prior covariance at time $k$: $P_{k|k-1} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix}$ meters$^2$\n*   Measurement model: $y = h(x) + v$, where $y = \\begin{pmatrix} y_{r} \\\\ y_{\\theta} \\end{pmatrix}$\n*   Measurement function: $h(x) = \\begin{pmatrix} \\sqrt{x_{1}^{2} + x_{2}^{2}} \\\\ \\operatorname{arctan2}(x_{2}, x_{1}) \\end{pmatrix}$\n*   Measurement noise: $v \\sim \\mathcal{N}(0, R)$\n*   Measurement noise covariance: $R = \\begin{pmatrix} 4.0 & 0 \\\\ 0 & 1.0 \\times 10^{-4} \\end{pmatrix}$ (range variance in m$^2$, bearing variance in rad$^2$)\n*   Actual measurement at time $k$: $y_k = \\begin{pmatrix} 49.0 \\\\ 0.93 \\end{pmatrix}$ (range in m, bearing in rad)\n*   Tasks:\n    1.  Derive the EKF measurement update for the posterior mean $\\hat{x}_{k|k}$ from first principles.\n    2.  Compute the Jacobian $H = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_{k|k-1}}$.\n    3.  Compute the numerical value of $\\hat{x}_{k|k}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is an application of the Extended Kalman Filter, a standard and fundamental algorithm in statistical signal processing and control theory for state estimation in nonlinear systems. The physical model (planar position, range, bearing) and mathematical framework (Gaussian statistics, Bayesian updates, Taylor series linearization) are all well-established and correct.\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary numerical values and functional forms to execute the required tasks. The prior covariance matrix $P_{k|k-1}$ is symmetric and positive definite (determinant is $9 \\times 16 - 3 \\times 3 = 135 > 0$). The measurement noise covariance $R$ is diagonal and positive definite. This ensures that all required matrix inversions will be possible. A unique solution for the posterior mean exists.\n3.  **Objective**: The problem is stated in precise, objective mathematical and engineering terms. It is free from ambiguity and subjective claims.\n4.  **Complete and Consistent**: All necessary givens, including prior statistics, measurement model, noise statistics, and the actual measurement, are provided. The dimensions of all vectors and matrices are consistent. For example, $x \\in \\mathbb{R}^2$, $y \\in \\mathbb{R}^2$, $P \\in \\mathbb{R}^{2 \\times 2}$, and $R \\in \\mathbb{R}^{2 \\times 2}$. The units are also consistent.\n5.  **Feasible**: All computations are tractable and based on standard matrix algebra and calculus.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution proceeds by addressing the three tasks in the specified order.\n\n**Task 1: Derivation of the EKF Measurement Update**\n\nThe foundation of the Extended Kalman Filter (EKF) measurement update is the linearization of the nonlinear measurement function $h(x)$ around the prior state estimate $\\hat{x}_{k|k-1}$. The measurement model is\n$$y_k = h(x_k) + v_k, \\quad v_k \\sim \\mathcal{N}(0, R_k)$$\nWe perform a first-order Taylor series expansion of $h(x_k)$ around the prior mean $\\hat{x}_{k|k-1}$:\n$$h(x_k) \\approx h(\\hat{x}_{k|k-1}) + \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_{k|k-1}} (x_k - \\hat{x}_{k|k-1})$$\nLet $H_k = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_{k|k-1}}$ be the Jacobian of the measurement function evaluated at the prior mean. The measurement model can be approximated as:\n$$y_k \\approx h(\\hat{x}_{k|k-1}) + H_k (x_k - \\hat{x}_{k|k-1}) + v_k$$\nThis equation can be rearranged to define the innovation, or measurement residual, $\\tilde{y}_k$:\n$$\\tilde{y}_k = y_k - h(\\hat{x}_{k|k-1}) \\approx H_k (x_k - \\hat{x}_{k|k-1}) + v_k$$\nLet the prior estimation error be $e_{k|k-1} = x_k - \\hat{x}_{k|k-1}$. By definition, this error has a zero mean and covariance $P_{k|k-1}$, so $e_{k|k-1} \\sim \\mathcal{N}(0, P_{k|k-1})$. The innovation can now be expressed as:\n$$\\tilde{y}_k \\approx H_k e_{k|k-1} + v_k$$\nThis establishes a linear relationship between the innovation $\\tilde{y}_k$ and the prior error $e_{k|k-1}$. We are now in the framework of linear-Gaussian Bayesian estimation. We seek the Minimum Mean Square Error (MMSE) estimate of the error, $\\hat{e}_{k|k} = \\mathbb{E}[e_{k|k-1} | \\tilde{y}_k]$.\n\nFor jointly Gaussian variables, the conditional expectation is given by:\n$$\\hat{e}_{k|k} = \\mathbb{E}[e_{k|k-1}] + \\text{Cov}(e_{k|k-1}, \\tilde{y}_k) \\text{Var}(\\tilde{y}_k)^{-1} (\\tilde{y}_k - \\mathbb{E}[\\tilde{y}_k])$$\nThe required expectations and covariances are:\n1.  $\\mathbb{E}[e_{k|k-1}] = 0$.\n2.  $\\mathbb{E}[\\tilde{y}_k] \\approx \\mathbb{E}[H_k e_{k|k-1} + v_k] = H_k \\mathbb{E}[e_{k|k-1}] + \\mathbb{E}[v_k] = 0$.\n3.  The innovation covariance, $S_k = \\text{Var}(\\tilde{y}_k) = \\mathbb{E}[\\tilde{y}_k \\tilde{y}_k^T] \\approx \\mathbb{E}[(H_k e_{k|k-1} + v_k)(H_k e_{k|k-1} + v_k)^T]$. Since the state error $e_{k|k-1}$ and measurement noise $v_k$ are uncorrelated, this expands to:\n    $$S_k = H_k \\mathbb{E}[e_{k|k-1}e_{k|k-1}^T] H_k^T + \\mathbb{E}[v_k v_k^T] = H_k P_{k|k-1} H_k^T + R_k$$\n4.  The cross-covariance between the state error and the innovation is $\\text{Cov}(e_{k|k-1}, \\tilde{y}_k) = \\mathbb{E}[e_{k|k-1} \\tilde{y}_k^T] \\approx \\mathbb{E}[e_{k|k-1}(H_k e_{k|k-1} + v_k)^T]$. Again, using the uncorrelation of $e_{k|k-1}$ and $v_k$:\n    $$\\text{Cov}(e_{k|k-1}, \\tilde{y}_k) = \\mathbb{E}[e_{k|k-1}e_{k|k-1}^T]H_k^T = P_{k|k-1}H_k^T$$\nSubstituting these into the conditional expectation formula gives the estimate for the error correction:\n$$\\hat{e}_{k|k} = (P_{k|k-1}H_k^T) (H_k P_{k|k-1} H_k^T + R_k)^{-1} \\tilde{y}_k$$\nWe define the Kalman Gain $K_k$ as:\n$$K_k = P_{k|k-1}H_k^T S_k^{-1} = P_{k|k-1}H_k^T (H_k P_{k|k-1} H_k^T + R_k)^{-1}$$\nThe posterior state estimate, $\\hat{x}_{k|k}$, is the prior estimate plus the estimated correction:\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + \\hat{e}_{k|k}$$\nSubstituting the expressions for $\\hat{e}_{k|k}$ and $\\tilde{y}_k$, we arrive at the final EKF measurement update equation for the posterior mean:\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - h(\\hat{x}_{k|k-1}))$$\n\n**Task 2: Computation of the Jacobian**\n\nThe measurement function is $h(x) = \\begin{pmatrix} h_1(x_1, x_2) \\\\ h_2(x_1, x_2) \\end{pmatrix} = \\begin{pmatrix} \\sqrt{x_{1}^{2} + x_{2}^{2}} \\\\ \\operatorname{arctan2}(x_{2}, x_{1}) \\end{pmatrix}$. The Jacobian matrix $H$ is the matrix of all first-order partial derivatives:\n$$H = \\frac{\\partial h}{\\partial x} = \\begin{pmatrix} \\frac{\\partial h_1}{\\partial x_1} & \\frac{\\partial h_1}{\\partial x_2} \\\\ \\frac{\\partial h_2}{\\partial x_1} & \\frac{\\partial h_2}{\\partial x_2} \\end{pmatrix}$$\nThe partial derivatives are:\n$\\frac{\\partial h_1}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (x_1^2 + x_2^2)^{1/2} = \\frac{1}{2}(x_1^2 + x_2^2)^{-1/2}(2x_1) = \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}}$\n$\\frac{\\partial h_1}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (x_1^2 + x_2^2)^{1/2} = \\frac{1}{2}(x_1^2 + x_2^2)^{-1/2}(2x_2) = \\frac{x_2}{\\sqrt{x_1^2 + x_2^2}}$\n$\\frac{\\partial h_2}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\arctan\\left(\\frac{x_2}{x_1}\\right) = \\frac{1}{1 + (x_2/x_1)^2} \\left(-\\frac{x_2}{x_1^2}\\right) = \\frac{-x_2}{x_1^2 + x_2^2}$\n$\\frac{\\partial h_2}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\arctan\\left(\\frac{x_2}{x_1}\\right) = \\frac{1}{1 + (x_2/x_1)^2} \\left(\\frac{1}{x_1}\\right) = \\frac{x_1}{x_1^2 + x_2^2}$\nSo, the Jacobian is:\n$$H(x) = \\begin{pmatrix} \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}} & \\frac{x_2}{\\sqrt{x_1^2 + x_2^2}} \\\\ \\frac{-x_2}{x_1^2 + x_2^2} & \\frac{x_1}{x_1^2 + x_2^2} \\end{pmatrix}$$\nWe evaluate this at the prior mean $\\hat{x}_{k|k-1} = \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix}$. First, we compute the range $r = \\sqrt{30^2 + 40^2} = \\sqrt{900 + 1600} = \\sqrt{2500} = 50$ meters.\n$$H_k = H(\\hat{x}_{k|k-1}) = \\begin{pmatrix} \\frac{30}{50} & \\frac{40}{50} \\\\ \\frac{-40}{50^2} & \\frac{30}{50^2} \\end{pmatrix} = \\begin{pmatrix} 0.6 & 0.8 \\\\ \\frac{-40}{2500} & \\frac{30}{2500} \\end{pmatrix} = \\begin{pmatrix} 0.6 & 0.8 \\\\ -0.016 & 0.012 \\end{pmatrix}$$\n\n**Task 3: Numerical Computation of the Posterior Mean**\n\nWe use the derived update equation $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - h(\\hat{x}_{k|k-1}))$. The steps are as follows:\n\n1.  **Compute the predicted measurement and innovation:**\n    The predicted measurement is $h(\\hat{x}_{k|k-1})$:\n    $$h(\\hat{x}_{k|k-1}) = \\begin{pmatrix} \\sqrt{30^2 + 40^2} \\\\ \\operatorname{arctan2}(40, 30) \\end{pmatrix} = \\begin{pmatrix} 50 \\\\ \\arctan(4/3) \\end{pmatrix} \\approx \\begin{pmatrix} 50 \\\\ 0.927295 \\end{pmatrix}$$\n    The innovation $\\tilde{y}_k$ is the difference between the actual measurement $y_k$ and the predicted measurement:\n    $$\\tilde{y}_k = y_k - h(\\hat{x}_{k|k-1}) = \\begin{pmatrix} 49.0 \\\\ 0.93 \\end{pmatrix} - \\begin{pmatrix} 50 \\\\ 0.927295 \\end{pmatrix} = \\begin{pmatrix} -1.0 \\\\ 0.002705 \\end{pmatrix}$$\n\n2.  **Compute the innovation covariance $S_k$:**\n    $$S_k = H_k P_{k|k-1} H_k^T + R_k$$\n    $$H_k P_{k|k-1} = \\begin{pmatrix} 0.6 & 0.8 \\\\ -0.016 & 0.012 \\end{pmatrix} \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix} = \\begin{pmatrix} 7.8 & 14.6 \\\\ -0.108 & 0.144 \\end{pmatrix}$$\n    $$H_k P_{k|k-1} H_k^T = \\begin{pmatrix} 7.8 & 14.6 \\\\ -0.108 & 0.144 \\end{pmatrix} \\begin{pmatrix} 0.6 & -0.016 \\\\ 0.8 & 0.012 \\end{pmatrix} = \\begin{pmatrix} 16.36 & 0.0504 \\\\ 0.0504 & 0.003456 \\end{pmatrix}$$\n    $$S_k = \\begin{pmatrix} 16.36 & 0.0504 \\\\ 0.0504 & 0.003456 \\end{pmatrix} + \\begin{pmatrix} 4.0 & 0 \\\\ 0 & 0.0001 \\end{pmatrix} = \\begin{pmatrix} 20.36 & 0.0504 \\\\ 0.0504 & 0.003556 \\end{pmatrix}$$\n\n3.  **Compute the Kalman Gain $K_k$:**\n    $$K_k = P_{k|k-1} H_k^T S_k^{-1}$$\n    $$P_{k|k-1} H_k^T = \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix} \\begin{pmatrix} 0.6 & -0.016 \\\\ 0.8 & 0.012 \\end{pmatrix} = \\begin{pmatrix} 7.8 & -0.108 \\\\ 14.6 & 0.144 \\end{pmatrix}$$\n    The inverse of $S_k$ is $S_k^{-1} = \\frac{1}{\\det(S_k)} \\begin{pmatrix} 0.003556 & -0.0504 \\\\ -0.0504 & 20.36 \\end{pmatrix}$.\n    $\\det(S_k) = (20.36)(0.003556) - (0.0504)^2 = 0.07238016 - 0.00254016 = 0.06984$.\n    $$K_k = \\frac{1}{0.06984} \\begin{pmatrix} 7.8 & -0.108 \\\\ 14.6 & 0.144 \\end{pmatrix} \\begin{pmatrix} 0.003556 & -0.0504 \\\\ -0.0504 & 20.36 \\end{pmatrix}$$\n    $$K_k = \\frac{1}{0.06984} \\begin{pmatrix} (7.8)(0.003556)+(-0.108)(-0.0504) & (7.8)(-0.0504)+(-0.108)(20.36) \\\\ (14.6)(0.003556)+(0.144)(-0.0504) & (14.6)(-0.0504)+(0.144)(20.36) \\end{pmatrix}$$\n    $$K_k = \\frac{1}{0.06984} \\begin{pmatrix} 0.03318 & -2.592 \\\\ 0.04466 & 2.196 \\end{pmatrix} \\approx \\begin{pmatrix} 0.47514 & -37.1134 \\\\ 0.63946 & 31.4433 \\end{pmatrix}$$\n\n4.  **Compute the posterior mean $\\hat{x}_{k|k}$:**\n    The state update is $K_k \\tilde{y}_k$:\n    $$\\begin{pmatrix} 0.47514 & -37.1134 \\\\ 0.63946 & 31.4433 \\end{pmatrix} \\begin{pmatrix} -1.0 \\\\ 0.002705 \\end{pmatrix} = \\begin{pmatrix} -0.47514 - 0.10038 \\\\ -0.63946 + 0.08505 \\end{pmatrix} = \\begin{pmatrix} -0.57552 \\\\ -0.55441 \\end{pmatrix}$$\n    Finally, we add this correction to the prior mean:\n    $$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k \\tilde{y}_k = \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix} + \\begin{pmatrix} -0.57552 \\\\ -0.55441 \\end{pmatrix} = \\begin{pmatrix} 29.42448 \\\\ 39.44559 \\end{pmatrix}$$\n\nRounding the final components to four significant figures, we get:\n$\\hat{x}_{k|k,1} = 29.42$ meters.\n$\\hat{x}_{k|k,2} = 39.45$ meters.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 29.42 & 39.45 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After developing an estimation algorithm, a crucial question is: how good is it? This final practice introduces the tools for performance evaluation and benchmarking. You will derive the Cramér-Rao Lower Bound (CRLB), a theoretical limit on the best possible accuracy for any unbiased estimator, by computing the Fisher Information Matrix for a given multi-sensor system. By comparing the performance of a simple, unweighted least-squares estimator to this fundamental bound, you will quantify the concept of statistical efficiency and appreciate why sophisticated fusion algorithms like the Kalman filter are essential for achieving optimal performance .",
            "id": "4233217",
            "problem": "A cyber-physical system digital twin monitors the planar position of a rigid body whose true state vector is $\\boldsymbol{\\theta} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$. A multi-sensor fusion architecture produces three independent scalar measurements $\\{z_{i}\\}_{i=1}^{3}$ with linear measurement models and zero-mean Gaussian noise:\n$z_{1} = x + n_{1}$, $z_{2} = y + n_{2}$, $z_{3} = x + y + n_{3}$,\nwhere $n_{1}$, $n_{2}$, and $n_{3}$ are mutually independent, zero-mean Gaussian noises with variances $\\operatorname{Var}(n_{1}) = 0.04$, $\\operatorname{Var}(n_{2}) = 0.01$, and $\\operatorname{Var}(n_{3}) = 0.09$, respectively. The stacked measurement model can be written as $\\mathbf{z} = \\mathbf{H}\\boldsymbol{\\theta} + \\mathbf{n}$ with\n$\\mathbf{H} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}$ and $\\mathbf{R} = \\operatorname{diag}\\!\\big(0.04,\\,0.01,\\,0.09\\big)$.\n\nStarting from the definition of the likelihood for independent Gaussian measurements and the associated log-likelihood, derive the Fisher information matrix (FIM) for $\\boldsymbol{\\theta}$ and then the Cramér–Rao lower bound (CRLB) for any unbiased estimator of $x$ by computing the appropriate element of the inverse of the FIM. Next, consider the naive unweighted Least Squares (LS) estimator that ignores heteroscedasticity, defined by $\\hat{\\boldsymbol{\\theta}}_{\\text{LS}} = \\big(\\mathbf{H}^{\\top}\\mathbf{H}\\big)^{-1}\\mathbf{H}^{\\top}\\mathbf{z}$. Derive the covariance of $\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}$ under the true noise covariance $\\mathbf{R}$, and compute the variance of its first component (the estimator of $x$). Form the ratio $r$ of this variance to the CRLB for $x$.\n\nReport the final ratio $r$ as an exact value. Do not round. The final answer must be a single real number without units.",
            "solution": "The problem asks for the ratio of the variance of an unweighted Least Squares (LS) estimator for the state component $x$ to the Cramér–Rao lower bound (CRLB) for the same component. The solution will proceed in three main steps:\n1.  Derive the Fisher information matrix (FIM) and compute the CRLB for the variance of any unbiased estimator of $x$.\n2.  Derive the covariance matrix for the unweighted LS estimator $\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}$ and find the variance of its first component, $\\hat{x}_{\\text{LS}}$.\n3.  Compute the ratio of the variance from step 2 to the CRLB from step 1.\n\nThe state vector is $\\boldsymbol{\\theta} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$. The linear measurement model is $\\mathbf{z} = \\mathbf{H}\\boldsymbol{\\theta} + \\mathbf{n}$, where $\\mathbf{z} \\in \\mathbb{R}^3$, $\\mathbf{n} \\in \\mathbb{R}^3$ is a vector of zero-mean, mutually independent Gaussian noises. The measurement matrix $\\mathbf{H}$ and the noise covariance matrix $\\mathbf{R}$ are given by:\n$$\n\\mathbf{H} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad \\mathbf{R} = \\operatorname{Cov}(\\mathbf{n}) = \\begin{pmatrix} 0.04 & 0 & 0 \\\\ 0 & 0.01 & 0 \\\\ 0 & 0 & 0.09 \\end{pmatrix}\n$$\nThe noise vector $\\mathbf{n}$ follows a multivariate Gaussian distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{R})$. Consequently, the measurement vector $\\mathbf{z}$ follows the distribution $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{H}\\boldsymbol{\\theta}, \\mathbf{R})$.\n\n**Step 1: Fisher Information Matrix and Cramér–Rao Lower Bound**\n\nThe likelihood function $p(\\mathbf{z}|\\boldsymbol{\\theta})$ for the measurements $\\mathbf{z}$ given the parameters $\\boldsymbol{\\theta}$ is the probability density function of the multivariate normal distribution:\n$$\np(\\mathbf{z}|\\boldsymbol{\\theta}) = \\frac{1}{(2\\pi)^{3/2} \\det(\\mathbf{R})^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{z} - \\mathbf{H}\\boldsymbol{\\theta})^{\\top} \\mathbf{R}^{-1} (\\mathbf{z} - \\mathbf{H}\\boldsymbol{\\theta}) \\right)\n$$\nThe log-likelihood function $\\mathcal{L}(\\boldsymbol{\\theta})$ is:\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\ln p(\\mathbf{z}|\\boldsymbol{\\theta}) = C - \\frac{1}{2} (\\mathbf{z} - \\mathbf{H}\\boldsymbol{\\theta})^{\\top} \\mathbf{R}^{-1} (\\mathbf{z} - \\mathbf{H}\\boldsymbol{\\theta})\n$$\nwhere $C$ is a constant that does not depend on $\\boldsymbol{\\theta}$. The Fisher information matrix (FIM), $\\mathbf{J}$, is defined as $\\mathbf{J} = -E\\left[ \\frac{\\partial^2 \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^{\\top}} \\right]$. For a linear-Gaussian model, this simplifies to the well-known form:\n$$\n\\mathbf{J} = \\mathbf{H}^{\\top} \\mathbf{R}^{-1} \\mathbf{H}\n$$\nWe first compute $\\mathbf{R}^{-1}$. The given variances are $\\sigma_1^2 = 0.04 = \\frac{4}{100} = \\frac{1}{25}$, $\\sigma_2^2 = 0.01 = \\frac{1}{100}$, and $\\sigma_3^2 = 0.09 = \\frac{9}{100}$.\n$$\n\\mathbf{R}^{-1} = \\begin{pmatrix} 1/0.04 & 0 & 0 \\\\ 0 & 1/0.01 & 0 \\\\ 0 & 0 & 1/0.09 \\end{pmatrix} = \\begin{pmatrix} 25 & 0 & 0 \\\\ 0 & 100 & 0 \\\\ 0 & 0 & 100/9 \\end{pmatrix}\n$$\nNow, we compute the FIM, $\\mathbf{J}$:\n$$\n\\mathbf{J} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 25 & 0 & 0 \\\\ 0 & 100 & 0 \\\\ 0 & 0 & 100/9 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\n\\mathbf{J} = \\begin{pmatrix} 25 & 0 & 100/9 \\\\ 0 & 100 & 100/9 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\n\\mathbf{J} = \\begin{pmatrix} 25 + 100/9 & 100/9 \\\\ 100/9 & 100 + 100/9 \\end{pmatrix} = \\begin{pmatrix} (225+100)/9 & 100/9 \\\\ 100/9 & (900+100)/9 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 325 & 100 \\\\ 100 & 1000 \\end{pmatrix}\n$$\nThe Cramér–Rao lower bound matrix is the inverse of the FIM, $\\mathbf{C}_{\\text{CRLB}} = \\mathbf{J}^{-1}$. For any unbiased estimator $\\hat{\\boldsymbol{\\theta}}$, its covariance matrix satisfies $\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}) \\succeq \\mathbf{J}^{-1}$ (in the Loewner order). The CRLB for the variance of an estimator for $x$ is the first diagonal element of $\\mathbf{J}^{-1}$.\n$$\n\\det(\\mathbf{J}) = \\left(\\frac{1}{9}\\right)^2 (325 \\times 1000 - 100 \\times 100) = \\frac{1}{81} (325000 - 10000) = \\frac{315000}{81}\n$$\n$$\n\\mathbf{J}^{-1} = \\frac{1}{\\det(\\mathbf{J})} \\begin{pmatrix} J_{22} & -J_{12} \\\\ -J_{21} & J_{11} \\end{pmatrix} = \\frac{81}{315000} \\frac{1}{9} \\begin{pmatrix} 1000 & -100 \\\\ -100 & 325 \\end{pmatrix} = \\frac{9}{315000} \\begin{pmatrix} 1000 & -100 \\\\ -100 & 325 \\end{pmatrix}\n$$\nThe CRLB for $x$, which we denote as $V_{\\text{CRLB}}$, is the $(1,1)$ element of $\\mathbf{J}^{-1}$:\n$$\nV_{\\text{CRLB}} = (\\mathbf{J}^{-1})_{11} = \\frac{9 \\times 1000}{315000} = \\frac{9000}{315000} = \\frac{9}{315} = \\frac{1}{35}\n$$\n\n**Step 2: Unweighted Least Squares Estimator Variance**\n\nThe unweighted LS estimator is given by $\\hat{\\boldsymbol{\\theta}}_{\\text{LS}} = \\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\top}\\mathbf{z}$. We need to find the covariance of this estimator under the true noise covariance $\\mathbf{R}$. The covariance of a linear transformation $\\mathbf{A}\\mathbf{z}$ of a random vector $\\mathbf{z}$ is $\\mathbf{A}\\operatorname{Cov}(\\mathbf{z})\\mathbf{A}^{\\top}$. Here, $\\mathbf{A} = \\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\top}$ and $\\operatorname{Cov}(\\mathbf{z})=\\mathbf{R}$.\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}) = \\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1}\\mathbf{H}^{\\top} \\mathbf{R} \\mathbf{H} \\left(\\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1}\\right)^{\\top}\n$$\nSince $\\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1}$ is symmetric, this simplifies to:\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}) = \\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1} \\left(\\mathbf{H}^{\\top}\\mathbf{R}\\mathbf{H}\\right) \\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1}\n$$\nFirst, let's compute the components:\n$$\n\\mathbf{H}^{\\top}\\mathbf{H} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\n$$\n\\left(\\mathbf{H}^{\\top}\\mathbf{H}\\right)^{-1} = \\frac{1}{2 \\cdot 2 - 1 \\cdot 1} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\nNext, we compute $\\mathbf{H}^{\\top}\\mathbf{R}\\mathbf{H}$:\n$$\n\\mathbf{H}^{\\top}\\mathbf{R}\\mathbf{H} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/25 & 0 & 0 \\\\ 0 & 1/100 & 0 \\\\ 0 & 0 & 9/100 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\n\\mathbf{H}^{\\top}\\mathbf{R}\\mathbf{H} = \\frac{1}{100} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 9 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\frac{1}{100} \\begin{pmatrix} 4 & 0 & 9 \\\\ 0 & 1 & 9 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\n\\mathbf{H}^{\\top}\\mathbf{R}\\mathbf{H} = \\frac{1}{100} \\begin{pmatrix} 4+9 & 9 \\\\ 9 & 1+9 \\end{pmatrix} = \\frac{1}{100} \\begin{pmatrix} 13 & 9 \\\\ 9 & 10 \\end{pmatrix}\n$$\nNow assemble the full covariance matrix:\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}) = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\frac{1}{100} \\begin{pmatrix} 13 & 9 \\\\ 9 & 10 \\end{pmatrix} \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}) = \\frac{1}{900} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 13 & 9 \\\\ 9 & 10 \\end{pmatrix} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}) = \\frac{1}{900} \\begin{pmatrix} 2(13)-1(9) & 2(9)-1(10) \\\\ -1(13)+2(9) & -1(9)+2(10) \\end{pmatrix} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\frac{1}{900} \\begin{pmatrix} 17 & 8 \\\\ 5 & 11 \\end{pmatrix} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_{\\text{LS}}) = \\frac{1}{900} \\begin{pmatrix} 17(2)+8(-1) & 17(-1)+8(2) \\\\ 5(2)+11(-1) & 5(-1)+11(2) \\end{pmatrix} = \\frac{1}{900} \\begin{pmatrix} 26 & -1 \\\\ -1 & 17 \\end{pmatrix}\n$$\nThe variance of the LS estimator of $x$, denoted $V_{\\text{LS}}$, is the $(1,1)$ element of this matrix:\n$$\nV_{\\text{LS}} = \\operatorname{Var}(\\hat{x}_{\\text{LS}}) = \\frac{26}{900}\n$$\n\n**Step 3: Compute the Ratio**\n\nThe final step is to compute the ratio $r = \\frac{V_{\\text{LS}}}{V_{\\text{CRLB}}}$.\n$$\nr = \\frac{26/900}{1/35} = \\frac{26}{900} \\times 35 = \\frac{26 \\times 35}{900}\n$$\nThe numerator is $26 \\times 35 = 910$.\n$$\nr = \\frac{910}{900} = \\frac{91}{90}\n$$\nThis ratio represents the efficiency of the unweighted LS estimator relative to the theoretical best possible performance for an unbiased estimator. Since the LS estimator does not account for the heteroscedasticity of the noise (i.e., unequal variances), it is suboptimal, and its variance is strictly greater than the CRLB, so $r > 1$, which is consistent with our result.",
            "answer": "$$\\boxed{\\frac{91}{90}}$$"
        }
    ]
}