{
    "hands_on_practices": [
        {
            "introduction": "Before deploying a Kalman filter, it is essential to understand the conditions under which it will perform reliably. This practice explores the concept of detectability, a fundamental property ensuring the boundedness of the filter's error covariance. By simulating the Riccati recursion for systems with different stability and observability properties, you will directly observe how an unobservable unstable mode causes the filter error to diverge, a critical failure condition to avoid in any real-world digital twin .",
            "id": "4228697",
            "problem": "A discrete-time linear time-invariant cyber-physical estimation problem concerns a system with state vector $x_k \\in \\mathbb{R}^n$ evolving as $x_{k+1} = A x_k + w_k$ and output $y_k = C x_k + v_k$, where $A \\in \\mathbb{R}^{n \\times n}$, $C \\in \\mathbb{R}^{p \\times n}$, $w_k$ and $v_k$ are zero-mean Gaussian white noises with covariances $Q \\succ 0$ and $R \\succ 0$, respectively. Consider the state estimation in a digital twin for a cyber-physical system using a Kalman Filter (KF). A pair $(A,C)$ is called detectable if every unobservable mode is stable, equivalently, every eigenvalue $\\lambda$ of $A$ with $|\\lambda| \\ge 1$ is observable by $C$. When $(A,C)$ is not detectable because of an unstable unobservable mode, the covariance recursion associated with the KF can diverge.\n\nYour task is to rigorously demonstrate, by direct simulation and verification, that the covariance Riccati recursion diverges when an unstable mode is unobservable, and remains bounded when all unstable modes are observable, or when unobservable modes are stable. You must implement the discrete-time Kalman covariance recursion for given parameter sets and automatically assess divergence. No formulas are provided here; your program must rely on first principles and well-tested facts to construct the correct recursion.\n\nImplement a program that:\n- Initializes with a given covariance $P_0 \\succeq 0$ and iterates the discrete-time Kalman covariance recursion for $N$ steps.\n- Declares divergence if at any iteration $k \\in \\{1,\\dots,N\\}$ the largest eigenvalue of the covariance $P_k$ exceeds a prescribed threshold $\\tau$, or if $P_k$ loses finiteness (e.g., contains $\\infty$ or $\\text{NaN}$).\n- Returns a boolean for each test case indicating divergence (true) or boundedness (false).\n\nUse the following test suite with $n=2$ states and scalar output $p=1$, and with common noise covariances and initial covariance:\n- $Q = \\operatorname{diag}(q_1, q_2)$ with $q_1 = 0.01$, $q_2 = 0.01$.\n- $R = [r]$ with $r = 0.1$.\n- $P_0 = I_2$.\n- $N = 200$ iterations.\n- Divergence threshold $\\tau = 10^{12}$.\n\nTest cases:\n1. Not detectable with an unstable unobservable mode:\n   - $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix}$.\n   - The unstable mode at eigenvalue $1.2$ is unobservable because it lies entirely in the subspace orthogonal to the measurement.\n2. Detectable with an unstable observable mode:\n   - $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\n   - The unstable mode at eigenvalue $1.2$ is observable.\n3. Unobservable but stable mode:\n   - $A = \\begin{bmatrix} 0.95 & 0.0 \\\\ 0.0 & 0.7 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\n   - All modes are stable; the second mode is unobservable but does not violate detectability.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each $result_i$ is a boolean with lowercase letters indicating whether the corresponding test case diverges (true) or remains bounded (false). No physical units or angles are involved in this task, and answers should be booleans as specified above. Your implementation must be self-contained, require no input, and adhere to the specified runtime environment.",
            "solution": "The problem requires a direct simulation to demonstrate a fundamental principle of Kalman filtering: the boundedness of the filter's error covariance matrix is contingent upon the detectability of the system. We will first establish the theoretical basis and then describe the algorithmic implementation for verification.\n\nA discrete-time linear time-invariant system is defined by the state and measurement equations:\n$$\nx_{k+1} = A x_k + w_k\n$$\n$$\ny_k = C x_k + v_k\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state vector, $y_k \\in \\mathbb{R}^p$ is the measurement vector, $A \\in \\mathbb{R}^{n \\times n}$ is the state transition matrix, and $C \\in \\mathbb{R}^{p \\times n}$ is the measurement matrix. The process noise $w_k$ and measurement noise $v_k$ are zero-mean, white Gaussian processes with respective covariance matrices $Q \\succ 0$ and $R \\succ 0$.\n\nThe Kalman filter provides an optimal estimate of the state $x_k$. The filter's performance is characterized by the error covariance matrix $P_k = \\mathbb{E}[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^\\top]$, where $\\hat{x}_{k|k}$ is the a posteriori state estimate at time $k$. The evolution of $P_k$ is governed by the discrete-time matrix Riccati recursion. Starting with an initial covariance $P_0$, the recursion for $k=1, 2, \\dots, N$ is a two-step process:\n\n1.  **Time Update (Prediction):** The error covariance is propagated forward in time. The a priori covariance $P_{k|k-1}$ is computed from the a posteriori covariance of the previous step, $P_{k-1|k-1}$.\n    $$\n    P_{k|k-1} = A P_{k-1|k-1} A^\\top + Q\n    $$\n\n2.  **Measurement Update (Correction):** The a priori covariance is corrected using the information from the measurement $y_k$. The resulting a posteriori covariance $P_{k|k}$ is given by:\n    $$\n    P_{k|k} = P_{k|k-1} - P_{k|k-1} C^\\top (C P_{k|k-1} C^\\top + R)^{-1} C P_{k|k-1}\n    $$\nThis form is equivalent to the update equation $P_{k|k} = (I - K_k C)P_{k|k-1}$ where $K_k$ is the Kalman gain. More robust implementations often use the Joseph form of the covariance update, which ensures symmetry and positive semi-definiteness under finite-precision arithmetic.\n\nA cornerstone theorem in estimation theory states that the error covariance recursion $P_k$ converges to a unique, finite, positive semi-definite steady-state solution $P_\\infty$ if and only if the pair $(A, C)$ is detectable and the pair $(A, Q^{1/2})$ is stabilizable. A system $(A, C)$ is detectable if every unobservable mode of $A$ is stable. A mode, corresponding to an eigenvalue $\\lambda$ of $A$, is stable if $|\\lambda| < 1$ and unstable if $|\\lambda| \\ge 1$. An eigenvector $v$ associated with $\\lambda$ corresponds to an unobservable mode if it lies in the nullspace of the measurement matrix, i.e., $C v = 0$.\n\nIn all test cases provided, the process noise covariance $Q$ is a diagonal matrix with strictly positive entries, which means $Q \\succ 0$. This ensures that the pair $(A, Q^{1/2})$ is controllable (a stronger condition than stabilizable). Therefore, the convergence and boundedness of the Riccati recursion depend solely on the detectability of the pair $(A, C)$.\n\nThe task is to simulate this recursion for three specific cases and verify the theoretical predictions. Divergence is defined as the largest eigenvalue of $P_k$ exceeding a threshold $\\tau = 10^{12}$ or $P_k$ containing non-finite values.\n\n**Analysis of Test Cases:**\nThe common parameters are $n=2$, $p=1$, $Q = \\operatorname{diag}(0.01, 0.01)$, $R = [0.1]$, $P_0 = I_2$, $N=200$, and $\\tau = 10^{12}$.\n\n**Case 1:** $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix}$.\nThe eigenvalues of $A$ are $\\lambda_1 = 1.2$ and $\\lambda_2 = 0.9$. The mode associated with $\\lambda_1 = 1.2$ is unstable. The corresponding eigenvector is $v_1 = [1.0, 0.0]^\\top$. We test for observability:\n$$\nC v_1 = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix} \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix} = 0.0\n$$\nSince $C v_1 = 0$, the unstable mode is unobservable. The system is not detectable. According to theory, the error covariance $P_k$ is expected to diverge. The variance of the unobserved and unstable state will grow without bound. The simulation should result in `true`.\n\n**Case 2:** $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\nThe unstable mode is again at $\\lambda_1 = 1.2$ with eigenvector $v_1 = [1.0, 0.0]^\\top$. We test for observability:\n$$\nC v_1 = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix} = 1.0 \\neq 0.0\n$$\nThe unstable mode is observable. The other mode $\\lambda_2=0.9$ is stable. As all unstable modes are observable, the system is detectable. The theory predicts that $P_k$ will converge to a finite steady-state value. The simulation should result in `false`.\n\n**Case 3:** $A = \\begin{bmatrix} 0.95 & 0.0 \\\\ 0.0 & 0.7 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\nThe eigenvalues of $A$ are $\\lambda_1 = 0.95$ and $\\lambda_2 = 0.7$. Both modes are stable since $|\\lambda_i| < 1$. The eigenvector for $\\lambda_2 = 0.7$ is $v_2 = [0.0, 1.0]^\\top$. We test its observability:\n$$\nC v_2 = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix} = 0.0\n$$\nThe mode associated with $\\lambda_2=0.7$ is unobservable. However, since this unobservable mode is stable, the condition for detectability is met. The system is detectable. Consequently, the error covariance $P_k$ will remain bounded and converge. The simulation should result in `false`.\n\nThe program will implement the covariance recursion and test for the divergence condition at each of the $N=200$ iterations for all three cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per the specified environment\n\ndef solve():\n    \"\"\"\n    Simulates the discrete-time Kalman covariance recursion for three test cases\n    to demonstrate the effect of system detectability on filter stability.\n    \"\"\"\n    \n    # Common parameters for all test cases\n    Q = np.diag([0.01, 0.01])\n    R = np.array([[0.1]])\n    P0 = np.identity(2)\n    N = 200\n    tau = 1.0e12\n\n    # Test cases defined by the problem statement\n    test_cases = [\n        {\n            \"name\": \"Not detectable with an unstable unobservable mode\",\n            \"A\": np.array([[1.2, 0.0], [0.0, 0.9]]),\n            \"C\": np.array([[0.0, 1.0]]),\n        },\n        {\n            \"name\": \"Detectable with an unstable observable mode\",\n            \"A\": np.array([[1.2, 0.0], [0.0, 0.9]]),\n            \"C\": np.array([[1.0, 0.0]]),\n        },\n        {\n            \"name\": \"Unobservable but stable mode\",\n            \"A\": np.array([[0.95, 0.0], [0.0, 0.7]]),\n            \"C\": np.array([[1.0, 0.0]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        C = case[\"C\"]\n        \n        # Initialize the posteriori covariance\n        P = P0.copy()\n        \n        diverged = False\n        for _ in range(N):\n            # 1. Time Update (Prediction)\n            P_predict = A @ P @ A.T + Q\n            \n            # 2. Measurement Update (Correction)\n            # Innovation covariance\n            S = C @ P_predict @ C.T + R\n            \n            # Check for non-invertibility, though R > 0 should prevent this\n            if np.linalg.det(S) == 0:\n                diverged = True\n                break\n\n            # Kalman Gain\n            K = P_predict @ C.T @ np.linalg.inv(S)\n\n            # Update posteriori covariance using the standard form (I - KC)P_predict\n            # This is computationally efficient and sufficient here.\n            # Joseph form P = (I - K@C)@P_predict@(I - K@C).T + K@R@K.T could be used\n            # for better numerical stability in general, but is not necessary for this problem.\n            I = np.identity(P.shape[0])\n            P = (I - K @ C) @ P_predict\n            \n            # 3. Check for divergence\n            # Check for non-finite values (inf or nan)\n            if not np.all(np.isfinite(P)):\n                diverged = True\n                break\n                \n            # Check if the largest eigenvalue exceeds the threshold.\n            # Use eigvalsh for real symmetric matrices. P should be symmetric.\n            try:\n                # Add a small stabilization term to ensure P remains symmetric under floating-point arithmetic\n                P = (P + P.T) / 2.0\n                eigenvalues = np.linalg.eigvalsh(P)\n                if np.max(eigenvalues) > tau:\n                    diverged = True\n                    break\n            except np.linalg.LinAlgError:\n                # This can happen if P becomes non-finite or badly scaled\n                diverged = True\n                break\n\n        # Append the boolean result (True for divergence, False for boundedness)\n        results.append(str(diverged).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A stable filter works by incorporating new information to reduce uncertainty, but how does this process unfold within the state space? This exercise dissects the mechanics of a single measurement update, revealing the connection between the measurement matrix $C$ and the reduction of state uncertainty. By computationally analyzing this \"covariance shrinkage,\" you will develop a tangible understanding of observability and how the filter prioritizes correcting the state components that measurements can actually \"see\" .",
            "id": "4228745",
            "problem": "Consider a discrete-time, linear, time-invariant, Gaussian state-space model, which forms the canonical foundation for a Digital Twin of a Cyber-Physical System. The model is defined by the state dynamics and measurement equations\n$$\nx_{k+1} = A x_k + w_k,\\quad y_k = C x_k + v_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state, $y_k \\in \\mathbb{R}^m$ is the measurement, $w_k \\sim \\mathcal{N}(0,Q)$ is the process noise, and $v_k \\sim \\mathcal{N}(0,R)$ is the measurement noise. Assume $A \\in \\mathbb{R}^{n \\times n}$, $C \\in \\mathbb{R}^{m \\times n}$, $Q \\in \\mathbb{R}^{n \\times n}$, $R \\in \\mathbb{R}^{m \\times m}$, and an a priori covariance $P^- \\in \\mathbb{R}^{n \\times n}$ is known. The Kalman filter update must be constructed from first principles of Gaussian conditioning, without using any pre-quoted formulas in the problem statement.\n\nDefine the instantaneous observable subspace at a measurement step as the column space of $C^\\top$ in $\\mathbb{R}^n$. Let $V \\in \\mathbb{R}^{n \\times n}$ be an orthonormal basis obtained by singular value decomposition of $C$, and let $r$ be the rank of $C$. Denote by $V_r \\in \\mathbb{R}^{n \\times r}$ the first $r$ columns of $V$, spanning the observable subspace, and define the orthogonal projections onto the observable and unobservable subspaces by\n$$\n\\Pi_{\\mathrm{obs}} = V_r V_r^\\top,\\quad \\Pi_{\\mathrm{unobs}} = I_n - \\Pi_{\\mathrm{obs}}.\n$$\nFor a one-step measurement update, let $P^+$ denote the posterior error covariance after incorporating a measurement at a given time step. Define the total covariance shrinkage and the shrinkage restricted to the observable and unobservable subspaces by\n$$\n\\Delta_{\\mathrm{tot}} = \\operatorname{tr}(P^-) - \\operatorname{tr}(P^+),\n$$\n$$\n\\Delta_{\\mathrm{obs}} = \\operatorname{tr}\\!\\big(\\Pi_{\\mathrm{obs}} P^- \\Pi_{\\mathrm{obs}}\\big) - \\operatorname{tr}\\!\\big(\\Pi_{\\mathrm{obs}} P^+ \\Pi_{\\mathrm{obs}}\\big),\\quad\n\\Delta_{\\mathrm{unobs}} = \\operatorname{tr}\\!\\big(\\Pi_{\\mathrm{unobs}} P^- \\Pi_{\\mathrm{unobs}}\\big) - \\operatorname{tr}\\!\\big(\\Pi_{\\mathrm{unobs}} P^+ \\Pi_{\\mathrm{unobs}}\\big).\n$$\nDefine the fraction of shrinkage that is concentrated on the observable subspace as\n$$\ns_{\\mathrm{obs}} = \n\\begin{cases}\n\\dfrac{\\Delta_{\\mathrm{obs}}}{\\Delta_{\\mathrm{tot}}}, & \\text{if } \\Delta_{\\mathrm{tot}} > \\varepsilon,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nfor a small numerical tolerance $\\varepsilon > 0$, and analogously define $s_{\\mathrm{unobs}}$ as the fraction for the unobservable subspace.\n\nYour task is to implement from first principles:\n1. The one-step linear Kalman filter covariance update derived from Gaussian conditioning.\n2. The construction of $\\Pi_{\\mathrm{obs}}$ and $\\Pi_{\\mathrm{unobs}}$ using the singular value decomposition of $C$ (or of the Jacobian of the nonlinear measurement function).\n3. The computation of $s_{\\mathrm{obs}}$ for each specified test case below.\n\nAdditionally, to address nonlinear variants consistent with real-world Digital Twins of Cyber-Physical Systems, consider a nonlinear measurement $y_k = h(x_k) + v_k$. Implement the Extended Kalman Filter (EKF) one-step covariance update by linearizing $h$ at a specified linearization point $x^\\ast$ to obtain its Jacobian $H(x^\\ast)$, and then treat $H(x^\\ast)$ as the measurement matrix in the covariance update. Construct $\\Pi_{\\mathrm{obs}}$ using the singular value decomposition of $H(x^\\ast)$.\n\nUse the following test suite, with all matrices and vectors specified numerically. In all cases, compute the one-step posterior covariance $P^+$, construct $\\Pi_{\\mathrm{obs}}$, and return $s_{\\mathrm{obs}}$ as a float.\n\nTest cases:\n- Case 1 (Happy path, block-diagonal, partially observable, linear):\n  - $n=4$, $m=2$.\n  - $C = \\begin{bmatrix}1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0\\end{bmatrix}$.\n  - $P^- = \\operatorname{diag}(1,1,1,1)$.\n  - $R = 0.05 I_2$.\n- Case 2 (Correlated prior across subspaces, linear):\n  - $n=4$, $m=2$.\n  - $C = \\begin{bmatrix}1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0\\end{bmatrix}$.\n  - Construct $S \\in \\mathbb{R}^{4 \\times 4}$ as\n    $S = \\begin{bmatrix}\n    1 & 0.2 & 0.1 & 0.1\\\\\n    0 & 1.0 & 0.2 & 0.1\\\\\n    0 & 0 & 1.0 & 0.3\\\\\n    0 & 0 & 0 & 1.0\n    \\end{bmatrix}$,\n    and set $P^- = S S^\\top$.\n  - $R = 0.10 I_2$.\n- Case 3 (Extremely noisy measurements, linear, boundary):\n  - $n=4$, $m=2$.\n  - $C = \\begin{bmatrix}1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0\\end{bmatrix}$.\n  - $P^- = \\operatorname{diag}(1,1,1,1)$.\n  - $R = 1000 I_2$.\n- Case 4 (Nonlinear measurement with Extended Kalman Filter, rank-deficient Jacobian):\n  - $n=3$, $m=2$.\n  - $h(x) = \\begin{bmatrix}\\sin(x_1)\\\\ \\sin(x_1)\\end{bmatrix}$.\n  - Linearize at $x^\\ast = \\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix}$ to obtain $H(x^\\ast)$.\n  - $P^- = \\operatorname{diag}(1,1,1)$.\n  - $R = 0.05 I_2$.\n\nAngle units are not applicable. No physical units are required. All computed outputs are dimensionless floats.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each entry is $s_{\\mathrm{obs}}$ for the corresponding test case, rounded to six decimal places. Use a numerical tolerance $\\varepsilon = 10^{-12}$ when deciding the denominator condition in the definition of $s_{\\mathrm{obs}}$.",
            "solution": "The problem requires the derivation of the one-step Kalman filter covariance update from the first principles of Gaussian conditioning, and its application to analyze the distribution of covariance shrinkage between observable and unobservable subspaces of a state-space model. The analysis is to be performed for both linear models and a nonlinear model via the Extended Kalman Filter (EKF) framework.\n\nFirst, we derive the posterior covariance update. We consider a state $x \\in \\mathbb{R}^n$ and a measurement $y \\in \\mathbb{R}^m$. The prior belief about the state is modeled by a Gaussian distribution, $x \\sim \\mathcal{N}(\\hat{x}^-, P^-)$, where $\\hat{x}^-$ is the prior mean and $P^-$ is the prior error covariance. The measurement is related to the state through the linear model $y = C x + v$, where the measurement noise $v \\sim \\mathcal{N}(0, R)$ is a zero-mean Gaussian random variable with covariance $R$, assumed to be independent of the state $x$.\n\nThe core of the Bayesian update is to find the conditional distribution of $x$ given an observation of $y$. To do this, we form an augmented random vector $z = [x^\\top, y^\\top]^\\top$. Since $x$ and $v$ are jointly Gaussian (due to their independence), and $y$ is a linear transformation of them, the augmented vector $z$ is also Gaussian. We proceed to find its mean and covariance.\n\nThe mean of the augmented vector is:\n$$\n\\mathbb{E}[z] = \n\\begin{bmatrix}\n\\mathbb{E}[x] \\\\\n\\mathbb{E}[y]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\hat{x}^- \\\\\n\\mathbb{E}[C x + v]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\hat{x}^- \\\\\nC \\mathbb{E}[x] + \\mathbb{E}[v]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\hat{x}^- \\\\\nC \\hat{x}^-\n\\end{bmatrix}\n$$\n\nThe covariance of the augmented vector is:\n$$\n\\operatorname{Cov}(z) = \\Sigma_z =\n\\begin{bmatrix}\n\\operatorname{Cov}(x, x) & \\operatorname{Cov}(x, y) \\\\\n\\operatorname{Cov}(y, x) & \\operatorname{Cov}(y, y)\n\\end{bmatrix}\n$$\nThe blocks are computed as follows:\n- $\\operatorname{Cov}(x, x) = P^-$ (the prior covariance).\n- $\\operatorname{Cov}(x, y) = \\operatorname{Cov}(x, C x + v) = \\operatorname{Cov}(x, C x) + \\operatorname{Cov}(x, v)$. Since $x$ and $v$ are independent, $\\operatorname{Cov}(x, v) = 0$. Thus, $\\operatorname{Cov}(x, y) = \\operatorname{Cov}(x, x) C^\\top = P^- C^\\top$.\n- $\\operatorname{Cov}(y, x) = (\\operatorname{Cov}(x, y))^\\top = C P^-$.\n- $\\operatorname{Cov}(y, y) = \\operatorname{Cov}(C x + v, C x + v) = \\operatorname{Cov}(C x, C x) + \\operatorname{Cov}(v, v) = C \\operatorname{Cov}(x, x) C^\\top + R = C P^- C^\\top + R$. This term is known as the innovation covariance, often denoted $S_k$.\n\nSo, the joint distribution is $z \\sim \\mathcal{N}\\left(\n\\begin{bmatrix} \\hat{x}^- \\\\ C \\hat{x}^- \\end{bmatrix}, \n\\begin{bmatrix} P^- & P^- C^\\top \\\\ C P^- & C P^- C^\\top + R \\end{bmatrix}\n\\right)$.\n\nFor a general partitioned Gaussian vector $z = [z_a^\\top, z_b^\\top]^\\top$ with covariance $\\Sigma = \\begin{bmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{bmatrix}$, the conditional covariance of $z_a$ given $z_b$ is given by the Schur complement of $\\Sigma_{bb}$ in $\\Sigma$:\n$$\n\\operatorname{Cov}(z_a | z_b) = \\Sigma_{aa} - \\Sigma_{ab} \\Sigma_{bb}^{-1} \\Sigma_{ba}\n$$\nApplying this formula to our problem, we identify $z_a \\equiv x$ and $z_b \\equiv y$. The posterior covariance $P^+$, which is the covariance of the state $x$ after conditioning on the measurement $y$, is therefore:\n$$\nP^+ = P^- - (P^- C^\\top) (C P^- C^\\top + R)^{-1} (C P^-)^\\top\n$$\nThis is a standard form of the covariance update equation. Defining the Kalman gain as $K = P^- C^\\top (C P^- C^\\top + R)^{-1}$, the update can be written more compactly as $P^+ = (I - K C) P^-$. This is the equation we will implement.\n\nThe problem further requires an analysis of how this update affects the uncertainty in different state subspaces. The instantaneous observable subspace at a measurement step is defined as the column space of $C^\\top$, which is equivalent to the row space of $C$. This is the subspace of the state that directly influences the measurement $y$. The orthogonal complement is the unobservable subspace.\nTo construct orthonormal bases for these subspaces, we use the Singular Value Decomposition (SVD) of the measurement matrix $C = U \\Sigma_s V^\\top$. The columns of $V \\in \\mathbb{R}^{n \\times n}$ form an orthonormal basis for the state space $\\mathbb{R}^n$. If the rank of $C$ is $r$, the first $r$ columns of $V$, denoted $V_r \\in \\mathbb{R}^{n \\times r}$, span the row space of $C$ (the observable subspace). The remaining $n-r$ columns span the null space of $C$ (the unobservable subspace). The orthogonal projection matrices onto these subspaces are $\\Pi_{\\mathrm{obs}} = V_r V_r^\\top$ and $\\Pi_{\\mathrm{unobs}} = I_n - \\Pi_{\\mathrm{obs}}$.\n\nThe total reduction in uncertainty, or covariance shrinkage, is the decrease in the trace of the covariance matrix: $\\Delta_{\\mathrm{tot}} = \\operatorname{tr}(P^-) - \\operatorname{tr}(P^+)$. We wish to partition this total shrinkage into components that lie in the observable and unobservable subspaces. The projected covariances are $\\Pi_{\\mathrm{obs}} P \\Pi_{\\mathrm{obs}}$ and $\\Pi_{\\mathrm{unobs}} P \\Pi_{\\mathrm{unobs}}$. The shrinkage in each subspace is:\n$$\n\\Delta_{\\mathrm{obs}} = \\operatorname{tr}(\\Pi_{\\mathrm{obs}} P^- \\Pi_{\\mathrm{obs}}) - \\operatorname{tr}(\\Pi_{\\mathrm{obs}} P^+ \\Pi_{\\mathrm{obs}})\n$$\n$$\n\\Delta_{\\mathrm{unobs}} = \\operatorname{tr}(\\Pi_{\\mathrm{unobs}} P^- \\Pi_{\\mathrm{unobs}}) - \\operatorname{tr}(\\Pi_{\\mathrm{unobs}} P^+ \\Pi_{\\mathrm{unobs}})\n$$\nUsing the cyclic property of the trace ($\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$) and the idempotent property of projectors ($\\Pi^2 = \\Pi$), we can simplify the trace terms: $\\operatorname{tr}(\\Pi P \\Pi) = \\operatorname{tr}(P \\Pi \\Pi) = \\operatorname{tr}(P \\Pi)$. This yields a computationally efficient form:\n$$\n\\Delta_{\\mathrm{obs}} = \\operatorname{tr}((P^- - P^+) \\Pi_{\\mathrm{obs}})\n$$\nThe fraction of shrinkage on the observable subspace, $s_{\\mathrm{obs}}$, is then the ratio $\\Delta_{\\mathrm{obs}} / \\Delta_{\\mathrm{tot}}$, with a check to prevent division by a very small number.\n\nFor the nonlinear measurement model $y_k = h(x_k) + v_k$, the Extended Kalman Filter (EKF) approximates the measurement function with a first-order Taylor series expansion around the current state estimate (or a given linearization point $x^\\ast$). This yields a linear approximation $y_k \\approx h(x^\\ast) + H(x^\\ast)(x_k - x^\\ast) + v_k$, where $H(x^\\ast) = \\left. \\frac{\\partial h}{\\partial x} \\right|_{x=x^\\ast}$ is the Jacobian matrix. The linear covariance update formulas are then applied with $C$ replaced by this Jacobian $H$. The observable subspace is correspondingly constructed from the SVD of $H$.\n\nThe following implementation calculates the posterior covariance $P^+$ and the shrinkage fraction $s_{\\mathrm{obs}}$ for the provided test cases based on these principles.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_s_obs(P_minus, C, R, epsilon=1e-12):\n    \"\"\"\n    Calculates the one-step posterior covariance and the observable shrinkage fraction.\n\n    Args:\n        P_minus (np.ndarray): Prior covariance matrix (n x n).\n        C (np.ndarray): Measurement matrix (m x n).\n        R (np.ndarray): Measurement noise covariance matrix (m x m).\n        epsilon (float): Numerical tolerance for division.\n\n    Returns:\n        float: The fraction of covariance shrinkage on the observable subspace, s_obs.\n    \"\"\"\n    n, _ = P_minus.shape\n\n    # 1. Kalman Filter Covariance Update\n    # Innovation covariance: S = C * P_minus * C.T + R\n    S = C @ P_minus @ C.T + R\n    \n    # Kalman gain: K = P_minus * C.T * inv(S)\n    # Using np.linalg.solve for better numerical stability than inv()\n    # We solve S.T @ K.T = C @ P_minus.T for K.T, then transpose. Since S is symmetric, S.T=S.\n    # K.T = solve(S, C @ P_minus.T)\n    # K = solve(S, C @ P_minus.T).T is equivalent to K = P_minus @ C.T @ inv(S)\n    K = P_minus @ C.T @ np.linalg.inv(S)\n\n    # Posterior covariance: P_plus = (I - K*C) * P_minus\n    I = np.eye(n)\n    P_plus = (I - K @ C) @ P_minus\n\n    # 2. Construct Projection Matrix\n    # SVD of C to find the basis for the observable subspace\n    # The columns of vh.T are the right singular vectors of C\n    _, _, vh = np.linalg.svd(C, full_matrices=True)\n    V = vh.T\n    \n    # Rank of C determines the dimension of the observable subspace\n    r = np.linalg.matrix_rank(C)\n    \n    # V_r contains the first r columns of V, spanning the observable subspace\n    V_r = V[:, :r]\n    \n    # Projection matrix onto the observable subspace: Pi_obs = V_r * V_r.T\n    Pi_obs = V_r @ V_r.T\n\n    # 3. Calculate Shrinkage Values\n    # Total covariance shrinkage\n    Delta_P = P_minus - P_plus\n    Delta_tot = np.trace(Delta_P)\n\n    # Use cyclic property of trace: tr(Pi*P*Pi) = tr(P*Pi)\n    Delta_obs = np.trace(Delta_P @ Pi_obs)\n    \n    # 4. Calculate Shrinkage Fraction\n    if Delta_tot > epsilon:\n        s_obs = Delta_obs / Delta_tot\n    else:\n        s_obs = 0.0\n\n    return s_obs\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the results.\n    \"\"\"\n    results = []\n\n    # Case 1: Happy path, block-diagonal, partially observable, linear\n    n1, m1 = 4, 2\n    C1 = np.array([[1.0, 0, 0, 0], [0, 1.0, 0, 0]])\n    P_minus1 = np.diag([1.0, 1.0, 1.0, 1.0])\n    R1 = 0.05 * np.eye(m1)\n    results.append(calculate_s_obs(P_minus1, C1, R1))\n\n    # Case 2: Correlated prior across subspaces, linear\n    n2, m2 = 4, 2\n    C2 = np.array([[1.0, 0, 0, 0], [0, 1.0, 0, 0]])\n    S_mat = np.array([\n        [1.0, 0.2, 0.1, 0.1],\n        [0.0, 1.0, 0.2, 0.1],\n        [0.0, 0.0, 1.0, 0.3],\n        [0.0, 0.0, 0.0, 1.0]\n    ])\n    P_minus2 = S_mat @ S_mat.T\n    R2 = 0.10 * np.eye(m2)\n    results.append(calculate_s_obs(P_minus2, C2, R2))\n\n    # Case 3: Extremely noisy measurements, linear, boundary\n    n3, m3 = 4, 2\n    C3 = np.array([[1.0, 0, 0, 0], [0, 1.0, 0, 0]])\n    P_minus3 = np.diag([1.0, 1.0, 1.0, 1.0])\n    R3 = 1000.0 * np.eye(m3)\n    results.append(calculate_s_obs(P_minus3, C3, R3))\n\n    # Case 4: Nonlinear measurement with EKF, rank-deficient Jacobian\n    n4, m4 = 3, 2\n    # Nonlinear function h(x) = [sin(x1), sin(x1)]\n    # Linearization point x_star = [0, 0, 0]\n    # Jacobian H = [[cos(x1), 0, 0], [cos(x1), 0, 0]]\n    # At x_star, cos(0) = 1, so H = [[1,0,0], [1,0,0]]\n    H4 = np.array([[1.0, 0, 0], [1.0, 0, 0]])\n    P_minus4 = np.diag([1.0, 1.0, 1.0])\n    R4 = 0.05 * np.eye(m4)\n    results.append(calculate_s_obs(P_minus4, H4, R4))\n\n    # Format output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Beyond state estimation, the Kalman filter is a powerful tool for system monitoring. This exercise focuses on using the filter's innovation—the residual between predicted and actual measurements—to detect anomalies. You will justify and implement a statistically grounded anomaly detector based on the Chi-Squared distribution, learning how to transform the filter's outputs into a robust health monitoring system for a digital twin or cyber-physical system .",
            "id": "4228746",
            "problem": "You are tasked with designing, justifying, and implementing an innovation-based anomaly detector for a Digital Twin in a Cyber-Physical System (CPS). The estimator is a member of the Kalman filtering family, possibly a linear Kalman Filter (KF), Extended Kalman Filter (EKF), or Unscented Kalman Filter (UKF). Under correct modeling assumptions, the innovation (also called residual) at discrete time index $k$, denoted by $e_k$, is a random vector with covariance matrix $S_k$. Assume the measurement noise is zero-mean Gaussian and that the estimator computes $e_k$ and $S_k$ consistently from its prediction and update steps.\n\nYour tasks are:\n- Derive, from first principles, why the scalar test statistic $e_k^\\top S_k^{-1} e_k$ has a Chi-Squared distribution with degrees of freedom equal to the measurement dimension under the Gaussianity assumption of the innovation.\n- Implement a numerically stable detector that, for each provided test case, computes the statistic $e_k^\\top S_k^{-1} e_k$ without explicitly inverting $S_k$, compares the value to the Chi-Squared critical value for a specified significance level $\\alpha$ and measurement dimension $m$, and outputs a boolean indicating anomaly if and only if the statistic is strictly greater than the critical value.\n\nUse a numerically stable approach by solving triangular systems via a Cholesky factorization. If the Cholesky factorization fails because $S_k$ is nearly singular, add a small diagonal jitter $\\epsilon I$ with $\\epsilon = 10^{-9}$ and retry, where $I$ is the identity matrix of appropriate dimension.\n\nThere are no physical units involved in this problem; treat all quantities as dimensionless. Angles are not involved. Express all final outputs as booleans.\n\nThe test suite consists of five independent cases, each specified by the measurement dimension $m$, a symmetric positive definite covariance $S_k$, an innovation vector $e_k$, and significance level $\\alpha$:\n\n- Case $1$ (general well-conditioned $2$-dimensional case):\n  - $m = 2$\n  - $S_k = \\begin{bmatrix} 1.0 & 0.1 \\\\ 0.1 & 1.5 \\end{bmatrix}$\n  - $e_k = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n\n- Case $2$ (boundary case exactly at the critical value for $m=2$ and $\\alpha=0.05$ with identity covariance):\n  - $m = 2$\n  - $S_k = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$\n  - $e_k = \\begin{bmatrix} \\sqrt{5.991464547107982} \\\\ 0.0 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n\n- Case $3$ (clear anomaly in $3$ dimensions):\n  - $m = 3$\n  - $S_k = \\operatorname{diag}(0.5, 0.5, 2.0)$\n  - $e_k = \\begin{bmatrix} 3.0 \\\\ -3.0 \\\\ 0.0 \\end{bmatrix}$\n  - $\\alpha = 0.01$\n\n- Case $4$ (scalar measurement with small variance):\n  - $m = 1$\n  - $S_k = \\begin{bmatrix} 0.01 \\end{bmatrix}$\n  - $e_k = \\begin{bmatrix} 0.2 \\end{bmatrix}$\n  - $\\alpha = 0.10$\n\n- Case $5$ (nearly singular covariance in $2$ dimensions):\n  - $m = 2$\n  - $S_k = \\operatorname{diag}(10^{-6}, 10.0)$\n  - $e_k = \\begin{bmatrix} 10^{-3} \\\\ 0.0 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n\nFor each case, compute the statistic $t_k = e_k^\\top S_k^{-1} e_k$ via Cholesky-based whitening and compare it to the Chi-Squared critical value $c_{\\alpha} = F_{\\chi^2(m)}^{-1}(1 - \\alpha)$, where $F_{\\chi^2(m)}^{-1}$ denotes the inverse cumulative distribution function (quantile function) of the Chi-Squared distribution with $m$ degrees of freedom. The detector should flag an anomaly if and only if $t_k > c_{\\alpha}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4,result_5]$), where each $result_i$ is a boolean corresponding to each case in the order listed above.",
            "solution": "### Derivation of the Test Statistic's Distribution\n\nThe objective is to demonstrate that the test statistic $t_k = e_k^\\top S_k^{-1} e_k$ follows a Chi-Squared ($\\chi^2$) distribution with $m$ degrees of freedom.\n\nThe derivation begins from the premise that under normal operating conditions (the null hypothesis), the innovation vector $e_k$ is a random variable that follows a multivariate Gaussian distribution with a mean vector of $0$ and a covariance matrix $S_k$. The dimension of $e_k$ is $m$. This is formally stated as:\n$$\ne_k \\sim \\mathcal{N}(0, S_k)\n$$\nwhere $e_k \\in \\mathbb{R}^m$ and $S_k \\in \\mathbb{R}^{m \\times m}$.\n\nSince $S_k$ is a covariance matrix, it is symmetric and positive semi-definite. For a properly operating filter with an observable system, $S_k$ is strictly positive definite. A positive definite matrix $S_k$ can be uniquely decomposed via Cholesky factorization into the product of a lower-triangular matrix $L_k$ and its transpose $L_k^\\top$:\n$$\nS_k = L_k L_k^\\top\n$$\nThe inverse of $S_k$ can then be expressed as:\n$$\nS_k^{-1} = (L_k L_k^\\top)^{-1} = (L_k^\\top)^{-1} L_k^{-1}\n$$\n\nThe next step is to \"whiten\" the innovation vector $e_k$. Whitening is a linear transformation that converts a random vector with a known covariance matrix into a new random vector with an identity covariance matrix (and zero mean). Let us define the whitened innovation vector, $z_k$, as:\n$$\nz_k = L_k^{-1} e_k\n$$\nWe now analyze the statistical properties of $z_k$. The expected value of $z_k$ is:\n$$\n\\mathbb{E}[z_k] = \\mathbb{E}[L_k^{-1} e_k] = L_k^{-1} \\mathbb{E}[e_k] = L_k^{-1} \\cdot 0 = 0\n$$\nThe covariance matrix of $z_k$ is calculated as follows:\n$$\n\\operatorname{Cov}(z_k) = \\mathbb{E}[ (z_k - \\mathbb{E}[z_k]) (z_k - \\mathbb{E}[z_k])^\\top ] = \\mathbb{E}[z_k z_k^\\top]\n$$\nSubstituting the definition of $z_k$:\n$$\n\\operatorname{Cov}(z_k) = \\mathbb{E}[ (L_k^{-1} e_k) (L_k^{-1} e_k)^\\top ] = \\mathbb{E}[ L_k^{-1} e_k e_k^\\top (L_k^{-1})^\\top ]\n$$\nUsing the linearity of the expectation operator:\n$$\n\\operatorname{Cov}(z_k) = L_k^{-1} \\mathbb{E}[e_k e_k^\\top] (L_k^\\top)^{-1}\n$$\nBy definition, $\\mathbb{E}[e_k e_k^\\top]$ is the covariance matrix of $e_k$, which is $S_k$.\n$$\n\\operatorname{Cov}(z_k) = L_k^{-1} S_k (L_k^\\top)^{-1}\n$$\nNow, substituting the Cholesky factorization $S_k = L_k L_k^\\top$:\n$$\n\\operatorname{Cov}(z_k) = L_k^{-1} (L_k L_k^\\top) (L_k^\\top)^{-1} = (L_k^{-1} L_k) (L_k^\\top (L_k^\\top)^{-1}) = I_m \\cdot I_m = I_m\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix.\n\nSince $e_k$ is a Gaussian random vector and $z_k$ is a linear transformation of $e_k$, $z_k$ is also a Gaussian random vector. We have shown that its mean is $0$ and its covariance is $I_m$. Thus, $z_k$ follows a standard multivariate normal distribution:\n$$\nz_k \\sim \\mathcal{N}(0, I_m)\n$$\nThis implies that the components of $z_k$, denoted $z_{k,i}$ for $i=1, \\dots, m$, are independent and identically distributed (i.i.d.) standard normal random variables, i.e., $z_{k,i} \\sim \\mathcal{N}(0, 1)$.\n\nNow, we can re-express the test statistic $t_k$ in terms of the whitened vector $z_k$:\n$$\nt_k = e_k^\\top S_k^{-1} e_k = e_k^\\top (L_k^\\top)^{-1} L_k^{-1} e_k\n$$\nUsing the property $(AB)^\\top = B^\\top A^\\top$, we can write this as:\n$$\nt_k = (L_k^{-1} e_k)^\\top (L_k^{-1} e_k) = z_k^\\top z_k\n$$\nThe quantity $z_k^\\top z_k$ is the sum of the squares of the components of $z_k$:\n$$\nt_k = \\sum_{i=1}^{m} z_{k,i}^2\n$$\nBy the definition of the Chi-Squared distribution, the sum of the squares of $m$ independent standard normal random variables follows a Chi-Squared distribution with $m$ degrees of freedom.\n\nTherefore, we have established that the test statistic $t_k = e_k^\\top S_k^{-1} e_k$ is distributed as $\\chi^2(m)$. This completes the derivation.\n\n### Numerically Stable Implementation\n\nThe implementation of the anomaly detector is based on the insights from the derivation. The goal is to compute $t_k = e_k^\\top S_k^{-1} e_k$ and compare it to a critical value without performing an explicit matrix inversion of $S_k$, which is numerically unstable and computationally inefficient.\n\nThe algorithmic steps are as follows:\n\n1.  **Cholesky Factorization**: Given the innovation covariance matrix $S_k$, compute its lower-triangular Cholesky factor $L_k$ such that $S_k = L_k L_k^\\top$. A robust implementation must handle cases where $S_k$ is not numerically positive definite (e.g., due to floating-point errors or model ill-conditioning). As specified, if the initial factorization fails, a small regularization term, or \"jitter,\" $\\epsilon I_m$ is added to $S_k$, where $\\epsilon = 10^{-9}$. The factorization is then re-attempted on the regularized matrix $S_k' = S_k + \\epsilon I_m$.\n\n2.  **Whitening via Triangular Solve**: Instead of computing $z_k = L_k^{-1} e_k$ by inverting $L_k$, we solve the equivalent linear system $L_k z_k = e_k$ for the vector $z_k$. Since $L_k$ is lower-triangular, this system can be solved efficiently and stably using forward substitution.\n\n3.  **Statistic Calculation**: Once the whitened vector $z_k$ is obtained, the test statistic $t_k$ is computed simply as the squared Euclidean norm of $z_k$: $t_k = z_k^\\top z_k$. This is computationally trivial and numerically benign.\n\n4.  **Hypothesis Test**:\n    - The critical value, $c_{\\alpha}$, is determined from the inverse cumulative distribution function (CDF) of the Chi-Squared distribution with $m$ degrees of freedom. Specifically, for a significance level $\\alpha$, the critical value is the point on the $\\chi^2(m)$ distribution such that the area under the curve to its right is $\\alpha$. This corresponds to a cumulative probability of $1 - \\alpha$. So, $c_{\\alpha} = F_{\\chi^2(m)}^{-1}(1-\\alpha)$, where $F^{-1}$ is the quantile function (or percent point function).\n    - An anomaly is declared if and only if the computed statistic $t_k$ is strictly greater than the critical value $c_{\\alpha}$. The output for each case is a boolean value: `True` if $t_k > c_{\\alpha}$, and `False` otherwise.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, LinAlgError\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the anomaly detection problem for all specified test cases.\n    \"\"\"\n\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        {\n            \"m\": 2,\n            \"S_k\": np.array([[1.0, 0.1], \n                             [0.1, 1.5]]),\n            \"e_k\": np.array([0.2, -0.1]),\n            \"alpha\": 0.05,\n        },\n        {\n            \"m\": 2,\n            \"S_k\": np.array([[1.0, 0.0], \n                             [0.0, 1.0]]),\n            \"e_k\": np.array([np.sqrt(5.991464547107982), 0.0]),\n            \"alpha\": 0.05,\n        },\n        {\n            \"m\": 3,\n            \"S_k\": np.diag([0.5, 0.5, 2.0]),\n            \"e_k\": np.array([3.0, -3.0, 0.0]),\n            \"alpha\": 0.01,\n        },\n        {\n            \"m\": 1,\n            \"S_k\": np.array([[0.01]]),\n            \"e_k\": np.array([0.2]),\n            \"alpha\": 0.10,\n        },\n        {\n            \"m\": 2,\n            \"S_k\": np.diag([1e-6, 10.0]),\n            \"e_k\": np.array([1e-3, 0.0]),\n            \"alpha\": 0.05,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        is_anomaly = detect_anomaly(\n            case[\"m\"], case[\"S_k\"], case[\"e_k\"], case[\"alpha\"]\n        )\n        results.append(str(is_anomaly).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef detect_anomaly(m, S_k, e_k, alpha):\n    \"\"\"\n    Performs innovation-based anomaly detection for a single time step.\n\n    Args:\n        m (int): The dimension of the measurement space.\n        S_k (np.ndarray): The innovation covariance matrix (m x m).\n        e_k (np.ndarray): The innovation vector (m,).\n        alpha (float): The significance level for the Chi-Squared test.\n\n    Returns:\n        bool: True if an anomaly is detected, False otherwise.\n    \"\"\"\n    epsilon = 1e-9  # Jitter for near-singular matrices\n\n    try:\n        # Perform Cholesky factorization: S_k = L * L.T\n        # 'lower=True' ensures L is lower-triangular.\n        L = cholesky(S_k, lower=True)\n    except LinAlgError:\n        # If S_k is not positive definite, add jitter and retry.\n        S_k_jittered = S_k + epsilon * np.identity(m)\n        L = cholesky(S_k_jittered, lower=True)\n\n    # Whiten the innovation vector by solving the triangular system L * z_k = e_k.\n    # This is numerically superior to computing z_k = inv(L) * e_k.\n    z_k = solve_triangular(L, e_k, lower=True)\n\n    # Compute the test statistic t_k = z_k.T * z_k\n    t_k = np.dot(z_k, z_k)\n\n    # Compute the critical value from the Chi-Squared distribution's\n    # percent point function (inverse CDF).\n    critical_value = chi2.ppf(1 - alpha, df=m)\n\n    # An anomaly is detected if the statistic is strictly greater than the threshold.\n    return t_k > critical_value\n\nsolve()\n```"
        }
    ]
}