{
    "hands_on_practices": [
        {
            "introduction": "Before deploying a Kalman filter, it is essential to ensure it will be stable. The filter's error covariance is guaranteed to remain bounded only if the underlying system is \"detectable,\" meaning any unstable dynamic modes are observable through the measurements. This exercise  provides a powerful hands-on demonstration of this principle, showing through direct simulation how the error covariance diverges when an unstable mode is hidden from the sensors, a critical failure condition to understand and avoid.",
            "id": "4228697",
            "problem": "A discrete-time linear time-invariant cyber-physical estimation problem concerns a system with state vector $x_k \\in \\mathbb{R}^n$ evolving as $x_{k+1} = A x_k + w_k$ and output $y_k = C x_k + v_k$, where $A \\in \\mathbb{R}^{n \\times n}$, $C \\in \\mathbb{R}^{p \\times n}$, $w_k$ and $v_k$ are zero-mean Gaussian white noises with covariances $Q \\succ 0$ and $R \\succ 0$, respectively. Consider the state estimation in a digital twin for a cyber-physical system using a Kalman Filter (KF). A pair $(A,C)$ is called detectable if every unobservable mode is stable, equivalently, every eigenvalue $\\lambda$ of $A$ with $|\\lambda| \\ge 1$ is observable by $C$. When $(A,C)$ is not detectable because of an unstable unobservable mode, the covariance recursion associated with the KF can diverge.\n\nYour task is to rigorously demonstrate, by direct simulation and verification, that the covariance Riccati recursion diverges when an unstable mode is unobservable, and remains bounded when all unstable modes are observable, or when unobservable modes are stable. You must implement the discrete-time Kalman covariance recursion for given parameter sets and automatically assess divergence. No formulas are provided here; your program must rely on first principles and well-tested facts to construct the correct recursion.\n\nImplement a program that:\n- Initializes with a given covariance $P_0 \\succeq 0$ and iterates the discrete-time Kalman covariance recursion for $N$ steps.\n- Declares divergence if at any iteration $k \\in \\{1,\\dots,N\\}$ the largest eigenvalue of the covariance $P_k$ exceeds a prescribed threshold $\\tau$, or if $P_k$ loses finiteness (e.g., contains $\\infty$ or $\\text{NaN}$).\n- Returns a boolean for each test case indicating divergence (true) or boundedness (false).\n\nUse the following test suite with $n=2$ states and scalar output $p=1$, and with common noise covariances and initial covariance:\n- $Q = \\operatorname{diag}(q_1, q_2)$ with $q_1 = 0.01$, $q_2 = 0.01$.\n- $R = [r]$ with $r = 0.1$.\n- $P_0 = I_2$.\n- $N = 200$ iterations.\n- Divergence threshold $\\tau = 10^{12}$.\n\nTest cases:\n1. Not detectable with an unstable unobservable mode:\n   - $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix}$.\n   - The unstable mode at eigenvalue $1.2$ is unobservable because it lies entirely in the subspace orthogonal to the measurement.\n2. Detectable with an unstable observable mode:\n   - $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\n   - The unstable mode at eigenvalue $1.2$ is observable.\n3. Unobservable but stable mode:\n   - $A = \\begin{bmatrix} 0.95 & 0.0 \\\\ 0.0 & 0.7 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\n   - All modes are stable; the second mode is unobservable but does not violate detectability.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each $result_i$ is a boolean with lowercase letters indicating whether the corresponding test case diverges (true) or remains bounded (false). No physical units or angles are involved in this task, and answers should be booleans as specified above. Your implementation must be self-contained, require no input, and adhere to the specified runtime environment.",
            "solution": "The problem requires a direct simulation to demonstrate a fundamental principle of Kalman filtering: the boundedness of the filter's error covariance matrix is contingent upon the detectability of the system. We will first establish the theoretical basis and then describe the algorithmic implementation for verification.\n\nA discrete-time linear time-invariant system is defined by the state and measurement equations:\n$$\nx_{k+1} = A x_k + w_k\n$$\n$$\ny_k = C x_k + v_k\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state vector, $y_k \\in \\mathbb{R}^p$ is the measurement vector, $A \\in \\mathbb{R}^{n \\times n}$ is the state transition matrix, and $C \\in \\mathbb{R}^{p \\times n}$ is the measurement matrix. The process noise $w_k$ and measurement noise $v_k$ are zero-mean, white Gaussian processes with respective covariance matrices $Q \\succ 0$ and $R \\succ 0$.\n\nThe Kalman filter provides an optimal estimate of the state $x_k$. The filter's performance is characterized by the error covariance matrix $P_k = E[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^T]$, where $\\hat{x}_{k|k}$ is the a posteriori state estimate at time $k$. The evolution of $P_k$ is governed by the discrete-time matrix Riccati recursion. Starting with an initial covariance $P_0$, the recursion for $k=1, 2, \\dots, N$ is a two-step process:\n\n1.  **Time Update (Prediction):** The error covariance is propagated forward in time. The a priori covariance $P_{k|k-1}$ is computed from the a posteriori covariance of the previous step, $P_{k-1|k-1}$.\n    $$\n    P_{k|k-1} = A P_{k-1|k-1} A^T + Q\n    $$\n\n2.  **Measurement Update (Correction):** The a priori covariance is corrected using the information from the measurement $y_k$. The resulting a posteriori covariance $P_{k|k}$ is given by:\n    $$\n    P_{k|k} = P_{k|k-1} - P_{k|k-1} C^T (C P_{k|k-1} C^T + R)^{-1} C P_{k|k-1}\n    $$\nThis form is equivalent to the Joseph form $P_{k|k} = (I - K_k C)P_{k|k-1}$ where $K_k$ is the Kalman gain, and it preserves the symmetry and positive semi-definiteness of the covariance matrix under exact arithmetic.\n\nA cornerstone theorem in estimation theory states that the error covariance recursion $P_k$ converges to a unique, finite, positive semi-definite steady-state solution $P_\\infty$ if and only if the pair $(A, C)$ is detectable and the pair $(A, Q^{1/2})$ is stabilizable. A system $(A, C)$ is detectable if every unobservable mode of $A$ is stable. A mode, corresponding to an eigenvalue $\\lambda$ of $A$, is stable if $|\\lambda| < 1$ and unstable if $|\\lambda| \\ge 1$. An eigenvector $v$ associated with $\\lambda$ corresponds to an unobservable mode if it lies in the nullspace of the measurement matrix, i.e., $C v = 0$.\n\nIn all test cases provided, the process noise covariance $Q$ is a diagonal matrix with strictly positive entries, which means $Q \\succ 0$. This ensures that the pair $(A, Q^{1/2})$ is controllable (a stronger condition than stabilizable). Therefore, the convergence and boundedness of the Riccati recursion depend solely on the detectability of the pair $(A, C)$.\n\nThe task is to simulate this recursion for three specific cases and verify the theoretical predictions. Divergence is defined as the largest eigenvalue of $P_k$ exceeding a threshold $\\tau = 10^{12}$ or $P_k$ containing non-finite values.\n\n**Analysis of Test Cases:**\nThe common parameters are $n=2$, $p=1$, $Q = \\operatorname{diag}(0.01, 0.01)$, $R = [0.1]$, $P_0 = I_2$, $N=200$, and $\\tau = 10^{12}$.\n\n**Case 1:** $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix}$.\nThe eigenvalues of $A$ are $\\lambda_1 = 1.2$ and $\\lambda_2 = 0.9$. The mode associated with $\\lambda_1 = 1.2$ is unstable. The corresponding eigenvector is $v_1 = [1.0, 0.0]^T$. We test for observability:\n$$\nC v_1 = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix} \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix} = 0.0\n$$\nSince $C v_1 = 0$, the unstable mode is unobservable. The system is not detectable. According to theory, the error covariance $P_k$ is expected to diverge. The variance of the unobserved and unstable state will grow without bound. The simulation should result in `true`.\n\n**Case 2:** $A = \\begin{bmatrix} 1.2 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\nThe unstable mode is again at $\\lambda_1 = 1.2$ with eigenvector $v_1 = [1.0, 0.0]^T$. We test for observability:\n$$\nC v_1 = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix} = 1.0 \\neq 0.0\n$$\nThe unstable mode is observable. The other mode $\\lambda_2=0.9$ is stable. As all unstable modes are observable, the system is detectable. The theory predicts that $P_k$ will converge to a finite steady-state value. The simulation should result in `false`.\n\n**Case 3:** $A = \\begin{bmatrix} 0.95 & 0.0 \\\\ 0.0 & 0.7 \\end{bmatrix}$, $C = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\nThe eigenvalues of $A$ are $\\lambda_1 = 0.95$ and $\\lambda_2 = 0.7$. Both modes are stable since $|\\lambda_i| < 1$. The eigenvector for $\\lambda_2 = 0.7$ is $v_2 = [0.0, 1.0]^T$. We test its observability:\n$$\nC v_2 = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix} = 0.0\n$$\nThe mode associated with $\\lambda_2=0.7$ is unobservable. However, since this unobservable mode is stable, the condition for detectability is met. The system is detectable. Consequently, the error covariance $P_k$ will remain bounded and converge. The simulation should result in `false`.\n\nThe program will implement the covariance recursion and test for the divergence condition at each of the $N=200$ iterations for all three cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per the specified environment\n\ndef solve():\n    \"\"\"\n    Simulates the discrete-time Kalman covariance recursion for three test cases\n    to demonstrate the effect of system detectability on filter stability.\n    \"\"\"\n    \n    # Common parameters for all test cases\n    Q = np.diag([0.01, 0.01])\n    R = np.array([[0.1]])\n    P0 = np.identity(2)\n    N = 200\n    tau = 1.0e12\n\n    # Test cases defined by the problem statement\n    test_cases = [\n        {\n            \"name\": \"Not detectable with an unstable unobservable mode\",\n            \"A\": np.array([[1.2, 0.0], [0.0, 0.9]]),\n            \"C\": np.array([[0.0, 1.0]]),\n        },\n        {\n            \"name\": \"Detectable with an unstable observable mode\",\n            \"A\": np.array([[1.2, 0.0], [0.0, 0.9]]),\n            \"C\": np.array([[1.0, 0.0]]),\n        },\n        {\n            \"name\": \"Unobservable but stable mode\",\n            \"A\": np.array([[0.95, 0.0], [0.0, 0.7]]),\n            \"C\": np.array([[1.0, 0.0]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        C = case[\"C\"]\n        \n        # Initialize the posteriori covariance\n        P = P0.copy()\n        \n        diverged = False\n        for _ in range(N):\n            # 1. Time Update (Prediction)\n            P_predict = A @ P @ A.T + Q\n            \n            # 2. Measurement Update (Correction)\n            # Innovation covariance\n            S = C @ P_predict @ C.T + R\n            \n            # Check for non-invertibility, though R > 0 should prevent this\n            if np.linalg.det(S) == 0:\n                diverged = True\n                break\n\n            # Kalman Gain\n            K = P_predict @ C.T @ np.linalg.inv(S)\n\n            # Update posteriori covariance using the standard form (I - KC)P_predict\n            # This is computationally efficient and sufficient here.\n            # Joseph form P = (I - K@C)@P_predict@(I - K@C).T + K@R@K.T could be used\n            # for better numerical stability in general, but is not necessary for this problem.\n            I = np.identity(P.shape[0])\n            P = (I - K @ C) @ P_predict\n            \n            # 3. Check for divergence\n            # Check for non-finite values (inf or nan)\n            if not np.all(np.isfinite(P)):\n                diverged = True\n                break\n                \n            # Check if the largest eigenvalue exceeds the threshold.\n            # Use eigvalsh for real symmetric matrices. P should be symmetric.\n            try:\n                # Add a small stabilization term to ensure P remains symmetric under floating-point arithmetic\n                P = (P + P.T) / 2.0\n                eigenvalues = np.linalg.eigvalsh(P)\n                if np.max(eigenvalues) > tau:\n                    diverged = True\n                    break\n            except np.linalg.LinAlgError:\n                # This can happen if P becomes non-finite or badly scaled\n                diverged = True\n                break\n\n        # Append the boolean result (True for divergence, False for boundedness)\n        results.append(str(diverged).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world cyber-physical systems often integrate data from heterogeneous sensors operating at different sampling rates. This exercise  addresses this common practical challenge by guiding you through the implementation of a multi-rate Kalman filter. You will learn how to schedule the prediction and correction steps to fuse data from fast and slow sensors, and analyze how the state uncertainty evolves as intermittent but valuable measurements become available.",
            "id": "4228742",
            "problem": "You are tasked with designing and implementing a multi-rate state estimation covariance recursion for a discrete-time linear system that models the state of a Digital Twin (DT) in a Cyber-Physical System (CPS). The estimation method must be based on the Kalman Filter (KF). The scenario considers heterogeneous sensors operating at different rates: a fast sensor that updates at every time step and a slow sensor that updates intermittently. Your program must compute and report the evolution of the posterior state covariance across time, with careful attention to the cross-rate update scheduling.\n\nBegin from the following foundational base that is universally accepted in control and estimation theory: a linear time-invariant (LTI) discrete-time stochastic system with Gaussian process and measurement noise. The stochastic system is defined by the state transition equation and two measurement equations,\n$$\n\\mathbf{x}_{k+1} = \\mathbf{A}\\,\\mathbf{x}_k + \\mathbf{w}_k,\n$$\n$$\n\\mathbf{y}^{(f)}_k = \\mathbf{H}_f\\,\\mathbf{x}_k + \\mathbf{v}^{(f)}_k,\n$$\n$$\n\\mathbf{y}^{(s)}_k = \\mathbf{H}_s\\,\\mathbf{x}_k + \\mathbf{v}^{(s)}_k,\n$$\nwhere $k$ is the discrete time index, $\\mathbf{x}_k \\in \\mathbb{R}^n$ is the state, $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is the state transition matrix, $\\mathbf{H}_f \\in \\mathbb{R}^{m_f \\times n}$ and $\\mathbf{H}_s \\in \\mathbb{R}^{m_s \\times n}$ are measurement matrices for the fast and slow sensors, respectively, and $\\mathbf{w}_k$, $\\mathbf{v}^{(f)}_k$, and $\\mathbf{v}^{(s)}_k$ are mutually independent, zero-mean, white Gaussian noise sequences with covariances $\\mathbf{Q} \\succeq \\mathbf{0}$, $\\mathbf{R}_f \\succ \\mathbf{0}$, and $\\mathbf{R}_s \\succ \\mathbf{0}$, respectively. Independence and Gaussianity are part of the fundamental base that underpins the Kalman Filter derivation via conditional Gaussian distributions and orthogonality of estimation error.\n\nDefine the posterior state error covariance as $\\mathbf{P}_k = \\mathbb{E}\\big[(\\mathbf{x}_k - \\hat{\\mathbf{x}}_k)(\\mathbf{x}_k - \\hat{\\mathbf{x}}_k)^\\top\\big]$, where $\\hat{\\mathbf{x}}_k$ is the estimator of $\\mathbf{x}_k$ at time $k$. At every discrete time step $k$, the fast sensor provides a measurement, and the slow sensor provides a measurement only when $k$ is a positive integer multiple of a given rate parameter $r \\in \\mathbb{N}$. At time $k$, define $\\mathbf{P}_k$ to be the posterior covariance after processing all available measurements at time $k$ in any fixed order that is consistent with Bayesian conditioning on independent measurement sets. You must analyze the multi-rate scheduling by evolving $\\mathbf{P}_k$ with a prediction step followed by one fast-sensor update at every $k$, and an additional slow-sensor update only at times when $k$ is a multiple of $r$.\n\nYour implementation must compute the sequence of traces of the posterior covariance, $\\operatorname{tr}(\\mathbf{P}_k)$, for $k = 1, 2, \\dots, N$, based solely on the covariance recursion implied by the linear Gaussian model, without simulating any measurement values. The evolution of $\\mathbf{P}_k$ in the linear Gaussian case is measurement-independent and determined by the system, noise, and scheduling matrices. You must use a numerically stable covariance update that preserves positive semidefiniteness.\n\nYou will solve this for the following fixed model dimension $n = 2$ with the following matrices:\n$$\n\\mathbf{A} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}, \\quad\n\\mathbf{H}_f = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad\n\\mathbf{H}_s = \\begin{bmatrix} 0 & 1 \\end{bmatrix},\n$$\nwith $\\mathbf{Q} = q\\,\\mathbf{I}_2$ for a given scalar $q > 0$, where $\\mathbf{I}_2$ is the $2 \\times 2$ identity, and initial covariance\n$$\n\\mathbf{P}_0 = \\operatorname{diag}(10, 10).\n$$\nSet the time horizon to $N = 10$. At each time $k = 1, 2, \\dots, N$, apply the following schedule: predict from $\\mathbf{P}_{k-1}$, then perform one fast-sensor update, then—only if $k$ is a multiple of $r$—perform an additional slow-sensor update.\n\nDesign your program to compute the sequence $\\big[\\operatorname{tr}(\\mathbf{P}_1), \\operatorname{tr}(\\mathbf{P}_2), \\dots, \\operatorname{tr}(\\mathbf{P}_{10})\\big]$ for each of the test cases below. Your program must not generate random numbers or simulate actual measurement values; it must compute the covariance recursion deterministically from the specified matrices.\n\nTest Suite:\n- Case $1$ (happy path: both sensors frequent and informative): $r = 1$, $q = 0.1$, $\\mathbf{R}_f = 1.0$, $\\mathbf{R}_s = 0.25$.\n- Case $2$ (intermittent slow sensing): $r = 3$, $q = 0.1$, $\\mathbf{R}_f = 1.0$, $\\mathbf{R}_s = 0.25$.\n- Case $3$ (rare but highly accurate slow sensing, noisy fast sensing): $r = 5$, $q = 0.1$, $\\mathbf{R}_f = 10.0$, $\\mathbf{R}_s = 0.01$.\n- Case $4$ (slow sensing extremely noisy, fast sensing highly accurate): $r = 4$, $q = 0.1$, $\\mathbf{R}_f = 0.01$, $\\mathbf{R}_s = 1000.0$.\n\nFor each case, your program must output the list of $N$ floats representing the traces of $\\mathbf{P}_k$ for $k = 1, \\dots, 10$, rounded to six decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, with each inner list corresponding to one test case and containing the sequence $\\big[\\operatorname{tr}(\\mathbf{P}_1), \\dots, \\operatorname{tr}(\\mathbf{P}_{10})\\big]$ rounded to six decimals. For example, the formatting must be:\n$$\n\\texttt{[[t\\_1^{(1)},t\\_2^{(1)},\\dots,t\\_{10}^{(1)}],[t\\_1^{(2)},\\dots,t\\_{10}^{(2)}],[t\\_1^{(3)},\\dots,t\\_{10}^{(3)}],[t\\_1^{(4)},\\dots,t\\_{10}^{(4)}]]}\n$$\nwith no spaces in the printed output.",
            "solution": "The problem requires the computation of the sequence of posterior state error covariance matrices, $\\mathbf{P}_k$, for a discrete-time linear stochastic system subject to multi-rate sensor measurements. The solution is derived from the standard Kalman filter covariance update equations, tailored to a specific update schedule.\n\nThe system is a linear time-invariant (LTI) model defined by the state transition equation:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{A}\\,\\mathbf{x}_k + \\mathbf{w}_k\n$$\nand two measurement equations corresponding to a fast and a slow sensor:\n$$\n\\mathbf{y}^{(f)}_k = \\mathbf{H}_f\\,\\mathbf{x}_k + \\mathbf{v}^{(f)}_k\n$$\n$$\n\\mathbf{y}^{(s)}_k = \\mathbf{H}_s\\,\\mathbf{x}_k + \\mathbf{v}^{(s)}_k\n$$\nHere, $k$ is the discrete time index, $\\mathbf{x}_k \\in \\mathbb{R}^n$ is the state vector, $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is the state transition matrix, and $\\mathbf{H}_f \\in \\mathbb{R}^{m_f \\times n}$ and $\\mathbf{H}_s \\in \\mathbb{R}^{m_s \\times n}$ are the measurement matrices. The process noise $\\mathbf{w}_k$ and measurement noises $\\mathbf{v}^{(f)}_k$ and $\\mathbf{v}^{(s)}_k$ are specified as independent, zero-mean, white Gaussian sequences with covariance matrices $\\mathbf{Q}$, $\\mathbf{R}_f$, and $\\mathbf{R}_s$, respectively.\n\nThe posterior state error covariance is defined as $\\mathbf{P}_k = \\mathbb{E}\\big[(\\mathbf{x}_k - \\hat{\\mathbf{x}}_k)(\\mathbf{x}_k - \\hat{\\mathbf{x}}_k)^\\top\\big]$, where $\\hat{\\mathbf{x}}_k$ is the state estimate given all measurements up to time $k$. The evolution of $\\mathbf{P}_k$ is governed by the Kalman filter's Riccati equation, which is deterministic and independent of the actual measurement values. The task is to compute the trace of $\\mathbf{P}_k$, denoted $\\operatorname{tr}(\\mathbf{P}_k)$, for $k = 1, 2, \\dots, N$.\n\nThe calculation at each time step $k$ proceeds sequentially, starting with the posterior covariance from the previous step, $\\mathbf{P}_{k-1}$ (with $\\mathbf{P}_0$ given as the initial condition), and following the specified schedule of prediction and correction steps.\n\n1.  **Prediction Step (Time Update)**:\n    The first step is to project the covariance from time $k-1$ to time $k$. This yields the *a priori* (predicted) covariance at time $k$, denoted $\\mathbf{P}_k^-$.\n    $$\n    \\mathbf{P}_k^{-} = \\mathbf{A} \\mathbf{P}_{k-1} \\mathbf{A}^\\top + \\mathbf{Q}\n    $$\n    This step increases the uncertainty due to the system dynamics and the addition of process noise $\\mathbf{Q}$.\n\n2.  **Correction Step 1 (Fast Sensor Update)**:\n    Next, the *a priori* covariance $\\mathbf{P}_k^-$ is updated using the measurement from the fast sensor, which is available at every time step $k$. To ensure numerical stability and preserve the positive semi-definiteness of the covariance matrix, the Joseph form of the covariance update is used. The posterior covariance after this update is denoted $\\mathbf{P}_k^{(f)}$.\n    The Kalman gain $\\mathbf{K}_k^{(f)}$ is first computed:\n    $$\n    \\mathbf{K}_k^{(f)} = \\mathbf{P}_k^{-} (\\mathbf{H}_f)^\\top (\\mathbf{H}_f \\mathbf{P}_k^{-} (\\mathbf{H}_f)^\\top + \\mathbf{R}_f)^{-1}\n    $$\n    Then, the covariance is updated:\n    $$\n    \\mathbf{P}_k^{(f)} = (\\mathbf{I} - \\mathbf{K}_k^{(f)} \\mathbf{H}_f) \\mathbf{P}_k^{-} (\\mathbf{I} - \\mathbf{K}_k^{(f)} \\mathbf{H}_f)^\\top + \\mathbf{K}_k^{(f)} \\mathbf{R}_f (\\mathbf{K}_k^{(f)})^\\top\n    $$\n    where $\\mathbf{I}$ is the identity matrix of size $n \\times n$.\n\n3.  **Correction Step 2 (Slow Sensor Update, Conditional)**:\n    A second correction is performed only if a measurement from the slow sensor is available, which occurs when $k$ is a positive integer multiple of the rate parameter $r$.\n    - If $k \\pmod{r} \\neq 0$, no slow sensor measurement is available. The final posterior covariance for the current step is simply the result from the fast sensor update: $\\mathbf{P}_k = \\mathbf{P}_k^{(f)}$.\n    - If $k \\pmod{r} = 0$, an additional update is performed. This sequential update is consistent with Bayesian conditioning on multiple independent measurements. The prior for this update is the posterior from the fast sensor update, $\\mathbf{P}_k^{(f)}$. The final posterior covariance $\\mathbf{P}_k$ is computed using the Joseph form again:\n    First, the Kalman gain $\\mathbf{K}_k^{(s)}$ for the slow sensor is calculated:\n    $$\n    \\mathbf{K}_k^{(s)} = \\mathbf{P}_k^{(f)} (\\mathbf{H}_s)^\\top (\\mathbf{H}_s \\mathbf{P}_k^{(f)} (\\mathbf{H}_s)^\\top + \\mathbf{R}_s)^{-1}\n    $$\n    Then, the final posterior covariance $\\mathbf{P}_k$ is computed:\n    $$\n    \\mathbf{P}_k = (\\mathbf{I} - \\mathbf{K}_k^{(s)} \\mathbf{H}_s) \\mathbf{P}_k^{(f)} (\\mathbf{I} - \\mathbf{K}_k^{(s)} \\mathbf{H}_s)^\\top + \\mathbf{K}_k^{(s)} \\mathbf{R}_s (\\mathbf{K}_k^{(s)})^\\top\n    $$\n\nThis three-stage process is iterated for $k = 1, \\dots, N=10$, using the specified system matrices $\\mathbf{A}$, $\\mathbf{H}_f$, $\\mathbf{H}_s$, noise covariances $\\mathbf{Q}$, $\\mathbf{R}_f$, $\\mathbf{R}_s$, and initial covariance $\\mathbf{P}_0 = \\operatorname{diag}(10, 10)$. For each time step $k$, the trace of the final posterior covariance matrix $\\mathbf{P}_k$ is calculated and stored. The trace, $\\operatorname{tr}(\\mathbf{P}_k)$, is a scalar measure of the total uncertainty in the state estimate.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the evolution of the posterior state covariance trace for a multi-rate Kalman filter.\n    \"\"\"\n\n    def joseph_update(p_prior, h_matrix, r_noise):\n        \"\"\"\n        Performs a numerically stable Kalman filter covariance update using the Joseph form.\n\n        Args:\n            p_prior (np.ndarray): The prior covariance matrix (n x n).\n            h_matrix (np.ndarray): The measurement matrix (m x n). In this problem, m=1.\n            r_noise (float): The measurement noise covariance (scalar, since m=1).\n\n        Returns:\n            np.ndarray: The posterior covariance matrix (n x n).\n        \"\"\"\n        # Innovation covariance: S = H * P_prior * H.T + R\n        # H is (1,n), P is (n,n), H.T is (n,1) -> H @ P @ H.T is (1,1) array.\n        s_innov = h_matrix @ p_prior @ h_matrix.T + r_noise\n        s_inv = 1.0 / s_innov[0, 0]\n\n        # Kalman gain: K = P_prior * H.T * S^-1\n        # P_prior @ H.T is (n,1). s_inv is scalar. K is (n,1) column vector.\n        k_gain = (p_prior @ h_matrix.T) * s_inv\n\n        # Joseph form update: P_post = (I - K @ H) @ P_prior @ (I - K @ H).T + K @ R @ K.T\n        n_states = p_prior.shape[0]\n        i_matrix = np.identity(n_states)\n        \n        # (I - K @ H) is (n,n)\n        i_minus_kh = i_matrix - k_gain @ h_matrix\n        \n        # r_noise is scalar, k_gain is (n,1) vector. k_gain @ k_gain.T is (n,n) outer product.\n        k_r_kt = r_noise * (k_gain @ k_gain.T)\n\n        p_post = i_minus_kh @ p_prior @ i_minus_kh.T + k_r_kt\n        \n        return p_post\n\n    def run_simulation(params):\n        \"\"\"\n        Runs a single test case of the multi-rate Kalman filter covariance recursion.\n\n        Args:\n            params (tuple): A tuple containing (r, q, R_f, R_s).\n\n        Returns:\n            list: A list of floats representing the trace of the posterior covariance at each step.\n        \"\"\"\n        r, q, r_f, r_s = params\n        n_horizon = 10\n        n_dim = 2\n\n        # System model matrices\n        mat_a = np.array([[1., 1.], [0., 1.]])\n        mat_hf = np.array([[1., 0.]])\n        mat_hs = np.array([[0., 1.]])\n        mat_q = q * np.identity(n_dim)\n        \n        # Initial covariance\n        p_current = np.array([[10., 0.], [0., 10.]])\n        \n        traces = []\n        for k in range(1, n_horizon + 1):\n            # 1. Prediction Step\n            p_predict = mat_a @ p_current @ mat_a.T + mat_q\n            \n            # 2. Fast Sensor Update\n            p_after_fast = joseph_update(p_predict, mat_hf, r_f)\n            \n            # 3. Slow Sensor Update (Conditional)\n            if k % r == 0:\n                p_final = joseph_update(p_after_fast, mat_hs, r_s)\n            else:\n                p_final = p_after_fast\n                \n            p_current = p_final\n            traces.append(np.trace(p_current))\n            \n        return [round(t, 6) for t in traces]\n\n    # (r, q, R_f, R_s)\n    test_cases = [\n        (1, 0.1, 1.0, 0.25),      # Case 1\n        (3, 0.1, 1.0, 0.25),      # Case 2\n        (5, 0.1, 10.0, 0.01),     # Case 3\n        (4, 0.1, 0.01, 1000.0),   # Case 4\n    ]\n\n    results = [run_simulation(case) for case in test_cases]\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "A well-tuned Kalman filter does more than just estimate states; it also serves as a model of the system's expected behavior, and its innovation sequence can be used for monitoring. This exercise  demonstrates how to use the innovation, normalized by its covariance $S_k$, to construct a powerful statistical test for anomaly detection based on the Mahalanobis distance $e_k^\\top S_k^{-1} e_k$. This practice is fundamental for developing self-monitoring capabilities in digital twins and cyber-physical systems.",
            "id": "4228746",
            "problem": "You are tasked with designing, justifying, and implementing an innovation-based anomaly detector for a Digital Twin in a Cyber-Physical System (CPS). The estimator is a member of the Kalman filtering family, possibly a linear Kalman Filter (KF), Extended Kalman Filter (EKF), or Unscented Kalman Filter (UKF). Under correct modeling assumptions, the innovation (also called residual) at discrete time index $k$, denoted by $e_k$, is a random vector with covariance matrix $S_k$. Assume the measurement noise is zero-mean Gaussian and that the estimator computes $e_k$ and $S_k$ consistently from its prediction and update steps.\n\nYour tasks are:\n- Derive, from first principles, why the scalar test statistic $e_k^\\top S_k^{-1} e_k$ has a Chi-Squared distribution with degrees of freedom equal to the measurement dimension under the Gaussianity assumption of the innovation.\n- Implement a numerically stable detector that, for each provided test case, computes the statistic $e_k^\\top S_k^{-1} e_k$ without explicitly inverting $S_k$, compares the value to the Chi-Squared critical value for a specified significance level $\\alpha$ and measurement dimension $m$, and outputs a boolean indicating anomaly if and only if the statistic is strictly greater than the critical value.\n\nUse a numerically stable approach by solving triangular systems via a Cholesky factorization. If the Cholesky factorization fails because $S_k$ is nearly singular, add a small diagonal jitter $\\epsilon I$ with $\\epsilon = 10^{-9}$ and retry, where $I$ is the identity matrix of appropriate dimension.\n\nThere are no physical units involved in this problem; treat all quantities as dimensionless. Angles are not involved. Express all final outputs as booleans.\n\nThe test suite consists of five independent cases, each specified by the measurement dimension $m$, a symmetric positive definite covariance $S_k$, an innovation vector $e_k$, and significance level $\\alpha$:\n\n- Case $1$ (general well-conditioned $2$-dimensional case):\n  - $m = 2$\n  - $S_k = \\begin{bmatrix} 1.0 & 0.1 \\\\ 0.1 & 1.5 \\end{bmatrix}$\n  - $e_k = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n\n- Case $2$ (boundary case exactly at the critical value for $m=2$ and $\\alpha=0.05$ with identity covariance):\n  - $m = 2$\n  - $S_k = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$\n  - $e_k = \\begin{bmatrix} \\sqrt{5.991464547107982} \\\\ 0.0 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n\n- Case $3$ (clear anomaly in $3$ dimensions):\n  - $m = 3$\n  - $S_k = \\operatorname{diag}(0.5, 0.5, 2.0)$\n  - $e_k = \\begin{bmatrix} 3.0 \\\\ -3.0 \\\\ 0.0 \\end{bmatrix}$\n  - $\\alpha = 0.01$\n\n- Case $4$ (scalar measurement with small variance):\n  - $m = 1$\n  - $S_k = \\begin{bmatrix} 0.01 \\end{bmatrix}$\n  - $e_k = \\begin{bmatrix} 0.2 \\end{bmatrix}$\n  - $\\alpha = 0.10$\n\n- Case $5$ (nearly singular covariance in $2$ dimensions):\n  - $m = 2$\n  - $S_k = \\operatorname{diag}(10^{-6}, 10.0)$\n  - $e_k = \\begin{bmatrix} 10^{-3} \\\\ 0.0 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n\nFor each case, compute the statistic $t_k = e_k^\\top S_k^{-1} e_k$ via Cholesky-based whitening and compare it to the Chi-Squared critical value $c_{\\alpha} = F_{\\chi^2(m)}^{-1}(1 - \\alpha)$, where $F_{\\chi^2(m)}^{-1}$ denotes the inverse cumulative distribution function (quantile function) of the Chi-Squared distribution with $m$ degrees of freedom. The detector should flag an anomaly if and only if $t_k > c_{\\alpha}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4,result_5]$), where each $result_i$ is a boolean corresponding to each case in the order listed above.",
            "solution": "### Principle-Based Solution\n\nThe solution is presented in two parts as required: a theoretical derivation followed by an explanation of the numerical implementation.\n\n#### Part 1: Derivation of the Test Statistic's Distribution\n\nThe objective is to demonstrate that the test statistic $t_k = e_k^\\top S_k^{-1} e_k$ follows a Chi-Squared ($\\chi^2$) distribution with $m$ degrees of freedom.\n\nThe derivation begins from the premise that under normal operating conditions (the null hypothesis), the innovation vector $e_k$ is a random variable that follows a multivariate Gaussian distribution with a mean vector of $0$ and a covariance matrix $S_k$. The dimension of $e_k$ is $m$. This is formally stated as:\n$$\ne_k \\sim \\mathcal{N}(0, S_k)\n$$\nwhere $e_k \\in \\mathbb{R}^m$ and $S_k \\in \\mathbb{R}^{m \\times m}$.\n\nSince $S_k$ is a covariance matrix, it is symmetric and positive semi-definite. For a properly operating filter with an observable system, $S_k$ is strictly positive definite. A positive definite matrix $S_k$ can be uniquely decomposed via Cholesky factorization into the product of a lower-triangular matrix $L_k$ and its transpose $L_k^\\top$:\n$$\nS_k = L_k L_k^\\top\n$$\nThe inverse of $S_k$ can then be expressed as:\n$$\nS_k^{-1} = (L_k L_k^\\top)^{-1} = (L_k^\\top)^{-1} L_k^{-1}\n$$\n\nThe next step is to \"whiten\" the innovation vector $e_k$. Whitening is a linear transformation that converts a random vector with a known covariance matrix into a new random vector with an identity covariance matrix (and zero mean). Let us define the whitened innovation vector, $z_k$, as:\n$$\nz_k = L_k^{-1} e_k\n$$\nWe now analyze the statistical properties of $z_k$. The expected value of $z_k$ is:\n$$\n\\mathbb{E}[z_k] = \\mathbb{E}[L_k^{-1} e_k] = L_k^{-1} \\mathbb{E}[e_k] = L_k^{-1} \\cdot 0 = 0\n$$\nThe covariance matrix of $z_k$ is calculated as follows:\n$$\n\\operatorname{Cov}(z_k) = \\mathbb{E}[ (z_k - \\mathbb{E}[z_k]) (z_k - \\mathbb{E}[z_k])^\\top ] = \\mathbb{E}[z_k z_k^\\top]\n$$\nSubstituting the definition of $z_k$:\n$$\n\\operatorname{Cov}(z_k) = \\mathbb{E}[ (L_k^{-1} e_k) (L_k^{-1} e_k)^\\top ] = \\mathbb{E}[ L_k^{-1} e_k e_k^\\top (L_k^{-1})^\\top ]\n$$\nUsing the linearity of the expectation operator:\n$$\n\\operatorname{Cov}(z_k) = L_k^{-1} \\mathbb{E}[e_k e_k^\\top] (L_k^\\top)^{-1}\n$$\nBy definition, $\\mathbb{E}[e_k e_k^\\top]$ is the covariance matrix of $e_k$, which is $S_k$.\n$$\n\\operatorname{Cov}(z_k) = L_k^{-1} S_k (L_k^\\top)^{-1}\n$$\nNow, substituting the Cholesky factorization $S_k = L_k L_k^\\top$:\n$$\n\\operatorname{Cov}(z_k) = L_k^{-1} (L_k L_k^\\top) (L_k^\\top)^{-1} = (L_k^{-1} L_k) (L_k^\\top (L_k^\\top)^{-1}) = I_m \\cdot I_m = I_m\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix.\n\nSince $e_k$ is a Gaussian random vector and $z_k$ is a linear transformation of $e_k$, $z_k$ is also a Gaussian random vector. We have shown that its mean is $0$ and its covariance is $I_m$. Thus, $z_k$ follows a standard multivariate normal distribution:\n$$\nz_k \\sim \\mathcal{N}(0, I_m)\n$$\nThis implies that the components of $z_k$, denoted $z_{k,i}$ for $i=1, \\dots, m$, are independent and identically distributed (i.i.d.) standard normal random variables, i.e., $z_{k,i} \\sim \\mathcal{N}(0, 1)$.\n\nNow, we can re-express the test statistic $t_k$ in terms of the whitened vector $z_k$:\n$$\nt_k = e_k^\\top S_k^{-1} e_k = e_k^\\top (L_k^\\top)^{-1} L_k^{-1} e_k\n$$\nUsing the property $(AB)^\\top = B^\\top A^\\top$, we can write this as:\n$$\nt_k = (L_k^{-1} e_k)^\\top (L_k^{-1} e_k) = z_k^\\top z_k\n$$\nThe quantity $z_k^\\top z_k$ is the sum of the squares of the components of $z_k$:\n$$\nt_k = \\sum_{i=1}^{m} z_{k,i}^2\n$$\nBy the definition of the Chi-Squared distribution, the sum of the squares of $m$ independent standard normal random variables follows a Chi-Squared distribution with $m$ degrees of freedom.\n\nTherefore, we have established that the test statistic $t_k = e_k^\\top S_k^{-1} e_k$ is distributed as $\\chi^2(m)$. This completes the derivation.\n\n#### Part 2: Numerically Stable Implementation\n\nThe implementation of the anomaly detector is based on the insights from the derivation. The goal is to compute $t_k = e_k^\\top S_k^{-1} e_k$ and compare it to a critical value without performing an explicit matrix inversion of $S_k$, which is numerically unstable and computationally inefficient.\n\nThe algorithmic steps are as follows:\n\n1.  **Cholesky Factorization**: Given the innovation covariance matrix $S_k$, compute its lower-triangular Cholesky factor $L_k$ such that $S_k = L_k L_k^\\top$. A robust implementation must handle cases where $S_k$ is not numerically positive definite (e.g., due to floating-point errors or model ill-conditioning). As specified, if the initial factorization fails, a small regularization term, or \"jitter,\" $\\epsilon I_m$ is added to $S_k$, where $\\epsilon = 10^{-9}$. The factorization is then re-attempted on the regularized matrix $S_k' = S_k + \\epsilon I_m$.\n\n2.  **Whitening via Triangular Solve**: Instead of computing $z_k = L_k^{-1} e_k$ by inverting $L_k$, we solve the equivalent linear system $L_k z_k = e_k$ for the vector $z_k$. Since $L_k$ is lower-triangular, this system can be solved efficiently and stably using forward substitution.\n\n3.  **Statistic Calculation**: Once the whitened vector $z_k$ is obtained, the test statistic $t_k$ is computed simply as the squared Euclidean norm of $z_k$: $t_k = z_k^\\top z_k$. This is computationally trivial and numerically benign.\n\n4.  **Hypothesis Test**:\n    - The critical value, $c_{\\alpha}$, is determined from the inverse cumulative distribution function (CDF) of the Chi-Squared distribution with $m$ degrees of freedom. Specifically, for a significance level $\\alpha$, the critical value is the point on the $\\chi^2(m)$ distribution such that the area under the curve to its right is $\\alpha$. This corresponds to a cumulative probability of $1 - \\alpha$. So, $c_{\\alpha} = F_{\\chi^2(m)}^{-1}(1-\\alpha)$, where $F^{-1}$ is the quantile function (or percent point function).\n    - An anomaly is declared if and only if the computed statistic $t_k$ is strictly greater than the critical value $c_{\\alpha}$. The output for each case is a boolean value: `True` if $t_k > c_{\\alpha}$, and `False` otherwise.\n\nThis approach avoids matrix inversion entirely, relying instead on the much more stable operations of Cholesky factorization and triangular system solving.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, LinAlgError\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the anomaly detection problem for all specified test cases.\n    \"\"\"\n\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        {\n            \"m\": 2,\n            \"S_k\": np.array([[1.0, 0.1], \n                             [0.1, 1.5]]),\n            \"e_k\": np.array([0.2, -0.1]),\n            \"alpha\": 0.05,\n        },\n        {\n            \"m\": 2,\n            \"S_k\": np.array([[1.0, 0.0], \n                             [0.0, 1.0]]),\n            \"e_k\": np.array([np.sqrt(5.991464547107982), 0.0]),\n            \"alpha\": 0.05,\n        },\n        {\n            \"m\": 3,\n            \"S_k\": np.diag([0.5, 0.5, 2.0]),\n            \"e_k\": np.array([3.0, -3.0, 0.0]),\n            \"alpha\": 0.01,\n        },\n        {\n            \"m\": 1,\n            \"S_k\": np.array([[0.01]]),\n            \"e_k\": np.array([0.2]),\n            \"alpha\": 0.10,\n        },\n        {\n            \"m\": 2,\n            \"S_k\": np.diag([1e-6, 10.0]),\n            \"e_k\": np.array([1e-3, 0.0]),\n            \"alpha\": 0.05,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        is_anomaly = detect_anomaly(\n            case[\"m\"], case[\"S_k\"], case[\"e_k\"], case[\"alpha\"]\n        )\n        results.append(is_anomaly)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\ndef detect_anomaly(m, S_k, e_k, alpha):\n    \"\"\"\n    Performs innovation-based anomaly detection for a single time step.\n\n    Args:\n        m (int): The dimension of the measurement space.\n        S_k (np.ndarray): The innovation covariance matrix (m x m).\n        e_k (np.ndarray): The innovation vector (m,).\n        alpha (float): The significance level for the Chi-Squared test.\n\n    Returns:\n        bool: True if an anomaly is detected, False otherwise.\n    \"\"\"\n    epsilon = 1e-9  # Jitter for near-singular matrices\n\n    try:\n        # Perform Cholesky factorization: S_k = L * L.T\n        # 'lower=True' ensures L is lower-triangular.\n        L = cholesky(S_k, lower=True)\n    except LinAlgError:\n        # If S_k is not positive definite, add jitter and retry.\n        S_k_jittered = S_k + epsilon * np.identity(m)\n        L = cholesky(S_k_jittered, lower=True)\n\n    # Whiten the innovation vector by solving the triangular system L * z_k = e_k.\n    # This is numerically superior to computing z_k = inv(L) * e_k.\n    z_k = solve_triangular(L, e_k, lower=True)\n\n    # Compute the test statistic t_k = z_k.T * z_k\n    t_k = np.dot(z_k, z_k)\n\n    # Compute the critical value from the Chi-Squared distribution's\n    # percent point function (inverse CDF).\n    critical_value = chi2.ppf(1 - alpha, df=m)\n\n    # An anomaly is detected if the statistic is strictly greater than the threshold.\n    return t_k > critical_value\n\nsolve()\n```"
        }
    ]
}