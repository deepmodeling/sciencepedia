{
    "hands_on_practices": [
        {
            "introduction": "Moving Horizon Estimation is powerful because it can directly handle nonlinear system models through optimization. A crucial step in this process is linearization, which provides the gradients needed by the solver. This exercise () provides fundamental practice in deriving the Jacobians—the matrices of first-order partial derivatives—for a realistic, nonlinear sensor model found in a cyber-physical system.",
            "id": "4231913",
            "problem": "A cyber-physical mobile robot is represented in its digital twin by the state vector $x_{k} \\in \\mathbb{R}^{2}$ containing planar position $x_{k} = \\begin{pmatrix} p_{x,k} \\\\ p_{y,k} \\end{pmatrix}$. The robot carries a time-of-flight acoustic sensor that measures the distance to a stationary beacon located at a known position $s = \\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}$. The sensor is heated by a control input $u_{k} \\in \\mathbb{R}$ that modifies the local speed of sound according to $c(u_{k}) = c_{0}\\sqrt{1 + \\beta u_{k}}$, where $c_{0} > 0$ and $\\beta > 0$ are known constants determined by calibration. The electronics apply a logarithmic amplifier to the raw time-of-flight, producing a dimensionless measurement output $y_{k}$ defined by the nonlinear function $y_{k} = h(x_{k}, u_{k})$, where $h$ must be constructed from the following physically grounded relationships:\n- The geometric distance between the robot and beacon is $d(x_{k}) = \\| x_{k} - s \\|_{2}$.\n- The time-of-flight is $t(x_{k}, u_{k}) = \\dfrac{d(x_{k})}{c(u_{k})}$.\n- The amplifier output is $y_{k} = \\ln\\!\\big(1 + \\alpha\\, t(x_{k}, u_{k})\\big)$, where $\\alpha > 0$ is a known dimensionless gain factor.\nStarting from these fundamentals, form the explicit measurement model $y_{k} = h(x_{k}, u_{k})$ and derive the Jacobians with respect to the state and input that are needed for Moving Horizon Estimation (MHE). Specifically, compute $\\dfrac{\\partial h}{\\partial x_{k}}$ and $\\dfrac{\\partial h}{\\partial u_{k}}$ using first principles and the chain rule, then evaluate them at the operating point given by $s = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_{k} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, $u_{k} = 1$, $\\alpha = 3$, $c_{0} = 1$ (normalized internal units of the digital twin), and $\\beta = 3$.\nProvide the final result as the single row vector $\\begin{pmatrix} \\dfrac{\\partial h}{\\partial p_{x,k}}  \\dfrac{\\partial h}{\\partial p_{y,k}}  \\dfrac{\\partial h}{\\partial u_{k}} \\end{pmatrix}$ in exact fractional form. For interpretability, the state Jacobian entries are to be understood in inverse meters and the input Jacobian entry in inverse-control units; do not include units inside the final boxed answer.",
            "solution": "The problem requires the derivation and evaluation of the Jacobians of a nonlinear measurement function $h(x_{k}, u_{k})$ for a cyber-physical system, as is common in estimation algorithms like Moving Horizon Estimation (MHE) or the Extended Kalman Filter (EKF). The Jacobians are the matrices of first-order partial derivatives of the measurement function with respect to the state vector $x_k$ and the control input $u_k$.\n\nFirst, we must construct the explicit form of the measurement function $y_{k} = h(x_{k}, u_{k})$. The problem provides the following relationships:\nThe state vector is $x_{k} = \\begin{pmatrix} p_{x,k} \\\\ p_{y,k} \\end{pmatrix}$.\nThe beacon is at a known position $s = \\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}$.\nThe geometric distance is $d(x_{k}) = \\| x_{k} - s \\|_{2} = \\sqrt{(p_{x,k} - s_{x})^{2} + (p_{y,k} - s_{y})^{2}}$.\nThe speed of sound is dependent on a control input $u_k$ as $c(u_{k}) = c_{0}\\sqrt{1 + \\beta u_{k}}$.\nThe time-of-flight is $t(x_{k}, u_{k}) = \\dfrac{d(x_{k})}{c(u_{k})}$.\nThe final measurement is $y_{k} = \\ln\\!\\big(1 + \\alpha\\, t(x_{k}, u_{k})\\big)$.\n\nSubstituting these expressions into one another, we obtain the complete measurement model:\n$$h(x_{k}, u_{k}) = \\ln\\left(1 + \\alpha \\frac{\\sqrt{(p_{x,k} - s_{x})^{2} + (p_{y,k} - s_{y})^{2}}}{c_{0}\\sqrt{1 + \\beta u_{k}}}\\right)$$\n\nOur task is to compute the partial derivatives of $h$ with respect to each component of the state $x_k$ (which are $p_{x,k}$ and $p_{y,k}$) and the control input $u_k$. We will use the chain rule for differentiation.\n\nLet's compute the state Jacobian, $\\dfrac{\\partial h}{\\partial x_{k}} = \\begin{pmatrix} \\dfrac{\\partial h}{\\partial p_{x,k}}  \\dfrac{\\partial h}{\\partial p_{y,k}} \\end{pmatrix}$.\nLet $g(d, c) = \\ln(1 + \\alpha \\frac{d}{c})$. Then $h(x_k, u_k) = g(d(x_k), c(u_k))$.\nThe derivative with respect to an intermediate variable, say $z$, is $\\dfrac{\\partial h}{\\partial z} = \\dfrac{\\partial g}{\\partial d}\\dfrac{\\partial d}{\\partial z} + \\dfrac{\\partial g}{\\partial c}\\dfrac{\\partial c}{\\partial z}$.\nFor the state variables $p_{x,k}$ and $p_{y,k}$, the speed of sound $c$ is constant, so $\\dfrac{\\partial c}{\\partial p_{x,k}} = 0$ and $\\dfrac{\\partial c}{\\partial p_{y,k}} = 0$.\nThe derivatives are thus simplified:\n$\\dfrac{\\partial h}{\\partial p_{x,k}} = \\dfrac{\\partial g}{\\partial d}\\dfrac{\\partial d}{\\partial p_{x,k}}$ and $\\dfrac{\\partial h}{\\partial p_{y,k}} = \\dfrac{\\partial g}{\\partial d}\\dfrac{\\partial d}{\\partial p_{y,k}}$.\n\nFirst, we compute $\\dfrac{\\partial g}{\\partial d}$:\n$$\\frac{\\partial g}{\\partial d} = \\frac{1}{1 + \\alpha \\frac{d}{c}} \\cdot \\frac{\\alpha}{c} = \\frac{\\alpha}{c + \\alpha d}$$\nNext, we compute the derivatives of the distance function $d(x_k) = \\sqrt{(p_{x,k} - s_{x})^{2} + (p_{y,k} - s_{y})^{2}}$:\n$$\\frac{\\partial d}{\\partial p_{x,k}} = \\frac{1}{2d} \\cdot 2(p_{x,k} - s_{x}) = \\frac{p_{x,k} - s_{x}}{d}$$\n$$\\frac{\\partial d}{\\partial p_{y,k}} = \\frac{1}{2d} \\cdot 2(p_{y,k} - s_{y}) = \\frac{p_{y,k} - s_{y}}{d}$$\nCombining these results, we get the components of the state Jacobian:\n$$\\frac{\\partial h}{\\partial p_{x,k}} = \\frac{\\alpha}{c + \\alpha d} \\cdot \\frac{p_{x,k} - s_{x}}{d} = \\frac{\\alpha (p_{x,k} - s_{x})}{d(c + \\alpha d)}$$\n$$\\frac{\\partial h}{\\partial p_{y,k}} = \\frac{\\alpha}{c + \\alpha d} \\cdot \\frac{p_{y,k} - s_{y}}{d} = \\frac{\\alpha (p_{y,k} - s_{y})}{d(c + \\alpha d)}$$\nThe state Jacobian row vector is $\\dfrac{\\partial h}{\\partial x_{k}} = \\dfrac{\\alpha (x_k - s)^T}{d(c+\\alpha d)}$.\n\nNow, we compute the input Jacobian, $\\dfrac{\\partial h}{\\partial u_{k}}$.\nFor the input $u_k$, the distance $d$ is constant, so $\\dfrac{\\partial d}{\\partial u_k} = 0$. The derivative simplifies to:\n$\\dfrac{\\partial h}{\\partial u_{k}} = \\dfrac{\\partial g}{\\partial c}\\dfrac{\\partial c}{\\partial u_{k}}$.\nFirst, we compute $\\dfrac{\\partial g}{\\partial c}$:\n$$\\frac{\\partial g}{\\partial c} = \\frac{1}{1 + \\alpha \\frac{d}{c}} \\cdot \\left(-\\frac{\\alpha d}{c^2}\\right) = \\frac{c}{c + \\alpha d} \\cdot \\left(-\\frac{\\alpha d}{c^2}\\right) = -\\frac{\\alpha d}{c(c + \\alpha d)}$$\nNext, we compute the derivative of the sound speed function $c(u_k) = c_{0}(1 + \\beta u_{k})^{1/2}$:\n$$\\frac{\\partial c}{\\partial u_{k}} = c_{0} \\cdot \\frac{1}{2}(1 + \\beta u_{k})^{-1/2} \\cdot \\beta = \\frac{c_{0}\\beta}{2\\sqrt{1 + \\beta u_{k}}}$$\nWe can express this in terms of $c$ itself: since $c = c_{0}\\sqrt{1 + \\beta u_{k}}$, we have $\\sqrt{1 + \\beta u_{k}} = c/c_0$.\n$$\\frac{\\partial c}{\\partial u_{k}} = \\frac{c_{0}\\beta}{2(c/c_0)} = \\frac{c_{0}^{2}\\beta}{2c}$$\nCombining these results gives the input Jacobian:\n$$\\frac{\\partial h}{\\partial u_{k}} = \\left(-\\frac{\\alpha d}{c(c + \\alpha d)}\\right) \\cdot \\left(\\frac{c_{0}^{2}\\beta}{2c}\\right) = -\\frac{\\alpha d c_{0}^{2}\\beta}{2c^{2}(c + \\alpha d)}$$\n\nThe final step is to evaluate these Jacobian expressions at the given operating point:\n$s = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_{k} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, $u_{k} = 1$, $\\alpha = 3$, $c_{0} = 1$, $\\beta = 3$.\n\nWe first calculate the intermediate values at this point:\nThe state components are $p_{x,k} = 3$ and $p_{y,k} = 4$.\nThe distance is $d = \\sqrt{(3-0)^2 + (4-0)^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\nThe speed of sound is $c = c_{0}\\sqrt{1 + \\beta u_{k}} = 1\\sqrt{1 + 3 \\cdot 1} = \\sqrt{4} = 2$.\nA useful denominator term is $c + \\alpha d = 2 + 3 \\cdot 5 = 2 + 15 = 17$.\n\nNow, we substitute these values into the Jacobian expressions.\nFor the state Jacobian:\n$$\\frac{\\partial h}{\\partial p_{x,k}} = \\frac{\\alpha (p_{x,k} - s_{x})}{d(c + \\alpha d)} = \\frac{3(3 - 0)}{5(17)} = \\frac{9}{85}$$\n$$\\frac{\\partial h}{\\partial p_{y,k}} = \\frac{\\alpha (p_{y,k} - s_{y})}{d(c + \\alpha d)} = \\frac{3(4 - 0)}{5(17)} = \\frac{12}{85}$$\nFor the input Jacobian:\n$$\\frac{\\partial h}{\\partial u_{k}} = -\\frac{\\alpha d c_{0}^{2}\\beta}{2c^{2}(c + \\alpha d)} = -\\frac{3 \\cdot 5 \\cdot 1^2 \\cdot 3}{2 \\cdot 2^2 \\cdot (17)} = -\\frac{45}{2 \\cdot 4 \\cdot 17} = -\\frac{45}{136}$$\nThe resulting Jacobian row vector is $\\begin{pmatrix} \\dfrac{9}{85}  \\dfrac{12}{85}  -\\dfrac{45}{136} \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{9}{85}  \\frac{12}{85}  -\\frac{45}{136} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "At its core, every MHE problem is an optimization problem, which for linear systems with constraints takes the form of a Quadratic Program (QP). This exercise () guides you through the process of translating a standard MHE formulation into a QP and deriving the Karush-Kuhn-Tucker (KKT) conditions. Understanding these conditions provides deep insight into the structure of the solution and the significance of active constraints.",
            "id": "4231989",
            "problem": "Consider a digital twin Moving Horizon Estimation (MHE) subproblem for a linear time-invariant cyber-physical system over a horizon of two steps. The plant evolves according to the discrete-time stochastic model\n- State dynamics: $x_{k+1} = A x_{k} + B u_{k} + w_{k}$,\n- Output equation: $y_{k} = C x_{k} + v_{k}$,\nwhere $x_{k} \\in \\mathbb{R}^{n}$, $u_{k} \\in \\mathbb{R}^{m}$ is a known input, $y_{k} \\in \\mathbb{R}^{p}$ is the measured output, and $w_{k} \\in \\mathbb{R}^{n}$, $v_{k} \\in \\mathbb{R}^{p}$ are process and measurement disturbances. The MHE cost penalizes the arrival deviation for the initial state and the process and measurement residuals with inverse covariance weightings:\n- Arrival term: $(x_{0} - \\bar{x}_{0})^{\\top} P^{-1} (x_{0} - \\bar{x}_{0})$,\n- Process terms: $w_{0}^{\\top} Q^{-1} w_{0}$ and $w_{1}^{\\top} Q^{-1} w_{1}$,\n- Measurement terms: $r_{1}^{\\top} R^{-1} r_{1}$ and $r_{2}^{\\top} R^{-1} r_{2}$,\nwhere $\\bar{x}_{0} \\in \\mathbb{R}^{n}$ is the a priori state estimate and $P \\in \\mathbb{S}_{++}^{n}$, $Q \\in \\mathbb{S}_{++}^{n}$, $R \\in \\mathbb{S}_{++}^{p}$ are symmetric positive definite.\n\nDefine the decision vector by stacking\n$$\nz \\;\\triangleq\\; \\begin{pmatrix} x_{0} \\\\ x_{1} \\\\ x_{2} \\\\ w_{0} \\\\ w_{1} \\\\ r_{1} \\\\ r_{2} \\end{pmatrix} \\in \\mathbb{R}^{3n + 2n + 2p} \\;=\\; \\mathbb{R}^{5n + 2p},\n$$\nand define linear equality constraints that enforce the dynamics and the definition of residuals:\n- $x_{1} - A x_{0} - B u_{0} - w_{0} = 0$,\n- $x_{2} - A x_{1} - B u_{1} - w_{1} = 0$,\n- $r_{1} - (y_{1} - C x_{1}) = 0$,\n- $r_{2} - (y_{2} - C x_{2}) = 0$,\nwith given data $u_{0}, u_{1} \\in \\mathbb{R}^{m}$ and $y_{1}, y_{2} \\in \\mathbb{R}^{p}$. Impose convex state box constraints at $k=1,2$:\n- $\\ell \\le x_{k} \\le u$ for $k \\in \\{1,2\\}$,\nwhere $\\ell, u \\in \\mathbb{R}^{n}$ satisfy $\\ell \\le u$ componentwise.\n\nUse linear selection matrices $S_{x0}, S_{x1}, S_{x2}, S_{w0}, S_{w1}, S_{r1}, S_{r2}$ such that $S_{x0} z = x_{0}$, $S_{x1} z = x_{1}$, $S_{x2} z = x_{2}$, $S_{w0} z = w_{0}$, $S_{w1} z = w_{1}$, $S_{r1} z = r_{1}$, $S_{r2} z = r_{2}$. Formulate the MHE subproblem as a convex Quadratic Program (QP) in the standard form\n$$\n\\min_{z} \\;\\; \\tfrac{1}{2} z^{\\top} H z + g^{\\top} z \\quad \\text{subject to} \\quad A z = b, \\;\\; G z \\le h,\n$$\nby specifying $H$, $g$, $A$, $b$, $G$, and $h$ in terms of the given data.\n\nStarting from the definition of the Lagrangian and the optimality conditions for convex programs, derive the necessary and sufficient Karush–Kuhn–Tucker (KKT) conditions for optimality of this QP. Explicitly identify the stationarity, primal feasibility, dual feasibility, and complementary slackness conditions in terms of the matrices $H$, $g$, $A$, $b$, $G$, $h$, and the primal-dual variables $(z, \\lambda, \\mu)$, where $\\lambda$ and $\\mu$ are the dual variables for the equality and inequality constraints, respectively.\n\nThen provide an interpretation of the dual variables as shadow prices on the constraints, carefully justifying the sign conventions used, in terms of the sensitivity of the optimal value with respect to perturbations of the right-hand sides $b$ and $h$.\n\nAnswer specification:\n- Your final boxed answer must be a single closed-form analytic expression: write the compact block saddle-point KKT linear system that simultaneously encodes stationarity and primal feasibility as a single matrix equation in the variables $(z, \\lambda, \\mu)$, using only the symbols defined above.\n- Do not include inequalities or complementarity in the final boxed answer; those should be established in your derivation.",
            "solution": "The problem is a valid and well-posed optimization problem. It asks to formulate a standard Moving Horizon Estimation (MHE) problem as a convex Quadratic Program (QP), derive its Karush-Kuhn-Tucker (KKT) optimality conditions, and interpret the dual variables. We will proceed by first constructing the components of the QP, then deriving the KKT conditions, and finally providing an interpretation of the dual variables before presenting the final KKT linear system.\n\n### 1. Formulation as a Convex Quadratic Program (QP)\n\nThe problem is to find the optimal state and disturbance trajectory by minimizing a cost function subject to system dynamics and constraints. The standard form for a QP is:\n$$\n\\min_{z} \\;\\; \\tfrac{1}{2} z^{\\top} H z + g^{\\top} z \\quad \\text{subject to} \\quad A z = b, \\;\\; G z \\le h\n$$\nWe need to specify the matrices $H, G, A$ and vectors $g, h, b$.\n\n**Objective Function**\n\nThe MHE cost function is given by:\n$$\nJ_{\\text{MHE}} = (x_{0} - \\bar{x}_{0})^{\\top} P^{-1} (x_{0} - \\bar{x}_{0}) + w_{0}^{\\top} Q^{-1} w_{0} + w_{1}^{\\top} Q^{-1} w_{1} + r_{1}^{\\top} R^{-1} r_{1} + r_{2}^{\\top} R^{-1} r_{2}\n$$\nTo match the QP standard form which includes a factor of $\\frac{1}{2}$, we will minimize an objective that shares the same minimizer as $J_{\\text{MHE}}$. This is typically done by defining an objective $J(z)$ such that $\\frac{1}{2} z^{\\top} H z + g^{\\top} z$ matches the quadratic and linear parts of $J_{\\text{MHE}}$.\nExpanding the first term gives:\n$$\n(x_{0} - \\bar{x}_{0})^{\\top} P^{-1} (x_{0} - \\bar{x}_{0}) = x_{0}^{\\top} P^{-1} x_{0} - 2\\bar{x}_{0}^{\\top} P^{-1} x_{0} + \\bar{x}_{0}^{\\top} P^{-1} \\bar{x}_{0}\n$$\nTo construct the QP objective $\\frac{1}{2} z^{\\top} H z + g^{\\top} z$, we drop constant terms and multiply the quadratic terms by $\\frac{1}{2}$. It is a standard convention to formulate the problem to minimize $\\frac{1}{2} J_{\\text{MHE}}$ (ignoring constants), which yields:\n$$\nJ(z) = \\frac{1}{2} x_{0}^{\\top} P^{-1} x_{0} - \\bar{x}_{0}^{\\top} P^{-1} x_{0} + \\frac{1}{2}w_{0}^{\\top} Q^{-1} w_{0} + \\frac{1}{2}w_{1}^{\\top} Q^{-1} w_{1} + \\frac{1}{2}r_{1}^{\\top} R^{-1} r_{1} + \\frac{1}{2}r_{2}^{\\top} R^{-1} r_{2}\n$$\nUsing the selection matrices to express variables in terms of the decision vector $z$ (e.g., $x_{0} = S_{x0}z$), the objective becomes:\n$$\nJ(z) = \\frac{1}{2} z^{\\top} (S_{x0}^{\\top} P^{-1} S_{x0} + S_{w0}^{\\top} Q^{-1} S_{w0} + S_{w1}^{\\top} Q^{-1} S_{w1} + S_{r1}^{\\top} R^{-1} S_{r1} + S_{r2}^{\\top} R^{-1} S_{r2}) z - (P^{-1}\\bar{x}_{0})^{\\top} S_{x0} z\n$$\nFrom this, we identify the Hessian matrix $H$ and the linear term vector $g$:\n$$\nH = S_{x0}^{\\top} P^{-1} S_{x0} + S_{w0}^{\\top} Q^{-1} S_{w0} + S_{w1}^{\\top} Q^{-1} S_{w1} + S_{r1}^{\\top} R^{-1} S_{r1} + S_{r2}^{\\top} R^{-1} S_{r2}\n$$\nThis is a block-diagonal matrix:\n$$\nH = \\text{diag}(P^{-1}, 0_{n \\times n}, 0_{n \\times n}, Q^{-1}, Q^{-1}, R^{-1}, R^{-1})\n$$\nThe linear term vector is:\n$$\ng = -S_{x0}^{\\top} P^{-1} \\bar{x}_{0}\n$$\nThis vector has only one non-zero block: $g = \\begin{pmatrix} -P^{-1}\\bar{x}_0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$. Since $P, Q, R$ are positive definite, so are their inverses. Thus $H$ is positive semidefinite, and the objective function is convex.\n\n**Constraints**\n\nThe problem statement uses the symbol $A$ for both the state dynamics matrix and the QP equality constraint matrix. To avoid ambiguity, we let $A_{sys}$ denote the state dynamics matrix from the model $x_{k+1} = A_{sys} x_k + \\dots$.\n\n**Equality Constraints $A z = b$**:\nThe equality constraints are given by the system dynamics and residual definitions:\n$x_{1} - A_{sys} x_{0} - w_{0} = B u_{0}$\n$x_{2} - A_{sys} x_{1} - w_{1} = B u_{1}$\n$C x_{1} + r_{1} = y_{1}$\n$C x_{2} + r_{2} = y_{2}$\n\nWe assemble these into the matrix form $Az=b$, where $A$ is the QP equality constraint matrix:\n$$\nA = \\begin{pmatrix}\n-A_{sys}  I_n  0  -I_n  0  0  0 \\\\\n0  -A_{sys}  I_n  0  -I_n  0  0 \\\\\n0  C  0  0  0  I_p  0 \\\\\n0  0  C  0  0  0  I_p\n\\end{pmatrix}\n$$\nThe right-hand side vector $b$ consists of the known inputs and measurements:\n$$\nb = \\begin{pmatrix} B u_0 \\\\ B u_1 \\\\ y_1 \\\\ y_2 \\end{pmatrix}\n$$\n\n**Inequality Constraints $G z \\le h$**:\nThe state box constraints are $\\ell \\le x_{k} \\le u$ for $k \\in \\{1,2\\}$, which is equivalent to the pair of linear inequalities $x_k \\le u$ and $-x_k \\le -\\ell$. Using selection matrices, we have:\n$S_{x1} z \\le u$\n$-S_{x1} z \\le -\\ell$\n$S_{x2} z \\le u$\n$-S_{x2} z \\le -\\ell$\n\nStacking these gives the matrix $G$ and vector $h$:\n$$\nG = \\begin{pmatrix} S_{x1} \\\\ -S_{x1} \\\\ S_{x2} \\\\ -S_{x2} \\end{pmatrix} = \\begin{pmatrix}\n0  I_n  0  0  \\dots  0 \\\\\n0  -I_n  0  0  \\dots  0 \\\\\n0  0  I_n  0  \\dots  0 \\\\\n0  0  -I_n  0  \\dots  0 \\\\\n\\end{pmatrix}\n\\qquad\nh = \\begin{pmatrix} u \\\\ -\\ell \\\\ u \\\\ -\\ell \\end{pmatrix}\n$$\n\n### 2. Karush-Kuhn-Tucker (KKT) Conditions\n\nThe Lagrangian for the convex QP is constructed by associating dual variables (Lagrange multipliers) $\\lambda$ with the equality constraints and $\\mu$ with the inequality constraints.\n$$\n\\mathcal{L}(z, \\lambda, \\mu) = \\frac{1}{2} z^{\\top} H z + g^{\\top} z + \\lambda^{\\top} (A z - b) + \\mu^{\\top} (G z - h)\n$$\nThe necessary and sufficient conditions for a point $z^*$ to be an optimal solution are the KKT conditions, as the problem is convex. Let $(z^*, \\lambda^*, \\mu^*)$ be an optimal primal-dual triplet.\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variable $z$ must be zero at the optimum.\n    $$\n    \\nabla_z \\mathcal{L}(z^*, \\lambda^*, \\mu^*) = H z^* + g + A^{\\top} \\lambda^* + G^{\\top} \\mu^* = 0\n    $$\n\n2.  **Primal Feasibility**: The solution $z^*$ must satisfy all original constraints.\n    $$\n    A z^* = b\n    $$\n    $$\n    G z^* \\le h\n    $$\n\n3.  **Dual Feasibility**: The dual variables corresponding to the inequality constraints must be non-negative.\n    $$\n    \\mu^* \\ge 0\n    $$\n\n4.  **Complementary Slackness**: For each inequality constraint, either the constraint is active (holds with equality) or its corresponding dual variable is zero.\n    $$\n    \\mu_i^* (G_i z^* - h_i) = 0 \\quad \\text{for all } i\n    $$\n    where $G_i$ is the $i$-th row of $G$ and $h_i$ is the $i$-th component of $h$.\n\n### 3. Interpretation of Dual Variables\n\nThe dual variables $\\lambda$ and $\\mu$ have an economic interpretation as shadow prices, quantifying the sensitivity of the optimal cost to perturbations in the constraints. Let $p^*(b,h)$ be the optimal value of the QP as a function of the right-hand sides of the constraints. Standard sensitivity analysis results for convex optimization show that for small perturbations, the change in the optimal value is given by:\n$$\ndp^* = -(\\lambda^*)^{\\top} db - (\\mu^*)^{\\top} dh\n$$\nThis implies $\\nabla_b p^*(b,h) = -\\lambda^*$ and $\\nabla_h p^*(b,h) = -\\mu^*$. The sign convention is a direct consequence of defining the Lagrangian as $f + \\text{multipliers}^{\\top} \\times \\text{constraints}$.\n\n-   **Interpretation of $\\lambda$**: The dual variable $\\lambda_j^*$ associated with the $j$-th equality constraint, $(Az-b)_j = 0$, represents the marginal change in the optimal cost for an infinitesimal change in $b_j$. Specifically, increasing $b_j$ by a small amount $\\epsilon > 0$ will change the optimal cost by approximately $-\\lambda_j^* \\epsilon$. Therefore, $-\\lambda_j^*$ is the shadow price for the resource represented by $b_j$. A positive value of $-\\lambda_j^*$ indicates that increasing $b_j$ would increase the final cost.\n\n-   **Interpretation of $\\mu$**: The dual variable $\\mu_i^*$ associated with the $i$-th inequality constraint, $(Gz-h)_i \\le 0$, represents the marginal change in the optimal cost for a change in $h_i$. Increasing $h_i$ by a small amount $\\epsilon > 0$ means relaxing the constraint (e.g., increasing the upper bound $u$). This will change the optimal cost by approximately $-\\mu_i^* \\epsilon$. Since $\\mu_i^* \\ge 0$, relaxing a constraint can only decrease or hold constant the optimal cost. If a constraint is inactive ($(G_i z^* - h_i)  0$), its shadow price $\\mu_i^*$ is zero, meaning small changes to $h_i$ have no effect on the optimal cost. If a constraint is active and $\\mu_i^* > 0$, relaxing it (increasing $h_i$) will strictly decrease the optimal cost. Thus, $\\mu_i^*$ represents the \"cost\" of the tightness of the $i$-th inequality constraint.\n\n### Final KKT System\n\nThe problem asks for a single matrix equation for the stationarity and primal feasibility (equality) conditions. These two conditions form a system of linear equations in the variables $(z, \\lambda, \\mu)$, which is a core component of many QP solution algorithms.\n$$\nH z + A^{\\top} \\lambda + G^{\\top} \\mu = -g \\quad (\\text{Stationarity})\n$$\n$$\nA z = b \\quad (\\text{Primal Feasibility})\n$$\nThese can be combined into a single block matrix equation, representing the linear part of the KKT saddle-point system.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nH  A^{\\top}  G^{\\top} \\\\\nA  0  0\n\\end{pmatrix}\n\\begin{pmatrix}\nz \\\\\n\\lambda \\\\\n\\mu\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-g \\\\\nb\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This final practice moves from theory to implementation, addressing the challenge of hybrid systems that combine continuous dynamics and discrete logic. You will implement a complete MHE for a power inverter, a common cyber-physical system, where the estimator must jointly determine the continuous current and the discrete switching modes. This exercise () integrates system modeling, optimization, and algorithm design into a practical coding challenge.",
            "id": "4232050",
            "problem": "Consider a digital twin (DT) of a single-phase grid-tied inverter in the broader context of Cyber-Physical Systems (CPS). The DT must perform Moving Horizon Estimation (MHE) to estimate the inductor current state while accounting for hybrid switching that induces mode-dependent measurement behavior. The inverter output stage is modeled as an $L$-$R$ filter driven by a direct-current (DC) voltage source and connected to an alternating-current (AC) grid that is well approximated as a Thevenin source over a short horizon.\n\nStart from fundamental electrical laws. The inductor obeys $v_L = L \\, \\frac{di}{dt}$, and by Kirchhoff's Voltage Law, $u - R \\, i - v_g - v_L = 0$, where $u$ is the inverter-side applied voltage, $R$ is the series resistance, $L$ is the inductance, $i$ is the inductor current, and $v_g$ is the grid voltage. Assume pulse-width modulation produces an average applied voltage $u = s \\, V_{\\mathrm{dc}}$, where $s \\in \\{-1, 0, +1\\}$ is the discrete switching mode and $V_{\\mathrm{dc}}$ is the DC bus voltage. Discretize the current dynamics with a forward Euler method with step $dt$ to obtain a discrete-time model suitable for MHE.\n\nLet the state be $x_k = i_k$ at discrete time $k$. Assume the DT receives measurements $y_k$ that depend on the switching mode via a mode-dependent sensor scaling and bias, so that $y_k = h_{s_k} \\, x_k + c_{s_k} + v_k$, where $h_{s_k}$ and $c_{s_k}$ are the measurement scale and bias determined by the current mode $s_k$, and $v_k$ is additive measurement noise.\n\nAssume additive process noise $w_k$ in the dynamics and additive measurement noise $v_k$ are independent, zero-mean Gaussian with known variances. The MHE must operate over a horizon of length $N$ with an arrival prior $x_0 \\sim \\mathcal{N}(\\bar{x}_0, \\sigma_P^2)$ to regularize the initial state. The objective is to jointly estimate the state trajectory $x_0, x_1, \\dots, x_N$ and the discrete mode sequence $s_0, s_1, \\dots, s_{N-1}$ by minimizing the negative log-likelihood implied by the Gaussian assumptions subject to the discretized dynamics and measurement models.\n\nFormally, derive the discrete dynamics from the laws above and assemble the MHE cost from first principles, then implement an algorithm that:\n- Enumerates all candidate hybrid mode sequences $\\{s_k\\}_{k=0}^{N-1}$ with $s_k \\in \\{-1, 0, +1\\}$ over the horizon.\n- For each candidate mode sequence, solves the weighted least-squares problem in the continuous variables $\\{x_k\\}_{k=0}^{N}$ that balances:\n  - The arrival regularization at $k=0$ using the prior.\n  - The process residuals defined by the discretized dynamics.\n  - The measurement residuals defined by the mode-dependent measurement model.\n- Selects the mode sequence and state trajectory that minimize the total cost.\n\nYour program must generate synthetic data for each test case by simulating the true dynamics for a specified true mode sequence and adding measurement noise. You may assume process noise is negligible in the data generation over the short horizon but must include the process residual weighting in the estimator. Use units consistently:\n- Inductance $L$ in henries (H), resistance $R$ in ohms ($\\Omega$), voltages $V_{\\mathrm{dc}}$ and $v_g$ in volts (V), time step $dt$ in seconds (s), and current $i$ in amperes (A).\n- Express the final current estimation error in amperes (A). No angle quantities are required.\n- All error outputs must be floats; mode misclassifications must be integers.\n\nDiscrete-time model to derive and use in the estimator:\n- Begin from $L \\, \\frac{di}{dt} = u - R \\, i - v_g$ with $u = s \\, V_{\\mathrm{dc}}$.\n- Use forward Euler with step $dt$ to obtain $x_{k+1}$ in terms of $x_k$, $s_k$, $V_{\\mathrm{dc}}$, $v_{g,k}$, $R$, and $L$.\n\nMeasurement model to use in the estimator:\n- $y_k = h_{s_k} \\, x_k + c_{s_k} + v_k$ with known $h_{s}$ and $c_s$ for each $s \\in \\{-1, 0, +1\\}$.\n\nNoise assumptions for the estimator:\n- $w_k \\sim \\mathcal{N}(0, \\sigma_Q^2)$ and $v_k \\sim \\mathcal{N}(0, \\sigma_R^2)$, with weights in the least-squares cost proportional to $1/\\sigma_Q^2$ and $1/\\sigma_R^2$, respectively.\n- Arrival prior $x_0 \\sim \\mathcal{N}(\\bar{x}_0, \\sigma_P^2)$ contributes a weight proportional to $1/\\sigma_P^2$.\n\nTest Suite:\nImplement the MHE and evaluate it on the following three test cases. For each case, generate synthetic measurements $y_k$ using the true mode sequence, simulate the true current using the discretized dynamics without process noise, and add independent Gaussian measurement noise with the specified standard deviation.\n\n- Case 1 (happy path):\n  - $L = 3 \\times 10^{-3}$ H, $R = 0.4$ $\\Omega$, $V_{\\mathrm{dc}} = 400$ V, $dt = 1 \\times 10^{-4}$ s, $N = 5$.\n  - Grid voltage $v_{g,k} = 170$ V for all $k$.\n  - True initial state $x_0^{\\mathrm{true}} = 5$ A; arrival prior mean $\\bar{x}_0 = 0$ A; arrival standard deviation $\\sigma_P = 3$ A.\n  - True mode sequence $(s_k)_{k=0}^{4} = [ +1, -1, +1, -1, +1 ]$.\n  - Measurement parameters: $h_{+1} = 1.0$, $h_{0} = 0.7$, $h_{-1} = 0.9$; $c_{+1} = 0.02$ A, $c_{0} = 0.0$ A, $c_{-1} = -0.02$ A.\n  - Estimator weights: $\\sigma_Q = 0.5$ A, $\\sigma_R = 0.05$ A, $\\sigma_P = 3$ A.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 0.05$ A.\n\n- Case 2 (includes zero-conduction mode and mild noise):\n  - $L = 2 \\times 10^{-3}$ H, $R = 0.3$ $\\Omega$, $V_{\\mathrm{dc}} = 350$ V, $dt = 1 \\times 10^{-4}$ s, $N = 5$.\n  - Grid voltage $v_{g,k} = 170$ V for all $k$.\n  - True initial state $x_0^{\\mathrm{true}} = 4$ A; arrival prior mean $\\bar{x}_0 = 6$ A; arrival standard deviation $\\sigma_P = 2$ A.\n  - True mode sequence $(s_k)_{k=0}^{4} = [ +1, 0, 0, -1, +1 ]$.\n  - Measurement parameters: $h_{+1} = 1.0$, $h_{0} = 0.7$, $h_{-1} = 0.9$; $c_{+1} = 0.02$ A, $c_{0} = 0.0$ A, $c_{-1} = -0.02$ A.\n  - Estimator weights: $\\sigma_Q = 0.5$ A, $\\sigma_R = 0.02$ A, $\\sigma_P = 2$ A.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 0.02$ A.\n\n- Case 3 (poor prior and low noise):\n  - $L = 4 \\times 10^{-3}$ H, $R = 0.5$ $\\Omega$, $V_{\\mathrm{dc}} = 450$ V, $dt = 2 \\times 10^{-4}$ s, $N = 5$.\n  - Grid voltage $v_{g,k} = 170$ V for all $k$.\n  - True initial state $x_0^{\\mathrm{true}} = 2$ A; arrival prior mean $\\bar{x}_0 = 8$ A; arrival standard deviation $\\sigma_P = 4$ A.\n  - True mode sequence $(s_k)_{k=0}^{4} = [ -1, -1, +1, +1, -1 ]$.\n  - Measurement parameters: $h_{+1} = 1.0$, $h_{0} = 0.7$, $h_{-1} = 0.9$; $c_{+1} = 0.02$ A, $c_{0} = 0.0$ A, $c_{-1} = -0.02$ A.\n  - Estimator weights: $\\sigma_Q = 0.5$ A, $\\sigma_R = 0.01$ A, $\\sigma_P = 4$ A.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 0.01$ A.\n\nYour program must:\n- Implement the estimator as described and select the mode sequence and state trajectory minimizing the cost over the horizon.\n- For each case, compute and return two quantities:\n  1. The root-mean-square (RMS) current estimation error over the entire horizon, including $k=0$ to $k=N$, in amperes (A), as a float.\n  2. The total number of mode misclassifications over the horizon, comparing the estimated discrete mode sequence to the true sequence, as an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n- Aggregate the results for the three cases in order and flatten them as \"[\\mathrm{err}_1,\\mathrm{mis}_1,\\mathrm{err}_2,\\mathrm{mis}_2,\\mathrm{err}_3,\\mathrm{mis}_3]\".",
            "solution": "The posed problem is valid as it is scientifically grounded in electrical engineering and control theory, well-posed, objective, and contains all necessary information for a unique solution. We are tasked with designing a Moving Horizon Estimator (MHE) for a hybrid system, specifically a grid-tied inverter, to jointly estimate the continuous inductor current state and the discrete switching mode.\n\n### 1. Derivation of the Discrete-Time System Dynamics\n\nThe continuous-time model for the inductor current $i(t)$ in the $L$-$R$ filter is derived from Kirchhoff's Voltage Law:\n$$L \\frac{di}{dt} = u - R i - v_g$$\nThe inverter's output voltage $u$ is determined by the switching mode $s \\in \\{-1, 0, +1\\}$ and the DC bus voltage $V_{\\mathrm{dc}}$, such that $u = s V_{\\mathrm{dc}}$. Substituting this into the dynamic equation yields:\n$$ \\frac{di}{dt} = -\\frac{R}{L}i + \\frac{sV_{\\mathrm{dc}} - v_g}{L} $$\nTo formulate a model suitable for a discrete-time estimator, we discretize this differential equation using the forward Euler method with a time step $dt$. Let $x_k = i(k \\cdot dt)$ be the state at discrete time $k$. The derivative is approximated as $\\frac{di}{dt} \\approx \\frac{x_{k+1} - x_k}{dt}$.\n$$ \\frac{x_{k+1} - x_k}{dt} = -\\frac{R}{L}x_k + \\frac{s_k V_{\\mathrm{dc}} - v_{g,k}}{L} $$\nRearranging to solve for $x_{k+1}$ gives the discrete-time state-space model:\n$$ x_{k+1} = \\left(1 - \\frac{R \\cdot dt}{L}\\right) x_k + \\frac{dt}{L} (s_k V_{\\mathrm{dc}} - v_{g,k}) $$\nThis equation is of the form $x_{k+1} = f(x_k, s_k)$, where $s_k$ is the control input (mode) applied during the interval $[k \\cdot dt, (k+1) \\cdot dt)$. The estimator must account for process noise $w_k \\sim \\mathcal{N}(0, \\sigma_Q^2)$, leading to the stochastic model:\n$$ x_{k+1} = f(x_k, s_k) + w_k $$\n\n### 2. MHE Cost Function Formulation\n\nThe objective is to find the state trajectory $X = \\{x_0, x_1, \\dots, x_N\\}$ and the mode sequence $S = \\{s_0, s_1, \\dots, s_{N-1}\\}$ that best explain the available measurements over a horizon of length $N$. Under the assumption of independent, zero-mean Gaussian noise, the maximum likelihood estimate is found by minimizing a weighted sum of squared residuals, which is equivalent to minimizing the negative log-likelihood. The total cost function $J(X, S)$ is composed of three parts:\n\n1.  **Arrival Cost ($J_P$)**: This term incorporates prior knowledge about the initial state $x_0 \\sim \\mathcal{N}(\\bar{x}_0, \\sigma_P^2)$. It penalizes the deviation of the estimated initial state $x_0$ from its prior mean $\\bar{x}_0$.\n    $$ J_P(x_0) = \\frac{(x_0 - \\bar{x}_0)^2}{\\sigma_P^2} $$\n\n2.  **Process Cost ($J_Q$)**: This term penalizes the process residuals, which represent the discrepancy between the estimated state trajectory and the dynamics predicted by the model. The process residual at step $k$ is $w_k = x_{k+1} - f(x_k, s_k)$.\n    $$ J_Q(X, S) = \\sum_{k=0}^{N-1} \\frac{w_k^2}{\\sigma_Q^2} = \\sum_{k=0}^{N-1} \\frac{\\left(x_{k+1} - f(x_k, s_k)\\right)^2}{\\sigma_Q^2} $$\n\n3.  **Measurement Cost ($J_R$)**: This term penalizes the measurement residuals, which are the differences between the actual measurements $y_k$ and the values predicted by the measurement model $y_k = h_{s_k} x_k + c_{s_k} + v_k$, where $v_k \\sim \\mathcal{N}(0, \\sigma_R^2)$. We assume measurements $y_k$ are available for $k=0, \\dots, N-1$.\n    $$ J_R(X, S) = \\sum_{k=0}^{N-1} \\frac{(y_k - (h_{s_k} x_k + c_{s_k}))^2}{\\sigma_R^2} $$\n\nThe total MHE cost to be minimized is the sum of these components:\n$$ \\min_{X, S} J(X, S) = \\min_{X, S} \\left( J_P(x_0) + J_Q(X, S) + J_R(X, S) \\right) $$\n\n### 3. Solution Algorithm\n\nThis is a mixed-integer optimization problem due to the continuous states $X$ and discrete modes $S$. Since the horizon $N$ is small ($N=5$), we can solve this by enumerating all possible mode sequences. For $s_k \\in \\{-1, 0, +1\\}$, there are $3^N$ candidate sequences.\n\nFor each fixed candidate mode sequence $S$, the cost function $J(X | S)$ is a quadratic function of the state vector $X = [x_0, x_1, \\dots, x_N]^T$. Minimizing this quadratic cost is a linear least-squares problem. We can structure this in the standard form $\\min_X \\| \\mathbf{M}X - \\mathbf{d} \\|_2^2$.\n\nThe vector of variables is $X \\in \\mathbb{R}^{N+1}$. The system is constructed from the residuals:\n-   **Prior residual**: $\\sqrt{W_P}(x_0 - \\bar{x}_0)$, where $W_P = 1/\\sigma_P^2$.\n-   **Measurement residuals ($k=0..N-1$):** $\\sqrt{W_R}(y_k - (h_{s_k} x_k + c_{s_k}))$, where $W_R = 1/\\sigma_R^2$. This is rewritten as $\\sqrt{W_R}(h_{s_k} x_k - (y_k - c_{s_k}))$.\n-   **Process residuals ($k=0..N-1$):** $\\sqrt{W_Q}(x_{k+1} - f(x_k, s_k))$, where $W_Q = 1/\\sigma_Q^2$. Let $f(x_k, s_k)=A x_k + B_k$, where $A = (1 - \\frac{R \\cdot dt}{L})$ and $B_k = \\frac{dt}{L}(s_k V_{\\mathrm{dc}} - v_{g,k})$. The residual is $\\sqrt{W_Q}(x_{k+1} - A x_k - B_k)$.\n\nThese $1+N+N=2N+1$ residuals are stacked to form the linear system $\\mathbf{M}X \\approx \\mathbf{d}$, where $\\mathbf{M}$ is a $(2N+1) \\times (N+1)$ matrix and $\\mathbf{d}$ is a $(2N+1)$-dimensional vector. This system is solved for $X$ using a standard least-squares solver. The minimum cost for the sequence $S$ is the resulting sum of squared residuals.\n\nThe overall algorithm is as follows:\n1.  Initialize $\\text{min\\_cost} = \\infty$.\n2.  Generate all $3^N$ candidate mode sequences $S$.\n3.  For each sequence $S$:\n    a. Construct the matrix $\\mathbf{M}$ and vector $\\mathbf{d}$ corresponding to $S$.\n    b. Solve for the optimal state trajectory $X_S = \\arg\\min_X \\| \\mathbf{M}X - \\mathbf{d} \\|_2^2$.\n    c. Calculate the cost $J_S = \\| \\mathbf{M}X_S - \\mathbf{d} \\|_2^2$.\n    d. If $J_S  \\text{min\\_cost}$, update $\\text{min\\_cost} = J_S$, and store $S$ and $X_S$ as the best-so-far solution.\n4.  The final estimate is the state and mode pair $(X_S, S)$ that achieved the overall minimum cost.\n\n### 4. Data Synthesis and Evaluation\n\nFor each test case, synthetic data is generated. A true state trajectory $\\{x_k^{\\mathrm{true}}\\}_{k=0}^{N}$ is created by simulating the deterministic discrete-time model with the given true mode sequence. Then, measurement data $\\{y_k\\}_{k=0}^{N-1}$ is generated by applying the mode-dependent measurement model to the true states and adding independent Gaussian noise with the specified standard deviation $\\sigma_{\\mathrm{meas}}$. The estimator's performance is quantified by the RMS current estimation error over the full horizon $[0, N]$ and the total number of misclassified modes over $[0, N-1]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run MHE for each, and print results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of noise generation.\n    np.random.seed(42)\n\n    # Define the three test cases as specified in the problem.\n    test_cases = [\n        {\n            \"name\": \"Case 1 (happy path)\",\n            \"L\": 3e-3, \"R\": 0.4, \"Vdc\": 400.0, \"dt\": 1e-4, \"N\": 5,\n            \"vg\": 170.0,\n            \"x0_true\": 5.0, \"x0_prior_mean\": 0.0,\n            \"s_true\": [+1, -1, +1, -1, +1],\n            \"h_map\": {+1: 1.0, 0: 0.7, -1: 0.9},\n            \"c_map\": {+1: 0.02, 0: 0.0, -1: -0.02},\n            \"sigma_P\": 3.0, \"sigma_Q\": 0.5, \"sigma_R\": 0.05,\n            \"sigma_meas\": 0.05,\n        },\n        {\n            \"name\": \"Case 2 (includes zero-conduction mode)\",\n            \"L\": 2e-3, \"R\": 0.3, \"Vdc\": 350.0, \"dt\": 1e-4, \"N\": 5,\n            \"vg\": 170.0,\n            \"x0_true\": 4.0, \"x0_prior_mean\": 6.0,\n            \"s_true\": [+1, 0, 0, -1, +1],\n            \"h_map\": {+1: 1.0, 0: 0.7, -1: 0.9},\n            \"c_map\": {+1: 0.02, 0: 0.0, -1: -0.02},\n            \"sigma_P\": 2.0, \"sigma_Q\": 0.5, \"sigma_R\": 0.02,\n            \"sigma_meas\": 0.02,\n        },\n        {\n            \"name\": \"Case 3 (poor prior and low noise)\",\n            \"L\": 4e-3, \"R\": 0.5, \"Vdc\": 450.0, \"dt\": 2e-4, \"N\": 5,\n            \"vg\": 170.0,\n            \"x0_true\": 2.0, \"x0_prior_mean\": 8.0,\n            \"s_true\": [-1, -1, +1, +1, -1],\n            \"h_map\": {+1: 1.0, 0: 0.7, -1: 0.9},\n            \"c_map\": {+1: 0.02, 0: 0.0, -1: -0.02},\n            \"sigma_P\": 4.0, \"sigma_Q\": 0.5, \"sigma_R\": 0.01,\n            \"sigma_meas\": 0.01,\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        rms_error, misclassifications = run_mhe_for_case(params)\n        results.extend([rms_error, misclassifications])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_mhe_for_case(params):\n    \"\"\"\n    Solves the MHE problem for a single test case.\n    \n    Args:\n        params (dict): A dictionary containing all parameters for the case.\n        \n    Returns:\n        tuple: A tuple containing (rms_error, misclassifications).\n    \"\"\"\n    # Unpack parameters\n    L, R, Vdc, dt, N = params['L'], params['R'], params['Vdc'], params['dt'], params['N']\n    vg = params['vg']\n    x0_true = params['x0_true']\n    x0_prior_mean = params['x0_prior_mean']\n    sigma_P, sigma_Q, sigma_R = params['sigma_P'], params['sigma_Q'], params['sigma_R']\n    s_true = params['s_true']\n    h_map, c_map = params['h_map'], params['c_map']\n    sigma_meas = params['sigma_meas']\n\n    # --- 1. Generate True Data and Synthetic Measurements ---\n    \n    # Dynamics matrix A and constant for discretization\n    A_dyn = 1.0 - (R * dt) / L\n    \n    # Generate true state trajectory (without process noise)\n    x_true = np.zeros(N + 1)\n    x_true[0] = x0_true\n    for k in range(N):\n        s_k = s_true[k]\n        B_dyn_k = (dt / L) * (s_k * Vdc - vg)\n        x_true[k + 1] = A_dyn * x_true[k] + B_dyn_k\n\n    # Generate synthetic measurements y_0, ..., y_{N-1}\n    y_meas = np.zeros(N)\n    for k in range(N):\n        s_k = s_true[k]\n        noise = np.random.normal(0, sigma_meas)\n        y_meas[k] = h_map[s_k] * x_true[k] + c_map[s_k] + noise\n\n    # --- 2. MHE Implementation ---\n    \n    modes = [-1, 0, 1]\n    mode_sequences = list(itertools.product(modes, repeat=N))\n\n    min_cost = float('inf')\n    best_x_est, best_s_est = None, None\n\n    # Pre-calculate weights for the cost function\n    W_P = 1.0 / (sigma_P**2)\n    W_Q = 1.0 / (sigma_Q**2)\n    W_R = 1.0 / (sigma_R**2)\n\n    # Least squares matrix dimensions\n    num_states = N + 1\n    num_residuals = 1 + N + N  # 1 prior, N dynamics, N measurements\n    \n    for s_candidate in mode_sequences:\n        # Construct the least-squares problem M*x = d for the current s_candidate\n        M = np.zeros((num_residuals, num_states))\n        d = np.zeros(num_residuals)\n        \n        row = 0\n\n        # Row for arrival cost (prior)\n        M[row, 0] = np.sqrt(W_P)\n        d[row] = np.sqrt(W_P) * x0_prior_mean\n        row += 1\n\n        # Rows for measurement cost\n        for k in range(N):\n            s_k = s_candidate[k]\n            h_k, c_k = h_map[s_k], c_map[s_k]\n            M[row, k] = np.sqrt(W_R) * h_k\n            d[row] = np.sqrt(W_R) * (y_meas[k] - c_k)\n            row += 1\n\n        # Rows for process cost (dynamics)\n        for k in range(N):\n            s_k = s_candidate[k]\n            B_dyn_k = (dt / L) * (s_k * Vdc - vg)\n            M[row, k] = -np.sqrt(W_Q) * A_dyn\n            M[row, k + 1] = np.sqrt(W_Q)\n            d[row] = np.sqrt(W_Q) * B_dyn_k\n            row += 1\n\n        # Solve the linear least squares problem\n        x_est, residuals, _, _ = np.linalg.lstsq(M, d, rcond=None)\n        \n        cost = residuals[0] if residuals.size > 0 else np.sum((M @ x_est - d)**2)\n\n        if cost  min_cost:\n            min_cost = cost\n            best_x_est = x_est\n            best_s_est = s_candidate\n    \n    # --- 3. Compute Output Metrics ---\n    \n    # Root-mean-square (RMS) current estimation error\n    rms_error = np.sqrt(np.mean((best_x_est - x_true)**2))\n    \n    # Total number of mode misclassifications\n    misclassifications = np.sum(np.array(best_s_est) != np.array(s_true))\n    \n    return rms_error, int(misclassifications)\n\nsolve()\n```"
        }
    ]
}