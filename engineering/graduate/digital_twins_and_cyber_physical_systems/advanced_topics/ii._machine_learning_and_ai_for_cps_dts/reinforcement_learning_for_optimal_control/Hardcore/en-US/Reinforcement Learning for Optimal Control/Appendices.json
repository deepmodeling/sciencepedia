{
    "hands_on_practices": [
        {
            "introduction": "The foundation of optimal control in a known environment is dynamic programming. This exercise guides you through a single step of value iteration, a cornerstone algorithm for solving Markov Decision Processes (MDPs). By applying the Bellman optimality equation, you will compute an updated estimate of the optimal value function, gaining a direct, hands-on understanding of how an agent can computationally determine the best long-term strategy when the system's dynamics are fully specified.",
            "id": "4239976",
            "problem": "Consider a cyber-physical system (CPS) whose Digital Twin provides a calibrated finite-state predictive model for asset health, represented as a discounted infinite-horizon Markov Decision Process (MDP). The MDP has three states: $s_{\\mathrm{N}}$ (nominal operation), $s_{\\mathrm{D}}$ (degraded operation), and $s_{\\mathrm{S}}$ (shutdown). The action set is $\\mathcal{A}=\\{a_{\\mathrm{O}}, a_{\\mathrm{M}}, a_{\\mathrm{Sh}}\\}$, denoting operate, maintenance, and shutdown, respectively. The immediate reward function $r(s,a)$ is as follows:\n- $r(s_{\\mathrm{N}}, a_{\\mathrm{O}})=+5.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{M}})=-1.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=-0.2$.\n- $r(s_{\\mathrm{D}}, a_{\\mathrm{O}})=+1.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{M}})=-0.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=-0.3$.\n- $r(s_{\\mathrm{S}}, a_{\\mathrm{O}})=-3.0$, $r(s_{\\mathrm{S}}, a_{\\mathrm{M}})=-0.2$, $r(s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=-0.1$.\n\nThe state transition probabilities $P(s' \\mid s,a)$, informed by the Digital Twinâ€™s one-step-ahead health forecasts, are:\n- From $s_{\\mathrm{N}}$:\n  - Under $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.85$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.05$.\n  - Under $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.95$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.04$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.01$.\n  - Under $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.90$.\n- From $s_{\\mathrm{D}}$:\n  - Under $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.30$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.50$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.20$.\n  - Under $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.60$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.35$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.05$.\n  - Under $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.90$.\n- From $s_{\\mathrm{S}}$:\n  - Under $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.60$.\n  - Under $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.40$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$.\n  - Under $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=1.00$.\n\nThe discount factor is $\\gamma=0.92$. The initial value function is $V_{0}(s_{\\mathrm{N}})=8$, $V_{0}(s_{\\mathrm{D}})=3$, $V_{0}(s_{\\mathrm{S}})=-1$.\n\nUsing the rigorous foundations of optimal control in Markov Decision Processes (MDPs) and the Bellman optimality principle applied in value iteration, perform one iteration of value iteration to compute $V_{1} = \\mathcal{T}V_{0}$. Express your final answer as the row vector $\\begin{pmatrix}V_{1}(s_{\\mathrm{N}}) & V_{1}(s_{\\mathrm{D}}) & V_{1}(s_{\\mathrm{S}})\\end{pmatrix}$ using the $\\mathrm{pmatrix}$ environment. Do not round; provide exact decimal values as implied by the data.",
            "solution": "The user has requested the execution of one iteration of the value iteration algorithm for a given Markov Decision Process (MDP).\n\n**Problem Validation**\n\nFirst, I will validate the problem statement according to the required protocol.\n\n**Step 1: Extract Givens**\n- State space: $\\mathcal{S}=\\{s_{\\mathrm{N}}, s_{\\mathrm{D}}, s_{\\mathrm{S}}\\}$\n- Action space: $\\mathcal{A}=\\{a_{\\mathrm{O}}, a_{\\mathrm{M}}, a_{\\mathrm{Sh}}\\}$\n- Reward function $r(s,a)$:\n  - $r(s_{\\mathrm{N}}, a_{\\mathrm{O}})=+5.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{M}})=-1.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=-0.2$\n  - $r(s_{\\mathrm{D}}, a_{\\mathrm{O}})=+1.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{M}})=-0.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=-0.3$\n  - $r(s_{\\mathrm{S}}, a_{\\mathrm{O}})=-3.0$, $r(s_{\\mathrm{S}}, a_{\\mathrm{M}})=-0.2$, $r(s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=-0.1$\n- Transition probabilities $P(s' \\mid s,a)$:\n  - From $s_{\\mathrm{N}}$:\n    - $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.85$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.05$\n    - $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.95$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.04$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.01$\n    - $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.90$\n  - From $s_{\\mathrm{D}}$:\n    - $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.30$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.50$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.20$\n    - $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.60$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.35$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.05$\n    - $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.90$\n  - From $s_{\\mathrm{S}}$:\n    - $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.60$\n    - $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.40$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$\n    - $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=1.00$\n- Discount factor: $\\gamma=0.92$\n- Initial value function: $V_{0}(s_{\\mathrm{N}})=8$, $V_{0}(s_{\\mathrm{D}})=3$, $V_{0}(s_{\\mathrm{S}})=-1$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined within the framework of Markov Decision Processes, a standard tool for optimal control. It provides all necessary components: states, actions, transition probabilities, rewards, a discount factor, and an initial value function. The transition probabilities for each state-action pair correctly sum to $1$, ensuring mathematical consistency. For example, for $(s_{\\mathrm{N}}, a_{\\mathrm{O}})$, $0.85 + 0.10 + 0.05 = 1.00$. All other distributions are also valid. The problem is self-contained, objective, and scientifically grounded. It presents a standard computational exercise in reinforcement learning, containing no logical flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n**Solution**\n\nThe value iteration algorithm updates the value function $V_k(s)$ for all states $s \\in \\mathcal{S}$ to produce the next value function $V_{k+1}(s)$. The update rule is defined by the Bellman optimality operator, $\\mathcal{T}$, applied to $V_k$:\n$$ V_{k+1}(s) = (\\mathcal{T}V_k)(s) = \\max_{a \\in \\mathcal{A}} \\left\\{ r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V_k(s') \\right\\} $$\nWe are asked to compute $V_1(s)$ given $V_0(s)$, $\\gamma=0.92$, and the provided MDP parameters.\n\nLet's compute the action-value function $Q_1(s,a)$ for each state-action pair:\n$$ Q_1(s,a) = r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V_0(s') $$\nThe given initial value function is $V_0(s_{\\mathrm{N}})=8$, $V_0(s_{\\mathrm{D}})=3$, and $V_0(s_{\\mathrm{S}})=-1$.\n\n**1. Calculation for state $s_{\\mathrm{N}}$**\nWe compute $Q_1(s_{\\mathrm{N}}, a)$ for each action $a \\in \\mathcal{A}$:\n- For $a = a_{\\mathrm{O}}$:\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{O}}) = r(s_{\\mathrm{N}}, a_{\\mathrm{O}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{N}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{N}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{N}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{O}}) = 5.0 + 0.92 [(0.85)(8) + (0.10)(3) + (0.05)(-1)] = 5.0 + 0.92 [6.8 + 0.3 - 0.05] = 5.0 + 0.92(7.05) = 5.0 + 6.486 = 11.486$\n- For $a = a_{\\mathrm{M}}$:\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{M}}) = r(s_{\\mathrm{N}}, a_{\\mathrm{M}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{N}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{N}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{N}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{M}}) = -1.0 + 0.92 [(0.95)(8) + (0.04)(3) + (0.01)(-1)] = -1.0 + 0.92 [7.6 + 0.12 - 0.01] = -1.0 + 0.92(7.71) = -1.0 + 7.0932 = 6.0932$\n- For $a = a_{\\mathrm{Sh}}$:\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{Sh}}) = r(s_{\\mathrm{N}}, a_{\\mathrm{Sh}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{N}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{N}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{N}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{Sh}}) = -0.2 + 0.92 [(0.05)(8) + (0.05)(3) + (0.90)(-1)] = -0.2 + 0.92 [0.4 + 0.15 - 0.9] = -0.2 + 0.92(-0.35) = -0.2 - 0.322 = -0.522$\n\nThus, $V_1(s_{\\mathrm{N}}) = \\max\\{11.486, 6.0932, -0.522\\} = 11.486$.\n\n**2. Calculation for state $s_{\\mathrm{D}}$**\nWe compute $Q_1(s_{\\mathrm{D}}, a)$ for each action $a \\in \\mathcal{A}$:\n- For $a = a_{\\mathrm{O}}$:\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{O}}) = r(s_{\\mathrm{D}}, a_{\\mathrm{O}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{D}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{D}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{D}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{O}}) = 1.5 + 0.92 [(0.30)(8) + (0.50)(3) + (0.20)(-1)] = 1.5 + 0.92 [2.4 + 1.5 - 0.2] = 1.5 + 0.92(3.7) = 1.5 + 3.404 = 4.904$\n- For $a = a_{\\mathrm{M}}$:\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{M}}) = r(s_{\\mathrm{D}}, a_{\\mathrm{M}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{D}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{D}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{D}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{M}}) = -0.5 + 0.92 [(0.60)(8) + (0.35)(3) + (0.05)(-1)] = -0.5 + 0.92 [4.8 + 1.05 - 0.05] = -0.5 + 0.92(5.8) = -0.5 + 5.336 = 4.836$\n- For $a = a_{\\mathrm{Sh}}$:\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{Sh}}) = r(s_{\\mathrm{D}}, a_{\\mathrm{Sh}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{D}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{D}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{D}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{Sh}}) = -0.3 + 0.92 [(0.00)(8) + (0.10)(3) + (0.90)(-1)] = -0.3 + 0.92 [0.0 + 0.3 - 0.9] = -0.3 + 0.92(-0.6) = -0.3 - 0.552 = -0.852$\n\nThus, $V_1(s_{\\mathrm{D}}) = \\max\\{4.904, 4.836, -0.852\\} = 4.904$.\n\n**3. Calculation for state $s_{\\mathrm{S}}$**\nWe compute $Q_1(s_{\\mathrm{S}}, a)$ for each action $a \\in \\mathcal{A}$:\n- For $a = a_{\\mathrm{O}}$:\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{O}}) = r(s_{\\mathrm{S}}, a_{\\mathrm{O}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{S}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{S}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{S}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{O}}) = -3.0 + 0.92 [(0.20)(8) + (0.20)(3) + (0.60)(-1)] = -3.0 + 0.92 [1.6 + 0.6 - 0.6] = -3.0 + 0.92(1.6) = -3.0 + 1.472 = -1.528$\n- For $a = a_{\\mathrm{M}}$:\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{M}}) = r(s_{\\mathrm{S}}, a_{\\mathrm{M}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{S}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{S}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{S}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{M}}) = -0.2 + 0.92 [(0.40)(8) + (0.30)(3) + (0.30)(-1)] = -0.2 + 0.92 [3.2 + 0.9 - 0.3] = -0.2 + 0.92(3.8) = -0.2 + 3.496 = 3.296$\n- For $a = a_{\\mathrm{Sh}}$:\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{Sh}}) = r(s_{\\mathrm{S}}, a_{\\mathrm{Sh}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{S}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{S}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{S}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{Sh}}) = -0.1 + 0.92 [(0.00)(8) + (0.00)(3) + (1.00)(-1)] = -0.1 + 0.92(-1.0) = -0.1 - 0.92 = -1.02$\n\nThus, $V_1(s_{\\mathrm{S}}) = \\max\\{-1.528, 3.296, -1.02\\} = 3.296$.\n\nThe resulting value function after one iteration is the vector $(V_1(s_{\\mathrm{N}}), V_1(s_{\\mathrm{D}}), V_1(s_{\\mathrm{S}}))$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11.486 & 4.904 & 3.296\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While dynamic programming is powerful, real-world agents often learn from sampled experience rather than a complete model. This practice explores a fundamental trade-off at the heart of reinforcement learning: the choice between Monte Carlo (MC) and Temporal Difference (TD) estimation. By deriving and comparing the variance of these two methods in a simplified setting, you will uncover the statistical reasons why TD's bootstrapping approach is central to reducing uncertainty and enabling efficient learning in complex systems.",
            "id": "4239983",
            "problem": "A digital twin (DT) of a cyber-physical system (CPS) models a deterministic operational chain with states $s_{0} \\rightarrow s_{1} \\rightarrow \\cdots \\rightarrow s_{H}$, where $s_{H}$ is terminal. A fixed policy induces these deterministic transitions until termination at horizon $H$. At each nonterminal step $t \\in \\{0,1,\\ldots,H-1\\}$, the DT emits a scalar reward $r_{t} = \\bar{r}_{t} + \\varepsilon_{t}$, where $\\bar{r}_{t}$ is a known deterministic baseline and $\\varepsilon_{t}$ are independent, zero-mean, identically distributed Gaussian random variables with variance $\\sigma^{2}$. Consider a finite-horizon discounted return with discount factor $\\gamma \\in [0,1)$.\n\nUsing the standard Reinforcement Learning (RL) definitions, the Monte Carlo (MC) return from $s_{0}$ is $G^{\\mathrm{MC}} = \\sum_{t=0}^{H-1} \\gamma^{t} r_{t}$. The Temporal Difference (TD) target at $t=0$ with perfect bootstrapping uses the true value function $V^{\\pi}$ under the fixed policy, namely $T^{\\mathrm{TD}} = r_{0} + \\gamma V^{\\pi}(s_{1})$, where $V^{\\pi}(s_{1})$ is deterministic given the DT baseline sequence and the horizon. Assume all randomness arises only from the measurement noises $\\{\\varepsilon_{t}\\}$.\n\nStarting only from the core definitions of expectation, variance, independence, and the deterministic nature of $V^{\\pi}(s_{1})$ under the stated assumptions, derive explicit expressions for $\\mathrm{Var}[G^{\\mathrm{MC}}]$ and $\\mathrm{Var}[T^{\\mathrm{TD}}]$ for the finite horizon $H$. Then simplify the ratio\n$$\n\\rho(H,\\gamma) \\equiv \\frac{\\mathrm{Var}[G^{\\mathrm{MC}}]}{\\mathrm{Var}[T^{\\mathrm{TD}}]}\n$$\nto a closed-form analytic expression depending only on $H$ and $\\gamma$. Report this final ratio as your answer. No units are required. Do not approximate; provide an exact closed form.",
            "solution": "The problem requires the derivation of a closed-form expression for the ratio of the variance of the Monte Carlo return, $\\mathrm{Var}[G^{\\mathrm{MC}}]$, to the variance of the one-step Temporal Difference target, $\\mathrm{Var}[T^{\\mathrm{TD}}]$. We will proceed by first deriving an expression for each variance based on the fundamental principles and definitions provided.\n\nFirst, we analyze the Monte Carlo return, $G^{\\mathrm{MC}}$. By definition, for a finite horizon $H$, it is given by:\n$$\nG^{\\mathrm{MC}} = \\sum_{t=0}^{H-1} \\gamma^{t} r_{t}\n$$\nThe reward at each step $t$ is $r_{t} = \\bar{r}_{t} + \\varepsilon_{t}$, where $\\bar{r}_{t}$ is a deterministic baseline and $\\varepsilon_{t}$ is a random noise term with $\\mathbb{E}[\\varepsilon_{t}] = 0$ and $\\mathrm{Var}[\\varepsilon_{t}] = \\sigma^{2}$. The noise terms $\\{\\varepsilon_{t}\\}$ are independent and identically distributed.\n\nWe can substitute the expression for $r_t$ into the definition of $G^{\\mathrm{MC}}$:\n$$\nG^{\\mathrm{MC}} = \\sum_{t=0}^{H-1} \\gamma^{t} (\\bar{r}_{t} + \\varepsilon_{t}) = \\sum_{t=0}^{H-1} \\gamma^{t} \\bar{r}_{t} + \\sum_{t=0}^{H-1} \\gamma^{t} \\varepsilon_{t}\n$$\nThe first term, $\\sum_{t=0}^{H-1} \\gamma^{t} \\bar{r}_{t}$, is a sum of deterministic quantities and is therefore a constant. The variance of a random variable is invariant to the addition of a constant, i.e., $\\mathrm{Var}[X+c] = \\mathrm{Var}[X]$. Thus, the variance of $G^{\\mathrm{MC}}$ is determined solely by the random part:\n$$\n\\mathrm{Var}[G^{\\mathrm{MC}}] = \\mathrm{Var}\\left[\\sum_{t=0}^{H-1} \\gamma^{t} \\bar{r}_{t} + \\sum_{t=0}^{H-1} \\gamma^{t} \\varepsilon_{t}\\right] = \\mathrm{Var}\\left[\\sum_{t=0}^{H-1} \\gamma^{t} \\varepsilon_{t}\\right]\n$$\nBecause the random variables $\\varepsilon_{t}$ are independent for $t \\in \\{0, 1, \\ldots, H-1\\}$, the variance of their weighted sum is the weighted sum of their variances. Specifically, for independent random variables $X_i$ and constants $a_i$, $\\mathrm{Var}[\\sum_{i} a_{i} X_{i}] = \\sum_{i} a_{i}^{2} \\mathrm{Var}[X_{i}]$. Applying this property:\n$$\n\\mathrm{Var}[G^{\\mathrm{MC}}] = \\sum_{t=0}^{H-1} \\mathrm{Var}[\\gamma^{t} \\varepsilon_{t}] = \\sum_{t=0}^{H-1} (\\gamma^{t})^{2} \\mathrm{Var}[\\varepsilon_{t}]\n$$\nSubstituting $\\mathrm{Var}[\\varepsilon_{t}]=\\sigma^{2}$, we get:\n$$\n\\mathrm{Var}[G^{\\mathrm{MC}}] = \\sum_{t=0}^{H-1} \\gamma^{2t} \\sigma^{2} = \\sigma^{2} \\sum_{t=0}^{H-1} (\\gamma^{2})^{t}\n$$\nThe summation is a finite geometric series with $H$ terms, a first term of $1$, and a common ratio of $\\gamma^{2}$. The sum is $\\frac{1 - (\\gamma^{2})^{H}}{1 - \\gamma^{2}} = \\frac{1 - \\gamma^{2H}}{1 - \\gamma^{2}}$. Therefore, the variance of the Monte Carlo return is:\n$$\n\\mathrm{Var}[G^{\\mathrm{MC}}] = \\sigma^{2} \\frac{1 - \\gamma^{2H}}{1 - \\gamma^{2}}\n$$\n\nNext, we analyze the Temporal Difference target, $T^{\\mathrm{TD}}$. It is defined as:\n$$\nT^{\\mathrm{TD}} = r_{0} + \\gamma V^{\\pi}(s_{1})\n$$\nThe problem states that $V^{\\pi}(s_{1})$ is the true value function and is deterministic. Let us verify this from first principles. The true value of a state $s_k$ under policy $\\pi$ is the expected discounted return starting from that state. For a finite horizon $H$, the value of state $s_1$ is:\n$$\nV^{\\pi}(s_{1}) = \\mathbb{E}\\left[\\sum_{t=1}^{H-1} \\gamma^{t-1} r_{t}\\right]\n$$\nSubstituting $r_t = \\bar{r}_t + \\varepsilon_t$:\n$$\nV^{\\pi}(s_{1}) = \\mathbb{E}\\left[\\sum_{t=1}^{H-1} \\gamma^{t-1} (\\bar{r}_{t} + \\varepsilon_{t})\\right]\n$$\nBy linearity of expectation:\n$$\nV^{\\pi}(s_{1}) = \\sum_{t=1}^{H-1} \\gamma^{t-1} \\mathbb{E}[\\bar{r}_{t} + \\varepsilon_{t}] = \\sum_{t=1}^{H-1} \\gamma^{t-1} (\\bar{r}_{t} + \\mathbb{E}[\\varepsilon_{t}])\n$$\nSince $\\mathbb{E}[\\varepsilon_{t}] = 0$, we are left with:\n$$\nV^{\\pi}(s_{1}) = \\sum_{t=1}^{H-1} \\gamma^{t-1} \\bar{r}_{t}\n$$\nThis expression depends only on the known deterministic baselines $\\bar{r}_t$ and the discount factor $\\gamma$. Thus, $V^{\\pi}(s_{1})$ is indeed a deterministic constant, as stipulated.\n\nNow we can compute the variance of $T^{\\mathrm{TD}}$. We substitute $r_0 = \\bar{r}_0 + \\varepsilon_0$ and the deterministic nature of $V^{\\pi}(s_1)$:\n$$\nT^{\\mathrm{TD}} = (\\bar{r}_{0} + \\varepsilon_{0}) + \\gamma V^{\\pi}(s_{1})\n$$\nThe terms $\\bar{r}_{0}$ and $\\gamma V^{\\pi}(s_{1})$ are both deterministic constants. The entire expression can be written as $\\varepsilon_{0}$ plus a constant term $C = \\bar{r}_{0} + \\gamma V^{\\pi}(s_{1})$. The variance of $T^{\\mathrm{TD}}$ is:\n$$\n\\mathrm{Var}[T^{\\mathrm{TD}}] = \\mathrm{Var}[\\varepsilon_{0} + C] = \\mathrm{Var}[\\varepsilon_{0}]\n$$\nWe are given that the variance of each noise term is $\\sigma^2$, so:\n$$\n\\mathrm{Var}[T^{\\mathrm{TD}}] = \\sigma^{2}\n$$\nIt is important to note that the randomness in $T^{\\mathrm{TD}}$ arises only from the noise in the immediate reward, $r_0$. The bootstrapped value $V^{\\pi}(s_1)$ is an expectation and contains no random variables from the specific trajectory realization.\n\nFinally, we compute the required ratio $\\rho(H,\\gamma)$:\n$$\n\\rho(H,\\gamma) \\equiv \\frac{\\mathrm{Var}[G^{\\mathrm{MC}}]}{\\mathrm{Var}[T^{\\mathrm{TD}}]} = \\frac{\\sigma^{2} \\frac{1 - \\gamma^{2H}}{1 - \\gamma^{2}}}{\\sigma^{2}}\n$$\nThe term $\\sigma^{2}$ cancels from the numerator and the denominator, yielding the final closed-form expression:\n$$\n\\rho(H,\\gamma) = \\frac{1 - \\gamma^{2H}}{1 - \\gamma^{2}}\n$$\nThis expression depends only on the horizon $H$ and the discount factor $\\gamma$, as required.",
            "answer": "$$\n\\boxed{\\frac{1 - \\gamma^{2H}}{1 - \\gamma^{2}}}\n$$"
        },
        {
            "introduction": "Building upon foundational principles, we now turn to a state-of-the-art algorithm used for complex control tasks. This exercise provides a look under the hood of Proximal Policy Optimization (PPO) by having you perform a single policy update step. You will work directly with the clipped surrogate objective, the key mechanism that allows PPO to achieve stable and reliable learning by carefully constraining policy changes, illustrating how modern algorithms balance exploration and exploitation in practice.",
            "id": "4240043",
            "problem": "A networked Digital Twin of a Cyber-Physical System (CPS) is used to synthesize a feedback controller for an underactuated robotic link via reinforcement learning for optimal control. The controller is a stochastic policy $\\pi_{\\theta}(a \\mid s)$ with a single scalar parameter $\\theta$. At training iteration $k$, you will perform one policy update using Proximal Policy Optimization (PPO), where the policy objective is the clipped surrogate objective. Begin from the foundational definitions of Markov Decision Processes (MDPs), the policy gradient theorem, and likelihood-ratio gradient estimators, and then reason to the PPO clipped surrogate gradient used for a single mini-batch update.\n\nYou are provided a mini-batch of $T=5$ samples collected under the old policy $\\pi_{\\theta_{\\text{old}}}$. For each sample $t \\in \\{1,\\dots,5\\}$, you are given the importance sampling ratio $r_t = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$ evaluated at the current parameter $\\theta=\\theta_0$, the generalized advantage estimate $\\hat{A}_t$, and the policy score $g_t = \\nabla_{\\theta}\\ln \\pi_{\\theta}(a_t \\mid s_t)$ evaluated at $\\theta=\\theta_0$. The clip parameter is $\\epsilon = 0.2$. The learning rate is $\\alpha = 0.1$. Assume pure policy optimization with no entropy bonus and no value-function term in this step.\n\nGiven:\n- Initial parameter: $\\theta_0 = 0$.\n- Clip parameter: $\\epsilon = 0.2$.\n- Learning rate: $\\alpha = 0.1$.\n- Mini-batch $T=5$ with the following tuples $(r_t, \\hat{A}_t, g_t)$ evaluated at $\\theta=\\theta_0$:\n  - $t=1: (r_1=1.10, \\hat{A}_1=2.0, g_1=0.5)$\n  - $t=2: (r_2=1.25, \\hat{A}_2=1.5, g_2=1.0)$\n  - $t=3: (r_3=0.75, \\hat{A}_3=-1.0, g_3=-0.4)$\n  - $t=4: (r_4=0.95, \\hat{A}_4=-0.5, g_4=0.2)$\n  - $t=5: (r_5=1.05, \\hat{A}_5=0.8, g_5=-0.4)$\n\nUsing the clipped surrogate objective from Proximal Policy Optimization and a single gradient-ascent step computed from this mini-batch, compute the updated parameter $\\theta_1$ obtained from $\\theta_0$. Express the final updated parameter value $\\theta_1$ as a pure number. Round your answer to four significant figures.",
            "solution": "The user wants me to solve a reinforcement learning problem that involves a single policy update step using the Proximal Policy Optimization (PPO) algorithm.\n\n### Step 1: Extract Givens\n- **Task**: Compute the updated policy parameter $\\theta_1$ from an initial parameter $\\theta_0$ using a single gradient ascent step on the PPO clipped surrogate objective.\n- **Policy**: A stochastic policy $\\pi_{\\theta}(a \\mid s)$ with a single scalar parameter $\\theta$.\n- **Algorithm**: Proximal Policy Optimization (PPO).\n- **Objective Function**: Clipped surrogate objective.\n- **Initial Parameter**: $\\theta_0 = 0$.\n- **Learning Rate**: $\\alpha = 0.1$.\n- **Clip Parameter**: $\\epsilon = 0.2$.\n- **Mini-batch Size**: $T=5$.\n- **Mini-batch Data**: A set of $T=5$ tuples $(r_t, \\hat{A}_t, g_t)$ collected under the old policy $\\pi_{\\theta_{\\text{old}}}$ and evaluated at the current parameter $\\theta = \\theta_0$.\n  - $r_t = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}\\bigg|_{\\theta=\\theta_0}$ is the importance sampling ratio.\n  - $\\hat{A}_t$ is the generalized advantage estimate.\n  - $g_t = \\nabla_{\\theta}\\ln \\pi_{\\theta}(a_t \\mid s_t)\\big|_{\\theta=\\theta_0}$ is the policy score function.\n- **Data points**:\n  - $t=1: (r_1=1.10, \\hat{A}_1=2.0, g_1=0.5)$\n  - $t=2: (r_2=1.25, \\hat{A}_2=1.5, g_2=1.0)$\n  - $t=3: (r_3=0.75, \\hat{A}_3=-1.0, g_3=-0.4)$\n  - $t=4: (r_4=0.95, \\hat{A}_4=-0.5, g_4=0.2)$\n  - $t=5: (r_5=1.05, \\hat{A}_5=0.8, g_5=-0.4)$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the well-established Proximal Policy Optimization (PPO) algorithm, a standard method in reinforcement learning. All concepts like Markov Decision Processes, policy gradients, importance sampling, and advantage estimation are fundamental to the field. The provided context of Digital Twins and Cyber-Physical Systems is a common and realistic application domain for these techniques. The problem is scientifically and factually sound.\n- **Well-Posed**: The problem provides all necessary data ($\\theta_0$, $\\alpha$, $\\epsilon$, and the mini-batch samples) to compute a unique value for the updated parameter $\\theta_1$. The task is clearly defined.\n- **Objective**: The problem is stated in precise, quantitative terms, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nThe goal of policy gradient methods in reinforcement learning is to optimize a policy $\\pi_{\\theta}(a \\mid s)$ by maximizing an objective function $J(\\theta)$, which typically represents the expected total discounted reward. The policy gradient theorem provides an expression for the gradient of this objective:\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) A^{\\pi_{\\theta}}(s_t, a_t) \\right]\n$$\nwhere $\\tau$ is a trajectory $(s_0, a_0, s_1, a_1, \\dots)$ sampled by following policy $\\pi_{\\theta}$, and $A^{\\pi_{\\theta}}(s_t, a_t)$ is the advantage function, which measures how much better an action $a_t$ is compared to the average action at state $s_t$.\n\nPPO is an on-policy algorithm, but it can use data collected from a slightly older version of the policy, $\\pi_{\\theta_{\\text{old}}}$, to update the current policy $\\pi_{\\theta}$. This is achieved using importance sampling. The ratio $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$ re-weights the contribution of each sample. The objective function becomes:\n$$\nJ^{\\text{IS}}(\\theta) = \\mathbb{E}_{t \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ r_t(\\theta) \\hat{A}_t \\right]\n$$\nwhere $\\hat{A}_t$ is an estimate of the advantage function.\n\nUnconstrained optimization of $J^{\\text{IS}}(\\theta)$ can lead to excessively large policy updates, causing instability. PPO addresses this by introducing a clipped surrogate objective, which discourages large changes in the policy ratio $r_t(\\theta)$. The PPO clipped surrogate objective for a mini-batch is:\n$$\nL^{\\text{CLIP}}(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\min\\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right)\n$$\nwhere $\\epsilon$ is a small hyperparameter that defines the clipping range.\n\nWe need to compute the gradient of this objective with respect to $\\theta$ and evaluate it at the current parameter $\\theta_0$. The gradient is used to perform a single step of gradient ascent:\n$$\n\\theta_1 = \\theta_0 + \\alpha \\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0}\n$$\n\nThe gradient of the ratio $r_t(\\theta)$ is found using the log-derivative trick:\n$$\n\\nabla_{\\theta} r_t(\\theta) = \\nabla_{\\theta} \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) = r_t(\\theta) g_t(\\theta)\n$$\nwhere $g_t(\\theta) = \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)$.\nAt $\\theta=\\theta_0$, the gradient of the unclipped term $r_t(\\theta) \\hat{A}_t$ is $r_t(\\theta_0) g_t(\\theta_0) \\hat{A}_t$. The problem provides these values as $r_t$, $g_t$, and $\\hat{A}_t$.\n\nThe gradient of the full objective $L^{\\text{CLIP}}$ depends on which term in the `min` function is active for each sample $t$. We analyze this based on the sign of the advantage estimate $\\hat{A}_t$. Let's evaluate each sample at $\\theta = \\theta_0$ where the given values apply. The clip interval for $r_t$ is $[1-\\epsilon, 1+\\epsilon] = [1-0.2, 1+0.2] = [0.8, 1.2]$.\n\nThe gradient contribution $G_t$ from each sample $t$ is non-zero only if the policy update is not \"too large\".\n- If $\\hat{A}_t \\ge 0$, we want to increase the probability of action $a_t$, but not by too much. The gradient is taken from the unclipped term only if $r_t \\le 1+\\epsilon$. If $r_t > 1+\\epsilon$, the objective is clipped, and the gradient becomes $0$ to prevent further increase.\n- If $\\hat{A}_t < 0$, we want to decrease the probability of action $a_t$, but not by too much. The gradient is taken from the unclipped term only if $r_t \\ge 1-\\epsilon$. If $r_t < 1-\\epsilon$, the objective is clipped, and the gradient becomes $0$ to prevent further decrease.\n\nThe gradient contribution for sample $t$ is thus:\n$$\nG_t = \\begin{cases} r_t g_t \\hat{A}_t & \\text{if } (\\hat{A}_t \\ge 0 \\text{ and } r_t \\le 1+\\epsilon) \\text{ or } (\\hat{A}_t < 0 \\text{ and } r_t \\ge 1-\\epsilon) \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\nNow, we calculate $G_t$ for each sample in the mini-batch:\n\n- **Sample $t=1$**: $(r_1=1.10, \\hat{A}_1=2.0, g_1=0.5)$\n  - $\\hat{A}_1 = 2.0 > 0$. We check the condition $r_1 \\le 1+\\epsilon$.\n  - $1.10 \\le 1.2$, which is true. The term is not clipped.\n  - $G_1 = r_1 g_1 \\hat{A}_1 = 1.10 \\times 0.5 \\times 2.0 = 1.1$.\n\n- **Sample $t=2$**: $(r_2=1.25, \\hat{A}_2=1.5, g_2=1.0)$\n  - $\\hat{A}_2 = 1.5 > 0$. We check the condition $r_2 \\le 1+\\epsilon$.\n  - $1.25 \\le 1.2$, which is false. The term is clipped.\n  - $G_2 = 0$.\n\n- **Sample $t=3$**: $(r_3=0.75, \\hat{A}_3=-1.0, g_3=-0.4)$\n  - $\\hat{A}_3 = -1.0 < 0$. We check the condition $r_3 \\ge 1-\\epsilon$.\n  - $0.75 \\ge 0.8$, which is false. The term is clipped.\n  - $G_3 = 0$.\n\n- **Sample $t=4$**: $(r_4=0.95, \\hat{A}_4=-0.5, g_4=0.2)$\n  - $\\hat{A}_4 = -0.5 < 0$. We check the condition $r_4 \\ge 1-\\epsilon$.\n  - $0.95 \\ge 0.8$, which is true. The term is not clipped.\n  - $G_4 = r_4 g_4 \\hat{A}_4 = 0.95 \\times 0.2 \\times (-0.5) = -0.095$.\n\n- **Sample $t=5$**: $(r_5=1.05, \\hat{A}_5=0.8, g_5=-0.4)$\n  - $\\hat{A}_5 = 0.8 > 0$. We check the condition $r_5 \\le 1+\\epsilon$.\n  - $1.05 \\le 1.2$, which is true. The term is not clipped.\n  - $G_5 = r_5 g_5 \\hat{A}_5 = 1.05 \\times (-0.4) \\times 0.8 = -0.336$.\n\nThe total gradient for the mini-batch is the average of the individual contributions:\n$$\n\\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0} = \\frac{1}{T} \\sum_{t=1}^{T} G_t = \\frac{1}{5} (G_1 + G_2 + G_3 + G_4 + G_5)\n$$\n$$\n\\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0} = \\frac{1}{5} (1.1 + 0 + 0 - 0.095 - 0.336) = \\frac{1}{5} (0.669) = 0.1338\n$$\n\nFinally, we perform the gradient ascent update step with learning rate $\\alpha=0.1$:\n$$\n\\theta_1 = \\theta_0 + \\alpha \\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0}\n$$\n$$\n\\theta_1 = 0 + 0.1 \\times 0.1338\n$$\n$$\n\\theta_1 = 0.01338\n$$\n\nThe updated parameter is $0.01338$. This value has four significant figures, so no further rounding is needed.",
            "answer": "$$\n\\boxed{0.01338}\n$$"
        }
    ]
}