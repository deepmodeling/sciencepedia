## 引言
在日益复杂的信息物理系统（CPS）中，实现[最优控制](@entry_id:138479)面临着系统动态未知、环境不确定以及高维度的挑战。传统控制方法在这些场景下常显不足，而强化学习（RL）作为一种强大的数据驱动决策框架，为解决这类复杂的[序贯决策问题](@entry_id:136955)提供了全新的范式。然而，在将强化学习的理论潜力转化为安全、高效的[最优控制](@entry_id:138479)策略时，研究者和工程师仍面临着从理论到实践的鸿沟。本文旨在系统性地弥合这一差距。

为了实现这一目标，我们将通过三个核心章节逐步展开。在“原理与机制”一章中，我们将奠定理论基石，深入探讨如何使用[马尔可夫决策过程](@entry_id:140981)（MDP）对问题进行形式化，并剖析[价值函数](@entry_id:144750)、[贝尔曼方程](@entry_id:1121499)以及各类核心RL算法的运作机制。随后，在“应用与跨学科连接”一章中，我们将展示这些原理如何与经典控制理论融合，并应用于解决能源、金融及医疗等领域的实际挑战，特别关注安全性和鲁棒性等关键问题。最后，“动手实践”部分将提供具体的编程练习，让您将所学知识付诸实践。

让我们首先从构建强化学习用于[最优控制](@entry_id:138479)的理论基础开始。

## 原理与机制

本章旨在深入探讨[强化学习](@entry_id:141144)（RL）在最优控制问题中的核心原理与关键机制。我们将从信息物理系统（CPS）的视角出发，系统地构建用于[序贯决策](@entry_id:145234)的形式化模型，阐明价值函数与[贝尔曼方程](@entry_id:1121499)的中心作用，并剖析主要的强化学习算法范式。本章内容将为后续章节中更高级的应用与[算法设计](@entry_id:634229)奠定坚实的理论基础。

### [序贯决策](@entry_id:145234)的形式化：马尔可夫决策过程

为了应用[强化学习](@entry_id:141144)来解决[最优控制](@entry_id:138479)问题，我们首先需要一个能够描述智能体（控制器）与环境（受控系统）之间动态交互的数学框架。**[马尔可夫决策过程](@entry_id:140981)（Markov Decision Process, MDP）** 正是这一标准框架。一个[折扣](@entry_id:139170)MDP由一个五元组 $(\mathcal{S}, \mathcal{A}, P, r, \gamma)$ 定义，其中：

*   **[状态空间](@entry_id:160914) (State Space) $\mathcal{S}$**: 系统所有可能状态的集合。在CPS中，这通常对应于系统的物理状态，如位置、速度或温度。
*   **动作空间 (Action Space) $\mathcal{A}$**: 智能体可以执行的所有可能控制输入的集合。
*   **转移概率 (Transition Probability) $P$**: 一个描述系统动态的函数。$P(s' \mid s, a) = \mathbb{P}(s_{t+1}=s' \mid s_t=s, a_t=a)$ 表示在状态 $s$ 执行动作 $a$ 后，系统转移到状态 $s'$ 的概率。
*   **奖励函数 (Reward Function) $r$**: $r(s, a)$ 或 $r(s, a, s')$ 是一个标量函数，量化了在状态 $s$ 执行动作 $a$ 的即时收益。在最优控制中，奖励通常是阶段成本（stage cost）的[相反数](@entry_id:151709)，即 $r = -c$。
*   **[折扣](@entry_id:139170)因子 (Discount Factor) $\gamma \in [0, 1)$**: 一个决定未来奖励相对当前奖励重要性的参数。较小的 $\gamma$ 使智能体更关注短期收益，而接近 $1$ 的 $\gamma$ 则意味着更长远的规划 horizon。

**[马尔可夫性质](@entry_id:139474)（Markov Property）** 是这一框架的基石，它要求系统的未来状态只依赖于当前状态和动作，而与过去的历史无关。考虑一个由离散时间动态 $x_{t+1} = f(x_t, a_t) + w_t$ 描述的CPS，其中 $x_t$ 是可测量的状态，$a_t$ 是控制输入，$w_t$ 是随机扰动 。如果扰动 $w_t$ 是[独立同分布](@entry_id:169067)的，并且与 $(x_t, a_t)$ 无关，那么物理状态 $x_t$ 就包含了预测未来所需的所有信息，满足[马尔可夫性质](@entry_id:139474)。此时，我们可以将[状态空间](@entry_id:160914) $\mathcal{S}$ 定义为所有可能的 $x_t$ 的集合。转移概率核 $P(\mathrm{d}x' \mid x, a)$ 则由扰动 $w$ 的分布通过映射 $w \mapsto f(x, a) + w$ 决定。

与此相对，经典的确定性[最优控制](@entry_id:138479)问题是该框架的一个特例。当扰动 $w_t=0$ 且系统动态 $f$ 已知时，转移就变得确定：$x_{t+1} = f(x_t, a_t)$。此时，转移概率 $P(\cdot \mid x, a)$ 成为一个在点 $f(x, a)$ 上的**[狄拉克测度](@entry_id:197577)（Dirac measure）**。因此，涉及期望的随机[动态规划](@entry_id:141107)方程将退化为纯粹的确定性递归，无需学习或估计转移模型 。

#### 部分[可观测性](@entry_id:152062)与信念状态

在许多实际的CPS中，系统的完整状态并不能被直接测量，我们只能获得带有噪声或不完整的观测。这类问题被建模为**部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process, [POMDP](@entry_id:637181)）**。一个[POMDP](@entry_id:637181)在MDP的基础上增加了两个元素：一个**观测空间（Observation Space）$\Omega$** 和一个**观测模型（Observation Model）$O(o' \mid s', a)$**，后者给出了在系统转移到状态 $s'$ 并执行动作 $a$ 后，接收到观测 $o'$ 的概率 。

由于真实状态未知，智能体必须在所有可能状态的一个概率分布上进行决策。这个概率分布被称为**[信念状态](@entry_id:195111)（Belief State）**，记为 $b(s) = \mathbb{P}(s_t=s \mid \text{history})$。[信念状态](@entry_id:195111)本身构成了新MDP（即信念MDP）的状态。智能体的任务是根据新接收的观测来更新其信念。这一过程通过**贝叶斯[信念更新](@entry_id:266192)（Bayesian Belief Update）** 来实现。给定[先验信念](@entry_id:264565) $b(s)$，执行动作 $a$ 并接收到新观测 $o'$ 后，后验信念 $b'(s')$ 的计算分为两步：

1.  **预测（Prediction）**: 计算在接收观测前的预测信念 $\bar{b}(s')$，即系统在执行动作 $a$ 后转移到状态 $s'$ 的概率。这通过对所有可能的先前状态 $s$ 进行[边缘化](@entry_id:264637)得到：
    $\bar{b}(s') = P(s' \mid a, b) = \sum_{s \in \mathcal{S}} P(s' \mid s, a) b(s)$

2.  **更新（Update）**: 使用[贝叶斯法则](@entry_id:275170)，将观测的可能性 $O(o' \mid s', a)$ 结合到预测信念中，并进行归一化：
    $b'(s') = \frac{O(o' \mid s', a) \bar{b}(s')}{\mathbb{P}(o' \mid a, b)} = \eta \cdot O(o' \mid s', a) \sum_{s \in \mathcal{S}} P(s' \mid s, a) b(s)$
    其中 $\eta = (\sum_{s'' \in \mathcal{S}} O(o' \mid s'', a) \bar{b}(s''))^{-1}$ 是一个[归一化常数](@entry_id:752675)。

例如，考虑一个具有两种潜在模式（正常 $s_1$ 和故障 $s_2$）的CPS，传感器可能发出正常读数 $o_1$ 或警报 $o_2$。假设在执行某项恢复性控制动作 $a_1$ 之前，我们相信系统有 $0.7$ 的概率处于[正常模式](@entry_id:139640)，有 $0.3$ 的概率处于故障模式（即 $b = [0.7, 0.3]$）。在执行动作 $a_1$ 后，我们收到了警报 $o_2$。利用已知的转移模型和观测模型，通过上述[信念更新](@entry_id:266192)公式，我们可以精确计算出新的信念分布 $b'$。如果计算结果为 $[0.1355, 0.8645]$，这意味着警报的出现极大地增强了我们对系统处于故障模式的信心 。

### 策略、[价值函数](@entry_id:144750)与[贝尔曼方程](@entry_id:1121499)

在MDP框架下，智能体的行为由一个**策略（Policy）** $\pi$ 决定。策略是一个从状态到动作的映射。**确定性策略（Deterministic Policy）** 为每个状态指定一个唯一的动作，即 $\mu: \mathcal{S} \to \mathcal{A}$。**随机性策略（Stochastic Policy）** 则为每个状态指定一个动作空间上的概率分布，$\pi(a \mid s)$ 。当一个固定的、不随时间变化的（即平稳的）策略被应用于MDP时，它“闭合了”控制回路，使得状态序列 $\{s_t\}$ 的演化成为一个**马尔可夫链（Markov Chain）**。其转移核 $P^\pi(s' \mid s)$ 是由原始MDP的转移核 $P(s' \mid s, a)$ 在策略 $\pi$ 的[动作选择](@entry_id:151649)下平均得到的：
$$ P^\pi(s' \mid s) = \int_{\mathcal{A}} P(s' \mid s, a) \pi(\mathrm{d}a \mid s) $$
如果策略是平稳的，那么这个[马尔可夫链](@entry_id:150828)是时齐的；如果策略随时间变化（$\pi_t(a \mid s)$），则马尔可夫链是时变的 。

[强化学习](@entry_id:141144)的目标是找到一个能最大化累积奖励的策略。我们定义从时间 $t$ 开始的**回报（Return）** $G_t$ 为[折扣](@entry_id:139170)奖励的总和：$G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$。为了评估一个策略的好坏，我们引入**价值函数（Value Function）**。

*   **状态价值函数 (State-Value Function) $V^\pi(s)$**: 从状态 $s$ 出发，并始终遵循策略 $\pi$ 所能获得的期望回报。
    $V^\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s]$

*   **动作[价值函数](@entry_id:144750) (Action-Value Function) $Q^\pi(s, a)$**: 从状态 $s$ 出发，执行动作 $a$，然后遵循策略 $\pi$ 所能获得的期望回报。
    $Q^\pi(s, a) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]$

这两个[价值函数](@entry_id:144750)之间存在直接关系：$V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot \mid s)}[Q^\pi(s, a)]$。

#### [贝尔曼方程](@entry_id:1121499)

[价值函数](@entry_id:144750)满足一组被称为**[贝尔曼方程](@entry_id:1121499)（Bellman Equations）** 的重要自洽性关系。这些方程是[动态规划](@entry_id:141107)和[强化学习](@entry_id:141144)的基石。

对于一个固定的策略 $\pi$，其状态[价值函数](@entry_id:144750) $V^\pi$ 必须满足**贝尔曼期望方程（Bellman Expectation Equation）** ：
$$ V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot|s), s' \sim P(\cdot|s,a)}[r(s,a,s') + \gamma V^\pi(s')] $$
该方程表明，当前状态的价值等于在该状态下遵循策略 $\pi$ 所得的期望即时奖励与下一状态的[折扣](@entry_id:139170)期望价值之和。这是一个关于 $V^\pi$ 的线性方程组（在有限状态和动作空间下）。

我们可以定义一个**贝尔曼期望算子（Bellman Expectation Operator）$T^\pi$** ：
$$ (T^\pi V)(s) = \mathbb{E}_{a \sim \pi(\cdot|s), s' \sim P(\cdot|s,a)}[r(s,a,s') + \gamma V(s')] $$
贝尔曼期望方程可以简洁地写为 $V^\pi = T^\pi V^\pi$，这意味着 $V^\pi$ 是算子 $T^\pi$ 的一个**不动点（Fixed Point）**。在赋范[函数空间](@entry_id:143478)中，可以证明当 $\gamma \lt 1$ 时，$T^\pi$ 是一个**$\gamma$-[压缩映射](@entry_id:139989)（$\gamma$-contraction）**。根据[巴拿赫不动点定理](@entry_id:146620)，这保证了 $V^\pi$ 存在且唯一。对于有限[状态空间](@entry_id:160914)，该方程可以表示为矩阵形式 $(I - \gamma P^\pi)V^\pi = r^\pi$，其[闭式](@entry_id:271343)解为 $V^\pi = (I - \gamma P^\pi)^{-1} r^\pi$，其中 $P^\pi$ 和 $r^\pi$ 分别是策略 $\pi$ 诱导的转移矩阵和奖励向量 。

当我们的目标是寻找最优策略时，我们关注的是**最优[价值函数](@entry_id:144750)** $V^*(s) = \max_\pi V^\pi(s)$。最优[价值函数](@entry_id:144750)满足**贝尔曼最优方程（Bellman Optimality Equation）** ：
$$ V^*(s) = \max_{a \in \mathcal{A}} \mathbb{E}_{s' \sim P(\cdot|s,a)}[r(s,a,s') + \gamma V^*(s')] $$
与期望方程不同，最优方程中包含一个 `max` 算子，这使其成为一个非线性方程。它表达了**最优性原理（Principle of Optimality）**：一个[最优策略](@entry_id:138495)必然包含从任何中间状态到最终状态的最优子策略。一旦我们求解出 $V^*$，[最优策略](@entry_id:138495)可以通过贪心地选择在每个状态下能最大化贝尔曼最优方程右侧的动作来获得。

### 求解的核心算法范式

求解[贝尔曼方程](@entry_id:1121499)并找到最优策略是[强化学习](@entry_id:141144)的核心任务。算法大致可分为两大类：基于模型的和无模型的。

#### 基于模型的强化学习

**基于模型的（Model-Based）** [强化学习](@entry_id:141144)算法首先尝试学习环境的模型，即转移概率 $P$ 和[奖励函数](@entry_id:138436) $r$。一旦模型被估计出来，算法就可以利用这个模型进行**规划（Planning）**，通过动态规划等方法直接计算出[最优策略](@entry_id:138495)，而无需与真实环境进行更多交互。

这种“学习模型，然后规划”的范式与CPS中许多经典的控制方法不谋而合。例如，**[线性二次调节器](@entry_id:267871)（Linear Quadratic Regulator, LQR）** 和 **模型预测控制（Model Predictive Control, MPC）** 都是典型的[基于模型的控制](@entry_id:276825)技术 。LQR通过求解依赖于系统模型矩阵 $(A, B)$ 的代数里卡提方程来获得最优反馈增益。MPC则在每个时间步利用系统模型来预测未来的状态轨迹，并在线求解一个有限时域的优化问题来决定当前的最优控制动作。从这个角度看，MPC可以被视为在已知或学习到的MDP模型上，反复进行有限步规划的一种实现方式 。[数字孪生](@entry_id:171650)（Digital Twin）作为物理系统的高保真模型，为基于模型的RL提供了理想的平台，可以在其中进行[模型辨识](@entry_id:139651)、策略验证和在线规划。

#### 无模型的强化学习

**无模型的（Model-Free）** 强化学习算法则不尝试学习完整的环境模型，而是直接从与环境交互得到的经验（状态、动作、奖励的序列）中学习价值函数或策略。这在模型难以获取或过于复杂时尤其有用。无模型方法主要分为三大类。

##### 1. 基于价值的方法

这类方法的核心是估计价值函数，然后隐式地从中推导出策略。最经典的应用场景是[策略评估](@entry_id:136637)，即估计一个给定策略 $\pi$ 的价值函数 $V^\pi$。

*   **[蒙特卡洛](@entry_id:144354)（[Monte Carlo](@entry_id:144354), MC）方法**: MC方法通过从环境中采样完整的轨迹（episodes），并计算每个状态访问的实际回报 $G_t$。然后，通过平均这些样本回报来更新[价值函数](@entry_id:144750)的估计值 $\hat{V}(s_t)$：
    $\hat{V}(s_t) \leftarrow \hat{V}(s_t) + \alpha [G_t - \hat{V}(s_t)]$
    其中 $\alpha$ 是[学习率](@entry_id:140210)。MC方法的目标 $G_t$ 是对 $V^\pi(s_t)$ 的**无偏（unbiased）**估计，但由于回报 $G_t$ 是一系列[随机变量](@entry_id:195330)（奖励和转移）的总和，其**方差（variance）**通常很高，导致学习过程不稳定 。

*   **时序差分（Temporal-Difference, TD）学习**: TD方法是RL的核心思想之一。它不等待一个轨迹结束，而是在每一步之后就进行学习。TD(0)算法的更新依赖于所谓的**TD目标（TD Target）**，它由即时奖励和下一状态的（当前）价值估计构成。这个过程被称为**自举（Bootstrapping）**。TD(0)的更新规则为：
    $\hat{V}(s_t) \leftarrow \hat{V}(s_t) + \alpha [r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t)]$
    括号中的项 $r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t)$ 被称为**TD误差（TD Error）**。由于TD目标 $\hat{V}(s_{t+1})$ 自身是一个估计值，所以TD目标是对 $V^\pi(s_t)$ 的**有偏（biased）**估计。然而，因为它只依赖于一个时间步的随机性，其方差远低于MC的回报，从而使得学习过程更稳定、更高效 。数字孪生可以为MC和TD方法提供大量低成本的仿真轨迹，用于[策略评估](@entry_id:136637)。如果[数字孪生](@entry_id:171650)模型不完美，那么基于其仿真的学习会引入[模型偏差](@entry_id:184783) 。

##### 2. 基于策略的方法

这类方法直接[参数化](@entry_id:265163)策略 $\pi_\theta(a \mid s)$，并试图通过优化参数 $\theta$ 来最大化性能目标 $J(\theta)$。**[策略梯度](@entry_id:635542)（Policy Gradient）** 方法是其中的主流，它沿着性能目标梯度的方向调整策略参数。根据**[策略梯度定理](@entry_id:635009)（Policy Gradient Theorem）**，梯度的形式通常为 $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a \mid s) \cdot (\text{某种回报的度量})]$。

为了减小[梯度估计](@entry_id:164549)的方差，一个关键技术是从回报度量中减去一个**基线（Baseline）** $b(s)$，这个基线只依赖于状态 $s$ 而不依赖于动作 $a$。减去这样的基线不会改变梯度的[期望值](@entry_id:150961)，因此是无偏的。一个理想的基线是状态价值函数 $V^\pi(s)$。此时，回报度量变为**[优势函数](@entry_id:635295)（Advantage Function）** $A^\pi(s, a)$ ：
$$ A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s) $$
[优势函数](@entry_id:635295)衡量了在状态 $s$ 下，选择动作 $a$ 相对于遵循策略 $\pi$ 的平均表现有多好。如果 $A^\pi(s, a) > 0$，意味着动作 $a$ 优于平均，策略更新会增加选择该动作的概率。反之亦然。根据定义，[优势函数](@entry_id:635295)在策略 $\pi$ 下的期望为零，即 $\mathbb{E}_{a \sim \pi(\cdot|s)}[A^\pi(s, a)] = 0$ 。对于一个[最优策略](@entry_id:138495) $\pi^*$，其所有动作的优势值都小于等于零，$A^{\pi^*}(s, a) \le 0$ 。

##### 3. [演员-评论家](@entry_id:634214)方法

**[演员-评论家](@entry_id:634214)（Actor-Critic）** 方法结合了基于价值和基于策略的方法的优点。它显式地维护两个[参数化](@entry_id:265163)模型：

*   **演员（Actor）**: 即策略 $\pi_\theta(a \mid s)$，负责选择动作。
*   **评论家（Critic）**: 即价值函数估计 $V_w(s)$ 或 $Q_w(s, a)$，负责评估演员所选动作的好坏。

在一个典型的在线[演员-评论家](@entry_id:634214)算法中，学习过程在每个时间步进行 。评论家使用[TD误差](@entry_id:634080)来更新其参数 $w$，以使其价值估计更接近TD目标：
$$ \delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t) $$
$$ \Delta w_t = \alpha_w \delta_t \nabla_w V_w(s_t) $$
演员则使用评论家提供的[TD误差](@entry_id:634080)（作为[优势函数](@entry_id:635295)的一个样本）来更新其策略参数 $\theta$。[TD误差](@entry_id:634080) $\delta_t$ 指示了动作 $a_t$ 的好坏，而[策略梯度](@entry_id:635542)项 $\nabla_\theta \log \pi_\theta(a_t \mid s_t)$ 指示了如何[调整参数](@entry_id:756220)以增加或减少该动作的概率：
$$ \Delta \theta_t = \alpha_\theta \delta_t \nabla_\theta \log \pi_\theta(a_t \mid s_t) $$
例如，在一个使用高斯策略 $\pi_\theta(a|s) \sim \mathcal{N}(\theta s, \sigma^2)$ 和线性价值函数 $V_w(s) = ws$ 的控制问题中，我们可以根据一次转移 $(s_t, a_t, r_t, s_{t+1})$ 的具体数值，精确计算出 $\delta_t$ 和 $\nabla_\theta \log \pi_\theta(a_t \mid s_t)$，从而完成对演员参数 $\theta$ 的一次更新 。

### 高级主题与实践考量

#### [离策略学习](@entry_id:634676)与重要性采样

在许多情况下，我们希望评估或改进一个**目标策略（target policy）** $\pi$，但我们拥有的数据却是由另一个**行为策略（behavior policy）** $\mu$ 生成的。这种情况被称为**离策略（Off-Policy）** 学习。[离策略学习](@entry_id:634676)非常重要，因为它允许我们重用历史数据，或者使用一个安全的探索性策略来收集数据，同时学习一个可能更优但有风险的策略。

为了修正行为策略和目标策略之间的分布不[匹配问题](@entry_id:275163)，我们使用**重要性采样（Importance Sampling, IS）**。其核心思想是通过一个权重来重新调整从行为策略中采样的回报，这个权重是该轨迹在目标策略和行为策略下发生概率的比值 。对于一条完整的轨迹，这个重要性权重是每一步动作概率比的连乘积：
$$ \rho_{0:T-1} = \prod_{t=0}^{T-1} \frac{\pi(a_t \mid s_t)}{\mu(a_t \mid s_t)} $$
于是，对目标策略性能 $J(\pi)$ 的一个无偏估计是：
$$ \hat{J}_{\mathrm{IS}} = \frac{1}{N} \sum_{i=1}^N \left( \prod_{t=0}^{T_i-1} \rho_t^{(i)} \right) R(\tau^{(i)}) $$
IS方法的一个关键前提是**覆盖性假设（Coverage Assumption）**：如果 $\pi(a \mid s) > 0$，那么必须有 $\mu(a \mid s) > 0$。也就是说，行为策略必须有几率采样到目标策略会选择的所有动作。

重要性采样的主要缺点是其[估计量的方差](@entry_id:167223)可能非常大，尤其是当轨迹很长或两个策略差异很大时，连乘的权重会变得极不稳定。此外，需要强调的是，标准的[重要性采样](@entry_id:145704)只能修正策略上的不匹配，它无法修正环境动态（即转移模型 $P$）的差异。如果数据来自一个与目标环境动态不同的模拟器（例如，一个不完美的[数字孪生](@entry_id:171650)），IS估计将是有偏的 。

#### [安全强化学习](@entry_id:1131184)

在CPS应用中，安全性是首要考虑的问题。例如，[自动驾驶](@entry_id:270800)汽车不能发生碰撞，机器人手臂不能超过其关节极限。标准RL最大化期望回报的目标并未显式考虑这些硬约束。**[安全强化学习](@entry_id:1131184)（Safe Reinforcement Learning）** 旨在解决这一问题。

一个有力的框架是**[约束马尔可夫决策过程](@entry_id:1122938)（Constrained MDP, CMDP）** 。CMDP在标准MDP的基础上增加了一个或多个约束。例如，我们可以定义一个安全成本函数 $c(s, a)$，它量化了在状态 $s$ 执行动作 $a$ 的瞬时风险。然后，我们可以要求策略 $\pi$ 在最大化奖励回报 $J(\pi)$ 的同时，满足其期望累积安全成本 $C(\pi)$ 不超过一个预设的预算 $d$：
$$ \max_\pi J(\pi) \quad \text{subject to} \quad C(\pi) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) \right] \le d $$
这类[约束优化问题](@entry_id:1122941)通常使用**[拉格朗日乘子法](@entry_id:176596)（Lagrangian method）** 求解。通过引入一个拉格朗日乘子（或对偶变量）$\lambda \ge 0$，我们将约束问题转化为一个无约束的优化问题，其目标是求解拉格朗日函数 $L(\pi, \lambda) = J(\pi) - \lambda(C(\pi) - d)$ 的鞍点。这可以通过**原始-对偶（primal-dual）** 算法实现：交替地在策略参数 $\theta$（[原始变量](@entry_id:753733)）上进行梯度上升，在 $\lambda$（[对偶变量](@entry_id:143282)）上进行梯度上升。[对偶变量](@entry_id:143282) $\lambda$ 的更新规则为：
$$ \lambda_{k+1} = [\lambda_k + \alpha_k (C(\pi_{\theta_k}) - d)]_+ $$
其中 $[\cdot]_+$ 表示投影到非负数。这个更新直观地解释了 $\lambda$ 的作用：当安全约束被违反时（$C(\pi) > d$），$\lambda$ 增加，从而加大对策略的惩罚；当约束被满足时，$\lambda$ 减小。数字孪生为训练这类安全策略提供了一个至关重要的安全环境，可以在不危及物理系统的情况下，通过仿真来估计 $J(\pi)$ 和 $C(\pi)$ 并执行原始-对偶更新 。