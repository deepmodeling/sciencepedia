{
    "hands_on_practices": [
        {
            "introduction": "为了在信息物理系统中实现最优控制，我们首先必须确定从任何给定状态可实现的最大潜在长期回报，即最优价值。本练习  将指导您完成价值迭代算法的单步计算，这是动态规划的基石。通过应用贝尔曼最优算子，您将亲身体验寻找最优控制策略的迭代过程，即便是在显式策略形成之前。",
            "id": "4239976",
            "problem": "考虑一个信息物理系统 (CPS)，其数字孪生为资产健康状况提供了一个经过校准的有限状态预测模型，该模型表示为一个折扣无限时域马尔可夫决策过程 (MDP)。该 MDP 有三个状态：$s_{\\mathrm{N}}$ (正常运行)、$s_{\\mathrm{D}}$ (降级运行) 和 $s_{\\mathrm{S}}$ (停机)。动作集为 $\\mathcal{A}=\\{a_{\\mathrm{O}}, a_{\\mathrm{M}}, a_{\\mathrm{Sh}}\\}$，分别表示操作、维护和停机。即时奖励函数 $r(s,a)$ 如下：\n- $r(s_{\\mathrm{N}}, a_{\\mathrm{O}})=+5.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{M}})=-1.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=-0.2$。\n- $r(s_{\\mathrm{D}}, a_{\\mathrm{O}})=+1.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{M}})=-0.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=-0.3$。\n- $r(s_{\\mathrm{S}}, a_{\\mathrm{O}})=-3.0$, $r(s_{\\mathrm{S}}, a_{\\mathrm{M}})=-0.2$, $r(s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=-0.1$。\n\n状态转移概率 $P(s' \\mid s,a)$ 由数字孪生的单步前向健康预测提供信息，具体如下：\n- 从 $s_{\\mathrm{N}}$ 开始：\n  - 在动作 $a_{\\mathrm{O}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.85$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.05$。\n  - 在动作 $a_{\\mathrm{M}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.95$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.04$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.01$。\n  - 在动作 $a_{\\mathrm{Sh}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.90$。\n- 从 $s_{\\mathrm{D}}$ 开始：\n  - 在动作 $a_{\\mathrm{O}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.30$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.50$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.20$。\n  - 在动作 $a_{\\mathrm{M}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.60$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.35$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.05$。\n  - 在动作 $a_{\\mathrm{Sh}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.90$。\n- 从 $s_{\\mathrm{S}}$ 开始：\n  - 在动作 $a_{\\mathrm{O}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.60$。\n  - 在动作 $a_{\\mathrm{M}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.40$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$。\n  - 在动作 $a_{\\mathrm{Sh}}$ 下：$P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=1.00$。\n\n折扣因子为 $\\gamma=0.92$。初始值函数为 $V_{0}(s_{\\mathrm{N}})=8$, $V_{0}(s_{\\mathrm{D}})=3$, $V_{0}(s_{\\mathrm{S}})=-1$。\n\n使用马尔可夫决策过程 (MDP) 中最优控制的严谨基础以及应用于值迭代的贝尔曼最优性原理，执行一次值迭代来计算 $V_{1} = \\mathcal{T}V_{0}$。使用 `pmatrix` 环境将您的最终答案表示为行向量 $\\begin{pmatrix}V_{1}(s_{\\mathrm{N}})  V_{1}(s_{\\mathrm{D}})  V_{1}(s_{\\mathrm{S}})\\end{pmatrix}$。不要四舍五入；根据数据提供精确的小数值。",
            "solution": "用户要求对给定的马尔可夫决策过程 (MDP) 执行一次值迭代算法。\n\n**问题验证**\n\n首先，我将根据所需协议验证问题陈述。\n\n**步骤 1：提取给定条件**\n- 状态空间：$\\mathcal{S}=\\{s_{\\mathrm{N}}, s_{\\mathrm{D}}, s_{\\mathrm{S}}\\}$\n- 动作空间：$\\mathcal{A}=\\{a_{\\mathrm{O}}, a_{\\mathrm{M}}, a_{\\mathrm{Sh}}\\}$\n- 奖励函数 $r(s,a)$:\n  - $r(s_{\\mathrm{N}}, a_{\\mathrm{O}})=+5.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{M}})=-1.0$, $r(s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=-0.2$\n  - $r(s_{\\mathrm{D}}, a_{\\mathrm{O}})=+1.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{M}})=-0.5$, $r(s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=-0.3$\n  - $r(s_{\\mathrm{S}}, a_{\\mathrm{O}})=-3.0$, $r(s_{\\mathrm{S}}, a_{\\mathrm{M}})=-0.2$, $r(s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=-0.1$\n- 转移概率 $P(s' \\mid s,a)$:\n  - 从 $s_{\\mathrm{N}}$ 开始：\n    - $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.85$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{O}})=0.05$\n    - $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.95$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.04$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{M}})=0.01$\n    - $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.05$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{N}}, a_{\\mathrm{Sh}})=0.90$\n  - 从 $s_{\\mathrm{D}}$ 开始：\n    - $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.30$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.50$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{O}})=0.20$\n    - $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.60$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.35$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{M}})=0.05$\n    - $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.10$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{D}}, a_{\\mathrm{Sh}})=0.90$\n  - 从 $s_{\\mathrm{S}}$ 开始：\n    - $a_{\\mathrm{O}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.20$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{O}})=0.60$\n    - $a_{\\mathrm{M}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.40$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{M}})=0.30$\n    - $a_{\\mathrm{Sh}}$: $P(s_{\\mathrm{N}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{D}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=0.00$, $P(s_{\\mathrm{S}} \\mid s_{\\mathrm{S}}, a_{\\mathrm{Sh}})=1.00$\n- 折扣因子：$\\gamma=0.92$\n- 初始值函数：$V_{0}(s_{\\mathrm{N}})=8$, $V_{0}(s_{\\mathrm{D}})=3$, $V_{0}(s_{\\mathrm{S}})=-1$\n\n**步骤 2：使用提取的给定条件进行验证**\n该问题在马尔可夫决策过程（最优控制的标准工具）的框架内是良定义的。它提供了所有必需的组成部分：状态、动作、转移概率、奖励、折扣因子和初始值函数。每个状态-动作对的转移概率之和正确地为 $1$，确保了数学上的一致性。例如，对于 $(s_{\\mathrm{N}}, a_{\\mathrm{O}})$，$0.85 + 0.10 + 0.05 = 1.00$。所有其他概率分布也都是有效的。该问题是自洽的、客观的且有科学依据的。它提出了一个强化学习中的标准计算练习，不包含任何逻辑缺陷、歧义或矛盾。\n\n**步骤 3：结论与行动**\n问题有效。我将继续进行求解。\n\n**解答**\n\n值迭代算法更新所有状态 $s \\in \\mathcal{S}$ 的值函数 $V_k(s)$，以产生下一个值函数 $V_{k+1}(s)$。更新规则由应用于 $V_k$ 的贝尔曼最优算子 $\\mathcal{T}$ 定义：\n$$ V_{k+1}(s) = (\\mathcal{T}V_k)(s) = \\max_{a \\in \\mathcal{A}} \\left\\{ r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V_k(s') \\right\\} $$\n我们被要求在给定 $V_0(s)$、$\\gamma=0.92$ 和所提供的 MDP 参数的情况下计算 $V_1(s)$。\n\n让我们为每个状态-动作对计算动作值函数 $Q_1(s,a)$：\n$$ Q_1(s,a) = r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V_0(s') $$\n给定的初始值函数为 $V_0(s_{\\mathrm{N}})=8$，$V_0(s_{\\mathrm{D}})=3$ 和 $V_0(s_{\\mathrm{S}})=-1$。\n\n**1. 状态 $s_{\\mathrm{N}}$ 的计算**\n我们为每个动作 $a \\in \\mathcal{A}$ 计算 $Q_1(s_{\\mathrm{N}}, a)$：\n- 对于 $a = a_{\\mathrm{O}}$：\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{O}}) = r(s_{\\mathrm{N}}, a_{\\mathrm{O}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{N}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{N}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{N}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{O}}) = 5.0 + 0.92 [(0.85)(8) + (0.10)(3) + (0.05)(-1)] = 5.0 + 0.92 [6.8 + 0.3 - 0.05] = 5.0 + 0.92(7.05) = 5.0 + 6.486 = 11.486$\n- 对于 $a = a_{\\mathrm{M}}$：\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{M}}) = r(s_{\\mathrm{N}}, a_{\\mathrm{M}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{N}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{N}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{N}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{M}}) = -1.0 + 0.92 [(0.95)(8) + (0.04)(3) + (0.01)(-1)] = -1.0 + 0.92 [7.6 + 0.12 - 0.01] = -1.0 + 0.92(7.71) = -1.0 + 7.0932 = 6.0932$\n- 对于 $a = a_{\\mathrm{Sh}}$：\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{Sh}}) = r(s_{\\mathrm{N}}, a_{\\mathrm{Sh}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{N}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{N}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{N}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{N}}, a_{\\mathrm{Sh}}) = -0.2 + 0.92 [(0.05)(8) + (0.05)(3) + (0.90)(-1)] = -0.2 + 0.92 [0.4 + 0.15 - 0.9] = -0.2 + 0.92(-0.35) = -0.2 - 0.322 = -0.522$\n\n因此，$V_1(s_{\\mathrm{N}}) = \\max\\{11.486, 6.0932, -0.522\\} = 11.486$。\n\n**2. 状态 $s_{\\mathrm{D}}$ 的计算**\n我们为每个动作 $a \\in \\mathcal{A}$ 计算 $Q_1(s_{\\mathrm{D}}, a)$：\n- 对于 $a = a_{\\mathrm{O}}$：\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{O}}) = r(s_{\\mathrm{D}}, a_{\\mathrm{O}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{D}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{D}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{D}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{O}}) = 1.5 + 0.92 [(0.30)(8) + (0.50)(3) + (0.20)(-1)] = 1.5 + 0.92 [2.4 + 1.5 - 0.2] = 1.5 + 0.92(3.7) = 1.5 + 3.404 = 4.904$\n- 对于 $a = a_{\\mathrm{M}}$：\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{M}}) = r(s_{\\mathrm{D}}, a_{\\mathrm{M}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{D}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{D}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{D}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{M}}) = -0.5 + 0.92 [(0.60)(8) + (0.35)(3) + (0.05)(-1)] = -0.5 + 0.92 [4.8 + 1.05 - 0.05] = -0.5 + 0.92(5.8) = -0.5 + 5.336 = 4.836$\n- 对于 $a = a_{\\mathrm{Sh}}$：\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{Sh}}) = r(s_{\\mathrm{D}}, a_{\\mathrm{Sh}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{D}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{D}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{D}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{D}}, a_{\\mathrm{Sh}}) = -0.3 + 0.92 [(0.00)(8) + (0.10)(3) + (0.90)(-1)] = -0.3 + 0.92 [0.0 + 0.3 - 0.9] = -0.3 + 0.92(-0.6) = -0.3 - 0.552 = -0.852$\n\n因此，$V_1(s_{\\mathrm{D}}) = \\max\\{4.904, 4.836, -0.852\\} = 4.904$。\n\n**3. 状态 $s_{\\mathrm{S}}$ 的计算**\n我们为每个动作 $a \\in \\mathcal{A}$ 计算 $Q_1(s_{\\mathrm{S}}, a)$：\n- 对于 $a = a_{\\mathrm{O}}$：\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{O}}) = r(s_{\\mathrm{S}}, a_{\\mathrm{O}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{S}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{S}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{S}}, a_{\\mathrm{O}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{O}}) = -3.0 + 0.92 [(0.20)(8) + (0.20)(3) + (0.60)(-1)] = -3.0 + 0.92 [1.6 + 0.6 - 0.6] = -3.0 + 0.92(1.6) = -3.0 + 1.472 = -1.528$\n- 对于 $a = a_{\\mathrm{M}}$：\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{M}}) = r(s_{\\mathrm{S}}, a_{\\mathrm{M}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{S}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{S}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{S}}, a_{\\mathrm{M}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{M}}) = -0.2 + 0.92 [(0.40)(8) + (0.30)(3) + (0.30)(-1)] = -0.2 + 0.92 [3.2 + 0.9 - 0.3] = -0.2 + 0.92(3.8) = -0.2 + 3.496 = 3.296$\n- 对于 $a = a_{\\mathrm{Sh}}$：\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{Sh}}) = r(s_{\\mathrm{S}}, a_{\\mathrm{Sh}}) + \\gamma [P(s_{\\mathrm{N}}|s_{\\mathrm{S}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{N}}) + P(s_{\\mathrm{D}}|s_{\\mathrm{S}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{D}}) + P(s_{\\mathrm{S}}|s_{\\mathrm{S}}, a_{\\mathrm{Sh}})V_0(s_{\\mathrm{S}})]$\n   $Q_1(s_{\\mathrm{S}}, a_{\\mathrm{Sh}}) = -0.1 + 0.92 [(0.00)(8) + (0.00)(3) + (1.00)(-1)] = -0.1 + 0.92(-1.0) = -0.1 - 0.92 = -1.02$\n\n因此，$V_1(s_{\\mathrm{S}}) = \\max\\{-1.528, 3.296, -1.02\\} = 3.296$。\n\n经过一次迭代后得到的值函数是向量 $(V_1(s_{\\mathrm{N}}), V_1(s_{\\mathrm{D}}), V_1(s_{\\mathrm{S}}))$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11.486  4.904  3.296\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在许多现实场景中，我们必须使用在先前不同策略下收集的数据来评估一个新的控制策略。本练习  通过应用重要性采样，深入探讨了离策略评估的核心。您将为给定的轨迹计算普通重要性采样和加权重要性采样两种估计值，从而具体理解如何校正行为策略与目标策略之间的分布差异。",
            "id": "4239977",
            "problem": "一个信息物理系统（CPS）的数字孪生（DT）将一个离散时间控制过程建模为马尔可夫决策过程（MDP），其状态空间为 $\\mathcal{S}$，动作空间为 $\\mathcal{A}$，转移核为 $p(s_{t+1} \\mid s_t, a_t)$，奖励函数为 $r(s_t,a_t)$。考虑使用在行为策略 $b(a \\mid s)$ 下收集的单条有限时域轨迹，通过离策略评估来估计目标策略 $\\pi(a \\mid s)$ 在初始状态 $s_0$ 的期望折扣回报。该轨迹长度为 $T=3$，时间索引为 $t=0,1,2$，由以下实现的状态-动作-奖励序列组成：\n- $s_0 \\xrightarrow{a_0} s_1$，奖励为 $r_0 = 1.2$，\n- $s_1 \\xrightarrow{a_1} s_2$，奖励为 $r_1 = -0.5$，\n- $s_2 \\xrightarrow{a_2} s_3$，奖励为 $r_2 = 2.0$。\n折扣因子为 $\\gamma = 0.9$。对于已实现的动作，行为策略的动作概率为 $b(a_0 \\mid s_0) = 0.7$，$b(a_1 \\mid s_1) = 0.2$ 和 $b(a_2 \\mid s_2) = 0.5$。对于相同的已实现动作，目标策略的动作概率为 $\\pi(a_0 \\mid s_0) = 0.5$，$\\pi(a_1 \\mid s_1) = 0.6$ 和 $\\pi(a_2 \\mid s_2) = 0.4$。\n\n从马尔可夫决策过程中期望回报的基本定义以及行为策略和目标策略所诱导的轨迹分布之间的测度变换原理出发，使用给定的单条轨迹，推导目标策略从 $s_0$ 开始的回报的幕级普通重要性采样估计和幕级加权（归一化）重要性采样估计。计算这两个数值。将两个估计值四舍五入到四位有效数字。按 $\\bigl[\\text{普通重要性采样},\\, \\text{加权重要性采样}\\bigr]$ 的顺序，将这两个数作为一个单行矩阵提供。回报是无量纲的；最终答案中不要包含任何单位。",
            "solution": "该问题要求从一个在不同行为策略 $b$ 下生成的单条轨迹中，估计目标策略 $\\pi$ 的价值，记为 $V^{\\pi}(s_0)$。价值函数定义为期望折扣回报：$V^{\\pi}(s_0) = \\mathbb{E}_{\\pi}[G_0 \\mid S_0 = s_0]$，其中 $G_0$ 是从时间 $t=0$ 开始的回报。\n\n对于一条长度为 $T$ 的有限时域轨迹，其回报是折扣奖励的总和：\n$$G_0 = \\sum_{t=0}^{T-1} \\gamma^t R_{t+1} = \\sum_{t=0}^{T-1} \\gamma^t r(s_t, a_t)$$\n在这个问题中，轨迹长度为 $T=3$，在时间步 $t=0, 1, 2$ 收集奖励。折扣因子为 $\\gamma = 0.9$。奖励分别为 $r_0 = 1.2$，$r_1 = -0.5$ 和 $r_2 = 2.0$。这条轨迹的已实现回报是：\n$$G_0 = \\gamma^0 r_0 + \\gamma^1 r_1 + \\gamma^2 r_2$$\n$$G_0 = (1)(1.2) + (0.9)(-0.5) + (0.9)^2(2.0)$$\n$$G_0 = 1.2 - 0.45 + (0.81)(2.0)$$\n$$G_0 = 1.2 - 0.45 + 1.62$$\n$$G_0 = 2.37$$\n\n离策略评估使用重要性采样来修正数据来自不同策略这一事实。其核心思想是通过将在目标策略 $\\pi$ 与行为策略 $b$ 下观测到给定轨迹的概率之比，来重新加权观测到的回报。一条状态-动作轨迹 $\\tau = (s_0, a_0, s_1, a_1, \\dots, s_{T-1}, a_{T-1})$ 的概率只取决于策略，因为状态转移动态 $p(s_{t+1} \\mid s_t, a_t)$ 对两者是相同的。重要性采样比 $\\rho$ 为：\n$$\\rho = \\frac{\\prod_{t=0}^{T-1} \\pi(a_t \\mid s_t) p(s_{t+1} \\mid s_t, a_t)}{\\prod_{t=0}^{T-1} b(a_t \\mid s_t) p(s_{t+1} \\mid s_t, a_t)} = \\prod_{t=0}^{T-1} \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}$$\n对于给定的长度为 $T=3$（索引为 $t=0, 1, 2$）的轨迹，该比率为：\n$$\\rho = \\frac{\\pi(a_0 \\mid s_0)}{b(a_0 \\mid s_0)} \\cdot \\frac{\\pi(a_1 \\mid s_1)}{b(a_1 \\mid s_1)} \\cdot \\frac{\\pi(a_2 \\mid s_2)}{b(a_2 \\mid s_2)}$$\n使用所提供的策略概率：\n$$b(a_0 \\mid s_0) = 0.7, \\quad \\pi(a_0 \\mid s_0) = 0.5$$\n$$b(a_1 \\mid s_1) = 0.2, \\quad \\pi(a_1 \\mid s_1) = 0.6$$\n$$b(a_2 \\mid s_2) = 0.5, \\quad \\pi(a_2 \\mid s_2) = 0.4$$\n重要性采样比的数值为：\n$$\\rho = \\frac{0.5}{0.7} \\times \\frac{0.6}{0.2} \\times \\frac{0.4}{0.5} = \\frac{5}{7} \\times 3 \\times \\frac{4}{5} = \\frac{12}{7}$$\n\n对于单条轨迹，普通重要性采样（OIS）估计 $V_{OIS}(s_0)$ 是重要性采样比与观测回报的乘积。该估计量是无偏的，即 $\\mathbb{E}[V_{OIS}(s_0)] = V^{\\pi}(s_0)$。\n$$V_{OIS}(s_0) = \\rho G_0$$\n$$V_{OIS}(s_0) = \\frac{12}{7} \\times 2.37 = \\frac{28.44}{7} \\approx 4.062857...$$\n\n加权重要性采样（WIS）估计 $V_{WIS}(s_0)$ 对一个包含 $N$ 条轨迹的集合 $\\{\\tau_i\\}_{i=1}^N$ 定义为：\n$$V_{WIS}(s_0) = \\frac{\\sum_{i=1}^N \\rho_i G_{0,i}}{\\sum_{i=1}^N \\rho_i}$$\n对于单条轨迹（$N=1$）的情况，该公式可显著简化：\n$$V_{WIS}(s_0) = \\frac{\\rho G_0}{\\rho} = G_0$$\nWIS 估计量通常是有偏的（对于有限的 $N$），但其方差远低于 OIS 估计量，这使其在实践中更为可靠。对于我们的单条轨迹，WIS 估计就是已实现的回报：\n$$V_{WIS}(s_0) = G_0 = 2.37$$\n\n问题要求将两个估计值四舍五入到四位有效数字。\n对于 OIS 估计：\n$$V_{OIS} \\approx 4.062857... \\rightarrow 4.063$$\n对于 WIS 估计：\n$$V_{WIS} = 2.37$$\n为了用四位有效数字表示 $2.37$，我们将其写为 $2.370$。\n\n最终答案是一个按该顺序包含 OIS 和 WIS 估计值的行矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.063  2.370\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "诸如近端策略优化 (PPO) 等稳定高效的策略梯度方法，已经彻底改变了现代强化学习。本问题  提供了一个动手实践的机会，让您深入了解 PPO 更新步骤的内部机制。通过为一个小型批次数据计算裁剪代理目标的梯度，您将精确地看到 PPO 如何约束策略变化以确保学习的稳定性，这是其广泛成功背后的关键原则。",
            "id": "4240043",
            "problem": "一个信息物理系统（CPS）的网络化数字孪生被用于通过强化学习进行最优控制，从而为一个欠驱动机器人连杆合成反馈控制器。该控制器是一个带有单一标量参数 $\\,\\theta\\,$ 的随机策略 $\\,\\pi_{\\theta}(a \\mid s)\\,$。在训练迭代 $\\,k\\,$ 时，您将使用近端策略优化（PPO）执行一次策略更新，其中策略目标是裁剪替代目标。请从马尔可夫决策过程（MDPs）、策略梯度定理和似然比梯度估计器的基本定义出发，然后推导出用于单个小批量更新的 PPO 裁剪替代梯度。\n\n给定一个在旧策略 $\\,\\pi_{\\theta_{\\text{old}}}\\,$下收集的大小为 $\\,T=5\\,$ 的小批量样本。对于每个样本 $\\,t \\in \\{1,\\dots,5\\}\\,$，给定在当前参数 $\\,\\theta=\\theta_0\\,$ 处评估的重要性采样比 $\\,r_t = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}\\,$、广义优势估计 $\\,\\hat{A}_t\\,$ 以及策略得分 $\\,g_t = \\nabla_{\\theta}\\ln \\pi_{\\theta}(a_t \\mid s_t)\\,$。裁剪参数为 $\\,\\epsilon = 0.2\\,$。学习率为 $\\,\\alpha = 0.1\\,$。假设此步骤中为纯策略优化，不含熵奖励和价值函数项。\n\n已知：\n- 初始参数：$\\,\\theta_0 = 0\\,$。\n- 裁剪参数：$\\,\\epsilon = 0.2\\,$。\n- 学习率：$\\,\\alpha = 0.1\\,$。\n- 大小为 $\\,T=5\\,$ 的小批量，包含以下在 $\\,\\theta=\\theta_0\\,$ 处评估的元组 $\\,(r_t,\\ \\hat{A}_t,\\ g_t)\\,$：\n  - $\\,t=1:\\ (\\,r_1=1.10,\\ \\hat{A}_1=2.0,\\ g_1=0.5\\,)$\n  - $\\,t=2:\\ (\\,r_2=1.25,\\ \\hat{A}_2=1.5,\\ g_2=1.0\\,)$\n  - $\\,t=3:\\ (\\,r_3=0.75,\\ \\hat{A}_3=-1.0,\\ g_3=-0.4\\,)$\n  - $\\,t=4:\\ (\\,r_4=0.95,\\ \\hat{A}_4=-0.5,\\ g_4=0.2\\,)$\n  - $\\,t=5:\\ (\\,r_5=1.05,\\ \\hat{A}_5=0.8,\\ g_5=-0.4\\,)$\n\n使用近端策略优化的裁剪替代目标和从此小批量计算出的单步梯度上升，计算从 $\\,\\theta_0\\,$ 得到的更新后参数 $\\,\\theta_1\\,$。将最终更新后的参数值 $\\,\\theta_1\\,$ 表示为一个纯数。将您的答案四舍五入到四位有效数字。",
            "solution": "用户要求我解决一个强化学习问题，该问题涉及使用近端策略优化（PPO）算法执行单步策略更新。\n\n### 步骤1：提取已知条件\n- **任务**：使用 PPO 裁剪替代目标上的单步梯度上升，从初始参数 $\\theta_0$ 计算更新后的策略参数 $\\theta_1$。\n- **策略**：带有单一标量参数 $\\theta$ 的随机策略 $\\pi_{\\theta}(a \\mid s)$。\n- **算法**：近端策略优化（PPO）。\n- **目标函数**：裁剪替代目标。\n- **初始参数**：$\\theta_0 = 0$。\n- **学习率**：$\\alpha = 0.1$。\n- **裁剪参数**：$\\epsilon = 0.2$。\n- **小批量大小**：$T=5$。\n- **小批量数据**：一组在旧策略 $\\pi_{\\theta_{\\text{old}}}$ 下收集、并在当前参数 $\\theta = \\theta_0$ 处评估的大小为 $T=5$ 的元组 $(r_t, \\hat{A}_t, g_t)$。\n  - $r_t = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}\\bigg|_{\\theta=\\theta_0}$ 是重要性采样比。\n  - $\\hat{A}_t$ 是广义优势估计。\n  - $g_t = \\nabla_{\\theta}\\ln \\pi_{\\theta}(a_t \\mid s_t)\\big|_{\\theta=\\theta_0}$ 是策略得分函数。\n- **数据点**：\n  - $t=1: (r_1=1.10, \\hat{A}_1=2.0, g_1=0.5)$\n  - $t=2: (r_2=1.25, \\hat{A}_2=1.5, g_2=1.0)$\n  - $t=3: (r_3=0.75, \\hat{A}_3=-1.0, g_3=-0.4)$\n  - $t=4: (r_4=0.95, \\hat{A}_4=-0.5, g_4=0.2)$\n  - $t=5: (r_5=1.05, \\hat{A}_5=0.8, g_5=-0.4)$\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题基于成熟的近端策略优化（PPO）算法，这是强化学习中的一种标准方法。所有概念，如马尔可夫决策过程、策略梯度、重要性采样和优势估计，都是该领域的基础。所提供的数字孪生和信息物理系统的背景是这些技术的常见且现实的应用领域。该问题在科学上和事实上都是合理的。\n- **适定性**：该问题提供了计算更新参数 $\\theta_1$ 的唯一值所需的所有必要数据（$\\theta_0$、$\\alpha$、$\\epsilon$ 和小批量样本）。任务定义清晰。\n- **客观性**：问题以精确、定量的术语陈述，没有歧义或主观性陈述。\n\n### 步骤3：结论与行动\n问题有效。我将继续进行求解。\n\n### 求解推导\n\n强化学习中策略梯度方法的目标是通过最大化一个目标函数 $J(\\theta)$ 来优化策略 $\\pi_{\\theta}(a \\mid s)$，该目标函数通常表示预期的总折扣奖励。策略梯度定理为该目标的梯度提供了一个表达式：\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) A^{\\pi_{\\theta}}(s_t, a_t) \\right]\n$$\n其中 $\\tau$ 是通过遵循策略 $\\pi_{\\theta}$ 采样得到的轨迹 $(s_0, a_0, s_1, a_1, \\dots)$，$A^{\\pi_{\\theta}}(s_t, a_t)$ 是优势函数，它衡量在状态 $s_t$ 下，动作 $a_t$ 相对于平均动作的好坏程度。\n\nPPO 是一种同策略（on-policy）算法，但它可以使用从一个稍旧版本的策略 $\\pi_{\\theta_{\\text{old}}}$ 收集的数据来更新当前策略 $\\pi_{\\theta}$。这是通过重要性采样实现的。比率 $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$ 对每个样本的贡献进行重新加权。目标函数变为：\n$$\nJ^{\\text{IS}}(\\theta) = \\mathbb{E}_{t \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ r_t(\\theta) \\hat{A}_t \\right]\n$$\n其中 $\\hat{A}_t$ 是优势函数的一个估计。\n\n对 $J^{\\text{IS}}(\\theta)$ 进行无约束优化可能导致过大的策略更新，从而引起不稳定性。PPO 通过引入一个裁剪替代目标来解决这个问题，该目标不鼓励策略比率 $r_t(\\theta)$ 发生大的变化。对于一个小批量，PPO 裁剪替代目标是：\n$$\nL^{\\text{CLIP}}(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\min\\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right)\n$$\n其中 $\\epsilon$ 是一个定义裁剪范围的小超参数。\n\n我们需要计算这个目标关于 $\\theta$ 的梯度，并在当前参数 $\\theta_0$ 处对其进行评估。该梯度用于执行单步梯度上升：\n$$\n\\theta_1 = \\theta_0 + \\alpha \\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0}\n$$\n\n比率 $r_t(\\theta)$ 的梯度使用对数求导技巧求得：\n$$\n\\nabla_{\\theta} r_t(\\theta) = \\nabla_{\\theta} \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) = r_t(\\theta) g_t(\\theta)\n$$\n其中 $g_t(\\theta) = \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)$。在 $\\theta=\\theta_0$ 处，未裁剪项 $r_t(\\theta) \\hat{A}_t$ 的梯度是 $r_t(\\theta_0) g_t(\\theta_0) \\hat{A}_t$。问题将这些值作为 $r_t$、$g_t$ 和 $\\hat{A}_t$ 提供。\n\n完整目标 $L^{\\text{CLIP}}$ 的梯度取决于对于每个样本 $t$，`min` 函数中的哪一项是激活的。我们根据优势估计 $\\hat{A}_t$ 的符号来分析这一点。让我们在给定值适用的 $\\theta = \\theta_0$ 处评估每个样本。$r_t$ 的裁剪区间是 $[1-\\epsilon, 1+\\epsilon] = [1-0.2, 1+0.2] = [0.8, 1.2]$。\n\n来自每个样本 $t$ 的梯度贡献 $G_t$ 仅在策略更新不是“过大”时才为非零。\n- 如果 $\\hat{A}_t \\ge 0$，我们希望增加动作 $a_t$ 的概率，但幅度不能太大。只有当 $r_t \\le 1+\\epsilon$ 时，梯度才取自未裁剪项。如果 $r_t  1+\\epsilon$，目标被裁剪，梯度变为 $0$ 以防止进一步增加。\n- 如果 $\\hat{A}_t  0$，我们希望减小动作 $a_t$ 的概率，但幅度不能太大。只有当 $r_t \\ge 1-\\epsilon$ 时，梯度才取自未裁剪项。如果 $r_t  1-\\epsilon$，目标被裁剪，梯度变为 $0$ 以防止进一步减小。\n\n因此，样本 $t$ 的梯度贡献为：\n$$\nG_t = \\begin{cases} r_t g_t \\hat{A}_t  \\text{如果 } (\\hat{A}_t \\ge 0 \\text{ 且 } r_t \\le 1+\\epsilon) \\text{ 或 } (\\hat{A}_t  0 \\text{ 且 } r_t \\ge 1-\\epsilon) \\\\ 0  \\text{其他情况} \\end{cases}\n$$\n\n现在，我们为小批量中的每个样本计算 $G_t$：\n\n- **样本 $t=1$**：$(r_1=1.10, \\hat{A}_1=2.0, g_1=0.5)$\n  - $\\hat{A}_1 = 2.0 > 0$。我们检查条件 $r_1 \\le 1+\\epsilon$。\n  - $1.10 \\le 1.2$，成立。该项未被裁剪。\n  - $G_1 = r_1 g_1 \\hat{A}_1 = 1.10 \\times 0.5 \\times 2.0 = 1.1$。\n\n- **样本 $t=2$**：$(r_2=1.25, \\hat{A}_2=1.5, g_2=1.0)$\n  - $\\hat{A}_2 = 1.5 > 0$。我们检查条件 $r_2 \\le 1+\\epsilon$。\n  - $1.25 \\le 1.2$，不成立。该项被裁剪。\n  - $G_2 = 0$。\n\n- **样本 $t=3$**：$(r_3=0.75, \\hat{A}_3=-1.0, g_3=-0.4)$\n  - $\\hat{A}_3 = -1.0  0$。我们检查条件 $r_3 \\ge 1-\\epsilon$。\n  - $0.75 \\ge 0.8$，不成立。该项被裁剪。\n  - $G_3 = 0$。\n\n- **样本 $t=4$**：$(r_4=0.95, \\hat{A}_4=-0.5, g_4=0.2)$\n  - $\\hat{A}_4 = -0.5  0$。我们检查条件 $r_4 \\ge 1-\\epsilon$。\n  - $0.95 \\ge 0.8$，成立。该项未被裁剪。\n  - $G_4 = r_4 g_4 \\hat{A}_4 = 0.95 \\times 0.2 \\times (-0.5) = -0.095$。\n\n- **样本 $t=5$**：$(r_5=1.05, \\hat{A}_5=0.8, g_5=-0.4)$\n  - $\\hat{A}_5 = 0.8 > 0$。我们检查条件 $r_5 \\le 1+\\epsilon$。\n  - $1.05 \\le 1.2$，成立。该项未被裁剪。\n  - $G_5 = r_5 g_5 \\hat{A}_5 = 1.05 \\times (-0.4) \\times 0.8 = -0.336$。\n\n该小批量的总梯度是各个贡献的平均值：\n$$\n\\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0} = \\frac{1}{T} \\sum_{t=1}^{T} G_t = \\frac{1}{5} (G_1 + G_2 + G_3 + G_4 + G_5)\n$$\n$$\n\\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0} = \\frac{1}{5} (1.1 + 0 + 0 - 0.095 - 0.336) = \\frac{1}{5} (0.669) = 0.1338\n$$\n\n最后，我们使用学习率 $\\alpha=0.1$ 执行梯度上升更新步骤：\n$$\n\\theta_1 = \\theta_0 + \\alpha \\nabla_{\\theta} L^{\\text{CLIP}}(\\theta) \\bigg|_{\\theta=\\theta_0}\n$$\n$$\n\\theta_1 = 0 + 0.1 \\times 0.1338\n$$\n$$\n\\theta_1 = 0.01338\n$$\n\n更新后的参数是 $0.01338$。该值有四位有效数字，因此无需进一步四舍五入。",
            "answer": "$$\n\\boxed{0.01338}\n$$"
        }
    ]
}