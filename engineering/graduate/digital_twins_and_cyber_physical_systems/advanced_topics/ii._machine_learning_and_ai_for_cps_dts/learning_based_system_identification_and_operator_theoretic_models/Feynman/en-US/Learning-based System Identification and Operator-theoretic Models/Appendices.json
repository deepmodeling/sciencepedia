{
    "hands_on_practices": [
        {
            "introduction": "At the heart of learning-based system identification lies the task of fitting a model to observed data. This first practice grounds us in the most fundamental technique: the method of least squares. By working through the process of estimating parameters for a simple linear model from a set of input-output pairs, you will solidify your understanding of how to formulate and solve a basic identification problem . This exercise serves as a crucial building block, illustrating how a linear operator mapping inputs to outputs can be learned by minimizing an error functional.",
            "id": "4229220",
            "problem": "A cyber-physical system digital twin for a single-input single-output (SISO) component seeks to learn a static input-to-output mapping using a linear model. The learning task is framed in terms of operator-theoretic modeling, where the output is approximated by a linear functional acting on a finite set of lifted features. Consider the feature map $\\phi(u)=[u,\\,1]$ and the parametric model $y_t = \\theta_1 u_t + \\theta_2$ for discrete-time samples indexed by $t$. You are given the input-output data $\\{(u_t,y_t)\\}_{t=1}^5$ with $(u_1,y_1)=(-2,-3)$, $(u_2,y_2)=(-1,-1)$, $(u_3,y_3)=(0,0)$, $(u_4,y_4)=(1,3)$, and $(u_5,y_5)=(2,6)$.\n\nStarting from the fundamental principle that the least-squares estimator minimizes the empirical squared error functional, formulate the identification problem as minimizing the sum of squared residuals and derive the conditions that characterize the minimizer. Then compute the parameter estimate $(\\theta_1, \\theta_2)$ that solves this problem. Express the final estimate as exact rational numbers if applicable. No rounding is required. The final answer must be a single analytical expression.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It represents a standard application of the method of least squares for linear parameter estimation, a fundamental task in system identification and machine learning.\n\nThe problem asks for the determination of the parameters $\\theta_1$ and $\\theta_2$ for the linear model $y_t = \\theta_1 u_t + \\theta_2$ that best fit the given data in a least-squares sense. The provided data consists of $N=5$ input-output pairs $\\{(u_t, y_t)\\}_{t=1}^5$:\n$(u_1, y_1) = (-2, -3)$\n$(u_2, y_2) = (-1, -1)$\n$(u_3, y_3) = (0, 0)$\n$(u_4, y_4) = (1, 3)$\n$(u_5, y_5) = (2, 6)$\n\nThe learning task is framed as minimizing the empirical squared error functional, which is the sum of the squared residuals. The residual for each data point is the difference between the observed output $y_t$ and the model's predicted output, $\\hat{y}_t = \\theta_1 u_t + \\theta_2$. The objective function to be minimized is thus:\n$$ J(\\theta_1, \\theta_2) = \\sum_{t=1}^{5} (y_t - \\hat{y}_t)^2 = \\sum_{t=1}^{5} (y_t - (\\theta_1 u_t + \\theta_2))^2 $$\n\nTo find the values of $\\theta_1$ and $\\theta_2$ that minimize this functional, we must find the point where the gradient of $J$ with respect to the parameter vector $\\theta = [\\theta_1, \\theta_2]$ is zero. This requires computing the partial derivatives of $J$ with respect to $\\theta_1$ and $\\theta_2$ and setting them to zero. These are the necessary conditions for a minimum.\n\nFirst, the partial derivative with respect to $\\theta_1$:\n$$ \\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\sum_{t=1}^{5} (y_t - \\theta_1 u_t - \\theta_2)^2 = \\sum_{t=1}^{5} 2(y_t - \\theta_1 u_t - \\theta_2)(-u_t) $$\nSetting this to zero:\n$$ \\sum_{t=1}^{5} -2 u_t (y_t - \\theta_1 u_t - \\theta_2) = 0 $$\n$$ \\sum_{t=1}^{5} (u_t y_t - \\theta_1 u_t^2 - \\theta_2 u_t) = 0 $$\n$$ (\\sum_{t=1}^{5} u_t^2) \\theta_1 + (\\sum_{t=1}^{5} u_t) \\theta_2 = \\sum_{t=1}^{5} u_t y_t \\quad (1) $$\n\nNext, the partial derivative with respect to $\\theta_2$:\n$$ \\frac{\\partial J}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2} \\sum_{t=1}^{5} (y_t - \\theta_1 u_t - \\theta_2)^2 = \\sum_{t=1}^{5} 2(y_t - \\theta_1 u_t - \\theta_2)(-1) $$\nSetting this to zero:\n$$ \\sum_{t=1}^{5} -2 (y_t - \\theta_1 u_t - \\theta_2) = 0 $$\n$$ \\sum_{t=1}^{5} (y_t - \\theta_1 u_t - \\theta_2) = 0 $$\n$$ (\\sum_{t=1}^{5} u_t) \\theta_1 + (\\sum_{t=1}^{5} 1) \\theta_2 = \\sum_{t=1}^{5} y_t $$\nSince $\\sum_{t=1}^{5} 1 = N = 5$, we have:\n$$ (\\sum_{t=1}^{5} u_t) \\theta_1 + 5 \\theta_2 = \\sum_{t=1}^{5} y_t \\quad (2) $$\n\nEquations $(1)$ and $(2)$ are the normal equations. To solve for $\\theta_1$ and $\\theta_2$, we need to compute the sums from the given data:\n- $\\sum_{t=1}^{5} u_t = -2 - 1 + 0 + 1 + 2 = 0$\n- $\\sum_{t=1}^{5} y_t = -3 - 1 + 0 + 3 + 6 = 5$\n- $\\sum_{t=1}^{5} u_t^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10$\n- $\\sum_{t=1}^{5} u_t y_t = (-2)(-3) + (-1)(-1) + (0)(0) + (1)(3) + (2)(6) = 6 + 1 + 0 + 3 + 12 = 22$\n\nSubstituting these values into the normal equations:\nFrom equation $(1)$:\n$$ (10) \\theta_1 + (0) \\theta_2 = 22 $$\n$$ 10 \\theta_1 = 22 $$\n$$ \\theta_1 = \\frac{22}{10} = \\frac{11}{5} $$\n\nFrom equation $(2)$:\n$$ (0) \\theta_1 + 5 \\theta_2 = 5 $$\n$$ 5 \\theta_2 = 5 $$\n$$ \\theta_2 = 1 $$\n\nThus, the parameter estimate that minimizes the sum of squared residuals is $(\\theta_1, \\theta_2) = (\\frac{11}{5}, 1)$.\nThe linear model is $y_t = \\frac{11}{5} u_t + 1$.\nThe problem can also be expressed in matrix form. Let the feature matrix be $X$ and the output vector be $Y$:\n$$ X = \\begin{pmatrix} u_1 & 1 \\\\ u_2 & 1 \\\\ u_3 & 1 \\\\ u_4 & 1 \\\\ u_5 & 1 \\end{pmatrix} = \\begin{pmatrix} -2 & 1 \\\\ -1 & 1 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ 6 \\end{pmatrix} $$\nThe normal equations are $(X^T X) \\theta = X^T Y$, where $\\theta = \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix}$.\n$$ X^T X = \\begin{pmatrix} \\sum u_t^2 & \\sum u_t \\\\ \\sum u_t & N \\end{pmatrix} = \\begin{pmatrix} 10 & 0 \\\\ 0 & 5 \\end{pmatrix} $$\n$$ X^T Y = \\begin{pmatrix} \\sum u_t y_t \\\\ \\sum y_t \\end{pmatrix} = \\begin{pmatrix} 22 \\\\ 5 \\end{pmatrix} $$\nThe system to solve is:\n$$ \\begin{pmatrix} 10 & 0 \\\\ 0 & 5 \\end{pmatrix} \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} = \\begin{pmatrix} 22 \\\\ 5 \\end{pmatrix} $$\nThis yields $10\\theta_1 = 22$ and $5\\theta_2 = 5$, giving the same solution: $\\theta_1 = \\frac{11}{5}$ and $\\theta_2 = 1$.\nThe final estimate is expressed as exact rational numbers as required.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{11}{5} & 1 \\end{pmatrix} } $$"
        },
        {
            "introduction": "While least squares is powerful for linear systems, many real-world phenomena are nonlinear. The Koopman operator provides a remarkable framework for analyzing such systems by lifting them into an infinite-dimensional space where their dynamics become linear. This exercise provides a concrete, hands-on calculation of a finite-dimensional approximation to the Koopman operator for a simple nonlinear map . By applying Carleman linearization, you will directly compute the matrix representing the operator's action on a basis of polynomial observables, gaining a practical understanding of how this powerful theoretical tool is applied.",
            "id": "4229160",
            "problem": "A digital twin for a scalar nonlinear discrete-time cyber-physical system uses an operator-theoretic surrogate based on the Koopman operator. Consider the map $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\alpha x+\\beta x^{2}$, where $\\alpha\\in\\mathbb{R}$ and $\\beta\\in\\mathbb{R}$ are fixed parameters. The Koopman operator $\\mathcal{K}$ acts on an observable $g:\\mathbb{R}\\to\\mathbb{R}$ by $(\\mathcal{K}g)(x)=g(f(x))$. To build a finite-dimensional surrogate, use the Carleman linearization truncated at total polynomial degree $2$: restrict attention to the dictionary $\\Psi(x)=[1,\\,x,\\,x^{2}]^{\\top}$ and define the truncated operator $\\mathcal{K}_{2}$ as the unique linear operator on $\\mathrm{span}\\{1,x,x^{2}\\}$ such that for each basis observable $g\\in\\{1,x,x^{2}\\}$, the action $\\mathcal{K}_{2}g$ equals the composition $g\\circ f$ followed by discarding all monomials of degree greater than $2$.\n\nStarting from the fundamental definition of the Koopman operator and the Carleman truncation principle, derive the matrix representation of $\\mathcal{K}_{2}$ in the ordered basis $\\{1,x,x^{2}\\}$ and compute its determinant. Provide your final answer as a single closed-form analytic expression in terms of $\\alpha$ and $\\beta$. No numerical evaluation or rounding is required, and no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in operator theory, well-posed, objective, and self-contained. There are no scientific or factual unsoundnesses, no ambiguities, and the required task is a direct application of the provided definitions.\n\nThe objective is to find the matrix representation of the truncated Koopman operator $\\mathcal{K}_{2}$ with respect to the ordered basis $\\{1, x, x^{2}\\}$ and then compute its determinant. Let the basis functions be denoted as $\\phi_1(x) = 1$, $\\phi_2(x) = x$, and $\\phi_3(x) = x^{2}$. The matrix representation, which we will call $K_2$, is a $3 \\times 3$ matrix whose $j$-th column is the coordinate vector of the transformed basis function $\\mathcal{K}_{2}(\\phi_j)$ with respect to the basis $\\{\\phi_1, \\phi_2, \\phi_3\\}$.\n\nThe procedure is as follows: for each basis function $\\phi_j$ in $\\{1, x, x^{2}\\}$, we first compute the action of the full Koopman operator, $(\\mathcal{K}\\phi_j)(x) = \\phi_j(f(x))$, where $f(x) = \\alpha x + \\beta x^{2}$. Then, we apply the truncation rule, which involves discarding all terms with a power of $x$ greater than $2$. The resulting polynomial is $\\mathcal{K}_{2}(\\phi_j)$. Finally, we express this polynomial as a linear combination of the basis functions to find the corresponding column of the matrix $K_2$.\n\nLet's compute the action of $\\mathcal{K}_{2}$ on each basis function.\n\n1.  Action on the first basis function, $\\phi_1(x) = 1$:\n    The action of the Koopman operator is $(\\mathcal{K}\\phi_1)(x) = \\phi_1(f(x)) = \\phi_1(\\alpha x + \\beta x^{2}) = 1$.\n    This is a polynomial of degree $0$, which is less than or equal to $2$. Therefore, no truncation is needed.\n    $$ \\mathcal{K}_{2}(\\phi_1)(x) = 1 $$\n    Expressing this result in the basis $\\{\\phi_1, \\phi_2, \\phi_3\\}$:\n    $$ \\mathcal{K}_{2}(\\phi_1)(x) = 1 \\cdot \\phi_1(x) + 0 \\cdot \\phi_2(x) + 0 \\cdot \\phi_3(x) $$\n    The coordinate vector is $[1, 0, 0]^{\\top}$. This forms the first column of the matrix $K_2$.\n\n2.  Action on the second basis function, $\\phi_2(x) = x$:\n    The action of the Koopman operator is $(\\mathcal{K}\\phi_2)(x) = \\phi_2(f(x)) = f(x) = \\alpha x + \\beta x^{2}$.\n    All terms in this polynomial have a degree less than or equal to $2$. No truncation is needed.\n    $$ \\mathcal{K}_{2}(\\phi_2)(x) = \\alpha x + \\beta x^{2} $$\n    Expressing this result in the basis $\\{\\phi_1, \\phi_2, \\phi_3\\}$:\n    $$ \\mathcal{K}_{2}(\\phi_2)(x) = 0 \\cdot \\phi_1(x) + \\alpha \\cdot \\phi_2(x) + \\beta \\cdot \\phi_3(x) $$\n    The coordinate vector is $[0, \\alpha, \\beta]^{\\top}$. This forms the second column of the matrix $K_2$.\n\n3.  Action on the third basis function, $\\phi_3(x) = x^{2}$:\n    The action of the Koopman operator is $(\\mathcal{K}\\phi_3)(x) = \\phi_3(f(x)) = (f(x))^{2} = (\\alpha x + \\beta x^{2})^{2}$.\n    We expand this expression:\n    $$ (\\alpha x + \\beta x^{2})^{2} = (\\alpha x)^{2} + 2(\\alpha x)(\\beta x^{2}) + (\\beta x^{2})^{2} = \\alpha^{2}x^{2} + 2\\alpha\\beta x^{3} + \\beta^{2}x^{4} $$\n    Now, we apply the truncation rule by discarding all monomials of degree greater than $2$. The terms $2\\alpha\\beta x^{3}$ (degree $3$) and $\\beta^{2}x^{4}$ (degree $4$) are discarded.\n    The result after truncation is:\n    $$ \\mathcal{K}_{2}(\\phi_3)(x) = \\alpha^{2}x^{2} $$\n    Expressing this result in the basis $\\{\\phi_1, \\phi_2, \\phi_3\\}$:\n    $$ \\mathcal{K}_{2}(\\phi_3)(x) = 0 \\cdot \\phi_1(x) + 0 \\cdot \\phi_2(x) + \\alpha^{2} \\cdot \\phi_3(x) $$\n    The coordinate vector is $[0, 0, \\alpha^{2}]^{\\top}$. This forms the third column of the matrix $K_2$.\n\nAssembling the matrix $K_2$ from its column vectors:\nThe first column is $[1, 0, 0]^{\\top}$.\nThe second column is $[0, \\alpha, \\beta]^{\\top}$.\nThe third column is $[0, 0, \\alpha^{2}]^{\\top}$.\n\nThe resulting matrix $K_2$ is:\n$$\nK_2 = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & \\alpha & 0 \\\\\n0 & \\beta & \\alpha^{2}\n\\end{pmatrix}\n$$\n\nThe final step is to compute the determinant of this matrix. The matrix $K_2$ is a lower triangular matrix. The determinant of a triangular matrix (either upper or lower) is the product of its diagonal entries.\n$$\n\\det(K_2) = (1) \\cdot (\\alpha) \\cdot (\\alpha^{2})\n$$\n$$\n\\det(K_2) = \\alpha^{3}\n$$\nThe determinant of the matrix representation of the truncated Koopman operator $\\mathcal{K}_{2}$ is $\\alpha^{3}$.",
            "answer": "$$\\boxed{\\alpha^{3}}$$"
        },
        {
            "introduction": "This final practice integrates the preceding concepts into a complete, data-driven workflow for discovering dynamical modes from a time series. You will implement Dynamic Mode Decomposition (DMD), a powerful algorithm that approximates the Koopman operator directly from measurement data without requiring an explicit model of the system. This coding exercise involves using delay embeddings to reconstruct the system's state space and then applying DMD to extract the dominant frequencies and coherent structures . This is a capstone practice that demonstrates how operator-theoretic methods are used to build powerful predictive models for complex cyber-physical systems.",
            "id": "4229200",
            "problem": "You are given the task of implementing a learning-based system identification workflow using delay embeddings and operator-theoretic models for a scalar time series. The goal is to reconstruct quasi-periodic modes of a system via Dynamic Mode Decomposition (DMD) on a delay-embedded Hankel matrix, thereby approximating the action of the Koopman operator on a finite-dimensional subspace.\n\nThe fundamental base for this problem consists of the following core definitions and well-tested facts:\n- A discrete-time scalar observable sampled from a continuous-time system yields a time series $x_n = x(t_n)$ with $t_n = n \\Delta t$ for integer $n \\geq 0$ and fixed sampling period $\\Delta t$.\n- Delay embedding constructs vectors of the form $\\psi_n = [x_n, x_{n+1}, \\dots, x_{n+m-1}]^\\top$ for an embedding dimension $m$, organizing these vectors as columns in a Hankel matrix.\n- Dynamic Mode Decomposition (DMD) approximates the action of the Koopman operator by finding a linear operator that best advances the embedded snapshots forward in time, in a least-squares sense over a finite-dimensional subspace obtained by singular value truncation.\n\nYour program must:\n1. For each test case, generate a scalar time series $x_n$ by sampling a sum of sinusoids of prescribed angular frequencies (in radians per second), amplitudes, and a specified sampling period $\\Delta t$ over $N$ time samples. The angle unit is radians. If noise is specified, add independent zero-mean Gaussian noise with the provided standard deviation to each sample.\n2. Construct the delay-embedded Hankel matrix $H \\in \\mathbb{R}^{m \\times L}$ where $m$ is the embedding dimension and $L = N - m + 1$, with entries defined by $H_{i,j} = x_{i + j - 1}$ for $i = 1, \\dots, m$ and $j = 1, \\dots, L$.\n3. Form two snapshot matrices $X$ and $Y$ by taking adjacent columns of $H$, namely $X = [h_1, h_2, \\dots, h_{L-1}]$ and $Y = [h_2, h_3, \\dots, h_L]$ where $h_j$ denotes the $j$-th column of $H$.\n4. Perform truncated Singular Value Decomposition (SVD) on $X$ with specified rank $r$ to construct a reduced-order linear operator on the embedded subspace, and carry out DMD to obtain eigenvalues that characterize the quasi-periodic modes.\n5. From the DMD eigenvalues, estimate the dominant modal angular frequencies in radians per second by interpreting the eigenspectrum as discrete-time propagators and mapping their arguments to continuous-time angular frequencies using the sampling period $\\Delta t$. Ensure that the angle unit is radians and the reported unit for frequency is radians per second (rad/s).\n6. Rank modes by their contribution to the initial embedded snapshot, select the top $k$ distinct positive angular frequencies (deduplicated within a small tolerance), and report them in decreasing order of modal contribution. Frequencies must be rounded to three decimals and expressed in radians per second.\n\nThe test suite consists of four test cases, each specified by parameters $(N, \\Delta t, \\{\\omega_j\\}, \\{a_j\\}, \\sigma, m, r, k)$ where:\n- $N$ is the number of samples.\n- $\\Delta t$ is the sampling period in seconds.\n- $\\{\\omega_j\\}$ are the sinusoidal angular frequencies (in radians per second).\n- $\\{a_j\\}$ are the corresponding amplitudes (dimensionless).\n- $\\sigma$ is the standard deviation of additive Gaussian noise (dimensionless).\n- $m$ is the embedding dimension.\n- $r$ is the SVD truncation rank.\n- $k$ is the number of dominant frequencies to return.\n\nUse the following parameter sets:\n- Case 1: $N = 500$, $\\Delta t = 0.01$ s, $\\{\\omega_1\\} = \\{\\pi\\}$ rad/s, $\\{a_1\\} = \\{1.0\\}$, $\\sigma = 0.0$, $m = 50$, $r = 20$, $k = 1$.\n- Case 2: $N = 800$, $\\Delta t = 0.01$ s, $\\{\\omega_1, \\omega_2\\} = \\{2\\pi \\cdot 0.7, 2\\pi \\cdot 1.2\\}$ rad/s, $\\{a_1, a_2\\} = \\{1.0, 0.5\\}$, $\\sigma = 0.01$, $m = 80$, $r = 40$, $k = 2$.\n- Case 3: $N = 900$, $\\Delta t = 0.01$ s, $\\{\\omega_1, \\omega_2\\} = \\{2\\pi \\cdot 2.0, 2\\pi \\cdot 2.2\\}$ rad/s, $\\{a_1, a_2\\} = \\{1.0, 0.8\\}$, $\\sigma = 0.005$, $m = 100$, $r = 60$, $k = 2$.\n- Case 4: $N = 120$, $\\Delta t = 0.01$ s, $\\{\\omega_1, \\omega_2\\} = \\{2\\pi \\cdot 1.5, 2\\pi \\cdot 3.0\\}$ rad/s, $\\{a_1, a_2\\} = \\{1.0, 0.3\\}$, $\\sigma = 0.02$, $m = 40$, $r = 10$, $k = 2$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item corresponds to one test case and is itself a list of the selected $k$ angular frequencies (in radians per second) rounded to three decimals. For example, the format must be like $[ [\\omega_{1,1}, \\dots], [\\omega_{2,1}, \\dots], \\dots ]$ with no additional text printed. All angles must be in radians, and all frequencies must be reported in radians per second.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of operator theory and system identification, and it is well-posed with a clear, algorithmically defined objective and complete parameter sets for all test cases. The procedure outlinedâ€”using delay embeddings to form a Hankel matrix, followed by Dynamic Mode Decomposition (DMD) to identify underlying frequenciesâ€”is a standard and powerful technique in the analysis of dynamical systems. We may therefore proceed with a formal solution.\n\n**1. Theoretical Framework: Delay Embedding and Koopman Operators**\n\nA discrete-time scalar time series $x_n = x(t_n)$ for $n=0, 1, \\dots, N-1$ and $t_n = n\\Delta t$ is often an observation of a more complex, higher-dimensional dynamical system. According to the Takens-Whitney embedding theorems, under certain general conditions, the dynamics of the original system can be faithfully reconstructed in a \"delay-coordinate space.\" We construct vectors in this space as $\\psi_n = [x_n, x_{n+1}, \\dots, x_{n+m-1}]^\\top \\in \\mathbb{R}^m$, where $m$ is the embedding dimension. For a sufficiently large $m$, the trajectory of these delay vectors, $\\psi_n \\to \\psi_{n+1}$, preserves the topological properties of the original system's attractor.\n\nThe Koopman operator, $\\mathcal{K}$, provides a linear perspective on nonlinear dynamics. It acts on observable functions $g$ of the system state $z$, evolving them forward in time: $(\\mathcal{K}g)(z_n) = g(z_{n+1})$. For quasi-periodic dynamics, the spectrum of $\\mathcal{K}$ contains eigenvalues on the unit circle, $e^{i\\omega_j \\Delta t}$, where $\\omega_j$ are the system's fundamental frequencies. DMD is a numerical method to find a finite-dimensional approximation of $\\mathcal{K}$ and its spectrum.\n\n**2. Algorithmic Procedure**\n\nThe implementation follows a sequence of well-defined steps to approximate the Koopman operator and extract its spectral information.\n\n**Step 2a: Hankel Matrix Construction**\nThe delay-embedded vectors $\\psi_n$ are organized as columns of a large Hankel matrix. We form the matrix $H \\in \\mathbb{R}^{m \\times L}$ where $L = N - m + 1$. The columns are the sequential delay vectors: $h_j = \\psi_{j-1}$ for $j=1, \\dots, L$. In terms of the original time series $x_n$ (with $0$-based indexing), the matrix entries are $H_{ij} = x_{i+j}$ (for $0$-based matrix indices $i, j$). Two snapshot matrices are then formed from adjacent columns of $H$:\n$$\nX = [h_1, h_2, \\dots, h_{L-1}] \\in \\mathbb{R}^{m \\times (L-1)}\n$$\n$$\nY = [h_2, h_3, \\dots, h_L] \\in \\mathbb{R}^{m \\times (L-1)}\n$$\nBy construction, these matrices satisfy the relationship $Y \\approx A X$, where $A \\in \\mathbb{R}^{m \\times m}$ is a finite-dimensional approximation of the Koopman operator acting on the space of delay-coordinate observables.\n\n**Step 2b: Reduced-Order DMD**\nDirectly computing $A = YX^\\dagger$ (where $X^\\dagger$ is the Moore-Penrose pseudoinverse) can be computationally expensive and sensitive to noise. Instead, we project the dynamics onto a lower-dimensional subspace that captures the most energy in the signal. This subspace is identified by the Singular Value Decomposition (SVD) of $X$:\n$$\nX \\approx U_r \\Sigma_r V_r^*\n$$\nHere, the columns of $U_r \\in \\mathbb{R}^{m \\times r}$ are the first $r$ Proper Orthogonal Modes (POD modes), $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of the top $r$ singular values, and $V_r \\in \\mathbb{R}^{(L-1) \\times r}$ contains the corresponding right singular vectors. The integer $r$ is the truncation rank.\n\nThe high-dimensional operator $A$ is projected onto the subspace spanned by $U_r$, yielding a low-dimensional operator $\\tilde{A} \\in \\mathbb{R}^{r \\times r}$:\n$$\n\\tilde{A} = U_r^* A U_r = U_r^* (Y X^\\dagger) U_r = U_r^* Y (V_r \\Sigma_r^{-1} U_r^*) U_r = U_r^* Y V_r \\Sigma_r^{-1}\n$$\nThe last step follows because $U_r^* U_r = I$. The matrix $\\tilde{A}$ captures the dynamics within the reduced-order subspace.\n\n**Step 2c: Eigenvalue and Frequency Analysis**\nThe eigenspectrum of $\\tilde{A}$ approximates the spectrum of the full operator $A$. We solve the eigenvalue problem:\n$$\n\\tilde{A} W = W \\Lambda\n$$\nwhere $\\Lambda$ is a diagonal matrix of eigenvalues $\\lambda_j$, and the columns of $W$ are the corresponding eigenvectors. Each eigenvalue $\\lambda_j$ is a discrete-time propagator. It relates to a continuous-time eigenvalue $z_j = \\gamma_j + i\\omega_j$ by $\\lambda_j = e^{z_j \\Delta t}$. The angular frequency $\\omega_j$ of the mode is thus extracted as:\n$$\n\\omega_j = \\frac{\\text{Im}(\\ln \\lambda_j)}{\\Delta t} = \\frac{\\text{arg}(\\lambda_j)}{\\Delta t}\n$$\n\n**Step 2d: Mode Contribution and Ranking**\nThe eigenvectors of the reduced operator, $W$, can be used to reconstruct the full-space DMD modes, which are approximations to the Koopman modes. A common choice for the DMD modes is $\\Phi = U_r W$. To determine the contribution of each mode to the system's observed behavior, we project the initial state, $\\psi_0 = X[:,0]$, onto this basis of DMD modes:\n$$\n\\psi_0 = \\sum_j b_j \\phi_j = \\Phi b\n$$\nThe vector of mode amplitudes, $b$, is found via a least-squares fit, typically using the pseudoinverse: \n$$\nb = \\Phi^\\dagger \\psi_0\n$$\nThe magnitude $|b_j|$ represents the contribution of the $j$-th mode to the initial state.\n\nThe modes are then ranked in descending order of $|b_j|$. We select the top $k$ modes corresponding to distinct positive frequencies, after a deduplication step to merge numerically close frequencies that arise from the same physical mode. The final result is a list of these dominant angular frequencies. For reproducibility when noise is present, the pseudo-random number generator is initialized with a fixed seed.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the DMD analysis for all specified test cases.\n    \"\"\"\n    # Use a fixed seed for the random number generator to ensure reproducibility.\n    rng = np.random.default_rng(seed=0)\n\n    test_cases = [\n        # Case 1: Single sinusoid, no noise\n        {'N': 500, 'dt': 0.01, 'omegas': [np.pi], 'amps': [1.0], 'sigma': 0.0, 'm': 50, 'r': 20, 'k': 1},\n        # Case 2: Two sinusoids, with noise\n        {'N': 800, 'dt': 0.01, 'omegas': [2*np.pi*0.7, 2*np.pi*1.2], 'amps': [1.0, 0.5], 'sigma': 0.01, 'm': 80, 'r': 40, 'k': 2},\n        # Case 3: Two close sinusoids, with noise\n        {'N': 900, 'dt': 0.01, 'omegas': [2*np.pi*2.0, 2*np.pi*2.2], 'amps': [1.0, 0.8], 'sigma': 0.005, 'm': 100, 'r': 60, 'k': 2},\n        # Case 4: Two sinusoids, short time series, with noise\n        {'N': 120, 'dt': 0.01, 'omegas': [2*np.pi*1.5, 2*np.pi*3.0], 'amps': [1.0, 0.3], 'sigma': 0.02, 'm': 40, 'r': 10, 'k': 2},\n    ]\n\n    all_results = []\n\n    for params in test_cases:\n        # 1. Generate time series\n        t = np.arange(params['N']) * params['dt']\n        x = np.zeros(params['N'])\n        for omega, amp in zip(params['omegas'], params['amps']):\n            x += amp * np.sin(omega * t)\n        \n        if params['sigma'] > 0:\n            x += rng.normal(0, params['sigma'], params['N'])\n\n        # 2. Construct Hankel matrix\n        m = params['m']\n        N = params['N']\n        L = N - m + 1\n        H = np.empty((m, L))\n        for j in range(L):\n            H[:, j] = x[j : j + m]\n\n        # 3. Form snapshot matrices\n        X = H[:, :-1]\n        Y = H[:, 1:]\n\n        # 4. Perform truncated SVD and DMD\n        U, s, Vh = linalg.svd(X, full_matrices=False)\n        \n        r = params['r']\n        Ur = U[:, :r]\n        Sr_inv = np.diag(1.0 / s[:r])\n        Vr = Vh[:r, :].T\n\n        # Build reduced-order operator\n        A_tilde = Ur.T @ Y @ Vr @ Sr_inv\n\n        # 5. Eigendecomposition and frequency extraction\n        Lambda, W = linalg.eig(A_tilde)\n        # Convert discrete-time eigenvalues to continuous-time angular frequencies\n        dmd_omegas = np.angle(Lambda) / params['dt']\n\n        # 6. Rank modes by contribution and select top k\n        # Reconstruct DMD modes\n        Phi = Ur @ W\n        # Project initial state onto DMD modes to get amplitudes\n        x1 = X[:, 0]\n        # Use pseudoinverse for stable computation of mode amplitudes\n        b = linalg.pinv(Phi) @ x1\n        \n        amplitudes = np.abs(b)\n        \n        # Pair frequencies with their amplitudes and sort\n        sorted_indices = np.argsort(amplitudes)[::-1]\n        sorted_omegas = dmd_omegas[sorted_indices]\n\n        # Deduplicate and select top k positive frequencies\n        top_k_omegas = []\n        added_omegas_set = []\n        # Tolerance for considering frequencies as distinct\n        freq_tolerance = 0.1 \n        \n        for omega in sorted_omegas:\n            if len(top_k_omegas) >= params['k']:\n                break\n            # Consider only positive frequencies\n            if omega > 0:\n                is_duplicate = False\n                for added_omega in added_omegas_set:\n                    if abs(omega - added_omega) < freq_tolerance:\n                        is_duplicate = True\n                        break\n                if not is_duplicate:\n                    top_k_omegas.append(omega)\n                    added_omegas_set.append(omega)\n        \n        # Round the final frequencies to three decimal places\n        result = [round(f, 3) for f in top_k_omegas]\n        # The problem asks for decreasing order of modal contribution.\n        # Since we iterate through the sorted list, the order is preserved.\n        all_results.append(result)\n\n    # Format output as a string representation of a list of lists.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}