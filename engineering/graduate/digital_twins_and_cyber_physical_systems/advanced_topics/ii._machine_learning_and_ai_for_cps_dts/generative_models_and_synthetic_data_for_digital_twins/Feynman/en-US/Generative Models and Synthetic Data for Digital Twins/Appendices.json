{
    "hands_on_practices": [
        {
            "introduction": "Synthetic data generated for a digital twin is rarely a perfect replica of real-world phenomena, creating a 'distributional shift' that can bias any model trained on it. To build reliable system models, we must compensate for this discrepancy. This exercise will guide you through the derivation of an importance weighting function , a cornerstone technique that mathematically corrects the learning process, ensuring that insights gained from synthetic data accurately reflect the real-world dynamics you aim to capture.",
            "id": "4224964",
            "problem": "Consider a digital twin of a controlled cyber-physical system where the one-step-ahead state evolution is modeled as $x_{t+1} = f_{\\theta}(x_{t}, u_{t})$, with $x_{t} \\in \\mathbb{R}^{n}$ and $u_{t} \\in \\mathbb{R}^{m}$. Let $(x_{t}, u_{t}, x_{t+1})$ be random variables on a common measurable space $(\\mathcal{Z}, \\mathcal{F})$ with a target real-world joint distribution $P_{r}$ and a synthetic joint distribution $P_{s}$ induced by a generative model. Assume $P_{r}$ is absolutely continuous with respect to $P_{s}$ on $(\\mathcal{Z}, \\mathcal{F})$ and both admit densities $p_{r}$ and $p_{s}$ with respect to a common dominating measure.\n\nYou seek to learn $\\theta$ by minimizing the expected risk under the real-world distribution\n$$\nR(\\theta) = \\mathbb{E}_{(x_{t}, u_{t}, x_{t+1}) \\sim P_{r}}\\big[\\ell\\big(x_{t+1}, f_{\\theta}(x_{t}, u_{t})\\big)\\big],\n$$\nwhere $\\ell: \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\to \\mathbb{R}_{\\ge 0}$ is a differentiable loss function satisfying the regularity conditions required to interchange differentiation and expectation.\n\nBecause real-world data may be limited, you form a joint objective that blends real and synthetic data:\n$$\nJ(\\theta; \\alpha, w_{s}) = (1 - \\alpha)\\,\\mathbb{E}_{P_{r}}\\big[\\ell\\big(x_{t+1}, f_{\\theta}(x_{t}, u_{t})\\big)\\big] + \\alpha\\,\\mathbb{E}_{P_{s}}\\big[w_{s}(x_{t}, u_{t}, x_{t+1})\\,\\ell\\big(x_{t+1}, f_{\\theta}(x_{t}, u_{t})\\big)\\big],\n$$\nfor some mixing coefficient $\\alpha \\in (0, 1)$ and a nonnegative weighting function $w_{s}: \\mathcal{Z} \\to \\mathbb{R}_{\\ge 0}$ to be determined.\n\nStarting only from the foundational definitions of expectation, absolute continuity, and change of measure, derive a closed-form expression for the unique weighting function $w_{s}^{\\star}(x_{t}, u_{t}, x_{t+1})$ that eliminates distribution-induced bias in the gradient of $J(\\theta; \\alpha, w_{s})$ with respect to $\\theta$, in the sense that $\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}^{\\star}) = \\nabla_{\\theta} R(\\theta)$ for all $\\theta$ and for all differentiable losses $\\ell$ satisfying the stated regularity conditions. Express your final answer as a single analytic expression in terms of the densities $p_{r}$ and $p_{s}$. No numerical approximation is required.",
            "solution": "The problem as stated is scientifically sound, self-contained, and mathematically well-posed. The concepts of generative models, digital twins, risk minimization, and importance weighting are standard in modern machine learning and control theory. The mathematical framework, which relies on measure theory and calculus, is rigorous. Therefore, I will proceed with a full derivation of the solution.\n\nThe objective is to find the unique weighting function $w_{s}^{\\star}(x_{t}, u_{t}, x_{t+1})$ that ensures the gradient of the blended objective, $J(\\theta; \\alpha, w_{s})$, is identical to the gradient of the true risk, $R(\\theta)$, for all valid model parameters $\\theta$ and loss functions $\\ell$. This condition is expressed as:\n$$\n\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}^{\\star}) = \\nabla_{\\theta} R(\\theta)\n$$\n\nLet us first express the risk $R(\\theta)$ and the objective $J(\\theta; \\alpha, w_{s})$ as integrals. Let $z$ be a shorthand for the random variable triplet $(x_{t}, u_{t}, x_{t+1})$. The domain $\\mathcal{Z}$ is a subset of $\\mathbb{R}^{n} \\times \\mathbb{R}^{m} \\times \\mathbb{R}^{n}$. The given densities are $p_r(z)$ and $p_s(z)$ with respect to a common dominating measure $\\mu$. The loss function evaluated at a point $z$ for a model $f_\\theta$ is $\\ell(x_{t+1}, f_{\\theta}(x_t, u_t))$, which we denote as $L(z, \\theta)$.\n\nThe expected risk under the real-world distribution $P_r$ is defined as:\n$$\nR(\\theta) = \\mathbb{E}_{P_{r}}[L(z, \\theta)] = \\int_{\\mathcal{Z}} L(z, \\theta) p_{r}(z) d\\mu(z)\n$$\nThe blended objective function $J(\\theta; \\alpha, w_{s})$ is given by:\n$$\nJ(\\theta; \\alpha, w_{s}) = (1 - \\alpha) \\mathbb{E}_{P_{r}}[L(z, \\theta)] + \\alpha \\mathbb{E}_{P_{s}}[w_{s}(z) L(z, \\theta)]\n$$\nSubstituting the integral forms:\n$$\nJ(\\theta; \\alpha, w_{s}) = (1-\\alpha) R(\\theta) + \\alpha \\int_{\\mathcal{Z}} w_{s}(z) L(z, \\theta) p_{s}(z) d\\mu(z)\n$$\nThe problem requires us to find $w_s = w_s^{\\star}$ such that $\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}^{\\star}) = \\nabla_{\\theta} R(\\theta)$. Let's compute the gradient of $J(\\theta; \\alpha, w_{s})$ with respect to $\\theta$:\n$$\n\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}) = \\nabla_{\\theta} \\left( (1-\\alpha) R(\\theta) + \\alpha \\int_{\\mathcal{Z}} w_{s}(z) L(z, \\theta) p_{s}(z) d\\mu(z) \\right)\n$$\nBy linearity of the gradient operator:\n$$\n\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}) = (1-\\alpha) \\nabla_{\\theta} R(\\theta) + \\alpha \\nabla_{\\theta} \\left( \\int_{\\mathcal{Z}} w_{s}(z) L(z, \\theta) p_{s}(z) d\\mu(z) \\right)\n$$\nThe problem states that the loss function $\\ell$ and model $f_{\\theta}$ satisfy regularity conditions that permit the interchange of differentiation and expectation (integration). Applying this to both $\\nabla_{\\theta}R(\\theta)$ and the second term:\n$$\n\\nabla_{\\theta} R(\\theta) = \\int_{\\mathcal{Z}} \\nabla_{\\theta} L(z, \\theta) p_{r}(z) d\\mu(z)\n$$\nand\n$$\n\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}) = (1-\\alpha) \\nabla_{\\theta} R(\\theta) + \\alpha \\int_{\\mathcal{Z}} w_{s}(z) (\\nabla_{\\theta} L(z, \\theta)) p_{s}(z) d\\mu(z)\n$$\nNow, we enforce the condition $\\nabla_{\\theta} J(\\theta; \\alpha, w_{s}^{\\star}) = \\nabla_{\\theta} R(\\theta)$:\n$$\n(1-\\alpha) \\nabla_{\\theta} R(\\theta) + \\alpha \\int_{\\mathcal{Z}} w_{s}^{\\star}(z) (\\nabla_{\\theta} L(z, \\theta)) p_{s}(z) d\\mu(z) = \\nabla_{\\theta} R(\\theta)\n$$\nSubtracting $(1-\\alpha) \\nabla_{\\theta} R(\\theta)$ from both sides yields:\n$$\n\\alpha \\int_{\\mathcal{Z}} w_{s}^{\\star}(z) (\\nabla_{\\theta} L(z, \\theta)) p_{s}(z) d\\mu(z) = \\alpha \\nabla_{\\theta} R(\\theta)\n$$\nSince $\\alpha \\in (0, 1)$, $\\alpha$ is non-zero, so we can divide by it:\n$$\n\\int_{\\mathcal{Z}} w_{s}^{\\star}(z) (\\nabla_{\\theta} L(z, \\theta)) p_{s}(z) d\\mu(z) = \\nabla_{\\theta} R(\\theta)\n$$\nSubstitute the integral expression for $\\nabla_{\\theta} R(\\theta)$:\n$$\n\\int_{\\mathcal{Z}} w_{s}^{\\star}(z) (\\nabla_{\\theta} L(z, \\theta)) p_{s}(z) d\\mu(z) = \\int_{\\mathcal{Z}} (\\nabla_{\\theta} L(z, \\theta)) p_{r}(z) d\\mu(z)\n$$\nThis equation can be rearranged into a single integral:\n$$\n\\int_{\\mathcal{Z}} \\left[ w_{s}^{\\star}(z) p_{s}(z) - p_{r}(z) \\right] (\\nabla_{\\theta} L(z, \\theta)) d\\mu(z) = 0\n$$\nThis identity must hold for all parameters $\\theta$ and for all differentiable loss functions $\\ell$ meeting the specified conditions. The term $\\nabla_{\\theta} L(z, \\theta)$ represents a large family of functions. By the fundamental lemma of the calculus of variations, if an integral of a function multiplied by an arbitrary test function from a sufficiently rich class is zero, then the function itself must be zero almost everywhere. Therefore, the term in the square brackets must be zero for $\\mu$-almost every $z \\in \\mathcal{Z}$.\n$$\nw_{s}^{\\star}(z) p_{s}(z) - p_{r}(z) = 0\n$$\nThis implies $w_{s}^{\\star}(z) p_{s}(z) = p_{r}(z)$.\n\nWe can now solve for $w_{s}^{\\star}(z)$. The problem states that $P_r$ is absolutely continuous with respect to $P_s$, which means that for any measurable set $A \\subseteq \\mathcal{Z}$, if $P_s(A) = 0$, then $P_r(A) = 0$. In terms of densities, this implies that $p_r(z) = 0$ for almost every $z$ where $p_s(z) = 0$.\nThus, we can safely divide by $p_s(z)$ for all $z$ where $p_s(z) \\neq 0$:\n$$\nw_{s}^{\\star}(z) = \\frac{p_{r}(z)}{p_{s}(z)}\n$$\nFor the set of measure zero where $p_s(z) = 0$, the value of $w_s^{\\star}(z)$ is irrelevant to the integral defining the expectation with respect to $P_s$. The expression above is the Radon-Nikodym derivative of $P_r$ with respect to $P_s$, denoted $\\frac{dP_r}{dP_s}(z)$, and it is unique $P_s$-almost everywhere.\n\nRe-expressing the result in terms of the original variables $(x_{t}, u_{t}, x_{t+1})$:\n$$\nw_{s}^{\\star}(x_{t}, u_{t}, x_{t+1}) = \\frac{p_{r}(x_{t}, u_{t}, x_{t+1})}{p_{s}(x_{t}, u_{t}, x_{t+1})}\n$$\nThis is the closed-form expression for the unique weighting function that eliminates the distributional bias in the gradient of the objective function.",
            "answer": "$$\n\\boxed{\\frac{p_{r}(x_{t}, u_{t}, x_{t+1})}{p_{s}(x_{t}, u_{t}, x_{t+1})}}\n$$"
        },
        {
            "introduction": "A digital twin's true power lies not in merely mirroring observations, but in its ability to answer 'what-if' questions through simulated interventions. Confusing the statistical association from passive observation with the causal effect of an active intervention is a critical error in policy planning and control design. This practice delves into the heart of causal inference , where you will use a simple structural model to quantify the difference between 'seeing' an event and 'doing' an action, revealing the hidden impact of confounding variables.",
            "id": "4225068",
            "problem": "A digital twin of a cyber-physical heating, ventilation, and air conditioning system simulates outcomes using a generative model learned from observational data. The generative model is a Directed Acyclic Graph (DAG) over binary variables with the following structure: an unobserved context variable $U$ influences both the control action $A$ and the outcome $Y$, and the control action $A$ also influences the outcome $Y$. The causal graph is $U \\rightarrow A$, $U \\rightarrow Y$, and $A \\rightarrow Y$. Assume a Structural Causal Model (SCM) whose observational joint distribution factorizes according to the DAG as $p(U)\\,p(A \\mid U)\\,p(Y \\mid A, U)$, with the following conditional probability tables:\n- $U \\sim \\text{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$, so $p(U=1)=\\frac{1}{2}$ and $p(U=0)=\\frac{1}{2}$.\n- $p(A=1 \\mid U=1)=\\frac{4}{5}$ and $p(A=1 \\mid U=0)=\\frac{1}{5}$.\n- $p(Y=1 \\mid A=1, U=1)=\\frac{9}{10}$, $p(Y=1 \\mid A=1, U=0)=\\frac{3}{5}$, $p(Y=1 \\mid A=0, U=1)=\\frac{7}{10}$, and $p(Y=1 \\mid A=0, U=0)=\\frac{1}{10}$.\n\nIn the digital twin, a counterfactual policy planning query requires distinguishing between conditioning on the observed action and intervening to set the action. Starting only from the core definitions of conditional probability and the intervention semantics in a Structural Causal Model (that an intervention $\\text{do}(A=a)$ replaces the mechanism for $A$ and severs incoming arrows into $A$), derive the observational quantity $p(Y=1 \\mid A=1)$ and the interventional quantity $p\\!\\left(Y=1 \\mid \\text{do}(A=1)\\right)$ for this system. Then compute the scalar difference\n$$\n\\Delta \\;=\\; p(Y=1 \\mid A=1)\\;-\\;p\\!\\left(Y=1 \\mid \\text{do}(A=1)\\right).\n$$\nReport the exact value of $\\Delta$ as a single number with no rounding. No units are required.",
            "solution": "The problem requires the computation of the scalar difference $\\Delta$, defined as $\\Delta = p(Y=1 \\mid A=1) - p(Y=1 \\mid \\text{do}(A=1))$. This involves deriving an observational probability and an interventional probability from the provided Structural Causal Model. We will calculate each term separately.\n\nFirst, we compute the observational conditional probability, $p(Y=1 \\mid A=1)$. By the definition of conditional probability, this is given by:\n$$p(Y=1 \\mid A=1) = \\frac{p(Y=1, A=1)}{p(A=1)}$$\nTo find the probabilities in the numerator and the denominator, we must marginalize the full joint distribution, $p(U, A, Y)$, over the unobserved context variable $U$. The problem specifies that the joint distribution factorizes according to the Directed Acyclic Graph as $p(U, A, Y) = p(U)p(A \\mid U)p(Y \\mid A, U)$.\n\nLet us compute the denominator, $p(A=1)$, by applying the law of total probability:\n$$p(A=1) = \\sum_{u \\in \\{0, 1\\}} p(A=1, U=u) = \\sum_{u \\in \\{0, 1\\}} p(A=1 \\mid U=u) p(U=u)$$\nThis expression expands to:\n$$p(A=1) = p(A=1 \\mid U=1) p(U=1) + p(A=1 \\mid U=0) p(U=0)$$\nUsing the provided values, $p(A=1 \\mid U=1)=\\frac{4}{5}$, $p(A=1 \\mid U=0)=\\frac{1}{5}$, and $p(U=1)=p(U=0)=\\frac{1}{2}$:\n$$p(A=1) = \\left(\\frac{4}{5}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{5}\\right)\\left(\\frac{1}{2}\\right) = \\frac{4}{10} + \\frac{1}{10} = \\frac{5}{10} = \\frac{1}{2}$$\n\nNext, we compute the numerator, the joint probability $p(Y=1, A=1)$, by marginalizing over $U$:\n$$p(Y=1, A=1) = \\sum_{u \\in \\{0, 1\\}} p(Y=1, A=1, U=u)$$\nApplying the chain rule based on the model's factorization:\n$$p(Y=1, A=1) = p(Y=1 \\mid A=1, U=1) p(A=1 \\mid U=1) p(U=1) + p(Y=1 \\mid A=1, U=0) p(A=1 \\mid U=0) p(U=0)$$\nSubstituting all the given probability values:\n$$p(Y=1, A=1) = \\left(\\frac{9}{10}\\right)\\left(\\frac{4}{5}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{3}{5}\\right)\\left(\\frac{1}{5}\\right)\\left(\\frac{1}{2}\\right)$$\n$$p(Y=1, A=1) = \\frac{36}{100} + \\frac{3}{50} = \\frac{36}{100} + \\frac{6}{100} = \\frac{42}{100} = \\frac{21}{50}$$\nNow we can calculate the observational probability:\n$$p(Y=1 \\mid A=1) = \\frac{p(Y=1, A=1)}{p(A=1)} = \\frac{21/50}{1/2} = \\frac{21 \\times 2}{50} = \\frac{42}{50} = \\frac{21}{25}$$\n\nSecond, we compute the interventional probability, $p(Y=1 \\mid \\text{do}(A=1))$. The intervention $\\text{do}(A=1)$ modifies the causal graph by severing all incoming arrows to $A$, in this case the arrow $U \\rightarrow A$, and setting the value of $A$ to $1$. The probability of an outcome $Y$ in this manipulated system is found by adjusting for the confounding variable $U$, which lies on the \"back-door path\" $A \\leftarrow U \\rightarrow Y$. The formula for this adjustment is:\n$$p(Y=1 \\mid \\text{do}(A=1)) = \\sum_{u \\in \\{0, 1\\}} p(Y=1 \\mid A=1, U=u) p(U=u)$$\nThis expression represents the average causal effect of $A=1$ on $Y$, weighted by the prior distribution of the confounder $U$. Expanding the sum:\n$$p(Y=1 \\mid \\text{do}(A=1)) = p(Y=1 \\mid A=1, U=1) p(U=1) + p(Y=1 \\mid A=1, U=0) p(U=0)$$\nSubstituting the known values:\n$$p(Y=1 \\mid \\text{do}(A=1)) = \\left(\\frac{9}{10}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{3}{5}\\right)\\left(\\frac{1}{2}\\right) = \\frac{9}{20} + \\frac{3}{10} = \\frac{9}{20} + \\frac{6}{20} = \\frac{15}{20} = \\frac{3}{4}$$\n\nFinally, we compute the required difference $\\Delta$:\n$$\\Delta = p(Y=1 \\mid A=1) - p(Y=1 \\mid \\text{do}(A=1))$$\n$$\\Delta = \\frac{21}{25} - \\frac{3}{4}$$\nTo subtract the fractions, we find a common denominator of $100$:\n$$\\Delta = \\frac{21 \\times 4}{25 \\times 4} - \\frac{3 \\times 25}{4 \\times 25} = \\frac{84}{100} - \\frac{75}{100} = \\frac{9}{100}$$\nThis non-zero difference represents the confounding bias, which is the portion of the statistical association between $A$ and $Y$ that is not due to a causal link from $A$ to $Y$, but rather due to their common cause $U$.",
            "answer": "$$\\boxed{\\frac{9}{100}}$$"
        },
        {
            "introduction": "While many generative models excel at creating 'typical' data, they often fail to generate the rare but critical extreme events that push a system to its limits. For a digital twin used in stress testing and safety analysis, this is a significant shortcoming. This exercise introduces you to Extreme Value Theory (EVT), a rigorous framework for modeling the tails of a distribution . You will derive the foundational model for threshold exceedances and apply it to estimate the likelihood of extreme loads, a vital skill for building robust and reliable cyber-physical systems.",
            "id": "4224998",
            "problem": "A cyber-physical system (CPS) monitors structural loads in kilonewtons (kN). Its digital twin must generate synthetic load data that realistically replicates extreme events for downstream stress testing. To correct the under-representation of extremes from a baseline body-distribution generator, you will derive the appropriate tail model via a peaks-over-threshold approach and then calibrate it using observed exceedance statistics from the CPS.\n\nStarting from the following fundamental base: if a parent distribution $F$ is in the maximum domain of attraction of a generalized extreme value distribution (GEV) with shape index $\\xi$, then the upper tail quantile function $U(t) = F^{-1}(1 - 1/t)$ exhibits extended regular variation. Additionally, the distribution of threshold exceedances can be expressed in terms of $F$ by conditioning and appropriate normalization. Use these foundational facts to:\n\n1. Derive the non-degenerate limiting family for scaled threshold exceedances $X - u$ conditional on $X > u$, by analyzing the limit of $P\\!\\left(\\frac{X - u}{\\beta(u)} \\leq y \\,\\middle|\\, X > u\\right)$ as $u$ increases, where $\\beta(u) > 0$ is an appropriate scaling function. Show that the only possible continuous limit family consistent with threshold stability and extended regular variation is the generalized Pareto family with a shape index $\\xi$ matching that of the associated GEV and a scale that is asymptotically proportional to $\\beta(u)$.\n\n2. The CPS produced a large sample of size $N = 10^{6}$. For a high threshold $u = 110$ kN, the number of exceedances is $n_{u} = 150$. The empirical mean of the excesses above $u$ is $\\hat{m} = 8$ kN and the empirical variance of the excesses is $\\hat{v} = 144$ kN².\n\n3. Treating the tail above $u$ as a splice to the baseline generator, model the unconditional survivor function for $x \\geq u$ as $P(X > x) \\approx p_{u}\\,S_{\\text{GPD}}(x - u)$, where $p_{u} = P(X > u)$ is estimated by $n_{u}/N$ and $S_{\\text{GPD}}$ is the generalized Pareto survivor with parameters $(\\hat{\\xi}, \\hat{\\beta})$. Using this model, compute the unconditional $p$-quantile for $p = 0.99995$. Express your final numerical answer in kilonewtons and round to four significant figures.\n\nReport only the $0.99995$-quantile value as your final answer.",
            "solution": "The problem requires a three-part analysis involving the derivation of the generalized Pareto distribution (GPD) as a tail model, the estimation of its parameters using the method of moments, and the calculation of a high quantile from the resulting model. The problem is scientifically sound, well-posed, and contains all necessary information. We proceed with the solution.\n\n### Part 1: Derivation of the Generalized Pareto Distribution\nWe are tasked with deriving the limiting distribution for scaled threshold exceedances, $X-u$, conditional on $X > u$. Let the parent distribution of the random variable $X$ be $F(x) = P(X \\leq x)$, and its survivor function be $S(x) = 1-F(x)$. The conditional distribution function of the exceedance $Y = X-u$ over a threshold $u$, given that an exceedance occurred, is\n$$F_u(y) = P(X-u \\leq y \\,|\\, X > u) = \\frac{P(u < X \\leq u+y)}{P(X > u)} = \\frac{F(u+y) - F(u)}{1 - F(u)}$$\nThe problem considers a scaled exceedance $(X-u)/\\beta(u)$ for some scaling function $\\beta(u) > 0$. The CDF of this scaled exceedance is\n$$P\\left(\\frac{X-u}{\\beta(u)} \\leq y \\,\\middle|\\, X > u\\right) = \\frac{F(u+y\\beta(u)) - F(u)}{1 - F(u)} = 1 - \\frac{1-F(u+y\\beta(u))}{1-F(u)} = 1 - \\frac{S(u+y\\beta(u))}{S(u)}$$\nThe premise is that the parent distribution $F$ is in the maximum domain of attraction of a generalized extreme value (GEV) distribution with shape parameter $\\xi$. A fundamental result in extreme value theory (the Pickands–Balkema–de Haan theorem) states that this condition is equivalent to the existence of a positive scaling function $\\sigma(u)$ such that for the right endpoint $x_F = \\sup\\{x: F(x)<1\\}$, the following limit holds:\n$$\\lim_{u \\to x_F} \\frac{S(u+y\\sigma(u))}{S(u)} = \\lim_{u \\to x_F} \\frac{1-F(u+y\\sigma(u))}{1-F(u)} = (1+\\xi y)^{-1/\\xi}$$\nfor all $y$ where $1+\\xi y > 0$. This property is known as threshold stability.\n\nTo find the limiting distribution of scaled exceedances, we choose the scaling function $\\beta(u)$ to be this very function $\\sigma(u)$. Taking the limit as $u \\to x_F$, we have:\n$$\\lim_{u \\to x_F} P\\left(\\frac{X-u}{\\sigma(u)} \\leq y \\,\\middle|\\, X > u\\right) = 1 - \\lim_{u \\to x_F} \\frac{S(u+y\\sigma(u))}{S(u)} = 1 - (1+\\xi y)^{-1/\\xi}$$\nThis limiting cumulative distribution function, $H(y) = 1 - (1+\\xi y)^{-1/\\xi}$, is the CDF of the standard Generalized Pareto Distribution (GPD) with shape parameter $\\xi$. The support for $y$ is $y \\geq 0$ if $\\xi \\geq 0$, and $0 \\leq y \\leq -1/\\xi$ if $\\xi < 0$. In the special case $\\xi=0$, the limit is taken, yielding the CDF of the standard exponential distribution, $H(y) = 1 - \\exp(-y)$.\n\nThis derivation shows that for a sufficiently high threshold $u$, the distribution of the exceedance $X-u$ can be approximated by a GPD with shape parameter $\\xi$ and a scale parameter $\\beta$ that is determined by the behavior of the parent distribution's tail, specifically $\\beta = \\sigma(u)$. The CDF of this non-standard GPD is given by $G_{\\xi, \\beta}(z) = 1 - (1+\\xi z/\\beta)^{-1/\\xi}$ for an exceedance $z$. This establishes the GPD as the correct limiting family for threshold exceedances under the given assumptions.\n\n### Part 2: Parameter Estimation by Method of Moments\nThe generalized Pareto distribution with shape parameter $\\xi$ and scale parameter $\\beta$ has the following mean and variance, provided the parameters satisfy certain conditions:\n$$E[Y] = \\frac{\\beta}{1-\\xi}, \\quad \\text{for } \\xi < 1$$\n$$\\text{Var}(Y) = \\frac{\\beta^2}{(1-\\xi)^2(1-2\\xi)}, \\quad \\text{for } \\xi < \\frac{1}{2}$$\nThe method of moments consists of equating these theoretical moments to the empirical moments calculated from the data. The given data are the empirical mean of excesses, $\\hat{m} = 8$, and the empirical variance of excesses, $\\hat{v} = 144$. We set up the system of equations for the estimates $\\hat{\\xi}$ and $\\hat{\\beta}$:\n$$ \\hat{m} = \\frac{\\hat{\\beta}}{1-\\hat{\\xi}} \\quad (1) $$\n$$ \\hat{v} = \\frac{\\hat{\\beta}^2}{(1-\\hat{\\xi})^2(1-2\\hat{\\xi})} \\quad (2) $$\nWe can find a relationship between the moments by noting that $\\text{Var}(Y) = (E[Y])^2 / (1-2\\xi)$. Substituting the estimators:\n$$ \\hat{v} = \\frac{\\hat{m}^2}{1-2\\hat{\\xi}} $$\nWe solve for $\\hat{\\xi}$:\n$$ 1-2\\hat{\\xi} = \\frac{\\hat{m}^2}{\\hat{v}} \\implies 2\\hat{\\xi} = 1 - \\frac{\\hat{m}^2}{\\hat{v}} \\implies \\hat{\\xi} = \\frac{1}{2}\\left(1 - \\frac{\\hat{m}^2}{\\hat{v}}\\right) $$\nSubstituting the given values $\\hat{m}=8$ and $\\hat{v}=144$:\n$$ \\hat{\\xi} = \\frac{1}{2}\\left(1 - \\frac{8^2}{144}\\right) = \\frac{1}{2}\\left(1 - \\frac{64}{144}\\right) = \\frac{1}{2}\\left(1 - \\frac{4}{9}\\right) = \\frac{1}{2}\\left(\\frac{5}{9}\\right) = \\frac{5}{18} $$\nThe value $\\hat{\\xi} = 5/18 \\approx 0.278$ is less than $1/2$, so the assumption of finite variance is consistent with the estimate. Now, we use equation (1) to solve for $\\hat{\\beta}$:\n$$ \\hat{\\beta} = \\hat{m}(1-\\hat{\\xi}) $$\nSubstituting the values for $\\hat{m}$ and $\\hat{\\xi}$:\n$$ \\hat{\\beta} = 8\\left(1 - \\frac{5}{18}\\right) = 8\\left(\\frac{13}{18}\\right) = \\frac{4 \\times 13}{9} = \\frac{52}{9} $$\nThe estimated parameters for the GPD fit to the excesses are $\\hat{\\xi} = 5/18$ and $\\hat{\\beta} = 52/9$.\n\n### Part 3: Quantile Calculation\nThe problem asks for the $p=0.99995$ quantile of the unconditional distribution $X$, which we denote as $x_p$. The tail of the distribution for $x \\geq u$ is modeled as a splice, where the survivor function is given by:\n$$ S(x) = P(X>x) \\approx P(X>u) \\cdot P(X>x | X>u) \\approx p_u \\cdot S_{\\text{GPD}}(x-u; \\hat{\\xi}, \\hat{\\beta}) $$\nThe probability of exceeding the threshold $u=110$ is estimated from the sample data:\n$$ \\hat{p}_u = \\frac{n_u}{N} = \\frac{150}{10^6} = 1.5 \\times 10^{-4} $$\nThe survivor function of the GPD for an exceedance $y=x-u$ is:\n$$ S_{\\text{GPD}}(y; \\xi, \\beta) = \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-1/\\xi} $$\nWe seek $x_p$ such that $P(X \\leq x_p) = p$, which is equivalent to $P(X > x_p) = 1-p$. Let $q = 1-p = 1 - 0.99995 = 5 \\times 10^{-5}$.\nSince $q = 5 \\times 10^{-5} < \\hat{p}_u = 1.5 \\times 10^{-4}$, the quantile $x_p$ must be greater than the threshold $u$, validating the use of the tail model. We set up the equation to solve for $x_p$:\n$$ q = \\hat{p}_u \\left(1 + \\frac{\\hat{\\xi}(x_p-u)}{\\hat{\\beta}}\\right)^{-1/\\hat{\\xi}} $$\nSolving for $x_p$:\n$$ \\frac{q}{\\hat{p}_u} = \\left(1 + \\frac{\\hat{\\xi}(x_p-u)}{\\hat{\\beta}}\\right)^{-1/\\hat{\\xi}} $$\n$$ \\left(\\frac{q}{\\hat{p}_u}\\right)^{-\\hat{\\xi}} = 1 + \\frac{\\hat{\\xi}(x_p-u)}{\\hat{\\beta}} $$\n$$ x_p = u + \\frac{\\hat{\\beta}}{\\hat{\\xi}}\\left[ \\left(\\frac{q}{\\hat{p}_u}\\right)^{-\\hat{\\xi}} - 1 \\right] = u + \\frac{\\hat{\\beta}}{\\hat{\\xi}}\\left[ \\left(\\frac{\\hat{p}_u}{q}\\right)^{\\hat{\\xi}} - 1 \\right] $$\nNow, we substitute the numerical values: $u = 110$, $\\hat{\\xi} = 5/18$, $\\hat{\\beta}=52/9$, $\\hat{p}_u = 1.5 \\times 10^{-4}$, and $q=5 \\times 10^{-5}$.\nThe ratio of probabilities is:\n$$ \\frac{\\hat{p}_u}{q} = \\frac{1.5 \\times 10^{-4}}{5 \\times 10^{-5}} = 3 $$\nThe ratio of parameters is:\n$$ \\frac{\\hat{\\beta}}{\\hat{\\xi}} = \\frac{52/9}{5/18} = \\frac{52}{9} \\cdot \\frac{18}{5} = \\frac{52 \\cdot 2}{5} = \\frac{104}{5} = 20.8 $$\nSubstituting these into the expression for $x_p$:\n$$ x_p = 110 + 20.8 \\left[ 3^{5/18} - 1 \\right] $$\nWe compute the numerical value:\n$$ 3^{5/18} \\approx 1.3569055 $$\n$$ x_p \\approx 110 + 20.8 \\times (1.3569055 - 1) = 110 + 20.8 \\times 0.3569055 $$\n$$ x_p \\approx 110 + 7.4236344 = 117.4236344 $$\nThe problem requires the answer to be rounded to four significant figures. The value $117.4236344$ rounded to four significant figures is $117.4$. The unit is kilonewtons (kN).",
            "answer": "$$ \\boxed{117.4} $$"
        }
    ]
}