{
    "hands_on_practices": [
        {
            "introduction": "In solving partial differential equations, correctly imposing boundary conditions is as crucial as satisfying the equation in the domain. This practice explores a powerful technique for building these constraints directly into the neural network's architecture, a method often called \"hard constraining\". By parameterizing the solution in a specific way, we can guarantee that the boundary conditions are met exactly, which can simplify and stabilize the training process .",
            "id": "4235640",
            "problem": "A digital twin of a one-dimensional nondimensionalized thermal rod, a canonical cyber-physical system, is governed in steady state by the second-order differential equation $-u''(x) = q(x)$ on the interval $\\Omega = [0,1]$ with Dirichlet boundary condition $u|_{\\partial\\Omega} = h$, meaning $u(0) = h(0)$ and $u(1) = h(1)$. To enforce the boundary condition exactly within a Physics-Informed Neural Network (PINN), consider the parameterization\n$$\nu_{\\theta}(x) = h(x) + \\eta(x)\\,N_{\\theta}(x),\n$$\nwhere $\\eta:\\Omega\\to\\mathbb{R}$ satisfies $\\eta(0)=\\eta(1)=0$ and $N_{\\theta}$ is a trainable function. Let $h(x) = \\alpha + (\\beta - \\alpha)\\,x$ with fixed constants $\\alpha,\\beta\\in\\mathbb{R}$, choose $\\eta(x)=x(1-x)$, and choose the trainable part to be a quadratic polynomial $N_{\\theta}(x) = \\theta_{0} + \\theta_{1}\\,x + \\theta_{2}\\,x^{2}$.\n\nFrom a first-principles perspective, begin by identifying the function space to which $u_{\\theta}$ belongs and how the exact Dirichlet constraint alters the admissible space of trainable functions. Then, impose the physics by requiring the strong-form residual to vanish identically,\n$$\n-u_{\\theta}''(x) \\equiv q(x) \\quad \\text{for all } x \\in [0,1],\n$$\nwith the nondimensional source $q(x) = 12\\,x^{2} + 6\\,x + 2$.\n\nUsing only the specification above and fundamental laws of heat conduction in one dimension, determine the parameter vector $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$ that makes the residual identically zero. Provide your final answer for $(\\theta_{0}, \\theta_{1}, \\theta_{2})$ as exact values. The answer has no physical units because the problem is posed in nondimensional variables.",
            "solution": "The user wants me to solve the problem by following these steps:\n1. Validate the problem.\n2. If valid, provide a detailed solution.\n3. The solution must use LaTeX for all mathematical notation.\n4. The final answer must be in a specific boxed format.\n\n### Step 1: Extract Givens\n- **Governing Differential Equation:** $-u''(x) = q(x)$ on the interval $\\Omega = [0,1]$.\n- **Boundary Condition:** $u|_{\\partial\\Omega} = h$, which implies $u(0) = h(0)$ and $u(1) = h(1)$.\n- **Physics-Informed Neural Network (PINN) Ansatz:** $u_{\\theta}(x) = h(x) + \\eta(x)\\,N_{\\theta}(x)$.\n- **Boundary Condition Function:** $h(x) = \\alpha + (\\beta - \\alpha)\\,x$ for fixed constants $\\alpha, \\beta \\in \\mathbb{R}$.\n- **Multiplier Function:** $\\eta(x) = x(1-x)$, which satisfies $\\eta(0)=\\eta(1)=0$.\n- **Trainable Function (Neural Network surrogate):** $N_{\\theta}(x) = \\theta_{0} + \\theta_{1}\\,x + \\theta_{2}\\,x^{2}$.\n- **Source Term:** $q(x) = 12\\,x^{2} + 6\\,x + 2$.\n- **Physics Constraint:** The strong-form residual must be identically zero: $-u_{\\theta}''(x) \\equiv q(x)$ for all $x \\in [0,1]$.\n- **Objective:** Determine the parameter vector $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem describes a one-dimensional steady-state heat conduction (Poisson) problem. The method of using a specific functional form $u_{\\theta}(x) = h(x) + \\eta(x) N_{\\theta}(x)$ to exactly satisfy Dirichlet boundary conditions is a standard and valid technique in the field of Physics-Informed Machine Learning. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** All necessary functions ($h(x)$, $\\eta(x)$, $N_{\\theta}(x)$, $q(x)$) are explicitly defined. The task is to find the coefficients of a polynomial by enforcing a polynomial identity. This leads to a well-defined system of linear equations for the parameters $\\theta_i$, which is expected to have a unique solution.\n3.  **Objective:** The problem is stated using precise mathematical language with no subjective or ambiguous terms.\n4.  **Completeness:** The problem is self-contained and provides all information required to find the unique solution.\n5.  **No Other Flaws:** The problem does not violate any of the other invalidity criteria. It is a canonical example used to illustrate the principles of constructing PINN solutions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe problem requires finding the parameter vector $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$ such that the function $u_{\\theta}(x)$ exactly satisfies the differential equation $-u_{\\theta}''(x) = q(x)$ for all $x \\in [0,1]$.\n\nFirst, we address the structure of the solution space. For the strong form of the differential equation to hold, the solution $u(x)$ must be at least twice continuously differentiable, i.e., $u \\in C^2([0,1])$. The problem specifies inhomogeneous Dirichlet boundary conditions $u(0)=h(0)=\\alpha$ and $u(1)=h(1)=\\beta$. The set of all such functions forms an affine space. The parameterization $u_{\\theta}(x) = h(x) + \\eta(x) N_{\\theta}(x)$ cleverly transforms this problem. By construction, $u_{\\theta}(x)$ automatically satisfies the boundary conditions for any choice of $N_{\\theta}(x)$ because $\\eta(x)$ vanishes at the boundaries:\n$u_{\\theta}(0) = h(0) + \\eta(0) N_{\\theta}(0) = h(0) + 0 = h(0) = \\alpha$.\n$u_{\\theta}(1) = h(1) + \\eta(1) N_{\\theta}(1) = h(1) + 0 = h(1) = \\beta$.\nThus, the boundary conditions are hard-coded into the ansatz. The task reduces to finding the specific $N_{\\theta}(x)$ that satisfies the differential equation itself. In this problem, $N_{\\theta}(x)$ is not a neural network but a simple polynomial, allowing for an exact analytical solution.\n\nWe begin by constructing the full expression for $u_{\\theta}(x)$ by substituting the given forms of $h(x)$, $\\eta(x)$, and $N_{\\theta}(x)$:\n$$\nu_{\\theta}(x) = \\left( \\alpha + (\\beta - \\alpha)x \\right) + \\left( x(1-x) \\right) \\left( \\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} \\right)\n$$\nExpanding the product $\\eta(x)N_{\\theta}(x)$:\n$$\n\\eta(x)N_{\\theta}(x) = (x - x^2)(\\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2}) = \\theta_{0}x + \\theta_{1}x^2 + \\theta_{2}x^3 - \\theta_{0}x^2 - \\theta_{1}x^3 - \\theta_{2}x^4\n$$\nGrouping terms by powers of $x$:\n$$\n\\eta(x)N_{\\theta}(x) = \\theta_{0}x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\nNow, substitute this back into the expression for $u_{\\theta}(x)$:\n$$\nu_{\\theta}(x) = \\alpha + (\\beta - \\alpha)x + \\theta_{0}x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\nWhich can be written as a single polynomial:\n$$\nu_{\\theta}(x) = \\alpha + (\\beta - \\alpha + \\theta_{0})x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\nNext, we compute the second derivative, $u_{\\theta}''(x)$. First, the first derivative $u_{\\theta}'(x)$ is:\n$$\nu_{\\theta}'(x) = \\frac{d}{dx} \\left( \\alpha + (\\beta - \\alpha + \\theta_{0})x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4 \\right)\n$$\n$$\nu_{\\theta}'(x) = (\\beta - \\alpha + \\theta_{0}) + 2(\\theta_{1}-\\theta_{0})x + 3(\\theta_{2}-\\theta_{1})x^2 - 4\\theta_{2}x^3\n$$\nNow, we compute the second derivative, $u_{\\theta}''(x)$:\n$$\nu_{\\theta}''(x) = \\frac{d}{dx} \\left( (\\beta - \\alpha + \\theta_{0}) + 2(\\theta_{1}-\\theta_{0})x + 3(\\theta_{2}-\\theta_{1})x^2 - 4\\theta_{2}x^3 \\right)\n$$\n$$\nu_{\\theta}''(x) = 2(\\theta_{1}-\\theta_{0}) + 6(\\theta_{2}-\\theta_{1})x - 12\\theta_{2}x^2\n$$\nThe governing equation is $-u_{\\theta}''(x) = q(x)$. Let's compute $-u_{\\theta}''(x)$:\n$$\n-u_{\\theta}''(x) = - \\left( 2(\\theta_{1}-\\theta_{0}) + 6(\\theta_{2}-\\theta_{1})x - 12\\theta_{2}x^2 \\right)\n$$\n$$\n-u_{\\theta}''(x) = (2\\theta_{0}-2\\theta_{1}) + (6\\theta_{1}-6\\theta_{2})x + 12\\theta_{2}x^2\n$$\nThe problem requires this expression to be identically equal to the source term $q(x) = 12x^2 + 6x + 2$.\n$$\n(2\\theta_{0}-2\\theta_{1}) + (6\\theta_{1}-6\\theta_{2})x + 12\\theta_{2}x^2 \\equiv 2 + 6x + 12x^2\n$$\nFor two polynomials to be identical for all $x$, the coefficients of corresponding powers of $x$ must be equal. This gives us a system of three linear equations for the three unknown parameters $\\theta_0, \\theta_1, \\theta_2$.\n\nEquating the coefficients of $x^2$:\n$$\n12\\theta_{2} = 12 \\implies \\theta_{2} = 1\n$$\nEquating the coefficients of $x^1$:\n$$\n6\\theta_{1}-6\\theta_{2} = 6\n$$\nSubstitute the value $\\theta_{2}=1$ into this equation:\n$$\n6\\theta_{1}-6(1) = 6 \\implies 6\\theta_{1} = 12 \\implies \\theta_{1} = 2\n$$\nEquating the constant terms (coefficients of $x^0$):\n$$\n2\\theta_{0}-2\\theta_{1} = 2\n$$\nSubstitute the value $\\theta_{1}=2$ into this equation:\n$$\n2\\theta_{0}-2(2) = 2 \\implies 2\\theta_{0} - 4 = 2 \\implies 2\\theta_{0} = 6 \\implies \\theta_{0} = 3\n$$\nThus, we have found the unique parameter vector $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$ that makes the residual identically zero. The parameters are independent of the boundary condition constants $\\alpha$ and $\\beta$, as expected, since the linear function $h(x)$ has a second derivative of zero and thus does not contribute to the residual $-u_{\\theta}''(x)$.\n\nThe parameter vector is $(\\theta_{0}, \\theta_{1}, \\theta_{2}) = (3, 2, 1)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3  2  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The core of training any neural network, including a PINN, lies in minimizing a loss function via gradient-based optimization. This exercise peels back the layers of automatic differentiation to reveal the underlying mechanics. You will derive the analytical gradient of a standard PINN loss function, providing a first-principles understanding of how the network's parameters are updated to satisfy the governing physics, initial conditions, and boundary data .",
            "id": "4235644",
            "problem": "A digital twin of a heat-conducting component is governed by the heat equation, a linear parabolic partial differential equation (PDE). Consider a bounded Lipschitz spatial domain $\\Omega \\subset \\mathbb{R}^{d}$ with boundary $\\partial \\Omega$ decomposed into a Dirichlet part $\\Gamma_{D}$ and a Neumann part $\\Gamma_{N}$, with $\\Gamma_{D} \\cup \\Gamma_{N} = \\partial \\Omega$ and $\\Gamma_{D} \\cap \\Gamma_{N} = \\emptyset$. Let $T \\in (0,\\infty)$ denote a final time. The state $u:\\Omega \\times [0,T] \\to \\mathbb{R}$ satisfies the heat equation $u_{t} - \\alpha \\Delta u = 0$ in $\\Omega \\times (0,T]$, with constant thermal diffusivity $\\alpha  0$, the initial condition $u(x,0) = u_{0}(x)$ for $x \\in \\Omega$, and boundary conditions $u(s,t) = g(s,t)$ for $(s,t) \\in \\Gamma_{D} \\times (0,T]$ and $n(s) \\cdot \\nabla u(s,t) = h(s,t)$ for $(s,t) \\in \\Gamma_{N} \\times (0,T]$, where $n(s)$ is the outward unit normal at $s \\in \\partial \\Omega$.\n\nIn a physics-informed neural network (PINN), the approximation $u_{\\theta}(x,t)$ is produced by a neural network with parameters $\\theta \\in \\mathbb{R}^{p}$. To train this model for use in a cyber-physical digital twin, we penalize violations of the PDE, the initial condition, and the boundary conditions using empirical averages over finite point sets:\n- Interior collocation set $\\mathcal{Z}_{f} = \\{(x_{i}, t_{i})\\}_{i=1}^{N_{f}} \\subset \\Omega \\times (0,T]$,\n- Initial set $\\mathcal{X}_{0} = \\{x_{j}\\}_{j=1}^{N_{0}} \\subset \\Omega$,\n- Dirichlet boundary set $\\mathcal{Z}_{D} = \\{(s_{k}, \\tau_{k})\\}_{k=1}^{N_{D}} \\subset \\Gamma_{D} \\times (0,T]$,\n- Neumann boundary set $\\mathcal{Z}_{N} = \\{(s_{\\ell}, \\tau_{\\ell})\\}_{\\ell=1}^{N_{N}} \\subset \\Gamma_{N} \\times (0,T]$.\n\nDefine the residuals\n- PDE residual $r_{f}(x,t;\\theta) := u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t)$,\n- Initial residual $r_{0}(x;\\theta) := u_{\\theta}(x,0) - u_{0}(x)$,\n- Dirichlet residual $r_{D}(s,t;\\theta) := u_{\\theta}(s,t) - g(s,t)$,\n- Neumann residual $r_{N}(s,t;\\theta) := n(s)\\cdot \\nabla u_{\\theta}(s,t) - h(s,t)$.\n\nWith positive weights $w_{f}, w_{0}, w_{D}, w_{N}$, the empirical loss is\n$$\nL(\\theta)\n:= \\frac{w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} \\big(r_{f}(x_{i},t_{i};\\theta)\\big)^{2}\n+ \\frac{w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} \\big(r_{0}(x_{j};\\theta)\\big)^{2}\n+ \\frac{w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} \\big(r_{D}(s_{k},\\tau_{k};\\theta)\\big)^{2}\n+ \\frac{w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} \\big(r_{N}(s_{\\ell},\\tau_{\\ell};\\theta)\\big)^{2}.\n$$\n\nUsing only fundamental calculus (chain rule, linearity of differentiation, and commutation of partial derivatives for smooth functions), Automatic Differentiation (AD) to evaluate derivatives of the neural network with respect to both inputs and parameters, and the above definitions, derive an explicit analytic expression for the gradient vector $\\nabla_{\\theta} L(\\theta)$ in terms of the residuals and the parameter Jacobians of $u_{\\theta}$ and its required input derivatives. Your final answer must be a single closed-form analytic expression for $\\nabla_{\\theta} L(\\theta)$ written as a sum over the data sets defined above. Do not give any intermediate steps in the final answer. You may assume $u_{\\theta}$ is sufficiently smooth that differentiation with respect to inputs and parameters commutes. Express your result using the notation $J_{\\theta}[\\cdot]$ to denote the gradient with respect to $\\theta$, for example $J_{\\theta}[u_{\\theta}](x,t)$, $J_{\\theta}[u_{\\theta,t}](x,t)$, and $J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)$. The final answer is symbolic; no rounding or units are required.",
            "solution": "The problem as stated is subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **PDE and Domain**: The state $u(x,t)$ is defined on a spatial domain $\\Omega \\subset \\mathbb{R}^{d}$ and a time interval $[0,T]$. It satisfies the heat equation $u_{t} - \\alpha \\Delta u = 0$ in $\\Omega \\times (0,T]$, where $\\alpha  0$ is the constant thermal diffusivity.\n- **Initial Condition**: $u(x,0) = u_{0}(x)$ for $x \\in \\Omega$.\n- **Boundary Conditions**: The boundary $\\partial \\Omega$ is decomposed into $\\Gamma_{D} \\cup \\Gamma_{N} = \\partial \\Omega$ with $\\Gamma_{D} \\cap \\Gamma_{N} = \\emptyset$.\n  - Dirichlet: $u(s,t) = g(s,t)$ for $(s,t) \\in \\Gamma_{D} \\times (0,T]$.\n  - Neumann: $n(s) \\cdot \\nabla u(s,t) = h(s,t)$ for $(s,t) \\in \\Gamma_{N} \\times (0,T]$, where $n(s)$ is the outward unit normal.\n- **PINN Approximation**: $u_{\\theta}(x,t)$ is a neural network with parameters $\\theta \\in \\mathbb{R}^{p}$.\n- **Collocation Sets**:\n  - Interior: $\\mathcal{Z}_{f} = \\{(x_{i}, t_{i})\\}_{i=1}^{N_{f}} \\subset \\Omega \\times (0,T]$.\n  - Initial: $\\mathcal{X}_{0} = \\{x_{j}\\}_{j=1}^{N_{0}} \\subset \\Omega$.\n  - Dirichlet: $\\mathcal{Z}_{D} = \\{(s_{k}, \\tau_{k})\\}_{k=1}^{N_{D}} \\subset \\Gamma_{D} \\times (0,T]$.\n  - Neumann: $\\mathcal{Z}_{N} = \\{(s_{\\ell}, \\tau_{\\ell})\\}_{\\ell=1}^{N_{N}} \\subset \\Gamma_{N} \\times (0,T]$.\n- **Residual Definitions**:\n  - PDE: $r_{f}(x,t;\\theta) := u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t)$.\n  - Initial: $r_{0}(x;\\theta) := u_{\\theta}(x,0) - u_{0}(x)$.\n  - Dirichlet: $r_{D}(s,t;\\theta) := u_{\\theta}(s,t) - g(s,t)$.\n  - Neumann: $r_{N}(s,t;\\theta) := n(s)\\cdot \\nabla u_{\\theta}(s,t) - h(s,t)$.\n- **Loss Function**:\n$$\nL(\\theta)\n:= \\frac{w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} \\big(r_{f}(x_{i},t_{i};\\theta)\\big)^{2}\n+ \\frac{w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} \\big(r_{0}(x_{j};\\theta)\\big)^{2}\n+ \\frac{w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} \\big(r_{D}(s_{k},\\tau_{k};\\theta)\\big)^{2}\n+ \\frac{w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} \\big(r_{N}(s_{\\ell},\\tau_{\\ell};\\theta)\\big)^{2}.\n$$\n- **Task Constraints**:\n  - Use only fundamental calculus (chain rule, linearity, commutation of partial derivatives).\n  - Use Automatic Differentiation (AD) conceptually for evaluating derivatives.\n  - Derive an explicit analytic expression for the gradient vector $\\nabla_{\\theta} L(\\theta)$.\n  - The final expression must be in terms of residuals and parameter Jacobians.\n  - Assume $u_{\\theta}$ is sufficiently smooth.\n  - Use the notation $J_{\\theta}[\\cdot]$ for the gradient with respect to $\\theta$, with examples $J_{\\theta}[u_{\\theta}](x,t)$, $J_{\\theta}[u_{\\theta,t}](x,t)$, and $J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the formulation of a Physics-Informed Neural Network (PINN) for solving the heat equation. This is a standard and well-established method in the field of scientific machine learning. The underlying physics (heat equation) and mathematical framework (calculus, optimization) are sound. The problem is scientifically grounded.\n- **Well-Posed**: The task is to compute the gradient of a well-defined, differentiable loss function $L(\\theta)$. Given that $u_{\\theta}$ is a neural network (a composition of differentiable functions) and is assumed to be sufficiently smooth, its derivatives with respect to parameters $\\theta$ are well-defined. The derivation of $\\nabla_{\\theta} L(\\theta)$ is a standard exercise in multivariable calculus. The problem is well-posed.\n- **Objective**: The problem is stated using formal mathematical language and definitions. It is entirely free of subjective, ambiguous, or opinion-based statements.\n\nThe problem passes all validation criteria and exhibits none of the specified flaws. It is a formal, self-contained, and solvable mathematical problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete and reasoned solution will be provided.\n\n### Solution Derivation\nThe objective is to compute the gradient of the loss function $L(\\theta)$ with respect to the neural network parameters $\\theta$, denoted by $\\nabla_{\\theta} L(\\theta)$. The loss function $L(\\theta)$ is a sum of four terms, which we can denote as $L_{f}(\\theta)$, $L_{0}(\\theta)$, $L_{D}(\\theta)$, and $L_{N}(\\theta)$.\n$$\nL(\\theta) = L_{f}(\\theta) + L_{0}(\\theta) + L_{D}(\\theta) + L_{N}(\\theta)\n$$\nBy the linearity of the gradient operator, we have:\n$$\n\\nabla_{\\theta} L(\\theta) = \\nabla_{\\theta} L_{f}(\\theta) + \\nabla_{\\theta} L_{0}(\\theta) + \\nabla_{\\theta} L_{D}(\\theta) + \\nabla_{\\theta} L_{N}(\\theta)\n$$\nWe will compute the gradient for each term individually using the chain rule. For a generic term of the form $\\sum_{i} (r_i(\\theta))^2$, the gradient is $\\sum_{i} 2 r_i(\\theta) \\nabla_{\\theta} r_i(\\theta)$.\n\n1.  **PDE Residual Term Gradient**:\n    The PDE loss term is $L_{f}(\\theta) = \\frac{w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} (r_{f}(x_{i},t_{i};\\theta))^{2}$.\n    Its gradient is:\n    $$\n    \\nabla_{\\theta} L_{f}(\\theta) = \\frac{2 w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} r_{f}(x_{i},t_{i};\\theta) \\nabla_{\\theta} r_{f}(x_{i},t_{i};\\theta)\n    $$\n    The PDE residual is $r_{f}(x,t;\\theta) = u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t)$. We compute its gradient with respect to $\\theta$:\n    $$\n    \\nabla_{\\theta} r_{f}(x,t;\\theta) = \\nabla_{\\theta} (u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t))\n    $$\n    Using the linearity of the gradient operator:\n    $$\n    \\nabla_{\\theta} r_{f}(x,t;\\theta) = \\nabla_{\\theta} u_{\\theta,t}(x,t) - \\alpha \\nabla_{\\theta} (\\Delta u_{\\theta}(x,t))\n    $$\n    The problem states to assume that differentiation with respect to inputs $(x,t)$ and parameters $\\theta$ commutes. This allows us to write $\\nabla_{\\theta} u_{\\theta,t} = \\frac{\\partial}{\\partial t}(\\nabla_{\\theta} u_{\\theta})$ and $\\nabla_{\\theta} (\\Delta u_{\\theta}) = \\Delta(\\nabla_{\\theta} u_{\\theta})$. Using the problem's notation $J_{\\theta}[\\cdot] = \\nabla_{\\theta}(\\cdot)$, we have $\\nabla_{\\theta}u_{\\theta} = J_{\\theta}[u_{\\theta}]$ and $\\nabla_{\\theta}u_{\\theta,t} = J_{\\theta}[u_{\\theta,t}]$. Thus, the gradient of the residual becomes:\n    $$\n    \\nabla_{\\theta} r_{f}(x,t;\\theta) = J_{\\theta}[u_{\\theta,t}](x,t) - \\alpha \\Delta(J_{\\theta}[u_{\\theta}](x,t))\n    $$\n\n2.  **Initial Condition Residual Term Gradient**:\n    The initial condition loss term is $L_{0}(\\theta) = \\frac{w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} (r_{0}(x_{j};\\theta))^{2}$.\n    Its gradient is:\n    $$\n    \\nabla_{\\theta} L_{0}(\\theta) = \\frac{2 w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} r_{0}(x_{j};\\theta) \\nabla_{\\theta} r_{0}(x_{j};\\theta)\n    $$\n    The initial residual is $r_{0}(x;\\theta) = u_{\\theta}(x,0) - u_{0}(x)$. The function $u_{0}(x)$ is a given initial condition and is independent of $\\theta$, so $\\nabla_{\\theta} u_{0}(x) = 0$.\n    $$\n    \\nabla_{\\theta} r_{0}(x;\\theta) = \\nabla_{\\theta} u_{\\theta}(x,0) = J_{\\theta}[u_{\\theta}](x,0)\n    $$\n\n3.  **Dirichlet Boundary Residual Term Gradient**:\n    The Dirichlet boundary loss term is $L_{D}(\\theta) = \\frac{w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} (r_{D}(s_{k},\\tau_{k};\\theta))^{2}$.\n    Its gradient is:\n    $$\n    \\nabla_{\\theta} L_{D}(\\theta) = \\frac{2 w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} r_{D}(s_{k},\\tau_{k};\\theta) \\nabla_{\\theta} r_{D}(s_{k},\\tau_{k};\\theta)\n    $$\n    The Dirichlet residual is $r_{D}(s,t;\\theta) = u_{\\theta}(s,t) - g(s,t)$. The function $g(s,t)$ is a given boundary condition and is independent of $\\theta$, so $\\nabla_{\\theta} g(s,t) = 0$.\n    $$\n    \\nabla_{\\theta} r_{D}(s,t;\\theta) = \\nabla_{\\theta} u_{\\theta}(s,t) = J_{\\theta}[u_{\\theta}](s,t)\n    $$\n\n4.  **Neumann Boundary Residual Term Gradient**:\n    The Neumann boundary loss term is $L_{N}(\\theta) = \\frac{w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} (r_{N}(s_{\\ell},\\tau_{\\ell};\\theta))^{2}$.\n    Its gradient is:\n    $$\n    \\nabla_{\\theta} L_{N}(\\theta) = \\frac{2 w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} r_{N}(s_{\\ell},\\tau_{\\ell};\\theta) \\nabla_{\\theta} r_{N}(s_{\\ell},\\tau_{\\ell};\\theta)\n    $$\n    The Neumann residual is $r_{N}(s,t;\\theta) = n(s) \\cdot \\nabla u_{\\theta}(s,t) - h(s,t)$. The functions $n(s)$ and $h(s,t)$ are given and independent of $\\theta$.\n    $$\n    \\nabla_{\\theta} r_{N}(s,t;\\theta) = \\nabla_{\\theta} (n(s) \\cdot \\nabla u_{\\theta}(s,t))\n    $$\n    The problem provides the specific notation $J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)$ for this term. Thus:\n    $$\n    \\nabla_{\\theta} r_{N}(s,t;\\theta) = J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)\n    $$\n\nCombining these four results, we obtain the complete expression for the gradient $\\nabla_{\\theta} L(\\theta)$. The expression is a sum over the four collocation point sets, where each term in the sum is a product of the residual at a point and the gradient of that residual, scaled by a constant factor.",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\nabla_{\\theta} L(\\theta) =\n \\frac{2 w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} r_{f}(x_{i}, t_{i}; \\theta) \\left( J_{\\theta}[u_{\\theta,t}](x_{i}, t_{i}) - \\alpha \\Delta J_{\\theta}[u_{\\theta}](x_{i}, t_{i}) \\right) \\\\\n+  \\frac{2 w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} r_{0}(x_{j}; \\theta) J_{\\theta}[u_{\\theta}](x_{j}, 0) \\\\\n+  \\frac{2 w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} r_{D}(s_{k}, \\tau_{k}; \\theta) J_{\\theta}[u_{\\theta}](s_{k}, \\tau_{k}) \\\\\n+  \\frac{2 w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} r_{N}(s_{\\ell}, \\tau_{\\ell}; \\theta) J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s_{\\ell}, \\tau_{\\ell})\n\\end{aligned}\n}\n$$"
        },
        {
            "introduction": "For digital twins to be truly reliable decision-making tools, they must not only provide predictions but also quantify the uncertainty in those predictions. This practice moves beyond deterministic models into the realm of Bayesian Physics-Informed Machine Learning. By framing the problem probabilistically, you will learn how to infer a full posterior distribution over the solution, enabling you to calculate credible intervals and assess the model's confidence .",
            "id": "4235615",
            "problem": "Consider a one-dimensional steady-state heat conduction model for a rod of length $1\\,\\mathrm{m}$ with thermal conductivity $k$ in $\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$ and internal volumetric heat generation $q(x)$ in $\\mathrm{W}/\\mathrm{m}^3$. The governing equation is the second-order ordinary differential equation (ODE) $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ for $x \\in (0,1)$, with Dirichlet boundary conditions $u(0) = 0\\,\\mathrm{K}$ and $u(1) = 0\\,\\mathrm{K}$. In a Physics-Informed Neural Network (PINN), the physics-based residual is enforced at a set of collocation points $\\{x_i\\}_{i=1}^N$ via an operator $\\mathcal{L}$ acting on the trial solution $u_\\theta(x)$ parameterized by $\\theta$. The likelihood is defined using the residuals $\\mathcal{L}u_\\theta(x_i) - g(x_i)$, where $g(x) = q(x)$, under heteroscedastic noise (noise variance depends on $x$). You will construct a Bayesian PINN that uses a linear parameterization of $u_\\theta(x)$ over boundary-conforming basis functions and compute the posterior predictive distribution at specified sensor locations.\n\nStart from the following fundamental bases: (i) the steady-state heat equation $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ and the definition of the physics residual $\\mathcal{R}(x) := \\mathcal{L}u_\\theta(x) - g(x)$ with $\\mathcal{L} = -k\\,\\frac{d^2}{dx^2}$; (ii) the Bayesian linear regression model with Gaussian prior and Gaussian likelihood; and (iii) properties of Gaussian distributions under linear transformations.\n\nParameterize the solution using a linear expansion that satisfies the boundary conditions: $u_\\theta(x) = \\sum_{j=1}^{K} \\theta_j\\,b_j(x)$ with $b_j(x) = x^{n_j} - x^{n_j+1}$, where $n_j = j+1$. These basis functions satisfy $b_j(0) = 0$ and $b_j(1) = 0$ for all $j$, ensuring $u_\\theta(0) = 0$ and $u_\\theta(1) = 0$. The operator applied to $u_\\theta$ at a point $x$ is $\\mathcal{L}u_\\theta(x) = -k \\sum_{j=1}^{K} \\theta_j\\,b_j''(x)$, where $b_j''(x) = n_j(n_j-1)\\,x^{n_j-2} - (n_j+1)n_j\\,x^{n_j-1}$. Define the heteroscedastic residual noise variance as $\\sigma^2(x) = \\sigma_0^2\\,(1 + \\beta x^2)$, where $\\sigma_0  0$ and $\\beta \\ge 0$ are given constants. Assume a zero-mean Gaussian prior over parameters $\\theta \\sim \\mathcal{N}(0, \\lambda^{-1} I_K)$ with precision $\\lambda  0$.\n\nAt collocation points $\\{x_i\\}_{i=1}^N$, define the linear system $\\mathbf{y} = A\\theta + \\varepsilon$, where $\\mathbf{y}_i = g(x_i)$, $A_{i,j} = -k\\,b_j''(x_i)$, and $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$ with $\\Sigma = \\mathrm{diag}(\\sigma^2(x_1), \\dots, \\sigma^2(x_N))$. Using this, derive the posterior distribution over $\\theta$ and then compute the posterior predictive distribution of $u_\\theta(x)$ at given sensor locations $\\{s_\\ell\\}_{\\ell=1}^M$. Each posterior predictive mean must be expressed in $\\mathrm{K}$ and each posterior predictive variance must be expressed in $\\mathrm{K}^2$.\n\nYour task is to implement a complete, runnable program that:\n- Constructs the matrix $A$ and vector $\\mathbf{y}$ for each test case using the specified $k$, $K$, collocation count $N$, source term $q(x)$, and heteroscedastic variance function $\\sigma^2(x)$.\n- Computes the posterior mean vector $m$ and posterior covariance matrix $S$ of $\\theta$ under the Gaussian prior and Gaussian likelihood model.\n- For each sensor location $s_\\ell$, computes the posterior predictive mean $\\mu_\\ell = b(s_\\ell)^\\top m$ and variance $\\nu_\\ell = b(s_\\ell)^\\top S\\,b(s_\\ell)$, where $b(s_\\ell) = [b_1(s_\\ell), \\dots, b_K(s_\\ell)]^\\top$.\n- Aggregates the numerical results for each test case into a flat list of floats ordered as $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots, \\mu_M, \\nu_M]$.\n\nUse the following source term $q(x) = Q_0\\,\\sin(\\pi x)$, where $Q_0$ is a case-specific constant in $\\mathrm{W}/\\mathrm{m}^3$, and use evenly spaced interior collocation points $x_i = \\frac{i}{N+1}$ for $i = 1, \\dots, N$.\n\nTest Suite:\n- Case A (happy path): $K=3$, $N=25$, $k=10\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=100\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=0.3$, $\\beta=2$, $\\lambda=10$, sensors at $[0.2, 0.5, 0.8]$.\n- Case B (edge, high noise and few points): $K=5$, $N=10$, $k=5\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=50\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=1.0$, $\\beta=5$, $\\lambda=1$, sensors at $[0.1, 0.9]$.\n- Case C (edge, strong physics, many points): $K=4$, $N=50$, $k=15\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=120\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=0.1$, $\\beta=1$, $\\lambda=50$, sensors at $[0.33, 0.66]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case. For each test case, output a flat list of floats in the order $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots]$ with $\\mu_\\ell$ in $\\mathrm{K}$ and $\\nu_\\ell$ in $\\mathrm{K}^2$. For example, the top-level output must look like $[[\\mu_1,\\nu_1,\\mu_2,\\nu_2], [\\dots], [\\dots]]$.",
            "solution": "The problem is well-posed and scientifically grounded, providing a complete specification for constructing a Bayesian Physics-Informed Neural Network (PINN) for a one-dimensional steady-state heat conduction problem. We will proceed with a full derivation and solution.\n\nThe problem is to determine the posterior predictive distribution of the temperature field $u(x)$ governed by the differential equation $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ on the domain $x \\in (0,1)$, with Dirichlet boundary conditions $u(0) = 0\\,\\mathrm{K}$ and $u(1) = 0\\,\\mathrm{K}$. This is accomplished within a Bayesian linear regression framework.\n\nFirst, we define the trial solution $u_\\theta(x)$ which is parameterized by a vector of weights $\\theta = [\\theta_1, \\dots, \\theta_K]^\\top$. The solution is represented as a linear combination of basis functions $b_j(x)$ that inherently satisfy the boundary conditions:\n$$\nu_\\theta(x) = \\sum_{j=1}^{K} \\theta_j\\,b_j(x)\n$$\nThe basis functions are given by $b_j(x) = x^{n_j} - x^{n_j+1}$ with $n_j = j+1$. For any $j \\geq 1$, we have $n_j \\geq 2$, so $b_j(0) = 0^{n_j} - 0^{n_j+1} = 0$ and $b_j(1) = 1^{n_j} - 1^{n_j+1} = 1 - 1 = 0$, thus satisfying the boundary conditions for any choice of $\\theta$.\n\nThe core of the PINN methodology is to enforce the governing physics equation. We define a physics-based residual operator $\\mathcal{L} = -k\\,\\frac{d^2}{dx^2}$. Applying this operator to our trial solution yields:\n$$\n\\mathcal{L}u_\\theta(x) = -k\\,\\frac{d^2}{dx^2} \\left( \\sum_{j=1}^{K} \\theta_j\\,b_j(x) \\right) = \\sum_{j=1}^{K} \\theta_j \\left( -k\\,b_j''(x) \\right)\n$$\nwhere $b_j''(x) = \\frac{d^2}{dx^2}(x^{n_j} - x^{n_j+1}) = n_j(n_j-1)x^{n_j-2} - (n_j+1)n_j x^{n_j-1}$.\n\nThe physics residual is defined as the discrepancy $\\mathcal{R}(x) = \\mathcal{L}u_\\theta(x) - q(x)$. The model assumes that the governing equation holds true up to some random noise, i.e., $\\mathcal{L}u_\\theta(x_i) = q(x_i) + \\varepsilon_i$ at a set of $N$ collocation points $\\{x_i\\}_{i=1}^N$. This forms a linear system of equations. Let $\\mathbf{y}$ be the vector of source term evaluations, where $\\mathbf{y}_i = q(x_i)$, and let $A$ be the design matrix where $A_{ij} = -k\\,b_j''(x_i)$. The system can be written in vector form as:\n$$\n\\mathbf{y} = A\\theta + \\varepsilon\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^N$, $A \\in \\mathbb{R}^{N \\times K}$, $\\theta \\in \\mathbb{R}^K$, and $\\varepsilon \\in \\mathbb{R}^N$. The noise vector $\\varepsilon$ is assumed to follow a zero-mean Gaussian distribution, $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$, with a diagonal covariance matrix $\\Sigma$ reflecting heteroscedastic (i.e., position-dependent) and uncorrelated noise. The diagonal entries are given by the specified variance function $\\Sigma_{ii} = \\sigma^2(x_i) = \\sigma_0^2(1 + \\beta x_i^2)$.\n\nThis setup defines the likelihood distribution of the data $\\mathbf{y}$ given the parameters $\\theta$:\n$$\np(\\mathbf{y}|\\theta) = \\mathcal{N}(\\mathbf{y} | A\\theta, \\Sigma) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - A\\theta)^\\top \\Sigma^{-1} (\\mathbf{y} - A\\theta)\\right)\n$$\n\nFor the Bayesian treatment, we introduce a prior distribution over the parameters $\\theta$. A zero-mean Gaussian prior is specified, $p(\\theta) = \\mathcal{N}(\\theta | 0, \\lambda^{-1}I_K)$, where $\\lambda  0$ is the precision parameter and $I_K$ is the $K \\times K$ identity matrix. This prior encourages smaller parameter values, acting as a form of regularization.\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ is proportional to the product of the likelihood and the prior: $p(\\theta|\\mathbf{y}) \\propto p(\\mathbf{y}|\\theta)p(\\theta)$. Since both distributions are Gaussian, the posterior is also Gaussian, $p(\\theta|\\mathbf{y}) = \\mathcal{N}(\\theta|m, S)$, where $m$ is the posterior mean and $S$ is the posterior covariance. By combining the exponents of the likelihood and prior and completing the square for $\\theta$, we find the posterior parameters:\nThe posterior precision matrix $S^{-1}$ is the sum of the prior precision and the precision from the data:\n$$\nS^{-1} = A^\\top \\Sigma^{-1} A + \\lambda I_K\n$$\nThe posterior covariance is its inverse:\n$$\nS = (A^\\top \\Sigma^{-1} A + \\lambda I_K)^{-1}\n$$\nThe posterior mean vector $m$ is given by:\n$$\nm = S (A^\\top \\Sigma^{-1} \\mathbf{y})\n$$\n\nWith the posterior distribution over $\\theta$ established, we can compute the posterior predictive distribution for the temperature $u(s)$ at any new sensor location $s$. The prediction at $s$ is a linear function of $\\theta$:\n$$\nu_\\theta(s) = \\sum_{j=1}^K \\theta_j b_j(s) = b(s)^\\top \\theta\n$$\nwhere $b(s) = [b_1(s), \\dots, b_K(s)]^\\top$ is the vector of basis functions evaluated at $s$. Since $u_\\theta(s)$ is a linear transformation of the Gaussian-distributed random variable $\\theta$, the resulting posterior predictive distribution for $u_\\theta(s)$ is also Gaussian. Its mean $\\mu_s$ and variance $\\nu_s$ are found using the properties of linear transformations of random variables:\nThe posterior predictive mean is:\n$$\n\\mu_s = \\mathbb{E}[u_\\theta(s) | \\mathbf{y}] = b(s)^\\top \\mathbb{E}[\\theta | \\mathbf{y}] = b(s)^\\top m\n$$\nThe posterior predictive variance is:\n$$\n\\nu_s = \\mathrm{Var}[u_\\theta(s) | \\mathbf{y}] = b(s)^\\top \\mathrm{Var}[\\theta | \\mathbf{y}] b(s) = b(s)^\\top S b(s)\n$$\nFor each sensor location $s_\\ell$ in the test cases, we will compute $\\mu_\\ell$ and $\\nu_\\ell$ using these formulas.\n\nThe computational procedure for each test case is as follows:\n1.  Discretize the domain by generating $N$ collocation points $x_i = \\frac{i}{N+1}$ for $i=1, \\dots, N$.\n2.  Construct the $N \\times K$ matrix $A$ where $A_{ij} = -k\\,b_j''(x_i)$ and the $N \\times 1$ vector $\\mathbf{y}$ where $\\mathbf{y}_i = Q_0\\,\\sin(\\pi x_i)$.\n3.  Construct the diagonal inverse covariance matrix $\\Sigma^{-1}$, whose diagonal elements are $1/\\sigma^2(x_i) = 1/(\\sigma_0^2(1 + \\beta x_i^2))$.\n4.  Calculate the posterior covariance $S = (A^\\top \\Sigma^{-1} A + \\lambda I_K)^{-1}$ and posterior mean $m = S (A^\\top \\Sigma^{-1} \\mathbf{y})$.\n5.  For each sensor location $s_\\ell$, construct the basis vector $b(s_\\ell)$ and compute the predictive mean $\\mu_\\ell = b(s_\\ell)^\\top m$ and variance $\\nu_\\ell = b(s_\\ell)^\\top S b(s_\\ell)$.\n6.  Aggregate the results $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots]$ into a list for the test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian PINN problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case A: Happy path\n        {'K': 3, 'N': 25, 'k': 10.0, 'Q0': 100.0, 'sigma0': 0.3, 'beta': 2.0, 'lambda_': 10.0, 'sensors': [0.2, 0.5, 0.8]},\n        # Case B: Edge, high noise and few points\n        {'K': 5, 'N': 10, 'k': 5.0, 'Q0': 50.0, 'sigma0': 1.0, 'beta': 5.0, 'lambda_': 1.0, 'sensors': [0.1, 0.9]},\n        # Case C: Edge, strong physics, many points\n        {'K': 4, 'N': 50, 'k': 15.0, 'Q0': 120.0, 'sigma0': 0.1, 'beta': 1.0, 'lambda_': 50.0, 'sensors': [0.33, 0.66]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        K = case['K']\n        N = case['N']\n        k = case['k']\n        Q0 = case['Q0']\n        sigma0 = case['sigma0']\n        beta = case['beta']\n        lambda_ = case['lambda_']\n        sensors = case['sensors']\n\n        # 1. Generate collocation points\n        x_colloc = np.linspace(0, 1, N + 2)[1:-1] # Equally spaced interior points\n\n        # Define basis function and its second derivative\n        # n_j = j + 1\n        # b_j(x) = x^n_j - x^(n_j+1)\n        # b_j''(x) = n_j(n_j-1)x^(n_j-2) - (n_j+1)n_jx^(n_j-1)\n        \n        def basis_b(x, j):\n            n_j = j + 1\n            return x**n_j - x**(n_j + 1)\n\n        def basis_b_dd(x, j):\n            n_j = j + 1\n            term1 = 0\n            # For j=0 (n_j=1), n_j-2 = -1, which is ill-defined at x=0. But j starts from 1, so n_j = 2.\n            # a*x^0 = a.\n            if n_j - 2 == 0:\n                term1 = n_j * (n_j - 1)\n            elif n_j - 2  0:\n                term1 = n_j * (n_j - 1) * x**(n_j - 2)\n\n            term2 = (n_j + 1) * n_j * x**(n_j - 1)\n            return term1 - term2\n\n        # 2. Construct matrix A and vector y\n        A = np.zeros((N, K))\n        y = np.zeros(N)\n\n        for i in range(N):\n            xi = x_colloc[i]\n            y[i] = Q0 * np.sin(np.pi * xi)\n            for j_idx in range(K):\n                j = j_idx + 1 # Basis function index from 1 to K\n                A[i, j_idx] = -k * basis_b_dd(xi, j)\n\n        # 3. Construct inverse noise covariance diagonal\n        sigma_sq = sigma0**2 * (1 + beta * x_colloc**2)\n        sigma_sq_inv_diag = 1.0 / sigma_sq\n        \n        # 4. Compute posterior mean and covariance\n        # S_inv = A.T @ Sigma_inv @ A + lambda * I\n        # Efficient computation without forming diagonal matrix Sigma_inv\n        S_inv = (A.T * sigma_sq_inv_diag) @ A + lambda_ * np.identity(K)\n        S = np.linalg.inv(S_inv)\n        \n        # m = S @ A.T @ Sigma_inv @ y\n        m = S @ (A.T @ (sigma_sq_inv_diag * y))\n\n        # 5. Compute posterior predictive mean and variance at sensor locations\n        case_results = []\n        for s_ell in sensors:\n            # Construct basis vector b(s_ell)\n            b_s = np.zeros(K)\n            for j_idx in range(K):\n                j = j_idx + 1\n                b_s[j_idx] = basis_b(s_ell, j)\n            \n            # Predictive mean\n            mu_ell = b_s @ m\n            \n            # Predictive variance\n            nu_ell = b_s.T @ S @ b_s\n            \n            case_results.extend([mu_ell, nu_ell])\n        \n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation\n    # e.g., [1.23, 4.56] - \"[1.23, 4.56]\"\n    # Then join these strings with commas.\n    str_results = [str(res) for res in all_results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        }
    ]
}