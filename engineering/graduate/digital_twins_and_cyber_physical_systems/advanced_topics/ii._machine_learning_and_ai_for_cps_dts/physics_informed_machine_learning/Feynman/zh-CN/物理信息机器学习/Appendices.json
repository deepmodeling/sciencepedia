{
    "hands_on_practices": [
        {
            "introduction": "任何物理信息神经网络（PINN）应用的第一步都是将控制系统的偏微分方程（PDE）及其相关条件转化为一个可微的损失函数。这个过程是连接物理定律和机器学习优化框架的桥梁。通过以下实践，您将为经典的热传导方程构建一个完整的损失函数，它将方程本身、初始条件和边界条件（包括狄利克雷和诺伊曼类型）的残差都结合在一起，从而奠定训练PINN模型的基础。",
            "id": "4235644",
            "problem": "一个导热组件的数字孪生由热方程控制，这是一个线性抛物型偏微分方程（PDE）。考虑一个有界利普希茨空间域 $\\Omega \\subset \\mathbb{R}^{d}$，其边界 $\\partial \\Omega$ 分解为一个狄利克雷部分 $\\Gamma_{D}$ 和一个诺伊曼部分 $\\Gamma_{N}$，满足 $\\Gamma_{D} \\cup \\Gamma_{N} = \\partial \\Omega$ 和 $\\Gamma_{D} \\cap \\Gamma_{N} = \\emptyset$。令 $T \\in (0,\\infty)$ 表示最终时间。状态 $u:\\Omega \\times [0,T] \\to \\mathbb{R}$ 在 $\\Omega \\times (0,T]$ 上满足热方程 $u_{t} - \\alpha \\Delta u = 0$，其中常数热扩散系数 $\\alpha > 0$，初始条件为 $u(x,0) = u_{0}(x)$（对于 $x \\in \\Omega$），以及边界条件 $u(s,t) = g(s,t)$（对于 $(s,t) \\in \\Gamma_{D} \\times (0,T]$）和 $n(s) \\cdot \\nabla u(s,t) = h(s,t)$（对于 $(s,t) \\in \\Gamma_{N} \\times (0,T]$），其中 $n(s)$ 是在 $s \\in \\partial \\Omega$ 处的向外单位法向量。\n\n在物理信息神经网络（PINN）中，近似解 $u_{\\theta}(x,t)$ 由一个带有参数 $\\theta \\in \\mathbb{R}^{p}$ 的神经网络生成。为了训练此模型用于信息物理数字孪生，我们通过在有限点集上取经验平均值来惩罚对 PDE、初始条件和边界条件的违反：\n- 内部配置点集 $\\mathcal{Z}_{f} = \\{(x_{i}, t_{i})\\}_{i=1}^{N_{f}} \\subset \\Omega \\times (0,T]$，\n- 初始点集 $\\mathcal{X}_{0} = \\{x_{j}\\}_{j=1}^{N_{0}} \\subset \\Omega$，\n- 狄利克雷边界点集 $\\mathcal{Z}_{D} = \\{(s_{k}, \\tau_{k})\\}_{k=1}^{N_{D}} \\subset \\Gamma_{D} \\times (0,T]$，\n- 诺伊曼边界点集 $\\mathcal{Z}_{N} = \\{(s_{\\ell}, \\tau_{\\ell})\\}_{\\ell=1}^{N_{N}} \\subset \\Gamma_{N} \\times (0,T]$。\n\n定义残差\n- PDE 残差 $r_{f}(x,t;\\theta) := u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t)$，\n- 初始残差 $r_{0}(x;\\theta) := u_{\\theta}(x,0) - u_{0}(x)$，\n- 狄利克雷残差 $r_{D}(s,t;\\theta) := u_{\\theta}(s,t) - g(s,t)$，\n- 诺伊曼残差 $r_{N}(s,t;\\theta) := n(s)\\cdot \\nabla u_{\\theta}(s,t) - h(s,t)$。\n\n使用正常数权重 $w_{f}, w_{0}, w_{D}, w_{N}$，经验损失为\n$$\nL(\\theta)\n:= \\frac{w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} \\big(r_{f}(x_{i},t_{i};\\theta)\\big)^{2}\n+ \\frac{w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} \\big(r_{0}(x_{j};\\theta)\\big)^{2}\n+ \\frac{w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} \\big(r_{D}(s_{k},\\tau_{k};\\theta)\\big)^{2}\n+ \\frac{w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} \\big(r_{N}(s_{\\ell},\\tau_{\\ell};\\theta)\\big)^{2}.\n$$\n\n仅使用基本微积分（链式法则、微分的线性和光滑函数的偏导数可交换性）、用于评估神经网络相对于输入和参数的导数的自动微分（AD），以及上述定义，推导梯度向量 $\\nabla_{\\theta} L(\\theta)$ 的显式解析表达式，该表达式需用残差以及 $u_{\\theta}$ 及其所需输入导数的参数雅可比矩阵表示。您的最终答案必须是 $\\nabla_{\\theta} L(\\theta)$ 的单一闭式解析表达式，写成在上述数据集上的求和形式。最终答案中不要给出任何中间步骤。您可以假设 $u_{\\theta}$ 足够光滑，以至于关于输入和参数的微分可以交换。使用符号 $J_{\\theta}[\\cdot]$ 来表示关于 $\\theta$ 的梯度，例如 $J_{\\theta}[u_{\\theta}](x,t)$、$J_{\\theta}[u_{\\theta,t}](x,t)$ 和 $J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)$ 来表示您的结果。最终答案是符号化的；无需四舍五入或单位。",
            "solution": "所述问题经过了严格的验证过程。\n\n### 步骤1：提取已知条件\n- **PDE 和域**：状态 $u(x,t)$ 定义在空间域 $\\Omega \\subset \\mathbb{R}^{d}$ 和时间区间 $[0,T]$ 上。它在 $\\Omega \\times (0,T]$ 中满足热方程 $u_{t} - \\alpha \\Delta u = 0$，其中 $\\alpha > 0$ 是常数热扩散系数。\n- **初始条件**：$u(x,0) = u_{0}(x)$，对于 $x \\in \\Omega$。\n- **边界条件**：边界 $\\partial \\Omega$ 分解为 $\\Gamma_{D} \\cup \\Gamma_{N} = \\partial \\Omega$ 且 $\\Gamma_{D} \\cap \\Gamma_{N} = \\emptyset$。\n  - 狄利克雷：$u(s,t) = g(s,t)$，对于 $(s,t) \\in \\Gamma_{D} \\times (0,T]$。\n  - 诺伊曼：$n(s) \\cdot \\nabla u(s,t) = h(s,t)$，对于 $(s,t) \\in \\Gamma_{N} \\times (0,T]$，其中 $n(s)$ 是向外单位法向量。\n- **PINN 近似**：$u_{\\theta}(x,t)$ 是一个带有参数 $\\theta \\in \\mathbb{R}^{p}$ 的神经网络。\n- **配置点集**：\n  - 内部：$\\mathcal{Z}_{f} = \\{(x_{i}, t_{i})\\}_{i=1}^{N_{f}} \\subset \\Omega \\times (0,T]$。\n  - 初始：$\\mathcal{X}_{0} = \\{x_{j}\\}_{j=1}^{N_{0}} \\subset \\Omega$。\n  - 狄利克雷：$\\mathcal{Z}_{D} = \\{(s_{k}, \\tau_{k})\\}_{k=1}^{N_{D}} \\subset \\Gamma_{D} \\times (0,T]$。\n  - 诺伊曼：$\\mathcal{Z}_{N} = \\{(s_{\\ell}, \\tau_{\\ell})\\}_{\\ell=1}^{N_{N}} \\subset \\Gamma_{N} \\times (0,T]$。\n- **残差定义**：\n  - PDE：$r_{f}(x,t;\\theta) := u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t)$。\n  - 初始：$r_{0}(x;\\theta) := u_{\\theta}(x,0) - u_{0}(x)$。\n  - 狄利克雷：$r_{D}(s,t;\\theta) := u_{\\theta}(s,t) - g(s,t)$。\n  - 诺伊曼：$r_{N}(s,t;\\theta) := n(s)\\cdot \\nabla u_{\\theta}(s,t) - h(s,t)$。\n- **损失函数**：\n$$\nL(\\theta)\n:= \\frac{w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} \\big(r_{f}(x_{i},t_{i};\\theta)\\big)^{2}\n+ \\frac{w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} \\big(r_{0}(x_{j};\\theta)\\big)^{2}\n+ \\frac{w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} \\big(r_{D}(s_{k},\\tau_{k};\\theta)\\big)^{2}\n+ \\frac{w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} \\big(r_{N}(s_{\\ell},\\tau_{\\ell};\\theta)\\big)^{2}.\n$$\n- **任务约束**：\n  - 仅使用基本微积分（链式法则、线性、偏导数可交换性）。\n  - 概念上使用自动微分（AD）来评估导数。\n  - 推导梯度向量 $\\nabla_{\\theta} L(\\theta)$ 的显式解析表达式。\n  - 最终表达式必须以残差和参数雅可比矩阵表示。\n  - 假设 $u_{\\theta}$ 足够光滑。\n  - 使用符号 $J_{\\theta}[\\cdot]$ 表示关于 $\\theta$ 的梯度，例如 $J_{\\theta}[u_{\\theta}](x,t)$、$J_{\\theta}[u_{\\theta,t}](x,t)$ 和 $J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)$。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题描述了用于求解热方程的物理信息神经网络（PINN）的构建。这是科学机器学习领域中一种标准且成熟的方法。其基础物理（热方程）和数学框架（微积分、优化）都是合理的。该问题具有科学依据。\n- **适定性**：任务是计算一个定义良好、可微的损失函数 $L(\\theta)$ 的梯度。鉴于 $u_{\\theta}$ 是一个神经网络（可微函数的复合），并假设其足够光滑，其关于参数 $\\theta$ 的导数是明确定义的。推导 $\\nabla_{\\theta} L(\\theta)$ 是多元微积分中的一个标准练习。该问题是适定的。\n- **客观性**：问题使用形式化的数学语言和定义进行陈述。它完全不含主观、模糊或基于观点的陈述。\n\n该问题通过了所有验证标准，没有任何指定的缺陷。这是一个形式化、自洽且可解的数学问题。\n\n### 步骤3：结论与行动\n问题是有效的。将提供一个完整且论证充分的解答。\n\n### 解题推导\n目标是计算损失函数 $L(\\theta)$ 相对于神经网络参数 $\\theta$ 的梯度，记为 $\\nabla_{\\theta} L(\\theta)$。损失函数 $L(\\theta)$ 是四项之和，我们可以将其表示为 $L_{f}(\\theta)$、$L_{0}(\\theta)$、$L_{D}(\\theta)$ 和 $L_{N}(\\theta)$。\n$$\nL(\\theta) = L_{f}(\\theta) + L_{0}(\\theta) + L_{D}(\\theta) + L_{N}(\\theta)\n$$\n根据梯度算子的线性性质，我们有：\n$$\n\\nabla_{\\theta} L(\\theta) = \\nabla_{\\theta} L_{f}(\\theta) + \\nabla_{\\theta} L_{0}(\\theta) + \\nabla_{\\theta} L_{D}(\\theta) + \\nabla_{\\theta} L_{N}(\\theta)\n$$\n我们将使用链式法则分别计算每一项的梯度。对于形如 $\\sum_{i} (r_i(\\theta))^2$ 的通项，其梯度为 $\\sum_{i} 2 r_i(\\theta) \\nabla_{\\theta} r_i(\\theta)$。\n\n1.  **PDE 残差项梯度**：\n    PDE 损失项为 $L_{f}(\\theta) = \\frac{w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} (r_{f}(x_{i},t_{i};\\theta))^{2}$。\n    其梯度为：\n    $$\n    \\nabla_{\\theta} L_{f}(\\theta) = \\frac{2 w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} r_{f}(x_{i},t_{i};\\theta) \\nabla_{\\theta} r_{f}(x_{i},t_{i};\\theta)\n    $$\n    PDE 残差为 $r_{f}(x,t;\\theta) = u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t)$。我们计算其关于 $\\theta$ 的梯度：\n    $$\n    \\nabla_{\\theta} r_{f}(x,t;\\theta) = \\nabla_{\\theta} (u_{\\theta,t}(x,t) - \\alpha \\Delta u_{\\theta}(x,t))\n    $$\n    利用梯度算子的线性性质：\n    $$\n    \\nabla_{\\theta} r_{f}(x,t;\\theta) = \\nabla_{\\theta} u_{\\theta,t}(x,t) - \\alpha \\nabla_{\\theta} (\\Delta u_{\\theta}(x,t))\n    $$\n    问题陈述中假设关于输入 $(x,t)$ 和参数 $\\theta$ 的微分可以交换。这允许我们写出 $\\nabla_{\\theta} u_{\\theta,t} = \\frac{\\partial}{\\partial t}(\\nabla_{\\theta} u_{\\theta})$ 和 $\\nabla_{\\theta} (\\Delta u_{\\theta}) = \\Delta(\\nabla_{\\theta} u_{\\theta})$。使用问题中的符号 $J_{\\theta}[\\cdot] = \\nabla_{\\theta}(\\cdot)$，我们有 $\\nabla_{\\theta}u_{\\theta} = J_{\\theta}[u_{\\theta}]$ 和 $\\nabla_{\\theta}u_{\\theta,t} = J_{\\theta}[u_{\\theta,t}]$。此外，根据假设，我们可以将 $\\nabla_\\theta$ 与空间导数交换，得到 $\\nabla_\\theta(\\Delta u_\\theta) = \\Delta(\\nabla_\\theta u_\\theta) = \\Delta J_\\theta[u_\\theta]$。因此，残差的梯度变为：\n    $$\n    \\nabla_{\\theta} r_{f}(x,t;\\theta) = J_{\\theta}[u_{\\theta,t}](x,t) - \\alpha \\Delta(J_{\\theta}[u_{\\theta}](x,t))\n    $$\n\n2.  **初始条件残差项梯度**：\n    初始条件损失项为 $L_{0}(\\theta) = \\frac{w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} (r_{0}(x_{j};\\theta))^{2}$。\n    其梯度为：\n    $$\n    \\nabla_{\\theta} L_{0}(\\theta) = \\frac{2 w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} r_{0}(x_{j};\\theta) \\nabla_{\\theta} r_{0}(x_{j};\\theta)\n    $$\n    初始残差为 $r_{0}(x;\\theta) = u_{\\theta}(x,0) - u_{0}(x)$。函数 $u_{0}(x)$ 是给定的初始条件，与 $\\theta$ 无关，因此 $\\nabla_{\\theta} u_{0}(x) = 0$。\n    $$\n    \\nabla_{\\theta} r_{0}(x;\\theta) = \\nabla_{\\theta} u_{\\theta}(x,0) = J_{\\theta}[u_{\\theta}](x,0)\n    $$\n\n3.  **狄利克雷边界残差项梯度**：\n    狄利克雷边界损失项为 $L_{D}(\\theta) = \\frac{w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} (r_{D}(s_{k},\\tau_{k};\\theta))^{2}$。\n    其梯度为：\n    $$\n    \\nabla_{\\theta} L_{D}(\\theta) = \\frac{2 w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} r_{D}(s_{k},\\tau_{k};\\theta) \\nabla_{\\theta} r_{D}(s_{k},\\tau_{k};\\theta)\n    $$\n    狄利克雷残差为 $r_{D}(s,t;\\theta) = u_{\\theta}(s,t) - g(s,t)$。函数 $g(s,t)$ 是给定的边界条件，与 $\\theta$ 无关，因此 $\\nabla_{\\theta} g(s,t) = 0$。\n    $$\n    \\nabla_{\\theta} r_{D}(s,t;\\theta) = \\nabla_{\\theta} u_{\\theta}(s,t) = J_{\\theta}[u_{\\theta}](s,t)\n    $$\n\n4.  **诺伊曼边界残差项梯度**：\n    诺伊曼边界损失项为 $L_{N}(\\theta) = \\frac{w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} (r_{N}(s_{\\ell},\\tau_{\\ell};\\theta))^{2}$。\n    其梯度为：\n    $$\n    \\nabla_{\\theta} L_{N}(\\theta) = \\frac{2 w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} r_{N}(s_{\\ell},\\tau_{\\ell};\\theta) \\nabla_{\\theta} r_{N}(s_{\\ell},\\tau_{\\ell};\\theta)\n    $$\n    诺伊曼残差为 $r_{N}(s,t;\\theta) = n(s) \\cdot \\nabla u_{\\theta}(s,t) - h(s,t)$。函数 $n(s)$ 和 $h(s,t)$ 是给定的，且与 $\\theta$ 无关。\n    $$\n    \\nabla_{\\theta} r_{N}(s,t;\\theta) = \\nabla_{\\theta} (n(s) \\cdot \\nabla u_{\\theta}(s,t))\n    $$\n    问题为此项提供了特定符号 $J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)$。因此：\n    $$\n    \\nabla_{\\theta} r_{N}(s,t;\\theta) = J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s,t)\n    $$\n\n结合这四个结果，我们得到梯度 $\\nabla_{\\theta} L(\\theta)$ 的完整表达式。该表达式是四个配置点集上的和，其中和的每一项都是某点处的残差与该残差梯度的乘积，并按一个常数因子进行缩放。",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\nabla_{\\theta} L(\\theta) =\n\\frac{2 w_{f}}{N_{f}} \\sum_{i=1}^{N_{f}} r_{f}(x_{i}, t_{i}; \\theta) \\left( J_{\\theta}[u_{\\theta,t}](x_{i}, t_{i}) - \\alpha \\Delta J_{\\theta}[u_{\\theta}](x_{i}, t_{i}) \\right) \\\\\n+ \\frac{2 w_{0}}{N_{0}} \\sum_{j=1}^{N_{0}} r_{0}(x_{j}; \\theta) J_{\\theta}[u_{\\theta}](x_{j}, 0) \\\\\n+ \\frac{2 w_{D}}{N_{D}} \\sum_{k=1}^{N_{D}} r_{D}(s_{k}, \\tau_{k}; \\theta) J_{\\theta}[u_{\\theta}](s_{k}, \\tau_{k}) \\\\\n+ \\frac{2 w_{N}}{N_{N}} \\sum_{\\ell=1}^{N_{N}} r_{N}(s_{\\ell}, \\tau_{\\ell}; \\theta) J_{\\theta}[n \\cdot \\nabla u_{\\theta}](s_{\\ell}, \\tau_{\\ell})\n\\end{aligned}\n}\n$$"
        },
        {
            "introduction": "虽然通用损失函数方法可以通过惩罚项来“软”施加边界条件，但另一种更巧妙且通常更稳健的方法是直接设计神经网络结构，使其“硬”性地精确满足这些条件。这种方法被称为“构造法”或硬约束法，能将解空间限制在始终满足边界条件的函数族中，从而简化学习任务。这个练习将通过一个可解析求解的简单问题，让您清晰地理解这种结构化施加约束的原理及其对优化过程的优势。",
            "id": "4235640",
            "problem": "一维无量纲化热棒（一种典型的信息物理系统）的数字孪生，在稳态下由区间 $\\Omega = [0,1]$ 上的二阶微分方程 $-u''(x) = q(x)$ 描述，并带有狄利克雷边界条件 $u|_{\\partial\\Omega} = h$，即 $u(0) = h(0)$ 和 $u(1) = h(1)$。为了在物理信息神经网络（PINN）中精确地施加边界条件，考虑以下参数化形式\n$$\nu_{\\theta}(x) = h(x) + \\eta(x)\\,N_{\\theta}(x),\n$$\n其中 $\\eta:\\Omega\\to\\mathbb{R}$ 满足 $\\eta(0)=\\eta(1)=0$，$N_{\\theta}$ 是一个可训练函数。设 $h(x) = \\alpha + (\\beta - \\alpha)\\,x$，其中 $\\alpha,\\beta\\in\\mathbb{R}$ 为固定常数，选择 $\\eta(x)=x(1-x)$，并选择可训练部分为二次多项式 $N_{\\theta}(x) = \\theta_{0} + \\theta_{1}\\,x + \\theta_{2}\\,x^{2}$。\n\n从第一性原理的角度出发，首先确定 $u_{\\theta}$ 所属的函数空间，以及精确的狄利克雷约束如何改变可训练函数的可行空间。然后，通过要求强形式残差恒为零来施加物理约束，\n$$\n-u_{\\theta}''(x) \\equiv q(x) \\quad \\text{for all } x \\in [0,1],\n$$\n其中无量纲源项为 $q(x) = 12\\,x^{2} + 6\\,x + 2$。\n\n仅使用上述规定和一维热传导的基本定律，确定使残差恒为零的参数矢量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$。请以精确值的形式给出你的最终答案 $(\\theta_{0}, \\theta_{1}, \\theta_{2})$。由于问题是在无量纲变量下提出的，答案没有物理单位。",
            "solution": "用户希望我按照以下步骤解决问题：\n1. 验证问题。\n2. 如果有效，提供详细的解决方案。\n3. 解决方案必须对所有数学符号使用 LaTeX。\n4. 最终答案必须采用特定的方框格式。\n\n### 步骤 1：提取已知条件\n- **控制微分方程：** $-u''(x) = q(x)$，在区间 $\\Omega = [0,1]$ 上。\n- **边界条件：** $u|_{\\partial\\Omega} = h$，意味着 $u(0) = h(0)$ 且 $u(1) = h(1)$。\n- **物理信息神经网络 (PINN) 拟设：** $u_{\\theta}(x) = h(x) + \\eta(x)\\,N_{\\theta}(x)$。\n- **边界条件函数：** $h(x) = \\alpha + (\\beta - \\alpha)\\,x$，其中 $\\alpha, \\beta \\in \\mathbb{R}$ 为固定常数。\n- **乘子函数：** $\\eta(x) = x(1-x)$，满足 $\\eta(0)=\\eta(1)=0$。\n- **可训练函数（神经网络代理）：** $N_{\\theta}(x) = \\theta_{0} + \\theta_{1}\\,x + \\theta_{2}\\,x^{2}$。\n- **源项：** $q(x) = 12\\,x^{2} + 6\\,x + 2$。\n- **物理约束：** 强形式残差必须恒为零：$-u_{\\theta}''(x) \\equiv q(x)$，对所有 $x \\in [0,1]$ 成立。\n- **目标：** 确定参数矢量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据：** 该问题描述了一个一维稳态热传导（泊松）问题。使用特定函数形式 $u_{\\theta}(x) = h(x) + \\eta(x) N_{\\theta}(x)$ 来精确满足狄利克雷边界条件的方法，是物理信息机器学习领域的标准有效技术。该问题在科学上和数学上都是合理的。\n2.  **适定性：** 所有必要的函数（$h(x)$、$\\eta(x)$、$N_{\\theta}(x)$、$q(x)$）都已明确定义。任务是通过强制一个多项式恒等式来找到多项式的系数。这会导出一个关于参数 $\\theta_i$ 的适定线性方程组，该方程组应有唯一解。\n3.  **目标：** 问题以精确的数学语言陈述，没有主观或模糊的术语。\n4.  **完整性：** 问题是自洽的，并提供了求得唯一解所需的所有信息。\n5.  **无其他缺陷：** 该问题不违反任何其他无效标准。它是一个典型的例子，用于说明构建 PINN 解的原理。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解决方案。\n\n### 解题推导\n\n该问题要求找到参数矢量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$，使得函数 $u_{\\theta}(x)$ 对所有 $x \\in [0,1]$ 都精确满足微分方程 $-u_{\\theta}''(x) = q(x)$。\n\n首先，我们讨论解空间的结构。为了使微分方程的强形式成立，解 $u(x)$ 必须至少是二次连续可微的，即 $u \\in C^2([0,1])$。问题指定了非齐次狄利克雷边界条件 $u(0)=h(0)=\\alpha$ 和 $u(1)=h(1)=\\beta$。所有此类函数的集合构成一个仿射空间。参数化 $u_{\\theta}(x) = h(x) + \\eta(x) N_{\\theta}(x)$ 巧妙地转换了这个问题。根据构造，$u_{\\theta}(x)$ 对 $N_{\\theta}(x)$ 的任何选择都自动满足边界条件，因为 $\\eta(x)$ 在边界处为零：\n$u_{\\theta}(0) = h(0) + \\eta(0) N_{\\theta}(0) = h(0) + 0 = h(0) = \\alpha$。\n$u_{\\theta}(1) = h(1) + \\eta(1) N_{\\theta}(1) = h(1) + 0 = h(1) = \\beta$。\n因此，边界条件被硬编码到拟设中。任务简化为找到满足微分方程本身的特定 $N_{\\theta}(x)$。在这个问题中，$N_{\\theta}(x)$ 不是一个神经网络，而是一个简单的多项式，这使得精确的解析解成为可能。\n\n我们首先通过代入给定的 $h(x)$、$\\eta(x)$ 和 $N_{\\theta}(x)$ 的形式来构建 $u_{\\theta}(x)$ 的完整表达式：\n$$\nu_{\\theta}(x) = \\left( \\alpha + (\\beta - \\alpha)x \\right) + \\left( x(1-x) \\right) \\left( \\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} \\right)\n$$\n展开乘积 $\\eta(x)N_{\\theta}(x)$：\n$$\n\\eta(x)N_{\\theta}(x) = (x - x^2)(\\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2}) = \\theta_{0}x + \\theta_{1}x^2 + \\theta_{2}x^3 - \\theta_{0}x^2 - \\theta_{1}x^3 - \\theta_{2}x^4\n$$\n按 $x$ 的幂次合并同类项：\n$$\n\\eta(x)N_{\\theta}(x) = \\theta_{0}x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\n现在，将其代回 $u_{\\theta}(x)$ 的表达式中：\n$$\nu_{\\theta}(x) = \\alpha + (\\beta - \\alpha)x + \\theta_{0}x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\n可以写成一个单一的多项式：\n$$\nu_{\\theta}(x) = \\alpha + (\\beta - \\alpha + \\theta_{0})x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\n接下来，我们计算二阶导数 $u_{\\theta}''(x)$。首先，一阶导数 $u_{\\theta}'(x)$ 是：\n$$\nu_{\\theta}'(x) = \\frac{d}{dx} \\left( \\alpha + (\\beta - \\alpha + \\theta_{0})x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4 \\right)\n$$\n$$\nu_{\\theta}'(x) = (\\beta - \\alpha + \\theta_{0}) + 2(\\theta_{1}-\\theta_{0})x + 3(\\theta_{2}-\\theta_{1})x^2 - 4\\theta_{2}x^3\n$$\n现在，我们计算二阶导数 $u_{\\theta}''(x)$：\n$$\nu_{\\theta}''(x) = \\frac{d}{dx} \\left( (\\beta - \\alpha + \\theta_{0}) + 2(\\theta_{1}-\\theta_{0})x + 3(\\theta_{2}-\\theta_{1})x^2 - 4\\theta_{2}x^3 \\right)\n$$\n$$\nu_{\\theta}''(x) = 2(\\theta_{1}-\\theta_{0}) + 6(\\theta_{2}-\\theta_{1})x - 12\\theta_{2}x^2\n$$\n控制方程为 $-u_{\\theta}''(x) = q(x)$。让我们计算 $-u_{\\theta}''(x)$：\n$$\n-u_{\\theta}''(x) = - \\left( 2(\\theta_{1}-\\theta_{0}) + 6(\\theta_{2}-\\theta_{1})x - 12\\theta_{2}x^2 \\right)\n$$\n$$\n-u_{\\theta}''(x) = (2\\theta_{0}-2\\theta_{1}) + (6\\theta_{1}-6\\theta_{2})x + 12\\theta_{2}x^2\n$$\n问题要求这个表达式与源项 $q(x) = 12x^2 + 6x + 2$ 恒等。\n$$\n(2\\theta_{0}-2\\theta_{1}) + (6\\theta_{1}-6\\theta_{2})x + 12\\theta_{2}x^2 \\equiv 2 + 6x + 12x^2\n$$\n要使两个多项式对所有 $x$ 都恒等，对应 $x$ 次幂的系数必须相等。这为我们提供了三个未知参数 $\\theta_0, \\theta_1, \\theta_2$ 的一个三元线性方程组。\n\n令 $x^2$ 的系数相等：\n$$\n12\\theta_{2} = 12 \\implies \\theta_{2} = 1\n$$\n令 $x^1$ 的系数相等：\n$$\n6\\theta_{1}-6\\theta_{2} = 6\n$$\n将值 $\\theta_{2}=1$ 代入此方程：\n$$\n6\\theta_{1}-6(1) = 6 \\implies 6\\theta_{1} = 12 \\implies \\theta_{1} = 2\n$$\n令常数项（$x^0$ 的系数）相等：\n$$\n2\\theta_{0}-2\\theta_{1} = 2\n$$\n将值 $\\theta_{1}=2$ 代入此方程：\n$$\n2\\theta_{0}-2(2) = 2 \\implies 2\\theta_{0} - 4 = 2 \\implies 2\\theta_{0} = 6 \\implies \\theta_{0} = 3\n$$\n因此，我们找到了使残差恒为零的唯一参数矢量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$。正如预期的那样，这些参数与边界条件常数 $\\alpha$ 和 $\\beta$ 无关，因为线性函数 $h(x)$ 的二阶导数为零，因此对残差 $-u_{\\theta}''(x)$ 没有贡献。\n\n参数矢量是 $(\\theta_{0}, \\theta_{1}, \\theta_{2}) = (3, 2, 1)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3  2  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "一个可靠的数字孪生不仅需要做出准确预测，还必须量化其预测的不确定性。为了实现这一点，我们必须从确定性模型走向概率模型。本练习将引导您进入贝叶斯物理信息神经网络的领域，其中模型参数$ \\theta $不再是固定值，而是被视为随机变量。您将为一个稳态热传导问题构建一个贝叶斯PINN，并学习如何推导后验预测分布，从而同时获得温度场的均值预测及其相关的不确定性（方差）。",
            "id": "4235615",
            "problem": "考虑一个长度为 $1\\,\\mathrm{m}$ 的杆的一维稳态热传导模型，其热导率为 $k$（单位 $\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$），内部体积产热率为 $q(x)$（单位 $\\mathrm{W}/\\mathrm{m}^3$）。其控制方程为二阶常微分方程 (ODE) $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$，其中 $x \\in (0,1)$，并带有狄利克雷边界条件 $u(0) = 0\\,\\mathrm{K}$ 和 $u(1) = 0\\,\\mathrm{K}$。在物理信息神经网络 (PINN) 中，通过一个作用于由 $\\theta$ 参数化的试探解 $u_\\theta(x)$ 的算子 $\\mathcal{L}$，在一组配置点 $\\{x_i\\}_{i=1}^N$ 上强制施加基于物理的残差。似然是使用残差 $\\mathcal{L}u_\\theta(x_i) - g(x_i)$ 定义的，其中 $g(x) = q(x)$，并且噪声为异方差噪声（即噪声方差取决于 $x$）。您将构建一个贝叶斯 PINN，该网络使用 $u_\\theta(x)$ 在满足边界条件的基函数上的线性参数化，并计算指定传感器位置的后验预测分布。\n\n从以下基本原理出发：(i) 稳态热传导方程 $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ 和物理残差的定义 $\\mathcal{R}(x) := \\mathcal{L}u_\\theta(x) - g(x)$，其中 $\\mathcal{L} = -k\\,\\frac{d^2}{dx^2}$；(ii) 具有高斯先验和高斯似然的贝叶斯线性回归模型；以及 (iii) 高斯分布在线性变换下的性质。\n\n使用满足边界条件的线性展开来参数化解：$u_\\theta(x) = \\sum_{j=1}^{K} \\theta_j\\,b_j(x)$，其中 $b_j(x) = x^{n_j} - x^{n_j+1}$ 且 $n_j = j+1$。对于所有 $j$，这些基函数都满足 $b_j(0) = 0$ 和 $b_j(1) = 0$，从而确保 $u_\\theta(0) = 0$ 和 $u_\\theta(1) = 0$。作用于 $u_\\theta$ 在点 $x$ 处的算子为 $\\mathcal{L}u_\\theta(x) = -k \\sum_{j=1}^{K} \\theta_j\\,b_j''(x)$，其中 $b_j''(x) = n_j(n_j-1)\\,x^{n_j-2} - (n_j+1)n_j\\,x^{n_j-1}$。将异方差残差噪声方差定义为 $\\sigma^2(x) = \\sigma_0^2\\,(1 + \\beta x^2)$，其中 $\\sigma_0 > 0$ 和 $\\beta \\ge 0$ 是给定的常数。假设参数 $\\theta$ 服从零均值高斯先验 $\\theta \\sim \\mathcal{N}(0, \\lambda^{-1} I_K)$，其精度为 $\\lambda > 0$。\n\n在配置点 $\\{x_i\\}_{i=1}^N$ 处，定义线性系统 $\\mathbf{y} = A\\theta + \\varepsilon$，其中 $\\mathbf{y}_i = g(x_i)$，$A_{i,j} = -k\\,b_j''(x_i)$，并且 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，其中 $\\Sigma = \\mathrm{diag}(\\sigma^2(x_1), \\dots, \\sigma^2(x_N))$。利用此模型，推导 $\\theta$ 的后验分布，然后计算在给定传感器位置 $\\{s_\\ell\\}_{\\ell=1}^M$ 处 $u_\\theta(x)$ 的后验预测分布。每个后验预测均值必须以 $\\mathrm{K}$ 为单位表示，每个后验预测方差必须以 $\\mathrm{K}^2$ 为单位表示。\n\n您的任务是实现一个完整的、可运行的程序，该程序能够：\n- 针对每个测试用例，使用指定的 $k$、$K$、配置点数量 $N$、源项 $q(x)$ 和异方差方差函数 $\\sigma^2(x)$ 来构建矩阵 $A$ 和向量 $\\mathbf{y}$。\n- 在高斯先验和高斯似然模型下，计算 $\\theta$ 的后验均值向量 $m$ 和后验协方差矩阵 $S$。\n- 对于每个传感器位置 $s_\\ell$，计算后验预测均值 $\\mu_\\ell = b(s_\\ell)^\\top m$ 和方差 $\\nu_\\ell = b(s_\\ell)^\\top S\\,b(s_\\ell)$，其中 $b(s_\\ell) = [b_1(s_\\ell), \\dots, b_K(s_\\ell)]^\\top$。\n- 将每个测试用例的数值结果汇总成一个浮点数的一维列表，顺序为 $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots, \\mu_M, \\nu_M]$。\n\n使用以下源项 $q(x) = Q_0\\,\\sin(\\pi x)$，其中 $Q_0$ 是一个特定于用例的常数，单位为 $\\mathrm{W}/\\mathrm{m}^3$。并使用均匀分布的内部配置点 $x_i = \\frac{i}{N+1}$，其中 $i = 1, \\dots, N$。\n\n测试套件：\n- 用例 A (正常路径)：$K=3$, $N=25$, $k=10\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=100\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=0.3$, $\\beta=2$, $\\lambda=10$, 传感器位于 $[0.2, 0.5, 0.8]$。\n- 用例 B (边缘情况，高噪声和少量点)：$K=5$, $N=10$, $k=5\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=50\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=1.0$, $\\beta=5$, $\\lambda=1$, 传感器位于 $[0.1, 0.9]$。\n- 用例 C (边缘情况，强物理约束和大量点)：$K=4$, $N=50$, $k=15\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=120\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=0.1$, $\\beta=1$, $\\lambda=50$, 传感器位于 $[0.33, 0.66]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例。对于每个测试用例，按 $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots]$ 的顺序输出一个浮点数的一维列表，其中 $\\mu_\\ell$ 的单位是 $\\mathrm{K}$，$\\nu_\\ell$ 的单位是 $\\mathrm{K}^2$。例如，顶层输出必须类似于 $[[\\mu_1,\\nu_1,\\mu_2,\\nu_2], [\\dots], [\\dots]]$。",
            "solution": "该问题提法明确且有科学依据，为构建用于一维稳态热传导问题的贝叶斯物理信息神经网络 (PINN) 提供了完整的规范。我们将进行完整的推导和求解。\n\n问题是在定义域 $x \\in (0,1)$上，求解由微分方程 $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ 和狄利克雷边界条件 $u(0) = 0\\,\\mathrm{K}$、$u(1) = 0\\,\\mathrm{K}$ 控制的温度场 $u(x)$ 的后验预测分布。这一任务在贝叶斯线性回归框架内完成。\n\n首先，我们定义试探解 $u_\\theta(x)$，它由权重向量 $\\theta = [\\theta_1, \\dots, \\theta_K]^\\top$ 参数化。该解表示为一组内在地满足边界条件的基函数 $b_j(x)$ 的线性组合：\n$$\nu_\\theta(x) = \\sum_{j=1}^{K} \\theta_j\\,b_j(x)\n$$\n基函数由 $b_j(x) = x^{n_j} - x^{n_j+1}$ 给出，其中 $n_j = j+1$。对于任何 $j \\geq 1$，我们有 $n_j \\geq 2$，因此 $b_j(0) = 0^{n_j} - 0^{n_j+1} = 0$ 且 $b_j(1) = 1^{n_j} - 1^{n_j+1} = 1 - 1 = 0$，从而对于任意选择的 $\\theta$ 都满足边界条件。\n\nPINN方法的核心是强制执行物理控制方程。我们定义一个基于物理的残差算子 $\\mathcal{L} = -k\\,\\frac{d^2}{dx^2}$。将此算子应用于我们的试探解可得：\n$$\n\\mathcal{L}u_\\theta(x) = -k\\,\\frac{d^2}{dx^2} \\left( \\sum_{j=1}^{K} \\theta_j\\,b_j(x) \\right) = \\sum_{j=1}^{K} \\theta_j \\left( -k\\,b_j''(x) \\right)\n$$\n其中 $b_j''(x) = \\frac{d^2}{dx^2}(x^{n_j} - x^{n_j+1}) = n_j(n_j-1)x^{n_j-2} - (n_j+1)n_j x^{n_j-1}$。\n\n物理残差定义为差异 $\\mathcal{R}(x) = \\mathcal{L}u_\\theta(x) - q(x)$。模型假设在存在一定随机噪声的情况下，控制方程是成立的，即在一组 $N$ 个配置点 $\\{x_i\\}_{i=1}^N$ 上，有 $\\mathcal{L}u_\\theta(x_i) = q(x_i) + \\varepsilon_i$。这就构成了一个线性方程组。令 $\\mathbf{y}$ 为源项求值向量，其中 $\\mathbf{y}_i = q(x_i)$，并令 $A$ 为设计矩阵，其中 $A_{ij} = -k\\,b_j''(x_i)$。该系统可以写成向量形式：\n$$\n\\mathbf{y} = A\\theta + \\varepsilon\n$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^N$，$A \\in \\mathbb{R}^{N \\times K}$，$\\theta \\in \\mathbb{R}^K$，$\\varepsilon \\in \\mathbb{R}^N$。噪声向量 $\\varepsilon$ 假定服从零均值高斯分布 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，其对角协方差矩阵 $\\Sigma$ 反映了异方差（即位置相关）且不相关的噪声。对角线元素由指定的方差函数给出：$\\Sigma_{ii} = \\sigma^2(x_i) = \\sigma_0^2(1 + \\beta x_i^2)$。\n\n这个设定定义了给定参数 $\\theta$ 时数据 $\\mathbf{y}$ 的似然分布：\n$$\np(\\mathbf{y}|\\theta) = \\mathcal{N}(\\mathbf{y} | A\\theta, \\Sigma) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - A\\theta)^\\top \\Sigma^{-1} (\\mathbf{y} - A\\theta)\\right)\n$$\n\n对于贝叶斯处理，我们引入参数 $\\theta$ 的先验分布。指定一个零均值高斯先验 $p(\\theta) = \\mathcal{N}(\\theta | 0, \\lambda^{-1}I_K)$，其中 $\\lambda > 0$ 是精度参数，$I_K$ 是 $K \\times K$ 的单位矩阵。该先验倾向于较小的参数值，起到一种正则化的作用。\n\n根据贝叶斯定理，$\\theta$ 的后验分布与似然和先验的乘积成正比：$p(\\theta|\\mathbf{y}) \\propto p(\\mathbf{y}|\\theta)p(\\theta)$。由于两个分布都是高斯分布，后验分布也是高斯分布，$p(\\theta|\\mathbf{y}) = \\mathcal{N}(\\theta|m, S)$，其中 $m$ 是后验均值，$S$ 是后验协方差。通过合并似然和先验的指数项并对 $\\theta$ 进行配方，我们可以找到后验参数：\n后验精度矩阵 $S^{-1}$ 是先验精度与数据精度的和：\n$$\nS^{-1} = A^\\top \\Sigma^{-1} A + \\lambda I_K\n$$\n后验协方差是其逆矩阵：\n$$\nS = (A^\\top \\Sigma^{-1} A + \\lambda I_K)^{-1}\n$$\n后验均值向量 $m$ 由下式给出：\n$$\nm = S (A^\\top \\Sigma^{-1} \\mathbf{y})\n$$\n\n在确定了 $\\theta$ 的后验分布后，我们可以计算在任何新的传感器位置 $s$ 处温度 $u(s)$ 的后验预测分布。在 $s$ 处的预测是 $\\theta$ 的一个线性函数：\n$$\nu_\\theta(s) = \\sum_{j=1}^K \\theta_j b_j(s) = b(s)^\\top \\theta\n$$\n其中 $b(s) = [b_1(s), \\dots, b_K(s)]^\\top$ 是在 $s$ 处求值的基函数向量。由于 $u_\\theta(s)$ 是服从高斯分布的随机变量 $\\theta$ 的线性变换，因此得到的 $u_\\theta(s)$ 的后验预测分布也是高斯分布。其均值 $\\mu_s$ 和方差 $\\nu_s$ 可以利用随机变量线性变换的性质求得：\n后验预测均值为：\n$$\n\\mu_s = \\mathbb{E}[u_\\theta(s) | \\mathbf{y}] = b(s)^\\top \\mathbb{E}[\\theta | \\mathbf{y}] = b(s)^\\top m\n$$\n后验预测方差为：\n$$\n\\nu_s = \\mathrm{Var}[u_\\theta(s) | \\mathbf{y}] = b(s)^\\top \\mathrm{Var}[\\theta | \\mathbf{y}] b(s) = b(s)^\\top S b(s)\n$$\n对于测试用例中的每个传感器位置 $s_\\ell$，我们将使用这些公式计算 $\\mu_\\ell$ 和 $\\nu_\\ell$。\n\n每个测试用例的计算过程如下：\n1.  通过生成 $N$ 个配置点 $x_i = \\frac{i}{N+1}$（其中 $i=1, \\dots, N$）来离散化定义域。\n2.  构建 $N \\times K$ 矩阵 $A$（其中 $A_{ij} = -k\\,b_j''(x_i)$）和 $N \\times 1$ 向量 $\\mathbf{y}$（其中 $\\mathbf{y}_i = Q_0\\,\\sin(\\pi x_i)$）。\n3.  构建对角逆协方差矩阵 $\\Sigma^{-1}$，其对角元素为 $1/\\sigma^2(x_i) = 1/(\\sigma_0^2(1 + \\beta x_i^2))$。\n4.  计算后验协方差 $S = (A^\\top \\Sigma^{-1} A + \\lambda I_K)^{-1}$ 和后验均值 $m = S (A^\\top \\Sigma^{-1} \\mathbf{y})$。\n5.  对于每个传感器位置 $s_\\ell$，构建基向量 $b(s_\\ell)$ 并计算预测均值 $\\mu_\\ell = b(s_\\ell)^\\top m$ 和方差 $\\nu_\\ell = b(s_\\ell)^\\top S b(s_\\ell)$。\n6.  将结果 $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots]$ 汇总成该测试用例的列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian PINN problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case A: Happy path\n        {'K': 3, 'N': 25, 'k': 10.0, 'Q0': 100.0, 'sigma0': 0.3, 'beta': 2.0, 'lambda_': 10.0, 'sensors': [0.2, 0.5, 0.8]},\n        # Case B: Edge, high noise and few points\n        {'K': 5, 'N': 10, 'k': 5.0, 'Q0': 50.0, 'sigma0': 1.0, 'beta': 5.0, 'lambda_': 1.0, 'sensors': [0.1, 0.9]},\n        # Case C: Edge, strong physics, many points\n        {'K': 4, 'N': 50, 'k': 15.0, 'Q0': 120.0, 'sigma0': 0.1, 'beta': 1.0, 'lambda_': 50.0, 'sensors': [0.33, 0.66]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        K = case['K']\n        N = case['N']\n        k = case['k']\n        Q0 = case['Q0']\n        sigma0 = case['sigma0']\n        beta = case['beta']\n        lambda_ = case['lambda_']\n        sensors = case['sensors']\n\n        # 1. Generate collocation points\n        x_colloc = np.linspace(0, 1, N + 2)[1:-1] # Equally spaced interior points\n\n        # Define basis function and its second derivative\n        # n_j = j + 1\n        # b_j(x) = x^n_j - x^(n_j+1)\n        # b_j''(x) = n_j(n_j-1)x^(n_j-2) - (n_j+1)n_jx^(n_j-1)\n        \n        def basis_b(x, j):\n            n_j = j + 1\n            return x**n_j - x**(n_j + 1)\n\n        def basis_b_dd(x, j):\n            n_j = j + 1\n            term1 = 0\n            # For j=0 (n_j=1), n_j-2 = -1, which is ill-defined at x=0. But j starts from 1, so n_j >= 2.\n            # a*x^0 = a.\n            if n_j - 2 == 0:\n                term1 = n_j * (n_j - 1)\n            elif n_j - 2 > 0:\n                term1 = n_j * (n_j - 1) * x**(n_j - 2)\n\n            term2 = (n_j + 1) * n_j * x**(n_j - 1)\n            return term1 - term2\n\n        # 2. Construct matrix A and vector y\n        A = np.zeros((N, K))\n        y = np.zeros(N)\n\n        for i in range(N):\n            xi = x_colloc[i]\n            y[i] = Q0 * np.sin(np.pi * xi)\n            for j_idx in range(K):\n                j = j_idx + 1 # Basis function index from 1 to K\n                A[i, j_idx] = -k * basis_b_dd(xi, j)\n\n        # 3. Construct inverse noise covariance diagonal\n        sigma_sq = sigma0**2 * (1 + beta * x_colloc**2)\n        sigma_sq_inv_diag = 1.0 / sigma_sq\n        \n        # 4. Compute posterior mean and covariance\n        # S_inv = A.T @ Sigma_inv @ A + lambda * I\n        # Efficient computation without forming diagonal matrix Sigma_inv\n        S_inv = (A.T * sigma_sq_inv_diag) @ A + lambda_ * np.identity(K)\n        S = np.linalg.inv(S_inv)\n        \n        # m = S @ A.T @ Sigma_inv @ y\n        m = S @ (A.T @ (sigma_sq_inv_diag * y))\n\n        # 5. Compute posterior predictive mean and variance at sensor locations\n        case_results = []\n        for s_ell in sensors:\n            # Construct basis vector b(s_ell)\n            b_s = np.zeros(K)\n            for j_idx in range(K):\n                j = j_idx + 1\n                b_s[j_idx] = basis_b(s_ell, j)\n            \n            # Predictive mean\n            mu_ell = b_s @ m\n            \n            # Predictive variance\n            nu_ell = b_s.T @ S @ b_s\n            \n            case_results.extend([mu_ell, nu_ell])\n        \n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation\n    # e.g., [1.23, 4.56] -> \"[1.23, 4.56]\"\n    # Then join these strings with commas.\n    str_results = [str(res).replace(\" \", \"\") for res in all_results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        }
    ]
}