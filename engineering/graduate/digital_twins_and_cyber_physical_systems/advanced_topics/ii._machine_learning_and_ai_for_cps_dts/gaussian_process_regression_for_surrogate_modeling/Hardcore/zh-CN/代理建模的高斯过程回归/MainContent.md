## 引言
在[数字孪生](@entry_id:171650)（Digital Twin, DT）和信息物理系统（Cyber-Physical System, CPS）的构建与运维中，我们常常依赖于高保真的物理仿真模型，例如[有限元分析](@entry_id:138109)或[计算流体动力学](@entry_id:142614)。然而，这些模型的高昂计算成本严重制约了它们在实时控制、大规模参数寻优和[不确定性分析](@entry_id:149482)中的应用。[高斯过程回归](@entry_id:276025)（Gaussian Process Regression, GPR）作为一种强大的非参数贝叶斯方法，为这一挑战提供了优雅的解决方案，它能够以极高的数据效率学习并构建复杂模型的快速代理。

本文旨在系统性地介绍[高斯过程回归](@entry_id:276025)及其在代理模型构建中的核心作用。我们将填补理论与实践之间的鸿沟，展示 GPR 如何从一个数学概念转变为解决工程与科学问题的实用工具。通过阅读本文，您将全面了解 GPR 的工作原理，掌握其在现实世界中的应用方法，并获得处理相关问题的实践指导。文章将分为三个核心部分：首先，在“原理与机制”中，我们将深入其数学基础；其次，在“应用与交叉学科联系”中，我们将探索其在 DT 和 CPS 等领域的广泛应用；最后，在“动手实践”部分，我们将通过具体问题引导您将理论付诸实践。

## 原理与机制

本章深入探讨高斯过程回归（Gaussian Process Regression, GPR）的核心原理与机制。我们将从其数学定义出发，逐步揭示高斯过程如何作为一种强大的工具，在函数空间中直接进行[贝叶斯推断](@entry_id:146958)。我们将重点阐述核函数（kernel）的关键作用、[后验预测分布](@entry_id:167931)的推导、不确定性的量化，以及模型选择中的内在权衡。本章旨在为读者构建一个坚实的理论基础，以便在数字孪生（Digital Twin, DT）和信息物理系统（Cyber-Physical System, CPS）的代理模型构建中，能够深刻理解并有效应用高斯过程。

### 定义[高斯过程](@entry_id:182192)：函数的分布

[高斯过程](@entry_id:182192)为在函数空间上定义概率分布提供了一个优雅而强大的框架。与传统[参数模型](@entry_id:170911)（如[线性回归](@entry_id:142318)）在有限维参数空间上定义分布不同，高斯过程直接对函数本身进行建模。

一个**[高斯过程](@entry_id:182192) (Gaussian Process, GP)** 是一个[随机变量](@entry_id:195330)的集合，其中任意有限个[随机变量](@entry_id:195330)的组合都服从[联合高斯](@entry_id:636452)分布。我们可以将一个函数 $f(x)$ 视为一个由无穷多个[随机变量](@entry_id:195330)组成的集合，其中每个[随机变量](@entry_id:195330) $f(x)$ 对应于函数在某个输入点 $x$ 的值。因此，将一个 GP 作为函数 $f$ 的先验，即 $f \sim \mathcal{GP}(m, k)$，意味着对于任意一组有限的输入点 $\mathcal{X}_{train} = \{x_1, x_2, \dots, x_n\}$，其对应的函数值向量 $\mathbf{f} = [f(x_1), f(x_2), \dots, f(x_n)]^T$ 都遵循一个多元高斯分布。

这个多元高斯分布完全由其一阶和二阶矩决定，即[均值向量](@entry_id:266544) $\boldsymbol{\mu}$ 和协方差矩阵 $\mathbf{K}$。在[高斯过程](@entry_id:182192)中，这两个矩是由**[均值函数](@entry_id:264860)** $m(x)$ 和**协方差函数**（或称**[核函数](@entry_id:145324)**）$k(x, x')$ 生成的：

- **[均值函数](@entry_id:264860)** $m(x)$ 定义了函数在每个点的[期望值](@entry_id:150961)：$\mu_i = \mathbb{E}[f(x_i)] = m(x_i)$。它代表了我们对函数在未见数据前“最可能”形态的先验信念。在实践中，为简化模型，通常假设一个零均值先验 $m(x) = 0$，将函数的整体趋势交给数据来决定。

- **协方差函数** $k(x, x')$ 定义了函数在任意两点 $x_i$ 和 $x_j$ 处的值之间的协方差：$K_{ij} = \text{Cov}(f(x_i), f(x_j)) = k(x_i, x_j)$。这个函数是 GP 的核心，因为它编码了函数的重要属性，如平滑度、长度尺度和幅度等。

因此，一个高斯过程完全由其[均值函数](@entry_id:264860)和[协方差函数](@entry_id:265031)唯一确定。一旦指定了 $m(x)$ 和 $k(x, x')$，任意有限维[边际分布](@entry_id:264862) $\mathbf{f} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{K})$ 就被唯一确定了。Kolmogorov 扩展定理保证了存在一个满足这些[一致性条件](@entry_id:637057)的[随机过程](@entry_id:268487)。 

### 核函数：编码先验信念

[核函数](@entry_id:145324) $k(x, x')$ 捕获了我们对未知函数行为的先验假设。它通过定义不同输入点对应的函数值之间的相关性，来塑造[函数空间](@entry_id:143478)的性质。

#### [半正定性](@entry_id:147720)约束

并非任何一个双变量函数都可以作为合法的[协方差函数](@entry_id:265031)。一个关键的数学要求是，由[核函数](@entry_id:145324)为任意有限输入集 $\{x_i\}_{i=1}^n$ 生成的任何 Gram 矩阵 $\mathbf{K}$（其中 $K_{ij} = k(x_i, x_j)$）都必须是**对称半正定的 (Positive Semidefinite, PSD)**。

这个要求源于一个基本[概率公理](@entry_id:262004)：任意[随机变量的方差](@entry_id:900888)必须是非负的。考虑函数值向量 $\mathbf{f} = [f(x_1), \dots, f(x_n)]^T$ 的任意[线性组合](@entry_id:154743) $Y = \mathbf{v}^T \mathbf{f} = \sum_{i=1}^n v_i f(x_i)$，其中 $\mathbf{v} \in \mathbb{R}^n$ 是任意的实系数向量。$Y$ 的方差为：
$$
\text{Var}(Y) = \text{Var}(\mathbf{v}^T \mathbf{f}) = \mathbf{v}^T \text{Cov}(\mathbf{f}) \mathbf{v} = \mathbf{v}^T \mathbf{K} \mathbf{v}
$$
由于方差必须非负，我们必须有 $\mathbf{v}^T \mathbf{K} \mathbf{v} \ge 0$ 对所有 $\mathbf{v} \in \mathbb{R}^n$ 成立。这正是矩阵 $\mathbf{K}$ 为半正定的定义。因此，一个函数 $k(x, x')$ 是一个合法的 PSD 核，当且仅当它对于任何有限输入集生成的 Gram 矩阵都是对称且半正定的。这个条件确保了GP先验在概率上是一致且有效的。

#### 从[参数模型](@entry_id:170911)到无限维度

高斯过程提供了一个所谓的**[函数空间](@entry_id:143478)视角 (function-space view)**。为了更好地理解其强大的表达能力，我们可以将其与传统的**权重空间视角 (weight-space view)** 进行对比，例如[参数化](@entry_id:265163)的线性模型。

一个参数线性模型通常假设未知函数 $f(x)$ 可以表示为一组有限基函数 $\boldsymbol{\phi}(x) = [\phi_1(x), \dots, \phi_p(x)]^T$ 的线性组合：$f(x) = \boldsymbol{\phi}(x)^T \mathbf{w}$，其中 $\mathbf{w} \in \mathbb{R}^p$ 是一个有限维的权重向量。模型的灵活性完全受限于预先选择的 $p$ 个基函数。

而[高斯过程](@entry_id:182192)则可以被看作是这种模型的无限维度推广。根据 Mercer 定理，任何满足特定条件的 PSD 核函数 $k(x, x')$ 都可以被分解为一个（可能无限的）[特征函数](@entry_id:186820) $\psi_j(x)$ 的加权和：
$$
k(x, x') = \sum_{j=1}^{\infty} \lambda_j \psi_j(x) \psi_j(x')
$$
其中 $\lambda_j \ge 0$ 是特征值。相应地，从该 GP 中抽样得到的函数可以表示为：
$$
f(x) = \sum_{j=1}^{\infty} w_j \psi_j(x), \quad \text{其中 } w_j \sim \mathcal{N}(0, \lambda_j)
$$
这个视角揭示了 GP 的非参数特性：它等价于在一个无限维的特征空间中进行[贝叶斯线性回归](@entry_id:634286)。[核函数](@entry_id:145324) $k$ 隐式地定义了这个无限维空间以及权重上的先验，使得我们无需显式地操作无限维向量，就能通过“[核技巧](@entry_id:144768)”完成所有计算。这种能力使得 GP 能够从数据中学习函数的复杂性，而无需预先固定模型的维度。

#### 核函数的动物园：编码平滑度与相关性

[核函数](@entry_id:145324)的选择至关重要，因为它直接将我们对函数行为的先验知识注入模型。在为CPS的[数字孪生](@entry_id:171650)构建代理模型时，选择与底层物理过程相匹配的核函数是成功的关键。

**平滑度与[可微性](@entry_id:140863)**：一个核心的先验信念是关于函数有多“平滑”。核函数的性质直接决定了 GP 样本路径的[可微性](@entry_id:140863)。这可以通过核函数的谱密度（即傅里叶变换）来理解。根据 Bochner 定理，任何平稳[核函数](@entry_id:145324) $k(\tau) = k(x - x')$（即协方差只依赖于输入点的相对位移）都可以表示为其谱密度 $S(\omega)$ 的[傅里叶逆变换](@entry_id:178300)。函数样本的 $m$ 阶均方[可微性](@entry_id:140863)等价于谱密度的 $2m$ 阶矩是有限的。

- **[平方指数核](@entry_id:191141) (Squared Exponential, SE)**：$k_{SE}(\tau) = \sigma_f^2 \exp(-\frac{\tau^2}{2\ell^2})$。
  这是一个非常流行的[核函数](@entry_id:145324)，其谱密度是高斯函数。由于[高斯函数](@entry_id:261394)的尾部衰减速度快于任何多项式，其所有阶矩都是有限的。这意味着从 SE 核的 GP 中抽取的函数样本是无限次可微的（$C^\infty$）。这种极强的平滑性假设在某些情况下可能不切实际。例如，在模拟具有接触、摩擦或[湍流](@entry_id:151300)的物理系统时，[响应函数](@entry_id:142629)可能只具有有限的平滑度，甚至存在[尖点](@entry_id:636792)或斜率突变。在这种情况下，SE 核的“[过度平滑](@entry_id:634349)”先验可能会导致模型无法捕捉这些关键的非平滑特征。

- **Matérn 核 (Matérn Kernel)**：
  为了提供更灵活的平滑度控制，Matérn 核族引入了一个平滑度参数 $\nu > 0$。其谱密度具有多项式衰减的“[肥尾](@entry_id:140093)”，具体为 $S(\omega) \propto (c + \omega^2)^{-(\nu + d/2)}$。这导致 GP 的样本路径恰好是 $\lfloor \nu \rfloor$ 次均方可微的。常用的特例包括：
  - $\nu = 1/2$：得到指数核，样本路径连续但处处不可微（类似于布朗运动）。
  - $\nu = 3/2$：样本路径一次可微，适合模拟许多由二阶动力学方程描述且受有界变化输入驱动的系统。
  - $\nu = 5/2$：样本路径二次可微。
  - $\nu \to \infty$：恢复为 SE 核。
  在为物理[系统建模](@entry_id:197208)时，如果先验知识表明系统响应具有有限的光滑度，选择一个具有合适 $\nu$ 值的 Matérn 核通常比 SE 核更为现实和有效。 

**输入相关性与[自动相关性确定 (ARD)](@entry_id:746593)**：在处理多维输入时，不同输入维度对输出的影响可能大相径庭。一个各向同性 (isotropic) 的[核函数](@entry_id:145324)（如标准 SE 核），对所有维度使用单一的长度尺度 $\ell$，假设函数在所有方向上以相同的速率变化。这在许多应用中是不合理的。

**[自动相关性确定](@entry_id:746592) (Automatic Relevance Determination, ARD)** 通过为每个输入维度 $j$ 分配一个独立的长度尺度 $\ell_j$ 来解决这个问题。例如，SE-ARD 核的形式为：
$$
k_{ARD}(x, x') = \sigma_f^2 \exp\left(-\frac{1}{2} \sum_{j=1}^d \frac{(x_j - x'_j)^2}{\ell_j^2}\right)
$$
长度尺度 $\ell_j$ 的作用是：
- **小 $\ell_j$**：意味着函数对第 $j$ 个维度的变化非常敏感。即使 $x_j$ 和 $x'_j$ 只有微小差异，协方差也会迅速下降。这表明维度 $j$ 是“相关的”。
- **大 $\ell_j$**：意味着函数在第 $j$ 个维度上变化缓慢，协方差对该维度的差异不敏感。当 $\ell_j \to \infty$ 时，该维度对协方差的贡献趋近于1（即不产生影响），相当于将该输入维度从模型中“移除”。
通过在模型训练过程中从数据中学习这些长度尺度，ARD 能够自动发现哪些输入维度对输出是重要的，哪些是无关的。这对于高维问题中的[特征选择](@entry_id:177971)和可解释性至关重要，例如在智能建筑的热模型中，ARD可以帮助确定众多传感器和控制变量中哪些是预测热负荷的关键因素。 

### 推断：从先验到后验

高斯过程回归的核心在于利用[贝叶斯定理](@entry_id:897366)，将先验知识（由 GP 先验定义）与观测数据相结合，得到关于未知函数的后验分布。

假设我们有一个训练数据集 $D = \{(\mathbf{X}, \mathbf{y})\} = \{(x_i, y_i)\}_{i=1}^n$，其中观测值 $y_i$ 是未知函数值 $f(x_i)$ 加上[独立同分布](@entry_id:169067)的[高斯噪声](@entry_id:260752) $\epsilon_i \sim \mathcal{N}(0, \sigma_n^2)$：
$$
y_i = f(x_i) + \epsilon_i
$$
我们的目标是预测在新的测试点 $x_*$ 处的函数值 $f_* = f(x_*)$ 或新的观测值 $y_* = f(x_*) + \epsilon_*$ 的分布。

关键的洞见在于，由于先验是高斯过程，而[似然函数](@entry_id:921601)是高斯分布，因此所有相关变量的[联合分布](@entry_id:263960)也是高斯的。具体来说，训练数据的潜在函数值 $\mathbf{f} = [f(x_1), \dots, f(x_n)]^T$ 和测试点的潜在函数值 $f_*$ 的联合先验分布是：
$$
\begin{pmatrix} \mathbf{f} \\ f_* \end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mathbf{m} \\ m_* \end{pmatrix}, \begin{pmatrix} \mathbf{K} & \mathbf{k}_* \\ \mathbf{k}_*^T & k_{**} \end{pmatrix} \right)
$$
其中 $\mathbf{m}$ 和 $m_*$ 是[均值向量](@entry_id:266544)/标量，$\mathbf{K} = k(\mathbf{X}, \mathbf{X})$, $\mathbf{k}_* = k(\mathbf{X}, x_*)$, $k_{**} = k(x_*, x_*)$。

由于观测值 $\mathbf{y} = \mathbf{f} + \boldsymbol{\epsilon}$，其中 $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma_n^2 \mathbf{I})$，那么观测值 $\mathbf{y}$ 和测试值 $f_*$ 的[联合分布](@entry_id:263960)也是高斯分布：
$$
\begin{pmatrix} \mathbf{y} \\ f_* \end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mathbf{m} \\ m_* \end{pmatrix}, \begin{pmatrix} \mathbf{K} + \sigma_n^2 \mathbf{I} & \mathbf{k}_* \\ \mathbf{k}_*^T & k_{**} \end{pmatrix} \right)
$$
**[后验预测分布](@entry_id:167931)** $p(f_* | \mathbf{X}, \mathbf{y}, x_*)$ 是通过对这个[联合高斯](@entry_id:636452)分布进行条件化得到的。多元高斯分布的一个基本性质是，其[条件分布](@entry_id:138367)和[边际分布](@entry_id:264862)都是高斯分布。因此，[后验预测分布](@entry_id:167931)必然是高斯分布。这种“[高斯先验](@entry_id:749752) + 高斯似然 $\implies$ 高斯后验”的特性被称为**共轭性**。

通过应用多元高斯分布的条件化公式，我们可以得到 $f_*$ 的[后验分布](@entry_id:145605) $p(f_* | D) = \mathcal{N}(\mu_*, \sigma_*^2)$，其均值和方差为：
$$
\mu_*(x_*) = m(x_*) + \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{m})
$$
$$
\sigma_*^2(x_*) = k_{**} - \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{k}_*
$$
预测新观测值 $y_*$ 的分布同样是高斯分布，其均值与 $f_*$ 相同，但方差需要加上观测噪声的方差：$\text{Var}(y_*) = \sigma_*^2(x_*) + \sigma_n^2$。

### [量化不确定性](@entry_id:272064)：认知不确定性与[偶然不确定性](@entry_id:634772)

高斯过程回归最吸引人的特性之一是它能够提供有意义的、量化的预测不确定性。这种不确定性可以分解为两种不同的类型：

- **[偶然不确定性](@entry_id:634772) (Aleatoric Uncertainty)**：也称为统计不确定性或数据不确定性，它源于数据生成过程中固有的、不可避免的随机性。在我们的模型中，这由观测噪声 $\epsilon$ 的方差 $\sigma_n^2$ 来表示。即使我们拥有无限多的关于函数 $f$ 的数据，我们对新观测 $y_*$ 的预测仍然会因为随机噪声而存在不确定性。这种不确定性是不可约减的。

- **认知不确定性 (Epistemic Uncertainty)**：也称为[模型不确定性](@entry_id:265539)或知识不确定性，它源于我们对真实函数 $f$ 的了解不足。这在 GP 模型中由后验方差 $\sigma_*^2(x_*)$ 来表示。这种不确定性是可约减的——通过收集更多的训练数据，我们可以减少对 $f$ 的不确定性。

总的预测方差是这两者的和：
$$
\text{Var}(y_* | D) = \underbrace{\sigma_*^2(x_*)}_{\text{认知不确定性}} + \underbrace{\sigma_n^2}_{\text{偶然不确定性}}
$$
认知不确定性的行为是 GP 代理模型的关键优势。观察后验方差 $\sigma_*^2(x_*)$ 的表达式可以发现，它依赖于测试点 $x_*$ 与训练数据点 $\mathbf{X}$ 的相对位置，但**不**依赖于观测值 $\mathbf{y}$。其典型行为是：
- 在训练数据点附近，$\sigma_*^2(x_*)$ 会很小，因为模型在这些区域“知道”函数的样子。
- 当 $x_*$ 远离所有训练数据点时，$\mathbf{k}_*$ 向量的元素会趋于零（对于大多数平稳核），导致后验方差 $\sigma_*^2(x_*)$ 趋近于先验方差 $k(x_*, x_*)$。这意味着在没有数据的区域，模型会“坦诚地”承认其无知，不确定性会回归到先验水平。

这种量化认知不确定性的能力对于数字孪生中的决策支持、主动学习（选择在何处进行下一次昂贵的模拟或实验）以及安全关键型 CPS 的[可靠性分析](@entry_id:192790)至关重要。 此外，当使用 ARD 核时，认知不确定性的降低会表现出方[向性](@entry_id:144651)：在数据点附近，不确定性会沿着长度尺度 $\ell_j$ 较小的维度（即“相关”维度）更快地收缩。

### [模型选择](@entry_id:155601)：[证据最大化](@entry_id:749132)与[奥卡姆剃刀](@entry_id:142853)

到目前为止，我们都假设[核函数](@entry_id:145324)的超参数 $\boldsymbol{\theta}$（例如长度尺度 $\ell$、信号方差 $\sigma_f^2$）和噪声方差 $\sigma_n^2$ 是已知的。在实践中，这些参数需要从数据中学习。

在贝叶斯框架下，一个原则性的方法是最大化**[边际似然](@entry_id:636856) (Marginal Likelihood)**，也称为**证据 (Evidence)**。[边际似然](@entry_id:636856)是通过对所有可能的函数 $f$ 进行积分（[边缘化](@entry_id:264637)）得到的观测数据 $\mathbf{y}$ 的概率：
$$
p(\mathbf{y} | \mathbf{X}, \boldsymbol{\theta}) = \int p(\mathbf{y} | \mathbf{f}, \boldsymbol{\theta}) p(\mathbf{f} | \mathbf{X}, \boldsymbol{\theta}) d\mathbf{f}
$$
对于我们的高斯模型，这个积分可以解析地计算出来，因为 $p(\mathbf{y} | \mathbf{X}, \boldsymbol{\theta}) = \mathcal{N}(\mathbf{y} | \mathbf{m}, \mathbf{K} + \sigma_n^2 \mathbf{I})$。其对数形式（忽略常数项）为：
$$
\log p(\mathbf{y} | \mathbf{X}, \boldsymbol{\theta}) = \underbrace{-\frac{1}{2} \mathbf{y}^T (\mathbf{K}_{\boldsymbol{\theta}} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{y}}_{\text{数据拟合项}} \underbrace{-\frac{1}{2} \log |\mathbf{K}_{\boldsymbol{\theta}} + \sigma_n^2 \mathbf{I}|}_{\text{模型复杂度惩罚项}}
$$
通过最大化这个对数边际似然（LML），我们可以找到最优的超参数 $\boldsymbol{\theta}$。这个过程被称为**第二类最大似然 (Type-II Maximum Likelihood)**。

LML 的表达式巧妙地体现了**偏见-方差权衡 (bias-variance tradeoff)**，并自动实现了**奥卡姆剃刀 (Occam's Razor)** 原则：
- **数据拟合项**：这一项奖励那些能够很好地解释观测数据 $\mathbf{y}$ 的模型。一个非常灵活（高方差）的模型，例如长度尺度 $\ell$ 非常小的模型，可以产生非常“崎岖”的函数，从而能够精确地穿过所有数据点，使得这一项的值变得更大（更接近零）。
- **[模型复杂度惩罚](@entry_id:752069)项**：$\log |\mathbf{K}_{\boldsymbol{\theta}} + \sigma_n^2 \mathbf{I}|$ 这一项惩罚过于复杂的模型。$|\mathbf{K}|$ 可以被看作是先验所允许的函数“体积”的一种度量。一个非常灵活的模型（例如小 $\ell$ 或大 $\sigma_f^2$）能够生成更多样化的函数，其协方差矩阵的特征值会更大，从而导致行列式也更大，惩罚也就更重。

因此，最大化 LML 的过程是在[数据拟合](@entry_id:149007)和[模型复杂度](@entry_id:145563)之间寻找一个最佳平衡点。它会自动惩罚那些“过于强大”以至于能解释任何随机噪声的模型，转而偏好能够以最简洁方式解释数据背后结构的模型。例如，在为一个昂贵的有限元热应力仿真建立代理模型时，[证据最大化](@entry_id:749132)框架会自动选择能够捕捉真实物理响应平滑度和相关性的核参数，而不是选择一个试图拟合仿真中微小数值噪声的过拟合模型。

### 实践考量：计算复杂度

尽管高斯过程回归理论优美，但在实践中，其标准（精确）实现面临着计算上的挑战，尤其是在处理大量来自[传感器网络](@entry_id:272524)的数据时。其计算复杂度主要源于对 $n \times n$ [协方差矩阵](@entry_id:139155) $\mathbf{K}_{\mathbf{y}} = \mathbf{K}_{\boldsymbol{\theta}} + \sigma_n^2 \mathbf{I}$ 的操作。

- **内存复杂度**：需要存储完整的 $n \times n$ [协方差矩阵](@entry_id:139155)，这需要 $O(n^2)$ 的内存。

- **[时间复杂度](@entry_id:145062)**：
  - **训练（[超参数优化](@entry_id:168477)）**：在每次LML评估中，最昂贵的操作是计算 $(\mathbf{K}_{\mathbf{y}})^{-1} \mathbf{y}$ 和 $\log|\mathbf{K}_{\mathbf{y}}|$。直接求逆在数值上不稳定且效率低下。标准的数值稳定方法是首先对 $\mathbf{K}_{\mathbf{y}}$ 进行**Cholesky 分解**，得到 $\mathbf{K}_{\mathbf{y}} = \mathbf{L}\mathbf{L}^T$，其中 $\mathbf{L}$ 是一个下[三角矩阵](@entry_id:636278)。对于一个稠密的 $n \times n$ 矩阵，Cholesky 分解的[时间复杂度](@entry_id:145062)为 $O(n^3)$。一旦分解完成，[求解线性系统](@entry_id:146035)和计算行列式都可以在 $O(n^2)$ 时间内完成。因此，单次 LML 评估的瓶颈是 Cholesky 分解，导致训练过程的整体[时间复杂度](@entry_id:145062)为 $O(k \cdot n^3)$，其中 $k$ 是优化器所需的迭代次数。
  - **预测**：计算[后验均值](@entry_id:173826)需要 $O(n)$ 时间（如果训练阶段的结果已被缓存），但计算后验方差需要一次 $O(n^2)$ 的三角系统求解。

综上所述，标准 GPR 的训练[时间复杂度](@entry_id:145062)为 $O(n^3)$，内存复杂度为 $O(n^2)$。这种三次方的计算扩展性使得精确 GPR 难以应用于超过数千个数据点的场景。这也催生了大量的研究工作，致力于开发各种稀疏方法和[近似推断](@entry_id:746496)技术，以将高斯过程扩展到更大规模的数据集上。