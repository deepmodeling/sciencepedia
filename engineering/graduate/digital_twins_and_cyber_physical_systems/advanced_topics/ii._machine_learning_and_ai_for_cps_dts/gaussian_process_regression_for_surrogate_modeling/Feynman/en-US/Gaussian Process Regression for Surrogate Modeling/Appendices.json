{
    "hands_on_practices": [
        {
            "introduction": "At the heart of Gaussian Process Regression lies the ability to condition a prior belief about a function on observed data to obtain a posterior distribution. This posterior then allows us to make predictions, complete with uncertainty estimates, at new, unobserved input locations. This first exercise will guide you through the fundamental mechanics of deriving and implementing the GPR predictive equations from first principles for a given kernel, a foundational skill for any practitioner . Mastering the stable numerical implementation of these equations is essential before moving to more advanced applications.",
            "id": "4224416",
            "problem": "Consider a surrogate modeling task for a Digital Twin of a Cyber-Physical System (CPS) in which the latent system response is modeled as a Gaussian Process (GP). A Gaussian Process (GP) is defined as a collection of random variables such that any finite subset follows a multivariate Gaussian distribution. Let the latent function be denoted by $f(\\cdot)$ and observed outputs be $y$, where $y$ are noisy observations of $f(\\cdot)$ with additive independent Gaussian noise.\n\nAssume a zero-mean prior $m(x)=0$ and a Matérn covariance function with known hyperparameters. The observation model is $y_i = f(x_i) + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ independently for $i \\in \\{1,\\dots,n\\}$. The Matérn kernel $k_\\nu(\\cdot,\\cdot)$ is defined via the Euclidean distance $r = \\lVert x - x' \\rVert_2$ by\n$$\nk_\\nu(r) \\;=\\; \\sigma_f^2 \\cdot \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)^\\nu K_\\nu \\!\\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right),\n$$\nwhere $\\sigma_f^2$ is the signal variance, $\\ell$ is the length-scale, $\\nu$ is the smoothness parameter, $\\Gamma(\\cdot)$ is the Gamma function, and $K_\\nu(\\cdot)$ is the modified Bessel function of the second kind. It holds that $k_\\nu(0) = \\sigma_f^2$.\n\nStarting from the definition of the Gaussian Process prior and the linear-Gaussian observation model, derive the predictive distribution of the latent function $f(x_*)$ at a new input $x_*$. Implement a program that computes the predictive mean and variance at $x_*$ using numerically stable linear algebra.\n\nYour implementation must:\n- Construct the Gram matrix $K$ using the given Matérn kernel and add the observation noise variance $\\sigma_n^2$ to the diagonal.\n- Use a numerically stable solver (for example, a Cholesky-based approach) to avoid explicit matrix inversion.\n- Compute the predictive mean and variance of the latent function $f(x_*)$.\n\nThere are no physical units required; all quantities are dimensionless real numbers. Angles do not appear. Percentages are not involved.\n\nTest Suite:\nUse the following test cases. Each case specifies $X \\in \\mathbb{R}^{n \\times d}$, $y \\in \\mathbb{R}^n$, hyperparameters $(\\sigma_f,\\ell,\\nu,\\sigma_n)$, and a query $x_* \\in \\mathbb{R}^d$. All numerical entries are real numbers.\n\n- Case $1$ (general two-dimensional, moderate length-scale):\n  $$\n  X^{(1)} = \\begin{bmatrix}\n  0.0 & 0.0\\\\\n  0.5 & -0.2\\\\\n  1.0 & 0.3\\\\\n  1.5 & -0.5\\\\\n  2.0 & 0.0\n  \\end{bmatrix},\\quad\n  y^{(1)} = \\begin{bmatrix}\n  0.0\\\\\n  0.329418342\\\\\n  0.807356092\\\\\n  0.782039086\\\\\n  0.999573603\n  \\end{bmatrix},\n  $$\n  $$\n  (\\sigma_f^{(1)},\\ell^{(1)},\\nu^{(1)},\\sigma_n^{(1)}) = (1.2,\\,0.7,\\,1.5,\\,0.1),\\quad\n  x_*^{(1)} = \\begin{bmatrix} 1.2\\\\ 0.1 \\end{bmatrix}.\n  $$\n\n- Case $2$ (edge case: query equals a training input):\n  $$\n  X^{(2)} = X^{(1)},\\quad y^{(2)} = y^{(1)},\\quad\n  (\\sigma_f^{(2)},\\ell^{(2)},\\nu^{(2)},\\sigma_n^{(2)}) = (1.2,\\,0.7,\\,1.5,\\,0.1),\\quad\n  x_*^{(2)} = \\begin{bmatrix} 1.0\\\\ 0.3 \\end{bmatrix}.\n  $$\n\n- Case $3$ (boundary: very small length-scale, strong locality):\n  $$\n  X^{(3)} = X^{(1)},\\quad y^{(3)} = y^{(1)},\\quad\n  (\\sigma_f^{(3)},\\ell^{(3)},\\nu^{(3)},\\sigma_n^{(3)}) = (1.2,\\,0.05,\\,1.5,\\,0.05),\\quad\n  x_*^{(3)} = \\begin{bmatrix} 1.1\\\\ 0.25 \\end{bmatrix}.\n  $$\n\n- Case $4$ (boundary: very large length-scale, near-constant behavior):\n  $$\n  X^{(4)} = X^{(1)},\\quad y^{(4)} = y^{(1)},\\quad\n  (\\sigma_f^{(4)},\\ell^{(4)},\\nu^{(4)},\\sigma_n^{(4)}) = (1.2,\\,50.0,\\,1.5,\\,0.1),\\quad\n  x_*^{(4)} = \\begin{bmatrix} 10.0\\\\ -10.0 \\end{bmatrix}.\n  $$\n\n- Case $5$ (one-dimensional input, different smoothness):\n  $$\n  X^{(5)} = \\begin{bmatrix}\n  0.0\\\\ 1.0\\\\ 2.0\\\\ 3.0\n  \\end{bmatrix},\\quad\n  y^{(5)} = \\begin{bmatrix}\n  1.0\\\\ 0.540302306\\\\ -0.416146837\\\\ -0.989992497\n  \\end{bmatrix},\n  $$\n  $$\n  (\\sigma_f^{(5)},\\ell^{(5)},\\nu^{(5)},\\sigma_n^{(5)}) = (1.0,\\,1.0,\\,2.5,\\,0.01),\\quad\n  x_*^{(5)} = \\begin{bmatrix} 1.5 \\end{bmatrix}.\n  $$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $[\\mu, s^2]$ with the predictive mean $\\mu$ and variance $s^2$ for the corresponding $x_*$. Specifically, the output must be\n$$\n\\big[ [\\mu^{(1)}, s^{2,(1)}], [\\mu^{(2)}, s^{2,(2)}], [\\mu^{(3)}, s^{2,(3)}], [\\mu^{(4)}, s^{2,(4)}], [\\mu^{(5)}, s^{2,(5)}] \\big].\n$$",
            "solution": "The user has provided a well-defined problem in the domain of statistical machine learning. The task is to derive and implement the predictive equations for Gaussian Process (GP) regression.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model**: A Gaussian Process is used to model a latent function $f(\\cdot)$.\n- **Prior**: The GP has a zero-mean prior, $m(x)=0$.\n- **Covariance**: The covariance function is the Matérn kernel $k_\\nu(\\cdot,\\cdot)$ with known hyperparameters $\\sigma_f^2$ (signal variance), $\\ell$ (length-scale), and $\\nu$ (smoothness). The kernel is defined as:\n$$\nk_\\nu(r) \\;=\\; \\sigma_f^2 \\cdot \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)^\\nu K_\\nu \\!\\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)\n$$\nwhere $r = \\lVert x - x' \\rVert_2$, $\\Gamma(\\cdot)$ is the Gamma function, and $K_\\nu(\\cdot)$ is the modified Bessel function of the second kind. A property $k_\\nu(0) = \\sigma_f^2$ is given.\n- **Observation Model**: Observations $y_i$ are noisy versions of the latent function, $y_i = f(x_i) + \\epsilon_i$, with noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ being independent and identically distributed.\n- **Objective**: Derive the predictive distribution for the latent function value $f(x_*)$ at a new input $x_*$.\n- **Implementation Constraints**:\n    1.  Construct the Gram matrix $K$.\n    2.  Add observation noise variance $\\sigma_n^2$ to the diagonal.\n    3.  Use a numerically stable method like Cholesky decomposition to avoid explicit matrix inversion.\n    4.  Compute the predictive mean and variance of $f(x_*)$.\n- **Data**: Five test cases are provided, each with training inputs $X$, training outputs $y$, hyperparameters $(\\sigma_f, \\ell, \\nu, \\sigma_n)$, and a test input $x_*$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is a standard application of Bayesian inference for Gaussian Processes, a cornerstone of modern machine learning and statistics. All definitions and formulas are correct and standard in the field.\n- **Well-Posed**: The problem is well-posed. Given the model, data, and hyperparameters, the posterior predictive distribution is uniquely defined. The requirement for a numerically stable solver acknowledges and addresses potential ill-conditioning of the covariance matrix, ensuring a stable solution can be computed.\n- **Objective**: The problem is stated in precise mathematical language, is free of ambiguity, and provides all necessary numerical data for a unique solution.\n- **Completeness and Consistency**: The problem is self-contained. It provides the full model specification (prior, likelihood, kernel function) and all data required for computation. There are no contradictions.\n- **Realism**: The task is a a canonical example of surrogate modeling, which is a critical function in Digital Twins and Cyber-Physical Systems. The numerical values are realistic for such a problem.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid** as it is scientifically sound, well-posed, and complete. I will now proceed with the derivation and implementation of the solution.\n\n### Derivation of the Predictive Distribution\n\nThe core principle of Gaussian Process regression is that any finite set of function values is jointly Gaussian. We are interested in the conditional distribution of the latent function value $f_* = f(x_*)$ at a test point $x_*$, given the training data $D = \\{X, \\mathbf{y}\\}$, where $X = \\{x_1, \\dots, x_n\\}$ is the set of training inputs and $\\mathbf{y} = [y_1, \\dots, y_n]^T$ is the vector of corresponding noisy observations.\n\n1.  **Joint Prior Distribution**: According to the GP prior, the vector of latent function values at the training points, $\\mathbf{f} = [f(x_1), \\dots, f(x_n)]^T$, and the value at the test point, $f_*$, are jointly Gaussian. Given the zero-mean prior, this distribution is:\n    $$\n    \\begin{bmatrix} \\mathbf{f} \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{0} \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K(X, X) & K(X, x_*) \\\\ K(x_*, X) & K(x_*, x_*) \\end{bmatrix} \\right)\n    $$\n    Let us denote the partitioned covariance matrix with more concise notation: $K = K(X, X)$ (the $n \\times n$ Gram matrix), $k_* = K(X, x_*)$ (the $n \\times 1$ vector of covariances between training points and the test point), and $k_{**} = K(x_*, x_*)$ (the prior variance at the test point). The covariance matrix is thus $\\begin{bmatrix} K & k_* \\\\ k_*^T & k_{**} \\end{bmatrix}$.\n\n2.  **Joint Distribution of Observations and Latent Test Value**: The observation model is $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where the noise vector $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I_n)$. Since $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$ are independent, their sum $\\mathbf{y}$ is also a Gaussian random variable. The joint distribution of the observed data $\\mathbf{y}$ and the latent value $f_*$ is:\n    $$\n    \\begin{bmatrix} \\mathbf{y} \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{0} \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K + \\sigma_n^2 I_n & k_* \\\\ k_*^T & k_{**} \\end{bmatrix} \\right)\n    $$\n    Here, the covariance between $\\mathbf{y}$ and $f_*$ is $\\text{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}, f_*) = \\text{Cov}(\\mathbf{f}, f_*) + \\text{Cov}(\\boldsymbol{\\epsilon}, f_*) = k_* + \\mathbf{0} = k_*$. The variance of $\\mathbf{y}$ is $\\text{Var}(\\mathbf{f} + \\boldsymbol{\\epsilon}) = \\text{Var}(\\mathbf{f}) + \\text{Var}(\\boldsymbol{\\epsilon}) = K + \\sigma_n^2 I_n$.\n\n3.  **Conditional (Posterior) Distribution**: We seek the distribution $p(f_* | \\mathbf{y}, X, x_*)$. Using the standard formula for conditional distributions of multivariate Gaussians, for a joint distribution $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$, the conditional distribution $p(\\mathbf{b}|\\mathbf{a})$ is Gaussian with mean $\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a)$ and covariance $\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}$.\n\n    By substituting our terms ($\\mathbf{a} \\to \\mathbf{y}$, $\\mathbf{b} \\to f_*$, zero means, and the derived covariance structure), we obtain the predictive distribution for $f_*$:\n    -   **Predictive Mean** $\\mu_*(x_*)$:\n        $$\n        \\mu_*(x_*) = \\mathbf{0} + k_*^T (K + \\sigma_n^2 I_n)^{-1} (\\mathbf{y} - \\mathbf{0}) = k_*^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}\n        $$\n    -   **Predictive Variance** $s^2_*(x_*)$:\n        $$\n        s^2_*(x_*) = k_{**} - k_*^T (K + \\sigma_n^2 I_n)^{-1} k_*\n        $$\n\n### Numerically Stable Implementation\n\nDirect computation of the matrix inverse $(K + \\sigma_n^2 I_n)^{-1}$ is numerically unstable and inefficient. A superior method is to use the Cholesky decomposition. Let $K_{yy} = K + \\sigma_n^2 I_n$. Since $K$ is symmetric and positive semi-definite, and $\\sigma_n^2 > 0$, $K_{yy}$ is symmetric and positive definite. Thus, it has a unique Cholesky decomposition $K_{yy} = LL^T$, where $L$ is a lower-triangular matrix.\n\nThe predictive mean and variance can then be computed as follows:\n\n1.  To compute the mean $\\mu_*(x_*) = k_*^T \\boldsymbol{\\alpha}$ where $\\boldsymbol{\\alpha} = K_{yy}^{-1}\\mathbf{y}$, we solve the linear system $K_{yy}\\boldsymbol{\\alpha} = \\mathbf{y}$. Using the decomposition, this becomes $LL^T\\boldsymbol{\\alpha} = \\mathbf{y}$. We solve this in two steps using substitution:\n    a. Solve $L\\mathbf{v} = \\mathbf{y}$ for $\\mathbf{v}$ (forward substitution).\n    b. Solve $L^T\\boldsymbol{\\alpha} = \\mathbf{v}$ for $\\boldsymbol{\\alpha}$ (backward substitution).\n\n2.  To compute the variance $s^2_*(x_*) = k_{**} - k_*^T K_{yy}^{-1} k_*$, we evaluate the quadratic form. Let $\\mathbf{z} = L^{-1}k_*$. We can find $\\mathbf{z}$ by solving $L\\mathbf{z} = k_*$ using forward substitution. Then the quadratic form can be written as:\n    $$\n    k_*^T K_{yy}^{-1} k_* = k_*^T (LL^T)^{-1} k_* = k_*^T (L^T)^{-1}L^{-1} k_* = (L^{-1}k_*)^T (L^{-1}k_*) = \\mathbf{z}^T\\mathbf{z} = \\lVert \\mathbf{z} \\rVert_2^2\n    $$\n    Therefore, the predictive variance is $s^2_*(x_*) = k_{**} - \\mathbf{z}^T\\mathbf{z}$. This approach avoids a second triangular solve and is numerically robust.\n\n### Matérn Kernel Computation\n\nThe Matérn kernel requires evaluation of the Gamma function $\\Gamma(\\nu)$ and the modified Bessel function of the second kind $K_\\nu(z)$. These are available in `scipy.special`. A crucial detail is the behavior at $r=0$. At this point, the argument to the Bessel function is zero, and $K_\\nu(z)$ diverges as $z \\to 0$. However, the product $z^\\nu K_\\nu(z)$ has a finite limit of $2^{\\nu-1}\\Gamma(\\nu)$. Substituting this into the kernel formula confirms the given property $k_\\nu(0) = \\sigma_f^2$. The implementation must handle the case $r=0$ separately to avoid numerical errors like $0 \\times \\infty$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma, kv\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves for the predictive mean and variance for a series of Gaussian Process\n    regression test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 0.7, 1.5, 0.1),\n            \"x_star\": np.array([1.2, 0.1])\n        },\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 0.7, 1.5, 0.1),\n            \"x_star\": np.array([1.0, 0.3])\n        },\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 0.05, 1.5, 0.05),\n            \"x_star\": np.array([1.1, 0.25])\n        },\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 50.0, 1.5, 0.1),\n            \"x_star\": np.array([10.0, -10.0])\n        },\n        {\n            \"X\": np.array([\n                [0.0], [1.0], [2.0], [3.0]\n            ]),\n            \"y\": np.array([1.0, 0.540302306, -0.416146837, -0.989992497]),\n            \"params\": (1.0, 1.0, 2.5, 0.01),\n            \"x_star\": np.array([1.5])\n        }\n    ]\n\n    def matern_kernel(X1, X2, sigma_f, l, nu):\n        \"\"\"\n        Computes the Matérn covariance matrix between two sets of points.\n        \"\"\"\n        # Compute squared Euclidean distance matrix\n        # dist_sq = sum_i( (X1_i - X2_i)^2 )\n        dist_sq = np.sum(X1**2, axis=1)[:, np.newaxis] + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n        # Numerical stability for small negative values from floating point errors\n        dist_sq = np.maximum(dist_sq, 0)\n        r = np.sqrt(dist_sq)\n\n        # Handle the r=0 special case\n        # For r=0, the kernel value is sigma_f^2\n        # For r>0, we use the standard formula\n        # A small epsilon is used for numerical stability\n        eps = 1e-9\n        r_is_zero = r < eps\n        \n        # Pre-compute constants\n        const_factor = (2**(1 - nu)) / gamma(nu)\n        sigma_f_sq = sigma_f**2\n\n        # Create a result matrix filled with the r=0 case\n        K = np.full(r.shape, sigma_f_sq)\n\n        # Compute arguments for the non-zero distance case\n        r_nz = r[~r_is_zero]\n        arg = (np.sqrt(2 * nu) * r_nz) / l\n        \n        # Calculate kernel values for r > 0\n        K[~r_is_zero] = sigma_f_sq * const_factor * (arg**nu) * kv(nu, arg)\n        \n        return K\n\n    results = []\n    for case in test_cases:\n        X, y = case[\"X\"], case[\"y\"]\n        sigma_f, l, nu, sigma_n = case[\"params\"]\n        x_star = case[\"x_star\"].reshape(1, -1) # Ensure x_star is 2D\n        n = X.shape[0]\n\n        # 1. Construct the Gram matrix K(X, X) and add noise\n        K = matern_kernel(X, X, sigma_f, l, nu)\n        K_yy = K + (sigma_n**2) * np.eye(n)\n\n        # 2. Compute the Cholesky decomposition of K_yy\n        # L L^T = K_yy\n        try:\n            L = cholesky(K_yy, lower=True)\n        except np.linalg.LinAlgError:\n            # Fallback for numerically challenging matrices\n            # Add a small jitter to the diagonal\n            jitter = 1e-6\n            L = cholesky(K_yy + jitter * np.eye(n), lower=True)\n\n\n        # 3. Compute the cross-covariance vector k_* = K(X, x_*)\n        k_star = matern_kernel(X, x_star, sigma_f, l, nu)\n\n        # 4. Compute the predictive mean\n        # Solve L v = y, then L^T alpha = v\n        v = solve_triangular(L, y, lower=True)\n        alpha = solve_triangular(L.T, v, lower=False)\n        mu_star = np.dot(k_star.T, alpha)\n\n        # 5. Compute the predictive variance\n        # Solve L z = k_*, then var = k_** - z^T z\n        z = solve_triangular(L, k_star, lower=True)\n        k_star_star = sigma_f**2 # k(x_*, x_*) where r=0\n        s2_star = k_star_star - np.dot(z.T, z)\n\n        # Ensure results are scalar floats\n        mu_val = float(mu_star)\n        s2_val = float(s2_star)\n        results.append([mu_val, s2_val])\n\n    # Format the final output string exactly as required\n    output_parts = [f\"[{mu},{s2}]\" for mu, s2 in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "One of the primary advantages of GPR as a surrogate model in Digital Twins is its inherent ability to quantify prediction uncertainty. This practice moves beyond simple point predictions to demonstrate how to rigorously propagate the GPR model's posterior uncertainty through a subsequent calculation—in this case, an integration to find total energy consumption . You will learn how the full covariance of the GPR predictions can be used to derive credible intervals for complex quantities calculated from the surrogate, a common and critical requirement in engineering and physics-based modeling.",
            "id": "4224377",
            "problem": "Consider a Digital Twin (DT) of a Cyber-Physical System (CPS) that uses Gaussian Process Regression (GPR) as a surrogate to predict instantaneous electric power consumption $f(t)$ in Watts as a function of time $t$ in seconds during a fixed operating mode. The goal is to compute uncertainty bounds on the total energy consumed over an operating cycle by rigorously propagating the surrogate model’s uncertainty. All quantities involving physical units must be reported with consistent units: time in seconds and energy in Joules.\n\nYou are provided with the following training data and modeling assumptions:\n\n- Training inputs (time samples in seconds): $[0,2,4,6,8]$.\n- Training outputs (measured power in Watts, corresponding to the above times): $[120.0,135.0,128.0,140.0,132.0]$.\n- Kernel choice: squared-exponential covariance with amplitude parameter $\\sigma_f^2$ and length-scale $\\ell$, given by\n$$\nk(t,t') = \\sigma_f^2 \\exp\\left(-\\frac{(t-t')^2}{2\\ell^2}\\right).\n$$\n- Hyperparameters: $\\sigma_f^2 = 900.0$ (Watts squared), $\\ell = 2.5$ (seconds), and independent Gaussian measurement noise variance $\\sigma_n^2 = 9.0$ (Watts squared).\n\nAssume the operating cycle is specified by a finite set of query times $t_i$ (in seconds), each representing an instantaneous sampling of the surrogate $f(t)$ at uniform time spacing $\\Delta t$ (in seconds). The total energy is approximated by the Riemann sum $\\sum_i f(t_i)\\,\\Delta t$ and must be reported in Joules. Since the surrogate prediction is uncertain, propagate the GPR posterior uncertainty to obtain lower and upper bounds of a two-sided central credible interval at level $0.95$ on the total energy. Report all bounds in Joules as real-valued decimals.\n\nImplement a program that:\n- Constructs the GPR posterior of $f(t)$ at the specified query times for each operating cycle case using the provided training data and hyperparameters.\n- Propagates the surrogate’s posterior uncertainty through the Riemann sum to obtain the central credible interval at level $0.95$ for the total energy in Joules.\n- Produces the final output as a single line containing the bounds for each case, where each case’s result is a two-element list $[L,U]$ for the lower bound $L$ and upper bound $U$ of the $0.95$ credible interval.\n\nUse the following test suite of operating cycles:\n1. Happy path: query times $[1,2,3,4,5,6,7,8,9]$ with uniform spacing $\\Delta t = 1.0$ seconds.\n2. Boundary case: empty query times $[]$ with $\\Delta t = 1.0$ seconds (this represents zero-length cycle and should yield zero energy with zero uncertainty).\n3. Single-sample edge: query times $[5.0]$ with $\\Delta t = 2.0$ seconds.\n4. Dense sampling edge: query times $[0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,5.5,6.0,6.5,7.0,7.5,8.0]$ with $\\Delta t = 0.5$ seconds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the form\n$$\n[[L_1,U_1],[L_2,U_2],[L_3,U_3],[L_4,U_4]],\n$$\nwhere each $L_i$ and $U_i$ are real numbers representing the lower and upper bounds (in Joules) of the $0.95$ credible interval for case $i$ in the order listed above.",
            "solution": "The problem is valid as it is scientifically grounded, objective, well-posed, and contains all necessary information for a unique solution. The task is to propagate the uncertainty from a Gaussian Process Regression (GPR) surrogate model of power consumption to an estimate of total energy consumption.\n\nThe solution involves three main stages:\n1.  Constructing the GPR posterior distribution for the power function $f(t)$ based on the provided training data.\n2.  Propagating the mean and covariance of this posterior distribution through the linear operator that calculates total energy, which is a Riemann sum.\n3.  Using the resulting distribution for total energy to compute the bounds of a $0.95$ central credible interval.\n\n**1. Gaussian Process Regression Model**\n\nA Gaussian Process (GP) defines a probability distribution over functions. We model the unknown power consumption function $f(t)$ as a draw from a GP:\n$$\nf(t) \\sim \\mathcal{GP}(\\mu(t), k(t, t'))\n$$\nwhere $\\mu(t)$ is the mean function and $k(t, t')$ is the covariance (or kernel) function. Following standard practice, we assume a zero-mean prior, $\\mu(t) = 0$. The problem specifies a squared-exponential kernel:\n$$\nk(t, t') = \\sigma_f^2 \\exp\\left(-\\frac{(t-t')^2}{2\\ell^2}\\right)\n$$\nwith hyperparameters $\\sigma_f^2 = 900.0 \\, \\text{W}^2$ (amplitude) and $\\ell = 2.5 \\, \\text{s}$ (length-scale).\n\nThe training data consists of $N=5$ noisy measurements of power. The training inputs are time points $X = [0, 2, 4, 6, 8]^T$ seconds, and the corresponding training outputs are power measurements $Y = [120.0, 135.0, 128.0, 140.0, 132.0]^T$ Watts. The observation model is $y_i = f(t_i) + \\epsilon_i$, where the measurement noise $\\epsilon_i$ is assumed to be independent and identically distributed Gaussian noise, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$, with noise variance $\\sigma_n^2 = 9.0 \\, \\text{W}^2$.\n\nGiven the training data $(X, Y)$ and a set of $N_*$ query time points $X_* = [t_{*1}, \\dots, t_{*N_*}]^T$, the GPR framework provides the posterior distribution of the function values $\\mathbf{f}_* = [f(t_{*1}), \\dots, f(t_{*N_*})]^T$. This posterior is a multivariate Gaussian distribution, $p(\\mathbf{f}_* | X, Y, X_*) = \\mathcal{N}(\\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)$, with mean vector $\\boldsymbol{\\mu}_*$ and covariance matrix $\\boldsymbol{\\Sigma}_*$.\n\nThe posterior mean and covariance are given by:\n$$\n\\boldsymbol{\\mu}_* = K(X_*, X) [K(X, X) + \\sigma_n^2 I_N]^{-1} Y\n$$\n$$\n\\boldsymbol{\\Sigma}_* = K(X_*, X_*) - K(X_*, X) [K(X, X) + \\sigma_n^2 I_N]^{-1} K(X, X_*)\n$$\nwhere:\n- $I_N$ is the $N \\times N$ identity matrix.\n- $K(X, X)$ is the $N \\times N$ covariance matrix of training inputs, with entries $[K(X, X)]_{ij} = k(t_i, t_j)$.\n- $K(X, X_*)$ is the $N \\times N_*$ cross-covariance matrix between training and query inputs, with entries $[K(X, X_*)]_{ij} = k(t_i, t_{*j})$.\n- $K(X_*, X) = K(X, X_*)^T$.\n- $K(X_*, X_*)$ is the $N_* \\times N_*$ covariance matrix of query inputs, with entries $[K(X_*, X_*)]_{ij} = k(t_{*i}, t_{*j})$.\n\n**2. Uncertainty Propagation for Total Energy**\n\nThe total energy $E$ (in Joules) consumed over the operating cycle is approximated by the Riemann sum:\n$$\nE = \\sum_{j=1}^{N_*} f(t_{*j}) \\Delta t\n$$\nwhere $\\Delta t$ is the uniform time spacing in seconds. This is a linear transformation of the vector of function values $\\mathbf{f}_*$:\n$$\nE = \\mathbf{w}^T \\mathbf{f}_* \\quad \\text{where} \\quad \\mathbf{w} = [\\Delta t, \\Delta t, \\dots, \\Delta t]^T\n$$\nSince $\\mathbf{f}_*$ follows a multivariate Gaussian distribution, the energy $E$, being a linear combination of its components, follows a univariate Gaussian distribution: $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$.\n\nThe mean and variance of the energy are derived using the properties of linear transformations of random variables:\n- **Mean Energy**: $\\mu_E = \\mathbb{E}[E] = \\mathbb{E}[\\mathbf{w}^T \\mathbf{f}_*] = \\mathbf{w}^T \\mathbb{E}[\\mathbf{f}_*] = \\mathbf{w}^T \\boldsymbol{\\mu}_*$.\n- **Variance of Energy**: $\\sigma_E^2 = \\text{Var}(E) = \\text{Var}(\\mathbf{w}^T \\mathbf{f}_*) = \\mathbf{w}^T \\text{Var}(\\mathbf{f}_*) \\mathbf{w} = \\mathbf{w}^T \\boldsymbol{\\Sigma}_* \\mathbf{w}$.\n\nSubstituting the expressions for $\\boldsymbol{\\mu}_*$ and $\\boldsymbol{\\Sigma}_*$, we can compute the mean and variance of the total energy.\n\n**3. Credible Interval Calculation**\n\nA two-sided central credible interval at level $1-\\alpha$ for a Gaussian random variable $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$ is given by:\n$$\n[L, U] = [\\mu_E - z_{\\alpha/2} \\sigma_E, \\mu_E + z_{\\alpha/2} \\sigma_E]\n$$\nwhere $\\sigma_E = \\sqrt{\\sigma_E^2}$ is the standard deviation, and $z_{\\alpha/2}$ is the upper $\\alpha/2$ critical value of the standard normal distribution. For the specified level of $0.95$, we have $1-\\alpha = 0.95$, so $\\alpha = 0.05$. The required critical value is $z_{0.025}$, which is the $0.975$ quantile of the standard normal distribution, $z_{0.025} \\approx 1.95996$.\n\nThe lower bound $L$ and upper bound $U$ of the $0.95$ credible interval for the total energy $E$ are therefore:\n$$\nL = \\mu_E - z_{0.025} \\sigma_E\n$$\n$$\nU = \\mu_E + z_{0.025} \\sigma_E\n$$\n\nThe computational procedure for each test case is as follows:\n1.  Define the training data $X, Y$ and hyperparameters $\\sigma_f^2, \\ell, \\sigma_n^2$.\n2.  Pre-compute the matrix $K_y = K(X, X) + \\sigma_n^2 I_N$ and the vector $\\boldsymbol{\\alpha} = K_y^{-1} Y$. For numerical stability, $\\boldsymbol{\\alpha}$ is found by solving the linear system $K_y \\boldsymbol{\\alpha} = Y$.\n3.  For each operating cycle (defined by query times $X_*$ and time step $\\Delta t$):\n    a. If $X_*$ is an empty set, the energy $E$ is $0$ with zero uncertainty, so $L=U=0$.\n    b. Otherwise, construct the matrices $K(X, X_*)$ and $K(X_*, X_*)$.\n    c. Calculate the posterior mean power vector: $\\boldsymbol{\\mu}_* = K(X_*, X) \\boldsymbol{\\alpha}$.\n    d. Calculate the posterior covariance power matrix: $\\boldsymbol{\\Sigma}_* = K(X_*, X_*) - K(X_*, X) K_y^{-1} K(X, X_*)$. This is also computed via a linear solve to avoid direct inversion.\n    e. Compute the mean energy $\\mu_E = \\Delta t \\sum_{j} (\\boldsymbol{\\mu}_*)_j$.\n    f. Compute the variance of energy $\\sigma_E^2 = (\\Delta t)^2 \\sum_{i,j} (\\boldsymbol{\\Sigma}_*)_{ij}$.\n    g. Compute the bounds $L$ and $U$ using the standard deviation $\\sigma_E = \\sqrt{\\sigma_E^2}$ and $z_{0.025}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the 95% credible interval for total energy consumption based on a\n    Gaussian Process Regression surrogate model of power.\n    \"\"\"\n    \n    # 1. Define GPR model parameters and training data\n    # Training inputs (time in seconds)\n    X_train = np.array([0., 2., 4., 6., 8.]).reshape(-1, 1)\n    \n    # Training outputs (power in Watts)\n    y_train = np.array([120.0, 135.0, 128.0, 140.0, 132.0]).reshape(-1, 1)\n    \n    # Hyperparameters\n    sigma_f_sq = 900.0  # Amplitude variance (Watts^2)\n    ell = 2.5          # Length-scale (seconds)\n    sigma_n_sq = 9.0   # Noise variance (Watts^2)\n    \n    # Test suite of operating cycles\n    test_cases = [\n        # Case 1: Happy path\n        (np.array([1., 2., 3., 4., 5., 6., 7., 8., 9.]), 1.0),\n        # Case 2: Boundary case (empty)\n        (np.array([]), 1.0),\n        # Case 3: Single-sample edge\n        (np.array([5.0]), 2.0),\n        # Case 4: Dense sampling edge\n        (np.arange(0.0, 8.01, 0.5), 0.5)\n    ]\n\n    def squared_exponential_kernel(xa, xb, sigma_f_sq_val, ell_val):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of points.\n        Vectorized implementation for 1D inputs.\n        \"\"\"\n        # Broadcasting creates a matrix of pairwise differences (xa_i - xb_j)\n        # and then squares them.\n        sqdist = (xa - xb.T)**2\n        return sigma_f_sq_val * np.exp(-0.5 / ell_val**2 * sqdist)\n\n    # 2. Pre-computation for GPR posterior\n    # This part is independent of the query points and can be done once.\n    N = X_train.shape[0]\n    Ky = squared_exponential_kernel(X_train, X_train, sigma_f_sq, ell) + sigma_n_sq * np.eye(N)\n    \n    # For numerical stability, solve the linear system K_y * alpha = y_train\n    # instead of computing the inverse of K_y directly.\n    alpha = np.linalg.solve(Ky, y_train)\n\n    # 3. Process each test case\n    results = []\n    \n    # Get the z-score for the 95% confidence interval once.\n    # z such that P(-z < Z < z) = 0.95 for Z ~ N(0,1)\n    z_score = norm.ppf(0.975)\n\n    for X_query_vals, dt in test_cases:\n        # Handle the boundary case of an empty set of query times\n        if X_query_vals.size == 0:\n            results.append([0.0, 0.0])\n            continue\n\n        X_query = X_query_vals.reshape(-1, 1)\n        \n        # 4. GPR prediction at query points\n        K_star = squared_exponential_kernel(X_train, X_query, sigma_f_sq, ell)\n        K_star_star = squared_exponential_kernel(X_query, X_query, sigma_f_sq, ell)\n        \n        # Posterior mean of power at query points\n        mu_star = K_star.T @ alpha\n        \n        # Posterior covariance of power at query points\n        # Solve K_y * v = K_star to get v = (K_y^-1) * K_star\n        v = np.linalg.solve(Ky, K_star)\n        Sigma_star = K_star_star - K_star.T @ v\n\n        # 5. Uncertainty propagation for total energy\n        # Mean energy (Joules)\n        # E = sum(f(t_i) * dt) -> mu_E = sum(mu_star_i * dt)\n        mu_E = dt * np.sum(mu_star)\n\n        # Variance of energy (Joules^2)\n        # Var(E) = w^T * Sigma_star * w, where w is a vector of dt\n        # This simplifies to dt^2 * sum(all elements of Sigma_star)\n        var_E = dt**2 * np.sum(Sigma_star)\n        \n        # Handle potential small negative variance from numerical precision errors\n        if var_E < 0:\n            var_E = 0.0\n        \n        sigma_E = np.sqrt(var_E)\n\n        # 6. Calculate the 95% credible interval\n        lower_bound = mu_E - z_score * sigma_E\n        upper_bound = mu_E + z_score * sigma_E\n        \n        results.append([lower_bound, upper_bound])\n\n    # 7. Format the final output string as specified\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In safety-critical Cyber-Physical Systems, decisions must often be made with verifiable, high-confidence guarantees, even when relying on learning-based models. This exercise showcases one of the most powerful applications of GPR: using its principled uncertainty quantification to create a formal \"safety certificate\" . By working through this problem, you will learn how to translate the probabilistic output of a GPR surrogate into a concrete assurance that a system's behavior will remain within a safe operating envelope, a key technique for safe learning-based control and verification.",
            "id": "4224426",
            "problem": "A Cyber-Physical System (CPS) controller must decide whether a candidate action satisfies a one-sided safety constraint with high confidence based on a Gaussian Process (GP) surrogate of the system response. You will construct a Gaussian Process Regression (GPR) model and compute a safety certificate for each decision that certifies the chance constraint is met at probability level $0.99$. The task is to implement a program that derives the required quantities from first principles of Gaussian conditioning.\n\nStart from the following fundamental bases:\n- The definition of a Gaussian Process (GP) as a collection of random variables, any finite subset of which is jointly Gaussian with a mean function $m(\\cdot)$ and covariance function $k(\\cdot,\\cdot)$.\n- The fact that a jointly Gaussian random vector has a conditional distribution that is Gaussian, obtained by applying standard conditioning rules for multivariate normal distributions.\n- The classical result for a scalar Gaussian random variable $Z \\sim \\mathcal{N}(\\mu,\\sigma^2)$ that the event probability $\\mathbb{P}[Z \\le b]$ equals the standard normal cumulative distribution function evaluated at $(b-\\mu)/\\sigma$.\n\nConsider a scalar, dimensionless CPS output modeled by a zero-mean GP with a squared-exponential covariance (also called the radial basis function covariance). The covariance is given by\n$$\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\tfrac{1}{2}\\tfrac{(x-x')^2}{\\ell^2}\\right),\n$$\nwith hyperparameters $\\ell$ (length-scale), $\\sigma_f$ (signal standard deviation), and independent, additive Gaussian observation noise with variance $\\sigma_n^2$. Given training inputs $X \\in \\mathbb{R}^n$ and noisy outputs $y \\in \\mathbb{R}^n$, the predictive distribution at a test input $x_\\star \\in \\mathbb{R}$ is Gaussian with some mean $m_\\star$ and variance $v_\\star$ determined by conditioning the joint Gaussian prior of $[y; f(x_\\star)]$ on $y$.\n\nFor a one-sided safety constraint of the form $f(x_\\star) \\le b$, a sufficient certificate to guarantee $\\mathbb{P}[f(x_\\star) \\le b] \\ge p$ with $p \\in (0,1)$ when the predictive distribution is Gaussian with mean $m_\\star$ and variance $v_\\star$ is that the following signed margin is nonnegative:\n$$\nC(x_\\star; b, p) \\triangleq b - \\left(m_\\star + z_p \\sqrt{v_\\star}\\right),\n$$\nwhere $z_p$ is the quantile of the standard normal distribution satisfying $\\Phi(z_p) = p$. The value $C(x_\\star; b, p)$ is the safety certificate: if $C(x_\\star; b, p) \\ge 0$, then the chance constraint holds; if $C(x_\\star; b, p)  0$, it does not.\n\nImplement a program that:\n- Constructs the GPR predictive mean $m_\\star$ and predictive variance $v_\\star$ for each given test input $x_\\star$ by conditioning the joint Gaussian distribution implied by the GP prior with the squared-exponential kernel and the noisy training observations.\n- Computes the safety certificate $C(x_\\star; b, p)$ for each test case at probability level $p = 0.99$.\n\nUse the following fixed hyperparameters and training data:\n- Hyperparameters: $\\ell = 0.7$, $\\sigma_f = 1.0$, $\\sigma_n = 0.05$.\n- Training inputs: $X = [-1.5,\\,-1.0,\\,-0.25,\\,0.0,\\,0.5,\\,0.75,\\,1.25]$.\n- Training outputs: $y_i = \\sin(2 x_i)$ for each $x_i \\in X$.\n\nTest suite of decisions and constraints:\n- Case $1$: $x_\\star = 0.25$, $b = 0.8$.\n- Case $2$: $x_\\star = 2.0$, $b = 0.4$.\n- Case $3$: $x_\\star = 0.75$, $b = 1.1$.\n\nComputational requirements and conventions:\n- All computations are dimensionless (no physical units).\n- You must compute the predictive mean $m_\\star$ and variance $v_\\star$ using only the fundamentals listed above and linear algebraic operations on the kernel matrices. Ensure numerical stability in matrix inversion by using a Cholesky factorization of the regularized kernel matrix.\n- Use $p = 0.99$ and the exact standard normal quantile $z_p$ consistent with this $p$.\n- For each case, output the safety certificate value $C(x_\\star; b, p)$ as a real number.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three certificate values in the order of the cases, enclosed in square brackets.\n- Each number must be rounded to exactly six digits after the decimal point.\n- For example, a valid output would look like $[0.123456,-0.123456,0.000000]$.",
            "solution": "The problem is evaluated as valid, as it is scientifically grounded in the principles of Gaussian Process Regression, is well-posed with all necessary data and constraints provided, and is expressed in objective, formal language. We may proceed with the solution.\n\nThe core of the problem is to compute a safety certificate $C(x_\\star; b, p)$ for a candidate action $x_\\star$ and a safety boundary $b$ at a confidence level $p$. This requires determining the predictive distribution of a system's response, modeled as a Gaussian Process (GP), at the point $x_\\star$. The certificate is defined as:\n$$\nC(x_\\star; b, p) \\triangleq b - \\left(m_\\star + z_p \\sqrt{v_\\star}\\right)\n$$\nwhere $m_\\star$ and $v_\\star$ are the mean and variance of the predictive distribution of the system's response $f(x_\\star)$, and $z_p$ is the quantile of the standard normal distribution for the probability $p$.\n\nWe begin by establishing the predictive equations from the first principles of conditioning a multivariate Gaussian distribution.\n\nA zero-mean GP prior is placed on the latent function $f(x)$. The observations $y$ at training locations $X = \\{x_1, \\dots, x_n\\}$ are related to the latent function values $f_X = [f(x_1), \\dots, f(x_n)]^T$ by the model $y = f_X + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2 I)$ is independent Gaussian noise.\n\nThe joint distribution of the observed training outputs $y$ and the latent function value $f_\\star = f(x_\\star)$ at a new test point $x_\\star$ is a multivariate Gaussian distribution. Given the zero-mean prior, its distribution is:\n$$\n\\begin{pmatrix} y \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I  K(X, x_\\star) \\\\\nK(x_\\star, X)  K(x_\\star, x_\\star)\n\\end{pmatrix}\n\\right)\n$$\nwhere $K(A, B)$ is the matrix of covariance function evaluations between points in sets $A$ and $B$. The specified covariance function is the squared-exponential kernel:\n$$\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\frac{(x-x')^2}{\\ell^2}\\right)\n$$\nwith hyperparameters $\\sigma_f = 1.0$, $\\ell = 0.7$, and noise standard deviation $\\sigma_n = 0.05$.\n\nLet us define the blocks of the covariance matrix for notational clarity:\n- $K = K(X, X) \\in \\mathbb{R}^{n \\times n}$\n- $k_\\star = K(X, x_\\star) \\in \\mathbb{R}^{n \\times 1}$\n- $k_{\\star\\star} = K(x_\\star, x_\\star) \\in \\mathbb{R}$\n\nThe joint distribution is then:\n$$\n\\begin{pmatrix} y \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix}\nK + \\sigma_n^2 I  k_\\star \\\\\nk_\\star^T  k_{\\star\\star}\n\\end{pmatrix}\n\\right)\n$$\n\nThe predictive distribution $p(f_\\star | X, y, x_\\star)$ is the conditional distribution of $f_\\star$ given the observations $y$. For a general partitioned Gaussian vector, if $\\begin{pmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}\\right)$, the conditional distribution of $\\mathbf{x}_2$ given $\\mathbf{x}_1$ is $\\mathcal{N}(\\mu_{2|1}, \\Sigma_{22|1})$ where:\n- $\\mu_{2|1} = \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{x}_1 - \\mu_1)$\n- $\\Sigma_{22|1} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$\n\nApplying this rule to our GP case (with $\\mathbf{x}_1=y, \\mathbf{x}_2=f_\\star, \\mu_1=0, \\mu_2=0$), we obtain the predictive mean $m_\\star$ and predictive variance $v_\\star$:\n$$\nm_\\star = k_\\star^T (K + \\sigma_n^2 I)^{-1} y\n$$\n$$\nv_\\star = k_{\\star\\star} - k_\\star^T (K + \\sigma_n^2 I)^{-1} k_\\star\n$$\n\nTo compute these quantities numerically while ensuring stability, we avoid the direct computation of the inverse of $K_y \\triangleq K + \\sigma_n^2 I$. Instead, we use the Cholesky decomposition of this positive-definite matrix: $K_y = L L^T$, where $L$ is a lower-triangular matrix.\n\nThe calculation proceeds as follows:\n1.  Construct the training data:\n    - Inputs: $X = [-1.5,\\,-1.0,\\,-0.25,\\,0.0,\\,0.5,\\,0.75,\\,1.25]$\n    - Outputs: $y = [\\sin(2x_i)]_{i=1}^n$\n2.  Compute the kernel matrix $K = K(X, X)$ and the noisy kernel matrix $K_y = K + \\sigma_n^2 I$, where $\\sigma_n^2 = 0.05^2 = 0.0025$.\n3.  Compute the Cholesky factorization $K_y = L L^T$.\n4.  To find the predictive mean $m_\\star$, we first solve for an intermediate vector $\\alpha = K_y^{-1} y$. This is done by solving two triangular systems:\n    - Solve $L v = y$ for $v$ using forward substitution.\n    - Solve $L^T \\alpha = v$ for $\\alpha$ using backward substitution.\n    The predictive mean is then $m_\\star = k_\\star^T \\alpha$.\n5.  To find the predictive variance $v_\\star$, we compute the term $k_\\star^T K_y^{-1} k_\\star$. We first solve the triangular system $L w = k_\\star$ for $w$ using forward substitution. Then, the term becomes $w^T w$, since $w^T w = (L^{-1} k_\\star)^T (L^{-1} k_\\star) = k_\\star^T (L^{-1})^T L^{-1} k_\\star = k_\\star^T (L L^T)^{-1} k_\\star = k_\\star^T K_y^{-1} k_\\star$.\n    The predictive variance is then $v_\\star = k_{\\star\\star} - w^T w$. Note that for the squared-exponential kernel, $k(x,x) = \\sigma_f^2 \\exp(0) = \\sigma_f^2$, so $k_{\\star\\star} = \\sigma_f^2 = 1.0$.\n6.  With $m_\\star$ and $v_\\star$ computed for a given test case $(x_\\star, b)$, we find the standard normal quantile $z_p$ for $p = 0.99$. This value is $z_{0.99} \\approx 2.32634787$.\n7.  Finally, we compute the safety certificate: $C(x_\\star; b, p) = b - (m_\\star + z_{0.99} \\sqrt{v_\\star})$.\n\nThis procedure is repeated for each of the three test cases specified in the problem statement. The resulting three certificate values are then reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve, solve_triangular\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes Gaussian Process Regression safety certificates from first principles.\n    \"\"\"\n    #\n    # Step 1: Define problem parameters and data\n    #\n    \n    # Hyperparameters for the squared-exponential kernel and noise\n    l_hyper = 0.7\n    sigma_f_hyper = 1.0\n    sigma_n_hyper = 0.05\n\n    # Training data\n    X_train = np.array([-1.5, -1.0, -0.25, 0.0, 0.5, 0.75, 1.25])\n    # The problem specifies generating y from the true function sin(2x)\n    y_train = np.sin(2 * X_train)\n    \n    # Test cases: each is a tuple of (x_star, b)\n    test_cases = [\n        (0.25, 0.8),  # Case 1\n        (2.0, 0.4),   # Case 2\n        (0.75, 1.1),  # Case 3\n    ]\n    \n    # Safety probability level and the corresponding standard normal quantile\n    p_safety = 0.99\n    z_p = norm.ppf(p_safety)\n    \n    #\n    # Step 2: Define the kernel function\n    #\n    def squared_exponential_kernel(x1, x2, l, sigma_f):\n        \"\"\"\n        Computes the squared-exponential covariance between two sets of points.\n        x1: np.ndarray of shape (N1,)\n        x2: np.ndarray of shape (N2,)\n        Returns a covariance matrix of shape (N1, N2).\n        \"\"\"\n        # Ensure inputs are 2D for broadcasting\n        x1 = np.asarray(x1).reshape(-1, 1)\n        x2 = np.asarray(x2).reshape(-1, 1)\n        \n        # Compute squared Euclidean distances\n        # dists is of shape (N1, N2)\n        dists = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n        \n        return sigma_f**2 * np.exp(-0.5 / l**2 * dists)\n\n    #\n    # Step 3: Pre-compute fixed matrices for GPR\n    #\n    \n    # Compute the covariance matrix of training points\n    K = squared_exponential_kernel(X_train, X_train, l_hyper, sigma_f_hyper)\n    \n    # Add noise variance to the diagonal for the observed data covariance\n    K_yy = K + (sigma_n_hyper**2) * np.eye(len(X_train))\n    \n    # Compute the Cholesky factorization for stable inversion: K_yy = L * L^T\n    # The `lower=True` argument ensures L is lower-triangular.\n    try:\n        L = cholesky(K_yy, lower=True)\n    except np.linalg.LinAlgError:\n        print(\"Error: The covariance matrix is not positive definite.\")\n        return\n\n    #\n    # Step 4: Process each test case\n    #\n    \n    results = []\n    for x_star, b in test_cases:\n        #\n        # Step 4a: Compute kernel vectors for the test point\n        #\n        # Covariance between test point and training points\n        k_star = squared_exponential_kernel([x_star], X_train, l_hyper, sigma_f_hyper).flatten()\n        \n        # Covariance of the test point with itself (prior variance)\n        k_star_star = squared_exponential_kernel([x_star], [x_star], l_hyper, sigma_f_hyper).item()\n\n        #\n        # Step 4b: Compute predictive mean (m_star)\n        #\n        \n        # Solve for alpha = (K + sigma_n^2*I)^-1 * y using the Cholesky factor.\n        # cho_solve takes a tuple (L, True) indicating a pre-computed lower-triangular factor.\n        alpha = cho_solve((L, True), y_train)\n        \n        # Predictive mean: m_star = k_star^T * alpha\n        m_star = k_star.T @ alpha\n        \n        #\n        # Step 4c: Compute predictive variance (v_star)\n        #\n        \n        # Solve the triangular system L*v = k_star for v\n        v = solve_triangular(L, k_star, lower=True)\n        \n        # Predictive variance: v_star = k_star_star - v^T * v\n        v_star = k_star_star - v.T @ v\n        \n        #\n        # Step 4d: Compute the safety certificate\n        #\n        \n        # C = b - (m_star + z_p * sqrt(v_star))\n        certificate = b - (m_star + z_p * np.sqrt(v_star))\n        results.append(certificate)\n\n    #\n    # Step 5: Format and print the final output\n    #\n    \n    # Format results to exactly six decimal places\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}