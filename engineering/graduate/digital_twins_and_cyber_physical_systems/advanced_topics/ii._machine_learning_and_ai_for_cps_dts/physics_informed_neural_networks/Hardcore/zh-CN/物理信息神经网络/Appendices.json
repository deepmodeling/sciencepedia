{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络 (PINN) 的核心在于其损失函数中包含了控制方程的残差。这个练习将理论付诸实践，要求你从线弹性力学的第一性原理出发，为二维问题推导物理残差的具体形式。通过为一个简单的神经网络“拟设” (ansatz) 手动计算这些残差，你将深刻理解 PINN 如何将物理定律编码到其训练过程中，这是从抽象概念到具体实现的关键一步。",
            "id": "2668927",
            "problem": "考虑一个二维、小应变、线性弹性固体，在准静态设定下，单位体积体力为 $\\mathbf{b} = (b_x, b_y)$。设位移场为 $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$。仅从线性动量平衡、小应变运动学关系以及用拉梅参数 $(\\lambda, \\mu)$ 表示的线性各向同性本构律出发，完成以下任务：\n\n1) 写出柯西应力张量 $\\boldsymbol{\\sigma}$ 关于应变张量 $\\boldsymbol{\\varepsilon}$ 和 $(\\lambda,\\mu)$ 的本构律，并用位移场定义小应变张量。所有物理量均用笛卡尔坐标表示。\n\n2) 使用您的定义，将物理信息神经网络 (PINN) 会在内部配置点上强制执行的内部平衡残差分量，即 $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$ 的两个分量，用 $u_x$ 和 $u_y$ 的空间导数以及拉梅参数 $(\\lambda,\\mu)$ 以笛卡尔坐标明确展开。\n\n3) 现在考虑位移场的一个单隐藏神经元神经网络拟设，其激活函数为双曲正切函数，\n$$\nz(x,y) = w_1 x + w_2 y + b_1,\\quad \\mathbf{u}(x,y) = \\mathbf{W}_2 \\,\\tanh\\!\\big(z(x,y)\\big) + \\mathbf{b}_2,\n$$\n其中 $\\mathbf{W}_2 \\in \\mathbb{R}^{2 \\times 1}$ 且 $\\mathbf{b}_2 \\in \\mathbb{R}^{2}$。取特定参数\n$$\nw_1 = 1,\\quad w_2 = 2,\\quad b_1 = 0,\\quad \\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\\quad \\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\n并假设体力为零 $\\mathbf{b}=\\mathbf{0}$。在点 $(x_0,y_0)=(1,0)$ 处计算内部平衡残差向量的两个分量。\n\n假设一个无量纲化的公式，从而所有量都是无量纲的。将最终结果表示为包含两个残差分量的单个行向量，写成闭式解析表达式。不要近似或舍入任何常数；保持双曲函数不被求值（例如，明确写出 $\\tanh(1)$、$\\cosh(1)$）。",
            "solution": "该问题经验证是适定的、有科学依据的，并且包含唯一解所需的所有信息。我们开始推导。\n\n该问题要求从连续介质力学的基本原理出发，逐步推导，以评估给定神经网络位移场拟设的平衡残差。\n\n首先，我们处理任务1：在笛卡尔坐标系中定义相关的运动学关系和本构关系。\n\n在坐标为 $(x,y)$ 的二维域中，位移场为 $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$。小应变（或无穷小应变）张量 $\\boldsymbol{\\varepsilon}$ 由位移场的梯度定义如下：\n$$\n\\boldsymbol{\\varepsilon} = \\frac{1}{2} \\left[ \\nabla \\mathbf{u} + (\\nabla \\mathbf{u})^T \\right]\n$$\n其笛卡尔分量形式为：\n$$\n\\varepsilon_{xx} = \\frac{\\partial u_x}{\\partial x}, \\quad \\varepsilon_{yy} = \\frac{\\partial u_y}{\\partial y}, \\quad \\varepsilon_{xy} = \\varepsilon_{yx} = \\frac{1}{2}\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n代表体积应变的应变张量的迹为 $\\text{tr}(\\boldsymbol{\\varepsilon}) = \\varepsilon_{kk} = \\varepsilon_{xx} + \\varepsilon_{yy}$。\n\n对于线性、各向同性、弹性材料，关联柯西应力张量 $\\boldsymbol{\\sigma}$ 和应变张量 $\\boldsymbol{\\varepsilon}$ 的本构律由胡克定律给出，它可以用拉梅参数 $\\lambda$ 和 $\\mu$（也称为第一和第二拉梅参数，其中 $\\mu$ 是剪切模量）来表示：\n$$\n\\boldsymbol{\\sigma} = \\lambda \\, \\text{tr}(\\boldsymbol{\\varepsilon}) \\, \\mathbf{I} + 2\\mu \\, \\boldsymbol{\\varepsilon}\n$$\n其中 $\\mathbf{I}$ 是二阶单位张量。在笛卡尔分量形式中，应力分量为：\n$$\n\\sigma_{xx} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{xx} = (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{yy} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{yy} = \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{xy} = \\sigma_{yx} = 2\\mu \\varepsilon_{xy} = \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n\n接下来，我们处理任务2：推导内部平衡残差分量的显式形式。准静态设定下的线性动量平衡即为平衡方程 $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b} = \\mathbf{0}$。物理信息神经网络 (PINN) 旨在最小化该方程的残差，即 $\\mathbf{R} = \\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$。该残差向量在笛卡尔坐标系中的分量是：\n$$\nR_x = \\frac{\\partial \\sigma_{xx}}{\\partial x} + \\frac{\\partial \\sigma_{xy}}{\\partial y} + b_x\n$$\n$$\nR_y = \\frac{\\partial \\sigma_{yx}}{\\partial x} + \\frac{\\partial \\sigma_{yy}}{\\partial y} + b_y\n$$\n将应力分量关于位移导数的表达式代入，得到纳维-柯西方程。我们假设材料是均匀的，因此 $\\lambda$ 和 $\\mu$ 是常数。\n对于残差的 $x$ 分量：\n$$\nR_x = \\frac{\\partial}{\\partial x} \\left( (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y} \\right) + \\frac{\\partial}{\\partial y} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + b_x\n$$\n$$\nR_x = (\\lambda + 2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_y}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_x}{\\partial y^2} + \\mu\\frac{\\partial^2 u_y}{\\partial y \\partial x} + b_x\n$$\n根据混合偏导数的相等性（$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial^2 u_y}{\\partial y \\partial x}$），我们可以对各项进行分组：\n$$\nR_x = (\\lambda+2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\mu \\frac{\\partial^2 u_x}{\\partial y^2} + (\\lambda+\\mu) \\frac{\\partial^2 u_y}{\\partial x \\partial y} + b_x\n$$\n对于残差的 $y$ 分量：\n$$\nR_y = \\frac{\\partial}{\\partial x} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + \\frac{\\partial}{\\partial y} \\left( \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y} \\right) + b_y\n$$\n$$\nR_y = \\mu\\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_y}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_x}{\\partial y \\partial x} + (\\lambda + 2\\mu)\\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\n再次进行分组：\n$$\nR_y = (\\lambda+\\mu) \\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu \\frac{\\partial^2 u_y}{\\partial x^2} + (\\lambda+2\\mu) \\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\n这些就是 PINN 将强制执行的残差分量的显式表达式。\n\n最后，我们处理任务3：为指定的神经网络拟设和参数评估这些残差。该拟设由下式给出：\n$z(x,y) = w_1 x + w_2 y + b_1$ 且 $\\mathbf{u}(x,y) = \\mathbf{W}_2 \\tanh(z(x,y)) + \\mathbf{b}_2$。\n使用给定的参数 $w_1 = 1$，$w_2 = 2$，$b_1 = 0$，$\\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $\\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，我们有：\n$z(x,y) = x + 2y$\n位移分量为：\n$$\nu_x(x,y) = 2 \\tanh(x+2y)\n$$\n$$\nu_y(x,y) = -1 \\tanh(x+2y)\n$$\n我们必须计算 $u_x$ 和 $u_y$ 的二阶偏导数。令 $T(x,y) = \\tanh(x+2y)$ 和 $S(x,y) = \\text{sech}^2(x+2y) = 1-\\tanh^2(x+2y)$。我们使用链式法则和导数恒等式 $\\frac{d}{d\\alpha}\\tanh(\\alpha) = \\text{sech}^2(\\alpha)$ 及 $\\frac{d}{d\\alpha}\\text{sech}^2(\\alpha) = -2\\tanh(\\alpha)\\text{sech}^2(\\alpha)$。\n\n$u_x = 2T$ 的导数：\n$\\frac{\\partial u_x}{\\partial x} = 2 \\frac{\\partial T}{\\partial x} = 2S \\cdot 1 = 2S$\n$\\frac{\\partial u_x}{\\partial y} = 2 \\frac{\\partial T}{\\partial y} = 2S \\cdot 2 = 4S$\n$\\frac{\\partial^2 u_x}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2S) = 2(-2TS) \\cdot 1 = -4TS$\n$\\frac{\\partial^2 u_x}{\\partial y^2} = \\frac{\\partial}{\\partial y}(4S) = 4(-2TS) \\cdot 2 = -16TS$\n$\\frac{\\partial^2 u_x}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2S) = 2(-2TS) \\cdot 2 = -8TS$\n\n$u_y = -T$ 的导数：\n$\\frac{\\partial u_y}{\\partial x} = -S \\cdot 1 = -S$\n$\\frac{\\partial u_y}{\\partial y} = -S \\cdot 2 = -2S$\n$\\frac{\\partial^2 u_y}{\\partial x^2} = \\frac{\\partial}{\\partial x}(-S) = -(-2TS) \\cdot 1 = 2TS$\n$\\frac{\\partial^2 u_y}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2S) = -2(-2TS) \\cdot 2 = 8TS$\n$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(-S) = -(-2TS) \\cdot 2 = 4TS$\n\n给定体力为零，所以 $\\mathbf{b}=\\mathbf{0}$。我们将这些导数代入残差表达式中：\n$$\nR_x = (\\lambda+2\\mu)(-4TS) + \\mu(-16TS) + (\\lambda+\\mu)(-8TS)\n$$\n$$\nR_x = [-4(\\lambda+2\\mu) - 16\\mu - 8(\\lambda+\\mu)]TS = [-4\\lambda - 8\\mu - 16\\mu - 8\\lambda - 8\\mu]TS = (-12\\lambda - 32\\mu)TS\n$$\n$$\nR_y = (\\lambda+\\mu)(-8TS) + \\mu(2TS) + (\\lambda+2\\mu)(8TS)\n$$\n$$\nR_y = [-8(\\lambda+\\mu) + 2\\mu + 8(\\lambda+2\\mu)]TS = [-8\\lambda - 8\\mu + 2\\mu + 8\\lambda + 16\\mu]TS = 10\\mu TS\n$$\n残差向量为 $\\mathbf{R}(x,y) = \\begin{pmatrix} (-12\\lambda - 32\\mu)  10\\mu \\end{pmatrix} \\tanh(x+2y)\\text{sech}^2(x+2y)$。\n\n我们必须在点 $(x_0, y_0) = (1, 0)$ 处计算该值。在该点，双曲函数的参数为 $z_0 = 1 + 2(0) = 1$。\n$TS$ 项变为 $\\tanh(1)\\text{sech}^2(1)$。使用恒等式 $\\text{sech}^2(\\alpha) = 1/\\cosh^2(\\alpha)$，此项为 $\\frac{\\tanh(1)}{\\cosh^2(1)}$。\n在 $(1, 0)$ 点的残差分量是：\n$$\nR_x(1,0) = (-12\\lambda - 32\\mu) \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n$$\nR_y(1,0) = 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n题目要求将结果表示为单个行向量。\n$$\n\\mathbf{R}(1,0) = \\begin{pmatrix} (-12\\lambda - 32\\mu) \\frac{\\tanh(1)}{\\cosh^{2}(1)}  10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} \\end{pmatrix}\n$$\n这就是最终的解析表达式。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(-12\\lambda - 32\\mu) \\frac{\\tanh(1)}{\\cosh^{2}(1)}  10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "一个完整的物理问题不仅包含控制方程，还必须满足特定的边界条件。这个练习探讨了如何为包含狄利克雷 (Dirichlet) 和诺伊曼 (Neumann) 条件的混合边界问题构建损失函数，这在实际工程应用中非常普遍。通过这个实践，你将学会如何为不同类型的边界约束设计相应的残差项，并理解自动微分 (AD) 在处理诺伊曼边界条件（涉及导数）时不可或缺的作用。",
            "id": "4235901",
            "problem": "一个稳态导电场的赛博物理系统数字孪生由拉普拉斯方程建模，该方程由通量守恒、恒定电导率和无源的条件推导得出。设物理势为 $u:\\Omega \\to \\mathbb{R}$，其中 $\\Omega \\subset \\mathbb{R}^{d}$ 是一个具有 Lipschitz 边界 $\\partial \\Omega$ 的有界域。其控制律为 $-\\Delta u = 0$ in $\\Omega$，带有混合边界条件 $u=g$ on $\\Gamma_{D} \\subset \\partial \\Omega$ 和 $\\partial_{n} u = h$ on $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$，其中 $g:\\Gamma_{D} \\to \\mathbb{R}$ 和 $h:\\Gamma_{N} \\to \\mathbb{R}$ 由边界传感器测量，$\\partial_{n} u$ 表示法向导数。\n\n您部署了一个带有参数 $\\theta$ 的物理信息神经网络 (PINN) $u_{\\theta}:\\Omega \\to \\mathbb{R}$，作为数字孪生代理模型。为了进行边界训练，您在每个边界段上收集求积样本点：\n- Dirichlet 样本点 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，及其相关的测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$ 和用以近似在 $\\Gamma_{D}$ 上积分的正求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。\n- Neumann 样本点 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，及其测得的通量 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$ 和正求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。\n\n令 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 为总求积权重。假设您选择正惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$ 来平衡 Dirichlet 和 Neumann 边界条件的施加强度。\n\n从基本定律 $-\\Delta u = 0$ 以及 Dirichlet 和 Neumann 边界条件的定义出发，分别推导在 $\\Gamma_{D}$ 和 $\\Gamma_{N}$ 上的边界残差泛函，这两个泛函分别度量 $u_{\\theta}$ 与 $g$ 之间以及 $\\nabla u_{\\theta} \\cdot n$ 与 $h$ 之间的平方不匹配度。然后，使用带有给定样本点和权重的求积近似，构建一个单一的、复合的、无量纲的纯边界损失泛函 $L_{\\mathrm{bc}}(\\theta)$，其值为两个由 $W_{D}$ 和 $W_{N}$ 归一化的边界残差的加权和。将此 $L_{\\mathrm{bc}}(\\theta)$ 明确地表示为一个闭式解析表达式，该表达式由给定数据 $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$、$\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$ 和网络 $u_{\\theta}$ 构成。在您的推导中，讨论如何使用自动微分 (AD) 来获得边界点上的 $\\nabla u_{\\theta}$，从而施加每个边界残差。\n\n您的最终答案必须是 $L_{\\mathrm{bc}}(\\theta)$ 作为 $\\theta$ 的函数的单一显式表达式，仅包含符号量和给定数据。不包括单位。无需四舍五入。",
            "solution": "该问题要求为物理势 $u$ 的物理信息神经网络 (PINN) 代理模型 $u_{\\theta}$ 推导一个复合的、无量纲的纯边界损失泛函，记为 $L_{\\mathrm{bc}}(\\theta)$。该系统由域 $\\Omega \\subset \\mathbb{R}^{d}$ 上的拉普拉斯方程 $-\\Delta u = 0$ 和混合 Dirichlet 与 Neumann 边界条件所控制。该损失泛函是根据边界上的离散传感器数据构建的。\n\n我们首先为每种类型的边界条件正式定义连续残差泛函。这些泛函度量了 PINN 代理模型 $u_{\\theta}$ 未能满足边界 $\\partial \\Omega$ 上规定条件的程度。\n\n首先，考虑 Dirichlet 边界 $\\Gamma_{D}$，其中势被规定为 $u=g$。在任意点 $x \\in \\Gamma_D$ 上的不匹配度，或称残差，由差值 $u_{\\theta}(x) - g(x)$ 给出。为了度量该边界段上的总误差，我们对该残差的平方进行积分。这便得到了连续 Dirichlet 边界残差泛函 $R_{D}(\\theta)$:\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, dS\n$$\n其中 $dS$ 是边界 $\\partial\\Omega$ 上的曲面测度。\n\n其次，考虑 Neumann 边界 $\\Gamma_{N}$，其中势的法向导数被规定为 $\\partial_{n} u = h$。法向导数是梯度在单位外法向量 $n(x)$ 上的投影，即 $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$。在点 $x \\in \\Gamma_N$ 上的残差是 PINN 近似的法向导数与规定函数 $h(x)$ 之间的差值，即 $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$。于是，Neumann 边界上的总平方误差由连续 Neumann 边界残差泛函 $R_{N}(\\theta)$ 给出：\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, dS\n$$\n\n在实际应用中，我们没有连续函数 $g$ 和 $h$，而是在特定传感器位置上的离散测量值。问题提供了样本点集和求积权重，用以近似 $R_{D}(\\theta)$ 和 $R_{N}(\\theta)$ 的积分。\n\n对于 Dirichlet 边界，我们有 $N_D$ 个样本点 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，以及对应的势测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$ (其中 $g_i = g(x_i^D)$) 和正求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。对 $R_{D}(\\theta)$ 的积分通过一个加权和来近似，我们将其表示为离散残差 $\\hat{R}_{D}(\\theta)$:\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\n这是积分 $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, dS$ 的一个数值求积近似。\n\n对于 Neumann 边界，我们有 $N_N$ 个样本点 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，以及通量测量值 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}}$ 和正求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。这里的关键计算是 $\\nabla u_{\\theta}(x_j^N)$ 这一项，即神经网络输出对其空间输入的梯度。这通过自动微分 (AD) 来计算。神经网络 $u_{\\theta}$ 是可微基本函数（例如，仿射变换和激活函数）的复合。因此，它对其输入的导数可以通过算法计算到机器精度。深度学习框架原生提供此功能，通常通过反向模式 AD（反向传播）实现，从而能够在损失计算所需的任何点 $x_j^N$ 处高效地评估 $\\nabla u_{\\theta}$。\n\n使用 AD 获得 $\\nabla u_{\\theta}(x_{j}^{N})$ 后，我们可以用离散 Neumann 残差 $\\hat{R}_{N}(\\theta)$ 来近似对 $R_{N}(\\theta)$ 的积分：\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\n问题要求一个单一的复合损失泛函 $L_{\\mathrm{bc}}(\\theta)$，它是两个边界残差的加权和，并由它们各自的总求积权重 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 进行归一化。这种归一化将每个离散残差转化为均方误差的近似。我们还被给予了正惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$，以平衡施加每个边界条件的相对重要性。\n归一化的 Dirichlet 损失项是 $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$。\n归一化的 Neumann 损失项是 $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$。\n\n问题指明最终的损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 应该是无量纲的。这通常意味着控制方程和边界条件已经事先使用长度、势等的特征尺度进行了无量纲化处理。在这个标准假设下，损失函数中的所有量都是无量纲的，惩罚系数 $\\lambda_D$ 和 $\\lambda_N$ 也是用于调整训练过程的无量纲因子。\n\n综合这些部分，复合边界损失泛函是归一化和加权的离散残差之和：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\n代入 $\\hat{R}_{D}(\\theta)$、$\\hat{R}_{N}(\\theta)$、$W_{D}$ 和 $W_{N}$ 的表达式：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\n使用给定的定义 $W_{D}$ 和 $W_{N}$，可以更简洁地写成：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n此表达式代表了纯边界损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 的最终闭式解析形式，它是网络参数 $\\theta$ 和给定数据的函数。通过最小化此损失泛函（相对于 $\\theta$）来训练网络，使其在最小二乘意义上满足边界条件。",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        },
        {
            "introduction": "构建一个形式正确的损失函数只是第一步，确保神经网络能够被有效训练同样至关重要。这个练习引入了无量纲化——一种源自经典物理学和工程学的强大技术，并阐释了它为何对 PINN 的训练至关重要。通过平衡偏微分方程中各项的尺度，无量纲化有助于创建一个更稳定的优化问题，从而实现更快、更可靠的收敛，这是将 PINN 应用于复杂系统数字孪生的关键实践技能。",
            "id": "4235875",
            "problem": "一个信息物理系统中的一维粘性输运过程的数字孪生由粘性 Burgers 方程建模，该方程在物理层面上的形式为 $u_{t} + u u_{x} - \\nu u_{xx} = 0$，定义在一个长度为 $L$ 的空间区间和感兴趣的时间范围内。将通过在配置点上强制执行偏微分方程残差以及初始和边界数据来训练一个物理信息神经网络 (PINN)。为促进在不同工作状态下的鲁棒优化，您需要对该模型进行无量纲化。\n\n从物理方程以及空间、时间和速度大小的特征尺度的核心定义出发，通过设置 $x = L \\,\\xi$、$t = T \\,\\tau$ 和 $u = U \\,v$ 来引入无量纲变量，其中 $L$ 和 $U$ 是固定的特征长度和速度尺度，$T$ 是一个待定的特征时间尺度。仅使用这些缩放关系和链式法则，推导关于 $v(\\xi,\\tau)$ 的方程的无量纲形式。选择 $T$ 使得无量纲方程的时间导数项和对流非线性项的系数均为一阶（数量级为1）。确定乘以扩散项的、控制对流与扩散之间平衡的单一无量纲群。\n\n然后，利用关于基于梯度的优化和条件数的基本原理，简要论证为什么在数字孪生中实现时，对无量纲形式的模型训练物理信息神经网络（PINN）可以改善优化的稳定性和收敛性。\n\n您最终报告的答案必须是与此次无量纲化相关的雷诺数的解析表达式，用 $U$、$L$ 和 $\\nu$ 表示。不要包含单位。如果您选择进行任何中间数值计算，请不要报告它们。最终答案必须是单一的闭式表达式。",
            "solution": "该问题要求对一维粘性 Burgers 方程进行无量纲化，分析得到的无量纲群，并论证为何此过程有益于物理信息神经网络（PINN）的训练。\n\n粘性 Burgers 方程的物理形式如下：\n$$\nu_{t} + u u_{x} - \\nu u_{xx} = 0\n$$\n其中 $u(x,t)$ 是速度，$\\nu$ 是运动粘度，$u_t = \\frac{\\partial u}{\\partial t}$，$u_x = \\frac{\\partial u}{\\partial x}$，以及 $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$。\n\n问题指定通过以下缩放关系引入无量纲变量 $\\xi$、$\\tau$ 和 $v$：\n$$\nx = L \\,\\xi, \\quad t = T \\,\\tau, \\quad u = U \\,v\n$$\n此处，$L$ 是特征长度尺度，$U$ 是特征速度尺度。特征时间尺度 $T$ 待定。函数 $v$ 是无量纲速度，它依赖于无量纲空间和时间变量，即 $v = v(\\xi, \\tau)$。\n\n为了变换该偏微分方程（PDE），我们必须将 $u$ 对 $x$ 和 $t$ 的偏导数用 $v$ 对 $\\xi$ 和 $\\tau$ 的偏导数来表示。我们使用链式法则进行此变换。\n\n首先，我们求时间导数 $u_t$:\n$$\nu_{t} = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (Uv)}{\\partial t} = U \\frac{\\partial v}{\\partial t} = U \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}\n$$\n根据关系 $t = T\\tau$，我们有 $\\frac{\\partial \\tau}{\\partial t} = \\frac{1}{T}$。因此，时间导数变为：\n$$\nu_{t} = \\frac{U}{T} \\frac{\\partial v}{\\partial \\tau} = \\frac{U}{T} v_{\\tau}\n$$\n\n接着，我们求一阶空间导数 $u_x$:\n$$\nu_{x} = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (Uv)}{\\partial x} = U \\frac{\\partial v}{\\partial x} = U \\frac{\\partial v}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x}\n$$\n根据关系 $x = L\\xi$，我们有 $\\frac{\\partial \\xi}{\\partial x} = \\frac{1}{L}$。因此，一阶空间导数为：\n$$\nu_{x} = \\frac{U}{L} \\frac{\\partial v}{\\partial \\xi} = \\frac{U}{L} v_{\\xi}\n$$\n\n最后，我们求二阶空间导数 $u_{xx}$:\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial u}{\\partial x} \\right) = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} v_{\\xi} \\right) = \\frac{U}{L} \\frac{\\partial v_{\\xi}}{\\partial x} = \\frac{U}{L} \\left( \\frac{\\partial v_{\\xi}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\right) = \\frac{U}{L} \\left( v_{\\xi\\xi} \\frac{1}{L} \\right)\n$$\n简化为：\n$$\nu_{xx} = \\frac{U}{L^2} v_{\\xi\\xi}\n$$\n\n现在，我们将这些导数和 $u$ 本身的表达式代入原始的粘性 Burgers 方程中：\n$$\n\\left( \\frac{U}{T} v_{\\tau} \\right) + (Uv) \\left( \\frac{U}{L} v_{\\xi} \\right) - \\nu \\left( \\frac{U}{L^2} v_{\\xi\\xi} \\right) = 0\n$$\n合并系数，我们得到：\n$$\n\\frac{U}{T} v_{\\tau} + \\frac{U^2}{L} v v_{\\xi} - \\frac{\\nu U}{L^2} v_{\\xi\\xi} = 0\n$$\n为了得到系数数量级为1的无量纲形式，我们可以将整个方程除以其中一个系数。一个标准的选择是除以非线性对流项的系数 $\\frac{U^2}{L}$，使其系数恰好为 $1$：\n$$\n\\left( \\frac{U}{T} \\frac{L}{U^2} \\right) v_{\\tau} + \\left( \\frac{U^2}{L} \\frac{L}{U^2} \\right) v v_{\\xi} - \\left( \\frac{\\nu U}{L^2} \\frac{L}{U^2} \\right) v_{\\xi\\xi} = 0\n$$\n简化系数可得：\n$$\n\\left( \\frac{L}{UT} \\right) v_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\n问题要求选择特征时间尺度 $T$，使得时间导数 ($v_\\tau$) 和对流非线性项 ($v v_\\xi$) 的系数均为数量级为1。我们已经将 $v v_\\xi$ 的系数设为 $1$。为了将 $v_\\tau$ 的系数也设为 $1$，我们必须有：\n$$\n\\frac{L}{UT} = 1 \\implies T = \\frac{L}{U}\n$$\n对 $T$ 的这一选择代表了对流时间尺度：即流体质点以特征速度 $U$ 行进特征长度 $L$ 所需的时间。\n\n将 $T$ 的这个表达式代回缩放后的方程，我们得到粘性 Burgers 方程的最终无量纲形式：\n$$\nv_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\n乘以扩散项 ($v_{\\xi\\xi}$) 的单一无量纲群是 $\\frac{\\nu}{UL}$。这个群是雷诺数的倒数，雷诺数是流体力学中的一个基本无量纲参数，用于量化惯性力与粘性力之比。雷诺数，记为 $Re$，定义为：\n$$\nRe = \\frac{\\text{惯性力}}{\\text{粘性力}} \\propto \\frac{U^2/L}{\\nu U/L^2} = \\frac{UL}{\\nu}\n$$\n因此，无量纲系数是 $\\frac{1}{Re}$，方程为：\n$$\nv_{\\tau} + v v_{\\xi} - \\frac{1}{Re} v_{\\xi\\xi} = 0\n$$\n所以雷诺数的表达式是 $\\frac{UL}{\\nu}$。\n\n关于为何在无量纲形式上训练 PINN 更具优势的论点，与优化问题的数值条件有关。PINN 的训练是通过最小化一个包含控制偏微分方程残差的损失函数来完成的。对于原始物理方程，PDE 残差为 $r = u_t + u u_x - \\nu u_{xx}$。损失函数通常是该残差在一组配置点上的均方误差，$\\mathcal{L}_{PDE} = \\langle (u_t + u u_x - \\nu u_{xx})^2 \\rangle$。\n\n在一个物理系统中，$u_t$、$u u_x$ 和 $\\nu u_{xx}$ 这些项的量级可能会相差好几个数量级。例如，在高速、低粘度的流动中，对流项 $u u_x$ 可能远大于粘性项 $\\nu u_{xx}$。当一个基于梯度的优化器（例如 Adam）最小化损失函数时，损失函数相对于神经网络参数（权重和偏置）的梯度将由残差中的最大项主导。这会造成一个病态的优化景观，其中包含陡峭的“峡谷”和平坦的“高原”，使得优化器难以收敛到一个好的解。神经网络可能学会满足主导的物理过程（如对流），而很大程度上忽略次主导的物理过程（如扩散），从而导致模型不准确。\n\n无量纲化起到了一种预处理的作用。通过缩放变量使得 $v, \\xi, \\tau$ 均为数量级为1，我们确保了无量纲残差 $r' = v_\\tau + v v_\\xi - \\frac{1}{Re} v_{\\xi\\xi}$ 中的各项具有可比的量级（除非 $Re$ 极大或极小，此时物理过程进入一个独特的渐近状态）。对于中等大小的 $Re$，所有系数均为数量级为1。这种项之间的平衡导致了一个条件更好的损失景观。源自每个物理项（时间变化、对流、扩散）的梯度被更均匀地缩放。因此，优化器可以在最小化所有物理效应的贡献方面取得更均等的进展，从而实现更稳定、更高效的训练，并最终得到一个更准确的基于 PINN 的数字孪生。",
            "answer": "$$\n\\boxed{\\frac{UL}{\\nu}}\n$$"
        }
    ]
}