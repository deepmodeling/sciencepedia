## 引言
在构建[数字孪生](@entry_id:171650)和理解复杂物理系统的征程中，科学家与工程师面临一个核心挑战：如何将源于第一性原理的物理模型与日益丰富的传感器数据有效融合？传统[数值模拟](@entry_id:146043)方法虽然物理上严谨，但计算成本高昂且难以处理数据不确定性；而纯数据驱动的机器学习模型虽灵活，却常因缺乏物理约束而产生不切实际的预测。物理信息神经网络（PINN）的出现，为弥合这一鸿沟提供了革命性的解决方案，它巧妙地将神经网络的强大[表示能力](@entry_id:636759)与物理定律的内在约束相结合。

本文将系统性地引导读者深入探索PINN的世界。在第一部分“原理与机制”中，我们将揭示PINN如何通过精心设计的损失函数将[偏微分](@entry_id:194612)方程编码为可训练的残差，并探讨自动微分如何成为实现这一过程的核心引擎。接着，在“应用与交叉学科联系”一章，我们将展示PINN如何超越纯粹的方程求解器，在[固体力学](@entry_id:164042)、流体力学乃至系统生物学等领域解决复杂的正、[逆问题](@entry_id:143129)。最后，通过“动手实践”环节，您将有机会将理论应用于具体问题，学习如何构建损失函数并应对实际训练中的挑战，从而真正掌握这一前沿技术。

## 原理与机制

[物理信息神经网络](@entry_id:145229)（PINN）的核心思想是利用神经网络的通用逼近能力，将其训练成一个能够满足给定物理定律（通常以[偏微分](@entry_id:194612)方程（PDE）形式描述）及其相关约束（如边界条件和初始条件）的函数代理。本章将深入探讨支撑这一范式的核心原理与关键机制，从其独特的[损失函数](@entry_id:634569)构造，到处处依赖的自动微分技术，再到更高级的变分表述和应对实际挑战的策略。

### 核心原理：作为物理编码器的[损失函数](@entry_id:634569)

与依赖大量“输入-输出”数据对的传统[监督学习](@entry_id:161081)不同，PINN 的学习过程由物理定律本身指导。这种指导是通过一个精心设计的**复合损失函数** $\mathcal{L}(\theta)$ 实现的，该函数将神经网络（以其参数 $\theta$ 为特征）的输出与所有已知的物理约束进行比较。网络的训练目标是最小化这个总损失，从而“迫使”网络参数 $\theta$ 调整，使其代表的函数 $u_\theta$ 成为控制方程的有效解。这个复合[损失函数](@entry_id:634569)通常由几个关键部分加权求和而成。

#### 物理残差损失

这是 PINN 的核心，直接将控制 PDE 编码到训练过程中。对于一个通用形式为 $\mathcal{F}(u; x, t) = 0$ 的 PDE，其**物理残差**（或称为**控制方程残差**）被定义为将网络近似解 $u_\theta(x, t)$ 代入该方程后得到的非零结果：
$$
r_\theta(x, t) = \mathcal{F}(u_\theta; x, t)
$$
理想情况下，如果 $u_\theta$ 是 PDE 的精确解，那么在求解域内的任意点 $(x,t)$，残差 $r_\theta(x, t)$ 都应为零。因此，PINN 通过在求解域内部署大量**[配置点](@entry_id:169000)**（collocation points），并最小化这些点上残差的范数（通常是均方误差），来强制执行该物理定律。

以一个[稳态扩散](@entry_id:154663)过程为例，其控制方程为泊松方程 $-\Delta u = f$，其中 $\Delta$ 是[拉普拉斯算子](@entry_id:146319)。PINN 的近似解为 $\hat{u}_\theta(\mathbf{x})$。物理残差损失 $\mathcal{L}_r$ 将被定义为在一系列内部[配置点](@entry_id:169000) $X_r = \{ \mathbf{x}_i \}_{i=1}^{N_r}$ 上残差的[均方误差](@entry_id:175403) ：
$$
\mathcal{L}_r(\theta) = \frac{1}{N_r} \sum_{i=1}^{N_r} \left( -\Delta \hat{u}_\theta(\mathbf{x}_i) - f(\mathbf{x}_i) \right)^2
$$
这个损失项将引导网络学习一个函数，使其二阶导数（通过[拉普拉斯算子](@entry_id:146319)计算）在整个域内都趋近于源项 $f$。

#### 边界与初始条件损失

物理问题除了受域内控制方程的约束外，还必须满足在时空域边界上的特定条件。PINN 通过在损失函数中加入相应的惩罚项来强制执行这些**边界条件 (BC)** 和**初始条件 (IC)**。

对于一个瞬态问题，如一维[热传导方程](@entry_id:194763) $u_t = \alpha u_{xx}$，定义在空间域 $x \in [0,1]$ 和时间域 $t \in [0,1]$ 上，我们通常有关初始温度分布 $u(x,0) = g(x)$ 和边界温度 $u(0,t) = b_0(t)$、$u(1,t) = b_1(t)$ 的信息。PINN 将通过在初始时刻和空间边界[上采样](@entry_id:275608)一系列点来构建损失项，以惩罚网络预测与这些给定条件之间的偏差 。

具体而言，初始条件损失 $\mathcal{L}_{\mathrm{ic}}$ 和边界条件损失 $\mathcal{L}_{\mathrm{bc}}$ 可以表示为：
$$
\mathcal{L}_{\mathrm{ic}}(\theta) = \frac{1}{N_{\mathrm{ic}}} \sum_{i=1}^{N_{\mathrm{ic}}} \left| u_\theta(x_{\mathrm{ic}}^{(i)}, 0) - g(x_{\mathrm{ic}}^{(i)}) \right|^2
$$
$$
\mathcal{L}_{\mathrm{bc}}(\theta) = \frac{1}{N_{\mathrm{bc}}} \sum_{k=1}^{N_{\mathrm{bc}}} \left( \left| u_\theta(0, t_{\mathrm{bc}}^{(k)}) - b_0(t_{\mathrm{bc}}^{(k)}) \right|^2 + \left| u_\theta(1, t_{\mathrm{bc}}^{(k)}) - b_1(t_{\mathrm{bc}}^{(k)}) \right|^2 \right)
$$
其中 $\{x_{\mathrm{ic}}^{(i)}\}$ 和 $\{t_{\mathrm{bc}}^{(k)}\}$ 分别是在初始边界和空间边界[上采样](@entry_id:275608)的点集。这些项确保了学习到的解在时空域的边缘处是“锚定”的。

#### 数据保真度损失

在[数字孪生](@entry_id:171650)和许多工程应用中，我们可能拥有来自物理系统传感器的稀疏、带噪声的测量数据。PINN 框架能够无缝地将这些数据融入训练过程，这一过程称为**数据同化**。这是通过添加一个**数据保真度损失项** $\mathcal{L}_d$ 来实现的，该损失项惩罚网络预测值与测量值之间的差异。

假设有一组测量数据 $\mathcal{D} = \{(x_m, t_m, y_m)\}_{m=1}^{N_d}$，其中 $y_m$ 是在点 $(x_m, t_m)$ 处对真实解 $u$ 的观测值。数据保真度损失通常被定义为[均方误差](@entry_id:175403)  ：
$$
\mathcal{L}_d(\theta) = \frac{1}{N_d} \sum_{m=1}^{N_d} \left| u_\theta(x_m, t_m) - y_m \right|^2
$$
这个损失项使得 PINN 不仅是一个 PDE 求解器，还是一个能够融合理论模型和实际观测的数据驱动模型，非常适合用于求解逆问题或校准[数字孪生](@entry_id:171650)模型。

最终，总的复合[损失函数](@entry_id:634569)是上述各项的加权和：
$$
\mathcal{L}(\theta) = \lambda_r \mathcal{L}_r(\theta) + \lambda_{\mathrm{ic}} \mathcal{L}_{\mathrm{ic}}(\theta) + \lambda_{\mathrm{bc}} \mathcal{L}_{\mathrm{bc}}(\theta) + \lambda_d \mathcal{L}_d(\theta)
$$
其中，$\lambda$ 系数是超参数，用于平衡不同物理约束的重要性。通过最小化 $\mathcal{L}(\theta)$，PINN 在一个由神经网络参数构成的巨大函数空间中搜索一个能够同时满足控制方程、边界/初始条件以及所有可用测量数据的最优解。

### 核心机制：[自动微分](@entry_id:144512)

从上述[损失函数](@entry_id:634569)的定义中可以明显看出，计算物理残差项需要获得神经网络输出 $u_\theta$ 关于其输入（如空间坐标 $x$ 和时间 $t$）的[偏导数](@entry_id:146280)，例如 $u_t$、$u_x$、$u_{xx}$ 等。**自动微分 (Automatic Differentiation, AD)** 是实现这一目标的关键技术，也是 PINN 得以实现的核心引擎。AD 能够在不进行符号推导或使用有损精度的[有限差分近似](@entry_id:1124978)的情况下，精确地计算复杂函数（如神经网络）的导数。

AD 主要有两种模式：前向模式和反向模式。在神经网络的背景下，反向模式 AD（即众所周知的**[反向传播](@entry_id:199535)**算法）尤为高效。

#### 一阶导数的计算

对于一个标量输出的神经网络 $u_\theta(x, t)$，其关于输入的[梯度向量](@entry_id:141180) $\nabla u_\theta = [\partial u_\theta / \partial x, \partial u_\theta / \partial t]^\top$ 可以通过一次反向模式 AD 高效计算得出。这个过程包括：
1.  **[前向传播](@entry_id:193086)**：在给定的输入点 $(x, t)$ 处，计算并存储网络每一层的激活值。
2.  **[反向传播](@entry_id:199535)**：从最终输出层开始，利用[链式法则](@entry_id:190743)将输出对各层参数和激活值的敏感度（称为“伴随变量”或“梯度”）逐层向后传递，直至输入层。

一次完整的反向传播后，我们便能得到输出 $u_\theta$ 关于输入 $x$ 和 $t$ 的所有一阶偏导数。

#### [高阶导数](@entry_id:140882)的计算：Hessian向量积

许多重要的 PDE，如[热传导方程](@entry_id:194763)和泊松方程，都涉及二阶导数。直接计算完整的 Hessian 矩阵（所有[二阶偏导数](@entry_id:635213)构成的矩阵）成本高昂。幸运的是，我们通常只需要其中的特定项，如[拉普拉斯算子](@entry_id:146319) $\Delta u = u_{xx} + u_{yy}$。这些项可以通过更高效的技巧获得，而无需计算整个 Hessian 矩阵。

一种强大的技术是计算 **Hessian-[向量积](@entry_id:156672) (Hessian-vector product, HVP)**。例如，要计算 $u_{xx}$，我们可以利用这样一个事实：它是 Hessian 矩阵与[单位向量](@entry_id:165907) $v = [1, 0]^\top$ 的乘积 $H v$ 的第一个分量。这个 HVP 可以通过在计算一阶梯度 $\nabla u_\theta$ 的反向传播图上再进行一[次微分](@entry_id:175641)来高效求得。具体而言，这通常通过一种称为“前向-反向模式” (forward-over-reverse) 的 AD 组合实现 ：
1.  首先，执行一次标准的**前向传播**和**反向传播**，计算并存储所有中间激活值 $a^l$ 和[伴随变量](@entry_id:1123110) $\delta^l$。这给出了梯度 $\nabla u_\theta$。
2.  然后，在整个[计算图](@entry_id:636350)上执行一次**[切线](@entry_id:268870)前向传播 (tangent forward pass)**。这次传播以[方向向量](@entry_id:169562) $v$（例如，用于 $u_{xx}$ 的 $[1, 0]^\top$）为种子，计算所有中间变量在 $v$ 方向上的[方向导数](@entry_id:189133)（“[切线](@entry_id:268870)变量” $\dot{a}^l, \dot{z}^l$）。
3.  接着，执行一次**[切线](@entry_id:268870)[反向传播](@entry_id:199535) (tangent reverse pass)**，计算[伴随变量](@entry_id:1123110)自身的切线变量 $\dot{\delta}^l$。
4.  最终，HVP 就是通过与标准梯度计算类似的最终步骤得到的 $\dot{g} = (W^1)^\top \dot{\delta}^1$。其分量即为所需的二阶导数。

这个过程虽然听起来复杂，但现代 AD 框架（如 TensorFlow, PyTorch, JAX）已经将其自动化，使得研究者可以像请求一阶导数一样方便地请求[高阶导数](@entry_id:140882)。

#### 计算[复杂度分析](@entry_id:634248)

理解 AD 的计算成本至关重要。对于一个具有 $P$ 个参数的神经网络，在 $N$ 个[配置点](@entry_id:169000)上进行评估：
-   **[前向传播](@entry_id:193086)**的时间和内存复杂度均为 $O(NP)$，因为需要为每个点计算并存储所有层的激活值。
-   **[反向传播](@entry_id:199535)**（用于计算一阶导数）的[时间复杂度](@entry_id:145062)也是 $O(NP)$。
-   计算二阶导数（通过 HVP 等方法）通常需要额外的一轮或几轮传播，但每一轮的成本仍然是 $O(NP)$。因此，计算所有需要的一阶和二阶导数的总[时间复杂度](@entry_id:145062)仍然是 $O(NP)$，只是常数因子有所增加 。

这个线性扩展性是 PINN 能够处理大规模问题的关键。与网格尺寸呈[多项式增长](@entry_id:177086)的传统方法相比，PINN 的计算成本主要由网络大小和[配置点](@entry_id:169000)数量决定。

### 高级建模：[强形式与弱形式](@entry_id:1132543)

前面讨论的基于逐点评估 PDE 残差的[损失函数](@entry_id:634569)，被称为**强形式**方法。这是 PINN 最常见的形式，但并非唯一形式。在[数值分析](@entry_id:142637)领域，许多问题，特别是那些解的正则性（光滑度）较低的问题，通过**[弱形式](@entry_id:142897)**或**[变分形式](@entry_id:166033)**来处理更为有效。PINN 同样可以采用这种思想。

#### 强形式残差

强形式 PINN 直接最小化 PDE 在[配置点](@entry_id:169000)上的残差。例如，对于线弹性问题中的[动量平衡](@entry_id:1128118)方程 $\nabla \cdot \boldsymbol{\sigma} + \boldsymbol{b} = \boldsymbol{0}$，由于应力 $\boldsymbol{\sigma}$ 是应变 $\boldsymbol{\varepsilon}(\boldsymbol{u})$ 的函数，而应变又是位移 $\boldsymbol{u}$ 的[一阶导数](@entry_id:749425)，因此该方程是一个二阶 PDE。在强形式中，这意味着损失函数需要计算[位移场](@entry_id:141476) $\boldsymbol{u}_\theta$ 的二阶空间导数。这隐含地要求网络能够逼近一个至少二次可微的函数（在[索博列夫空间](@entry_id:141995)中，对应于 $H^2$ 正则性）。

**优点**：
-   实现简单直观。
-   计算成本相对较低，因为它避免了[数值积分](@entry_id:136578)，只需在离散点上进行评估。
-   对于解足够光滑的问题非常有效 。

**缺点**：
-   对解的光滑度要求较高。

#### [弱形式](@entry_id:142897)（变分）残差

弱形式通过将 PDE 乘以一个**[测试函数](@entry_id:166589)** $\varphi$ 并在整个域 $\Omega$ 上积分来导出。然后，利用**分部积分**（或高维的[格林公式](@entry_id:173118)）将一部分微分算子从待求解 $u$ 转移到测试函数 $\varphi$ 上。

以泊松方程 $-\Delta u = f$ 为例，其弱形式旨在寻找一个解 $u$，使得对于所有满足适当边界条件的测试函数 $\varphi$，以下积分等式成立 ：
$$
\int_{\Omega} \nabla u \cdot \nabla \varphi \, dx = \int_{\Omega} f \varphi \, dx
$$
可以看到，原始方程中的二阶导数 $\Delta u$ 消失了，取而代之的是解 $u$ 和[测试函数](@entry_id:166589) $\varphi$ 的一阶导数。这种形式只需要解具有一阶[弱导数](@entry_id:189356)（即 $H^1$ 正则性），大大降低了对解的光滑度要求。

**优点**：
-   **正则性要求更低**：这是最核心的优势。对于解中存在[奇点](@entry_id:266699)（如[裂纹尖端](@entry_id:182807)的应力集中 ）或在非凸域上导致解的正则性降低（$u \notin H^2$）的问题，弱形式依然是良定义的，而强形式残差可能趋于无穷大 。
-   **自然地处理边界条件**：特别是诺伊曼（Neumann）边界条件（如指定的[表面力](@entry_id:188034)或热通量），可以通过[分部积分](@entry_id:136350)自然地融入到积分项中，能够稳健地处理粗糙的边界数据 。
-   **与经典方法的联系**：弱形式将 PINN 与有限元法（FEM）等经典的[变分方法](@entry_id:163656)紧密联系起来，可以借鉴其丰富的理论，如关于稳定性和收敛性的分析 。

**缺点**：
-   实现更复杂，需要选择合适的[测试函数](@entry_id:166589)空间。
-   计算成本更高，因为它涉及到在域和边界上的[数值积分](@entry_id:136578)，这通常需要比强形式[配置点](@entry_id:169000)更多的求值点（即求积点）。

总而言之，强形式和[弱形式](@entry_id:142897)的选择是一个权衡。对于解光滑且几何简单的“友好”问题，强形式因其简单高效而备受青睐。而对于具有挑战性的工程问题，如[断裂力学](@entry_id:141480)或涉及复杂几何与材料界面的问题，[弱形式](@entry_id:142897)因其更低的正则性要求和更强的数学鲁棒性而展现出巨大潜力。

### 理论基础与实践挑战

PINN 的成功不仅依赖于其巧妙的构造，还植根于[深度学习](@entry_id:142022)的理论基础，同时也面临着一些固有的实践挑战。

#### 逼近理论：为什么神经网络可以[求解PDE](@entry_id:138485)？

一个根本性的问题是：为什么我们相信一个由[激活函数](@entry_id:141784)和[线性变换](@entry_id:149133)构成的神经网络能够表示一个复杂 PDE 的解？答案部分在于**通用逼近定理**。该定理表明，具有足够宽度的单层神经网络可以以任意精度逼近任何[连续函数](@entry_id:137361)。

然而，PDE 的解通常需要满足比连续性更强的光滑性条件，例如存在[弱导数](@entry_id:189356)。幸运的是，逼近理论已经扩展到**[索博列夫空间](@entry_id:141995)**。研究表明，对于有界[利普希茨域](@entry_id:751354) $\Omega$ 上的函数，基于 ReLU [激活函数](@entry_id:141784)的神经网络是 $W^{1,p}(\Omega)$ 空间中的**稠密**集。这意味着，对于任何属于该空间的函数 $u$（包括许多二阶椭圆 PDE 的[弱解](@entry_id:161732)），总能找到一个 ReLU 网络 $u_\theta$ 使其在 $W^{1,p}$ 范数下任意接近 $u$。这种逼近能力确保了 PINN 的解空间足够丰富，原则上可以包含真实解的良好近似 。只要 PDE 问题是适定的（即解存在、唯一且稳定），并且训练能够成功地最小化损失，那么 PINN 就有望收敛到真实解。

#### 谱偏差：高频解的学习难题

尽管神经网络具有强大的理论逼近能力，但在实践中，标准的[梯度下降](@entry_id:145942)训练方法存在一种被称为**谱偏差 (spectral bias)** 的固有倾向。即，神经网络在训练初期会优先学习[目标函数](@entry_id:267263)的**低频分量**，而学习高频分量的速度则要慢得多 。

这种现象在求解许多类型的 PDE 时会成为一个严重的障碍。例如，在**平流主导 (advection-dominated)** 的输运问题中，解可能包含随时间移动的**尖锐锋面**或**边界层**。这些局部化的剧变在傅里叶空间中对应着大量的**高频（或高波数）**分量。由于谱偏差，PINN 在学习这些高频特征时会异常困难，常常导致解被[过度平滑](@entry_id:634349)，无法捕捉到锋面的陡峭程度，或者收敛速度极慢 。

#### 谱偏差的缓解策略

为了克服谱偏差，研究者们开发了多种策略，其核心思想是帮助网络更容易地“看到”和“构建”高频函数。

一种非常有效的技术是**傅里叶特征嵌入 (Fourier feature embedding)**。其思想不是直接将原始坐标 $(x, t)$ 输入网络，而是先通过一个[非线性映射](@entry_id:272931)，将它们转换为包含多种频率的正弦和余弦特征。例如，一个改进的输入映射 $g(x, t)$ 可以是：
$$
g(x, t) = [x, t, \sin(\omega_1 x), \cos(\omega_1 x), \dots, \sin(\omega_M x), \cos(\omega_M x)]
$$
通过这种方式，高频函数（如 $\sin(\omega_M x)$）不再需要网络通过多层[非线性变换](@entry_id:636115)来“费力地”合成，而是直接作为输入特征提供给第一层。网络只需学习这些特征的线性组合，便能轻松地构造出高频输出。

更进一步，这种嵌入可以与物理知识相结合。对于线性平流方程 $u_t + c u_x = 0$，其解的形式为 $f(x-ct)$，即所有信息都沿着[特征坐标](@entry_id:166542) $s = x - ct$ 传播。因此，一个物理对齐的傅里叶特征嵌入应该沿着这个[特征坐标](@entry_id:166542)来构建 ：
$$
g(x,t) = [x, t, \sin(2\pi \omega_1 s), \cos(2\pi \omega_1 s), \dots, \sin(2\pi \omega_M s), \cos(2\pi \omega_M s)]
$$
这种方法将物理洞察（特征线）与架构设计（输入嵌入）相结合，极大地提升了 PINN 解决[平流主导问题](@entry_id:746320)的能力。

另一种相关的方法是对 PDE 本身进行**[坐标变换](@entry_id:172727)**。通过切换到[特征坐标](@entry_id:166542)系（如令 $y=x-ct, \tau=t$），可以将[平流-扩散方程](@entry_id:746317)简化为一个纯粹的[扩散方程](@entry_id:170713) $\partial_\tau u = \nu \partial_{yy} u$ 。在这个新坐标系中，PINN 需要学习的函数不再是移动的高频波，而是一个空间上静态、振幅随时间衰减的波，这大大降低了学习的难度。

### 宏观视角：PINN 与[算子学习](@entry_id:752958)

最后，为了全面理解 PINN，有必要将其置于更广阔的[科学机器学习](@entry_id:145555)领域中，并与另一大主流范式——**[算子学习](@entry_id:752958) (operator learning)** 进行对比。

#### 单实例求解 vs. 算子逼近

这两种方法的核心区别在于它们学习的目标不同 。

-   **PINN：单实例求解器**。标准的 PINN 旨在求解一个**特定实例**的 PDE 问题。例如，对于泊松方程 $-\Delta u = f$，给定一个**固定**的源项 $f$，PINN 通过优化找到对应的那个解 $u$。如果源项 $f$ 改变为 $f'$，则必须重新运行整个训练过程来求解新的问题。PINN 的输入是时空坐标 $(x,t)$，输出是解在这些坐标上的值 $u(x,t)$。

-   **[算子学习](@entry_id:752958)：算子逼近器**。像[深度算子网络](@entry_id:748262) ([DeepONet](@entry_id:748262)) 或[傅里叶神经算子](@entry_id:189138) (Fourier Neural Operator, FNO) 这样的模型，其目标是学习 PDE 的**解算子** $\mathcal{G}$ 本身。解算子是一个映射，它将输入函数（如源项 $f$ 或边界条件 $g$）映射到输出函数（解 $u$），即 $\mathcal{G}: f \mapsto u$。[算子学习](@entry_id:752958)模型的训练通常需要一个由许多“输入函数-输出函数”对 $\{(f_i, u_i)\}$ 组成的数据集。一旦训练完成，该模型就成了一个通用的、快速的代理求解器：对于一个全新的输入函数 $f_{new}$，只需一次[前向传播](@entry_id:193086)，即可立即获得近似解 $u_{new}$。

#### 应用场景与权衡

-   **PINN 的优势**在于其灵活性和“零数据”（指无需提前获得解的样本）的特性。它特别适用于：
    -   求解定义在复杂几何域上的单个、困难的 PDE 问题。
    -   **[逆问题](@entry_id:143129)**：当 PDE 的某些参数（如材料属性 $\alpha$）未知时，可以将其作为网络的可训练参数，与解 $u_\theta$ 一同从稀疏的测量数据中推断出来。
    -   无需大量预计算或实验数据来构建[训练集](@entry_id:636396)。

-   **[算子学习](@entry_id:752958)的优势**在于其**[摊销推断](@entry_id:1120981) (amortized inference)** 的效率。它适用于：
    -   需要对[参数化](@entry_id:265163) PDE 进行大量、快速、重复求解的场景，例如[设计优化](@entry_id:748326)、[不确定性量化](@entry_id:138597)或[实时控制](@entry_id:754131)。
    -   高昂的离线训练成本可以通过极快的在线查询速度来弥补。

在处理变化的边界条件时，两者的差异也很明显。[算子学习](@entry_id:752958)需要将变化的边界条件函数 $g$ 作为算子的一个输入，即学习 $\mathcal{G}: (f, g) \mapsto u$。而单实例 PINN 则是将特定的边界条件 $g$ 硬编码到该次训练的损失函数中 。

综上所述，PINN 和[算子学习](@entry_id:752958)并非相互竞争，而是互为补充的工具。PINN 像一个精密的“工匠”，为每一个独特问题量身打造解决方案；而神经算子则像一个“工厂”，为一类问题提供大规模、[标准化](@entry_id:637219)的快速求解能力。在构建复杂的数字孪生系统时，这两种技术往往可以协同工作，分别应对不同的建模与预测需求。