{
    "hands_on_practices": [
        {
            "introduction": "在将物理学启发的神经网络（PINN）应用于现实世界的物理系统时，不同变量的尺度可能存在巨大差异。本练习  探讨了无量纲化，这是科学计算中的一项关键技术，并将其应用于伯格斯方程。通过平衡偏微分方程中各项的量级，您将看到这个预处理步骤如何显著改善优化问题的条件，从而使神经网络的训练更加稳定和高效。",
            "id": "4235875",
            "problem": "一个信息物理系统中的一维粘性输运过程的数字孪生由粘性 Burgers 方程建模，其物理形式在长度为 $L$ 的空间区间和所关注的时间范围内表示为 $u_{t} + u u_{x} - \\nu u_{xx} = 0$。将通过在配置点上强制执行偏微分方程残差以及初始和边界数据，来训练一个基于此模型的物理信息神经网络 (PINN)。为了促进在不同运行工况下的鲁棒优化，您需要对该模型进行无量纲化。\n\n从物理方程以及空间、时间和速度大小的特征尺度的核心定义出发，通过设置 $x = L \\,\\xi$、$t = T \\,\\tau$ 和 $u = U \\,v$ 来引入无量纲变量，其中 $L$ 和 $U$ 是固定的特征长度和速度尺度，$T$ 是一个待定的特征时间尺度。仅使用这些尺度变换和链式法则，推导关于 $v(\\xi,\\tau)$ 的方程的无量纲形式。选择 $T$ 使得无量纲方程中的时间导数项和对流非线性项的系数均为一量级。确定乘以扩散项的、控制对流与扩散之间平衡的单一无量纲数群。\n\n然后，运用关于基于梯度的优化和条件数的基本原理，简要论证为什么在数字孪生中实现时，基于无量纲形式训练物理信息神经网络 (PINN) 可以改善优化的稳定性和收敛性。\n\n您最终报告的答案必须是与此无量纲化相关的雷诺数的解析表达式，用 $U$、$L$ 和 $\\nu$ 表示。不要包含单位。如果您选择进行任何中间数值计算，请不要报告它们。最终答案必须是单一的闭式表达式。",
            "solution": "该问题要求对一维粘性 Burgers 方程进行无量纲化，分析得到的无量纲数群，并论证为何此过程有益于物理信息神经网络 (PINN) 的训练。\n\n粘性 Burgers 方程的物理形式如下：\n$$\nu_{t} + u u_{x} - \\nu u_{xx} = 0\n$$\n其中 $u(x,t)$ 是速度，$\\nu$ 是运动粘度，$u_t = \\frac{\\partial u}{\\partial t}$，$u_x = \\frac{\\partial u}{\\partial x}$，以及 $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$。\n\n问题指定通过以下尺度关系引入无量纲变量 $\\xi$、$\\tau$ 和 $v$：\n$$\nx = L \\,\\xi, \\quad t = T \\,\\tau, \\quad u = U \\,v\n$$\n此处，$L$ 是一个特征长度尺度，$U$ 是一个特征速度尺度。特征时间尺度 $T$ 待定。函数 $v$ 是无量纲速度，它依赖于无量纲空间和时间变量，即 $v = v(\\xi, \\tau)$。\n\n为了变换该偏微分方程 (PDE)，我们必须将 $u$ 对 $x$ 和 $t$ 的偏导数用 $v$ 对 $\\xi$ 和 $\\tau$ 的偏导数来表示。我们使用链式法则进行此变换。\n\n首先，我们求时间导数 $u_t$：\n$$\nu_{t} = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (Uv)}{\\partial t} = U \\frac{\\partial v}{\\partial t} = U \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}\n$$\n根据关系式 $t = T\\tau$，我们有 $\\frac{\\partial \\tau}{\\partial t} = \\frac{1}{T}$。因此，时间导数变为：\n$$\nu_{t} = \\frac{U}{T} \\frac{\\partial v}{\\partial \\tau} = \\frac{U}{T} v_{\\tau}\n$$\n\n接下来，我们求一阶空间导数 $u_x$：\n$$\nu_{x} = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (Uv)}{\\partial x} = U \\frac{\\partial v}{\\partial x} = U \\frac{\\partial v}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x}\n$$\n根据关系式 $x = L\\xi$，我们有 $\\frac{\\partial \\xi}{\\partial x} = \\frac{1}{L}$。因此，一阶空间导数为：\n$$\nu_{x} = \\frac{U}{L} \\frac{\\partial v}{\\partial \\xi} = \\frac{U}{L} v_{\\xi}\n$$\n\n最后，我们求二阶空间导数 $u_{xx}$：\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial u}{\\partial x} \\right) = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} v_{\\xi} \\right) = \\frac{U}{L} \\frac{\\partial v_{\\xi}}{\\partial x} = \\frac{U}{L} \\left( \\frac{\\partial v_{\\xi}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\right) = \\frac{U}{L} \\left( v_{\\xi\\xi} \\frac{1}{L} \\right)\n$$\n这可以简化为：\n$$\nu_{xx} = \\frac{U}{L^2} v_{\\xi\\xi}\n$$\n\n现在，我们将这些导数表达式以及 $u$ 本身代入原始的粘性 Burgers 方程：\n$$\n\\left( \\frac{U}{T} v_{\\tau} \\right) + (Uv) \\left( \\frac{U}{L} v_{\\xi} \\right) - \\nu \\left( \\frac{U}{L^2} v_{\\xi\\xi} \\right) = 0\n$$\n合并系数，我们得到：\n$$\n\\frac{U}{T} v_{\\tau} + \\frac{U^2}{L} v v_{\\xi} - \\frac{\\nu U}{L^2} v_{\\xi\\xi} = 0\n$$\n为了得到系数为一量级的无量纲形式，我们可以将整个方程除以其中一个系数。一个标准的选择是除以非线性对流项的系数 $\\frac{U^2}{L}$，使其系数恰好为 $1$：\n$$\n\\left( \\frac{U}{T} \\frac{L}{U^2} \\right) v_{\\tau} + \\left( \\frac{U^2}{L} \\frac{L}{U^2} \\right) v v_{\\xi} - \\left( \\frac{\\nu U}{L^2} \\frac{L}{U^2} \\right) v_{\\xi\\xi} = 0\n$$\n简化系数可得：\n$$\n\\left( \\frac{L}{UT} \\right) v_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\n问题要求选择特征时间尺度 $T$，使得时间导数 ($v_\\tau$) 和对流非线性项 ($v v_\\xi$) 的系数均为一量级。我们已经将 $v v_\\xi$ 的系数设为 1。为了也将 $v_\\tau$ 的系数设为 1，我们必须有：\n$$\n\\frac{L}{UT} = 1 \\implies T = \\frac{L}{U}\n$$\n这个对 $T$ 的选择代表了对流时间尺度：即流体质点以特征速度 $U$ 行进特征长度 $L$ 所需的时间。\n\n将 $T$ 的这个表达式代回尺度变换后的方程，我们得到粘性 Burgers 方程的最终无量纲形式：\n$$\nv_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\n乘以扩散项 ($v_{\\xi\\xi}$) 的单一无量纲数群是 $\\frac{\\nu}{UL}$。这个数群是雷诺数的倒数，雷诺数是流体力学中的一个基本无量纲参数，它量化了惯性力与粘性力之比。雷诺数，记作 $Re$，定义为：\n$$\nRe = \\frac{\\text{惯性力}}{\\text{粘性力}} \\propto \\frac{U^2/L}{\\nu U/L^2} = \\frac{UL}{\\nu}\n$$\n因此，无量纲系数是 $\\frac{1}{Re}$，方程为：\n$$\nv_{\\tau} + v v_{\\xi} - \\frac{1}{Re} v_{\\xi\\xi} = 0\n$$\n所以，雷诺数的表达式为 $\\frac{UL}{\\nu}$。\n\n关于为何在无量纲形式上训练 PINN 更具优势的论证，与优化问题的数值条件有关。PINN 的训练是通过最小化一个包含控制偏微分方程残差的损失函数来完成的。对于原始物理方程，PDE 残差为 $r = u_t + u u_x - \\nu u_{xx}$。损失函数通常是该残差在一组配置点上的均方误差，$\\mathcal{L}_{PDE} = \\langle (u_t + u u_x - \\nu u_{xx})^2 \\rangle$。\n\n在一个物理系统中，$u_t$、$u u_x$ 和 $\\nu u_{xx}$ 这些项的量级可能会相差好几个数量级。例如，在高速、低粘度的流动中，对流项 $u u_x$ 可能远大于粘性项 $\\nu u_{xx}$。当一个基于梯度的优化器（例如 Adam）最小化损失函数时，损失函数相对于神经网络参数（权重和偏置）的梯度将由残差中最大的项主导。这会产生一个病态的优化景观，其特征是陡峭的“峡谷”和平坦的“高原”，使得优化器难以收敛到一个好的解。神经网络可能学会满足主导的物理过程（如对流），而很大程度上忽略次主导的物理过程（如扩散），从而导致模型不准确。\n\n无量纲化起到一种预处理的作用。通过缩放变量使得 $v, \\xi, \\tau$ 均为一量级，我们确保了无量纲残差 $r' = v_\\tau + v v_\\xi - \\frac{1}{Re} v_{\\xi\\xi}$ 中的各项具有可比的量级（除非 $Re$ 极大或极小，此时物理过程进入一个独特的渐近区域）。对于中等大小的 $Re$，所有系数都是一量级的。这种项之间的平衡会带来一个条件更好的损失景观。源于每个物理项（时间变化、对流、扩散）的梯度被更均匀地缩放。因此，优化器可以在最小化所有物理效应的贡献方面取得更均衡的进展，从而实现更稳定、更高效的训练，并最终得到一个更准确的基于 PINN 的数字孪生。",
            "answer": "$$\n\\boxed{\\frac{UL}{\\nu}}\n$$"
        },
        {
            "introduction": "PINN 的核心思想是将物理定律直接嵌入到神经网络的损失函数中。本练习  将引导您从第一性原理出发，推导二维线弹性问题的控制方程——纳维-柯西方程。接着，您将把这些方程转化为 PINN 需要最小化的精确残差形式，从而在基本连续介质力学和神经网络的训练目标之间建立起具体的联系。",
            "id": "2668927",
            "problem": "考虑一个二维、小应变、线性弹性固体，在准静态设定下，其单位体积体力为 $\\mathbf{b} = (b_x, b_y)$。设位移场为 $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$。仅从线性动量守恒、小应变运动学关系以及用拉梅参数 $(\\lambda, \\mu)$ 表示的线性各向同性本构律出发，完成以下任务：\n\n1) 写出柯西应力张量 $\\boldsymbol{\\sigma}$ 关于应变张量 $\\boldsymbol{\\varepsilon}$ 和 $(\\lambda,\\mu)$ 的本构律，并用位移场定义小应变张量。所有物理量均用笛卡尔坐标表示。\n\n2) 使用您的定义，将物理信息神经网络 (PINN) 会在内部配置点上强制执行的内部平衡残差分量，即 $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$ 的两个分量，用 $u_x$ 和 $u_y$ 的空间导数以及拉梅参数 $(\\lambda,\\mu)$ 在笛卡尔坐标系下显式展开。\n\n3) 现在考虑一个用于位移场的单隐藏神经元神经网络拟设，其激活函数为双曲正切函数，\n$$\nz(x,y) = w_1 x + w_2 y + b_1,\\quad \\mathbf{u}(x,y) = \\mathbf{W}_2 \\,\\tanh\\!\\big(z(x,y)\\big) + \\mathbf{b}_2,\n$$\n其中 $\\mathbf{W}_2 \\in \\mathbb{R}^{2 \\times 1}$ 且 $\\mathbf{b}_2 \\in \\mathbb{R}^{2}$。取特定参数\n$$\nw_1 = 1,\\quad w_2 = 2,\\quad b_1 = 0,\\quad \\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\\quad \\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\n并假设体力为零 $\\mathbf{b}=\\mathbf{0}$。计算内部平衡残差向量在点 $(x_0,y_0)=(1,0)$ 处的两个分量。\n\n假设采用无量纲化公式，因此所有量均为无量纲。将最终结果表示为包含两个残差分量的单个行向量，写成闭式解析表达式。不要近似或四舍五入任何常数；保留双曲函数不进行计算（例如，明确写出 $\\tanh(1)$, $\\cosh(1)$）。",
            "solution": "该问题经验证是适定的、有科学依据的，并包含获得唯一解所需的所有信息。我们着手进行推导。\n\n该问题要求从连续介质力学的基本原理出发，进行分步推导，以评估给定神经网络位移场拟设的平衡残差。\n\n首先，我们解决任务1：在笛卡尔坐标系中定义相关的运动学和本构关系。\n\n在坐标为 $(x,y)$ 的二维域中，位移场为 $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$。小应变（或无穷小应变）张量 $\\boldsymbol{\\varepsilon}$ 由位移场的梯度定义为：\n$$\n\\boldsymbol{\\varepsilon} = \\frac{1}{2} \\left[ \\nabla \\mathbf{u} + (\\nabla \\mathbf{u})^T \\right]\n$$\n其笛卡尔分量形式为：\n$$\n\\varepsilon_{xx} = \\frac{\\partial u_x}{\\partial x}, \\quad \\varepsilon_{yy} = \\frac{\\partial u_y}{\\partial y}, \\quad \\varepsilon_{xy} = \\varepsilon_{yx} = \\frac{1}{2}\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n应变张量的迹代表体积应变，为 $\\text{tr}(\\boldsymbol{\\varepsilon}) = \\varepsilon_{kk} = \\varepsilon_{xx} + \\varepsilon_{yy}$。\n\n对于线性、各向同性、弹性材料，关联柯西应力张量 $\\boldsymbol{\\sigma}$ 和应变张量 $\\boldsymbol{\\varepsilon}$ 的本构律由胡克定律给出，该定律可以使用拉梅参数 $\\lambda$ 和 $\\mu$ （也称为第一和第二拉梅参数，其中 $\\mu$ 是剪切模量）表示：\n$$\n\\boldsymbol{\\sigma} = \\lambda \\, \\text{tr}(\\boldsymbol{\\varepsilon}) \\, \\mathbf{I} + 2\\mu \\, \\boldsymbol{\\varepsilon}\n$$\n其中 $\\mathbf{I}$ 是二阶单位张量。在笛卡尔分量形式中，应力分量为：\n$$\n\\sigma_{xx} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{xx} = (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{yy} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{yy} = \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{xy} = \\sigma_{yx} = 2\\mu \\varepsilon_{xy} = \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n\n接下来，我们解决任务2：推导内部平衡残差分量的显式形式。准静态设定下的线性动量守恒即为平衡方程 $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b} = \\mathbf{0}$。物理信息神经网络 (PINN) 旨在最小化此方程的残差，即 $\\mathbf{R} = \\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$。此残差向量在笛卡尔坐标系中的分量为：\n$$\nR_x = \\frac{\\partial \\sigma_{xx}}{\\partial x} + \\frac{\\partial \\sigma_{xy}}{\\partial y} + b_x\n$$\n$$\nR_y = \\frac{\\partial \\sigma_{yx}}{\\partial x} + \\frac{\\partial \\sigma_{yy}}{\\partial y} + b_y\n$$\n将应力分量关于位移导数的表达式代入，得到纳维-柯西方程。我们假设材料是均质的，因此 $\\lambda$ 和 $\\mu$ 是常数。\n对于残差的 $x$-分量：\n$$\nR_x = \\frac{\\partial}{\\partial x} \\left( (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y} \\right) + \\frac{\\partial}{\\partial y} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + b_x\n$$\n$$\nR_x = (\\lambda + 2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_y}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_x}{\\partial y^2} + \\mu\\frac{\\partial^2 u_y}{\\partial y \\partial x} + b_x\n$$\n根据混合偏导数的相等性（$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial^2 u_y}{\\partial y \\partial x}$），我们可以对各项进行分组：\n$$\nR_x = (\\lambda+2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\mu \\frac{\\partial^2 u_x}{\\partial y^2} + (\\lambda+\\mu) \\frac{\\partial^2 u_y}{\\partial x \\partial y} + b_x\n$$\n对于残差的 $y$-分量：\n$$\nR_y = \\frac{\\partial}{\\partial x} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + \\frac{\\partial}{\\partial y} \\left( \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y} \\right) + b_y\n$$\n$$\nR_y = \\mu\\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_y}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_x}{\\partial y \\partial x} + (\\lambda + 2\\mu)\\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\n再次对各项进行分组：\n$$\nR_y = (\\lambda+\\mu) \\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu \\frac{\\partial^2 u_y}{\\partial x^2} + (\\lambda+2\\mu) \\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\n这些就是 PINN 会强制执行的残差分量的显式表达式。\n\n最后，我们解决任务3：为指定的神经网络拟设和参数评估这些残差。该拟设由以下公式给出：\n$z(x,y) = w_1 x + w_2 y + b_1$ 且 $\\mathbf{u}(x,y) = \\mathbf{W}_2 \\tanh(z(x,y)) + \\mathbf{b}_2$。\n根据给定的参数 $w_1 = 1$，$w_2 = 2$，$b_1 = 0$，$\\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 以及 $\\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，我们有：\n$z(x,y) = x + 2y$\n位移分量为：\n$$\nu_x(x,y) = 2 \\tanh(x+2y)\n$$\n$$\nu_y(x,y) = -1 \\tanh(x+2y)\n$$\n我们必须计算 $u_x$ 和 $u_y$ 的二阶偏导数。令 $T(x,y) = \\tanh(x+2y)$ 且 $S(x,y) = \\text{sech}^2(x+2y) = 1-\\tanh^2(x+2y)$。我们使用链式法则和导数恒等式 $\\frac{d}{d\\alpha}\\tanh(\\alpha) = \\text{sech}^2(\\alpha)$ 和 $\\frac{d}{d\\alpha}\\text{sech}^2(\\alpha) = -2\\tanh(\\alpha)\\text{sech}^2(\\alpha)$。\n\n$u_x = 2T$ 的导数：\n$\\frac{\\partial u_x}{\\partial x} = 2 \\frac{\\partial T}{\\partial x} = 2S \\cdot 1 = 2S$\n$\\frac{\\partial u_x}{\\partial y} = 2 \\frac{\\partial T}{\\partial y} = 2S \\cdot 2 = 4S$\n$\\frac{\\partial^2 u_x}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2S) = 2(-2TS) \\cdot 1 = -4TS$\n$\\frac{\\partial^2 u_x}{\\partial y^2} = \\frac{\\partial}{\\partial y}(4S) = 4(-2TS) \\cdot 2 = -16TS$\n$\\frac{\\partial^2 u_x}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2S) = 2(-2TS) \\cdot 2 = -8TS$\n\n$u_y = -T$ 的导数：\n$\\frac{\\partial u_y}{\\partial x} = -S \\cdot 1 = -S$\n$\\frac{\\partial u_y}{\\partial y} = -S \\cdot 2 = -2S$\n$\\frac{\\partial^2 u_y}{\\partial x^2} = \\frac{\\partial}{\\partial x}(-S) = -(-2TS) \\cdot 1 = 2TS$\n$\\frac{\\partial^2 u_y}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2S) = -2(-2TS) \\cdot 2 = 8TS$\n$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(-S) = -(-2TS) \\cdot 2 = 4TS$\n\n给定体力为零，即 $\\mathbf{b}=\\mathbf{0}$。我们将这些导数代入残差表达式中：\n$$\nR_x = (\\lambda+2\\mu)(-4TS) + \\mu(-16TS) + (\\lambda+\\mu)(4TS)\n$$\n$$\nR_x = [-4(\\lambda+2\\mu) - 16\\mu + 4(\\lambda+\\mu)]TS = [-4\\lambda - 8\\mu - 16\\mu + 4\\lambda + 4\\mu]TS = -20\\mu TS\n$$\n$$\nR_y = (\\lambda+\\mu)(-8TS) + \\mu(2TS) + (\\lambda+2\\mu)(8TS)\n$$\n$$\nR_y = [-8(\\lambda+\\mu) + 2\\mu + 8(\\lambda+2\\mu)]TS = [-8\\lambda - 8\\mu + 2\\mu + 8\\lambda + 16\\mu]TS = 10\\mu TS\n$$\n残差向量为 $\\mathbf{R}(x,y) = \\begin{pmatrix} -20\\mu & 10\\mu \\end{pmatrix} \\tanh(x+2y)\\text{sech}^2(x+2y)$。\n\n我们必须在点 $(x_0, y_0) = (1, 0)$ 处计算该值。在该点，双曲函数的自变量为 $z_0 = 1 + 2(0) = 1$。\n项 $TS$ 变为 $\\tanh(1)\\text{sech}^2(1)$。使用恒等式 $\\text{sech}^2(\\alpha) = 1/\\cosh^2(\\alpha)$，即为 $\\frac{\\tanh(1)}{\\cosh^2(1)}$。\n在 $(1, 0)$ 点的残差分量为：\n$$\nR_x(1,0) = -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n$$\nR_y(1,0) = 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n题目要求将结果表示为单个行向量。\n$$\n\\mathbf{R}(1,0) = \\begin{pmatrix} -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} \\end{pmatrix}\n$$\n这就是最终的解析表达式。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "一个适定的物理问题不仅需要控制方程，还需要边界条件。本练习  专注于为具有混合边界条件（狄利克雷和诺伊曼条件）的问题构建损失函数，这在工程应用中非常常见。您将学习如何处理不同类型的边界数据，并理解自动微分在评估基于导数的边界条件中所起的关键作用，从而完善您构建 PINN 问题的工具集。",
            "id": "4235901",
            "problem": "一个稳态传导场的赛博物理系统数字孪生由拉普拉斯方程建模，该方程由通量守恒、传导率恒定且无源的条件推导得出。设物理势为 $u:\\Omega \\to \\mathbb{R}$，其中 $\\Omega \\subset \\mathbb{R}^{d}$ 是一个具有Lipschitz边界 $\\partial \\Omega$ 的有界域。控制方程为在 $\\Omega$ 内 $-\\Delta u = 0$，并带有混合边界条件：在 $\\Gamma_{D} \\subset \\partial \\Omega$ 上 $u=g$，以及在 $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$ 上 $\\partial_{n} u = h$。其中 $g:\\Gamma_{D} \\to \\mathbb{R}$ 和 $h:\\Gamma_{N} \\to \\mathbb{R}$ 由边界传感器测量，$\\partial_{n} u$ 表示法向导数。\n\n我们部署一个带有参数 $\\theta$ 的物理信息神经网络（PINN）$u_{\\theta}:\\Omega \\to \\mathbb{R}$ 作为数字孪生代理模型。为进行边界训练，我们在每个边界段上收集求积样本：\n- 狄利克雷样本 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，其具有相关的测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$ 和用于近似 $\\Gamma_{D}$ 上积分的正求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。\n- 诺伊曼样本 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，其具有测量的通量 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$ 和正的求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。\n\n令 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 为总求积权重。假设选择正的惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$ 来平衡狄利克雷和诺伊曼边界条件的实施强度。\n\n从基本定律 $-\\Delta u = 0$ 以及狄利克雷和诺伊曼边界条件的定义出发，推导在 $\\Gamma_{D}$ 和 $\\Gamma_{N}$ 上各自的边界残差泛函，这些泛函分别用于衡量 $u_{\\theta}$ 与 $g$ 之间以及 $\\nabla u_{\\theta} \\cdot n$ 与 $h$ 之间的平方失配。然后，利用给定的样本和权重进行求积近似，构造一个单一的、复合的、无量纲的纯边界损失泛函 $L_{\\mathrm{bc}}(\\theta)$，其值为两个由 $W_{D}$ 和 $W_{N}$ 归一化后的边界残差的加权和。请将此 $L_{\\mathrm{bc}}(\\theta)$ 显式地表达为一个闭式解析表达式，该表达式应使用给定的数据 $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$、$\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$ 和网络 $u_{\\theta}$ 来表示。在推导过程中，请讨论如何使用自动微分（AD）在边界点上获得 $\\nabla u_{\\theta}$ 来实施每个边界残差。\n\n最终答案必须是 $L_{\\mathrm{bc}}(\\theta)$ 作为 $\\theta$ 函数的单一显式表达式，仅包含符号量和给定数据。不应包含单位。无需四舍五入。",
            "solution": "该问题要求为一个物理势 $u$ 的物理信息神经网络（PINN）代理模型 $u_{\\theta}$ 推导一个复合、无量纲、纯边界损失泛函，记为 $L_{\\mathrm{bc}}(\\theta)$。该系统由定义在域 $\\Omega \\subset \\mathbb{R}^{d}$ 上的拉普拉斯方程 $-\\Delta u = 0$ 控制，并带有混合狄利克雷和诺伊曼边界条件。该损失泛函是根据边界上的离散传感器数据构建的。\n\n我们首先为每种类型的边界条件正式定义连续残差泛函。这些泛函衡量了PINN代理模型 $u_{\\theta}$ 未能满足边界 $\\partial \\Omega$ 上规定条件的程度。\n\n首先，考虑狄利克雷边界 $\\Gamma_{D}$，其中势被规定为 $u=g$。在任意点 $x \\in \\Gamma_D$ 处的失配或残差由差值 $u_{\\theta}(x) - g(x)$ 给出。为了衡量该边界段上的总误差，我们对该残差的平方进行积分。这得到了连续的狄利克雷边界残差泛函 $R_{D}(\\theta)$：\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, dS\n$$\n其中 $dS$ 是边界 $\\partial\\Omega$ 上的曲面测度。\n\n其次，考虑诺伊曼边界 $\\Gamma_{N}$，其中势的法向导数被规定为 $\\partial_{n} u = h$。法向导数是梯度在单位外法向量 $n(x)$ 上的投影，即 $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$。在点 $x \\in \\Gamma_N$ 处的残差是 PINN 近似的法向导数与规定函数 $h(x)$ 之间的差值，即 $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$。诺伊曼边界上的总平方误差则由连续的诺伊曼边界残差泛函 $R_{N}(\\theta)$ 给出：\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, dS\n$$\n\n在实际应用中，我们没有连续函数 $g$ 和 $h$，而是有在特定传感器位置的离散测量值。问题提供了样本点和求积权重的集合，用于近似 $R_{D}(\\theta)$ 和 $R_{N}(\\theta)$ 的积分。\n\n对于狄利克雷边界，我们有 $N_D$ 个样本点 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，以及相应的势测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$（其中 $g_i = g(x_i^D)$）和正的求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。$R_{D}(\\theta)$ 的积分通过一个加权和来近似，我们将其表示为离散残差 $\\hat{R}_{D}(\\theta)$：\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\n这是积分 $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, dS$ 的一个数值求积近似。\n\n对于诺伊曼边界，我们有 $N_N$ 个样本点 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，以及通量测量值 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}}$ 和正的求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。此处的关键计算是项 $\\nabla u_{\\theta}(x_j^N)$，即神经网络输出相对于其空间输入的梯度。这是使用自动微分（AD）计算的。神经网络 $u_{\\theta}$ 是可微基本函数（例如，仿射变换和激活函数）的复合。因此，它相对于其输入的导数可以通过算法计算到机器精度。深度学习框架原生提供此功能，通常通过反向模式AD（反向传播）实现，从而可以在损失计算所需的任何点 $x_j^N$ 上高效地评估 $\\nabla u_{\\theta}$。\n\n使用AD获得 $\\nabla u_{\\theta}(x_{j}^{N})$后，我们就可以用离散诺伊曼残差 $\\hat{R}_{N}(\\theta)$ 来近似 $R_{N}(\\theta)$ 的积分：\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\n问题要求一个单一的复合损失泛函 $L_{\\mathrm{bc}}(\\theta)$，它是两个边界残差的加权和，并由各自的总求积权重 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 进行归一化。这种归一化将每个离散残差转化为均方误差的近似。我们还被给予了正的惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$，以平衡实施每个边界条件的相对重要性。\n归一化的狄利克雷损失项是 $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$。\n归一化的诺伊曼损失项是 $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$。\n\n问题指明最终的损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 应该是无量纲的。这通常意味着控制方程和边界条件已经事先使用长度、势等特征尺度进行了无量纲化。在此标准假设下，损失函数中的所有量都是无量纲的，并且惩罚系数 $\\lambda_D$ 和 $\\lambda_N$ 也是用于调整训练过程的无量纲因子。\n\n结合这些分量，复合边界损失泛函是归一化和加权的离散残差之和：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\n代入 $\\hat{R}_{D}(\\theta)$、$\\hat{R}_{N}(\\theta)$、$W_{D}$ 和 $W_{N}$ 的表达式：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\n使用给定的定义 $W_{D}$ 和 $W_{N}$，可以将其写得更简洁：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n此表达式代表了纯边界损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 的最终闭式解析形式，它是网络参数 $\\theta$ 和给定数据的函数。关于 $\\theta$ 最小化此损失泛函，可在最小二乘意义上训练网络以满足边界条件。",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        }
    ]
}