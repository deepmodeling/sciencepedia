{
    "hands_on_practices": [
        {
            "introduction": "To effectively use and scrutinize explainable AI methods, it is essential to grasp their underlying mathematical principles. This exercise provides a foundational deep dive into SHapley Additive exPlanations (SHAP), a unifying and theoretically grounded framework for feature attribution. By deriving the SHAP values for a simple linear model from first principles , you will see precisely how model structure and statistical assumptions about features interact to produce an explanation.",
            "id": "4220831",
            "problem": "A digital twin (DT) of a Cyber-Physical System (CPS) uses a linear surrogate model to predict a scalar performance metric from a vector of standardized sensor features. Let the features be denoted by $X = (X_{1},\\dots,X_{d})$, and let the model be $f(x) = w^{\\top}x = \\sum_{j=1}^{d} w_{j} x_{j}$ with no intercept. The DT’s background data distribution for $X$ (used to define missing-feature semantics in explainable artificial intelligence) is such that the components are mutually independent and standardized, meaning $\\mathbb{E}[X_{j}] = 0$ for all $j \\in \\{1,\\dots,d\\}$ and $X_{1},\\dots,X_{d}$ are independent. The explanation method follows the SHapley Additive exPlanations (SHAP) framework: for any subset $S \\subseteq \\{1,\\dots,d\\}$, define the coalition value as $v(S) = \\mathbb{E}\\!\\left[ f(X) \\mid X_{S} = x_{S} \\right]$, and define the feature attribution for index $i$ as the Shapley value in this game, that is,\n$$\n\\phi_{i} \\;=\\; \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} \\left( v(S \\cup \\{i\\}) - v(S) \\right),\n$$\nwhere $N = \\{1,\\dots,d\\}$.\n\nStarting from these definitions and the stated statistical properties of $X$, derive a closed-form analytic expression for $\\phi_{i}$ in terms of $w_{i}$ and the query point’s coordinate $x_{i}$. Your final answer must be a single expression. Do not assume any additional properties beyond those stated above. Express the final answer without units. No rounding is required.",
            "solution": "The problem is valid. It is a well-posed mathematical derivation based on established principles of explainable artificial intelligence (XAI) and statistics. All givens are consistent, and the objective is clearly defined. We shall now proceed with the derivation.\n\nThe objective is to derive a closed-form expression for the Shapley feature attribution, $\\phi_i$, for a linear model $f(x) = \\sum_{j=1}^{d} w_{j} x_{j}$ where the feature vector $X = (X_{1},\\dots,X_{d})$ consists of mutually independent random variables with zero mean, i.e., $\\mathbb{E}[X_{j}] = 0$ for all $j \\in \\{1,\\dots,d\\}$.\n\nThe attribution $\\phi_i$ is defined by the Shapley value formula:\n$$\n\\phi_{i} \\;=\\; \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} \\left( v(S \\cup \\{i\\}) - v(S) \\right)\n$$\nwhere $N = \\{1,\\dots,d\\}$ and the coalition value function $v(S)$ is given by the conditional expectation of the model's output:\n$$\nv(S) = \\mathbb{E}\\!\\left[ f(X) \\mid X_{S} = x_{S} \\right]\n$$\nHere, $X_S = x_S$ denotes the event that the features with indices in the set $S$ take the specific values from the query point $x$.\n\nOur first step is to derive a simplified expression for the coalition value function, $v(S)$. We substitute the definition of the linear model $f(X)$ into the expression for $v(S)$:\n$$\nv(S) = \\mathbb{E}\\!\\left[ \\sum_{j=1}^{d} w_{j} X_{j} \\mid X_{S} = x_{S} \\right]\n$$\nBy the linearity of the expectation operator, we can move the summation outside the expectation:\n$$\nv(S) = \\sum_{j=1}^{d} w_{j} \\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right]\n$$\nNow, we must evaluate the conditional expectation $\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right]$ for each feature $X_j$. We consider two cases based on whether the index $j$ is in the set $S$.\n\nCase 1: $j \\in S$.\nIf the index $j$ is in the set $S$, the value of the random variable $X_j$ is fixed by the condition $X_S = x_S$. Specifically, $X_j$ is fixed to the value $x_j$. The expectation of a constant is the constant itself, so:\n$$\n\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] = x_{j} \\quad \\text{for } j \\in S\n$$\n\nCase 2: $j \\notin S$.\nIf the index $j$ is not in the set $S$, we must consider the statistical properties of the features. The problem states that the features $X_1, \\dots, X_d$ are mutually independent. This means that knowing the values of features in the set $S$ ($X_S$) provides no information about the value of a feature $X_j$ where $j \\notin S$. Therefore, the conditional expectation is equal to the unconditional expectation:\n$$\n\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] = \\mathbb{E}[X_{j}] \\quad \\text{for } j \\notin S\n$$\nThe problem also states that the features are standardized such that $\\mathbb{E}[X_j] = 0$ for all $j$. Thus:\n$$\n\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] = 0 \\quad \\text{for } j \\notin S\n$$\n\nCombining these two cases, we can write the expression for $v(S)$ by splitting the sum over indices in $S$ and indices not in $S$:\n$$\nv(S) = \\sum_{j \\in S} w_{j} \\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] + \\sum_{j \\notin S} w_{j} \\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right]\n$$\n$$\nv(S) = \\sum_{j \\in S} w_{j} x_{j} + \\sum_{j \\notin S} w_{j} (0) = \\sum_{j \\in S} w_{j} x_{j}\n$$\nThis provides a simple, closed-form expression for the coalition value function.\n\nThe next step is to compute the marginal contribution of feature $i$, which is the term $v(S \\cup \\{i\\}) - v(S)$ in the Shapley formula. The summation is over all subsets $S \\subseteq N \\setminus \\{i\\}$, which implies that $i \\notin S$.\nUsing our expression for $v(\\cdot)$:\n$$\nv(S \\cup \\{i\\}) = \\sum_{j \\in S \\cup \\{i\\}} w_{j} x_{j} = w_i x_i + \\sum_{j \\in S} w_{j} x_{j}\n$$\nAnd, as before:\n$$\nv(S) = \\sum_{j \\in S} w_{j} x_{j}\n$$\nThe difference is therefore:\n$$\nv(S \\cup \\{i\\}) - v(S) = \\left(w_i x_i + \\sum_{j \\in S} w_{j} x_{j}\\right) - \\left(\\sum_{j \\in S} w_{j} x_{j}\\right) = w_i x_i\n$$\nRemarkably, the marginal contribution of feature $i$ is constant for any coalition $S \\subseteq N \\setminus \\{i\\}$ and is equal to $w_i x_i$.\n\nNow we substitute this result back into the Shapley value formula for $\\phi_i$:\n$$\n\\phi_{i} = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} (w_i x_i)\n$$\nSince the term $w_i x_i$ is independent of the summation variable $S$, we can factor it out:\n$$\n\\phi_{i} = (w_i x_i) \\left( \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} \\right)\n$$\nThe final step is to evaluate the summation of the Shapley weights. Let's denote this sum by $\\Sigma$:\n$$\n\\Sigma = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!}\n$$\nThe sum is over all subsets of $N \\setminus \\{i\\}$, a set with $d-1$ elements. We can group the subsets $S$ by their cardinality, $k = |S|$, which ranges from $k=0$ to $k=d-1$. For any given size $k$, the number of subsets $S \\subseteq N \\setminus \\{i\\}$ with $|S|=k$ is given by the binomial coefficient $\\binom{d-1}{k}$. Since all such subsets have the same cardinality, the weight term $\\frac{k!\\,(d-k-1)!}{d!}$ is the same for all of them.\nWe can rewrite the sum as:\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\binom{d-1}{k} \\frac{k!\\,(d-k-1)!}{d!}\n$$\nSubstitute the combinatorial formula for $\\binom{d-1}{k} = \\frac{(d-1)!}{k!\\,(d-1-k)!}$:\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\frac{(d-1)!}{k!\\,(d-1-k)!} \\cdot \\frac{k!\\,(d-k-1)!}{d!}\n$$\nWe can cancel the $k!$ and $(d-k-1)!$ terms:\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\frac{(d-1)!}{d!}\n$$\nSince $d! = d \\cdot (d-1)!$, this simplifies to:\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\frac{1}{d}\n$$\nThe summation runs from $k=0$ to $k=d-1$, which constitutes $d$ identical terms. Therefore, the sum is:\n$$\n\\Sigma = d \\cdot \\frac{1}{d} = 1\n$$\nThe sum of the Shapley weights is unity, which is a known property of Shapley values.\n\nFinally, we substitute this result back into our expression for $\\phi_i$:\n$$\n\\phi_{i} = (w_i x_i) \\cdot \\Sigma = (w_i x_i) \\cdot 1 = w_i x_i\n$$\nThus, for a linear model with independent, zero-mean features, the SHAP value (feature attribution) for feature $i$ is simply the term contribution $w_i x_i$.",
            "answer": "$$\\boxed{w_i x_i}$$"
        },
        {
            "introduction": "In many real-world Cyber-Physical Systems, the deployed AI model may be a complex \"black box,\" hindering transparency. A powerful technique for explaining such models is to approximate their behavior with a simpler, inherently interpretable surrogate model. This hands-on exercise  simulates this scenario, challenging you to build a sparse linear surrogate and quantitatively evaluate the fundamental trade-off between an explanation's simplicity (sparsity) and its faithfulness (fidelity) to the original complex model.",
            "id": "4220836",
            "problem": "A cyber-physical system (CPS) is monitored by its Digital Twin, which aggregates sensor states into feature vectors and computes a black-box anomaly score. To obtain a transparent explanation in the sense of Explainable Artificial Intelligence (XAI), we fit a sparse linear surrogate to approximate the black-box anomaly detector. Consider a dataset of feature vectors $x_i \\in \\mathbb{R}^d$ for $i \\in \\{1,\\dots,n\\}$ and a black-box anomaly score function $f:\\mathbb{R}^d \\to \\mathbb{R}$. The surrogate is $g(x)=w^{\\top}x$ with $w \\in \\mathbb{R}^d$, and we train $w$ by empirical risk minimization with an $\\ell_0$ sparsity penalty. Use the centered target $y_i = f(x_i)-\\bar{f}$, where $\\bar{f} = \\frac{1}{n}\\sum_{i=1}^{n}f(x_i)$, to respect the zero-intercept surrogate $g(x)=w^{\\top}x$.\n\nFundamental base:\n- Empirical risk under Mean Squared Error (MSE) is defined as\n$$\n\\ell(w;X,y) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(w^{\\top}x_i - y_i\\right)^2,\n$$\nwhere $X \\in \\mathbb{R}^{n\\times d}$ is the design matrix with rows $x_i$ and $y \\in \\mathbb{R}^n$ is the centered target vector with entries $y_i$.\n- The $\\ell_0$ \"norm\" counts nonzero coefficients: $$\\|w\\|_0 = |\\{j \\in \\{1,\\dots,d\\}: w_j \\neq 0\\}|.$$\n- The penalized objective is\n$$\nJ_{\\lambda}(w) = \\ell(w;X,y) + \\lambda \\|w\\|_0,\n$$\nwhere $\\lambda \\ge 0$ regulates sparsity.\n- Fidelity is quantified by the coefficient of determination:\n$$\nF = 1 - \\frac{\\ell(w;X,y)}{\\mathrm{Var}(y)}, \\quad \\mathrm{Var}(y) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2, \\quad \\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n} y_i,\n$$\nso $F \\in (-\\infty,1]$ and higher $F$ indicates better approximation of the black-box anomaly scores by the surrogate.\n- Define completeness relative to the best unconstrained linear surrogate (no sparsity penalty, all features available) as\n$$\nC(\\lambda) = \n\\begin{cases}\n\\frac{F(\\lambda)}{F(0^{\\star})},  \\text{if } F(0^{\\star})0,\\\\\n0,  \\text{otherwise,}\n\\end{cases}\n$$\nwhere $F(0^{\\star})$ is the fidelity of the least-squares solution using all features (the maximum-fidelity linear surrogate without sparsity constraints).\n\nYour tasks:\n1. For each test case below, construct $X$ and $f$ exactly as specified, compute $y_i=f(x_i)-\\bar{f}$, and for each given $\\lambda$ solve\n$$\nw^{\\star}_{\\lambda} \\in \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ \\ell(w;X,y) + \\lambda \\|w\\|_0 \\right\\}.\n$$\nTo ensure a globally optimal solution to the nonconvex $\\ell_0$-penalized problem, restrict $w$ to have support on a subset of features $S \\subseteq \\{1,\\dots,d\\}$ and, for each $S$, obtain the least-squares minimizer on $S$; then select the subset $S$ that minimizes $J_{\\lambda}$.\n2. For each chosen $w^{\\star}_{\\lambda}$, compute $F(\\lambda)$ and $C(\\lambda)$ on the dataset.\n3. Aggregate all results into a single list in the exact order specified under “Final Output Format”.\n\nTest suite (all random draws must use the specified seeds):\n- Test Case $A$ (partly linear black-box):\n  - $n=80$, $d=5$, seed $=1$.\n  - Draw $X$ with entries independently from a standard normal distribution $\\mathcal{N}(0,1)$.\n  - Define\n    $$\n    f(x) = 0.9\\,x_1 - 0.6\\,x_2 + 0.4\\,x_3 + 0.2\\,\\sin(x_4) + \\epsilon,\n    $$\n    with $\\epsilon \\sim \\mathcal{N}(0,0.05^2)$ independent of $X$.\n  - Sparsity penalties: $\\lambda \\in [0.0, 0.01, 0.05, 0.2]$.\n- Test Case $B$ (exactly linear black-box):\n  - $n=60$, $d=6$, seed $=2$.\n  - Draw $X$ with entries independently from $\\mathcal{N}(0,1)$.\n  - Define\n    $$\n    f(x) = 1.0\\,x_1 - 1.0\\,x_2 + 0.5\\,x_3.\n    $$\n    No noise is added.\n  - Sparsity penalties: $\\lambda \\in [0.0, 0.5, 1.5]$.\n- Test Case $C$ (nonlinear black-box):\n  - $n=100$, $d=5$, seed $=3$.\n  - Draw $X$ with entries independently from $\\mathcal{N}(0,1)$.\n  - Define\n    $$\n    f(x) = \\tanh(1.2\\,x_1 + 1.0\\,x_2) + 0.3\\,\\cos(x_3\\,x_4) + 0.1\\,\\sin(x_5) + \\epsilon,\n    $$\n    with $\\epsilon \\sim \\mathcal{N}(0,0.02^2)$ independent of $X$.\n  - Sparsity penalties: $\\lambda \\in [0.0, 0.1, 0.3]$.\n\nAngle units: any trigonometric function is in radians.\n\nOutput specification:\n- For each test case, and for each $\\lambda$ in the given order, compute $(F(\\lambda),C(\\lambda))$ using the dataset, and output all values as a single list of floating-point numbers in the following exact order:\n  - For Test Case $A$: $F(0.0),C(0.0),F(0.01),C(0.01),F(0.05),C(0.05),F(0.2),C(0.2)$.\n  - For Test Case $B$: $F(0.0),C(0.0),F(0.5),C(0.5),F(1.5),C(1.5)$.\n  - For Test Case $C$: $F(0.0),C(0.0),F(0.1),C(0.1),F(0.3),C(0.3)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,\\dots,r_{14}]$. No additional text should be printed.",
            "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded problem in the domain of explainable artificial intelligence (XAI) applied to cyber-physical systems. The problem statement is complete, providing all necessary data, definitions, and constraints for a reproducible solution. The computational task, while intensive, is feasible for the specified dimensions.\n\nThe core task is to find a sparse linear surrogate model, $g(x) = w^{\\top}x$, that approximates a black-box anomaly score function, $f(x)$. The weight vector $w \\in \\mathbb{R}^d$ is trained by minimizing a penalized empirical risk function. The objective function, $J_{\\lambda}(w)$, combines the Mean Squared Error (MSE) loss with an $\\ell_0$ sparsity penalty:\n$$\nJ_{\\lambda}(w) = \\ell(w;X,y) + \\lambda \\|w\\|_0 = \\frac{1}{n}\\sum_{i=1}^{n}\\left(w^{\\top}x_i - y_i\\right)^2 + \\lambda |\\{j : w_j \\neq 0\\}|\n$$\nHere, $X \\in \\mathbb{R}^{n\\times d}$ is the design matrix, and $y \\in \\mathbb{R}^n$ is the vector of centered target values, $y_i = f(x_i) - \\bar{f}$, where $\\bar{f} = \\frac{1}{n}\\sum_i f(x_i)$. The parameter $\\lambda \\ge 0$ controls the trade-off between the model's fidelity to the data and its sparsity.\n\nThe $\\ell_0$ penalty makes the objective function $J_{\\lambda}(w)$ non-convex, and finding the global minimum is an NP-hard problem in general. However, the problem specifies a method that guarantees a globally optimal solution: an exhaustive search over all possible subsets of features. Since the feature dimension $d$ is small in all test cases ($d=5$ or $d=6$), this combinatorial search is computationally tractable. There are $2^d$ possible subsets of features to consider.\n\nThe solution methodology for each given test case and each value of $\\lambda$ is as follows:\n\n1.  **Data Generation**: For each test case, we first generate the data. A design matrix $X \\in \\mathbb{R}^{n\\times d}$ is created with entries drawn from a standard normal distribution, $\\mathcal{N}(0,1)$, using the specified random seed. The black-box function $f(x)$ is evaluated for each row $x_i$ of $X$ to produce the scores $f(x_i)$. Noise $\\epsilon$, if specified, is also generated and added. Finally, the target vector $y$ is computed by centering the scores: $y_i = f(x_i) - \\frac{1}{n}\\sum_{j=1}^{n}f(x_j)$. By construction, the mean of $y$ is $\\bar{y} = 0$.\n\n2.  **Best Subset Selection**: To find the optimal weight vector $w^{\\star}_{\\lambda}$ that minimizes $J_{\\lambda}(w)$, we iterate through all $2^d$ possible subsets of feature indices, $S \\subseteq \\{1, \\dots, d\\}$.\n    *   For each subset $S$ of size $k = |S|$, we solve a standard Ordinary Least Squares (OLS) regression problem. We seek a weight vector $w_S$ with non-zero elements only for indices in $S$ that minimizes the MSE. This is equivalent to regressing $y$ onto the columns of $X$ corresponding to the subset $S$, denoted $X_S$. The OLS solution is $w_S^* = (X_S^{\\top}X_S)^{-1}X_S^{\\top}y$.\n    *   The minimized MSE for this subset is $\\ell_S = \\frac{1}{n}\\|X_S w_S^* - y\\|_2^2$.\n    *   The penalized objective value for this subset is $J_{\\lambda, S} = \\ell_S + \\lambda k$.\n    *   We compare the $J_{\\lambda, S}$ values across all $2^d$ subsets. The subset $S^\\star$ that yields the minimum value, $J_{\\lambda}^{\\star} = \\min_S J_{\\lambda, S}$, is the optimal support set for the given $\\lambda$. The corresponding minimal MSE is $\\ell^{\\star}_{\\lambda}$.\n\n3.  **Metrics Calculation**:\n    *   **Fidelity ($F(\\lambda)$)**: The fidelity of the surrogate model for a given $\\lambda$ is computed using the coefficient of determination formula. It requires the variance of the target vector, $\\mathrm{Var}(y) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\frac{1}{n}\\sum_{i=1}^n y_i^2$ (since $\\bar{y}=0$). The fidelity is then:\n        $$\n        F(\\lambda) = 1 - \\frac{\\ell^{\\star}_{\\lambda}}{\\mathrm{Var}(y)}\n        $$\n    *   **Completeness ($C(\\lambda)$)**: First, we need the baseline fidelity $F(0^{\\star})$, which corresponds to the best possible linear model without sparsity constraints. This is achieved by the OLS solution using all $d$ features (which is also the solution for $\\lambda=0$). Let the corresponding MSE be $\\ell_{OLS}$. Then $F(0^{\\star}) = 1 - \\ell_{OLS} / \\mathrm{Var}(y)$. Completeness is then calculated as the ratio:\n        $$\n        C(\\lambda) = \\frac{F(\\lambda)}{F(0^{\\star})}, \\quad \\text{if } F(0^{\\star})  0, \\text{ and } C(\\lambda)=0 \\text{ otherwise.}\n        $$\n\nThis entire procedure is implemented for each test case. For a given case, to optimize the computation, the MSE for every possible feature subset is pre-calculated. Then, for each $\\lambda$, we simply find the subset that minimizes the pre-calculated MSE plus the new penalty term $\\lambda k$. Finally, all computed $(F(\\lambda), C(\\lambda))$ pairs are collected into a single ordered list as specified by the problem.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve_case(X, y, n, d, lambdas):\n    \"\"\"\n    Solves for F(lambda) and C(lambda) for a single test case.\n    \"\"\"\n    case_results = []\n    \n    # Target vector y is already centered, so its mean is effectively zero.\n    var_y = np.var(y)\n    if var_y = 0:\n        # If variance is zero or negative (due to float precision), F is ill-defined.\n        # This case is not expected with the given problem data.\n        # Return zeros for all metrics.\n        return [0.0] * 2 * len(lambdas)\n\n    # Calculate baseline fidelity F(0*) from the full OLS model.\n    # This corresponds to lambda=0.\n    w_ols, resid_ols, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    mse_ols = resid_ols[0] / n if resid_ols.size  0 else np.mean((X @ w_ols - y)**2)\n    F_star = 1 - mse_ols / var_y\n\n    # Pre-calculate the MSE for all 2^d possible feature subsets.\n    # This avoids redundant least-squares computations for each lambda.\n    subset_mses = {}\n    for k in range(d + 1):\n        for subset in combinations(range(d), k):\n            if k == 0:\n                # For an empty set of features, the prediction is 0. MSE is Var(y).\n                mse = var_y\n            else:\n                X_S = X[:, subset]\n                # Solve the OLS problem for the subset of features.\n                _, resid_S, _, _ = np.linalg.lstsq(X_S, y, rcond=None)\n                mse = resid_S[0] / n if resid_S.size  0 else np.mean((X_S @ np.linalg.lstsq(X_S, y, rcond=None)[0] - y)**2)\n            subset_mses[subset] = mse\n\n    # For each lambda, find the best subset that minimizes the J(w) objective.\n    for lam in lambdas:\n        min_J = float('inf')\n        best_mse_for_lam = float('inf')\n\n        for subset, mse in subset_mses.items():\n            k = len(subset)  # Number of non-zero coefficients\n            J = mse + lam * k\n            if J  min_J:\n                min_J = J\n                best_mse_for_lam = mse\n\n        # Calculate fidelity F(lambda) for the optimal model.\n        F_lam = 1 - best_mse_for_lam / var_y\n        \n        # Calculate completeness C(lambda).\n        C_lam = 0.0\n        if F_star  0:\n            C_lam = F_lam / F_star\n        \n        case_results.extend([F_lam, C_lam])\n\n    return case_results\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    all_results = []\n\n    # --- Test Case A ---\n    n_A, d_A, seed_A = 80, 5, 1\n    lambdas_A = [0.0, 0.01, 0.05, 0.2]\n    rng_A = np.random.default_rng(seed_A)\n    X_A = rng_A.standard_normal(size=(n_A, d_A))\n    epsilon_A = rng_A.normal(0, 0.05, size=n_A)\n    f_A_vals = (0.9 * X_A[:, 0] - 0.6 * X_A[:, 1] + 0.4 * X_A[:, 2] +\n                0.2 * np.sin(X_A[:, 3]) + epsilon_A)\n    y_A = f_A_vals - np.mean(f_A_vals)\n    results_A = solve_case(X_A, y_A, n_A, d_A, lambdas_A)\n    all_results.extend(results_A)\n\n    # --- Test Case B ---\n    n_B, d_B, seed_B = 60, 6, 2\n    lambdas_B = [0.0, 0.5, 1.5]\n    rng_B = np.random.default_rng(seed_B)\n    X_B = rng_B.standard_normal(size=(n_B, d_B))\n    f_B_vals = 1.0 * X_B[:, 0] - 1.0 * X_B[:, 1] + 0.5 * X_B[:, 2]\n    y_B = f_B_vals - np.mean(f_B_vals)\n    results_B = solve_case(X_B, y_B, n_B, d_B, lambdas_B)\n    all_results.extend(results_B)\n\n    # --- Test Case C ---\n    n_C, d_C, seed_C = 100, 5, 3\n    lambdas_C = [0.0, 0.1, 0.3]\n    rng_C = np.random.default_rng(seed_C)\n    X_C = rng_C.standard_normal(size=(n_C, d_C))\n    epsilon_C = rng_C.normal(0, 0.02, size=n_C)\n    f_C_vals = (np.tanh(1.2 * X_C[:, 0] + 1.0 * X_C[:, 1]) +\n                0.3 * np.cos(X_C[:, 2] * X_C[:, 3]) +\n                0.1 * np.sin(X_C[:, 4]) + epsilon_C)\n    y_C = f_C_vals - np.mean(f_C_vals)\n    results_C = solve_case(X_C, y_C, n_C, d_C, lambdas_C)\n    all_results.extend(results_C)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For an AI to be trusted in a safety-critical CPS, its confidence scores must be reliable; a stated $0.8$ confidence should reflect a $0.8$ chance of being correct. This exercise tackles the crucial concept of *calibration*, the process of aligning a model's probabilistic outputs with real-world frequencies. You will implement calibration for both a fault detector's predictions and an explanation's reliability score, using the Expected Calibration Error (ECE) to rigorously quantify their trustworthiness .",
            "id": "4220883",
            "problem": "You are given a binary fault detection setting in Cyber-Physical Systems (CPS) with an associated Digital Twin. A classifier produces uncalibrated scores for the presence of a fault, and an explanation module produces an importance distribution over features. You must compute calibrated probability estimates for fault detection and for explanation correctness, and then assess reliability via the Expected Calibration Error (ECE). The problem is to design and implement a complete algorithm grounded in fundamental probabilistic definitions and well-tested statistical models, and produce numerical results on a specified test suite.\n\nFundamental base:\n- A binary event with label $y \\in \\{0,1\\}$ is modeled as a Bernoulli random variable with probability $p \\in [0,1]$ so that $\\mathbb{P}(Y=1)=p$ and $\\mathbb{P}(Y=0)=1-p$.\n- The logistic (sigmoid) link function is $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$. The logit function is $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$ for $p \\in (0,1)$.\n- For calibration, consider parametric mappings learned via maximum likelihood on a calibration set. For detection score $x$, a logistic calibration mapping is $\\tilde{p}(x) = \\sigma(a x + b)$ with parameters $a$ and $b$ chosen to minimize the negative log-likelihood under the Bernoulli model. For explanation reliability with an uncalibrated probability $q \\in (0,1)$, use a monotone mapping of the form $\\tilde{r}(q) = \\sigma(a \\cdot \\text{logit}(q) + b)$ with $a$ and $b$ learned similarly.\n- Expected Calibration Error (ECE) with $M$ bins partitions the unit interval into $M$ equal-width bins. For predictions $\\{p_i\\}_{i=1}^N$ and binary labels $\\{y_i\\}_{i=1}^N$, define bin $B_m$ as those indices $i$ with $p_i \\in \\left[\\frac{m-1}{M}, \\frac{m}{M}\\right)$. Let $n_m$ be the number of samples in $B_m$, $\\hat{c}_m = \\frac{1}{n_m}\\sum_{i \\in B_m} p_i$ the average confidence, and $\\hat{a}_m = \\frac{1}{n_m}\\sum_{i \\in B_m} y_i$ the empirical accuracy. The ECE is\n$$\n\\text{ECE} = \\sum_{m=1}^M \\frac{n_m}{N} \\left| \\hat{a}_m - \\hat{c}_m \\right|.\n$$\nIf some bin has $n_m = 0$, its contribution is defined to be $0$.\n\nYour program must implement the following steps for each test case:\n1. Learn detection calibration parameters $(a_d,b_d)$ on the provided calibration split by minimizing the Bernoulli negative log-likelihood for the mapping $x \\mapsto \\sigma(a_d x + b_d)$.\n2. Learn explanation calibration parameters $(a_e,b_e)$ on the provided calibration split by minimizing the Bernoulli negative log-likelihood for the mapping $q \\mapsto \\sigma\\left(a_e \\cdot \\text{logit}(q) + b_e\\right)$ where $q \\in (0,1)$ is a per-sample uncalibrated explanation reliability.\n3. Apply the learned calibrators to the test split to produce calibrated detection probabilities and calibrated explanation reliability probabilities.\n4. Compute ECE for the calibrated detection probabilities and ECE for the calibrated explanation reliability probabilities on the test split, using $M = 10$ bins.\n\nExplanation reliability construction: For each sample $i$, you are given a nonnegative importance vector $\\mathbf{e}_i \\in \\mathbb{R}^d$ whose components sum to $1$. Let the fault-critical feature set be $F \\subseteq \\{0,1,\\dots,d-1\\}$. Define the uncalibrated explanation reliability $q_i = \\sum_{j \\in F} e_{i,j}$. Define the binary explanation correctness label $z_i$ as a Bernoulli draw with probability $p^{\\text{true}}_i$ specified per test case below.\n\nTest suite and data generation (all random draws must use the specified seeds for reproducibility):\n- Binning: Use $M = 10$ bins for ECE in all cases.\n- Confidence rounding: The final outputs must be decimal floats. No physical units or angles are involved.\n\nTest Case 1 (general case):\n- Calibration set size $N_{\\text{cal}} = 160$, test set size $N_{\\text{test}} = 160$, feature dimension $d = 5$, seeds: detection seed $7$, explanation seed $19$ for calibration and $29$ for test.\n- Detection scores:\n  - For $i = 0,1,\\dots,N_{\\text{cal}}-1$, let $t_i = \\frac{2i}{N_{\\text{cal}}-1} - 1$. Generate independent Gaussian noise $\\xi_i \\sim \\mathcal{N}(0,0.5^2)$ with seed $7$. Set $x^{\\text{cal}}_i = 0.9 t_i + 0.2 \\xi_i$. Define true probability $p^{\\text{true}}_i = \\sigma(1.8 x^{\\text{cal}}_i - 0.4)$. Draw $y^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$ with seed $7$.\n  - For test, repeat with $N_{\\text{test}}$, seed $17$ for noise and Bernoulli draws: $x^{\\text{test}}_i = 0.9 t_i + 0.2 \\xi_i$, $p^{\\text{true}}_i = \\sigma(1.8 x^{\\text{test}}_i - 0.4)$, $y^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$.\n- Explanation importance and correctness:\n  - Fault set $F = \\{1,4\\}$. For each sample $i$ and feature $j \\in \\{0,1,2,3,4\\}$, define\n    $$\n    w_{i,j} = \\cos(0.7 j + 0.03 i) + 0.2 \\sin\\!\\big(0.3 (j+1) (i+2)\\big) + 0.1,\n    $$\n    $u_{i,j} = \\exp(w_{i,j})$, and $e_{i,j} = \\frac{u_{i,j}}{\\sum_{k=0}^{d-1} u_{i,k}}$.\n  - For calibration, with seed $19$: $q^{\\text{cal}}_i = \\sum_{j \\in F} e_{i,j}$, true correctness probability $p^{\\text{true,exp}}_i = \\min\\big(1,\\max\\big(0, 0.1 + 0.8 q^{\\text{cal}}_i\\big)\\big)$, and $z^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$.\n  - For test, with seed $29$: define $q^{\\text{test}}_i$ analogously and $z^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$.\n\nTest Case 2 (near-perfect calibration):\n- Calibration set size $N_{\\text{cal}} = 180$, test set size $N_{\\text{test}} = 180$, feature dimension $d = 4$, seeds: detection seed $11$, explanation seed $13$ for both calibration and test.\n- Detection scores:\n  - Generate $t_i$ and noise $\\xi_i \\sim \\mathcal{N}(0,0.3^2)$ with seed $11$. Set $x^{\\text{cal}}_i = 0.8 t_i + 0.1 \\xi_i$, $p^{\\text{true}}_i = \\sigma(x^{\\text{cal}}_i)$, and $y^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$.\n  - For test, seed $13$, $x^{\\text{test}}_i = 0.8 t_i + 0.1 \\xi_i$, $p^{\\text{true}}_i = \\sigma(x^{\\text{test}}_i)$, and $y^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$.\n- Explanation importance and correctness:\n  - Fault set $F = \\{0,3\\}$. For each sample $i$ and feature $j \\in \\{0,1,2,3\\}$,\n    $$\n    w_{i,j} = 0.5 + 0.5\\left(\\sin(0.5 j) + \\cos\\!\\left(0.02 i + \\frac{j}{3}\\right)\\right),\n    $$\n    $u_{i,j} = \\exp(w_{i,j})$, $e_{i,j} = \\frac{u_{i,j}}{\\sum_{k=0}^{d-1} u_{i,k}}$, $q_i = \\sum_{j \\in F} e_{i,j}$.\n  - For both calibration and test, with seeds $13$: $p^{\\text{true,exp}}_i = q_i$, and $z_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$.\n\nTest Case 3 (strong miscalibration):\n- Calibration set size $N_{\\text{cal}} = 140$, test set size $N_{\\text{test}} = 140$, feature dimension $d = 6$, seeds: detection seed $23$, explanation seeds $31$ for calibration and $37$ for test.\n- Detection scores:\n  - Generate $t_i$ and noise $\\xi_i \\sim \\mathcal{N}(0,0.6^2)$ with seed $23$. Set $x^{\\text{cal}}_i = 1.0 t_i + 0.3 \\xi_i$, true probability $p^{\\text{true}}_i = \\sigma(3.0 x^{\\text{cal}}_i + 1.2)$, and $y^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$.\n  - For test, seed $29$ for detection draws, set $x^{\\text{test}}_i = 1.0 t_i + 0.3 \\xi_i$, $p^{\\text{true}}_i = \\sigma(3.0 x^{\\text{test}}_i + 1.2)$, and $y^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$.\n- Explanation importance and correctness:\n  - Fault set $F = \\{2,5\\}$. For each sample $i$ and feature $j \\in \\{0,1,2,3,4,5\\}$,\n    $$\n    w_{i,j} = 0.3 + \\sin(0.4 j i) + 0.1 \\cos(0.1 i - 0.2 j),\n    $$\n    $u_{i,j} = \\exp(w_{i,j})$, $e_{i,j} = \\frac{u_{i,j}}{\\sum_{k=0}^{d-1} u_{i,k}}$, $q_i = \\sum_{j \\in F} e_{i,j}$.\n  - For calibration, with seed $31$: $p^{\\text{true,exp}}_i = \\min\\left(1, \\max\\left(0, 0.05 + 0.9 q_i^{1/3}\\right)\\right)$ and $z^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$.\n  - For test, with seed $37$: define $p^{\\text{true,exp}}_i$ analogously and $z^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$.\n\nOutput specification:\n- For each test case, compute two numbers: the ECE for calibrated detection probabilities and the ECE for calibrated explanation reliability probabilities, both on the test split, using $M = 10$ bins.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, with each test case contributing two floats: $[\\text{ECE}_{\\text{det,1}},\\text{ECE}_{\\text{exp,1}},\\text{ECE}_{\\text{det,2}},\\text{ECE}_{\\text{exp,2}},\\text{ECE}_{\\text{det,3}},\\text{ECE}_{\\text{exp,3}}]$. Express all values as decimal floats rounded to six decimal places.",
            "solution": "The problem requires the design and implementation of an algorithm to calibrate probabilistic outputs from a fault detection classifier and an associated explanation module within a Cyber-Physical System (CPS) context. The calibration is to be performed using logistic scaling, and its effectiveness is evaluated using the Expected Calibration Error (ECE). The entire process is to be demonstrated on three distinct test cases with specified data generation procedures.\n\nThe solution proceeds in several stages:\n1.  Formal definition of the calibration models and the maximum likelihood estimation (MLE) procedure for learning their parameters.\n2.  Formal definition of the Expected Calibration Error (ECE) metric.\n3.  A step-by-step description of the algorithm to generate data, train calibrators, and evaluate their performance for each test case.\n\n**1. Probabilistic Modeling and Calibration**\n\nThe core of the problem lies in mapping uncalibrated scores to well-calibrated probabilities. A probability estimate $\\hat{p}$ for a binary event $Y \\in \\{0, 1\\}$ is well-calibrated if, for any value $p^* \\in [0, 1]$, the conditional expectation of the event given the prediction is equal to the prediction itself: $\\mathbb{E}[Y | \\hat{p} = p^*] = p^*$.\n\nWe are given two calibration tasks: one for detection scores and one for explanation reliability. Both are addressed using parametric calibration maps learned via MLE on a dedicated calibration dataset.\n\n**1.1. Detection Calibration**\n\nLet $x \\in \\mathbb{R}$ be the uncalibrated score from a fault detector, and let $y \\in \\{0, 1\\}$ be the true fault label. The proposed calibration model is the logistic (or Platt) scaling function:\n$$\n\\tilde{p}(x; a_d, b_d) = \\sigma(a_d x + b_d) = \\frac{1}{1 + \\exp(-(a_d x + b_d))}\n$$\nwhere $(a_d, b_d)$ are the scalar and bias parameters to be learned.\n\nGiven a calibration set $\\{(x_i^{\\text{cal}}, y_i^{\\text{cal}})\\}_{i=1}^{N_{\\text{cal}}}$, the parameters $(a_d, b_d)$ are found by minimizing the negative log-likelihood (NLL) of the data under the Bernoulli model. The NLL, which serves as the loss function, is:\n$$\n\\mathcal{L}(a_d, b_d) = - \\sum_{i=1}^{N_{\\text{cal}}} \\left[ y_i^{\\text{cal}} \\log(\\tilde{p}_i) + (1 - y_i^{\\text{cal}}) \\log(1 - \\tilde{p}_i) \\right]\n$$\nwhere $\\tilde{p}_i = \\sigma(a_d x_i^{\\text{cal}} + b_d)$. Substituting the definition of the sigmoid function and simplifying, we obtain a more numerically stable form for the loss:\n$$\n\\mathcal{L}(a_d, b_d) = \\sum_{i=1}^{N_{\\text{cal}}} \\left[ \\log(1 + \\exp(a_d x_i^{\\text{cal}} + b_d)) - y_i^{\\text{cal}}(a_d x_i^{\\text{cal}} + b_d) \\right]\n$$\nThis is a convex optimization problem, and its minimum can be found using gradient-based methods. The partial derivatives of the loss function are:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_d} = \\sum_{i=1}^{N_{\\text{cal}}} x_i^{\\text{cal}} (\\sigma(a_d x_i^{\\text{cal}} + b_d) - y_i^{\\text{cal}})\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_d} = \\sum_{i=1}^{N_{\\text{cal}}} (\\sigma(a_d x_i^{\\text{cal}} + b_d) - y_i^{\\text{cal}})\n$$\n\n**1.2. Explanation Calibration**\n\nLet $q \\in (0, 1)$ be the uncalibrated reliability score for an explanation, and let $z \\in \\{0, 1\\}$ be the true label for its correctness. The proposed calibration model is a monotone function applied in the logit space:\n$$\n\\tilde{r}(q; a_e, b_e) = \\sigma(a_e \\cdot \\text{logit}(q) + b_e)\n$$\nwhere $\\text{logit}(q) = \\log\\left(\\frac{q}{1-q}\\right)$. This model structure is equivalent to the detection calibration model if we define a new feature $x' = \\text{logit}(q)$. The model becomes $\\tilde{r}(x'; a_e, b_e) = \\sigma(a_e x' + b_e)$.\n\nGiven a calibration set $\\{(q_i^{\\text{cal}}, z_i^{\\text{cal}})\\}_{i=1}^{N_{\\text{cal}}}$, we first transform the inputs: $x'_i = \\text{logit}(q_i^{\\text{cal}})$. The parameters $(a_e, b_e)$ are then found by minimizing the same NLL function as before, but with inputs $\\{(x'_i, z_i^{\\text{cal}})\\}$:\n$$\n\\mathcal{L}(a_e, b_e) = \\sum_{i=1}^{N_{\\text{cal}}} \\left[ \\log(1 + \\exp(a_e x'_i + b_e)) - z_i^{\\text{cal}}(a_e x'_i + b_e) \\right]\n$$\nThe gradients with respect to $a_e$ and $b_e$ are analogous to those for detection calibration.\n\n**2. Evaluation via Expected Calibration Error (ECE)**\n\nECE measures the discrepancy between confidence and accuracy. Given a test set of $N$ predictions $\\{p_i\\}_{i=1}^N$ and corresponding true labels $\\{y_i\\}_{i=1}^N$, the predictions are partitioned into $M$ equally spaced bins $B_m$ for $m=1, \\dots, M$. The bin $B_m$ contains all samples $i$ for which the predicted probability $p_i$ falls into the interval $I_m = \\left(\\frac{m-1}{M}, \\frac{m}{M}\\right]$. Note: the problem states a half-open interval $\\left[\\frac{m-1}{M}, \\frac{m}{M}\\right)$; we will implement it as such.\n\nFor each bin $B_m$ with $n_m = |B_m|  0$, we calculate:\n- The average confidence: $\\hat{c}_m = \\frac{1}{n_m} \\sum_{i \\in B_m} p_i$\n- The empirical accuracy: $\\hat{a}_m = \\frac{1}{n_m} \\sum_{i \\in B_m} y_i$\n\nThe ECE is the weighted average of the absolute differences between accuracy and confidence across all bins:\n$$\n\\text{ECE} = \\sum_{m=1}^M \\frac{n_m}{N} \\left| \\hat{a}_m - \\hat{c}_m \\right|\n$$\nIf a bin is empty ($n_m = 0$), its contribution to the sum is $0$. A lower ECE indicates better calibration.\n\n**3. Algorithmic Procedure**\n\nThe overall algorithm proceeds by iterating through each of the three specified test cases. For each case:\n\n1.  **Data Generation**: Generate the calibration and test datasets according to the specified parameters ($N_{\\text{cal}}, N_{\\text{test}}, d$), formulas, and random number generator seeds. This involves creating:\n    - Detection data: $(x^{\\text{cal}}, y^{\\text{cal}})$ and $(x^{\\text{test}}, y^{\\text{test}})$.\n    - Explanation data: from the importance vectors $\\mathbf{e}_i$, derive the uncalibrated reliabilities $q_i = \\sum_{j \\in F} e_{i,j}$ and the correctness labels $z_i$. This yields $(q^{\\text{cal}}, z^{\\text{cal}})$ and $(q^{\\text{test}}, z^{\\text{test}})$. All random draws must use the specified seeds for reproducibility.\n\n2.  **Detection Calibrator Training**:\n    - Use the calibration data $(x^{\\text{cal}}, y^{\\text{cal}})$.\n    - Numerically solve the optimization problem $\\arg\\min_{a_d, b_d} \\mathcal{L}(a_d, b_d)$ to find the optimal parameters $\\hat{a}_d, \\hat{b}_d$. This is done using a gradient-based solver.\n\n3.  **Explanation Calibrator Training**:\n    - Use the calibration data $(q^{\\text{cal}}, z^{\\text{cal}})$.\n    - First, compute the transformed features $x'^{\\text{cal}}_i = \\text{logit}(q^{\\text{cal}}_i)$.\n    - Numerically solve the optimization problem $\\arg\\min_{a_e, b_e} \\mathcal{L}(a_e, b_e)$ on the data $(x'^{\\text{cal}}, z^{\\text{cal}})$ to find the optimal parameters $\\hat{a}_e, \\hat{b}_e$.\n\n4.  **Evaluation on Test Set**:\n    - **Detection ECE**:\n        - Apply the learned detection calibrator to the test scores: $\\tilde{p}^{\\text{test}}_i = \\sigma(\\hat{a}_d x^{\\text{test}}_i + \\hat{b}_d)$.\n        - Compute the ECE on the set of calibrated probabilities $\\{\\tilde{p}^{\\text{test}}_i\\}$ and true labels $\\{y^{\\text{test}}_i\\}$ using $M=10$ bins.\n    - **Explanation ECE**:\n        - Apply the learned explanation calibrator to the test reliabilities: $\\tilde{r}^{\\text{test}}_i = \\sigma(\\hat{a}_e \\cdot \\text{logit}(q^{\\text{test}}_i) + \\hat{b}_e)$.\n        - Compute the ECE on the set of calibrated probabilities $\\{\\tilde{r}^{\\text{test}}_i\\}$ and true labels $\\{z^{\\text{test}}_i\\}$ using $M=10$ bins.\n\n5.  **Output**: Collect the two ECE values for the current test case. After processing all cases, format the results as a single list of six floating-point numbers, rounded to six decimal places, and print them in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit as sigmoid, logit\n\ndef compute_ece(probs, labels, M=10):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE).\n\n    Args:\n        probs (np.ndarray): Array of predicted probabilities.\n        labels (np.ndarray): Array of true binary labels (0 or 1).\n        M (int): Number of bins to use.\n\n    Returns:\n        float: The ECE value.\n    \"\"\"\n    if len(probs) == 0:\n        return 0.0\n\n    bins = np.linspace(0.0, 1.0, M + 1)\n    bin_lowers = bins[:-1]\n    bin_uppers = bins[1:]\n\n    ece = 0.0\n    N = len(probs)\n\n    for i in range(M):\n        in_bin = np.logical_and(probs = bin_lowers[i], probs  bin_uppers[i])\n        # Special case for the last bin to include 1.0\n        if i == M - 1:\n            in_bin = np.logical_or(in_bin, probs == bin_uppers[i])\n        \n        n_m = np.sum(in_bin)\n\n        if n_m  0:\n            bin_probs = probs[in_bin]\n            bin_labels = labels[in_bin]\n            \n            avg_confidence = np.mean(bin_probs)\n            accuracy = np.mean(bin_labels)\n            \n            ece += (n_m / N) * np.abs(accuracy - avg_confidence)\n\n    return ece\n\ndef train_calibrator(features, labels):\n    \"\"\"\n    Trains a logistic calibration model by minimizing negative log-likelihood.\n\n    Args:\n        features (np.ndarray): Input features for the model.\n        labels (np.ndarray): True binary labels.\n\n    Returns:\n        tuple[float, float]: The learned parameters (a, b).\n    \"\"\"\n    def nll_and_grad(params, X, y):\n        a, b = params\n        z = a * X + b\n        p = sigmoid(z)\n        \n        # Negative Log-Likelihood\n        # Using a more stable formulation to avoid log(0)\n        log_likelihood = np.sum(y * z - np.log(1 + np.exp(z)))\n        nll = -log_likelihood\n\n        # Gradient\n        grad_a = -np.sum(X * (y - p))\n        grad_b = -np.sum(y - p)\n        \n        return nll, np.array([grad_a, grad_b])\n\n    # Initial guess for parameters (a, b)\n    initial_params = np.array([1.0, 0.0])\n\n    res = minimize(\n        fun=nll_and_grad,\n        x0=initial_params,\n        args=(features, labels),\n        jac=True,  # We provide the gradient\n        method='L-BFGS-B'\n    )\n    \n    a_opt, b_opt = res.x\n    return a_opt, b_opt\n\ndef generate_test_case_data(case_config):\n    \"\"\"\n    Generates all data for a single test case.\n    \"\"\"\n    case_id = case_config['id']\n    N_cal, N_test, d = case_config['N_cal'], case_config['N_test'], case_config['d']\n    \n    data = {}\n    \n    # Common i-indices and t-sequences\n    i_cal = np.arange(N_cal)\n    t_cal = (2 * i_cal / (N_cal - 1)) - 1\n    i_test = np.arange(N_test)\n    t_test = (2 * i_test / (N_test - 1)) - 1\n    \n    # --- Test Case Specifics ---\n    if case_id == 1:\n        # Detection Data\n        rng_det_cal = np.random.default_rng(7)\n        xi_cal = rng_det_cal.normal(0, 0.5, size=N_cal)\n        data['x_cal'] = 0.9 * t_cal + 0.2 * xi_cal\n        p_true_cal = sigmoid(1.8 * data['x_cal'] - 0.4)\n        data['y_cal'] = rng_det_cal.binomial(1, p_true_cal)\n\n        rng_det_test = np.random.default_rng(17)\n        xi_test = rng_det_test.normal(0, 0.5, size=N_test)\n        data['x_test'] = 0.9 * t_test + 0.2 * xi_test\n        p_true_test = sigmoid(1.8 * data['x_test'] - 0.4)\n        data['y_test'] = rng_det_test.binomial(1, p_true_test)\n        \n        # Explanation Data\n        F = {1, 4}\n        j = np.arange(d)\n        w_cal = np.cos(0.7 * j[None, :] + 0.03 * i_cal[:, None]) + 0.2 * np.sin(0.3 * (j[None, :] + 1) * (i_cal[:, None] + 2)) + 0.1\n        u_cal = np.exp(w_cal)\n        e_cal = u_cal / u_cal.sum(axis=1, keepdims=True)\n        data['q_cal'] = e_cal[:, list(F)].sum(axis=1)\n        p_true_exp_cal = np.clip(0.1 + 0.8 * data['q_cal'], 0, 1)\n        rng_exp_cal = np.random.default_rng(19)\n        data['z_cal'] = rng_exp_cal.binomial(1, p_true_exp_cal)\n\n        w_test = np.cos(0.7 * j[None, :] + 0.03 * i_test[:, None]) + 0.2 * np.sin(0.3 * (j[None, :] + 1) * (i_test[:, None] + 2)) + 0.1\n        u_test = np.exp(w_test)\n        e_test = u_test / u_test.sum(axis=1, keepdims=True)\n        data['q_test'] = e_test[:, list(F)].sum(axis=1)\n        p_true_exp_test = np.clip(0.1 + 0.8 * data['q_test'], 0, 1)\n        rng_exp_test = np.random.default_rng(29)\n        data['z_test'] = rng_exp_test.binomial(1, p_true_exp_test)\n\n    elif case_id == 2:\n        # Detection Data\n        rng_det_cal = np.random.default_rng(11)\n        xi_cal = rng_det_cal.normal(0, 0.3, size=N_cal)\n        data['x_cal'] = 0.8 * t_cal + 0.1 * xi_cal\n        p_true_cal = sigmoid(data['x_cal'])\n        data['y_cal'] = rng_det_cal.binomial(1, p_true_cal)\n\n        rng_det_test = np.random.default_rng(13)\n        xi_test = rng_det_test.normal(0, 0.3, size=N_test)\n        data['x_test'] = 0.8 * t_test + 0.1 * xi_test\n        p_true_test = sigmoid(data['x_test'])\n        data['y_test'] = rng_det_test.binomial(1, p_true_test)\n\n        # Explanation Data\n        F = {0, 3}\n        j = np.arange(d)\n        rng_exp = np.random.default_rng(13)\n\n        w_cal = 0.5 + 0.5 * (np.sin(0.5 * j[None, :]) + np.cos(0.02 * i_cal[:, None] + j[None, :] / 3))\n        u_cal = np.exp(w_cal)\n        e_cal = u_cal / u_cal.sum(axis=1, keepdims=True)\n        data['q_cal'] = e_cal[:, list(F)].sum(axis=1)\n        data['z_cal'] = rng_exp.binomial(1, data['q_cal'])\n\n        w_test = 0.5 + 0.5 * (np.sin(0.5 * j[None, :]) + np.cos(0.02 * i_test[:, None] + j[None, :] / 3))\n        u_test = np.exp(w_test)\n        e_test = u_test / u_test.sum(axis=1, keepdims=True)\n        data['q_test'] = e_test[:, list(F)].sum(axis=1)\n        data['z_test'] = rng_exp.binomial(1, data['q_test'])\n\n    elif case_id == 3:\n        # Detection Data\n        rng_det_cal = np.random.default_rng(23)\n        xi_cal = rng_det_cal.normal(0, 0.6, size=N_cal)\n        data['x_cal'] = 1.0 * t_cal + 0.3 * xi_cal\n        p_true_cal = sigmoid(3.0 * data['x_cal'] + 1.2)\n        data['y_cal'] = rng_det_cal.binomial(1, p_true_cal)\n\n        rng_det_test = np.random.default_rng(29)\n        xi_test = rng_det_test.normal(0, 0.6, size=N_test)\n        data['x_test'] = 1.0 * t_test + 0.3 * xi_test\n        p_true_test = sigmoid(3.0 * data['x_test'] + 1.2)\n        data['y_test'] = rng_det_test.binomial(1, p_true_test)\n\n        # Explanation Data\n        F = {2, 5}\n        j = np.arange(d)\n        \n        w_cal = 0.3 + np.sin(0.4 * j[None, :] * i_cal[:, None]) + 0.1 * np.cos(0.1 * i_cal[:, None] - 0.2 * j[None, :])\n        u_cal = np.exp(w_cal)\n        e_cal = u_cal / u_cal.sum(axis=1, keepdims=True)\n        data['q_cal'] = e_cal[:, list(F)].sum(axis=1)\n        p_true_exp_cal = np.clip(0.05 + 0.9 * np.power(data['q_cal'], 1/3), 0, 1)\n        rng_exp_cal = np.random.default_rng(31)\n        data['z_cal'] = rng_exp_cal.binomial(1, p_true_exp_cal)\n\n        w_test = 0.3 + np.sin(0.4 * j[None, :] * i_test[:, None]) + 0.1 * np.cos(0.1 * i_test[:, None] - 0.2 * j[None, :])\n        u_test = np.exp(w_test)\n        e_test = u_test / u_test.sum(axis=1, keepdims=True)\n        data['q_test'] = e_test[:, list(F)].sum(axis=1)\n        p_true_exp_test = np.clip(0.05 + 0.9 * np.power(data['q_test'], 1/3), 0, 1)\n        rng_exp_test = np.random.default_rng(37)\n        data['z_test'] = rng_exp_test.binomial(1, p_true_exp_test)\n\n    return data\n\ndef solve():\n    test_cases = [\n        {'id': 1, 'N_cal': 160, 'N_test': 160, 'd': 5},\n        {'id': 2, 'N_cal': 180, 'N_test': 180, 'd': 4},\n        {'id': 3, 'N_cal': 140, 'N_test': 140, 'd': 6},\n    ]\n\n    results = []\n    M_bins = 10\n\n    for case_config in test_cases:\n        data = generate_test_case_data(case_config)\n        \n        # --- 1. Detection Calibration ---\n        a_d, b_d = train_calibrator(data['x_cal'], data['y_cal'])\n        calibrated_probs_d = sigmoid(a_d * data['x_test'] + b_d)\n        ece_d = compute_ece(calibrated_probs_d, data['y_test'], M_bins)\n        \n        # --- 2. Explanation Calibration ---\n        # Note: q values are guaranteed to be in (0, 1) by construction\n        features_exp_cal = logit(data['q_cal'])\n        a_e, b_e = train_calibrator(features_exp_cal, data['z_cal'])\n        features_exp_test = logit(data['q_test'])\n        calibrated_probs_e = sigmoid(a_e * features_exp_test + b_e)\n        ece_e = compute_ece(calibrated_probs_e, data['z_test'], M_bins)\n        \n        results.extend([ece_d, ece_e])\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}