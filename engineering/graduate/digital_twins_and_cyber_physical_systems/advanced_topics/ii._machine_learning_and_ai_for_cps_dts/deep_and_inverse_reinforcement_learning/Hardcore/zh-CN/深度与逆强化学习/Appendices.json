{
    "hands_on_practices": [
        {
            "introduction": "逆强化学习 (IRL) 的核心目标是从观察到的专家行为中恢复其潜在的奖励函数。本实践为此过程提供了一个深刻的几何直觉 。通过将问题置于特征期望的空间中，我们将看到最优奖励函数如何对应于一个能将专家策略与其他候选策略分离开来的超平面，从而揭示偏好学习的本质。",
            "id": "4212767",
            "problem": "一个智能建筑的信息物理系统 (CPS) 数字孪生通过一个由深度强化学习 (DRL) 学得的控制器来操作供暖、通风和空调执行器。状态被映射为特征，这些特征将能源、舒适度和执行器磨损聚合为一个维度为 $3$ 的特征向量。该控制问题被建模为一个马尔可夫决策过程 (MDP)，其具有未知的线性奖励 $r(s,a) = w^{\\top} \\phi(s,a)$，其中 $w \\in \\mathbb{R}^{3}$ 是待通过逆强化学习 (IRL) 恢复的奖励权重。特征期望是根据一个固定的折扣因子（未公开且推导中无需使用）从长期折扣轨迹中计算得出，并经过归一化处理，以使每个坐标在不同策略间具有可比性。\n\n给你专家策略的特征期望和三个候选 DRL 策略的特征期望，每个都在 $\\mathbb{R}^{3}$ 中：\n$$\n\\mu_{E} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\n\\mu_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n考虑凸包 $\\mathcal{C} = \\mathrm{conv}\\{\\mu_{E}, \\mu_{1}, \\mu_{2}, \\mu_{3}\\}$。仅使用凸分析和几何学的基本原理（凸包的定义、欧几里得范数、欧几里得投影和分离超平面定理），通过以下步骤恢复奖励权重 $w$：\n- 首先，计算原点 $0 \\in \\mathbb{R}^{3}$ 在 $\\mathcal{C}$ 上的欧几里得投影。\n- 其次，使用分离超平面论证来确定使原点与 $\\mathcal{C}$ 之间间隔最大化的单位法向量，并将此单位法向量确定为恢复的奖励权重 $w$。\n\n将您关于 $w$ 的最终答案表示为单个闭式解析表达式。无需四舍五入。不涉及物理单位。",
            "solution": "首先验证该问题具有科学依据、适定、客观且自洽。它在逆强化学习 (IRL) 的背景下提出了一个标准的几何问题，提供了所有必要的数据和清晰、数学上合理的求解步骤。未检测到任何缺陷。\n\n问题的核心是找到奖励权重向量 $w \\in \\mathbb{R}^{3}$。指定的步骤包括两个主要部分：首先，计算原点在凸集上的欧几里得投影；其次，确定与此投影相关的单位法向量。\n\n给定的特征期望是：\n$$\n\\mu_{E} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\n\\mu_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n我们感兴趣的凸集是这四个点的凸包：\n$$\n\\mathcal{C} = \\mathrm{conv}\\{\\mu_{E}, \\mu_{1}, \\mu_{2}, \\mu_{3}\\}.\n$$\n该集合是 $\\mathbb{R}^{3}$ 中的一个四面体，其顶点为 $\\mu_{E}$、$\\mu_{1}$、$\\mu_{2}$ 和 $\\mu_{3}$。\n\n**步骤 1：计算原点在 $\\mathcal{C}$ 上的欧几里得投影。**\n\n我们的任务是找到集合 $\\mathcal{C}$ 中离原点 $0 \\in \\mathbb{R}^{3}$ 最近的点 $p$。这个点最小化了欧几里得范数，或者等价地，最小化了欧几里得范数的平方。问题是找到：\n$$\np = \\arg\\min_{x \\in \\mathcal{C}} \\|x\\|_{2}^{2}.\n$$\n由于 $\\mathcal{C}$ 是一个闭凸集，所以存在唯一的解 $p$。可以通过检查原点在四面体 $\\mathcal{C}$ 的面、边和顶点上的投影来寻找 $p$。\n\n让我们考虑由顶点 $\\mu_{1}$、$\\mu_{2}$ 和 $\\mu_{3}$ 构成的四面体的面 $F$：\n$$\nF = \\mathrm{conv}\\{\\mu_{1}, \\mu_{2}, \\mu_{3}\\} = \\mathrm{conv}\\left\\{\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\}.\n$$\n$F$ 的仿射包中的任意点 $x = (x_1, x_2, x_3)^{\\top}$ 满足方程 $x_1 + x_2 + x_3 = 1$。原点在该平面上的投影是平面上与该平面法向量 $n = (1, 1, 1)^{\\top}$ 共线的点。设此投影为 $p_{F} = k n$，其中 $k$ 为某个标量。代入平面方程：\n$$\nk \\cdot 1 + k \\cdot 1 + k \\cdot 1 = 1 \\implies 3k = 1 \\implies k = \\frac{1}{3}.\n$$\n所以，原点在 $F$ 的仿射包上的投影是 $p_{F} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$。\n\n为了确认该点位于面 $F$ 内，我们必须检查它是否能表示为 $F$ 的顶点的凸组合。我们寻求系数 $\\alpha_1, \\alpha_2, \\alpha_3 \\ge 0$ 使得 $\\sum_{i=1}^{3} \\alpha_i = 1$ 并且：\n$$\n\\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\alpha_1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\alpha_3 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix}.\n$$\n这得到 $\\alpha_1 = \\alpha_2 = \\alpha_3 = 1/3$。这些系数是非负的，且和为 1。因此，点 $p_{F} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$ 位于面 $F$ 内，并且是 $F$ 中离原点最近的点。\n\n现在，我们必须验证这个点是到整个四面体 $\\mathcal{C}$ 上的投影。让我们的候选投影为 $p = p_{F} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$。一个点 $p \\in \\mathcal{C}$ 是原点在 $\\mathcal{C}$ 上的唯一投影，当且仅当它满足变分不等式：\n$$\n(x - p)^{\\top}(0 - p) \\le 0 \\quad \\forall x \\in \\mathcal{C},\n$$\n化简为 $p^{\\top}(x - p) \\ge 0$。\n由于 $\\mathcal{C}$ 中的任何点 $x$ 都是顶点的凸组合，即 $x = \\alpha_{E}\\mu_{E} + \\alpha_{1}\\mu_{1} + \\alpha_{2}\\mu_{2} + \\alpha_{3}\\mu_{3}$，其中 $\\alpha_{i} \\ge 0$ 且 $\\sum \\alpha_{i} = 1$。根据点积的线性性质，我们只需要检查 $\\mathcal{C}$ 的顶点的条件即可。\n对于 $x \\in \\{\\mu_1, \\mu_2, \\mu_3\\}$，点 $x$ 在面 $F$ 中。由于 $p$ 是到 $F$ 上的投影，条件 $p^{\\top}(x - p) \\ge 0$ 对于所有 $x \\in F$ 已经满足。\n我们只需要检查顶点 $\\mu_{E}$ 的条件：\n$$\np^{\\top}(\\mu_{E} - p) = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}^{\\top} \\left( \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} \\right) = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}^{\\top} \\begin{pmatrix} 2/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}.\n$$\n$$\np^{\\top}(\\mu_{E} - p) = \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{1}{3} \\cdot \\frac{2}{3} = 3 \\cdot \\frac{2}{9} = \\frac{2}{3}.\n$$\n由于 $\\frac{2}{3} \\ge 0$，该条件对 $\\mu_{E}$ 成立。因为 $\\mathcal{C}$ 中的所有点都是其顶点的凸组合，所以该条件对所有 $x \\in \\mathcal{C}$ 均成立。因此，原点在 $\\mathcal{C}$ 上的欧几里得投影确实是 $p = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$。\n\n**步骤 2：使用分离超平面论证来确定 $w$。**\n\n问题指出，恢复的奖励权重 $w$ 是使原点与 $\\mathcal{C}$ 之间间隔最大化的单位法向量。根据分离超平面定理，由于原点 $0$ 和凸集 $\\mathcal{C}$ 不相交，存在一个超平面将它们分离。提供最大分离间隔的超平面是与连接两个集合最近点的线段垂直的那个。在我们的例子中，这两个点是原点 $0$ 和它在 $\\mathcal{C}$ 上的投影 $p$。\n\n这个超平面的法向量就是 $p - 0 = p$。问题要求的是*单位*法向量，我们将其确定为 $w$。\n$$\nw = \\frac{p}{\\|p\\|_{2}}.\n$$\n我们有 $p = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$。我们计算其欧几里得范数：\n$$\n\\|p\\|_{2} = \\sqrt{\\left(\\frac{1}{3}\\right)^{2} + \\left(\\frac{1}{3}\\right)^{2} + \\left(\\frac{1}{3}\\right)^{2}} = \\sqrt{3 \\cdot \\frac{1}{9}} = \\sqrt{\\frac{1}{3}} = \\frac{1}{\\sqrt{3}}.\n$$\n最后，我们计算单位向量 $w$：\n$$\nw = \\frac{1}{1/\\sqrt{3}} \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\sqrt{3} \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\end{pmatrix}.\n$$\n这个向量 $w$ 代表了恢复的三个特征（能源、舒适度和执行器磨损）的奖励权重。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "无论是通过逆强化学习还是其他方法得到的网络物理系统模型，都不可避免地存在不确定性。本实践探讨如何在这种不确定性下设计一个鲁棒的控制器 。你将应用极小化极大框架，在模型不确定性边界所允许的最坏情况下，寻找一个能最大化性能的策略，这是确保系统安全的关键一步。",
            "id": "4212778",
            "problem": "考虑一个信息物理系统 (CPS) 的数字孪生 (DT)，它被建模为一个双状态马尔可夫决策过程 (MDP)，其中标称操作模式为状态 $0$，故障安全模式为状态 $1$。一个通过深度强化学习 (DRL) 训练的控制器在状态 $0$ 使用一个平稳随机策略，该策略混合了两个动作 $a$ 和 $b$。在状态 $1$（这是一个吸收状态）中没有可用的控制。奖励为 $r(0,a)=2.0$，$r(0,b)=0.8$ 和 $r(1,\\cdot)=0$。折扣因子为 $\\gamma=0.95$。由于环境可变性和模型失配，转移概率是不确定的，但受限于通过逆强化学习 (IRL) 从操作日志中识别出的区间集。具体来说，当处于状态 $0$ 时，动作 $a$ 导致以概率 $q_a \\in [0.35,\\,0.50]$ 转移到状态 $1$，而动作 $b$ 导致以概率 $q_b \\in [0.15,\\,0.25]$ 转移到状态 $1$。在动作 $a$ 下保持在状态 $0$ 的概率是 $1-q_a$，在动作 $b$ 下是 $1-q_b$。状态 $1$ 是吸收状态，其转移概率为 $q(1 \\to 1)=1$。\n\n设状态 $0$ 的平稳随机策略由 $p \\in [0,1]$ 参数化，其中动作 $a$ 以概率 $p$ 被选择，动作 $b$ 以概率 $1-p$ 被选择。在用于 MDP 的极小化极大鲁棒控制范式下，策略寻求在所有与区间不确定性一致的转移核中，最大化状态 $0$ 的最坏情况折扣价值。形式上，一个策略 $\\pi$ 的鲁棒价值定义为 $\\min_{P \\in \\mathcal{P}} V^{\\pi}_{P}(0)$，其中 $V^{\\pi}_{P}(0)$ 是在转移核 $P$ 下状态 $0$ 的通常折扣回报，而 $\\mathcal{P}$ 是与上述矩形区间一致的所有转移核的集合。\n\n仅从马尔可夫决策过程 (MDP) 的基本定义、折扣回报以及带矩形不确定性集的极小化极大鲁棒控制出发，并利用“对于给定策略，最坏情况的转移选择将在每一步最小化期望未来价值”这一事实，推导出状态 $0$ 的鲁棒折扣价值作为 $p$ 的函数，然后求解在 $p \\in [0,1]$ 上的相应极小化极大优化问题，以获得鲁棒最优混合概率 $p^{\\star}$。请提供 $p^{\\star}$ 作为您的最终答案。无需四舍五入。最终答案必须是一个实数。",
            "solution": "该问题陈述被评估为有效。这是一个在鲁棒马尔可夫决策过程（MDP）领域内的适定问题，这是控制理论和强化学习中的一个标准课题。所有必要的参数——状态、动作、奖励、折扣因子和不确定性模型（转移概率的矩形集）——都得到了清晰的定义，并且在数学和科学上是一致的。其目标是在极小化极大准则下找到一个最优的随机策略，这是一个标准的表述方式。其中没有矛盾、歧义或事实不健全之处。\n\n我们的任务是为一个双状态 MDP 中的平稳随机策略找到最优混合概率 $p^{\\star}$。该问题是在极小化极大鲁棒控制范式下构建的。\n\n状态空间为 $S = \\{0, 1\\}$。状态 $0$ 是标称操作模式，状态 $1$ 是一个吸收性的故障安全模式。\n状态 $0$ 的动作空间是 $A_0 = \\{a, b\\}$。在状态 $1$ 中，没有可用的控制。\n奖励给定为 $r(0,a) = 2.0$，$r(0,b) = 0.8$，以及 $r(s=1, \\cdot) = 0$。\n折扣因子为 $\\gamma = 0.95$。\n\n在状态 $0$ 的平稳随机策略，记为 $\\pi_p$，由概率 $p \\in [0,1]$ 定义：\n$$ \\pi_p(a|0) = p $$\n$$ \\pi_p(b|0) = 1-p $$\n\n转移概率是不确定的，并且属于某个区间。令 $P(s'|s,u)$ 为在动作 $u$ 下从状态 $s$ 转移到 $s'$ 的概率。\n对于状态 $0$ 的动作 $a$：\n$$ P(1|0,a) = q_a \\in [0.35, 0.50] $$\n$$ P(0|0,a) = 1 - q_a $$\n对于状态 $0$ 的动作 $b$：\n$$ P(1|0,b) = q_b \\in [0.15, 0.25] $$\n$$ P(0|0,b) = 1 - q_b $$\n状态 $1$ 是吸收状态：$P(1|1,\\cdot) = 1$。所有有效的转移核的集合记为 $\\mathcal{P}$。\n\n目标是找到解决以下极小化极大问题的 $p^{\\star}$：\n$$ p^{\\star} = \\arg\\max_{p \\in [0,1]} \\left( \\min_{P \\in \\mathcal{P}} V^{\\pi_p}_{P}(0) \\right) $$\n其中 $V^{\\pi_p}_{P}(0)$ 是策略 $\\pi_p$ 和特定转移核 $P$ 下状态 $0$ 的价值函数。\n\n令 $V(s)$ 为状态 $s$ 的鲁棒价值函数，即 $V(s) = \\min_{P \\in \\mathcal{P}} V^{\\pi_p}_{P}(s)$。\n首先，我们确定状态 $1$ 的价值。由于它是一个奖励为零的吸收状态，其价值总是零，无论策略或转移概率如何。\n$$ V(1) = r(1) + \\gamma \\sum_{s' \\in \\{0,1\\}} P(s'|1) V(s') = 0 + \\gamma \\cdot (1 \\cdot V(1)) $$\n$$ V(1) (1-\\gamma) = 0 $$\n由于 $\\gamma=0.95 \\neq 1$，我们必然有 $V(1) = 0$。\n\n现在，我们写出状态 $0$ 的鲁棒贝尔曼方程。给定策略 $\\pi_p$ 的鲁棒价值是通过考虑对于采取的任何动作，自然会对抗性地从允许的集合中选择转移概率，以最小化智能体的期望未来价值来找到的。\n在策略 $\\pi_p$ 下，状态 $0$ 的价值函数是关于 $\\pi_p$ 中动作的期望：\n$$ V_p(0) = \\pi_p(a|0) \\left( r(0,a) + \\gamma \\min_{P(\\cdot|0,a)} \\sum_{s' \\in \\{0,1\\}} P(s'|0,a) V_p(s') \\right) + \\pi_p(b|0) \\left( r(0,b) + \\gamma \\min_{P(\\cdot|0,b)} \\sum_{s' \\in \\{0,1\\}} P(s'|0,b) V_p(s') \\right) $$\n这个表达式捕捉了极小化极大的结构：策略玩家最大化，而对于策略玩家的每一步，一个对抗性的自然玩家最小化。\n\n让我们评估每个动作的内部最小化项。我们需要 $V_p(0)$ 的值，但我们可以首先确定它的符号。由于所有奖励 $r(s,u)$ 都是非负的，奖励的折扣和（即价值函数）也必须是非负的。因此，$V_p(0) \\ge 0$。\n\n对于动作 $a$：\n自然要最小化的项是期望未来价值：\n$$ \\mathbb{E}_{P(\\cdot|0,a)}[V_p] = P(0|0,a)V_p(0) + P(1|0,a)V_p(1) = (1-q_a)V_p(0) + q_a \\cdot 0 = (1-q_a)V_p(0) $$\n自然选择 $q_a \\in [0.35, 0.50]$ 来最小化 $(1-q_a)V_p(0)$。因为 $V_p(0) \\ge 0$，这等价于最大化 $q_a$。最坏情况的概率是区间的上界：\n$$ q_{a,\\text{worst}} = 0.50 $$\n\n对于动作 $b$：\n类似地，自然要最小化的项是：\n$$ \\mathbb{E}_{P(\\cdot|0,b)}[V_p] = (1-q_b)V_p(0) $$\n自然选择 $q_b \\in [0.15, 0.25]$ 来最小化这个值。这是通过最大化 $q_b$ 来实现的。最坏情况的概率是：\n$$ q_{b,\\text{worst}} = 0.25 $$\n\n现在我们可以写出策略 $\\pi_p$ 的鲁棒价值的贝尔曼方程，我们记为 $J(p) = V_p(0)$。\n$$ J(p) = p \\left[ r(0,a) + \\gamma (1-q_{a,\\text{worst}}) J(p) \\right] + (1-p) \\left[ r(0,b) + \\gamma (1-q_{b,\\text{worst}}) J(p) \\right] $$\n我们需要解出关于 $J(p)$ 的这个方程。\n$$ J(p) = p \\cdot r(0,a) + (1-p) \\cdot r(0,b) + \\gamma \\left[ p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right] J(p) $$\n$$ J(p) \\left( 1 - \\gamma \\left[ p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right] \\right) = p \\cdot r(0,a) + (1-p) \\cdot r(0,b) $$\n$$ J(p) = \\frac{p \\cdot r(0,a) + (1-p) \\cdot r(0,b)}{1 - \\gamma \\left( p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right)} $$\n现在，我们代入数值：$r(0,a)=2.0$，$r(0,b)=0.8$，$\\gamma=0.95$，$q_{a,\\text{worst}}=0.50$，$q_{b,\\text{worst}}=0.25$。\n\n分子是：\n$$ N(p) = p(2.0) + (1-p)(0.8) = 2.0p + 0.8 - 0.8p = 1.2p + 0.8 $$\n分母是：\n$$ D(p) = 1 - 0.95 \\left[ p(1-0.50) + (1-p)(1-0.25) \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.50p + 0.75(1-p) \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.50p + 0.75 - 0.75p \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.75 - 0.25p \\right] $$\n$$ D(p) = 1 - (0.95 \\cdot 0.75) + (0.95 \\cdot 0.25)p $$\n$$ D(p) = 1 - 0.7125 + 0.2375p $$\n$$ D(p) = 0.2875 + 0.2375p $$\n\n所以，我们想在 $p \\in [0,1]$ 上最大化的函数是：\n$$ J(p) = \\frac{1.2p + 0.8}{0.2375p + 0.2875} $$\n这是 $p$ 的一个线性分式变换（或单应函数）。为了找到它的最大值，我们分析它关于 $p$ 的导数。\n令 $J(p) = \\frac{N(p)}{D(p)}$。导数是 $J'(p) = \\frac{N'(p)D(p) - N(p)D'(p)}{[D(p)]^2}$。\n分子和分母的导数是：\n$$ N'(p) = 1.2 $$\n$$ D'(p) = 0.2375 $$\n$J'(p)$ 的符号由其分子的符号决定，因为 $[D(p)]^2  0$。\n$J'(p)$ 的分子是：\n$$ (1.2)(0.2875 + 0.2375p) - (1.2p + 0.8)(0.2375) $$\n$$ = (1.2 \\cdot 0.2875) + (1.2 \\cdot 0.2375)p - (1.2 \\cdot 0.2375)p - (0.8 \\cdot 0.2375) $$\n$$ = 1.2 \\cdot 0.2875 - 0.8 \\cdot 0.2375 $$\n$$ = 0.345 - 0.19 $$\n$$ = 0.155 $$\n由于导数的分子是一个正常数 ($0.155$)，我们有 $J'(p)  0$ 对于其定义域内所有的 $p$。这意味着 $J(p)$ 是 $p$ 的一个严格单调递增函数。\n\n为了在闭区间 $[0,1]$ 上最大化一个单调递增函数，我们必须为变量选择可能的最大值。因此，$J(p)$ 的最大值在 $p=1$ 处取得。\n鲁棒最优混合概率是 $p^{\\star} = 1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "当我们设计出一个新的、可能更优的策略后，如何在没有风险的实时部署的情况下预估其性能？本实践深入探讨了使用重要性采样进行离策略评估的方法，这是一种强大的统计技术 。你将分析两种核心估计器的偏差和方差，从而理解在使用数字孪生收集的数据来可靠评估策略时所涉及的权衡。",
            "id": "4212729",
            "problem": "一个信息物理系统（CPS）的数字孪生（DT）被用于通过重要性采样（IS）在离策略设置下评估一个深度目标策略。考虑一个有限时域强化学习（RL）任务，其时域长度为 $H \\in \\mathbb{N}$，折扣因子为 $\\gamma \\in (0,1)$，并且在行为策略下收集了 $N \\in \\mathbb{N}$ 条独立同分布的轨迹。对于时间步 $t \\in \\{0,1,\\dots,H-1\\}$ 和轨迹索引 $i \\in \\{1,2,\\dots,N\\}$，定义逐决策重要性采样（IS）比率 $\\rho_{t}^{(i)} = \\pi(a_{t}^{(i)} \\mid s_{t}^{(i)}) / \\mu(a_{t}^{(i)} \\mid s_{t}^{(i)})$。令逐决策累积权重为 $W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$，每步奖励为 $r_{t}^{(i)}$。目标策略的折扣回报的普通逐决策重要性采样（PDIS）估计器和加权（自归一化）PDIS估计器分别为\n$$\n\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right),\n\\qquad\n\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } \\right).\n$$\n假设以下适用于基于DT的CPS评估的基础建模条件：\n- 逐决策比率 $\\rho_{t}^{(i)}$ 在时间和轨迹上独立，对所有 $t$ 都有 $\\mathbb{E}[\\rho_{t}^{(i)}] = 1$ 和 $\\operatorname{Var}(\\rho_{t}^{(i)}) = \\sigma_{\\rho}^{2} \\in [0,\\infty)$。在时间 $t$ 上的独立性意味着 $W_{t}^{(i)}$ 的期望为 $\\mathbb{E}[W_{t}^{(i)}] = 1$，二阶矩为 $\\mathbb{E}\\!\\left[(W_{t}^{(i)})^{2}\\right] = \\left(1 + \\sigma_{\\rho}^{2} \\right)^{t+1}$。\n- 随机变量 $r_{t}^{(i)}$ 在时间和轨迹上独立，并且对于每个固定的 $t$ 都独立于 $W_{t}^{(i)}$，其目标策略均值为 $m_{t} = \\mathbb{E}_{\\pi}[r_{t}]$，方差为 $v_{t} = \\operatorname{Var}_{\\pi}(r_{t})$，从而得到 $\\mathbb{E}\\!\\left[ (r_{t}^{(i)})^{2} \\right] = v_{t} + m_{t}^{2}$。\n- 目标策略的真实折扣回报为 $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$。\n\n从这些定义以及比率估计器的大数定律和 delta 方法渐近理论出发，执行以下操作：\n1. 推导 $\\hat{J}^{o}$ 的偏差和方差的闭式解，用 $H$, $\\gamma$, $N$, $\\sigma_{\\rho}^{2}$, $\\{m_{t}\\}_{t=0}^{H-1}$ 和 $\\{v_{t}\\}_{t=0}^{H-1}$ 表示。\n2. 在所述的独立性假设下，使用均值比率展开推导 $\\hat{J}^{w}$ 的一阶（关于 $1/N$）偏差和方差。\n3. 使用你的结果，给出普通 PDIS 估计器和加权 PDIS 估计器之间均方误差（MSE）的主阶差的闭式表达式，定义为 $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$，保留到 $1/N$ 阶的项并忽略高阶项。\n\n你的最终答案必须是 $\\Delta_{\\mathrm{MSE}}$ 作为 $H$, $\\gamma$, $N$, $\\sigma_{\\rho}^{2}$, $\\{m_{t}\\}$ 和 $\\{v_{t}\\}$ 的函数的单一闭式解析表达式，且不应包含任何单位。无需四舍五入。",
            "solution": "所述问题具有科学依据、自洽且良定。这些假设虽然是简化的，但被明确地表述为基础建模条件，并且不包含内部矛盾。因此，我们可以进行完整的推导。\n\n总体目标是求出均方误差（MSE）的主阶差 $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$。这需要推导每个估计器的偏差和方差。参数 $\\theta$ 的估计器 $\\hat{\\theta}$ 的均方误差由 $\\mathrm{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + (\\operatorname{Bias}(\\hat{\\theta}))^2$ 给出，其中偏差 $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。\n\n让我们首先根据问题的已知条件建立必要的矩。\n随机变量是为每个轨迹 $i \\in \\{1,\\dots,N\\}$ 和时间步 $t \\in \\{0, \\dots, H-1\\}$ 定义的。期望 $\\mathbb{E}[\\cdot]$ 是关于行为策略 $\\mu$ 引致的分布计算的。\n-   重要性权重：$W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$。给定 $\\rho_{k}^{(i)}$ 是独立同分布的，且 $\\mathbb{E}[\\rho_{k}^{(i)}] = 1$ 和 $\\operatorname{Var}(\\rho_{k}^{(i)}) = \\sigma_{\\rho}^{2}$。\n    $\\mathbb{E}[W_{t}^{(i)}] = \\prod_{k=0}^{t} \\mathbb{E}[\\rho_{k}^{(i)}] = 1^{t+1} = 1$。\n    $\\mathbb{E}[(W_{t}^{(i)})^2] = \\prod_{k=0}^{t} \\mathbb{E}[(\\rho_{k}^{(i)})^2] = \\prod_{k=0}^{t} (\\operatorname{Var}(\\rho_{k}^{(i)}) + (\\mathbb{E}[\\rho_{k}^{(i)}])^2) = (\\sigma_{\\rho}^{2}+1)^{t+1}$。\n    $\\operatorname{Var}(W_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)}])^2 = (1+\\sigma_{\\rho}^{2})^{t+1} - 1$。\n-   奖励：$r_{t}^{(i)}$。问题假设对于固定的 $t$，$r_t^{(i)}$ 和 $W_t^{(i)}$ 是独立的。重要性采样的基本恒等式是 $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}_{\\pi}[r_{t}] = m_{t}$。使用独立性假设，我们有 $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}[W_{t}^{(i)}] \\mathbb{E}[r_{t}^{(i)}] = 1 \\cdot \\mathbb{E}[r_{t}^{(i)}]$。这意味着 $\\mathbb{E}[r_{t}^{(i)}] = m_{t}$。\n-   问题给出 $\\mathbb{E}[(r_{t}^{(i)})^2] = v_{t} + m_{t}^{2}$。这是行为策略分布下的二阶矩。因此行为策略下奖励的方差为 $\\operatorname{Var}(r_{t}^{(i)}) = \\mathbb{E}[(r_{t}^{(i)})^2] - (\\mathbb{E}[r_{t}^{(i)}])^2 = (v_{t}+m_{t}^2) - m_{t}^2 = v_{t}$。\n-   要估计的真实值为 $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$。\n\n我们按问题的三个部分进行。\n\n1. 推导 $\\hat{J}^{o}$ 的偏差和方差。\n\n普通 PDIS 估计器是 $\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)$。\n为了求偏差，我们计算其期望：\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\mathbb{E}\\left[ \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) \\right] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]\n$$\n由于轨迹是独立同分布的，$\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]$ 对所有 $i$ 都是相同的。使用 IS 恒等式，$\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = m_{t}$。\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} (N \\cdot m_{t}) = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t} = J^{\\pi}\n$$\n偏差为 $\\operatorname{Bias}(\\hat{J}^{o}) = \\mathbb{E}[\\hat{J}^{o}] - J^{\\pi} = 0$。$\\hat{J}^{o}$ 是一个无偏估计器。\n\n为了求方差，我们利用不同时间步的项是独立的这一事实，因为 $\\rho_t^{(i)}$ 和 $r_t^{(i)}$ 在时间上是独立的。\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\operatorname{Var}\\left( \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}\\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)\n$$\n由于轨迹是独立同分布的，均值的方差是单个项的方差除以 $N$。\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\frac{1}{N} \\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)})\n$$\n我们计算乘积项的方差：\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}])^2\n$$\n利用 $W_t^{(i)}$ 和 $r_t^{(i)}$ 的独立性，我们得到 $\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[(r_{t}^{(i)})^2]$。代入已知的矩：\n$$\n\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\left( (1+\\sigma_{\\rho}^{2})^{t+1} \\right) \\left( v_{t} + m_{t}^{2} \\right)\n$$\n且 $\\mathbb{E}[W_t^{(i)} r_t^{(i)}] = m_t$。综合起来：\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2}\n$$\n所以，$\\hat{J}^{o}$ 的方差是：\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\n\n2. 推导 $\\hat{J}^{w}$ 的一阶偏差和方差。\n\n加权 PDIS 估计器是 $\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\hat{m}_{t}$，其中 $\\hat{m}_{t} = \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } = \\frac{A_{t}}{B_{t}}$。令 $A_t = \\frac{1}{N}\\sum_i W_t^{(i)}r_t^{(i)}$ 且 $B_t = \\frac{1}{N}\\sum_i W_t^{(i)}$。我们使用两个随机变量比率的泰勒展开。对于 $\\hat{m}_t = A_t/B_t$，一阶偏差近似为：\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{\\mathbb{E}[A_{t}]}{\\mathbb{E}[B_{t}]^3} \\operatorname{Var}(B_{t}) - \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Cov}(A_{t}, B_{t})\n$$\n我们有 $\\mathbb{E}[A_t] = m_t$ 和 $\\mathbb{E}[B_t] = 1$。$\\operatorname{Var}(B_{t}) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}) = \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)$。\n协方差项是 $\\operatorname{Cov}(A_{t}, B_{t}) = \\frac{1}{N} \\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)})$。\n$$\n\\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)}) = \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)} \\cdot W_{t}^{(i)}] - \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] \\mathbb{E}[W_{t}^{(i)}]\n$$\n$$\n= \\mathbb{E}[(W_{t}^{(i)})^2 r_{t}^{(i)}] - m_{t} \\cdot 1 = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[r_{t}^{(i)}] - m_{t} = (1+\\sigma_{\\rho}^{2})^{t+1} m_{t} - m_{t} = m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)\n$$\n将这些代入 $\\hat{m}_t$ 的偏差公式中：\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{m_{t}}{1^3} \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) - \\frac{1}{1^2} \\frac{1}{N}m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) = 0\n$$\n$\\hat{m}_{t}$ 的一阶偏差为零。因此，$\\hat{J}^{w} = \\sum_t \\gamma^t \\hat{m}_t$ 的一阶偏差也为零。偏差的阶为 $O(1/N^2)$。\n\n比率估计器 $\\hat{m}_t$ 的一阶方差近似为：\n$$\n\\operatorname{Var}(\\hat{m}_{t}) \\approx \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Var}(A_t - \\frac{\\mathbb{E}[A_t]}{\\mathbb{E}[B_t]} B_t) = \\operatorname{Var}(A_t - m_t B_t)\n$$\n$A_t - m_t B_t = \\frac{1}{N} \\sum_i (W_t^{(i)} r_t^{(i)} - m_t W_t^{(i)}) = \\frac{1}{N} \\sum_i W_t^{(i)}(r_t^{(i)} - m_t)$。由于轨迹是独立同分布的：\n$$\n\\operatorname{Var}(A_t - m_t B_t) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t))\n$$\n该项的均值为 $\\mathbb{E}[W_t^{(i)}(r_t^{(i)} - m_t)] = \\mathbb{E}[W_t^{(i)}r_t^{(i)}] - m_t\\mathbb{E}[W_t^{(i)}] = m_t - m_t \\cdot 1 = 0$。\n所以其方差是其二阶矩：\n$$\n\\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t)) = \\mathbb{E}[(W_t^{(i)}(r_t^{(i)} - m_t))^2] = \\mathbb{E}[(W_t^{(i)})^2 (r_t^{(i)} - m_t)^2]\n$$\n利用 $W_t^{(i)}$ 和 $r_t^{(i)}$ 的独立性，这变为：\n$$\n= \\mathbb{E}[(W_t^{(i)})^2] \\mathbb{E}[(r_t^{(i)} - m_t)^2] = (1+\\sigma_\\rho^2)^{t+1} \\operatorname{Var}(r_t^{(i)}) = (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n所以，$\\operatorname{Var}(\\hat{m}_t) \\approx \\frac{1}{N}(1+\\sigma_\\rho^2)^{t+1} v_t$。$\\hat{J}^w$ 的总方差是对独立时间步求和：\n$$\n\\operatorname{Var}(\\hat{J}^{w}) \\approx \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}(\\hat{m}_{t}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n\n3. 推导 MSE 的主阶差。\n\nMSE 是 $\\operatorname{Var}(\\cdot) + (\\operatorname{Bias}(\\cdot))^2$。我们保留到 $O(1/N)$ 阶的项。\n对于 $\\hat{J}^o$，偏差恰好为零。\n$$\n\\mathrm{MSE}(\\hat{J}^{o}) = \\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\n对于 $\\hat{J}^w$，偏差的阶为 $O(1/N^2)$，所以偏差的平方为 $O(1/N^4)$，可以忽略不计。\n$$\n\\mathrm{MSE}(\\hat{J}^{w}) \\approx \\operatorname{Var}(\\hat{J}^{w}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n现在我们计算差值 $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$，保留到 $O(1/N)$ 阶：\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right] - \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right] \\right)\n$$\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}v_{t} + (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} - (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right)\n$$\n包含 $v_t$ 的项相互抵消。\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} \\right)\n$$\n提出公因子 $m_t^2$ 即可得到主阶 MSE 差的最终表达式。\n$$\n\\Delta_{\\mathrm{MSE}} = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_t^2 \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)\n$$\n该表达式表示加权估计器中自归一化所实现的方差减少，它移除了与平均奖励 $m_t$ 相关的方差分量。",
            "answer": "$$\n\\boxed{\\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_{t}^{2} \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)}\n$$"
        }
    ]
}