{
    "hands_on_practices": [
        {
            "introduction": "Before a controller can be synthesized, its desired behavior must be captured unambiguously in a formal language. This exercise focuses on this crucial translation step, converting high-level, natural-language requirements for a cyber-physical system into precise Linear Temporal Logic (LTL) formulas . You will practice distinguishing between safety properties, which state that \"something bad never happens,\" and liveness properties, which state that \"something good eventually happens,\" a fundamental dichotomy that drives the synthesis process.",
            "id": "4210942",
            "problem": "A Cyber-Physical System (CPS) integrates physical processes with computation and communication, and a Digital Twin (DT) mirrors the CPS state to support monitoring and control. Consider synthesizing a correct-by-construction controller from Linear Temporal Logic (LTL) specifications for a coolant loop subsystem. The control objective is to coordinate actuation and supervisory modes while respecting safety interlocks and recovery behavior under communication loss and fault conditions.\n\nYou are given the following atomic propositions, all Boolean and evaluated at discrete time steps along an execution trace:\n- $h$: the physical system reports high pressure (above a certified threshold).\n- $o$: the controller issues the command to open the coolant valve.\n- $d$: the diagnostic twin flags a fault detected in the loop.\n- $s$: the controller is in safe mode.\n- $r$: a reset command is observed.\n- $p$: the polling action for sensor data occurs.\n- $c$: communication is up (the DT is connected to the CPS).\n- $y$: the DT is synchronized with the CPS state.\n\nThe temporal requirements of interest are:\n- $\\mathsf{R\\_1}$: The controller must not open the valve while pressure is high.\n- $\\mathsf{R\\_2}$: If a fault is detected, the controller should eventually enter safe mode.\n- $\\mathsf{R\\_3}$: Sensor polling should occur infinitely often.\n- $\\mathsf{R\\_4}$: Once in safe mode, it remains active until a reset occurs (and may remain active forever if no reset occurs).\n- $\\mathsf{R\\_5}$: Whenever communication is up, synchronization with the physical system should eventually be achieved.\n\nSelect the option that provides both the correct safety-versus-liveness classification for each requirement and a correct LTL encoding using the standard operators $\\mathbf{G}$ (globally), $\\mathbf{F}$ (eventually), $\\mathbf{U}$ (until), $\\mathbf{R}$ (release), $\\neg$ (negation), $\\wedge$ (conjunction), and $\\rightarrow$ (implication).\n\nA. \n- $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,\\neg(h \\wedge o)$.\n- $\\mathsf{R\\_2}$: liveness; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{F}\\,s)$.\n- $\\mathsf{R\\_3}$: liveness; $\\mathbf{G}\\mathbf{F}\\,p$.\n- $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (\\,r \\mathbin{R} s\\,)\\big)$.\n- $\\mathsf{R\\_5}$: liveness; $\\mathbf{G}\\,(c \\rightarrow \\mathbf{F}\\,y)$.\n\nB.\n- $\\mathsf{R\\_1}$: liveness; $\\mathbf{F}\\,\\neg(h \\wedge o)$.\n- $\\mathsf{R\\_2}$: safety; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{G}\\,s)$.\n- $\\mathsf{R\\_3}$: safety; $\\mathbf{F}\\mathbf{G}\\,p$.\n- $\\mathsf{R\\_4}$: liveness; $\\mathbf{G}\\,(s \\rightarrow \\mathbf{F}\\,r)$.\n- $\\mathsf{R\\_5}$: safety; $\\mathbf{G}\\,(c \\rightarrow \\mathbf{G}\\,y)$.\n\nC.\n- $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,(h \\rightarrow o)$.\n- $\\mathsf{R\\_2}$: liveness; $\\mathbf{F}\\,(s \\wedge d)$.\n- $\\mathsf{R\\_3}$: liveness; $\\mathbf{G}\\mathbf{F}\\,p$.\n- $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (\\,s \\mathbin{U} r\\,)\\big)$.\n- $\\mathsf{R\\_5}$: liveness; $\\mathbf{F}\\,y$.\n\nD.\n- $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,\\neg(h \\wedge o)$.\n- $\\mathsf{R\\_2}$: liveness; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{F}\\,s)$.\n- $\\mathsf{R\\_3}$: liveness; $\\mathbf{F}\\mathbf{G}\\,p$.\n- $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (\\,s \\mathbin{R} r\\,)\\big)$.\n- $\\mathsf{R\\_5}$: liveness; $\\mathbf{G}\\,(\\mathbf{F}\\,y)$.",
            "solution": "The problem statement is a well-posed exercise in formal methods, specifically the translation of natural-language requirements for a Cyber-Physical System (CPS) into Linear Temporal Logic (LTL) specifications and their classification as safety or liveness properties.\n\n### Step 1: Extract Givens\nThe problem provides the following verbatim information:\n- **Atomic Propositions (Boolean):**\n  - $h$: the physical system reports high pressure.\n  - $o$: the controller issues the command to open the coolant valve.\n  - $d$: the diagnostic twin flags a fault detected in the loop.\n  - $s$: the controller is in safe mode.\n  - $r$: a reset command is observed.\n  - $p$: the polling action for sensor data occurs.\n  - $c$: communication is up.\n  - $y$: the DT is synchronized with the CPS state.\n- **Temporal Requirements:**\n  - $\\mathsf{R\\_1}$: The controller must not open the valve while pressure is high.\n  - $\\mathsf{R\\_2}$: If a fault is detected, the controller should eventually enter safe mode.\n  - $\\mathsf{R\\_3}$: Sensor polling should occur infinitely often.\n  - $\\mathsf{R\\_4}$: Once in safe mode, it remains active until a reset occurs (and may remain active forever if no reset occurs).\n  - $\\mathsf{R\\_5}$: Whenever communication is up, synchronization with the physical system should eventually be achieved.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established validation criteria.\n- **Scientifically Grounded:** The problem uses standard, well-defined concepts from computer science and control theory, namely Linear Temporal Logic (LTL) for specifying system behavior. The scenario represents a typical application in the domain of CPS and digital twins. The problem is scientifically sound.\n- **Well-Posed:** The natural language requirements are stated with sufficient clarity to permit a unique formalization in LTL. The task is to identify the correct LTL formula and property classification (safety/liveness), which is a determinate problem.\n- **Objective:** The language is technical and free of subjectivity or ambiguity.\n- **Complete and Consistent:** The provided atomic propositions are sufficient to express the given requirements. There are no evident contradictions in the problem setup.\n- **Realistic and Feasible:** The scenario of a coolant loop with safety interlocks is a realistic engineering problem for which formal specification is a valid approach.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The analysis can proceed.\n\n### Derivation of Correct Specifications\n\nBefore evaluating the options, we will first derive the correct classifications and LTL formulas for each requirement based on first principles.\n\nA **safety** property asserts that \"something bad never happens\". A violation of a safety property can always be demonstrated by a finite execution trace.\nA **liveness** property asserts that \"something good will eventually happen\". A violation of a liveness property can only be demonstrated by an infinite execution trace, as the \"good thing\" could always happen at some later time.\n\n- **$\\mathsf{R\\_1}$: The controller must not open the valve while pressure is high.**\n  - **Classification:** This is a **safety** property. The \"bad thing\" is the simultaneous occurrence of high pressure ($h$) and the valve-open command ($o$). If a state where $h \\wedge o$ is true is observed at any finite time, the property is violated.\n  - **LTL Encoding:** The property \"never ($h \\wedge o$)\" must hold at all times (globally). This is expressed as $\\mathbf{G}\\,\\neg(h \\wedge o)$.\n\n- **$\\mathsf{R\\_2}$: If a fault is detected, the controller should eventually enter safe mode.**\n  - **Classification:** This is a **liveness** property. The \"good thing\" is entering safe mode ($s$) at some point after a fault ($d$) is detected. A finite trace where a fault has occurred but safe mode has not yet been entered does not constitute a violation, as safe mode could still be entered in the future. Only an infinite trace where a fault occurs and safe mode is never entered would be a violation.\n  - **LTL Encoding:** This requirement must hold for all time. \"Globally, if $d$ is true, then eventually $s$ becomes true\". This translates to the LTL formula $\\mathbf{G}\\,(d \\rightarrow \\mathbf{F}\\,s)$.\n\n- **$\\mathsf{R\\_3}$: Sensor polling should occur infinitely often.**\n  - **Classification:** This is a quintessential **liveness** property. It requires that the \"good thing\" of polling ($p$) happens again and again, without end. A violation would be a trace where, after some finite time, polling never occurs again. This can only be confirmed by observing an infinite trace.\n  - **LTL Encoding:** The property \"infinitely often $p$\" is expressed as \"globally, eventually $p$\" or $\\mathbf{G}\\mathbf{F}\\,p$.\n\n- **$\\mathsf{R\\_4}$: Once in safe mode, it remains active until a reset occurs (and may remain active forever if no reset occurs).**\n  - **Classification:** This is a **safety** property. The \"bad thing\" is leaving safe mode ($\\neg s$) without having been preceded by a reset ($r$). A finite trace showing a transition from a state with $s$ to one with $\\neg s$, where the first state did not have $r$, is a finite witness to a violation.\n  - **LTL Encoding:** The statement begins \"Once in safe mode...\", which translates to a global implication \"Globally, if in safe mode...\": $\\mathbf{G}\\,(s \\rightarrow \\dots)$. The consequent specifies that the system \"remains active ($s$) until a reset ($r$)\", including the case where a reset never occurs. This is the definition of \"weak until\". The standard `Until` operator $\\mathbf{U}$ is \"strong\" and would require $r$ to eventually occur. The `Release` operator $\\mathbf{R}$ is suitable here. The formula $\\phi_1 \\mathbin{R} \\phi_2$ means that $\\phi_2$ must remain true up to and including the point where $\\phi_1$ becomes true; if $\\phi_1$ never becomes true, $\\phi_2$ must remain true forever. Here, the property to be maintained is $s$, and the releasing condition is $r$. Thus, the state-property we want to hold from the moment $s$ is true is $r \\mathbin{R} s$. The full formula is $\\mathbf{G}\\big(s \\rightarrow (r \\mathbin{R} s)\\big)$.\n\n- **$\\mathsf{R\\_5}$: Whenever communication is up, synchronization with the physical system should eventually be achieved.**\n  - **Classification:** This is a **liveness** property. The \"good thing\" is achieving synchronization ($y$) after communication is up ($c$). A finite trace where $c$ is true but $y$ has not yet become true is not a violation, because synchronization could happen later.\n  - **LTL Encoding:** The word \"Whenever\" implies a global scope. \"Globally, if communication is up ($c$), then eventually synchronization ($y$) is achieved\". This translates directly to $\\mathbf{G}\\,(c \\rightarrow \\mathbf{F}\\,y)$.\n\n### Summary of Correct Specifications\n- $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,\\neg(h \\wedge o)$\n- $\\mathsf{R\\_2}$: liveness; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{F}\\,s)$\n- $\\mathsf{R\\_3}$: liveness; $\\mathbf{G}\\mathbf{F}\\,p$\n- $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (r \\mathbin{R} s)\\big)$\n- $\\mathsf{R\\_5}$: liveness; $\\mathbf{G}\\,(c \\rightarrow \\mathbf{F}\\,y)$\n\n### Option-by-Option Analysis\n\n- **A.**\n  - $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,\\neg(h \\wedge o)$. **Correct**. Matches our derived classification and formula.\n  - $\\mathsf{R\\_2}$: liveness; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{F}\\,s)$. **Correct**. Matches our derived classification and formula.\n  - $\\mathsf{R\\_3}$: liveness; $\\mathbf{G}\\mathbf{F}\\,p$. **Correct**. Matches our derived classification and formula for \"infinitely often\".\n  - $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (\\,r \\mathbin{R} s\\,)\\big)$. **Correct**. Matches our derived classification and formula for weak until.\n  - $\\mathsf{R\\_5}$: liveness; $\\mathbf{G}\\,(c \\rightarrow \\mathbf{F}\\,y)$. **Correct**. Matches our derived classification and formula.\n  This option provides the correct classification and LTL formula for all five requirements.\n\n- **B.**\n  - $\\mathsf{R\\_1}$: liveness; $\\mathbf{F}\\,\\neg(h \\wedge o)$. **Incorrect**. $\\mathsf{R\\_1}$ is a safety property. The formula means \"eventually, the bad state does not occur\", which is a trivial liveness property and does not enforce the safety constraint.\n  - $\\mathsf{R\\_2}$: safety; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{G}\\,s)$. **Incorrect**. $\\mathsf{R\\_2}$ is a liveness property. The formula specifies that if a fault occurs, the system must *immediately and forever* be in safe mode, which is much stronger than required and is a safety property.\n  - $\\mathsf{R\\_3}$: safety; $\\mathbf{F}\\mathbf{G}\\,p$. **Incorrect**. $\\mathsf{R\\_3}$ is a liveness property. The formula $\\mathbf{F}\\mathbf{G}\\,p$ means \"eventually, the system reaches a state from which polling is always true\", which is not the same as \"infinitely often\".\n  - $\\mathsf{R\\_4}$: liveness; $\\mathbf{G}\\,(s \\rightarrow \\mathbf{F}\\,r)$. **Incorrect**. $\\mathsf{R\\_4}$ is a safety property. The formula incorrectly asserts that a reset must eventually happen after entering safe mode.\n  - $\\mathsf{R\\_5}$: safety; $\\mathbf{G}\\,(c \\rightarrow \\mathbf{G}\\,y)$. **Incorrect**. $\\mathsf{R\\_5}$ is a liveness property. The formula is too strong, requiring immediate and permanent synchronization once communication is up.\n\n- **C.**\n  - $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,(h \\rightarrow o)$. **Incorrect**. The formula is logically flawed; it states that high pressure implies the valve must open, which is the opposite of the requirement.\n  - $\\mathsf{R\\_2}$: liveness; $\\mathbf{F}\\,(s \\wedge d)$. **Incorrect**. The formula requires a state to eventually exist where $s$ and $d$ are simultaneously true. This does not capture the required implication \"if $d$, then eventually $s$\".\n  - $\\mathsf{R\\_3}$: liveness; $\\mathbf{G}\\mathbf{F}\\,p$. **Correct**. This is the only correct part of this option.\n  - $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (\\,s \\mathbin{U} r\\,)\\big)$. **Incorrect**. The use of the strong `Until` operator ($\\mathbf{U}$) incorrectly implies that a reset ($r$) *must* eventually occur.\n  - $\\mathsf{R\\_5}$: liveness; $\\mathbf{F}\\,y$. **Incorrect**. The formula omits the crucial precondition that this requirement only applies when communication ($c$) is up.\n\n- **D.**\n  - $\\mathsf{R\\_1}$: safety; $\\mathbf{G}\\,\\neg(h \\wedge o)$. **Correct**.\n  - $\\mathsf{R\\_2}$: liveness; $\\mathbf{G}\\,(d \\rightarrow \\mathbf{F}\\,s)$. **Correct**.\n  - $\\mathsf{R\\_3}$: liveness; $\\mathbf{F}\\mathbf{G}\\,p$. **Incorrect**. Same error as in option B. $\\mathbf{F}\\mathbf{G}\\,p$ means \"eventually forever $p$\", not \"infinitely often $p$\".\n  - $\\mathsf{R\\_4}$: safety; $\\mathbf{G}\\big(s \\rightarrow (\\,s \\mathbin{R} r\\,)\\big)$. **Incorrect**. The arguments of the `Release` operator are swapped. $s \\mathbin{R} r$ means \"$r$ must hold until $s$ becomes true,\" which does not match the requirement.\n  - $\\mathsf{R\\_5}$: liveness; $\\mathbf{G}\\,(\\mathbf{F}\\,y)$. **Incorrect**. The formula is missing the precondition $c$. It requires synchronization infinitely often, regardless of communication status.\n\nBased on the analysis, only option A correctly identifies the classification and provides the correct LTL formula for all five requirements.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Not all specifications are possible to satisfy, and unrealizability often stems from a mismatch between desired behavior and physical reality. This exercise explores a common scenario in cyber-physical systems where the environment's disruptive power, modeled by a disturbance $w_k$, can overwhelm the controller's actuation authority, modeled by an action $u_k$ . By analyzing a simple thermal regulation model, you will construct a concrete environmental \"counter-strategy\" to prove the specification is unrealizable and calculate the minimum time to failure.",
            "id": "4210961",
            "problem": "Consider a discrete-time Cyber-Physical System (CPS) that models thermal regulation in a laboratory as mirrored by its Digital Twin. The plant temperature state is denoted by $x_{k} \\in \\mathbb{R}$ at discrete time $k \\in \\mathbb{N}$. The controller selects a cooling action $u_{k} \\in \\mathbb{R}$ subject to an actuation bound, and the environment selects a heat disturbance $w_{k} \\in \\mathbb{R}$ subject to disturbance bounds and fairness. The dynamics are given by the additive model\n$$\nx_{k+1} = x_{k} + u_{k} + w_{k},\n$$\nwith initial condition $x_{0} = 7.3$, controller bound $|u_{k}| \\leq U$ with $U = 1.1$, and disturbance bounds $0 \\leq w_{k} \\leq W$ with $W = 2.0$.\n\nThe Linear Temporal Logic (LTL) specification is in the Generalized Reactivity (1) (GR(1)) form with the following environment assumptions and system guarantees:\n\n- Environment assumptions:\n  1. $\\mathbf{G}(0 \\leq w \\leq W)$,\n  2. $\\mathbf{G}\\mathbf{F}(w = 0)$,\n\n- System guarantees:\n  1. $\\mathbf{G}(x \\leq X_{m})$ with $X_{m} = 10$,\n  2. $\\mathbf{G}\\mathbf{F}(x \\leq X_{g})$ with $X_{g} = 6$.\n\nHere, $\\mathbf{G}$ denotes the “globally” operator (always) and $\\mathbf{F}$ denotes the “eventually” operator (sometime). The synthesis semantics are standard for reactive synthesis over infinite traces: the environment chooses the disturbance sequence $\\{w_{k}\\}_{k \\geq 0}$ satisfying the assumptions, while the controller chooses the action sequence $\\{u_{k}\\}_{k \\geq 0}$ subject to the bounds, attempting to enforce the guarantees along the resulting trace of states $\\{x_{k}\\}_{k \\geq 0}$.\n\nStarting only from the definitions of Linear Temporal Logic satisfaction over traces, the CPS update law, and bounded-input bounded-disturbance constraints, demonstrate that the above GR(1) specification is unrealizable by constructing an environment counterstrategy that produces a violating trace and explaining precisely how it falsifies the guarantees. Then, compute the minimal number of steps $K^{\\star} \\in \\mathbb{N}$ needed by the environment, under its assumptions, to force a violation of the safety guarantee $\\mathbf{G}(x \\leq X_{m})$ regardless of the controller’s decisions. Your final answer must be the single integer $K^{\\star}$, representing the shortest prefix length at which the violation necessarily occurs. No rounding is required.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of control theory and formal methods for Cyber-Physical Systems, is well-posed with a clear objective, and is free from any scientific, logical, or factual flaws. Essential data and constraints are provided and are mutually consistent. We may therefore proceed with the solution.\n\nThe problem requires us to demonstrate that the given Generalized Reactivity (1) (GR(1)) specification is unrealizable for the specified discrete-time system and to compute the minimum number of steps $K^{\\star}$ for the environment to force a violation of the safety guarantee.\n\nThe system dynamics are given by the linear equation:\n$$x_{k+1} = x_{k} + u_{k} + w_{k}$$\nwith initial state $x_{0} = 7.3$. The controller's action $u_{k}$ is bounded by $|u_{k}| \\leq U$, where $U=1.1$, meaning $u_{k} \\in [-1.1, 1.1]$. The environment's disturbance $w_{k}$ is bounded by $0 \\leq w_{k} \\leq W$, where $W=2.0$.\n\nThe LTL specification consists of:\n- Environment assumptions:\n  1. $\\mathbf{G}(0 \\leq w \\leq W)$: The disturbance is always within its bounds. This is congruent with the problem description.\n  2. $\\mathbf{G}\\mathbf{F}(w = 0)$: The disturbance must be zero infinitely often (fairness).\n- System guarantees:\n  1. $\\mathbf{G}(x \\leq X_{m})$: The state must always be less than or equal to $X_{m}=10$ (safety guarantee).\n  2. $\\mathbf{G}\\mathbf{F}(x \\leq X_{g})$: The state must infinitely often be less than or equal to $X_{g}=6$ (liveness guarantee).\n\nA specification is unrealizable if there exists a strategy for the environment, satisfying all its assumptions, for which no possible controller strategy can satisfy all system guarantees. To demonstrate unrealizability, we must construct such an environmental counterstrategy. The environment's objective in this counterstrategy is to falsify a system guarantee. Falsifying the safety guarantee $\\mathbf{G}(x \\leq X_{m})$ is sufficient, as it requires finding just one time step $k$ where $x_{k} > X_{m}$.\n\nLet us construct an environment counterstrategy. The environment can try to increase the state $x_k$ as rapidly as possible to breach the safety boundary $X_{m}=10$. To do this, it should apply the maximum possible disturbance, $w_{k}=W=2.0$, at each time step.\n\nLet's analyze the system's evolution under this aggressive environment strategy. The state update is $x_{k+1} = x_{k} + u_{k} + 2.0$. The controller's goal is to keep the state low, so it must apply the maximum cooling effect. The controller's action is bounded by $u_{k} \\geq -U = -1.1$. Thus, for any action $u_k$ chosen by the controller, the state $x_{k+1}$ will satisfy:\n$$x_{k+1} = x_{k} + u_{k} + W \\geq x_{k} - U + W$$\nThe minimal possible rate of change of the state, achieved when the controller applies maximal cooling ($u_k = -U$), is:\n$$\\Delta x_{\\min} = x_{k+1} - x_k = -U + W = -1.1 + 2.0 = 0.9$$\nSince this minimal rate of change is positive, the state will increase at every step, even under the controller's best effort, as long as the environment applies the maximal disturbance $w_k=W$.\n\nLet's trace the minimum possible state trajectory, where the controller always chooses $u_k = -1.1$ to counteract the environment's strategy of choosing $w_k = 2.0$.\nThe initial state is $x_{0} = 7.3$.\n- At step $k=1$: $x_{1} = x_{0} + u_{0} + w_{0} = 7.3 - 1.1 + 2.0 = 8.2$. Since $x_{1} \\leq 10$, the guarantee holds.\n- At step $k=2$: $x_{2} = x_{1} + u_{1} + w_{1} = 8.2 - 1.1 + 2.0 = 9.1$. Since $x_{2} \\leq 10$, the guarantee holds.\n- At step $k=3$: $x_{3} = x_{2} + u_{2} + w_{2} = 9.1 - 1.1 + 2.0 = 10.0$. Since $x_{3} \\leq 10$, the guarantee holds.\n- At step $k=4$: $x_{4} = x_{3} + u_{3} + w_{3} = 10.0 - 1.1 + 2.0 = 10.9$.\n\nAt time step $k=4$, the state becomes $x_{4} = 10.9$. This value is greater than $X_{m}=10$, thus violating the safety guarantee $\\mathbf{G}(x \\leq X_{m})$. Since we calculated this trajectory using the controller's optimal strategy for minimizing the state, any other choice of controls $u_{k} > -1.1$ would have resulted in an even larger state value at $k=4$. Therefore, regardless of the controller's actions, the state will exceed $10$ at $k=4$ if the environment plays $w_k=2.0$ for the first four steps.\n\nThe environmental counterstrategy is the finite prefix $\\{w_0, w_1, w_2, w_3\\} = \\{2.0, 2.0, 2.0, 2.0\\}$. This finite sequence of actions can be extended into an infinite trace that satisfies the environment's fairness assumption $\\mathbf{G}\\mathbf{F}(w=0)$, for instance, by setting $w_k = 0$ for all $k \\geq 4$. Since this valid environmental strategy forces a violation of a system guarantee, the specification is unrealizable. The system is unsafe because the disturbance authority ($W$) is greater than the control authority ($U$), allowing the environment to guarantee a net increase in state over time.\n\nNow, we compute the minimal number of steps $K^{\\star}$ required for the environment to *force* a violation. This means finding the smallest integer $k$ such that for the environmental strategy $w_i = W$ for all $i \\in \\{0, 1, \\dots, k-1\\}$, the resulting state $x_k$ is guaranteed to be greater than $X_m$, irrespective of the controller's actions $u_i \\in [-U, U]$.\n\nThe state after $k$ steps is given by:\n$$x_{k} = x_{0} + \\sum_{i=0}^{k-1} (u_{i} + w_{i})$$\nTo force a violation, the environment plays $w_i = W = 2.0$ for $i=0, \\dots, k-1$. The state is then:\n$$x_{k} = x_{0} + \\sum_{i=0}^{k-1} u_{i} + \\sum_{i=0}^{k-1} W = 7.3 + k(2.0) + \\sum_{i=0}^{k-1} u_{i}$$\nA violation is forced at step $k$ if $x_k > X_m$ for all valid sequences of controller actions $\\{u_i\\}_{i=0}^{k-1}$. This is equivalent to checking if the minimum possible value of $x_k$ is greater than $X_m$. The minimum value of $x_k$ occurs when the controller applies maximum cooling at every step, i.e., $u_i = -U = -1.1$ for all $i$.\n$$x_{k, \\min} = 7.3 + k(2.0) + \\sum_{i=0}^{k-1} (-1.1) = 7.3 + 2.0k - 1.1k = 7.3 + 0.9k$$\nWe seek the smallest integer $k$ such that $x_{k, \\min} > X_m$.\n$$7.3 + 0.9k > 10$$\n$$0.9k > 10 - 7.3$$\n$$0.9k > 2.7$$\n$$k > \\frac{2.7}{0.9}$$\n$$k > 3$$\nThe smallest integer $k$ satisfying this inequality is $k=4$. Thus, the minimal number of steps for the environment to force a safety violation is $4$. This is the value of $K^{\\star}$.\nAt $k=3$, the best the controller can do is achieve $x_{3,\\min} = 7.3 + 0.9(3) = 7.3 + 2.7 = 10.0$, which does not violate the safety guarantee. Therefore, $3$ steps are insufficient for the environment to force a violation. At $k=4$, the minimum possible state is $x_{4,\\min} = 7.3 + 0.9(4) = 7.3 + 3.6 = 10.9$, which is strictly greater than $10$.\n\nThe minimal number of steps for the environment to force a violation of the safety guarantee, $\\mathbf{G}(x \\leq X_m)$, is $K^{\\star} = 4$.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Discovering that a specification is unrealizable is often just the beginning; the crucial next step is to diagnose and fix the underlying issue. This final practice addresses the engineering task of specification repair, using a Generalized Reactivity (1) or $GR(1)$ specification for a mobile robot that is logically flawed . You will analyze how an unconstrained environment can permanently prevent the system from achieving its goals and evaluate different modifications, such as adding fairness assumptions, to restore realizability.",
            "id": "4210951",
            "problem": "A digital twin of a Cyber-Physical System (CPS) models a mobile service robot that must cyclically visit two service zones, denoted by the predicates $Z_A$ and $Z_B$, over an infinite horizon. The abstracted interaction is specified in Linear Temporal Logic (LTL) within the Generalized Reactivity of rank $1$ (GR(1)) fragment. The environment exposes a Boolean input $\\ell$ indicating a safety lock: when $\\ell$ holds, the robot’s zone cannot change; when $\\neg \\ell$ holds, the robot may change zones. The system controls a Boolean state variable $z$ that encodes its current zone, with $Z_A \\equiv (z = A)$ and $Z_B \\equiv (z = B)$ and $A \\neq B$. The plant abstraction imposes the following safety on the combined transition relation:\n- $\\mathbf{G}\\big(\\ell \\rightarrow X z = z\\big)$ and $\\mathbf{G}\\big(\\neg \\ell \\rightarrow (X z = z \\lor X z \\neq z)\\big)$, expressing that zone changes are forbidden under lock and permitted (but not forced) when unlocked.\nAssume unconstrained environment initials and safety beyond the above coupling, and assume the system initial zone is arbitrary but fixed by $z$.\n\nThe GR(1) specification is organized as follows:\n- Environment assumptions: no initial or safety constraints beyond the plant coupling above, and no environment justice constraints.\n- System guarantees: $\\mathbf{G}$-safety includes the plant coupling above, and the system justice (Büchi) constraints require the robot to visit both zones infinitely often: $\\mathbf{G}\\mathbf{F}\\, Z_A$ and $\\mathbf{G}\\mathbf{F}\\, Z_B$.\n\nTask:\n- Using the standard semantics of LTL and GR(1) winning conditions in two-player games (environment moves first, system responds), determine why the specification above is unrealizable. In particular, decompose the cause of unrealizability down to the conflicting justice constraints in the presence of the lock dynamics.\n- Then, select all modifications among the choices below that restore realizability while preserving the original intent to visit both zones infinitely often under physically sound assumptions. Modifications may alter environment assumptions, system safety, and/or system justice, but must respect the plant coupling and scientific realism of the CPS.\n\nChoices:\nA. Add an environment justice constraint $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$ while leaving the system justice $\\mathbf{G}\\mathbf{F}\\, Z_A \\land \\mathbf{G}\\mathbf{F}\\, Z_B$ unchanged.\n\nB. Replace the system justice $\\mathbf{G}\\mathbf{F}\\, Z_A \\land \\mathbf{G}\\mathbf{F}\\, Z_B$ with a single justice $\\mathbf{G}\\mathbf{F}\\,(Z_A \\lor Z_B)$, leaving environment assumptions unchanged.\n\nC. Add a system safety constraint $\\mathbf{G}\\,(X z \\neq z)$ that forces the robot to change zones on every step, leaving environment assumptions and system justice unchanged.\n\nD. Refine the system justice to $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_A)$ and $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_B)$, and add an environment justice $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$.\n\nE. Introduce a system scheduler variable $s$ with $\\mathbf{G}\\,(X s = \\neg s)$ and guarantees $\\mathbf{G}\\mathbf{F}\\,(s \\rightarrow Z_A)$ and $\\mathbf{G}\\mathbf{F}\\,(\\neg s \\rightarrow Z_B)$, leaving environment assumptions unchanged.\n\nSelect all correct choices.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. The scenario describes a standard problem in the synthesis of controllers from temporal logic specifications, a core topic in cyber-physical systems and formal methods. The task is to identify a fundamental contradiction in the specification that leads to unrealizability and then to identify which proposed modifications resolve this issue while preserving the system's intended behavior.\n\n### Analysis of Unrealizability\n\nThe specification is formulated as a two-player game between the environment, which controls the Boolean input $\\ell$, and the system, which controls the Boolean state variable $z$. A specification is realizable if the system possesses a winning strategy to satisfy its guarantees for all possible valid behaviors of the environment.\n\nThe key components of the specification are:\n1.  **Environment:** Controls $\\ell$ and has no constraints on its behavior. This means the environment can choose to set $\\ell$ to true or false at any step, for any duration.\n2.  **System:** Controls $z$ (the robot's zone, $Z_A$ or $Z_B$).\n3.  **Safety Guarantee (Plant Coupling):** $\\mathbf{G}(\\ell \\rightarrow X z = z)$. This is a hard constraint on the system. If the environment sets $\\ell$ to true, the system is forbidden from changing the zone $z$ in the next state.\n4.  **Liveness/Justice Guarantees (System):** $\\mathbf{G}\\mathbf{F}\\, Z_A \\land \\mathbf{G}\\mathbf{F}\\, Z_B$. The system promises to visit zone $A$ infinitely often and zone $B$ infinitely often.\n\nThe specification is unrealizable due to a conflict between the system's unconditional liveness guarantees and the environment's unconstrained power to permanently restrict the system's actions. The environment has a simple winning strategy to make the system fail:\n\n1.  Wait for the system to be in one of the zones, for instance, zone $A$ (i.e., the state predicate $Z_A$ is true).\n2.  From that point onwards, the environment sets the lock input $\\ell$ to true and keeps it true forever. This is a valid environment behavior as there are no justice (fairness) assumptions on the environment.\n3.  Due to the safety constraint $\\mathbf{G}(\\ell \\rightarrow X z = z)$, once $\\ell$ is permanently true, the system's state variable $z$ can no longer change. If the system was in zone $A$, it will remain in zone $A$ for all future steps.\n4.  This behavior satisfies the system guarantee $\\mathbf{G}\\mathbf{F}\\, Z_A$ but makes it impossible to satisfy $\\mathbf{G}\\mathbf{F}\\, Z_B$. The system will never again visit zone $B$.\n\nSince the environment has a strategy that forces the system to violate one of its guarantees, the system does not have a winning strategy. Therefore, the specification is unrealizable. To restore realizability, the system must be protected from this \"permanent lock\" strategy.\n\n### Evaluation of Modifications\n\nThe goal is to find modifications that restore realizability while ensuring the robot continues to visit both zones infinitely often.\n\n**A. Add an environment justice constraint $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$ while leaving the system justice $\\mathbf{G}\\mathbf{F}\\, Z_A \\land \\mathbf{G}\\mathbf{F}\\, Z_B$ unchanged.**\n\nThis modification introduces a fairness assumption on the environment: it must release the lock (set $\\ell$ to false) infinitely often. This directly counters the hostile environment strategy described above. The environment is no longer allowed to keep $\\ell$ true forever.\nIf the system is in zone $A$ and needs to transition to zone $B$ to satisfy its $\\mathbf{G}\\mathbf{F}\\, Z_B$ guarantee, it can simply wait. The environment, bound by its own justice constraint $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$, must eventually provide a state where $\\neg \\ell$ holds. In such a state, the safety constraint $\\mathbf{G}(\\ell \\rightarrow X z = z)$ does not apply, and the system is free to transition to zone $B$. The same logic applies for transitioning from $B$ to $A$.\nThis modification makes the specification realizable and explicitly preserves the original system justice goals. The robot is guaranteed to be able to visit both zones infinitely often.\n\n**Verdict: Correct.**\n\n**B. Replace the system justice $\\mathbf{G}\\mathbf{F}\\, Z_A \\land \\mathbf{G}\\mathbf{F}\\, Z_B$ with a single justice $\\mathbf{G}\\mathbf{F}\\,(Z_A \\lor Z_B)$, leaving environment assumptions unchanged.**\n\nThe predicates $Z_A \\equiv (z = A)$ and $Z_B \\equiv (z = B)$ are mutually exclusive and cover the state space of $z$. Thus, the proposition $(Z_A \\lor Z_B)$ is always true (a tautology). The new system justice requirement $\\mathbf{G}\\mathbf{F}\\,(Z_A \\lor Z_B)$ is equivalent to $\\mathbf{G}\\mathbf{F}\\,\\text{true}$, which is also a tautology. A tautological guarantee requires nothing from the system. The specification becomes trivially realizable (the system can do anything, including staying in one zone forever).\nHowever, this modification fails the condition to \"preserve the original intent to visit both zones infinitely often.\" It completely removes the liveness requirement for cyclic visits.\n\n**Verdict: Incorrect.**\n\n**C. Add a system safety constraint $\\mathbf{G}\\,(X z \\neq z)$ that forces the robot to change zones on every step, leaving environment assumptions and system justice unchanged.**\n\nThis modification introduces a new system safety constraint, $\\mathbf{G}(X z \\neq z)$. This must hold simultaneously with the existing plant coupling constraint, $\\mathbf{G}(\\ell \\rightarrow X z = z)$. Let's analyze their interaction.\nSuppose the environment sets $\\ell$ to true at some time step $t$.\n- From $\\mathbf{G}(\\ell \\rightarrow X z = z)$, it must be that at time $t+1$, $X z = z$.\n- From $\\mathbf{G}(X z \\neq z)$, it must be that at time $t+1$, $X z \\neq z$.\nThese two conditions, $X z = z$ and $X z \\neq z$, are a logical contradiction. If the environment ever plays $\\ell$, the system has no valid next move. Since the environment is unconstrained and free to play $\\ell$, the safety portion of the specification is unsatisfiable. An unrealizable specification is the result.\n\n**Verdict: Incorrect.**\n\n**D. Refine the system justice to $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_A)$ and $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_B)$, and add an environment justice $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$.**\n\nThis modification does two things: it adds the same environment justice constraint as in option A, and it strengthens the system's justice goals.\n1.  **Environment Justice $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$:** As established in the analysis of option A, this alone is sufficient to restore realizability for the original system goals.\n2.  **Refined System Justice:** The new goals are $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_A)$ and $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_B)$. This means the system must infinitely often be in zone $A$ while the lock is off, and infinitely often be in zone $B$ while the lock is off.\n\nLet's check for realizability. A winning strategy for the system is as follows: If in zone $A$, wait for the environment to play $\\neg \\ell$. This is guaranteed to happen eventually due to $\\mathbf{G}\\mathbf{F}\\, \\neg \\ell$. When it does, the goal $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_A)$ is being worked towards. Then, while the lock is off, transition to zone $B$. Now in zone $B$, wait for the next $\\neg \\ell$ state, satisfying progress towards $\\mathbf{G}\\mathbf{F}\\,(\\neg \\ell \\land Z_B)$, and then transition back to $A$. This strategy works, so the specification is realizable.\n\nDoes it preserve the original intent? The original intent was $\\mathbf{G}\\mathbf{F}\\,Z_A \\land \\mathbf{G}\\mathbf{F}\\,Z_B$. The new justice goals logically imply the original ones. For example, if the state $(\\neg \\ell \\land Z_A)$ occurs infinitely often, then $Z_A$ must also occur infinitely often. Thus, any system satisfying the new guarantees will also satisfy the original ones. The intent is not only preserved but made more specific by tying the required visits to the opportunities for movement. This is a sound and common refinement pattern in synthesis.\n\n**Verdict: Correct.**\n\n**E. Introduce a system scheduler variable $s$ with $\\mathbf{G}\\,(X s = \\neg s)$ and guarantees $\\mathbf{G}\\mathbf{F}\\,(s \\rightarrow Z_A)$ and $\\mathbf{G}\\mathbf{F}\\,(\\neg s \\rightarrow Z_B)$, leaving environment assumptions unchanged.**\n\nThis modification introduces an internal system variable $s$ that toggles at every step. It replaces the original justice goals with $\\mathbf{G}\\mathbf{F}\\,(s \\rightarrow Z_A)$ and $\\mathbf{G}\\mathbf{F}\\,(\\neg s \\rightarrow Z_B)$. Let's examine these new goals under the hostile environment strategy (permanent lock).\nAssume the system is in zone $A$ and the environment plays $\\ell$ forever, trapping the system in zone $A$. So, from some point on, $\\mathbf{G}(Z_A)$ holds and $\\mathbf{G}(\\neg Z_B)$ holds.\n- The first guarantee is $\\mathbf{G}\\mathbf{F}\\,(s \\rightarrow Z_A)$, which is $\\mathbf{G}\\mathbf{F}\\,(\\neg s \\lor Z_A)$. Since $Z_A$ is always true, the expression $(\\neg s \\lor Z_A)$ is always true. This guarantee is satisfied.\n- The second guarantee is $\\mathbf{G}\\mathbf{F}\\,(\\neg s \\rightarrow Z_B)$, which is $\\mathbf{G}\\mathbf{F}\\,(s \\lor Z_B)$. Since $Z_B$ is always false, this simplifies to $\\mathbf{G}\\mathbf{F}\\,(s)$. The system controls $s$ and its rule is $\\mathbf{G}(X s = \\neg s)$, which means $s$ toggles value at every step. Therefore, $s$ is true infinitely often, and $\\mathbf{G}\\mathbf{F}\\,(s)$ is satisfied.\n\nBoth guarantees are met even when the system is permanently stuck in zone $A$. A symmetric argument shows they are also met if the system is stuck in zone $B$. This means the specification is realizable; a valid strategy is for the system to simply stay in its initial zone forever. This fails to \"preserve the original intent to visit both zones infinitely often.\"\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}