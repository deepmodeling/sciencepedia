{
    "hands_on_practices": [
        {
            "introduction": "The foundation of correct-by-construction controller synthesis lies in the precise expression of system requirements. This exercise  provides hands-on practice in the crucial first step: translating high-level, informal objectives for a cyber-physical system into the unambiguous language of Linear Temporal Logic (LTL). You will learn to distinguish between safety properties, which stipulate that \"something bad never happens,\" and liveness properties, which ensure that \"something good eventually happens,\" and encode them into formal specifications.",
            "id": "4210942",
            "problem": "A Cyber-Physical System (CPS) integrates physical processes with computation and communication, and a Digital Twin (DT) mirrors the CPS state to support monitoring and control. Consider synthesizing a correct-by-construction controller from Linear Temporal Logic (LTL) specifications for a coolant loop subsystem. The control objective is to coordinate actuation and supervisory modes while respecting safety interlocks and recovery behavior under communication loss and fault conditions.\n\nYou are given the following atomic propositions, all Boolean and evaluated at discrete time steps along an execution trace:\n- $h$: the physical system reports high pressure (above a certified threshold).\n- $o$: the controller issues the command to open the coolant valve.\n- $d$: the diagnostic twin flags a fault detected in the loop.\n- $s$: the controller is in safe mode.\n- $r$: a reset command is observed.\n- $p$: the polling action for sensor data occurs.\n- $c$: communication is up (the DT is connected to the CPS).\n- $y$: the DT is synchronized with the CPS state.\n\nThe temporal requirements of interest are:\n- $\\mathsf{R\\_1}$: The controller must not open the valve while pressure is high.\n- $\\mathsf{R\\_2}$: If a fault is detected, the controller should eventually enter safe mode.\n- $\\mathsf{R\\_3}$: Sensor polling should occur infinitely often.\n- $\\mathsf{R\\_4}$: Once in safe mode, it remains active until a reset occurs (and may remain active forever if no reset occurs).\n- $\\mathsf{R\\_5}$: Whenever communication is up, synchronization with the physical system should eventually be achieved.\n\nSelect the option that provides both the correct safety-versus-liveness classification for each requirement and a correct LTL encoding using the standard operators $G$ (globally), $F$ (eventually), $U$ (until), $R$ (release), $\\neg$ (negation), $\\wedge$ (conjunction), and $\\rightarrow$ (implication).\n\nA. \n- $\\mathsf{R\\_1}$: safety; $G\\,\\neg(h \\wedge o)$.\n- $\\mathsf{R\\_2}$: liveness; $G\\,(d \\rightarrow F\\,s)$.\n- $\\mathsf{R\\_3}$: liveness; $G F\\,p$.\n- $\\mathsf{R\\_4}$: safety; $G\\big(s \\rightarrow (\\,r \\mathbin{R} s\\,)\\big)$.\n- $\\mathsf{R\\_5}$: liveness; $G\\,(c \\rightarrow F\\,y)$.\n\nB.\n- $\\mathsf{R\\_1}$: liveness; $F\\,\\neg(h \\wedge o)$.\n- $\\mathsf{R\\_2}$: safety; $G\\,(d \\rightarrow G\\,s)$.\n- $\\mathsf{R\\_3}$: safety; $F G\\,p$.\n- $\\mathsf{R\\_4}$: liveness; $G\\,(s \\rightarrow F\\,r)$.\n- $\\mathsf{R\\_5}$: safety; $G\\,(c \\rightarrow G\\,y)$.\n\nC.\n- $\\mathsf{R\\_1}$: safety; $G\\,(h \\rightarrow o)$.\n- $\\mathsf{R\\_2}$: liveness; $F\\,(s \\wedge d)$.\n- $\\mathsf{R\\_3}$: liveness; $G F\\,p$.\n- $\\mathsf{R\\_4}$: safety; $G\\big(s \\rightarrow (\\,s \\mathbin{U} r\\,)\\big)$.\n- $\\mathsf{R\\_5}$: liveness; $F\\,y$.\n\nD.\n- $\\mathsf{R\\_1}$: safety; $G\\,\\neg(h \\wedge o)$.\n- $\\mathsf{R\\_2}$: liveness; $G\\,(d \\rightarrow F\\,s)$.\n- $\\mathsf{R\\_3}$: liveness; $F G\\,p$.\n- $\\mathsf{R\\_4}$: safety; $G\\big(s \\rightarrow (\\,s \\mathbin{R} r\\,)\\big)$.\n- $\\mathsf{R\\_5}$: liveness; $G\\,(F\\,y)$.",
            "solution": "The problem statement is a well-posed exercise in formal methods, specifically the translation of natural-language requirements for a Cyber-Physical System (CPS) into Linear Temporal Logic (LTL) specifications and their classification as safety or liveness properties.\n\n### Step 1: Extract Givens\nThe problem provides the following verbatim information:\n- **Atomic Propositions (Boolean):**\n  - $h$: the physical system reports high pressure.\n  - $o$: the controller issues the command to open the coolant valve.\n  - $d$: the diagnostic twin flags a fault detected in the loop.\n  - $s$: the controller is in safe mode.\n  - $r$: a reset command is observed.\n  - $p$: the polling action for sensor data occurs.\n  - $c$: communication is up.\n  - $y$: the DT is synchronized with the CPS state.\n- **Temporal Requirements:**\n  - $\\mathsf{R\\_1}$: The controller must not open the valve while pressure is high.\n  - $\\mathsf{R\\_2}$: If a fault is detected, the controller should eventually enter safe mode.\n  - $\\mathsf{R\\_3}$: Sensor polling should occur infinitely often.\n  - $\\mathsf{R\\_4}$: Once in safe mode, it remains active until a reset occurs (and may remain active forever if no reset occurs).\n  - $\\mathsf{R\\_5}$: Whenever communication is up, synchronization with the physical system should eventually be achieved.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established validation criteria.\n- **Scientifically Grounded:** The problem uses standard, well-defined concepts from computer science and control theory, namely Linear Temporal Logic (LTL) for specifying system behavior. The scenario represents a typical application in the domain of CPS and digital twins. The problem is scientifically sound.\n- **Well-Posed:** The natural language requirements are stated with sufficient clarity to permit a unique formalization in LTL. The task is to identify the correct LTL formula and property classification (safety/liveness), which is a determinate problem.\n- **Objective:** The language is technical and free of subjectivity or ambiguity.\n- **Complete and Consistent:** The provided atomic propositions are sufficient to express the given requirements. There are no evident contradictions in the problem setup.\n- **Realistic and Feasible:** The scenario of a coolant loop with safety interlocks is a realistic engineering problem for which formal specification is a valid approach.\n\n### Step 3: Derivation of Correct Specifications\n\nBefore evaluating the options, we will first derive the correct classifications and LTL formulas for each requirement based on first principles.\n\nA **safety** property asserts that \"something bad never happens\". A violation of a safety property can always be demonstrated by a finite execution trace.\nA **liveness** property asserts that \"something good will eventually happen\". A violation of a liveness property can only be demonstrated by an infinite execution trace, as the \"good thing\" could always happen at some later time.\n\n- **$\\mathsf{R\\_1}$: The controller must not open the valve while pressure is high.**\n  - **Classification:** This is a **safety** property. The \"bad thing\" is the simultaneous occurrence of high pressure ($h$) and the valve-open command ($o$). If a state where $h \\wedge o$ is true is observed at any finite time, the property is violated.\n  - **LTL Encoding:** The property \"never ($h \\wedge o$)\" must hold at all times (globally). This is expressed as $G\\,\\neg(h \\wedge o)$.\n\n- **$\\mathsf{R\\_2}$: If a fault is detected, the controller should eventually enter safe mode.**\n  - **Classification:** This is a **liveness** property. The \"good thing\" is entering safe mode ($s$) at some point after a fault ($d$) is detected. A finite trace where a fault has occurred but safe mode has not yet been entered does not constitute a violation, as safe mode could still be entered in the future.\n  - **LTL Encoding:** This requirement must hold for all time. \"Globally, if $d$ is true, then eventually $s$ becomes true\". This translates to the LTL formula $G\\,(d \\rightarrow F\\,s)$.\n\n- **$\\mathsf{R\\_3}$: Sensor polling should occur infinitely often.**\n  - **Classification:** This is a quintessential **liveness** property. It requires that the \"good thing\" of polling ($p$) happens again and again, without end. A violation would be a trace where, after some finite time, polling never occurs again.\n  - **LTL Encoding:** The property \"infinitely often $p$\" is expressed as \"globally, eventually $p$\" or $G F\\,p$.\n\n- **$\\mathsf{R\\_4}$: Once in safe mode, it remains active until a reset occurs (and may remain active forever if no reset occurs).**\n  - **Classification:** This is a **safety** property. The \"bad thing\" is leaving safe mode ($\\neg s$) without having been preceded by a reset ($r$). A finite trace showing a transition from a state with $s$ to one with $\\neg s$, where the first state did not have $r$, is a finite witness to a violation.\n  - **LTL Encoding:** The statement \"Once in safe mode...\" translates to a global implication \"Globally, if in safe mode...\": $G\\,(s \\rightarrow \\dots)$. The consequent specifies that the system \"remains active ($s$) until a reset ($r$)\", including the case where a reset never occurs. This is the definition of \"weak until\". The `Release` operator $R$ is suitable here. The formula $\\phi_1 \\mathbin{R} \\phi_2$ means that $\\phi_2$ must remain true up to and including the point where $\\phi_1$ becomes true; if $\\phi_1$ never becomes true, $\\phi_2$ must remain true forever. Here, the property to be maintained is $s$, and the releasing condition is $r$. Thus, the state-property we want to hold from the moment $s$ is true is $r \\mathbin{R} s$. The full formula is $G\\big(s \\rightarrow (r \\mathbin{R} s)\\big)$.\n\n- **$\\mathsf{R\\_5}$: Whenever communication is up, synchronization with the physical system should eventually be achieved.**\n  - **Classification:** This is a **liveness** property. The \"good thing\" is achieving synchronization ($y$) after communication is up ($c$). A finite trace where $c$ is true but $y$ has not yet become true is not a violation, because synchronization could happen later.\n  - **LTL Encoding:** The word \"Whenever\" implies a global scope. \"Globally, if communication is up ($c$), then eventually synchronization ($y$) is achieved\". This translates directly to $G\\,(c \\rightarrow F\\,y)$.\n\n### Summary of Correct Specifications\n- $\\mathsf{R\\_1}$: safety; $G\\,\\neg(h \\wedge o)$\n- $\\mathsf{R\\_2}$: liveness; $G\\,(d \\rightarrow F\\,s)$\n- $\\mathsf{R\\_3}$: liveness; $G F\\,p$\n- $\\mathsf{R\\_4}$: safety; $G\\big(s \\rightarrow (r \\mathbin{R} s)\\big)$\n- $\\mathsf{R\\_5}$: liveness; $G\\,(c \\rightarrow F\\,y)$\n\n### Option-by-Option Analysis\n\n- **A.** This option provides the correct classification and LTL formula for all five requirements, matching our derived specifications exactly.\n- **B.** This option incorrectly classifies $\\mathsf{R\\_1}$ as liveness, $\\mathsf{R\\_2}$ as safety, and uses the incorrect formula $F G\\,p$ for \"infinitely often.\"\n- **C.** This option uses a logically flawed formula for $\\mathsf{R\\_1}$, an incorrect formula for $\\mathsf{R\\_2}$, and the \"strong until\" operator ($U$) for $\\mathsf{R\\_4}$, which would wrongly imply a reset must eventually occur.\n- **D.** This option incorrectly uses $F G\\,p$ for $\\mathsf{R\\_3}$ (\"eventually forever\" instead of \"infinitely often\") and swaps the arguments of the Release operator for $\\mathsf{R\\_4}$.\n\nBased on the analysis, only option A is entirely correct.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a specification is formalized, we must determine if it is achievable, or \"realizable.\" This exercise  frames this question as a strategic game between the controller and an adversarial environment. By analyzing a simple robot navigation task, you will explore how the existence of a winning controller strategy can depend critically on the initial state of the system and the assumptions made about the environment's behavior, revealing the game-theoretic heart of controller synthesis.",
            "id": "4210918",
            "problem": "A digital twin of a corridor-door-robot system is used to synthesize a controller for a cyber-physical robot. The corridor has discrete positions $p \\in \\{0,1,2\\}$, where position $p=2$ is the goal region beyond a door located between $p=1$ and $p=2$. At each discrete time step $t \\in \\mathbb{N}$, the environment (door process) first sets the door state $d_t \\in \\{0,1\\}$, where $d_t=1$ means “open” and $d_t=0$ means “closed,” and then the controller chooses an action $a_t \\in \\{\\text{stay}, \\text{right}\\}$ based on the observed history. The kinematics are\n- if $a_t=\\text{stay}$, then $p_{t+1}=p_t$,\n- if $a_t=\\text{right}$, then $p_{t+1}=\\min\\{p_t+1,2\\}$.\n\nThe closed-loop behavior is evaluated against a Linear Temporal Logic (LTL) specification $\\varphi$ over the trace of $(p_t,d_t)$:\n$$\n\\varphi \\;\\;=\\;\\; G\\big((p=1 \\wedge d=0)\\rightarrow X(p=1)\\big)\\;\\wedge\\; F(p=2).\n$$\nThe safety conjunct $G\\big((p=1 \\wedge d=0)\\rightarrow X(p=1)\\big)$ requires that if the robot is at $p=1$ while the door is closed, then on the next step the robot must remain at $p=1$ (i.e., it must not pass a closed door). The liveness conjunct $F(p=2)$ requires that eventually the robot reaches the goal position $p=2$. There are no environment fairness or liveness assumptions (the environment may keep the door closed forever).\n\nA controller strategy is a function $\\sigma_C$ mapping finite histories of states and door signals to actions. An environment strategy is a function $\\sigma_E$ mapping finite histories to door signals. Given an initial-state set $S_0 \\subseteq \\{0,1,2\\}$, realizability “from $S_0$” means: there exists a controller strategy $\\sigma_C$ such that, for every initial position $p_0 \\in S_0$ and for every environment strategy $\\sigma_E$, the resulting closed-loop trace satisfies $\\varphi$.\n\nWhich option correctly characterizes how realizability depends on the choice of initial-state set $S_0$ for this system and specification, and justifies the difference?\n\nA. The specification $\\varphi$ is realizable exactly for initial-state sets $S_0$ satisfying $S_0 \\subseteq \\{2\\}$; in particular from $S_0=\\{2\\}$ it is realizable (e.g., by always choosing $a_t=\\text{stay}$), while from any $S_0$ containing a state with $p \\neq 2$ it is not realizable, since the environment can keep $d_t=0$ forever, falsifying $F(p=2)$.\n\nB. The specification $\\varphi$ is realizable from $S_0=\\{1\\}$ by a strategy that waits until $d=1$ and then moves right, but it is not realizable from $S_0=\\{0\\}$ because the safety conjunct prevents reaching the goal from $p=0$.\n\nC. The specification $\\varphi$ is realizable from every nonempty $S_0$ because the controller can force $F(p=2)$ by repeatedly choosing $a_t=\\text{right}$, eventually reaching $p=2$ regardless of the door state.\n\nD. The specification $\\varphi$ is realizable from $S_0=\\{2\\}$ and from any $S_0$ that intersects $\\{2\\}$, because the controller can ensure the run effectively begins at $p=2$ and then maintain safety thereafter.",
            "solution": "The problem asks for the set of initial positions $S_0 \\subseteq \\{0,1,2\\}$ from which the Linear Temporal Logic (LTL) specification $\\varphi$ is realizable. Realizability from a set $S_0$ means there exists a single controller strategy $\\sigma_C$ that guarantees $\\varphi$ is satisfied for any run starting from any $p_0 \\in S_0$ and against any environment strategy $\\sigma_E$. This is equivalent to finding the set of winning initial states for the controller in a two-player game against the environment.\n\nThe LTL specification is $\\varphi = \\varphi_S \\wedge \\varphi_L$, where:\n-   The safety part is $\\varphi_S = G\\big((p=1 \\wedge d=0)\\rightarrow X(p=1)\\big)$.\n-   The liveness part is $\\varphi_L = F(p=2)$.\n\nA controller must satisfy both parts for all possible environment behaviors.\n\nFirst, let's analyze the safety conjunct $\\varphi_S$. It states that \"Globally, if the robot is at position $p=1$ and the door is closed ($d=0$), then in the next time step, the robot must be at position $p=1$.\" Let's examine the controller's options at $p_t=1$:\n-   If the controller chooses $a_t = \\text{stay}$, the next position is $p_{t+1}=p_t=1$. This satisfies the requirement $X(p=1)$.\n-   If the controller chooses $a_t = \\text{right}$, the next position is $p_{t+1}=\\min\\{1+1,2\\}=2$. This violates the requirement $X(p=1)$.\n\nTherefore, to satisfy $\\varphi_S$, any valid controller strategy $\\sigma_C$ must enforce the following rule: if the system state is $(p_t, d_t) = (1, 0)$, the controller must choose the action $a_t = \\text{stay}$.\n\nNext, let's analyze the liveness conjunct $\\varphi_L$, which requires that the robot eventually reaches the goal position $p=2$. The controller must be able to force this outcome against any environment strategy. The problem critically states that there are no fairness assumptions on the environment; specifically, \"the environment may keep the door closed forever.\"\n\nWe now determine the set of winning initial positions by checking each possible starting position $p_0 \\in \\{0, 1, 2\\}$.\n\n**Case 1: $p_0 = 2$.**\nThe liveness requirement $F(p=2)$ is immediately satisfied at $t=0$. To satisfy the safety requirement $\\varphi_S$, the controller can adopt the strategy \"always choose $a_t = \\text{stay}$.\" With this strategy, the robot's position remains $p_t=2$ for all $t \\in \\mathbb{N}$. The antecedent of the implication in $\\varphi_S$, $(p=1 \\wedge d=0)$, is never true. Thus, the implication is vacuously true for all time, and $\\varphi_S$ is satisfied. Since both $\\varphi_S$ and $\\varphi_L$ are satisfied, the specification is **realizable** from $p_0=2$.\n\n**Case 2: $p_0 = 1$.**\nTo satisfy the liveness requirement $F(p=2)$, the controller must eventually choose the action $a_t = \\text{right}$ while at $p_t=1$. However, this action is only permissible if the safety condition is not violated. Now, consider a hostile environment strategy $\\sigma_E$ that always chooses $d_t=0$.\n- At $t=0$, the state is $p_0=1$. The environment chooses $d_0=0$.\n- The controller must choose an action. To satisfy $\\varphi_S$, it must choose $a_0=\\text{stay}$. So, $p_1=1$.\n- At $t=1$, the state is $p_1=1$. The environment again chooses $d_1=0$.\n- The controller must again choose $a_1=\\text{stay}$. So, $p_2=1$.\nThis continues indefinitely. The resulting trace has $p_t=1$ for all $t \\in \\mathbb{N}$. On this trace, the position $p=2$ is never reached, so the liveness requirement $F(p=2)$ is violated. Since there exists an environment strategy that can force a violation of $\\varphi$, the specification is **not realizable** from $p_0=1$.\n\n**Case 3: $p_0=0$.**\nTo make progress towards the goal $p=2$, the controller must choose $a_t=\\text{right}$.\n- Let's say the controller chooses $a_0=\\text{right}$. Then $p_1=\\min\\{0+1,2\\}=1$.\n- At time $t=1$, the robot is at position $p=1$. This reduces the problem to Case 2. As established, from $p=1$, an adversarial environment can prevent the robot from ever reaching $p=2$.\nTherefore, the specification is **not realizable** from $p_0=0$.\n\n**Conclusion:** The set of winning initial states for the controller is $\\{2\\}$. Realizability from a set $S_0$ requires that every $p_0 \\in S_0$ be a winning state. Thus, the specification is realizable if and only if $S_0 \\subseteq \\{2\\}$.\n\nNow we evaluate the given options.\n\n**A.** This option correctly states that realizability holds for $S_0 \\subseteq \\{2\\}$ and explains that from any other state, the environment can keep the door closed ($d_t=0$), trapping the robot due to the safety constraint and thus falsifying the liveness goal $F(p=2)$. This matches our analysis perfectly.\n\n**B.** This option incorrectly claims the specification is realizable from $S_0=\\{1\\}$.\n\n**C.** This option proposes a strategy (always move right) that violates the safety specification.\n\n**D.** This option misunderstands the definition of realizability from a set. Realizability from $S_0=\\{1,2\\}$ would require winning from both $p_0=1$ and $p_0=2$, but $p_0=1$ is a losing state.\n\nTherefore, option A provides the only correct characterization.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "In practice, an initial specification is often found to be unrealizable, indicating a fundamental conflict between the system's goals and the environment's capabilities. This exercise  guides you through this vital debugging process. You will diagnose why a seemingly reasonable specification for a service robot is impossible to fulfill and then evaluate different modifications—such as adding fairness assumptions about the environment—to repair the specification and restore realizability.",
            "id": "4210951",
            "problem": "A digital twin of a Cyber-Physical System (CPS) models a mobile service robot that must cyclically visit two service zones, denoted by the predicates $Z_A$ and $Z_B$, over an infinite horizon. The abstracted interaction is specified in Linear Temporal Logic (LTL) within the Generalized Reactivity of rank $1$ (GR(1)) fragment. The environment exposes a Boolean input $\\ell$ indicating a safety lock: when $\\ell$ holds, the robot’s zone cannot change; when $\\neg \\ell$ holds, the robot may change zones. The system controls a Boolean state variable $z$ that encodes its current zone, with $Z_A \\equiv (z = A)$ and $Z_B \\equiv (z = B)$ and $A \\neq B$. The plant abstraction imposes the following safety on the combined transition relation:\n- $G\\big(\\ell \\rightarrow X z = z\\big)$ and $G\\big(\\neg \\ell \\rightarrow (X z = z \\lor X z \\neq z)\\big)$, expressing that zone changes are forbidden under lock and permitted (but not forced) when unlocked.\nAssume unconstrained environment initials and safety beyond the above coupling, and assume the system initial zone is arbitrary but fixed by $z$.\n\nThe GR(1) specification is organized as follows:\n- Environment assumptions: no initial or safety constraints beyond the plant coupling above, and no environment justice constraints.\n- System guarantees: $G$-safety includes the plant coupling above, and the system justice (Büchi) constraints require the robot to visit both zones infinitely often: $\\mathbf{GF}\\, Z_A$ and $\\mathbf{GF}\\, Z_B$.\n\nTask:\n- Using the standard semantics of LTL and GR(1) winning conditions in two-player games (environment moves first, system responds), determine why the specification above is unrealizable. In particular, decompose the cause of unrealizability down to the conflicting justice constraints in the presence of the lock dynamics.\n- Then, select all modifications among the choices below that restore realizability while preserving the original intent to visit both zones infinitely often under physically sound assumptions. Modifications may alter environment assumptions, system safety, and/or system justice, but must respect the plant coupling and scientific realism of the CPS.\n\nChoices:\nA. Add an environment justice constraint $\\mathbf{GF}\\, \\neg \\ell$ while leaving the system justice $\\mathbf{GF}\\, Z_A \\land \\mathbf{GF}\\, Z_B$ unchanged.\n\nB. Replace the system justice $\\mathbf{GF}\\, Z_A \\land \\mathbf{GF}\\, Z_B$ with a single justice $\\mathbf{GF}\\,(Z_A \\lor Z_B)$, leaving environment assumptions unchanged.\n\nC. Add a system safety constraint $G\\,(X z \\neq z)$ that forces the robot to change zones on every step, leaving environment assumptions and system justice unchanged.\n\nD. Refine the system justice to $\\mathbf{GF}\\,(\\neg \\ell \\land Z_A)$ and $\\mathbf{GF}\\,(\\neg \\ell \\land Z_B)$, and add an environment justice $\\mathbf{GF}\\, \\neg \\ell$.\n\nE. Introduce a system scheduler variable $s$ with $G\\,(X s = \\neg s)$ and guarantees $\\mathbf{GF}\\,(s \\rightarrow Z_A)$ and $\\mathbf{GF}\\,(\\neg s \\rightarrow Z_B)$, leaving environment assumptions unchanged.\n\nSelect all correct choices.",
            "solution": "The problem describes a standard scenario in reactive synthesis where a specification is unrealizable due to an unconstrained, adversarial environment. We first diagnose the problem and then evaluate the proposed fixes.\n\n### Analysis of Unrealizability\n\nThe specification is unrealizable due to a conflict between the system's unconditional liveness guarantees and the environment's power to permanently restrict the system's actions.\n1.  **System's Liveness Guarantees:** The system promises to visit zone $A$ infinitely often ($\\mathbf{GF}\\, Z_A$) and zone $B$ infinitely often ($\\mathbf{GF}\\, Z_B$).\n2.  **Environment's Power:** The environment controls the lock input $\\ell$. Critically, there are no fairness or justice assumptions on the environment, meaning it can choose any sequence of values for $\\ell$.\n3.  **Safety Constraint (Plant Coupling):** The system must obey $G(\\ell \\rightarrow X z = z)$. If the environment sets $\\ell$ to true, the system is forbidden from changing zones.\n\nThe environment has a simple winning strategy to make the system fail:\n1.  Wait for the system to be in one zone, for example, zone $A$ (so $Z_A$ is true).\n2.  From that point on, the environment sets $\\ell$ to true and keeps it true forever. This is a valid environment behavior.\n3.  Due to the safety constraint, if the system is in zone $A$ and $\\ell$ is always true, the system's zone $z$ can never change. The robot will remain in zone $A$ forever.\n4.  This behavior makes it impossible for the system to satisfy its guarantee $\\mathbf{GF}\\, Z_B$. The system will never again visit zone $B$.\n\nSince the environment has a strategy that forces the system to violate one of its guarantees, the specification is unrealizable. To restore realizability, the specification must be modified to prevent this \"permanent lock\" scenario.\n\n### Evaluation of Modifications\n\n**A. Add an environment justice constraint $\\mathbf{GF}\\, \\neg \\ell$ while leaving the system justice $\\mathbf{GF}\\, Z_A \\land \\mathbf{GF}\\, Z_B$ unchanged.**\nThis modification introduces a fairness assumption: the environment must release the lock (set $\\ell$ to false) infinitely often. This directly counters the hostile environment strategy. If the system is in zone $A$ and needs to transition to zone $B$, it can wait. The environment, bound by its justice constraint, must eventually provide a state where $\\neg \\ell$ holds. In such a state, the system is free to transition to zone $B$. This logic applies symmetrically for transitioning from $B$ to $A$. The specification becomes realizable, and the original intent is preserved.\n**Verdict: Correct.**\n\n**B. Replace the system justice $\\mathbf{GF}\\, Z_A \\land \\mathbf{GF}\\, Z_B$ with a single justice $\\mathbf{GF}\\,(Z_A \\lor Z_B)$, leaving environment assumptions unchanged.**\nThe predicates $Z_A$ and $Z_B$ are mutually exclusive and cover the state space of $z$. Thus, $(Z_A \\lor Z_B)$ is always true. The new justice requirement $\\mathbf{GF}\\,(Z_A \\lor Z_B)$ is equivalent to $\\mathbf{GF}\\,\\text{true}$, which is a tautology and requires nothing from the system. While this makes the specification trivially realizable, it completely abandons the \"original intent to visit both zones infinitely often.\"\n**Verdict: Incorrect.**\n\n**C. Add a system safety constraint $G\\,(X z \\neq z)$ that forces the robot to change zones on every step, leaving environment assumptions and system justice unchanged.**\nThis introduces a new safety constraint $G(X z \\neq z)$ that must hold simultaneously with the plant coupling $G(\\ell \\rightarrow X z = z)$. Suppose the environment sets $\\ell$ to true. From the plant coupling, the system must ensure $X z = z$. From the new constraint, the system must ensure $X z \\neq z$. These two conditions, $X z = z$ and $X z \\neq z$, are contradictory. If the environment ever plays $\\ell$, the system has no valid move, making the safety part of the specification unsatisfiable.\n**Verdict: Incorrect.**\n\n**D. Refine the system justice to $\\mathbf{GF}\\,(\\neg \\ell \\land Z_A)$ and $\\mathbf{GF}\\,(\\neg \\ell \\land Z_B)$, and add an environment justice $\\mathbf{GF}\\, \\neg \\ell$.**\nThis modification adds the same sufficient environment justice constraint as in option A, making the specification realizable. It also strengthens the system's justice goals. The new goals require the system to infinitely often be in zone $A$ *while the lock is off*, and in zone $B$ *while the lock is off*. These new goals are stronger, but because the environment must provide $\\neg\\ell$ infinitely often, they are still achievable. Furthermore, they logically imply the original goals: if $\\mathbf{GF}\\,(\\neg \\ell \\land Z_A)$ is true, then $Z_A$ must be true infinitely often. Thus, the original intent is preserved. This is a sound refinement.\n**Verdict: Correct.**\n\n**E. Introduce a system scheduler variable $s$ with $G\\,(X s = \\neg s)$ and guarantees $\\mathbf{GF}\\,(s \\rightarrow Z_A)$ and $\\mathbf{GF}\\,(\\neg s \\rightarrow Z_B)$, leaving environment assumptions unchanged.**\nThis modification replaces the original justice goals. Let's test it against the hostile environment strategy. Assume the system is trapped in zone $A$ ($G(Z_A)$ holds) and the environment holds the lock $\\ell$ forever.\n- The first guarantee is $\\mathbf{GF}\\,(s \\rightarrow Z_A)$, which is $\\mathbf{GF}\\,(\\neg s \\lor Z_A)$. Since $Z_A$ is always true, this guarantee is satisfied.\n- The second guarantee is $\\mathbf{GF}\\,(\\neg s \\rightarrow Z_B)$, which is $\\mathbf{GF}\\,(s \\lor Z_B)$. Since the system is trapped, $Z_B$ is always false. The guarantee simplifies to $\\mathbf{GF}\\,(s)$. The system controls $s$ and makes it toggle at every step, so $s$ is true infinitely often. This guarantee is also satisfied.\nBoth guarantees are met even when the robot is permanently stuck in one zone. The specification becomes realizable, but it fails to \"preserve the original intent to visit both zones infinitely often.\"\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}