{
    "hands_on_practices": [
        {
            "introduction": "The interaction topology of a distributed system, mathematically captured by a graph Laplacian, fundamentally governs its collective dynamics. Understanding how modifications to the network structure affect the Laplacian's spectral properties is a cornerstone of designing and analyzing cyber-physical systems. This exercise  provides a foundational exploration into how augmenting a physical network with a corresponding virtual \"digital twin\" layer impacts the system's eigenvalues, offering direct insight into how twin-to-plant coupling can be tuned to shape overall network behavior.",
            "id": "4218234",
            "problem": "Consider an undirected, connected plant network of $n$ physical nodes, modeled as a Cyber-Physical System (CPS), with graph Laplacian $L \\in \\mathbb{R}^{n \\times n}$ constructed from the standard degree-adjacency definition $L = D - A$, where $D$ is the degree matrix and $A$ is the adjacency matrix. Assume the eigenvalues of $L$ are ordered as $0 = \\lambda_1 \\lt \\lambda_2 \\leq \\cdots \\leq \\lambda_n$. A Digital Twin (DT) layer is introduced by augmenting the plant graph with $n$ virtual twin nodes, one per plant node. The DT layer replicates the plant’s intra-layer topology exactly (same adjacency and weights), and each plant node is coupled only to its corresponding twin node with a positive scalar coupling strength $c \\gt 0$. The resulting $2n$-node augmented supra-Laplacian is the block matrix\n$$\nL_{\\mathrm{aug}}(c) \\;=\\;\n\\begin{pmatrix}\nL + c I & -\\,c I \\\\\n-\\,c I & L + c I\n\\end{pmatrix}\n\\in \\mathbb{R}^{2n \\times 2n},\n$$\nwhere $I$ is the $n \\times n$ identity matrix. Starting from the core definition of a Laplacian and fundamental linear algebra facts about eigenvalues and eigenvectors of symmetric matrices, derive the full change in the Laplacian spectrum caused by this augmentation by constructing eigenvectors of $L_{\\mathrm{aug}}(c)$ from those of $L$ and characterizing their associated eigenvalues. Using your derivation, determine the minimal coupling strength $c^{\\star}$ such that the algebraic connectivity (the second-smallest eigenvalue) of $L_{\\mathrm{aug}}(c)$ equals the plant’s algebraic connectivity $\\lambda_2$. Express your final answer for $c^{\\star}$ as a closed-form analytical expression in terms of $\\lambda_2$. No rounding is required and no units are involved.",
            "solution": "The problem is valid as it is well-posed, scientifically grounded in network theory and linear algebra, and contains all necessary information for a unique solution.\n\nLet $L \\in \\mathbb{R}^{n \\times n}$ be the Laplacian matrix of the plant network. By definition, $L$ is a symmetric, positive semi-definite matrix. Since the network is connected, its smallest eigenvalue is $\\lambda_1 = 0$ with multiplicity one, and the corresponding eigenvector is the all-ones vector, up to a scaling factor. Let $\\{v_i\\}_{i=1}^n$ be a set of orthonormal eigenvectors of $L$ with corresponding eigenvalues $0 = \\lambda_1 < \\lambda_2 \\leq \\cdots \\leq \\lambda_n$. The eigenvalue equation for the plant Laplacian is:\n$$\nL v_i = \\lambda_i v_i \\quad \\text{for } i = 1, 2, \\ldots, n\n$$\nThe augmented supra-Laplacian for the combined plant and Digital Twin system is given by the $2n \\times 2n$ block matrix:\n$$\nL_{\\mathrm{aug}}(c) =\n\\begin{pmatrix}\nL + c I & -c I \\\\\n-c I & L + c I\n\\end{pmatrix}\n$$\nwhere $I$ is the $n \\times n$ identity matrix and $c > 0$ is the coupling strength.\n\nOur first objective is to determine the eigenvalues of $L_{\\mathrm{aug}}(c)$. We seek to construct the eigenvectors of $L_{\\mathrm{aug}}(c)$ from the known eigenvectors of $L$. Let's propose a candidate eigenvector for $L_{\\mathrm{aug}}(c)$ of the form $V = \\begin{pmatrix} \\alpha v_i \\\\ \\beta v_i \\end{pmatrix}$, where $v_i$ is an eigenvector of $L$ for some $i \\in \\{1, \\ldots, n\\}$, and $\\alpha, \\beta$ are scalar coefficients to be determined.\n\nThe eigenvalue equation for the augmented system is $L_{\\mathrm{aug}}(c) V = \\mu V$, where $\\mu$ is the eigenvalue corresponding to the eigenvector $V$. Substituting the block matrix form of $L_{\\mathrm{aug}}(c)$ and the proposed form of $V$, we get:\n$$\n\\begin{pmatrix}\nL + c I & -c I \\\\\n-c I & L + c I\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha v_i \\\\\n\\beta v_i\n\\end{pmatrix}\n= \\mu\n\\begin{pmatrix}\n\\alpha v_i \\\\\n\\beta v_i\n\\end{pmatrix}\n$$\nThis matrix equation can be expanded into a system of two vector equations:\n$$\n\\begin{cases}\n(L + cI)(\\alpha v_i) - cI(\\beta v_i) = \\mu (\\alpha v_i) \\\\\n-cI(\\alpha v_i) + (L + cI)(\\beta v_i) = \\mu (\\beta v_i)\n\\end{cases}\n$$\nUsing the linearity of matrix multiplication and the property $L v_i = \\lambda_i v_i$, the system simplifies to:\n$$\n\\begin{cases}\n\\alpha L v_i + c \\alpha v_i - c \\beta v_i = \\mu \\alpha v_i \\\\\n-c \\alpha v_i + \\beta L v_i + c \\beta v_i = \\mu \\beta v_i\n\\end{cases}\n\\implies\n\\begin{cases}\n\\alpha \\lambda_i v_i + c \\alpha v_i - c \\beta v_i = \\mu \\alpha v_i \\\\\n-c \\alpha v_i + \\beta \\lambda_i v_i + c \\beta v_i = \\mu \\beta v_i\n\\end{cases}\n$$\nSince $v_i$ is a non-zero vector (as it is an eigenvector), we can factor it out from each equation:\n$$\n\\begin{cases}\n(\\alpha \\lambda_i + c \\alpha - c \\beta) v_i = (\\mu \\alpha) v_i \\\\\n(-c \\alpha + \\beta \\lambda_i + c \\beta) v_i = (\\mu \\beta) v_i\n\\end{cases}\n$$\nThis leads to a homogeneous system of linear equations for the coefficients $\\alpha$ and $\\beta$:\n$$\n\\begin{cases}\n(\\lambda_i + c - \\mu) \\alpha - c \\beta = 0 \\\\\n-c \\alpha + (\\lambda_i + c - \\mu) \\beta = 0\n\\end{cases}\n$$\nThis system can be written in matrix form as:\n$$\n\\begin{pmatrix}\n\\lambda_i + c - \\mu & -c \\\\\n-c & \\lambda_i + c - \\mu\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha \\\\\n\\beta\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\n$$\nFor a non-trivial solution for $(\\alpha, \\beta)$ to exist (which is required to form a non-zero eigenvector $V$), the determinant of the coefficient matrix must be zero:\n$$\n\\det\n\\begin{pmatrix}\n\\lambda_i + c - \\mu & -c \\\\\n-c & \\lambda_i + c - \\mu\n\\end{pmatrix}\n= 0\n$$\n$$\n(\\lambda_i + c - \\mu)^2 - (-c)^2 = 0\n$$\n$$\n(\\lambda_i + c - \\mu)^2 = c^2\n$$\nTaking the square root of both sides yields two possibilities for the eigenvalue $\\mu$:\n$$\n\\lambda_i + c - \\mu = \\pm c\n$$\nCase 1: $\\lambda_i + c - \\mu = c$\nThis simplifies to $\\mu = \\lambda_i$. This means that for each eigenvalue $\\lambda_i$ of $L$, there is an eigenvalue of $L_{\\mathrm{aug}}(c)$ with the same value.\n\nCase 2: $\\lambda_i + c - \\mu = -c$\nThis simplifies to $\\mu = \\lambda_i + 2c$. This means that for each eigenvalue $\\lambda_i$ of $L$, there is also an eigenvalue of $L_{\\mathrm{aug}}(c)$ equal to $\\lambda_i + 2c$.\n\nTherefore, the spectrum of $L_{\\mathrm{aug}}(c)$ is the union of the spectra of two shifted versions of $L$:\n$$\n\\text{Spec}(L_{\\mathrm{aug}}(c)) = \\{ \\lambda_1, \\lambda_2, \\ldots, \\lambda_n \\} \\cup \\{ \\lambda_1 + 2c, \\lambda_2 + 2c, \\ldots, \\lambda_n + 2c \\}\n$$\nThis completes the first part of the problem.\n\nThe second part is to find the minimal coupling strength $c^{\\star}$ such that the algebraic connectivity of $L_{\\mathrm{aug}}(c)$ equals the plant's algebraic connectivity, $\\lambda_2$. The algebraic connectivity is defined as the second-smallest eigenvalue of a Laplacian matrix.\n\nLet $\\mu_k$ denote the $k$-th smallest eigenvalue of $L_{\\mathrm{aug}}(c)$. We need to find $\\mu_2$. The eigenvalues of the plant network are ordered as $0 = \\lambda_1 < \\lambda_2 \\leq \\cdots \\leq \\lambda_n$. The eigenvalues of the augmented system, sorted in non-decreasing order, must be constructed from the set we derived.\nThe smallest eigenvalue is $\\mu_1 = \\lambda_1 = 0$. This confirms $L_{\\mathrm{aug}}(c)$ is a valid Laplacian for a (potentially disconnected) graph.\nThe candidates for the second-smallest eigenvalue, $\\mu_2$, are the smallest non-zero values from the full set of $2n$ eigenvalues. These are:\n\\begin{enumerate}\n    \\item The second-smallest eigenvalue from the first set, which is $\\lambda_2$.\n    \\item The smallest eigenvalue from the second set, which is $\\lambda_1 + 2c = 0 + 2c = 2c$.\n\\end{enumerate}\nAll other eigenvalues, such as $\\lambda_3$, $\\lambda_2 + 2c$, etc., are greater than or equal to $\\lambda_2$. And since $c>0$, all other eigenvalues in the second set are greater than $2c$.\nThus, the algebraic connectivity of the augmented system is the minimum of these two candidates:\n$$\n\\mu_2(c) = \\min(\\lambda_2, 2c)\n$$\nWe are asked to find the minimal $c^{\\star} > 0$ such that $\\mu_2(c^{\\star}) = \\lambda_2$. Substituting into the expression for $\\mu_2(c)$:\n$$\n\\min(\\lambda_2, 2c^{\\star}) = \\lambda_2\n$$\nFor the minimum of two positive numbers to be equal to the first number, the first number must be less than or equal to the second. This implies the condition:\n$$\n\\lambda_2 \\leq 2c^{\\star}\n$$\nRearranging for $c^{\\star}$, we get:\n$$\nc^{\\star} \\geq \\frac{\\lambda_2}{2}\n$$\nThe problem asks for the *minimal* coupling strength $c^{\\star}$ that satisfies this condition. The set of all valid $c$ values is the interval $[\\frac{\\lambda_2}{2}, \\infty)$. The minimal value in this set is achieved at the lower bound.\nTherefore, the minimal coupling strength is:\n$$\nc^{\\star} = \\frac{\\lambda_2}{2}\n$$\nSince the plant network is connected, $\\lambda_2 > 0$, which ensures that $c^{\\star} > 0$ in accordance with the problem's constraints.",
            "answer": "$$\n\\boxed{\\frac{\\lambda_2}{2}}\n$$"
        },
        {
            "introduction": "Building on the principles of spectral graph theory, a primary goal in distributed control is to design local control laws that achieve a desired global objective, such as cohesive formation flying. The stability and convergence speed of such multi-agent systems are intimately linked to the graph's spectral properties and the chosen controller gains. This practice problem  challenges you to apply these concepts by designing a distributed Proportional-Derivative (PD) controller that guides agents into a target formation, focusing on the crucial task of tuning gains to optimize performance across all network modes.",
            "id": "4218264",
            "problem": "A networked formation of $N=4$ identical mobile agents is represented within a Digital Twin (DT) of a Cyber-Physical System (CPS). Each agent has one-dimensional double-integrator dynamics with unit mass, given by $\\ddot{x}_{i}(t) = u_{i}(t)$, where $x_{i}(t) \\in \\mathbb{R}$ is the position of agent $i$ and $u_{i}(t) \\in \\mathbb{R}$ is its control input. The agents interact over a fixed undirected ring graph (cycle) with unit weights between adjacent agents: $1-2-3-4-1$. The target formation is specified by constant desired relative offsets $d_{ij}$ on each edge $(i,j)$, which are consistent and lead to a unique shape up to global translation and uniform motion.\n\nConsider the distributed Proportional-Derivative (PD) formation control law\n$$\nu_{i}(t) \\;=\\; -\\,k_{p}\\sum_{j \\in \\mathcal{N}_{i}} \\left( \\big(x_{i}(t)-x_{j}(t)\\big) - d_{ij} \\right)\\;-\\;k_{v}\\sum_{j \\in \\mathcal{N}_{i}} \\left( \\dot{x}_{i}(t)-\\dot{x}_{j}(t) \\right),\n$$\nwhere $\\mathcal{N}_{i}$ denotes the neighbor set of agent $i$, and $k_{p} > 0$, $k_{v} > 0$ are scalar gains to be designed. Assume the DT verifies that $k_{p}$ has been fixed to $k_{p} = 9$ in normalized units.\n\nStarting from fundamental definitions of the graph Laplacian and the double-integrator modal decomposition for distributed PD coupling, derive the closed-loop modal dynamics and the corresponding modal decay rates for all nonzero Laplacian eigenvalues. Then, determine the single value of $k_{v}$ that maximizes the guaranteed worst-case exponential convergence rate of the formation error across all nontrivial graph modes by balancing the slowest underdamped and the slowest overdamped modal decay rates. Compute this $k_{v}$ for the given ring graph and $k_{p}$.\n\nRound your final numerical value for $k_{v}$ to four significant figures. Express your answer in $\\mathrm{s}^{-1}$.",
            "solution": "The dynamics of the system can be expressed in vector form. Let $x(t) = [x_1(t), x_2(t), x_3(t), x_4(t)]^T$ be the vector of agent positions. The system dynamics are $\\ddot{x}(t) = u(t)$, where $u(t) = [u_1(t), u_2(t), u_3(t), u_4(t)]^T$.\n\nThe control law for agent $i$ can be written using the Graph Laplacian matrix $L$. For an undirected graph, the quantity $\\sum_{j \\in \\mathcal{N}_{i}} (y_i - y_j)$ is the $i$-th component of the vector $Ly$, where $y$ is a vector of states. The Laplacian matrix for the $N=4$ undirected ring graph is:\n$$\nL = D - A = \\begin{pmatrix} 2 & -1 & 0 & -1 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ -1 & 0 & -1 & 2 \\end{pmatrix}\n$$\nThe control law includes a constant term related to the desired offsets $d_{ij}$. Let $b$ be a constant vector whose $i$-th component is $b_i = \\sum_{j \\in \\mathcal{N}_i} d_{ij}$. The control law in vector form is:\n$$\nu(t) = -k_p (Lx(t) - b) - k_v L\\dot{x}(t)\n$$\nThe closed-loop dynamics are $\\ddot{x}(t) = -k_p (Lx(t) - b) - k_v L\\dot{x}(t)$, which rearranges to:\n$$\n\\ddot{x}(t) + k_v L\\dot{x}(t) + k_p Lx(t) = k_p b\n$$\nTo analyze convergence to the formation, we define an error vector. A static formation $x^*$ is an equilibrium state where $\\dot{x}^* = 0$ and $\\ddot{x}^*=0$. From the dynamics, this implies $k_p Lx^* = k_p b$, or $Lx^*=b$. The problem states that the offsets $d_{ij}$ are consistent, which guarantees that such a solution $x^*$ exists. Let the error vector be $e(t) = x(t) - x^*$. Then $\\dot{e}(t) = \\dot{x}(t)$ and $\\ddot{e}(t) = \\ddot{x}(t)$. Substituting these into the dynamics gives:\n$$\n\\ddot{e}(t) + k_v L\\dot{e}(t) + k_p L(e(t) + x^*) = k_p b\n$$\nUsing $Lx^* = b$, we obtain the homogeneous error dynamics equation:\n$$\n\\ddot{e}(t) + k_v L\\dot{e}(t) + k_p Le(t) = 0\n$$\nTo solve this system, we perform a modal decomposition using the spectral properties of the Laplacian matrix $L$. As $L$ is real and symmetric, it is diagonalizable by an orthogonal matrix of its eigenvectors. Let $L=V\\Lambda V^T$, where $\\Lambda = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4)$ is the diagonal matrix of eigenvalues and $V$ is the matrix of corresponding orthonormal eigenvectors.\nThe eigenvalues $\\lambda_k$ of a circulant matrix for an $N$-node ring graph are given by $\\lambda_k = 2 - 2\\cos(2\\pi (k-1)/N)$ for $k=1, \\dots, N$. For $N=4$:\n$\\lambda_1 = 2 - 2\\cos(0) = 0$\n$\\lambda_2 = 2 - 2\\cos(\\pi/2) = 2$\n$\\lambda_3 = 2 - 2\\cos(\\pi) = 4$\n$\\lambda_4 = 2 - 2\\cos(3\\pi/2) = 2$\nThe sorted eigenvalues are $0$, $2$, $2$, $4$. The distinct nonzero eigenvalues are $\\lambda_a=2$ and $\\lambda_b=4$.\n\nLet $e(t) = V\\eta(t)$, where $\\eta(t)$ are the modal coordinates. Substituting into the error dynamics and left-multiplying by $V^T$:\n$V^T V \\ddot{\\eta}(t) + k_v V^T L V \\dot{\\eta}(t) + k_p V^T L V \\eta(t) = 0$\n$\\ddot{\\eta}(t) + k_v \\Lambda \\dot{\\eta}(t) + k_p \\Lambda \\eta(t) = 0$\nThis system decouples into $N=4$ independent scalar second-order ordinary differential equations for each modal coordinate $\\eta_k$:\n$$\n\\ddot{\\eta}_k(t) + k_v \\lambda_k \\dot{\\eta}_k(t) + k_p \\lambda_k \\eta_k(t) = 0, \\quad k=1,2,3,4\n$$\nThe mode for $\\lambda_1 = 0$ gives $\\ddot{\\eta}_1(t) = 0$. This corresponds to the un-damped motion of the formation's center of mass, which does not affect the shape of the formation. The convergence of the formation error is determined by the modes associated with the nonzero eigenvalues.\n\nFor a nonzero eigenvalue $\\lambda_k \\in \\{2, 4\\}$, the modal dynamics are a standard second-order LTI system. The characteristic equation is $s^2 + (k_v \\lambda_k)s + (k_p \\lambda_k) = 0$. This corresponds to a system with natural frequency $\\omega_{n,k} = \\sqrt{k_p \\lambda_k}$ and damping ratio $\\zeta_k = \\frac{k_v \\lambda_k}{2\\omega_{n,k}} = \\frac{k_v}{2}\\sqrt{\\frac{\\lambda_k}{k_p}}$.\n\nThe exponential rate of convergence of each mode is given by the magnitude of the real part of the poles of its characteristic equation, $\\sigma_k = -\\mathrm{Re}(s_k)$. The poles are $s_{1,2} = -\\zeta_k \\omega_{n,k} \\pm \\omega_{n,k}\\sqrt{\\zeta_k^2 - 1}$.\nThe decay rate $\\sigma_k$ depends on the damping ratio $\\zeta_k$:\n-   If the mode is underdamped ($\\zeta_k < 1$), the poles are complex. The decay rate is $\\sigma_k = \\zeta_k \\omega_{n,k} = \\frac{k_v \\lambda_k}{2}$.\n-   If the mode is overdamped ($\\zeta_k > 1$), the poles are real and distinct. The decay is governed by the slower pole (closer to the origin), giving a rate of $\\sigma_k = \\zeta_k \\omega_{n,k} - \\omega_{n,k}\\sqrt{\\zeta_k^2 - 1}$.\n\nOur objective is to maximize the guaranteed worst-case convergence rate, which is $\\sigma_{worst} = \\min_{k} \\sigma_k$ over all non-trivial modes. Here, the non-trivial modes correspond to $\\lambda_a=2$ and $\\lambda_b=4$.\nGiven $k_p=9$:\nFor a mode with eigenvalue $\\lambda$, the condition for critical damping ($\\zeta=1$) is $k_v = 2\\sqrt{k_p/\\lambda}$.\n-   For $\\lambda_a=2$: $\\zeta_a=1$ at $k_v = 2\\sqrt{9/2} = 3\\sqrt{2}$.\n-   For $\\lambda_b=4$: $\\zeta_b=1$ at $k_v = 2\\sqrt{9/4} = 3$.\n\nLet's analyze the decay rates $\\sigma_a(k_v)$ and $\\sigma_b(k_v)$:\n-   For mode $a$ with $\\lambda_a=2$:\n    -   If $k_v < 3\\sqrt{2}$ (underdamped), $\\sigma_a(k_v) = \\frac{k_v \\lambda_a}{2} = k_v$.\n    -   If $k_v \\ge 3\\sqrt{2}$ (overdamped), $\\sigma_a(k_v) = \\frac{k_v \\cdot 2}{2} - \\sqrt{(\\frac{k_v \\cdot 2}{2})^2 - 9 \\cdot 2} = k_v - \\sqrt{k_v^2 - 18}$.\n-   For mode $b$ with $\\lambda_b=4$:\n    -   If $k_v < 3$ (underdamped), $\\sigma_b(k_v) = \\frac{k_v \\lambda_b}{2} = 2k_v$.\n    -   If $k_v \\ge 3$ (overdamped), $\\sigma_b(k_v) = \\frac{k_v \\cdot 4}{2} - \\sqrt{(\\frac{k_v \\cdot 4}{2})^2 - 9 \\cdot 4} = 2k_v - \\sqrt{4k_v^2 - 36}$.\n\nWe wish to maximize $\\sigma_{worst}(k_v) = \\min(\\sigma_a(k_v), \\sigma_b(k_v))$.\n-   For $0 < k_v \\le 3$: Both modes are underdamped. $\\sigma_a(k_v)=k_v$ and $\\sigma_b(k_v)=2k_v$. The minimum is $\\sigma_{worst}(k_v) = k_v$. This function increases with $k_v$.\n-   For $k_v > 3$: Mode $b$ becomes overdamped and its decay rate $\\sigma_b(k_v)$ starts to decrease from its peak value of $\\sigma_b(3)=6$. Mode $a$ is still underdamped and its decay rate $\\sigma_a(k_v)=k_v$ continues to increase.\nThe worst-case rate is determined by the slower mode. For $k_v \\le 3$, this is mode $a$. As $k_v$ increases past $3$, $\\sigma_a(k_v)$ continues to increase while $\\sigma_b(k_v)$ decreases. The optimal value of $k_v$ that maximizes the minimum of these two functions will occur at the point where their decay rates are equal, i.e., $\\sigma_a(k_v) = \\sigma_b(k_v)$. This balances the rates. This intersection must occur for $3 < k_v < 3\\sqrt{2}$, where mode $a$ is underdamped and mode $b$ is overdamped.\nWe set the decay rates equal:\n$$\n\\sigma_a(k_v) = \\sigma_b(k_v)\n$$\n$$\nk_v = 2k_v - \\sqrt{4k_v^2 - 36}\n$$\nRearranging the equation:\n$$\n\\sqrt{4k_v^2 - 36} = k_v\n$$\nSquaring both sides (valid since $k_v>0$):\n$$\n4k_v^2 - 36 = k_v^2\n$$\n$$\n3k_v^2 = 36\n$$\n$$\nk_v^2 = 12\n$$\n$$\nk_v = \\sqrt{12} = 2\\sqrt{3}\n$$\nWe must verify that this value of $k_v$ lies in the assumed interval $3 < k_v < 3\\sqrt{2}$.\n$(3)^2 = 9$, $(2\\sqrt{3})^2 = 12$, and $(3\\sqrt{2})^2 = 18$. Since $9 < 12 < 18$, our assumption was correct.\nThus, the optimal gain is $k_v = 2\\sqrt{3}$.\n\nThe problem requires a numerical value rounded to four significant figures.\n$$\nk_v = 2\\sqrt{3} \\approx 3.4641016...\n$$\nRounding to four significant figures, we get $k_v = 3.464$. The units are $\\mathrm{s}^{-1}$ as determined by dimensional analysis of the equation of motion $\\ddot{\\eta}_k + (k_v \\lambda_k)\\dot{\\eta}_k + \\dots = 0$, where the coefficient of the velocity term $\\dot{\\eta}_k$ must have units of $\\mathrm{s}^{-1}$ for the term to have units of acceleration.",
            "answer": "$$\n\\boxed{3.464}\n$$"
        },
        {
            "introduction": "Beyond analytical controller design, many modern distributed systems rely on optimization-based techniques like Model Predictive Control (MPC) to handle complex objectives and constraints. The Alternating Direction Method of Multipliers (ADMM) is a powerful framework for solving such problems in a decentralized manner, making it a key algorithm in the practitioner's toolbox. This hands-on coding exercise  requires you to implement an ADMM-based distributed MPC solver, bridging the gap from theory to practice by having you construct the optimization problem and derive robust termination criteria from first principles.",
            "id": "4218235",
            "problem": "Consider a network of $N$ discrete-time linear subsystems indexed by $i \\in \\{1,\\dots,N\\}$. Each subsystem evolves according to the state equation $x_{i,t+1} = A_i x_{i,t} + B_i u_{i,t}$ for $t \\in \\{0,1,\\dots,T-1\\}$, where $x_{i,t} \\in \\mathbb{R}$ and $u_{i,t} \\in \\mathbb{R}$ are scalar state and control at time $t$, and $A_i \\in \\mathbb{R}$, $B_i \\in \\mathbb{R}$. Let the initial state be $x_{i,0} \\in \\mathbb{R}$ for each subsystem $i$. The Model Predictive Control (MPC) objective over a horizon of length $T$ is the sum of quadratic stage and terminal costs, given by the well-tested formulation\n$$\nJ_i(x_{i,0}, u_i) = \\sum_{t=0}^{T-1} \\left( Q_i x_{i,t}^2 + R_i u_{i,t}^2 \\right) + P_i x_{i,T}^2,\n$$\nwhere $Q_i \\ge 0$, $R_i > 0$, and $P_i \\ge 0$ are scalar cost weights. The total objective is $\\sum_{i=1}^N J_i(x_{i,0}, u_i)$.\n\nA coupling resource constraint across subsystems is imposed at each time step $t$, requiring the sum of controls to equal a prescribed resource vector $r \\in \\mathbb{R}^T$:\n$$\n\\sum_{i=1}^N u_{i,t} = r_t \\quad \\text{for all } t \\in \\{0,1,\\dots,T-1\\}.\n$$\n\nThe task is to implement a distributed optimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) to solve the condensed MPC quadratic program under these coupling constraints. The algorithm must operate on the condensed control formulation in which states are eliminated using the linear dynamics and the cost is represented as a strictly convex quadratic function of the control sequence for each subsystem.\n\nYou must derive stopping criteria that guarantee, up to a prescribed tolerance, satisfaction of both primal feasibility (agreement between local and auxiliary variables and satisfaction of the coupling constraint) and dual feasibility (stability of multipliers), starting from the Karush-Kuhn-Tucker conditions of convex optimization and the definition of the augmented Lagrangian. The derivation must be grounded in fundamental convexity and linear algebra facts, and must not assume shortcut formulas. Your implementation must use these stopping criteria to terminate iterations: return a boolean indicating whether the criteria were met within a maximum number of iterations.\n\nYour program should:\n- Construct, for each subsystem $i$, the condensed quadratic objective in the form\n$$\n\\frac{1}{2} u_i^\\top H_i u_i + h_i^\\top u_i + c_i,\n$$\nwhere $u_i \\in \\mathbb{R}^T$ is the stacked control sequence, $H_i \\in \\mathbb{R}^{T \\times T}$ is positive definite under $R_i > 0$, $h_i \\in \\mathbb{R}^T$, and $c_i \\in \\mathbb{R}$ is constant not needed for optimization.\n- Implement the ADMM iterations over local variables $u_i$, auxiliary variables $y_i$ (copies used to enforce coupling), and dual variables $\\lambda_i$, with an augmented Lagrangian penalty parameter $\\rho > 0$.\n- At each iteration, perform:\n  1. Local quadratic minimization for each $i$,\n  2. Projection onto the affine coupling constraint,\n  3. Dual variable updates.\n- Evaluate primal residual norms and a dual residual norm, and apply absolute-relative stopping tolerances to decide convergence.\n- Output, for each test case in the suite below, a boolean indicating whether the algorithm satisfied the stopping criteria within the maximum number of iterations.\n\nUse the following test suite, which probes a typical case, tight tolerance with slow convergence, a boundary horizon, and a mixed-sign resource profile. All numbers must be exactly as specified:\n\n- Case $1$ (happy path):\n  - $N = 2$, $T = 8$.\n  - Subsystem $1$: $A_1 = 1.0$, $B_1 = 1.0$, $Q_1 = 1.0$, $R_1 = 0.1$, $P_1 = 2.0$, $x_{1,0} = 0.5$.\n  - Subsystem $2$: $A_2 = 0.9$, $B_2 = 1.1$, $Q_2 = 1.2$, $R_2 = 0.15$, $P_2 = 2.5$, $x_{2,0} = -0.2$.\n  - Resource vector $r = [0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]$.\n  - ADMM parameter $\\rho = 1.0$.\n  - Stopping tolerances $\\epsilon_{\\mathrm{abs}} = 1 \\times 10^{-4}$, $\\epsilon_{\\mathrm{rel}} = 1 \\times 10^{-3}$.\n  - Maximum iterations $K_{\\max} = 1000$.\n- Case $2$ (tight tolerance, slow parameter):\n  - $N = 2$, $T = 12$.\n  - Subsystem $1$: $A_1 = 1.0$, $B_1 = 1.0$, $Q_1 = 1.0$, $R_1 = 0.05$, $P_1 = 1.5$, $x_{1,0} = 1.0$.\n  - Subsystem $2$: $A_2 = 0.95$, $B_2 = 1.0$, $Q_2 = 1.0$, $R_2 = 0.05$, $P_2 = 1.5$, $x_{2,0} = -1.0$.\n  - Resource vector $r = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - ADMM parameter $\\rho = 0.01$.\n  - Stopping tolerances $\\epsilon_{\\mathrm{abs}} = 1 \\times 10^{-9}$, $\\epsilon_{\\mathrm{rel}} = 1 \\times 10^{-9}$.\n  - Maximum iterations $K_{\\max} = 200$.\n- Case $3$ (boundary horizon):\n  - $N = 2$, $T = 1$.\n  - Subsystem $1$: $A_1 = 1.1$, $B_1 = 1.0$, $Q_1 = 1.0$, $R_1 = 0.2$, $P_1 = 0.0$, $x_{1,0} = 0.1$.\n  - Subsystem $2$: $A_2 = 0.8$, $B_2 = 1.0$, $Q_2 = 1.0$, $R_2 = 0.2$, $P_2 = 0.0$, $x_{2,0} = 0.2$.\n  - Resource vector $r = [0.0]$.\n  - ADMM parameter $\\rho = 1.0$.\n  - Stopping tolerances $\\epsilon_{\\mathrm{abs}} = 1 \\times 10^{-6}$, $\\epsilon_{\\mathrm{rel}} = 1 \\times 10^{-6}$.\n  - Maximum iterations $K_{\\max} = 50$.\n- Case $4$ (mixed-sign resource and asymmetry):\n  - $N = 2$, $T = 5$.\n  - Subsystem $1$: $A_1 = 0.95$, $B_1 = 1.0$, $Q_1 = 1.0$, $R_1 = 0.1$, $P_1 = 1.0$, $x_{1,0} = 0.3$.\n  - Subsystem $2$: $A_2 = 1.05$, $B_2 = 0.9$, $Q_2 = 1.0$, $R_2 = 0.12$, $P_2 = 1.0$, $x_{2,0} = -0.3$.\n  - Resource vector $r = [0.2, 0.0, -0.1, 0.1, 0.0]$.\n  - ADMM parameter $\\rho = 0.5$.\n  - Stopping tolerances $\\epsilon_{\\mathrm{abs}} = 1 \\times 10^{-5}$, $\\epsilon_{\\mathrm{rel}} = 1 \\times 10^{-4}$.\n  - Maximum iterations $K_{\\max} = 1000$.\n\nYour program must produce a single line of output containing a comma-separated list of booleans enclosed in square brackets, representing, in order, the convergence success for Cases $1$ through $4$. The exact format is $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. No additional text should be printed.",
            "solution": "The task is to solve a distributed Model Predictive Control (MPC) problem with coupling constraints using the Alternating Direction Method of Multipliers (ADMM). This involves three principal stages: condensing the MPC problem into a quadratic program (QP), formulating and implementing the ADMM algorithm, and deriving and applying appropriate stopping criteria.\n\n**1. Condensing the MPC Problem**\n\nThe first step is to eliminate the state variables $x_{i,t}$ from the cost function $J_i$ to express it as a quadratic function of the control sequence $u_i = [u_{i,0}, u_{i,1}, \\dots, u_{i,T-1}]^\\top \\in \\mathbb{R}^T$.\n\nThe state evolution equation is $x_{i,t+1} = A_i x_{i,t} + B_i u_{i,t}$. By unrolling this recursion, we can express any state $x_{i,k}$ as an affine function of the initial state $x_{i,0}$ and the control sequence up to time $k-1$:\n$$\nx_{i,k} = A_i^k x_{i,0} + \\sum_{j=0}^{k-1} A_i^{k-1-j} B_i u_{i,j} \\quad \\text{for } k \\in \\{1, \\dots, T\\}.\n$$\nWe can represent the sequence of states being penalized, $\\mathbf{x}_{i,\\text{seq}} = [x_{i,0}, x_{i,1}, \\dots, x_{i,T}]^\\top \\in \\mathbb{R}^{T+1}$, in a compact matrix form:\n$$\n\\mathbf{x}_{i,\\text{seq}} = \\mathbf{S}_{i,u} u_i + \\mathbf{s}_{i,x}\n$$\nwhere $\\mathbf{S}_{i,u} \\in \\mathbb{R}^{(T+1) \\times T}$ and $\\mathbf{s}_{i,x} \\in \\mathbb{R}^{T+1}$ are defined element-wise as:\n$$\n(\\mathbf{S}_{i,u})_{k,j} = \\begin{cases} A_i^{k-1-j} B_i & \\text{if } j < k \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\text{for } k \\in \\{0, \\dots, T\\}, j \\in \\{0, \\dots, T-1\\}\n$$\n$$\n(\\mathbf{s}_{i,x})_k = A_i^k x_{i,0} \\quad \\text{for } k \\in \\{0, \\dots, T\\}\n$$\nThe cost function is:\n$$\nJ_i(x_{i,0}, u_i) = \\sum_{t=0}^{T-1} (Q_i x_{i,t}^2 + R_i u_{i,t}^2) + P_i x_{i,T}^2\n$$\nThis can be written in matrix form as:\n$$\nJ_i(u_i) = \\mathbf{x}_{i,\\text{seq}}^\\top \\mathbf{Q}'_i \\mathbf{x}_{i,\\text{seq}} + u_i^\\top \\mathbf{R}'_i u_i\n$$\nwhere $\\mathbf{Q}'_i = \\text{diag}(Q_i, \\dots, Q_i, P_i) \\in \\mathbb{R}^{(T+1)\\times(T+1)}$ and $\\mathbf{R}'_i = \\text{diag}(R_i, \\dots, R_i) \\in \\mathbb{R}^{T\\times T}$.\n\nSubstituting the expression for $\\mathbf{x}_{i,\\text{seq}}$:\n$$\nJ_i(u_i) = (\\mathbf{S}_{i,u} u_i + \\mathbf{s}_{i,x})^\\top \\mathbf{Q}'_i (\\mathbf{S}_{i,u} u_i + \\mathbf{s}_{i,x}) + u_i^\\top \\mathbf{R}'_i u_i\n$$\nExpanding this expression and grouping terms by powers of $u_i$ yields the desired quadratic form $\\frac{1}{2} u_i^\\top H_i u_i + h_i^\\top u_i + c_i$:\n- **Quadratic term**: $u_i^\\top (\\mathbf{S}_{i,u}^\\top \\mathbf{Q}'_i \\mathbf{S}_{i,u} + \\mathbf{R}'_i) u_i$\n- **Linear term**: $2 \\mathbf{s}_{i,x}^\\top \\mathbf{Q}'_i \\mathbf{S}_{i,u} u_i$\n- **Constant term**: $\\mathbf{s}_{i,x}^\\top \\mathbf{Q}'_i \\mathbf{s}_{i,x}$\n\nFrom this, we identify the Hessian matrix $H_i \\in \\mathbb{R}^{T \\times T}$ and the linear coefficient vector $h_i \\in \\mathbb{R}^T$:\n$$\nH_i = 2(\\mathbf{S}_{i,u}^\\top \\mathbf{Q}'_i \\mathbf{S}_{i,u} + \\mathbf{R}'_i)\n$$\n$$\nh_i = 2 \\mathbf{S}_{i,u}^\\top \\mathbf{Q}'_i \\mathbf{s}_{i,x}\n$$\nSince $R_i > 0$, $\\mathbf{R}'_i$ is positive definite. Since $Q_i, P_i \\ge 0$, $\\mathbf{S}_{i,u}^\\top \\mathbf{Q}'_i \\mathbf{S}_{i,u}$ is positive semi-definite. Thus, $H_i$ is positive definite, ensuring the strict convexity of the local objective function.\n\n**2. ADMM Formulation**\nThe full optimization problem is:\n$$\n\\min_{u_1, \\dots, u_N} \\sum_{i=1}^N \\left( \\frac{1}{2} u_i^\\top H_i u_i + h_i^\\top u_i \\right) \\quad \\text{s.t.} \\quad \\sum_{i=1}^N u_i = r\n$$\nwhere $r = [r_0, \\dots, r_{T-1}]^\\top$. To apply ADMM, we introduce auxiliary variables $y_i \\in \\mathbb{R}^T$ and formulate an equivalent consensus problem:\n$$\n\\min_{u_i, y_i} \\sum_{i=1}^N f_i(u_i) + g(y) \\quad \\text{s.t.} \\quad u_i = y_i \\; \\forall i\n$$\nHere, $f_i(u_i) = \\frac{1}{2} u_i^\\top H_i u_i + h_i^\\top u_i$, and $g(y)$ is the indicator function for the affine constraint set $C = \\{(y_1, \\dots, y_N) \\mid \\sum_{i=1}^N y_i = r\\}$.\n\nThe augmented Lagrangian $\\mathcal{L}_\\rho$ for this problem, with dual variables $\\lambda_i$, is:\n$$\n\\mathcal{L}_\\rho(u, y, \\lambda) = \\sum_{i=1}^N \\left( f_i(u_i) + \\lambda_i^\\top(u_i - y_i) + \\frac{\\rho}{2} \\|u_i - y_i\\|_2^2 \\right) + g(y)\n$$\nADMM proceeds by iteratively minimizing $\\mathcal{L}_\\rho$ with respect to $u$ and $y$, followed by a dual variable update. The iterations (at step $k+1$) are:\n\n1.  **$u$-minimization**: For each subsystem $i$, solve for $u_i^{k+1}$ by minimizing $\\mathcal{L}_\\rho$, which is a strictly convex QP:\n    $$\n    u_i^{k+1} = \\arg\\min_{u_i} \\left( \\frac{1}{2} u_i^\\top H_i u_i + h_i^\\top u_i + (\\lambda_i^k)^\\top u_i + \\frac{\\rho}{2} \\|u_i - y_i^k\\|_2^2 \\right)\n    $$\n    Setting the gradient to zero gives the first-order condition $(H_i + \\rho I)u_i^{k+1} + h_i + \\lambda_i^k - \\rho y_i^k = 0$, leading to the update:\n    $$\n    u_i^{k+1} = (H_i + \\rho I)^{-1} (\\rho y_i^k - h_i - \\lambda_i^k)\n    $$\n    The matrix $(H_i + \\rho I)$ is inverted only once at the beginning of the algorithm.\n\n2.  **$y$-minimization**: The update for $y^{k+1} = (y_1^{k+1}, \\dots, y_N^{k+1})$ involves minimizing $\\mathcal{L}_\\rho$ over the set $C$. This is equivalent to finding a Euclidean projection onto $C$:\n    $$\n    y^{k+1} = \\arg\\min_{\\sum y_i = r} \\sum_{i=1}^N \\frac{\\rho}{2} \\left\\| y_i - \\left(u_i^{k+1} + \\frac{1}{\\rho}\\lambda_i^k\\right) \\right\\|_2^2\n    $$\n    The solution is found by averaging. Let $z_i^k = u_i^{k+1} + \\frac{1}{\\rho}\\lambda_i^k$. The update is:\n    $$\n    y_i^{k+1} = z_i^k - \\bar{z}^k + \\frac{1}{N}r, \\quad \\text{where} \\quad \\bar{z}^k = \\frac{1}{N}\\sum_{j=1}^N z_j^k\n    $$\n\n3.  **$\\lambda$-update**: The dual variables are updated using a standard gradient ascent step:\n    $$\n    \\lambda_i^{k+1} = \\lambda_i^k + \\rho(u_i^{k+1} - y_i^{k+1})\n    $$\n\n**3. Derivation of Stopping Criteria**\n\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality are primal feasibility, dual feasibility, and complementary slackness. For our consensus problem, these translate to:\n1.  **Primal Feasibility**: $u_i^* = y_i^*$ for all $i$, and $\\sum_i y_i^* = r$.\n2.  **Dual Feasibility**: The gradients of the Lagrangian with respect to primal variables must vanish.\n    - $\\nabla_{u_i} \\mathcal{L} = \\nabla f_i(u_i^*) + \\lambda_i^* = 0 \\implies H_i u_i^* + h_i + \\lambda_i^* = 0$.\n    - $\\nabla_{y_i} \\mathcal{L}$ relates to the subdifferential of $g$, implying $\\lambda_i^*$ must be equal for all $i$.\n\nThe ADMM algorithm's residuals measure how far the current iterate is from satisfying these conditions.\n\n- **Primal Residual**: Measures the violation of the consensus constraint $u_i = y_i$. The primal residual at iteration $k+1$ is $r_p^{k+1} = u^{k+1} - y^{k+1}$. Its norm is $\\|r_p^{k+1}\\|_2 = \\sqrt{\\sum_i \\|u_i^{k+1} - y_i^{k+1}\\|_2^2}$.\n\n- **Dual Residual**: Measures the violation of the stationarity condition. From the $u$-update, we have $H_i u_i^{k+1} + h_i + \\lambda_i^k + \\rho(u_i^{k+1}-y_i^k)=0$. Using the $\\lambda$-update to substitute for $\\lambda_i^k$, we get $H_i u_i^{k+1} + h_i + \\lambda_i^{k+1} + \\rho(y_i^{k+1}-y_i^k)=0$. The dual feasibility condition requires $H_i u_i^* + h_i + \\lambda_i^* = 0$. The term preventing this at iteration $k+1$ is $\\rho(y_i^{k+1}-y_i^k)$. This leads to the definition of the dual residual $r_d^{k+1} = \\rho(y^{k+1} - y^k)$. Its norm is $\\|r_d^{k+1}\\|_2 = \\rho \\sqrt{\\sum_i \\|y_i^{k+1} - y_i^k\\|_2^2}$.\n\nConvergence is declared when both residual norms are smaller than a threshold. These thresholds, $\\epsilon^{\\text{primal}}$ and $\\epsilon^{\\text{dual}}$, combine absolute and relative tolerances to handle various problem scales:\n$$\n\\|r_p^{k+1}\\|_2 \\le \\epsilon^{\\text{primal}} = \\sqrt{NT} \\epsilon_{\\text{abs}} + \\epsilon_{\\text{rel}} \\max\\{\\|u^{k+1}\\|_2, \\|y^{k+1}\\|_2\\}\n$$\n$$\n\\|r_d^{k+1}\\|_2 \\le \\epsilon^{\\text{dual}} = \\sqrt{NT} \\epsilon_{\\text{abs}} + \\epsilon_{\\text{rel}} \\|\\lambda^{k+1}\\|_2\n$$\nwhere $u, y, \\lambda$ represent the stacked vectors of all subsystems, e.g., $\\|u\\|_2 = \\sqrt{\\sum_i \\|u_i\\|_2^2}$. The term $\\sqrt{NT}$ scales the absolute tolerance with the problem dimension.\n\n```python\nimport numpy as np\n\ndef build_condensed_matrices(T, A, B, Q, R, P, x0):\n    \"\"\"\n    Constructs the matrices H and h for the condensed QP form for one subsystem.\n    \"\"\"\n    # State sequence matrices\n    S_u = np.zeros((T + 1, T))\n    s_x = np.zeros(T + 1)\n    \n    # Populate S_u and s_x based on unrolled dynamics\n    # k is the state index x_k, j is the control index u_j\n    for k in range(T + 1):\n        s_x[k] = (A**k) * x0\n        if k > 0:\n            for j in range(k):\n                # Coefficient of u_j in the expansion of x_k\n                S_u[k, j] = (A**(k - 1 - j)) * B\n\n    # Cost matrices\n    Q_prime = np.diag([Q] * T + [P])\n    R_prime = np.diag([R] * T)\n    \n    # Condensed QP matrices\n    H = 2.0 * (S_u.T @ Q_prime @ S_u + R_prime)\n    h = 2.0 * S_u.T @ Q_prime @ s_x\n    \n    return H, h\n\ndef run_admm_for_case(case_params):\n    \"\"\"\n    Runs the ADMM algorithm for a single test case.\n    \"\"\"\n    N = case_params['N']\n    T = case_params['T']\n    subsystems = case_params['subsystems']\n    r = np.array(case_params['r'], dtype=float)\n    rho = case_params['rho']\n    eps_abs = case_params['eps_abs']\n    eps_rel = case_params['eps_rel']\n    K_max = case_params['K_max']\n\n    # Step 1: Condensation and pre-computation for each subsystem\n    H_list, h_list, P_H_list = [], [], []\n    for params in subsystems:\n        A, B, Q, R, P, x0 = params\n        H, h = build_condensed_matrices(T, float(A), float(B), float(Q), float(R), float(P), float(x0))\n        H_list.append(H)\n        h_list.append(h)\n        # Pre-compute inverse for u-update\n        P_H_list.append(np.linalg.inv(H + rho * np.identity(T)))\n\n    # Step 2: ADMM Initialization\n    u_list = [np.zeros(T) for _ in range(N)]\n    y_list = [np.zeros(T) for _ in range(N)]\n    lambda_list = [np.zeros(T) for _ in range(N)]\n    \n    # Step 3: ADMM Iterations\n    for k in range(K_max):\n        y_list_old = [y.copy() for y in y_list]\n\n        # u-updates (local solves)\n        u_list_new = []\n        for i in range(N):\n            rhs = rho * y_list[i] - h_list[i] - lambda_list[i]\n            u_i_new = P_H_list[i] @ rhs\n            u_list_new.append(u_i_new)\n\n        # y-updates (projection onto consensus set)\n        z_list = [u_list_new[i] + lambda_list[i] / rho for i in range(N)]\n        avg_z = sum(z_list) / N\n        \n        y_list_new = []\n        for i in range(N):\n            y_i_new = z_list[i] - avg_z + r / N\n            y_list_new.append(y_i_new)\n\n        # lambda-updates (dual ascent)\n        lambda_list_new = []\n        for i in range(N):\n            lambda_i_new = lambda_list[i] + rho * (u_list_new[i] - y_list_new[i])\n            lambda_list_new.append(lambda_i_new)\n        \n        # Update state variables for the next iteration\n        u_list, y_list, lambda_list = u_list_new, y_list_new, lambda_list_new\n\n        # Step 4: Check stopping criteria\n        u_stack = np.concatenate(u_list)\n        y_stack = np.concatenate(y_list)\n        lambda_stack = np.concatenate(lambda_list)\n        y_old_stack = np.concatenate(y_list_old)\n\n        # Primal residual norm\n        res_p_norm = np.linalg.norm(u_stack - y_stack)\n        \n        # Dual residual norm\n        res_d_norm = rho * np.linalg.norm(y_stack - y_old_stack)\n\n        # Primal and dual stopping thresholds (tolerances)\n        eps_p = np.sqrt(N * T) * eps_abs + eps_rel * max(np.linalg.norm(u_stack), np.linalg.norm(y_stack))\n        eps_d = np.sqrt(N * T) * eps_abs + eps_rel * np.linalg.norm(lambda_stack)\n\n        if res_p_norm = eps_p and res_d_norm = eps_d:\n            return True\n\n    return False\n\ndef solve():\n    \"\"\"\n    Defines test cases and orchestrates the solution process.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            'N': 2, 'T': 8,\n            'subsystems': [\n                (1.0, 1.0, 1.0, 0.1, 2.0, 0.5),\n                (0.9, 1.1, 1.2, 0.15, 2.5, -0.2)\n            ],\n            'r': [0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05],\n            'rho': 1.0, 'eps_abs': 1e-4, 'eps_rel': 1e-3, 'K_max': 1000\n        },\n        # Case 2 (tight tolerance, slow parameter)\n        {\n            'N': 2, 'T': 12,\n            'subsystems': [\n                (1.0, 1.0, 1.0, 0.05, 1.5, 1.0),\n                (0.95, 1.0, 1.0, 0.05, 1.5, -1.0)\n            ],\n            'r': [0.0] * 12,\n            'rho': 0.01, 'eps_abs': 1e-9, 'eps_rel': 1e-9, 'K_max': 200\n        },\n        # Case 3 (boundary horizon)\n        {\n            'N': 2, 'T': 1,\n            'subsystems': [\n                (1.1, 1.0, 1.0, 0.2, 0.0, 0.1),\n                (0.8, 1.0, 1.0, 0.2, 0.0, 0.2)\n            ],\n            'r': [0.0],\n            'rho': 1.0, 'eps_abs': 1e-6, 'eps_rel': 1e-6, 'K_max': 50\n        },\n        # Case 4 (mixed-sign resource and asymmetry)\n        {\n            'N': 2, 'T': 5,\n            'subsystems': [\n                (0.95, 1.0, 1.0, 0.1, 1.0, 0.3),\n                (1.05, 0.9, 1.0, 0.12, 1.0, -0.3)\n            ],\n            'r': [0.2, 0.0, -0.1, 0.1, 0.0],\n            'rho': 0.5, 'eps_abs': 1e-5, 'eps_rel': 1e-4, 'K_max': 1000\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        converged = run_admm_for_case(case)\n        results.append(converged)\n\n    print(f\"[{','.join(map(str, results))}]\")\n```",
            "answer": "[True,False,True,True]"
        }
    ]
}