{
    "hands_on_practices": [
        {
            "introduction": "Before we can optimize for safety, we must first be able to measure it. This foundational exercise  requires you to compute the expected discounted cumulative safety cost for a given policy within a tabular Constrained Markov Decision Process (CMDP). By working through the Bellman expectation equations, you will gain a concrete understanding of how a policy's long-term safety performance is evaluated, a critical first step in any constrained reinforcement learning problem.",
            "id": "4242700",
            "problem": "A Digital Twin of a microgrid inverter in a Cyber-Physical System (CPS) is used to evaluate the safety performance of a learning-based controller. The safety signal is modeled via a Markov Decision Process (MDP) within Reinforcement Learning (RL). The MDP has a finite state space $S = \\{s_{N}, s_{C}, s_{E}\\}$ representing operating modes: $s_{N}$ (normal), $s_{C}$ (congested), and $s_{E}$ (emergency). The action space is $A = \\{a_{0}, a_{1}\\}$, where $a_0$ is a conservative control (low actuation stress) and $a_1$ is an aggressive control (high actuation stress). The one-step safety cost function $c(s,a)$ encodes hazard exposure (dimensionless) as follows:\n- $c(s_{N}, a_{0}) = \\frac{1}{10}$, $c(s_{N}, a_{1}) = \\frac{3}{10}$,\n- $c(s_{C}, a_{0}) = \\frac{2}{5}$, $c(s_{C}, a_{1}) = \\frac{4}{5}$,\n- $c(s_{E}, a_{0}) = 1$, $c(s_{E}, a_{1}) = \\frac{6}{5}$.\n\nThe controlled transition dynamics $P(s' \\mid s, a)$ are:\n- From $s_N$: with $a_0$, $P(s_{N} \\mid s_{N}, a_{0}) = \\frac{4}{5}$, $P(s_{C} \\mid s_{N}, a_{0}) = \\frac{1}{5}$, $P(s_{E} \\mid s_{N}, a_{0}) = 0$; with $a_1$, $P(s_{N} \\mid s_{N}, a_{1}) = \\frac{1}{2}$, $P(s_{C} \\mid s_{N}, a_{1}) = \\frac{1}{2}$, $P(s_{E} \\mid s_{N}, a_{1}) = 0$.\n- From $s_C$: with $a_0$, $P(s_{N} \\mid s_{C}, a_{0}) = \\frac{1}{2}$, $P(s_{C} \\mid s_{C}, a_{0}) = \\frac{1}{3}$, $P(s_{E} \\mid s_{C}, a_{0}) = \\frac{1}{6}$; with $a_1$, $P(s_{N} \\mid s_{C}, a_{1}) = \\frac{1}{3}$, $P(s_{C} \\mid s_{C}, a_{1}) = \\frac{1}{3}$, $P(s_{E} \\mid s_{C}, a_{1}) = \\frac{1}{3}$.\n- From $s_E$: with $a_0$, $P(s_{N} \\mid s_{E}, a_{0}) = 0$, $P(s_{C} \\mid s_{E}, a_{0}) = \\frac{1}{2}$, $P(s_{E} \\mid s_{E}, a_{0}) = \\frac{1}{2}$; with $a_1$, $P(s_{N} \\mid s_{E}, a_{1}) = 0$, $P(s_{C} \\mid s_{E}, a_{1}) = 0$, $P(s_{E} \\mid s_{E}, a_{1}) = 1$.\n\nThe controller follows a fixed tabular policy $\\pi(a \\mid s)$:\n- $\\pi(a_{0} \\mid s_{N}) = \\frac{3}{4}$, $\\pi(a_{1} \\mid s_{N}) = \\frac{1}{4}$,\n- $\\pi(a_{0} \\mid s_{C}) = \\frac{2}{3}$, $\\pi(a_{1} \\mid s_{C}) = \\frac{1}{3}$,\n- $\\pi(a_{0} \\mid s_{E}) = 1$, $\\pi(a_{1} \\mid s_{E}) = 0$.\n\nLet the discount factor be $\\gamma = \\frac{1}{2}$. Using only foundational definitions of Markov Decision Processes and discounted expected returns in Reinforcement Learning, compute the expected discounted cumulative safety cost $C^{\\pi}(s_{N})$ under policy $\\pi$ starting from state $s_N$. Provide all intermediate steps. Express your final answer as an exact fraction with no units. The safety cost is dimensionless.",
            "solution": "The problem is valid as it is self-contained, mathematically consistent, and well-posed within the standard framework of Markov Decision Processes (MDPs) and Reinforcement Learning (RL). All necessary parameters—states, actions, transition probabilities, cost functions, a policy, and a discount factor—are provided and conform to their definitions. The task is to compute the value function for a given policy, which is a standard policy evaluation problem.\n\nThe expected discounted cumulative safety cost for a policy $\\pi$, denoted as $C^{\\pi}(s)$, represents the total expected cost accumulated from a starting state $s$, with future costs discounted by a factor $\\gamma$. It is defined by the Bellman expectation equation:\n$$C^{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} c(s_t, a_t) \\mid s_0 = s \\right]$$\nThis can be expressed recursively for each state $s \\in S$:\n$$C^{\\pi}(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\left( c(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) C^{\\pi}(s') \\right)$$\nwhere $S = \\{s_{N}, s_{C}, s_{E}\\}$ is the state space, $A = \\{a_0, a_1\\}$ is the action space, $c(s, a)$ is the one-step cost function, $P(s' \\mid s, a)$ is the transition probability, $\\pi(a \\mid s)$ is the policy, and $\\gamma = \\frac{1}{2}$ is the discount factor.\n\nTo simplify the Bellman equations, we first compute the policy-averaged one-step costs, $c^{\\pi}(s)$, and policy-averaged transition probabilities, $P^{\\pi}(s' \\mid s)$.\n\nFor state $s_N$:\n$c^{\\pi}(s_{N}) = \\sum_{a \\in A} \\pi(a \\mid s_{N}) c(s_{N}, a) = \\pi(a_0 \\mid s_{N})c(s_{N}, a_0) + \\pi(a_1 \\mid s_{N})c(s_{N}, a_1) = \\frac{3}{4} \\cdot \\frac{1}{10} + \\frac{1}{4} \\cdot \\frac{3}{10} = \\frac{3}{40} + \\frac{3}{40} = \\frac{6}{40} = \\frac{3}{20}$.\n$P^{\\pi}(s_{N} \\mid s_{N}) = \\sum_{a \\in A} \\pi(a \\mid s_{N}) P(s_{N} \\mid s_{N}, a) = \\frac{3}{4} \\cdot \\frac{4}{5} + \\frac{1}{4} \\cdot \\frac{1}{2} = \\frac{3}{5} + \\frac{1}{8} = \\frac{24+5}{40} = \\frac{29}{40}$.\n$P^{\\pi}(s_{C} \\mid s_{N}) = \\sum_{a \\in A} \\pi(a \\mid s_{N}) P(s_{C} \\mid s_{N}, a) = \\frac{3}{4} \\cdot \\frac{1}{5} + \\frac{1}{4} \\cdot \\frac{1}{2} = \\frac{3}{20} + \\frac{1}{8} = \\frac{6+5}{40} = \\frac{11}{40}$.\n$P^{\\pi}(s_{E} \\mid s_{N}) = \\sum_{a \\in A} \\pi(a \\mid s_{N}) P(s_{E} \\mid s_{N}, a) = \\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 0 = 0$.\n\nFor state $s_C$:\n$c^{\\pi}(s_{C}) = \\sum_{a \\in A} \\pi(a \\mid s_{C}) c(s_{C}, a) = \\frac{2}{3} \\cdot \\frac{2}{5} + \\frac{1}{3} \\cdot \\frac{4}{5} = \\frac{4}{15} + \\frac{4}{15} = \\frac{8}{15}$.\n$P^{\\pi}(s_{N} \\mid s_{C}) = \\sum_{a \\in A} \\pi(a \\mid s_{C}) P(s_{N} \\mid s_{C}, a) = \\frac{2}{3} \\cdot \\frac{1}{2} + \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{1}{3} + \\frac{1}{9} = \\frac{4}{9}$.\n$P^{\\pi}(s_{C} \\mid s_{C}) = \\sum_{a \\in A} \\pi(a \\mid s_{C}) P(s_{C} \\mid s_{C}, a) = \\frac{2}{3} \\cdot \\frac{1}{3} + \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{2}{9} + \\frac{1}{9} = \\frac{3}{9} = \\frac{1}{3}$.\n$P^{\\pi}(s_{E} \\mid s_{C}) = \\sum_{a \\in A} \\pi(a \\mid s_{C}) P(s_{E} \\mid s_{C}, a) = \\frac{2}{3} \\cdot \\frac{1}{6} + \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{1}{9} + \\frac{1}{9} = \\frac{2}{9}$.\n\nFor state $s_E$:\nThe policy is deterministic: $\\pi(a_0 \\mid s_E)=1$, $\\pi(a_1 \\mid s_E)=0$.\n$c^{\\pi}(s_{E}) = 1 \\cdot c(s_{E}, a_0) = 1$.\n$P^{\\pi}(s_{N} \\mid s_{E}) = 1 \\cdot P(s_{N} \\mid s_{E}, a_0) = 0$.\n$P^{\\pi}(s_{C} \\mid s_{E}) = 1 \\cdot P(s_{C} \\mid s_{E}, a_0) = \\frac{1}{2}$.\n$P^{\\pi}(s_{E} \\mid s_{E}) = 1 \\cdot P(s_{E} \\mid s_{E}, a_0) = \\frac{1}{2}$.\n\nNow, the Bellman expectation equation for the policy $\\pi$ simplifies to:\n$$C^{\\pi}(s) = c^{\\pi}(s) + \\gamma \\sum_{s' \\in S} P^{\\pi}(s' \\mid s) C^{\\pi}(s')$$\nLet $C_N = C^{\\pi}(s_N)$, $C_C = C^{\\pi}(s_C)$, and $C_E = C^{\\pi}(s_E)$. This defines a system of three linear equations:\n\n1. For $s_N$:\n$C_N = c^{\\pi}(s_N) + \\gamma (P^{\\pi}(s_N \\mid s_N) C_N + P^{\\pi}(s_C \\mid s_N) C_C)$\n$C_N = \\frac{3}{20} + \\frac{1}{2} \\left( \\frac{29}{40} C_N + \\frac{11}{40} C_C \\right)$\n$C_N = \\frac{3}{20} + \\frac{29}{80} C_N + \\frac{11}{80} C_C$\n$(1 - \\frac{29}{80}) C_N - \\frac{11}{80} C_C = \\frac{3}{20}$\n$\\frac{51}{80} C_N - \\frac{11}{80} C_C = \\frac{12}{80} \\implies 51 C_N - 11 C_C = 12$ (Eq. I)\n\n2. For $s_C$:\n$C_C = c^{\\pi}(s_C) + \\gamma (P^{\\pi}(s_N \\mid s_C) C_N + P^{\\pi}(s_C \\mid s_C) C_C + P^{\\pi}(s_E \\mid s_C) C_E)$\n$C_C = \\frac{8}{15} + \\frac{1}{2} \\left( \\frac{4}{9} C_N + \\frac{1}{3} C_C + \\frac{2}{9} C_E \\right)$\n$C_C = \\frac{8}{15} + \\frac{2}{9} C_N + \\frac{1}{6} C_C + \\frac{1}{9} C_E$\n$(1 - \\frac{1}{6}) C_C - \\frac{2}{9} C_N - \\frac{1}{9} C_E = \\frac{8}{15}$\n$\\frac{5}{6} C_C - \\frac{2}{9} C_N - \\frac{1}{9} C_E = \\frac{8}{15}$\nMultiplying by the least common multiple of the denominators ($90$):\n$75 C_C - 20 C_N - 10 C_E = 48 \\implies -20 C_N + 75 C_C - 10 C_E = 48$ (Eq. II)\n\n3. For $s_E$:\n$C_E = c^{\\pi}(s_E) + \\gamma (P^{\\pi}(s_C \\mid s_E) C_C + P^{\\pi}(s_E \\mid s_E) C_E)$\n$C_E = 1 + \\frac{1}{2} \\left( \\frac{1}{2} C_C + \\frac{1}{2} C_E \\right)$\n$C_E = 1 + \\frac{1}{4} C_C + \\frac{1}{4} C_E$\n$(1 - \\frac{1}{4}) C_E - \\frac{1}{4} C_C = 1$\n$\\frac{3}{4} C_E - \\frac{1}{4} C_C = 1 \\implies 3 C_E - C_C = 4$ (Eq. III)\n\nWe now solve this system of equations. From Eq. III, we express $C_C$ in terms of $C_E$:\n$C_C = 3 C_E - 4$\n\nSubstitute this expression for $C_C$ into Eq. I:\n$51 C_N - 11 (3 C_E - 4) = 12$\n$51 C_N - 33 C_E + 44 = 12$\n$51 C_N - 33 C_E = -32$ (Eq. IV)\n\nSubstitute the expression for $C_C$ into Eq. II:\n$-20 C_N + 75 (3 C_E - 4) - 10 C_E = 48$\n$-20 C_N + 225 C_E - 300 - 10 C_E = 48$\n$-20 C_N + 215 C_E = 348$ (Eq. V)\n\nNow we have a system of two equations (IV and V) with two variables, $C_N$ and $C_E$.\nFrom Eq. IV, we can express $C_E$ in terms of $C_N$:\n$33 C_E = 51 C_N + 32 \\implies C_E = \\frac{51 C_N + 32}{33}$\n\nSubstitute this into Eq. V:\n$-20 C_N + 215 \\left( \\frac{51 C_N + 32}{33} \\right) = 348$\nMultiply the entire equation by $33$:\n$-20 \\cdot 33 \\cdot C_N + 215 (51 C_N + 32) = 348 \\cdot 33$\n$-660 C_N + (215 \\cdot 51) C_N + (215 \\cdot 32) = 11484$\n$-660 C_N + 10965 C_N + 6880 = 11484$\n$(10965 - 660) C_N = 11484 - 6880$\n$10305 C_N = 4604$\n$C_N = \\frac{4604}{10305}$\n\nThe problem asks for $C^{\\pi}(s_{N})$, which is $C_N$. The fraction $\\frac{4604}{10305}$ is irreducible because the prime factorization of the numerator is $4604 = 2^2 \\cdot 1151$ and the denominator is $10305 = 3^2 \\cdot 5 \\cdot 229$, and there are no common prime factors.",
            "answer": "$$\\boxed{\\frac{4604}{10305}}$$"
        },
        {
            "introduction": "Moving from policy evaluation to policy optimization, this practice  introduces a cornerstone technique for solving CMDPs: Lagrangian duality. You will tackle the dual problem, where the goal is to find the optimal Lagrange multiplier $\\lambda^{\\star}$ that correctly prices the safety constraint. By implementing a numerical line search to solve $\\min_{\\lambda \\ge 0} g(\\lambda)$, you will develop a practical intuition for how reward and safety are balanced to derive an optimal constrained policy.",
            "id": "4242687",
            "problem": "Consider a single-step Constrained Markov Decision Process (CMDP) in the context of digital twins and cyber-physical systems, where a controller must choose one action from a finite set to maximize expected reward while satisfying a safety constraint on an expected safety cost. The CMDP has a single state and a finite action set $\\mathcal{A} = \\{0,1,\\dots,n-1\\}$. Each action $a \\in \\mathcal{A}$ deterministically terminates the episode, yielding an immediate reward $r_a \\in \\mathbb{R}$ and an immediate safety cost $c_a \\in \\mathbb{R}_{\\ge 0}$. The bound on the safety cost is a scalar $d \\in \\mathbb{R}_{\\ge 0}$.\n\nThe primal problem is to maximize the expected reward subject to the expected safety cost being at most $d$. Using the Lagrangian relaxation for a maximization CMDP with one inequality constraint, the Lagrangian is\n$$\n\\mathcal{L}(\\pi,\\lambda) = \\mathbb{E}_{\\pi}[r] - \\lambda\\left(\\mathbb{E}_{\\pi}[c] - d\\right),\n$$\nwhere $\\pi$ is a randomized policy (a probability distribution over actions in $\\mathcal{A}$) and $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is the Lagrange multiplier. The dual function for the maximization problem is defined as\n$$\ng(\\lambda) = \\sup_{\\pi}\\, \\mathcal{L}(\\pi,\\lambda) = \\max_{a \\in \\mathcal{A}}\\left(r_a - \\lambda c_a\\right) + \\lambda d,\n$$\nwhich is the pointwise supremum over affine functions of $\\lambda$, hence $g(\\lambda)$ is convex on $\\lambda \\in \\mathbb{R}_{\\ge 0}$. The dual problem is\n$$\n\\min_{\\lambda \\ge 0} g(\\lambda).\n$$\nThe minimizing $\\lambda^{\\star}$ is an optimal multiplier. The active set at $\\lambda^{\\star}$ is defined as\n$$\n\\mathcal{A}^{\\star}(\\lambda^{\\star}) = \\left\\{a \\in \\mathcal{A}: r_a - \\lambda^{\\star} c_a = \\max_{b \\in \\mathcal{A}}\\left(r_b - \\lambda^{\\star} c_b\\right)\\right\\},\n$$\nnamely, the set of actions that are simultaneously optimal in the inner maximization of the dual function at the minimizing multiplier. The safety constraint is considered active if and only if $\\lambda^{\\star}  0$ and the bound $d$ lies within the convex hull of the safety costs of the active actions, that is, if $d \\in [\\min_{a \\in \\mathcal{A}^{\\star}} c_a,\\, \\max_{a \\in \\mathcal{A}^{\\star}} c_a]$.\n\nYour task is to write a complete, runnable program that:\n- Implements a one-dimensional convex line search to solve $\\min_{\\lambda \\ge 0} g(\\lambda)$ for each provided test case using a bracketing method suitable for convex functions (for example, golden-section search). The line search must operate over a user-specified closed interval $[0, \\lambda_{\\max}]$, treat $g(\\lambda)$ as a black-box function, and return a numerical approximation $\\lambda^{\\star}$.\n- Computes the active set $\\mathcal{A}^{\\star}(\\lambda^{\\star})$ for each test case.\n- Determines whether the safety constraint is active (boolean) under the above definition.\n\nYou must not assume any closed-form “shortcut” for $\\lambda^{\\star}$; you must treat $g(\\lambda)$ as a convex objective accessible only through function evaluations. All mathematical reasoning must start from core CMDP and Lagrangian definitions given above.\n\nUse the following test suite. Each test case is specified by the tuple $(\\mathbf{r}, \\mathbf{c}, d, \\lambda_{\\max})$, where $\\mathbf{r} = [r_0, r_1, \\dots, r_{n-1}]$ and $\\mathbf{c} = [c_0, c_1, \\dots, c_{n-1}]$.\n\n- Test Case $1$ (happy path with interior minimizer at a nondifferentiable point):\n  - $\\mathbf{r} = [5, 3, 4]$\n  - $\\mathbf{c} = [4, 1, 2]$\n  - $d = 2.5$\n  - $\\lambda_{\\max} = 3.0$\n\n- Test Case $2$ (boundary case with inactive constraint at the lower bound):\n  - $\\mathbf{r} = [5, 3, 4]$\n  - $\\mathbf{c} = [4, 1, 2]$\n  - $d = 5.0$\n  - $\\lambda_{\\max} = 3.0$\n\n- Test Case $3$ (edge case with infeasible bound, driving the minimizer to the upper bound):\n  - $\\mathbf{r} = [5, 3, 4]$\n  - $\\mathbf{c} = [4, 1, 2]$\n  - $d = 0.2$\n  - $\\lambda_{\\max} = 3.0$\n\nOutput specification:\n- For each test case, return a list with three elements:\n  - The minimizing multiplier $\\lambda^{\\star}$ (as a float).\n  - A boolean indicating whether the safety constraint is active under the above definition.\n  - The active set $\\mathcal{A}^{\\star}(\\lambda^{\\star})$ represented as a list of action indices.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if there are three test cases, the output must be of the form $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$, where each $\\text{result}_i$ is itself a list structured as described above. No extra text or spaces are permitted.",
            "solution": "The problem has been validated and is determined to be sound, well-posed, and scientifically grounded in the principles of constrained optimization and reinforcement learning.\n\nThe objective is to analyze a single-step Constrained Markov Decision Process (CMDP) by solving its Lagrangian dual problem. We are given a set of actions $\\mathcal{A} = \\{0, 1, \\dots, n-1\\}$, with corresponding rewards $\\mathbf{r} = [r_0, \\dots, r_{n-1}]$ and safety costs $\\mathbf{c} = [c_0, \\dots, c_{n-1}]$. The goal is to find a policy $\\pi$ that maximizes expected reward $\\mathbb{E}_{\\pi}[r]$ subject to the constraint that the expected safety cost $\\mathbb{E}_{\\pi}[c]$ does not exceed a given bound $d$.\n\nThe Lagrangian for this constrained maximization problem is given by:\n$$\n\\mathcal{L}(\\pi, \\lambda) = \\mathbb{E}_{\\pi}[r] - \\lambda(\\mathbb{E}_{\\pi}[c] - d)\n$$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier associated with the safety constraint. Since the policy $\\pi$ is a probability distribution over the discrete action set $\\mathcal{A}$, the expectations are $\\mathbb{E}_{\\pi}[r] = \\sum_{a \\in \\mathcal{A}} \\pi(a)r_a$ and $\\mathbb{E}_{\\pi}[c] = \\sum_{a \\in \\mathcal{A}} \\pi(a)c_a$. The Lagrangian can be rewritten as:\n$$\n\\mathcal{L}(\\pi, \\lambda) = \\sum_{a \\in \\mathcal{A}} \\pi(a)r_a - \\lambda\\left(\\sum_{a \\in \\mathcal{A}} \\pi(a)c_a - d\\right) = \\sum_{a \\in \\mathcal{A}} \\pi(a)(r_a - \\lambda c_a) + \\lambda d\n$$\n\nThe dual function, $g(\\lambda)$, is defined as the supremum of the Lagrangian over all policies $\\pi$:\n$$\ng(\\lambda) = \\sup_{\\pi} \\mathcal{L}(\\pi, \\lambda) = \\sup_{\\pi} \\left[ \\sum_{a \\in \\mathcal{A}} \\pi(a)(r_a - \\lambda c_a) + \\lambda d \\right]\n$$\nTo maximize this expression, a deterministic policy should place all probability mass ($\\pi(a) = 1$) on the action $a$ that maximizes the term $r_a - \\lambda c_a$. Therefore, the supremum is achieved by a greedy choice:\n$$\ng(\\lambda) = \\max_{a \\in \\mathcal{A}} (r_a - \\lambda c_a) + \\lambda d\n$$\nEach term $r_a - \\lambda c_a$ is an affine (linear) function of $\\lambda$. The function $g(\\lambda)$ is the pointwise maximum of a set of affine functions, which guarantees that $g(\\lambda)$ is a convex function for $\\lambda \\in \\mathbb{R}_{\\ge 0}$.\n\nThe dual problem is to find the value of $\\lambda$ that minimizes the dual function:\n$$\n\\min_{\\lambda \\ge 0} g(\\lambda)\n$$\nWe are tasked to solve this minimization problem over a specified bounded interval $[0, \\lambda_{\\max}]$. The convexity of $g(\\lambda)$ makes it amenable to one-dimensional convex optimization algorithms like the golden-section search. This method does not require gradient information and treats $g(\\lambda)$ as a \"black box,\" making it ideal for this problem where $g(\\lambda)$ is non-differentiable at points where the maximizing action changes.\n\nThe golden-section search algorithm iteratively reduces a bracketing interval $[a, b]$ known to contain the minimum. It maintains this bracket by evaluating the function at two internal points, $x_1$ and $x_2$, chosen according to the golden ratio, $\\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618$. Let the interval be $[a,b]$. The points are $x_1 = b - (b-a)/\\phi$ and $x_2 = a + (b-a)/\\phi$. If $g(x_1)  g(x_2)$, the new interval becomes $[a, x_2]$; otherwise, it becomes $[x_1, b]$. This process is repeated for a fixed number of iterations or until the interval width is sufficiently small, yielding a numerical approximation of the optimal multiplier, $\\lambda^{\\star}$.\n\nOnce $\\lambda^{\\star}$ is found, we must identify the active set $\\mathcal{A}^{\\star}(\\lambda^{\\star})$, which consists of all actions that are optimal for the inner maximization problem in $g(\\lambda^{\\star})$:\n$$\n\\mathcal{A}^{\\star}(\\lambda^{\\star}) = \\left\\{ a \\in \\mathcal{A} : r_a - \\lambda^{\\star} c_a = \\max_{b \\in \\mathcal{A}}(r_b - \\lambda^{\\star} c_b) \\right\\}\n$$\nNumerically, this involves calculating $v_b = r_b - \\lambda^{\\star} c_b$ for all actions $b$, finding the maximum value $v_{\\max} = \\max_b v_b$, and collecting all actions $a$ for which $v_a$ is numerically close to $v_{\\max}$ (i.e., $|v_{\\max} - v_a|  \\epsilon$ for a small tolerance $\\epsilon  0$).\n\nFinally, we determine if the safety constraint is active. According to the problem's definition, this requires satisfying two conditions:\n1.  The optimal multiplier must be strictly positive: $\\lambda^{\\star}  0$. From duality theory, $\\lambda^{\\star}=0$ implies the constraint is inactive, as the unconstrained solution is already feasible.\n2.  The constraint bound $d$ must lie within the convex hull of the costs of the actions in the active set $\\mathcal{A}^{\\star}(\\lambda^{\\star})$. For a set of scalar costs, this convex hull is simply the closed interval from the minimum to the maximum cost: $d \\in [\\min_{a \\in \\mathcal{A}^{\\star}(\\lambda^{\\star})} c_a, \\max_{a \\in \\mathcal{A}^{\\star}(\\lambda^{\\star})} c_a]$.\nIf both conditions are met, the constraint is considered active. This means that achieving the optimal reward requires a policy that might \"mix\" between actions at the boundary of the feasible region, precisely balancing on the cost constraint $d$.\n\nThe overall procedure for each test case $(\\mathbf{r}, \\mathbf{c}, d, \\lambda_{\\max})$ is:\n1.  Define a function for $g(\\lambda)$ based on the provided $\\mathbf{r}$, $\\mathbf{c}$, and $d$.\n2.  Apply the golden-section search to this function over the interval $[0, \\lambda_{\\max}]$ to find an approximation of $\\lambda^{\\star}$.\n3.  Calculate the active set $\\mathcal{A}^{\\star}(\\lambda^{\\star})$ using the obtained $\\lambda^{\\star}$.\n4.  Evaluate the two conditions for constraint activity to obtain a boolean result.\n5.  Package the results $(\\lambda^{\\star}, \\text{is_active}, \\mathcal{A}^{\\star})$ for the test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CMDP dual problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (happy path with interior minimizer)\n        (np.array([5, 3, 4], dtype=float), np.array([4, 1, 2], dtype=float), 2.5, 3.0),\n        # Test Case 2 (boundary case with inactive constraint)\n        (np.array([5, 3, 4], dtype=float), np.array([4, 1, 2], dtype=float), 5.0, 3.0),\n        # Test Case 3 (edge case with infeasible bound)\n        (np.array([5, 3, 4], dtype=float), np.array([4, 1, 2], dtype=float), 0.2, 3.0),\n    ]\n\n    all_results = []\n    \n    TOLERANCE = 1e-9\n    NUM_ITERATIONS = 100\n    GOLDEN_RATIO = (1 + np.sqrt(5)) / 2\n\n    def g_factory(r, c, d):\n        \"\"\"Creates the dual function g(lambda) for a given problem instance.\"\"\"\n        def g(lambda_val):\n            # Calculate r_a - lambda * c_a for all actions\n            q_values = r - lambda_val * c\n            # Return max(r_a - lambda * c_a) + lambda * d\n            return np.max(q_values) + lambda_val * d\n        return g\n\n    def golden_section_search(func, a, b, n_iter):\n        \"\"\"\n        Minimizes a 1D convex function `func` on the interval [a, b]\n        using golden-section search for `n_iter` iterations.\n        \"\"\"\n        c = b - (b - a) / GOLDEN_RATIO\n        d = a + (b - a) / GOLDEN_RATIO\n        fc = func(c)\n        fd = func(d)\n\n        for _ in range(n_iter):\n            if fc  fd:\n                b = d\n                d = c\n                fd = fc\n                c = b - (b - a) / GOLDEN_RATIO\n                fc = func(c)\n            else:\n                a = c\n                c = d\n                fc = fd\n                d = a + (b - a) / GOLDEN_RATIO\n                fd = func(d)\n        \n        return (a + b) / 2\n\n    for r_vec, c_vec, d_val, lambda_max_val in test_cases:\n        # Step 1: Create the dual function g(lambda)\n        g_func = g_factory(r_vec, c_vec, d_val)\n\n        # Step 2: Find lambda_star using golden-section search\n        lambda_star = golden_section_search(g_func, 0.0, lambda_max_val, NUM_ITERATIONS)\n\n        # Step 3: Compute the active set A_star(lambda_star)\n        q_vals = r_vec - lambda_star * c_vec\n        max_q_val = np.max(q_vals)\n        active_set = [i for i, q in enumerate(q_vals) if abs(q - max_q_val)  TOLERANCE]\n\n        # Step 4: Determine if the safety constraint is active\n        is_active = False\n        if lambda_star > TOLERANCE:\n            if active_set:\n                active_costs = c_vec[active_set]\n                min_cost = np.min(active_costs)\n                max_cost = np.max(active_costs)\n                if min_cost - TOLERANCE = d_val = max_cost + TOLERANCE:\n                    is_active = True\n        \n        all_results.append([lambda_star, is_active, active_set])\n\n    # Final print statement in the exact required format.\n    def format_result(res):\n        lambda_val, active_bool, active_list = res\n        # Format to avoid spaces from default str() representation\n        active_list_str = f\"[{','.join(map(str, active_list))}]\"\n        return f\"[{lambda_val},{str(active_bool)},{active_list_str}]\"\n    \n    formatted_strings = [format_result(res) for res in all_results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While offline optimization provides a safe policy, online enforcement ensures safety even when the nominal policy is uncertain or exploratory. This exercise  focuses on implementing a real-time safety filter, a common component in safety-critical cyber-physical systems. You will develop a module that projects a potentially unsafe nominal action onto a state-dependent safe set by solving a quadratic program, providing hands-on experience with creating minimally invasive safety shields.",
            "id": "4242673",
            "problem": "You are building a safety filter for actions generated by Reinforcement Learning (RL) in a Digital Twin (DT) of a Cyber-Physical System (CPS). At each time, given a current state $x \\in \\mathbb{R}^n$ and a nominal action $u_0 \\in \\mathbb{R}^p$, the safety filter must compute the minimally invasive correction by projecting $u_0$ onto the feasible set prescribed by the DT. The feasible set is defined by state-dependent linear safety constraints of the form $c(x,u) \\le 0$, where $c(x,u) = G u - (H x + d)$ with $G \\in \\mathbb{R}^{m \\times p}$, $H \\in \\mathbb{R}^{m \\times n}$, and $d \\in \\mathbb{R}^m$. The minimally invasive correction is the solution to the strictly convex Quadratic Program (QP) that minimizes the Euclidean distance to $u_0$ subject to the constraints. The projection problem is expressed as the optimization problem\n$$\n\\min_{u \\in \\mathbb{R}^p} \\ \\|u - u_0\\|_2^2 \\quad \\text{subject to} \\quad G u \\le H x + d.\n$$\nThe dimension $p$ is $p = 2$ and the number of constraints is $m = 5$. The state dimension is $n = 2$. Use the fixed matrices\n$$\nG = \\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n-1  0 \\\\\n0  -1 \\\\\n1  1\n\\end{bmatrix}, \\quad\nH = \\begin{bmatrix}\n-0.5  0.0 \\\\\n0.0  0.0 \\\\\n0.0  0.0 \\\\\n0.0  0.0 \\\\\n-0.2  0.1\n\\end{bmatrix}, \\quad\nd = \\begin{bmatrix}\n1.0 \\\\\n1.0 \\\\\n1.0 \\\\\n1.0 \\\\\n1.2\n\\end{bmatrix}.\n$$\nFor each test case in the suite below, compute the projected action $u^\\star(x,u_0)$ that solves the above optimization problem. All quantities are dimensionless and must be treated in $\\mathbb{R}$.\n\nTest suite (each case is a pair $(x,u_0)$):\n- Case $1$: $x = [0.0, 0.0]$, $u_0 = [0.5, 0.25]$.\n- Case $2$: $x = [0.0, 0.0]$, $u_0 = [2.0, 0.0]$.\n- Case $3$: $x = [0.0, 0.0]$, $u_0 = [2.0, -2.0]$.\n- Case $4$: $x = [0.0, -7.0]$, $u_0 = [0.8, 0.4]$.\n- Case $5$: $x = [0.0, 0.0]$, $u_0 = [1.0, 0.2]$.\n- Case $6$: $x = [2.0, 0.0]$, $u_0 = [0.8, 0.9]$.\n\nYour program must:\n- Implement a mathematically sound algorithm to compute $u^\\star(x,u_0)$ for each test case, using only the provided fixed $G$, $H$, and $d$, with $h(x) = H x + d$.\n- Ensure numerical stability and theoretical correctness for all cases, including borderline feasibility (on-constraint) and multi-constraint activation scenarios.\n- Produce the final output as a single line containing the results aggregated into a list of lists. Each inner list must be the projected action vector $u^\\star$ for the corresponding test case in the same order as above. The exact output format must be a single line:\n$$\n[\\,[u^\\star_1[0],u^\\star_1[1]],[u^\\star_2[0],u^\\star_2[1]],\\dots,[u^\\star_6[0],u^\\star_6[1]]\\,]\n$$\nNo additional text should be printed.",
            "solution": "We derive the minimally invasive projection algorithm from first principles. For a given nominal action $u_0 \\in \\mathbb{R}^p$ and state $x \\in \\mathbb{R}^n$, the safety set is defined as the intersection of $m$ linear halfspaces\n$$\n\\mathcal{C}(x) = \\{u \\in \\mathbb{R}^p \\mid G u \\le H x + d\\}.\n$$\nThe safety filter seeks the Euclidean projection of $u_0$ onto $\\mathcal{C}(x)$, which is the unique solution to the strictly convex Quadratic Program (QP)\n$$\n\\min_{u \\in \\mathbb{R}^p} \\ \\|u - u_0\\|_2^2 \\ \\text{ subject to } \\ G u \\le H x + d.\n$$\nThis formulation follows from the definition of Euclidean projection onto a nonempty closed convex set: for any closed convex $\\mathcal{C}(x)$, the point $u^\\star \\in \\mathcal{C}(x)$ minimizing $\\|u - u_0\\|_2^2$ exists and is unique due to strict convexity of the objective and convexity of $\\mathcal{C}(x)$.\n\nKarush-Kuhn-Tucker (KKT) optimality conditions provide a principled characterization of the solution. Introduce Lagrange multipliers $\\lambda \\in \\mathbb{R}^m$ for the inequality constraints $G u \\le h(x)$, where $h(x) = H x + d$. The Lagrangian is\n$$\n\\mathcal{L}(u,\\lambda) = \\tfrac{1}{2}\\|u - u_0\\|_2^2 + \\lambda^\\top(G u - h(x)),\n$$\nwith $\\lambda \\ge 0$. The KKT conditions are:\n1. Stationarity: $\\nabla_u \\mathcal{L}(u^\\star,\\lambda^\\star) = 0 \\Rightarrow u^\\star - u_0 + G^\\top \\lambda^\\star = 0$, hence $u^\\star = u_0 - G^\\top \\lambda^\\star$.\n2. Primal feasibility: $G u^\\star \\le h(x)$.\n3. Dual feasibility: $\\lambda^\\star \\ge 0$.\n4. Complementary slackness: $\\lambda_i^\\star \\cdot \\left(G_i u^\\star - h_i(x)\\right) = 0$ for all $i \\in \\{1,\\dots,m\\}$.\n\nThese conditions establish that the optimal projected point lies along an affine displacement from $u_0$ in directions spanned by constraint normals, with only active constraints contributing nonzero multipliers. While one can solve for the active set and multipliers directly via KKT when the active set is known, a robust algorithm that does not require prior knowledge of the active set is preferable.\n\nWe design the algorithm via projections onto halfspaces using Dykstra’s algorithm, a principled method for Euclidean projection onto intersections of closed convex sets. Each constraint defines a closed convex halfspace\n$$\n\\mathcal{H}_i(x) = \\{u \\in \\mathbb{R}^p \\mid a_i^\\top u \\le b_i(x)\\},\n$$\nwhere $a_i^\\top$ is the $i$-th row of $G$ and $b_i(x) = h_i(x)$. The Euclidean projection of any $y \\in \\mathbb{R}^p$ onto a single halfspace $\\mathcal{H}_i(x)$ is given by\n$$\n\\operatorname{proj}_{\\mathcal{H}_i(x)}(y) =\n\\begin{cases}\ny,  \\text{if } a_i^\\top y \\le b_i(x), \\\\\ny - \\dfrac{a_i^\\top y - b_i(x)}{\\|a_i\\|_2^2} \\, a_i,  \\text{if } a_i^\\top y  b_i(x).\n\\end{cases}\n$$\nDykstra’s algorithm constructs the projection onto $\\mathcal{C}(x) = \\bigcap_{i=1}^m \\mathcal{H}_i(x)$ by iteratively cycling through projections onto each halfspace with correction terms that ensure convergence to the true Euclidean projection (not just a point feasible in the intersection). Specifically, initialize $z^{(0)} = u_0$ and correction vectors $p_i^{(0)} = 0$ for all $i$. For $k = 0,1,2,\\dots$, perform:\n- For $i = 1,2,\\dots,m$ sequentially:\n  - Set $y = z^{(k)} + p_i^{(k)}$.\n  - Compute $z_i = \\operatorname{proj}_{\\mathcal{H}_i(x)}(y)$.\n  - Update $p_i^{(k+1)} = y - z_i$ and set $z^{(k+1)} = z_i$.\nUnder standard conditions (closed convex sets and nonempty intersection), $z^{(k)}$ converges to the Euclidean projection $u^\\star$ of $u_0$ onto $\\mathcal{C}(x)$. This algorithm is grounded in convex analysis and monotone operator theory, and it avoids fragile active-set enumeration.\n\nWe now apply this to the given matrices. The constraints are:\n- Row $1$: $[1,0] \\cdot u \\le 1.0 - 0.5 x_1$.\n- Row $2$: $[0,1] \\cdot u \\le 1.0$.\n- Row $3$: $[-1,0] \\cdot u \\le 1.0$ (equivalently $u_1 \\ge -1.0$).\n- Row $4$: $[0,-1] \\cdot u \\le 1.0$ (equivalently $u_2 \\ge -1.0$).\n- Row $5$: $[1,1] \\cdot u \\le 1.2 - 0.2 x_1 + 0.1 x_2$.\n\nFor each case, compute $h(x) = H x + d$, then project $u_0$ onto the intersection of the corresponding halfspaces via Dykstra’s algorithm to obtain $u^\\star(x,u_0)$.\n\nAnalytical verification for selected cases:\n- Case $1$ with $x = [0.0,0.0]$ and $u_0 = [0.5,0.25]$: All constraints are satisfied strictly ($[1,1] \\cdot u_0 = 0.75 \\le 1.2$), hence $u^\\star = u_0$ by the characterization of projection onto convex sets (if $u_0 \\in \\mathcal{C}(x)$, its projection is itself).\n- Case $2$ with $x = [0.0,0.0]$ and $u_0 = [2.0,0.0]$: Violates $u_1 \\le 1.0$ and $u_1 + u_2 \\le 1.2$; the closest feasible point is $[1.0,0.0]$ which satisfies all constraints and minimizes squared distance $(2.0-1.0)^2 + (0.0-0.0)^2 = 1.0$ (any change in $u_2$ would increase distance).\n- Case $3$ with $x = [0.0,0.0]$ and $u_0 = [2.0,-2.0]$: Violates $u_1 \\le 1.0$ and $u_2 \\ge -1.0$; the nearest corner consistent with other constraints is $[1.0,-1.0]$, with squared distance $(2.0-1.0)^2 + (-2.0+1.0)^2 = 2.0$.\n- Case $4$ with $x = [0.0,-7.0]$ gives $h_5(x) = 1.2 + 0.1(-7.0) = 0.5$ and the other $h_i(x)$ unchanged; $u_0 = [0.8,0.4]$ violates only the sum constraint $u_1 + u_2 \\le 0.5$. The projection onto this halfspace adjusts along the normal $[1,1]^\\top$: $u^\\star = u_0 - \\frac{(1,1)\\cdot u_0 - 0.5}{\\|[1,1]\\|_2^2} [1,1] = [0.8,0.4] - \\frac{1.2 - 0.5}{2} [1,1] = [0.45,0.05]$.\n- Case $5$ with $x = [0.0,0.0]$ and $u_0 = [1.0,0.2]$: Lies exactly on constraints $u_1 \\le 1.0$ and $u_1 + u_2 \\le 1.2$, thus $u^\\star = u_0$.\n- Case $6$ with $x = [2.0,0.0]$ gives $h_1(x) = 1.0 - 0.5(2.0) = 0.0$ and $h_5(x) = 1.2 - 0.2(2.0) = 0.8$; with $u_0 = [0.8,0.9]$, both $u_1 \\le 0.0$ and $u_1 + u_2 \\le 0.8$ are violated. The KKT active-set solution with both constraints active yields $u^\\star = u_0 - [1,0]^\\top \\lambda_1 - [1,1]^\\top \\lambda_5$ and the equalities $u_1^\\star = 0.0$, $u_1^\\star + u_2^\\star = 0.8$. Solving gives $\\lambda_5 = 0.1$, $\\lambda_1 = 0.7$, and $u^\\star = [0.0,0.8]$.\n\nTherefore, the algorithm based on Dykstra’s method yields the correct Euclidean projections consistent with KKT analysis. The final program computes $u^\\star(x,u_0)$ for all test cases and prints them as a single list of lists on one line, in the exact order specified. No physical units or angle units are involved; all quantities are dimensionless real numbers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_halfspace(y: np.ndarray, a: np.ndarray, b: float, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Euclidean projection of y onto the halfspace {u | a^T u = b}.\n    If a^T y = b (within tolerance), return y unchanged.\n    Otherwise, project along the normal vector a.\n    \"\"\"\n    ay = float(np.dot(a, y))\n    if ay = b + tol:\n        return y.copy()\n    norm_a2 = float(np.dot(a, a))\n    if norm_a2 == 0.0:\n        # Degenerate constraint; ignore.\n        return y.copy()\n    t = (ay - b) / norm_a2\n    return y - t * a\n\n\ndef dykstra_projection(u0: np.ndarray, A: np.ndarray, b: np.ndarray,\n                       tol: float = 1e-10, max_iter: int = 10000) -> np.ndarray:\n    \"\"\"\n    Dykstra's algorithm to project u0 onto intersection of halfspaces {u | A u = b}.\n    A: (m, p) matrix, rows are normals a_i^T.\n    b: (m,) vector of halfspace offsets.\n    \"\"\"\n    m, p = A.shape\n    z = u0.copy()\n    corrections = np.zeros((m, p), dtype=float)\n\n    for _ in range(max_iter):\n        z_prev = z.copy()\n        # Cycle through constraints\n        for i in range(m):\n            y = z + corrections[i]\n            z_new = project_halfspace(y, A[i], b[i])\n            corrections[i] = y - z_new\n            z = z_new\n        # Convergence check\n        if np.linalg.norm(z - z_prev) = tol:\n            break\n    return z\n\n\ndef solve():\n    # Fixed matrices G, H, d as specified.\n    G = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [-1.0, 0.0],\n        [0.0, -1.0],\n        [1.0, 1.0]\n    ], dtype=float)\n    H = np.array([\n        [-0.5, 0.0],\n        [ 0.0, 0.0],\n        [ 0.0, 0.0],\n        [ 0.0, 0.0],\n        [-0.2, 0.1]\n    ], dtype=float)\n    d = np.array([1.0, 1.0, 1.0, 1.0, 1.2], dtype=float)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Each case: (x, u0)\n        (np.array([0.0, 0.0], dtype=float), np.array([0.5, 0.25], dtype=float)),\n        (np.array([0.0, 0.0], dtype=float), np.array([2.0, 0.0], dtype=float)),\n        (np.array([0.0, 0.0], dtype=float), np.array([2.0, -2.0], dtype=float)),\n        (np.array([0.0, -7.0], dtype=float), np.array([0.8, 0.4], dtype=float)),\n        (np.array([0.0, 0.0], dtype=float), np.array([1.0, 0.2], dtype=float)),\n        (np.array([2.0, 0.0], dtype=float), np.array([0.8, 0.9], dtype=float)),\n    ]\n\n    results = []\n    for x, u0 in test_cases:\n        h = H @ x + d\n        u_star = dykstra_projection(u0, G, h, tol=1e-12, max_iter=20000)\n        results.append([float(u_star[0]), float(u_star[1])])\n\n    # Final print statement in the exact required format.\n    # Single line: list of lists of floats.\n    print(f\"[{','.join(['['+','.join(map(str, r))+']' for r in results])}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}