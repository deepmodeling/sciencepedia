## Introduction
Controlling large-scale, interconnected systems—from smart grids and autonomous vehicle fleets to complex industrial processes—presents a formidable challenge. While centralized control offers a theoretical optimum, its practical implementation is often crippled by computational demands, communication bottlenecks, and a [single point of failure](@entry_id:267509). Distributed Model Predictive Control (DMPC) emerges as a powerful and scalable paradigm to overcome these limitations by intelligently decomposing control tasks among a network of collaborating agents. This article provides a comprehensive exploration of DMPC. The first chapter, **Principles and Mechanisms**, lays the theoretical groundwork, dissecting the transition from centralized to distributed architectures, the nature of system coupling, and the core algorithms and stability guarantees that make DMPC reliable. Building on this foundation, the second chapter, **Applications and Interdisciplinary Connections**, demonstrates the practical utility of DMPC across diverse fields and addresses advanced topics critical for real-world deployment. Finally, **Hands-On Practices** will offer opportunities to apply these concepts through guided problems. Let us begin by delving into the foundational principles that enable this distributed control framework.

## Principles and Mechanisms

This chapter delves into the foundational principles and core mechanisms that underpin Distributed Model Predictive Control (DMPC). We will transition from the idealized concept of centralized control to the practical necessities of distributed decision-making. We will dissect the nature of system coupling, explore a [taxonomy](@entry_id:172984) of control architectures, and examine the coordination algorithms that enable cooperation among autonomous agents. Subsequently, we will establish the theoretical guarantees of stability and [recursive feasibility](@entry_id:167169) that are essential for any reliable control scheme. Finally, we will investigate advanced methods designed to handle the complexities of real-world systems, including nonlinearity, external disturbances, and imperfect communication networks.

### From Centralized to Distributed Optimal Control

The theoretical ideal for controlling a networked system is a **centralized Model Predictive Control (MPC)** architecture. In this paradigm, a single, omniscient controller gathers all state information from every subsystem, possesses a perfect model of the entire network's dynamics, and solves one large-scale optimization problem to determine the optimal control inputs for all agents simultaneously.

Consider a network of $M$ agents, where the dynamics of agent $j$ are described by a linear time-invariant (LTI) model that includes influences from its neighbors $\ell \in \mathcal{N}_j$:
$$
x_{t+1}^j = A^{jj} x_t^j + \sum_{\ell \in \mathcal{N}_j} A^{j\ell} x_t^\ell + B^j u_t^j
$$
If the goal is to minimize a cumulative cost over a prediction horizon $N$, for instance, a quadratic cost function that is separable or includes penalties on the differences between neighbor states, and to satisfy both local constraints on states and inputs as well as global, coupling constraints (e.g., a shared resource limit $\sum_{j=1}^M C^j u_t^j \le d$), the centralized MPC controller formulates this as a single, large-scale [convex optimization](@entry_id:137441) problem . The decision variables are the complete input trajectories for all agents, $\{u_t^j\}_{t=0, j=1}^{N-1, M}$. This centralized problem yields the true "socially optimal" solution, maximizing the collective performance of the network.

However, the centralized approach suffers from severe practical limitations that motivate the shift towards [distributed control](@entry_id:167172):
1.  **Computational Complexity**: The size of the optimization problem grows rapidly with the number of agents and the length of the prediction horizon, quickly becoming computationally intractable for a single processor.
2.  **Communication Overhead**: It requires all-to-all communication of state information to a central unit at every sampling instant, which can overwhelm network bandwidth.
3.  **Lack of Robustness**: A failure of the central controller or a critical communication link can paralyze the entire network.
4.  **Scalability and Modularity**: Adding or removing agents from the network requires a complete redesign of the central controller.

**Distributed Model Predictive Control (DMPC)** emerges as a solution to these challenges. The fundamental idea is to decompose the monolithic centralized optimization problem into smaller, more manageable subproblems, one for each agent (or a local group of agents). Each agent solves its own local MPC problem but coordinates its decisions with its neighbors by exchanging information. This decomposition is the first and most critical mechanism of DMPC, enabling [scalability](@entry_id:636611) and resilience. The core challenge, which the remainder of this chapter addresses, is to design coordination strategies and local control laws such that the collective behavior of these interacting agents approximates the performance of the ideal centralized controller while guaranteeing stability and [constraint satisfaction](@entry_id:275212).

### A Taxonomy of Large-Scale Control Architectures

The term "[distributed control](@entry_id:167172)" encompasses a spectrum of architectures, each defined by its unique information structure and coordination mechanism. Understanding these distinctions is crucial for selecting an appropriate design for a given application .

**Decentralized MPC**: This is the simplest form of large-scale control. In a purely decentralized architecture, there is no online communication between agents for the purpose of coordination. Each agent's controller, $i$, makes decisions based solely on its local information set, $\mathcal{I}_i^k$, which typically contains its own state measurements and local model. The influence of neighboring subsystems, such as the term $\sum_{j \in \mathcal{N}_i} A_{ij} x_j$, is treated as an unknown disturbance. To maintain safety, the controller must be designed robustly, often by assuming this disturbance is bounded and tightening constraints accordingly. While simple and communication-free, decentralized MPC is often conservative and can lead to poor performance or even instability if the system coupling is strong.

**Distributed MPC (DMPC)**: This architecture represents a middle ground and is the primary focus of this text. In DMPC, agents are permitted to communicate with their neighbors over a predefined communication graph. At each control step, agents engage in an iterative peer-to-peer negotiation process to coordinate their planned actions. They solve their local [optimization problems](@entry_id:142739) and then exchange information, such as predicted state trajectories, proposed resource usage, or [dual variables](@entry_id:151022) associated with coupling constraints. Based on the information received, they update and resolve their local problems. This iterative process continues for a fixed number of rounds or until some measure of consensus is achieved. This explicit coordination allows DMPC to handle [strong coupling](@entry_id:136791) far more effectively than decentralized control, often approaching the performance of centralized MPC.

**Hierarchical MPC**: This architecture imposes a layered, [top-down control](@entry_id:150596) structure. It typically consists of at least two levels. A high-level coordinator or "upper layer" oversees the entire network, often using a simplified, aggregate model. It solves a coarse optimization problem to determine long-term strategic goals, which it then communicates to the lower-level agents as directives. These directives can take the form of reference setpoints, modified local constraints, or economic "prices" for shared resources. Each low-level agent then solves its own local MPC problem, treating the coordinator's directives as additional constraints or cost terms. The information flow is typically bidirectional and vertical: directives flow down, and aggregated reports on performance or resource needs flow up. This architecture is particularly well-suited for systems with clear separations of timescale or physical authority, such as in power grid management or large-scale manufacturing processes.

### The Anatomy of System Coupling

The specific mechanisms required for distributed coordination depend critically on the nature of the coupling between subsystems. System interconnections can be categorized into three distinct types, each demanding a different algorithmic approach .

**Dynamic Coupling**: This is the most direct form of coupling, present when the [state evolution](@entry_id:755365) of one subsystem explicitly depends on the states or inputs of others. In our running example, $x_{i,k+1} = f_i(x_{i,k}, u_{i,k}, x_{\mathcal{N}_i,k})$, the presence of neighbor states $x_{\mathcal{N}_i,k}$ in the function $f_i$ signifies dynamic coupling. This means that agent $i$'s prediction of its own future state trajectory is impossible without a prediction of its neighbors' trajectories. To handle this, distributed solvers must facilitate the exchange of these predicted trajectories among agents. An agent's digital twin can use the latest received neighbor plans to formulate its local optimization, and through iterative exchange, these plans converge towards a mutually consistent solution.

**Constraint Coupling**: This arises when a system-wide constraint involves the variables of multiple subsystems. A canonical example is a shared resource limit, such as a total power budget for a group of robots, expressed as $\sum_{j=1}^M C_j u_j(k) \le d$. Here, agent $i$'s choice of input $u_i(k)$ directly impacts the feasible input choices for all other agents. This type of coupling is often addressed not by exchanging full trajectories, but through economic principles using **[dual decomposition](@entry_id:169794)**. The shared constraint is associated with a Lagrange multiplier, or "price." A coordinator (which can be implemented in a distributed or hierarchical fashion) sets this price, and each agent solves its local problem, which now includes an additional cost for consuming the shared resource. The agents report their intended consumption, and the coordinator updates the price to balance supply and demand. This price-based mechanism effectively coordinates actions without requiring agents to share their detailed internal state information .

**Objective Coupling**: This form of coupling occurs when the overall performance index (the cost function) is not additively separable. For instance, a [formation control](@entry_id:170979) objective might penalize the distance between agents, resulting in cross-terms like $(x_i - x_j)^\top S_{ij} (x_i - x_j)$ in the cost function. In this case, the gradient of the global objective with respect to agent $i$'s decision variables depends on agent $j$'s variables. An agent cannot optimize its contribution to the global cost in isolation. Handling objective coupling typically requires specialized [consensus algorithms](@entry_id:164644). For instance, using the Alternating Direction Method of Multipliers (ADMM), agents can create local copies of their neighbors' variables, optimize their local cost assuming these copies are correct, and then iteratively average their copies with their neighbors' actual variables to reach a consensus, guided by associated [dual variables](@entry_id:151022).

### Coordination Mechanisms and Game-Theoretic Perspectives

At its heart, DMPC is a problem of multi-agent decision-making. Game theory provides a powerful lens through which to analyze and design coordination mechanisms. In this view, each agent is a rational player seeking to minimize its own local cost function, but its feasible actions are constrained by the actions of others .

A key concept is the **Nash Equilibrium**, a state where no single agent can improve its own situation by unilaterally changing its strategy, given that all other agents' strategies remain fixed. In a general DMPC game with coupling, the Nash equilibrium does not typically coincide with the socially [optimal solution](@entry_id:171456) (i.e., the solution to the centralized problem). This is because selfish agents ignore the "[externalities](@entry_id:142750)" they impose on others—for example, how their use of a shared resource affects the costs of their neighbors.

The goal of a well-designed DMPC mechanism is to structure the "game" such that the Nash equilibrium aligns with the social optimum. Several powerful techniques achieve this:

1.  **Price-Based Coordination (Dual Decomposition)**: As mentioned for constraint coupling, introducing a common price (Lagrange multiplier) for a shared resource modifies each agent's local cost. A specific type of equilibrium, known as a **variational Generalized Nash Equilibrium (v-GNE)**, is one where all agents base their decisions on the same shared price. If this shared price is identical to the optimal Lagrange multiplier from the centralized problem, the resulting equilibrium is guaranteed to be socially optimal. Distributed algorithms can be designed to find this optimal price iteratively .

2.  **Potential Games**: Another approach is to modify the local cost functions so that the non-cooperative game becomes an **exact potential game**. This is a special class of games where the change in any player's individual cost due to a unilateral change in strategy is exactly equal to the change in a global "[potential function](@entry_id:268662)." In this case, the Nash equilibria of the game are simply the local [minimizers](@entry_id:897258) of the [potential function](@entry_id:268662). A common technique is to add an identical penalty term to each agent's cost that penalizes violation of a coupling constraint. For example, to enforce $\sum_j S_j u_j \le \bar{s}$, one can add the term $\rho \|[\sum_j S_j u_j - \bar{s}]_+\|^2$ to every agent's cost. The potential function becomes the sum of the original costs plus this penalty. As the penalty weight $\rho \to \infty$, the Nash equilibria of this game converge to the socially [optimal solution](@entry_id:171456) of the original constrained problem .

A cornerstone algorithm for implementing such coordination is the **Alternating Direction Method of Multipliers (ADMM)**. ADMM is particularly effective for problems with separable objectives and linear coupling constraints, a common structure in DMPC. Consider a [consensus problem](@entry_id:637652) where each agent $i$ wishes to minimize a local quadratic cost $\frac{1}{2} x_i^\top Q_i x_i + q_i^\top x_i$, but all must agree on a common value, $x_i = z$ for all $i$. ADMM solves this by forming an augmented Lagrangian and iterating through three steps :

-   **Local Optimization ($x_i$-update)**: Each agent solves its local problem, which now includes a quadratic penalty for deviating from the current global consensus variable $z^k$ and a linear term from a dual variable $u_i^k$:
    $$
    x_i^{k+1} = \arg\min_{x_i} \left\{ \frac{1}{2} x_i^\top Q_i x_i + q_i^\top x_i + \frac{\rho}{2} \left\| x_i - z^k + u_i^k \right\|_2^2 \right\}
    $$
    This step can be performed in parallel by all agents. The parameter $\rho > 0$ acts as a penalty, or a "virtual spring stiffness," that pulls the local solutions toward agreement.

-   **Consensus Update ($z$-update)**: The consensus variable $z$ is updated by gathering the new local solutions $x_i^{k+1}$ and averaging them, effectively finding the point that minimizes the sum of squared distances to all local solutions:
    $$
    z^{k+1} = \frac{1}{N} \sum_{i=1}^{N} \left(x_i^{k+1} + u_i^k\right)
    $$
    This step requires communication to a central point or a distributed averaging protocol.

-   **Price Update ($u_i$-update)**: Each agent updates its local scaled dual variable $u_i$, which acts as a running sum of the error between its local variable and the consensus variable:
    $$
    u_i^{k+1} = u_i^k + x_i^{k+1} - z^{k+1}
    $$
    These [dual variables](@entry_id:151022) are the "prices" that drive the local solutions toward consensus.

This iterative process blends local optimization with global consensus-making, providing a robust and widely applicable mechanism for distributed control.

### Foundational Guarantees: Stability and Recursive Feasibility

An MPC controller, whether centralized or distributed, is only viable if it can be proven to be stable and recursively feasible.

**Stability in MPC**: The standard method for proving [asymptotic stability](@entry_id:149743) of a nominal MPC scheme for regulating a system to the origin is to show that the optimal cost function of the MPC problem acts as a **Lyapunov function** for the closed-loop system. This requires several key ingredients in the MPC formulation :
1.  A **[positive definite](@entry_id:149459) stage cost**, such as $\|x\|_Q^2 + \|u\|_R^2$ with $Q \succeq 0$ and $R \succ 0$.
2.  A **terminal cost** function, $\|x_N\|_P^2$, where $P \succ 0$ is a [positive definite matrix](@entry_id:150869).
3.  A **[terminal constraint](@entry_id:176488) set**, $\mathcal{X}_f$, which is a positively invariant set for the system under a pre-designed local stabilizing feedback law, $u=Kx$.
The crucial condition linking these components is a **discrete-time Lyapunov inequality**. The terminal [cost matrix](@entry_id:634848) $P$ and the terminal [controller gain](@entry_id:262009) $K$ must be designed together such that the cost-to-go from any state within the [terminal set](@entry_id:163892) is bounded by the terminal cost. This is satisfied if $P$ is the solution to the Lyapunov inequality:
$$
(A+BK)^\top P (A+BK) - P \preceq -(Q + K^\top R K)
$$
This inequality ensures that one step of the terminal controller $u=Kx$ decreases the Lyapunov function $\|x\|_P^2$ by at least the value of one stage cost. When this structure is in place, the optimal value function of the MPC problem can be shown to decrease at every time step, proving [asymptotic stability](@entry_id:149743).

**Recursive Feasibility in DMPC**: In a distributed and constrained setting, **[recursive feasibility](@entry_id:167169)**—the guarantee that if the optimization problem is feasible at time $t$, it will remain feasible for all future times $t+1, t+2, \dots$—is a paramount and non-trivial concern. A controller that drives the system into a state from which no feasible plan exists has failed.

In DMPC, the challenge is amplified by the mismatch between the predicted actions of neighbors and their actual actions. To guarantee [recursive feasibility](@entry_id:167169), the design must be robust to this mismatch . The proof relies on constructing a feasible candidate solution for the optimization problem at time $t+1$ based on the optimal plan from time $t$. This is typically done by using a "shifted" version of the previous plan. The key mechanism is the use of a **Robust Positively Invariant (RPI)** [terminal set](@entry_id:163892) $\mathcal{X}_{f,i}$. This set must be designed such that for any state $x_i \in \mathcal{X}_{f,i}$, the terminal control law $u_i = K_i x_i$ keeps the next state within $\mathcal{X}_{f,i}$ *for any possible realization of the coupling disturbance* $w_i$, which arises from the mismatch between predicted and actual neighbor states. This requires a bound on the coupling disturbance, which is achieved through **coupling feasibility constraints**—agreements between agents that limit how much their future plans can deviate from what they have broadcast. With an RPI set and bounded coupling disturbances, it can be proven that a [feasible solution](@entry_id:634783) always exists, thus ensuring the controller can operate indefinitely.

### Advanced Implementations for Robustness and Performance

The principles discussed thus far form the basis of DMPC. We now turn to advanced mechanisms that address the challenges of real-world systems, including external disturbances, nonlinear dynamics, and imperfect communication.

**Tube-Based DMPC for Robustness**: Physical systems are subject to [process noise](@entry_id:270644) and modeling errors, which can be modeled as additive bounded disturbances $w_k^i \in \mathcal{W}_i$. **Tube-based MPC** is a powerful framework for handling such uncertainties . The core idea is to decompose the control action and state trajectory into two parts: a *nominal* component, which is optimized, and an *error* component, which is handled by a separate feedback law.
-   The state and input are decomposed as $x_k^i = z_k^i + e_k^i$ and $u_k^i = v_k^i + K_i e_k^i$, where $(z_k^i, v_k^i)$ is the nominal trajectory and control, and $e_k^i$ is the state error.
-   An **ancillary feedback controller** $K_i$ is designed offline to stabilize the error dynamics $e_{k+1}^i = (A_i + B_i K_i) e_k^i + \dots$.
-   The key is to compute a **Robust Positively Invariant (RPI) set**, or "tube," $\mathcal{E}_i$, which is guaranteed to contain the error $e_k^i$ at all times, provided it starts within it. In a distributed setting, the computation of $\mathcal{E}_i$ must account for not only the local disturbance $w_k^i \in \mathcal{W}_i$ but also the propagation of errors from coupled neighbors, $\sum_{j \in \mathcal{N}_i} A_{ij} e_k^j$. The RPI condition becomes a coupled set-theoretic equation:
    $$
    (A_i + B_i K_i) \mathcal{E}_i \oplus \Big( \mathcal{W}_i \oplus \bigoplus_{j \in \mathcal{N}_i} A_{ij} \mathcal{E}_j \Big) \subseteq \mathcal{E}_i.
    $$
-   To ensure the actual state $x_k^i$ and input $u_k^i$ satisfy their hard constraints, the nominal MPC problem is solved with **tightened constraints**. The nominal state $z_k^i$ must remain within a smaller set $\mathcal{X}_i \ominus \mathcal{E}_i$, where $\ominus$ is the Pontryagin [set difference](@entry_id:140904). This ensures that even for the [worst-case error](@entry_id:169595) at the boundary of the tube $\mathcal{E}_i$, the true state $z_k^i+e_k^i$ will not violate the original constraint $\mathcal{X}_i$. A similar tightening is applied to the input constraints.

**DMPC for Nonlinear Systems**: Many real-world systems exhibit nonlinear dynamics, $x_{i,k+1} = f_i(x_{i,k}, u_{i,k}, \dots)$. Solving the resulting nonlinear DMPC problem directly is often too slow for real-time applications. Two dominant strategies have emerged :

1.  **Successive Linearization**: This technique is a core component of Sequential Quadratic Programming (SQP). At each iteration of the optimization, the nonlinear dynamics are linearized around the current nominal trajectory using a first-order Taylor expansion. This transforms the nonlinear problem into a Quadratic Program (QP) subproblem with linear time-varying (LTV) dynamics, which can be solved efficiently. The solution to the QP is then used to update the nominal trajectory, and the process is repeated.

2.  **Real-Time Iteration (RTI)**: Recognizing that full convergence of the SQP algorithm at each sampling instant is a bottleneck, the RTI scheme performs only a **single SQP iteration per sampling instant**. At time $k$, the controller takes the optimal trajectory computed at time $k-1$, shifts it forward in time, and uses this as a high-quality "warm start" to linearize the dynamics. It then solves only one QP subproblem to find an improved control input, applies it to the system, and moves to the next time step. The control feedback loop itself drives the optimization process towards optimality over time, leveraging the powerful contraction properties of the underlying Newton-type method to ensure high performance and feasibility without the need for computationally expensive inner-loop convergence.

**DMPC over Unreliable Networks**: A final practical consideration is the communication network, which is rarely perfect. Packets carrying information between agents can be delayed or dropped entirely . A robust DMPC framework must be designed with these imperfections in mind. A principled approach involves:
-   **Explicitly Modeling Uncertainty**: Communication is modeled with bounded delays $0 \le \tau_{ij}(t) \le \tau_{\max}$ and stochastic packet drops (e.g., a Bernoulli process).
-   **Using Stale Information**: At decision time $k$, agent $i$ does not have access to its neighbors' current states $x_j(k)$. Its information set $\mathcal{I}_i(k)$ contains only its own state $x_i(k)$ and the last successfully received, timestamped messages from its neighbors.
-   **Robust Prediction**: Agent $i$ uses this stale information to predict its neighbors' behavior, for example, by assuming they will continue to execute their last broadcasted plan (a "[zero-order hold](@entry_id:264751)" on the plan). The inevitable mismatch between this prediction and the neighbors' true actions, which is exacerbated by delays and drops, is treated as a bounded disturbance.
-   **Leveraging Robust Control**: The problem is now transformed into one of controlling a system with bounded disturbances. The tube-based MPC framework described earlier is a perfect tool for this. The size of the disturbance set, and thus the size of the tube and the degree of [constraint tightening](@entry_id:174986), is determined by the maximum delay $\tau_{\max}$ and the worst-case sequence of packet drops the system is designed to tolerate. This ensures that constraints are satisfied and the system remains stable despite the unreliable communication. The system's digital twins are essential for performing the complex predictions and robust optimizations required by this approach.