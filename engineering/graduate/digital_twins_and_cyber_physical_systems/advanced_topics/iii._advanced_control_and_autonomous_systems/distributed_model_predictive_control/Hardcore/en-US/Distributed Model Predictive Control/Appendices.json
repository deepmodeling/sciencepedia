{
    "hands_on_practices": [
        {
            "introduction": "To solve large-scale optimization problems in a distributed manner, we must first decompose them into smaller, manageable subproblems that can be solved locally by individual agents. The Alternating Direction Method of Multipliers (ADMM) is a powerful and popular algorithm for this task, blending the benefits of dual decomposition and augmented Lagrangian methods. This practice  challenges you to derive the ADMM update steps from first principles for a canonical DMPC setup, providing fundamental insight into the mechanics of distributed convex optimization.",
            "id": "4218526",
            "problem": "Consider a networked Cyber-Physical System (CPS) whose Digital Twin (DT) is used to coordinate the control of $N$ subsystems via Distributed Model Predictive Control (MPC). At a given sampling instant, each subsystem $i \\in \\{1,\\dots,N\\}$ selects a local control vector $u^{i} \\in \\mathbb{R}^{n_{i}}$ to minimize a local quadratic stage cost while respecting a global coupling constraint. The local cost is given by $f_{i}(u^{i}) = \\frac{1}{2}(u^{i})^{\\top} Q_{i} u^{i} + c_{i}^{\\top} u^{i}$, where $Q_{i} \\in \\mathbb{R}^{n_{i} \\times n_{i}}$ is symmetric positive definite and $c_{i} \\in \\mathbb{R}^{n_{i}}$. The global coupling constraint arises from physical resource sharing across subsystems and is modeled by\n$$\n\\sum_{i=1}^{N} H_{i} u^{i} = h,\n$$\nwhere, for each $i$, $H_{i} \\in \\mathbb{R}^{m \\times n_{i}}$ and $h \\in \\mathbb{R}^{m}$ is given by the DT at the current sampling instant. Assume each $H_{i}$ has full column rank.\n\nThe DT aims to compute the optimal controls by solving the convex program\n$$\n\\min_{\\{u^{i}\\}_{i=1}^{N}} \\ \\sum_{i=1}^{N} f_{i}(u^{i}) \\quad \\text{subject to} \\quad \\sum_{i=1}^{N} H_{i} u^{i} = h.\n$$\nTo obtain a distributed algorithm that preserves locality, introduce a consensus variable $z := (z^{1},\\dots,z^{N})$ with components $z^{i} \\in \\mathbb{R}^{m}$, together with local consistency constraints $H_{i} u^{i} - z^{i} = 0$ and the global affine constraint $\\sum_{i=1}^{N} z^{i} = h$. Formulate the problem equivalently as\n$$\n\\min_{\\{u^{i}\\}, \\{z^{i}\\}} \\ \\sum_{i=1}^{N} f_{i}(u^{i}) + I_{\\mathcal{C}}(z) \\quad \\text{subject to} \\quad H_{i} u^{i} - z^{i} = 0 \\ \\text{for all } i,\n$$\nwhere $I_{\\mathcal{C}}(z)$ is the indicator function of the affine set $\\mathcal{C} := \\left\\{ z \\in \\mathbb{R}^{mN} : \\sum_{i=1}^{N} z^{i} = h \\right\\}$, defined by $I_{\\mathcal{C}}(z) = 0$ if $z \\in \\mathcal{C}$ and $I_{\\mathcal{C}}(z) = +\\infty$ otherwise.\n\nStarting from the core definitions of augmented Lagrangian methods and the Alternating Direction Method of Multipliers (ADMM), derive the scaled-form ADMM iterations with penalty parameter $\\rho > 0$ for the local controls $u^{i}$ and the consensus variable components $z^{i}$, together with the scaled dual variables $y^{i} \\in \\mathbb{R}^{m}$ associated with the constraints $H_{i} u^{i} - z^{i} = 0$. Your derivation must begin from the augmented Lagrangian construction and show how each block-minimization and dual ascent step is obtained from first principles. Then, specialize the $u^{i}$-update to the given quadratic costs and provide explicit, closed-form update equations for $u^{i}_{k+1}$, $z^{i}_{k+1}$, and $y^{i}_{k+1}$ in terms of $Q_{i}$, $c_{i}$, $H_{i}$, $h$, $\\rho$, and previously iterated quantities. Express your final answer as three analytic expressions corresponding to $u^{i}_{k+1}$, $z^{i}_{k+1}$, and $y^{i}_{k+1}$ that are valid for all $i \\in \\{1,\\dots,N\\}$.\n\nNo numerical evaluation is required, and no rounding should be applied. Provide your final expressions as a single row matrix using the LaTeX `pmatrix` environment, with entries ordered as the updates for $u^{i}_{k+1}$, $z^{i}_{k+1}$, and $y^{i}_{k+1}$.",
            "solution": "The problem requires the derivation of the scaled-form Alternating Direction Method of Multipliers (ADMM) iterations for a distributed optimization problem arising in the context of Digital Twins and Cyber-Physical Systems. We begin by validating the problem statement.\n\nThe givens are:\n- $N$ subsystems, indexed by $i \\in \\{1, \\dots, N\\}$.\n- Local control vectors $u^{i} \\in \\mathbb{R}^{n_{i}}$.\n- Local cost functions $f_{i}(u^{i}) = \\frac{1}{2}(u^{i})^{\\top} Q_{i} u^{i} + c_{i}^{\\top} u^{i}$, where $Q_{i} \\in \\mathbb{R}^{n_{i} \\times n_{i}}$ is symmetric positive definite and $c_{i} \\in \\mathbb{R}^{n_{i}}$.\n- A global coupling constraint $\\sum_{i=1}^{N} H_{i} u^{i} = h$, where $H_{i} \\in \\mathbb{R}^{m \\times n_{i}}$ has full column rank and $h \\in \\mathbb{R}^{m}$.\n- The equivalent ADMM-ready formulation:\n$$ \\min_{\\{u^{i}\\}, \\{z^{i}\\}} \\ \\sum_{i=1}^{N} f_{i}(u^{i}) + I_{\\mathcal{C}}(z) \\quad \\text{subject to} \\quad H_{i} u^{i} - z^{i} = 0 \\ \\text{for all } i, $$\nwhere $z = (z^{1}, \\dots, z^{N})$ with $z^{i} \\in \\mathbb{R}^{m}$, and $I_{\\mathcal{C}}(z)$ is the indicator function of the affine set $\\mathcal{C} := \\left\\{ z \\in \\mathbb{R}^{mN} : \\sum_{i=1}^{N} z^{i} = h \\right\\}$.\n- The ADMM penalty parameter is $\\rho > 0$.\n- The scaled dual variables are $y^{i} \\in \\mathbb{R}^{m}$.\n\nThe problem is scientifically grounded, well-posed, objective, complete, and mathematically rigorous. It is a standard application of convex optimization techniques to distributed control, a core topic in the specified field. The problem is therefore deemed valid. We proceed with the derivation.\n\nThe ADMM algorithm is applied to problems of the form $\\min_{x,w} F(x) + G(w)$ subject to $Ax + Bw = c$. In our case, the optimization variables are the collection of local controls $u = (u^{1}, \\dots, u^{N})$ and the consensus variables $z = (z^{1}, \\dots, z^{N})$. The objective function is separable: $F(u) = \\sum_{i=1}^{N} f_{i}(u^{i})$ and $G(z) = I_{\\mathcal{C}}(z)$. The constraints are $H_{i}u^{i} - z^{i} = 0$ for all $i \\in \\{1, \\dots, N\\}$.\n\nThe scaled-form augmented Lagrangian $\\mathcal{L}_{\\rho}$ is constructed for the problem. Let $y = (y^{1}, \\dots, y^{N})$ be the stacked vector of scaled dual variables.\n$$ \\mathcal{L}_{\\rho}(u, z, y) = \\sum_{i=1}^{N} f_{i}(u^{i}) + I_{\\mathcal{C}}(z) + \\sum_{i=1}^{N} \\frac{\\rho}{2} \\| H_{i}u^{i} - z^{i} + y^{i} \\|_{2}^{2} - \\sum_{i=1}^{N} \\frac{\\rho}{2} \\| y^{i} \\|_{2}^{2} $$\nThe constant term involving $\\|y^{i}\\|_{2}^{2}$ can be omitted during minimization. The ADMM algorithm proceeds by iterating three steps at each iteration $k$: a minimization with respect to $u$, a minimization with respect to $z$, and a dual variable update.\n\nStep 1: The $u$-update\nAt iteration $k+1$, we update the primal variables $u$ by minimizing $\\mathcal{L}_{\\rho}$ with $z$ and $y$ fixed at their values from iteration $k$:\n$$ u_{k+1} = \\arg\\min_{u} \\mathcal{L}_{\\rho}(u, z_{k}, y_{k}) $$\nDue to the separable structure of the objective and constraints, this minimization decomposes into $N$ independent subproblems, one for each subsystem $i$:\n$$ u^{i}_{k+1} = \\arg\\min_{u^{i}} \\left( f_{i}(u^{i}) + \\frac{\\rho}{2} \\| H_{i}u^{i} - z^{i}_{k} + y^{i}_{k} \\|_{2}^{2} \\right) $$\nSubstituting the quadratic form of $f_{i}(u^{i})$, the objective for this subproblem is:\n$$ J_{i}(u^{i}) = \\frac{1}{2}(u^{i})^{\\top}Q_{i}u^{i} + c_{i}^{\\top}u^{i} + \\frac{\\rho}{2} (H_{i}u^{i} - z^{i}_{k} + y^{i}_{k})^{\\top}(H_{i}u^{i} - z^{i}_{k} + y^{i}_{k}) $$\nThis function is strictly convex in $u^{i}$ because $Q_{i}$ is positive definite and $\\rho > 0$. The unique minimizer is found by setting the gradient with respect to $u^{i}$ to zero:\n$$ \\nabla_{u^{i}} J_{i}(u^{i}) = Q_{i}u^{i} + c_{i} + \\rho H_{i}^{\\top}(H_{i}u^{i} - z^{i}_{k} + y^{i}_{k}) = 0 $$\nRearranging terms to solve for $u^{i}$:\n$$ (Q_{i} + \\rho H_{i}^{\\top}H_{i}) u^{i} = \\rho H_{i}^{\\top}(z^{i}_{k} - y^{i}_{k}) - c_{i} $$\nThe matrix $(Q_{i} + \\rho H_{i}^{\\top}H_{i})$ is invertible because $Q_{i}$ is positive definite and $H_{i}^{\\top}H_{i}$ is positive semi-definite. Thus, we obtain the closed-form update for $u^{i}_{k+1}$:\n$$ u^{i}_{k+1} = (Q_{i} + \\rho H_{i}^{\\top}H_{i})^{-1} \\left( \\rho H_{i}^{\\top}(z^{i}_{k} - y^{i}_{k}) - c_{i} \\right) $$\n\nStep 2: The $z$-update\nNext, we update the consensus variables $z$ by minimizing $\\mathcal{L}_{\\rho}$ with $u$ fixed at $u_{k+1}$ and $y$ at $y_{k}$:\n$$ z_{k+1} = \\arg\\min_{z} \\mathcal{L}_{\\rho}(u_{k+1}, z, y_{k}) $$\nThis minimization is equivalent to solving:\n$$ \\min_{z} \\left( I_{\\mathcal{C}}(z) + \\sum_{i=1}^{N} \\frac{\\rho}{2} \\| H_{i}u^{i}_{k+1} - z^{i} + y^{i}_{k} \\|_{2}^{2} \\right) $$\nThe indicator function $I_{\\mathcal{C}}(z)$ enforces the constraint $\\sum_{i=1}^{N} z^{i} = h$. The problem can be rewritten as a projection onto the affine set $\\mathcal{C}$:\n$$ \\min_{\\{z^{i}\\}} \\sum_{i=1}^{N} \\| z^{i} - (H_{i}u^{i}_{k+1} + y^{i}_{k}) \\|_{2}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{N} z^{i} = h $$\nWe form the Lagrangian for this subproblem with a multiplier $\\nu \\in \\mathbb{R}^{m}$:\n$$ \\mathcal{L}_{z}(\\{z^{i}\\}, \\nu) = \\sum_{i=1}^{N} \\| z^{i} - (H_{i}u^{i}_{k+1} + y^{i}_{k}) \\|_{2}^{2} + 2\\nu^{\\top}\\left(\\sum_{i=1}^{N} z^{i} - h\\right) $$\nTaking the gradient with respect to each $z^{i}$ and setting it to zero yields:\n$$ 2(z^{i} - (H_{i}u^{i}_{k+1} + y^{i}_{k})) + 2\\nu = 0 \\implies z^{i} = (H_{i}u^{i}_{k+1} + y^{i}_{k}) - \\nu $$\nTo find $\\nu$, we enforce the constraint $\\sum_{i=1}^{N} z^{i} = h$:\n$$ \\sum_{i=1}^{N} \\left( (H_{i}u^{i}_{k+1} + y^{i}_{k}) - \\nu \\right) = h \\implies \\sum_{i=1}^{N} (H_{i}u^{i}_{k+1} + y^{i}_{k}) - N\\nu = h $$\nSolving for $\\nu$:\n$$ \\nu = \\frac{1}{N} \\left( \\sum_{j=1}^{N} (H_{j}u^{j}_{k+1} + y^{j}_{k}) - h \\right) $$\nSubstituting this expression for $\\nu$ back into the equation for $z^{i}$ gives the update for $z^{i}_{k+1}$:\n$$ z^{i}_{k+1} = (H_{i}u^{i}_{k+1} + y^{i}_{k}) - \\frac{1}{N} \\left( \\sum_{j=1}^{N} (H_{j}u^{j}_{k+1} + y^{j}_{k}) - h \\right) $$\n\nStep 3: The $y$-update\nFinally, the scaled dual variables are updated using a dual ascent step. For each constraint $H_{i}u^{i} - z^{i} = 0$, the update rule is:\n$$ y^{i}_{k+1} = y^{i}_{k} + (H_{i}u^{i}_{k+1} - z^{i}_{k+1}) $$\nThis represents the update of the scaled dual variable based on the residual of the local consistency constraint at iteration $k+1$.\n\nThese three equations constitute the complete set of ADMM iterations for the given distributed optimization problem.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(Q_{i} + \\rho H_{i}^{\\top}H_{i})^{-1} \\left( \\rho H_{i}^{\\top}(z^{i}_{k} - y^{i}_{k}) - c_{i} \\right) & (H_{i}u^{i}_{k+1} + y^{i}_{k}) - \\frac{1}{N} \\left( \\sum_{j=1}^{N} (H_{j}u^{j}_{k+1} + y^{j}_{k}) - h \\right) & y^{i}_{k} + H_{i}u^{i}_{k+1} - z^{i}_{k+1}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In many distributed systems, agents act to optimize their own local objectives, leading to behavior that can be modeled as a non-cooperative game. The resulting system state, known as a Nash Equilibrium, is often less efficient than the one achieved by a centralized controller optimizing for the collective good. This exercise  invites you to quantify this inefficiency, often called the \"Price of Anarchy,\" by comparing the control sequences of a decentralized Nash equilibrium against a centralized optimum for a dynamic multi-agent system.",
            "id": "4218474",
            "problem": "Consider two agents in a Cyber-Physical System (CPS) whose Digital Twin (DT) employs Distributed Model Predictive Control (DMPC) over a finite horizon. Each agent has a single-input, single-state, discrete-time linear model, and their costs are cross-coupled through the states. The goal is to compute the open-loop Nash equilibrium control sequences and compare them to the centralized optimum that minimizes the sum of the agents’ costs.\n\nAssume the following for agent $i \\in \\{1,2\\}$:\n- The dynamics are given by $x_i(k+1) = a_i x_i(k) + b_i u_i(k)$ for $k \\in \\{0,1,\\dots,N-1\\}$.\n- The initial condition is $x_i(0) = x_{i0}$.\n- The stage cost for agent $i$ at time $k$ is $q_i x_i(k)^2 + r_i u_i(k)^2 + \\gamma x_1(k) x_2(k)$, where $q_i \\ge 0$, $r_i > 0$, and $\\gamma \\in \\mathbb{R}$.\n- The agent’s total cost over the horizon is $J_i = \\sum_{k=0}^{N-1} \\left(q_i x_i(k)^2 + r_i u_i(k)^2 + \\gamma x_1(k) x_2(k)\\right)$.\n\nWe will represent the state trajectory of each agent over the horizon in terms of its control sequence via standard linear system stacking. Define for agent $i$:\n- The stacked state vector $\\mathbf{x}_i = \\begin{bmatrix}x_i(0) & x_i(1) & \\dots & x_i(N-1)\\end{bmatrix}^\\top \\in \\mathbb{R}^N$.\n- The stacked control vector $\\mathbf{u}_i = \\begin{bmatrix}u_i(0) & u_i(1) & \\dots & u_i(N-1)\\end{bmatrix}^\\top \\in \\mathbb{R}^N$.\n- The stacked affine relation $\\mathbf{x}_i = \\boldsymbol{\\alpha}_i + T_i \\mathbf{u}_i$, where $\\boldsymbol{\\alpha}_i \\in \\mathbb{R}^N$ and $T_i \\in \\mathbb{R}^{N \\times N}$ are determined by the dynamics. Specifically, for $k \\in \\{0,1,\\dots,N-1\\}$ and $j \\in \\{0,1,\\dots,N-1\\}$,\n$$\n\\left(\\boldsymbol{\\alpha}_i\\right)_k = a_i^k x_{i0}, \\quad\n\\left(T_i\\right)_{k,j} =\n\\begin{cases}\nb_i a_i^{k-1-j}, & \\text{if } j \\le k-1,\\\\\n0, & \\text{if } j \\ge k.\n\\end{cases}\n$$\n\nLet $Q_i = q_i I_N$, $R_i = r_i I_N$, and $W = I_N$ where $I_N$ is the $N \\times N$ identity matrix. The Nash equilibrium is defined as the pair of control sequences $(\\mathbf{u}_1^\\star, \\mathbf{u}_2^\\star)$ such that each agent minimizes its own cost $J_i$ given the other’s decision, which is equivalent to the stationarity of the gradient of $J_i$ with respect to $\\mathbf{u}_i$. The centralized optimum is defined as the minimizer of the sum $J_{\\mathrm{sum}} = J_1 + J_2$ with respect to both $\\mathbf{u}_1$ and $\\mathbf{u}_2$.\n\nYour task is to:\n- Construct the block linear systems for the Nash equilibrium and the centralized optimum from first principles using the definitions above.\n- Solve these systems to obtain $\\mathbf{u}_1^\\star, \\mathbf{u}_2^\\star$ for the Nash equilibrium and $\\mathbf{u}_1^{\\mathrm{c}}, \\mathbf{u}_2^{\\mathrm{c}}$ for the centralized optimum.\n- Compute the comparison metric for each test case defined as the root-mean-square (RMS) difference between the Nash and centralized control sequences across both agents and all time steps:\n$$\n\\mathrm{RMS} = \\sqrt{\\frac{1}{2N} \\left\\| \\begin{bmatrix} \\mathbf{u}_1^\\star \\\\ \\mathbf{u}_2^\\star \\end{bmatrix} - \\begin{bmatrix} \\mathbf{u}_1^{\\mathrm{c}} \\\\ \\mathbf{u}_2^{\\mathrm{c}} \\end{bmatrix} \\right\\|_2^2 }.\n$$\n\nUse the following test suite of parameter sets, covering typical, boundary, and edge cases. For each case, compute the RMS difference. No physical units apply.\n\n- Test Case $1$: $N = 3$, $a_1 = 1.0$, $b_1 = 1.0$, $q_1 = 1.0$, $r_1 = 0.1$, $a_2 = 0.9$, $b_2 = 1.2$, $q_2 = 1.0$, $r_2 = 0.1$, $\\gamma = 0.5$, $x_{10} = 1.0$, $x_{20} = -1.0$.\n- Test Case $2$: $N = 4$, $a_1 = 1.0$, $b_1 = 1.0$, $q_1 = 1.0$, $r_1 = 0.2$, $a_2 = 1.0$, $b_2 = 1.0$, $q_2 = 1.0$, $r_2 = 0.2$, $\\gamma = 0.0$, $x_{10} = 0.5$, $x_{20} = 0.5$.\n- Test Case $3$: $N = 1$, $a_1 = 1.1$, $b_1 = 1.0$, $q_1 = 1.0$, $r_1 = 1.0$, $a_2 = 0.8$, $b_2 = 1.1$, $q_2 = 1.0$, $r_2 = 1.0$, $\\gamma = 0.5$, $x_{10} = 2.0$, $x_{20} = -3.0$.\n- Test Case $4$: $N = 5$, $a_1 = 1.2$, $b_1 = 1.0$, $q_1 = 0.5$, $r_1 = 0.5$, $a_2 = 1.05$, $b_2 = 0.8$, $q_2 = 1.5$, $r_2 = 0.3$, $\\gamma = 0.8$, $x_{10} = 1.0$, $x_{20} = 1.5$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4]$), where each entry is the RMS difference for the corresponding test case.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **System Dynamics**: For agent $i \\in \\{1,2\\}$, the discrete-time dynamics are $x_i(k+1) = a_i x_i(k) + b_i u_i(k)$ over the horizon $k \\in \\{0, 1, \\dots, N-1\\}$.\n- **Initial Condition**: $x_i(0) = x_{i0}$.\n- **Stage Cost**: For agent $i$, the cost at time $k$ is $q_i x_i(k)^2 + r_i u_i(k)^2 + \\gamma x_1(k) x_2(k)$, with $q_i \\ge 0$, $r_i > 0$, $\\gamma \\in \\mathbb{R}$.\n- **Total Cost**: $J_i = \\sum_{k=0}^{N-1} \\left(q_i x_i(k)^2 + r_i u_i(k)^2 + \\gamma x_1(k) x_2(k)\\right)$.\n- **Stacked Vectors**:\n    - State: $\\mathbf{x}_i = \\begin{bmatrix}x_i(0) & x_i(1) & \\dots & x_i(N-1)\\end{bmatrix}^\\top \\in \\mathbb{R}^N$.\n    - Control: $\\mathbf{u}_i = \\begin{bmatrix}u_i(0) & u_i(1) & \\dots & u_i(N-1)\\end{bmatrix}^\\top \\in \\mathbb{R}^N$.\n- **Stacked Affine Relation**: $\\mathbf{x}_i = \\boldsymbol{\\alpha}_i + T_i \\mathbf{u}_i$, with component definitions:\n    - $\\left(\\boldsymbol{\\alpha}_i\\right)_k = a_i^k x_{i0}$.\n    - $\\left(T_i\\right)_{k,j} = b_i a_i^{k-1-j}$ if $j \\le k-1$, and $0$ otherwise.\n- **Cost Matrices**: $Q_i = q_i I_N$, $R_i = r_i I_N$, and $W = I_N$, where $I_N$ is the $N \\times N$ identity matrix.\n- **Objectives**:\n    1.  Find Nash equilibrium control sequences $(\\mathbf{u}_1^\\star, \\mathbf{u}_2^\\star)$.\n    2.  Find centralized optimal control sequences $(\\mathbf{u}_1^{\\mathrm{c}}, \\mathbf{u}_2^{\\mathrm{c}})$ that minimize $J_{\\mathrm{sum}} = J_1 + J_2$.\n    3.  Compute the metric $\\mathrm{RMS} = \\sqrt{\\frac{1}{2N} \\left\\| \\begin{bmatrix} \\mathbf{u}_1^\\star \\\\ \\mathbf{u}_2^\\star \\end{bmatrix} - \\begin{bmatrix} \\mathbf{u}_1^{\\mathrm{c}} \\\\ \\mathbf{u}_2^{\\mathrm{c}} \\end{bmatrix} \\right\\|_2^2 }$.\n- **Test Data**: Four sets of parameters $\\{N, a_i, b_i, q_i, r_i, \\gamma, x_{i0}\\}$ are provided.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n- **Scientifically Grounded**: The problem is firmly rooted in established principles of linear systems theory, optimal control (specifically, Linear Quadratic Regulator theory), and game theory (Nash equilibrium). The setup represents a standard non-cooperative dynamic game.\n- **Well-Posed**: The cost functions $J_i$ are quadratic. The condition $r_i > 0$ ensures that $J_i$ is strictly convex with respect to $\\mathbf{u}_i$ (holding $\\mathbf{u}_j$ fixed, $j \\neq i$). This guarantees the existence and uniqueness of each agent's best response, and consequently, a unique Nash equilibrium. Similarly, the summed cost $J_{\\mathrm{sum}}$ is strictly convex in the joint control vector $(\\mathbf{u}_1, \\mathbf{u}_2)$, guaranteeing a unique centralized optimum. The problem is therefore well-posed.\n- **Objective**: The problem is stated in precise mathematical language, free of ambiguity or subjective claims. All terms are formally defined.\n- The setup is **complete and consistent**. All required parameters and definitions are provided for solving the problem. The provided stacked system representation is a standard derivation from the scalar dynamics. The parameters in the test cases are physically reasonable within the abstract context of the problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined and solvable problem in distributed control. I will now proceed with the solution.\n\n## Principle-Based Design\n\nThe solution requires deriving and solving two distinct optimization problems: the non-cooperative Nash game and the cooperative (or centralized) social optimum. Both solutions are found by identifying the stationary points of the respective cost functions.\n\n### 1. Vectorized Cost Functions\nFirst, we express the total costs $J_1$ and $J_2$ in matrix-vector form using the provided stacked representations. The sum $\\sum_{k=0}^{N-1} y(k) z(k)$ is equivalent to the dot product $\\mathbf{y}^\\top \\mathbf{z}$.\nThe cost functions for agent $1$ and agent $2$ are:\n$$\nJ_1(\\mathbf{u}_1, \\mathbf{u}_2) = \\mathbf{x}_1^\\top Q_1 \\mathbf{x}_1 + \\mathbf{u}_1^\\top R_1 \\mathbf{u}_1 + \\gamma \\mathbf{x}_1^\\top W \\mathbf{x}_2\n$$\n$$\nJ_2(\\mathbf{u}_1, \\mathbf{u}_2) = \\mathbf{x}_2^\\top Q_2 \\mathbf{x}_2 + \\mathbf{u}_2^\\top R_2 \\mathbf{u}_2 + \\gamma \\mathbf{x}_1^\\top W \\mathbf{x}_2\n$$\nSubstituting the state-control relationship $\\mathbf{x}_i = \\boldsymbol{\\alpha}_i + T_i \\mathbf{u}_i$:\n$$\nJ_1(\\mathbf{u}_1, \\mathbf{u}_2) = (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1)^\\top Q_1 (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1) + \\mathbf{u}_1^\\top R_1 \\mathbf{u}_1 + \\gamma (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1)^\\top W (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2)\n$$\n$$\nJ_2(\\mathbf{u}_1, \\mathbf{u}_2) = (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2)^\\top Q_2 (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2) + \\mathbf{u}_2^\\top R_2 \\mathbf{u}_2 + \\gamma (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1)^\\top W (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2)\n$$\n\n### 2. Nash Equilibrium Solution\nThe Nash equilibrium $(\\mathbf{u}_1^\\star, \\mathbf{u}_2^\\star)$ is a pair of strategies where each agent's control input is a best response to the other's, meaning neither can improve its cost by unilaterally changing its strategy. This is found by solving the coupled first-order necessary conditions for optimality:\n$$\n\\nabla_{\\mathbf{u}_1} J_1(\\mathbf{u}_1^\\star, \\mathbf{u}_2^\\star) = \\mathbf{0} \\quad \\text{and} \\quad \\nabla_{\\mathbf{u}_2} J_2(\\mathbf{u}_1^\\star, \\mathbf{u}_2^\\star) = \\mathbf{0}\n$$\nUsing standard rules for vector calculus (e.g., $\\nabla_{\\mathbf{z}} (\\mathbf{b}^\\top A \\mathbf{z}) = A^\\top \\mathbf{b}$ and $\\nabla_{\\mathbf{z}} (\\mathbf{z}^\\top A \\mathbf{z}) = 2 A \\mathbf{z}$ for symmetric $A$), we compute the gradients.\n\nFor agent $1$:\n$$\n\\nabla_{\\mathbf{u}_1} J_1 = 2 T_1^\\top Q_1 (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1) + 2 R_1 \\mathbf{u}_1 + \\gamma T_1^\\top W (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2) = \\mathbf{0}\n$$\nRearranging terms:\n$$\n2(T_1^\\top Q_1 T_1 + R_1)\\mathbf{u}_1 + \\gamma T_1^\\top W T_2 \\mathbf{u}_2 = -2 T_1^\\top Q_1 \\boldsymbol{\\alpha}_1 - \\gamma T_1^\\top W \\boldsymbol{\\alpha}_2\n$$\n\nFor agent $2$:\n$$\n\\nabla_{\\mathbf{u}_2} J_2 = 2 T_2^\\top Q_2 (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2) + 2 R_2 \\mathbf{u}_2 + \\gamma T_2^\\top W^\\top (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1) = \\mathbf{0}\n$$\nRearranging terms (and using $W=W^\\top$ since $W=I_N$):\n$$\n\\gamma T_2^\\top W T_1 \\mathbf{u}_1 + 2(T_2^\\top Q_2 T_2 + R_2)\\mathbf{u}_2 = -2 T_2^\\top Q_2 \\boldsymbol{\\alpha}_2 - \\gamma T_2^\\top W \\boldsymbol{\\alpha}_1\n$$\n\nThese two equations form a $2N \\times 2N$ block linear system for the concatenated vector $\\mathbf{u}_{Nash} = \\begin{bmatrix} \\mathbf{u}_1^\\star \\\\ \\mathbf{u}_2^\\star \\end{bmatrix}$:\n$$\n\\underbrace{\n\\begin{bmatrix}\n2(T_1^\\top Q_1 T_1 + R_1) & \\gamma T_1^\\top W T_2 \\\\\n\\gamma T_2^\\top W T_1 & 2(T_2^\\top Q_2 T_2 + R_2)\n\\end{bmatrix}\n}_{M_{Nash}}\n\\begin{bmatrix} \\mathbf{u}_1^\\star \\\\ \\mathbf{u}_2^\\star \\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n-2 T_1^\\top Q_1 \\boldsymbol{\\alpha}_1 - \\gamma T_1^\\top W \\boldsymbol{\\alpha}_2 \\\\\n-2 T_2^\\top Q_2 \\boldsymbol{\\alpha}_2 - \\gamma T_2^\\top W \\boldsymbol{\\alpha}_1\n\\end{bmatrix}\n}_{c_{Nash}}\n$$\n\n### 3. Centralized Optimum Solution\nThe centralized optimum $(\\mathbf{u}_1^{\\mathrm{c}}, \\mathbf{u}_2^{\\mathrm{c}})$ minimizes the sum of the agents' costs, $J_{\\mathrm{sum}} = J_1 + J_2$. This is a standard linear-quadratic optimal control problem.\n$$\nJ_{\\mathrm{sum}} = \\mathbf{x}_1^\\top Q_1 \\mathbf{x}_1 + \\mathbf{u}_1^\\top R_1 \\mathbf{u}_1 + \\mathbf{x}_2^\\top Q_2 \\mathbf{x}_2 + \\mathbf{u}_2^\\top R_2 \\mathbf{u}_2 + 2\\gamma \\mathbf{x}_1^\\top W \\mathbf{x}_2\n$$\nThe optimal solution is found from the stationarity condition $\\nabla_{\\mathbf{u}} J_{\\mathrm{sum}} = \\mathbf{0}$, where $\\mathbf{u} = \\begin{bmatrix} \\mathbf{u}_1 \\\\ \\mathbf{u}_2 \\end{bmatrix}$. This is equivalent to solving $\\nabla_{\\mathbf{u}_1} J_{\\mathrm{sum}} = \\mathbf{0}$ and $\\nabla_{\\mathbf{u}_2} J_{\\mathrm{sum}} = \\mathbf{0}$ simultaneously.\n\nGradient with respect to $\\mathbf{u}_1$:\n$$\n\\nabla_{\\mathbf{u}_1} J_{\\mathrm{sum}} = 2 T_1^\\top Q_1 (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1) + 2 R_1 \\mathbf{u}_1 + 2\\gamma T_1^\\top W (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2) = \\mathbf{0}\n$$\nDividing by $2$ and rearranging:\n$$\n(T_1^\\top Q_1 T_1 + R_1)\\mathbf{u}_1 + \\gamma T_1^\\top W T_2 \\mathbf{u}_2 = - T_1^\\top Q_1 \\boldsymbol{\\alpha}_1 - \\gamma T_1^\\top W \\boldsymbol{\\alpha}_2\n$$\n\nGradient with respect to $\\mathbf{u}_2$:\n$$\n\\nabla_{\\mathbf{u}_2} J_{\\mathrm{sum}} = 2 T_2^\\top Q_2 (\\boldsymbol{\\alpha}_2 + T_2 \\mathbf{u}_2) + 2 R_2 \\mathbf{u}_2 + 2\\gamma T_2^\\top W^\\top (\\boldsymbol{\\alpha}_1 + T_1 \\mathbf{u}_1) = \\mathbf{0}\n$$\nDividing by $2$ and rearranging:\n$$\n\\gamma T_2^\\top W T_1 \\mathbf{u}_1 + (T_2^\\top Q_2 T_2 + R_2)\\mathbf{u}_2 = - T_2^\\top Q_2 \\boldsymbol{\\alpha}_2 - \\gamma T_2^\\top W \\boldsymbol{\\alpha}_1\n$$\n\nThis yields another $2N \\times 2N$ block linear system for $\\mathbf{u}_{Cent} = \\begin{bmatrix} \\mathbf{u}_1^{\\mathrm{c}} \\\\ \\mathbf{u}_2^{\\mathrm{c}} \\end{bmatrix}$:\n$$\n\\underbrace{\n\\begin{bmatrix}\nT_1^\\top Q_1 T_1 + R_1 & \\gamma T_1^\\top W T_2 \\\\\n\\gamma T_2^\\top W T_1 & T_2^\\top Q_2 T_2 + R_2\n\\end{bmatrix}\n}_{M_{Cent}}\n\\begin{bmatrix} \\mathbf{u}_1^{\\mathrm{c}} \\\\ \\mathbf{u}_2^{\\mathrm{c}} \\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n-T_1^\\top Q_1 \\boldsymbol{\\alpha}_1 - \\gamma T_1^\\top W \\boldsymbol{\\alpha}_2 \\\\\n-T_2^\\top Q_2 \\boldsymbol{\\alpha}_2 - \\gamma T_2^\\top W \\boldsymbol{\\alpha}_1\n\\end{bmatrix}\n}_{c_{Cent}}\n$$\n\n### 4. Computational Procedure\nFor each test case, the algorithm is as follows:\n1.  Construct the model-dependent matrices $T_1, T_2$ and vectors $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2$ from the given parameters ($a_i, b_i, x_{i0}, N$).\n2.  Construct the cost-dependent matrices $Q_1, R_1, Q_2, R_2, W$.\n3.  Assemble the block matrix $M_{Nash}$ and vector $c_{Nash}$. Solve the linear system $M_{Nash}\\mathbf{u}_{Nash} = c_{Nash}$ to find the Nash equilibrium controls $\\mathbf{u}^\\star_1, \\mathbf{u}^\\star_2$.\n4.  Assemble the block matrix $M_{Cent}$ and vector $c_{Cent}$. Solve the linear system $M_{Cent}\\mathbf{u}_{Cent} = c_{Cent}$ to find the centralized optimal controls $\\mathbf{u}^{\\mathrm{c}}_1, \\mathbf{u}^{\\mathrm{c}}_2$.\n5.  Compute the RMS difference using the formula provided. The factor of $2$ in the Nash case 'inflates' the effect of the agents' own costs relative to the coupling term, driving the Nash solution away from the social optimum. This difference, captured by the RMS metric, is often called the \"price of anarchy\".",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_rms(params):\n    \"\"\"\n    Constructs and solves the Nash and Centralized systems for a given set of parameters.\n    \"\"\"\n    N, a1, b1, q1, r1, a2, b2, q2, r2, gamma, x10, x20 = params\n\n    # 1. Construct model-dependent matrices and vectors\n    # Q, R, W matrices\n    Q1 = q1 * np.identity(N)\n    R1 = r1 * np.identity(N)\n    Q2 = q2 * np.identity(N)\n    R2 = r2 * np.identity(N)\n    W = np.identity(N)\n\n    # alpha vectors\n    k_vals = np.arange(N)\n    alpha1 = (a1 ** k_vals) * x10\n    alpha2 = (a2 ** k_vals) * x20\n\n    # T matrices (lower triangular)\n    T1 = np.zeros((N, N))\n    T2 = np.zeros((N, N))\n    for k in range(1, N):\n        for j in range(k):\n            T1[k, j] = b1 * (a1 ** (k - 1 - j))\n            T2[k, j] = b2 * (a2 ** (k - 1 - j))\n\n    # 2. Solve for Nash Equilibrium\n    # Construct block matrix M_Nash and vector c_Nash\n    H1 = T1.T @ Q1 @ T1 + R1\n    H2 = T2.T @ Q2 @ T2 + R2\n    \n    M_Nash_11 = 2 * H1\n    M_Nash_12 = gamma * T1.T @ W @ T2\n    M_Nash_21 = gamma * T2.T @ W @ T1\n    M_Nash_22 = 2 * H2\n    M_Nash = np.block([[M_Nash_11, M_Nash_12], [M_Nash_21, M_Nash_22]])\n\n    c_Nash_1 = -2 * T1.T @ Q1 @ alpha1 - gamma * T1.T @ W @ alpha2\n    c_Nash_2 = -2 * T2.T @ Q2 @ alpha2 - gamma * T2.T @ W @ alpha1\n    c_Nash = np.concatenate([c_Nash_1, c_Nash_2])\n\n    u_nash = np.linalg.solve(M_Nash, c_Nash)\n    \n    # 3. Solve for Centralized Optimum\n    # Construct block matrix M_Cent and vector c_Cent\n    M_Cent_11 = H1\n    M_Cent_12 = M_Nash_12 # Same off-diagonal blocks\n    M_Cent_21 = M_Nash_21\n    M_Cent_22 = H2\n    M_Cent = np.block([[M_Cent_11, M_Cent_12], [M_Cent_21, M_Cent_22]])\n    \n    c_Cent_1 = -T1.T @ Q1 @ alpha1 - gamma * T1.T @ W @ alpha2\n    c_Cent_2 = -T2.T @ Q2 @ alpha2 - gamma * T2.T @ W @ alpha1\n    c_Cent = np.concatenate([c_Cent_1, c_Cent_2])\n\n    u_cent = np.linalg.solve(M_Cent, c_Cent)\n\n    # 4. Compute RMS difference\n    # The norm squared of the difference vector\n    diff_norm_sq = np.sum((u_nash - u_cent) ** 2)\n    # The RMS value\n    rms = np.sqrt(diff_norm_sq / (2 * N))\n    \n    return rms\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, a1, b1, q1, r1, a2, b2, q2, r2, gamma, x10, x20)\n        (3, 1.0, 1.0, 1.0, 0.1, 0.9, 1.2, 1.0, 0.1, 0.5, 1.0, -1.0),\n        (4, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.2, 0.0, 0.5, 0.5),\n        (1, 1.1, 1.0, 1.0, 1.0, 0.8, 1.1, 1.0, 1.0, 0.5, 2.0, -3.0),\n        (5, 1.2, 1.0, 0.5, 0.5, 1.05, 0.8, 1.5, 0.3, 0.8, 1.0, 1.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_rms(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Ensuring safety is a paramount concern in the design of cyber-physical systems, from autonomous vehicles to robotic teams. Control Barrier Functions (CBFs) provide a formal way to guarantee that a system remains within a predefined safe region of its state space. This practice  provides hands-on experience with integrating CBF-based constraints into a DMPC framework, demonstrating how to design controllers that proactively avoid unsafe states while still achieving performance goals.",
            "id": "4218496",
            "problem": "A cyber-physical system consists of two agents, each modeled as a one-dimensional discrete-time linear dynamical system. Each agent obeys the fundamental update equation of discrete-time integrators, which follows from sampling a continuous-time velocity control law with unit sampling time: $$x_i[k+1] = x_i[k] + u_i[k],$$ where $x_i[k] \\in \\mathbb{R}$ is the position of agent $i \\in \\{1,2\\}$ at discrete time index $k$, and $u_i[k] \\in \\mathbb{R}$ is the control input (velocity command). The two agents are coordinated through Distributed Model Predictive Control (DMPC), where each agent $i$ solves a local optimization problem over a horizon of length $N$ using the latest predicted trajectory of the other agent $j \\neq i$.\n\nSafety is encoded by a Control Barrier Function (CBF) $h_i(x_i) = x_{b,i} - x_i$, defining the safe set $$\\mathcal{C}_i = \\{x_i \\in \\mathbb{R} \\mid h_i(x_i) \\ge 0\\} = \\{x_i \\in \\mathbb{R} \\mid x_i \\le x_{b,i}\\}.$$ Each agent must remain within its safe set across the entire prediction horizon. To enforce safety, incorporate barrier constraints $h_i(x_i[k]) \\ge \\epsilon$ for all $k$ in the local optimization (with $\\epsilon > 0$ a small margin to avoid degenerate log-barrier terms), and add a logarithmic barrier term $-\\mu \\sum_{k=0}^{N-1} \\log(h_i(x_i[k]))$ to the objective to discourage approaching the boundary.\n\nEach agent $i$ has a local stage cost that encodes reference tracking, actuator regularization, and formation-keeping relative to the other agent’s predicted position. For a horizon length $N$, reference $x_{\\mathrm{ref},i} \\in \\mathbb{R}$, weights $q_i > 0$, $r_i > 0$, coupling weight $c > 0$, desired separation $s \\in \\mathbb{R}$, and barrier parameter $\\mu > 0$, define the local cost for agent $i$ as\n$$J_i = \\sum_{k=0}^{N-1} \\left(q_i \\left(x_i[k] - x_{\\mathrm{ref},i}\\right)^2 + r_i \\, u_i[k]^2 + c \\left(\\left(x_i[k] - x_j[k]\\right) - s\\right)^2 \\right) - \\mu \\sum_{k=0}^{N-1} \\log\\left(x_{b,i} - x_i[k]\\right),$$\nsubject to the dynamics $x_i[k+1] = x_i[k] + u_i[k]$, input bounds $u_{\\min,i} \\le u_i[k] \\le u_{\\max,i}$, and barrier constraints $x_{b,i} - x_i[k] \\ge \\epsilon$ for all $k \\in \\{0,\\dots,N-1\\}$. In distributed operation, $x_j[k]$ is taken as the most recent predicted trajectory of agent $j$ from the previous DMPC iteration, and the agents solve their local problems alternately for a fixed number of iterations. The initial predicted neighbor trajectory can be the static trajectory induced by zero input.\n\nYour task is to implement a DMPC controller that, for each provided test case, computes a feasible control sequence for both agents using the above formulation with barrier constraints, performs a fixed number of alternating local optimizations, and then verifies that the computed predicted trajectories satisfy $h_i(x_i[k]) \\ge 0$ for all $k \\in \\{1,\\dots,N\\}$ and $i \\in \\{1,2\\}$. The verification should be based on the predicted trajectories after the final DMPC iteration. The final output for each test case is a boolean indicating whether all barrier constraints $h_i(x_i[k]) \\ge 0$ are satisfied across the horizon for both agents.\n\nAngles are not involved, and no physical units are required; the problem is purely in normalized units.\n\nImplement the algorithm in a single, complete, runnable program and produce the aggregate results for all test cases on one line as a comma-separated list enclosed in square brackets.\n\nUse the following test suite:\n\n- Test Case 1 (nominal tracking within safe region):\n    - $N = 5$\n    - Initial states: $x_1[0] = 0.20$, $x_2[0] = 0.10$\n    - Safety boundaries: $x_{b,1} = 1.00$, $x_{b,2} = 1.00$\n    - References: $x_{\\mathrm{ref},1} = 0.80$, $x_{\\mathrm{ref},2} = 0.75$\n    - Weights: $q_1 = q_2 = 1.0$, $r_1 = r_2 = 0.1$, $c = 0.5$\n    - Desired separation: $s = 0.10$\n    - Barrier parameters: $\\mu = 10^{-3}$, $\\epsilon = 10^{-4}$\n    - Input bounds: $u_{\\min,1} = u_{\\min,2} = -0.30$, $u_{\\max,1} = u_{\\max,2} = 0.30$\n    - DMPC iterations: $3$\n\n- Test Case 2 (near-boundary aggressive references, safety-critical):\n    - $N = 6$\n    - Initial states: $x_1[0] = 0.95$, $x_2[0] = 0.92$\n    - Safety boundaries: $x_{b,1} = 1.00$, $x_{b,2} = 1.00$\n    - References: $x_{\\mathrm{ref},1} = 1.10$, $x_{\\mathrm{ref},2} = 1.05$\n    - Weights: $q_1 = q_2 = 1.0$, $r_1 = r_2 = 0.1$, $c = 0.5$\n    - Desired separation: $s = 0.05$\n    - Barrier parameters: $\\mu = 5 \\times 10^{-3}$, $\\epsilon = 10^{-3}$\n    - Input bounds: $u_{\\min,1} = u_{\\min,2} = -0.20$, $u_{\\max,1} = u_{\\max,2} = 0.20$\n    - DMPC iterations: $4$\n\n- Test Case 3 (edge case extremely close to boundary; retreat if needed):\n    - $N = 4$\n    - Initial states: $x_1[0] = 0.9990$, $x_2[0] = 0.9995$\n    - Safety boundaries: $x_{b,1} = 1.0000$, $x_{b,2} = 1.0000$\n    - References: $x_{\\mathrm{ref},1} = 1.0000$, $x_{\\mathrm{ref},2} = 1.0000$\n    - Weights: $q_1 = q_2 = 1.0$, $r_1 = r_2 = 0.05$, $c = 0.2$\n    - Desired separation: $s = 0.00$\n    - Barrier parameters: $\\mu = 10^{-2}$, $\\epsilon = 10^{-4}$\n    - Input bounds: $u_{\\min,1} = u_{\\min,2} = -0.05$, $u_{\\max,1} = u_{\\max,2} = 0.05$\n    - DMPC iterations: $5$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean indicating whether all constraints $h_i(x_i[k]) \\ge 0$ are satisfied across the horizon for both agents in the corresponding test case.",
            "solution": "The user-provided problem has been analyzed and is deemed valid. It is scientifically grounded in the principles of control theory, specifically Distributed Model Predictive Control (DMPC) for linear systems with state constraints. The problem is well-posed, self-contained, and all necessary parameters for a numerical solution are provided. The task requires the implementation of an iterative optimization algorithm to find control sequences for two coupled agents and to verify the safety of the resulting trajectories.\n\n### 1. Problem Formulation and System Dynamics\n\nThe system consists of two agents, $i \\in \\{1, 2\\}$, each described by a discrete-time single-integrator model:\n$$x_i[k+1] = x_i[k] + u_i[k]$$\nwhere $x_i[k] \\in \\mathbb{R}$ is the state (position) and $u_i[k] \\in \\mathbb{R}$ is the control input at time step $k$. The goal is to design a DMPC strategy where each agent $i$ computes its control sequence $\\mathbf{u}_i = \\{u_i[0], \\dots, u_i[N-1]\\}$ over a prediction horizon of length $N$ by solving a local, constrained optimization problem.\n\n### 2. Local Optimization Problem\n\nFor each agent $i$, the optimization problem is to minimize a local cost function $J_i$ subject to system dynamics and constraints. The decision variables are the control inputs over the horizon, $\\mathbf{u}_i$. The state trajectory $\\mathbf{x}_i = \\{x_i[0], \\dots, x_i[N]\\}$ is determined by the initial state $x_i[0]$ and the control sequence $\\mathbf{u}_i$ through the relation:\n$$x_i[k] = x_i[0] + \\sum_{m=0}^{k-1} u_i[m] \\quad \\text{for } k \\in \\{1, \\dots, N\\}$$\n\nThe local cost function $J_i$ for agent $i$ is given by:\n$$J_i(\\mathbf{u}_i) = \\sum_{k=0}^{N-1} \\left[ q_i (x_i[k] - x_{\\mathrm{ref},i})^2 + r_i u_i[k]^2 + c ((x_i[k] - x_j[k]) - s)^2 \\right] - \\mu \\sum_{k=0}^{N-1} \\log(x_{b,i} - x_i[k])$$\nThis cost function consists of four terms:\n1.  **Reference Tracking**: A quadratic penalty $q_i (x_i[k] - x_{\\mathrm{ref},i})^2$ drives the agent's state towards its reference $x_{\\mathrm{ref},i}$.\n2.  **Control Effort Regularization**: A quadratic penalty $r_i u_i[k]^2$ penalizes large control actions.\n3.  **Formation Keeping**: A quadratic term $c ((x_i[k] - x_j[k]) - s)^2$ penalizes deviations from a desired separation $s$ relative to the other agent $j$. In the DMPC scheme, the trajectory of the other agent, $\\mathbf{x}_j$, is treated as a fixed parameter based on its most recent prediction.\n4.  **Logarithmic Barrier**: The term $-\\mu \\sum \\log(h_i(x_i[k]))$, where $h_i(x_i) = x_{b,i} - x_i$, creates a large penalty for states approaching the safety boundary $x_{b,i}$, effectively keeping the trajectory within the interior of the safe set.\n\nThe optimization is subject to the following constraints for $k \\in \\{0, \\dots, N-1\\}$ for the inputs and $k \\in \\{1, \\dots, N\\}$ for the states:\n1.  **Input Bounds**: $u_{\\min,i} \\le u_i[k] \\le u_{\\max,i}$. These are simple box constraints on the decision variables.\n2.  **State Constraints**: $x_i[k] \\le x_{b,i} - \\epsilon$. This enforces that the predicted state trajectory remains strictly within the safe set, with a margin $\\epsilon > 0$. Since $x_i[k]$ is a linear function of $\\mathbf{u}_i$, these are linear inequality constraints on the decision variables.\n\nThe cost function $J_i$ is a sum of quadratic terms and a negative logarithmic term. Since quadratic functions are convex and the negative logarithm is convex, the objective function is convex. The constraints are linear. Therefore, each local optimization problem is a convex program, which guarantees that a unique optimal solution can be found efficiently by numerical solvers.\n\n### 3. Distributed Model Predictive Control (DMPC) Algorithm\n\nThe DMPC scheme operates iteratively, with agents solving their local problems in an alternating (Gauss-Seidel) fashion. The algorithm is as follows:\n\n1.  **Initialization**: For each agent $i$, initialize its predicted trajectory. A static trajectory is assumed, where $u_i[k] = 0$ for all $k$, resulting in $x_i^{\\text{pred}}[k] = x_i[0]$ for all $k \\in \\{0, \\dots, N\\}$.\n2.  **Iterative Optimization**: For a fixed number of DMPC iterations:\n    a.  **Solve for Agent 1**: Agent 1 solves its local optimization problem to find its optimal control sequence $\\mathbf{u}_1^*$. This optimization uses agent 2's current predicted trajectory $\\mathbf{x}_2^{\\text{pred}}$. Agent 1's predicted trajectory $\\mathbf{x}_1^{\\text{pred}}$ is then updated using $\\mathbf{u}_1^*$.\n    b.  **Solve for Agent 2**: Agent 2 solves its local optimization problem to find its optimal control sequence $\\mathbf{u}_2^*$. This optimization uses agent 1's newly updated predicted trajectory $\\mathbf{x}_1^{\\text{pred}}$. Agent 2's predicted trajectory $\\mathbf{x}_2^{\\text{pred}}$ is then updated using $\\mathbf{u}_2^*$.\n3.  **Termination**: After the specified number of iterations, the final predicted trajectories for both agents are obtained.\n\n### 4. Numerical Implementation and Safety Verification\n\nThe numerical solution is implemented in Python using the `scipy.optimize.minimize` function with the 'SLSQP' (Sequential Least Squares Programming) method, which is well-suited for non-linearly constrained optimization problems.\n\n-   **Objective Function**: A Python function is defined to compute $J_i$ given a control vector $\\mathbf{u}_i$. This function first computes the corresponding state trajectory $\\mathbf{x}_i$ and then evaluates the sum of the cost terms. A safeguard is included to return infinity if any argument to a logarithm is non-positive, guiding the solver away from infeasible regions.\n-   **Constraints**: The input bounds are passed directly to the `bounds` parameter of the solver. The state constraints $x_i[k] \\le x_{b,i} - \\epsilon$ are formulated as functions of $\\mathbf{u}_i$ and provided to the `constraints` parameter. For each $k \\in \\{1, \\dots, N\\}$, a constraint function $g_k(\\mathbf{u}_i) = (x_{b,i} - x_i[k]) - \\epsilon \\ge 0$ is created.\n-   **Final Verification**: After the DMPC iterations complete, the final predicted trajectories $\\mathbf{x}_1^{\\text{pred}}$ and $\\mathbf{x}_2^{\\text{pred}}$ are checked against the fundamental safety condition $h_i(x_i[k]) \\ge 0$, which is equivalent to $x_i[k] \\le x_{b,i}$. The check is performed for each agent $i$ and for all steps in the predicted trajectory $k \\in \\{1, \\dots, N\\}$. The final output for a test case is `True` if and only if this condition holds for both agents across their entire predicted trajectories.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the DMPC problem.\n    \"\"\"\n    test_cases = [\n        { # Test Case 1\n            \"N\": 5, \"x0\": (0.20, 0.10), \"xb\": (1.00, 1.00),\n            \"xref\": (0.80, 0.75), \"q\": (1.0, 1.0), \"r\": (0.1, 0.1),\n            \"c\": 0.5, \"s\": 0.10, \"mu\": 1e-3, \"epsilon\": 1e-4,\n            \"umin\": (-0.30, -0.30), \"umax\": (0.30, 0.30),\n            \"dmpc_iterations\": 3\n        },\n        { # Test Case 2\n            \"N\": 6, \"x0\": (0.95, 0.92), \"xb\": (1.00, 1.00),\n            \"xref\": (1.10, 1.05), \"q\": (1.0, 1.0), \"r\": (0.1, 0.1),\n            \"c\": 0.5, \"s\": 0.05, \"mu\": 5e-3, \"epsilon\": 1e-3,\n            \"umin\": (-0.20, -0.20), \"umax\": (0.20, 0.20),\n            \"dmpc_iterations\": 4\n        },\n        { # Test Case 3\n            \"N\": 4, \"x0\": (0.9990, 0.9995), \"xb\": (1.0000, 1.0000),\n            \"xref\": (1.0000, 1.0000), \"q\": (1.0, 1.0), \"r\": (0.05, 0.05),\n            \"c\": 0.2, \"s\": 0.00, \"mu\": 1e-2, \"epsilon\": 1e-4,\n            \"umin\": (-0.05, -0.05), \"umax\": (0.05, 0.05),\n            \"dmpc_iterations\": 5\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        is_safe = run_dmpc_for_case(params)\n        results.append(str(is_safe).lower())\n\n    print(f\"[{','.join(results)}]\")\n\ndef compute_trajectory(x0, u_vec):\n    \"\"\"\n    Computes the state trajectory from an initial state and control sequence.\n    \"\"\"\n    N = len(u_vec)\n    x_traj = np.zeros(N + 1)\n    x_traj[0] = x0\n    for k in range(N):\n        x_traj[k+1] = x_traj[k] + u_vec[k]\n    return x_traj\n\ndef optimize_agent_trajectory(agent_idx, params, x0, neighbor_traj):\n    \"\"\"\n    Solves the local optimization problem for a single agent.\n    \"\"\"\n    N = params['N']\n    q = params['q'][agent_idx]\n    r = params['r'][agent_idx]\n    c = params['c']\n    s = params['s']\n    mu = params['mu']\n    epsilon = params['epsilon']\n    xb = params['xb'][agent_idx]\n    xref = params['xref'][agent_idx]\n    umin = params['umin'][agent_idx]\n    umax = params['umax'][agent_idx]\n\n    def objective(u_vec):\n        x_traj = compute_trajectory(x0, u_vec)\n        \n        # Barrier term check\n        h_vals = xb - x_traj[:N]\n        if np.any(h_vals = 0):\n            return np.inf\n\n        cost_tracking = q * np.sum((x_traj[:N] - xref)**2)\n        cost_actuation = r * np.sum(u_vec**2)\n        cost_coupling = c * np.sum(((x_traj[:N] - neighbor_traj[:N]) - s)**2)\n        cost_barrier = -mu * np.sum(np.log(h_vals))\n\n        return cost_tracking + cost_actuation + cost_coupling + cost_barrier\n\n    constraints = []\n    for k in range(1, N + 1):\n        def constr_func(u_vec, k_val=k):\n            x_k = x0 + np.sum(u_vec[:k_val])\n            return xb - x_k - epsilon\n        constraints.append({'type': 'ineq', 'fun': constr_func})\n    \n    bounds = [(umin, umax)] * N\n    u_initial_guess = np.zeros(N)\n\n    result = minimize(objective, u_initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n    return result\n\ndef run_dmpc_for_case(params):\n    \"\"\"\n    Runs the DMPC simulation for a single test case.\n    \"\"\"\n    N = params['N']\n    x1_0, x2_0 = params['x0']\n    xb1, xb2 = params['xb']\n\n    # Initial static trajectories (zero control input)\n    u1_pred = np.zeros(N)\n    u2_pred = np.zeros(N)\n    x1_pred = compute_trajectory(x1_0, u1_pred)\n    x2_pred = compute_trajectory(x2_0, u2_pred)\n    \n    # DMPC iterations\n    for _ in range(params['dmpc_iterations']):\n        # Agent 1 solves its optimization problem\n        res1 = optimize_agent_trajectory(0, params, x1_0, x2_pred)\n        if res1.success:\n            u1_pred = res1.x\n        x1_pred = compute_trajectory(x1_0, u1_pred)\n\n        # Agent 2 solves its optimization problem\n        res2 = optimize_agent_trajectory(1, params, x2_0, x1_pred)\n        if res2.success:\n            u2_pred = res2.x\n        x2_pred = compute_trajectory(x2_0, u2_pred)\n\n    # Final safety verification h_i(x_i[k]) >= 0 for k=1..N\n    # This is equivalent to x_i[k] = x_{b,i}\n    is_safe1 = np.all(x1_pred[1:] = xb1)\n    is_safe2 = np.all(x2_pred[1:] = xb2)\n    \n    return is_safe1 and is_safe2\n\nsolve()\n```"
        }
    ]
}