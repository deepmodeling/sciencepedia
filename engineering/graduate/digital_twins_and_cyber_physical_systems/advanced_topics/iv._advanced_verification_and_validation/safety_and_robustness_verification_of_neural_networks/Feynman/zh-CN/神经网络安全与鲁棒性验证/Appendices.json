{
    "hands_on_practices": [
        {
            "introduction": "本首个练习将介绍神经网络验证的基本“精确”方法。通过枚举所有可能的 ReLU 神经元激活模式，我们可以将网络的函数划分为一组不同的线性变换，从而使用线性规划精确计算输出的可达集。这个练习旨在为验证问题提供基础性理解，并突显其计算挑战，进而引出对更具可扩展性方法的需求。",
            "id": "4243524",
            "problem": "考虑一个带有修正线性单元（ReLU）激活函数的双层前馈神经网络，它被用作一个信息物理系统数字孪生中的组件。该网络通过以下变换将输入向量 $x \\in \\mathbb{R}^n$ 映射到一个标量输出 $y \\in \\mathbb{R}$：隐藏层预激活 $z \\in \\mathbb{R}^m$ 由 $z = W_1 x + b_1$ 给出，隐藏层激活 $h \\in \\mathbb{R}^m$ 对所有索引 $i$ 逐元素满足 $h_i = \\max(0, z_i)$，输出为 $y = W_2 h + b_2$。所有运算都在实数上进行。输入集是一个由下界 $l \\in \\mathbb{R}^n$ 和上界 $u \\in \\mathbb{R}^n$ 定义的超矩形（盒子），即 $x$ 逐元素满足 $l \\le x \\le u$。一项安全性规范要求对于盒子内的所有输入，输出都保持在一个给定的闭区间 $[L, U] \\subset \\mathbb{R}$ 内。\n\n从仿射变换、凸多面体和分段线性激活函数的基本定义出发，实现多面体传播算法，以计算网络在输入盒上输出的精确可达集。利用以下特性：对于隐藏层的每个激活模式，变量 $(x, z, h, y)$ 中的可行集是一个由线性等式和不等式定义的凸多面体，并且所有模式的并集覆盖了可达集。通过在每个这样的多面体上优化一个线性目标，来获得 $y$ 的紧密下界和上界。判断可达输出区间是否位于安全区间 $[L, U]$ 内。\n\n您的程序必须：\n- 枚举给定 $m$ 的所有隐藏层激活模式。\n- 对于每个模式，建立线性约束来捕捉 $z = W_1 x + b_1$、该模式下的 ReLU 关系、$y = W_2 h + b_2$ 以及输入盒约束 $l \\le x \\le u$。\n- 为每个可行模式求解两个线性规划问题，以计算该模式多面体上的 $\\min y$ 和 $\\max y$，并跨模式聚合结果，以获得可达集上的全局最小值和最大值。\n\n所有数值输出必须不带物理单位表示。不涉及角度。对于每个测试用例，最终答案必须包含三个输出：$y$ 的全局下界（浮点数）、$y$ 的全局上界（浮点数），以及一个布尔值，指示可达区间 $[y_{\\min}, y_{\\max}]$ 是否包含在 $[L, U]$ 内（即，$y_{\\min} \\ge L$ 且 $y_{\\max} \\le U$）。浮点数必须四舍五入到四位小数并以十进制形式打印。布尔值必须打印为 Python 的规范字面量 $True$ 或 $False$。\n\n测试套件：\n- 案例 1（一般情况）：\n    - 维度：$n = 2$，$m = 3$。\n    - 隐藏层权重：$W_1 = \\begin{bmatrix} 1.0  -2.0 \\\\ 0.5  1.0 \\\\ -1.5  0.5 \\end{bmatrix}$。\n    - 隐藏层偏置：$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$。\n    - 输出层权重：$W_2 = \\begin{bmatrix} 0.8  -0.4  0.6 \\end{bmatrix}$。\n    - 输出偏置：$b_2 = 0.05$。\n    - 输入盒：$l = \\begin{bmatrix} -0.5 \\\\ -0.5 \\end{bmatrix}$，$u = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [-1.0, 1.0]$。\n- 案例 2（更大的输入盒，更紧的安全区间）：\n    - 维度和网络参数与案例 1 相同。\n    - 输入盒：$l = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}$，$u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [-0.3, 0.3]$。\n- 案例 3（退化的输出层产生恒定输出）：\n    - 维度：$n = 2$，$m = 3$。\n    - 隐藏层权重：$W_1 = \\begin{bmatrix} 1.0  -2.0 \\\\ 0.5  1.0 \\\\ -1.5  0.5 \\end{bmatrix}$。\n    - 隐藏层偏置：$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$。\n    - 输出层权重：$W_2 = \\begin{bmatrix} 0.0  0.0  0.0 \\end{bmatrix}$。\n    - 输出偏置：$b_2 = 0.2$。\n    - 输入盒：$l = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}$，$u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [0.2, 0.2]$。\n- 案例 4（混合符号和不对称盒）：\n    - 维度：$n = 2$，$m = 3$。\n    - 隐藏层权重：$W_1 = \\begin{bmatrix} 2.0  -1.0 \\\\ 1.0  1.0 \\\\ -2.0  0.5 \\end{bmatrix}$。\n    - 隐藏层偏置：$b_1 = \\begin{bmatrix} -0.5 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$。\n    - 输出层权重：$W_2 = \\begin{bmatrix} -0.6  0.4  -0.3 \\end{bmatrix}$。\n    - 输出偏置：$b_2 = 0.1$。\n    - 输入盒：$l = \\begin{bmatrix} -0.2 \\\\ -0.2 \\end{bmatrix}$，$u = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [-0.5, 0.0]$。\n\n最终输出格式：\n您的程序应生成单行文本，其中包含一个用方括号括起来的逗号分隔列表。该列表必须是按顺序连接每个测试用例的三个输出，即 $[y_{\\min}^{(1)}, y_{\\max}^{(1)}, \\text{safe}^{(1)}, y_{\\min}^{(2)}, y_{\\max}^{(2)}, \\text{safe}^{(2)}, y_{\\min}^{(3)}, y_{\\max}^{(3)}, \\text{safe}^{(3)}, y_{\\min}^{(4)}, y_{\\max}^{(4)}, \\text{safe}^{(4)}]$，其中每个 $y_{\\min}^{(k)}$ 和 $y_{\\max}^{(k)}$ 四舍五入到四位小数，每个 $\\text{safe}^{(k)}$ 是一个布尔值。不应打印任何其他文本。",
            "solution": "用户要求为一个带有修正线性单元（ReLU）激活函数的双层前馈神经网络实现一个精确可达性分析算法。目标是计算网络标量输出 $y$ 的精确范围 $[y_{\\min}, y_{\\max}]$，给定输入向量 $x$ 被约束在一个超矩形集合内。然后将计算出的输出范围与给定的安全区间 $[L, U]$ 进行比较。指定的方法是多面体传播，它涉及枚举隐藏层所有可能的激活模式并求解一组线性规划（LP）问题。\n\n该问题是有效的，因为它在神经网络形式化验证这一成熟科学领域有坚实的理论基础，数学上是适定的，并且所有必要的参数和约束都已提供。我们基于第一性原理进行求解。\n\n### 基于原理的设计\n\n该问题的核心在于神经网络的分段线性性质。网络函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 由仿射变换和逐元素 ReLU 激活函数 $h_i = \\max(0, z_i)$ 的复合定义。ReLU 函数本身是分段线性的，这一属性延伸到整个网络。\n\n该算法被称为多面体传播或精确方法，它利用了这种结构。输入域，即由 $l \\le x \\le u$ 定义的超矩形，是一个凸多面体。基本原理是，凸多面体在仿射变换下的像也是一个凸多面体。ReLU 激活函数 $h_i = \\max(0, z_i)$ 充当一个“分裂器”：它将其输入 $z_i$ 的定义域划分为两个半空间，$z_i \\ge 0$ 和 $z_i \\le 0$。在每个半空间内，该函数是线性的（$h_i = z_i$ 或 $h_i = 0$）。\n\n因此，我们可以通过基于 $m$ 个隐藏神经元的离散状态来划分问题，从而确定精确的输出集。一个“激活模式”是一个向量 $\\sigma \\in \\{0, 1\\}^m$，它为每个神经元 $i \\in \\{1, \\dots, m\\}$ 指定其是激活的（$\\sigma_i=1$，对应于 $z_i \\ge 0$）还是非激活的（$\\sigma_i=0$，对应于 $z_i \\le 0$）。总共有 $2^m$ 种这样的模式。\n\n对于每个固定的激活模式 $\\sigma$，网络的行为由一组纯粹的线性关系决定。这使我们能够在变量 $(x, z, h)$ 的空间中定义一个凸多面体，该多面体表示与该模式一致的所有可能的网络状态。定义此多面体的约束是：\n1.  **输入盒**：$l \\le x \\le u$。这些是 $2n$ 个线性不等式。\n2.  **第一仿射层**：$z = W_1 x + b_1$。这些是 $m$ 个线性等式。\n3.  **ReLU 激活（依赖于模式）**：对于每个神经元 $i=1, \\dots, m$：\n    *   如果神经元 $i$ 是激活的（$\\sigma_i=1$）：$z_i \\ge 0$（不等式）和 $h_i = z_i$（等式）。\n    *   如果神经元 $i$ 是非激活的（$\\sigma_i=0$）：$z_i \\le 0$（不等式）和 $h_i = 0$（等式）。\n4.  **输出变换**：网络输出为 $y = W_2 h + b_2$。这是变量 $h_i$ 的一个线性函数。\n\n在该多面体上寻找最小和最大输出 $y$ 的任务是一个线性规划（LP）问题。对于 $2^m$ 个激活模式中的每一个，我们构建并求解两个 LP：\n*   **最小化**：$\\min_{x,z,h} (W_2 h + b_2)$，受该模式的线性约束。\n*   **最大化**：$\\max_{x,z,h} (W_2 h + b_2)$，受该模式的线性约束。\n\n这些多面体中有很多可能是空的（不可行），这意味着给定盒子中的任何输入 $x$ 都无法产生该激活模式。LP 求解器将检测到这种不可行性。\n\n网络的整体或全局最小和最大输出，是所有*可行*激活模式中找到的所有最小值中的最小值和所有最大值中的最大值：\n$$y_{\\min} = \\min_{\\sigma \\in \\text{可行模式}} \\left( \\min_{x,z,h} \\{ W_2 h + b_2 \\mid \\text{模式 } \\sigma \\text{ 的约束} \\} \\right)$$\n$$y_{\\max} = \\max_{\\sigma \\in \\text{可行模式}} \\left( \\max_{x,z,h} \\{ W_2 h + b_2 \\mid \\text{模式 } \\sigma \\text{ 的约束} \\} \\right)$$\n\n最后，安全性验证步骤包括检查计算出的可达输出区间 $[y_{\\min}, y_{\\max}]$ 是否完全包含在指定的安全区间 $[L, U]$ 内，这当且仅当 $y_{\\min} \\ge L$ 和 $y_{\\max} \\le U$ 时成立。\n\n### 实现策略\n\n实现将使用 `scipy.optimize.linprog` 来求解线性规划问题。对于每个 $m=3$ 的测试用例，我们有 $n=2$ 个输入，因此有 $2^3 = 8$ 个激活模式需要分析。\n\nLP 的变量将是拼接向量 $[x^T, z^T, h^T]^T$，其维度为 $n+2m = 2+2(3) = 8$。\n\n对于 8 个模式中的每一个：\n1.  我们构建与约束相对应的 LP 矩阵和向量（$A_{ub}, b_{ub}, A_{eq}, b_{eq}$）。\n2.  `linprog` 的目标函数向量 $c$ 源自 $W_2 h$。为了找到 $\\min y$，我们最小化 $c^T \\cdot [x^T, z^T, h^T]^T$。为了找到 $\\max y$，我们最小化 $-c^T \\cdot [x^T, z^T, h^T]^T$。常数偏置 $b_2$ 被加到 LP 的结果上。\n3.  调用 `linprog` 函数。如果它返回成功，表示该模式的多面体是可行的，我们就更新全局最小值和最大值。\n4.  在迭代完所有模式后，将得到的全局界限 $[y_{\\min}, y_{\\max}]$ 与 $[L, U]$ 进行比较以确定安全性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef analyze_case(W1, b1, W2, b2, l, u, L, U):\n    \"\"\"\n    Computes the exact output bounds of a 2-layer ReLU network for a given\n    input box and verifies safety.\n\n    Args:\n        W1 (np.array): Weight matrix of the first layer (m x n).\n        b1 (np.array): Bias vector of the first layer (m x 1).\n        W2 (np.array): Weight matrix of the second layer (1 x m).\n        b2 (float): Bias of the second layer.\n        l (np.array): Lower bounds of the input box (n x 1).\n        u (np.array): Upper bounds of the input box (n x 1).\n        L (float): Lower bound of the safety interval.\n        U (float): Upper bound of the safety interval.\n\n    Returns:\n        tuple: (y_min_str, y_max_str, is_safe_bool)\n    \"\"\"\n    m, n = W1.shape\n    num_vars = n + 2 * m  # Variables are [x, z, h]\n\n    # Objective function vector c for linprog. The objective is to min/max W2*h.\n    # The variables are arranged as [x1..xn, z1..zm, h1..hm].\n    c = np.zeros(num_vars)\n    c[n + m:] = W2.flatten()\n    c_min = c\n    c_max = -c\n\n    # Bounds on variables. x is bounded by l and u. z and h are unbounded here,\n    # their constraints are handled by A_eq and A_ub.\n    bounds = []\n    for i in range(n):\n        bounds.append((l[i], u[i]))\n    for _ in range(2 * m):\n        bounds.append((None, None))\n\n    # Equality constraints from the first layer: z = W1*x + b1 => -W1*x + z = b1\n    # These are common to all activation patterns.\n    A_eq_base = np.zeros((m, num_vars))\n    A_eq_base[:, :n] = -W1\n    A_eq_base[:, n:n + m] = np.eye(m)\n    b_eq_base = b1.flatten()\n\n    global_min_y = np.inf\n    global_max_y = -np.inf\n\n    # Iterate through all 2^m activation patterns for the hidden layer.\n    for i in range(2 ** m):\n        pattern = [(i >> j)  1 for j in range(m)]\n\n        # Build pattern-specific constraints for ReLU.\n        A_eq_pattern = np.zeros((m, num_vars))\n        b_eq_pattern = np.zeros(m)\n        A_ub_pattern = np.zeros((m, num_vars))\n        b_ub_pattern = np.zeros(m)\n\n        for j in range(m):  # neuron index\n            if pattern[j] == 1:  # Active neuron: z_j >= 0, h_j = z_j\n                # z_j >= 0  => -z_j = 0\n                A_ub_pattern[j, n + j] = -1\n                b_ub_pattern[j] = 0\n                # h_j = z_j  => -z_j + h_j = 0\n                A_eq_pattern[j, n + j] = -1\n                A_eq_pattern[j, n + m + j] = 1\n                b_eq_pattern[j] = 0\n            else:  # Inactive neuron: z_j = 0, h_j = 0\n                # z_j = 0\n                A_ub_pattern[j, n + j] = 1\n                b_ub_pattern[j] = 0\n                # h_j = 0\n                A_eq_pattern[j, n + m + j] = 1\n                b_eq_pattern[j] = 0\n\n        # Combine base and pattern-specific constraints\n        A_eq = np.vstack([A_eq_base, A_eq_pattern])\n        b_eq = np.concatenate([b_eq_base, b_eq_pattern])\n        A_ub = A_ub_pattern\n        b_ub = b_ub_pattern\n\n        # Solve for min_y in this polyhedron.\n        res_min = linprog(c=c_min, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n        \n        # If the polyhedron is feasible (LP succeeds)\n        if res_min.success:\n            # Solve for max_y in the same polyhedron.\n            res_max = linprog(c=c_max, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n            \n            # The minimization of -y gives -max_y. Don't forget to add the bias b2.\n            current_min = res_min.fun + b2\n            current_max = -res_max.fun + b2\n\n            # Update global bounds\n            global_min_y = min(global_min_y, current_min)\n            global_max_y = max(global_max_y, current_max)\n\n    # After checking all patterns, verify safety.\n    is_safe = (global_min_y >= L) and (global_max_y = U)\n\n    # Format output as required.\n    y_min_str = f\"{round(global_min_y, 4):.4f}\"\n    y_max_str = f\"{round(global_max_y, 4):.4f}\"\n\n    return y_min_str, y_max_str, is_safe\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.8, -0.4, 0.6]]),\n            \"b2\": 0.05,\n            \"l\": np.array([-0.5, -0.5]),\n            \"u\": np.array([0.5, 0.5]),\n            \"L\": -1.0, \"U\": 1.0\n        },\n        # Case 2 (larger input box, tighter safety)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.8, -0.4, 0.6]]),\n            \"b2\": 0.05,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"L\": -0.3, \"U\": 0.3\n        },\n        # Case 3 (degenerate output layer yielding constant output)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.0, 0.0, 0.0]]),\n            \"b2\": 0.2,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"L\": 0.2, \"U\": 0.2\n        },\n        # Case 4 (mixed signs and asymmetric box)\n        {\n            \"W1\": np.array([[2.0, -1.0], [1.0, 1.0], [-2.0, 0.5]]),\n            \"b1\": np.array([[-0.5], [0.0], [0.2]]),\n            \"W2\": np.array([[-0.6, 0.4, -0.3]]),\n            \"b2\": 0.1,\n            \"l\": np.array([-0.2, -0.2]),\n            \"u\": np.array([0.7, 0.3]),\n            \"L\": -0.5, \"U\": 0.0\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y_min_str, y_max_str, is_safe = analyze_case(\n            case[\"W1\"], case[\"b1\"], case[\"W2\"], case[\"b2\"],\n            case[\"l\"], case[\"u\"], case[\"L\"], case[\"U\"]\n        )\n        all_results.extend([y_min_str, y_max_str, is_safe])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在精确方法面临组合爆炸挑战的基础上，本练习将探索可扩展的过近似技术。您将实现区间边界传播 (IBP) 和一个更精确的类 DeepPoly 抽象域，以计算网络输出的保守边界。这项对比分析将揭示计算效率与分析精度之间的关键权衡，这是现代验证工具中的一个核心主题。",
            "id": "4243461",
            "problem": "给定一个前馈神经网络，该网络具有两个使用修正线性单元（ReLU）激活函数的隐藏层和一个单一的线性输出。此神经网络旨在作为信息物理系统控制器数字孪生（高保真虚拟副本）内部的安全关键组件使用。目标是使用两种过近似方法计算不确定输入下的抽象输出界，并通过检查指定安全属性的误报来评估其保守性。\n\n基本原理和定义：\n- 前馈神经网络定义了一个函数 $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$，该函数由仿射变换 $z = W a + b$ 和非线性激活函数组成。修正线性单元（ReLU）定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$，它是单调非递减的。\n- 对于任何元素级有界的盒式输入域 $X = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$，仿射映射 $x \\mapsto c^\\top x + d$ 下 $X$ 的像具有通过区间算术和在盒上进行符号感知的最大化/最小化得出的界：\n  $$\\max_{x \\in [\\ell, u]} c^\\top x + d = \\sum_{i} \\max(c_i, 0) u_i + \\min(c_i, 0) \\ell_i + d,$$\n  $$\\min_{x \\in [\\ell, u]} c^\\top x + d = \\sum_{i} \\max(c_i, 0) \\ell_i + \\min(c_i, 0) u_i + d.$$\n- 区间界传播（IBP）通过使用符号感知的仿射变换和 $\\mathrm{ReLU}$ 的元素级单调性，逐层计算前向区间界。\n- 一个类DeepPoly的抽象域为每个神经元追踪一个相对于原始输入的下仿射界 $a \\ge \\alpha_L^\\top x + \\beta_L$ 和一个上仿射界 $a \\le \\alpha_U^\\top x + \\beta_U$，并使用 $\\mathrm{ReLU}$ 的可靠线性松弛：\n  - 如果预激活值 $z$ 的区间界为 $[z_L, z_U]$ 且 $z_U \\le 0$，那么 $\\mathrm{ReLU}(z) = 0$ 精确成立。\n  - 如果 $z_L \\ge 0$，那么 $\\mathrm{ReLU}(z) = z$ 精确成立。\n  - 如果 $z_L  0  z_U$，则使用标准凸松弛：上界为 $a \\le \\alpha z + \\beta$，其中 $\\alpha = \\frac{z_U}{z_U - z_L}$ 且 $\\beta = -\\alpha z_L$，下界为 $a \\ge 0$。\n\n安全规范：\n- 安全属性要求网络的标量输出 $y$ 在整个输入盒上位于一个固定的区间 $[s_{\\min}, s_{\\max}]$ 内。如果抽象输出界与 $[s_{\\min}, s_{\\max}]$ 的补集相交，则可能存在安全违规。当一种抽象方法预测可能存在违规，而对输入盒的经验性密集网格评估发现没有任何样本违反 $[s_{\\min}, s_{\\max}]$ 时，就会发生误报。\n\n网络参数：\n- 输入维度为 $n = 2$。每个隐藏层有 $3$ 个神经元。输出维度为 $m = 1$。\n- 第一层权重和偏置：\n  $$W_1 = \\begin{bmatrix} 1.0  -1.0 \\\\ -1.0  1.0 \\\\ 0.2  0.2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.05 \\end{bmatrix}.$$\n- 第二层权重和偏置：\n  $$W_2 = \\begin{bmatrix} 0.6  -0.6  0.2 \\\\ -0.6  0.6  -0.2 \\\\ 0.3  0.3  0.0 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.$$\n- 输出层权重和偏置：\n  $$W_3 = \\begin{bmatrix} 0.5  -0.4  0.3 \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}.$$\n\n任务：\n1. 实现区间界传播（IBP），为给定的输入盒 $[\\ell, u]$ 计算输出区间界 $[y_L^{\\mathrm{IBP}}, y_U^{\\mathrm{IBP}}]$。\n2. 实现一个类DeepPoly的抽象，通过为每个神经元维持关于原始输入的下、上仿射界来计算输出界。使用上述描述的ReLU线性松弛和符号感知的仿射复合。从输出端的这些仿射界，通过盒式最大化/最小化计算输出区间界 $[y_L^{\\mathrm{DP}}, y_U^{\\mathrm{DP}}]$。\n3. 对于每个测试用例，在输入盒上的密集二维网格上评估实际网络输出（在每个维度上均匀间隔，每个维度恰好使用 $21$ 个点，总共 $441$ 个样本），并确定是否有任何样本违反安全区间 $[s_{\\min}, s_{\\max}]$。\n4. 对于每个测试用例和方法，报告是否发生误报。误报定义为一个布尔值，当且仅当方法的抽象界预测可能存在违规（即 $y_L  s_{\\min}$ 或 $y_U > s_{\\max}$），而没有网格样本违反该区间（即所有采样输出都位于 $[s_{\\min}, s_{\\max}]$ 内）时，该值为真。\n\n测试套件：\n- 情况 1：$\\ell = [-0.1, -0.1]$, $u = [0.1, 0.1]$, $s_{\\min} = -0.05$, $s_{\\max} = 0.05$。\n- 情况 2（单点盒）：$\\ell = [0.0, 0.0]$, $u = [0.0, 0.0]$, $s_{\\min} = -0.01$, $s_{\\max} = 0.01$。\n- 情况 3（更大的盒）：$\\ell = [-0.5, -0.5]$, $u = [0.5, 0.5]$, $s_{\\min} = -0.5$, $s_{\\max} = 0.5$。\n- 情况 4（中等大小盒上的更严格安全要求）：$\\ell = [-0.2, -0.2]$, $u = [0.2, 0.2]$, $s_{\\min} = -0.02$, $s_{\\max} = 0.02$。\n\n要求输出：\n- 你的程序应产生单行输出，包含一个用方括号括起来的逗号分隔列表的结果。对于每个测试用例，按顺序先输出区间界传播的误报布尔值，然后是类DeepPoly的误报布尔值。例如，输出格式必须是：\n$$[\\text{IBP\\_case1}, \\text{DP\\_case1}, \\text{IBP\\_case2}, \\text{DP\\_case2}, \\text{IBP\\_case3}, \\text{DP\\_case3}, \\text{IBP\\_case4}, \\text{DP\\_case4}]$$\n其中每个条目都是一个布尔字面量。",
            "solution": "目标是分析给定前馈神经网络在输入不确定性下的安全性。这涉及到使用两种不同的抽象解释技术来计算网络输出范围的过近似：区间界传播（IBP）和类DeepPoly抽象。然后，我们将通过识别误报来比较这些方法的保守性。误报发生在一个方法预测了潜在的安全违规，但在对输入空间进行密集的经验性采样中未观察到这种违规。\n\n该神经网络是一个函数 $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^1$，具有两个隐藏层。其计算过程如下：\n令输入为 $x \\in \\mathbb{R}^2$。\n第一个隐藏层的输出是 $h_1 = \\mathrm{ReLU}(z_1)$，其中预激活值为 $z_1 = W_1 x + b_1$。这里 $h_1, z_1, b_1 \\in \\mathbb{R}^3$ 且 $W_1 \\in \\mathbb{R}^{3 \\times 2}$。\n第二个隐藏层的输出是 $h_2 = \\mathrm{ReLU}(z_2)$，其中预激活值为 $z_2 = W_2 h_1 + b_2$。这里 $h_2, z_2, b_2 \\in \\mathbb{R}^3$ 且 $W_2 \\in \\mathbb{R}^{3 \\times 3}$。\n最终输出是 $y = z_3$，其中 $z_3 = W_3 h_2 + b_3$。这里 $y, b_3 \\in \\mathbb{R}^1$ 且 $W_3 \\in \\mathbb{R}^{1 \\times 3}$。\n\n输入不确定性由一个盒式域 $X = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$ 建模，其中 $\\ell, u \\in \\mathbb{R}^n$ 是按元素的下界和上界。\n\n### 任务1：区间界传播 (IBP)\n\nIBP通过从输入到输出逐层传播区间来计算网络输出的区间界。\n\n令第 $k$ 层激活值的区间界为 $[\\ell^{(k)}, u^{(k)}]$。传播到第 $k+1$ 层的过程包括两个步骤：\n\n1.  **仿射变换**：预激活值计算为 $z^{(k+1)} = W^{(k+1)} a^{(k)} + b^{(k+1)}$。每个分量 $z_i^{(k+1)}$ 的界是通过应用提供的公式来计算仿射函数在一个盒上的最小值和最大值。令 $w_i^\\top$ 为 $W^{(k+1)}$ 的第 $i$ 行，$b_i$ 为 $b^{(k+1)}$ 的第 $i$ 个分量。$z_i^{(k+1)}$ 的界 $[\\ell_{z,i}^{(k+1)}, u_{z,i}^{(k+1)}]$ 为：\n    $$u_{z,i}^{(k+1)} = \\sum_{j} \\max(w_{ij}, 0) u_j^{(k)} + \\min(w_{ij}, 0) \\ell_j^{(k)} + b_i$$\n    $$\\ell_{z,i}^{(k+1)} = \\sum_{j} \\max(w_{ij}, 0) \\ell_j^{(k)} + \\min(w_{ij}, 0) u_j^{(k)} + b_i$$\n\n2.  **激活函数**：激活后的界 $[\\ell_a^{(k+1)}, u_a^{(k+1)}]$ 是通过将激活函数应用于预激活值的界来找到的。由于 $\\mathrm{ReLU}$ 是按元素且单调非递减的，新的界为：\n    $$\\ell_a^{(k+1)} = \\mathrm{ReLU}(\\ell_z^{(k+1)})$$\n    $$u_a^{(k+1)} = \\mathrm{ReLU}(u_z^{(k+1)})$$\n\n这个过程从输入盒 $a^{(0)} \\in [\\ell, u]$ 开始，并为每一层重复进行，直到计算出最终的输出界 $[y_L^{\\mathrm{IBP}}, y_U^{\\mathrm{IBP}}]$。\n\n### 任务2：类DeepPoly抽象\n\n这种方法通过为每个神经元追踪一个原始网络输入 $x$ 的仿射函数来提供更紧的界，该函数从上方和下方界定其激活值。对于一个神经元 $a_i$，我们维持形式为 $a_i \\ge (\\alpha_{i,L})^\\top x + \\beta_{i,L}$ 和 $a_i \\le (\\alpha_{i,U})^\\top x + \\beta_{i,U}$ 的符号界。\n\n这些符号仿射界的传播是逐层进行的：\n\n1.  **输入层**：对于网络输入 $x = (x_1, \\dots, x_n)^\\top$，每个分量 $x_i$ 都被自身平凡地界定。这表示为 $x_i \\ge e_i^\\top x + 0$ 和 $x_i \\le e_i^\\top x + 0$，其中 $e_i$ 是第 $i$ 个标准基向量。因此，对于输入层，下界alpha系数矩阵是单位矩阵 $\\mathbf{A}_L^{(0)} = I$，beta系数向量是零向量 $\\mathbf{B}_L^{(0)} = \\vec{0}$。类似地，$\\mathbf{A}_U^{(0)} = I$ 且 $\\mathbf{B}_U^{(0)} = \\vec{0}$。\n\n2.  **仿射变换**：给定第 $k-1$ 层激活的仿射界，由系数矩阵 $\\mathbf{A}_{L/U}^{(k-1)}$ 和向量 $\\mathbf{B}_{L/U}^{(k-1)}$ 表示，我们计算预激活值 $z^{(k)} = W^{(k)} a^{(k-1)} + b^{(k)}$ 的界。通过代入 $a^{(k-1)}$ 的仿射界并重新整理项，我们推导出 $z^{(k)}$ 的新仿射系数：\n    令 $(W^{(k)})^+ = \\max(W^{(k)}, 0)$ 和 $(W^{(k)})^- = \\min(W^{(k)}, 0)$ 分别是权重矩阵的元素级正部和负部。\n    $$\\mathbf{A}_{L,z}^{(k)} = (W^{(k)})^+\\mathbf{A}_L^{(k-1)} + (W^{(k)})^-\\mathbf{A}_U^{(k-1)}$$\n    $$\\mathbf{B}_{L,z}^{(k)} = (W^{(k)})^+\\mathbf{B}_L^{(k-1)} + (W^{(k)})^-\\mathbf{B}_U^{(k-1)} + b^{(k)}$$\n    $$\\mathbf{A}_{U,z}^{(k)} = (W^{(k)})^+\\mathbf{A}_U^{(k-1)} + (W^{(k)})^-\\mathbf{A}_L^{(k-1)}$$\n    $$\\mathbf{B}_{U,z}^{(k)} = (W^{(k)})^+\\mathbf{B}_U^{(k-1)} + (W^{(k)})^-\\mathbf{B}_L^{(k-1)} + b^{(k)}$$\n\n3.  **ReLU变换**：这是该抽象的核心。对于第 $k$ 层的每个神经元 $j$，我们将预激活仿射界转换为激活后 ($a_j^{(k)} = \\mathrm{ReLU}(z_j^{(k)})$) 的仿射界。这首先需要通过在输入盒 $[\\ell, u]$ 上最大化/最小化其仿射界来计算每个预激活神经元 $z_j^{(k)}$ 的具体区间界 $[z_{j,L}, z_{j,U}]$。基于这些具体界，我们应用以下规则之一：\n    *   **情况1：$z_{j,U} \\le 0$ (ReLU非激活)**。输出恰好为 $0$。新的仿射界为零：$\\alpha_{j,L/U,a}^{(k)} = \\vec{0}, \\beta_{j,L/U,a}^{(k)} = 0$。\n    *   **情况2：$z_{j,L} \\ge 0$ (ReLU激活)**。输出是恒等映射，$a_j^{(k)} = z_j^{(k)}$。$a_j^{(k)}$ 的仿射界与 $z_j^{(k)}$ 的相同。\n    *   **情况3：$z_{j,L}  0  z_{j,U}$ (ReLU不稳定)**。我们使用线性松弛。\n        *   **下界**：问题指定 $a_j^{(k)} \\ge 0$。这对应于仿射界 $\\alpha_{j,L,a}^{(k)} = \\vec{0}$ 和 $\\beta_{j,L,a}^{(k)} = 0$。\n        *   **上界**：标准凸松弛是一条通过 $(z_{j,L}, 0)$ 和 $(z_{j,U}, z_{j,U})$ 的直线，由 $a_j^{(k)} \\le \\lambda z_j^{(k)} + \\mu$ 给出，其中 $\\lambda = \\frac{z_{j,U}}{z_{j,U} - z_{j,L}}$ 和 $\\mu = -\\lambda z_{j,L}$。由于 $\\lambda > 0$，我们代入 $z_j^{(k)}$ 的上仿射界来获得 $a_j^{(k)}$ 的新上仿射界：\n            $$\\alpha_{j,U,a}^{(k)} = \\lambda \\cdot \\alpha_{j,U,z}^{(k)}$$\n            $$\\beta_{j,U,a}^{(k)} = \\lambda \\cdot \\beta_{j,U,z}^{(k)} + \\mu$$\n\n4.  **最终输出界**：在将这些符号界传播到整个网络后，我们获得输出的最终仿射界，$y \\ge \\alpha_{L,y}^\\top x + \\beta_{L,y}$ 和 $y \\le \\alpha_{U,y}^\\top x + \\beta_{U,y}$。然后通过在输入盒 $[\\ell, u]$ 上最小化下仿射界和最大化上仿射界，来找到具体的输出区间 $[y_L^{\\mathrm{DP}}, y_U^{\\mathrm{DP}}]$。\n\n### 任务3：网格评估\n\n为了获得网络行为的经验性基准真相，我们在输入盒 $[\\ell, u]$ 内的密集点网格上对其进行评估。使用在两个输入维度上均匀间隔的点构建一个 $21 \\times 21 = 441$ 个点的网格。对于这个网格上的每个点 $x_{sample}$，我们计算精确的网络输出 $y_{sample} = f(x_{sample})$。然后我们检查这些样本中是否有任何一个违反了安全属性，即是否有任何 $y_{sample}$ 落在了指定的安全区间 $[s_{\\min}, s_{\\max}]$ 之外。这由一个布尔值 `grid_violation` 来量化。\n\n### 任务4：误报评估\n\n对于给定的验证方法（IBP或DeepPoly），误报被定义为这样一种情况：该方法预测了潜在的安全违规，但密集的网格评估没有发现违规。\n\n对于每种方法，令其计算出的输出界为 $[y_L, y_U]$。\n如果抽象界与不安全区域相交，则预测存在潜在违规：\n`abstract_violation` = $(y_L  s_{\\min})$ 或 $(y_U > s_{\\max})$。\n网格评估结果为：\n`grid_violation` = 如果任何 $y_{sample} \\notin [s_{\\min}, s_{\\max}]$，则为 `True`，否则为 `False`。\n\n然后误报计算为：\n`false_alarm` = `abstract_violation` AND (NOT `grid_violation`)。\n\n对四个测试用例中的每一个，都为IBP和类DeepPoly方法遵循此过程，从而产生总共八个布尔结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... (scipy is listed but not needed for this problem)\n\n# --- Network Parameters ---\nW1 = np.array([[1.0, -1.0], [-1.0, 1.0], [0.2, 0.2]])\nb1 = np.array([0.0, 0.0, 0.05])\n\nW2 = np.array([[0.6, -0.6, 0.2], [-0.6, 0.6, -0.2], [0.3, 0.3, 0.0]])\nb2 = np.array([0.0, 0.0, 0.0])\n\nW3 = np.array([[0.5, -0.4, 0.3]])\nb3 = np.array([0.0])\n\nWs = [W1, W2, W3]\nbs = [b1, b2, b3]\n\ndef relu(x):\n    \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n    return np.maximum(0, x)\n\ndef forward_pass(x, weights, biases):\n    \"\"\"Computes the exact network output for a single input vector.\"\"\"\n    a = x\n    # Hidden layers with ReLU\n    for i in range(len(weights) - 1):\n        z = weights[i] @ a + biases[i]\n        a = relu(z)\n    # Output layer (linear)\n    y = weights[-1] @ a + biases[-1]\n    return y[0]\n\ndef get_affine_bounds(c, d, l, u):\n    \"\"\"Computes min/max of c.T @ x + d over the box [l, u].\"\"\"\n    c_plus = np.maximum(c, 0)\n    c_minus = np.minimum(c, 0)\n    \n    lower_bound = c_plus @ l + c_minus @ u + d\n    upper_bound = c_plus @ u + c_minus @ l + d\n    \n    return lower_bound, upper_bound\n\ndef run_ibp(l, u, weights, biases):\n    \"\"\"Computes output bounds using Interval Bound Propagation.\"\"\"\n    l_current, u_current = l, u\n    \n    # Hidden layers with ReLU\n    for i in range(len(weights) - 1):\n        W, b = weights[i], biases[i]\n        W_plus = np.maximum(W, 0)\n        W_minus = np.minimum(W, 0)\n        \n        l_z = W_plus @ l_current + W_minus @ u_current + b\n        u_z = W_plus @ u_current + W_minus @ l_current + b\n        \n        l_current = relu(l_z)\n        u_current = relu(u_z)\n        \n    # Output layer (linear)\n    W, b = weights[-1], biases[-1]\n    W_plus = np.maximum(W, 0)\n    W_minus = np.minimum(W, 0)\n    \n    y_l = (W_plus @ l_current + W_minus @ u_current + b)[0]\n    y_u = (W_plus @ u_current + W_minus @ l_current + b)[0]\n    \n    return y_l, y_u\n\ndef run_deeppoly(l, u, weights, biases):\n    \"\"\"Computes output bounds using a DeepPoly-like abstraction.\"\"\"\n    n_in = len(l)\n    \n    # Initialize affine bounds for input layer x\n    # alpha_l, alpha_u are (n_neurons, n_in)\n    # beta_l, beta_u are (n_neurons,)\n    alpha_l, alpha_u = np.eye(n_in), np.eye(n_in)\n    beta_l, beta_u = np.zeros(n_in), np.zeros(n_in)\n\n    # Propagate through hidden layers\n    for i in range(len(weights) - 1):\n        W, b = weights[i], biases[i]\n        \n        # 1. Affine transformation\n        W_plus = np.maximum(W, 0)\n        W_minus = np.minimum(W, 0)\n        \n        alpha_lz = W_plus @ alpha_l + W_minus @ alpha_u\n        beta_lz = W_plus @ beta_l + W_minus @ beta_u + b\n        \n        alpha_uz = W_plus @ alpha_u + W_minus @ alpha_l\n        beta_uz = W_plus @ beta_u + W_minus @ beta_l + b\n\n        # 2. ReLU transformation\n        # First, compute concrete bounds for pre-activations\n        z_l, _ = get_affine_bounds(alpha_lz, beta_lz, l, u)\n        _, z_u = get_affine_bounds(alpha_uz, beta_uz, l, u)\n        \n        n_neurons_layer = W.shape[0]\n        alpha_l_new = np.zeros_like(alpha_lz)\n        beta_l_new = np.zeros_like(beta_lz)\n        alpha_u_new = np.zeros_like(alpha_uz)\n        beta_u_new = np.zeros_like(beta_uz)\n        \n        for j in range(n_neurons_layer):\n            zl_j, zu_j = z_l[j], z_u[j]\n            \n            if zu_j = 0: # ReLU is inactive\n                # Bounds are 0\n                pass # Already initialized to zero\n            elif zl_j >= 0: # ReLU is active\n                alpha_l_new[j] = alpha_lz[j]\n                beta_l_new[j] = beta_lz[j]\n                alpha_u_new[j] = alpha_uz[j]\n                beta_u_new[j] = beta_uz[j]\n            else: # ReLU is unstable\n                # Lower bound: a >= 0\n                # alpha_l_new[j] and beta_l_new[j] remain 0\n                \n                # Upper bound: a = lambda * z + mu\n                lambda_ = zu_j / (zu_j - zl_j)\n                mu = -lambda_ * zl_j\n                \n                # Since lambda > 0, we use upper bound for z\n                alpha_u_new[j] = lambda_ * alpha_uz[j]\n                beta_u_new[j] = lambda_ * beta_uz[j] + mu\n\n        alpha_l, beta_l = alpha_l_new, beta_l_new\n        alpha_u, beta_u = alpha_u_new, beta_u_new\n\n    # Final affine transformation for output layer\n    W_out, b_out = weights[-1], biases[-1]\n    W_out_plus = np.maximum(W_out, 0)\n    W_out_minus = np.minimum(W_out, 0)\n\n    final_alpha_l = W_out_plus @ alpha_l + W_out_minus @ alpha_u\n    final_beta_l = W_out_plus @ beta_l + W_out_minus @ beta_u + b_out\n\n    final_alpha_u = W_out_plus @ alpha_u + W_out_minus @ alpha_l\n    final_beta_u = W_out_plus @ beta_u + W_out_minus @ beta_l + b_out\n\n    # Compute final concrete bounds\n    y_l, _ = get_affine_bounds(final_alpha_l[0], final_beta_l[0], l, u)\n    _, y_u = get_affine_bounds(final_alpha_u[0], final_beta_u[0], l, u)\n    \n    return y_l, y_u\n\ndef grid_evaluation(l, u, weights, biases, s_min, s_max):\n    \"\"\"Performs dense grid evaluation to check for safety violations.\"\"\"\n    grid_points = 21\n    x1_vals = np.linspace(l[0], u[0], grid_points)\n    x2_vals = np.linspace(l[1], u[1], grid_points)\n    \n    has_violation = False\n    for x1 in x1_vals:\n        for x2 in x2_vals:\n            x_sample = np.array([x1, x2])\n            y_sample = forward_pass(x_sample, weights, biases)\n            if not (s_min = y_sample = s_max):\n                has_violation = True\n                break\n        if has_violation:\n            break\n            \n    return has_violation\n\ndef solve():\n    \"\"\"Main function to run test cases and generate final output.\"\"\"\n    test_cases = [\n        # Case 1\n        {'l': np.array([-0.1, -0.1]), 'u': np.array([0.1, 0.1]), 's_min': -0.05, 's_max': 0.05},\n        # Case 2\n        {'l': np.array([0.0, 0.0]), 'u': np.array([0.0, 0.0]), 's_min': -0.01, 's_max': 0.01},\n        # Case 3\n        {'l': np.array([-0.5, -0.5]), 'u': np.array([0.5, 0.5]), 's_min': -0.5, 's_max': 0.5},\n        # Case 4\n        {'l': np.array([-0.2, -0.2]), 'u': np.array([0.2, 0.2]), 's_min': -0.02, 's_max': 0.02},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        l, u, s_min, s_max = case['l'], case['u'], case['s_min'], case['s_max']\n\n        # Run grid evaluation to find \"ground truth\"\n        grid_has_violation = grid_evaluation(l, u, Ws, bs, s_min, s_max)\n        \n        # --- IBP Analysis ---\n        y_l_ibp, y_u_ibp = run_ibp(l, u, Ws, bs)\n        ibp_predicts_violation = (y_l_ibp  s_min) or (y_u_ibp > s_max)\n        ibp_false_alarm = ibp_predicts_violation and not grid_has_violation\n        results.append(ibp_false_alarm)\n        \n        # --- DeepPoly Analysis ---\n        y_l_dp, y_u_dp = run_deeppoly(l, u, Ws, bs)\n        dp_predicts_violation = (y_l_dp  s_min) or (y_u_dp > s_max)\n        dp_false_alarm = dp_predicts_violation and not grid_has_violation\n        results.append(dp_false_alarm)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的练习将验证技术应用于一个动态的、安全关键的场景，其中神经网络控制器位于一个赛博物理系统的反馈回路中。您将使用区域 (zonotope) 这一强大的几何抽象域，来过近似系统在有限时间范围内的状态，并验证其是否满足安全约束。这个练习将静态神经网络验证与确保整个受控系统时序安全之间的鸿沟联系起来。",
            "id": "4243426",
            "problem": "考虑一个离散时间信息物理系统 (CPS) 的线性对象，该对象由一个前馈修正线性单元 (ReLU) 神经网络 (NN) 控制，此背景设定在数字孪生和信息物理系统的安全性分析中。该对象根据离散时间线性模型和有界过程扰动进行演化：\n$$\nx_{k+1} = A x_k + B u_k + w_k,\n$$\n其中 $x_k \\in \\mathbb{R}^n$ 是离散时间 $k$ 的状态， $u_k \\in \\mathbb{R}^m$ 是由 NN 控制器生成的控制输入，$A \\in \\mathbb{R}^{n \\times n}$ 是对象的状态转移矩阵，$B \\in \\mathbb{R}^{n \\times m}$ 是输入矩阵，$w_k$ 是一个有界扰动，被建模为带状体 (zonotope) 的一个成员。NN 控制器是一个单隐藏层的 ReLU 网络，定义如下：\n$$\nu_k = W_2 \\, \\sigma\\!\\left(W_1 x_k + b_1\\right) + b_2,\n$$\n其中 $W_1 \\in \\mathbb{R}^{h \\times n}$ 和 $b_1 \\in \\mathbb{R}^{h}$ 是第一层的权重和偏置，$W_2 \\in \\mathbb{R}^{m \\times h}$ 和 $b_2 \\in \\mathbb{R}^{m}$ 是输出层的权重和偏置，$h$ 是隐藏单元的数量，$\\sigma(\\cdot)$ 是逐元素的修正线性单元激活函数，定义为 $\\sigma(z)_i = \\max\\{0, z_i\\}$。\n\n初始状态集和扰动集表示为带状体。$\\mathbb{R}^n$ 中的一个带状体定义为：\n$$\n\\mathcal{Z}(c, G) = \\left\\{x \\in \\mathbb{R}^n \\; \\bigg| \\; x = c + G \\beta, \\; \\|\\beta\\|_\\infty \\le 1 \\right\\},\n$$\n其中 $c \\in \\mathbb{R}^n$ 是中心，$G \\in \\mathbb{R}^{n \\times p}$ 是生成矩阵，$p$ 是生成元的数量。扰动带状体 $\\mathcal{W} = \\mathcal{Z}(c_w, G_w)$ 假定以原点为中心，即 $c_w = 0$。\n\n安全性要求是一个轴对齐的安全集，定义为超矩形：\n$$\nS = \\left\\{x \\in \\mathbb{R}^n \\; \\big| \\; -s_i \\le x_i \\le s_i \\text{ for all } i \\in \\{1, \\dots, n\\}\\right\\},\n$$\n对于一个给定的向量 $s \\in \\mathbb{R}^n_{>0}$。任务是使用带状体传播方法，为闭环系统在有限时域 $H \\in \\mathbb{N}$ 内计算一个有限时间可达集，并返回在每一步 $k \\in \\{0,1,\\dots,H\\}$，可达集是否包含在安全集 $S$ 内。\n\n必须实现的传播规则：\n- 带状体的仿射变换：对于 $y = M x + b$，如果 $x \\in \\mathcal{Z}(c, G)$，那么 $y \\in \\mathcal{Z}(M c + b, M G)$。\n- 带状体的加法：如果 $x \\in \\mathcal{Z}(c_x, G_x)$ 且 $y \\in \\mathcal{Z}(c_y, G_y)$，那么 $x + y \\in \\mathcal{Z}(c_x + c_y, [G_x \\; G_y])$，其中 $[G_x \\; G_y]$ 表示生成元的水平拼接。\n- 对带状体的 ReLU 映射将按如下方式进行过近似。对于 $\\mathbb{R}^h$ 中的一个激活前带状体 $\\mathcal{Z}(c, G)$，通过以下方式计算每个坐标 $i$ 的区间下界和上界：\n$$\n\\ell_i = c_i - \\sum_{j=1}^p |G_{i,j}|, \\quad u_i = c_i + \\sum_{j=1}^p |G_{i,j}|.\n$$\n然后逐坐标为激活后的 $y = \\sigma(z)$ 定义一个过近似带状体：\n1. 如果 $\\ell_i \\ge 0$，则第 $i$ 行保持不变（精确线性区域）。\n2. 如果 $u_i \\le 0$，则将第 $i$ 个输出恒定置为零（非激活区域），即中心坐标为 $0$ 且第 $i$ 行的生成元为零。\n3. 如果 $l_i  0  u_i$，则对第 $i$ 个坐标用轴对齐区间 $[0, u_i]$ 进行过近似：将第 $i$ 个中心项设为 $u_i/2$，将第 $i$ 行的现有生成元清零，并追加一个新的生成元列，该列在第 $i$ 行有一个唯一的非零项 $u_i/2$（不相关盒式过近似）。\n\n使用上述规则计算直到 $H$ 的每个 $k$ 的有限时间可达带状体 $\\mathcal{Z}_k$：\n$$\n\\mathcal{Z}_{k+1} = A \\,\\mathcal{Z}_k + B \\,\\mathcal{U}(\\mathcal{Z}_k) + \\mathcal{W},\n$$\n其中 $\\mathcal{U}(\\mathcal{Z}_k)$ 是由当前状态带状体 $\\mathcal{Z}_k$ 引起的 NN 控制器输出的带状体过近似。在每个时间 $k$，使用带状体的区间界测试 $\\mathcal{Z}_k \\subseteq S$ 是否成立。\n\n您的程序必须实现上述过程并评估以下测试套件。所有值都以精确形式给出；请完全按照规定实现它们。\n\n测试用例 1（标称稳定动态，小不确定性）：\n- 状态维度 $n = 2$，控制维度 $m = 1$，隐藏层大小 $h = 2$，时域 $H = 5$。\n- 对象矩阵：\n$$\nA = \\begin{bmatrix} 0.98  0.05 \\\\ 0  0.95 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.10 \\\\ 0.05 \\end{bmatrix}.\n$$\n- 控制器参数：\n$$\nW_1 = \\begin{bmatrix} 0.60  -0.25 \\\\ 0.20  0.40 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ -0.05 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.70  -0.30 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- 初始状态带状体：\n$$\nc_0 = \\begin{bmatrix} 0.10 \\\\ -0.05 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.05  0.00 \\\\ 0.00  0.05 \\end{bmatrix}.\n$$\n- 扰动带状体（中心为零）：\n$$\nG_w = \\begin{bmatrix} 0.01  0.00 \\\\ 0.00  0.01 \\end{bmatrix}.\n$$\n- 安全集半宽度：\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\n测试用例 2（近边界初始条件，中等不确定性）：\n- 维度 $n = 2$, $m = 1$, $h = 2$，时域 $H = 4$。\n- 对象矩阵：\n$$\nA = \\begin{bmatrix} 0.98  0.05 \\\\ 0  0.95 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.10 \\\\ 0.05 \\end{bmatrix}.\n$$\n- 控制器参数：\n$$\nW_1 = \\begin{bmatrix} 0.60  -0.25 \\\\ 0.20  0.40 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ -0.05 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.70  -0.30 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- 初始状态带状体：\n$$\nc_0 = \\begin{bmatrix} 0.45 \\\\ 0.45 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.02  0.00 \\\\ 0.00  0.02 \\end{bmatrix}.\n$$\n- 扰动带状体：\n$$\nG_w = \\begin{bmatrix} 0.005  0.00 \\\\ 0.00  0.005 \\end{bmatrix}.\n$$\n- 安全集半宽度：\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\n测试用例 3（轻度不稳定动态，较大扰动）：\n- 维度 $n = 2$, $m = 1$, $h = 2$，时域 $H = 5$。\n- 对象矩阵：\n$$\nA = \\begin{bmatrix} 1.20  0.00 \\\\ 0.00  1.10 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.05 \\\\ 0.02 \\end{bmatrix}.\n$$\n- 控制器参数：\n$$\nW_1 = \\begin{bmatrix} 0.50  -0.20 \\\\ 0.30  0.30 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.60  -0.10 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- 初始状态带状体：\n$$\nc_0 = \\begin{bmatrix} 0.20 \\\\ 0.20 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.02  0.00 \\\\ 0.00  0.02 \\end{bmatrix}.\n$$\n- 扰动带状体：\n$$\nG_w = \\begin{bmatrix} 0.05  0.00 \\\\ 0.00  0.05 \\end{bmatrix}.\n$$\n- 安全集半宽度：\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\n您的程序必须：\n1.  对每个测试用例，使用上述仿射和 ReLU 过近似方法，在指定的时域 $H$ 内实现闭环系统的带状体传播。\n2.  在每个时间 $k \\in \\{0,1,\\dots,H\\}$，通过以下方式计算当前状态带状体的区间界：\n    $$\n    \\ell = c - \\sum_{j=1}^p |G_{:,j}|, \\quad u = c + \\sum_{j=1}^p |G_{:,j}|,\n    $$\n    并检查对于所有 $i$，是否有 $\\ell_i \\ge -s_i$ 和 $u_i \\le s_i$ 成立。\n3.  为每个测试用例返回一个布尔值，指示系统在直到 $H$ 的所有时间步长内是否安全。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三个测试用例的结果，形式为方括号括起来的逗号分隔列表（例如，`[True,False,True]`）。不应打印任何其他文本。",
            "solution": "用户提供的问题已经过分析，并被认为是有效的，因为它具有科学依据、问题明确且内容完整。该任务是为一个由修正线性单元 (ReLU) 神经网络 (NN) 控制的离散时间信息物理系统 (CPS) 执行安全性验证。这涉及到使用基于带状体 (zonotope) 的过近似方法，计算系统在有限时间范围内的前向可达集。\n\n该方法的核心在于带状体在系统动态中的传播，这包括线性变换和非线性的 ReLU 激活函数。一个带状体 $\\mathcal{Z}(c, G)$ 是由一个中心 $c \\in \\mathbb{R}^n$ 和一个生成矩阵 $G \\in \\mathbb{R}^{n \\times p}$ 定义的点集，即 $\\mathcal{Z}(c, G) = \\{c + G \\beta \\mid \\|\\beta\\|_\\infty \\le 1\\}$。\n\n整个验证过程如下：\n从初始状态带状体 $\\mathcal{Z}_0 = \\mathcal{Z}(c_0, G_0)$ 开始，我们迭代计算可达状态带状体 $\\mathcal{Z}_{k+1}$，其中 $k = 0, 1, \\dots, H-1$。在每一步 $k \\in \\{0, 1, \\dots, H\\}$，我们验证当前可达集 $\\mathcal{Z}_k$ 是否包含在指定的安全集 $S$ 内。\n\n从 $\\mathcal{Z}_k$ 计算后续可达集 $\\mathcal{Z}_{k+1}$ 涉及多个步骤，这些步骤基于闭环系统动态 $x_{k+1} = A x_k + B u_k + w_k$，其中 $u_k$ 是 NN 控制器的输出。\n\n1.  **在步骤 $k$ 的安全性验证**：在传播之前，我们检查 $\\mathcal{Z}_k = \\mathcal{Z}(c_k, G_k)$ 是否完全在安全集 $S = \\{x \\mid -s_i \\le x_i \\le s_i\\}$ 内。这是通过计算紧密包围该带状体的轴对齐超矩形来实现的。每个维度 $i$ 的下界和上界由以下公式给出：\n    $$\n    \\ell_i = c_{k,i} - \\sum_{j=1}^{p} |G_{k,i,j}|\n    $$\n    $$\n    u_i = c_{k,i} + \\sum_{j=1}^{p} |G_{k,i,j}|\n    $$\n    如果对于所有维度 $i=1, \\dots, n$，$\\ell_i \\ge -s_i$ 和 $u_i \\le s_i$ 都成立，则满足安全条件。如果在任何步骤 $k$ 违反此条件，则系统被视为不安全，当前测试用例的验证终止，结果为 `False`。\n\n2.  **NN 控制器输出的过近似**：我们将当前状态带状体 $\\mathcal{Z}_k$ 通过 NN 控制器传播，以获得控制输入集 $\\mathcal{U}_k$ 的一个过近似。\n    a.  **第一个仿射层**：隐藏层的激活前值形成一个带状体 $\\mathcal{Z}_{pre}$。给定 $\\mathcal{Z}_k = \\mathcal{Z}(c_k, G_k)$，变换为 $z = W_1 x_k + b_1$。使用带状体的仿射变换规则，我们得到：\n        $$\n        \\mathcal{Z}_{pre} = \\mathcal{Z}(W_1 c_k + b_1, W_1 G_k)\n        $$\n    b.  **ReLU 激活的过近似**：将逐元素的 ReLU 函数 $\\sigma(\\cdot)$ 应用于 $\\mathcal{Z}_{pre} = \\mathcal{Z}(c_{pre}, G_{pre})$。通过对 $h$ 个隐藏神经元中每一个的 ReLU 输出进行过近似，构造一个新的带状体 $\\mathcal{Z}_{post} = \\mathcal{Z}(c_{post}, G_{post})$。对于每个神经元 $i \\in \\{1, \\dots, h\\}$，我们首先计算其激活前的区间界 $[\\ell_i, u_i]$。\n        - 如果 $\\ell_i \\ge 0$（总是激活），神经元的输出是线性的。$c_{post}$ 和 $G_{post}$ 的第 $i$ 行与 $c_{pre}$ 和 $G_{pre}$ 的相同。\n        - 如果 $u_i \\le 0$（总是不激活），神经元的输出为零。$c_{post}$ 和 $G_{post}$ 的第 $i$ 行被设置为零。\n        - 如果 $\\ell_i  0  u_i$（不确定），输出在 $[0, u_i]$ 内。这通过一个中心为 $u_i/2$、一个新的独立生成元大小为 $u_i/2$ 的新带状体来对此维度进行过近似。这会为生成矩阵 $G_{post}$ 增加一列。\n    c.  **第二个仿射层**：最终的控制输入带状体 $\\mathcal{U}_k = \\mathcal{Z}(c_u, G_u)$ 是通过将输出层变换 $u_k = W_2 y_k + b_2$ 应用于 $\\mathcal{Z}_{post}$ 来计算的：\n        $$\n        \\mathcal{U}_k = \\mathcal{Z}(W_2 c_{post} + b_2, W_2 G_{post})\n        $$\n\n3.  **对象动态传播**：下一个时间步的状态带状体 $\\mathcal{Z}_{k+1}$ 是三个带状体的闵可夫斯基和：对象动态对当前状态的影响、控制输入的影响以及过程扰动。\n    - $A \\mathcal{Z}_k = \\mathcal{Z}(A c_k, A G_k)$\n    - $B \\mathcal{U}_k = \\mathcal{Z}(B c_u, B G_u)$\n    - $\\mathcal{W} = \\mathcal{Z}(0, G_w)$\n    \n    使用闵可夫斯基和规则（即中心相加，生成矩阵拼接），我们得到 $\\mathcal{Z}_{k+1} = \\mathcal{Z}(c_{k+1}, G_{k+1})$：\n    $$\n    c_{k+1} = A c_k + B c_u\n    $$\n    $$\n    G_{k+1} = [A G_k, B G_u, G_w]\n    $$\n\n此过程对 $k = 0, \\dots, H-1$ 重复进行。如果对于所有步骤 $k=0, \\dots, H$，安全检查都通过，则声明系统在给定时域内是安全的，结果为 `True`。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 5,\n            \"A\": np.array([[0.98, 0.05], [0.0, 0.95]]),\n            \"B\": np.array([[0.10], [0.05]]),\n            \"W1\": np.array([[0.60, -0.25], [0.20, 0.40]]),\n            \"b1\": np.array([[0.00], [-0.05]]),\n            \"W2\": np.array([[0.70, -0.30]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.10], [-0.05]]),\n            \"G0\": np.array([[0.05, 0.00], [0.00, 0.05]]),\n            \"Gw\": np.array([[0.01, 0.00], [0.00, 0.01]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n        # Test Case 2\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 4,\n            \"A\": np.array([[0.98, 0.05], [0.0, 0.95]]),\n            \"B\": np.array([[0.10], [0.05]]),\n            \"W1\": np.array([[0.60, -0.25], [0.20, 0.40]]),\n            \"b1\": np.array([[0.00], [-0.05]]),\n            \"W2\": np.array([[0.70, -0.30]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.45], [0.45]]),\n            \"G0\": np.array([[0.02, 0.00], [0.00, 0.02]]),\n            \"Gw\": np.array([[0.005, 0.00], [0.00, 0.005]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n        # Test Case 3\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 5,\n            \"A\": np.array([[1.20, 0.00], [0.00, 1.10]]),\n            \"B\": np.array([[0.05], [0.02]]),\n            \"W1\": np.array([[0.50, -0.20], [0.30, 0.30]]),\n            \"b1\": np.array([[0.00], [0.00]]),\n            \"W2\": np.array([[0.60, -0.10]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.20], [0.20]]),\n            \"G0\": np.array([[0.02, 0.00], [0.00, 0.02]]),\n            \"Gw\": np.array([[0.05, 0.00], [0.00, 0.05]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        results.append(run_verification(params))\n    \n    # Format the print statement exactly as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef relu_approx(zono):\n    \"\"\"\n    Over-approximates the ReLU function for a given zonotope.\n    zono: A tuple (c, G) representing the zonotope.\n    \"\"\"\n    c, G = zono\n    h = c.shape[0]\n\n    c_post = np.zeros_like(c)\n    G_base = np.zeros_like(G)\n    new_generators = []\n\n    # Calculate interval bounds for each neuron's pre-activation\n    if G.shape[1] > 0:\n        interval_radii = np.sum(np.abs(G), axis=1, keepdims=True)\n    else:\n        interval_radii = np.zeros_like(c)\n        \n    l_bounds = c - interval_radii\n    u_bounds = c + interval_radii\n\n    for i in range(h):\n        l_i, u_i = l_bounds[i, 0], u_bounds[i, 0]\n        \n        if l_i >= 0:  # Case 1: Always active\n            c_post[i] = c[i]\n            G_base[i, :] = G[i, :]\n        elif u_i = 0:  # Case 2: Always inactive\n            c_post[i] = 0\n            G_base[i, :] = 0\n        else:  # Case 3: Uncertain\n            c_post[i] = u_i / 2.0\n            G_base[i, :] = 0  # Zero out existing generators for this dimension\n            new_gen = np.zeros((h, 1))\n            new_gen[i] = u_i / 2.0\n            new_generators.append(new_gen)\n\n    if new_generators:\n        G_new = np.hstack(new_generators)\n        if G_base.shape[1] > 0:\n            G_post = np.hstack((G_base, G_new))\n        else:\n            G_post = G_new\n    else:\n        G_post = G_base\n\n    return (c_post, G_post)\n\ndef run_verification(params):\n    \"\"\"\n    Runs the zonotope-based reachability analysis for a single test case.\n    \"\"\"\n    H = params[\"H\"]\n    A, B = params[\"A\"], params[\"B\"]\n    W1, b1 = params[\"W1\"], params[\"b1\"]\n    W2, b2 = params[\"W2\"], params[\"b2\"]\n    Gw = params[\"Gw\"]\n    s = params[\"s\"]\n\n    current_zono = (params[\"c0\"], params[\"G0\"])\n\n    for k in range(H + 1):\n        # 1. Safety Check\n        c_k, G_k = current_zono\n        if G_k.shape[1] > 0:\n            interval_radii = np.sum(np.abs(G_k), axis=1, keepdims=True)\n        else:\n            interval_radii = np.zeros_like(c_k)\n            \n        lower_bounds = c_k - interval_radii\n        upper_bounds = c_k + interval_radii\n        \n        if not np.all((lower_bounds >= -s)  (upper_bounds = s)):\n            return False\n\n        # If it's the last step, no need to compute the next state\n        if k == H:\n            break\n\n        # 2. Propagate to compute next state zonotope\n        \n        # 2a. Controller output over-approximation\n        # First layer (affine)\n        c_pre = W1 @ c_k + b1\n        G_pre = W1 @ G_k\n        zono_pre = (c_pre, G_pre)\n        \n        # ReLU activation\n        c_post, G_post = relu_approx(zono_pre)\n\n        # Output layer (affine)\n        c_u = W2 @ c_post + b2\n        G_u = W2 @ G_post\n        \n        # 2b. Plant dynamics\n        # Term 1: A * x_k\n        c_ax = A @ c_k\n        G_ax = A @ G_k\n        \n        # Term 2: B * u_k\n        c_bu = B @ c_u\n        G_bu = B @ G_u\n\n        # Term 3: w_k (disturbance) is already a zonotope with center 0\n        \n        # 2c. Minkowski Sum for x_{k+1}\n        c_next = c_ax + c_bu\n        G_next = np.hstack([G_ax, G_bu, Gw])\n        \n        current_zono = (c_next, G_next)\n\n    return True\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}