{
    "hands_on_practices": [
        {
            "introduction": "要真正理解带有修正线性单元（Rectified Linear Unit, ReLU）激活函数的神经网络验证，最好的方法莫过于从第一性原理出发，实现一个精确的可达集计算方法。这个练习的核心是利用ReLU网络的分段线性特性，将输入空间划分为多个多面体区域，在每个区域内网络是纯线性的。通过在每个区域上求解线性规划（Linear Programming, LP）问题，我们可以精确地确定网络输出的边界，从而验证其安全性。通过完成这项练习 ，你将深刻理解ReLU网络验证的内在几何原理，并为其他近似方法的评估建立一个“地面真实”（ground truth）基准。",
            "id": "4243524",
            "problem": "考虑一个带有修正线性单元（ReLU）激活函数的双层前馈神经网络，该网络用作信息物理系统数字孪生的一个组件。该网络通过以下变换将输入向量 $x \\in \\mathbb{R}^n$ 映射到标量输出 $y \\in \\mathbb{R}$：隐藏层的预激活值 $z \\in \\mathbb{R}^m$ 由 $z = W_1 x + b_1$ 给出，隐藏层的激活值 $h \\in \\mathbb{R}^m$ 是逐元素的 $h_i = \\max(0, z_i)$（对于所有索引 $i$），输出为 $y = W_2 h + b_2$。所有运算都在实数上进行。输入集是由下界 $l \\in \\mathbb{R}^n$ 和上界 $u \\in \\mathbb{R}^n$ 定义的超矩形（盒子），即 $x$ 逐元素满足 $l \\le x \\le u$。一项安全规范要求对于盒子内的所有输入，输出必须保持在给定的闭区间 $[L, U] \\subset \\mathbb{R}$ 内。\n\n从仿射变换、凸多面体和分段线性激活函数的基本定义出发，实现多面体传播来计算网络在输入盒上输出的精确可达集。利用以下特性：对于隐藏层的每种激活模式，变量 $(x, z, h, y)$ 中的可行集是由线性等式和不等式定义的凸多面体，并且所有模式的并集覆盖了整个可达集。通过在每个这样的多面体上优化一个线性目标来获得 $y$ 的紧密下界和上界。判断可达输出区间是否位于安全区间 $[L, U]$ 内。\n\n您的程序必须：\n- 枚举给定 $m$ 的所有隐藏层激活模式。\n- 对每种模式，建立线性约束来捕捉 $z = W_1 x + b_1$、该模式下的 ReLU 关系、$y = W_2 h + b_2$ 以及输入盒约束 $l \\le x \\le u$。\n- 对每个可行模式求解两个线性规划问题，以计算该模式多面体上的 $\\min y$ 和 $\\max y$，并在所有模式间进行聚合，以获得整个可达集上的全局最小值和最大值。\n\n所有数值输出都必须不带物理单位表示。不涉及角度。对于每个测试用例，最终答案必须包含三个输出：$y$ 的全局下界（浮点数）、$y$ 的全局上界（浮点数），以及一个布尔值，指示可达区间 $[y_{\\min}, y_{\\max}]$ 是否包含在 $[L, U]$ 内（即 $y_{\\min} \\ge L$ 且 $y_{\\max} \\le U$）。浮点数必须四舍五入到四位小数并以十进制形式打印。布尔值必须打印为 Python 的规范字面量 $True$ 或 $False$。\n\n测试套件：\n- 用例 1（一般情况）：\n    - 维度：$n = 2$，$m = 3$。\n    - 隐藏层权重：$W_1 = \\begin{bmatrix} 1.0  -2.0 \\\\ 0.5  1.0 \\\\ -1.5  0.5 \\end{bmatrix}$。\n    - 隐藏层偏置：$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$。\n    - 输出层权重：$W_2 = \\begin{bmatrix} 0.8  -0.4  0.6 \\end{bmatrix}$。\n    - 输出偏置：$b_2 = 0.05$。\n    - 输入盒：$l = \\begin{bmatrix} -0.5 \\\\ -0.5 \\end{bmatrix}$，$u = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [-1.0, 1.0]$。\n- 用例 2（更大的输入盒，更紧的安全区间）：\n    - 维度和网络参数与用例 1 相同。\n    - 输入盒：$l = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}$，$u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [-0.3, 0.3]$。\n- 用例 3（产生恒定输出的退化输出层）：\n    - 维度：$n = 2$，$m = 3$。\n    - 隐藏层权重：$W_1 = \\begin{bmatrix} 1.0  -2.0 \\\\ 0.5  1.0 \\\\ -1.5  0.5 \\end{bmatrix}$。\n    - 隐藏层偏置：$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$。\n    - 输出层权重：$W_2 = \\begin{bmatrix} 0.0  0.0  0.0 \\end{bmatrix}$。\n    - 输出偏置：$b_2 = 0.2$。\n    - 输入盒：$l = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}$，$u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [0.2, 0.2]$。\n- 用例 4（混合符号和非对称盒）：\n    - 维度：$n = 2$，$m = 3$。\n    - 隐藏层权重：$W_1 = \\begin{bmatrix} 2.0  -1.0 \\\\ 1.0  1.0 \\\\ -2.0  0.5 \\end{bmatrix}$。\n    - 隐藏层偏置：$b_1 = \\begin{bmatrix} -0.5 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$。\n    - 输出层权重：$W_2 = \\begin{bmatrix} -0.6  0.4  -0.3 \\end{bmatrix}$。\n    - 输出偏置：$b_2 = 0.1$。\n    - 输入盒：$l = \\begin{bmatrix} -0.2 \\\\ -0.2 \\end{bmatrix}$，$u = \\begin{bmatrix} 0.7 \\\\ 0.3 \\end{bmatrix}$。\n    - 安全区间：$[L, U] = [-0.5, 0.0]$。\n\n最终输出格式：\n您的程序应生成一行包含在方括号内的逗号分隔列表。该列表必须是按顺序连接每个测试用例的三个输出，即 $[y_{\\min}^{(1)}, y_{\\max}^{(1)}, \\text{safe}^{(1)}, y_{\\min}^{(2)}, y_{\\max}^{(2)}, \\text{safe}^{(2)}, y_{\\min}^{(3)}, y_{\\max}^{(3)}, \\text{safe}^{(3)}, y_{\\min}^{(4)}, y_{\\max}^{(4)}, \\text{safe}^{(4)}]$，其中每个 $y_{\\min}^{(k)}$ 和 $y_{\\max}^{(k)}$ 都四舍五入到四位小数，每个 $\\text{safe}^{(k)}$ 是一个布尔值。不应打印任何其他文本。",
            "solution": "用户要求为一个带有修正线性单元（ReLU）激活函数的双层前馈神经网络实现一个精确可达性分析算法。目标是在输入向量 $x$ 被约束在一个超矩形集合内的前提下，计算网络标量输出 $y$ 的精确范围。然后将计算出的输出范围 $[y_{\\min}, y_{\\max}]$ 与给定的安全区间 $[L, U]$ 进行核对。指定的方法是多面体传播，这涉及到枚举隐藏层所有可能的激活模式并求解一组线性规划（LPs）。\n\n该问题是有效的，因为它在神经网络形式化验证这一成熟领域有科学依据，数学上是适定的，并且所有必要的参数和约束都已提供。我们基于第一性原理来构建解决方案。\n\n### 基于原理的设计\n\n问题的核心在于神经网络的分段线性特性。网络函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 由仿射变换和逐元素的 ReLU 激活函数 $h_i = \\max(0, z_i)$ 复合而成。ReLU 函数本身是分段线性的，这一性质扩展到了整个网络。\n\n该算法，被称为多面体传播或精确方法，利用了这一结构。输入域，一个由 $l \\le x \\le u$ 定义的超矩形，是一个凸多面体。基本原理是，一个凸多面体在仿射变换下的像也是一个凸多面体。ReLU 激活函数 $h_i = \\max(0, z_i)$ 充当一个“分割器”：它将其输入 $z_i$ 的定义域划分为两个半空间，$z_i \\ge 0$ 和 $z_i \\le 0$。在每个半空间内，该函数是线性的（$h_i = z_i$ 或 $h_i = 0$）。\n\n因此，我们可以通过基于 $m$ 个隐藏神经元的离散状态来划分问题，从而确定精确的输出集。一个“激活模式”是一个向量 $\\sigma \\in \\{0, 1\\}^m$，它为每个神经元 $i \\in \\{1, \\dots, m\\}$ 指定其是激活的（$\\sigma_i=1$，对应于 $z_i \\ge 0$）还是非激活的（$\\sigma_i=0$，对应于 $z_i \\le 0$）。总共有 $2^m$ 种这样的模式。\n\n对于每个固定的激活模式 $\\sigma$，网络的行为由一组纯线性关系控制。这使我们能够在变量 $(x, z, h)$ 的空间中定义一个凸多面体，它代表了与该模式一致的网络所有可能的状态。定义该多面体的约束条件是：\n1.  **输入盒**：$l \\le x \\le u$。这些是 $2n$ 个线性不等式。\n2.  **第一个仿射层**：$z = W_1 x + b_1$。这些是 $m$ 个线性等式。\n3.  **ReLU 激活（依赖于模式）**：对每个神经元 $i=1, \\dots, m$：\n    *   如果神经元 $i$ 是激活的（$\\sigma_i=1$）：$z_i \\ge 0$（不等式）和 $h_i = z_i$（等式）。\n    *   如果神经元 $i$ 是非激活的（$\\sigma_i=0$）：$z_i \\le 0$（不等式）和 $h_i = 0$（等式）。\n4.  **输出变换**：网络输出是 $y = W_2 h + b_2$。这是变量 $h_i$ 的一个线性函数。\n\n在该多面体上寻找输出 $y$ 的最小值和最大值的任务是一个线性规划（LP）问题。对于 $2^m$ 个激活模式中的每一个，我们构建并求解两个线性规划问题：\n*   **最小化**：$\\min_{x,z,h} (W_2 h + b_2)$，受该模式的线性约束条件限制。\n*   **最大化**：$\\max_{x,z,h} (W_2 h + b_2)$，受该模式的线性约束条件限制。\n\n这些多面体中有很多可能是空的（不可行），意味着给定盒子中的任何输入 $x$ 都无法产生该激活模式。LP 求解器将检测到这种不可行性。\n\n网络的整体（或全局）最小和最大输出，分别是所有*可行*激活模式中找到的所有最小值中的最小值和所有最大值中的最大值：\n$$y_{\\min} = \\min_{\\sigma \\in \\text{feasible patterns}} \\left( \\min_{x,z,h} \\{ W_2 h + b_2 \\mid \\text{constraints for } \\sigma \\} \\right)$$\n$$y_{\\max} = \\max_{\\sigma \\in \\text{feasible patterns}} \\left( \\max_{x,z,h} \\{ W_2 h + b_2 \\mid \\text{constraints for } \\sigma \\} \\right)$$\n\n最后，安全验证步骤包括检查计算出的可达输出区间 $[y_{\\min}, y_{\\max}]$ 是否完全包含在指定的安全区间 $[L, U]$ 内，这当且仅当 $y_{\\min} \\ge L$ 且 $y_{\\max} \\le U$ 时成立。\n\n### 实现策略\n\n实现将使用 `scipy.optimize.linprog` 来求解线性规划问题。在测试用例中 $m=3$ 且输入 $n=2$，因此有 $2^3 = 8$ 种激活模式需要分析。\n\nLP 的变量将是连接向量 $[x^T, z^T, h^T]^T$，其维度为 $n+2m = 2+2(3) = 8$。\n\n对于 8 种模式中的每一种：\n1.  我们构建与约束相对应的 LP 矩阵和向量（$A_{ub}, b_{ub}, A_{eq}, b_{eq}$）。\n2.  用于 `linprog` 的目标函数向量 $c$ 来自 $W_2 h$。为了找到 $\\min y$，我们最小化 $c^T \\cdot [x^T, z^T, h^T]^T$。为了找到 $\\max y$，我们最小化 $-c^T \\cdot [x^T, z^T, h^T]^T$。常数偏置 $b_2$ 会加到 LP 的结果上。\n3.  调用 `linprog` 函数。如果它返回成功，表明该模式的多面体是可行的，我们就更新全局最小值和最大值。\n4.  在遍历所有模式后，将得到的全局边界 $[y_{\\min}, y_{\\max}]$ 与 $[L, U]$进行比较以确定安全性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef analyze_case(W1, b1, W2, b2, l, u, L, U):\n    \"\"\"\n    Computes the exact output bounds of a 2-layer ReLU network for a given\n    input box and verifies safety.\n\n    Args:\n        W1 (np.array): Weight matrix of the first layer (m x n).\n        b1 (np.array): Bias vector of the first layer (m x 1).\n        W2 (np.array): Weight matrix of the second layer (1 x m).\n        b2 (float): Bias of the second layer.\n        l (np.array): Lower bounds of the input box (n x 1).\n        u (np.array): Upper bounds of the input box (n x 1).\n        L (float): Lower bound of the safety interval.\n        U (float): Upper bound of the safety interval.\n\n    Returns:\n        tuple: (y_min_str, y_max_str, is_safe_bool)\n    \"\"\"\n    m, n = W1.shape\n    num_vars = n + 2 * m  # Variables are [x, z, h]\n\n    # Objective function vector c for linprog. The objective is to min/max W2*h.\n    # The variables are arranged as [x1..xn, z1..zm, h1..hm].\n    c = np.zeros(num_vars)\n    c[n + m:] = W2.flatten()\n    c_min = c\n    c_max = -c\n\n    # Bounds on variables. x is bounded by l and u. z and h are unbounded here,\n    # their constraints are handled by A_eq and A_ub.\n    bounds = []\n    for i in range(n):\n        bounds.append((l[i], u[i]))\n    for _ in range(2 * m):\n        bounds.append((None, None))\n\n    # Equality constraints from the first layer: z = W1*x + b1 => -W1*x + z = b1\n    # These are common to all activation patterns.\n    A_eq_base = np.zeros((m, num_vars))\n    A_eq_base[:, :n] = -W1\n    A_eq_base[:, n:n + m] = np.eye(m)\n    b_eq_base = b1.flatten()\n\n    global_min_y = np.inf\n    global_max_y = -np.inf\n\n    # Iterate through all 2^m activation patterns for the hidden layer.\n    for i in range(2 ** m):\n        pattern = [(i >> j)  1 for j in range(m)]\n\n        # Build pattern-specific constraints for ReLU.\n        A_eq_pattern = np.zeros((m, num_vars))\n        b_eq_pattern = np.zeros(m)\n        A_ub_pattern = np.zeros((m, num_vars))\n        b_ub_pattern = np.zeros(m)\n\n        for j in range(m):  # neuron index\n            if pattern[j] == 1:  # Active neuron: z_j >= 0, h_j = z_j\n                # z_j >= 0  => -z_j = 0\n                A_ub_pattern[j, n + j] = -1\n                b_ub_pattern[j] = 0\n                # h_j = z_j  => -z_j + h_j = 0\n                A_eq_pattern[j, n + j] = -1\n                A_eq_pattern[j, n + m + j] = 1\n                b_eq_pattern[j] = 0\n            else:  # Inactive neuron: z_j = 0, h_j = 0\n                # z_j = 0\n                A_ub_pattern[j, n + j] = 1\n                b_ub_pattern[j] = 0\n                # h_j = 0\n                A_eq_pattern[j, n + m + j] = 1\n                b_eq_pattern[j] = 0\n\n        # Combine base and pattern-specific constraints\n        A_eq = np.vstack([A_eq_base, A_eq_pattern])\n        b_eq = np.concatenate([b_eq_base, b_eq_pattern])\n        A_ub = A_ub_pattern\n        b_ub = b_ub_pattern\n\n        # Solve for min_y in this polyhedron.\n        res_min = linprog(c=c_min, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n        \n        # If the polyhedron is feasible (LP succeeds)\n        if res_min.success:\n            # Solve for max_y in the same polyhedron.\n            res_max = linprog(c=c_max, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n            \n            # The minimization of -y gives -max_y. Don't forget to add the bias b2.\n            current_min = res_min.fun + b2\n            current_max = -res_max.fun + b2\n\n            # Update global bounds\n            global_min_y = min(global_min_y, current_min)\n            global_max_y = max(global_max_y, current_max)\n\n    # After checking all patterns, verify safety.\n    is_safe = (global_min_y >= L) and (global_max_y = U)\n\n    # Format output as required.\n    y_min_str = f\"{round(global_min_y, 4):.4f}\"\n    y_max_str = f\"{round(global_max_y, 4):.4f}\"\n\n    return y_min_str, y_max_str, is_safe\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.8, -0.4, 0.6]]),\n            \"b2\": 0.05,\n            \"l\": np.array([-0.5, -0.5]),\n            \"u\": np.array([0.5, 0.5]),\n            \"L\": -1.0, \"U\": 1.0\n        },\n        # Case 2 (larger input box, tighter safety)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.8, -0.4, 0.6]]),\n            \"b2\": 0.05,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"L\": -0.3, \"U\": 0.3\n        },\n        # Case 3 (degenerate output layer yielding constant output)\n        {\n            \"W1\": np.array([[1.0, -2.0], [0.5, 1.0], [-1.5, 0.5]]),\n            \"b1\": np.array([[0.1], [-0.2], [0.3]]),\n            \"W2\": np.array([[0.0, 0.0, 0.0]]),\n            \"b2\": 0.2,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"L\": 0.2, \"U\": 0.2\n        },\n        # Case 4 (mixed signs and asymmetric box)\n        {\n            \"W1\": np.array([[2.0, -1.0], [1.0, 1.0], [-2.0, 0.5]]),\n            \"b1\": np.array([[-0.5], [0.0], [0.2]]),\n            \"W2\": np.array([[-0.6, 0.4, -0.3]]),\n            \"b2\": 0.1,\n            \"l\": np.array([-0.2, -0.2]),\n            \"u\": np.array([0.7, 0.3]),\n            \"L\": -0.5, \"U\": 0.0\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y_min_str, y_max_str, is_safe = analyze_case(\n            case[\"W1\"], case[\"b1\"], case[\"W2\"], case[\"b2\"],\n            case[\"l\"], case[\"u\"], case[\"L\"], case[\"U\"]\n        )\n        all_results.extend([y_min_str, y_max_str, is_safe])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "尽管精确方法提供了理论上的保证，但其计算成本随神经元数量呈指数增长，对于大型网络并不可行，因此我们需要可扩展的过近似（over-approximation）技术。本练习将引导你进入抽象释义（abstract interpretation）的世界，通过网络传播抽象“域”（如区间或更复杂的关系形式）来计算输出的边界。你将实现两种关键方法：简单的区间边界传播（Interval Bound Propagation, IBP）和更精确的、类似DeepPoly的关系型抽象域。通过实现和比较这些方法 ，你将亲身体验验证领域的核心权衡——精度与可扩展性，并学会通过检查“虚假警报”（false alarm）来量化一种方法的保守性。",
            "id": "4243461",
            "problem": "给定一个前馈神经网络，它有两个使用修正线性单元 (ReLU) 激活函数的隐藏层和一个单一的线性输出。该神经网络旨在用作一个信息物理系统控制器的高保真虚拟副本——数字孪生 (Digital Twin) 内部的安全关键组件。目标是使用两种过近似方法计算不确定输入下的抽象输出边界，并通过检查指定安全属性的误报来评估其保守性。\n\n基本原理和定义：\n- 一个前馈神经网络定义了一个函数 $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$，该函数由仿射变换 $z = W a + b$ 和非线性激活函数复合而成。修正线性单元 (ReLU) 定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$，它是单调不减的。\n- 对于任何具有逐元素边界的盒式输入域 $X = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$，$X$ 在仿射映射 $x \\mapsto c^\\top x + d$ 下的像的边界可以通过区间算术和在盒上的符号感知最大化/最小化导出：\n  $$\\max_{x \\in [\\ell, u]} c^\\top x + d = \\sum_{i} \\max(c_i, 0) u_i + \\min(c_i, 0) \\ell_i + d,$$\n  $$\\min_{x \\in [\\ell, u]} c^\\top x + d = \\sum_{i} \\max(c_i, 0) \\ell_i + \\min(c_i, 0) u_i + d.$$\n- 区间边界传播 (IBP) 通过使用符号感知的仿射变换和 $\\mathrm{ReLU}$ 的逐元素单调性，逐层计算前向区间边界。\n- 一个类 DeepPoly 的抽象域为每个神经元追踪一个关于原始输入的下仿射界 $a \\ge \\alpha_L^\\top x + \\beta_L$ 和一个上仿射界 $a \\le \\alpha_U^\\top x + \\beta_U$，并使用 $\\mathrm{ReLU}$ 的可靠线性松弛：\n  - 如果预激活值 $z$ 的区间边界为 $[z_L, z_U]$ 且 $z_U \\le 0$，则 $\\mathrm{ReLU}(z) = 0$ 精确成立。\n  - 如果 $z_L \\ge 0$，则 $\\mathrm{ReLU}(z) = z$ 精确成立。\n  - 如果 $z_L  0  z_U$，则使用标准凸松弛：上界为 $a \\le \\alpha z + \\beta$，其中 $\\alpha = \\frac{z_U}{z_U - z_L}$ 和 $\\beta = -\\alpha z_L$，下界为 $a \\ge 0$。\n\n安全规范：\n- 安全属性要求网络在整个输入盒上的标量输出 $y$ 位于一个固定的区间 $[s_{\\min}, s_{\\max}]$ 内。如果抽象输出边界与 $[s_{\\min}, s_{\\max}]$ 的补集相交，则可能存在安全违例。当一种抽象方法预测可能存在违例，而对输入盒的经验性密集网格评估发现没有任何样本违反 $[s_{\\min}, s_{\\max}]$ 时，就会发生误报。\n\n网络参数：\n- 输入维度为 $n = 2$。每个隐藏层有 $3$ 个神经元。输出维度为 $m = 1$。\n- 第一层权重和偏置：\n  $$W_1 = \\begin{bmatrix} 1.0  -1.0 \\\\ -1.0  1.0 \\\\ 0.2  0.2 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.05 \\end{bmatrix}.$$\n- 第二层权重和偏置：\n  $$W_2 = \\begin{bmatrix} 0.6  -0.6  0.2 \\\\ -0.6  0.6  -0.2 \\\\ 0.3  0.3  0.0 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.$$\n- 输出层权重和偏置：\n  $$W_3 = \\begin{bmatrix} 0.5  -0.4  0.3 \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}.$$\n\n任务：\n1. 实现区间边界传播 (IBP) 来为给定的输入盒 $[\\ell, u]$ 计算输出区间边界 $[y_L^{\\mathrm{IBP}}, y_U^{\\mathrm{IBP}}]$。\n2. 实现一个类 DeepPoly 的抽象，通过为每个神经元维护关于原始输入的下仿射界和上仿射界，使用上述描述的 ReLU 线性松弛和符号感知的仿射复合来计算输出边界。从输出端的这些仿射界，通过盒式最大化/最小化计算输出区间边界 $[y_L^{\\mathrm{DP}}, y_U^{\\mathrm{DP}}]$。\n3. 对于每个测试用例，在输入盒上的一个密集二维网格（每个维度上使用完全 $21$ 个均匀分布的点，总共 $441$ 个样本）上评估实际网络输出，并确定是否有任何样本违反安全区间 $[s_{\\min}, s_{\\max}]$。\n4. 对于每个测试用例和方法，报告是否发生误报。误报定义为一个布尔值，当且仅当该方法的抽象边界预测了潜在的违例（即 $y_L  s_{\\min}$ 或 $y_U > s_{\\max}$），而没有网格样本违反该区间（即所有采样输出都在 $[s_{\\min}, s_{\\max}]$ 内）时，该值为真。\n\n测试套件：\n- 案例 $1$：$\\ell = [-0.1, -0.1]$，$u = [0.1, 0.1]$，$s_{\\min} = -0.05$，$s_{\\max} = 0.05$。\n- 案例 $2$（单点盒）：$\\ell = [0.0, 0.0]$，$u = [0.0, 0.0]$，$s_{\\min} = -0.01$，$s_{\\max} = 0.01$。\n- 案例 $3$（较大的盒）：$\\ell = [-0.5, -0.5]$，$u = [0.5, 0.5]$，$s_{\\min} = -0.5$，$s_{\\max} = 0.5$。\n- 案例 $4$（在中等大小的盒上更紧的安全约束）：$\\ell = [-0.2, -0.2]$，$u = [0.2, 0.2]$，$s_{\\min} = -0.02$，$s_{\\max} = 0.02$。\n\n所需输出：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。对于每个测试用例，按顺序先输出区间边界传播的误报布尔值，然后是类 DeepPoly 的误报布尔值。例如，输出格式必须是：\n$$[\\text{IBP\\_case1}, \\text{DP\\_case1}, \\text{IBP\\_case2}, \\text{DP\\_case2}, \\text{IBP\\_case3}, \\text{DP\\_case3}, \\text{IBP\\_case4}, \\text{DP\\_case4}]$$\n其中每个条目都是一个布尔字面量。",
            "solution": "目标是分析给定前馈神经网络在输入不确定性下的安全性。这涉及到使用两种不同的抽象解释技术：区间边界传播 (IBP) 和类 DeepPoly 抽象，来计算网络输出范围的过近似。然后，我们将通过识别误报来比较这些方法的保守性。误报发生于当一种方法预测了潜在的安全违例，但在输入的密集经验性采样中并未观察到该违例时。\n\n该神经网络是一个函数 $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^1$，具有两个隐藏层。其计算过程如下：\n令输入为 $x \\in \\mathbb{R}^2$。\n第一个隐藏层的输出是 $h_1 = \\mathrm{ReLU}(z_1)$，其中预激活值为 $z_1 = W_1 x + b_1$。这里 $h_1, z_1, b_1 \\in \\mathbb{R}^3$ 且 $W_1 \\in \\mathbb{R}^{3 \\times 2}$。\n第二个隐藏层的输出是 $h_2 = \\mathrm{ReLU}(z_2)$，其中预激活值为 $z_2 = W_2 h_1 + b_2$。这里 $h_2, z_2, b_2 \\in \\mathbb{R}^3$ 且 $W_2 \\in \\mathbb{R}^{3 \\times 3}$。\n最终输出是 $y = z_3$，其中 $z_3 = W_3 h_2 + b_3$。这里 $y, b_3 \\in \\mathbb{R}^1$ 且 $W_3 \\in \\mathbb{R}^{1 \\times 3}$。\n\n输入不确定性由一个盒式域 $X = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$ 建模，其中 $\\ell, u \\in \\mathbb{R}^n$ 是逐元素的下界和上界。\n\n### 任务 1：区间边界传播 (IBP)\n\nIBP 通过从输入到输出逐层传播区间来计算网络输出的区间边界。\n\n令第 $k$ 层激活值的区间边界为 $[\\ell^{(k)}, u^{(k)}]$。到第 $k+1$ 层的传播包括两个步骤：\n\n1.  **仿射变换**：预激活值计算为 $z^{(k+1)} = W^{(k+1)} a^{(k)} + b^{(k+1)}$。每个分量 $z_i^{(k+1)}$ 的边界是通过应用给定公式计算仿射函数在盒上的最小值和最大值来找到的。令 $w_i^\\top$ 为 $W^{(k+1)}$ 的第 $i$ 行，$b_i$ 为 $b^{(k+1)}$ 的第 $i$ 个分量。$z_i^{(k+1)}$ 的边界 $[\\ell_{z,i}^{(k+1)}, u_{z,i}^{(k+1)}]$ 是：\n    $$u_{z,i}^{(k+1)} = \\sum_{j} \\max(w_{ij}, 0) u_j^{(k)} + \\min(w_{ij}, 0) \\ell_j^{(k)} + b_i$$\n    $$\\ell_{z,i}^{(k+1)} = \\sum_{j} \\max(w_{ij}, 0) \\ell_j^{(k)} + \\min(w_{ij}, 0) u_j^{(k)} + b_i$$\n\n2.  **激活函数**：后激活边界 $[\\ell_a^{(k+1)}, u_a^{(k+1)}]$ 是通过将激活函数应用于预激活边界来找到的。由于 $\\mathrm{ReLU}$ 是逐元素且单调不减的，新的边界是：\n    $$\\ell_a^{(k+1)} = \\mathrm{ReLU}(\\ell_z^{(k+1)})$$\n    $$u_a^{(k+1)} = \\mathrm{ReLU}(u_z^{(k+1)})$$\n\n此过程从输入盒 $a^{(0)} \\in [\\ell, u]$ 开始，对每一层重复进行，直到计算出最终的输出边界 $[y_L^{\\mathrm{IBP}}, y_U^{\\mathrm{IBP}}]$。\n\n### 任务 2：类 DeepPoly 抽象\n\n该方法通过为每个神经元追踪一个原始网络输入 $x$ 的仿射函数来提供更紧的边界，该仿射函数从上方和下方界定其激活值。对于一个神经元 $a_i$，我们维护形式为 $a_i \\ge (\\alpha_{i,L})^\\top x + \\beta_{i,L}$ 和 $a_i \\le (\\alpha_{i,U})^\\top x + \\beta_{i,U}$ 的符号边界。\n\n这些符号仿射边界的传播逐层进行：\n\n1.  **输入层**：对于网络输入 $x = (x_1, \\dots, x_n)^\\top$，每个分量 $x_i$ 都被其自身平凡地界定。这表示为 $x_i \\ge e_i^\\top x + 0$ 和 $x_i \\le e_i^\\top x + 0$，其中 $e_i$ 是第 $i$ 个标准基向量。因此，对于输入层，下界 alpha 系数矩阵是单位矩阵，$\\mathbf{A}_L^{(0)} = I$，beta 系数向量是零向量，$\\mathbf{B}_L^{(0)} = \\vec{0}$。类似地，$\\mathbf{A}_U^{(0)} = I$ 和 $\\mathbf{B}_U^{(0)} = \\vec{0}$。\n\n2.  **仿射变换**：给定第 $k-1$ 层激活值的仿射边界，由系数矩阵 $\\mathbf{A}_{L/U}^{(k-1)}$ 和向量 $\\mathbf{B}_{L/U}^{(k-1)}$ 表示，我们计算预激活值 $z^{(k)} = W^{(k)} a^{(k-1)} + b^{(k)}$ 的边界。通过代入 $a^{(k-1)}$ 的仿射边界并重新整理项，我们导出 $z^{(k)}$ 的新仿射系数：\n    令 $(W^{(k)})^+ = \\max(W^{(k)}, 0)$ 和 $(W^{(k)})^- = \\min(W^{(k)}, 0)$ 分别是权重矩阵的逐元素正部和负部。\n    $$\\mathbf{A}_{L,z}^{(k)} = (W^{(k)})^+\\mathbf{A}_L^{(k-1)} + (W^{(k)})^-\\mathbf{A}_U^{(k-1)}$$\n    $$\\mathbf{B}_{L,z}^{(k)} = (W^{(k)})^+\\mathbf{B}_L^{(k-1)} + (W^{(k)})^-\\mathbf{B}_U^{(k-1)} + b^{(k)}$$\n    $$\\mathbf{A}_{U,z}^{(k)} = (W^{(k)})^+\\mathbf{A}_U^{(k-1)} + (W^{(k)})^-\\mathbf{A}_L^{(k-1)}$$\n    $$\\mathbf{B}_{U,z}^{(k)} = (W^{(k)})^+\\mathbf{B}_U^{(k-1)} + (W^{(k)})^-\\mathbf{B}_L^{(k-1)} + b^{(k)}$$\n\n3.  **ReLU 变换**：这是该抽象的核心。对于第 $k$ 层的每个神经元 $j$，我们将预激活仿射边界转换为后激活 ($a_j^{(k)} = \\mathrm{ReLU}(z_j^{(k)})$) 仿射边界。这需要首先通过在输入盒 $[\\ell, u]$ 上最大化/最小化其仿射边界来计算每个预激活神经元 $z_j^{(k)}$ 的具体区间边界 $[z_{j,L}, z_{j,U}]$。基于这些具体边界，我们应用以下规则之一：\n    *   **情况 1：$z_{j,U} \\le 0$ (ReLU 不激活)**。输出恰好为 $0$。新的仿射边界为零：$\\alpha_{j,L/U,a}^{(k)} = \\vec{0}, \\beta_{j,L/U,a}^{(k)} = 0$。\n    *   **情况 2：$z_{j,L} \\ge 0$ (ReLU 激活)**。输出是恒等函数，$a_j^{(k)} = z_j^{(k)}$。$a_j^{(k)}$ 的仿射边界与 $z_j^{(k)}$ 的相同。\n    *   **情况 3：$z_{j,L}  0  z_{j,U}$ (ReLU 不稳定)**。我们使用线性松弛。\n        *   **下界**：问题指定 $a_j^{(k)} \\ge 0$。这对应于仿射边界 $\\alpha_{j,L,a}^{(k)} = \\vec{0}$ 和 $\\beta_{j,L,a}^{(k)} = 0$。\n        *   **上界**：标准凸松弛是一条通过 $(z_{j,L}, 0)$ 和 $(z_{j,U}, z_{j,U})$ 的直线，由 $a_j^{(k)} \\le \\lambda z_j^{(k)} + \\mu$ 给出，其中 $\\lambda = \\frac{z_{j,U}}{z_{j,U} - z_{j,L}}$ 且 $\\mu = -\\lambda z_{j,L}$。由于 $\\lambda > 0$，我们代入 $z_j^{(k)}$ 的上仿射界以获得 $a_j^{(k)}$ 的新上仿射界：\n            $$\\alpha_{j,U,a}^{(k)} = \\lambda \\cdot \\alpha_{j,U,z}^{(k)}$$\n            $$\\beta_{j,U,a}^{(k)} = \\lambda \\cdot \\beta_{j,U,z}^{(k)} + \\mu$$\n\n4.  **最终输出边界**：在通过整个网络传播这些符号边界后，我们获得输出的最终仿射边界，$y \\ge \\alpha_{L,y}^\\top x + \\beta_{L,y}$ 和 $y \\le \\alpha_{U,y}^\\top x + \\beta_{U,y}$。然后通过在输入盒 $[\\ell, u]$ 上最小化下仿射界和最大化上仿射界来找到具体的输出区间 $[y_L^{\\mathrm{DP}}, y_U^{\\mathrm{DP}}]$。\n\n### 任务 3：网格评估\n\n为了获得网络行为的经验性基准真相，我们在输入盒 $[\\ell, u]$ 内的一个密集点网格上对其进行评估。使用在两个输入维度上均匀分布的点构造一个 $21 \\times 21 = 441$ 个点的网格。对于该网格上的每个点 $x_{sample}$，我们计算精确的网络输出 $y_{sample} = f(x_{sample})$。然后我们检查是否有任何样本违反安全属性，即是否有任何 $y_{sample}$ 落在指定的安全区间 $[s_{\\min}, s_{\\max}]$ 之外。这由一个布尔值 `grid_violation` 来量化。\n\n### 任务 4：误报评估\n\n对于给定的验证方法（IBP 或 DeepPoly），误报被定义为这样一种情况：该方法预测了潜在的安全违例，但密集的网格评估没有发现任何违例。\n\n对于每种方法，令其计算出的输出边界为 $[y_L, y_U]$。\n如果抽象边界与不安全区域相交，则预测存在潜在违例：\n`abstract_violation` = $(y_L  s_{\\min})$ 或 $(y_U > s_{\\max})$。\n网格评估的结果是：\n`grid_violation` = 如果有任何 $y_{sample} \\notin [s_{\\min}, s_{\\max}]$，则为 `True`，否则为 `False`。\n\n然后，误报计算为：\n`false_alarm` = `abstract_violation` AND (NOT `grid_violation`)。\n\n对于四个测试用例中的每一个，都对 IBP 和类 DeepPoly 方法遵循此过程，从而产生总共八个布尔值结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# --- Network Parameters ---\nW1 = np.array([[1.0, -1.0], [-1.0, 1.0], [0.2, 0.2]])\nb1 = np.array([0.0, 0.0, 0.05])\n\nW2 = np.array([[0.6, -0.6, 0.2], [-0.6, 0.6, -0.2], [0.3, 0.3, 0.0]])\nb2 = np.array([0.0, 0.0, 0.0])\n\nW3 = np.array([[0.5, -0.4, 0.3]])\nb3 = np.array([0.0])\n\nWs = [W1, W2, W3]\nbs = [b1, b2, b3]\n\ndef relu(x):\n    \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n    return np.maximum(0, x)\n\ndef forward_pass(x, weights, biases):\n    \"\"\"Computes the exact network output for a single input vector.\"\"\"\n    a = x\n    # Hidden layers with ReLU\n    for i in range(len(weights) - 1):\n        z = weights[i] @ a + biases[i]\n        a = relu(z)\n    # Output layer (linear)\n    y = weights[-1] @ a + biases[-1]\n    return y[0]\n\ndef get_affine_bounds(c, d, l, u):\n    \"\"\"Computes min/max of c.T @ x + d over the box [l, u].\"\"\"\n    c_plus = np.maximum(c, 0)\n    c_minus = np.minimum(c, 0)\n    \n    lower_bound = c_plus @ l + c_minus @ u + d\n    upper_bound = c_plus @ u + c_minus @ l + d\n    \n    return lower_bound, upper_bound\n\ndef run_ibp(l, u, weights, biases):\n    \"\"\"Computes output bounds using Interval Bound Propagation.\"\"\"\n    l_current, u_current = l, u\n    \n    # Hidden layers with ReLU\n    for i in range(len(weights) - 1):\n        W, b = weights[i], biases[i]\n        W_plus = np.maximum(W, 0)\n        W_minus = np.minimum(W, 0)\n        \n        l_z = W_plus @ l_current + W_minus @ u_current + b\n        u_z = W_plus @ u_current + W_minus @ l_current + b\n        \n        l_current = relu(l_z)\n        u_current = relu(u_z)\n        \n    # Output layer (linear)\n    W, b = weights[-1], biases[-1]\n    W_plus = np.maximum(W, 0)\n    W_minus = np.minimum(W, 0)\n    \n    y_l = (W_plus @ l_current + W_minus @ u_current + b)[0]\n    y_u = (W_plus @ u_current + W_minus @ l_current + b)[0]\n    \n    return y_l, y_u\n\ndef run_deeppoly(l, u, weights, biases):\n    \"\"\"Computes output bounds using a DeepPoly-like abstraction.\"\"\"\n    n_in = len(l)\n    \n    # Initialize affine bounds for input layer x\n    # alpha_l, alpha_u are (n_neurons, n_in)\n    # beta_l, beta_u are (n_neurons,)\n    alpha_l, alpha_u = np.eye(n_in), np.eye(n_in)\n    beta_l, beta_u = np.zeros(n_in), np.zeros(n_in)\n\n    # Propagate through hidden layers\n    for i in range(len(weights) - 1):\n        W, b = weights[i], biases[i]\n        \n        # 1. Affine transformation\n        W_plus = np.maximum(W, 0)\n        W_minus = np.minimum(W, 0)\n        \n        alpha_lz = W_plus @ alpha_l + W_minus @ alpha_u\n        beta_lz = W_plus @ beta_l + W_minus @ beta_u + b\n        \n        alpha_uz = W_plus @ alpha_u + W_minus @ alpha_l\n        beta_uz = W_plus @ beta_u + W_minus @ beta_l + b\n\n        # 2. ReLU transformation\n        # First, compute concrete bounds for pre-activations\n        z_l, _ = get_affine_bounds(alpha_lz.T, beta_lz, l, u)\n        _, z_u = get_affine_bounds(alpha_uz.T, beta_uz, l, u)\n        \n        n_neurons_layer = W.shape[0]\n        alpha_l_new = np.zeros_like(alpha_lz)\n        beta_l_new = np.zeros_like(beta_lz)\n        alpha_u_new = np.zeros_like(alpha_uz)\n        beta_u_new = np.zeros_like(beta_uz)\n        \n        for j in range(n_neurons_layer):\n            zl_j, zu_j = z_l[j], z_u[j]\n            \n            if zu_j = 0: # ReLU is inactive\n                # Bounds are 0\n                pass # Already initialized to zero\n            elif zl_j >= 0: # ReLU is active\n                alpha_l_new[j] = alpha_lz[j]\n                beta_l_new[j] = beta_lz[j]\n                alpha_u_new[j] = alpha_uz[j]\n                beta_u_new[j] = beta_uz[j]\n            else: # ReLU is unstable\n                # Lower bound: a >= 0\n                # alpha_l_new[j] and beta_l_new[j] remain 0\n                \n                # Upper bound: a = lambda * z + mu\n                lambda_ = zu_j / (zu_j - zl_j)\n                mu = -lambda_ * zl_j\n                \n                # Since lambda > 0, we use upper bound for z\n                alpha_u_new[j] = lambda_ * alpha_uz[j]\n                beta_u_new[j] = lambda_ * beta_uz[j] + mu\n\n        alpha_l, beta_l = alpha_l_new, beta_l_new\n        alpha_u, beta_u = alpha_u_new, beta_u_new\n\n    # Final affine transformation for output layer\n    W_out, b_out = weights[-1], biases[-1]\n    W_out_plus = np.maximum(W_out, 0)\n    W_out_minus = np.minimum(W_out, 0)\n\n    final_alpha_l = W_out_plus @ alpha_l + W_out_minus @ alpha_u\n    final_beta_l = W_out_plus @ beta_l + W_out_minus @ beta_u + b_out\n\n    final_alpha_u = W_out_plus @ alpha_u + W_out_minus @ alpha_l\n    final_beta_u = W_out_plus @ beta_u + W_out_minus @ beta_l + b_out\n\n    # Compute final concrete bounds\n    y_l, _ = get_affine_bounds(final_alpha_l[0], final_beta_l[0], l, u)\n    _, y_u = get_affine_bounds(final_alpha_u[0], final_beta_u[0], l, u)\n    \n    return y_l, y_u\n\ndef grid_evaluation(l, u, weights, biases, s_min, s_max):\n    \"\"\"Performs dense grid evaluation to check for safety violations.\"\"\"\n    grid_points = 21\n    x1_vals = np.linspace(l[0], u[0], grid_points)\n    x2_vals = np.linspace(l[1], u[1], grid_points)\n    \n    has_violation = False\n    for x1 in x1_vals:\n        for x2 in x2_vals:\n            x_sample = np.array([x1, x2])\n            y_sample = forward_pass(x_sample, weights, biases)\n            if not (s_min = y_sample = s_max):\n                has_violation = True\n                break\n        if has_violation:\n            break\n            \n    return has_violation\n\ndef solve():\n    \"\"\"Main function to run test cases and generate final output.\"\"\"\n    test_cases = [\n        # Case 1\n        {'l': np.array([-0.1, -0.1]), 'u': np.array([0.1, 0.1]), 's_min': -0.05, 's_max': 0.05},\n        # Case 2\n        {'l': np.array([0.0, 0.0]), 'u': np.array([0.0, 0.0]), 's_min': -0.01, 's_max': 0.01},\n        # Case 3\n        {'l': np.array([-0.5, -0.5]), 'u': np.array([0.5, 0.5]), 's_min': -0.5, 's_max': 0.5},\n        # Case 4\n        {'l': np.array([-0.2, -0.2]), 'u': np.array([0.2, 0.2]), 's_min': -0.02, 's_max': 0.02},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        l, u, s_min, s_max = case['l'], case['u'], case['s_min'], case['s_max']\n\n        # Run grid evaluation to find \"ground truth\"\n        grid_has_violation = grid_evaluation(l, u, Ws, bs, s_min, s_max)\n        \n        # --- IBP Analysis ---\n        y_l_ibp, y_u_ibp = run_ibp(l, u, Ws, bs)\n        ibp_predicts_violation = (y_l_ibp  s_min) or (y_u_ibp > s_max)\n        ibp_false_alarm = ibp_predicts_violation and not grid_has_violation\n        results.append(str(ibp_false_alarm))\n        \n        # --- DeepPoly Analysis ---\n        y_l_dp, y_u_dp = run_deeppoly(l, u, Ws, bs)\n        dp_predicts_violation = (y_l_dp  s_min) or (y_u_dp > s_max)\n        dp_false_alarm = dp_predicts_violation and not grid_has_violation\n        results.append(str(dp_false_alarm))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "验证的最终目标通常是确保整个系统的安全，而不仅仅是孤立的神经网络。此练习将一个神经网络控制器置于一个与线性“被控对象”（plant）组成的反馈回路中，这是信息物理系统（cyber-physical system, CPS）中的一个典型场景。我们将使用一种强大的几何抽象域——区域多胞体（zonotope），它非常适合用于分析线性动态系统的可达集。你将实现区域多胞体在有限时间范围内，如何通过被控对象动态和ReLU网络控制器进行传播。这个总结性的练习  展示了如何将验证技术应用于动态闭环系统，为你从理论性的神经网络验证到实际的CPS安全分析之间架起一座关键的桥梁。",
            "id": "4243426",
            "problem": "考虑一个离散时间信息物理系统（CPS）线性被控对象，该对象由一个前馈修正线性单元（ReLU）神经网络（NN）控制器控制，放在数字孪生和信息物理系统安全性分析的背景下。该被控对象根据离散时间线性模型和有界过程扰动进行演化：\n$$\nx_{k+1} = A x_k + B u_k + w_k,\n$$\n其中 $x_k \\in \\mathbb{R}^n$ 是离散时间 $k$ 的状态，$u_k \\in \\mathbb{R}^m$ 是由 NN 控制器产生的控制输入，$A \\in \\mathbb{R}^{n \\times n}$ 是被控对象的状态转移矩阵，$B \\in \\mathbb{R}^{n \\times m}$ 是输入矩阵，而 $w_k$ 是一个有界扰动，被建模为带状多胞体的一个成员。该 NN 控制器是一个单隐藏层的 ReLU 网络，定义为\n$$\nu_k = W_2 \\, \\sigma\\!\\left(W_1 x_k + b_1\\right) + b_2,\n$$\n其中 $W_1 \\in \\mathbb{R}^{h \\times n}$ 和 $b_1 \\in \\mathbb{R}^{h}$ 是第一层的权重和偏置，$W_2 \\in \\mathbb{R}^{m \\times h}$ 和 $b_2 \\in \\mathbb{R}^{m}$ 是输出层的权重和偏置，$h$ 是隐藏单元的数量，$\\sigma(\\cdot)$ 是按元素应用的修正线性单元激活函数，定义为 $\\sigma(z)_i = \\max\\{0, z_i\\}$。\n\n初始状态集和扰动集均表示为带状多胞体。在 $\\mathbb{R}^n$ 中的一个带状多胞体定义为\n$$\n\\mathcal{Z}(c, G) = \\left\\{x \\in \\mathbb{R}^n \\; \\bigg| \\; x = c + G \\beta, \\; \\|\\beta\\|_\\infty \\le 1 \\right\\},\n$$\n其中 $c \\in \\mathbb{R}^n$ 是中心，$G \\in \\mathbb{R}^{n \\times p}$ 是生成矩阵，$p$ 是生成元的数量。扰动带状多胞体 $\\mathcal{W} = \\mathcal{Z}(c_w, G_w)$ 假设以原点为中心，即 $c_w = 0$。\n\n安全性要求是一个轴对齐的安全集，定义为超矩形\n$$\nS = \\left\\{x \\in \\mathbb{R}^n \\; \\big| \\; -s_i \\le x_i \\le s_i \\text{ for all } i \\in \\{1, \\dots, n\\}\\right\\},\n$$\n对于给定的向量 $s \\in \\mathbb{R}^n_{>0}$。任务是使用带状多胞体传播方法，为闭环系统计算在有限时间域 $H \\in \\mathbb{N}$ 上的时间有界可达集，并返回在每一步 $k \\in \\{0,1,\\dots,H\\}$ 的可达集是否都包含在安全集 $S$ 内。\n\n必须实现的传播规则：\n- 带状多胞体的仿射变换：对于 $y = M x + b$，如果 $x \\in \\mathcal{Z}(c, G)$，那么 $y \\in \\mathcal{Z}(M c + b, M G)$。\n- 带状多胞体的加法：如果 $x \\in \\mathcal{Z}(c_x, G_x)$ 且 $y \\in \\mathcal{Z}(c_y, G_y)$，那么 $x + y \\in \\mathcal{Z}(c_x + c_y, [G_x \\; G_y])$，其中 $[G_x \\; G_y]$ 表示生成元的水平拼接。\n- 对带状多胞体的 ReLU 映射将按如下方式进行过近似。对于 $\\mathbb{R}^h$ 中的一个激活前带状多胞体 $\\mathcal{Z}(c, G)$，通过以下方式计算每个坐标 $i$ 的区间下界和上界\n$$\n\\ell_i = c_i - \\sum_{j=1}^p |G_{i,j}|, \\quad u_i = c_i + \\sum_{j=1}^p |G_{i,j}|.\n$$\n然后按坐标为激活后的 $y = \\sigma(z)$ 定义一个过近似带状多胞体：\n1. 如果 $\\ell_i \\ge 0$，则保持第 $i$ 行不变（精确线性区域）。\n2. 如果 $u_i \\le 0$，则将第 $i$ 个输出恒定设置为零（非激活区域），即中心坐标为 $0$ 且第 $i$ 行的生成元为零。\n3. 如果 $\\ell_i  0  u_i$，则对第 $i$ 个坐标用一个轴对齐的区间 $[0, u_i]$ 进行过近似：将第 $i$ 个中心条目设为 $u_i/2$，将第 $i$ 行的现有生成元置零，并追加一个新的生成元列，该列在第 $i$ 行有一个值为 $u_i/2$ 的非零条目（不相关盒式过近似）。\n\n使用以上规则计算直到 $H$ 的每个 $k$ 的时间有界可达带状多胞体 $\\mathcal{Z}_k$：\n$$\n\\mathcal{Z}_{k+1} = A \\,\\mathcal{Z}_k + B \\,\\mathcal{U}(\\mathcal{Z}_k) + \\mathcal{W},\n$$\n其中 $\\mathcal{U}(\\mathcal{Z}_k)$ 是由当前状态带状多胞体 $\\mathcal{Z}_k$ 引出的 NN 控制器输出的带状多胞体过近似。在每个时间 $k$，使用带状多胞体的区间界测试是否 $\\mathcal{Z}_k \\subseteq S$。\n\n你的程序必须实现上述过程，并评估以下测试套件。所有值都以精确形式给出；请完全按照规定实现它们。\n\n测试用例 1（标称稳定动态，小不确定性）：\n- 状态维度 $n = 2$，控制维度 $m = 1$，隐藏层大小 $h = 2$，时间域 $H = 5$。\n- 被控对象矩阵：\n$$\nA = \\begin{bmatrix} 0.98  0.05 \\\\ 0  0.95 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.10 \\\\ 0.05 \\end{bmatrix}.\n$$\n- 控制器参数：\n$$\nW_1 = \\begin{bmatrix} 0.60  -0.25 \\\\ 0.20  0.40 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ -0.05 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.70  -0.30 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- 初始状态带状多胞体：\n$$\nc_0 = \\begin{bmatrix} 0.10 \\\\ -0.05 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.05  0.00 \\\\ 0.00  0.05 \\end{bmatrix}.\n$$\n- 扰动带状多胞体（中心为零）：\n$$\nG_w = \\begin{bmatrix} 0.01  0.00 \\\\ 0.00  0.01 \\end{bmatrix}.\n$$\n- 安全集半宽度：\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\n测试用例 2（近边界初始条件，中等不确定性）：\n- 维度 $n = 2$, $m = 1$, $h = 2$, 时间域 $H = 4$。\n- 被控对象矩阵：\n$$\nA = \\begin{bmatrix} 0.98  0.05 \\\\ 0  0.95 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.10 \\\\ 0.05 \\end{bmatrix}.\n$$\n- 控制器参数：\n$$\nW_1 = \\begin{bmatrix} 0.60  -0.25 \\\\ 0.20  0.40 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ -0.05 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.70  -0.30 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- 初始状态带状多胞体：\n$$\nc_0 = \\begin{bmatrix} 0.45 \\\\ 0.45 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.02  0.00 \\\\ 0.00  0.02 \\end{bmatrix}.\n$$\n- 扰动带状多胞体：\n$$\nG_w = \\begin{bmatrix} 0.005  0.00 \\\\ 0.00  0.005 \\end{bmatrix}.\n$$\n- 安全集半宽度：\n$$\ns = \\beginbmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\n测试用例 3（轻度不稳定动态，较大扰动）：\n- 维度 $n = 2$, $m = 1$, $h = 2$, 时间域 $H = 5$。\n- 被控对象矩阵：\n$$\nA = \\begin{bmatrix} 1.20  0.00 \\\\ 0.00  1.10 \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 0.05 \\\\ 0.02 \\end{bmatrix}.\n$$\n- 控制器参数：\n$$\nW_1 = \\begin{bmatrix} 0.50  -0.20 \\\\ 0.30  0.30 \\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}, \\quad\nW_2 = \\begin{bmatrix} 0.60  -0.10 \\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix} 0.00 \\end{bmatrix}.\n$$\n- 初始状态带状多胞体：\n$$\nc_0 = \\begin{bmatrix} 0.20 \\\\ 0.20 \\end{bmatrix}, \\quad\nG_0 = \\begin{bmatrix} 0.02  0.00 \\\\ 0.00  0.02 \\end{bmatrix}.\n$$\n- 扰动带状多胞体：\n$$\nG_w = \\begin{bmatrix} 0.05  0.00 \\\\ 0.00  0.05 \\end{bmatrix}.\n$$\n- 安全集半宽度：\n$$\ns = \\begin{bmatrix} 0.50 \\\\ 0.50 \\end{bmatrix}.\n$$\n\n你的程序必须：\n1. 对于每个测试用例，在指定的时间域 $H$ 上，使用上述的仿射和 ReLU 过近似方法，为闭环系统实现带状多胞体传播。\n2. 在每个时间 $k \\in \\{0,1,\\dots,H\\}$，通过以下方式计算当前状态带状多胞体的区间界\n$$\n\\ell = c - \\sum_{j=1}^p |G_{:,j}|, \\quad u = c + \\sum_{j=1}^p |G_{:,j}|,\n$$\n并检查对于所有的 $i$，$\\ell_i \\ge -s_i$ 和 $u_i \\le s_i$ 是否成立。\n3. 每个测试用例返回一个布尔值，指示系统在直到 $H$ 的所有时间步长是否都是安全的。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含三个测试用例的结果，形式为方括号括起来的逗号分隔列表（例如，`[True,False,True]`）。不应打印任何其他文本。",
            "solution": "用户提供的问题已被分析并被认为是有效的，因为它具有科学依据、提法得当且内容自洽。任务是为一个由修正线性单元（ReLU）神经网络（NN）控制的离散时间信息物理系统（CPS）执行安全性验证。这涉及到使用基于带状多胞体的过近似方法，计算系统在有限时间域内的前向可达集。\n\n该方法的核心在于通过系统动态传播带状多胞体，系统动态包括线性变换和非线性的 ReLU 激活函数。一个带状多胞体 $\\mathcal{Z}(c, G)$ 是一个由中心 $c \\in \\mathbb{R}^n$ 和生成矩阵 $G \\in \\mathbb{R}^{n \\times p}$ 定义的点集，形式为 $\\mathcal{Z}(c, G) = \\{c + G \\beta \\mid \\|\\beta\\|_\\infty \\le 1\\}$。\n\n整体验证过程如下：从初始状态带状多胞体 $\\mathcal{Z}_0 = \\mathcal{Z}(c_0, G_0)$ 开始，我们对 $k = 0, 1, \\dots, H-1$ 迭代计算可达状态带状多胞体 $\\mathcal{Z}_{k+1}$。在每一步 $k \\in \\{0, 1, \\dots, H\\}$，我们验证当前的可达集 $\\mathcal{Z}_k$ 是否包含在指定的安全集 $S$ 内。\n\n从 $\\mathcal{Z}_k$ 计算后续可达集 $\\mathcal{Z}_{k+1}$ 的过程涉及几个步骤，这些步骤基于闭环系统动态 $x_{k+1} = A x_k + B u_k + w_k$，其中 $u_k$ 是 NN 控制器的输出。\n\n1.  **步骤 $k$ 的安全性验证**：在传播之前，我们检查 $\\mathcal{Z}_k = \\mathcal{Z}(c_k, G_k)$ 是否完全位于安全集 $S = \\{x \\mid -s_i \\le x_i \\le s_i\\}$ 内。这通过计算紧密包围该带状多胞体的轴对齐超矩形来实现。每个维度 $i$ 的下界和上界由以下公式给出：\n    $$\n    \\ell_i = c_{k,i} - \\sum_{j=1}^{p} |G_{k,i,j}|\n    $$\n    $$\n    u_i = c_{k,i} + \\sum_{j=1}^{p} |G_{k,i,j}|\n    $$\n    如果对于所有维度 $i=1, \\dots, n$ 都有 $\\ell_i \\ge -s_i$ 且 $u_i \\le s_i$，则满足安全条件。如果在任何步骤 $k$ 违反了此条件，则系统被视为不安全，当前测试用例的验证将以 `False` 结果终止。\n\n2.  **NN 控制器输出过近似**：我们将当前状态带状多胞体 $\\mathcal{Z}_k$ 传播通过 NN 控制器，以获得控制输入集 $\\mathcal{U}_k$ 的一个过近似。\n    a.  **第一个仿射层**：隐藏层的激活前值形成一个带状多胞体 $\\mathcal{Z}_{pre}$。给定 $\\mathcal{Z}_k = \\mathcal{Z}(c_k, G_k)$，变换为 $z = W_1 x_k + b_1$。使用带状多胞体的仿射变换规则，我们得到：\n        $$\n        \\mathcal{Z}_{pre} = \\mathcal{Z}(W_1 c_k + b_1, W_1 G_k)\n        $$\n    b.  **ReLU 激活过近似**：将按元素的 ReLU 函数 $\\sigma(\\cdot)$ 应用于 $\\mathcal{Z}_{pre} = \\mathcal{Z}(c_{pre}, G_{pre})$。通过对 $h$ 个隐藏神经元中每一个的 ReLU 输出进行过近似，构造一个新的带状多胞体 $\\mathcal{Z}_{post} = \\mathcal{Z}(c_{post}, G_{post})$。对于每个神经元 $i \\in \\{1, \\dots, h\\}$，我们首先计算其激活前的区间界 $[\\ell_i, u_i]$。\n        - 如果 $\\ell_i \\ge 0$（总是激活），神经元的输出是线性的。$c_{post}$ 和 $G_{post}$ 的第 $i$ 行与 $c_{pre}$ 和 $G_{pre}$ 的相同。\n        - 如果 $u_i \\le 0$（总是不激活），神经元的输出为零。$c_{post}$ 和 $G_{post}$ 的第 $i$ 行被设为零。\n        - 如果 $\\ell_i  0  u_i$（不确定），输出位于 $[0, u_i]$ 内。这通过为该维度创建一个新的带状多胞体进行过近似，其中心为 $u_i/2$，并带有一个大小为 $u_i/2$ 的新的独立生成元。这会向生成矩阵 $G_{post}$ 添加一列。\n    c.  **第二个仿射层**：最终的控制输入带状多胞体 $\\mathcal{U}_k = \\mathcal{Z}(c_u, G_u)$ 是通过将输出层变换 $u_k = W_2 y_k + b_2$ 应用于 $\\mathcal{Z}_{post}$ 计算得出的：\n        $$\n        \\mathcal{U}_k = \\mathcal{Z}(W_2 c_{post} + b_2, W_2 G_{post})\n        $$\n\n3.  **被控对象动态传播**：下一个时间步的状态带状多胞体 $\\mathcal{Z}_{k+1}$ 是三个带状多胞体的闵可夫斯基和：被控对象动态对当前状态的影响、控制输入的影响以及过程扰动。\n    - $A \\mathcal{Z}_k = \\mathcal{Z}(A c_k, A G_k)$\n    - $B \\mathcal{U}_k = \\mathcal{Z}(B c_u, B G_u)$\n    - $\\mathcal{W} = \\mathcal{Z}(0, G_w)$\n    \n    使用闵可夫斯基和规则，即对中心求和并拼接生成矩阵，我们得到 $\\mathcal{Z}_{k+1} = \\mathcal{Z}(c_{k+1}, G_{k+1})$：\n    $$\n    c_{k+1} = A c_k + B c_u\n    $$\n    $$\n    G_{k+1} = [A G_k, B G_u, G_w]\n    $$\n\n对 $k = 0, \\dots, H-1$ 重复此过程。如果所有步骤 $k=0, \\dots, H$ 的安全性检查都通过，则系统在给定的时间域内被宣布为安全，结果为 `True`。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 5,\n            \"A\": np.array([[0.98, 0.05], [0.0, 0.95]]),\n            \"B\": np.array([[0.10], [0.05]]),\n            \"W1\": np.array([[0.60, -0.25], [0.20, 0.40]]),\n            \"b1\": np.array([[0.00], [-0.05]]),\n            \"W2\": np.array([[0.70, -0.30]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.10], [-0.05]]),\n            \"G0\": np.array([[0.05, 0.00], [0.00, 0.05]]),\n            \"Gw\": np.array([[0.01, 0.00], [0.00, 0.01]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n        # Test Case 2\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 4,\n            \"A\": np.array([[0.98, 0.05], [0.0, 0.95]]),\n            \"B\": np.array([[0.10], [0.05]]),\n            \"W1\": np.array([[0.60, -0.25], [0.20, 0.40]]),\n            \"b1\": np.array([[0.00], [-0.05]]),\n            \"W2\": np.array([[0.70, -0.30]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.45], [0.45]]),\n            \"G0\": np.array([[0.02, 0.00], [0.00, 0.02]]),\n            \"Gw\": np.array([[0.005, 0.00], [0.00, 0.005]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n        # Test Case 3\n        {\n            \"n\": 2, \"m\": 1, \"h\": 2, \"H\": 5,\n            \"A\": np.array([[1.20, 0.00], [0.00, 1.10]]),\n            \"B\": np.array([[0.05], [0.02]]),\n            \"W1\": np.array([[0.50, -0.20], [0.30, 0.30]]),\n            \"b1\": np.array([[0.00], [0.00]]),\n            \"W2\": np.array([[0.60, -0.10]]),\n            \"b2\": np.array([[0.00]]),\n            \"c0\": np.array([[0.20], [0.20]]),\n            \"G0\": np.array([[0.02, 0.00], [0.00, 0.02]]),\n            \"Gw\": np.array([[0.05, 0.00], [0.00, 0.05]]),\n            \"s\": np.array([[0.50], [0.50]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        results.append(run_verification(params))\n    \n    # Format the print statement exactly as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef relu_approx(zono):\n    \"\"\"\n    Over-approximates the ReLU function for a given zonotope.\n    zono: A tuple (c, G) representing the zonotope.\n    \"\"\"\n    c, G = zono\n    h = c.shape[0]\n\n    c_post = np.zeros_like(c)\n    G_base = np.zeros_like(G)\n    new_generators = []\n\n    # Calculate interval bounds for each neuron's pre-activation\n    if G.shape[1] > 0:\n        interval_radii = np.sum(np.abs(G), axis=1, keepdims=True)\n    else:\n        interval_radii = np.zeros_like(c)\n        \n    l_bounds = c - interval_radii\n    u_bounds = c + interval_radii\n\n    for i in range(h):\n        l_i, u_i = l_bounds[i, 0], u_bounds[i, 0]\n        \n        if l_i >= 0:  # Case 1: Always active\n            c_post[i] = c[i]\n            G_base[i, :] = G[i, :]\n        elif u_i = 0:  # Case 2: Always inactive\n            c_post[i] = 0\n            G_base[i, :] = 0\n        else:  # Case 3: Uncertain\n            c_post[i] = u_i / 2.0\n            G_base[i, :] = 0  # Zero out existing generators for this dimension\n            new_gen = np.zeros((h, 1))\n            new_gen[i] = u_i / 2.0\n            new_generators.append(new_gen)\n\n    if new_generators:\n        G_new = np.hstack(new_generators)\n        if G_base.shape[1] > 0:\n            G_post = np.hstack((G_base, G_new))\n        else:\n            G_post = G_new\n    else:\n        G_post = G_base\n\n    return (c_post, G_post)\n\ndef run_verification(params):\n    \"\"\"\n    Runs the zonotope-based reachability analysis for a single test case.\n    \"\"\"\n    H = params[\"H\"]\n    A, B = params[\"A\"], params[\"B\"]\n    W1, b1 = params[\"W1\"], params[\"b1\"]\n    W2, b2 = params[\"W2\"], params[\"b2\"]\n    Gw = params[\"Gw\"]\n    s = params[\"s\"]\n\n    current_zono = (params[\"c0\"], params[\"G0\"])\n\n    for k in range(H + 1):\n        # 1. Safety Check\n        c_k, G_k = current_zono\n        if G_k.shape[1] > 0:\n            interval_radii = np.sum(np.abs(G_k), axis=1, keepdims=True)\n        else:\n            interval_radii = np.zeros_like(c_k)\n            \n        lower_bounds = c_k - interval_radii\n        upper_bounds = c_k + interval_radii\n        \n        if not np.all((lower_bounds >= -s)  (upper_bounds = s)):\n            return False\n\n        # If it's the last step, no need to compute the next state\n        if k == H:\n            break\n\n        # 2. Propagate to compute next state zonotope\n        \n        # 2a. Controller output over-approximation\n        # First layer (affine)\n        c_pre = W1 @ c_k + b1\n        G_pre = W1 @ G_k\n        zono_pre = (c_pre, G_pre)\n        \n        # ReLU activation\n        c_post, G_post = relu_approx(zono_pre)\n\n        # Output layer (affine)\n        c_u = W2 @ c_post + b2\n        G_u = W2 @ G_post\n        \n        # 2b. Plant dynamics\n        # Term 1: A * x_k\n        c_ax = A @ c_k\n        G_ax = A @ G_k\n        \n        # Term 2: B * u_k\n        c_bu = B @ c_u\n        G_bu = B @ G_u\n\n        # Term 3: w_k (disturbance) is already a zonotope with center 0\n        \n        # 2c. Minkowski Sum for x_{k+1}\n        c_next = c_ax + c_bu\n        G_next = np.hstack([G_ax, G_bu, Gw])\n        \n        current_zono = (c_next, G_next)\n\n    return True\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}