{
    "hands_on_practices": [
        {
            "introduction": "Global Sensitivity Analysis often begins with 'screening' methods to efficiently identify the most influential parameters in a complex model. This exercise focuses on interpreting the results of the Morris method, a popular screening technique. By analyzing its two key outputs, the mean effect $\\mu^*$ and the standard deviation $\\sigma$, you will learn to distinguish between parameters that have a strong, linear influence and those whose effects are highly nonlinear or interactive.",
            "id": "1436441",
            "problem": "A team of bioengineers is analyzing a synthetic gene regulatory network designed to produce a specific therapeutic protein. The steady-state concentration of this protein is the key output of their mathematical model. To understand the system's behavior, they perform a Morris screening, which is a method for Global Sensitivity Analysis (GSA). This method calculates two sensitivity indices for each model parameter:\n1.  $\\mu^*$ (mu-star), which represents the mean of the absolute values of the \"elementary effects\" for a parameter. This index measures the overall or \"main\" effect of a parameter on the model output. A higher $\\mu^*$ indicates greater influence.\n2.  $\\sigma$ (sigma), which represents the standard deviation of the elementary effects. This index measures the extent of non-linear effects or interactions the parameter has with other parameters. A higher $\\sigma$ suggests a more complex influence.\n\nThe team has analyzed five key parameters of their model and obtained the following sensitivity indices:\n\n*   **Parameter $p_1$ (Promoter Strength):** $\\mu^* = 0.12$, $\\sigma = 0.15$\n*   **Parameter $p_2$ (Ribosome Binding Site Strength):** $\\mu^* = 1.85$, $\\sigma = 0.25$\n*   **Parameter $p_3$ (Protein Degradation Rate):** $\\mu^* = 0.95$, $\\sigma = 0.30$\n*   **Parameter $p_4$ (Allosteric Inhibition Constant):** $\\mu^* = 1.60$, $\\sigma = 2.10$\n*   **Parameter $p_5$ (mRNA Half-life):** $\\mu^* = 0.08$, $\\sigma = 0.07$\n\nBased on this analysis, which parameter has an influence on the protein concentration that is both strong and characterized by a high degree of non-linearity or interaction with other parameters?\n\nA. $p_1$\n\nB. $p_2$\n\nC. $p_3$\n\nD. $p_4$\n\nE. $p_5$",
            "solution": "The Morris screening method provides two indices per parameter: the main effect magnitude $\\mu^{*}$ and the dispersion $\\sigma$ capturing nonlinearity and interactions. The problem asks for the parameter whose influence is both strong and highly nonlinear or interactive, which corresponds to simultaneously large $\\mu^{*}$ and large $\\sigma$.\n\nThe given pairs $(\\mu^{*},\\sigma)$ are: for $p_{1}$, $(0.12,0.15)$; for $p_{2}$, $(1.85,0.25)$; for $p_{3}$, $(0.95,0.30)$; for $p_{4}$, $(1.60,2.10)$; for $p_{5}$, $(0.08,0.07)$. Ranking by $\\mu^{*}$ gives $p_{2}$ largest, then $p_{4}$, then $p_{3}$, with $p_{1}$ and $p_{5}$ much smaller. Ranking by $\\sigma$ gives $p_{4}$ overwhelmingly largest, with the others $p_{3}$, $p_{2}$, $p_{1}$, and $p_{5}$ all comparatively small.\n\nTo satisfy both criteria simultaneously, we require a parameter with large $\\mu^{*}$ and large $\\sigma$. Parameter $p_{2}$, while having the largest $\\mu^{*}$, has a relatively small $\\sigma=0.25$, indicating weak nonlinearity or interactions. Parameter $p_{4}$ has a large $\\mu^{*}=1.60$ (second largest) and the largest $\\sigma=2.10$ by a wide margin, indicating strong influence and high nonlinearity/interaction. No other parameter exhibits both a large $\\mu^{*}$ and a large $\\sigma$.\n\nTherefore, the parameter meeting the stated criterion is $p_{4}$, corresponding to option D.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "To truly master Global Sensitivity Analysis, it is essential to understand its foundations in probability theory. This problem strips GSA down to its core by asking you to analytically compute the output variance for a simple nonlinear model. By deriving how the total variance is composed of contributions from each input, you will gain a fundamental appreciation for how variance-based methods rigorously account for different types of parameter influences.",
            "id": "4225410",
            "problem": "In a Digital Twin for a Cyber-Physical System (CPS), consider a simplified uncertainty propagation module in which a normalized performance indicator is modeled as the sum of a nonlinear transformation of a control deviation and an exogenous normalized load. Let the model output be $Y = X_{1}^{2} + X_{2}$ where the inputs are independent and distributed as $X_{1} \\sim \\mathrm{Unif}(0,1)$ and $X_{2} \\sim \\mathrm{Unif}(0,1)$. \n\n- Using only fundamental definitions from probability (definitions of expectation, variance, independence, and basic integral calculus), derive an exact analytic expression for $\\mathrm{Var}(Y)$.\n- Then, using the foundational definition of variance-based Global Sensitivity Analysis (GSA), explain qualitatively how a variance-based decomposition attributes sensitivity in the presence of nonlinearity in $X_{1}$ and linearity in $X_{2}$, and why this decomposition is well suited for nonlinear models used in digital twins of CPS.\n\nExpress the final answer as the exact value of $\\mathrm{Var}(Y)$ in a single simplified rational form. No rounding is required.",
            "solution": "The problem is solved in two parts as requested. First, the exact analytic expression for $\\mathrm{Var}(Y)$ is derived. Second, a qualitative explanation of variance-based Global Sensitivity Analysis (GSA) for the given model is provided.\n\n**Part 1: Derivation of $\\mathrm{Var}(Y)$**\n\nThe model output is given by the equation $Y = X_{1}^{2} + X_{2}$. The inputs $X_{1}$ and $X_{2}$ are independent random variables, both distributed uniformly on the interval $[0, 1]$.\n$X_{1} \\sim \\mathrm{Unif}(0,1)$\n$X_{2} \\sim \\mathrm{Unif}(0,1)$\n\nThe probability density function (PDF) for a random variable $X \\sim \\mathrm{Unif}(0,1)$ is:\n$$f(x) = \\begin{cases} 1  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}$$\n\nThe variance of $Y$, denoted $\\mathrm{Var}(Y)$, can be computed using the property of variances for sums of independent random variables. Because $X_{1}$ and $X_{2}$ are independent, any function of $X_{1}$ (such as $X_{1}^{2}$) is also independent of any function of $X_{2}$ (such as $X_{2}$ itself). Therefore, the random variables $X_{1}^{2}$ and $X_{2}$ are independent.\n\nFor two independent random variables $A$ and $B$, the variance of their sum is the sum of their variances: $\\mathrm{Var}(A + B) = \\mathrm{Var}(A) + \\mathrm{Var}(B)$.\nApplying this to our model:\n$$\\mathrm{Var}(Y) = \\mathrm{Var}(X_{1}^{2} + X_{2}) = \\mathrm{Var}(X_{1}^{2}) + \\mathrm{Var}(X_{2})$$\n\nWe will now compute each term separately using the fundamental definition of variance, $\\mathrm{Var}(Z) = E[Z^2] - (E[Z])^2$, where $E[\\cdot]$ denotes the expectation operator.\n\nFirst, we compute $\\mathrm{Var}(X_{2})$.\nThe expectation of $X_{2}$ is:\n$$E[X_{2}] = \\int_{-\\infty}^{\\infty} x_{2} f(x_{2}) \\,dx_{2} = \\int_{0}^{1} x_{2} \\cdot 1 \\,dx_{2} = \\left[ \\frac{x_{2}^{2}}{2} \\right]_{0}^{1} = \\frac{1}{2}$$\nThe expectation of $X_{2}^{2}$ is:\n$$E[X_{2}^{2}] = \\int_{0}^{1} x_{2}^{2} \\cdot 1 \\,dx_{2} = \\left[ \\frac{x_{2}^{3}}{3} \\right]_{0}^{1} = \\frac{1}{3}$$\nThe variance of $X_{2}$ is therefore:\n$$\\mathrm{Var}(X_{2}) = E[X_{2}^{2}] - (E[X_{2}])^{2} = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^{2} = \\frac{1}{3} - \\frac{1}{4} = \\frac{4-3}{12} = \\frac{1}{12}$$\n\nNext, we compute $\\mathrm{Var}(X_{1}^{2})$. Let the random variable $Z = X_{1}^{2}$. We need to compute $\\mathrm{Var}(Z) = E[Z^2] - (E[Z])^2 = E[(X_1^2)^2] - (E[X_1^2])^2 = E[X_1^4] - (E[X_1^2])^2$.\nThe expectation of $X_{1}^{2}$ is:\n$$E[X_{1}^{2}] = \\int_{0}^{1} x_{1}^{2} \\cdot 1 \\,dx_{1} = \\left[ \\frac{x_{1}^{3}}{3} \\right]_{0}^{1} = \\frac{1}{3}$$\nThe expectation of $X_{1}^{4}$ is:\n$$E[X_{1}^{4}] = \\int_{0}^{1} x_{1}^{4} \\cdot 1 \\,dx_{1} = \\left[ \\frac{x_{1}^{5}}{5} \\right]_{0}^{1} = \\frac{1}{5}$$\nThe variance of $X_{1}^{2}$ is therefore:\n$$\\mathrm{Var}(X_{1}^{2}) = E[X_{1}^{4}] - (E[X_{1}^{2}])^{2} = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^{2} = \\frac{1}{5} - \\frac{1}{9} = \\frac{9-5}{45} = \\frac{4}{45}$$\n\nFinally, we sum the two variances to find $\\mathrm{Var}(Y)$:\n$$\\mathrm{Var}(Y) = \\mathrm{Var}(X_{1}^{2}) + \\mathrm{Var}(X_{2}) = \\frac{4}{45} + \\frac{1}{12}$$\nTo sum these fractions, we find a common denominator, which is the least common multiple of $45$ and $12$. $45 = 3^2 \\cdot 5$ and $12 = 2^2 \\cdot 3$. The LCM is $2^2 \\cdot 3^2 \\cdot 5 = 4 \\cdot 9 \\cdot 5 = 180$.\n$$\\mathrm{Var}(Y) = \\frac{4 \\cdot 4}{45 \\cdot 4} + \\frac{1 \\cdot 15}{12 \\cdot 15} = \\frac{16}{180} + \\frac{15}{180} = \\frac{31}{180}$$\n\n**Part 2: Qualitative Explanation of Variance-Based GSA**\n\nThe foundational principle of variance-based Global Sensitivity Analysis (GSA), often known as the Sobol method, is the decomposition of the total variance of the model output, $\\mathrm{Var}(Y)$, into contributions from each input factor and their interactions. For a model $Y=f(X_1, X_2, \\dots, X_k)$, the decomposition is:\n$$\\mathrm{Var}(Y) = \\sum_{i} V_i + \\sum_{ij} V_{ij} + \\dots + V_{12...k}$$\nwhere $V_i = \\mathrm{Var}_{X_i}(E_{X_{\\sim i}}[Y | X_i])$ is the first-order effect of $X_i$ (variance of the conditional expectation), and $V_{ij}$ and higher-order terms represent interaction effects. The first-order sensitivity index is $S_i = V_i / \\mathrm{Var}(Y)$.\n\nIn the specific problem $Y = X_{1}^{2} + X_{2}$, the model is additive in functions of the inputs, i.e., $Y = f_1(X_1) + f_2(X_2)$ where $f_1(X_1) = X_1^2$ and $f_2(X_2) = X_2$. For a purely additive model with independent inputs, all interaction effects are zero ($V_{12}=0$). The variance decomposition simplifies to:\n$$\\mathrm{Var}(Y) = V_1 + V_2 = \\mathrm{Var}(f_1(X_1)) + \\mathrm{Var}(f_2(X_2)) = \\mathrm{Var}(X_{1}^{2}) + \\mathrm{Var}(X_{2})$$\nThis confirms that our calculation in Part 1 is consistent with the GSA framework. The variance is additively composed of the variance contributed by the nonlinear term $X_1^2$ and the linear term $X_2$.\n\nQualitatively, GSA attributes sensitivity as follows:\n1.  **Nonlinearity in $X_1$**: The output $Y$ depends on $X_1$ via the nonlinear (quadratic) function $X_1^2$. GSA does not rely on local derivatives. Instead, it correctly quantifies the overall contribution of $X_1$'s uncertainty to the output's variance across its entire range of variation. The \"main effect\" of $X_1$ is captured by $\\mathrm{Var}(X_1^2) = 4/45$. This demonstrates that GSA is capable of handling nonlinear input-output relationships directly.\n2.  **Linearity in $X_2$**: The output $Y$ depends linearly on $X_2$. The main effect of $X_2$ is simply its own variance, $\\mathrm{Var}(X_2) = 1/12$. GSA quantifies this contribution straightforwardly.\n3.  **Decomposition**: The analysis shows that the total output variance $\\mathrm{Var}(Y) = 31/180$ is partitioned between the two inputs. The first-order sensitivity indices are $S_1 = V_1/\\mathrm{Var}(Y) = (4/45)/(31/180) = 16/31$ and $S_2 = V_2/\\mathrm{Var}(Y) = (1/12)/(31/180) = 15/31$. This shows that the two inputs contribute almost equally to the output variance ($S_1 \\approx 51.6\\%$, $S_2 \\approx 48.4\\%$), despite the different functional forms (nonlinear vs. linear) of their influence.\n\nThis decomposition is well suited for nonlinear models in Digital Twins of Cyber-Physical Systems (CPS) for several critical reasons:\n- **Captures Nonlinearity and Interactions**: CPS models are typically highly nonlinear and may exhibit strong interactions between parameters. Unlike local sensitivity methods (e.g., based on partial derivatives), GSA provides a comprehensive picture of how input uncertainties affect the output over their entire probability distributions, fully accounting for nonlinearities and interactions.\n- **Factor Prioritization**: By quantifying each input's contribution to output uncertainty, GSA allows engineers to identify the most critical sources of variability. This is essential for robust design, guiding efforts to either tighten tolerances on influential components (e.g., $X_1$) or implement control strategies to mitigate the effects of external disturbances (e.g., $X_2$).\n- **Model Reduction and Validation**: In complex Digital Twins with thousands of parameters, GSA can identify non-influential parameters ($S_i \\approx 0$), which can then be fixed at their nominal values, simplifying the model without significant loss of accuracy. This is crucial for creating computationally tractable digital twins.\n\nIn summary, GSA's ability to provide a full, quantitative decomposition of output variance in the presence of nonlinearities makes it an indispensable tool for understanding and managing uncertainty in the complex models that form the core of Digital Twins for CPS.",
            "answer": "$$\\boxed{\\frac{31}{180}}$$"
        },
        {
            "introduction": "While theoretical understanding is crucial, applying GSA to real-world models immediately raises practical questions of computational cost. This exercise delves into the widely-used Saltelli sampling scheme for estimating Sobol indices. Your task is to derive the formula for the total number of model simulations required, connecting the theoretical structure of the analysis to the practical constraints of its implementation.",
            "id": "3883337",
            "problem": "Consider a deterministic environmental model represented as $Y = f(\\mathbf{X})$, where $\\mathbf{X} = (X_1, X_2, \\ldots, X_d)$ is a $d$-dimensional vector of independent input factors with each $X_j$ supported on the unit interval and $Y$ is a scalar response. Variance-based global sensitivity analysis decomposes the output variance to attribute contributions to individual inputs and their interactions. The first-order Sobol index for factor $X_j$ is defined as $S_j = \\operatorname{Var}\\left(\\mathbb{E}[Y \\mid X_j]\\right) / \\operatorname{Var}(Y)$, and the total-order Sobol index for $X_j$ is defined as $T_j = 1 - \\operatorname{Var}\\left(\\mathbb{E}[Y \\mid \\mathbf{X}_{-j}]\\right) / \\operatorname{Var}(Y)$, where $\\mathbf{X}_{-j}$ denotes all inputs except $X_j$. These definitions follow from the Analysis of Variance (ANOVA) decomposition under independence and square-integrability of $Y$.\n\nA practical Monte Carlo or Quasi-Monte Carlo (QMC) estimation scheme due to Saltelli requires two independent base sample matrices of size $N \\times d$, denoted $A$ and $B$, constructed by sampling each column independently in $[0,1]$. For each input factor $j \\in \\{1, \\ldots, d\\}$, construct a mixed matrix $A^{(j)}_B$ by replacing the $j$-th column of $A$ with the $j$-th column of $B$. The model is then evaluated on rows of $A$, $B$, and each $A^{(j)}_B$ to provide consistent estimators of all first-order and total-order Sobol indices simultaneously.\n\nTask:\n1. Starting only from the variance-based definitions stated above and the independence of inputs, design the sampling plan based on the matrices $A$, $B$, and $A^{(j)}_B$ that enables the joint estimation of all first-order and total-order Sobol indices. Clearly state the number and sizes of all matrices that need to be constructed for a given $N$ and $d$.\n2. Derive the total number of model runs required by this plan as a function of $N$ and $d$, ensuring the argument is valid under the model and sampling assumptions. Treat the boundary case $d = 0$ carefully and provide a logically consistent count.\n3. Implement a program that, given pairs $(N, d)$, returns the total number of model runs required by Saltelliâ€™s scheme you have derived.\n\nUse the following test suite of input pairs $(N, d)$:\n- Case $1$: $N = 512$, $d = 6$.\n- Case $2$: $N = 1$, $d = 3$.\n- Case $3$: $N = 50$, $d = 0$.\n- Case $4$: $N = 137$, $d = 7$.\n- Case $5$: $N = 10000$, $d = 2$.\n- Case $6$: $N = 256$, $d = 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\ldots,r_6]$), where each $r_i$ is an integer corresponding to the total number of model runs required for the respective test case.",
            "solution": "The problem requires an analysis of the computational cost of Saltelli's sampling scheme for global sensitivity analysis. The total number of model evaluations is determined by the structure and dimensions of the sample matrices used in the procedure. The derivation proceeds in three stages: first, formalizing the sampling plan as described; second, deriving a general formula for the number of model runs as a function of the base sample size $N$ and the number of input dimensions $d$; and third, verifying the formula for the boundary case of $d=0$.\n\nThe sampling plan for estimating all first-order $S_j$ and total-order $T_j$ Sobol indices is constructed using three types of matrices. The model $Y=f(\\mathbf{X})$ must be evaluated for each row of these matrices.\n\n$1$. **Base Sample Matrices**: The plan begins with two independent sample matrices, $A$ and $B$. Both are of size $N \\times d$, where $N$ is the base sample size and $d$ is the number of input factors. Each matrix is generated by sampling $N \\times d$ points from the independent distributions of the input factors (e.g., uniform on $[0,1]$).\n-   Matrix $A$ has $N$ rows. Evaluating the model for each row of $A$ requires $N$ model runs.\n-   Matrix $B$ also has $N$ rows. Evaluating the model for each row of $B$ requires an additional $N$ model runs.\n\n$2$. **Mixed Sample Matrices**: For each input factor $X_j$ where $j \\in \\{1, 2, \\ldots, d\\}$, a \"mixed\" matrix $A^{(j)}_B$ is constructed. This matrix is identical to matrix $A$, except its $j$-th column is replaced by the $j$-th column from matrix $B$.\n-   There are $d$ input factors, so $d$ such matrices are constructed: $A^{(1)}_B, A^{(2)}_B, \\ldots, A^{(d)}_B$.\n-   Each matrix $A^{(j)}_B$ is of size $N \\times d$ and thus has $N$ rows.\n-   Evaluating the model for the rows of all these $d$ matrices requires $d \\times N$ model runs.\n\nThe total number of model runs is the sum of the runs required for each of these matrices. Assuming the rows of all constructed matrices are unique (which is true with probability $1$ for continuous sampling distributions), there is no overlap in the required evaluations.\n\nThe total number of runs, which we denote by $N_{total}$, is the sum of runs from matrix $A$, matrix $B$, and the $d$ mixed matrices $A^{(j)}_B$.\n$$N_{total} = (\\text{runs for } A) + (\\text{runs for } B) + (\\text{runs for all } A^{(j)}_B)$$\n$$N_{total} = N + N + \\sum_{j=1}^{d} N$$\n$$N_{total} = 2N + dN$$\n$$N_{total} = N(d+2)$$\nThis formula gives the total number of model executions for $d \\geq 1$.\n\nThe problem requires special consideration for the boundary case where $d=0$. In this scenario, the model $Y=f()$ has no input factors. Let us apply the described sampling plan algorithmically:\n-   The base matrices $A$ and $B$ are of size $N \\times d$, which becomes $N \\times 0$. Each matrix consists of $N$ rows, where each row is an empty vector representing a single point in the zero-dimensional input space. Evaluating the model for matrix $A$ requires $N$ runs, and for matrix $B$ requires another $N$ runs.\n-   The construction of mixed matrices $A^{(j)}_B$ is specified for each input factor $j \\in \\{1, \\ldots, d\\}$. When $d=0$, this set of indices is empty. Consequently, no mixed matrices are constructed.\n-   The total number of runs for $d=0$ is therefore the sum of runs for $A$ and $B$, which is $N + N = 2N$.\n\nIt is essential to check if our general formula $N_{total} = N(d+2)$ is consistent with this result. Substituting $d=0$ into the formula yields:\n$$N_{total} = N(0+2) = 2N$$\nThe formula holds for the boundary case $d=0$, providing a logically consistent expression for all non-negative integer values of $d$.\n\nTherefore, for a given base sample size $N$ and number of input factors $d$, the total number of model evaluations required by Saltelli's scheme is $N(d+2)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the total number of model runs required by Saltelli's sampling\n    scheme for global sensitivity analysis based on a series of (N, d) pairs.\n    \"\"\"\n    \n    # Define the test cases from the problem statement as pairs of (N, d).\n    # N is the base sample size, d is the number of input dimensions.\n    test_cases = [\n        (512, 6),    # Case 1\n        (1, 3),      # Case 2\n        (50, 0),     # Case 3\n        (137, 7),    # Case 4\n        (10000, 2),  # Case 5\n        (256, 1),    # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        N, d = case\n        \n        # The Saltelli sampling scheme requires N evaluations for a base matrix A,\n        # N evaluations for a second base matrix B, and N evaluations for each\n        # of the d mixed matrices A_B^(j).\n        # Total model runs = N (for A) + N (for B) + d * N (for d mixed matrices)\n        # This simplifies to the formula N * (d + 2).\n        # This formula is also valid for the boundary case d=0, where it\n        # correctly yields 2*N (as no mixed matrices are generated).\n        total_runs = N * (d + 2)\n        results.append(total_runs)\n\n    # Final print statement in the exact required format: [r_1,r_2,...,r_6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}