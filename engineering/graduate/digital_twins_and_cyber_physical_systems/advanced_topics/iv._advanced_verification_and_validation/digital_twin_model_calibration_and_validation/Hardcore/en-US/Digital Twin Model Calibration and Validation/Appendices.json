{
    "hands_on_practices": [
        {
            "introduction": "Before attempting to calibrate a digital twin, we must address a fundamental prerequisite: structural identifiability. This concept asks whether it is theoretically possible to uniquely determine the model's parameters from the available input-output data. This exercise  guides you through a simple but powerful analysis to detect non-identifiability, a crucial step that can prevent wasted effort on an impossible calibration task.",
            "id": "4214597",
            "problem": "Consider a digital twin of a single-degree-of-freedom cyber-physical actuator-sensor chain modeled as a Linear Time-Invariant (LTI) system. The physical actuator applies a commanded input $u(t)$ to a plant modeled as a pure integrator, and the sensor measures the state with an unknown gain. The digital twin uses the parametric model\n$$\n\\dot{x}(t) = g_{a}\\,u(t), \\qquad y(t) = g_{s}\\,x(t),\n$$\nwith initial condition $x(0) = 0$, where the actuator gain $g_{a} \\in \\mathbb{R}$ and sensor gain $g_{s} \\in \\mathbb{R}$ are unknown but nonzero. Assume the admissible input signals $u(t)$ lie in the set $L^2([0,T])$ for some finite horizon $T > 0$.\n\nStarting from foundational definitions for solutions of first-order linear differential equations and the principle of superposition for Linear Time-Invariant (LTI) systems, analyze the mapping $\\theta = (g_{a}, g_{s}) \\mapsto y(\\cdot)$ induced by the model and determine whether calibration using output-only measurements $y(t)$ over $t \\in [0,T]$ is sufficient to uniquely identify $\\theta$ for all admissible inputs. If it is not, construct, in closed form, a general expression for an alternative parameter vector $\\theta' = (g_{a}', g_{s}')$ that is distinct from $\\theta$ yet produces identical outputs $y(t)$ for all admissible inputs $u(t)$ and the specified initial condition.\n\nExpress your final answer as a single analytic expression for $\\theta'$ in terms of $\\theta$ and a free nonzero scalar. No numerical rounding is required. Units are not required.",
            "solution": "We are given the Linear Time-Invariant (LTI) model\n$$\n\\dot{x}(t) = g_{a}\\,u(t), \\qquad y(t) = g_{s}\\,x(t),\n$$\nwith $x(0)=0$, and admissible inputs $u \\in L^2([0,T])$.\n\nWe start from the fundamental solution of a first-order linear ordinary differential equation. For the given system,\n$$\n\\dot{x}(t) = g_{a}\\,u(t),\n$$\nwith $x(0)=0$, the solution is obtained by integrating both sides with respect to time:\n$$\nx(t) = x(0) + \\int_{0}^{t} g_{a}\\,u(\\tau)\\,d\\tau = g_{a}\\,\\int_{0}^{t} u(\\tau)\\,d\\tau.\n$$\nThis is valid for $u \\in L^2([0,T])$ because, on a finite interval, $L^2([0,T]) \\subset L^1([0,T])$ and hence the integral $\\int_{0}^{t} u(\\tau)\\,d\\tau$ exists for all $t \\in [0,T]$.\n\nThe output is given by the measurement equation\n$$\ny(t) = g_{s}\\,x(t) = g_{s}\\,g_{a}\\,\\int_{0}^{t} u(\\tau)\\,d\\tau.\n$$\nTherefore, the input-output map of the system can be written as\n$$\nu(\\cdot) \\mapsto y(t) = \\left(g_{a}\\,g_{s}\\right)\\,\\int_{0}^{t} u(\\tau)\\,d\\tau.\n$$\nThis shows that the output depends on the parameter vector $\\theta = (g_{a}, g_{s})$ only through the product $g_{a}\\,g_{s}$, not on $g_{a}$ and $g_{s}$ individually. Consequently, from output-only measurements $y(t)$ over $t \\in [0,T]$ and arbitrary admissible inputs $u$, one can at best identify the scalar $g_{a}\\,g_{s}$, but not the individual values of $g_{a}$ and $g_{s}$.\n\nTo construct a set of distinct parameter vectors that produce identical outputs for all admissible inputs, we require two parameter pairs $(g_{a}, g_{s})$ and $(g_{a}', g_{s}')$ to satisfy\n$$\ng_{a}\\,g_{s} = g_{a}'\\,g_{s}'.\n$$\nA general way to parameterize all such distinct pairs is to introduce a free nonzero scalar $\\alpha \\in \\mathbb{R}\\setminus\\{0\\}$ and define\n$$\ng_{a}' = \\alpha\\,g_{a}, \\qquad g_{s}' = \\frac{g_{s}}{\\alpha}.\n$$\nThen\n$$\ng_{a}'\\,g_{s}' = \\left(\\alpha\\,g_{a}\\right)\\left(\\frac{g_{s}}{\\alpha}\\right) = g_{a}\\,g_{s},\n$$\nwhich implies that $y'(t) = g_{a}'\\,g_{s}'\\int_{0}^{t}u(\\tau)\\,d\\tau = y(t)$ for all admissible $u(\\cdot)$ and all $t \\in [0,T]$. For any $\\alpha \\neq 1$, the pair $(g_{a}', g_{s}')$ is distinct from $(g_{a}, g_{s})$, demonstrating non-identifiability. The entire equivalence class of observationally indistinguishable parameter vectors is given by the one-parameter family\n$$\n\\theta'(\\alpha) = \\left(\\alpha\\,g_{a}, \\frac{g_{s}}{\\alpha}\\right), \\quad \\alpha \\in \\mathbb{R}\\setminus\\{0\\}.\n$$\nThis completes the construction of a counterexample in which two distinct parameter vectors produce identical outputs for all admissible inputs, illustrating non-identifiability in the calibration of this digital twin model.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\alpha\\,g_{a}  \\dfrac{g_{s}}{\\alpha}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Assuming a model is identifiable, the next question is: what is the best possible precision we can achieve in our calibration? The Cramér–Rao Lower Bound (CRLB) provides a theoretical floor on the variance of any unbiased parameter estimate, serving as a vital benchmark for performance. By deriving this bound from first principles , you will uncover the deep connection between measurement noise, model sensitivity, and the ultimate limits of calibration precision.",
            "id": "4214587",
            "problem": "A digital twin for a cyber-physical thermal subsystem is used to calibrate a single unknown scalar parameter $\\theta$ (for example, a nondimensionalized effective thermal conductivity) using sensor data. The forward model of the digital twin maps a sequence of known, deterministic inputs $\\{x_{i}\\}_{i=1}^{N}$ (e.g., heating rates at $N$ time instants) to predicted sensor outputs via a differentiable function $f(x;\\theta)$. The physical experiment produces noisy measurements $y_{i}$ according to\n$$\ny_{i} = f(x_{i};\\theta) + \\varepsilon_{i}, \\quad i=1,\\dots,N,\n$$\nwhere $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent and identically distributed Gaussian noise terms with known variance $\\sigma^{2}$. Assume standard regularity conditions that justify differentiation under the integral sign and interchange of integration and differentiation, and that $\\frac{\\partial f(x;\\theta)}{\\partial \\theta}$ exists and is continuous in a neighborhood of the true $\\theta$. All quantities are nondimensionalized, so no physical units appear.\n\nStarting only from the definitions of likelihood, score, and Fisher information for the joint distribution of $(y_{1},\\dots,y_{N})$, derive the Cramér–Rao lower bound for the variance of any unbiased estimator of $\\theta$ based on the data $\\{(x_{i},y_{i})\\}_{i=1}^{N}$. Your derivation must proceed from these fundamental statistical definitions and the independence structure of the measurements, without invoking any pre-stated inequality result other than the Cauchy–Schwarz inequality. Conclude with the explicit closed-form analytic expression of the bound in terms of $\\sigma^{2}$, $f$, and $\\{x_{i}\\}_{i=1}^{N}$.\n\nFinally, in concise terms, explain why this bound serves as a benchmark for calibration precision in the digital twin context, articulating how it relates to experimental design choices for $\\{x_{i}\\}_{i=1}^{N}$ and the achievable uncertainty of unbiased estimators. Your final reported bound must be a single closed-form analytic expression; do not include units.",
            "solution": "The problem is to derive the Cramér–Rao lower bound (CRLB) for the variance of an unbiased estimator of a scalar parameter $\\theta$ in a nonlinear regression model with additive Gaussian noise. The derivation must proceed from the fundamental definitions of likelihood, score, and Fisher information, using only the Cauchy–Schwarz inequality as an external result.\n\nFirst, we establish the statistical model for the observations. The measurements $\\{y_{i}\\}_{i=1}^{N}$ are given by\n$$y_{i} = f(x_{i};\\theta) + \\varepsilon_{i}$$\nwhere the inputs $\\{x_{i}\\}_{i=1}^{N}$ are known and deterministic, $\\theta$ is the unknown parameter, and the noise terms $\\varepsilon_{i}$ are independent and identically distributed (i.i.d.) normal random variables, $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$, with known variance $\\sigma^{2}$.\n\nFrom this definition, each measurement $y_{i}$ is itself a random variable. Its expected value is $E[y_{i}] = E[f(x_{i};\\theta) + \\varepsilon_{i}] = f(x_{i};\\theta) + E[\\varepsilon_{i}] = f(x_{i};\\theta)$. Its variance is $\\text{Var}(y_{i}) = \\text{Var}(f(x_{i};\\theta) + \\varepsilon_{i}) = \\text{Var}(\\varepsilon_{i}) = \\sigma^{2}$, since $f(x_{i};\\theta)$ is a deterministic quantity for a given $\\theta$. Therefore, each $y_{i}$ follows a normal distribution:\n$$y_{i} \\sim \\mathcal{N}(f(x_{i};\\theta), \\sigma^{2})$$\nThe probability density function (PDF) for a single observation $y_{i}$ is\n$$p(y_{i} | \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{i} - f(x_{i};\\theta))^{2}}{2\\sigma^{2}}\\right)$$\nSince the measurements are independent, the joint PDF of the entire data vector $\\mathbf{y} = (y_{1}, \\dots, y_{N})$ is the product of the individual PDFs. This joint PDF, viewed as a function of the parameter $\\theta$ for a given set of observations $\\mathbf{y}$, is the likelihood function $L(\\theta; \\mathbf{y})$:\n$$L(\\theta; \\mathbf{y}) = \\prod_{i=1}^{N} p(y_{i} | \\theta) = \\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{N/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - f(x_{i};\\theta))^{2}\\right)$$\nFor mathematical convenience, we work with the natural logarithm of the likelihood, known as the log-likelihood function $\\ell(\\theta; \\mathbf{y})$:\n$$\\ell(\\theta; \\mathbf{y}) = \\ln L(\\theta; \\mathbf{y}) = -\\frac{N}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - f(x_{i};\\theta))^{2}$$\nThe score function, $S(\\theta)$, is defined as the partial derivative of the log-likelihood function with respect to the parameter $\\theta$:\n$$S(\\theta) = \\frac{\\partial \\ell(\\theta; \\mathbf{y})}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left(-\\frac{N}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - f(x_{i};\\theta))^{2}\\right)$$\n$$S(\\theta) = -\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} 2(y_{i} - f(x_{i};\\theta)) \\cdot \\left(-\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right)$$\n$$S(\\theta) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - f(x_{i};\\theta)) \\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}$$\nA key property of the score function is that its expected value is zero. The expectation is taken over the distribution of the data $\\mathbf{y}$ for a fixed $\\theta$.\n$$E[S(\\theta)] = E\\left[\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - f(x_{i};\\theta)) \\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right]$$\nBy linearity of expectation, and noting that $\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}$ is not random with respect to $\\mathbf{y}$:\n$$E[S(\\theta)] = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} E[y_{i} - f(x_{i};\\theta)] \\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}$$\nSince $E[y_{i}] = f(x_{i};\\theta)$, we have $E[y_{i} - f(x_{i};\\theta)] = 0$. Thus,\n$$E[S(\\theta)] = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} 0 \\cdot \\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta} = 0$$\nNow, let $\\hat{\\theta} = \\hat{\\theta}(\\mathbf{y})$ be any unbiased estimator for $\\theta$. By definition, this means $E[\\hat{\\theta}] = \\theta$. We differentiate this identity with respect to $\\theta$. The expectation is an integral over the data space, so we have:\n$$E[\\hat{\\theta}] = \\int_{\\mathbb{R}^{N}} \\hat{\\theta}(\\mathbf{y}) L(\\theta; \\mathbf{y}) d\\mathbf{y} = \\theta$$\nDifferentiating both sides with respect to $\\theta$ and, per the given regularity conditions, interchanging differentiation and integration:\n$$1 = \\frac{\\partial}{\\partial \\theta} \\int_{\\mathbb{R}^{N}} \\hat{\\theta}(\\mathbf{y}) L(\\theta; \\mathbf{y}) d\\mathbf{y} = \\int_{\\mathbb{R}^{N}} \\hat{\\theta}(\\mathbf{y}) \\frac{\\partial L(\\theta; \\mathbf{y})}{\\partial \\theta} d\\mathbf{y}$$\nUsing the identity $\\frac{\\partial L}{\\partial \\theta} = L \\frac{\\partial \\ln L}{\\partial \\theta} = L \\cdot S(\\theta)$, we can rewrite the integral as an expectation:\n$$1 = \\int_{\\mathbb{R}^{N}} \\hat{\\theta}(\\mathbf{y}) S(\\theta) L(\\theta; \\mathbf{y}) d\\mathbf{y} = E[\\hat{\\theta}(\\mathbf{y}) S(\\theta)]$$\nThe covariance between the estimator $\\hat{\\theta}$ and the score $S(\\theta)$ is given by:\n$$\\text{Cov}(\\hat{\\theta}, S(\\theta)) = E[\\hat{\\theta} S(\\theta)] - E[\\hat{\\theta}] E[S(\\theta)]$$\nSubstituting the results $E[\\hat{\\theta} S(\\theta)] = 1$, $E[\\hat{\\theta}] = \\theta$, and $E[S(\\theta)] = 0$:\n$$\\text{Cov}(\\hat{\\theta}, S(\\theta)) = 1 - (\\theta)(0) = 1$$\nWe now invoke the Cauchy–Schwarz inequality for variances and covariances, which states $|\\text{Cov}(A, B)|^{2} \\le \\text{Var}(A)\\text{Var}(B)$ for any two random variables $A$ and $B$. Applying this to $\\hat{\\theta}$ and $S(\\theta)$:\n$$|\\text{Cov}(\\hat{\\theta}, S(\\theta))|^{2} \\le \\text{Var}(\\hat{\\theta}) \\text{Var}(S(\\theta))$$\n$$1^{2} \\le \\text{Var}(\\hat{\\theta}) \\text{Var}(S(\\theta))$$\nThe variance of the score function is, by definition, the Fisher information $I(\\theta)$:\n$$\\text{Var}(S(\\theta)) = E[S(\\theta)^{2}] - (E[S(\\theta)])^{2} = E[S(\\theta)^{2}] - 0^{2} = E[S(\\theta)^{2}] = I(\\theta)$$\nThus, a lower bound on the variance of the estimator is established:\n$$1 \\le \\text{Var}(\\hat{\\theta}) I(\\theta) \\implies \\text{Var}(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)}$$\nThis is the general form of the Cramér–Rao lower bound. To find the specific bound for this problem, we must compute the Fisher information $I(\\theta)$.\n$$I(\\theta) = E[S(\\theta)^{2}] = E\\left[ \\left( \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - f(x_{i};\\theta)) \\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta} \\right)^{2} \\right]$$\nLet $\\varepsilon_{i} = y_{i} - f(x_{i};\\theta)$, and let $f'_{i}(\\theta) = \\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}$.\n$$I(\\theta) = \\frac{1}{\\sigma^{4}} E\\left[ \\left( \\sum_{i=1}^{N} \\varepsilon_{i} f'_{i}(\\theta) \\right)^{2} \\right] = \\frac{1}{\\sigma^{4}} E\\left[ \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\varepsilon_{i} \\varepsilon_{j} f'_{i}(\\theta) f'_{j}(\\theta) \\right]$$\nBy linearity of expectation:\n$$I(\\theta) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{N} \\sum_{j=1}^{N} E[\\varepsilon_{i}\\varepsilon_{j}] f'_{i}(\\theta) f'_{j}(\\theta)$$\nThe noise terms are independent with zero mean, so $E[\\varepsilon_{i}\\varepsilon_{j}] = E[\\varepsilon_{i}]E[\\varepsilon_{j}] = 0$ for $i \\neq j$. For $i = j$, we have $E[\\varepsilon_{i}^{2}] = \\text{Var}(\\varepsilon_{i}) = \\sigma^{2}$. The double summation thus collapses to a single sum over the diagonal terms where $i = j$:\n$$I(\\theta) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{N} E[\\varepsilon_{i}^{2}] (f'_{i}(\\theta))^{2} = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{N} \\sigma^{2} \\left(\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right)^{2}$$\nSimplifying gives the Fisher information:\n$$I(\\theta) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{N} \\left(\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right)^{2}$$\nSubstituting this expression back into the CRLB inequality yields the final closed-form expression for the bound:\n$$\\text{Var}(\\hat{\\theta}) \\ge \\frac{\\sigma^{2}}{\\sum_{i=1}^{N} \\left(\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right)^{2}}$$\n\nThis bound serves as a fundamental benchmark for calibration precision in the digital twin context. It establishes a theoretical \"best-case\" limit on the variance, and thus the uncertainty, of any unbiased estimate of the parameter $\\theta$ obtained from the experimental data. No matter how sophisticated a calibration algorithm is, it cannot produce an unbiased estimate with a variance lower than this bound.\n\nThe expression for the bound reveals its direct relationship with experimental design, governed by the choice of inputs $\\{x_{i}\\}_{i=1}^{N}$. The denominator, $\\sum_{i=1}^{N} \\left(\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right)^{2}$, is the sum of the squared sensitivities of the model output to the parameter $\\theta$ at each input point. To minimize the variance bound (i.e., to maximize the potential calibration precision), an experimentalist should choose the inputs $\\{x_{i}\\}$ that maximize this sum. This means selecting test conditions where the physical system's observable output is most sensitive to changes in the parameter being calibrated. This principle is a cornerstone of optimal experimental design for parameter estimation. The bound thus quantitatively links experimental design choices to the best achievable uncertainty of the calibrated digital twin model.",
            "answer": "$$\\boxed{\\frac{\\sigma^{2}}{\\sum_{i=1}^{N} \\left(\\frac{\\partial f(x_{i};\\theta)}{\\partial \\theta}\\right)^{2}}}$$"
        },
        {
            "introduction": "Moving from theoretical limits to practical implementation, Bayesian inference offers a robust framework for performing calibration and rigorously quantifying uncertainty. A critical aspect of this approach is understanding how our prior beliefs about a parameter interact with the information contained in new data. This hands-on coding exercise  allows you to explore the concept of prior sensitivity, demonstrating how the strength of your initial assumptions can shape the final calibration result.",
            "id": "4214490",
            "problem": "Consider a calibration task in the context of Digital Twin (DT) development for Cyber-Physical Systems (CPS), where a cyber-physical sensor inside the physical system produces measurements that are modeled by a simple linear relationship between a known input and an unknown gain parameter. The Digital Twin must calibrate this unknown gain parameter using observed data and validate the sensitivity of the calibration to assumptions about the prior distribution. Assume the following model: the measurements are described by the relation $y_i = \\theta x_i + \\varepsilon_i$ for $i \\in \\{1, \\dots, n\\}$, where the errors are independent and identically distributed with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, and the prior for the unknown gain parameter is $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$. The calibration should be performed using Bayesian inference principles.\n\nYour task is to write a complete, runnable program that, given a fixed dataset $(x_i, y_i)$, a fixed prior mean $\\mu_0$, and a fixed noise variance $\\sigma^2$, computes the posterior mean of $\\theta$ and a symmetric credible interval of level $0.95$ for $\\theta$ under several different choices of the prior variance $\\tau_0^2$. The credible interval endpoints must be computed in the same unit as $\\theta$, which is unitless in this setup, and must correspond to the $(\\alpha/2)$ and $(1-\\alpha/2)$ quantiles for $\\alpha = 0.05$ of the posterior distribution, assuming a Gaussian posterior induced by conjugacy.\n\nFundamental base to use: Bayes’ theorem for continuous random variables, the definition of the Gaussian likelihood under linear models, and the conjugacy of Gaussian prior with Gaussian likelihood for linear models. All derivations and calculations should follow from these principles, starting from the definitions of likelihood and prior, and proceeding to the posterior distribution using standard manipulations (e.g., completing the square and recognition of Gaussian kernels).\n\nUse the following dataset and fixed hyperparameters:\n- Inputs: $x = [\\,0.5,\\,1.0,\\,1.6,\\,2.1,\\,2.8\\,]$.\n- Observations: $y = [\\,1.13,\\,2.15,\\,3.64,\\,4.54,\\,6.17\\,]$.\n- Prior mean: $\\mu_0 = 2.0$.\n- Noise variance: $\\sigma^2 = 0.04$ (so the noise standard deviation is $\\sigma = 0.2$).\n- Credible level: $0.95$ (so $\\alpha = 0.05$ is the complement).\n\nAnalyze prior sensitivity by varying the prior variance $\\tau_0^2$ over the following test suite:\n- Test case set of prior variances: $[\\,0.0001,\\,0.01,\\,0.1,\\,1.0,\\,100.0\\,]$.\n  - The case $\\tau_0^2 = 0.0001$ represents an extremely informative prior.\n  - The case $\\tau_0^2 = 0.01$ represents a highly informative prior.\n  - The case $\\tau_0^2 = 0.1$ represents a moderately informative prior.\n  - The case $\\tau_0^2 = 1.0$ represents a weakly informative prior.\n  - The case $\\tau_0^2 = 100.0$ represents a very diffuse prior approaching a data-dominant regime.\n\nFor each value of $\\tau_0^2$ in the test suite, your program must compute:\n- The posterior mean of $\\theta$ (unitless).\n- The lower endpoint of the symmetric $0.95$ credible interval (unitless).\n- The upper endpoint of the symmetric $0.95$ credible interval (unitless).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be represented as a list of three decimal numbers rounded to six decimal places, ordered as $[\\,\\text{posterior\\_mean},\\,\\text{lower\\_endpoint},\\,\\text{upper\\_endpoint}\\,]$. Aggregate all test cases into one outer list. For example, the output must have the form $[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],\\dots]$ where each $m_j$, $\\ell_j$, and $u_j$ is rounded to six decimal places. All outputs are unitless decimals and must be printed with exactly six digits after the decimal point.",
            "solution": "The problem requires the calibration of a sensor's gain parameter, $\\theta$, within a digital twin model. The approach is Bayesian inference. We are given a linear model for measurements $y_i$, a Gaussian noise model, and a Gaussian prior for $\\theta$. The task is to compute the posterior mean and a $0.95$ credible interval for $\\theta$ for several choices of the prior variance, $\\tau_0^2$, to analyze the sensitivity of the posterior to the prior's informativeness.\n\nFirst, we formalize the probabilistic model. The measurement model is given by $y_i = \\theta x_i + \\varepsilon_i$, for $i = 1, \\dots, n$. The errors $\\varepsilon_i$ are independent and identically distributed as a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that the conditional distribution of an observation $y_i$ given the input $x_i$ and the parameter $\\theta$ is also normal: $y_i | \\theta, x_i \\sim \\mathcal{N}(\\theta x_i, \\sigma^2)$.\n\nThe likelihood function, which is the probability of observing the entire dataset $\\mathbf{y} = \\{y_1, \\dots, y_n\\}$ given $\\theta$ and the inputs $\\mathbf{x} = \\{x_1, \\dots, x_n\\}$, is the product of the individual probability densities due to the independence of the measurements:\n$$ p(\\mathbf{y} | \\theta, \\mathbf{x}, \\sigma^2) = \\prod_{i=1}^{n} p(y_i | \\theta, x_i, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta x_i)^2}{2\\sigma^2}\\right) $$\nAs a function of $\\theta$, the likelihood is proportional to:\n$$ p(\\mathbf{y} | \\theta, \\mathbf{x}, \\sigma^2) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\theta x_i)^2\\right) $$\n\nThe prior distribution for the parameter $\\theta$ is specified as a normal distribution, $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$. The probability density function for the prior is:\n$$ p(\\theta | \\mu_0, \\tau_0^2) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ is proportional to the product of the likelihood and the prior:\n$$ p(\\theta | \\mathbf{y}, \\mathbf{x}, \\sigma^2, \\mu_0, \\tau_0^2) \\propto p(\\mathbf{y} | \\theta, \\mathbf{x}, \\sigma^2) \\cdot p(\\theta | \\mu_0, \\tau_0^2) $$\nSubstituting the expressions for the likelihood and prior, we get:\n$$ p(\\theta | \\dots) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\theta x_i)^2\\right) \\cdot \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\n$$ p(\\theta | \\dots) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{\\sum_{i=1}^{n}(y_i - \\theta x_i)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] \\right) $$\nTo identify the form of the posterior distribution, we analyze the exponent by expanding the squared terms and collecting terms involving $\\theta$:\n$$ \\frac{1}{\\sigma^2}\\sum(y_i^2 - 2\\theta x_i y_i + \\theta^2 x_i^2) + \\frac{1}{\\tau_0^2}(\\theta^2 - 2\\theta \\mu_0 + \\mu_0^2) $$\n$$ = \\theta^2 \\left( \\frac{\\sum x_i^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right) - 2\\theta \\left( \\frac{\\sum x_i y_i}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) + C $$\nwhere $C$ contains terms that do not depend on $\\theta$. This quadratic form in $\\theta$ within the exponential indicates that the posterior distribution is also a normal distribution. Let the posterior be $\\theta | \\dots \\sim \\mathcal{N}(\\mu_n, \\tau_n^2)$. The exponent of its density function is proportional to $-\\frac{1}{2\\tau_n^2}(\\theta - \\mu_n)^2$, which expands to:\n$$ -\\frac{1}{2} \\left( \\frac{1}{\\tau_n^2}\\theta^2 - \\frac{2\\mu_n}{\\tau_n^2}\\theta + \\frac{\\mu_n^2}{\\tau_n^2} \\right) $$\nBy matching the coefficients of $\\theta^2$ and $\\theta$ from the two forms of the exponent, we derive the posterior parameters $\\mu_n$ and $\\tau_n^2$.\nThe coefficient of $\\theta^2$ gives the posterior precision ($1/\\tau_n^2$):\n$$ \\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{\\sum_{i=1}^{n} x_i^2}{\\sigma^2} $$\nThe coefficient of $\\theta$ gives the posterior mean $\\mu_n$:\n$$ \\frac{\\mu_n}{\\tau_n^2} = \\frac{\\mu_0}{\\tau_0^2} + \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sigma^2} \\implies \\mu_n = \\tau_n^2 \\left( \\frac{\\mu_0}{\\tau_0^2} + \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sigma^2} \\right) $$\nThese formulas demonstrate the principle of Bayesian updating: the posterior precision is the sum of the prior precision and the data precision, and the posterior mean is a precision-weighted average of the prior mean and a data-derived mean.\n\nTo compute the required quantities, we use the provided data:\n- Inputs: $\\mathbf{x} = [\\,0.5,\\,1.0,\\,1.6,\\,2.1,\\,2.8\\,]$\n- Observations: $\\mathbf{y} = [\\,1.13,\\,2.15,\\,3.64,\\,4.54,\\,6.17\\,]$\n- Prior mean: $\\mu_0 = 2.0$\n- Noise variance: $\\sigma^2 = 0.04$\nFirst, we compute the summary statistics from the data:\n$$ \\sum_{i=1}^{n} x_i^2 = (0.5)^2 + (1.0)^2 + (1.6)^2 + (2.1)^2 + (2.8)^2 = 16.06 $$\n$$ \\sum_{i=1}^{n} x_i y_i = (0.5)(1.13) + (1.0)(2.15) + (1.6)(3.64) + (2.1)(4.54) + (2.8)(6.17) = 35.349 $$\n\nFor each given prior variance $\\tau_0^2$ in the set $[\\,0.0001,\\,0.01,\\,0.1,\\,1.0,\\,100.0\\,]$, we calculate the posterior mean $\\mu_n$ and variance $\\tau_n^2$.\n\nA symmetric credible interval of level $1-\\alpha$ for a normally distributed parameter $\\theta \\sim \\mathcal{N}(\\mu_n, \\tau_n^2)$ is given by $[\\mu_n - z_{1-\\alpha/2} \\tau_n, \\mu_n + z_{1-\\alpha/2} \\tau_n]$, where $\\tau_n = \\sqrt{\\tau_n^2}$ is the posterior standard deviation and $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For a $0.95$ credible interval, $\\alpha=0.05$, so we need the $z_{0.975}$ quantile, which is approximately $1.96$.\n\nThe program will implement these formulas, iterating through each specified value of $\\tau_0^2$ to compute the posterior mean $\\mu_n$, and the lower and upper endpoints of the $0.95$ credible interval.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the posterior mean and a 0.95 credible interval for a sensor gain\n    parameter theta using Bayesian linear regression for different prior variances.\n    \"\"\"\n    #\n    # Step 1: Define givens from the problem statement.\n    #\n\n    # Dataset\n    x = np.array([0.5, 1.0, 1.6, 2.1, 2.8])\n    y = np.array([1.13, 2.15, 3.64, 4.54, 6.17])\n\n    # Fixed hyperparameters\n    mu_0 = 2.0\n    sigma_sq = 0.04\n\n    # Test cases for prior variance\n    tau_0_sq_cases = [0.0001, 0.01, 0.1, 1.0, 100.0]\n\n    # Credible interval level\n    confidence_level = 0.95\n    alpha = 1 - confidence_level\n\n    #\n    # Step 2: Pre-compute data-dependent summary statistics.\n    #\n    sum_x_sq = np.sum(x**2)\n    sum_xy = np.sum(x * y)\n\n    #\n    # Step 3: Pre-compute the z-score for the credible interval.\n    #\n    # For a symmetric 95% credible interval, we need the quantile for\n    # 1 - alpha/2 = 1 - 0.05/2 = 0.975.\n    z_score = norm.ppf(1 - alpha / 2)\n\n    #\n    # Step 4: Iterate through test cases and perform Bayesian updates.\n    #\n    all_results = []\n    for tau_0_sq in tau_0_sq_cases:\n        #\n        # Step 4a: Calculate posterior precision and variance.\n        # Posterior precision = Prior precision + Data precision\n        #\n        prior_precision = 1 / tau_0_sq\n        data_precision = sum_x_sq / sigma_sq\n        posterior_precision = prior_precision + data_precision\n        tau_n_sq = 1 / posterior_precision\n\n        #\n        # Step 4b: Calculate posterior mean.\n        # The posterior mean is a precision-weighted average of the prior mean and\n        # a data-driven estimate.\n        #\n        mu_n = tau_n_sq * ((mu_0 / tau_0_sq) + (sum_xy / sigma_sq))\n\n        #\n        # Step 4c: Calculate the credible interval.\n        # The interval is [mean - z*stdev, mean + z*stdev].\n        #\n        tau_n = np.sqrt(tau_n_sq)\n        margin_of_error = z_score * tau_n\n        lower_endpoint = mu_n - margin_of_error\n        upper_endpoint = mu_n + margin_of_error\n\n        all_results.append([mu_n, lower_endpoint, upper_endpoint])\n\n    #\n    # Step 5: Format the results for the final output.\n    #\n    output_parts = []\n    for result_triple in all_results:\n        # Format each number to exactly six decimal places.\n        formatted_nums = [f\"{num:.6f}\" for num in result_triple]\n        # Create the inner list string, e.g., \"[2.007624,1.988407,2.026842]\".\n        output_parts.append(f\"[{','.join(formatted_nums)}]\")\n\n    # Join all test case results into the final output string.\n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}