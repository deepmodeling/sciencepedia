{
    "hands_on_practices": [
        {
            "introduction": "在我们能够自动寻找系统故障之前，我们需要一种数学方法来衡量系统行为距离违反某个需求的“远近”。本练习介绍了信号时序逻辑（Signal Temporal Logic, STL）中的核心概念——“鲁棒性”，它将一个简单的“通过/失败”判断转化为一个定量的分数。通过本练习，您将为一个给定的信号轨迹亲手计算这个关键的鲁棒性值。",
            "id": "4221695",
            "problem": "一个安全关键信息物理系统的数字孪生正在监控一个归一化标量信号 $x(t)$，该信号代表在时间 $t$ 已达到的热安全阈值的比例。待监控的属性用信号时序逻辑 (STL) 表示为 $\\phi = \\Box_{[0,10]}(x \\leq 1)$，该属性要求信号在区间 $[0,10]$ 内的所有时间 $t$ 都保持在阈值或阈值以下。在鲁棒满足语义中，每个公式都被赋予一个实值鲁棒性 $\\rho$，它量化了系统行为与满足集之间的有符号距离。\n\n从鲁棒满足作为到满足原子谓词的状态集的有符号距离的定义，以及有界区间上“总是”算子的时序组合语义出发，请根据在 $[0,10]$ 上的 $x(t)$，为公式 $\\phi = \\Box_{[0,10]}(x \\leq 1)$ 在时间 $t=0$ 时的鲁棒满足语义 $\\rho$ 给出形式化定义。\n\n然后，考虑以下分段线性信号\n$$\nx(t) = \n\\begin{cases}\n0.2 + 0.15\\,t,   t \\in [0,3], \\\\\n0.65 + 0.10\\,(t-3),   t \\in (3,7], \\\\\n1.05 - 0.25\\,(t-7),   t \\in (7,10].\n\\end{cases}\n$$\n使用在区间 $[0,10]$ 上的有符号距离解释，计算鲁棒性值 $\\rho(\\phi,x,0)$。将最终的鲁棒性值表示为一个纯数（无量纲），并且不要在最终的方框答案中包含任何单位。无需四舍五入。",
            "solution": "该问题陈述清晰，其科学基础植根于信息物理系统的形式化方法原理，并提供了一套完整且一致的给定条件。因此，该问题被认定为有效。我们将按要求分两部分进行解答。\n\n首先，我们必须为给定的信号时序逻辑 (STL) 公式 $\\phi = \\Box_{[0,10]}(x \\leq 1)$ 的鲁棒满足语义（记为 $\\rho$）给出一个形式化定义。STL 公式的鲁棒性量化了信号满足或违反该公式的程度。它是从原子谓词开始递归定义的。\n\n公式 $\\phi$ 中的原子谓词是 $\\mu \\equiv (x \\leq 1)$。这可以改写为 $f(x) \\geq 0$ 的不等式形式，具体为 $1 - x \\geq 0$。根据鲁棒满足作为有符号距离的标准定义，在状态 $\\mathbf{s}$ 上评估的原子谓词 $p(\\mathbf{s}) \\geq 0$ 的鲁棒性就是函数 $p(\\mathbf{s})$ 的值。对于我们的信号 $x(t)$，在时间 $t$ 的状态就是值 $x(t)$。因此，原子谓词 $\\mu$ 在特定时间 $t$ 的鲁棒性由以下公式给出：\n$$\n\\rho(\\mu, x, t) = 1 - x(t)\n$$\n$\\rho(\\mu, x, t)$ 的正值表示满足谓词（即 $x(t)  1$），值为 $0$ 表示在边界上满足（即 $x(t) = 1$），负值表示违反（即 $x(t) > 1$）。\n\n接下来，我们考虑时序算子 $\\Box_{[a,b]}$，它表示在时间区间 $[a,b]$ 内“总是”或“全局”。对于信号 $s$ 在时间 $t_0$，公式 $\\psi = \\Box_{[a,b]} \\psi'$ 的鲁棒性被定义为子公式 $\\psi'$ 在所有相关未来时间点的鲁棒性的最小值（对于连续时间则为下确界）。数学上表示为：\n$$\n\\rho(\\Box_{[a,b]} \\psi', s, t_0) = \\inf_{t' \\in [t_0+a, t_0+b]} \\rho(\\psi', s, t')\n$$\n在我们具体的问题中，公式是 $\\phi = \\Box_{[0,10]}(x \\leq 1)$，我们需要在时间 $t=0$ 评估其鲁棒性。因此，$t_0=0$，区间是 $[0,10]$，子公式是 $\\mu = (x \\leq 1)$。应用该定义，我们得到：\n$$\n\\rho(\\phi, x, 0) = \\rho(\\Box_{[0,10]}(x \\leq 1), x, 0) = \\inf_{t \\in [0,10]} \\rho((x \\leq 1), x, t)\n$$\n代入原子谓词鲁棒性的表达式，我们有：\n$$\n\\rho(\\phi, x, 0) = \\inf_{t \\in [0,10]} (1 - x(t))\n$$\n利用属性 $\\inf_{t} (c - f(t)) = c - \\sup_{t} f(t)$，我们得到 $\\phi$ 的鲁棒性的最终形式化定义：\n$$\n\\rho(\\phi, x, 0) = 1 - \\sup_{t \\in [0,10]} x(t)\n$$\n该表达式抓住了问题的要求：系统保持在阈值以下的鲁棒性，取决于其在整个区间内的最高峰值与该阈值的距离。\n\n第二，我们计算给定分段线性信号 $x(t)$ 的鲁棒性值：\n$$\nx(t) = \n\\begin{cases}\n0.2 + 0.15\\,t,   t \\in [0,3], \\\\\n0.65 + 0.10\\,(t-3),   t \\in (3,7], \\\\\n1.05 - 0.25\\,(t-7),   t \\in (7,10].\n\\end{cases}\n$$\n为了找到 $\\rho(\\phi, x, 0)$，我们必须找到 $x(t)$ 在紧区间 $[0,10]$ 上的上确界。由于 $x(t)$ 是连续且分段线性的，其上确界將是其最大值，该最大值必定出现在子区间的端点之一，或导数为零的点（此处在任何开子区间内均未发生）。我们逐段检查：\n\n1.  对于 $t \\in [0,3]$，$x(t) = 0.2 + 0.15t$。其导数为 $x'(t) = 0.15 > 0$，因此函数在此区间上是严格递增的。此段的最大值在 $t=3$ 处，为 $x(3) = 0.2 + 0.15(3) = 0.2 + 0.45 = 0.65$。\n\n2.  对于 $t \\in (3,7]$，$x(t) = 0.65 + 0.10(t-3)$。其导数为 $x'(t) = 0.10 > 0$，因此函数在此区间上也是严格递增的。此段的最大值在 $t=7$ 处，为 $x(7) = 0.65 + 0.10(7-3) = 0.65 + 0.10(4) = 0.65 + 0.40 = 1.05$。\n\n3.  对于 $t \\in (7,10]$，$x(t) = 1.05 - 0.25(t-7)$。其导数为 $x'(t) = -0.25  0$，因此函数在此区间上是严格递减的。此段的最大值在它的起点 $t=7$ 处，为 $x(7) = 1.05 - 0.25(7-7) = 1.05$。\n\n函数 $x(t)$ 是连续的，从 $t=0$ 到 $t=7$ 递增，然后从 $t=7$ 到 $t=10$ 递减。因此，全局最大值必定出现在导数从正变负的点，即 $t=7$。\n\n信号在整个区间 $[0,10]$ 上的最大值为：\n$$\n\\sup_{t \\in [0,10]} x(t) = x(7) = 1.05\n$$\n现在，我们可以使用推导出的公式计算鲁棒性值：\n$$\n\\rho(\\phi, x, 0) = 1 - \\sup_{t \\in [0,10]} x(t) = 1 - 1.05 = -0.05\n$$\n得到的鲁棒性为 $-0.05$。负号正确地表明属性 $\\phi = \\Box_{[0,10]}(x \\leq 1)$ 被违反，因为信号 $x(t)$ 超过了阈值 $1$。数值 $0.05$ 量化了信号的峰值比允许的最大值高出 $0.05$ 个单位。",
            "answer": "$$\n\\boxed{-0.05}\n$$"
        },
        {
            "introduction": "有了鲁棒性这个定量指标，我们就可以将寻找故障的任务重新定义为一个优化问题。本练习将指导您实现一个基于梯度下降的算法，该算法通过主动搜索输入参数来最小化鲁棒性值，从而有效地将系统“引向”违规状态。这是许多现代证伪工具背后的核心技术。",
            "id": "4221647",
            "problem": "考虑一个由其数字孪生仿真的信息物理系统，其中一个规范由信号时序逻辑 (STL) 编码，并且鲁棒语义函数表示为参数化输入 $u \\in \\mathbb{R}^n$ 的二次连续可微映射 $\\rho : \\mathbb{R}^n \\to \\mathbb{R}$。定义寻找反例的目标函数 $J : \\mathbb{R}^n \\to \\mathbb{R}$ 为 $J(u) = \\phi(\\rho(u))$，其中 $\\phi : \\mathbb{R} \\to \\mathbb{R}$ 是如下给出的光滑、严格递增的函数\n$$\n\\phi(r) = \\left( \\frac{1}{\\alpha} \\log\\left(1 + e^{\\alpha r}\\right) \\right)^2,\n$$\n给定 $\\alpha  0$。这种选择在惩罚正鲁棒性值的同时，为算法优化保持了光滑性。\n\n您的任务是，在输入空间中构建一种线搜索方法，该方法使用基于鲁棒语义的下降方向来单调递减 $J(u)$，并在关于 $\\rho$ 相对于参数的曲率的假设下证明其收敛性。具体而言：\n\n- 实现一种强制执行 Armijo 条件的回溯线搜索。在第 $k$ 次迭代时，给定 $u_k$，选择下降方向 $d_k = - \\nabla J(u_k)$，并通过从初始步长 $\\eta_0$ 开始使用缩减因子进行回溯来选择步长 $\\eta_k$，直到 Armijo 不等式\n$$\nJ(u_k + \\eta_k d_k) \\le J(u_k) + \\sigma \\eta_k \\nabla J(u_k)^\\top d_k\n$$\n成立，其中 $\\sigma \\in (0,1)$ 为固定值。更新 $u_{k+1} = u_k + \\eta_k d_k$ 并重复此过程，直到满足终止规则。\n\n- 使用链式法则 $\\nabla J(u) = \\phi'(\\rho(u)) \\nabla \\rho(u)$ 来计算梯度，其中\n$$\n\\phi'(r) = 2 \\log\\left(1 + e^{\\alpha r}\\right)\\cdot\\frac{e^{\\alpha r}}{1 + e^{\\alpha r}}.\n$$\n\n- 假设 $\\rho$ 具有 Lipschitz 连续梯度，其 Lipschitz 常数为 $L_\\rho$，并且其 Hessian 矩阵在关注的域上的算子范数有界，同时假设 $\\phi$ 是凸的，在 $\\mathbb{R}$ 上具有 Lipschitz 连续的导数。在额外的曲率假设下（例如，$\\rho$ 在一个邻域内的强凸性以及 $\\phi'$ 的正性），证明该方法生成的迭代点 $u_k$ 收敛到 $J$ 的一个驻点。如果 $J$ 是强凸的，证明其线性收敛速率。\n\n- 当 $\\|\\nabla J(u_k)\\|_2 \\le \\varepsilon$ 或当 $k$ 达到最大迭代预算 $N$ 时终止。\n\n该算法应为纯数学和逻辑上指定的，不依赖任何物理单位。角度（如有）应以弧度解释。\n\n对所有测试用例，线搜索使用以下固定参数：$\\alpha = 2$，$\\sigma = 10^{-4}$，初始步长 $\\eta_0 = 1$，缩减因子 $\\gamma = \\tfrac{1}{2}$，容差 $\\varepsilon = 10^{-6}$，以及最大迭代次数 $N = 5000$。\n\n在以下纯数学术语表示的鲁棒性函数和初始点测试套件上实现并运行您的方法：\n\n- 测试用例 1 (两个线性边界最小值的平滑近似):\n  - 维度 $n = 2$。\n  - 定义 $g_1(u) = a_1^\\top u + b_1$ 和 $g_2(u) = a_2^\\top u + b_2$，其中 $a_1 = (1, 0)$，$b_1 = 0.5$，$a_2 = (0, 1)$，$b_2 = -0.25$。\n  - 定义平滑最小值 (softmin) 鲁棒性：\n    $$\n    \\rho(u) = -\\frac{1}{\\beta}\\log\\left(e^{-\\beta g_1(u)} + e^{-\\beta g_2(u)}\\right),\n    $$\n    其中 $\\beta = 5$。\n  - 初始点 $u_0 = (1.0, -1.0)$。\n\n- 测试用例 2 (非凸但平滑的鲁棒性):\n  - 维度 $n = 2$。\n  - 定义\n    $$\n    \\rho(u) = \\sin(u_1) + 0.1 u_2^2.\n    $$\n  - 初始点 $u_0 = (2.5, 3.0)$。\n\n- 测试用例 3 (逻辑鲁棒性):\n  - 维度 $n = 3$。\n  - 令 $a = (0.2, -0.5, 0.3)$，$b = 0.1$，$c = 0.7$，并定义\n    $$\n    \\rho(u) = \\log\\left(1 + e^{a^\\top u + b}\\right) - c.\n    $$\n  - 初始点 $u_0 = (0.0, 0.0, 0.0)$。\n\n- 测试用例 4 (强凸二次鲁棒性):\n  - 维度 $n = 3$。\n  - 令\n    $$\n    Q = \\begin{bmatrix} 1.0   0   0 \\\\ 0   10.0   0 \\\\ 0   0   0.5 \\end{bmatrix}, \\quad c = \\begin{bmatrix} -1.0 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix},\n    $$\n    并定义\n    $$\n    \\rho(u) = \\frac{1}{2} u^\\top Q u + c^\\top u.\n    $$\n  - 初始点 $u_0 = (5.0, -5.0, 5.0)$。\n\n对每个测试用例，运行算法并返回一个包含三个量的列表：最终目标值 $J(u_{\\text{final}})$（作为浮点数），一个指示整个序列 $\\{J(u_k)\\}_{k=0}^{k_{\\text{final}}}$ 是否单调非增的布尔值，以及执行的总迭代次数。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个包含在方括号中的逗号分隔列表（例如，\"[[result_case1],[result_case2],[result_case3],[result_case4]]\"）。每个 \"[result_casei]\" 本身必须是一个方括号内的逗号分隔列表，格式为 \"[final_J,monotone_boolean,iteration_count]\"。",
            "solution": "所提出的问题是有效的。这是一个定义明确的数值优化练习，应用于由信号时序逻辑 (STL) 指定的信息物理系统的证伪。该问题有科学依据，内部一致，并提供了构建解决方案所需的所有必要信息。\n\n核心任务是找到一个输入参数向量 $u \\in \\mathbb{R}^n$ 来证伪一个 STL 规范，这对应于找到一个使鲁棒性度量 $\\rho(u)$ 为负的 $u$。这被构建为一个优化问题：最小化目标函数 $J(u) = \\phi(\\rho(u))$，其中 $\\phi(r)$ 是一个惩罚正鲁棒性值的光滑惩罚函数。具体函数为 $J(u) = \\left( \\frac{1}{\\alpha} \\log\\left(1 + e^{\\alpha \\rho(u)}\\right) \\right)^2$。函数 $f(r) = \\frac{1}{\\alpha}\\log(1+e^{\\alpha r})$ 是 $\\max(0, r)$ 的一个平滑近似，被称为 softplus 函数。因此，$J(u)$ 近似于 $(\\max(0, \\rho(u)))^2$，驱动优化过程去寻找 $\\rho(u) \\le 0$ 的区域。\n\n所选的优化算法是带有回溯线搜索的梯度下降法，以确保收敛。在每次迭代 $k$ 中，参数向量 $u_k$ 根据以下规则更新：\n$$\nu_{k+1} = u_k + \\eta_k d_k\n$$\n其中 $d_k$ 是下降方向，$\\eta_k > 0$ 是步长。\n\n下降方向取为目标函数的负梯度，即 $d_k = - \\nabla J(u_k)$。梯度使用链式法则计算：\n$$\n\\nabla J(u) = \\frac{d\\phi}{dr}\\bigg|_{r=\\rho(u)} \\nabla \\rho(u)\n$$\n问题陈述为该导数项提供了一个公式，记为 $\\phi'(r)$，即 $\\phi'(r) = 2 \\log\\left(1 + e^{\\alpha r}\\right)\\cdot\\frac{e^{\\alpha r}}{1 + e^{\\alpha r}}$。需要注意的是，给定 $\\phi(r) = \\left( \\frac{1}{\\alpha} \\log\\left(1 + e^{\\alpha r}\\right) \\right)^2$ 的真实导数是 $\\frac{d\\phi}{dr} = \\frac{2}{\\alpha} \\log(1+e^{\\alpha r}) \\frac{e^{\\alpha r}}{1+e^{\\alpha r}}$。所提供的 $\\phi'(r)$ 正好是真实导数的 $\\alpha$ 倍。使用所提供的 $\\nabla J(u_k)$ 公式意味着搜索方向为 $d_k = - \\alpha \\nabla_{\\text{true}}J(u_k)$。这是一个有效的下降方向，因为 $J$ 沿 $d_k$ 的方向导数为 $(\\nabla_{\\text{true}}J(u_k))^\\top d_k = -\\alpha \\|\\nabla_{\\text{true}}J(u_k)\\|_2^2  0$（对于非零梯度）。因此，该算法将正确地寻求 $J(u)$ 的最小值。\n\n步长 $\\eta_k$ 由回溯线搜索确定。从初始步长 $\\eta_0$ 开始，步长被重复地以因子 $\\gamma \\in (0, 1)$ 缩减，直到满足 Armijo-Goldstein 条件：\n$$\nJ(u_k + \\eta_k d_k) \\le J(u_k) + \\sigma \\eta_k \\nabla J(u_k)^\\top d_k\n$$\n其中 $\\sigma \\in (0,1)$ 是一个固定的控制参数。由于 $d_k = -\\nabla J(u_k)$，这变为 $J(u_k - \\eta_k \\nabla J(u_k)) \\le J(u_k) - \\sigma \\eta_k \\|\\nabla J(u_k)\\|_2^2$。此条件确保每一步都能使目标函数值有充分的下降。因此，目标值序列 $\\{J(u_k)\\}$ 被保证是单调非增的。\n\n在标准假设下，该算法收敛到一个驻点（即点 $u^*$ 使得 $\\nabla J(u^*) = 0$）是有保证的。鉴于 $\\rho$ 是 $C^2$ 的，因此 $\\nabla \\rho$ 是局部 Lipschitz 的，并且 $\\phi$ 是 $C^2$ 的，复合函数 $J(u)$ 具有局部 Lipschitz 连续的梯度 $\\nabla J(u)$。回溯线搜索确保步长 $\\eta_k$ 不会变得任意小。优化理论中的一个经典结果（与 Zoutendijk 定理相关）指出，对于一个使用满足 Wolfe 条件（Armijo 条件是其中一部分）的线搜索的基于梯度的方法，如果 $\\nabla J$ 在迭代路径上是 Lipschitz 连续的，那么和 $\\sum_{k=0}^{\\infty} \\|\\nabla J(u_k)\\|_2^2 \\cos^2\\theta_k$ 是有限的，其中 $\\theta_k$ 是 $d_k$ 和 $-\\nabla J(u_k)$ 之间的夹角。由于我们使用 $d_k = -\\nabla J(u_k)$，因此 $\\cos\\theta_k=1$。这意味着当 $k \\to \\infty$ 时，$\\|\\nabla J(u_k)\\|_2 \\to 0$。因此，序列 $\\{u_k\\}$ 的任何极限点都必须是驻点。如果额外证明 $J$ 是强凸的，该算法会展现出线性收敛速率。\n\n当梯度的范数低于容差 $\\varepsilon$（即 $\\|\\nabla J(u_k)\\|_2 \\le \\varepsilon$）或达到最大迭代次数 $N$ 时，算法终止。\n\n对于每个测试用例，我们必须推导出特定鲁棒性函数 $\\rho(u)$ 的梯度。\n1.  **测试用例 1 (Softmin)**: $\\rho(u) = -\\frac{1}{\\beta}\\log\\left(e^{-\\beta (u_1+0.5)} + e^{-\\beta (u_2-0.25)}\\right)$。令 $g_1(u) = u_1+0.5$ 且 $g_2(u) = u_2-0.25$。\n    $$ \\nabla\\rho(u) = \\frac{e^{-\\beta g_1(u)}\\nabla g_1(u) + e^{-\\beta g_2(u)}\\nabla g_2(u)}{e^{-\\beta g_1(u)} + e^{-\\beta g_2(u)}} = \\frac{e^{-\\beta (u_1+0.5)}[1,0]^\\top + e^{-\\beta (u_2-0.25)}[0,1]^\\top}{e^{-\\beta (u_1+0.5)} + e^{-\\beta (u_2-0.25)}} $$\n2.  **测试用例 2 (非凸)**: $\\rho(u) = \\sin(u_1) + 0.1 u_2^2$。\n    $$ \\nabla\\rho(u) = [\\cos(u_1), 0.2 u_2]^\\top $$\n3.  **测试用例 3 (逻辑)**: $\\rho(u) = \\log\\left(1 + e^{a^\\top u + b}\\right) - c$。令 $x = a^\\top u + b$。\n    $$ \\nabla\\rho(u) = \\frac{1}{1+e^x} \\cdot (e^x \\cdot \\nabla x) = \\frac{e^{a^\\top u + b}}{1+e^{a^\\top u + b}} a $$\n    这是应用于 $a^\\top u+b$ 的逻辑 S 型函数（logistic sigmoid function），再乘以向量 $a$。\n4.  **测试用例 4 (二次)**: $\\rho(u) = \\frac{1}{2} u^\\top Q u + c^\\top u$，其中 $Q$ 是一个对称矩阵。\n    $$ \\nabla\\rho(u) = Qu + c $$\n实现将使用这些解析梯度，并对 $\\phi$ 及其导数中涉及的指数和对数函数采用数值稳定的计算方法。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to configure and run the backtracking line-search\n    algorithm for each test case.\n    \"\"\"\n    # Fixed parameters for the line-search algorithm\n    ALPHA = 2.0\n    SIGMA = 1e-4\n    ETA0 = 1.0\n    GAMMA = 0.5\n    EPSILON = 1e-6\n    N_MAX = 5000\n\n    # --- Helper functions for the objective function J ---\n\n    def softplus(x):\n        \"\"\"Numerically stable softplus function: log(1+exp(x)).\"\"\"\n        return np.where(x  35, x, np.log1p(np.exp(x)))\n\n    def sigmoid(x):\n        \"\"\"Numerically stable sigmoid function: 1/(1+exp(-x)).\"\"\"\n        return np.where(x = 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n\n    def phi(r, alpha):\n        \"\"\"Smooth penalty function phi(r).\"\"\"\n        return (softplus(alpha * r) / alpha)**2\n\n    def phi_prime(r, alpha):\n        \"\"\"\n        Problem-defined derivative of phi. This is alpha times the true derivative.\n        phi'(r) = 2 * log(1+exp(alpha*r)) * sigmoid(alpha*r)\n        \"\"\"\n        return 2 * softplus(alpha * r) * sigmoid(alpha * r)\n\n    def J_objective(u, rho_func, alpha):\n        \"\"\"Counterexample-seeking objective function J(u).\"\"\"\n        return phi(rho_func(u), alpha)\n\n    def grad_J_objective(u, rho_func, grad_rho_func, alpha):\n        \"\"\"Gradient of the objective function, grad J(u).\"\"\"\n        r = rho_func(u)\n        grad_rho_val = grad_rho_func(u)\n        phi_prime_val = phi_prime(r, alpha)\n        return phi_prime_val * grad_rho_val\n\n    # --- Main backtracking line-search algorithm ---\n\n    def backtracking_line_search(u0, rho_func, grad_rho_func):\n        u = np.copy(u0)\n        j_history = []\n        \n        for k in range(N_MAX):\n            j_val = J_objective(u, rho_func, ALPHA)\n            j_history.append(j_val)\n            \n            grad_j = grad_J_objective(u, rho_func, grad_rho_func, ALPHA)\n            grad_norm = np.linalg.norm(grad_j)\n\n            if grad_norm = EPSILON:\n                # Monotonicity check, allowing for minor floating point tolerances\n                is_monotone = all(j_history[i] = j_history[i+1] - 1e-9 for i in range(len(j_history)-1))\n                return j_val, is_monotone, k\n\n            d = -grad_j\n            eta = ETA0\n            \n            # Armijo condition check\n            while True:\n                u_new = u + eta * d\n                j_new = J_objective(u_new, rho_func, ALPHA)\n                armijo_rhs = j_val + SIGMA * eta * np.dot(grad_j, d)\n                if j_new = armijo_rhs:\n                    break\n                eta *= GAMMA\n                if eta  1e-18: # Failsafe\n                    break\n            \n            u += eta * d\n\n        # Max iterations reached\n        j_final = J_objective(u, rho_func, ALPHA)\n        j_history.append(j_final)\n        is_monotone = all(j_history[i] = j_history[i+1] - 1e-9 for i in range(len(j_history)-1))\n        return j_final, is_monotone, N_MAX\n\n\n    # --- Test Case Definitions ---\n\n    # Case 1: Smooth minimum of two linear margins\n    BETA1 = 5.0\n    A1 = np.array([1.0, 0.0])\n    B1 = 0.5\n    A2 = np.array([0.0, 1.0])\n    B2 = -0.25\n    def rho_case1(u):\n        g1 = A1 @ u + B1\n        g2 = A2 @ u + B2\n        # Numerically stable softmin\n        m = np.max([-BETA1 * g1, -BETA1 * g2])\n        return -(1/BETA1) * (m + np.log(np.exp(-BETA1 * g1 - m) + np.exp(-BETA1 * g2 - m)))\n    def grad_rho_case1(u):\n        g1 = A1 @ u + B1\n        g2 = A2 @ u + B2\n        # Numerically stable weights\n        m = np.max([-BETA1 * g1, -BETA1 * g2])\n        w1 = np.exp(-BETA1 * g1 - m)\n        w2 = np.exp(-BETA1 * g2 - m)\n        return (w1 * A1 + w2 * A2) / (w1 + w2)\n\n    # Case 2: Nonconvex but smooth robustness\n    def rho_case2(u):\n        return np.sin(u[0]) + 0.1 * u[1]**2\n    def grad_rho_case2(u):\n        return np.array([np.cos(u[0]), 0.2 * u[1]])\n\n    # Case 3: Logistic robustness\n    A3 = np.array([0.2, -0.5, 0.3])\n    B3 = 0.1\n    C3 = 0.7\n    def rho_case3(u):\n        x = A3 @ u + B3\n        return softplus(x) - C3\n    def grad_rho_case3(u):\n        x = A3 @ u + B3\n        return sigmoid(x) * A3\n\n    # Case 4: Strongly convex quadratic robustness\n    Q4 = np.array([[1.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 0.5]])\n    C4 = np.array([-1.0, 0.0, 0.5])\n    def rho_case4(u):\n        return 0.5 * u.T @ Q4 @ u + C4.T @ u\n    def grad_rho_case4(u):\n        return Q4 @ u + C4\n    \n    test_cases = [\n        {\"u0\": np.array([1.0, -1.0]), \"rho\": rho_case1, \"grad_rho\": grad_rho_case1},\n        {\"u0\": np.array([2.5, 3.0]), \"rho\": rho_case2, \"grad_rho\": grad_rho_case2},\n        {\"u0\": np.array([0.0, 0.0, 0.0]), \"rho\": rho_case3, \"grad_rho\": grad_rho_case3},\n        {\"u0\": np.array([5.0, -5.0, 5.0]), \"rho\": rho_case4, \"grad_rho\": grad_rho_case4},\n    ]\n\n    results = []\n    for case in test_cases:\n        final_j, is_monotone, n_iter = backtracking_line_search(case[\"u0\"], case[\"rho\"], case[\"grad_rho\"])\n        results.append(f\"[{final_j},{is_monotone},{n_iter}]\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}