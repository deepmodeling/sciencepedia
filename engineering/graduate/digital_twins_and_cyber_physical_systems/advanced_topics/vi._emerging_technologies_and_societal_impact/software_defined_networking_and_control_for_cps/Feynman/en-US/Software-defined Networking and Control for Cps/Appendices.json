{
    "hands_on_practices": [
        {
            "introduction": "One of the most fundamental architectural decisions in a centralized Software-Defined Networking (SDN) deployment is determining the optimal locations for the controllers. This decision directly influences the worst-case communication latency between any network switch and the control plane, thereby setting a lower bound on the system's reaction time to events. This exercise  guides you through solving this classic optimization problem, known as the k-center problem, to quantify how strategic placement impacts the overall responsiveness and reliability of a Cyber-Physical System (CPS).",
            "id": "4246512",
            "problem": "Consider a centralized Software-Defined Networking (SDN) control plane for a Cyber-Physical System (CPS), where a set of switches $V=\\{1,2,3,4,5\\}$ are interconnected with symmetric communication latencies. The network is modeled as an undirected graph with edge latencies $w_{uv}$ in milliseconds between directly connected switches $u$ and $v$. The nonzero direct links and their latencies are:\n- $w_{1,2}=3\\,\\mathrm{ms}$,\n- $w_{2,3}=2\\,\\mathrm{ms}$,\n- $w_{3,4}=4\\,\\mathrm{ms}$,\n- $w_{4,5}=3\\,\\mathrm{ms}$,\n- $w_{1,5}=8\\,\\mathrm{ms}$,\n- $w_{2,5}=5\\,\\mathrm{ms}$,\n- $w_{1,3}=6\\,\\mathrm{ms}$.\n\nAll links not listed have no direct connection. Assume negligible queueing in the data plane and symmetric latency on each path; that is, propagation delay on a path equals the sum of its constituent link latencies, and the reverse path has the same latency.\n\nYou are to place $k=2$ SDN controllers at two distinct switches (controller locations must be chosen from $V$). Each switch sends control messages to its nearest controller over the shortest-latency path, and the associated control-policy reaction completes when the controller’s response arrives back at the originating switch. Assume a controller processing time of $t_{\\mathrm{proc}}=2\\,\\mathrm{ms}$ (this aggregates computation and any software overhead at the controller) and negligible processing time at switches.\n\nStarting from first principles (shortest-path latency as the sum of link latencies and reaction time as the time for a request to travel from a switch to a controller and for the response to return), determine:\n- the controller placement that minimizes the worst-case one-way latency from any switch to its nearest controller, and\n- the corresponding upper bound on the control-policy reaction time experienced by any switch event under that optimal placement.\n\nExpress the final reaction-time bound in milliseconds. No rounding is required; report the exact value.",
            "solution": "The problem requires us to determine the optimal placement of $k=2$ Software-Defined Networking (SDN) controllers within a network of $V=\\{1,2,3,4,5\\}$ switches to minimize the worst-case one-way communication latency from any switch to its assigned controller. Subsequently, we must calculate the upper bound on the control-policy reaction time for this optimal placement.\n\nThis problem is a specific instance of the unweighted vertex $k$-center problem on a graph. We seek to find a subset of vertices $C \\subset V$ with a size $|C|=k=2$, which will serve as controller locations. The objective is to minimize the maximum shortest-path latency from any switch $v \\in V$ to its nearest controller in $C$. Let $d(u,v)$ denote the shortest-path latency between switches $u$ and $v$. The optimization problem is to find the set $C^*$ that solves:\n$$ \\min_{C \\subset V, |C|=2} \\left( \\max_{v \\in V} \\left( \\min_{c \\in C} d(v,c) \\right) \\right) $$\n\nThe solution process involves three main steps:\n1.  Compute the all-pairs shortest-path latencies for the given network graph.\n2.  Evaluate all possible controller placements to find the one(s) that minimize the worst-case one-way latency.\n3.  Calculate the upper bound on the control-policy reaction time using the result from the optimal placement.\n\n**Step 1: All-Pairs Shortest Path Calculation**\n\nThe network of switches and links can be modeled as an undirected graph $G=(V, E)$, where $V=\\{1,2,3,4,5\\}$ and the edge weights are the given latencies in milliseconds. The adjacency matrix, representing direct link latencies ($w_{uv}$), is given by (with $\\infty$ for no direct link):\n$$ W = \\begin{pmatrix} 0 & 3 & 6 & \\infty & 8 \\\\ 3 & 0 & 2 & \\infty & 5 \\\\ 6 & 2 & 0 & 4 & \\infty \\\\ \\infty & \\infty & 4 & 0 & 3 \\\\ 8 & 5 & \\infty & 3 & 0 \\end{pmatrix} $$\nWe can use an algorithm such as the Floyd-Warshall algorithm or run Dijkstra's algorithm from each vertex to find the shortest-path latencies $d(u,v)$ between all pairs of switches. The resulting all-pairs shortest-path matrix, $D$, is:\n$$ D = \\begin{pmatrix} 0 & 3 & 5 & 9 & 8 \\\\ 3 & 0 & 2 & 6 & 5 \\\\ 5 & 2 & 0 & 4 & 7 \\\\ 9 & 6 & 4 & 0 & 3 \\\\ 8 & 5 & 7 & 3 & 0 \\end{pmatrix} $$\nFor example, the shortest path from switch $1$ to switch $3$ is $d(1,3)=5\\,\\mathrm{ms}$ via the path $1 \\to 2 \\to 3$ (latency $3+2=5$), which is shorter than the direct link with latency $6\\,\\mathrm{ms}$.\n\n**Step 2: Evaluation of Candidate Controller Placements**\n\nWe need to place $k=2$ controllers. The number of possible placements is the number of ways to choose $2$ switches from $5$, which is $\\binom{5}{2} = \\frac{5 \\times 4}{2} = 10$. For each candidate placement $C = \\{c_1, c_2\\}$, we calculate the worst-case one-way latency, $L(C) = \\max_{v \\in V} \\min(d(v, c_1), d(v, c_2))$.\n\n1.  $C = \\{1, 2\\}$: Latencies to nearest controller for $v \\in \\{1,2,3,4,5\\}$ are $\\{\\min(d(1,1),d(1,2)), \\min(d(2,1),d(2,2)), \\dots\\}$. This gives $\\{\\min(0,3), \\min(3,0), \\min(5,2), \\min(9,6), \\min(8,5)\\} = \\{0, 0, 2, 6, 5\\}$. $L(\\{1,2\\}) = \\max\\{0,0,2,6,5\\} = 6\\,\\mathrm{ms}$.\n2.  $C = \\{1, 3\\}$: Latencies are $\\{\\min(0,5), \\min(3,2), \\min(5,0), \\min(9,4), \\min(8,7)\\} = \\{0, 2, 0, 4, 7\\}$. $L(\\{1,3\\}) = \\max\\{0,2,0,4,7\\} = 7\\,\\mathrm{ms}$.\n3.  $C = \\{1, 4\\}$: Latencies are $\\{\\min(0,9), \\min(3,6), \\min(5,4), \\min(9,0), \\min(8,3)\\} = \\{0, 3, 4, 0, 3\\}$. $L(\\{1,4\\}) = \\max\\{0,3,4,0,3\\} = 4\\,\\mathrm{ms}$.\n4.  $C = \\{1, 5\\}$: Latencies are $\\{\\min(0,8), \\min(3,5), \\min(5,7), \\min(9,3), \\min(8,0)\\} = \\{0, 3, 5, 3, 0\\}$. $L(\\{1,5\\}) = \\max\\{0,3,5,3,0\\} = 5\\,\\mathrm{ms}$.\n5.  $C = \\{2, 3\\}$: Latencies are $\\{\\min(3,5), \\min(0,2), \\min(2,0), \\min(6,4), \\min(5,7)\\} = \\{3, 0, 0, 4, 5\\}$. $L(\\{2,3\\}) = \\max\\{3,0,0,4,5\\} = 5\\,\\mathrm{ms}$.\n6.  $C = \\{2, 4\\}$: Latencies are $\\{\\min(3,9), \\min(0,6), \\min(2,4), \\min(6,0), \\min(5,3)\\} = \\{3, 0, 2, 0, 3\\}$. $L(\\{2,4\\}) = \\max\\{3,0,2,0,3\\} = 3\\,\\mathrm{ms}$.\n7.  $C = \\{2, 5\\}$: Latencies are $\\{\\min(3,8), \\min(0,5), \\min(2,7), \\min(6,3), \\min(5,0)\\} = \\{3, 0, 2, 3, 0\\}$. $L(\\{2,5\\}) = \\max\\{3,0,2,3,0\\} = 3\\,\\mathrm{ms}$.\n8.  $C = \\{3, 4\\}$: Latencies are $\\{\\min(5,9), \\min(2,6), \\min(0,4), \\min(4,0), \\min(7,3)\\} = \\{5, 2, 0, 0, 3\\}$. $L(\\{3,4\\}) = \\max\\{5,2,0,0,3\\} = 5\\,\\mathrm{ms}$.\n9.  $C = \\{3, 5\\}$: Latencies are $\\{\\min(5,8), \\min(2,5), \\min(0,7), \\min(4,3), \\min(7,0)\\} = \\{5, 2, 0, 3, 0\\}$. $L(\\{3,5\\}) = \\max\\{5,2,0,3,0\\} = 5\\,\\mathrm{ms}$.\n10. $C = \\{4, 5\\}$: Latencies are $\\{\\min(9,8), \\min(6,5), \\min(4,7), \\min(0,3), \\min(3,0)\\} = \\{8, 5, 4, 0, 0\\}$. $L(\\{4,5\\}) = \\max\\{8,5,4,0,0\\} = 8\\,\\mathrm{ms}$.\n\nComparing the worst-case latencies for all placements, the minimum value is $3\\,\\mathrm{ms}$.\n$$ \\min_{C} L(C) = 3\\,\\mathrm{ms} $$\nThis minimum is achieved for two distinct controller placements: $C^*_1 = \\{2, 4\\}$ and $C^*_2 = \\{2, 5\\}$. Both are optimal placements. The question asks for the controller placement, and either of these sets constitutes a correct answer.\n\n**Step 3: Calculation of the Control-Policy Reaction Time Bound**\n\nThe control-policy reaction time for a given switch is the round-trip time for a message to its assigned controller, including a specified controller processing time.\nLet $t_{\\text{one-way}}(v)$ be the one-way latency for a switch $v$ to its nearest controller. The reaction time for switch $v$ is:\n$$ T_{\\text{reaction}}(v) = t_{\\text{one-way}}(v) + t_{\\text{proc}} + t_{\\text{one-way}}(v) = 2 \\cdot t_{\\text{one-way}}(v) + t_{\\text{proc}} $$\nThe problem asks for the upper bound on this reaction time, which corresponds to the switch experiencing the worst-case (maximum) one-way latency under the optimal controller placement. This maximum one-way latency is precisely the value $L(C^*)$ we minimized, which is $3\\,\\mathrm{ms}$.\n\nLet $L_{\\text{opt}} = \\max_{v \\in V} \\min_{c \\in C^*} d(v,c) = 3\\,\\mathrm{ms}$.\nThe controller processing time is given as $t_{\\text{proc}} = 2\\,\\mathrm{ms}$.\nThe upper bound on the control-policy reaction time, $T_{\\text{reaction,max}}$, experienced by any switch is therefore:\n$$ T_{\\text{reaction,max}} = 2 \\cdot L_{\\text{opt}} + t_{\\text{proc}} $$\n$$ T_{\\text{reaction,max}} = 2 \\cdot (3\\,\\mathrm{ms}) + 2\\,\\mathrm{ms} = 6\\,\\mathrm{ms} + 2\\,\\mathrm{ms} = 8\\,\\mathrm{ms} $$\nThus, with an optimal placement of controllers (either at switches $\\{2, 4\\}$ or $\\{2, 5\\}$), the maximum reaction time any switch will experience is $8\\,\\mathrm{ms}$.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "Effective control of a CPS network requires not only high-level orchestration but also precise, predictable behavior at each individual switch. When multiple forwarding rules could potentially match a single packet, deterministic conflict resolution is essential for predictable behavior. In this practice , you will implement a priority scheme based on rule specificity to resolve overlaps and then analyze its performance impact, calculating the worst-case latency for a safety-critical flow in a non-preemptive priority queuing system.",
            "id": "4246510",
            "problem": "An OpenFlow-like Software-Defined Networking (SDN) data plane is used to enforce communication policies for a safety-critical Cyber-Physical System (CPS). The SDN switch uses integer rule priorities where higher integers denote higher matching precedence. The controller programs the following six flow rules, each with a three-field predicate consisting of Virtual Local Area Network (VLAN) Identifier, Internet Protocol version 4 (IPv4) destination prefix, and Transmission Control Protocol (TCP) destination port. Wildcards denote “any value”.\n\nRule $f_1$: VLAN $10$, IPv4 destination $10.0.0.0/8$, TCP destination port any.\n\nRule $f_2$: VLAN $10$, IPv4 destination $10.1.0.0/16$, TCP destination port $12345$.\n\nRule $f_3$ (safety-critical CPS control): VLAN $10$, IPv4 destination $10.1.2.0/24$, TCP destination port $12345$.\n\nRule $f_4$: VLAN any, IPv4 destination $10.1.2.0/24$, TCP destination port any.\n\nRule $f_5$: VLAN any, IPv4 destination any, TCP destination port any (table-miss default).\n\nRule $f_6$: VLAN $10$, IPv4 destination $10.1.2.34/32$, TCP destination port any.\n\nPackets are matched by choosing the highest-priority rule among all rules whose predicate is satisfied. To guarantee deterministic matching order in the presence of overlapping wildcard patterns, the controller defines the following specificity metric for a rule’s predicate. Let $s_{\\text{VLAN}}$ be the number of matched bits in the VLAN Identifier (exact match counts as $12$ bits; wildcard counts as $0$), let $s_{\\text{IP}}$ be the matched prefix length in the IPv4 destination address (from $0$ to $32$), and let $s_{\\text{TCP}}$ be the number of matched bits in the TCP destination port (exact match counts as $16$ bits; wildcard counts as $0$). The combined specificity is $S = s_{\\text{VLAN}} + s_{\\text{IP}} + s_{\\text{TCP}}$. The deterministic priority ordering requirement is: for any pair of rules with a non-empty intersection of their match sets, the rule with the strictly larger $S$ must have a strictly higher integer priority; ties in $S$ must be broken lexicographically by the tuple $(s_{\\text{TCP}}, s_{\\text{IP}}, s_{\\text{VLAN}})$ so that the rule with the larger tuple in lexicographic order has higher priority. All six rules must be assigned distinct integer priorities $p_1, p_2, p_3, p_4, p_5, p_6$, where $p_i$ is the priority of $f_i$.\n\nIn addition, the switch scheduler is strict non-preemptive priority: packets matching $f_3$ are placed into a high-priority queue, while all other packets are placed into a single lower-priority queue. The link rate is $C = 1 \\times 10^{9}$ bits per second, the maximum lower-priority frame size is $L_{\\text{BE}} = 1500$ bytes, the safety-critical frame size is $L_{\\text{CPS}} = 100$ bytes, the per-packet switch processing time is $t_{\\text{proc}} = 5 \\,\\mu\\text{s}$, the cable length is $d = 100$ meters, and the propagation speed in the medium is $v = 2 \\times 10^{8}$ meters per second. Using first principles of non-preemptive strict-priority queuing and serialization and propagation delays, derive the worst-case per-hop latency $\\Delta_{\\max}$ for a packet of the safety-critical flow $f_3$.\n\nCompute:\n- The integer priority assignment $(p_1, p_2, p_3, p_4, p_5, p_6)$ that satisfies the determinism requirement described above.\n- The worst-case per-hop latency $\\Delta_{\\max}$ for $f_3$.\n\nReport your final answer as a single row vector containing the six priorities followed by the scalar $\\Delta_{\\max}$. Round the latency to four significant figures and express it in milliseconds.",
            "solution": "The problem requires the determination of two quantities: an integer priority assignment for a set of six Software-Defined Networking (SDN) flow rules, and the worst-case per-hop latency for a specific safety-critical flow. The solution is derived in two corresponding parts.\n\nPart 1: Integer Priority Assignment\n\nThe priority of each flow rule $f_i$ is determined by a specificity metric. The combined specificity $S$ is defined as the sum of specificities of the three predicate fields: $S = s_{\\text{VLAN}} + s_{\\text{IP}} + s_{\\text{TCP}}$. The individual specificities are defined as:\n- $s_{\\text{VLAN}}$: The number of matched bits in the Virtual Local Area Network (VLAN) Identifier. An exact match corresponds to $12$ bits, while a wildcard corresponds to $0$ bits.\n- $s_{\\text{IP}}$: The prefix length of the Internet Protocol version 4 (IPv4) destination address, ranging from $0$ to $32$.\n- $s_{\\text{TCP}}$: The number of matched bits in the Transmission Control Protocol (TCP) destination port. An exact match corresponds to $16$ bits, while a wildcard corresponds to $0$ bits.\n\nThe rules are to be ordered based on the following criteria: a rule with a strictly larger $S$ value has strictly higher precedence. In case of a tie in $S$, the tie is broken by the lexicographical order of the tuple $(s_{\\text{TCP}}, s_{\\text{IP}}, s_{\\text{VLAN}})$; the rule with the lexicographically larger tuple has higher precedence. \"Higher integers denote higher matching precedence\". We will assign integer priorities from $1$ (lowest) to $6$ (highest).\n\nFirst, we calculate the specificity vector $(s_{\\text{VLAN}}, s_{\\text{IP}}, s_{\\text{TCP}})$ and the combined specificity $S$ for each of the six rules.\n\n- Rule $f_1$: (VLAN $10$, IPv4 $10.0.0.0/8$, TCP any)\n  $s_{\\text{VLAN},1} = 12$, $s_{\\text{IP},1} = 8$, $s_{\\text{TCP},1} = 0$.\n  $S_1 = 12 + 8 + 0 = 20$. Tie-breaking tuple: $(0, 8, 12)$.\n\n- Rule $f_2$: (VLAN $10$, IPv4 $10.1.0.0/16$, TCP $12345$)\n  $s_{\\text{VLAN},2} = 12$, $s_{\\text{IP},2} = 16$, $s_{\\text{TCP},2} = 16$.\n  $S_2 = 12 + 16 + 16 = 44$. Tie-breaking tuple: $(16, 16, 12)$.\n\n- Rule $f_3$: (VLAN $10$, IPv4 $10.1.2.0/24$, TCP $12345$)\n  $s_{\\text{VLAN},3} = 12$, $s_{\\text{IP},3} = 24$, $s_{\\text{TCP},3} = 16$.\n  $S_3 = 12 + 24 + 16 = 52$. Tie-breaking tuple: $(16, 24, 12)$.\n\n- Rule $f_4$: (VLAN any, IPv4 $10.1.2.0/24$, TCP any)\n  $s_{\\text{VLAN},4} = 0$, $s_{\\text{IP},4} = 24$, $s_{\\text{TCP},4} = 0$.\n  $S_4 = 0 + 24 + 0 = 24$. Tie-breaking tuple: $(0, 24, 0)$.\n\n- Rule $f_5$: (VLAN any, IPv4 any, TCP any)\n  $s_{\\text{VLAN},5} = 0$, $s_{\\text{IP},5} = 0$, $s_{\\text{TCP},5} = 0$.\n  $S_5 = 0 + 0 + 0 = 0$. Tie-breaking tuple: $(0, 0, 0)$.\n\n- Rule $f_6$: (VLAN $10$, IPv4 $10.1.2.34/32$, TCP any)\n  $s_{\\text{VLAN},6} = 12$, $s_{\\text{IP},6} = 32$, $s_{\\text{TCP},6} = 0$.\n  $S_6 = 12 + 32 + 0 = 44$. Tie-breaking tuple: $(0, 32, 12)$.\n\nNext, we sort the rules in descending order of precedence.\n1.  $f_3$ has the highest combined specificity, $S_3 = 52$.\n2.  $f_2$ and $f_6$ have the same specificity, $S_2 = S_6 = 44$. We apply the tie-breaking rule by comparing their tuples lexicographically:\n    - $f_2$: $(16, 16, 12)$\n    - $f_6$: $(0, 32, 12)$\n    Since $16 > 0$, the tuple for $f_2$ is lexicographically greater. Thus, $f_2$ has higher precedence than $f_6$.\n3.  The remaining rules have unique $S$ values: $S_4 = 24$, $S_1 = 20$, and $S_5 = 0$.\n\nThe final ranking from highest precedence to lowest is: $f_3, f_2, f_6, f_4, f_1, f_5$.\n\nWe assign distinct integer priorities $p_i$ from $6$ down to $1$:\n- Precedence 1 (highest): $f_3 \\implies p_3 = 6$\n- Precedence 2: $f_2 \\implies p_2 = 5$\n- Precedence 3: $f_6 \\implies p_6 = 4$\n- Precedence 4: $f_4 \\implies p_4 = 3$\n- Precedence 5: $f_1 \\implies p_1 = 2$\n- Precedence 6 (lowest): $f_5 \\implies p_5 = 1$\n\nThe resulting priority assignment vector is $(p_1, p_2, p_3, p_4, p_5, p_6) = (2, 5, 6, 3, 1, 4)$.\n\nPart 2: Worst-Case Per-Hop Latency Calculation\n\nThe worst-case per-hop latency $\\Delta_{\\max}$ for a safety-critical packet (matching rule $f_3$) is the sum of the maximum time spent in each stage of its journey through the switch: processing, queuing, serialization (transmission), and propagation.\n$$ \\Delta_{\\max} = t_{\\text{proc}} + t_{\\text{queue,max}} + t_{\\text{tx,CPS}} + t_{\\text{prop}} $$\n\nWe calculate each component using the provided values:\n- **Processing delay ($t_{\\text{proc}}$)**: This is given as a constant value for any packet.\n  $$ t_{\\text{proc}} = 5 \\,\\mu\\text{s} = 5 \\times 10^{-6} \\, \\text{s} $$\n\n- **Propagation delay ($t_{\\text{prop}}$)**: This is the time for a signal to travel the length of the cable.\n  $$ t_{\\text{prop}} = \\frac{d}{v} = \\frac{100 \\, \\text{m}}{2 \\times 10^{8} \\, \\text{m/s}} = 0.5 \\times 10^{-6} \\, \\text{s} = 0.5 \\,\\mu\\text{s} $$\n\n- **Serialization delay ($t_{\\text{tx,CPS}}$)**: This is the time required to place all bits of the safety-critical packet onto the link. The size is $L_{\\text{CPS}} = 100$ bytes.\n  $$ t_{\\text{tx,CPS}} = \\frac{L_{\\text{CPS}} \\times 8 \\, \\text{bits/byte}}{C} = \\frac{100 \\times 8}{1 \\times 10^{9} \\, \\text{bits/s}} = \\frac{800}{10^9} \\, \\text{s} = 0.8 \\times 10^{-6} \\, \\text{s} = 0.8 \\,\\mu\\text{s} $$\n\n- **Queuing delay ($t_{\\text{queue,max}}$)**: Packets matching $f_3$ are placed in a high-priority queue. All other packets go to a lower-priority queue. The scheduler is strict non-preemptive. The worst-case queuing delay for a high-priority packet occurs when it arrives to find the server (the link) having just started transmitting a maximum-sized lower-priority packet. Due to the non-preemptive nature, the high-priority packet must wait for this transmission to complete. This waiting time is the blocking time. The maximum size for a lower-priority (best-effort) frame is $L_{\\text{BE}} = 1500$ bytes.\n  $$ t_{\\text{queue,max}} = \\frac{L_{\\text{BE}} \\times 8 \\, \\text{bits/byte}}{C} = \\frac{1500 \\times 8}{1 \\times 10^{9} \\, \\text{bits/s}} = \\frac{12000}{10^9} \\, \\text{s} = 12 \\times 10^{-6} \\, \\text{s} = 12 \\,\\mu\\text{s} $$\n  This calculation assumes no other high-priority packets are already in the queue, which is standard for calculating the fundamental worst-case blocking in the absence of traffic characterization for the high-priority flow.\n\n- **Total Latency ($\\Delta_{\\max}$)**: We sum the individual delay components.\n  $$ \\Delta_{\\max} = (5 + 12 + 0.8 + 0.5) \\,\\mu\\text{s} = 18.3 \\,\\mu\\text{s} $$\n  The problem requires the latency to be expressed in milliseconds, rounded to four significant figures.\n  $$ \\Delta_{\\max} = 18.3 \\times 10^{-3} \\, \\text{ms} = 0.0183 \\, \\text{ms} $$\n  Rounding to four significant figures gives $0.01830 \\, \\text{ms}$.\n\nThe final combined answer is the row vector of the six priorities followed by the calculated latency.\n$$ (p_1, p_2, p_3, p_4, p_5, p_6, \\Delta_{\\max}) = (2, 5, 6, 3, 1, 4, 0.01830) $$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 2 & 5 & 6 & 3 & 1 & 4 & 0.01830 \\end{pmatrix} } $$"
        },
        {
            "introduction": "A key advantage of SDN is the ability to dynamically update network policies, but doing so in a live CPS network poses significant challenges to system safety and consistency. Naive updates can create transient states where packets are processed by a mixture of old and new policies, leading to unpredictable behavior. This problem  explores the concept of atomic updates, challenging you to analyze various protocols and identify a correct two-phase commit sequence that leverages versioning to guarantee a seamless and safe policy transition.",
            "id": "4246519",
            "problem": "A Cyber-Physical System (CPS) plant network is controlled via Software-Defined Networking (SDN). A digital twin orchestration service intends to transition the forwarding and policing policy from an existing policy $\\pi_0$ to a new policy $\\pi_1$ for a set of critical control flows $\\mathcal{F}$. The forwarding fabric consists of switches $S_1, S_2, \\dots, S_N$, an ingress gateway $G$ that stamps packets, and a logically centralized SDN controller $C$. The network must maintain the safety invariant that for any time $t$ and any packet $p$ in flight, the forwarding and policing decisions applied to $p$ along its path are derived from a single, well-defined policy version, modeled by a version tag $v \\in \\{0,1\\}$ embedded in a match field set at $G$, and that all matched rules across $S_1, \\dots, S_N$ are consistent with that $v$. The controller interacts with each switch over a reliable control channel with maximum one-way propagation delay $D_{\\max}$, and each switch requires at most $T_{\\text{inst}}$ to install and activate rules after receipt. Packets have a maximum network residence time $T_{\\text{flight}}$ from ingress to egress. There may be concurrent updates from an independent control application that modifies rate-limiters (meters) for the same flows $\\mathcal{F}$, and the controller processes messages asynchronously.\n\nExplain the root cause of race conditions during concurrent updates from first principles, using event ordering and atomicity definitions, and then determine which of the following two-phase commit (Two-Phase Commit, $2\\text{PC}$) sequences guarantees atomic policy transition from $\\pi_0$ to $\\pi_1$ for $\\mathcal{F}$—that is, no packet is processed by a mix of $\\pi_0$ and $\\pi_1$ rules—under arbitrary interleavings and the given bounds $D_{\\max}$, $T_{\\text{inst}}$, and $T_{\\text{flight}}$.\n\nChoose the single best sequence:\n\nA. Prepare: For each $S_i$, install $\\pi_1$ rules matching $v=1$ without removing $\\pi_0$ rules matching $v=0$, and install local fences so that any packet with $v=1$ is accepted and consistently forwarded only after the $\\pi_1$ rules are fully present; issue a barrier to each $S_i$ and wait for acknowledgments; proceed only if all acknowledgments are received before a controller-side timeout $\\tau$ chosen such that $\\tau \\ge D_{\\max} + T_{\\text{inst}}$. Commit: Atomically update $G$ to stamp $v=1$ for new packets in $\\mathcal{F}$; maintain both versions concurrently; after waiting at least $T_{\\text{flight}}$ to allow all $v=0$ packets to drain, garbage-collect $\\pi_0$ rules. Any switch that fails to acknowledge by time $\\tau$ triggers rollback of its $\\pi_1$ staging and leaves the system at $\\pi_0$.\n\nB. Prepare: Update $\\pi_1$ rules and remove $\\pi_0$ rules on $S_1$ and $S_2$; commit by immediately updating $G$ to stamp $v=1$; then asynchronously install $\\pi_1$ rules on the remaining $S_i$ as bandwidth allows. Rely on ordered updates along the path from ingress to egress to minimize inconsistency windows, without using version tags or barriers.\n\nC. Prepare: Install $\\pi_1$ rules on all $S_i$ and remove $\\pi_0$ rules on any $S_i$ that acknowledges quickly; commit after the first barrier acknowledgment is received to minimize delay; update $G$ to stamp $v=1$; if any $S_j$ is slow, packets with $v=1$ are temporarily dropped at $S_j$ until its installation completes.\n\nD. Single-phase optimistic: Broadcast $\\pi_1$ rule updates to all $S_i$ and immediately update $G$ to stamp $v=1$; rely on eventual consistency and the bounded delays $D_{\\max}$ and $T_{\\text{inst}}$ to ensure the network converges rapidly; remove $\\pi_0$ rules after a fixed grace period shorter than $T_{\\text{flight}}$ to reduce overlap.\n\nE. Prepare: Acquire a coarse-grained controller lock on the set $\\mathcal{F}$ to block concurrent meter updates; then sequentially update each $S_i$ in any order by removing $\\pi_0$ rules and installing $\\pi_1$ rules; commit by releasing the lock and updating $G$ to stamp $v=1$; assume that the lock ensures end-to-end atomicity even if packets traverse switches updated at different times.\n\nAnswer by selecting the option that correctly enforces atomicity and prevents race conditions while respecting the given timing bounds and the safety invariant.",
            "solution": "The core of the problem lies in guaranteeing atomicity for a policy update across a distributed system (the set of switches $S_i$) in the presence of network delays and potential concurrent operations. A race condition arises when the correctness of an operation depends on the unpredictable ordering of concurrent events. In this context, there are two primary sources of race conditions.\n\nFirst, from first principles, a distributed system lacks a global clock and simultaneous actions. Event ordering is partial. The policy update operation, which appears as a single logical action, is implemented as a set of concurrent events: message transmissions from the controller $C$ to each switch $S_i$, and subsequent rule installations on each $S_i$. Let $t_{send}(S_i)$ be the time the controller sends an update to $S_i$, $t_{recv}(S_i)$ be the time $S_i$ receives it, and $t_{act}(S_i)$ be the time the new rules are active on $S_i$. Due to variable propagation delay (up to $D_{\\max}$) and installation time (up to $T_{\\text{inst}}$), the activation times $t_{act}(S_i)$ for different switches $S_i$ will be different and occur in an unpredictable order. A data-plane race condition occurs when a packet $p$ traverses the network during this transition window. If $p$ travels from $S_i$ to $S_j$, and $t_{act}(S_i)$ has passed but $t_{act}(S_j)$ has not, $p$ will be processed by the new policy $\\pi_1$ at $S_i$ and the old policy $\\pi_0$ at $S_j$. This observation of a mixed, intermediate state violates the specified safety invariant. The root cause is the race between the packet's propagation through the network and the propagation of the state update through the network.\n\nSecond, a control-plane race condition can occur due to the \"concurrent updates from an independent control application.\" This is a classic Read-Modify-Write hazard. The policy update process might read the current state of a flow's rules (e.g., policy $\\pi_0$ on switch $S_i$) to compute the new rules for $\\pi_1$. Concurrently, another application might also read the same state to apply a modification (e.g., change a rate-limiter). If these operations are interleaved without synchronization, the second write operation can overwrite the first, leading to a \"lost update.\" The root cause here is the lack of mutual exclusion for modifications to shared state representations within the control plane.\n\nAn atomic update protocol must prevent both types of race conditions. It must ensure that no packet observes an inconsistent data plane and that concurrent control-plane modifications do not lead to lost updates. The use of version tags $v \\in \\{0, 1\\}$ provides a fundamental mechanism to achieve this by logically separating the state space of the old and new policies.\n\nNow, we evaluate each proposed sequence:\n\n**A. Prepare: For each $S_i$, install $\\pi_1$ rules matching $v=1$ without removing $\\pi_0$ rules matching $v=0$, and install local fences so that any packet with $v=1$ is accepted and consistently forwarded only after the $\\pi_1$ rules are fully present; issue a barrier to each $S_i$ and wait for acknowledgments; proceed only if all acknowledgments are received before a controller-side timeout $\\tau$ chosen such that $\\tau \\ge D_{\\max} + T_{\\text{inst}}$. Commit: Atomically update $G$ to stamp $v=1$ for new packets in $\\mathcal{F}$; maintain both versions concurrently; after waiting at least $T_{\\text{flight}}$ to allow all $v=0$ packets to drain, garbage-collect $\\pi_0$ rules. Any switch that fails to acknowledge by time $\\tau$ triggers rollback of its $\\pi_1$ staging and leaves the system at $\\pi_0$.**\n\nThis sequence describes a correct two-phase, version-based update protocol.\n1.  **Prepare Phase**: The controller instructs all switches to install the new policy rules ($\\pi_1$) which are matched using version tag $v=1$. Crucially, the old rules ($\\pi_0$, matching $v=0$) are kept. This is a \"make-before-break\" strategy. The use of barriers and waiting for acknowledgments from all switches ensures that the controller does not proceed to the next phase until the entire data plane is prepared to handle $\\pi_1$ traffic. The timeout and rollback mechanism correctly implement the \"abort\" logic of a two-phase commit (2PC) protocol. The timeout condition $\\tau \\ge D_{\\max} + T_{\\text{inst}}$ ensures the controller waits long enough for a message to reach a switch and be processed.\n2.  **Commit Phase**: The commit is a single, atomic action: instructing the ingress gateway $G$ to start stamping new packets with $v=1$. After this point, all new packets will carry the tag $v=1$ and will exclusively match the $\\pi_1$ rules, which are guaranteed to be present on all switches. Packets already in the network with tag $v=0$ will continue to match the still-present $\\pi_0$ rules. Thus, no packet is ever processed by a mixture of $\\pi_0$ and $\\pi_1$ rules. The safety invariant is upheld.\n3.  **Cleanup**: The \"garbage-collection\" step, which removes $\\pi_0$ rules after waiting for $T_{\\text{flight}}$, correctly ensures that all $v=0$ packets have exited the network before their rules are deleted, preventing dropped packets.\nThis protocol correctly addresses the data-plane race condition by ensuring the new path is fully provisioned before use. It also mitigates the control-plane race condition because the update process creates *new* rules for $\\pi_1$ rather than modifying $\\pi_0$ _in situ_, thus isolating the update from concurrent modifications to the active $\\pi_0$ policy.\n\nVerdict: **Correct**.\n\n**B. Prepare: Update $\\pi_1$ rules and remove $\\pi_0$ rules on $S_1$ and $S_2$; commit by immediately updating $G$ to stamp $v=1$; then asynchronously install $\\pi_1$ rules on the remaining $S_i$ as bandwidth allows. Rely on ordered updates along the path from ingress to egress to minimize inconsistency windows, without using version tags or barriers.**\n\nThis protocol is fundamentally flawed. It explicitly creates an inconsistent state by updating only a subset of switches ($S_1, S_2$) before directing new traffic, which is then expected to traverse un-updated switches like $S_3, \\dots, S_N$. A packet processed by $\\pi_1$ at $S_1$ would then be processed by $\\pi_0$ at $S_3$. This is a direct violation of the safety invariant. Eschewing version tags removes the primary mechanism for maintaining consistency. Relying on \"ordered updates along the path\" is not a guarantee in a distributed system with non-deterministic delays.\n\nVerdict: **Incorrect**.\n\n**C. Prepare: Install $\\pi_1$ rules on all $S_i$ and remove $\\pi_0$ rules on any $S_i$ that acknowledges quickly; commit after the first barrier acknowledgment is received to minimize delay; update $G$ to stamp $v=1$; if any $S_j$ is slow, packets with $v=1$ are temporarily dropped at $S_j$ until its installation completes.**\n\nThis is an improperly implemented, overly optimistic protocol. Committing after the *first* acknowledgment means the controller will trigger the ingress gateway $G$ to send packets with $v=1$ while other switches are not yet ready. The protocol acknowledges this will happen and prescribes dropping packets as the solution. For critical CPS control flows, dropping packets is a failure, not a feature of a correct protocol. It violates the implicit goal of a seamless transition. Furthermore, removing $\\pi_0$ rules prematurely on some switches complicates rollback and introduces another source of inconsistency.\n\nVerdict: **Incorrect**.\n\n**D. Single-phase optimistic: Broadcast $\\pi_1$ rule updates to all $S_i$ and immediately update $G$ to stamp $v=1$; rely on eventual consistency and the bounded delays $D_{\\max}$ and $T_{\\text{inst}}$ to ensure the network converges rapidly; remove $\\pi_0$ rules after a fixed grace period shorter than $T_{\\text{flight}}$ to reduce overlap.**\n\nThis \"spray and pray\" approach offers no guarantees of atomicity. There is a clear race condition between a newly stamped packet from $G$ and the propagation of rule updates from $C$. A packet with $v=1$ can easily arrive at a switch $S_i$ before the $\\pi_1$ rules arrive and are installed, causing the packet to be dropped. \"Eventual consistency\" is the wrong consistency model for this problem; it permits transient inconsistencies, which the safety invariant expressly forbids. Removing $\\pi_0$ rules after a period shorter than $T_{\\text{flight}}$ is also incorrect, as it guarantees that some in-flight $v=0$ packets will have their rules removed from under them.\n\nVerdict: **Incorrect**.\n\n**E. Prepare: Acquire a coarse-grained controller lock on the set $\\mathcal{F}$ to block concurrent meter updates; then sequentially update each $S_i$ in any order by removing $\\pi_0$ rules and installing $\\pi_1$ rules; commit by releasing the lock and updating $G$ to stamp $v=1$; assume that the lock ensures end-to-end atomicity even if packets traverse switches updated at different times.**\n\nThis option confuses control-plane synchronization with data-plane atomicity. A lock within the controller process can solve the control-plane race condition (the lost update problem), ensuring that the policy update logic and the meter update logic do not interfere with each other *within the controller's software*. However, this lock has no effect on the distributed state across the switches. The procedure of sequentially updating switches creates a prolonged period of network-wide inconsistency where some switches have $\\pi_0$ and others have $\\pi_1$. The assumption that a controller-side lock \"ensures end-to-end atomicity\" is false; it conflates two different problems. The distributed systems problem of data plane consistency remains completely unsolved.\n\nVerdict: **Incorrect**.\n\nIn conclusion, only option A describes a protocol that correctly uses versioning and a two-phase commit structure to guarantee the safety invariant under the given conditions.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}