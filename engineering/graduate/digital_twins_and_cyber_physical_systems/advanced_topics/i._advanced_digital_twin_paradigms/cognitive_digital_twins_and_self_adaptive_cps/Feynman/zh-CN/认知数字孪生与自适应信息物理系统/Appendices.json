{
    "hands_on_practices": [
        {
            "introduction": "认知数字孪生的一个核心功能是准确感知其物理对应物的状态。然而，传感器数据总是伴随着噪声，并且许多关键状态无法直接测量。本练习将引导你实践卡尔曼滤波器中的核心“更新”步骤，学习如何将带有不确定性的先验知识与新的测量数据进行最优融合，从而获得对系统状态的更精确后验估计。",
            "id": "4208983",
            "problem": "一个用于自适应信息物理系统（CPS）的认知数字孪生，使用单个标量传感器对二维潜在状态进行在线状态估计。该数字孪生采用源于线性高斯贝叶斯条件化原理的最优线性最小均方误差估计器，通常实例化为卡尔曼滤波器（KF）。在时间索引 $k$，状态的先验信念是高斯分布，其均值为 $\\hat{x}_{k}^{-}$，协方差为 $P_{k}^{-}$，测量模型是线性的，并带有加性高斯噪声：\n$$\ny_{k} = H x_{k} + v_{k}, \\quad v_{k} \\sim \\mathcal{N}(0, R),\n$$\n其中 $H \\in \\mathbb{R}^{1 \\times 2}$ 且 $R \\in \\mathbb{R}$。\n\n从线性高斯模型和贝叶斯条件化的核心定义出发，推导在同化 $y_{k}$ 后状态估计的最优线性增益 $K_{k}$ 和后验协方差 $P_{k}^{+}$，并仅用 $H$、$P_{k}^{-}$ 和 $R$ 表示。然后，使用以下科学上合理的参数对后验协方差进行数值计算：\n$$\nP_{k}^{-} = \\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix}, \\quad H = \\begin{bmatrix}1  0\\end{bmatrix}, \\quad R = 1.\n$$\n提供最终的后验协方差，形式为一个精确矩阵，无需四舍五入。此计算不需要物理单位。",
            "solution": "该问题是有效的。它在科学上基于贝叶斯状态估计和卡尔曼滤波器的原理，在数学上是适定的，提供了所有必要的信息，并且是客观陈述的。我们将进行推导和数值评估。\n\n目标是在给定先验信念和线性测量的情况下，找到状态 $x_k \\in \\mathbb{R}^2$ 的最优线性估计器。后验估计 $\\hat{x}_k^+$ 是先验估计 $\\hat{x}_k^-$ 和新测量值 $y_k$ 的线性函数。更新形式如下：\n$$\n\\hat{x}_k^+ = \\hat{x}_k^- + K_k (y_k - \\hat{y}_k^-)\n$$\n其中 $K_k$ 是待确定的增益矩阵，$\\hat{y}_k^-$ 是基于先验的预测测量值。预测测量值是在给定先验信息下 $y_k$ 的期望：\n$$\n\\hat{y}_k^- = E[y_k] = E[H x_k + v_k] = H E[x_k] + E[v_k] = H \\hat{x}_k^- + 0 = H \\hat{x}_k^-\n$$\n项 $y_k - \\hat{y}_k^-$ 是新息或测量残差。因此，后验估计为：\n$$\n\\hat{x}_k^+ = \\hat{x}_k^- + K_k (y_k - H \\hat{x}_k^-)\n$$\n后验估计误差为 $e_k^+ = x_k - \\hat{x}_k^+$。代入 $\\hat{x}_k^+$ 的表达式：\n$$\ne_k^+ = x_k - \\left( \\hat{x}_k^- + K_k (y_k - H \\hat{x}_k^-) \\right)\n$$\n代入测量模型 $y_k = H x_k + v_k$：\n$$\ne_k^+ = (x_k - \\hat{x}_k^-) - K_k ( (H x_k + v_k) - H \\hat{x}_k^- )\n$$\n设先验估计误差为 $e_k^- = x_k - \\hat{x}_k^-$。这可将表达式简化为：\n$$\ne_k^+ = e_k^- - K_k ( H(x_k - \\hat{x}_k^-) + v_k ) = e_k^- - K_k (H e_k^- + v_k)\n$$\n合并各项，我们得到：\n$$\ne_k^+ = (I - K_k H) e_k^- - K_k v_k\n$$\n后验协方差矩阵定义为 $P_k^+ = E[e_k^+ (e_k^+)^\\top]$。我们代入 $e_k^+$ 的表达式：\n$$\nP_k^+ = E \\left[ \\left( (I - K_k H) e_k^- - K_k v_k \\right) \\left( (I - K_k H) e_k^- - K_k v_k \\right)^\\top \\right]\n$$\n展开乘积：\n$$\nP_k^+ = E \\left[ (I - K_k H) e_k^- (e_k^-)^\\top (I - K_k H)^\\top - (I - K_k H) e_k^- v_k^\\top K_k^\\top - K_k v_k (e_k^-)^\\top (I - K_k H)^\\top + K_k v_k v_k^\\top K_k^\\top \\right]\n$$\n先验状态误差 $e_k^-$ 和测量噪声 $v_k$ 不相关，因此 $E[e_k^- v_k^\\top] = 0$ 且 $E[v_k (e_k^-)^\\top] = 0$。使用定义 $E[e_k^- (e_k^-)^\\top] = P_k^-$ 和 $E[v_k v_k^\\top] = R$，表达式简化为：\n$$\nP_k^+ = (I - K_k H) P_k^- (I - K_k H)^\\top + K_k R K_k^\\top\n$$\n这是协方差更新的 Joseph 形式。为了找到最小化均方误差（即 $P_k^+$ 的迹）的最优增益 $K_k$，我们对 $\\text{tr}(P_k^+)$ 关于 $K_k$ 求导，并令结果为零。首先，展开 $P_k^+$ 的表达式：\n$$\nP_k^+ = P_k^- - K_k H P_k^- - P_k^- H^\\top K_k^\\top + K_k H P_k^- H^\\top K_k^\\top + K_k R K_k^\\top\n$$\n$$\nP_k^+ = P_k^- - K_k H P_k^- - P_k^- H^\\top K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top\n$$\n迹是线性算子，所以我们可以对每一项取迹。然后我们使用标准矩阵微积分恒等式（对于对称矩阵 $C$，有 $\\frac{\\partial}{\\partial X} \\text{tr}(AX) = A^\\top$，$\\frac{\\partial}{\\partial X} \\text{tr}(XA^\\top) = A$，$\\frac{\\partial}{\\partial X} \\text{tr}(XCX^\\top) = 2XC$）对 $\\text{tr}(P_k^+)$ 关于 $K_k$ 求导：\n$$\n\\frac{\\partial}{\\partial K_k} \\text{tr}(P_k^+) = 0 - (H P_k^-)^\\top - (P_k^- H^\\top) + 2 K_k (H P_k^- H^\\top + R)\n$$\n因为 $P_k^-$ 是对称的，所以 $(P_k^-)^\\top = P_k^-$。\n$$\n\\frac{\\partial}{\\partial K_k} \\text{tr}(P_k^+) = - (P_k^-)^\\top H^\\top - P_k^- H^\\top + 2 K_k (H P_k^- H^\\top + R) = -2 P_k^- H^\\top + 2 K_k (H P_k^- H^\\top + R)\n$$\n为求最优解，将导数设为零：\n$$\n-2 P_k^- H^\\top + 2 K_k (H P_k^- H^\\top + R) = 0\n$$\n$$\nK_k (H P_k^- H^\\top + R) = P_k^- H^\\top\n$$\n解出最优增益 $K_k$：\n$$\nK_k = P_k^- H^\\top (H P_k^- H^\\top + R)^{-1}\n$$\n这就是最优卡尔曼增益。为了找到后验协方差，我们可以将这个 $K_k$ 的表达式代回到 $P_k^+$ 方程的简化形式中。我们使用 $P_k^+ = P_k^- - K_k H P_k^- - P_k^- H^\\top K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top$。从最优性条件可知，$P_k^- H^\\top = K_k (H P_k^- H^\\top + R)$。将此代入第三项：\n$$\nP_k^+ = P_k^- - K_k H P_k^- - \\left( K_k (H P_k^- H^\\top + R) \\right)^\\top K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top\n$$\n因为 $(AB)^\\top = B^\\top A^\\top$ 且 $(H P_k^- H^\\top + R)$ 是对称的：\n$$\nP_k^+ = P_k^- - K_k H P_k^- - K_k (H P_k^- H^\\top + R) K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top\n$$\n$$\nP_k^+ = P_k^- - K_k H P_k^- = (I - K_k H) P_k^-\n$$\n这是更新后验协方差的最简形式。问题要求 $P_k^+$ 仅用 $H$、$P_k^-$ 和 $R$ 表示。代入 $K_k$ 的表达式：\n$$\nP_{k}^{+} = \\left(I - \\left(P_k^- H^\\top (H P_k^- H^\\top + R)^{-1}\\right) H\\right) P_k^-\n$$\n\n现在，我们用给定的参数进行数值计算：\n$$\nP_{k}^{-} = \\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix}, \\quad H = \\begin{bmatrix}1  0\\end{bmatrix}, \\quad R = 1\n$$\n首先，我们计算新息协方差 $S_k = H P_k^- H^\\top + R$。\n$$\nH P_k^- = \\begin{bmatrix}1  0\\end{bmatrix} \\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix} = \\begin{bmatrix}(1)(2)+(0)(0)  (1)(0)+(0)(1)\\end{bmatrix} = \\begin{bmatrix}2  0\\end{bmatrix}\n$$\n$$\nH P_k^- H^\\top = \\begin{bmatrix}2  0\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = (2)(1) + (0)(0) = 2\n$$\n因为 $R=1$，新息协方差是一个标量：\n$$\nS_k = H P_k^- H^\\top + R = 2 + 1 = 3\n$$\n其逆为 $S_k^{-1} = \\frac{1}{3}$。\n\n接下来，我们计算最优卡尔曼增益 $K_k = P_k^- H^\\top S_k^{-1}$。\n$$\nP_k^- H^\\top = \\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}(2)(1)+(0)(0) \\\\ (0)(1)+(1)(0)\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}\n$$\n$$\nK_k = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\left(\\frac{1}{3}\\right) = \\begin{bmatrix}2/3 \\\\ 0\\end{bmatrix}\n$$\n最后，我们计算后验协方差 $P_k^+ = (I - K_k H) P_k^-$。\n首先，计算 $K_k H$：\n$$\nK_k H = \\begin{bmatrix}2/3 \\\\ 0\\end{bmatrix} \\begin{bmatrix}1  0\\end{bmatrix} = \\begin{bmatrix}(2/3)(1)  (2/3)(0) \\\\ (0)(1)  (0)(0)\\end{bmatrix} = \\begin{bmatrix}2/3  0 \\\\ 0  0\\end{bmatrix}\n$$\n接下来，计算 $(I - K_k H)$：\n$$\nI - K_k H = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} - \\begin{bmatrix}2/3  0 \\\\ 0  0\\end{bmatrix} = \\begin{bmatrix}1 - 2/3  0-0 \\\\ 0-0  1-0\\end{bmatrix} = \\begin{bmatrix}1/3  0 \\\\ 0  1\\end{bmatrix}\n$$\n现在，计算 $P_k^+$：\n$$\nP_k^+ = (I - K_k H) P_k^- = \\begin{bmatrix}1/3  0 \\\\ 0  1\\end{bmatrix} \\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix} = \\begin{bmatrix}(1/3)(2)+(0)(0)  (1/3)(0)+(0)(1) \\\\ (0)(2)+(1)(0)  (0)(0)+(1)(1)\\end{bmatrix}\n$$\n$$\nP_k^+ = \\begin{bmatrix}2/3  0 \\\\ 0  1\\end{bmatrix}\n$$\n后验协方差矩阵如上计算得出。由于信息丰富的测量，第一个状态分量的不确定性从 $2$ 减少到 $2/3$，而未被观测的第二个分量的不确定性保持在 $1$ 不变。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3}  0 \\\\ 0  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在状态估计的基础上，一个真正的“认知”孪生体必须能够学习和调整其内部模型以适应变化。本练习探讨了如何使用普通最小二乘法 (Ordinary Least Squares, OLS) 从观测数据中估计一个关键的系统参数。通过这个实践，你将理解数字孪生体是如何通过数据驱动的方式来提炼和更新其对物理资产动态特性的认识的。",
            "id": "4209011",
            "problem": "一个自适应信息物理系统（CPS）集成了一个认知数字孪生（CDT），该孪生系统持续估计一个标量装置增益，以在变化条件下保持性能。CDT 将装置输出建模为对一个带有加性扰动的控制特征的线性响应，采用回归模型 $y = \\Phi \\theta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$ 是测量输出向量，$\\Phi \\in \\mathbb{R}^{n \\times 1}$ 是特征（设计）向量，$\\theta \\in \\mathbb{R}$ 是代表装置增益的未知标量参数，$\\varepsilon \\in \\mathbb{R}^{n}$ 用于模拟外生扰动。CDT 使用普通最小二乘法（OLS）更新 $\\theta$，OLS 定义为使残差平方和 $\\sum_{i=1}^{n} (y_{i} - \\Phi_{i} \\theta)^{2}$ 最小化的值。\n\n在一个包含 $n = 3$ 次试验的校准阶段，CDT 收集了特征向量和输出\n$$\n\\Phi = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix}, \\quad y = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix}.\n$$\n从作为残差最小化器的 OLS 定义出发（不假设任何预先推导的估计量），推导估计量 $\\hat{\\theta}$ 并精确计算其数值。然后，运用欧几里得空间中几何投影的基本原理，通过将残差向量与 $\\Phi$ 的列空间相关联，定性地解释拟合质量。\n\n请以精确有理数的形式提供 $\\hat{\\theta}$ 的最终数值答案。无需四舍五入，该参数没有单位。解释性讨论应包含在您的推理过程中，但不是最终答案的一部分。",
            "solution": "本题要求在线性模型 $y = \\Phi \\theta + \\varepsilon$ 中，推导并计算标量参数 $\\theta$ 的普通最小二乘法（OLS）估计量，然后对结果进行几何解释。\n\n首先，我们对问题陈述进行验证。\n\n### 第1步：提取已知条件\n- 模型：$y = \\Phi \\theta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$，$\\Phi \\in \\mathbb{R}^{n \\times 1}$，$\\theta \\in \\mathbb{R}$，以及 $\\varepsilon \\in \\mathbb{R}^{n}$。\n- OLS 定义：最小化残差平方和（RSS）$S(\\theta) = \\sum_{i=1}^{n} (y_{i} - \\Phi_{i} \\theta)^{2}$。\n- 试验次数：$n = 3$。\n- 特征向量：$\\Phi = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix}$。\n- 输出向量：$y = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix}$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据，利用了线性回归中标准且基础的统计方法——普通最小二乘法。这是数据分析、控制理论和机器学习中的一个核心概念，直接适用于信息物理系统（CPS）中的参数估计情境。该问题是适定的；由于特征向量 $\\Phi$ 不是零向量，项 $\\Phi^\\top\\Phi$ 将是一个非零标量，因此是可逆的，这保证了估计量 $\\hat{\\theta}$ 的唯一解。该问题是客观的，提供了精确的数值数据和一个清晰、可形式化的目标。所有必要信息均已提供，不存在内部矛盾或科学上不合理的主张。该问题是线性代数和统计学中的一个标准练习，因此是一个有效且可解的问题。\n\n在验证了问题之后，我们开始求解。\n\n### 第1部分：OLS估计量 $\\hat{\\theta}$ 的推导与计算\n\nOLS 估计量 $\\hat{\\theta}$ 定义为使残差平方和 $S(\\theta)$ 最小的 $\\theta$ 值。第 $i$ 个观测的残差为 $e_i = y_i - \\Phi_i \\theta$。残差平方和为：\n$$\nS(\\theta) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\Phi_i \\theta)^2\n$$\n用向量表示法，残差向量为 $e = y - \\Phi\\theta$。RSS 可以写成残差向量的欧几里得范数的平方，$S(\\theta) = \\|e\\|^2 = e^\\top e$。\n$$\nS(\\theta) = (y - \\Phi\\theta)^\\top (y - \\Phi\\theta)\n$$\n展开此表达式，我们得到：\n$$\nS(\\theta) = (y^\\top - (\\Phi\\theta)^\\top) (y - \\Phi\\theta) = (y^\\top - \\theta^\\top \\Phi^\\top) (y - \\Phi\\theta)\n$$\n由于 $\\theta$ 是一个标量，$\\theta^\\top = \\theta$。\n$$\nS(\\theta) = y^\\top y - y^\\top\\Phi\\theta - \\theta\\Phi^\\top y + \\theta\\Phi^\\top\\Phi\\theta\n$$\n项 $y^\\top\\Phi\\theta$ 和 $\\theta\\Phi^\\top y$ 都是标量。一个标量的转置是其自身，所以 $(y^\\top\\Phi\\theta)^\\top = \\theta^\\top\\Phi^\\top y = \\theta\\Phi^\\top y$。这意味着 $y^\\top\\Phi\\theta = \\theta\\Phi^\\top y$。因此，我们可以合并这些项：\n$$\nS(\\theta) = y^\\top y - 2\\theta(\\Phi^\\top y) + \\theta^2(\\Phi^\\top\\Phi)\n$$\n为了找到使这个二次函数最小化的 $\\theta$ 值，我们对 $S(\\theta)$ 关于 $\\theta$ 求导，并令其等于零。\n$$\n\\frac{dS}{d\\theta} = \\frac{d}{d\\theta} \\left( y^\\top y - 2\\theta(\\Phi^\\top y) + \\theta^2(\\Phi^\\top\\Phi) \\right)\n$$\n$$\n\\frac{dS}{d\\theta} = 0 - 2(\\Phi^\\top y) + 2\\theta(\\Phi^\\top\\Phi)\n$$\n将导数设为零，并将最小值对应的 $\\theta$ 记为 $\\hat{\\theta}$：\n$$\n-2(\\Phi^\\top y) + 2\\hat{\\theta}(\\Phi^\\top\\Phi) = 0\n$$\n$$\n2\\hat{\\theta}(\\Phi^\\top\\Phi) = 2(\\Phi^\\top y)\n$$\n求解 $\\hat{\\theta}$ 得到 OLS 估计量：\n$$\n\\hat{\\theta} = (\\Phi^\\top\\Phi)^{-1} (\\Phi^\\top y)\n$$\n这样就完成了从基本原理出发的推导。现在我们使用给定的数据计算其数值：$\\Phi = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix}$ 和 $y = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix}$。\n\n首先，我们计算标量项 $\\Phi^\\top\\Phi$：\n$$\n\\Phi^\\top\\Phi = \\begin{bmatrix}1  2  3\\end{bmatrix} \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix} = (1)(1) + (2)(2) + (3)(3) = 1 + 4 + 9 = 14\n$$\n接着，我们计算标量项 $\\Phi^\\top y$：\n$$\n\\Phi^\\top y = \\begin{bmatrix}1  2  3\\end{bmatrix} \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix} = (1)(2) + (2)(2) + (3)(5) = 2 + 4 + 15 = 21\n$$\n将这些值代回到 $\\hat{\\theta}$ 的表达式中：\n$$\n\\hat{\\theta} = (14)^{-1} (21) = \\frac{21}{14}\n$$\n化简该分数得到精确有理数：\n$$\n\\hat{\\theta} = \\frac{3}{2}\n$$\n\n### 第2部分：拟合质量的几何解释\n\n在欧几里得空间 $\\mathbb{R}^n$ 中，OLS 有着深刻的几何解释。所有可能的模型预测值的集合构成了由设计矩阵的列所张成的子空间。在本题中，设计“矩阵”$\\Phi$ 是一个单独的向量，因此其列空间，记作 $\\text{Col}(\\Phi)$，是 $\\mathbb{R}^3$ 的一个一维子空间——具体来说，是一条穿过原点和点 $(1, 2, 3)$ 的直线。\n$$\n\\text{Col}(\\Phi) = \\{c \\cdot \\Phi \\mid c \\in \\mathbb{R}\\}\n$$\nOLS 过程找到了这个子空间内离观测输出向量 $y$ 最近的向量。这个“最近”的向量是 $y$ 在 $\\text{Col}(\\Phi)$ 上的正交投影。我们将这个投影向量称为 $\\hat{y}$。\n$$\n\\hat{y} = \\Phi\\hat{\\theta} = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix} \\left(\\frac{3}{2}\\right) = \\begin{bmatrix}3/2\\\\ 3\\\\ 9/2\\end{bmatrix}\n$$\n向量 $\\hat{y}$ 是使用该线性模型能得到的对 $y$ 的最佳近似。\n\n“拟合质量”与近似值 $\\hat{y}$ 和实际数据 $y$ 的接近程度有关。这可以通过残差向量 $e$ 来量化：\n$$\ne = y - \\hat{y} = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix} - \\begin{bmatrix}3/2\\\\ 3\\\\ 9/2\\end{bmatrix} = \\begin{bmatrix}4/2 - 3/2\\\\ 4/2 - 6/2\\\\ 10/2 - 9/2\\end{bmatrix} = \\begin{bmatrix}1/2\\\\ -1\\\\ 1/2\\end{bmatrix}\n$$\nOLS 的一个基本性质是残差向量 $e$ 与列空间 $\\text{Col}(\\Phi)$ 正交。这意味着 $e$ 与 $\\text{Col}(\\Phi)$ 中任意向量的点积为零。我们只需验证 $e$ 与张成向量 $\\Phi$ 正交即可：\n$$\n\\Phi^\\top e = \\begin{bmatrix}1  2  3\\end{bmatrix} \\begin{bmatrix}1/2\\\\ -1\\\\ 1/2\\end{bmatrix} = (1)\\left(\\frac{1}{2}\\right) + (2)(-1) + (3)\\left(\\frac{1}{2}\\right) = \\frac{1}{2} - 2 + \\frac{3}{2} = \\frac{4}{2} - 2 = 2 - 2 = 0\n$$\n正交性得到证实。从几何上看，这意味着我们将观测向量 $y$ 分解为两个正交分量：一个位于模型子空间中（$\\hat{y}$），另一个与该子空间正交（$e$）。拟合不是完美的，因为 $e$ 不是零向量。非零残差 $e$ 代表了数据 $y$ 中无法被线性模型解释的部分，这部分归因于扰动项 $\\varepsilon$。拟合的质量与该残差向量的幅值（长度）成反比；较小的幅值表示数据点更接近由 $\\Phi$ 定义的直线，意味着拟合效果更好。",
            "answer": "$$\n\\boxed{\\frac{3}{2}}\n$$"
        },
        {
            "introduction": "拥有了精确的状态估计和完善的系统模型后，数字孪生便可以进行智能决策，从而实现自主适应。模型预测控制 (Model Predictive Control, MPC) 是一种强大的前瞻性控制策略，它通过预测系统未来的行为来计算最优的控制序列。本练习将从基本原理出发，解决一个简单的MPC问题，展示支撑自适应系统背后的预测性优化控制的核心逻辑。",
            "id": "4208970",
            "problem": "一个监控自适应信息物理系统 (CPS) 的认知数字孪生 (CDT) 使用模型预测控制 (MPC) 来计算在每个采样步要应用的控制作用。受控对象是一个标量离散时间积分器，其动态方程为 $x_{k+1} = x_k + u_k$。在时间 $k=0$ 时，该孪生求解一个时域长度为 $N=2$ 的有限时域、无约束MPC问题，其中使用二次阶段成本 $(x_k - x^{\\text{ref}})^2$ 和控制努力成本 $u_k^2$，以及二次终端成本 $(x_N - x^{\\text{ref}})^2$。假设 $x^{\\text{ref}}=0$，$Q=1$，$R=1$，终端权重 $Q_f=1$。初始状态为 $x_0=2$，并且没有状态或输入约束。\n\n从基本离散时间动态方程 $x_{k+1}=x_k+u_k$、有限时域最优控制的二次成本构造以及Bellman最优性原理出发，建立所得到的无约束MPC问题，并精确求解，以获得在 $k=0$ 时要应用的最优初始控制作用 $u_0^\\star$。请以精确的简化有理数形式给出你的答案，不进行四舍五入，不带单位。",
            "solution": "这个问题是有效的，因为它是离散时间最优控制中一个适定的、有科学依据的练习，这是工程学和应用数学中的一个标准课题。它提供了一套完整且一致的给定条件，没有矛盾或歧义。我们将继续进行形式化求解。\n\n问题是为一个有限时域、无约束模型预测控制 (MPC) 问题找到最优初始控制作用 $u_0^\\star$。该系统是一个标量离散时间积分器，其动态方程由下式给出：\n$$x_{k+1} = x_k + u_k$$\n优化在长度为 $N=2$ 的时域上进行。目标是最小化一个二次成本函数。在步骤 $k$ 的阶段成本是 $L(x_k, u_k) = (x_k - x^{\\text{ref}})^2 Q + u_k^2 R$，终端成本是 $V_f(x_N) = (x_N - x^{\\text{ref}})^2 Q_f$。要最小化的总成本是：\n$$J = V_f(x_N) + \\sum_{k=0}^{N-1} L(x_k, u_k)$$\n给定的参数是：\n- 时域：$N=2$\n- 参考状态：$x^{\\text{ref}}=0$\n- 状态权重：$Q=1$\n- 控制权重：$R=1$\n- 终端权重：$Q_f=1$\n- 初始状态：$x_0=2$\n\n代入这些值，成本函数变为：\n$$J(u_0, u_1) = x_2^2 + \\sum_{k=0}^{1} (x_k^2 + u_k^2) = x_2^2 + x_1^2 + u_1^2 + x_0^2 + u_0^2$$\n我们寻求找到最小化此成本的控制序列 $(u_0^\\star, u_1^\\star)$，该序列受系统动态方程的约束。MPC范式规定，只有该序列的第一个元素 $u_0^\\star$ 被应用于系统。\n\n解决此类问题的标准方法是动态规划，它应用了Bellman最优性原理。我们从最后一步 $k=N=2$ 开始，在时间上向后倒推。\n\n在最终时间步 $k=2$ 时，未来成本是终端成本：\n$$J_2(x_2) = x_2^2 Q_f = 1 \\cdot x_2^2 = x_2^2$$\n这是一个二次型 $J_2(x_2) = P_2 x_2^2$，其中 $P_2=1$。\n\n接下来，我们回到 $k=1$。从状态 $x_1$ 出发的最优未来成本由Bellman方程给出：\n$$J_1(x_1) = \\min_{u_1} \\{ L(x_1, u_1) + J_2(x_2) \\}$$\n代入阶段成本和未来成本 $J_2$ 的表达式，并使用系统动态方程 $x_2=x_1+u_1$：\n$$J_1(x_1) = \\min_{u_1} \\{ x_1^2 + u_1^2 + (x_1 + u_1)^2 \\}$$\n为了找到最小化此表达式的控制 $u_1$，我们对其关于 $u_1$ 求导并令其为零：\n$$\\frac{\\partial}{\\partial u_1} \\left( x_1^2 + u_1^2 + (x_1 + u_1)^2 \\right) = 2u_1 + 2(x_1 + u_1) = 0$$\n$$2u_1 + 2x_1 + 2u_1 = 0$$\n$$4u_1 = -2x_1$$\n$$u_1^\\star(x_1) = -\\frac{1}{2}x_1$$\n这是在 $k=1$ 时的最优反馈控制律。将其代回 $J_1(x_1)$ 的表达式，得到从状态 $x_1$ 出发的最优未来成本：\n$$J_1(x_1) = x_1^2 + \\left(-\\frac{1}{2}x_1\\right)^2 + \\left(x_1 - \\frac{1}{2}x_1\\right)^2 = x_1^2 + \\frac{1}{4}x_1^2 + \\left(\\frac{1}{2}x_1\\right)^2$$\n$$J_1(x_1) = x_1^2 + \\frac{1}{4}x_1^2 + \\frac{1}{4}x_1^2 = \\left(1 + \\frac{1}{2}\\right)x_1^2 = \\frac{3}{2}x_1^2$$\n这是一个二次型 $J_1(x_1) = P_1 x_1^2$，其中 $P_1 = \\frac{3}{2}$。\n\n最后，我们回到初始时间步 $k=0$。从初始状态 $x_0$ 出发的最优未来成本是：\n$$J_0(x_0) = \\min_{u_0} \\{ L(x_0, u_0) + J_1(x_1) \\}$$\n使用系统动态方程 $x_1=x_0+u_0$ 和 $J_1(x_1)$ 的表达式：\n$$J_0(x_0) = \\min_{u_0} \\left\\{ x_0^2 + u_0^2 + \\frac{3}{2}(x_0+u_0)^2 \\right\\}$$\n为了找到最优控制 $u_0^\\star$，我们对其关于 $u_0$ 求导并令其为零：\n$$\\frac{\\partial}{\\partial u_0} \\left( x_0^2 + u_0^2 + \\frac{3}{2}(x_0+u_0)^2 \\right) = 2u_0 + \\frac{3}{2} \\cdot 2(x_0+u_0) = 0$$\n$$2u_0 + 3(x_0+u_0) = 0$$\n$$2u_0 + 3x_0 + 3u_0 = 0$$\n$$5u_0 = -3x_0$$\n$$u_0^\\star(x_0) = -\\frac{3}{5}x_0$$\n这是第一步的最优反馈控制律。问题要求给出在初始状态 $x_0=2$ 的情况下，在 $k=0$ 时要应用的特定控制作用。我们将此值代入控制律：\n$$u_0^\\star = -\\frac{3}{5} \\cdot 2 = -\\frac{6}{5}$$\n最优初始控制作用是 $-\\frac{6}{5}$。",
            "answer": "$$\n\\boxed{-\\frac{6}{5}}\n$$"
        }
    ]
}