## 引言
随着机器学习技术深度融入信息物理系统（CPS），这些智能系统在性能和自主性上取得了巨大飞跃。然而，这种集成也引入了新的、严峻的安全挑战，尤其是对微小、恶意扰动（即[对抗性攻击](@entry_id:635501)）的脆弱性，这种脆弱性可能导致物理世界的灾难性后果。本文旨在填补这一关键知识空白，系统性地阐述如何保护支持学习的CPS免受对抗性威胁。为了实现这一目标，我们将首先在**“Principles and Mechanisms”**一章中，从形式化定义风险开始，深入剖析攻击与防御的核心理论。随后，在**“Applications and Interdisciplinary Connections”**一章中，我们将展示这些理论如何在复杂的实际应用中扩展，并与[控制论](@entry_id:262536)、博弈论等领域交叉融合。最后，在**“Hands-On Practices”**部分，读者将通过具体练习，将理论知识应用于关键的分析任务中，从而全面掌握这一前沿领域。

## Principles and Mechanisms

本章旨在深入探讨支持学习的CPS中对抗攻击与防御的基础原则和核心机制。我们将从形式化定义对抗风险开始，逐步剖析攻击者的能力、多样化的攻击向量以及新兴的攻击面。随后，我们将系统地介绍防御策略，涵盖从经验性对策到可验证安全的理论基础，最终将讨论的范畴从[模型鲁棒性](@entry_id:636975)提升至系统级安全性。

### 形式化对抗性挑战

在机器学习的框架内，一个模型的性能通常通过其在某个数据分布上的预期损失来衡量。对于一个分类器 $f$，给定一个从数据生成分布 $\mathcal{D}$ 中抽取的样本 $(x,y)$，以及一个衡量分类错误的非负[损失函数](@entry_id:634569) $\ell$，其**真实风险**（true risk）定义为 $R(f) = \mathbb{E}_{(x,y)\sim \mathcal{D}}[\ell(f(x), y)]$。然而，由于真实分布 $\mathcal{D}$ 未知，我们通常使用从该分布中[独立同分布](@entry_id:169067)采样的数据集 $\{(x_i,y_i)\}_{i=1}^{n}$ 来[估计风险](@entry_id:139340)。这个估计值被称为**[经验风险](@entry_id:633993)**（empirical risk），其标准定义为样本上的平均损失：

$$
\hat{R}(f) = \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i), y_i)
$$

标准的模型训练过程，即**[经验风险最小化](@entry_id:633880)**（Empirical Risk Minimization, ERM），旨在找到一组模型参数，以最小化 $\hat{R}(f)$。然而，这种方法并未考虑输入数据可能存在的恶意扰动。

为了量化模型在面对蓄意攻击时的脆弱性，我们引入了**对抗风险**（adversarial risk）的概念。假设一个攻击者可以在输入 $x$ 上施加一个扰动 $\delta$，该扰动受限于一个特定的范数球，例如 $\|\delta\|_p \le \epsilon$。一个理性的攻击者会选择能够最大化损失的扰动。因此，对于单个输入 $(x,y)$，其在最坏情况下的损失为 $\max_{\delta:\,\|\delta\|_p \le \epsilon} \ell\big(f(x+\delta), y\big)$。对抗风险就是这个最坏情况损失在整个数据分布 $\mathcal{D}$ 上的[期望值](@entry_id:150961) ：

$$
R_{\mathrm{adv}}(f;\epsilon,p) = \mathbb{E}_{(x,y)\sim \mathcal{D}}\!\left[ \max_{\delta:\,\|\delta\|_p \le \epsilon} \ell\!\big(f(x+\delta), y\big) \right]
$$

这个定义体现了对抗分析的核心——一种针对每个输入的、智能的、最坏情况的威胁模型。它与**平均情况鲁棒性**（average-case robustness）形成鲜明对比。在CPS场景中，系统可能受到来[自环](@entry_id:274670)境的随机扰动，这些扰动可以通过[数字孪生](@entry_id:171650)（Digital Twin, DT）建模为一个扰动分布 $\mathcal{Q}$。此时，平均情况风险可以表示为 $\mathbb{E}_{(x,y)\sim \mathcal{D}} \mathbb{E}_{\delta\sim \mathcal{Q}}\![\ell(f(x+\delta),y)]$。由于[期望值](@entry_id:150961)小于或等于最大值，对于任何支撑集在 $\epsilon$-范数球内的扰动分布 $\mathcal{Q}$，平均情况风险总是对抗风险的一个下界。这种区别至关重要，因为它反映了随机噪声和智能攻击之间的根本差异 。

从几何角度看，鲁棒性也可以通过**间隔**（margin）来理解。点对 $(x,y)$ 的**逐点 $p$-范数间隔**（pointwise $p$-norm margin）$m_f(x,y)$ 定义为能够改变分类器对 $x$ 的正确预测所需的最小扰动的大小：$m_f(x,y) = \inf\left\{ \|\delta\|_p : \arg\max_{y'} s_{y'}(x+\delta) \neq y \right\}$，其中 $s_{y'}(x)$ 是类别得分。如果一个点的间隔大于 $\epsilon$，意味着任何大小不超过 $\epsilon$ 的扰动都无法改变其[分类结果](@entry_id:924005)。因此，**基于间隔的鲁棒性**可以定义为模型在分布 $\mathcal{D}$ 上保持分类正确的概率，即 $\mathbb{P}_{(x,y)\sim \mathcal{D}}\![ m_f(x,y) > \epsilon ]$ 。

### 攻击者的画像

理解对抗攻击的第一步是定义攻击者的能力，通常根据其掌握的系统知识和访问权限，分为三种典型的威胁模型 ：

- **白盒（White-box）攻击**：攻击者拥有关于系统的完全信息。在学习型CPS中，这包括学习策略 $\pi_\theta$ 的架构和参数 $\theta$、被控对象和传感器的精确模型（$f$ 和 $h$）、控制器结构，甚至可能包括训练数据集。这种完全的透明度允许攻击者通过整个[闭环系统](@entry_id:270770)计算梯度，从而发动高效的、基于梯度的攻击。

- **黑盒（Black-box）攻击**：攻击者对系统内部一无所知，仅能通过与系统交互来收集信息。例如，他们可以向传感器注入一个测试信号 $\delta y(t)$，并观察系统的输出 $y(t)$。黑盒攻击通常依赖于对查询结果的分析，例如通过[有限差分法](@entry_id:1124968)估计梯度，或训练一个替代模型（surrogate model）来模拟目标系统，然后利用白盒攻击的可迁移性。

- **灰盒（Gray-box）攻击**：此模型介于白盒和黑盒之间，代表攻击者拥有部分知识。例如，攻击者可能知道学习策略 $\pi_\theta$ 的[网络架构](@entry_id:268981)但不知道具体的权重参数 $\theta$，或者拥有一个关于被控对象动力学的粗略模型。

### CPS对抗攻击的分类

CPS中的对抗攻击形式多样，可以根据攻击发生的时间（测试时或训练时）和攻击的目标（输入、模型参数等）进行分类。

#### 测试时规避攻击

这类攻击发生在系统部署后，旨在通过操纵输入来规避检测或引发错误行为。

##### 物理合理性：CPS的核心约束

与纯数字领域的对抗攻击（如[图像分类](@entry_id:1126387)）相比，CPS中的攻击面临着一个根本性的额外约束：**物理合理性**（physical plausibility）。在数字世界中，只要扰动 $\delta \mathbf{x}$ 满足范数预算（例如 $\|\delta \mathbf{x}\|_p \le \varepsilon$），它就是一个有效的对抗样本。然而，在CPS中，一个受扰动的传感器读数序列 $\tilde{y}_{0:T}$ 只有在物理上可能发生时才是一个有效的攻击。

一个扰动是物理上合理的，当且仅当存在一个由控制输入、过程扰动和[测量噪声](@entry_id:275238)构成的有效[系统轨迹](@entry_id:1132840) $(\tilde{x}_{0:T}, \tilde{u}_{0:T}, \tilde{w}_{0:T}, \tilde{\eta}_{0:T})$，它能够产生该受扰动的传感器读数，同时满足系统的所有内在约束。这包括系统的动力学方程 $\tilde{x}_{t+1} = f_p(\tilde{x}_t, \tilde{u}_t, \tilde{w}_t)$，传感器模型 $\tilde{y}_t = h(\tilde{x}_t) + \tilde{\eta}_t$，以及任何已知的物理**不变量**（invariants），如能量守恒定律（[等式约束](@entry_id:175290) $c(x_t)=0$）或安全边界（[不等式约束](@entry_id:176084) $d(x_t) \le 0$）。因此，CPS中可行攻击的集合是纯数字领域中可行攻击集合的一个高度结构化的严格子集。

##### 扰动预算和影响的建模

对扰动大小的度量，即 $p$-范数，在CPS背景下具有具体的物理解释。例如：
- **$\ell_\infty$-范数** ($\|\delta\|_\infty$) 限制了每个传感器通道上的峰值偏差，与传感器信号的最大瞬时误差有关。
- **$\ell_2$-范数** ($\|\delta\|_2$) 与扰动注入的总能量有关。
- **$\ell_0$-范数** ($\|\delta\|_0$) 衡量扰动的[稀疏性](@entry_id:136793)，即受影响的传感器通道数量，这在模拟局部化的物理操纵或故障时特别有用 。

除了范数预算，硬件的物理限制进一步定义了攻击的可行域  。例如：
- **传感器限制**：传感器的**动态范围** $[x_{\min}, x_{\max}]$ 和**量化**（quantization）步长 $q$ 决定了扰动的“可见性”。一个幅值小于量化步长一半的扰动可能不会改变数字输出，因此是“数字上不可察觉的”。此外，传感器的模拟前端具有有限**带宽**，其作用类似于一个低通滤波器，会衰减高频攻击信号。
- **执行器限制**：执行器具有**饱和限制**（如 $\|u_k\|_\infty \le U_{\max}$）和**速率限制**（如 $\|u_{k+1} - u_k\|_\infty \le R_{\max}$）。这些约束不仅限制了攻击信号的瞬时幅值，还限制了其变化速度，施加了时间上的平滑性要求。
- **时序限制**：[数字控制](@entry_id:275588)器以离散的采样周期 $\Delta t$ 运行，这意味着它只能在特定的时间点对输入做出反应，这为攻击的设计和[影响评估](@entry_id:896910)引入了时间结构。

##### 基于梯度的攻击生成机制

在白盒场景下，攻击者可以利用模型梯度来高效地[生成对](@entry_id:906691)抗样本。其基本思想是将损失函数 $\mathcal{L}(\mathbf{x}+\boldsymbol{\delta},y; \boldsymbol{\theta})$ 关于扰动 $\boldsymbol{\delta}$ 进行最大化。

**[快速梯度符号法](@entry_id:635534)**（Fast Gradient Sign Method, FGSM）是一种简单而高效的单步攻击方法。它源于对损失函数的一阶[泰勒展开](@entry_id:145057)：
$$
\mathcal{L}(\mathbf{x}+\boldsymbol{\delta}, y; \boldsymbol{\theta}) \approx \mathcal{L}(\mathbf{x}, y; \boldsymbol{\theta}) + \nabla_{\mathbf{x}}\mathcal{L}(\mathbf{x}, y; \boldsymbol{\theta})^T \boldsymbol{\delta}
$$
为了在 $\ell_\infty$ 范数约束 $\|\boldsymbol{\delta}\|_\infty \le \varepsilon$ 下最大化这个线性近似，扰动 $\boldsymbol{\delta}$ 的每个分量应与梯度的相应分量的符号相同，并取最大可能值 $\varepsilon$。由此产生的扰动为：
$$
\boldsymbol{\delta} = \varepsilon \cdot \text{sign}(\nabla_{\mathbf{x}}\mathcal{L}(\mathbf{x}, y; \boldsymbol{\theta}))
$$
FGSM的核心假设是损失函数在输入点 $\mathbf{x}$ 附近是[局部线性](@entry_id:266981)的 。

**[投影梯度下降](@entry_id:637587)**（Projected Gradient Descent, PGD）是FGSM的迭代版本，也是目前最强大的白盒攻击之一。PGD通过多步梯度上升来寻找更优的扰动，并在每一步之后将结果投影回可行扰动集 $\mathcal{S}$（例如，$\ell_p$ 球和物理约束定义的区域）内。迭代过程如下：
$$
\boldsymbol{\delta}_{k+1} = \Pi_{\mathcal{S}} \left( \boldsymbol{\delta}_k + \alpha \cdot \text{sign}(\nabla_{\mathbf{x}}\mathcal{L}(\mathbf{x}+\boldsymbol{\delta}_k, y; \boldsymbol{\theta})) \right)
$$
其中 $\alpha$ 是步长，$\Pi_{\mathcal{S}}$ 是到可行集 $\mathcal{S}$ 的投影算子。在CPS感知任务中，这些攻击通常是在单个传感器快照（如图像帧）上生成的，这隐含了一个“准静态”（quasi-static）假设，即在攻击生成和应用期间，系统状态变化可以忽略不计 。

#### 扩展的攻击面：CPS-DT集成系统

在集成了数字孪生（DT）的现代CPS中，攻击面远不止于传统的传感器输入和执行器输出 。
- **模型参数篡改**：如果DT和物理系统之间存在用于[在线学习](@entry_id:637955)或模型同步的参数更新通道，攻击者可以篡改这些通道，直接修改控制器 $\pi_\theta$ 的参数 $\theta$。这种攻击直接改变了控制律，可能导致[非线性](@entry_id:637147)的、难以预测的系统行为。
- **协仿真接口攻击**：CPS与DT之间的协仿真（co-simulation）接口，通常用于状态同步和控制验证，引入了新的时序漏洞。这些接口依赖于带时间戳的消息传递。攻击者如果能够操纵这些消息的时间戳或顺序，就可以在控制回路中有效地引入**延迟**。控制理论表明，即使是微小的延迟也可能破坏一个原本稳定的闭环系统。例如，对于一个标量系统 $x_{t+1} = ax_t + bu_t$ 和控制器 $u_t = -kx_t$，在无延迟时系统可能是稳定的。但如果攻击引入一个时间步的延迟，使得控制器变为 $u_t = -kx_{t-1}$，系统的[特征方程](@entry_id:265849)会改变，可能导致其极点移到单位圆外，从而变得不稳定 。

#### 训练时数据投毒攻击

与在测试时操纵输入的规避攻击不同，**数据投毒**（data poisoning）发生在模型的训练阶段。攻击者通过向训练数据集中注入少量精心制作的“毒药”样本，来破坏最终训练出的模型的性能或植入后门。

在CPS的背景下，特别是当训练数据由DT生成时，投毒攻击也必须考虑其[隐蔽](@entry_id:196364)性 。常见的攻击类型包括：
- **目标性投毒**（Targeted poisoning）：攻击的目标是使训练后的模型在某个特定的目标测试点 $(x^*, y^*)$ 上产生错误分类。
- **干净标签投毒**（Clean-label poisoning）：这是一种更隐蔽的攻击方式。攻击者选择一个合法的样本 $(x,y)$，在保持其标签 $y$ 不变的情况下，对其特征 $x$ 进行微小但精心设计的扰动，生成毒药样本 $(\tilde{x}, y)$。由于标签是正确的，这种攻击可以轻易绕过简单的[标签噪声](@entry_id:636605)检测器。

为了防御投毒，[数据管理](@entry_id:893478)者（curator）可能会部署多种防御措施。静态防御可能包括基于马氏距离的**[离群点检测](@entry_id:175858)**。对于由DT生成的[时序数据](@entry_id:636380)，更可能部署**时序一致性检测**，例如使用一步预测器 $g_\phi$ 检查每个数据点 $x_t$ 的“新息”（innovation）$e_t = x_t - g_\phi(x_{t-1})$ 是否过大。聪明的攻击者会利用这些知识来设计更[隐蔽](@entry_id:196364)的攻击。例如，他们会确保毒药样本在特征空间中靠近其类别的中心，以躲避[离群点检测](@entry_id:175858)。更进一步，为了绕过时序检测，攻击者可以将一个大的扰动分解为一系列微小的、分布在多个连续时间步上的扰动。每个小扰动产生的新息都在检测阈值以下，但其累积效应仍能成功投毒 。

### 基础防御策略

针对上述攻击，研究人员发展了多种防御策略，其目标和保证的强度各不相同。

#### 经验性防御：[对抗训练](@entry_id:635216)

**[对抗训练](@entry_id:635216)**（Adversarial Training）是目前最有效的经验性防御方法之一。它没有改变标准的ERM训练框架，而是改变了训练数据本身。其核心思想是“在攻击中训练”，将对抗样本动态地生成并注入到训练过程中。这可以形式化为一个**最小-最大（min-max）**优化问题 ：

$$
\min_{\theta} \mathbb{E}_{(x,y)\sim \mathcal{D}}\left[ \max_{\delta \in \Delta(x)} \ell\!\left(f_{\theta}(x+\delta),y\right) \right]
$$

与ERM（$\min_{\theta} \mathbb{E}[\ell(f_\theta(x),y)]$）相比，[对抗训练](@entry_id:635216)的目标不再是仅仅在干净样本上表现良好，而是在扰动集 $\Delta(x)$ 内的最坏情况扰动下也能表现良好。

然而，[对抗训练](@entry_id:635216)存在一个重要的陷阱，即**梯度掩码**（gradient masking）。由于内部的最大化问题通常是通过几步PGD等近似方法求解的，如果这个内部优化器不够强大（例如，步数太少或步长不当），外部的最小化过程可能会“欺骗”这个弱的攻击者。模型会学到一个损失函数面，其在干净样本点附近的梯度变得非常小或呈高度[非线性](@entry_id:637147)的“破碎”状态。这使得基于梯度的攻击难以找到有效的攻击方向，从而给人一种[模型鲁棒性](@entry_id:636975)很强的“假象”。但这种鲁棒性并非真实的，因为更强大的攻击（例如，使用更多迭代步数、随机重启的PGD，或从其他模型迁移来的攻击）仍然可以轻易地找到对抗样本。因此，评估[对抗训练](@entry_id:635216)效果时，必须使用强大的、自适应的攻击策略，以避免被梯度掩码所误导 。

#### 可验证防御：认证鲁棒性

与依赖于特定攻击算法进行评估的经验性鲁棒性不同，**认证鲁棒性**（certified robustness）提供了一种数学上可证明的、针对某一扰动集内**所有**可能攻击的 worst-case 保证 。

##### 通过[利普希茨连续性](@entry_id:142246)进行认证

一个函数的**[利普希茨常数](@entry_id:146583)**（Lipschitz constant）是其输出变化率相对于输入变化率的[上界](@entry_id:274738)。对于一个以 $L_f$ 为[利普希茨常数](@entry_id:146583)的函数 $f$，我们有 $\|f(x_1) - f(x_2)\|_2 \le L_f \|x_1 - x_2\|_2$。这个性质可以直接用于认证鲁棒性。如果我们可以计算或约束一个神经网络的[利普希茨常数](@entry_id:146583) $L_f$，那么对于任何大小为 $\|\delta\|_2 \le \epsilon$ 的扰动，其输出的变化量被限制在 $\|f(x+\delta) - f(x)\|_2 \le L_f \epsilon$ 之内。同样，如果[损失函数](@entry_id:634569) $\ell$ 本身是 $L_\ell$-Lipschitz 的，那么损失的变化也可以被约束：$|\ell(f(x+\delta), y) - \ell(f(x), y)| \le L_\ell L_f \epsilon$ 。

对于一个由多层[函数复合](@entry_id:144881)而成的神经网络 $f = f_{L-1} \circ \dots \circ f_0$，其中每层 $f_i(x) = \phi_i(W_i x + b_i)$，其全局[利普希茨常数](@entry_id:146583)可以通过各层[利普希茨常数](@entry_id:146583)的乘积来界定。对于第 $i$ 层，其[利普希茨常数](@entry_id:146583)是其线性部分（权重矩阵 $W_i$）和[非线性激活函数](@entry_id:635291) $\phi_i$ 的[利普希茨常数](@entry_id:146583)的乘积。权重矩阵 $W_i$ 在 $\ell_2$ 范数下的[利普希茨常数](@entry_id:146583)正是其**[算子范数](@entry_id:752960)**（或[谱范数](@entry_id:143091)），即其最大[奇异值](@entry_id:152907) $\|W_i\|_2$。如果[激活函数](@entry_id:141784)是1-Lipschitz的（如ReLU、[tanh](@entry_id:636446)、sigmoid），则全局[利普希茨常数](@entry_id:146583)的一个[上界](@entry_id:274738)为 $L_f \le \prod_{i=0}^{L-1} \|W_i\|_2$。这个界限同样适用于卷积层，因为卷积也是一种线性操作，可以表示为一个（通常是结构化的）矩阵 。

##### 通过[随机平滑](@entry_id:634498)进行概率性认证

**[随机平滑](@entry_id:634498)**（Randomized Smoothing）是另一种强大的认证鲁棒性技术 。它不直接认证原始的、确定性的分类器 $f$，而是构造一个新的**平滑分类器**（smoothed classifier）$g$。$g(x)$ 的输出被定义为当向 $x$ 添加高斯噪声 $\eta \sim \mathcal{N}(0, \sigma^2 I)$ 时，$f$ 最可能输出的类别：

$$
g(x) = \arg\max_{c \in \{1, \dots, C\}} \mathbb{P}( f(x + \eta) = c )
$$

Cohen等人 (2019) 的一个关键定理表明，如果平滑分类器 $g$ 在输入 $x$ 处预测类别 $c_A$ 的概率下界为 $\underline{p_A}$，那么可以保证 $g$ 在以 $x$ 为中心、半径为 $R = \sigma \Phi^{-1}(\underline{p_A})$ 的 $\ell_2$ 范数球内，其预测结果恒为 $c_A$。这里 $\Phi^{-1}$ 是标准高斯[累积分布函数](@entry_id:143135)的[反函数](@entry_id:141256)。

在实践中，概率 $\underline{p_A}$ 是通过[蒙特卡洛采样](@entry_id:752171)估计的，因此这个保证是**概率性的**：我们可以用 $1-\alpha$ 的[置信度](@entry_id:267904)声明，所计算出的半径 $R$ 是有效的。总结来说，[随机平滑](@entry_id:634498)为平滑分类器 $g$（而非基础分类器 $f$）提供了一个非平凡的、可计算的、但带有统计置信度的 $\ell_2$ 鲁棒性半径。

#### 系统级[安全验证](@entry_id:1131179)

最终，在CPS中，我们关心的不仅仅是单个ML组件的鲁棒性，而是整个[闭环系统](@entry_id:270770)的**安全性**（safety）。这需要我们将分析从输入-输出关系扩展到系统状态的时序演化。

**对抗可达性分析**（Adversarial Reachability Analysis）为此提供了形式化框架 。给定一个初始状态集 $X_0$ 和一个定义了所有允许扰动信号的空间 $\Delta$，**对抗可达集**（adversarial reachable set）$\mathcal{R}^{\mathrm{adv}}(t; X_{0})$ 是指在时间 $t$ 所有可能被系统达到的状态的集合。它考虑了从 $X_0$ 出发的所有初始状态，以及在时间 $[0, t]$ 内所有可行的对[抗扰动](@entry_id:262021)信号 $\delta(\cdot)$。形式上，它是通过对所有这些可能性的联合（union）来定义的：

$$
\mathcal{R}^{\mathrm{adv}}(t; X_{0}) = \left\{ x(t) \,\middle|\, \exists x(\cdot), \exists \delta(\cdot): [0, t] \to \Delta, x(0) \in X_{0}, \dot{x}(\tau) = f\big(x(\tau), \pi\big(h(x(\tau)) + \delta(\tau)\big)\big) \text{ a.e. on } [0, t] \right\}
$$

随时间演化的可达集构成了一个**可达管道**（reachable tube） $\bigcup_{s \in [0, T]} \mathcal{R}^{\mathrm{adv}}(s; X_{0})$。系统的鲁棒安全性可以被定义为：对于所有初始状态、所有允许的对抗信号和所有时间点，系统的状态轨迹始终停留在预定义的安全集 $S$ 内。验证这一属性等价于[证明系统](@entry_id:156272)的可达管道与不安[全集](@entry_id:264200) $U=X \setminus S$ 的交集为空，即 $\bigcup_{s \in [0, T]} \mathcal{R}^{\mathrm{adv}}(s; X_{0}) \cap U = \emptyset$ 。这种方法将[对抗鲁棒性](@entry_id:636207)问题转化为控制理论中的一个[形式验证](@entry_id:149180)问题，为保证学习型CPS的最高级别安全提供了理论基础。