{
    "hands_on_practices": [
        {
            "introduction": "The parity-space approach offers a direct and powerful method for fault detection by checking the consistency of sensor data against a system's dynamic model. By designing a transformation that eliminates the influence of unknown states, we can generate residuals that are non-zero only in the presence of faults or noise. This foundational exercise  will guide you through the complete workflow, from constructing a parity matrix for a given system to using the resulting residuals and fault signatures to isolate a specific sensor bias from measurement data.",
            "id": "4221798",
            "problem": "A discrete-time linear time-invariant cyber-physical system (CPS) is modeled in state space as $x_{k+1} = A x_{k} + B u_{k}$ and $y_{k} = C x_{k} + f b$, where $x_{k} \\in \\mathbb{R}^{n}$ is the state, $u_{k} \\in \\mathbb{R}^{p}$ is the input, $y_{k} \\in \\mathbb{R}^{m}$ is the sensor output, $f \\in \\mathbb{R}^{m}$ is an unknown constant sensor fault direction that equals a canonical basis vector $e_{i}$ if the $i$-th sensor has a constant bias, and $b \\in \\mathbb{R}$ is the (unknown) bias magnitude. Assume zero input, $u_{k} = 0$, and an unknown initial condition $x_{0}$. The parity-space principle for fault detection, isolation, and diagnosis (FDI) requires a matrix $W$ such that $W \\mathcal{O}_{L} = 0$, where $\\mathcal{O}_{L}$ is the extended observability over a finite horizon of length $L$. Then the parity residual $r = W Y$ is insensitive to $x_{0}$ (and to $u_{k}$ when present) but sensitive to sensor faults, where $Y$ is the stacked output over the horizon.\n\nConsider the system with $n = 1$, $m = 2$, and horizon $L = 2$, with\n$$\nA = [\\,0.6\\,], \\quad B = [\\,1\\,], \\quad C = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.\n$$\nThe stacked output over two steps is $Y = \\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix} \\in \\mathbb{R}^{4}$, and the extended observability is\n$$\n\\mathcal{O}_{2} = \\begin{bmatrix} C \\\\ C A \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0.6 \\\\ 1.2 \\end{bmatrix}.\n$$\nAssume exactly one sensor has a constant bias over the horizon. The measured outputs are\n$$\ny_{0} = \\begin{bmatrix} 1.9 \\\\ 3.0 \\end{bmatrix}, \\qquad y_{1} = \\begin{bmatrix} 1.3 \\\\ 1.8 \\end{bmatrix}.\n$$\nTasks:\n1) From first principles, construct a nontrivial parity matrix $W \\in \\mathbb{R}^{q \\times 4}$ with $q = 4 - 1 = 3$ such that $W \\mathcal{O}_{2} = 0$.\n2) Using the definition of parity residuals, compute $r = W Y$.\n3) For isolation, define the fault signature vectors in parity space $s_{i} = W F_{L} e_{i}$, where $F_{L} = \\begin{bmatrix} I_{2} \\\\ I_{2} \\end{bmatrix}$ and $e_{i} \\in \\mathbb{R}^{2}$ is the $i$-th canonical basis vector. Design a decision rule that selects the index $i$ maximizing the normalized correlation\n$$\ng_{i} = \\frac{| \\langle r, s_{i} \\rangle |}{\\lVert r \\rVert \\, \\lVert s_{i} \\rVert},\n$$\nand apply it to the given data.\nProvide as your final answer only the isolated sensor index $i$ as an integer (use $1$-based indexing). No rounding is required and no units are needed.",
            "solution": "The user has asked for a solution to a problem in fault detection and isolation.\n\n### Problem Validation\n\nThe problem is firmly based on the established theory of fault detection and isolation using the parity-space approach for LTI systems. All concepts—state-space models, observability, parity residuals, and fault signatures—are standard in control engineering and system identification. The problem provides a clear objective and all necessary data to carry out the required tasks, making it a well-posed and valid exercise.\n\n### Solution\n\nThe solution proceeds by addressing the three tasks specified in the problem statement.\n\n#### Task 1: Construct the Parity Matrix $W$\n\nThe parity matrix $W$ must satisfy the condition $W \\mathcal{O}_{2} = 0$. This means that the rows of $W$ must form a basis for the left null space of $\\mathcal{O}_{2}$. The matrix $\\mathcal{O}_{2}$ is a $4 \\times 1$ matrix:\n$$\n\\mathcal{O}_{2} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0.6 \\\\ 1.2 \\end{bmatrix}\n$$\nThe left null space of $\\mathcal{O}_{2} \\in \\mathbb{R}^{4 \\times 1}$ has dimension $4 - \\text{rank}(\\mathcal{O}_{2}) = 4 - 1 = 3$. This matches the required row dimension $q=3$ of $W$. Let a row vector of $W$ be $w^{T} = \\begin{bmatrix} w_1 & w_2 & w_3 & w_4 \\end{bmatrix}$. The condition $w^{T}\\mathcal{O}_{2} = 0$ implies the scalar equation:\n$$\n1 \\cdot w_1 + 2 \\cdot w_2 + 0.6 \\cdot w_3 + 1.2 \\cdot w_4 = 0\n$$\nWe need to find three linearly independent row vectors that satisfy this equation. A systematic way is to find a basis for the null space. A standard choice for the parity matrix $W$ can be constructed as follows:\n$$\nW = \\begin{bmatrix} -2 & 1 & 0 & 0 \\\\ -0.6 & 0 & 1 & 0 \\\\ -1.2 & 0 & 0 & 1 \\end{bmatrix}\n$$\nThis matrix consists of three linearly independent rows, and each row vector is orthogonal to $\\mathcal{O}_2$.\n\n#### Task 2: Compute the Residual $r$\n\nThe residual is defined as $r = W Y$. First, we construct the stacked output vector $Y$ from the given measurements $y_0$ and $y_1$.\n$$\nY = \\begin{bmatrix} y_0 \\\\ y_1 \\end{bmatrix} = \\begin{bmatrix} 1.9 \\\\ 3.0 \\\\ 1.3 \\\\ 1.8 \\end{bmatrix}\n$$\nNow, we compute the product $r = W Y$:\n$$\nr = \\begin{bmatrix} -2 & 1 & 0 & 0 \\\\ -0.6 & 0 & 1 & 0 \\\\ -1.2 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1.9 \\\\ 3.0 \\\\ 1.3 \\\\ 1.8 \\end{bmatrix} = \\begin{bmatrix} (-2)(1.9) + (1)(3.0) \\\\ (-0.6)(1.9) + (1)(1.3) \\\\ (-1.2)(1.9) + (1)(1.8) \\end{bmatrix} = \\begin{bmatrix} -3.8 + 3.0 \\\\ -1.14 + 1.3 \\\\ -2.28 + 1.8 \\end{bmatrix} = \\begin{bmatrix} -0.8 \\\\ 0.16 \\\\ -0.48 \\end{bmatrix}\n$$\n\n#### Task 3: Isolate the Fault\n\nFirst, we compute the fault signature vectors $s_i = W F_L e_i$. The matrix $F_L$ is:\n$$\nF_{L} = \\begin{bmatrix} I_{2} \\\\ I_{2} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n$$\nThe canonical basis vectors in $\\mathbb{R}^2$ are $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n\nFor a fault in sensor $1$ ($i=1$):\nThe fault vector in output space is $F_L e_1$, which is the first column of $F_L$. The signature $s_1$ is:\n$$\ns_1 = W (F_L e_1) = \\begin{bmatrix} -2 & 1 & 0 & 0 \\\\ -0.6 & 0 & 1 & 0 \\\\ -1.2 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -0.6+1 \\\\ -1.2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 0.4 \\\\ -1.2 \\end{bmatrix}\n$$\n\nFor a fault in sensor $2$ ($i=2$):\nThe fault vector in output space is $F_L e_2$, which is the second column of $F_L$. The signature $s_2$ is:\n$$\ns_2 = W (F_L e_2) = \\begin{bmatrix} -2 & 1 & 0 & 0 \\\\ -0.6 & 0 & 1 & 0 \\\\ -1.2 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\n$$\n\nNext, we apply the decision rule by computing the normalized correlation $g_i$ for each possible fault. We have $r = \\begin{bmatrix} -0.8 \\\\ 0.16 \\\\ -0.48 \\end{bmatrix}$. The residual vector $r$ is a scalar multiple of the signature vector $s_1$, specifically $r = 0.4 s_1$.\n$$\n0.4 s_1 = 0.4 \\begin{bmatrix} -2 \\\\ 0.4 \\\\ -1.2 \\end{bmatrix} = \\begin{bmatrix} -0.8 \\\\ 0.16 \\\\ -0.48 \\end{bmatrix} = r\n$$\nSince $r$ is perfectly aligned with $s_1$, the normalized correlation $g_1$ will be $1$. The vector $r$ is not a scalar multiple of $s_2$, so $g_2$ will be less than $1$.\nCalculation for $g_1$:\n$$\ng_1 = \\frac{| \\langle r, s_{1} \\rangle |}{\\lVert r \\rVert \\, \\lVert s_{1} \\rVert} = \\frac{| \\langle 0.4s_1, s_{1} \\rangle |}{\\lVert 0.4s_1 \\rVert \\, \\lVert s_{1} \\rVert} = \\frac{0.4 | \\langle s_1, s_{1} \\rangle |}{0.4 \\lVert s_1 \\rVert \\, \\lVert s_{1} \\rVert} = \\frac{\\lVert s_1 \\rVert^2}{\\lVert s_1 \\rVert^2} = 1\n$$\nCalculation for $g_2$:\n$$\n\\langle r, s_{2} \\rangle = (-0.8)(1) + (0.16)(0) + (-0.48)(1) = -0.8 - 0.48 = -1.28\n$$\n$$\n\\lVert r \\rVert^2 = (-0.8)^2 + (0.16)^2 + (-0.48)^2 = 0.64 + 0.0256 + 0.2304 = 0.896\n$$\n$$\n\\lVert s_2 \\rVert^2 = 1^2 + 0^2 + 1^2 = 2\n$$\n$$\ng_2 = \\frac{| -1.28 |}{\\sqrt{0.896} \\sqrt{2}} = \\frac{1.28}{\\sqrt{1.792}} \\approx 0.956  1\n$$\nThe decision rule is to select the index $i$ that maximizes $g_i$. Since $g_1 = 1$ is the maximum, the fault is isolated to sensor $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "In stochastic systems, the Kalman filter provides an optimal state estimate, and its innovation sequence serves as a natural residual for fault detection. However, a crucial trade-off exists: a filter tuned for aggressive noise rejection can sometimes learn to track a fault, effectively masking it from detection. This practice  delves into this fundamental conflict by having you derive the steady-state Kalman gain and analyze how it directly impacts the sensitivity to a constant sensor fault, providing critical insight into the design and tuning of robust monitoring systems.",
            "id": "4221867",
            "problem": "Consider a discrete-time, scalar Cyber-Physical System (CPS) and its Digital Twin (DT) estimator. The plant evolves according to the linear time-invariant stochastic dynamics\n$$\nx_{k+1} = a\\,x_{k} + w_{k},\n$$\nwith the measurement model\n$$\ny_{k} = x_{k} + v_{k} + f_{k}.\n$$\nAssume the process noise $w_{k}$ and measurement noise $v_{k}$ are mutually independent, zero-mean, white, Gaussian sequences with covariances $\\mathbb{E}[w_{k}^{2}] = Q  0$ and $\\mathbb{E}[v_{k}^{2}] = R  0$, respectively. The scalar parameter $a$ satisfies $|a|  1$. The additive fault $f_{k}$ represents an unknown bias injected into the measurement channel. The Digital Twin employs a steady-state Kalman Filter (KF) with a constant gain $K$ and the predictor-corrector recursion\n$$\n\\hat{x}_{k}^{-} = a\\,\\hat{x}_{k-1}, \\quad r_{k} = y_{k} - \\hat{x}_{k}^{-}, \\quad \\hat{x}_{k} = \\hat{x}_{k}^{-} + K\\,r_{k},\n$$\nwhere $r_{k}$ is the innovation. Define the a priori estimation error $e_{k}^{-} = x_{k} - \\hat{x}_{k}^{-}$ and its covariance $P_{k}^{-} = \\mathbb{E}[(e_{k}^{-})^{2}]$, and the a posteriori estimation error $e_{k} = x_{k} - \\hat{x}_{k}$ with covariance $P_{k} = \\mathbb{E}[e_{k}^{2}]$.\n\nStarting from the foundational definitions above and the least-mean-square optimality principle for linear Gaussian estimation, derive the steady-state Kalman gain $K$ as a closed-form analytic expression in terms of $a$, $Q$, and $R$. Then, using the innovation representation, analyze how $K$ affects fault sensitivity by expressing the innovation covariance $S = \\mathbb{E}[(r_{k} - \\mathbb{E}[r_{k}])^{2}]$ in steady state and discussing the dependence of the normalized fault effect $F/\\sqrt{S}$, where $f_{k} \\equiv F$ is a constant bias, on $K$, $Q$, and $R$.\n\nYour final answer must be the closed-form analytic expression for the steady-state Kalman gain $K$ in terms of $a$, $Q$, and $R$. No numerical evaluation is required.",
            "solution": "The problem statement is a well-posed and scientifically grounded exercise in steady-state Kalman filtering and fault diagnosis, foundational topics in the study of Cyber-Physical Systems and Digital Twins. All provided information is self-contained, consistent, and adheres to standard formulations in estimation theory. The problem is therefore valid.\n\nThe derivation proceeds in two parts as requested: first, the derivation of the steady-state Kalman gain $K$, and second, the analysis of fault sensitivity.\n\n### Part 1: Derivation of the Steady-State Kalman Gain $K$\n\nThe optimal Kalman filter is designed based on the system and measurement models, assuming the fault $f_k$ is zero. The standard Kalman filter equations for the error covariances are:\n1. Covariance prediction: $P_{k}^{-} = a^2 P_{k-1} + Q$\n2. Optimal gain: $K_k = \\frac{P_k^{-}}{P_k^{-} + R}$\n3. Covariance correction: $P_k = (1-K_k)P_k^{-}$\n\nIn steady state, the covariances and gain become constant: $P_k^{-} \\to P^{-}$, $P_k \\to P$, and $K_k \\to K$. The equations become:\n1. $P^{-} = a^2 P + Q$\n2. $K = \\frac{P^{-}}{P^{-} + R}$\n3. $P = (1-K)P^{-}$\n\nWe derive a single equation for the steady-state gain $K$. From relation (2), we express $P^{-}$ in terms of $K$:\n$$K(P^{-} + R) = P^{-} \\implies KP^{-} + KR = P^{-} \\implies KR = P^{-}(1-K) \\implies P^{-} = \\frac{KR}{1-K}$$\nFrom relation (3), we express $P$ in terms of $K$ and $P^{-}$:\n$$P = (1-K)P^{-} = (1-K) \\left( \\frac{KR}{1-K} \\right) = KR$$\nNow, substitute these expressions for $P^{-}$ and $P$ into relation (1), which is a form of the discrete-time Algebraic Riccati Equation (ARE):\n$$P^{-} = a^2 P + Q \\implies \\frac{KR}{1-K} = a^2(KR) + Q$$\nThis equation contains only the unknown steady-state gain $K$ and the known system parameters $a$, $Q$, and $R$. We rearrange it into a standard quadratic form $A_K K^2 + B_K K + C_K = 0$:\n$$KR = (a^2 KR + Q)(1-K)$$\n$$KR = a^2 KR - a^2 KR^2 + Q - QK$$\n$$a^2 R K^2 + (KR - a^2 KR + QK) - Q = 0$$\n$$a^2 R K^2 + (R(1-a^2) + Q)K - Q = 0$$\n\nThis is a quadratic equation for $K$. The coefficients are $A_K = a^2 R$, $B_K = (1-a^2)R + Q$, and $C_K = -Q$. Since $|a|1$, $Q0$, and $R0$, we have $A_K \\ge 0$, $B_K0$, and $C_K0$. The solution is given by the quadratic formula:\n$$K = \\frac{-B_K \\pm \\sqrt{B_K^2 - 4A_K C_K}}{2A_K}$$\n$$K = \\frac{-((1-a^2)R + Q) \\pm \\sqrt{((1-a^2)R + Q)^2 - 4(a^2 R)(-Q)}}{2a^2 R}$$\n$$K = \\frac{-((1-a^2)R + Q) \\pm \\sqrt{((1-a^2)R + Q)^2 + 4a^2 QR}}{2a^2 R}$$\nThe Kalman gain must be physically meaningful. For a stable system with $Q,R0$, the steady-state gain $K$ must be in the range $(0, 1)$. The term under the square root is strictly greater than $B_K^2$, so $\\sqrt{B_K^2 - 4A_K C_K}  B_K$. The solution with the minus sign would yield a negative value for $K$, which is not physical. Therefore, we must take the positive root.\n\n### Part 2: Fault Sensitivity Analysis\n\nNow we analyze the behavior of the filter in the presence of a constant additive fault, $f_k = F$. The innovation is $r_k = y_k - \\hat{x}_k^{-} = (x_k + v_k + F) - \\hat{x}_k^{-} = e_k^{-} + v_k + F$.\nThe fault introduces a bias in the innovation. The steady-state mean of the innovation is:\n$$\\mathbb{E}[r_k] = \\frac{(1-a)F}{1-a+aK}$$\nThe innovation covariance is $S = \\mathbb{E}[(r_{k} - \\mathbb{E}[r_{k}])^{2}]$. The bias does not affect the covariance, so it remains the same as in the fault-free case:\n$$S = P^{-} + R$$\nFrom the gain equation $K = P^{-}/(P^{-}+R)$, we have $S = P^{-}/K$. From the ARE derivation, we also found $P^{-} = \\frac{KR}{1-K}$. Therefore:\n$$S = \\frac{1}{K} \\left( \\frac{KR}{1-K} \\right) = \\frac{R}{1-K}$$\nThe normalized fault effect is a measure of detectability, essentially a signal-to-noise ratio for the fault. It is given by $\\mathbb{E}[r_k]/\\sqrt{S}$. However, the problem asks for the dependence of $F/\\sqrt{S}$, which simplifies the analysis by focusing on how the system parameters amplify or attenuate the raw fault magnitude $F$ relative to the innovation noise level.\n$$\\frac{F}{\\sqrt{S}} = \\frac{F}{\\sqrt{R/(1-K)}} = F\\sqrt{\\frac{1-K}{R}}$$\nThis expression quantifies the detectability of the fault.\n- If the gain $K \\to 1$ (e.g., when $R \\to 0$ or $Q \\to \\infty$), the normalized fault effect goes to $0$. The filter tracks measurements so aggressively that it \"learns\" the fault as a new baseline, masking it.\n- If the gain $K \\to 0$ (e.g., when $R \\to \\infty$ or $Q \\to 0$), the normalized fault effect approaches $F/\\sqrt{R}$. The filter relies more on its model, so a persistent measurement bias $F$ stands out strongly against the large measurement noise $R$.\nThis reveals the fundamental trade-off: high-gain filters are good for tracking but poor for detecting constant faults, while low-gain filters are robust to noise but more sensitive to such faults.\n\nThe final answer required is the closed-form expression for $K$.",
            "answer": "$$\n\\boxed{\\frac{-((1-a^{2})R + Q) + \\sqrt{((1-a^{2})R + Q)^{2} + 4a^{2}QR}}{2a^{2}R}}\n$$"
        },
        {
            "introduction": "Beyond simply ensuring stable state estimation, observer design can be tailored to actively enhance fault diagnosis capabilities. Eigenstructure assignment is an advanced technique that allows a designer to not only place the observer's poles for desired error dynamics but also to shape the corresponding eigenvectors to control the direction and magnitude of the system's response to faults. In this problem , you will apply this principle to design a Luenberger observer that is explicitly optimized to maximize the sensitivity of the residual to a specific actuator fault, showcasing a proactive approach to building high-performance diagnostic systems.",
            "id": "4221840",
            "problem": "A digital twin for a cyber-physical system uses a Luenberger observer to generate residuals for Fault Detection, Isolation, and Diagnosis (FDI). Consider the continuous-time linear time-invariant plant\n$$\n\\dot{x}(t) = A x(t) + B u(t) + B_f f, \\quad y(t) = C x(t),\n$$\nwhere $x(t) \\in \\mathbb{R}^{2}$ is the state, $u(t) \\in \\mathbb{R}$ is a known control input, $f \\in \\mathbb{R}$ is a constant actuator bias fault entering through $B_f$, and $y(t) \\in \\mathbb{R}^{2}$ is the measured output. The plant matrices are\n$$\nA = \\begin{bmatrix} 0  1 \\\\ -2  -1 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}, \\quad B_f = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\nThe observer has the standard Luenberger form and uses the residual for detection,\n$$\nr(t) = y(t) - \\hat{y}(t), \\quad \\hat{y}(t) = C \\hat{x}(t),\n$$\nand a scalar residual channel is formed as\n$$\nr_s(t) = w^{\\top} r(t), \\quad w = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nTo maintain stability via pole placement while enabling sensitivity eigenstructure tuning, design the observer gain $L \\in \\mathbb{R}^{2 \\times 2}$ such that the error dynamics matrix $A - L C$ has the prescribed eigenvalues\n$$\n\\Lambda = \\operatorname{diag}(-1, -3).\n$$\nLet the right-eigenvector matrix of $A - L C$ be\n$$\nV = \\begin{bmatrix} 1  1 \\\\ t  1 \\end{bmatrix},\n$$\nwith columns $v_1 = \\begin{bmatrix} 1 \\\\ t \\end{bmatrix}$ and $v_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, where $t \\in \\mathbb{R}$ is a tunable sensitivity eigenstructure parameter. To ensure numerical robustness and avoid excessive noise amplification in the digital twin, impose the conditioning constraint\n$$\n|\\det(V)| \\ge \\delta, \\quad \\delta = 0.5.\n$$\nStarting from the fundamental definitions of the Luenberger observer error dynamics and the steady-state response to a constant fault input, derive the steady-state scalar sensitivity coefficient $s(t)$ mapping the constant actuator bias $f$ to the scalar residual $r_s(t)$, defined by the steady-state relationship $r_s^{\\star} = s(t)\\, f$. Then, subject to the above pole placement and conditioning constraint, determine the maximum attainable magnitude of $s(t)$ over all admissible $t$. Express your final answer as an exact number, with no units and no rounding.",
            "solution": "The problem is well-posed and scientifically grounded in the principles of Luenberger observer design and eigenstructure assignment for fault detection. The problem is valid. The solution requires deriving the fault-to-residual sensitivity and maximizing its magnitude under the given constraints.\n\n### 1. Error and Residual Dynamics\n\nLet the observer state be $\\hat{x}(t)$ and the estimation error be $e(t) = x(t) - \\hat{x}(t)$. The error dynamics are given by:\n$$\n\\dot{e}(t) = (A - LC)e(t) + B_f f\n$$\nThe residual is $r(t) = y(t) - \\hat{y}(t) = C(x(t) - \\hat{x}(t)) = C e(t)$.\n\n### 2. Steady-State Sensitivity\n\nFor a constant fault $f$, the error dynamics reach a steady state $e^{\\star}$ where $\\dot{e}(t) = 0$.\n$$\n0 = (A - LC)e^{\\star} + B_f f \\implies e^{\\star} = -(A - LC)^{-1} B_f f\n$$\nThe steady-state residual is $r^{\\star} = C e^{\\star} = -C(A - LC)^{-1} B_f f$. The scalar residual is $r_s^{\\star} = w^{\\top} r^{\\star} = -w^{\\top} C(A - LC)^{-1} B_f f$.\nThe sensitivity coefficient $s(t)$ is therefore:\n$$\ns(t) = -w^{\\top} C(A - LC)^{-1} B_f\n$$\n\n### 3. Using Eigenstructure Assignment\n\nWe are given the eigen-decomposition of the error matrix $A-LC$: $(A - LC) V = V \\Lambda$. This implies $(A - LC)^{-1} = V \\Lambda^{-1} V^{-1}$.\nThe given matrices are $\\Lambda = \\operatorname{diag}(-1, -3)$ and $V = \\begin{bmatrix} 1  1 \\\\ t  1 \\end{bmatrix}$.\nFirst, we find the inverse of $V$:\n$$\n\\det(V) = 1 \\cdot 1 - 1 \\cdot t = 1 - t\n$$\n$$\nV^{-1} = \\frac{1}{1 - t} \\begin{bmatrix} 1  -1 \\\\ -t  1 \\end{bmatrix}\n$$\nThe inverse of the eigenvalue matrix is $\\Lambda^{-1} = \\begin{bmatrix} -1  0 \\\\ 0  -1/3 \\end{bmatrix}$.\nNow we assemble $(A - LC)^{-1}$:\n$$\n(A - LC)^{-1} = V \\Lambda^{-1} V^{-1} = \\begin{bmatrix} 1  1 \\\\ t  1 \\end{bmatrix} \\begin{bmatrix} -1  0 \\\\ 0  -1/3 \\end{bmatrix} \\frac{1}{1-t} \\begin{bmatrix} 1  -1 \\\\ -t  1 \\end{bmatrix}\n$$\n$$\n(A - LC)^{-1} = \\frac{1}{1-t} \\begin{bmatrix} -1  -1/3 \\\\ -t  -1/3 \\end{bmatrix} \\begin{bmatrix} 1  -1 \\\\ -t  1 \\end{bmatrix} = \\frac{1}{1-t} \\begin{bmatrix} -1 + t/3  1 - 1/3 \\\\ -t + t/3  t - 1/3 \\end{bmatrix} = \\frac{1}{3(1-t)} \\begin{bmatrix} t-3  2 \\\\ -2t  3t-1 \\end{bmatrix}\n$$\nNow we plug this into the expression for $s(t)$, using $C = I_2$, $B_f = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, and $w = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$:\n$$\ns(t) = - \\begin{bmatrix} 1  0 \\end{bmatrix} \\left( \\frac{1}{3(1-t)} \\begin{bmatrix} t-3  2 \\\\ -2t  3t-1 \\end{bmatrix} \\right) \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n$$\ns(t) = - \\frac{1}{3(1-t)} \\left( \\begin{bmatrix} t-3  2 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right) = - \\frac{1}{3(1-t)} (2) = \\frac{2}{3(t-1)}\n$$\n\n### 4. Maximization under Constraints\n\nWe want to maximize the magnitude $|s(t)| = |\\frac{2}{3(t-1)}| = \\frac{2}{3|t-1|}$.\nThis is subject to the conditioning constraint $|\\det(V)| \\ge \\delta = 0.5$.\nSince $\\det(V) = 1-t$, the constraint is $|1-t| \\ge 0.5$, which is equivalent to $|t-1| \\ge 0.5$.\nThis gives two disjoint intervals for $t$:\n1.  $t-1 \\ge 0.5 \\implies t \\ge 1.5$\n2.  $t-1 \\le -0.5 \\implies t \\le 0.5$\nThe admissible set for $t$ is $t \\in (-\\infty, 0.5] \\cup [1.5, \\infty)$.\n\nTo maximize $|s(t)|$, we must minimize its denominator, $3|t-1|$. This is equivalent to minimizing $|t-1|$ over the admissible set. The value $|t-1|$ is the distance from $t$ to $1$. The closest points in the admissible set to $t=1$ are $t=0.5$ and $t=1.5$. At both of these points, the minimum value of $|t-1|$ is $0.5$.\n\nTherefore, the maximum attainable magnitude of the sensitivity is:\n$$\n\\max |s(t)| = \\frac{2}{3 \\times \\min|t-1|} = \\frac{2}{3 \\times 0.5} = \\frac{2}{1.5} = \\frac{4}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{4}{3}}\n$$"
        }
    ]
}