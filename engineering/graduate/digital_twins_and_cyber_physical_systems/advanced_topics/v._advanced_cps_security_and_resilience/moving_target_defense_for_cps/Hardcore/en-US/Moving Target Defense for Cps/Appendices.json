{
    "hands_on_practices": [
        {
            "introduction": "A primary concern for any defender employing Moving Target Defense is maintaining their own situational awareness, often through a Digital Twin. This practice addresses the fundamental property of observability, which guarantees that the system's internal state can be reconstructed from its outputs despite MTD-induced changes . You will derive the conditions for state reconstructability in a time-varying system and apply them to a concrete MTD schedule, grounding abstract control theory in a practical security context.",
            "id": "4232341",
            "problem": "A Cyber-Physical System (CPS) employs Moving Target Defense (MTD) by periodically switching its dynamics and sensing to confuse attackers while maintaining state awareness in its Digital Twin. Consider the discrete-time, linear, time-varying plant\n$$\nx_{k+1} = A_k x_k,\\quad y_k = C_k x_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$, $y_k \\in \\mathbb{R}^p$, and the pair sequence $\\{(A_k,C_k)\\}_{k \\in \\mathbb{Z}_{\\ge 0}}$ is known to the defender and identically replicated in the Digital Twin.\n\nStarting only from the state-transition composition rule and the definition that state reconstructability over a finite horizon requires injectivity of the map from an an initial state to the stacked outputs, do the following:\n\n1) Define uniform observability for the time-varying pair sequence $\\{(A_k,C_k)\\}$. Your definition must be given in terms of the finite-horizon observability Gramian and a uniform positive-definiteness property that holds for all starting indices.\n\n2) Derive, from first principles, a necessary and sufficient rank condition on the time-varying observability matrix over a finite horizon that ensures state reconstructability by the Digital Twin under MTD, uniformly over time. Your derivation must explicitly construct the state-transition map and the stacked-output map, and show why the rank condition is equivalent to injectivity.\n\n3) The defender’s MTD schedule alternates models with period $2$ as follows. For even $k$, the plant uses\n$$\nA_k = A_1 = \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix},\\quad C_k = C_1 = \\begin{pmatrix} 1  0  0 \\end{pmatrix}.\n$$\nFor odd $k$, the plant uses\n$$\nA_k = A_2 = \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix},\\quad C_k = C_2 = \\begin{pmatrix} 0  1  0 \\end{pmatrix}.\n$$\nAssuming noise-free measurements and known switching, compute the smallest integer horizon length $L^{\\star}$ such that, for every starting index $k$, the time-varying observability matrix over $L^{\\star}$ steps has full column rank and thus guarantees state reconstructability by the Digital Twin under this MTD schedule. Give your final answer as the value of $L^{\\star}$. No rounding is required, and no physical units are involved.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective. The problem statement describes a discrete-time Linear Time-Varying (LTV) system in the context of Moving Target Defense (MTD) for Cyber-Physical Systems (CPS). The tasks involve defining uniform observability, deriving the condition for state reconstructability, and computing a specific minimum observation horizon for a given switching schedule. All aspects of the problem are found to be rooted in established principles of linear systems and control theory. The problem is self-contained, with all necessary data and definitions provided, and is free from contradiction, ambiguity, or factual unsoundness. It is a well-posed theoretical problem that admits a unique and meaningful solution. The problem is therefore deemed valid.\n\nThe solution is presented in three parts, corresponding to the three tasks requested in the problem statement.\n\n### Part 1: Definition of Uniform Observability\n\nConsider the discrete-time, linear, time-varying (LTV) system described by the equations:\n$$\nx_{k+1} = A_k x_k\n$$\n$$\ny_k = C_k x_k\n$$\nwhere $k \\in \\mathbb{Z}_{\\ge 0}$ is the discrete time index, $x_k \\in \\mathbb{R}^n$ is the state vector, and $y_k \\in \\mathbb{R}^p$ is the output vector. The matrices $A_k$ and $C_k$ are time-varying.\n\nThe state at time $j$ can be related to the state at an earlier time $k$ ($j>k$) using the state transition matrix $\\Phi(j,k)$, which is defined as:\n$$\n\\Phi(j, k) = A_{j-1} A_{j-2} \\cdots A_k, \\quad \\text{for } j > k\n$$\nand $\\Phi(k, k) = I$, where $I$ is the $n \\times n$ identity matrix.\nThus, the state at time $j$ evolved from an initial state $x_{k_0}$ at time $k_0$ is given by $x_j = \\Phi(j, k_0) x_{k_0}$.\n\nThe output at time $j$ is then expressed in terms of the initial state $x_{k_0}$ as:\n$$\ny_j = C_j x_j = C_j \\Phi(j, k_0) x_{k_0}\n$$\nTo reconstruct the initial state $x_{k_0}$, we collect the outputs over a finite time horizon of length $L$, from time $k_0$ to $k_0+L-1$. Let $Y(k_0, L)$ be the stacked vector of these outputs:\n$$\nY(k_0, L) = \\begin{pmatrix} y_{k_0} \\\\ y_{k_0+1} \\\\ \\vdots \\\\ y_{k_0+L-1} \\end{pmatrix} = \\begin{pmatrix} C_{k_0} \\Phi(k_0, k_0) \\\\ C_{k_0+1} \\Phi(k_0+1, k_0) \\\\ \\vdots \\\\ C_{k_0+L-1} \\Phi(k_0+L-1, k_0) \\end{pmatrix} x_{k_0}\n$$\nThis equation is of the form $Y(k_0, L) = \\mathcal{O}(k_0, L) x_{k_0}$, where $\\mathcal{O}(k_0, L)$ is the time-varying observability matrix over the interval $[k_0, k_0+L-1]$.\n\nThe finite-horizon observability Gramian for a starting time $k_0$ and horizon length $L$ is an $n \\times n$ matrix defined as:\n$$\nW_o(k_0, L) = \\mathcal{O}(k_0, L)^T \\mathcal{O}(k_0, L) = \\sum_{j=k_0}^{k_0+L-1} \\Phi(j, k_0)^T C_j^T C_j \\Phi(j, k_0)\n$$\nThe system is uniformly observable if the ability to distinguish any two initial states from the output sequence is uniformly bounded from below, regardless of the starting time. This is formally defined as follows:\n\nThe LTV system $\\{(A_k, C_k)\\}$ is **uniformly observable** if there exist a positive integer $L$ and positive constants $\\alpha$ and $\\beta$ such that for all starting times $k_0 \\ge 0$, the observability Gramian satisfies:\n$$\n\\alpha I \\le W_o(k_0, L) \\le \\beta I\n$$\nThe crucial part of this definition is the existence of a uniform lower bound $\\alpha > 0$, which ensures that the Gramian $W_o(k_0, L)$ is positive definite for any starting time $k_0$, with its smallest eigenvalue bounded away from zero. This guarantees that $x_{k_0}$ can be robustly reconstructed from the measurements.\n\n### Part 2: Derivation of the Rank Condition for State Reconstructability\n\nState reconstructability by the Digital Twin requires that the initial state $x_{k_0}$ can be uniquely determined from the sequence of outputs $y_{k_0}, y_{k_0+1}, \\dots, y_{k_0+L-1}$ for some finite horizon $L$. This analysis begins from the state-transition composition rule, which leads to the stacked output equation derived in Part 1:\n$$\nY(k_0, L) = \\mathcal{O}(k_0, L) x_{k_0}\n$$\nThe problem of unique state reconstruction is equivalent to the injectivity of the linear map from the initial state $x_{k_0} \\in \\mathbb{R}^n$ to the stacked output vector $Y(k_0, L) \\in \\mathbb{R}^{pL}$. This map is represented by the time-varying observability matrix $\\mathcal{O}(k_0, L)$.\n\nA linear map is injective if and only if its null space (or kernel) is trivial, containing only the zero vector. That is, the only solution to $\\mathcal{O}(k_0, L) x_{k_0} = 0$ is $x_{k_0} = 0$. This condition means that no non-zero initial state can produce a zero output sequence over the horizon, and thus any two distinct initial states must produce distinct output sequences.\n\nFor the matrix $\\mathcal{O}(k_0, L)$, which has dimensions $(pL) \\times n$, the condition of having a trivial null space is equivalent to its columns being linearly independent. A matrix has linearly independent columns if and only if its rank is equal to the number of columns.\n\nTherefore, a necessary and sufficient condition for state reconstructability over the finite horizon $[k_0, k_0+L-1]$ is:\n$$\n\\text{rank}(\\mathcal{O}(k_0, L)) = n\n$$\nFor this reconstructability to be guaranteed uniformly over time under the MTD scheme, there must exist a finite integer horizon length $L$ such that this rank condition holds for **every** possible starting index $k \\ge 0$.\n\nThis rank condition is equivalent to the observability Gramian $W_o(k, L) = \\mathcal{O}(k, L)^T \\mathcal{O}(k, L)$ being invertible (and thus positive definite, since it is always positive semidefinite) for all $k$. The rank of $W_o(k,L)$ is the same as the rank of $\\mathcal{O}(k,L)$. Since $W_o(k,L)$ is an $n \\times n$ matrix, it is invertible if and only if its rank is $n$.\n\n### Part 3: Computation of the Smallest Horizon Length $L^{\\star}$\n\nThe specific MTD schedule involves a periodic switching with period $2$. The state dimension is $n=3$, and the output dimension is $p=1$.\nFor even $k$: $A_k = A_1 = \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix}$, $C_k = C_1 = \\begin{pmatrix} 1  0  0 \\end{pmatrix}$.\nFor odd $k$: $A_k = A_2 = \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix}$, $C_k = C_2 = \\begin{pmatrix} 0  1  0 \\end{pmatrix}$.\n\nWe need to find the smallest integer $L^{\\star}$ such that $\\text{rank}(\\mathcal{O}(k, L^{\\star}))=3$ for all $k \\ge 0$.\nThe observability matrix $\\mathcal{O}(k, L)$ is an $L \\times 3$ matrix (since $p=1$). For its rank to be $3$, it must have at least $3$ rows, which means the horizon length $L$ must be at least $3$. Therefore, $L^{\\star} \\ge 3$.\n\nDue to the $2$-periodicity of the system matrices, the condition $\\text{rank}(\\mathcal{O}(k, L)) = 3$ for all $k$ can be verified by checking only two representative cases: a start at an even time ($k=0$) and a start at an odd time ($k=1$).\n\nLet's test the minimal possible horizon, $L=3$.\n\n**Case 1: Start at an even time ($k=0$)**\nThe sequence of system pairs is $(A_1, C_1), (A_2, C_2), (A_1, C_1), \\dots$.\nThe observability matrix for a horizon $L=3$ is:\n$$\n\\mathcal{O}(0, 3) = \\begin{pmatrix} C_0 \\\\ C_1 A_0 \\\\ C_2 A_1 A_0 \\end{pmatrix}\n$$\nLet's compute the rows, substituting the scheduled matrix types for each time index:\n- Row 1 ($C_0$, type 1): $\\begin{pmatrix} 1  0  0 \\end{pmatrix}$.\n- Row 2 ($C_1 A_0$, types 2 and 1): $\\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  1  1 \\end{pmatrix}$.\n- Product $A_1 A_0$ (types 2 and 1): $A_2 A_1 = \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1  0 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix}$.\n- Row 3 ($C_2 A_1 A_0$, types 1, 2, and 1): $C_1 (A_2 A_1) = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1  0 \\end{pmatrix}$.\nSo, the observability matrix is:\n$$\n\\mathcal{O}(0, 3) = \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 1  1  0 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathcal{O}(0, 3)) = 1(0-1) - 0 + 0 = -1 \\ne 0$.\nSince the determinant is non-zero, $\\text{rank}(\\mathcal{O}(0, 3)) = 3$.\n\n**Case 2: Start at an odd time ($k=1$)**\nThe sequence of system pairs is $(A_2, C_2), (A_1, C_1), (A_2, C_2), \\dots$.\nThe observability matrix for a horizon $L=3$ starting at $k_0=1$ is:\n$$\n\\mathcal{O}(1, 3) = \\begin{pmatrix} C_1 \\\\ C_2 A_1 \\\\ C_3 A_2 A_1 \\end{pmatrix}\n$$\nLet's compute the rows, substituting the scheduled matrix types for each time index:\n- Row 1 ($C_1$, type 2): $\\begin{pmatrix} 0  1  0 \\end{pmatrix}$.\n- Row 2 ($C_2 A_1$, types 1 and 2): $\\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\end{pmatrix}$.\n- Product $A_2 A_1$ (types 1 and 2): $A_1 A_2 = \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix}$.\n- Row 3 ($C_3 A_2 A_1$, types 2, 1, and 2): $C_2 (A_1 A_2) = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  1  1 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  1  2 \\end{pmatrix}$.\nSo, the observability matrix is:\n$$\n\\mathcal{O}(1, 3) = \\begin{pmatrix} 0  1  0 \\\\ 1  0  0 \\\\ 0  1  2 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathcal{O}(1, 3)) = 0 - 1(1 \\cdot 2 - 0 \\cdot 0) + 0 = -2 \\ne 0$.\nSince the determinant is non-zero, $\\text{rank}(\\mathcal{O}(1, 3)) = 3$.\n\nBoth representative cases yield a full-rank observability matrix for $L=3$. Since any even starting time $k=2m$ behaves like $k=0$ and any odd starting time $k=2m+1$ behaves like $k=1$, we can conclude that $\\text{rank}(\\mathcal{O}(k, 3))=3$ for all $k \\ge 0$.\n\nWe established that $L^{\\star}$ must be at least $3$. We have now shown that $L=3$ is sufficient. Therefore, the smallest integer horizon length that guarantees uniform state reconstructability is $L^{\\star}=3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "Once we can ensure state awareness, the next step is to design an effective MTD schedule that actively thwarts an adversary. This exercise moves from analysis to synthesis, framing MTD as a constrained optimization problem where you must balance competing goals . You will implement an algorithm that searches for an optimal schedule to minimize the information available to an attacker, quantified by the Fisher Information, while satisfying a crucial constraint on operational performance.",
            "id": "4232324",
            "problem": "Consider a discrete-time linear time-invariant cyber-physical system with moving target defense (MTD) scheduling. The state evolves according to the recursion $x_{k+1} = \\left(A_{\\sigma_k} + \\theta F\\right) x_k + B u_k$ and the output is $y_k = C x_k + v_k$, where $x_k \\in \\mathbb{R}^n$ is the state, $y_k \\in \\mathbb{R}^m$ is the measured output, $A_{\\sigma_k} \\in \\mathbb{R}^{n \\times n}$ is the system matrix selected at time $k$ from a finite set of MTD modes indexed by $\\sigma_k \\in \\{0,1,\\dots,M-1\\}$, $F \\in \\mathbb{R}^{n \\times n}$ is a fixed matrix encoding a scalar unknown parameter $\\theta \\in \\mathbb{R}$ that the attacker seeks to estimate, $B \\in \\mathbb{R}^{n \\times p}$ is the input matrix, $u_k \\in \\mathbb{R}^p$ is the control input at time $k$, $C \\in \\mathbb{R}^{m \\times n}$ is the output matrix, and $v_k \\in \\mathbb{R}^m$ is measurement noise modeled as independent and identically distributed Gaussian with zero mean and covariance $R_v \\in \\mathbb{R}^{m \\times m}$, which is positive definite. The initial condition $x_0 \\in \\mathbb{R}^n$ is known. The control input sequence $\\{u_k\\}_{k=0}^{T-1}$ is known and fixed by the defender. The system dimension $n$, output dimension $m$, and input dimension $p$ are specified in each test case.\n\nAn adversary observes $T$ outputs $\\{y_k\\}_{k=0}^{T-1}$ generated under a chosen schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ and attempts to estimate the scalar parameter $\\theta$. Under the assumption that the measurement noise covariance $R_v$ does not depend on $\\theta$, the Fisher information for $\\theta$ extractable from the stacked observation vector $Y = [y_0^\\top, y_1^\\top, \\dots, y_{T-1}^\\top]^\\top$ equals the quadratic form of the sensitivity of the mean of $Y$ with respect to $\\theta$ weighted by $R_v^{-1}$ across time. The defender designs the MTD schedule to minimize this Fisher information, subject to a constraint on control performance evaluated at the nominal parameter $\\theta = 0$.\n\nDefine the performance cost at $\\theta = 0$ as $J = \\sum_{k=0}^{T-1} \\left( x_k^\\top Q_c x_k + u_k^\\top R_c u_k \\right)$, where $Q_c \\in \\mathbb{R}^{n \\times n}$ is symmetric positive semidefinite and $R_c \\in \\mathbb{R}^{p \\times p}$ is symmetric positive semidefinite. The defender must ensure $J \\leq J_{\\max}$ for a specified bound $J_{\\max}  0$. The goal is to select a schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ that minimizes the Fisher information evaluated at $\\theta = 0$, subject to the performance constraint $J \\leq J_{\\max}$.\n\nStarting from the foundational definitions of discrete-time linear systems, Gaussian random variables, and Fisher information for Gaussian models with parameter-independent covariance, derive principled expressions for the Fisher information and the performance cost in terms of the schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$, and implement an algorithm that searches over all schedules to find the one that minimizes the Fisher information subject to the performance constraint at $\\theta = 0$.\n\nYour program must evaluate the following test suite of parameter sets. For all cases, $M = 2$ modes are available, indexed by $0$ and $1$. For each case, report the selected schedule and the corresponding Fisher information value evaluated at $\\theta = 0$.\n\nCase 1 (general operation with moderate bound):\n- $n = 2$, $m = 1$, $p = 1$, $T = 4$.\n- $A_0 = \\begin{bmatrix} 0.92  0.08 \\\\ 0  0.97 \\end{bmatrix}$, $A_1 = \\begin{bmatrix} 0.85  -0.05 \\\\ 0.10  0.90 \\end{bmatrix}$.\n- $F = \\begin{bmatrix} 0  0.2 \\\\ 0  0 \\end{bmatrix}$.\n- $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$, $C = \\begin{bmatrix} 1  0.5 \\end{bmatrix}$.\n- $R_v = \\begin{bmatrix} 0.02 \\end{bmatrix}$.\n- $x_0 = \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix}$.\n- $u_k = 0.2$ for all $k$.\n- $Q_c = \\operatorname{diag}(0.4, 0.4)$, $R_c = \\begin{bmatrix} 0.02 \\end{bmatrix}$.\n- $J_{\\max} = 1.5$.\n\nCase 2 (tighter performance bound, reduced input):\n- $n = 2$, $m = 1$, $p = 1$, $T = 3$.\n- $A_0 = \\begin{bmatrix} 0.92  0.08 \\\\ 0  0.97 \\end{bmatrix}$, $A_1 = \\begin{bmatrix} 0.85  -0.05 \\\\ 0.10  0.90 \\end{bmatrix}$.\n- $F = \\begin{bmatrix} 0  0.1 \\\\ 0  0 \\end{bmatrix}$.\n- $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$, $C = \\begin{bmatrix} 1  0.5 \\end{bmatrix}$.\n- $R_v = \\begin{bmatrix} 0.01 \\end{bmatrix}$.\n- $x_0 = \\begin{bmatrix} 0.2 \\\\ 0.2 \\end{bmatrix}$.\n- $u_k = 0$ for all $k$.\n- $Q_c = \\operatorname{diag}(0.3, 0.3)$, $R_c = \\begin{bmatrix} 0.01 \\end{bmatrix}$.\n- $J_{\\max} = 0.15$.\n\nCase 3 (single-step edge case):\n- $n = 2$, $m = 1$, $p = 1$, $T = 1$.\n- $A_0 = \\begin{bmatrix} 0.92  0.08 \\\\ 0  0.97 \\end{bmatrix}$, $A_1 = \\begin{bmatrix} 0.85  -0.05 \\\\ 0.10  0.90 \\end{bmatrix}$.\n- $F = \\begin{bmatrix} 0  0.15 \\\\ 0  0 \\end{bmatrix}$.\n- $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$, $C = \\begin{bmatrix} 1  0.5 \\end{bmatrix}$.\n- $R_v = \\begin{bmatrix} 0.02 \\end{bmatrix}$.\n- $x_0 = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n- $u_k = 0.1$ for all $k$.\n- $Q_c = \\operatorname{diag}(0.5, 0.5)$, $R_c = \\begin{bmatrix} 0.01 \\end{bmatrix}$.\n- $J_{\\max} = 2.5$.\n\nFor each case, assume the attacker computes the Fisher information at $\\theta = 0$. Use the following definitions for the defender’s computation:\n- Define the nominal state trajectory at $\\theta = 0$ by $x_{k+1} = A_{\\sigma_k} x_k + B u_k$ with $x_0$ given.\n- Define the sensitivity recursion $s_{k+1} = A_{\\sigma_k} s_k + F x_k$ with $s_0 = 0$, which gives the derivative $s_k = \\left.\\frac{\\partial x_k}{\\partial \\theta}\\right|_{\\theta=0}$.\n- The output mean sensitivity is $\\left.\\frac{\\partial \\mu_k}{\\partial \\theta}\\right|_{\\theta=0} = C s_k$.\n- The Fisher information evaluated at $\\theta = 0$ is $I(\\theta)\\big|_{\\theta=0} = \\sum_{k=0}^{T-1} \\left( C s_k \\right)^\\top R_v^{-1} \\left( C s_k \\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order listed above. Each element must be a list of two items: the chosen schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ represented as a list of integers, and the numerical Fisher information value as a floating-point number. For example, the output format must be exactly like \"[[schedule1,FI1],[schedule2,FI2],[schedule3,FI3]]\" with no spaces.",
            "solution": "The problem requires finding an optimal Moving Target Defense (MTD) schedule for a discrete-time linear time-invariant (LTI) cyber-physical system (CPS). The objective is to minimize the adversary's ability to estimate a hidden system parameter, $\\theta$, quantified by the Fisher information, while adhering to a constraint on the nominal system's control performance.\n\nFirst, we formalize the problem by deriving expressions for the performance cost and the Fisher information as functions of the MTD schedule. An MTD schedule is a sequence $\\{\\sigma_k\\}_{k=0}^{T-1}$, where each $\\sigma_k \\in \\{0, 1, \\dots, M-1\\}$ selects the system matrix $A_{\\sigma_k}$ to be used at time step $k$.\n\nThe system dynamics are given by:\n$$x_{k+1} = \\left(A_{\\sigma_k} + \\theta F\\right) x_k + B u_k$$\n$$y_k = C x_k + v_k$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state, $u_k \\in \\mathbb{R}^p$ is the control input, $y_k \\in \\mathbb{R}^m$ is the measurement, and $v_k \\sim \\mathcal{N}(0, R_v)$ is i.i.d. Gaussian noise.\n\n**1. Performance Cost ($J$)**\n\nThe performance cost is defined at the nominal parameter value $\\theta = 0$. The nominal state trajectory, which we denote as $\\bar{x}_k$ to be precise, evolves according to:\n$$\\bar{x}_{k+1} = A_{\\sigma_k} \\bar{x}_k + B u_k$$\nwith a given initial condition $\\bar{x}_0 = x_0$. The cost function is:\n$$J = \\sum_{k=0}^{T-1} \\left( \\bar{x}_k^\\top Q_c \\bar{x}_k + u_k^\\top R_c u_k \\right)$$\nFor any given schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ and known input sequence $\\{u_k\\}_{k=0}^{T-1}$, the nominal state trajectory $\\{\\bar{x}_k\\}_{k=0}^{T-1}$ can be computed by iterating the state equation starting from $\\bar{x}_0$. Subsequently, the cost $J$ can be calculated by summing the quadratic terms at each time step. The defender's task includes ensuring that $J \\leq J_{\\max}$.\n\n**2. Fisher Information ($I$)**\n\nThe Fisher information quantifies the amount of information that an observation carries about an unknown parameter. For a set of observations $Y$ from a Gaussian distribution with mean $\\mu(\\theta)$ and a covariance matrix independent of $\\theta$, the Fisher information for a scalar parameter $\\theta$ is given by:\n$$I(\\theta) = \\left( \\frac{\\partial \\mu(\\theta)}{\\partial \\theta} \\right)^\\top \\Sigma_Y^{-1} \\left( \\frac{\\partial \\mu(\\theta)}{\\partial \\theta} \\right)$$\nIn our problem, the observations are $y_k$ for $k \\in \\{0, \\dots, T-1\\}$. The stacked observation vector is $Y = [y_0^\\top, \\dots, y_{T-1}^\\top]^\\top$. The mean of $Y$ is $\\mu_Y(\\theta) = [E[y_0]^\\top, \\dots, E[y_{T-1}]^\\top]^\\top = [(C x_0(\\theta))^\\top, \\dots, (C x_{T-1}(\\theta))^\\top]^\\top$. Since the noise terms $v_k$ are i.i.d., the covariance matrix of $Y$ is block-diagonal, $\\Sigma_Y = \\text{diag}(R_v, \\dots, R_v)$. Its inverse is $\\Sigma_Y^{-1} = \\text{diag}(R_v^{-1}, \\dots, R_v^{-1})$.\n\nThe Fisher information can then be written as a sum over time:\n$$I(\\theta) = \\sum_{k=0}^{T-1} \\left( \\frac{\\partial E[y_k]}{\\partial \\theta} \\right)^\\top R_v^{-1} \\left( \\frac{\\partial E[y_k]}{\\partial \\theta} \\right)$$\nThe expected value of the output is $E[y_k] = C x_k(\\theta)$, where we write $x_k(\\theta)$ to emphasize its dependence on $\\theta$. The derivative is $\\frac{\\partial E[y_k]}{\\partial \\theta} = C \\frac{\\partial x_k(\\theta)}{\\partial \\theta}$. Let us define the sensitivity state as $s_k(\\theta) = \\frac{\\partial x_k(\\theta)}{\\partial \\theta}$.\n\nTo find the dynamics of the sensitivity, we differentiate the state equation with respect to $\\theta$:\n$$\\frac{\\partial x_{k+1}(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ (A_{\\sigma_k} + \\theta F) x_k(\\theta) + B u_k \\right]$$\n$$s_{k+1}(\\theta) = (A_{\\sigma_k} + \\theta F) s_k(\\theta) + F x_k(\\theta)$$\nThe problem asks to evaluate the Fisher information at $\\theta=0$. We evaluate the sensitivity dynamics at $\\theta=0$:\n$$s_{k+1}(0) = A_{\\sigma_k} s_k(0) + F x_k(0)$$\nLet $s_k \\triangleq s_k(0)$ and $\\bar{x}_k \\triangleq x_k(0)$. The recursion becomes:\n$$s_{k+1} = A_{\\sigma_k} s_k + F \\bar{x}_k$$\nThe initial state $x_0$ is a known constant, so it does not depend on $\\theta$. Thus, the initial sensitivity is $s_0 = \\frac{\\partial x_0}{\\partial \\theta} = 0$.\n\nSubstituting into the Fisher information formula and evaluating at $\\theta=0$:\n$$I \\triangleq I(\\theta)\\big|_{\\theta=0} = \\sum_{k=0}^{T-1} \\left( C s_k \\right)^\\top R_v^{-1} \\left( C s_k \\right)$$\nThis is the expression the defender seeks to minimize. Note that the calculation of $\\{s_k\\}$ requires the nominal state trajectory $\\{\\bar{x}_k\\}$, coupling the two calculations.\n\n**3. Algorithmic Approach**\n\nThe problem is to find a schedule $\\sigma^* = \\{\\sigma_k^*\\}_{k=0}^{T-1}$ that solves the constrained optimization problem:\n$$\\min_{\\{\\sigma_k\\}} \\sum_{k=0}^{T-1} (C s_k)^\\top R_v^{-1} (C s_k)$$\n$$\\text{subject to} \\quad \\sum_{k=0}^{T-1} \\left( \\bar{x}_k^\\top Q_c \\bar{x}_k + u_k^\\top R_c u_k \\right) \\leq J_{\\max}$$\n\nThe set of possible schedules is finite. Given $M$ modes and a time horizon of $T$, there are $M^T$ possible schedules. For the given test cases, $M=2$ and $T$ is small ($4$, $3$, and $1$), leading to $2^4=16$, $2^3=8$, and $2^1=2$ schedules, respectively. This makes an exhaustive search (brute force) a feasible and rigorous approach.\n\nThe algorithm is as follows:\n1.  Generate all $M^T$ possible schedules.\n2.  Initialize a variable for the minimum Fisher information found so far, $I_{min}$, to infinity, and a variable for the corresponding optimal schedule, $\\sigma_{best}$, to a null value.\n3.  For each schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$:\n    a. Initialize $\\bar{x} \\leftarrow x_0$, $s \\leftarrow 0$, $J \\leftarrow 0$, $I \\leftarrow 0$.\n    b. For $k$ from $0$ to $T-1$:\n        i.   Select the matrix $A_{\\sigma_k}$ based on the current schedule element.\n        ii.  Add the cost contribution of the current state $\\bar{x}_k$: $J \\leftarrow J + \\bar{x}^\\top Q_c \\bar{x} + u_k^\\top R_c u_k$.\n        iii. Add the Fisher information contribution of the current sensitivity $s_k$: $I \\leftarrow I + (C s)^\\top R_v^{-1} (C s)$.\n        iv.  Compute the next state and sensitivity: $\\bar{x}_{next} \\leftarrow A_{\\sigma_k} \\bar{x} + B u_k$ and $s_{next} \\leftarrow A_{\\sigma_k} s + F \\bar{x}$.\n        v.   Update for the next iteration: $\\bar{x} \\leftarrow \\bar{x}_{next}$, $s \\leftarrow s_{next}$.\n    c. After iterating through all time steps, check if the performance constraint is met: $J \\leq J_{\\max}$.\n    d. If the constraint is met and the calculated Fisher information $I$ is less than the current minimum $I_{min}$, update $I_{min} \\leftarrow I$ and $\\sigma_{best} \\leftarrow \\{\\sigma_k\\}$.\n4.  After checking all schedules, the pair $(\\sigma_{best}, I_{min})$ is the solution. This process is repeated for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves the MTD scheduling problem for a series of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            \"n\": 2, \"m\": 1, \"p\": 1, \"T\": 4,\n            \"A0\": np.array([[0.92, 0.08], [0, 0.97]]),\n            \"A1\": np.array([[0.85, -0.05], [0.10, 0.90]]),\n            \"F\": np.array([[0, 0.2], [0, 0]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1, 0.5]]),\n            \"R_v\": np.array([[0.02]]),\n            \"x0\": np.array([[0.5], [-0.2]]),\n            \"u_k_val\": 0.2,\n            \"Qc\": np.diag([0.4, 0.4]),\n            \"Rc\": np.array([[0.02]]),\n            \"J_max\": 1.5\n        },\n        # Case 2\n        {\n            \"n\": 2, \"m\": 1, \"p\": 1, \"T\": 3,\n            \"A0\": np.array([[0.92, 0.08], [0, 0.97]]),\n            \"A1\": np.array([[0.85, -0.05], [0.10, 0.90]]),\n            \"F\": np.array([[0, 0.1], [0, 0]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1, 0.5]]),\n            \"R_v\": np.array([[0.01]]),\n            \"x0\": np.array([[0.2], [0.2]]),\n            \"u_k_val\": 0.0,\n            \"Qc\": np.diag([0.3, 0.3]),\n            \"Rc\": np.array([[0.01]]),\n            \"J_max\": 0.15\n        },\n        # Case 3\n        {\n            \"n\": 2, \"m\": 1, \"p\": 1, \"T\": 1,\n            \"A0\": np.array([[0.92, 0.08], [0, 0.97]]),\n            \"A1\": np.array([[0.85, -0.05], [0.10, 0.90]]),\n            \"F\": np.array([[0, 0.15], [0, 0]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1, 0.5]]),\n            \"R_v\": np.array([[0.02]]),\n            \"x0\": np.array([[1.0], [-1.0]]),\n            \"u_k_val\": 0.1,\n            \"Qc\": np.diag([0.5, 0.5]),\n            \"Rc\": np.array([[0.01]]),\n            \"J_max\": 2.5\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        T = case[\"T\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n        x0 = case[\"x0\"]\n        \n        As = [case[\"A0\"], case[\"A1\"]]\n        F = case[\"F\"]\n        B = case[\"B\"]\n        C = case[\"C\"]\n        R_v_inv = np.linalg.inv(case[\"R_v\"])\n        Qc = case[\"Qc\"]\n        Rc = case[\"Rc\"]\n        J_max = case[\"J_max\"]\n        \n        u_list = [np.array([[case[\"u_k_val\"]]]) for _ in range(T)]\n        \n        # Generate all possible schedules\n        schedules = list(itertools.product([0, 1], repeat=T))\n        \n        best_schedule = None\n        min_fisher_info = float('inf')\n\n        for schedule in schedules:\n            x = x0.copy()\n            s = np.zeros((n, 1))\n            \n            total_cost = 0.0\n            total_fisher = 0.0\n            \n            for k in range(T):\n                mode = schedule[k]\n                A_k = As[mode]\n                u_k = u_list[k]\n                \n                # Add contributions from current state x_k and sensitivity s_k\n                total_cost += (x.T @ Qc @ x + u_k.T @ Rc @ u_k).item()\n                \n                fisher_term = C @ s\n                total_fisher += (fisher_term.T @ R_v_inv @ fisher_term).item()\n\n                # Update state and sensitivity for the next step (k+1)\n                x_next = A_k @ x + B @ u_k\n                s_next = A_k @ s + F @ x\n                \n                x = x_next\n                s = s_next\n\n            # Check constraint and update best schedule\n            if total_cost = J_max:\n                if total_fisher  min_fisher_info:\n                    min_fisher_info = total_fisher\n                    best_schedule = schedule\n\n        final_results.append([list(best_schedule), min_fisher_info])\n\n    # Format the final output string\n    formatted_parts = []\n    for sched, fi in final_results:\n        sched_str = str(sched).replace(\" \", \"\")\n        formatted_parts.append(f\"[{sched_str},{fi}]\")\n    \n    print(f\"[{','.join(formatted_parts)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Fixed defense schedules can eventually become predictable, but what if the defender could learn and adapt to an adversary's behavior in real time? This final practice introduces a powerful game-theoretic approach to MTD, moving beyond static optimization to the realm of online learning . You will derive a \"no-regret\" algorithm from first principles, providing a formal guarantee that the defender's adaptive strategy will perform nearly as well as the best possible fixed strategy in hindsight, ensuring robust security in a dynamic adversarial environment.",
            "id": "4232276",
            "problem": "Consider a Cyber-Physical System (CPS) employing Moving Target Defense (MTD), where at each discrete round $t \\in \\{1,2,\\dots,T\\}$, the defender selects one of $K$ configurations $i \\in \\{1,2,\\dots,K\\}$ to schedule. An adversary selects an attack action $a_t$ that is observed by the defender after each round. A validated Digital Twin (DT) of the CPS maps the observed attack $a_t$ and each configuration $i$ to a scalar loss $\\ell_t(i) \\in [0,L]$, where $L  0$ is a known bound on per-round loss. Assume that the DT produces a vector of losses $\\ell_t(1), \\dots, \\ell_t(K)$ at each round based on the observed $a_t$.\n\nThe defender’s objective is to design a learning algorithm for repeated play that is no-regret and whose time-averaged play converges to a Coarse Correlated Equilibrium (CCE) when the attacker also uses a no-regret learning process. Begin from fundamental definitions of regret in repeated decision problems and well-tested facts of online learning in full-information settings. You must not assume or invoke any specific pre-given algorithmic update rule; instead, derive an update scheme from first principles (e.g., potential-based arguments consistent with convexity and exponential inequalities) that ensures the defender’s average regret vanishes as $T \\to \\infty$.\n\nDefine the defender’s randomized strategy at round $t$ as a probability vector $p_t \\in \\Delta^K$, where $\\Delta^K$ is the $K$-dimensional simplex, and the realized expected loss at round $t$ is $p_t \\cdot \\ell_t := \\sum_{i=1}^{K} p_t(i)\\,\\ell_t(i)$. The cumulative regret after $T$ rounds is\n$$\nR_T := \\sum_{t=1}^{T} p_t \\cdot \\ell_t \\;-\\; \\min_{j \\in \\{1,\\dots,K\\}} \\sum_{t=1}^{T} \\ell_t(j),\n$$\nand the average regret is $r_T := \\frac{R_T}{T}$.\n\nTasks:\n- Derive a full-information no-regret update for the defender’s $p_t$ using only the assumptions above and standard inequalities valid for bounded losses.\n- Prove a non-asymptotic bound on $r_T$ in terms of $K$, $T$, $L$, and a tunable learning rate parameter, and then optimize that parameter to minimize the bound.\n- Using the definition of Coarse Correlated Equilibrium (CCE), argue why, under both players’ no-regret learning, the empirical joint distribution of play converges to a CCE of the stage game induced by the loss mappings $\\ell_t(\\cdot)$.\n\nAnswer specification:\n- Your final reported quantity must be the single closed-form analytic expression giving the tightest upper bound you derive for the defender’s average regret $r_T$ after optimizing the learning rate, expressed as a function of $K$, $T$, and $L$. No other quantities should be included in the final answer.\n- No rounding is required, and no units are to be reported.",
            "solution": "The problem asks for the derivation of a no-regret online learning algorithm for a defender in a Cyber-Physical System, an analysis of its performance, and a connection to the game-theoretic concept of a Coarse Correlated Equilibrium (CCE). The setting is a repeated game with full information, where at each round $t$, the defender observes a loss vector $\\ell_t$ over its $K$ available configurations.\n\nThe core of the solution is to derive the Exponential Weights algorithm and its regret bound from first principles, using a potential function argument as suggested.\n\nLet the defender's strategy at round $t \\in \\{1, 2, \\dots, T\\}$ be a probability distribution $p_t$ over the $K$ configurations. The probability of choosing configuration $i \\in \\{1, \\dots, K\\}$ is $p_t(i)$. The vector $p_t$ belongs to the $K$-dimensional simplex, $\\Delta^K$. The expected loss incurred by the defender at round $t$ is $p_t \\cdot \\ell_t = \\sum_{i=1}^{K} p_t(i) \\ell_t(i)$. The cumulative regret after $T$ rounds is defined as the difference between the cumulative loss of the algorithm and the cumulative loss of the best single configuration in hindsight:\n$$\nR_T := \\sum_{t=1}^{T} p_t \\cdot \\ell_t - \\min_{j \\in \\{1,\\dots,K\\}} \\sum_{t=1}^{T} \\ell_t(j)\n$$\nOur goal is to find an update rule for $p_t$ that guarantees that the average regret $r_T = R_T/T$ approaches $0$ as $T \\to \\infty$.\n\n**Derivation of the No-Regret Update Rule (Exponential Weights)**\n\nLet us associate a non-negative weight $w_t(i)$ with each configuration $i$ at the beginning of round $t$. We initialize the weights uniformly, say $w_1(i) = 1$ for all $i \\in \\{1, \\dots, K\\}$. At each round $t$, the defender's strategy $p_t$ is formed by normalizing these weights:\n$$\np_t(i) = \\frac{w_t(i)}{\\sum_{j=1}^K w_t(j)}\n$$\nAfter observing the loss vector $\\ell_t = (\\ell_t(1), \\dots, \\ell_t(K))$, the defender updates the weights for the next round, $t+1$. To achieve no-regret, configurations with lower loss should be favored. A multiplicative update rule is a natural choice. We update the weight of each configuration $i$ based on its incurred loss $\\ell_t(i)$ using an exponential factor:\n$$\nw_{t+1}(i) = w_t(i) \\exp(-\\eta \\ell_t(i))\n$$\nwhere $\\eta  0$ is a learning rate parameter that controls the trade-off between exploiting past performance and exploring new options. A larger $\\eta$ makes the update more sensitive to recent losses. The initial weights $w_1(i)=1$ and this update rule imply that at any round $t$, the weight $w_t(i)$ is given by:\n$$\nw_t(i) = \\exp\\left(-\\eta \\sum_{\\tau=1}^{t-1} \\ell_{\\tau}(i)\\right)\n$$\n\n**Derivation of the Regret Bound**\n\nTo analyze the regret, we introduce a potential function $\\Phi_t$ defined as the sum of the weights at the start of round $t$:\n$$\n\\Phi_t = \\sum_{i=1}^K w_t(i)\n$$\nWe will bound the growth of this potential function from above and below.\nOur initial potential is $\\Phi_1 = \\sum_{i=1}^K w_1(i) = \\sum_{i=1}^K 1 = K$.\n\nLet's analyze the change from $\\Phi_t$ to $\\Phi_{t+1}$:\n$$\n\\Phi_{t+1} = \\sum_{i=1}^K w_{t+1}(i) = \\sum_{i=1}^K w_t(i) \\exp(-\\eta \\ell_t(i))\n$$\nWe can factor out $\\Phi_t = \\sum_j w_t(j)$ to get:\n$$\n\\Phi_{t+1} = \\Phi_t \\sum_{i=1}^K \\frac{w_t(i)}{\\Phi_t} \\exp(-\\eta \\ell_t(i)) = \\Phi_t \\sum_{i=1}^K p_t(i) \\exp(-\\eta \\ell_t(i))\n$$\nThe losses are bounded: $\\ell_t(i) \\in [0, L]$. Thus, the argument of the exponential, $-\\eta \\ell_t(i)$, is in $[-\\eta L, 0]$. We use the inequality $e^x \\le 1+x+x^2/2$, which holds for $x \\le 0$. Let $x_i = -\\eta \\ell_t(i)$.\n$$\n\\exp(-\\eta \\ell_t(i)) \\le 1 - \\eta \\ell_t(i) + \\frac{(-\\eta \\ell_t(i))^2}{2} = 1 - \\eta \\ell_t(i) + \\frac{\\eta^2 \\ell_t(i)^2}{2}\n$$\nSubstituting this into the expression for $\\Phi_{t+1}$:\n$$\n\\Phi_{t+1} \\le \\Phi_t \\sum_{i=1}^K p_t(i) \\left(1 - \\eta \\ell_t(i) + \\frac{\\eta^2 \\ell_t(i)^2}{2}\\right)\n$$\n$$\n\\Phi_{t+1} \\le \\Phi_t \\left( \\sum_{i=1}^K p_t(i) - \\eta \\sum_{i=1}^K p_t(i)\\ell_t(i) + \\frac{\\eta^2}{2} \\sum_{i=1}^K p_t(i)\\ell_t(i)^2 \\right)\n$$\nUsing $\\sum_i p_t(i) = 1$ and $p_t \\cdot \\ell_t = \\sum_i p_t(i)\\ell_t(i)$, and since $\\ell_t(i) \\in [0, L]$, we have $\\ell_t(i)^2 \\le L^2$.\n$$\n\\Phi_{t+1} \\le \\Phi_t \\left( 1 - \\eta (p_t \\cdot \\ell_t) + \\frac{\\eta^2 L^2}{2} \\sum_{i=1}^K p_t(i) \\right) = \\Phi_t \\left( 1 - \\eta (p_t \\cdot \\ell_t) + \\frac{\\eta^2 L^2}{2} \\right)\n$$\nUsing the inequality $\\ln(1+x) \\le x$ for $x > -1$:\n$$\n\\ln(\\Phi_{t+1}) \\le \\ln(\\Phi_t) + \\ln\\left(1 - \\eta (p_t \\cdot \\ell_t) + \\frac{\\eta^2 L^2}{2}\\right) \\le \\ln(\\Phi_t) - \\eta (p_t \\cdot \\ell_t) + \\frac{\\eta^2 L^2}{2}\n$$\nThis inequality holds if $\\eta$ is small enough such that the argument of the logarithm is positive. We will see that the optimal $\\eta$ becomes small for large $T$. Summing this inequality from $t=1$ to $T$:\n$$\n\\sum_{t=1}^T (\\ln(\\Phi_{t+1}) - \\ln(\\Phi_t)) \\le \\sum_{t=1}^T \\left( - \\eta (p_t \\cdot \\ell_t) + \\frac{\\eta^2 L^2}{2} \\right)\n$$\nThe left side is a telescoping sum:\n$$\n\\ln(\\Phi_{T+1}) - \\ln(\\Phi_1) \\le - \\eta \\sum_{t=1}^T (p_t \\cdot \\ell_t) + \\frac{T \\eta^2 L^2}{2}\n$$\nNow, we establish a lower bound a on $\\Phi_{T+1}$. Let $j^* = \\arg\\min_{j \\in \\{1,\\dots,K\\}} \\sum_{t=1}^T \\ell_t(j)$ be the best configuration in hindsight.\n$$\n\\Phi_{T+1} = \\sum_{i=1}^K w_{T+1}(i) \\ge w_{T+1}(j^*) = \\exp\\left(-\\eta \\sum_{t=1}^T \\ell_t(j^*)\\right)\n$$\nTaking the natural logarithm:\n$$\n\\ln(\\Phi_{T+1}) \\ge -\\eta \\sum_{t=1}^T \\ell_t(j^*) = -\\eta \\min_{j \\in \\{1,\\dots,K\\}} \\sum_{t=1}^{T} \\ell_t(j)\n$$\nCombining the upper and lower bounds on $\\ln(\\Phi_{T+1})$:\n$$\n-\\eta \\min_{j} \\sum_{t=1}^{T} \\ell_t(j) \\le \\ln(\\Phi_{T+1}) \\le \\ln(\\Phi_1) - \\eta \\sum_{t=1}^T (p_t \\cdot \\ell_t) + \\frac{T \\eta^2 L^2}{2}\n$$\nSubstituting $\\Phi_1 = K$ and rearranging to isolate the regret term:\n$$\n\\eta \\sum_{t=1}^T (p_t \\cdot \\ell_t) - \\eta \\min_{j} \\sum_{t=1}^{T} \\ell_t(j) \\le \\ln(K) + \\frac{T \\eta^2 L^2}{2}\n$$\n$$\n\\eta R_T \\le \\ln(K) + \\frac{T \\eta^2 L^2}{2}\n$$\nThis gives the cumulative regret bound:\n$$\nR_T \\le \\frac{\\ln(K)}{\\eta} + \\frac{T \\eta L^2}{2}\n$$\n\n**Optimization of the Learning Rate**\n\nTo find the tightest possible bound, we minimize the right-hand side with respect to $\\eta$. Let $f(\\eta) = \\frac{\\ln(K)}{\\eta} + \\frac{T \\eta L^2}{2}$. We find the minimum by setting the derivative to zero:\n$$\n\\frac{df}{d\\eta} = -\\frac{\\ln(K)}{\\eta^2} + \\frac{T L^2}{2} = 0\n$$\n$$\n\\eta^2 = \\frac{2 \\ln(K)}{T L^2} \\implies \\eta^* = \\sqrt{\\frac{2 \\ln(K)}{T L^2}} = \\frac{\\sqrt{2 \\ln(K)}}{L\\sqrt{T}}\n$$\nSubstituting this optimal value $\\eta^*$ back into the regret bound:\n$$\nR_T \\le \\frac{\\ln(K)}{\\frac{\\sqrt{2 \\ln(K)}}{L\\sqrt{T}}} + \\frac{T L^2}{2} \\frac{\\sqrt{2 \\ln(K)}}{L\\sqrt{T}}\n$$\n$$\nR_T \\le \\frac{L\\sqrt{T}\\ln(K)}{\\sqrt{2 \\ln(K)}} + \\frac{L\\sqrt{T}\\sqrt{2 \\ln(K)}}{2} = \\frac{L\\sqrt{T}\\sqrt{\\ln(K)}}{\\sqrt{2}} + \\frac{L\\sqrt{T}\\sqrt{2\\ln(K)}}{2}\n$$\n$$\nR_T \\le L\\sqrt{T \\ln(K)} \\left( \\frac{1}{\\sqrt{2}} + \\frac{\\sqrt{2}}{2} \\right) = L\\sqrt{T \\ln(K)} \\left( \\frac{\\sqrt{2}}{2} + \\frac{\\sqrt{2}}{2} \\right) = L\\sqrt{2T \\ln(K)}\n$$\nThe bound on the average regret, $r_T = R_T/T$, is therefore:\n$$\nr_T \\le \\frac{L\\sqrt{2T \\ln(K)}}{T} = L\\sqrt{\\frac{2\\ln(K)}{T}}\n$$\nThis bound shows that $r_T \\to 0$ as $T \\to \\infty$, which confirms that the Exponential Weights algorithm is a no-regret learning algorithm.\n\n**Convergence to Coarse Correlated Equilibrium (CCE)**\n\nA Coarse Correlated Equilibrium (CCE) is a distribution over joint actions such that no player can improve their expected payoff by unilaterally deviating to a fixed action. Let the empirical distribution of play over $T$ rounds be $\\hat{\\sigma}_T$. In our case, this corresponds to the sequence of defender's and attacker's plays.\n\nFor the defender, the CCE condition states that the average loss they incurred is no greater than the average loss they would have incurred by playing any fixed configuration $j$ against the same sequence of attacker actions. Mathematically:\n$$\n\\frac{1}{T}\\sum_{t=1}^T p_t \\cdot \\ell_t \\le \\frac{1}{T}\\sum_{t=1}^T \\ell_t(j) \\quad \\text{for all } j \\in \\{1,\\dots,K\\}\n$$\nThis is equivalent to:\n$$\n\\frac{1}{T}\\left(\\sum_{t=1}^T p_t \\cdot \\ell_t - \\min_{j \\in \\{1,\\dots,K\\}} \\sum_{t=1}^T \\ell_t(j)\\right) \\le 0\n$$\nThe left-hand side is precisely the definition of the average regret, $r_T$. The property of a no-regret algorithm is that $\\limsup_{T\\to\\infty} r_T \\le 0$. Our derived bound $r_T \\le L\\sqrt{2\\ln(K)/T}$ shows that $r_T \\to 0$ as $T \\to \\infty$. This means that for any $\\epsilon  0$, the CCE condition for the defender is satisfied up to $\\epsilon$ (i.e., $r_T \\le \\epsilon$) for all sufficiently large $T$. In the limit, the CCE condition holds.\n\nThe problem states that the attacker also uses a no-regret learning process. By a symmetric argument, the attacker's average regret also converges to zero. Let the attacker's action be $a \\in \\mathcal{A}$ and their loss be $\\ell^A(i, a)$. Their no-regret property implies that in the limit, they have no incentive to unilaterally deviate to any fixed attack $a' \\in \\mathcal{A}$.\n\nWhen all players in a game employ no-regret learning algorithms, the empirical joint distribution of their actions converges to the set of Coarse Correlated Equilibria of the stage game. Therefore, the combined no-regret properties of the defender and the attacker ensure that their time-averaged play converges to a CCE.",
            "answer": "$$\n\\boxed{L\\sqrt{\\frac{2\\ln(K)}{T}}}\n$$"
        }
    ]
}