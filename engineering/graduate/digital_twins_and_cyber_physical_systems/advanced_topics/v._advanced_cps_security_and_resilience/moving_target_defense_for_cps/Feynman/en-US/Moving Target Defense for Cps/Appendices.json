{
    "hands_on_practices": [
        {
            "introduction": "A primary concern when implementing a Moving Target Defense (MTD) strategy is ensuring that the core operational capabilities of the Cyber-Physical System (CPS) are not compromised. This exercise addresses the fundamental property of controllability, which is the ability to steer the system to any desired state. By working through the calculation of the finite-horizon controllability Gramian for a time-varying system, you will verify that the MTD schedule preserves the defender's ability to control the plant, a critical prerequisite for any defense mechanism .",
            "id": "4232307",
            "problem": "Consider a $2$-dimensional discrete-time Cyber-Physical System (CPS) with Moving Target Defense (MTD), modeled as a time-varying linear system with switching pairs $\\{(A_{k}, B_{k})\\}$ selected by a Digital Twin (DT) to frustrate an adversary’s system identification. The system evolves according to $x_{k+1} = A_{k} x_{k} + B_{k} u_{k}$, where $x_{k} \\in \\mathbb{R}^{2}$ and $u_{k} \\in \\mathbb{R}$.\n\nThe DT enacts a defense schedule over a finite horizon of $N=3$ steps with the following mode sequence:\n$$\nA_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad B_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\qquad\nA_{1} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}, \\quad B_{1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n$$\n$$\nA_{2} = \\begin{pmatrix} 1 & 0 \\\\ \\delta & 1 \\end{pmatrix}, \\quad B_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\nwhere $\\delta \\in \\mathbb{R}$ is a tunable MTD parameter representing a small model perturbation inserted to alter the effective dynamics while maintaining physical feasibility.\n\nUsing the fundamental definition of the finite-horizon controllability Gramian for time-varying discrete-time linear systems (with the appropriate state-transition operator), derive the explicit finite-horizon controllability Gramian $W_{3}(\\delta)$ for the given schedule and compute $\\det(W_{3}(\\delta))$ as a closed-form analytic expression in $\\delta$. Then, state the condition in terms of the finite-horizon controllability Gramian that guarantees controllability despite parameter changes induced by MTD. Report only $\\det(W_{3}(\\delta))$ as your final answer. No rounding is required, and no units are involved.",
            "solution": "The user wants to find the determinant of the finite-horizon controllability Gramian for a discrete-time linear time-varying system.\n\n### Step 1: Extract Givens\n- System equation: $x_{k+1} = A_{k} x_{k} + B_{k} u_{k}$, with $x_{k} \\in \\mathbb{R}^{2}$ and $u_{k} \\in \\mathbb{R}$.\n- Finite horizon: $N=3$ steps.\n- Mode sequence for $k=0, 1, 2$:\n  - $A_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $B_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n  - $A_{1} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$, $B_{1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n  - $A_{2} = \\begin{pmatrix} 1 & 0 \\\\ \\delta & 1 \\end{pmatrix}$, $B_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- Parameter: $\\delta \\in \\mathbb{R}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is based on standard principles of linear systems and control theory, specifically the concept of the controllability Gramian for discrete-time linear time-varying (LTV) systems. The application to Moving Target Defense in Cyber-Physical Systems is a valid and contemporary research area. The problem is self-contained, with all necessary matrices, parameters, and the time horizon specified. It requests a specific, derivable mathematical expression, and therefore a unique solution exists. The language is formal and unambiguous. The problem is not trivial, as it requires a multi-step calculation involving state-transition matrices and matrix algebra.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with providing a complete solution.\n\nThe finite-horizon controllability Gramian for a discrete-time LTV system over a horizon of $N$ steps is defined as:\n$$\nW_{N} = \\sum_{k=0}^{N-1} \\Phi(N, k+1) B_{k} B_{k}^{T} \\Phi(N, k+1)^{T}\n$$\nwhere the state-transition matrix $\\Phi(i, j)$ is given by $\\Phi(i, j) = A_{i-1} A_{i-2} \\cdots A_{j}$ for $i > j$, and $\\Phi(i, i) = I$, the identity matrix.\n\nGiven the horizon $N=3$, the Gramian $W_{3}(\\delta)$ is:\n$$\nW_{3}(\\delta) = \\sum_{k=0}^{2} \\Phi(3, k+1) B_{k} B_{k}^{T} \\Phi(3, k+1)^{T}\n$$\n$$\nW_{3}(\\delta) = \\Phi(3, 1) B_{0} B_{0}^{T} \\Phi(3, 1)^{T} + \\Phi(3, 2) B_{1} B_{1}^{T} \\Phi(3, 2)^{T} + \\Phi(3, 3) B_{2} B_{2}^{T} \\Phi(3, 3)^{T}\n$$\n\nFirst, we compute the necessary state-transition matrices:\n\\begin{itemize}\n    \\item $\\Phi(3, 3) = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n    \\item $\\Phi(3, 2) = A_{2} = \\begin{pmatrix} 1 & 0 \\\\ \\delta & 1 \\end{pmatrix}$\n    \\item $\\Phi(3, 1) = A_{2} A_{1} = \\begin{pmatrix} 1 & 0 \\\\ \\delta & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(0)(0) & (1)(1)+(0)(1) \\\\ (\\delta)(1)+(1)(0) & (\\delta)(1)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ \\delta & \\delta+1 \\end{pmatrix}$\n\\end{itemize}\n\nNext, we compute the individual terms of the summation for $W_{3}(\\delta)$.\n\nFor $k=0$:\nThe term is $\\Phi(3, 1) B_{0} B_{0}^{T} \\Phi(3, 1)^{T}$.\nWe have $B_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, so $B_{0} B_{0}^{T} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\nThe product is:\n$$\n\\begin{pmatrix} 1 & 1 \\\\ \\delta & \\delta+1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ \\delta & \\delta+1 \\end{pmatrix}^{T} = \\begin{pmatrix} 1 & 0 \\\\ \\delta & 0 \\end{pmatrix} \\begin{pmatrix} 1 & \\delta \\\\ 1 & \\delta+1 \\end{pmatrix} = \\begin{pmatrix} 1 & \\delta \\\\ \\delta & \\delta^{2} \\end{pmatrix}\n$$\n\nFor $k=1$:\nThe term is $\\Phi(3, 2) B_{1} B_{1}^{T} \\Phi(3, 2)^{T}$.\nWe have $B_{1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, so $B_{1} B_{1}^{T} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe product is:\n$$\n\\begin{pmatrix} 1 & 0 \\\\ \\delta & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\delta & 1 \\end{pmatrix}^{T} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & \\delta \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n\nFor $k=2$:\nThe term is $\\Phi(3, 3) B_{2} B_{2}^{T} \\Phi(3, 3)^{T}$.\nWe have $B_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, so $B_{2} B_{2}^{T} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\nThe product is:\n$$\nI \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} I^{T} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n\nNow, we sum these three matrices to obtain $W_{3}(\\delta)$:\n$$\nW_{3}(\\delta) = \\begin{pmatrix} 1 & \\delta \\\\ \\delta & \\delta^{2} \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\nW_{3}(\\delta) = \\begin{pmatrix} 1+0+1 & \\delta+0+1 \\\\ \\delta+0+1 & \\delta^{2}+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 & \\delta+1 \\\\ \\delta+1 & \\delta^{2}+2 \\end{pmatrix}\n$$\n\nThe final step is to compute the determinant of $W_{3}(\\delta)$:\n$$\n\\det(W_{3}(\\delta)) = \\det\\begin{pmatrix} 2 & \\delta+1 \\\\ \\delta+1 & \\delta^{2}+2 \\end{pmatrix}\n$$\n$$\n\\det(W_{3}(\\delta)) = (2)(\\delta^{2}+2) - (\\delta+1)(\\delta+1)\n$$\n$$\n\\det(W_{3}(\\delta)) = 2\\delta^{2} + 4 - (\\delta^{2} + 2\\delta + 1)\n$$\n$$\n\\det(W_{3}(\\delta)) = 2\\delta^{2} + 4 - \\delta^{2} - 2\\delta - 1\n$$\n$$\n\\det(W_{3}(\\delta)) = \\delta^{2} - 2\\delta + 3\n$$\n\nThe condition that guarantees controllability for a discrete-time LTV system over the horizon $[0, N)$ is that the finite-horizon controllability Gramian $W_{N}$ is non-singular, i.e., invertible. This is equivalent to the condition $\\det(W_{N}) \\neq 0$. For the given system, the condition is $\\det(W_{3}(\\delta)) \\neq 0$. The expression $\\delta^{2} - 2\\delta + 3$ is a quadratic in $\\delta$ with a discriminant of $(-2)^2 - 4(1)(3) = 4 - 12 = -8 < 0$. Since the leading coefficient is positive and the discriminant is negative, the quadratic has no real roots and is strictly positive for all real $\\delta$. Therefore, $\\det(W_{3}(\\delta)) > 0$ for all $\\delta \\in \\mathbb{R}$, which implies the system is controllable for any choice of the MTD parameter $\\delta$. The problem asks for the expression for $\\det(W_{3}(\\delta))$.",
            "answer": "$$\\boxed{\\delta^{2} - 2\\delta + 3}$$"
        },
        {
            "introduction": "While MTD is designed to obscure system dynamics from an adversary, it is crucial that the defender's own Digital Twin can maintain full state awareness. This practice explores the concept of observability, the dual of controllability, which guarantees that the internal state of the system can be uniquely determined from its external outputs. You will derive and apply the rank condition for the time-varying observability matrix to find the minimum observation horizon needed for state reconstruction under a specific MTD schedule, reinforcing the importance of maintaining visibility while creating uncertainty for the attacker .",
            "id": "4232341",
            "problem": "A Cyber-Physical System (CPS) employs Moving Target Defense (MTD) by periodically switching its dynamics and sensing to confuse attackers while maintaining state awareness in its Digital Twin. Consider the discrete-time, linear, time-varying plant\n$$\nx_{k+1} = A_k x_k,\\quad y_k = C_k x_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$, $y_k \\in \\mathbb{R}^p$, and the pair sequence $\\{(A_k,C_k)\\}_{k \\in \\mathbb{Z}_{\\ge 0}}$ is known to the defender and identically replicated in the Digital Twin.\n\nStarting only from the state-transition composition rule and the definition that state reconstructability over a finite horizon requires injectivity of the map from an initial state to the stacked outputs, do the following:\n\n1) Define uniform observability for the time-varying pair sequence $\\{(A_k,C_k)\\}$. Your definition must be given in terms of the finite-horizon observability Gramian and a uniform positive-definiteness property that holds for all starting indices.\n\n2) Derive, from first principles, a necessary and sufficient rank condition on the time-varying observability matrix over a finite horizon that ensures state reconstructability by the Digital Twin under MTD, uniformly over time. Your derivation must explicitly construct the state-transition map and the stacked-output map, and show why the rank condition is equivalent to injectivity.\n\n3) The defender’s MTD schedule alternates models with period $2$ as follows. For even $k$, the plant uses\n$$\nA_k = A_1 = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix},\\quad C_k = C_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}.\n$$\nFor odd $k$, the plant uses\n$$\nA_k = A_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix},\\quad C_k = C_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}.\n$$\nAssuming noise-free measurements and known switching, compute the smallest integer horizon length $L^{\\star}$ such that, for every starting index $k$, the time-varying observability matrix over $L^{\\star}$ steps has full column rank and thus guarantees state reconstructability by the Digital Twin under this MTD schedule. Give your final answer as the value of $L^{\\star}$. No rounding is required, and no physical units are involved.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective. The problem statement describes a discrete-time Linear Time-Varying (LTV) system in the context of Moving Target Defense (MTD) for Cyber-Physical Systems (CPS). The tasks involve defining uniform observability, deriving the condition for state reconstructability, and computing a specific minimum observation horizon for a given switching schedule. All aspects of the problem are found to be rooted in established principles of linear systems and control theory. The problem is self-contained, with all necessary data and definitions provided, and is free from contradiction, ambiguity, or factual unsoundness. It is a well-posed theoretical problem that admits a unique and meaningful solution. The problem is therefore deemed valid.\n\nThe solution is presented in three parts, corresponding to the three tasks requested in the problem statement.\n\n### Part 1: Definition of Uniform Observability\n\nConsider the discrete-time, linear, time-varying (LTV) system described by the equations:\n$$\nx_{k+1} = A_k x_k\n$$\n$$\ny_k = C_k x_k\n$$\nwhere $k \\in \\mathbb{Z}_{\\ge 0}$ is the discrete time index, $x_k \\in \\mathbb{R}^n$ is the state vector, and $y_k \\in \\mathbb{R}^p$ is the output vector. The matrices $A_k$ and $C_k$ are time-varying.\n\nThe state at time $j$ can be related to the state at an earlier time $k$ ($j>k$) using the state transition matrix $\\Phi(j,k)$, which is defined as:\n$$\n\\Phi(j, k) = A_{j-1} A_{j-2} \\cdots A_k, \\quad \\text{for } j > k\n$$\nand $\\Phi(k, k) = I$, where $I$ is the $n \\times n$ identity matrix.\nThus, the state at time $j$ evolved from an initial state $x_{k_0}$ at time $k_0$ is given by $x_j = \\Phi(j, k_0) x_{k_0}$.\n\nThe output at time $j$ is then expressed in terms of the initial state $x_{k_0}$ as:\n$$\ny_j = C_j x_j = C_j \\Phi(j, k_0) x_{k_0}\n$$\nTo reconstruct the initial state $x_{k_0}$, we collect the outputs over a finite time horizon of length $L$, from time $k_0$ to $k_0+L-1$. Let $Y(k_0, L)$ be the stacked vector of these outputs:\n$$\nY(k_0, L) = \\begin{pmatrix} y_{k_0} \\\\ y_{k_0+1} \\\\ \\vdots \\\\ y_{k_0+L-1} \\end{pmatrix} = \\begin{pmatrix} C_{k_0} \\Phi(k_0, k_0) \\\\ C_{k_0+1} \\Phi(k_0+1, k_0) \\\\ \\vdots \\\\ C_{k_0+L-1} \\Phi(k_0+L-1, k_0) \\end{pmatrix} x_{k_0}\n$$\nThis equation is of the form $Y(k_0, L) = \\mathcal{O}(k_0, L) x_{k_0}$, where $\\mathcal{O}(k_0, L)$ is the time-varying observability matrix over the interval $[k_0, k_0+L-1]$.\n\nThe finite-horizon observability Gramian for a starting time $k_0$ and horizon length $L$ is an $n \\times n$ matrix defined as:\n$$\nW_o(k_0, L) = \\mathcal{O}(k_0, L)^T \\mathcal{O}(k_0, L) = \\sum_{j=k_0}^{k_0+L-1} \\Phi(j, k_0)^T C_j^T C_j \\Phi(j, k_0)\n$$\nThe system is uniformly observable if the ability to distinguish any two initial states from the output sequence is uniformly bounded from below, regardless of the starting time. This is formally defined as follows:\n\nThe LTV system $\\{(A_k, C_k)\\}$ is **uniformly observable** if there exist a positive integer $L$ and positive constants $\\alpha$ and $\\beta$ such that for all starting times $k_0 \\ge 0$, the observability Gramian satisfies:\n$$\n\\alpha I \\le W_o(k_0, L) \\le \\beta I\n$$\nThe crucial part of this definition is the existence of a uniform lower bound $\\alpha > 0$, which ensures that the Gramian $W_o(k_0, L)$ is positive definite for any starting time $k_0$, with its smallest eigenvalue bounded away from zero. This guarantees that $x_{k_0}$ can be robustly reconstructed from the measurements.\n\n### Part 2: Derivation of the Rank Condition for State Reconstructability\n\nState reconstructability by the Digital Twin requires that the initial state $x_{k_0}$ can be uniquely determined from the sequence of outputs $y_{k_0}, y_{k_0+1}, \\dots, y_{k_0+L-1}$ for some finite horizon $L$. This analysis begins from the state-transition composition rule, which leads to the stacked output equation derived in Part 1:\n$$\nY(k_0, L) = \\mathcal{O}(k_0, L) x_{k_0}\n$$\nThe problem of unique state reconstruction is equivalent to the injectivity of the linear map from the initial state $x_{k_0} \\in \\mathbb{R}^n$ to the stacked output vector $Y(k_0, L) \\in \\mathbb{R}^{pL}$. This map is represented by the time-varying observability matrix $\\mathcal{O}(k_0, L)$.\n\nA linear map is injective if and only if its null space (or kernel) is trivial, containing only the zero vector. That is, the only solution to $\\mathcal{O}(k_0, L) x_{k_0} = 0$ is $x_{k_0} = 0$. This condition means that no non-zero initial state can produce a zero output sequence over the horizon, and thus any two distinct initial states must produce distinct output sequences.\n\nFor the matrix $\\mathcal{O}(k_0, L)$, which has dimensions $(pL) \\times n$, the condition of having a trivial null space is equivalent to its columns being linearly independent. A matrix has linearly independent columns if and only if its rank is equal to the number of columns.\n\nTherefore, a necessary and sufficient condition for state reconstructability over the finite horizon $[k_0, k_0+L-1]$ is:\n$$\n\\text{rank}(\\mathcal{O}(k_0, L)) = n\n$$\nFor this reconstructability to be guaranteed uniformly over time under the MTD scheme, there must exist a finite integer horizon length $L$ such that this rank condition holds for **every** possible starting index $k \\ge 0$.\n\nThis rank condition is equivalent to the observability Gramian $W_o(k, L) = \\mathcal{O}(k, L)^T \\mathcal{O}(k, L)$ being invertible (and thus positive definite, since it is always positive semidefinite) for all $k$. The rank of $W_o(k,L)$ is the same as the rank of $\\mathcal{O}(k,L)$. Since $W_o(k,L)$ is an $n \\times n$ matrix, it is invertible if and only if its rank is $n$.\n\n### Part 3: Computation of the Smallest Horizon Length $L^{\\star}$\n\nThe specific MTD schedule involves a periodic switching with period $2$. The state dimension is $n=3$, and the output dimension is $p=1$.\nFor even $k$: $A_k = A_1 = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix}$, $C_k = C_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$.\nFor odd $k$: $A_k = A_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix}$, $C_k = C_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}$.\n\nWe need to find the smallest integer $L^{\\star}$ such that $\\text{rank}(\\mathcal{O}(k, L^{\\star}))=3$ for all $k \\ge 0$.\nThe observability matrix $\\mathcal{O}(k, L)$ is an $L \\times 3$ matrix (since $p=1$). For its rank to be $3$, it must have at least $3$ rows, which means the horizon length $L$ must be at least $3$. Therefore, $L^{\\star} \\ge 3$.\n\nDue to the $2$-periodicity of the system matrices, the condition $\\text{rank}(\\mathcal{O}(k, L)) = 3$ for all $k$ can be verified by checking only two representative cases: a start at an even time ($k=0$) and a start at an odd time ($k=1$).\n\nLet's test the minimal possible horizon, $L=3$.\n\n**Case 1: Start at an even time ($k=0$)**\nThe observability matrix for a horizon $L=3$ is:\n$$\n\\mathcal{O}(0, 3) = \\begin{pmatrix} C_0 \\Phi(0,0) \\\\ C_1 \\Phi(1,0) \\\\ C_2 \\Phi(2,0) \\end{pmatrix} = \\begin{pmatrix} C_0 \\\\ C_1 A_0 \\\\ C_2 A_1 A_0 \\end{pmatrix}\n$$\nSubstituting the matrices according to the 2-periodic schedule gives:\n$$\n\\mathcal{O}(0, 3) = \\begin{pmatrix} C_1 \\\\ C_2 A_1 \\\\ C_1 A_2 A_1 \\end{pmatrix}\n$$\nLet's compute the rows:\n- Row 1: $C_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$.\n- Row 2: $C_2 A_1 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 1 \\end{pmatrix}$.\n- Product $A_2 A_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\n- Row 3: $C_1 (A_2 A_1) = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}$.\nSo, the observability matrix is:\n$$\n\\mathcal{O}(0, 3) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathcal{O}(0, 3)) = 1(0-1) - 0(0-1) + 0(0-1) = -1 \\ne 0$.\nSince the determinant is non-zero, $\\text{rank}(\\mathcal{O}(0, 3)) = 3$.\n\n**Case 2: Start at an odd time ($k=1$)**\nThe observability matrix for a horizon $L=3$ starting at $k_0=1$ is:\n$$\n\\mathcal{O}(1, 3) = \\begin{pmatrix} C_1 \\Phi(1,1) \\\\ C_2 \\Phi(2,1) \\\\ C_3 \\Phi(3,1) \\end{pmatrix} = \\begin{pmatrix} C_1 \\\\ C_2 A_1 \\\\ C_3 A_2 A_1 \\end{pmatrix}\n$$\nSubstituting the matrices according to the 2-periodic schedule gives:\n$$\n\\mathcal{O}(1, 3) = \\begin{pmatrix} C_2 \\\\ C_1 A_2 \\\\ C_2 A_1 A_2 \\end{pmatrix}\n$$\nLet's compute the rows:\n- Row 1: $C_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}$.\n- Row 2: $C_1 A_2 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$.\n- Product $A_1 A_2 = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\n- Row 3: $C_2 (A_1 A_2) = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 2 \\end{pmatrix}$.\nSo, the observability matrix is:\n$$\n\\mathcal{O}(1, 3) = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathcal{O}(1, 3)) = 0 - 1(1 \\cdot 2 - 0 \\cdot 0) + 0 = -2 \\ne 0$.\nSince the determinant is non-zero, $\\text{rank}(\\mathcal{O}(1, 3)) = 3$.\n\nBoth representative cases yield a full-rank observability matrix for $L=3$. Since any even starting time $k=2m$ behaves like $k=0$ and any odd starting time $k=2m+1$ behaves like $k=1$, we can conclude that $\\text{rank}(\\mathcal{O}(k, 3))=3$ for all $k \\ge 0$.\n\nWe established that $L^{\\star}$ must be at least $3$. We have now shown that $L=3$ is sufficient. Therefore, the smallest integer horizon length that guarantees uniform state reconstructability is $L^{\\star}=3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "Effective MTD design moves beyond random switching and into the realm of strategic optimization, balancing the dual objectives of security and performance. This advanced practice frames the selection of an MTD schedule as a constrained optimization problem where the goal is to minimize the information an attacker can learn, quantified by the Fisher information. By implementing an algorithm to find an optimal schedule that respects operational performance constraints, you will directly engage with the core trade-off at the heart of practical MTD deployment .",
            "id": "4232324",
            "problem": "Consider a discrete-time linear time-invariant cyber-physical system with moving target defense (MTD) scheduling. The state evolves according to the recursion $x_{k+1} = \\left(A_{\\sigma_k} + \\theta F\\right) x_k + B u_k$ and the output is $y_k = C x_k + v_k$, where $x_k \\in \\mathbb{R}^n$ is the state, $y_k \\in \\mathbb{R}^m$ is the measured output, $A_{\\sigma_k} \\in \\mathbb{R}^{n \\times n}$ is the system matrix selected at time $k$ from a finite set of MTD modes indexed by $\\sigma_k \\in \\{0,1,\\dots,M-1\\}$, $F \\in \\mathbb{R}^{n \\times n}$ is a fixed matrix encoding a scalar unknown parameter $\\theta \\in \\mathbb{R}$ that the attacker seeks to estimate, $B \\in \\mathbb{R}^{n \\times p}$ is the input matrix, $u_k \\in \\mathbb{R}^p$ is the control input at time $k$, $C \\in \\mathbb{R}^{m \\times n}$ is the output matrix, and $v_k \\in \\mathbb{R}^m$ is measurement noise modeled as independent and identically distributed Gaussian with zero mean and covariance $R_v \\in \\mathbb{R}^{m \\times m}$, which is positive definite. The initial condition $x_0 \\in \\mathbb{R}^n$ is known. The control input sequence $\\{u_k\\}_{k=0}^{T-1}$ is known and fixed by the defender. The system dimension $n$, output dimension $m$, and input dimension $p$ are specified in each test case.\n\nAn adversary observes $T$ outputs $\\{y_k\\}_{k=0}^{T-1}$ generated under a chosen schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ and attempts to estimate the scalar parameter $\\theta$. Under the assumption that the measurement noise covariance $R_v$ does not depend on $\\theta$, the Fisher information for $\\theta$ extractable from the stacked observation vector $Y = [y_0^\\top, y_1^\\top, \\dots, y_{T-1}^\\top]^\\top$ equals the quadratic form of the sensitivity of the mean of $Y$ with respect to $\\theta$ weighted by $R_v^{-1}$ across time. The defender designs the MTD schedule to minimize this Fisher information, subject to a constraint on control performance evaluated at the nominal parameter $\\theta = 0$.\n\nDefine the performance cost at $\\theta = 0$ as $J = \\sum_{k=0}^{T-1} \\left( x_k^\\top Q_c x_k + u_k^\\top R_c u_k \\right)$, where $Q_c \\in \\mathbb{R}^{n \\times n}$ is symmetric positive semidefinite and $R_c \\in \\mathbb{R}^{p \\times p}$ is symmetric positive semidefinite. The defender must ensure $J \\leq J_{\\max}$ for a specified bound $J_{\\max} > 0$. The goal is to select a schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ that minimizes the Fisher information evaluated at $\\theta = 0$, subject to the performance constraint $J \\leq J_{\\max}$.\n\nStarting from the foundational definitions of discrete-time linear systems, Gaussian random variables, and Fisher information for Gaussian models with parameter-independent covariance, derive principled expressions for the Fisher information and the performance cost in terms of the schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$, and implement an algorithm that searches over all schedules to find the one that minimizes the Fisher information subject to the performance constraint at $\\theta = 0$.\n\nYour program must evaluate the following test suite of parameter sets. For all cases, $M = 2$ modes are available, indexed by $0$ and $1$. For each case, report the selected schedule and the corresponding Fisher information value evaluated at $\\theta = 0$.\n\nCase 1 (general operation with moderate bound):\n- $n = 2$, $m = 1$, $p = 1$, $T = 4$.\n- $A_0 = \\begin{bmatrix} 0.92 & 0.08 \\\\ 0 & 0.97 \\end{bmatrix}$, $A_1 = \\begin{bmatrix} 0.85 & -0.05 \\\\ 0.10 & 0.90 \\end{bmatrix}$.\n- $F = \\begin{bmatrix} 0 & 0.2 \\\\ 0 & 0 \\end{bmatrix}$.\n- $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix}$.\n- $R_v = \\begin{bmatrix} 0.02 \\end{bmatrix}$.\n- $x_0 = \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix}$.\n- $u_k = 0.2$ for all $k$.\n- $Q_c = \\operatorname{diag}(0.4, 0.4)$, $R_c = \\begin{bmatrix} 0.02 \\end{bmatrix}$.\n- $J_{\\max} = 1.5$.\n\nCase 2 (tighter performance bound, reduced input):\n- $n = 2$, $m = 1$, $p = 1$, $T = 3$.\n- $A_0 = \\begin{bmatrix} 0.92 & 0.08 \\\\ 0 & 0.97 \\end{bmatrix}$, $A_1 = \\begin{bmatrix} 0.85 & -0.05 \\\\ 0.10 & 0.90 \\end{bmatrix}$.\n- $F = \\begin{bmatrix} 0 & 0.1 \\\\ 0 & 0 \\end{bmatrix}$.\n- $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix}$.\n- $R_v = \\begin{bmatrix} 0.01 \\end{bmatrix}$.\n- $x_0 = \\begin{bmatrix} 0.2 \\\\ 0.2 \\end{bmatrix}$.\n- $u_k = 0$ for all $k$.\n- $Q_c = \\operatorname{diag}(0.3, 0.3)$, $R_c = \\begin{bmatrix} 0.01 \\end{bmatrix}$.\n- $J_{\\max} = 0.15$.\n\nCase 3 (single-step edge case):\n- $n = 2$, $m = 1$, $p = 1$, $T = 1$.\n- $A_0 = \\begin{bmatrix} 0.92 & 0.08 \\\\ 0 & 0.97 \\end{bmatrix}$, $A_1 = \\begin{bmatrix} 0.85 & -0.05 \\\\ 0.10 & 0.90 \\end{bmatrix}$.\n- $F = \\begin{bmatrix} 0 & 0.15 \\\\ 0 & 0 \\end{bmatrix}$.\n- $B = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix}$, $C = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix}$.\n- $R_v = \\begin{bmatrix} 0.02 \\end{bmatrix}$.\n- $x_0 = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n- $u_k = 0.1$ for all $k$.\n- $Q_c = \\operatorname{diag}(0.5, 0.5)$, $R_c = \\begin{bmatrix} 0.01 \\end{bmatrix}$.\n- $J_{\\max} = 2.5$.\n\nFor each case, assume the attacker computes the Fisher information at $\\theta = 0$. Use the following definitions for the defender’s computation:\n- Define the nominal state trajectory at $\\theta = 0$ by $x_{k+1} = A_{\\sigma_k} x_k + B u_k$ with $x_0$ given.\n- Define the sensitivity recursion $s_{k+1} = A_{\\sigma_k} s_k + F x_k$ with $s_0 = 0$, which gives the derivative $s_k = \\left.\\frac{\\partial x_k}{\\partial \\theta}\\right|_{\\theta=0}$.\n- The output mean sensitivity is $\\left.\\frac{\\partial \\mu_k}{\\partial \\theta}\\right|_{\\theta=0} = C s_k$.\n- The Fisher information evaluated at $\\theta = 0$ is $I(\\theta)\\big|_{\\theta=0} = \\sum_{k=0}^{T-1} \\left( C s_k \\right)^\\top R_v^{-1} \\left( C s_k \\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order listed above. Each element must be a list of two items: the chosen schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ represented as a list of integers, and the numerical Fisher information value as a floating-point number. For example, the output format must be exactly like \"[[schedule1,FI1],[schedule2,FI2],[schedule3,FI3]]\" with no spaces.",
            "solution": "The problem requires finding an optimal Moving Target Defense (MTD) schedule for a discrete-time linear time-invariant (LTI) cyber-physical system (CPS). The objective is to minimize the adversary's ability to estimate a hidden system parameter, $\\theta$, quantified by the Fisher information, while adhering to a constraint on the nominal system's control performance.\n\nFirst, we formalize the problem by deriving expressions for the performance cost and the Fisher information as functions of the MTD schedule. An MTD schedule is a sequence $\\{\\sigma_k\\}_{k=0}^{T-1}$, where each $\\sigma_k \\in \\{0, 1, \\dots, M-1\\}$ selects the system matrix $A_{\\sigma_k}$ to be used at time step $k$.\n\nThe system dynamics are given by:\n$$x_{k+1} = \\left(A_{\\sigma_k} + \\theta F\\right) x_k + B u_k$$\n$$y_k = C x_k + v_k$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state, $u_k \\in \\mathbb{R}^p$ is the control input, $y_k \\in \\mathbb{R}^m$ is the measurement, and $v_k \\sim \\mathcal{N}(0, R_v)$ is i.i.d. Gaussian noise.\n\n**1. Performance Cost ($J$)**\n\nThe performance cost is defined at the nominal parameter value $\\theta = 0$. The nominal state trajectory, which we denote as $\\bar{x}_k$ to be precise, evolves according to:\n$$\\bar{x}_{k+1} = A_{\\sigma_k} \\bar{x}_k + B u_k$$\nwith a given initial condition $\\bar{x}_0 = x_0$. The cost function is:\n$$J = \\sum_{k=0}^{T-1} \\left( \\bar{x}_k^\\top Q_c \\bar{x}_k + u_k^\\top R_c u_k \\right)$$\nFor any given schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$ and known input sequence $\\{u_k\\}_{k=0}^{T-1}$, the nominal state trajectory $\\{\\bar{x}_k\\}_{k=0}^{T-1}$ can be computed by iterating the state equation starting from $\\bar{x}_0$. Subsequently, the cost $J$ can be calculated by summing the quadratic terms at each time step. The defender's task includes ensuring that $J \\leq J_{\\max}$.\n\n**2. Fisher Information ($I$)**\n\nThe Fisher information quantifies the amount of information that an observation carries about an unknown parameter. For a set of observations $Y$ from a Gaussian distribution with mean $\\mu(\\theta)$ and a covariance matrix independent of $\\theta$, the Fisher information for a scalar parameter $\\theta$ is given by:\n$$I(\\theta) = \\left( \\frac{\\partial \\mu(\\theta)}{\\partial \\theta} \\right)^\\top \\Sigma_Y^{-1} \\left( \\frac{\\partial \\mu(\\theta)}{\\partial \\theta} \\right)$$\nIn our problem, the observations are $y_k$ for $k \\in \\{0, \\dots, T-1\\}$. The stacked observation vector is $Y = [y_0^\\top, \\dots, y_{T-1}^\\top]^\\top$. The mean of $Y$ is $\\mu_Y(\\theta) = [E[y_0]^\\top, \\dots, E[y_{T-1}]^\\top]^\\top = [(C x_0(\\theta))^\\top, \\dots, (C x_{T-1}(\\theta))^\\top]^\\top$. Since the noise terms $v_k$ are i.i.d., the covariance matrix of $Y$ is block-diagonal, $\\Sigma_Y = \\text{diag}(R_v, \\dots, R_v)$. Its inverse is $\\Sigma_Y^{-1} = \\text{diag}(R_v^{-1}, \\dots, R_v^{-1})$.\n\nThe Fisher information can then be written as a sum over time:\n$$I(\\theta) = \\sum_{k=0}^{T-1} \\left( \\frac{\\partial E[y_k]}{\\partial \\theta} \\right)^\\top R_v^{-1} \\left( \\frac{\\partial E[y_k]}{\\partial \\theta} \\right)$$\nThe expected value of the output is $E[y_k] = C x_k(\\theta)$, where we write $x_k(\\theta)$ to emphasize its dependence on $\\theta$. The derivative is $\\frac{\\partial E[y_k]}{\\partial \\theta} = C \\frac{\\partial x_k(\\theta)}{\\partial \\theta}$. Let us define the sensitivity state as $s_k(\\theta) = \\frac{\\partial x_k(\\theta)}{\\partial \\theta}$.\n\nTo find the dynamics of the sensitivity, we differentiate the state equation with respect to $\\theta$:\n$$\\frac{\\partial x_{k+1}(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ (A_{\\sigma_k} + \\theta F) x_k(\\theta) + B u_k \\right]$$\n$$s_{k+1}(\\theta) = (A_{\\sigma_k} + \\theta F) s_k(\\theta) + F x_k(\\theta)$$\nThe problem asks to evaluate the Fisher information at $\\theta=0$. We evaluate the sensitivity dynamics at $\\theta=0$:\n$$s_{k+1}(0) = A_{\\sigma_k} s_k(0) + F x_k(0)$$\nLet $s_k \\triangleq s_k(0)$ and $\\bar{x}_k \\triangleq x_k(0)$. The recursion becomes:\n$$s_{k+1} = A_{\\sigma_k} s_k + F \\bar{x}_k$$\nThe initial state $x_0$ is a known constant, so it does not depend on $\\theta$. Thus, the initial sensitivity is $s_0 = \\frac{\\partial x_0}{\\partial \\theta} = 0$.\n\nSubstituting into the Fisher information formula and evaluating at $\\theta=0$:\n$$I \\triangleq I(\\theta)\\big|_{\\theta=0} = \\sum_{k=0}^{T-1} \\left( C s_k \\right)^\\top R_v^{-1} \\left( C s_k \\right)$$\nThis is the expression the defender seeks to minimize. Note that the calculation of $\\{s_k\\}$ requires the nominal state trajectory $\\{\\bar{x}_k\\}$, coupling the two calculations.\n\n**3. Algorithmic Approach**\n\nThe problem is to find a schedule $\\sigma^* = \\{\\sigma_k^*\\}_{k=0}^{T-1}$ that solves the constrained optimization problem:\n$$\\min_{\\{\\sigma_k\\}} \\sum_{k=0}^{T-1} (C s_k)^\\top R_v^{-1} (C s_k)$$\n$$\\text{subject to} \\quad \\sum_{k=0}^{T-1} \\left( \\bar{x}_k^\\top Q_c \\bar{x}_k + u_k^\\top R_c u_k \\right) \\leq J_{\\max}$$\n\nThe set of possible schedules is finite. Given $M$ modes and a time horizon of $T$, there are $M^T$ possible schedules. For the given test cases, $M=2$ and $T$ is small ($4$, $3$, and $1$), leading to $2^4=16$, $2^3=8$, and $2^1=2$ schedules, respectively. This makes an exhaustive search (brute force) a feasible and rigorous approach.\n\nThe algorithm is as follows:\n1.  Generate all $M^T$ possible schedules.\n2.  Initialize a variable for the minimum Fisher information found so far, $I_{min}$, to infinity, and a variable for the corresponding optimal schedule, $\\sigma_{best}$, to a null value.\n3.  For each schedule $\\{\\sigma_k\\}_{k=0}^{T-1}$:\n    a. Initialize $\\bar{x} \\leftarrow x_0$, $s \\leftarrow 0$, $J \\leftarrow 0$, $I \\leftarrow 0$.\n    b. For $k$ from $0$ to $T-1$:\n        i.   Select the matrix $A_{\\sigma_k}$ based on the current schedule element.\n        ii.  Add the cost contribution of the current state $\\bar{x}_k$: $J \\leftarrow J + \\bar{x}^\\top Q_c \\bar{x} + u_k^\\top R_c u_k$.\n        iii. Add the Fisher information contribution of the current sensitivity $s_k$: $I \\leftarrow I + (C s)^\\top R_v^{-1} (C s)$.\n        iv.  Compute the next state and sensitivity: $\\bar{x}_{next} \\leftarrow A_{\\sigma_k} \\bar{x} + B u_k$ and $s_{next} \\leftarrow A_{\\sigma_k} s + F \\bar{x}$.\n        v.   Update for the next iteration: $\\bar{x} \\leftarrow \\bar{x}_{next}$, $s \\leftarrow s_{next}$.\n    c. After iterating through all time steps, check if the performance constraint is met: $J \\leq J_{\\max}$.\n    d. If the constraint is met and the calculated Fisher information $I$ is less than the current minimum $I_{min}$, update $I_{min} \\leftarrow I$ and $\\sigma_{best} \\leftarrow \\{\\sigma_k\\}$.\n4.  After checking all schedules, the pair $(\\sigma_{best}, I_{min})$ is the solution. This process is repeated for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves the MTD scheduling problem for a series of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            \"n\": 2, \"m\": 1, \"p\": 1, \"T\": 4,\n            \"A0\": np.array([[0.92, 0.08], [0, 0.97]]),\n            \"A1\": np.array([[0.85, -0.05], [0.10, 0.90]]),\n            \"F\": np.array([[0, 0.2], [0, 0]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1, 0.5]]),\n            \"R_v\": np.array([[0.02]]),\n            \"x0\": np.array([[0.5], [-0.2]]),\n            \"u_k_val\": 0.2,\n            \"Qc\": np.diag([0.4, 0.4]),\n            \"Rc\": np.array([[0.02]]),\n            \"J_max\": 1.5\n        },\n        # Case 2\n        {\n            \"n\": 2, \"m\": 1, \"p\": 1, \"T\": 3,\n            \"A0\": np.array([[0.92, 0.08], [0, 0.97]]),\n            \"A1\": np.array([[0.85, -0.05], [0.10, 0.90]]),\n            \"F\": np.array([[0, 0.1], [0, 0]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1, 0.5]]),\n            \"R_v\": np.array([[0.01]]),\n            \"x0\": np.array([[0.2], [0.2]]),\n            \"u_k_val\": 0.0,\n            \"Qc\": np.diag([0.3, 0.3]),\n            \"Rc\": np.array([[0.01]]),\n            \"J_max\": 0.15\n        },\n        # Case 3\n        {\n            \"n\": 2, \"m\": 1, \"p\": 1, \"T\": 1,\n            \"A0\": np.array([[0.92, 0.08], [0, 0.97]]),\n            \"A1\": np.array([[0.85, -0.05], [0.10, 0.90]]),\n            \"F\": np.array([[0, 0.15], [0, 0]]),\n            \"B\": np.array([[0.1], [0.05]]),\n            \"C\": np.array([[1, 0.5]]),\n            \"R_v\": np.array([[0.02]]),\n            \"x0\": np.array([[1.0], [-1.0]]),\n            \"u_k_val\": 0.1,\n            \"Qc\": np.diag([0.5, 0.5]),\n            \"Rc\": np.array([[0.01]]),\n            \"J_max\": 2.5\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        T = case[\"T\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n        x0 = case[\"x0\"]\n        \n        As = [case[\"A0\"], case[\"A1\"]]\n        F = case[\"F\"]\n        B = case[\"B\"]\n        C = case[\"C\"]\n        R_v_inv = np.linalg.inv(case[\"R_v\"])\n        Qc = case[\"Qc\"]\n        Rc = case[\"Rc\"]\n        J_max = case[\"J_max\"]\n        \n        u_list = [np.array([[case[\"u_k_val\"]]]) for _ in range(T)]\n        \n        # Generate all possible schedules\n        schedules = list(itertools.product([0, 1], repeat=T))\n        \n        best_schedule = None\n        min_fisher_info = float('inf')\n\n        for schedule in schedules:\n            x = x0.copy()\n            s = np.zeros((n, 1))\n            \n            total_cost = 0.0\n            total_fisher = 0.0\n            \n            for k in range(T):\n                mode = schedule[k]\n                A_k = As[mode]\n                u_k = u_list[k]\n                \n                # Add contributions from current state x_k and sensitivity s_k\n                total_cost += (x.T @ Qc @ x + u_k.T @ Rc @ u_k).item()\n                \n                fisher_term = C @ s\n                total_fisher += (fisher_term.T @ R_v_inv @ fisher_term).item()\n\n                # Update state and sensitivity for the next step (k+1)\n                x_next = A_k @ x + B @ u_k\n                s_next = A_k @ s + F @ x\n                \n                x = x_next\n                s = s_next\n\n            # Check constraint and update best schedule\n            if total_cost <= J_max:\n                if total_fisher < min_fisher_info:\n                    min_fisher_info = total_fisher\n                    best_schedule = schedule\n\n        final_results.append([list(best_schedule), min_fisher_info])\n\n    # Format the final output string\n    formatted_parts = []\n    for sched, fi in final_results:\n        sched_str = str(sched).replace(\" \", \"\")\n        formatted_parts.append(f\"[{sched_str},{fi}]\")\n    \n    print(f\"[{','.join(formatted_parts)}]\")\n\nsolve()\n```"
        }
    ]
}