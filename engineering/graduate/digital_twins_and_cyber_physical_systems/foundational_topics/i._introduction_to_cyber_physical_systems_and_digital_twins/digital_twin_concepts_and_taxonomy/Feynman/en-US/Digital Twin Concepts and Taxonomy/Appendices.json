{
    "hands_on_practices": [
        {
            "introduction": "A digital twin's fidelity begins with the quality of its input data. This exercise explores the foundational principle of data acquisition from a cyber-physical system, the Nyquist-Shannon Sampling Theorem. By determining the correct sampling frequency, you will learn how to prevent aliasing and ensure that the continuous dynamics of a physical asset are faithfully represented in the discrete digital domain, a critical first step for any high-fidelity twin .",
            "id": "4213713",
            "problem": "A continuous-time sensor signal from a Cyber-Physical System (CPS) is streamed into the data acquisition layer of a Digital Twin (DT). The signal’s measured one-sided power spectral density is negligible beyond $120\\,\\mathrm{Hz}$, and system stakeholders report that the physically meaningful dynamics have dominant frequency content up to $120\\,\\mathrm{Hz}$. The DT requires that the sampled data be sufficient for perfect reconstruction under the assumptions of the Nyquist–Shannon Sampling Theorem (NSST), and that aliasing be practically suppressed by an anti-aliasing analog filter placed prior to sampling.\n\nStarting from the statement of the Nyquist–Shannon Sampling Theorem and the definition of aliasing in time-discrete sampling, reason from first principles to determine the minimum sampling frequency that guarantees perfect reconstruction for a strictly bandlimited signal whose highest non-zero frequency component is $120\\,\\mathrm{Hz}$.\n\nThen, discuss the qualitative requirements that the anti-aliasing filter must satisfy in the CPS-to-DT ingestion pipeline to make the bandlimited assumption true in practice. Address passband and stopband specifications in terms of the sampling frequency and the signal’s dominant content and explain how the transition band relates to the achievable suppression of out-of-band energy.\n\nExpress the final sampling frequency numerically in hertz ($\\mathrm{Hz}$). No rounding is required; report the exact value.",
            "solution": "The problem requires a determination of the minimum sampling frequency for a bandlimited signal based on the Nyquist–Shannon Sampling Theorem (NSST), followed by a qualitative discussion of the practical requirements for an anti-aliasing filter.\n\nFirst, we address the determination of the minimum sampling frequency from first principles.\n\nThe Nyquist–Shannon Sampling Theorem establishes the fundamental condition under which a continuous-time signal can be perfectly reconstructed from a sequence of its uniformly spaced samples. Let $x(t)$ be a continuous-time signal and its Fourier transform be $X(f)$. The signal $x(t)$ is said to be bandlimited to a maximum frequency $f_{\\text{max}}$ if its Fourier transform $X(f)$ is zero for all frequencies $|f| > f_{\\text{max}}$.\n$$X(f) = 0 \\quad \\text{for} \\quad |f| > f_{\\text{max}}$$\nSampling the signal $x(t)$ at a frequency $f_s$ with a sampling period $T_s = 1/f_s$ produces a discrete-time sequence $x[n] = x(nT_s)$. The process of ideal sampling can be modeled as multiplying the continuous-time signal by an impulse train. The Fourier transform of the sampled signal, denoted $X_s(f)$, is a periodic summation of the original signal's spectrum:\n$$X_s(f) = f_s \\sum_{k=-\\infty}^{\\infty} X(f - kf_s)$$\nThis equation shows that the spectrum of the sampled signal consists of replicas of the original spectrum $X(f)$, scaled by $f_s$ and shifted to be centered at integer multiples of the sampling frequency, $kf_s$.\n\nAliasing occurs when these spectral replicas overlap. If the original signal is bandlimited to $f_{\\text{max}}$, the central replica (for $k=0$) occupies the frequency band $[-f_{\\text{max}}, f_{\\text{max}}]$. The adjacent replicas (for $k = \\pm 1$) are centered at $\\pm f_s$ and occupy the bands $[f_s - f_{\\text{max}}, f_s + f_{\\text{max}}]$ and $[-f_s - f_{\\text{max}}, -f_s + f_{\\text{max}}]$. For the replicas not to overlap, the upper edge of the central replica ($f_{\\text{max}}$) must not exceed the lower edge of the first positive replica ($f_s - f_{\\text{max}}$). This leads to the condition:\n$$f_{\\text{max}} \\le f_s - f_{\\text{max}}$$\nRearranging this inequality gives the Nyquist criterion:\n$$f_s \\ge 2f_{\\text{max}}$$\nIf this condition holds, the original signal can be perfectly reconstructed from its samples by passing the sampled signal through an ideal low-pass filter with a cutoff frequency between $f_{\\text{max}}$ and $f_s - f_{\\text{max}}$. The frequency $2f_{\\text{max}}$ is known as the Nyquist rate. The sampling frequency $f_s$ must be at least the Nyquist rate. Therefore, the minimum sampling frequency, $f_{s, \\text{min}}$, that guarantees perfect reconstruction is precisely the Nyquist rate.\n\nThe problem states that the signal is strictly bandlimited with the highest non-zero frequency component at $f_{\\text{max}} = 120\\,\\mathrm{Hz}$. Applying the Nyquist criterion, the minimum sampling frequency is:\n$$f_{s, \\text{min}} = 2 \\times f_{\\text{max}} = 2 \\times 120\\,\\mathrm{Hz} = 240\\,\\mathrm{Hz}$$\n\nNext, we discuss the qualitative requirements for the anti-aliasing filter.\n\nThe premise of a \"strictly bandlimited\" signal is a mathematical idealization. Real-world signals, such as those from a Cyber-Physical System sensor, are typically not strictly bandlimited. The problem text acknowledges this by stating the signal's power spectral density is \"negligible\" beyond $120\\,\\mathrm{Hz}$. An anti-aliasing filter, which is an analog low-pass filter, is required to be placed before the analog-to-digital converter (sampler) to enforce the bandlimited condition in practice. Its function is to attenuate frequency components above a certain cutoff to a level where they will not cause significant aliasing upon sampling.\n\nThe specifications of this filter are defined by its behavior in three regions: the passband, the stopband, and the transition band.\n\n$1$. **Passband**: This is the range of frequencies that the filter must pass with minimal distortion. For this problem, the physically meaningful dynamics have content up to $120\\,\\mathrm{Hz}$. Therefore, the filter's passband must extend up to this frequency. The passband edge frequency, $f_p$, should be set at or just above the highest frequency of interest, so $f_p \\ge 120\\,\\mathrm{Hz}$. Within this band, $[0, f_p]$, the filter should ideally have a flat magnitude response (e.g., gain close to $1$) and a linear phase response (constant group delay) to avoid distorting the signal.\n\n$2$. **Stopband**: This is the range of frequencies that the filter must significantly attenuate. Aliasing folds frequencies above the Nyquist frequency, $f_s/2$, back into the baseband $[0, f_s/2]$. Therefore, to prevent aliasing, the filter's stopband must begin at or before the Nyquist frequency. The stopband edge frequency is denoted $f_{\\text{stop}}$. The attenuation in the stopband (e.g., measured in decibels, dB) must be sufficient to suppress any out-of-band noise and interference to below the noise floor or quantization error of the system.\n\n$3$. **Transition Band**: A physical filter cannot transition instantaneously from the passband to the stopband. This region of finite width, from $f_p$ to $f_{\\text{stop}}$, is the transition band. The steepness of the filter's response in this band is its \"roll-off\".\n\nThe relationship between these parameters and the sampling frequency is critical. If we were to sample at the theoretical minimum, $f_s = 240\\,\\mathrm{Hz}$, the Nyquist frequency would be $f_s/2 = 120\\,\\mathrm{Hz}$. In this case, the passband must extend up to $f_{\\text{max}} = 120\\,\\mathrm{Hz}$, and the stopband must begin at $f_s/2 = 120\\,\\mathrm{Hz}$. This implies $f_p = f_{\\text{stop}} = 120\\,\\mathrm{Hz}$, which corresponds to a zero-width transition band. Such a filter is a \"brick-wall\" filter, which is physically unrealizable.\n\nTo accommodate a real-world filter with a non-zero transition band, the sampling frequency must be chosen to be greater than the Nyquist rate ($f_s > 2f_{\\text{max}}$). This technique, known as oversampling, creates a \"guard band\" in the frequency domain between the highest frequency of interest, $f_{\\text{max}}$, and the Nyquist frequency, $f_s/2$. The width of this guard band is $(f_s/2) - f_{\\text{max}}$.\nThe filter can now be designed with its passband edge at $f_p \\approx f_{\\text{max}}~(=120\\,\\mathrm{Hz})$ and its stopband edge at $f_{\\text{stop}} \\le f_s/2$. The transition band, of width $f_{\\text{stop}} - f_p$, now fits entirely within the guard band.\n\nThe achievable suppression of out-of-band energy is determined by the filter's order and its roll-off characteristics. A steeper roll-off (associated with a higher-order filter) allows for a narrower transition band. This means that for a given level of stopband attenuation, a steeper filter allows the sampling frequency $f_s$ to be closer to the theoretical minimum of $2f_{\\text{max}}$. Conversely, using a higher sampling frequency (a wider guard band) relaxes the requirement on the filter's roll-off, potentially allowing for a simpler, lower-order filter to achieve the necessary suppression of aliased components.",
            "answer": "$$\\boxed{240}$$"
        },
        {
            "introduction": "Once a digital twin is operational, we must quantify its predictive accuracy, a core dimension in the digital twin taxonomy. This practice guides you through the calculation of two essential error metrics, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), using a hypothetical prediction dataset. Understanding the distinction between these $L_{p}$-norm-based metrics provides crucial insight into a twin's sensitivity to outliers and helps in calibrating its robustness and trustworthiness .",
            "id": "4213798",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) is evaluated along a fidelity dimension in the twin taxonomy using data-driven error functionals grounded in empirical risk. Consider a single-output DT predicting a scalar measurement from the CPS over $n=3$ synchronized samples. The ground-truth sequence is $y=[1,2,3]$ and the DT predictions are $\\hat{y}=[1.1,1.8,3.4]$. Starting from the core definitions of empirical $L_{p}$ risk and norms on residuals (with $p=2$ and $p=1$), derive the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE), and use them to form the following dimensionless robustness coefficient that captures the $L_{2}$-to-$L_{1}$ sensitivity ratio of the DT:\n$$\nC \\equiv \\frac{\\mathrm{RMSE}}{\\mathrm{MAE}}.\n$$\nExplain, based on first principles of $L_{p}$ risk and the twin taxonomy’s emphasis on fidelity and robustness, what the relative magnitudes of RMSE and MAE imply for DT accuracy and outlier sensitivity in this scenario. Report only the final value of $C$ as your answer, rounded to four significant figures. The quantity $C$ is dimensionless; do not include units in your final reported value.",
            "solution": "A Digital Twin (DT) approximates a Cyber-Physical System (CPS) by mapping inputs to outputs, and its predictive fidelity can be assessed using empirical risk functionals. Let the residuals be defined as $r_{i}=\\hat{y}_{i}-y_{i}$ for $i=1,\\dots,n$. The empirical $L_{p}$ risk over residuals corresponds to the $L_{p}$-type average magnitude of $r$, and the $p=2$ and $p=1$ cases yield the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), respectively. Specifically, using the standard constructions:\n- The empirical $L_{2}$ measure of residuals is the square root of the mean of squared residuals, which is RMSE.\n- The empirical $L_{1}$ measure of residuals is the mean of absolute residuals, which is MAE.\n\nWe compute residuals from the given data. With $y=[1,2,3]$ and $\\hat{y}=[1.1,1.8,3.4]$,\n$$\nr_{1}=\\hat{y}_{1}-y_{1}=1.1-1=0.1,\\quad\nr_{2}=\\hat{y}_{2}-y_{2}=1.8-2=-0.2,\\quad\nr_{3}=\\hat{y}_{3}-y_{3}=3.4-3=0.4.\n$$\nThe absolute residuals are $|r_{1}|=0.1$, $|r_{2}|=0.2$, $|r_{3}|=0.4$, and the squared residuals are $r_{1}^{2}=0.01$, $r_{2}^{2}=0.04$, $r_{3}^{2}=0.16$.\n\nCompute the Mean Absolute Error (MAE) using the empirical $L_{1}$ measure:\n$$\n\\mathrm{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}|r_{i}|=\\frac{0.1+0.2+0.4}{3}=\\frac{0.7}{3}=\\frac{7}{30}.\n$$\n\nCompute the Root Mean Squared Error (RMSE) using the empirical $L_{2}$ measure:\n$$\n\\mathrm{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}r_{i}^{2}}=\\sqrt{\\frac{0.01+0.04+0.16}{3}}=\\sqrt{\\frac{0.21}{3}}=\\sqrt{0.07}=\\sqrt{\\frac{7}{100}}=\\frac{\\sqrt{7}}{10}.\n$$\n\nForm the robustness coefficient $C$:\n$$\nC=\\frac{\\mathrm{RMSE}}{\\mathrm{MAE}}=\\frac{\\frac{\\sqrt{7}}{10}}{\\frac{7}{30}}=\\frac{\\sqrt{7}}{10}\\cdot\\frac{30}{7}=\\frac{3\\sqrt{7}}{7}.\n$$\nNumerically, using $\\sqrt{7}\\approx 2.64575131$,\n$$\nC\\approx \\frac{3\\times 2.64575131}{7}\\approx \\frac{7.93725393}{7}\\approx 1.13389342.\n$$\nRounded to four significant figures, $C=1.134$.\n\nInterpretation grounded in $L_{p}$ risk and twin taxonomy: In general, for nonnegative magnitudes, the quadratic mean exceeds the arithmetic mean, so $\\mathrm{RMSE}\\ge \\mathrm{MAE}$, with equality only when all absolute residuals are equal. Here, the residual magnitudes are unequal, with one larger error at $0.4$; squaring emphasizes this larger deviation, making $\\mathrm{RMSE}$ exceed $\\mathrm{MAE}$ and yielding $C>1$. The value $C\\approx 1.134$ indicates a modest elevation of the $L_{2}$-sensitive error over the $L_{1}$-sensitive error, reflecting limited outlier influence rather than severe heavy-tailed deviations. In the digital twin taxonomy’s fidelity axis, this suggests that the DT’s descriptive accuracy is reasonably consistent across samples but not perfectly uniform; the presence of a somewhat larger error implies that optimization and monitoring strategies sensitive to $L_{2}$ penalization will register slightly worse performance than robust $L_{1}$ assessments. Consequently, for robust operation and trust calibration, one may prioritize $L_{1}$-robust tuning if outliers are expected, whereas $L_{2}$-based tuning remains suitable when larger deviations like the $0.4$ residual are infrequent or acceptable within the DT’s intended use (for example, predictive monitoring rather than strict prescriptive control).",
            "answer": "$$\\boxed{1.134}$$"
        },
        {
            "introduction": "A defining characteristic of a functional digital twin is its ability to maintain synchronization with its physical counterpart amidst uncertainty. This advanced practice delves into the Kalman filter, a cornerstone algorithm for probabilistic state estimation. You will derive the filter from first principles for a linear system and apply it to a practical scenario, gaining hands-on experience with the core mechanism that allows a digital twin to fuse model predictions with noisy sensor data to track a system's true state in real time .",
            "id": "4213735",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) uses probabilistic state estimation to synchronize its virtual state with the physical asset. Consider a continuous-time Linear Time-Invariant (LTI) stochastic system with a Gaussian process model and sampled measurements:\n$$\\dot{x}(t) = A x(t) + B u(t) + w(t), \\quad y_{k} = C x(t_{k}) + v_{k},$$\nwhere $x(t) \\in \\mathbb{R}^{n}$ is the physical state, $u(t) \\in \\mathbb{R}^{m}$ is a known input, $y_{k} \\in \\mathbb{R}^{p}$ is the measurement at sampling instants $t_{k} = k \\Delta$, $w(t)$ is zero-mean Gaussian white process noise with spectral density (covariance rate) $Q \\succeq 0$, and $v_{k}$ is zero-mean independent and identically distributed Gaussian measurement noise with covariance $R \\succ 0$. Assume $x(0)$ is Gaussian, independent of $w(t)$ and $v_{k}$. The Digital Twin’s estimator must be derived from first principles (linear-Gaussian modeling, discretization of the continuous-time dynamics, and Bayesian conditioning).\n\nTask 1: Using the foundational laws of linear system discretization and Gaussian inference, derive the discrete-time Kalman Filter (KF) recursion implemented by the DT at sampling interval $\\Delta$, explicitly expressing the time update and measurement update equations in terms of $A_{\\Delta}$, $B_{\\Delta}$, and $Q_{\\Delta}$ obtained from the continuous-time model. Your derivation must start from the continuous-time stochastic model, produce the discrete-time equivalent with $x_{k+1} = A_{\\Delta} x_{k} + B_{\\Delta} u_{k} + w_{k}$, $y_{k} = C x_{k} + v_{k}$, and express $A_{\\Delta}$, $B_{\\Delta}$, and $Q_{\\Delta}$ from $A$, $B$, $Q$, and $\\Delta$ using scientifically standard integrals and operators. Then derive the Kalman time update for $(\\hat{x}_{k+1|k}, P_{k+1|k})$ and the measurement update for $(\\hat{x}_{k+1|k+1}, P_{k+1|k+1})$ and the gain $K_{k+1}$, all based on Gaussian conditioning, without invoking any unproven shortcut formulas.\n\nTask 2: Specialize your derived recursion to a scalar DT estimator for a single-input single-output CPS with parameters $A = -1$, $B = 0$, $C = 1$, $Q = 2$, $R = 1$, sampling interval $\\Delta = 0.5$, zero input $u_{k} = 0$ for all $k$, and initial posterior covariance $P_{0|0} = 3$. Compute the posterior covariance after two complete measurement updates, $P_{2|2}$. Round your final numerical answer to four significant figures. Express your answer as unitless.",
            "solution": "The solution is divided into two tasks. First, the derivation of the general discrete-time Kalman Filter from the given continuous-time model. Second, the application of this filter to a specific scalar system to compute the posterior covariance after two steps.\n\n**Task 1: Derivation of the Discrete-Time Kalman Filter**\n\nThe derivation starts from the continuous-time stochastic differential equation and proceeds to the discretization of the system dynamics, followed by the derivation of the Kalman Filter equations based on Bayesian inference for linear-Gaussian systems.\n\n**Discretization of the Continuous-Time Model**\nThe state of the physical system is governed by the linear stochastic differential equation:\n$$\n\\dot{x}(t) = A x(t) + B u(t) + w(t)\n$$\nThe formal solution to this equation over the time interval $[t_k, t_{k+1}]$, where $t_{k+1} - t_k = \\Delta$, is given by the variation of constants formula:\n$$\nx(t_{k+1}) = \\exp(A\\Delta) x(t_k) + \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) [B u(\\tau) + w(\\tau)] d\\tau\n$$\nWe define $x_k \\equiv x(t_k)$. Assuming a zero-order hold on the input, where $u(\\tau) = u_k$ for $\\tau \\in [t_k, t_{k+1})$, the equation becomes:\n$$\nx_{k+1} = \\exp(A\\Delta) x_k + \\left(\\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) d\\tau \\right) B u_k + \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) w(\\tau) d\\tau\n$$\nBy performing a change of variable $\\sigma = t_{k+1} - \\tau$ in the first integral, we obtain the discrete-time state-space model:\n$$\nx_{k+1} = A_{\\Delta} x_k + B_{\\Delta} u_k + w_k\n$$\nwhere the discrete-time system matrix $A_{\\Delta}$, input matrix $B_{\\Delta}$, and process noise term $w_k$ are defined as:\n$$\nA_{\\Delta} = \\exp(A\\Delta)\n$$\n$$\nB_{\\Delta} = \\left(\\int_{0}^{\\Delta} \\exp(A\\sigma) d\\sigma\\right) B\n$$\n$$\nw_k = \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) w(\\tau) d\\tau\n$$\nThe term $w_k$ is a zero-mean Gaussian random variable, as it is a linear transformation of the Gaussian process $w(t)$. Its covariance, $Q_{\\Delta}$, is given by:\n$$\nQ_{\\Delta} = \\mathbb{E}[w_k w_k^T] = \\mathbb{E}\\left[ \\left(\\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau_1)) w(\\tau_1) d\\tau_1 \\right) \\left(\\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau_2)) w(\\tau_2) d\\tau_2 \\right)^T \\right]\n$$\n$$\nQ_{\\Delta} = \\int_{t_k}^{t_{k+1}} \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau_1)) \\mathbb{E}[w(\\tau_1) w(\\tau_2)^T] \\exp(A(t_{k+1} - \\tau_2))^T d\\tau_1 d\\tau_2\n$$\nThe continuous-time process noise $w(t)$ is white, meaning its autocorrelation is $\\mathbb{E}[w(\\tau_1) w(\\tau_2)^T] = Q \\delta(\\tau_1 - \\tau_2)$, where $\\delta(\\cdot)$ is the Dirac delta function. Substituting this and using the sifting property of the delta function, we get:\n$$\nQ_{\\Delta} = \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) Q \\exp(A(t_{k+1} - \\tau))^T d\\tau\n$$\nWith the change of variable $\\sigma = t_{k+1} - \\tau$, this becomes:\n$$\nQ_{\\Delta} = \\int_{0}^{\\Delta} \\exp(A\\sigma) Q \\exp(A\\sigma)^T d\\sigma\n$$\nThe full discrete-time model is thus:\n$$\nx_{k+1} = A_{\\Delta} x_k + B_{\\Delta} u_k + w_k, \\quad w_k \\sim \\mathcal{N}(0, Q_{\\Delta})\n$$\n$$\ny_k = C x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, R)\n$$\n\n**Derivation of the Kalman Filter Recursion**\nThe Kalman filter is a recursive Bayesian estimator. We assume that at time $k$, the posterior distribution of the state $x_k$ given all measurements up to time $k$, denoted $y_{1:k}$, is Gaussian: $p(x_k | y_{1:k}) = \\mathcal{N}(x_k; \\hat{x}_{k|k}, P_{k|k})$. The recursion consists of two steps: time update and measurement update.\n\n**Time Update (Prediction):**\nThe goal is to find the distribution of $x_{k+1}$ given measurements $y_{1:k}$. This is the prior distribution for step $k+1$.\n$$\np(x_{k+1} | y_{1:k}) = \\int p(x_{k+1} | x_k) p(x_k | y_{1:k}) dx_k\n$$\nSince both distributions in the integral are Gaussian and the relationship $x_{k+1} = A_{\\Delta}x_k + B_{\\Delta}u_k + w_k$ is linear, the resulting distribution for $x_{k+1}$ is also Gaussian. Its mean and covariance are found as follows.\nThe predicted mean is the expectation of $x_{k+1}$ conditioned on $y_{1:k}$:\n$$\n\\hat{x}_{k+1|k} = \\mathbb{E}[x_{k+1} | y_{1:k}] = \\mathbb{E}[A_{\\Delta} x_k + B_{\\Delta} u_k + w_k | y_{1:k}]\n$$\n$$\n\\hat{x}_{k+1|k} = A_{\\Delta} \\mathbb{E}[x_k | y_{1:k}] + B_{\\Delta} u_k + \\mathbb{E}[w_k] = A_{\\Delta} \\hat{x}_{k|k} + B_{\\Delta} u_k\n$$\nThe predicted covariance is:\n$$\nP_{k+1|k} = \\mathbb{E}[(x_{k+1} - \\hat{x}_{k+1|k})(x_{k+1} - \\hat{x}_{k+1|k})^T | y_{1:k}]\n$$\n$$\nP_{k+1|k} = \\mathbb{E}[(A_{\\Delta}(x_k - \\hat{x}_{k|k}) + w_k)(A_{\\Delta}(x_k - \\hat{x}_{k|k}) + w_k)^T | y_{1:k}]\n$$\nSince the estimation error $(x_k - \\hat{x}_{k|k})$ is independent of the future process noise $w_k$, the cross-terms are zero:\n$$\nP_{k+1|k} = A_{\\Delta} \\mathbb{E}[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^T | y_{1:k}] A_{\\Delta}^T + \\mathbb{E}[w_k w_k^T]\n$$\n$$\nP_{k+1|k} = A_{\\Delta} P_{k|k} A_{\\Delta}^T + Q_{\\Delta}\n$$\nSo, the prior distribution at step $k+1$ is $p(x_{k+1} | y_{1:k}) = \\mathcal{N}(x_{k+1}; \\hat{x}_{k+1|k}, P_{k+1|k})$.\n\n**Measurement Update (Correction):**\nUpon receiving the measurement $y_{k+1}$, we update the prior to obtain the posterior $p(x_{k+1} | y_{1:k+1})$. Using Bayes' rule:\n$$\np(x_{k+1} | y_{1:k+1}) \\propto p(y_{k+1} | x_{k+1}) p(x_{k+1} | y_{1:k})\n$$\nThe likelihood $p(y_{k+1} | x_{k+1})$ is obtained from the measurement model $y_{k+1} = C x_{k+1} + v_{k+1}$, which implies $p(y_{k+1} | x_{k+1}) = \\mathcal{N}(y_{k+1}; C x_{k+1}, R)$. We are multiplying two Gaussian distributions. The exponent of the resulting posterior is the sum of the exponents of the prior and the likelihood (ignoring constant terms):\n$$\nL = -\\frac{1}{2} (x_{k+1} - \\hat{x}_{k+1|k})^T P_{k+1|k}^{-1} (x_{k+1} - \\hat{x}_{k+1|k}) -\\frac{1}{2} (y_{k+1} - C x_{k+1})^T R^{-1} (y_{k+1} - C x_{k+1})\n$$\nExpanding and collecting terms quadratic and linear in $x_{k+1}$:\n$$\nL = -\\frac{1}{2} \\left( x_{k+1}^T(P_{k+1|k}^{-1} + C^T R^{-1} C)x_{k+1} - 2x_{k+1}^T(P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1}) + \\dots \\right)\n$$\nBy completing the square, we can identify the mean $\\hat{x}_{k+1|k+1}$ and covariance $P_{k+1|k+1}$ of the posterior Gaussian. The inverse of the posterior covariance is:\n$$\nP_{k+1|k+1}^{-1} = P_{k+1|k}^{-1} + C^T R^{-1} C\n$$\nAnd the posterior mean satisfies:\n$$\nP_{k+1|k+1}^{-1} \\hat{x}_{k+1|k+1} = P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1}\n$$\n$$\n\\hat{x}_{k+1|k+1} = P_{k+1|k+1} (P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1})\n$$\nTo get the standard filter equations, we apply the Woodbury matrix identity to $P_{k+1|k+1}^{-1}$:\n$$\nP_{k+1|k+1} = (P_{k+1|k}^{-1} + C^T R^{-1} C)^{-1} = P_{k+1|k} - P_{k+1|k} C^T (C P_{k+1|k} C^T + R)^{-1} C P_{k+1|k}\n$$\nWe define the Kalman Gain $K_{k+1}$ as:\n$$\nK_{k+1} = P_{k+1|k} C^T (C P_{k+1|k} C^T + R)^{-1}\n$$\nThe posterior covariance can then be written as:\n$$\nP_{k+1|k+1} = P_{k+1|k} - K_{k+1} C P_{k+1|k} = (I - K_{k+1} C) P_{k+1|k}\n$$\nSubstituting this back into the expression for $\\hat{x}_{k+1|k+1}$:\n$$\n\\hat{x}_{k+1|k+1} = (I - K_{k+1} C) P_{k+1|k} (P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1})\n$$\n$$\n\\hat{x}_{k+1|k+1} = (I - K_{k+1} C) \\hat{x}_{k+1|k} + (P_{k+1|k} - K_{k+1} C P_{k+1|k}) C^T R^{-1} y_{k+1}\n$$\nUsing $K_{k+1} (C P_{k+1|k} C^T + R) = P_{k+1|k} C^T$, which implies $K_{k+1} C P_{k+1|k} C^T + K_{k+1} R = P_{k+1|k} C^T$, we can show that $(P_{k+1|k} - K_{k+1} C P_{k+1|k}) C^T R^{-1} = K_{k+1}$.\nTherefore, the posterior mean update is:\n$$\n\\hat{x}_{k+1|k+1} = (I - K_{k+1} C) \\hat{x}_{k+1|k} + K_{k+1} y_{k+1} = \\hat{x}_{k+1|k} + K_{k+1}(y_{k+1} - C \\hat{x}_{k+1|k})\n$$\n\n**Task 2: Application to Scalar System**\n\nWe now apply these derived equations to the specific scalar system.\nThe given parameters are: $A = -1$, $B = 0$, $C = 1$, $Q = 2$, $R = 1$, $\\Delta = 0.5$.\nThe initial posterior covariance is $P_{0|0} = 3$.\n\n**Parameter Calculation**\nFirst, we compute the discrete-time parameters $A_{\\Delta}$ and $Q_{\\Delta}$.\n$$\nA_{\\Delta} = \\exp(A\\Delta) = \\exp((-1)(0.5)) = \\exp(-0.5)\n$$\nSince $B = 0$, we have $B_{\\Delta} = 0$.\nThe discrete process noise covariance $Q_{\\Delta}$ is:\n$$\nQ_{\\Delta} = \\int_{0}^{\\Delta} \\exp(A\\sigma) Q \\exp(A\\sigma)^T d\\sigma = \\int_{0}^{0.5} \\exp(-1\\sigma) \\cdot 2 \\cdot \\exp(-1\\sigma) d\\sigma = 2 \\int_{0}^{0.5} \\exp(-2\\sigma) d\\sigma\n$$\n$$\nQ_{\\Delta} = 2 \\left[ \\frac{\\exp(-2\\sigma)}{-2} \\right]_{0}^{0.5} = -[\\exp(-2 \\cdot 0.5) - \\exp(0)] = -(\\exp(-1) - 1) = 1 - \\exp(-1)\n$$\n\n**Covariance Propagation**\nWe need to compute $P_{2|2}$ starting from $P_{0|0} = 3$. The recursion involves propagation of the covariance only.\n\n**Step 1: from $k=0$ to $k=1$**\nTime Update (Prediction for $k=1$):\n$$\nP_{1|0} = A_{\\Delta} P_{0|0} A_{\\Delta}^T + Q_{\\Delta} = A_{\\Delta}^2 P_{0|0} + Q_{\\Delta}\n$$\n$$\nP_{1|0} = (\\exp(-0.5))^2 \\cdot 3 + (1 - \\exp(-1)) = 3\\exp(-1) + 1 - \\exp(-1) = 1 + 2\\exp(-1)\n$$\nMeasurement Update (Correction at $k=1$):\nFor the scalar case, the covariance update equation simplifies. The Kalman gain is $K_1 = P_{1|0}C^T(C P_{1|0} C^T + R)^{-1} = P_{1|0}(1 \\cdot P_{1|0} \\cdot 1 + 1)^{-1} = \\frac{P_{1|0}}{P_{1|0}+1}$.\nThe posterior covariance is $P_{1|1} = (1 - K_1 C)P_{1|0} = (1 - K_1)P_{1|0}$.\n$$\nP_{1|1} = \\left(1 - \\frac{P_{1|0}}{P_{1|0}+1}\\right) P_{1|0} = \\left(\\frac{1}{P_{1|0}+1}\\right) P_{1|0} = \\frac{P_{1|0}}{P_{1|0}+1}\n$$\nSubstituting the value of $P_{1|0}$:\n$$\nP_{1|1} = \\frac{1 + 2\\exp(-1)}{1 + 2\\exp(-1) + 1} = \\frac{1 + 2\\exp(-1)}{2 + 2\\exp(-1)}\n$$\n\n**Step 2: from $k=1$ to $k=2$**\nTime Update (Prediction for $k=2$):\n$$\nP_{2|1} = A_{\\Delta}^2 P_{1|1} + Q_{\\Delta} = \\exp(-1) P_{1|1} + (1 - \\exp(-1))\n$$\n$$\nP_{2|1} = \\exp(-1)\\left(\\frac{1 + 2\\exp(-1)}{2 + 2\\exp(-1)}\\right) + (1 - \\exp(-1)) = \\frac{\\exp(-1) + 2\\exp(-2)}{2(1 + \\exp(-1))} + \\frac{(1 - \\exp(-1)) \\cdot 2(1 + \\exp(-1))}{2(1 + \\exp(-1))}\n$$\n$$\nP_{2|1} = \\frac{\\exp(-1) + 2\\exp(-2) + 2(1 - \\exp(-2))}{2(1 + \\exp(-1))} = \\frac{\\exp(-1) + 2\\exp(-2) + 2 - 2\\exp(-2)}{2(1 + \\exp(-1))}\n$$\n$$\nP_{2|1} = \\frac{2 + \\exp(-1)}{2(1 + \\exp(-1))}\n$$\nMeasurement Update (Correction at $k=2$):\nSimilar to the previous step, the posterior covariance is $P_{2|2} = \\frac{P_{2|1}}{P_{2|1}+1}$.\n$$\nP_{2|2} = \\frac{\\frac{2 + \\exp(-1)}{2(1 + \\exp(-1))}}{\\frac{2 + \\exp(-1)}{2(1 + \\exp(-1))} + 1} = \\frac{2 + \\exp(-1)}{2 + \\exp(-1) + 2(1 + \\exp(-1))}\n$$\n$$\nP_{2|2} = \\frac{2 + \\exp(-1)}{2 + \\exp(-1) + 2 + 2\\exp(-1)} = \\frac{2 + \\exp(-1)}{4 + 3\\exp(-1)}\n$$\n\n**Numerical Result**\nWe now compute the final numerical value of $P_{2|2}$.\nUsing the value $\\exp(-1) \\approx 0.36787944$:\n$$\nP_{2|2} = \\frac{2 + 0.36787944}{4 + 3(0.36787944)} = \\frac{2.36787944}{4 + 1.10363832} = \\frac{2.36787944}{5.10363832} \\approx 0.463953503\n$$\nRounding to four significant figures, we get $0.4640$.\nThe posterior covariance after two measurement updates is $P_{2|2} \\approx 0.4640$.",
            "answer": "$$\n\\boxed{0.4640}\n$$"
        }
    ]
}