{
    "hands_on_practices": [
        {
            "introduction": "降阶基方法的根本优势在于其计算效率，它通过昂贵的“离线”阶段来换取极速的“在线”查询。本练习将引导你通过推导和计算，量化地理解这一核心权衡 。掌握这种成本分析，对于理解为何降阶基方法在需要大量重复计算的自动化设计和优化任务中如此有效至关重要。",
            "id": "3945459",
            "problem": "考虑一个参数化的电化学模型，该模型在空间离散化后，产生一系列形式为 $A(\\mu) x(\\mu) = b(\\mu)$ 的对称正定线性系统，其中 $A(\\mu) \\in \\mathbb{R}^{N \\times N}$ 是一个仿射参数化 $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q$，包含 $Q$ 个与参数无关的矩阵 $A_q \\in \\mathbb{R}^{N \\times N}$ 和标量系数函数 $\\theta_q(\\mu)$。通过离线生成 $N_s$ 个全阶快照，然后通过对快照矩阵进行奇异值分解（SVD）来计算本征正交分解（POD）基，从而构建一个降阶基（RB）逼近。通过将每个 $A_q$ 投影到由RB基 $V \\in \\mathbb{R}^{N \\times N_r}$ 张成的降阶空间上，离线执行仿射降阶组装，从而产生降阶算子 $A_q^r = V^\\top A_q V \\in \\mathbb{R}^{N_r \\times N_r}$。在在线阶段，对于一个新的参数 $\\mu$，组装降阶算子 $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$ 并求解降阶状态 $x^r(\\mu) \\in \\mathbb{R}^{N_r}$，然后从降阶解中评估状态的一个线性泛函（例如，像端电压这样的标量输出）。\n\n从基本的线性代数运算计数和标准的计算事实出发，推导离线计算成本的表达式，用于：\n- 生成 $N_s$ 个全阶快照，\n- 通过对 $N \\times N_s$ 快照矩阵进行经济奇异值分解（SVD）来构建POD基，\n- 对所有的 $q \\in \\{1,\\dots,Q\\}$，预计算仿射降阶算子 $A_q^r = V^\\top A_q V$，\n\n并推导每个参数查询的在线计算成本的表达式，用于：\n- 评估参数系数 $\\theta_q(\\mu)$，\n- 组装降阶算子 $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$，\n- 求解降阶系统，以及\n- 评估降阶解的一个线性泛函。\n\n在你的推导中仅使用以下基本事实：\n- 一个大小为 $m \\times k$ 和 $k \\times n$ 的稠密矩阵-矩阵乘积大约需要 $2 m k n$ 次浮点运算（flops）。\n- 对于一个仿射线性组合 $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q$，其中 $A_q \\in \\mathbb{R}^{N \\times N}$ 是稠密的，通过直接缩放和求和来构成 $A(\\mu)$ 的成本是（每个条目 $Q$ 次乘法 + $(Q-1)$ 次加法） $\\times N^2 = (2Q-1) N^2$ flops。\n- 对于一个对称正定矩阵 $M \\in \\mathbb{R}^{n \\times n}$，Cholesky分解大约需要 $(1/3) n^3$ flops，每次三角求解大约需要 $n^2$ flops。通过Cholesky求解 $M y = f$ 需要一次分解和两次三角求解，总计 $(1/3) n^3 + 2 n^2$ flops。\n- 对于一个稠密的 $N \\times N_s$ 矩阵（其中 $N \\ge N_s$）的经济奇异值分解（SVD），一个广泛使用的flop模型是大约 $4 N N_s^2 + 8 N_s^3$ flops。\n- 评估 $Q$ 个标量系数函数 $\\theta_q(\\mu)$ 的成本与 $Q$ 线性相关。假设每个系数的评估成本为 $C_\\theta = 1$ flop，则总成本为 $Q$ flops。\n- 评估一个标量线性泛函 $y = c^\\top x$（其中 $c \\in \\mathbb{R}^{n}$）的乘-加操作大约需要 $2 n$ flops。\n\n假设整个过程都使用稠密代数，并且对于所有考虑的 $\\mu$，$A(\\mu)$ 都是对称正定的。使用推导出的表达式，实现一个程序，对下面列出的每个测试用例，计算：\n- 离线总成本（以flops为单位），\n- $M_q$ 次查询的在线总成本（以flops为单位），\n- $M_q$ 次查询的全阶（非降阶）总成本（以flops为单位），\n- 定义为 $(\\text{全阶单次查询flops}) / (\\text{在线单次查询flops})$ 的单次查询加速比，\n- 收支平衡查询次数 $M_{\\text{break}} = \\left\\lceil \\frac{\\text{离线flops}}{\\text{全阶单次查询flops} - \\text{在线单次查询flops}} \\right\\rceil$，并约定如果分母为非正数，则报告-1。\n\n所有输出都必须以浮点运算（flops）表示，并且必须由您的程序进行数值计算。测试套件使用以下参数集 $(N, N_s, N_r, Q, M_q)$：\n- $(2000, 50, 30, 5, 100)$\n- $(100, 10, 1, 1, 1)$\n- $(1500, 80, 40, 20, 250)$\n- $(3000, 300, 60, 10, 50)$\n\n您的程序应该生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试用例按顺序贡献其自己的列表：$[\\text{离线}, \\text{在线\\_总计}, \\text{全阶\\_总计}, \\text{加速比}, \\text{收支平衡点}]$。最终输出格式必须是这些列表的单个列表，没有空格，例如 $[[x_1,y_1,z_1,s_1,b_1],[x_2,y_2,z_2,s_2,b_2],\\dots]$。不涉及角度。将收支平衡点表示为整数。所有其他量可以是浮点数。单位是flops。",
            "solution": "该问题陈述被评估为有效。它在科学上基于数值线性代数和模型降阶的原理，特别是降阶基方法。该问题是适定的、客观的、自洽的，并提供了一个清晰、可形式化的任务。\n\n在此，我们根据提供的基本运算计数，推导所述的降阶基（RB）方法的离线和在线阶段的计算成本。全阶模型（FOM）是一个参数化线性系统 $A(\\mu) x(\\mu) = b(\\mu)$，其中 $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q \\in \\mathbb{R}^{N \\times N}$ 是对称正定（SPD）的。降阶基的维度为 $N_r$，从 $N_s$ 个快照中导出。\n\n### 计算成本的推导\n\n我们假设所有矩阵和向量操作都涉及稠密代数，如指定。所有成本均以浮点运算次数（flops）衡量。\n\n#### I. 离线计算成本 ($C_{\\text{offline}}$)\n\n离线阶段包括三个主要任务：快照生成、基的构建和算子投影。\n\n1.  **生成 $N_s$ 个全阶快照**:\n    对于 $N_s$ 个训练参数中的每一个 $\\mu_i$，必须计算一个全阶解 $x(\\mu_i)$。这涉及到组装矩阵 $A(\\mu_i)$ 和求解线性系统 $A(\\mu_i) x(\\mu_i) = b(\\mu_i)$。\n    *   组装一个矩阵 $A(\\mu_i) \\in \\mathbb{R}^{N \\times N}$ 的成本：仿射组装涉及对 $N^2$ 个条目中的每一个进行 $Q$ 次缩放和 $Q-1$ 次加法，成本为 $(2Q-1)N^2$ flops。\n    *   使用Cholesky分解求解 $N \\times N$ SPD系统的成本：此成本被给出为 $(1/3)N^3 + 2N^2$ flops。\n    *   单个快照的成本是这两部分之和：$(2Q-1)N^2 + (1/3)N^3 + 2N^2 = (1/3)N^3 + (2Q+1)N^2$ flops。\n    *   $N_s$ 个快照的总成本 $C_{\\text{snapshots}}$:\n        $$C_{\\text{snapshots}} = N_s \\left( \\frac{1}{3}N^3 + (2Q+1)N^2 \\right)$$\n\n2.  **构建POD基**:\n    基是通过对 $N \\times N_s$ 快照矩阵进行经济奇异值分解（SVD）来构建的。此操作的成本 $C_{\\text{SVD}}$ 被直接给出。假设 $N \\geq N_s$:\n    $$C_{\\text{SVD}} = 4 N N_s^2 + 8 N_s^3$$\n    得到的基由标准正交矩阵 $V \\in \\mathbb{R}^{N \\times N_r}$ 表示，其中 $N_r$ 是保留的奇异模式的数量。\n\n3.  **仿射降阶算子的预计算**:\n    每个与参数无关的矩阵 $A_q \\in \\mathbb{R}^{N \\times N}$ 被投影到降阶基上，形成 $A_q^r = V^\\top A_q V \\in \\mathbb{R}^{N_r \\times N_r}$。为了效率，此计算分两步进行：\n    *   首先，计算中间乘积 $T_q = A_q V$。这是一个 $N \\times N$ 矩阵和一个 $N \\times N_r$ 矩阵的乘积。成本为 $2 \\cdot N \\cdot N \\cdot N_r = 2N^2 N_r$ flops。\n    *   其次，计算最终乘积 $A_q^r = V^\\top T_q$。这是一个 $N_r \\times N$ 矩阵 ($V^\\top$) 和一个 $N \\times N_r$ 矩阵 ($T_q$) 的乘积。成本为 $2 \\cdot N_r \\cdot N \\cdot N_r = 2N N_r^2$ flops。\n    *   形成一个降阶算子 $A_q^r$ 的成本是 $2N^2 N_r + 2N N_r^2$。\n    *   所有 $Q$ 个算子的总成本 $C_{\\text{proj}}$:\n        $$C_{\\text{proj}} = Q \\left( 2N^2 N_r + 2N N_r^2 \\right)$$\n\n离线总成本 $C_{\\text{offline}}$ 是这三个组成部分之和：\n$$C_{\\text{offline}} = C_{\\text{snapshots}} + C_{\\text{SVD}} + C_{\\text{proj}}$$\n$$C_{\\text{offline}} = N_s \\left( \\frac{1}{3}N^3 + (2Q+1)N^2 \\right) + \\left( 4 N N_s^2 + 8 N_s^3 \\right) + Q \\left( 2N^2 N_r + 2N N_r^2 \\right)$$\n\n#### II. 在线计算成本（单次查询）($C_{\\text{online, per-query}}$)\n\n在线阶段涉及为新参数 $\\mu$ 快速计算解。\n\n1.  **参数系数的评估**:\n    评估 $Q$ 个标量函数 $\\theta_q(\\mu)$。每个函数的成本为 $C_\\theta=1$ flop，总成本 $C_{\\text{coeffs}}$ 为：\n    $$C_{\\text{coeffs}} = Q$$\n\n2.  **降阶算子的组装**:\n    组装降阶算子 $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$。由于每个 $A_q^r$ 是一个 $N_r \\times N_r$ 矩阵，成本 $C_{\\text{assemble_r}}$ 由大小为 $N_r \\times N_r$ 矩阵的仿射组合规则给出：\n    $$C_{\\text{assemble_r}} = (2Q-1)N_r^2$$\n\n3.  **降阶系统的求解**:\n    求解 $N_r \\times N_r$ 降阶系统 $A^r(\\mu) x^r(\\mu) = b^r(\\mu)$。由于 $A(\\mu)$ 是SPD且 $V$ 具有满列秩，因此 $A^r(\\mu)=V^\\top A(\\mu) V$ 也是SPD。对一个 $N_r \\times N_r$ 系统使用Cholesky方法，成本 $C_{\\text{solve_r}}$ 为：\n    $$C_{\\text{solve_r}} = \\frac{1}{3}N_r^3 + 2N_r^2$$\n    （形成 $b^r(\\mu)$ 的成本假定可以忽略不计，这与问题关注系统矩阵运算的焦点一致）。\n\n4.  **线性泛函的评估**:\n    计算降阶状态 $x^r(\\mu) \\in \\mathbb{R}^{N_r}$ 的一个线性泛函。对于与 $\\mathbb{R}^{N_r}$ 中向量的点积，成本 $C_{\\text{output}}$ 为：\n    $$C_{\\text{output}} = 2N_r$$\n\n每次查询的在线总成本 $C_{\\text{online, per-query}}$ 是这些组成部分之和：\n$$C_{\\text{online, per-query}} = C_{\\text{coeffs}} + C_{\\text{assemble_r}} + C_{\\text{solve_r}} + C_{\\text{output}}$$\n$$C_{\\text{online, per-query}} = Q + (2Q-1)N_r^2 + \\left( \\frac{1}{3}N_r^3 + 2N_r^2 \\right) + 2N_r = \\frac{1}{3}N_r^3 + (2Q+1)N_r^2 + 2N_r + Q$$\n\n#### III. 全阶模型成本（单次查询）($C_{\\text{fom, per-query}}$)\n\n在不使用模型降阶的情况下，单次查询求解全阶模型的成本为：\n1.  **$A(\\mu)$的组装**：与快照计算中相同，为 $(2Q-1)N^2$ flops。\n2.  **FOM系统的求解**：与快照计算中相同，为 $(1/3)N^3 + 2N^2$ flops。\n3.  **线性泛函的评估**：状态 $x(\\mu)$ 在 $\\mathbb{R}^N$ 中，因此成本为 $2N$ flops。\n\n每次查询的FOM总成本：\n$$C_{\\text{fom, per-query}} = ((2Q-1)N^2) + \\left( \\frac{1}{3}N^3 + 2N^2 \\right) + 2N = \\frac{1}{3}N^3 + (2Q+1)N^2 + 2N$$\n\n#### IV. 派生量\n\n剩余的量计算如下：\n\n-   **$M_q$ 次查询的在线总成本**: $C_{\\text{online, total}} = M_q \\times C_{\\text{online, per-query}}$\n-   **$M_q$ 次查询的全阶总成本**: $C_{\\text{fom, total}} = M_q \\times C_{\\text{fom, per-query}}$\n-   **单次查询加速比 ($S$)**: $S = \\frac{C_{\\text{fom, per-query}}}{C_{\\text{online, per-query}}}$\n-   **收支平衡查询次数 ($M_{\\text{break}}$)**: 这是RB方法的总成本等于FOM总成本所需的查询次数。\n    $C_{\\text{offline}} + M_{\\text{break}} \\cdot C_{\\text{online, per-query}} = M_{\\text{break}} \\cdot C_{\\text{fom, per-query}}$\n    $$M_{\\text{break}} = \\left\\lceil \\frac{C_{\\text{offline}}}{C_{\\text{fom, per-query}} - C_{\\text{online, per-query}}} \\right\\rceil$$\n    如果分母为非正数，则 $M_{\\text{break}}$ 报告为-1。\n\n这些推导出的表达式将被实现，以计算给定测试用例所需的值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes computational costs for a reduced basis method and compares them\n    to a full-order model based on provided cost formulas.\n    \"\"\"\n    # Test cases are defined as tuples: (N, N_s, N_r, Q, M_q)\n    test_cases = [\n        (2000, 50, 30, 5, 100),\n        (100, 10, 1, 1, 1),\n        (1500, 80, 40, 20, 250),\n        (3000, 300, 60, 10, 50),\n    ]\n\n    results_list = []\n\n    for case in test_cases:\n        N, Ns, Nr, Q, Mq = case\n\n        # Convert to float to ensure float arithmetic throughout\n        N, Ns, Nr, Q, Mq = float(N), float(Ns), float(Nr), float(Q), float(Mq)\n\n        # 1. Offline Cost Calculation\n        # Cost of generating Ns snapshots\n        cost_snapshots = Ns * (1/3 * N**3 + (2*Q + 1) * N**2)\n        # Cost of SVD for POD basis construction\n        cost_svd = 4 * N * Ns**2 + 8 * Ns**3\n        # Cost of projecting operators\n        cost_proj = Q * (2 * N**2 * Nr + 2 * N * Nr**2)\n        \n        offline_cost = cost_snapshots + cost_svd + cost_proj\n\n        # 2. Online Cost (per query) Calculation\n        online_per_query_cost = (1/3 * Nr**3 + (2*Q + 1) * Nr**2 + 2 * Nr + Q)\n        \n        # 3. Full-Order Model (FOM) Cost (per query) Calculation\n        fom_per_query_cost = (1/3 * N**3 + (2*Q + 1) * N**2 + 2 * N)\n\n        # 4. Total costs for Mq queries\n        online_total_cost = Mq * online_per_query_cost\n        fom_total_cost = Mq * fom_per_query_cost\n\n        # 5. Speedup Ratio\n        # Handle case where online cost could be zero, though unlikely with given formulas\n        if online_per_query_cost > 0:\n            speedup_ratio = fom_per_query_cost / online_per_query_cost\n        else:\n            speedup_ratio = float('inf') # Or handle as appropriate\n\n        # 6. Break-Even Number of Queries\n        cost_difference = fom_per_query_cost - online_per_query_cost\n        if cost_difference > 0:\n            # Ceiling of the division, converted to integer\n            break_even_queries = int(np.ceil(offline_cost / cost_difference))\n        else:\n            # If online is not faster, break-even is not achieved\n            break_even_queries = -1\n        \n        results_list.append([\n            offline_cost,\n            online_total_cost,\n            fom_total_cost,\n            speedup_ratio,\n            break_even_queries\n        ])\n\n    # Format the final output string exactly as required, with no spaces\n    inner_parts = []\n    for res in results_list:\n        # res[4] is break_even_queries, which is already an integer\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\"\n        inner_parts.append(res_str)\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "降阶模型的预测精度很大程度上取决于其投影所依赖的基。本练习将让你亲手构建并比较两种主流的降阶基构建方法：本征正交分解 (POD) 和基于误差估计的贪心算法 。你将通过实践，深入理解“数据驱动”的POD方法与“误差驱动”的贪心方法之间的核心差异，并洞察在不同应用场景下的选择依据。",
            "id": "3945577",
            "problem": "考虑在长度为 $L$ 的隔膜中的一维电解质浓度场 $c_e(x;p)$，对于参数 $p = (D,k,j)$，该场由以下稳态线性边值问题建模\n$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(D\\,\\frac{\\mathrm{d} c_e}{\\mathrm{d}x}\\right) + k\\,c_e = s_j(x) \\quad \\text{for } x \\in (0,L), \\quad c_e(0;p) = 0, \\quad c_e(L;p) = 0,\n$$\n其中 $D$ 是电解质扩散系数，$k$ 是一级反应速率，$s_j(x) = j \\sin\\left(\\frac{\\pi x}{L}\\right)$ 是一个振幅为 $j$ 的体积源。假设 $D > 0$，$k > 0$，$j > 0$。使用 $L = 10^{-3}\\ \\text{m}$，该域通过间距为 $\\Delta x = \\frac{L}{N+1}$ 的均匀网格离散为 $N$ 个内部节点，并采用中心有限差分法，得到一个对称正定线性系统\n$$\nA(p)\\,u(p) = f(p),\n$$\n其中 $u(p) \\in \\mathbb{R}^N$ 表示在内部节点处近似 $c_e(x;p)$ 的离散浓度向量，$A(p) = D\\,T + k\\,I$，其中 $T \\in \\mathbb{R}^{N \\times N}$ 是标准三对角刚度矩阵\n$$\nT = \\frac{1}{\\Delta x^2}\n\\begin{bmatrix}\n2  -1  0  \\cdots  0 \\\\\n-1  2  -1  \\ddots  \\vdots \\\\\n0  -1  2  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1 \\\\\n0  \\cdots  0  -1  2\n\\end{bmatrix},\n$$\n且 $f(p) = j\\,\\mathbf{s}$，其中 $\\mathbf{s}_i = \\sin\\left(\\frac{\\pi x_i}{L}\\right)$，$x_i = i\\,\\Delta x$，$i = 1,\\ldots,N$。该模型允许仿射分解\n$$\nA(p) = \\theta_1(p)\\,A_1 + \\theta_2(p)\\,A_2, \\quad f(p) = \\theta_3(p)\\,f_1,\n$$\n其中 $\\theta_1(p) = D$，$\\theta_2(p) = k$，$\\theta_3(p) = j$，$A_1 = T$，$A_2 = I$，$f_1 = \\mathbf{s}$。\n\n令训练参数集 $\\mathcal{P}_{\\mathrm{train}}$ 为以下笛卡尔积\n$$\n\\mathcal{D} = \\{1.5\\times 10^{-10},\\, 3.0\\times 10^{-10},\\, 4.5\\times 10^{-10}\\}\\ \\text{m}^2/\\text{s}, \\quad\n\\mathcal{K} = \\{0.1,\\, 0.55,\\, 1.0\\}\\ \\text{s}^{-1}, \\quad\n\\mathcal{J} = \\{150,\\, 275,\\, 400\\}\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1}),\n$$\n因此 $\\mathcal{P}_{\\mathrm{train}} = \\mathcal{D} \\times \\mathcal{K} \\times \\mathcal{J}$。使用上述全阶模型，为所有 $p \\in \\mathcal{P}_{\\mathrm{train}}$ 计算快照 $u(p)$。\n\n根据这些训练快照，构建：\n- 一个固有正交分解 (POD) 基，其维度 $r_{\\mathrm{POD}}$ 被确定为能使快照矩阵的累积奇异值能量达到或超过阈值 $\\tau = 0.999$ 的最小 $r$。\n- 一个与 $r_{\\mathrm{POD}}$ 相同维度的贪婪降阶基 (RB)，使用以下针对2-范数的认证后验误差度量：\n$$\n\\eta(p;u_r) = \\frac{\\|r(p;u_r)\\|_2}{\\lambda_{\\min}(A(p))}, \\quad r(p;u_r) = f(p) - A(p)\\,u_r,\n$$\n其中 $\\lambda_{\\min}(A(p))$ 是对称正定矩阵 $A(p)$ 的最小特征值，$u_r$ 是通过Galerkin投影到当前RB空间上获得的降阶近似。用一个空基初始化贪婪算法。在每次贪婪迭代中，对所有 $p \\in \\mathcal{P}_{\\mathrm{train}}$，计算当前基下的 $u_r(p)$，评估 $\\eta(p;u_r)$，选择具有最大 $\\eta$ 的 $p^\\star$，并将正交归一化后的全阶解 $u(p^\\star)$ 附加到基中。继续此过程，直到基的维度达到 $r_{\\mathrm{POD}}$。\n\n定义样本外测试集 $\\mathcal{P}_{\\mathrm{test}}$ 为以下四个参数元组：\n$$\np_1 = (2.2\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.2\\ \\text{s}^{-1},\\ 300\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_2 = (4.2\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.9\\ \\text{s}^{-1},\\ 350\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_3 = (1.6\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.15\\ \\text{s}^{-1},\\ 120\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_4 = (4.4\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.95\\ \\text{s}^{-1},\\ 450\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})).\n$$\n\n对于每个 $p_i \\in \\mathcal{P}_{\\mathrm{test}}$，计算POD-Galerkin解 $u_{r,\\mathrm{POD}}(p_i)$ 和贪婪RB-Galerkin解 $u_{r,\\mathrm{RB}}(p_i)$。使用如上定义的认证度量 $\\eta(p_i;u_r)$，评估两个降阶解的界。每个界都以 $\\text{mol}/\\text{m}^3$ 为单位表示。您的程序必须输出一个单行，其中包含一个用方括号括起来的逗号分隔列表，其顺序为\n$$\n[\\eta_{\\mathrm{POD}}(p_1),\\ \\eta_{\\mathrm{RB}}(p_1),\\ \\eta_{\\mathrm{POD}}(p_2),\\ \\eta_{\\mathrm{RB}}(p_2),\\ \\eta_{\\mathrm{POD}}(p_3),\\ \\eta_{\\mathrm{RB}}(p_3),\\ \\eta_{\\mathrm{POD}}(p_4),\\ \\eta_{\\mathrm{RB}}(p_4)],\n$$\n每个条目都是一个浮点数。使用 $N = 200$ 个内部节点和 $L = 10^{-3}\\ \\text{m}$。源项 $\\sin\\left(\\frac{\\pi x}{L}\\right)$ 中的角度以弧度为单位。最终的单行输出必须严格遵守所述格式。",
            "solution": "用户提供了一个有效的问题陈述。它在科学上是合理的、适定的，并包含了完整解决方案所需的所有细节。我现在将提供合理的解答。\n\n该问题要求为参数化的一维稳态反应-扩散方程构建和比较两种降阶模型 (ROM)，这是一种电化学系统中常见的模型。全阶模型 (FOM) 是通过对控制偏微分方程 (PDE) 进行有限差分离散化得到的。求解过程分为几个不同阶段：FOM 设置、快照生成、通过两种方法（固有正交分解和贪婪算法）构建基，最后在一组样本外测试参数上评估得到的ROM。\n\n**1. 全阶模型 (FOM) 离散化**\n\n控制偏微分方程是一个线性的二阶边值问题：\n$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(D\\,\\frac{\\mathrm{d} c_e}{\\mathrm{d}x}\\right) + k\\,c_e = j \\sin\\left(\\frac{\\pi x}{L}\\right) \\quad \\text{for } x \\in (0,L),\n$$\n具有齐次Dirichlet边界条件 $c_e(0) = c_e(L) = 0$。参数是扩散系数 $D$、反应速率 $k$ 和源振幅 $j$。\n\n空间域 $(0, L)$ 使用 $N=200$ 个内部节点进行离散化，创建一个间距为 $\\Delta x = \\frac{L}{N+1}$ 的均匀网格。令 $x_i = i \\Delta x$（$i=1, \\dots, N$）为内部节点的坐标。这些节点上的浓度被组合成一个向量 $u(p) \\in \\mathbb{R}^N$，其中 $u_i(p) \\approx c_e(x_i; p)$。\n\n使用中心有限差分格式来计算二阶导数，我们在节点 $x_i$ 处用 $\\frac{-u_{i-1} + 2u_i - u_{i+1}}{\\Delta x^2}$ 近似 $-\\frac{\\mathrm{d}^2 c_e}{\\mathrm{d}x^2}$。结合边界条件 $u_0 = u_{N+1} = 0$，这导致了一个线性方程组 $A(p)u(p)=f(p)$，其中：\n- 系统矩阵为 $A(p) = D\\,T + k\\,I$。这里，$T \\in \\mathbb{R}^{N \\times N}$ 是由拉普拉斯算子产生的缩放刚度矩阵，其元素为 $T_{ii} = \\frac{2}{\\Delta x^2}$ 和 $T_{i,i\\pm 1} = -\\frac{1}{\\Delta x^2}$。$I$ 是单位矩阵。\n- 右侧向量为 $f(p) = j\\,\\mathbf{s}$，其中 $\\mathbf{s}_i = \\sin\\left(\\frac{\\pi x_i}{L}\\right)$。\n\n这种结构具有仿射参数可分离性，这对于高效的降阶基计算至关重要：\n$$\nA(p) = \\theta_1(p) A_1 + \\theta_2(p) A_2 = D\\,T + k\\,I,\n$$\n$$\nf(p) = \\theta_3(p) f_1 = j\\,\\mathbf{s}.\n$$\n\n**2. 快照生成**\n\n为了构建一个ROM，我们首先对FOM的行为进行采样。我们为一组代表性参数计算解，这些解被称为“快照”。训练集 $\\mathcal{P}_{\\mathrm{train}}$ 是给定 $D$、$k$ 和 $j$ 值的笛卡尔积，产生 $3 \\times 3 \\times 3 = 27$ 个参数元组。对于每个 $p \\in \\mathcal{P}_{\\mathrm{train}}$，我们组装 $A(p)$ 和 $f(p)$，并求解线性系统 $A(p)u(p)=f(p)$ 以获得快照向量 $u(p)$。这27个快照被收集为快照矩阵 $S \\in \\mathbb{R}^{N \\times 27}$ 的列。\n\n**3. 固有正交分解 (POD) 基**\n\nPOD提供了一个最优的正交基，用于在最小二乘意义上表示快照数据。该基是通过对快照矩阵 $S$ 进行奇异值分解 (SVD) 来计算的：\n$$\nS = U \\Sigma V^T.\n$$\n矩阵 $U \\in \\mathbb{R}^{N \\times 27}$ 的列是左奇异向量，它们构成了POD模态。$\\Sigma$ 对角线上的奇异值量化了每个模态的“能量”或重要性。为了选择维度为 $r_{\\mathrm{POD}}$ 的降阶基，我们找到累积平方奇异值能量满足阈值 $\\tau=0.999$ 的最小模态数：\n$$\nr_{\\mathrm{POD}} = \\min \\left\\{ r \\in \\{1,\\dots,27\\} \\left| \\frac{\\sum_{i=1}^r \\sigma_i^2}{\\sum_{i=1}^{27} \\sigma_i^2} \\ge \\tau \\right. \\right\\}.\n$$\n然后，POD基 $V_{\\mathrm{POD}} \\in \\mathbb{R}^{N \\times r_{\\mathrm{POD}}}$ 由 $U$ 的前 $r_{\\mathrm{POD}}$ 列构成。\n\n**4. 贪婪降阶基 (RB) 构建**\n\n贪婪算法通过在每一步中寻求最小化训练集上的最坏情况误差来迭代地构建基。这是由一个认证的后验误差估计器指导的。对于基空间 $V_r$ 中的给定降阶解 $u_r(p)$，误差界为：\n$$\n\\eta(p;u_r) = \\frac{\\|r(p;u_r)\\|_2}{\\lambda_{\\min}(A(p))},\n$$\n其中 $r(p;u_r) = f(p) - A(p)u_r$ 是残差。分母 $\\lambda_{\\min}(A(p))$ 是 $A(p)$ 的最小特征值，并作为问题矫顽常数的下界。对于SPD矩阵 $A(p) = D\\,T + k\\,I$，我们可以高效地计算 $\\lambda_{\\min}(A(p)) = D\\,\\lambda_{\\min}(T) + k$。未缩放的三对角矩阵 $\\text{tridiag}(-1, 2, -1)$ 的最小特征值是 $2 - 2 \\cos(\\frac{\\pi}{N+1})$，因此 $\\lambda_{\\min}(T) = \\frac{2}{\\Delta x^2}(1 - \\cos(\\frac{\\pi}{N+1}))$。\n\n算法按以下步骤进行（$k=1, \\dots, r_{\\mathrm{POD}}$）：\n1.  用空基 $V_0 = \\emptyset$ 初始化。\n2.  在第 $k$ 次迭代中，使用当前基 $V_{k-1}$，在训练集上找到使误差估计器最大化的参数 $p_k^\\star$：$p_k^\\star = \\arg\\max_{p \\in \\mathcal{P}_{\\mathrm{train}}} \\eta(p; u_{r,k-1}(p))$。\n3.  计算全阶解（快照）$u(p_k^\\star)$。\n4.  使用Gram-Schmidt过程将这个新的快照与现有的基向量 $V_{k-1}$ 进行正交归一化。\n5.  将得到的正交向量附加到基中，形成 $V_k$。重复此过程，直到基的维度达到 $r_{\\mathrm{POD}}$。\n\n**5. 在测试参数上的评估**\n\n构建好 $V_{\\mathrm{POD}}$ 和 $V_{\\mathrm{RB}}$ 两个基后，我们在样本外测试集 $\\mathcal{P}_{\\mathrm{test}}$ 上评估它们的性能。对于给定的测试参数 $p_i \\in \\mathcal{P}_{\\mathrm{test}}$ 和一个基 $V_r$，降阶解 $u_r(p_i)$ 通过Galerkin投影找到。这涉及到求解一个规模小得多的线性系统：\n$$\nA_r(p_i) u_{r, \\text{coeffs}} = f_r(p_i),\n$$\n其中 $A_r(p_i) = V_r^T A(p_i) V_r \\in \\mathbb{R}^{r \\times r}$ 且 $f_r(p_i) = V_r^T f(p_i) \\in \\mathbb{R}^r$。然后高维近似解恢复为 $u_r(p_i) = V_r u_{r, \\text{coeffs}}$。\n\n问题要求为四个测试参数中的每一个，计算POD解和贪婪RB解的误差界 $\\eta(p_i; u_r)$ 的值。这展示了误差估计器能够在不计算（昂贵的）全阶解的情况下，为新参数提供有保证的误差界。$\\eta$ 的单位是 $\\text{mol}/\\text{m}^3$，对应于浓度，因为残差向量分量的单位是 $\\text{mol}/(\\text{m}^3 \\cdot \\text{s})$，而 $\\lambda_{\\min}(A(p))$ 的单位是 $\\text{s}^{-1}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reduced-basis method problem.\n    \"\"\"\n    # 1. Define constants and discretization\n    L = 1e-3  # m\n    N = 200\n    tau = 0.999\n    dx = L / (N + 1)\n    x_nodes = np.arange(1, N + 1) * dx\n\n    # 2. Build Full-Order Model (FOM) affine components\n    # A1 = T (tridiagonal stiffness matrix)\n    diag_main = 2.0 * np.ones(N)\n    diag_off = -1.0 * np.ones(N - 1)\n    A1 = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / dx**2\n    \n    # A2 = I (identity matrix)\n    A2 = np.identity(N)\n    \n    # f1 = s (source vector)\n    f1 = np.sin(np.pi * x_nodes / L)\n\n    # 3. Define training parameter set and generate snapshots\n    D_train_vals = [1.5e-10, 3.0e-10, 4.5e-10]\n    K_train_vals = [0.1, 0.55, 1.0]\n    J_train_vals = [150, 275, 400]\n    \n    P_train = []\n    for d in D_train_vals:\n        for k in K_train_vals:\n            for j in J_train_vals:\n                P_train.append((d, k, j))\n\n    num_snapshots = len(P_train)\n    S = np.zeros((N, num_snapshots))\n    \n    for i, p in enumerate(P_train):\n        D, k, j = p\n        A = D * A1 + k * A2\n        f = j * f1\n        u = np.linalg.solve(A, f)\n        S[:, i] = u\n\n    # 4. Construct Proper Orthogonal Decomposition (POD) basis\n    U, s_vals, Vh = np.linalg.svd(S, full_matrices=False)\n    energy = np.cumsum(s_vals**2) / np.sum(s_vals**2)\n    r_pod = np.where(energy >= tau)[0][0] + 1\n    V_pod = U[:, :r_pod]\n\n    # 5. Construct Greedy Reduced Basis (RB)\n    lambda_min_T = (2 - 2 * np.cos(np.pi / (N + 1))) / dx**2\n    \n    basis_vectors = []\n    for i in range(r_pod):\n        max_eta = -1.0\n        p_star = None\n        \n        # Search for p_star maximizing the error estimator\n        for p in P_train:\n            D, k, j = p\n            A = D * A1 + k * A2\n            f = j * f1\n            \n            if not basis_vectors:  # First iteration, basis is empty\n                residual = f\n            else:\n                V = np.array(basis_vectors).T\n                Ar = V.T @ A @ V\n                fr = V.T @ f\n                try:\n                    u_r_coeffs = np.linalg.solve(Ar, fr)\n                    u_r = V @ u_r_coeffs\n                    residual = f - A @ u_r\n                except np.linalg.LinAlgError:\n                    residual = f # Failsafe if matrix is singular, just use FOM\n            \n            lambda_min_A = D * lambda_min_T + k\n            eta = np.linalg.norm(residual) / lambda_min_A\n            \n            if eta > max_eta:\n                max_eta = eta\n                p_star = p\n\n        # Found p_star, get FOM solution\n        D_star, k_star, j_star = p_star\n        A_star = D_star * A1 + k_star * A2\n        f_star = j_star * f1\n        u_star = np.linalg.solve(A_star, f_star)\n        \n        # Orthonormalize u_star against current basis (Modified Gram-Schmidt)\n        new_vec = u_star\n        for v in basis_vectors:\n            new_vec = new_vec - np.dot(new_vec, v) * v\n        \n        norm_v = np.linalg.norm(new_vec)\n        if norm_v > 1e-12: # Avoid division by zero\n            basis_vectors.append(new_vec / norm_v)\n\n    V_rb = np.array(basis_vectors).T\n\n    # 6. Evaluate error bounds on the test set\n    test_cases = [\n        (2.2e-10, 0.2, 300),\n        (4.2e-10, 0.9, 350),\n        (1.6e-10, 0.15, 120),\n        (4.4e-10, 0.95, 450),\n    ]\n\n    def evaluate_error_bound(p, V_basis, A1, A2, f1, lambda_min_T_val):\n        D, k, j = p\n        A = D * A1 + k * A2\n        f = j * f1\n        \n        # Galerkin projection\n        Ar = V_basis.T @ A @ V_basis\n        fr = V_basis.T @ f\n        u_r_coeffs = np.linalg.solve(Ar, fr)\n        u_r = V_basis @ u_r_coeffs\n        \n        # Residual and error bound\n        residual = f - A @ u_r\n        lambda_min_A = D * lambda_min_T_val + k\n        eta = np.linalg.norm(residual) / lambda_min_A\n        return eta\n\n    results = []\n    for p_test in test_cases:\n        eta_pod = evaluate_error_bound(p_test, V_pod, A1, A2, f1, lambda_min_T)\n        eta_rb = evaluate_error_bound(p_test, V_rb, A1, A2, f1, lambda_min_T)\n        results.extend([eta_pod, eta_rb])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多真实的电化学模型具有非线性特性，这对标准降阶基方法的在线效率构成了挑战。本练习将带你攻克这一关键难题，学习如何使用离散经验插值方法 (DEIM) 来高效处理非线性项 。你不仅将实现DEIM，还将从理论上推导并用数值方式验证该近似所引入的误差界，从而将降阶方法的能力从线性系统拓展至更广泛、更真实的非线性问题。",
            "id": "3945440",
            "problem": "考虑一个源于锂离子电池多孔电极理论的参数化半离散非线性电化学模型，该模型以适用于模型降阶的纯数学形式表示。设 $n \\in \\mathbb{N}$ 为高保真空间离散化的维数。对于每个参数值 $\\mu \\in \\mathbb{R}$，定义两个代表线性算子的对称正定矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 和 $M \\in \\mathbb{R}^{n \\times n}$，一个代表施加电流分布的强迫向量 $b(\\mu) \\in \\mathbb{R}^{n}$，以及一个模拟 Butler–Volmer 型反应动力学的非线性项 $N(u,\\mu) \\in \\mathbb{R}^{n}$。全阶稳态模型由残差\n$$\nF(u;\\mu) \\;=\\; \\big(K + \\mu M\\big) u \\;+\\; N(u,\\mu) \\;-\\; b(\\mu),\n$$\n给出，全阶解 $u^\\star(\\mu) \\in \\mathbb{R}^{n}$ 满足 $F(u^\\star(\\mu);\\mu) = 0$。\n\n假设非线性项是逐分量且光滑的，具体为\n$$\nN(u,\\mu) \\;=\\; \\alpha(\\mu) \\, \\sinh(u),\n$$\n其中 $\\sinh(\\cdot)$ 逐元素作用，$\\alpha(\\mu) > 0$ 是 $\\mu$ 的一个光滑标量函数。其关于 u 的雅可比矩阵为\n$$\nD_u N(u,\\mu) \\;=\\; \\alpha(\\mu) \\, \\operatorname{diag}\\big(\\cosh(u)\\big).\n$$\n\n设 $V_r \\in \\mathbb{R}^{n \\times r}$ 是一个标准正交降阶基 (RB) 矩阵，它由在参数值训练集上 $u^\\star(\\mu)$ 的快照构建而成，其中 $r \\ll n$。具有精确非线性的 RB (Galerkin) 降阶系统寻求 $a^\\star(\\mu) \\in \\mathbb{R}^{r}$，使得\n$$\nF_r(a;\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r \\, a \\;+\\; V_r^\\top N\\!\\left(V_r a,\\mu\\right) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0,\n$$\n成立，从而得到 RB 精确状态 $u_r^\\text{exact}(\\mu) = V_r a^\\star(\\mu)$。\n\n为了实现快速在线评估，使用经验插值法 (Empirical Interpolation Method, EIM) 来近似非线性项 $N(\\cdot,\\mu)$，其离散实现称为离散经验插值法 (Discrete Empirical Interpolation Method, DEIM)。设 $U_m \\in \\mathbb{R}^{n \\times m}$ 是一个非线性快照的标准正交基，其中 $m \\ll n$；并设 $P \\in \\{0,1\\}^{n \\times m}$ 是一个列选择矩阵，用于提取 $m$ 个经验性选择的插值索引。DEIM 近似为\n$$\n\\widetilde{N}(w,\\mu) \\;=\\; U_m \\big(P^\\top U_m\\big)^{-1} P^\\top N(w,\\mu),\n$$\n对任意 $w \\in \\mathbb{R}^{n}$ 成立。RB-DEIM 降阶系统寻求 $a^{\\text{deim}}(\\mu) \\in \\mathbb{R}^{r}$，使得\n$$\n\\widetilde{F}_r(a;\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r \\, a \\;+\\; V_r^\\top \\widetilde{N}\\!\\left(V_r a,\\mu\\right) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0,\n$$\n成立，从而得到 RB-DEIM 状态 $u_r^{\\text{deim}}(\\mu) = V_r a^{\\text{deim}}(\\mu)$。\n\n通过\n$$\ny(\\mu) \\;=\\; c^\\top u^\\star(\\mu),\n$$\n定义一个关注的标量输出 $y(\\mu) \\in \\mathbb{R}$，其中 $c \\in \\mathbb{R}^{n}$ 是一个固定向量（例如，一个模拟终端电压或平均状态的线性泛函）。在降阶模型中，RB 精确输出和 RB-DEIM 输出分别为 $y_r^\\text{exact}(\\mu) = c^\\top u_r^\\text{exact}(\\mu)$ 和 $y_r^\\text{deim}(\\mu) = c^\\top u_r^\\text{deim}(\\mu)$。\n\n任务：\n1. 从 $F_r(a;\\mu)$ 的 Newton 线性化出发，并假设 $V_r$ 具有标准正交列，推导一个一阶扰动界，该界量化了非线性项中的 DEIM 近似误差\n$$\ne_N(\\mu) \\;=\\; \\big\\| N\\!\\left(V_r a^\\star(\\mu),\\mu\\right) \\;-\\; \\widetilde{N}\\!\\left(V_r a^\\star(\\mu),\\mu\\right) \\big\\|_2\n$$\n如何传播到 RB 解系数误差 $\\big\\| a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\big\\|_2$、RB 状态误差 $\\big\\| u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu) \\big\\|_2$ 以及输出误差 $\\big| y_r^{\\text{deim}}(\\mu) - y_r^\\text{exact}(\\mu) \\big|$。用 $a^\\star(\\mu)$ 处的 RB 雅可比矩阵的逆和 $c$ 的范数来表示这些界。\n\n2. 实现一个完整的程序，该程序：\n   - 构建可复现的对称正定矩阵 $K$ 和 $M$ 以及向量 $b(\\mu)$ 和 $c$。\n   - 通过对一个参数训练集求解 $F(u;\\mu)=0$ 来生成全阶训练快照，然后通过对 $u^\\star(\\mu)$ 的快照矩阵进行本征正交分解 (Proper Orthogonal Decomposition, POD) 来构建 $V_r$。\n   - 通过对 $N\\big(u^\\star(\\mu),\\mu\\big)$ 的快照矩阵进行 POD 来构建非线性快照基 $U_m$，并通过主元 QR 分解计算 DEIM 索引以形成 $P$。\n   - 对于每个测试参数，通过 Newton 法求解 RB 精确系统和 RB-DEIM 系统，计算在 $u_r^\\text{exact}(\\mu)$ 处的 $e_N(\\mu)$，计算由 DEIM 引起的实际 RB 状态误差和输出误差，并评估任务 1 中推导的一阶扰动界。\n\n使用以下参数值测试套件来检验不同的非线性区域：\n- 正常路径：$\\mu = 0.20$（中等非线性）。\n- 弱非线性边界：$\\mu = 0.05$（小的 $\\alpha(\\mu)$）。\n- 强非线性区域：$\\mu = 1.50$（大的 $\\alpha(\\mu)$）。\n- 中间区域：$\\mu = 0.80$。\n\n所有量都是无量纲的，不需要物理单位。您的程序应生成单行输出，其中包含按上述顺序排列的每个测试用例的一个包含五个浮点数的列表\n$$\n\\big[ e_N(\\mu), \\; \\|u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu)\\|_2, \\; B_u(\\mu), \\; |y_r^{\\text{deim}}(\\mu) - y_r^\\text{exact}(\\mu)|, \\; B_y(\\mu) \\big],\n$$\n其中 $B_u(\\mu)$ 是 RB 状态误差的一阶界，$B_y(\\mu)$ 是输出误差的一阶界。将这四个测试用例的列表聚合成一个用方括号括起来的、无空格的、逗号分隔的列表，例如\n$$\n\\big[ [e_1,s_1,b_{u,1},o_1,b_{y,1}], [e_2,s_2,b_{u,2},o_2,b_{y,2}], [e_3,s_3,b_{u,3},o_3,b_{y,3}], [e_4,s_4,b_{u,4},o_4,b_{y,4}] \\big].\n$$\n您的程序必须精确地打印此聚合格式，数值计算到小数点后六位。",
            "solution": "该问题被认为是有效的，因为它在科学上基于参数化偏微分方程的模型降阶理论，是适定的、客观的，并包含足够的信息来构建一个唯一且有意义的解。\n\n### 任务 1：一阶扰动界的推导\n\n目标是推导由离散经验插值法 (DEIM) 引入到降阶基 (RB) 解中的误差的一阶界。我们分析近似非线性项的误差如何传播到 RB 系数向量、RB 状态向量和关注的输出中的误差。\n\n设 $a^\\star(\\mu) \\in \\mathbb{R}^r$ 为使用精确非线性的 RB 解的系数向量，满足 Galerkin 投影系统：\n$$\nF_r(a^\\star;\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r a^\\star \\;+\\; V_r^\\top N(V_r a^\\star,\\mu) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0.\n$$\n设 $a^{\\text{deim}}(\\mu) \\in \\mathbb{R}^r$ 为 RB-DEIM 解的系数向量，它满足：\n$$\n\\widetilde{F}_r(a^{\\text{deim}};\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r a^{\\text{deim}} \\;+\\; V_r^\\top \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0.\n$$\n非线性的 DEIM 近似由 $\\widetilde{N}(w,\\mu) = U_m (P^\\top U_m)^{-1} P^\\top N(w,\\mu)$ 给出。\n\n我们的分析始于在*精确* RB 系统 $F_r$ 中评估 RB-DEIM 解 $a^{\\text{deim}}$ 的残差。\n$$\nF_r(a^{\\text{deim}};\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r a^{\\text{deim}} \\;+\\; V_r^\\top N(V_r a^{\\text{deim}},\\mu) \\;-\\; V_r^\\top b(\\mu).\n$$\n根据 RB-DEIM 系统的定义，我们可以代入线性和强迫项：\n$$\nV_r^\\top \\big(K + \\mu M\\big) V_r a^{\\text{deim}} \\;-\\; V_r^\\top b(\\mu) = -V_r^\\top \\widetilde{N}(V_r a^{\\text{deim}},\\mu).\n$$\n将此代入 $F_r(a^{\\text{deim}};\\mu)$ 的表达式中可得：\n$$\nF_r(a^{\\text{deim}};\\mu) \\;=\\; V_r^\\top N(V_r a^{\\text{deim}},\\mu) \\;-\\; V_r^\\top \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\;=\\; V_r^\\top \\big[ N(V_r a^{\\text{deim}},\\mu) - \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\big].\n$$\n这表明 RB-DEIM 解在精确系统中的残差是 DEIM 近似误差的投影。\n\n接下来，我们在精确解系数 $a^\\star(\\mu)$ 周围对 $F_r(a;\\mu)$ 进行一阶泰勒展开：\n$$\nF_r(a^{\\text{deim}};\\mu) \\;\\approx\\; F_r(a^\\star;\\mu) \\;+\\; J_r(a^\\star(\\mu);\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right),\n$$\n其中 $J_r(a^\\star;\\mu)$ 是 $F_r$ 关于 $a$ 在 $a^\\star(\\mu)$ 处求值的雅可比矩阵。根据定义，$F_r(a^\\star;\\mu) = 0$。因此，我们有：\n$$\nF_r(a^{\\text{deim}};\\mu) \\;\\approx\\; J_r(a^\\star(\\mu);\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right).\n$$\n结合 $F_r(a^{\\text{deim}};\\mu)$ 的两个表达式，我们得到：\n$$\nJ_r(a^\\star;\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right) \\;\\approx\\; V_r^\\top \\big[ N(V_r a^{\\text{deim}},\\mu) - \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\big].\n$$\n为了获得一阶界，我们通过在精确 RB 解 $V_r a^\\star$ 而非 RB-DEIM 解 $V_r a^{\\text{deim}}$ 处评估右侧的误差项来进行近似。这样做的理由是假设 DEIM 近似引入了一个小扰动，使得 $a^{\\text{deim}} \\approx a^\\star$。\n$$\nJ_r(a^\\star;\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right) \\;\\approx\\; V_r^\\top \\big[ N(V_r a^{\\star},\\mu) - \\widetilde{N}(V_r a^{\\star},\\mu) \\big].\n$$\n求解系数误差 $\\delta a(\\mu) = a^{\\text{deim}}(\\mu) - a^\\star(\\mu)$，我们发现：\n$$\n\\delta a(\\mu) \\;\\approx\\; J_r(a^\\star(\\mu);\\mu)^{-1} V_r^\\top \\big[ N(V_r a^{\\star}(\\mu),\\mu) - \\widetilde{N}(V_r a^{\\star}(\\mu),\\mu) \\big].\n$$\n取欧几里得范数 $\\|\\cdot\\|_2$ 并应用矩阵范数的次乘法性质：\n$$\n\\|\\delta a(\\mu)\\|_2 \\;\\lesssim\\; \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\left\\| V_r^\\top \\right\\|_2 \\left\\| N(V_r a^{\\star}(\\mu),\\mu) - \\widetilde{N}(V_r a^{\\star}(\\mu),\\mu) \\right\\|_2.\n$$\n问题陈述 $V_r$ 具有标准正交列，因此 $V_r^\\top V_r = I_r$。$V_r^\\top$ 的诱导 2-范数是其最大奇异值，即 1。项 $\\left\\| N(V_r a^{\\star}(\\mu),\\mu) - \\widetilde{N}(V_r a^{\\star}(\\mu),\\mu) \\right\\|_2$ 被定义为 DEIM 非线性近似误差 $e_N(\\mu)$。\n这导出了 RB 系数误差的一阶界：\n$$\n\\left\\| a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right\\|_2 \\;\\lesssim\\; \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\, e_N(\\mu).\n$$\n雅可比矩阵 $J_r(a;\\mu)$ 由 $D_a F_r(a;\\mu) = V_r^\\top (K + \\mu M) V_r + V_r^\\top D_uN(V_ra,\\mu) V_r$ 给出。\n\n现在，我们推导 RB 状态误差 $\\delta u_r(\\mu) = u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu)$ 的界。\n$$\n\\delta u_r(\\mu) \\;=\\; V_r a^{\\text{deim}}(\\mu) - V_r a^\\star(\\mu) \\;=\\; V_r \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right) = V_r \\delta a(\\mu).\n$$\n取欧几里得范数：\n$$\n\\left\\| \\delta u_r(\\mu) \\right\\|_2 = \\left\\| V_r \\delta a(\\mu) \\right\\|_2.\n$$\n由于 $V_r$ 具有标准正交列，它在其值域内的向量上充当等距映射。因此，对于任意 $x \\in \\mathbb{R}^r$，都有 $\\left\\| V_r x \\right\\|_2 = \\left\\| x \\right\\|_2$。\n$$\n\\left\\| u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu) \\right\\|_2 = \\left\\| a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right\\|_2.\n$$\n因此，状态误差的一阶界 $B_u(\\mu)$ 与系数误差的界相同：\n$$\nB_u(\\mu) \\;=\\; \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\, e_N(\\mu).\n$$\n\n最后，我们推导输出误差 $\\delta y_r(\\mu) = y_r^{\\text{deim}}(\\mu) - y_r^\\text{exact}(\\mu)$ 的界。\n$$\n\\delta y_r(\\mu) \\;=\\; c^\\top u_r^{\\text{deim}}(\\mu) - c^\\top u_r^\\text{exact}(\\mu) \\;=\\; c^\\top \\left( u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu) \\right) = c^\\top \\delta u_r(\\mu).\n$$\n取绝对值并应用柯西-施瓦茨不等式：\n$$\n\\left| \\delta y_r(\\mu) \\right| \\;=\\; \\left| c^\\top \\delta u_r(\\mu) \\right| \\;\\le\\; \\left\\| c \\right\\|_2 \\left\\| \\delta u_r(\\mu) \\right\\|_2.\n$$\n代入状态误差范数的界，我们得到输出误差的一阶界 $B_y(\\mu)$：\n$$\nB_y(\\mu) \\;=\\; \\left\\| c \\right\\|_2 B_u(\\mu) \\;=\\; \\left\\| c \\right\\|_2 \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\, e_N(\\mu).\n$$\n\n### 任务 2：实现细节\n\n该实现构建了指定的数学对象和数值方法，以验证所推导的界。\n1.  **模型构建**：$K \\in \\mathbb{R}^{n \\times n}$ 取为大小为 $n$ 的网格上的一维负拉普拉斯算子 $(-1, 2, -1)$ 的二阶有限差分矩阵，它是对称正定 (SPD) 的。$M \\in \\mathbb{R}^{n \\times n}$ 是单位矩阵，也是 SPD。强迫向量 $b(\\mu)$ 和输出泛函向量 $c$ 被选为空间坐标的确定性函数。非线性缩放函数为 $\\alpha(\\mu) = 0.1 e^\\mu$。\n2.  **离线阶段**：\n    *   **快照生成**：定义一组训练参数 $\\mu_{\\text{train}}$。对于每个 $\\mu_i \\in \\mu_{\\text{train}}$，使用 Newton 法求解高保真系统 $F(u;\\mu_i)=0$ 得到 $u^\\star(\\mu_i)$。这些解构成了状态快照矩阵 $S_u$ 的列。相应的非线性项 $N(u^\\star(\\mu_i), \\mu_i)$ 构成了非线性快照矩阵 $S_N$。\n    *   **基生成**：状态基 $V_r$ 通过本征正交分解 (Proper Orthogonal Decomposition, POD) 计算得到，它对应于 $S_u$ 的前 $r$ 个左奇异向量。类似地，非线性基 $U_m$ 从 $S_N$ 的前 $m$ 个左奇异向量计算得到。\n    *   **DEIM 设置**：DEIM 插值索引使用标准贪心算法计算，该算法通过 $U_m^\\top$ 的主元 QR 分解高效实现。在线阶段所需的矩阵 $(P^\\top U_m)^{-1}$ 会被预先计算。\n3.  **在线阶段**：\n    *   对于每个测试参数 $\\mu$，使用单独的 Newton 求解器求解 RB 精确系统和 RB-DEIM 系统。这些求解器在小的 r 维系统上运行。\n    *   RB 精确解 $a^\\star(\\mu)$ 用于计算误差界所需的量：非线性误差 $e_N(\\mu)$ 和 RB 雅可比矩阵逆的范数 $\\|J_r(a^\\star(\\mu);\\mu)^{-1}\\|_2$。\n    *   计算出的 RB-DEIM 解 $a^{\\text{deim}}(\\mu)$ 用于求得真实状态误差 $\\|u_r^{\\text{deim}} - u_r^\\text{exact}\\|_2$ 和输出误差 $|y_r^{\\text{deim}} - y_r^\\text{exact}|$。\n    *   然后将这些真实误差与推导出的后验界 $B_u(\\mu)$ 和 $B_y(\\mu)$ 进行比较。结果按规定进行收集和格式化。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd, qr, inv, norm\n\ndef solve():\n    \"\"\"\n    Main function to perform the reduced-basis modeling and error analysis.\n    \"\"\"\n    # -- 0. Problem Setup --\n    n = 101  # Full-order dimension\n    r = 10   # Reduced basis dimension for state\n    m = 15   # Reduced basis dimension for nonlinearity\n    \n    test_cases = [0.20, 0.05, 1.50, 0.80]\n    \n    # Reproducible model construction\n    # K: 1D discrete Laplacian, SPD\n    K = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n    K *= (n - 1)**2 # Scaling for grid spacing h=1/(n-1)\n    # M: Mass matrix (identity)\n    M = np.identity(n)\n    # Spatial coordinate\n    x_coords = np.linspace(0, 1, n)\n    # Output vector c: Point evaluation at center\n    c = np.zeros(n)\n    c[n // 2] = 1.0\n    \n    def alpha_func(mu):\n        return 0.1 * np.exp(mu)\n\n    def b_func(mu, x):\n        return (1.0 + mu) * np.sin(np.pi * x) * 100\n\n    def N_func(u, mu):\n        return alpha_func(mu) * np.sinh(u)\n\n    def DN_func(u, mu):\n        return alpha_func(mu) * np.cosh(u)\n\n    def newton_solver(F, J, u0, tol=1e-10, max_iter=50):\n        u = u0.copy()\n        for _ in range(max_iter):\n            res = F(u)\n            jac = J(u)\n            delta_u = np.linalg.solve(jac, -res)\n            u += delta_u\n            if np.linalg.norm(delta_u)  tol:\n                return u\n        raise RuntimeError(\"Newton solver did not converge.\")\n\n    # -- 1. Offline Stage: Basis Generation --\n    \n    # Training parameters\n    train_mu = np.linspace(0.05, 1.5, 20)\n    \n    # Generate full-order snapshots\n    S_u = np.zeros((n, len(train_mu)))\n    S_N = np.zeros((n, len(train_mu)))\n    \n    u_fom_prev = np.zeros(n)\n    for i, mu_train in enumerate(train_mu):\n        b_train = b_func(mu_train, x_coords)\n        \n        def fom_residual(u):\n            return (K + mu_train * M) @ u + N_func(u, mu_train) - b_train\n        \n        def fom_jacobian(u):\n            return K + mu_train * M + np.diag(DN_func(u, mu_train))\n\n        u_star = newton_solver(fom_residual, fom_jacobian, u_fom_prev)\n        S_u[:, i] = u_star\n        S_N[:, i] = N_func(u_star, mu_train)\n        u_fom_prev = u_star\n\n    # POD for state basis Vr\n    U_svd_u, _, _ = svd(S_u, full_matrices=False)\n    V_r = U_svd_u[:, :r]\n\n    # POD for nonlinearity basis Um\n    U_svd_n, _, _ = svd(S_N, full_matrices=False)\n    U_m = U_svd_n[:, :m]\n    \n    # DEIM indices via pivoted QR\n    _, _, p_indices = qr(U_m.T, pivoting=True)\n    p = p_indices[:m]\n\n    # Precompute online-efficient matrices\n    K_r = V_r.T @ K @ V_r\n    M_r = V_r.T @ M @ V_r\n    inv_PT_Um = inv(U_m[p, :])\n    VrT_Um = V_r.T @ U_m\n    norm_c = norm(c)\n    Vr_p = V_r[p, :]\n\n    # -- 2. Online Stage: Solve for test cases --\n    all_results = []\n    a_prev = np.zeros(r)\n\n    for mu_test in test_cases:\n        b_test = b_func(mu_test, x_coords)\n        b_r = V_r.T @ b_test\n        alpha_test = alpha_func(mu_test)\n\n        # -- Solve RB-exact system --\n        def rb_exact_res(a):\n            u_r = V_r @ a\n            return (K_r + mu_test * M_r) @ a + V_r.T @ N_func(u_r, mu_test) - b_r\n\n        def rb_exact_jac(a):\n            u_r = V_r @ a\n            d_N = DN_func(u_r, mu_test)[:, np.newaxis]\n            return K_r + mu_test * M_r + V_r.T @ (d_N * V_r)\n        \n        a_star = newton_solver(rb_exact_res, rb_exact_jac, a_prev)\n        u_r_exact = V_r @ a_star\n        y_r_exact = c.T @ u_r_exact\n\n        # -- Solve RB-DEIM system --\n        def rb_deim_res(a):\n            u_r_p = (V_r @ a)[p]\n            N_approx_proj = VrT_Um @ inv_PT_Um @ (alpha_test * np.sinh(u_r_p))\n            return (K_r + mu_test * M_r) @ a + N_approx_proj - b_r\n        \n        def rb_deim_jac(a):\n            u_r_p = (V_r @ a)[p]\n            diag_term = np.diag(alpha_test * np.cosh(u_r_p))\n            return K_r + mu_test * M_r + VrT_Um @ inv_PT_Um @ diag_term @ Vr_p\n\n        a_deim = newton_solver(rb_deim_res, rb_deim_jac, a_star)\n        u_r_deim = V_r @ a_deim\n        y_r_deim = c.T @ u_r_deim\n        a_prev = a_deim\n\n        # -- Compute errors and bounds --\n        # Nonlinearity error e_N\n        N_at_ur_exact = N_func(u_r_exact, mu_test)\n        N_tilde_at_ur_exact = U_m @ inv_PT_Um @ N_at_ur_exact[p]\n        e_N = norm(N_at_ur_exact - N_tilde_at_ur_exact)\n\n        # Actual state and output errors\n        state_err = norm(u_r_deim - u_r_exact)\n        output_err = abs(y_r_deim - y_r_exact)\n\n        # Error bounds B_u, B_y\n        J_r_at_astar = rb_exact_jac(a_star)\n        inv_Jr_norm = norm(inv(J_r_at_astar), 2)\n        \n        B_u = inv_Jr_norm * e_N\n        B_y = norm_c * B_u\n        \n        case_results = [\n            round(e_N, 6),\n            round(state_err, 6),\n            round(B_u, 6),\n            round(output_err, 6),\n            round(B_y, 6)\n        ]\n        all_results.append(case_results)\n\n    # -- 3. Format and Print Output --\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}