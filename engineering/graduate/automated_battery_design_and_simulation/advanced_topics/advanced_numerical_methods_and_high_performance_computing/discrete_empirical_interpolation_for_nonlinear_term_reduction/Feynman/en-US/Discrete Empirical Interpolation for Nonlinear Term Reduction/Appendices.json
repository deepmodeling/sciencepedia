{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for employing model reduction techniques like the Discrete Empirical Interpolation Method (DEIM) is to drastically reduce computational costs in complex simulations. This exercise provides a concrete understanding of this advantage by guiding you through a first-principles complexity analysis of a nonlinear term common in battery models, such as the Butler–Volmer kinetics. By comparing the operational cost of a full evaluation versus a DEIM-based evaluation, you will quantify the theoretical speedup that makes large-scale simulation and design optimization feasible .",
            "id": "3907597",
            "problem": "Consider a reduced-order electrochemical model used in automated battery design and simulation. The model approximates the overpotential field across $n$ control volumes by a linear subspace with basis matrix $V \\in \\mathbb{R}^{n \\times r}$ and reduced coordinates $a \\in \\mathbb{R}^{r}$, so that the overpotential vector is $\\eta = V a$. The nonlinear current-density map $f:\\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is component-wise Butler–Volmer kinetics, with the $i$-th component given by\n$$\nf_i(\\eta_i) = i_{0,i} \\left( \\exp\\!\\left( \\frac{\\alpha_a F}{R T} \\eta_i \\right) - \\exp\\!\\left( -\\frac{\\alpha_c F}{R T} \\eta_i \\right) \\right),\n$$\nwhere $i_{0,i}$ is the exchange current density at the $i$-th control volume, $F$ is the Faraday constant, $R$ is the universal gas constant, $T$ is the absolute temperature, and $\\alpha_a$, $\\alpha_c$ are anodic and cathodic charge-transfer coefficients. Assume $V$ is dense. The cost model counts one multiplication or addition as $1$ operation and the evaluation of one exponential $\\exp(\\cdot)$ as $\\gamma$ operations, with $\\gamma > 1$. Precomputed constants such as $\\frac{\\alpha_a F}{R T}$ and $\\frac{\\alpha_c F}{R T}$ are available and do not incur runtime cost.\n\nYou will compare the computational complexity of evaluating the full nonlinear vector $f(V a)$ against a hyper-reduced evaluation using the Discrete Empirical Interpolation Method (DEIM). In DEIM, one has a basis $U \\in \\mathbb{R}^{n \\times m}$, a sampling matrix $P \\in \\mathbb{R}^{n \\times m}$ that selects $m$ component indices, and assumes $(P^\\top U)$ is invertible and its inverse is precomputed offline. The DEIM approximation forms $P^\\top \\eta = (P^\\top V) a$, evaluates $P^\\top f(\\eta)$ at those $m$ sampled components, and reconstructs an approximation to $f(\\eta)$ by $U (P^\\top U)^{-1} (P^\\top f(\\eta))$.\n\nStarting from first principles and the above physical and algebraic definitions, derive the exact symbolic expression for the speedup factor $S$ defined as the ratio of the total operation count for the full evaluation of $f(V a)$ over all $n$ control volumes to the total operation count of the DEIM hyper-reduced evaluation described above. Express $S$ in terms of $n$, $m$, $r$, and $\\gamma$. The final answer must be a single closed-form analytic expression. No rounding is required and no units should be included in the final answer.",
            "solution": "The problem requires the derivation of the speedup factor $S$, defined as the ratio of the computational cost of a full evaluation of a nonlinear function to the cost of a hyper-reduced evaluation using the Discrete Empirical Interpolation Method (DEIM). The costs are measured in floating-point operations (flops), where one addition or multiplication counts as $1$ flop, and one exponential function evaluation, $\\exp(\\cdot)$, counts as $\\gamma$ flops, with $\\gamma > 1$.\n\nLet $C_{full}$ be the total operation count for the full evaluation, and $C_{DEIM}$ be the total operation count for the DEIM-based evaluation. The speedup factor is then defined as $S = \\frac{C_{full}}{C_{DEIM}}$.\n\nFirst, we derive the cost of the full evaluation, $C_{full}$. This process involves two main steps:\n1.  Compute the full overpotential vector $\\eta = V a$.\n2.  Evaluate the nonlinear current-density vector $f(\\eta)$ using the computed vector $\\eta$.\n\nStep 1: Cost of computing $\\eta = V a$.\nThe basis matrix $V$ is of size $n \\times r$ and the reduced coordinates vector $a$ is of size $r \\times 1$. The resulting overpotential vector $\\eta$ is of size $n \\times 1$. The computation of each component $\\eta_i$ is given by the dot product $\\eta_i = \\sum_{j=1}^{r} V_{ij} a_j$. A dot product of two vectors of length $r$ requires $r$ multiplications and $r-1$ additions. According to the specified cost model, this amounts to a total of $r + (r-1) = 2r - 1$ operations. Since there are $n$ components in $\\eta$ and the problem states that $V$ is a dense matrix, the total cost for this matrix-vector multiplication is:\n$$C_1 = n(2r-1)$$\n\nStep 2: Cost of evaluating $f(\\eta)$.\nThe nonlinear function $f$ is evaluated component-wise for each of the $n$ control volumes. For each component $i$, the evaluation is given by the Butler-Volmer equation:\n$$f_i(\\eta_i) = i_{0,i} \\left( \\exp\\left( c_a \\eta_i \\right) - \\exp\\left( -c_c \\eta_i \\right) \\right)$$\nwhere the composite constants $c_a = \\frac{\\alpha_a F}{R T}$ and $c_c = \\frac{\\alpha_c F}{R T}$ are precomputed. The cost for evaluating one component $f_i(\\eta_i)$ is determined as follows:\n-   The term $c_a \\eta_i$ requires $1$ multiplication.\n-   The term $-c_c \\eta_i$ requires $1$ multiplication (multiplication by the negative constant $-c_c$).\n-   The evaluation of $\\exp(c_a \\eta_i)$ costs $\\gamma$ operations.\n-   The evaluation of $\\exp(-c_c \\eta_i)$ costs $\\gamma$ operations.\n-   The subtraction between the two exponential terms requires $1$ addition/subtraction operation.\n-   The final multiplication by the exchange current density $i_{0,i}$ requires $1$ multiplication.\nThe total cost per component is therefore $1 + 1 + \\gamma + \\gamma + 1 + 1 = 4 + 2\\gamma$ operations. Since there are $n$ components to evaluate, the total cost for this step is:\n$$C_2 = n(4 + 2\\gamma)$$\n\nThe total cost for the full evaluation is the sum of the costs of these two steps:\n$$C_{full} = C_1 + C_2 = n(2r-1) + n(4+2\\gamma) = n(2r - 1 + 4 + 2\\gamma) = n(2r + 3 + 2\\gamma)$$\n\nNext, we derive the cost of the DEIM hyper-reduced evaluation, $C_{DEIM}$. The problem describes this as reconstructing an approximation $\\tilde{f}(\\eta) = U (P^\\top U)^{-1} (P^\\top f(\\eta))$. For computational efficiency, this online evaluation proceeds in four steps:\n\nStep 1: Compute the overpotential values $\\eta_p$ at the $m$ interpolation points.\nThis is achieved by computing $\\eta_p = (P^\\top V)a$. The matrix $P^\\top$ is a sampling matrix that selects $m$ rows from $V$. The resulting matrix $P^\\top V$ has dimensions $m \\times r$. The product of this $m \\times r$ matrix with the $r \\times 1$ vector $a$ is a matrix-vector multiplication that requires $m(2r-1)$ operations.\n$$C_{DEIM,1} = m(2r-1)$$\n\nStep 2: Evaluate the nonlinear function $f$ at the $m$ interpolation points.\nUsing the $m$ values in the vector $\\eta_p$, we evaluate the corresponding components of the nonlinear function. The cost per component is $4 + 2\\gamma$, as determined previously. For $m$ components, the total cost is:\n$$C_{DEIM,2} = m(4+2\\gamma)$$\nThis step produces a vector $f_p \\in \\mathbb{R}^m$.\n\nStep 3: Compute the reduced coefficients of the DEIM approximation.\nThis is performed by calculating $c = (P^\\top U)^{-1} f_p$. The problem states that the inverse matrix $(P^\\top U)^{-1} \\in \\mathbb{R}^{m \\times m}$ is precomputed. The calculation is thus a product of this $m \\times m$ matrix with the $m \\times 1$ vector $f_p$. The cost for this matrix-vector multiplication is $m(2m-1)$ operations.\n$$C_{DEIM,3} = m(2m-1)$$\n\nStep 4: Reconstruct the full-dimensional approximation of the nonlinear vector.\nThe approximation is given by $\\tilde{f} = U c$. The basis matrix $U$ has dimensions $n \\times m$ and the coefficient vector $c$ has dimensions $m \\times 1$. The cost of this matrix-vector product is $n(2m-1)$ operations.\n$$C_{DEIM,4} = n(2m-1)$$\n\nThe total cost for the DEIM evaluation is the sum of the costs of these four steps:\n$$C_{DEIM} = C_{DEIM,1} + C_{DEIM,2} + C_{DEIM,3} + C_{DEIM,4}$$\n$$C_{DEIM} = m(2r-1) + m(4+2\\gamma) + m(2m-1) + n(2m-1)$$\nSimplifying the expression for $C_{DEIM}$ by grouping terms with factor $m$:\n$$C_{DEIM} = m((2r-1) + (4+2\\gamma) + (2m-1)) + n(2m-1)$$\n$$C_{DEIM} = m(2r - 1 + 4 + 2\\gamma + 2m - 1) + n(2m-1)$$\n$$C_{DEIM} = m(2m + 2r + 2\\gamma + 2) + n(2m-1)$$\n$$C_{DEIM} = 2m(m + r + \\gamma + 1) + n(2m-1)$$\n\nFinally, we compute the speedup factor $S$ as the ratio of $C_{full}$ to $C_{DEIM}$:\n$$S = \\frac{C_{full}}{C_{DEIM}} = \\frac{n(2r + 3 + 2\\gamma)}{n(2m-1) + 2m(m + r + \\gamma + 1)}$$\nThis is the final exact symbolic expression for the speedup factor.",
            "answer": "$$\\boxed{\\frac{n(2r + 2\\gamma + 3)}{n(2m - 1) + 2m(m + r + \\gamma + 1)}}$$"
        },
        {
            "introduction": "Having established the computational benefits, we now turn to a key mechanism of DEIM: the selection of interpolation points. This process is not random; it follows a greedy algorithm designed to capture the most important features of the nonlinear term with a minimal set of samples. This practice will have you execute this algorithm step-by-step, providing a clear, hands-on understanding of how the interpolation indices are determined and why the resulting subsystem is well-posed for reconstruction .",
            "id": "3907604",
            "problem": "In reduced-order modeling of nonlinear source terms in automated battery design and simulation, consider a nonlinear map that arises from electrochemical kinetics and transport, for example in the Butler–Volmer reaction term coupled to electrolyte transport. After obtaining a reduced basis of nonlinear snapshots, the Discrete Empirical Interpolation Method (DEIM) is applied to select interpolation indices for efficient online evaluation. Let the reduced basis matrix be given by\n$$\nU \\;=\\; \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n1 & 0\\\\\n1 & 1\\\\\n0 & 1\n\\end{bmatrix},\n$$\nwhere the columns of $U$ span the two-dimensional approximation subspace for the nonlinear term. Using the standard greedy DEIM selection procedure with the convention that ties are broken by choosing the smallest index, compute the DEIM interpolation indices $p_1$ and $p_2$. Define the sampling matrix $P \\in \\mathbb{R}^{3\\times 2}$ by $P = \\begin{bmatrix} e_{p_1} & e_{p_2} \\end{bmatrix}$, where $e_i$ is the $i$-th column of the identity matrix in $\\mathbb{R}^{3\\times 3}$. Verify that $P^\\top U$ is invertible by evaluating its determinant. Report the ordered pair $(p_1,p_2)$ as your final answer. No rounding is required and no units are needed.",
            "solution": "The problem requires the computation of the first two Discrete Empirical Interpolation Method (DEIM) indices, $p_1$ and $p_2$, for a given reduced basis matrix $U$. The DEIM algorithm is a greedy procedure for selecting a set of interpolation points. Let the columns of the reduced basis matrix $U \\in \\mathbb{R}^{n \\times m}$ be denoted by $u_1, u_2, \\dots, u_m$. In this problem, $n=3$ and $m=2$.\n\nThe given reduced basis matrix is:\n$$\nU \\;=\\; \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n1 & 0\\\\\n1 & 1\\\\\n0 & 1\n\\end{bmatrix}\n$$\nThe columns of $U$ are the basis vectors $u_1$ and $u_2$:\n$$\nu_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad u_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n$$\n\nThe DEIM algorithm proceeds as follows:\n\n**Step 1: Compute the first interpolation index $p_1$.**\nThe first index $p_1$ is chosen as the index of the component of the first basis vector, $u_1$, that has the largest absolute value.\n$$\np_1 = \\arg\\max_{i \\in \\{1, 2, 3\\}} |(u_1)_i|\n$$\nThe components of $u_1$ are $(u_1)_1 = \\frac{1}{\\sqrt{2}}$, $(u_1)_2 = \\frac{1}{\\sqrt{2}}$, and $(u_1)_3 = 0$. The absolute values are $|\\frac{1}{\\sqrt{2}}|$, $|\\frac{1}{\\sqrt{2}}|$, and $|0|$. The maximum absolute value is $\\frac{1}{\\sqrt{2}}$, which occurs at indices $i=1$ and $i=2$. According to the problem statement, ties are broken by choosing the smallest index. Therefore, we select:\n$$\np_1 = 1\n$$\n\n**Step 2: Compute the second interpolation index $p_2$.**\nThe second index $p_2$ is found by first computing the residual vector $r_2$. This residual is the error between the second basis vector $u_2$ and its projection onto the subspace spanned by the first basis vector $u_1$. The projection is constructed using the first interpolation point $p_1$.\n\nThe approximation of $u_2$ is of the form $c_1 u_1$. The coefficient $c_1$ is determined by enforcing that the approximation matches $u_2$ at the first interpolation point $p_1=1$:\n$$\n(u_2)_{p_1} = c_1 (u_1)_{p_1}\n$$\nSubstituting the values for $p_1=1$:\n$$\n(u_2)_1 = c_1 (u_1)_1 \\implies \\frac{1}{\\sqrt{2}}(0) = c_1 \\frac{1}{\\sqrt{2}}(1) \\implies 0 = c_1\n$$\nThe coefficient is $c_1 = 0$.\n\nThe residual vector $r_2$ is then computed as:\n$$\nr_2 = u_2 - c_1 u_1 = u_2 - 0 \\cdot u_1 = u_2\n$$\nSo, the residual vector is simply $r_2 = u_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\nThe second index $p_2$ is the index of the component of the residual vector $r_2$ with the largest absolute value:\n$$\np_2 = \\arg\\max_{i \\in \\{1, 2, 3\\}} |(r_2)_i|\n$$\nThe components of $r_2$ are $(r_2)_1 = 0$, $(r_2)_2 = \\frac{1}{\\sqrt{2}}$, and $(r_2)_3 = \\frac{1}{\\sqrt{2}}$. The absolute values are $|0|$, $|\\frac{1}{\\sqrt{2}}|$, and $|\\frac{1}{\\sqrt{2}}|$. The maximum absolute value is $\\frac{1}{\\sqrt{2}}$, which occurs at indices $i=2$ and $i=3$. Again, breaking the tie by choosing the smallest index, we get:\n$$\np_2 = 2\n$$\nThe DEIM interpolation indices are therefore the ordered pair $(p_1, p_2) = (1, 2)$.\n\n**Step 3: Verification of invertibility of $P^\\top U$.**\nThe problem requires verification that the matrix $P^\\top U$ is invertible. The sampling matrix $P$ is constructed from the standard basis vectors $e_i \\in \\mathbb{R}^3$ corresponding to the computed indices:\n$$\nP = \\begin{bmatrix} e_{p_1} & e_{p_2} \\end{bmatrix} = \\begin{bmatrix} e_1 & e_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n$$\nThe transpose of $P$ is:\n$$\nP^\\top = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\n$$\nNow, we compute the product $P^\\top U$:\n$$\nP^\\top U = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\left( \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 0\\\\ 1 & 1\\\\ 0 & 1 \\end{bmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\cdot 1 + 0 \\cdot 1 + 0 \\cdot 0 & 1 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 + 0 \\cdot 0 & 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 1 \\end{bmatrix}\n$$\n$$\nP^\\top U = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\n$$\nTo verify invertibility, we compute the determinant of this $2 \\times 2$ matrix:\n$$\n\\det(P^\\top U) = \\det\\left(\\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\right) = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 \\det\\left(\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\right)\n$$\n$$\n\\det(P^\\top U) = \\frac{1}{2} \\left( (1)(1) - (0)(1) \\right) = \\frac{1}{2}(1) = \\frac{1}{2}\n$$\nSince $\\det(P^\\top U) = \\frac{1}{2} \\neq 0$, the matrix $P^\\top U$ is invertible. This completes the verification.\n\nThe final answer is the ordered pair of DEIM interpolation indices $(p_1, p_2)$.",
            "answer": "$$\n\\boxed{(1, 2)}\n$$"
        },
        {
            "introduction": "Ultimately, DEIM is a tool used within larger numerical frameworks, particularly in solvers for nonlinear differential equations that arise in battery modeling. This final practice situates DEIM within an implicit time-integration scheme, where solving a nonlinear system is required at each step. You will derive the Jacobian for a DEIM-approximated residual, a critical skill for implementing and analyzing the convergence of reduced-order models that rely on Newton-based solvers .",
            "id": "3907582",
            "problem": "In automated battery design and simulation, reduced-order electrochemical models are used to accelerate design-space exploration. Consider a Petrov–Galerkin projection of a spatially discretized battery partial differential equation with mass matrix $M \\in \\mathbb{R}^{N \\times N}$, stiffness-like matrix $A \\in \\mathbb{R}^{N \\times N}$, left and right trial-test bases $W \\in \\mathbb{R}^{N \\times r}$ and $V \\in \\mathbb{R}^{N \\times r}$, and a nonlinear reaction source $f:\\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ modeling Butler–Volmer kinetics. An implicit backward-Euler time integrator over time step $\\Delta t > 0$ produces the reduced residual\n$$\nR(a) \\;=\\; W^\\top M V \\,\\frac{a - a^{\\ast}}{\\Delta t} \\;-\\; W^\\top A V\\, a \\;-\\; W^\\top \\,\\hat{f}(V a),\n$$\nwhere $a \\in \\mathbb{R}^{r}$ are reduced coordinates at the new time level and $a^{\\ast} \\in \\mathbb{R}^{r}$ is known from the previous time level. The Discrete Empirical Interpolation Method (DEIM) approximates the nonlinear term via a basis $U \\in \\mathbb{R}^{N \\times m}$ and a selection matrix $P \\in \\mathbb{R}^{N \\times m}$ as\n$$\n\\hat{f}(z) \\;=\\; U\\,(P^\\top U)^{-1}\\,P^\\top\\,f(z), \\quad z \\in \\mathbb{R}^{N},\n$$\nwith $m \\ll N$.\n\nStarting from first principles, specifically the backward-Euler definition and the first-order Taylor expansion for nonlinear mappings, derive the Gauss–Newton linearization of $R(a)$ about a current iterate $a_{k} \\in \\mathbb{R}^{r}$. Express the linearized system in the form $J(a_{k})\\,\\delta a \\approx -R(a_{k})$, and identify the Jacobian $J(a_{k})$ in terms of $M$, $A$, $W$, $V$, and the Jacobian of $f$ evaluated at $V a_{k}$. Then, incorporate the DEIM approximation into the Jacobian by differentiating $\\hat{f}(V a)$ consistently, and state explicitly where DEIM enters the linearization.\n\nProvide as your final answer a single analytic expression for the Gauss–Newton Jacobian under DEIM, $J_{\\mathrm{DEIM}}(a_{k})$, in closed form. No numerical evaluation is required, and no units are needed. The final answer must be a single expression only, without explanatory text or additional equations.",
            "solution": "The problem requires the derivation of the Jacobian matrix for the Gauss-Newton linearization of a given reduced-order model residual. The process begins from the first-order Taylor expansion of the residual function, which is the foundation of Newton's method for solving nonlinear systems of equations.\n\nLet the reduced residual be a function $R: \\mathbb{R}^r \\to \\mathbb{R}^r$. To solve the nonlinear system $R(a) = 0$, we employ an iterative Newton-Raphson scheme. Starting from an initial guess $a_k$, we seek a correction $\\delta a$ such that $a_{k+1} = a_k + \\delta a$ is a better approximation of the root. The first-order Taylor series expansion of $R(a)$ around $a_k$ is:\n$$\nR(a_k + \\delta a) \\approx R(a_k) + \\frac{\\partial R}{\\partial a}\\bigg|_{a=a_k} \\delta a\n$$\nSetting $R(a_k + \\delta a) = 0$ yields the linear system for the update step $\\delta a$:\n$$\nJ(a_k) \\delta a = -R(a_k)\n$$\nwhere $J(a_k) = \\frac{\\partial R}{\\partial a}\\big|_{a=a_k}$ is the Jacobian matrix of the residual function $R$ evaluated at the current iterate $a_k$. Our task is to find the analytical expression for this Jacobian.\n\nThe problem provides the explicit form of the reduced residual, incorporating the Discrete Empirical Interpolation Method (DEIM) for the nonlinear term. However, the prompt asks to first identify the Jacobian without DEIM, and then with DEIM. The full residual (without the DEIM approximation $\\hat{f}$) would be:\n$$\nR_{\\text{full}}(a) = W^\\top M V \\,\\frac{a - a^{\\ast}}{\\Delta t} - W^\\top A V\\, a - W^\\top f(V a)\n$$\nWe will first derive the Jacobian of this expression, $J_{\\text{full}}(a_k)$, and then derive the Jacobian for the residual given in the problem statement which uses $\\hat{f}$, which we will call $J_{\\mathrm{DEIM}}(a_k)$.\n\nThe Jacobian is the sum of the derivatives of each term in the residual with respect to the vector $a$.\nLet's analyze each term of $R(a)$:\n1.  The time-derivative term: $T_1(a) = W^\\top M V \\frac{a - a^{\\ast}}{\\Delta t} = \\frac{1}{\\Delta t}W^\\top M V a - \\frac{1}{\\Delta t}W^\\top M V a^{\\ast}$. This term is linear in $a$. Its derivative with respect to $a$ is the matrix multiplying $a$:\n    $$\n    \\frac{\\partial T_1}{\\partial a} = \\frac{1}{\\Delta t} W^\\top M V\n    $$\n2.  The stiffness term: $T_2(a) = - W^\\top A V a$. This term is also linear in $a$. Its derivative is:\n    $$\n    \\frac{\\partial T_2}{\\partial a} = - W^\\top A V\n    $$\n3.  The nonlinear source term. First, without DEIM: $T_{3, \\text{full}}(a) = - W^\\top f(V a)$. This is a composition of functions. We use the chain rule for vector calculus. Let $z(a) = V a$. The derivative of $z$ with respect to $a$ is the matrix $V$. The derivative of $f(z)$ with respect to $z$ is the Jacobian of $f$, denoted $J_f(z)$.\n    $$\n    \\frac{\\partial T_{3, \\text{full}}}{\\partial a} = - W^\\top \\left( \\frac{\\partial f(z)}{\\partial z} \\bigg|_{z=Va} \\right) \\left( \\frac{\\partial z}{\\partial a} \\right) = - W^\\top J_f(V a) V\n    $$\n    Here, $J_f(V a)$ denotes the Jacobian of the function $f$ evaluated at the state $V a$.\n\nCombining these results, the Jacobian of the full residual, evaluated at $a_k$, is:\n$$\nJ_{\\text{full}}(a_k) = \\frac{W^\\top M V}{\\Delta t} - W^\\top A V - W^\\top J_f(V a_k) V\n$$\nNow, we incorporate the DEIM approximation as specified in the problem. The residual is:\n$$\nR(a) = W^\\top M V \\,\\frac{a - a^{\\ast}}{\\Delta t} - W^\\top A V\\, a - W^\\top \\,\\hat{f}(V a)\n$$\nwhere $\\hat{f}(z) = U(P^\\top U)^{-1}P^\\top f(z)$. The first two terms are identical to the full case. We only need to find the derivative of the new nonlinear term, $T_{3, \\mathrm{DEIM}}(a) = - W^\\top \\hat{f}(V a)$.\nAgain, we use the chain rule:\n$$\n\\frac{\\partial T_{3, \\mathrm{DEIM}}}{\\partial a} = - W^\\top \\left( \\frac{\\partial \\hat{f}(z)}{\\partial z} \\bigg|_{z=Va} \\right) \\left( \\frac{\\partial (Va)}{\\partial a} \\right)\n$$\nThe core task is to find the Jacobian of the DEIM approximation $\\hat{f}(z)$. The matrix term $U(P^\\top U)^{-1}P^\\top$ is constant with respect to $z$. Let this matrix be denoted by $\\mathcal{P}_{DEIM} = U(P^\\top U)^{-1}P^\\top$. Then $\\hat{f}(z) = \\mathcal{P}_{DEIM}f(z)$. Applying the product rule for matrix calculus (which simplifies as one term is constant):\n$$\nJ_{\\hat{f}}(z) = \\frac{\\partial \\hat{f}}{\\partial z} = \\frac{\\partial}{\\partial z} (\\mathcal{P}_{DEIM} f(z)) = \\mathcal{P}_{DEIM} \\frac{\\partial f}{\\partial z} = \\mathcal{P}_{DEIM} J_f(z)\n$$\nSubstituting the expression for $\\mathcal{P}_{DEIM}$:\n$$\nJ_{\\hat{f}}(z) = U(P^\\top U)^{-1}P^\\top J_f(z)\n$$\nNow, we can complete the chain rule for the third term of the residual:\n$$\n\\frac{\\partial T_{3, \\mathrm{DEIM}}}{\\partial a} = - W^\\top J_{\\hat{f}}(V a) V = - W^\\top \\left( U(P^\\top U)^{-1}P^\\top J_f(V a) \\right) V\n$$\nThis derivation explicitly shows how the DEIM approximation enters the linearization. Instead of the full Jacobian of the nonlinear term, $J_f(Va)$, being projected onto the test space via $W^\\top(\\cdot)V$, the Jacobian $J_f(Va)$ is first approximated itself using the DEIM projection operator $\\mathcal{P}_{DEIM}$ before being projected onto the reduced space.\n\nFinally, we assemble the complete Jacobian for the residual with the DEIM approximation, $J_{\\mathrm{DEIM}}(a)$, by summing the derivatives of the three terms. Evaluating at the iterate $a_k$ gives the final expression for the Gauss-Newton Jacobian:\n$$\nJ_{\\mathrm{DEIM}}(a_k) = \\frac{\\partial T_1}{\\partial a} + \\frac{\\partial T_2}{\\partial a} + \\frac{\\partial T_{3, \\mathrm{DEIM}}}{\\partial a} \\bigg|_{a=a_k}\n$$\n$$\nJ_{\\mathrm{DEIM}}(a_k) = \\frac{W^\\top M V}{\\Delta t} - W^\\top A V - W^\\top U (P^\\top U)^{-1} P^\\top J_{f}(V a_{k}) V\n$$\nThis is the required closed-form expression for the Jacobian matrix.",
            "answer": "$$\n\\boxed{\\frac{W^\\top M V}{\\Delta t} - W^\\top A V - W^\\top U (P^\\top U)^{-1} P^\\top J_{f}(V a_{k}) V}\n$$"
        }
    ]
}