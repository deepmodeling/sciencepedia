{
    "hands_on_practices": [
        {
            "introduction": "This practice focuses on a core mechanical step in Galerkin projection: constructing a suitable basis. In model reduction, the notion of \"orthogonality\" is often defined by the physics of the system, encoded in a mass matrix $M$. This exercise  will guide you through the Gram-Schmidt process using this physically-weighted inner product, a fundamental skill for building reduced-order models from simulation data.",
            "id": "3915341",
            "problem": "In automated battery design and simulation, reduced-order models of electrolyte concentration dynamics are often derived by Galerkin projection of a high-dimensional semi-discrete system onto a low-dimensional subspace spanned by basis vectors. The Galerkin projection uses the mass matrix from the weak formulation to define the inner product and enforce orthonormality of the reduced basis. Consider a two-degree-of-freedom electrolyte diffusion state vector $x(t) \\in \\mathbb{R}^{2}$ representing spatially averaged concentration states at two finite volume nodes in a pouch cell. The semi-discrete weak form yields a symmetric positive definite (SPD) mass matrix $M \\in \\mathbb{R}^{2 \\times 2}$ such that the inner product is defined by $(u,v)_{M} = u^{\\top} M v$. Suppose that $M = \\mathrm{diag}(2,3)$ arises from mass-lumping of two control volumes with weights proportional to their volumes, and two linearly independent snapshot vectors $w_{1}, w_{2} \\in \\mathbb{R}^{2}$ are obtained from detailed simulation: $w_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $w_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nStarting from the definition of the $M$-inner product and the requirement of orthonormality for a basis $\\{v_{i}\\}$, construct an $M$-orthonormal basis $\\{v_{1}, v_{2}\\}$ from $\\{w_{1}, w_{2}\\}$ using the Gramâ€“Schmidt process with respect to $(u,v)_{M} = u^{\\top} M v$. Form the matrix $V = \\begin{pmatrix} v_{1}  v_{2} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$ and verify orthonormality by evaluating the Frobenius norm of the deviation from the identity, $\\|V^{\\top} M V - I\\|_{F}$, where $I$ is the $2 \\times 2$ identity matrix and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm.\n\nReport the single real number $\\|V^{\\top} M V - I\\|_{F}$ as your final answer. Express the value as a pure number without units. If any numerical approximation is necessary, round your answer to four significant figures.",
            "solution": "The problem requires the construction of an orthonormal basis from a given set of vectors with respect to a non-standard inner product defined by a mass matrix $M$. This process is fundamental in Galerkin projection methods for model order reduction. After constructing the basis, we must verify its orthonormality by computing a specific matrix norm.\n\nFirst, we establish the given quantities. The mass matrix is $M = \\mathrm{diag}(2,3) = \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix}$. This matrix is symmetric and positive definite, with eigenvalues $2$ and $3$, thus it can properly define an inner product. The inner product is given by $(u,v)_{M} = u^{\\top} M v$ for any vectors $u, v \\in \\mathbb{R}^{2}$. The initial set of linearly independent vectors is $\\{w_{1}, w_{2}\\}$, where $w_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $w_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nWe apply the Gram-Schmidt process to the set $\\{w_{1}, w_{2}\\}$ to obtain an $M$-orthonormal basis $\\{v_{1}, v_{2}\\}$.\n\nStep 1: Construct the first orthonormal vector $v_{1}$.\nLet the first unnormalized vector be $u_{1} = w_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe compute its norm with respect to the $M$-inner product, denoted as $\\|u_{1}\\|_{M}$.\n$$ \\|u_{1}\\|_{M}^{2} = (u_{1}, u_{1})_{M} = u_{1}^{\\top} M u_{1} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} = 2 \\cdot 1 + 3 \\cdot 1 = 5 $$\nThe norm is $\\|u_{1}\\|_{M} = \\sqrt{5}$.\nThe first orthonormal vector $v_{1}$ is obtained by normalizing $u_{1}$:\n$$ v_{1} = \\frac{u_{1}}{\\|u_{1}\\|_{M}} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\nStep 2: Construct the second orthonormal vector $v_{2}$.\nFirst, we find a vector $u_{2}$ that is $M$-orthogonal to $v_{1}$ by subtracting the projection of $w_{2}$ onto $v_{1}$ from $w_{2}$.\n$$ u_{2} = w_{2} - (w_{2}, v_{1})_{M} v_{1} $$\nWe compute the inner product $(w_{2}, v_{1})_{M}$:\n$$ (w_{2}, v_{1})_{M} = w_{2}^{\\top} M v_{1} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} = \\frac{2}{\\sqrt{5}} $$\nNow we can compute $u_{2}$:\n$$ u_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\left( \\frac{2}{\\sqrt{5}} \\right) \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{2}{5} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{2}{5} \\\\ 0 - \\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} $$\nNext, we normalize $u_{2}$ to obtain $v_{2}$. We compute the squared norm $\\|u_{2}\\|_{M}^{2}$:\n$$ \\|u_{2}\\|_{M}^{2} = (u_{2}, u_{2})_{M} = u_{2}^{\\top} M u_{2} = \\begin{pmatrix} \\frac{3}{5}  -\\frac{2}{5} \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} $$\n$$ = \\begin{pmatrix} \\frac{6}{5}  -\\frac{6}{5} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\left(\\frac{6}{5}\\right)\\left(\\frac{3}{5}\\right) + \\left(-\\frac{6}{5}\\right)\\left(-\\frac{2}{5}\\right) = \\frac{18}{25} + \\frac{12}{25} = \\frac{30}{25} = \\frac{6}{5} $$\nThe norm is $\\|u_{2}\\|_{M} = \\sqrt{\\frac{6}{5}}$.\nThe second orthonormal vector $v_{2}$ is:\n$$ v_{2} = \\frac{u_{2}}{\\|u_{2}\\|_{M}} = \\frac{1}{\\sqrt{\\frac{6}{5}}} \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\frac{\\sqrt{5}}{\\sqrt{6}} \\frac{1}{5} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}\\sqrt{6}} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{30}} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} $$\nThe resulting $M$-orthonormal basis is $\\{v_{1}, v_{2}\\}$ where $v_{1} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_{2} = \\frac{1}{\\sqrt{30}}\\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$.\n\nStep 3: Form the matrix $V$ and evaluate the specified norm.\nThe matrix $V$ is formed by using $v_1$ and $v_2$ as its columns:\n$$ V = \\begin{pmatrix} v_{1}  v_{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{5}}  \\frac{3}{\\sqrt{30}} \\\\ \\frac{1}{\\sqrt{5}}  -\\frac{2}{\\sqrt{30}} \\end{pmatrix} $$\nThe problem requires computing $\\|V^{\\top} M V - I\\|_{F}$. By the definition of an $M$-orthonormal basis, the matrix $V$ must satisfy the property $V^{\\top} M V = I$, where $I$ is the identity matrix. The elements of the matrix $V^{\\top} M V$ are given by $(V^{\\top} M V)_{ij} = v_i^{\\top} M v_j = (v_i, v_j)_M$. Since the basis $\\{v_1, v_2\\}$ is $M$-orthonormal, we have $(v_i, v_j)_M = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nLet's verify this property through explicit calculation:\n$$ V^{\\top} M V = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\end{pmatrix} M \\begin{pmatrix} v_1  v_2 \\end{pmatrix} = \\begin{pmatrix} v_1^{\\top} M v_1  v_1^{\\top} M v_2 \\\\ v_2^{\\top} M v_1  v_2^{\\top} M v_2 \\end{pmatrix} = \\begin{pmatrix} (v_1, v_1)_M  (v_1, v_2)_M \\\\ (v_2, v_1)_M  (v_2, v_2)_M \\end{pmatrix} $$\nFrom our previous calculations:\n$(v_1, v_1)_M = \\|v_1\\|_M^2 = 1$\n$(v_2, v_2)_M = \\|v_2\\|_M^2 = 1$\nAnd by construction, $v_1$ and $v_2$ are orthogonal:\n$(v_1, v_2)_M = v_1^{\\top} M v_2 = \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1  1 \\end{pmatrix} \\right) \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{30}} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{150}} \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -6 \\end{pmatrix} = \\frac{1}{\\sqrt{150}} (6-6) = 0$.\nSince the inner product is symmetric, $(v_2, v_1)_M = (v_1, v_2)_M = 0$.\n\nTherefore, the matrix product is:\n$$ V^{\\top} M V = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I $$\nWe are asked to compute $\\|V^{\\top} M V - I\\|_{F}$. Substituting the result:\n$$ V^{\\top} M V - I = I - I = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = O $$\nwhere $O$ is the $2 \\times 2$ zero matrix.\n\nThe Frobenius norm of a matrix $A$ is defined as $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |a_{ij}|^2}$. For the zero matrix, this is:\n$$ \\|O\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0 $$\nThe calculation is exact and does not require any numerical approximation. The result is precisely $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Moving beyond a generic basis, how do we construct one that is efficient and accurate for systems with varying parameters, a common scenario in battery design? This problem explores the logic behind the \"Greedy\" algorithm, a cornerstone of the reduced basis method. By critically analyzing different strategies , you will understand how to intelligently select simulation \"snapshots\" to build a powerful basis that captures system behavior across a wide operational range.",
            "id": "3915410",
            "problem": "Consider a parameterized, coercive linear variational model representative of lithium intercalation and ionic conduction in a battery electrode. Let $\\mathcal{P} \\subset \\mathbb{R}^p$ be a compact parameter domain encoding manufacturing and operating variability (for example, solid-phase diffusion coefficients and effective conductivities), and let $X$ be a finite-dimensional Hilbert space resulting from a convergent spatial discretization of the governing partial differential equations. For each parameter $\\mu \\in \\mathcal{P}$, the high-fidelity solution $u(\\mu) \\in X$ satisfies the variational problem: find $u(\\mu) \\in X$ such that\n$a(u(\\mu), v; \\mu) = \\ell(v; \\mu)$ for all $v \\in X$,\nwhere $a(\\cdot,\\cdot;\\mu)$ is a continuous, coercive bilinear form with coercivity constant $\\alpha(\\mu) \\ge \\alpha_{\\mathrm{lb}}(\\mu)  0$, and $\\ell(\\cdot;\\mu)$ is a continuous linear functional. Assume an affine parameter dependence such that $a(\\cdot,\\cdot;\\mu)$ and $\\ell(\\cdot;\\mu)$ admit accurate affine expansions over $\\mu$.\n\nFor reduced basis model reduction, let $X_r = \\mathrm{span}\\{\\phi_1,\\dots,\\phi_r\\} \\subset X$ be the reduced trial space. The Galerkin reduced solution $u_r(\\mu) \\in X_r$ is defined by\n$a(u_r(\\mu), v_r; \\mu) = \\ell(v_r; \\mu)$ for all $v_r \\in X_r$.\nDefine the residual functional $r(\\cdot;\\mu) \\in X'$ associated with $u_r(\\mu)$ by $r(v;\\mu) := \\ell(v;\\mu) - a(u_r(\\mu), v; \\mu)$ for all $v \\in X$. Suppose one has available a residual-based a posteriori error estimator that upper-bounds the energy-norm error using a coercivity lower bound, and that this estimator can be evaluated efficiently for many parameters via an offline-online decomposition leveraging the affine expansions.\n\nYou are given a finite training set $\\Theta_{\\mathrm{train}} \\subset \\mathcal{P}$. The goal is to construct a basis enrichment strategy that, at each iteration, selects a new parameter sample for which the current reduced basis performs worst over $\\Theta_{\\mathrm{train}}$, computes the corresponding high-fidelity snapshot, and augments the basis, until a prescribed accuracy target is reached or a budget on the reduced dimension is exhausted.\n\nWhich of the following outlines correctly describes a Greedy algorithm that selects new parameter samples for basis enrichment based on the maximal residual-based error estimator over the training set, while maintaining offline-online efficiency?\n\nA. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators and structures needed to evaluate the residual-based estimator quickly for any $\\mu$. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ by solving the reduced Galerkin problem and evaluate the residual-based estimator; select $\\mu^\\star$ as the parameter that maximizes the estimator over $\\Theta_{\\mathrm{train}}$; compute the high-fidelity solution $u(\\mu^\\star)$ and orthonormalize it against the current basis to enrich $X_r$; terminate when the maximal estimator over $\\Theta_{\\mathrm{train}}$ is below a tolerance or when $r$ reaches a budget.\n\nB. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, randomly pick a subset of $\\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ for these parameters, and select the parameter $\\mu^\\star$ that minimizes the residual-based estimator; enrich $X_r$ with $u_r(\\mu^\\star)$; stop when the average estimator over the subset falls below a tolerance.\n\nC. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, for every $\\mu \\in \\Theta_{\\mathrm{train}}$ compute the high-fidelity solution $u(\\mu)$ to evaluate an error proxy; select the parameter with minimal coercivity lower bound to stress stability; enrich $X_r$ with the reduced solution $u_r(\\mu)$ at that parameter; stop when the smallest coercivity lower bound exceeds a threshold.\n\nD. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ and evaluate the norm of the reduced solution; select $\\mu^\\star$ as the parameter with the largest reduced solution norm; compute $u_r(\\mu^\\star)$ and add it to $X_r$ without orthonormalization; terminate when the maximal reduced solution norm over $\\Theta_{\\mathrm{train}}$ stops increasing.",
            "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n- A parameterized, coercive linear variational model representative of a battery electrode.\n- A compact parameter domain $\\mathcal{P} \\subset \\mathbb{R}^p$.\n- A finite-dimensional Hilbert space $X$ obtained from spatial discretization.\n- For each parameter $\\mu \\in \\mathcal{P}$, a high-fidelity solution $u(\\mu) \\in X$ satisfying the variational problem: $a(u(\\mu), v; \\mu) = \\ell(v; \\mu)$ for all $v \\in X$.\n- The bilinear form $a(\\cdot,\\cdot;\\mu)$ is continuous and coercive, with a coercivity constant $\\alpha(\\mu) \\ge \\alpha_{\\mathrm{lb}}(\\mu)  0$.\n- The linear functional $\\ell(\\cdot;\\mu)$ is continuous.\n- Both $a(\\cdot,\\cdot;\\mu)$ and $\\ell(\\cdot;\\mu)$ admit accurate affine expansions with respect to the parameter $\\mu$.\n- A reduced trial space $X_r = \\mathrm{span}\\{\\phi_1,\\dots,\\phi_r\\} \\subset X$ is used for a reduced basis method.\n- The Galerkin reduced solution $u_r(\\mu) \\in X_r$ is defined by: $a(u_r(\\mu), v_r; \\mu) = \\ell(v_r; \\mu)$ for all $v_r \\in X_r$.\n- A residual functional is defined for a given reduced solution $u_r(\\mu)$ as $r(v;\\mu) := \\ell(v;\\mu) - a(u_r(\\mu), v; \\mu)$ for all $v \\in X$.\n- A residual-based a posteriori error estimator is available, which upper-bounds the energy-norm error. This estimator uses the coercivity lower bound $\\alpha_{\\mathrm{lb}}(\\mu)$ and can be evaluated efficiently due to an offline-online decomposition enabled by the affine expansions.\n- A finite training set $\\Theta_{\\mathrm{train}} \\subset \\mathcal{P}$ is provided.\n- The objective is to construct a basis enrichment strategy by iteratively selecting the parameter from $\\Theta_{\\mathrm{train}}$ for which the current reduced basis performs worst, computing the corresponding high-fidelity snapshot, and augmenting the basis. The process continues until a specified accuracy target or basis size budget is met.\n- The question asks for the correct description of a Greedy algorithm that accomplishes this while maintaining offline-online efficiency.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes a standard setup for the reduced basis method (RBM), a well-established technique for model order reduction of parameterized partial differential equations.\n\n- **Scientifically Grounded (Critical):** The formulation is entirely consistent with the mathematical theory of RBMs. The concepts of coercive bilinear forms, Galerkin projection, residuals, a posteriori error estimation (specifically, using the residual and coercivity constant, suggestive of the form $\\Delta_r(\\mu) \\propto \\|r(\\cdot;\\mu)\\|_{X'}/\\alpha_{\\mathrm{lb}}(\\mu)$), and affine parameter dependence are foundational to this field. The application to battery modeling is a common and scientifically valid use case. The problem is scientifically sound.\n- **Well-Posed:** The existence and uniqueness of both the high-fidelity solution $u(\\mu)$ and the reduced solution $u_r(\\mu)$ are guaranteed by the Lax-Milgram theorem, given the stated properties of the bilinear form $a(\\cdot,\\cdot;\\mu)$. The question asks for the description of a specific algorithm (the Greedy algorithm), which is a well-defined procedure in this context. The problem is well-posed.\n- **Objective (Critical):** The language is precise, formal, and free of ambiguity or subjectivity. It uses standard mathematical terminology. The problem is objective.\n\nThe problem statement does not violate any of the invalidity criteria. It is a complete, consistent, and formally sound description of a standard problem in computational science and engineering.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Derivation of the Correct Algorithm\n\nThe standard Greedy algorithm for reduced basis generation aims to build an accurate basis by iteratively correcting for the largest error. The key components are:\n1.  **Error Indication:** Since computing the true error $\\|u(\\mu) - u_r(\\mu)\\|$ is expensive (it requires $u(\\mu)$), a cheap and reliable *a posteriori* error estimator, $\\Delta_r(\\mu)$, is used as a proxy. The problem states such an estimator is available and can be computed efficiently (the \"online\" phase).\n2.  **Greedy Selection:** At each iteration $k$, with a basis of size $r$, the algorithm searches over the entire training set $\\Theta_{\\mathrm{train}}$ to find the parameter $\\mu^\\star$ that yields the maximum estimated error.\n    $$\n    \\mu^\\star = \\arg\\max_{\\mu \\in \\Theta_{\\mathrm{train}}} \\Delta_r(\\mu)\n    $$\n    This \"greedy\" choice identifies where the current reduced basis is performing the worst.\n3.  **Basis Enrichment:** The high-fidelity solution (\"snapshot\") $u(\\mu^\\star)$ is computed for this worst-case parameter. This snapshot contains the information most lacking in the current basis. The basis is then augmented with this new information. To ensure numerical stability and linear independence, the snapshot is typically orthonormalized (e.g., via Gram-Schmidt) against the existing basis vectors to form the next basis vector, $\\phi_{r+1}$. The new space becomes $X_{r+1} = \\mathrm{span}\\{X_r, \\phi_{r+1}\\}$.\n4.  **Offline-Online Strategy:** To make the greedy search efficient, computations that depend only on the basis functions are performed once \"offline\" after each enrichment. This includes computing the components of the reduced operators and the error estimator that arise from the affine expansion. The \"online\" stage then involves, for any given $\\mu$, rapidly assembling and solving the small $r \\times r$ reduced system and evaluating the error estimator.\n5.  **Termination:** The process is repeated until the maximum estimated error over the training set, $\\max_{\\mu \\in \\Theta_{\\mathrm{train}}} \\Delta_r(\\mu)$, falls below a prescribed tolerance $\\epsilon_{\\mathrm{tol}}$, or a maximum basis size $r_{\\mathrm{max}}$ is reached.\n\n### Option-by-Option Analysis\n\n**A. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators and structures needed to evaluate the residual-based estimator quickly for any $\\mu$. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ by solving the reduced Galerkin problem and evaluate the residual-based estimator; select $\\mu^\\star$ as the parameter that maximizes the estimator over $\\Theta_{\\mathrm{train}}$; compute the high-fidelity solution $u(\\mu^\\star)$ and orthonormalize it against the current basis to enrich $X_r$; terminate when the maximal estimator over $\\Theta_{\\mathrm{train}}$ is below a tolerance or when $r$ reaches a budget.**\n\nThis option correctly describes all the key steps of the standard Greedy algorithm:\n-   **Initialization:** Correct.\n-   **Offline-Online Decomposition:** Correctly described for efficient evaluation of the estimator.\n-   **Greedy Selection:** Correctly states that for each parameter in the training set, the reduced solution and estimator are found, and the parameter maximizing the estimator is selected.\n-   **Enrichment:** Correctly states that the *high-fidelity solution* $u(\\mu^\\star)$ is computed and then used to enrich the basis, with orthonormalization for stability.\n-   **Termination:** Correctly identifies the standard stopping criteria based on accuracy tolerance or computational budget.\n\n**Verdict:** **Correct**.\n\n**B. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, randomly pick a subset of $\\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ for these parameters, and select the parameter $\\mu^\\star$ that minimizes the residual-based estimator; enrich $X_r$ with $u_r(\\mu^\\star)$; stop when the average estimator over the subset falls below a tolerance.**\n\nThis option has several fundamental flaws:\n-   It selects the parameter that *minimizes* the estimator. The goal is to find the *largest* error to correct it.\n-   It enriches the basis with the *reduced solution* $u_r(\\mu^\\star)$. Since $u_r(\\mu^\\star)$ is already in the span of the current basis $X_r$, adding it to the basis does not enrich the space or increase its dimension.\n-   It uses a randomized subset, which is a feature of some variants, but the subsequent steps are incorrect.\n-   The stopping criterion is based on an average over a subset, which is not the standard criterion for ensuring certified accuracy over the whole parameter domain.\n\n**Verdict:** **Incorrect**.\n\n**C. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, for every $\\mu \\in \\Theta_{\\mathrm{train}}$ compute the high-fidelity solution $u(\\mu)$ to evaluate an error proxy; select the parameter with minimal coercivity lower bound to stress stability; enrich $X_r$ with the reduced solution $u_r(\\mu)$ at that parameter; stop when the smallest coercivity lower bound exceeds a threshold.**\n\nThis option is flawed for multiple reasons:\n-   It suggests computing the high-fidelity solution $u(\\mu)$ for *every* parameter in the training set at each iteration. This would be computationally prohibitive and defeats the purpose of model reduction, which is to *avoid* these expensive solves. The error estimator is specifically designed to be cheap.\n-   The selection criterion is to minimize the coercivity lower bound, which is not the greedy criterion. The greedy criterion is to maximize the error estimator, of which the coercivity is only one component.\n-   It enriches with the reduced solution $u_r(\\mu)$, which, as noted before, does not expand the basis space.\n\n**Verdict:** **Incorrect**.\n\n**D. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ and evaluate the norm of the reduced solution; select $\\mu^\\star$ as the parameter with the largest reduced solution norm; compute $u_r(\\mu^\\star)$ and add it to $X_r$ without orthonormalization; terminate when the maximal reduced solution norm over \\Theta_{\\mathrm{train}}$ stops increasing.**\n\nThis option is entirely incorrect:\n-   It uses the norm of the reduced solution, $\\|u_r(\\mu)\\|$, as the error proxy. This is not a reliable error indicator; a large solution norm does not imply a large error. The problem explicitly provides a residual-based estimator for this purpose.\n-   It enriches with the reduced solution $u_r(\\mu^\\star)$, which is a fatal flaw.\n-   It explicitly advocates *against* orthonormalization, which is poor numerical practice and leads to ill-conditioned systems.\n-   The stopping criterion is arbitrary and not related to the actual approximation error.\n\n**Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A powerful reduced-order model is not just accurate; it is also physically meaningful. A key challenge is that standard Galerkin projection does not inherently guarantee that physical constraints, such as the non-negativity of species concentration, are respected. This advanced problem  delves into structure-preserving model reduction, asking you to identify valid techniques that ensure the ROM's predictions remain within physical bounds, a crucial step for building trustworthy simulations.",
            "id": "3915427",
            "problem": "Consider a diffusion-dominated concentration dynamics for lithium in the solid phase of an electrode particle used in automated battery design and simulation. The concentration field $c(x,t)$ evolves on a bounded domain $\\Omega \\subset \\mathbb{R}$ with $x \\in [0,L]$ according to the conservation law and Fick's second law of diffusion:\n$$\n\\frac{\\partial c}{\\partial t}(x,t) \\;=\\; D \\frac{\\partial^2 c}{\\partial x^2}(x,t) \\;+\\; s(x,t,c),\n$$\nwith diffusion coefficient $D0$ and source term $s(x,t,c)$ representing intercalation fluxes mapped into the bulk dynamics via homogenization. The boundary conditions are Neumann type reflecting current-induced fluxes, for example $-D \\frac{\\partial c}{\\partial x}(0,t) = q_0(t)$ and $D \\frac{\\partial c}{\\partial x}(L,t) = q_L(t)$, where the fluxes $q_0(t)$ and $q_L(t)$ are bounded and consistent with mass conservation. Assume the initial condition satisfies $c(x,0) \\ge 0$ for all $x \\in \\Omega$, and the physics imposes $0 \\le c(x,t) \\le c_{\\max}$ for some $c_{\\max}  0$.\n\nA Reduced-Order Model (ROM) is constructed via Galerkin projection: choose spatial basis functions $\\{\\phi_i(x)\\}_{i=1}^r$ and approximate $c(x,t) \\approx \\sum_{i=1}^r z_i(t)\\,\\phi_i(x)$, with reduced coordinates $z(t) \\in \\mathbb{R}^r$. The Galerkin condition (orthogonality of the residual to the span of the basis) yields a reduced Ordinary Differential Equation (ODE)\n$M \\dot{z}(t) \\;=\\; f(z(t),u(t))$,\nwhere $M \\in \\mathbb{R}^{r \\times r}$ is the symmetric positive definite mass matrix with entries $M_{ij} = \\int_{\\Omega} \\phi_i(x)\\phi_j(x)\\,dx$, and $u(t)$ denotes exogenous inputs (for instance, current-dependent boundary fluxes). The mapping from reduced coordinates to the approximate concentration field is $x \\mapsto c(x;z) := \\sum_{i=1}^{r} z_i \\phi_i(x)$.\n\nDefine the set\n$$\n\\mathcal{S} \\;:=\\; \\Big\\{ z \\in \\mathbb{R}^r \\;\\Big|\\; 0 \\le c(x;z) \\le c_{\\max} \\;\\;\\text{for all}\\;\\; x \\in \\Omega \\Big\\},\n$$\nwhich encodes pointwise nonnegativity and upper boundedness of the concentration in the ROM. You are asked to analyze, using only first principles such as conservation laws, the parabolic maximum principle for the Partial Differential Equation (PDE), and positive invariance for dynamical systems, which approaches to ROM construction and enforcement make $\\mathcal{S}$ positively invariant for the reduced dynamics. Positive invariance means that if $z(0) \\in \\mathcal{S}$ then $z(t) \\in \\mathcal{S}$ for all $t \\ge 0$.\n\nWhich of the following statements provide correct sufficient conditions or constructions under which the ROM concentration states remain in $\\mathcal{S}$ for all time, assuming the full-order model preserves $0 \\le c(x,t) \\le c_{\\max}$?\n\nA. Choose basis functions that are nonnegative everywhere, i.e., $\\phi_i(x) \\ge 0$ for all $x \\in \\Omega$ and $i=1,\\dots,r$, and include the constant function in the span. Augment the reduced dynamics with a barrier feedback term\n$u_{\\text{bar}}(z) \\;=\\; -\\gamma \\,\\nabla_z B(z), \\quad B(z) \\;=\\; \\int_{\\Omega} \\left[ -\\ln\\big(c(x;z)\\big) \\;-\\; \\ln\\big(c_{\\max} - c(x;z)\\big) \\right] dx$,\nwith gain $\\gamma0$. Provided $f(z,u)$ is locally Lipschitz and the input-dependent boundary fluxes are bounded, the set $\\mathcal{S}$ is positively invariant because $B(z)$ is proper and diverges on $\\partial \\mathcal{S}$, and the closed-loop vector field points strictly inward on $\\partial \\mathcal{S}$.\n\nB. Use standard Proper Orthogonal Decomposition (POD) bases that are $L^2(\\Omega)$-orthonormal, take $M=I$, and initialize with $z_i(0) \\ge 0$ for all $i$. Then the ROM preserves $c(x;z(t)) \\ge 0$ pointwise for all $t \\ge 0$ because nonnegative coefficients guarantee nonnegative superpositions in any orthonormal basis.\n\nC. Impose the reduced operator $A$ obtained from diffusion to be an $M$-matrix (Metzler with nonpositive off-diagonals and inverse nonnegative) in the coordinates $z$, and enforce $z(t) \\ge 0$ componentwise via projection if needed. This ensures positivity of the field $c(x;z(t))$ pointwise, regardless of the signs of the basis functions.\n\nD. Apply a logarithmic change of variables $y(x,t) := \\ln(c(x,t)/c_{\\mathrm{ref}})$ with a fixed reference $c_{\\mathrm{ref}}  0$, derive the transformed PDE via the chain rule and conservation laws, and perform Galerkin projection on $y(x,t) \\approx \\sum_{i=1}^r y_i(t)\\,\\psi_i(x)$. In the ROM, recover $c(x; y) = c_{\\mathrm{ref}} \\exp(\\sum_i y_i \\psi_i(x))$. Under smoothness and bounded-input assumptions, this guarantees $c(x;y(t))  0$ for all $x$ and $t$ without additional constraints, and upper bounds can be handled similarly by transforming $c_{\\max} - c$.\n\nE. Clip the reduced coordinates in time by saturating each $z_i(t)$ to the interval $[0, z_{i,\\max}]$ for some chosen bounds $z_{i,\\max}$, so that $0 \\le z_i(t) \\le z_{i,\\max}$ for all $t$. Since $c(x;z)$ is linear in $z$, such coefficient saturation ensures $0 \\le c(x;z(t)) \\le c_{\\max}$ pointwise provided the bounds are chosen sufficiently small.\n\nSelect all statements that are correct.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Extract Givens\n- **Governing PDE:** $\\frac{\\partial c}{\\partial t}(x,t) = D \\frac{\\partial^2 c}{\\partial x^2}(x,t) + s(x,t,c)$ for $x \\in \\Omega = [0,L]$, with $D0$.\n- **Boundary Conditions (BCs):** Neumann type, $-D \\frac{\\partial c}{\\partial x}(0,t) = q_0(t)$ and $D \\frac{\\partial c}{\\partial x}(L,t) = q_L(t)$, with bounded fluxes.\n- **Initial Condition (IC):** $c(x,0) \\ge 0$ for all $x \\in \\Omega$.\n- **Physical Bounds:** The full-order model (FOM) is assumed to preserve $0 \\le c(x,t) \\le c_{\\max}$.\n- **Reduced-Order Model (ROM):**\n    - Approximation: $c(x,t) \\approx \\sum_{i=1}^r z_i(t)\\,\\phi_i(x) =: c(x;z(t))$.\n    - Reduced ODE: $M \\dot{z}(t) = f(z(t),u(t))$.\n    - Mass Matrix: $M_{ij} = \\int_{\\Omega} \\phi_i(x)\\phi_j(x)\\,dx$, symmetric and positive definite.\n- **Set of Admissible States:** $\\mathcal{S} = \\big\\{ z \\in \\mathbb{R}^r \\mid 0 \\le c(x;z) \\le c_{\\max} \\text{ for all } x \\in \\Omega \\big\\}$.\n- **Objective:** Identify methods that make $\\mathcal{S}$ positively invariant, meaning if $z(0) \\in \\mathcal{S}$, then $z(t) \\in \\mathcal{S}$ for all $t \\ge 0$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem describes a standard parabolic PDE for diffusion, a fundamental process in battery science. Galerkin projection is a canonical method for model reduction. The issue of preserving physical invariants (like positivity) in reduced models is a critical and well-researched topic in scientific computing. The problem is scientifically sound.\n- **Well-Posedness:** The PDE with given BCs is a well-posed problem. The question about ensuring positive invariance for the derived ODE system is a clear and meaningful question in control and systems theory.\n- **Objectivity  Completeness:** The problem is stated in precise mathematical terms. It provides all necessary definitions (PDE, ROM structure, the set $\\mathcal{S}$) to analyze the proposed methods. The assumption that the full model preserves the bounds is a crucial and clearly stated premise.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-formulated question concerning structure-preserving model order reduction for a physically-motivated system. I will proceed to analyze each option.\n\nThe core challenge is that a standard Galerkin projection does not, in general, preserve the structure of the original PDE. The maximum principle of the parabolic PDE, which guarantees $0 \\le c(x,t) \\le c_{\\max}$, is lost upon projection onto a finite-dimensional subspace. The analysis must focus on whether each proposed method correctly re-establishes this property for the ROM.\n\n### Option-by-Option Analysis\n\n**A. Choose basis functions that are nonnegative everywhere... and augment the reduced dynamics with a barrier feedback term...**\n\nThis option proposes a control-theoretic approach using a logarithmic barrier function $B(z)$. The function is defined as\n$$ B(z) \\;=\\; \\int_{\\Omega} \\left[ -\\ln\\big(c(x;z)\\big) \\;-\\; \\ln\\big(c_{\\max} - c(x;z)\\big) \\right] dx. $$\nThis function is smooth in the interior of the set $\\mathcal{S}$ and diverges to $+\\infty$ as $z$ approaches the boundary $\\partial \\mathcal{S}$, where $c(x;z)$ touches either $0$ or $c_{\\max}$ for some $x$. This makes $B(z)$ a valid barrier function for the set $\\mathcal{S}$.\nThe proposed feedback is $u_{\\text{bar}}(z) = -\\gamma \\nabla_z B(z)$, which modifies the dynamics to be $\\dot{z} = M^{-1}f(z,u) - \\gamma M^{-1} \\nabla_z B(z)$.\nTo check for positive invariance, we can analyze the time derivative of $B(z)$ along the system's trajectories.\n$$ \\frac{d}{dt} B(z(t)) = (\\nabla_z B(z))^T \\dot{z} = (\\nabla_z B(z))^T \\left[ M^{-1}f(z,u) - \\gamma M^{-1} \\nabla_z B(z) \\right] $$\n$$ \\frac{d}{dt} B(z(t)) = (\\nabla_z B(z))^T M^{-1}f(z,u) - \\gamma (\\nabla_z B(z))^T M^{-1} \\nabla_z B(z) $$\nSince $M$ is positive definite, so is $M^{-1}$. The term $(\\nabla_z B(z))^T M^{-1} \\nabla_z B(z)$ defines a squared norm and is thus non-negative. As $z \\to \\partial \\mathcal{S}$, $\\|\\nabla_z B(z)\\|$ grows without bound. Since $f(z,u)$ is assumed locally Lipschitz and inputs are bounded, its growth will be outpaced by the quadratic term $-\\gamma \\|\\nabla_z B(z)\\|_{M^{-1}}^2$. For any trajectory approaching the boundary, $\\frac{d}{dt}B(z(t))$ will become negative and large, indicating that the value of $B(z)$ is decreasing. Since $B(z)$ increases towards the boundary, this means the trajectory is being repelled from the boundary. This is a standard result in the theory of barrier functions for rendering a set positively invariant. The conditions listed are sufficient. The use of non-negative basis functions is helpful but not strictly necessary for the barrier function methodology itself to work.\n\n**Verdict:** Correct.\n\n**B. Use standard Proper Orthogonal Decomposition (POD) bases... take $M=I$, and initialize with $z_i(0) \\ge 0$ for all $i$. Then the ROM preserves $c(x;z(t)) \\ge 0$ pointwise...**\n\nThis statement contains a critical flaw. The approximation is $c(x;z) = \\sum_{i=1}^r z_i \\phi_i(x)$. The positivity of $c(x;z)$ depends on the signs of both the coefficients $z_i$ and the basis functions $\\phi_i(x)$. Standard POD bases, which are the eigenfunctions of the snapshot correlation matrix, are $L^2$-orthogonal. For the basis functions to be orthogonal, they must change sign. Typically, only the first POD mode, $\\phi_1(x)$, which captures the most energy (variance), is of a single sign (e.g., everywhere non-negative). Higher-order modes $\\phi_i(x)$ for $i \\ge 2$ must be orthogonal to $\\phi_1(x)$, and thus must have both positive and negative regions.\nTherefore, even if all coefficients $z_i(t)$ were non-negative, the sum $\\sum z_i(t)\\phi_i(x)$ could easily become negative for some $x$ due to the contributions from $\\phi_i(x)$ where $i \\ge 2$. Furthermore, a standard Galerkin system $\\dot{z} = f(z,u)$ does not intrinsically guarantee that non-negative initial coefficients $z(0) \\ge 0$ will remain non-negative for all time. The claim made in the option is therefore fundamentally incorrect.\n\n**Verdict:** Incorrect.\n\n**C. Impose the reduced operator $A$ obtained from diffusion to be an $M$-matrix... and enforce $z(t) \\ge 0$ componentwise... This ensures positivity of the field $c(x;z(t))$ pointwise, regardless of the signs of the basis functions.**\n\nThis option suffers from the same fundamental flaw as option B. The first part of the statement discusses ensuring that the reduced state vector $z(t)$ has non-negative components, i.e., $z_i(t) \\ge 0$ for all $i$. A system matrix being an $M$-matrix (or Metzler for the continuous-time dynamics) is a condition for the non-negativity of the state vector to be an invariant property (for non-negative inputs). The statement correctly identifies a valid method for ensuring $z(t) \\ge 0$.\nHowever, the conclusion is incorrect. It claims that $z_i(t) \\ge 0$ for all $i$ is sufficient to ensure $c(x;z(t)) = \\sum_i z_i(t) \\phi_i(x) \\ge 0$ \"regardless of the signs of the basis functions.\" As explained for option B, if any of the basis functions $\\phi_i(x)$ take on negative values, a linear combination with positive coefficients $z_i(t)$ is not guaranteed to be non-negative. This strategy only works if the basis functions $\\{\\phi_i(x)\\}$ are themselves chosen to be non-negative everywhere on $\\Omega$.\n\n**Verdict:** Incorrect.\n\n**D. Apply a logarithmic change of variables $y(x,t) := \\ln(c(x,t)/c_{\\mathrm{ref}})$... perform Galerkin projection on $y(x,t)$... In the ROM, recover $c(x; y) = c_{\\mathrm{ref}} \\exp(\\sum_i y_i \\psi_i(x))$.**\n\nThis method proposes a transformation of the state variable itself before model reduction. The transformation $c = c_{\\text{ref}}e^y$ maps the entire real line of possible values for $y$ to the set of strictly positive real numbers for $c$. The ROM is constructed for the transformed variable $y(x,t)$, yielding a reduced state with coefficients $y_i(t)$. The approximate concentration is then reconstructed via the inverse mapping:\n$c_{\\text{ROM}}(x,t) = c_{\\text{ref}} \\exp\\left(\\sum_{i=1}^r y_i(t) \\psi_i(x)\\right)$.\nFor any real-valued coefficients $y_i(t)$ and any real-valued basis functions $\\psi_i(x)$, the argument of the exponential function is a real number. The exponential function maps any real number to a strictly positive number. Thus, $c_{\\text{ROM}}(x,t)$ is guaranteed to be positive for all $x$ and $t$, by construction. This elegantly enforces the positivity constraint without needing any special properties of the basis functions or imposing constraints on the reduced coordinates $y_i$. The statement also correctly notes that a similar approach can handle the upper bound, for instance by transforming a variable like $c_{\\max} - c$ or using a logistic map. This is a well-known and valid approach for structure-preserving model reduction.\n\n**Verdict:** Correct.\n\n**E. Clip the reduced coordinates in time by saturating each $z_i(t)$ to the interval $[0, z_{i,\\max}]$... such coefficient saturation ensures $0 \\le c(x;z(t)) \\le c_{\\max}$ pointwise...**\n\nThis option suggests a brute-force approach of clipping the computed coefficients. This method has two major failings.\nFirst, as with options B and C, it does not guarantee the desired property. Saturating $z_i(t)$ to be non-negative does not ensure the positivity of $c(x;z) = \\sum_i z_i \\phi_i(x)$ when the basis functions $\\phi_i(x)$ can be negative. The hedge \"provided the bounds are chosen sufficiently small\" is not a practical solution; it would require such restrictive bounds (essentially forbidding the use of higher-order modes) that the ROM would lose its accuracy and purpose.\nSecond, this procedure is mathematically inconsistent with the Galerkin method. The Galerkin projection requires the residual of the ODE to be orthogonal to the basis space at all times. When a coefficient $z_i$ is \"clipped\", its time derivative is artificially set to a value (e.g., $0$) that does not satisfy the original reduced ODE $M\\dot{z} = f(z,u)$. This means the orthogonality condition is violated, and the resulting dynamics no longer represent a proper Galerkin projection of the original PDE. It is an ad-hoc modification that corrupts the model.\n\n**Verdict:** Incorrect.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}