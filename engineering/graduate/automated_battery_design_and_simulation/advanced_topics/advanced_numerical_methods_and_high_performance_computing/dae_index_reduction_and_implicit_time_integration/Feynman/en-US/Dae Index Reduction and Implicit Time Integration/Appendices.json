{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge when numerically solving DAEs is preventing the solution from \"drifting\" away from the algebraic constraint manifold. This exercise vividly demonstrates this phenomenon by comparing a naive integration scheme with one that includes a projection step. By implementing both and measuring their error against an exact solution, you will gain a practical understanding of why explicitly enforcing constraints is crucial for long-term simulation accuracy. ",
            "id": "3903303",
            "problem": "Consider a semi-explicit Differential-Algebraic Equation (DAE) model that captures a simplified coupling between a differential state and an algebraic constraint typical of electrochemical models used in automated battery design and simulation. Let the differential variable be $x(t)$ (dimensionless electrode-state proxy) and the algebraic variable be $z(t)$ (dimensionless electrolyte-potential proxy). The model consists of the following system:\n$$\\frac{dx}{dt} = f(x,z,t) = -a\\,x(t) + b\\,z(t),$$\nwith the algebraic constraint\n$$g(x,z,t) = z(t) - \\phi(x,t) = 0,$$\nwhere the constraint manifold is given by\n$$\\phi(x,t) = x(t) + \\sin(t).$$\nAll quantities are dimensionless, and all angles must be in radians.\n\nThe initial condition is consistent with the constraint manifold:\n$$x(0) = x_0,\\quad z(0) = x_0 + \\sin(0) = x_0.$$\n\nYou must implement an implicit time integration scheme using the Backward Euler method for the differential equation and demonstrate a projection step after each implicit time-step to drive the solution back to the constraint manifold by solving $g(x,z,t)=0$ for the algebraic variable $z$. Specifically, consider two algorithms:\n\n- Algorithm A (without projection): Advance $x$ using Backward Euler with lagged algebraic variable $z_n$, and keep $z$ constant:\n$$x_{n+1} = \\frac{x_n + h\\,b\\,z_n}{1 + h\\,a},\\quad z_{n+1} = z_n.$$\n\n- Algorithm B (with projection): Advance $x$ using Backward Euler with lagged algebraic variable $z_n$, then project onto the constraint manifold by solving $g(x_{n+1},z_{n+1},t_{n+1})=0$ for $z_{n+1}$:\n$$x_{n+1} = \\frac{x_n + h\\,b\\,z_n}{1 + h\\,a},\\quad z_{n+1} = x_{n+1} + \\sin(t_{n+1}).$$\n\nThe goal is to assess the effect of the projection step on the global error. Define the exact solution by performing DAE index reduction (eliminating the algebraic variable via the constraint):\n$$z(t) = x(t) + \\sin(t),$$\nwhich yields the reduced ordinary differential equation\n$$\\frac{dx}{dt} = -a\\,x(t) + b\\,(x(t) + \\sin(t)) = c\\,x(t) + b\\,\\sin(t),\\quad c = -a + b.$$\nWith the consistent initial condition $x(0) = x_0$, the exact solution for $x(t)$ is\n$$x(t) = e^{c t}\\left(x_0 + \\frac{b}{c^2 + 1}\\right) + \\frac{b}{c^2 + 1}\\left(-c\\,\\sin(t) - \\cos(t)\\right),$$\nand the exact algebraic variable is\n$$z(t) = x(t) + \\sin(t).$$\n\nFor a given uniform time-step $h$ and final time $T$, compute the Root-Mean-Square (RMS) global error for each algorithm over the discrete time grid $\\{t_n\\}_{n=0}^{N}$ with $N = T/h$:\n$$E = \\sqrt{\\frac{1}{N+1}\\sum_{n=0}^{N}\\left[(x_n - x(t_n))^2 + (z_n - z(t_n))^2\\right]}.$$\nReport the ratio of RMS errors,\n$$R = \\frac{E_{\\text{no\\_proj}}}{E_{\\text{proj}}},$$\nwhere $E_{\\text{no\\_proj}}$ is the RMS error for Algorithm A (without projection) and $E_{\\text{proj}}$ is the RMS error for Algorithm B (with projection). A value $R > 1$ indicates that the projection reduces the global error.\n\nAll computations must be performed using angles in radians. All quantities are dimensionless.\n\nTest Suite:\nUse the following parameter sets to evaluate the method. In each case, use the same consistent initial condition $x_0 = 0.1$:\n1. Happy path: $a = 1.5$, $b = 1.0$, $T = 10.0$, $h = 0.1$.\n2. Small time-step boundary: $a = 1.5$, $b = 1.0$, $T = 10.0$, $h = 0.001$.\n3. Large time-step edge case: $a = 1.5$, $b = 1.0$, $T = 10.0$, $h = 2.0$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the floating-point error ratio $R$ for one test case in the order listed above, for example, $[R_1,R_2,R_3]$. Each $R_i$ must be output as a Python float.",
            "solution": "The problem requires a critical comparison of two numerical integration schemes for a semi-explicit Differential-Algebraic Equation (DAE) system, which is a simplified model analogous to those used in battery simulation. The primary objective is to quantify the improvement in global accuracy gained by applying a projection step to enforce the algebraic constraint after each time step.\n\nFirst, we restate the governing equations. The system consists of a differential equation for the state $x(t)$ and an algebraic constraint for the variable $z(t)$:\n$$\n\\frac{dx}{dt} = -a\\,x(t) + b\\,z(t)\n$$\n$$\ng(x,z,t) = z(t) - (x(t) + \\sin(t)) = 0\n$$\nThe quantities $a$ and $b$ are constant parameters. The initial conditions are given as $x(0) = x_0$ and $z(0) = z_0$, which must be consistent with the algebraic constraint at $t=0$. This consistency is confirmed: $z(0) = x(0) + \\sin(0) = x_0$.\n\nTo assess the numerical solutions, we require an exact analytical solution. This is obtained through DAE index reduction. The DAE system is index-1, meaning one differentiation of the algebraic constraint is sufficient to obtain a differential equation for $z$. However, a simpler approach is to substitute the algebraic constraint directly into the differential equation. From the constraint, we have $z(t) = x(t) + \\sin(t)$. Substituting this into the equation for $\\frac{dx}{dt}$ yields:\n$$\n\\frac{dx}{dt} = -a\\,x(t) + b\\,(x(t) + \\sin(t)) = (b-a)x(t) + b\\,\\sin(t)\n$$\nLetting $c = b-a$, we have the reduced Ordinary Differential Equation (ODE):\n$$\n\\frac{dx}{dt} = c\\,x(t) + b\\,\\sin(t)\n$$\nThis is a standard first-order linear non-homogeneous ODE. Its solution, as provided and verified, for the initial condition $x(0) = x_0$ is:\n$$\nx(t) = e^{c t}\\left(x_0 + \\frac{b}{c^2 + 1}\\right) + \\frac{b}{c^2 + 1}\\left(-c\\,\\sin(t) - \\cos(t)\\right)\n$$\nThe exact solution for the algebraic variable $z(t)$ is then trivially found from the constraint:\n$$\nz(t) = x(t) + \\sin(t)\n$$\nThese expressions for $x(t)$ and $z(t)$ constitute our ground truth.\n\nWe now turn to the numerical schemes. The time domain $[0, T]$ is discretized into $N$ steps of size $h = T/N$, with discrete time points $t_n = n \\cdot h$ for $n = 0, 1, \\dots, N$. Both algorithms use the Backward Euler method for the differential part, made semi-implicit by using the value of the algebraic variable from the previous time step, $z_n$:\n$$\n\\frac{x_{n+1} - x_n}{h} = -a\\,x_{n+1} + b\\,z_n\n$$\nSolving for $x_{n+1}$ yields the update rule common to both algorithms:\n$$\nx_{n+1} = \\frac{x_n + h\\,b\\,z_n}{1 + h\\,a}\n$$\n\nThe algorithms differ in how they update $z_{n+1}$:\n\n**Algorithm A (Without Projection):** This is the most naive approach. The algebraic variable is treated as a constant parameter over the time step and is not updated to satisfy the constraint. This is computationally inexpensive but is expected to be inaccurate.\n$$\nx_{n+1} = \\frac{x_n + h\\,b\\,z_n}{1 + h\\,a}\n$$\n$$\nz_{n+1} = z_n\n$$\nThe numerical solution $(x_n, z_n)$ will generally drift away from the constraint manifold defined by $g(x,z,t)=0$.\n\n**Algorithm B (With Projection):** This algorithm introduces a correction, or projection, step. After computing a new value for the differential state, $x_{n+1}$, the algebraic state $z_{n+1}$ is explicitly calculated to satisfy the constraint at the new time $t_{n+1}$.\n$$\nx_{n+1} = \\frac{x_n + h\\,b\\,z_n}{1 + h\\,a} \\quad (\\text{Integration step})\n$$\n$$\nz_{n+1} = x_{n+1} + \\sin(t_{n+1}) \\quad (\\text{Projection step})\n$$\nThis method, a form of coordinate projection, ensures that the numerical solution $(x_{n+1}, z_{n+1})$ lies on the constraint manifold at every time step $t_{n+1}$. This is expected to prevent the accumulation of drift error and yield a more accurate global solution.\n\nTo compare these algorithms, we calculate the Root-Mean-Square (RMS) global error for each. The error $E$ is defined as the square root of the mean squared Euclidean distance between the numerical solution $(x_n, z_n)$ and the exact solution $(x(t_n), z(t_n))$ over all time points, including $t_0$:\n$$\nE = \\sqrt{\\frac{1}{N+1}\\sum_{n=0}^{N}\\left[(x_n - x(t_n))^2 + (z_n - z(t_n))^2\\right]}\n$$\nWe compute this error for Algorithm A ($E_{\\text{no\\_proj}}$) and Algorithm B ($E_{\\text{proj}}$). The final metric for comparison is the ratio of these errors:\n$$\nR = \\frac{E_{\\text{no\\_proj}}}{E_{\\text{proj}}}\n$$\nA value of $R > 1$ signifies that the projection step in Algorithm B reduces the global error compared to Algorithm A.\n\nThe implementation will proceed by first defining a function for the exact solution. Then, for each test case, we will run two simulations, one for each algorithm, from $t=0$ to $t=T$. In each simulation, we will store the sequence of computed states $(x_n, z_n)$. After the simulations, we will compute the exact solution at all discrete time points $t_n$. Finally, we will use these numerical and exact solutions to compute the RMS errors $E_{\\text{no\\_proj}}$ and $E_{\\text{proj}}$, and their ratio $R$. This process will be repeated for all parameter sets in the test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the DAE problem for the given test cases.\n    It calculates the error ratio R for each test case and prints the results.\n    \"\"\"\n    # Test cases: (a, b, T, h)\n    test_cases = [\n        (1.5, 1.0, 10.0, 0.1),    # Happy path\n        (1.5, 1.0, 10.0, 0.001),  # Small time-step boundary\n        (1.5, 1.0, 10.0, 2.0),    # Large time-step edge case\n    ]\n    x0 = 0.1  # Consistent initial condition for x(0)\n\n    results = []\n    for a, b, T, h in test_cases:\n        params = {'a': a, 'b': b, 'T': T, 'h': h, 'x0': x0}\n        \n        # Simulate without projection (Algorithm A)\n        x_num_A, z_num_A, t_grid = simulate('A', params)\n        \n        # Simulate with projection (Algorithm B)\n        x_num_B, z_num_B, _ = simulate('B', params)\n\n        # Calculate exact solution for the time grid\n        x_exact, z_exact = exact_solution(t_grid, params)\n        \n        # Calculate RMS error for both algorithms\n        E_no_proj = calculate_rms_error(x_num_A, z_num_A, x_exact, z_exact)\n        E_proj = calculate_rms_error(x_num_B, z_num_B, x_exact, z_exact)\n        \n        # Compute the error ratio R\n        # Handle the case where E_proj is zero to avoid division by zero,\n        # though it's highly unlikely in floating point arithmetic for\n        # a non-trivial problem.\n        if E_proj == 0.0:\n            # If projection error is zero, and no-projection error is non-zero,\n            # the ratio is effectively infinite. If both are zero, the ratio is 1.\n            ratio = np.inf if E_no_proj > 0.0 else 1.0\n        else:\n            ratio = E_no_proj / E_proj\n\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef exact_solution(t, params):\n    \"\"\"\n    Calculates the exact solution for x(t) and z(t) at given time points.\n    \n    Args:\n        t (np.ndarray): Array of time points.\n        params (dict): Dictionary of parameters {a, b, x0}.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the exact solutions \n                                       for x and z at the given time points.\n    \"\"\"\n    a, b, x0 = params['a'], params['b'], params['x0']\n    c = b - a\n    \n    # Check for the special case c=0 which is not in our test suite\n    # to avoid potential division by zero if this function were reused.\n    # For this problem c = -0.5, so this branch is not taken.\n    if c == 0:\n        x_t = x0 - b * (np.cos(t) - 1)\n    else:\n        c_sq = c**2\n        term1_factor = x0 + b / (c_sq + 1.0)\n        term1 = np.exp(c * t) * term1_factor\n        \n        term2_factor = b / (c_sq + 1.0)\n        term2 = term2_factor * (-c * np.sin(t) - np.cos(t))\n        \n        x_t = term1 + term2\n\n    z_t = x_t + np.sin(t)\n    return x_t, z_t\n\ndef simulate(algorithm_type, params):\n    \"\"\"\n    Performs time integration using either Algorithm A or B.\n\n    Args:\n        algorithm_type (str): 'A' for no projection, 'B' for projection.\n        params (dict): Dictionary of parameters {a, b, T, h, x0}.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]: Numerical solution arrays for \n                                                    x and z, and the time grid.\n    \"\"\"\n    a, b, T, h, x0 = params['a'], params['b'], params['T'], params['h'], params['x0']\n    \n    # Create the time grid. Use np.ceil to ensure an integer number of steps\n    # and that the grid includes T. The +1 is for the N+1 points.\n    N = int(np.ceil(T / h))\n    t_grid = np.linspace(0, T, N + 1)\n    num_points = len(t_grid)\n    \n    # Recalculate h to match the grid if linspace created a slightly different step\n    if N > 0:\n        h = t_grid[1] - t_grid[0]\n\n    x_num = np.zeros(num_points)\n    z_num = np.zeros(num_points)\n    \n    # Set initial conditions (consistent)\n    x_num[0] = x0\n    z_num[0] = x0  # z(0) = x(0) + sin(0) = x0\n\n    # Denominator in the x update rule\n    denom = 1.0 + h * a\n\n    for n in range(num_points - 1):\n        # Semi-implicit update for x\n        x_num[n+1] = (x_num[n] + h * b * z_num[n]) / denom\n\n        # Update for z depends on the algorithm\n        if algorithm_type == 'A':\n            # Algorithm A: No projection, z is lagged\n            z_num[n+1] = z_num[n]\n        elif algorithm_type == 'B':\n            # Algorithm B: Projection onto constraint manifold\n            z_num[n+1] = x_num[n+1] + np.sin(t_grid[n+1])\n        else:\n            raise ValueError(\"Invalid algorithm_type. Must be 'A' or 'B'.\")\n            \n    return x_num, z_num, t_grid\n\n\ndef calculate_rms_error(x_num, z_num, x_exact, z_exact):\n    \"\"\"\n    Calculates the RMS global error between numerical and exact solutions.\n\n    Args:\n        x_num (np.ndarray): Numerical solution for x.\n        z_num (np.ndarray): Numerical solution for z.\n        x_exact (np.ndarray): Exact solution for x.\n        z_exact (np.ndarray): Exact solution for z.\n\n    Returns:\n        float: The RMS error.\n    \"\"\"\n    squared_error_x = (x_num - x_exact)**2\n    squared_error_z = (z_num - z_exact)**2\n    \n    mean_squared_error = np.mean(squared_error_x + squared_error_z)\n    \n    return np.sqrt(mean_squared_error)\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Before a DAE simulation can begin, it needs a \"consistent\" starting point where all variables satisfy the governing equations at time $t=0$. This is often a non-trivial problem, especially for complex multiphysics models. In this practice, you will tackle the consistent initialization for a semi-discretized Doyle-Fuller-Newman (DFN) battery model, setting up and solving a nonlinear system to find initial potentials consistent with a specified state of charge and current. ",
            "id": "3903333",
            "problem": "Consider the Doyle–Fuller–Newman (DFN) model of a Lithium-ion battery under semi-discretization in space, where the solid-phase and electrolyte-phase electric potentials are algebraic variables and the concentrations are differential variables. At the start of an implicit time integration (e.g., backward Euler), consistent initial conditions must satisfy the algebraic constraints at time $t = 0$ for a specified applied current and state of charge. Assume a two-electrode cell with a negative electrode (n) and a positive electrode (p), and suppose that the electrolyte potential at the negative electrode is chosen as the gauge reference $0$ (index-reduction via gauge fixing). The following well-tested scientific relations are taken as the fundamental base:\n\n- Charge conservation at each electrode interface equates the reaction current density to the applied current scaled by interfacial area.\n- Butler–Volmer kinetics with symmetric transfer coefficients relate reaction current density to the activation overpotential.\n- An ohmic relation in the electrolyte approximates the electrolyte potential drop between electrodes as proportional to the applied current.\n\nLet $i_{\\mathrm{app}}$ denote the dimensionless applied current (normalized by a characteristic current and area), $a_n$ and $a_p$ denote dimensionless interfacial area scalings for the negative and positive electrodes, respectively, $k_n$ and $k_p$ denote dimensionless kinetic prefactors, $r_e$ denote the dimensionless electrolyte resistance, and let the open-circuit potentials $U_n$ and $U_p$ be given by a Nernst-type relation\n$$\nU_{\\ell}(c_{\\ell}) = U_{\\ell,\\mathrm{ref}} + \\beta_{\\ell}\\,\\ln\\!\\left(\\frac{c_{\\ell}}{1 - c_{\\ell}}\\right), \\quad \\ell \\in \\{n,p\\},\n$$\nwhere $c_{\\ell}\\in(0,1)$ is the dimensionless solid-phase lithium fraction at the electrode surface (equal initially to the state of charge at each electrode), $U_{\\ell,\\mathrm{ref}}$ is a dimensionless reference potential, and $\\beta_{\\ell}$ is a dimensionless coefficient.\n\nLet the unknowns be the solid-phase potentials at each electrode $\\phi_{s,n}$, $\\phi_{s,p}$ and the electrolyte potential at the positive electrode $\\phi_{e,p}$, with the gauge $\\phi_{e,n} = 0$. The Butler–Volmer reaction current densities are\n$$\nj_n = 2\\,k_n\\,\\sinh\\!\\left(\\frac{\\phi_{s,n} - \\phi_{e,n} - U_n(c_n)}{2}\\right) = 2\\,k_n\\,\\sinh\\!\\left(\\frac{\\phi_{s,n} - U_n(c_n)}{2}\\right),\n$$\n$$\nj_p = 2\\,k_p\\,\\sinh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p(c_p)}{2}\\right),\n$$\nand the algebraic system to enforce consistency with the specified applied current and state of charge is\n$$\nF_1(\\phi_{s,n}) = a_n\\,j_n - i_{\\mathrm{app}} = 0,\n$$\n$$\nF_2(\\phi_{s,p},\\phi_{e,p}) = a_p\\,j_p + i_{\\mathrm{app}} = 0,\n$$\n$$\nF_3(\\phi_{e,p}) = \\phi_{e,p} - r_e\\,i_{\\mathrm{app}} = 0.\n$$\nThe initial solid concentrations are differential variables and are therefore free to be set by the specified state of charge at $t=0$, i.e., $ c_n = \\mathrm{SOC}_n$ and $ c_p = \\mathrm{SOC}_p$. The problem asks to formulate this system and outline a Newton method to solve it.\n\nTasks:\n1. Starting from the fundamental base stated above, write the nonlinear system $F(\\mathbf{x}) = \\mathbf{0}$ with $\\mathbf{x} = [\\phi_{s,n}, \\phi_{s,p}, \\phi_{e,p}]^\\top$ that must be solved to find initial potentials and concentrations consistent with a specified initial current $i_{\\mathrm{app}}$ and state of charge values $c_n$ and $c_p$.\n2. Derive the analytic Jacobian matrix $J(\\mathbf{x}) = \\partial F/\\partial \\mathbf{x}$ suitable for Newton iterations.\n3. Implement a damped Newton method to solve for $\\phi_{s,n}$, $\\phi_{s,p}$, and $\\phi_{e,p}$ given parameter values and initial guesses. Use the gauge $\\phi_{e,n} = 0$. Treat $c_n$ and $c_p$ as equal to the given $\\mathrm{SOC}_n$ and $\\mathrm{SOC}_p$, respectively.\n4. Provide a test suite of three parameter sets that test different regimes:\n   - A typical operating current case.\n   - A near-equilibrium case with $i_{\\mathrm{app}} = 0$.\n   - A high-current case.\n   For each case, specify $(i_{\\mathrm{app}}, \\mathrm{SOC}_n, \\mathrm{SOC}_p, a_n, a_p, k_n, k_p, r_e, U_{n,\\mathrm{ref}}, U_{p,\\mathrm{ref}}, \\beta_n, \\beta_p)$.\n5. Final output format: Your program should produce a single line of output containing the results aggregated across all test cases as a comma-separated list enclosed in square brackets. For each test case, output the six floats $[\\phi_{s,n}, \\phi_{e,n}, \\phi_{s,p}, \\phi_{e,p}, c_n, c_p]$ flattened into a single list over all test cases, in that order. All quantities are dimensionless, so no physical units are required.\n\nTest suite parameters to use:\n- Case 1 (typical): $(i_{\\mathrm{app}}, \\mathrm{SOC}_n, \\mathrm{SOC}_p, a_n, a_p, k_n, k_p, r_e, U_{n,\\mathrm{ref}}, U_{p,\\mathrm{ref}}, \\beta_n, \\beta_p) = (0.5, 0.7, 0.4, 1.0, 1.0, 1.0, 1.0, 0.2, 0.1, 4.0, 0.1, 0.1)$.\n- Case 2 (near-equilibrium): $(0.0, 0.6, 0.6, 1.0, 1.0, 1.0, 1.0, 0.3, 0.15, 3.9, 0.2, 0.2)$.\n- Case 3 (high-current): $(2.0, 0.9, 0.3, 1.0, 1.0, 1.5, 1.0, 0.5, 0.2, 4.2, 0.15, 0.12)$.\n\nYour program must implement the Newton solve for the nonlinear system defined above and produce the specified output format. No user input is required.",
            "solution": "The problem requires the formulation and solution of a nonlinear algebraic system to determine consistent initial conditions for a semi-discretized Doyle-Fuller-Newman (DFN) battery model. The analysis proceeds in three steps: first, formulating the governing system of equations; second, deriving the Jacobian matrix required for a Newton-Raphson iterative solver; and third, outlining the implementation of a damped Newton method to find the solution.\n\n### 1. Formulation of the Nonlinear System\n\nThe state vector of unknowns is defined as $\\mathbf{x} = [\\phi_{s,n}, \\phi_{s,p}, \\phi_{e,p}]^\\top$, representing the solid-phase potential at the negative electrode, the solid-phase potential at the positive electrode, and the electrolyte-phase potential at the positive electrode, respectively. The electrolyte potential at the negative electrode is fixed by a gauge condition, $\\phi_{e,n} = 0$. The initial solid-phase surface concentrations, $c_n$ and $c_p$, are given as fixed parameters, equal to the electrode-specific states of charge, $\\mathrm{SOC}_n$ and $\\mathrm{SOC}_p$.\n\nThe system of equations, $F(\\mathbf{x}) = \\mathbf{0}$, arises from fundamental electrochemical and transport principles:\n\n1.  **Charge Conservation at the Negative Electrode Interface:** The total current generated by the electrochemical reaction over the entire interfacial area must equal the applied current, $i_{\\mathrm{app}}$. This gives the first equation:\n    $$F_1 = a_n\\,j_n - i_{\\mathrm{app}} = 0$$\n    Here, $a_n$ is the interfacial area scaling, and $j_n$ is the reaction current density at the negative electrode.\n\n2.  **Charge Conservation at the Positive Electrode Interface:** Similarly, accounting for the sign convention (current enters the positive electrode during discharge), we have:\n    $$F_2 = a_p\\,j_p + i_{\\mathrm{app}} = 0$$\n    where $a_p$ is the area scaling and $j_p$ is the reaction current density at the positive electrode.\n\n3.  **Ohmic Drop in the Electrolyte:** A simplified ohmic model relates the potential drop in the electrolyte to the applied current:\n    $$F_3 = \\phi_{e,p} - \\phi_{e,n} - r_e\\,i_{\\mathrm{app}} = 0$$\n    With the gauge $\\phi_{e,n}=0$, this simplifies to:\n    $$F_3 = \\phi_{e,p} - r_e\\,i_{\\mathrm{app}} = 0$$\n    where $r_e$ is the effective dimensionless resistance of the electrolyte.\n\nThe reaction current densities, $j_n$ and $j_p$, are described by the Butler-Volmer equation for symmetric transfer coefficients ($\\alpha_a = \\alpha_c = 0.5$):\n$$j_{\\ell} = 2\\,k_{\\ell}\\,\\sinh\\!\\left(\\frac{\\eta_{\\ell}}{2}\\right), \\quad \\ell \\in \\{n, p\\}$$\nwhere $k_{\\ell}$ is the kinetic rate prefactor and $\\eta_{\\ell}$ is the activation overpotential, defined as $\\eta_{\\ell} = \\phi_{s,\\ell} - \\phi_{e,\\ell} - U_{\\ell}(c_{\\ell})$. The open-circuit potentials, $U_n$ and $U_p$, are functions of the surface concentrations and are given by:\n$$U_{\\ell}(c_{\\ell}) = U_{\\ell,\\mathrm{ref}} + \\beta_{\\ell}\\,\\ln\\!\\left(\\frac{c_{\\ell}}{1 - c_{\\ell}}\\right)$$\nFor this initialization problem, $c_n$ and $c_p$ are fixed, so $U_n$ and $U_p$ are treated as constants.\n\nSubstituting the expressions for $j_n$ and $j_p$ into the conservation equations, we obtain the explicit nonlinear system $F(\\mathbf{x}) = [F_1, F_2, F_3]^\\top = \\mathbf{0}$:\n$$\nF_1(\\phi_{s,n}) = 2\\,a_n\\,k_n\\,\\sinh\\!\\left(\\frac{\\phi_{s,n} - U_n}{2}\\right) - i_{\\mathrm{app}} = 0\n$$\n$$\nF_2(\\phi_{s,p}, \\phi_{e,p}) = 2\\,a_p\\,k_p\\,\\sinh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right) + i_{\\mathrm{app}} = 0\n$$\n$$\nF_3(\\phi_{e,p}) = \\phi_{e,p} - r_e\\,i_{\\mathrm{app}} = 0\n$$\n\n### 2. Derivation of the Jacobian Matrix\n\nThe Newton-Raphson method requires the Jacobian matrix, $J(\\mathbf{x}) = \\partial F / \\partial \\mathbf{x}$, whose elements are $J_{ij} = \\partial F_i / \\partial x_j$. The state vector is $\\mathbf{x} = [\\phi_{s,n}, \\phi_{s,p}, \\phi_{e,p}]^\\top$.\n\nThe partial derivatives are calculated as follows, using the chain rule and the fact that $d(\\sinh(u))/dx = \\cosh(u) \\cdot du/dx$.\n\n- **Derivatives of $F_1$:**\n  - $\\frac{\\partial F_1}{\\partial \\phi_{s,n}} = 2\\,a_n\\,k_n\\,\\cosh\\!\\left(\\frac{\\phi_{s,n} - U_n}{2}\\right) \\cdot \\frac{1}{2} = a_n\\,k_n\\,\\cosh\\!\\left(\\frac{\\phi_{s,n} - U_n}{2}\\right)$\n  - $\\frac{\\partial F_1}{\\partial \\phi_{s,p}} = 0$\n  - $\\frac{\\partial F_1}{\\partial \\phi_{e,p}} = 0$\n\n- **Derivatives of $F_2$:**\n  - $\\frac{\\partial F_2}{\\partial \\phi_{s,n}} = 0$\n  - $\\frac{\\partial F_2}{\\partial \\phi_{s,p}} = 2\\,a_p\\,k_p\\,\\cosh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right) \\cdot \\frac{1}{2} = a_p\\,k_p\\,\\cosh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right)$\n  - $\\frac{\\partial F_2}{\\partial \\phi_{e,p}} = 2\\,a_p\\,k_p\\,\\cosh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right) \\cdot \\left(-\\frac{1}{2}\\right) = -a_p\\,k_p\\,\\cosh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right)$\n\n- **Derivatives of $F_3$:**\n  - $\\frac{\\partial F_3}{\\partial \\phi_{s,n}} = 0$\n  - $\\frac{\\partial F_3}{\\partial \\phi_{s,p}} = 0$\n  - $\\frac{\\partial F_3}{\\partial \\phi_{e,p}} = 1$\n\nAssembling these components yields the Jacobian matrix:\n$$\nJ(\\mathbf{x}) = \\begin{pmatrix}\na_n k_n \\cosh\\!\\left(\\frac{\\phi_{s,n} - U_n}{2}\\right) & 0 & 0 \\\\\n0 & a_p k_p \\cosh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right) & -a_p k_p \\cosh\\!\\left(\\frac{\\phi_{s,p} - \\phi_{e,p} - U_p}{2}\\right) \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nThe upper triangular structure of the Jacobian indicates that the system is only partially coupled.\n\n### 3. Damped Newton Method\n\nThe Newton-Raphson method is an iterative procedure for finding the root of $F(\\mathbf{x}) = \\mathbf{0}$. Starting from an initial guess $\\mathbf{x}_0$, successive approximations $\\mathbf{x}_{k+1}$ are found by solving a linear system based on the first-order Taylor expansion of $F(\\mathbf{x})$ around the current iterate $\\mathbf{x}_k$.\n\nThe iterative update is given by:\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\Delta\\mathbf{x}_k $$\nwhere the step direction $\\Delta\\mathbf{x}_k$ is the solution to the linear system:\n$$ J(\\mathbf{x}_k) \\Delta\\mathbf{x}_k = -F(\\mathbf{x}_k) $$\nThe scalar $\\alpha_k \\in (0, 1]$ is a damping factor, chosen to ensure convergence from a wider range of initial guesses. A backtracking line search is employed to select $\\alpha_k$: starting with $\\alpha_k = 1$, it is successively reduced (e.g., by half) until the condition $\\|F(\\mathbf{x}_k + \\alpha_k \\Delta\\mathbf{x}_k)\\| < \\|F(\\mathbf{x}_k)\\|$ is met, ensuring a decrease in the residual norm at each step.\n\nThe algorithm proceeds as follows:\n1.  Initialize with a guess $\\mathbf{x}_0$. A physically motivated choice is $\\mathbf{x}_0 = [U_n, U_p, 0]^\\top$, corresponding to the equilibrium state.\n2.  For $k=0, 1, 2, \\dots$:\n    a. Evaluate the residual vector $F(\\mathbf{x}_k)$ and its norm $\\|F(\\mathbf{x}_k)\\|$. If the norm is below a specified tolerance $\\epsilon$, the algorithm has converged.\n    b. Evaluate the Jacobian matrix $J(\\mathbf{x}_k)$.\n    c. Solve the linear system $J(\\mathbf{x}_k) \\Delta\\mathbf{x}_k = -F(\\mathbf{x}_k)$ for the update step $\\Delta\\mathbf{x}_k$.\n    d. Perform a backtracking line search to find a suitable damping factor $\\alpha_k$.\n    e. Update the solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\Delta\\mathbf{x}_k$.\n3.  The final iterate $\\mathbf{x}_k$ is the numerical solution for the initial potentials $[\\phi_{s,n}, \\phi_{s,p}, \\phi_{e,p}]^\\top$.\nThe initial conditions to be used in a subsequent time integration are then given by the solved potentials and the specified initial concentrations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve for consistent initial conditions for all test cases.\n    \"\"\"\n    # Test suite parameters:\n    # (i_app, SOC_n, SOC_p, a_n, a_p, k_n, k_p, r_e, U_n_ref, U_p_ref, beta_n, beta_p)\n    test_cases = [\n        # Case 1 (typical)\n        (0.5, 0.7, 0.4, 1.0, 1.0, 1.0, 1.0, 0.2, 0.1, 4.0, 0.1, 0.1),\n        # Case 2 (near-equilibrium)\n        (0.0, 0.6, 0.6, 1.0, 1.0, 1.0, 1.0, 0.3, 0.15, 3.9, 0.2, 0.2),\n        # Case 3 (high-current)\n        (2.0, 0.9, 0.3, 1.0, 1.0, 1.5, 1.0, 0.5, 0.2, 4.2, 0.15, 0.12),\n    ]\n\n    all_results_flat = []\n\n    for i, case_params in enumerate(test_cases):\n        params = {\n            'i_app': case_params[0],\n            'soc_n': case_params[1],\n            'soc_p': case_params[2],\n            'a_n': case_params[3],\n            'a_p': case_params[4],\n            'k_n': case_params[5],\n            'k_p': case_params[6],\n            'r_e': case_params[7],\n            'U_n_ref': case_params[8],\n            'U_p_ref': case_params[9],\n            'beta_n': case_params[10],\n            'beta_p': case_params[11],\n        }\n\n        # Calculate open-circuit potentials\n        params['U_n'] = _calc_ocp(params['U_n_ref'], params['beta_n'], params['soc_n'])\n        params['U_p'] = _calc_ocp(params['U_p_ref'], params['beta_p'], params['soc_p'])\n\n        # Initial guess for x = [phi_s,n, phi_s,p, phi_e,p]\n        # A good guess is the equilibrium potential state.\n        x0 = np.array([params['U_n'], params['U_p'], 0.0])\n\n        # Solve the nonlinear system using the Newton method\n        x_sol = _newton_solver(params, x0)\n\n        phi_sn_sol, phi_sp_sol, phi_ep_sol = x_sol\n\n        # Collect results for this case in the specified order\n        # [phi_s,n, phi_e,n, phi_s,p, phi_e,p, c_n, c_p]\n        # phi_e,n is 0 by gauge choice. c_n/c_p are the given SOCs.\n        case_results = [\n            phi_sn_sol,\n            0.0,\n            phi_sp_sol,\n            phi_ep_sol,\n            params['soc_n'],\n            params['soc_p'],\n        ]\n        all_results_flat.extend(case_results)\n\n    # Print the flattened list in the required format\n    print(f\"[{','.join(f'{v:.6f}' for v in all_results_flat)}]\")\n\n\ndef _calc_ocp(U_ref, beta, c):\n    \"\"\"Calculates the open-circuit potential using the Nernst-type relation.\"\"\"\n    if not (0  c  1):\n        raise ValueError(\"Concentration c must be in the interval (0, 1).\")\n    return U_ref + beta * np.log(c / (1.0 - c))\n\n\ndef _evaluate_F(x, params):\n    \"\"\"\n    Evaluates the residual vector F(x) of the nonlinear system.\n    x = [phi_s,n, phi_s,p, phi_e,p]\n    \"\"\"\n    phi_sn, phi_sp, phi_ep = x\n\n    # Butler-Volmer current densities\n    jn = 2.0 * params['k_n'] * np.sinh((phi_sn - params['U_n']) / 2.0)\n    jp = 2.0 * params['k_p'] * np.sinh((phi_sp - phi_ep - params['U_p']) / 2.0)\n\n    # Residuals\n    F1 = params['a_n'] * jn - params['i_app']\n    F2 = params['a_p'] * jp + params['i_app']\n    F3 = phi_ep - params['r_e'] * params['i_app']\n\n    return np.array([F1, F2, F3])\n\n\ndef _evaluate_J(x, params):\n    \"\"\"\n    Evaluates the Jacobian matrix J(x) of the nonlinear system.\n    x = [phi_s,n, phi_s,p, phi_e,p]\n    \"\"\"\n    phi_sn, phi_sp, phi_ep = x\n\n    # Jacobian components\n    J11 = params['a_n'] * params['k_n'] * np.cosh((phi_sn - params['U_n']) / 2.0)\n\n    common_term_p = params['a_p'] * params['k_p'] * np.cosh((phi_sp - phi_ep - params['U_p']) / 2.0)\n    J22 = common_term_p\n    J23 = -common_term_p\n\n    return np.array([\n        [J11, 0.0, 0.0],\n        [0.0, J22, J23],\n        [0.0, 0.0, 1.0]\n    ])\n\n\ndef _newton_solver(params, x0, tol=1e-10, max_iter=50, max_backtrack=10):\n    \"\"\"\n    Solves F(x)=0 using a damped Newton-Raphson method.\n    \"\"\"\n    x = np.copy(x0).astype(float)\n\n    for _ in range(max_iter):\n        F = _evaluate_F(x, params)\n        norm_F = np.linalg.norm(F)\n\n        if norm_F  tol:\n            return x\n\n        J = _evaluate_J(x, params)\n        if np.linalg.det(J) == 0:\n            raise RuntimeError(\"Newton solver failed: Jacobian is singular.\")\n\n        delta_x = np.linalg.solve(J, -F)\n\n        # Damping via backtracking line search\n        alpha = 1.0\n        for _ in range(max_backtrack):\n            x_new = x + alpha * delta_x\n            norm_F_new = np.linalg.norm(_evaluate_F(x_new, params))\n            if norm_F_new  norm_F:\n                break\n            alpha /= 2.0\n        else: # Loop completed without break\n            raise RuntimeError(\"Newton solver failed: line search did not converge.\")\n\n        x = x_new\n    \n    raise RuntimeError(\"Newton solver failed: maximum iterations reached.\")\n\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "Each step of an implicit integrator for a DAE involves solving a system of algebraic equations, which is often nonlinear and large. The efficiency of the entire simulation hinges on solving this system effectively. This exercise guides you through the process for a battery pack model, where the system naturally has a symmetric indefinite \"saddle-point\" structure, and asks you to implement an efficient block-elimination solver based on the Schur complement. ",
            "id": "3903309",
            "problem": "You are designing an implicit time integrator for a battery pack model assembled using Modified Nodal Analysis (MNA). Your task is to derive and implement the Newton system for a single implicit Euler step of a two-node battery pack Differential-Algebraic Equation (DAE), and to exploit the resulting sparse, symmetric indefinite saddle-point structure to construct an efficient linear solver based on block elimination.\n\nFundamental base:\n- Kirchhoff’s Current Law (KCL): the algebraic sum of currents into a node is zero.\n- Capacitor constitutive law: $i_C = C \\, \\dfrac{dv}{dt}$.\n- Resistor constitutive law: $i_R = \\dfrac{v}{R}$.\n- Nonlinear Faradaic interface current modeled by a hyperbolic sine: $i_F(v) = i_0 \\, \\sinh\\!\\big(\\beta \\, (v - E)\\big)$.\n\nModel setup:\n- There are two dynamic nodes with voltages $v_1$ and $v_2$ measured relative to ground, forming the vector $v = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}$.\n- Capacitors to ground at each node with capacitances $C_1$ and $C_2$, forming the diagonal matrix $C = \\mathrm{diag}(C_1,C_2)$.\n- Internal resistors to ground with resistances $R_1$ at node $1$ and $R_2$ at node $2$.\n- An interconnect resistor between node $1$ and node $2$ with resistance $R_{12}$.\n- Nonlinear Faradaic currents at each node: $i_{F,1}(v_1) = i_{0,1} \\, \\sinh\\!\\big(\\beta \\, (v_1 - E_1)\\big)$ and $i_{F,2}(v_2) = i_{0,2} \\, \\sinh\\!\\big(\\beta \\, (v_2 - E_2)\\big)$.\n- A single ideal voltage source that constrains node $2$ to a specified value $V_{\\mathrm{src}}(t)$; this introduces an algebraic constraint $A \\, v = g(t)$ with $A = \\begin{bmatrix} 0  1 \\end{bmatrix}$ and $g(t) = V_{\\mathrm{src}}(t)$, and a corresponding Lagrange multiplier $\\lambda$ representing the source current.\n\nUsing MNA, the semi-explicit DAE can be written in the form\n$$\nC \\, \\dfrac{dv}{dt} + G \\, v + i_F(v) + A^\\top \\lambda = s(t), \\quad A \\, v = g(t),\n$$\nwhere $G \\in \\mathbb{R}^{2 \\times 2}$ is the symmetric conductance matrix assembled from the resistors:\n$$\nG = \\begin{bmatrix}\n\\dfrac{1}{R_1} + \\dfrac{1}{R_{12}}  -\\dfrac{1}{R_{12}} \\\\\n-\\dfrac{1}{R_{12}}  \\dfrac{1}{R_2} + \\dfrac{1}{R_{12}}\n\\end{bmatrix},\n$$\nand $s(t) \\in \\mathbb{R}^2$ is a known source vector (assume $s(t) = 0$ for this problem). The Jacobian of $i_F(v)$ is diagonal:\n$$\nJ_F(v) = \\mathrm{diag}\\!\\Big(i_{0,1} \\beta \\cosh\\!\\big(\\beta (v_1 - E_1)\\big), \\; i_{0,2} \\beta \\cosh\\!\\big(\\beta (v_2 - E_2)\\big)\\Big).\n$$\n\nImplicit Euler step:\nFor a time step of size $\\Delta t$ from time $t_n$ to $t_{n+1}$, denote the known previous state by $v_n$ and the unknown next state by $v_{n+1}$. Discretize the DAE with implicit Euler to obtain two nonlinear equations in $v_{n+1}$ and $\\lambda_{n+1}$.\n\nYour tasks:\n1. Derive the residual equations for the implicit Euler step starting from the fundamental laws and definitions given above, and set up the Newton linearization. Show that the Newton system at an iterate $(v^{(k)}, \\lambda^{(k)})$ has the saddle-point form\n$$\n\\begin{bmatrix}\nM(v^{(k)})  A^\\top \\\\\nA  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\delta v^{(k)} \\\\\n\\delta \\lambda^{(k)}\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\nr_v(v^{(k)}, \\lambda^{(k)}) \\\\\nr_c(v^{(k)})\n\\end{bmatrix},\n$$\nand identify $M(v^{(k)})$, $r_v(\\cdot)$, and $r_c(\\cdot)$ explicitly in terms of $C$, $G$, $J_F(v^{(k)})$, $v_n$, $s(t_{n+1})$, and $g(t_{n+1})$.\n2. Propose an efficient solver that exploits the sparse, symmetric indefinite structure of the Newton matrix. Specifically, use block elimination to form the Schur complement in $\\lambda$ and solve using sparse linear algebra, assuming $M(v^{(k)})$ is symmetric positive definite. Clearly state the sequence of linear solves required to obtain $(\\delta v^{(k)}, \\delta \\lambda^{(k)})$.\n3. Implement a program that, for each test case below, performs Newton iterations for a single implicit Euler time step starting from the initial guess $v^{(0)} = v_n$ and $\\lambda^{(0)} = 0$, stopping when the Euclidean norm of the voltage update satisfies $\\|\\delta v^{(k)}\\|_2  \\varepsilon$ with tolerance $\\varepsilon = 10^{-12}$ or when a maximum of $50$ iterations is reached. Use the block-elimination solver described in task $2$, with a fallback to a direct sparse solve of the full saddle-point system if the Schur complement becomes ill-conditioned. For each test case, output the final Euclidean norm $\\|v_{n+1} - v_n\\|_2$ of the converged voltage increment. Express the output values in volts (V) as decimal numbers.\n\nTest suite:\n- Case $1$ (happy path): $C_1 = 500 \\text{ F}$, $C_2 = 500 \\text{ F}$, $R_1 = 0.01 \\, \\Omega$, $R_2 = 0.01 \\, \\Omega$, $R_{12} = 0.02 \\, \\Omega$, $i_{0,1} = 2 \\text{ A}$, $i_{0,2} = 2 \\text{ A}$, $\\beta = 10 \\text{ V}^{-1}$, $E_1 = 3.7 \\text{ V}$, $E_2 = 3.7 \\text{ V}$, $V_{\\mathrm{src}}(t_{n+1}) = 3.8 \\text{ V}$, $\\Delta t = 1 \\times 10^{-3} \\text{ s}$, $v_n = \\begin{bmatrix} 3.7 \\\\ 3.8 \\end{bmatrix} \\text{ V}$.\n- Case $2$ (stiff small-step edge): $C_1 = 200 \\text{ F}$, $C_2 = 200 \\text{ F}$, $R_1 = 0.005 \\, \\Omega$, $R_2 = 0.005 \\, \\Omega$, $R_{12} = 0.001 \\, \\Omega$, $i_{0,1} = 5 \\text{ A}$, $i_{0,2} = 5 \\text{ A}$, $\\beta = 20 \\text{ V}^{-1}$, $E_1 = 3.75 \\text{ V}$, $E_2 = 3.75 \\text{ V}$, $V_{\\mathrm{src}}(t_{n+1}) = 3.9 \\text{ V}$, $\\Delta t = 1 \\times 10^{-6} \\text{ s}$, $v_n = \\begin{bmatrix} 3.75 \\\\ 3.9 \\end{bmatrix} \\text{ V}$.\n- Case $3$ (weak coupling boundary): $C_1 = 100 \\text{ F}$, $C_2 = 100 \\text{ F}$, $R_1 = 0.02 \\, \\Omega$, $R_2 = 0.02 \\, \\Omega$, $R_{12} = 1000 \\, \\Omega$, $i_{0,1} = 0.5 \\text{ A}$, $i_{0,2} = 0.5 \\text{ A}$, $\\beta = 5 \\text{ V}^{-1}$, $E_1 = 3.6 \\text{ V}$, $E_2 = 3.6 \\text{ V}$, $V_{\\mathrm{src}}(t_{n+1}) = 3.6 \\text{ V}$, $\\Delta t = 1 \\times 10^{-1} \\text{ s}$, $v_n = \\begin{bmatrix} 3.6 \\\\ 3.6 \\end{bmatrix} \\text{ V}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the test suite cases, for example, $[ r_1, r_2, r_3 ]$, where $r_k$ is the final Euclidean norm $\\|v_{n+1} - v_n\\|_2$ in volts (V) for case $k$.",
            "solution": "The user has requested the derivation and implementation of a Newton-based implicit Euler solver for a two-node battery pack model. The core tasks are to:\n1.  Derive the Newton system for one implicit Euler step.\n2.  Propose a block-elimination solver for the resulting linear system.\n3.  Implement the solver and run it for three test cases.\n\nThe problem statement has been validated and is found to be scientifically grounded, well-posed, and complete. All necessary physical laws, component models, and parameters are provided. The tasks are mathematically and computationally well-defined.\n\n### Task 1: Derivation of the Newton System\n\nThe system is described by a set of Differential-Algebraic Equations (DAEs) in the Modified Nodal Analysis (MNA) formulation. Let $v(t) \\in \\mathbb{R}^2$ be the vector of node voltages and $\\lambda(t) \\in \\mathbb{R}$ be the Lagrange multiplier for the voltage source constraint.\n\nThe governing DAEs are:\n$$\n\\begin{align*}\nC \\, \\frac{dv}{dt} + G \\, v + i_F(v) + A^\\top \\lambda = s(t) \\quad (1a) \\\\\nA \\, v = g(t) \\quad (1b)\n\\end{align*}\n$$\nwhere $C$ is the capacitance matrix, $G$ is the conductance matrix, $i_F(v)$ is the vector of nonlinear Faradaic currents, $A$ is the constraint matrix, $\\lambda$ is the current through the voltage source, $s(t)$ is the external current source vector (given as $s(t)=0$), and $g(t)$ is the specified voltage from the source.\n\nWe discretize the time derivative $\\frac{dv}{dt}$ using the implicit Euler method for a time step from $t_n$ to $t_{n+1}$ with step size $\\Delta t = t_{n+1} - t_n$:\n$$\n\\frac{dv}{dt}\\bigg|_{t_{n+1}} \\approx \\frac{v_{n+1} - v_n}{\\Delta t}\n$$\nHere, $v_n = v(t_n)$ is the known voltage vector from the previous time step, and $v_{n+1} = v(t_{n+1})$ is the unknown voltage vector at the current time step. Substituting this into the DAE system (1a) and evaluating all terms at $t_{n+1}$ yields the nonlinear algebraic system for the unknowns $v_{n+1}$ and $\\lambda_{n+1}$:\n$$\n\\begin{align*}\nC \\frac{v_{n+1} - v_n}{\\Delta t} + G v_{n+1} + i_F(v_{n+1}) + A^\\top \\lambda_{n+1} - s(t_{n+1}) = 0 \\\\\nA v_{n+1} - g(t_{n+1}) = 0\n\\end{align*}\n$$\nTo solve this nonlinear system, we use Newton's method. Let $(v^{(k)}, \\lambda^{(k)})$ be the $k$-th iterate for the solution $(v_{n+1}, \\lambda_{n+1})$. We define the residual functions:\n$$\n\\begin{align*}\nr_v(v, \\lambda) = \\frac{C(v - v_n)}{\\Delta t} + Gv + i_F(v) + A^\\top \\lambda - s(t_{n+1}) \\\\\nr_c(v) = Av - g(t_{n+1})\n\\end{align*}\n$$\nThe Newton update step is defined by solving the linear system $J^{(k)} \\begin{bmatrix} \\delta v^{(k)} \\\\ \\delta \\lambda^{(k)} \\end{bmatrix} = - \\begin{bmatrix} r_v(v^{(k)}, \\lambda^{(k)}) \\\\ r_c(v^{(k)}) \\end{bmatrix}$, where $J^{(k)}$ is the Jacobian of the residual vector $\\begin{bmatrix} r_v \\\\ r_c \\end{bmatrix}$ evaluated at $(v^{(k)}, \\lambda^{(k)})$, and $(\\delta v^{(k)}, \\delta \\lambda^{(k)})$ is the update, such that $v^{(k+1)} = v^{(k)} + \\delta v^{(k)}$ and $\\lambda^{(k+1)} = \\lambda^{(k)} + \\delta \\lambda^{(k)}$.\n\nThe Jacobian matrix $J$ is composed of partial derivatives:\n$$\nJ(v, \\lambda) = \\begin{bmatrix} \\frac{\\partial r_v}{\\partial v}  \\frac{\\partial r_v}{\\partial \\lambda} \\\\ \\frac{\\partial r_c}{\\partial v}  \\frac{\\partial r_c}{\\partial \\lambda} \\end{bmatrix}\n$$\nThe individual blocks are:\n- $\\frac{\\partial r_v}{\\partial v} = \\frac{C}{\\Delta t} + G + \\frac{\\partial i_F(v)}{\\partial v} = \\frac{C}{\\Delta t} + G + J_F(v)$, where $J_F(v)$ is the Jacobian of the Faradaic current.\n- $\\frac{\\partial r_v}{\\partial \\lambda} = A^\\top$\n- $\\frac{\\partial r_c}{\\partial v} = A$\n- $\\frac{\\partial r_c}{\\partial \\lambda} = 0$ (a zero matrix, or scalar $0$ in this case)\n\nThe Newton system at iteration $k$ is therefore:\n$$\n\\begin{bmatrix}\n\\frac{C}{\\Delta t} + G + J_F(v^{(k)})  A^\\top \\\\\nA  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\delta v^{(k)} \\\\\n\\delta \\lambda^{(k)}\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\nr_v(v^{(k)}, \\lambda^{(k)}) \\\\\nr_c(v^{(k)})\n\\end{bmatrix}\n$$\nThis is the required saddle-point form. By comparing with the problem statement's template, we identify:\n- $M(v^{(k)}) = \\frac{C}{\\Delta t} + G + J_F(v^{(k)})$\n- $r_v(v^{(k)}, \\lambda^{(k)}) = \\frac{C}{\\Delta t}(v^{(k)} - v_n) + Gv^{(k)} + i_F(v^{(k)}) + A^\\top \\lambda^{(k)} - s(t_{n+1})$\n- $r_c(v^{(k)}) = Av^{(k)} - g(t_{n+1})$\n\nThe matrix $M(v^{(k)})$ is symmetric because $C$, $G$, and $J_F(v^{(k)})$ are all symmetric. It is also positive definite because $C$ and $J_F$ are diagonal with positive entries, $G$ is positive semi-definite, and $\\Delta t > 0$. The sum is therefore symmetric positive definite (SPD).\n\n### Task 2: Block-Elimination Solver\n\nThe Newton system has a $2 \\times 2$ block structure, commonly known as a Karush-Kuhn-Tucker (KKT) or saddle-point system.\n$$\n\\begin{align*}\nM \\delta v + A^\\top \\delta \\lambda = -r_v \\quad (2a) \\\\\nA \\delta v = -r_c \\quad (2b)\n\\end{align*}\n$$\nGiven that $M = M(v^{(k)})$ is SPD and thus invertible, we can solve for $\\delta v$ from (2a):\n$$\n\\delta v = M^{-1}(-r_v - A^\\top \\delta \\lambda) \\quad (3)\n$$\nSubstituting this into (2b):\n$$\nA \\big( M^{-1}(-r_v - A^\\top \\delta \\lambda) \\big) = -r_c\n$$\n$$\n-A M^{-1} r_v - (A M^{-1} A^\\top) \\delta \\lambda = -r_c\n$$\nRearranging to solve for the scalar update $\\delta \\lambda$:\n$$\n(A M^{-1} A^\\top) \\delta \\lambda = r_c - A M^{-1} r_v\n$$\nThe term $S = A M^{-1} A^\\top$ is the Schur complement of the system with respect to the block $M$. Since $M$ is SPD and $A$ has full row rank (in this case $A = \\begin{bmatrix} 0  1 \\end{bmatrix} \\neq 0$), the Schur complement $S$ is also SPD. As it is a $1 \\times 1$ matrix (a scalar), this means $S > 0$.\n\nThe efficient solution procedure, which avoids explicitly forming a large inverse in the general sparse case, is as follows:\n1.  Form the matrix $M(v^{(k)})$.\n2.  Solve the linear system $M z_v = r_v$ for the vector $z_v$.\n3.  Solve the linear system $M z_A = A^\\top$ for the vector $z_A$.\n4.  Compute the Schur complement: $S = A z_A$.\n5.  Solve for the Lagrange multiplier update: $\\delta \\lambda = S^{-1} (r_c - A z_v)$. For a scalar $S$, this is a simple division $\\delta \\lambda = (r_c - A z_v) / S$.\n6.  Back-substitute to find the voltage update: $\\delta v = -z_v - z_A \\delta \\lambda$.\n\nThis method requires two linear solves with the matrix $M$ (which can be factorized once) and scalar/vector operations, making it efficient for large-scale systems where a direct solve on the full indefinite system can be less stable or more expensive. For the given $2 \\times 2$ problem, solving with $M$ is trivial, but this procedure demonstrates the general principle.\n\n### Task 3: Implementation\n\nThe provided Python code implements the Newton's method using the block-elimination (Schur complement) solver derived above. It iterates until the L2-norm of the voltage update, $\\|\\delta v^{(k)}\\|_2$, falls below a tolerance of $\\varepsilon = 10^{-12}$. A fallback to a direct sparse solve of the full $3 \\times 3$ KKT system is included for the case where the Schur complement $S$ is near-zero (ill-conditioned), although this is unlikely given the problem's structure. For each test case, the program calculates and reports the L2-norm of the total voltage change for the time step, $\\|v_{n+1} - v_n\\|_2$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"C1\": 500.0, \"C2\": 500.0, \"R1\": 0.01, \"R2\": 0.01, \"R12\": 0.02,\n            \"i01\": 2.0, \"i02\": 2.0, \"beta\": 10.0, \"E1\": 3.7, \"E2\": 3.7,\n            \"V_src\": 3.8, \"dt\": 1e-3, \"v_n\": np.array([3.7, 3.8]),\n        },\n        {\n            \"C1\": 200.0, \"C2\": 200.0, \"R1\": 0.005, \"R2\": 0.005, \"R12\": 0.001,\n            \"i01\": 5.0, \"i02\": 5.0, \"beta\": 20.0, \"E1\": 3.75, \"E2\": 3.75,\n            \"V_src\": 3.9, \"dt\": 1e-6, \"v_n\": np.array([3.75, 3.9]),\n        },\n        {\n            \"C1\": 100.0, \"C2\": 100.0, \"R1\": 0.02, \"R2\": 0.02, \"R12\": 1000.0,\n            \"i01\": 0.5, \"i02\": 0.5, \"beta\": 5.0, \"E1\": 3.6, \"E2\": 3.6,\n            \"V_src\": 3.6, \"dt\": 0.1, \"v_n\": np.array([3.6, 3.6]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_newton_step(params)\n        results.append(f\"{result:.15f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_newton_step(params):\n    \"\"\"\n    Performs Newton iterations for a single implicit Euler time step.\n    \n    Args:\n        params (dict): A dictionary containing all the parameters for the simulation.\n\n    Returns:\n        float: The Euclidean norm of the converged voltage increment ||v_{n+1} - v_n||_2.\n    \"\"\"\n    # Unpack parameters\n    C1, C2 = params[\"C1\"], params[\"C2\"]\n    R1, R2, R12 = params[\"R1\"], params[\"R2\"], params[\"R12\"]\n    i01, i02, beta = params[\"i01\"], params[\"i02\"], params[\"beta\"]\n    E1, E2 = params[\"E1\"], params[\"E2\"]\n    V_src, dt, v_n = params[\"V_src\"], params[\"dt\"], params[\"v_n\"]\n    \n    # Constants and constant matrices\n    C = np.diag([C1, C2])\n    G = np.array([\n        [1/R1 + 1/R12, -1/R12],\n        [-1/R12, 1/R2 + 1/R12]\n    ])\n    A = np.array([[0.0, 1.0]])\n    A_T = A.T\n    g = V_src\n    s = np.zeros(2) # s(t) = 0\n    \n    # Newton iteration settings\n    v_k = np.copy(v_n)\n    lambda_k = 0.0\n    max_iter = 50\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        v1k, v2k = v_k\n        \n        # Evaluate nonlinear currents and their Jacobian\n        arg1 = beta * (v1k - E1)\n        arg2 = beta * (v2k - E2)\n        \n        # Check for potential overflow in sinh/cosh\n        if np.abs(arg1) > 100 or np.abs(arg2) > 100:\n             # If arguments get too big, Newton is likely diverging.\n             # Terminate and let the max_iter condition handle it.\n             break\n\n        iF = np.array([i01 * np.sinh(arg1), i02 * np.sinh(arg2)])\n        \n        JF_diag = np.array([\n            i01 * beta * np.cosh(arg1),\n            i02 * beta * np.cosh(arg2)\n        ])\n        JF = np.diag(JF_diag)\n        \n        # Calculate residuals\n        r_v = (C @ (v_k - v_n)) / dt + G @ v_k + iF + A_T.flatten() * lambda_k - s\n        r_c = (A @ v_k)[0] - g\n        \n        # Form the (1,1) block of the Newton matrix\n        M = C / dt + G + JF\n        \n        # --- Solve the KKT system ---\n        # Schur Complement: S = A * M^-1 * A^T\n        # For A = [0, 1], S is the (2,2) element of M^-1\n        det_M = M[0, 0] * M[1, 1] - M[0, 1] * M[1, 0]\n        \n        # Use Schur complement by default\n        # The condition for fallback is if Schur complement is ill-conditioned (near zero)\n        if abs(det_M)  1e-20 or abs(M[0, 0]/det_M)  1e-14:\n            # Fallback to direct sparse solve of the full KKT system\n            KKT_mat = np.zeros((3, 3))\n            KKT_mat[0:2, 0:2] = M\n            KKT_mat[0:2, 2] = A_T.flatten()\n            KKT_mat[2, 0:2] = A.flatten()\n            KKT_sparse = csr_matrix(KKT_mat)\n            \n            rhs = -np.concatenate([r_v, [r_c]])\n            \n            delta_x = spsolve(KKT_sparse, rhs)\n            delta_v = delta_x[:2]\n            delta_lambda = delta_x[2]\n        else:\n            # Block elimination via Schur complement\n            M_inv = (1.0 / det_M) * np.array([[M[1, 1], -M[0, 1]], [-M[1, 0], M[0, 0]]])\n            S = M_inv[1, 1] # A M^-1 A.T\n\n            # Solve for delta_lambda\n            # delta_lambda = S^-1 * (r_c - A * M^-1 * r_v)\n            z_v = M_inv @ r_v\n            A_z_v = z_v[1] # A @ z_v\n            delta_lambda = (r_c - A_z_v) / S\n            \n            # Solve for delta_v\n            # delta_v = -M^-1 * r_v - M^-1 * A^T * delta_lambda\n            z_A = M_inv @ A_T\n            delta_v = -z_v - z_A.flatten() * delta_lambda\n\n        # Update solution\n        v_k += delta_v\n        lambda_k += delta_lambda\n        \n        # Check for convergence\n        if np.linalg.norm(delta_v)  tolerance:\n            break\n            \n    v_np1 = v_k\n    return np.linalg.norm(v_np1 - v_n)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}