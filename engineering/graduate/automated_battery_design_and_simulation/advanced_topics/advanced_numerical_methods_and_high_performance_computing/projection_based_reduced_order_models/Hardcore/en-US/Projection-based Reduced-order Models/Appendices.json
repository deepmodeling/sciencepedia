{
    "hands_on_practices": [
        {
            "introduction": "This exercise introduces the core mechanism of Galerkin projection, where a high-dimensional system is mapped to a low-dimensional subspace. By computing the reduced mass and stiffness matrices for a simple system, you will directly investigate how the choice of basis vectors impacts the numerical properties of the reduced-order model. This practice is fundamental to understanding the trade-offs between model reduction and the conditioning and stability of the resulting simulation .",
            "id": "3943055",
            "problem": "Consider a semi-discrete submodel of the Doyle–Fuller–Newman (DFN) battery model obtained from mass conservation and Fickian diffusion in a spherically symmetric solid particle. Under linearization and spatial discretization, the evolution of the state vector $x(t) \\in \\mathbb{R}^n$ is governed by\n$$\nM \\,\\dot{x}(t) + A \\, x(t) = f(t),\n$$\nwhere $M \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite mass matrix, $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite stiffness matrix arising from the diffusion operator and boundary conditions, and $f(t)$ is an input. In a projection-based reduced-order model, a trial basis $V \\in \\mathbb{R}^{n \\times r}$ with full column rank, $r \\ll n$, is used to approximate $x(t) \\approx V z(t)$, yielding reduced matrices\n$$\nM_r = V^\\top M V, \\quad A_r = V^\\top A V,\n$$\nand the reduced dynamics $M_r \\,\\dot{z}(t) + A_r \\, z(t) = V^\\top f(t)$. The conditioning and stiffness of the reduced operator can be analyzed via the generalized spectrum of $(A_r, M_r)$, i.e., the eigenvalues $\\lambda$ solving\n$$\nA_r u = \\lambda \\, M_r u,\n$$\nequivalently the spectrum of $M_r^{-1} A_r$ when $M_r$ is invertible.\n\nYour task is to implement a program that, for each test case below, computes:\n- The reduced matrices $M_r$ and $A_r$.\n- The spectral condition number of $M_r$ in the $2$-norm, defined as the ratio of its largest to smallest eigenvalue.\n- The minimum and maximum generalized eigenvalues of $(A_r, M_r)$.\n- The eigenvalue spread defined as the ratio of the maximum to the minimum generalized eigenvalue.\n\nAll computations must be in floating-point arithmetic. There are no physical units to report. Angles are not applicable. Express all floating-point outputs rounded to eight decimal places.\n\nFundamental base:\n- Mass conservation in a control volume implies, upon spatial discretization, $M \\dot{x}(t) + A x(t) = f(t)$ with symmetric positive definite $M$ and $A$ for diffusion-dominated subproblems under proper boundary conditions.\n- For any full column rank $V$, the Galerkin projection yields $M_r = V^\\top M V$ and $A_r = V^\\top A V$.\n- For symmetric positive definite $M_r$ and $A_r$, the generalized eigenvalues of $(A_r, M_r)$ are real and positive.\n\nTest suite:\nLet $n = 5$. Use the following full-order matrices in all cases:\n$$\nM = \\mathrm{diag}(0.8,\\,1.0,\\,1.2,\\,1.0,\\,0.9),\n$$\n$$\nA =\n\\begin{bmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{bmatrix}.\n$$\nFor each case below, use the specified basis $V$:\n\n- Case $1$ (non-orthonormal, low-dimensional basis, $r=2$):\n$$\nV_1 =\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 2 \\\\\n1 & 1\n\\end{bmatrix}.\n$$\n\n- Case $2$ (canonical coordinate subspace, $r=4$):\n$$\nV_2 =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\n\n- Case $3$ (nearly linearly dependent basis, $r=3$):\nLet $\\varepsilon = 10^{-6}$. Define\n$$\nv_1 = \\begin{bmatrix}1\\\\1\\\\1\\\\1\\\\1\\end{bmatrix}, \\quad\nv_2 = v_1 + \\varepsilon \\begin{bmatrix}1\\\\-1\\\\1\\\\-1\\\\1\\end{bmatrix} =\n\\begin{bmatrix}1+\\varepsilon\\\\1-\\varepsilon\\\\1+\\varepsilon\\\\1-\\varepsilon\\\\1+\\varepsilon\\end{bmatrix}, \\quad\nv_3 = \\begin{bmatrix}1\\\\0\\\\0\\\\0\\\\0\\end{bmatrix},\n$$\nand set\n$$\nV_3 = \\begin{bmatrix} v_1 & v_2 & v_3 \\end{bmatrix}.\n$$\n\n- Case $4$ (full-order, $r=5$):\n$$\nV_4 = I_5,\n$$\nthe $5 \\times 5$ identity matrix.\n\nFor each case $i \\in \\{1,2,3,4\\}$, compute:\n- $r_i$, the reduced dimension.\n- $\\kappa_i$, the spectral condition number of $M_{r_i}$ in the $2$-norm.\n- $\\lambda_{\\min,i}$ and $\\lambda_{\\max,i}$, the smallest and largest generalized eigenvalues of $(A_{r_i}, M_{r_i})$.\n- $s_i = \\lambda_{\\max,i} / \\lambda_{\\min,i}$, the eigenvalue spread.\n\nFinal output format:\nYour program should produce a single line of output containing a list of four inner lists, one per test case, in order. Each inner list must be\n$$\n[r_i,\\, \\kappa_i,\\, \\lambda_{\\min,i},\\, \\lambda_{\\max,i},\\, s_i],\n$$\nwith all floating-point entries rounded to eight decimal places and printed with exactly eight digits after the decimal point. The top-level list must be enclosed in square brackets and the inner lists must also be enclosed in square brackets. For example, the overall shape should look like\n$$\n\\big[ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot] \\big].\n$$",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of model order reduction for linear time-invariant systems, specifically using Galerkin projection. The setup is mathematically consistent and well-posed. The provided matrices $M$ and $A$ are symmetric and positive definite as stated, ensuring that all subsequent computations are well-defined. The tasks involve standard numerical linear algebra operations for which stable algorithms exist. The problem is objective and free of ambiguity.\n\nThe problem requires the analysis of several projection-based reduced-order models (ROMs) for a given full-order linear system, $\\dot{x}(t) + A x(t) = f(t)$. The state vector is $x(t) \\in \\mathbb{R}^n$, the mass matrix $M \\in \\mathbb{R}^{n \\times n}$ and stiffness matrix $A \\in \\mathbb{R}^{n \\times n}$ are symmetric positive definite (SPD), and $f(t)$ is an input vector.\n\nThe core principle of the projection-based ROM is to approximate the high-dimensional state vector $x(t)$ with a lower-dimensional representation. A trial basis is chosen, represented by the columns of a matrix $V \\in \\mathbb{R}^{n \\times r}$, where $r$ is the reduced dimension, and typically $r \\ll n$. The basis vectors are assumed to be linearly independent, so $V$ has full column rank. The approximation takes the form $x(t) \\approx V z(t)$, where $z(t) \\in \\mathbb{R}^r$ is the vector of reduced state variables (or generalized coordinates).\n\nSubstituting this approximation into the original system equation yields a residual, $R(t) = M V \\dot{z}(t) + A V z(t) - f(t)$. The Galerkin projection method requires this residual to be orthogonal to the subspace spanned by the basis $V$. This orthogonality condition is expressed as $V^\\top R(t) = 0$, which leads to:\n$$\nV^\\top (M V \\dot{z}(t) + A V z(t) - f(t)) = 0\n$$\nRearranging this equation gives the reduced-order model:\n$$\n(V^\\top M V) \\dot{z}(t) + (V^\\top A V) z(t) = V^\\top f(t)\n$$\nThis is a smaller linear system of the form $M_r \\dot{z}(t) + A_r z(t) = f_r(t)$, with the reduced mass matrix $M_r = V^\\top M V \\in \\mathbb{R}^{r \\times r}$ and the reduced stiffness matrix $A_r = V^\\top A V \\in \\mathbb{R}^{r \\times r}$.\n\nGiven that $M$ and $A$ are SPD and $V$ has full column rank, the reduced matrices $M_r$ and $A_r$ are also SPD. For any non-zero vector $u \\in \\mathbb{R}^r$, $v = V u \\in \\mathbb{R}^n$ is also non-zero. Thus, $u^\\top M_r u = (V u)^\\top M (V u) = v^\\top M v > 0$ since $M$ is SPD. The same logic applies to $A_r$.\n\nThe characteristics of the reduced system are analyzed through its matrices.\n1.  The reduced dimension for each case, $r_i$, is the number of columns in the basis matrix $V_i$.\n2.  The spectral condition number of the reduced mass matrix, $\\kappa(M_r)$, influences the stability of time-integration schemes. For an SPD matrix $M_r$, it is defined as the ratio of its largest eigenvalue to its smallest eigenvalue: $\\kappa(M_r) = \\lambda_{\\max}(M_r) / \\lambda_{\\min}(M_r)$. A large condition number indicates that the basis vectors chosen in $V$ are \"seen\" by the mass matrix $M$ as being nearly linearly dependent.\n3.  The dynamic behavior of the reduced system is governed by the generalized eigenvalues of the matrix pair $(A_r, M_r)$. These are the scalars $\\lambda$ that solve the generalized eigenvalue problem $A_r u = \\lambda M_r u$ for a non-zero eigenvector $u$. Since $A_r$ and $M_r$ are SPD, all generalized eigenvalues are real and positive. The minimum and maximum eigenvalues, $\\lambda_{\\min}$ and $\\lambda_{\\max}$, correspond to the slowest and fastest modes of the reduced system, respectively.\n4.  The eigenvalue spread, $s = \\lambda_{\\max} / \\lambda_{\\min}$, is a measure of the stiffness of the reduced system. A large spread indicates a stiff system, which can pose challenges for numerical simulation.\n\nThe computational procedure for each test case is as follows:\n- Define the full-order matrices $M$ and $A$, and the basis matrix $V_i$.\n- Determine the reduced dimension $r_i$ from the shape of $V_i$.\n- Compute $M_{r_i} = V_i^\\top M V_i$ and $A_{r_i} = V_i^\\top A V_i$.\n- Calculate the eigenvalues of $M_{r_i}$ to find its maximum and minimum eigenvalues, and then compute the condition number $\\kappa_i$.\n- Solve the generalized eigenvalue problem for $(A_{r_i}, M_{r_i})$ to find the set of generalized eigenvalues.\n- Identify the minimum ($\\lambda_{\\min,i}$) and maximum ($\\lambda_{\\max,i}$) of these generalized eigenvalues.\n- Compute the spread $s_i = \\lambda_{\\max,i} / \\lambda_{\\min,i}$.\n- Collect and format these five values for each case.\n\nThis procedure will be applied to the four specified test cases, each with a different basis matrix $V_i$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Computes reduced matrices and their spectral properties for four test cases\n    of a projection-based reduced-order model.\n    \"\"\"\n\n    # Define the full-order matrices, n=5\n    M = np.diag([0.8, 1.0, 1.2, 1.0, 0.9])\n    A = np.array([\n        [2, -1, 0, 0, 0],\n        [-1, 2, -1, 0, 0],\n        [0, -1, 2, -1, 0],\n        [0, 0, -1, 2, -1],\n        [0, 0, 0, -1, 2]\n    ])\n\n    # Define the basis matrices for each test case\n    # Case 1: non-orthonormal, r=2\n    V1 = np.array([\n        [1, 1],\n        [1, 2],\n        [1, 3],\n        [1, 2],\n        [1, 1]\n    ], dtype=float)\n\n    # Case 2: canonical coordinate subspace, r=4\n    V2 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [0, 0, 0, 0]\n    ], dtype=float)\n\n    # Case 3: nearly linearly dependent basis, r=3\n    epsilon = 1e-6\n    v1 = np.array([1, 1, 1, 1, 1], dtype=float)\n    v2 = v1 + epsilon * np.array([1, -1, 1, -1, 1], dtype=float)\n    v3 = np.array([1, 0, 0, 0, 0], dtype=float)\n    V3 = np.vstack([v1, v2, v3]).T\n\n    # Case 4: full-order, r=5\n    V4 = np.eye(5)\n\n    test_cases = [V1, V2, V3, V4]\n    \n    all_results = []\n\n    for V in test_cases:\n        # 1. Get reduced dimension\n        r = V.shape[1]\n\n        # 2. Compute reduced matrices\n        Mr = V.T @ M @ V\n        Ar = V.T @ A @ V\n\n        # 3. Compute spectral condition number of Mr\n        # eigvalsh returns sorted eigenvalues for a symmetric matrix\n        eigs_Mr = np.linalg.eigvalsh(Mr)\n        kappa = eigs_Mr[-1] / eigs_Mr[0]\n\n        # 4. Compute generalized eigenvalues of (Ar, Mr)\n        # eigh for generalized problems also returns sorted eigenvalues\n        gen_eigs = eigh(Ar, Mr, eigvals_only=True)\n        lambda_min = gen_eigs[0]\n        lambda_max = gen_eigs[-1]\n        \n        # 5. Compute eigenvalue spread\n        spread = lambda_max / lambda_min\n        \n        all_results.append(\n            [r, kappa, lambda_min, lambda_max, spread]\n        )\n\n    # Format the final output string as specified in the problem\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        r, k, lmin, lmax, s = res\n        output_str += f\"[{r},{k:.8f},{lmin:.8f},{lmax:.8f},{s:.8f}]\"\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the concept of projection, this practice addresses a crucial step for models derived from Finite Element Methods (FEM): constructing a basis that is orthonormal with respect to the mass matrix $M$. You will implement a numerically stable Gram-Schmidt procedure to enforce the $M$-orthonormality condition, $V^T M V = I$, which simplifies the reduced system dynamics. This skill is essential for developing robust reduced-order models for systems where the standard Euclidean geometry does not apply .",
            "id": "3943080",
            "problem": "Consider a semi-discrete model of transport and electrochemical kinetics in a porous electrode, obtained through a consistent spatial discretization of a one-dimensional slab of length $L$ using linear shape functions on a uniform mesh with $n$ nodes. The resulting system of ordinary differential equations can be written in the form $M \\,\\dot{x}(t) = A\\,x(t) + f(t)$, where $M \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive-definite mass matrix capturing the weighted $L^2$ inner product of the finite element space, $A \\in \\mathbb{R}^{n \\times n}$ is a stiffness-like matrix, and $f(t)$ is a source term. In projection-based reduced-order modeling, a reduced basis $V \\in \\mathbb{R}^{n \\times r}$ must satisfy the $M$-orthonormality condition $V^\\top M V = I_r$, where $I_r$ is the identity matrix of size $r$. The $M$-orthonormality is defined with respect to the inner product $\\langle u, v \\rangle_M = u^\\top M v$. \n\nStarting from the core definition of an inner product induced by a symmetric positive-definite matrix and the concept of orthogonal projection, implement a numerically stable modified Gram–Schmidt procedure to transform a given set of candidate vectors $W = [w_1, w_2, \\dots, w_k] \\in \\mathbb{R}^{n \\times k}$ into an $M$-orthonormal basis $Q = [q_1, q_2, \\dots, q_r] \\in \\mathbb{R}^{n \\times r}$ with $r \\leq k$, such that $q_i^\\top M q_j = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. If a candidate vector becomes $M$-nearly linearly dependent upon orthogonalization, it should be discarded. The algorithm must perform one re-orthogonalization pass to improve numerical stability.\n\nUse the following well-tested formula to construct the consistent finite element mass matrix for a uniform mesh: for each element of length $h = L/(n-1)$, the local mass matrix is $(h/6)\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}$, and the global mass matrix $M$ is obtained by standard assembly over $n-1$ elements. This yields a symmetric positive-definite, non-diagonal $M$.\n\nDefine the test suite below. For each case, construct $M$ by assembling the consistent mass matrix as described, and construct $W$ deterministically as indicated. Angles in trigonometric functions must be interpreted in radians.\n\n- Case 1 (general, well-conditioned): $n=8$, $L=1$. Let $x_i = \\frac{i}{n-1}$ for $i=0,1,\\dots,n-1$. Define $W$ with columns $w_1$ with entries $w_{1,i} = \\sin(\\pi x_i)$, $w_2$ with entries $w_{2,i} = \\sin(2\\pi x_i)$, and $w_3$ with entries $w_{3,i} = \\cos(\\pi x_i)$.\n\n- Case 2 (near-linear dependence): $n=10$, $L=1$. Let $x_i = \\frac{i}{n-1}$ for $i=0,1,\\dots,n-1$. Define $W$ with columns $w_1$ with entries $w_{1,i} = \\sin(\\pi x_i)$, $w_2$ with entries $w_{2,i} = w_{1,i} + 10^{-8}\\sin(2\\pi x_i)$, and $w_3$ with entries $w_{3,i} = \\cos(\\pi x_i)$.\n\n- Case 3 (boundary, zero column): $n=6$, $L=1$. Let $x_i = \\frac{i}{n-1}$ for $i=0,1,\\dots,n-1$. Define $W$ with columns $w_1$ with entries $w_{1,i} = 0$, $w_2$ with entries $w_{2,i} = \\sin(\\pi x_i)$, and $w_3$ with entries $w_{3,i} = \\cos(2\\pi x_i)$.\n\nFor each case, compute:\n1. The $M$-orthonormal basis $Q$ using modified Gram–Schmidt with one re-orthogonalization pass and an $M$-norm tolerance of $10^{-10}$ to decide whether to discard a candidate vector.\n2. The Frobenius norm of the orthonormality residual $\\|Q^\\top M Q - I_r\\|_F$ as a floating-point number.\n3. The integer rank $r$ (the number of accepted vectors).\n\nYour program should produce a single line of output containing the results for the three cases aggregated as a comma-separated list enclosed in square brackets in the order: residual for Case 1, rank for Case 1, residual for Case 2, rank for Case 2, residual for Case 3, rank for Case 3. For example, it should print a line like \"[0.0000001,3,0.000002,2,0.0,2]\". No units are required because the quantities are dimensionless.",
            "solution": "The user requires the implementation of a numerically stable modified Gram-Schmidt (MGS) procedure to generate an $M$-orthonormal basis from a given set of candidate vectors. This process is a cornerstone of projection-based reduced-order modeling, particularly for systems derived from finite element (FE) discretizations, such as the battery model described. The basis $Q = [q_1, q_2, \\dots, q_r]$ must satisfy the $M$-orthonormality condition $q_i^\\top M q_j = \\delta_{ij}$, where $M$ is a symmetric positive-definite (SPD) matrix defining a weighted inner product. The implementation must include one re-orthogonalization pass for numerical stability and a mechanism to discard nearly linearly dependent vectors.\n\n### Principle-Based Design\n\n**1. The $M$-Inner Product and $M$-Norm**\n\nFor an SPD matrix $M \\in \\mathbb{R}^{n \\times n}$, the $M$-inner product between two vectors $u, v \\in \\mathbb{R}^n$ is defined as:\n$$\n\\langle u, v \\rangle_M = u^\\top M v\n$$\nThe well-defined properties of an inner product (linearity, symmetry, positive-definiteness) are guaranteed by the properties of the matrix $M$. This inner product induces the $M$-norm, which is the corresponding measure of a vector's magnitude:\n$$\n\\|u\\|_M = \\sqrt{\\langle u, u \\rangle_M} = \\sqrt{u^\\top M u}\n$$\nThese definitions replace the standard Euclidean inner product ($u^\\top v$) and norm ($\\|u\\|_2$) in the Gram-Schmidt procedure.\n\n**2. Finite Element Mass Matrix Assembly**\n\nThe problem specifies a consistent mass matrix $M$ for a one-dimensional uniform mesh of length $L$ with $n$ nodes. The internodal spacing (element length) is $h = L/(n-1)$. For each element connecting a pair of nodes, the local mass matrix is $M_{\\text{local}} = \\frac{h}{6}\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}$. The global mass matrix $M$ is formed by allocating a zero matrix of size $n \\times n$ and then iterating through the $n-1$ elements, adding the local matrix entries to the corresponding global indices. For an element connecting global nodes $i$ and $i+1$, the contributions are added to the submatrix $M([i, i+1], [i, i+1])$. This \"assembly\" process results in an SPD, tridiagonal global matrix $M$ with the following structure:\n$$\nM_{ij} = \\begin{cases}\n    2h/3  \\text{if } i=j \\text{ and } 1 \\le i \\le n-2 \\text{ (interior nodes)} \\\\\n    h/3   \\text{if } i=j \\text{ and } i=0 \\text{ or } i=n-1 \\text{ (boundary nodes)} \\\\\n    h/6   \\text{if } |i-j|=1 \\\\\n    0       \\text{otherwise}\n\\end{cases}\n$$\n\n**3. Modified Gram-Schmidt (MGS) with Re-orthogonalization**\n\nThe MGS algorithm is chosen for its superior numerical stability compared to the classical version. The algorithm iteratively orthogonalizes and normalizes vectors. A re-orthogonalization pass is added to mitigate the loss of orthogonality that can still occur due to floating-point arithmetic.\n\nThe algorithm proceeds as follows, given a set of candidate vectors $W = [w_1, w_2, \\dots, w_k]$:\n\n1.  Let $V$ be a mutable copy of the input vectors $W$. Initialize an empty list, $Q_{\\text{list}}$, to store the resulting $M$-orthonormal basis vectors.\n2.  Iterate through the columns of $V$ from $i = 0$ to $k-1$:\n    a. Let $v$ be the current vector $V[:, i]$. This vector has, through previous steps, been made $M$-orthogonal to all basis vectors already in $Q_{\\text{list}}$.\n    b. **Linear Dependence Check**: Compute the $M$-norm of the vector, $\\rho = \\|v\\|_M$. If $\\rho$ is less than the specified tolerance ($10^{-10}$), the vector is considered numerically dependent on the existing basis. It is discarded, and the algorithm proceeds to the next vector in $V$.\n    c. **Normalization**: If $\\rho$ is above the tolerance, normalize the vector to create a new basis vector $q$:\n       $$\n       q = \\frac{v}{\\rho}\n       $$\n       Append $q$ to the list $Q_{\\text{list}}$.\n    d. **Orthogonalization of Remaining Vectors**: For each subsequent vector $u = V[:, j]$ where $j  i$, remove its component along the new basis vector $q$. This step is performed twice for re-orthogonalization.\n       i.  **First Pass**: Compute the projection coefficient and subtract:\n           $$\n           u \\leftarrow u - \\langle u, q \\rangle_M \\, q\n           $$\n       ii. **Second Pass (Re-orthogonalization)**: Repeat the subtraction to remove any residual component due to numerical error:\n           $$\n           u \\leftarrow u - \\langle u, q \\rangle_M \\, q\n           $$\n           The updated vector $u$ replaces the old version at column $j$ of $V$.\n3.  After processing all vectors in $W$, the final basis $Q$ is constructed by stacking the vectors from $Q_{\\text{list}}$ as columns. The rank $r$ is the number of vectors in this list.\n\n**4. Evaluation Metrics**\n\nThe success of the procedure for each test case is quantified by two metrics:\n\n*   **Rank ($r$)**: The number of vectors in the final basis $Q$, which reflects the dimension of the space spanned by the linearly independent vectors from $W$.\n*   **Orthonormality Residual**: The Frobenius norm of the residual matrix, $\\|Q^\\top M Q - I_r\\|_F$. This value measures how close the matrix $Q^\\top M Q$ is to the identity matrix $I_r$. A value close to zero indicates a high-quality $M$-orthonormal basis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_mass_matrix(n: int, L: float) - np.ndarray:\n    \"\"\"\n    Assembles the 1D finite element consistent mass matrix for linear shape functions\n    on a uniform mesh.\n\n    Args:\n        n: The number of nodes.\n        L: The length of the domain.\n\n    Returns:\n        The (n x n) global mass matrix M.\n    \"\"\"\n    if n  2:\n        return np.array([[L/3]]) if n == 1 else np.empty((0, 0))\n    \n    h = L / (n - 1)\n    M = np.zeros((n, n))\n    \n    # Local mass matrix for an element of length h\n    m_local = (h / 6.0) * np.array([[2, 1],\n                                     [1, 2]])\n\n    # Assembly loop over elements\n    for i in range(n - 1):\n        # Global indices for the current element\n        idx = np.array([i, i + 1])\n        M[np.ix_(idx, idx)] += m_local\n        \n    return M\n\ndef mgs_orthonormalization(W: np.ndarray, M: np.ndarray, tol: float) - np.ndarray:\n    \"\"\"\n    Performs M-orthonormalization on a set of vectors W using the\n    modified Gram-Schmidt algorithm with one re-orthogonalization pass.\n\n    Args:\n        W: An (n x k) matrix where columns are the vectors to be orthonormalized.\n        M: The (n x n) symmetric positive-definite matrix defining the inner product.\n        tol: The tolerance for the M-norm to detect linear dependence.\n\n    Returns:\n        An (n x r) matrix Q where columns form an M-orthonormal basis (r = k).\n    \"\"\"\n    n, k = W.shape\n    V = W.copy().astype(np.float64)\n    Q_list = []\n\n    for i in range(k):\n        v_i = V[:, i]\n        \n        # Compute M-norm\n        m_norm_sq = v_i.T @ M @ v_i\n        if m_norm_sq  0: # Should not happen with SPD M, but for safety\n             m_norm = 0.0\n        else:\n             m_norm = np.sqrt(m_norm_sq)\n\n        # Check for linear dependence\n        if m_norm  tol:\n            continue\n\n        # Normalize to get the new basis vector\n        q = v_i / m_norm\n        Q_list.append(q)\n\n        # Orthogonalize remaining vectors against the new basis vector q\n        # with one re-orthogonalization pass.\n        for j in range(i + 1, k):\n            v_j = V[:, j]\n            \n            # First pass\n            coeff1 = q.T @ M @ v_j\n            v_j = v_j - coeff1 * q\n            \n            # Re-orthogonalization pass\n            coeff2 = q.T @ M @ v_j\n            v_j = v_j - coeff2 * q\n            \n            V[:, j] = v_j\n\n    if not Q_list:\n        return np.empty((n, 0))\n    \n    return np.stack(Q_list, axis=1)\n\ndef solve():\n    \"\"\"\n    Main solver function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 8, \"L\": 1.0,\n            \"W_funcs\": [\n                lambda x: np.sin(np.pi * x),\n                lambda x: np.sin(2 * np.pi * x),\n                lambda x: np.cos(np.pi * x)\n            ]\n        },\n        {\n            \"n\": 10, \"L\": 1.0,\n            \"W_funcs\": [\n                lambda x: np.sin(np.pi * x),\n                lambda x: np.sin(np.pi * x) + 1e-8 * np.sin(2 * np.pi * x),\n                lambda x: np.cos(np.pi * x)\n            ]\n        },\n        {\n            \"n\": 6, \"L\": 1.0,\n            \"W_funcs\": [\n                lambda x: np.zeros_like(x),\n                lambda x: np.sin(np.pi * x),\n                lambda x: np.cos(2 * np.pi * x)\n            ]\n        }\n    ]\n    \n    results = []\n    tol = 1e-10\n\n    for case in test_cases:\n        n, L, W_funcs = case[\"n\"], case[\"L\"], case[\"W_funcs\"]\n        x = np.linspace(0, L, n, dtype=np.float64)\n        \n        # Construct the candidate vectors matrix W\n        W = np.stack([func(x) for func in W_funcs], axis=1)\n        \n        # Construct the mass matrix M\n        M = assemble_mass_matrix(n, L)\n        \n        # Perform M-orthonormalization\n        Q = mgs_orthonormalization(W, M, tol)\n        \n        # Get the rank\n        r = Q.shape[1]\n        \n        # Compute the residual\n        if r > 0:\n            residual_matrix = Q.T @ M @ Q - np.identity(r)\n            residual_norm = np.linalg.norm(residual_matrix, 'fro')\n        else:\n            residual_norm = 0.0\n\n        results.append(residual_norm)\n        results.append(r)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This capstone exercise integrates the concepts of projection and basis construction into a complete workflow for data-driven model reduction. You will generate a Proper Orthogonal Decomposition (POD) basis from simulation snapshots and use an energy-based criterion to select the model's rank, a key step in balancing accuracy and computational cost. By building and evaluating a ROM for a diffusion process, you will gain practical experience in the entire model reduction pipeline, from data generation to performance analysis .",
            "id": "3943087",
            "problem": "You are tasked to implement rank selection for a projection-based reduced-order model of a linear diffusion process representative of solid-phase lithium concentration dynamics in automated battery design and simulation. Start from the following fundamental base.\n\nConsider the one-dimensional diffusion of solid-phase concentration modeled by the parabolic partial differential equation: $$\\frac{\\partial c(x,t)}{\\partial t} = D \\frac{\\partial^2 c(x,t)}{\\partial x^2} + s(t),$$ where $c(x,t)$ is the concentration, $D$ is the diffusion coefficient, and $s(t)$ is a spatially uniform source term representing an intercalation rate that is proportional to the applied current. Assume zero-flux (Neumann) boundary conditions: $$\\frac{\\partial c(0,t)}{\\partial x} = 0,\\quad \\frac{\\partial c(L,t)}{\\partial x} = 0,$$ and an initial condition $c(x,0) = 0$.\n\nDiscretize the spatial domain $[0,L]$ using $n$ equally spaced nodes to obtain a semi-discrete linear time-invariant system using the method of lines. Let $\\Delta x$ be the spatial step size, and let the state vector be $\\mathbf{x}(t) \\in \\mathbb{R}^n$ approximating $c(x,t)$ at the grid points. The semi-discrete dynamics can be written as $$\\frac{d\\mathbf{x}(t)}{dt} = A \\mathbf{x}(t) + B u(t),$$ where $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix representing the second spatial derivative with zero-flux boundary adjustments, $B \\in \\mathbb{R}^{n}$ is a uniform source vector, and $u(t)$ is the scalar input corresponding to the intercalation rate. Define the output as the spatial average $$y(t) = C \\mathbf{x}(t),$$ where $C = \\frac{1}{n} [1,1,\\dots,1] \\in \\mathbb{R}^{1 \\times n}$. Use explicit Euler time stepping with a time step $h$ satisfying the canonical stability limit for the diffusion equation. Explicitly, the discrete-time update is $$\\mathbf{x}_{k+1} = \\mathbf{x}_k + h(A \\mathbf{x}_k + B u_k),$$ and the output is $$y_k = C \\mathbf{x}_k.$$\n\nCollect a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ from a training simulation by stacking the state vectors at $m$ consecutive time steps: $$X = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_m].$$ Compute the Singular Value Decomposition (SVD; Singular Value Decomposition) of $X$, $$X = U \\Sigma V^\\top,$$ where $U \\in \\mathbb{R}^{n \\times k}$ contains the left singular vectors, $\\Sigma \\in \\mathbb{R}^{k \\times k}$ is diagonal with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, and $V \\in \\mathbb{R}^{m \\times k}$ contains the right singular vectors, with $k = \\min(n,m)$. Construct a Proper Orthogonal Decomposition (POD; Proper Orthogonal Decomposition) basis $\\Phi_r \\in \\mathbb{R}^{n \\times r}$ by selecting the first $r$ columns of $U$. Use Galerkin projection to obtain the reduced-order model (ROM; Reduced-Order Model): $$A_r = \\Phi_r^\\top A \\Phi_r,\\quad B_r = \\Phi_r^\\top B,\\quad C_r = C \\Phi_r,$$ and the reduced dynamics $$\\mathbf{z}_{k+1} = \\mathbf{z}_k + h(A_r \\mathbf{z}_k + B_r u_k),\\quad y_k^{\\text{ROM}} = C_r \\mathbf{z}_k,$$ where $\\mathbf{z}_k \\in \\mathbb{R}^r$ is the reduced state.\n\nImplement rank selection via the tolerance criterion $$\\frac{\\sum_{i=1}^r \\sigma_i^2}{\\sum_{i=1}^k \\sigma_i^2} \\ge \\tau,$$ i.e., choose the smallest integer $r$ such that the above inequality holds for a given tolerance $\\tau$ with $0 \\le \\tau \\le 1$. If $\\tau = 1$, set $r = k$. If $\\tau = 0$, set $r = 1$.\n\nAnalyze how the tolerance $\\tau$ influences the ROM accuracy and computational cost by performing the following:\n\n1. Generate training snapshots using a smooth input $u_k^{\\text{train}}$ formed by a combination of sinusoids over a training horizon of $m$ steps. Use these snapshots to compute the SVD and the POD basis.\n2. Evaluate the ROM accuracy using a distinct evaluation input $u_k^{\\text{eval}}$ composed of piecewise-constant pulses over the same number of steps, and compute the output sequence $y_k^{\\text{ROM}}$ versus the full-order output $y_k$.\n3. Define the ROM accuracy metric as the relative root-mean-square error across the evaluation horizon: $$\\varepsilon_{\\text{rel}} = \\sqrt{\\frac{\\sum_{k=1}^{m} \\left(y_k^{\\text{ROM}} - y_k\\right)^2}{\\sum_{k=1}^{m} y_k^2}}.$$ Report this as a dimensionless float.\n4. Define a computational cost proxy per time step for online simulation as the ratio of the reduced dense matrix-vector multiplication count to the full tridiagonal multiplication count: $$\\rho_{\\text{cost}} = \\frac{r^2}{3n}.$$ This proxy is dimensionless and captures the difference between dense reduced dynamics and banded full dynamics.\n\nUse the following physically plausible and self-consistent parameters:\n- Number of spatial nodes $n = 50$.\n- Domain length $L = 5 \\times 10^{-6}$ meters.\n- Diffusion coefficient $D = 1 \\times 10^{-14}$ square meters per second.\n- Spatial step $\\Delta x = \\frac{L}{n-1}$.\n- Time step $h = 0.4 \\frac{\\Delta x^2}{D}$ seconds (guaranteeing stability for explicit Euler for the diffusion operator).\n- Number of training and evaluation steps $m = 300$.\n- Uniform source vector $B = \\beta \\mathbf{1}$ with $\\beta = 1$ (dimensionless scaling for numerical analysis).\n- Initial condition $\\mathbf{x}_0 = \\mathbf{0}$.\n\nConstruct the training input as $$u_k^{\\text{train}} = a \\sin\\left(2\\pi \\frac{k}{m}\\right) + b \\sin\\left(4\\pi \\frac{k}{m}\\right),$$ where $a = 1$ and $b = 0.5$. Construct the evaluation input as the following piecewise-constant sequence: for $k \\in \\{1,\\dots,m\\}$, $$u_k^{\\text{eval}} = \\begin{cases} u_1  \\text{if } k \\le \\frac{m}{3}, \\\\ u_2  \\text{if } \\frac{m}{3}  k \\le \\frac{2m}{3}, \\\\ u_3  \\text{if } k  \\frac{2m}{3}, \\end{cases}$$ with $u_1 = 1$, $u_2 = -0.5$, and $u_3 = 0.2$.\n\nTest Suite and Answer Specification:\n- Use the following tolerance values as test cases: $\\tau \\in \\{0, 0.1, 0.5, 0.9, 0.99, 1\\}$.\n- For each $\\tau$ value, compute the selected rank $r$, the relative error $\\varepsilon_{\\text{rel}}$, and the computational cost proxy $\\rho_{\\text{cost}}$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[r,\\varepsilon_{\\text{rel}},\\rho_{\\text{cost}}]$. For example, the output format must be exactly like: $[[r_1,\\varepsilon_1,\\rho_1],[r_2,\\varepsilon_2,\\rho_2],\\dots]$.\n\nAll computations are to be performed in real numbers. The final outputs are dimensionless floats or integers, and no physical units are required in the final outputs.",
            "solution": "The user has provided a comprehensive problem statement for implementing and evaluating a projection-based reduced-order model (ROM) for a 1D linear diffusion process. The task involves several standard steps in numerical methods and model order reduction: spatial discretization, time integration, snapshot generation, Proper Orthogonal Decomposition (POD) via Singular Value Decomposition (SVD), Galerkin projection, and finally, evaluation of the resulting ROMs for different ranks. The rank selection is to be performed based on an energy tolerance criterion applied to the singular values.\n\n### Step 1: Extract Givens\n- **Governing PDE:** $\\frac{\\partial c(x,t)}{\\partial t} = D \\frac{\\partial^2 c(x,t)}{\\partial x^2} + s(t)$\n- **Boundary Conditions:** Zero-flux (Neumann) at $x=0$ and $x=L$: $\\frac{\\partial c(0,t)}{\\partial x} = 0, \\frac{\\partial c(L,t)}{\\partial x} = 0$.\n- **Initial Condition:** $c(x,0) = 0$.\n- **Full-Order Model (FOM):** Semi-discrete system $\\frac{d\\mathbf{x}(t)}{dt} = A \\mathbf{x}(t) + B u(t)$, with output $y(t) = C \\mathbf{x}(t)$.\n- **Time Discretization:** Explicit Euler, $\\mathbf{x}_{k+1} = \\mathbf{x}_k + h(A \\mathbf{x}_k + B u_k)$, output $y_k = C \\mathbf{x}_k$.\n- **Parameters:**\n    - Spatial nodes: $n = 50$.\n    - Domain length: $L = 5 \\times 10^{-6}$ m.\n    - Diffusion coefficient: $D = 1 \\times 10^{-14}$ m²/s.\n    - Spatial step: $\\Delta x = \\frac{L}{n-1}$.\n    - Time step: $h = 0.4 \\frac{\\Delta x^2}{D}$.\n    - Simulation steps: $m = 300$.\n    - Source vector: $B = \\beta \\mathbf{1}$ with $\\beta = 1$.\n    - Output vector: $C = \\frac{1}{n} [1,1,\\dots,1]$.\n    - Initial state: $\\mathbf{x}_0 = \\mathbf{0}$.\n- **Training Input:** $u_k^{\\text{train}} = \\sin(2\\pi \\frac{k}{m}) + 0.5 \\sin(4\\pi \\frac{k}{m})$ for $k \\in \\{1, \\dots, m\\}$.\n- **Evaluation Input:** Piecewise-constant $u_k^{\\text{eval}}$ for $k \\in \\{1, \\dots, m\\}$: $1$ for $k \\le m/3$, $-0.5$ for $m/3  k \\le 2m/3$, and $0.2$ for $k  2m/3$.\n- **Model Reduction:**\n    - Snapshot matrix: $X = [\\mathbf{x}_1, \\dots, \\mathbf{x}_m]$.\n    - POD basis $\\Phi_r$ from the first $r$ left singular vectors of $X$.\n    - Galerkin projection: $A_r = \\Phi_r^\\top A \\Phi_r, B_r = \\Phi_r^\\top B, C_r = C \\Phi_r$.\n- **Rank Selection:** Smallest integer $r$ such that $\\frac{\\sum_{i=1}^r \\sigma_i^2}{\\sum_{i=1}^k \\sigma_i^2} \\ge \\tau$. Special cases: $r=1$ for $\\tau=0$, $r=k$ for $\\tau=1$, where $k=\\min(n,m)$.\n- **Metrics:**\n    - Relative error: $\\varepsilon_{\\text{rel}} = \\sqrt{\\frac{\\sum_{k=1}^{m} (y_k^{\\text{ROM}} - y_k)^2}{\\sum_{k=1}^{m} y_k^2}}$.\n    - Cost proxy: $\\rho_{\\text{cost}} = \\frac{r^2}{3n}$.\n- **Test Cases:** $\\tau \\in \\{0, 0.1, 0.5, 0.9, 0.99, 1\\}$.\n- **Output Format:** A comma-separated list of lists: $[[r_1,\\varepsilon_1,\\rho_1],[r_2,\\varepsilon_2,\\rho_2],\\dots]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound.\n1.  **Scientifically Grounded:** The diffusion equation, Neumann boundary conditions, and model order reduction via POD-Galerkin are standard, well-established techniques in engineering and physics. The chosen parameters are physically plausible for lithium-ion battery modeling. The time step $h = 0.4 \\frac{\\Delta x^2}{D}$ respects the stability limit for explicit Euler on a diffusion problem, which is $h \\le 0.5 \\frac{\\Delta x^2}{D}$.\n2.  **Well-Posed:** All necessary parameters, equations, initial/boundary conditions, and procedures are defined. The construction of the system matrix $A$ from the second derivative operator with Neumann boundary conditions is a standard procedure in numerical PDEs. The problem is self-contained and free of contradictions. The metrics are unambiguous.\n3.  **Objective:** The problem is stated using precise mathematical and algorithmic language, free from subjectivity or opinion.\n4.  **No other flaws detected:** The problem is not trivial, unrealistic, or un-verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be developed.\n\n### Solution Design\n\nThe solution proceeds by systematically implementing the steps described in the problem statement.\n\n1.  **FOM Construction:** The first step is to construct the matrices for the full-order model (FOM). The spatial domain is discretized into $n$ nodes. The system matrix $A$ is constructed as an $n \\times n$ matrix representing the finite difference approximation of the Laplacian operator $D \\frac{\\partial^2}{\\partial x^2}$. For interior nodes, this leads to the standard $[1, -2, 1]$ stencil. The zero-flux Neumann boundary conditions are implemented using a second-order accurate ghost point method, resulting in modified first and last rows of the matrix. The $B$ and $C$ vectors are constructed as defined.\n\n2.  **Snapshot Generation:** A training simulation is run using the FOM and the specified sinusoidal input $u_k^{\\text{train}}$. The state vector $\\mathbf{x}_k$ is saved at each of the $m$ time steps. These state vectors are stacked as columns to form the $n \\times m$ snapshot matrix $X$. The simulation starts from a zero initial condition $\\mathbf{x}_0=\\mathbf{0}$ and evolves according to the explicit Euler rule.\n\n3.  **SVD and Basis Generation:** The Singular Value Decomposition (SVD) of the snapshot matrix $X$ is computed, yielding the left singular vectors $U$, the singular values $\\sigma_i$, and the right singular vectors. The columns of $U$ form the POD modes, which are an optimal orthonormal basis for the snapshot data in the least-squares sense.\n\n4.  **Evaluation Data Generation:** To provide an unbiased assessment of the ROMs, a separate evaluation is performed. The true system output, $y_k$, is computed by simulating the FOM with a distinct, piecewise-constant input signal $u_k^{\\text{eval}}$. This sequence serves as the \"ground truth\" against which the ROMs will be compared.\n\n5.  **ROM Creation and Evaluation Loop:** The core of the solution is a loop over the specified tolerance values $\\tau$. For each $\\tau$:\n    a.  **Rank Selection:** The singular values are used to determine the required ROM rank $r$. The squared singular values $\\sigma_i^2$ represent the \"energy\" of each POD mode. The rank $r$ is chosen as the minimum number of modes whose cumulative energy meets the fraction $\\tau$ of the total energy. The special cases for $\\tau=0$ and $\\tau=1$ are handled as specified.\n    b.  **ROM Construction:** A POD basis $\\Phi_r$ of size $n \\times r$ is formed by taking the first $r$ columns of $U$. The FOM matrices are then projected onto this basis via Galerkin projection to create the reduced-order system matrices $A_r$, $B_r$, and $C_r$.\n    c.  **ROM Simulation:** The reduced-order model is simulated with the evaluation input $u_k^{\\text{eval}}$ to generate the ROM output sequence $y_k^{\\text{ROM}}$.\n    d.  **Metric Calculation:** The relative root-mean-square error $\\varepsilon_{\\text{rel}}$ and the computational cost proxy $\\rho_{\\text{cost}}$ are calculated based on their definitions. The results $(r, \\varepsilon_{\\text{rel}}, \\rho_{\\text{cost}})$ are stored.\n\n6.  **Final Output:** After iterating through all tolerance values, the collected results are formatted into a single string as specified by the problem and printed.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a projection-based reduced-order model for a 1D diffusion process.\n    \"\"\"\n    # --- 1. Define Parameters and Constants ---\n    n = 50          # Number of spatial nodes\n    L = 5e-6        # Domain length (m)\n    D = 1e-14       # Diffusion coefficient (m^2/s)\n    m = 300         # Number of time steps\n    beta = 1.0      # Source term scaling\n    \n    # Discretization parameters\n    dx = L / (n - 1)\n    h = 0.4 * dx**2 / D\n    \n    # Test cases for tolerance\n    test_cases = [0.0, 0.1, 0.5, 0.9, 0.99, 1.0]\n\n    # --- 2. Construct Full-Order Model (FOM) Matrices ---\n    # A matrix (D * Laplacian with Neumann BCs)\n    A_stencil = np.zeros((n, n))\n    diag_main = np.full(n, -2.0)\n    diag_off = np.full(n - 1, 1.0)\n    np.fill_diagonal(A_stencil, diag_main)\n    np.fill_diagonal(A_stencil[1:], diag_off, wrap=False)\n    np.fill_diagonal(A_stencil[:, 1:], diag_off, wrap=False)\n    # Apply Neumann boundary conditions via ghost point method\n    A_stencil[0, 1] = 2.0\n    A_stencil[n - 1, n - 2] = 2.0\n    A = (D / dx**2) * A_stencil\n\n    # B vector (uniform source)\n    B = beta * np.ones(n)\n\n    # C vector (spatial average)\n    C = (1.0 / n) * np.ones((1, n))\n\n    # --- 3. Generate Training Data (Snapshots) ---\n    # Training input signal, u_k for k in {1, ..., m}\n    a_train, b_train = 1.0, 0.5\n    k_steps = np.arange(1, m + 1)\n    u_train = a_train * np.sin(2 * np.pi * k_steps / m) + b_train * np.sin(4 * np.pi * k_steps / m)\n\n    # Simulate FOM to get snapshots X = [x_1, ..., x_m]\n    X = np.zeros((n, m))\n    x_current = np.zeros(n)  # Initial condition x_0\n    for k in range(m):\n        # Explicit Euler update: x_{k+1} = x_k + h * (A*x_k + B*u_{k+1})\n        x_current = x_current + h * (A @ x_current + B * u_train[k])\n        X[:, k] = x_current\n\n    # --- 4. SVD of Snapshot Matrix ---\n    # Decompose X = U * Sigma * Vh. Use full_matrices=False for efficiency (n  m)\n    U, s, Vh = np.linalg.svd(X, full_matrices=False)\n    \n    # --- 5. Generate Evaluation Data (FOM Ground Truth) ---\n    # Evaluation input signal, u_k for k in {1, ..., m}\n    u_eval = np.zeros(m)\n    third_m = m // 3\n    u_eval[0:third_m] = 1.0\n    u_eval[third_m:2 * third_m] = -0.5\n    u_eval[2 * third_m:m] = 0.2\n\n    # Simulate FOM with evaluation input to get the true output y_k\n    y_fom_list = []\n    x_fom = np.zeros(n)\n    for k in range(m):\n        x_fom = x_fom + h * (A @ x_fom + B * u_eval[k])\n        y_fom_list.append(C @ x_fom)\n    y_fom = np.array(y_fom_list).flatten()\n    \n    # Pre-calculate singular value energies for rank selection\n    s_sq = s**2\n    total_energy = np.sum(s_sq)\n    cumulative_energy_ratio = np.cumsum(s_sq) / total_energy\n    \n    # --- 6. Loop over Tolerances to Build and Evaluate ROMs ---\n    results = []\n    k_max_rank = U.shape[1] # max possible rank is min(n, m)\n\n    for tau in test_cases:\n        # a) Rank Selection\n        if tau == 0.0:\n            r = 1\n        elif tau == 1.0:\n            r = k_max_rank\n        else:\n            # Find smallest r such that cumulative energy = tau * total_energy\n            r = np.searchsorted(cumulative_energy_ratio, tau, side='left') + 1\n\n        # b) Build ROM via Galerkin Projection\n        Phi_r = U[:, :r]\n        Ar = Phi_r.T @ A @ Phi_r\n        Br = Phi_r.T @ B\n        Cr = C @ Phi_r\n\n        # c) Simulate ROM with evaluation input\n        y_rom_list = []\n        z_rom = np.zeros(r) # Initial reduced state\n        for k in range(m):\n            z_rom = z_rom + h * (Ar @ z_rom + Br * u_eval[k])\n            y_rom_list.append(Cr @ z_rom)\n        y_rom = np.array(y_rom_list).flatten()\n\n        # d) Calculate Metrics\n        # Relative root-mean-square error\n        err_num = np.sum((y_rom - y_fom)**2)\n        err_den = np.sum(y_fom**2)\n        \n        eps_rel = 0.0 if err_den == 0 else np.sqrt(err_num / err_den)\n\n        # Computational cost proxy\n        rho_cost = (r**2) / (3.0 * n)\n\n        results.append([r, eps_rel, rho_cost])\n    \n    # --- 7. Format and Print Final Output ---\n    output_str = \",\".join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}