{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a foundational, hands-on understanding of a key limitation of global Proper Orthogonal Decomposition. While POD is optimal for compressing a given dataset in an energy sense, its efficiency degrades significantly for problems dominated by advection or translation. By numerically implementing POD for a simple translating Gaussian pulse, you will directly observe the slow decay of singular values and the resulting high reconstruction error, providing crucial intuition for when more advanced techniques are necessary .",
            "id": "3265968",
            "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_1, E_2, E_5, E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like `[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]`, with no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Snapshot function**: $u(x,t) = \\exp(-(x - c t)^2)$\n- **Spatial domain**: $x \\in [-L,L]$\n- **Time domain**: $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$, which are $m$ equally spaced points in $[0,T]$.\n- **Spatial discretization**: $N_x$ uniformly spaced grid points in $[-L,L]$.\n- **Snapshot matrix**: $X \\in \\mathbb{R}^{N_x \\times m}$, where the $k$-th column is the sampled snapshot $u(x,t_k)$.\n- **Task**: Compute the best rank-$r$ approximation $X_r$ using Proper Orthogonal Decomposition (POD) for ranks $r \\in \\{1, 2, 5, 10\\}$.\n- **Error metric**: Relative reconstruction error $E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F}$, where $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n- **Constants**: $L = 10$, $N_x = 401$.\n- **Test cases**:\n    1. Case A: $c = 0$, $T = 5$, $m = 50$.\n    2. Case B: $c = 0.5$, $T = 10$, $m = 100$.\n    3. Case C: $c = 2.0$, $T = 4$, $m = 80$.\n    4. Case D: $c = 0.5$, $T = 10$, $m = 5$.\n- **Output specification**: A single-line comma-separated list of lists, e.g., `[[e_{A,1},...,e_{A,10}],[e_{B,1},...,e_{B,10}],...]`, with numbers rounded to six decimal places and no spaces.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard application of Proper Orthogonal Decomposition (POD), a cornerstone of reduced-order modeling in scientific computing. The mathematical foundation is the Singular Value Decomposition (SVD), which is a principal result in linear algebra. The physics is a simple translating Gaussian pulse, which is a common and valid test case. The problem is scientifically sound.\n- **Well-Posed**: All necessary parameters ($L, N_x, c, T, m$) are provided for each case. The function $u(x,t)$, the procedure for constructing the snapshot matrix $X$, and the error metric $E_r$ are all defined unambiguously. The SVD provides a unique construction for the best rank-$r$ approximation, ensuring a unique solution exists.\n- **Objective**: The problem is expressed in precise mathematical terms, devoid of any subjectivity, ambiguity, or opinion-based statements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, self-contained, and scientifically sound problem in the field of numerical methods. A complete solution will be provided.\n\n### Principle-Based Solution\nThe objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Selecting the optimal rank $r$ is a critical step in building any reduced-order model, requiring a balance between accuracy and compactness. This exercise moves beyond simple heuristics, guiding you to implement a robust, data-driven approach using K-fold cross-validation. By learning to quantify and minimize generalization error on held-out data, you will develop a core skill for constructing reliable and parsimonious POD models applicable to a wide range of scientific problems .",
            "id": "3178045",
            "problem": "You are given a task to implement a model order selection procedure for Proper Orthogonal Decomposition (POD) using cross-validation by reconstructing held-out snapshots and quantifying generalization error. Proper Orthogonal Decomposition (POD) seeks orthonormal modes that optimally capture variance in a set of snapshots under the Euclidean norm. Use Singular Value Decomposition (SVD) to compute orthonormal modes from training snapshots and orthogonal projection to reconstruct held-out snapshots. Use K-fold Cross-Validation (CV) to estimate generalization error. Your program must compute the best rank $r$ for several specified datasets by minimizing the average held-out reconstruction error and must produce the final results in the specified format.\n\nUse the following fundamental basis:\n- Linear algebra of orthogonal projections: For a data matrix $X \\in \\mathbb{R}^{m \\times n}$, if $U_r \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns, then the orthogonal projection of a centered vector $x \\in \\mathbb{R}^m$ onto the subspace spanned by the columns of $U_r$ is $U_r U_r^\\top x$.\n- Singular Value Decomposition (SVD): Any real matrix $A \\in \\mathbb{R}^{m \\times n}$ admits a decomposition $A = U \\Sigma V^\\top$ with $U \\in \\mathbb{R}^{m \\times q}$ and $V \\in \\mathbb{R}^{n \\times q}$ having orthonormal columns, diagonal nonnegative singular values in $\\Sigma \\in \\mathbb{R}^{q \\times q}$, and $q = \\min(m,n)$.\n- The best rank-$r$ subspace in the least-squares sense for centered training data is spanned by the first $r$ left singular vectors.\n\nYour algorithm must implement the following procedure for a given snapshot matrix $X \\in \\mathbb{R}^{m \\times n}$, a fold count $K \\in \\mathbb{N}$ with $2 \\le K \\le n$, and a candidate set of ranks $\\mathcal{R} = \\{0,1,2,\\dots,r_{\\max}\\}$:\n- Partition the column indices $\\{0,1,\\dots,n-1\\}$ into $K$ folds deterministically: for fold $k \\in \\{0,1,\\dots,K-1\\}$, the validation set is the set of indices $J_k = \\{ j \\in \\{0,\\dots,n-1\\} : j \\bmod K = k \\}$; the training set is the complement.\n- For each candidate rank $r \\in \\mathcal{R}$:\n  1. For each fold $k$:\n     - Form the training matrix $X_{\\text{train}}$ by selecting columns of $X$ with indices not in $J_k$, and the validation matrix $X_{\\text{val}}$ using columns indexed by $J_k$.\n     - Compute the training mean $\\mu = \\frac{1}{n_{\\text{train}}} \\sum_{x \\in X_{\\text{train}}} x$.\n     - Center the data: $A = X_{\\text{train}} - \\mu \\mathbf{1}^\\top$ and $B = X_{\\text{val}} - \\mu \\mathbf{1}^\\top$.\n     - Compute the singular value decomposition $A = U \\Sigma V^\\top$ with $U \\in \\mathbb{R}^{m \\times q}$, where $q = \\min(m, n_{\\text{train}})$, and form $U_r$ by taking the first $r$ columns of $U$ (if $r = 0$, define $U_r$ as an empty matrix so the projection is the zero operator).\n     - Reconstruct the validation data by orthogonal projection: $\\widehat{X}_{\\text{val}} = \\mu \\mathbf{1}^\\top + U_r U_r^\\top B$.\n     - Compute the fold error as the relative Frobenius norm $e_k(r) = \\frac{\\| X_{\\text{val}} - \\widehat{X}_{\\text{val}} \\|_F}{\\| X_{\\text{val}} \\|_F}$.\n  2. Compute the cross-validation error as the average $\\overline{e}(r) = \\frac{1}{K} \\sum_{k=0}^{K-1} e_k(r)$.\n- Select the best rank $r^\\star$ that minimizes $\\overline{e}(r)$, breaking ties by choosing the smallest $r$. In numerical computations, treat two errors as equal if their absolute difference is less than $\\varepsilon = 10^{-12}$.\n\nImplement the above procedure and apply it to the following test suite. In every case, columns of $X$ are the snapshots to be reconstructed. All data are purely mathematical; no physical units are involved.\n\nTest case $1$ (happy path, rank-$1$ data):\n- Dimensions: $m = 5$, $n = 8$.\n- Construct a nonzero vector $v \\in \\mathbb{R}^5$ as $v = [1,2,3,4,5]^\\top$ and scalars $s = [1,2,3,4,5,6,7,8]$.\n- Form snapshots $x_j = s_j \\, v$ for $j \\in \\{0,\\dots,7\\}$ and assemble $X = [x_0, x_1, \\dots, x_7]$.\n- Use $K = 4$ and candidate ranks $\\mathcal{R} = \\{0,1,2,3,4\\}$.\n\nTest case $2$ (rank-$2$ data, multi-fold coverage):\n- Dimensions: $m = 6$, $n = 9$.\n- Let $U_2 \\in \\mathbb{R}^{6 \\times 2}$ be the first two columns of the $6 \\times 6$ identity.\n- Let the coefficient matrix $C \\in \\mathbb{R}^{2 \\times 9}$ have columns\n  $$\n  C = \\begin{bmatrix}\n  1 & 0 & 1 & 2 & 1 & 2 & 3 & 1 & 3 \\\\\n  0 & 1 & 1 & 1 & 2 & 2 & 1 & 3 & 3\n  \\end{bmatrix}.\n  $$\n- Form $X = U_2 C$.\n- Use $K = 3$ and candidate ranks $\\mathcal{R} = \\{0,1,2,3,4,5\\}$.\n\nTest case $3$ (boundary case $r = 0$ optimal; constant snapshots):\n- Dimensions: $m = 4$, $n = 7$.\n- Let $c = 3$ and $u = [1,1,1,1]^\\top$.\n- Set all snapshots equal to $x_j = c \\, u$ for $j \\in \\{0,\\dots,6\\}$, and form $X = [x_0, \\dots, x_6]$.\n- Use $K = 7$ and candidate ranks $\\mathcal{R} = \\{0,1,2,3\\}$.\n\nTest case $4$ (rank-$3$ data with higher ambient dimension):\n- Dimensions: $m = 7$, $n = 12$.\n- Let $U_3 \\in \\mathbb{R}^{7 \\times 3}$ be the first three columns of the $7 \\times 7$ identity.\n- Let the coefficient matrix $C \\in \\mathbb{R}^{3 \\times 12}$ have columns\n  $$\n  C = \\begin{bmatrix}\n  1 & 0 & 0 & 1 & 1 & 0 & 1 & 2 & 0 & 1 & 2 & 1 \\\\\n  0 & 1 & 0 & 1 & 0 & 1 & 1 & 1 & 2 & 0 & 2 & 2 \\\\\n  0 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 2 & 1 & 2\n  \\end{bmatrix}.\n  $$\n- Form $X = U_3 C$.\n- Use $K = 4$ and candidate ranks $\\mathcal{R} = \\{0,1,2,3,4,5,6\\}$.\n\nComputational details:\n- Always mean-center using only the training columns within each fold and add the mean back after projection.\n- Use the orthogonal projection $U_r U_r^\\top$ for reconstruction of centered validation data.\n- Use the relative Frobenius norm for errors.\n- If $r = 0$, define the reconstruction as the mean $\\mu$ replicated across validation columns.\n- Tie-breaking tolerance is $\\varepsilon = 10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the selected ranks for the four test cases as a comma-separated list enclosed in square brackets, for example, `[r_1,r_2,r_3,r_4]`, where each $r_i$ is an integer. There must be no spaces in the output line.",
            "solution": "The solution implements a model order selection procedure for Proper Orthogonal Decomposition (POD) using K-fold cross-validation. This method systematically determines the optimal number of basis vectors (the rank $r$) needed to represent a dataset by balancing model fidelity against its ability to generalize to unseen data.\n\nThe core principle behind POD is to find an orthonormal basis that is optimal for representing a given set of data snapshots, typically in a least-squares sense. For a set of $n$ snapshots in $\\mathbb{R}^m$, organized as columns of a matrix $X \\in \\mathbb{R}^{m \\times n}$, we first compute the temporal mean of the snapshots, $\\mu = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i$. The fluctuations around the mean are captured in the centered data matrix $A = X - \\mu \\mathbf{1}^\\top$. Singular Value Decomposition (SVD) of this matrix, $A = U \\Sigma V^\\top$, provides the optimal basis. The columns of $U$ are the POD modes, which are orthonormal vectors. The singular values on the diagonal of $\\Sigma$, sorted in descending order $\\sigma_0 \\ge \\sigma_1 \\ge \\dots$, quantify the amount of variance (or \"energy\") captured by each corresponding mode. The Eckart-Young-Mirsky theorem guarantees that the subspace spanned by the first $r$ columns of $U$ is the best rank-$r$ subspace for approximating the columns of $A$.\n\nThe problem is to select the rank $r$. A rank that is too low may underfit the data, while a rank that is too high may overfit by capturing noise and other artifacts specific to the training data, leading to poor generalization. Cross-validation is a standard technique to estimate this generalization performance. The provided algorithm uses K-fold cross-validation. The snapshot dataset is partitioned into $K$ disjoint subsets (folds). The procedure iterates $K$ times; in each iteration $k$, one fold is held out as the validation set ($X_{\\text{val}}$), and the remaining $K-1$ folds are used as the training set ($X_{\\text{train}}$).\n\nThe algorithm proceeds as follows for each candidate rank $r$ from a set $\\mathcal{R}$:\n$1$. For each fold $k \\in \\{0, \\dots, K-1\\}$:\n   a. The mean $\\mu$ is computed *only* from the training data, $X_{\\text{train}}$. This is crucial to avoid information leakage from the validation set into the model training process.\n   b. Both training and validation data are centered using this training mean: $A = X_{\\text{train}} - \\mu \\mathbf{1}^\\top$ and $B = X_{\\text{val}} - \\mu \\mathbf{1}^\\top$.\n   c. The SVD of the centered training matrix $A$ is computed to find the POD modes, yielding the matrix $U$. The first $r$ columns of $U$ form the rank-$r$ basis, $U_r$.\n   d. The centered validation snapshots in $B$ are reconstructed by orthogonal projection onto the subspace spanned by the columns of $U_r$. The reconstructed centered data is $\\widehat{B} = U_r U_r^\\top B$. For the case $r=0$, this projection is the zero operator, so $\\widehat{B}$ is a zero matrix.\n   e. The full reconstructed snapshots are then $\\widehat{X}_{\\text{val}} = \\mu \\mathbf{1}^\\top + \\widehat{B}$. For $r=0$, this means the reconstruction is simply the training mean.\n   f. The reconstruction error for this fold is quantified using the relative Frobenius norm: $e_k(r) = \\frac{\\| X_{\\text{val}} - \\widehat{X}_{\\text{val}} \\|_F}{\\| X_{\\text{val}} \\|_F}$. This measures the size of the error relative to the size of the original validation data.\n\n$2$. After iterating through all $K$ folds, the errors for a given rank $r$ are averaged to produce the cross-validation error: $\\overline{e}(r) = \\frac{1}{K} \\sum_{k=0}^{K-1} e_k(r)$. This average error provides a more robust estimate of the model's generalization performance than a single train-test split.\n\n$3$. Finally, the best rank $r^\\star$ is selected as the one that minimizes the average cross-validation error, $\\overline{e}(r)$. According to the problem specification, any ties are broken by selecting the smallest rank, which adheres to the principle of parsimony. A numerical tolerance of $\\varepsilon = 10^{-12}$ is used to determine if two error values are equal.\n\nThis procedure is applied to each of the four test cases specified, yielding the optimal rank for each dataset.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _find_best_rank(X, K, r_candidates):\n    \"\"\"\n    Finds the best POD rank using K-fold cross-validation as specified.\n\n    Args:\n        X (np.ndarray): Snapshot matrix of size (m, n).\n        K (int): Number of folds for cross-validation.\n        r_candidates (list): List of candidate ranks to evaluate.\n\n    Returns:\n        int: The best rank found.\n    \"\"\"\n    m, n = X.shape\n    epsilon = 1e-12\n    avg_errors = []\n\n    all_indices = np.arange(n)\n\n    for r in r_candidates:\n        fold_errors = []\n        for k in range(K):\n            # 1. Partition data deterministically into training and validation sets\n            val_indices = all_indices[k::K]\n            train_indices = np.setdiff1d(all_indices, val_indices, assume_unique=True)\n\n            X_train = X[:, train_indices]\n            X_val = X[:, val_indices]\n            \n            # 2. Compute training mean\n            mu = np.mean(X_train, axis=1, keepdims=True)\n\n            # 3. Center data using training mean\n            A = X_train - mu\n            B = X_val - mu\n            \n            # 4. Compute SVD of centered training data\n            # Use full_matrices=False for thin SVD\n            U, _, _ = np.linalg.svd(A, full_matrices=False)\n            \n            # 5. Reconstruct validation data via projection\n            if r == 0:\n                B_recon = np.zeros_like(B)\n            else:\n                # Ensure we don't request more modes than available\n                rank_limit = min(r, U.shape[1])\n                Ur = U[:, :rank_limit]\n                # Orthogonal projection of centered validation data\n                B_recon = Ur @ (Ur.T @ B)\n            \n            # Reconstruct the full validation data for error calculation\n            X_val_recon = mu + B_recon\n\n            # 6. Compute fold error as relative Frobenius norm\n            norm_diff = np.linalg.norm(X_val - X_val_recon, 'fro')\n            norm_val = np.linalg.norm(X_val, 'fro')\n\n            if norm_val  epsilon:\n                # If validation data norm is effectively zero, the relative error\n                # is 0 if the difference is also zero, otherwise handle appropriately.\n                fold_error = 0.0 if norm_diff  epsilon else 1.0\n            else:\n                fold_error = norm_diff / norm_val\n            \n            fold_errors.append(fold_error)\n\n        # 7. Compute average error across folds for the current rank r\n        avg_errors.append(np.mean(fold_errors))\n\n    # 8. Select best rank: minimizes average error, with smallest rank as tie-breaker\n    avg_errors = np.array(avg_errors)\n    min_error = np.min(avg_errors)\n    \n    # Find all ranks that achieve the minimum error within the tolerance\n    best_indices = np.where(avg_errors = min_error + epsilon)[0]\n\n    # The first index corresponds to the smallest rank due to sorted r_candidates\n    best_rank = r_candidates[best_indices[0]]\n    \n    return best_rank\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = []\n    \n    # Test case 1 (happy path, rank-1 data)\n    m1, n1 = 5, 8\n    v1 = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n    s1 = np.arange(1, 9)\n    X1 = v1 @ s1.reshape(1, -1)\n    K1, R1 = 4, list(range(5))\n    test_cases.append((X1, K1, R1))\n\n    # Test case 2 (rank-2 data, multi-fold coverage)\n    m2, n2 = 6, 9\n    U_basis2 = np.eye(m2)[:, :2]\n    C2 = np.array([\n        [1, 0, 1, 2, 1, 2, 3, 1, 3],\n        [0, 1, 1, 1, 2, 2, 1, 3, 3]\n    ], dtype=float)\n    X2 = U_basis2 @ C2\n    K2, R2 = 3, list(range(6))\n    test_cases.append((X2, K2, R2))\n\n    # Test case 3 (boundary case r = 0 optimal; constant snapshots)\n    m3, n3 = 4, 7\n    c3 = 3.0\n    u3 = np.ones((m3, 1))\n    X3 = c3 * u3 @ np.ones((1, n3))\n    K3, R3 = 7, list(range(4))\n    test_cases.append((X3, K3, R3))\n    \n    # Test case 4 (rank-3 data with higher ambient dimension)\n    m4, n4 = 7, 12\n    U_basis4 = np.eye(m4)[:, :3]\n    C4 = np.array([\n        [1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 2, 1],\n        [0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 2],\n        [0, 0, 1, 0, 1, 1, 1, 0, 1, 2, 1, 2]\n    ], dtype=float)\n    X4 = U_basis4 @ C4\n    K4, R4 = 4, list(range(7))\n    test_cases.append((X4, K4, R4))\n\n    results = []\n    for X, K, r_candidates in test_cases:\n        best_rank = _find_best_rank(X, K, r_candidates)\n        results.append(best_rank)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For applications in automated design and optimization, we need reduced-order models that are accurate over a range of design parameters. This advanced practice introduces a powerful greedy algorithm to adaptively construct a POD basis for such parametric systems. You will implement a method that uses an *a posteriori* error estimator to intelligently query the parameter space, ensuring the resulting basis is both compact and accurate across all designs of interest .",
            "id": "3943535",
            "problem": "You are given a parametric, symmetric positive-definite linear system that represents a steady-state, one-dimensional diffusion-reaction model for a battery electrode discretized over $N$ nodes. The system for parameter $u \\in \\mathbb{R}$ is\n$$\nA(u) x(u) = f(u),\n$$\nwhere $A(u) \\in \\mathbb{R}^{N \\times N}$ and $f(u) \\in \\mathbb{R}^{N}$ are defined by the following scientifically grounded constructs:\n\n- The discrete Dirichlet Laplacian $L \\in \\mathbb{R}^{N \\times N}$ on a one-dimensional grid is defined by:\n$$\nL_{i,i} = 2,\\quad L_{i,i+1} = L_{i+1,i} = -1\\quad \\text{for } i=1,\\dots,N-1,\\quad \\text{and } L_{i,j}=0 \\text{ otherwise}.\n$$\n- The reaction-rate diagonal $D(u) = \\mathrm{diag}\\big(d_0 + u\\, d_1\\big)$ is defined by two positive vectors $d_0, d_1 \\in \\mathbb{R}^N$ with entries:\n$$\n(d_0)_i = 0.5 + 0.5 \\sin\\left(\\frac{\\pi i}{N+1}\\right), \\quad (d_1)_i = 0.2 + 0.1 \\cos\\left(\\frac{\\pi i}{N+1}\\right),\n$$\nfor $i=1,\\dots,N$. Both $(d_0)_i$ and $(d_1)_i$ are nonnegative for all $i$.\n- The load vector $f(u)$ is:\n$$\nf(u) = s + u\\, t,\n$$\nwhere $s \\in \\mathbb{R}^N$ has entries $s_i = 1.0$ and $t \\in \\mathbb{R}^N$ has entries:\n$$\nt_i = \\exp\\left(-\\left(\\frac{i - (N+1)/2}{N/10}\\right)^2\\right),\n$$\nfor $i=1,\\dots,N$.\n\nSet $A(u) = L + D(u)$. Because $L$ is symmetric positive semi-definite and $D(u)$ is diagonal with strictly positive diagonal entries for any $u \\ge 0$, $A(u)$ is symmetric positive definite for all $u$ in the specified candidate sets below.\n\nYour task is to implement a greedy adaptive snapshot enrichment algorithm driven by an error estimator to refine the Proper Orthogonal Decomposition (POD) basis during the prospective optimization of electrode design. Proper Orthogonal Decomposition (POD) seeks an orthonormal basis $V \\in \\mathbb{R}^{N \\times r}$ that approximates the snapshot subspace spanned by $\\{x(u_k)\\}$, minimizing the projection error over the collected snapshots. Given the snapshot matrix $X = [x(u_1), x(u_2), \\dots, x(u_m)] \\in \\mathbb{R}^{N \\times m}$, the POD basis is obtained from the left singular vectors of $X$.\n\nFor a candidate parameter $\\theta$, given a POD basis $V$, define the reduced-order solution $x_r(\\theta) = V y(\\theta)$ where $y(\\theta) \\in \\mathbb{R}^r$ solves the reduced Galerkin system\n$$\n\\left(V^\\top A(\\theta) V\\right) y(\\theta) = V^\\top f(\\theta).\n$$\nDefine the residual $r(\\theta) = f(\\theta) - A(\\theta) x_r(\\theta)$. The error estimator is the energy norm of the error, which for symmetric positive definite $A(\\theta)$ can be computed exactly via\n$$\n\\eta(\\theta) = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}.\n$$\n\nYou must implement the following greedy algorithm:\n\n1. Initialize with an initial snapshot set $\\mathcal{S}_0 = \\{u^{(1)}, u^{(2)}\\}$ and construct the initial POD basis $V$ from the corresponding full-order solutions. In this problem, take $u^{(1)} = 0.0$ and $u^{(2)} = 1.0$.\n2. Given a candidate parameter set $\\Theta$, at each greedy iteration evaluate $\\eta(\\theta)$ for all $\\theta \\in \\Theta$ not yet in the snapshot set. Select $\\theta^\\star$ that maximizes $\\eta(\\theta)$.\n3. If $\\eta(\\theta^\\star) \\le \\varepsilon$ (tolerance), terminate; otherwise, compute the full-order solution $x(\\theta^\\star)$, augment the snapshot set, and recompute the POD basis $V$ from all collected snapshots.\n4. Repeat until the tolerance is met or the snapshot budget $m_{\\max}$ (maximum number of total snapshots) is reached.\n\nFor verification, after termination compute:\n- The final basis dimension $r$.\n- The maximum error estimator over all candidates $\\max_{\\theta \\in \\Theta} \\eta(\\theta)$.\n- The maximum actual energy-norm error over all candidates\n$$\nE_{\\max} = \\max_{\\theta \\in \\Theta} \\sqrt{\\big(x(\\theta) - x_r(\\theta)\\big)^\\top A(\\theta) \\big(x(\\theta) - x_r(\\theta)\\big)}.\n$$\n- A boolean indicating whether $E_{\\max} \\le \\varepsilon$ holds.\n\nImplement the program for $N=50$ and the following test suite. All quantities are dimensionless; there are no physical units to report.\n\nTest suite:\n- Case 1 (general case): $\\Theta = \\{0.0, 0.1, 0.2, \\dots, 1.0\\}$, tolerance $\\varepsilon = 10^{-6}$, budget $m_{\\max} = 11$.\n- Case 2 (budget-limited): $\\Theta = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$, tolerance $\\varepsilon = 10^{-3}$, budget $m_{\\max} = 3$.\n- Case 3 (duplicate candidates edge case): $\\Theta = \\{0.0, 0.0, 0.5, 0.5, 1.0\\}$, tolerance $\\varepsilon = 10^{-2}$, budget $m_{\\max} = 5$.\n\nYour program should produce a single line of output containing the results across all cases as a comma-separated list enclosed in square brackets. Each case’s result must be a list of the form $[r, \\eta_{\\max}, E_{\\max}, \\text{satisfied}]$, where $r$ is an integer, $\\eta_{\\max}$ and $E_{\\max}$ are floats, and $\\text{satisfied}$ is a boolean. The final output must therefore be a single line of the form `[[r_1,eta_max_1,E_max_1,satisfied_1],[r_2,eta_max_2,E_max_2,satisfied_2],[r_3,eta_max_3,E_max_3,satisfied_3]]`.",
            "solution": "We begin with the governing steady-state diffusion-reaction model on a one-dimensional grid, which follows from conservation of species and Fick’s law. The discretized form using a Dirichlet Laplacian and a spatially varying reaction term yields a symmetric positive definite linear system\n$$\nA(u) x(u) = f(u),\n$$\nwhere $A(u) = L + \\mathrm{diag}(d_0 + u d_1)$ and $L$ is the standard tridiagonal matrix with $2$ on the diagonal and $-1$ on the first off-diagonals. Because $L$ is symmetric positive semi-definite and the diagonal contribution is strictly positive for the specified parameter ranges, $A(u)$ is symmetric positive definite for $u \\ge 0$ and thus admits a unique solution $x(u)$ for each parameter value.\n\nThe aim of Proper Orthogonal Decomposition (POD) is to construct an orthonormal basis $V \\in \\mathbb{R}^{N \\times r}$ that best approximates the snapshot subspace generated by collected full-order solutions $\\{x(u_k)\\}$. The canonical characterization of the POD basis is the solution to the variational problem that minimizes the sum of squared projection errors over the snapshot set. This yields that $V$ is formed by the leading left singular vectors of the snapshot matrix $X = [x(u_1), \\dots, x(u_m)]$ obtained through the singular value decomposition $X = U \\Sigma W^\\top$, where $U$ contains orthonormal columns and $\\Sigma$ contains singular values. Choosing $V$ as the first $r$ columns of $U$ corresponding to the nonzero singular values minimizes the Frobenius norm of the projection error.\n\nA reduced-order model is obtained by Galerkin projection onto the POD basis $V$. For a parameter $\\theta$, define the reduced coordinates $y(\\theta) \\in \\mathbb{R}^r$ as the solution to\n$$\n\\left(V^\\top A(\\theta) V\\right) y(\\theta) = V^\\top f(\\theta),\n$$\nand the reduced-order state $x_r(\\theta) = V y(\\theta)$. Because $A(\\theta)$ is symmetric positive definite and $V$ has orthonormal columns, the reduced operator $V^\\top A(\\theta) V$ is also symmetric positive definite, so $y(\\theta)$ exists uniquely.\n\nTo guide greedy snapshot enrichment, we use the residual-based energy-norm error estimator. Define the residual\n$$\nr(\\theta) = f(\\theta) - A(\\theta) x_r(\\theta).\n$$\nThe full-order solution satisfies $A(\\theta) x(\\theta) = f(\\theta)$, so the error $e(\\theta) = x(\\theta) - x_r(\\theta)$ obeys\n$$\nA(\\theta) e(\\theta) = r(\\theta).\n$$\nFor symmetric positive definite $A(\\theta)$, the energy norm of the error is\n$$\n\\|e(\\theta)\\|_{A(\\theta)} = \\sqrt{e(\\theta)^\\top A(\\theta) e(\\theta)}.\n$$\nUsing $A(\\theta) e(\\theta) = r(\\theta)$ and symmetry,\n$$\n\\|e(\\theta)\\|_{A(\\theta)}^2 = e(\\theta)^\\top A(\\theta) e(\\theta) = e(\\theta)^\\top r(\\theta) = r(\\theta)^\\top A(\\theta)^{-1} r(\\theta),\n$$\nwhich implies\n$$\n\\|e(\\theta)\\|_{A(\\theta)} = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}.\n$$\nTherefore, the estimator\n$$\n\\eta(\\theta) = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}\n$$\nequals the true energy-norm error for the reduced Galerkin solution. Computationally, this may be evaluated by solving $A(\\theta) z(\\theta) = r(\\theta)$ and computing $\\eta(\\theta) = \\sqrt{r(\\theta)^\\top z(\\theta)}$.\n\nThe greedy algorithm proceeds as follows:\n- Start with an initial snapshot set $\\mathcal{S}_0 = \\{u^{(1)}, u^{(2)}\\}$, compute the full-order solutions $x(u^{(1)})$ and $x(u^{(2)})$, and build the initial POD basis $V$ from these snapshots via singular value decomposition.\n- For each unsampled candidate $\\theta$ in $\\Theta$, compute the reduced solution $x_r(\\theta)$ by solving the reduced system, form the residual $r(\\theta)$, and compute the estimator $\\eta(\\theta)$ via the energy-norm identity. Select the parameter with the largest $\\eta(\\theta)$ to maximize information gain.\n- If the largest estimator does not exceed the tolerance $\\varepsilon$, terminate. Otherwise, augment the snapshot set with the full-order solution at the selected parameter, recompute the POD basis $V$ from all snapshots, and repeat until the tolerance criterion is satisfied or the snapshot budget $m_{\\max}$ is reached.\n\nBecause the POD basis is recomputed from an expanded snapshot matrix at each iteration, the reduced subspace can only increase (or remain the same if the new snapshot is collinear with the existing span) and the Galerkin projection minimizes the energy-norm error over the current subspace. As a consequence, the maximum energy-norm error over the finite candidate set $\\Theta$ is nonincreasing across iterations, and the greedy selection using the largest estimated error prioritizes parameters that most challenge the current reduced-order model.\n\nFinally, after termination, we verify performance by computing:\n- The final basis dimension $r$, equal to the rank of the snapshot matrix used to build the POD basis.\n- The maximum estimator $\\eta_{\\max} = \\max_{\\theta \\in \\Theta} \\eta(\\theta)$.\n- The maximum actual energy-norm error $E_{\\max} = \\max_{\\theta \\in \\Theta} \\|x(\\theta) - x_r(\\theta)\\|_{A(\\theta)}$.\n- The boolean condition $\\text{satisfied} = (E_{\\max} \\le \\varepsilon)$.\n\nThe implementation uses linear algebra operations to construct $L$, $A(u)$, and $f(u)$; solve full-order and reduced-order systems; compute singular value decompositions for POD; and iteratively enrich the snapshot set according to the error estimator. The test suite covers a general case, a budget-limited case to examine termination without meeting tolerance, and an edge case with duplicate candidates to ensure robustness under redundant parameter entries. The final output follows the specified single-line nested list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_dirichlet_laplacian(n: int) -> np.ndarray:\n    \"\"\"Build 1D Dirichlet Laplacian matrix L of size n x n.\"\"\"\n    L = np.zeros((n, n), dtype=float)\n    # Tridiagonal: 2 on diagonal, -1 on off-diagonals\n    for i in range(n):\n        L[i, i] = 2.0\n        if i - 1 >= 0:\n            L[i, i - 1] = -1.0\n        if i + 1  n:\n            L[i, i + 1] = -1.0\n    return L\n\ndef build_reaction_vectors(n: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build d0 and d1 reaction-rate vectors.\"\"\"\n    idx = np.arange(1, n + 1, dtype=float)\n    d0 = 0.5 + 0.5 * np.sin(np.pi * idx / (n + 1))\n    d1 = 0.2 + 0.1 * np.cos(np.pi * idx / (n + 1))\n    return d0, d1\n\ndef build_load_vectors(n: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build s and t load vectors.\"\"\"\n    s = np.ones(n, dtype=float)\n    center = (n + 1) / 2.0\n    sigma = n / 10.0\n    idx = np.arange(1, n + 1, dtype=float)\n    t = np.exp(-((idx - center) / sigma) ** 2)\n    return s, t\n\ndef assemble_system(u: float, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Assemble A(u) and f(u).\"\"\"\n    diag = d0 + u * d1\n    A = L + np.diag(diag)\n    f = s + u * t\n    return A, f\n\ndef full_solution(u: float, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the full-order solution x(u).\"\"\"\n    A, f = assemble_system(u, L, d0, d1, s, t)\n    x = np.linalg.solve(A, f)\n    return x\n\ndef pod_basis_from_snapshots(snapshots: list[np.ndarray], tol: float = 1e-12) -> np.ndarray:\n    \"\"\"Compute POD basis V from snapshot list using SVD.\n    Returns V with orthonormal columns. Rank determined by singular values above tol.\n    \"\"\"\n    if len(snapshots) == 0:\n        raise ValueError(\"No snapshots provided for POD basis.\")\n    X = np.column_stack(snapshots)  # N x m\n    U, S, VT = np.linalg.svd(X, full_matrices=False)\n    rank = int(np.sum(S > tol))\n    if rank == 0:\n        # If all singular values are tiny, keep at least one vector to avoid empty basis\n        rank = 1\n    V = U[:, :rank]\n    return V\n\ndef reduced_solution(u: float, V: np.ndarray, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute reduced-order solution x_r(u) = V y, residual r, and A, f.\"\"\"\n    A, f = assemble_system(u, L, d0, d1, s, t)\n    # Reduced Galerkin system\n    Ar = V.T @ A @ V\n    fr = V.T @ f\n    y = np.linalg.solve(Ar, fr)\n    x_r = V @ y\n    r_vec = f - A @ x_r\n    return x_r, r_vec, A\n\ndef energy_norm_error_estimator(r_vec: np.ndarray, A: np.ndarray) -> float:\n    \"\"\"Compute the exact energy-norm error estimator sqrt(r^T A^{-1} r).\"\"\"\n    z = np.linalg.solve(A, r_vec)\n    eta = float(np.sqrt(r_vec.T @ z))\n    return eta\n\ndef energy_norm_error(x_full: np.ndarray, x_red: np.ndarray, A: np.ndarray) -> float:\n    \"\"\"Compute energy-norm error sqrt((x - x_r)^T A (x - x_r)).\"\"\"\n    e = x_full - x_red\n    val = float(np.sqrt(e.T @ (A @ e)))\n    return val\n\ndef greedy_adaptive_pod(theta_candidates: list[float], epsilon: float, m_max: int, initial_params: list[float], n: int) -> list:\n    \"\"\"Run the greedy adaptive snapshot enrichment algorithm.\n    Returns [final_basis_dim, max_estimator, max_actual_error, satisfied_boolean].\n    \"\"\"\n    # Build global components\n    L = build_dirichlet_laplacian(n)\n    d0, d1 = build_reaction_vectors(n)\n    s, t = build_load_vectors(n)\n\n    # Make candidate set unique and sorted for determinism\n    theta_unique = sorted(set(theta_candidates))\n    # Initialize snapshot set with unique initial params (ensure they are considered sampled)\n    sampled_params = []\n    snapshots = []\n    for u0 in initial_params:\n        if u0 not in sampled_params:\n            x0 = full_solution(u0, L, d0, d1, s, t)\n            sampled_params.append(u0)\n            snapshots.append(x0)\n\n    # Enforce budget: if initial exceeds budget, truncate (edge safeguard)\n    if len(sampled_params) > m_max:\n        sampled_params = sampled_params[:m_max]\n        snapshots = snapshots[:m_max]\n\n    # Build initial POD basis\n    V = pod_basis_from_snapshots(snapshots)\n\n    # Greedy loop\n    # If budget allows adding more snapshots\n    while len(sampled_params)  m_max:\n        # Evaluate estimator over unsampled candidates\n        best_theta = None\n        best_eta = -np.inf\n        for theta in theta_unique:\n            if theta in sampled_params:\n                continue\n            x_r, r_vec, A = reduced_solution(theta, V, L, d0, d1, s, t)\n            eta = energy_norm_error_estimator(r_vec, A)\n            if eta > best_eta:\n                best_eta = eta\n                best_theta = theta\n        # If no unsampled candidates remain, break\n        if best_theta is None:\n            break\n        # Check tolerance\n        if best_eta = epsilon:\n            break\n        # Enrich snapshot set with full solution at best_theta\n        x_best = full_solution(best_theta, L, d0, d1, s, t)\n        sampled_params.append(best_theta)\n        snapshots.append(x_best)\n        # Recompute POD basis\n        V = pod_basis_from_snapshots(snapshots)\n\n    # After termination, compute final metrics over all candidates\n    max_eta = -np.inf\n    max_actual_err = -np.inf\n    for theta in theta_unique:\n        # Reduced solution and residual\n        x_r, r_vec, A = reduced_solution(theta, V, L, d0, d1, s, t)\n        eta = energy_norm_error_estimator(r_vec, A)\n        if eta > max_eta:\n            max_eta = eta\n        # Actual energy-norm error\n        x_full = np.linalg.solve(A, s + theta * t)\n        e = energy_norm_error(x_full, x_r, A)\n        if e > max_actual_err:\n            max_actual_err = e\n\n    final_basis_dim = V.shape[1]\n    satisfied = max_actual_err = epsilon\n    return [final_basis_dim, max_eta, max_actual_err, satisfied]\n\ndef solve():\n    # Define problem size\n    N = 50\n\n    # Test cases as specified\n    test_cases = [\n        # Case 1: General case\n        {\n            \"theta\": [0.1 * k for k in range(11)],  # 0.0 to 1.0 inclusive\n            \"epsilon\": 1e-6,\n            \"m_max\": 11,\n            \"initial\": [0.0, 1.0],\n        },\n        # Case 2: Budget-limited\n        {\n            \"theta\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"epsilon\": 1e-3,\n            \"m_max\": 3,\n            \"initial\": [0.0, 1.0],\n        },\n        # Case 3: Duplicate candidates edge case\n        {\n            \"theta\": [0.0, 0.0, 0.5, 0.5, 1.0],\n            \"epsilon\": 1e-2,\n            \"m_max\": 5,\n            \"initial\": [0.0, 1.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        res = greedy_adaptive_pod(\n            theta_candidates=case[\"theta\"],\n            epsilon=case[\"epsilon\"],\n            m_max=case[\"m_max\"],\n            initial_params=case[\"initial\"],\n            n=N,\n        )\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Nested list of [int, float, float, bool] per case\n    def format_item(item):\n        # Ensure Python booleans print as True/False\n        return f\"[{item[0]},{item[1]},{item[2]},{str(item[3])}]\"\n    print(f\"[{','.join(format_item(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}