{
    "hands_on_practices": [
        {
            "introduction": "Before optimizing a kernel, it is crucial to understand its theoretical performance limits. The Roofline model provides a powerful visual and quantitative framework for this analysis by relating a kernel's arithmetic intensity—the ratio of computations to memory traffic—to the hardware's peak throughput and memory bandwidth. This practice  will guide you through calculating the arithmetic intensity for a typical stencil operation and using the Roofline model to predict its attainable performance, revealing whether the computation is limited by memory bandwidth or raw processing power.",
            "id": "3917076",
            "problem": "A high-fidelity electrochemical-thermal model for a lithium-ion battery pack is discretized on a uniform Cartesian grid using a second-order central finite-difference scheme for heat conduction and an explicit forward-Euler time integrator. On a Graphics Processing Unit (GPU), a naive seven-point three-dimensional stencil kernel updates the temperature field from time level $n$ to $n+1$ using the following per-node operation pattern: read the center temperature value and its $6$ face-adjacent neighbors from the input array at time level $n$, read a volumetric heat source from a separate array, compute a weighted linear combination with preloaded scalar coefficients, and write the updated temperature to an output array at time level $n+1$. Assume double precision ($s=8$ bytes per value), no use of fused multiply-add, coefficients reside in registers, and the GPU exhibits no read-for-ownership behavior for global stores. All global memory accesses are perfectly coalesced, and there is no inter-thread data reuse via shared memory or caches.\n\nFor a single lattice-point update, the arithmetic work follows this explicit form:\n- Compute $P_0=\\gamma_0 T_{i,j,k}$, and for each of the $6$ face neighbors $p \\in \\{1,\\dots,6\\}$ compute $P_p=\\gamma_p T_{\\text{nb}(p)}$.\n- Accumulate the $7$ products $\\{P_0,P_1,\\dots,P_6\\}$ by pairwise addition into a single sum.\n- Compute $P_s=\\sigma s_{i,j,k}$ and add it to the accumulated sum to produce $T^{n+1}_{i,j,k}$.\n\nCount floating-point operations using the convention that each multiply or add counts as $1$ floating-point operation and that no multiply-add fusion occurs.\n\nAssume the following hardware properties for the GPU:\n- Sustained global-memory bandwidth $B_{\\text{sust}}=1.2 \\times 10^{12}$ bytes/s under the given access pattern.\n- Peak double-precision throughput $P_{\\text{peak}}=9.5 \\times 10^{12}$ floating-point operations/s.\n\nStarting only from the definitions of arithmetic intensity and the roofline performance model, do the following:\n- Derive the arithmetic intensity, defined as floating-point operations per byte of global memory traffic, for the described stencil update.\n- Map this arithmetic intensity to the roofline model to estimate the attainable bandwidth-limited performance of the kernel, and verify whether the kernel is bandwidth-bound under the given hardware parameters.\n\nRound your final numerical answer for the attainable performance to $3$ significant figures and express the final performance in GFLOP/s. The final answer must be a single number.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of computational science and high-performance computing, specifically utilizing the well-established roofline performance model. The problem is well-posed, providing all necessary data and constraints for a unique and meaningful solution. The language is objective and precise, and the setup, while simplified by assumptions such as the absence of caches, represents a standard, analyzable case for a naive stencil kernel implementation.\n\nThe solution proceeds in three stages: first, we determine the number of floating-point operations (FLOPs) per stencil point update; second, we determine the total global memory traffic in bytes per point update; and third, we use these quantities to calculate the arithmetic intensity and apply the roofline model to find the attainable performance.\n\n**1. Calculation of Floating-Point Operations**\n\nThe problem specifies the arithmetic work for a single lattice-point update. We count the operations according to the provided breakdown, where each multiplication and each addition counts as $1$ FLOP.\n\n- Computation of the central point's contribution: $P_0 = \\gamma_0 T_{i,j,k}$. This is $1$ multiplication.\n- Computation of the $6$ neighbors' contributions: $P_p = \\gamma_p T_{\\text{nb}(p)}$ for $p \\in \\{1, \\dots, 6\\}$. This is $6$ multiplications.\n- Accumulation of the $7$ product terms $\\{P_0, P_1, \\dots, P_6\\}$ into a single sum. Summing $7$ numbers requires $7-1=6$ pairwise additions.\n- Computation of the source term's contribution: $P_s = \\sigma s_{i,j,k}$. This is $1$ multiplication.\n- Addition of the source term to the accumulated sum: This is $1$ final addition.\n\nThe total number of floating-point operations, $N_{\\text{FLOPs}}$, per point update is the sum of all multiplications and additions:\n$$N_{\\text{FLOPs}} = (1 + 6 + 1) \\text{ multiplications} + (6 + 1) \\text{ additions}$$\n$$N_{\\text{FLOPs}} = 8 + 7 = 15 \\text{ FLOPs}$$\n\n**2. Calculation of Global Memory Traffic**\n\nThe problem states that each thread updates one point in the grid. We calculate the total data transfer to and from global memory for this single update. The assumptions of no data reuse via caches or shared memory and perfectly coalesced accesses simplify this to a direct count of reads and writes. Data is in double precision, so each value requires $s=8$ bytes.\n\n- **Reads from Global Memory:**\n  - $1$ center temperature value ($T_{i,j,k}$).\n  - $6$ face-adjacent neighbor temperature values.\n  - $1$ volumetric heat source value ($s_{i,j,k}$) from a separate array.\n  - Total values read = $1 + 6 + 1 = 8$ values.\n  - Total bytes read = $8 \\text{ values} \\times 8 \\frac{\\text{bytes}}{\\text{value}} = 64$ bytes.\n\n- **Writes to Global Memory:**\n  - $1$ updated temperature value ($T^{n+1}_{i,j,k}$) to the output array.\n  - Total values written = $1$ value.\n  - Total bytes written = $1 \\text{ value} \\times 8 \\frac{\\text{bytes}}{\\text{value}} = 8$ bytes.\n\nThe total memory traffic in bytes, $N_{\\text{Bytes}}$, per point update is the sum of bytes read and bytes written:\n$$N_{\\text{Bytes}} = 64 \\text{ bytes (read)} + 8 \\text{ bytes (write)} = 72 \\text{ bytes}$$\n\n**3. Application of the Roofline Performance Model**\n\nThe arithmetic intensity, $I$, is defined as the ratio of floating-point operations to bytes of global memory traffic.\n$$I = \\frac{N_{\\text{FLOPs}}}{N_{\\text{Bytes}}} = \\frac{15 \\text{ FLOPs}}{72 \\text{ bytes}} = \\frac{5}{24} \\frac{\\text{FLOPs}}{\\text{byte}}$$\n\nThe roofline model predicts that the attainable performance, $P_{\\text{attainable}}$, is the minimum of the peak compute throughput, $P_{\\text{peak}}$, and the performance sustainable by the memory subsystem, which is the product of the arithmetic intensity and the sustained memory bandwidth, $B_{\\text{sust}}$.\n$$P_{\\text{attainable}} = \\min(P_{\\text{peak}}, I \\times B_{\\text{sust}})$$\n\nTo determine whether the kernel is bandwidth-bound or compute-bound, we can compute the machine's ridge point, $I_{\\text{ridge}}$, which is the minimum arithmetic intensity required to saturate the peak compute performance.\n$$I_{\\text{ridge}} = \\frac{P_{\\text{peak}}}{B_{\\text{sust}}} = \\frac{9.5 \\times 10^{12} \\text{ FLOP/s}}{1.2 \\times 10^{12} \\text{ bytes/s}} = \\frac{9.5}{1.2} \\frac{\\text{FLOPs}}{\\text{byte}} \\approx 7.917 \\frac{\\text{FLOPs}}{\\text{byte}}$$\n\nThe kernel's arithmetic intensity is $I = \\frac{5}{24} \\approx 0.2083$ FLOPs/byte. Since $I  I_{\\text{ridge}}$, the kernel is **bandwidth-bound**. Its performance is limited by the memory bandwidth.\n\nTherefore, the attainable performance is given by:\n$$P_{\\text{attainable}} = I \\times B_{\\text{sust}}$$\n$$P_{\\text{attainable}} = \\left(\\frac{5}{24} \\frac{\\text{FLOPs}}{\\text{byte}}\\right) \\times \\left(1.2 \\times 10^{12} \\frac{\\text{bytes}}{\\text{s}}\\right)$$\n$$P_{\\text{attainable}} = \\frac{5 \\times 1.2}{24} \\times 10^{12} \\frac{\\text{FLOPs}}{\\text{s}} = \\frac{6}{24} \\times 10^{12} \\frac{\\text{FLOPs}}{\\text{s}}$$\n$$P_{\\text{attainable}} = 0.25 \\times 10^{12} \\frac{\\text{FLOPs}}{\\text{s}}$$\n\nThe problem requires the answer in GigaFLOPs per second (GFLOP/s), where $1 \\text{ GFLOP/s} = 10^9$ FLOP/s.\n$$P_{\\text{attainable}} = 0.25 \\times 10^{12} \\text{ FLOP/s} = 250 \\times 10^9 \\text{ FLOP/s} = 250 \\text{ GFLOP/s}$$\n\nThis value is exact. Rounding to $3$ significant figures gives $250$.",
            "answer": "$$\\boxed{250}$$"
        },
        {
            "introduction": "Having identified that many solver kernels are memory-bound, the next logical step is to maximize the efficiency of data transfer from global memory. On GPUs, this is achieved through coalesced memory access, where the hardware services a warp's memory requests in as few transactions as possible. This hands-on problem  challenges you to derive the relationship between memory access patterns and effective bandwidth, providing a fundamental understanding of how to structure data and loops for optimal throughput.",
            "id": "3917106",
            "problem": "An electrochemical-thermal finite volume kernel in automated battery design and simulation runs on a Graphics Processing Unit (GPU). Each GPU warp consists of $32$ threads that cooperatively update control volumes by reading one double-precision value (size $8$ bytes) per thread from a large array in global memory and performing a fixed number of floating-point operations per update. Assume the following well-tested facts and definitions:\n- Global memory is serviced in aligned segments of size $128$ bytes (i.e., each transaction fetches one $128$-byte-aligned segment containing any requested word), and the hardware coalescer issues the minimal number of such segments necessary to satisfy a warp’s requests.\n- A double-precision value occupies $8$ bytes, so there are $16$ doubles in a $128$-byte segment.\n- The peak sustainable global memory bandwidth is $B_{g}$, and the peak floating-point throughput is $F_{\\max}$.\n- The arithmetic intensity $I$ of the kernel, defined as floating-point operations per byte fetched from global memory, is low in stencil-like updates typical of electrochemical-thermal solvers.\n\nThe warp’s $t$-th thread, with $t \\in \\{0,1,\\dots,31\\}$, reads element index\n$$\ne(t) = e_{0} + S\\,t,\n$$\nfrom a very large array of doubles, where $S \\in \\mathbb{N}$ is the stride in elements between consecutive threads and $e_{0} \\in \\mathbb{N}$ is the base element index. Define the element-alignment residue\n$$\nr \\equiv e_{0} \\bmod 16,\n$$\nso that $r \\in \\{0,1,\\dots,15\\}$. Assume cold-cache behavior so that no inter-warp reuse occurs.\n\nTasks:\n1. Starting from the roofline model, and without invoking any untested shortcut formulas, argue symbolically why, for electrochemical-thermal stencil kernels with low arithmetic intensity $I$, the execution is dominated by global memory access rather than arithmetic throughput. Your argument must be based on fundamental definitions of arithmetic intensity and bandwidth-limited performance.\n2. Using only the stated memory-transaction model, derive from first principles a closed-form expression for the number of distinct $128$-byte segments touched by the warp’s $32$ loads as a function of $S$ and $r$. Clearly state any intermediate definitions you rely on.\n3. Using your result from part 2, obtain a closed-form analytic expression for the normalized effective bandwidth\n$$\n\\eta(S,r) \\equiv \\frac{B_{\\text{eff}}(S,r)}{B_{g}},\n$$\nfor this warp’s memory access, where $B_{\\text{eff}}(S,r)$ is the effective bandwidth delivered to useful data. Treat each $128$-byte transaction as transferring the same amount of time-equivalent bandwidth and count only the $32 \\times 8$ useful bytes in the numerator.\n4. State the alignment and stride conditions that minimize the number of segments and thus maximize $\\eta(S,r)$ for double-precision arrays.\n\nAnswer specification:\n- Provide as your final answer only the single closed-form analytic expression for $\\eta(S,r)$ obtained in part 3. Do not include any units in the final boxed expression; it is a dimensionless ratio. Do not round.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-contained. The model of GPU memory access, while simplified, is based on established principles of high-performance computing, such as memory coalescing, bandwidth, and arithmetic intensity, making it suitable for rigorous analysis. We proceed with the solution.\n\nThe solution is presented in four parts, corresponding to the tasks in the problem description.\n\n1. Starting from the roofline model, we argue that the execution is dominated by global memory access. The roofline model provides an upper bound on the attainable floating-point performance, $F$, of a computational kernel. The performance is limited by either the peak floating-point throughput, $F_{\\max}$ (in FLOP/s), or the performance sustainable by the memory subsystem, $F_{\\text{mem}}$. This is expressed as:\n$$\nF = \\min(F_{\\max}, F_{\\text{mem}})\n$$\nThe memory-limited performance, $F_{\\text{mem}}$, is determined by the peak global memory bandwidth, $B_{g}$ (in bytes/s), and the kernel's arithmetic intensity, $I$ (in FLOP/byte). The arithmetic intensity is the ratio of floating-point operations performed to the bytes transferred from global memory. Thus, the performance limit imposed by memory bandwidth is:\n$$\nF_{\\text{mem}} = I \\times B_{g}\n$$\nSubstituting this into the roofline model gives:\n$$\nF = \\min(F_{\\max}, I \\times B_{g})\n$$\nThe problem states that electrochemical-thermal stencil kernels exhibit low arithmetic intensity $I$. The threshold intensity, $I_{\\text{cross}}$, at which the performance limitation transitions from memory-bound to compute-bound is found by setting $F_{\\max} = I_{\\text{cross}} \\times B_{g}$, which yields $I_{\\text{cross}} = F_{\\max} / B_{g}$. For a \"low\" arithmetic intensity, we have $I  I_{\\text{cross}}$. This implies that $I \\times B_{g}  F_{\\max}$. Consequently, the minimum of the two terms in the roofline model is $I \\times B_{g}$. The attainable performance is therefore $F = I \\times B_{g}$, which is directly proportional to the memory bandwidth $B_{g}$. This demonstrates that the execution is limited by the rate at which data can be fetched from global memory, not by the speed of arithmetic calculations. The kernel is memory-bound.\n\n2. We derive a closed-form expression for the number of distinct $128$-byte segments, $N_{\\text{seg}}(S,r)$, accessed by a warp. A warp consists of $32$ threads, indexed by $t \\in \\{0, 1, \\dots, 31\\}$. Each thread reads a double-precision value of size $8$ bytes. The global memory is accessed in aligned segments of $128$ bytes. Therefore, each segment contains $128 / 8 = 16$ double-precision values.\n\nThe element index accessed by thread $t$ is $e(t) = e_{0} + S\\,t$. The memory segment index for an element at index $e$ is given by $\\lfloor e/16 \\rfloor$. Thus, the segment accessed by thread $t$ is $s(t) = \\lfloor (e_{0} + S\\,t) / 16 \\rfloor$.\n\nThe problem states that the hardware issues the *minimal number of segments necessary* to satisfy the warp's requests. We interpret this, in line with common simplified coalescing models, as the total number of segments in the contiguous span from the segment containing the minimum accessed address to the segment containing the maximum accessed address.\n\nThe minimum element index is accessed by thread $t=0$: $e_{\\min} = e(0) = e_{0}$.\nThe maximum element index is accessed by thread $t=31$: $e_{\\max} = e(31) = e_{0} + 31S$.\n\nThe segment holding the first element is $s_{\\min} = \\lfloor e_{0} / 16 \\rfloor$.\nThe segment holding the last element is $s_{\\max} = \\lfloor (e_{0} + 31S) / 16 \\rfloor$.\n\nThe number of segments in the span is $N_{\\text{seg}} = s_{\\max} - s_{\\min} + 1$. Substituting the expressions for $s_{\\min}$ and $s_{\\max}$:\n$$\nN_{\\text{seg}}(S, e_{0}) = \\left\\lfloor \\frac{e_{0} + 31S}{16} \\right\\rfloor - \\left\\lfloor \\frac{e_{0}}{16} \\right\\rfloor + 1\n$$\nWe are given the element-alignment residue $r \\equiv e_{0} \\bmod 16$. This means $e_{0}$ can be written as $e_{0} = 16k + r$ for some integer $k \\ge 0$ and $r \\in \\{0, 1, \\dots, 15\\}$. Substituting this into the expression for $N_{\\text{seg}}$:\n$$\nN_{\\text{seg}}(S, r) = \\left\\lfloor \\frac{16k + r + 31S}{16} \\right\\rfloor - \\left\\lfloor \\frac{16k + r}{16} \\right\\rfloor + 1\n$$\nUsing the property $\\lfloor x+n \\rfloor = \\lfloor x \\rfloor + n$ for any integer $n$:\n$$\nN_{\\text{seg}}(S, r) = \\left(k + \\left\\lfloor \\frac{r + 31S}{16} \\right\\rfloor\\right) - \\left(k + \\left\\lfloor \\frac{r}{16} \\right\\rfloor\\right) + 1\n$$\nSince $r \\in \\{0, 1, \\dots, 15\\}$, we have $\\lfloor r/16 \\rfloor = 0$. The expression simplifies to:\n$$\nN_{\\text{seg}}(S, r) = \\left\\lfloor \\frac{r + 31S}{16} \\right\\rfloor + 1\n$$\nThis is the closed-form expression for the number of memory segments accessed by the warp.\n\n3. We derive the normalized effective bandwidth, $\\eta(S,r)$. The effective bandwidth, $B_{\\text{eff}}$, is the rate at which useful data is delivered. The normalized effective bandwidth, $\\eta$, is the ratio of useful data bytes to the total data bytes transferred for the operation.\nFor a single warp-level memory access, the total number of useful bytes is the data required by the $32$ threads:\n$$\n\\text{Useful Data} = 32 \\text{ threads} \\times 8 \\frac{\\text{bytes}}{\\text{thread}} = 256 \\text{ bytes}\n$$\nThe total number of bytes transferred from global memory is the number of segments accessed, $N_{\\text{seg}}(S,r)$, multiplied by the size of each segment, $128$ bytes:\n$$\n\\text{Total Data Transferred} = N_{\\text{seg}}(S, r) \\times 128 \\text{ bytes}\n$$\nThe normalized effective bandwidth is the ratio of these two quantities:\n$$\n\\eta(S,r) = \\frac{\\text{Useful Data}}{\\text{Total Data Transferred}} = \\frac{256}{128 \\times N_{\\text{seg}}(S, r)} = \\frac{2}{N_{\\text{seg}}(S, r)}\n$$\nSubstituting the expression for $N_{\\text{seg}}(S,r)$ from part 2, we obtain the final analytic expression:\n$$\n\\eta(S,r) = \\frac{2}{\\left\\lfloor \\frac{r + 31S}{16} \\right\\rfloor + 1}\n$$\n\n4. We state the conditions on stride $S$ and alignment $r$ that maximize $\\eta(S,r)$. To maximize $\\eta(S,r)$, we must minimize its denominator, $N_{\\text{seg}}(S,r)$. The minimum possible number of segments is determined by the total useful data. Since $256$ bytes of data are needed, and each segment is $128$ bytes, a minimum of $256/128 = 2$ segments must be fetched. Thus, the minimum possible value for $N_{\\text{seg}}(S,r)$ is $2$.\nWe seek conditions for which $N_{\\text{seg}}(S,r) = 2$:\n$$\n\\left\\lfloor \\frac{r + 31S}{16} \\right\\rfloor + 1 = 2 \\implies \\left\\lfloor \\frac{r + 31S}{16} \\right\\rfloor = 1\n$$\nThis floor equation is equivalent to the inequality:\n$$\n1 \\le \\frac{r + 31S}{16}  2\n$$\nMultiplying by $16$ gives:\n$$\n16 \\le r + 31S  32\n$$\nGiven that $S \\in \\mathbb{N}$ (so $S \\ge 1$) and $r \\in \\{0, 1, \\dots, 15\\}$. If $S \\ge 2$, then $31S \\ge 62$, which violates the upper bound of the inequality. Therefore, we must have $S=1$.\nSubstituting $S=1$ into the inequality:\n$$\n16 \\le r + 31  32 \\implies -15 \\le r  1\n$$\nGiven that $r$ must be in the set $\\{0, 1, \\dots, 15\\}$, the only value that satisfies $-15 \\le r  1$ is $r=0$.\nThus, the number of segments is minimized, and hence the effective bandwidth is maximized, under the conditions $S=1$ and $r=0$. This corresponds to a unit-stride access ($S=1$) where the base address is perfectly aligned to the start of a $16$-element block ($r=0$), which is also the start of a $128$-byte memory segment. In this case, $\\eta(1,0) = 2/(\\lfloor 31/16 \\rfloor + 1) = 2/(1+1) = 1$, representing $100\\%$ memory efficiency.",
            "answer": "$$\n\\boxed{\\frac{2}{\\left\\lfloor \\frac{r + 31S}{16} \\right\\rfloor + 1}}\n$$"
        },
        {
            "introduction": "While coalescing maximizes global memory throughput, the most powerful optimizations often involve reducing global memory traffic altogether. By using the GPU's fast, on-chip shared memory, we can stage data for reuse within a thread block, which drastically increases the kernel's arithmetic intensity. In this practice , you will engage in a practical design exercise to determine the optimal tile size for a stencil computation by balancing shared memory capacity, register pressure, and the need for block-level concurrency, a core trade-off in high-performance kernel design.",
            "id": "3917132",
            "problem": "In a high-fidelity electrochemical-thermal battery pack simulator, the heat conduction subsolver advances the temperature field by an explicit finite-difference scheme for the three-dimensional heat equation, where each update uses the central point and its six axis-aligned neighbors. On a Graphics Processing Unit (GPU), the solver maps the grid to thread blocks and uses on-chip shared memory to stage a cubic tile of grid values so that each thread updates one interior cell of the tile. To correctly compute neighbor differences without extra global memory traffic, the shared-memory tile must include a one-cell halo in every direction around the interior region.\n\nAssume the following scientifically realistic configuration and constraints for a Streaming Multiprocessor (SM) on the target GPU:\n- Double-precision storage is used, so each scalar value requires $b = 8$ bytes.\n- The halo thickness is $h = 1$ cell on each of the six faces.\n- Each thread processes one interior cell and consumes $r = 64$ registers.\n- The total shared memory capacity per SM is $S_{\\mathrm{SM}} = 98{,}304$ bytes, and the per-block shared memory allocation cap is $S_{\\mathrm{block,max}} = 49{,}152$ bytes.\n- The total register file per SM contains $R_{\\mathrm{SM}} = 65{,}536$ registers.\n- For adequate latency hiding and memory-compute overlap, the scheduler requires at least $m = 2$ resident (concurrent) thread blocks per SM.\n\nLet the interior region of a cubic tile processed by one block have edge length $T$, so there are $T^{3}$ interior threads and $(T+2)^{3}$ stored values in shared memory including the halo. Derive from first principles the constraints that must be satisfied by $T$ in terms of shared memory usage and register availability to guarantee at least $m = 2$ resident blocks per SM. Argue why maximizing arithmetic intensity for this stencil under these constraints reduces to maximizing $T$ subject to those constraints. Using the provided parameters, determine the largest integer $T$ that is feasible. Report $T$ as a pure number with no units. No rounding is necessary.",
            "solution": "The problem statement is a well-posed optimization problem grounded in the principles of high-performance computing and numerical methods. All constants and conditions are provided, scientifically realistic, and internally consistent. No flaws are identified. We may therefore proceed with the solution.\n\nThe problem asks for three things: first, to derive the constraints on the interior cubic tile edge length, $T$, that guarantee at least $m=2$ thread blocks can be resident on a Streaming Multiprocessor (SM). Second, to argue why maximizing arithmetic intensity under these constraints simplifies to maximizing $T$. Third, to determine the largest feasible integer value for $T$ using the provided parameters.\n\nLet us begin by formalizing the constraints imposed by the GPU hardware resources: shared memory and registers.\n\n**1. Shared Memory Constraint**\n\nA single thread block processes an interior region of volume $T \\times T \\times T = T^3$ cells. To update the cells at the boundary of this interior region using the 7-point stencil, a halo of data from neighboring cells is required. The problem states this halo has a thickness of $h=1$ cell on each of the six faces of the cube.\n\nThe total cubic tile of data staged in a block's shared memory therefore has dimensions of $(T+2h) \\times (T+2h) \\times (T+2h)$. Given $h=1$, the dimensions are $(T+2) \\times (T+2) \\times (T+2)$. The total number of scalar values in the shared memory tile for one block is $(T+2)^3$.\n\nEach scalar value is stored in double-precision, requiring $b=8$ bytes. The total shared memory required by a single thread block, $S_{\\text{block}}$, is:\n$$S_{\\text{block}} = b(T+2h)^3 = 8(T+2)^3 \\text{ bytes}$$\n\nThere are two limits on shared memory usage. First, the memory used by a single block cannot exceed the per-block maximum, $S_{\\text{block,max}} = 49{,}152$ bytes.\n$$8(T+2)^3 \\le 49{,}152$$\n$$(T+2)^3 \\le \\frac{49{,}152}{8}$$\n$$(T+2)^3 \\le 6144 \\quad (\\text{Constraint 1a})$$\n\nSecond, to ensure latency hiding, at least $m=2$ blocks must be resident on the SM. The total shared memory consumed by these $m$ blocks cannot exceed the SM's total capacity, $S_{\\mathrm{SM}} = 98{,}304$ bytes.\n$$m \\cdot S_{\\text{block}} \\le S_{\\mathrm{SM}}$$\n$$2 \\cdot 8(T+2)^3 \\le 98{,}304$$\n$$16(T+2)^3 \\le 98{,}304$$\n$$(T+2)^3 \\le \\frac{98{,}304}{16}$$\n$$(T+2)^3 \\le 6144 \\quad (\\text{Constraint 1b})$$\n\nIn this case, the per-block limit and the total SM limit for $m=2$ blocks yield the identical constraint. This is because the total SM capacity is exactly twice the per-block cap ($98{,}304 = 2 \\times 49{,}152$), a common feature in GPU architectures that partition resources among concurrent blocks. Thus, the governing shared memory constraint is $(T+2)^3 \\le 6144$.\n\n**2. Register Constraint**\n\nEach thread block is composed of threads that update the $T^3$ interior cells, so there are $N_{\\text{threads}} = T^3$ threads per block. Each thread consumes $r = 64$ registers. The total number of registers required by a single block, $R_{\\text{block}}$, is:\n$$R_{\\text{block}} = N_{\\text{threads}} \\cdot r = T^3 \\cdot 64$$\n\nTo have $m=2$ resident blocks, the total register usage must not exceed the SM's total register file capacity, $R_{\\mathrm{SM}} = 65{,}536$.\n$$m \\cdot R_{\\text{block}} \\le R_{\\mathrm{SM}}$$\n$$2 \\cdot (64 T^3) \\le 65{,}536$$\n$$128 T^3 \\le 65{,}536$$\n$$T^3 \\le \\frac{65{,}536}{128}$$\n$$T^3 \\le 512 \\quad (\\text{Constraint 2})$$\n\nIn summary, the two governing constraints that $T$ must satisfy are:\n1. Shared Memory: $(T+2)^3 \\le 6144$\n2. Registers: $T^3 \\le 512$\nAdditionally, $T$ must be a positive integer, $T \\ge 1$.\n\n**3. Maximizing Arithmetic Intensity**\n\nArithmetic intensity, $I$, is the ratio of floating-point operations (FLOPs) performed to the amount of data (in bytes) transferred from a slow memory (global memory) to a fast memory (on-chip shared memory). Maximizing this ratio is key to performance, as it signifies that more computation is done per byte of expensive memory traffic.\n\nFor a block processing $T^3$ interior cells, the number of FLOPs is proportional to the number of cells updated. Let $F_{\\text{op}}$ be the number of FLOPs per cell update.\n$$\\text{FLOPs per block} = F_{\\text{op}} \\cdot T^3$$\n\nThe data loaded from global memory into shared memory for this block is the entire $(T+2)^3$ tile. The number of bytes transferred is:\n$$\\text{Bytes per block} = b \\cdot (T+2)^3 = 8(T+2)^3$$\n\nThe arithmetic intensity is therefore:\n$$I = \\frac{\\text{FLOPs per block}}{\\text{Bytes per block}} = \\frac{F_{\\text{op}} \\cdot T^3}{b \\cdot (T+2)^3} = \\frac{F_{\\text{op}}}{b} \\left(\\frac{T}{T+2}\\right)^3$$\n\nSince $F_{\\text{op}}$ and $b$ are constants for this problem, maximizing $I$ is equivalent to maximizing the term $\\left(\\frac{T}{T+2}\\right)^3$. The function $f(T) = \\frac{T}{T+2} = 1 - \\frac{2}{T+2}$ is a monotonically increasing function for $T  0$. As $T$ increases, $\\frac{2}{T+2}$ decreases, and thus $f(T)$ increases. Consequently, maximizing arithmetic intensity for this stencil computation reduces to finding the largest possible value of $T$ that satisfies the hardware constraints.\n\n**4. Determining the Largest Feasible Integer T**\n\nWe must find the largest integer $T$ that satisfies both derived constraints.\n\nFrom the register constraint:\n$$T^3 \\le 512$$\n$$T \\le \\sqrt[3]{512}$$\nSince $8^3 = 512$, this gives:\n$$T \\le 8$$\n\nFrom the shared memory constraint:\n$$(T+2)^3 \\le 6144$$\n$$T+2 \\le \\sqrt[3]{6144}$$\nWe can find the integer bound by testing cubes of integers:\n$18^3 = 5832$\n$19^3 = 6859$\nSince $5832 \\le 6144  6859$, the largest integer value for $T+2$ is $18$.\n$$T+2 \\le 18$$\n$$T \\le 16$$\n\nTo satisfy both constraints simultaneously, $T$ must be less than or equal to the minimum of the two upper bounds:\n$$T \\le \\min(8, 16)$$\n$$T \\le 8$$\n\nThe limiting resource is the register file. The largest integer value for the interior tile edge length $T$ that is feasible under all given conditions is $8$.",
            "answer": "$$\\boxed{8}$$"
        }
    ]
}