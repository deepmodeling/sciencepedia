{
    "hands_on_practices": [
        {
            "introduction": "Real-world particle size distributions are often obtained from characterization techniques like image analysis or laser diffraction, which provide data in the form of histograms. This practice addresses the fundamental first step of converting this discrete, binned data into a set of continuous statistical moments. By working from first principles, you will learn how to construct a probability density function and compute its moments, a crucial skill for bridging experimental data with theoretical models. This exercise  also emphasizes the importance of understanding and quantifying the impact of measurement artifacts, such as the truncation of the distribution's tail.",
            "id": "3938198",
            "problem": "A particle size distribution (PSD) of electrode active material is acquired via image analysis, which yields binned counts of particle diameters. Let $d$ denote the particle diameter in meters, and let $f(d)$ denote the Probability Density Function (PDF) of $d$, normalized so that $\\int f(d)\\,\\mathrm{d}d = 1$. The $k$-th raw moment is defined as $m_k = \\int d^k f(d)\\,\\mathrm{d}d$ and has units of $\\mathrm{m}^k$. You are given finite-resolution histograms defined by bin edges $\\{a_i,b_i\\}_{i=1}^{n}$ with $0  a_1  b_n$, $a_i  b_i$ for each bin, and nonnegative counts $\\{c_i\\}_{i=1}^{n}$ observed in each bin. The imaging system may truncate the upper tail by failing to detect particles above a threshold diameter $T$, which effectively restricts the support of the observed distribution to $d \\le T$.\n\nStarting only from the definitions of a Probability Density Function (PDF), moments, and the Riemann integral, construct consistent estimates of the first four raw moments $m_k$ for $k \\in \\{1,2,3,4\\}$ using only the given bin edges and counts, under a scientifically reasonable assumption about the distribution of $d$ within each bin. Then assess the sensitivity of these moment estimates to upper-tail truncation at a specified threshold $T$ by computing a scalar sensitivity measure defined as the maximum absolute relative change across the first four moments:\n$$\nS(T) = \\max_{k\\in\\{1,2,3,4\\}} \\left| \\frac{m_k(T) - m_k}{m_k} \\right|,\n$$\nwhere $m_k(T)$ denotes the $k$-th raw moment computed from the truncated distribution restricted to $d \\le T$.\n\nYour program must implement the following without using any external files or user input:\n1. Construct moment estimates $m_k$ for $k \\in \\{1,2,3,4\\}$ in $\\mathrm{m}^k$ from the provided binned data.\n2. Implement upper-tail truncation at threshold $T$ by partially including any bin that straddles $T$ according to a physically justified rule based on your within-bin assumption, then renormalize the truncated distribution to form a valid Probability Density Function (PDF).\n3. Compute $S(T)$ as defined above for each test case.\n\nUse the following test suite, where bin edges are specified in meters and counts are dimensionless:\n- Test Case 1 (typical lognormal-like PSD):\n  - Bin edges: $[2.0\\times 10^{-7},\\,4.0\\times 10^{-7},\\,6.0\\times 10^{-7},\\,9.0\\times 10^{-7},\\,1.3\\times 10^{-6},\\,1.8\\times 10^{-6},\\,2.5\\times 10^{-6},\\,3.5\\times 10^{-6},\\,5.0\\times 10^{-6}]$\n  - Counts: $[5,\\,20,\\,60,\\,100,\\,140,\\,120,\\,80,\\,30]$\n  - Truncation threshold: $T = 3.0\\times 10^{-6}\\,\\mathrm{m}$\n- Test Case 2 (heavy upper tail):\n  - Bin edges: $[5.0\\times 10^{-7},\\,1.0\\times 10^{-6},\\,1.5\\times 10^{-6},\\,2.0\\times 10^{-6},\\,3.0\\times 10^{-6},\\,4.0\\times 10^{-6},\\,6.0\\times 10^{-6},\\,8.0\\times 10^{-6},\\,1.0\\times 10^{-5},\\,1.2\\times 10^{-5},\\,1.4\\times 10^{-5},\\,1.7\\times 10^{-5},\\,2.0\\times 10^{-5}]$\n  - Counts: $[10,\\,15,\\,20,\\,25,\\,40,\\,60,\\,90,\\,120,\\,100,\\,80,\\,60,\\,40]$\n  - Truncation threshold: $T = 8.0\\times 10^{-6}\\,\\mathrm{m}$\n- Test Case 3 (left-heavy, sparse upper tail):\n  - Bin edges: $[5.0\\times 10^{-8},\\,1.0\\times 10^{-7},\\,2.0\\times 10^{-7},\\,3.0\\times 10^{-7},\\,5.0\\times 10^{-7},\\,8.0\\times 10^{-7},\\,1.2\\times 10^{-6}]$\n  - Counts: $[200,\\,150,\\,80,\\,30,\\,10,\\,5]$\n  - Truncation threshold: $T = 6.0\\times 10^{-7}\\,\\mathrm{m}$\n- Test Case 4 (zeros in some bins):\n  - Bin edges: $[1.0\\times 10^{-7},\\,5.0\\times 10^{-7},\\,1.0\\times 10^{-6},\\,2.0\\times 10^{-6},\\,4.0\\times 10^{-6}]$\n  - Counts: $[0,\\,50,\\,0,\\,150]$\n  - Truncation threshold: $T = 1.2\\times 10^{-6}\\,\\mathrm{m}$\n\nYour program should produce a single line of output containing the four scalar sensitivity results $S(T)$ for the four test cases as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places, for example, $[0.123456,0.234567,0.345678,0.456789]$. Since $S(T)$ is a ratio, it is dimensionless. All intermediate moment computations must be internally handled in $\\mathrm{m}^k$ units for $k \\in \\{1,2,3,4\\}$.",
            "solution": "The problem requires the estimation of the first four raw moments of a particle size distribution from binned histogram data and the evaluation of the sensitivity of these estimates to upper-tail truncation. The solution shall be constructed from first principles, beginning with the definition of a probability density function and its moments.\n\n**1. Constructing the Probability Density Function (PDF) from Histogram Data**\n\nThe provided data consists of a set of $n$ bins, each defined by a lower edge $a_i$ and an upper edge $b_i$, and a corresponding particle count $c_i$ for $i \\in \\{1, \\dots, n\\}$. The total number of observed particles is $C = \\sum_{i=1}^{n} c_i$.\n\nTo proceed, a \"scientifically reasonable assumption about the distribution of $d$ within each bin\" is required. The most direct and standard assumption is that the probability density is uniform within each bin. This models the underlying continuous Probability Density Function (PDF), denoted $f(d)$, as a piecewise-constant function.\n\nFor the $i$-th bin, $[a_i, b_i]$, the probability $p_i$ of a particle's diameter falling within this range is given by the relative frequency of counts:\n$$\np_i = \\frac{c_i}{C}\n$$\nAccording to the definition of a PDF, this probability is also the integral of $f(d)$ over the bin's interval:\n$$\np_i = \\int_{a_i}^{b_i} f(d) \\, \\mathrm{d}d\n$$\nAssuming $f(d)$ is a constant, let's call it $h_i$, for $d \\in [a_i, b_i]$, the integral becomes:\n$$\np_i = h_i (b_i - a_i)\n$$\nBy equating the two expressions for $p_i$, we can solve for the constant density $h_i$ for each bin:\n$$\nh_i = \\frac{p_i}{b_i - a_i} = \\frac{c_i}{C(b_i - a_i)}\n$$\nThis defines our estimated PDF over the full range of observation, $[a_1, b_n]$:\n$$\nf(d) = \\begin{cases}\nh_i  \\text{for } d \\in [a_i, b_i], i \\in \\{1,\\dots,n\\} \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nThis function is properly normalized, as $\\int_{a_1}^{b_n} f(d) \\, \\mathrm{d}d = \\sum_{i=1}^n h_i(b_i-a_i) = \\sum_{i=1}^n \\frac{c_i}{C} = 1$.\n\n**2. Calculation of Raw Moments**\n\nThe $k$-th raw moment, $m_k$, is defined as the expectation of $d^k$:\n$$\nm_k = \\int_{0}^{\\infty} d^k f(d) \\, \\mathrm{d}d\n$$\nUsing our piecewise-constant PDF, the integral becomes a sum over the bins:\n$$\nm_k = \\sum_{i=1}^{n} \\int_{a_i}^{b_i} d^k h_i \\, \\mathrm{d}d = \\sum_{i=1}^{n} h_i \\int_{a_i}^{b_i} d^k \\, \\mathrm{d}d\n$$\nThe integral of $d^k$ is a standard result from calculus: $\\int d^k \\, \\mathrm{d}d = \\frac{d^{k+1}}{k+1}$. Applying the limits of integration for each bin yields:\n$$\nm_k = \\sum_{i=1}^{n} h_i \\left[ \\frac{d^{k+1}}{k+1} \\right]_{a_i}^{b_i} = \\sum_{i=1}^{n} h_i \\left( \\frac{b_i^{k+1} - a_i^{k+1}}{k+1} \\right)\n$$\nSubstituting the expression for $h_i$, we arrive at the final formula for the moments based on the histogram data:\n$$\nm_k = \\frac{1}{C(k+1)} \\sum_{i=1}^{n} c_i \\frac{b_i^{k+1} - a_i^{k+1}}{b_i - a_i}\n$$\nThis formula is used to compute the first four raw moments, $m_k$, for $k \\in \\{1, 2, 3, 4\\}$.\n\n**3. Implementing Upper-Tail Truncation**\n\nTruncating the distribution at a diameter $T$ means constructing a new PDF, $f_T(d)$, that is zero for $d  T$ and proportional to the original PDF $f(d)$ for $d \\le T$. This requires renormalization.\n\nThe new PDF is defined as:\n$$\nf_T(d) = \\begin{cases}\n\\frac{f(d)}{P(T)}  \\text{for } d \\le T \\\\\n0  \\text{for } d  T\n\\end{cases}\n$$\nwhere $P(T)$ is the total probability mass of the original distribution up to the threshold $T$:\n$$\nP(T) = \\int_0^T f(d) \\, \\mathrm{d}d\n$$\nTo compute $P(T)$, we find the bin $j$ that contains $T$, such that $a_j \\le T  b_j$. The integral is the sum of probabilities of all bins fully below $T$, plus the partial probability of the bin straddling $T$:\n$$\nP(T) = \\left( \\sum_{i=1}^{j-1} \\int_{a_i}^{b_i} h_i \\, \\mathrm{d}d \\right) + \\int_{a_j}^{T} h_j \\, \\mathrm{d}d = \\left( \\sum_{i=1}^{j-1} h_i (b_i - a_i) \\right) + h_j (T - a_j)\n$$\nThis is a \"physically justified rule\" as it is a direct consequence of the uniform intra-bin density assumption.\n\n**4. Calculation of Truncated Moments and Sensitivity**\n\nThe $k$-th raw moment of the truncated distribution, $m_k(T)$, is computed using $f_T(d)$:\n$$\nm_k(T) = \\int_0^\\infty d^k f_T(d) \\, \\mathrm{d}d = \\frac{1}{P(T)} \\int_0^T d^k f(d) \\, \\mathrm{d}d\n$$\nThe integral in the numerator is calculated similarly to $P(T)$:\n$$\n\\int_0^T d^k f(d) \\, \\mathrm{d}d = \\left( \\sum_{i=1}^{j-1} \\int_{a_i}^{b_i} d^k h_i \\, \\mathrm{d}d \\right) + \\int_{a_j}^{T} d^k h_j \\, \\mathrm{d}d = \\left( \\sum_{i=1}^{j-1} h_i \\frac{b_i^{k+1} - a_i^{k+1}}{k+1} \\right) + h_j \\frac{T^{k+1} - a_j^{k+1}}{k+1}\n$$\nWith both the original moments $m_k$ and the truncated moments $m_k(T)$ computed for $k \\in \\{1, 2, 3, 4\\}$, the scalar sensitivity measure $S(T)$ is calculated as the maximum absolute relative change:\n$$\nS(T) = \\max_{k\\in\\{1,2,3,4\\}} \\left| \\frac{m_k(T) - m_k}{m_k} \\right|\n$$\nThe implementation will systematically apply these derived formulas to the provided test cases. All calculations will be performed using standard floating-point arithmetic, with care taken to handle the potential for large exponents in the moment calculations by using base SI units throughout.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"edges\": np.array([2.0e-7, 4.0e-7, 6.0e-7, 9.0e-7, 1.3e-6, 1.8e-6, 2.5e-6, 3.5e-6, 5.0e-6]),\n            \"counts\": np.array([5, 20, 60, 100, 140, 120, 80, 30]),\n            \"T\": 3.0e-6\n        },\n        {\n            \"edges\": np.array([5.0e-7, 1.0e-6, 1.5e-6, 2.0e-6, 3.0e-6, 4.0e-6, 6.0e-6, 8.0e-6, 1.0e-5, 1.2e-5, 1.4e-5, 1.7e-5, 2.0e-5]),\n            \"counts\": np.array([10, 15, 20, 25, 40, 60, 90, 120, 100, 80, 60, 40]),\n            \"T\": 8.0e-6\n        },\n        {\n            \"edges\": np.array([5.0e-8, 1.0e-7, 2.0e-7, 3.0e-7, 5.0e-7, 8.0e-7, 1.2e-6]),\n            \"counts\": np.array([200, 150, 80, 30, 10, 5]),\n            \"T\": 6.0e-7\n        },\n        {\n            \"edges\": np.array([1.0e-7, 5.0e-7, 1.0e-6, 2.0e-6, 4.0e-6]),\n            \"counts\": np.array([0, 50, 0, 150]),\n            \"T\": 1.2e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        sensitivity = calculate_sensitivity(case[\"edges\"], case[\"counts\"], case[\"T\"])\n        results.append(f\"{sensitivity:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_sensitivity(bin_edges, counts, T):\n    \"\"\"\n    Calculates the sensitivity S(T) for a given PSD histogram.\n\n    Args:\n        bin_edges (np.array): Array of bin edges in meters (size n+1).\n        counts (np.array): Array of particle counts for each bin (size n).\n        T (float): Truncation threshold in meters.\n\n    Returns:\n        float: The scalar sensitivity S(T).\n    \"\"\"\n    n_bins = len(counts)\n    a = bin_edges[:-1]\n    b = bin_edges[1:]\n    \n    total_count = np.sum(counts)\n    if total_count == 0:\n        return 0.0\n\n    # Ensure bin widths are not zero to avoid division by zero\n    bin_widths = b - a\n    # Mask to handle bins with zero width or zero count to avoid NaN\n    valid_bins = (counts  0)  (bin_widths  0)\n\n    # 1. Calculate PDF heights h_i for each bin\n    h = np.zeros(n_bins)\n    h[valid_bins] = counts[valid_bins] / (total_count * bin_widths[valid_bins])\n\n    m_full = np.zeros(4)\n    m_trunc = np.zeros(4)\n    \n    # 2. Calculate untruncated moments m_k\n    for k_idx, k in enumerate(range(1, 5)):\n        k_plus_1 = float(k + 1)\n        # Integral of d^k over each bin\n        moment_contribs = (b**k_plus_1 - a**k_plus_1) / k_plus_1\n        m_full[k_idx] = np.sum(h * moment_contribs)\n\n    # 3. Calculate terms for truncated moments m_k(T)\n    # Find the index of the bin that contains T\n    # np.searchsorted returns j such that a[j-1] = T  a[j]\n    j = np.searchsorted(bin_edges, T)\n    \n    # Bins to be fully included are from index 0 to j-2 (since bin_edges has size n+1)\n    # The straddling bin is at index j-1.\n    \n    # Calculate P(T), the normalization factor for the truncated distribution\n    prob_mass_T = 0.0\n    if j  1: # There are fully included bins\n        prob_mass_T += np.sum(h[:j-1] * bin_widths[:j-1])\n    if j  0 and j = n_bins: # There is a straddling bin\n        if T  a[j-1]:\n            prob_mass_T += h[j-1] * (T - a[j-1])\n            \n    if prob_mass_T = 0: # This case shouldn't happen with given test data\n        return 0.0\n\n    # Calculate numerator of truncated moments: integral of d^k * f(d) up to T\n    for k_idx, k in enumerate(range(1, 5)):\n        k_plus_1 = float(k + 1)\n        moment_integral_T = 0.0\n        \n        # Contribution from fully included bins\n        if j  1:\n            full_contribs = h[:j-1] * (b[:j-1]**k_plus_1 - a[:j-1]**k_plus_1) / k_plus_1\n            moment_integral_T += np.sum(full_contribs)\n        \n        # Contribution from straddling bin\n        if j  0 and j = n_bins:\n            if T  a[j-1]:\n                partial_contrib = h[j-1] * (T**k_plus_1 - a[j-1]**k_plus_1) / k_plus_1\n                moment_integral_T += partial_contrib\n        \n        m_trunc[k_idx] = moment_integral_T / prob_mass_T\n\n    # 4. Calculate sensitivity S(T)\n    rel_changes = np.abs((m_trunc - m_full) / m_full)\n    max_rel_change = np.max(rel_changes)\n    \n    return max_rel_change\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "While single-particle models are instructive, accurately simulating an electrode requires accounting for the entire particle size distribution. This practice introduces a powerful and efficient solution: the Quadrature-Based Method of Moments (QBMM). You will explore how a small number of PSD moments, such as those derived in the previous exercise, can be used to reconstruct a quadrature rule that approximates the integrated response of the full distribution . This advanced technique is essential for developing high-fidelity battery models that capture the complex effects of particle heterogeneity on electrochemical performance.",
            "id": "3938240",
            "problem": "Consider an automated battery design and simulation task where the Pseudo-Two-Dimensional (P2D) single-particle model is extended to account for a distribution of particle sizes using a Quadrature-Based Method of Moments (QBMM). The goal is to formalize the closure of particle-size-dependent fluxes using a finite set of Particle Size Distribution (PSD) moments, and to implement a computational procedure that reconstructs a quadrature over particle radii from moments and uses it to approximate integrals needed for electrode flux calculations.\n\nAssume an electrode composed of spherical particles with radius $R \\ge 0$. Let $n(R)$ denote the particle number density per unit electrode volume as a function of radius. The $k$-th raw moment of $n(R)$ is defined as\n$$\n\\mu_k = \\int_{0}^{\\infty} R^k n(R)\\, dR,\n$$\nfor $k \\in \\{0,1,2,3\\}$, where $\\mu_0$ is the total particle number density per unit electrode volume. Consider a particle-surface intercalation flux per unit area that depends on particle size as\n$$\nj_\\mathrm{surf}(R) = k\\, R^\\gamma,\n$$\nwhere $k  0$ and $\\gamma \\ge 0$ are constants. The electrode volumetric intercalation current density is the integral of surface flux over all particle surfaces per unit electrode volume:\n$$\nJ = \\int_{0}^{\\infty} 4\\pi R^2\\, j_\\mathrm{surf}(R)\\, n(R)\\, dR = 4\\pi k \\int_{0}^{\\infty} R^{2+\\gamma} n(R)\\, dR.\n$$\n\nThe QBMM closure approximates integrals over $R$ by a finite quadrature constructed to match a finite number of moments. Specifically, using a two-node quadrature,\n$$\n\\int_{0}^{\\infty} f(R)\\, n(R)\\, dR \\approx w_1 f(r_1) + w_2 f(r_2),\n$$\nwith nodes $r_1, r_2  0$ and weights $w_1, w_2  0$ chosen such that the quadrature is exact for polynomials up to degree $3$, i.e.,\n$$\nw_1 r_1^k + w_2 r_2^k = \\mu_k \\quad \\text{for} \\quad k=0,1,2,3.\n$$\nA constructive way to obtain $r_1, r_2$ is via the generalized eigenvalue problem associated with the moment Hankel matrices,\n$$\nH_0 = \\begin{bmatrix} \\mu_0  \\mu_1 \\\\ \\mu_1  \\mu_2 \\end{bmatrix}, \\quad\nH_1 = \\begin{bmatrix} \\mu_1  \\mu_2 \\\\ \\mu_2  \\mu_3 \\end{bmatrix},\n$$\nby solving for eigenvalues $\\lambda$ in\n$$\nH_1 \\mathbf{v} = \\lambda\\, H_0 \\mathbf{v}.\n$$\nUnder standard moment positivity conditions, the eigenvalues are real and equal to $r_1$ and $r_2$. The weights $w_1$ and $w_2$ are then obtained by solving\n$$\n\\begin{bmatrix} 1  1 \\\\ r_1  r_2 \\end{bmatrix}\n\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} =\n\\begin{bmatrix} \\mu_0 \\\\ \\mu_1 \\end{bmatrix}.\n$$\n\nStarting from the definitions above and the conservation-law interpretation that the electrode volumetric current density $J$ is an area-weighted integral of the intercalation flux over the PSD, derive the QBMM closure and implement it to compute an approximation $J_\\mathrm{QBMM}$ to $J$. For test distributions where analytic expressions for moments of arbitrary real order are available, compare $J_\\mathrm{QBMM}$ against the exact value\n$$\nJ_\\mathrm{exact} = 4\\pi k\\, \\mu_{2+\\gamma},\n$$\nand report the relative error\n$$\ne = \\frac{|J_\\mathrm{QBMM} - J_\\mathrm{exact}|}{J_\\mathrm{exact}}.\n$$\n\nUse the following scientifically plausible test suite of PSDs and parameters. In each case, compute $\\mu_k$ for $k \\in \\{0,1,2,3\\}$ from the given PSD, reconstruct the two-node quadrature, compute $J_\\mathrm{QBMM}$, and produce the relative error $e$ as a float. When the PSD is a finite mixture of delta distributions, additionally verify node recovery.\n\nDefinitions and facts to use:\n- For a lognormal PSD with parameters median $m$ and geometric standard deviation $s_g$, where $\\ln R \\sim \\mathcal{N}(\\ln m, (\\ln s_g)^2)$, the real-order raw moment is\n$$\n\\mu_q = N_0\\, \\mathbb{E}[R^q] = N_0\\, \\exp\\!\\left(q \\ln m + \\tfrac{1}{2} q^2 (\\ln s_g)^2\\right),\n$$\nfor any real $q$, and $N_0$ is the total particle number density per unit volume.\n- For a two-point atomic PSD $n(R) = N_1\\, \\delta(R-a) + N_2\\, \\delta(R-b)$ with $a,b0$, the raw moments are $\\mu_k = N_1 a^k + N_2 b^k$ for all integer $k \\ge 0$.\n\nTest cases:\n1. Lognormal PSD with $N_0 = 1.0\\times 10^{18}$, $m = 5.0\\times 10^{-6}$, $s_g = 1.5$, $k = 2.5$, $\\gamma = 1.0$. Report the relative error $e$ (dimensionless).\n2. Nearly monodisperse lognormal PSD with $N_0 = 1.0\\times 10^{18}$, $m = 5.0\\times 10^{-6}$, $s_g = 1.01$, $k = 1.75$, $\\gamma = 0.5$. Report the relative error $e$ (dimensionless).\n3. Broad lognormal PSD with $N_0 = 1.0\\times 10^{18}$, $m = 2.0\\times 10^{-6}$, $s_g = 3.0$, $k = 3.0$, $\\gamma = 1.2$. Report the relative error $e$ (dimensionless).\n4. Atomic two-point PSD with $N_1 = 6.0\\times 10^{17}$, $N_2 = 4.0\\times 10^{17}$, $a = 3.0\\times 10^{-6}$, $b = 8.0\\times 10^{-6}$. Reconstruct the quadrature nodes $r_1, r_2$ and weights $w_1, w_2$ from $\\mu_0, \\mu_1, \\mu_2, \\mu_3$ and verify whether $\\{r_1, r_2\\}$ matches $\\{a,b\\}$ and $\\{w_1, w_2\\}$ matches $\\{N_1, N_2\\}$ to within an absolute tolerance of $1.0\\times 10^{-12}$. Report the result as a boolean.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above (i.e., $[e_1,e_2,e_3,b_4]$). All reported errors are dimensionless floats, and the final entry is a boolean.",
            "solution": "The problem requires the derivation and implementation of a Quadrature-Based Method of Moments (QBMM) closure to approximate the volumetric intercalation current density in a battery electrode. This involves using a two-node quadrature, constructed from the first four moments of the particle size distribution (PSD), to approximate an integral over the PSD.\n\nThe electrode volumetric intercalation current density, $J$, is given by the integral of the surface flux over all particle surfaces within a unit volume of the electrode. Given a particle number density $n(R)$ and a surface flux $j_{\\mathrm{surf}}(R) = k R^\\gamma$, where $R$ is the particle radius, $k  0$ and $\\gamma \\ge 0$, the expression for $J$ is:\n$$\nJ = \\int_{0}^{\\infty} (4\\pi R^2) \\, j_{\\mathrm{surf}}(R) \\, n(R) \\, dR\n$$\nSubstituting the expression for $j_{\\mathrm{surf}}(R)$, we get:\n$$\nJ = \\int_{0}^{\\infty} 4\\pi R^2 \\, (k R^\\gamma) \\, n(R) \\, dR = 4\\pi k \\int_{0}^{\\infty} R^{2+\\gamma} n(R) \\, dR\n$$\nBy the definition of the $q$-th raw moment of the PSD, $\\mu_q = \\int_{0}^{\\infty} R^q n(R) dR$, the exact expression for the current density is:\n$$\nJ_{\\mathrm{exact}} = 4\\pi k \\, \\mu_{2+\\gamma}\n$$\n\nThe QBMM approach approximates the integral over the PSD using a finite quadrature. For a two-node quadrature with nodes $r_1, r_2$ and weights $w_1, w_2$, the approximation is:\n$$\n\\int_{0}^{\\infty} f(R) \\, n(R) \\, dR \\approx w_1 f(r_1) + w_2 f(r_2)\n$$\nIn our case, the function to be integrated is $f(R) = R^{2+\\gamma}$. Applying this approximation to the integral in the expression for $J$ yields the QBMM-approximated current density, $J_{\\mathrm{QBMM}}$:\n$$\nJ_{\\mathrm{QBMM}} = 4\\pi k \\left( w_1 r_1^{2+\\gamma} + w_2 r_2^{2+\\gamma} \\right)\n$$\nThis expression constitutes the QBMM closure for the volumetric current density.\n\nThe core of the method is the reconstruction of the quadrature nodes and weights from a set of known moments. The problem specifies that the two-node quadrature must be exact for polynomials of degree up to $3$. This requires the nodes and weights to satisfy the following system of four equations, using the first four moments $\\mu_0, \\mu_1, \\mu_2, \\mu_3$:\n$$\nw_1 r_1^k + w_2 r_2^k = \\mu_k \\quad \\text{for} \\quad k \\in \\{0, 1, 2, 3\\}\n$$\n\nA standard and robust algorithm to determine the nodes $r_1, r_2$ from these moments involves solving a generalized eigenvalue problem. First, two Hankel matrices are constructed from the moments:\n$$\nH_0 = \\begin{bmatrix} \\mu_0  \\mu_1 \\\\ \\mu_1  \\mu_2 \\end{bmatrix}, \\quad\nH_1 = \\begin{bmatrix} \\mu_1  \\mu_2 \\\\ \\mu_2  \\mu_3 \\end{bmatrix}\n$$\nThe quadrature nodes, $r_1$ and $r_2$, are the eigenvalues $\\lambda$ of the generalized eigenvalue problem $H_1 \\mathbf{v} = \\lambda H_0 \\mathbf{v}$. For a valid set of moments from a positive measure (which is the case for any physically realistic PSD), $H_0$ is positive definite and the eigenvalues are guaranteed to be real and positive, corresponding to the quadrature nodes.\n\nOnce the nodes $r_1$ and $r_2$ are found, the weights $w_1$ and $w_2$ can be determined by solving the first two moment-matching equations ($k=0$ and $k=1$), which form a linear system:\n$$\n\\begin{bmatrix} 1  1 \\\\ r_1  r_2 \\end{bmatrix}\n\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} =\n\\begin{bmatrix} \\mu_0 \\\\ \\mu_1 \\end{bmatrix}\n$$\nThis system is guaranteed to have a unique solution for $w_1, w_2$ since the determinant of the coefficient matrix, $r_2 - r_1$, is non-zero for distinct nodes, a condition met by non-monodisperse distributions.\n\nThe overall computational procedure for each test case is as follows:\n1.  **Calculate Moments**: Given a PSD, calculate the first four raw moments $\\mu_0, \\mu_1, \\mu_2, \\mu_3$.\n    - For a lognormal PSD with parameters $N_0$, $m$, $s_g$, the moments are $\\mu_k = N_0 \\exp(k \\ln m + \\frac{1}{2} k^2 (\\ln s_g)^2)$.\n    - For a two-point atomic PSD $n(R) = N_1\\delta(R-a) + N_2\\delta(R-b)$, the moments are $\\mu_k = N_1 a^k + N_2 b^k$.\n2.  **Determine Quadrature**:\n    - Construct the Hankel matrices $H_0$ and $H_1$.\n    - Solve the generalized eigenvalue problem $H_1 \\mathbf{v} = \\lambda H_0 \\mathbf{v}$ to find the nodes $r_1, r_2$ (the eigenvalues $\\lambda$).\n    - Solve the linear system for the weights $w_1, w_2$.\n3.  **Compute Current Density and Error (Cases 1-3)**:\n    - Calculate $J_{\\mathrm{QBMM}} = 4\\pi k (w_1 r_1^{2+\\gamma} + w_2 r_2^{2+\\gamma})$.\n    - Calculate the exact moment $\\mu_{2+\\gamma}$ using the analytical formula for the lognormal distribution.\n    - Calculate $J_{\\mathrm{exact}} = 4\\pi k \\mu_{2+\\gamma}$.\n    - Compute the relative error $e = |J_{\\mathrm{QBMM}} - J_{\\mathrm{exact}}| / J_{\\mathrm{exact}}$.\n4.  **Verify Node/Weight Recovery (Case 4)**:\n    - A two-node quadrature is exact for an atomic distribution with two support points. Therefore, the reconstructed nodes $\\{r_1, r_2\\}$ and weights $\\{w_1, w_2\\}$ must match the true locations $\\{a, b\\}$ and magnitudes $\\{N_1, N_2\\}$ of the delta functions, respectively.\n    - We verify this by comparing the sorted sets of reconstructed and true parameters, checking if the absolute differences are within the specified tolerance of $1.0 \\times 10^{-12}$.\n\nThis procedure is implemented numerically using `scipy.linalg.eig` for the generalized eigenvalue problem and `numpy.linalg.solve` for the linear system determining the weights.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    def calculate_lognormal_moments(N0, m, sg, k_orders):\n        \"\"\"Calculates moments for a lognormal distribution.\"\"\"\n        ln_m = np.log(m)\n        ln_sg_sq = np.log(sg)**2\n        moments = [N0 * np.exp(k * ln_m + 0.5 * k**2 * ln_sg_sq) for k in k_orders]\n        return moments\n\n    def calculate_atomic_moments(N1, N2, a, b, k_orders):\n        \"\"\"Calculates moments for a two-point atomic distribution.\"\"\"\n        moments = [N1 * a**k + N2 * b**k for k in k_orders]\n        return moments\n\n    def reconstruct_quadrature(mu):\n        \"\"\"Reconstructs two-node quadrature from the first four moments.\"\"\"\n        mu0, mu1, mu2, mu3 = mu\n        \n        # Construct Hankel matrices\n        H0 = np.array([[mu0, mu1], [mu1, mu2]])\n        H1 = np.array([[mu1, mu2], [mu2, mu3]])\n\n        # Solve the generalized eigenvalue problem H1*v = lambda*H0*v for the nodes\n        # The eigenvalues are the nodes r1, r2.\n        # We take the real part to discard negligible imaginary parts from numerical error.\n        nodes = np.real(linalg.eig(H1, H0)[0])\n        r1, r2 = nodes[0], nodes[1]\n\n        # Solve the linear system for the weights w1, w2\n        # [ 1  1 ] [w1] = [mu0]\n        # [r1 r2] [w2]   [mu1]\n        A = np.array([[1, 1], [r1, r2]])\n        b = np.array([mu0, mu1])\n        weights = linalg.solve(A, b)\n        \n        return nodes, weights\n\n    def solve_lognormal_case(N0, m, sg, k_const, gamma):\n        \"\"\"Solves a lognormal PSD case and returns the relative error.\"\"\"\n        # Calculate the first four moments (k=0,1,2,3)\n        mu = calculate_lognormal_moments(N0, m, sg, [0, 1, 2, 3])\n        \n        # Reconstruct quadrature\n        nodes, weights = reconstruct_quadrature(mu)\n        r1, r2 = nodes[0], nodes[1]\n        w1, w2 = weights[0], weights[1]\n        \n        # Calculate J_QBMM\n        integral_approx = w1 * r1**(2 + gamma) + w2 * r2**(2 + gamma)\n        J_QBMM = 4 * np.pi * k_const * integral_approx\n        \n        # Calculate J_exact\n        mu_exact_order = 2 + gamma\n        mu_exact = calculate_lognormal_moments(N0, m, sg, [mu_exact_order])[0]\n        J_exact = 4 * np.pi * k_const * mu_exact\n        \n        # Calculate relative error\n        if J_exact == 0:\n            return 0.0 # Avoid division by zero\n        relative_error = np.abs(J_QBMM - J_exact) / np.abs(J_exact)\n        return relative_error\n\n    def solve_atomic_case(N1, N2, a, b, tol):\n        \"\"\"Solves the atomic PSD case and verifies node/weight recovery.\"\"\"\n        # Calculate the first four moments\n        mu = calculate_atomic_moments(N1, N2, a, b, [0, 1, 2, 3])\n\n        # Reconstruct quadrature\n        nodes_recon, weights_recon_unsorted = reconstruct_quadrature(mu)\n\n        # Sort the reconstructed nodes and their corresponding weights\n        sort_indices_recon = np.argsort(nodes_recon)\n        r_recon_sorted = nodes_recon[sort_indices_recon]\n        w_recon_sorted = weights_recon_unsorted[sort_indices_recon]\n\n        # Define and sort the true nodes and weights\n        nodes_true = np.array([a, b])\n        weights_true = np.array([N1, N2])\n        sort_indices_true = np.argsort(nodes_true)\n        r_true_sorted = nodes_true[sort_indices_true]\n        w_true_sorted = weights_true[sort_indices_true]\n\n        # Verify recovery within tolerance\n        nodes_match = np.allclose(r_recon_sorted, r_true_sorted, atol=tol, rtol=0)\n        weights_match = np.allclose(w_recon_sorted, w_true_sorted, atol=tol, rtol=0)\n        \n        return nodes_match and weights_match\n\n    # Test Case 1\n    err1 = solve_lognormal_case(N0=1.0e18, m=5.0e-6, sg=1.5, k_const=2.5, gamma=1.0)\n\n    # Test Case 2\n    err2 = solve_lognormal_case(N0=1.0e18, m=5.0e-6, sg=1.01, k_const=1.75, gamma=0.5)\n\n    # Test Case 3\n    err3 = solve_lognormal_case(N0=1.0e18, m=2.0e-6, sg=3.0, k_const=3.0, gamma=1.2)\n\n    # Test Case 4\n    bool4 = solve_atomic_case(N1=6.0e17, N2=4.0e17, a=3.0e-6, b=8.0e-6, tol=1.0e-12)\n\n    results = [err1, err2, err3, bool4]\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The effects of particle size distribution extend beyond electrochemical kinetics to the very fabric of the electrode's microstructure. This practice delves into the critical issue of electronic connectivity, particularly for large active material particles that are prone to isolation after electrode calendering. You will apply principles of stochastic geometry and marked point processes to build a model that predicts the probability of a particle losing its connection to the conductive network . Mastering this approach provides a quantitative tool to analyze and mitigate a key degradation mechanism tied to electrode design and manufacturing.",
            "id": "3938255",
            "problem": "You are modeling electronic connectivity in a calendered composite electrode consisting of active material particles and a conductive additive network. Consider a three-dimensional marked point process model in which potential conductive neighbors are represented by points in space with independent, identically distributed marks. The spatial locations are modeled by a homogeneous Poisson point process of intensity $\\rho_0$ (number density in $\\mathrm{m}^{-3}$), and each neighbor carries a mark $R$ (in $\\mathrm{m}$) that encodes its effective interaction size. The coarse active particle of interest has a fixed radius $r$ (in $\\mathrm{m}$). A conductive contact between the coarse particle and a neighbor is present if the neighbor lies within a distance threshold determined by calendering-induced deformation, modeled as $d_{\\mathrm{th}} = \\alpha(\\varepsilon)\\,(r + R)$, where $\\varepsilon \\in [0,1)$ is the imposed compressive strain (dimensionless) and $\\alpha(\\varepsilon) = 1 + \\beta\\,\\varepsilon$ with $\\beta  0$ (dimensionless).\n\nTo capture binder and conductive additive redistribution under calendering, assume that the local effective intensity of conductive neighbors accessible to the coarse particle is attenuated by its size according to\n$$\n\\rho_{\\mathrm{eff}}(r) = \\frac{\\rho_0}{1 + \\left(\\frac{r}{r_s}\\right)^{\\delta}},\n$$\nwhere $r_s  0$ (in $\\mathrm{m}$) is a size scale that sets the onset of depletion and $\\delta \\ge 0$ (dimensionless) controls the severity of depletion.\n\nThe neighbor mark $R$ follows a lognormal distribution or a mixture of lognormal distributions. For a single lognormal component, the natural logarithm $\\ln R$ is distributed as a normal random variable with mean $\\mu$ and standard deviation $\\sigma$, so that $R$ has probability density function\n$$\np_R(R) = \\frac{1}{R\\,\\sigma\\,\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln R - \\mu)^2}{2\\sigma^2}\\right), \\quad R  0.\n$$\nFor a mixture, assume components $i = 1,\\dots,M$ with weights $w_i$ satisfying $\\sum_{i=1}^M w_i = 1$, and $\\ln R$ distributed as $\\mathcal{N}(\\mu_i,\\sigma_i^2)$ in component $i$.\n\nStart from the properties of a homogeneous Poisson point process in three dimensions and independent mark-dependent thinning induced by the contact criterion. Using only these foundations, derive an expression for the probability that the coarse particle has fewer than $k_{\\mathrm{req}}$ conductive contacts (i.e., degree less than $k_{\\mathrm{req}}$). Then, implement a program to evaluate this isolation probability for each of the test cases below.\n\nAssumptions to use:\n- The spatial process of neighbors is homogeneous and independent of the coarse particle except through the attenuation factor $\\rho_{\\mathrm{eff}}(r)$.\n- The contact condition is purely geometric and determined by $d_{\\mathrm{th}} = \\alpha(\\varepsilon)\\,(r + R)$.\n- The system is three-dimensional, and the relevant geometric volume associated with the contact region for a given $R$ is the ball volume $V_3(x) = \\frac{4}{3}\\pi x^3$.\n\nUnits and output specification:\n- Use $\\mathrm{m}$ for all lengths, and $\\mathrm{m}^{-3}$ for number densities. Strain $\\varepsilon$, attenuation exponent $\\delta$, and $\\beta$ are dimensionless.\n- For each test case, output the isolation probability as a decimal number (a float). No angle units or percentages are involved.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nTest suite:\nEvaluate the isolation probability for the following parameter sets. In all cases, dimension is three, and the neighbor mark distribution is a mixture with $M=2$ components.\n\n- Case 1 (general polydisperse neighbors, moderate coarse size, moderate compression):\n    - $\\rho_0 = 5.0 \\times 10^{14}\\ \\mathrm{m}^{-3}$,\n    - $r = 8.0 \\times 10^{-6}\\ \\mathrm{m}$,\n    - $\\varepsilon = 0.30$,\n    - $\\beta = 0.60$,\n    - $r_s = 1.0 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\delta = 1.20$,\n    - Mixture weights $(w_1,w_2) = (0.7,0.3)$,\n    - Component 1: $\\mu_1 = \\ln(5.0 \\times 10^{-7})$, $\\sigma_1 = 0.35$,\n    - Component 2: $\\mu_2 = \\ln(2.0 \\times 10^{-6})$, $\\sigma_2 = 0.50$,\n    - $k_{\\mathrm{req}} = 1$.\n\n- Case 2 (larger coarse size, same neighbor population, low compression):\n    - $\\rho_0 = 5.0 \\times 10^{14}\\ \\mathrm{m}^{-3}$,\n    - $r = 2.5 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\varepsilon = 0.25$,\n    - $\\beta = 0.60$,\n    - $r_s = 1.0 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\delta = 1.20$,\n    - Mixture weights $(w_1,w_2) = (0.7,0.3)$,\n    - Component 1: $\\mu_1 = \\ln(5.0 \\times 10^{-7})$, $\\sigma_1 = 0.35$,\n    - Component 2: $\\mu_2 = \\ln(2.0 \\times 10^{-6})$, $\\sigma_2 = 0.50$,\n    - $k_{\\mathrm{req}} = 1$.\n\n- Case 3 (larger coarse size, higher compression):\n    - $\\rho_0 = 5.0 \\times 10^{14}\\ \\mathrm{m}^{-3}$,\n    - $r = 2.5 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\varepsilon = 0.60$,\n    - $\\beta = 0.60$,\n    - $r_s = 1.0 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\delta = 1.20$,\n    - Mixture weights $(w_1,w_2) = (0.7,0.3)$,\n    - Component 1: $\\mu_1 = \\ln(5.0 \\times 10^{-7})$, $\\sigma_1 = 0.35$,\n    - Component 2: $\\mu_2 = \\ln(2.0 \\times 10^{-6})$, $\\sigma_2 = 0.50$,\n    - $k_{\\mathrm{req}} = 1$.\n\n- Case 4 (low neighbor density, moderate coarse size, moderate compression):\n    - $\\rho_0 = 1.0 \\times 10^{13}\\ \\mathrm{m}^{-3}$,\n    - $r = 8.0 \\times 10^{-6}\\ \\mathrm{m}$,\n    - $\\varepsilon = 0.30$,\n    - $\\beta = 0.60$,\n    - $r_s = 1.0 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\delta = 1.20$,\n    - Mixture weights $(w_1,w_2) = (0.7,0.3)$,\n    - Component 1: $\\mu_1 = \\ln(5.0 \\times 10^{-7})$, $\\sigma_1 = 0.35$,\n    - Component 2: $\\mu_2 = \\ln(2.0 \\times 10^{-6})$, $\\sigma_2 = 0.50$,\n    - $k_{\\mathrm{req}} = 1$.\n\n- Case 5 (same as Case 1 but requiring two contacts to avoid isolation):\n    - $\\rho_0 = 5.0 \\times 10^{14}\\ \\mathrm{m}^{-3}$,\n    - $r = 8.0 \\times 10^{-6}\\ \\mathrm{m}$,\n    - $\\varepsilon = 0.30$,\n    - $\\beta = 0.60$,\n    - $r_s = 1.0 \\times 10^{-5}\\ \\mathrm{m}$,\n    - $\\delta = 1.20$,\n    - Mixture weights $(w_1,w_2) = (0.7,0.3)$,\n    - Component 1: $\\mu_1 = \\ln(5.0 \\times 10^{-7})$, $\\sigma_1 = 0.35$,\n    - Component 2: $\\mu_2 = \\ln(2.0 \\times 10^{-6})$, $\\sigma_2 = 0.50$,\n    - $k_{\\mathrm{req}} = 2$.\n\nYour program must implement the computation implied by your derivation and print a single line containing a list of five floats, corresponding to the isolation probabilities for Cases 1 through 5, in order.",
            "solution": "The problem requires the derivation and calculation of the probability that a coarse active material particle has fewer than $k_{\\mathrm{req}}$ conductive contacts in a composite electrode. The underlying model is a three-dimensional marked Poisson point process. The solution proceeds by first deriving the expected number of contacts, $\\Lambda$, and then using this value as the parameter of a Poisson distribution to calculate the desired probability.\n\nThe number of conductive contacts, $K$, that a coarse particle of a fixed radius $r$ makes with its neighboring conductive additive particles is a random variable. Based on the properties of a homogeneous Poisson point process subjected to independent, mark-dependent thinning, the distribution of $K$ is a Poisson distribution, denoted $K \\sim \\text{Poisson}(\\Lambda)$. The parameter $\\Lambda$ is the mean number of contacts. Our first goal is to derive an expression for $\\Lambda$.\n\nThe mean number of contacts, $\\Lambda$, is the product of the effective number density of potential neighbors, $\\rho_{\\mathrm{eff}}(r)$, and the expected volume of the region where a contact can occur, $\\mathbb{E}[V(R)]$.\n$$\n\\Lambda = \\rho_{\\mathrm{eff}}(r) \\cdot \\mathbb{E}[V(R)]\n$$\nThe problem provides the expression for the effective density, which accounts for the local depletion of conductive additive around a large active particle:\n$$\n\\rho_{\\mathrm{eff}}(r) = \\frac{\\rho_0}{1 + \\left(\\frac{r}{r_s}\\right)^{\\delta}}\n$$\nwhere $\\rho_0$ is the bulk number density, $r_s$ is a characteristic size scale, and $\\delta$ is the depletion exponent.\n\nThe volume of the contact region, $V(R)$, depends on the size (mark) $R$ of the neighboring particle. A contact occurs if the neighbor's center is within a distance $d_{\\mathrm{th}}$ of the coarse particle's center. This threshold distance is given by $d_{\\mathrm{th}} = \\alpha(\\varepsilon)(r + R)$, where $\\alpha(\\varepsilon) = 1 + \\beta\\varepsilon$ captures the effect of compressive strain $\\varepsilon$. Since the system is three-dimensional, the contact volume for a specific neighbor of size $R$ is a sphere of radius $d_{\\mathrm{th}}$:\n$$\nV(R) = \\frac{4}{3}\\pi [d_{\\mathrm{th}}(R)]^3 = \\frac{4}{3}\\pi [\\alpha(\\varepsilon)(r+R)]^3\n$$\nTo find the expected contact volume, $\\mathbb{E}[V(R)]$, we must average $V(R)$ over the probability distribution of the neighbor mark $R$.\n$$\n\\mathbb{E}[V(R)] = \\mathbb{E}\\left[\\frac{4}{3}\\pi \\alpha(\\varepsilon)^3 (r+R)^3\\right]\n$$\nSince $r$, $\\varepsilon$, and $\\beta$ are fixed for a given scenario, $\\alpha(\\varepsilon)$ is a constant. By linearity of expectation:\n$$\n\\mathbb{E}[V(R)] = \\frac{4}{3}\\pi \\alpha(\\varepsilon)^3 \\mathbb{E}[(r+R)^3]\n$$\nWe expand the term $(r+R)^3$ and apply the expectation operator:\n$$\n\\mathbb{E}[(r+R)^3] = \\mathbb{E}[r^3 + 3r^2R + 3rR^2 + R^3] = r^3 + 3r^2\\mathbb{E}[R] + 3r\\mathbb{E}[R^2] + \\mathbb{E}[R^3]\n$$\nThis demonstrates that the calculation of $\\mathbb{E}[V(R)]$ requires the first three moments of the neighbor size distribution, $\\mathbb{E}[R]$, $\\mathbb{E}[R^2]$, and $\\mathbb{E}[R^3]$.\n\nThe neighbor size $R$ follows a mixture of $M$ lognormal distributions. For each component $i$, $\\ln R$ is normally distributed as $\\mathcal{N}(\\mu_i, \\sigma_i^2)$. The $k$-th moment of a single lognormal distribution with these parameters is given by:\n$$\n\\mathbb{E}[R_i^k] = \\exp\\left(k\\mu_i + \\frac{k^2\\sigma_i^2}{2}\\right)\n$$\nFor a mixture distribution with component weights $w_i$ (where $\\sum_{i=1}^M w_i = 1$), the $k$-th moment is the weighted average of the component moments:\n$$\n\\mathbb{E}[R^k] = \\sum_{i=1}^{M} w_i \\mathbb{E}[R_i^k] = \\sum_{i=1}^{M} w_i \\exp\\left(k\\mu_i + \\frac{k^2\\sigma_i^2}{2}\\right)\n$$\nWe can now compute $\\mathbb{E}[R]$, $\\mathbb{E}[R^2]$, and $\\mathbb{E}[R^3]$ by setting $k=1, 2, 3$ respectively in the above formula.\n\nBy substituting these expressions back, we obtain the final formula for the mean number of contacts, $\\Lambda$:\n$$\n\\Lambda = \\left( \\frac{\\rho_0}{1 + (r/r_s)^\\delta} \\right) \\left( \\frac{4}{3}\\pi (1+\\beta\\varepsilon)^3 \\right) \\left( r^3 + 3r^2\\mathbb{E}[R] + 3r\\mathbb{E}[R^2] + \\mathbb{E}[R^3] \\right)\n$$\n\nWith $\\Lambda$ determined, we can calculate the \"isolation probability\", which is the probability that the particle has fewer than $k_{\\mathrm{req}}$ contacts, $P(K  k_{\\mathrm{req}})$. Since $K$ follows a Poisson distribution with mean $\\Lambda$, this probability corresponds to the cumulative distribution function (CDF) of the Poisson distribution evaluated at $k_{\\mathrm{req}}-1$:\n$$\nP(K  k_{\\mathrm{req}}) = P(K \\le k_{\\mathrm{req}}-1) = \\sum_{j=0}^{k_{\\mathrm{req}}-1} P(K=j) = \\sum_{j=0}^{k_{\\mathrm{req}}-1} \\frac{\\Lambda^j e^{-\\Lambda}}{j!}\n$$\nFor the special case $k_{\\mathrm{req}}=1$, this probability simplifies to $P(K1) = P(K=0) = e^{-\\Lambda}$. For $k_{\\mathrm{req}}=2$, it is $P(K2) = P(K=0) + P(K=1) = e^{-\\Lambda} + \\Lambda e^{-\\Lambda} = e^{-\\Lambda}(1+\\Lambda)$. The general sum is used for implementation.\n\nThe computational algorithm proceeds as follows:\n1.  For a given set of parameters ($\\rho_0$, $r$, $\\varepsilon$, $\\beta$, $r_s$, $\\delta$, $\\{w_i, \\mu_i, \\sigma_i\\}_{i=1..M}$, $k_{\\mathrm{req}}$), calculate the intermediate constants $\\alpha(\\varepsilon)$ and $\\rho_{\\mathrm{eff}}(r)$.\n2.  Compute the first three moments of the mark distribution, $\\mathbb{E}[R]$, $\\mathbb{E}[R^2]$, $\\mathbb{E}[R^3]$, using the mixture model formula.\n3.  Calculate the expected value $\\mathbb{E}[(r+R)^3]$.\n4.  Combine these results to compute the mean number of contacts, $\\Lambda$.\n5.  Finally, calculate the desired probability $P(K  k_{\\mathrm{req}})$ using the Poisson CDF with parameter $\\Lambda$.\nThis procedure is applied to each test case to generate the final results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Solves the particle connectivity problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"rho_0\": 5.0e14, \"r\": 8.0e-6, \"epsilon\": 0.30, \"beta\": 0.60,\n            \"r_s\": 1.0e-5, \"delta\": 1.20, \"k_req\": 1,\n            \"mixture\": {\n                \"w\": [0.7, 0.3],\n                \"mu\": [np.log(5.0e-7), np.log(2.0e-6)],\n                \"sigma\": [0.35, 0.50]\n            }\n        },\n        {\n            \"rho_0\": 5.0e14, \"r\": 2.5e-5, \"epsilon\": 0.25, \"beta\": 0.60,\n            \"r_s\": 1.0e-5, \"delta\": 1.20, \"k_req\": 1,\n            \"mixture\": {\n                \"w\": [0.7, 0.3],\n                \"mu\": [np.log(5.0e-7), np.log(2.0e-6)],\n                \"sigma\": [0.35, 0.50]\n            }\n        },\n        {\n            \"rho_0\": 5.0e14, \"r\": 2.5e-5, \"epsilon\": 0.60, \"beta\": 0.60,\n            \"r_s\": 1.0e-5, \"delta\": 1.20, \"k_req\": 1,\n            \"mixture\": {\n                \"w\": [0.7, 0.3],\n                \"mu\": [np.log(5.0e-7), np.log(2.0e-6)],\n                \"sigma\": [0.35, 0.50]\n            }\n        },\n        {\n            \"rho_0\": 1.0e13, \"r\": 8.0e-6, \"epsilon\": 0.30, \"beta\": 0.60,\n            \"r_s\": 1.0e-5, \"delta\": 1.20, \"k_req\": 1,\n            \"mixture\": {\n                \"w\": [0.7, 0.3],\n                \"mu\": [np.log(5.0e-7), np.log(2.0e-6)],\n                \"sigma\": [0.35, 0.50]\n            }\n        },\n        {\n            \"rho_0\": 5.0e14, \"r\": 8.0e-6, \"epsilon\": 0.30, \"beta\": 0.60,\n            \"r_s\": 1.0e-5, \"delta\": 1.20, \"k_req\": 2,\n            \"mixture\": {\n                \"w\": [0.7, 0.3],\n                \"mu\": [np.log(5.0e-7), np.log(2.0e-6)],\n                \"sigma\": [0.35, 0.50]\n            }\n        }\n    ]\n\n    def calculate_isolation_probability(params):\n        \"\"\"\n        Calculates the isolation probability for a single set of parameters.\n        \"\"\"\n        rho_0 = params[\"rho_0\"]\n        r = params[\"r\"]\n        epsilon = params[\"epsilon\"]\n        beta = params[\"beta\"]\n        r_s = params[\"r_s\"]\n        delta = params[\"delta\"]\n        k_req = params[\"k_req\"]\n        mixture_params = params[\"mixture\"]\n\n        # Step 1: Calculate intermediate constants\n        alpha = 1.0 + beta * epsilon\n        rho_eff = rho_0 / (1.0 + (r / r_s)**delta)\n\n        # Step 2: Calculate moments of the mark distribution R\n        weights = mixture_params[\"w\"]\n        mus = mixture_params[\"mu\"]\n        sigmas = mixture_params[\"sigma\"]\n        \n        moments_R = [0.0, 0.0, 0.0] # For E[R^1], E[R^2], E[R^3]\n        for k in range(1, 4):\n            moment_k = 0.0\n            for i in range(len(weights)):\n                moment_k += weights[i] * np.exp(k * mus[i] + 0.5 * (k * sigmas[i])**2)\n            moments_R[k-1] = moment_k\n        \n        E_R1, E_R2, E_R3 = moments_R[0], moments_R[1], moments_R[2]\n\n        # Step 3: Calculate the expected value of (r+R)^3\n        E_r_plus_R_cubed = r**3 + 3 * r**2 * E_R1 + 3 * r * E_R2 + E_R3\n\n        # Step 4: Combine to compute the mean number of contacts, Lambda\n        E_V_R = (4.0 / 3.0) * np.pi * (alpha**3) * E_r_plus_R_cubed\n        Lambda = rho_eff * E_V_R\n\n        # Step 5: Calculate the probability P(K  k_req) using Poisson CDF\n        # P(K  k_req) is the CDF evaluated at k_req - 1\n        if k_req == 0:\n            return 0.0\n        \n        # scipy.stats.poisson.cdf(k, mu) computes P(K = k) for a Poisson(mu)\n        prob = poisson.cdf(k_req - 1, Lambda)\n        \n        return prob\n\n    results = []\n    for case in test_cases:\n        result = calculate_isolation_probability(case)\n        results.append(f\"{result:.10f}\") # Format for consistent output\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}