{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Bayesian calibration is not merely finding the single best-fit parameter set, but rigorously quantifying the uncertainty associated with it. This practice delves into this by using the Fisher Information Matrix, a concept that links the curvature of the likelihood surface to the precision of our parameter estimates. By deriving and computing the Fisher Information for a simplified battery model, you will gain a concrete understanding of how experimental conditions influence posterior uncertainty through the Laplace approximation .",
            "id": "3895848",
            "problem": "You are tasked with deriving and implementing a computation of the Fisher information for a simplified Single Particle Model (SPM) of a lithium-ion battery that includes Solid Electrolyte Interphase (SEI) effects, under a dimensionless normalization. The measurement model for the dimensionless terminal voltage residuals is defined by a deterministic prediction combined with additive Gaussian noise. The deterministic prediction includes contributions from ohmic drop, charge-transfer effects, diffusion-limited transport, and SEI film resistance growth. All quantities are dimensionless due to normalization against suitable reference scales, so no physical units are to be used anywhere in the calculations.\n\nConsider a constant-current experiment with dimensionless current $I$ and observation times $\\{t_i\\}_{i=1}^N$. The dimensionless predicted terminal voltage is modeled as\n$$\nV_i(\\theta) = U_0 - I \\left( R_{\\mathrm{ohm}} + R_{\\mathrm{ct},0} + \\frac{\\gamma}{D_s} + R_{f,0} + k_{\\mathrm{SEI}} \\sqrt{t_i} \\right),\n$$\nwhere $U_0$, $R_{\\mathrm{ohm}}$, $R_{\\mathrm{ct},0}$, and $\\gamma$ are known positive constants, and the vector of unknown parameters is\n$$\n\\theta = \\begin{bmatrix} k_{\\mathrm{SEI}} \\\\ R_{f,0} \\\\ D_s \\end{bmatrix},\n$$\nwith $k_{\\mathrm{SEI}}$ the SEI growth coefficient, $R_{f,0}$ the initial SEI film resistance term, and $D_s$ the dimensionless solid diffusion coefficient. Observations satisfy\n$$\ny_i = V_i(\\theta) + \\varepsilon_i,\n$$\nwith independent and identically distributed noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, for known noise level $\\sigma > 0$.\n\nStarting from the Gaussian likelihood for independent observations and the definition of Fisher information as the expectation of the negative Hessian of the log-likelihood with respect to $\\theta$, derive the expression for the Fisher information matrix at a nominal parameter value $\\theta_0$ and compute it for the given model. Then, under a Gaussian prior $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_{\\mathrm{prior}})$ with known diagonal covariance matrix $\\Sigma_{\\mathrm{prior}}$, derive the Laplace approximation to the posterior covariance,\n$$\n\\Sigma_{\\mathrm{post}} \\approx \\left( \\Sigma_{\\mathrm{prior}}^{-1} + \\mathcal{I}(\\theta_0) \\right)^{-1},\n$$\nwhere $\\mathcal{I}(\\theta_0)$ is the Fisher information matrix at $\\theta_0$. Implement a program that, for each test case, computes the diagonal entries of $\\Sigma_{\\mathrm{post}}$.\n\nYour derivation must begin from the log-likelihood of independent Gaussian noise and proceed through the computation of the gradient of the model prediction with respect to $\\theta$, forming the Jacobian matrix $J(\\theta_0)$ whose $(i,j)$ entry is $\\partial V_i(\\theta)/\\partial \\theta_j$ evaluated at $\\theta_0$, and then using this to evaluate the Fisher information. Do not invoke any results without derivation from these bases. All calculations and outputs must be in dimensionless form.\n\nUse the following test suite. Each test case specifies the known constants, the experiment design, the nominal parameter $\\theta_0$, and the prior covariance. For each case, compute the posterior covariance approximation and output its diagonal entries as a list of three floats in the order corresponding to $[k_{\\mathrm{SEI}}, R_{f,0}, D_s]$:\n\n- Test Case 1 (general, moderate information):\n  - Known constants: $U_0 = 3.8$, $R_{\\mathrm{ohm}} = 0.02$, $R_{\\mathrm{ct},0} = 0.05$, $\\gamma = 0.2$.\n  - Experiment: $I = 1.0$, times $t = [0.01, 0.1, 1.0, 3.0, 10.0]$, noise $\\sigma = 0.005$.\n  - Nominal parameter: $\\theta_0 = [1\\times 10^{-3},\\; 0.1,\\; 0.5]$.\n  - Prior covariance (diagonal): $\\Sigma_{\\mathrm{prior}} = \\mathrm{diag}([1\\times 10^{-4},\\; 1\\times 10^{-2},\\; 1\\times 10^{-1}])$.\n\n- Test Case 2 (boundary times including $t=0$, low noise):\n  - Known constants: $U_0 = 3.8$, $R_{\\mathrm{ohm}} = 0.02$, $R_{\\mathrm{ct},0} = 0.05$, $\\gamma = 0.2$.\n  - Experiment: $I = 2.0$, times $t = [0.0, 0.1, 0.4, 1.6, 6.4]$, noise $\\sigma = 0.001$.\n  - Nominal parameter: $\\theta_0 = [5\\times 10^{-4},\\; 0.05,\\; 0.3]$.\n  - Prior covariance (diagonal): $\\Sigma_{\\mathrm{prior}} = \\mathrm{diag}([1\\times 10^{-5},\\; 5\\times 10^{-3},\\; 5\\times 10^{-2}])$.\n\n- Test Case 3 (edge case, zero current leading to degenerate information):\n  - Known constants: $U_0 = 3.8$, $R_{\\mathrm{ohm}} = 0.02$, $R_{\\mathrm{ct},0} = 0.05$, $\\gamma = 0.2$.\n  - Experiment: $I = 0.0$, times $t = [0.1, 1.0, 10.0]$, noise $\\sigma = 0.01$.\n  - Nominal parameter: $\\theta_0 = [1\\times 10^{-3},\\; 0.1,\\; 0.5]$.\n  - Prior covariance (diagonal): $\\Sigma_{\\mathrm{prior}} = \\mathrm{diag}([1\\times 10^{-4},\\; 1\\times 10^{-2},\\; 1\\times 10^{-1}])$.\n\n- Test Case 4 (high noise, weak information):\n  - Known constants: $U_0 = 3.8$, $R_{\\mathrm{ohm}} = 0.02$, $R_{\\mathrm{ct},0} = 0.05$, $\\gamma = 0.2$.\n  - Experiment: $I = 1.0$, times $t = [0.1, 0.2, 0.4]$, noise $\\sigma = 0.1$.\n  - Nominal parameter: $\\theta_0 = [1\\times 10^{-3},\\; 0.1,\\; 1.0]$.\n  - Prior covariance (diagonal): $\\Sigma_{\\mathrm{prior}} = \\mathrm{diag}([1\\times 10^{-3},\\; 1\\times 10^{-1},\\; 1])$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, for example, \"[[v11,v12,v13],[v21,v22,v23],...]\", where each inner list contains the three diagonal entries of $\\Sigma_{\\mathrm{post}}$ for the corresponding test case, in the order $[k_{\\mathrm{SEI}}, R_{f,0}, D_s]$. All outputs must be dimensionless floats.",
            "solution": "The user has provided a valid problem statement. The task is to derive and compute the Laplace approximation to the posterior covariance for the parameters of a simplified battery degradation model. The derivation will proceed from first principles, starting with the log-likelihood of the measurement model.\n\n### 1. Derivation of the Fisher Information Matrix\n\nThe problem posits a measurement model where each observation $y_i$ is the sum of a deterministic model prediction $V_i(\\theta)$ and a random noise term $\\varepsilon_i$. The noise terms are assumed to be independent and identically distributed following a Gaussian distribution, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, with known variance $\\sigma^2$. The parameter vector to be inferred is $\\theta = [k_{\\mathrm{SEI}}, R_{f,0}, D_s]^T$.\n\nThe probability density function (PDF) for a single observation $y_i$ given the parameters $\\theta$ is:\n$$\np(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - V_i(\\theta))^2}{2\\sigma^2}\\right)\n$$\nFor a set of $N$ independent observations $Y = \\{y_1, \\dots, y_N\\}$, the joint likelihood $L(\\theta|Y)$ is the product of the individual PDFs. It is more convenient to work with the log-likelihood $\\ell(\\theta|Y) = \\log L(\\theta|Y)$:\n$$\n\\ell(\\theta|Y) = \\sum_{i=1}^{N} \\log p(y_i|\\theta) = \\sum_{i=1}^{N} \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i - V_i(\\theta))^2}{2\\sigma^2} \\right]\n$$\n$$\n\\ell(\\theta|Y) = -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} (y_i - V_i(\\theta))^2\n$$\nThe Fisher Information Matrix (FIM), denoted $\\mathcal{I}(\\theta)$, is defined as the negative expectation of the Hessian of the log-likelihood with respect to the parameters $\\theta$. The $(j,k)$-th element of the FIM is:\n$$\n\\mathcal{I}_{jk}(\\theta) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\theta|Y)}{\\partial \\theta_j \\partial \\theta_k}\\right]\n$$\nwhere a parameter $\\theta_j$ is the $j$-th element of the vector $\\theta$.\n\nFirst, we find the gradient of the log-likelihood (the score function):\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} (y_i - V_i(\\theta))^2 \\right) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{N} (y_i - V_i(\\theta)) \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j}\n$$\nNext, we compute the Hessian by differentiating with respect to $\\theta_k$:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{N} \\frac{\\partial}{\\partial \\theta_k} \\left( (y_i - V_i(\\theta)) \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j} \\right)\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{N} \\left[ -\\frac{\\partial V_i(\\theta)}{\\partial \\theta_k} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j} + (y_i - V_i(\\theta)) \\frac{\\partial^2 V_i(\\theta)}{\\partial \\theta_j \\partial \\theta_k} \\right]\n$$\nTaking the expectation $\\mathbb{E}[\\cdot]$ over the data distribution, we use the fact that $\\mathbb{E}[y_i] = V_i(\\theta)$, which implies $\\mathbb{E}[y_i - V_i(\\theta)] = 0$. This simplifies the expectation of the Hessian:\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k}\\right] = -\\frac{1}{\\sigma^2}\\sum_{i=1}^{N} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_k}\n$$\nApplying the definition of the FIM, we get:\n$$\n\\mathcal{I}_{jk}(\\theta) = - \\left( -\\frac{1}{\\sigma^2}\\sum_{i=1}^{N} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_k} \\right) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{N} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j} \\frac{\\partial V_i(\\theta)}{\\partial \\theta_k}\n$$\nThis expression can be conveniently written in matrix form. Let $J(\\theta)$ be the $N \\times 3$ Jacobian matrix where the entry $J_{ij}(\\theta) = \\frac{\\partial V_i(\\theta)}{\\partial \\theta_j}$. Then the FIM is:\n$$\n\\mathcal{I}(\\theta) = \\frac{1}{\\sigma^2} J(\\theta)^T J(\\theta)\n$$\n\n### 2. Jacobian Matrix of the Model\n\nThe voltage model is given by:\n$$\nV_i(\\theta) = U_0 - I \\left( R_{\\mathrm{ohm}} + R_{\\mathrm{ct},0} + \\frac{\\gamma}{D_s} + R_{f,0} + k_{\\mathrm{SEI}} \\sqrt{t_i} \\right)\n$$\nThe parameter vector is $\\theta = [\\theta_1, \\theta_2, \\theta_3]^T = [k_{\\mathrm{SEI}}, R_{f,0}, D_s]^T$. We compute the partial derivatives of $V_i$ with respect to each parameter:\n$$\n\\frac{\\partial V_i}{\\partial \\theta_1} = \\frac{\\partial V_i}{\\partial k_{\\mathrm{SEI}}} = -I \\sqrt{t_i}\n$$\n$$\n\\frac{\\partial V_i}{\\partial \\theta_2} = \\frac{\\partial V_i}{\\partial R_{f,0}} = -I\n$$\n$$\n\\frac{\\partial V_i}{\\partial \\theta_3} = \\frac{\\partial V_i}{\\partial D_s} = -I \\frac{\\partial}{\\partial D_s}\\left(\\frac{\\gamma}{D_s}\\right) = -I \\left(-\\frac{\\gamma}{D_s^2}\\right) = \\frac{I \\gamma}{D_s^2}\n$$\nThe $i$-th row of the Jacobian matrix $J(\\theta)$ is the transpose of the gradient of $V_i(\\theta)$, i.e., $\\nabla_\\theta V_i(\\theta)^T$:\n$$\nJ(\\theta)_{i,:} = \\left[ -I \\sqrt{t_i}, \\quad -I, \\quad \\frac{I \\gamma}{D_s^2} \\right]\n$$\n\n### 3. Posterior Covariance Approximation\n\nThe problem requires using the Laplace approximation for the posterior covariance, given a Gaussian prior $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_{\\mathrm{prior}})$. Bayes' theorem states that the posterior distribution is proportional to the product of the likelihood and the prior: $p(\\theta|Y) \\propto L(Y|\\theta)p(\\theta)$. The Laplace approximation approximates the posterior distribution with a Gaussian centered at the posterior mode, and its covariance matrix is the inverse of the negative Hessian of the log-posterior evaluated at the mode.\n\nThe log-posterior is $\\log p(\\theta|Y) = \\ell(\\theta|Y) + \\log p(\\theta) + \\text{const.}$. The negative Hessian of the log-posterior is:\n$$\n-\\frac{\\partial^2 \\log p(\\theta|Y)}{\\partial\\theta\\partial\\theta^T} = -\\frac{\\partial^2 \\ell(\\theta|Y)}{\\partial\\theta\\partial\\theta^T} - \\frac{\\partial^2 \\log p(\\theta)}{\\partial\\theta\\partial\\theta^T}\n$$\nFor a Gaussian prior, the Hessian of its logarithm is constant: $-\\frac{\\partial^2 \\log p(\\theta)}{\\partial\\theta\\partial\\theta^T} = \\Sigma_{\\mathrm{prior}}^{-1}$. Approximating the posterior mode with the prior mean $\\theta_0$ and taking the expectation yields the posterior precision matrix:\n$$\n\\Sigma_{\\mathrm{post}}^{-1} \\approx \\mathbb{E}\\left[-\\left.\\frac{\\partial^2 \\ell}{\\partial\\theta\\partial\\theta^T}\\right|_{\\theta_0}\\right] + \\Sigma_{\\mathrm{prior}}^{-1} = \\mathcal{I}(\\theta_0) + \\Sigma_{\\mathrm{prior}}^{-1}\n$$\nThe posterior covariance matrix is therefore:\n$$\n\\Sigma_{\\mathrm{post}} \\approx \\left( \\mathcal{I}(\\theta_0) + \\Sigma_{\\mathrm{prior}}^{-1} \\right)^{-1}\n$$\n\n### 4. Computational Steps\n\nFor each test case, the algorithm is as follows:\n1.  Initialize known constants and experimental design parameters: $\\gamma$, $I$, $\\{t_i\\}$, $\\sigma$, $\\theta_0 = [k_{\\mathrm{SEI},0}, R_{f,0,0}, D_{s,0}]^T$, and the diagonal elements of $\\Sigma_{\\mathrm{prior}}$.\n2.  Construct the $N \\times 3$ Jacobian matrix $J(\\theta_0)$ by evaluating the derived partial derivatives at $\\theta_0$ for each time point $t_i$.\n3.  Compute the Fisher Information Matrix $\\mathcal{I}(\\theta_0) = \\frac{1}{\\sigma^2} J(\\theta_0)^T J(\\theta_0)$.\n4.  Construct the prior precision matrix $\\Sigma_{\\mathrm{prior}}^{-1}$, which is a diagonal matrix whose diagonal entries are the reciprocals of the given diagonal entries of $\\Sigma_{\\mathrm{prior}}$.\n5.  Compute the posterior precision matrix $M = \\mathcal{I}(\\theta_0) + \\Sigma_{\\mathrm{prior}}^{-1}$.\n6.  Invert $M$ to obtain the posterior covariance approximation: $\\Sigma_{\\mathrm{post}} = M^{-1}$.\n7.  Extract the diagonal entries of $\\Sigma_{\\mathrm{post}}$, which correspond to the posterior variances of $k_{\\mathrm{SEI}}$, $R_{f,0}$, and $D_s$.\nThis procedure is directly implemented in the provided Python code. A special case occurs when $I=0$, which makes the Jacobian matrix a zero matrix, resulting in $\\mathcal{I}(\\theta_0) = 0$. In this scenario, the posterior covariance $\\Sigma_{\\mathrm{post}}$ becomes identical to the prior covariance $\\Sigma_{\\mathrm{prior}}$, reflecting that a zero-current experiment provides no new information about the model parameters.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the diagonal of the Laplace-approximated posterior covariance matrix\n    for parameters of a simplified battery model for several test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            \"constants\": {\"gamma\": 0.2},\n            \"experiment\": {\"I\": 1.0, \"t\": [0.01, 0.1, 1.0, 3.0, 10.0], \"sigma\": 0.005},\n            \"theta0\": [1e-3, 0.1, 0.5],\n            \"prior_cov_diag\": [1e-4, 1e-2, 1e-1],\n        },\n        # Test Case 2\n        {\n            \"constants\": {\"gamma\": 0.2},\n            \"experiment\": {\"I\": 2.0, \"t\": [0.0, 0.1, 0.4, 1.6, 6.4], \"sigma\": 0.001},\n            \"theta0\": [5e-4, 0.05, 0.3],\n            \"prior_cov_diag\": [1e-5, 5e-3, 5e-2],\n        },\n        # Test Case 3\n        {\n            \"constants\": {\"gamma\": 0.2},\n            \"experiment\": {\"I\": 0.0, \"t\": [0.1, 1.0, 10.0], \"sigma\": 0.01},\n            \"theta0\": [1e-3, 0.1, 0.5],\n            \"prior_cov_diag\": [1e-4, 1e-2, 1e-1],\n        },\n        # Test Case 4\n        {\n            \"constants\": {\"gamma\": 0.2},\n            \"experiment\": {\"I\": 1.0, \"t\": [0.1, 0.2, 0.4], \"sigma\": 0.1},\n            \"theta0\": [1e-3, 0.1, 1.0],\n            \"prior_cov_diag\": [1e-3, 1e-1, 1.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract parameters for the current test case\n        gamma = case[\"constants\"][\"gamma\"]\n        I = case[\"experiment\"][\"I\"]\n        t = np.array(case[\"experiment\"][\"t\"], dtype=float)\n        sigma = case[\"experiment\"][\"sigma\"]\n        k_sei0, r_f0_0, d_s0 = case[\"theta0\"]\n        prior_cov_diag = np.array(case[\"prior_cov_diag\"], dtype=float)\n\n        # Number of observation times\n        N = len(t)\n\n        # 1. Construct the Jacobian matrix J(theta_0)\n        # J is an N x 3 matrix where J_ij = dV_i / d(theta_j)\n        # theta = [k_sei, r_f0, d_s]\n        \n        # Column 1: Derivatives with respect to k_sei\n        # dV/dk_sei = -I * sqrt(t_i)\n        col1 = -I * np.sqrt(t)\n\n        # Column 2: Derivatives with respect to r_f0\n        # dV/dr_f0 = -I\n        col2 = -I * np.ones(N)\n\n        # Column 3: Derivatives with respect to d_s\n        # dV/d_ds = I * gamma / d_s^2\n        col3 = (I * gamma / (d_s0**2)) * np.ones(N)\n\n        # Assemble the N x 3 Jacobian matrix\n        J = np.vstack([col1, col2, col3]).T\n\n        # 2. Compute the Fisher Information Matrix I(theta_0)\n        # I = (1/sigma^2) * J^T @ J\n        fisher_info = (1 / sigma**2) * (J.T @ J)\n\n        # 3. Construct the prior precision matrix (inverse of prior covariance)\n        # Since Sigma_prior is diagonal, its inverse is also diagonal\n        prior_precision = np.diag(1.0 / prior_cov_diag)\n\n        # 4. Compute the posterior precision matrix\n        # M = Sigma_prior^-1 + I(theta_0)\n        M = prior_precision + fisher_info\n\n        # 5. Invert M to get the posterior covariance matrix Sigma_post\n        try:\n            post_cov = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            # This case should not be reached with a proper prior\n            post_cov = np.full((3, 3), np.nan)\n\n        # 6. Extract the diagonal entries of Sigma_post\n        diag_post_cov = np.diagonal(post_cov).tolist()\n        results.append(diag_post_cov)\n\n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world experiments rarely produce perfectly complete datasets; often, tests conclude before every unit has failed. This exercise introduces the concept of right-censoring, a common occurrence in battery lifetime testing, where the true failure time is only known to be greater than the observation time. You will learn to construct the correct likelihood function that combines information from both failed and censored units, allowing for a robust Bayesian calibration of a time-to-failure model without discarding valuable data .",
            "id": "3895926",
            "problem": "An automated battery cycling platform is used to calibrate the stochastic time-to-failure of lithium plating under an aggressive fast-charge profile in a lithium-ion cell design workflow. Assume the time-to-failure $T$ for each cell under this profile is independent and follows an exponential distribution with constant hazard rate parameter $\\lambda$, where the probability density function is $f(t \\mid \\lambda)$ and the survival function is $S(t \\mid \\lambda)$. In survival analysis, right-censoring occurs when a test stops at a predetermined censoring time $t_{c}$ and the cell has not failed by that time; the contribution of a right-censored observation to the likelihood is the survival at $t_{c}$.\n\nThe platform runs $8$ parallel tests. Five cells fail at times $t = \\{120, 350, 600, 700, 760\\}$ hours, while three cells have not failed by the stop time $t_{c} = 800$ hours (right-censored at $t_{c}$). You aim to perform Bayesian calibration of $\\lambda$ using a Gamma prior with shape $\\alpha$ and rate $\\beta$, where the Gamma density is $p(\\lambda \\mid \\alpha, \\beta) \\propto \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$. Assume $\\alpha = 2$ and $\\beta = 500$ (hours as the unit in the rate parameter).\n\nStarting from the fundamental definitions of the exponential model—namely, $f(t \\mid \\lambda) = \\lambda \\exp(-\\lambda t)$ and $S(t \\mid \\lambda) = \\exp(-\\lambda t)$—and the right-censoring rule that the likelihood for mixed failed and censored observations is the product of densities for failures and the product of survivals for censored, derive the posterior distribution for $\\lambda$ and obtain the maximum a posteriori (MAP; Maximum A Posteriori) estimate of $\\lambda$. Express your final answer for the MAP estimate in $\\mathrm{h}^{-1}$, rounded to four significant figures. Your final answer must be a single real number.",
            "solution": "The objective is to find the Maximum A Posteriori (MAP) estimate for the hazard rate parameter $\\lambda$ of an exponential time-to-failure model, given a set of failure times and right-censored data. This will be achieved through Bayesian calibration.\n\nFirst, we must construct the likelihood function for the observed data. The data consists of $k=5$ failures at times $t_i$ and $m=3$ right-censored observations at a censoring time $t_c$. The total number of cells tested is $N=k+m=8$.\nThe failure times are given as the set $T_{fail} = \\{120, 350, 600, 700, 760\\}$ hours.\nThe censoring time is $t_c = 800$ hours.\n\nThe time-to-failure $T$ is modeled by an exponential distribution with probability density function (PDF) $f(t \\mid \\lambda) = \\lambda \\exp(-\\lambda t)$ and survival function $S(t \\mid \\lambda) = \\exp(-\\lambda t)$.\n\nFor each of the $k=5$ cells that failed, the contribution to the likelihood is the value of the PDF at its failure time $t_i$.\nFor each of the $m=3$ cells that did not fail by the test stop time $t_c$, the observation is right-censored. The contribution to the likelihood is the probability of survival beyond $t_c$, which is given by the survival function $S(t_c \\mid \\lambda)$.\n\nAssuming the failure events are independent, the total likelihood function $L(\\lambda \\mid \\text{data})$ is the product of the individual contributions:\n$$L(\\lambda \\mid \\text{data}) = \\left( \\prod_{i=1}^{k} f(t_i \\mid \\lambda) \\right) \\times \\left( S(t_c \\mid \\lambda) \\right)^m$$\nSubstituting the given expressions for the PDF and survival function:\n$$L(\\lambda \\mid \\text{data}) = \\left( \\prod_{i=1}^{5} \\lambda \\exp(-\\lambda t_i) \\right) \\times \\left( \\exp(-\\lambda t_c) \\right)^3$$\nThis can be simplified:\n$$L(\\lambda \\mid \\text{data}) = \\lambda^5 \\left( \\prod_{i=1}^{5} \\exp(-\\lambda t_i) \\right) \\times \\exp(-3 \\lambda t_c)$$\n$$L(\\lambda \\mid \\text{data}) = \\lambda^5 \\exp\\left(-\\lambda \\sum_{i=1}^{5} t_i\\right) \\exp(-3 \\lambda t_c)$$\n$$L(\\lambda \\mid \\text{data}) = \\lambda^5 \\exp\\left(-\\lambda \\left( \\sum_{i=1}^{5} t_i + 3 t_c \\right) \\right)$$\nLet's calculate the total time observed. The sum of the failure times is:\n$$\\sum_{i=1}^{5} t_i = 120 + 350 + 600 + 700 + 760 = 2530 \\text{ hours}$$\nThe total time contributed by the censored observations is:\n$$3 t_c = 3 \\times 800 = 2400 \\text{ hours}$$\nLet $T_{\\text{total}} = \\sum_{i=1}^{5} t_i + 3 t_c = 2530 + 2400 = 4930$ hours.\nThe likelihood function can now be written as:\n$$L(\\lambda \\mid \\text{data}) = \\lambda^5 \\exp(-4930 \\lambda)$$\n\nNext, we define the prior distribution for $\\lambda$. The problem specifies a Gamma prior, $p(\\lambda \\mid \\alpha, \\beta)$, with shape parameter $\\alpha=2$ and rate parameter $\\beta=500$ hours. The probability density of the prior is proportional to:\n$$p(\\lambda) \\propto \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) = \\lambda^{2-1} \\exp(-500 \\lambda) = \\lambda^{1} \\exp(-500 \\lambda)$$\n\nAccording to Bayes' theorem, the posterior distribution of $\\lambda$, $p(\\lambda \\mid \\text{data})$, is proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid \\text{data}) \\propto L(\\lambda \\mid \\text{data}) \\times p(\\lambda)$$\n$$p(\\lambda \\mid \\text{data}) \\propto \\left( \\lambda^5 \\exp(-4930 \\lambda) \\right) \\times \\left( \\lambda^{1} \\exp(-500 \\lambda) \\right)$$\nCombining the terms:\n$$p(\\lambda \\mid \\text{data}) \\propto \\lambda^{5+1} \\exp(-(4930+500)\\lambda)$$\n$$p(\\lambda \\mid \\text{data}) \\propto \\lambda^6 \\exp(-5430 \\lambda)$$\nThis is the kernel of a Gamma distribution. The posterior distribution is a Gamma distribution, $p(\\lambda \\mid \\text{data}) \\sim \\text{Gamma}(\\alpha', \\beta')$, where the density is proportional to $\\lambda^{\\alpha' - 1} \\exp(-\\beta' \\lambda)$.\n\nBy comparing the derived posterior form with the general Gamma kernel, we can identify the posterior parameters $\\alpha'$ and $\\beta'$:\n$$\\alpha' - 1 = 6 \\implies \\alpha' = 7$$\n$$\\beta' = 5430 \\text{ hours}$$\nSo, the posterior distribution is $\\text{Gamma}(7, 5430)$. This is expected, as the Gamma distribution is the conjugate prior for the rate parameter of an exponential distribution.\n\nThe MAP estimate of $\\lambda$, denoted $\\lambda_{\\text{MAP}}$, is the mode of the posterior distribution. For a Gamma distribution $\\text{Gamma}(\\alpha', \\beta')$ with $\\alpha' > 1$, the mode is given by the formula:\n$$\\text{Mode} = \\frac{\\alpha' - 1}{\\beta'}$$\nIn our case, $\\alpha' = 7$, which satisfies the condition $\\alpha' > 1$. Therefore, the MAP estimate is:\n$$\\lambda_{\\text{MAP}} = \\frac{7 - 1}{5430} = \\frac{6}{5430}$$\nNow, we compute the numerical value:\n$$\\lambda_{\\text{MAP}} = \\frac{6}{5430} \\approx 0.001104972375... \\text{ h}^{-1}$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\lambda_{\\text{MAP}} \\approx 0.001105 \\text{ h}^{-1}$$\n\nThe MAP estimate for the hazard rate parameter $\\lambda$ is $0.001105$ $\\mathrm{h}^{-1}$.",
            "answer": "$$\\boxed{0.001105}$$"
        },
        {
            "introduction": "Choosing an appropriate model is as critical as calibrating its parameters. Should one use a simple, physically-motivated parametric model or a more flexible, data-driven nonparametric one? This advanced practice explores the use of the Bayesian marginal likelihood (or model evidence) as a principled method for model selection, which inherently balances data fit against model complexity. By comparing a classic parametric SEI growth model against a more flexible Gaussian Process, you will learn to quantitatively answer the question: which model is better supported by the data? ",
            "id": "3895902",
            "problem": "You are tasked with implementing a principled comparison between a nonparametric Gaussian Process (GP) rate model and a parametric Solid Electrolyte Interphase (SEI) rate model by computing marginal likelihoods under a Bayesian framework. The physical system is a lithium-ion battery anode forming a Solid Electrolyte Interphase (SEI) layer, for which the rate of SEI growth is observed over time.\n\nStart from the following foundational base and definitions:\n- Bayes' theorem for model evidence states that, for observed data $\\mathbf{y}$ and model $\\mathcal{M}$ with latent parameters $\\boldsymbol{\\theta}$, the marginal likelihood (also called model evidence) is defined as $$p(\\mathbf{y} \\mid \\mathcal{M}) = \\int p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M}) \\, p(\\boldsymbol{\\theta} \\mid \\mathcal{M}) \\, d\\boldsymbol{\\theta}.$$\n- A Gaussian observation model assumes additive independent and identically distributed noise: for observation times $\\mathbf{t} = [t_1,\\dots,t_n]^T$, $$y_i = f(t_i) + \\varepsilon_i,$$ with $$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2),$$ where $\\sigma > 0$ is known and $f(\\cdot)$ is the latent rate function.\n- A Gaussian Process (GP) is defined as a collection of random variables, any finite subset of which is multivariate Gaussian. A zero-mean GP prior with covariance function $k(\\cdot,\\cdot)$ implies $$\\mathbf{f} = [f(t_1),\\dots,f(t_n)]^T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}),$$ where $\\mathbf{K}_{ij} = k(t_i, t_j)$.\n\nYou must not use shortcut formulas in your design. Derive the required quantities from these bases using valid linear algebraic manipulations and properties of Gaussian integrals.\n\nModel definitions to implement:\n1. Nonparametric Gaussian Process (GP) rate model:\n   - Use a zero-mean GP prior on $f(\\cdot)$ with the squared-exponential kernel $$k(t, t') = s_f^2 \\exp\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right),$$ where $s_f^2$ is the signal variance and $\\ell$ is the characteristic length scale.\n   - Observation model is $y_i = f(t_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\n2. Parametric SEI rate model:\n   - Use the parametric form $$f(t) = \\theta_1 t^{-1/2} + \\theta_2,$$ which captures a prototypical diffusion-limited SEI growth behavior plus a baseline offset.\n   - Place a zero-mean Gaussian prior on the parameter vector $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^T$ with covariance $$\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(s_1^2, s_2^2).$$\n   - Observation model is $y_i = f(t_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nYour program must compute, for each test case provided, the natural logarithm of the marginal likelihood for the GP model and for the parametric SEI model, and then compute the difference defined as $$\\Delta = \\log p(\\mathbf{y} \\mid \\text{GP}) - \\log p(\\mathbf{y} \\mid \\text{Parametric}).$$\n\nScientific realism and units:\n- The observation times $\\mathbf{t}$ must be in seconds ($\\mathrm{s}$).\n- The observed SEI rate values $\\mathbf{y}$ must be in moles per square meter per second ($\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$).\n- The noise standard deviation $\\sigma$ must be specified in $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n- The GP signal variance $s_f^2$ must be specified in $(\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1})^2$ and the length scale $\\ell$ in seconds ($\\mathrm{s}$).\n- The SEI parameter prior variances $s_1^2$ and $s_2^2$ must be specified in $(\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1/2})^2$ and $(\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1})^2$, respectively, so that units are consistent with $f(t)$ in $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n\nData generation for reproducibility:\n- For each test case, construct $\\mathbf{y}$ deterministically by $$y_i = \\theta_1 t_i^{-1/2} + \\theta_2 + \\varepsilon_i,$$ where $\\varepsilon_i$ is a fixed sequence provided per test case (not random).\n\nTest suite to implement:\n- Test Case 1 (balanced, moderately informative data):\n  - Times: $\\mathbf{t} = [300, 600, 1200, 2400, 4800]$ $\\mathrm{s}$.\n  - True parameters: $\\theta_1 = 1.5 \\times 10^{-7}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1/2}$, $\\theta_2 = 2.0 \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - Noise standard deviation: $\\sigma = 1.0 \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - Fixed noise sequence: $\\boldsymbol{\\varepsilon} = [1.0, -1.0, 0.5, -0.2, 0.3] \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - SEI prior variances: $s_1^2 = (1.0 \\times 10^{-7})^2$, $s_2^2 = (5.0 \\times 10^{-8})^2$.\n  - GP hyperparameters: $s_f^2 = (1.0 \\times 10^{-7})^2$, $\\ell = 1000$ $\\mathrm{s}$.\n\n- Test Case 2 (small sample, highly flexible GP):\n  - Times: $\\mathbf{t} = [1000, 2000, 3000]$ $\\mathrm{s}$.\n  - True parameters: $\\theta_1 = 1.0 \\times 10^{-7}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1/2}$, $\\theta_2 = 1.0 \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - Noise standard deviation: $\\sigma = 5.0 \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - Fixed noise sequence: $\\boldsymbol{\\varepsilon} = [2.0, -1.0, 0.5] \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - SEI prior variances: $s_1^2 = (5.0 \\times 10^{-7})^2$, $s_2^2 = (1.0 \\times 10^{-7})^2$.\n  - GP hyperparameters: $s_f^2 = (1.0 \\times 10^{-7})^2$, $\\ell = 10$ $\\mathrm{s}$.\n\n- Test Case 3 (dense sampling, nearly constant GP prior):\n  - Times: $\\mathbf{t} = [500, 1000, 1500, 2000, 2500, 3000]$ $\\mathrm{s}$.\n  - True parameters: $\\theta_1 = 2.0 \\times 10^{-7}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1/2}$, $\\theta_2 = 0.0$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - Noise standard deviation: $\\sigma = 5.0 \\times 10^{-9}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - Fixed noise sequence: $\\boldsymbol{\\varepsilon} = [-0.2, 0.1, -0.1, 0.0, 0.2, -0.3] \\times 10^{-8}$ $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - SEI prior variances: $s_1^2 = (1.0 \\times 10^{-7})^2$, $s_2^2 = (1.0 \\times 10^{-8})^2$.\n  - GP hyperparameters: $s_f^2 = (1.0 \\times 10^{-7})^2$, $\\ell = 10000$ $\\mathrm{s}$.\n\nAlgorithmic requirements:\n- Use the above foundations to derive closed-form expressions for the marginal likelihoods that are valid for Gaussian priors and Gaussian likelihoods.\n- Implement numerically stable computations for logarithms of determinants and quadratic forms using matrix factorizations that are appropriate for positive-definite covariance matrices.\n- The output must be computed in natural logarithm units and be deterministic.\n\nFinal output format:\n- Your program should produce a single line of output containing the three $\\Delta$ values for the test cases as a comma-separated list enclosed in square brackets (e.g., \"[delta1,delta2,delta3]\").\n- Each element must be a floating-point number.\n\nInterpretation requirement:\n- Although your program outputs only numeric values, your internal solution must reason about the flexibility versus identifiability trade-off: the GP model is more flexible due to its infinite-dimensional prior and can adapt to complex patterns, whereas the parametric SEI model is more identifiable with fewer parameters. The marginal likelihood inherently balances data fit and model complexity. Your solution must connect these concepts to the computed evidence values.\n\nAnswer all quantities in the specified physical units when applicable, and ensure all computations adhere to those units.",
            "solution": "The user has requested a principled comparison of two models for SEI growth rate in a lithium-ion battery anode: a nonparametric Gaussian Process (GP) model and a parametric physical model. The comparison is to be performed by computing the Bayesian marginal likelihood for each model. This document first validates the problem, then derives the necessary formulae from foundational principles, and finally provides an implementation strategy.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Bayes' Theorem for Marginal Likelihood:** $p(\\mathbf{y} \\mid \\mathcal{M}) = \\int p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M}) \\, p(\\boldsymbol{\\theta} \\mid \\mathcal{M}) \\, d\\boldsymbol{\\theta}$.\n- **Observation Model:** Gaussian with i.i.d. noise, $y_i = f(t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and $\\sigma > 0$ is known.\n- **Gaussian Process (GP) Prior:** Zero-mean, $\\mathbf{f} = [f(t_1),\\dots,f(t_n)]^T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K})$, where $\\mathbf{K}_{ij} = k(t_i, t_j)$.\n\n- **Model 1: Nonparametric GP Rate Model**\n  - Prior: Zero-mean GP on $f(\\cdot)$.\n  - Kernel: Squared-exponential, $k(t, t') = s_f^2 \\exp\\left(-\\frac{(t - t')^2}{2 \\ell^2}\\right)$.\n  - Hyperparameters: Signal variance $s_f^2$, characteristic length scale $\\ell$.\n\n- **Model 2: Parametric SEI Rate Model**\n  - Functional Form: $f(t) = \\theta_1 t^{-1/2} + \\theta_2$.\n  - Prior: Zero-mean Gaussian on $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^T$ with covariance $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(s_1^2, s_2^2)$.\n  - Hyperparameters: Prior variances $s_1^2$, $s_2^2$.\n\n- **Comparison Metric:** $\\Delta = \\log p(\\mathbf{y} \\mid \\text{GP}) - \\log p(\\mathbf{y} \\mid \\text{Parametric})$.\n\n- **Data Generation:** $y_i = \\theta_1 t_i^{-1/2} + \\theta_2 + \\varepsilon_i$, with $\\varepsilon_i$ provided as a fixed sequence.\n\n- **Units:**\n  - $\\mathbf{t}$ in seconds ($\\mathrm{s}$).\n  - $\\mathbf{y}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - $\\sigma$ in $\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1}$.\n  - $s_f^2$ in $(\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1})^2$.\n  - $\\ell$ in $\\mathrm{s}$.\n  - $s_1^2$ in $(\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1/2})^2$.\n  - $s_2^2$ in $(\\mathrm{mol}\\,\\mathrm{m}^{-2}\\,\\mathrm{s}^{-1})^2$.\n\n- **Test Cases:** Three distinct test cases are fully specified with all necessary parameters and data generation values.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the established validation criteria.\n1.  **Scientifically Grounded:** The problem uses standard Bayesian statistical methods (marginal likelihood) to compare two well-established modeling approaches (parametric physical models and nonparametric Gaussian Processes). The chosen parametric form $f(t) \\propto t^{-1/2}$ is characteristic of diffusion-limited growth processes, which is a physically sound model for SEI layer formation. All specified units are dimensionally consistent. The problem is scientifically sound.\n2.  **Well-Posed:** The models are fully specified (priors, likelihoods, and all hyperparameters are given). For a Gaussian prior and a Gaussian likelihood, the marginal likelihood integral is analytically tractable, yielding a unique, stable, and meaningful result. The problem is well-posed.\n3.  **Objective:** The problem statement is written in precise, mathematical language, free from any subjective or ambiguous terms.\n4.  **Flaw Checklist:** A detailed check confirms the problem exhibits none of the specified flaws. It is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, or ill-posed. The a priori specification of parameters makes the problem a direct computation rather than an inference task, which is a valid setup for a direct model comparison exercise.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n### Derivation of the Marginal Likelihood\n\nThe core of the problem is to compute the marginal likelihood, $p(\\mathbf{y} \\mid \\mathcal{M})$, which quantifies the probability of observing the data $\\mathbf{y}$ given a model $\\mathcal{M}$. This value, also known as model evidence, naturally embodies Occam's razor by balancing model fit with model complexity. A model that is too complex will have its prior probability spread thinly over a large space of possible datasets, leading to a lower evidence value even if it fits the observed data well.\n\nBoth models presented are linear-Gaussian, meaning the observed data $\\mathbf{y}$ can be expressed as a linear transformation of Gaussian-distributed latent variables, with additive Gaussian noise. This structure ensures that the marginal distribution of $\\mathbf{y}$ is also Gaussian, and its parameters can be derived analytically.\n\n**1. Marginal Likelihood for the Nonparametric GP Model**\n\nThe GP model is defined hierarchically:\n1.  A prior is placed on the latent function values $\\mathbf{f} = [f(t_1), \\dots, f(t_n)]^T$ at the observation times $\\mathbf{t}$. Since the GP prior is zero-mean, this is:\n    $$p(\\mathbf{f} \\mid \\text{GP}) = \\mathcal{N}(\\mathbf{f} \\mid \\mathbf{0}, \\mathbf{K})$$\n    where $\\mathbf{K}$ is the $n \\times n$ covariance matrix with entries $\\mathbf{K}_{ij} = k(t_i, t_j) = s_f^2 \\exp\\left(-\\frac{(t_i - t_j)^2}{2 \\ell^2}\\right)$.\n\n2.  The observation model specifies the likelihood of the data $\\mathbf{y}$ given the latent function values $\\mathbf{f}$:\n    $$p(\\mathbf{y} \\mid \\mathbf{f}) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{f}, \\sigma^2 \\mathbf{I})$$\n    where $\\mathbf{I}$ is the $n \\times n$ identity matrix and $\\sigma^2$ is the noise variance.\n\nThe marginal likelihood is obtained by integrating out the latent function values $\\mathbf{f}$:\n$$p(\\mathbf{y} \\mid \\text{GP}) = \\int p(\\mathbf{y} \\mid \\mathbf{f}) \\, p(\\mathbf{f} \\mid \\text{GP}) \\, d\\mathbf{f}$$\nThis is a convolution of two Gaussian distributions. The resulting distribution for $\\mathbf{y}$ is also Gaussian. Its mean is $E[\\mathbf{y}] = E[\\mathbf{f} + \\boldsymbol{\\varepsilon}] = E[\\mathbf{f}] + E[\\boldsymbol{\\varepsilon}] = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}$. Its covariance is $\\text{Cov}(\\mathbf{y}) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}) = \\text{Cov}(\\mathbf{f}) + \\text{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbf{K} + \\sigma^2\\mathbf{I}$, because $\\mathbf{f}$ and $\\boldsymbol{\\varepsilon}$ are independent.\n\nThus, the marginal likelihood for the GP model is given by the multivariate Gaussian probability density function:\n$$p(\\mathbf{y} \\mid \\text{GP}) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{0}, \\mathbf{C}_y^{\\text{GP}})$$\nwhere the data covariance matrix is $\\mathbf{C}_y^{\\text{GP}} = \\mathbf{K} + \\sigma^2\\mathbf{I}$.\n\n**2. Marginal Likelihood for the Parametric SEI Model**\n\nThe parametric model is also defined hierarchically:\n1.  A prior is placed on the model parameters $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^T$:\n    $$p(\\boldsymbol{\\theta} \\mid \\text{Parametric}) = \\mathcal{N}(\\boldsymbol{\\theta} \\mid \\mathbf{0}, \\boldsymbol{\\Sigma}_0)$$\n    where $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(s_1^2, s_2^2)$.\n\n2.  The latent function is a linear function of the parameters: $f(t_i; \\boldsymbol{\\theta}) = \\theta_1 t_i^{-1/2} + \\theta_2$. This can be written in matrix form as $\\mathbf{f} = \\mathbf{\\Phi}\\boldsymbol{\\theta}$, where $\\mathbf{\\Phi}$ is the $n \\times 2$ design matrix with the $i$-th row being $[t_i^{-1/2}, 1]$.\n\n3.  The observation model is again $p(\\mathbf{y} \\mid \\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{\\Phi}\\boldsymbol{\\theta}, \\sigma^2\\mathbf{I})$.\n\nThe marginal likelihood is obtained by integrating out the parameters $\\boldsymbol{\\theta}$:\n$$p(\\mathbf{y} \\mid \\text{Parametric}) = \\int p(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\, p(\\boldsymbol{\\theta} \\mid \\text{Parametric}) \\, d\\boldsymbol{\\theta}$$\nThe vector $\\mathbf{y}$ is a sum of two independent Gaussian random variables: $\\mathbf{\\Phi}\\boldsymbol{\\theta}$ and $\\boldsymbol{\\varepsilon}$. The distribution of $\\mathbf{\\Phi}\\boldsymbol{\\theta}$ is Gaussian with mean $E[\\mathbf{\\Phi}\\boldsymbol{\\theta}] = \\mathbf{\\Phi}E[\\boldsymbol{\\theta}] = \\mathbf{0}$ and covariance $\\text{Cov}(\\mathbf{\\Phi}\\boldsymbol{\\theta}) = \\mathbf{\\Phi}\\text{Cov}(\\boldsymbol{\\theta})\\mathbf{\\Phi}^T = \\mathbf{\\Phi}\\boldsymbol{\\Sigma}_0\\mathbf{\\Phi}^T$.\nThe distribution of $\\mathbf{y}$ is therefore Gaussian with mean $E[\\mathbf{y}] = \\mathbf{0}$ and covariance $\\text{Cov}(\\mathbf{y}) = \\text{Cov}(\\mathbf{\\Phi}\\boldsymbol{\\theta}) + \\text{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbf{\\Phi}\\boldsymbol{\\Sigma}_0\\mathbf{\\Phi}^T + \\sigma^2\\mathbf{I}$.\n\nThus, the marginal likelihood for the parametric model is:\n$$p(\\mathbf{y} \\mid \\text{Parametric}) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{0}, \\mathbf{C}_y^{\\text{Parametric}})$$\nwhere the data covariance matrix is $\\mathbf{C}_y^{\\text{Parametric}} = \\mathbf{\\Phi}\\boldsymbol{\\Sigma}_0\\mathbf{\\Phi}^T + \\sigma^2\\mathbf{I}$.\n\n**3. Numerically Stable Computation of the Log-Marginal Likelihood**\n\nFor a general $n$-dimensional multivariate Gaussian distribution $\\mathcal{N}(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\mathbf{C})$, the log-probability density is:\n$$\\log p(\\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\mathbf{C}) - \\frac{1}{2}(\\mathbf{y}-\\boldsymbol{\\mu})^T\\mathbf{C}^{-1}(\\mathbf{y}-\\boldsymbol{\\mu})$$\nIn our case, the mean $\\boldsymbol{\\mu}$ is $\\mathbf{0}$ for both models. The primary computational challenge lies in calculating the log-determinant, $\\log\\det(\\mathbf{C}_y)$, and the quadratic form, $\\mathbf{y}^T\\mathbf{C}_y^{-1}\\mathbf{y}$, for an $n \\times n$ positive-definite covariance matrix $\\mathbf{C}_y$.\nDirect computation of the determinant and inverse is numerically unstable and computationally expensive. A superior method uses the Cholesky decomposition, $\\mathbf{C}_y = \\mathbf{L}\\mathbf{L}^T$, where $\\mathbf{L}$ is a lower-triangular matrix.\n\n-   **Log-Determinant:** $\\log\\det(\\mathbf{C}_y) = \\log\\det(\\mathbf{L}\\mathbf{L}^T) = \\log(\\det(\\mathbf{L})^2) = 2\\log\\det(\\mathbf{L})$. The determinant of a triangular matrix is the product of its diagonal elements, so $\\det(\\mathbf{L}) = \\prod_i L_{ii}$. Therefore,\n    $$\\log\\det(\\mathbf{C}_y) = 2 \\sum_{i=1}^n \\log(L_{ii})$$\n\n-   **Quadratic Form:** The quadratic term can be rewritten as $\\mathbf{y}^T (\\mathbf{L}\\mathbf{L}^T)^{-1} \\mathbf{y} = \\mathbf{y}^T (\\mathbf{L}^T)^{-1} \\mathbf{L}^{-1} \\mathbf{y} = (\\mathbf{L}^{-1}\\mathbf{y})^T (\\mathbf{L}^{-1}\\mathbf{y})$. Let $\\mathbf{z} = \\mathbf{L}^{-1}\\mathbf{y}$. This vector $\\mathbf{z}$ can be found by solving the triangular system $\\mathbf{Lz} = \\mathbf{y}$ using efficient forward substitution. The quadratic form is then simply the squared Euclidean norm of $\\mathbf{z}$:\n    $$\\mathbf{y}^T\\mathbf{C}_y^{-1}\\mathbf{y} = \\mathbf{z}^T\\mathbf{z} = ||\\mathbf{z}||_2^2$$\n\nCombining these, the final expression for the log-marginal likelihood is:\n$$\\log p(\\mathbf{y} \\mid \\mathcal{M}) = -\\frac{n}{2}\\log(2\\pi) - \\sum_{i=1}^n \\log(L_{ii}) - \\frac{1}{2}\\mathbf{z}^T\\mathbf{z}$$\nThis formulation will be used for both the GP and parametric models, with their respective data covariance matrices $\\mathbf{C}_y$.\n\n**4. Interpretation of Test Case Results**\nThe quantity $\\Delta = \\log p(\\mathbf{y} \\mid \\text{GP}) - \\log p(\\mathbf{y} \\mid \\text{Parametric})$ is the log-Bayes factor. A positive $\\Delta$ indicates that the data provides more evidence for the GP model, while a negative $\\Delta$ favors the parametric model.\n- **Test Case 1** uses a moderate number of data points and a GP length scale ($\\ell = 1000\\,\\mathrm{s}$) that is on the order of the time span of the data. This sets up a balanced comparison.\n- **Test Case 2** has few data points ($n=3$) and a very short GP length scale ($\\ell=10\\,\\mathrm{s}$). A short length scale corresponds to a very flexible, \"wiggly\" GP prior. With limited data, such a complex model is likely to be penalized by the marginal likelihood for overfitting, potentially favoring the simpler parametric model.\n- **Test Case 3** has denser data and a very long GP length scale ($\\ell=10000\\,\\mathrm{s}$). A long length scale makes the GP prior very smooth, approaching a constant function. Since the true data is generated with $\\theta_2=0$, the underlying function is a simple decaying curve. The very smooth GP might be a poor structural match compared to the parametric $t^{-1/2}$ form, but its simplicity (low effective complexity) might still result in a competitive evidence score.\n\nThe algorithm will compute these values, allowing for a quantitative model comparison guided by these principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian model comparison problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (balanced, moderately informative data)\n        {\n            \"t\": np.array([300, 600, 1200, 2400, 4800]), # s\n            \"true_theta1\": 1.5e-7, # mol m^-2 s^-1/2\n            \"true_theta2\": 2.0e-8, # mol m^-2 s^-1\n            \"sigma\": 1.0e-8, # mol m^-2 s^-1\n            \"eps\": np.array([1.0, -1.0, 0.5, -0.2, 0.3]) * 1e-8, # mol m^-2 s^-1\n            \"s1_sq\": (1.0e-7)**2, # (mol m^-2 s^-1/2)^2\n            \"s2_sq\": (5.0e-8)**2, # (mol m^-2 s^-1)^2\n            \"sf_sq\": (1.0e-7)**2, # (mol m^-2 s^-1)^2\n            \"l\": 1000.0, # s\n        },\n        # Test Case 2 (small sample, highly flexible GP)\n        {\n            \"t\": np.array([1000, 2000, 3000]), # s\n            \"true_theta1\": 1.0e-7, # mol m^-2 s^-1/2\n            \"true_theta2\": 1.0e-8, # mol m^-2 s^-1\n            \"sigma\": 5.0e-8, # mol m^-2 s^-1\n            \"eps\": np.array([2.0, -1.0, 0.5]) * 1e-8, # mol m^-2 s^-1\n            \"s1_sq\": (5.0e-7)**2, # (mol m^-2 s^-1/2)^2\n            \"s2_sq\": (1.0e-7)**2, # (mol m^-2 s^-1)^2\n            \"sf_sq\": (1.0e-7)**2, # (mol m^-2 s^-1)^2\n            \"l\": 10.0, # s\n        },\n        # Test Case 3 (dense sampling, nearly constant GP prior)\n        {\n            \"t\": np.array([500, 1000, 1500, 2000, 2500, 3000]), # s\n            \"true_theta1\": 2.0e-7, # mol m^-2 s^-1/2\n            \"true_theta2\": 0.0, # mol m^-2 s^-1\n            \"sigma\": 5.0e-9, # mol m^-2 s^-1\n            \"eps\": np.array([-0.2, 0.1, -0.1, 0.0, 0.2, -0.3]) * 1e-8, # mol m^-2 s^-1\n            \"s1_sq\": (1.0e-7)**2, # (mol m^-2 s^-1/2)^2\n            \"s2_sq\": (1.0e-8)**2, # (mol m^-2 s^-1)^2\n            \"sf_sq\": (1.0e-7)**2, # (mol m^-2 s^-1)^2\n            \"l\": 10000.0, # s\n        }\n    ]\n\n    def compute_log_marginal_likelihood(y, C_y):\n        \"\"\"\n        Computes the log marginal likelihood for a zero-mean Gaussian model.\n\n        Args:\n            y (np.ndarray): The observation vector.\n            C_y (np.ndarray): The data covariance matrix.\n\n        Returns:\n            float: The natural logarithm of the marginal likelihood.\n        \"\"\"\n        n = len(y)\n        # Cholesky decomposition for stable computation\n        # L L^T = C_y\n        try:\n            L = cholesky(C_y, lower=True)\n        except np.linalg.LinAlgError:\n            return -np.inf # Matrix not positive definite\n\n        # Log-determinant term: log(det(C_y)) = 2 * sum(log(diag(L)))\n        log_det_C = 2 * np.sum(np.log(np.diag(L)))\n\n        # Quadratic form: y^T C_y^-1 y\n        # Solve L z = y for z\n        z = solve_triangular(L, y, lower=True)\n        # Quadratic form is z^T z\n        quadratic_form = np.dot(z, z)\n\n        # Assemble the log marginal likelihood\n        log_ml = -0.5 * (n * np.log(2 * np.pi) + log_det_C + quadratic_form)\n        return log_ml\n\n    def log_ml_gp(t, y, sf_sq, l, sigma):\n        \"\"\"Computes log marginal likelihood for the GP model.\"\"\"\n        n = len(t)\n        sigma_sq = sigma**2\n        \n        # Build kernel matrix K\n        t_col = t.reshape(-1, 1)\n        sq_dist = (t_col - t_col.T)**2\n        K = sf_sq * np.exp(-0.5 * sq_dist / l**2)\n\n        # Data covariance C_y = K + sigma^2 * I\n        C_y_gp = K + sigma_sq * np.identity(n)\n        \n        return compute_log_marginal_likelihood(y, C_y_gp)\n\n    def log_ml_parametric(t, y, s1_sq, s2_sq, sigma):\n        \"\"\"Computes log marginal likelihood for the parametric SEI model.\"\"\"\n        n = len(t)\n        sigma_sq = sigma**2\n\n        # Build design matrix Phi\n        Phi = np.vstack((t**-0.5, np.ones(n))).T\n\n        # Prior covariance Sigma0\n        Sigma0 = np.diag([s1_sq, s2_sq])\n\n        # Data covariance C_y = Phi Sigma0 Phi^T + sigma^2 I\n        C_y_param = Phi @ Sigma0 @ Phi.T + sigma_sq * np.identity(n)\n        \n        return compute_log_marginal_likelihood(y, C_y_param)\n\n    results = []\n    for case in test_cases:\n        # 1. Generate data y\n        t = case[\"t\"]\n        f_true = case[\"true_theta1\"] * t**-0.5 + case[\"true_theta2\"]\n        y = f_true + case[\"eps\"]\n\n        # 2. Compute log marginal likelihood for GP model\n        log_ml_gp_val = log_ml_gp(t, y, case[\"sf_sq\"], case[\"l\"], case[\"sigma\"])\n        \n        # 3. Compute log marginal likelihood for parametric model\n        log_ml_param_val = log_ml_parametric(t, y, case[\"s1_sq\"], case[\"s2_sq\"], case[\"sigma\"])\n\n        # 4. Compute the difference Delta\n        delta = log_ml_gp_val - log_ml_param_val\n        results.append(delta)\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}