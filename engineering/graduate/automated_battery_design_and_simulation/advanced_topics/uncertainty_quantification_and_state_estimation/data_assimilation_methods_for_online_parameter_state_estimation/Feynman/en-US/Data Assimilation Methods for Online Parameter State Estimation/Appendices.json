{
    "hands_on_practices": [
        {
            "introduction": "The Kalman filter is the cornerstone of modern state estimation for linear systems. To truly master its application, it is essential to understand its core mechanism: the two-step predict-update cycle. This exercise guides you through a single iteration of the filter for a linearized battery model, providing a concrete, hands-on calculation of how a prior state estimate is projected forward in time (prediction) and then refined using a new measurement (update) to yield a posterior estimate. Completing this practice will solidify your understanding of how the filter optimally fuses model predictions with noisy sensor data. ",
            "id": "3903722",
            "problem": "A lithium-ion cell is modeled near a fixed operating point by a discrete-time linearized Thevenin resistor-capacitor (RC) equivalent circuit. The state vector is $x_k = [x_{1,k}\\; x_{2,k}]^{\\top}$, where $x_{1,k}$ is the state of charge (SOC) fraction and $x_{2,k}$ is the RC overpotential in volts. The input $u_{k-1}$ is the applied current (positive for discharge), and the measured output $y_k$ is the terminal voltage deviation from the operating-point voltage after precompensating the ohmic drop. The stochastic linear-Gaussian state-space model is\n$$x_k = A x_{k-1} + B u_{k-1} + w_{k-1}, \\quad w_{k-1} \\sim \\mathcal{N}(0,Q),$$\n$$y_k = H x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0,R),$$\nwith independent process and measurement noises.\n\nAssume the system matrices and noise covariances are numerically specified as\n$$A = \\begin{bmatrix} 1  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix}, \\quad B = \\begin{bmatrix} -\\tfrac{1}{10} \\\\ \\tfrac{1}{5} \\end{bmatrix}, \\quad H = \\begin{bmatrix} \\tfrac{3}{10}  -1 \\end{bmatrix},$$\n$$Q = \\begin{bmatrix} \\tfrac{1}{100}  0 \\\\ 0  \\tfrac{1}{25} \\end{bmatrix}, \\quad R = \\tfrac{9}{100}.$$\n\nAt time $k-1$, the posterior state estimate and covariance are\n$$x_{k-1}^{+} = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{5} \\end{bmatrix}, \\quad P_{k-1}^{+} = \\begin{bmatrix} \\tfrac{1}{25}  0 \\\\ 0  \\tfrac{9}{100} \\end{bmatrix}.$$\nAt the next step, the applied current is $u_{k-1} = 1$, and the measured terminal-voltage deviation is $y_k = -\\tfrac{1}{5}$.\n\nStarting from the linear-Gaussian state transition and observation models and standard probabilistic conditioning of Gaussian random variables, perform one complete Kalman filter iteration (prediction and update) to obtain the posterior state $x_k^{+}$ and covariance $P_k^{+}$ at time $k$.\n\nAnswer specification:\n- Provide exact values in closed form; do not round.\n- Report the final answer as a single row matrix in the order $[x_{1,k}^{+},\\, x_{2,k}^{+},\\, P_{11,k}^{+},\\, P_{12,k}^{+},\\, P_{21,k}^{+},\\, P_{22,k}^{+}]$.\n- Express $x_{1,k}^{+}$ as an SOC fraction and $x_{2,k}^{+}$ in volts; the covariance entries are in the corresponding squared units. In the final boxed answer, do not include units.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in standard state-space modeling and estimation theory for electrochemical systems, specifically using a linearized equivalent circuit model for a lithium-ion cell, which is a common practice. The problem is well-posed, providing all necessary matrices, covariances, initial conditions, and measurements to perform a single, unique iteration of the Kalman filter. The provided numerical values are dimensionally consistent and do not represent any physical or mathematical contradictions. The task is a direct application of the standard Kalman filter algorithm, a cornerstone of data assimilation and online state estimation.\n\nThe Kalman filter algorithm consists of two sequential steps: prediction (time update) and correction (measurement update). We will perform one complete iteration.\n\nThe posterior state estimate and its covariance at time $k-1$ are given as:\n$$x_{k-1}^{+} = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{5} \\end{bmatrix}, \\quad P_{k-1}^{+} = \\begin{bmatrix} \\tfrac{1}{25}  0 \\\\ 0  \\tfrac{9}{100} \\end{bmatrix}$$\nThe system matrices, input, and measurement are:\n$$A = \\begin{bmatrix} 1  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix}, \\quad B = \\begin{bmatrix} -\\tfrac{1}{10} \\\\ \\tfrac{1}{5} \\end{bmatrix}, \\quad H = \\begin{bmatrix} \\tfrac{3}{10}  -1 \\end{bmatrix}$$\n$$Q = \\begin{bmatrix} \\tfrac{1}{100}  0 \\\\ 0  \\tfrac{1}{25} \\end{bmatrix}, \\quad R = \\tfrac{9}{100}$$\n$$u_{k-1} = 1, \\quad y_k = -\\tfrac{1}{5}$$\n\n**1. Prediction Step**\n\nThe prediction step projects the state and covariance estimates forward in time from step $k-1$ to $k$.\n\nFirst, we compute the predicted (a priori) state estimate $x_k^{-}$:\n$$x_k^{-} = A x_{k-1}^{+} + B u_{k-1}$$\n$$x_k^{-} = \\begin{bmatrix} 1  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix} \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{5} \\end{bmatrix} + \\begin{bmatrix} -\\tfrac{1}{10} \\\\ \\tfrac{1}{5} \\end{bmatrix} (1) = \\begin{bmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{10} \\end{bmatrix} + \\begin{bmatrix} -\\tfrac{1}{10} \\\\ \\tfrac{2}{10} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{4}{10} \\\\ \\tfrac{3}{10} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{2}{5} \\\\ \\tfrac{3}{10} \\end{bmatrix}$$\n\nNext, we compute the predicted (a priori) error covariance $P_k^{-}$:\n$$P_k^{-} = A P_{k-1}^{+} A^{\\top} + Q$$\nSince $A$ is a diagonal matrix, $A^{\\top} = A$.\n$$P_k^{-} = \\begin{bmatrix} 1  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix} \\begin{bmatrix} \\tfrac{1}{25}  0 \\\\ 0  \\tfrac{9}{100} \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix} + \\begin{bmatrix} \\tfrac{1}{100}  0 \\\\ 0  \\tfrac{1}{25} \\end{bmatrix}$$\n$$P_k^{-} = \\begin{bmatrix} \\tfrac{1}{25}  0 \\\\ 0  \\tfrac{9}{200} \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  \\tfrac{1}{2} \\end{bmatrix} + \\begin{bmatrix} \\tfrac{1}{100}  0 \\\\ 0  \\tfrac{1}{25} \\end{bmatrix}$$\n$$P_k^{-} = \\begin{bmatrix} \\tfrac{1}{25}  0 \\\\ 0  \\tfrac{9}{400} \\end{bmatrix} + \\begin{bmatrix} \\tfrac{4}{100}  0 \\\\ 0  \\tfrac{16}{400} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{5}{100}  0 \\\\ 0  \\tfrac{25}{400} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{1}{20}  0 \\\\ 0  \\tfrac{1}{16} \\end{bmatrix}$$\n\n**2. Update Step**\n\nThe update step corrects the predicted estimates using the new measurement $y_k$.\n\nFirst, we compute the Kalman gain $K_k$. This requires the innovation covariance, $S_k = H P_k^{-} H^{\\top} + R$.\n$$P_k^{-} H^{\\top} = \\begin{bmatrix} \\tfrac{1}{20}  0 \\\\ 0  \\tfrac{1}{16} \\end{bmatrix} \\begin{bmatrix} \\tfrac{3}{10} \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} \\tfrac{3}{200} \\\\ -\\tfrac{1}{16} \\end{bmatrix}$$\n$$H P_k^{-} H^{\\top} = \\begin{bmatrix} \\tfrac{3}{10}  -1 \\end{bmatrix} \\begin{bmatrix} \\tfrac{3}{200} \\\\ -\\tfrac{1}{16} \\end{bmatrix} = \\tfrac{9}{2000} + \\tfrac{1}{16} = \\tfrac{9}{2000} + \\tfrac{125}{2000} = \\tfrac{134}{2000} = \\tfrac{67}{1000}$$\n$$S_k = H P_k^{-} H^{\\top} + R = \\tfrac{67}{1000} + \\tfrac{9}{100} = \\tfrac{67}{1000} + \\tfrac{90}{1000} = \\tfrac{157}{1000}$$\nThe Kalman gain $K_k$ is then:\n$$K_k = P_k^{-} H^{\\top} S_k^{-1} = \\begin{bmatrix} \\tfrac{3}{200} \\\\ -\\tfrac{1}{16} \\end{bmatrix} \\left(\\tfrac{1000}{157}\\right) = \\begin{bmatrix} \\tfrac{3 \\times 5}{157} \\\\ -\\tfrac{1000/16}{157} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{15}{157} \\\\ -\\tfrac{125/2}{157} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{15}{157} \\\\ -\\tfrac{125}{314} \\end{bmatrix}$$\n\nNext, we update the state estimate to obtain the posterior (a posteriori) estimate $x_k^{+}$. This requires the measurement residual (innovation), $y_k - H x_k^{-}$.\n$$H x_k^{-} = \\begin{bmatrix} \\tfrac{3}{10}  -1 \\end{bmatrix} \\begin{bmatrix} \\tfrac{2}{5} \\\\ \\tfrac{3}{10} \\end{bmatrix} = \\tfrac{6}{50} - \\tfrac{3}{10} = \\tfrac{6}{50} - \\tfrac{15}{50} = -\\tfrac{9}{50}$$\nThe innovation is $y_k - H x_k^{-} = -\\tfrac{1}{5} - (-\\tfrac{9}{50}) = -\\tfrac{10}{50} + \\tfrac{9}{50} = -\\tfrac{1}{50}$.\n$$x_k^{+} = x_k^{-} + K_k (y_k - H x_k^{-})$$\n$$x_k^{+} = \\begin{bmatrix} \\tfrac{2}{5} \\\\ \\tfrac{3}{10} \\end{bmatrix} + \\begin{bmatrix} \\tfrac{15}{157} \\\\ -\\tfrac{125}{314} \\end{bmatrix} \\left(-\\tfrac{1}{50}\\right) = \\begin{bmatrix} \\tfrac{2}{5} - \\tfrac{15}{157 \\times 50} \\\\ \\tfrac{3}{10} + \\tfrac{125}{314 \\times 50} \\end{bmatrix}$$\nThe components of $x_k^{+}$ are:\n$$x_{1,k}^{+} = \\tfrac{2}{5} - \\tfrac{3}{1570} = \\tfrac{2 \\times 314 - 3}{1570} = \\tfrac{628-3}{1570} = \\tfrac{625}{1570} = \\tfrac{125}{314}$$\n$$x_{2,k}^{+} = \\tfrac{3}{10} + \\tfrac{125}{15700} = \\tfrac{3}{10} + \\tfrac{5}{628} = \\tfrac{3 \\times 314 + 5 \\times 5}{3140} = \\tfrac{942+25}{3140} = \\tfrac{967}{3140}$$\nSo, $x_k^{+} = \\begin{bmatrix} \\tfrac{125}{314} \\\\ \\tfrac{967}{3140} \\end{bmatrix}$.\n\nFinally, we update the error covariance to obtain the posterior covariance $P_k^{+}$.\n$$P_k^{+} = (I - K_k H) P_k^{-}$$\n$$I - K_k H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\begin{bmatrix} \\tfrac{15}{157} \\\\ -\\tfrac{125}{314} \\end{bmatrix} \\begin{bmatrix} \\tfrac{3}{10}  -1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\begin{bmatrix} \\tfrac{45}{1570}  -\\tfrac{15}{157} \\\\ -\\tfrac{375}{3140}  \\tfrac{125}{314} \\end{bmatrix}$$\n$$I - K_k H = \\begin{bmatrix} 1 - \\tfrac{9}{314}  \\tfrac{15}{157} \\\\ \\tfrac{375}{3140}  1 - \\tfrac{125}{314} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{305}{314}  \\tfrac{30}{314} \\\\ \\tfrac{75}{628}  \\tfrac{189}{314} \\end{bmatrix}$$\nNow, multiplying by the diagonal $P_k^{-}$:\n$$P_k^{+} = \\begin{bmatrix} \\tfrac{305}{314}  \\tfrac{30}{314} \\\\ \\tfrac{75}{628}  \\tfrac{189}{314} \\end{bmatrix} \\begin{bmatrix} \\tfrac{1}{20}  0 \\\\ 0  \\tfrac{1}{16} \\end{bmatrix} = \\begin{bmatrix} \\tfrac{305}{314 \\times 20}  \\tfrac{30}{314 \\times 16} \\\\ \\tfrac{75}{628 \\times 20}  \\tfrac{189}{314 \\times 16} \\end{bmatrix}$$\nThe components of $P_k^{+}$ are:\n$$P_{11,k}^{+} = \\tfrac{305}{6280} = \\tfrac{61}{1256}$$\n$$P_{12,k}^{+} = \\tfrac{30}{5024} = \\tfrac{15}{2512}$$\n$$P_{21,k}^{+} = \\tfrac{75}{12560} = \\tfrac{15}{2512}$$\n$$P_{22,k}^{+} = \\tfrac{189}{5024}$$\nAs expected, $P_k^{+}$ is symmetric, i.e., $P_{12,k}^{+} = P_{21,k}^{+}$.\n\nThe final posterior state and covariance are:\n$$x_k^{+} = \\begin{bmatrix} \\tfrac{125}{314} \\\\ \\tfrac{967}{3140} \\end{bmatrix}, \\quad P_k^{+} = \\begin{bmatrix} \\tfrac{61}{1256}  \\tfrac{15}{2512} \\\\ \\tfrac{15}{2512}  \\tfrac{189}{5024} \\end{bmatrix}$$\nThe requested output is the row matrix $[x_{1,k}^{+},\\, x_{2,k}^{+},\\, P_{11,k}^{+},\\, P_{12,k}^{+},\\, P_{21,k}^{+},\\, P_{22,k}^{+}]$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{125}{314}  \\frac{967}{3140}  \\frac{61}{1256}  \\frac{15}{2512}  \\frac{15}{2512}  \\frac{189}{5024}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the standard Kalman filter is powerful, its assumption of linearity limits its use, as most real-world battery models are nonlinear. The Extended Kalman Filter (EKF) overcomes this by repeatedly linearizing the model around the current state estimate. This practice focuses on the most critical part of this process: deriving the measurement Jacobian matrix, $H_k$, for a common battery model where terminal voltage is a nonlinear function of the state of charge. By working through this problem, you will learn how to adapt the Kalman filtering framework to perform joint state and parameter estimation on nonlinear systems. ",
            "id": "3903746",
            "problem": "Consider a single-cell lithium-ion battery operated at a constant temperature, modeled for online state and parameter estimation in an Extended Kalman Filter (EKF). The measurement equation for the terminal voltage is given by the relationship between the open-circuit voltage and the state of charge, with an ohmic drop due to internal resistance:\n$$\nV_k = \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k + w_k,\n$$\nwhere $V_k$ is the measured terminal voltage at discrete time $k$, $\\mathrm{SOC}_k$ is the state of charge, $R_k$ is the ohmic resistance, $I_k$ is the applied current (positive for discharge), and $w_k$ is the measurement noise. The EKF measurement function is $h(x_k,u_k) = \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k$, with the state vector $x_k = \\begin{pmatrix} \\mathrm{SOC}_k \\\\ R_k \\end{pmatrix}$ and the input $u_k = I_k$.\n\nStarting from the definition of the Extended Kalman Filter linearization based on the first-order Taylor expansion of the measurement function, derive the measurement Jacobian matrix $H_k = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_k^{-}}$ using the partial derivative $\\frac{\\partial \\mathrm{OCV}}{\\partial \\mathrm{SOC}}$ and the functional dependence on $R_k$ and $I_k$. Then, for the specific open-circuit voltage function\n$$\n\\mathrm{OCV}(\\mathrm{SOC}) = 3.7 + 0.1\\,\\mathrm{SOC} + 0.05\\,\\mathrm{SOC}^{2} - 0.03\\,\\mathrm{SOC}^{3},\n$$\nevaluate $H_k$ at the predicted state $\\hat{x}_k^{-} = \\begin{pmatrix} \\hat{\\mathrm{SOC}}_k^{-} \\\\ \\hat{R}_k^{-} \\end{pmatrix} = \\begin{pmatrix} 0.60 \\\\ 0.050 \\end{pmatrix}$ with applied current $I_k = 2.000$, assuming standard International System of Units (SI) for numerical evaluation. Express the first entry of $H_k$ in volts per unit state of charge and the second entry in volts per ohm, and report the final numerical row vector $H_k$ rounded to four significant figures. Provide only the numerical values in the final row vector, without units.",
            "solution": "The problem requires the derivation and evaluation of the measurement Jacobian matrix, $H_k$, for an Extended Kalman Filter (EKF) applied to a battery model.\n\nFirst, we must validate the problem statement.\nThe givens are:\n- The measurement equation: $V_k = \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k + w_k$.\n- The EKF measurement function: $h(x_k,u_k) = \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k$.\n- The state vector: $x_k = \\begin{pmatrix} \\mathrm{SOC}_k \\\\ R_k \\end{pmatrix}$.\n- The input: $u_k = I_k$.\n- The definition of the Jacobian: $H_k = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_k^{-}}$.\n- A specific functional form for the open-circuit voltage: $\\mathrm{OCV}(\\mathrm{SOC}) = 3.7 + 0.1\\,\\mathrm{SOC} + 0.05\\,\\mathrm{SOC}^{2} - 0.03\\,\\mathrm{SOC}^{3}$.\n- The numerical values for evaluation: the predicted state $\\hat{x}_k^{-} = \\begin{pmatrix} \\hat{\\mathrm{SOC}}_k^{-} \\\\ \\hat{R}_k^{-} \\end{pmatrix} = \\begin{pmatrix} 0.60 \\\\ 0.050 \\end{pmatrix}$ and the applied current $I_k = 2.000$.\n\nThe problem is scientifically grounded, using standard models (equivalent circuit model for a battery) and methods (EKF) from control theory and electrical engineering. The problem is well-posed, providing all necessary functions and data for a unique solution. The language is objective and precise. The numerical values are physically plausible for a lithium-ion cell. Therefore, the problem is deemed valid.\n\nWe can now proceed with the solution. The measurement Jacobian matrix $H_k$ is defined as the partial derivative of the scalar measurement function $h$ with respect to the state vector $x_k$. Since the state vector $x_k$ has two components, $\\mathrm{SOC}_k$ and $R_k$, the Jacobian $H_k$ will be a $1 \\times 2$ row vector:\n$$\nH_k = \\left.\\begin{pmatrix} \\frac{\\partial h}{\\partial \\mathrm{SOC}_k}  \\frac{\\partial h}{\\partial R_k} \\end{pmatrix}\\right|_{x_k = \\hat{x}_k^{-}}\n$$\nThe measurement function is $h(x_k, u_k) = \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k$. Let's compute each partial derivative.\n\nThe first component of $H_k$ is the partial derivative of $h$ with respect to $\\mathrm{SOC}_k$:\n$$\n\\frac{\\partial h}{\\partial \\mathrm{SOC}_k} = \\frac{\\partial}{\\partial \\mathrm{SOC}_k} \\left( \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k \\right)\n$$\nSince the term $- R_k I_k$ does not depend on $\\mathrm{SOC}_k$, its derivative with respect to $\\mathrm{SOC}_k$ is zero. Thus,\n$$\n\\frac{\\partial h}{\\partial \\mathrm{SOC}_k} = \\frac{d\\mathrm{OCV}}{d\\mathrm{SOC}_k}\n$$\nThe second component of $H_k$ is the partial derivative of $h$ with respect to $R_k$:\n$$\n\\frac{\\partial h}{\\partial R_k} = \\frac{\\partial}{\\partial R_k} \\left( \\mathrm{OCV}(\\mathrm{SOC}_k) - R_k I_k \\right)\n$$\nThe term $\\mathrm{OCV}(\\mathrm{SOC}_k)$ does not depend on $R_k$, so its derivative is zero. The derivative of the second term is:\n$$\n\\frac{\\partial h}{\\partial R_k} = -I_k\n$$\nCombining these results, the symbolic expression for the Jacobian matrix is:\n$$\nH_k = \\begin{pmatrix} \\frac{d\\mathrm{OCV}}{d\\mathrm{SOC}_k}  -I_k \\end{pmatrix}\n$$\nThis matrix must be evaluated at the a priori state estimate, $x_k = \\hat{x}_k^{-} = \\begin{pmatrix} \\hat{\\mathrm{SOC}}_k^{-} \\\\ \\hat{R}_k^{-} \\end{pmatrix}$.\n\nNow, we use the specific functional form for $\\mathrm{OCV}(\\mathrm{SOC})$ provided in the problem statement:\n$$\n\\mathrm{OCV}(\\mathrm{SOC}) = 3.7 + 0.1\\,\\mathrm{SOC} + 0.05\\,\\mathrm{SOC}^{2} - 0.03\\,\\mathrm{SOC}^{3}\n$$\nWe compute its derivative with respect to $\\mathrm{SOC}$:\n$$\n\\frac{d\\mathrm{OCV}}{d\\mathrm{SOC}} = \\frac{d}{d\\mathrm{SOC}} \\left( 3.7 + 0.1\\,\\mathrm{SOC} + 0.05\\,\\mathrm{SOC}^{2} - 0.03\\,\\mathrm{SOC}^{3} \\right)\n$$\n$$\n\\frac{d\\mathrm{OCV}}{d\\mathrm{SOC}} = 0.1 + 2(0.05)\\,\\mathrm{SOC} - 3(0.03)\\,\\mathrm{SOC}^{2}\n$$\n$$\n\\frac{d\\mathrm{OCV}}{d\\mathrm{SOC}} = 0.1 + 0.1\\,\\mathrm{SOC} - 0.09\\,\\mathrm{SOC}^{2}\n$$\nWe need to evaluate this derivative at $\\mathrm{SOC}_k = \\hat{\\mathrm{SOC}}_k^{-} = 0.60$:\n$$\n\\left.\\frac{d\\mathrm{OCV}}{d\\mathrm{SOC}_k}\\right|_{\\hat{\\mathrm{SOC}}_k^{-}=0.60} = 0.1 + 0.1(0.60) - 0.09(0.60)^{2}\n$$\n$$\n= 0.1 + 0.06 - 0.09(0.36)\n$$\n$$\n= 0.16 - 0.0324\n$$\n$$\n= 0.1276\n$$\nThe first entry of $H_k$ is $0.1276$. The unit is Volts per unit SOC.\n\nThe second entry of $H_k$ is $-I_k$. We are given $I_k = 2.000$ Amperes.\n$$\n-I_k = -2.000\n$$\nThe second entry of $H_k$ is $-2.000$. The unit is Volts per Ohm, which is equivalent to Amperes.\n\nAssembling the numerical Jacobian matrix $H_k$:\n$$\nH_k = \\begin{pmatrix} 0.1276  -2.000 \\end{pmatrix}\n$$\nThe problem requires the final answer to be rounded to four significant figures.\nThe first value, $0.1276$, has four significant figures.\nThe second value, $-2.000$, has four significant figures.\nTherefore, no further rounding is needed. The final numerical row vector for $H_k$ is $\\begin{pmatrix} 0.1276  -2.000 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1276  -2.000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A filter's performance is critically dependent on how accurately its noise covariance matrices, $Q$ and $R$, model the true system and measurement uncertainties. While these are often tuned by hand, a more rigorous approach is to learn them directly from data. This advanced exercise introduces the Expectation-Maximization (EM) algorithm, a powerful method for estimating these noise parameters by treating the true state trajectory as missing data. You will implement the full E-step (using a Kalman filter and RTS smoother) and M-step to iteratively find the $Q$ and $R$ that best explain your observed measurements, providing you with a key skill for building robust, high-performance estimators. ",
            "id": "3903741",
            "problem": "Consider online estimation of the process noise covariance and measurement noise covariance in a discrete-time linear stochastic state-space model used in automated battery simulation. The model is a linearization of a two-state Equivalent Circuit Model (ECM) of a lithium-ion cell around a fixed operating point. The discrete-time dynamics and measurement equations are\n$$\n\\mathbf{x}_{k+1} = \\mathbf{A}\\,\\mathbf{x}_k + \\mathbf{B}\\,u_k + \\mathbf{w}_k,\\quad \\mathbf{w}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}),\n$$\n$$\ny_k = \\mathbf{H}\\,\\mathbf{x}_k + \\mathbf{D}\\,u_k + v_k,\\quad v_k \\sim \\mathcal{N}(0, R),\n$$\nwhere $\\mathbf{x}_k \\in \\mathbb{R}^2$ denotes the state vector containing state of charge and a first-order resistor-capacitor (RC) pair voltage, $u_k \\in \\mathbb{R}$ denotes the applied current (treated as a known input), $y_k \\in \\mathbb{R}$ denotes the terminal voltage measurement, $\\mathbf{w}_k$ is the process noise with covariance $\\mathbf{Q} \\in \\mathbb{R}^{2 \\times 2}$, and $v_k$ is the measurement noise with variance $R \\in \\mathbb{R}$. The matrices $\\mathbf{A} \\in \\mathbb{R}^{2 \\times 2}$, $\\mathbf{B} \\in \\mathbb{R}^{2 \\times 1}$, $\\mathbf{H} \\in \\mathbb{R}^{1 \\times 2}$, and $\\mathbf{D} \\in \\mathbb{R}^{1 \\times 1}$ are known and fixed for each test case. The initial state $\\mathbf{x}_0$ and its covariance $\\mathbf{P}_0$ are given.\n\nThe goal is to compute updated $\\mathbf{Q}$ and $R$ using smoothed residuals obtained from a Rauch–Tung–Striebel (RTS) smoother, iterating until convergence. Convergence is assessed by the relative Frobenius norm change in $\\mathbf{Q}$ and $R$ and by checking monotonic non-decrease of the innovations log-likelihood over the final iterations. Specifically, after each update:\n- Compute the relative changes\n$$\n\\delta_Q = \\frac{\\|\\mathbf{Q}_{\\text{new}} - \\mathbf{Q}_{\\text{old}}\\|_F}{\\max(\\|\\mathbf{Q}_{\\text{old}}\\|_F, \\epsilon)},\\quad\n\\delta_R = \\frac{|R_{\\text{new}} - R_{\\text{old}}|}{\\max(|R_{\\text{old}}|, \\epsilon)},\n$$\nwith $\\epsilon = 10^{-12}$.\n- Define a tolerance $\\tau = 0.05$ and a maximum iteration count $N_{\\text{max}} = 60$.\n- Define convergence as $\\delta_Q \\le \\tau$ and $\\delta_R \\le \\tau$ and a non-decreasing innovations log-likelihood over the final $5$ iterations (allow exact equality within numerical tolerance).\n\nYou must implement the Expectation–Maximization (EM) algorithm using smoothed residuals computed by the RTS smoother. In the Expectation step, run a Kalman Filter (KF) followed by the RTS smoother to obtain smoothed means, covariances, and lag-one covariances. In the Maximization step, update $\\mathbf{Q}$ and $R$ using the expected sufficient statistics of residuals:\n- Define the one-step state residuals $\\mathbf{r}_k^{(x)} = \\mathbf{x}_{k+1} - \\mathbf{A}\\,\\mathbf{x}_k - \\mathbf{B}\\,u_k$ and the measurement residuals $r_k^{(y)} = y_k - \\mathbf{H}\\,\\mathbf{x}_k - \\mathbf{D}\\,u_k$. Use the smoothed expectations to form\n$$\n\\mathbf{Q}_{\\text{new}} = \\frac{1}{T-1} \\sum_{k=0}^{T-2} \\mathbb{E}\\left[\\mathbf{r}_k^{(x)}\\mathbf{r}_k^{(x)\\top}\\,\\big|\\, y_{0:T-1}\\right],\n$$\n$$\nR_{\\text{new}} = \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{E}\\left[r_k^{(y)}r_k^{(y)}\\,\\big|\\, y_{0:T-1}\\right].\n$$\nThe expectations must be computed from smoothed means and covariances, including lag-one covariances $\\mathrm{Cov}(\\mathbf{x}_k,\\mathbf{x}_{k+1}\\,|\\,y_{0:T-1})$.\n\nFor model assessment within data assimilation, compute the innovations log-likelihood using the KF at each iteration,\n$$\n\\mathcal{L} = -\\frac{1}{2} \\sum_{k=0}^{T-1} \\left( \\log\\left((2\\pi)^m \\det \\mathbf{S}_k\\right) + \\mathbf{e}_k^\\top \\mathbf{S}_k^{-1} \\mathbf{e}_k \\right),\n$$\nwhere $\\mathbf{e}_k = y_k - \\mathbf{H}\\,\\hat{\\mathbf{x}}_{k|k-1} - \\mathbf{D}\\,u_k$, $\\hat{\\mathbf{x}}_{k|k-1}$ is the one-step predicted mean, $\\mathbf{S}_k = \\mathbf{H}\\,\\mathbf{P}_{k|k-1}\\,\\mathbf{H}^\\top + R$, and $m = 1$ is the measurement dimension.\n\nImplement the above and apply it to the following test suite. For each test case, simulate a dataset using the given true covariances and a fixed pseudo-random seed. Then, starting from a common initial guess for $\\mathbf{Q}$ and $R$, run EM updates until convergence or reaching the maximum iteration count. For each case, output a boolean indicating whether convergence was achieved according to the criterion above.\n\nCommon initial conditions and initial guesses for all cases:\n- Initial state mean $\\mathbf{x}_0 = \\begin{bmatrix} 0.5 \\\\ 0.0 \\end{bmatrix}$.\n- Initial state covariance $\\mathbf{P}_0 = \\begin{bmatrix} 10^{-4}  0 \\\\ 0  10^{-3} \\end{bmatrix}$.\n- Initial guesses $\\mathbf{Q}^{(0)} = \\begin{bmatrix} 10^{-5}  0 \\\\ 0  10^{-3} \\end{bmatrix}$ and $R^{(0)} = 10^{-2}$.\n\nUse the input current sequence\n$$\nu_k = 0.5 + \\sin(0.03\\,k) + 0.1\\,\\sin(0.005\\,k^2),\\quad k = 0,1,\\dots,T-1,\n$$\nwhich is deterministic.\n\nTest case $1$ (happy path, moderate noises):\n- Length $T = 200$.\n- Matrices\n$$\n\\mathbf{A} = \\begin{bmatrix} 1  0 \\\\ 0  0.98 \\end{bmatrix},\\quad\n\\mathbf{B} = \\begin{bmatrix} -0.001 \\\\ 0.02 \\end{bmatrix},\\quad\n\\mathbf{H} = \\begin{bmatrix} 0.1  1.0 \\end{bmatrix},\\quad\n\\mathbf{D} = \\begin{bmatrix} -0.05 \\end{bmatrix}.\n$$\n- True covariances\n$$\n\\mathbf{Q}_{\\text{true}} = \\begin{bmatrix} 10^{-6}  0 \\\\ 0  10^{-4} \\end{bmatrix},\\quad R_{\\text{true}} = 10^{-3}.\n$$\n- Random seed $7$.\n\nTest case $2$ (boundary with very small process noise):\n- Length $T = 150$.\n- Same $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{H}$, $\\mathbf{D}$ as in test case $1$.\n- True covariances\n$$\n\\mathbf{Q}_{\\text{true}} = \\begin{bmatrix} 10^{-9}  0 \\\\ 0  10^{-6} \\end{bmatrix},\\quad R_{\\text{true}} = 5\\times 10^{-4}.\n$$\n- Random seed $11$.\n\nTest case $3$ (edge with high measurement noise):\n- Length $T = 250$.\n- Same $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{H}$, $\\mathbf{D}$ as in test case $1$.\n- True covariances\n$$\n\\mathbf{Q}_{\\text{true}} = \\begin{bmatrix} 5\\times 10^{-6}  0 \\\\ 0  2\\times 10^{-4} \\end{bmatrix},\\quad R_{\\text{true}} = 5\\times 10^{-2}.\n$$\n- Random seed $19$.\n\nSimulation procedure for each case:\n- Generate $\\mathbf{x}_{k+1}$ recursively from the true dynamics using the given $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{Q}_{\\text{true}}$, and input $u_k$, starting from $\\mathbf{x}_0$ and with $\\mathbf{w}_k$ sampled from $\\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_{\\text{true}})$ using the specified seed.\n- Generate $y_k$ from the measurement equation using $\\mathbf{H}$, $\\mathbf{D}$, and $R_{\\text{true}}$, with $v_k$ sampled from $\\mathcal{N}(0, R_{\\text{true}})$ using the specified seed.\n\nAlgorithmic requirements:\n- Use the Kalman Filter (KF) followed by the Rauch–Tung–Striebel (RTS) smoother to compute smoothed means $\\mathbb{E}[\\mathbf{x}_k \\mid y_{0:T-1}]$, smoothed covariances $\\mathrm{Cov}(\\mathbf{x}_k \\mid y_{0:T-1})$, and lag-one covariances $\\mathrm{Cov}(\\mathbf{x}_k,\\mathbf{x}_{k+1} \\mid y_{0:T-1})$ for the EM updates.\n- Ensure the updated $\\mathbf{Q}$ and $R$ remain symmetric positive definite by symmetrization and eigenvalue clipping if necessary.\n- Compute the innovations log-likelihood at each iteration using the KF with the current $\\mathbf{Q}$ and $R$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$), where each result is a boolean indicating whether the convergence criterion was satisfied for the corresponding test case.\n\nNo physical units are required for this problem. Angles are not used. Percentages are not used.",
            "solution": "The problem of estimating process and measurement noise covariances in a linear state-space model is a well-defined and standard problem in system identification and control theory. The proposed methodology, using an Expectation-Maximization (EM) algorithm with a Rauch–Tung–Striebel (RTS) smoother, is a canonical approach. All provided parameters, equations, and conditions are scientifically sound, consistent, and computationally feasible. The problem is valid and can be solved as stated.\n\nThe solution involves implementing the EM algorithm to iteratively refine estimates of the process noise covariance $\\mathbf{Q}$ and the measurement noise variance $R$. Each iteration consists of two steps: an Expectation (E) step and a Maximization (M) step.\n\n**1. The State-Space Model and Data Simulation**\n\nFirst, for each test case, we generate a synthetic dataset of length $T$ based on the provided true model. The discrete-time linear state-space model is given by:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{A}\\,\\mathbf{x}_k + \\mathbf{B}\\,u_k + \\mathbf{w}_k, \\quad \\mathbf{w}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_{\\text{true}})\n$$\n$$\ny_k = \\mathbf{H}\\,\\mathbf{x}_k + \\mathbf{D}\\,u_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, R_{\\text{true}})\n$$\nStarting with the given initial state $\\mathbf{x}_0$, we recursively generate the state sequence $\\{\\mathbf{x}_k\\}_{k=1}^{T-1}$ and the measurement sequence $\\{y_k\\}_{k=0}^{T-1}$. The process noise $\\mathbf{w}_k$ and measurement noise $v_k$ are drawn from their respective zero-mean Gaussian distributions with the true covariances $\\mathbf{Q}_{\\text{true}}$ and $R_{\\text{true}}$ using a fixed pseudo-random seed for reproducibility. The input sequence $u_k$ is deterministic. This simulated dataset $\\{y_k, u_k\\}_{k=0}^{T-1}$ is the \"observed data\" for the estimation problem.\n\n**2. The Expectation-Maximization (EM) Algorithm**\n\nThe EM algorithm aims to find the parameters ($\\mathbf{Q}$, $R$) that maximize the log-likelihood of the observed data. It does so by iteratively maximizing the expectation of the complete-data log-likelihood, where the hidden states $\\{\\mathbf{x}_k\\}$ are treated as missing data.\n\n**2.1. E-Step: State Smoothing**\n\nIn the E-step, given the current parameter estimates $\\mathbf{Q}^{(i)}$ and $R^{(i)}$, we compute the conditional expectation of sufficient statistics of the complete data, given the observed measurements $y_{0:T-1}$. For a linear-Gaussian model, these statistics are functions of the smoothed state means and covariances. We compute these using a two-pass procedure:\n\n**a) Kalman Filter (Forward Pass):** The Kalman Filter (KF) processes the data from $k=0$ to $T-1$. It recursively computes the filtered state estimate $\\hat{\\mathbf{x}}_{k|k} = \\mathbb{E}[\\mathbf{x}_k | y_{0:k}]$ and its covariance $\\mathbf{P}_{k|k} = \\mathrm{Cov}(\\mathbf{x}_k | y_{0:k})$. The key steps are:\n-   **Initialization:** $\\hat{\\mathbf{x}}_{0|-1} = \\mathbf{x}_0$, $\\mathbf{P}_{0|-1} = \\mathbf{P}_0$.\n-   **Prediction:** For $k=0, \\dots, T-1$:\n    $$\n    \\hat{\\mathbf{x}}_{k|k-1} = \\mathbf{A}\\,\\hat{\\mathbf{x}}_{k-1|k-1} + \\mathbf{B}\\,u_{k-1} \\quad (\\text{for } k0)\n    $$\n    $$\n    \\mathbf{P}_{k|k-1} = \\mathbf{A}\\,\\mathbf{P}_{k-1|k-1}\\,\\mathbf{A}^\\top + \\mathbf{Q}^{(i)} \\quad (\\text{for } k0)\n    $$\n-   **Update:**\n    $$\n    \\mathbf{e}_k = y_k - (\\mathbf{H}\\,\\hat{\\mathbf{x}}_{k|k-1} + \\mathbf{D}\\,u_k)\n    $$\n    $$\n    S_k = \\mathbf{H}\\,\\mathbf{P}_{k|k-1}\\,\\mathbf{H}^\\top + R^{(i)}\n    $$\n    $$\n    \\mathbf{K}_k = \\mathbf{P}_{k|k-1}\\,\\mathbf{H}^\\top S_k^{-1}\n    $$\n    $$\n    \\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + \\mathbf{K}_k \\mathbf{e}_k\n    $$\n    $$\n    \\mathbf{P}_{k|k} = (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H})\\mathbf{P}_{k|k-1}\n    $$\nThe KF also yields the innovations log-likelihood, used for monitoring convergence:\n$$\n\\mathcal{L}(\\mathbf{Q}^{(i)}, R^{(i)}) = -\\frac{1}{2} \\sum_{k=0}^{T-1} \\left( \\log(2\\pi S_k) + \\frac{\\mathbf{e}_k^2}{S_k} \\right)\n$$\nAll predicted and filtered quantities ($\\hat{\\mathbf{x}}_{k|k-1}, \\mathbf{P}_{k|k-1}, \\hat{\\mathbf{x}}_{k|k}, \\mathbf{P}_{k|k}$) for $k=0, \\dots, T-1$ are stored for the backward pass.\n\n**b) Rauch–Tung–Striebel (RTS) Smoother (Backward Pass):** The RTS smoother uses the KF results to compute the smoothed estimates, conditioned on the entire data sequence. It runs backward from $k=T-1$ to $0$. Let $\\hat{\\mathbf{x}}_{k|T-1} \\equiv \\hat{\\mathbf{x}}_k^s$ and $\\mathbf{P}_{k|T-1} \\equiv \\mathbf{P}_k^s$.\n-   **Initialization:** $\\hat{\\mathbf{x}}_{T-1}^s = \\hat{\\mathbf{x}}_{T-1|T-1}$, $\\mathbf{P}_{T-1}^s = \\mathbf{P}_{T-1|T-1}$.\n-   **Backward Recursion:** For $k=T-2, \\dots, 0$:\n    $$\n    \\mathbf{J}_k = \\mathbf{P}_{k|k}\\,\\mathbf{A}^\\top \\mathbf{P}_{k+1|k}^{-1}\n    $$\n    $$\n    \\hat{\\mathbf{x}}_k^s = \\hat{\\mathbf{x}}_{k|k} + \\mathbf{J}_k(\\hat{\\mathbf{x}}_{k+1}^s - \\hat{\\mathbf{x}}_{k+1|k})\n    $$\n    $$\n    \\mathbf{P}_k^s = \\mathbf{P}_{k|k} + \\mathbf{J}_k(\\mathbf{P}_{k+1}^s - \\mathbf{P}_{k+1|k})\\mathbf{J}_k^\\top\n    $$\nThe smoother also computes the smoothed lag-one cross-covariance, $\\mathbf{P}_{k+1,k|T-1} \\equiv \\mathbf{P}_{k+1,k}^s = \\mathrm{Cov}(\\mathbf{x}_{k+1}, \\mathbf{x}_k | y_{0:T-1})$, which is essential for the $\\mathbf{Q}$ update:\n$$\n\\mathbf{P}_{k+1,k}^s = \\mathbf{P}_{k+1}^s \\mathbf{J}_k^\\top\n$$\n\n**2.2. M-Step: Parameter Update**\n\nIn the M-step, we update the parameters $\\mathbf{Q}$ and $R$ to maximize the expected complete-data log-likelihood, using the smoothed statistics from the E-step. The update equations are:\n$$\n\\mathbf{Q}^{(i+1)} = \\frac{1}{T-1} \\sum_{k=0}^{T-2} \\mathbb{E}\\left[(\\mathbf{x}_{k+1} - \\mathbf{A}\\mathbf{x}_k - \\mathbf{B}u_k)(\\mathbf{x}_{k+1} - \\mathbf{A}\\mathbf{x}_k - \\mathbf{B}u_k)^\\top \\mid y_{0:T-1}\\right]\n$$\n$$\nR^{(i+1)} = \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{E}\\left[(y_k - \\mathbf{H}\\mathbf{x}_k - \\mathbf{D}u_k)^2 \\mid y_{0:T-1}\\right]\n$$\nExpanding these expectations using the properties of smoothed estimates yields:\n$$\n\\mathbf{Q}^{(i+1)} = \\frac{1}{T-1} \\sum_{k=0}^{T-2} \\left[ (\\hat{\\mathbf{x}}_{k+1}^s - \\mathbf{A}\\hat{\\mathbf{x}}_k^s - \\mathbf{B}u_k)(\\hat{\\mathbf{x}}_{k+1}^s - \\mathbf{A}\\hat{\\mathbf{x}}_k^s - \\mathbf{B}u_k)^\\top + \\boldsymbol{\\Gamma}_k \\right]\n$$\nwhere $\\boldsymbol{\\Gamma}_k = \\mathbf{P}_{k+1}^s + \\mathbf{A}\\mathbf{P}_k^s\\mathbf{A}^\\top - \\mathbf{A}(\\mathbf{P}_{k+1,k}^s)^\\top - \\mathbf{P}_{k+1,k}^s\\mathbf{A}^\\top$.\nAnd for the measurement variance:\n$$\nR^{(i+1)} = \\frac{1}{T} \\sum_{k=0}^{T-1} \\left[ (y_k - \\mathbf{H}\\hat{\\mathbf{x}}_k^s - \\mathbf{D}u_k)^2 + \\mathbf{H}\\mathbf{P}_k^s\\mathbf{H}^\\top \\right]\n$$\n\nTo ensure numerical stability and physical meaning, the updated covariances must be symmetric positive semi-definite. $\\mathbf{Q}^{(i+1)}$ is symmetrized via $\\mathbf{Q} \\leftarrow (\\mathbf{Q} + \\mathbf{Q}^\\top)/2$. Then, its eigenvalues are computed, and any eigenvalue less than a small positive floor (e.g., $10^{-15}$) is replaced by that floor, before reconstructing the matrix. $R^{(i+1)}$ is simply floored at the same small value.\n\n**3. Iteration and Convergence**\n\nThe E and M steps are repeated until convergence criteria are met or a maximum number of iterations, $N_{\\text{max}} = 60$, is reached. Convergence at iteration $i$ is defined by three conditions:\n1.  Relative change in $\\mathbf{Q}$ is below a tolerance $\\tau = 0.05$: $\\frac{\\|\\mathbf{Q}^{(i)} - \\mathbf{Q}^{(i-1)}\\|_F}{\\max(\\|\\mathbf{Q}^{(i-1)}\\|_F, \\epsilon)} \\le \\tau$.\n2.  Relative change in $R$ is below the tolerance: $\\frac{|R^{(i)} - R^{(i-1)}|}{\\max(|R^{(i-1)}|, \\epsilon)} \\le \\tau$.\n3.  The innovations log-likelihood is monotonically non-decreasing over the last five iterations (from $i-4$ to $i$, requiring $i \\ge 4$): $\\mathcal{L}^{(j)} \\ge \\mathcal{L}^{(j-1)}$ for $j=i-3, \\dots, i$, allowing for a small numerical tolerance.\n\nThe procedure is applied to each test case, starting from the specified initial guesses $\\mathbf{Q}^{(0)}$ and $R^{(0)}$, and the boolean result indicating whether convergence was achieved is recorded.",
            "answer": "```python\nimport numpy as np\nfrom numpy.linalg import inv, norm, eigh\n\ndef solve():\n    \"\"\"\n    Main solver function to run EM algorithm on all test cases and print results.\n    \"\"\"\n    \n    # Common parameters for all test cases\n    x0 = np.array([0.5, 0.0])\n    P0 = np.array([[1e-4, 0], [0, 1e-3]])\n    Q_init = np.array([[1e-5, 0], [0, 1e-3]])\n    R_init = 1e-2\n    \n    # Common system matrices for all test cases\n    A = np.array([[1.0, 0.0], [0.0, 0.98]])\n    B = np.array([[-0.001], [0.02]])\n    H = np.array([[0.1, 1.0]])\n    D = np.array([[-0.05]])\n    \n    test_cases = [\n        # Case 1\n        {\n            \"T\": 200, \"A\": A, \"B\": B, \"H\": H, \"D\": D, \"x0\": x0, \"P0\": P0,\n            \"Q_true\": np.array([[1e-6, 0], [0, 1e-4]]),\n            \"R_true\": 1e-3, \"seed\": 7,\n            \"Q_init\": Q_init, \"R_init\": R_init\n        },\n        # Case 2\n        {\n            \"T\": 150, \"A\": A, \"B\": B, \"H\": H, \"D\": D, \"x0\": x0, \"P0\": P0,\n            \"Q_true\": np.array([[1e-9, 0], [0, 1e-6]]),\n            \"R_true\": 5e-4, \"seed\": 11,\n            \"Q_init\": Q_init, \"R_init\": R_init\n        },\n        # Case 3\n        {\n            \"T\": 250, \"A\": A, \"B\": B, \"H\": H, \"D\": D, \"x0\": x0, \"P0\": P0,\n            \"Q_true\": np.array([[5e-6, 0], [0, 2e-4]]),\n            \"R_true\": 5e-2, \"seed\": 19,\n            \"Q_init\": Q_init, \"R_init\": R_init\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        converged = run_em_for_case(case)\n        results.append(converged)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_data(params):\n    \"\"\"Generates synthetic data from the state-space model.\"\"\"\n    T, A, B, H, D = params[\"T\"], params[\"A\"], params[\"B\"], params[\"H\"], params[\"D\"]\n    x0, Q_true, R_true, seed = params[\"x0\"], params[\"Q_true\"], params[\"R_true\"], params[\"seed\"]\n    \n    np.random.seed(seed)\n    \n    n_states = x0.shape[0]\n    x = np.zeros((T, n_states))\n    y = np.zeros(T)\n    u = np.zeros(T)\n    \n    x[0] = x0\n    \n    for k in range(T):\n        u[k] = 0.5 + np.sin(0.03 * k) + 0.1 * np.sin(0.005 * k**2)\n        v_k = np.random.normal(0, np.sqrt(R_true))\n        y[k] = (H @ x[k] + D @ np.array([[u[k]]]))[0, 0] + v_k\n        if k  T - 1:\n            w_k = np.random.multivariate_normal(np.zeros(n_states), Q_true)\n            x[k+1] = A @ x[k] + B.flatten() * u[k] + w_k\n            \n    return y, u\n\ndef kalman_filter(y, u, A, B, H, D, Q, R, x0, P0):\n    \"\"\"Performs the Kalman filter forward pass.\"\"\"\n    T = len(y)\n    n_states = A.shape[0]\n\n    # Storage for filter outputs\n    x_pred = np.zeros((T, n_states))\n    P_pred = np.zeros((T, n_states, n_states))\n    x_filt = np.zeros((T, n_states))\n    P_filt = np.zeros((T, n_states, n_states))\n    log_likelihood = 0.0\n\n    # Initialization\n    x_pred[0] = x0\n    P_pred[0] = P0\n    \n    for k in range(T):\n        if k > 0:\n            x_pred[k] = A @ x_filt[k-1] + B.flatten() * u[k-1]\n            P_pred[k] = A @ P_filt[k-1] @ A.T + Q\n        \n        # Update step\n        e_k = y[k] - (H @ x_pred[k] + D @ np.array([[u[k]]]))[0, 0]\n        S_k = (H @ P_pred[k] @ H.T + R)[0, 0]\n        \n        if S_k = 0: return None # Filter divergence\n        \n        K_k = P_pred[k] @ H.T / S_k\n        \n        x_filt[k] = x_pred[k] + K_k.flatten() * e_k\n        P_filt[k] = (np.eye(n_states) - K_k @ H) @ P_pred[k]\n        \n        log_likelihood += -0.5 * (np.log(2 * np.pi * S_k) + e_k**2 / S_k)\n        \n    return x_pred, P_pred, x_filt, P_filt, log_likelihood\n\ndef rts_smoother(A, x_pred, P_pred, x_filt, P_filt):\n    \"\"\"Performs the Rauch-Tung-Striebel backward pass.\"\"\"\n    T = x_filt.shape[0]\n    n_states = A.shape[0]\n\n    x_smooth = np.copy(x_filt)\n    P_smooth = np.copy(P_filt)\n    P_lag1_smooth = np.zeros((T - 1, n_states, n_states))\n\n    for k in range(T - 2, -1, -1):\n        try:\n            P_pred_k1_inv = inv(P_pred[k+1])\n        except np.linalg.LinAlgError:\n            return None # Smoother divergence\n        \n        J_k = P_filt[k] @ A.T @ P_pred_k1_inv\n        \n        x_smooth[k] += J_k @ (x_smooth[k+1] - x_pred[k+1])\n        P_smooth[k] += J_k @ (P_smooth[k+1] - P_pred[k+1]) @ J_k.T\n        \n    # Lag-one covariance (an alternative formulation exists but this is common)\n    for k in range(T-2, -1, -1):\n        try:\n            J_k = P_filt[k] @ A.T @ inv(P_pred[k+1])\n            P_lag1_smooth[k] = P_smooth[k+1] @ J_k.T\n        except np.linalg.LinAlgError:\n            return None\n\n    return x_smooth, P_smooth, P_lag1_smooth\n\ndef m_step(y, u, A, B, H, D, x_smooth, P_smooth, P_lag1_smooth):\n    \"\"\"Performs the M-step of the EM algorithm.\"\"\"\n    T = len(y)\n    n_states = A.shape[0]\n    \n    # Update Q\n    sum_Q = np.zeros((n_states, n_states))\n    for k in range(T-1):\n        res_mean = x_smooth[k+1] - A @ x_smooth[k] - B.flatten() * u[k]\n        term1 = np.outer(res_mean, res_mean)\n        \n        Gamma_k = P_smooth[k+1] + A @ P_smooth[k] @ A.T - A @ P_lag1_smooth[k].T - P_lag1_smooth[k] @ A.T\n        sum_Q += term1 + Gamma_k\n    Q_new = sum_Q / (T-1)\n\n    # Update R\n    sum_R = 0.0\n    for k in range(T):\n        res_mean = y[k] - (H @ x_smooth[k] + D @ np.array([[u[k]]]))[0, 0]\n        term1 = res_mean**2\n        term2 = (H @ P_smooth[k] @ H.T)[0, 0]\n        sum_R += term1 + term2\n    R_new = sum_R / T\n    \n    return Q_new, R_new\n\ndef enforce_psd(M, min_eig=1e-15):\n    \"\"\"Enforces a matrix to be symmetric positive semi-definite.\"\"\"\n    M_sym = (M + M.T) / 2\n    eigvals, eigvecs = eigh(M_sym)\n    eigvals[eigvals  min_eig] = min_eig\n    return eigvecs @ np.diag(eigvals) @ eigvecs.T\n\ndef run_em_for_case(case_params):\n    \"\"\"Runs the EM algorithm for a single test case.\"\"\"\n    y, u = simulate_data(case_params)\n    \n    A, B, H, D = case_params[\"A\"], case_params[\"B\"], case_params[\"H\"], case_params[\"D\"]\n    x0, P0 = case_params[\"x0\"], case_params[\"P0\"]\n    Q, R = np.copy(case_params[\"Q_init\"]), case_params[\"R_init\"]\n    \n    max_iter = 60\n    tol = 0.05\n    eps_norm = 1e-12\n    ll_history = []\n    \n    for i in range(max_iter):\n        Q_old, R_old = np.copy(Q), R\n        \n        # E-step\n        kf_out = kalman_filter(y, u, A, B, H, D, Q, R, x0, P0)\n        if kf_out is None: return False # Divergence\n        x_pred, P_pred, x_filt, P_filt, log_likelihood = kf_out\n        ll_history.append(log_likelihood)\n\n        rts_out = rts_smoother(A, x_pred, P_pred, x_filt, P_filt)\n        if rts_out is None: return False # Divergence\n        x_smooth, P_smooth, P_lag1_smooth = rts_out\n\n        # M-step\n        Q_new, R_new = m_step(y, u, A, B, H, D, x_smooth, P_smooth, P_lag1_smooth)\n        \n        # Enforce PSD constraints\n        Q = enforce_psd(Q_new)\n        R = max(R_new, 1e-15)\n\n        # Check convergence\n        delta_Q = norm(Q - Q_old, 'fro') / max(norm(Q_old, 'fro'), eps_norm)\n        delta_R = abs(R - R_old) / max(abs(R_old), eps_norm)\n\n        if delta_Q = tol and delta_R = tol:\n            if i >= 4:\n                last_five_ll = ll_history[-5:]\n                is_non_decreasing = all(last_five_ll[j] >= last_five_ll[j-1] - 1e-9 for j in range(1, 5))\n                if is_non_decreasing:\n                    return True\n\n    return False\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}