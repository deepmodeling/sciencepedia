{
    "hands_on_practices": [
        {
            "introduction": "The heart of Bayesian optimization is the probabilistic surrogate model, which updates its beliefs as new data becomes available. This first practice isolates that core mechanism, focusing on how a Gaussian Process (GP) model for a single design point updates its posterior distribution when faced with noisy experimental replicates. By working through the Bayesian inference for a Normal-Normal model , you will gain a foundational understanding of how GPs learn from data and quantify uncertainty, which is essential for all subsequent steps in the optimization loop.",
            "id": "3896095",
            "problem": "In automated battery design and simulation, the cycle life of a candidate design point $x$ is evaluated by running expensive charge-discharge experiments that yield noisy replicates. To guide Bayesian optimization under experimental noise, consider a single design $x$ where the latent cycle life $f(x)$ is modeled as a Gaussian Process (GP), which at a fixed $x$ induces a Normal prior $f(x) \\sim \\mathcal{N}(m_0, \\sigma_f^2)$. Each replicate measurement is modeled as $y_i = f(x) + \\varepsilon_i$ with independent, identically distributed $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. This setup corresponds to a Normal likelihood with known noise variance and a Normal prior.\n\nSuppose a physics-informed surrogate provides prior mean $m_0 = 950$ cycles and prior variance $\\sigma_f^2 = 1000$ cycles$^2$ at this $x$. A metrology study estimates the observation noise variance as $\\sigma_n^2 = 1000$ cycles$^2$. You then run $n=3$ replicates at $x$, obtaining observed cycle lives $y_1 = 972$ cycles, $y_2 = 961$ cycles, and $y_3 = 986$ cycles.\n\nUsing only the foundational definitions of Bayes’ theorem and the Normal-Normal conjugate model, do the following:\n\n1. Derive the posterior predictive distribution for a future noisy measurement $y_*$ at the same design $x$, that is, compute the posterior predictive mean and variance at $x$ based on the three observed replicates and the given prior and noise parameters.\n\n2. Determine the recommended number of additional replicates $k$ at the same design $x$ needed so that the posterior variance of the latent $f(x)$ is halved relative to its current value after $n=3$ replicates. Treat $k$ as the minimal nonnegative integer that achieves this halving.\n\nExpress the posterior predictive mean in cycles and the posterior predictive variance in cycles$^2$, both rounded to four significant figures. Report $k$ as an integer. Provide your final answer as three entries in a single row matrix in the following order: posterior predictive mean, posterior predictive variance, and $k$.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Prior distribution for the latent cycle life $f(x)$: $f(x) \\sim \\mathcal{N}(m_0, \\sigma_f^2)$.\n-   Prior mean: $m_0 = 950$ cycles.\n-   Prior variance: $\\sigma_f^2 = 1000$ cycles$^2$.\n-   Likelihood model for replicates: $y_i = f(x) + \\varepsilon_i$.\n-   Observation noise distribution: $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n-   Observation noise variance: $\\sigma_n^2 = 1000$ cycles$^2$.\n-   Number of observed replicates: $n=3$.\n-   Observed data: $y_1 = 972$, $y_2 = 961$, $y_3 = 986$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing the standard and widely accepted Bayesian framework of a Normal-Normal conjugate model for inference, which is fundamental to Gaussian Process regression. The context—automated battery design using expensive simulations—is a realistic application of Bayesian optimization. The problem is well-posed, providing all necessary parameters and data to uniquely determine the required quantities. The language is objective and precise. There are no contradictions, missing information, or pseudoscientific claims. The numbers provided are physically plausible for battery cycle life.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n---\n\nThe solution is divided into two parts as requested by the problem statement.\n\n### Part 1: Posterior Predictive Distribution\n\nThe goal is to find the posterior predictive distribution for a future noisy measurement $y_*$, which is $p(y_* | D)$, where $D = \\{y_1, y_2, y_3\\}$. This requires first finding the posterior distribution of the latent cycle life $f(x)$ given the data $D$, which is $p(f(x)|D)$.\n\n**1. Posterior Distribution of $f(x)$**\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the prior distribution and the likelihood function:\n$$ p(f(x)|D) \\propto p(D|f(x)) \\, p(f(x)) $$\nThe prior is given as a Normal distribution:\n$$ p(f(x)) = \\mathcal{N}(f(x) | m_0, \\sigma_f^2) \\propto \\exp\\left(-\\frac{1}{2}\\frac{(f(x) - m_0)^2}{\\sigma_f^2}\\right) $$\nThe likelihood of the $n$ independent and identically distributed observations $y_i \\sim \\mathcal{N}(f(x), \\sigma_n^2)$ is:\n$$ p(D|f(x)) = \\prod_{i=1}^n \\mathcal{N}(y_i | f(x), \\sigma_n^2) \\propto \\exp\\left(-\\frac{1}{2\\sigma_n^2} \\sum_{i=1}^n (y_i - f(x))^2\\right) $$\nThe sum in the exponent can be centered around the sample mean $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$:\n$$ \\sum_{i=1}^n (y_i - f(x))^2 = \\sum_{i=1}^n (y_i - \\bar{y} + \\bar{y} - f(x))^2 = \\sum_{i=1}^n(y_i - \\bar{y})^2 + n(\\bar{y} - f(x))^2 $$\nThe term $\\sum_{i=1}^n(y_i - \\bar{y})^2$ does not depend on $f(x)$ and can be absorbed into the normalization constant. Thus, the effective likelihood can be expressed in terms of the sufficient statistic $\\bar{y}$:\n$$ p(D|f(x)) \\propto \\exp\\left(-\\frac{n}{2\\sigma_n^2}(\\bar{y} - f(x))^2\\right) $$\nThis shows that the likelihood of the sample mean is $p(\\bar{y}|f(x)) = \\mathcal{N}(\\bar{y} | f(x), \\sigma_n^2/n)$.\n\nCombining the prior and likelihood, the exponent of the posterior $p(f(x)|D)$ is, up to an additive constant:\n$$ -\\frac{1}{2}\\left( \\frac{(f(x) - m_0)^2}{\\sigma_f^2} + \\frac{(f(x) - \\bar{y})^2}{\\sigma_n^2/n} \\right) $$\nExpanding the squares and collecting terms in $f(x)^2$ and $f(x)$:\n$$ -\\frac{1}{2}\\left( f(x)^2\\left(\\frac{1}{\\sigma_f^2} + \\frac{n}{\\sigma_n^2}\\right) - 2f(x)\\left(\\frac{m_0}{\\sigma_f^2} + \\frac{n\\bar{y}}{\\sigma_n^2}\\right) + \\text{const} \\right) $$\nThis is the exponent of a Normal distribution for $f(x)$, $\\mathcal{N}(f(x)|m_n, \\sigma_{f,n}^2)$, where the posterior inverse variance (precision) $\\frac{1}{\\sigma_{f,n}^2}$ and posterior mean $m_n$ are identified by completing the square:\n$$ \\frac{1}{\\sigma_{f,n}^2} = \\frac{1}{\\sigma_f^2} + \\frac{n}{\\sigma_n^2} $$\n$$ m_n = \\sigma_{f,n}^2 \\left( \\frac{m_0}{\\sigma_f^2} + \\frac{n\\bar{y}}{\\sigma_n^2} \\right) $$\nFirst, we compute the sample mean $\\bar{y}$ from the data:\n$$ \\bar{y} = \\frac{y_1 + y_2 + y_3}{n} = \\frac{972 + 961 + 986}{3} = \\frac{2919}{3} = 973 $$\nNow, we substitute the given values: $m_0=950$, $\\sigma_f^2=1000$, $\\sigma_n^2=1000$, $n=3$, $\\bar{y}=973$.\nThe posterior variance $\\sigma_{f,n}^2$ is:\n$$ \\frac{1}{\\sigma_{f,n}^2} = \\frac{1}{1000} + \\frac{3}{1000} = \\frac{4}{1000} = \\frac{1}{250} \\implies \\sigma_{f,n}^2 = 250 $$\nThe posterior mean $m_n$ is:\n$$ m_n = 250 \\left( \\frac{950}{1000} + \\frac{3 \\times 973}{1000} \\right) = \\frac{250}{1000} (950 + 2919) = \\frac{1}{4}(3869) = 967.25 $$\nSo, the posterior distribution of the latent function is $f(x)|D \\sim \\mathcal{N}(967.25, 250)$.\n\n**2. Posterior Predictive Distribution of $y_{*}$**\n\nThe posterior predictive distribution is the distribution of a future observation $y_*$ given the past data $D$. It is obtained by marginalizing the likelihood of $y_*$ over the posterior distribution of $f(x)$:\n$$ p(y_*|D) = \\int p(y_*|f(x)) \\, p(f(x)|D) \\, df(x) $$\nWe have $y_* = f(x) + \\varepsilon_*$ where $f(x) \\sim \\mathcal{N}(m_n, \\sigma_{f,n}^2)$ and $\\varepsilon_* \\sim \\mathcal{N}(0, \\sigma_n^2)$. Since $y_*$ is the sum of two independent normally distributed random variables, it is also normally distributed.\nThe mean of the posterior predictive distribution is:\n$$ E[y_*|D] = E[f(x) + \\varepsilon_*|D] = E[f(x)|D] + E[\\varepsilon_*] = m_n + 0 = m_n $$\nThe variance of the posterior predictive distribution is:\n$$ \\text{Var}(y_*|D) = \\text{Var}(f(x) + \\varepsilon_*|D) = \\text{Var}(f(x)|D) + \\text{Var}(\\varepsilon_*) = \\sigma_{f,n}^2 + \\sigma_n^2 $$\nSubstituting the computed and given values:\n$$ E[y_*|D] = 967.25 $$\n$$ \\text{Var}(y_*|D) = 250 + 1000 = 1250 $$\nRounding to four significant figures as requested:\n-   Posterior predictive mean: $967.3$ cycles.\n-   Posterior predictive variance: $1250$ cycles$^2$.\n\n### Part 2: Required Number of Additional Replicates\n\nThe task is to find the minimal non-negative integer of additional replicates, $k$, required to halve the posterior variance of the latent function $f(x)$.\n\nThe current posterior variance with $n=3$ replicates is $\\sigma_{f,3}^2 = 250$.\nThe target posterior variance is $\\sigma_{f, \\text{target}}^2 = \\frac{1}{2} \\sigma_{f,3}^2 = \\frac{1}{2}(250) = 125$.\n\nLet $N$ be the total number of replicates. The posterior variance after $N$ replicates, $\\sigma_{f,N}^2$, is given by the formula:\n$$ \\frac{1}{\\sigma_{f,N}^2} = \\frac{1}{\\sigma_f^2} + \\frac{N}{\\sigma_n^2} $$\nSubstituting the known variances:\n$$ \\frac{1}{\\sigma_{f,N}^2} = \\frac{1}{1000} + \\frac{N}{1000} = \\frac{1+N}{1000} $$\nThis gives the posterior variance as a function of $N$:\n$$ \\sigma_{f,N}^2 = \\frac{1000}{1+N} $$\nWe need to find the smallest integer $N$ such that the posterior variance is less than or equal to the target variance of $125$.\n$$ \\sigma_{f,N}^2 \\le 125 $$\n$$ \\frac{1000}{1+N} \\le 125 $$\n$$ 1000 \\le 125(1+N) $$\n$$ \\frac{1000}{125} \\le 1+N $$\n$$ 8 \\le 1+N $$\n$$ N \\ge 7 $$\nThe minimum total number of replicates required is $N=7$.\nSince we have already performed $n=3$ replicates, the number of additional replicates $k$ is:\n$$ k = N - n = 7 - 3 = 4 $$\nThe minimal non-negative integer of additional replicates required is $k=4$.\n\nThe final answer is a row matrix containing the posterior predictive mean, posterior predictive variance, and $k$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n967.3 & 1250 & 4\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once the surrogate model provides a posterior distribution over the objective function, the crucial question becomes: where should we sample next? This decision is governed by an acquisition function, which balances the trade-off between exploiting promising regions and exploring uncertain ones. This exercise  provides a direct, quantitative comparison of two classic acquisition functions, the Probability of Improvement (PI) and the Upper Confidence Bound (UCB), revealing their differing strategies and how they can lead to selecting different candidate points for the next expensive evaluation.",
            "id": "3896133",
            "problem": "A laboratory automates the selection of electrolyte salts for a lithium-ion battery formulation by optimizing a dimensionless performance objective, normalized to the interval $[0,1]$, that combines simulated ionic conductivity and electrochemical stability. The performance is modeled with a Gaussian Process (GP) surrogate; for any candidate salt at input $x$, the GP posterior predictive distribution for the unknown objective $f(x)$ is Gaussian with mean $\\mu(x)$ and variance $\\sigma^2(x)$. The aim is to maximize $f(x)$.\n\nYou are given three candidate salts {$S_1, S_2, S_3$} with GP posterior summaries computed from a set of expensive multiphysics simulations at other design points. The best observed objective value so far is $f^*=0.95$. An improvement margin $\\xi=0.01$ is used. The Upper Confidence Bound (UCB) exploration parameter is $\\beta=4$. For the three candidates, the GP posterior predictive means and variances are:\n- $S_1$: $\\mu_1=0.955$, $\\sigma_1^2=1.0\\times 10^{-4}$,\n- $S_2$: $\\mu_2=0.85$, $\\sigma_2^2=1.44\\times 10^{-2}$,\n- $S_3$: $\\mu_3=0.95$, $\\sigma_3^2=4.0\\times 10^{-4}$.\n\nUse the following fundamental bases:\n- The GP predictive model implies $f(x)|\\mathcal{D}\\sim\\mathcal{N}(\\mu(x),\\sigma^2(x))$ for a fixed $x$ and dataset $\\mathcal{D}$.\n- The Probability of Improvement (PI) acquisition for maximization at $x$ is the probability that the improvement random variable $I(x)=\\max\\{0, f(x)-(f^*+\\xi)\\}$ is strictly positive under the predictive distribution.\n- The Upper Confidence Bound (UCB) acquisition for maximization at $x$ is $\\mu(x)+\\sqrt{\\beta}\\,\\sigma(x)$, which arises from concentration inequalities for sub-Gaussian processes.\n\nTasks:\n1) Derive from first principles the analytical expression for the Probability of Improvement at each candidate $S_i$ in terms of $\\mu_i$, $\\sigma_i$, $f^*$, and $\\xi$, starting from the definition of $I(x)$ and the Gaussian predictive model. Evaluate the resulting expressions numerically for $S_1$, $S_2$, and $S_3$.\n2) Using the definition of the Upper Confidence Bound, compute the UCB value at each $S_i$ and evaluate numerically.\n3) Rank the three candidates under PI and under UCB from highest to lowest acquisition value. If there is an exact tie in an acquisition value, break ties by preferring the candidate with the larger predictive mean $\\mu_i$. State which candidate would be selected by each acquisition.\n4) Let $i_{\\mathrm{PI}}\\in\\{1,2,3\\}$ and $i_{\\mathrm{UCB}}\\in\\{1,2,3\\}$ denote the indices of the candidates selected by PI and UCB, respectively, under the tie-breaking rule above. Report the single integer $i_{\\mathrm{UCB}}-i_{\\mathrm{PI}}$ as your final answer. Do not include units.\n\nAll computations are dimensionless. Where intermediate numerical values are needed, you may approximate standard normal probabilities to at least four significant figures, but your final reported integer $i_{\\mathrm{UCB}}-i_{\\mathrm{PI}}$ must be exact (no rounding instruction is needed for an integer).",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of Bayesian optimization, is well-posed with all necessary information provided, and is expressed in objective, formal language. We proceed with the solution.\n\nThe solution is structured into four parts, corresponding to the four tasks specified in the problem statement.\n\n**1) Derivation and Calculation of Probability of Improvement (PI)**\n\nThe Probability of Improvement, $\\mathrm{PI}(x)$, is defined as the probability that the improvement random variable, $I(x) = \\max\\{0, f(x)-(f^*+\\xi)\\}$, is strictly positive.\nThe condition $I(x) > 0$ is equivalent to the condition $f(x) - (f^* + \\xi) > 0$, or $f(x) > f^* + \\xi$.\n\nThe problem states that the GP posterior predictive distribution for the objective function $f(x)$ is Gaussian: $f(x) \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))$.\nTo calculate the probability, we standardize the random variable $f(x)$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0,1)$. Then we can write $f(x) = \\mu(x) + \\sigma(x)Z$.\n\nThe inequality $f(x) > f^* + \\xi$ becomes:\n$$ \\mu(x) + \\sigma(x)Z > f^* + \\xi $$\nRearranging for $Z$, we get:\n$$ \\sigma(x)Z > f^* + \\xi - \\mu(x) $$\n$$ Z > \\frac{f^* + \\xi - \\mu(x)}{\\sigma(x)} $$\nThe probability is thus:\n$$ \\mathrm{PI}(x) = P\\left(Z > \\frac{f^* + \\xi - \\mu(x)}{\\sigma(x)}\\right) $$\nLet $\\Phi(\\cdot)$ denote the cumulative distribution function (CDF) of the standard normal distribution. The probability $P(Z > z)$ can be written as $1 - \\Phi(z)$, or equivalently, $\\Phi(-z)$. Therefore, the analytical expression for the Probability of Improvement is:\n$$ \\mathrm{PI}(x) = \\Phi\\left(\\frac{\\mu(x) - f^{*} - \\xi}{\\sigma(x)}\\right) $$\nNow, we evaluate this expression for the three candidate salts using the given values: $f^* = 0.95$ and $\\xi = 0.01$. The improvement threshold is $f^* + \\xi = 0.95 + 0.01 = 0.96$.\n\nFor candidate $S_1$:\n$\\mu_1 = 0.955$ and $\\sigma_1^2 = 1.0 \\times 10^{-4}$, so $\\sigma_1 = \\sqrt{1.0 \\times 10^{-4}} = 0.01$.\nThe argument of the CDF is:\n$$ z_1 = \\frac{\\mu_1 - (f^* + \\xi)}{\\sigma_1} = \\frac{0.955 - 0.96}{0.01} = \\frac{-0.005}{0.01} = -0.5 $$\nSo, $\\mathrm{PI}_1 = \\Phi(-0.5) \\approx 0.3085$.\n\nFor candidate $S_2$:\n$\\mu_2 = 0.85$ and $\\sigma_2^2 = 1.44 \\times 10^{-2}$, so $\\sigma_2 = \\sqrt{1.44 \\times 10^{-2}} = 0.12$.\nThe argument of the CDF is:\n$$ z_2 = \\frac{\\mu_2 - (f^* + \\xi)}{\\sigma_2} = \\frac{0.85 - 0.96}{0.12} = \\frac{-0.11}{0.12} \\approx -0.9167 $$\nSo, $\\mathrm{PI}_2 = \\Phi(-0.9167) \\approx 0.1797$.\n\nFor candidate $S_3$:\n$\\mu_3 = 0.95$ and $\\sigma_3^2 = 4.0 \\times 10^{-4}$, so $\\sigma_3 = \\sqrt{4.0 \\times 10^{-4}} = 0.02$.\nThe argument of the CDF is:\n$$ z_3 = \\frac{\\mu_3 - (f^* + \\xi)}{\\sigma_3} = \\frac{0.95 - 0.96}{0.02} = \\frac{-0.01}{0.02} = -0.5 $$\nSo, $\\mathrm{PI}_3 = \\Phi(-0.5) \\approx 0.3085$.\n\n**2) Calculation of Upper Confidence Bound (UCB)**\n\nThe Upper Confidence Bound is defined as $\\mathrm{UCB}(x) = \\mu(x) + \\sqrt{\\beta}\\sigma(x)$.\nGiven the exploration parameter $\\beta = 4$, we have $\\sqrt{\\beta} = \\sqrt{4} = 2$. We compute the UCB value for each candidate.\n\nFor candidate $S_1$:\n$\\mu_1 = 0.955$ and $\\sigma_1 = 0.01$.\n$$ \\mathrm{UCB}_1 = 0.955 + 2 \\times 0.01 = 0.955 + 0.02 = 0.975 $$\n\nFor candidate $S_2$:\n$\\mu_2 = 0.85$ and $\\sigma_2 = 0.12$.\n$$ \\mathrm{UCB}_2 = 0.85 + 2 \\times 0.12 = 0.85 + 0.24 = 1.09 $$\n\nFor candidate $S_3$:\n$\\mu_3 = 0.95$ and $\\sigma_3 = 0.02$.\n$$ \\mathrm{UCB}_3 = 0.95 + 2 \\times 0.02 = 0.95 + 0.04 = 0.99 $$\n\n**3) Ranking of Candidates and Selection**\n\nWe rank the candidates based on the PI and UCB values, from highest to lowest.\n\nRanking by PI:\nThe computed PI values are $\\mathrm{PI}_1 \\approx 0.3085$, $\\mathrm{PI}_2 \\approx 0.1797$, and $\\mathrm{PI}_3 \\approx 0.3085$.\nWe observe that $\\mathrm{PI}_1 = \\mathrm{PI}_3 > \\mathrm{PI}_2$. There is an exact tie between $S_1$ and $S_3$.\nThe tie-breaking rule states to prefer the candidate with the larger predictive mean $\\mu_i$.\nComparing $S_1$ and $S_3$: $\\mu_1 = 0.955$ and $\\mu_3 = 0.95$. Since $\\mu_1 > \\mu_3$, candidate $S_1$ is ranked higher than $S_3$.\nThe final PI ranking is $S_1 > S_3 > S_2$.\nThe candidate selected by the PI acquisition function is $S_1$. Therefore, $i_{\\mathrm{PI}} = 1$.\n\nRanking by UCB:\nThe computed UCB values are $\\mathrm{UCB}_1 = 0.975$, $\\mathrm{UCB}_2 = 1.09$, and $\\mathrm{UCB}_3 = 0.99$.\nComparing these values, we have $1.09 > 0.99 > 0.975$.\nThe UCB ranking is $S_2 > S_3 > S_1$.\nThe candidate selected by the UCB acquisition function is $S_2$. Therefore, $i_{\\mathrm{UCB}} = 2$.\n\n**4) Final Calculation**\n\nThe problem asks for the integer value of $i_{\\mathrm{UCB}} - i_{\\mathrm{PI}}$.\nWe found that $i_{\\mathrm{PI}} = 1$ and $i_{\\mathrm{UCB}} = 2$.\nThus, the final result is:\n$$ i_{\\mathrm{UCB}} - i_{\\mathrm{PI}} = 2 - 1 = 1 $$\nThis is the final integer to be reported.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "An acquisition function provides a cheap-to-evaluate utility score for every point in the design space, and our goal is to find the point that maximizes this utility. This final practice moves from theory to implementation by focusing on how to numerically optimize the acquisition function itself. By deriving the analytical gradient of the Expected Improvement (EI) function and performing a gradient ascent step , you will engage with the inner optimization loop of the Bayesian optimization algorithm, a critical step for efficiently identifying the most valuable point for the next experiment.",
            "id": "3896098",
            "problem": "An automated battery design team is using Bayesian optimization to tune two normalized design variables $x = (x_1, x_2)$ that encode cathode porosity and coating thickness after affine scaling to $[0,1]^2$. Each expensive multi-physics simulation returns a scalar objective $f(x)$ (to be minimized) that aggregates discharge overpotential and manufacturing penalty. A Gaussian Process (GP) surrogate model is fit to the collected data, yielding a Gaussian posterior predictive distribution for the latent response $Y(x) \\sim \\mathcal{N}(\\mu(x), \\sigma(x)^2)$ at any $x$. The expected improvement acquisition function $\\mathrm{EI}(x)$ is defined from the improvement $I(x) = \\max\\{f_{\\min} - Y(x), 0\\}$, where $f_{\\min}$ is the best observed objective to date, and $\\mathrm{EI}(x) = \\mathbb{E}[I(x)]$. Starting only from these base definitions, the properties of the normal distribution, and standard differentiation rules, derive an explicit expression for the gradient $\\nabla \\mathrm{EI}(x)$ in terms of $\\mu(x)$, $\\sigma(x)$, $\\nabla \\mu(x)$, and $\\nabla \\sigma(x)$.\n\nThen, at the current candidate $x^{(0)} = (x_1^{(0)}, x_2^{(0)}) = (0.42, 0.73)$, the surrogate provides the following local quantities: $\\mu(x^{(0)}) = 2.1$, $\\sigma(x^{(0)}) = 0.5$, $\\nabla \\mu(x^{(0)}) = \\begin{pmatrix}0.8 \\\\ -0.02\\end{pmatrix}$, and $\\nabla \\sigma(x^{(0)}) = \\begin{pmatrix}0.1 \\\\ 0.005\\end{pmatrix}$. The current best (minimum) observed objective is $f_{\\min} = 1.8$. Compute the gradient $\\nabla \\mathrm{EI}(x^{(0)})$ numerically, and perform one gradient ascent step on $\\mathrm{EI}$ with step size $\\eta = 0.15$ to obtain $x^{(1)} = x^{(0)} + \\eta \\nabla \\mathrm{EI}(x^{(0)})$.\n\nReport only the updated design vector $x^{(1)}$ as a row vector. Round your answer to $4$ significant figures. The design variables are dimensionless due to normalization; do not report any physical units.",
            "solution": "The user wants me to solve a problem about Bayesian optimization.\n\nHere is my plan:\n1.  **Validate the Problem**: I will perform the mandatory validation check.\n2.  **Solve the Problem**:\n    *   Derive the analytical expression for the gradient of the Expected Improvement (EI) acquisition function.\n    *   Use the derived expression and the provided numerical values to compute the gradient at the specified point.\n    *   Perform a single gradient ascent step to find the updated design vector.\n    *   Format the final answer as requested.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   Design variables: $x = (x_1, x_2) \\in [0,1]^2$.\n*   Objective function to be minimized: $f(x)$.\n*   Surrogate model posterior: $Y(x) \\sim \\mathcal{N}(\\mu(x), \\sigma(x)^2)$.\n*   Improvement function: $I(x) = \\max\\{f_{\\min} - Y(x), 0\\}$, where $f_{\\min}$ is the best observed objective.\n*   Acquisition function: $\\mathrm{EI}(x) = \\mathbb{E}[I(x)]$.\n*   Task 1: Derive $\\nabla \\mathrm{EI}(x)$ in terms of $\\mu(x)$, $\\sigma(x)$, $\\nabla \\mu(x)$, and $\\nabla \\sigma(x)$.\n*   Task 2: Compute $x^{(1)} = x^{(0)} + \\eta \\nabla \\mathrm{EI}(x^{(0)})$.\n*   Current candidate point: $x^{(0)} = (0.42, 0.73)$.\n*   Surrogate model quantities at $x^{(0)}$:\n    *   $\\mu(x^{(0)}) = 2.1$\n    *   $\\sigma(x^{(0)}) = 0.5$\n    *   $\\nabla \\mu(x^{(0)}) = \\begin{pmatrix}0.8 \\\\ -0.02\\end{pmatrix}$\n    *   $\\nabla \\sigma(x^{(0)}) = \\begin{pmatrix}0.1 \\\\ 0.005\\end{pmatrix}$\n*   Best observed objective: $f_{\\min} = 1.8$.\n*   Step size: $\\eta = 0.15$.\n*   Final answer requirement: Report $x^{(1)}$ as a row vector, rounded to $4$ significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded**: The problem is based on well-established principles of Bayesian optimization, Gaussian processes, and multivariate calculus. These are standard topics in machine learning and applied mathematics.\n*   **Well-Posed**: The problem provides all necessary definitions, functions, and numerical values to derive the requested gradient and compute the updated design vector. The tasks are clearly defined, leading to a unique solution.\n*   **Objective**: The problem is stated using precise mathematical language and objective numerical data. There are no subjective or ambiguous terms.\n*   The problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, contradiction, etc.). It is a standard, solvable problem in its domain.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. The solution process will now proceed.\n\n### Derivation and Solution\n\nThe first step is to derive an explicit expression for the Expected Improvement, $\\mathrm{EI}(x)$, and then find its gradient, $\\nabla \\mathrm{EI}(x)$.\n\n**1. Expression for $\\mathrm{EI}(x)$**\n\nThe Expected Improvement is defined as the expectation of the improvement function $I(x) = \\max\\{f_{\\min} - Y(x), 0\\}$, where $Y(x)$ follows a Gaussian distribution $Y(x) \\sim \\mathcal{N}(\\mu(x), \\sigma(x)^2)$. The probability density function (PDF) of $Y(x)$ is $p(y|\\mu(x),\\sigma(x)) = \\frac{1}{\\sigma(x)\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu(x))^2}{2\\sigma(x)^2}\\right)$.\n\nThe expectation is an integral over all possible values of the latent response $y$:\n$$\n\\mathrm{EI}(x) = \\mathbb{E}[I(x)] = \\int_{-\\infty}^{\\infty} \\max\\{f_{\\min} - y, 0\\} p(y|\\mu(x),\\sigma(x)) dy\n$$\nThe term $\\max\\{f_{\\min} - y, 0\\}$ is non-zero only when $y  f_{\\min}$. Thus, the integral range can be restricted:\n$$\n\\mathrm{EI}(x) = \\int_{-\\infty}^{f_{\\min}} (f_{\\min} - y) \\frac{1}{\\sigma(x)\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu(x))^2}{2\\sigma(x)^2}\\right) dy\n$$\nTo simplify the integral, we perform a change of variables to the standard normal distribution. Let $z = \\frac{y-\\mu(x)}{\\sigma(x)}$. Then $y = \\mu(x) + z\\sigma(x)$ and $dy = \\sigma(x)dz$. The upper limit of integration for $y=f_{\\min}$ becomes $z_{up} = \\frac{f_{\\min}-\\mu(x)}{\\sigma(x)}$. Let's denote this standardized improvement as $Z(x) = \\frac{f_{\\min}-\\mu(x)}{\\sigma(x)}$. The integral transforms to:\n$$\n\\mathrm{EI}(x) = \\int_{-\\infty}^{Z(x)} (f_{\\min} - (\\mu(x) + z\\sigma(x))) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz\n$$\nLet $\\phi(z)$ be the PDF and $\\Phi(z)$ be the CDF of the standard normal distribution.\n$$\n\\mathrm{EI}(x) = \\int_{-\\infty}^{Z(x)} (f_{\\min} - \\mu(x)) \\phi(z) dz - \\sigma(x) \\int_{-\\infty}^{Z(x)} z \\phi(z) dz\n$$\nThe first term evaluates to:\n$$\n(f_{\\min} - \\mu(x)) \\int_{-\\infty}^{Z(x)} \\phi(z) dz = (f_{\\min} - \\mu(x)) \\Phi(Z(x))\n$$\nFor the second term, we use the fact that $\\frac{d\\phi(z)}{dz} = -z\\phi(z)$:\n$$\n\\int_{-\\infty}^{Z(x)} z \\phi(z) dz = \\int_{-\\infty}^{Z(x)} -\\frac{d\\phi(z)}{dz} dz = [-\\phi(z)]_{-\\infty}^{Z(x)} = -\\phi(Z(x)) - (-\\phi(-\\infty)) = -\\phi(Z(x))\n$$\nsince $\\phi(-\\infty) = 0$.\nSubstituting these results back, we obtain the standard expression for Expected Improvement:\n$$\n\\mathrm{EI}(x) = (f_{\\min} - \\mu(x)) \\Phi(Z(x)) - \\sigma(x)(-\\phi(Z(x)))\n$$\n$$\n\\mathrm{EI}(x) = (f_{\\min} - \\mu(x)) \\Phi\\left(\\frac{f_{\\min}-\\mu(x)}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{f_{\\min}-\\mu(x)}{\\sigma(x)}\\right)\n$$\nIf $\\sigma(x)=0$, the improvement is deterministic. If $\\mu(x) \\ge f_{min}$, $I(x)=0$. If $\\mu(x)  f_{min}$, $I(x) = f_{min} - \\mu(x)$. The formula correctly handles the case $\\sigma(x)\\to 0^+$. If $f_{\\min} > \\mu(x)$, $Z \\to \\infty$, $\\Phi(Z)\\to 1$, $\\phi(Z) \\to 0$, so $\\mathrm{EI}(x) \\to f_{\\min} - \\mu(x)$. If $f_{\\min}  \\mu(x)$, $Z \\to -\\infty$, $\\Phi(Z)\\to 0$, $\\phi(Z)\\to 0$, so $\\mathrm{EI}(x) \\to 0$.\n\n**2. Gradient of $\\mathrm{EI}(x)$**\n\nTo find the gradient $\\nabla \\mathrm{EI}(x)$, we differentiate with respect to $x$ using the chain rule and product rule. Let $Z=Z(x)$, $\\mu=\\mu(x)$, and $\\sigma=\\sigma(x)$.\n$$\n\\nabla \\mathrm{EI} = \\nabla[(f_{\\min} - \\mu) \\Phi(Z)] + \\nabla[\\sigma \\phi(Z)]\n$$\nApplying the product rule to both terms:\n$$\n\\nabla \\mathrm{EI} = [\\nabla(f_{\\min} - \\mu)]\\Phi(Z) + (f_{\\min} - \\mu)[\\nabla\\Phi(Z)] + [\\nabla\\sigma]\\phi(Z) + \\sigma[\\nabla\\phi(Z)]\n$$\nUsing the chain rule, $\\nabla\\Phi(Z) = \\frac{d\\Phi(Z)}{dZ}\\nabla Z = \\phi(Z)\\nabla Z$ and $\\nabla\\phi(Z) = \\frac{d\\phi(Z)}{dZ}\\nabla Z = -Z\\phi(Z)\\nabla Z$. Substituting these in:\n$$\n\\nabla \\mathrm{EI} = -\\nabla\\mu \\cdot \\Phi(Z) + (f_{\\min} - \\mu) \\phi(Z)\\nabla Z + \\nabla\\sigma \\cdot \\phi(Z) - \\sigma Z \\phi(Z)\\nabla Z\n$$\nGroup the terms:\n$$\n\\nabla \\mathrm{EI} = -\\nabla\\mu \\cdot \\Phi(Z) + \\nabla\\sigma \\cdot \\phi(Z) + [(f_{\\min} - \\mu) - \\sigma Z] \\phi(Z)\\nabla Z\n$$\nBy definition, $Z = \\frac{f_{\\min} - \\mu}{\\sigma}$, which implies $f_{\\min} - \\mu = \\sigma Z$. Therefore, the term in the square brackets is exactly zero: $[(f_{\\min} - \\mu) - \\sigma Z] = [\\sigma Z - \\sigma Z] = 0$. This simplifies the expression for the gradient immensely:\n$$\n\\nabla \\mathrm{EI}(x) = -\\nabla\\mu(x) \\Phi\\left(\\frac{f_{\\min} - \\mu(x)}{\\sigma(x)}\\right) + \\nabla\\sigma(x) \\phi\\left(\\frac{f_{\\min} - \\mu(x)}{\\sigma(x)}\\right)\n$$\nThis is the required analytical expression for the gradient of the Expected Improvement.\n\n**3. Numerical Computation**\n\nNow, we apply this formula to the given numerical data.\nGiven values:\n$f_{\\min} = 1.8$, $\\mu(x^{(0)}) = 2.1$, $\\sigma(x^{(0)}) = 0.5$.\n$\\nabla \\mu(x^{(0)}) = \\begin{pmatrix}0.8 \\\\ -0.02\\end{pmatrix}$, $\\nabla \\sigma(x^{(0)}) = \\begin{pmatrix}0.1 \\\\ 0.005\\end{pmatrix}$.\n\nFirst, compute the standardized improvement $Z$ at $x^{(0)}$:\n$$\nZ(x^{(0)}) = \\frac{f_{\\min} - \\mu(x^{(0)})}{\\sigma(x^{(0)})} = \\frac{1.8 - 2.1}{0.5} = \\frac{-0.3}{0.5} = -0.6\n$$\nNext, we need the values of the standard normal PDF $\\phi(z)$ and CDF $\\Phi(z)$ at $z = -0.6$:\n$$\n\\phi(-0.6) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(-0.6)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-0.18) \\approx 0.3332246\n$$\n$$\n\\Phi(-0.6) = \\int_{-\\infty}^{-0.6} \\phi(z)dz \\approx 0.2742531\n$$\nNow, substitute these into the gradient formula:\n$$\n\\nabla \\mathrm{EI}(x^{(0)}) = -\\begin{pmatrix}0.8 \\\\ -0.02\\end{pmatrix} (0.2742531) + \\begin{pmatrix}0.1 \\\\ 0.005\\end{pmatrix} (0.3332246)\n$$\n$$\n\\nabla \\mathrm{EI}(x^{(0)}) = \\begin{pmatrix} -0.8 \\times 0.2742531 + 0.1 \\times 0.3332246 \\\\ -(-0.02) \\times 0.2742531 + 0.005 \\times 0.3332246 \\end{pmatrix}\n$$\n$$\n\\nabla \\mathrm{EI}(x^{(0)}) = \\begin{pmatrix} -0.21940248 + 0.03332246 \\\\ 0.005485062 + 0.001666123 \\end{pmatrix} = \\begin{pmatrix} -0.18608002 \\\\ 0.007151185 \\end{pmatrix}\n$$\n\n**4. Gradient Ascent Step**\n\nFinally, perform one gradient ascent step with step size $\\eta = 0.15$ to get $x^{(1)}$:\n$$\nx^{(1)} = x^{(0)} + \\eta \\nabla \\mathrm{EI}(x^{(0)})\n$$\nThe design vector $x$ is a row vector, so we perform the update component-wise.\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix} = \\begin{pmatrix} 0.42 \\\\ 0.73 \\end{pmatrix} + 0.15 \\begin{pmatrix} -0.18608002 \\\\ 0.007151185 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix} = \\begin{pmatrix} 0.42 + (0.15 \\times -0.18608002) \\\\ 0.73 + (0.15 \\times 0.007151185) \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix} = \\begin{pmatrix} 0.42 - 0.027912003 \\\\ 0.73 + 0.00107267775 \\end{pmatrix} = \\begin{pmatrix} 0.392087997 \\\\ 0.73107267775 \\end{pmatrix}\n$$\nThe problem asks to round the answer to $4$ significant figures.\n$x_1^{(1)} \\approx 0.3921$\n$x_2^{(1)} \\approx 0.7311$\n\nThe updated design vector is $x^{(1)} = (0.3921, 0.7311)$.\nNote: The problem states that the search space is $[0,1]^2$. The updated point $(0.3921, 0.7311)$ is within this domain, so no projection or clipping is necessary.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.3921  0.7311 \\end{pmatrix} } $$"
        }
    ]
}