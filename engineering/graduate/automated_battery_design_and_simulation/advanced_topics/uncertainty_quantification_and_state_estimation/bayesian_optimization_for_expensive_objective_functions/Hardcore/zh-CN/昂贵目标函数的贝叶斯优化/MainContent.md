## 引言
在[自动化电池设计](@entry_id:1121262)、新材料发现等前沿科学与工程领域，我们常常面临一个艰巨的挑战：如何在一个巨大的设计空间中，找到使某个性能指标（如电池循环寿命或[催化剂活性](@entry_id:1122120)）最优化的参数组合？解决这类问题的关键，往往依赖于高保真度的[物理模拟](@entry_id:144318)或复杂的实验，而每一次评估都可能耗费数小时乃至数天。这种“昂贵”的函数评估使得传统的[优化方法](@entry_id:164468)，如[网格搜索](@entry_id:636526)或[遗传算法](@entry_id:172135)，因其需要海量的评估次数而变得不切实际。这便引出了一个核心的知识缺口：我们如何能以极高的样本效率，在有限的计算或实验预算内，逼近全局最优解？

[贝叶斯优化](@entry_id:175791)（Bayesian Optimization, BO）正是为应对这一挑战而生的强大框架。它并非盲目地搜索，而是通过构建一个关于未知[目标函数](@entry_id:267263)的“信念”模型，并利用该模型的不确定性来智能地决定下一次评估的位置，从而在探索未知领域和利用已知信息之间做出最佳权衡。

本文将系统地引导您深入贝叶斯优化的世界。在第一部分“原理与机制”中，我们将剖析其两大支柱——概率代理模型（特别是[高斯过程](@entry_id:182192)）和采集函数，并构建完整的算法流程。接下来的“应用与跨学科联系”部分将展示BO如何通过各种高级扩展，解决高维、多目标、带约束等复杂的现实问题。最后，通过“动手实践”部分，您将有机会亲手实现其核心组件。让我们首先深入其根本，探索贝叶斯优化的“原理与机制”。

## 原理与机制

本章深入探讨了贝叶斯优化（Bayesian Optimization, BO）的核心原理和运行机制。我们将从其根本动机——处理昂贵的[目标函数](@entry_id:267263)——出发，系统地剖析其两大支柱：概率代理模型和采集函数。最终，我们将把这些组件整合成一个强大的、适用于解决复杂工程设计问题的完整算法框架。

### 根本挑战：知识的代价

在许多科学和工程领域，尤其是如[自动化电池设计](@entry_id:1121262)等前沿领域，我们优化的目标函数 $f(x)$ 通常是一个“黑箱”。这意味着我们无法访问其解析形式或梯度信息。更关键的是，对这个函数的每一次评估都可能极其昂贵。例如，在电池设计中，评估一个参数向量 $x$（如电极孔隙率、隔膜厚度、电解液盐浓度等）的性能（如特定充放电协议下的循环寿命），需要运行高保真度的[多物理场模拟](@entry_id:145294)器。

这些模拟器通过求[解耦](@entry_id:160890)合的、[非线性](@entry_id:637147)的[偏微分方程组](@entry_id:172573)（PDEs）来描述电池内部复杂的电化学-热行为，例如基于[多孔电极理论](@entry_id:148271)的模型。这类模型的计算成本极高，其根源在于几个方面 。首先，[电化学动力学](@entry_id:263644)（如巴特勒-沃尔默方程）引入了指数项，导致方程组具有**刚性（stiffness）**，即系统内部存在多个数量级差异的时间尺度。为了保证数值稳定性，必须使用[隐式时间积分](@entry_id:171761)方法。其次，在每个时间步，[隐式方法](@entry_id:138537)都需要求解一个由离散化算子产生的大型稀疏[非线性方程组](@entry_id:178110)，这通常通过牛顿法等迭代方法完成。牛顿法的每一步又需要求解一个[雅可比矩阵](@entry_id:178326)相关的[线性系统](@entry_id:147850)。对于刚性问题，雅可比[矩阵的[条件](@entry_id:150947)数](@entry_id:145150) $\kappa(x)$ 很大，导致 [Krylov 子空间](@entry_id:751067)等[迭代线性求解器](@entry_id:1126792)的收敛变慢。最后，为了保证精度，时间步长 $\Delta t$ 必须能解析最小的物理尺度，如扩散时间尺度 $\tau_d \sim h^2/D$，其中 $h$ 是网格尺寸。在一个 $d$ 维空间中，自由度数量 $M$ 与网格尺寸的关系为 $M \sim h^{-d}$。综合来看，单次模拟的计算时间 $C(M, \kappa(x))$ 通常随自由度 $M$ 和刚度 $\kappa(x)$ 超线性增长，例如，其复杂度可能为 $\Omega(M^{1+2/d}\sqrt{\kappa(x)})$。

当单次评估需要数小时甚至数天时，传统的依赖大量函数评估的[优化算法](@entry_id:147840)（如[网格搜索](@entry_id:636526)、[随机搜索](@entry_id:637353)、遗传算法）变得不切实际。我们的总计算预算 $B$ 是有限的，因此能够在极少数的样本下找到全局最优解变得至关重要。这正是贝叶斯优化发挥作用的地方：它通过一种数据高效（sample-efficient）的方式，在昂贵的“知识”和有限的资源之间做出明智的权衡。

### 贝叶斯优化框架

贝叶斯优化是一个[序贯决策](@entry_id:145234)（sequential decision-making）框架，专门用于优化昂贵的[黑箱函数](@entry_id:163083) 。其核心思想是，用一个廉价且易于优化的**概率代理模型（probabilistic surrogate model）**来近似[目标函数](@entry_id:267263)，并利用这个模型的不确定性来指导下一次的评估点选择。这个过程在每次获取新数据后迭代进行，逐步逼近全局最优点。

该框架主要由两个关键部分组成：

1.  **概率代理模型**：这是一个[统计模型](@entry_id:165873)，它不仅提供对目标函数在未探索点的预测值，还量化了这些预测的**不确定性**。[高斯过程](@entry_id:182192)（Gaussian Process, GP）是迄今为止最常用和最成功的代理模型。它对未知的目标函数 $f$ 定义了一个[先验分布](@entry_id:141376)。当我们收集到一组数据 $\mathcal{D}_n = \{(x_i, y_i)\}_{i=1}^n$（其中 $y_i = f(x_i) + \epsilon_i$ 是带有噪声的观测值）后，这个[先验分布](@entry_id:141376)会通过[贝叶斯定理](@entry_id:897366)更新为[后验分布](@entry_id:145605) $p(f | \mathcal{D}_n)$。

2.  **[采集函数](@entry_id:168889)（Acquisition Function）**：这是一个[效用函数](@entry_id:137807)，记为 $\alpha(x; \mathcal{D}_n)$，它利用代理模型的后验分布来评估在某一点 $x$ 进行下一次函数评估的“价值”。选择下一个评估点 $x_{n+1}$ 的策略就是最大化这个采集函数：$x_{n+1} = \arg\max_{x \in \mathcal{X}} \alpha(x; \mathcal{D}_n)$。采集函数的设计精妙之处在于它能够平衡**探索（exploration）**和**利用（exploitation）**。

这个框架与其它优化范式有本质区别。它不同于**[确定性全局优化](@entry_id:634455)（deterministic global optimization）**方法，后者通常假设函数评估无噪声，并依赖确定性的采样规则（如空间分割），而不构建函数的后验分布或[量化不确定性](@entry_id:272064)。它也不同于经典的**多臂老虎机（multi-armed bandit, MAB）**问题。经典MAB处理的是一个有限的、离散的选项集（“臂”），并且假设各臂之间相互独立。而[贝叶斯优化](@entry_id:175791)通常在连续的高维空间中操作，其代理模型具有**空间泛化（spatial generalization）**能力，即在某点 $x$ 的一次评估会影响到其邻近点的预测，这是通过核函数实现的。此外，BO 的目标是找到[全局最优解](@entry_id:175747)（最小化**简单遗憾 (simple regret)**），而 MAB 的目标通常是最大化累积回报（最小化**累积遗憾 (cumulative regret)**）。

### 概率代理模型：高斯过程

[高斯过程](@entry_id:182192)是贝叶斯优化成功的基石。它是一种灵活的[非参数模型](@entry_id:201779)，能够对目标函数复杂的[非线性](@entry_id:637147)行为进行建模，并提供关键的[不确定性估计](@entry_id:191096)。

#### 定义先验：函数空间上的分布

一个**[高斯过程](@entry_id:182192) (Gaussian Process, GP)** 是一个[随机过程](@entry_id:268487)，其任意有限个点的集合的函数值都服从一个[联合高斯](@entry_id:636452)分布。一个 GP 完全由其**[均值函数](@entry_id:264860)** $m(x)$ 和**[协方差函数](@entry_id:265031)（或[核函数](@entry_id:145324)）** $k(x, x')$ 决定，记为 $f \sim \mathcal{GP}(m(x), k(x, x'))$。

-   **[均值函数](@entry_id:264860)** $m(x) = \mathbb{E}[f(x)]$ 代表了我们对函数 $f(x)$ 的先验平均信念。在缺乏具体领域知识的情况下，通常选择一个简单的**零[均值函数](@entry_id:264860)** $m(x)=0$。这是一种无信息的选择，让数据本身来决定函数的行为。然而，如果存在一个廉价的、近似的物理模型 $g(x)$（例如，来自[简化理论](@entry_id:1130761)的降阶模型），我们可以将其用作[均值函数](@entry_id:264860) $m(x)=g(x)$。这种方法将领域知识融入先验，使得 GP 只需学习真实函数与近似模型之间的残差 $f(x) - g(x)$，从而在数据稀疏时显著提高样本效率 。

-   **[协方差函数](@entry_id:265031)** $k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]$ 描述了函数在不同点 $x$ 和 $x'$ 之间的相关性。核函数的选择至关重要，因为它编码了我们对函数基本性质（如平滑度）的假设。
    -   一个不恰当的选择是**[白噪声](@entry_id:145248)核** $k(x,x') = \sigma_f^2 \delta(x-x')$，它假设任意不同点的函数值都不相关，这使得模型无法从数据中学习和泛化。同样，**周期核**假设函数具有周期性，这在大多数物理问题（如电池性能随电极厚度的变化）中是不现实的 。
    -   **[平方指数核](@entry_id:191141)（Squared-Exponential, SE）**，也称[径向基函数](@entry_id:754004)（RBF）核，形式为 $k(x, x') = \sigma_f^2 \exp\left(-\frac{\|x-x'\|^2}{2\ell^2}\right)$。它假设函数是无限可微的（即极其平滑），这对于某些物理系统来说可能是一个过强的假设。
    -   **Matérn 核族** 提供了一个更灵活的选择，其形式允许我们[控制函数](@entry_id:183140)的平滑度。例如，当平滑度参数 $\nu = 5/2$ 时，它假设函数是二次均方可微的，这对于模拟由 PDE 控制的物理系统通常是一个合理且稳健的假设。Matérn ($\nu=5/2$) 核的表达式为：
        $$k(\mathbf{x},\mathbf{x}')=\sigma_f^2\left(1+\frac{\sqrt{5}\,r(\mathbf{x},\mathbf{x}')}{\ell}+\frac{5\,r(\mathbf{x},\mathbf{x}')^2}{3\ell^2}\right)\exp\left(-\frac{\sqrt{5}\,r(\mathbf{x},\mathbf{x}')}{\ell}\right)$$
    -   此外，对于多维输入，不同设计变量（如孔隙度和厚度）对目标函数的影响程度可能不同。**[自动相关性确定](@entry_id:746592)（Automatic Relevance Determination, ARD）**机制通过为每个维度 $j$ 引入一个独立的**长度尺度（length-scale）** $\ell_j$ 来解决这个问题。距离度量变为 $r(\mathbf{x},\mathbf{x}')=\sqrt{\sum_{j=1}^d\left(\frac{x_j-x_j'}{\ell_j}\right)^2}$。模型可以通过数据学习到哪些维度的长度尺度较小（函数变化快，关联性强），哪些较大（函数变化慢，关联性弱），从而自动判断输入变量的“相关性”。

#### 从数据中学习：后验与超参数

给定一个 GP 先验和一个观测数据集 $\mathcal{D}_n = \{(x_i, y_i)\}_{i=1}^n$，其中观测值包含噪声 $y_i = f(x_i) + \epsilon_i, \epsilon_i \sim \mathcal{N}(0, \sigma_n^2)$，我们可以推导出任意点 $x_*$ 的[后验预测分布](@entry_id:167931)。这个后验分布仍然是高斯分布，其均值 $\mu_n(x_*)$ 和方差 $\sigma_n^2(x_*)$ 有解析解。

然而，[核函数](@entry_id:145324)自身也包含超参数 $\theta$（如信号方差 $\sigma_f^2$ 和长度尺度 $\ell_j$），同时观测噪声方差 $\sigma_n^2$ 也可能未知。这些超参数需要从数据中学习。一个标准且有原则的方法是**II 型[最大似然估计](@entry_id:142509)（Type-II Maximum Likelihood Estimation）**，即最大化观测数据的**对数[边际似然](@entry_id:636856)（log marginal likelihood）** 。

在零均值 GP 先验和高斯噪声假设下，观测向量 $\mathbf{y}$ 的[边际分布](@entry_id:264862)是一个均值为零，协方差矩阵为 $\mathbf{K} = K_{\theta}(X, X) + \sigma_n^2 I$ 的多元高斯分布。其对数边际似然为：
$$
\log p(\mathbf{y} \mid X, \theta, \sigma_n^2) = -\frac{1}{2} \mathbf{y}^{\top} \mathbf{K}^{-1} \mathbf{y} - \frac{1}{2} \log \lvert \mathbf{K} \rvert - \frac{n}{2} \log(2\pi)
$$
这个表达式的三个组成部分有着深刻的含义：

1.  **[数据拟合](@entry_id:149007)项** ($-\frac{1}{2} \mathbf{y}^{\top} \mathbf{K}^{-1} \mathbf{y}$)：这一项衡量模型对观测数据 $\mathbf{y}$ 的解释程度。当模型预测与观测数据吻合时，这一项的值会更大（更接近于零）。它奖励那些能很好拟合数据的模型。

2.  **[模型复杂度惩罚](@entry_id:752069)项** ($-\frac{1}{2} \log \lvert \mathbf{K} \rvert$)：协方差[矩阵的行列式](@entry_id:148198) $|\mathbf{K}|$ 与模型能产生的函数空间的“体积”或“复杂度”相关。一个非常灵活、能够产生剧烈变化函数的模型（例如，长度尺度很小）会有很大的 $|\mathbf{K}|$ 值。由于此项带负号，它会惩罚过于复杂的模型。这体现了**奥卡姆剃刀原理（Occam's razor）**，即在同等拟合数据的模型中，自动偏好更简单的模型，从而有效[防止过拟合](@entry_id:635166)。

3.  **[归一化常数](@entry_id:752675)** ($-\frac{n}{2} \log(2\pi)$)：这一项与超参数无关，在优化过程中可以忽略。

通过梯度上升等方法最大化对数[边际似然](@entry_id:636856)，我们可以为 GP 模型找到最适合当前数据的超参数。

### 指导搜索：[采集函数](@entry_id:168889)

有了经过数据校准的 GP 后验模型，我们便拥有了在任意点 $x$ 的预测均值 $\mu(x)$ 和不确定性（标准差） $\sigma(x)$。下一步是利用这些信息来决定在哪里进行下一次昂贵的评估。这就是[采集函数](@entry_id:168889)的任务。

#### [探索与利用的权衡](@entry_id:1124777)

[采集函数](@entry_id:168889)的核心是量化和平衡**探索（exploration）**与**利用（exploitation）**之间的权衡 。

-   **利用（Exploitation）**：在模型预测性能较好的区域进行采样，即在[后验均值](@entry_id:173826) $\mu(x)$ 高的地方进行评估。这是一种贪心策略，旨在尽快找到当前模型认为的最优解。
-   **探索（Exploration）**：在[模型不确定性](@entry_id:265539)较大的区域进行采样，即在后验标准差 $\sigma(x)$ 高的地方进行评估。这种策略旨在减少模型的不确定性，发现可能被当前模型低估的、具有潜力的未知区域。

只利用不探索，容易陷入局部最优；只探索不利用，则会浪费大量评估在本身并无潜力的区域。一个好的采集函数能够在整个优化过程中动态地平衡这两者。

此外，在某些实际问题中，函数评估的成本 $c(x)$ 并非恒定，而是依赖于输入 $x$。在这种**异构成本（heterogeneous costs）**的情况下，一个有原则的决策应考虑成本效益，即最大化每单位成本带来的预期收益。这可以通过将标准[采集函数](@entry_id:168889)值 $\alpha(x)$ 除以成本 $c(x)$ 来实现，即优化 $\alpha(x)/c(x)$ 。

#### [采集函数](@entry_id:168889)家族

存在多种[采集函数](@entry_id:168889)，它们以不同的方式来平衡[探索与利用](@entry_id:174107)。

**概率提升（Probability of Improvement, PI）**

这是最简单的采集函数之一。它计算在点 $x$ 的函数值 $f(x)$ 超过当前最佳观测值 $f^*$ 的概率。为了避免过于贪心，可以引入一个[调节参数](@entry_id:756220) $\xi \ge 0$：
$$
\text{PI}(x) = P(f(x) \ge f^* + \xi) = \Phi\left(\frac{\mu(x) - f^* - \xi}{\sigma(x)}\right)
$$
其中 $\Phi(\cdot)$ 是[标准正态分布](@entry_id:184509)的[累积分布函数](@entry_id:143135)（CDF）。参数 $\xi$ 控制着探索的程度：增加 $\xi$ 会使得 PI 更倾向于在不确定性高（$\sigma(x)$ 大）的区域进行探索 。然而，PI 只关心超越阈值的概率，不关心超越的幅度，这可能导致其表现不够理想。

**[期望提升](@entry_id:749168)（Expected Improvement, EI）**

[期望提升](@entry_id:749168)是目前最流行和广泛使用的采集函数之一。它计算在点 $x$ 的函数值相对于当前最佳值 $f^*$ 的**期望**提升量。提升量定义为 $I(x) = \max\{0, f(x) - f^*\}$。EI 则是这个量的期望：
$$
\text{EI}(x) = \mathbb{E}[I(x)] = \mathbb{E}[\max\{0, f(x) - f^*\}]
$$
在 GP 后验 $f(x) \sim \mathcal{N}(\mu(x), \sigma^2(x))$ 的假设下，EI 有一个优美的解析形式 。通过对高斯分布进行积分，我们可以推导出：
$$
\text{EI}(x) = (\mu(x) - f^*) \Phi(z) + \sigma(x) \phi(z), \quad \text{其中} \quad z = \frac{\mu(x) - f^*}{\sigma(x)}
$$
这里 $\phi(\cdot)$ 是[标准正态分布](@entry_id:184509)的概率密度函数（PDF）。这个表达式直观地体现了[探索与利用](@entry_id:174107)的平衡：第一项 $(\mu(x) - f^*) \Phi(z)$ 在 $\mu(x)$ 远大于 $f^*$ 时占主导，体现了**利用**；第二项 $\sigma(x) \phi(z)$ 在不确定性 $\sigma(x)$ 较大时占主导，体现了**探索**。

**上置信界（Upper Confidence Bound, UCB）**

UCB 采用了一种基于“乐观主义”的策略。它通过在后验均值上增加一定倍数的后验标准差，来构造一个置信区间的[上界](@entry_id:274738)：
$$
\text{UCB}(x) = \mu(x) + \beta^{1/2} \sigma(x)
$$
选择下一个评估点就是最大化这个“乐观”的估计。参数 $\beta \ge 0$ 直接控制了[探索与利用的权衡](@entry_id:1124777)：$\beta$ 越大，对不确定性项 $\sigma(x)$ 的权重就越大，算法就越倾向于**探索**；反之，当 $\beta \to 0$ 时，UCB 就退化为纯粹的**利用** 。

### 完整的流程：从理论到实践

将上述原理组合起来，我们便得到了一个功能完备的贝叶斯优化算法。

#### 初始化：构建初始知识基础

贝叶斯优化始于一个初始数据集。这个初始数据集的质量对后续优化的效率有显著影响。如果初始点过于集中，GP 模型可能无法捕捉到目标函数的全局特征，导致优化陷入局部区域。因此，理想的初始设计应该是**空间填充（space-filling）**的，即点在设计空间中分布得尽可能均匀。

一个好的[空间填充设计](@entry_id:755078)能够最小化**填充距离（fill distance）** $h_{\mathcal{X},\mathcal{D}}$，即设计空间 $\mathcal{D}$ 中任意一点到最近的采样点集 $\mathcal{X}$ 的最大距离。理论分析表明，对于常用的 Matérn 核，GP 后验不确定性的最坏情况（即 $\sup_{x \in \mathcal{D}} \sigma(x)$）由填充距离控制。具体来说，最坏情况下的后验标准差满足一个[上界](@entry_id:274738)，该[上界](@entry_id:274738)与 $h_{\mathcal{X},\mathcal{D}}^{\nu}$ 成正比（其中 $\nu$ 是核的平滑度参数）。因此，通过最小化填充距离，[空间填充设计](@entry_id:755078)（如**[拉丁超立方抽样](@entry_id:751167) (Latin Hypercube Sampling, LHS)**）能够最有效地降低全局范围内的初始不确定性，为后续的探索和利用提供一个更可靠的起点。

#### 内循环：最大化采集函数

在[贝叶斯优化](@entry_id:175791)的每一次迭代中，核心计算任务是求解以下优化问题，这被称为**内[循环优化](@entry_id:751480)（inner optimization）**：
$$
x_{t+1} = \arg\max_{x \in \mathcal{X}} \alpha_t(x)
$$
与昂贵的[目标函数](@entry_id:267263) $f(x)$ 不同，采集函数 $\alpha_t(x)$ 的评估成本非常低廉。这个问题的解决效率直接影响到整个 BO 流程的速度 。

在连续设计空间中，如果 GP 使用的核函数 $k(x,x')$（如 Matérn 或 SE 核）对输入 $x$ 是可微的，那么后验均值 $\mu_t(x)$ 和方差 $\sigma_t^2(x)$ 也是可微的（除已采样点外）。因此，大多数采集函数（如 EI 和 UCB）也都是可微的。这意味着我们可以计算采集函数的梯度 $\nabla_x \alpha_t(x)$，例如通过自动微分技术。

梯度的可用性使得我们可以采用高效的**[基于梯度的优化](@entry_id:169228)算法**（如 [L-BFGS](@entry_id:167263)-B, SLSQP）来求解这个内循环问题。这比[网格搜索](@entry_id:636526)或[随机搜索](@entry_id:637353)等无梯度方法在维度增加时具有更好的扩展性。然而，[采集函数](@entry_id:168889)通常是**非凸（non-convex）**的，存在多个[局部极大值](@entry_id:137813)。为了缓解这个问题，通常采用**多起点（multi-start）**策略，即从多个不同的初始点开始进行梯度上升，然后取所有结果中的最[优值](@entry_id:1124939)作为 $x_{t+1}$。

#### 一个完整的[约束优化](@entry_id:635027)算法

最后，我们可以勾勒出应用于实际工程问题（如带约束的[电池设计](@entry_id:1121392)）的完整[贝叶斯优化](@entry_id:175791)流程 ：

1.  **初始化**：使用[空间填充设计](@entry_id:755078)（如 LHS）生成一个小的初始[设计点](@entry_id:748327)集 $X_0 = \{x_i\}_{i=1}^{n_0}$。在这些点上运行昂贵的模拟器，获得[目标函数](@entry_id:267263)观测值 $\{y_i\}$ 和约束函数观测值 $\{c_j(x_i)\}$。

2.  **建模**：
    *   为[目标函数](@entry_id:267263) $f(x)$ 拟合一个 GP 模型。
    *   为每一个黑箱约束函数 $c_j(x)$ 拟合一个独立的 GP 模型。
    *   通过最大化各自的对数[边际似然](@entry_id:636856)，学习所有 GP 模型的超参数。

3.  **迭代优化**（对于 $t = n_0, n_0+1, \dots, T-1$）：
    *   **选择**：基于所有 GP 模型的[后验分布](@entry_id:145605)，构建一个**约束[采集函数](@entry_id:168889)**。例如，可以将[期望提升](@entry_id:749168)（EI）与满足所有约束的概率 $P(\text{feasible at } x)$ 相乘，得到 $\alpha_c(x) = \text{EI}(x) \times P(\text{feasible at } x)$。
    *   **求解内循环**：在整个设计空间 $\mathcal{X}$ 中全局最大化约束采集函数 $\alpha_c(x)$，找到下一个最有希望的评估点 $x_{t+1}$。
    *   **评估**：运行昂贵的模拟器，在 $x_{t+1}$ 处获得新的观测值 $(y_{t+1}, \{c_j(x_{t+1})\})$.
    *   **更新**：将新的数据点 $(x_{t+1}, y_{t+1}, \{c_j(x_{t+1})\})$ 加入数据集，并更新所有 GP 模型的后验分布。

4.  **终止**：当评估次数达到预设的总预算 $T$ 时，停止迭代。

5.  **推荐**：从所有已评估且满足约束的点中，选择观测到的[目标函数](@entry_id:267263)值最高的那一点作为最终的最优设计方案 $x^\star$。

通过这一系统性的流程，[贝叶斯优化](@entry_id:175791)能够在严苛的计算预算下，高效地探索复杂的设计空间，最终找到满足所有工程要求的、性能最优的设计方案。