{
    "hands_on_practices": [
        {
            "introduction": "The Bayesian optimization process must begin with an initial set of experiments to build the first surrogate model. This exercise introduces Latin Hypercube Design (LHD), a powerful technique for creating a space-filling initial sample that explores the design space broadly without clustering. By constructing an LHD from scratch and evaluating its quality, you will gain practical insight into this crucial first step of automated experimental design .",
            "id": "3896144",
            "problem": "You are designing an initial experimental plan for automated battery electrode optimization where the objective function evaluations are expensive. In Bayesian optimization for expensive objective functions, it is standard practice to begin with a space-filling design to reduce posterior uncertainty under a Gaussian Process (GP) surrogate and to cover the domain broadly before sequential improvement. A proven choice is a Latin Hypercube Design (LHD). Consider two design variables: electrode porosity and active material particle diameter. You must construct a $5$-point Latin hypercube design over these two variables within given bounds, using a deterministic scheme for point placement inside each stratum, and then compute the minimum pairwise Euclidean distance between the $5$ points in the normalized design space.\n\nFundamental base:\n- A Latin Hypercube Design (LHD) is a stratified sampling scheme over $m$ points that partitions each variable’s range into $m$ equal-probability strata and places exactly one sample in each stratum per variable; equivalently, it uses one-to-one assignments via permutations to avoid collisions along any coordinate.\n- Normalization of each variable to the unit interval enforces comparability across heterogeneous units and is defined by $$z = \\frac{x - L}{U - L},$$ where $x$ is the physical variable, $L$ is the lower bound, and $U$ is the upper bound.\n- The Euclidean distance between points $\\mathbf{a} = (a_1,a_2)$ and $\\mathbf{b} = (b_1,b_2)$ in a metric space is $$\\lVert \\mathbf{a} - \\mathbf{b} \\rVert_2 = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2}.$$\n\nDesign specification:\n- Let $m = 5$ be the number of points.\n- For each variable, partition the normalized interval $[0,1]$ into $m$ equal strata. Use the $m$ midpoints $$c_j = \\frac{j - \\tfrac{1}{2}}{m},\\;\\; j \\in \\{1,2,3,4,5\\}.$$ \n- Construct the $5$ points by providing two permutations $\\pi_{\\phi}$ and $\\pi_{d}$ over $\\{1,2,3,4,5\\}$ for porosity and diameter, respectively. For $i \\in \\{1,\\dots,5\\}$, the normalized coordinates are $$z_i = c_{\\pi_{\\phi}[i]},\\quad w_i = c_{\\pi_{d}[i]}.$$ \n- Map normalized points $(z_i,w_i)$ to physical coordinates $(\\phi_i,d_{p,i})$ using the provided bounds, although the pairwise distances must be computed in the normalized space to ensure dimensionless, unit-consistent results suitable for Gaussian Process modeling.\n\nCompute:\n- Compute all pairwise Euclidean distances between the $5$ normalized points and report the minimum value. Express the final distances as dimensionless floats rounded to $6$ decimal places.\n\nTest suite:\nProvide the following three test cases. For each case, construct the LHD using the midpoints and permutations as described, then compute the minimum pairwise Euclidean distance in the normalized space.\n- Case $1$ (typical electrode-scale ranges):\n  - Porosity bounds: $\\phi \\in [\\phi_{\\min},\\phi_{\\max}] = [0.25,0.45]$.\n  - Particle diameter bounds: $d_p \\in [d_{\\min},d_{\\max}] = [5,15]$ micrometers.\n  - Permutations: $\\pi_{\\phi} = (1,2,3,4,5)$, $\\pi_{d} = (5,4,3,2,1)$.\n- Case $2$ (narrow porosity, micro-scale diameter in meters):\n  - Porosity bounds: $\\phi \\in [\\phi_{\\min},\\phi_{\\max}] = [0.30,0.40]$.\n  - Particle diameter bounds: $d_p \\in [d_{\\min},d_{\\max}] = [2\\times 10^{-6},12\\times 10^{-6}]$ meters.\n  - Permutations: $\\pi_{\\phi} = (2,5,1,4,3)$, $\\pi_{d} = (1,3,5,2,4)$.\n- Case $3$ (moderate porosity spread, micrometer diameter):\n  - Porosity bounds: $\\phi \\in [\\phi_{\\min},\\phi_{\\max}] = [0.32,0.50]$.\n  - Particle diameter bounds: $d_p \\in [d_{\\min},d_{\\max}] = [6,10]$ micrometers.\n  - Permutations: $\\pi_{\\phi} = (5,1,3,2,4)$, $\\pi_{d} = (1,5,2,4,3)$.\n\nFinal output format:\nYour program should produce a single line of output containing the minimum pairwise Euclidean distances for the three cases as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example, $$[0.123456,0.234567,0.345678].$$",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of experimental design, specifically Latin Hypercube Sampling (LHS), which is a standard technique for exploring parameter spaces of expensive functions, such as those encountered in computational materials science and battery simulation. The problem is well-posed, providing a deterministic and fully-specified procedure for constructing the sample points and a clear objective for calculation. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities. The problem is objective and formalizable.\n\nThe task is to construct a $5$-point Latin Hypercube Design (LHD) for two variables in a normalized unit square, $[0,1] \\times [0,1]$, and to compute the minimum pairwise Euclidean distance between these points. This procedure is repeated for three distinct test cases, each defined by a different pair of permutations.\n\nFirst, we establish the foundational components of the design. The number of points, and thus the number of strata for each variable, is $m=5$. The design space is the normalized unit hypercube. The problem specifies a deterministic placement of points at the center of each stratum. For a variable in the normalized interval $[0,1]$, the $m$ strata are $[0, 1/m), [1/m, 2/m), \\ldots, [(m-1)/m, 1]$. The midpoints $c_j$ of these strata are given by the formula:\n$$c_j = \\frac{j - \\frac{1}{2}}{m}, \\quad j \\in \\{1, 2, 3, 4, 5\\}$$\nFor $m=5$, the set of midpoints is:\n$c_1 = \\frac{1 - 0.5}{5} = \\frac{0.5}{5} = 0.1$\n$c_2 = \\frac{2 - 0.5}{5} = \\frac{1.5}{5} = 0.3$\n$c_3 = \\frac{3 - 0.5}{5} = \\frac{2.5}{5} = 0.5$\n$c_4 = \\frac{4 - 0.5}{5} = \\frac{3.5}{5} = 0.7$\n$c_5 = \\frac{5 - 0.5}{5} = \\frac{4.5}{5} = 0.9$\n\nAn LHD is formed by creating $m$ points, ensuring that for each variable, each of the $m$ strata is sampled exactly once. This is achieved by using permutations. For our two-dimensional problem with variables porosity ($\\phi$) and particle diameter ($d_p$), we are given two permutations, $\\pi_{\\phi}$ and $\\pi_{d}$, each being a permutation of the set $\\{1, 2, 3, 4, 5\\}$. The $i$-th point of the LHD, for $i \\in \\{1, \\dots, 5\\}$, has normalized coordinates $(z_i, w_i)$ given by:\n$$z_i = c_{\\pi_{\\phi}[i]}, \\quad w_i = c_{\\pi_{d}[i]}$$\nHere, $\\pi_{\\phi}[i]$ and $\\pi_{d}[i]$ refer to the $i$-th elements of the given permutation tuples.\n\nThe distance between any two points, $\\mathbf{p}_i = (z_i, w_i)$ and $\\mathbf{p}_k = (z_k, w_k)$, is the Euclidean distance, calculated as:\n$$d(\\mathbf{p}_i, \\mathbf{p}_k) = \\sqrt{(z_i - z_k)^2 + (w_i - w_k)^2}$$\nFor $m=5$ points, there are $\\binom{5}{2} = 10$ unique pairwise distances to compute. The final objective is to find the minimum of these $10$ distances. The physical bounds on porosity and particle diameter are contextual and not required for the calculation, as it is performed in the normalized space.\n\nWe now proceed with the analysis for each test case.\n\n**Case 1:**\n- Permutations: $\\pi_{\\phi} = (1,2,3,4,5)$, $\\pi_{d} = (5,4,3,2,1)$.\nThe $5$ normalized points $\\mathbf{p}_i = (z_i, w_i)$ are:\n- $\\mathbf{p}_1$: indices $(1,5) \\rightarrow (c_1, c_5) = (0.1, 0.9)$\n- $\\mathbf{p}_2$: indices $(2,4) \\rightarrow (c_2, c_4) = (0.3, 0.7)$\n- $\\mathbf{p}_3$: indices $(3,3) \\rightarrow (c_3, c_3) = (0.5, 0.5)$\n- $\\mathbf{p}_4$: indices $(4,2) \\rightarrow (c_4, c_2) = (0.7, 0.3)$\n- $\\mathbf{p}_5$: indices $(5,1) \\rightarrow (c_5, c_1) = (0.9, 0.1)$\nWe compute the $10$ pairwise distances. As an example, the distance between $\\mathbf{p}_1$ and $\\mathbf{p}_2$ is:\n$$d(\\mathbf{p}_1, \\mathbf{p}_2) = \\sqrt{(0.1 - 0.3)^2 + (0.9 - 0.7)^2} = \\sqrt{(-0.2)^2 + (0.2)^2} = \\sqrt{0.04 + 0.04} = \\sqrt{0.08}$$\nAll adjacent points in this sequence (e.g., $(\\mathbf{p}_1, \\mathbf{p}_2)$, $(\\mathbf{p}_2, \\mathbf{p}_3)$, etc.) have this same distance. Distances between non-adjacent points are larger. For example, $d(\\mathbf{p}_1, \\mathbf{p}_3) = \\sqrt{(0.1 - 0.5)^2 + (0.9 - 0.5)^2} = \\sqrt{0.16 + 0.16} = \\sqrt{0.32}$.\nThe minimum distance is $\\sqrt{0.08} \\approx 0.2828427$.\n\n**Case 2:**\n- Permutations: $\\pi_{\\phi} = (2,5,1,4,3)$, $\\pi_{d} = (1,3,5,2,4)$.\nThe $5$ normalized points are:\n- $\\mathbf{p}_1$: indices $(2,1) \\rightarrow (c_2, c_1) = (0.3, 0.1)$\n- $\\mathbf{p}_2$: indices $(5,3) \\rightarrow (c_5, c_3) = (0.9, 0.5)$\n- $\\mathbf{p}_3$: indices $(1,5) \\rightarrow (c_1, c_5) = (0.1, 0.9)$\n- $\\mathbf{p}_4$: indices $(4,2) \\rightarrow (c_4, c_2) = (0.7, 0.3)$\n- $\\mathbf{p}_5$: indices $(3,4) \\rightarrow (c_3, c_4) = (0.5, 0.7)$\nWe compute the $10$ pairwise distances. Let's inspect the distance between $\\mathbf{p}_2$ and $\\mathbf{p}_4$:\n$$d(\\mathbf{p}_2, \\mathbf{p}_4) = \\sqrt{(0.9 - 0.7)^2 + (0.5 - 0.3)^2} = \\sqrt{(0.2)^2 + (0.2)^2} = \\sqrt{0.04 + 0.04} = \\sqrt{0.08}$$\nA systematic search of all $10$ pairs reveals that this is the minimum distance. For example, $d(\\mathbf{p}_1, \\mathbf{p}_4) = \\sqrt{(0.3 - 0.7)^2 + (0.1 - 0.3)^2} = \\sqrt{0.16 + 0.04} = \\sqrt{0.20}$.\nThe minimum distance is $\\sqrt{0.08} \\approx 0.2828427$.\n\n**Case 3:**\n- Permutations: $\\pi_{\\phi} = (5,1,3,2,4)$, $\\pi_{d} = (1,5,2,4,3)$.\nThe $5$ normalized points are:\n- $\\mathbf{p}_1$: indices $(5,1) \\rightarrow (c_5, c_1) = (0.9, 0.1)$\n- $\\mathbf{p}_2$: indices $(1,5) \\rightarrow (c_1, c_5) = (0.1, 0.9)$\n- $\\mathbf{p}_3$: indices $(3,2) \\rightarrow (c_3, c_2) = (0.5, 0.3)$\n- $\\mathbf{p}_4$: indices $(2,4) \\rightarrow (c_2, c_4) = (0.3, 0.7)$\n- $\\mathbf{p}_5$: indices $(4,3) \\rightarrow (c_4, c_3) = (0.7, 0.5)$\nWe compute the $10$ pairwise distances. Consider the distance between $\\mathbf{p}_2$ and $\\mathbf{p}_4$:\n$$d(\\mathbf{p}_2, \\mathbf{p}_4) = \\sqrt{(0.1 - 0.3)^2 + (0.9 - 0.7)^2} = \\sqrt{(-0.2)^2 + (0.2)^2} = \\sqrt{0.04 + 0.04} = \\sqrt{0.08}$$\nAlso, consider the distance between $\\mathbf{p}_3$ and $\\mathbf{p}_5$:\n$$d(\\mathbf{p}_3, \\mathbf{p}_5) = \\sqrt{(0.5 - 0.7)^2 + (0.3 - 0.5)^2} = \\sqrt{(-0.2)^2 + (-0.2)^2} = \\sqrt{0.04 + 0.04} = \\sqrt{0.08}$$\nA full comparison confirms this is the minimum distance.\nThe minimum distance is $\\sqrt{0.08} \\approx 0.2828427$.\n\nIn all three cases, the minimum pairwise Euclidean distance is $\\sqrt{0.08}$. The squared distance between any two points $\\mathbf{p}_i$ and $\\mathbf{p}_k$ is $d^2 = ( (k_{\\phi}/m)^2 + (k_{d}/m)^2 )$, where $k_{\\phi} = \\pi_{\\phi}[i] - \\pi_{\\phi}[k]$ and $k_{d} = \\pi_{d}[i] - \\pi_{d}[k]$ are non-zero integers. The minimum possible squared distance is achieved when $|k_{\\phi}|=1$ and $|k_{d}|=1$, yielding $d^2 = (1^2 + 1^2)/5^2 = 2/25 = 0.08$. Each of the three designs contains at least one pair of points that satisfies this condition.\n\nThe final results, rounded to $6$ decimal places, are identical for all three cases.\nMinimum distance for each case: $0.282843$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Constructs 5-point Latin Hypercube Designs for three test cases,\n    and computes the minimum pairwise Euclidean distance between the\n    points in the normalized [0,1]x[0,1] space for each case.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of two permutation tuples (one for each dimension).\n    test_cases = [\n        # Case 1\n        ((1, 2, 3, 4, 5), (5, 4, 3, 2, 1)),\n        # Case 2\n        ((2, 5, 1, 4, 3), (1, 3, 5, 2, 4)),\n        # Case 3\n        ((5, 1, 3, 2, 4), (1, 5, 2, 4, 3)),\n    ]\n\n    results = []\n    m = 5  # Number of points and strata\n\n    # Calculate the m strata midpoints in the normalized interval [0,1].\n    # The formula is c_j = (j - 0.5) / m for j in {1, ..., m}.\n    # np.arange(1, m + 1) generates the values for j.\n    midpoints = (np.arange(1, m + 1) - 0.5) / m\n\n    # Process each test case\n    for pi_phi, pi_d in test_cases:\n        # Generate the m points for the current LHD.\n        # The points are stored in a numpy array of shape (m, 2).\n        points = np.zeros((m, 2))\n        for i in range(m):\n            # The i-th point (0-indexed) is constructed from the i-th elements\n            # of the permutation tuples.\n            # The values in the permutation tuples are 1-based indices, so we\n            # subtract 1 to get the 0-based index for the midpoints array.\n            idx_phi = pi_phi[i] - 1\n            idx_d = pi_d[i] - 1\n            points[i, 0] = midpoints[idx_phi]\n            points[i, 1] = midpoints[idx_d]\n\n        # Calculate all pairwise distances and find the minimum.\n        # It is computationally more efficient to find the minimum squared\n        # distance first and then take the square root once at the end.\n        min_dist_sq = float('inf')\n\n        # Use itertools.combinations to generate all unique pairs of points.\n        for p1_idx, p2_idx in combinations(range(m), 2):\n            point1 = points[p1_idx]\n            point2 = points[p2_idx]\n            \n            # Calculate the squared Euclidean distance\n            dist_sq = np.sum((point1 - point2)**2)\n            \n            # Update the minimum squared distance if the current one is smaller\n            if dist_sq < min_dist_sq:\n                min_dist_sq = dist_sq\n        \n        # Calculate the minimum distance by taking the square root.\n        min_dist = np.sqrt(min_dist_sq)\n        results.append(min_dist)\n\n    # Final print statement in the exact required format.\n    # The list contains the minimum distances for the three cases,\n    # each formatted as a float rounded to six decimal places.\n    print(f\"[{','.join(f'{d:.6f}' for d in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "At the heart of Bayesian optimization lies the surrogate model's ability to learn from data, even when observations are corrupted by experimental noise. This practice focuses on the fundamental mechanism of this learning process: the Bayesian update of the model's beliefs. You will apply the principles of the Normal-Normal conjugate model to compute the posterior distribution of a battery's latent cycle life, providing a clear illustration of how a Gaussian Process quantifies and reduces its uncertainty as new data arrives .",
            "id": "3896095",
            "problem": "In automated battery design and simulation, the cycle life of a candidate design point $x$ is evaluated by running expensive charge-discharge experiments that yield noisy replicates. To guide Bayesian optimization under experimental noise, consider a single design $x$ where the latent cycle life $f(x)$ is modeled as a Gaussian Process (GP), which at a fixed $x$ induces a Normal prior $f(x) \\sim \\mathcal{N}(m_{0}, \\sigma_{f}^{2})$. Each replicate measurement is modeled as $y_{i} = f(x) + \\varepsilon_{i}$ with independent, identically distributed $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{n}^{2})$. This setup corresponds to a Normal likelihood with known noise variance and a Normal prior.\n\nSuppose a physics-informed surrogate provides prior mean $m_{0} = 950$ cycles and prior variance $\\sigma_{f}^{2} = 1000$ cycles$^{2}$ at this $x$. A metrology study estimates the observation noise variance as $\\sigma_{n}^{2} = 1000$ cycles$^{2}$. You then run $n=3$ replicates at $x$, obtaining observed cycle lives $y_{1} = 972$ cycles, $y_{2} = 961$ cycles, and $y_{3} = 986$ cycles.\n\nUsing only the foundational definitions of Bayes’ theorem and the Normal-Normal conjugate model, do the following:\n\n1. Derive the posterior predictive distribution for a future noisy measurement $y_{*}$ at the same design $x$, that is, compute the posterior predictive mean and variance at $x$ based on the three observed replicates and the given prior and noise parameters.\n\n2. Determine the recommended number of additional replicates $k$ at the same design $x$ needed so that the posterior variance of the latent $f(x)$ is halved relative to its current value after $n=3$ replicates. Treat $k$ as the minimal nonnegative integer that achieves this halving.\n\nExpress the posterior predictive mean in cycles and the posterior predictive variance in cycles$^{2}$, both rounded to four significant figures. Report $k$ as an integer. Provide your final answer as three entries in a single row matrix in the following order: posterior predictive mean, posterior predictive variance, and $k$.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Prior distribution for the latent cycle life $f(x)$: $f(x) \\sim \\mathcal{N}(m_{0}, \\sigma_{f}^{2})$.\n-   Prior mean: $m_{0} = 950$ cycles.\n-   Prior variance: $\\sigma_{f}^{2} = 1000$ cycles$^{2}$.\n-   Likelihood model for replicates: $y_{i} = f(x) + \\varepsilon_{i}$.\n-   Observation noise distribution: $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{n}^{2})$.\n-   Observation noise variance: $\\sigma_{n}^{2} = 1000$ cycles$^{2}$.\n-   Number of observed replicates: $n=3$.\n-   Observed data: $y_{1} = 972$, $y_{2} = 961$, $y_{3} = 986$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing the standard and widely accepted Bayesian framework of a Normal-Normal conjugate model for inference, which is fundamental to Gaussian Process regression. The context—automated battery design using expensive simulations—is a realistic application of Bayesian optimization. The problem is well-posed, providing all necessary parameters and data to uniquely determine the required quantities. The language is objective and precise. There are no contradictions, missing information, or pseudoscientific claims. The numbers provided are physically plausible for battery cycle life.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n---\n\nThe solution is divided into two parts as requested by the problem statement.\n\n### Part 1: Posterior Predictive Distribution\n\nThe goal is to find the posterior predictive distribution for a future noisy measurement $y_{*}$, which is $p(y_{*} | D)$, where $D = \\{y_1, y_2, y_3\\}$. This requires first finding the posterior distribution of the latent cycle life $f(x)$ given the data $D$, which is $p(f(x)|D)$.\n\n**1. Posterior Distribution of $f(x)$**\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the prior distribution and the likelihood function:\n$$ p(f(x)|D) \\propto p(D|f(x)) \\, p(f(x)) $$\nThe prior is given as a Normal distribution:\n$$ p(f(x)) = \\mathcal{N}(f(x) | m_0, \\sigma_f^2) \\propto \\exp\\left(-\\frac{1}{2}\\frac{(f(x) - m_0)^2}{\\sigma_f^2}\\right) $$\nThe likelihood of the $n$ independent and identically distributed observations $y_i \\sim \\mathcal{N}(f(x), \\sigma_n^2)$ is:\n$$ p(D|f(x)) = \\prod_{i=1}^n \\mathcal{N}(y_i | f(x), \\sigma_n^2) \\propto \\exp\\left(-\\frac{1}{2\\sigma_n^2} \\sum_{i=1}^n (y_i - f(x))^2\\right) $$\nThe sum in the exponent can be centered around the sample mean $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$:\n$$ \\sum_{i=1}^n (y_i - f(x))^2 = \\sum_{i=1}^n (y_i - \\bar{y} + \\bar{y} - f(x))^2 = \\sum_{i=1}^n(y_i - \\bar{y})^2 + n(\\bar{y} - f(x))^2 $$\nThe term $\\sum_{i=1}^n(y_i - \\bar{y})^2$ does not depend on $f(x)$ and can be absorbed into the normalization constant. Thus, the effective likelihood can be expressed in terms of the sufficient statistic $\\bar{y}$:\n$$ p(D|f(x)) \\propto \\exp\\left(-\\frac{n}{2\\sigma_n^2}(\\bar{y} - f(x))^2\\right) $$\nThis shows that the likelihood of the sample mean is $p(\\bar{y}|f(x)) = \\mathcal{N}(\\bar{y} | f(x), \\sigma_n^2/n)$.\n\nCombining the prior and likelihood, the exponent of the posterior $p(f(x)|D)$ is, up to an additive constant:\n$$ -\\frac{1}{2}\\left( \\frac{(f(x) - m_0)^2}{\\sigma_f^2} + \\frac{(f(x) - \\bar{y})^2}{\\sigma_n^2/n} \\right) $$\nExpanding the squares and collecting terms in $f(x)^2$ and $f(x)$:\n$$ -\\frac{1}{2}\\left( f(x)^2\\left(\\frac{1}{\\sigma_f^2} + \\frac{n}{\\sigma_n^2}\\right) - 2f(x)\\left(\\frac{m_0}{\\sigma_f^2} + \\frac{n\\bar{y}}{\\sigma_n^2}\\right) + \\text{const} \\right) $$\nThis is the exponent of a Normal distribution for $f(x)$, $\\mathcal{N}(f(x)|m_n, \\sigma_{f,n}^2)$, where the posterior inverse variance (precision) $\\frac{1}{\\sigma_{f,n}^2}$ and posterior mean $m_n$ are identified by completing the square:\n$$ \\frac{1}{\\sigma_{f,n}^2} = \\frac{1}{\\sigma_f^2} + \\frac{n}{\\sigma_n^2} $$\n$$ m_n = \\sigma_{f,n}^2 \\left( \\frac{m_0}{\\sigma_f^2} + \\frac{n\\bar{y}}{\\sigma_n^2} \\right) $$\nFirst, we compute the sample mean $\\bar{y}$ from the data:\n$$ \\bar{y} = \\frac{y_1 + y_2 + y_3}{n} = \\frac{972 + 961 + 986}{3} = \\frac{2919}{3} = 973 $$\nNow, we substitute the given values: $m_0=950$, $\\sigma_f^2=1000$, $\\sigma_n^2=1000$, $n=3$, $\\bar{y}=973$.\nThe posterior variance $\\sigma_{f,n}^2$ is:\n$$ \\frac{1}{\\sigma_{f,n}^2} = \\frac{1}{1000} + \\frac{3}{1000} = \\frac{4}{1000} = \\frac{1}{250} \\implies \\sigma_{f,n}^2 = 250 $$\nThe posterior mean $m_n$ is:\n$$ m_n = 250 \\left( \\frac{950}{1000} + \\frac{3 \\times 973}{1000} \\right) = \\frac{250}{1000} (950 + 2919) = \\frac{1}{4}(3869) = 967.25 $$\nSo, the posterior distribution of the latent function is $f(x)|D \\sim \\mathcal{N}(967.25, 250)$.\n\n**2. Posterior Predictive Distribution of $y_{*}$**\n\nThe posterior predictive distribution is the distribution of a future observation $y_{*}$ given the past data $D$. It is obtained by marginalizing the likelihood of $y_{*}$ over the posterior distribution of $f(x)$:\n$$ p(y_{*}|D) = \\int p(y_{*}|f(x)) \\, p(f(x)|D) \\, df(x) $$\nWe have $y_{*} = f(x) + \\varepsilon_{*}$ where $f(x) \\sim \\mathcal{N}(m_n, \\sigma_{f,n}^2)$ and $\\varepsilon_{*} \\sim \\mathcal{N}(0, \\sigma_n^2)$. Since $y_{*}$ is the sum of two independent normally distributed random variables, it is also normally distributed.\nThe mean of the posterior predictive distribution is:\n$$ E[y_{*}|D] = E[f(x) + \\varepsilon_{*}|D] = E[f(x)|D] + E[\\varepsilon_{*}] = m_n + 0 = m_n $$\nThe variance of the posterior predictive distribution is:\n$$ \\text{Var}(y_{*}|D) = \\text{Var}(f(x) + \\varepsilon_{*}|D) = \\text{Var}(f(x)|D) + \\text{Var}(\\varepsilon_{*}) = \\sigma_{f,n}^2 + \\sigma_n^2 $$\nSubstituting the computed and given values:\n$$ E[y_{*}|D] = 967.25 $$\n$$ \\text{Var}(y_{*}|D) = 250 + 1000 = 1250 $$\nRounding to four significant figures as requested:\n-   Posterior predictive mean: $967.3$ cycles.\n-   Posterior predictive variance: $1250$ cycles$^{2}$.\n\n### Part 2: Required Number of Additional Replicates\n\nThe task is to find the minimal non-negative integer of additional replicates, $k$, required to halve the posterior variance of the latent function $f(x)$.\n\nThe current posterior variance with $n=3$ replicates is $\\sigma_{f,3}^2 = 250$.\nThe target posterior variance is $\\sigma_{f, \\text{target}}^2 = \\frac{1}{2} \\sigma_{f,3}^2 = \\frac{1}{2}(250) = 125$.\n\nLet $N$ be the total number of replicates. The posterior variance after $N$ replicates, $\\sigma_{f,N}^2$, is given by the formula:\n$$ \\frac{1}{\\sigma_{f,N}^2} = \\frac{1}{\\sigma_f^2} + \\frac{N}{\\sigma_n^2} $$\nSubstituting the known variances:\n$$ \\frac{1}{\\sigma_{f,N}^2} = \\frac{1}{1000} + \\frac{N}{1000} = \\frac{1+N}{1000} $$\nThis gives the posterior variance as a function of $N$:\n$$ \\sigma_{f,N}^2 = \\frac{1000}{1+N} $$\nWe need to find the smallest integer $N$ such that the posterior variance is less than or equal to the target variance of $125$.\n$$ \\sigma_{f,N}^2 \\le 125 $$\n$$ \\frac{1000}{1+N} \\le 125 $$\n$$ 1000 \\le 125(1+N) $$\n$$ \\frac{1000}{125} \\le 1+N $$\n$$ 8 \\le 1+N $$\n$$ N \\ge 7 $$\nThe minimum total number of replicates required is $N=7$.\nSince we have already performed $n=3$ replicates, the number of additional replicates $k$ is:\n$$ k = N - n = 7 - 3 = 4 $$\nThe minimal non-negative integer of additional replicates required is $k=4$.\n\nThe final answer is a row matrix containing the posterior predictive mean, posterior predictive variance, and $k$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n967.3 & 1250 & 4\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With an updated surrogate model providing predictions and uncertainties, an acquisition function is needed to intelligently decide where to sample next. This exercise explores how two different acquisition functions, Probability of Improvement (PI) and Upper Confidence Bound (UCB), balance the trade-off between exploiting promising regions and exploring uncertain ones. By calculating these values for several candidate materials, you will see firsthand how different optimization strategies can prioritize different points in the search space .",
            "id": "3896133",
            "problem": "A laboratory automates the selection of electrolyte salts for a lithium-ion battery formulation by optimizing a dimensionless performance objective, normalized to the interval $[0,1]$, that combines simulated ionic conductivity and electrochemical stability. The performance is modeled with a Gaussian Process (GP) surrogate; for any candidate salt at input $x$, the GP posterior predictive distribution for the unknown objective $f(x)$ is Gaussian with mean $\\mu(x)$ and variance $\\sigma^{2}(x)$. The aim is to maximize $f(x)$.\n\nYou are given three candidate salts $\\{S_{1},S_{2},S_{3}\\}$ with GP posterior summaries computed from a set of expensive multiphysics simulations at other design points. The best observed objective value so far is $f^{*}=0.95$. An improvement margin $\\xi=0.01$ is used. The Upper Confidence Bound (UCB) exploration parameter is $\\beta=4$. For the three candidates, the GP posterior predictive means and variances are:\n- $S_{1}$: $\\mu_{1}=0.955$, $\\sigma_{1}^{2}=1.0\\times 10^{-4}$,\n- $S_{2}$: $\\mu_{2}=0.85$, $\\sigma_{2}^{2}=1.44\\times 10^{-2}$,\n- $S_{3}$: $\\mu_{3}=0.95$, $\\sigma_{3}^{2}=4.0\\times 10^{-4}$.\n\nUse the following fundamental bases:\n- The GP predictive model implies $f(x)\\mid\\mathcal{D}\\sim\\mathcal{N}(\\mu(x),\\sigma^{2}(x))$ for a fixed $x$ and dataset $\\mathcal{D}$.\n- The Probability of Improvement (PI) acquisition for maximization at $x$ is the probability that the improvement random variable $I(x)=\\max\\{0, f(x)-(f^{*}+\\xi)\\}$ is strictly positive under the predictive distribution.\n- The Upper Confidence Bound (UCB) acquisition for maximization at $x$ is $\\mu(x)+\\sqrt{\\beta}\\,\\sigma(x)$, which arises from concentration inequalities for sub-Gaussian processes.\n\nTasks:\n1) Derive from first principles the analytical expression for the Probability of Improvement at each candidate $S_{i}$ in terms of $\\mu_{i}$, $\\sigma_{i}$, $f^{*}$, and $\\xi$, starting from the definition of $I(x)$ and the Gaussian predictive model. Evaluate the resulting expressions numerically for $S_{1}$, $S_{2}$, and $S_{3}$.\n2) Using the definition of the Upper Confidence Bound, compute the UCB value at each $S_{i}$ and evaluate numerically.\n3) Rank the three candidates under PI and under UCB from highest to lowest acquisition value. If there is an exact tie in an acquisition value, break ties by preferring the candidate with the larger predictive mean $\\mu_{i}$. State which candidate would be selected by each acquisition.\n4) Let $i_{\\mathrm{PI}}\\in\\{1,2,3\\}$ and $i_{\\mathrm{UCB}}\\in\\{1,2,3\\}$ denote the indices of the candidates selected by PI and UCB, respectively, under the tie-breaking rule above. Report the single integer $i_{\\mathrm{UCB}}-i_{\\mathrm{PI}}$ as your final answer. Do not include units.\n\nAll computations are dimensionless. Where intermediate numerical values are needed, you may approximate standard normal probabilities to at least four significant figures, but your final reported integer $i_{\\mathrm{UCB}}-i_{\\mathrm{PI}}$ must be exact (no rounding instruction is needed for an integer).",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of Bayesian optimization, is well-posed with all necessary information provided, and is expressed in objective, formal language. We proceed with the solution.\n\nThe solution is structured into four parts, corresponding to the four tasks specified in the problem statement.\n\n**1) Derivation and Calculation of Probability of Improvement (PI)**\n\nThe Probability of Improvement, $\\mathrm{PI}(x)$, is defined as the probability that the improvement random variable, $I(x) = \\max\\{0, f(x)-(f^{*}+\\xi)\\}$, is strictly positive.\nThe condition $I(x) > 0$ is equivalent to the condition $f(x) - (f^{*} + \\xi) > 0$, or $f(x) > f^{*} + \\xi$.\n\nThe problem states that the GP posterior predictive distribution for the objective function $f(x)$ is Gaussian: $f(x) \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))$.\nTo calculate the probability, we standardize the random variable $f(x)$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0,1)$. Then we can write $f(x) = \\mu(x) + \\sigma(x)Z$.\n\nThe inequality $f(x) > f^{*} + \\xi$ becomes:\n$$ \\mu(x) + \\sigma(x)Z > f^{*} + \\xi $$\nRearranging for $Z$, we get:\n$$ \\sigma(x)Z > f^{*} + \\xi - \\mu(x) $$\n$$ Z > \\frac{f^{*} + \\xi - \\mu(x)}{\\sigma(x)} $$\nThe probability is thus:\n$$ \\mathrm{PI}(x) = P\\left(Z > \\frac{f^{*} + \\xi - \\mu(x)}{\\sigma(x)}\\right) $$\nLet $\\Phi(\\cdot)$ denote the cumulative distribution function (CDF) of the standard normal distribution. The probability $P(Z > z)$ can be written as $1 - \\Phi(z)$, or equivalently, $\\Phi(-z)$. Therefore, the analytical expression for the Probability of Improvement is:\n$$ \\mathrm{PI}(x) = \\Phi\\left(\\frac{\\mu(x) - f^{*} - \\xi}{\\sigma(x)}\\right) $$\nNow, we evaluate this expression for the three candidate salts using the given values: $f^{*} = 0.95$ and $\\xi = 0.01$. The improvement threshold is $f^{*} + \\xi = 0.95 + 0.01 = 0.96$.\n\nFor candidate $S_{1}$:\n$\\mu_{1} = 0.955$ and $\\sigma_{1}^{2} = 1.0 \\times 10^{-4}$, so $\\sigma_{1} = \\sqrt{1.0 \\times 10^{-4}} = 0.01$.\nThe argument of the CDF is:\n$$ z_1 = \\frac{\\mu_{1} - (f^{*} + \\xi)}{\\sigma_{1}} = \\frac{0.955 - 0.96}{0.01} = \\frac{-0.005}{0.01} = -0.5 $$\nSo, $\\mathrm{PI}_{1} = \\Phi(-0.5) \\approx 0.3085$.\n\nFor candidate $S_{2}$:\n$\\mu_{2} = 0.85$ and $\\sigma_{2}^{2} = 1.44 \\times 10^{-2}$, so $\\sigma_{2} = \\sqrt{1.44 \\times 10^{-2}} = 0.12$.\nThe argument of the CDF is:\n$$ z_2 = \\frac{\\mu_{2} - (f^{*} + \\xi)}{\\sigma_{2}} = \\frac{0.85 - 0.96}{0.12} = \\frac{-0.11}{0.12} \\approx -0.9167 $$\nSo, $\\mathrm{PI}_{2} = \\Phi(-0.9167) \\approx 0.1797$.\n\nFor candidate $S_{3}$:\n$\\mu_{3} = 0.95$ and $\\sigma_{3}^{2} = 4.0 \\times 10^{-4}$, so $\\sigma_{3} = \\sqrt{4.0 \\times 10^{-4}} = 0.02$.\nThe argument of the CDF is:\n$$ z_3 = \\frac{\\mu_{3} - (f^{*} + \\xi)}{\\sigma_{3}} = \\frac{0.95 - 0.96}{0.02} = \\frac{-0.01}{0.02} = -0.5 $$\nSo, $\\mathrm{PI}_{3} = \\Phi(-0.5) \\approx 0.3085$.\n\n**2) Calculation of Upper Confidence Bound (UCB)**\n\nThe Upper Confidence Bound is defined as $\\mathrm{UCB}(x) = \\mu(x) + \\sqrt{\\beta}\\sigma(x)$.\nGiven the exploration parameter $\\beta = 4$, we have $\\sqrt{\\beta} = \\sqrt{4} = 2$. We compute the UCB value for each candidate.\n\nFor candidate $S_{1}$:\n$\\mu_{1} = 0.955$ and $\\sigma_{1} = 0.01$.\n$$ \\mathrm{UCB}_{1} = 0.955 + 2 \\times 0.01 = 0.955 + 0.02 = 0.975 $$\n\nFor candidate $S_{2}$:\n$\\mu_{2} = 0.85$ and $\\sigma_{2} = 0.12$.\n$$ \\mathrm{UCB}_{2} = 0.85 + 2 \\times 0.12 = 0.85 + 0.24 = 1.09 $$\n\nFor candidate $S_{3}$:\n$\\mu_{3} = 0.95$ and $\\sigma_{3} = 0.02$.\n$$ \\mathrm{UCB}_{3} = 0.95 + 2 \\times 0.02 = 0.95 + 0.04 = 0.99 $$\n\n**3) Ranking of Candidates and Selection**\n\nWe rank the candidates based on the PI and UCB values, from highest to lowest.\n\nRanking by PI:\nThe computed PI values are $\\mathrm{PI}_{1} \\approx 0.3085$, $\\mathrm{PI}_{2} \\approx 0.1797$, and $\\mathrm{PI}_{3} \\approx 0.3085$.\nWe observe that $\\mathrm{PI}_{1} = \\mathrm{PI}_{3} > \\mathrm{PI}_{2}$. There is an exact tie between $S_{1}$ and $S_{3}$.\nThe tie-breaking rule states to prefer the candidate with the larger predictive mean $\\mu_{i}$.\nComparing $S_{1}$ and $S_{3}$: $\\mu_{1} = 0.955$ and $\\mu_{3} = 0.95$. Since $\\mu_{1} > \\mu_{3}$, candidate $S_{1}$ is ranked higher than $S_{3}$.\nThe final PI ranking is $S_{1} > S_{3} > S_{2}$.\nThe candidate selected by the PI acquisition function is $S_{1}$. Therefore, $i_{\\mathrm{PI}} = 1$.\n\nRanking by UCB:\nThe computed UCB values are $\\mathrm{UCB}_{1} = 0.975$, $\\mathrm{UCB}_{2} = 1.09$, and $\\mathrm{UCB}_{3} = 0.99$.\nComparing these values, we have $1.09 > 0.99 > 0.975$.\nThe UCB ranking is $S_{2} > S_{3} > S_{1}$.\nThe candidate selected by the UCB acquisition function is $S_{2}$. Therefore, $i_{\\mathrm{UCB}} = 2$.\n\n**4) Final Calculation**\n\nThe problem asks for the integer value of $i_{\\mathrm{UCB}} - i_{\\mathrm{PI}}$.\nWe found that $i_{\\mathrm{PI}} = 1$ and $i_{\\mathrm{UCB}} = 2$.\nThus, the final result is:\n$$ i_{\\mathrm{UCB}} - i_{\\mathrm{PI}} = 2 - 1 = 1 $$\nThis is the final integer to be reported.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}