{
    "hands_on_practices": [
        {
            "introduction": "The first step in constructing a Polynomial Chaos Expansion is selecting a basis that is orthogonal with respect to the probability distribution of the uncertain inputs. This practice explores this fundamental principle by tackling a common scenario in battery modeling where a parameter, like porosity, is defined on a bounded interval and modeled by a Beta distribution. You will derive the orthonormal Jacobi polynomials appropriate for this case, providing a concrete example of the Wiener-Askey scheme which is central to PCE theory .",
            "id": "3941378",
            "problem": "In automated battery design and simulation, uncertain microstructural parameters such as the electrolyte porosity $\\,\\varepsilon\\,$ of a porous electrode are commonly modeled probabilistically. Suppose $\\,\\varepsilon\\,$ follows a Beta distribution $\\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha0$ and $\\beta0$ on the interval $(0,1)$, so that its probability density function is proportional to $\\,w(x)\\propto x^{\\alpha-1}(1-x)^{\\beta-1}\\,$ for $\\,x\\in(0,1)$. To construct a Polynomial Chaos Expansion (PCE) basis tailored to this uncertainty for propagating uncertainty in a scalar performance metric of interest, we require an orthonormal polynomial basis $\\{\\psi_n(x)\\}_{n\\ge 0}$ on $(0,1)$ with respect to the weight $\\,w(x)\\,$.\n\nStarting from first principles of orthogonality with respect to a weight and the definition of the Beta function $\\,B(\\alpha,\\beta)\\,$ and Gamma function $\\,\\Gamma(\\cdot)\\,$, derive an orthonormal basis on $(0,1)$ by appropriately shifting and scaling the classical Jacobi polynomials so that\n$$\n\\int_{0}^{1} w(x)\\,\\psi_m(x)\\,\\psi_n(x)\\,dx \\;=\\; \\delta_{mn}.\n$$\nExpress your orthonormal polynomials in the form\n$$\n\\psi_n(x)\\;=\\;c_n\\,P_n^{(\\beta-1,\\alpha-1)}\\!\\big(2x-1\\big),\n$$\nwhere $\\,P_n^{(a,b)}(\\cdot)\\,$ denotes the Jacobi polynomial of degree $n$ with parameters $(a,b)$, and determine the normalization constants $\\,c_n\\,$ explicitly for degrees $\\,n=0,1,2\\,$.\n\nProvide your final answer as a single row vector containing the three normalization constants in the order $\\,n=0,1,2\\,$, expressed in closed form using $\\,\\Gamma(\\cdot)\\,$ and $\\,B(\\alpha,\\beta)\\,$ as needed. No numerical evaluation or rounding is required, and no units are needed.",
            "solution": "The problem is valid. It is a standard, well-posed problem in the theory of orthogonal polynomials, specifically as it applies to the construction of Polynomial Chaos Expansions for uncertainty quantification. All provided information is scientifically sound and mathematically consistent.\n\nWe are tasked with finding the normalization constants $c_n$ for $n=0,1,2$ such that the polynomials $\\psi_n(x) = c_n P_n^{(\\beta-1,\\alpha-1)}(2x-1)$ form an orthonormal set with respect to the weight function $w(x) = x^{\\alpha-1}(1-x)^{\\beta-1}$ on the interval $(0,1)$. The condition for orthonormality is\n$$\n\\int_{0}^{1} w(x)\\,\\psi_m(x)\\,\\psi_n(x)\\,dx = \\delta_{mn}\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta.\n\nFor the case $m=n$, this condition becomes a normalization requirement:\n$$\n\\int_{0}^{1} w(x)\\,[\\psi_n(x)]^2\\,dx = 1\n$$\nSubstituting the given forms for $w(x)$ and $\\psi_n(x)$, we have:\n$$\n\\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1} \\left[c_n P_n^{(\\beta-1,\\alpha-1)}(2x-1)\\right]^2 dx = 1\n$$\nThe constant $c_n^2$ can be factored out of the integral:\n$$\nc_n^2 \\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(2x-1)\\right]^2 dx = 1\n$$\nLet's name the integral $I_n$:\n$$\nI_n = \\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(2x-1)\\right]^2 dx\n$$\nFrom this, the normalization constant is given by $c_n^2 = 1/I_n$.\n\nTo evaluate $I_n$, we transform the integral to the canonical domain of Jacobi polynomials, which is the interval $[-1, 1]$. We introduce the change of variables $y = 2x-1$.\nThis implies:\n$x = \\frac{y+1}{2}$\n$1-x = 1 - \\frac{y+1}{2} = \\frac{1-y}{2}$\n$dx = \\frac{1}{2}dy$\nAs $x$ goes from $0$ to $1$, $y$ goes from $-1$ to $1$.\n\nSubstituting these into the expression for $I_n$:\n$$\nI_n = \\int_{-1}^{1} \\left(\\frac{y+1}{2}\\right)^{\\alpha-1} \\left(\\frac{1-y}{2}\\right)^{\\beta-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(y)\\right]^2 \\frac{dy}{2}\n$$\nWe can separate the constants from the integrand:\n$$\nI_n = \\frac{1}{2^{\\alpha-1}} \\frac{1}{2^{\\beta-1}} \\frac{1}{2} \\int_{-1}^{1} (1-y)^{\\beta-1} (1+y)^{\\alpha-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(y)\\right]^2 dy\n$$\n$$\nI_n = \\frac{1}{2^{\\alpha+\\beta-1}} \\int_{-1}^{1} (1-y)^{\\beta-1} (1+y)^{\\alpha-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(y)\\right]^2 dy\n$$\nThe integral on the right-hand side is the squared norm of the Jacobi polynomial $P_n^{(a,b)}(y)$ with parameters $a = \\beta-1$ and $b = \\alpha-1$. The standard formula for this squared norm, let's call it $h_n^{(a,b)}$, is:\n$$\nh_n^{(a,b)} = \\int_{-1}^{1} (1-y)^a (1+y)^b \\left[P_n^{(a,b)}(y)\\right]^2 dy = \\frac{2^{a+b+1}}{2n+a+b+1} \\frac{\\Gamma(n+a+1)\\Gamma(n+b+1)}{n!\\Gamma(n+a+b+1)}\n$$\nFor our specific parameters $a = \\beta-1$ and $b = \\alpha-1$, we have:\n$a+b+1 = (\\beta-1) + (\\alpha-1) + 1 = \\alpha+\\beta-1$\n$n+a+1 = n+\\beta$\n$n+b+1 = n+\\alpha$\n$n+a+b+1 = n+\\alpha+\\beta-1$\n\nSubstituting these into the formula for $h_n^{(a,b)}$:\n$$\nh_n^{(\\beta-1, \\alpha-1)} = \\frac{2^{\\alpha+\\beta-1}}{2n+\\alpha+\\beta-1} \\frac{\\Gamma(n+\\beta)\\Gamma(n+\\alpha)}{n!\\Gamma(n+\\alpha+\\beta-1)}\n$$\nNow, we can express $I_n$ in terms of this squared norm:\n$$\nI_n = \\frac{1}{2^{\\alpha+\\beta-1}} h_n^{(\\beta-1, \\alpha-1)} = \\frac{1}{2^{\\alpha+\\beta-1}} \\left( \\frac{2^{\\alpha+\\beta-1}}{2n+\\alpha+\\beta-1} \\frac{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n!\\Gamma(n+\\alpha+\\beta-1)} \\right)\n$$\n$$\nI_n = \\frac{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n!(2n+\\alpha+\\beta-1)\\Gamma(n+\\alpha+\\beta-1)}\n$$\nSince $c_n^2 = 1/I_n$, we get the general formula for the squared normalization constant:\n$$\nc_n^2 = \\frac{n!(2n+\\alpha+\\beta-1)\\Gamma(n+\\alpha+\\beta-1)}{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}\n$$\nBy convention, we take the positive square root for $c_n$. We now compute $c_n$ for $n=0, 1, 2$.\n\nFor $n=0$:\n$$\nc_0^2 = \\frac{0!(\\alpha+\\beta-1)\\Gamma(\\alpha+\\beta-1)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n$$\nUsing the property $\\Gamma(z+1)=z\\Gamma(z)$, we have $(\\alpha+\\beta-1)\\Gamma(\\alpha+\\beta-1) = \\Gamma(\\alpha+\\beta)$.\n$$\nc_0^2 = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} = \\frac{1}{B(\\alpha,\\beta)}\n$$\nThus, $c_0 = \\frac{1}{\\sqrt{B(\\alpha,\\beta)}}$.\n\nFor $n=1$:\n$$\nc_1^2 = \\frac{1!(2+\\alpha+\\beta-1)\\Gamma(1+\\alpha+\\beta-1)}{\\Gamma(1+\\alpha)\\Gamma(1+\\beta)} = \\frac{(\\alpha+\\beta+1)\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha+1)\\Gamma(\\beta+1)}\n$$\nUsing $\\Gamma(\\alpha+1)=\\alpha\\Gamma(\\alpha)$ and $\\Gamma(\\beta+1)=\\beta\\Gamma(\\beta)$:\n$$\nc_1^2 = \\frac{(\\alpha+\\beta+1)\\Gamma(\\alpha+\\beta)}{\\alpha\\Gamma(\\alpha)\\beta\\Gamma(\\beta)} = \\frac{\\alpha+\\beta+1}{\\alpha\\beta} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} = \\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}\n$$\nThus, $c_1 = \\sqrt{\\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}}$.\n\nFor $n=2$:\n$$\nc_2^2 = \\frac{2!(4+\\alpha+\\beta-1)\\Gamma(2+\\alpha+\\beta-1)}{\\Gamma(2+\\alpha)\\Gamma(2+\\beta)} = \\frac{2(\\alpha+\\beta+3)\\Gamma(\\alpha+\\beta+1)}{\\Gamma(\\alpha+2)\\Gamma(\\beta+2)}\n$$\nUsing the Gamma function properties repeatedly:\n$\\Gamma(\\alpha+\\beta+1) = (\\alpha+\\beta)\\Gamma(\\alpha+\\beta)$\n$\\Gamma(\\alpha+2) = (\\alpha+1)\\alpha\\Gamma(\\alpha)$\n$\\Gamma(\\beta+2) = (\\beta+1)\\beta\\Gamma(\\beta)$\n$$\nc_2^2 = \\frac{2(\\alpha+\\beta+3)(\\alpha+\\beta)\\Gamma(\\alpha+\\beta)}{(\\alpha+1)\\alpha\\Gamma(\\alpha) (\\beta+1)\\beta\\Gamma(\\beta)} = \\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n$$\n$$\nc_2^2 = \\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}\n$$\nThus, $c_2 = \\sqrt{\\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}}$.\n\nThe three normalization constants are:\n$c_0 = \\frac{1}{\\sqrt{B(\\alpha,\\beta)}}$\n$c_1 = \\sqrt{\\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}}$\n$c_2 = \\sqrt{\\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{B(\\alpha,\\beta)}}  \\sqrt{\\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}}  \\sqrt{\\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world engineering systems often feature non-smooth behaviors like switches or hard cut-offs, which pose a significant challenge for approximation with smooth global polynomials. This exercise demonstrates how such discontinuities lead to Gibbs-like oscillations in a PCE context, compromising the accuracy of the surrogate model near the switching point. By analyzing a battery model with a voltage cut-off, you will learn to identify and evaluate advanced strategies, such as piecewise expansions, to mitigate these artifacts .",
            "id": "3941459",
            "problem": "An automated battery discharge test uses a simple Thevenin-equivalent single-resistor model with voltage $V(t;\\xi)=V_{\\mathrm{oc}}(z(t)) - I R(\\xi)$, where $V_{\\mathrm{oc}}$ is the open-circuit voltage as a function of state-of-charge $z(t)$, $I$ is a prescribed constant discharge current, and $R(\\xi)$ is the internal resistance parameterized by a scalar random input $\\xi$ that captures manufacturing variability. The test includes a hard cut-off at voltage $V_{\\mathrm{cut}}$: discharge is terminated at the stopping time $\\tau(\\xi)$ defined by the event $V(\\tau(\\xi);\\xi)=V_{\\mathrm{cut}}$, or at a fixed horizon $T$ if the cut-off is not reached earlier. Consider the quantity of interest $Y(\\xi)$ defined as the safety trip indicator up to time $T$, \n$$\nY(\\xi)=\\mathbf{1}\\{\\tau(\\xi)\\le T\\}.\n$$\nAssume the input $\\xi\\sim \\mathcal{U}(-1,1)$ with probability density $\\rho(\\xi)=\\tfrac{1}{2}$, and that $R(\\xi)$ is monotone in $\\xi$ so that the event surface $\\{\\xi: \\tau(\\xi)=T\\}$ is a single threshold $\\xi_0\\in(-1,1)$, hence \n$$\nY(\\xi)=H(\\xi-\\xi_0),\n$$\nwhere $H$ is the Heaviside step. To propagate uncertainty, one constructs a Polynomial Chaos Expansion (PCE) of $Y(\\xi)$ using the Legendre basis $\\{P_n(\\xi)\\}_{n\\ge 0}$ orthonormal on $[-1,1]$ with respect to $\\rho(\\xi)$. The truncated $L^2$ projection of degree $p$ is\n$$\nY_p(\\xi)=\\sum_{n=0}^p c_n \\,\\phi_n(\\xi), \\quad \\phi_n(\\xi)=\\sqrt{\\tfrac{2n+1}{2}}\\,P_n(\\xi), \\quad c_n=\\int_{-1}^1 Y(\\zeta)\\,\\phi_n(\\zeta)\\,\\rho(\\zeta)\\,d\\zeta.\n$$\n\nStarting from the definitions of $L^2$ projection and orthogonality of Legendre polynomials, explain why the truncation $Y_p(\\xi)$ exhibits non-vanishing oscillatory overshoots and undershoots localized near the threshold $\\xi_0$ as $p\\to\\infty$ (a Gibbs-like phenomenon), even though $Y\\in L^2([-1,1],\\rho)$. Your explanation should use only properties of orthogonal projections and kernels (for example, the Christoffelâ€“Darboux identity) and should not assume any special Fourier-series facts. Then, based on this analysis and the battery switching mechanism, identify which of the following statements are correct about mitigation strategies and their implications in this context.\n\nA. Simply increasing the global polynomial degree $p$ in the Legendre PCE will eliminate the oscillations and recover exponential convergence of $Y_p$ to $Y$ in the $L^\\infty$ norm, even in the presence of the cut-off switching.\n\nB. Constructing a piecewise PCE by partitioning the input domain at $\\xi_0$ into two elements $[-1,\\xi_0)$ and $(\\xi_0,1]$ and fitting separate local Legendre expansions on each element can suppress Gibbs-like ringing and restore fast convergence consistent with the smoothness of $Y$ restricted to each element.\n\nC. When estimating a PCE from data via regression, augmenting the least-squares fit with a Total Variation (TV) penalty in the input domain, i.e., solving $\\min_{u\\in\\mathcal{P}_p}\\sum_{j=1}^m\\big(u(\\xi^{(j)})-Y^{(j)}\\big)^2+\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ with $\\lambda0$, can reduce oscillations near $\\xi_0$ but may introduce bias in estimates of moments such as the mean and variance.\n\nD. Pre-smoothing the output with a Gaussian kernel, i.e., replacing $Y(\\xi)$ by $(G_\\sigma\\ast Y)(\\xi)$ with a small standard deviation $\\sigma0$, yields a PCE whose mean and variance remain unbiased estimators of those of $Y$ while removing Gibbs oscillations.\n\nE. An affine reparameterization of the input, $\\eta=a\\xi+b$ with constants $a\\ne 0$ and $b$, always removes Gibbs-like oscillations for step-type quantities like $Y(\\xi)$, regardless of the orientation of the discontinuity surface in higher-dimensional inputs.\n\nSelect all that apply. Provide a rigorous, principle-based argument to justify the Gibbs-like phenomenon and to evaluate each option. All mathematical symbols, variables, functions, operators, and numbers must appear in LaTeX. Assume $V_{\\mathrm{oc}}$ is smooth and strictly decreasing in $z$ over the relevant range, $R(\\xi)$ is smooth and strictly increasing in $\\xi$, and $I$, $V_{\\mathrm{cut}}$, and $T$ are fixed positive constants.",
            "solution": "The problem statement poses a valid and well-posed question in the field of uncertainty quantification, grounded in a realistic (though simplified) model of battery discharge. The core of the problem lies in the mathematical properties of Polynomial Chaos Expansions (PCE) when approximating discontinuous functions, which is a standard and important topic. All assumptions are clearly stated and are physically and mathematically consistent.\n\nThe first task is to explain the Gibbs-like phenomenon for the Legendre PCE of the step function $Y(\\xi) = H(\\xi - \\xi_0)$.\n\nThe truncated PCE of degree $p$, denoted $Y_p(\\xi)$, is the orthogonal projection of the function $Y(\\xi)$ onto the subspace $\\mathcal{P}_p$ of polynomials of degree at most $p$. The projection is defined with respect to the $L^2$ inner product $\\langle f, g \\rangle = \\int_{-1}^1 f(\\xi)g(\\xi)\\rho(\\xi)d\\xi$. The projection $Y_p$ is the unique polynomial in $\\mathcal{P}_p$ that minimizes the squared error in the $L^2$ norm:\n$$\nY_p = \\arg\\min_{u \\in \\mathcal{P}_p} \\|Y - u\\|_{L^2}^2 = \\arg\\min_{u \\in \\mathcal{P}_p} \\int_{-1}^1 (Y(\\xi) - u(\\xi))^2 \\rho(\\xi) d\\xi\n$$\nThe formal expression for this projection is given by\n$$\nY_p(\\xi) = \\sum_{n=0}^p c_n \\phi_n(\\xi) = \\int_{-1}^1 Y(\\zeta) \\left( \\sum_{n=0}^p \\phi_n(\\xi) \\phi_n(\\zeta) \\right) \\rho(\\zeta) d\\zeta\n$$\nLet's define the kernel of the projection operator as $K_p(\\xi, \\zeta) = \\sum_{n=0}^p \\phi_n(\\xi) \\phi_n(\\zeta)$. The projection can then be written as a convolution-like integral:\n$$\nY_p(\\xi) = \\int_{-1}^1 Y(\\zeta) K_p(\\xi, \\zeta) \\rho(\\zeta) d\\zeta\n$$\nSince $Y(\\xi)$ is the Heaviside function $H(\\xi-\\xi_0)$, the integral becomes:\n$$\nY_p(\\xi) = \\int_{\\xi_0}^1 K_p(\\xi, \\zeta) \\rho(\\zeta) d\\zeta\n$$\nThe Christoffel-Darboux formula for the orthonormal Legendre polynomials $\\phi_n(\\xi) = \\sqrt{\\frac{2n+1}{2}}P_n(\\xi)$ provides an expression for the kernel:\n$$\nK_p(\\xi, \\zeta) = \\sqrt{\\tfrac{p+1}{2}\\tfrac{2p+3}{2}} \\frac{\\phi_{p+1}(\\xi)\\phi_p(\\zeta) - \\phi_p(\\xi)\\phi_{p+1}(\\zeta)}{\\xi-\\zeta}\n$$\nThe key to understanding the Gibbs phenomenon lies in the properties of this projection.\n1.  **Nature of Convergence**: Since $Y(\\xi)$ is square-integrable on $[-1, 1]$ (i.e., $Y \\in L^2([-1,1], \\rho)$), the theory of orthogonal expansions guarantees that the sequence of projections $Y_p$ converges to $Y$ in the $L^2$ norm. That is, $\\|Y_p - Y\\|_{L^2} \\to 0$ as $p \\to \\infty$.\n2.  **Lack of Uniform Convergence**: However, $L^2$ convergence does not imply uniform convergence (convergence in the $L^\\infty$ norm, $\\|f\\|_\\infty = \\sup_\\xi |f(\\xi)|$). For uniform convergence to hold, the limit function must be continuous, as the uniform limit of a sequence of continuous functions (the polynomials $Y_p$) must be continuous. Our target function $Y(\\xi)$ has a jump discontinuity at $\\xi = \\xi_0$. Therefore, the convergence of $Y_p$ to $Y$ cannot be uniform.\n3.  **Mechanism of Oscillation**: The failure to converge uniformly is precisely the Gibbs phenomenon. The polynomial $Y_p(\\xi)$, being infinitely smooth, cannot reproduce a sharp jump. To minimize the integrated squared error, the polynomial must rise very steeply near $\\xi_0$. A steep polynomial rise inevitably leads to an \"overshoot\" past the target value of $1$. As the degree $p$ increases, the approximation gets better in the $L^2$ sense: the oscillations are squeezed into a narrower and narrower region around the discontinuity $\\xi_0$. However, the maximum height of the overshoot does not decrease to zero. Instead, it converges to a fixed value greater than $1$ (approximately $1.09$ for a normalized jump of $1$ in the classic Fourier series case; the value is different but the principle is the same for Legendre series). This non-vanishing amplitude of the oscillation is the hallmark of the Gibbs phenomenon. The integral nature of the $L^2$ projection means it is insensitive to localized, large pointwise errors as long as the region they occupy has a measure that shrinks sufficiently fast.\n\nNow, we evaluate each of the proposed statements.\n\n**A. Simply increasing the global polynomial degree $p$ in the Legendre PCE will eliminate the oscillations and recover exponential convergence of $Y_p$ to $Y$ in the $L^\\infty$ norm, even in the presence of the cut-off switching.**\nThis statement is fundamentally incorrect. As explained above, the Gibbs phenomenon is characterized by non-vanishing oscillations for PCEs of discontinuous functions. Increasing the polynomial degree $p$ localizes the oscillations closer to the discontinuity but does not eliminate them. Therefore, convergence in the $L^\\infty$ norm (uniform convergence) is not achieved. Furthermore, exponential convergence of PCEs is typically associated with functions that are analytic within an ellipse in the complex plane containing the interval $[-1, 1]$. The function $Y(\\xi)$ is a step function, which is not even continuous, let alone analytic. The convergence rate in $L^2$ is algebraic, not exponential.\n**Verdict: Incorrect**\n\n**B. Constructing a piecewise PCE by partitioning the input domain at $\\xi_0$ into two elements $[-1,\\xi_0)$ and $(\\xi_0,1]$ and fitting separate local Legendre expansions on each element can suppress Gibbs-like ringing and restore fast convergence consistent with the smoothness of $Y$ restricted to each element.**\nThis statement is correct. The Gibbs phenomenon arises from approximating a non-smooth function with a single global basis of smooth functions. By partitioning the domain at the point of discontinuity $\\xi_0$, this strategy treats the problem as two separate approximation problems. On the subdomain $[-1, \\xi_0)$, the function $Y(\\xi)$ is identically zero. On the subdomain $(\\xi_0, 1]$, the function is identically one. Both $f(\\xi)=0$ and $f(\\xi)=1$ are constant functions, hence they are infinitely smooth ($C^\\infty$). A polynomial expansion can represent a constant exactly with a single, degree-$0$ term. Therefore, a piecewise PCE (with degree $p \\ge 0$ on each piece) will represent $Y(\\xi)$ exactly, with no error and consequently no Gibbs oscillations. This restores the convergence properties to be consistent with the smoothness of the function *within each subdomain*.\n**Verdict: Correct**\n\n**C. When estimating a PCE from data via regression, augmenting the least-squares fit with a Total Variation (TV) penalty in the input domain, i.e., solving $\\min_{u\\in\\mathcal{P}_p}\\sum_{j=1}^m\\big(u(\\xi^{(j)})-Y^{(j)}\\big)^2+\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ with $\\lambda0$, can reduce oscillations near $\\xi_0$ but may introduce bias in estimates of moments such as the mean and variance.**\nThis statement is correct. The standard least-squares fit is a discrete analogue of the $L^2$ projection and thus exhibits Gibbs oscillations. The term $\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ is a Total Variation (TV) regularization penalty. This penalty term discourages solutions $u(\\xi)$ with large variations, such as the rapid oscillations of the Gibbs phenomenon. Maximizing this penalty favors smoother, less oscillatory functions. Thus, including it in the minimization objective will suppress the ringing artifacts. However, this comes at a cost. The solution to the regularized problem is no longer the true projection; it is a biased approximation. The regularization systematically alters the coefficients of the polynomial away from their unbiased least-squares values. Since the moments of the random variable (e.g., mean $\\mathbb{E}[Y]$, variance $\\text{Var}[Y]$) are computed from these coefficients (e.g., $\\mathbb{E}[Y_p] = c_0 \\phi_0(0)$), using the biased coefficients from the TV-regularized fit will result in biased estimates of the moments. This is a classic example of the bias-variance trade-off in statistics and machine learning.\n**Verdict: Correct**\n\n**D. Pre-smoothing the output with a Gaussian kernel, i.e., replacing $Y(\\xi)$ by $(G_\\sigma\\ast Y)(\\xi)$ with a small standard deviation $\\sigma0$, yields a PCE whose mean and variance remain unbiased estimators of those of $Y$ while removing Gibbs oscillations.**\nThis statement is incorrect. Convolving the step function $Y(\\xi)$ with a Gaussian kernel $G_\\sigma$ produces a smooth function $Y_\\sigma(\\xi)$. A PCE of this new, smooth function $Y_\\sigma$ will indeed converge rapidly and without Gibbs oscillations. So the first part of the claim is correct. However, the second part is false. The PCE of $Y_\\sigma$ gives us approximations of the moments of $Y_\\sigma$, not $Y$. The process of smoothing changes the function, and therefore changes its moments. For example, the mean is $\\mathbb{E}[Y] = \\int_{-1}^1 Y(\\xi)\\rho(\\xi)d\\xi$. The mean of the smoothed function is $\\mathbb{E}[Y_\\sigma] = \\int_{-1}^1 Y_\\sigma(\\xi)\\rho(\\xi)d\\xi$. Since $Y_\\sigma \\neq Y$, there is no reason to expect $\\mathbb{E}[Y_\\sigma] = \\mathbb{E}[Y]$. The smoothing operation introduces a systematic error, or bias, into the moments. Thus, the mean and variance of the PCE of $Y_\\sigma$ are biased estimators of the mean and variance of $Y$.\n**Verdict: Incorrect**\n\n**E. An affine reparameterization of the input, $\\eta=a\\xi+b$ with constants $a\\ne 0$ and $b$, always removes Gibbs-like oscillations for step-type quantities like $Y(\\xi)$, regardless of the orientation of the discontinuity surface in higher-dimensional inputs.**\nThis statement is incorrect. An affine reparameterization $\\eta=a\\xi+b$ simply maps the interval $[-1, 1]$ to another interval and shifts/scales the coordinate system. The function $Y(\\xi) = H(\\xi-\\xi_0)$ becomes $\\tilde{Y}(\\eta) = H((\\eta-b)/a - \\xi_0)$, which is still a step function with a discontinuity at $\\eta_0 = a\\xi_0+b$. Approximating this new step function $\\tilde{Y}(\\eta)$ with polynomials in $\\eta$ will still produce the Gibbs phenomenon for the exact same reasons as before. An affine map does not alter the fundamental nature of the problem: approximating a discontinuous function with smooth global basis functions. The claim is patently false, and its generalization to higher dimensions is equally invalid.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "A critical requirement for any physical model is that its state variables, such as concentration or temperature, must adhere to physical bounds. Standard Polynomial Chaos Expansions, being unconstrained polynomial sums, can violate these bounds, yielding non-physical results. This practice investigates how to enforce such constraints, specifically positivity, by applying a logarithmic transformation to the state variable and analyzing the consequences for the intrusive Galerkin formulation .",
            "id": "3941450",
            "problem": "In automated battery design and simulation, intrusive polynomial chaos expansions (PCE) are used to propagate parametric uncertainty in reduced-order electrochemical models while preserving governing physical constraints. Consider the requirement that the lithium concentration $c(t,\\boldsymbol{\\xi})$ in an active-material control volume must remain positive and, in practice, bounded by a maximum site density $c_{\\max}$. As a fundamental base, take mass conservation with Fickian transport and intercalation source at the representative volume element scale: in a spatially well-mixed approximation the state $c(t,\\boldsymbol{\\xi})$ satisfies an ordinary differential equation of the form\n$$\n\\frac{d c}{d t} = -k(\\boldsymbol{\\xi})\\,c + s(t), \\quad c(0,\\boldsymbol{\\xi}) = c_0(\\boldsymbol{\\xi}),\n$$\nwhere $k(\\boldsymbol{\\xi})0$ models an effective first-order consumption rate (arising from transport-limited or reaction-limited uptake) depending on uncertain inputs $\\boldsymbol{\\xi}$, and $s(t)\\ge 0$ is a deterministic source proportional to the applied current. The intrusive stochastic Galerkin method expands uncertain fields in an orthogonal polynomial basis $\\{\\Psi_\\alpha(\\boldsymbol{\\xi})\\}$ tied to the distributions of $\\boldsymbol{\\xi}$, and projects the governing equation onto this basis to obtain a coupled system for the expansion coefficients.\n\nDirectly expanding $c(t,\\boldsymbol{\\xi})$ in a finite-order PCE can lead to $c0$ for some realizations of $\\boldsymbol{\\xi}$ due to truncation and mode coupling, violating positivity of concentration. A common remedy is to transform the state to an unbounded variable, for example by a logarithmic map $y = \\log c$ to enforce $c0$, or by a logistic-type map $y = \\log\\!\\big(c/(c_{\\max}-c)\\big)$ to enforce both $0cc_{\\max}$, and to perform an intrusive PCE on $y(t,\\boldsymbol{\\xi})$.\n\nBased on first principles of mass conservation and the definition of the intrusive PCE (orthogonal projection in the probability space of $\\boldsymbol{\\xi}$), which of the following statements about enforcing positivity via state transformation and its implications for the intrusive PCE system and statistical post-processing are correct? Select all that apply.\n\nA. If the original model in $c$ is linear in the state, then the logarithmic transform $y=\\log c$ preserves linearity of the governing equation, so the intrusive Galerkin algebraic structure (e.g., block-diagonal mass and stiffness matrices) remains unchanged aside from coefficient values.\n\nB. The transformation $y=\\log c$ guarantees $c=\\exp(y)0$ by construction; however, it introduces nonlinear terms (e.g., exponentials or gradient-squared terms under diffusion) that couple polynomial chaos modes in the intrusive Galerkin system, requiring higher-order inner products or numerical quadrature to assemble; the choice of polynomial basis in $\\boldsymbol{\\xi}$ remains determined by the input distributions (for example, Hermite polynomials for Gaussian inputs).\n\nC. After solving the intrusive PCE for $y(t,\\boldsymbol{\\xi})$, the mean concentration is given exactly by $\\mathbb{E}[c(t,\\boldsymbol{\\xi})]=\\exp\\!\\big(y_0(t)\\big)$, where $y_0(t)$ is the zeroth PCE coefficient of $y$.\n\nD. To enforce both positivity and saturation $0cc_{\\max}$, the logit transform $y=\\log\\!\\big(c/(c_{\\max}-c)\\big)$ may be used; in an intrusive PCE this induces additional nonlinearities in the projected system, but reconstructing $c$ from $y$ ensures the physical bounds are satisfied for all realizations of $\\boldsymbol{\\xi}$.\n\nE. Because $c=\\exp(y)$ is positive, one must switch the polynomial chaos basis to generalized Laguerre polynomials to represent $c$ as a lognormal variable; otherwise, the intrusive PCE will not converge for Gaussian-distributed inputs $\\boldsymbol{\\xi}$.",
            "solution": "The problem statement is evaluated to be scientifically sound, well-posed, objective, and self-contained. It describes a standard and valid application of intrusive polynomial chaos expansions (PCE) for uncertainty quantification in a simplified electrochemical model, addressing the common and important issue of preserving physical bounds on state variables. The transformations proposed ($y=\\log c$ and $y=\\log(c/(c_{\\max}-c))$) are well-known techniques for this purpose. The question asks for a correct assessment of the implications of these transformations within the PCE framework, which is a verifiable conceptual problem. Therefore, a full analysis of the options is warranted.\n\nThe governing ordinary differential equation (ODE) for the state variable $c(t,\\boldsymbol{\\xi})$ is given by:\n$$\n\\frac{d c}{d t} = -k(\\boldsymbol{\\xi})\\,c + s(t), \\quad c(0,\\boldsymbol{\\xi}) = c_0(\\boldsymbol{\\xi})\n$$\nThis is a linear ODE in the state variable $c$. The intrusive Galerkin method involves expanding the uncertain quantities, such as $c(t,\\boldsymbol{\\xi})$ and $k(\\boldsymbol{\\xi})$, in an orthogonal polynomial basis $\\{\\Psi_\\alpha(\\boldsymbol{\\xi})\\}$ that is chosen according to the probability distribution of the random inputs $\\boldsymbol{\\xi}$.\n$$\nc(t, \\boldsymbol{\\xi}) \\approx \\sum_{\\alpha=0}^{P} c_\\alpha(t) \\Psi_\\alpha(\\boldsymbol{\\xi}) \\quad \\text{and} \\quad k(\\boldsymbol{\\xi}) = \\sum_{\\beta=0}^{Q} k_\\beta \\Psi_\\beta(\\boldsymbol{\\xi})\n$$\nThe problem arises when this finite-sum approximation for $c(t, \\boldsymbol{\\xi})$ yields negative values for some realizations of $\\boldsymbol{\\xi}$, violating the physical constraint $c  0$. The proposed remedies involve transforming the state variable. We will analyze the consequences of these transformations for each option.\n\n**A. If the original model in $c$ is linear in the state, then the logarithmic transform $y=\\log c$ preserves linearity of the governing equation, so the intrusive Galerkin algebraic structure (e.g., block-diagonal mass and stiffness matrices) remains unchanged aside from coefficient values.**\n\nTo evaluate this statement, we must derive the governing equation for the transformed state $y(t,\\boldsymbol{\\xi}) = \\log c(t,\\boldsymbol{\\xi})$. The inverse transformation is $c = e^y$. Using the chain rule for the time derivative, we have:\n$$\n\\frac{dc}{dt} = \\frac{d(e^y)}{dt} = e^y \\frac{dy}{dt}\n$$\nSubstituting this and $c = e^y$ into the original ODE gives:\n$$\ne^y \\frac{dy}{dt} = -k(\\boldsymbol{\\xi})\\,e^y + s(t)\n$$\nAssuming $c  0$ and thus $e^y  0$, we can divide by $e^y$:\n$$\n\\frac{dy}{dt} = -k(\\boldsymbol{\\xi}) + s(t) e^{-y}\n$$\nThe original equation was linear in $c$. The new equation for $y$ contains the term $s(t)e^{-y}$, which is a nonlinear function of the state variable $y$. The presence of this nonlinear term fundamentally changes the structure of the resulting system of equations after the Galerkin projection. A linear system for $c_\\alpha(t)$ would lead to a system of the form $M \\frac{d\\mathbf{c}}{dt} = A \\mathbf{c} + \\mathbf{f}$, where matrices $M$ and $A$ are constant. A nonlinear system for $y_\\alpha(t)$ will be of the form $M \\frac{d\\mathbf{y}}{dt} = \\mathbf{F}(\\mathbf{y},t)$, where $\\mathbf{F}$ is a nonlinear vector function of the coefficients $\\mathbf{y}$. Therefore, the transformation does not preserve linearity.\n\nVerdict: **Incorrect**.\n\n**B. The transformation $y=\\log c$ guarantees $c=\\exp(y)0$ by construction; however, it introduces nonlinear terms (e.g., exponentials or gradient-squared terms under diffusion) that couple polynomial chaos modes in the intrusive Galerkin system, requiring higher-order inner products or numerical quadrature to assemble; the choice of polynomial basis in $\\boldsymbol{\\xi}$ remains determined by the input distributions (for example, Hermite polynomials for Gaussian inputs).**\n\nThis statement makes three main points.\n1.  **Positivity Guarantee:** The transformation $y = \\log c$ implies $c = \\exp(y)$. Since the PCE for $y$ produces a real-valued function $y(t,\\boldsymbol{\\xi})$, the reconstructed concentration $c(t,\\boldsymbol{\\xi}) = \\exp(y(t,\\boldsymbol{\\xi}))$ will always be positive, as the range of the exponential function is $(0, \\infty)$. This is correct.\n2.  **Introduction of Nonlinearity and Mode Coupling:** As derived in the analysis of option A, the transformed equation is $\\frac{dy}{dt} = -k(\\boldsymbol{\\xi}) + s(t) e^{-y}$. Applying intrusive Galerkin projection requires computing the inner product $\\langle \\cdot, \\Psi_j \\rangle_{\\boldsymbol{\\xi}}$ for each basis function $\\Psi_j$. Projecting the nonlinear term $e^{-y}$ involves computing $\\langle e^{-y}, \\Psi_j \\rangle_{\\boldsymbol{\\xi}}$. With $y = \\sum_{\\alpha} y_\\alpha(t)\\Psi_\\alpha(\\boldsymbol{\\xi})$, this term becomes $\\langle \\exp(-\\sum_{\\alpha} y_\\alpha \\Psi_\\alpha), \\Psi_j \\rangle_{\\boldsymbol{\\xi}}$. This is a nonlinear function of all coefficients $\\{y_\\alpha\\}$, coupling them together. Evaluating this inner product integral generally requires numerical quadrature, as analytical solutions are typically unavailable. The mention of \"gradient-squared terms under diffusion\" is an accurate observation for models with spatial dependence, e.g., $\\nabla \\cdot (D \\nabla c)$ becomes $\\nabla \\cdot (D e^y \\nabla y)$, which involves terms like $D e^y (\\nabla y)^2$ that create further nonlinearity. This point is correct.\n3.  **Choice of Basis:** The Wiener-Askey framework for PCE dictates that the orthogonal polynomial basis $\\{\\Psi_\\alpha(\\boldsymbol{\\xi})\\}$ must be chosen to be orthogonal with respect to the probability measure of the random inputs $\\boldsymbol{\\xi}$. For instance, if $\\boldsymbol{\\xi}$ follows a standard normal (Gaussian) distribution, Hermite polynomials are the appropriate choice. The transformation is applied to the state variable $c$, not the random inputs $\\boldsymbol{\\xi}$. Therefore, the choice of basis remains coupled to the input statistics, not the output statistics. This point is correct.\n\nSince all three parts of the statement are correct, the statement as a whole is correct.\n\nVerdict: **Correct**.\n\n**C. After solving the intrusive PCE for $y(t,\\boldsymbol{\\xi})$, the mean concentration is given exactly by $\\mathbb{E}[c(t,\\boldsymbol{\\xi})]=\\exp\\!\\big(y_0(t)\\big)$, where $y_0(t)$ is the zeroth PCE coefficient of $y$.**\n\nThe PCE of $y(t,\\boldsymbol{\\xi})$ is $y(t, \\boldsymbol{\\xi}) \\approx \\sum_{\\alpha=0}^{P} y_\\alpha(t) \\Psi_\\alpha(\\boldsymbol{\\xi})$. Assuming the standard normalization where $\\Psi_0=1$ and $\\mathbb{E}[\\Psi_\\alpha]=0$ for $\\alpha0$, the expected value (mean) of $y$ is the zeroth coefficient: $\\mathbb{E}[y(t,\\boldsymbol{\\xi})] = y_0(t)$.\nThe question asks for the mean of $c(t,\\boldsymbol{\\xi}) = \\exp(y(t,\\boldsymbol{\\xi}))$. The statement claims $\\mathbb{E}[c] = \\exp(y_0(t))$, which is equivalent to claiming $\\mathbb{E}[e^y] = e^{\\mathbb{E}[y]}$.\nThis is generally false. The function $f(y) = e^y$ is a convex function. By Jensen's inequality, for a random variable $y$, we have $\\mathbb{E}[f(y)] \\ge f(\\mathbb{E}[y])$. In this case, $\\mathbb{E}[e^y] \\ge e^{\\mathbb{E}[y]}$. Equality holds only if $y$ is a constant (i.e., has zero variance), which contradicts the premise that $y$ is an uncertain quantity dependent on $\\boldsymbol{\\xi}$. The mean of the log-transformed variable is not the log of the mean of the original variable. To compute the mean of $c$, one would need to evaluate the full expectation: $\\mathbb{E}[c] = \\mathbb{E}[\\exp(\\sum_{\\alpha=0}^{P} y_\\alpha \\Psi_\\alpha)]$. This is a complex function of all the PCE coefficients $\\{y_\\alpha\\}$, not just $y_0$.\n\nVerdict: **Incorrect**.\n\n**D. To enforce both positivity and saturation $0cc_{\\max}$, the logit transform $y=\\log\\!\\big(c/(c_{\\max}-c)\\big)$ may be used; in an intrusive PCE this induces additional nonlinearities in the projected system, but reconstructing $c$ from $y$ ensures the physical bounds are satisfied for all realizations of $\\boldsymbol{\\xi}$.**\n\nThis statement has three parts to check.\n1.  **Enforcing Bounds:** The logit transform maps the bounded interval $(0, c_{\\max})$ to the entire real line $(-\\infty, \\infty)$. The inverse transformation is $c = c_{\\max} \\frac{e^y}{1+e^y} = c_{\\max} \\frac{1}{1+e^{-y}}$. For any real value of $y$, the denominator $1+e^{-y}$ is always greater than $1$, so $c  c_{\\max}$. Also, since $e^{-y}0$, the denominator is positive, and so is the numerator, thus $c0$. So, the reconstruction $c(t,\\boldsymbol{\\xi}) = c_{\\max}/(1+e^{-y(t,\\boldsymbol{\\xi})})$ guarantees that $c$ will lie in the interval $(0, c_{\\max})$ for any real-valued PCE representation of $y$. This part is correct.\n2.  **Inducing Nonlinearities:** We need to derive the ODE for $y$. The time derivative is $\\frac{dy}{dt} = \\frac{dy}{dc}\\frac{dc}{dt}$. We have $\\frac{dy}{dc} = \\frac{c_{\\max}}{c(c_{\\max}-c)}$. So, $\\frac{dy}{dt} = \\frac{c_{\\max}}{c(c_{\\max}-c)} \\left( -k(\\boldsymbol{\\xi})c + s(t) \\right) = -k(\\boldsymbol{\\xi})\\frac{c_{\\max}}{c_{\\max}-c} + s(t)\\frac{c_{\\max}}{c(c_{\\max}-c)}$. Expressing the right-hand side in terms of $y$ using the inverse transform yields highly nonlinear terms such as $1+e^y$ and $(1+e^y)^2/e^y$. The resulting ODE for $y$ is strongly nonlinear, thus inducing nonlinearities in the projected Galerkin system. This part is correct.\n3.  **Reconstruction Ensures Bounds:** As established in the first point, because the PCE for $y$ yields a real-valued function, applying the inverse logit transform will always produce a value for $c$ strictly between $0$ and $c_{\\max}$. This part is correct.\n\nSince all parts of the statement are logically and mathematically sound, the entire statement is correct.\n\nVerdict: **Correct**.\n\n**E. Because $c=\\exp(y)$ is positive, one must switch the polynomial chaos basis to generalized Laguerre polynomials to represent $c$ as a lognormal variable; otherwise, the intrusive PCE will not converge for Gaussian-distributed inputs $\\boldsymbol{\\xi}$.**\n\nThis statement fundamentally misunderstands the principle of intrusive PCE. The choice of the orthogonal polynomial basis $\\{\\Psi_\\alpha(\\boldsymbol{\\xi})\\}$ is determined by the probability distribution of the **random inputs** $\\boldsymbol{\\xi}$, not the distribution of the output quantity of interest like $c(t, \\boldsymbol{\\xi})$. This is the core of the Wiener-Askey polynomial chaos scheme, which ensures orthogonality $\\langle \\Psi_\\alpha, \\Psi_\\beta \\rangle = \\delta_{\\alpha\\beta} \\langle \\Psi_\\alpha^2 \\rangle$ and optimizes the convergence of the expansion. If the inputs $\\boldsymbol{\\xi}$ are Gaussian, the correct and optimal basis is the Hermite polynomials, regardless of what transformation is applied to the state variable $c$. The method described in the problem is to solve for the coefficients of $y$ in the basis appropriate for $\\boldsymbol{\\xi}$. Switching the basis to Laguerre polynomials (which are orthogonal with respect to the Gamma distribution) would be incorrect and would violate the principles of the Galerkin projection unless the inputs $\\boldsymbol{\\xi}$ themselves followed a Gamma distribution. The fact that $c$ might have a distribution that resembles a log-normal distribution does not have any bearing on the choice of basis functions of the input variables $\\boldsymbol{\\xi}$. The claim that the PCE \"will not converge\" is also an unjustified and overly strong assertion; while using a mismatched basis degrades the convergence rate, it does not necessarily preclude convergence altogether. The primary error is the reason for switching the basis.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{BD}$$"
        }
    ]
}