{
    "hands_on_practices": [
        {
            "introduction": "When designing a battery pack, individual cells are often connected in parallel to increase total capacity. However, manufacturing processes inevitably lead to small variations in the capacity of each cell. This foundational exercise guides you through the process of using fundamental probability theory to quantify the total capacity and its expected variation for a parallel block of cells, a critical first step for predicting pack-level performance and ensuring reliability. ",
            "id": "3899181",
            "problem": "An automated battery pack design algorithm models cell-to-cell capacity dispersion when aggregating lithium-ion cells in parallel. Let the capacity of cell $i$ be a random variable $C_{i}$ with finite mean $\\mu_{i} = \\mathbb{E}[C_{i}]$ and finite variance $\\sigma_{i}^{2} = \\mathrm{Var}(C_{i})$. Cells are assembled in a parallel block so that the aggregated capacity is the random sum $C_{\\mathrm{tot}} = \\sum_{i=1}^{N} C_{i}$. Assume that all cells are mutually independent due to uncorrelated manufacturing variations across lots. Starting only from the fundamental definitions $\\mathbb{E}[X]$, $\\mathrm{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$, and the definition of independence $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$ for independent $X$ and $Y$, derive an expression for $\\mathrm{Var}\\!\\left(C_{\\mathrm{tot}}\\right)$ in terms of $\\{\\sigma_{i}^{2}\\}_{i=1}^{N}$.\n\nThen, consider a $N$-cell parallel block composed of three manufacturing bins with independent cells in each bin:\n- Bin A: $n_{A} = 5$ cells, each with mean capacity $\\mu_{A} = 3.00$ A·h and standard deviation $\\sigma_{A} = 0.05$ A·h.\n- Bin B: $n_{B} = 4$ cells, each with mean capacity $\\mu_{B} = 3.10$ A·h and standard deviation $\\sigma_{B} = 0.08$ A·h.\n- Bin C: $n_{C} = 3$ cells, each with mean capacity $\\mu_{C} = 2.95$ A·h and standard deviation $\\sigma_{C} = 0.04$ A·h.\n\nAssume all $n_{A} + n_{B} + n_{C}$ cells are mutually independent across and within bins. Using your derived expression, compute the coefficient of variation (CV), defined as $\\mathrm{CV} = \\sigma_{\\mathrm{tot}}/\\mu_{\\mathrm{tot}}$ where $\\mu_{\\mathrm{tot}} = \\mathbb{E}[C_{\\mathrm{tot}}]$ and $\\sigma_{\\mathrm{tot}} = \\sqrt{\\mathrm{Var}(C_{\\mathrm{tot}})}$, for the aggregated parallel block.\n\nRound your final numerical answer for the coefficient of variation to four significant figures and express it as a pure decimal number without units.",
            "solution": "The problem consists of two parts. First, we must derive an expression for the variance of a sum of mutually independent random variables. Second, we must apply this result to a specific numerical example involving a parallel block of battery cells.\n\nLet us begin with the derivation. We are given a set of $N$ mutually independent random variables $\\{C_{i}\\}_{i=1}^{N}$, where $C_{i}$ represents the capacity of cell $i$. The mean of each random variable is $\\mu_{i} = \\mathbb{E}[C_{i}]$ and the variance is $\\sigma_{i}^{2} = \\mathrm{Var}(C_{i})$. The total capacity of the parallel block is the sum $C_{\\mathrm{tot}} = \\sum_{i=1}^{N} C_{i}$. We need to find $\\mathrm{Var}(C_{\\mathrm{tot}})$.\n\nWe start from the fundamental definition of variance provided:\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = \\mathbb{E}[C_{\\mathrm{tot}}^{2}] - (\\mathbb{E}[C_{\\mathrm{tot}}])^{2}\n$$\n\nFirst, we compute the expected value of the total capacity, $\\mathbb{E}[C_{\\mathrm{tot}}]$. By the linearity of the expectation operator:\n$$\n\\mathbb{E}[C_{\\mathrm{tot}}] = \\mathbb{E}\\left[\\sum_{i=1}^{N} C_{i}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[C_{i}] = \\sum_{i=1}^{N} \\mu_{i}\n$$\n\nNext, we compute the expected value of the square of the total capacity, $\\mathbb{E}[C_{\\mathrm{tot}}^{2}]$. We expand the square of the sum:\n$$\nC_{\\mathrm{tot}}^{2} = \\left(\\sum_{i=1}^{N} C_{i}\\right)^{2} = \\left(\\sum_{i=1}^{N} C_{i}\\right)\\left(\\sum_{j=1}^{N} C_{j}\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} C_{i}C_{j}\n$$\nWe can separate the sum into terms where the indices are equal ($i=j$) and terms where they are not ($i \\neq j$):\n$$\nC_{\\mathrm{tot}}^{2} = \\sum_{i=1}^{N} C_{i}^{2} + \\sum_{i \\neq j} C_{i}C_{j}\n$$\nNow, we apply the expectation operator to this expression. Using its linearity:\n$$\n\\mathbb{E}[C_{\\mathrm{tot}}^{2}] = \\mathbb{E}\\left[\\sum_{i=1}^{N} C_{i}^{2} + \\sum_{i \\neq j} C_{i}C_{j}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[C_{i}^{2}] + \\sum_{i \\neq j} \\mathbb{E}[C_{i}C_{j}]\n$$\nThe problem states that the cells are mutually independent. A consequence of this, as given in the problem, is that for any two distinct independent random variables $C_{i}$ and $C_{j}$ (where $i \\neq j$), we have $\\mathbb{E}[C_{i}C_{j}] = \\mathbb{E}[C_{i}]\\mathbb{E}[C_{j}] = \\mu_{i}\\mu_{j}$. Substituting this into our expression for $\\mathbb{E}[C_{\\mathrm{tot}}^{2}]$:\n$$\n\\mathbb{E}[C_{\\mathrm{tot}}^{2}] = \\sum_{i=1}^{N} \\mathbb{E}[C_{i}^{2}] + \\sum_{i \\neq j} \\mu_{i}\\mu_{j}\n$$\nNow we substitute our results for $\\mathbb{E}[C_{\\mathrm{tot}}]$ and $\\mathbb{E}[C_{\\mathrm{tot}}^{2}]$ back into the variance definition. Let's first evaluate $(\\mathbb{E}[C_{\\mathrm{tot}}])^{2}$:\n$$\n(\\mathbb{E}[C_{\\mathrm{tot}}])^{2} = \\left(\\sum_{i=1}^{N} \\mu_{i}\\right)^{2} = \\sum_{i=1}^{N} \\mu_{i}^{2} + \\sum_{i \\neq j} \\mu_{i}\\mu_{j}\n$$\nSo, the variance is:\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = \\left(\\sum_{i=1}^{N} \\mathbb{E}[C_{i}^{2}] + \\sum_{i \\neq j} \\mu_{i}\\mu_{j}\\right) - \\left(\\sum_{i=1}^{N} \\mu_{i}^{2} + \\sum_{i \\neq j} \\mu_{i}\\mu_{j}\\right)\n$$\nThe cross-product terms cancel out:\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = \\sum_{i=1}^{N} \\mathbb{E}[C_{i}^{2}] - \\sum_{i=1}^{N} \\mu_{i}^{2} = \\sum_{i=1}^{N} \\left(\\mathbb{E}[C_{i}^{2}] - \\mu_{i}^{2}\\right)\n$$\nRecalling that $\\mu_{i} = \\mathbb{E}[C_{i}]$, we can write this as:\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = \\sum_{i=1}^{N} \\left(\\mathbb{E}[C_{i}^{2}] - (\\mathbb{E}[C_{i}])^{2}\\right)\n$$\nThe term inside the summation is the definition of the variance of $C_{i}$, $\\mathrm{Var}(C_{i}) = \\sigma_{i}^{2}$. Therefore, we arrive at the final expression for the variance of the sum of independent random variables:\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = \\sum_{i=1}^{N} \\sigma_{i}^{2}\n$$\nThis is the well-known Bienaymé formula, which we have derived from first principles as requested.\n\nNow, we proceed to the second part of the problem: computing the coefficient of variation (CV) for a specific parallel block. The block consists of cells from three bins:\n- Bin A: $n_{A} = 5$ cells, $\\mu_{A} = 3.00$, $\\sigma_{A} = 0.05$. The variance is $\\sigma_{A}^{2} = (0.05)^{2} = 0.0025$.\n- Bin B: $n_{B} = 4$ cells, $\\mu_{B} = 3.10$, $\\sigma_{B} = 0.08$. The variance is $\\sigma_{B}^{2} = (0.08)^{2} = 0.0064$.\n- Bin C: $n_{C} = 3$ cells, $\\mu_{C} = 2.95$, $\\sigma_{C} = 0.04$. The variance is $\\sigma_{C}^{2} = (0.04)^{2} = 0.0016$.\n\nThe total number of cells is $N = n_{A} + n_{B} + n_{C} = 5 + 4 + 3 = 12$. All $12$ cells are mutually independent.\n\nFirst, we calculate the total mean capacity, $\\mu_{\\mathrm{tot}} = \\mathbb{E}[C_{\\mathrm{tot}}]$. Using the linearity of expectation, the total mean is the sum of the individual means:\n$$\n\\mu_{\\mathrm{tot}} = n_{A}\\mu_{A} + n_{B}\\mu_{B} + n_{C}\\mu_{C}\n$$\n$$\n\\mu_{\\mathrm{tot}} = 5(3.00) + 4(3.10) + 3(2.95) = 15.00 + 12.40 + 8.85 = 36.25 \\text{ A·h}\n$$\nNext, we calculate the total variance, $\\mathrm{Var}(C_{\\mathrm{tot}})$. Using the formula we derived, the total variance is the sum of the individual variances:\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = \\sum_{i=1}^{N} \\sigma_{i}^{2} = n_{A}\\sigma_{A}^{2} + n_{B}\\sigma_{B}^{2} + n_{C}\\sigma_{C}^{2}\n$$\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = 5(0.0025) + 4(0.0064) + 3(0.0016)\n$$\n$$\n\\mathrm{Var}(C_{\\mathrm{tot}}) = 0.0125 + 0.0256 + 0.0048 = 0.0429 \\text{ (A·h)}^{2}\n$$\nThe total standard deviation is the square root of the total variance:\n$$\n\\sigma_{\\mathrm{tot}} = \\sqrt{\\mathrm{Var}(C_{\\mathrm{tot}})} = \\sqrt{0.0429} \\text{ A·h}\n$$\nThe coefficient of variation (CV) is defined as the ratio of the total standard deviation to the total mean:\n$$\n\\mathrm{CV} = \\frac{\\sigma_{\\mathrm{tot}}}{\\mu_{\\mathrm{tot}}} = \\frac{\\sqrt{0.0429}}{36.25}\n$$\nNow, we compute the numerical value:\n$$\n\\mathrm{CV} \\approx \\frac{0.20712315}{36.25} \\approx 0.005713742\n$$\nThe problem requires rounding the result to four significant figures. The first significant figure is $5$. The fourth is $3$. The next digit is $7$, so we round up.\n$$\n\\mathrm{CV} \\approx 0.005714\n$$\nThis is a dimensionless quantity, as required.",
            "answer": "$$\n\\boxed{0.005714}\n$$"
        },
        {
            "introduction": "Electrochemical Impedance Spectroscopy (EIS) is a powerful non-destructive technique for diagnosing a battery's internal state. The features of an EIS spectrum, such as the characteristic frequency of the main semicircle, are directly linked to underlying physical parameters. This practice explores how cell-to-cell dispersion in these parameters, specifically charge-transfer resistance $R_{ct}$ and double-layer capacitance $C_{dl}$, affects the measured spectrum. By applying a Taylor series expansion, you will derive how this microscopic parameter uncertainty propagates to create a predictable shift in a key macroscopic diagnostic feature. ",
            "id": "3899235",
            "problem": "In an automated battery design and simulation workflow for lithium-ion cells, the high-frequency portion of the Electrochemical Impedance Spectroscopy (EIS) is modeled by a Randles circuit approximation consisting of a series ohmic resistance $R_{s}$ in series with a parallel branch of charge-transfer resistance $R_{ct}$ and double-layer capacitance $C_{dl}$. Over the frequency window of interest, diffusion contributions are negligible. Define the impedance feature of interest as the frequency $f^{\\ast}$ (in hertz) at which the magnitude of the imaginary part of the total impedance attains its maximum (the apex of the high-frequency Nyquist semicircle).\n\nCell-to-cell parameter dispersion is modeled as independent, small, zero-mean relative variations in $R_{ct}$ and $C_{dl}$:\n$$\nR_{ct} = \\mu_{R}\\left(1+\\delta_{R}\\right), \\qquad C_{dl} = \\mu_{C}\\left(1+\\delta_{C}\\right),\n$$\nwith $\\mathbb{E}[\\delta_{R}] = 0$, $\\mathbb{E}[\\delta_{C}] = 0$, $\\mathrm{Var}(\\delta_{R}) = v_{R}^{2}$, $\\mathrm{Var}(\\delta_{C}) = v_{C}^{2}$, and $\\mathrm{Cov}(\\delta_{R},\\delta_{C})=0$. Assume that $|\\delta_{R}| \\ll 1$ and $|\\delta_{C}| \\ll 1$ so that a second-order expansion in $\\delta_{R}$ and $\\delta_{C}$ is valid.\n\nStarting only from fundamental impedance definitions for a resistor and a capacitor and the rules for series and parallel combinations, derive the expression for $f^{\\ast}$ in terms of $R_{ct}$ and $C_{dl}$ by locating the maximum of the magnitude of the imaginary part of the total impedance with respect to angular frequency $\\omega$. Then, using a second-order small-variation expansion and the above stochastic model, compute the expected value $\\mathbb{E}[f^{\\ast}]$ to second order in $v_{R}$ and $v_{C}$.\n\nUse the parameter values $\\mu_{R} = 0.5$ ohms, $\\mu_{C} = 5.0 \\times 10^{-3}$ farads, $v_{R} = 0.10$, and $v_{C} = 0.20$. Express the final result $\\mathbb{E}[f^{\\ast}]$ in hertz. State the relationship between angular frequency and frequency as $\\omega = 2\\pi f$. Round your final numerical answer to four significant figures.",
            "solution": "### Derivation of Characteristic Frequency $f^{\\ast}$\nThe impedance of a resistor $R$ is $Z_R = R$, and the impedance of a capacitor $C$ at an angular frequency $\\omega$ is $Z_C = \\frac{1}{j\\omega C}$, where $j$ is the imaginary unit.\n\nThe parallel branch consists of the charge-transfer resistance $R_{ct}$ and the double-layer capacitance $C_{dl}$. Its impedance, $Z_p$, is given by the rule for parallel components:\n$$ \\frac{1}{Z_p} = \\frac{1}{Z_{R_{ct}}} + \\frac{1}{Z_{C_{dl}}} = \\frac{1}{R_{ct}} + \\frac{1}{1/(j\\omega C_{dl})} = \\frac{1}{R_{ct}} + j\\omega C_{dl} $$\nSolving for $Z_p$:\n$$ Z_p = \\frac{1}{\\frac{1}{R_{ct}} + j\\omega C_{dl}} = \\frac{R_{ct}}{1 + j\\omega R_{ct} C_{dl}} $$\nTo separate the real and imaginary parts, we multiply the numerator and denominator by the complex conjugate of the denominator:\n$$ Z_p = \\frac{R_{ct}}{1 + j\\omega R_{ct} C_{dl}} \\cdot \\frac{1 - j\\omega R_{ct} C_{dl}}{1 - j\\omega R_{ct} C_{dl}} = \\frac{R_{ct}(1 - j\\omega R_{ct} C_{dl})}{1 - (j\\omega R_{ct} C_{dl})^2} = \\frac{R_{ct} - j\\omega R_{ct}^2 C_{dl}}{1 + \\omega^2 R_{ct}^2 C_{dl}^2} $$\nThe total impedance of the circuit is $Z_{total} = R_s + Z_p$:\n$$ Z_{total} = R_s + \\frac{R_{ct}}{1 + \\omega^2 R_{ct}^2 C_{dl}^2} - j \\frac{\\omega R_{ct}^2 C_{dl}}{1 + \\omega^2 R_{ct}^2 C_{dl}^2} $$\nThe imaginary part of the total impedance is:\n$$ \\mathrm{Im}(Z_{total}) = - \\frac{\\omega R_{ct}^2 C_{dl}}{1 + \\omega^2 R_{ct}^2 C_{dl}^2} $$\nThe problem defines $f^{\\ast}$ as the frequency at which the magnitude of the imaginary part, which we denote as $Y(\\omega)=|\\mathrm{Im}(Z_{total})|$, reaches its maximum.\n$$ Y(\\omega) = \\frac{\\omega R_{ct}^2 C_{dl}}{1 + \\omega^2 R_{ct}^2 C_{dl}^2} $$\nTo find the maximum, we compute the derivative of $Y(\\omega)$ with respect to $\\omega$ and set it to zero. Using the quotient rule, $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$:\n$$ \\frac{dY}{d\\omega} = \\frac{(R_{ct}^2 C_{dl})(1 + \\omega^2 R_{ct}^2 C_{dl}^2) - (\\omega R_{ct}^2 C_{dl})(2\\omega R_{ct}^2 C_{dl}^2)}{(1 + \\omega^2 R_{ct}^2 C_{dl}^2)^2} = 0 $$\nFor the derivative to be zero, the numerator must be zero (assuming a non-zero denominator):\n$$ R_{ct}^2 C_{dl} (1 + \\omega^2 R_{ct}^2 C_{dl}^2) - 2\\omega^2 R_{ct}^4 C_{dl}^3 = 0 $$\nSince $R_{ct} > 0$ and $C_{dl} > 0$, we can divide by $R_{ct}^2 C_{dl}$:\n$$ 1 + \\omega^2 R_{ct}^2 C_{dl}^2 - 2\\omega^2 R_{ct}^2 C_{dl}^2 = 0 $$\n$$ 1 - \\omega^2 R_{ct}^2 C_{dl}^2 = 0 $$\nThis gives the angular frequency $\\omega^{\\ast}$ at the maximum:\n$$ \\omega^{\\ast} = \\frac{1}{R_{ct} C_{dl}} $$\nUsing the relation $\\omega = 2\\pi f$, the characteristic frequency $f^{\\ast}$ is:\n$$ f^{\\ast} = \\frac{\\omega^{\\ast}}{2\\pi} = \\frac{1}{2\\pi R_{ct} C_{dl}} $$\nThis completes the first part of the task.\n\n### Derivation of the Expected Value $\\mathbb{E}[f^{\\ast}]$\nWe now introduce the stochastic models for $R_{ct}$ and $C_{dl}$:\n$$ f^{\\ast} = \\frac{1}{2\\pi [\\mu_{R}(1+\\delta_{R})][\\mu_{C}(1+\\delta_{C})]} $$\nLet's define the nominal frequency $f_0 = \\frac{1}{2\\pi \\mu_{R} \\mu_{C}}$. The expression for $f^{\\ast}$ becomes:\n$$ f^{\\ast} = f_0 \\frac{1}{(1+\\delta_{R})(1+\\delta_{C})} = f_0 (1+\\delta_{R})^{-1} (1+\\delta_{C})^{-1} $$\nGiven that $|\\delta_{R}| \\ll 1$ and $|\\delta_{C}| \\ll 1$, we can use the Taylor series expansion for $(1+x)^{-1} \\approx 1 - x + x^2 - \\dots$. We expand up to second-order terms:\n$$ (1+\\delta_{R})^{-1} \\approx 1 - \\delta_{R} + \\delta_{R}^2 $$\n$$ (1+\\delta_{C})^{-1} \\approx 1 - \\delta_{C} + \\delta_{C}^2 $$\nMultiplying these two expansions and keeping terms up to the second order in $\\delta_{R}$ and $\\delta_{C}$:\n$$ (1+\\delta_{R})^{-1} (1+\\delta_{C})^{-1} \\approx (1 - \\delta_{R} + \\delta_{R}^2)(1 - \\delta_{C} + \\delta_{C}^2) $$\n$$ \\approx 1 - \\delta_{C} + \\delta_{C}^2 - \\delta_{R} + \\delta_{R}\\delta_{C} - \\delta_{R}\\delta_{C}^2 + \\delta_{R}^2 - \\delta_{R}^2\\delta_{C} + \\delta_{R}^2\\delta_{C}^2 $$\nThe terms of order higher than two ($\\delta_{R}\\delta_{C}^2$, $\\delta_{R}^2\\delta_{C}$, $\\delta_{R}^2\\delta_{C}^2$) are neglected.\n$$ (1+\\delta_{R})^{-1} (1+\\delta_{C})^{-1} \\approx 1 - \\delta_{R} - \\delta_{C} + \\delta_{R}^2 + \\delta_{C}^2 + \\delta_{R}\\delta_{C} $$\nThus, the second-order expansion for $f^{\\ast}$ is:\n$$ f^{\\ast} \\approx f_0 (1 - \\delta_{R} - \\delta_{C} + \\delta_{R}^2 + \\delta_{C}^2 + \\delta_{R}\\delta_{C}) $$\nNow we take the expected value, using the linearity of the expectation operator $\\mathbb{E}$:\n$$ \\mathbb{E}[f^{\\ast}] \\approx f_0 \\mathbb{E}[1 - \\delta_{R} - \\delta_{C} + \\delta_{R}^2 + \\delta_{C}^2 + \\delta_{R}\\delta_{C}] $$\n$$ \\mathbb{E}[f^{\\ast}] \\approx f_0 (1 - \\mathbb{E}[\\delta_{R}] - \\mathbb{E}[\\delta_{C}] + \\mathbb{E}[\\delta_{R}^2] + \\mathbb{E}[\\delta_{C}^2] + \\mathbb{E}[\\delta_{R}\\delta_{C}]) $$\nWe use the provided statistical properties. For a zero-mean variable $X$, $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{E}[X^2]$.\n- $\\mathbb{E}[\\delta_{R}] = 0$\n- $\\mathbb{E}[\\delta_{C}] = 0$\n- $\\mathbb{E}[\\delta_{R}^2] = \\mathrm{Var}(\\delta_{R}) = v_{R}^2$\n- $\\mathbb{E}[\\delta_{C}^2] = \\mathrm{Var}(\\delta_{C}) = v_{C}^2$\nFor two uncorrelated variables $X$ and $Y$, $\\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0$. Since $\\mathbb{E}[\\delta_{R}] = 0$ and $\\mathbb{E}[\\delta_{C}] = 0$:\n- $\\mathbb{E}[\\delta_{R}\\delta_{C}] = \\mathrm{Cov}(\\delta_{R},\\delta_{C}) + \\mathbb{E}[\\delta_{R}]\\mathbb{E}[\\delta_{C}] = 0 + (0)(0) = 0$\nSubstituting these into the expression for $\\mathbb{E}[f^{\\ast}]$:\n$$ \\mathbb{E}[f^{\\ast}] \\approx f_0 (1 - 0 - 0 + v_{R}^2 + v_{C}^2 + 0) $$\n$$ \\mathbb{E}[f^{\\ast}] \\approx f_0(1 + v_{R}^2 + v_{C}^2) = \\frac{1}{2\\pi \\mu_{R} \\mu_{C}}(1 + v_{R}^2 + v_{C}^2) $$\n\n### Numerical Calculation\nWe now substitute the given numerical values:\n$\\mu_{R} = 0.5$, $\\mu_{C} = 5.0 \\times 10^{-3}$, $v_{R} = 0.10$, and $v_{C} = 0.20$.\nFirst, calculate the product $\\mu_{R} \\mu_{C}$:\n$$ \\mu_{R} \\mu_{C} = (0.5)(5.0 \\times 10^{-3}) = 2.5 \\times 10^{-3} $$\nNext, calculate the term $(1 + v_{R}^2 + v_{C}^2)$:\n$$ v_{R}^2 = (0.10)^2 = 0.01 $$\n$$ v_{C}^2 = (0.20)^2 = 0.04 $$\n$$ 1 + v_{R}^2 + v_{C}^2 = 1 + 0.01 + 0.04 = 1.05 $$\nNow, combine these results to find $\\mathbb{E}[f^{\\ast}]$:\n$$ \\mathbb{E}[f^{\\ast}] \\approx \\frac{1}{2\\pi (2.5 \\times 10^{-3})} (1.05) = \\frac{1.05}{5.0 \\pi \\times 10^{-3}} = \\frac{1050}{5.0 \\pi} = \\frac{210}{\\pi} $$\nCalculating the numerical value:\n$$ \\mathbb{E}[f^{\\ast}] \\approx \\frac{210}{3.14159265...} \\approx 66.845076... $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\mathbb{E}[f^{\\ast}] \\approx 66.85 $$",
            "answer": "$$\n\\boxed{66.85}\n$$"
        },
        {
            "introduction": "Accurate State of Charge (SOC) estimation is a cornerstone of any modern Battery Management System (BMS). Advanced estimators like the Extended Kalman Filter (EKF) rely on an internal model of the cell, but what happens when the real cell's parameters differ from the model's nominal values due to manufacturing dispersion? This advanced practice uncovers a subtle but critical consequence: the introduction of a systematic bias in the SOC estimate. You will derive an analytical expression for this bias, revealing how the interplay between parameter uncertainty and the nonlinearity of the cell's voltage curve can degrade the performance of the estimation algorithm. ",
            "id": "3899242",
            "problem": "Consider a lithium-ion cell whose State of Charge (SOC), denoted by $x \\in (0,1)$, evolves according to the Coulomb counting relation $x_{k+1} = x_k - \\eta I_k \\Delta t / Q$ with known charge efficiency $\\eta$, current $I_k$, time step $\\Delta t$, and capacity $Q$. At a given sampling instant $k$, suppose an Extended Kalman Filter (EKF) is used to estimate the SOC from the terminal voltage measurement modeled as $V_k = f(x_k, \\boldsymbol{\\theta}) + R I_k + v_k$, where $f(x,\\boldsymbol{\\theta})$ is the Open-Circuit Voltage (OCV) function parameterized by $\\boldsymbol{\\theta}$, $R$ is the internal resistance, and $v_k$ is the measurement noise. Let Open-Circuit Voltage (OCV) parameters $\\boldsymbol{\\theta}$ exhibit cell-to-cell dispersion around a nominal value $\\bar{\\boldsymbol{\\theta}}$ due to manufacturing variability, so that the true cell parameters satisfy $\\boldsymbol{\\theta} = \\bar{\\boldsymbol{\\theta}} + \\Delta \\boldsymbol{\\theta}$, where $\\Delta \\boldsymbol{\\theta}$ is a zero-mean random vector with covariance $\\boldsymbol{\\Sigma}_{\\theta}$. The EKF is designed and operated with the nominal parameter $\\bar{\\boldsymbol{\\theta}}$ and the known resistance $R$. Assume $I_k$ is known, $R I_k$ is perfectly compensated inside the filter, and the measurement noise is negligible at the considered instant (i.e., $v_k \\approx 0$). Let the SOC estimate after the measurement update be $\\hat{x}_k$, and let the true SOC be $x_k$, and define the estimation error $e \\triangleq \\hat{x}_k - x_k$. \n\nExplain, starting from the measurement consistency condition used by the EKF at the update step and using a second-order Taylor expansion, how dispersion in OCV parameters $\\Delta \\boldsymbol{\\theta}$ biases SOC estimation, and derive the expected estimation error $\\mathbb{E}[e]$ as a function of the OCV variability encoded in $\\boldsymbol{\\Sigma}_{\\theta}$. Specialize your derivation to the scientifically realistic polynomial OCV parameterization\n$$\nf(x,\\boldsymbol{\\theta}) = \\theta_0 + \\theta_1 x + \\theta_2 x^2,\n$$\nwith nominal parameters $\\bar{\\boldsymbol{\\theta}} = (\\bar{\\theta}_0,\\bar{\\theta}_1,\\bar{\\theta}_2)^{\\top}$. Assume the analysis is conducted at a fixed true SOC $x_k$ and that the EKF update is sufficiently high-gain such that the updated estimate $\\hat{x}_k$ enforces the measurement consistency $f(\\hat{x}_k,\\bar{\\boldsymbol{\\theta}}) \\approx f(x_k,\\boldsymbol{\\theta})$. Provide the final analytic expression for $\\mathbb{E}[e]$ to second order in $\\Delta \\boldsymbol{\\theta}$ in terms of $x_k$, $\\bar{\\boldsymbol{\\theta}}$, and $\\boldsymbol{\\Sigma}_{\\theta}$. Express your final answer as a single closed-form expression. No numerical rounding is required. SOC is dimensionless; express the final answer without units.",
            "solution": "The analysis begins with the measurement consistency condition, which, based on the problem's high-gain assumption, equates the estimated voltage (using the estimated state $\\hat{x}_k$ and nominal parameters $\\bar{\\boldsymbol{\\theta}}$) with the true voltage (using the true state $x_k$ and true parameters $\\boldsymbol{\\theta}$). The problem states that the $R I_k$ term is compensated and noise $v_k$ is negligible, so the voltage is given solely by the OCV function. Thus, we have:\n$$\nf(\\hat{x}_k, \\bar{\\boldsymbol{\\theta}}) = f(x_k, \\boldsymbol{\\theta})\n$$\nWe introduce the estimation error $e \\triangleq \\hat{x}_k - x_k$, which implies $\\hat{x}_k = x_k + e$. We also use the given relation for the parameters $\\boldsymbol{\\theta} = \\bar{\\boldsymbol{\\theta}} + \\Delta \\boldsymbol{\\theta}$. Substituting these into the consistency condition gives:\n$$\nf(x_k + e, \\bar{\\boldsymbol{\\theta}}) = f(x_k, \\bar{\\boldsymbol{\\theta}} + \\Delta \\boldsymbol{\\theta})\n$$\nTo analyze the relationship between the error $e$ and the parameter deviation $\\Delta \\boldsymbol{\\theta}$, we perform a second-order Taylor expansion on both sides of the equation.\n\nThe left-hand side (LHS) is expanded around the point $(x_k, \\bar{\\boldsymbol{\\theta}})$ with respect to the state $x_k$:\n$$\nf(x_k + e, \\bar{\\boldsymbol{\\theta}}) \\approx f(x_k, \\bar{\\boldsymbol{\\theta}}) + \\frac{\\partial f}{\\partial x} e + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} e^2\n$$\nwhere the partial derivatives are evaluated at $(x_k, \\bar{\\boldsymbol{\\theta}})$.\n\nThe right-hand side (RHS) is expanded around the point $(x_k, \\bar{\\boldsymbol{\\theta}})$ with respect to the parameters $\\boldsymbol{\\theta}$:\n$$\nf(x_k, \\bar{\\boldsymbol{\\theta}} + \\Delta \\boldsymbol{\\theta}) \\approx f(x_k, \\bar{\\boldsymbol{\\theta}}) + (\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta} + \\frac{1}{2} (\\Delta \\boldsymbol{\\theta})^{\\top} (\\mathbf{H}_{\\theta} f) (\\Delta \\boldsymbol{\\theta})\n$$\nwhere $\\nabla_{\\theta} f$ is the gradient vector of $f$ with respect to $\\boldsymbol{\\theta}$ and $\\mathbf{H}_{\\theta} f$ is the Hessian matrix of $f$ with respect to $\\boldsymbol{\\theta}$, both evaluated at $(x_k, \\bar{\\boldsymbol{\\theta}})$.\n\nEquating the two expansions and cancelling the common term $f(x_k, \\bar{\\boldsymbol{\\theta}})$ yields:\n$$\n\\frac{\\partial f}{\\partial x} e + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} e^2 \\approx (\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta} + \\frac{1}{2} (\\Delta \\boldsymbol{\\theta})^{\\top} (\\mathbf{H}_{\\theta} f) (\\Delta \\boldsymbol{\\theta})\n$$\nThe error $e$ is induced by the parameter perturbation $\\Delta \\boldsymbol{\\theta}$. To a first approximation, $e$ is linear in $\\Delta \\boldsymbol{\\theta}$. We can thus establish a perturbative solution for $e$.\nFrom the first-order terms, we have:\n$$\n\\frac{\\partial f}{\\partial x} e \\approx (\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta} \\implies e \\approx \\left(\\frac{\\partial f}{\\partial x}\\right)^{-1} (\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta}\n$$\nThis shows that $e$ is of order $\\mathcal{O}(\\|\\Delta \\boldsymbol{\\theta}\\|)$. Consequently, the term $e^2$ is of order $\\mathcal{O}(\\|\\Delta \\boldsymbol{\\theta}\\|^2)$.\nTo find the expected error $\\mathbb{E}[e]$, we take the expectation of the full second-order equation:\n$$\n\\mathbb{E}\\left[\\frac{\\partial f}{\\partial x} e + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} e^2\\right] \\approx \\mathbb{E}\\left[(\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta} + \\frac{1}{2} (\\Delta \\boldsymbol{\\theta})^{\\top} (\\mathbf{H}_{\\theta} f) (\\Delta \\boldsymbol{\\theta})\\right]\n$$\nThe derivatives are deterministic quantities evaluated at the known point $(x_k, \\bar{\\boldsymbol{\\theta}})$. Using the linearity of expectation:\n$$\n\\frac{\\partial f}{\\partial x} \\mathbb{E}[e] + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} \\mathbb{E}[e^2] \\approx (\\nabla_{\\theta} f)^{\\top} \\mathbb{E}[\\Delta \\boldsymbol{\\theta}] + \\frac{1}{2} \\mathbb{E}[(\\Delta \\boldsymbol{\\theta})^{\\top} (\\mathbf{H}_{\\theta} f) (\\Delta \\boldsymbol{\\theta})]\n$$\nWe are given that $\\mathbb{E}[\\Delta \\boldsymbol{\\theta}] = \\mathbf{0}$. The expected error $\\mathbb{E}[e]$ is a bias, which we expect to be of second order. To compute $\\mathbb{E}[e^2]$ to the lowest non-zero order, we use the first-order approximation for $e$:\n$$\n\\mathbb{E}[e^2] \\approx \\mathbb{E}\\left[ \\left( \\left(\\frac{\\partial f}{\\partial x}\\right)^{-1} (\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta} \\right)^2 \\right] = \\left(\\frac{\\partial f}{\\partial x}\\right)^{-2} \\mathbb{E}\\left[ ((\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta}) ((\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta})^{\\top} \\right] \\\\\n= \\left(\\frac{\\partial f}{\\partial x}\\right)^{-2} \\mathbb{E}\\left[ (\\nabla_{\\theta} f)^{\\top} \\Delta \\boldsymbol{\\theta} (\\Delta \\boldsymbol{\\theta})^{\\top} \\nabla_{\\theta} f \\right] = \\left(\\frac{\\partial f}{\\partial x}\\right)^{-2} (\\nabla_{\\theta} f)^{\\top} \\mathbb{E}[\\Delta \\boldsymbol{\\theta} (\\Delta \\boldsymbol{\\theta})^{\\top}] \\nabla_{\\theta} f \\\\\n= \\left(\\frac{\\partial f}{\\partial x}\\right)^{-2} (\\nabla_{\\theta} f)^{\\top} \\boldsymbol{\\Sigma}_{\\theta} \\nabla_{\\theta} f\n$$\nFor the quadratic form on the RHS, we use the general property $\\mathbb{E}[\\mathbf{z}^{\\top}\\mathbf{A}\\mathbf{z}] = \\text{tr}(\\mathbf{A}\\boldsymbol{\\Sigma}_{\\mathbf{z}}) + \\boldsymbol{\\mu}_{\\mathbf{z}}^{\\top}\\mathbf{A}\\boldsymbol{\\mu}_{\\mathbf{z}}$. For $\\mathbf{z}=\\Delta\\boldsymbol{\\theta}$, the mean $\\boldsymbol{\\mu}_{\\mathbf{z}}=\\mathbf{0}$ and covariance is $\\boldsymbol{\\Sigma}_{\\theta}$. Thus, $\\mathbb{E}[(\\Delta \\boldsymbol{\\theta})^{\\top} (\\mathbf{H}_{\\theta} f) (\\Delta \\boldsymbol{\\theta})] = \\text{tr}((\\mathbf{H}_{\\theta} f) \\boldsymbol{\\Sigma}_{\\theta})$.\nSubstituting these results back into the expectation equation:\n$$\n\\frac{\\partial f}{\\partial x} \\mathbb{E}[e] + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} \\left( \\left(\\frac{\\partial f}{\\partial x}\\right)^{-2} (\\nabla_{\\theta} f)^{\\top} \\boldsymbol{\\Sigma}_{\\theta} \\nabla_{\\theta} f \\right) \\approx \\mathbf{0} + \\frac{1}{2} \\text{tr}((\\mathbf{H}_{\\theta} f) \\boldsymbol{\\Sigma}_{\\theta})\n$$\nSolving for $\\mathbb{E}[e]$:\n$$\n\\mathbb{E}[e] \\approx \\left(\\frac{\\partial f}{\\partial x}\\right)^{-1} \\left[ \\frac{1}{2} \\text{tr}((\\mathbf{H}_{\\theta} f) \\boldsymbol{\\Sigma}_{\\theta}) - \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} \\left(\\frac{\\partial f}{\\partial x}\\right)^{-2} (\\nabla_{\\theta} f)^{\\top} \\boldsymbol{\\Sigma}_{\\theta} \\nabla_{\\theta} f \\right]\n$$\n$$\n\\mathbb{E}[e] \\approx \\frac{\\text{tr}((\\mathbf{H}_{\\theta} f) \\boldsymbol{\\Sigma}_{\\theta})}{2 \\frac{\\partial f}{\\partial x}} - \\frac{\\frac{\\partial^2 f}{\\partial x^2}}{2 \\left(\\frac{\\partial f}{\\partial x}\\right)^3} (\\nabla_{\\theta} f)^{\\top} \\boldsymbol{\\Sigma}_{\\theta} (\\nabla_{\\theta} f)\n$$\nThis is the general expression for the expected estimation error (bias) to the second order.\n\nNow, we specialize this result to the given polynomial OCV model:\n$$\nf(x_k, \\boldsymbol{\\theta}) = \\theta_0 + \\theta_1 x_k + \\theta_2 x_k^2\n$$\nWe compute the required derivatives, which are evaluated at $(x_k, \\bar{\\boldsymbol{\\theta}})$:\n1. First derivative with respect to $x$:\n$$\n\\frac{\\partial f}{\\partial x} = \\bar{\\theta}_1 + 2 \\bar{\\theta}_2 x_k\n$$\n2. Second derivative with respect to $x$:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = 2 \\bar{\\theta}_2\n$$\n3. Gradient with respect to $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2)^{\\top}$:\n$$\n\\nabla_{\\theta} f = \\begin{pmatrix} \\frac{\\partial f}{\\partial \\theta_0} \\\\ \\frac{\\partial f}{\\partial \\theta_1} \\\\ \\frac{\\partial f}{\\partial \\theta_2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ x_k \\\\ x_k^2 \\end{pmatrix}\n$$\n4. Hessian with respect to $\\boldsymbol{\\theta}$: Since $f$ is linear in each component of $\\boldsymbol{\\theta}$, all second-order partial derivatives with respect to $\\boldsymbol{\\theta}$ are zero.\n$$\n\\mathbf{H}_{\\theta} f = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}^{\\top}} (\\nabla_{\\theta} f) = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\mathbf{0}\n$$\nThe fact that $\\mathbf{H}_{\\theta} f = \\mathbf{0}$ simplifies the general expression for $\\mathbb{E}[e]$ significantly. The first term, involving the trace, becomes zero:\n$$\n\\frac{\\text{tr}(\\mathbf{0} \\cdot \\boldsymbol{\\Sigma}_{\\theta})}{2 \\frac{\\partial f}{\\partial x}} = 0\n$$\nThe expected error is thus given by the second term alone:\n$$\n\\mathbb{E}[e] \\approx - \\frac{\\frac{\\partial^2 f}{\\partial x^2}}{2 \\left(\\frac{\\partial f}{\\partial x}\\right)^3} (\\nabla_{\\theta} f)^{\\top} \\boldsymbol{\\Sigma}_{\\theta} (\\nabla_{\\theta} f)\n$$\nSubstituting the calculated derivatives for the polynomial model:\n$$\n\\mathbb{E}[e] \\approx - \\frac{2 \\bar{\\theta}_2}{2 (\\bar{\\theta}_1 + 2 \\bar{\\theta}_2 x_k)^3} \\begin{pmatrix} 1 & x_k & x_k^2 \\end{pmatrix} \\boldsymbol{\\Sigma}_{\\theta} \\begin{pmatrix} 1 \\\\ x_k \\\\ x_k^2 \\end{pmatrix}\n$$\nSimplifying this expression gives the final result for the expected estimation error, or bias, in the SOC estimate:\n$$\n\\mathbb{E}[e] = - \\frac{\\bar{\\theta}_2}{(\\bar{\\theta}_1 + 2 \\bar{\\theta}_2 x_k)^3} \\begin{pmatrix} 1 & x_k & x_k^2 \\end{pmatrix} \\boldsymbol{\\Sigma}_{\\theta} \\begin{pmatrix} 1 \\\\ x_k \\\\ x_k^2 \\end{pmatrix}\n$$\nThis expression reveals that the bias is driven by the curvature of the OCV-SOC relationship (represented by $\\bar{\\theta}_2$) and the interaction between the OCV sensitivity to parameters ($\\nabla_{\\theta} f$) and the parameter covariance $\\boldsymbol{\\Sigma}_{\\theta}$. If the OCV were linear in SOC ($\\bar{\\theta}_2=0$), this second-order bias effect would vanish. The denominator shows that the bias is also highly sensitive to the slope of the OCV curve, $\\frac{\\partial f}{\\partial x}$, becoming very large in flat regions of the OCV curve where the slope is close to zero.",
            "answer": "$$\n\\boxed{- \\frac{\\bar{\\theta}_2}{(\\bar{\\theta}_1 + 2 \\bar{\\theta}_2 x_k)^3} \\begin{pmatrix} 1 & x_k & x_k^2 \\end{pmatrix} \\boldsymbol{\\Sigma}_{\\theta} \\begin{pmatrix} 1 \\\\ x_k \\\\ x_k^2 \\end{pmatrix}}\n$$"
        }
    ]
}