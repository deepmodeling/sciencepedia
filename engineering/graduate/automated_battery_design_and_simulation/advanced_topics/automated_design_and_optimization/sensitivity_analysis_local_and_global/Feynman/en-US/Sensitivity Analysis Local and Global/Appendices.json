{
    "hands_on_practices": [
        {
            "introduction": "The foundation of sensitivity analysis lies in understanding the local response of a system to small perturbations. This practice hones your ability to derive local sensitivities analytically from first principles, a crucial skill for connecting mathematical theory to physical models. You will work with the Butler-Volmer equation, a cornerstone of electrochemical kinetics, to derive an explicit expression for how discharge capacity sensitivity depends on key reaction parameters, providing direct insight into the model's behavior at a specific operating point .",
            "id": "3948524",
            "problem": "An automated battery design and simulation workflow screens candidate negative-electrode chemistries by evaluating how the delivered discharge capacity responds to changes in the exchange current density under charge-transfer controlled kinetics. Consider a single, planar electrode of area $A$ operated isothermally at temperature $T$ under a fixed applied electrode potential $\\phi$, with an interfacial equilibrium potential $U$ that is approximately constant over a short discharge window of duration $\\tau$. The interfacial overpotential is defined by $\\eta = \\phi - U - i R_{s}$, where $R_{s}$ is a lumped series resistance and $i = A j$ is the total current with $j$ the interfacial current density. Neglect concentration polarization and mass-transport limitations so that the kinetics are governed by the Butler–Volmer relation,\n$$\nj = j_{0} \\left[\\exp\\!\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\!\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right],\n$$\nwhere $j_{0}$ is the exchange current density, $F$ is the Faraday constant, $R$ is the universal gas constant, and $\\alpha_{a}$ and $\\alpha_{c}$ are the anodic and cathodic charge-transfer coefficients, respectively. The delivered discharge capacity over the window $\\tau$ is defined as the charge $Q = A \\int_{0}^{\\tau} j(t)\\,\\mathrm{d}t$. Assume quasi-steady operation during $\\tau$ so that $j$ is constant in time at the operating point consistent with the above relations. Treat $A$, $T$, $R_{s}$, $\\phi$, $U$, $\\alpha_{a}$, and $\\alpha_{c}$ as constants with respect to $j_{0}$.\n\nStarting from the above fundamental definitions, derive a closed-form analytic expression for the local sensitivity of the delivered discharge capacity with respect to the exchange current density, namely the derivative $\\frac{\\partial Q}{\\partial j_{0}}$, evaluated at the operating point. Your expression must explicitly show its dependence on $\\alpha_{a}$, $\\alpha_{c}$, and the overpotential $\\eta$, and incorporate the coupling of $\\eta$ to $j$ through the series resistance $R_{s}$. Use the International System of Units (SI) for all quantities. Express your final answer as a single closed-form analytic expression and do not include units in the final expression. No numerical rounding is required.",
            "solution": "The user has requested the derivation of the local sensitivity of the delivered discharge capacity, $Q$, with respect to the exchange current density, $j_{0}$. This sensitivity is expressed as the partial derivative $\\frac{\\partial Q}{\\partial j_{0}}$. The derivation must start from the fundamental definitions provided.\n\nFirst, we establish the relationship between the discharge capacity $Q$ and the interfacial current density $j$. The problem defines the capacity as $Q = A \\int_{0}^{\\tau} j(t)\\,\\mathrm{d}t$, where $A$ is the electrode area and $\\tau$ is the discharge duration. A crucial simplifying assumption is that the system operates under quasi-steady conditions, meaning the current density $j$ is constant over the time interval $\\tau$. Therefore, the integral simplifies to:\n$$\nQ = A j \\tau\n$$\nThe parameters $A$ and $\\tau$ are constants with respect to $j_{0}$. To find $\\frac{\\partial Q}{\\partial j_{0}}$, we can apply the chain rule:\n$$\n\\frac{\\partial Q}{\\partial j_{0}} = \\frac{\\partial (A \\tau j)}{\\partial j_{0}} = A \\tau \\frac{\\partial j}{\\partial j_{0}}\n$$\nThe core of the problem is thus to determine the partial derivative of the current density $j$ with respect to the exchange current density $j_{0}$.\n\nThe system is described by two coupled equations. The first is the Butler-Volmer equation for the current density $j$:\n$$\nj = j_{0} \\left[\\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right] \\quad (1)\n$$\nThe second equation defines the interfacial overpotential $\\eta$:\n$$\n\\eta = \\phi - U - i R_{s}\n$$\nSubstituting $i = A j$, where $j$ is the current density, we have:\n$$\n\\eta = \\phi - U - A R_s j \\quad (2)\n$$\nIn this system, both $j$ and $\\eta$ are functions of the independent parameter $j_{0}$, while all other quantities ($A$, $T$, $R_s$, $\\phi$, $U$, $\\alpha_a$, $\\alpha_c$, $F$, $R$) are treated as constants with respect to $j_0$. To find $\\frac{\\partial j}{\\partial j_{0}}$, we must use implicit differentiation.\n\nWe differentiate Equation (1) with respect to $j_{0}$. Applying the product rule for the right-hand side, we get:\n$$\n\\frac{\\partial j}{\\partial j_{0}} = \\frac{\\partial}{\\partial j_{0}} \\left( j_{0} \\right) \\left[\\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right] + j_{0} \\frac{\\partial}{\\partial j_{0}} \\left[\\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right]\n$$\nThe first term simplifies. By recognizing that the term in square brackets is equal to $j/j_0$ from Equation (1), we have:\n$$\n\\frac{\\partial j}{\\partial j_{0}} = 1 \\cdot \\frac{j}{j_{0}} + j_{0} \\frac{\\partial}{\\partial j_{0}} \\left[\\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right]\n$$\nNow, we apply the chain rule to the second term, as $\\eta$ is a function of $j_{0}$:\n$$\n\\frac{\\partial j}{\\partial j_{0}} = \\frac{j}{j_{0}} + j_{0} \\left[ \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) \\cdot \\frac{\\alpha_{a} F}{R T} \\frac{\\partial \\eta}{\\partial j_{0}} - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\cdot \\left(-\\frac{\\alpha_{c} F}{R T}\\right) \\frac{\\partial \\eta}{\\partial j_{0}} \\right]\n$$\nFactoring out common terms yields:\n$$\n\\frac{\\partial j}{\\partial j_{0}} = \\frac{j}{j_{0}} + j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right] \\frac{\\partial \\eta}{\\partial j_{0}} \\quad (3)\n$$\nThis equation contains the unknown derivative $\\frac{\\partial \\eta}{\\partial j_{0}}$. We find this by differentiating Equation (2) with respect to $j_{0}$:\n$$\n\\frac{\\partial \\eta}{\\partial j_{0}} = \\frac{\\partial}{\\partial j_{0}} (\\phi - U - A R_s j)\n$$\nSince $\\phi$, $U$, $A$, and $R_s$ are constants with respect to $j_{0}$, this simplifies to:\n$$\n\\frac{\\partial \\eta}{\\partial j_{0}} = -A R_s \\frac{\\partial j}{\\partial j_{0}} \\quad (4)\n$$\nNow, we substitute Equation (4) into Equation (3):\n$$\n\\frac{\\partial j}{\\partial j_{0}} = \\frac{j}{j_{0}} + j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right] \\left( -A R_s \\frac{\\partial j}{\\partial j_{0}} \\right)\n$$\nWe can now solve for $\\frac{\\partial j}{\\partial j_{0}}$. Let's gather all terms containing $\\frac{\\partial j}{\\partial j_{0}}$ on one side:\n$$\n\\frac{\\partial j}{\\partial j_{0}} + A R_s j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right] \\frac{\\partial j}{\\partial j_{0}} = \\frac{j}{j_{0}}\n$$\nFactoring out $\\frac{\\partial j}{\\partial j_{0}}$:\n$$\n\\frac{\\partial j}{\\partial j_{0}} \\left( 1 + A R_s j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right] \\right) = \\frac{j}{j_{0}}\n$$\nIsolating $\\frac{\\partial j}{\\partial j_{0}}$ gives:\n$$\n\\frac{\\partial j}{\\partial j_{0}} = \\frac{j/j_{0}}{1 + A R_s j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right]}\n$$\nFinally, we substitute this result back into the expression for the sensitivity of the capacity, $\\frac{\\partial Q}{\\partial j_{0}} = A \\tau \\frac{\\partial j}{\\partial j_{0}}$:\n$$\n\\frac{\\partial Q}{\\partial j_{0}} = \\frac{A \\tau (j/j_{0})}{1 + A R_s j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right]}\n$$\nTo obtain the final expression in terms of the specified variables, we replace the term $j/j_0$ in the numerator using its definition from Equation (1):\n$$\n\\frac{j}{j_{0}} = \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\n$$\nSubstituting this yields the final closed-form analytic expression for the sensitivity:\n$$\n\\frac{\\partial Q}{\\partial j_{0}} = \\frac{A \\tau \\left[\\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right]}{1 + A R_s j_{0} \\frac{F}{R T} \\left[ \\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right) \\right]}\n$$\nThis expression is evaluated at the operating point, defined by the specific values of $\\eta$ and $j$ that simultaneously satisfy Equations (1) and (2) for a given $j_0$. The expression correctly shows the explicit dependence on $\\alpha_a$, $\\alpha_c$, and $\\eta$, and incorporates the effect of the series resistance $R_s$.",
            "answer": "$$\n\\boxed{\\frac{A \\tau \\left[\\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) - \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right]}{1 + A R_{s} j_{0} \\frac{F}{R T} \\left[\\alpha_{a} \\exp\\left(\\frac{\\alpha_{a} F \\eta}{R T}\\right) + \\alpha_{c} \\exp\\left(-\\frac{\\alpha_{c} F \\eta}{R T}\\right)\\right]}}\n$$"
        },
        {
            "introduction": "While local analysis is precise, real-world design often requires understanding how parameters influence performance across their entire range of uncertainty. This is the domain of Global Sensitivity Analysis (GSA), which quantifies the impact of large parameter variations and their interactions. In this hands-on computational exercise, you will implement a variance-based GSA using the Sobol method to rank the factors influencing a battery's thermal behavior, a core task in designing effective thermal management systems .",
            "id": "3948598",
            "problem": "You are tasked with implementing a Monte Carlo global sensitivity analysis of a simplified battery thermal management model and ranking the parameters by their total Sobol indices. The context is automated battery design and simulation; the goal is to quantify how cooling boundary conditions and internal heat sources influence the maximum cell temperature over a fixed horizon.\n\nAssume a single, thermally lumped battery cell with constant internal heat generation. The cell is cooled by convection to the ambient environment. Starting from the integral energy balance and Newtonian cooling, adopt the lumped-capacitance model where the battery cell temperature evolves according to\n$$\nm c_p \\frac{d T(t)}{dt} = q - h A \\left(T(t) - T_{\\text{amb}}\\right),\n$$\nwhere $m$ is mass, $c_p$ is specific heat capacity, $q$ is total internal heat generation rate, $h$ is the convective heat transfer coefficient, $A$ is the effective cooling area, and $T_{\\text{amb}}$ is the ambient temperature. Assuming the initial condition $T(0) = T_{\\text{amb}}$ and constant $q$, solve for the absolute temperature at time $t = \\tau$ to define the thermal management effectiveness metric\n$$\nf(h, q, T_{\\text{amb}}, A) = T(\\tau) = T_{\\text{amb}} + \\frac{q}{h A}\\left(1 - e^{- \\frac{h A}{m c_p} \\tau}\\right).\n$$\nAll physical units are to be treated consistently as follows: $m$ in $\\mathrm{kg}$, $c_p$ in $\\mathrm{J}\\cdot\\mathrm{kg}^{-1}\\cdot\\mathrm{K}^{-1}$, $q$ in $\\mathrm{W}$, $h$ in $\\mathrm{W}\\cdot\\mathrm{m}^{-2}\\cdot\\mathrm{K}^{-1}$, $A$ in $\\mathrm{m}^2$, $T_{\\text{amb}}$ in $\\mathrm{K}$, and $\\tau$ in $\\mathrm{s}$. The output of the program will not directly report temperatures; instead, it will provide parameter rankings. Nevertheless, the model must compute $f$ in $\\mathrm{K}$ internally.\n\nPerform a global sensitivity analysis using the variance-based Sobol method. Let the uncertain input vector be\n$$\n\\mathbf{X} = \\left(h, q, T_{\\text{amb}}, A\\right) \\in \\mathbb{R}^4,\n$$\nwith independent uniform distributions over specified ranges. For the total Sobol index of parameter $i$, denoted $S_{T_i}$, use the Jansen estimator. Draw two independent Monte Carlo sample matrices $\\mathbf{A}$ and $\\mathbf{B}$ of shape $N \\times d$ (with $d = 4$), where each column is sampled independently and uniformly from its parameter range. For each parameter $i \\in \\{0,1,2,3\\}$, construct the mixed matrix $\\mathbf{A}_{B}^{(i)}$ by taking all columns from $\\mathbf{B}$ except the $i$-th, which must come from $\\mathbf{A}$. Evaluate the model $f$ on $\\mathbf{A}$ and each $\\mathbf{A}_{B}^{(i)}$ to obtain vectors $Y_{\\mathbf{A}}$ and $Y_{\\mathbf{A}_{B}^{(i)}}$. Estimate the output variance using\n$$\n\\operatorname{Var}(Y) \\approx \\frac{1}{N} \\sum_{j=1}^{N} Y_{\\mathbf{A},j}^2 - \\left(\\frac{1}{N} \\sum_{j=1}^{N} Y_{\\mathbf{A},j}\\right)^2,\n$$\nand estimate the total Sobol index using\n$$\nS_{T_i} \\approx \\frac{1}{2 N \\operatorname{Var}(Y)} \\sum_{j=1}^{N} \\left(Y_{\\mathbf{A},j} - Y_{\\mathbf{A}_{B}^{(i)},j}\\right)^2.\n$$\nIf $\\operatorname{Var}(Y) = 0$, define all $S_{T_i} = 0$.\n\nYour program must:\n- Implement the model $f$ as specified.\n- Implement the Jansen total-effect estimator as specified.\n- Use pseudorandom uniform sampling over the defined ranges for each test case.\n- Rank the parameters by descending total Sobol index for each test case. Ties must be broken by ascending parameter index.\n\nParameter indexing must be exactly:\n- $i = 0$ corresponds to $h$ in $\\mathrm{W}\\cdot\\mathrm{m}^{-2}\\cdot\\mathrm{K}^{-1}$.\n- $i = 1$ corresponds to $q$ in $\\mathrm{W}$.\n- $i = 2$ corresponds to $T_{\\text{amb}}$ in $\\mathrm{K}$.\n- $i = 3$ corresponds to $A$ in $\\mathrm{m}^2$.\n\nUse the following constants for all test cases:\n- $m = 0.5$ in $\\mathrm{kg}$.\n- $c_p = 900$ in $\\mathrm{J}\\cdot\\mathrm{kg}^{-1}\\cdot\\mathrm{K}^{-1}$.\n- $\\tau = 600$ in $\\mathrm{s}$.\n\nTest suite:\n- Case $1$ (general case): $N = 4096$, seed $= 2025$, parameter ranges $h \\in [10, 100]$, $q \\in [1, 20]$, $T_{\\text{amb}} \\in [293, 313]$, $A \\in [0.01, 0.05]$.\n- Case $2$ (small-sample boundary): $N = 128$, seed $= 2026$, parameter ranges $h \\in [10, 100]$, $q \\in [1, 20]$, $T_{\\text{amb}} \\in [293, 313]$, $A \\in [0.01, 0.05]$.\n- Case $3$ (edge case with negligible internal heat): $N = 2048$, seed $= 2027$, parameter ranges $h \\in [5, 150]$, $q \\in [0, 0.1]$, $T_{\\text{amb}} \\in [293, 313]$, $A \\in [0.01, 0.05]$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of integers representing the parameter indices in descending order of total Sobol index for that test case. For example, produce an output in the form\n$$\n[\\,[i_{1,1}, i_{1,2}, i_{1,3}, i_{1,4}],\\,[i_{2,1}, i_{2,2}, i_{2,3}, i_{2,4}],\\,[i_{3,1}, i_{3,2}, i_{3,3}, i_{3,4}]\\,],\n$$\nwhere $[i_{k,1}, i_{k,2}, i_{k,3}, i_{k,4}]$ is the ranking for case $k \\in \\{1,2,3\\}$.",
            "solution": "The user wants to perform a global sensitivity analysis for a simplified battery thermal model.\n\n### Step 1: Extract Givens\n\n*   **Model Equation:** The temperature $T(t)$ of a thermally lumped battery cell evolves according to the ordinary differential equation $m c_p \\frac{d T(t)}{dt} = q - h A \\left(T(t) - T_{\\text{amb}}\\right)$.\n*   **Initial Condition:** $T(0) = T_{\\text{amb}}$.\n*   **Model Output Function:** The solution at time $t=\\tau$ is given by $f(h, q, T_{\\text{amb}}, A) = T(\\tau) = T_{\\text{amb}} + \\frac{q}{h A}\\left(1 - e^{- \\frac{h A}{m c_p} \\tau}\\right)$.\n*   **Input Parameters (Uncertain):** $\\mathbf{X} = \\left(h, q, T_{\\text{amb}}, A\\right)$, with $h$ (convective heat transfer coefficient, index $0$), $q$ (internal heat generation rate, index $1$), $T_{\\text{amb}}$ (ambient temperature, index $2$), and $A$ (effective cooling area, index $3$). These are assumed to be independent and uniformly distributed over given ranges.\n*   **Fixed Constants:** $m = 0.5 \\, \\mathrm{kg}$, $c_p = 900 \\, \\mathrm{J}\\cdot\\mathrm{kg}^{-1}\\cdot\\mathrm{K}^{-1}$, $\\tau = 600 \\, \\mathrm{s}$.\n*   **Sensitivity Analysis Method:** Variance-based Sobol method to compute total sensitivity indices, $S_{T_i}$.\n*   **Estimator:** Jansen estimator.\n*   **Sampling:** Two independent Monte Carlo sample matrices, $\\mathbf{A}$ and $\\mathbf{B}$, of shape $N \\times d$ (with $d=4$).\n*   **Mixed Matrix Construction:** For each parameter $i$, the matrix $\\mathbf{A}_{B}^{(i)}$ is constructed by taking all columns from $\\mathbf{B}$ except for the $i$-th column, which is taken from $\\mathbf{A}$.\n*   **Variance Estimation:** $\\operatorname{Var}(Y) \\approx \\frac{1}{N} \\sum_{j=1}^{N} Y_{\\mathbf{A},j}^2 - \\left(\\frac{1}{N} \\sum_{j=1}^{N} Y_{\\mathbf{A},j}\\right)^2$, where $Y_{\\mathbf{A}}$ is the vector of model outputs evaluated on matrix $\\mathbf{A}$.\n*   **Total Sobol Index Estimation:** $S_{T_i} \\approx \\frac{1}{2 N \\operatorname{Var}(Y)} \\sum_{j=1}^{N} \\left(Y_{\\mathbf{A},j} - Y_{\\mathbf{A}_{B}^{(i)},j}\\right)^2$. If $\\operatorname{Var}(Y)=0$, then $S_{T_i}=0$.\n*   **Ranking Rule:** Parameters are to be ranked in descending order of their total Sobol index, $S_{T_i}$. Ties are broken by ascending parameter index ($0, 1, 2, 3$).\n*   **Test Cases:**\n    1.  $N = 4096$, seed $= 2025$, ranges: $h \\in [10, 100]$, $q \\in [1, 20]$, $T_{\\text{amb}} \\in [293, 313]$, $A \\in [0.01, 0.05]$.\n    2.  $N = 128$, seed $= 2026$, ranges: $h \\in [10, 100]$, $q \\in [1, 20]$, $T_{\\text{amb}} \\in [293, 313]$, $A \\in [0.01, 0.05]$.\n    3.  $N = 2048$, seed $= 2027$, ranges: $h \\in [5, 150]$, $q \\in [0, 0.1]$, $T_{\\text{amb}} \\in [293, 313]$, $A \\in [0.01, 0.05]$.\n\n### Step 2: Validate Using Extracted Givens\n\n*   **Scientific Grounding:** The problem is scientifically sound. It employs the lumped-capacitance model, a standard and appropriate simplification in thermal engineering for objects with a low Biot number. The governing ODE and its analytical solution are correct. The Sobol method with the Jansen estimator is a well-established and rigorous technique for global sensitivity analysis. All physical constants and parameter ranges are realistic.\n*   **Well-Posedness:** The problem is well-posed. It provides a unique model function, a clearly defined computational algorithm for the sensitivity indices, all necessary constants, specific parameter ranges for each input, and random seeds for reproducibility. The tie-breaking rule ensures a unique ranking.\n*   **Objectivity:** The problem is stated in precise, objective, and quantitative terms, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution can be constructed by faithfully implementing the specified model and numerical method.\n\n### Principle-Based Design of the Solution\n\nThe solution is implemented by adhering to the principles of Monte Carlo simulation and variance-based sensitivity analysis as prescribed.\n\n1.  **Model Implementation:** The core of the analysis is the model function $f(h, q, T_{\\text{amb}}, A)$. This function is implemented to be numerically efficient by using `numpy` for vectorized calculations, allowing it to evaluate the model for an entire matrix of $N$ input samples simultaneously. The function computes $T(\\tau) = T_{\\text{amb}} + \\frac{q}{h A}\\left(1 - e^{- \\frac{h A \\tau}{m c_p}}\\right)$ for given input vectors $h, q, T_{\\text{amb}}, A$.\n\n2.  **Monte Carlo Sampling:** For each test case, the procedure begins by setting the specified pseudorandom number generator seed for reproducibility. Two independent sample matrices, $\\mathbf{A}$ and $\\mathbf{B}$, are generated. Each matrix has $N$ rows and $d=4$ columns. Each column corresponds to one of the four input parameters ($h, q, T_{\\text{amb}}, A$), and its values are drawn from an independent uniform distribution defined by the parameter's specified range.\n\n3.  **Jansen Estimator for Total Sobol Index:** The total Sobol index, $S_{T_i}$, quantifies the total contribution of the input parameter $X_i$ to the output variance, including all its interactions with other parameters. We implement the Jansen estimator as specified:\n    *   First, the model is evaluated for all sample points in matrix $\\mathbf{A}$, yielding the output vector $Y_{\\mathbf{A}}$.\n    *   The total variance of the output, $\\operatorname{Var}(Y)$, is estimated from $Y_{\\mathbf{A}}$ using the provided formula, which is equivalent to the biased sample variance. A check is in place for the case $\\operatorname{Var}(Y)=0$.\n    *   Then, for each parameter $i \\in \\{0, 1, 2, 3\\}$, a \"mixed\" matrix $\\mathbf{A}_{B}^{(i)}$ is constructed. Following the problem's explicit instructions, this matrix consists of all columns from matrix $\\mathbf{B}$ except for the $i$-th column, which is taken from matrix $\\mathbf{A}$.\n    *   The model is evaluated on this mixed matrix to produce the output vector $Y_{\\mathbf{A}_{B}^{(i)}}$.\n    *   The total Sobol index $S_{T_i}$ is then calculated using the provided formula: $S_{T_i} \\approx \\frac{\\sum_{j=1}^{N} \\left(Y_{\\mathbf{A},j} - Y_{\\mathbf{A}_{B}^{(i)},j}\\right)^2}{2 N \\operatorname{Var}(Y)}$. This formula quantifies how much the output changes, on average, when only the $i$-th parameter's value is varied between two different samples, while all other parameters are also varied.\n\n4.  **Parameter Ranking:** After calculating the four total Sobol indices ($S_{T_0}, S_{T_1}, S_{T_2}, S_{T_3}$), the corresponding parameter indices ($0, 1, 2, 3$) are ranked. The primary sorting criterion is the value of $S_{T_i}$ in descending order. The secondary criterion, used to break any ties, is the parameter index $i$ in ascending order. This is achieved by sorting the indices based on a tuple key `(-S_Ti, i)`.\n\n5.  **Execution and Formatting:** The entire process is encapsulated in a function that iterates through the three specified test cases. For each case, it performs the sampling, estimation, and ranking, and stores the resulting list of ranked indices. Finally, the results for all cases are formatted into a single string, representing a list of lists of integers with no whitespace, as per the required output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs global sensitivity analysis on a battery thermal model and ranks\n    parameters based on their total Sobol indices for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, seed, parameter_ranges)\n    test_cases = [\n        # Case 1 (general case)\n        (4096, 2025, [(10, 100), (1, 20), (293, 313), (0.01, 0.05)]),\n        # Case 2 (small-sample boundary)\n        (128, 2026, [(10, 100), (1, 20), (293, 313), (0.01, 0.05)]),\n        # Case 3 (edge case with negligible internal heat)\n        (2048, 2027, [(5, 150), (0, 0.1), (293, 313), (0.01, 0.05)]),\n    ]\n\n    # Global constants for the model\n    m = 0.5  # kg\n    cp = 900.0  # J/kg/K\n    tau = 600.0  # s\n    d = 4  # Number of parameters\n\n    def model_f(h, q, T_amb, A):\n        \"\"\"\n        Calculates the battery cell temperature at time tau.\n        This is a vectorized implementation using numpy.\n        \n        Args:\n            h (np.ndarray): Convective heat transfer coefficient vector.\n            q (np.ndarray): Internal heat generation rate vector.\n            T_amb (np.ndarray): Ambient temperature vector.\n            A (np.ndarray): Cooling area vector.\n\n        Returns:\n            np.ndarray: Final temperature T(tau) vector.\n        \"\"\"\n        # To prevent division by zero, although problem ranges make it impossible\n        hA = h * A\n        # The term is set to a large number to ensure the result is T_amb when hA is zero.\n        # This is a safe guard not strictly needed for the given parameter ranges.\n        safe_hA = np.where(hA == 0, 1e12, hA)\n        \n        term1 = q / safe_hA\n        exponent = -safe_hA * tau / (m * cp)\n        exp_term = np.exp(exponent)\n        \n        return T_amb + term1 * (1 - exp_term)\n\n    def analyze_case(N, seed, param_ranges):\n        \"\"\"\n        Performs the sensitivity analysis for a single case.\n        \"\"\"\n        np.random.seed(seed)\n        \n        # Generate two independent N x d sample matrices, A and B\n        A_matrix = np.zeros((N, d))\n        B_matrix = np.zeros((N, d))\n        \n        for i in range(d):\n            low, high = param_ranges[i]\n            A_matrix[:, i] = np.random.uniform(low, high, size=N)\n            B_matrix[:, i] = np.random.uniform(low, high, size=N)\n            \n        # Evaluate the model on matrix A\n        Y_A = model_f(A_matrix[:, 0], A_matrix[:, 1], A_matrix[:, 2], A_matrix[:, 3])\n        \n        # Estimate the variance of the output Y based on Y_A samples\n        # Var(Y) = E[Y^2] - (E[Y])^2\n        var_Y = np.mean(Y_A**2) - (np.mean(Y_A))**2\n\n        total_sobol_indices = []\n\n        if var_Y == 0:\n            # If variance is zero, all sensitivities are zero.\n            # Ranking follows the tie-breaking rule (ascending index).\n            return list(range(d))\n\n        # Calculate total Sobol index for each parameter\n        for i in range(d):\n            # Construct the mixed matrix A_B^(i) as per the problem description:\n            # \"all columns from B except the i-th, which must come from A\"\n            A_B_i = B_matrix.copy()\n            A_B_i[:, i] = A_matrix[:, i]\n            \n            # Evaluate the model on the mixed matrix\n            Y_A_B_i = model_f(A_B_i[:, 0], A_B_i[:, 1], A_B_i[:, 2], A_B_i[:, 3])\n            \n            # Estimate the total Sobol index using the Jansen estimator formula\n            # S_Ti = (1 / (2*N)) * sum((Y_A - Y_A_B_i)^2) / Var(Y)\n            numerator = np.sum((Y_A - Y_A_B_i)**2)\n            S_Ti = numerator / (2 * N * var_Y)\n            total_sobol_indices.append(S_Ti)\n\n        # Rank parameters by descending Sobol index, breaking ties with ascending index\n        param_indices = list(range(d))\n        ranked_indices = sorted(param_indices, key=lambda idx: (-total_sobol_indices[idx], idx))\n        \n        return ranked_indices\n\n    results = []\n    for case in test_cases:\n        N_val, seed_val, ranges_val = case\n        ranking = analyze_case(N_val, seed_val, ranges_val)\n        results.append(ranking)\n\n    # Format the final output string to match [[i1,i2,..],[i1,i2,..]] with no spaces.\n    inner_strs = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output = f\"[{','.join(inner_strs)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A computed sensitivity is only as valuable as its correctness. In computational science, rigorous verification is paramount to ensure that our model's predictions are reliable. This exercise challenges you to step into the role of a methods developer and design a robust protocol for \"gradient checking,\" which uses numerical finite differences to validate analytically derived sensitivities from a complex battery model. Successfully completing this task requires a nuanced understanding of numerical error sources and how to design a test that is both accurate and efficient .",
            "id": "3948566",
            "problem": "A Pseudo Two-Dimensional (P2D) electrochemical model maps a parameter vector $\\mathbf{p} \\in \\mathbb{R}^{n}$ (including parameters such as solid-phase diffusion coefficient $D_{s}$, reaction rate constant $k$, separator porosity $\\varepsilon_{s}$, and film thickness $\\delta_{\\mathrm{SEI}}$) to a vector of terminal voltages sampled at $m$ time points under a prescribed current profile, $f(\\mathbf{p}) = \\mathbf{V}(\\mathbf{p}) \\in \\mathbb{R}^{m}$. An automated solver computes an analytic Jacobian $J(\\mathbf{p}) \\in \\mathbb{R}^{m \\times n}$ using adjoint methods, with the intention of using these sensitivities for design optimization and uncertainty quantification. The solver uses IEEE-754 double precision arithmetic with machine epsilon $\\epsilon_{\\mathrm{mach}}$, and its time integration tolerances are absolute tolerance $\\tau_{\\mathrm{abs}}$ and relative tolerance $\\tau_{\\mathrm{rel}}$.\n\nYour task is to select a scientifically sound and robust protocol to validate $J(\\mathbf{p})$ via gradient checking. The protocol must:\n\n- Be grounded in first principles for smooth functions $f$ (assume $f$ is twice continuously differentiable in the neighborhood of $\\mathbf{p}$).\n- Balance truncation error from Taylor approximation with floating-point round-off.\n- Be appropriate for vector-valued outputs $f(\\mathbf{p}) \\in \\mathbb{R}^{m}$ and high-dimensional parameter vectors $\\mathbf{p} \\in \\mathbb{R}^{n}$.\n- Specify how perturbation magnitudes $h_{i}$ are chosen for each parameter $p_{i}$.\n- Define how the comparison is performed (including the use of directional derivatives versus componentwise partials).\n- Set explicit acceptance tolerances and criteria tied to the solver’s numerical characteristics.\n\nChoose the best protocol from the options:\n\nA. Use a directional derivative central-difference check. For $k \\in \\{1,2,3,4,5\\}$, draw random unit vectors $\\mathbf{v}^{(k)} \\in \\mathbb{R}^{n}$ with $\\|\\mathbf{v}^{(k)}\\|_{2} = 1$. For each $\\mathbf{v}^{(k)}$, evaluate the symmetric perturbations and form the finite-difference directional derivative\n$$\n\\tilde{\\mathbf{d}}^{(k)}(h) = \\frac{f(\\mathbf{p} + h\\, \\mathbf{v}^{(k)}) - f(\\mathbf{p} - h\\, \\mathbf{v}^{(k)})}{2h},\n$$\nand compare to the analytic directional derivative $\\mathbf{d}^{(k)} = J(\\mathbf{p})\\, \\mathbf{v}^{(k)}$. Choose parameter-scaled steps $h_{i} = \\alpha\\,(1 + |p_{i}|)$ with a base scale $\\alpha = \\epsilon_{\\mathrm{mach}}^{1/3}$, and conduct a three-level refinement for each direction using $h \\in \\{\\alpha, \\alpha/2, \\alpha/4\\}$. Define the relative discrepancy\n$$\ne_{\\mathrm{rel}}^{(k)}(h) = \\frac{\\left\\|\\mathbf{d}^{(k)} - \\tilde{\\mathbf{d}}^{(k)}(h)\\right\\|_{2}}{\\left\\|\\mathbf{d}^{(k)}\\right\\|_{2} + \\left\\|\\tilde{\\mathbf{d}}^{(k)}(h)\\right\\|_{2}},\n$$\nand an absolute discrepancy $e_{\\mathrm{abs}}^{(k)}(h) = \\left\\|\\mathbf{d}^{(k)} - \\tilde{\\mathbf{d}}^{(k)}(h)\\right\\|_{2}$. Acceptance requires, for at least $4$ of the $5$ directions, that $e_{\\mathrm{rel}}^{(k)}(\\alpha) \\le 10^{-3}$ and that $e_{\\mathrm{rel}}^{(k)}$ decreases approximately quadratically over refinements, i.e., $e_{\\mathrm{rel}}^{(k)}(\\alpha/2) \\in [e_{\\mathrm{rel}}^{(k)}(\\alpha)/8,\\, e_{\\mathrm{rel}}^{(k)}(\\alpha)/2]$ and $e_{\\mathrm{rel}}^{(k)}(\\alpha/4) \\in [e_{\\mathrm{rel}}^{(k)}(\\alpha)/32,\\, e_{\\mathrm{rel}}^{(k)}(\\alpha)/8]$. Additionally, require $e_{\\mathrm{abs}}^{(k)}(\\alpha) \\le 10\\, \\tau_{\\mathrm{abs}}$. Perform all simulations with identical solver tolerances $(\\tau_{\\mathrm{abs}}, \\tau_{\\mathrm{rel}})$ and identical time grids to avoid confounding discretization differences.\n\nB. Use single-sided forward differences on coordinate directions. For each parameter $p_{i}$, compute\n$$\n\\tilde{\\mathbf{g}}_{i} = \\frac{f(\\mathbf{p} + h\\, \\mathbf{e}_{i}) - f(\\mathbf{p})}{h}\n$$\nwith fixed $h = 10^{-4}$ for all $i$, where $\\mathbf{e}_{i}$ is the $i$-th canonical basis vector in $\\mathbb{R}^{n}$. Compare $\\tilde{\\mathbf{g}}_{i}$ to the analytic column $\\mathbf{j}_{i}$ of $J(\\mathbf{p})$ using the maximum norm discrepancy\n$$\n\\max_{i} \\left\\|\\mathbf{j}_{i} - \\tilde{\\mathbf{g}}_{i}\\right\\|_{\\infty} \\le 10^{-9}\n$$\nfor acceptance. Do not scale $h$ by parameter magnitude and do not refine $h$; fix solver tolerances as given.\n\nC. Use a single directional central difference without refinement and a tight relative threshold. Draw a single unit vector $\\mathbf{v} \\in \\mathbb{R}^{n}$, set $h_{i} = \\sqrt{\\epsilon_{\\mathrm{mach}}}\\,(1 + |p_{i}|)$, and compute\n$$\n\\tilde{\\mathbf{d}}(h) = \\frac{f(\\mathbf{p} + h\\, \\mathbf{v}) - f(\\mathbf{p} - h\\, \\mathbf{v})}{2h}.\n$$\nAccept if\n$$\n\\frac{\\left\\|\\mathbf{d} - \\tilde{\\mathbf{d}}(h)\\right\\|_{\\infty}}{\\left\\|\\mathbf{d}\\right\\|_{2}} \\le 10^{-6},\n$$\nwhere $\\mathbf{d} = J(\\mathbf{p})\\, \\mathbf{v}$. No multi-$h$ convergence check is performed.\n\nD. Compare the full Jacobian via exhaustive componentwise central differences using a step tied to the square root of machine epsilon. For each $i$, compute\n$$\n\\tilde{\\mathbf{j}}_{i}(h) = \\frac{f(\\mathbf{p} + h_{i}\\, \\mathbf{e}_{i}) - f(\\mathbf{p} - h_{i}\\, \\mathbf{e}_{i})}{2h_{i}},\n$$\nwith $h_{i} = \\epsilon_{\\mathrm{mach}}^{1/2}\\,(1 + |p_{i}|)$ and assemble $\\tilde{J}(h) = [\\tilde{\\mathbf{j}}_{1}(h)\\ \\cdots\\ \\tilde{\\mathbf{j}}_{n}(h)]$. Accept if the Frobenius norm discrepancy satisfies\n$$\n\\frac{\\left\\|J(\\mathbf{p}) - \\tilde{J}(h)\\right\\|_{F}}{\\left\\|J(\\mathbf{p})\\right\\|_{F}} \\le 10^{-8},\n$$\nwith no multi-$h$ refinement and no tie-in to $(\\tau_{\\mathrm{abs}}, \\tau_{\\mathrm{rel}})$.",
            "solution": "The problem statement is valid. It presents a realistic scenario from computational science—the verification of an analytically computed Jacobian for a complex physics-based model (P2D for batteries). The problem is scientifically grounded, well-posed, objective, and provides sufficient information to evaluate the options based on established principles of numerical analysis.\n\nThe task is to select the most robust and scientifically sound protocol for gradient checking. A robust protocol must adhere to several core principles derived from the analysis of errors in numerical differentiation.\n\n1.  **Choice of Finite Difference Formula**: For a twice continuously differentiable function $f$, the Taylor expansion shows that the central difference formula, $\\frac{f(\\mathbf{p}+h\\mathbf{v}) - f(\\mathbf{p}-h\\mathbf{v})}{2h}$, approximates the directional derivative $\\nabla f(\\mathbf{p}) \\cdot \\mathbf{v}$ with a truncation error of order $O(h^2)$. The forward difference formula, $\\frac{f(\\mathbf{p}+h\\mathbf{v}) - f(\\mathbf{p})}{h}$, has a larger truncation error of order $O(h)$. Therefore, the central difference is significantly more accurate for a given step size $h$ and is the preferred method.\n\n2.  **Balancing Truncation and Round-off Errors**: The total error of a finite difference approximation is the sum of the truncation error, which decreases with $h$, and the round-off error, which increases as $h$ decreases due to catastrophic cancellation. Round-off error in evaluating $f(\\mathbf{p}+\\delta\\mathbf{p}) - f(\\mathbf{p})$ is on the order of $\\epsilon_{\\mathrm{mach}} |f(\\mathbf{p})|$, where $\\epsilon_{\\mathrm{mach}}$ is the machine epsilon. The total error for the central difference is approximately $Ah^2 + B\\frac{\\epsilon_{\\mathrm{mach}}}{h}$. Minimizing this with respect to $h$ yields an optimal step size $h_{\\mathrm{opt}} \\propto \\epsilon_{\\mathrm{mach}}^{1/3}$. The minimum achievable error is then on the order of $O(\\epsilon_{\\mathrm{mach}}^{2/3})$. For forward differences, the error is $Ah + B\\frac{\\epsilon_{\\mathrm{mach}}}{h}$, yielding $h_{\\mathrm{opt}} \\propto \\epsilon_{\\mathrm{mach}}^{1/2}$ and a minimum error of $O(\\epsilon_{\\mathrm{mach}}^{1/2})$. For double precision, $\\epsilon_{\\mathrm{mach}} \\approx 2.2 \\times 10^{-16}$, so these correspond to $h_{\\mathrm{opt}} \\sim 10^{-5}$ and error $\\sim 10^{-11}$ for central difference, versus $h_{\\mathrm{opt}} \\sim 10^{-8}$ and error $\\sim 10^{-8}$ for forward difference.\n\n3.  **High-Dimensionality**: For a high-dimensional parameter space $\\mathbf{p} \\in \\mathbb{R}^{n}$, computing the full Jacobian $J(\\mathbf{p})$ by evaluating each of its $n$ columns via component-wise finite differences requires $2n$ (for central) or $n$ (for forward) function evaluations. This can be computationally prohibitive. A more efficient strategy is the \"Jacobian-vector product check\" or directional derivative check. This involves comparing the analytically computed product $J(\\mathbf{p})\\mathbf{v}$ with the finite difference approximation of the directional derivative of $f$ along a vector $\\mathbf{v}$. By choosing a few random directions $\\mathbf{v}$, one can gain high confidence in the correctness of the entire Jacobian $J(\\mathbf{p})$ with only a small number of function evaluations (2 per direction for central differences).\n\n4.  **Robustness and Verification**: A reliable protocol does not depend on a single, fixed step size. It should verify the expected theoretical rate of convergence. By computing the finite difference for a sequence of decreasing step sizes (e.g., $h, h/2, h/4$), one can check if the error decreases quadratically ($E(h/2) \\approx E(h)/4$), confirming that the computation is in the truncation-error dominated regime and that the function is sufficiently smooth.\n\n5.  **Practical Considerations for Solvers**: The function $f(\\mathbf{p})$ is not an exact mathematical function but the output of a numerical solver with its own error controls, such as absolute tolerance $\\tau_{\\mathrm{abs}}$ and relative tolerance $\\tau_{\\mathrm{rel}}$. This numerical noise from the solver effectively sets a floor on the precision of the function evaluation. Any gradient checking protocol must recognize that the achievable accuracy of the finite difference approximation is limited by this noise. Therefore, acceptance tolerances should be realistically set in relation to solver tolerances (e.g., must be looser than $\\tau_{\\mathrm{abs}}$ and $\\tau_{\\mathrm{rel}}$), and sources of non-determinism (like adaptive grids) should be controlled.\n\nWith these principles in mind, we evaluate the options:\n\n**A. Use a directional derivative central-difference check...**\nThis protocol is exceptionally well-designed and aligns with all the principles of a robust gradient check.\n-   It uses the superior central difference formula.\n-   It targets high-dimensional problems efficiently by checking $5$ random directional derivatives.\n-   It uses an optimal base step size scaling, $\\alpha = \\epsilon_{\\mathrm{mach}}^{1/3}$, which correctly balances truncation and round-off error for central differences. The slight ambiguity in the text between the scalar step $h$ and the component-wise scaling $h_i$ is a minor formulation weakness, but the core choice of $\\alpha$ is correct.\n-   It performs a multi-level refinement ($h, h/2, h/4$) and includes a quantitative check on the convergence rate, ensuring the error decreases approximately quadratically.\n-   It uses robust relative and absolute error metrics for comparison.\n-   Critically, it ties the absolute acceptance tolerance to the solver's tolerance ($e_{\\mathrm{abs}}^{(k)}(\\alpha) \\le 10\\, \\tau_{\\mathrm{abs}}$) and mandates controlling for numerical noise by using identical time grids.\n\nThis option describes a state-of-the-art validation protocol.\n**Verdict: Correct**\n\n**B. Use single-sided forward differences on coordinate directions...**\nThis protocol is fundamentally flawed.\n-   It uses the less accurate forward difference ($O(h)$) method.\n-   It is inefficient for high-dimensional problems, requiring $n$ function evaluations.\n-   The step size is a fixed, arbitrary value $h=10^{-4}$, which is not scaled by parameter magnitude and is far from the optimal $h_{\\mathrm{opt}} \\sim \\epsilon_{\\mathrm{mach}}^{1/2} \\approx 10^{-8}$ for forward differences. This choice guarantees the test is dominated by truncation error.\n-   The acceptance tolerance of $10^{-9}$ is impossible to achieve with an $O(h)$ method where $h=10^{-4}$, as the truncation error alone will be of order $10^{-4}$.\n-   It includes no refinement check and no consideration for solver tolerances.\n\nThis protocol is naive and internally inconsistent.\n**Verdict: Incorrect**\n\n**C. Use a single directional central difference without refinement...**\nThis protocol is an improvement over B but remains weak and not robust.\n-   It correctly uses the central difference method and a directional derivative approach.\n-   However, it uses a suboptimal step size $h \\propto \\epsilon_{\\mathrm{mach}}^{1/2}$ for a central difference calculation. This step size is too small and pushes the calculation into the round-off error dominated regime. The optimal choice is $\\epsilon_{\\mathrm{mach}}^{1/3}$. The formulation `h_i = ...` is also contradictory with `f(p + h v)`.\n-   It lacks robustness by checking only a single direction and performing no refinement to verify the convergence rate.\n-   The acceptance criterion is not linked to the underlying solver's precision.\n\nThis protocol lacks the necessary checks and balances for a reliable validation.\n**Verdict: Incorrect**\n\n**D. Compare the full Jacobian via exhaustive componentwise central differences...**\nThis protocol is thorough but inappropriate for the problem's context and lacks key robustness features.\n-   Its primary flaw is being \"exhaustive\" and \"componentwise\", which is not scalable to the \"high-dimensional parameter vectors\" mentioned in the problem setup. This violates a key requirement.\n-   Like option C, it uses a suboptimal step size scaling $h_{i} \\propto \\epsilon_{\\mathrm{mach}}^{1/2}$ for a central difference method.\n-   It lacks a multi-step refinement to verify the quadratic convergence rate.\n-   Its acceptance tolerance of $10^{-8}$ is very tight for a step size of $h \\sim 10^{-8}$ and leaves no margin for solver noise.\n-   It fails to connect the tolerance to the solver's characteristics $(\\tau_{\\mathrm{abs}}, \\tau_{\\mathrm{rel}})$.\n\nThis protocol is computationally inefficient for the stated problem class and lacks robustness.\n**Verdict: Incorrect**\n\nIn summary, Option A is the only choice that presents a comprehensive, theoretically sound, and practically robust protocol for validating the Jacobian. It correctly applies principles of numerical analysis to balance accuracy and efficiency, and it properly accounts for the realities of working with numerical solvers.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}