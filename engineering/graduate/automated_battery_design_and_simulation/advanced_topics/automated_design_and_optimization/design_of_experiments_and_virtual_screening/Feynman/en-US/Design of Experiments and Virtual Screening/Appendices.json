{
    "hands_on_practices": [
        {
            "introduction": "In the initial stages of materials discovery, such as formulating a new battery electrolyte, researchers often face a vast design space with numerous potential factors. To navigate this complexity without incurring prohibitive computational or experimental costs, efficient screening strategies are essential. This practice introduces the foundational principles of fractional factorial designs, a cornerstone of Design of Experiments (DOE), which allow for the investigation of many factors in a minimal number of runs. You will gain hands-on experience by constructing the alias structure for a screening design, a critical skill for correctly interpreting experimental outcomes where certain effects are deliberately confounded .",
            "id": "3905377",
            "problem": "A research team is executing a virtual screening campaign for electrolyte formulation in automated battery design and simulation, using Design of Experiments (DOE) to interrogate five binary controllable factors: solvent family $A$, co-solvent presence $B$, salt type $C$, additive package $D$, and synthesis temperature regime $E$. To reduce the number of required high-fidelity electrochemical simulations, they opt for a regular two-level fractional factorial design with $2^{5-2}$ runs, created by imposing two independent generators $G_1$ and $G_2$ such that the defining relations satisfy $I = G_1$ and $I = G_2$, where $I$ denotes the identity. The chosen generators are $G_1 = ABCD$ and $G_2 = ABCE$.\n\nStarting from the fundamental effect-aliasing principle for regular fractional factorial designs—namely, that the alias set of any effect is obtained by multiplying that effect by every word in the complete defining relation derived from the generators, and using the algebraic rules $A^2 = B^2 = C^2 = D^2 = E^2 = I$ and commutativity of factor-word multiplication—construct the complete alias structure implied by these generators and identify which main and two-factor effects are aliased with each other. Then, from the complete defining relation, determine the resolution $R$ of the design, where resolution is defined as the smallest word length (number of letters) among the non-identity words in the defining relation.\n\nExpress your final answer as the single value of $R$ with no units. No rounding is required.",
            "solution": "The problem requires an analysis of a $2^{5-2}$ regular two-level fractional factorial design. The analysis proceeds in three stages as stipulated: first, the derivation of the complete defining relation from the given generators; second, the construction of the alias structure for main and two-factor effects; and third, the determination of the design resolution $R$.\n\n### Step 1: Problem Validation\nThe problem statement provides all necessary information for a standard Design of Experiments (DOE) analysis.\n- **Givens**: Five binary factors denoted $A, B, C, D, E$. The design is a $2^{5-2}$ fractional factorial. The generators are $G_1 = ABCD$ and $G_2 = ABCE$. The algebraic rules are $X^2 = I$ for any factor $X$, where $I$ is the identity, and commutativity of multiplication.\n- **Tasks**:\n    1. Construct the complete alias structure.\n    2. Identify aliasing among main and two-factor effects.\n    3. Determine the design resolution $R$.\n- **Verdict**: The problem is scientifically grounded in the mathematical theory of experimental design, is well-posed, and uses objective, unambiguous language. It is a valid problem.\n\n### Step 2: Derivation of the Complete Defining Relation\nA fractional factorial design is defined by a set of words that are aliased with the identity, $I$. This set, called the defining relation, forms a mathematical group under the operation of word multiplication. The group is generated by the specified generators. For this problem, the generators are $G_1 = ABCD$ and $G_2 = ABCE$.\n\nThe complete defining relation consists of the identity $I$, the generators themselves, and all their possible products.\nThe initial defining relations are:\n$I = G_1 = ABCD$\n$I = G_2 = ABCE$\n\nTo find the complete relation, we must compute the product of the generators, $G_1 G_2$. Using the provided algebraic rules (commutativity and $X^2 = I$ for any factor $X$):\n$$G_1 G_2 = (ABCD)(ABCE) = A \\cdot A \\cdot B \\cdot B \\cdot C \\cdot C \\cdot D \\cdot E = A^2 B^2 C^2 DE$$\nSince $A^2 = I$, $B^2 = I$, and $C^2 = I$, the expression simplifies to:\n$$A^2 B^2 C^2 DE = (I)(I)(I)DE = DE$$\nThus, the third non-identity word in the defining relation is $DE$, and we have $I = DE$.\n\nThe complete defining relation is the set $\\{I, G_1, G_2, G_1 G_2\\}$, which is:\n$$I = ABCD = ABCE = DE$$\n\n### Step 3: Determination of the Design Resolution\nThe resolution of a fractional factorial design, denoted by $R$, is defined as the length of the shortest word in its complete defining relation (excluding the identity word $I$).\nThe non-identity words in the defining relation are $ABCD$, $ABCE$, and $DE$. We determine their lengths (number of letters):\n- Length of $ABCD$ is $4$.\n- Length of $ABCE$ is $4$.\n- Length of $DE$ is $2$.\n\nThe resolution $R$ is the minimum of these lengths:\n$$R = \\min(4, 4, 2) = 2$$\nA design of Resolution II is one in which main effects are aliased with other main effects. This is a highly undesirable property, as it makes it impossible to distinguish the effects of the aliased main factors.\n\n### Step 4: Construction of the Alias Structure\nThe alias set for any effect is obtained by multiplying the effect by every word in the complete defining relation.\n\n**Main Effect Aliases:**\nThe alias chain for a main effect, say $X$, is given by the set $\\{X, X \\cdot (ABCD), X \\cdot (ABCE), X \\cdot (DE)\\}$.\n- For factor $D$:\n  $D \\cdot ABCD = ABC(D^2) = ABCI = ABC$\n  $D \\cdot ABCE = ABCDE$\n  $D \\cdot DE = (D^2)E = IE = E$\n  Thus, the alias chain for $D$ is $D \\leftrightarrow ABC \\leftrightarrow ABCDE \\leftrightarrow E$. Most critically, this shows that the main effect of $D$ is aliased with the main effect of $E$.\n\n- The full set of alias chains for the main effects is:\n  $A \\leftrightarrow BCD \\leftrightarrow BCE \\leftrightarrow ADE$\n  $B \\leftrightarrow ACD \\leftrightarrow ACE \\leftrightarrow BDE$\n  $C \\leftrightarrow ABD \\leftrightarrow ABE \\leftrightarrow CDE$\n  $D \\leftrightarrow E \\leftrightarrow ABC \\leftrightarrow ABCDE$\n\n**Two-Factor Interaction Aliases:**\nThe aliasing among two-factor interactions is determined similarly.\n- For interaction $AB$:\n  $AB \\cdot ABCD = (A^2)(B^2)CD = I \\cdot I \\cdot CD = CD$\n  $AB \\cdot ABCE = (A^2)(B^2)CE = I \\cdot I \\cdot CE = CE$\n  $AB \\cdot DE = ABDE$\n  Thus, the alias chain is $AB \\leftrightarrow CD \\leftrightarrow CE$.\n\n- The full alias structure for two-factor interactions is partitioned into the following sets:\n  1. $\\{AB, CD, CE, ABDE\\}$\n  2. $\\{AC, BD, BE, ACDE\\}$\n  3. $\\{AD, BC, AE, BCDE\\}$\n\n  This can be expressed as aliasing relationships between the two-factor interactions:\n  - $AB \\leftrightarrow CD \\leftrightarrow CE$\n  - $AC \\leftrightarrow BD \\leftrightarrow BE$\n  - $AD \\leftrightarrow BC \\leftrightarrow AE$\n\n  Finally, the interaction $DE$ is itself a word in the defining relation:\n  $DE \\cdot I = DE$\n  $DE \\cdot DE = D^2 E^2 = I \\cdot I = I$\n  This means that the two-factor interaction $DE$ is aliased with the identity, $I$. Its effect is confounded with the overall mean (the intercept term in a regression model) and cannot be estimated from the experimental data.\n\nThe analysis confirms the implications of a Resolution II design: the main effects $D$ and $E$ are aliased with each other. The core question regarding the resolution $R$ has been answered.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Once screening experiments have identified the most influential factors, the focus of virtual screening shifts from exploration to optimization. The goal becomes finding the specific combination of factor levels that yields the best performance for a critical metric, such as long-term capacity retention. This is typically achieved by fitting a more detailed Response Surface Model (RSM), often a quadratic equation, to data from a more focused experiment. This practice provides a direct application of such a model, where you will use multivariate calculus to analyze the fitted response surface, locate its stationary point, and determine if it represents a maximum, minimum, or saddle point—thereby predicting the optimal electrolyte composition .",
            "id": "3905189",
            "problem": "A virtual screening campaign for lithium-ion battery electrolytes uses a second-order response surface model to predict long-term capacity retention after cycling under a constant protocol. Two composition variables are varied using a Central Composite Design (CCD): the coded deviation in the volumetric fraction of ethylene carbonate relative to ethyl methyl carbonate, denoted $x_1$, and the coded deviation in the volumetric ratio of dimethyl carbonate to diethyl carbonate, denoted $x_2$. The fitted quadratic model for the predicted capacity retention (expressed as a unitless decimal fraction) is, in coded variables,\n$$\n\\hat{R}(x_1,x_2) \\;=\\; b_0 \\;+\\; b_1 x_1 \\;+\\; b_2 x_2 \\;+\\; b_{11} x_1^2 \\;+\\; b_{22} x_2^2 \\;+\\; b_{12} x_1 x_2,\n$$\nwith coefficients\n$$\nb_0 = 0.92,\\quad b_1 = 0.05,\\quad b_2 = -0.03,\\quad b_{11} = -0.15,\\quad b_{22} = -0.10,\\quad b_{12} = -0.02.\n$$\nStarting from the definitions of a stationary point in multivariate calculus and the Hessian-based second derivative test, carry out the following:\n\n1. Derive analytically the stationary point $(x_1^{\\star},x_2^{\\star})$ by solving the first-order optimality conditions $\\frac{\\partial \\hat{R}}{\\partial x_1} = 0$ and $\\frac{\\partial \\hat{R}}{\\partial x_2} = 0$.\n\n2. Classify the stationary point as a local minimum, local maximum, or saddle point by analyzing the definiteness of the Hessian matrix of $\\hat{R}(x_1,x_2)$ at $(x_1^{\\star},x_2^{\\star})$.\n\n3. Compute the predicted capacity retention at the stationary point, $\\hat{R}(x_1^{\\star},x_2^{\\star})$, and report this scalar as your final answer.\n\nExpress the final answer as a unitless decimal fraction and round to five significant figures. Do not convert from coded variables to physical composition units for this problem.",
            "solution": "The problem requires finding and classifying the stationary point of a given second-order response surface model for battery capacity retention, and then calculating the predicted retention at that point.\n\nThe model for the predicted capacity retention, $\\hat{R}(x_1, x_2)$, is given by the quadratic function:\n$$\n\\hat{R}(x_1,x_2) = b_0 + b_1 x_1 + b_2 x_2 + b_{11} x_1^2 + b_{22} x_2^2 + b_{12} x_1 x_2\n$$\nwith the coefficients:\n$$\nb_0 = 0.92,\\quad b_1 = 0.05,\\quad b_2 = -0.03,\\quad b_{11} = -0.15,\\quad b_{22} = -0.10,\\quad b_{12} = -0.02.\n$$\nSubstituting the coefficients, the function is:\n$$\n\\hat{R}(x_1,x_2) = 0.92 + 0.05 x_1 - 0.03 x_2 - 0.15 x_1^2 - 0.10 x_2^2 - 0.02 x_1 x_2\n$$\n\n**1. Derivation of the Stationary Point**\n\nA stationary point $(x_1^{\\star}, x_2^{\\star})$ occurs where the gradient of the function is the zero vector. This is found by setting the first-order partial derivatives with respect to $x_1$ and $x_2$ equal to zero.\n\nFirst, we compute the partial derivative with respect to $x_1$:\n$$\n\\frac{\\partial \\hat{R}}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (0.92 + 0.05 x_1 - 0.03 x_2 - 0.15 x_1^2 - 0.10 x_2^2 - 0.02 x_1 x_2) = 0.05 - 2(0.15) x_1 - 0.02 x_2\n$$\n$$\n\\frac{\\partial \\hat{R}}{\\partial x_1} = 0.05 - 0.30 x_1 - 0.02 x_2\n$$\nNext, we compute the partial derivative with respect to $x_2$:\n$$\n\\frac{\\partial \\hat{R}}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (0.92 + 0.05 x_1 - 0.03 x_2 - 0.15 x_1^2 - 0.10 x_2^2 - 0.02 x_1 x_2) = -0.03 - 2(0.10) x_2 - 0.02 x_1\n$$\n$$\n\\frac{\\partial \\hat{R}}{\\partial x_2} = -0.03 - 0.02 x_1 - 0.20 x_2\n$$\nThe first-order optimality conditions are $\\frac{\\partial \\hat{R}}{\\partial x_1} = 0$ and $\\frac{\\partial \\hat{R}}{\\partial x_2} = 0$. This gives the following system of linear equations:\n$$\n\\begin{cases}\n0.05 - 0.30 x_1 - 0.02 x_2 = 0 \\\\\n-0.03 - 0.02 x_1 - 0.20 x_2 = 0\n\\end{cases}\n$$\nRearranging the terms, we get:\n$$\n\\begin{cases}\n0.30 x_1 + 0.02 x_2 = 0.05 & (1) \\\\\n0.02 x_1 + 0.20 x_2 = -0.03 & (2)\n\\end{cases}\n$$\nWe can solve this system. From equation (1), we can express $x_2$ in terms of $x_1$:\n$$\n0.02 x_2 = 0.05 - 0.30 x_1 \\implies x_2 = \\frac{0.05 - 0.30 x_1}{0.02} = 2.5 - 15 x_1\n$$\nSubstitute this expression for $x_2$ into equation (2):\n$$\n0.02 x_1 + 0.20 (2.5 - 15 x_1) = -0.03\n$$\n$$\n0.02 x_1 + 0.5 - 3.0 x_1 = -0.03\n$$\n$$\n-2.98 x_1 = -0.53\n$$\n$$\nx_1^{\\star} = \\frac{-0.53}{-2.98} = \\frac{53}{298}\n$$\nNow, substitute the value of $x_1^{\\star}$ back into the expression for $x_2$:\n$$\nx_2^{\\star} = 2.5 - 15 \\left(\\frac{53}{298}\\right) = \\frac{5}{2} - \\frac{795}{298} = \\frac{5 \\times 149 - 795}{298} = \\frac{745 - 795}{298} = \\frac{-50}{298} = -\\frac{25}{149}\n$$\nThus, the stationary point is $(x_1^{\\star}, x_2^{\\star}) = \\left(\\frac{53}{298}, -\\frac{25}{149}\\right)$.\n\n**2. Classification of the Stationary Point**\n\nTo classify the stationary point, we use the second derivative test, which involves analyzing the Hessian matrix of $\\hat{R}$. The elements of the Hessian matrix $H$ are the second-order partial derivatives.\n$$\n\\frac{\\partial^2 \\hat{R}}{\\partial x_1^2} = \\frac{\\partial}{\\partial x_1} (0.05 - 0.30 x_1 - 0.02 x_2) = -0.30\n$$\n$$\n\\frac{\\partial^2 \\hat{R}}{\\partial x_2^2} = \\frac{\\partial}{\\partial x_2} (-0.03 - 0.02 x_1 - 0.20 x_2) = -0.20\n$$\n$$\n\\frac{\\partial^2 \\hat{R}}{\\partial x_1 \\partial x_2} = \\frac{\\partial^2 \\hat{R}}{\\partial x_2 \\partial x_1} = \\frac{\\partial}{\\partial x_1} (-0.03 - 0.02 x_1 - 0.20 x_2) = -0.02\n$$\nThe Hessian matrix is constant for this quadratic model:\n$$\nH = \\begin{pmatrix} \\frac{\\partial^2 \\hat{R}}{\\partial x_1^2} & \\frac{\\partial^2 \\hat{R}}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 \\hat{R}}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 \\hat{R}}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} -0.30 & -0.02 \\\\ -0.02 & -0.20 \\end{pmatrix}\n$$\nWe calculate the determinant of the Hessian matrix, $\\det(H)$:\n$$\n\\det(H) = (-0.30)(-0.20) - (-0.02)(-0.02) = 0.06 - 0.0004 = 0.0596\n$$\nSince $\\det(H) > 0$, the stationary point is either a local maximum or a local minimum. To determine which, we examine the sign of the first leading principal minor, $\\frac{\\partial^2 \\hat{R}}{\\partial x_1^2}$:\n$$\n\\frac{\\partial^2 \\hat{R}}{\\partial x_1^2} = -0.30\n$$\nSince $\\frac{\\partial^2 \\hat{R}}{\\partial x_1^2} < 0$ and $\\det(H) > 0$, the Hessian matrix is negative definite. Therefore, the stationary point $(x_1^{\\star}, x_2^{\\star})$ is a local maximum.\n\n**3. Computation of Predicted Capacity Retention at the Stationary Point**\n\nFinally, we compute the predicted capacity retention $\\hat{R}(x_1^{\\star}, x_2^{\\star})$ by substituting the coordinates of the stationary point into the model equation.\nA useful simplification for a quadratic response surface is that the value at the stationary point, $\\hat{R}(\\mathbf{x}^{\\star})$, is given by:\n$$\n\\hat{R}(\\mathbf{x}^{\\star}) = b_0 + \\frac{1}{2} (b_1 x_1^{\\star} + b_2 x_2^{\\star})\n$$\nWe first calculate the term $b_1 x_1^{\\star} + b_2 x_2^{\\star}$:\n$$\nb_1 x_1^{\\star} + b_2 x_2^{\\star} = (0.05) \\left(\\frac{53}{298}\\right) + (-0.03) \\left(-\\frac{25}{149}\\right)\n$$\n$$\n= \\frac{2.65}{298} + \\frac{0.75}{149} = \\frac{2.65}{298} + \\frac{2 \\times 0.75}{2 \\times 149} = \\frac{2.65}{298} + \\frac{1.5}{298} = \\frac{2.65 + 1.5}{298} = \\frac{4.15}{298}\n$$\nNow, substitute this result into the formula for $\\hat{R}(\\mathbf{x}^{\\star})$:\n$$\n\\hat{R}(x_1^{\\star}, x_2^{\\star}) = 0.92 + \\frac{1}{2} \\left(\\frac{4.15}{298}\\right) = 0.92 + \\frac{4.15}{596}\n$$\nTo obtain the numerical value, we perform the division:\n$$\n\\frac{4.15}{596} \\approx 0.006963087248\n$$\n$$\n\\hat{R}(x_1^{\\star}, x_2^{\\star}) \\approx 0.92 + 0.006963087248 = 0.926963087248\n$$\nThe problem requires the final answer to be rounded to five significant figures. The first five significant figures of the result are $9$, $2$, $6$, $9$, and $6$. The sixth significant figure is $3$, so we round down.\nThe predicted capacity retention at the stationary point is $0.92696$.",
            "answer": "$$\n\\boxed{0.92696}\n$$"
        },
        {
            "introduction": "Modern battery design rarely involves optimizing a single property; instead, it demands a delicate balance between multiple, often competing, objectives like maximizing ionic conductivity while simultaneously ensuring a wide electrochemical stability window. This multi-objective challenge requires moving beyond traditional RSM to more advanced, algorithm-driven strategies. This practice introduces you to the core components of state-of-the-art automated discovery workflows: non-dominated sorting for identifying optimal trade-offs (the Pareto front) and Expected Hypervolume Improvement (EHVI) for intelligently selecting the next experiment. By implementing these concepts, you will engage with the decision-making logic that powers autonomous research platforms in materials science .",
            "id": "3905367",
            "problem": "You are designing an automated multi-objective campaign for lithium-ion battery electrolyte discovery focused on virtual screening and design of experiments. Each electrolyte is characterized by two measurable objectives to be maximized: ionic conductivity and electrochemical stability window. The task is to implement two core components: non-dominated sorting and selection of the next experiment via Expected Hypervolume Improvement (EHVI) maximization under a Gaussian predictive model.\n\nStart from first principles that are valid bases in this context:\n- Pareto dominance in multi-objective optimization: point $\\mathbf{p} \\in \\mathbb{R}^m$ dominates point $\\mathbf{q} \\in \\mathbb{R}^m$ for a maximization problem if $\\forall i \\in \\{1,\\dots,m\\}$, $p_i \\ge q_i$ and $\\exists j$ such that $p_j > q_j$.\n- The hypervolume indicator: given a set $S \\subset \\mathbb{R}^m$ and a reference point $\\mathbf{r} \\in \\mathbb{R}^m$, the hypervolume $\\mathcal{H}(S;\\mathbf{r})$ is the Lebesgue measure of the union of axis-aligned rectangles $\\prod_{i=1}^m [r_i,s_i]$ for each $\\mathbf{s} \\in S$ in a maximization setting (only rectangles with $s_i \\ge r_i$ contribute nonzero measure).\n- The Expected Hypervolume Improvement (EHVI): given a probabilistic predictive model for a candidate point $\\mathbf{Y}$, the EHVI is $\\mathbb{E}\\left[\\max\\left(0, \\mathcal{H}(S \\cup \\{\\mathbf{Y}\\};\\mathbf{r}) - \\mathcal{H}(S;\\mathbf{r})\\right)\\right]$.\n\nYou must:\n1. Implement a non-dominated sorting routine to assign ranks to a set of measured electrolytes. The rank of a candidate is $1$ if it is non-dominated, $2$ if it is dominated only by rank $1$ points, and so on, following the classic layering definition.\n2. Implement EHVI maximization under a two-objective independent Gaussian predictive model, where each candidate’s predicted objectives are modeled as independent normal random variables. Use Monte Carlo integration to approximate the expectation: draw $N$ independent samples per candidate and compute the average hypervolume improvement. For reproducibility and scientific rigor, use a fixed pseudo-random seed and ensure deterministic outputs.\n3. Select the next experiment as the candidate with the largest EHVI. In case of ties in EHVI values within a tolerance of $10^{-9}$, select the smallest index candidate.\n\nScientific realism and units:\n- Objective $1$ (ionic conductivity) must be expressed in $\\mathrm{S/m}$.\n- Objective $2$ (electrochemical stability window) must be expressed in $\\mathrm{V}$.\n- The hypervolume and EHVI must be expressed in $\\mathrm{S/m}\\cdot\\mathrm{V}$.\n- Angles are not involved.\n- No percentages are used.\n\nMathematical and algorithmic definitions must be used without shortcut formulas; derive your approach from the definitions above. For the Monte Carlo integral, implement direct sampling from the specified Gaussian predictive distributions and compute $\\mathcal{H}$ differences in two dimensions using a correct union-of-rectangles method.\n\nTest suite and required output:\nImplement your program to process the following three test cases. For each case, compute the non-dominated sorting ranks for the measured electrolytes and then select the next experiment by EHVI maximization among the proposed candidates. Use $N=10000$ samples per candidate and a base seed $s=42$; for candidate $k$ within a case, use seed $s+k$.\n\nCase $1$:\n- Measured electrolytes (in the given order), each as $(\\text{conductivity in } \\mathrm{S/m},\\ \\text{window in } \\mathrm{V})$:\n  - $E_1$: $(1.1,\\ 4.2)$\n  - $E_2$: $(0.9,\\ 4.8)$\n  - $E_3$: $(1.3,\\ 4.0)$\n  - $E_4$: $(0.7,\\ 4.6)$\n  - $E_5$: $(1.0,\\ 4.4)$\n- Reference point $\\mathbf{r} = (0.0,\\ 0.0)$.\n- Candidate predictive means $\\boldsymbol{\\mu}_k$ and standard deviations $\\boldsymbol{\\sigma}_k$ (independent normal per objective):\n  - $C_1$: $\\boldsymbol{\\mu}_1 = (1.05,\\ 4.9)$, $\\boldsymbol{\\sigma}_1 = (0.1,\\ 0.2)$\n  - $C_2$: $\\boldsymbol{\\mu}_2 = (1.4,\\ 4.1)$, $\\boldsymbol{\\sigma}_2 = (0.05,\\ 0.15)$\n  - $C_3$: $\\boldsymbol{\\mu}_3 = (0.85,\\ 4.7)$, $\\boldsymbol{\\sigma}_3 = (0.12,\\ 0.1)$\n\nCase $2$:\n- Measured electrolytes:\n  - $E_1$: $(1.2,\\ 4.7)$\n  - $E_2$: $(1.2,\\ 4.7)$\n  - $E_3$: $(0.6,\\ 5.0)$\n  - $E_4$: $(1.5,\\ 3.9)$\n  - $E_5$: $(0.4,\\ 4.0)$\n- Reference point $\\mathbf{r} = (0.0,\\ 0.0)$.\n- Candidates:\n  - $C_1$: $\\boldsymbol{\\mu}_1 = (1.2,\\ 4.8)$, $\\boldsymbol{\\sigma}_1 = (0.0,\\ 0.05)$\n  - $C_2$: $\\boldsymbol{\\mu}_2 = (0.5,\\ 4.1)$, $\\boldsymbol{\\sigma}_2 = (0.1,\\ 0.1)$\n  - $C_3$: $\\boldsymbol{\\mu}_3 = (1.6,\\ 4.0)$, $\\boldsymbol{\\sigma}_3 = (0.05,\\ 0.05)$\n\nCase $3$:\n- Measured electrolytes:\n  - $E_1$: $(1.3,\\ 4.9)$\n  - $E_2$: $(1.1,\\ 4.8)$\n  - $E_3$: $(1.0,\\ 5.0)$\n  - $E_4$: $(1.2,\\ 4.7)$\n- Reference point $\\mathbf{r} = (0.0,\\ 0.0)$.\n- Candidates:\n  - $C_1$: $\\boldsymbol{\\mu}_1 = (0.8,\\ 4.2)$, $\\boldsymbol{\\sigma}_1 = (0.1,\\ 0.1)$\n  - $C_2$: $\\boldsymbol{\\mu}_2 = (0.9,\\ 4.3), \\boldsymbol{\\sigma}_2 = (0.05,\\ 0.1)$\n  - $C_3$: $\\boldsymbol{\\mu}_3 = (1.0,\\ 4.4), \\boldsymbol{\\sigma}_3 = (0.02,\\ 0.05)$\n\nProgram I/O and output specification:\n- There is no input; all data are embedded.\n- For each test case, produce a result structured as $[\\text{ranks},\\ \\text{best\\_index},\\ \\text{best\\_EHVI}]$, where:\n  - $\\text{ranks}$ is a list of integers giving the non-dominated sorting rank of each measured electrolyte in the order provided.\n  - $\\text{best\\_index}$ is an integer identifying the index of the candidate with maximum EHVI under $0$-based indexing (i.e., $0$ for $C_1$, $1$ for $C_2$, $2$ for $C_3$).\n  - $\\text{best\\_EHVI}$ is a float equal to the EHVI of the selected candidate in $\\mathrm{S/m}\\cdot\\mathrm{V}$, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases. For example, the final output must look like $[\\text{case1\\_result},\\text{case2\\_result},\\text{case3\\_result}]$ with nested lists shown explicitly.\n\nAdditional constraints:\n- Implement the hypervolume computation in $2$ dimensions for a maximization problem using the exact union-of-rectangles principle and a correct plane sweep over the $x$-coordinate: sort by $x$ in descending order, maintain the running maximum of $y$, and sum rectangle strips against the reference point.\n- Implement the non-dominated sorting using the classical definition of dominance stated above, without relying on pre-existing library routines.\n- Use deterministic pseudo-random seeds as specified and independent sampling across candidates.",
            "solution": "The task is to design an automated experimental selection strategy for a multi-objective optimization problem in battery electrolyte discovery. The process involves two main components: ranking existing experimental data using non-dominated sorting and selecting the next experiment by maximizing the Expected Hypervolume Improvement (EHVI). This solution is derived from the first principles of Pareto optimization and probabilistic modeling as specified.\n\nThe two objectives to be maximized are ionic conductivity, denoted $o_1$ (in $\\mathrm{S/m}$), and electrochemical stability window, denoted $o_2$ (in $\\mathrm{V}$). A point in the objective space is represented by a vector $\\mathbf{p} = (p_1, p_2)$.\n\n### Part 1: Non-Dominated Sorting\n\nThe first step is to evaluate a set of existing measured electrolyte data points, $E = \\{\\mathbf{e}_1, \\mathbf{e}_2, \\dots, \\mathbf{e}_n\\}$. This evaluation is performed by partitioning the set into ranked, non-dominated fronts.\n\n**Pareto Dominance**: A point $\\mathbf{p} \\in \\mathbb{R}^2$ is said to dominate another point $\\mathbf{q} \\in \\mathbb{R}^2$, denoted $\\mathbf{p} \\succ \\mathbf{q}$, if it is at least as good in all objectives and strictly better in at least one. For this maximization problem, this is formally defined as:\n$$\n\\mathbf{p} \\succ \\mathbf{q} \\iff (p_1 \\ge q_1 \\land p_2 \\ge q_2) \\land (p_1 > q_1 \\lor p_2 > q_2)\n$$\n\n**Sorting Algorithm**: A classical algorithm for non-dominated sorting is employed to assign a rank to each point. The rank of a point corresponds to its front or layer in the dominance hierarchy.\n\n1.  **Initialization**: For each point $\\mathbf{p}$ in the dataset $E$:\n    *   Calculate its domination count, $n_p$, which is the number of other points in $E$ that dominate $\\mathbf{p}$.\n    *   Construct a set, $S_p$, containing all points in $E$ that are dominated by $\\mathbf{p}$.\n\n2.  **First Front Identification**: All points with a domination count $n_p = 0$ are non-dominated by any other point in the set. These points constitute the first Pareto front, $F_1$. All points in $F_1$ are assigned rank $1$.\n\n3.  **Iterative Front Construction**: The algorithm proceeds iteratively to identify subsequent fronts. For each point $\\mathbf{p}$ in the current front $F_i$:\n    *   Iterate through each point $\\mathbf{q}$ in its dominated set $S_p$.\n    *   Decrement the domination count of $\\mathbf{q}$: $n_q \\leftarrow n_q - 1$.\n    *   If $n_q$ becomes $0$, it means that $\\mathbf{q}$ is no longer dominated by any remaining un-ranked points. Thus, $\\mathbf{q}$ belongs to the next front, $F_{i+1}$, and is assigned rank $i+1$.\n\nThis process is repeated until all points in $E$ have been assigned a rank.\n\n### Part 2: Hypervolume Indicator Calculation\n\nThe hypervolume indicator $\\mathcal{H}(S; \\mathbf{r})$ measures the volume (or area in $2$D) of the objective space dominated by a set of points $S$, relative to a reference point $\\mathbf{r}$. For a set $S = \\{\\mathbf{s}_1, \\dots, \\mathbf{s}_k\\}$ and a reference point $\\mathbf{r}=(r_1, r_2)$, the hypervolume is the Lebesgue measure of the union of hyperrectangles formed by each point and the reference point:\n$$\n\\mathcal{H}(S; \\mathbf{r}) = \\text{measure}\\left(\\bigcup_{\\mathbf{s} \\in S} [r_1, s_1] \\times [r_2, s_2]\\right)\n$$\nwhere $[a, b]$ denotes a closed interval, and it is assumed that $s_i \\ge r_i$ for all objectives $i$.\n\n**Calculation Algorithm (2D Slicing Method)**: To compute the area of this union of possibly overlapping rectangles, we use a general slicing algorithm equivalent to a plane sweep.\n\n1.  **Filtering**: Remove any points from $S$ that are dominated by the reference point $\\mathbf{r}$, as they do not contribute to the hypervolume. Let the filtered set be $S'$.\n2.  **Collect X-Coordinates**: Create a sorted list of unique first-coordinate values from all points in $S'$ and the reference point, $X = \\text{sorted}(\\text{unique}(\\{r_1\\} \\cup \\{s_1 | \\mathbf{s} \\in S'\\}))$. Let these be $x'_0, x'_1, \\dots, x'_m$, where $x'_0=r_1$.\n3.  **Sum Slice Areas**: The total hypervolume is calculated by summing the areas of vertical rectangular slices. For each interval $[x'_{j-1}, x'_{j}]$ on the first axis (where $j=1, \\dots, m$):\n    *   The width of the slice is $w_j = x'_{j} - x'_{j-1}$.\n    *   The height of the slice, $h_j$, is determined by the highest point in the union of rectangles over this interval. This corresponds to the maximum second-coordinate value among all points in $S'$ whose first coordinate is greater than or equal to $x'_{j}$. Formally, $h_j = \\max(\\{r_2\\} \\cup \\{s_2 | \\mathbf{s} \\in S', s_1 \\ge x'_{j}\\})$.\n    *   The area of this slice is $w_j \\times (h_j - r_2)$.\n4.  **Total Hypervolume**: The total hypervolume is the sum of these slice areas:\n    $$\n    \\mathcal{H}(S; \\mathbf{r}) = \\sum_{j=1}^{m} (x'_{j} - x'_{j-1}) \\cdot (\\max(\\{r_2\\} \\cup \\{s_2 | \\mathbf{s} \\in S', s_1 \\ge x'_{j}\\}) - r_2)\n    $$\nThis method is robust and correctly computes the hypervolume for any set of points, not just for non-dominated fronts.\n\n### Part 3: Expected Hypervolume Improvement (EHVI)\n\nThe EHVI is a criterion for selecting the next experiment. It quantifies the expected increase in hypervolume from evaluating a new candidate point, given a probabilistic model of its outcome.\n\n**Hypervolume Improvement (HVI)**: The improvement from adding a new point $\\mathbf{y}$ to the current non-dominated front $F_1$ is:\n$$\n\\text{HVI}(\\mathbf{y}) = \\mathcal{H}(F_1 \\cup \\{\\mathbf{y}\\}; \\mathbf{r}) - \\mathcal{H}(F_1; \\mathbf{r})\n$$\nNote that if $\\mathbf{y}$ is dominated by any point in $F_1$, the union of rectangles does not change, so $\\mathcal{H}(F_1 \\cup \\{\\mathbf{y}\\}; \\mathbf{r}) = \\mathcal{H}(F_1; \\mathbf{r})$ and the improvement is $0$.\n\n**Expected Hypervolume Improvement (EHVI)**: For a candidate whose predicted outcome is a random variable $\\mathbf{Y}$, the EHVI is the expectation of the HVI:\n$$\n\\text{EHVI} = \\mathbb{E}[\\text{HVI}(\\mathbf{Y})] = \\mathbb{E}\\left[\\max\\left(0, \\mathcal{H}(F_1 \\cup \\{\\mathbf{Y}\\};\\mathbf{r}) - \\mathcal{H}(F_1;\\mathbf{r})\\right)\\right]\n$$\nThe problem specifies that for each candidate $k$, the predictive model for its objectives $\\mathbf{Y}_k = (Y_{k1}, Y_{k2})$ is a pair of independent Gaussian random variables, $Y_{k1} \\sim \\mathcal{N}(\\mu_{k1}, \\sigma_{k1}^2)$ and $Y_{k2} \\sim \\mathcal{N}(\\mu_{k2}, \\sigma_{k2}^2)$.\n\n**Monte Carlo Approximation**: The EHVI expectation is approximated using Monte Carlo integration.\n1.  **Calculate Current Hypervolume**: First, compute the hypervolume of the current rank-1 front, $\\mathcal{H}_{\\text{current}} = \\mathcal{H}(F_1; \\mathbf{r})$.\n2.  **Sampling**: For each candidate $k$, generate $N$ independent samples, $\\{\\mathbf{y}^{(1)}, \\mathbf{y}^{(2)}, \\dots, \\mathbf{y}^{(N)}\\}$, where each $\\mathbf{y}^{(i)} = (y_1^{(i)}, y_2^{(i)})$ is drawn from the candidate's predictive distribution $\\mathcal{N}(\\boldsymbol{\\mu}_k, \\text{diag}(\\boldsymbol{\\sigma}_k^2))$.\n3.  **Average Improvement**: For each sample $\\mathbf{y}^{(i)}$, calculate the hypervolume of the augmented set, $\\mathcal{H}_{\\text{new}}^{(i)} = \\mathcal{H}(F_1 \\cup \\{\\mathbf{y}^{(i)}\\}; \\mathbf{r})$. The improvement for this sample is $\\text{HVI}^{(i)} = \\mathcal{H}_{\\text{new}}^{(i)} - \\mathcal{H}_{\\text{current}}$.\n4.  **EHVI Estimate**: The EHVI for candidate $k$ is the average of these sample improvements:\n    $$\n    \\text{EHVI}_k \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\text{HVI}^{(i)}\n    $$\n\n### Part 4: Selection Criterion\n\nThe final step is to select the next experiment from the set of candidates. The candidate with the highest EHVI is chosen. To ensure a deterministic outcome in the case of ties, if multiple candidates have EHVI values that are maximal within a numerical tolerance of $10^{-9}$, the one with the smallest original index is selected.\n\nThe overall procedure for each test case is:\n1.  Apply non-dominated sorting to the measured electrolytes to find their ranks and identify the first Pareto front $F_1$.\n2.  Calculate the baseline hypervolume $\\mathcal{H}_{\\text{current}} = \\mathcal{H}(F_1; \\mathbf{r})$.\n3.  For each candidate, estimate its $\\text{EHVI}$ using the Monte Carlo procedure with $N=10000$ samples and the specified seeding protocol ($s+k$ for candidate $k$).\n4.  Select the candidate with the maximum EHVI, breaking ties by choosing the lowest index.\n5.  Report the ranks, the index of the best candidate, and its EHVI value.",
            "answer": "```python\nimport numpy as np\n\n# Note: scipy is listed in the environment but not strictly necessary for this problem.\n# All required functionality is available in numpy.\n\ndef solve():\n    \"\"\"\n    Main function to process test cases for battery electrolyte optimization.\n    \n    This function orchestrates the entire process:\n    1. Defines the test cases.\n    2. For each case, performs non-dominated sorting on measured data.\n    3. Calculates the current Pareto front and its hypervolume.\n    4. For each candidate, computes the Expected Hypervolume Improvement (EHVI) via Monte Carlo simulation.\n    5. Selects the best candidate based on maximizing EHVI.\n    6. Formats and prints the final results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"measured\": [(1.1, 4.2), (0.9, 4.8), (1.3, 4.0), (0.7, 4.6), (1.0, 4.4)],\n            \"ref_point\": (0.0, 0.0),\n            \"candidates\": {\n                \"means\": [(1.05, 4.9), (1.4, 4.1), (0.85, 4.7)],\n                \"stds\": [(0.1, 0.2), (0.05, 0.15), (0.12, 0.1)],\n            },\n        },\n        {\n            \"measured\": [(1.2, 4.7), (1.2, 4.7), (0.6, 5.0), (1.5, 3.9), (0.4, 4.0)],\n            \"ref_point\": (0.0, 0.0),\n            \"candidates\": {\n                \"means\": [(1.2, 4.8), (0.5, 4.1), (1.6, 4.0)],\n                \"stds\": [(0.0, 0.05), (0.1, 0.1), (0.05, 0.05)],\n            },\n        },\n        {\n            \"measured\": [(1.3, 4.9), (1.1, 4.8), (1.0, 5.0), (1.2, 4.7)],\n            \"ref_point\": (0.0, 0.0),\n            \"candidates\": {\n                \"means\": [(0.8, 4.2), (0.9, 4.3), (1.0, 4.4)],\n                \"stds\": [(0.1, 0.1), (0.05, 0.1), (0.02, 0.05)],\n            },\n        },\n    ]\n\n    N_SAMPLES = 10000\n    BASE_SEED = 42\n    TIE_TOLERANCE = 1e-9\n\n    def dominates(p1, p2):\n        \"\"\"Checks if point p1 dominates point p2 for a maximization problem.\"\"\"\n        p1 = np.asarray(p1)\n        p2 = np.asarray(p2)\n        return np.all(p1 >= p2) and np.any(p1 > p2)\n\n    def non_dominated_sorting(points):\n        \"\"\"Performs non-dominated sorting on a list of points.\"\"\"\n        n_points = len(points)\n        domination_counts = [0] * n_points\n        dominated_sets = [[] for _ in range(n_points)]\n        \n        point_indices = list(range(n_points))\n        \n        for i in range(n_points):\n            for j in range(i + 1, n_points):\n                p1 = points[i]\n                p2 = points[j]\n                \n                if dominates(p1, p2):\n                    domination_counts[j] += 1\n                    dominated_sets[i].append(j)\n                elif dominates(p2, p1):\n                    domination_counts[i] += 1\n                    dominated_sets[j].append(i)\n\n        ranks = [0] * n_points\n        fronts = [[] for _ in range(n_points + 1)]\n        \n        for i in range(n_points):\n            if domination_counts[i] == 0:\n                ranks[i] = 1\n                fronts[1].append(i)\n        \n        rank_idx = 1\n        while fronts[rank_idx]:\n            next_front = []\n            for p_idx in fronts[rank_idx]:\n                for q_idx in dominated_sets[p_idx]:\n                    domination_counts[q_idx] -= 1\n                    if domination_counts[q_idx] == 0:\n                        ranks[q_idx] = rank_idx + 1\n                        next_front.append(q_idx)\n            rank_idx += 1\n            fronts[rank_idx] = next_front\n            \n        return ranks, fronts[1]\n\n    def hypervolume_2d(points, ref_point):\n        \"\"\"\n        Calculates the 2D hypervolume of a set of points relative to a reference point.\n        Uses a slicing method for general applicability.\n        \"\"\"\n        ref_x, ref_y = ref_point\n        \n        # Filter points that are dominated by the reference point\n        effective_points = [p for p in points if p[0] >= ref_x and p[1] >= ref_y]\n        \n        if not effective_points:\n            return 0.0\n\n        # Collect and sort unique x-coordinates\n        x_coords = sorted(list(set([p[0] for p in effective_points] + [ref_x])))\n        \n        total_hv = 0.0\n        \n        # Iterate through vertical slices defined by x-coordinates\n        for i in range(1, len(x_coords)):\n            x_i = x_coords[i]\n            x_prev = x_coords[i-1]\n            width = x_i - x_prev\n            \n            # Determine the height of the slice\n            max_y = ref_y\n            # Find max y among points whose x-coordinate covers the current slice\n            relevant_points_y = [p[1] for p in effective_points if p[0] >= x_i]\n            if relevant_points_y:\n                max_y = max(relevant_points_y)\n            \n            height = max_y - ref_y\n            total_hv += width * height\n            \n        return total_hv\n\n    def calculate_ehvi(candidate_mean, candidate_std, pareto_front, h_current, ref_point, n_samples, seed):\n        \"\"\"\n        Calculates EHVI for a candidate using Monte Carlo simulation.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        samples_x = rng.normal(candidate_mean[0], candidate_std[0], n_samples)\n        samples_y = rng.normal(candidate_mean[1], candidate_std[1], n_samples)\n\n        total_hvi = 0.0\n        for i in range(n_samples):\n            sample_point = (samples_x[i], samples_y[i])\n            \n            # Calculate new hypervolume with the sample point\n            h_new = hypervolume_2d(pareto_front + [sample_point], ref_point)\n            \n            # Improvement is the difference (will be >= 0)\n            hvi = h_new - h_current\n            total_hvi += hvi\n            \n        return total_hvi / n_samples\n\n    all_results = []\n    for case in test_cases:\n        measured_points = case[\"measured\"]\n        ref_point = case[\"ref_point\"]\n        candidates = case[\"candidates\"]\n        \n        ranks, front1_indices = non_dominated_sorting(measured_points)\n        pareto_front = [measured_points[i] for i in front1_indices]\n        \n        h_current = hypervolume_2d(pareto_front, ref_point)\n        \n        ehvi_values = []\n        num_candidates = len(candidates[\"means\"])\n        \n        for k in range(num_candidates):\n            mean = candidates[\"means\"][k]\n            std = candidates[\"stds\"][k]\n            seed = BASE_SEED + k\n            \n            ehvi = calculate_ehvi(mean, std, pareto_front, h_current, ref_point, N_SAMPLES, seed)\n            ehvi_values.append(ehvi)\n        \n        # Selection: find max EHVI, break ties with smallest index\n        max_ehvi = -1.0\n        # Find the true maximum value first\n        if ehvi_values:\n            max_ehvi = max(ehvi_values)\n\n        best_index = -1\n        for k, ehvi in enumerate(ehvi_values):\n            # Check if this ehvi is within tolerance of the maximum\n            if max_ehvi - ehvi <= TIE_TOLERANCE:\n                best_index = k\n                break # First one found is the smallest index\n        \n        best_ehvi = ehvi_values[best_index]\n        \n        # Format the result list for this case\n        case_result = [ranks, best_index, round(best_ehvi, 6)]\n        all_results.append(case_result)\n\n    # Convert list of lists to string representation for final output\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}