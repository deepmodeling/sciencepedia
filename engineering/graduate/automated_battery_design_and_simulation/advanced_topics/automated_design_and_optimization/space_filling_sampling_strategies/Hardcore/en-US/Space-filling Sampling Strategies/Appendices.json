{
    "hands_on_practices": [
        {
            "introduction": "To build effective surrogate models, the underlying sample points must cover the design space thoroughly. This exercise introduces fundamental geometric metrics to quantify this 'space-filling' quality: the fill distance $h_X$, separation distance $q_X$, and mesh ratio $\\rho_X$. By computing these values for various sample distributions, you will develop a concrete understanding of how they diagnose common design flaws like clustering and poor coverage .",
            "id": "3951685",
            "problem": "Consider a three-parameter, normalized design space for automated battery design and simulation, defined as the unit cube $[0,1]^3$, where the parameters are electrode porosity $\\phi$, electrolyte salt concentration $c$, and separator thickness $\\delta$, each normalized to lie in $[0,1]$. Let $X = \\{x_i\\}_{i=1}^N \\subset [0,1]^3$ be a finite sample set used to train a surrogate model for a high-fidelity electrochemical simulator. The space-filling quality of $X$ can be characterized by the fill distance $h_X$, the separation distance $q_X$, and the mesh ratio $\\rho_X = h_X/q_X$, under the standard Euclidean metric in $\\mathbb{R}^3$. The fill distance $h_X$ is the worst-case distance from any point in the domain to its nearest sample in $X$, the separation distance $q_X$ is half the minimum Euclidean distance between distinct samples in $X$, and the mesh ratio $\\rho_X$ is the ratio of these two quantities.\n\nYou must compute $h_X$, $q_X$, and $\\rho_X$ for several specified sample sets by approximating $h_X$ using a uniform Cartesian grid search over $[0,1]^3$ with a prescribed resolution $n_g$ points per axis (including the endpoints) to evaluate the nearest-sample distances at grid points. Use the Euclidean norm to measure distances. Distances are to be expressed in units of the normalized cube edge length (dimensionless). The angle unit is not applicable. The outputs must be rounded to $6$ decimal places.\n\nFor each test case, the input to your program is fixed and embedded in the program: the sample set $X$ and the grid resolution $n_g$. Your program should calculate a triple $[h_X,q_X,\\rho_X]$ for each test case and aggregate the results across all test cases into a single line of output in the exact format described below.\n\nUse the following test suite (each case provides a specific $X$ and $n_g$):\n\n- Case A (diverse design, akin to Latin Hypercube Sampling (LHS)): $N = 10$, $n_g = 25$, with\n  $$\n  X_A = \\Big\\{\n  (0.05,0.62,0.11),\\,\n  (0.18,0.86,0.73),\\,\n  (0.29,0.19,0.44),\\,\n  (0.41,0.48,0.95),\\,\n  (0.52,0.03,0.29),\\,\n  (0.63,0.77,0.05),\\,\n  (0.74,0.34,0.58),\\,\n  (0.80,0.10,0.88),\\,\n  (0.92,0.56,0.21),\\,\n  (0.15,0.72,0.37)\n  \\Big\\}.\n  $$\n\n- Case B (clustered near one corner, illustrating poor coverage): $N = 10$, $n_g = 25$, with\n  $$\n  X_B = \\Big\\{\n  (0.02,0.01,0.03),\\,\n  (0.05,0.04,0.02),\\,\n  (0.08,0.06,0.05),\\,\n  (0.10,0.02,0.08),\\,\n  (0.07,0.09,0.04),\\,\n  (0.12,0.11,0.07),\\,\n  (0.15,0.14,0.06),\\,\n  (0.18,0.16,0.10),\\,\n  (0.20,0.19,0.12),\\,\n  (0.22,0.18,0.15)\n  \\Big\\}.\n  $$\n\n- Case C (structured interior grid, $3\\times 3\\times 3$): $N = 27$, $n_g = 30$, with\n  $$\n  X_C = \\{(x,y,z) \\mid x \\in \\{1/6,\\, 1/2,\\, 5/6\\},\\; y \\in \\{1/6,\\, 1/2,\\, 5/6\\},\\; z \\in \\{1/6,\\, 1/2,\\, 5/6\\}\\}.\n  $$\n\n- Case D (only cube corners): $N = 8$, $n_g = 30$, with\n  $$\n  X_D = \\{(x,y,z) \\mid x \\in \\{0,\\,1\\},\\; y \\in \\{0,\\,1\\},\\; z \\in \\{0,\\,1\\}\\}.\n  $$\n\nScientific realism requirement: You must use the Euclidean distance in $\\mathbb{R}^3$; approximate $h_X$ by evaluating the nearest-sample distance at each point of a uniform Cartesian grid of resolution $n_g$ per axis over $[0,1]^3$, including endpoints; compute $q_X$ exactly from pairwise sample distances as half the minimum distance between distinct samples; then compute $\\rho_X = h_X/q_X$. No other simplifications are allowed.\n\nDesign for coverage: The test suite explores a general \"happy path\" case (Case A), a severe clustering edge case (Case B), a structured quasi-uniform interior grid (Case C), and a boundary-only case (Case D).\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of the four per-case triples, each triple itself being a comma-separated list of three floats rounded to $6$ decimal places, with no spaces, enclosed in square brackets. For example, your output must look like\n$$\n[\\,[h_A,q_A,\\rho_A],\\,[h_B,q_B,\\rho_B],\\,[h_C,q_C,\\rho_C],\\,[h_D,q_D,\\rho_D]\\,]\n$$\nbut printed as plain text without extra spaces, using decimal floats. The values for $h_X$, $q_X$, and $\\rho_X$ are dimensionless.",
            "solution": "The problem requires the computation of three standard metrics for characterizing the space-filling properties of a sample set $X = \\{x_i\\}_{i=1}^N$ within the three-dimensional unit cube $[0,1]^3$. These metrics are the fill distance $h_X$, the separation distance $q_X$, and the mesh ratio $\\rho_X$. The computations are to be performed for four distinct sample sets, $X_A, X_B, X_C,$ and $X_D$. The underlying space is $\\mathbb{R}^3$ equipped with the standard Euclidean norm, $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2 + v_3^2}$.\n\nThe solution methodology is implemented in three sequential steps for each test case: calculation of the separation distance $q_X$, approximation of the fill distance $h_X$, and finally, computation of the mesh ratio $\\rho_X$.\n\n### 1. Calculation of the Separation Distance, $q_X$\n\nThe separation distance $q_X$ quantifies the minimum spacing between points in the sample set $X$. It is formally defined as half the minimum Euclidean distance between any two distinct points in the set:\n$$\nq_X = \\frac{1}{2} \\min_{i \\neq j} \\|x_i - x_j\\|_2\n$$\nA larger $q_X$ implies that the sample points are well-separated, avoiding clustering.\n\n**Algorithmic Approach**:\nTo compute $q_X$, we must calculate the Euclidean distance between all unique pairs of points $(x_i, x_j)$ in the set $X$ where $i \\neq j$. The total number of such pairs for a set of $N$ points is $\\binom{N}{2} = N(N-1)/2$. After computing all these pairwise distances, the minimum among them, $d_{min} = \\min_{i \\neq j} \\|x_i - x_j\\|_2$, is found. The separation distance is then $q_X = d_{min} / 2$. This computation can be efficiently performed using vectorized library functions, such as `scipy.spatial.distance.pdist`, which computes the pairwise distances for all points in a given collection.\n\n### 2. Approximation of the Fill Distance, $h_X$\n\nThe fill distance $h_X$ (also known as dispersion) measures the largest \"gap\" in the sampling. It is defined as the maximum possible distance from any point in the domain $[0,1]^3$ to its nearest sample point in $X$:\n$$\nh_X = \\sup_{p \\in [0,1]^3} \\left( \\min_{x_i \\in X} \\|p - x_i\\|_2 \\right)\n$$\nA smaller $h_X$ indicates better coverage of the design space, as no point in the domain is too far from a sample.\n\n**Algorithmic Approach**:\nCalculating the exact value of $h_X$ is a computationally difficult global optimization problem. The problem specifies an approximation method based on a discrete search over a uniform Cartesian grid.\nA grid $G$ is constructed within the domain $[0,1]^3$ with a resolution of $n_g$ points along each axis, including the endpoints. The coordinates for each axis are given by the set $\\{0, 1/(n_g-1), 2/(n_g-1), \\ldots, 1\\}$. The full grid $G$ is the Cartesian product of these coordinate sets, resulting in $n_g^3$ grid points in total.\n\nThe approximated fill distance, which we will also denote by $h_X$, is then the maximum of the nearest-sample distances evaluated at each grid point $g_k \\in G$:\n$$\nh_X \\approx \\max_{g_k \\in G} \\left( \\min_{x_i \\in X} \\|g_k - x_i\\|_2 \\right)\n$$\nThis procedure involves the following steps:\n1.  Generate the set of all $n_g^3$ grid points $G$.\n2.  For each grid point $g_k \\in G$, calculate its distance to every sample point $x_i \\in X$.\n3.  For each $g_k$, find the minimum of these distances, which is the distance to its nearest neighbor in $X$.\n4.  The fill distance $h_X$ is the maximum value among all these minimum distances.\n\nThis computation can be effectively vectorized. For example, `scipy.spatial.distance.cdist` can compute a matrix of distances between all grid points and all sample points. Subsequent `min` and `max` operations along the appropriate axes of this matrix yield the final value for $h_X$.\n\n### 3. Calculation of the Mesh Ratio, $\\rho_X$\n\nThe mesh ratio $\\rho_X$ is a dimensionless quantity that combines the fill and separation distances to provide a single measure of sample quality. It is defined as:\n$$\n\\rho_X = \\frac{h_X}{q_X}\n$$\nAn ideal set of points would have a mesh ratio close to $1$, which would correspond to the case where the radius of the largest empty sphere ($h_X$) is equal to the radius of the smallest sphere of influence around a sample point ($q_X$). Large values of $\\rho_X$ can indicate either poor coverage (large $h_X$) or significant clustering (small $q_X$).\n\n**Algorithmic Approach**:\nOnce the values for $h_X$ and $q_X$ are computed as described above, $\\rho_X$ is obtained by their direct division. If $q_X$ is zero (which happens if there are duplicate points in the sample set), the mesh ratio is undefined. However, the provided test cases consist of distinct points, so $q_X > 0$.\n\nBy applying this three-step procedure to each of the four specified test cases, we systematically derive the required metrics. The results are then rounded to $6$ decimal places and formatted as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, cdist\n\ndef solve():\n    \"\"\"\n    Computes space-filling metrics (h_X, q_X, rho_X) for four test cases\n    of sample sets in a 3D unit cube.\n    \"\"\"\n\n    def compute_metrics(X, n_g):\n        \"\"\"\n        Calculates separation distance, fill distance, and mesh ratio.\n\n        Args:\n            X (np.ndarray): A set of N sample points, shape (N, 3).\n            n_g (int): The number of grid points per axis for h_X approximation.\n\n        Returns:\n            tuple: A tuple containing (h_X, q_X, rho_X).\n        \"\"\"\n        # Ensure X is a NumPy array\n        X = np.array(X)\n        N = X.shape[0]\n\n        # 1. Compute Separation Distance (q_X)\n        # q_X is half the minimum Euclidean distance between distinct samples.\n        if N > 1:\n            # pdist computes pairwise distances between all points in X.\n            pairwise_distances = pdist(X, 'euclidean')\n            min_dist = np.min(pairwise_distances)\n            q_X = 0.5 * min_dist\n        else:\n            # Undefined for a single point, but we can set it to infinity\n            # to indicate no \"separation\" constraint.\n            # Handle as per problem context if it arises. For these tests N > 1.\n            q_X = np.inf\n\n        # 2. Approximate Fill Distance (h_X)\n        # h_X is the max distance from any point in the domain to its nearest sample.\n        # We approximate this by searching over a uniform grid.\n        \n        # Create grid points\n        axis_coords = np.linspace(0, 1, n_g)\n        grid_points = np.stack(np.meshgrid(axis_coords, axis_coords, axis_coords), axis=-1).reshape(-1, 3)\n\n        # Compute distances from each grid point to each sample point.\n        # cdist(A, B) creates a matrix where entry (i,j) is the distance from A[i] to B[j].\n        # Shape: (n_g**3, N)\n        dist_matrix = cdist(grid_points, X, 'euclidean')\n\n        # For each grid point, find the minimum distance to any sample point.\n        # Shape: (n_g**3,)\n        min_dists_from_grid = np.min(dist_matrix, axis=1)\n\n        # The fill distance h_X is the maximum of these minimum distances.\n        h_X = np.max(min_dists_from_grid)\n\n        # 3. Compute Mesh Ratio (rho_X)\n        if q_X > 0:\n            rho_X = h_X / q_X\n        else:\n            # This case (duplicate points) does not occur in the test suite.\n            rho_X = np.inf\n            \n        return h_X, q_X, rho_X\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Diverse design\n        {\n            \"name\": \"A\",\n            \"n_g\": 25,\n            \"X\": [\n                (0.05, 0.62, 0.11), (0.18, 0.86, 0.73), (0.29, 0.19, 0.44),\n                (0.41, 0.48, 0.95), (0.52, 0.03, 0.29), (0.63, 0.77, 0.05),\n                (0.74, 0.34, 0.58), (0.80, 0.10, 0.88), (0.92, 0.56, 0.21),\n                (0.15, 0.72, 0.37)\n            ]\n        },\n        # Case B: Clustered design\n        {\n            \"name\": \"B\",\n            \"n_g\": 25,\n            \"X\": [\n                (0.02, 0.01, 0.03), (0.05, 0.04, 0.02), (0.08, 0.06, 0.05),\n                (0.10, 0.02, 0.08), (0.07, 0.09, 0.04), (0.12, 0.11, 0.07),\n                (0.15, 0.14, 0.06), (0.18, 0.16, 0.10), (0.20, 0.19, 0.12),\n                (0.22, 0.18, 0.15)\n            ]\n        },\n        # Case C: Structured interior grid\n        {\n            \"name\": \"C\",\n            \"n_g\": 30,\n            \"X\": [(x, y, z) for x in [1/6, 1/2, 5/6] \n                              for y in [1/6, 1/2, 5/6]\n                              for z in [1/6, 1/2, 5/6]]\n        },\n        # Case D: Cube corners\n        {\n            \"name\": \"D\",\n            \"n_g\": 30,\n            \"X\": [(x, y, z) for x in [0, 1] for y in [0, 1] for z in [0, 1]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        h, q, rho = compute_metrics(case[\"X\"], case[\"n_g\"])\n        results.append([round(val, 6) for val in [h, q, rho]])\n\n    # Format the output string as per the requirement:\n    # [[h_A,q_A,rho_A],[h_B,q_B,rho_B],...] with 6 decimal places.\n    formatted_cases = []\n    for case_result in results:\n        # Format each number to 6 decimal places.\n        formatted_nums = [f\"{num:.6f}\" for num in case_result]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_cases)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond evaluating existing designs, a key skill is generating new ones. This practice introduces the greedy farthest-point algorithm, a powerful sequential method for creating space-filling samples by iteratively placing new points in the largest voids. You will implement this strategy in a weighted metric space, a crucial technique for handling engineering parameters with different physical scales and sensitivities .",
            "id": "3951689",
            "problem": "Consider a parameter domain for lithium-ion battery design represented as a hyperrectangle $\\Omega \\subset \\mathbb{R}^d$, with axes corresponding to physically meaningful parameters. To construct space-filling designs suitable for automated battery simulation, implement a greedy farthest-point sampling strategy in a weighted metric space, where weights encode dimension-specific characteristic scales to render the metric dimensionless and reflect parameter sensitivity. Let $X_k = \\{x_1, \\dots, x_k\\} \\subset \\Omega$ denote the current design set, and let $C \\subset \\Omega$ be a finite candidate set sampled on a grid. Define a diagonal positive-definite weight matrix $W = \\mathrm{diag}(w_1, \\dots, w_d)$ with $w_i = 1/s_i^2$, where $s_i > 0$ is the characteristic scale for the $i$-th parameter. Define the weighted distance between $x, y \\in \\Omega$ by\n$$\nd_W(x, y) = \\sqrt{(x - y)^\\top W (x - y)} = \\sqrt{\\sum_{i=1}^d w_i (x_i - y_i)^2},\n$$\nwhich is dimensionless when $s_i$ is chosen to match the unit of the $i$-th coordinate.\n\nFor a finite set $X \\subset \\Omega$, define the separation distance\n$$\nq_X = \\frac{1}{2} \\min_{\\substack{x, y \\in X\\\\ x \\neq y}} d_W(x, y),\n$$\nand the fill distance with respect to the candidate set $C$\n$$\nh_X = \\max_{z \\in C} \\min_{x \\in X} d_W(z, x).\n$$\nAdopt the convention $q_X = 0$ when $|X| = 1$. The greedy farthest-point algorithm iteratively augments $X_k$ by selecting\n$$\nx_{k+1} = \\arg\\max_{z \\in C \\setminus X_k} \\left( \\min_{x \\in X_k} d_W(z, x) \\right),\n$$\nstarting from an initial point chosen as the candidate closest to the center of $\\Omega$ under $d_W$. After each addition, compute $q_{X_k}$ and $h_{X_k}$. All distances are dimensionless and must be reported as floats rounded to six decimal places.\n\nYou must implement a complete program that:\n- Builds the candidate set $C$ as a full grid over $\\Omega$ with specified resolutions along each axis.\n- Initializes $X_1$ by selecting the candidate closest (under $d_W$) to the center of $\\Omega$.\n- Applies the greedy farthest-point rule to select $K$ points and, after each addition, computes $q_{X_k}$ and $h_{X_k}$ on the candidate set $C$.\n\nUse the following test suite, which covers typical and edge behaviors in battery parameter domains:\n\nTest Case A (Happy path, three-dimensional, moderate anisotropy):\n- Dimension $d = 3$ with parameters: electrode thickness $t$ in meters, porosity $\\varepsilon$ dimensionless, and electrolyte conductivity $\\kappa$ in siemens per meter.\n- Bounds $\\Omega = [60 \\times 10^{-6}, 120 \\times 10^{-6}] \\times [0.25, 0.45] \\times [0.8, 2.0]$.\n- Characteristic scales $s = [30 \\times 10^{-6}, 0.10, 0.60]$ so $W = \\mathrm{diag}(1/s_i^2)$.\n- Candidate grid resolutions $\\text{res} = [7, 5, 6]$.\n- Number of points $K = 6$.\n\nTest Case B (Boundary behavior, two-dimensional, strong anisotropy):\n- Dimension $d = 2$ with parameters: solid-phase diffusion coefficient $D_s$ in square meters per second, and exchange current density $i_0$ in amperes per square meter.\n- Bounds $\\Omega = [1 \\times 10^{-14}, 5 \\times 10^{-14}] \\times [0.5, 2.5]$.\n- Characteristic scales $s = [0.5 \\times 10^{-14}, 1.0]$ so $W = \\mathrm{diag}(1/s_i^2)$.\n- Candidate grid resolutions $\\text{res} = [8, 9]$.\n- Number of points $K = 5$.\n\nTest Case C (Edge case, one-dimensional, candidate exhaustion):\n- Dimension $d = 1$ with parameter: separator thickness $\\delta$ in meters.\n- Bounds $\\Omega = [10 \\times 10^{-6}, 12 \\times 10^{-6}]$.\n- Characteristic scales $s = [1 \\times 10^{-6}]$ so $W = \\mathrm{diag}(1/s_i^2)$.\n- Candidate grid resolution $\\text{res} = [5]$.\n- Number of points $K = 5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of pairs $[q_{X_k}, h_{X_k}]$ for $k = 1, \\dots, K$. For example, the output must be of the form\n$$\n[ \\text{caseA}, \\text{caseB}, \\text{caseC} ]\n$$\nwith each $\\text{caseX}$ equal to a list of lists of floats rounded to six decimal places. All distances are dimensionless and must be reported as floats rounded to six decimal places. Angle units are not applicable. Percentages are not applicable.",
            "solution": "The problem statement is valid. It is scientifically grounded in the field of Design of Experiments (DoE) for computational simulations, mathematically well-posed, objective, and provides a complete and consistent set of definitions and data for implementation. The problem asks for the implementation of a greedy farthest-point sampling algorithm to generate a space-filling design within a specified hyperrectangular parameter domain $\\Omega \\subset \\mathbb{R}^d$. The solution involves several integrated steps, based on the principles of weighted metric spaces and iterative optimization.\n\nFirst, we establish the geometric setting. The parameter domain $\\Omega$ is defined by its lower and upper bounds along each of its $d$ dimensions. The candidate points $C$, from which the design points are selected, form a discrete grid over $\\Omega$. This grid is constructed by taking the Cartesian product of one-dimensional, linearly spaced points along each axis, with specified resolutions. The number of points in the grid for the $i$-th dimension is $\\text{res}_i$.\n\nA crucial concept is the weighted distance $d_W(x, y)$, defined as:\n$$\nd_W(x, y) = \\sqrt{(x - y)^\\top W (x - y)} = \\sqrt{\\sum_{i=1}^d w_i (x_i - y_i)^2}\n$$\nThe weight matrix $W$ is diagonal, with entries $w_i = 1/s_i^2$, where $s_i$ is a characteristic scale for the $i$-th parameter. This metric serves two purposes: it renders the distance dimensionless by normalizing each coordinate difference $(x_i - y_i)$ by its scale $s_i$, and it allows for encoding differing sensitivities or importance across parameters. All subsequent geometric calculations are performed in this weighted space.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: The design set is initialized with a single point, $X_1 = \\{x_1\\}$. The point $x_1$ is chosen from the candidate set $C$ to be the one closest to the geometric center of the domain $\\Omega$ under the weighted distance $d_W$. The center $c$ of $\\Omega = \\prod_{i=1}^d [l_i, u_i]$ is the point with coordinates $c_i = (l_i + u_i)/2$. Thus, $x_1 = \\arg\\min_{z \\in C} d_W(z, c)$.\n\n2.  **Iterative Selection**: For $k = 1, \\dots, K-1$, the design set $X_k$ is augmented to $X_{k+1}$ by adding the candidate point that is farthest from the existing set $X_k$. This is the greedy farthest-point or minimax selection rule:\n    $$\n    x_{k+1} = \\arg\\max_{z \\in C \\setminus X_k} \\left( \\min_{x \\in X_k} d_W(z, x) \\right)\n    $$\n    This rule iteratively places new points in the least-sampled regions of the domain, as measured by the fill distance on the candidate set. To implement this efficiently, we maintain an array of minimum distances from each candidate point $z \\in C$ to the current design set $X_k$. When a new point $x_{k+1}$ is added, this array is updated by taking the element-wise minimum of the old distances and the distances to the new point $x_{k+1}$. The next point to add is then simply the candidate corresponding to the maximum value in this updated array of minimum distances.\n\n3.  **Metric Computation**: After each point $x_k$ is added (for $k=1, \\dots, K$), two quality metrics for the set $X_k$ are computed.\n    *   **Separation Distance ($q_{X_k}$)**: This metric measures the minimum spacing between points within the design set. It is defined as:\n        $$\n        q_{X_k} = \\frac{1}{2} \\min_{\\substack{x, y \\in X_k \\\\ x \\neq y}} d_W(x, y)\n        $$\n        By convention, for a singleton set $X_1$, $q_{X_1}$ is set to $0$. For $k > 1$, this requires computing all $\\binom{k}{2}$ pairwise distances within $X_k$ and finding the minimum.\n    *   **Fill Distance ($h_{X_k}$)**: This metric measures how well the design set covers the candidate set $C$. It is defined as the largest distance from any candidate point to its nearest neighbor in the design set:\n        $$\n        h_{X_k} = \\max_{z \\in C} \\min_{x \\in X_k} d_W(z, x)\n        $$\n        As noted in the selection step, the value $h_{X_k}$ is precisely the maximum of the minimum-distance array used to select the subsequent point $x_{k+2}$.\n\nThe process is repeated for each of the three test cases provided. For each case, we generate a list of pairs $[q_{X_k}, h_{X_k}]$ for $k=1, \\dots, K$. All distance values are rounded to six decimal places as required. The implementation relies on `numpy` for efficient, vectorized calculations of distances and for managing the candidate and design sets.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"d\": 3,\n            \"bounds\": np.array([[60e-6, 120e-6], [0.25, 0.45], [0.8, 2.0]]),\n            \"scales\": np.array([30e-6, 0.10, 0.60]),\n            \"resolutions\": [7, 5, 6],\n            \"K\": 6,\n        },\n        {\n            \"name\": \"Case B\",\n            \"d\": 2,\n            \"bounds\": np.array([[1e-14, 5e-14], [0.5, 2.5]]),\n            \"scales\": np.array([0.5e-14, 1.0]),\n            \"resolutions\": [8, 9],\n            \"K\": 5,\n        },\n        {\n            \"name\": \"Case C\",\n            \"d\": 1,\n            \"bounds\": np.array([[10e-6, 12e-6]]),\n            \"scales\": np.array([1e-6]),\n            \"resolutions\": [5],\n            \"K\": 5,\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_case(case[\"bounds\"], case[\"scales\"], case[\"resolutions\"], case[\"K\"])\n        all_results.append(result)\n\n    # The problem description's textual example `[ caseA, caseB, caseC ]` suggests\n    # standard Python list string representation, which includes spaces.\n    # Therefore, we use the standard string representation of the list.\n    print(str(all_results))\n\n\ndef run_case(bounds, scales, resolutions, K):\n    \"\"\"\n    Executes the greedy farthest-point sampling for a single test case.\n    \"\"\"\n    # 1. Setup: Weights and Candidate Set\n    weights = 1.0 / (scales**2)\n    grid_axes = [np.linspace(b[0], b[1], r) for b, r in zip(bounds, resolutions)]\n    mesh = np.meshgrid(*grid_axes, indexing='ij')\n    candidates = np.vstack([m.ravel() for m in mesh]).T\n    \n    # 2. Initialization: Find starting point x_1\n    center = np.mean(bounds, axis=1)\n    \n    # helper for weighted distance from one point p1 to many points p2\n    def weighted_dist(p1, p2_array):\n        diff = p2_array - p1\n        return np.sqrt(np.sum(weights * diff**2, axis=1))\n\n    dists_to_center = weighted_dist(center, candidates)\n    initial_idx = np.argmin(dists_to_center)\n    \n    X_indices = {initial_idx}\n    X_points = [candidates[initial_idx]]\n    \n    case_results = []\n\n    # 3. Iteratively select points and compute metrics\n    min_dists_to_X = weighted_dist(X_points[0], candidates)\n\n    for k in range(1, K + 1):\n        # Metrics for the current set X_k\n        \n        # Calculate q_Xk (Separation Distance)\n        if k == 1:\n            q_k = 0.0\n        else:\n            current_X_array = np.array(X_points)\n            pairwise_dists = []\n            for i in range(k):\n                for j in range(i + 1, k):\n                    diff = current_X_array[i] - current_X_array[j]\n                    dist = np.sqrt(np.sum(weights * diff**2))\n                    pairwise_dists.append(dist)\n            q_k = 0.5 * np.min(pairwise_dists)\n            \n        # Calculate h_Xk (Fill Distance)\n        # h_k is the max of the current minimum distances to X_k\n        h_k = np.max(min_dists_to_X)\n        \n        case_results.append([round(q_k, 6), round(h_k, 6)])\n        \n        # If we have collected K points, we stop.\n        if k == K:\n            break\n            \n        # Select next point (x_{k+1})\n        # The next point is the candidate farthest from the current set X_k.\n        # To strictly implement C \\ X_k, we mask already selected points.\n        temp_min_dists = np.copy(min_dists_to_X)\n        temp_min_dists[list(X_indices)] = -1.0 # Ensure selected points are not chosen again\n        next_idx = np.argmax(temp_min_dists)\n        \n        next_point = candidates[next_idx]\n        X_indices.add(next_idx)\n        X_points.append(next_point)\n        \n        # Update minimum distances for the new set X_{k+1}\n        dists_to_new_point = weighted_dist(next_point, candidates)\n        min_dists_to_X = np.minimum(min_dists_to_X, dists_to_new_point)\n        \n    return case_results\n\nsolve()\n```"
        },
        {
            "introduction": "In many engineering systems, some regions of the parameter space are more critical than others. This advanced exercise moves from uniform to adaptive sampling by implementing a weighted Lloyd's algorithm to generate a Centroidal Voronoi Tessellation (CVT). By using a weight function that reflects regions of interest—such as high-stress operating conditions for a battery—you will learn to create designs that intelligently concentrate sampling effort where it is most needed .",
            "id": "3951723",
            "problem": "Consider a two-dimensional parameter domain for automated battery design and simulation, where each point is a pair of parameters representing the charge rate and temperature. Let the charge rate be denoted by $c$ in units of C-rate (C), and let the temperature be denoted by $T$ in units of Kelvin (K). Define the domain as a closed rectangle $\\Omega = [c_{\\min}, c_{\\max}] \\times [T_{\\min}, T_{\\max}] \\subset \\mathbb{R}^2$. A space-filling sampling strategy based on centroidal Voronoi tessellations is to be adapted to emphasize regions of high charge rate and high temperature. Specifically, define a positive weighting function $w(c, T)$ that increases with both $c$ and $T$ according to\n$$\nw(c, T) = \\exp\\left(\\alpha \\cdot \\frac{c - c_{\\min}}{c_{\\max} - c_{\\min}} + \\beta \\cdot \\frac{T - T_{\\min}}{T_{\\max} - T_{\\min}}\\right),\n$$\nwhere $\\alpha > 0$ and $\\beta > 0$ are given constants.\n\nStarting from a finite set of generator points $\\{(c_i, T_i)\\}_{i=1}^k \\subset \\Omega$, define the Voronoi partition of $\\Omega$ under the standard Euclidean metric, with each Voronoi cell corresponding to the subset of $\\Omega$ whose points are closer to one generator than any other. The objective is to compute updated generator positions by moving each generator to the weighted centroid of its Voronoi cell under the weight function $w(c, T)$, using an algorithmic procedure derived from first principles. The final updated positions must be expressed in the same physical units as the domain, that is, in C for charge rate and in K for temperature.\n\nTo make the computation numerically tractable and universally implementable, approximate the required integrals using uniform rectangular quadrature on a Cartesian grid of resolution $n_x \\times n_y$ over $\\Omega$. Treat each grid node $(c_j, T_\\ell)$ as a sample point; assign each sample point to the nearest generator (by Euclidean distance) to approximate the Voronoi partition; and then approximate each cell’s weighted centroid using discrete weighted sums. Handle any empty Voronoi cell (i.e., no grid nodes assigned) by leaving its generator unchanged for that iteration.\n\nImplement an iterative update (Lloyd-type iteration) with a specified number of iterations $N_{\\text{iter}} \\in \\mathbb{N}$, applying the weighted centroid update at each iteration.\n\nYour program must compute the final updated generator positions after the specified number of iterations for each of the following test cases. For every test case, the output must be the list of generator positions in the format $[[c_1, T_1], [c_2, T_2], \\dots, [c_k, T_k]]$, where each $c_i$ is in C and each $T_i$ is in K. The program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets.\n\nTest Suite:\n- Test Case $1$ (general case):\n  - Domain: $c \\in [0, 6]$ C, $T \\in [293, 333]$ K, so $c_{\\min} = 0$, $c_{\\max} = 6$, $T_{\\min} = 293$, $T_{\\max} = 333$.\n  - Weight parameters: $\\alpha = 3$, $\\beta = 2$.\n  - Initial generators: $[(1.0, 300.0), (3.0, 320.0), (5.5, 330.0), (4.0, 295.0)]$.\n  - Grid resolution: $n_x = 64$, $n_y = 64$.\n  - Iterations: $N_{\\text{iter}} = 2$.\n\n- Test Case $2$ (boundary-stress case):\n  - Domain: $c \\in [0, 10]$ C, $T \\in [273, 343]$ K, so $c_{\\min} = 0$, $c_{\\max} = 10$, $T_{\\min} = 273$, $T_{\\max} = 343$.\n  - Weight parameters: $\\alpha = 1$, $\\beta = 1$.\n  - Initial generators: $[(0.2, 273.0), (9.8, 343.0), (0.2, 343.0)]$.\n  - Grid resolution: $n_x = 60$, $n_y = 60$.\n  - Iterations: $N_{\\text{iter}} = 1$.\n\n- Test Case $3$ (extreme-concentration case):\n  - Domain: $c \\in [0, 8]$ C, $T \\in [298, 358]$ K, so $c_{\\min} = 0$, $c_{\\max} = 8$, $T_{\\min} = 298$, $T_{\\max} = 358$.\n  - Weight parameters: $\\alpha = 8$, $\\beta = 8$.\n  - Initial generators: $[(1.0, 300.0), (2.0, 330.0), (7.5, 356.0)]$.\n  - Grid resolution: $n_x = 80$, $n_y = 80$.\n  - Iterations: $N_{\\text{iter}} = 1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact format\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3],\n$$\nwhere each $\\text{result}_i$ is the list of updated generator coordinates for the $i$-th test case, represented as $[[c_1, T_1], [c_2, T_2], \\dots]$, with $c$ in C and $T$ in K, without unit symbols. No additional text should be printed.",
            "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Domain:** A 2D rectangular domain $\\Omega = [c_{\\min}, c_{\\max}] \\times [T_{\\min}, T_{\\max}] \\subset \\mathbb{R}^2$.\n- **Parameters:** Charge rate $c$ (C-rate, C) and temperature $T$ (Kelvin, K).\n- **Weighting Function:** $w(c, T) = \\exp\\left(\\alpha \\cdot \\frac{c - c_{\\min}}{c_{\\max} - c_{\\min}} + \\beta \\cdot \\frac{T - T_{\\min}}{T_{\\max} - T_{\\min}}\\right)$, with $\\alpha > 0$ and $\\beta > 0$.\n- **Generators:** A finite set of points $\\{(c_i, T_i)\\}_{i=1}^k \\subset \\Omega$.\n- **Partition:** Voronoi partition based on the standard Euclidean metric.\n- **Objective:** Update each generator to the weighted centroid of its Voronoi cell.\n- **Numerical Approximation:**\n    - A Cartesian grid of resolution $n_x \\times n_y$ is used.\n    - Grid nodes are assigned to the nearest generator to approximate Voronoi cells.\n    - Integrals for centroids are approximated by discrete weighted sums.\n- **Empty Cell Handling:** If a generator's approximated Voronoi cell contains no grid nodes, its position remains unchanged for that iteration.\n- **Algorithm:** An iterative update (Lloyd-type) is performed for $N_{\\text{iter}}$ iterations.\n- **Test Cases:**\n    1.  **Case 1:** $c \\in [0, 6]$, $T \\in [293, 333]$. $\\alpha = 3$, $\\beta = 2$. Initial generators: $[(1.0, 300.0), (3.0, 320.0), (5.5, 330.0), (4.0, 295.0)]$. Grid: $n_x = 64$, $n_y = 64$. Iterations: $N_{\\text{iter}} = 2$.\n    2.  **Case 2:** $c \\in [0, 10]$, $T \\in [273, 343]$. $\\alpha = 1$, $\\beta = 1$. Initial generators: $[(0.2, 273.0), (9.8, 343.0), (0.2, 343.0)]$. Grid: $n_x = 60$, $n_y = 60$. Iterations: $N_{\\text{iter}} = 1$.\n    3.  **Case 3:** $c \\in [0, 8]$, $T \\in [298, 358]$. $\\alpha = 8$, $\\beta = 8$. Initial generators: $[(1.0, 300.0), (2.0, 330.0), (7.5, 356.0)]$. Grid: $n_x = 80$, $n_y = 80$. Iterations: $N_{\\text{iter}} = 1$.\n- **Output:** The final updated generator positions for each test case, formatted as a comma-separated list of lists of coordinates.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem describes a weighted version of Lloyd's algorithm for computing Centroidal Voronoi Tessellations (CVT). This is a standard and well-established method in computational geometry and scientific computing. Its application to optimizing sampling in a parameter space for battery design is a realistic and relevant use case in engineering and materials science.\n- **Well-Posed:** The problem is well-posed. The objective is clearly stated, the update rule is mathematically unambiguous, and all required parameters and initial conditions are provided for each test case. The iterative procedure is defined for a fixed number of steps, ensuring a unique terminal state. The rule for handling empty Voronoi cells removes any ambiguity.\n- **Objective:** The problem is formulated with precise mathematical and algorithmic language. All terms are defined, and the requirements are objective and quantifiable.\n- **Completeness and Consistency:** The problem is self-contained. All necessary numerical values, functions, and constraints are provided. There are no internal contradictions.\n- **Realism and Feasibility:** The physical parameters (C-rate, temperature) are within realistic ranges for battery research. The computational task, while requiring careful implementation, is feasible.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined computational problem grounded in established mathematical principles. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe objective is to implement a discrete, weighted version of Lloyd's algorithm to find the positions of $k$ generator points, $\\{p_i = (c_i, T_i)\\}_{i=1}^k$, that are the weighted centroids of their own Voronoi cells within a domain $\\Omega$. The position of the weighted centroid $p_i'$ of a region $V_i \\subseteq \\Omega$ with respect to a weight function $w(p)$ is given by the integral formula:\n$$\np_i' = \\frac{\\int_{V_i} p \\cdot w(p) \\, dA}{\\int_{V_i} w(p) \\, dA}\n$$\nwhere $p = (c, T)$ represents a point in the domain.\n\nThe problem specifies a numerical approximation of this continuous problem. The domain $\\Omega = [c_{\\min}, c_{\\max}] \\times [T_{\\min}, T_{\\max}]$ is discretized into a uniform Cartesian grid of $n_x \\times n_y$ nodes. Let this set of grid nodes be denoted by $\\mathcal{G} = \\{q_j\\}_{j=1}^{n_x n_y}$, where each $q_j = (c_j, T_j)$ is a point in $\\Omega$. The integrals are replaced by sums over these grid nodes.\n\nThe algorithm proceeds iteratively for a specified number of iterations, $N_{\\text{iter}}$. Let $P^{(m)} = \\{p_i^{(m)}\\}_{i=1}^k$ be the set of generator points at iteration $m$.\n\n**1. Initialization:**\nStart with the initial set of generators $P^{(0)}$. Discretize the domain $\\Omega$ into a grid $\\mathcal{G}$. The coordinates of the grid nodes are generated using uniformly spaced points:\n$$ c_j = c_{\\min} + j \\frac{c_{\\max} - c_{\\min}}{n_x - 1}, \\quad j = 0, \\dots, n_x-1 $$\n$$ T_\\ell = T_{\\min} + \\ell \\frac{T_{\\max} - T_{\\min}}{n_y - 1}, \\quad \\ell = 0, \\dots, n_y-1 $$\nThe weight function $w(q)$ is evaluated and stored for each grid node $q \\in \\mathcal{G}$.\n\n**2. Iterative Update:**\nFor each iteration $m = 0, 1, \\dots, N_{\\text{iter}}-1$:\n\n**a. Partition Step (Approximate Voronoi Tessellation):**\nThe continuous Voronoi partition is approximated by assigning each grid node $q_j \\in \\mathcal{G}$ to the nearest generator in the current set $P^{(m)}$. The distance metric is the standard Euclidean distance. This partitions the grid $\\mathcal{G}$ into $k$ disjoint subsets $\\mathcal{G}_1^{(m)}, \\mathcal{G}_2^{(m)}, \\dots, \\mathcal{G}_k^{(m)}$, where:\n$$\n\\mathcal{G}_i^{(m)} = \\{q \\in \\mathcal{G} \\mid \\|q - p_i^{(m)}\\|_2 \\leq \\|q - p_j^{(m)}\\|_2 \\text{ for all } j \\neq i\\}\n$$\nTies can be broken arbitrarily, for instance, by assigning the point to the generator with the smaller index.\n\n**b. Centroid Update Step:**\nFor each $i \\in \\{1, \\dots, k\\}$, the next generator position $p_i^{(m+1)}$ is computed as the weighted centroid of the grid nodes in its assigned cell $\\mathcal{G}_i^{(m)}$. The continuous integral is replaced by a discrete sum:\n$$\np_i^{(m+1)} = \\frac{\\sum_{q \\in \\mathcal{G}_i^{(m)}} q \\cdot w(q)}{\\sum_{q \\in \\mathcal{G}_i^{(m)}} w(q)}\n$$\nThis vector equation can be expressed for each coordinate:\n$$\nc_i^{(m+1)} = \\frac{\\sum_{(c_j, T_j) \\in \\mathcal{G}_i^{(m)}} c_j \\cdot w(c_j, T_j)}{\\sum_{(c_j, T_j) \\in \\mathcal{G}_i^{(m)}} w(c_j, T_j)}\n$$\n$$\nT_i^{(m+1)} = \\frac{\\sum_{(c_j, T_j) \\in \\mathcal{G}_i^{(m)}} T_j \\cdot w(c_j, T_j)}{\\sum_{(c_j, T_j) \\in \\mathcal{G}_i^{(m)}} w(c_j, T_j)}\n$$\nAs per the problem specification, if a cell $\\mathcal{G}_i^{(m)}$ is empty (i.e., contains no grid nodes), the generator position is not updated: $p_i^{(m+1)} = p_i^{(m)}$. This condition is naturally handled if the sums are over an empty set, but must be checked explicitly to avoid division by zero. Since the weight function $w(c, T)$ is strictly positive, the denominator is zero if and only if the set $\\mathcal{G}_i^{(m)}$ is empty.\n\n**3. Termination:**\nAfter $N_{\\text{iter}}$ iterations, the final positions of the generators are given by the set $P^{(N_{\\text{iter}})}$. These are the results to be reported. This procedure will be implemented for each test case provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef compute_weighted_cvt(domain, weight_params, initial_generators, grid_resolution, n_iter):\n    \"\"\"\n    Computes weighted centroidal Voronoi tessellations using a discrete Lloyd-type algorithm.\n\n    Args:\n        domain (dict): Dictionary with keys 'c_min', 'c_max', 't_min', 't_max'.\n        weight_params (dict): Dictionary with keys 'alpha', 'beta'.\n        initial_generators (list of tuples): Initial positions of the generators.\n        grid_resolution (dict): Dictionary with keys 'nx', 'ny'.\n        n_iter (int): Number of iterations to perform.\n\n    Returns:\n        list of lists: Final positions of the generators.\n    \"\"\"\n    c_min, c_max = domain['c_min'], domain['c_max']\n    t_min, t_max = domain['t_min'], domain['t_max']\n    alpha, beta = weight_params['alpha'], weight_params['beta']\n    nx, ny = grid_resolution['nx'], grid_resolution['ny']\n\n    # Step 1: Create the grid and pre-compute weights\n    c_coords = np.linspace(c_min, c_max, nx)\n    t_coords = np.linspace(t_min, t_max, ny)\n    c_grid, t_grid = np.meshgrid(c_coords, t_coords)\n    grid_points = np.vstack([c_grid.ravel(), t_grid.ravel()]).T\n\n    # Normalize coordinates for weight calculation, handling potential division by zero if domain is a point.\n    c_range = c_max - c_min\n    t_range = t_max - t_min\n    c_norm = (grid_points[:, 0] - c_min) / c_range if c_range > 0 else np.zeros(grid_points.shape[0])\n    t_norm = (grid_points[:, 1] - t_min) / t_range if t_range > 0 else np.zeros(grid_points.shape[0])\n    \n    weights = np.exp(alpha * c_norm + beta * t_norm)\n\n    # Initialize generators\n    generators = np.array(initial_generators, dtype=float)\n    num_generators = generators.shape[0]\n\n    # Step 2: Iterative updates\n    for _ in range(n_iter):\n        # Step 2a: Partition the grid by assigning each grid point to the nearest generator\n        # cdist computes the Euclidean distance between each grid point and each generator.\n        # 'sqeuclidean' is used for efficiency as it avoids the sqrt operation.\n        dists_sq = cdist(grid_points, generators, 'sqeuclidean')\n        assignments = np.argmin(dists_sq, axis=1)\n\n        # Step 2b: Update generator positions to the weighted centroid of their cells\n        new_generators = generators.copy()\n        for i in range(num_generators):\n            # Find all grid points assigned to the current generator\n            mask = (assignments == i)\n            \n            # If the cell is empty, do not update the generator position\n            if not np.any(mask):\n                continue\n            \n            points_in_cell = grid_points[mask]\n            weights_in_cell = weights[mask]\n            \n            # Calculate the weighted centroid\n            denominator = np.sum(weights_in_cell)\n            \n            # The numerator is the sum of (point_coordinate * weight)\n            # Use broadcasting to multiply each coordinate of each point by its corresponding weight\n            weighted_coords = points_in_cell * weights_in_cell[:, np.newaxis]\n            \n            # Sum the weighted coordinates and divide by the sum of weights\n            centroid = np.sum(weighted_coords, axis=0) / denominator\n            \n            new_generators[i] = centroid\n        \n        generators = new_generators\n\n    return generators.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"domain\": {\"c_min\": 0.0, \"c_max\": 6.0, \"t_min\": 293.0, \"t_max\": 333.0},\n            \"weight_params\": {\"alpha\": 3.0, \"beta\": 2.0},\n            \"initial_generators\": [(1.0, 300.0), (3.0, 320.0), (5.5, 330.0), (4.0, 295.0)],\n            \"grid_resolution\": {\"nx\": 64, \"ny\": 64},\n            \"n_iter\": 2,\n        },\n        {\n            \"domain\": {\"c_min\": 0.0, \"c_max\": 10.0, \"t_min\": 273.0, \"t_max\": 343.0},\n            \"weight_params\": {\"alpha\": 1.0, \"beta\": 1.0},\n            \"initial_generators\": [(0.2, 273.0), (9.8, 343.0), (0.2, 343.0)],\n            \"grid_resolution\": {\"nx\": 60, \"ny\": 60},\n            \"n_iter\": 1,\n        },\n        {\n            \"domain\": {\"c_min\": 0.0, \"c_max\": 8.0, \"t_min\": 298.0, \"t_max\": 358.0},\n            \"weight_params\": {\"alpha\": 8.0, \"beta\": 8.0},\n            \"initial_generators\": [(1.0, 300.0), (2.0, 330.0), (7.5, 356.0)],\n            \"grid_resolution\": {\"nx\": 80, \"ny\": 80},\n            \"n_iter\": 1,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        final_generators = compute_weighted_cvt(\n            case[\"domain\"],\n            case[\"weight_params\"],\n            case[\"initial_generators\"],\n            case[\"grid_resolution\"],\n            case[\"n_iter\"]\n        )\n        # Format list to string without spaces for compact output\n        result_str = str(final_generators).replace(\" \", \"\")\n        results.append(result_str)\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}