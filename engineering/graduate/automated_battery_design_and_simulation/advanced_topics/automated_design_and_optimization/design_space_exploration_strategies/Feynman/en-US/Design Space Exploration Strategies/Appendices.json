{
    "hands_on_practices": [
        {
            "introduction": "Real-world engineering challenges, such as battery design, are rarely about optimizing a single metric. We often need to balance conflicting objectives like maximizing energy density while minimizing cost and degradation. This practice exercise delves into a fundamental strategy for tackling such multi-objective optimization (MOO) problems: scalarization. You will derive and apply the weighted Tchebycheff method (), a powerful technique that can identify any point on the optimal trade-off frontier, even when the relationship between objectives is non-convex—a common scenario that simpler methods like linear weighting can fail to handle.",
            "id": "3905756",
            "problem": "In automated battery design and simulation, multiple objective optimization (MOO) is used to explore trade-offs among competing performance metrics. Consider a simplified cell design vector $x = (\\ell_{s}, \\epsilon_{p})$, where $\\ell_{s}$ is the separator thickness and $\\epsilon_{p}$ is the electrode porosity. Suppose the following two dimensionless, normalized objectives are to be minimized over the feasible set $X = \\{ (\\ell_{s}, \\epsilon_{p}) \\,|\\, \\ell_{s} \\in [0.2, 1.0], \\epsilon_{p} \\in [0.3, 0.7] \\}$:\n$$\nf_{1}(x) = 0.5\\,\\ell_{s} + \\frac{0.1}{\\epsilon_{p}},\n\\qquad\nf_{2}(x) = \\frac{0.3}{\\ell_{s}} + 0.4\\,\\epsilon_{p}^{2}.\n$$\nThese proxies capture, respectively, normalized ohmic-heating propensity under a fixed high-rate pulse and normalized degradation tendency under reference cycling; lower values indicate better designs. Let the utopia point be defined component-wise by $z_{i}^{*} = \\inf_{x \\in X} f_{i}(x)$ for $i \\in \\{1,2\\}$. Let the designer’s priority vector be $w = (w_{1}, w_{2})$ with $w_{1} > 0$, $w_{2} > 0$, and $w_{1} + w_{2} = 1$.\n\nStarting from the definitions of Pareto dominance and the utopia point, derive an aggregate scalar objective that encodes the worst-case weighted deviation of $f(x)$ from the utopia point across objectives, and express it in closed form. Using your derived scalarization, evaluate it for the weights $w_{1} = 0.6$, $w_{2} = 0.4$ at the candidate design $x = (\\ell_{s}, \\epsilon_{p}) = (0.6, 0.5)$. Round your final numerical result to four significant figures and express it as a dimensionless number.\n\nFinally, provide a concise, principle-based explanation of why this worst-case weighted-deviation scalarization can capture Pareto-optimal points on non-convex trade-off fronts that linear weighted-sum aggregation cannot, in the context of battery design trade-offs. Your explanation must be grounded in the geometry of objective-space level sets and the definition of Pareto optimality, without relying on empirical claims.",
            "solution": "The user wants me to solve a multi-objective optimization problem.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- **Design Vector**: $x = (\\ell_{s}, \\epsilon_{p})$, where $\\ell_{s}$ is separator thickness and $\\epsilon_{p}$ is electrode porosity.\n- **Feasible Set**: $X = \\{ (\\ell_{s}, \\epsilon_{p}) \\,|\\, \\ell_{s} \\in [0.2, 1.0], \\epsilon_{p} \\in [0.3, 0.7] \\}$.\n- **Objective Functions to be Minimized**:\n  - $f_{1}(x) = 0.5\\,\\ell_{s} + \\frac{0.1}{\\epsilon_{p}}$\n  - $f_{2}(x) = \\frac{0.3}{\\ell_{s}} + 0.4\\,\\epsilon_{p}^{2}$\n- **Utopia Point Definition**: $z_{i}^{*} = \\inf_{x \\in X} f_{i}(x)$ for $i \\in \\{1,2\\}$.\n- **Priority Vector**: $w = (w_{1}, w_{2})$ with $w_{1} > 0$, $w_{2} > 0$, and $w_{1} + w_{2} = 1$.\n- **Specific Evaluation Case**:\n  - Weights: $w_{1} = 0.6$, $w_{2} = 0.4$.\n  - Candidate design: $x = (0.6, 0.5)$.\n- **Tasks**:\n  1. Derive an aggregate scalar objective for the worst-case weighted deviation from the utopia point.\n  2. Evaluate this scalar objective for the given weights and design point, rounding to four significant figures.\n  3. Provide a principle-based explanation for why this scalarization can capture non-convex Pareto fronts.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a mathematical optimization problem set in the context of battery design. The objective functions are simplified proxies, but their forms represent plausible trade-offs (e.g., ohmic heating versus degradation) and do not violate any scientific principles. The methods used (multi-objective optimization, Chebyshev scalarization) are standard in engineering and operations research.\n- **Well-Posed**: The feasible set $X$ is a compact (closed and bounded) subset of $\\mathbb{R}^{2}$. The objective functions $f_{1}$ and $f_{2}$ are continuous on $X$. By the Extreme Value Theorem, both functions attain their minima on $X$, ensuring the utopia point $z^{*}$ is well-defined. The tasks are specific and lead to a unique numerical and theoretical answer.\n- **Objective**: The problem is stated in precise mathematical language, free of subjective or ambiguous terminology.\n- **Completeness**: All necessary functions, domains, constants, and values are provided to complete the tasks.\n- **Consistency and Feasibility**: The constraints and definitions are internally consistent. The numerical ranges for the design variables are physically plausible for normalized parameters.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. I will proceed with the solution.\n\n***\n\nThe solution proceeds in three parts as requested by the problem statement.\n\nFirst, we derive the aggregate scalar objective function. The problem asks for a scalarization that encodes the \"worst-case weighted deviation of $f(x)$ from the utopia point across objectives\". The deviation for objective $i$ is $f_{i}(x) - z_{i}^{*}$, where $z_{i}^{*}$ is the ideal minimum value for that objective. The weighted deviation is $w_{i}(f_{i}(x) - z_{i}^{*})$. The \"worst-case\" among these weighted deviations is their maximum. Therefore, the aggregate scalar objective function, which we denote as $S(x)$, is given by the weighted Chebyshev (or Tchebycheff) metric:\n$$\nS(x) = \\max_{i \\in \\{1,2\\}} \\{ w_{i} (f_{i}(x) - z_{i}^{*}) \\}\n$$\nThis is the required closed-form expression for the scalar objective.\n\nSecond, we evaluate this objective for the given parameters. This requires calculating the utopia point $z^{*} = (z_{1}^{*}, z_{2}^{*})$.\nThe utopia point components are the infima of the objectives over the feasible set $X$.\nFor $z_{1}^{*}$, we minimize $f_{1}(x)$:\n$$\nz_{1}^{*} = \\inf_{(\\ell_{s}, \\epsilon_{p}) \\in X} \\left( 0.5\\,\\ell_{s} + \\frac{0.1}{\\epsilon_{p}} \\right)\n$$\nThe function $f_{1}$ is a sum of two terms, each depending on a single independent variable. To minimize the sum, we minimize each term independently over its domain. The term $0.5\\,\\ell_{s}$ is minimized at the smallest possible value of $\\ell_{s}$, which is $\\ell_{s} = 0.2$. The term $\\frac{0.1}{\\epsilon_{p}}$ is minimized at the largest possible value of $\\epsilon_{p}$, which is $\\epsilon_{p} = 0.7$. Thus:\n$$\nz_{1}^{*} = 0.5(0.2) + \\frac{0.1}{0.7} = 0.1 + \\frac{1}{7} = \\frac{1}{10} + \\frac{1}{7} = \\frac{7+10}{70} = \\frac{17}{70}\n$$\nFor $z_{2}^{*}$, we minimize $f_{2}(x)$:\n$$\nz_{2}^{*} = \\inf_{(\\ell_{s}, \\epsilon_{p}) \\in X} \\left( \\frac{0.3}{\\ell_{s}} + 0.4\\,\\epsilon_{p}^{2} \\right)\n$$\nSimilarly, we minimize the terms independently. The term $\\frac{0.3}{\\ell_{s}}$ is minimized at the largest possible value of $\\ell_{s}$, which is $\\ell_{s} = 1.0$. The term $0.4\\,\\epsilon_{p}^{2}$ is minimized at the smallest possible value of $\\epsilon_{p}$, which is $\\epsilon_{p} = 0.3$. Thus:\n$$\nz_{2}^{*} = \\frac{0.3}{1.0} + 0.4(0.3)^{2} = 0.3 + 0.4(0.09) = 0.3 + 0.036 = 0.336\n$$\nThe utopia point is $z^{*} = (\\frac{17}{70}, 0.336)$.\n\nNow, we evaluate the objective functions at the candidate design point $x = (0.6, 0.5)$:\n$$\nf_{1}(0.6, 0.5) = 0.5(0.6) + \\frac{0.1}{0.5} = 0.3 + 0.2 = 0.5\n$$\n$$\nf_{2}(0.6, 0.5) = \\frac{0.3}{0.6} + 0.4(0.5)^{2} = 0.5 + 0.4(0.25) = 0.5 + 0.1 = 0.6\n$$\nWe use the given weights $w_{1} = 0.6$ and $w_{2} = 0.4$ to calculate the weighted deviations:\n$$\nw_{1} (f_{1}(x) - z_{1}^{*}) = 0.6 \\left( 0.5 - \\frac{17}{70} \\right) = 0.6 \\left( \\frac{35}{70} - \\frac{17}{70} \\right) = 0.6 \\left( \\frac{18}{70} \\right) = \\frac{10.8}{70} = \\frac{108}{700} = \\frac{27}{175}\n$$\n$$\nw_{2} (f_{2}(x) - z_{2}^{*}) = 0.4 (0.6 - 0.336) = 0.4 (0.264) = 0.1056\n$$\nNow we find the maximum of these two values to get $S(x)$:\n$$\nS(0.6, 0.5) = \\max \\left( \\frac{27}{175}, 0.1056 \\right)\n$$\nTo compare them, we convert the fraction to a decimal: $\\frac{27}{175} \\approx 0.1542857$.\nSince $0.1542857... > 0.1056$, the value of the scalar objective is $\\frac{27}{175}$.\nRounding to four significant figures, we get $0.1543$.\n\nThird, we provide the principle-based explanation.\nIn multi-objective optimization, the set of all non-dominated feasible points in the objective space forms the Pareto-optimal front. A common goal of design space exploration is to identify points on this front. The geometry of the scalarization method's level sets determines its ability to find all such points.\n\nA point $x^{*}$ is Pareto-optimal if there is no other feasible point $x$ such that $f_{i}(x) \\le f_{i}(x^{*})$ for all $i$ and $f_{j}(x) < f_{j}(x^{*})$ for at least one index $j$.\n\nThe linear weighted-sum aggregation method uses the scalar objective $S_{sum}(x) = \\sum_{i} w_{i}f_{i}(x)$. In the two-dimensional objective space $(f_{1}, f_{2})$, the level sets of this function ($S_{sum}(x) = \\text{constant}$) are straight lines with a slope of $-w_{1}/w_{2}$. Minimizing $S_{sum}$ is geometrically equivalent to moving this line until it is tangent to the feasible objective region. If the Pareto front is non-convex (i.e., it has regions that are concave as viewed from the origin), there exist points on the front that can never be found by such a tangency condition. These are known as unsupported Pareto-optimal points.\n\nThe worst-case weighted-deviation scalarization (weighted Chebyshev method) uses $S_{Cheby}(x) = \\max_{i} \\{w_{i}(f_{i}(x) - z_{i}^{*})\\}$. A level set $S_{Cheby}(x) = \\alpha$ (where $\\alpha > 0$ is a constant) corresponds to the boundary of the region defined by $w_{i}(f_{i}(x) - z_{i}^{*}) \\le \\alpha$ for all $i$, or $f_{i}(x) \\le z_{i}^{*} + \\alpha/w_{i}$. In the $(f_{1}, f_{2})$ objective space, this region is a rectangle anchored at the utopia point $z^{*}=(z_1^*, z_2^*)$ and extending to $(z_1^* + \\alpha/w_1, z_2^* + \\alpha/w_2)$. Minimizing $S_{Cheby}(x)$ is equivalent to finding the smallest such rectangle (i.e., smallest $\\alpha$) that intersects the feasible objective region. The first point of contact between this expanding rectangular level set and the feasible region is the solution. Due to the sharp, right-angled corner of the rectangle, it can \"probe\" into and make contact with points within non-convex \"dents\" of the Pareto front. By varying the weight vector $w$, the aspect ratio of the rectangle changes, allowing its corner to touch any point on the Pareto front, whether it is convex or non-convex. Therefore, the weighted Chebyshev method is guaranteed to be able to find any Pareto-optimal point, making it a more robust method for exploring design trade-offs, especially when the shape of the Pareto front is unknown, as is typical in automated battery design.",
            "answer": "$$\n\\boxed{0.1543}\n$$"
        },
        {
            "introduction": "Exploring vast design spaces with expensive simulations requires an intelligent search strategy. Instead of sampling randomly, we can build a surrogate model to guide our search toward promising regions. This exercise focuses on a cornerstone of modern Design Space Exploration: using the uncertainty estimates from a Gaussian Process (GP) surrogate to adaptively refine sampling. You will derive the leave-one-out (LOO) error from first principles (), gaining a deep understanding of how to diagnose a GP's predictive accuracy and use that information to decide where new simulations will be most informative.",
            "id": "3905833",
            "problem": "In the automated design and simulation of advanced lithium-ion cells, a common design variable is electrode thickness, denoted by $t$, and a critical quantity of interest is a plating risk metric at the end of a specified fast-charge protocol, denoted by $r(t)$. Suppose that a Gaussian Process (GP) surrogate is used to model $r(t)$ over a one-dimensional design space of thickness $t \\in \\mathbb{R}$ based on $n$ simulated data pairs $\\mathcal{D} = \\{(t_i, y_i)\\}_{i=1}^n$, where $y_i$ is a noisy realization of $r(t_i)$ with additive independent Gaussian noise of variance $\\sigma_n^2$. Assume a zero-mean GP prior with covariance function $k_{\\boldsymbol{\\theta}}(t,t')$ that is strictly positive definite, and let $K \\in \\mathbb{R}^{n \\times n}$ denote the covariance matrix with entries $K_{ij} = k_{\\boldsymbol{\\theta}}(t_i, t_j)$. Define $A = K + \\sigma_n^2 I$, where $I$ is the identity matrix, and let $\\mathbf{y} = [y_1,\\dots,y_n]^{\\top}$. Assume that the GP hyperparameters $\\boldsymbol{\\theta}$ and the observation noise variance $\\sigma_n^2$ are fixed and known.\n\nStarting only from the following fundamental bases:\n- the property that any finite collection of GP values is jointly Gaussian;\n- the standard multivariate normal conditioning identities; and\n- elementary block-matrix inversion via the Schur complement,\n\ndo the following.\n\n1) Derive the leave-one-out predictive mean $\\mu_{-i}(t_i)$ and predictive variance $\\sigma_{-i}^2(t_i)$ for the $i$-th training input when the model is trained on the dataset with the $i$-th pair removed, in terms of $A^{-1}$ and $\\mathbf{y}$. Make no assumptions on the specific form of $k_{\\boldsymbol{\\theta}}(\\cdot,\\cdot)$ beyond positive definiteness.\n\n2) From your derivation, obtain a compact expression for the leave-one-out residual at $t_i$, defined as $e^{\\mathrm{loo}}_i = y_i - \\mu_{-i}(t_i)$, expressed entirely using the vector $\\boldsymbol{\\alpha} = A^{-1} \\mathbf{y}$ and the diagonal entries of $A^{-1}$.\n\n3) Define the leave-one-out root-mean-square error over the $n$ training inputs as $\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(e^{\\mathrm{loo}}_i\\right)^2}$. Provide a single closed-form analytic expression for $\\mathrm{RMSE}_{\\mathrm{loo}}$ purely in terms of $A^{-1}$ and $\\mathbf{y}$, eliminating $\\mu_{-i}(t_i)$ and $\\sigma_{-i}^2(t_i)$.\n\n4) Briefly justify, using first principles of Bayesian uncertainty quantification and design space exploration, how the magnitude of $e^{\\mathrm{loo}}_i$ and $\\sigma_{-i}(t_i)$ can be used to adaptively refine sampling near thickness regions where $r(t)$ varies rapidly with $t$. Your justification should connect the gradient magnitude $|\\partial \\mu(t)/\\partial t|$ and local predictive uncertainty to the decision of where to add new simulation points, without invoking any acquisition function beyond what can be derived from the stated bases.\n\nYour final answer must be the single closed-form analytic expression requested in item $3)$. No numerical evaluation is required, and no units are needed. If you choose to simplify notation, define it explicitly. The final answer must be a single analytic expression, not an inequality or an equation with free verbal qualifiers.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It presents a standard but rigorous exercise in the statistical theory of Gaussian Processes (GPs), with a direct and relevant application to automated engineering design. All necessary definitions and constraints are provided, and the problem is solvable using the specified fundamental bases. We may therefore proceed with a full derivation.\n\nThe problem is addressed in four parts as requested. Our analysis will rely on the properties of multivariate normal distributions and block matrix inversion. Let $A = K + \\sigma_n^2 I$, where $K$ is the $n \\times n$ covariance matrix with entries $K_{ij} = k_{\\boldsymbol{\\theta}}(t_i, t_j)$. Since the kernel $k_{\\boldsymbol{\\theta}}(\\cdot, \\cdot)$ is strictly positive definite, $K$ is a positive definite matrix. Given that the noise variance $\\sigma_n^2$ must be non-negative (and is implicitly positive for a realistic noisy system), the matrix $A$ is the sum of a positive definite matrix and a positive semi-definite matrix ($ \\sigma_n^2 I $), which makes $A$ itself strictly positive definite and therefore invertible.\n\n**1) Derivation of Leave-One-Out (LOO) Predictive Mean and Variance**\n\nThe goal is to find the predictive distribution for the latent function value $r(t_i)$ using the training data with the $i$-th point removed, denoted $\\mathcal{D}_{-i} = \\mathcal{D} \\setminus \\{(t_i, y_i)\\}$. The vector of training inputs with $t_i$ removed is $\\mathbf{t}_{-i}$, and the corresponding outputs are $\\mathbf{y}_{-i}$.\n\nAccording to the definition of a Gaussian Process, any finite collection of function values is jointly Gaussian. Considering the noisy observations $\\mathbf{y}$, their joint distribution is given by $p(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{0}, A)$. We can partition this distribution with respect to the $i$-th element, $y_i$, and the remaining elements, $\\mathbf{y}_{-i}$. Let's permute the elements for clarity, though it's not strictly necessary. The joint distribution of $(y_i, \\mathbf{y}_{-i}^\\top)^\\top$ is:\n$$\n\\begin{pmatrix} y_i \\\\ \\mathbf{y}_{-i} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} a_{ii} & \\mathbf{a}_{i,-i} \\\\ \\mathbf{a}_{-i,i} & A_{-i,-i} \\end{pmatrix} \\right)\n$$\nwhere $a_{ii} = K_{ii} + \\sigma_n^2$, $\\mathbf{a}_{i,-i} = [K_{i1}, \\dots, K_{i,i-1}, K_{i,i+1}, \\dots, K_{in}]$, and $A_{-i,-i}$ is the matrix $A$ with the $i$-th row and column removed. Note that $\\mathbf{a}_{-i,i} = \\mathbf{a}_{i,-i}^\\top$. The matrix $A_{-i,-i}$ is equivalent to $K_{-i,-i} + \\sigma_n^2 I_{n-1}$, where $K_{-i,-i}$ is the covariance matrix for inputs $\\mathbf{t}_{-i}$.\n\nUsing the standard conditioning rules for a multivariate normal distribution, the conditional distribution $p(y_i|\\mathbf{y}_{-i})$ is a Gaussian with mean and variance:\n$$\n\\mathbb{E}[y_i|\\mathbf{y}_{-i}] = \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{y}_{-i}\n$$\n$$\n\\mathrm{Var}[y_i|\\mathbf{y}_{-i}] = a_{ii} - \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{a}_{-i,i}\n$$\nThe leave-one-out predictive mean $\\mu_{-i}(t_i)$ for the latent function value $r(t_i)$ is calculated by conditioning on the noisy data $\\mathbf{y}_{-i}$. The joint distribution of $(r(t_i), \\mathbf{y}_{-i}^\\top)^\\top$ is:\n$$\n\\begin{pmatrix} r(t_i) \\\\ \\mathbf{y}_{-i} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} K_{ii} & \\mathbf{k}_{i,-i} \\\\ \\mathbf{k}_{-i,i} & K_{-i,-i} + \\sigma_n^2 I_{n-1} \\end{pmatrix} \\right)\n$$\nwhere $\\mathbf{k}_{i,-i}$ is the row vector of covariances $[k(t_i, t_j)]_{j \\neq i}$, which is identical to $\\mathbf{a}_{i,-i}$. Conditioning $r(t_i)$ on $\\mathbf{y}_{-i}$ gives the LOO predictive mean:\n$$\n\\mu_{-i}(t_i) = \\mathbb{E}[r(t_i)|\\mathbf{y}_{-i}] = \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{y}_{-i} = \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{y}_{-i}\n$$\nSo we find that $\\mu_{-i}(t_i) = \\mathbb{E}[y_i|\\mathbf{y}_{-i}]$. The LOO predictive variance of the latent function is:\n$$\n\\sigma_{-i}^2(t_i) = \\mathrm{Var}[r(t_i)|\\mathbf{y}_{-i}] = K_{ii} - \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{k}_{-i,i}\n$$\nThe task is to express these quantities using the full inverse matrix $A^{-1}$. We use the block matrix inversion formula, which relies on the Schur complement. The Schur complement of the block $A_{-i,-i}$ in matrix $A$ is $S_i = a_{ii} - \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{a}_{-i,i}$. The $(i,i)$-th element of the inverse matrix $A^{-1}$ is the inverse of this Schur complement:\n$$\n(A^{-1})_{ii} = S_i^{-1} = (a_{ii} - \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{a}_{-i,i})^{-1}\n$$\nThis is precisely the inverse of the conditional variance $\\mathrm{Var}[y_i|\\mathbf{y}_{-i}]$.\n$$\n\\mathrm{Var}[y_i|\\mathbf{y}_{-i}] = \\frac{1}{(A^{-1})_{ii}}\n$$\nSubstituting the definitions $a_{ii} = K_{ii} + \\sigma_n^2$ and $\\mathbf{a}_{i,-i} = \\mathbf{k}_{i,-i}$, we get:\n$$\n\\frac{1}{(A^{-1})_{ii}} = (K_{ii} + \\sigma_n^2) - \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{k}_{-i,i}\n$$\nBy rearranging and comparing with the expression for $\\sigma_{-i}^2(t_i)$, we find:\n$$\n\\sigma_{-i}^2(t_i) = K_{ii} - \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{k}_{-i,i} = \\frac{1}{(A^{-1})_{ii}} - \\sigma_n^2\n$$\nThis provides the LOO predictive variance.\n\nFor the LOO predictive mean, we use another identity from partitioned matrices and Gaussian conditioning. Consider the precision matrix $\\Lambda = A^{-1}$. The conditional mean of $y_i$ given $\\mathbf{y}_{-i}$ (for a zero-mean process) is given by $\\mathbb{E}[y_i|\\mathbf{y}_{-i}] = -(\\Lambda_{ii})^{-1} \\sum_{j \\neq i} \\Lambda_{ij} y_j$. Since we have shown $\\mu_{-i}(t_i) = \\mathbb{E}[y_i|\\mathbf{y}_{-i}]$, we have:\n$$\n\\mu_{-i}(t_i) = -\\frac{1}{(A^{-1})_{ii}} \\sum_{j \\neq i} (A^{-1})_{ij} y_j\n$$\nThis expression is in terms of $A^{-1}$ and $\\mathbf{y}$. An alternative, more compact form can be found by relating it to the vector $\\boldsymbol{\\alpha} = A^{-1} \\mathbf{y}$. By definition, $\\boldsymbol{\\alpha}_i = \\sum_{j=1}^n (A^{-1})_{ij} y_j = (A^{-1})_{ii} y_i + \\sum_{j \\neq i} (A^{-1})_{ij} y_j$.\nRearranging gives $\\sum_{j \\neq i} (A^{-1})_{ij} y_j = \\boldsymbol{\\alpha}_i - (A^{-1})_{ii} y_i$. Substituting this into the expression for $\\mu_{-i}(t_i)$:\n$$\n\\mu_{-i}(t_i) = -\\frac{1}{(A^{-1})_{ii}} (\\boldsymbol{\\alpha}_i - (A^{-1})_{ii} y_i) = y_i - \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}}\n$$\n\n**2) Expression for the Leave-One-Out Residual**\n\nThe leave-one-out residual is defined as $e^{\\mathrm{loo}}_i = y_i - \\mu_{-i}(t_i)$. Using the compact expression for $\\mu_{-i}(t_i)$ derived above:\n$$\ne^{\\mathrm{loo}}_i = y_i - \\left( y_i - \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}} \\right) = \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}}\n$$\nwhere $\\boldsymbol{\\alpha} = A^{-1} \\mathbf{y}$. This expresses the residual entirely in terms of the vector $\\boldsymbol{\\alpha}$ and the diagonal entries of $A^{-1}$, as required.\n\n**3) Closed-Form Expression for $\\mathrm{RMSE}_{\\mathrm{loo}}$**\n\nThe leave-one-out root-mean-square error is defined as $\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (e^{\\mathrm{loo}}_i)^2}$. We substitute the expression for $e^{\\mathrm{loo}}_i$ from part 2 into this definition:\n$$\n\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}} \\right)^2}\n$$\nTo make the dependence on $A^{-1}$ and $\\mathbf{y}$ fully explicit, we substitute $\\boldsymbol{\\alpha}_i = (A^{-1} \\mathbf{y})_i = \\sum_{j=1}^n (A^{-1})_{ij} y_j$. This yields the final closed-form expression:\n$$\n\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\sum_{j=1}^n (A^{-1})_{ij} y_j}{(A^{-1})_{ii}} \\right)^2}\n$$\nThis expression depends only on the pre-computed matrix $A^{-1}$ and the observation vector $\\mathbf{y}$, and provides a computationally efficient way to estimate the LOO cross-validation error without retraining the GP model $n$ times.\n\n**4) Justification for Adaptive Sampling**\n\nThe goal of adaptive sampling in design space exploration is to add new simulation points in regions that most effectively improve the model's accuracy and reduce its uncertainty. The LOO metrics provide valuable heuristics for identifying such regions.\n\nThe magnitude of the LOO residual, $|e^{\\mathrm{loo}}_i| = |\\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}}|$, quantifies the \"surprise\" of data point $(t_i, y_i)$ with respect to the rest of the dataset. A large $|e^{\\mathrm{loo}}_i|$ signifies that the model trained on $\\mathcal{D}_{-i}$ makes a poor prediction at $t_i$. Within the GP framework, this implies that the function $r(t)$ is behaving in a way not easily interpolated from neighboring points, likely exhibiting rapid change or high curvature near $t_i$. To capture this rapid variation, the posterior mean $\\mu(t)$ must have a large gradient magnitude $|\\partial \\mu(t)/\\partial t|$ in the vicinity of $t_i$. Therefore, regions with high LOO residuals are prime candidates for adding new samples to better resolve the function's local behavior.\n\nThe LOO predictive uncertainty, represented by the standard deviation $\\sigma_{-i}(t_i) = \\sqrt{(A^{-1})_{ii}^{-1} - \\sigma_n^2}$, measures the model's epistemic (or reducible) uncertainty at location $t_i$ when that point is excluded from training. A large $\\sigma_{-i}(t_i)$ indicates that the point $t_i$ is poorly constrained by the other data points. In a one-dimensional space, this typically occurs when $t_i$ is in a sparsely sampled region, i.e., the distance to its neighbors is large. The correlation function $k(t,t')$ decays with distance, so distant points offer little information, resulting in high posterior variance. Sampling in regions of high $\\sigma_{-i}(t_i)$ directly targets areas of high model uncertainty, reducing the overall predictive variance of the surrogate and making it more reliable.\n\nIn summary, a principled adaptive sampling strategy can be devised by inspecting the LOO metrics across the training set. One would compute $|e^{\\mathrm{loo}}_i|$ and $\\sigma_{-i}(t_i)$ for all $i=1, \\dots, n$. The next simulation should be run at a new point $t_{\\text{new}}$ in the neighborhood of the existing point $t_k$ which exhibits the largest error or uncertainty (e.g., maximum $|e^{\\mathrm{loo}}_k|$). This iteratively refines the model by adding data where it is most needed, either to correct prediction errors (indicated by $|e^{\\mathrm{loo}}_i|$) or to reduce model ignorance (indicated by $\\sigma_{-i}(t_i)$). This approach uses information intrinsic to the GP's Bayesian formulation to guide the exploration process efficiently.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{\\sum_{j=1}^{n} (A^{-1})_{ij} y_j}{(A^{-1})_{ii}} \\right)^{2}}}\n$$"
        },
        {
            "introduction": "Surrogate models accelerate design exploration, but their predictions are never perfect. A critical task is to assess whether a predicted set of optimal designs—the Pareto front—is truly optimal or merely an artifact of model error and miscalibration. This hands-on coding practice challenges you to build a robust diagnostic tool () to identify such 'false' Pareto optimal points. By combining surrogate uncertainty, model calibration, and probabilistic analysis, you will develop a method to quantify the reliability of a predicted optimal front and suggest targeted new simulations to resolve the most critical ambiguities.",
            "id": "3905822",
            "problem": "You are given a set of candidate designs and surrogate model outputs for two conflicting objectives in automated battery design and simulation. The goal is to construct an uncertainty-aware diagnostic that detects false Pareto optimality caused by surrogate miscalibration and to suggest corrective sampling actions. Work in the multi-objective minimization regime on two objectives, denoted by $f_1$ and $f_2$. Assume surrogate predictions are independent across designs and objectives, and each objective prediction at a design is modeled as a Gaussian random variable with a mean and a standard deviation.\n\nFundamental base and definitions:\n- A design $i$ with objective vector $\\boldsymbol{f}(i) = [f_1(i), f_2(i)]$ is strictly Pareto dominated by design $j$ if $f_k(j) \\le f_k(i)$ for both $k \\in \\{1,2\\}$ and $f_m(j) < f_m(i)$ for at least one $m \\in \\{1,2\\}$.\n- The mean-based nondominated set $S$ is the subset of candidate designs that are not strictly Pareto dominated when comparing predicted means alone.\n- Surrogate predictions for each objective are modeled as $X_{i,k} \\sim \\mathcal{N}(\\mu_{i,k}, \\sigma_{i,k}^2)$ where $i$ indexes designs and $k \\in \\{1,2\\}$ indexes objectives. Let the holdout set contain $H$ observations and associated surrogate predictions $(\\mu_{h,k}, \\sigma_{h,k})$ and observed values $y_{h,k}$ for $h \\in \\{1,\\dots,H\\}$.\n- Miscalibration is quantified via the normalized residuals $z_{h,k} = \\dfrac{y_{h,k} - \\mu_{h,k}}{\\sigma_{h,k}}$. Under perfect calibration, $z_{h,k}$ should be distributed approximately as $\\mathcal{N}(0,1)$, so its empirical variance should be near $1$. Define the per-objective calibration variance $v_k = \\operatorname{Var}(z_{h,k})$ and the corresponding scale factor $\\alpha_k = \\sqrt{v_k}$. Use $\\alpha_k$ to rescale candidate predictive standard deviations: $\\tilde{\\sigma}_{i,k} = \\alpha_k \\sigma_{i,k}$.\n- The probability that design $j$ dominates design $i$ under the Gaussian predictive distributions is computed by modeling $T_{i,j,k} = X_{j,k} - X_{i,k} \\sim \\mathcal{N}(\\Delta\\mu_{j,i,k}, \\Delta\\sigma_{j,i,k}^2)$ with $\\Delta\\mu_{j,i,k} = \\mu_{j,k} - \\mu_{i,k}$ and $\\Delta\\sigma_{j,i,k}^2 = \\tilde{\\sigma}_{j,k}^2 + \\tilde{\\sigma}_{i,k}^2$. With independence across objectives, the joint probability of dominance is\n$$\np_{j \\preceq i} = \\prod_{k=1}^{2} \\Phi\\!\\left(\\frac{0 - \\Delta\\mu_{j,i,k}}{\\sqrt{\\Delta\\sigma_{j,i,k}^2}}\\right),\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. The robust nondominance probability for design $i$ is then\n$$\np^{\\text{ND}}_i = \\prod_{\\substack{j=1 \\\\ j \\ne i}}^{N} \\left(1 - p_{j \\preceq i}\\right),\n$$\ncomputed only for $i \\in S$, where $N$ is the number of candidate designs.\n\nDiagnostic and corrective action:\n- Given a threshold $\\tau \\in (0,1)$, flag the presence of false Pareto optimality if there exists any $i \\in S$ such that $p^{\\text{ND}}_i < \\tau$.\n- Suggest a single corrective sampling action by returning the $0$-based index of the design $i^\\star \\in S$ with the smallest $p^{\\text{ND}}_i$ (ties broken by the smallest index). The recommended action is to draw an additional measurement for $i^\\star$ to reduce ambiguity in the front. If $S$ is empty, return $-1$.\n\nImplement a program that performs the following steps for each test case:\n1. Compute the mean-based nondominated set $S$ from $\\mu_{i,k}$.\n2. Compute per-objective calibration factors $\\alpha_k$ from the holdout set and rescale $\\sigma_{i,k}$ to obtain $\\tilde{\\sigma}_{i,k}$.\n3. Compute $p_{j \\preceq i}$ for all pairs $(i,j)$ and then $p^{\\text{ND}}_i$ for $i \\in S$.\n4. Produce a boolean indicating whether false Pareto optimality is detected and an integer specifying the recommended $0$-based index for corrective sampling.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where for each test case you output two items appended in order: the boolean diagnostic and the suggested index. For example, for three test cases it should print a list with $6$ elements of the form $[b_1,i_1,b_2,i_2,b_3,i_3]$.\n\nUse the following test suite. All arrays are specified as rows corresponding to designs or holdouts and columns corresponding to objectives $(f_1,f_2)$, and all indices are $0$-based.\n\nTest case $1$ (moderate miscalibration in the second objective):\n- Candidate means $\\mu$:\n  $$\n  \\begin{bmatrix}\n  1.0 & 3.0 \\\\\n  1.2 & 2.6 \\\\\n  0.9 & 3.4 \\\\\n  1.4 & 2.2\n  \\end{bmatrix}\n  $$\n- Candidate standard deviations $\\sigma$:\n  $$\n  \\begin{bmatrix}\n  0.2 & 0.4 \\\\\n  0.3 & 0.3 \\\\\n  0.25 & 0.35 \\\\\n  0.15 & 0.2\n  \\end{bmatrix}\n  $$\n- Holdout means $\\mu_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  1.1 & 2.7 \\\\\n  1.3 & 2.4 \\\\\n  0.95 & 3.3 \\\\\n  1.5 & 2.1 \\\\\n  1.05 & 3.1 \\\\\n  1.25 & 2.5\n  \\end{bmatrix}\n  $$\n- Holdout standard deviations $\\sigma_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  0.2 & 0.25 \\\\\n  0.25 & 0.2 \\\\\n  0.2 & 0.3 \\\\\n  0.2 & 0.2 \\\\\n  0.15 & 0.25 \\\\\n  0.2 & 0.2\n  \\end{bmatrix}\n  $$\n- Holdout observed values $y_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  1.15 & 3.3 \\\\\n  1.35 & 2.8 \\\\\n  0.90 & 3.9 \\\\\n  1.52 & 2.0 \\\\\n  1.08 & 3.55 \\\\\n  1.20 & 3.2\n  \\end{bmatrix}\n  $$\n- Threshold $\\tau$: $0.65$.\n\nTest case $2$ (near-perfect calibration and well-separated trade-offs):\n- Candidate means $\\mu$:\n  $$\n  \\begin{bmatrix}\n  0.8 & 4.0 \\\\\n  1.6 & 2.4 \\\\\n  2.4 & 1.6 \\\\\n  3.2 & 1.2\n  \\end{bmatrix}\n  $$\n- Candidate standard deviations $\\sigma$:\n  $$\n  \\begin{bmatrix}\n  0.1 & 0.1 \\\\\n  0.1 & 0.1 \\\\\n  0.1 & 0.1 \\\\\n  0.1 & 0.1\n  \\end{bmatrix}\n  $$\n- Holdout means $\\mu_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  1.0 & 2.0 \\\\\n  1.5 & 2.5 \\\\\n  2.0 & 1.8 \\\\\n  3.0 & 1.2 \\\\\n  0.7 & 4.1 \\\\\n  2.6 & 1.4\n  \\end{bmatrix}\n  $$\n- Holdout standard deviations $\\sigma_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  0.2 & 0.2 \\\\\n  0.2 & 0.2 \\\\\n  0.2 & 0.2 \\\\\n  0.2 & 0.2 \\\\\n  0.2 & 0.2 \\\\\n  0.2 & 0.2\n  \\end{bmatrix}\n  $$\n- Holdout observed values $y_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  1.05 & 2.02 \\\\\n  1.45 & 2.55 \\\\\n  2.02 & 1.78 \\\\\n  3.02 & 1.22 \\\\\n  0.69 & 4.09 \\\\\n  2.62 & 1.42\n  \\end{bmatrix}\n  $$\n- Threshold $\\tau$: $0.50$.\n\nTest case $3$ (nearly identical means causing ambiguity):\n- Candidate means $\\mu$:\n  $$\n  \\begin{bmatrix}\n  1.0 & 2.0 \\\\\n  1.0 & 2.0 \\\\\n  0.95 & 2.05 \\\\\n  1.2 & 1.8\n  \\end{bmatrix}\n  $$\n- Candidate standard deviations $\\sigma$:\n  $$\n  \\begin{bmatrix}\n  0.5 & 0.5 \\\\\n  0.5 & 0.5 \\\\\n  0.4 & 0.4 \\\\\n  0.6 & 0.3\n  \\end{bmatrix}\n  $$\n- Holdout means $\\mu_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  1.0 & 2.0 \\\\\n  1.1 & 2.1 \\\\\n  0.9 & 2.2 \\\\\n  1.2 & 1.9\n  \\end{bmatrix}\n  $$\n- Holdout standard deviations $\\sigma_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  0.5 & 0.5 \\\\\n  0.4 & 0.4 \\\\\n  0.6 & 0.6 \\\\\n  0.5 & 0.4\n  \\end{bmatrix}\n  $$\n- Holdout observed values $y_{\\text{hold}}$:\n  $$\n  \\begin{bmatrix}\n  1.05 & 1.9 \\\\\n  1.2 & 2.0 \\\\\n  0.8 & 2.3 \\\\\n  1.1 & 2.0\n  \\end{bmatrix}\n  $$\n- Threshold $\\tau$: $0.60$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, append first the boolean diagnostic and then the recommended $0$-based index. For the three test cases above, the output must be of the form $[b_1,i_1,b_2,i_2,b_3,i_3]$ where each $b_t$ is a boolean and each $i_t$ is an integer.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of multi-objective optimization, uncertainty quantification, and statistical modeling. The problem is well-posed, with all necessary data, definitions, and constraints provided to compute a unique and meaningful solution. The methodology is clearly articulated and follows established practices in surrogate-based design optimization. We now proceed with a complete solution.\n\nThe core of the task is to implement a diagnostic for detecting \"false\" Pareto optimal points—designs that appear nondominated based on mean predictions from a surrogate model, but whose nondominated status is questionable due to model uncertainty and miscalibration. The procedure involves four main steps, which we will detail.\n\nLet us walk through the procedure for Test Case $1$.\n\n**Step 1: Compute the Mean-Based Nondominated Set $S$**\n\nThe set $S$ contains designs that are not strictly Pareto dominated by any other design when considering only their predicted mean objective values. A design $i$ with mean objective vector $\\boldsymbol{\\mu}(i) = [\\mu_{i,1}, \\mu_{i,2}]$ is strictly dominated by design $j$ if $(\\mu_{j,1} \\le \\mu_{i,1} \\land \\mu_{j,2} < \\mu_{i,2}) \\lor (\\mu_{j,1} < \\mu_{i,1} \\land \\mu_{j,2} \\le \\mu_{i,2})$. We are in a minimization setting.\n\nFor Test Case $1$, the candidate means are:\n$$\n\\boldsymbol{\\mu} =\n\\begin{bmatrix}\n1.0 & 3.0 \\\\\n1.2 & 2.6 \\\\\n0.9 & 3.4 \\\\\n1.4 & 2.2\n\\end{bmatrix}\n$$\n- Design $0$ ($\\boldsymbol{\\mu}=[1.0, 3.0]$): Not dominated by any other design. For example, design $2$ has a better first objective ($\\mu_{2,1}=0.9 < 1.0$) but a worse second objective ($\\mu_{2,2}=3.4 > 3.0$).\n- Design $1$ ($\\boldsymbol{\\mu}=[1.2, 2.6]$): Not dominated by any other design.\n- Design $2$ ($\\boldsymbol{\\mu}=[0.9, 3.4]$): Not dominated by any other design.\n- Design $3$ ($\\boldsymbol{\\mu}=[1.4, 2.2]$): Not dominated by any other design.\n\nAll four designs are mutually non-dominating. Therefore, the mean-based nondominated set is $S = \\{0, 1, 2, 3\\}$.\n\n**Step 2: Compute Calibration Factors and Rescale Standard Deviations**\n\nWe use the holdout set to assess the surrogate model's calibration. The normalized residuals for each objective $k$ are calculated as $z_{h,k} = (y_{h,k} - \\mu_{h,k}) / \\sigma_{h,k}$.\n\nFor objective $k=1$:\nThe normalized residuals are $z_1 = [0.25, 0.2, -0.25, 0.1, 0.2, -0.25]$. The sample variance (using Bessel's correction, with a denominator of $H-1=5$) is $v_1 = \\operatorname{Var}(z_{1}) \\approx 0.050417$. The calibration factor is $\\alpha_1 = \\sqrt{v_1} \\approx 0.2245$.\n\nFor objective $k=2$:\nThe normalized residuals are $z_2 = [2.4, 2.0, 2.0, -0.5, 1.8, 3.5]$. The sample variance is $v_2 = \\operatorname{Var}(z_{2}) \\approx 1.658667$. The calibration factor is $\\alpha_2 = \\sqrt{v_2} \\approx 1.2879$. A value of $\\alpha_2 > 1$ indicates that the surrogate is underconfident for the second objective (its predicted standard deviations are, on average, too small).\n\nWe use these factors to rescale the candidate predictive standard deviations: $\\tilde{\\sigma}_{i,k} = \\alpha_k \\sigma_{i,k}$.\nThe original candidate standard deviations $\\boldsymbol{\\sigma}$:\n$$\n\\boldsymbol{\\sigma} = \\begin{bmatrix}\n0.2 & 0.4 \\\\ 0.3 & 0.3 \\\\ 0.25 & 0.35 \\\\ 0.15 & 0.2\n\\end{bmatrix}\n$$\nThe rescaled standard deviations $\\tilde{\\boldsymbol{\\sigma}}$ are:\n$$\n\\tilde{\\boldsymbol{\\sigma}} = \\begin{bmatrix}\n0.2 \\cdot \\alpha_1 & 0.4 \\cdot \\alpha_2 \\\\\n0.3 \\cdot \\alpha_1 & 0.3 \\cdot \\alpha_2 \\\\\n0.25 \\cdot \\alpha_1 & 0.35 \\cdot \\alpha_2 \\\\\n0.15 \\cdot \\alpha_1 & 0.2 \\cdot \\alpha_2\n\\end{bmatrix}\n\\approx\n\\begin{bmatrix}\n0.0449 & 0.5152 \\\\\n0.0674 & 0.3864 \\\\\n0.0561 & 0.4508 \\\\\n0.0337 & 0.2576\n\\end{bmatrix}\n$$\n\n**Step 3: Compute Nondominance Probabilities**\n\nFor each design $i \\in S$, we calculate its robust nondominance probability, $p^{\\text{ND}}_i$. This is the probability that no other design $j$ will dominate it, considering the calibrated predictive distributions.\n$$\np^{\\text{ND}}_i = \\prod_{\\substack{j=1 \\\\ j \\ne i}}^{N} \\left(1 - p_{j \\preceq i}\\right)\n$$\nwhere $p_{j \\preceq i}$ is the probability that $j$ weakly dominates $i$. This is computed as:\n$$\np_{j \\preceq i} = \\prod_{k=1}^{2} \\operatorname{P}(X_{j,k} \\le X_{i,k}) = \\prod_{k=1}^{2} \\Phi\\!\\left(\\frac{\\mu_{i,k} - \\mu_{j,k}}{\\sqrt{\\tilde{\\sigma}_{i,k}^2 + \\tilde{\\sigma}_{j,k}^2}}\\right)\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function (CDF).\n\nLet's compute $p^{\\text{ND}}_3$ as an example. We need $p_{0 \\preceq 3}$, $p_{1 \\preceq 3}$, and $p_{2 \\preceq 3}$.\n- For $j=1, i=3$: We compare design $1$ ($\\boldsymbol{\\mu}=[1.2, 2.6], \\tilde{\\boldsymbol{\\sigma}}\\approx[0.0674, 0.3864]$) and design $3$ ($\\boldsymbol{\\mu}=[1.4, 2.2], \\tilde{\\boldsymbol{\\sigma}}\\approx[0.0337, 0.2576]$).\n  - $k=1$: $\\frac{\\mu_{3,1}-\\mu_{1,1}}{\\sqrt{\\tilde{\\sigma}_{3,1}^2+\\tilde{\\sigma}_{1,1}^2}} = \\frac{1.4-1.2}{\\sqrt{0.0337^2+0.0674^2}} \\approx \\frac{0.2}{0.0753} \\approx 2.654$. $\\Phi(2.654) \\approx 0.9960$.\n  - $k=2$: $\\frac{\\mu_{3,2}-\\mu_{1,2}}{\\sqrt{\\tilde{\\sigma}_{3,2}^2+\\tilde{\\sigma}_{1,2}^2}} = \\frac{2.2-2.6}{\\sqrt{0.2576^2+0.3864^2}} \\approx \\frac{-0.4}{0.4644} \\approx -0.8613$. $\\Phi(-0.8613) \\approx 0.1945$.\n  - $p_{1 \\preceq 3} \\approx 0.9960 \\times 0.1945 \\approx 0.1937$.\n\nRepeating this for all pairs and then for all $i \\in S$, we obtain the following nondominance probabilities:\n- $p^{\\text{ND}}_0 \\approx 0.9419$\n- $p^{\\text{ND}}_1 \\approx 0.7423$\n- $p^{\\text{ND}}_2 \\approx 0.9972$\n- $p^{\\text{ND}}_3 \\approx 0.6272$\n\n**Step 4: Produce Diagnostic and Corrective Action**\n\nThe threshold is given as $\\tau=0.65$.\n- **Diagnostic**: We check if there exists any $i \\in S$ such that $p^{\\text{ND}}_i < \\tau$.\n  For design $3$, we find $p^{\\text{ND}}_3 \\approx 0.6272 < 0.65$. Thus, the condition is met. The diagnostic is `True`, indicating the presence of potential false Pareto optimality.\n- **Corrective Action**: We must suggest the index $i^\\star \\in S$ corresponding to the minimum $p^{\\text{ND}}_i$.\n  The probabilities are $\\{p^{\\text{ND}}_0: 0.9419, p^{\\text{ND}}_1: 0.7423, p^{\\text{ND}}_2: 0.9972, p^{\\text{ND}}_3: 0.6272\\}$.\n  The minimum value is $\\approx 0.6272$, which corresponds to design index $i^\\star=3$.\n\nThe results for Test Case $1$ are: diagnostic=`True`, suggested index=`3`.\n\nThis same four-step procedure is applied to Test Cases $2$ and $3$.\n- For Test Case $2$, the calibration factors are nearly $1$, and the mean designs are well-separated. This results in high nondominance probabilities for all designs in $S$, so the diagnostic is `False`.\n- For Test Case $3$, the means are very close, leading to high uncertainty about the true Pareto front. This results in low nondominance probabilities, triggering the diagnostic and a suggested sampling action.\n\nThe final output is generated by concatenating the boolean diagnostic and integer index for each test case into a single list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the battery design diagnostic problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"mu\": np.array([\n                [1.0, 3.0],\n                [1.2, 2.6],\n                [0.9, 3.4],\n                [1.4, 2.2]\n            ]),\n            \"sigma\": np.array([\n                [0.2, 0.4],\n                [0.3, 0.3],\n                [0.25, 0.35],\n                [0.15, 0.2]\n            ]),\n            \"mu_hold\": np.array([\n                [1.1, 2.7],\n                [1.3, 2.4],\n                [0.95, 3.3],\n                [1.5, 2.1],\n                [1.05, 3.1],\n                [1.25, 2.5]\n            ]),\n            \"sigma_hold\": np.array([\n                [0.2, 0.25],\n                [0.25, 0.2],\n                [0.2, 0.3],\n                [0.2, 0.2],\n                [0.15, 0.25],\n                [0.2, 0.2]\n            ]),\n            \"y_hold\": np.array([\n                [1.15, 3.3],\n                [1.35, 2.8],\n                [0.90, 3.9],\n                [1.52, 2.0],\n                [1.08, 3.55],\n                [1.20, 3.2]\n            ]),\n            \"tau\": 0.65\n        },\n        {\n            \"mu\": np.array([\n                [0.8, 4.0],\n                [1.6, 2.4],\n                [2.4, 1.6],\n                [3.2, 1.2]\n            ]),\n            \"sigma\": np.array([\n                [0.1, 0.1],\n                [0.1, 0.1],\n                [0.1, 0.1],\n                [0.1, 0.1]\n            ]),\n            \"mu_hold\": np.array([\n                [1.0, 2.0],\n                [1.5, 2.5],\n                [2.0, 1.8],\n                [3.0, 1.2],\n                [0.7, 4.1],\n                [2.6, 1.4]\n            ]),\n            \"sigma_hold\": np.array([\n                [0.2, 0.2],\n                [0.2, 0.2],\n                [0.2, 0.2],\n                [0.2, 0.2],\n                [0.2, 0.2],\n                [0.2, 0.2]\n            ]),\n            \"y_hold\": np.array([\n                [1.05, 2.02],\n                [1.45, 2.55],\n                [2.02, 1.78],\n                [3.02, 1.22],\n                [0.69, 4.09],\n                [2.62, 1.42]\n            ]),\n            \"tau\": 0.50\n        },\n        {\n            \"mu\": np.array([\n                [1.0, 2.0],\n                [1.0, 2.0],\n                [0.95, 2.05],\n                [1.2, 1.8]\n            ]),\n            \"sigma\": np.array([\n                [0.5, 0.5],\n                [0.5, 0.5],\n                [0.4, 0.4],\n                [0.6, 0.3]\n            ]),\n            \"mu_hold\": np.array([\n                [1.0, 2.0],\n                [1.1, 2.1],\n                [0.9, 2.2],\n                [1.2, 1.9]\n            ]),\n            \"sigma_hold\": np.array([\n                [0.5, 0.5],\n                [0.4, 0.4],\n                [0.6, 0.6],\n                [0.5, 0.4]\n            ]),\n            \"y_hold\": np.array([\n                [1.05, 1.9],\n                [1.2, 2.0],\n                [0.8, 2.3],\n                [1.1, 2.0]\n            ]),\n            \"tau\": 0.60\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        mu, sigma, mu_hold, sigma_hold, y_hold, tau = \\\n            case[\"mu\"], case[\"sigma\"], case[\"mu_hold\"], case[\"sigma_hold\"], case[\"y_hold\"], case[\"tau\"]\n        \n        num_designs, num_objectives = mu.shape\n\n        # Step 1: Compute the mean-based nondominated set S\n        nondominated_indices = []\n        for i in range(num_designs):\n            is_dominated = False\n            for j in range(num_designs):\n                if i == j:\n                    continue\n                # Check for strict Pareto dominance (minimization)\n                if (np.all(mu[j] <= mu[i]) and np.any(mu[j] < mu[i])):\n                    is_dominated = True\n                    break\n            if not is_dominated:\n                nondominated_indices.append(i)\n        \n        S = nondominated_indices\n        \n        if not S:\n            final_results.extend([False, -1])\n            continue\n\n        # Step 2: Compute calibration factors and rescale standard deviations\n        z_residuals = (y_hold - mu_hold) / sigma_hold\n        \n        # Use ddof=1 for sample variance\n        calib_variance = np.var(z_residuals, axis=0, ddof=1)\n        alpha = np.sqrt(calib_variance)\n        \n        sigma_tilde = sigma * alpha\n\n        # Step 3: Compute nondominance probabilities\n        p_nondominated = {}\n        \n        for i in S:\n            p_survival = 1.0\n            for j in range(num_designs):\n                if i == j:\n                    continue\n                \n                # Probability that j weakly dominates i: P(X_j <= X_i)\n                # P(X_j,k <= X_i,k) = P(X_i,k - X_j,k >= 0)\n                delta_mu = mu[i, :] - mu[j, :]\n                # Variance of difference is sum of variances\n                delta_sigma_sq = sigma_tilde[i, :]**2 + sigma_tilde[j, :]**2\n                delta_sigma = np.sqrt(delta_sigma_sq)\n\n                # Standard normal CDF argument for P(Diff >= 0)\n                z_score = delta_mu / delta_sigma\n                \n                # Probability of weak dominance in each objective\n                p_weak_dominance_per_obj = norm.cdf(z_score)\n                # Joint probability assuming independence\n                p_j_preceq_i = np.prod(p_weak_dominance_per_obj)\n                \n                p_survival *= (1.0 - p_j_preceq_i)\n            \n            p_nondominated[i] = p_survival\n\n        # Step 4: Produce diagnostic and corrective action\n        min_p_nd = float('inf')\n        rec_index = -1\n        \n        # Iterate through sorted keys to handle tie-breaking rule\n        for i in sorted(p_nondominated.keys()):\n            if p_nondominated[i] < min_p_nd:\n                min_p_nd = p_nondominated[i]\n                rec_index = i\n        \n        diagnostic = min_p_nd < tau if rec_index !=-1 else False\n\n        final_results.extend([diagnostic, rec_index])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}