{
    "hands_on_practices": [
        {
            "introduction": "A robust automated workflow must first guarantee the numerical accuracy of its underlying simulations. This exercise connects the physical theory of diffusion to the practical requirement of mesh resolution in a finite difference scheme. You will derive a mesh-spacing constraint based on a Key Performance Indicator (KPI)—the error in the computed gradient—to ensure the simulation accurately captures critical physical phenomena like boundary layer formation. ",
            "id": "3893789",
            "problem": "An automated simulation workflow for lithium-ion battery design must allocate spatial resolution in the separator to accurately capture electrolyte concentration boundary layers that form under a step to high current. The workflow uses a uniform one-dimensional mesh of the separator of thickness $\\delta_{s}$ and advances the electrolyte salt concentration $c_{e}(x,t)$ using a second-order centered Finite Difference (FD) scheme. The design Key Performance Indicator (KPI) for mesh sufficiency is the relative pointwise error in the gradient $\\nabla c_{e}$, constrained by a tolerance $\\varepsilon$.\n\nAssume early-time boundary layer formation is diffusion-dominated and can be approximated by Fick’s second law, $\\partial c_{e}/\\partial t = D_{e} \\, \\partial^{2} c_{e}/\\partial x^{2}$, on a semi-infinite domain $x \\geq 0$ subjected at $t > 0$ to a step change in interfacial concentration at $x=0$ from $c_{0}$ to $c_{s}$, with $c_{e}(x,0)=c_{0}$ and $c_{e}(\\infty,t)=c_{0}$. This canonical diffusion problem yields a boundary layer near $x=0$ whose gradient sets the most stringent resolution requirement. The mesh spacing is $h = \\delta_{s}/(N-1)$ for $N$ uniform nodes including both separator boundaries.\n\nStarting from these principles and without invoking any shortcut formulas, derive the mesh-spacing constraint needed to enforce that the relative error in $\\nabla c_{e}$ at the location of its maximum is bounded by $\\varepsilon$ when using the second-order centered FD gradient. Then compute the smallest integer number of nodes $N$ that satisfies this constraint for:\n- $\\delta_{s} = 25 \\times 10^{-6} \\ \\mathrm{m}$,\n- $D_{e} = 1.2 \\times 10^{-10} \\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$,\n- $t = 1 \\ \\mathrm{s}$,\n- $\\varepsilon = 0.01$.\n\nState any mathematical assumptions you make explicitly and keep constants symbolic until the final numerical evaluation. Report the final result as an exact integer number of nodes $N$ (unitless). No rounding rule is needed beyond the integer requirement.",
            "solution": "The problem requires the derivation of a mesh-spacing constraint for an automated battery simulation workflow. The constraint is based on the relative error of a numerical gradient calculation for the electrolyte concentration, $c_e(x,t)$, during early-time boundary layer formation. The final objective is to calculate the minimum number of mesh nodes, $N$, needed to satisfy this constraint under specified conditions.\n\nThe validation of the problem statement is performed first.\n\n### Step 1: Extract Givens\n-   Governing Equation: Fick's second law, $\\frac{\\partial c_{e}}{\\partial t} = D_{e} \\frac{\\partial^{2} c_{e}}{\\partial x^{2}}$.\n-   Domain: Semi-infinite, $x \\geq 0$.\n-   Initial Condition: $c_{e}(x,0) = c_{0}$.\n-   Boundary Conditions: $c_{e}(0,t) = c_{s}$ for $t > 0$, and $c_{e}(\\infty,t) = c_{0}$.\n-   Numerical Method: Second-order centered Finite Difference (FD) for the gradient $\\nabla c_{e}$.\n-   Performance Constraint (KPI): Relative pointwise error in $\\nabla c_{e}$ at its maximum is bounded by $\\varepsilon$.\n-   Mesh Definition: Uniform mesh with $N$ nodes on a separator of thickness $\\delta_s$. Mesh spacing is $h = \\frac{\\delta_{s}}{N-1}$.\n-   Numerical values:\n    -   $\\delta_{s} = 25 \\times 10^{-6} \\ \\mathrm{m}$\n    -   $D_{e} = 1.2 \\times 10^{-10} \\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$\n    -   $t = 1 \\ \\mathrm{s}$\n    -   $\\varepsilon = 0.01$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the principles of diffusion (Fick's law) and numerical analysis (finite differences). The setup, involving the use of a semi-infinite domain solution to ascertain resolution requirements for a finite domain problem at early times, is a standard and physically sound engineering approximation. The characteristic diffusion length $\\sqrt{D_e t} = \\sqrt{1.2 \\times 10^{-10} \\times 1} \\approx 3.46 \\times 10^{-6} \\ \\mathrm{m}$ is substantially smaller than the separator thickness $\\delta_s = 25 \\times 10^{-6} \\ \\mathrm{m}$, justifying the semi-infinite assumption at the given time $t=1 \\ \\mathrm{s}$. The problem is well-posed, objective, and contains sufficient information for a unique solution.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full derivation and solution will be provided.\n\n### Derivation of the Mesh-Spacing Constraint\n\n**1. Analytical Solution of the Governing Equation**\n\nThe diffusion process is described by the partial differential equation (PDE):\n$$\n\\frac{\\partial c_{e}}{\\partial t} = D_{e} \\frac{\\partial^{2} c_{e}}{\\partial x^{2}}\n$$\nwith initial and boundary conditions:\n$$\nc_{e}(x,0) = c_{0} \\quad \\text{for } x \\geq 0\n$$\n$$\nc_{e}(0,t) = c_{s} \\quad \\text{for } t > 0\n$$\n$$\n\\lim_{x\\to\\infty} c_{e}(x,t) = c_{0} \\quad \\text{for } t > 0\n$$\nThis is a standard one-dimensional diffusion problem on a semi-infinite domain. We solve it using a similarity transformation. Let us define a dimensionless similarity variable $\\eta$:\n$$\n\\eta = \\frac{x}{2\\sqrt{D_e t}}\n$$\nAssuming a solution of the form $c_e(x,t) = f(\\eta)$, the PDE transforms into an ordinary differential equation (ODE). After normalizing the concentration as $u(\\eta) = \\frac{c_e - c_0}{c_s - c_0}$, the PDE becomes:\n$$\nu''(\\eta) + 2\\eta u'(\\eta) = 0\n$$\nThe boundary conditions transform to $u(0) = 1$ and $u(\\infty) = 0$. The solution to this ODE is given in terms of the complementary error function, $\\mathrm{erfc}(z)$:\n$$\nu(\\eta) = \\mathrm{erfc}(\\eta)\n$$\nReverting to the original variables, the analytical solution for the concentration profile is:\n$$\nc_e(x,t) = c_0 + (c_s - c_0) \\mathrm{erfc}\\left(\\frac{x}{2\\sqrt{D_e t}}\\right)\n$$\n\n**2. Analytical Gradient and Location of its Maximum**\n\nThe concentration gradient, $G(x,t) = \\nabla c_e = \\frac{\\partial c_e}{\\partial x}$, is found by differentiating the solution with respect to $x$. Using the property $\\frac{d}{dz}\\mathrm{erfc}(z) = -\\frac{2}{\\sqrt{\\pi}}\\exp(-z^2)$ and the chain rule:\n$$\n\\frac{\\partial c_e}{\\partial x} = (c_s - c_0) \\left( -\\frac{2}{\\sqrt{\\pi}} \\exp\\left(-\\frac{x^2}{4 D_e t}\\right) \\right) \\left( \\frac{1}{2\\sqrt{D_e t}} \\right)\n$$\n$$\nG(x,t) = \\frac{\\partial c_e}{\\partial x} = -\\frac{c_s - c_0}{\\sqrt{\\pi D_e t}} \\exp\\left(-\\frac{x^2}{4 D_e t}\\right)\n$$\nThe problem requires analysis at the location of the gradient's maximum. The magnitude of the gradient is:\n$$\n|G(x,t)| = \\frac{|c_s - c_0|}{\\sqrt{\\pi D_e t}} \\exp\\left(-\\frac{x^2}{4 D_e t}\\right)\n$$\nThis function is a Gaussian centered at $x=0$, and thus its maximum value occurs at $x=0$. The location of the maximum gradient magnitude is therefore $x_{max} = 0$.\n\n**3. Finite Difference Error Analysis**\n\nThe workflow uses a second-order centered finite difference scheme to approximate the gradient. For a function $f(x)$, the approximation at a point $x_i$ is given by:\n$$\nf'_{FD}(x_i) = \\frac{f(x_i+h) - f(x_i-h)}{2h}\n$$\nThe leading term of the truncation error for this approximation is given by Taylor's theorem:\n$$\nE_{abs}(x) = |f'_{FD}(x) - f'(x)| \\approx \\left| \\frac{h^2}{6} f'''(x) \\right|\n$$\nThe problem specifies the use of this scheme. However, at the boundary point $x=0$, a centered difference stencil is ill-defined as it requires a point at $x=-h$ which is outside the domain. The scheme is applicable to all interior nodes $x_i$ for $i=2, \\dots, N-1$. The point of maximum gradient is $x=0$, and the error is expected to be largest in this region. Therefore, we must enforce the error constraint at the first interior node where the centered scheme can be applied, which is $x_1=h$.\n\n**Assumption:** The error constraint is enforced at the first interior grid point, $x=h$, as this is the closest computable point to the location of maximum gradient magnitude ($x=0$).\n\nThe relative error at $x=h$ is defined as:\n$$\nE_{rel}(h) = \\frac{|G_{FD}(h) - G(h)|}{|G(h)|} \\approx \\frac{h^2}{6} \\frac{|c_e'''(h)|}{|c_e'(h)|}\n$$\nWe need to compute the first and third derivatives of $c_e(x,t)$. Let $C = -\\frac{c_s-c_0}{\\sqrt{\\pi D_e t}}$ and $a = \\frac{1}{4D_e t}$. Then $G(x,t) = c_e'(x) = C \\exp(-ax^2)$.\n$$\nc_e''(x) = C(-2ax)\\exp(-ax^2)\n$$\n$$\nc_e'''(x) = C(-2a)\\exp(-ax^2) + C(-2ax)^2\\exp(-ax^2) = -2aC(1 - 2ax^2)\\exp(-ax^2)\n$$\nThe relative error at an arbitrary point $x$ is:\n$$\nE_{rel}(x) \\approx \\frac{h^2}{6} \\frac{|-2aC(1 - 2ax^2)\\exp(-ax^2)|}{|C\\exp(-ax^2)|} = \\frac{ah^2}{3} |1 - 2ax^2|\n$$\nSubstituting $a = \\frac{1}{4D_e t}$:\n$$\nE_{rel}(x) \\approx \\frac{h^2}{12 D_e t} \\left| 1 - \\frac{x^2}{2 D_e t} \\right|\n$$\nEvaluating this at $x=h$, we get the constraint:\n$$\nE_{rel}(h) \\approx \\frac{h^2}{12 D_e t} \\left| 1 - \\frac{h^2}{2 D_e t} \\right| \\le \\varepsilon\n$$\nFor a sufficiently fine mesh, $h$ must be small, implying that the characteristic length of the discretization $h$ is much smaller than the characteristic diffusion length $\\sqrt{D_e t}$. This means that the term $\\frac{h^2}{2 D_e t}$ is much smaller than $1$. We can therefore approximate $|1 - \\frac{h^2}{2 D_e t}| \\approx 1$. The constraint simplifies to:\n$$\n\\frac{h^2}{12 D_e t} \\le \\varepsilon\n$$\nThis leads to the mesh-spacing constraint:\n$$\nh \\le \\sqrt{12 D_e t \\varepsilon}\n$$\n(Note: A similar analysis using a second-order accurate forward difference stencil at $x=0$ yields the same constraint, $\\frac{h^2}{12 D_e t} \\le \\varepsilon$, reinforcing the validity of this result.)\n\n**4. Calculation of the Number of Nodes N**\n\nThe mesh spacing $h$ is related to the number of nodes $N$ and the separator thickness $\\delta_s$ by $h = \\frac{\\delta_s}{N-1}$. Substituting this into the constraint:\n$$\n\\frac{\\delta_s}{N-1} \\le \\sqrt{12 D_e t \\varepsilon}\n$$\nWe rearrange to solve for $N$:\n$$\nN - 1 \\ge \\frac{\\delta_s}{\\sqrt{12 D_e t \\varepsilon}}\n$$\n$$\nN \\ge 1 + \\frac{\\delta_s}{\\sqrt{12 D_e t \\varepsilon}}\n$$\nNow, we substitute the given numerical values:\n-   $\\delta_{s} = 25 \\times 10^{-6} \\ \\mathrm{m}$\n-   $D_{e} = 1.2 \\times 10^{-10} \\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$\n-   $t = 1 \\ \\mathrm{s}$\n-   $\\varepsilon = 0.01$\n\nThe term in the denominator is:\n$$\n\\sqrt{12 D_e t \\varepsilon} = \\sqrt{12 \\times (1.2 \\times 10^{-10}) \\times 1 \\times 0.01} = \\sqrt{14.4 \\times 10^{-12}} = \\sqrt{14.4} \\times 10^{-6} \\ \\mathrm{m}\n$$\nSubstituting this into the inequality for $N$:\n$$\nN \\ge 1 + \\frac{25 \\times 10^{-6}}{\\sqrt{14.4} \\times 10^{-6}} = 1 + \\frac{25}{\\sqrt{14.4}}\n$$\nWe can write $\\sqrt{14.4} = \\sqrt{\\frac{144}{10}} = \\frac{12}{\\sqrt{10}}$.\n$$\nN \\ge 1 + \\frac{25}{12/\\sqrt{10}} = 1 + \\frac{25\\sqrt{10}}{12}\n$$\nNumerically, $\\sqrt{10} \\approx 3.16227766$.\n$$\nN \\ge 1 + \\frac{25 \\times 3.16227766}{12} \\approx 1 + \\frac{79.05694}{12} \\approx 1 + 6.588078\n$$\n$$\nN \\ge 7.588078\n$$\nSince $N$ must be an integer, the smallest integer value for $N$ that satisfies this condition is $8$.\n\nTo verify the approximation made earlier, we calculate $h=\\frac{25 \\times 10^{-6}}{8-1} \\approx 3.57 \\times 10^{-6} \\ \\mathrm{m}$. The term $\\frac{h^2}{2 D_e t} = \\frac{(3.57 \\times 10^{-6})^2}{2 \\times 1.2 \\times 10^{-10} \\times 1} \\approx \\frac{12.76 \\times 10^{-12}}{2.4 \\times 10^{-10}} \\approx 0.053$, which is indeed small compared to $1$, confirming the validity of the simplified constraint.\n\nThe smallest integer number of nodes required is $8$.",
            "answer": "$$\n\\boxed{8}\n$$"
        },
        {
            "introduction": "Once individual simulations are reliable, the focus shifts to the reproducibility and auditability of the entire workflow. This practice models a multi-stage software build process to demonstrate how content-addressable hashing can create a unique, verifiable fingerprint for a runtime environment. By separating build-time artifacts from the final runtime, you will implement a system where the resulting audit digest is sensitive to runtime changes but invariant to irrelevant build-time modifications. ",
            "id": "3893781",
            "problem": "You are tasked with formalizing, implementing, and verifying a content-addressable audit mechanism for an automated battery simulation workflow. Instead of operating an actual container engine, you will model a multi-stage build process and its runtime environment in purely mathematical terms and implement it as a program. The goal is to demonstrate that a canonical, content-based digest computed from the runtime environment uniquely identifies it for auditability, independent of build-time artifacts or ordering effects.\n\nFundamental base to use:\n- Cryptographic hash functions are deterministic maps from finite byte strings to fixed-length digests. Let $H$ denote Secure Hash Algorithm 256 ($\\operatorname{SHA256}$), which maps any byte string $x$ to a $256$-bit digest $H(x)$.\n- Collision resistance and preimage resistance are well-tested properties of $\\operatorname{SHA256}$ used in content-addressable storage and reproducibility. The uniqueness of the digest within feasible computational bounds arises from the content-addressable mapping and canonical serialization, not from absolute impossibility of collisions.\n- Canonical serialization is a deterministic function $\\mathcal{S}$ that maps composite data structures to a unique byte string given the content, independent of insertion order or non-semantic formatting.\n\nWorkflow abstraction:\n- The build stage produces artifacts and logs using a mapping $g$ that takes builder parameters $P$ (including development dependencies and model configuration) and returns a set of artifacts $A = g(P)$. Some of these artifacts are build-time only (e.g., logs) and must not enter the runtime environment.\n- The runtime stage $R$ consists of selected artifacts $S \\subseteq A$, environment metadata (such as programming language version), and runtime library versions. The audit digest is $D = H\\!\\left(\\mathcal{S}(R)\\right)$.\n- This models a multi-stage container build where only runtime artifacts are carried to the final image, and build-time dependencies are discarded to reduce attack surface and size while preserving reproducibility and auditability.\n\nYour program must:\n1. Implement a canonical serialization function $\\mathcal{S}$ that maps nested structures (dictionaries, lists, strings, integers, floating-point numbers, and raw bytes) to a canonical byte string. The canonicalization must sort dictionary keys lexicographically and include unambiguous type and length markers to avoid ambiguity in concatenation. Lists should be serialized with explicit type and positional markers. All strings must be encoded in $\\mathrm{UTF\\text{-}8}$. The output of $\\mathcal{S}$ must be deterministic for equal content, and invariant to the insertion order of dictionary keys.\n2. Implement a builder function that takes builder parameters $P$ containing:\n   - Development dependency versions encoded as a dictionary $D_b$ mapping names to versions.\n   - A deterministic battery simulation configuration (e.g., model name, electrode porosity $p$ in a dimensionless fraction, particle radius $r$ in meters, and temperature $T$ in kelvin, plus a set of Key Performance Indicator (KPI) names).\n   The builder must return artifacts $A$ including:\n   - A compiled runtime binary as raw bytes produced deterministically from $P$ (e.g., by concatenating content strings and encoding them).\n   - Build-time logs as raw bytes produced deterministically from $P$.\n   - The development dependency metadata $D_b$.\n3. Implement a runtime packaging function that constructs $R$ containing:\n   - The language version string $L$ (e.g., $\"3.12.0\"$).\n   - Runtime library versions encoded as a dictionary $V$ mapping names to versions.\n   - A runtime configuration dictionary $C$ (e.g., unit system setting).\n   - The compiled runtime binary selected from $A$.\n   Build-only artifacts (such as logs and development dependencies) must be excluded from $R$.\n4. Compute the digest $D = H\\!\\left(\\mathcal{S}(R)\\right)$.\n\nTest suite:\nUse the following five test cases. Each test case must yield a boolean result indicating whether the expected property holds.\n\n- Test case $1$ (happy path, equality): Construct two identical environments $R_1$ and $R_2$ from identical $P$ and runtime metadata; verify $H\\!\\left(\\mathcal{S}(R_1)\\right) = H\\!\\left(\\mathcal{S}(R_2)\\right)$; the result should be $ \\text{True} $.\n- Test case $2$ (build-only change, invariance): Modify $D_b$ in $P$ (e.g., change a development dependency version) while keeping the compiled runtime binary content identical; construct $R'$ and verify $H\\!\\left(\\mathcal{S}(R)\\right) = H\\!\\left(\\mathcal{S}(R')\\right)$; the result should be $ \\text{True} $. This demonstrates that build-time artifacts excluded from $R$ do not affect the audit digest.\n- Test case $3$ (runtime change, sensitivity): Change a runtime library version in $V$ while keeping all other content constant; verify $H\\!\\left(\\mathcal{S}(R)\\right) \\neq H\\!\\left(\\mathcal{S}(R'')\\right)$; the result should be $ \\text{True} $. This demonstrates that any change in the runtime environment affects the audit digest.\n- Test case $4$ (ordering invariance): Construct two environments $R_a$ and $R_b$ that are identical in content but built with different dictionary insertion orders; verify $H\\!\\left(\\mathcal{S}(R_a)\\right) = H\\!\\left(\\mathcal{S}(R_b)\\right)$; the result should be $ \\text{True} $. This demonstrates canonicalization invariance to ordering.\n- Test case $5$ (boundary, digest length): Construct a minimal runtime environment $R_{\\min}$ containing only the language version $L$; verify that the length of $H\\!\\left(\\mathcal{S}(R_{\\min})\\right)$ in bytes is equal to $32$; the result should be $ \\text{True} $. This checks the boundary condition of the hashing mechanism.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean corresponding to the five test cases in the order specified above. No physical units are required in the output because the results are booleans. All angles, if any, must be in radians, but this task does not involve angles. Percentages, if any appear, must be handled as decimal fractions, but this task does not involve percentages.",
            "solution": "The problem is valid. It presents a well-posed, scientifically grounded task in the domain of computational reproducibility and automated workflows. The problem is self-contained, its requirements are internally consistent, and it is formalizable into a solvable program. The provided constraints and test cases are clear and sufficient for developing and verifying a solution.\n\nThe solution requires formalizing and implementing a content-addressable audit mechanism. The core idea is that the identity of a computational environment should be uniquely and deterministically derived from its content, independent of incidental factors like build-time artifacts or data representation order. This is achieved by combining two key components: a canonical serialization function, denoted $\\mathcal{S}$, and a cryptographic hash function, $H$.\n\nFirst, we design the canonical serialization function, $\\mathcal{S}$. This function's purpose is to convert a complex, nested data structure representing the runtime environment, $R$, into a single, canonical byte string. Canonical means that any two environments with identical content must produce byte-for-byte identical serializations. The function is implemented recursively to handle various data types specified in the problem: integers (`int`), floating-point numbers (`float`), strings (`str`), raw bytes (`bytes`), lists (`list`), and dictionaries (`dict`).\n\nTo ensure the serialization is unambiguous, each serialized element is prefixed with a unique single-byte type marker (e.g., `b'i'` for integers, `b'd'` for dictionaries). This prevents, for example, the byte representation of an integer from being misinterpreted as part of a string. For variable-length types like strings, bytes, lists, and dictionaries, the type marker is followed by an $8$-byte integer specifying the length or number of items, which guards against concatenation ambiguities.\n\nThe most critical part of the canonicalization process is the handling of dictionaries. Since the internal ordering of key-value pairs in a dictionary is not guaranteed, a naive serialization could produce different outputs for dictionaries that are semantically identical. To solve this, $\\mathcal{S}$ first sorts the dictionary's keys lexicographically. It then iterates through the sorted keys, serializing each key followed by its corresponding value. This procedure guarantees that the byte output for a dictionary is dependent only on its content, not on the insertion order of its keys. This directly addresses the requirement for order invariance (Test Case $4$).\n\nFor other data types:\n- Integers are converted to a standard $8$-byte signed little-endian representation.\n- Floating-point numbers are converted to their standard IEEE $754$ double-precision $8$-byte representation, using the `numpy` library as permitted.\n- Strings are encoded to $\\mathrm{UTF\\text{-}8}$ bytes.\n\nThe second component is the cryptographic hash function, $H$, defined as $\\operatorname{SHA256}$. This function takes the canonical byte string produced by $\\mathcal{S}(R)$ and computes a $256$-bit ($32$-byte) digest, $D$. The fundamental properties of $\\operatorname{SHA256}$—determinism (same input always yields same output) and collision resistance (it is computationally infeasible to find two different inputs that produce the same output)—are central to this mechanism. Determinism ensures that identical environments yield identical digests (Test Case $1$), while collision resistance ensures that any modification to the environment results in a different digest (Test Case $3$).\n\nNext, we model the multi-stage build and runtime packaging process. A builder function, $g$, simulates the creation of software artifacts, $A$, from a set of builder parameters, $P$. The parameters $P$ include both the essential simulation configuration and extraneous build-time dependencies, $D_b$. A critical design choice, mandated by the problem to test for build-time invariance, is that the 'compiled binary' artifact depends *only* on the simulation configuration portion of $P$. Other artifacts, like build logs, may depend on the entirety of $P$.\n\nA runtime packaging function then constructs the final runtime environment, $R$. This function carefully selects only the runtime-essential components: the compiled binary from $A$, and runtime-specific metadata like language version $L$, runtime library versions $V$, and runtime configuration $C$. All build-time-only artifacts, such as build logs and development dependency metadata $D_b$, are explicitly excluded from $R$. This separation is fundamental to creating lean, reproducible, and auditable runtime environments.\n\nFinally, the five test cases are implemented to verify that the system behaves as expected:\n1.  **Equality:** Two environments, $R_1$ and $R_2$, are built from identical parameters. Since all functions ($g$, $\\mathcal{S}$, $H$) are deterministic, the resulting digests must be equal: $H(\\mathcal{S}(R_1)) = H(\\mathcal{S}(R_2))$. The test succeeds if this equality holds.\n2.  **Build-only Change Invariance:** A new set of builder parameters $P'$ is created by modifying a build-time dependency in $D_b$. The builder produces new artifacts $A'$, but since the compiled binary's content is independent of $D_b$ and all other changes are confined to build-time artifacts, the resulting runtime environment $R'$ is identical to the original $R$. Thus, their digests must be equal. The test succeeds if $H(\\mathcal{S}(R)) = H(\\mathcal{S}(R'))$.\n3.  **Runtime Change Sensitivity:** The runtime environment is modified by changing a runtime library version in $V$, creating $R''$. This change is part of the content of $R''$. The canonical serialization $\\mathcal{S}(R'')$ will differ from $\\mathcal{S}(R)$, and due to the properties of $\\operatorname{SHA256}$, the digests must also differ. The test succeeds if $H(\\mathcal{S}(R)) \\neq H(\\mathcal{S}(R''))$.\n4.  **Ordering Invariance:** Two environments, $R_a$ and $R_b$, are constructed using dictionaries with identical content but different key insertion orders. The lexicographical sorting within $\\mathcal{S}$ ensures that $\\mathcal{S}(R_a) = \\mathcal{S}(R_b)$, and therefore their digests must be identical. The test succeeds if this equality holds, proving the serialization is truly canonical.\n5.  **Digest Length:** A minimal environment $R_{\\min}$ is created and hashed. The test verifies that the length of the resulting $\\operatorname{SHA256}$ digest is exactly $32$ bytes ($256$ bits), as per the algorithm's specification. This serves as a basic sanity check of the hashing implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport hashlib\nimport numpy as np\n\ndef canonical_serialize(obj: object) -> bytes:\n    \"\"\"\n    Recursively serializes a supported Python object into a canonical byte string.\n    The serialization format is unambiguous and deterministic.\n\n    Format:\n    - Type Marker (1 byte)\n    - Length/Count (8 bytes, for variable-size types)\n    - Payload (variable size)\n    \"\"\"\n    if obj is None:\n        # Type 'n' for None\n        return b'n'\n    elif isinstance(obj, int):\n        # Type 'i' for integer. Payload is 8-byte signed little-endian.\n        return b'i' + obj.to_bytes(8, 'little', signed=True)\n    elif isinstance(obj, float):\n        # Type 'f' for float. Payload is IEEE 754 double precision (8 bytes).\n        # numpy.float64 is the standard double-precision float.\n        return b'f' + np.float64(obj).tobytes()\n    elif isinstance(obj, str):\n        # Type 's' for string. Encoded as UTF-8.\n        # Length of encoded bytes is prefixed.\n        utf8_bytes = obj.encode('utf-8')\n        return b's' + len(utf8_bytes).to_bytes(8, 'little') + utf8_bytes\n    elif isinstance(obj, bytes):\n        # Type 'b' for bytes. Length of bytes is prefixed.\n        return b'b' + len(obj).to_bytes(8, 'little') + obj\n    elif isinstance(obj, list):\n        # Type 'l' for list. Number of items is prefixed.\n        # Items are recursively serialized and concatenated.\n        items_bytes = [canonical_serialize(item) for item in obj]\n        return b'l' + len(items_bytes).to_bytes(8, 'little') + b''.join(items_bytes)\n    elif isinstance(obj, dict):\n        # Type 'd' for dictionary. Number of key-value pairs is prefixed.\n        # To ensure canonical form, keys are sorted lexicographically.\n        # Each pair is serialized as key then value.\n        items_bytes = []\n        try:\n            sorted_keys = sorted(obj.keys())\n        except TypeError:\n            raise TypeError(\"Dictionary keys must be of a sortable type (e.g., string) for canonical serialization.\")\n\n        for key in sorted_keys:\n            items_bytes.append(canonical_serialize(key))\n            items_bytes.append(canonical_serialize(obj[key]))\n        return b'd' + len(obj).to_bytes(8, 'little') + b''.join(items_bytes)\n    else:\n        raise TypeError(f\"Unsupported type for serialization: {type(obj)}\")\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the content-addressable audit mechanism.\n    \"\"\"\n    \n    # 1. Implement a builder function `g(P) -> A`\n    def builder(builder_params: dict) -> dict:\n        \"\"\"\n        Simulates a build process, creating artifacts from builder parameters.\n        The compiled binary content deliberately depends only on `sim_config`.\n        \"\"\"\n        # The runtime binary's content is derived ONLY from the simulation config.\n        # This is critical for Test Case 2 (build-only change invariance).\n        binary_content = canonical_serialize(builder_params['sim_config'])\n        \n        # Build logs, however, can depend on all builder parameters.\n        log_content = canonical_serialize(builder_params)\n\n        artifacts = {\n            'compiled_binary': binary_content,\n            'build_logs': log_content,\n            'dev_dependencies': builder_params['dev_dependencies']\n        }\n        return artifacts\n\n    # 2. Implement a runtime packaging function `package(A, M) -> R`\n    def package_runtime(artifacts: dict, runtime_metadata: dict) -> dict:\n        \"\"\"\n        Constructs the runtime environment `R` by selecting only\n        runtime-essential artifacts and metadata. Build-time-only components\n        are excluded.\n        \"\"\"\n        runtime_env = {\n            'language_version': runtime_metadata['language_version'],\n            'runtime_libraries': runtime_metadata['runtime_libraries'],\n            'runtime_config': runtime_metadata['runtime_config'],\n            'binary': artifacts['compiled_binary']  # The only artifact from the build.\n        }\n        return runtime_env\n\n    # 3. Define the hash function H = SHA256\n    H = lambda data: hashlib.sha256(data).digest()\n\n    results = []\n\n    # --- Test Suite Setup ---\n\n    # Common builder parameters for a baseline build\n    base_builder_params = {\n        'dev_dependencies': {\n            'compiler': 'gcc-11.1',\n            'linter': 'pylint-2.15.0'\n        },\n        'sim_config': {\n            'model_name': 'DFN',\n            'porosity': 0.3,\n            'radius': 1.0e-5,\n            'temperature': 298.15,\n            'kpis': ['capacity', 'efficiency']\n        }\n    }\n\n    # Common runtime metadata for a baseline environment\n    base_runtime_metadata = {\n        'language_version': '3.12.0',\n        'runtime_libraries': {\n            'numpy': '1.23.5',\n            'scipy': '1.11.4'\n        },\n        'runtime_config': {\n            'unit_system': 'SI'\n        }\n    }\n\n    # --- Test Case 1 (happy path, equality) ---\n    artifacts1 = builder(base_builder_params)\n    runtime_env1 = package_runtime(artifacts1, base_runtime_metadata)\n    digest1 = H(canonical_serialize(runtime_env1))\n\n    artifacts2 = builder(base_builder_params)\n    runtime_env2 = package_runtime(artifacts2, base_runtime_metadata)\n    digest2 = H(canonical_serialize(runtime_env2))\n    \n    results.append(digest1 == digest2)\n\n    # --- Test Case 2 (build-only change, invariance) ---\n    builder_params_prime = {\n        'dev_dependencies': {'compiler': 'gcc-11.2', 'linter': 'pylint-2.15.0'}, # Changed\n        'sim_config': base_builder_params['sim_config']\n    }\n    \n    artifacts_prime = builder(builder_params_prime)\n    runtime_env_prime = package_runtime(artifacts_prime, base_runtime_metadata)\n    digest_prime = H(canonical_serialize(runtime_env_prime))\n\n    results.append(digest1 == digest_prime)\n\n    # --- Test Case 3 (runtime change, sensitivity) ---\n    runtime_metadata_doubleprime = {\n        'language_version': '3.12.0',\n        'runtime_libraries': {'numpy': '1.23.6', 'scipy': '1.11.4'}, # Changed\n        'runtime_config': {'unit_system': 'SI'}\n    }\n\n    runtime_env_doubleprime = package_runtime(artifacts1, runtime_metadata_doubleprime)\n    digest_doubleprime = H(canonical_serialize(runtime_env_doubleprime))\n\n    results.append(digest1 != digest_doubleprime)\n\n    # --- Test Case 4 (ordering invariance) ---\n    # Construct metadata with different dictionary key insertion order\n    runtime_metadata_b = {\n        'language_version': '3.12.0',\n        'runtime_libraries': {\n            'scipy': '1.11.4', # Order swapped\n            'numpy': '1.23.5'\n        },\n        'runtime_config': {\n            'unit_system': 'SI'\n        }\n    }\n    \n    runtime_env_b = package_runtime(artifacts1, runtime_metadata_b)\n    digest_b = H(canonical_serialize(runtime_env_b))\n    \n    results.append(digest1 == digest_b)\n\n    # --- Test Case 5 (boundary, digest length) ---\n    runtime_env_min = {'language_version': '3.12.0'}\n    digest_min = H(canonical_serialize(runtime_env_min))\n    \n    results.append(len(digest_min) == 32)\n    \n    # Final print statement in the exact required format.\n    # The str() of a Python bool is 'True' or 'False' (capitalized).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of many automated workflows is to accelerate design discovery. This final exercise introduces you to Active Learning, a powerful technique for intelligently guiding a design-of-experiments loop. You will implement the Expected Improvement ($EI$) acquisition function, a cornerstone of Bayesian Optimization, to select the most promising candidate designs for the next round of simulations, effectively balancing the exploration of new designs with the exploitation of known high-performing regions. ",
            "id": "3893813",
            "problem": "You are automating a design-of-experiments loop for lithium-ion battery cells in which simulated cycle life is the key performance indicator (KPI). In each iteration, an Active Learning (AL) policy must rank candidate designs using an acquisition score derived from Expected Improvement and select a subset to simulate next. The pipeline assumes a Gaussian Process (GP) surrogate model that provides, for each candidate design, a posterior predictive mean and variance of cycle life. The simulated cycle life is a physical quantity measured in cycles, which must be treated as a count. The acquisition scores must be expressed in cycles. The selection indices are unitless.\n\nStarting from the following fundamentals:\n- The GP predictive distribution at a candidate design is Gaussian with predictive mean $\\mu$ and predictive variance $\\sigma^2$ for the KPI cycle life $Y$.\n- The improvement random variable is defined as $I = \\max(0, Y - y^\\star)$, where $y^\\star$ is the current best observed cycle life (in cycles).\n- The standard normal probability density function is $\\phi(z)$ and the standard normal cumulative distribution function is $\\Phi(z)$.\n\nYour tasks are:\n1. Derive, from first principles and the above definitions, the Expected Improvement acquisition for cycle life and express it in cycles. Handle the boundary case $\\sigma = 0$ rigorously.\n2. Define a sampling policy that balances exploitation and exploration by a convex combination of the Expected Improvement and the predictive standard deviation. Let the policy be parameterized by $\\lambda \\in [0,1]$ and a nonnegative scale factor $\\kappa$, and select the top $B$ candidates by the resulting acquisition score. Break ties by selecting the smallest indices.\n3. Implement a self-contained program that:\n   - Computes the Expected Improvement for each candidate and the balanced acquisition score in cycles.\n   - Selects the indices of the top $B$ candidates under the policy.\n   - Produces the final output in the exact format specified below.\n\nUse the following test suite. For each test case, the inputs are:\n- A list of predictive means $\\mu$ in cycles.\n- A list of predictive standard deviations $\\sigma$ in cycles.\n- The current best observed cycle life $y^\\star$ in cycles.\n- The exploration weight $\\lambda$ (dimensionless).\n- The scale factor $\\kappa$ (dimensionless).\n- The simulation budget $B$ (integer).\n\nCompute and report acquisition scores in cycles, rounded to six decimal places. The angle unit is not applicable. Do not use percentages.\n\nTest Suite:\n- Case $1$ (happy path):\n  - $\\mu = [800, 600, 900, 700]$ cycles\n  - $\\sigma = [50, 120, 30, 80]$ cycles\n  - $y^\\star = 750$ cycles\n  - $\\lambda = 0.3$\n  - $\\kappa = 1.0$\n  - $B = 2$\n- Case $2$ (boundary with zero variance):\n  - $\\mu = [760, 740, 755]$ cycles\n  - $\\sigma = [0, 0, 0]$ cycles\n  - $y^\\star = 755$ cycles\n  - $\\lambda = 0.5$\n  - $\\kappa = 1.0$\n  - $B = 1$\n- Case $3$ (exploration-dominant edge case):\n  - $\\mu = [700, 730, 720]$ cycles\n  - $\\sigma = [150, 200, 10]$ cycles\n  - $y^\\star = 750$ cycles\n  - $\\lambda = 0.7$\n  - $\\kappa = 1.0$\n  - $B = 2$\n- Case $4$ (exploitation-dominant case):\n  - $\\mu = [1000, 760, 755]$ cycles\n  - $\\sigma = [5, 100, 100]$ cycles\n  - $y^\\star = 800$ cycles\n  - $\\lambda = 0.1$\n  - $\\kappa = 1.0$\n  - $B = 1$\n\nOutput specification:\n- For each test case, output a pair consisting of:\n  - The list of balanced acquisition scores in cycles for all candidates, rounded to six decimal places.\n  - The list of selected indices (top $B$ under tie-breaking by smallest index).\n- Aggregate the per-case results into a single outer list. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For example, for two cases the format is $[[\\text{scores}_1,\\text{indices}_1],[\\text{scores}_2,\\text{indices}_2]]$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of Bayesian optimization for engineering design, is mathematically well-posed, and all necessary data and constraints are provided. We proceed with a step-by-step derivation and algorithmic design.\n\nThe first task is to derive the analytical expression for the Expected Improvement (EI) acquisition function from first principles. The improvement, $I$, over the current best observed cycle life, $y^\\star$, is defined for a candidate design whose cycle life $Y$ is a random variable. The improvement is given by $I = \\max(0, Y - y^\\star)$. The cycle life $Y$ is modeled by a Gaussian Process, resulting in a predictive distribution for $Y$ that is a normal distribution, $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, where $\\mu$ is the predictive mean and $\\sigma^2$ is the predictive variance.\n\nThe Expected Improvement, $E[I]$, is the expectation of the improvement random variable $I$ with respect to the predictive distribution of $Y$. Let $f_Y(y)$ be the probability density function (PDF) of $Y$. The expectation is calculated by the integral:\n$$E[I] = E[\\max(0, Y - y^\\star)] = \\int_{-\\infty}^{\\infty} \\max(0, y - y^\\star) f_Y(y) \\, dy$$\nThe term $\\max(0, y - y^\\star)$ is non-zero only for $y > y^\\star$. Therefore, the integral is non-zero only over the domain $[y^\\star, \\infty)$:\n$$E[I] = \\int_{y^\\star}^{\\infty} (y - y^\\star) f_Y(y) \\, dy$$\nwhere $f_Y(y) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$. We can split the integral into two parts:\n$$E[I] = \\int_{y^\\star}^{\\infty} y f_Y(y) \\, dy - y^\\star \\int_{y^\\star}^{\\infty} f_Y(y) \\, dy$$\nThe second term is $y^\\star$ multiplied by the probability that $Y$ exceeds $y^\\star$, i.e., $y^\\star P(Y > y^\\star)$. To evaluate these terms, we perform a change of variables to the standard normal distribution. Let $z = \\frac{y - \\mu}{\\sigma}$, which implies $y = \\mu + \\sigma z$ and $dy = \\sigma dz$. The random variable $Z = \\frac{Y - \\mu}{\\sigma}$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$, with PDF $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$ and CDF $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(u) du$.\n\nThe integration limit $y = y^\\star$ becomes $z = \\frac{y^\\star - \\mu}{\\sigma}$. The integral becomes:\n$$E[I] = \\int_{\\frac{y^\\star - \\mu}{\\sigma}}^{\\infty} (\\mu + \\sigma z - y^\\star) \\phi(z) \\, dz$$\n$$E[I] = \\int_{\\frac{y^\\star - \\mu}{\\sigma}}^{\\infty} (\\mu - y^\\star) \\phi(z) \\, dz + \\int_{\\frac{y^\\star - \\mu}{\\sigma}}^{\\infty} \\sigma z \\phi(z) \\, dz$$\n$$E[I] = (\\mu - y^\\star) \\int_{\\frac{y^\\star - \\mu}{\\sigma}}^{\\infty} \\phi(z) \\, dz + \\sigma \\int_{\\frac{y^\\star - \\mu}{\\sigma}}^{\\infty} z \\phi(z) \\, dz$$\nThe first integral is the probability $P(Z > \\frac{y^\\star - \\mu}{\\sigma}) = 1 - \\Phi(\\frac{y^\\star - \\mu}{\\sigma}) = \\Phi(\\frac{\\mu - y^\\star}{\\sigma})$.\nThe second integral can be solved directly: $\\int_{a}^{\\infty} z \\phi(z) dz = \\int_{a}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz$. Using the substitution $u = -z^2/2$, we find this integral is equal to $\\phi(a)$.\nThus, with $a = \\frac{y^\\star - \\mu}{\\sigma}$, the second integral evaluates to $\\sigma \\phi(\\frac{y^\\star - \\mu}{\\sigma})$. Since the standard normal PDF $\\phi(z)$ is an even function, $\\phi(\\frac{y^\\star - \\mu}{\\sigma}) = \\phi(\\frac{\\mu - y^\\star}{\\sigma})$.\n\nCombining the terms, the Expected Improvement is:\n$$E[I] = (\\mu - y^\\star) \\Phi\\left(\\frac{\\mu - y^\\star}{\\sigma}\\right) + \\sigma \\phi\\left(\\frac{\\mu - y^\\star}{\\sigma}\\right)$$\nA unit analysis confirms the correctness of the expression. The term $(\\mu - y^\\star)$ has units of cycles. The argument of the CDF $\\Phi$ and PDF $\\phi$, $z = (\\mu - y^\\star) / \\sigma$, is dimensionless. Both $\\Phi(z)$ and $\\phi(z)$ are dimensionless functions. Therefore, the first term, $(\\mu - y^\\star)\\Phi(z)$, has units of cycles. The second term, $\\sigma\\phi(z)$, also has units of cycles, as $\\sigma$ is in cycles. The sum, $E[I]$, is correctly expressed in units of cycles.\n\nNext, we must rigorously handle the boundary case where the predictive variance is zero, i.e., $\\sigma = 0$. In this scenario, the Gaussian distribution collapses to a Dirac delta function centered at $\\mu$, meaning $Y = \\mu$ with probability $1$. The improvement is no longer a random variable but a deterministic quantity: $I = \\max(0, \\mu - y^\\star)$. Consequently, the expectation is simply $E[I] = \\max(0, \\mu - y^\\star)$. This can also be derived by taking the limit of the EI formula as $\\sigma \\to 0^+$. Let $z = \\frac{\\mu - y^\\star}{\\sigma}$.\n- If $\\mu > y^\\star$, then $z \\to \\infty$. As $z \\to \\infty$, $\\Phi(z) \\to 1$ and $\\phi(z) \\to 0$. $E[I] \\to (\\mu - y^\\star) \\cdot 1 + \\sigma \\cdot 0 = \\mu - y^\\star$.\n- If $\\mu  y^\\star$, then $z \\to -\\infty$. As $z \\to -\\infty$, $\\Phi(z) \\to 0$ and $\\phi(z) \\to 0$. $E[I] \\to (\\mu - y^\\star) \\cdot 0 + \\sigma \\cdot 0 = 0$.\n- If $\\mu = y^\\star$, then $z=0$ for $\\sigma>0$, giving $E[I] = 0 \\cdot \\Phi(0) + \\sigma \\phi(0) = \\sigma/\\sqrt{2\\pi}$, which goes to $0$ as $\\sigma \\to 0$.\nIn all cases, the limit is $\\max(0, \\mu - y^\\star)$. Thus, the complete definition for EI is:\n$$EI(\\mu, \\sigma, y^\\star) = \\begin{cases} (\\mu - y^\\star)\\Phi\\left(\\frac{\\mu - y^\\star}{\\sigma}\\right) + \\sigma\\phi\\left(\\frac{\\mu - y^\\star}{\\sigma}\\right)  \\text{if } \\sigma  0 \\\\ \\max(0, \\mu - y^\\star)  \\text{if } \\sigma = 0 \\end{cases}$$\n\nThe second task is to define the sampling policy. The policy constructs a balanced acquisition score, $A$, by taking a convex combination of the Expected Improvement (exploitation) and the predictive standard deviation $\\sigma$ (exploration). The score is parameterized by a weight $\\lambda \\in [0, 1]$ and a non-negative scale factor $\\kappa$. The formula for the acquisition score for a candidate $i$ with predictive mean $\\mu_i$ and standard deviation $\\sigma_i$ is:\n$$A_i = (1 - \\lambda) \\cdot EI(\\mu_i, \\sigma_i, y^\\star) + \\lambda \\kappa \\sigma_i$$\nThis score is in cycles, as both $EI$ and $\\sigma_i$ are in cycles, while $\\lambda$ and $\\kappa$ are dimensionless.\n\nThe third task is implementation. The algorithm first computes the acquisition score $A_i$ for each candidate design. Then, it selects the top $B$ candidates with the highest scores. To ensure a unique selection, ties in scores are broken by choosing the candidate with the smallest original index. This is achieved by sorting the candidates based on a primary key of the acquisition score (in descending order) and a secondary key of the original index (in ascending order). The program must process the provided test suite and produce output in the specified format. The implementation will use `numpy` for efficient vectorized calculations and `scipy.stats.norm` for the CDF ($\\Phi$) and PDF ($\\phi$) of the standard normal distribution.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport json\n\ndef solve():\n    \"\"\"\n    Computes acquisition scores and selects candidate designs for a battery\n    simulation workflow based on Expected Improvement and a balanced policy.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {'mu': [800, 600, 900, 700], 'sigma': [50, 120, 30, 80], 'y_star': 750, 'lambda_': 0.3, 'kappa': 1.0, 'B': 2},\n        # Case 2 (boundary with zero variance)\n        {'mu': [760, 740, 755], 'sigma': [0, 0, 0], 'y_star': 755, 'lambda_': 0.5, 'kappa': 1.0, 'B': 1},\n        # Case 3 (exploration-dominant edge case)\n        {'mu': [700, 730, 720], 'sigma': [150, 200, 10], 'y_star': 750, 'lambda_': 0.7, 'kappa': 1.0, 'B': 2},\n        # Case 4 (exploitation-dominant case)\n        {'mu': [1000, 760, 755], 'sigma': [5, 100, 100], 'y_star': 800, 'lambda_': 0.1, 'kappa': 1.0, 'B': 1},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        mu = np.array(case['mu'], dtype=float)\n        sigma = np.array(case['sigma'], dtype=float)\n        y_star = float(case['y_star'])\n        lambda_ = float(case['lambda_'])\n        kappa = float(case['kappa'])\n        B = int(case['B'])\n\n        # Initialize Expected Improvement array\n        ei = np.zeros_like(mu)\n\n        # Handle sigma  0 case\n        mask_pos_sigma = sigma  1e-9 # Use a small epsilon for floating point stability\n        if np.any(mask_pos_sigma):\n            mu_pos = mu[mask_pos_sigma]\n            sigma_pos = sigma[mask_pos_sigma]\n            \n            improvement = mu_pos - y_star\n            z = improvement / sigma_pos\n            \n            ei_pos = improvement * norm.cdf(z) + sigma_pos * norm.pdf(z)\n            ei[mask_pos_sigma] = ei_pos\n            \n        # Handle sigma = 0 case\n        mask_zero_sigma = ~mask_pos_sigma\n        if np.any(mask_zero_sigma):\n            mu_zero = mu[mask_zero_sigma]\n            \n            ei_zero = np.maximum(0.0, mu_zero - y_star)\n            ei[mask_zero_sigma] = ei_zero\n\n        # Calculate the balanced acquisition score\n        acquisition_scores = (1 - lambda_) * ei + lambda_ * kappa * sigma\n        \n        # Round scores to six decimal places for the output\n        rounded_scores = np.round(acquisition_scores, 6).tolist()\n        \n        # Select top B candidates with tie-breaking\n        # Create a list of (score, original_index) tuples\n        indexed_scores = list(enumerate(acquisition_scores))\n        \n        # Sort by score (descending) and then by index (ascending) to break ties\n        sorted_candidates = sorted(indexed_scores, key=lambda x: (-x[1], x[0]))\n        \n        # Extract the indices of the top B candidates\n        top_indices = [idx for idx, score in sorted_candidates[:B]]\n        \n        all_results.append([rounded_scores, top_indices])\n\n    # Convert the list of lists to the specified string format with no spaces\n    # json.dumps provides a compact representation.\n    output_str = json.dumps(all_results, separators=(',', ':'))\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}