{
    "hands_on_practices": [
        {
            "introduction": "Before embarking on the construction of a complex surrogate model, a fundamental question must be addressed: can we uniquely determine the model's parameters from the available data? This exercise explores the concept of model identifiability, particularly in the common scenario where the number of polynomial features ($m$) exceeds the number of simulation runs ($n$). By analyzing this underdetermined system, you will understand why ordinary least-squares fitting fails and why techniques like regularization or strategic data acquisition are not just helpful, but mathematically necessary. ",
            "id": "3941886",
            "problem": "In automated battery design and simulation, suppose one seeks a surrogate model that maps a vector of electrode design variables $\\mathbf{x} \\in \\mathbb{R}^{p}$ (e.g., thickness, porosity, particle size, binder fraction) to a response such as discharge capacity $C$. The surrogate is specified as a polynomial regression of total degree $3$, constructed over a polynomial feature map $\\phi(\\mathbf{x}) \\in \\mathbb{R}^{m}$ with $m=35$ basis functions (including interaction and cubic terms), yielding a linear-in-parameters model $C \\approx \\phi(\\mathbf{x})^{\\top}\\boldsymbol{\\beta}$ with parameter vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m}$. A synthetic dataset of $n=30$ battery designs is collected via automated simulation, yielding the design matrix $X \\in \\mathbb{R}^{n \\times m}$ with $n=30$ and $m=35$, and responses $\\mathbf{y} \\in \\mathbb{R}^{n}$.\n\nBased on the definition of identifiability for linear models and the properties of least-squares estimation, determine whether the parameter vector $\\boldsymbol{\\beta}$ is identifiable from $(X,\\mathbf{y})$ in this setup, and select the most appropriate remedy to obtain a unique and physically meaningful surrogate within the constraints of automated battery design and simulation.\n\nChoose the single best option:\n\nA. The model is not identifiable because $m>n$ implies $\\operatorname{rank}(X) \\leq n  m$, so $\\boldsymbol{\\beta}$ is not uniquely determined by $(X,\\mathbf{y})$. A remedy is to impose regularization (e.g., ridge with penalty parameter $\\lambda>0$) or reduce $m$ via physics-informed feature selection, and/or increase $n$ through design-of-experiments to achieve $\\operatorname{rank}(X)=m$.\n\nB. The model becomes identifiable by including an intercept term, because adding a constant column stabilizes the regression; therefore simply augmenting $\\phi(\\mathbf{x})$ with $1$ resolves the issue without changing $n$.\n\nC. The model is identifiable because a degree-$3$ polynomial is sufficiently flexible to capture battery physics, so one can proceed with ordinary least squares without modification.\n\nD. The model is identifiable if the measurement noise is Gaussian, because Gaussian noise implies maximum likelihood coincides with least squares, so one can fit without regularization.\n\nE. The model is not identifiable only because of multicollinearity among polynomial features; orthogonalizing the basis (e.g., via Gram–Schmidt) alone will ensure identifiability even when $m>n$.",
            "solution": "The problem statement describes a scenario in which a surrogate model is to be learned for an automated battery design process. The model is a linear regression on a set of polynomial features. We must first analyze the conditions for the unique identifiability of the model parameters.\n\nA linear model is given by the equation $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the vector of observed responses, $X \\in \\mathbb{R}^{n \\times m}$ is the design matrix (whose rows are the feature vectors $\\phi(\\mathbf{x}_i)^\\top$), $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m}$ is the vector of parameters to be estimated, and $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{n}$ is the vector of unobserved errors.\n\nThe parameter vector $\\boldsymbol{\\beta}$ is considered identifiable if it can be uniquely determined from the data $(X, \\mathbf{y})$. In the context of Ordinary Least Squares (OLS), the estimate $\\hat{\\boldsymbol{\\beta}}$ is the vector that minimizes the sum of squared residuals, $\\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|_2^2$. The solution to this minimization problem is given by the normal equations:\n$$X^{\\top}X\\boldsymbol{\\beta} = X^{\\top}\\mathbf{y}$$\nFor a unique solution for $\\boldsymbol{\\beta}$ to exist, the matrix $X^{\\top}X$ must be invertible. The matrix $X^{\\top}X$ is an $m \\times m$ square matrix. A square matrix is invertible if and only if it has full rank, i.e., $\\operatorname{rank}(X^{\\top}X) = m$. This condition is met if and only if the design matrix $X$ has full column rank, meaning $\\operatorname{rank}(X) = m$.\n\nThe rank of any matrix cannot exceed the minimum of its number of rows and its number of columns. For the design matrix $X \\in \\mathbb{R}^{n \\times m}$, we have:\n$$\\operatorname{rank}(X) \\leq \\min(n, m)$$\nIn the problem, we are given:\n- Number of observations (battery designs): $n=30$\n- Number of model parameters (basis functions): $m=35$\n\nSince $n  m$, the maximum possible rank of the design matrix $X$ is limited by the number of rows:\n$$\\operatorname{rank}(X) \\leq \\min(30, 35) = 30$$\nThis implies that $\\operatorname{rank}(X) \\leq 30$, which is strictly less than the number of parameters, $m=35$. Therefore, $X$ does not have full column rank. Consequently, the matrix $X^{\\top}X$ is singular (not invertible), and the normal equations $X^{\\top}X\\boldsymbol{\\beta} = X^{\\top}\\mathbf{y}$ do not have a unique solution. There exists an infinite number of vectors $\\boldsymbol{\\beta}$ that satisfy the equations, meaning the parameter vector is not identifiable from the data using OLS. This is a classic \"underdetermined\" or \"high-dimensional\" problem where there are more features than observations.\n\nTo obtain a unique and stable solution in such a scenario, one must modify the problem. Standard approaches include:\n1.  **Regularization:** Add a penalty term to the least-squares objective function. For example, ridge regression (Tikhonov regularization) minimizes $\\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2$ for some penalty parameter $\\lambda > 0$. The solution is $\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}\\mathbf{y}$. For any $\\lambda > 0$, the matrix $(X^{\\top}X + \\lambda I)$ is invertible, guaranteeing a unique solution. Other methods like Lasso (L1 regularization) also yield unique solutions and can perform feature selection.\n2.  **Feature Reduction:** Decrease the number of parameters $m$ so that $m \\leq n$. This can be done by selecting a subset of the original $35$ features based on prior knowledge (e.g., physics-informed feature selection) or automated techniques.\n3.  **Data Augmentation:** Increase the number of observations $n$ so that $n \\geq m$. This involves running more simulations, often guided by a Design of Experiments (DoE) methodology to ensure the new data points are maximally informative and help ensure the resulting design matrix $X$ has full column rank.\n\nNow we evaluate each option:\n\n**A. The model is not identifiable because $m>n$ implies $\\operatorname{rank}(X) \\leq n  m$, so $\\boldsymbol{\\beta}$ is not uniquely determined by $(X,\\mathbf{y})$. A remedy is to impose regularization (e.g., ridge with penalty parameter $\\lambda>0$) or reduce $m$ via physics-informed feature selection, and/or increase $n$ through design-of-experiments to achieve $\\operatorname{rank}(X)=m$.**\nThis statement accurately diagnoses the problem based on the relationship between $n=30$ and $m=35$. It correctly concludes that $m>n$ leads to $\\operatorname{rank}(X)  m$ and thus non-identifiability. It then correctly lists the three standard and appropriate remedies for this situation: regularization, reducing the number of features $m$, and increasing the number of observations $n$. The proposed remedies are both mathematically sound and contextually appropriate for automated design problems.\n**Verdict:** Correct.\n\n**B. The model becomes identifiable by including an intercept term, because adding a constant column stabilizes the regression; therefore simply augmenting $\\phi(\\mathbf{x})$ with $1$ resolves the issue without changing $n$.**\nThe problem states there are $m=35$ basis functions, which typically include the intercept term (a polynomial of degree $0$). If it were not included, adding it would increase the number of parameters to $m=36$. This would make the problem even more underdetermined ($n=30$, $m=36$), worsening the identifiability issue, not resolving it. Adding a column cannot increase the rank of the matrix beyond $n=30$.\n**Verdict:** Incorrect.\n\n**C. The model is identifiable because a degree-$3$ polynomial is sufficiently flexible to capture battery physics, so one can proceed with ordinary least squares without modification.**\nThe physical relevance or flexibility of the chosen functional form (a degree-$3$ polynomial) is unrelated to the mathematical problem of parameter identifiability. Identifiability is determined by the properties of the design matrix $X$ and the relationship between $n$ and $m$. No matter how appropriate the model is from a physics perspective, its $m=35$ parameters cannot be uniquely estimated from only $n=30$ observations using OLS.\n**Verdict:** Incorrect.\n\n**D. The model is identifiable if the measurement noise is Gaussian, because Gaussian noise implies maximum likelihood coincides with least squares, so one can fit without regularization.**\nThe assumption of Gaussian noise on the errors provides a statistical justification for using the least-squares cost function, as it becomes equivalent to maximizing the likelihood of the data. However, this assumption does not change the underlying linear algebra. If the matrix $X^{\\top}X$ is singular because $m>n$, the likelihood function will not have a unique maximum. Instead, there will be a subspace of parameter vectors $\\boldsymbol{\\beta}$ that all achieve the same maximum likelihood, so the estimator is still not unique.\n**Verdict:** Incorrect.\n\n**E. The model is not identifiable only because of multicollinearity among polynomial features; orthogonalizing the basis (e.g., via Gram–Schmidt) alone will ensure identifiability even when $m>n$.**\nWhen $m>n$, the $m$ columns of $X$ lie in an $n$-dimensional space (or a subspace thereof), so they are guaranteed to be linearly dependent. This is an extreme form of multicollinearity. Orthogonalization via a process like Gram-Schmidt can produce an orthogonal basis for the column space of $X$, but this space has a dimension of at most $n=30$. It is mathematically impossible to find $m=35$ orthogonal vectors in a $30$-dimensional space. Orthogonalization does not solve the fundamental problem that the number of parameters exceeds the number of observations.\n**Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Even when you have more data points than model parameters ($n > p$), fitting a polynomial surrogate can fail due to severe numerical instability. This practice positions you as a detective, tasked with diagnosing the root causes of an ill-conditioned regression problem by interpreting key metrics like the condition number and Variance Inflation Factors (VIFs). You will learn to distinguish between issues of poor input scaling and the more subtle problem of multicollinearity, which is often inherent in polynomial feature sets, and propose a comprehensive suite of remedies. ",
            "id": "3942011",
            "problem": "You are building a multivariate polynomial regression surrogate for a lithium-ion cell’s discharge resistance as a function of design and material variables in an automated battery design and simulation workflow. The surrogate is linear-in-parameters with a basis that includes an intercept and all monomials up to total degree $2$ in the inputs $x_1, x_2, x_3, x_4$, where $x_1$ is current-collector thickness (in micrometers), $x_2$ is electrode porosity (dimensionless), $x_3$ is electrolyte ionic conductivity (in siemens per meter), and $x_4$ is specific surface area (in inverse meters). You have $n$ simulated experiments with $n \\gg p$, where $p$ is the number of columns in the design matrix $X \\in \\mathbb{R}^{n \\times p}$. The following observations are reported from your fitting pipeline:\n\n- The $2$-norm condition number of $X^{\\top}X$ is approximately $1.2 \\times 10^{16}$.\n- After column-wise centering to zero mean and scaling each column to unit sample standard deviation (z-scoring), the $2$-norm condition number of the new Gram matrix drops to approximately $1.8 \\times 10^{3}$.\n- The empirical correlation matrix of the z-scored columns shows $|\\rho(x_1, x_1^2)| \\approx 0.98$, $|\\rho(x_1, x_1 x_4)| \\approx 0.97$, and $|\\rho(x_4, x_4^2)| \\approx 0.99$.\n- The variance inflation factor for the $x_1$ column, computed by regressing $x_1$ on all other columns of the z-scored design matrix, is about $220$; the intercept’s variance inflation factor is about $1$.\n- Repeating some design points to obtain replicates (without changing the set of unique design points) scales $X^{\\top}X$ by an integer factor but does not materially change its condition number.\n\nAssume the data are generated by a physically smooth mapping from inputs to the true response and that numerical precision is standard double precision. Based on first principles of linear regression and numerical linear algebra, determine whether the near-singularity of $X^{\\top}X$ arises primarily from multicollinearity, poor scaling, both, or neither, and choose the most appropriate set of diagnostics and fixes.\n\nWhich option is most appropriate?\n\nA) The issue is solely poor scaling. Diagnostics should focus on comparing raw and z-scored condition numbers, and the fix is to rescale inputs to comparable magnitudes. No further changes are needed.\n\nB) Both poor scaling and multicollinearity are present. Diagnostics should include comparing condition numbers before and after z-scoring, inspecting the correlation matrix or singular value spectrum on standardized columns, and computing variance inflation factors. Fixes should include mapping inputs to a common domain such as $[-1,1]$, centering to reduce polynomial-term correlation, switching to an orthogonal polynomial basis on the standardized domain, redesigning the experiment to reduce alignment of columns (for example, via space-filling designs or response surface designs with axial points), and, if needed, applying regularization or truncated singular value decomposition in solving the normal equations.\n\nC) The issue is solely numerical roundoff in forming $X^{\\top}X$. Diagnostics should emphasize recomputing $X^{\\top}X$ in higher precision. The fix is to use extended precision arithmetic or Cholesky factorization of $X^{\\top}X$ without changing the design or basis.\n\nD) The problem is due to the intercept column creating collinearity. The fix is to remove the intercept from the basis; no rescaling or redesign is needed.\n\nE) The issue is insufficient sample size. The fix is to add many replicate runs at existing design points to decrease the condition number, without modifying the basis or input scaling.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- **Model:** Multivariate polynomial regression surrogate.\n- **Response:** Lithium-ion cell's discharge resistance.\n- **Inputs:** $x_1$ (current-collector thickness, $\\mu m$), $x_2$ (electrode porosity, dimensionless), $x_3$ (electrolyte ionic conductivity, S/m), $x_4$ (specific surface area, $m^{-1}$).\n- **Basis:** Intercept plus all monomials in $x_1, x_2, x_3, x_4$ up to total degree $2$. This results in $p = 1 + 4 + \\binom{4}{2} + 4 = 15$ basis functions (columns in the design matrix $X$). The basis functions are $1$, $x_i$, $x_i^2$, and $x_i x_j$ for $i,j \\in \\{1,2,3,4\\}$ with $i  j$.\n- **Data:** $n$ simulated experiments, with $n \\gg p$. The design matrix is $X \\in \\mathbb{R}^{n \\times p}$.\n- **Observation 1:** The $2$-norm condition number of the Gram matrix, $\\kappa_2(X^{\\top}X)$, is approximately $1.2 \\times 10^{16}$.\n- **Observation 2:** After z-scoring the columns of $X$, the condition number of the new Gram matrix drops to approximately $1.8 \\times 10^3$.\n- **Observation 3:** For z-scored columns, the empirical correlations are high: $|\\rho(x_1, x_1^2)| \\approx 0.98$, $|\\rho(x_1, x_1 x_4)| \\approx 0.97$, and $|\\rho(x_4, x_4^2)| \\approx 0.99$.\n- **Observation 4:** The variance inflation factor (VIF) for the $x_1$ column in the z-scored design matrix is $\\approx 220$. The VIF for the intercept is $\\approx 1$.\n- **Observation 5:** Replicating design points scales $X^{\\top}X$ by an integer factor but does not materially change its condition number.\n- **Assumptions:** Data are generated from a smooth physical mapping; standard double precision arithmetic is used.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing a common task in engineering modeling. It is well-posed, providing quantitative data that allow for a definitive diagnosis. The language is objective and precise. All provided observations are consistent with established principles of numerical linear algebra and regression diagnostics.\n- An initial condition number $\\kappa_2(X^{\\top}X) \\approx 1.2 \\times 10^{16}$ is on the order of $1/\\epsilon_m$ for double precision ($\\epsilon_m \\approx 2.22 \\times 10^{-16}$), indicating numerical singularity. This is consistent with $\\kappa_2(X^{\\top}X) = [\\kappa_2(X)]^2$.\n- A large drop in condition number after scaling is a classic symptom of poor scaling among the columns of $X$.\n- A residual condition number of $1.8 \\times 10^3$ after scaling, high correlations between predictors, and a VIF of $220$ are classic symptoms of severe multicollinearity. A VIF of $220$ implies an $R^2$ of $1 - 1/220 \\approx 0.995$ when regressing that predictor against all others.\n- The observation on replicates is mathematically correct: if $X_{new}$ is formed by stacking $X$ for $k$ times, then $X_{new}^{\\top}X_{new} = k X^{\\top}X$. The condition number is invariant to scalar multiplication, $\\kappa(cA) = \\kappa(A)$.\nThe problem is therefore valid.\n\n**Derivation and Evaluation**\nThe goal is to diagnose the cause of the near-singularity of $X^{\\top}X$. We analyze the provided evidence.\n\n1.  **Poor Scaling:** The initial condition number of the Gram matrix is $\\kappa_2(X^{\\top}X) \\approx 1.2 \\times 10^{16}$. After z-scoring the columns of the design matrix $X$, which standardizes their means to $0$ and standard deviations to $1$, the condition number drops precipitously to $\\approx 1.8 \\times 10^3$. This dramatic improvement of over $13$ orders of magnitude is definitive evidence that **poor scaling** is a primary contributor to the ill-conditioning. The original columns of $X$, which include terms like $x_1$ and $x_1^2$, likely have vastly different magnitudes and variances, leading to a poorly scaled Gram matrix.\n\n2.  **Multicollinearity:** Despite the significant improvement from scaling, the new condition number is still $1.8 \\times 10^3$. While manageable by robust solvers, this value is far from the ideal of $\\approx 1$ and indicates that the columns of the scaled matrix are still not close to orthogonal. This points to a remaining issue: **multicollinearity**.\n    - This diagnosis is directly confirmed by Observation 3: the correlation coefficients between certain columns in the standardized matrix are extremely high, e.g., $|\\rho(x_1, x_1^2)| \\approx 0.98$ and $|\\rho(x_4, x_4^2)| \\approx 0.99$. This shows a near-perfect linear relationship between these predictors. Such high correlation is a hallmark of multicollinearity, common in polynomial regression when input variables are not centered around zero before powers are taken.\n    - Observation 4 provides further confirmation. The Variance Inflation Factor (VIF) for the predictor $x_1$ is given as $\\approx 220$. The VIF for the $j$-th predictor is $VIF_j = 1/(1-R_j^2)$, where $R_j^2$ is the coefficient of determination from regressing the $j$-th predictor on all other predictors. A VIF of $220$ is exceptionally high (common rules of thumb suggest VIFs above $5$ or $10$ are problematic) and indicates that the $x_1$ vector in the design matrix lies very close to the subspace spanned by the other column vectors.\n    - Observation 5 correctly rules out that simply increasing the number of data points via replication would solve the problem, as this does not change the geometric relationships between the column vectors of $X$.\n\n**Conclusion:** The evidence points unambiguously to the presence of **both** severe poor scaling and severe multicollinearity. An appropriate response must acknowledge and propose solutions for both issues.\n\n**Option-by-Option Analysis**\n\n**A) The issue is solely poor scaling. Diagnostics should focus on comparing raw and z-scored condition numbers, and the fix is to rescale inputs to comparable magnitudes. No further changes are needed.**\nThis option correctly identifies poor scaling but incorrectly states it is the *sole* issue. The residual condition number of $1.8 \\times 10^3$, the VIF of $220$, and the high correlations after scaling all provide strong evidence of multicollinearity. Proposing no further changes is incorrect.\n**Verdict: Incorrect.**\n\n**B) Both poor scaling and multicollinearity are present. Diagnostics should include comparing condition numbers before and after z-scoring, inspecting the correlation matrix or singular value spectrum on standardized columns, and computing variance inflation factors. Fixes should include mapping inputs to a common domain such as $[-1,1]$, centering to reduce polynomial-term correlation, switching to an orthogonal polynomial basis on the standardized domain, redesigning the experiment to reduce alignment of columns (for example, via space-filling designs or response surface designs with axial points), and, if needed, applying regularization or truncated singular value decomposition in solving the normal equations.**\nThis option correctly identifies that both poor scaling and multicollinearity are issues. It lists a comprehensive and correct set of diagnostics (condition numbers, correlations, VIFs) that match the problem's data. It also proposes a full suite of state-of-the-art fixes that address both problems at their roots: mapping to a standardized domain (like $[-1,1]$) for scaling and initial centering, using an orthogonal polynomial basis (e.g., Legendre) to directly combat polynomial term correlation, improving the experimental design to break predictor dependencies, and using numerical/statistical remedies like regularization (Ridge/Lasso) or TSVD if the design cannot be changed. This is a complete and correct assessment and plan of action.\n**Verdict: Correct.**\n\n**C) The issue is solely numerical roundoff in forming $X^{\\top}X$. Diagnostics should emphasize recomputing $X^{\\top}X$ in higher precision. The fix is to use extended precision arithmetic or Cholesky factorization of $X^{\\top}X$ without changing the design or basis.**\nThe extreme ill-conditioning does cause numerical roundoff to dominate calculations in standard precision. However, roundoff is a symptom, not the root cause. The root causes are poor scaling and multicollinearity. Using higher precision would only compute an unstable solution more accurately; it would not fix the statistical problem of high variance in the estimated coefficients caused by multicollinearity. Cholesky factorization is a standard numerical method that would fail or be inaccurate for such an ill-conditioned matrix in standard precision. This option mistakes the symptom for the disease.\n**Verdict: Incorrect.**\n\n**D) The problem is due to the intercept column creating collinearity. The fix is to remove the intercept from the basis; no rescaling or redesign is needed.**\nThis is factually incorrect. Observation 4 states the VIF for the intercept is $\\approx 1$, indicating it is orthogonal to the other (centered) predictors. The collinearity exists among the non-intercept polynomial terms. Removing the intercept is generally incorrect and would likely introduce bias.\n**Verdict: Incorrect.**\n\n**E) The issue is insufficient sample size. The fix is to add many replicate runs at existing design points to decrease the condition number, without modifying the basis or input scaling.**\nThis is explicitly contradicted by Observation 5, which correctly states that adding replicates does not materially change the condition number. The problem is the choice of design points (their locations in the input space), not their quantity. The statement $n \\gg p$ also suggests that sample size is not the limiting factor.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The ultimate goal of surrogate modeling in automated design is not just to approximate a complex simulation, but to use that approximation to make intelligent decisions. This capstone exercise puts you in the driver's seat of an active learning workflow, where you will implement a procedure to strategically select the next simulation point to run. By building a surrogate that quantifies its own uncertainty and sensitivity, you will guide the search for optimal designs within a constrained feasible space, directly engaging with the trade-off between exploring the unknown and exploiting promising regions. ",
            "id": "3941922",
            "problem": "You are designing an active learning procedure to improve a polynomial regression surrogate for a dimensionless battery design response while respecting feasibility constraints. The design vector is $\\mathbf{x} = (L,\\varepsilon)$, where $L$ is a normalized electrode thickness and $\\varepsilon$ is the porosity fraction, both dimensionless. The true response is an unknown scalar function $f(L,\\varepsilon)$ that your simulation uses only to generate initial training data. The surrogate is a total-degree-$2$ polynomial regression $\\hat{f}(L,\\varepsilon)$ fit by ridge-regularized least squares. Your active learning rule must select a next sample $\\mathbf{x}^\\star$ inside a constrained feasible set, by maximizing an acquisition function that balances model uncertainty and local sensitivity of the surrogate, while strictly respecting feasibility.\n\nFoundational base to use:\n- Ordinary Least Squares (OLS) regression minimizes the sum of squared residuals, yielding coefficients that solve the normal equations.\n- Ridge regression (a form of Tikhonov regularization) augments the normal equations with a nonzero penalty, improving numerical stability when the design matrix is ill-conditioned.\n- The Gauss–Markov Theorem yields the structure of the prediction variance under the linear model, which is proportional to the leverage induced by the design matrix.\n- The gradient of a polynomial surrogate can be derived by differentiating the basis functions.\n\nYour program must implement the following steps for each test case:\n1. Construct a feature map $\\boldsymbol{\\phi}(L,\\varepsilon)$ corresponding to a total-degree-$2$ polynomial: \n   $$\\boldsymbol{\\phi}(L,\\varepsilon) = [\\,1,\\;L,\\;\\varepsilon,\\;L^2,\\;L\\varepsilon,\\;\\varepsilon^2\\,]^\\top.$$\n2. Given training data $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, fit ridge-regularized coefficients $\\boldsymbol{\\beta} \\in \\mathbb{R}^6$ via \n   $$\\boldsymbol{\\beta} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\left(y_i - \\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top \\boldsymbol{\\beta}\\right)^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2,$$\n   with ridge parameter $\\lambda = 10^{-6}$, implemented through the linear system\n   $$\\left(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}\\right) \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y},$$\n   where $\\mathbf{X}$ stacks $\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top$ row-wise and $\\mathbf{y}$ stacks $y_i$.\n3. Estimate the residual variance as \n   $$\\hat{\\sigma}^2 = \\frac{\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2}{\\max(n-p,1)},$$\n   with $p=6$ the number of coefficients. Use the ridge-adjusted inverse \n   $$\\mathbf{A}^{-1} = \\left(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}\\right)^{-1}.$$\n4. For a candidate point $\\mathbf{x}$, compute the uncertainty proxy\n   $$s^2(\\mathbf{x}) = \\hat{\\sigma}^2\\, \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{A}^{-1} \\boldsymbol{\\phi}(\\mathbf{x}), \\quad s(\\mathbf{x}) = \\sqrt{s^2(\\mathbf{x})},$$\n   and compute the gradient of the surrogate\n   $$\\nabla \\hat{f}(L,\\varepsilon) = \\left[\\;\\frac{\\partial \\hat{f}}{\\partial L},\\; \\frac{\\partial \\hat{f}}{\\partial \\varepsilon}\\;\\right]^\\top,$$\n   with\n   $$\\frac{\\partial \\hat{f}}{\\partial L} = \\beta_1 + 2\\beta_3 L + \\beta_4 \\varepsilon,\\qquad \\frac{\\partial \\hat{f}}{\\partial \\varepsilon} = \\beta_2 + \\beta_4 L + 2\\beta_5 \\varepsilon.$$\n5. Define the acquisition function for a tunable scalar $\\alpha$ as\n   $$A(\\mathbf{x}) = s(\\mathbf{x})\\left(1 + \\alpha \\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2\\right).$$\n6. Impose feasibility constraints. The feasible set is the intersection of box constraints and a coupling constraint\n   $$\\mathcal{F} = \\left\\{(L,\\varepsilon): L_{\\min} \\le L \\le L_{\\max},\\; \\varepsilon_{\\min} \\le \\varepsilon \\le \\varepsilon_{\\max},\\; \\underline{c} \\le L\\varepsilon \\le \\overline{c}\\right\\}.$$\n7. On a uniform grid of candidates over the box bounds, filter by feasibility and select\n   $$\\mathbf{x}^\\star = \\arg\\max_{\\mathbf{x} \\in \\mathcal{F}} A(\\mathbf{x}).$$\n\nSimulation of the true response (used only to generate initial training outputs $y_i$):\n$$f(L,\\varepsilon) = e^{-L}\\left(1 + 0.2\\,L\\,\\varepsilon\\right) + \\sqrt{\\varepsilon} - 0.6\\,L - 0.15\\,\\varepsilon^2 + 0.05\\,L\\,\\sin(6\\,\\varepsilon).$$\n\nTest Suite:\n- Case $1$ (general case): \n  - Bounds: $L \\in [0.8,1.4]$, $\\varepsilon \\in [0.5,0.7]$, coupling $L\\varepsilon \\in [0.5,0.9]$.\n  - Training inputs: $(1.0,0.5)$, $(1.2,0.5)$, $(1.0,0.7)$, $(1.3,0.6)$, $(0.8,0.7)$, $(1.4,0.5)$.\n  - Acquisition parameter: $\\alpha = 0.25$.\n  - Grid resolution: $51 \\times 51$ in each dimension.\n- Case $2$ (narrow coupling band):\n  - Bounds: $L \\in [0.9,1.2]$, $\\varepsilon \\in [0.6,0.8]$, coupling $L\\varepsilon \\in [0.72,0.75]$.\n  - Training inputs: $(0.9,0.8)$, $(1.0,0.72)$, $(1.2,0.6)$, $(1.1,0.68)$.\n  - Acquisition parameter: $\\alpha = 0.25$.\n  - Grid resolution: $51 \\times 51$.\n- Case $3$ (ill-conditioned design, ridge required):\n  - Bounds: $L \\in [0.9,1.1]$, $\\varepsilon \\in [0.5,0.7]$, coupling $L\\varepsilon \\in [0.5,0.77]$.\n  - Training inputs: $(1.0,0.5)$, $(1.0,0.55)$, $(1.0,0.6)$, $(1.0,0.65)$.\n  - Acquisition parameter: $\\alpha = 0.25$.\n  - Grid resolution: $51 \\times 51$.\n\nYour program must:\n- Implement the above procedure for each case.\n- Output the selected next sample $\\mathbf{x}^\\star$ for each case as a list of lists in a single line.\n- Round each coordinate to $4$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[L^\\star,\\varepsilon^\\star]$ corresponding to the selected next sample for one case (e.g., $[[L_1^\\star,\\varepsilon_1^\\star],[L_2^\\star,\\varepsilon_2^\\star],[L_3^\\star,\\varepsilon_3^\\star]]$). Each numeric entry must be rounded to $4$ decimal places and dimensionless.",
            "solution": "The user-provided problem has been validated and found to be scientifically grounded, well-posed, objective, and self-contained. The procedure outlined is a standard methodology in surrogate-based optimization and active learning. I will now provide a complete, reasoned solution.\n\nThe objective is to implement an active learning strategy to select the next sample point, $\\mathbf{x}^\\star = (L, \\varepsilon)$, for improving a polynomial surrogate model of a battery design response. The selection criterion is to maximize an acquisition function $A(\\mathbf{x})$ that balances model uncertainty and sensitivity, subject to a set of feasibility constraints. This process is executed for three distinct test cases.\n\nThe procedure is broken down into the following steps:\n\n**Step 1: Data Generation and Model Definition**\nThe process begins with a set of $n$ initial training points $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$. The input design vectors $\\mathbf{x}_i = (L_i, \\varepsilon_i)$ are given. The corresponding outputs $y_i$ are generated by evaluating a provided \"true\" function, $f(L, \\varepsilon)$:\n$$f(L,\\varepsilon) = e^{-L}\\left(1 + 0.2\\,L\\,\\varepsilon\\right) + \\sqrt{\\varepsilon} - 0.6\\,L - 0.15\\,\\varepsilon^2 + 0.05\\,L\\,\\sin(6\\,\\varepsilon)$$\nThe surrogate model, $\\hat{f}(L,\\varepsilon)$, is a total-degree-$2$ polynomial. Its value at a point $\\mathbf{x}=(L,\\varepsilon)$ is a linear combination of basis functions, expressed as $\\hat{f}(\\mathbf{x}) = \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\boldsymbol{\\beta}$. The feature map $\\boldsymbol{\\phi}(\\mathbf{x})$ and the coefficient vector $\\boldsymbol{\\beta}$ are defined as:\n$$\\boldsymbol{\\phi}(L,\\varepsilon) = [\\,1,\\;L,\\;\\varepsilon,\\;L^2,\\;L\\varepsilon,\\;\\varepsilon^2\\,]^\\top \\in \\mathbb{R}^6$$\n$$\\boldsymbol{\\beta} = [\\,\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;\\beta_3,\\;\\beta_4,\\;\\beta_5\\,]^\\top \\in \\mathbb{R}^6$$\n\n**Step 2: Ridge-Regularized Regression**\nThe coefficient vector $\\boldsymbol{\\beta}$ is determined by fitting the surrogate model to the training data using ridge-regularized least squares. This involves solving the following minimization problem:\n$$\\boldsymbol{\\beta} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\left(y_i - \\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top \\boldsymbol{\\beta}\\right)^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2$$\nThe ridge parameter $\\lambda$ is given as $10^{-6}$. This regularization term penalizes large coefficient values, which is crucial for numerical stability, especially when the design matrix is ill-conditioned (i.e., when training points are collinear or nearly so). The solution to this minimization problem is found by solving the linear system known as the regularized normal equations:\n$$(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}$$\nHere, $\\mathbf{X}$ is the $n \\times p$ design matrix, where each row is a feature vector $\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top$, $p=6$ is the number of features, $\\mathbf{y}$ is the $n \\times 1$ vector of training outputs, and $\\mathbf{I}$ is the $p \\times p$ identity matrix.\n\n**Step 3: Uncertainty Quantification**\nThe uncertainty in the surrogate model's prediction is estimated. First, the residual variance, $\\hat{\\sigma}^2$, is calculated from the model fit:\n$$\\hat{\\sigma}^2 = \\frac{\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2}{\\max(n-p,1)}$$\nThe denominator $\\max(n-p,1)$ is a robust degree-of-freedom adjustment that avoids division by zero or a negative number when the number of training points $n$ is less than or equal to the number of parameters $p$.\nThe uncertainty proxy for a new candidate point $\\mathbf{x}$, denoted $s^2(\\mathbf{x})$, is a heuristic based on the statistical variance of the prediction in an ordinary least squares setting, adapted for ridge regression:\n$$s^2(\\mathbf{x}) = \\hat{\\sigma}^2\\, \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{A}^{-1} \\boldsymbol{\\phi}(\\mathbf{x})$$\nwhere $\\mathbf{A}^{-1} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1}$. The term $\\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{A}^{-1} \\boldsymbol{\\phi}(\\mathbf{x})$ is known as the leverage of point $\\mathbf{x}$ and is higher for points far from the center of the training data. The uncertainty is then $s(\\mathbf{x}) = \\sqrt{s^2(\\mathbf{x})}$.\n\n**Step 4: Sensitivity Analysis via Gradient**\nThe local sensitivity of the surrogate model is quantified by the magnitude of its gradient, $\\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2$. The gradient components are derived by differentiating $\\hat{f}(\\mathbf{x}) = \\sum_{j=0}^{5} \\beta_j \\phi_j(\\mathbf{x})$ with respect to $L$ and $\\varepsilon$:\n$$\\frac{\\partial \\hat{f}}{\\partial L} = \\beta_1 + 2\\beta_3 L + \\beta_4 \\varepsilon$$\n$$\\frac{\\partial \\hat{f}}{\\partial \\varepsilon} = \\beta_2 + \\beta_4 L + 2\\beta_5 \\varepsilon$$\nThe squared Euclidean norm of the gradient is then $\\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2^2 = (\\frac{\\partial \\hat{f}}{\\partial L})^2 + (\\frac{\\partial \\hat{f}}{\\partial \\varepsilon})^2$.\n\n**Step 5: Acquisition Function**\nThe acquisition function $A(\\mathbf{x})$ combines the uncertainty and sensitivity metrics to guide the search for the next sample point. It is defined as:\n$$A(\\mathbf{x}) = s(\\mathbf{x})\\left(1 + \\alpha \\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2\\right)$$\nwhere $\\alpha=0.25$ is a tunable parameter. Maximizing this function favors points that are either in regions of high model uncertainty (exploration) or in regions where the model response changes rapidly (exploitation), or both.\n\n**Step 6 and 7: Constrained Optimization via Grid Search**\nThe optimal next sample point $\\mathbf{x}^\\star$ is the one that maximizes the acquisition function within a specified feasible set $\\mathcal{F}$. The feasible set is defined by the intersection of box constraints and a coupling constraint:\n$$\\mathcal{F} = \\left\\{(L,\\varepsilon): L_{\\min} \\le L \\le L_{\\max},\\; \\varepsilon_{\\min} \\le \\varepsilon \\le \\varepsilon_{\\max},\\; \\underline{c} \\le L\\varepsilon \\le \\overline{c}\\right\\}$$\nThe optimization problem is:\n$$\\mathbf{x}^\\star = \\arg\\max_{\\mathbf{x} \\in \\mathcal{F}} A(\\mathbf{x})$$\nThis is solved computationally by performing a search over a discrete grid of candidate points. A uniform $51 \\times 51$ grid is generated over the box constraints $[L_{\\min}, L_{\\max}] \\times [\\varepsilon_{\\min}, \\varepsilon_{\\max}]$. The acquisition function $A(\\mathbf{x})$ is evaluated at each grid point. Points that do not satisfy the coupling constraint $ \\underline{c} \\le L\\varepsilon \\le \\overline{c}$ are discarded. The grid point that yields the highest acquisition value among the feasible candidates is selected as $\\mathbf{x}^\\star$. This entire procedure is repeated for each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an active learning procedure to select the next sample point for improving\n    a polynomial regression surrogate model of a battery design response.\n    \"\"\"\n\n    # True response function (used only to generate initial training data)\n    def f_true(L, eps):\n        return (np.exp(-L) * (1 + 0.2 * L * eps) + np.sqrt(eps)\n                - 0.6 * L - 0.15 * eps**2 + 0.05 * L * np.sin(6 * eps))\n\n    # Feature map for total-degree-2 polynomial\n    def phi(L, eps):\n        return np.array([1, L, eps, L**2, L * eps, eps**2])\n\n    test_cases = [\n        {\n            \"name\": \"Case 1 (general)\",\n            \"L_bounds\": [0.8, 1.4],\n            \"eps_bounds\": [0.5, 0.7],\n            \"coupling_bounds\": [0.5, 0.9],\n            \"training_inputs\": [(1.0, 0.5), (1.2, 0.5), (1.0, 0.7), (1.3, 0.6), (0.8, 0.7), (1.4, 0.5)],\n            \"alpha\": 0.25,\n            \"grid_res\": 51,\n        },\n        {\n            \"name\": \"Case 2 (narrow coupling band)\",\n            \"L_bounds\": [0.9, 1.2],\n            \"eps_bounds\": [0.6, 0.8],\n            \"coupling_bounds\": [0.72, 0.75],\n            \"training_inputs\": [(0.9, 0.8), (1.0, 0.72), (1.2, 0.6), (1.1, 0.68)],\n            \"alpha\": 0.25,\n            \"grid_res\": 51,\n        },\n        {\n            \"name\": \"Case 3 (ill-conditioned design)\",\n            \"L_bounds\": [0.9, 1.1],\n            \"eps_bounds\": [0.5, 0.7],\n            \"coupling_bounds\": [0.5, 0.77],\n            \"training_inputs\": [(1.0, 0.5), (1.0, 0.55), (1.0, 0.6), (1.0, 0.65)],\n            \"alpha\": 0.25,\n            \"grid_res\": 51,\n        },\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        # ===== Step 1: Generate training data =====\n        train_x = np.array(case['training_inputs'])\n        train_y = np.array([f_true(L, eps) for L, eps in train_x])\n        \n        # ===== Step 2: Fit ridge regression model =====\n        lambda_reg = 1e-6\n        p = 6  # Number of polynomial features\n        X = np.array([phi(L, eps) for L, eps in train_x])\n        n = X.shape[0]\n        \n        I = np.identity(p)\n        A = X.T @ X + lambda_reg * I\n        b = X.T @ train_y\n        beta = np.linalg.solve(A, b)\n\n        # ===== Step 3: Estimate residual variance =====\n        y_hat = X @ beta\n        residuals = train_y - y_hat\n        ssr = residuals.T @ residuals\n        sigma_sq = ssr / max(n - p, 1)\n\n        # Pre-compute the inverse of the regularized matrix for efficiency\n        A_inv = np.linalg.inv(A)\n\n        # ===== Step 6 and 7: Grid search over feasible set =====\n        L_vals = np.linspace(case['L_bounds'][0], case['L_bounds'][1], case['grid_res'])\n        eps_vals = np.linspace(case['eps_bounds'][0], case['eps_bounds'][1], case['grid_res'])\n        L_grid, Eps_grid = np.meshgrid(L_vals, eps_vals)\n\n        # Flatten grid for vectorized processing\n        L_flat = L_grid.flatten()\n        Eps_flat = Eps_grid.flatten()\n\n        # Filter by feasibility coupling constraint\n        LE_prod = L_flat * Eps_flat\n        c_min, c_max = case['coupling_bounds']\n        feasible_mask = (LE_prod >= c_min)  (LE_prod = c_max)\n        \n        L_feasible = L_flat[feasible_mask]\n        Eps_feasible = Eps_flat[feasible_mask]\n\n        if L_feasible.size == 0:\n            # Handle the case where no grid points are feasible (not expected for these test cases)\n            final_results.append([np.nan, np.nan])\n            continue\n            \n        # Vectorized calculation of acquisition function over feasible points\n        \n        # 1. Feature matrix for all feasible points\n        phi_feasible = np.vstack([\n            np.ones_like(L_feasible),\n            L_feasible,\n            Eps_feasible,\n            L_feasible**2,\n            L_feasible * Eps_feasible,\n            Eps_feasible**2\n        ]).T\n        \n        # ===== Step 4: Compute uncertainty s(x) =====\n        s_sq_vals = sigma_sq * np.sum((phi_feasible @ A_inv) * phi_feasible, axis=1)\n        s_vals = np.sqrt(s_sq_vals)\n\n        # ===== Step 4 (cont.): Compute gradient norm ||grad f_hat(x)|| =====\n        grad_L = beta[1] + 2 * beta[3] * L_feasible + beta[4] * Eps_feasible\n        grad_Eps = beta[2] + beta[4] * L_feasible + 2 * beta[5] * Eps_feasible\n        grad_norm_vals = np.sqrt(grad_L**2 + grad_Eps**2)\n        \n        # ===== Step 5: Compute acquisition function A(x) =====\n        alpha = case['alpha']\n        acq_vals = s_vals * (1 + alpha * grad_norm_vals)\n\n        # Find the point that maximizes the acquisition function\n        max_idx = np.argmax(acq_vals)\n        x_star = [L_feasible[max_idx], Eps_feasible[max_idx]]\n        \n        # Round to 4 decimal places and append\n        final_results.append([round(c, 4) for c in x_star])\n\n    # Format the final output string\n    # e.g., [[val1,val2],[val3,val4]]\n    # Using str().replace() is a simple way to achieve the required no-space format.\n    print(str(final_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}