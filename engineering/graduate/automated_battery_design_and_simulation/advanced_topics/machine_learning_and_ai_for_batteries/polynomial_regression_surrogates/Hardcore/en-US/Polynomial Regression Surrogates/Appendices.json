{
    "hands_on_practices": [
        {
            "introduction": "A key step in building robust and numerically stable surrogate models is to transform physical variables onto a standardized domain and express the model in an orthogonal basis. This practice mitigates issues arising from poorly scaled variables and correlations between polynomial terms, such as $s$ and $s^2$. This exercise will guide you through the concrete mechanics of mapping a physical variable to the standard interval $[-1, 1]$ and performing a change of basis from the standard power basis to the more stable Legendre polynomial basis .",
            "id": "3941928",
            "problem": "In automated battery design and simulation, surrogate modeling is applied to replace expensive multiphysics computations with tractable approximations. Consider a polynomial regression surrogate for the effective ionic conductivity of a liquid electrolyte in a porous separator, denoted by $\\kappa_{\\mathrm{eff}}(\\varepsilon)$, where $\\varepsilon$ is the separator porosity. Under isothermal operation at a fixed salt composition, suppose that on the physically admissible porosity range $\\varepsilon \\in [0.35, 0.55]$, the surrogate is given by the degree-$3$ polynomial\n$$\n\\kappa_{\\mathrm{eff}}(\\varepsilon) = a_0 + a_1 \\varepsilon + a_2 \\varepsilon^2 + a_3 \\varepsilon^3,\n$$\nwith known coefficients $a_0 = 0.1$, $a_1 = 2.0$, $a_2 = -1.0$, and $a_3 = 0.5$. To facilitate orthogonal-basis regression and robust coefficient interpretation, it is standard practice to map the physical variable to a standardized domain and rewrite the surrogate in terms of orthogonal polynomials on that domain.\n\nMap $\\varepsilon \\in [0.35, 0.55]$ affinely onto a standardized variable $s \\in [-1, 1]$, and express $\\kappa_{\\mathrm{eff}}$ as a degree-$3$ expansion in Legendre polynomials $\\{P_0(s), P_1(s), P_2(s), P_3(s)\\}$ on $[-1,1]$ with unit weight. That is, find coefficients $c_0, c_1, c_2, c_3$ such that\n$$\n\\kappa_{\\mathrm{eff}}(\\varepsilon(s)) = c_0 P_0(s) + c_1 P_1(s) + c_2 P_2(s) + c_3 P_3(s).\n$$\nProvide the final coefficients ordered as $[c_0 \\; c_1 \\; c_2 \\; c_3]$, rounded to six significant figures, and express them in siemens per meter (S/m). The final answer must be a single row matrix using the $\\mathrm{pmatrix}$ environment, as specified.",
            "solution": "The task is to find the coefficients $c_0, c_1, c_2, c_3$ of an expansion of a polynomial surrogate model $\\kappa_{\\mathrm{eff}}(\\varepsilon)$ in terms of Legendre polynomials $P_k(s)$. The original model is a degree-$3$ polynomial in the physical variable $\\varepsilon \\in [0.35, 0.55]$, which must first be mapped to a standardized variable $s \\in [-1, 1]$.\n\nFirst, we determine the affine transformation connecting $\\varepsilon$ and $s$. Let this transformation be $\\varepsilon(s) = ms + b$. The boundary conditions are $\\varepsilon(-1) = 0.35$ and $\\varepsilon(1) = 0.55$. This leads to the following system of linear equations for the parameters $m$ and $b$:\n$$\n\\begin{cases}\n0.35 = -m + b \\\\\n0.55 = m + b\n\\end{cases}\n$$\nAdding the two equations gives $0.35 + 0.55 = 2b$, which simplifies to $0.90 = 2b$, yielding $b = 0.45$. Substituting $b$ into the second equation gives $0.55 = m + 0.45$, which results in $m = 0.10$.\nTherefore, the affine mapping from the standardized domain to the physical domain is:\n$$\n\\varepsilon(s) = 0.1s + 0.45\n$$\n\nNext, we substitute this expression for $\\varepsilon$ into the given polynomial for the effective ionic conductivity, $\\kappa_{\\mathrm{eff}}(\\varepsilon)$. The polynomial is:\n$$\n\\kappa_{\\mathrm{eff}}(\\varepsilon) = a_0 + a_1 \\varepsilon + a_2 \\varepsilon^2 + a_3 \\varepsilon^3\n$$\nwith the coefficients $a_0 = 0.1$, $a_1 = 2.0$, $a_2 = -1.0$, and $a_3 = 0.5$. We express $\\kappa_{\\mathrm{eff}}$ as a polynomial in $s$, denoted $K(s) = \\kappa_{\\mathrm{eff}}(\\varepsilon(s))$:\n$$\nK(s) = 0.5(0.1s + 0.45)^3 - 1.0(0.1s + 0.45)^2 + 2.0(0.1s + 0.45) + 0.1\n$$\nTo find the coefficients of $K(s)$ in the standard power basis $\\{1, s, s^2, s^3\\}$, we expand the terms:\n$$\n(0.1s + 0.45)^2 = (0.1)^2s^2 + 2(0.1)(0.45)s + (0.45)^2 = 0.01s^2 + 0.09s + 0.2025\n$$\n$$\n(0.1s + 0.45)^3 = (0.1)^3s^3 + 3(0.1)^2(0.45)s^2 + 3(0.1)(0.45)^2s + (0.45)^3 = 0.001s^3 + 0.0135s^2 + 0.06075s + 0.091125\n$$\nSubstituting these expansions into the expression for $K(s)$ and collecting terms by powers of $s$:\n$K(s) = d_3s^3 + d_2s^2 + d_1s + d_0$\nThe coefficients $d_k$ are:\n$d_3 = 0.5(0.001) = 0.0005$\n$d_2 = 0.5(0.0135) - 1.0(0.01) = 0.00675 - 0.01 = -0.00325$\n$d_1 = 0.5(0.06075) - 1.0(0.09) + 2.0(0.1) = 0.030375 - 0.09 + 0.2 = 0.140375$\n$d_0 = 0.5(0.091125) - 1.0(0.2025) + 2.0(0.45) + 0.1 = 0.0455625 - 0.2025 + 0.9 + 0.1 = 0.8430625$\nSo, the polynomial in the standardized variable $s$ is:\n$$\nK(s) = 0.0005s^3 - 0.00325s^2 + 0.140375s + 0.8430625\n$$\n\nThe final step is to convert this polynomial from the power basis to the Legendre polynomial basis. We want to find coefficients $c_k$ such that:\n$$\nK(s) = c_0 P_0(s) + c_1 P_1(s) + c_2 P_2(s) + c_3 P_3(s)\n$$\nThe relevant Legendre polynomials are:\n$P_0(s) = 1$\n$P_1(s) = s$\n$P_2(s) = \\frac{1}{2}(3s^2 - 1)$\n$P_3(s) = \\frac{1}{2}(5s^3 - 3s)$\nWe invert these relations to express the power basis in terms of the Legendre basis:\n$1 = P_0(s)$\n$s = P_1(s)$\n$s^2 = \\frac{2}{3}P_2(s) + \\frac{1}{3}P_0(s)$\n$s^3 = \\frac{2}{5}P_3(s) + \\frac{3}{5}P_1(s)$\nSubstituting these into the expression for $K(s)$:\n$$\nK(s) = d_3\\left(\\frac{2}{5}P_3(s) + \\frac{3}{5}P_1(s)\\right) + d_2\\left(\\frac{2}{3}P_2(s) + \\frac{1}{3}P_0(s)\\right) + d_1P_1(s) + d_0P_0(s)\n$$\nCollecting the coefficients of each $P_k(s)$ gives the desired coefficients $c_k$:\n$c_0 = d_0 + \\frac{1}{3}d_2 = 0.8430625 + \\frac{1}{3}(-0.00325) = 0.8430625 - 0.00108333... = 0.8419791666...$\n$c_1 = d_1 + \\frac{3}{5}d_3 = 0.140375 + \\frac{3}{5}(0.0005) = 0.140375 + 0.0003 = 0.140675$\n$c_2 = \\frac{2}{3}d_2 = \\frac{2}{3}(-0.00325) = -0.00216666...$\n$c_3 = \\frac{2}{5}d_3 = \\frac{2}{5}(0.0005) = 0.0002$\n\nFinally, we round these values to six significant figures as requested:\n$c_0 \\approx 0.841979$\n$c_1 = 0.140675$\n$c_2 \\approx -0.00216667$\n$c_3 = 0.000200000$\n\nThese coefficients are presented in the specified row matrix format.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.841979  0.140675  -0.00216667  0.000200000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Even with a sufficient number of data points, a polynomial regression can suffer from severe numerical instability, signaled by an extremely large condition number of the Gram matrix $X^{\\top}X$. This hands-on problem presents a realistic diagnostic scenario where you must act as a data detective, using numerical evidence like condition numbers, variance inflation factors (VIFs), and correlation matrices to pinpoint the root causes. By analyzing the provided data, you will learn to distinguish between ill-conditioning caused by poor variable scaling and that caused by inherent multicollinearity in the experimental design, a critical skill for any practitioner building reliable models .",
            "id": "3942011",
            "problem": "You are building a multivariate polynomial regression surrogate for a lithium-ion cell’s discharge resistance as a function of design and material variables in an automated battery design and simulation workflow. The surrogate is linear-in-parameters with a basis that includes an intercept and all monomials up to total degree $2$ in the inputs $x_1, x_2, x_3, x_4$, where $x_1$ is current-collector thickness (in micrometers), $x_2$ is electrode porosity (dimensionless), $x_3$ is electrolyte ionic conductivity (in siemens per meter), and $x_4$ is specific surface area (in inverse meters). You have $n$ simulated experiments with $n \\gg p$, where $p$ is the number of columns in the design matrix $X \\in \\mathbb{R}^{n \\times p}$. The following observations are reported from your fitting pipeline:\n\n- The $2$-norm condition number of $X^{\\top}X$ is approximately $1.2 \\times 10^{16}$.\n- After column-wise centering to zero mean and scaling each column to unit sample standard deviation (z-scoring), the $2$-norm condition number of the new Gram matrix drops to approximately $1.8 \\times 10^{3}$.\n- The empirical correlation matrix of the z-scored columns shows $|\\rho(x_1, x_1^2)| \\approx 0.98$, $|\\rho(x_1, x_1 x_4)| \\approx 0.97$, and $|\\rho(x_4, x_4^2)| \\approx 0.99$.\n- The variance inflation factor for the $x_1$ column, computed by regressing $x_1$ on all other columns of the z-scored design matrix, is about $220$; the intercept’s variance inflation factor is about $1$.\n- Repeating some design points to obtain replicates (without changing the set of unique design points) scales $X^{\\top}X$ by an integer factor but does not materially change its condition number.\n\nAssume the data are generated by a physically smooth mapping from inputs to the true response and that numerical precision is standard double precision. Based on first principles of linear regression and numerical linear algebra, determine whether the near-singularity of $X^{\\top}X$ arises primarily from multicollinearity, poor scaling, both, or neither, and choose the most appropriate set of diagnostics and fixes.\n\nWhich option is most appropriate?\n\nA) The issue is solely poor scaling. Diagnostics should focus on comparing raw and z-scored condition numbers, and the fix is to rescale inputs to comparable magnitudes. No further changes are needed.\n\nB) Both poor scaling and multicollinearity are present. Diagnostics should include comparing condition numbers before and after z-scoring, inspecting the correlation matrix or singular value spectrum on standardized columns, and computing variance inflation factors. Fixes should include mapping inputs to a common domain such as $[-1,1]$, centering to reduce polynomial-term correlation, switching to an orthogonal polynomial basis on the standardized domain, redesigning the experiment to reduce alignment of columns (for example, via space-filling designs or response surface designs with axial points), and, if needed, applying regularization or truncated singular value decomposition in solving the normal equations.\n\nC) The issue is solely numerical roundoff in forming $X^{\\top}X$. Diagnostics should emphasize recomputing $X^{\\top}X$ in higher precision. The fix is to use extended precision arithmetic or Cholesky factorization of $X^{\\top}X$ without changing the design or basis.\n\nD) The problem is due to the intercept column creating collinearity. The fix is to remove the intercept from the basis; no rescaling or redesign is needed.\n\nE) The issue is insufficient sample size. The fix is to add many replicate runs at existing design points to decrease the condition number, without modifying the basis or input scaling.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- **Model:** Multivariate polynomial regression surrogate.\n- **Response:** Lithium-ion cell's discharge resistance.\n- **Inputs:** $x_1$ (current-collector thickness, $\\mu m$), $x_2$ (electrode porosity, dimensionless), $x_3$ (electrolyte ionic conductivity, S/m), $x_4$ (specific surface area, $m^{-1}$).\n- **Basis:** Intercept plus all monomials in $x_1, x_2, x_3, x_4$ up to total degree $2$. This results in $p = 1 + 4 + \\binom{4}{2} + 4 = 1 + 4 + 6 + 4 = 15$ basis functions (columns in the design matrix $X$). The basis functions are $1$, $x_i$, $x_i^2$, and $x_i x_j$ for $i,j \\in \\{1,2,3,4\\}$ with $i \\leq j$.\n- **Data:** $n$ simulated experiments, with $n \\gg p$. The design matrix is $X \\in \\mathbb{R}^{n \\times p}$.\n- **Observation 1:** The $2$-norm condition number of the Gram matrix, $\\kappa_2(X^{\\top}X)$, is approximately $1.2 \\times 10^{16}$.\n- **Observation 2:** After z-scoring the columns of $X$, the condition number of the new Gram matrix drops to approximately $1.8 \\times 10^3$.\n- **Observation 3:** For z-scored columns, the empirical correlations are high: $|\\rho(x_1, x_1^2)| \\approx 0.98$, $|\\rho(x_1, x_1 x_4)| \\approx 0.97$, and $|\\rho(x_4, x_4^2)| \\approx 0.99$.\n- **Observation 4:** The variance inflation factor (VIF) for the $x_1$ column in the z-scored design matrix is $\\approx 220$. The VIF for the intercept is $\\approx 1$.\n- **Observation 5:** Replicating design points scales $X^{\\top}X$ by an integer factor but does not materially change its condition number.\n- **Assumptions:** Data are generated from a smooth physical mapping; standard double precision arithmetic is used.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing a common task in engineering modeling. It is well-posed, providing quantitative data that allow for a definitive diagnosis. The language is objective and precise. All provided observations are consistent with established principles of numerical linear algebra and regression diagnostics.\n- An initial condition number $\\kappa_2(X^{\\top}X) \\approx 1.2 \\times 10^{16}$ is on the order of $1/\\epsilon_m$ for double precision ($\\epsilon_m \\approx 2.22 \\times 10^{-16}$), indicating numerical singularity. This is consistent with $\\kappa_2(X^{\\top}X) = [\\kappa_2(X)]^2$.\n- A large drop in condition number after scaling is a classic symptom of poor scaling among the columns of $X$.\n- A residual condition number of $1.8 \\times 10^3$ after scaling, high correlations between predictors, and a VIF of $220$ are classic symptoms of severe multicollinearity. A VIF of $220$ implies an $R^2$ of $1 - 1/220 \\approx 0.995$ when regressing that predictor against all others.\n- The observation on replicates is mathematically correct: if $X_{new}$ is formed by stacking $X$ for $k$ times, then $X_{new}^{\\top}X_{new} = k X^{\\top}X$. The condition number is invariant to scalar multiplication, $\\kappa(cA) = \\kappa(A)$.\nThe problem is therefore valid.\n\n**Derivation and Evaluation**\nThe goal is to diagnose the cause of the near-singularity of $X^{\\top}X$. We analyze the provided evidence.\n\n1.  **Poor Scaling:** The initial condition number of the Gram matrix is $\\kappa_2(X^{\\top}X) \\approx 1.2 \\times 10^{16}$. After z-scoring the columns of the design matrix $X$, which standardizes their means to $0$ and standard deviations to $1$, the condition number drops precipitously to $\\approx 1.8 \\times 10^3$. This dramatic improvement of over $13$ orders of magnitude is definitive evidence that **poor scaling** is a primary contributor to the ill-conditioning. The original columns of $X$, which include terms like $x_1$ and $x_1^2$, likely have vastly different magnitudes and variances, leading to a poorly scaled Gram matrix.\n\n2.  **Multicollinearity:** Despite the significant improvement from scaling, the new condition number is still $1.8 \\times 10^3$. While manageable by robust solvers, this value is far from the ideal of $\\approx 1$ and indicates that the columns of the scaled matrix are still not close to orthogonal. This points to a remaining issue: **multicollinearity**.\n    - This diagnosis is directly confirmed by Observation 3: the correlation coefficients between certain columns in the standardized matrix are extremely high, e.g., $|\\rho(x_1, x_1^2)| \\approx 0.98$ and $|\\rho(x_4, x_4^2)| \\approx 0.99$. This shows a near-perfect linear relationship between these predictors. Such high correlation is a hallmark of multicollinearity, common in polynomial regression when input variables are not centered around zero before powers are taken.\n    - Observation 4 provides further confirmation. The Variance Inflation Factor (VIF) for the predictor $x_1$ is given as $\\approx 220$. The VIF for the $j$-th predictor is $VIF_j = 1/(1-R_j^2)$, where $R_j^2$ is the coefficient of determination from regressing the $j$-th predictor on all other predictors. A VIF of $220$ is exceptionally high (common rules of thumb suggest VIFs above $5$ or $10$ are problematic) and indicates that the $x_1$ vector in the design matrix lies very close to the subspace spanned by the other column vectors.\n    - Observation 5 correctly rules out that simply increasing the number of data points via replication would solve the problem, as this does not change the geometric relationships between the column vectors of $X$.\n\n**Conclusion:** The evidence points unambiguously to the presence of **both** severe poor scaling and severe multicollinearity. An appropriate response must acknowledge and propose solutions for both issues.\n\n**Option-by-Option Analysis**\n\n**A) The issue is solely poor scaling. Diagnostics should focus on comparing raw and z-scored condition numbers, and the fix is to rescale inputs to comparable magnitudes. No further changes are needed.**\nThis option correctly identifies poor scaling but incorrectly states it is the *sole* issue. The residual condition number of $1.8 \\times 10^3$, the VIF of $220$, and the high correlations after scaling all provide strong evidence of multicollinearity. Proposing no further changes is incorrect.\n**Verdict: Incorrect.**\n\n**B) Both poor scaling and multicollinearity are present. Diagnostics should include comparing condition numbers before and after z-scoring, inspecting the correlation matrix or singular value spectrum on standardized columns, and computing variance inflation factors. Fixes should include mapping inputs to a common domain such as $[-1,1]$, centering to reduce polynomial-term correlation, switching to an orthogonal polynomial basis on the standardized domain, redesigning the experiment to reduce alignment of columns (for example, via space-filling designs or response surface designs with axial points), and, if needed, applying regularization or truncated singular value decomposition in solving the normal equations.**\nThis option correctly identifies that both poor scaling and multicollinearity are issues. It lists a comprehensive and correct set of diagnostics (condition numbers, correlations, VIFs) that match the problem's data. It also proposes a full suite of state-of-the-art fixes that address both problems at their roots: mapping to a standardized domain (like $[-1,1]$) for scaling and initial centering, using an orthogonal polynomial basis (e.g., Legendre) to directly combat polynomial term correlation, improving the experimental design to break predictor dependencies, and using numerical/statistical remedies like regularization (Ridge/Lasso) or TSVD if the design cannot be changed. This is a complete and correct assessment and plan of action.\n**Verdict: Correct.**\n\n**C) The issue is solely numerical roundoff in forming $X^{\\top}X$. Diagnostics should emphasize recomputing $X^{\\top}X$ in higher precision. The fix is to use extended precision arithmetic or Cholesky factorization of $X^{\\top}X$ without changing the design or basis.**\nThe extreme ill-conditioning does cause numerical roundoff to dominate calculations in standard precision. However, roundoff is a symptom, not the root cause. The root causes are poor scaling and multicollinearity. Using higher precision would only compute an unstable solution more accurately; it would not fix the statistical problem of high variance in the estimated coefficients caused by multicollinearity. Cholesky factorization is a standard numerical method that would fail or be inaccurate for such an ill-conditioned matrix in standard precision. This option mistakes the symptom for the disease.\n**Verdict: Incorrect.**\n\n**D) The problem is due to the intercept column creating collinearity. The fix is to remove the intercept from the basis; no rescaling or redesign is needed.**\nThis is factually incorrect. Observation 4 states the VIF for the intercept is $\\approx 1$, indicating it is orthogonal to the other (centered) predictors. The collinearity exists among the non-intercept polynomial terms. Removing the intercept is generally incorrect and would likely introduce bias.\n**Verdict: Incorrect.**\n\n**E) The issue is insufficient sample size. The fix is to add many replicate runs at existing design points to decrease the condition number, without modifying the basis or input scaling.**\nThis is explicitly contradicted by Observation 5, which correctly states that adding replicates does not materially change the condition number. The problem is the choice of design points (their locations in the input space), not their quantity. The statement $n \\gg p$ also suggests that sample size is not the limiting factor.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "To enhance the accuracy and data efficiency of surrogate models, we can incorporate more physical information than just the scalar output of a simulation. When our simulations can provide gradient information, we can use gradient-enhanced regression to significantly improve the model's fidelity. This exercise focuses on the foundational step of this advanced technique: constructing the augmented design matrix that maps the model's coefficients to both the function value and its partial derivatives, providing a clear blueprint for implementing this powerful modeling approach .",
            "id": "3941907",
            "problem": "In automated battery design and simulation, surrogate models are often used to approximate outputs of computationally expensive electrochemical simulations. Consider a gradient-enhanced polynomial regression surrogate for the open-circuit voltage $V$ of a lithium-ion cell as a function of three scalar inputs: state of charge (SOC), temperature, and current. Let $s$ denote state of charge (SOC), $T$ denote temperature, and $I$ denote current. Assume $V$ is approximated by a complete multivariate polynomial of total degree $3$ in the variables $s$, $T$, and $I$, with all monomials up to total degree $3$ included.\n\nAdopt the following canonical monomial ordering: lexicographic order with $s \\prec T \\prec I$, grouped by total degree from $0$ to $3$. Let the coefficient vector be $ \\boldsymbol{\\beta} \\in \\mathbb{R}^{20}$ ordered consistently with this monomial basis. Suppose one wishes to perform gradient-enhanced linear regression by stacking the output $V$ with its partial derivatives $\\partial V/\\partial s$, $\\partial V/\\partial T$, and $\\partial V/\\partial I$ evaluated at a single input sample $(s_0, T_0, I_0)$.\n\nStarting from the definitions of a polynomial basis, linear regression design matrices, and the calculus definition of partial derivatives, derive the analytic expressions for the partial derivatives $\\partial V/\\partial s$, $\\partial V/\\partial T$, and $\\partial V/\\partial I$ by differentiating the degree-$3$ monomial basis with respect to each variable. Then, assemble the $4 \\times 20$ block design matrix that maps the coefficient vector $ \\boldsymbol{\\beta}$ to the stacked outputs vector $\\left[V(s_0,T_0,I_0), \\left.\\frac{\\partial V}{\\partial s}\\right|_{(s_0,T_0,I_0)}, \\left.\\frac{\\partial V}{\\partial T}\\right|_{(s_0,T_0,I_0)}, \\left.\\frac{\\partial V}{\\partial I}\\right|_{(s_0,T_0,I_0)}\\right]^{\\top}$.\n\nExpress your final answer as a single closed-form analytic matrix in terms of $s_0$, $T_0$, and $I_0$. No numerical evaluation or rounding is required.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the field of numerical methods and machine learning, specifically surrogate modeling using polynomial regression. The problem is well-posed, with all necessary variables, constraints, and objectives clearly defined, leading to a unique and meaningful solution. It is free from factual unsoundness, ambiguity, and contradictions.\n\nThe goal is to derive the $4 \\times 20$ design matrix for a gradient-enhanced polynomial regression. The surrogate model approximates the open-circuit voltage $V$ as a function of state of charge $s$, temperature $T$, and current $I$. The model is a complete multivariate polynomial of total degree $3$.\n\nFirst, we establish the monomial basis vector, denoted as $\\boldsymbol{\\phi}(s, T, I)$, which is a $20 \\times 1$ column vector. The\nmonomials are ordered by total degree (from $0$ to $3$), and within each degree group, by lexicographic order with $s \\prec T \\prec I$. The number of monomials for $n=3$ variables and total degree up to $d=3$ is given by the binomial coefficient $\\binom{n+d}{d} = \\binom{3+3}{3} = \\frac{6!}{3!3!} = 20$.\n\nThe components of the basis vector $\\boldsymbol{\\phi}(s, T, I)^T$ are:\n- Degree 0: $1$\n- Degree 1: $s, T, I$\n- Degree 2: $s^2, sT, sI, T^2, TI, I^2$\n- Degree 3: $s^3, s^2T, s^2I, sT^2, sTI, sI^2, T^3, T^2I, TI^2, I^3$\n\nThus, the monomial basis row vector is:\n$$\n\\boldsymbol{\\phi}(s,T,I)^T = \\begin{pmatrix} 1  s  T  I  s^2  sT  sI  T^2  TI  I^2  s^3  s^2T  s^2I  sT^2  sTI  sI^2  T^3  T^2I  TI^2  I^3 \\end{pmatrix}\n$$\n\nThe polynomial regression model for the voltage $V$ is a linear combination of these basis functions, with coefficients given by the vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{20}$:\n$$\nV(s, T, I) = \\boldsymbol{\\phi}(s,T,I)^T \\boldsymbol{\\beta}\n$$\n\nFor gradient-enhanced regression, we augment the output with the partial derivatives of $V$ with respect to each input variable. The stacked output vector, evaluated at a sample point $(s_0, T_0, I_0)$, is:\n$$\n\\mathbf{y} = \\begin{pmatrix} V(s_0, T_0, I_0) \\\\ \\left.\\frac{\\partial V}{\\partial s}\\right|_{(s_0, T_0, I_0)} \\\\ \\left.\\frac{\\partial V}{\\partial T}\\right|_{(s_0, T_0, I_0)} \\\\ \\left.\\frac{\\partial V}{\\partial I}\\right|_{(s_0, T_0, I_0)} \\end{pmatrix}\n$$\n\nThe relationship between the stacked outputs $\\mathbf{y}$ and the coefficient vector $\\boldsymbol{\\beta}$ is linear, defined by a design matrix $\\mathbf{X}$ such that $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta}$. The rows of $\\mathbf{X}$ are the basis vector and its partial derivatives, evaluated at $(s_0, T_0, I_0)$.\n\nThe partial derivatives of $V$ are:\n$$\n\\frac{\\partial V}{\\partial s} = \\frac{\\partial}{\\partial s} (\\boldsymbol{\\phi}^T \\boldsymbol{\\beta}) = \\left(\\frac{\\partial \\boldsymbol{\\phi}}{\\partial s}\\right)^T \\boldsymbol{\\beta}\n$$\n$$\n\\frac{\\partial V}{\\partial T} = \\frac{\\partial}{\\partial T} (\\boldsymbol{\\phi}^T \\boldsymbol{\\beta}) = \\left(\\frac{\\partial \\boldsymbol{\\phi}}{\\partial T}\\right)^T \\boldsymbol{\\beta}\n$$\n$$\n\\frac{\\partial V}{\\partial I} = \\frac{\\partial}{\\partial I} (\\boldsymbol{\\phi}^T \\boldsymbol{\\beta}) = \\left(\\frac{\\partial \\boldsymbol{\\phi}}{\\partial I}\\right)^T \\boldsymbol{\\beta}\n$$\n\nWe must compute the partial derivatives of the monomial basis row vector $\\boldsymbol{\\phi}^T$.\n\n1.  Derivative with respect to $s$:\n    $$\n    \\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial s} = \\begin{pmatrix} 0  1  0  0  2s  T  I  0  0  0  3s^2  2sT  2sI  T^2  TI  I^2  0  0  0  0 \\end{pmatrix}\n    $$\n2.  Derivative with respect to $T$:\n    $$\n    \\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial T} = \\begin{pmatrix} 0  0  1  0  0  s  0  2T  I  0  0  s^2  0  2sT  sI  0  3T^2  2TI  I^2  0 \\end{pmatrix}\n    $$\n3.  Derivative with respect to $I$:\n    $$\n    \\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial I} = \\begin{pmatrix} 0  0  0  1  0  0  s  0  T  2I  0  0  s^2  0  sT  2sI  0  T^2  2TI  3I^2 \\end{pmatrix}\n    $$\n\nThe design matrix $\\mathbf{X}$ is a $4 \\times 20$ matrix formed by stacking these row vectors and evaluating them at the point $(s_0, T_0, I_0)$.\n$$\n\\mathbf{X} = \\begin{pmatrix}\n\\boldsymbol{\\phi}(s_0, T_0, I_0)^T \\\\\n\\left.\\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial s}\\right|_{(s_0, T_0, I_0)} \\\\\n\\left.\\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial T}\\right|_{(s_0, T_0, I_0)} \\\\\n\\left.\\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial I}\\right|_{(s_0, T_0, I_0)}\n\\end{pmatrix}\n$$\n\nExplicitly, the rows of the matrix $\\mathbf{X}$ are:\n- Row 1: $\\boldsymbol{\\phi}(s_0, T_0, I_0)^T = \\begin{pmatrix} 1  s_0  T_0  I_0  s_0^2  s_0T_0  s_0I_0  T_0^2  T_0I_0  I_0^2  s_0^3  s_0^2T_0  s_0^2I_0  s_0T_0^2  s_0T_0I_0  s_0I_0^2  T_0^3  T_0^2I_0  T_0I_0^2  I_0^3 \\end{pmatrix}$\n- Row 2: $\\left.\\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial s}\\right|_{(s_0, T_0, I_0)} = \\begin{pmatrix} 0  1  0  0  2s_0  T_0  I_0  0  0  0  3s_0^2  2s_0T_0  2s_0I_0  T_0^2  T_0I_0  I_0^2  0  0  0  0 \\end{pmatrix}$\n- Row 3: $\\left.\\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial T}\\right|_{(s_0, T_0, I_0)} = \\begin{pmatrix} 0  0  1  0  0  s_0  0  2T_0  I_0  0  0  s_0^2  0  2s_0T_0  s_0I_0  0  3T_0^2  2T_0I_0  I_0^2  0 \\end{pmatrix}$\n- Row 4: $\\left.\\frac{\\partial \\boldsymbol{\\phi}^T}{\\partial I}\\right|_{(s_0, T_0, I_0)} = \\begin{pmatrix} 0  0  0  1  0  0  s_0  0  T_0  2I_0  0  0  s_0^2  0  s_0T_0  2s_0I_0  0  T_0^2  2T_0I_0  3I_0^2 \\end{pmatrix}$\n\nAssembling these rows gives the final $4 \\times 20$ design matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  s_0  T_0  I_0  s_0^{2}  s_0T_0  s_0I_0  T_0^{2}  T_0I_0  I_0^{2}  s_0^{3}  s_0^{2}T_0  s_0^{2}I_0  s_0T_0^{2}  s_0T_0I_0  s_0I_0^{2}  T_0^{3}  T_0^{2}I_0  T_0I_0^{2}  I_0^{3} \\\\\n0  1  0  0  2s_0  T_0  I_0  0  0  0  3s_0^{2}  2s_0T_0  2s_0I_0  T_0^{2}  T_0I_0  I_0^{2}  0  0  0  0 \\\\\n0  0  1  0  0  s_0  0  2T_0  I_0  0  0  s_0^{2}  0  2s_0T_0  s_0I_0  0  3T_0^{2}  2T_0I_0  I_0^{2}  0 \\\\\n0  0  0  1  0  0  s_0  0  T_0  2I_0  0  0  s_0^{2}  0  s_0T_0  2s_0I_0  0  T_0^{2}  2T_0I_0  3I_0^{2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}