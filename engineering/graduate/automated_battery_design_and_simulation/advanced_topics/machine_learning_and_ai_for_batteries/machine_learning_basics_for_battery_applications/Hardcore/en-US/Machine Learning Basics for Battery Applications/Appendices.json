{
    "hands_on_practices": [
        {
            "introduction": "Modern battery testing generates vast datasets with numerous correlated features for each cycle, making it challenging to identify clear degradation patterns. Principal Component Analysis (PCA) provides a powerful, unsupervised method to distill this complexity into a few informative dimensions, revealing the most significant sources of variation in the data. This practice  guides you through the fundamental derivation of PCA, connecting the abstract linear algebra concept of eigenvalues to the tangible metric of \"explained variance\" and demonstrating how to quantify the information captured by the principal components.",
            "id": "3926067",
            "problem": "A dataset of $n$ lithium-ion battery cycles is described by $p$ scalar features extracted per cycle: voltage plateau duration during charge, differential capacity peak magnitude near the $\\mathrm{LiC}_6/\\mathrm{LiC}_{12}$ transition, impedance magnitude at a mid-frequency, charge time to a preset cutoff, and round-trip energy efficiency. Each feature is standardized across the dataset to zero mean and unit variance. Let the mean-centered and standardized data matrix be $X \\in \\mathbb{R}^{n \\times p}$, and let the unbiased sample covariance matrix be $\\Sigma \\in \\mathbb{R}^{p \\times p}$.\n\nDefine Principal Component Analysis (PCA) and derive, from first principles starting with the definitions of covariance and orthonormal eigen-decomposition, the expression for the fraction of total variance explained by the first $k$ principal components in terms of the eigenvalues of $\\Sigma$.\n\nThen, in the specific case $p = 5$ with eigenvalues of $\\Sigma$ given by $\\lambda_{1} = 2.1$, $\\lambda_{2} = 1.3$, $\\lambda_{3} = 0.9$, $\\lambda_{4} = 0.5$, and $\\lambda_{5} = 0.2$, compute the fraction of variance explained by the first $k = 2$ principal components. Express your final result as a decimal, and round your answer to four significant figures. Do not use a percentage sign.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- A dataset of $n$ lithium-ion battery cycles.\n- $p$ scalar features are extracted per cycle.\n- The features are standardized to have zero mean and unit variance.\n- The mean-centered and standardized data matrix is $X \\in \\mathbb{R}^{n \\times p}$.\n- The unbiased sample covariance matrix is $\\Sigma \\in \\mathbb{R}^{p \\times p}$.\n- Task 1: Define Principal Component Analysis (PCA) and derive the expression for the fraction of total variance explained by the first $k$ principal components in terms of the eigenvalues of $\\Sigma$. The derivation must start from the definitions of covariance and orthonormal eigen-decomposition.\n- Task 2: For the specific case where $p = 5$ and the eigenvalues of $\\Sigma$ are $\\lambda_{1} = 2.1$, $\\lambda_{2} = 1.3$, $\\lambda_{3} = 0.9$, $\\lambda_{4} = 0.5$, and $\\lambda_{5} = 0.2$, compute the fraction of variance explained by the first $k = 2$ principal components.\n- The final numerical result must be expressed as a decimal rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard, well-established principles of linear algebra and statistics (PCA, covariance, eigenvalues) in the context of a realistic application (battery data analysis). The premise is sound. A key consistency check is that for standardized data (where each feature has variance $1$), the total variance is the number of features, $p$. The total variance is also the trace of the covariance matrix, which equals the sum of its eigenvalues. Here, $p=5$, and the sum of the given eigenvalues is $\\lambda_1 + \\lambda_2 + \\lambda_3 + \\lambda_4 + \\lambda_5 = 2.1 + 1.3 + 0.9 + 0.5 + 0.2 = 5.0$. This matches $p=5$, confirming the internal consistency and scientific validity of the problem statement.\n- **Well-Posed:** The problem asks for a standard theoretical derivation and a specific calculation. It provides all necessary information for a unique solution.\n- **Objective:** The language is clear, precise, and devoid of subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will be provided, starting with the derivation and followed by the specific calculation.\n\n### Part 1: Derivation\n\nPrincipal Component Analysis (PCA) is a linear transformation technique used for dimensionality reduction. It transforms a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The transformation is defined in such a way that the first principal component has the largest possible variance (that is, it accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n\nLet the mean-centered data matrix be $X \\in \\mathbb{R}^{n \\times p}$. The unbiased sample covariance matrix $\\Sigma$ is given by:\n$$\n\\Sigma = \\frac{1}{n-1} X^T X\n$$\nA principal component is a linear combination of the original features. A direction for this linear combination can be represented by a unit vector $w \\in \\mathbb{R}^p$ (where $w^T w = 1$). The scores of the $n$ data points when projected onto this direction are given by the vector $z = Xw$.\n\nThe variance of these projected scores is:\n$$\n\\text{Var}(z) = \\frac{1}{n-1} z^T z = \\frac{1}{n-1} (Xw)^T (Xw) = \\frac{1}{n-1} w^T X^T X w\n$$\nSubstituting the expression for $\\Sigma$, we get:\n$$\n\\text{Var}(z) = w^T \\left( \\frac{1}{n-1} X^T X \\right) w = w^T \\Sigma w\n$$\nThe first principal component is found by choosing the direction $w_1$ that maximizes this variance, subject to the constraint that $w_1$ is a unit vector. This is a constrained optimization problem:\n$$\n\\max_{w_1} w_1^T \\Sigma w_1 \\quad \\text{subject to} \\quad w_1^T w_1 = 1\n$$\nWe use the method of Lagrange multipliers. The Lagrangian function is:\n$$\nL(w_1, \\lambda) = w_1^T \\Sigma w_1 - \\lambda(w_1^T w_1 - 1)\n$$\nTo find the maximum, we compute the gradient with respect to $w_1$ and set it to zero. Since $\\Sigma$ is symmetric, the derivative of $w_1^T \\Sigma w_1$ with respect to $w_1$ is $2\\Sigma w_1$. The derivative of $w_1^T w_1$ is $2w_1$.\n$$\n\\frac{\\partial L}{\\partial w_1} = 2\\Sigma w_1 - 2\\lambda w_1 = 0\n$$\nThis simplifies to the eigenvalue equation:\n$$\n\\Sigma w_1 = \\lambda w_1\n$$\nThis shows that the optimal direction $w_1$ must be an eigenvector of the covariance matrix $\\Sigma$, and $\\lambda$ is its corresponding eigenvalue. To determine which eigenvector maximizes the variance, we substitute $\\Sigma w_1 = \\lambda w_1$ back into the variance expression:\n$$\n\\text{Var}(z) = w_1^T \\Sigma w_1 = w_1^T (\\lambda w_1) = \\lambda (w_1^T w_1)\n$$\nSince $w_1^T w_1 = 1$, we have:\n$$\n\\text{Var}(z) = \\lambda\n$$\nThe variance of the data projected onto an eigenvector is equal to the corresponding eigenvalue. To maximize the variance, we must choose the eigenvector $w_1$ corresponding to the largest eigenvalue, $\\lambda_1$. Thus, the first principal component is the direction of the eigenvector with the largest eigenvalue, and the variance it explains is $\\lambda_1$.\n\nSubsequent principal components are found by maximizing variance in directions orthogonal to all previous components. This procedure systematically selects the eigenvectors of $\\Sigma$ in descending order of their corresponding eigenvalues. The $j$-th principal component corresponds to the eigenvector $w_j$ with the $j$-th largest eigenvalue $\\lambda_j$, and it explains a variance of $\\lambda_j$.\n\nThe total variance of the original dataset is the sum of the variances of the individual features, which is the sum of the diagonal elements of the covariance matrix, i.e., its trace:\n$$\n\\text{Total Variance} = \\text{tr}(\\Sigma) = \\sum_{i=1}^{p} \\Sigma_{ii}\n$$\nThe covariance matrix $\\Sigma$ is a real, symmetric matrix, so it admits an orthonormal eigen-decomposition $\\Sigma = W \\Lambda W^T$, where $W$ is an orthogonal matrix ($W^T W = I$) whose columns are the eigenvectors $w_j$, and $\\Lambda$ is a diagonal matrix of the eigenvalues $\\lambda_j$. Using the cyclic property of the trace ($\\text{tr}(ABC) = \\text{tr}(BCA)$):\n$$\n\\text{tr}(\\Sigma) = \\text{tr}(W \\Lambda W^T) = \\text{tr}(W^T W \\Lambda) = \\text{tr}(I \\Lambda) = \\text{tr}(\\Lambda)\n$$\nThe trace of the diagonal matrix $\\Lambda$ is the sum of its diagonal entries, which are the eigenvalues:\n$$\n\\text{Total Variance} = \\sum_{j=1}^{p} \\lambda_j\n$$\nThe total variance of the dataset is equal to the sum of the eigenvalues of its covariance matrix.\n\nThe variance explained by the first $k$ principal components is the sum of their individual variances:\n$$\n\\text{Variance explained by first } k \\text{ PCs} = \\sum_{j=1}^{k} \\lambda_j\n$$\nTherefore, the fraction of total variance explained by the first $k$ principal components is the ratio of the variance they explain to the total variance:\n$$\n\\text{Fraction of Variance} = \\frac{\\sum_{j=1}^{k} \\lambda_j}{\\sum_{j=1}^{p} \\lambda_j}\n$$\nThis completes the derivation.\n\n### Part 2: Calculation\n\nWe are given $p = 5$ features and asked to find the fraction of variance explained by the first $k=2$ principal components. The eigenvalues are provided in descending order:\n$\\lambda_{1} = 2.1$, $\\lambda_{2} = 1.3$, $\\lambda_{3} = 0.9$, $\\lambda_{4} = 0.5$, and $\\lambda_{5} = 0.2$.\n\nThe variance explained by the first $k=2$ principal components is the sum of the first two eigenvalues:\n$$\n\\sum_{j=1}^{2} \\lambda_j = \\lambda_1 + \\lambda_2 = 2.1 + 1.3 = 3.4\n$$\nThe total variance is the sum of all $p=5$ eigenvalues:\n$$\n\\sum_{j=1}^{5} \\lambda_j = \\lambda_1 + \\lambda_2 + \\lambda_3 + \\lambda_4 + \\lambda_5 = 2.1 + 1.3 + 0.9 + 0.5 + 0.2 = 5.0\n$$\nThe fraction of variance explained by the first $k=2$ components is:\n$$\n\\text{Fraction} = \\frac{\\sum_{j=1}^{2} \\lambda_j}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{3.4}{5.0} = 0.68\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n0.68 = 0.6800\n$$",
            "answer": "$$\\boxed{0.6800}$$"
        },
        {
            "introduction": "Predicting battery lifetime is a critical task, but experimental data is often plagued by outliers from sensor malfunctions or testing interruptions. This exercise  explores how the choice of a loss function during model training can make a predictive model robust to such anomalies. By deriving the gradient of the Huber loss, you will understand mathematically why it is less sensitive to large errors than the common squared error, a crucial insight for building reliable and stable degradation models from real-world data.",
            "id": "3926194",
            "problem": "In automated battery design and simulation, consider supervised regression to predict cycle life from high-fidelity simulation descriptors. Let the input descriptor vectors be $\\{x_i\\}_{i=1}^{N}$ with $x_i \\in \\mathbb{R}^{d}$, and the measured cycle lives be $\\{y_i\\}_{i=1}^{N}$, where each $y_i$ is the number of charge-discharge cycles until a specified end-of-life criterion. Assume a linear predictor $f_{\\theta}(x) = x^{\\top}\\theta$ with parameter vector $\\theta \\in \\mathbb{R}^{d}$ and residuals $r_i(\\theta) = x_i^{\\top}\\theta - y_i$. Measurement processes for $y_i$ are subject to occasional large errors due to test interruptions and sensor miscalibrations, yielding heavy-tailed residual distributions.\n\nStarting from the empirical risk minimization principle, define a robust loss that interpolates between the squared loss for small residuals and the absolute-value loss for large residuals. Using this definition as the data fidelity term, construct the empirical objective $J(\\theta)$ for the dataset $\\{(x_i, y_i)\\}_{i=1}^{N}$ and derive the gradient $\\nabla_{\\theta} J(\\theta)$. Your derivation must begin from fundamental definitions of loss-based risk and proceed by applying the chain rule for vector-valued parameters.\n\nExplain, using the structure of your derived gradient, why this loss is more robust to outliers than the pure squared loss in the presence of noisy battery life measurements. Explicitly characterize how the per-sample contribution to the gradient changes as $|r_i(\\theta)|$ grows and how this affects optimization stability in the face of extreme residuals.\n\nExpress your final answer as a single closed-form analytical expression for $\\nabla_{\\theta} J(\\theta)$ in terms of $x_i$, $y_i$, $\\theta$, and a positive threshold parameter $\\delta$, with $N$ samples. No numerical evaluation is required. Do not report intermediate results. If you introduce any auxiliary functions, define them within your derivation but ensure the final answer is written as a single expression. The final answer must be unitless and does not require rounding.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in standard machine learning principles, well-posed with a clear objective, and free from any listed invalidating flaws.\n\n**1. Definition of the Robust Loss Function**\n\nThe empirical risk minimization principle states that we seek a parameter vector $\\theta$ that minimizes an objective function $J(\\theta)$, defined as the average loss over a dataset.\n$$\nJ(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f_{\\theta}(x_i))\n$$\nThe problem requires a robust loss function that interpolates between the squared loss ($L_2$) for small residuals and the absolute-value loss ($L_1$) for large residuals. This is characteristic of the Huber loss, which is defined using a positive threshold parameter $\\delta$. Let the residual be $r = f_{\\theta}(x) - y = x^{\\top}\\theta - y$. The Huber loss, $L_{\\delta}(r)$, is given by:\n$$\nL_{\\delta}(r) =\n\\begin{cases}\n\\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n\\delta|r| - \\frac{1}{2}\\delta^2 & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThis function is quadratic for small residuals ($|r| \\le \\delta$), promoting efficiency, and linear for large residuals ($|r| > \\delta$), providing robustness to outliers. The term $-\\frac{1}{2}\\delta^2$ ensures that the function is continuous at the points $|r| = \\delta$. The first derivative of the loss with respect to the residual $r$ is also continuous:\n$$\n\\frac{dL_{\\delta}}{dr}(r) =\n\\begin{cases}\nr & \\text{if } |r| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(r) & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nwhere $\\text{sgn}(r)$ is the sign function.\n\n**2. Construction of the Empirical Objective and Gradient Derivation**\n\nUsing the Huber loss as the data fidelity term, the empirical objective function $J(\\theta)$ for the dataset $\\{(x_i, y_i)\\}_{i=1}^{N}$ is the average loss over all samples:\n$$\nJ(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L_{\\delta}(r_i(\\theta))\n$$\nwhere $r_i(\\theta) = x_i^{\\top}\\theta - y_i$.\n\nTo find the gradient $\\nabla_{\\theta} J(\\theta)$, we differentiate $J(\\theta)$ with respect to the vector $\\theta$. By linearity of the gradient operator:\n$$\n\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^{N} L_{\\delta}(r_i(\\theta)) \\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} L_{\\delta}(r_i(\\theta))\n$$\nWe apply the chain rule for vector-valued parameters to each term in the summation. The loss $L_{\\delta}$ is a scalar function of the scalar residual $r_i$, which in turn is a scalar function of the vector parameter $\\theta$.\n$$\n\\nabla_{\\theta} L_{\\delta}(r_i(\\theta)) = \\frac{dL_{\\delta}}{dr_i}(r_i(\\theta)) \\cdot \\nabla_{\\theta} r_i(\\theta)\n$$\nFirst, we compute the gradient of the residual $r_i(\\theta)$ with respect to $\\theta$:\n$$\nr_i(\\theta) = x_i^{\\top}\\theta - y_i\n$$\n$$\n\\nabla_{\\theta} r_i(\\theta) = \\nabla_{\\theta} (x_i^{\\top}\\theta - y_i) = x_i\n$$\nNow, substituting the derivatives back into the chain rule expression:\n$$\n\\nabla_{\\theta} L_{\\delta}(r_i(\\theta)) = \\left( \\begin{cases} r_i(\\theta) & \\text{if } |r_i(\\theta)| \\le \\delta \\\\ \\delta \\cdot \\text{sgn}(r_i(\\theta)) & \\text{if } |r_i(\\theta)| > \\delta \\end{cases} \\right) \\cdot x_i\n$$\nSubstituting $r_i(\\theta) = x_i^{\\top}\\theta - y_i$, the per-sample contribution to the gradient is:\n$$\n\\nabla_{\\theta} L_{\\delta}(r_i(\\theta)) =\n\\begin{cases}\n(x_i^{\\top}\\theta - y_i)x_i & \\text{if } |x_i^{\\top}\\theta - y_i| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(x_i^{\\top}\\theta - y_i) \\cdot x_i & \\text{if } |x_i^{\\top}\\theta - y_i| > \\delta\n\\end{cases}\n$$\nFinally, summing over all $N$ samples and dividing by $N$ gives the complete gradient of the objective function:\n$$\n\\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N}\n\\begin{cases}\n(x_i^{\\top}\\theta - y_i)x_i & \\text{if } |x_i^{\\top}\\theta - y_i| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(x_i^{\\top}\\theta - y_i) \\cdot x_i & \\text{if } |x_i^{\\top}\\theta - y_i| > \\delta\n\\end{cases}\n$$\n\n**3. Analysis of Robustness to Outliers**\n\nThe robustness of the Huber loss is evident from the structure of its gradient, especially when compared to the gradient of the pure squared loss (Mean Squared Error, MSE). For MSE, the loss for a single sample is $\\frac{1}{2}r_i^2$, and the corresponding gradient contribution is always $r_i(\\theta)x_i = (x_i^{\\top}\\theta - y_i)x_i$.\n\n-   **For small residuals ($|r_i(\\theta)| \\le \\delta$):** The gradient contribution for the Huber loss is $(x_i^{\\top}\\theta - y_i)x_i$. This is identical to the gradient contribution for the squared loss. In this regime, where data points are considered \"inliers\" and fit the model well, the optimization behaves like standard least squares.\n\n-   **For large residuals ($|r_i(\\theta)| > \\delta$):** This is the critical region for robustness.\n    -   For the **squared loss**, the magnitude of the gradient contribution, $\\|(x_i^{\\top}\\theta - y_i)x_i\\| = |r_i(\\theta)| \\|x_i\\|$, grows *linearly and without bound* as the residual magnitude $|r_i(\\theta)|$ increases. An outlier with a very large residual (due to sensor miscalibration, etc.) will generate a disproportionately large gradient. In an optimization algorithm like gradient descent, this single outlier can dominate the gradient update step, pulling the parameter vector $\\theta$ far from a solution that is optimal for the rest of the data. This leads to instability and a final model that is heavily skewed by outliers.\n\n    -   For the **Huber loss**, the magnitude of the gradient contribution becomes constant once the residual exceeds $\\delta$. The contribution is $\\delta \\cdot \\text{sgn}(x_i^{\\top}\\theta - y_i) \\cdot x_i$, and its magnitude is $\\|\\delta \\cdot \\text{sgn}(r_i(\\theta)) \\cdot x_i\\| = \\delta \\|x_i\\|$. This value is *independent* of the residual magnitude $|r_i(\\theta)|$. This means that an outlier with an extremely large residual contributes no more to the gradient update than a point whose residual just barely exceeds $\\delta$. The gradient is effectively \"clipped\".\n\nThis clipping mechanism prevents outliers from exerting an unbounded influence on the gradient. It ensures that the optimization process is more stable and that the final parameters $\\theta$ reflect the underlying trend of the majority of the data, rather than being distorted by a few anomalous measurements. Therefore, the Huber loss is more robust than the squared loss in the presence of heavy-tailed noise and outliers, such as the noisy battery life measurements described in the problem.",
            "answer": "$$ \\boxed{ \\frac{1}{N} \\sum_{i=1}^{N} \\begin{cases} (x_i^{\\top}\\theta - y_i)x_i & \\text{if } |x_i^{\\top}\\theta - y_i| \\le \\delta \\\\ \\delta \\, \\text{sgn}(x_i^{\\top}\\theta - y_i) x_i & \\text{if } |x_i^{\\top}\\theta - y_i| > \\delta \\end{cases} } $$"
        },
        {
            "introduction": "In automated battery characterization, not all data is created equal; it must conform to fundamental physical laws to be considered valid. Electrochemical Impedance Spectroscopy (EIS) data, for instance, must obey the principles of causality and linearity, which are mathematically embodied in the Kramers-Kronig relations. This hands-on coding challenge  tasks you with implementing a validation algorithm to programmatically enforce these physical constraints, a critical quality control step in any robust battery data pipeline.",
            "id": "3926141",
            "problem": "You are given the task of deriving and implementing a consistency validation for Electrochemical Impedance Spectroscopy (EIS) data in automated battery design and simulation. Let the impedance spectrum be denoted by the complex function $Z(\\omega) = Z'(\\omega) + j Z''(\\omega)$, where $\\omega$ is angular frequency in radians per second and $Z$ is the impedance in ohms. Assume that the battery electrode-electrolyte system is a Linear Time-Invariant (LTI) and causal input-output system in the small-signal limit, and that the time-domain impulse response $z(t)$ vanishes for $t < 0$. Under these assumptions, $Z(\\omega)$ arises as the Fourier transform of $z(t)$, the response function is analytic in the upper half-plane of complex frequency, and its real and imaginary parts are not independent.\n\nYour tasks are:\n- Starting from the definitions of causality and linearity for $z(t)$, the Fourier transform linking $z(t)$ to $Z(\\omega)$, and analyticity of causal response functions in the upper half-plane, state the Kramers–Kronig consistency conditions that connect $Z'(\\omega)$ and $Z''(\\omega)$ for functions that decay as $\\omega \\to \\infty$. Explicitly state the conditions for $Z(\\omega)$ and explain the role of the Cauchy Principal Value (PV) in these integrals.\n- Design an algorithmic validation test that, given discrete samples $\\{(\\omega_j, Z'(\\omega_j), Z''(\\omega_j))\\}_{j=1}^N$ on a nonuniform grid of positive frequencies, rejects spectra that violate the causality and linearity assumptions. Your validation test must:\n  - Use the Kramers–Kronig relations derived above to predict $Z'(\\omega)$ from $Z''(\\omega)$ and $Z''(\\omega)$ from $Z'(\\omega)$ via numerically approximated PV integrals on $[0,\\infty)$.\n  - Quantify discrepancy using a relative root-mean-square error for $Z'$ and for $Z''$; denote the tolerance by $\\tau$, expressed as a decimal (for example, $\\tau = 0.1$). A spectrum should be accepted only if both relative errors are less than or equal to $\\tau$.\n  - Enforce passivity via the positive-real condition $Z'(\\omega) \\ge 0$ for all sampled $\\omega$, up to a small numerical tolerance $\\delta$, expressed in ohms (for example, $\\delta = 10^{-9}$). If the condition is violated for any sample, the spectrum must be rejected.\n\nImplementation requirements:\n- Frequencies $\\omega$ must be in radians per second.\n- Impedance values must be in ohms.\n- Angles, if any, must be in radians.\n- The final output of your program must be a single line with a comma-separated list of booleans enclosed in square brackets and in the order of the test suite cases, for example, $[{\\tt True},{\\tt False},{\\tt True}]$.\n\nTest suite:\n- Case $1$ (happy path, Debye element): Use a single-pole (Debye) impedance $Z(\\omega) = \\dfrac{R}{1 + j \\omega \\tau}$ with $R = 4$ ohm and $\\tau = 0.02$ second. Use a nonuniform logarithmic frequency grid $\\omega_j$ spanning $[10^1, 10^5]$ radians per second with $N = 200$ points. Compute $Z'(\\omega) = \\dfrac{R}{1 + (\\omega \\tau)^2}$ and $Z''(\\omega) = -\\dfrac{R \\omega \\tau}{1 + (\\omega \\tau)^2}$.\n- Case $2$ (causality violation by imaginary-part corruption): Use the same $R$, $\\tau$, and $\\omega_j$ as Case $1$, the same $Z'(\\omega)$ as Case $1$, but corrupt the imaginary part by adding a large oscillatory component: $Z''_{\\text{corr}}(\\omega) = -\\dfrac{R \\omega \\tau}{1 + (\\omega \\tau)^2} + A \\cos\\!\\left(2\\pi \\dfrac{\\omega}{\\max_j \\omega_j}\\right)$ with $A = 1.0$ ohm.\n- Case $3$ (passivity violation): Use the same $R$, $\\tau$, and $\\omega_j$ as Case $1$, and the same $Z''(\\omega)$ as Case $1$. Modify the real part to introduce negative values over a mid-frequency band: define $Z'_{\\text{neg}}(\\omega_j) = Z'(\\omega_j)$ for all $j$ except for indices $j$ in the range $80 \\le j \\le 120$, for which $Z'_{\\text{neg}}(\\omega_j) = -0.5\\, Z'(\\omega_j)$.\n- Case $4$ (boundary condition: sparse grid): Use a sparse frequency grid $\\omega \\in \\{10^2, 10^3, 10^4, 10^5\\}$ radians per second with $N = 4$ points, and the Debye element $Z(\\omega) = \\dfrac{R}{1 + j \\omega \\tau}$ with $R = 4$ ohm and $\\tau = 0.02$ second. Optionally add small noise to both parts to reflect measurement imperfections: add $\\epsilon$ to both $Z'$ and $Z''$ where $\\epsilon$ is an independent random variable with zero mean and standard deviation $0.01$ ohm; in your implementation use a deterministic small perturbation so the output is reproducible.\n\nValidation parameters:\n- Use tolerance $\\tau = 0.1$ (decimal).\n- Use numerical positive-real tolerance $\\delta = 10^{-9}$ ohm.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\tt result1},{\\tt result2},{\\tt result3},{\\tt result4}]$, with each ${\\tt resultk}$ being either ${\\tt True}$ or ${\\tt False}$ corresponding to acceptance of the spectrum in Case $k$ under the specified criteria.",
            "solution": "The problem of validating Electrochemical Impedance Spectroscopy (EIS) data for consistency is a critical step in automated battery analysis. The core of this validation rests on the fundamental principles of causality and linearity for the electrochemical system. A system that is Linear and Time-Invariant (LTI) and causal must have a response function, in this case, the impedance $Z(\\omega)$, that satisfies the Kramers–Kronig (K-K) relations.\n\nThe problem is scientifically well-grounded, algorithmically well-posed, and free of ambiguity. It provides sufficient data and clear criteria for a unique, verifiable solution. Therefore, the problem is deemed valid, and we proceed to the solution.\n\n### Part 1: The Kramers–Kronig Relations\n\nAn electrochemical cell, in the small-signal AC limit, can be modeled as a linear time-invariant (LTI) system. The impulse response function, denoted by $z(t)$, represents the voltage response to a current impulse at time $t=0$. A fundamental property of physical systems is causality: the system cannot respond before the stimulus is applied. Mathematically, this is expressed as:\n$$\nz(t) = 0 \\quad \\text{for} \\quad t < 0\n$$\nThe impedance, $Z(\\omega)$, is defined as the Fourier transform of this impulse response:\n$$\nZ(\\omega) = \\int_{-\\infty}^{\\infty} z(t) e^{-j\\omega t} dt\n$$\nDue to causality, the integral's lower limit becomes $0$:\n$$\nZ(\\omega) = \\int_{0}^{\\infty} z(t) e^{-j\\omega t} dt\n$$\nThis mathematical form allows us to extend the definition of $Z(\\omega)$ into the complex frequency plane, $\\tilde{\\omega} = \\omega + j\\sigma$. For the integral to converge, we require that the response $z(t)$ does not grow exponentially. The transform becomes $Z(\\tilde{\\omega}) = \\int_{0}^{\\infty} z(t) e^{\\sigma t} e^{-j\\omega t} dt$. This integral is well-defined and analytic for $\\sigma > 0$, meaning $Z(\\tilde{\\omega})$ is an analytic function in the entire upper half of the complex frequency plane.\n\nThis analyticity is a powerful constraint. By Cauchy's integral theorem, the real and imaginary parts of an analytic function are related. For $Z(\\omega) = Z'(\\omega) + jZ''(\\omega)$, these relations are the Kramers–Kronig relations. Assuming that $Z(\\omega) \\to 0$ as $|\\omega| \\to \\infty$ and that the underlying time-domain response $z(t)$ is real, which implies $Z(-\\omega) = Z^*(\\omega)$ (i.e., $Z'$ is an even function and $Z''$ is an odd function of $\\omega$), the K-K relations connecting the real and imaginary parts of the impedance are:\n$$\nZ'(\\omega_0) = \\frac{2}{\\pi} \\text{PV} \\int_0^\\infty \\frac{\\omega Z''(\\omega)}{\\omega^2 - \\omega_0^2} d\\omega\n$$\n$$\nZ''(\\omega_0) = -\\frac{2\\omega_0}{\\pi} \\text{PV} \\int_0^\\infty \\frac{Z'(\\omega)}{\\omega^2 - \\omega_0^2} d\\omega\n$$\nThe notation $\\text{PV}$ denotes the Cauchy Principal Value of the integral. This is necessary because the integrands have a singularity at $\\omega = \\omega_0$, where the denominator vanishes. The principal value is a method of assigning a value to such an improper integral by taking a symmetric limit around the singularity:\n$$\n\\text{PV} \\int_a^b f(x) dx = \\lim_{\\epsilon \\to 0^+} \\left( \\int_a^{c-\\epsilon} f(x) dx + \\int_{c+\\epsilon}^b f(x) dx \\right)\n$$\nwhere $c$ is the location of the singularity. Physically, it rests on the assumption of symmetric cancellation of the diverging parts of the integral on either side of the pole.\n\n### Part 2: Algorithmic Validation Test\n\nThe validation test involves two main checks based on the provided discrete EIS data $\\{(\\omega_j, Z'(\\omega_j), Z''(\\omega_j))\\}_{j=1}^N$.\n\n**1. Passivity Check:**\nA passive electrical network cannot generate energy. For an impedance, this translates to the condition that its real part must be non-negative for all frequencies, $Z'(\\omega) \\ge 0$. Any violation of this, beyond a small numerical tolerance $\\delta$, indicates an unphysical system or measurement artifact. The test is:\nReject the spectrum if $Z'(\\omega_j) < -\\delta$ for any $j$.\n\n**2. K-K Consistency Check:**\nThis check verifies if the measured real and imaginary parts are consistent with the K-K relations. This is done by using one part of the spectrum to predict the other and then quantifying the difference.\n\nTo perform the numerical integration required by the K-K relations with discrete data, the principal value integrals must be handled carefully. A standard and robust technique is to reformulate the integrals to remove the singularity. This is achieved by subtracting and adding a term, which results in an equivalent, but numerically stable, expression:\n$$\nZ'(\\omega_k) = \\frac{2}{\\pi} \\int_0^\\infty \\frac{\\omega Z''(\\omega) - \\omega_k Z''(\\omega_k)}{\\omega^2 - \\omega_k^2} d\\omega\n$$\n$$\nZ''(\\omega_k) = -\\frac{2\\omega_k}{\\pi} \\int_0^\\infty \\frac{Z'(\\omega) - Z'(\\omega_k)}{\\omega^2 - \\omega_k^2} d\\omega\n$$\nThe integrands are now well-behaved at $\\omega = \\omega_k$. At this point, the value of the integrand can be found using L'Hôpital's rule. For the first integral, the integrand at $\\omega = \\omega_k$ is $\\frac{1}{\\pi} \\left( \\frac{Z''(\\omega_k)}{\\omega_k} + \\frac{dZ''}{d\\omega}\\Big|_{\\omega_k} \\right)$. For the second, it is $-\\frac{Z'(\\omega_k)}{\\pi} \\frac{dZ'}{d\\omega}\\Big|_{\\omega_k}$.\n\nThe algorithm proceeds as follows for a given experimental spectrum $({\\boldsymbol \\omega}, \\mathbf{Z'}, \\mathbf{Z''})$:\n1.  **Passivity Check**: Iterate through all $Z'_j$ values. If any $Z'_j < -\\delta$, the spectrum is invalid. Terminate and return `False`.\n2.  **Predict $Z'_{KK}$ from $Z''$**:\n    For each frequency $\\omega_k$ in the dataset:\n    a. Construct the integrand $f_j = \\frac{2}{\\pi} \\frac{\\omega_j Z''_j - \\omega_k Z''_k}{\\omega_j^2 - \\omega_k^2}$ for all $j \\neq k$.\n    b. At $j=k$, calculate the integrand's limit using a numerical approximation for the derivative $\\frac{dZ''}{d\\omega}|_{\\omega_k}$. This derivative is obtained using finite differences on the discrete data (e.g., via `numpy.gradient`).\n    c. Numerically integrate the constructed integrand array with respect to $\\omega$ using the trapezoidal rule (e.g., via `numpy.trapz`) to obtain the predicted value $Z'_{KK}(\\omega_k)$.\n    This yields the predicted spectrum $\\mathbf{Z'}_{KK}$.\n3.  **Predict $Z''_{KK}$ from $Z'$**:\n    Perform the analogous procedure to calculate the predicted spectrum $\\mathbf{Z''}_{KK}$ from the experimental $\\mathbf{Z'}$.\n4.  **Quantify Discrepancy**:\n    Calculate the relative root-mean-square error (rRMSE) for both the real and imaginary parts:\n    $$\n    err_{Z'} = \\sqrt{\\frac{\\sum_{j=1}^N (Z'_j - Z'_{KK}(\\omega_j))^2}{\\sum_{j=1}^N (Z'_j)^2}}\n    $$\n    $$\n    err_{Z''} = \\sqrt{\\frac{\\sum_{j=1}^N (Z''_j - Z''_{KK}(\\omega_j))^2}{\\sum_{j=1}^N (Z''_j)^2}}\n    $$\n5.  **Final Verdict**:\n    The spectrum is considered valid and consistent if and only if both errors are within the specified tolerance $\\tau$:\n    Accept if ($err_{Z'} \\le \\tau$ AND $err_{Z''} \\le \\tau$). Otherwise, reject.\n\nThis comprehensive procedure ensures that the supplied EIS data conforms to the fundamental physical constraints of passivity, causality, and linearity, making it suitable for further analysis or model fitting in an automated workflow.",
            "answer": "```python\nimport numpy as np\n\ndef compute_kk_real_from_imag(omega, z_imag):\n    \"\"\"\n    Computes the Kramers-Kronig transform to predict Z_real from Z_imag.\n    \n    Z_real(w_k) = (2/pi) * PV-Integral[ (w * Z_imag(w)) / (w^2 - w_k^2) dw ] from 0 to inf\n    \"\"\"\n    n_freqs = len(omega)\n    z_real_kk = np.zeros(n_freqs)\n    \n    # Pre-calculate derivative for use in the singularity term\n    d_z_imag_dw = np.gradient(z_imag, omega)\n\n    for k in range(n_freqs):\n        wk = omega[k]\n        zk_imag = z_imag[k]\n        \n        integrand = np.zeros(n_freqs)\n        for j in range(n_freqs):\n            if j == k:\n                # L'Hopital's rule for the singularity at w = wk\n                # limit of (w*z_imag(w) - wk*z_imag(wk)) / (w^2 - wk^2) as w -> wk\n                # is (z_imag(wk) + wk * d_z_imag_dw(wk)) / (2*wk)\n                integrand[j] = (1.0 / np.pi) * (zk_imag / wk + d_z_imag_dw[k])\n            else:\n                wj = omega[j]\n                zj_imag = z_imag[j]\n                integrand[j] = (2.0 / np.pi) * (wj * zj_imag - wk * zk_imag) / (wj**2 - wk**2)\n\n        z_real_kk[k] = np.trapz(integrand, omega)\n        \n    return z_real_kk\n\ndef compute_kk_imag_from_real(omega, z_real):\n    \"\"\"\n    Computes the Kramers-Kronig transform to predict Z_imag from Z_real.\n    \n    Z_imag(w_k) = (-2*w_k/pi) * PV-Integral[ Z_real(w) / (w^2 - w_k^2) dw ] from 0 to inf\n    \"\"\"\n    n_freqs = len(omega)\n    z_imag_kk = np.zeros(n_freqs)\n    \n    # Pre-calculate derivative for use in the singularity term\n    d_z_real_dw = np.gradient(z_real, omega)\n\n    for k in range(n_freqs):\n        wk = omega[k]\n        zk_real = z_real[k]\n        \n        integrand = np.zeros(n_freqs)\n        for j in range(n_freqs):\n            if j == k:\n                # L'Hopital's rule for the singularity at w = wk\n                # limit of (z_real(w) - z_real(wk)) / (w^2 - wk^2) as w -> wk\n                # is d_z_real_dw(wk) / (2*wk)\n                integrand[j] = d_z_real_dw[k] / (2.0 * wk)\n            else:\n                wj = omega[j]\n                zj_real = z_real[j]\n                integrand[j] = (zj_real - zk_real) / (wj**2 - wk**2)\n        \n        integral_val = np.trapz(integrand, omega)\n        z_imag_kk[k] = (-2.0 * wk / np.pi) * integral_val\n\n    return z_imag_kk\n\n\ndef kramers_kronig_validator(omega, z_real, z_imag, tau, delta):\n    \"\"\"\n    Validates EIS data for passivity and Kramers-Kronig consistency.\n    \"\"\"\n    # 1. Passivity Check\n    if np.any(z_real < -delta):\n        return False\n\n    # 2. K-K Consistency Check\n    # Predict Z_real from Z_imag\n    z_real_kk = compute_kk_real_from_imag(omega, z_imag)\n    \n    # Predict Z_imag from Z_real\n    z_imag_kk = compute_kk_imag_from_real(omega, z_real)\n\n    # 3. Quantify Discrepancy\n    # Add a small epsilon to denominator to prevent division by zero\n    epsilon = 1e-12\n    err_real_num = np.sum((z_real - z_real_kk)**2)\n    err_real_den = np.sum(z_real**2) + epsilon\n    err_real = np.sqrt(err_real_num / err_real_den)\n\n    err_imag_num = np.sum((z_imag - z_imag_kk)**2)\n    err_imag_den = np.sum(z_imag**2) + epsilon\n    err_imag = np.sqrt(err_imag_num / err_imag_den)\n\n    # 4. Final Verdict\n    return err_real <= tau and err_imag <= tau\n\n\ndef solve():\n    # Validation parameters from the problem\n    tau = 0.1\n    delta = 1e-9\n\n    # --- Test Case 1: Happy path, Debye element ---\n    R1 = 4.0\n    tau1 = 0.02\n    omega1 = np.logspace(1, 5, 200)\n    z_real1 = R1 / (1 + (omega1 * tau1)**2)\n    z_imag1 = -R1 * omega1 * tau1 / (1 + (omega1 * tau1)**2)\n    \n    # --- Test Case 2: Causality violation ---\n    R2, tau2, omega2 = R1, tau1, omega1\n    z_real2 = R2 / (1 + (omega2 * tau2)**2)\n    z_imag2_base = -R2 * omega2 * tau2 / (1 + (omega2 * tau2)**2)\n    A = 1.0\n    z_imag2 = z_imag2_base + A * np.cos(2 * np.pi * omega2 / np.max(omega2))\n\n    # --- Test Case 3: Passivity violation ---\n    R3, tau3, omega3 = R1, tau1, omega1\n    z_real3 = R3 / (1 + (omega3 * tau3)**2)\n    z_imag3 = -R3 * omega3 * tau3 / (1 + (omega3 * tau3)**2)\n    # 0-based indices for 80 <= j <= 120 (inclusive) is [79:120]\n    z_real3[79:120] = -0.5 * z_real3[79:120] \n    \n    # --- Test Case 4: Sparse grid with noise ---\n    R4, tau4 = R1, tau1\n    omega4 = np.array([1e2, 1e3, 1e4, 1e5])\n    z_real4_base = R4 / (1 + (omega4 * tau4)**2)\n    z_imag4_base = -R4 * omega4 * tau4 / (1 + (omega4 * tau4)**2)\n    # Add a small deterministic perturbation\n    perturbation = 0.01\n    z_real4 = z_real4_base + perturbation\n    z_imag4 = z_imag4_base + perturbation\n\n    test_cases = [\n        (omega1, z_real1, z_imag1),\n        (omega2, z_real2, z_imag2),\n        (omega3, z_real3, z_imag3),\n        (omega4, z_real4, z_imag4),\n    ]\n\n    results = []\n    for omega, z_real, z_imag in test_cases:\n        is_valid = kramers_kronig_validator(omega, z_real, z_imag, tau, delta)\n        results.append(is_valid)\n\n    # Format the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}