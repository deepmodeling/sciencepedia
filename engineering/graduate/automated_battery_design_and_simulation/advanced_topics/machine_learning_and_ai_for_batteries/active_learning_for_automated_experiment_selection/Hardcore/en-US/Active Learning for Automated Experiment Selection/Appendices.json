{
    "hands_on_practices": [
        {
            "introduction": "Bayesian optimization guides the search for an optimal experiment by balancing exploration and exploitation. The Expected Improvement (EI) acquisition function is a cornerstone of this approach, elegantly quantifying the value of sampling a new point based on its potential to surpass the best result found so far. This practice will guide you through deriving the closed-form expression for EI from first principles, providing a solid theoretical foundation for how a Gaussian Process surrogate model translates uncertainty into a concrete decision-making criterion. ",
            "id": "3892015",
            "problem": "An automated battery design and simulation workflow uses Bayesian optimization (BO) to actively select the next experiment that maximizes the post-formation capacity retention after $100$ cycles for a nickel-manganese-cobalt cathode composition. The surrogate model is a Gaussian process (GP) whose posterior predictive distribution for the capacity retention at a candidate composition $x^{\\ast}$ is approximately normal. Let the predicted retention at $x^{\\ast}$ be the random variable $Y$ with $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, where $\\mu$ and $\\sigma$ are the GP posterior predictive mean and standard deviation, respectively. The current best observed retention is $y_{\\text{best}}$. To implement a cautious exploration strategy, the acquisition is the Expected Improvement (EI) with a nonnegative improvement margin $\\xi$ defined by the random improvement $I = \\max\\{0, Y - (y_{\\text{best}} + \\xi)\\}$ for a maximization objective. Starting from the fundamental definition of mathematical expectation and the normal probability density function, derive a closed-form expression for the expected improvement $\\mathbb{E}[I]$ under the normal predictive model, and evaluate it for the following scientifically plausible values: $\\mu = 0.92$, $\\sigma = 0.03$, $y_{\\text{best}} = 0.90$, and $\\xi = 0.01$. Treat capacity retention as a dimensionless fraction. Round your final numerical answer to four significant figures and express it as a dimensionless fraction (no unit symbols in your final numeric value).",
            "solution": "The user wants to solve the following problem:\nAn automated battery design and simulation workflow uses Bayesian optimization (BO) to actively select the next experiment that maximizes the post-formation capacity retention after $100$ cycles for a nickel-manganese-cobalt cathode composition. The surrogate model is a Gaussian process (GP) whose posterior predictive distribution for the capacity retention at a candidate composition $x^{\\ast}$ is approximately normal. Let the predicted retention at $x^{\\ast}$ be the random variable $Y$ with $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, where $\\mu$ and $\\sigma$ are the GP posterior predictive mean and standard deviation, respectively. The current best observed retention is $y_{\\text{best}}$. To implement a cautious exploration strategy, the acquisition is the Expected Improvement (EI) with a nonnegative improvement margin $\\xi$ defined by the random improvement $I = \\max\\{0, Y - (y_{\\text{best}} + \\xi)\\}$ for a maximization objective. Starting from the fundamental definition of mathematical expectation and the normal probability density function, derive a closed-form expression for the expected improvement $\\mathbb{E}[I]$ under the normal predictive model, and evaluate it for the following scientifically plausible values: $\\mu = 0.92$, $\\sigma = 0.03$, $y_{\\text{best}} = 0.90$, and $\\xi = 0.01$. Treat capacity retention as a dimensionless fraction. Round your final numerical answer to four significant figures and express it as a dimensionless fraction (no unit symbols in your final numeric value).\n\n### Step 1: Extract Givens\n- The random variable for predicted retention at a candidate point is $Y$.\n- The distribution of $Y$ is normal: $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$.\n- $\\mu$ is the posterior predictive mean.\n- $\\sigma$ is the posterior predictive standard deviation.\n- $y_{\\text{best}}$ is the current best observed retention.\n- $\\xi$ is a nonnegative improvement margin.\n- The random improvement is defined as $I = \\max\\{0, Y - (y_{\\text{best}} + \\xi)\\}$.\n- The objective is to derive the closed-form expression for the expected improvement, $\\mathbb{E}[I]$.\n- The numerical values for evaluation are: $\\mu = 0.92$, $\\sigma = 0.03$, $y_{\\text{best}} = 0.90$, $\\xi = 0.01$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is validated against the specified criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the fields of machine learning (Bayesian optimization, Gaussian processes) and materials science (battery design). Expected Improvement is a standard acquisition function, and its derivation for a GP model is a canonical problem. The context and numerical values are scientifically plausible.\n- **Well-Posed**: The problem is mathematically well-defined. The random variable, its distribution, and the function to be evaluated are all specified clearly. A unique solution exists and can be derived from first principles.\n- **Objective**: The problem is stated using precise, objective, and formal mathematical language.\n\nAll other criteria for validity are also met. There are no contradictions, missing information, or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe expected improvement $\\mathbb{E}[I]$ is defined by the expectation of the improvement function $I = \\max\\{0, Y - (y_{\\text{best}} + \\xi)\\}$. The random variable $Y$ follows a normal distribution, $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with probability density function (PDF) $f_Y(y)$.\n\nThe definition of expectation is:\n$$ \\mathbb{E}[I] = \\int_{-\\infty}^{\\infty} \\max\\{0, y - (y_{\\text{best}} + \\xi)\\} f_Y(y) \\, dy $$\nLet us define a threshold value $y_{\\text{th}} = y_{\\text{best}} + \\xi$. The integrand is non-zero only when $y > y_{\\text{th}}$. Therefore, the integral simplifies to:\n$$ \\mathbb{E}[I] = \\int_{y_{\\text{th}}}^{\\infty} (y - y_{\\text{th}}) f_Y(y) \\, dy $$\nThe PDF of the normal distribution is given by:\n$$ f_Y(y) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) $$\nSubstituting this into the integral for $\\mathbb{E}[I]$:\n$$ \\mathbb{E}[I] = \\int_{y_{\\text{th}}}^{\\infty} (y - y_{\\text{th}}) \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) \\, dy $$\nTo solve this integral, we perform a change of variables to the standard normal distribution. Let $z = \\frac{y-\\mu}{\\sigma}$. This implies $y = \\mu + z\\sigma$ and $dy = \\sigma \\, dz$. The lower limit of integration changes from $y_{\\text{th}}$ to $z_{\\text{th}} = \\frac{y_{\\text{th}}-\\mu}{\\sigma}$. The upper limit remains $\\infty$.\n\nThe integral becomes:\n$$ \\mathbb{E}[I] = \\int_{z_{\\text{th}}}^{\\infty} (\\mu + z\\sigma - y_{\\text{th}}) \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\sigma \\, dz $$\nLet $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$ be the PDF of the standard normal distribution. The integral simplifies to:\n$$ \\mathbb{E}[I] = \\int_{z_{\\text{th}}}^{\\infty} ((\\mu - y_{\\text{th}}) + z\\sigma) \\phi(z) \\, dz $$\nWe can split this into two parts:\n$$ \\mathbb{E}[I] = (\\mu - y_{\\text{th}}) \\int_{z_{\\text{th}}}^{\\infty} \\phi(z) \\, dz + \\sigma \\int_{z_{\\text{th}}}^{\\infty} z \\phi(z) \\, dz $$\nLet's evaluate each part.\nThe first integral is the tail probability of the standard normal distribution. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution.\n$$ \\int_{z_{\\text{th}}}^{\\infty} \\phi(z) \\, dz = P(Z > z_{\\text{th}}) = 1 - \\Phi(z_{\\text{th}}) $$\nThe second integral can be solved directly:\n$$ \\int_{z_{\\text{th}}}^{\\infty} z \\phi(z) \\, dz = \\int_{z_{\\text{th}}}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\, dz $$\nUsing the substitution $u = -z^2/2$, for which $du = -z \\, dz$:\n$$ \\int z \\exp\\left(-\\frac{z^2}{2}\\right) \\, dz = -\\int \\exp(u) \\, du = -\\exp(u) = -\\exp\\left(-\\frac{z^2}{2}\\right) $$\nEvaluating the definite integral:\n$$ \\frac{1}{\\sqrt{2\\pi}} \\left[ -\\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{z_{\\text{th}}}^{\\infty} = \\frac{1}{\\sqrt{2\\pi}} \\left( 0 - \\left(-\\exp\\left(-\\frac{z_{\\text{th}}^2}{2}\\right)\\right) \\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_{\\text{th}}^2}{2}\\right) = \\phi(z_{\\text{th}}) $$\nCombining the two parts, the expression for expected improvement is:\n$$ \\mathbb{E}[I] = (\\mu - y_{\\text{th}})(1 - \\Phi(z_{\\text{th}})) + \\sigma \\phi(z_{\\text{th}}) $$\nLet's define the improvement difference $\\Delta = \\mu - y_{\\text{th}} = \\mu - (y_{\\text{best}} + \\xi)$. Then $z_{\\text{th}} = \\frac{y_{\\text{th}} - \\mu}{\\sigma} = -\\frac{\\Delta}{\\sigma}$.\nSubstituting this into the equation and using the properties of the standard normal distribution, $\\phi(-x) = \\phi(x)$ and $1 - \\Phi(-x) = \\Phi(x)$:\n$$ \\mathbb{E}[I] = \\Delta (1 - \\Phi(-\\Delta/\\sigma)) + \\sigma \\phi(-\\Delta/\\sigma) $$\n$$ \\mathbb{E}[I] = \\Delta \\Phi(\\Delta/\\sigma) + \\sigma \\phi(\\Delta/\\sigma) $$\nThis is the closed-form expression for Expected Improvement.\n\n### Numerical Evaluation\n\nNow we evaluate this expression for the given values: $\\mu = 0.92$, $\\sigma = 0.03$, $y_{\\text{best}} = 0.90$, and $\\xi = 0.01$.\n\nFirst, calculate the improvement difference $\\Delta$:\n$$ \\Delta = \\mu - (y_{\\text{best}} + \\xi) = 0.92 - (0.90 + 0.01) = 0.92 - 0.91 = 0.01 $$\nNext, calculate the standardized improvement $Z_{\\text{std}}$:\n$$ Z_{\\text{std}} = \\frac{\\Delta}{\\sigma} = \\frac{0.01}{0.03} = \\frac{1}{3} $$\nThe expression for $\\mathbb{E}[I]$ becomes:\n$$ \\mathbb{E}[I] = (0.01) \\cdot \\Phi\\left(\\frac{1}{3}\\right) + (0.03) \\cdot \\phi\\left(\\frac{1}{3}\\right) $$\nWe need the values of the standard normal PDF $\\phi(z)$ and CDF $\\Phi(z)$ at $z = 1/3$.\n$$ \\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) $$\n$$ \\phi\\left(\\frac{1}{3}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(1/3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{18}\\right) \\approx 0.377390 $$\n$$ \\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) \\, dt $$\nThe value of the CDF at $z=1/3$ from standard tables is:\n$$ \\Phi\\left(\\frac{1}{3}\\right) \\approx 0.630559 $$\nSubstituting these values into the expression for $\\mathbb{E}[I]$:\n$$ \\mathbb{E}[I] \\approx (0.01) \\cdot (0.630559) + (0.03) \\cdot (0.377390) $$\n$$ \\mathbb{E}[I] \\approx 0.00630559 + 0.0113217 $$\n$$ \\mathbb{E}[I] \\approx 0.01762729 $$\nThe problem requires the final answer to be rounded to four significant figures.\n$$ \\mathbb{E}[I] \\approx 0.01763 $$",
            "answer": "$$\n\\boxed{0.01763}\n$$"
        },
        {
            "introduction": "While Expected Improvement focuses on maximizing the objective function, an alternative strategy is to select experiments that maximally reduce our uncertainty about the system. This information-theoretic approach, rooted in Bayesian experimental design, aims to maximize the expected reduction in the posterior entropy of the model parameters. This coding exercise will challenge you to implement this strategy, revealing a powerful and elegant result: for a linear-Gaussian model, maximizing information gain simplifies to a straightforward computation involving the prior uncertainty and the proposed experimental design. ",
            "id": "3892001",
            "problem": "You are designing an automated experiment selection strategy for estimating parameters of a simplified electrochemical model relevant to automated battery design and simulation. Consider an unknown parameter vector $\\theta \\in \\mathbb{R}^n$ with a Gaussian prior distribution $p_0(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0)$. An experiment is characterized by a design that yields a scalar measurement modeled (after linearization around expected operating conditions) as $y = h^\\top \\theta + \\varepsilon$, where $h \\in \\mathbb{R}^n$ is the sensitivity vector associated with the design and $\\varepsilon$ is zero-mean Gaussian noise with variance $\\sigma^2$. You must select the experiment that maximizes the expected reduction in the posterior entropy of $\\theta$ after performing a single measurement.\n\nStart from the following fundamental base:\n- Bayes' theorem for updating beliefs: $p(\\theta \\mid y, h) \\propto p(y \\mid \\theta, h) \\, p_0(\\theta)$.\n- The Gaussian likelihood $p(y \\mid \\theta, h) = \\mathcal{N}(h^\\top \\theta, \\sigma^2)$ and Gaussian prior $p_0(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0)$ lead to a Gaussian posterior distribution.\n- The Shannon differential entropy of a continuous random variable with probability density function (PDF) $p(x)$ is defined as $H(X) = - \\int p(x) \\ln p(x) \\, dx$, with the natural logarithm, so entropies are in nats.\n\nYour task is to implement a program that, given a set of candidate sensitivity vectors $h$, a prior covariance $\\Sigma_0$, and a noise variance $\\sigma^2$, computes the expected reduction in entropy of $\\theta$ in nats for each candidate design and returns the index (using zero-based indexing) of the design that maximizes this expected reduction. If multiple designs attain the same maximum expected reduction, select the smallest index. The output of your program should be a single line containing the results for all test cases as a comma-separated list enclosed in square brackets; for example, [$r_1$,$r_2$,$r_3$].\n\nCompute entropies in nats. Angles do not appear in this problem. No physical unit outputs are required.\n\nTest suite:\n- Case $1$ (happy path, two-dimensional parameters, informative measurement):\n  - Prior covariance: $$\\Sigma_0 = \\begin{bmatrix} 0.04 & 0 \\\\ 0 & 0.09 \\end{bmatrix}$$\n  - Noise variance: $\\sigma^2 = 0.01$\n  - Candidate sensitivity vectors: $h^{(0)} = [0.8, 0.1]$, $h^{(1)} = [0.2, 1.0]$, $h^{(2)} = [0.5, 0.5]$\n- Case $2$ (boundary condition, very noisy measurement):\n  - Prior covariance: $$\\Sigma_0 = \\begin{bmatrix} 0.04 & 0 \\\\ 0 & 0.09 \\end{bmatrix}$$\n  - Noise variance: $\\sigma^2 = 10.0$\n  - Candidate sensitivity vectors: $h^{(0)} = [0.8, 0.1]$, $h^{(1)} = [0.2, 1.0]$, $h^{(2)} = [0.5, 0.5]$\n- Case $3$ (edge case, highly anisotropic prior):\n  - Prior covariance: $$\\Sigma_0 = \\begin{bmatrix} 0.0001 & 0 \\\\ 0 & 4.0 \\end{bmatrix}$$\n  - Noise variance: $\\sigma^2 = 0.5$\n  - Candidate sensitivity vectors: $h^{(0)} = [1.0, 0.0]$, $h^{(1)} = [0.0, 1.0]$, $h^{(2)} = [0.5, 0.5]$\n- Case $4$ (scalar parameter, includes a non-informative design):\n  - Prior covariance: $$\\Sigma_0 = \\begin{bmatrix} 1.0 \\end{bmatrix}$$\n  - Noise variance: $\\sigma^2 = 0.1$\n  - Candidate sensitivity vectors: $h^{(0)} = [0.0]$, $h^{(1)} = [0.1]$, $h^{(2)} = [2.0]$\n- Case $5$ (tie case, equal information across designs):\n  - Prior covariance: $$\\Sigma_0 = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}$$\n  - Noise variance: $\\sigma^2 = 0.5$\n  - Candidate sensitivity vectors: $h^{(0)} = [1.0, 0.0]$, $h^{(1)} = [0.0, 1.0]$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be the zero-based index of the selected design for the corresponding case. For example, [$r_1$,$r_2$,$r_3$,$r_4$,$r_5$].",
            "solution": "The user wants me to identify the optimal experiment design from a set of candidates to maximize the information gained about an unknown parameter vector $\\theta$. The problem is well-posed and scientifically grounded in Bayesian inference and information theory, specifically within the context of Bayesian optimal experimental design (BOED) for linear-Gaussian systems.\n\nThe core task is to select the experiment, characterized by a sensitivity vector $h$, that maximizes the expected reduction in the Shannon differential entropy of the parameter vector $\\theta$. The prior belief about $\\theta$ is given by a Gaussian distribution $p_0(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0)$. A single experiment yields a scalar measurement $y$ according to the linear model $y = h^\\top \\theta + \\varepsilon$, where the measurement noise $\\varepsilon$ is also Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe expected reduction in entropy is equivalent to the mutual information between the unknown parameters $\\theta$ and the measurement $y$, denoted $I(\\theta; y)$. By definition, mutual information can be expressed in two equivalent ways:\n$$I(\\theta; y) = H(\\theta) - H(\\theta|y)$$\n$$I(\\theta; y) = H(y) - H(y|\\theta)$$\nHere, $H(\\theta)$ is the prior entropy of the parameters, $H(\\theta|y)$ is the posterior entropy after observing $y$, $H(y)$ is the entropy of the marginal distribution of the measurement (the \"pre-posterior\" or evidence), and $H(y|\\theta)$ is the entropy of the measurement conditioned on the true parameters. For this problem, calculating $I(\\theta; y)$ via the second form, $H(y) - H(y|\\theta)$, is more direct. The goal is to find the design vector $h$ that maximizes this quantity.\n\nLet us first determine the required distributions and their entropies. All entropies are in nats.\n\n1.  **Conditional Entropy $H(y|\\theta)$**: The measurement model is $p(y | \\theta, h) = \\mathcal{N}(h^\\top \\theta, \\sigma^2)$. This is a univariate Gaussian distribution in $y$. The differential entropy of a Gaussian random variable with variance $\\sigma^2_z$ is $\\frac{1}{2} \\ln(2\\pi e \\sigma^2_z)$. Therefore, the entropy of $y$ conditioned on a specific $\\theta$ is constant:\n    $$H(y|\\theta) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)$$\n    This value is independent of both $\\theta$ and the chosen design $h$.\n\n2.  **Marginal Entropy $H(y)$**: The marginal distribution of the measurement $y$, $p(y|h)$, is obtained by integrating the joint distribution $p(y, \\theta|h)$ over all possible $\\theta$:\n    $$p(y|h) = \\int p(y|\\theta, h) p_0(\\theta) d\\theta$$\n    This is the evidence for the model. Since both the prior on $\\theta$ and the likelihood of $y$ are Gaussian, their convolution results in another Gaussian. The random variable $y$ is a sum of two independent Gaussian random variables, $h^\\top\\theta$ and $\\varepsilon$. The mean of $y$ is $\\mathbb{E}[y] = \\mathbb{E}[h^\\top\\theta + \\varepsilon] = h^\\top\\mathbb{E}[\\theta] + \\mathbb{E}[\\varepsilon] = h^\\top\\mu_0$. The variance of $y$ is:\n    $$\\text{Var}(y) = \\text{Var}(h^\\top\\theta + \\varepsilon) = \\text{Var}(h^\\top\\theta) + \\text{Var}(\\varepsilon)$$\n    Using the properties of variance, $\\text{Var}(h^\\top\\theta) = h^\\top \\text{Cov}(\\theta) h = h^\\top \\Sigma_0 h$. The noise variance is $\\text{Var}(\\varepsilon) = \\sigma^2$. Thus, the marginal distribution of $y$ is Gaussian:\n    $$p(y|h) = \\mathcal{N}(h^\\top\\mu_0, h^\\top\\Sigma_0 h + \\sigma^2)$$\n    The entropy of this distribution is:\n    $$H(y) = \\frac{1}{2} \\ln\\left(2\\pi e (h^\\top\\Sigma_0 h + \\sigma^2)\\right)$$\n\n3.  **Mutual Information / Expected Entropy Reduction**: Now we can compute the mutual information for a given design $h$:\n    $$I(\\theta; y) = H(y) - H(y|\\theta)$$\n    $$I(\\theta; y) = \\frac{1}{2} \\ln\\left(2\\pi e (h^\\top\\Sigma_0 h + \\sigma^2)\\right) - \\frac{1}{2} \\ln\\left(2\\pi e \\sigma^2\\right)$$\n    Using the property $\\ln(a) - \\ln(b) = \\ln(a/b)$, this simplifies to:\n    $$I(\\theta; y) = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e (h^\\top\\Sigma_0 h + \\sigma^2)}{2\\pi e \\sigma^2}\\right) = \\frac{1}{2} \\ln\\left(\\frac{h^\\top\\Sigma_0 h + \\sigma^2}{\\sigma^2}\\right)$$\n    $$I(\\theta; y) = \\frac{1}{2} \\ln\\left(1 + \\frac{h^\\top\\Sigma_0 h}{\\sigma^2}\\right)$$\n    This final expression represents the expected reduction in entropy of $\\theta$ achieved by performing the experiment defined by $h$.\n\n4.  **Optimal Design Selection**: The task is to choose the sensitivity vector $h$ from a set of candidates $\\{h^{(i)}\\}$ that maximizes this information gain.\n    $$\\text{argmax}_{h^{(i)}} \\; \\frac{1}{2} \\ln\\left(1 + \\frac{(h^{(i)})^\\top\\Sigma_0 h^{(i)}}{\\sigma^2}\\right)$$\n    The natural logarithm $\\ln(x)$ is a strictly monotonically increasing function. Therefore, maximizing the expression is equivalent to maximizing its argument, $1 + \\frac{(h^{(i)})^\\top\\Sigma_0 h^{(i)}}{\\sigma^2}$. Since $\\sigma^2$ is a positive constant for any given test case, this is further equivalent to maximizing the quadratic form $(h^{(i)})^\\top\\Sigma_0 h^{(i)}$.\n\n    The quantity $h^\\top\\Sigma_0 h$ has a clear physical interpretation: it is the prior variance of the signal part of the measurement, $h^\\top\\theta$. Therefore, the optimal experiment is the one that probes the system in a direction where the parameter uncertainty (as described by $\\Sigma_0$) leads to the greatest uncertainty in the expected measurement outcome. This maximizes the potential for the measurement to be \"surprising\" and thus informative.\n\nThe algorithm is as follows: For each candidate vector $h^{(i)}$, compute the scalar value $q_i = (h^{(i)})^\\top \\Sigma_0 h^{(i)}$. The optimal design is the one corresponding to the largest value of $q_i$. In case of a tie, the problem specifies selecting the design with the smallest index. Note that the prior mean $\\mu_0$ is not required for this calculation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal experiment design problem for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"Sigma0\": np.array([[0.04, 0.0], [0.0, 0.09]]),\n            \"sigma_sq\": 0.01,\n            \"h_candidates\": [\n                np.array([0.8, 0.1]),\n                np.array([0.2, 1.0]),\n                np.array([0.5, 0.5])\n            ]\n        },\n        {\n            \"Sigma0\": np.array([[0.04, 0.0], [0.0, 0.09]]),\n            \"sigma_sq\": 10.0,\n            \"h_candidates\": [\n                np.array([0.8, 0.1]),\n                np.array([0.2, 1.0]),\n                np.array([0.5, 0.5])\n            ]\n        },\n        {\n            \"Sigma0\": np.array([[0.0001, 0.0], [0.0, 4.0]]),\n            \"sigma_sq\": 0.5,\n            \"h_candidates\": [\n                np.array([1.0, 0.0]),\n                np.array([0.0, 1.0]),\n                np.array([0.5, 0.5])\n            ]\n        },\n        {\n            \"Sigma0\": np.array([[1.0]]),\n            \"sigma_sq\": 0.1,\n            \"h_candidates\": [\n                np.array([0.0]),\n                np.array([0.1]),\n                np.array([2.0])\n            ]\n        },\n        {\n            \"Sigma0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"sigma_sq\": 0.5,\n            \"h_candidates\": [\n                np.array([1.0, 0.0]),\n                np.array([0.0, 1.0])\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        Sigma0 = case[\"Sigma0\"]\n        h_candidates = case[\"h_candidates\"]\n        \n        # As derived, maximizing the expected entropy reduction is equivalent to\n        # maximizing the quadratic form h^T * Sigma0 * h.\n        # The noise variance sigma^2 does not affect the choice of the optimal h.\n        \n        q_values = []\n        for h in h_candidates:\n            # h is a 1D array, so we can use np.dot for h.T @ Sigma0 and then another dot.\n            # Or just use the @ operator which handles it correctly.\n            q = h.T @ Sigma0 @ h\n            q_values.append(q)\n            \n        # np.argmax returns the index of the first occurrence of the maximum value,\n        # which satisfies the tie-breaking rule (select the smallest index).\n        best_index = np.argmax(q_values)\n        results.append(best_index)\n\n    # Format the output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In many real-world engineering applications, simply maximizing a performance metric is insufficient; we must also manage downside risk and experimental cost. This exercise introduces a risk-sensitive approach to experiment selection by using the Conditional Value at Risk (CVaR) as a utility function, focusing on improving the worst-case outcomes of a battery's cycle life. By integrating Bayesian posterior updates with this sophisticated risk measure and an explicit cost penalty, you will learn how to design an active learning strategy that makes pragmatic, cost-aware decisions under uncertainty. ",
            "id": "3892012",
            "problem": "Consider an automated battery design workflow where the cycle life of a cell under a fixed protocol is characterized by a random variable $L$ measured in cycles. Before any experiment, assume a Bayesian prior $L \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$ with $\\mu_0$ cycles and $\\sigma_0$ cycles as parameters. An available experiment $i$ produces a single noisy measurement $y_i$ modeled as $y_i = L + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0, \\tau_i^2)$ where $\\tau_i$ is the known measurement noise standard deviation in cycles. Performing experiment $i$ incurs a monetary cost $c_i$ measured in dollars. After performing exactly one experiment, the system is deployed without further experimentation, and risk is assessed using the lower-tail Conditional Value at Risk (CVaR) at level $\\alpha \\in (0,1)$ of the cycle life distribution. The risk-sensitive utility for experiment $i$ is defined as the lower-tail Conditional Value at Risk (CVaR) of the cycle life after incorporating the information from experiment $i$, minus a cost penalty $\\lambda c_i$, where $\\lambda$ is a positive scalar with units of cycles per dollar that converts monetary cost into an equivalent cycle penalty.\n\nFundamental base:\n- Use Bayes' rule with Gaussian priors and Gaussian likelihoods, and properties of the Normal distribution, to determine the posterior distribution of $L$ after observing one measurement $y_i$ with noise variance $\\tau_i^2$.\n- The lower-tail Value at Risk at level $\\alpha$ for a random variable $X$ is $\\mathrm{VaR}_{\\alpha}^{\\mathrm{lower}}(X)$ defined as the $\\alpha$-quantile of $X$, that is, $\\mathrm{VaR}_{\\alpha}^{\\mathrm{lower}}(X) = F_X^{-1}(\\alpha)$ where $F_X$ is the cumulative distribution function.\n- The lower-tail Conditional Value at Risk (CVaR) at level $\\alpha$ for a random variable $X$ is the conditional expectation $\\mathrm{CVaR}_{\\alpha}^{\\mathrm{lower}}(X) = \\mathbb{E}[X \\mid X \\le \\mathrm{VaR}_{\\alpha}^{\\mathrm{lower}}(X)]$.\n- The standard Normal probability density function is $\\phi(x)$ and the standard Normal cumulative distribution function is $\\Phi(x)$, and the inverse cumulative distribution function (quantile function) is $\\Phi^{-1}(p)$.\n\nTask:\n- For each candidate experiment $i$ with $(\\tau_i, c_i)$, use the Gaussian update to obtain the posterior distribution of $L$ after one measurement from experiment $i$. Then compute the lower-tail Conditional Value at Risk (CVaR) at level $\\alpha$ for the posterior $L$ distribution, and subtract the cost penalty $\\lambda c_i$ to obtain a risk-sensitive utility $U_i$ in cycles. Select the experiment index $i^\\star$ that maximizes $U_i$.\n- Indexing must be zero-based: the first experiment is index $0$, the second is index $1$, and so on.\n- For each test case, output a list with two entries: the selected experiment index $i^\\star$ (an integer), and the corresponding maximum risk-sensitive utility $U_{i^\\star}$ (a float). Express $U_{i^\\star}$ in cycles and round it to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path): $\\mu_0 = 800$ cycles, $\\sigma_0 = 120$ cycles, $\\alpha = 0.1$, $\\lambda = 0.2$ cycles per dollar, experiments $[(\\tau_0, c_0), (\\tau_1, c_1), (\\tau_2, c_2)] = [(200, 500), (80, 900), (50, 2000)]$ with $\\tau_i$ in cycles and $c_i$ in dollars.\n- Case $2$ (risk-sensitive edge): $\\mu_0 = 1000$ cycles, $\\sigma_0 = 300$ cycles, $\\alpha = 0.01$, $\\lambda = 0.1$ cycles per dollar, experiments $[(\\tau_0, c_0), (\\tau_1, c_1), (\\tau_2, c_2)] = [(500, 100), (50, 5000), (150, 800)]$.\n- Case $3$ (cost-dominant boundary): $\\mu_0 = 900$ cycles, $\\sigma_0 = 200$ cycles, $\\alpha = 0.2$, $\\lambda = 0.5$ cycles per dollar, experiments $[(\\tau_0, c_0), (\\tau_1, c_1), (\\tau_2, c_2)] = [(10^9, 100), (60, 10000), (100, 2000)]$.\n- Case $4$ (high prior confidence): $\\mu_0 = 700$ cycles, $\\sigma_0 = 30$ cycles, $\\alpha = 0.05$, $\\lambda = 0.3$ cycles per dollar, experiments $[(\\tau_0, c_0), (\\tau_1, c_1), (\\tau_2, c_2)] = [(40, 3000), (20, 5000), (100, 500)]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each test case result is itself a two-element list in the form $[i^\\star, U_{i^\\star}]$. For example: $[[i_1,U_1],[i_2,U_2],[i_3,U_3],[i_4,U_4]]$.",
            "solution": "The problem requires selecting the optimal experiment to characterize the cycle life of a battery cell. The selection criterion is a risk-sensitive utility function that balances the informational gain from an experiment against its monetary cost.\n\nFirst, a validation of the problem statement is necessary. The problem defines all parameters, distributions, and objectives using standard, scientifically-grounded concepts from Bayesian statistics and risk theory. The core of the problem lies in evaluating the utility of an experiment, which is defined based on the posterior distribution of the cycle life $L$. A critical observation is that the posterior distribution, specifically its mean, depends on the measurement outcome $y_i$, which is unknown at the time of selecting the experiment. A decision criterion must be based on information available *before* the experiment is conducted. The only scientifically rigorous interpretation is that the utility of experiment $i$ is its *expected* utility, where the expectation is taken over the prior predictive distribution of the measurement $y_i$. This pre-posterior analysis is a standard approach in Bayesian experimental design. With this interpretation, the problem is well-posed and solvable.\n\nLet us proceed with the derivation.\n\nThe cycle life $L$ is a random variable with a Gaussian prior distribution:\n$$ L \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2) $$\nAn experiment $i$ yields a measurement $y_i$ with a Gaussian likelihood:\n$$ y_i | L \\sim \\mathcal{N}(L, \\tau_i^2) $$\nwhere $\\tau_i$ is the known measurement noise standard deviation.\n\nUsing Bayes' rule for conjugate Gaussian distributions, the posterior distribution of $L$ given the measurement $y_i$ is also Gaussian, $L|y_i \\sim \\mathcal{N}(\\mu_{post, i}, \\sigma_{post, i}^2)$. The posterior parameters are determined as follows:\n\nThe posterior precision (inverse variance) is the sum of the prior precision and the likelihood precision:\n$$ \\frac{1}{\\sigma_{post, i}^2} = \\frac{1}{\\sigma_0^2} + \\frac{1}{\\tau_i^2} $$\nSolving for the posterior variance $\\sigma_{post, i}^2$ gives:\n$$ \\sigma_{post, i}^2 = \\left( \\frac{\\sigma_0^2 + \\tau_i^2}{\\sigma_0^2 \\tau_i^2} \\right)^{-1} = \\frac{\\sigma_0^2 \\tau_i^2}{\\sigma_0^2 + \\tau_i^2} $$\nThe posterior standard deviation is therefore:\n$$ \\sigma_{post, i} = \\sqrt{\\frac{\\sigma_0^2 \\tau_i^2}{\\sigma_0^2 + \\tau_i^2}} $$\nNotice that $\\sigma_{post, i}$ depends only on prior and experiment parameters, not on the measurement $y_i$.\n\nThe posterior mean $\\mu_{post, i}$ is a precision-weighted average of the prior mean $\\mu_0$ and the measurement $y_i$:\n$$ \\mu_{post, i} = \\sigma_{post, i}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{y_i}{\\tau_i^2} \\right) $$\nThis mean depends on the yet-unknown outcome $y_i$.\n\nThe utility function involves the lower-tail Conditional Value at Risk (CVaR) at level $\\alpha$ of the posterior distribution. For a normally distributed random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the lower-tail CVaR is given by:\n$$ \\mathrm{CVaR}_{\\alpha}^{\\mathrm{lower}}(X) = \\mu - \\sigma \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha} $$\nwhere $\\phi(\\cdot)$ and $\\Phi(\\cdot)$ are the probability density function and cumulative distribution function of the standard normal distribution, respectively.\n\nApplying this formula to the posterior distribution $L|y_i$, we obtain the CVaR, which is a function of $y_i$:\n$$ \\mathrm{CVaR}_{\\alpha}^{\\mathrm{lower}}(L|y_i) = \\mu_{post, i} - \\sigma_{post, i} \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha} $$\nThe risk-sensitive utility for experiment $i$ after observing $y_i$ is defined as this CVaR minus the cost penalty $\\lambda c_i$:\n$$ U_i(y_i) = \\mu_{post, i} - \\sigma_{post, i} \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha} - \\lambda c_i $$\nTo select an experiment a priori, we must compute the expected utility $U_i = \\mathbb{E}_{y_i}[U_i(y_i)]$. The expectation is taken over the prior predictive distribution of $y_i$. This distribution is found by integrating the product of the prior and the likelihood over all possible values of $L$:\n$$ p(y_i) = \\int_{-\\infty}^{\\infty} p(y_i|L) p(L) dL $$\nThe convolution of two Gaussian distributions is another Gaussian. Thus, $y_i$ follows a normal distribution with mean $\\mu_0$ and variance $\\sigma_0^2 + \\tau_i^2$:\n$$ y_i \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2 + \\tau_i^2) $$\nThe expected utility is:\n$$ U_i = \\mathbb{E}_{y_i} \\left[ \\mu_{post, i} - \\sigma_{post, i} \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha} - \\lambda c_i \\right] $$\nBy linearity of expectation, this becomes:\n$$ U_i = \\mathbb{E}_{y_i}[\\mu_{post, i}] - \\sigma_{post, i} \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha} - \\lambda c_i $$\nWe compute the expectation of the posterior mean:\n$$ \\mathbb{E}_{y_i}[\\mu_{post, i}] = \\mathbb{E}_{y_i} \\left[ \\sigma_{post, i}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{y_i}{\\tau_i^2} \\right) \\right] = \\sigma_{post, i}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{\\mathbb{E}_{y_i}[y_i]}{\\tau_i^2} \\right) $$\nSince $\\mathbb{E}_{y_i}[y_i] = \\mu_0$, we substitute this in:\n$$ \\mathbb{E}_{y_i}[\\mu_{post, i}] = \\sigma_{post, i}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{\\mu_0}{\\tau_i^2} \\right) = \\mu_0 \\sigma_{post, i}^2 \\left( \\frac{1}{\\sigma_0^2} + \\frac{1}{\\tau_i^2} \\right) $$\nUsing the identity $1/\\sigma_{post, i}^2 = 1/\\sigma_0^2 + 1/\\tau_i^2$, we find that the expected posterior mean is simply the prior mean:\n$$ \\mathbb{E}_{y_i}[\\mu_{post, i}] = \\mu_0 \\sigma_{post, i}^2 \\left( \\frac{1}{\\sigma_{post, i}^2} \\right) = \\mu_0 $$\nTherefore, the final expression for the utility of experiment $i$, which can be evaluated before any measurement is taken, is:\n$$ U_i = \\mu_0 - \\sigma_{post, i} \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha} - \\lambda c_i $$\nwhere $\\sigma_{post, i} = \\sqrt{\\frac{\\sigma_0^2 \\tau_i^2}{\\sigma_0^2 + \\tau_i^2}}$.\n\nThe algorithm to solve the problem is as follows:\n1.  For each test case, retrieve the parameters $\\mu_0$, $\\sigma_0$, $\\alpha$, $\\lambda$, and the list of experiments $[(\\tau_i, c_i)]$.\n2.  Calculate the constant factor $K = \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha}$, which depends only on the risk level $\\alpha$.\n3.  For each experiment $i$ in the list:\n    a. Calculate the posterior standard deviation $\\sigma_{post, i} = \\sqrt{(\\sigma_0^2 \\tau_i^2) / (\\sigma_0^2 + \\tau_i^2)}$.\n    b. Calculate the utility $U_i = \\mu_0 - \\sigma_{post, i} \\cdot K - \\lambda c_i$.\n4.  Identify the experiment index $i^\\star$ that yields the maximum utility $U_{i^\\star} = \\max_i \\{U_i\\}$.\n5.  Return the pair $[i^\\star, U_{i^\\star}]$, with $U_{i^\\star}$ rounded to $6$ decimal places.\nThis procedure will be applied to each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the automated battery experiment selection problem for multiple test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"mu0\": 800.0, \"sigma0\": 120.0, \"alpha\": 0.1, \"lambda_\": 0.2,\n            \"experiments\": [(200.0, 500.0), (80.0, 900.0), (50.0, 2000.0)]\n        },\n        {\n            \"mu0\": 1000.0, \"sigma0\": 300.0, \"alpha\": 0.01, \"lambda_\": 0.1,\n            \"experiments\": [(500.0, 100.0), (50.0, 5000.0), (150.0, 800.0)]\n        },\n        {\n            \"mu0\": 900.0, \"sigma0\": 200.0, \"alpha\": 0.2, \"lambda_\": 0.5,\n            \"experiments\": [(1e9, 100.0), (60.0, 10000.0), (100.0, 2000.0)]\n        },\n        {\n            \"mu0\": 700.0, \"sigma0\": 30.0, \"alpha\": 0.05, \"lambda_\": 0.3,\n            \"experiments\": [(40.0, 3000.0), (20.0, 5000.0), (100.0, 500.0)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu0 = case[\"mu0\"]\n        sigma0 = case[\"sigma0\"]\n        alpha = case[\"alpha\"]\n        lambda_ = case[\"lambda_\"]\n        experiments = case[\"experiments\"]\n\n        # Pre-calculate the constant factor for CVaR\n        # K = phi(Phi^-1(alpha)) / alpha\n        z_alpha = norm.ppf(alpha)\n        phi_z_alpha = norm.pdf(z_alpha)\n        cvar_factor = phi_z_alpha / alpha\n\n        max_utility = -np.inf\n        best_index = -1\n\n        sigma0_sq = sigma0 ** 2\n\n        for i, (tau_i, c_i) in enumerate(experiments):\n            tau_i_sq = tau_i ** 2\n\n            # Calculate the posterior standard deviation\n            sigma_post_i_sq = (sigma0_sq * tau_i_sq) / (sigma0_sq + tau_i_sq)\n            sigma_post_i = np.sqrt(sigma_post_i_sq)\n\n            # Calculate the expected risk-sensitive utility\n            # U_i = E[mu_post] - sigma_post * K - lambda * c\n            # E[mu_post] is mu0\n            utility = mu0 - sigma_post_i * cvar_factor - lambda_ * c_i\n\n            if utility > max_utility:\n                max_utility = utility\n                best_index = i\n        \n        # Round the final utility to 6 decimal places as required\n        rounded_utility = round(max_utility, 6)\n        results.append([best_index, rounded_utility])\n\n    # Format the final output string\n    output_parts = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n\n```"
        }
    ]
}