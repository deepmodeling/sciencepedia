{
    "hands_on_practices": [
        {
            "introduction": "A Gaussian Process surrogate is defined by its kernel function, whose hyperparameters, such as the lengthscale $\\ell$, dictate the model's behavior. To create an effective surrogate, these hyperparameters must be tuned to the data. This exercise guides you through the foundational process of deriving the gradient of the log-marginal likelihood, which is the objective function used to fit the model . Mastering this derivation provides a deep understanding of how gradient-based optimization is used to automatically learn a GP's properties from data, a critical skill for building robust surrogates.",
            "id": "3915981",
            "problem": "In an automated lithium-ion battery cell design workflow, a high-fidelity electrochemical simulation produces training data for a surrogate of the discharge capacity as a function of design variables. Let $X \\in \\mathbb{R}^{n \\times d}$ denote $n$ simulated designs (each row $x_{i} \\in \\mathbb{R}^{d}$ encodes features such as porosity, active material fraction, calendering pressure, and separator tortuosity), and let $y \\in \\mathbb{R}^{n}$ be the corresponding standardized discharge capacities (zero mean and unit variance). Consider a Gaussian Process (GP) regression surrogate with zero mean and an isotropic squared-exponential kernel $k(x, x^{\\prime}) = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{\\|x - x^{\\prime}\\|_{2}^{2}}{2 \\ell^{2}}\\right)$, and additive independent Gaussian noise of variance $\\sigma_{n}^{2}$. The covariance matrix is $K = K_{y} + \\sigma_{n}^{2} I$, where $[K_{y}]_{ij} = k(x_{i}, x_{j})$, $I$ is the identity, and the hyperparameters are $\\theta = \\{\\ell, \\sigma_{f}^{2}, \\sigma_{n}^{2}\\}$.\n\nStarting only from the definition of the multivariate Gaussian log-likelihood for $y \\sim \\mathcal{N}(0, K)$, use matrix calculus to derive the gradient $\\frac{\\partial \\log p(y \\mid X, \\theta)}{\\partial \\ell}$ with respect to the kernel lengthscale $\\ell$. Your derivation must explicitly use identities for the derivative of a matrix inverse and the derivative of the log-determinant, and must keep all constants in symbolic form. Then, specialize the result to the isotropic squared-exponential kernel by writing the result in terms of pairwise Euclidean distances $d_{ij} = \\|x_{i} - x_{j}\\|_{2}$, the vector $\\alpha = K^{-1} y$, and the matrix $K^{-1}$.\n\nIn a brief paragraph, explain how this gradient enables gradient-based hyperparameter optimization in practice for uncertainty-aware surrogates in automated battery design, including how to enforce the positivity constraint on $\\ell$ and how to compute the required linear algebra stably.\n\nExpress your final gradient as a single closed-form analytic expression involving a double summation over indices $i$ and $j$. No numerical evaluation is required. Do not include physical units in your final expression. No rounding is required.",
            "solution": "The problem requires the derivation of the gradient of the log-marginal likelihood of a Gaussian Process (GP) with respect to the kernel lengthscale hyperparameter $\\ell$, followed by an explanation of its use in hyperparameter optimization.\n\nThe observed data vector $y \\in \\mathbb{R}^{n}$ is modeled as being drawn from a multivariate Gaussian distribution with zero mean and covariance matrix $K$, i.e., $y \\sim \\mathcal{N}(0, K)$. The log-marginal likelihood, denoted as $\\mathcal{L}(\\theta) = \\log p(y \\mid X, \\theta)$, is given by:\n$$\n\\mathcal{L}(\\theta) = \\log p(y \\mid K) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log|K| - \\frac{1}{2} y^T K^{-1} y\n$$\nHere, $K$ is the covariance matrix, which depends on the hyperparameters $\\theta = \\{\\ell, \\sigma_f^2, \\sigma_n^2\\}$. We are tasked with finding the partial derivative of $\\mathcal{L}(\\theta)$ with respect to the lengthscale $\\ell$.\n\nFirst, we differentiate $\\mathcal{L}(\\theta)$ with respect to $\\ell$. The first term, $-\\frac{n}{2} \\log(2\\pi)$, is a constant with respect to $\\ell$, so its derivative is zero. The remaining two terms depend on $\\ell$ through the matrix $K$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = -\\frac{1}{2} \\frac{\\partial}{\\partial \\ell} (\\log|K|) - \\frac{1}{2} \\frac{\\partial}{\\partial \\ell} (y^T K^{-1} y)\n$$\nWe apply the chain rule for matrix derivatives. For a generic scalar function of a matrix $f(K)$, its derivative with respect to a scalar parameter $\\lambda$ is given by $\\frac{\\partial f(K)}{\\partial \\lambda} = \\text{Tr}\\left( \\frac{\\partial f(K)}{\\partial K} \\frac{\\partial K}{\\partial \\lambda} \\right)$, assuming $K$ is symmetric.\n\nFor the log-determinant term, we use the identity $\\frac{\\partial \\log|K|}{\\partial K} = K^{-1}$. Applying the chain rule:\n$$\n\\frac{\\partial}{\\partial \\ell} (\\log|K|) = \\text{Tr}\\left( \\frac{\\partial \\log|K|}{\\partial K} \\frac{\\partial K}{\\partial \\ell} \\right) = \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right)\n$$\nFor the quadratic form term, we first use the identity for the derivative of a matrix inverse: $\\frac{\\partial K^{-1}}{\\partial \\lambda} = -K^{-1} \\frac{\\partial K}{\\partial \\lambda} K^{-1}$. Applying this:\n$$\n\\frac{\\partial}{\\partial \\ell} (y^T K^{-1} y) = y^T \\left( \\frac{\\partial K^{-1}}{\\partial \\ell} \\right) y = y^T \\left( -K^{-1} \\frac{\\partial K}{\\partial \\ell} K^{-1} \\right) y\n$$\nCombining these results, the gradient of the log-likelihood is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = -\\frac{1}{2} \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) - \\frac{1}{2} \\left( -y^T K^{-1} \\frac{\\partial K}{\\partial \\ell} K^{-1} y \\right) = \\frac{1}{2} \\left( y^T K^{-1} \\frac{\\partial K}{\\partial \\ell} K^{-1} y - \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) \\right)\n$$\nAs specified in the problem, let $\\alpha = K^{-1} y$. Since $K$ is symmetric, $\\alpha^T = (K^{-1}y)^T = y^T (K^{-1})^T = y^T K^{-1}$. Substituting $\\alpha$ into the expression:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\frac{1}{2} \\left( \\alpha^T \\frac{\\partial K}{\\partial \\ell} \\alpha - \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) \\right)\n$$\nThe first term $\\alpha^T \\frac{\\partial K}{\\partial \\ell} \\alpha$ is a scalar, so it equals its own trace. Using the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(BCA)$, we have $\\text{Tr}(\\alpha^T \\frac{\\partial K}{\\partial \\ell} \\alpha) = \\text{Tr}(\\alpha \\alpha^T \\frac{\\partial K}{\\partial \\ell})$. This allows us to factor out the derivative term:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\frac{1}{2} \\text{Tr}\\left( \\alpha \\alpha^T \\frac{\\partial K}{\\partial \\ell} \\right) - \\frac{1}{2} \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) = \\frac{1}{2} \\text{Tr}\\left( (\\alpha \\alpha^T - K^{-1}) \\frac{\\partial K}{\\partial \\ell} \\right)\n$$\nNext, we specialize this result to the isotropic squared-exponential kernel. The covariance matrix is $K = K_y + \\sigma_n^2 I$, where $[K_y]_{ij} = k(x_i, x_j)$. The noise variance $\\sigma_n^2$ and identity matrix $I$ do not depend on $\\ell$, so $\\frac{\\partial K}{\\partial \\ell} = \\frac{\\partial K_y}{\\partial \\ell}$. The kernel is $k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{\\|x - x'\\|_2^2}{2\\ell^2}\\right)$. Let $d_{ij} = \\|x_i - x_j\\|_2$. The partial derivative of the $(i, j)$-th element of $K$ with respect to $\\ell$ is:\n$$\n\\frac{\\partial K_{ij}}{\\partial \\ell} = \\frac{\\partial}{\\partial \\ell} \\left( \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\right) = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\cdot \\frac{\\partial}{\\partial \\ell} \\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\cdot \\left( - \\frac{d_{ij}^2}{2} \\cdot (-2\\ell^{-3}) \\right)\n$$\n$$\n\\frac{\\partial K_{ij}}{\\partial \\ell} = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\frac{d_{ij}^2}{\\ell^3}\n$$\nThe trace term $\\text{Tr}(AB)$ can be calculated as the sum of the element-wise product, $\\sum_{i,j} A_{ij} B_{ji}$. Since both matrices $(\\alpha\\alpha^T - K^{-1})$ and $\\frac{\\partial K}{\\partial \\ell}$ are symmetric, this becomes $\\sum_{i,j} (\\alpha\\alpha^T - K^{-1})_{ij} (\\frac{\\partial K}{\\partial \\ell})_{ij}$. We can now write the final expression for the gradient as a double summation:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\left( [\\alpha\\alpha^T]_{ij} - [K^{-1}]_{ij} \\right) \\frac{\\partial K_{ij}}{\\partial \\ell} = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\left( \\alpha_i \\alpha_j - [K^{-1}]_{ij} \\right) \\left( \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\frac{d_{ij}^2}{\\ell^3} \\right)\n$$\nThis expression can be rearranged slightly to factor out constants from the summation, leading to the final form requested.\n\nThe derived gradient is essential for automating the tuning of the GP surrogate model's hyperparameters within a battery design workflow. The objective is to maximize the log-marginal likelihood with respect to the hyperparameters $\\theta = \\{\\ell, \\sigma_f^2, \\sigma_n^2\\}$, a procedure known as Type-II Maximum Likelihood Estimation. This optimization is typically performed using a gradient-based algorithm, such as L-BFGS-B, which requires the analytical gradient of the objective function (the log-likelihood) with respect to each hyperparameter. The expression derived above provides the gradient component for $\\ell$. Because the lengthscale $\\ell$ must be strictly positive, direct optimization of $\\ell$ is constrained. To handle this, one typically reparameterizes $\\ell$ by optimizing over an unconstrained variable $\\xi \\in \\mathbb{R}$ where $\\ell = \\exp(\\xi)$. The gradient with respect to $\\xi$ is then computed via the chain rule: $\\frac{\\partial \\mathcal{L}}{\\partial \\xi} = \\frac{\\partial \\mathcal{L}}{\\partial \\ell} \\frac{\\partial \\ell}{\\partial \\xi} = \\frac{\\partial \\mathcal{L}}{\\partial \\ell} \\exp(\\xi) = \\frac{\\partial \\mathcal{L}}{\\partial \\ell} \\ell$. From a practical standpoint, the computation of $\\alpha=K^{-1}y$ and $K^{-1}$ is not performed by direct matrix inversion, which is numerically unstable and computationally expensive ($O(n^3)$). Instead, a Cholesky decomposition $K = LL^T$ is computed. This decomposition is numerically stable for the symmetric positive-definite covariance matrix $K$ and allows $\\alpha$ to be found efficiently by solving two triangular systems, $Lz=y$ and $L^T\\alpha=z$, in $O(n^2)$ time. The matrix inverse $K^{-1}$ needed for the gradient can also be computed stably from the Cholesky factor as $K^{-1} = (L^T)^{-1}L^{-1}$.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_f^2}{2\\ell^3} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left( \\alpha_i \\alpha_j - [K^{-1}]_{ij} \\right) d_{ij}^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right)}\n$$"
        },
        {
            "introduction": "Once a GP surrogate is trained, its predictive mean and variance can guide the search for optimal designs in a process called Bayesian Optimization. This is done using an acquisition function, which scores the utility of evaluating a new candidate design. This practice explores one of the simplest acquisition functions, the Probability of Improvement (PI), which calculates the chance that a new point will outperform the best observation so far . By deriving PI and contrasting it with the more widely used Expected Improvement (EI), you will develop an intuition for how these functions balance the crucial trade-off between exploiting known high-performance regions and exploring areas of high uncertainty.",
            "id": "3915948",
            "problem": "In automated battery design and simulation, discharge capacity as a function of design variables, denoted by $f(x)$, is modeled with Gaussian Process Regression (GPR). Under the GPR predictive model at a candidate design $x$, assume the posterior of $f(x)$ given data is Gaussian with mean $\\mu(x)$ and variance $\\sigma^{2}(x)$, so $f(x) \\mid \\mathcal{D} \\sim \\mathcal{N}\\!\\left(\\mu(x), \\sigma^{2}(x)\\right)$. Let the best observed discharge capacity to date be $f^\\star$, and define the improvement random variable $I(x)$ by $I(x) = \\max(f(x) - f^\\star - \\xi, 0)$, where $\\xi \\ge 0$ is a user-specified margin that encodes minimal improvement over the current best. Using only the definition of $I(x)$, the Gaussian predictive model for $f(x)$, and properties of the Gaussian (normal) distribution, derive a closed-form analytic expression for the Probability of Improvement (PI) acquisition function, defined as the probability that a draw from the predictive distribution at $x$ exceeds $f^\\star + \\xi$. Then, for a lithium-ion cathode design scenario with $f^\\star = 195\\,\\mathrm{mAh/g}$, margin $\\xi = 2\\,\\mathrm{mAh/g}$, and a candidate $x$ having $\\mu(x) = 192\\,\\mathrm{mAh/g}$ and $\\sigma(x) = 10\\,\\mathrm{mAh/g}$, evaluate the PI numerically. Express the numerical PI as a decimal and round your answer to four significant figures. Finally, construct and discuss a scientifically plausible pair of candidates $x_{1}$ and $x_{2}$ for which the PI values are equal but the Expected Improvement (EI) acquisition function values differ substantially, explaining why PI can fail relative to EI due to its insensitivity to the magnitude of improvement. You may assume the standard normal cumulative distribution function and probability density function are well-defined and can be used as needed, but do not use any shortcut formulas without derivation.",
            "solution": "The problem requires the derivation of the Probability of Improvement (PI) acquisition function, a numerical calculation for a specific scenario, and a conceptual discussion comparing PI with the Expected Improvement (EI) acquisition function. The problem is scientifically grounded, well-posed, and objective, and is therefore deemed valid.\n\n### Part 1: Derivation of the Probability of Improvement (PI)\n\nThe Probability of Improvement, $\\text{PI}(x)$, is defined as the probability that a new observation $f(x)$ at a candidate design point $x$ will yield a value greater than the best observed value so far, $f^\\star$, by at least a margin $\\xi$. Mathematically, this is expressed as:\n$$\n\\text{PI}(x) = P(f(x) > f^\\star + \\xi)\n$$\nThe problem states that the discharge capacity $f(x)$ is modeled by a Gaussian Process, and its posterior predictive distribution at $x$ is a normal distribution with mean $\\mu(x)$ and variance $\\sigma^2(x)$:\n$$\nf(x) \\mid \\mathcal{D} \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))\n$$\nTo calculate the probability, we standardize the random variable $f(x)$ to a standard normal random variable $Z \\sim \\mathcal{N}(0, 1)$. The standardization transformation is:\n$$\nZ = \\frac{f(x) - \\mu(x)}{\\sigma(x)}\n$$\nWe apply this transformation to the inequality inside the probability statement:\n$$\nf(x) > f^\\star + \\xi\n$$\nSubtracting the mean $\\mu(x)$ from both sides gives:\n$$\nf(x) - \\mu(x) > f^\\star + \\xi - \\mu(x)\n$$\nAssuming $\\sigma(x) > 0$ (a degenerate case with zero variance is uninformative for optimization), we can divide by the standard deviation $\\sigma(x)$ without changing the inequality's direction:\n$$\n\\frac{f(x) - \\mu(x)}{\\sigma(x)} > \\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\n$$\nThe left side is our standard normal variable $Z$. Therefore, the probability becomes:\n$$\n\\text{PI}(x) = P\\left(Z > \\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\\right)\n$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, defined as $\\Phi(z) = P(Z \\le z)$. The probability of $Z$ exceeding a value $z$ is given by $P(Z > z) = 1 - P(Z \\le z) = 1 - \\Phi(z)$.\nUsing this, we have:\n$$\n\\text{PI}(x) = 1 - \\Phi\\left(\\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\\right)\n$$\nDue to the symmetry of the standard normal distribution, we have the property $1 - \\Phi(z) = \\Phi(-z)$. Applying this property, we can write the expression more compactly:\n$$\n\\text{PI}(x) = \\Phi\\left(-\\left(\\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\\right)\\right) = \\Phi\\left(\\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)}\\right)\n$$\nThis is the closed-form analytic expression for the Probability of Improvement acquisition function. For convenience, let's define the argument of the CDF as $\\gamma(x)$:\n$$\n\\gamma(x) = \\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)}\n$$\nThen, the final expression is $\\text{PI}(x) = \\Phi(\\gamma(x))$.\n\n### Part 2: Numerical Evaluation\n\nWe are given the following values for a lithium-ion cathode design scenario:\n- Best observed capacity: $f^\\star = 195\\,\\mathrm{mAh/g}$\n- Margin: $\\xi = 2\\,\\mathrm{mAh/g}$\n- Predictive mean at candidate $x$: $\\mu(x) = 192\\,\\mathrm{mAh/g}$\n- Predictive standard deviation at candidate $x$: $\\sigma(x) = 10\\,\\mathrm{mAh/g}$\n\nFirst, we compute the value of the standardized variable $\\gamma(x)$:\n$$\n\\gamma(x) = \\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)} = \\frac{192 - 195 - 2}{10} = \\frac{-5}{10} = -0.5\n$$\nThe Probability of Improvement is then:\n$$\n\\text{PI}(x) = \\Phi(-0.5)\n$$\nUsing a standard normal distribution table or a computational library, the value of the CDF at $z = -0.5$ is approximately $\\Phi(-0.5) \\approx 0.3085375...$.\nRounding this value to four significant figures as requested gives:\n$$\n\\text{PI}(x) \\approx 0.3085\n$$\n\n### Part 3: Comparison of PI and EI\n\nTo illustrate the limitations of PI, we must construct a scenario with two candidate designs, $x_1$ and $x_2$, such that $\\text{PI}(x_1) = \\text{PI}(x_2)$ but their Expected Improvement, $\\text{EI}(x_1)$ and $\\text{EI}(x_2)$, differ substantially.\n\nThe condition $\\text{PI}(x_1) = \\text{PI}(x_2)$ implies $\\Phi(\\gamma(x_1)) = \\Phi(\\gamma(x_2))$. Since $\\Phi$ is a monotonically increasing function, this is equivalent to $\\gamma(x_1) = \\gamma(x_2)$.\n$$\n\\frac{\\mu(x_1) - f^\\star - \\xi}{\\sigma(x_1)} = \\frac{\\mu(x_2) - f^\\star - \\xi}{\\sigma(x_2)}\n$$\nThe Expected Improvement acquisition function is defined as $\\text{EI}(x) = \\mathbb{E}[\\max(f(x) - f^\\star - \\xi, 0)]$. Its closed-form expression is:\n$$\n\\text{EI}(x) = (\\mu(x) - f^\\star - \\xi) \\Phi(\\gamma(x)) + \\sigma(x) \\phi(\\gamma(x))\n$$\nwhere $\\phi(\\cdot)$ is the standard normal probability density function (PDF). Substituting $\\mu(x) - f^\\star - \\xi = \\gamma(x)\\sigma(x)$ into the EI formula, we get:\n$$\n\\text{EI}(x) = (\\gamma(x)\\sigma(x)) \\Phi(\\gamma(x)) + \\sigma(x) \\phi(\\gamma(x)) = \\sigma(x) \\left[ \\gamma(x)\\Phi(\\gamma(x)) + \\phi(\\gamma(x)) \\right]\n$$\nIf we set $\\gamma(x_1) = \\gamma(x_2) = \\gamma_0$, the term in the brackets becomes a constant for both candidates. The EI values are then:\n$\\text{EI}(x_1) = \\sigma(x_1) \\left[ \\gamma_0\\Phi(\\gamma_0) + \\phi(\\gamma_0) \\right]$\n$\\text{EI}(x_2) = \\sigma(x_2) \\left[ \\gamma_0\\Phi(\\gamma_0) + \\phi(\\gamma_0) \\right]$\nThe ratio of the EIs is simply the ratio of the standard deviations: $\\frac{\\text{EI}(x_2)}{\\text{EI}(x_1)} = \\frac{\\sigma(x_2)}{\\sigma(x_1)}$. Therefore, if we choose $\\sigma(x_1) \\neq \\sigma(x_2)$, the EI values will be different.\n\nLet's construct a plausible pair of candidates, using the same context $f^\\star = 195\\,\\mathrm{mAh/g}$ and $\\xi = 2\\,\\mathrm{mAh/g}$, so the improvement threshold is $f^\\star + \\xi = 197\\,\\mathrm{mAh/g}$. Let's select a common value $\\gamma_0 = 0.5$.\n\n- **Candidate $x_1$ (low uncertainty, exploitative choice):**\n  Let the predictive uncertainty be low, e.g., $\\sigma(x_1) = 4\\,\\mathrm{mAh/g}$.\n  To satisfy $\\gamma(x_1) = 0.5$, we need $\\frac{\\mu(x_1) - 197}{4} = 0.5$.\n  This gives a predictive mean of $\\mu(x_1) = 197 + 0.5 \\times 4 = 199\\,\\mathrm{mAh/g}$.\n  So, $x_1$ has $(\\mu_1, \\sigma_1) = (199, 4)$.\n\n- **Candidate $x_2$ (high uncertainty, explorative choice):**\n  Let the predictive uncertainty be high, e.g., $\\sigma(x_2) = 12\\,\\mathrm{mAh/g}$, three times larger than for $x_1$.\n  To satisfy $\\gamma(x_2) = 0.5$, we need $\\frac{\\mu(x_2) - 197}{12} = 0.5$.\n  This gives a predictive mean of $\\mu(x_2) = 197 + 0.5 \\times 12 = 203\\,\\mathrm{mAh/g}$.\n  So, $x_2$ has $(\\mu_2, \\sigma_2) = (203, 12)$.\n\nFor both candidates, the Probability of Improvement is $\\text{PI}(x_1) = \\text{PI}(x_2) = \\Phi(0.5) \\approx 0.6915$. From the perspective of PI, both candidates are equally desirable.\n\nHowever, their Expected Improvements will differ significantly. The term in brackets is $C = [0.5 \\cdot \\Phi(0.5) + \\phi(0.5)]$.\n$\\text{EI}(x_1) = 4 \\times C$\n$\\text{EI}(x_2) = 12 \\times C = 3 \\times \\text{EI}(x_1)$\n\n**Discussion:**\nThe PI acquisition function fails in this scenario because it is insensitive to the *magnitude* of the potential improvement. It only registers the probability of an improvement occurring, treating all improvements, whether minor or substantial, as equal. Candidate $x_1$ offers a high probability of a small improvement (its mean is only slightly above the threshold, with low variance). Candidate $x_2$ has the same probability of improvement, but its higher mean and much larger variance imply a significant chance for a very large improvement. The wider distribution of $f(x_2)$ means that the upper tail, which corresponds to large capacity values, has more probability mass far from the mean compared to $f(x_1)$.\n\nThe EI acquisition function, by calculating the expectation of the improvement, correctly balances the probability of improvement with its likely magnitude. It integrates the product of the improvement amount ($f(x) - (f^\\star + \\xi)$) and its probability density over the improvement region. Consequently, EI correctly identifies $x_2$ as the more promising candidate because the potential for a large, game-changing discovery outweighs the similar probability of a minor improvement offered by $x_1$. This makes EI a more effective strategy for guiding the search, as it naturally balances exploitation (sampling near known good points, like $x_1$) and exploration (sampling in high-uncertainty regions, like $x_2$, which could lead to new optima). PI, being \"magnitude-blind\", can get stuck exploiting small local improvements and may fail to explore regions with high potential gains.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\Phi\\left(\\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)}\\right) & 0.3085 \\end{pmatrix}}$$"
        },
        {
            "introduction": "A key assumption in standard GP regression is that observation noise is homoscedastic, meaning its variance is constant across the entire input space. This assumption is often violated in complex simulations, where the uncertainty of the output may depend on the input parameters. This hands-on coding challenge introduces a more advanced heteroscedastic GP model, which uses a second GP to explicitly model this input-dependent noise variance . By implementing a single step of a variational inference algorithm, you will gain practical experience with a powerful technique for building more flexible and realistic uncertainty-aware surrogates, a vital step for applications in safety-critical domains like battery design.",
            "id": "3916007",
            "problem": "Consider the task of constructing an uncertainty-aware surrogate model for normalized discharge capacity in automated battery design and simulation. Let the input design vector be denoted by $x \\in \\mathbb{R}^d$, and let the observed scalar response be $y \\in \\mathbb{R}$. We aim to model heteroscedastic simulation noise by coupling two Gaussian Processes (GPs): one for the latent function $f(x)$ and one for the log noise variance $g(x) = \\log \\sigma_n^2(x)$. The generative model is\n$$\nf \\sim \\mathcal{GP}\\!\\left(0, k_f(\\cdot,\\cdot)\\right), \\quad g \\sim \\mathcal{GP}\\!\\left(m_g(\\cdot), k_g(\\cdot,\\cdot)\\right),\n$$\nwith independent priors, and the likelihood\n$$\np(y \\mid f(x), g(x)) = \\mathcal{N}\\!\\left(y \\mid f(x), \\exp(g(x))\\right).\n$$\nHere $\\mathcal{GP}$ denotes a Gaussian Process, and $\\mathcal{N}$ denotes a normal distribution. The model should be derived from first principles: Bayesâ€™ rule, independent GP priors, and the Gaussian likelihood. The inference procedure should be outlined through variational approximations, employing mean-field assumptions and moment-matching for the log-normal observation noise via the identity\n$$\n\\mathbb{E}\\left[\\exp(g)\\right] = \\exp\\!\\left(\\mu_g + \\tfrac{1}{2}\\sigma_g^2\\right)\n$$\nfor $g \\sim \\mathcal{N}(\\mu_g, \\sigma_g^2)$.\n\nYour program must implement a single iteration of a variational heteroscedastic GP approximation with the following steps:\n- Use an initial homoscedastic noise variance $\\sigma_0^2$ to compute the posterior mean at the training inputs for $f(x)$ under a GP with kernel $k_f$. Denote the posterior mean at training inputs by $\\mu_f(x_i)$.\n- Form residuals $r_i = y_i - \\mu_f(x_i)$ and pseudo-targets $t_i = \\log(r_i^2 + \\epsilon)$ for a small $\\epsilon > 0$.\n- Fit a GP for $g(x)$ to $(x_i, t_i)$ with kernel $k_g$ and observation noise variance $\\beta^2$ to obtain a Gaussian posterior approximation $q(g)$ at training inputs and at a test input $x_\\star$.\n- Use the log-normal moment identity to set the heteroscedastic noise diagonal $\\nu_i = \\exp\\!\\left(\\mu_{g}(x_i) + \\tfrac{1}{2}\\sigma^2_{g}(x_i)\\right)$ at training inputs, and $\\nu_\\star = \\exp\\!\\left(\\mu_g(x_\\star) + \\tfrac{1}{2}\\sigma_g^2(x_\\star)\\right)$ at the test input.\n- Recompute the posterior for $f$ with the heteroscedastic noise diagonal $\\nu_i$, and report the predictive mean $\\mu_f(x_\\star)$ and the predictive variance for $y_\\star$ via $\\mathrm{Var}(y_\\star) = \\mathrm{Var}(f(x_\\star)) + \\nu_\\star$.\n\nAll kernels must be the squared exponential (also known as radial basis function) kernel with Automatic Relevance Determination (ARD):\n$$\nk_{\\text{SE}}(x, x'; \\alpha, \\ell_1,\\dots,\\ell_d) = \\alpha^2 \\exp\\!\\left(-\\tfrac{1}{2}\\sum_{j=1}^d \\frac{(x_j - x'_j)^2}{\\ell_j^2}\\right).\n$$\nAssume zero mean for $f$, and a constant mean $m_g(x) = 0$ for $g$. All quantities are dimensionless.\n\nTest suite specification. For all cases, set the numerical jitter to be $10^{-6}$ and $\\epsilon = 10^{-8}$. The observation $y_i$ for each case must be deterministically constructed from\n$$\nf_{\\text{true}}(x) = \\sin(\\pi x_1) + \\tfrac{1}{2}\\cos(\\pi x_2) + 0.1\\,x_1 x_2,\n$$\n$$\ng_{\\text{true}}(x) = -1.8 + 1.2\\,x_1^2 + 0.8\\,x_2,\n$$\nand $c_i = 0.7\\,\\sin(1+i)$ for index $i=1,2,\\dots,n$, via\n$$\ny_i = f_{\\text{true}}(x_i) + \\sqrt{\\exp(g_{\\text{true}}(x_i))}\\,c_i,\n$$\nso that $y_i$ exhibits input-dependent deterministic perturbations mimicking heteroscedastic effects.\n\n- Case A (happy path):\n    - Training inputs $x_i \\in \\mathbb{R}^2$:\n      $[0.15,0.20],[0.35,0.50],[0.55,0.65],[0.75,0.85],[0.90,0.10]$.\n    - Test input $x_\\star = [0.50,0.50]$.\n    - $k_f$: amplitude $\\alpha_f = 1.2$, length-scales $\\ell_f = [0.4,0.6]$.\n    - $k_g$: amplitude $\\alpha_g = 0.7$, length-scales $\\ell_g = [0.7,0.3]$.\n    - Initial homoscedastic noise variance $\\sigma_0^2 = 0.06$.\n    - Log-variance GP observation noise $\\beta^2 = 0.04$.\n\n- Case B (near-duplicate boundary condition and tighter length-scales):\n    - Training inputs $x_i \\in \\mathbb{R}^2$:\n      $[0.20,0.20],[0.20,0.20],[0.40,0.60],[0.80,0.40]$.\n    - Test input $x_\\star = [0.25,0.22]$.\n    - $k_f$: amplitude $\\alpha_f = 1.0$, length-scales $\\ell_f = [0.3,0.3]$.\n    - $k_g$: amplitude $\\alpha_g = 0.5$, length-scales $\\ell_g = [0.5,0.5]$.\n    - Initial homoscedastic noise variance $\\sigma_0^2 = 0.05$.\n    - Log-variance GP observation noise $\\beta^2 = 0.03$.\n\n- Case C (few-point edge case with smoother $f$):\n    - Training inputs $x_i \\in \\mathbb{R}^2$:\n      $[0.10,0.90],[0.90,0.10]$.\n    - Test input $x_\\star = [0.50,0.80]$.\n    - $k_f$: amplitude $\\alpha_f = 1.5$, length-scales $\\ell_f = [0.8,0.5]$.\n    - $k_g$: amplitude $\\alpha_g = 0.6$, length-scales $\\ell_g = [1.0,0.4]$.\n    - Initial homoscedastic noise variance $\\sigma_0^2 = 0.08$.\n    - Log-variance GP observation noise $\\beta^2 = 0.02$.\n\nYour program must:\n- Construct $y_i$ deterministically for each case using the provided $f_{\\text{true}}$, $g_{\\text{true}}$, and $c_i$.\n- Perform the single variational iteration described above to obtain the predictive mean $\\mu_f(x_\\star)$ and predictive variance $\\mathrm{Var}(y_\\star)$ for each case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\mu_A,\\mathrm{Var}_A,\\mu_B,\\mathrm{Var}_B,\\mu_C,\\mathrm{Var}_C]$, where all entries are floats rounded to six decimal places. No other text should be printed.",
            "solution": "The user has provided a problem that requires the implementation of a single iteration of a variational inference algorithm for a heteroscedastic Gaussian Process (GP) regression model. The problem is well-defined, scientifically sound, and contains all necessary components for a unique solution. My task is to implement the specified algorithm and apply it to three test cases.\n\n### Principles of Heteroscedastic Gaussian Process Regression\n\nA standard Gaussian Process model assumes homoscedastic noise, meaning the observation noise variance $\\sigma_n^2$ is constant across the input space. However, in many real-world scenarios, such as battery simulation, the noise or uncertainty associated with an observation $y$ depends on the input parameters $x$. The heteroscedastic GP model addresses this by making the noise variance a function of the input, $\\sigma_n^2(x)$.\n\nA common and principled approach is to model the logarithm of the noise variance, $g(x) = \\log \\sigma_n^2(x)$, with its own Gaussian Process. This ensures that the noise variance $\\sigma_n^2(x) = \\exp(g(x))$ is always positive.\n\nThe full generative model is thus:\n1.  A latent function $f(x)$ for the mean response is drawn from a GP prior:\n    $$f \\sim \\mathcal{GP}(m_f(x), k_f(x, x'))$$\n    The problem specifies a zero mean prior, $m_f(x) = 0$.\n\n2.  A latent function $g(x)$ for the log-noise-variance is drawn from another, independent GP prior:\n    $$g \\sim \\mathcal{GP}(m_g(x), k_g(x, x'))$$\n    The problem specifies a constant zero mean prior, $m_g(x) = 0$.\n\n3.  Observations $y$ are generated from a Gaussian likelihood where the mean is $f(x)$ and the variance is $\\exp(g(x))$:\n    $$p(y|f(x), g(x)) = \\mathcal{N}(y | f(x), \\exp(g(x)))$$\n\nExact Bayesian inference for this model is intractable due to the non-Gaussian relationship between $g(x)$ and the likelihood. Variational inference provides a tractable approximation by positing a factorized approximate posterior $q(f, g) \\approx p(f, g | \\mathbf{y}, \\mathbf{X})$ of the form $q(f,g) = q(f)q(g)$. The algorithm then iteratively optimizes $q(f)$ and $q(g)$ to minimize the KL-divergence between the approximation and the true posterior. The problem statement outlines a single step of such a coordinate-ascent-style variational update.\n\n### Algorithmic Steps and Mathematical Formulation\n\nThe solution will be implemented by following the prescribed steps for a single variational iteration. Let $\\mathbf{X} = \\{x_1, \\dots, x_n\\}^T$ be the training inputs and $\\mathbf{y} = \\{y_1, \\dots, y_n\\}^T$ be the training observations. A numerical jitter value $\\delta$ (given as $10^{-6}$) is added to the diagonal of kernel matrices for numerical stability.\n\n**Step 0: Kernel Function**\nThe squared exponential (SE) kernel with Automatic Relevance Determination (ARD) is used for both GPs:\n$$k_{\\text{SE}}(x, x'; \\alpha, \\boldsymbol{\\ell}) = \\alpha^2 \\exp\\left(-\\frac{1}{2} \\sum_{j=1}^d \\frac{(x_j - x'_j)^2}{\\ell_j^2}\\right)$$\nwhere $\\alpha$ is the amplitude and $\\boldsymbol{\\ell}$ is the vector of length-scales for each input dimension.\n\n**GP Prediction Equations**\nFor a GP with training data $(\\mathbf{X}, \\mathbf{z})$, a test input $x_*$, zero mean prior, kernel $k$, and an additive noise covariance matrix $\\mathbf{\\Sigma}_n$, the predictive distribution for the latent function value $f(x_*)$ is Gaussian, $p(f(x_*)|\\mathbf{X}, \\mathbf{z}) = \\mathcal{N}(\\mu_f(x_*), \\sigma_f^2(x_*))$, with mean and variance given by:\n$$ \\mu_f(x_*) = \\mathbf{k}_*^T (\\mathbf{K} + \\mathbf{\\Sigma}_n)^{-1} \\mathbf{z} $$\n$$ \\sigma_f^2(x_*) = k(x_*, x_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\mathbf{\\Sigma}_n)^{-1} \\mathbf{k}_* $$\nwhere $\\mathbf{K} = k(\\mathbf{X}, \\mathbf{X})$ is the $n \\times n$ training kernel matrix, and $\\mathbf{k}_* = k(\\mathbf{X}, x_*)$ is the $n \\times 1$ vector of covariances between training and test points. For numerical stability, we solve the linear system rather than inverting the matrix, often via Cholesky decomposition.\n\n---\n\n**Step 1: Initial Homoscedastic Fit for $f$**\nFirst, we assume a constant (homoscedastic) noise variance $\\sigma_0^2$ to get an initial estimate of the latent function means at the training inputs.\n- Noise covariance: $\\mathbf{\\Sigma}_n = (\\sigma_0^2 + \\delta) \\mathbf{I}$.\n- We compute the posterior mean of $f$ at the training inputs $\\mathbf{X}$ themselves. Let $\\boldsymbol{\\mu}_f$ be this vector of means.\n- Using the prediction equations with $\\mathbf{X}$ as test points:\n  $$ \\boldsymbol{\\mu}_f = \\mathbf{K}_f (\\mathbf{K}_f + (\\sigma_0^2 + \\delta)\\mathbf{I})^{-1} \\mathbf{y} $$\n  where $\\mathbf{K}_f = k_f(\\mathbf{X}, \\mathbf{X})$ is the kernel matrix for $f$.\n\n**Step 2: Form Pseudo-targets for $g$**\nWith the initial estimate $\\boldsymbol{\\mu}_f$, we can approximate the squared residuals, which serve as noisy observations of the variance $\\sigma_n^2(x_i) = \\exp(g(x_i))$.\n- Residuals: $r_i = y_i - \\mu_f(x_i)$.\n- To create targets for the GP on the *log*-variance $g$, we take the logarithm of the squared residuals. A small constant $\\epsilon$ (given as $10^{-8}$) is added for numerical stability to prevent $\\log(0)$.\n- Pseudo-targets for $g$: $t_i = \\log(r_i^2 + \\epsilon)$.\n\n**Step 3: Fit GP for $g$**\nNow, we perform a standard GP regression to model $g(x)$ using the training pairs $(\\mathbf{X}, \\mathbf{t})$. This GP has its own kernel $k_g$ and an assumed observation noise $\\beta^2$.\n- Training data: $(\\mathbf{X}, \\mathbf{t})$.\n- Noise covariance: $\\mathbf{\\Sigma}_n = (\\beta^2 + \\delta)\\mathbf{I}$.\n- We predict the posterior distribution for $g$ at both the training inputs $\\mathbf{X}$ and the test input $x_*$. The posterior is Gaussian: $q(g) \\approx \\mathcal{N}(g|\\boldsymbol{\\mu}_g, \\mathbf{\\Sigma}_g)$. We need the posterior means $\\mu_g(x_i)$, $\\mu_g(x_*)$ and variances $\\sigma_g^2(x_i)$, $\\sigma_g^2(x_*)$. These are computed using the standard GP prediction equations with kernel $k_g$.\n\n**Step 4: Compute Heteroscedastic Noise**\nThe expected noise variance at any point $x$ is $\\mathbb{E}[\\sigma_n^2(x)] = \\mathbb{E}[\\exp(g(x))]$. Since our posterior approximation $q(g(x))$ is $\\mathcal{N}(\\mu_g(x), \\sigma_g^2(x))$, we use the moment identity for a log-normal distribution:\n$$ \\mathbb{E}[\\exp(g(x))] = \\exp(\\mu_g(x) + \\tfrac{1}{2}\\sigma_g^2(x)) $$\n- We apply this identity to compute the estimated input-dependent noise variances at the training points, forming a diagonal noise matrix for the next step.\n  $$ \\nu_i = \\exp(\\mu_g(x_i) + \\tfrac{1}{2}\\sigma_g^2(x_i)) $$\n- We also compute the expected noise variance at the test point $x_*$:\n  $$ \\nu_* = \\exp(\\mu_g(x_*) + \\tfrac{1}{2}\\sigma_g^2(x_*)) $$\n\n**Step 5: Recompute Posterior for $f$**\nFinally, we update our estimate for $f$ by performing another GP regression, this time incorporating the estimated heteroscedastic noise.\n- Training data: $(\\mathbf{X}, \\mathbf{y})$.\n- Noise covariance: $\\mathbf{\\Sigma}_n = \\text{diag}(\\boldsymbol{\\nu}) + \\delta\\mathbf{I}$, where $\\boldsymbol{\\nu} = \\{\\nu_1, \\dots, \\nu_n\\}^T$.\n- We compute the predictive mean and variance of $f$ at the test point $x_*$ using the standard GP equations with kernel $k_f$ and this new noise structure. Let these be $\\mu_f(x_*)$ and $\\sigma_f^2(x_*)$.\n- The final predictive variance for the *observation* $y_*$ includes both the uncertainty in the latent function $f(x_*)$ and the estimated noise level at that point:\n  $$ \\text{Var}(y_*) = \\sigma_f^2(x_*) + \\nu_* $$\n\nThis completes one iteration. The deterministic nature of the data generation ensures that, despite the name \"noise,\" the process is fully reproducible. The implementation will follow these steps for each test case provided.",
            "answer": "[0.957581,0.292723,0.768630,0.220556,0.306026,1.401140]"
        }
    ]
}