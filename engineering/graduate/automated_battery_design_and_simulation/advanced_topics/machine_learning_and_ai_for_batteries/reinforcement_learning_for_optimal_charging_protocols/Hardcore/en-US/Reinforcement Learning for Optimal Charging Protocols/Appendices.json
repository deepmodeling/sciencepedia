{
    "hands_on_practices": [
        {
            "introduction": "Defining the reward function is the most critical step in formulating a reinforcement learning problem, as it implicitly specifies the agent's goal. A common approach in battery charging is to penalize factors that cause degradation, such as Joule heating, which is proportional to the square of the current, $I^2$. This practice uses a simplified thought experiment to reveal a potential bias introduced by such a quadratic penalty, demonstrating how it can discourage effective strategies like pulse charging even when thermal effects are perfectly managed . By working through this example, you will develop a deeper intuition for the non-linear effects of reward shaping.",
            "id": "3946202",
            "problem": "A battery-charging agent is modeled as a Markov Decision Process (MDP) within a reinforcement learning (RL) framework for automated battery design and simulation. At each discrete time step $t \\in \\{0,1\\}$, the agent selects a normalized charging current $i_t \\in [0,1]$, where the physical current is $I_t = i_t I_{\\max}$. The normalized state-of-charge $s_t \\in [0,1]$ evolves according to the definition of coulombic charge transfer with unit efficiency, so $s_{t+1} = s_t + i_t$, and the delivered charge increment is $s_{t+1} - s_t = i_t$. Joule heating in the cell is proportional to $I_t^2$, but a dedicated thermal-management controller (exogenous to the RL agent) applies cooling that perfectly compensates the Joule heating at every time step, maintaining a constant normalized temperature $\\theta_t$; that is, $\\theta_{t+1} = \\theta_t$ for all $t$ despite the values of $i_t$.\n\nThe agent’s shaped reward at each time step is defined by\n$$\nr_t = \\beta \\, (s_{t+1} - s_t) - \\alpha \\, i_t^{2},\n$$\nwith $\\beta = 1$ and $\\alpha = 1$. The episode length is $N = 2$ (time steps $t=0,1$), and the return is $G = \\sum_{t=0}^{N-1} r_t$.\n\nTo interrogate whether naive reward shaping that directly penalizes $i_t^2$ can bias the policy against pulse charging even when thermal safety is fully managed, compare the following two trajectories that deliver the same total charge and maintain identical temperature due to perfect thermal management:\n\n- Constant-current trajectory: $i_0 = \\frac{1}{2}$ and $i_1 = \\frac{1}{2}$.\n- Pulse-charging trajectory: $i_0 = 1$ and $i_1 = 0$.\n\nCompute the exact value of the difference in cumulative shaped return,\n$$\nG_{\\mathrm{pulse}} - G_{\\mathrm{const}},\n$$\nas a real number. Express your final answer as an exact value without units (the return is dimensionless).",
            "solution": "The problem asks for the difference in cumulative shaped return, $G_{\\mathrm{pulse}} - G_{\\mathrm{const}}$, between two distinct charging trajectories. The analysis begins with the definition of the reward function and the total return.\n\nThe reward at each time step $t$ is given by the expression:\n$$\nr_t = \\beta \\, (s_{t+1} - s_t) - \\alpha \\, i_t^{2}\n$$\nThe state evolution for the normalized state-of-charge, $s_t$, is defined as:\n$$\ns_{t+1} = s_t + i_t\n$$\nThis implies that the charge increment in one time step is equal to the normalized current applied during that step:\n$$\ns_{t+1} - s_t = i_t\n$$\nSubstituting this relationship into the reward function simplifies the expression for $r_t$. It becomes dependent only on the action $i_t$ taken at that step:\n$$\nr_t = \\beta \\, i_t - \\alpha \\, i_t^{2}\n$$\nThe problem specifies the values of the parameters $\\beta$ and $\\alpha$ as:\n$$\n\\beta = 1\n$$\n$$\n\\alpha = 1\n$$\nThus, the specific reward function for this problem is:\n$$\nr_t = i_t - i_t^{2}\n$$\nThe total return, $G$, is the sum of the rewards over the entire episode, which has a length of $N=2$ (for time steps $t=0$ and $t=1$):\n$$\nG = \\sum_{t=0}^{N-1} r_t = r_0 + r_1\n$$\nSubstituting the expression for $r_t$, the total return is:\n$$\nG = (i_0 - i_0^{2}) + (i_1 - i_1^{2})\n$$\nWe now compute the total return for each of the two specified trajectories.\n\nFirst, consider the constant-current trajectory, where $i_0 = \\frac{1}{2}$ and $i_1 = \\frac{1}{2}$. The total return, $G_{\\mathrm{const}}$, is calculated as follows:\n$$\nG_{\\mathrm{const}} = \\left(\\frac{1}{2} - \\left(\\frac{1}{2}\\right)^2\\right) + \\left(\\frac{1}{2} - \\left(\\frac{1}{2}\\right)^2\\right)\n$$\n$$\nG_{\\mathrm{const}} = \\left(\\frac{1}{2} - \\frac{1}{4}\\right) + \\left(\\frac{1}{2} - \\frac{1}{4}\\right)\n$$\n$$\nG_{\\mathrm{const}} = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\n\nNext, consider the pulse-charging trajectory, where $i_0 = 1$ and $i_1 = 0$. The total return, $G_{\\mathrm{pulse}}$, is calculated as:\n$$\nG_{\\mathrm{pulse}} = (1 - 1^2) + (0 - 0^2)\n$$\n$$\nG_{\\mathrm{pulse}} = (1 - 1) + (0 - 0)\n$$\n$$\nG_{\\mathrm{pulse}} = 0 + 0 = 0\n$$\n\nFinally, we compute the difference between the return of the pulse-charging trajectory and the constant-current trajectory:\n$$\nG_{\\mathrm{pulse}} - G_{\\mathrm{const}} = 0 - \\frac{1}{2}\n$$\n$$\nG_{\\mathrm{pulse}} - G_{\\mathrm{const}} = -\\frac{1}{2}\n$$\nThis result demonstrates that, for the given reward structure, the constant-current trajectory yields a higher cumulative return than the pulse-charging trajectory, despite both delivering the same total charge ($i_0 + i_1 = 1$) and having their thermal effects perfectly managed. The penalty term $-i_t^2$ is non-linear and penalizes higher instantaneous currents more severely, leading to this bias.",
            "answer": "$$\n\\boxed{-\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "While many reinforcement learning problems are modeled as fully observable Markov Decision Processes (MDPs), real-world systems often contend with imperfect or incomplete state information. In battery management, for example, the true state-of-charge can be difficult to measure precisely, especially near the end-of-charge regime. This exercise challenges you to construct a counterexample that quantifies the danger of ignoring this partial observability, comparing a naive agent to a more sophisticated belief-aware agent that avoids hazardous overcurrents by reasoning about state uncertainty . This practice is essential for understanding how to design safe and robust control policies under the Partially Observable MDP (POMDP) framework.",
            "id": "3946159",
            "problem": "Consider a sequential optimal charging control problem modeled as a Partially Observable Markov Decision Process (POMDP) for a lithium-ion cell near end-of-charge. The underlying state variable is the state-of-charge $x_t \\in [0,1]$ at discrete time $t \\in \\{0,1,\\ldots,T-1\\}$. The agent selects a charging current $I_t \\in [0,I_{\\max}]$ each step. The physical evolution and observability are defined by the following elements derived from well-tested battery physics and circuit relations:\n\n- Charge balance: $x_{t+1} = \\min\\left(1, x_t + \\eta \\dfrac{I_t \\Delta t}{C}\\right)$, where $C$ is capacity in coulombs, $\\eta \\in (0,1]$ is coulombic efficiency, and $\\Delta t$ is the sampling interval in seconds.\n- Open-circuit voltage: $V_{\\mathrm{oc}}(x) = V_{\\min} + k_{\\mathrm{voc}} x$, a linearized approximation capturing monotonic increase with state-of-charge in the upper regime.\n- Internal resistance growth near end-of-charge: $R(x) = R_0 + \\dfrac{k_R}{1 - x + \\varepsilon}$, where $R_0 > 0$ is baseline ohmic resistance, $k_R > 0$ captures resistance growth as $x \\to 1$, and $\\varepsilon > 0$ regularizes the expression to remain finite and models finite steepness.\n- Terminal voltage under current $I_t$: $V_t = V_{\\mathrm{oc}}(x_t) + R(x_t) I_t$, by Ohm’s law combining voltage sources in series with internal resistance.\n- Maximum voltage constraint: $V_t \\le V_{\\max}$; violations represent overvoltage/overcurrent stress near end-of-charge.\n\nPartial observability arises from sensor saturation in the state-of-charge estimate. The observable at each step is\n$$\ny_t = \\min(x_t,\\, 1 - \\delta),\n$$\nwhere $\\delta \\in [0,1)$ encodes the saturation offset such that measurements cannot exceed $1 - \\delta$ even when the true $x_t$ is larger. The agent’s reward at time $t$ is defined as\n$$\nr_t(I_t, x_t) = I_t \\Delta t - \\lambda \\cdot \\mathbb{1}\\!\\left\\{V_{\\mathrm{oc}}(x_t) + R(x_t) I_t > V_{\\max}\\right\\},\n$$\nmeasured in coulombs ($\\mathrm{A}\\cdot \\mathrm{s}$). The objective is to maximize the cumulative reward $\\sum_{t=0}^{T-1} r_t(I_t, x_t)$.\n\nLet an agent that ignores partial observability treat $y_t$ as the true state and choose $I_t$ myopically to maximize immediate reward subject to the maximum voltage constraint evaluated at $y_t$. Let a belief-aware agent propagate a belief over $x_t$ using the observation model and charge balance, and at each step choose $I_t$ to maximize the expected immediate reward under a uniform belief over the admissible interval of $x_t$ consistent with the saturated observation and the dynamics. Both agents operate over $T$ steps starting from a specified initial $x_0$.\n\nYour task is to construct a counterexample demonstrating that ignoring partial observability leads to systematic overcurrent near end-of-charge and to quantify the performance loss as the difference in cumulative reward (in $\\mathrm{A}\\cdot \\mathrm{s}$) between the belief-aware agent and the agent that ignores partial observability. Use the following scientifically realistic parameterizations. Throughout, express the final performance loss in $\\mathrm{A}\\cdot \\mathrm{s}$ as decimal floats. Angles are not involved. Percentages must not be used.\n\nFundamental bases to use:\n- Ohm’s law for terminal voltage under load.\n- Monotone increase of open-circuit voltage with state-of-charge in the upper regime.\n- Charge conservation for state-of-charge dynamics.\n- Uniform prior on the hidden state induced by saturated observation and monotonic dynamics.\n\nImplement both agents as described above and simulate for the specified horizon, updating $x_t$ according to the physical model. For the belief-aware agent, maintain a conservative belief interval $[x^{\\min}_t, x^{\\max}_t]$ with $x^{\\max}_t = 1$ and $x^{\\min}_{t+1} = \\min\\!\\left(1, \\max\\!\\left(y_t, x^{\\min}_t + \\eta \\dfrac{I_t \\Delta t}{C}\\right)\\right)$. At each step, compute the expected immediate reward by integrating over a uniform grid on the current belief interval and choose $I_t$ to maximize this expected immediate reward. For the agent ignoring partial observability, at each step treat $y_t$ as the true state and choose $I_t$ to maximize immediate reward subject to the voltage constraint evaluated at $y_t$.\n\nTest suite. Use $T = 5$, $\\eta = 1$, and the parameter sets below:\n\n- Case 1 (happy path with saturation): $x_0 = 0.98$, $\\delta = 0.04$, $V_{\\min} = 3.0\\,\\mathrm{V}$, $k_{\\mathrm{voc}} = 1.2\\,\\mathrm{V}$, $V_{\\max} = 4.2\\,\\mathrm{V}$, $R_0 = 0.05\\,\\Omega$, $k_R = 0.04\\,\\Omega$, $\\varepsilon = 0.01$, $C = 10800\\,\\mathrm{C}$, $\\Delta t = 1\\,\\mathrm{s}$, $I_{\\max} = 2.0\\,\\mathrm{A}$, $\\lambda = 0.08\\,\\mathrm{C}$.\n- Case 2 (boundary, full observability): $x_0 = 0.96$, $\\delta = 0.0$, same remaining parameters as Case 1.\n- Case 3 (edge, steeper resistance growth): $x_0 = 0.99$, $\\delta = 0.02$, $V_{\\min} = 3.0\\,\\mathrm{V}$, $k_{\\mathrm{voc}} = 1.2\\,\\mathrm{V}$, $V_{\\max} = 4.2\\,\\mathrm{V}$, $R_0 = 0.05\\,\\Omega$, $k_R = 0.08\\,\\Omega$, $\\varepsilon = 0.01$, $C = 10800\\,\\mathrm{C}$, $\\Delta t = 1\\,\\mathrm{s}$, $I_{\\max} = 2.0\\,\\mathrm{A}$, $\\lambda = 0.12\\,\\mathrm{C}$.\n\nYour program should produce a single line of output containing the performance losses for the three cases as a comma-separated list enclosed in square brackets, for example, \"[loss_case1,loss_case2,loss_case3]\". Each loss must be a float in $\\mathrm{A}\\cdot \\mathrm{s}$.",
            "solution": "The solution requires simulating two different charging agents over a time horizon of $T$ steps and comparing their performance. The core of the problem lies in the different ways the agents handle uncertainty about the true state-of-charge, $x_t$.\n\n**Physical Model Implementation**\n\nThe physical behavior of the battery is governed by a set of given equations. We will implement these as helper functions.\n-   $V_{\\mathrm{oc}}(x_t) = V_{\\min} + k_{\\mathrm{voc}} x_t$: Open-circuit voltage as a linear function of state-of-charge.\n-   $R(x_t) = R_0 + k_R / (1 - x_t + \\varepsilon)$: Internal resistance, which increases sharply as $x_t \\to 1$.\n-   $V_t(x_t, I_t) = V_{\\mathrm{oc}}(x_t) + R(x_t) I_t$: Terminal voltage.\n-   $x_{t+1}(x_t, I_t) = \\min(1, x_t + \\eta I_t \\Delta t / C)$: State-of-charge update.\n-   $y_t(x_t) = \\min(x_t, 1 - \\delta)$: Saturated observation of the state.\n-   $r_t(x_t, I_t) = I_t \\Delta t - \\lambda \\cdot \\mathbb{1}\\{V_t(x_t, I_t) > V_{\\max}\\}$: The reward function, which incentivizes charging ($I_t > 0$) but penalizes voltage violations.\n\n**Agent 1: The Naive Agent**\n\nThis agent ignores the partial observability. At each time step $t$, it receives the observation $y_t$ and treats it as the true state. Its policy is to choose a current $I_t$ that is as large as possible without violating the voltage constraint *at the believed state $y_t$*. It aims to satisfy $V_{\\mathrm{oc}}(y_t) + R(y_t) I_t \\le V_{\\max}$. Solving for $I_t$, we get $I_t \\le (V_{\\max} - V_{\\mathrm{oc}}(y_t)) / R(y_t)$. The agent's chosen current is therefore:\n$$\nI_t^{\\text{naive}} = \\max\\left(0, \\min\\left(I_{\\max}, \\frac{V_{\\max} - V_{\\mathrm{oc}}(y_t)}{R(y_t)}\\right)\\right)\n$$\nThis action is taken, and the system evolves based on the *true* state $x_t$. The reward is also calculated based on the true state. The agent's misperception can lead to voltage violations if the true state $x_t$ is higher than the observed state $y_t$, because $R(x_t)$ will be higher than $R(y_t)$.\n\n**Agent 2: The Belief-Aware Agent**\n\nThis agent accounts for uncertainty. It maintains a belief about the current state $x_t$ in the form of a conservative interval $[x_t^{\\min}, 1]$. At $t=0$, we initialize $x_0^{\\min} = x_0$. For each subsequent step, the agent chooses a current $I_t$ to maximize the *expected* immediate reward, where the expectation is taken over a uniform distribution of $x_t$ on the belief interval $[x_t^{\\min}, 1]$.\n$$\n\\max_{I_t \\in [0, I_{\\max}]} \\mathbb{E}_{x \\sim U[x_t^{\\min}, 1]}[r_t(I_t, x)] = \\max_{I_t \\in [0, I_{\\max}]} \\int_{x_t^{\\min}}^{1} \\frac{1}{1-x_t^{\\min}} r_t(I_t, x) \\, dx\n$$\nSince this integral is complex, we will approximate it numerically by sampling points on the interval. For a set of candidate currents, we calculate the expected reward and select the current that yields the maximum.\nAfter choosing and applying $I_t$, the true state evolves to $x_{t+1}$ and the agent updates its belief for the next step using the specified formula:\n$$\nx_{t+1}^{\\min} = \\min\\left(1, \\max\\left(y_t, x_t^{\\min} + \\eta \\frac{I_t \\Delta t}{C}\\right)\\right)\n$$\nwhere $y_t$ is the observation of the true state $x_t$. This update rule incorporates both the new observation ($y_t$) and the dynamics of the system.\n\n**Simulation and Comparison**\n\nFor each test case, we will run two separate simulations, one for each agent, starting from the same initial state $x_0$ and proceeding for $T=5$ steps.\n1.  **Naive Agent Simulation**: At each step $t$, we compute $y_t$, determine $I_t^{\\text{naive}}$, calculate the true reward $r_t(x_t, I_t^{\\text{naive}})$, and update the true state to $x_{t+1}$. We sum the rewards to get the total cumulative reward.\n2.  **Belief-Aware Agent Simulation**: At each step $t$, we determine $I_t^{\\text{belief}}$ by maximizing the expected reward over $[x_t^{\\min}, 1]$, calculate the true reward $r_t(x_t, I_t^{\\text{belief}})$, update the true state to $x_{t+1}$, and update the belief lower bound to $x_{t+1}^{\\min}$. We sum the rewards to get the total.\n\nThe final performance loss is computed as $R_{\\text{total}}^{\\text{belief}} - R_{\\text{total}}^{\\text{naive}}$. A positive value indicates that a belief-aware approach is superior, successfully mitigating the negative effects of partial observability.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates and solves the battery charging POMDP problem.\n    \"\"\"\n\n    # --- Physical Model Functions ---\n\n    def V_oc(x, p):\n        \"\"\"Calculates open-circuit voltage.\"\"\"\n        return p['V_min'] + p['k_voc'] * x\n\n    def R(x, p):\n        \"\"\"Calculates internal resistance.\"\"\"\n        return p['R_0'] + p['k_R'] / (1.0 - x + p['epsilon'])\n\n    def V_terminal(x, I, p):\n        \"\"\"Calculates terminal voltage.\"\"\"\n        return V_oc(x, p) + R(x, p) * I\n\n    def charge_balance(x, I, p):\n        \"\"\"Updates state-of-charge.\"\"\"\n        return min(1.0, x + p['eta'] * I * p['delta_t'] / p['C'])\n\n    def observation(x, p):\n        \"\"\"Generates the saturated observation.\"\"\"\n        return min(x, 1.0 - p['delta'])\n\n    def reward(I, x, p):\n        \"\"\"Calculates the reward for a given action and true state.\"\"\"\n        voltage = V_terminal(x, I, p)\n        penalty = p['lambda'] if voltage > p['V_max'] else 0.0\n        return I * p['delta_t'] - penalty\n\n    # --- Agent Action Selection ---\n\n    def get_naive_action(y_t, p):\n        \"\"\"\n        Selects current for the naive agent, who treats y_t as the true state.\n        The agent maximizes current subject to the voltage constraint at y_t.\n        \"\"\"\n        if y_t >= 1.0 - p['epsilon']:\n            return 0.0\n            \n        R_at_y = R(y_t, p)\n        Voc_at_y = V_oc(y_t, p)\n        \n        if Voc_at_y >= p['V_max']:\n            return 0.0\n        \n        I_allowed = (p['V_max'] - Voc_at_y) / R_at_y\n        \n        return max(0.0, min(p['I_max'], I_allowed))\n\n    def get_belief_action(x_min_t, p, I_grid_size=201, x_grid_size=101):\n        \"\"\"\n        Selects current for the belief-aware agent.\n        Maximizes expected reward over the belief interval [x_min_t, 1.0].\n        \"\"\"\n        if x_min_t >= 1.0:\n            return 0.0\n\n        belief_interval_x = np.linspace(x_min_t, 1.0, x_grid_size)\n        current_options = np.linspace(0, p['I_max'], I_grid_size)\n        \n        best_I = 0.0\n        max_expected_reward = -np.inf\n\n        for I_candidate in current_options:\n            rewards_on_grid = np.array([reward(I_candidate, x, p) for x in belief_interval_x])\n            expected_reward = np.mean(rewards_on_grid)\n            \n            if expected_reward > max_expected_reward:\n                max_expected_reward = expected_reward\n                best_I = I_candidate\n                \n        return best_I\n\n    # --- Simulation Logic ---\n\n    def simulate_naive_agent(p):\n        \"\"\"Simulates the naive agent and returns total cumulative reward.\"\"\"\n        x_true = p['x0']\n        total_reward_val = 0.0\n        \n        for _ in range(p['T']):\n            y_t = observation(x_true, p)\n            I_t = get_naive_action(y_t, p)\n            r_t = reward(I_t, x_true, p)\n            total_reward_val += r_t\n            x_true = charge_balance(x_true, I_t, p)\n            \n        return total_reward_val\n\n    def simulate_belief_agent(p):\n        \"\"\"Simulates the belief-aware agent and returns total cumulative reward.\"\"\"\n        x_true = p['x0']\n        x_min = p['x0']\n        total_reward_val = 0.0\n        \n        for _ in range(p['T']):\n            I_t = get_belief_action(x_min, p)\n            r_t = reward(I_t, x_true, p)\n            total_reward_val += r_t\n            \n            x_true_next = charge_balance(x_true, I_t, p)\n            y_t = observation(x_true_next, p)\n            predicted_x_min = x_min + p['eta'] * I_t * p['delta_t'] / p['C']\n\n            x_min = min(1.0, max(y_t, predicted_x_min))\n            x_true = x_true_next\n\n        return total_reward_val\n\n    # --- Test Suite ---\n\n    common_params = {\n        'T': 5, 'eta': 1.0, 'delta_t': 1.0, 'C': 10800.0, 'I_max': 2.0,\n        'V_min': 3.0, 'k_voc': 1.2, 'V_max': 4.2, 'epsilon': 0.01\n    }\n\n    test_cases = [\n        {**common_params, 'x0': 0.98, 'delta': 0.04, 'R_0': 0.05, 'k_R': 0.04, 'lambda': 0.08},\n        {**common_params, 'x0': 0.96, 'delta': 0.0, 'R_0': 0.05, 'k_R': 0.04, 'lambda': 0.08},\n        {**common_params, 'x0': 0.99, 'delta': 0.02, 'R_0': 0.05, 'k_R': 0.08, 'lambda': 0.12},\n    ]\n\n    results = []\n    for params in test_cases:\n        reward_naive = simulate_naive_agent(params)\n        reward_belief = simulate_belief_agent(params)\n        performance_loss = reward_belief - reward_naive\n        results.append(performance_loss)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Transitioning from controlling a single cell to managing an entire battery pack introduces new layers of complexity, most notably the coupled dynamics between interacting cells. When multiple learning agents control individual cells, their updates can influence each other, potentially leading to system-wide instability. This practice provides a hands-on introduction to analyzing the stability of a multi-agent learning system using a linearized model derived from physical principles . You will use tools from linear systems theory to assess how parameters like learning rate and inter-cell coupling affect convergence, a critical skill for developing scalable and reliable charging protocols for large battery packs.",
            "id": "3946199",
            "problem": "You are tasked with evaluating stability and convergence properties of a simplified multi-agent reinforcement learning update for optimal charging protocol design in a battery pack with coupled cell dynamics, and proposing a training curriculum that scales with pack size. The context is automated battery design and simulation, and the agents correspond to per-cell charging controllers. The goal is to reason from first principles, starting from conservation laws and physically meaningful couplings, and to produce a complete, runnable program that computes stability metrics and a curriculum recommendation for specified test cases.\n\nConsider a battery pack of $N$ cells arranged in a one-dimensional ring geometry. Each cell $i$ has a local control policy parameter $\\theta_i$, reflecting a scalar parameterization of its charging control law near a fixed operating point. The multi-agent gradient-based update (per synchronous iteration index $t$) is linearized around the operating point and takes the form\n$$\n\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\alpha D \\boldsymbol{\\theta}_t - \\gamma L \\boldsymbol{\\theta}_t,\n$$\nwhere $\\boldsymbol{\\theta}_t \\in \\mathbb{R}^N$, $\\alpha > 0$ is the learning rate, $D \\in \\mathbb{R}^{N \\times N}$ is a diagonal positive definite curvature matrix capturing the local second-order sensitivity of the per-cell objective (a proxy for stress minimization that obeys conservation of charge and Ohm's law locally), $\\gamma \\ge 0$ is the coupling strength for a regularization that penalizes disagreement between neighboring cells stemming from coupled dynamics (e.g., Joule heating and conductive transfer), and $L \\in \\mathbb{R}^{N \\times N}$ is the graph Laplacian of the ring topology with nearest-neighbor coupling. The ring Laplacian is defined via an edge weight $\\kappa > 0$ as follows: for each $i$, $L_{ii} = 2\\kappa$, and for nearest neighbors $L_{i,i+1} = L_{i,i-1} = -\\kappa$ with indices taken modulo $N$; all other entries are $0$. This $L$ emerges by discretizing conservation laws and Fourier-type coupling on a ring with homogeneous nearest-neighbor interactions and is standard in coupled linear diffusion models.\n\nDefine the update matrix\n$$\nA(\\alpha,\\gamma,N,D,\\kappa) = I - \\alpha D - \\gamma L,\n$$\nwhere $I$ is the identity matrix. The discrete-time linear system is Schur stable if and only if the spectral radius $\\rho(A)$ is strictly less than $1$, that is, all eigenvalues of $A$ lie in the open unit disk. The spectral radius is defined by $\\rho(A) = \\max_i |\\lambda_i(A)|$ where $\\lambda_i(A)$ are the eigenvalues of $A$.\n\nYour program must:\n- Construct $L$ for a given $N$ and $\\kappa$ according to the ring definition above.\n- Construct $D$ according to the specified profile for each test case.\n- Form $A$ and compute the spectral radius $\\rho(A)$ numerically.\n- Compute an upper bound on $\\rho(A)$ using the induced infinity norm of $A$, i.e., the maximum absolute row sum $\\|A\\|_{\\infty} = \\max_i \\sum_j |A_{ij}|$, which is a well-tested matrix norm bound satisfying $\\rho(A) \\le \\|A\\|_{\\infty}$.\n- Determine stability as a boolean by checking whether $\\rho(A) < 1$.\n- Determine a curriculum that scales with pack size by incrementally increasing $N$ starting from $N_{\\min} = 2$ in steps of $\\Delta N$ until the specified $N_{\\mathrm{eval}}$ is reached or stability is lost. The recommended maximum stable pack size $N_{\\max}$ is then the largest $N \\le N_{\\mathrm{eval}}$ with $\\rho(A) < 1$. The curriculum is the list of pack sizes $[2, 2+\\Delta N, \\dots, N_{\\max}]$. If $N=2$ is unstable, the curriculum is the empty list.\n\nAngle units: whenever a sine function $\\sin(\\cdot)$ is used to define $D$, its argument must be in radians.\n\nThe final output must be purely numerical and dimensionless; do not include any physical units in the outputs. Express floats directly as decimal numbers. Round all reported floats (the spectral radius and the upper bound) to $6$ decimal places.\n\nTest Suite:\nProvide the results for the following test cases, which cover a general case, a near-boundary case, a strong-coupling edge case with heterogeneity, and a decoupled baseline case.\n\n- Case $1$ (general, homogeneous curvature):\n  - $N_{\\mathrm{eval}} = 8$\n  - $\\alpha = 0.05$\n  - $\\gamma = 0.1$\n  - $\\kappa = 0.25$\n  - $D_i = 0.8$ for all $i$ (uniform curvature)\n  - $\\Delta N = 2$\n\n- Case $2$ (near-boundary, homogeneous curvature):\n  - $N_{\\mathrm{eval}} = 4$\n  - $\\alpha = 0.45$\n  - $\\gamma = 0.2$\n  - $\\kappa = 0.25$\n  - $D_i = 0.6$ for all $i$ (uniform curvature)\n  - $\\Delta N = 2$\n\n- Case $3$ (strong coupling, heterogeneous curvature):\n  - $N_{\\mathrm{eval}} = 16$\n  - $\\alpha = 0.02$\n  - $\\gamma = 0.6$\n  - $\\kappa = 0.5$\n  - $D_i = 0.4 + 0.1 \\sin\\left(\\frac{2\\pi i}{N}\\right)$ for $i = 0,1,\\dots,N-1$ (radians)\n  - $\\Delta N = 4$\n\n- Case $4$ (decoupled baseline, homogeneous curvature):\n  - $N_{\\mathrm{eval}} = 8$\n  - $\\alpha = 0.2$\n  - $\\gamma = 0$\n  - $\\kappa = 0.25$\n  - $D_i = 0.5$ for all $i$ (uniform curvature)\n  - $\\Delta N = 2$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list with the following five entries: $[\\text{stable\\_boolean}, \\text{spectral\\_radius\\_float}, \\text{upper\\_bound\\_float}, \\text{N\\_max\\_int}, \\text{curriculum\\_list}]$. For example, the overall output should look like\n$$\n[\\,[\\text{bool}, \\text{float}, \\text{float}, \\text{int}, [\\text{int},\\dots]],\\,\\dots\\,]\n$$\nwith four such case results in order from Case $1$ to Case $4$.",
            "solution": "The user-provided problem is a valid, well-posed, and scientifically grounded exercise in computational linear algebra and stability theory, applied to a simplified model of multi-agent reinforcement learning for battery charging control. The problem is self-contained, with all parameters and procedures clearly defined. We will proceed with a solution.\n\nThe core of the problem is to analyze the stability of a discrete-time linear dynamical system described by the vector recurrence relation:\n$$\n\\boldsymbol{\\theta}_{t+1} = A \\boldsymbol{\\theta}_t\n$$\nwhere $\\boldsymbol{\\theta}_t \\in \\mathbb{R}^N$ is the state vector of control policy parameters for an $N$-cell battery pack at iteration $t$, and $A \\in \\mathbb{R}^{N \\times N}$ is the system's update matrix. The system represents a linearized multi-agent gradient-based update rule, a common model in optimization and control theory.\n\nThe update matrix $A$ is given by:\n$$\nA = I - \\alpha D - \\gamma L\n$$\nHere, $I$ is the $N \\times N$ identity matrix. The term $\\alpha D$ represents the local, per-cell learning dynamics. The learning rate $\\alpha > 0$ scales the contribution from $D$, a diagonal positive definite matrix representing the curvature (second-order sensitivity) of each cell's local objective function. The term $\\gamma L$ represents the coupling between cells. The coupling strength $\\gamma \\ge 0$ scales the graph Laplacian $L$ of the ring topology. This term acts as a regularizer, penalizing discrepancies between the policy parameters of neighboring cells, which models physical phenomena such as thermal conduction between cells.\n\nThe graph Laplacian $L$ for a 1D ring with $N$ nodes and uniform edge weight $\\kappa > 0$ is a symmetric, positive semi-definite matrix. Its entries are defined as:\n$$\nL_{ij} = \\begin{cases}\n2\\kappa & \\text{if } i=j \\\\\n-\\kappa & \\text{if } j = (i \\pm 1) \\pmod N \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThis structure arises naturally from the discretization of diffusion processes (like heat flow) on a discrete grid, reflecting the conservation laws mentioned in the problem description.\n\nThe stability of the system is determined by the eigenvalues of the matrix $A$. The system is convergent (i.e., $\\boldsymbol{\\theta}_t \\to \\mathbf{0}$ as $t \\to \\infty$) if and only if it is Schur stable. This condition requires that the spectral radius of $A$, denoted $\\rho(A)$, is strictly less than $1$. The spectral radius is the maximum absolute value of the eigenvalues of $A$:\n$$\n\\rho(A) = \\max_{i} |\\lambda_i(A)| < 1\n$$\nwhere $\\lambda_i(A)$ are the eigenvalues of $A$. Since $I$, $D$, and $L$ are all symmetric matrices, the update matrix $A$ is also symmetric. Consequently, all its eigenvalues $\\lambda_i(A)$ are real, simplifying the calculation of the spectral radius to $\\rho(A) = \\max_{i} |\\lambda_i(A)|$.\n\nThe computational procedure involves the following steps for a given set of parameters $(N, \\alpha, \\gamma, \\kappa, D)$:\n1.  **Construct Matrices**: The Laplacian matrix $L$ is constructed according to its definition for the specified $N$ and $\\kappa$. The diagonal matrix $D$ is constructed using the specified profile for its diagonal entries $D_{ii}$.\n2.  **Form Update Matrix**: The update matrix $A$ is assembled as $A = I - \\alpha D - \\gamma L$.\n3.  **Compute Eigenvalues**: A numerical eigensolver is employed to find the eigenvalues of the symmetric matrix $A$. For this, a numerically stable algorithm specialized for symmetric/Hermitian matrices is preferable.\n4.  **Calculate Spectral Radius**: The spectral radius $\\rho(A)$ is determined by finding the maximum absolute value among the computed eigenvalues.\n5.  **Calculate Norm Bound**: An upper bound for the spectral radius is given by any induced matrix norm. We calculate the infinity norm, $\\|A\\|_{\\infty}$, defined as the maximum absolute row sum: $\\|A\\|_{\\infty} = \\max_i \\sum_j |A_{ij}|$. This provides a useful, though sometimes loose, analytical bound on $\\rho(A)$.\n6.  **Determine Stability**: For a given $N$ (specifically $N_{\\mathrm{eval}}$ for the first part of the output), stability is a boolean value indicating whether $\\rho(A) < 1$.\n7.  **Generate Curriculum**: A training curriculum is designed by progressively increasing the system size. Starting from $N=2$ and incrementing by $\\Delta N$, we test for stability at each step. The process continues as long as the system remains stable and the size $N$ does not exceed $N_{\\mathrm{eval}}$. The recommended maximum stable pack size, $N_{\\max}$, is the largest $N$ in this sequence for which stability holds. The curriculum is the list of all stable pack sizes tested in this sequence. If the system is unstable even for the smallest size ($N=2$), the curriculum is an empty list.\n\nThis complete procedure is applied to each test case to generate the required outputs, which include stability metrics for $N_{\\mathrm{eval}}$ and the derived training curriculum.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the battery pack stability problem for the given test cases.\n    \"\"\"\n\n    # test_cases defines the parameters for each of the 4 cases.\n    # D_profile is a lambda function to generate the diagonal entries of D for a given N.\n    test_cases = [\n        {\n            \"N_eval\": 8, \"alpha\": 0.05, \"gamma\": 0.1, \"kappa\": 0.25,\n            \"D_profile\": lambda N: 0.8 * np.ones(N),\n            \"DeltaN\": 2\n        },\n        {\n            \"N_eval\": 4, \"alpha\": 0.45, \"gamma\": 0.2, \"kappa\": 0.25,\n            \"D_profile\": lambda N: 0.6 * np.ones(N),\n            \"DeltaN\": 2\n        },\n        {\n            \"N_eval\": 16, \"alpha\": 0.02, \"gamma\": 0.6, \"kappa\": 0.5,\n            \"D_profile\": lambda N: 0.4 + 0.1 * np.sin(2 * np.pi * np.arange(N) / N),\n            \"DeltaN\": 4\n        },\n        {\n            \"N_eval\": 8, \"alpha\": 0.2, \"gamma\": 0, \"kappa\": 0.25,\n            \"D_profile\": lambda N: 0.5 * np.ones(N),\n            \"DeltaN\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        N_eval = case[\"N_eval\"]\n        alpha = case[\"alpha\"]\n        gamma = case[\"gamma\"]\n        kappa = case[\"kappa\"]\n        D_profile = case[\"D_profile\"]\n        DeltaN = case[\"DeltaN\"]\n\n        # 1. Compute stability metrics for N = N_eval\n        rho_eval, norm_inf_eval = compute_properties(N_eval, alpha, gamma, kappa, D_profile)\n        \n        stable_boolean = rho_eval < 1.0\n        spectral_radius_float = round(rho_eval, 6)\n        upper_bound_float = round(norm_inf_eval, 6)\n\n        # 2. Determine the curriculum and N_max\n        N_max = 0\n        curriculum_list = []\n        is_first_unstable = False\n        \n        # Iterate from N=2 up to N_eval with step DeltaN\n        for N_curr in range(2, N_eval + 1, DeltaN):\n            if is_first_unstable:\n                break\n            \n            rho_curr, _ = compute_properties(N_curr, alpha, gamma, kappa, D_profile)\n            \n            if rho_curr < 1.0:\n                curriculum_list.append(N_curr)\n                N_max = N_curr\n            else:\n                # System is unstable, stop curriculum search\n                is_first_unstable = True\n        \n        # Assemble the results for the current case\n        case_result = [\n            stable_boolean,\n            spectral_radius_float,\n            upper_bound_float,\n            N_max,\n            curriculum_list\n        ]\n        results.append(case_result)\n\n    # Format the final output string as specified\n    final_output = \"[\" + \",\".join(map(str, results)) + \"]\"\n    final_output = final_output.replace(\"'\", \"\").replace(\" \", \"\")\n\n    print(final_output)\n\ndef compute_properties(N, alpha, gamma, kappa, D_profile):\n    \"\"\"\n    Constructs matrices and computes stability properties for a given configuration.\n\n    Args:\n        N (int): Number of cells.\n        alpha (float): Learning rate.\n        gamma (float): Coupling strength.\n        kappa (float): Laplacian edge weight.\n        D_profile (function): A function that takes N and returns the diagonal of D.\n\n    Returns:\n        tuple: A tuple containing the spectral radius (float) and the infinity norm (float).\n    \"\"\"\n    # Construct the graph Laplacian L for a 1D ring\n    L = np.zeros((N, N))\n    for i in range(N):\n        L[i, i] = 2 * kappa\n        L[i, (i + 1) % N] = -kappa\n        L[i, (i - 1 + N) % N] = -kappa\n\n    # Construct the diagonal curvature matrix D\n    D = np.diag(D_profile(N))\n\n    # Construct the update matrix A\n    I = np.eye(N)\n    A = I - alpha * D - gamma * L\n\n    # Since A is symmetric, its eigenvalues are real. We use eigvalsh for efficiency.\n    eigenvalues = np.linalg.eigvalsh(A)\n\n    # Compute the spectral radius rho(A)\n    spectral_radius = np.max(np.abs(eigenvalues))\n\n    # Compute the infinity norm ||A||_inf\n    infinity_norm = np.max(np.sum(np.abs(A), axis=1))\n\n    return spectral_radius, infinity_norm\n\nsolve()\n```"
        }
    ]
}