## Applications and Interdisciplinary Connections

Having established the foundational principles and mechanisms of Physics-Informed Neural Networks (PINNs) as applied to electrochemical systems, we now turn our attention to their practical utility. This chapter explores how these advanced computational tools are leveraged across a diverse range of applications, demonstrating their power to solve complex, real-world problems in battery engineering and beyond. We will see that the true value of PINNs lies not only in their ability to approximate solutions to complex partial differential equations (PDEs), but also in their capacity to serve as differentiable, physically consistent surrogates that can be integrated into larger engineering workflows.

We begin by situating PINNs within the broader landscape of [scientific modeling](@entry_id:171987), framing them as a sophisticated class of "gray-box" models that bridge the gap between first-principles theory and empirical data. This perspective is crucial for understanding their role in the development of high-fidelity digital twins. We will then survey several key application domains, including [optimal control](@entry_id:138479), automated design, electrochemical diagnostics, and lifetime prediction. Throughout these sections, the focus will remain on how the core principles of PINNs are adapted and extended to meet the unique challenges of each domain. Finally, we will examine the profound interdisciplinary connections to the fields of uncertainty quantification and explainable artificial intelligence (XAI), illustrating how PINNs can provide not just predictions, but also confidence bounds and interpretable physical insights.

### The Role of PINNs as Gray-Box Surrogates in Digital Twins

The modeling of complex physical systems, such as [lithium-ion batteries](@entry_id:150991), can be viewed as a spectrum. At one end lies the **white-box** model, which is derived entirely from first principles—in our case, the conservation laws and [constitutive relations](@entry_id:186508) of the Doyle-Fuller-Newman (DFN) model. These models offer maximum physical interpretability but can be computationally prohibitive and may suffer from bias if the underlying physical description is incomplete. At the other end is the **black-box** model, such as a standard neural network, which learns input-output relationships directly from data with minimal structural assumptions. While highly flexible, these models often require vast amounts of data, may generalize poorly outside their training distribution, and offer little physical insight.

Between these extremes lies the **gray-box** model, which strategically combines a known physical structure with data-driven components to capture unmodeled or poorly understood dynamics. PINNs are a preeminent example of this paradigm. By incorporating the residuals of governing PDEs into the training loss, they anchor the neural network's learning process to the bedrock of physical law. For instance, in modeling the thermal dynamics of a battery pack, a gray-box approach might combine a known energy balance structure—including terms for applied heating from a controller, $u(t)$, and convective cooling, $k(T - T_{\text{amb}})$—with a flexible, learned function $g_{\phi}(T, I)$ that captures complex, unmodeled heat sources arising from electrochemical side reactions. The resulting model, $\dot{T} = \frac{1}{C_{\theta}}(u - k_{\theta}(T - T_{\text{amb}})) + g_{\phi}(T, I)$, leverages the known physics to reduce the learning burden on the data-driven component, leading to better data efficiency and more robust generalization compared to a purely black-box approach that attempts to learn the entire dynamics from scratch .

This gray-box nature makes PINNs ideal components for **digital twins**—virtual replicas of physical assets that are updated in real-time with operational data. A digital twin requires models that are both computationally efficient for real-time execution and physically consistent to provide reliable predictions. Here, it is important to distinguish PINNs from another class of techniques known as projection-based Reduced-Order Models (ROMs). A typical projection-based ROM, such as one built using Proper Orthogonal Decomposition (POD) and Galerkin projection, approximates the high-dimensional state of the system $x(t)$ as a linear combination of a few basis functions, $x(t) \approx \Phi a(t)$. These basis functions are extracted from high-fidelity simulation data. The governing equations are then projected onto this low-dimensional subspace to yield a much smaller system of equations for the coefficients $a(t)$. In this way, the physical structure is explicitly retained and projected. A PINN, in contrast, does not perform such a projection; it learns the solution function directly, enforcing the physics "softly" through the loss function. While a well-constructed projection-based ROM can be designed to structurally preserve properties like mass conservation, a PINN achieves conservation approximately by minimizing the governing residuals across the domain .

Finally, it is useful to distinguish between two fundamental ways neural operators, a class of models that includes PINNs, can be employed. The first is learning a **solution operator**, which maps the defining parameters of a problem (e.g., material properties, boundary conditions) to its solution. This is a supervised learning task aimed at creating a fast surrogate for a conventional solver. The second is **discovering a [differential operator](@entry_id:202628)**, an unsupervised task where the model infers the governing PDE itself from measurements of the system's state over time. PINNs excel at both, making them a remarkably versatile tool for both engineering simulation and fundamental scientific discovery .

### Application Domain I: Optimal Control and Thermal Management

One of the most impactful applications of PINNs in [electrochemical engineering](@entry_id:271372) is in the domain of real-time control and management. Traditional battery management systems (BMS) rely on simple [equivalent circuit models](@entry_id:1124621) that lack predictive power regarding internal electrochemical states. High-fidelity models like the DFN are too slow for online control. Differentiable PINN surrogates elegantly solve this dilemma.

A prime example is the optimization of **fast-charging protocols**. The goal is to charge a battery as quickly as possible without inducing degradation mechanisms, such as lithium plating, or violating safety limits on temperature and voltage. This can be formulated as a Model Predictive Control (MPC) problem, where at each time step, an optimal [charging current](@entry_id:267426) profile is computed over a future horizon. The key challenge is the need for a predictive model that is both accurate and computationally efficient. A PINN trained on the DFN equations provides a differentiable map from the current input to the future states (e.g., concentration, potential, temperature). This [differentiability](@entry_id:140863) is crucial, as it allows for the use of efficient gradient-based optimization algorithms to solve the MPC problem at each step. The controller can thus explicitly minimize charging time while enforcing complex, state-dependent constraints on quantities like negative electrode overpotential (a proxy for plating risk), which are inaccessible to simpler models .

This control problem highlights the critical need for **coupled [electro-thermal modeling](@entry_id:1124257)**. The performance and degradation of a battery are governed by a tight interplay between electrochemical processes and heat generation. An increase in current leads to larger overpotentials, which in turn generate heat. This temperature rise affects kinetic and transport parameters such as the exchange current density, $i_0(T)$, and diffusion coefficients, $D_s(T)$, creating a strong feedback loop. PINNs are exceptionally well-suited to capture these multi-physics couplings. A single neural network, or a coupled architecture like a Deep Operator Network (DeepONet), can be trained to predict both the electrical response ($V(t)$) and the thermal response ($T(t)$) simultaneously. By including residuals from both the electrochemical (DFN) and thermal (energy balance) equations in the loss function, and by using shared layers or parameters within the network, the model can learn the intricate cross-couplings directly from the governing physics. For instance, such a model can capture the sensitivity of the terminal voltage to temperature, $\frac{\partial V}{\partial T}$, which arises from the combined effects of the entropic potential, the Arrhenius dependence of kinetics, and the temperature-dependent ohmic resistance .

A more granular understanding of thermal behavior, essential for designing effective **predictive thermal management** systems, can also be achieved. The total heat generated within a battery, $\dot{Q}_{\text{gen}}$, can be decomposed into its physical origins: irreversible ohmic heating ($I^2 R$), irreversible heating from reaction activation overpotentials ($I \eta_{\text{act}}$), and reversible [entropic heating](@entry_id:1124552) ($I T \frac{\partial U}{\partial T}$). A PINN-based DFN model can predict not just the total heat, but each of these individual components as they evolve in time and space. These physically decomposed heat sources can then be used as inputs to a system-level thermal model (e.g., a lumped-parameter or finite-element model) to predict the temperature distribution under various operating and cooling strategies. This provides engineers with a powerful tool to design and optimize battery packs and their thermal management systems for enhanced safety and lifetime .

### Application Domain II: Automated Design and Model Calibration

The traditional process of battery design is an expensive, iterative loop of fabrication and testing. PINNs offer a path toward accelerating this cycle through [virtual prototyping](@entry_id:1133826) and automated design. The key innovation is the development of **parameterized PINNs**, where design parameters—such as electrode thickness, porosity, or particle size—are treated as inputs to the neural network, alongside spatial and temporal coordinates.

By training such a network on the governing PDE residuals across a range of design parameters, the PINN learns a continuous and differentiable surrogate model that maps a design vector $\theta$ to a performance metric of interest, such as discharge capacity or power capability. For example, a PINN can be trained to predict the full spatio-temporal fields ($c_e, c_s, \phi_e, \phi_s$) as a function of design parameters. From these fields, any performance metric, like the time-to-cutoff-voltage, can be computed. Because the entire [computational graph](@entry_id:166548) is differentiable via automatic differentiation, one can efficiently compute the gradient of the performance metric with respect to the design variables, $\nabla_{\theta} M(\theta)$. This gradient can then be used in an [optimization algorithm](@entry_id:142787) to rapidly find an optimal cell design, drastically reducing the reliance on costly, time-consuming full-order simulations or physical experiments .

Beyond designing the battery itself, PINNs can also help design the experiments needed to characterize it. The process of **model calibration**, or estimating unknown physical parameters like diffusion coefficients ($D_s$) and [reaction rate constants](@entry_id:187887) ($k_0$), is fundamental to building accurate models. A significant challenge is determining which experiments are most informative for identifying a particular set of parameters. This is the domain of **[optimal experiment design](@entry_id:181055)**. A PINN provides a natural framework for this task. After training, the PINN surrogate can be used to compute the sensitivity of measurable outputs (e.g., voltage) to the unknown parameters. This sensitivity information is encapsulated in the Fisher Information Matrix (FIM), $\mathbf{F}$. The inverse of the FIM, $\mathbf{F}^{-1}$, provides a lower bound on the variance of any unbiased parameter estimator (the Cramér–Rao Lower Bound). By evaluating the FIM for various candidate experiments (e.g., different C-rates and temperatures), one can select the minimal set of experiments that collectively maximize the Fisher information, thereby minimizing the uncertainty of the estimated parameters to a desired level of confidence. This "meta-application" uses the PINN not just as a simulator, but as a tool to guide the physical scientific process itself, ensuring that laboratory resources are used with maximum efficiency .

### Application Domain III: Diagnostics and Degradation Modeling

Understanding and predicting the health and remaining useful life of a battery are critical for nearly all applications. PINNs are emerging as powerful tools for both non-invasive diagnostics and for modeling the complex, multi-physics phenomena that drive battery degradation.

**Electrochemical Impedance Spectroscopy (EIS)** is a standard non-invasive technique used to diagnose the internal state of a battery. It involves probing the cell with a small sinusoidal current over a range of frequencies and measuring the [complex impedance](@entry_id:273113) response, $Z(\omega)$. The resulting Nyquist plot contains features that correspond to different physical processes (e.g., charge transfer, diffusion). PINNs can be formulated to solve the linearized DFN equations directly in the frequency domain. Using a [harmonic balance](@entry_id:166315) approach, where time derivatives $\partial / \partial t$ are replaced by multiplication with $\mathrm{j}\omega$, a PINN can be trained to predict the complex-valued [phasors](@entry_id:270266) of the electrochemical state variables and, consequently, the full impedance spectrum. This provides a first-principles-based simulator for EIS that can be used for state estimation and health monitoring . Furthermore, such a model can be used to establish explicit analytical sensitivities between the features of a Nyquist plot and the underlying physical parameters. For example, one can relate the diameter of the high-frequency semicircle to the [charge-transfer resistance](@entry_id:263801) $R_{ct}$ (and thus the kinetic rate constant $k_0$) and the slope of the low-frequency tail to the Warburg impedance (and thus the solid diffusion coefficient $D_s$). This provides a rigorous link between observable diagnostic signatures and the unobservable physical properties that govern cell health .

Perhaps the most challenging aspect of [battery modeling](@entry_id:746700) is predicting long-term **degradation and lifetime**. Aging is driven by a multitude of coupled, parasitic side reactions, a primary one being the growth of the Solid Electrolyte Interphase (SEI) on the negative electrode. PINNs provide a framework for modeling such phenomena by coupling the main DFN equations with additional equations governing the side reactions. For instance, a model can include a parasitic current density, $j_{\text{SEI}}$, which consumes cyclable lithium and contributes to the growth of the resistive SEI layer. A PINN can be trained to solve this coupled system, learning the unknown kinetic parameters of the SEI growth reaction (e.g., its activation energy) by fitting to experimental data of [capacity fade](@entry_id:1122046) and impedance rise over time. This requires careful formulation of the conservation laws for both charge (current splitting between intercalation and side reactions) and mass (loss of cyclable lithium) .

In building such degradation surrogates, it is often advantageous to embed physical knowledge directly into the **neural network architecture**, rather than solely through the loss function. For example, it is known from first principles that SEI growth rates should be non-negative and should increase monotonically with temperature (Arrhenius dependence) and with increasing stress (e.g., higher state-of-charge, which corresponds to lower electrode potential). These monotonicity and positivity constraints can be "hard-coded" into the network architecture by, for example, constraining the signs of certain weights and using monotonic activation functions (like the `softplus` function). A surrogate model constructed in this way is guaranteed to be physically plausible by design, making it more robust and reliable for long-term [extrapolation](@entry_id:175955) than one trained with soft penalties alone .

### Interdisciplinary Connection: Uncertainty Quantification and Explainability

Beyond providing deterministic point predictions, a critical function of advanced models is to quantify their own uncertainty and provide interpretable results. PINNs are at the forefront of this push, connecting electrochemical modeling with the fields of Bayesian statistics and Explainable AI (XAI).

The standard PINN framework can be extended into a **Bayesian PINN** to enable principled **Uncertainty Quantification (UQ)**. Instead of finding a single optimal set of network weights and physical parameters, the Bayesian approach aims to infer a full posterior probability distribution over them. This is achieved by specifying prior distributions for all unknown parameters (e.g., Gaussian priors on network weights and physical parameters like $D_s$ and $k_0$) and combining them with a [likelihood function](@entry_id:141927) derived from both the [data misfit](@entry_id:748209) and the physics residuals. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior. The standard PINN loss function can thus be interpreted as the negative log of this posterior distribution, with the data and physics terms corresponding to the [negative log-likelihood](@entry_id:637801) and the [weight decay](@entry_id:635934) regularization terms corresponding to the negative log-prior. Sampling from this posterior distribution (e.g., using Markov Chain Monte Carlo methods) allows one to compute [credible intervals](@entry_id:176433) for any model prediction .

This quantified uncertainty is not merely an academic exercise; it has profound practical implications. For example, a probabilistic PINN can predict the [open-circuit voltage](@entry_id:270130) (OCV) not as a single value, but as a mean with a corresponding [credible interval](@entry_id:175131). The model can even learn a physically-informed uncertainty structure, for instance, predicting higher uncertainty in regions where the OCV curve is steep ($\left| \partial U / \partial \theta \right|$ is large). This uncertainty can then be **propagated** through the downstream DFN model equations to produce a full [credible interval](@entry_id:175131) for the terminal voltage curve under load. This provides engineers with crucial confidence bounds on performance predictions, enabling more robust design and control decisions under uncertainty .

Finally, the physics-informed nature of these models inherently enhances their **explainability**. By decomposing a complex phenomenon into components grounded in physical theory, the model becomes less of a "black box." A powerful technique in this regard is the use of **hybrid models** that incorporate known physical priors. For instance, the thermodynamic open-circuit voltage, $U(\theta, T)$, is a quantity that is relatively stable and can be measured accurately in [quasi-equilibrium](@entry_id:1130431) experiments. Instead of requiring a neural network to learn the entire voltage response from scratch, one can structure the model as $V(t) = U(\theta(t), T) + \eta_{\text{learned}}(t)$. Here, the well-understood thermodynamic component $U(\theta, T)$ is provided as a fixed physical prior, and the neural network is tasked only with learning the highly non-linear, non-equilibrium overpotential term, $\eta_{\text{learned}}(t)$, which lumps together kinetics and transport effects. This decomposition not only simplifies the learning problem, leading to better accuracy and data efficiency, but also makes the model's output more interpretable. Deviations from the [equilibrium potential](@entry_id:166921) are explicitly attributed to the learned dynamic terms, providing a clear and physically meaningful explanation for the battery's behavior under load .