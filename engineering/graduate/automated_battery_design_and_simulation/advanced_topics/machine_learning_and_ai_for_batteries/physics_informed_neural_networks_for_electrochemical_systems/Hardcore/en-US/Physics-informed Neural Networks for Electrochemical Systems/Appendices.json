{
    "hands_on_practices": [
        {
            "introduction": "The core of a Physics-Informed Neural Network is its composite loss function, which translates the governing laws of a system into a trainable objective. This exercise provides a foundational walkthrough of this process for a characteristic electrochemical problem: lithium diffusion in a solid electrode coupled with Butler-Volmer reaction kinetics at the boundary. By constructing the loss function from its constituent parts—the PDE residual, boundary condition residuals, and initial conditions—you will gain hands-on experience in encoding complex, multi-physics models into a PINN framework .",
            "id": "3955483",
            "problem": "You are tasked with designing a Physics-Informed Neural Network (PINN) for a one-dimensional solid-state diffusion subsystem of a battery Digital Twin (DT). The PINN must encode the diffusion Partial Differential Equation (PDE), boundary conditions, and a data assimilation term to compare against sparse sensor-like measurements. Your final output must be a complete, runnable program that computes the composite loss for three scientifically plausible test cases, each representing distinct electrochemical regimes. The program must produce a single line containing a comma-separated list enclosed in square brackets with one floating-point loss value per test case.\n\nThe physical subsystem is one-dimensional solid diffusion in a thin active-material slab with spatial coordinate $x \\in [0,L]$ and time $t \\in [0,T_{\\mathrm{end}}]$. The governing law is Fick’s second law, and the boundary flux at the external surface $x=L$ is constrained by Butler–Volmer (BV) kinetics with a Nernst equilibrium potential. The PINN approximates the lithium concentration field $\\hat{c}(x,t;\\theta)$.\n\nFundamental base and core definitions:\n- Fick’s second law: $\\dfrac{\\partial c}{\\partial t} = D \\dfrac{\\partial^2 c}{\\partial x^2}$, where $D$ is the solid diffusion coefficient.\n- Symmetry (no-flux) boundary at $x=0$: $\\left.\\dfrac{\\partial c}{\\partial x}\\right|_{x=0} = 0$.\n- Flux boundary at $x=L$: $-D \\left.\\dfrac{\\partial c}{\\partial x}\\right|_{x=L} = \\dfrac{j_{\\mathrm{BV}}}{n F}$, where $n$ is the number of electrons per reaction (here $n=1$), $F$ is Faraday’s constant, and $j_{\\mathrm{BV}}$ is the Butler–Volmer current density.\n- Butler–Volmer current density: $j_{\\mathrm{BV}}(c_s) = i_0(c_s)\\left[\\exp\\!\\left(\\dfrac{\\alpha_a F \\eta}{R T}\\right) - \\exp\\!\\left(-\\dfrac{\\alpha_c F \\eta}{R T}\\right)\\right]$, where $\\eta = E_{\\mathrm{app}} - U(c_s)$ is the overpotential, $E_{\\mathrm{app}}$ is applied potential, $\\alpha_a$ and $\\alpha_c$ are anodic and cathodic charge-transfer coefficients, $R$ is the gas constant, $T$ is temperature, and $i_0(c_s)$ is the exchange current density.\n- Nernst equilibrium potential: $U(c) = U_{\\mathrm{ref}} + \\dfrac{R T}{F}\\ln\\!\\left(\\dfrac{c}{c_{\\max}-c}\\right)$, where $c_{\\max}$ is the maximum concentration.\n- Exchange current density model: $i_0(c) = k_0 \\sqrt{c(c_{\\max}-c)}$, with kinetic prefactor $k_0$.\n- Initial condition: $c(x,0) = c_0$ for all $x$.\n\nTo ensure numerical robustness in the Butler–Volmer evaluation, use a regularized concentration at the boundary, $\\tilde{c}_s = \\max\\!\\left(\\epsilon,\\min\\!\\left(c_s,c_{\\max}-\\epsilon\\right)\\right)$, with small $\\epsilon > 0$.\n\nPINN architecture and analytical derivatives:\n- Use a single-hidden-layer feedforward network with $m$ neurons and hyperbolic tangent activation. Let the input be $(x,t)$, with weight matrix $W_1 \\in \\mathbb{R}^{m\\times 2}$, bias $b_1 \\in \\mathbb{R}^m$, output weights $W_2 \\in \\mathbb{R}^m$, and output bias $b_2 \\in \\mathbb{R}$.\n- The network output is\n$$\n\\hat{c}(x,t;\\theta) = b_2 + \\sum_{k=1}^{m} W_{2,k}\\,\\tanh\\!\\big(z_k(x,t)\\big), \\quad z_k(x,t) = b_{1,k} + W_{1,k,1} x + W_{1,k,2} t.\n$$\n- Let $\\operatorname{sech}(z) = 1/\\cosh(z)$ and note $\\dfrac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ and $\\dfrac{d^2}{dz^2}\\tanh(z) = -2\\,\\tanh(z)\\,\\operatorname{sech}^2(z)$.\n- The required derivatives are\n$$\n\\frac{\\partial \\hat{c}}{\\partial x} = \\sum_{k=1}^m W_{2,k}\\,\\operatorname{sech}^2(z_k)\\,W_{1,k,1}, \\quad\n\\frac{\\partial \\hat{c}}{\\partial t} = \\sum_{k=1}^m W_{2,k}\\,\\operatorname{sech}^2(z_k)\\,W_{1,k,2},\n$$\n$$\n\\frac{\\partial^2 \\hat{c}}{\\partial x^2} = \\sum_{k=1}^m W_{2,k}\\,\\big(-2\\,\\tanh(z_k)\\,\\operatorname{sech}^2(z_k)\\big)\\,W_{1,k,1}^2.\n$$\n\nComposite loss design:\n- Define residuals at interior collocation points $(x_i,t_i)$: $r_{\\mathrm{pde}}(x_i,t_i) = \\dfrac{\\partial \\hat{c}}{\\partial t}(x_i,t_i) - D\\,\\dfrac{\\partial^2 \\hat{c}}{\\partial x^2}(x_i,t_i)$.\n- Define symmetry boundary residuals at times $t_j$: $r_{0}(t_j) = \\dfrac{\\partial \\hat{c}}{\\partial x}(0,t_j)$.\n- Define flux boundary residuals at times $t_j$ using BV: $r_{L}(t_j) = -D\\,\\dfrac{\\partial \\hat{c}}{\\partial x}(L,t_j) - \\dfrac{j_{\\mathrm{BV}}(\\tilde{c}_s(L,t_j))}{n F}$.\n- Define initial condition residuals at positions $x_\\ell$: $r_{\\mathrm{ic}}(x_\\ell) = \\hat{c}(x_\\ell,0) - c_0$.\n- Define data mismatch residuals at given data points $(x_d,t_d)$ with target $c_{\\mathrm{data}}$: $r_{\\mathrm{data}}(x_d,t_d) = \\hat{c}(x_d,t_d) - c_{\\mathrm{data}}$.\n\n- The composite loss is\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}}\\,\\frac{1}{N_{\\mathrm{pde}}}\\sum_i r_{\\mathrm{pde}}(x_i,t_i)^2\n+ \\lambda_{0}\\,\\frac{1}{N_{0}}\\sum_j r_{0}(t_j)^2\n+ \\lambda_{L}\\,\\frac{1}{N_{L}}\\sum_j r_{L}(t_j)^2\n+ \\lambda_{\\mathrm{ic}}\\,\\frac{1}{N_{\\mathrm{ic}}}\\sum_\\ell r_{\\mathrm{ic}}(x_\\ell)^2\n+ \\lambda_{\\mathrm{data}}\\,\\frac{1}{N_{\\mathrm{data}}}\\sum_{d} r_{\\mathrm{data}}(x_d,t_d)^2,\n$$\nwith nonnegative weights $\\lambda_{\\cdot}$.\n\nScientific realism:\n- Use physically plausible ranges: $D$ on the order of $10^{-14}\\,\\mathrm{m^2/s}$, $L$ on the order of $10^{-6}\\,\\mathrm{m}$, $T$ near room temperature, exchange kinetics that yield reasonable $j_{\\mathrm{BV}}$ magnitudes, and concentration ranges strictly within $(0,c_{\\max})$. Use $n=1$.\n\nProgram requirements:\n- Implement the PINN and compute the composite loss for each test case using only analytical derivatives as defined above.\n- Do not train the network; simply evaluate the loss for the predefined parameter sets.\n- The final output must be a single line of the form $[\\ell_1,\\ell_2,\\ell_3]$, where $\\ell_k$ are floating-point losses for each test case. The loss values are dimensionless.\n\nUnits and angle specification:\n- Express all physical quantities internally in SI units: $x$ in $\\mathrm{m}$, $t$ in $\\mathrm{s}$, $c$ in $\\mathrm{mol/m^3}$, potentials in $\\mathrm{V}$, temperature in $\\mathrm{K}$, and current density in $\\mathrm{A/m^2}$. No angles are involved. The output loss values are unitless floats.\n\nTest suite:\n- Use three test cases with the following parameters and evaluation grids.\n\nCase $\\mathbf{1}$ (baseline, moderate flux):\n- $m = 3$,\n- $L = 1.0\\times 10^{-6}\\,\\mathrm{m}$,\n- $D = 1.0\\times 10^{-14}\\,\\mathrm{m^2/s}$,\n- $T = 298.0\\,\\mathrm{K}$,\n- $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$,\n- $F = 96485.0\\,\\mathrm{C/mol}$,\n- $\\alpha_a = 0.5$, $\\alpha_c = 0.5$,\n- $U_{\\mathrm{ref}} = 4.0\\,\\mathrm{V}$,\n- $c_{\\max} = 25000.0\\,\\mathrm{mol/m^3}$,\n- $c_0 = 15000.0\\,\\mathrm{mol/m^3}$,\n- $k_0 = 5.0\\,\\mathrm{A/m^2}$,\n- $E_{\\mathrm{app}} = 4.2\\,\\mathrm{V}$,\n- $\\epsilon = 1.0\\,\\mathrm{mol/m^3}$,\n- Weights: $W_1 = \\begin{bmatrix} 1.0\\times 10^{6} & -1.0 \\\\ -1.0\\times 10^{6} & 2.0 \\\\ 5.0\\times 10^{5} & -0.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}$, $W_2 = \\begin{bmatrix} 100.0 \\\\ -50.0 \\\\ 25.0 \\end{bmatrix}$, $b_2 = 15000.0$,\n- Interior collocation: $x \\in \\{L/4,\\,L/2,\\,3L/4\\} = \\{2.5\\times 10^{-7},\\,5.0\\times 10^{-7},\\,7.5\\times 10^{-7}\\}\\,\\mathrm{m}$, $t \\in \\{2.0,\\,5.0,\\,8.0\\}\\,\\mathrm{s}$,\n- Boundary times: $t \\in \\{1.0,\\,4.0,\\,7.0\\}\\,\\mathrm{s}$,\n- Initial positions: $x \\in \\{0.0,\\,L/3,\\,2L/3,\\,L\\} = \\{0.0,\\,3.\\overline{3}\\times 10^{-7},\\,6.\\overline{6}\\times 10^{-7},\\,1.0\\times 10^{-6}\\}\\,\\mathrm{m}$,\n- Data points: $(x,t,c_{\\mathrm{data}}) \\in \\{(0.75L,\\,5.0,\\,15050.0),\\,(0.25L,\\,5.0,\\,14980.0)\\}$,\n- Loss weights: $\\lambda_{\\mathrm{pde}} = 1.0$, $\\lambda_{0} = 10.0$, $\\lambda_{L} = 10.0$, $\\lambda_{\\mathrm{ic}} = 5.0$, $\\lambda_{\\mathrm{data}} = 2.0$.\n\nCase $\\mathbf{2}$ (strong kinetics and higher overpotential):\n- $m = 3$,\n- $L = 1.0\\times 10^{-6}\\,\\mathrm{m}$,\n- $D = 5.0\\times 10^{-15}\\,\\mathrm{m^2/s}$,\n- $T = 298.0\\,\\mathrm{K}$,\n- $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$,\n- $F = 96485.0\\,\\mathrm{C/mol}$,\n- $\\alpha_a = 0.5$, $\\alpha_c = 0.5$,\n- $U_{\\mathrm{ref}} = 4.0\\,\\mathrm{V}$,\n- $c_{\\max} = 25000.0\\,\\mathrm{mol/m^3}$,\n- $c_0 = 15000.0\\,\\mathrm{mol/m^3}$,\n- $k_0 = 8.0\\,\\mathrm{A/m^2}$,\n- $E_{\\mathrm{app}} = 4.5\\,\\mathrm{V}$,\n- $\\epsilon = 1.0\\,\\mathrm{mol/m^3}$,\n- Weights: $W_1 = \\begin{bmatrix} 0.8\\times 10^{6} & -0.8 \\\\ -0.8\\times 10^{6} & 1.5 \\\\ 0.4\\times 10^{6} & -0.3 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\\\ 0.1 \\end{bmatrix}$, $W_2 = \\begin{bmatrix} 120.0 \\\\ -60.0 \\\\ 30.0 \\end{bmatrix}$, $b_2 = 15000.0$,\n- Interior collocation: $x \\in \\{2.5\\times 10^{-7},\\,5.0\\times 10^{-7},\\,7.5\\times 10^{-7}\\}\\,\\mathrm{m}$, $t \\in \\{2.0,\\,5.0,\\,8.0\\}\\,\\mathrm{s}$,\n- Boundary times: $t \\in \\{1.0,\\,4.0,\\,7.0\\}\\,\\mathrm{s}$,\n- Initial positions: $x \\in \\{0.0,\\,3.\\overline{3}\\times 10^{-7},\\,6.\\overline{6}\\times 10^{-7},\\,1.0\\times 10^{-6}\\}\\,\\mathrm{m}$,\n- Data points: $(x,t,c_{\\mathrm{data}}) \\in \\{(0.75L,\\,5.0,\\,15100.0),\\,(0.25L,\\,5.0,\\,14950.0)\\}$,\n- Loss weights: $\\lambda_{\\mathrm{pde}} = 1.0$, $\\lambda_{0} = 10.0$, $\\lambda_{L} = 10.0$, $\\lambda_{\\mathrm{ic}} = 5.0$, $\\lambda_{\\mathrm{data}} = 2.0$.\n\nCase $\\mathbf{3}$ (near saturation, lower overpotential):\n- $m = 3$,\n- $L = 1.0\\times 10^{-6}\\,\\mathrm{m}$,\n- $D = 1.0\\times 10^{-14}\\,\\mathrm{m^2/s}$,\n- $T = 298.0\\,\\mathrm{K}$,\n- $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$,\n- $F = 96485.0\\,\\mathrm{C/mol}$,\n- $\\alpha_a = 0.5$, $\\alpha_c = 0.5$,\n- $U_{\\mathrm{ref}} = 4.0\\,\\mathrm{V}$,\n- $c_{\\max} = 25000.0\\,\\mathrm{mol/m^3}$,\n- $c_0 = 24000.0\\,\\mathrm{mol/m^3}$,\n- $k_0 = 4.0\\,\\mathrm{A/m^2}$,\n- $E_{\\mathrm{app}} = 3.9\\,\\mathrm{V}$,\n- $\\epsilon = 1.0\\,\\mathrm{mol/m^3}$,\n- Weights: $W_1 = \\begin{bmatrix} 1.0\\times 10^{6} & -1.0 \\\\ -1.0\\times 10^{6} & 2.0 \\\\ 5.0\\times 10^{5} & -0.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}$, $W_2 = \\begin{bmatrix} 50.0 \\\\ -25.0 \\\\ 12.5 \\end{bmatrix}$, $b_2 = 24000.0$,\n- Interior collocation: $x \\in \\{2.5\\times 10^{-7},\\,5.0\\times 10^{-7},\\,7.5\\times 10^{-7}\\}\\,\\mathrm{m}$, $t \\in \\{2.0,\\,5.0,\\,8.0\\}\\,\\mathrm{s}$,\n- Boundary times: $t \\in \\{1.0,\\,4.0,\\,7.0\\}\\,\\mathrm{s}$,\n- Initial positions: $x \\in \\{0.0,\\,3.\\overline{3}\\times 10^{-7},\\,6.\\overline{6}\\times 10^{-7},\\,1.0\\times 10^{-6}\\}\\,\\mathrm{m}$,\n- Data points: $(x,t,c_{\\mathrm{data}}) \\in \\{(0.75L,\\,5.0,\\,23950.0),\\,(0.25L,\\,5.0,\\,24050.0)\\}$,\n- Loss weights: $\\lambda_{\\mathrm{pde}} = 1.0$, $\\lambda_{0} = 10.0$, $\\lambda_{L} = 10.0$, $\\lambda_{\\mathrm{ic}} = 5.0$, $\\lambda_{\\mathrm{data}} = 2.0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\ell_1,\\ell_2,\\ell_3]$), where the entries are the composite loss values (floats) for Case $\\mathbf{1}$, Case $\\mathbf{2}$, and Case $\\mathbf{3}$, respectively. No other text should be printed.",
            "solution": "The problem is valid. It presents a well-posed, scientifically grounded task to compute the composite loss of a Physics-Informed Neural Network (PINN) designed to model solid-state diffusion in a battery electrode. All necessary physical laws, model equations, network parameters, and evaluation points are provided, enabling a direct and verifiable computation.\n\n### Principle-Based Design for PINN Loss Computation\n\nThe core of this problem lies in evaluating how well a given neural network, with fixed parameters $\\theta$, satisfies a set of physical laws and data constraints. This is quantified by a composite loss function, $\\mathcal{L}(\\theta)$. The PINN framework replaces the traditional numerical discretization of a Partial Differential Equation (PDE) system with a differentiable, analytical representation—the neural network—and recasts the problem of solving the PDE into an optimization problem of minimizing the loss function. Here, the task is a single forward pass: to compute the loss for a pre-defined network, not to perform the optimization (training).\n\n**1. Physical System and Mathematical Model**\n\nThe physical system is the one-dimensional diffusion of lithium ions within a solid active material slab of thickness $L$. The concentration of lithium, $c(x,t)$, is governed by Fick's second law, a fundamental principle of mass transfer:\n$$\n\\frac{\\partial c}{\\partial t} = D \\frac{\\partial^2 c}{\\partial x^2}\n$$\nwhere $D$ is the diffusion coefficient, $x \\in [0,L]$ is the spatial coordinate, and $t$ is time. The system is subject to specific initial and boundary conditions that define a unique physical scenario:\n- **Initial Condition:** At time $t=0$, the concentration is uniform: $c(x,0) = c_0$.\n- **Symmetry Boundary Condition:** At the center of the slab ($x=0$), there is no flux, representing symmetry: $\\frac{\\partial c}{\\partial x}(0,t) = 0$.\n- **Flux Boundary Condition:** At the slab's surface ($x=L$), the flux of lithium ions is driven by an electrochemical reaction. The flux is related to the Butler-Volmer current density, $j_{\\mathrm{BV}}$, which models the kinetics of the charge-transfer reaction:\n$$\n-D \\frac{\\partial c}{\\partial x}(L,t) = \\frac{j_{\\mathrm{BV}}(\\tilde{c}_s(L,t))}{nF}\n$$\nThe Butler-Volmer equation depends on the surface overpotential $\\eta = E_{\\mathrm{app}} - U(c_s)$, which is the difference between the applied potential $E_{\\mathrm{app}}$ and the Nernst equilibrium potential $U(c_s)$. The Nernst potential itself is a function of the surface concentration $c_s = c(L,t)$, and the exchange current density $i_0$ also depends on $c_s$. These relationships tightly couple the PDE to electrochemical principles.\n\n**2. The PINN Approximation**\n\nThe PINN, denoted $\\hat{c}(x,t;\\theta)$, serves as an analytical surrogate for the true concentration field $c(x,t)$. For this problem, a single-hidden-layer feedforward network with hyperbolic tangent ($\\tanh$) activation functions is specified. The network's output is:\n$$\n\\hat{c}(x,t;\\theta) = b_2 + \\sum_{k=1}^{m} W_{2,k}\\,\\tanh(b_{1,k} + W_{1,k,1} x + W_{1,k,2} t)\n$$\nA key advantage of this representation is that its derivatives with respect to its inputs, $x$ and $t$, can be computed analytically and exactly via the chain rule. The problem provides these analytical expressions, which are crucial for evaluating the PDE and boundary condition residuals without numerical differentiation errors. For instance, the time derivative is:\n$$\n\\frac{\\partial \\hat{c}}{\\partial t} = \\sum_{k=1}^m W_{2,k}\\,\\operatorname{sech}^2(z_k)\\,W_{1,k,2}\n$$\nwhere $z_k$ is the argument of the $k$-th neuron's activation function and $\\operatorname{sech}^2$ is the derivative of $\\tanh$.\n\n**3. The Composite Loss Function**\n\nThe composite loss function, $\\mathcal{L}(\\theta)$, is the sum of mean squared residuals, where each residual measures the violation of a specific physical law or data constraint. The total loss is a weighted sum of five components:\n\n- **PDE Residual Loss ($\\mathcal{L}_{\\mathrm{pde}}$):** This term enforces Fick's second law at a set of collocation points in the interior of the $(x,t)$ domain. The residual is $r_{\\mathrm{pde}} = \\frac{\\partial \\hat{c}}{\\partial t} - D\\frac{\\partial^2 \\hat{c}}{\\partial x^2}$. A small residual indicates that the network's output is a good solution to the PDE.\n- **Symmetry Boundary Loss ($\\mathcal{L}_{0}$):** This term enforces the no-flux condition at $x=0$. The residual is $r_{0} = \\frac{\\partial \\hat{c}}{\\partial x}(0,t)$.\n- **Flux Boundary Loss ($\\mathcal{L}_{L}$):** This term enforces the Butler-Volmer flux condition at $x=L$. The residual is $r_{L} = -D\\frac{\\partial \\hat{c}}{\\partial x}(L,t) - \\frac{j_{\\mathrm{BV}}(\\tilde{c}_s(L,t))}{nF}$. This is the most complex term, as it involves evaluating the non-linear Butler-Volmer kinetics based on the PINN's predicted surface concentration. A regularization, $\\tilde{c}_s = \\max(\\epsilon, \\min(c_s, c_{\\max}-\\epsilon))$, is used to ensure numerical stability in the logarithmic and square-root terms of the Nernst and exchange current density models, preventing arguments from reaching $0$ or $c_{\\max}$.\n- **Initial Condition Loss ($\\mathcal{L}_{\\mathrm{ic}}$):** This term anchors the simulation to its starting state by penalizing deviations from the initial concentration $c_0$. The residual is $r_{\\mathrm{ic}}(x_{\\ell}) = \\hat{c}(x_\\ell,0) - c_0$.\n- **Data Mismatch Loss ($\\mathcal{L}_{\\mathrm{data}}$):** This term represents the data assimilation aspect of the digital twin. It measures the discrepancy between the PINN's prediction and sparse \"sensor\" measurements, $c_{\\mathrm{data}}$, at specific locations $(x_d, t_d)$. The residual is $r_{\\mathrm{data}}(x_d,t_d) = \\hat{c}(x_d,t_d) - c_{\\mathrm{data}}$.\n\nThe total loss is the weighted sum of the mean squared values of these residuals:\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}}\\,\\mathcal{L}_{\\mathrm{pde}}\n+ \\lambda_{0}\\,\\mathcal{L}_{0}\n+ \\lambda_{L}\\,\\mathcal{L}_{L}\n+ \\lambda_{\\mathrm{ic}}\\,\\mathcal{L}_{\\mathrm{ic}}\n+ \\lambda_{\\mathrm{data}}\\,\\mathcal{L}_{\\mathrm{data}}\n$$\nThe weights $\\lambda_{\\cdot}$ balance the relative importance of each physical constraint and data source.\n\n**4. Algorithmic Implementation**\n\nThe solution program implements this framework precisely.\n1.  **Parameterization:** The physical constants, model parameters, network weights, collocation points, and loss weights for each of the three test cases are organized into data structures.\n2.  **Vectorized PINN Functions:** The PINN output $\\hat{c}$ and its analytical derivatives ($\\frac{\\partial\\hat{c}}{\\partial t}$, $\\frac{\\partial\\hat{c}}{\\partial x}$, $\\frac{\\partial^2\\hat{c}}{\\partial x^2}$) are implemented as vectorized Python functions using `NumPy`. This allows for efficient evaluation over batches of $(x,t)$ points. For example, the summation over neurons becomes a matrix-vector product.\n3.  **Residual Calculation:** For each of the five loss components, the corresponding residuals are calculated:\n    - Points for each residual type are generated as specified.\n    - The PINN and its derivative functions are called with these points to get the necessary terms.\n    - The non-linear electrochemical functions (Nernst, exchange current, Butler-Volmer) are evaluated as needed for the $\\mathcal{L}_L$ term.\n    - The residual for each point is computed according to its definition.\n4.  **Loss Aggregation:** The squared residuals for each component are averaged, and the final composite loss is computed as the weighted sum of these mean squared errors.\n5.  **Execution Loop:** A main loop iterates through the three test cases, executing the loss computation for each and collecting the results. The final output is formatted as a comma-separated list in brackets, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute the composite PINN loss for each.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline, moderate flux)\n        {\n            \"m\": 3, \"L\": 1.0e-6, \"D\": 1.0e-14, \"T\": 298.0, \"R\": 8.314, \"F\": 96485.0,\n            \"alpha_a\": 0.5, \"alpha_c\": 0.5, \"U_ref\": 4.0, \"c_max\": 25000.0, \"c_0\": 15000.0,\n            \"k_0\": 5.0, \"E_app\": 4.2, \"epsilon\": 1.0, \"n\": 1.0,\n            \"W1\": np.array([[1.0e6, -1.0], [-1.0e6, 2.0], [5.0e5, -0.5]]),\n            \"b1\": np.array([0.0, 0.1, -0.1]),\n            \"W2\": np.array([100.0, -50.0, 25.0]),\n            \"b2\": 15000.0,\n            \"x_pde_rel\": np.array([1/4, 1/2, 3/4]), \"t_pde\": np.array([2.0, 5.0, 8.0]),\n            \"t_bc\": np.array([1.0, 4.0, 7.0]),\n            \"x_ic_rel\": np.array([0.0, 1/3, 2/3, 1.0]),\n            \"data_points\": np.array([[0.75, 5.0, 15050.0], [0.25, 5.0, 14980.0]]),\n            \"lambdas\": {\"pde\": 1.0, \"bc0\": 10.0, \"bcL\": 10.0, \"ic\": 5.0, \"data\": 2.0}\n        },\n        # Case 2 (strong kinetics and higher overpotential)\n        {\n            \"m\": 3, \"L\": 1.0e-6, \"D\": 5.0e-15, \"T\": 298.0, \"R\": 8.314, \"F\": 96485.0,\n            \"alpha_a\": 0.5, \"alpha_c\": 0.5, \"U_ref\": 4.0, \"c_max\": 25000.0, \"c_0\": 15000.0,\n            \"k_0\": 8.0, \"E_app\": 4.5, \"epsilon\": 1.0, \"n\": 1.0,\n            \"W1\": np.array([[0.8e6, -0.8], [-0.8e6, 1.5], [0.4e6, -0.3]]),\n            \"b1\": np.array([0.05, -0.05, 0.1]),\n            \"W2\": np.array([120.0, -60.0, 30.0]),\n            \"b2\": 15000.0,\n            \"x_pde_rel\": np.array([1/4, 1/2, 3/4]), \"t_pde\": np.array([2.0, 5.0, 8.0]),\n            \"t_bc\": np.array([1.0, 4.0, 7.0]),\n            \"x_ic_rel\": np.array([0.0, 1/3, 2/3, 1.0]),\n            \"data_points\": np.array([[0.75, 5.0, 15100.0], [0.25, 5.0, 14950.0]]),\n            \"lambdas\": {\"pde\": 1.0, \"bc0\": 10.0, \"bcL\": 10.0, \"ic\": 5.0, \"data\": 2.0}\n        },\n        # Case 3 (near saturation, lower overpotential)\n        {\n            \"m\": 3, \"L\": 1.0e-6, \"D\": 1.0e-14, \"T\": 298.0, \"R\": 8.314, \"F\": 96485.0,\n            \"alpha_a\": 0.5, \"alpha_c\": 0.5, \"U_ref\": 4.0, \"c_max\": 25000.0, \"c_0\": 24000.0,\n            \"k_0\": 4.0, \"E_app\": 3.9, \"epsilon\": 1.0, \"n\": 1.0,\n            \"W1\": np.array([[1.0e6, -1.0], [-1.0e6, 2.0], [5.0e5, -0.5]]),\n            \"b1\": np.array([0.0, 0.1, -0.1]),\n            \"W2\": np.array([50.0, -25.0, 12.5]),\n            \"b2\": 24000.0,\n            \"x_pde_rel\": np.array([1/4, 1/2, 3/4]), \"t_pde\": np.array([2.0, 5.0, 8.0]),\n            \"t_bc\": np.array([1.0, 4.0, 7.0]),\n            \"x_ic_rel\": np.array([0.0, 1/3, 2/3, 1.0]),\n            \"data_points\": np.array([[0.75, 5.0, 23950.0], [0.25, 5.0, 24050.0]]),\n            \"lambdas\": {\"pde\": 1.0, \"bc0\": 10.0, \"bcL\": 10.0, \"ic\": 5.0, \"data\": 2.0}\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        loss = compute_composite_loss(params)\n        results.append(loss)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\ndef compute_composite_loss(params):\n    \"\"\"\n    Computes the total composite loss for a given set of parameters.\n    \"\"\"\n    \n    # Unpack network parameters for convenience\n    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n    \n    # --- PINN and derivatives definitions ---\n    def get_z(x, t):\n        xt_pairs = np.stack([x.flatten(), t.flatten()], axis=1)\n        z = xt_pairs @ W1.T + b1\n        return z\n\n    def pinn_c(x, t):\n        z = get_z(x, t)\n        c = np.tanh(z) @ W2 + b2\n        return c.reshape(x.shape)\n\n    def pinn_dc_dx(x, t):\n        z = get_z(x, t)\n        sech_z_sq = (1.0 / np.cosh(z))**2\n        dc_dx = (sech_z_sq * W1[:, 0]) @ W2\n        return dc_dx.reshape(x.shape)\n\n    def pinn_dc_dt(x, t):\n        z = get_z(x, t)\n        sech_z_sq = (1.0 / np.cosh(z))**2\n        dc_dt = (sech_z_sq * W1[:, 1]) @ W2\n        return dc_dt.reshape(x.shape)\n\n    def pinn_d2c_dx2(x, t):\n        z = get_z(x, t)\n        tanh_z = np.tanh(z)\n        sech_z_sq = (1.0 / np.cosh(z))**2\n        term = -2 * tanh_z * sech_z_sq * (W1[:, 0]**2)\n        d2c_dx2 = term @ W2\n        return d2c_dx2.reshape(x.shape)\n\n    # --- Electrochemical functions ---\n    def butler_volmer(c_s):\n        R, T, F = params[\"R\"], params[\"T\"], params[\"F\"]\n        c_max, k_0 = params[\"c_max\"], params[\"k_0\"]\n        alpha_a, alpha_c = params[\"alpha_a\"], params[\"alpha_c\"]\n        E_app, U_ref, epsilon = params[\"E_app\"], params[\"U_ref\"], params[\"epsilon\"]\n\n        c_s_reg = np.maximum(epsilon, np.minimum(c_s, c_max - epsilon))\n        \n        # Exchange current density i_0\n        i_0 = k_0 * np.sqrt(c_s_reg * (c_max - c_s_reg))\n        \n        # Nernst potential U\n        U = U_ref + (R * T / F) * np.log(c_s_reg / (c_max - c_s_reg))\n        \n        # Overpotential eta\n        eta = E_app - U\n        \n        # BV equation\n        term_exp = F * eta / (R * T)\n        j_bv = i_0 * (np.exp(alpha_a * term_exp) - np.exp(-alpha_c * term_exp))\n        return j_bv\n\n    # --- Residual Calculations ---\n    L = params[\"L\"]\n    \n    # 1. PDE Loss\n    x_pde_grid, t_pde_grid = np.meshgrid(params[\"x_pde_rel\"] * L, params[\"t_pde\"])\n    dc_dt_vals = pinn_dc_dt(x_pde_grid, t_pde_grid)\n    d2c_dx2_vals = pinn_d2c_dx2(x_pde_grid, t_pde_grid)\n    r_pde = dc_dt_vals - params[\"D\"] * d2c_dx2_vals\n    loss_pde = np.mean(r_pde**2)\n    \n    # 2. Boundary Loss at x=0\n    x_bc0 = np.zeros_like(params[\"t_bc\"])\n    r_bc0 = pinn_dc_dx(x_bc0, params[\"t_bc\"])\n    loss_bc0 = np.mean(r_bc0**2)\n    \n    # 3. Boundary Loss at x=L\n    x_bcL = np.full_like(params[\"t_bc\"], L)\n    c_s_L = pinn_c(x_bcL, params[\"t_bc\"])\n    j_bv_vals = butler_volmer(c_s_L)\n    dc_dx_L = pinn_dc_dx(x_bcL, params[\"t_bc\"])\n    r_bcL = -params[\"D\"] * dc_dx_L - j_bv_vals / (params[\"n\"] * params[\"F\"])\n    loss_bcL = np.mean(r_bcL**2)\n    \n    # 4. Initial Condition Loss\n    t_ic = np.zeros_like(params[\"x_ic_rel\"])\n    c_ic = pinn_c(params[\"x_ic_rel\"] * L, t_ic)\n    r_ic = c_ic - params[\"c_0\"]\n    loss_ic = np.mean(r_ic**2)\n\n    # 5. Data Loss\n    if params[\"data_points\"].shape[0] > 0:\n        x_data = params[\"data_points\"][:, 0] * L\n        t_data = params[\"data_points\"][:, 1]\n        c_target = params[\"data_points\"][:, 2]\n        c_pred = pinn_c(x_data, t_data)\n        r_data = c_pred - c_target\n        loss_data = np.mean(r_data**2)\n    else:\n        loss_data = 0.0\n\n    # --- Composite Loss ---\n    lmb = params[\"lambdas\"]\n    total_loss = (lmb[\"pde\"] * loss_pde +\n                    lmb[\"bc0\"] * loss_bc0 +\n                    lmb[\"bcL\"] * loss_bcL +\n                    lmb[\"ic\"] * loss_ic +\n                    lmb[\"data\"] * loss_data)\n    \n    return total_loss\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "While PINNs excel at enforcing differential equations, real-world systems must also adhere to inequality constraints, such as the non-negativity of concentrations or limits on state-of-charge. This practice addresses the critical task of enforcing such physical admissibility by designing smooth, differentiable penalty terms that discourage the network from making non-physical predictions. You will implement penalties to prevent negative concentrations and to flag conditions conducive to lithium plating, a key failure mode in batteries, thereby making the PINN model more robust and reliable .",
            "id": "3940640",
            "problem": "Design and implement a program that constructs Physics-Informed Neural Network (PINN) loss terms for a Lithium-Ion Battery (LIB) electrode-electrolyte system that enforce physically admissible states during training. The PINN is trained on collocation points in space and time, where it outputs predicted fields. The goal is to define three smooth loss terms that impose the following physically motivated constraints: non-negativity of electrolyte concentration, upper bounds for solid-phase lithium concentration, and penalization of excursions into lithium plating regimes. The program must compute these losses for a specified test suite of predicted values and return the aggregate loss per test case.\n\nUse the following context as the fundamental base:\n\n- Mass conservation implies that electrolyte concentration is non-negative, hence $c_e \\ge 0$.\n- Intercalation capacity of the host solid is finite, implying a maximal admissible solid-phase concentration $c_s \\le c_{s,\\max}$.\n- Reaction thermodynamics and kinetics imply plating risk when the interfacial overpotential $\\eta$ drives the reduction branch beyond intercalation preference. In particular, the sign and magnitude of $\\eta$ in the Butler-Volmer (BV) relation are diagnostic of risk; negative values of $\\eta$ increase the tendency for metal deposition. Use the definition of reaction overpotential $\\eta$ consistent with standard electrochemical conventions as the driving variable for penalizing plating.\n\nYour program must:\n\n- Derive and implement smooth, differentiable penalty terms that approximate indicator functions for violations of the constraints. For a generic smooth hinge, use the classical softplus function defined by $\\mathrm{softplus}(x) = \\ln\\left(1 + e^{x}\\right)$, which approximates $\\max(0,x)$ but remains differentiable for all x.\n- Define the three loss terms as:\n  1. Electrolyte non-negativity penalty:\n     $$L_{e}^{\\ge 0} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(-c_{e,i}\\right)\\right]^2.$$\n  2. Solid concentration upper-bound penalty:\n     $$L_{s}^{\\le c_{s,\\max}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(c_{s,i} - c_{s,\\max}\\right)\\right]^2.$$\n  3. Plating penalty weighted by electrolyte starvation:\n     $$w_i = 1 + \\exp\\left(-\\frac{c_{e,i}}{c_{e,\\mathrm{ref}}}\\right), \\quad L_{\\mathrm{plate}} = \\frac{1}{N} \\sum_{i=1}^{N} w_i \\left[\\mathrm{softplus}\\left(-\\eta_i\\right)\\right]^2.$$\n- Aggregate the losses into a total loss:\n  $$L_{\\mathrm{total}} = \\alpha \\, L_{e}^{\\ge 0} + \\beta \\, L_{s}^{\\le c_{s,\\max}} + \\gamma \\, L_{\\mathrm{plate}},$$\n  with weights $\\alpha = 1$, $\\beta = 1$, $\\gamma = 1$.\n\nAll quantities, including $c_e$, $c_s$, $c_{s,\\max}$, $\\eta$, and $c_{e,\\mathrm{ref}}$, must be treated as real-valued fields or parameters. Electrolyte concentration $c_e$ and solid-phase concentration $c_s$ must be expressed in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$, and overpotential $\\eta$ must be expressed in $\\mathrm{V}$. The final loss values are dimensionless real numbers. Your program must use the specified test suite below and produce the final outputs as dimensionless floats rounded to six decimal places.\n\nTest suite of parameter values (arrays are per-collocation-point predictions; the array dimension is $N$, where $N$ is the length of the array in each case):\n\n- Case $1$ (nominal regime; all constraints satisfied approximately):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[1000, 950, 1100, 1020, 980, 1010]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[24000, 23000, 25000, 24500, 23500, 24200]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[0.02, 0.015, 0.03, 0.01, 0.025, 0.018]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\n- Case $2$ (electrolyte negativity and mild upper-bound violations near the boundary):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[10, -5, 0, 20, -1, 15]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[29000, 30500, 29500, 30000, 31000, 28500]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[0.0, 0.005, -0.005, 0.01, -0.002, 0.0]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\n- Case $3$ (solid-phase capacity violations dominate; overpotential positive):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[1000, 800, 900, 950, 700, 850]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[32000, 31000, 30500, 31500, 32500, 30000]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[0.02, 0.015, 0.01, 0.02, 0.005, 0.012]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\n- Case $4$ (plating regime with strongly negative overpotentials and electrolyte starvation):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[200, 150, 100, 80, 50, 120]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[25000, 26000, 24000, 23000, 22000, 24500]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[-0.05, -0.08, -0.12, -0.03, -0.06, -0.09]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and rounded to six decimal places, where each entry is the total loss $L_{\\mathrm{total}}$ for the corresponding test case, i.e., $[L_{\\mathrm{total}}^{(1)}, L_{\\mathrm{total}}^{(2)}, L_{\\mathrm{total}}^{(3)}, L_{\\mathrm{total}}^{(4)}]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of electrochemical engineering, employs standard numerical techniques for constrained optimization, is well-posed with all necessary information provided, and lacks any contradictions or ambiguities.\n\nThe objective is to compute a total loss, $L_{\\mathrm{total}}$, for a Physics-Informed Neural Network (PINN) applied to a Lithium-Ion Battery (LIB) model. This loss function is designed to enforce physical admissibility of the network's predictions for key state variables. The total loss is an aggregation of three distinct penalty terms, each corresponding to a fundamental physical constraint. The calculations are to be performed for four separate test cases, each representing a different operating regime.\n\nThe core of the methodology lies in using a smooth, differentiable penalty function to approximate an indicator function for constraint violations. This is essential for use in gradient-based optimization algorithms, which are standard for training neural networks. The problem specifies the softplus function, defined as $\\mathrm{softplus}(x) = \\ln(1 + e^x)$. This function smoothly approximates the rectifier function $\\max(0, x)$. By feeding an appropriate argument into the softplus function, we can construct a penalty that is near zero when a constraint is satisfied and grows quadratically with the magnitude of the violation.\n\nThe three loss components are derived as follows:\n\n1.  Electrolyte Non-Negativity Penalty, $L_{e}^{\\ge 0}$: The physical principle of mass conservation dictates that the electrolyte concentration, $c_e$, cannot be negative. The constraint is $c_{e,i} \\ge 0$ for each collocation point $i$. To penalize violations ($c_{e,i} < 0$), we construct the argument for the softplus function as $-c_{e,i}$. If $c_{e,i} \\ge 0$, then $-c_{e,i} \\le 0$, and $\\mathrm{softplus}(-c_{e,i})$ is close to $0$. If $c_{e,i} < 0$, then $-c_{e,i} > 0$, and $\\mathrm{softplus}(-c_{e,i}) \\approx -c_{e,i} = |c_{e,i}|$. The squared penalty, averaged over all $N$ collocation points, is given by:\n    $$L_{e}^{\\ge 0} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(-c_{e,i}\\right)\\right]^2$$\n\n2.  Solid Concentration Upper-Bound Penalty, $L_{s}^{\\le c_{s,\\max}}$: The intercalation host material in an electrode has a finite capacity for lithium ions, imposing an upper limit on the solid-phase concentration, $c_s$. The constraint is $c_{s,i} \\le c_{s,\\max}$. Violations occur when $c_{s,i} > c_{s,\\max}$. The argument for the softplus function is therefore $c_{s,i} - c_{s,\\max}$. If $c_{s,i} \\le c_{s,\\max}$, the argument is non-positive, and the softplus output is near zero. If $c_{s,i} > c_{s,\\max}$, the argument is positive, and $\\mathrm{softplus}(c_{s,i} - c_{s,\\max}) \\approx c_{s,i} - c_{s,\\max}$. The corresponding loss term is:\n    $$L_{s}^{\\le c_{s,\\max}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(c_{s,i} - c_{s,\\max}\\right)\\right]^2$$\n\n3.  Lithium Plating Penalty, $L_{\\mathrm{plate}}$: Lithium plating is an undesirable side reaction that is more likely to occur at strongly negative interfacial overpotentials, $\\eta < 0$. This penalty term is designed to discourage such conditions. The argument for the softplus function is $-\\eta_i$. If $\\eta_i \\ge 0$, the penalty is negligible. If $\\eta_i < 0$, a penalty proportional to its magnitude is incurred. Furthermore, this risk is exacerbated by electrolyte starvation (low $c_e$). A weighting factor, $w_i = 1 + \\exp\\left(-c_{e,i}/c_{e,\\mathrm{ref}}\\right)$, is introduced to amplify the penalty when $c_{e,i}$ is low. As $c_{e,i} \\to 0$, $w_i \\to 2$, and for large $c_{e,i}$, $w_i \\to 1$. The combined plating loss is:\n    $$L_{\\mathrm{plate}} = \\frac{1}{N} \\sum_{i=1}^{N} w_i \\left[\\mathrm{softplus}\\left(-\\eta_i\\right)\\right]^2$$\n\nFinally, the total loss, $L_{\\mathrm{total}}$, is a weighted sum of these three components. The problem specifies the weights as $\\alpha = 1$, $\\beta = 1$, and $\\gamma = 1$. The total loss is therefore a simple sum:\n$$L_{\\mathrm{total}} = L_{e}^{\\ge 0} + L_{s}^{\\le c_{s,\\max}} + L_{\\mathrm{plate}}$$\n\nThe program will implement these four equations. For each test case, it will take the arrays of predicted values for $c_e$, $c_s$, and $\\eta$, along with the scalar parameters $c_{s,\\max}$ and $c_{e,\\mathrm{ref}}$, and compute $L_{\\mathrm{total}}$. The use of vectorized operations on numerical arrays is the most efficient approach to compute the sums over the collocation points. The final result for each case will be rounded to six decimal places as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes total physics-informed loss for lithium-ion battery PINN predictions.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"c_e\": np.array([1000, 950, 1100, 1020, 980, 1010]),\n            \"c_s\": np.array([24000, 23000, 25000, 24500, 23500, 24200]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([0.02, 0.015, 0.03, 0.01, 0.025, 0.018]),\n            \"c_e_ref\": 1000,\n        },\n        {\n            \"c_e\": np.array([10, -5, 0, 20, -1, 15]),\n            \"c_s\": np.array([29000, 30500, 29500, 30000, 31000, 28500]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([0.0, 0.005, -0.005, 0.01, -0.002, 0.0]),\n            \"c_e_ref\": 1000,\n        },\n        {\n            \"c_e\": np.array([1000, 800, 900, 950, 700, 850]),\n            \"c_s\": np.array([32000, 31000, 30500, 31500, 32500, 30000]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([0.02, 0.015, 0.01, 0.02, 0.005, 0.012]),\n            \"c_e_ref\": 1000,\n        },\n        {\n            \"c_e\": np.array([200, 150, 100, 80, 50, 120]),\n            \"c_s\": np.array([25000, 26000, 24000, 23000, 22000, 24500]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([-0.05, -0.08, -0.12, -0.03, -0.06, -0.09]),\n            \"c_e_ref\": 1000,\n        },\n    ]\n\n    # Loss weights, as specified in the problem\n    alpha = 1.0\n    beta = 1.0\n    gamma = 1.0\n\n    def softplus(x):\n        \"\"\"\n        Computes the softplus function log(1 + exp(x)).\n        This implementation is numerically stable.\n        \"\"\"\n        # For large x, softplus(x) approx x.\n        # For large negative x, softplus(x) approx 0.\n        return np.log1p(np.exp(-np.abs(x))) + np.maximum(0, x)\n\n    results = []\n    for case in test_cases:\n        c_e = case[\"c_e\"]\n        c_s = case[\"c_s\"]\n        eta = case[\"eta\"]\n        c_s_max = case[\"c_s_max\"]\n        c_e_ref = case[\"c_e_ref\"]\n        \n        # 1. Electrolyte non-negativity penalty\n        l_e = np.mean(softplus(-c_e) ** 2)\n        \n        # 2. Solid concentration upper-bound penalty\n        l_s = np.mean(softplus(c_s - c_s_max) ** 2)\n\n        # 3. Plating penalty\n        w = 1.0 + np.exp(-c_e / c_e_ref)\n        l_plate = np.mean(w * (softplus(-eta) ** 2))\n        \n        # 4. Total Loss\n        l_total = alpha * l_e + beta * l_s + gamma * l_plate\n        \n        results.append(round(l_total, 6))\n\n    # Format the final output string\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A well-known challenge in training PINNs is the \"spectral bias\" of standard neural networks, which makes them inherently better at learning low-frequency functions and struggle with sharp gradients found at electrochemical interfaces or reaction fronts. This exercise explores a powerful technique to mitigate this issue: Fourier feature embeddings. By transforming the input coordinates into a basis of sinusoids, you will see how the network's ability to represent high-frequency details is dramatically improved, leading to a more accurate approximation of the underlying physics .",
            "id": "3940605",
            "problem": "Consider a one-dimensional electrolyte domain along the thickness coordinate, nondimensionalized as $\\xi \\in [0,1]$. Let the electrolyte potential be denoted by $\\phi_e(\\xi)$ and the electrolyte ionic conductivity be taken as a constant equal to $\\kappa_e = 1$ in nondimensional units. By charge conservation and Ohm's law, in one spatial dimension the governing equation reduces to the second-order linear ordinary differential equation\n$$\\frac{d^2 \\phi_e}{d \\xi^2} = s(\\xi),$$\nsubject to Dirichlet boundary conditions at $\\xi = 0$ and $\\xi = 1$. To generate a physically consistent source term $s(\\xi)$ that produces sharp gradients, define a ground-truth potential with a localized transition layer,\n$$\\phi_e^{\\mathrm{true}}(\\xi) = V_0 + (V_1 - V_0)\\,\\xi + A\\,\\tanh\\left(\\frac{\\xi - \\xi_0}{\\varepsilon}\\right),$$\nwhere $V_0$ and $V_1$ are boundary potentials, $A$ is the amplitude of the localized feature, $\\xi_0$ is the center of the transition, and $\\varepsilon$ controls the width of the transition layer. The source term is then set to\n$$s(\\xi) = \\frac{d^2 \\phi_e^{\\mathrm{true}}}{d \\xi^2}(\\xi),$$\nensuring that $\\phi_e^{\\mathrm{true}}(\\xi)$ is the exact solution of the boundary value problem with boundary conditions $\\phi_e(0) = \\phi_e^{\\mathrm{true}}(0)$ and $\\phi_e(1) = \\phi_e^{\\mathrm{true}}(1)$.\n\nYou must approximate $\\phi_e(\\xi)$ using two physics-informed models and compare their accuracy:\n- A baseline polynomial embedding $\\gamma_P(\\xi) = [\\xi^0, \\xi^1, \\xi^2, \\ldots, \\xi^P]$, where $P$ is a positive integer. The model is $\\hat{\\phi}_P(\\xi) = \\mathbf{w}_P^\\top \\gamma_P(\\xi)$.\n- A Fourier feature embedding designed to capture sharp gradients, $\\gamma_F(\\xi) = [1, \\xi, \\sin(2\\pi \\omega_1 \\xi), \\cos(2\\pi \\omega_1 \\xi), \\ldots, \\sin(2\\pi \\omega_K \\xi), \\cos(2\\pi \\omega_K \\xi)]$, with $\\omega_k = k\\,\\omega_0$ for $k \\in \\{1,2,\\ldots,K\\}$ and base frequency $\\omega_0 > 0$. The model is $\\hat{\\phi}_F(\\xi) = \\mathbf{w}_F^\\top \\gamma_F(\\xi)$.\n\nBoth models must be trained by minimizing a physics-informed objective that penalizes the governing equation residual at interior collocation points and the boundary mismatch at $\\xi = 0$ and $\\xi = 1$, together with an $\\ell_2$ regularization on the weights. Let the collocation set be $\\{\\xi_i\\}_{i=1}^{N_c}$ uniformly spaced in $(0,1)$, and let the boundary weights and residual weights be $w_b > 0$ and $w_r > 0$, respectively. The training problem for a generic embedding $\\gamma(\\xi)$ is the ridge-regularized least-squares problem over the weight vector $\\mathbf{w}$:\n$$\\min_{\\mathbf{w}} \\left( \\sum_{i=1}^{N_c} w_r \\left( \\frac{d^2}{d \\xi^2} \\big( \\mathbf{w}^\\top \\gamma(\\xi_i) \\big) - s(\\xi_i) \\right)^2 + w_b \\left( \\mathbf{w}^\\top \\gamma(0) - \\phi_e^{\\mathrm{true}}(0) \\right)^2 + w_b \\left( \\mathbf{w}^\\top \\gamma(1) - \\phi_e^{\\mathrm{true}}(1) \\right)^2 + \\lambda \\|\\mathbf{w}\\|_2^2 \\right),$$\nwhere $\\lambda > 0$ is the regularization parameter. Note that for the Fourier embedding, the second derivative with respect to $\\xi$ of the $\\sin$ and $\\cos$ components is $-\\left(2\\pi \\omega_k\\right)^2$ times the original $\\sin$ or $\\cos$ component, while the second derivative of the constant and linear terms is zero; for the polynomial embedding, the second derivative of $\\xi^n$ is $n(n-1)\\,\\xi^{n-2}$ for $n \\geq 2$ and zero otherwise.\n\nDefine the relative $\\ell_2$ approximation error over a uniform evaluation grid $\\{\\xi_j\\}_{j=1}^{N_{\\mathrm{eval}}}$ as\n$$\\mathcal{E}(\\hat{\\phi}) = \\sqrt{\\frac{\\sum_{j=1}^{N_{\\mathrm{eval}}} \\left( \\hat{\\phi}(\\xi_j) - \\phi_e^{\\mathrm{true}}(\\xi_j) \\right)^2}{\\sum_{j=1}^{N_{\\mathrm{eval}}} \\left( \\phi_e^{\\mathrm{true}}(\\xi_j) \\right)^2}},$$\nwhich is dimensionless. For each test case, compute the improvement factor\n$$I = \\frac{\\mathcal{E}(\\hat{\\phi}_P)}{\\mathcal{E}(\\hat{\\phi}_F)},$$\nso that $I > 1$ indicates that the Fourier feature embedding improves accuracy relative to the polynomial baseline.\n\nImplement a complete, runnable program that constructs both embeddings, trains the corresponding physics-informed models by solving the ridge-regularized normal equations, evaluates the relative $\\ell_2$ errors, and reports the improvement factor for each test case. Use the following test suite, which spans a typical case, sharp-gradient cases, and an edge case where Fourier features are insufficient:\n- Test case $1$: $A = 0.2$, $\\varepsilon = 0.03$, $\\xi_0 = 0.5$, $V_0 = 0.0$, $V_1 = 0.1$, $K = 20$, $\\omega_0 = 3.0$, $P = 7$, $N_c = 200$, $N_{\\mathrm{eval}} = 1000$, $w_r = 1.0$, $w_b = 1000.0$, $\\lambda = 10^{-6}$.\n- Test case $2$: $A = 0.2$, $\\varepsilon = 0.005$, $\\xi_0 = 0.5$, $V_0 = 0.0$, $V_1 = 0.1$, $K = 60$, $\\omega_0 = 5.0$, $P = 11$, $N_c = 400$, $N_{\\mathrm{eval}} = 1200$, $w_r = 1.0$, $w_b = 1000.0$, $\\lambda = 10^{-6}$.\n- Test case $3$: $A = 0.2$, $\\varepsilon = 0.10$, $\\xi_0 = 0.5$, $V_0 = 0.0$, $V_1 = 0.1$, $K = 8$, $\\omega_0 = 2.0$, $P = 5$, $N_c = 100$, $N_{\\mathrm{eval}} = 800$, $w_r = 1.0$, $w_b = 500.0$, $\\lambda = 10^{-6}$.\n- Test case $4$ (edge case with insufficient Fourier features): $A = 0.2$, $\\varepsilon = 0.02$, $\\xi_0 = 0.5$, $V_0 = 0.0$, $V_1 = 0.1$, $K = 3$, $\\omega_0 = 1.0$, $P = 9$, $N_c = 200$, $N_{\\mathrm{eval}} = 1000$, $w_r = 1.0$, $w_b = 1000.0$, $\\lambda = 10^{-6}$.\n\nYour program should produce a single line of output containing the improvement factors for the four test cases as a comma-separated list enclosed in square brackets (for example, $[\\mathrm{I}_1,\\mathrm{I}_2,\\mathrm{I}_3,\\mathrm{I}_4]$). The improvement factors are dimensionless floats. No other output should be produced.",
            "solution": "The problem requires the approximation of the solution to a one-dimensional second-order linear ordinary differential equation (ODE) using two different physics-informed linear models. The accuracy of these models, one based on a polynomial embedding and the other on a Fourier feature embedding, is to be compared. The problem is well-posed and provides a complete specification for its solution. We will proceed by first formulating the training task as a standard ridge-regularized linear least-squares problem, then constructing the necessary matrices for each model, solving for the optimal model weights, and finally evaluating the approximation error.\n\nThe governing ODE for the electrolyte potential $\\phi_e(\\xi)$ is given by:\n$$\n\\frac{d^2 \\phi_e}{d \\xi^2} = s(\\xi), \\quad \\xi \\in [0, 1]\n$$\nwith Dirichlet boundary conditions at $\\xi=0$ and $\\xi=1$. The source term $s(\\xi)$ is manufactured from a known ground-truth solution $\\phi_e^{\\mathrm{true}}(\\xi)$ to facilitate error analysis:\n$$\n\\phi_e^{\\mathrm{true}}(\\xi) = V_0 + (V_1 - V_0)\\,\\xi + A\\,\\tanh\\left(\\frac{\\xi - \\xi_0}{\\varepsilon}\\right)\n$$\nThe source term is the second derivative of this function:\n$$\ns(\\xi) = \\frac{d^2 \\phi_e^{\\mathrm{true}}}{d \\xi^2}(\\xi) = -\\frac{2A}{\\varepsilon^2} \\mathrm{sech}^2\\left(\\frac{\\xi - \\xi_0}{\\varepsilon}\\right) \\tanh\\left(\\frac{\\xi - \\xi_0}{\\varepsilon}\\right)\n$$\nThe boundary conditions are set to match the ground truth: $\\phi_e(0) = \\phi_e^{\\mathrm{true}}(0)$ and $\\phi_e(1) = \\phi_e^{\\mathrm{true}}(1)$.\n\nA generic linear model approximates the potential as a weighted sum of basis functions $\\gamma_m(\\xi)$:\n$$\n\\hat{\\phi}(\\xi) = \\mathbf{w}^\\top \\gamma(\\xi) = \\sum_{m=1}^{M} w_m \\gamma_m(\\xi)\n$$\nwhere $\\gamma(\\xi)$ is a column vector of $M$ basis functions and $\\mathbf{w}$ is the vector of corresponding weights. The weights are determined by minimizing a composite objective function that includes the PDE residual, boundary condition mismatch, and $\\ell_2$ regularization:\n$$\nL(\\mathbf{w}) = w_r \\sum_{i=1}^{N_c} \\left( \\frac{d^2 \\hat{\\phi}}{d \\xi^2}(\\xi_i) - s(\\xi_i) \\right)^2 + w_b \\left( \\hat{\\phi}(0) - \\phi_e^{\\mathrm{true}}(0) \\right)^2 + w_b \\left( \\hat{\\phi}(1) - \\phi_e^{\\mathrm{true}}(1) \\right)^2 + \\lambda \\|\\mathbf{w}\\|_2^2\n$$\nThis is a standard ridge regression problem, which can be expressed in the matrix form $\\min_{\\mathbf{w}} \\|\\mathbf{A}\\mathbf{w} - \\mathbf{b}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2$. The optimal weight vector $\\mathbf{w}$ is found by solving the normal equations:\n$$\n(\\mathbf{A}^\\top \\mathbf{A} + \\lambda \\mathbf{I}) \\mathbf{w} = \\mathbf{A}^\\top \\mathbf{b}\n$$\nwhere $\\mathbf{I}$ is the identity matrix of size $M \\times M$.\n\nThe system matrix $\\mathbf{A}$ and target vector $\\mathbf{b}$ are constructed by assembling the contributions from the residual and boundary terms. Let $\\gamma''(\\xi)$ be the vector of second derivatives of the basis functions. The matrix $\\mathbf{A}$ of size $(N_c + 2) \\times M$ and vector $\\mathbf{b}$ of size $(N_c + 2) \\times 1$ are:\n$$\n\\mathbf{A} = \\begin{pmatrix}\n\\sqrt{w_r} (\\gamma''(\\xi_1))^\\top \\\\\n\\vdots \\\\\n\\sqrt{w_r} (\\gamma''(\\xi_{N_c}))^\\top \\\\\n\\sqrt{w_b} (\\gamma(0))^\\top \\\\\n\\sqrt{w_b} (\\gamma(1))^\\top\n\\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix}\n\\sqrt{w_r} s(\\xi_1) \\\\\n\\vdots \\\\\n\\sqrt{w_r} s(\\xi_{N_c}) \\\\\n\\sqrt{w_b} \\phi_e^{\\mathrm{true}}(0) \\\\\n\\sqrt{w_b} \\phi_e^{\\mathrm{true}}(1)\n\\end{pmatrix}\n$$\nWe now specify the construction for the two models.\n\n**Model 1: Polynomial Embedding**\nThe basis functions are monomials $\\gamma_{P,n}(\\xi) = \\xi^{n-1}$ for $n=1, \\ldots, P+1$. The size of the basis is $M_P = P+1$.\nThe vector of basis functions is $\\gamma_P(\\xi) = [\\xi^0, \\xi^1, \\dots, \\xi^P]^\\top$.\nThe vector of second derivatives is $\\gamma_P''(\\xi) = [0, 0, 2\\xi^0, 6\\xi^1, \\dots, P(P-1)\\xi^{P-2}]^\\top$.\nThese expressions are used to construct the system matrix $\\mathbf{A}_P$ and solve for the weights $\\mathbf{w}_P$.\n\n**Model 2: Fourier Feature Embedding**\nThe basis consists of a constant, a linear term, and $K$ pairs of sine and cosine functions. The size of the basis is $M_F = 2 + 2K$. The vector of basis functions is $\\gamma_F(\\xi) = [1, \\xi, \\sin(2\\pi \\omega_1 \\xi), \\cos(2\\pi \\omega_1 \\xi), \\ldots, \\sin(2\\pi \\omega_K \\xi), \\cos(2\\pi \\omega_K \\xi)]^\\top$, where $\\omega_k = k \\omega_0$.\nThe corresponding second derivatives are zero for the constant and linear terms, and for the trigonometric terms are:\n$\\frac{d^2}{d\\xi^2}\\sin(2\\pi \\omega_k \\xi) = -(2\\pi \\omega_k)^2 \\sin(2\\pi \\omega_k \\xi)$\n$\\frac{d^2}{d\\xi^2}\\cos(2\\pi \\omega_k \\xi) = -(2\\pi \\omega_k)^2 \\cos(2\\pi \\omega_k \\xi)$\nThese relations are used to construct the system matrix $\\mathbf{A}_F$ and solve for weights $\\mathbf{w}_F$.\n\nAfter finding the optimal weights $\\mathbf{w}_P$ and $\\mathbf{w}_F$ for each test case, we predict the potentials $\\hat{\\phi}_P(\\xi)$ and $\\hat{\\phi}_F(\\xi)$ on a fine evaluation grid $\\{\\xi_j\\}_{j=1}^{N_{\\mathrm{eval}}}$. The relative $\\ell_2$ approximation error is calculated for each model:\n$$\n\\mathcal{E}(\\hat{\\phi}) = \\sqrt{\\frac{\\sum_{j=1}^{N_{\\mathrm{eval}}} \\left( \\hat{\\phi}(\\xi_j) - \\phi_e^{\\mathrm{true}}(\\xi_j) \\right)^2}{\\sum_{j=1}^{N_{\\mathrm{eval}}} \\left( \\phi_e^{\\mathrm{true}}(\\xi_j) \\right)^2}}\n$$\nThe final metric is the improvement factor $I = \\mathcal{E}(\\hat{\\phi}_P) / \\mathcal{E}(\\hat{\\phi}_F)$, which quantifies the performance gain of the Fourier feature model over the polynomial baseline. A value of $I > 1$ signifies an improvement.\n\nThe implementation will loop through each test case, construct and solve the linear systems for both polynomial and Fourier models, compute their respective errors, and report the resulting improvement factor.",
            "answer": "```python\n# The final answer must be a single, complete, standalone program.\n# Execution Environment: Python 3.12, numpy 1.23.5\n# No other libraries outside the Python standard library are permitted.\n\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the physics-informed regression problem for polynomial and Fourier embeddings\n    and computes the improvement factor for each test case.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Typical sharp gradient\n        {'A': 0.2, 'eps': 0.03, 'xi0': 0.5, 'V0': 0.0, 'V1': 0.1, 'K': 20, 'omega0': 3.0, 'P': 7,\n         'Nc': 200, 'N_eval': 1000, 'wr': 1.0, 'wb': 1000.0, 'lambda_reg': 1e-6},\n        # Case 2: Very sharp gradient, more features\n        {'A': 0.2, 'eps': 0.005, 'xi0': 0.5, 'V0': 0.0, 'V1': 0.1, 'K': 60, 'omega0': 5.0, 'P': 11,\n         'Nc': 400, 'N_eval': 1200, 'wr': 1.0, 'wb': 1000.0, 'lambda_reg': 1e-6},\n        # Case 3: Smoother gradient\n        {'A': 0.2, 'eps': 0.10, 'xi0': 0.5, 'V0': 0.0, 'V1': 0.1, 'K': 8, 'omega0': 2.0, 'P': 5,\n         'Nc': 100, 'N_eval': 800, 'wr': 1.0, 'wb': 500.0, 'lambda_reg': 1e-6},\n        # Case 4: Edge case with insufficient Fourier features\n        {'A': 0.2, 'eps': 0.02, 'xi0': 0.5, 'V0': 0.0, 'V1': 0.1, 'K': 3, 'omega0': 1.0, 'P': 9,\n         'Nc': 200, 'N_eval': 1000, 'wr': 1.0, 'wb': 1000.0, 'lambda_reg': 1e-6},\n    ]\n\n    improvement_factors = []\n\n    for case in test_cases:\n        A, eps, xi0, V0, V1 = case['A'], case['eps'], case['xi0'], case['V0'], case['V1']\n        K, omega0, P = case['K'], case['omega0'], case['P']\n        Nc, N_eval = case['Nc'], case['N_eval']\n        wr, wb, lambda_reg = case['wr'], case['wb'], case['lambda_reg']\n\n        xi_c = np.linspace(0, 1, Nc + 2)[1:-1]\n        xi_eval = np.linspace(0, 1, N_eval)\n\n        def phi_true(xi):\n            return V0 + (V1 - V0) * xi + A * np.tanh((xi - xi0) / eps)\n\n        def source(xi):\n            u = (xi - xi0) / eps\n            sech_u = 1.0 / np.cosh(u)\n            return -2.0 * A / (eps**2) * (sech_u**2) * np.tanh(u)\n\n        phi_true_eval = phi_true(xi_eval)\n        phi_true_0 = phi_true(0.0)\n        phi_true_1 = phi_true(1.0)\n        s_c = source(xi_c)\n\n        def get_model_error(basis_type):\n            if basis_type == 'poly':\n                M = P + 1\n                n = np.arange(M)\n                gamma_eval = np.power.outer(xi_eval, n)\n                coeffs = n * (n - 1)\n                gamma_ddot_c = coeffs[None, :] * np.power.outer(xi_c, n - 2)\n                gamma_ddot_c[:, :2] = 0.0\n                gamma_0 = np.power.outer(np.array([0.0]), n).flatten()\n                gamma_1 = np.power.outer(np.array([1.0]), n).flatten()\n\n            elif basis_type == 'fourier':\n                M = 2 + 2 * K\n                omegas = omega0 * np.arange(1, K + 1)\n\n                def build_gamma(xi_grid):\n                    N_grid = len(xi_grid)\n                    gamma = np.zeros((N_grid, M))\n                    gamma[:, 0] = 1.0\n                    gamma[:, 1] = xi_grid\n                    if K > 0:\n                        arg_matrix = 2 * np.pi * xi_grid[:, None] * omegas[None, :]\n                        gamma[:, 2::2] = np.sin(arg_matrix)\n                        gamma[:, 3::2] = np.cos(arg_matrix)\n                    return gamma\n\n                def build_gamma_ddot(xi_grid):\n                    N_grid = len(xi_grid)\n                    gamma_ddot = np.zeros((N_grid, M))\n                    if K > 0:\n                        arg_matrix = 2 * np.pi * xi_grid[:, None] * omegas[None, :]\n                        factors = -(2 * np.pi * omegas)**2\n                        gamma_ddot[:, 2::2] = factors * np.sin(arg_matrix)\n                        gamma_ddot[:, 3::2] = factors * np.cos(arg_matrix)\n                    return gamma_ddot\n                \n                gamma_eval = build_gamma(xi_eval)\n                gamma_ddot_c = build_gamma_ddot(xi_c)\n                gamma_0 = build_gamma(np.array([0.0])).flatten()\n                gamma_1 = build_gamma(np.array([1.0])).flatten()\n            else:\n                raise ValueError(\"Unknown basis type\")\n\n            sqrt_wr, sqrt_wb = np.sqrt(wr), np.sqrt(wb)\n            A_res = sqrt_wr * gamma_ddot_c\n            A_b0 = sqrt_wb * gamma_0\n            A_b1 = sqrt_wb * gamma_1\n            A_mat = np.vstack([A_res, A_b0.reshape(1, -1), A_b1.reshape(1, -1)])\n            \n            b_res = sqrt_wr * s_c\n            b_b0 = sqrt_wb * phi_true_0\n            b_b1 = sqrt_wb * phi_true_1\n            b_vec = np.concatenate([b_res, [b_b0], [b_b1]])\n\n            lhs = A_mat.T @ A_mat + lambda_reg * np.identity(M)\n            rhs = A_mat.T @ b_vec\n            w = np.linalg.solve(lhs, rhs)\n\n            phi_hat_eval = gamma_eval @ w\n            numerator = np.sum((phi_hat_eval - phi_true_eval)**2)\n            denominator = np.sum(phi_true_eval**2)\n            \n            if denominator == 0:\n                return 0.0 if numerator == 0.0 else np.inf\n            return np.sqrt(numerator / denominator)\n\n        error_poly = get_model_error('poly')\n        error_fourier = get_model_error('fourier')\n        \n        if error_fourier > 0:\n            improvement = error_poly / error_fourier\n        else:\n            improvement = np.inf if error_poly > 1e-12 else 1.0\n        \n        improvement_factors.append(improvement)\n\n    print(f\"[{','.join(f'{f:.8f}' for f in improvement_factors)}]\")\n\n# Self-contained execution\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}