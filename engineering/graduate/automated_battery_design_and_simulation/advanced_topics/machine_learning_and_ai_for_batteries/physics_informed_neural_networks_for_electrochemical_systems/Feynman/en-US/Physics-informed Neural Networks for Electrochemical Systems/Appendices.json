{
    "hands_on_practices": [
        {
            "introduction": "The heart of a Physics-Informed Neural Network (PINN) is its composite loss function, which translates the system's governing equations and boundary conditions into a single, optimizable objective. This initial practice focuses on the fundamental task of constructing such a loss function for a core electrochemical process: one-dimensional solid-state diffusion with a highly non-linear Butler-Volmer flux boundary condition. By implementing the analytical derivatives and assembling the residuals for the PDE, boundary conditions, and initial state, you will gain a foundational, code-level understanding of how physical laws are encoded into a neural network framework .",
            "id": "3955483",
            "problem": "You are tasked with designing a Physics-Informed Neural Network (PINN) for a one-dimensional solid-state diffusion subsystem of a battery Digital Twin (DT). The PINN must encode the diffusion Partial Differential Equation (PDE), boundary conditions, and a data assimilation term to compare against sparse sensor-like measurements. Your final output must be a complete, runnable program that computes the composite loss for three scientifically plausible test cases, each representing distinct electrochemical regimes. The program must produce a single line containing a comma-separated list enclosed in square brackets with one floating-point loss value per test case.\n\nThe physical subsystem is one-dimensional solid diffusion in a thin active-material slab with spatial coordinate $x \\in [0,L]$ and time $t \\in [0,T_{\\mathrm{end}}]$. The governing law is Fick’s second law, and the boundary flux at the external surface $x=L$ is constrained by Butler–Volmer (BV) kinetics with a Nernst equilibrium potential. The PINN approximates the lithium concentration field $\\hat{c}(x,t;\\theta)$.\n\nFundamental base and core definitions:\n- Fick’s second law: $\\dfrac{\\partial c}{\\partial t} = D \\dfrac{\\partial^2 c}{\\partial x^2}$, where $D$ is the solid diffusion coefficient.\n- Symmetry (no-flux) boundary at $x=0$: $\\left.\\dfrac{\\partial c}{\\partial x}\\right|_{x=0} = 0$.\n- Flux boundary at $x=L$: $-D \\left.\\dfrac{\\partial c}{\\partial x}\\right|_{x=L} = \\dfrac{j_{\\mathrm{BV}}}{n F}$, where $n$ is the number of electrons per reaction (here $n=1$), $F$ is Faraday’s constant, and $j_{\\mathrm{BV}}$ is the Butler–Volmer current density.\n- Butler–Volmer current density: $j_{\\mathrm{BV}}(c_s) = i_0(c_s)\\left[\\exp\\!\\left(\\dfrac{\\alpha_a F \\eta}{R T}\\right) - \\exp\\!\\left(-\\dfrac{\\alpha_c F \\eta}{R T}\\right)\\right]$, where $\\eta = E_{\\mathrm{app}} - U(c_s)$ is the overpotential, $E_{\\mathrm{app}}$ is applied potential, $\\alpha_a$ and $\\alpha_c$ are anodic and cathodic charge-transfer coefficients, $R$ is the gas constant, $T$ is temperature, and $i_0(c_s)$ is the exchange current density.\n- Nernst equilibrium potential: $U(c) = U_{\\mathrm{ref}} + \\dfrac{R T}{F}\\ln\\!\\left(\\dfrac{c}{c_{\\max}-c}\\right)$, where $c_{\\max}$ is the maximum concentration.\n- Exchange current density model: $i_0(c) = k_0 \\sqrt{c(c_{\\max}-c)}$, with kinetic prefactor $k_0$.\n- Initial condition: $c(x,0) = c_0$ for all $x$.\n\nTo ensure numerical robustness in the Butler–Volmer evaluation, use a regularized concentration at the boundary, $\\tilde{c}_s = \\max\\!\\left(\\epsilon,\\min\\!\\left(c_s,c_{\\max}-\\epsilon\\right)\\right)$, with small $\\epsilon>0$.\n\nPINN architecture and analytical derivatives:\n- Use a single-hidden-layer feedforward network with $m$ neurons and hyperbolic tangent activation. Let the input be $(x,t)$, with weight matrix $W_1 \\in \\mathbb{R}^{m\\times 2}$, bias $b_1 \\in \\mathbb{R}^m$, output weights $W_2 \\in \\mathbb{R}^m$, and output bias $b_2 \\in \\mathbb{R}$.\n- The network output is\n$$\n\\hat{c}(x,t;\\theta) = b_2 + \\sum_{k=1}^{m} W_{2,k}\\,\\tanh\\!\\big(z_k(x,t)\\big), \\quad z_k(x,t) = b_{1,k} + W_{1,k,1} x + W_{1,k,2} t.\n$$\n- Let $\\operatorname{sech}(z) = 1/\\cosh(z)$ and note $\\dfrac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ and $\\dfrac{d^2}{dz^2}\\tanh(z) = -2\\,\\tanh(z)\\,\\operatorname{sech}^2(z)$.\n- The required derivatives are\n$$\n\\frac{\\partial \\hat{c}}{\\partial x} = \\sum_{k=1}^m W_{2,k}\\,\\operatorname{sech}^2(z_k)\\,W_{1,k,1}, \\quad\n\\frac{\\partial \\hat{c}}{\\partial t} = \\sum_{k=1}^m W_{2,k}\\,\\operatorname{sech}^2(z_k)\\,W_{1,k,2},\n$$\n$$\n\\frac{\\partial^2 \\hat{c}}{\\partial x^2} = \\sum_{k=1}^m W_{2,k}\\,\\big(-2\\,\\tanh(z_k)\\,\\operatorname{sech}^2(z_k)\\big)\\,W_{1,k,1}^2.\n$$\n\nComposite loss design:\n- Define residuals at interior collocation points $(x_i,t_i)$: $r_{\\mathrm{pde}}(x_i,t_i) = \\dfrac{\\partial \\hat{c}}{\\partial t}(x_i,t_i) - D\\,\\dfrac{\\partial^2 \\hat{c}}{\\partial x^2}(x_i,t_i)$.\n- Define symmetry boundary residuals at times $t_j$: $r_{0}(t_j) = \\dfrac{\\partial \\hat{c}}{\\partial x}(0,t_j)$.\n- Define flux boundary residuals at times $t_j$ using BV: $r_{L}(t_j) = -D\\,\\dfrac{\\partial \\hat{c}}{\\partial x}(L,t_j) - \\dfrac{j_{\\mathrm{BV}}(\\tilde{c}_s(L,t_j))}{n F}$.\n- Define initial condition residuals at positions $x_\\ell$: $r_{\\mathrm{ic}}(x_\\ell) = \\hat{c}(x_\\ell,0) - c_0$.\n- Define data mismatch residuals at given data points $(x_d,t_d)$ with target $c_{\\mathrm{data}}$: $r_{\\mathrm{data}}(x_d,t_d) = \\hat{c}(x_d,t_d) - c_{\\mathrm{data}}$.\n\n- The composite loss is\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}}\\,\\frac{1}{N_{\\mathrm{pde}}}\\sum_i r_{\\mathrm{pde}}(x_i,t_i)^2\n+ \\lambda_{0}\\,\\frac{1}{N_{0}}\\sum_j r_{0}(t_j)^2\n+ \\lambda_{L}\\,\\frac{1}{N_{L}}\\sum_j r_{L}(t_j)^2\n+ \\lambda_{\\mathrm{ic}}\\,\\frac{1}{N_{\\mathrm{ic}}}\\sum_\\ell r_{\\mathrm{ic}}(x_\\ell)^2\n+ \\lambda_{\\mathrm{data}}\\,\\frac{1}{N_{\\mathrm{data}}}\\sum_{d} r_{\\mathrm{data}}(x_d,t_d)^2,\n$$\nwith nonnegative weights $\\lambda_{\\cdot}$.\n\nScientific realism:\n- Use physically plausible ranges: $D$ on the order of $10^{-14}\\,\\mathrm{m^2/s}$, $L$ on the order of $10^{-6}\\,\\mathrm{m}$, $T$ near room temperature, exchange kinetics that yield reasonable $j_{\\mathrm{BV}}$ magnitudes, and concentration ranges strictly within $(0,c_{\\max})$. Use $n=1$.\n\nProgram requirements:\n- Implement the PINN and compute the composite loss for each test case using only analytical derivatives as defined above.\n- Do not train the network; simply evaluate the loss for the predefined parameter sets.\n- The final output must be a single line of the form $[\\ell_1,\\ell_2,\\ell_3]$, where $\\ell_k$ are floating-point losses for each test case. The loss values are dimensionless.\n\nUnits and angle specification:\n- Express all physical quantities internally in SI units: $x$ in $\\mathrm{m}$, $t$ in $\\mathrm{s}$, $c$ in $\\mathrm{mol/m^3}$, potentials in $\\mathrm{V}$, temperature in $\\mathrm{K}$, and current density in $\\mathrm{A/m^2}$. No angles are involved. The output loss values are unitless floats.\n\nTest suite:\n- Use three test cases with the following parameters and evaluation grids.\n\nCase $\\mathbf{1}$ (baseline, moderate flux):\n- $m = 3$,\n- $L = 1.0\\times 10^{-6}\\,\\mathrm{m}$,\n- $D = 1.0\\times 10^{-14}\\,\\mathrm{m^2/s}$,\n- $T = 298.0\\,\\mathrm{K}$,\n- $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$,\n- $F = 96485.0\\,\\mathrm{C/mol}$,\n- $\\alpha_a = 0.5$, $\\alpha_c = 0.5$,\n- $U_{\\mathrm{ref}} = 4.0\\,\\mathrm{V}$,\n- $c_{\\max} = 25000.0\\,\\mathrm{mol/m^3}$,\n- $c_0 = 15000.0\\,\\mathrm{mol/m^3}$,\n- $k_0 = 5.0\\,\\mathrm{A/m^2}$,\n- $E_{\\mathrm{app}} = 4.2\\,\\mathrm{V}$,\n- $\\epsilon = 1.0\\,\\mathrm{mol/m^3}$,\n- Weights: $W_1 = \\begin{bmatrix} 1.0\\times 10^{6} & -1.0 \\\\ -1.0\\times 10^{6} & 2.0 \\\\ 5.0\\times 10^{5} & -0.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}$, $W_2 = \\begin{bmatrix} 100.0 \\\\ -50.0 \\\\ 25.0 \\end{bmatrix}$, $b_2 = 15000.0$,\n- Interior collocation: $x \\in \\{L/4,\\,L/2,\\,3L/4\\} = \\{2.5\\times 10^{-7},\\,5.0\\times 10^{-7},\\,7.5\\times 10^{-7}\\}\\,\\mathrm{m}$, $t \\in \\{2.0,\\,5.0,\\,8.0\\}\\,\\mathrm{s}$,\n- Boundary times: $t \\in \\{1.0,\\,4.0,\\,7.0\\}\\,\\mathrm{s}$,\n- Initial positions: $x \\in \\{0.0,\\,L/3,\\,2L/3,\\,L\\} = \\{0.0,\\,3.\\overline{3}\\times 10^{-7},\\,6.\\overline{6}\\times 10^{-7},\\,1.0\\times 10^{-6}\\}\\,\\mathrm{m}$,\n- Data points: $(x,t,c_{\\mathrm{data}}) \\in \\{(0.75L,\\,5.0,\\,15050.0),\\,(0.25L,\\,5.0,\\,14980.0)\\}$,\n- Loss weights: $\\lambda_{\\mathrm{pde}} = 1.0$, $\\lambda_{0} = 10.0$, $\\lambda_{L} = 10.0$, $\\lambda_{\\mathrm{ic}} = 5.0$, $\\lambda_{\\mathrm{data}} = 2.0$.\n\nCase $\\mathbf{2}$ (strong kinetics and higher overpotential):\n- $m = 3$,\n- $L = 1.0\\times 10^{-6}\\,\\mathrm{m}$,\n- $D = 5.0\\times 10^{-15}\\,\\mathrm{m^2/s}$,\n- $T = 298.0\\,\\mathrm{K}$,\n- $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$,\n- $F = 96485.0\\,\\mathrm{C/mol}$,\n- $\\alpha_a = 0.5$, $\\alpha_c = 0.5$,\n- $U_{\\mathrm{ref}} = 4.0\\,\\mathrm{V}$,\n- $c_{\\max} = 25000.0\\,\\mathrm{mol/m^3}$,\n- $c_0 = 15000.0\\,\\mathrm{mol/m^3}$,\n- $k_0 = 8.0\\,\\mathrm{A/m^2}$,\n- $E_{\\mathrm{app}} = 4.5\\,\\mathrm{V}$,\n- $\\epsilon = 1.0\\,\\mathrm{mol/m^3}$,\n- Weights: $W_1 = \\begin{bmatrix} 0.8\\times 10^{6} & -0.8 \\\\ -0.8\\times 10^{6} & 1.5 \\\\ 0.4\\times 10^{6} & -0.3 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\\\ 0.1 \\end{bmatrix}$, $W_2 = \\begin{bmatrix} 120.0 \\\\ -60.0 \\\\ 30.0 \\end{bmatrix}$, $b_2 = 15000.0$,\n- Interior collocation: $x \\in \\{2.5\\times 10^{-7},\\,5.0\\times 10^{-7},\\,7.5\\times 10^{-7}\\}\\,\\mathrm{m}$, $t \\in \\{2.0,\\,5.0,\\,8.0\\}\\,\\mathrm{s}$,\n- Boundary times: $t \\in \\{1.0,\\,4.0,\\,7.0\\}\\,\\mathrm{s}$,\n- Initial positions: $x \\in \\{0.0,\\,3.\\overline{3}\\times 10^{-7},\\,6.\\overline{6}\\times 10^{-7},\\,1.0\\times 10^{-6}\\}\\,\\mathrm{m}$,\n- Data points: $(x,t,c_{\\mathrm{data}}) \\in \\{(0.75L,\\,5.0,\\,15100.0),\\,(0.25L,\\,5.0,\\,14950.0)\\}$,\n- Loss weights: $\\lambda_{\\mathrm{pde}} = 1.0$, $\\lambda_{0} = 10.0$, $\\lambda_{L} = 10.0$, $\\lambda_{\\mathrm{ic}} = 5.0$, $\\lambda_{\\mathrm{data}} = 2.0$.\n\nCase $\\mathbf{3}$ (near saturation, lower overpotential):\n- $m = 3$,\n- $L = 1.0\\times 10^{-6}\\,\\mathrm{m}$,\n- $D = 1.0\\times 10^{-14}\\,\\mathrm{m^2/s}$,\n- $T = 298.0\\,\\mathrm{K}$,\n- $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$,\n- $F = 96485.0\\,\\mathrm{C/mol}$,\n- $\\alpha_a = 0.5$, $\\alpha_c = 0.5$,\n- $U_{\\mathrm{ref}} = 4.0\\,\\mathrm{V}$,\n- $c_{\\max} = 25000.0\\,\\mathrm{mol/m^3}$,\n- $c_0 = 24000.0\\,\\mathrm{mol/m^3}$,\n- $k_0 = 4.0\\,\\mathrm{A/m^2}$,\n- $E_{\\mathrm{app}} = 3.9\\,\\mathrm{V}$,\n- $\\epsilon = 1.0\\,\\mathrm{mol/m^3}$,\n- Weights: $W_1 = \\begin{bmatrix} 1.0\\times 10^{6} & -1.0 \\\\ -1.0\\times 10^{6} & 2.0 \\\\ 5.0\\times 10^{5} & -0.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}$, $W_2 = \\begin{bmatrix} 50.0 \\\\ -25.0 \\\\ 12.5 \\end{bmatrix}$, $b_2 = 24000.0$,\n- Interior collocation: $x \\in \\{2.5\\times 10^{-7},\\,5.0\\times 10^{-7},\\,7.5\\times 10^{-7}\\}\\,\\mathrm{m}$, $t \\in \\{2.0,\\,5.0,\\,8.0\\}\\,\\mathrm{s}$,\n- Boundary times: $t \\in \\{1.0,\\,4.0,\\,7.0\\}\\,\\mathrm{s}$,\n- Initial positions: $x \\in \\{0.0,\\,3.\\overline{3}\\times 10^{-7},\\,6.\\overline{6}\\times 10^{-7},\\,1.0\\times 10^{-6}\\}\\,\\mathrm{m}$,\n- Data points: $(x,t,c_{\\mathrm{data}}) \\in \\{(0.75L,\\,5.0,\\,23950.0),\\,(0.25L,\\,5.0,\\,24050.0)\\}$,\n- Loss weights: $\\lambda_{\\mathrm{pde}} = 1.0$, $\\lambda_{0} = 10.0$, $\\lambda_{L} = 10.0$, $\\lambda_{\\mathrm{ic}} = 5.0$, $\\lambda_{\\mathrm{data}} = 2.0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\ell_1,\\ell_2,\\ell_3]$), where the entries are the composite loss values (floats) for Case $\\mathbf{1}$, Case $\\mathbf{2}$, and Case $\\mathbf{3}$, respectively. No other text should be printed.",
            "solution": "The problem is valid. It presents a well-posed, scientifically grounded task to compute the composite loss of a Physics-Informed Neural Network (PINN) designed to model solid-state diffusion in a battery electrode. All necessary physical laws, model equations, network parameters, and evaluation points are provided, enabling a direct and verifiable computation.\n\n### Principle-Based Design for PINN Loss Computation\n\nThe core of this problem lies in evaluating how well a given neural network, with fixed parameters $\\theta$, satisfies a set of physical laws and data constraints. This is quantified by a composite loss function, $\\mathcal{L}(\\theta)$. The PINN framework replaces the traditional numerical discretization of a Partial Differential Equation (PDE) system with a differentiable, analytical representation—the neural network—and recasts the problem of solving the PDE into an optimization problem of minimizing the loss function. Here, the task is a single forward pass: to compute the loss for a pre-defined network, not to perform the optimization (training).\n\n**1. Physical System and Mathematical Model**\n\nThe physical system is the one-dimensional diffusion of lithium ions within a solid active material slab of thickness $L$. The concentration of lithium, $c(x,t)$, is governed by Fick's second law, a fundamental principle of mass transfer:\n$$\n\\frac{\\partial c}{\\partial t} = D \\frac{\\partial^2 c}{\\partial x^2}\n$$\nwhere $D$ is the diffusion coefficient, $x \\in [0,L]$ is the spatial coordinate, and $t$ is time. The system is subject to specific initial and boundary conditions that define a unique physical scenario:\n- **Initial Condition:** At time $t=0$, the concentration is uniform: $c(x,0) = c_0$.\n- **Symmetry Boundary Condition:** At the center of the slab ($x=0$), there is no flux, representing symmetry: $\\frac{\\partial c}{\\partial x}(0,t) = 0$.\n- **Flux Boundary Condition:** At the slab's surface ($x=L$), the flux of lithium ions is driven by an electrochemical reaction. The flux is related to the Butler-Volmer current density, $j_{\\mathrm{BV}}$, which models the kinetics of the charge-transfer reaction:\n$$\n-D \\frac{\\partial c}{\\partial x}(L,t) = \\frac{j_{\\mathrm{BV}}(\\tilde{c}_s(L,t))}{nF}\n$$\nThe Butler-Volmer equation depends on the surface overpotential $\\eta = E_{\\mathrm{app}} - U(c_s)$, which is the difference between the applied potential $E_{\\mathrm{app}}$ and the Nernst equilibrium potential $U(c_s)$. The Nernst potential itself is a function of the surface concentration $c_s = c(L,t)$, and the exchange current density $i_0$ also depends on $c_s$. These relationships tightly couple the PDE to electrochemical principles.\n\n**2. The PINN Approximation**\n\nThe PINN, denoted $\\hat{c}(x,t;\\theta)$, serves as an analytical surrogate for the true concentration field $c(x,t)$. For this problem, a single-hidden-layer feedforward network with hyperbolic tangent ($\\tanh$) activation functions is specified. The network's output is:\n$$\n\\hat{c}(x,t;\\theta) = b_2 + \\sum_{k=1}^{m} W_{2,k}\\,\\tanh(b_{1,k} + W_{1,k,1} x + W_{1,k,2} t)\n$$\nA key advantage of this representation is that its derivatives with respect to its inputs, $x$ and $t$, can be computed analytically and exactly via the chain rule. The problem provides these analytical expressions, which are crucial for evaluating the PDE and boundary condition residuals without numerical differentiation errors. For instance, the time derivative is:\n$$\n\\frac{\\partial \\hat{c}}{\\partial t} = \\sum_{k=1}^m W_{2,k}\\,\\operatorname{sech}^2(z_k)\\,W_{1,k,2}\n$$\nwhere $z_k$ is the argument of the $k$-th neuron's activation function and $\\operatorname{sech}^2$ is the derivative of $\\tanh$.\n\n**3. The Composite Loss Function**\n\nThe composite loss function, $\\mathcal{L}(\\theta)$, is the sum of mean squared residuals, where each residual measures the violation of a specific physical law or data constraint. The total loss is a weighted sum of five components:\n\n- **PDE Residual Loss ($\\mathcal{L}_{\\mathrm{pde}}$):** This term enforces Fick's second law at a set of collocation points in the interior of the $(x,t)$ domain. The residual is $r_{\\mathrm{pde}} = \\frac{\\partial \\hat{c}}{\\partial t} - D\\frac{\\partial^2 \\hat{c}}{\\partial x^2}$. A small residual indicates that the network's output is a good solution to the PDE.\n- **Symmetry Boundary Loss ($\\mathcal{L}_{0}$):** This term enforces the no-flux condition at $x=0$. The residual is $r_{0} = \\frac{\\partial \\hat{c}}{\\partial x}(0,t)$.\n- **Flux Boundary Loss ($\\mathcal{L}_{L}$):** This term enforces the Butler-Volmer flux condition at $x=L$. The residual is $r_{L} = -D\\frac{\\partial \\hat{c}}{\\partial x}(L,t) - \\frac{j_{\\mathrm{BV}}(\\tilde{c}_s(L,t))}{nF}$. This is the most complex term, as it involves evaluating the non-linear Butler-Volmer kinetics based on the PINN's predicted surface concentration. A regularization, $\\tilde{c}_s = \\max(\\epsilon, \\min(c_s, c_{\\max}-\\epsilon))$, is used to ensure numerical stability in the logarithmic and square-root terms of the Nernst and exchange current density models, preventing arguments from reaching $0$ or $c_{\\max}$.\n- **Initial Condition Loss ($\\mathcal{L}_{\\mathrm{ic}}$):** This term anchors the simulation to its starting state by penalizing deviations from the initial concentration $c_0$. The residual is $r_{\\mathrm{ic}}(x_{\\ell}) = \\hat{c}(x_\\ell,0) - c_0$.\n- **Data Mismatch Loss ($\\mathcal{L}_{\\mathrm{data}}$):** This term represents the data assimilation aspect of the digital twin. It measures the discrepancy between the PINN's prediction and sparse \"sensor\" measurements, $c_{\\mathrm{data}}$, at specific locations $(x_d, t_d)$. The residual is $r_{\\mathrm{data}}(x_d,t_d) = \\hat{c}(x_d,t_d) - c_{\\mathrm{data}}$.\n\nThe total loss is the weighted sum of the mean squared values of these residuals:\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}}\\,\\mathcal{L}_{\\mathrm{pde}}\n+ \\lambda_{0}\\,\\mathcal{L}_{0}\n+ \\lambda_{L}\\,\\mathcal{L}_{L}\n+ \\lambda_{\\mathrm{ic}}\\,\\mathcal{L}_{\\mathrm{ic}}\n+ \\lambda_{\\mathrm{data}}\\,\\mathcal{L}_{\\mathrm{data}}\n$$\nThe weights $\\lambda_{\\cdot}$ balance the relative importance of each physical constraint and data source.\n\n**4. Algorithmic Implementation**\n\nThe solution program implements this framework precisely.\n1.  **Parameterization:** The physical constants, model parameters, network weights, collocation points, and loss weights for each of the three test cases are organized into data structures.\n2.  **Vectorized PINN Functions:** The PINN output $\\hat{c}$ and its analytical derivatives ($\\frac{\\partial\\hat{c}}{\\partial t}$, $\\frac{\\partial\\hat{c}}{\\partial x}$, $\\frac{\\partial^2\\hat{c}}{\\partial x^2}$) are implemented as vectorized Python functions using `NumPy`. This allows for efficient evaluation over batches of $(x,t)$ points. For example, the summation over neurons becomes a matrix-vector product.\n3.  **Residual Calculation:** For each of the five loss components, the corresponding residuals are calculated:\n    - Points for each residual type are generated as specified.\n    - The PINN and its derivative functions are called with these points to get the necessary terms.\n    - The non-linear electrochemical functions (Nernst, exchange current, Butler-Volmer) are evaluated as needed for the $\\mathcal{L}_L$ term.\n    - The residual for each point is computed according to its definition.\n4.  **Loss Aggregation:** The squared residuals for each component are averaged, and the final composite loss is computed as the weighted sum of these mean squared errors.\n5.  **Execution Loop:** A main loop iterates through the three test cases, executing the loss computation for each and collecting the results. The final output is formatted as a comma-separated list in brackets, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute the composite PINN loss for each.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline, moderate flux)\n        {\n            \"m\": 3, \"L\": 1.0e-6, \"D\": 1.0e-14, \"T\": 298.0, \"R\": 8.314, \"F\": 96485.0,\n            \"alpha_a\": 0.5, \"alpha_c\": 0.5, \"U_ref\": 4.0, \"c_max\": 25000.0, \"c_0\": 15000.0,\n            \"k_0\": 5.0, \"E_app\": 4.2, \"epsilon\": 1.0, \"n\": 1.0,\n            \"W1\": np.array([[1.0e6, -1.0], [-1.0e6, 2.0], [5.0e5, -0.5]]),\n            \"b1\": np.array([0.0, 0.1, -0.1]),\n            \"W2\": np.array([100.0, -50.0, 25.0]),\n            \"b2\": 15000.0,\n            \"x_pde_rel\": np.array([1/4, 1/2, 3/4]), \"t_pde\": np.array([2.0, 5.0, 8.0]),\n            \"t_bc\": np.array([1.0, 4.0, 7.0]),\n            \"x_ic_rel\": np.array([0.0, 1/3, 2/3, 1.0]),\n            \"data_points\": np.array([[0.75, 5.0, 15050.0], [0.25, 5.0, 14980.0]]),\n            \"lambdas\": {\"pde\": 1.0, \"bc0\": 10.0, \"bcL\": 10.0, \"ic\": 5.0, \"data\": 2.0}\n        },\n        # Case 2 (strong kinetics and higher overpotential)\n        {\n            \"m\": 3, \"L\": 1.0e-6, \"D\": 5.0e-15, \"T\": 298.0, \"R\": 8.314, \"F\": 96485.0,\n            \"alpha_a\": 0.5, \"alpha_c\": 0.5, \"U_ref\": 4.0, \"c_max\": 25000.0, \"c_0\": 15000.0,\n            \"k_0\": 8.0, \"E_app\": 4.5, \"epsilon\": 1.0, \"n\": 1.0,\n            \"W1\": np.array([[0.8e6, -0.8], [-0.8e6, 1.5], [0.4e6, -0.3]]),\n            \"b1\": np.array([0.05, -0.05, 0.1]),\n            \"W2\": np.array([120.0, -60.0, 30.0]),\n            \"b2\": 15000.0,\n            \"x_pde_rel\": np.array([1/4, 1/2, 3/4]), \"t_pde\": np.array([2.0, 5.0, 8.0]),\n            \"t_bc\": np.array([1.0, 4.0, 7.0]),\n            \"x_ic_rel\": np.array([0.0, 1/3, 2/3, 1.0]),\n            \"data_points\": np.array([[0.75, 5.0, 15100.0], [0.25, 5.0, 14950.0]]),\n            \"lambdas\": {\"pde\": 1.0, \"bc0\": 10.0, \"bcL\": 10.0, \"ic\": 5.0, \"data\": 2.0}\n        },\n        # Case 3 (near saturation, lower overpotential)\n        {\n            \"m\": 3, \"L\": 1.0e-6, \"D\": 1.0e-14, \"T\": 298.0, \"R\": 8.314, \"F\": 96485.0,\n            \"alpha_a\": 0.5, \"alpha_c\": 0.5, \"U_ref\": 4.0, \"c_max\": 25000.0, \"c_0\": 24000.0,\n            \"k_0\": 4.0, \"E_app\": 3.9, \"epsilon\": 1.0, \"n\": 1.0,\n            \"W1\": np.array([[1.0e6, -1.0], [-1.0e6, 2.0], [5.0e5, -0.5]]),\n            \"b1\": np.array([0.0, 0.1, -0.1]),\n            \"W2\": np.array([50.0, -25.0, 12.5]),\n            \"b2\": 24000.0,\n            \"x_pde_rel\": np.array([1/4, 1/2, 3/4]), \"t_pde\": np.array([2.0, 5.0, 8.0]),\n            \"t_bc\": np.array([1.0, 4.0, 7.0]),\n            \"x_ic_rel\": np.array([0.0, 1/3, 2/3, 1.0]),\n            \"data_points\": np.array([[0.75, 5.0, 23950.0], [0.25, 5.0, 24050.0]]),\n            \"lambdas\": {\"pde\": 1.0, \"bc0\": 10.0, \"bcL\": 10.0, \"ic\": 5.0, \"data\": 2.0}\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        loss = compute_composite_loss(params)\n        results.append(loss)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\ndef compute_composite_loss(params):\n    \"\"\"\n    Computes the total composite loss for a given set of parameters.\n    \"\"\"\n    \n    # Unpack network parameters for convenience\n    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n    \n    # --- PINN and derivatives definitions ---\n    def get_z(x, t):\n        xt_pairs = np.stack([x.flatten(), t.flatten()], axis=1)\n        z = xt_pairs @ W1.T + b1\n        return z\n\n    def pinn_c(x, t):\n        z = get_z(x, t)\n        c = np.tanh(z) @ W2 + b2\n        return c.reshape(x.shape)\n\n    def pinn_dc_dx(x, t):\n        z = get_z(x, t)\n        sech_z_sq = (1.0 / np.cosh(z))**2\n        dc_dx = (sech_z_sq * W1[:, 0]) @ W2\n        return dc_dx.reshape(x.shape)\n\n    def pinn_dc_dt(x, t):\n        z = get_z(x, t)\n        sech_z_sq = (1.0 / np.cosh(z))**2\n        dc_dt = (sech_z_sq * W1[:, 1]) @ W2\n        return dc_dt.reshape(x.shape)\n\n    def pinn_d2c_dx2(x, t):\n        z = get_z(x, t)\n        tanh_z = np.tanh(z)\n        sech_z_sq = (1.0 / np.cosh(z))**2\n        term = -2 * tanh_z * sech_z_sq * (W1[:, 0]**2)\n        d2c_dx2 = term @ W2\n        return d2c_dx2.reshape(x.shape)\n\n    # --- Electrochemical functions ---\n    def butler_volmer(c_s):\n        R, T, F = params[\"R\"], params[\"T\"], params[\"F\"]\n        c_max, k_0 = params[\"c_max\"], params[\"k_0\"]\n        alpha_a, alpha_c = params[\"alpha_a\"], params[\"alpha_c\"]\n        E_app, U_ref, epsilon = params[\"E_app\"], params[\"U_ref\"], params[\"epsilon\"]\n\n        c_s_reg = np.maximum(epsilon, np.minimum(c_s, c_max - epsilon))\n        \n        # Exchange current density i_0\n        i_0 = k_0 * np.sqrt(c_s_reg * (c_max - c_s_reg))\n        \n        # Nernst potential U\n        U = U_ref + (R * T / F) * np.log(c_s_reg / (c_max - c_s_reg))\n        \n        # Overpotential eta\n        eta = E_app - U\n        \n        # BV equation\n        term_exp = F * eta / (R * T)\n        j_bv = i_0 * (np.exp(alpha_a * term_exp) - np.exp(-alpha_c * term_exp))\n        return j_bv\n\n    # --- Residual Calculations ---\n    L = params[\"L\"]\n    \n    # 1. PDE Loss\n    x_pde_grid, t_pde_grid = np.meshgrid(params[\"x_pde_rel\"] * L, params[\"t_pde\"])\n    dc_dt_vals = pinn_dc_dt(x_pde_grid, t_pde_grid)\n    d2c_dx2_vals = pinn_d2c_dx2(x_pde_grid, t_pde_grid)\n    r_pde = dc_dt_vals - params[\"D\"] * d2c_dx2_vals\n    loss_pde = np.mean(r_pde**2)\n    \n    # 2. Boundary Loss at x=0\n    x_bc0 = np.zeros_like(params[\"t_bc\"])\n    r_bc0 = pinn_dc_dx(x_bc0, params[\"t_bc\"])\n    loss_bc0 = np.mean(r_bc0**2)\n    \n    # 3. Boundary Loss at x=L\n    x_bcL = np.full_like(params[\"t_bc\"], L)\n    c_s_L = pinn_c(x_bcL, params[\"t_bc\"])\n    j_bv_vals = butler_volmer(c_s_L)\n    dc_dx_L = pinn_dc_dx(x_bcL, params[\"t_bc\"])\n    r_bcL = -params[\"D\"] * dc_dx_L - j_bv_vals / (params[\"n\"] * params[\"F\"])\n    loss_bcL = np.mean(r_bcL**2)\n    \n    # 4. Initial Condition Loss\n    t_ic = np.zeros_like(params[\"x_ic_rel\"])\n    c_ic = pinn_c(params[\"x_ic_rel\"] * L, t_ic)\n    r_ic = c_ic - params[\"c_0\"]\n    loss_ic = np.mean(r_ic**2)\n\n    # 5. Data Loss\n    if params[\"data_points\"].shape[0] > 0:\n        x_data = params[\"data_points\"][:, 0] * L\n        t_data = params[\"data_points\"][:, 1]\n        c_target = params[\"data_points\"][:, 2]\n        c_pred = pinn_c(x_data, t_data)\n        r_data = c_pred - c_target\n        loss_data = np.mean(r_data**2)\n    else:\n        loss_data = 0.0\n\n    # --- Composite Loss ---\n    lmb = params[\"lambdas\"]\n    total_loss = (lmb[\"pde\"] * loss_pde +\n                    lmb[\"bc0\"] * loss_bc0 +\n                    lmb[\"bcL\"] * loss_bcL +\n                    lmb[\"ic\"] * loss_ic +\n                    lmb[\"data\"] * loss_data)\n    \n    return total_loss\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "While PINNs can learn to satisfy differential equations, they do not inherently guarantee that their predictions will obey fundamental physical inequality constraints, such as non-negative concentrations or finite material capacities. This exercise addresses the critical challenge of enforcing \"physical admissibility\" on the model's output, a necessity for creating reliable and trustworthy digital twins of battery systems. You will learn to implement smooth, differentiable penalty functions that penalize excursions into unphysical regimes, effectively guiding the optimization process toward solutions that are not only mathematically consistent but also physically realistic .",
            "id": "3940640",
            "problem": "Design and implement a program that constructs Physics-Informed Neural Network (PINN) loss terms for a Lithium-Ion Battery (LIB) electrode-electrolyte system that enforce physically admissible states during training. The PINN is trained on collocation points in space and time, where it outputs predicted fields. The goal is to define three smooth loss terms that impose the following physically motivated constraints: non-negativity of electrolyte concentration, upper bounds for solid-phase lithium concentration, and penalization of excursions into lithium plating regimes. The program must compute these losses for a specified test suite of predicted values and return the aggregate loss per test case.\n\nUse the following context as the fundamental base:\n\n- Mass conservation implies that electrolyte concentration is non-negative, hence $c_e \\ge 0$.\n- Intercalation capacity of the host solid is finite, implying a maximal admissible solid-phase concentration $c_s \\le c_{s,\\max}$.\n- Reaction thermodynamics and kinetics imply plating risk when the interfacial overpotential $\\eta$ drives the reduction branch beyond intercalation preference. In particular, the sign and magnitude of $\\eta$ in the Butler-Volmer (BV) relation are diagnostic of risk; negative values of $\\eta$ increase the tendency for metal deposition. Use the definition of reaction overpotential $\\eta$ consistent with standard electrochemical conventions as the driving variable for penalizing plating.\n\nYour program must:\n\n- Derive and implement smooth, differentiable penalty terms that approximate indicator functions for violations of the constraints. For a generic smooth hinge, use the classical softplus function defined by $\\mathrm{softplus}(x) = \\ln\\left(1 + e^{x}\\right)$, which approximates $\\max(0,x)$ but remains differentiable for all $x$.\n- Define the three loss terms as:\n  1. Electrolyte non-negativity penalty:\n     $$L_{e}^{\\ge 0} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(-c_{e,i}\\right)\\right]^2.$$\n  2. Solid concentration upper-bound penalty:\n     $$L_{s}^{\\le c_{s,\\max}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(c_{s,i} - c_{s,\\max}\\right)\\right]^2.$$\n  3. Plating penalty weighted by electrolyte starvation:\n     $$w_i = 1 + \\exp\\left(-\\frac{c_{e,i}}{c_{e,\\mathrm{ref}}}\\right), \\quad L_{\\mathrm{plate}} = \\frac{1}{N} \\sum_{i=1}^{N} w_i \\left[\\mathrm{softplus}\\left(-\\eta_i\\right)\\right]^2.$$\n- Aggregate the losses into a total loss:\n  $$L_{\\mathrm{total}} = \\alpha \\, L_{e}^{\\ge 0} + \\beta \\, L_{s}^{\\le c_{s,\\max}} + \\gamma \\, L_{\\mathrm{plate}},$$\n  with weights $\\alpha = 1$, $\\beta = 1$, $\\gamma = 1$.\n\nAll quantities, including $c_e$, $c_s$, $c_{s,\\max}$, $\\eta$, and $c_{e,\\mathrm{ref}}$, must be treated as real-valued fields or parameters. Electrolyte concentration $c_e$ and solid-phase concentration $c_s$ must be expressed in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$, and overpotential $\\eta$ must be expressed in $\\mathrm{V}$. The final loss values are dimensionless real numbers. Your program must use the specified test suite below and produce the final outputs as dimensionless floats rounded to six decimal places.\n\nTest suite of parameter values (arrays are per-collocation-point predictions; the array dimension is $N$, where $N$ is the length of the array in each case):\n\n- Case $1$ (nominal regime; all constraints satisfied approximately):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[1000, 950, 1100, 1020, 980, 1010]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[24000, 23000, 25000, 24500, 23500, 24200]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[0.02, 0.015, 0.03, 0.01, 0.025, 0.018]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\n- Case $2$ (electrolyte negativity and mild upper-bound violations near the boundary):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[10, -5, 0, 20, -1, 15]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[29000, 30500, 29500, 30000, 31000, 28500]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[0.0, 0.005, -0.005, 0.01, -0.002, 0.0]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\n- Case $3$ (solid-phase capacity violations dominate; overpotential positive):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[1000, 800, 900, 950, 700, 850]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[32000, 31000, 30500, 31500, 32500, 30000]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[0.02, 0.015, 0.01, 0.02, 0.005, 0.012]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\n- Case $4$ (plating regime with strongly negative overpotentials and electrolyte starvation):\n  - $c_e$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[200, 150, 100, 80, 50, 120]$,\n  - $c_s$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $[25000, 26000, 24000, 23000, 22000, 24500]$,\n  - $c_{s,\\max}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $30000$,\n  - $\\eta$ in $\\mathrm{V}$: $[-0.05, -0.08, -0.12, -0.03, -0.06, -0.09]$,\n  - $c_{e,\\mathrm{ref}}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$: $1000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and rounded to six decimal places, where each entry is the total loss $L_{\\mathrm{total}}$ for the corresponding test case, i.e., $[L_{\\mathrm{total}}^{(1)}, L_{\\mathrm{total}}^{(2)}, L_{\\mathrm{total}}^{(3)}, L_{\\mathrm{total}}^{(4)}]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of electrochemical engineering, employs standard numerical techniques for constrained optimization, is well-posed with all necessary information provided, and lacks any contradictions or ambiguities.\n\nThe objective is to compute a total loss, $L_{\\mathrm{total}}$, for a Physics-Informed Neural Network (PINN) applied to a Lithium-Ion Battery (LIB) model. This loss function is designed to enforce physical admissibility of the network's predictions for key state variables. The total loss is an aggregation of three distinct penalty terms, each corresponding to a fundamental physical constraint. The calculations are to be performed for four separate test cases, each representing a different operating regime.\n\nThe core of the methodology lies in using a smooth, differentiable penalty function to approximate an indicator function for constraint violations. This is essential for use in gradient-based optimization algorithms, which are standard for training neural networks. The problem specifies the softplus function, defined as $\\mathrm{softplus}(x) = \\ln(1 + e^x)$. This function smoothly approximates the rectifier function $\\max(0, x)$. By feeding an appropriate argument into the softplus function, we can construct a penalty that is near zero when a constraint is satisfied and grows quadratically with the magnitude of the violation.\n\nThe three loss components are derived as follows:\n\n1.  Electrolyte Non-Negativity Penalty, $L_{e}^{\\ge 0}$: The physical principle of mass conservation dictates that the electrolyte concentration, $c_e$, cannot be negative. The constraint is $c_{e,i} \\ge 0$ for each collocation point $i$. To penalize violations ($c_{e,i} < 0$), we construct the argument for the softplus function as $-c_{e,i}$. If $c_{e,i} \\ge 0$, then $-c_{e,i} \\le 0$, and $\\mathrm{softplus}(-c_{e,i})$ is close to $0$. If $c_{e,i} < 0$, then $-c_{e,i} > 0$, and $\\mathrm{softplus}(-c_{e,i}) \\approx -c_{e,i} = |c_{e,i}|$. The squared penalty, averaged over all $N$ collocation points, is given by:\n    $$L_{e}^{\\ge 0} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(-c_{e,i}\\right)\\right]^2$$\n\n2.  Solid Concentration Upper-Bound Penalty, $L_{s}^{\\le c_{s,\\max}}$: The intercalation host material in an electrode has a finite capacity for lithium ions, imposing an upper limit on the solid-phase concentration, $c_s$. The constraint is $c_{s,i} \\le c_{s,\\max}$. Violations occur when $c_{s,i} > c_{s,\\max}$. The argument for the softplus function is therefore $c_{s,i} - c_{s,\\max}$. If $c_{s,i} \\le c_{s,\\max}$, the argument is non-positive, and the softplus output is near zero. If $c_{s,i} > c_{s,\\max}$, the argument is positive, and $\\mathrm{softplus}(c_{s,i} - c_{s,\\max}) \\approx c_{s,i} - c_{s,\\max}$. The corresponding loss term is:\n    $$L_{s}^{\\le c_{s,\\max}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[\\mathrm{softplus}\\left(c_{s,i} - c_{s,\\max}\\right)\\right]^2$$\n\n3.  Lithium Plating Penalty, $L_{\\mathrm{plate}}$: Lithium plating is an undesirable side reaction that is more likely to occur at strongly negative interfacial overpotentials, $\\eta < 0$. This penalty term is designed to discourage such conditions. The argument for the softplus function is $-\\eta_i$. If $\\eta_i \\ge 0$, the penalty is negligible. If $\\eta_i < 0$, a penalty proportional to its magnitude is incurred. Furthermore, this risk is exacerbated by electrolyte starvation (low $c_e$). A weighting factor, $w_i = 1 + \\exp\\left(-c_{e,i}/c_{e,\\mathrm{ref}}\\right)$, is introduced to amplify the penalty when $c_{e,i}$ is low. As $c_{e,i} \\to 0$, $w_i \\to 2$, and for large $c_{e,i}$, $w_i \\to 1$. The combined plating loss is:\n    $$L_{\\mathrm{plate}} = \\frac{1}{N} \\sum_{i=1}^{N} w_i \\left[\\mathrm{softplus}\\left(-\\eta_i\\right)\\right]^2$$\n\nFinally, the total loss, $L_{\\mathrm{total}}$, is a weighted sum of these three components. The problem specifies the weights as $\\alpha = 1$, $\\beta = 1$, and $\\gamma = 1$. The total loss is therefore a simple sum:\n$$L_{\\mathrm{total}} = L_{e}^{\\ge 0} + L_{s}^{\\le c_{s,\\max}} + L_{\\mathrm{plate}}$$\n\nThe program will implement these four equations. For each test case, it will take the arrays of predicted values for $c_e$, $c_s$, and $\\eta$, along with the scalar parameters $c_{s,\\max}$ and $c_{e,\\mathrm{ref}}$, and compute $L_{\\mathrm{total}}$. The use of vectorized operations on numerical arrays is the most efficient approach to compute the sums over the collocation points. The final result for each case will be rounded to six decimal places as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes total physics-informed loss for lithium-ion battery PINN predictions.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"c_e\": np.array([1000, 950, 1100, 1020, 980, 1010]),\n            \"c_s\": np.array([24000, 23000, 25000, 24500, 23500, 24200]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([0.02, 0.015, 0.03, 0.01, 0.025, 0.018]),\n            \"c_e_ref\": 1000,\n        },\n        {\n            \"c_e\": np.array([10, -5, 0, 20, -1, 15]),\n            \"c_s\": np.array([29000, 30500, 29500, 30000, 31000, 28500]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([0.0, 0.005, -0.005, 0.01, -0.002, 0.0]),\n            \"c_e_ref\": 1000,\n        },\n        {\n            \"c_e\": np.array([1000, 800, 900, 950, 700, 850]),\n            \"c_s\": np.array([32000, 31000, 30500, 31500, 32500, 30000]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([0.02, 0.015, 0.01, 0.02, 0.005, 0.012]),\n            \"c_e_ref\": 1000,\n        },\n        {\n            \"c_e\": np.array([200, 150, 100, 80, 50, 120]),\n            \"c_s\": np.array([25000, 26000, 24000, 23000, 22000, 24500]),\n            \"c_s_max\": 30000,\n            \"eta\": np.array([-0.05, -0.08, -0.12, -0.03, -0.06, -0.09]),\n            \"c_e_ref\": 1000,\n        },\n    ]\n\n    # Loss weights, as specified in the problem\n    alpha = 1.0\n    beta = 1.0\n    gamma = 1.0\n\n    def softplus(x):\n        \"\"\"\n        Computes the softplus function log(1 + exp(x)).\n        This implementation is numerically stable.\n        \"\"\"\n        # For large x, softplus(x) approx x.\n        # For large negative x, softplus(x) approx 0.\n        return np.log1p(np.exp(-np.abs(x))) + np.maximum(0, x)\n\n    results = []\n    for case in test_cases:\n        c_e = case[\"c_e\"]\n        c_s = case[\"c_s\"]\n        eta = case[\"eta\"]\n        c_s_max = case[\"c_s_max\"]\n        c_e_ref = case[\"c_e_ref\"]\n        \n        # 1. Electrolyte non-negativity penalty\n        l_e = np.mean(softplus(-c_e) ** 2)\n        \n        # 2. Solid concentration upper-bound penalty\n        l_s = np.mean(softplus(c_s - c_s_max) ** 2)\n\n        # 3. Plating penalty\n        w = 1.0 + np.exp(-c_e / c_e_ref)\n        l_plate = np.mean(w * (softplus(-eta) ** 2))\n        \n        # 4. Total Loss\n        l_total = alpha * l_e + beta * l_s + gamma * l_plate\n        \n        results.append(round(l_total, 6))\n\n    # Format the final output string\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A major practical hurdle in training PINNs for multi-physics systems like batteries is numerical stiffness, which arises from the vastly different characteristic time scales of concurrent processes (e.g., slow solid-phase diffusion versus fast electrolyte dynamics). This stiffness can severely impede or stall the training process by creating imbalanced gradients across the loss components. This advanced practice introduces a powerful, two-part strategy to combat stiffness: first, by preconditioning the PDE residuals to equalize their effective time scales, and second, by implementing an adaptive weighting scheme that dynamically balances the influence of each loss term based on its gradient magnitude during training .",
            "id": "3940664",
            "problem": "Consider training a Physics-Informed Neural Network (PINN) for an electrochemical battery model where the governing equations are based on conservation laws and phenomenological transport relations. Focus on a one-dimensional spatial domain with coordinate $x \\in [0,L]$ and time $t \\ge 0$. The core physical base is:\n\n- Conservation of mass for lithium in the solid phase coupled with Fick's law of diffusion: the solid-phase concentration $c_s(x,t)$ obeys $\\frac{\\partial c_s}{\\partial t} = D_s \\frac{\\partial^2 c_s}{\\partial x^2}$, where $D_s$ is the solid-phase diffusion coefficient.\n- Conservation of mass for lithium in the electrolyte phase coupled with Fick's law of diffusion: the electrolyte concentration $c_e(x,t)$ obeys $\\frac{\\partial c_e}{\\partial t} = D_e \\frac{\\partial^2 c_e}{\\partial x^2}$, where $D_e$ is the electrolyte diffusion coefficient.\n- Boundary conditions are imposed at $x=0$ and $x=L$ to ensure physically consistent fluxes and continuity; the corresponding boundary residuals are denoted generically and do not require explicit formulae for this task.\n\nA standard PINN residual loss is the weighted sum of squared residuals for the Partial Differential Equation (PDE) terms and boundary conditions across collocation points. Training stiffness arises when the characteristic time scales associated with the PDE terms differ by orders of magnitude. A preconditioning strategy is sought that rescales PDE residuals to reduce stiffness and adaptively weights the loss components to balance the gradient magnitudes during optimization.\n\nStarting from the above physical base, derive a principled preconditioning and adaptive weighting method with the following objectives:\n\n1. Preconditioning of PDE residuals by nondimensionalization:\n   - Define the characteristic length scale $L$ (in meters) and diffusion coefficients $D_s$ and $D_e$ (in $\\mathrm{m}^2/\\mathrm{s}$) for the solid and electrolyte phases respectively.\n   - From Fickian diffusion, derive characteristic time scales $T_s = \\frac{L^2}{D_s}$ and $T_e = \\frac{L^2}{D_e}$ (in seconds).\n   - Construct residual scaling factors $s_s$ and $s_e$ such that the effective time scales of the solid and electrolyte diffusion residuals are equalized. Explicitly define $s_s$ and $s_e$ in terms of $T_s$ and $T_e$.\n\n2. Adaptive loss weighting based on gradient magnitudes:\n   - Let the gradient magnitude sequences of the loss components be provided for three tasks: solid diffusion residual ($g_s$ sequence), electrolyte diffusion residual ($g_e$ sequence), and boundary condition residual ($g_{bc}$ sequence).\n   - Aggregate each sequence into a representative magnitude using an Exponential Moving Average (EMA), defined for a sequence $\\{u_t\\}$ by $m_1 = u_1$ and $m_t = \\beta m_{t-1} + (1-\\beta) u_t$ for $t \\ge 2$, where $\\beta \\in (0,1)$ is a smoothing parameter. Use $\\beta = 0.9$.\n   - Construct adaptive weights $w_s$, $w_e$, and $w_{bc}$ from the representative magnitudes $m_s$, $m_e$, and $m_{bc}$ via $w_k \\propto (m_k + \\varepsilon)^{-\\alpha}$, where $\\alpha \\in (0,1]$ and $\\varepsilon > 0$ is a small constant to avoid division by zero; use $\\alpha = 0.5$ and $\\varepsilon = 10^{-12}$. Normalize the weights so that their arithmetic mean equals $1$.\n\n3. Quantify the improvement due to the strategy:\n   - Define the time-scale conditioning factor before preconditioning as $\\kappa_{\\text{before}} = \\frac{\\max(T_s, T_e)}{\\min(T_s, T_e)}$ and after preconditioning as $\\kappa_{\\text{after}} = \\frac{\\max(T_s s_s, T_e s_e)}{\\min(T_s s_s, T_e s_e)}$.\n   - Define the gradient imbalance for the three tasks before weighting by the Coefficient of Variation (CV), $\\mathrm{CV}_{\\text{before}} = \\frac{\\sigma(\\{m_s, m_e, m_{bc}\\})}{\\mu(\\{m_s, m_e, m_{bc}\\})}$, and after weighting by $\\mathrm{CV}_{\\text{after}} = \\frac{\\sigma(\\{w_s m_s, w_e m_e, w_{bc} m_{bc}\\})}{\\mu(\\{w_s m_s, w_e m_e, w_{bc} m_{bc}\\})}$, where $\\sigma$ and $\\mu$ denote standard deviation and mean respectively.\n   - Define the composite improvement score as the geometric mean of the two improvement ratios: $I = \\sqrt{\\left(\\frac{\\kappa_{\\text{before}}}{\\kappa_{\\text{after}}}\\right) \\left(\\frac{\\mathrm{CV}_{\\text{before}}}{\\mathrm{CV}_{\\text{after}}}\\right)}$. $I$ is dimensionless and should be reported as a floating-point number.\n\nImplement a program that, for each provided test case, computes $I$ and outputs all results on a single line as specified below.\n\nPhysical units must be used for the input parameters $L$ in meters and $D_s$, $D_e$ in $\\mathrm{m}^2/\\mathrm{s}$. Time scales $T_s$ and $T_e$ must be computed in seconds. The final improvement score $I$ is dimensionless; report it as a floating-point number.\n\nTest Suite:\n- Case 1 (stiff diffusion contrast): $L = 10^{-4} \\ \\mathrm{m}$, $D_s = 10^{-14} \\ \\mathrm{m}^2/\\mathrm{s}$, $D_e = 10^{-10} \\ \\mathrm{m}^2/\\mathrm{s}$, gradient sequences $g_s = [1000, 900, 800, 700, 650]$, $g_e = [5, 5.5, 6, 5.8, 5.7]$, $g_{bc} = [20, 18, 17, 16, 16]$.\n- Case 2 (moderate contrast): $L = 10^{-4} \\ \\mathrm{m}$, $D_s = 10^{-12} \\ \\mathrm{m}^2/\\mathrm{s}$, $D_e = 2 \\cdot 10^{-12} \\ \\mathrm{m}^2/\\mathrm{s}$, gradient sequences $g_s = [10, 9.5, 9, 9.2, 9.1]$, $g_e = [8, 8.2, 8.1, 8.0, 8.05]$, $g_{bc} = [9, 9, 8.9, 9.1, 9.0]$.\n- Case 3 (extreme stiffness): $L = 5 \\cdot 10^{-5} \\ \\mathrm{m}$, $D_s = 10^{-16} \\ \\mathrm{m}^2/\\mathrm{s}$, $D_e = 5 \\cdot 10^{-10} \\ \\mathrm{m}^2/\\mathrm{s}$, gradient sequences $g_s = [5000, 4000, 3000, 2500, 2300]$, $g_e = [3, 3.1, 3.2, 3.3, 3.25]$, $g_{bc} = [15, 14, 13, 12.5, 12.2]$.\n- Case 4 (near-balanced diffusion, unbalanced gradients): $L = 10^{-4} \\ \\mathrm{m}$, $D_s = 10^{-12} \\ \\mathrm{m}^2/\\mathrm{s}$, $D_e = 1.1 \\cdot 10^{-12} \\ \\mathrm{m}^2/\\mathrm{s}$, gradient sequences $g_s = [50, 49, 48, 47, 46]$, $g_e = [2, 2.1, 2.2, 2.15, 2.1]$, $g_{bc} = [1, 1.1, 1.2, 1.15, 1.1]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$), where each $resulti$ is the composite improvement score $I$ for Case $i$ as a floating-point number.",
            "solution": "The problem requires the derivation and implementation of a combined preconditioning and adaptive weighting strategy to mitigate training stiffness in Physics-Informed Neural Networks (PINNs) for a simplified electrochemical battery model. The improvement conferred by this strategy is to be quantified by a composite score, $I$. The model consists of two uncoupled Fickian diffusion equations for solid-phase concentration $c_s(x,t)$ and electrolyte-phase concentration $c_e(x,t)$ in a one-dimensional domain $x \\in [0,L]$.\n\nThe governing partial differential equations (PDEs) are:\n$$\n\\frac{\\partial c_s}{\\partial t} = D_s \\frac{\\partial^2 c_s}{\\partial x^2}\n$$\n$$\n\\frac{\\partial c_e}{\\partial t} = D_e \\frac{\\partial^2 c_e}{\\partial x^2}\n$$\nwhere $D_s$ and $D_e$ are the respective diffusion coefficients. Stiffness arises when the characteristic time scales of these processes are disparate. The solution is presented in three parts as outlined in the problem.\n\n**1. Preconditioning of PDE Residuals by Nondimensionalization**\n\nThe first objective is to re-scale the PDE residuals to equalize their effective time scales. The characteristic time scale for a diffusion process over a length $L$ with diffusivity $D$ is given by dimensional analysis as $T = \\frac{L^2}{D}$. For the solid and electrolyte phases, these scales are:\n$$\nT_s = \\frac{L^2}{D_s} \\quad \\text{and} \\quad T_e = \\frac{L^2}{D_e}\n$$\nThe disparity between these time scales is a primary source of stiffness. This disparity can be quantified by the time-scale conditioning factor, defined as the ratio of the larger to the smaller time scale:\n$$\n\\kappa_{\\text{before}} = \\frac{\\max(T_s, T_e)}{\\min(T_s, T_e)}\n$$\nA large value of $\\kappa_{\\text{before}}$ indicates significant stiffness. The strategy is to introduce scaling factors $s_s$ and $s_e$ that multiply the solid and electrolyte PDE residuals, respectively, in the PINN loss function. The problem defines the \"effective time scales\" after scaling as $T_s s_s$ and $T_e s_e$. The goal is to choose $s_s$ and $s_e$ such that these effective time scales are equal:\n$$\nT_s s_s = T_e s_e\n$$\nThis equation has infinitely many solutions for $s_s$ and $s_e$. A principled and symmetric choice is to set both effective time scales to a common reference scale, $T_{\\text{ref}}$, which is itself constructed from $T_s$ and $T_e$. The geometric mean is a natural choice for this reference scale:\n$$\nT_{\\text{ref}} = \\sqrt{T_s T_e}\n$$\nBy setting $T_s s_s = T_{\\text{ref}}$ and $T_e s_e = T_{\\text{ref}}$, we derive the scaling factors:\n$$\ns_s = \\frac{T_{\\text{ref}}}{T_s} = \\frac{\\sqrt{T_s T_e}}{T_s} = \\sqrt{\\frac{T_e}{T_s}}\n$$\n$$\ns_e = \\frac{T_{\\text{ref}}}{T_e} = \\frac{\\sqrt{T_s T_e}}{T_e} = \\sqrt{\\frac{T_s}{T_e}}\n$$\nWith these factors, the new effective time scales are perfectly equalized: $T_s s_s = \\sqrt{T_s T_e}$ and $T_e s_e = \\sqrt{T_s T_e}$. Consequently, the conditioning factor after preconditioning is:\n$$\n\\kappa_{\\text{after}} = \\frac{\\max(T_s s_s, T_e s_e)}{\\min(T_s s_s, T_e s_e)} = \\frac{\\sqrt{T_s T_e}}{\\sqrt{T_s T_e}} = 1\n$$\nThis demonstrates that the preconditioning perfectly balances the time scales of the PDE components.\n\n**2. Adaptive Loss Weighting Based on Gradient Magnitudes**\n\nThe second objective is to balance the contributions of different components of the total loss function (solid diffusion residual, electrolyte diffusion residual, and boundary condition residuals) during training. This is achieved by adaptively weighting each component based on the magnitude of its gradient with respect to the network parameters.\n\nFirst, a representative magnitude for each loss component's gradient is computed from their respective time-series data ($g_s$, $g_e$, $g_{bc}$). An Exponential Moving Average (EMA) is used for this purpose. For a generic gradient magnitude sequence $\\{u_t\\}$, the EMA sequence $\\{m_t\\}$ is calculated as:\n$$\nm_1 = u_1\n$$\n$$\nm_t = \\beta m_{t-1} + (1-\\beta) u_t \\quad \\text{for } t \\ge 2\n$$\nwith a smoothing parameter $\\beta = 0.9$. The last value of this sequence is taken as the representative magnitude for that gradient, denoted as $m_k$ for task $k \\in \\{s, e, bc\\}$.\n\nNext, adaptive weights $w_k$ are constructed from these representative magnitudes $m_k$. The formula is given as $w_k \\propto (m_k + \\varepsilon)^{-\\alpha}$, where $\\alpha = 0.5$ and $\\varepsilon = 10^{-12}$ is a small stabilization constant. Let the unnormalized weights be $w'_k = (m_k + \\varepsilon)^{-\\alpha}$.\n\nThese weights must be normalized such that their arithmetic mean equals $1$. For the three tasks, this condition is $\\frac{1}{3}(w_s + w_e + w_{bc}) = 1$, which simplifies to $w_s + w_e + w_{bc} = 3$. The normalization is performed as follows:\n$$\nw_k = \\frac{3 \\cdot w'_k}{\\sum_{j \\in \\{s, e, bc\\}} w'_j} = \\frac{3 \\cdot (m_k + \\varepsilon)^{-\\alpha}}{\\sum_{j \\in \\{s, e, bc\\}} (m_j + \\varepsilon)^{-\\alpha}}\n$$\n\n**3. Quantifying the Improvement**\n\nThe final part of the problem is to combine the effects of preconditioning and adaptive weighting into a single composite improvement score $I$.\n\nThe improvement in gradient balance is quantified by comparing the coefficient of variation (CV) of the gradient magnitudes before and after weighting. The CV is the ratio of the standard deviation $\\sigma$ to the mean $\\mu$.\n\nBefore weighting, the set of magnitudes is $\\{m_s, m_e, m_{bc}\\}$. The imbalance is:\n$$\n\\mathrm{CV}_{\\text{before}} = \\frac{\\sigma(\\{m_s, m_e, m_{bc}\\})}{\\mu(\\{m_s, m_e, m_{bc}\\})}\n$$\nAfter weighting, the effective magnitudes are $\\{w_s m_s, w_e m_e, w_{bc} m_{bc}\\}$. The imbalance is:\n$$\n\\mathrm{CV}_{\\text{after}} = \\frac{\\sigma(\\{w_s m_s, w_e m_e, w_{bc} m_{bc}\\})}{\\mu(\\{w_s m_s, w_e m_e, w_{bc} m_{bc}\\})}\n$$\nThe composite improvement score $I$ is the geometric mean of the improvement ratios for time-scale conditioning and gradient imbalance:\n$$\nI = \\sqrt{\\left(\\frac{\\kappa_{\\text{before}}}{\\kappa_{\\text{after}}}\\right) \\left(\\frac{\\mathrm{CV}_{\\text{before}}}{\\mathrm{CV}_{\\text{after}}}\\right)}\n$$\nSubstituting $\\kappa_{\\text{after}} = 1$, the formula simplifies to:\n$$\nI = \\sqrt{\\kappa_{\\text{before}} \\frac{\\mathrm{CV}_{\\text{before}}}{\\mathrm{CV}_{\\text{after}}}}\n$$\nThis score holistically measures the reduction in stiffness from both disparate time scales and imbalanced gradient contributions, providing a quantitative assessment of the proposed strategy's effectiveness.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PINN preconditioning and weighting problem for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (stiff diffusion contrast)\n        {\n            \"L\": 1e-4, \"Ds\": 1e-14, \"De\": 1e-10,\n            \"gs\": [1000, 900, 800, 700, 650],\n            \"ge\": [5, 5.5, 6, 5.8, 5.7],\n            \"gbc\": [20, 18, 17, 16, 16]\n        },\n        # Case 2 (moderate contrast)\n        {\n            \"L\": 1e-4, \"Ds\": 1e-12, \"De\": 2e-12,\n            \"gs\": [10, 9.5, 9, 9.2, 9.1],\n            \"ge\": [8, 8.2, 8.1, 8.0, 8.05],\n            \"gbc\": [9, 9, 8.9, 9.1, 9.0]\n        },\n        # Case 3 (extreme stiffness)\n        {\n            \"L\": 5e-5, \"Ds\": 1e-16, \"De\": 5e-10,\n            \"gs\": [5000, 4000, 3000, 2500, 2300],\n            \"ge\": [3, 3.1, 3.2, 3.3, 3.25],\n            \"gbc\": [15, 14, 13, 12.5, 12.2]\n        },\n        # Case 4 (near-balanced diffusion, unbalanced gradients)\n        {\n            \"L\": 1e-4, \"Ds\": 1e-12, \"De\": 1.1e-12,\n            \"gs\": [50, 49, 48, 47, 46],\n            \"ge\": [2, 2.1, 2.2, 2.15, 2.1],\n            \"gbc\": [1, 1.1, 1.2, 1.15, 1.1]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        I = calculate_improvement(\n            case[\"L\"], case[\"Ds\"], case[\"De\"],\n            case[\"gs\"], case[\"ge\"], case[\"gbc\"]\n        )\n        results.append(f\"{I:.10f}\") # Using f-string formatting to avoid trailing zeros\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_ema(sequence, beta):\n    \"\"\"\n    Computes the Exponential Moving Average of a sequence.\n    \"\"\"\n    m = sequence[0]\n    for i in range(1, len(sequence)):\n        m = beta * m + (1 - beta) * sequence[i]\n    return m\n\ndef calculate_cv(data):\n    \"\"\"\n    Calculates the Coefficient of Variation (sigma / mu) for a dataset.\n    Uses population standard deviation (ddof=0).\n    \"\"\"\n    mean = np.mean(data)\n    std_dev = np.std(data, ddof=0)\n    if mean == 0:\n        return 0.0 # Or handle as an error, but for this problem, mean > 0\n    return std_dev / mean\n\ndef calculate_improvement(L, Ds, De, gs, ge, gbc):\n    \"\"\"\n    Calculates the composite improvement score I for a single test case.\n    \"\"\"\n    # Constants from the problem description\n    beta = 0.9\n    alpha = 0.5\n    epsilon = 1e-12\n\n    # Part 1: Preconditioning\n    Ts = L**2 / Ds\n    Te = L**2 / De\n\n    kappa_before = max(Ts, Te) / min(Ts, Te)\n    # As derived, the preconditioning equalizes the time scales perfectly.\n    kappa_after = 1.0\n    \n    kappa_ratio = kappa_before / kappa_after\n\n    # Part 2: Adaptive Weighting\n    # Calculate representative gradient magnitudes using EMA\n    ms = calculate_ema(gs, beta)\n    me = calculate_ema(ge, beta)\n    mbc = calculate_ema(gbc, beta)\n    \n    magnitudes_before = np.array([ms, me, mbc])\n\n    # Part 3: Quantify Improvement\n    # Calculate CV before weighting\n    cv_before = calculate_cv(magnitudes_before)\n    \n    # Calculate weights and weighted magnitudes\n    # Unnormalized weights: w_k' = (m_k + eps)^-alpha\n    w_prime_s = (ms + epsilon)**(-alpha)\n    w_prime_e = (me + epsilon)**(-alpha)\n    w_prime_bc = (mbc + epsilon)**(-alpha)\n    \n    w_primes = np.array([w_prime_s, w_prime_e, w_prime_bc])\n\n    # Normalization constant C = 3 / sum(w_primes)\n    norm_const = 3.0 / np.sum(w_primes)\n    \n    # Normalized weights\n    ws = w_prime_s * norm_const\n    we = w_prime_e * norm_const\n    wbc = w_prime_bc * norm_const\n\n    # Magnitudes after weighting\n    magnitudes_after = np.array([ws * ms, we * me, wbc * mbc])\n\n    # Calculate CV after weighting\n    cv_after = calculate_cv(magnitudes_after)\n    \n    if cv_after == 0:\n        # If perfect balancing occurs, the improvement ratio is theoretically infinite.\n        # This case is unlikely with alpha=0.5 but handle for robustness.\n        # For this problem, it will not happen, so we can assume cv_after > 0.\n        cv_ratio = np.inf\n    else:\n        cv_ratio = cv_before / cv_after\n    \n    # Calculate composite improvement score\n    I = np.sqrt(kappa_ratio * cv_ratio)\n    \n    return I\n\nsolve()\n\n```"
        }
    ]
}