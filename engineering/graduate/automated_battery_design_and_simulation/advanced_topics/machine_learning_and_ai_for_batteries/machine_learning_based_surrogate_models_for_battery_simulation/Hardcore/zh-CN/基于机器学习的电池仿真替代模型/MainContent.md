## 引言
随着电池技术在电动汽车、储能系统等领域的广泛应用，对其性能、寿命和安全性的精准预测与优化变得至关重要。高保真电化学仿真模型（如Doyle-Fuller-Newman模型）是理解电池内部复杂过程的黄金标准，然而其巨大的计算成本严重制约了其在设计优化、实时控制和大规模参数研究中的应用。这一“仿真瓶颈”构成了当前电池研发与管理中的一个核心挑战，而基于机器学习的代理模型为突破这一瓶颈提供了极具前景的解决方案。

本文旨在系统性地介绍如何构建、应用和验证用于电池仿真的[机器学习代理模型](@entry_id:1127558)。通过学习，您将掌握利用数据驱动方法大幅加速仿真流程，并将其与物理知识深度融合以确保模型预测的准确性与可靠性的关键技术。文章分为三个核心部分：

- **第一章：原理与机制** 将深入剖析代理建模的理论基础，对比[高斯过程](@entry_id:182192)与神经网络等核心范式，并重点阐述融合物理约束的先进架构与方法。
- **第二章：应用与跨学科连接** 将展示这些代理模型如何在自动化设计、[参数辨识](@entry_id:275549)、数字孪生和安全监控等实际工程问题中发挥关键作用。
- **第三章：动手实践** 将提供一系列编程练习，帮助您将理论知识转化为解决具体问题的实践技能。

现在，让我们从构建这些强大代理模型所需的核心原理与机制开始。

## 原理与机制

本章旨在深入探讨基于机器学习的电池仿真代理模型的构建、训练和应用中所涉及的核心科学原理与关键技术机制。我们将从代理模型的基本定义出发，系统性地剖析用于构建这些模型的多种[机器学习范式](@entry_id:637731)。在此基础上，我们将重点阐述如何将物理知识融入数据驱动模型，以提升其预测能力、泛化性能和物理一致性。最后，我们将讨论在实际工作流程中至关重要的数据效率和模型验证等高级议题。

### 代理建模的基础

在深入探讨具体的机器学习技术之前，我们必须首先明确代理模型在电池仿真领域中的确切含义，并理解其数据来源的物理基础。

#### 代理模型与降阶模型的区分

在计算科学领域，**代理模型 (surrogate model)**，或称元模型 (metamodel)，是一个复杂高保真模型（如有限元或有限体积求解器）的 computationally inexpensive 近似。其核心目标是在保持可接[受精](@entry_id:274949)度的前提下，大幅降低评估成本。在电池设计和仿真中，高保真模型通常指求解一套复杂的、耦合的[偏微分](@entry_id:194612)方程（PDEs）的电化学模型，例如 [Doyle-Fuller-Newman (DFN) 模型](@entry_id:1123958)。给定一组输入参数（如材料属性 $\theta$ 和电流曲线 $I(t)$），高保真模型通过数值求解输出一系列响应（如端电压 $V(t)$）。这个过程可能需要数分钟到数小时。一个数据驱动的代理模型 $\hat{\mathcal{S}}$ 旨在学习这个从输入到输出的映射关系 $\mathcal{S}:(\theta, I) \mapsto V(\cdot)$，一旦训练完成，其评估（即预测）过程通常仅涉及一次[前向计算](@entry_id:193086)（例如，通过一个神经网络），耗时在毫秒级别。

需要将数据驱动的代理模型与另一类[模型简化](@entry_id:171175)技术——**基于物理的降阶模型 (Reduced-Order Models, ROMs)**——明确区分开来 。ROMs 并非直接学习输入-输出映射，而是通过将原始的控制方程投影到一个低维的基函数子空间上来降低模型的自由度。这些基函数通常从物理系统本身（例如，通过对高保真模型解的快照进行本征正交分解 Proper Orthogonal Decomposition, POD）中提取。虽然 ROMs 的维度远低于原始模型，但在进行新的预测时，仍然需要在线求解这个降阶后的（常[微分](@entry_id:158422)或[偏微分](@entry_id:194612)）方程系统。这个“在[线积分](@entry_id:141417)成本”使得 ROMs 在预测速度上通常慢于数据驱动的代理模型，但快于原始的高保真模型。简而言之，代理模型在推理阶段“绕过”了物理方程求解，而 ROMs 则是“加速”了物理方程求解。

#### 训练数据的来源：电化学模型层级

代理模型的性能在很大程度上取决于用于训练它的数据的质量和覆盖范围。这些数据通常由更高保真的物理模型生成。在[锂离子电池](@entry_id:150991)领域，存在一个公认的模型层级，工程师可根据精度和计算成本之间的权衡来选择数据生成器 。

处于高保真一端的是**伪二维 (Pseudo-Two-Dimensional, P2D) 模型**。P2D 模型是[电化学工程](@entry_id:271372)领域的标准连续介质模型。其名称“伪二维”源于它耦合了两个一维[空间域](@entry_id:911295)：
1.  **宏观维度 ($x$)**: 描述沿着电芯厚度方向（负极-隔膜-正极）的物理过程。模型在此维度上求解电解液中的离子浓度 $c_e(x, t)$ 和电势 $\phi_e(x, t)$，以及固相电极中的电势 $\phi_s(x, t)$。电解液中的离子传输由[浓差极化](@entry_id:266906)理论描述，能够捕捉高倍率下不均匀的电化学反应分布。
2.  **微观维度 ($r$)**: 在电极内的每一个宏观位置 $x$ 处，活性材料被建模为一组相同的球形颗粒。模型在[球坐标](@entry_id:146054)的径向维度 $r$ 上求解固相内部的锂[离子浓度](@entry_id:268003) $c_s(x, r, t)$。

这两个维度通过颗粒表面的 Butler-Volmer [动力学方程](@entry_id:751029)耦合。由于 P2D 模型能够捕捉到诸多关键的性能限制因素（如电解液极化和[固相扩散](@entry_id:1131915)限制），它常被用作生成高质量训练标签的“金标准”，前提是计算预算允许。

为了降低计算成本，研究人员开发了多种简化模型，其中最著名的是**单颗粒模型 (Single Particle Model, SPM)** 及其改进版——**含[电解质](@entry_id:261072)的单颗粒模型 (Single Particle Model with electrolyte, SPMe)**。SPMe 模型的关键简化假设是，在整个电极厚度上，电化学反应电流密度是均匀的。这一假设的直接后果是，电极内的所有活性颗粒行为完全一致。因此，不再需要在每个 $x$ 位置追踪颗粒状态，而是可以将整个电极简化为单个具有代表性的球形颗粒。固相扩散问题从求解 $c_s(x, r, t)$ 简化为仅求解 $c_s(r, t)$。然而，与更简单的 SPM 不同，SPMe 模型保留了沿 $x$ 维度的[电解质](@entry_id:261072)物理场描述，即它仍然求解关于 $c_e(x, t)$ 和 $\phi_e(x, t)$ 的[偏微分](@entry_id:194612)方程。这使得 SPMe 在保持较高计算效率的同时，仍能捕捉到电解液相的部分动态特性。

因此，在为代理模型生成数据的层级结构中，P2D 模型是高保真基准，而 SPMe 是一个计算成本较低的中保真选项，非常适合进行大规模[参数扫描](@entry_id:1129336)、代理模型预训练或主动学习中的快速筛选。这两者都处于[等效电路模型](@entry_id:1124621)（经验性，保真度最低）和微[结构解析](@entry_id:174508)模型（保真度最高，但计算上不适用于大规模数据生成）之间。

### 代理建模的核心[机器学习范式](@entry_id:637731)

一旦确定了数据来源，下一步便是选择合适的机器学习模型来学习输入-输出映射。最主流的两类模型是高斯过程和神经网络。它们各自具有独特的[归纳偏置](@entry_id:137419)和适用场景，理解其内在差异对于[模型选择](@entry_id:155601)至关重要。

#### [高斯过程](@entry_id:182192) (Gaussian Processes, GPs)

**高斯过程**是一种非参数的贝叶斯方法，它直接在函数空间上定义概率分布。一个 GP 由一个**[均值函数](@entry_id:264860)** $m(\cdot)$ 和一个**协方差函数（或核函数）** $k(\cdot, \cdot')$ 完全确定。其核心思想是，任何函数上有限个点的集合的函数值都服从一个[联合高斯](@entry_id:636452)分布。

在代理建模中，GP 的关键优势在于其**内建的[不确定性量化](@entry_id:138597)**能力 。对于一个新的输入点，GP 不仅提供一个预测值（[后验均值](@entry_id:173826)），还提供一个预测方差，该方差表示模型对预测的不确定性。这种不确定性可以分解为：由于观测噪声导致的**[偶然不确定性](@entry_id:634772) (aleatoric uncertainty)** 和由于该区域训练数据稀疏导致的**认知不确定性 (epistemic uncertainty)**。在电池设计等高风险应用中，这种经过校准的不确定性对于[风险评估](@entry_id:170894)、主动学习（选择下一个最有信息量的仿真点）以及安全操作至关重要  。

GP 的行为很大程度上由核函数决定，核函数编码了关于目标函数的先验知识（或假设），例如光滑度。常用的核函数如[平方指数核](@entry_id:191141)（对应于无限次可微的函数）和 Matérn 核（对应于有限光滑度的函数）。

#### 神经网络 (Neural Networks, NNs)

**神经网络**，特别是[深度神经网络](@entry_id:636170) (DNNs)，是强大的[函数逼近](@entry_id:141329)器，以其**通用逼近定理**而闻名，该定理表明具有足够容量的 NN 可以逼近任意连续函数。NN 通过层级化的结构学习特征的组合与抽象，使其能够从大规模数据中发现复杂的、[非线性](@entry_id:637147)的、层次化的模式。

与 GP 不同，标准 NN 提供的是点预测，本身不提供不确定性量化。虽然可以通过[贝叶斯神经网络](@entry_id:746725) (BNN) 或[深度集成](@entry_id:636362) (Deep Ensembles) 等技术来获取不确定性，但这通常需要更复杂的训练和推理过程。NN 的主要优势在于其**[可扩展性](@entry_id:636611)**。使用基于小批量数据的[随机梯度下降](@entry_id:139134)进行训练，NN 可以高效地处理数十万甚至数百万个数据点，而标准 GP 的计算复杂度（训练为 $O(n^3)$）使其在数据量超过数千时变得不切实际。此外，NN 在处理高维输入方面通常表现更佳，因为它们能够通过其层次结构发现数据内在的低维结构，从而在一定程度上缓解**维度灾难** 。

#### 模型选择：GP vs. NN

在为电池 P2D 模型的输入-输出映射 $f: \mathcal{X} \subset \mathbb{R}^d \to \mathbb{R}$ 选择代理模型时，应考虑以下权衡 ：

*   **数据量小、维度低、函数光滑的场景**: 当训练数据量不大（例如，$n \le 10^4$）、输入维度较低（例如，$d \le 10$），且有理由相信底层物理映射 $f$ 非常光滑时，**高斯过程**通常是首选。其强大的先验（通过光滑[核函数](@entry_id:145324)）带来了极高的数据效率，并且其原生的、经过校准的不确定性量化能力在这种情况下表现出色。

*   **数据量大、维度高、函数复杂的场景**: 当训练数据量非常大（例如，$n \ge 10^5$）、输入维度很高（$d \ge 50$），或者函数 $f$ 表现出非平稳行为或急剧的状态变化（例如，由于电解液耗尽或析锂导致的“悬崖”效应）时，**深度神经网络**是更优的选择。NN 的[可扩展性](@entry_id:636611)使其能够利用海量数据，其灵活性使其能够捕捉非光滑和非平稳的行为，其层次化的[归纳偏置](@entry_id:137419)有助于在高维空间中发现有意义的结构。

*   **对不确定性有严格要求的场景**: 如果任务需要基于不确定性进行决策（如主动学习、[鲁棒优化](@entry_id:163807)或安全边界识别），即使数据量较大，GP（或其可扩展的变体）也具有明显优势。例如，在探索一个设计空间时，GP 的后验方差可以直接用于指导下一个仿真点的选择，以最大化[信息增益](@entry_id:262008) 。

*   **噪声特性**: 标准 GP 假设高斯噪声，如果模拟器输出的噪声是[重尾](@entry_id:274276)的或有离群点，标准 GP 可能会表现不佳。而 NN 可以通过选择鲁棒的损失函数（如 Huber 损失）来更好地处理非高斯噪声。当然，GP 也可以通过使用非高斯[似然](@entry_id:167119)（如学生 t 分布似然）来扩展，但这会增加模型的复杂性。

总之，GP 可被视作一个“小型、精确、带测量尺的工具”，而 NN 则是一个“大型、强大、可适应的工具”。选择哪一个取决于手头任务的具体需求。

### 先进代理架构与物理融合技术

虽然 GP 和标准 NN 是强大的基础工具，但电池仿真的复杂性（高维输出、时序依赖、严格的物理约束）催生了更先进的代理模型架构。一个核心主题是如何将物理知识融入这些模型，以超越纯粹的“黑箱”逼近。

#### 处理高维与函数式数据

P2D 模型的输出不仅是单个标量，而是分布在空间和时间上的**场变量**（如 $c_s(x,r,t)$, $\phi_e(x,t)$）。直接对这些高维向量进行回归是低效的。

一种策略是**[降维](@entry_id:142982)**。首先将高维输出场压缩到一个低维的**隐空间 (latent space)**，然后学习从输入参数到这个隐空间的映射。**[主成分分析](@entry_id:145395) (Principal Component Analysis, PCA)** 是一种经典的线性[降维](@entry_id:142982)方法，它通过寻找[数据协方差](@entry_id:748192)矩阵的主要[特征向量](@entry_id:151813)来找到捕获最大方差的[线性子空间](@entry_id:151815)。一个线性**自编码器 (Autoencoder, AE)**，在[均方误差](@entry_id:175403)损失下，本质上学习的是与 PCA 相同的子空间 。然而，当数据背后的物理现象（如移动的反应锋面）导致数据点分布在一个[非线性](@entry_id:637147)的[低维流形](@entry_id:1127469)上时，线性方法如 PCA 的效率会很低。在这种情况下，**[非线性](@entry_id:637147)自编码器**，特别是利用了[平移不变性](@entry_id:195885)[归纳偏置](@entry_id:137419)的**[卷积自编码器](@entry_id:905501) (Convolutional AE)**，表现要优越得多。它们能够学习到弯曲的流形，并用更少的[隐变量](@entry_id:150146)来高效地表示平移的特征 。

另一种更直接的策略是**[算子学习](@entry_id:752958) (Operator Learning)**。这种方法旨在直接学习从一个函数空间到另一个[函数空间](@entry_id:143478)的映射（即算子）。这对于电池模型尤其有吸[引力](@entry_id:189550)，因为输入（如电流曲线 $I(t)$）和输出（如电压曲线 $V(t)$）本身就是函数。**[深度算子网络](@entry_id:748262) (Deep Operator Network, [DeepONet](@entry_id:748262))** 是一种领先的[算子学习](@entry_id:752958)架构。它由两个网络组成：一个**分支网络 (branch net)**，用于处理输入函数并输出一组系数；一个**主干网络 (trunk net)**，用于处理输出函数的查询点。这两个网络的输出通过点积结合，得到在特定查询点的输出值。[DeepONet](@entry_id:748262) 的关键优势在于它与网格无关，并且可以处理不同形式的输入函数。在为电池等时序系统建模时，必须仔细设计 [DeepONet](@entry_id:748262) 以保证**因果性 (causality)**，即在时刻 $t$ 的预测输出只能依赖于时刻 $t$ 之前的输入。这可以通过在分支网络中使用因果掩码等技术来实现，确保模型在推理时不会“看到未来” 。

#### 融入物理知识的方法

纯数据驱动的“黑箱”模型虽然灵活，但可能需要大量数据才能学习，并且其预测可能违反基本的物理定律（如质量守恒），导致在训练数据范围之外的泛化能力很差。将物理知识融入模型是提升其性能和可靠性的关键。

##### [灰箱建模](@entry_id:1125753)与守恒律

**[灰箱模型](@entry_id:1125766) (Gray-box model)** 是一种混合建模方法，它将已知的物理模型（白箱部分）与一个学习到的数据驱动组件（黑箱部分）相结合 。一个常见的形式是在物理模型的控制方程中加入一个学习到的**残差项** $r_{\phi}$：
$$ \dot{z}(t) = f_{\text{phys}}(z(t), u(t); \theta) + r_{\phi}(z(t), u(t)) $$
这里，$f_{\text{phys}}$ 是已知的物理动态模型，$r_{\phi}$ 是一个神经网络，用于学习物理模型与真实数据之间的差异。与需要从头学习所有动态的纯黑箱模型相比，[灰箱模型](@entry_id:1125766)通常需要更少的数据，因为物理模型 $f_{\text{phys}}$ 提供了强大的[归纳偏置](@entry_id:137419)。

对于[电池模型](@entry_id:1121428)，一个至关重要的物理约束是**守恒律**，例如锂离子的总[质量守恒](@entry_id:204015)。一个无约束的残差项 $r_{\phi}$ 很容易在预测中“创造”或“消灭”锂离子。为了避免这种情况，必须在模型架构层面施加**硬约束**。一种优雅的方法是在**通量 (flux)** 层面引入残差。例如，如果物理模型中的[电解质](@entry_id:261072)通量是 $J_{e, \text{phys}}$，我们可以将总通量建模为 $J_e = J_{e, \text{phys}} + \tilde{J}_{\phi}$，其中 $\tilde{J}_{\phi}$ 是一个学习到的残差通量。如果通过架构设计保证残差通量在系统边界处为零（$\tilde{J}_{\phi} \cdot n = 0$），那么根据散度定理，其对总质量变化率的贡献恒为零。这样，无论[残差网络](@entry_id:634620) $\tilde{J}_{\phi}$ 如何学习，总的质量守恒定律都得到严格保证。这种**结构上守恒 (conservative by construction)** 的模型在面对分布外推时表现出极大的鲁棒性 。相比之下，通过在[损失函数](@entry_id:634569)中添加惩罚项的“软约束”方法，无法保证物理定律的精确满足。

##### [物理信息神经网络](@entry_id:145229) ([PINNs](@entry_id:145229))

**物理信息神经网络 (Physics-Informed Neural Networks, [PINNs](@entry_id:145229))** 是实现物理约束的一种特定且流行的方法。PINN 的核心思想是将控制方程（PDEs）的残差作为正则化项加入到[损失函数](@entry_id:634569)中。一个神经网络被构建用来直接逼近 PDE 的解，例如，输入为 $(x, r, t)$，输出为 $c_s(x, r, t)$。利用自动微分技术，可以精确计算网络输出相对于其输入的导数。

PINN 的总损失函数 $\mathcal{L}$ 通常由几部分构成：
$$ \mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_{\text{pde}} \mathcal{L}_{\text{pde}} + \lambda_{\text{bc}} \mathcal{L}_{\text{bc}} + \lambda_{\text{ic}} \mathcal{L}_{\text{ic}} $$
*   $\mathcal{L}_{\text{data}}$ 是在少量可用测量数据点上的拟合误差（如均方误差）。
*   $\mathcal{L}_{\text{pde}}$ 是在大量[随机采样](@entry_id:175193)的时空点（称为[配置点](@entry_id:169000)）上计算的 PDE 残差的均方值。例如，对于 P2D 模型中的固相扩散方程，残差为 $r_s = \partial_t c_s - D_s \frac{1}{r^2} \partial_r (r^2 \partial_r c_s)$。PINN 会最小化 $|r_s|^2$。
*   $\mathcal{L}_{\text{bc}}$ 和 $\mathcal{L}_{\text{ic}}$ 分别是在边界和初始时刻采样的点上，对边界条件和初始条件的违反程度的惩罚项。例如，颗粒中心的对称性边界条件 $\partial_r c_s(x, r=0, t) = 0$ 可以通过惩罚项 $|\partial_r c_s(x, r=0, t)|^2$ 来施加 。

通过在大量无标签的[配置点](@entry_id:169000)上强制物理定律的满足，PINN 能够利用物理知识来指导学习过程，从而在数据稀疏的情况下获得良好泛化能力的解。

##### 物理启发的先验与约束

除了在模型结构或[损失函数](@entry_id:634569)中融入物理，我们还可以在模型的先验假设或后处理步骤中施加物理约束。

对于[高斯过程](@entry_id:182192)，核函数的选择本身就是一种先验。我们可以设计**非平稳核 (non-stationary kernel)** 来反映电池动态在不同工作区域的变化。例如，电池放电曲线在中间[平台区](@entry_id:753520)非常平滑，但在放电[末期](@entry_id:169480)会急剧下降。一个平稳核假设函数在所有区域具有相同的相关性结构（如光滑度），这与物理现实不符。一个更合理的选择是使用一个在时间维度上非平稳的核，例如，使其[特征长度尺度](@entry_id:266383) (characteristic length scale) 随时间变化——在[平台区](@entry_id:753520)较长，在放电[末期](@entry_id:169480)较短。这使得 GP 能够自适应地捕捉不同放电阶段的不同动态特性 。

对于某些物理量，我们有明确的形状约束，例如，[开路电压 (OCV)](@entry_id:1129139) 随[荷电状态 (SOC)](@entry_id:1132303) 单调变化。即使我们使用了灵活的无约束模型（如 GP），我们仍然可以在得到后验预测后强制施加这种约束。一种严谨的方法是**后验样本投影**。我们可以从无约束的 GP 后验中抽取大量样本函数，然后将每个样本函数投影到满足[单调性](@entry_id:143760)约束的函数空间中。这个投影操作通常是**[保序回归](@entry_id:912334) (isotonic regression)**。理论分析表明，如果真实的函数本身满足约束条件，这个投影步骤不会破坏后验分布的[统计一致性](@entry_id:162814)（即收敛到真值的能力），甚至可能改善其性能 。

### 实际工作流程中的考量

除了模型架构和物理融合，构建一个成功的代理模型工作流程还涉及两个关键的实际问题：如何高效地获取训练数据，以及如何可靠地评估模型性能。

#### [多保真度建模](@entry_id:752240)

生成高质量的 P2D 仿真数据成本高昂。一个有效的策略是**[多保真度建模](@entry_id:752240) (Multi-fidelity modeling)**，它结合了大量廉价的低保真度数据（例如，来自 SPMe 模型）和少量昂贵的高保真度数据（来自 P2D 模型）。

两种主流的多保真度 GP 方法是：

1.  **[残差学习](@entry_id:634200) (Residual Learning)**: 这种方法假设高保真度输出 $y_H(\mathbf{x})$ 与低保真度输出 $y_L(\mathbf{x})$ 之间存在一个加性关系：$y_H(\mathbf{x}) = y_L(\mathbf{x}) + r(\mathbf{x})$。我们首先用大量数据训练一个关于 $y_L$ 的代理模型，然后用少量数据训练另一个模型来学习残差 $r(\mathbf{x})$。当两个模型之间的差异主要是系统性偏差（例如，一个近似恒定的电压偏移）时，这种方法很有效。

2.  **自回归协同克里金 (Autoregressive Co-kriging)**: 这是一个更通用的模型，它假设一个更复杂的关系：$y_H(\mathbf{x}) = \rho \cdot y_L(\mathbf{x}) + \delta(\mathbf{x})$。这里，$\rho$ 是一个学习到的缩放因子，$\delta(\mathbf{x})$ 是一个独立的残差过程。这个模型能够同时捕捉[乘性](@entry_id:187940)差异（通过 $\rho$）和加性差异（通过 $\delta$）。加性[残差学习](@entry_id:634200)可以被看作是自回归模型中 $\rho=1$ 的一个特例。通过从数据中学习 $\rho$，自回归模型可以适应更广泛的保真度间相关性，通常比固定 $\rho=1$ 的[残差学习](@entry_id:634200)更为鲁棒和准确 。

#### 可靠的[模型验证](@entry_id:141140)

评估代理模型的泛化性能至关重要。一个常见的错误是使用**随机样本级 k 折[交叉验证](@entry_id:164650) (random sample-wise k-fold CV)**。对于电池数据这种具有时序结构（例如，[循环老化](@entry_id:1123334)）或分组结构（例如，同一循环内的多个时间点）的数据，这种验证方法是严重错误的 。

问题在于，随机拆分会导致**[数据泄露](@entry_id:260649)**。例如，在循环老化数据中，随机拆分会将来自同一循环的样本同时分到[训练集](@entry_id:636396)和测试集。模型可以在训练集样本中“窥见”并学习到该循环特有的、未被模型显式捕捉的效应（如该循环的[健康状态](@entry_id:1132306) $\theta_i$），然后在[测试集](@entry_id:637546)上做出过于乐观的预测。这导致评估出的[泛化误差](@entry_id:637724)远低于模型在面对一个全新、未见过的循环时的真实误差。

正确的验证方法必须尊重数据的内在结构。对于具有时序依赖的循环数据，应使用**前向链式交叉验证 (forward-chaining CV)** 或称**滚动原点[交叉验证](@entry_id:164650) (rolling-origin CV)**。在这种方案中，[训练集](@entry_id:636396)始终由过去的数据组成，而[测试集](@entry_id:637546)则由紧随其后的未来数据组成。此外，必须按**组（即整个循环）**进行分割，以防止同一循环的数据泄露。例如，一个验证折可能是用第 1 到 10 个循环的数据进行训练，然后在第 11 个循环上进行测试。不正确的验证方法所导致的误差估计偏差可能是显著的，其大小与未被模型捕捉的[组间方差](@entry_id:900909)（如循环间的[健康状态](@entry_id:1132306)方差 $\sigma_{\theta}^2$）成正比，从而对模型的实际性能产生严重误判 。