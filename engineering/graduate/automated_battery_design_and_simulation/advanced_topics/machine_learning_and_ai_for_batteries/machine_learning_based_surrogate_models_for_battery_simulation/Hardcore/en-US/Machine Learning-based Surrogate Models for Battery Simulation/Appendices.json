{
    "hands_on_practices": [
        {
            "introduction": "Before delving into complex neural network surrogates, it is essential to master the fundamentals of linear model reduction. This practice focuses on Principal Component Analysis (PCA), a cornerstone technique for compressing high-dimensional simulation data, such as the electrolyte potential field within a battery. By deriving the reconstruction error bound , you will gain a rigorous understanding of how the choice of model complexity—the number of principal components—directly impacts prediction accuracy and what physical information is lost in the process.",
            "id": "3926888",
            "problem": "A zero-mean electrolyte potential field in a lithium-ion cell electrolyte domain is represented after spatial discretization by a random vector $\\boldsymbol{\\phi}\\in\\mathbb{R}^{M}$ with covariance matrix $\\mathbf{C}=\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]$. Assume $\\mathbf{C}$ is symmetric positive semidefinite with an eigendecomposition $\\mathbf{C}=\\mathbf{U}\\,\\boldsymbol{\\Lambda}\\,\\mathbf{U}^{\\top}$, where $\\mathbf{U}=\\big[\\mathbf{u}_{1},\\dots,\\mathbf{u}_{M}\\big]$ is orthonormal and $\\boldsymbol{\\Lambda}=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{M})$ has eigenvalues ordered $\\lambda_{1}\\ge\\lambda_{2}\\ge\\cdots\\ge\\lambda_{M}\\ge 0$. Consider linear rank-$k$ surrogates for the electrolyte potential field obtained by orthogonal projection onto a $k$-dimensional subspace, and in particular the subspace spanned by the top $k$ eigenvectors (Principal Component Analysis (PCA)). Throughout, use the Euclidean norm and assume the discretization is scaled so that $\\|\\boldsymbol{\\phi}\\|_{2}^{2}$ has units of volt-squared.\n\nTask:\n- Starting only from the definitions of covariance, orthogonal projection, and the spectral theorem, derive an expression for the minimal achievable expected squared reconstruction error $\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]$ among all rank-$k$ linear surrogates, and show that the PCA surrogate attains this minimum. Express the result in terms of the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{M}$. Then, interpret its physical meaning for electrolyte potential fields.\n- For a particular electrolyte dataset discretized to $M\\ge 5$ nodes, the top five eigenvalues (in volt-squared) are measured to be\n$$\n\\lambda_{1}=3.2\\times 10^{-4},\\quad\n\\lambda_{2}=1.6\\times 10^{-4},\\quad\n\\lambda_{3}=8.0\\times 10^{-5},\\quad\n\\lambda_{4}=4.0\\times 10^{-5},\\quad\n\\lambda_{5}=2.0\\times 10^{-5},\n$$\nwith all subsequent eigenvalues negligible for the purpose of this question. If a PCA surrogate with $k=2$ is used, compute the numerical value of the minimal expected squared reconstruction error bound implied by your derivation. Round your answer to four significant figures and express it in volt-squared.",
            "solution": "We begin from the definitions. Let $\\boldsymbol{\\phi}\\in\\mathbb{R}^{M}$ be a zero-mean random vector with covariance $\\mathbf{C}=\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]$. Consider any rank-$k$ linear surrogate formed by an orthogonal projector $\\mathbf{P}$ onto a $k$-dimensional subspace, so that the reconstruction is $\\hat{\\boldsymbol{\\phi}}=\\mathbf{P}\\boldsymbol{\\phi}$ with $\\mathbf{P}^{2}=\\mathbf{P}$ and $\\mathbf{P}^{\\top}=\\mathbf{P}$, and $\\operatorname{rank}(\\mathbf{P})=k$. The expected squared reconstruction error is\n$$\n\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]\n=\\mathbb{E}\\big[\\|(\\mathbf{I}-\\mathbf{P})\\boldsymbol{\\phi}\\|_{2}^{2}\\big]\n=\\mathbb{E}\\big[\\boldsymbol{\\phi}^{\\top}(\\mathbf{I}-\\mathbf{P})^{\\top}(\\mathbf{I}-\\mathbf{P})\\boldsymbol{\\phi}\\big].\n$$\nSince $\\mathbf{P}$ is an orthogonal projector, $(\\mathbf{I}-\\mathbf{P})^{\\top}(\\mathbf{I}-\\mathbf{P})=\\mathbf{I}-\\mathbf{P}$. Using the cyclic property of the trace and the fact that $\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]=\\mathbf{C}$, we have\n$$\n\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]\n=\\mathbb{E}\\big[\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}\\big)\\big]\n=\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]\\big)\n=\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{C}\\big).\n$$\nWrite the spectral decomposition $\\mathbf{C}=\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^{\\top}$ and define $\\tilde{\\mathbf{P}}=\\mathbf{U}^{\\top}\\mathbf{P}\\mathbf{U}$, which is also an orthogonal projector of rank $k$ (congruence preserves idempotency and rank). Then\n$$\n\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{C}\\big)\n=\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^{\\top}\\big)\n=\\operatorname{tr}\\big(\\mathbf{U}^{\\top}(\\mathbf{I}-\\mathbf{P})\\mathbf{U}\\,\\boldsymbol{\\Lambda}\\big)\n=\\operatorname{tr}\\big((\\mathbf{I}-\\tilde{\\mathbf{P}})\\,\\boldsymbol{\\Lambda}\\big).\n$$\nBecause $\\boldsymbol{\\Lambda}$ is diagonal with entries $\\lambda_{1},\\dots,\\lambda_{M}$ and $\\tilde{\\mathbf{P}}$ is a projector of rank $k$, the quantity $\\operatorname{tr}((\\mathbf{I}-\\tilde{\\mathbf{P}})\\boldsymbol{\\Lambda})$ equals the sum of those $\\lambda_{i}$ whose indices correspond to directions excluded by the projector. The minimal value over all rank-$k$ orthogonal projectors is achieved by choosing $\\tilde{\\mathbf{P}}$ to select the $k$ largest diagonal entries of $\\boldsymbol{\\Lambda}$, that is, to project onto the span of the top $k$ eigenvectors. This is a consequence of the rearrangement inequality or equivalently Ky Fan’s maximum principle, which states that for any symmetric matrix $\\mathbf{A}$ with eigenvalues ordered nonincreasingly, the maximum of $\\operatorname{tr}(\\mathbf{P}\\mathbf{A})$ over rank-$k$ orthogonal projectors $\\mathbf{P}$ is $\\sum_{i=1}^{k}\\lambda_{i}(\\mathbf{A})$. Applying this with $\\mathbf{A}=\\mathbf{C}$ gives\n$$\n\\min_{\\substack{\\mathbf{P}^{\\top}=\\mathbf{P}=\\mathbf{P}^{2}\\\\ \\operatorname{rank}(\\mathbf{P})=k}}\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{C}\\big)\n=\\operatorname{tr}(\\mathbf{C})-\\max_{\\mathbf{P}}\\operatorname{tr}(\\mathbf{P}\\mathbf{C})\n=\\Big(\\sum_{i=1}^{M}\\lambda_{i}\\Big)-\\Big(\\sum_{i=1}^{k}\\lambda_{i}\\Big)\n=\\sum_{i=k+1}^{M}\\lambda_{i}.\n$$\nTherefore, the minimal achievable expected squared reconstruction error among all rank-$k$ linear surrogates is\n$$\n\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]_{\\min}=\\sum_{i=k+1}^{M}\\lambda_{i},\n$$\nand the Principal Component Analysis (PCA) surrogate that projects onto the span of the top $k$ eigenvectors attains this minimum. This quantity also serves as an error bound, since any other rank-$k$ linear reconstruction cannot have a smaller expected squared error.\n\nPhysical interpretation for electrolyte potential fields: The eigenvalues $\\{\\lambda_{i}\\}$ are the variances of the random field $\\boldsymbol{\\phi}$ along orthogonal spatial modes (eigenvectors) that capture coherent patterns of electrolyte potential fluctuation across operating conditions. The sum $\\sum_{i=k+1}^{M}\\lambda_{i}$ equals the total variance residing in the neglected modes. Thus, the bound quantifies the average unresolved squared electrolyte potential fluctuation that any rank-$k$ linear surrogate must leave out. In practical terms, it measures the mean-squared magnitude (in volt-squared) of electrolyte potential features—such as fine-scale ohmic drops and tortuosity-induced heterogeneities—not captured by the chosen $k$-dimensional representation.\n\nNow evaluate the bound for the given spectrum when using $k=2$. With\n$$\n\\lambda_{1}=3.2\\times 10^{-4},\\quad\n\\lambda_{2}=1.6\\times 10^{-4},\\quad\n\\lambda_{3}=8.0\\times 10^{-5},\\quad\n\\lambda_{4}=4.0\\times 10^{-5},\\quad\n\\lambda_{5}=2.0\\times 10^{-5},\n$$\nand all subsequent eigenvalues negligible, we have\n$$\n\\sum_{i=k+1}^{M}\\lambda_{i}=\\lambda_{3}+\\lambda_{4}+\\lambda_{5}\n=\\big(8.0\\times 10^{-5}\\big)+\\big(4.0\\times 10^{-5}\\big)+\\big(2.0\\times 10^{-5}\\big)\n=1.4\\times 10^{-4}.\n$$\nRounded to four significant figures and expressed in volt-squared, the minimal expected squared reconstruction error bound is $1.400\\times 10^{-4}$.",
            "answer": "$$\\boxed{1.400 \\times 10^{-4}}$$"
        },
        {
            "introduction": "A powerful surrogate model not only makes accurate predictions but also quantifies its own uncertainty. This practice transitions from deterministic models to probabilistic ones, focusing on Gaussian Process (GP) regression. GPs provide a robust framework for capturing complex, non-linear relationships while also delivering principled confidence intervals for their predictions. This exercise  will guide you through the critical task of propagating this uncertainty from the model's primary outputs ($V$ and $I$) to a derived engineering quantity (power $P=VI$), a skill essential for robust battery management and design.",
            "id": "3926913",
            "problem": "A lithium-ion cell is modeled with a multi-output Gaussian Process Regression (GPR) surrogate that predicts terminal voltage $V$ and discharge current $I$ at an operating condition $x^{\\star}$ defined by temperature, State of Charge (SOC), and current setpoint. By the definition of a Gaussian Process (GP), the finite-dimensional posterior predictive distribution at $x^{\\star}$ is multivariate normal for any set of outputs; in particular, the joint predictive distribution of $(V,I)$ at $x^{\\star}$ is Gaussian with some mean vector and covariance matrix determined by the kernel and the training data. Confidence Interval (CI) construction for a Gaussian random variable uses standard normal quantiles.\n\nAt the specific operating condition $x^{\\star}$, the trained surrogate yields the following posterior predictive summary for $(V,I)$:\n- Mean voltage $\\mu_{V} = 3.7$ and mean current $\\mu_{I} = 2.0$,\n- Variance of voltage $\\sigma_{V}^{2} = (0.05)^{2}$,\n- Variance of current $\\sigma_{I}^{2} = (0.2)^{2}$,\n- Covariance $\\sigma_{VI} = -0.006$.\n\nAssume the joint predictive distribution of $(V,I)$ is Gaussian with these parameters, and that model and observation noise have been appropriately integrated into $\\sigma_{V}^{2}$, $\\sigma_{I}^{2}$, and $\\sigma_{VI}$ by the GPR posterior. The derived quantity of interest is power $P = VI$. Starting from the Gaussian Process definition and the properties of multivariate normal moments, derive the approximate two-sided $95\\%$ CI for $P$ by:\n1. Deriving the first two moments of $P$ from the joint Gaussian predictive distribution of $(V,I)$,\n2. Approximating the distribution of $P$ by a normal distribution with these moments,\n3. Using the standard normal quantile $z_{0.975} \\approx 1.96$ to construct a symmetric two-sided confidence interval.\n\nCompute the two-sided $95\\%$ CI half-width for $P$ under this approximation and express your final numerical answer in watts (W). Round your answer to four significant figures.",
            "solution": "The problem states that the joint posterior predictive distribution of terminal voltage $V$ and discharge current $I$ at a specific operating condition $x^{\\star}$ is a bivariate Gaussian. We can represent the random vector $\\mathbf{X} = \\begin{pmatrix} V \\\\ I \\end{pmatrix}$ as follows:\n$$\n\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n$$\nwhere the mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ are given by the problem statement:\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\mu_V \\\\ \\mu_I \\end{pmatrix} = \\begin{pmatrix} 3.7 \\\\ 2.0 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_V^2 & \\sigma_{VI} \\\\ \\sigma_{VI} & \\sigma_I^2 \\end{pmatrix} = \\begin{pmatrix} (0.05)^2 & -0.006 \\\\ -0.006 & (0.2)^2 \\end{pmatrix} = \\begin{pmatrix} 0.0025 & -0.006 \\\\ -0.006 & 0.04 \\end{pmatrix}\n$$\nThe derived quantity of interest is the power, $P = VI$. We are tasked with finding an approximate $95\\%$ confidence interval (CI) for $P$. This will be done by first deriving the exact first two moments of $P$ (mean and variance), then approximating the distribution of $P$ as a normal distribution.\n\n**1. Derivation of the Moments of Power ($P$)**\n\n**Mean of $P$:**\nThe expected value of the product of two random variables is given by the product of their means plus their covariance.\n$$\n\\mu_P = E[P] = E[VI] = E[V]E[I] + \\text{Cov}(V, I) = \\mu_V \\mu_I + \\sigma_{VI}\n$$\nSubstituting the given values:\n$$\n\\mu_P = (3.7)(2.0) + (-0.006) = 7.4 - 0.006 = 7.394\n$$\nThe mean power is $7.394$ W.\n\n**Variance of $P$:**\nThe variance of the product of two correlated normal variables is given by the formula:\n$$\n\\sigma_P^2 = \\mu_V^2 \\sigma_I^2 + \\mu_I^2 \\sigma_V^2 + 2\\mu_V \\mu_I \\sigma_{VI} + \\sigma_V^2 \\sigma_I^2 + \\sigma_{VI}^2\n$$\nThe derivation from first principles using Isserlis' theorem gives the same result:\n$\\sigma_P^2 = E[(VI)^2] - (E[VI])^2$. With $E[VI] = \\mu_V\\mu_I + \\sigma_{VI}$ and $E[V^2I^2] = E[V^2]E[I^2] + 2(E[VI])^2 - 2(E[V]E[I])^2 = (\\sigma_V^2+\\mu_V^2)(\\sigma_I^2+\\mu_I^2) + 2(\\mu_V\\mu_I+\\sigma_{VI})^2 - 2(\\mu_V\\mu_I)^2$, substitution and algebraic simplification leads to the same formula above.\n\nNow we can substitute the numerical values:\n$\\mu_V = 3.7$, $\\mu_I = 2.0$, $\\sigma_V^2 = 0.0025$, $\\sigma_I^2 = 0.04$, $\\sigma_{VI} = -0.006$.\n$$\n\\sigma_P^2 = (3.7)^2(0.04) + (2.0)^2(0.0025) + 2(3.7)(2.0)(-0.006) + (0.0025)(0.04) + (-0.006)^2\n$$\n$$\n\\sigma_P^2 = (13.69)(0.04) + (4)(0.0025) - 0.0888 + 0.0001 + 0.000036\n$$\n$$\n\\sigma_P^2 = 0.5476 + 0.01 - 0.0888 + 0.0001 + 0.000036\n$$\n$$\n\\sigma_P^2 = 0.468936\n$$\nThe standard deviation of $P$ is:\n$$\n\\sigma_P = \\sqrt{0.468936} \\approx 0.684789 \\text{ W}\n$$\n\n**2. Normal Approximation for the Distribution of $P$**\n\nThe distribution of the product of two Gaussian random variables is not strictly Gaussian. However, for the purpose of constructing an approximate confidence interval, we approximate the distribution of $P$ by a normal distribution with the moments we just derived:\n$$\nP \\approx \\mathcal{N}(\\mu_P, \\sigma_P^2) \\quad \\text{where} \\quad \\mu_P = 7.394, \\quad \\sigma_P^2 = 0.468936\n$$\n\n**3. Construction of the $95\\%$ Confidence Interval**\n\nA two-sided $95\\%$ confidence interval for a normally distributed variable is constructed using the mean and standard deviation, along with the appropriate quantile from the standard normal distribution. For a $95\\%$ CI, the total probability in the tails is $\\alpha = 1 - 0.95 = 0.05$, so we use the quantile $z_{1-\\alpha/2} = z_{0.975}$. The problem provides the value $z_{0.975} \\approx 1.96$.\n\nThe CI is given by $\\mu_P \\pm z_{0.975} \\sigma_P$. The half-width (HW) of this interval is:\n$$\nHW = z_{0.975} \\sigma_P\n$$\nUsing our calculated values:\n$$\nHW = 1.96 \\times \\sqrt{0.468936} \\approx 1.96 \\times 0.684789 \\approx 1.3421864 \\text{ W}\n$$\nThe problem asks for the numerical answer to be rounded to four significant figures.\n$$\nHW \\approx 1.342 \\text{ W}\n$$\nThus, the approximate $95\\%$ CI for the power is $7.394 \\pm 1.342$ W. The computed half-width is $1.342$ W.",
            "answer": "$$\n\\boxed{1.342}\n$$"
        },
        {
            "introduction": "The frontier of surrogate modeling lies in seamlessly integrating physical laws with the flexibility of deep learning. This practice explores Physics-Informed Neural Networks (PINNs), which embed governing equations directly into the training process to create more accurate and generalizable models. You will tackle the advanced concept of enforcing a global conservation law—the conservation of total lithium—which is an integral constraint rather than a local partial differential equation. This exercise  challenges you to reason about how such a fundamental physical constraint can dramatically improve a model's performance on downstream prediction tasks, illustrating the core power of the physics-informed approach.",
            "id": "3926983",
            "problem": "A Physics-Informed Neural Network (PINN) is trained as a surrogate for a porous-electrode lithium-ion cell, with two network outputs: the solid-phase lithium concentration $c_{s}(\\mathbf{x}, t; \\boldsymbol{\\theta}_{s})$ defined on the solid domain $\\Omega_{s}$ (volume $V_{s}$), and the electrolyte-phase lithium concentration $c_{e}(\\mathbf{y}, t; \\boldsymbol{\\theta}_{e})$ defined on the electrolyte domain $\\Omega_{e}$ (volume $V_{e}$). Assume a closed cell with no side reactions and negligible loss of cyclable lithium. By conservation of mass, the total lithium inventory is conserved for all times $t$:\n$$\n\\int_{\\Omega_{s}} c_{s}(\\mathbf{x}, t)\\, dV + \\int_{\\Omega_{e}} c_{e}(\\mathbf{y}, t)\\, dV = M_{\\text{tot}} \\quad \\text{for all } t,\n$$\nwhere $M_{\\text{tot}}$ is a constant. The PINN is trained with a loss that contains data misfit and Partial Differential Equation (PDE) residual terms plus boundary and interface terms. You are asked to enforce the global integral conservation law in the PINN and to quantify its effect on generalization of a downstream voltage predictor.\n\nConsider a downstream linear voltage surrogate defined by\n$$\nV(t) = \\alpha\\, I_{s}(t) + \\beta\\, I_{e}(t) + \\gamma,\n$$\nwhere $I_{s}(t) = \\int_{\\Omega_{s}} c_{s}(\\mathbf{x}, t)\\, dV$ and $I_{e}(t) = \\int_{\\Omega_{e}} c_{e}(\\mathbf{y}, t)\\, dV$, with constants $\\alpha$, $\\beta$, and $\\gamma$. Suppose after training, the PINN’s integral conservation residual $r(t) \\equiv I_{s}(t) + I_{e}(t) - M_{\\text{tot}}$ satisfies $\\sup_{t} |r(t)| \\le \\varepsilon$ for some small $\\varepsilon > 0$, and the PINN’s integral prediction errors $e_{s}(t) \\equiv \\widehat{I}_{s}(t) - I_{s}(t)$ and $e_{e}(t) \\equiv \\widehat{I}_{e}(t) - I_{e}(t)$ on unseen current profiles are zero-mean, with variances $\\sigma_{s}^{2}$ and $\\sigma_{e}^{2}$, respectively. Assume independence of $e_{s}(t)$ and $e_{e}(t)$ in the unconstrained case, and that the conservation enforcement couples these errors so that $e_{e}(t) \\approx -e_{s}(t) + \\delta(t)$ with $|\\delta(t)| \\le \\varepsilon$.\n\nWhich option correctly specifies a global PINN constraint that enforces integral lithium conservation and the resulting test-time upper bound on the expected squared voltage error for the downstream predictor under the stated assumptions?\n\nA. Add a soft global constraint term to the PINN loss\n$$\n\\mathcal{L}_{\\text{cons}} = \\frac{1}{T}\\int_{0}^{T} \\left(I_{s}(t) + I_{e}(t) - M_{\\text{tot}}\\right)^{2}\\, dt,\n$$\nwhere $I_{s}(t) = \\int_{\\Omega_{s}} c_{s}(\\mathbf{x}, t)\\, dV$ and $I_{e}(t) = \\int_{\\Omega_{e}} c_{e}(\\mathbf{y}, t)\\, dV$, whose functional gradient with respect to $c_{s}$ and $c_{e}$ couples the two outputs through the scalar residual $I_{s}(t) + I_{e}(t) - M_{\\text{tot}}$. Under the given assumptions, the test-time expected squared voltage error satisfies\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] \\le (\\alpha - \\beta)^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\varepsilon^{2},\n$$\nwhile in the unconstrained case\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] = \\alpha^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\sigma_{e}^{2}.\n$$\n\nB. Add a local pointwise constraint\n$$\n\\mathcal{L}_{\\text{local}} = \\int_{\\Omega_{s}\\cup \\Omega_{e}} \\left(c_{s}(\\mathbf{z}, t) + c_{e}(\\mathbf{z}, t) - \\frac{M_{\\text{tot}}}{V_{s}+V_{e}}\\right)^{2}\\, dV,\n$$\nand assume the downstream expected squared voltage error bound becomes\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] \\le (\\alpha + \\beta)^{2}\\,\\sigma_{s}^{2}.\n$$\n\nC. Introduce a hard Lagrange multiplier $\\lambda(t)$ to enforce conservation exactly via\n$$\n\\mathcal{L}_{\\text{aug}} = \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{BC}} + \\mathcal{L}_{\\text{data}} + \\int_{0}^{T}\\lambda(t)\\,\\big(I_{s}(t) + I_{e}(t) - M_{\\text{tot}}\\big)\\, dt,\n$$\nand claim the downstream expected squared voltage error is unchanged by the constraint:\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] = \\alpha^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\sigma_{e}^{2}.\n$$\n\nD. Penalize conservation at endpoints only\n$$\n\\mathcal{L}_{\\text{end}} = \\big(I_{s}(0) + I_{e}(0) - M_{\\text{tot}}\\big)^{2} + \\big(I_{s}(T) + I_{e}(T) - M_{\\text{tot}}\\big)^{2},\n$$\nand conclude that the constraint yields no improvement in the downstream expected squared voltage error:\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] = \\alpha^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\sigma_{e}^{2}.\n$$",
            "solution": "Let's analyze the problem by deriving the expected squared voltage error in both the unconstrained and constrained cases. The voltage error is $\\Delta V(t) = \\widehat{V}(t) - V(t) = \\alpha(\\widehat{I}_s(t) - I_s(t)) + \\beta(\\widehat{I}_e(t) - I_e(t)) = \\alpha e_s(t) + \\beta e_e(t)$.\n\n**Unconstrained Case:**\nThe problem states that $e_s(t)$ and $e_e(t)$ are independent and zero-mean. The expected squared error is:\n$$ \\mathbb{E}[(\\Delta V(t))^2] = \\mathbb{E}[(\\alpha e_s(t) + \\beta e_e(t))^2] = \\mathbb{E}[\\alpha^2 e_s(t)^2 + \\beta^2 e_e(t)^2 + 2\\alpha\\beta e_s(t)e_e(t)] $$\nBy linearity of expectation and independence, $\\mathbb{E}[e_s(t)e_e(t)] = \\mathbb{E}[e_s(t)]\\mathbb{E}[e_e(t)] = 0$.\n$$ \\mathbb{E}[(\\Delta V(t))^2] = \\alpha^2 \\mathbb{E}[e_s(t)^2] + \\beta^2 \\mathbb{E}[e_e(t)^2] = \\alpha^2\\sigma_s^2 + \\beta^2\\sigma_e^2 $$\nThis matches the unconstrained error expression in option A.\n\n**Constrained Case:**\nThe conservation law for the true quantities is $I_s(t) + I_e(t) = M_{\\text{tot}}$. The PINN's predictions $\\widehat{I}_s(t)$ and $\\widehat{I}_e(t)$ have a residual $\\widehat{r}(t) = \\widehat{I}_s(t) + \\widehat{I}_e(t) - M_{\\text{tot}}$. The problem states that after training, the prediction errors are coupled such that $e_e(t) \\approx -e_s(t) + \\delta(t)$ with $|\\delta(t)| \\le \\varepsilon$. This relation arises because $e_s(t) + e_e(t) = (\\widehat{I}_s - I_s) + (\\widehat{I}_e - I_e) = (\\widehat{I}_s + \\widehat{I}_e) - (I_s + I_e) = \\widehat{r}(t) + M_{\\text{tot}} - M_{\\text{tot}} = \\widehat{r}(t)$, so $\\delta(t)$ represents the PINN's conservation residual.\n\nSubstitute this into the voltage error:\n$$ \\Delta V(t) = \\alpha e_s(t) + \\beta(-e_s(t) + \\delta(t)) = (\\alpha - \\beta)e_s(t) + \\beta\\delta(t) $$\nThe expected squared error is:\n$$ \\mathbb{E}[(\\Delta V(t))^2] = \\mathbb{E}[((\\alpha - \\beta)e_s(t) + \\beta\\delta(t))^2] = (\\alpha-\\beta)^2\\mathbb{E}[e_s^2] + \\beta^2\\mathbb{E}[\\delta^2] + 2\\beta(\\alpha-\\beta)\\mathbb{E}[e_s\\delta] $$\nAssuming the model's conservation residual $\\delta(t)$ is uncorrelated with the error in one of its components $e_s(t)$, the cross-term $\\mathbb{E}[e_s\\delta]$ is zero because $e_s(t)$ is zero-mean. This simplifies the expression to:\n$$ \\mathbb{E}[(\\Delta V(t))^2] = (\\alpha-\\beta)^2\\sigma_s^2 + \\beta^2\\mathbb{E}[\\delta(t)^2] $$\nUsing the given bound $|\\delta(t)| \\le \\varepsilon$, we have $\\mathbb{E}[\\delta(t)^2] \\le \\varepsilon^2$. This yields the upper bound:\n$$ \\mathbb{E}[(\\Delta V(t))^2] \\le (\\alpha - \\beta)^2\\sigma_s^2 + \\beta^2\\varepsilon^2 $$\nThis derivation matches the constrained error bound in option A.\n\n**Evaluating the Constraint Formulation:**\n- **Option A:** Proposes a soft constraint by penalizing the mean-squared integral of the conservation residual over time. This is a standard and physically correct way to enforce such a global law in a PINN framework.\n- **Option B:** Proposes a pointwise constraint. This is physically incorrect as the concentrations exist on separate domains and have no reason to sum to a constant locally.\n- **Option C:** Proposes a hard constraint via a Lagrange multiplier. While a valid technique, it incorrectly claims the downstream error is unchanged. The constraint fundamentally alters the error structure.\n- **Option D:** Proposes penalizing the residual only at the start and end times. This is insufficient to enforce a law that must hold for all time.\n\nTherefore, option A provides both a correct method for enforcing the constraint and a correctly derived analysis of its effect on the downstream prediction error.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}