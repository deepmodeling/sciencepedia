{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of any physical model is dimensional homogeneity: every term in a valid equation must have the same physical units. Before attempting to discover complex governing equations from data, it is crucial to have a mechanism to enforce this fundamental principle. This exercise, , guides you in building an automated checker for dimensional consistency, a vital tool for filtering out physically nonsensical models generated during a data-driven search. This practice reinforces the importance of using foundational physical laws as powerful constraints in the modeling process.",
            "id": "3904049",
            "problem": "You are tasked with building a unit-consistency checker to validate candidate governing equations discovered by data-driven methods within automated battery design and simulation. The goal is to ensure that every additive term on the right-hand side of an equation matches the dimensions of the left-hand side and that any special function constraints are satisfied. Your program must be a single, complete, runnable script and must produce a single-line output in the specified format.\n\nFundamental base. The checker must operate from the following base, which is limited to universally accepted laws and definitions:\n- Dimensional analysis based on the seven International System of Units (SI) base dimensions reduced to the six actually needed in this context: mass $M$ (kilogram, $kg$), length $L$ (meter, $m$), time $T$ (second, $s$), electric current $I$ (ampere, $A$), thermodynamic temperature $\\Theta$ (kelvin, $K$), and amount of substance $N$ (mole, $mol$).\n- Operator dimensions:\n  - Spatial gradient $\\nabla$ contributes a factor of $L^{-1}$.\n  - Spatial divergence $\\nabla \\cdot$ contributes a factor of $L^{-1}$.\n  - Spatial Laplacian $\\nabla^{2}$ contributes a factor of $L^{-2}$.\n  - Time derivative $\\partial/\\partial t$ contributes a factor of $T^{-1}$.\n- Special function constraints:\n  - The exponential function $\\exp(\\cdot)$ and the natural logarithm $\\log(\\cdot)$ require a dimensionless argument.\n  - Absolute value $|\\cdot|$ preserves dimensions.\n  - Norm squared $|\\cdot|^{2}$ raises the dimensional exponents by $2$.\n  - Raising a quantity with dimensions to a real power is permitted; the dimensional exponents are scaled by the exponent.\n\nVariable and parameter dimensional assignments. Use the following consistent and scientifically grounded units for common battery model quantities:\n- Electrolyte concentration $c$: $N L^{-3}$ ($mol \\, m^{-3}$).\n- Electric potential $\\phi$: $M L^{2} T^{-3} I^{-1}$ ($V$).\n- Current density $i$: $I L^{-2}$ ($A \\, m^{-2}$).\n- Diffusion coefficient $D$: $L^{2} T^{-1}$ ($m^{2} \\, s^{-1}$).\n- Reaction rate (molar flux) $j$: $N L^{-2} T^{-1}$ ($mol \\, m^{-2} \\, s^{-1}$).\n- Faraday constant $F$: $I T N^{-1}$ ($C \\, mol^{-1}$).\n- Specific interfacial area $a$: $L^{-1}$ ($m^{2} \\, m^{-3}$).\n- Electrolyte electrical conductivity $\\kappa$: $M^{-1} L^{-3} T^{3} I^{2}$ ($S \\, m^{-1}$).\n- Solid electrical conductivity $\\sigma$: $M^{-1} L^{-3} T^{3} I^{2}$ ($S \\, m^{-1}$).\n- Thermal conductivity $k$: $M L T^{-3} \\Theta^{-1}$ ($W \\, m^{-1} \\, K^{-1}$).\n- Mass density $\\rho$: $M L^{-3}$ ($kg \\, m^{-3}$).\n- Specific heat capacity $c_{p}$: $L^{2} T^{-2} \\Theta^{-1}$ ($J \\, kg^{-1} \\, K^{-1}$).\n- Temperature $T$: $\\Theta$ ($K$).\n- Universal gas constant $R$: $M L^{2} T^{-2} N^{-1} \\Theta^{-1}$ ($J \\, mol^{-1} \\, K^{-1}$).\n- Activation energy $E_{a}$: $M L^{2} T^{-2} N^{-1}$ ($J \\, mol^{-1}$).\n- Dimensionless porosity $\\varepsilon$: dimensionless $M^{0} L^{0} T^{0} I^{0} \\Theta^{0} N^{0}$.\n- A first-order surface reaction rate constant $k_{0}$: $L T^{-1}$ ($m \\, s^{-1}$).\n\nDimension-composition rules. You must implement algebra on exponents of $(M, L, T, I, \\Theta, N)$:\n- Multiplication adds exponent vectors.\n- Division subtracts exponent vectors.\n- Addition requires all addends to have identical exponent vectors; the result has that same exponent vector.\n- A time derivative multiplies by $T^{-1}$; a spatial gradient multiplies by $L^{-1}$; a spatial divergence multiplies by $L^{-1}$; a spatial Laplacian multiplies by $L^{-2}$.\n- The exponential and logarithm require dimensionless input and produce dimensionless output.\n\nProgram objective. Your program must:\n- Encode a small expression algebra supporting variables, constants, and the operators above.\n- For each candidate equation, compute dimensions of the left-hand side and each right-hand term independently.\n- Return a boolean indicating whether all right-hand terms match the left-hand side dimensions and all special function constraints are satisfied.\n\nPhysical and numerical units. This problem is purely dimensional; no numeric values are requested. Booleans are unitless and should be returned as $true$ or $false$ values in programming language terms.\n\nTest suite. Check the following nine cases, each presented as an equation with a left-hand side and one or more right-hand terms. For each case, return $true$ if the equation is dimensionally consistent under the rules above and $false$ otherwise.\n\n- Case $1$ (species diffusion, consistent):\n  - Left: $\\dfrac{\\partial c}{\\partial t}$.\n  - Right terms: $D \\, \\nabla^{2} c$.\n\n- Case $2$ (species diffusion, inconsistent extra term):\n  - Left: $\\dfrac{\\partial c}{\\partial t}$.\n  - Right terms: $D \\, \\nabla^{2} c$, $\\kappa \\, \\nabla^{2} \\phi$.\n\n- Case $3$ (electrolyte charge conservation, consistent):\n  - Left: $\\nabla \\cdot i$.\n  - Right terms: $a \\, F \\, j$.\n\n- Case $4$ (electrolyte charge conservation, inconsistent missing specific area):\n  - Left: $\\nabla \\cdot i$.\n  - Right terms: $F \\, j$.\n\n- Case $5$ (energy balance with Joule heating, consistent):\n  - Left: $\\rho \\, c_{p} \\, \\dfrac{\\partial T}{\\partial t}$.\n  - Right terms: $k \\, \\nabla^{2} T$, $\\sigma \\, \\left| \\nabla \\phi \\right|^{2}$.\n\n- Case $6$ (energy balance, inconsistent Joule term form):\n  - Left: $\\rho \\, c_{p} \\, \\dfrac{\\partial T}{\\partial t}$.\n  - Right terms: $k \\, \\nabla^{2} T$, $\\sigma \\, \\nabla \\phi$.\n\n- Case $7$ (Arrhenius-type surface reaction rate, consistent):\n  - Left: $j$.\n  - Right terms: $k_{0} \\, \\exp\\!\\left( - \\dfrac{E_{a}}{R \\, T} \\right) \\, c$.\n\n- Case $8$ (Arrhenius-type surface reaction rate, inconsistent exponent argument):\n  - Left: $j$.\n  - Right terms: $k_{0} \\, \\exp\\!\\left( - \\dfrac{E_{a}}{T} \\right) \\, c$.\n\n- Case $9$ (dimensionless multiplier preserves consistency):\n  - Left: $\\dfrac{\\partial c}{\\partial t}$.\n  - Right terms: $\\varepsilon \\, \\dfrac{\\partial c}{\\partial t}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the nine cases in the order listed above (for example, $[true,false,true,\\dots]$). In Python, use the built-in boolean literals without changing case, so the output should look like $[True,False,True,\\dots]$.",
            "solution": "The process of validating a candidate governing equation for dimensional consistency is a fundamental step in physics and engineering, ensuring that the mathematical model respects the principle of dimensional homogeneity. This principle states that an equation can only be physically meaningful if all its additive terms have the same physical dimensions. The solution involves implementing a computational framework to perform this validation automatically based on a set of predefined rules for physical quantities and mathematical operators.\n\nThe core of the methodology is to represent the dimension of any physical quantity as a vector of exponents corresponding to a set of fundamental base dimensions. For this problem, the six base dimensions are mass ($M$), length ($L$), time ($T$), electric current ($I$), thermodynamic temperature ($\\Theta$), and amount of substance ($N$). Thus, any dimension can be represented by a $6$-element vector of real numbers, $[d_M, d_L, d_T, d_I, d_\\Theta, d_N]$. For instance, the electrolyte concentration $c$, with units of $mol \\, m^{-3}$ or dimensions $N L^{-3}$, is represented by the vector $[0, -3, 0, 0, 0, 1]$. A dimensionless quantity has the exponent vector $[0, 0, 0, 0, 0, 0]$.\n\nThe algebra of dimensional analysis is then mapped to vector operations:\n1.  **Multiplication**: When two quantities are multiplied ($A \\cdot B$), their dimensional vectors are added. If $\\dim(A) = \\vec{v}_A$ and $\\dim(B) = \\vec{v}_B$, then $\\dim(A \\cdot B) = \\vec{v}_A + \\vec{v}_B$.\n2.  **Division**: When one quantity is divided by another ($A / B$), their dimensional vectors are subtracted: $\\dim(A / B) = \\vec{v}_A - \\vec{v}_B$.\n3.  **Powers**: Raising a quantity to a power $p$ ($A^p$) scales its dimensional vector by $p$: $\\dim(A^p) = p \\cdot \\vec{v}_A$.\n4.  **Addition/Subtraction**: For terms to be added or subtracted ($A + B$), they must be dimensionally homogeneous, i.e., $\\dim(A) = \\dim(B)$. The result has the same dimension.\n5.  **Operators**: Differential operators also contribute to the dimensions. A time derivative $\\partial/\\partial t$ introduces a dimension of $T^{-1}$ (vector $[0, 0, -1, 0, 0, 0]$), and spatial operators like gradient ($\\nabla$) or divergence ($\\nabla \\cdot$) introduce $L^{-1}$ (vector $[0, -1, 0, 0, 0, 0]$). The Laplacian ($\\nabla^2$) introduces $L^{-2}$ (vector $[0, -2, 0, 0, 0, 0]$).\n6.  **Special Functions**: Transcendental functions like $\\exp(\\cdot)$ and $\\log(\\cdot)$ are only defined for dimensionless arguments. The function and its argument must both be dimensionless.\n\nTo validate an equation of the form $LHS = T_1 + T_2 + \\dots + T_n$, we perform the following steps:\n1.  Compute the dimensional vector for the left-hand side, $\\vec{d}_{LHS}$.\n2.  For each term $T_i$ on the right-hand side, compute its dimensional vector, $\\vec{d}_{T_i}$. During this computation, all constraints on special function arguments must be verified. If any argument (e.g., of an $\\exp$ function) is found to be dimensional, the entire equation is declared invalid immediately.\n3.  Compare the dimensional vector of each right-hand side term to the left-hand side's vector. The equation is valid if and only if $\\vec{d}_{LHS} = \\vec{d}_{T_1} = \\vec{d}_{T_2} = \\dots = \\vec{d}_{T_n}$ and all special function constraints are met.\n\nLet us illustrate with two test cases:\n\n**Case 1 (Consistent): $\\dfrac{\\partial c}{\\partial t} = D \\, \\nabla^{2} c$**\n- Dimensions of variables: $\\dim(c) = N L^{-3} \\implies [0, -3, 0, 0, 0, 1]$, $\\dim(D) = L^{2} T^{-1} \\implies [0, 2, -1, 0, 0, 0]$.\n- Dimensions of operators: $\\dim(\\partial/\\partial t) = T^{-1} \\implies [0, 0, -1, 0, 0, 0]$, $\\dim(\\nabla^2) = L^{-2} \\implies [0, -2, 0, 0, 0, 0]$.\n- Left-Hand Side (LHS): $\\dim(\\frac{\\partial c}{\\partial t}) = \\dim(c) + \\dim(\\frac{\\partial}{\\partial t}) = [0, -3, 0, 0, 0, 1] + [0, 0, -1, 0, 0, 0] = [0, -3, -1, 0, 0, 1]$.\n- Right-Hand Side (RHS): $\\dim(D \\nabla^2 c) = \\dim(D) + \\dim(\\nabla^2) + \\dim(c) = [0, 2, -1, 0, 0, 0] + [0, -2, 0, 0, 0, 0] + [0, -3, 0, 0, 0, 1] = [0, -3, -1, 0, 0, 1]$.\n- Since $\\dim(LHS) = \\dim(RHS)$, the equation is dimensionally consistent ($True$).\n\n**Case 8 (Inconsistent): $j = k_{0} \\, \\exp\\!\\left( - \\dfrac{E_{a}}{T} \\right) \\, c$**\n- This equation involves a special function, $\\exp(\\cdot)$, whose argument must be dimensionless.\n- Dimensions of variables: $\\dim(E_a) = M L^2 T^{-2} N^{-1} \\implies [1, 2, -2, 0, 0, -1]$, $\\dim(T_{temp}) = \\Theta \\implies [0, 0, 0, 0, 1, 0]$.\n- Argument of $\\exp$: $\\dim(\\frac{E_a}{T_{temp}}) = \\dim(E_a) - \\dim(T_{temp}) = [1, 2, -2, 0, 0, -1] - [0, 0, 0, 0, 1, 0] = [1, 2, -2, 0, -1, -1]$.\n- The resulting vector is not the zero vector $[0, 0, 0, 0, 0, 0]$, so the argument is not dimensionless.\n- The constraint for the exponential function is violated. Therefore, the equation is dimensionally invalid ($False$), and no further checks are necessary.\n\nThis systematic procedure is implemented for all nine test cases to produce the final boolean results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the dimensional analysis problem by defining a checker class,\n    running through the test cases, and printing the results.\n    \"\"\"\n\n    class DimensionalityChecker:\n        \"\"\"\n        A class to perform dimensional analysis on physical equations.\n        It represents dimensions as 6D vectors (M, L, T, I, Theta, N)\n        and checks for consistency based on predefined rules.\n        \"\"\"\n        def __init__(self):\n            # Base dimensions are represented by a 6-element numpy array corresponding\n            # to the exponents of Mass, Length, Time, Current, Temperature, and Moles.\n            # M, L, T, I, Theta, N\n            self.dims = {\n                'c':      np.array([0, -3,  0,  0,  0,  1]), # N L^-3\n                'phi':    np.array([1,  2, -3, -1,  0,  0]), # M L^2 T^-3 I^-1\n                'i':      np.array([0, -2,  0,  1,  0,  0]), # I L^-2\n                'D':      np.array([0,  2, -1,  0,  0,  0]), # L^2 T^-1\n                'j':      np.array([0, -2, -1,  0,  0,  1]), # N L^-2 T^-1\n                'F':      np.array([0,  0,  1,  1,  0, -1]), # I T N^-1\n                'a':      np.array([0, -1,  0,  0,  0,  0]), # L^-1\n                'kappa':  np.array([-1,-3,  3,  2,  0,  0]), # M^-1 L^-3 T^3 I^2\n                'sigma':  np.array([-1,-3,  3,  2,  0,  0]), # M^-1 L^-3 T^3 I^2\n                'k':      np.array([1,  1, -3,  0, -1,  0]), # M L T^-3 Theta^-1\n                'rho':    np.array([1, -3,  0,  0,  0,  0]), # M L^-3\n                'cp':     np.array([0,  2, -2,  0, -1,  0]), # L^2 T^-2 Theta^-1\n                'T_':     np.array([0,  0,  0,  0,  1,  0]), # Theta (Temp)\n                'R':      np.array([1,  2, -2,  0, -1, -1]), # M L^2 T^-2 N^-1 Theta^-1\n                'Ea':     np.array([1,  2, -2,  0,  0, -1]), # M L^2 T^-2 N^-1\n                'eps':    np.array([0,  0,  0,  0,  0,  0]), # dimensionless\n                'k0':     np.array([0,  1, -1,  0,  0,  0]), # L T^-1\n            }\n            self.op_dims = {\n                'dt':        np.array([0,  0, -1,  0,  0,  0]), # T^-1\n                'grad_div':  np.array([0, -1,  0,  0,  0,  0]), # L^-1\n                'laplacian': np.array([0, -2,  0,  0,  0,  0]), # L^-2\n            }\n            self.DIMENSIONLESS = np.array([0, 0, 0, 0, 0, 0])\n\n        def get_dim(self, key):\n            \"\"\"Returns a copy of the dimensional vector for a variable or operator.\"\"\"\n            if key in self.dims:\n                return self.dims[key].copy()\n            return self.op_dims[key].copy()\n\n        def is_dimensionless(self, dim_vector):\n            \"\"\"Checks if a dimensional vector corresponds to a dimensionless quantity.\"\"\"\n            return np.array_equal(dim_vector, self.DIMENSIONLESS)\n\n        def check_equation(self, lhs_calc, rhs_calcs):\n            \"\"\"\n            Checks a single equation for dimensional consistency.\n\n            Args:\n                lhs_calc (callable): A function that computes the LHS dimensions.\n                rhs_calcs (list[callable]): A list of functions for each RHS term.\n\n            Returns:\n                bool: True if the equation is consistent, False otherwise.\n            \"\"\"\n            try:\n                lhs_dim = lhs_calc()\n                if lhs_dim is None: return False\n\n                for calc in rhs_calcs:\n                    rhs_dim = calc()\n                    if rhs_dim is None or not np.array_equal(lhs_dim, rhs_dim):\n                        return False\n                return True\n            except Exception:\n                return False\n\n    checker = DimensionalityChecker()\n    results = []\n\n    # Case 1: ∂c/∂t = D ∇²c\n    case1_lhs = lambda: checker.get_dim('c') + checker.get_dim('dt')\n    case1_rhs1 = lambda: checker.get_dim('D') + checker.get_dim('laplacian') + checker.get_dim('c')\n    results.append(checker.check_equation(case1_lhs, [case1_rhs1]))\n\n    # Case 2: ∂c/∂t = D ∇²c + κ ∇²ϕ\n    case2_rhs2 = lambda: checker.get_dim('kappa') + checker.get_dim('laplacian') + checker.get_dim('phi')\n    results.append(checker.check_equation(case1_lhs, [case1_rhs1, case2_rhs2]))\n\n    # Case 3: ∇⋅i = a F j\n    case3_lhs = lambda: checker.get_dim('grad_div') + checker.get_dim('i')\n    case3_rhs1 = lambda: checker.get_dim('a') + checker.get_dim('F') + checker.get_dim('j')\n    results.append(checker.check_equation(case3_lhs, [case3_rhs1]))\n\n    # Case 4: ∇⋅i = F j\n    case4_rhs1 = lambda: checker.get_dim('F') + checker.get_dim('j')\n    results.append(checker.check_equation(case3_lhs, [case4_rhs1]))\n\n    # Case 5: ρ cp ∂T/∂t = k ∇²T + σ |∇ϕ|²\n    case5_lhs = lambda: checker.get_dim('rho') + checker.get_dim('cp') + checker.get_dim('dt') + checker.get_dim('T_')\n    case5_rhs1 = lambda: checker.get_dim('k') + checker.get_dim('laplacian') + checker.get_dim('T_')\n    def case5_rhs2():\n        grad_phi_dim = checker.get_dim('grad_div') + checker.get_dim('phi')\n        norm_sq_grad_phi_dim = grad_phi_dim * 2\n        return checker.get_dim('sigma') + norm_sq_grad_phi_dim\n    results.append(checker.check_equation(case5_lhs, [case5_rhs1, case5_rhs2]))\n\n    # Case 6: ρ cp ∂T/∂t = k ∇²T + σ ∇ϕ\n    case6_rhs2 = lambda: checker.get_dim('sigma') + checker.get_dim('grad_div') + checker.get_dim('phi')\n    results.append(checker.check_equation(case5_lhs, [case5_rhs1, case6_rhs2]))\n\n    # Case 7: j = k₀ exp(-Eₐ/(RT)) c\n    case7_lhs = lambda: checker.get_dim('j')\n    def case7_rhs1():\n        exp_arg_dim = checker.get_dim('Ea') - (checker.get_dim('R') + checker.get_dim('T_'))\n        if not checker.is_dimensionless(exp_arg_dim):\n            return None  # Invalid operation\n        return checker.get_dim('k0') + checker.DIMENSIONLESS + checker.get_dim('c')\n    results.append(checker.check_equation(case7_lhs, [case7_rhs1]))\n\n    # Case 8: j = k₀ exp(-Eₐ/T) c\n    def case8_rhs1():\n        exp_arg_dim = checker.get_dim('Ea') - checker.get_dim('T_')\n        if not checker.is_dimensionless(exp_arg_dim):\n            return None  # Invalid operation\n        return checker.get_dim('k0') + checker.DIMENSIONLESS + checker.get_dim('c')\n    results.append(checker.check_equation(case7_lhs, [case8_rhs1]))\n\n    # Case 9: ∂c/∂t = ε ∂c/∂t\n    case9_rhs1 = lambda: checker.get_dim('eps') + checker.get_dim('dt') + checker.get_dim('c')\n    results.append(checker.check_equation(case1_lhs, [case9_rhs1]))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful data-driven model must be more than just a good fit to the data; its parameters must be physically plausible and its form must be suitable for subsequent use in simulations. This practice, , tackles the discovery of a diffusion coefficient $D$, where simple regression might yield unphysical negative values or estimates that lead to unstable numerical schemes. You will explore how to bake constraints like positivity ($D > 0$) and numerical stability directly into the estimation process, demonstrating a powerful method for producing models that are not only accurate but also robust and usable.",
            "id": "3904028",
            "problem": "You are tasked with designing and implementing a data-driven estimator for a one-dimensional diffusion coefficient in the context of automated battery design and simulation, with explicit enforcement of physical positivity and numerical stability constraints. The fundamental base of this problem consists of Fick's second law of diffusion and the explicit forward-Euler finite difference stability condition for the parabolic diffusion equation.\n\nFundamental laws and definitions:\n- Fick's second law in one spatial dimension with spatially constant diffusion coefficient: $$\\frac{\\partial c}{\\partial t}(x,t) = D \\frac{\\partial^2 c}{\\partial x^2}(x,t),$$ where $c(x,t)$ is concentration, and $D$ is the constant diffusion coefficient. Physical realism requires $D \\ge 0$.\n- For the explicit forward Euler method with second-order central differencing applied to the one-dimensional diffusion (heat) equation on a uniform grid of spacing $dx$, the classical stability condition is $$\\Delta t \\le \\frac{dx^2}{2 D}.$$ This condition arises from the Von Neumann stability analysis and is widely accepted.\n\nSynthetic data generation protocol:\n- Consider a one-dimensional domain of length $L$ with a uniform grid of $N$ points and a snapshot at time $t_0$. Let the true concentration field be $$c(x,t_0) = \\sin\\left(\\frac{\\pi x}{L}\\right) \\exp\\left(- D_{\\text{true}} \\left(\\frac{\\pi}{L}\\right)^2 t_0 \\right).$$\n- Then $$\\frac{\\partial^2 c}{\\partial x^2}(x,t_0) = -\\left(\\frac{\\pi}{L}\\right)^2 \\sin\\left(\\frac{\\pi x}{L}\\right) \\exp\\left(- D_{\\text{true}} \\left(\\frac{\\pi}{L}\\right)^2 t_0 \\right),$$ and $$\\frac{\\partial c}{\\partial t}(x,t_0) = D_{\\text{true}} \\frac{\\partial^2 c}{\\partial x^2}(x,t_0).$$\n- Construct observation pairs $(x_i, y_i)$ at grid points, where $x_i$ represents noisy measurements of $\\frac{\\partial^2 c}{\\partial x^2}(x_i,t_0)$ in units of $1/\\mathrm{m}^2$, and $y_i$ represents noisy measurements of $\\frac{\\partial c}{\\partial t}(x_i,t_0)$ in units of $1/\\mathrm{s}$. Add independent zero-mean Gaussian noise with specified standard deviations to both derivatives.\n\nEstimation tasks:\n- Goal is to discover the governing equation coefficient $D$ from noisy linear relation $y \\approx D x$ using regression, while ensuring physical positivity $D \\ge 0$ and explicit-scheme stability under a prescribed time step $\\Delta t_0$ and grid spacing $dx$.\n- Implement three estimators:\n  1. Unconstrained regression: minimize $$\\sum_i \\left(y_i - D x_i \\right)^2$$ over $D \\in \\mathbb{R}$ to obtain the ordinary least squares estimate.\n  2. Positivity-constrained regression via reparameterization: let $D = \\exp(\\theta)$, and minimize $$\\sum_i \\left(y_i - \\exp(\\theta) x_i \\right)^2$$ over $\\theta \\in \\mathbb{R}$, thereby enforcing $D > 0$.\n  3. Stability-constrained regression using the same reparameterization $D = \\exp(\\theta)$ and augment the objective with a penalty term that discourages violation of the explicit stability condition for the given $\\Delta t_0$ and $dx$. Define $D_{\\max} = \\frac{dx^2}{2 \\Delta t_0}$, and minimize $$\\sum_i \\left(y_i - \\exp(\\theta) x_i \\right)^2 + \\lambda \\left(\\max\\left(0, \\exp(\\theta) - D_{\\max}\\right)\\right)^2,$$ over $\\theta \\in \\mathbb{R}$, where $\\lambda$ is a positive penalty weight.\n\nOutputs to compute for each test case:\n- The unconstrained estimate $D_{\\text{unc}}$ in $\\mathrm{m}^2/\\mathrm{s}$.\n- The positivity-reparameterized estimate $D_{\\text{pos}}$ in $\\mathrm{m}^2/\\mathrm{s}$.\n- The stability-constrained estimate $D_{\\text{stab}}$ in $\\mathrm{m}^2/\\mathrm{s}$.\n- The root-mean-square error (RMSE) for each estimator, defined as $$\\sqrt{\\frac{1}{M} \\sum_i \\left(y_i - D x_i \\right)^2},$$ in $1/\\mathrm{s}$, where $M$ is the number of measurements.\n- A boolean stability flag for each estimator indicating whether the explicit scheme with the given $\\Delta t_0$ would be stable, defined as $\\text{stable}(D) = ( D > 0 ) \\wedge ( \\Delta t_0 \\le \\frac{dx^2}{2 D} )$.\n\nPhysical and numerical units:\n- Report all diffusion coefficients $D$ in $\\mathrm{m}^2/\\mathrm{s}$.\n- Report all RMSE values in $1/\\mathrm{s}$.\n- Angles appearing in trigonometric functions are in radians.\n\nTest suite:\nProvide three test cases with scientifically plausible parameters:\n- Case $1$: $L = 1.0$ meters, $N = 101$, $D_{\\text{true}} = 1.0 \\times 10^{-10}$ $\\mathrm{m}^2/\\mathrm{s}$, $t_0 = 1000.0$ seconds, $\\Delta t_0 = 100.0$ seconds, noise standard deviations $\\sigma_{x} = 1.0 \\times 10^{-12}$ $1/\\mathrm{m}^2$, $\\sigma_{y} = 1.0 \\times 10^{-12}$ $1/\\mathrm{s}$, penalty weight $\\lambda = 1.0$.\n- Case $2$: $L = 1.0$ meters, $N = 51$, $D_{\\text{true}} = 5.0 \\times 10^{-9}$ $\\mathrm{m}^2/\\mathrm{s}$, $t_0 = 1000.0$ seconds, $\\Delta t_0 = 50000.0$ seconds, noise standard deviations $\\sigma_{x} = 5.0 \\times 10^{-4}$ $1/\\mathrm{m}^2$, $\\sigma_{y} = 2.0 \\times 10^{-8}$ $1/\\mathrm{s}$, penalty weight $\\lambda = 1.0$.\n- Case $3$: $L = 1.0$ meters, $N = 5$, $D_{\\text{true}} = 1.0 \\times 10^{-11}$ $\\mathrm{m}^2/\\mathrm{s}$, $t_0 = 10.0$ seconds, $\\Delta t_0 = 10.0$ seconds, noise standard deviations $\\sigma_{x} = 5.0 \\times 10^{-1}$ $1/\\mathrm{m}^2$, $\\sigma_{y} = 5.0 \\times 10^{-10}$ $1/\\mathrm{s}$, penalty weight $\\lambda = 1.0$.\n\nProgram requirements:\n- Implement the synthetic data generation exactly as specified, then fit all three estimators for each case and compute the requested outputs.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case output must be a bracketed comma-separated list of the following $9$ entries in order: $D_{\\text{unc}}$, $D_{\\text{pos}}$, $D_{\\text{stab}}$, $\\text{RMSE}_{\\text{unc}}$, $\\text{RMSE}_{\\text{pos}}$, $\\text{RMSE}_{\\text{stab}}$, $\\text{stable}(D_{\\text{unc}})$, $\\text{stable}(D_{\\text{pos}})$, $\\text{stable}(D_{\\text{stab}})$. For example, an output with three cases should look like $$\\left[ [d_{1},d_{2},d_{3},r_{1},r_{2},r_{3},b_{1},b_{2},b_{3}], [\\cdots], [\\cdots] \\right],$$ with numerical values replacing the placeholders. All diffusion coefficients must be in $\\mathrm{m}^2/\\mathrm{s}$ and RMSE values in $1/\\mathrm{s}$.",
            "solution": "The objective is to estimate the diffusion coefficient $D$ from noisy data by solving a regression problem derived from Fick's second law, $\\frac{\\partial c}{\\partial t} = D \\frac{\\partial^2 c}{\\partial x^2}$. We are given observation pairs $(x_i, y_i)$, which are noisy measurements of the second spatial derivative and the time derivative, respectively. The underlying model is linear: $y_i \\approx D x_i$. We will implement and compare three estimators for $D$.\n\n**1. Unconstrained Estimator ($D_{\\text{unc}}$)**\n\nThis estimator is the ordinary least squares (OLS) solution for $D$. It minimizes the sum of squared residuals, $J(D) = \\sum_{i} (y_i - D x_i)^2$. The analytical solution, which does not enforce any physical constraints, is given by:\n$$D_{\\text{unc}} = \\frac{\\sum_{i} x_i y_i}{\\sum_{i} x_i^2}$$\nThis estimate can potentially be negative if the data is very noisy, which is physically unrealistic.\n\n**2. Positivity-Constrained Estimator ($D_{\\text{pos}}$)**\n\nTo enforce the physical constraint $D > 0$, we reparameterize the diffusion coefficient as $D = \\exp(\\theta)$, where $\\theta$ can be any real number. The optimization problem becomes finding the $\\theta$ that minimizes the nonlinear least squares objective:\n$$J(\\theta) = \\sum_{i} (y_i - \\exp(\\theta) x_i)^2$$\nThis problem is solved numerically using an optimization algorithm like BFGS. The resulting estimate, $D_{\\text{pos}} = \\exp(\\theta^*)$, where $\\theta^*$ is the optimal value, is guaranteed to be positive.\n\n**3. Stability-Constrained Estimator ($D_{\\text{stab}}$)**\n\nThis estimator incorporates both the positivity constraint and the numerical stability condition for a given time step $\\Delta t_0$ and grid spacing $dx$. The stability condition is $D \\le D_{\\max}$, where $D_{\\max} = \\frac{dx^2}{2 \\Delta t_0}$. The objective function is augmented with a penalty term that becomes active if $D$ exceeds $D_{\\max}$. Using the same reparameterization $D = \\exp(\\theta)$, we minimize:\n$$J(\\theta) = \\sum_{i} (y_i - \\exp(\\theta) x_i)^2 + \\lambda \\left(\\max\\left(0, \\exp(\\theta) - D_{\\max}\\right)\\right)^2$$\nThe parameter $\\lambda > 0$ controls the strength of the penalty. This nonlinear optimization problem is also solved numerically. The resulting estimate, $D_{\\text{stab}} = \\exp(\\theta^*)$, is not only positive but is also discouraged from taking values that would lead to an unstable explicit simulation.\n\n**4. Performance Metrics**\n\nFor each estimate $D \\in \\{D_{\\text{unc}}, D_{\\text{pos}}, D_{\\text{stab}}\\}$, we compute two metrics:\n-   **Root-Mean-Square Error (RMSE)**: Measures the goodness of fit to the noisy data.\n    $$\\text{RMSE}(D) = \\sqrt{\\frac{1}{M} \\sum_{i=1}^{M} (y_i - D x_i)^2}$$\n-   **Stability Flag**: A boolean indicating if the estimate $D$ would lead to a stable explicit simulation with time step $\\Delta t_0$.\n    $$\\text{stable}(D) = (D > 0) \\wedge (\\Delta t_0 \\le \\frac{dx^2}{2 D})$$\n\nThese calculations are performed for each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the data-driven diffusion coefficient estimation problem for a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"L\": 1.0, \"N\": 101, \"D_true\": 1.0e-10, \"t0\": 1000.0, \"delta_t0\": 100.0,\n            \"sigma_x\": 1.0e-12, \"sigma_y\": 1.0e-12, \"lambda_\": 1.0, \"seed\": 0\n        },\n        {\n            \"L\": 1.0, \"N\": 51, \"D_true\": 5.0e-9, \"t0\": 1000.0, \"delta_t0\": 50000.0,\n            \"sigma_x\": 5.0e-4, \"sigma_y\": 2.0e-8, \"lambda_\": 1.0, \"seed\": 1\n        },\n        {\n            \"L\": 1.0, \"N\": 5, \"D_true\": 1.0e-11, \"t0\": 10.0, \"delta_t0\": 10.0,\n            \"sigma_x\": 5.0e-1, \"sigma_y\": 5.0e-10, \"lambda_\": 1.0, \"seed\": 2\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters\n        L, N, D_true, t0 = case['L'], case['N'], case['D_true'], case['t0']\n        delta_t0, sigma_x, sigma_y = case['delta_t0'], case['sigma_x'], case['sigma_y']\n        lambda_ = case['lambda_']\n        \n        # Set seed for reproducible noise\n        np.random.seed(case['seed'])\n        \n        # 1. Synthetic Data Generation\n        dx = L / (N - 1)\n        grid_points = np.linspace(0, L, N)\n        \n        # Calculate true derivatives\n        k_sq = (np.pi / L)**2\n        exp_term = np.exp(-D_true * k_sq * t0)\n        sin_term = np.sin(np.pi * grid_points / L)\n        \n        d2c_dx2_true = -k_sq * sin_term * exp_term\n        dc_dt_true = D_true * d2c_dx2_true\n        \n        # Add Gaussian noise\n        x_data = d2c_dx2_true + np.random.normal(0, sigma_x, size=N)\n        y_data = dc_dt_true + np.random.normal(0, sigma_y, size=N)\n\n        # 2. Estimator 1: Unconstrained Regression (OLS)\n        sum_xy = np.dot(x_data, y_data)\n        sum_x2 = np.dot(x_data, x_data)\n        D_unc = sum_xy / sum_x2 if sum_x2 != 0 else 0.0\n\n        # 3. Estimator 2: Positivity-Constrained Regression\n        def objective_pos(theta):\n            D = np.exp(theta[0])\n            residuals = y_data - D * x_data\n            return np.sum(residuals**2)\n        \n        def jacobian_pos(theta):\n            D = np.exp(theta[0])\n            # dJ/d(theta) = -2 * exp(theta) * sum((y - exp(theta)*x)*x)\n            grad = -2.0 * D * np.sum((y_data - D * x_data) * x_data)\n            return np.array([grad])\n        \n        # Use log of D_unc if positive, else 0 as initial guess for theta\n        theta0 = np.log(D_unc) if D_unc > 1e-20 else 0.0\n        res_pos = minimize(objective_pos, x0=[theta0], method='BFGS', jac=jacobian_pos)\n        D_pos = np.exp(res_pos.x[0])\n        \n        # 4. Estimator 3: Stability-Constrained Regression\n        D_max = dx**2 / (2.0 * delta_t0)\n        \n        def objective_stab(theta):\n            D = np.exp(theta[0])\n            residuals = y_data - D * x_data\n            lsq_term = np.sum(residuals**2)\n            penalty_term = lambda_ * (max(0, D - D_max))**2\n            return lsq_term + penalty_term\n\n        def jacobian_stab(theta):\n            D = np.exp(theta[0])\n            grad_lsq = -2.0 * D * np.sum((y_data - D * x_data) * x_data)\n            \n            if D > D_max:\n                grad_penalty = 2.0 * lambda_ * (D - D_max) * D\n            else:\n                grad_penalty = 0.0\n            \n            return np.array([grad_lsq + grad_penalty])\n        \n        # Use the same initial guess\n        res_stab = minimize(objective_stab, x0=[theta0], method='BFGS', jac=jacobian_stab)\n        D_stab = np.exp(res_stab.x[0])\n\n        # 5. Compute Outputs\n        def calculate_rmse(D_est, x, y):\n            residuals = y - D_est * x\n            return np.sqrt(np.mean(residuals**2))\n\n        def check_stability(D_est, dt, dx_val):\n            if D_est = 0:\n                return False\n            # original: dt = dx^2 / (2*D). Rearrange to avoid division by D.\n            return 2.0 * D_est * dt = dx_val**2\n        \n        rmse_unc = calculate_rmse(D_unc, x_data, y_data)\n        rmse_pos = calculate_rmse(D_pos, x_data, y_data)\n        rmse_stab = calculate_rmse(D_stab, x_data, y_data)\n\n        stable_unc = check_stability(D_unc, delta_t0, dx)\n        stable_pos = check_stability(D_pos, delta_t0, dx)\n        stable_stab = check_stability(D_stab, delta_t0, dx)\n        \n        case_results = [\n            D_unc, D_pos, D_stab,\n            rmse_unc, rmse_pos, rmse_stab,\n            stable_unc, stable_pos, stable_stab\n        ]\n        results.append(case_results)\n\n    # Format output as a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Discovering a governing equation from experimental data is incomplete without an honest assessment of the uncertainty in its discovered coefficients. In this exercise, , you will implement a complete, modern pipeline to discover the coefficients of a reaction-diffusion PDE and, crucially, to quantify their uncertainty. By generating synthetic data, performing regression on numerically computed derivatives, and using bootstrap resampling to construct confidence intervals, you will gain hands-on experience with a powerful workflow for robust scientific discovery.",
            "id": "3904092",
            "problem": "You are tasked with implementing a rigorous, data-driven pipeline to estimate and quantify uncertainty in coefficients of a governing Partial Differential Equation (PDE) for a homogenized one-dimensional lithium concentration field in a porous electrode, suitable for automated battery design and simulation. The scientific base for this task is conservation of species mass and Fickian diffusion. Specifically, assume that the lithium concentration field $c(x,t)$ on a one-dimensional periodic domain of length $L$ obeys conservation of mass with flux $J$ and a linear volumetric reaction sink:\n$$\\frac{\\partial c}{\\partial t} + \\frac{\\partial J}{\\partial x} = R,$$\nwith Fickian flux $J = -D \\frac{\\partial c}{\\partial x}$ and a linear reaction $R = -k c$, where $D$ is the diffusion coefficient and $k$ is a first-order reaction rate constant. Under these assumptions, the governing PDE is\n$$\\frac{\\partial c}{\\partial t} = D \\frac{\\partial^2 c}{\\partial x^2} - k c.$$\n\nYou will generate synthetic, physically plausible data by solving the above PDE analytically for a periodic domain, starting from a superposition of sinusoidal Fourier modes. Let\n$$c(x,0) = c_0 + A_1 \\sin\\left(\\frac{2\\pi x}{L}\\right) + A_2 \\sin\\left(\\frac{4\\pi x}{L}\\right),$$\nand for $t \\ge 0$, the exact solution is\n$$c(x,t) = c_0 e^{-k t} + A_1 \\exp\\left(-\\left[k + D\\left(\\frac{2\\pi}{L}\\right)^2\\right] t\\right) \\sin\\left(\\frac{2\\pi x}{L}\\right) + A_2 \\exp\\left(-\\left[k + D\\left(\\frac{4\\pi}{L}\\right)^2\\right] t\\right) \\sin\\left(\\frac{4\\pi x}{L}\\right).$$\n\nTo mimic measurement noise, add independent, zero-mean Gaussian noise with standard deviation $\\sigma$ to $c(x,t)$ at every space–time grid point. Use a uniform spatial grid with $N_x$ points and spacing $\\Delta x = L/N_x$, and uniform temporal sampling with $N_t$ points and time step $\\Delta t$. Use periodic boundary conditions in space when approximating spatial derivatives. Compute discrete approximations of the time derivative and the second spatial derivative as\n$$c_t(x_i,t_j) \\approx \\frac{c(x_i,t_{j+1}) - c(x_i,t_j)}{\\Delta t}, \\quad c_{xx}(x_i,t_j) \\approx \\frac{c(x_{i-1},t_j) - 2 c(x_i,t_j) + c(x_{i+1},t_j)}{\\Delta x^2},$$\nwhere spatial indices are interpreted modulo $N_x$ due to periodicity.\n\nRestrict the discovery candidate library to the two functions $\\{c_{xx},\\, c\\}$ and perform linear regression of the form\n$$c_t = \\Theta \\, \\xi,$$\nwhere $\\Theta = [\\, c_{xx},\\; c\\,]$ and $\\xi = [\\, D,\\; -k\\,]^\\top$. Use ordinary least squares to estimate $\\hat{\\xi}$.\n\nTo quantify uncertainty, perform nonparametric bootstrapping by resampling space–time patches with replacement. A space–time patch is an axis-aligned subarray of the $(x,t)$ grid of size $p_x \\times p_t$, collected over contiguous spatial indices (with periodic wrap) and contiguous time indices. For each bootstrap replicate, sample $M$ patches with replacement, concatenate their flattened data vectors to form the regression data, estimate $\\hat{\\xi}^{(b)}$, and repeat for $B$ replicates. Use the empirical percentile method to form two-sided $95\\%$ Confidence Intervals (CI) for $D$ and $k$ based on the bootstrap distribution of the corresponding estimates. Report point estimates from the full (non-resampled) set of $M$ patches.\n\nAll quantities must be expressed in the International System of Units (SI units): report $D$ in $\\mathrm{m^2/s}$ and $k$ in $\\mathrm{s^{-1}}$. Angles do not appear and no angle unit is needed. Do not use percentages in the output; represent all CI bounds as floating-point numbers.\n\nImplement a single program that:\n- Generates the synthetic dataset,\n- Constructs discrete derivative approximations,\n- Defines and samples space–time patches,\n- Performs bootstrap resampling of patches,\n- Estimates the coefficients and $95\\%$ Confidence Intervals.\n\nUse the following test suite. For each test case, you must use the specified parameters. In all cases, use $c_0 = 1000$ (in $\\mathrm{mol/m^3}$), $A_1 = 50$ (in $\\mathrm{mol/m^3}$), and $A_2 = 30$ (in $\\mathrm{mol/m^3}$), unless otherwise stated. The spatial domain length is $L = 10^{-3}$ (in $\\mathrm{m}$). For periodicity, interpret indices modulo $N_x$ in space.\n\n- Test case $1$ (general case):\n    - $D = 5 \\times 10^{-12}$, $k = 10^{-3}$, $L = 10^{-3}$, $N_x = 128$, $N_t = 120$, $\\Delta t = 0.5$, $\\sigma = 10^{-6}$, $p_x = 16$, $p_t = 20$, $M = 50$, $B = 200$.\n- Test case $2$ (noise-free boundary):\n    - $D = 5 \\times 10^{-12}$, $k = 10^{-3}$, $L = 10^{-3}$, $N_x = 128$, $N_t = 120$, $\\Delta t = 0.5$, $\\sigma = 0$, $p_x = 32$, $p_t = 30$, $M = 40$, $B = 150$.\n- Test case $3$ (higher reaction rate and higher noise):\n    - $D = 5 \\times 10^{-12}$, $k = 2 \\times 10^{-3}$, $L = 10^{-3}$, $N_x = 96$, $N_t = 100$, $\\Delta t = 0.5$, $\\sigma = 5 \\times 10^{-6}$, $p_x = 12$, $p_t = 16$, $M = 60$, $B = 200$.\n- Test case $4$ (small dataset and weaker diffusion/reaction):\n    - $D = 10^{-12}$, $k = 5 \\times 10^{-4}$, $L = 10^{-3}$, $N_x = 64$, $N_t = 80$, $\\Delta t = 0.5$, $\\sigma = 10^{-6}$, $p_x = 8$, $p_t = 10$, $M = 12$, $B = 60$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must yield a list of six floating-point numbers in the order\n$$[\\, \\hat{D},\\; \\hat{k},\\; D_{\\mathrm{low}},\\; D_{\\mathrm{high}},\\; k_{\\mathrm{low}},\\; k_{\\mathrm{high}}\\,],$$\nwhere $\\hat{D}$ and $\\hat{k}$ are the point estimates (in $\\mathrm{m^2/s}$ and $\\mathrm{s^{-1}}$), and $(D_{\\mathrm{low}}, D_{\\mathrm{high}})$ and $(k_{\\mathrm{low}}, k_{\\mathrm{high}})$ are the lower and upper bounds of the two-sided $95\\%$ Confidence Intervals. The final output must therefore be a list of four such lists, one per test case, for example\n$$[ [d_1,k_1,d_{1,\\ell},d_{1,u},k_{1,\\ell},k_{1,u}], [d_2,k_2,d_{2,\\ell},d_{2,u},k_{2,\\ell},k_{2,u}], [d_3,k_3,d_{3,\\ell},d_{3,u},k_{3,\\ell},k_{3,u}], [d_4,k_4,d_{4,\\ell},d_{4,u},k_{4,\\ell},k_{4,u}] ].$$",
            "solution": "The design proceeds from the conservation of species mass and Fickian diffusion, combined with linear reaction kinetics. In one dimension, the conservation statement is\n$$\\frac{\\partial c}{\\partial t} + \\frac{\\partial J}{\\partial x} = R,$$\nwhere $c(x,t)$ is the concentration of intercalated lithium, $J(x,t)$ is the spatial flux, and $R(x,t)$ is the volumetric reaction term. Under Fickian diffusion, the flux is $J = -D \\frac{\\partial c}{\\partial x}$, where $D$ is the diffusivity. Under small departures from equilibrium, first-order kinetics approximates the volumetric reaction as $R = -k c$, where $k$ is a rate constant. Substituting these constitutive relations yields the governing Partial Differential Equation (PDE)\n$$\\frac{\\partial c}{\\partial t} = D \\frac{\\partial^2 c}{\\partial x^2} - k c.$$\n\nThis PDE is linear with constant coefficients, and with periodic boundary conditions it admits a Fourier-mode solution. If the initial condition is a superposition of sinusoidal modes,\n$$c(x,0) = c_0 + A_1 \\sin\\left(\\frac{2\\pi x}{L}\\right) + A_2 \\sin\\left(\\frac{4\\pi x}{L}\\right),$$\nthen each Fourier mode decays independently. The zero mode decays as $c_0 e^{-k t}$, while the sine modes decay as\n$$A_n \\exp\\left(-\\left[k + D\\left(\\frac{2\\pi n}{L}\\right)^2\\right] t\\right) \\sin\\left(\\frac{2\\pi n x}{L}\\right),$$\nfor $n \\in \\{1,2\\}$. Therefore, the exact solution is\n$$c(x,t) = c_0 e^{-k t} + A_1 \\exp\\left(-\\left[k + D\\left(\\frac{2\\pi}{L}\\right)^2\\right] t\\right) \\sin\\left(\\frac{2\\pi x}{L}\\right) + A_2 \\exp\\left(-\\left[k + D\\left(\\frac{4\\pi}{L}\\right)^2\\right] t\\right) \\sin\\left(\\frac{4\\pi x}{L}\\right).$$\n\nTo emulate realistic data acquisition, independent zero-mean Gaussian noise of standard deviation $\\sigma$ is added to every space–time sample of $c(x,t)$. The domain is discretized using a uniform spatial grid of $N_x$ points with spacing $\\Delta x = L/N_x$ and periodic indexing, and a uniform temporal grid of $N_t$ points with time step $\\Delta t$. Using finite differences, the time derivative and second spatial derivative are approximated by\n$$c_t(x_i,t_j) \\approx \\frac{c(x_i,t_{j+1}) - c(x_i,t_j)}{\\Delta t},$$\n$$c_{xx}(x_i,t_j) \\approx \\frac{c(x_{i-1},t_j) - 2 c(x_i,t_j) + c(x_{i+1},t_j)}{\\Delta x^2},$$\nwith $i \\in \\{0,1,\\dots,N_x-1\\}$ and $j \\in \\{0,1,\\dots,N_t-2\\}$, and spatial indices evaluated modulo $N_x$ due to periodic boundaries.\n\nFor data-driven discovery, we restrict the candidate library of terms to $\\{c_{xx},\\, c\\}$ and postulate a linear regression relation\n$$c_t = \\Theta \\, \\xi,$$\nwhere $\\Theta = [\\, c_{xx},\\; c\\,]$ and the coefficient vector $\\xi = [\\, \\xi_1,\\; \\xi_2\\,]^\\top$ maps to physics as $\\xi_1 = D$ and $\\xi_2 = -k$. The ordinary least squares estimator is obtained by minimizing\n$$\\min_{\\xi} \\| \\Theta \\xi - y \\|_2^2,$$\nwith $y$ denoting the vector of $c_t$ samples. The solution is the Moore–Penrose least squares estimate\n$$\\hat{\\xi} = (\\Theta^\\top \\Theta)^{-1} \\Theta^\\top y,$$\nassuming $\\Theta^\\top \\Theta$ is nonsingular, which is generically satisfied for sufficiently diverse patches.\n\nTo model spatial and temporal correlations realistically and to obtain valid uncertainty quantification, we apply nonparametric bootstrapping at the level of space–time patches. A patch is defined as an axis-aligned subarray of the $(x,t)$ grid of size $p_x \\times p_t$, constructed from contiguous spatial indices (with periodic wrap) and contiguous time indices. For each bootstrap replicate $b \\in \\{1,2,\\dots,B\\}$, we sample $M$ patches with replacement from the available patch set, concatenate the flattened patch data for $c_t$, $c_{xx}$, and $c$ to assemble the regression matrix $\\Theta^{(b)}$ and vector $y^{(b)}$, and compute $\\hat{\\xi}^{(b)}$ by least squares. This produces bootstrap samples $\\{\\hat{\\xi}^{(b)}\\}_{b=1}^B$. Using the empirical percentile method, the two-sided $95\\%$ Confidence Interval (CI) for $D$ is the interval between the $2.5$th and $97.5$th percentiles of $\\{ \\hat{\\xi}_1^{(b)} \\}$, and the CI for $k$ is derived from $\\{ -\\hat{\\xi}_2^{(b)} \\}$ analogously. The point estimate $(\\hat{D}, \\hat{k})$ is computed from the full, non-resampled set of $M$ patches.\n\nAlgorithmic steps for each test case are as follows:\n- Fix $D$, $k$, $L$, $N_x$, $N_t$, $\\Delta t$, $\\sigma$, $p_x$, $p_t$, $M$, and $B$ as specified.\n- Construct $x_i = i \\Delta x$ for $i \\in \\{0,\\dots,N_x-1\\}$ and $t_j = j \\Delta t$ for $j \\in \\{0,\\dots,N_t-1\\}$.\n- Evaluate $c(x_i,t_j)$ using the exact formula, then add independent Gaussian noise with standard deviation $\\sigma$.\n- Compute discrete arrays for $c_t(x_i,t_j)$ on $j \\in \\{0,\\dots,N_t-2\\}$ and $c_{xx}(x_i,t_j)$ on the same temporal indices, and collect $c(x_i,t_j)$ on $j \\in \\{0,\\dots,N_t-2\\}$ to align with $c_t$.\n- Sample $M$ space–time patches uniformly at random: choose spatial start indices $s_x \\in \\{0,\\dots,N_x-1\\}$ (with periodic wrap) and temporal start indices $s_t \\in \\{0,\\dots,N_t-2-p_t+1\\}$, extract $p_x \\times p_t$ blocks, and flatten.\n- Compute the full-data least squares estimate $(\\hat{D}, \\hat{k})$ by solving $y = \\Theta \\xi$, remembering that $\\xi_2 = -k$.\n- Perform $B$ bootstrap replicates by resampling $M$ patches with replacement, solving least squares for each, and forming empirical percentile $95\\%$ CIs for $D$ and $k$.\n- Return $[\\, \\hat{D},\\; \\hat{k},\\; D_{\\mathrm{low}},\\; D_{\\mathrm{high}},\\; k_{\\mathrm{low}},\\; k_{\\mathrm{high}}\\,]$ in SI units.\n\nThe test suite includes four cases that collectively probe the algorithm’s behavior under a variety of conditions: a general “happy path” case with moderate noise, a noise-free boundary case to test consistency, a higher reaction rate with higher noise to test robustness, and a small dataset case to test performance under limited data. The final output is a single line containing a list of the four result lists, ordered by the test case definitions. No additional text should be printed.",
            "answer": "```python\nimport numpy as np\n\n# Set a global random seed for reproducibility\nnp.random.seed(123)\n\ndef generate_concentration(D, k, L, Nx, Nt, dt, c0=1000.0, A1=50.0, A2=30.0):\n    \"\"\"\n    Generate the exact solution c(x,t) for the diffusion-reaction PDE on a periodic domain,\n    using the specified initial condition composed of sine modes and a constant offset.\n    Returns an array C of shape (Nt, Nx).\n    \"\"\"\n    x = np.linspace(0.0, L, Nx, endpoint=False)  # spatial grid\n    t = dt * np.arange(Nt)                       # time grid\n\n    # Wavenumbers for the sine modes\n    k1 = 2.0 * np.pi / L\n    k2 = 4.0 * np.pi / L\n\n    # Mode decay rates\n    decay0 = np.exp(-k * t)  # zero mode\n    decay1 = np.exp(-(k + D * (k1 ** 2)) * t)\n    decay2 = np.exp(-(k + D * (k2 ** 2)) * t)\n\n    # Construct solution: broadcast over x\n    C = (\n        (c0 * decay0)[:, None]\n        + (A1 * decay1)[:, None] * np.sin(k1 * x)[None, :]\n        + (A2 * decay2)[:, None] * np.sin(k2 * x)[None, :]\n    )\n    return C\n\ndef add_noise(C, sigma):\n    \"\"\"\n    Add independent Gaussian noise with standard deviation sigma to the concentration data.\n    \"\"\"\n    if sigma = 0.0:\n        return C.copy()\n    noise = np.random.normal(loc=0.0, scale=sigma, size=C.shape)\n    return C + noise\n\ndef compute_derivatives(C, L, dt):\n    \"\"\"\n    Compute discrete time derivative c_t and second spatial derivative c_xx with periodic wrapping.\n    c_t is computed using forward differences over time.\n    c_xx is computed using central differences in space (periodic).\n    Returns (ct, cxx, c_slice) where ct and cxx have shape (Nt-1, Nx) and c_slice aligns with ct in time.\n    \"\"\"\n    Nt, Nx = C.shape\n    dx = L / Nx\n\n    # Time derivative using forward difference: shape (Nt-1, Nx)\n    ct = (C[1:, :] - C[:-1, :]) / dt\n\n    # Second spatial derivative at times 0..Nt-2 to align with ct\n    C_time = C[:-1, :]  # shape (Nt-1, Nx)\n    # Periodic roll for spatial neighbors\n    C_left = np.roll(C_time, shift=1, axis=1)\n    C_right = np.roll(C_time, shift=-1, axis=1)\n\n    cxx = (C_left - 2.0 * C_time + C_right) / (dx ** 2)\n\n    # c aligned with ct in time\n    c_slice = C_time.copy()\n\n    return ct, cxx, c_slice\n\ndef sample_patches(ct, cxx, c_slice, px, pt, M):\n    \"\"\"\n    Sample M space-time patches with replacement.\n    Spatial dimension wraps periodically; time does not wrap.\n    Returns a list of patches, each patch is a tuple (y_flat, Theta_flat)\n    where y_flat is ct flattened and Theta_flat has columns [cxx_flat, c_flat].\n    \"\"\"\n    T, Nx = ct.shape  # T = Nt-1\n    patches = []\n\n    # Valid time start indices to fit pt within T\n    max_t_start = T - pt\n    if max_t_start  0:\n        raise ValueError(\"Patch time size exceeds available time samples.\")\n\n    for _ in range(M):\n        s_t = np.random.randint(0, max_t_start + 1)\n        s_x = np.random.randint(0, Nx)  # start in space; will wrap\n\n        # Spatial indices with periodic wrapping\n        idx = (s_x + np.arange(px)) % Nx\n\n        # Extract blocks and flatten in row-major\n        ct_patch = ct[s_t:s_t + pt, :][:, idx].reshape(-1)\n        cxx_patch = cxx[s_t:s_t + pt, :][:, idx].reshape(-1)\n        c_patch = c_slice[s_t:s_t + pt, :][:, idx].reshape(-1)\n\n        # Build Theta for this patch\n        Theta_patch = np.column_stack([cxx_patch, c_patch])\n\n        patches.append((ct_patch, Theta_patch))\n\n    return patches\n\ndef aggregate_patches(patches):\n    \"\"\"\n    Concatenate all patch data into a single regression dataset.\n    Returns y (vector) and Theta (matrix).\n    \"\"\"\n    y_list = []\n    Theta_list = []\n    for y_p, Th_p in patches:\n        y_list.append(y_p)\n        Theta_list.append(Th_p)\n    y = np.concatenate(y_list, axis=0)\n    Theta = np.vstack(Theta_list)\n    return y, Theta\n\ndef estimate_coefficients(y, Theta):\n    \"\"\"\n    Ordinary least squares estimate of xi in y = Theta @ xi.\n    Returns xi_hat (length-2 vector).\n    \"\"\"\n    # Use numpy lstsq for numerical stability\n    xi_hat, _, _, _ = np.linalg.lstsq(Theta, y, rcond=None)\n    return xi_hat\n\ndef bootstrap_ci(patches, B):\n    \"\"\"\n    Perform B bootstrap replicates by resampling patches with replacement.\n    Returns arrays of bootstrap estimates for D_hat and k_hat (k_hat computed as -xi2).\n    \"\"\"\n    P = len(patches)\n    D_boot = np.zeros(B)\n    k_boot = np.zeros(B)\n    for b in range(B):\n        # Sample patch indices with replacement\n        idx = np.random.randint(0, P, size=P)\n        resampled = [patches[i] for i in idx]\n        y_b, Theta_b = aggregate_patches(resampled)\n        xi_b = estimate_coefficients(y_b, Theta_b)\n        D_boot[b] = xi_b[0]\n        k_boot[b] = -xi_b[1]\n    return D_boot, k_boot\n\ndef run_case(D_true, k_true, L, Nx, Nt, dt, sigma, px, pt, M, B):\n    \"\"\"\n    Run the full pipeline for a single test case:\n    - Generate synthetic data\n    - Compute derivatives\n    - Sample patches\n    - Estimate coefficients\n    - Bootstrap confidence intervals\n    Returns [D_hat, k_hat, D_low, D_high, k_low, k_high]\n    \"\"\"\n    # Generate concentration and add noise\n    C = generate_concentration(D_true, k_true, L, Nx, Nt, dt)\n    C_noisy = add_noise(C, sigma)\n\n    # Derivatives\n    ct, cxx, c_slice = compute_derivatives(C_noisy, L, dt)\n\n    # Sample patches\n    patches = sample_patches(ct, cxx, c_slice, px, pt, M)\n\n    # Point estimate from full (non-resampled) set of M patches\n    y_all, Theta_all = aggregate_patches(patches)\n    xi_hat = estimate_coefficients(y_all, Theta_all)\n    D_hat = float(xi_hat[0])\n    k_hat = float(-xi_hat[1])\n\n    # Bootstrap\n    D_boot, k_boot = bootstrap_ci(patches, B)\n\n    # 95% percentile CIs\n    D_low = float(np.percentile(D_boot, 2.5))\n    D_high = float(np.percentile(D_boot, 97.5))\n    k_low = float(np.percentile(k_boot, 2.5))\n    k_high = float(np.percentile(k_boot, 97.5))\n\n    return [D_hat, k_hat, D_low, D_high, k_low, k_high]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: general case\n        {\n            \"D\": 5e-12, \"k\": 1e-3, \"L\": 1e-3,\n            \"Nx\": 128, \"Nt\": 120, \"dt\": 0.5, \"sigma\": 1e-6,\n            \"px\": 16, \"pt\": 20, \"M\": 50, \"B\": 200\n        },\n        # Test case 2: noise-free boundary\n        {\n            \"D\": 5e-12, \"k\": 1e-3, \"L\": 1e-3,\n            \"Nx\": 128, \"Nt\": 120, \"dt\": 0.5, \"sigma\": 0.0,\n            \"px\": 32, \"pt\": 30, \"M\": 40, \"B\": 150\n        },\n        # Test case 3: higher reaction rate and higher noise\n        {\n            \"D\": 5e-12, \"k\": 2e-3, \"L\": 1e-3,\n            \"Nx\": 96, \"Nt\": 100, \"dt\": 0.5, \"sigma\": 5e-6,\n            \"px\": 12, \"pt\": 16, \"M\": 60, \"B\": 200\n        },\n        # Test case 4: small dataset, weaker diffusion/reaction\n        {\n            \"D\": 1e-12, \"k\": 5e-4, \"L\": 1e-3,\n            \"Nx\": 64, \"Nt\": 80, \"dt\": 0.5, \"sigma\": 1e-6,\n            \"px\": 8, \"pt\": 10, \"M\": 12, \"B\": 60\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(\n            D_true=case[\"D\"], k_true=case[\"k\"], L=case[\"L\"],\n            Nx=case[\"Nx\"], Nt=case[\"Nt\"], dt=case[\"dt\"], sigma=case[\"sigma\"],\n            px=case[\"px\"], pt=case[\"pt\"], M=case[\"M\"], B=case[\"B\"]\n        )\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Print as a single line: list of four lists, each with six floats.\n    # Avoid additional text.\n    print(f\"[{','.join('[' + ','.join(map(str, r)) + ']' for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}