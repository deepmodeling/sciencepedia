{
    "hands_on_practices": [
        {
            "introduction": "Direct Numerical Simulation begins with a high-fidelity digital representation of a material's internal structure, often obtained from 3D imaging techniques like X-ray tomography. This exercise tackles the first critical step in the simulation pipeline: image segmentation. You will explore Otsu's method, a powerful and widely-used algorithm that automatically determines an optimal threshold to separate a grayscale image into distinct phases, such as the solid and pore regions in a battery electrode. ",
            "id": "3907165",
            "problem": "A grayscale microstructure image of a porous lithium-ion battery cathode reconstructed from X-ray Computed Tomography (XCT) is to be segmented into pore and solid phases to enable Direct Numerical Simulation (DNS) of ionic transport. Segmentation is performed by a single global threshold on the $8$-bit grayscale intensity, thereby assigning all voxels with intensity $\\leq t$ to the pore phase and all voxels with intensity $> t$ to the solid phase. The observed grayscale histogram is bimodal and has nonzero counts only at the following $8$ gray levels: $(g_i, n_i)$ pairs are $(30, 400)$, $(60, 800)$, $(90, 1200)$, $(120, 600)$, $(150, 300)$, $(180, 600)$, $(210, 900)$, $(240, 450)$, where $g_i$ is the gray level and $n_i$ the number of voxels at that level.\n\nStarting only from the definitions of probability, mean, and variance of a discrete distribution, and from the variance decomposition identity that the total variance can be written as the sum of within-class and between-class contributions, derive the criterion used in Otsu’s method for choosing the optimal threshold $t$ by maximizing the inter-class (between-class) variance of the two classes induced by thresholding. Then, apply the derived criterion to the provided histogram to determine the unique threshold $t^{\\star}$ among the candidate gray levels $t \\in \\{30, 60, 90, 120, 150, 180, 210\\}$ that maximizes the inter-class variance.\n\nExpress the final threshold as an integer gray level on the $8$-bit scale. No rounding is necessary.",
            "solution": "The task is to first derive the criterion for Otsu's method of image thresholding and then apply it to a given histogram to find the optimal threshold.\n\n### Part 1: Derivation of the Otsu's Method Criterion\n\nLet the discrete gray levels be denoted by $i$, where $i \\in \\{0, 1, \\dots, L-1\\}$. For an $8$-bit image, $L=256$. Let $n_i$ be the number of voxels with gray level $i$ in the image, and let $N$ be the total number of voxels, $N = \\sum_{i=0}^{L-1} n_i$.\n\nFrom the definition of probability for a discrete distribution, the probability of a voxel having gray level $i$ is:\n$$\np_i = \\frac{n_i}{N}\n$$\nThese probabilities satisfy $\\sum_{i=0}^{L-1} p_i = 1$.\n\nFrom the definition of the mean of a discrete distribution, the total mean gray level of the image is:\n$$\n\\mu_T = \\sum_{i=0}^{L-1} i \\cdot p_i\n$$\n\nA threshold at gray level $t$ partitions the voxels into two classes:\n- Class 1 ($C_1$): Pore phase, containing voxels with gray levels $\\{0, 1, \\dots, t\\}$.\n- Class 2 ($C_2$): Solid phase, containing voxels with gray levels $\\{t+1, \\dots, L-1\\}$.\n\nThe probability of a randomly selected voxel belonging to Class 1, $\\omega_1(t)$, is the sum of the probabilities of the gray levels in $C_1$:\n$$\n\\omega_1(t) = P(C_1) = \\sum_{i=0}^{t} p_i\n$$\nSimilarly, the probability of belonging to Class 2, $\\omega_2(t)$, is:\n$$\n\\omega_2(t) = P(C_2) = \\sum_{i=t+1}^{L-1} p_i\n$$\nIt follows that $\\omega_1(t) + \\omega_2(t) = 1$.\n\nThe mean gray level for each class is calculated as a conditional expectation:\n$$\n\\mu_1(t) = \\frac{\\sum_{i=0}^{t} i \\cdot p_i}{\\omega_1(t)} \\quad \\text{and} \\quad \\mu_2(t) = \\frac{\\sum_{i=t+1}^{L-1} i \\cdot p_i}{\\omega_2(t)}\n$$\nThe total mean $\\mu_T$ can be expressed as a weighted average of the class means: $\\mu_T = \\omega_1(t)\\mu_1(t) + \\omega_2(t)\\mu_2(t)$.\n\nThe problem states that we must use the variance decomposition identity, which expresses the total variance of the image's gray level distribution, $\\sigma_T^2$, as the sum of two components: the within-class variance, $\\sigma_W^2(t)$, and the between-class variance, $\\sigma_B^2(t)$.\n$$\n\\sigma_T^2 = \\sigma_W^2(t) + \\sigma_B^2(t)\n$$\nThe total variance $\\sigma_T^2 = \\sum_{i=0}^{L-1} (i - \\mu_T)^2 p_i$ is a constant for a given image and does not depend on the threshold $t$.\n\nThe within-class variance, $\\sigma_W^2(t)$, is the weighted average of the variances of each class: $\\sigma_W^2(t) = \\omega_1(t)\\sigma_1^2(t) + \\omega_2(t)\\sigma_2^2(t)$, where $\\sigma_1^2(t)$ and $\\sigma_2^2(t)$ are the variances within $C_1$ and $C_2$, respectively. It measures the homogeneity within the classes. A good segmentation should yield homogeneous classes, so the goal is to minimize $\\sigma_W^2(t)$.\n\nThe between-class variance, $\\sigma_B^2(t)$, is defined as:\n$$\n\\sigma_B^2(t) = \\omega_1(t)(\\mu_1(t) - \\mu_T)^2 + \\omega_2(t)(\\mu_2(t) - \\mu_T)^2\n$$\nThis measures the separability of the two classes.\n\nSince $\\sigma_T^2$ is constant, minimizing the within-class variance $\\sigma_W^2(t)$ is mathematically equivalent to maximizing the between-class variance $\\sigma_B^2(t)$. Otsu's method is defined by this principle. Therefore, the criterion for selecting the optimal threshold $t^\\star$ is to maximize the between-class variance:\n$$\nt^\\star = \\arg\\max_{t} \\left\\{ \\sigma_B^2(t) \\right\\}\n$$\nFor computational purposes, the expression for $\\sigma_B^2(t)$ can be simplified. Using $\\mu_T = \\omega_1\\mu_1 + \\omega_2\\mu_2$ and $\\omega_1+\\omega_2=1$:\n$$\n\\begin{align*} \\sigma_B^2(t) &= \\omega_1(\\mu_1 - (\\omega_1\\mu_1 + \\omega_2\\mu_2))^2 + \\omega_2(\\mu_2 - (\\omega_1\\mu_1 + \\omega_2\\mu_2))^2 \\\\ &= \\omega_1((1-\\omega_1)\\mu_1 - \\omega_2\\mu_2)^2 + \\omega_2(-\\omega_1\\mu_1 + (1-\\omega_2)\\mu_2)^2 \\\\ &= \\omega_1(\\omega_2\\mu_1 - \\omega_2\\mu_2)^2 + \\omega_2(-\\omega_1\\mu_1 + \\omega_1\\mu_2)^2 \\\\ &= \\omega_1\\omega_2^2(\\mu_1 - \\mu_2)^2 + \\omega_2\\omega_1^2(\\mu_1 - \\mu_2)^2 \\\\ &= (\\omega_1\\omega_2^2 + \\omega_2\\omega_1^2)(\\mu_1 - \\mu_2)^2 \\\\ &= \\omega_1\\omega_2(\\omega_2 + \\omega_1)(\\mu_1 - \\mu_2)^2 \\\\ &= \\omega_1(t)\\omega_2(t)(\\mu_1(t) - \\mu_2(t))^2 \\end{align*}\n$$\nThis completes the derivation of the criterion.\n\n### Part 2: Application to the Given Data\n\nThe provided histogram data is:\n$(g_i, n_i) \\in \\{(30, 400), (60, 800), (90, 1200), (120, 600), (150, 300), (180, 600), (210, 900), (240, 450)\\}$.\n\nFirst, we calculate the total number of voxels, $N$:\n$$\nN = 400 + 800 + 1200 + 600 + 300 + 600 + 900 + 450 = 5250\n$$\nThe total first moment (sum of $g_i \\cdot n_i$) is:\n$$\nM_T = (30)(400) + (60)(800) + (90)(1200) + (120)(600) + (150)(300) + (180)(600) + (210)(900) + (240)(450)\n$$\n$$\nM_T = 12000 + 48000 + 108000 + 72000 + 45000 + 108000 + 189000 + 108000 = 690000\n$$\nWe will test each candidate threshold $t \\in \\{30, 60, 90, 120, 150, 180, 210\\}$. For each $t$, we compute the necessary quantities for $C_1$ (gray levels $\\le t$) and $C_2$ (gray levels $> t$). Let $N_1(t)$ and $M_1(t)$ be the number of voxels and the first moment for $C_1$, respectively. Then $\\omega_1(t) = N_1(t)/N$, $\\mu_1(t) = M_1(t)/N_1(t)$, $\\omega_2(t) = (N - N_1(t))/N$, and $\\mu_2(t) = (M_T - M_1(t))/(N-N_1(t))$.\n\nThe cumulative sums of counts ($N_1$) and moments ($M_1$) are pre-calculated for efficiency:\n- $g_i \\le 30$: $N_1=400$, $M_1=12000$\n- $g_i \\le 60$: $N_1=1200$, $M_1=60000$\n- $g_i \\le 90$: $N_1=2400$, $M_1=168000$\n- $g_i \\le 120$: $N_1=3000$, $M_1=240000$\n- $g_i \\le 150$: $N_1=3300$, $M_1=285000$\n- $g_i \\le 180$: $N_1=3900$, $M_1=393000$\n- $g_i \\le 210$: $N_1=4800$, $M_1=582000$\n\nNow, we calculate $\\sigma_B^2(t) = \\omega_1(t)\\omega_2(t)(\\mu_1(t) - \\mu_2(t))^2$ for each candidate $t$.\n\n- **For $t=30$:**\n  $N_1=400$, $M_1=12000 \\implies \\mu_1=\\frac{12000}{400}=30$.\n  $N_2=4850$, $M_2=678000 \\implies \\mu_2=\\frac{678000}{4850} \\approx 139.79$.\n  $\\sigma_B^2(30) = \\left(\\frac{400}{5250}\\right)\\left(\\frac{4850}{5250}\\right)(30-139.79)^2 \\approx 848.7$\n\n- **For $t=60$:**\n  $N_1=1200$, $M_1=60000 \\implies \\mu_1=\\frac{60000}{1200}=50$.\n  $N_2=4050$, $M_2=630000 \\implies \\mu_2=\\frac{630000}{4050} \\approx 155.56$.\n  $\\sigma_B^2(60) = \\left(\\frac{1200}{5250}\\right)\\left(\\frac{4050}{5250}\\right)(50-155.56)^2 \\approx 1965.7$\n\n- **For $t=90$:**\n  $N_1=2400$, $M_1=168000 \\implies \\mu_1=\\frac{168000}{2400}=70$.\n  $N_2=2850$, $M_2=522000 \\implies \\mu_2=\\frac{522000}{2850} \\approx 183.16$.\n  $\\sigma_B^2(90) = \\left(\\frac{2400}{5250}\\right)\\left(\\frac{2850}{5250}\\right)(70-183.16)^2 \\approx 3179.5$\n\n- **For $t=120$:**\n  $N_1=3000$, $M_1=240000 \\implies \\mu_1=\\frac{240000}{3000}=80$.\n  $N_2=2250$, $M_2=450000 \\implies \\mu_2=\\frac{450000}{2250}=200$.\n  $\\sigma_B^2(120) = \\left(\\frac{3000}{5250}\\right)\\left(\\frac{2250}{5250}\\right)(80-200)^2 = \\left(\\frac{4}{7}\\right)\\left(\\frac{3}{7}\\right)(-120)^2 = \\frac{12}{49}(14400) = \\frac{172800}{49} \\approx 3526.5$\n\n- **For $t=150$:**\n  $N_1=3300$, $M_1=285000 \\implies \\mu_1=\\frac{285000}{3300} \\approx 86.36$.\n  $N_2=1950$, $M_2=405000 \\implies \\mu_2=\\frac{405000}{1950} \\approx 207.69$.\n  $\\sigma_B^2(150) = \\left(\\frac{3300}{5250}\\right)\\left(\\frac{1950}{5250}\\right)(86.36-207.69)^2 \\approx 3435.3$\n\n- **For $t=180$:**\n  $N_1=3900$, $M_1=393000 \\implies \\mu_1=\\frac{393000}{3900} \\approx 100.77$.\n  $N_2=1350$, $M_2=297000 \\implies \\mu_2=\\frac{297000}{1350}=220$.\n  $\\sigma_B^2(180) = \\left(\\frac{3900}{5250}\\right)\\left(\\frac{1350}{5250}\\right)(100.77-220)^2 \\approx 2715.2$\n\n- **For $t=210$:**\n  $N_1=4800$, $M_1=582000 \\implies \\mu_1=\\frac{582000}{4800}=121.25$.\n  $N_2=450$, $M_2=108000 \\implies \\mu_2=\\frac{108000}{450}=240$.\n  $\\sigma_B^2(210) = \\left(\\frac{4800}{5250}\\right)\\left(\\frac{450}{5250}\\right)(121.25-240)^2 \\approx 1109.5$\n\nComparing the calculated values for the between-class variance:\n- $\\sigma_B^2(30) \\approx 848.7$\n- $\\sigma_B^2(60) \\approx 1965.7$\n- $\\sigma_B^2(90) \\approx 3179.5$\n- $\\sigma_B^2(120) \\approx 3526.5$\n- $\\sigma_B^2(150) \\approx 3435.3$\n- $\\sigma_B^2(180) \\approx 2715.2$\n- $\\sigma_B^2(210) \\approx 1109.5$\n\nThe maximum value of $\\sigma_B^2(t)$ occurs at $t=120$. Therefore, the optimal threshold is $t^\\star = 120$.",
            "answer": "$$\\boxed{120}$$"
        },
        {
            "introduction": "Before a simulation can yield trustworthy physical insights, we must first confirm its mathematical integrity through a process known as code verification. This practice introduces the Method of Manufactured Solutions (MMS), a gold-standard technique for this purpose. By contriving a problem with a known analytical solution, you will learn how to systematically test your numerical solver and verify that its error converges at the theoretically expected rate, ensuring the code correctly implements the governing equations. ",
            "id": "3907242",
            "problem": "Consider an anisotropic diffusion model on a reconstructed voxel grid representing a homogenized microstructure slice in a lithium-ion battery electrode. Let the domain be the unit square $\\Omega = (0,1) \\times (0,1)$ with boundary $\\partial \\Omega$. The steady diffusion equation for a scalar concentration field $u(x,y)$ under an anisotropic, constant, symmetric, positive-definite tensor $\\boldsymbol{K}$ is given by the Partial Differential Equation (PDE) $-\\nabla \\cdot \\left( \\boldsymbol{K} \\nabla u \\right) = f$ in $\\Omega$, with Dirichlet boundary condition $u = u_{\\text{exact}}$ on $\\partial \\Omega$. The tensor is represented as $\\boldsymbol{K} = \\begin{bmatrix} k_{xx} & k_{xy} \\\\ k_{xy} & k_{yy} \\end{bmatrix}$, with $k_{xx} > 0$, $k_{yy} > 0$, and $k_{xx} k_{yy} - k_{xy}^{2} > 0$.\n\nYour tasks are:\n- Starting from mass conservation $\\nabla \\cdot \\mathbf{J} = -f$ with flux $\\mathbf{J} = - \\boldsymbol{K} \\nabla u$, and using a manufactured exact solution $u_{\\text{exact}}(x,y) = \\sin(\\pi x) \\sin(2\\pi y)$, derive the source term $f(x,y)$ such that $u_{\\text{exact}}$ is an exact solution of the PDE for any constant $\\boldsymbol{K}$ satisfying the stated conditions. Do not assume any special structure beyond symmetry and positive-definiteness.\n- Discretize the PDE on a uniform voxel grid with $N \\times N$ nodes (including boundary nodes) using a second-order accurate central difference discretization consistent with the anisotropic operator, including the mixed derivative implied by the tensor off-diagonal entries. Impose Dirichlet boundary conditions by setting boundary nodes to the exact solution and assembling the linear system for the interior nodes.\n- For a given parameter set $\\left(k_{xx}, k_{xy}, k_{yy}, \\{N_{\\ell}\\}_{\\ell=1}^{L}\\right)$, solve the discrete problem for each $N_{\\ell}$, compute the discrete $L^{2}$ error $E_{N} = \\left( h^{2} \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} \\left( u_{ij}^{\\text{num}} - u_{\\text{exact}}(x_{i}, y_{j}) \\right)^{2} \\right)^{1/2}$ with $h = 1/(N-1)$, and estimate the Observed Order of Accuracy (OOA) by fitting a straight line $y = p x + c$ to the data $\\left(\\log h, \\log E_{N}\\right)$ using least squares and reporting the slope $p$.\n- Implement the above as a complete, runnable program that performs these steps without any external input.\n\nUse the following test suite of parameter values to exercise different regimes:\n- Test case $1$ (happy path, isotropic): $(k_{xx}, k_{xy}, k_{yy}) = (1.0, 0.0, 1.0)$ with $N \\in \\{16, 32, 64, 128\\}$.\n- Test case $2$ (strong diagonal anisotropy): $(k_{xx}, k_{xy}, k_{yy}) = (1.0, 0.0, 100.0)$ with $N \\in \\{16, 32, 64, 128\\}$.\n- Test case $3$ (anisotropy with cross coupling): $(k_{xx}, k_{xy}, k_{yy}) = (2.0, 0.3, 3.0)$ with $N \\in \\{16, 32, 64, 128\\}$.\n- Test case $4$ (edge case with coarse levels and near-degenerate coupling while remaining positive-definite): $(k_{xx}, k_{xy}, k_{yy}) = (1.0, 0.09, 0.1)$ with $N \\in \\{8, 16, 32, 64\\}$.\n\nYour program must:\n- Construct the manufactured source $f(x,y)$ implied by $u_{\\text{exact}}(x,y)$.\n- Assemble and solve the linear system for each $N$ in each test case, compute $E_{N}$, and then estimate the OOA $p$ by least-squares fit over all available levels for that test case.\n- Produce a single line of output containing the four OOA values corresponding to the four test cases, in the same order as specified above, rounded to three decimal places, as a comma-separated list enclosed in square brackets, e.g., $\\left[\\text{p}_{1},\\text{p}_{2},\\text{p}_{3},\\text{p}_{4}\\right]$.\n\nNo physical units are required because the problem is nondimensionalized. Angles, where present in trigonometric functions, are in radians by convention. The final output format must be exactly one line as specified, with no additional text.",
            "solution": "The user-provided problem has been validated and is deemed sound. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The specified physical and numerical parameters are consistent and feasible. The task is a standard exercise in the numerical analysis of partial differential equations using the method of manufactured solutions for code verification.\n\nThe solution proceeds in three stages:\n1.  Analytical derivation of the source term $f(x,y)$.\n2.  Discretization of the governing partial differential equation (PDE) using a second-order accurate finite difference scheme.\n3.  Formulation of the numerical algorithm to assemble and solve the discrete system, calculate the error, and estimate the order of accuracy.\n\n### 1. Derivation of the Source Term\n\nThe governing PDE is the steady-state anisotropic diffusion equation:\n$$\n-\\nabla \\cdot (\\boldsymbol{K} \\nabla u) = f\n$$\nwhere $u(x,y)$ is the scalar concentration field, $f(x,y)$ is a source term, and $\\boldsymbol{K}$ is a constant, symmetric, positive-definite diffusion tensor:\n$$\n\\boldsymbol{K} = \\begin{bmatrix} k_{xx} & k_{xy} \\\\ k_{xy} & k_{yy} \\end{bmatrix}\n$$\nExpanding the divergence operator, we get:\n$$\n-\\left[ \\frac{\\partial}{\\partial x} \\left( k_{xx} \\frac{\\partial u}{\\partial x} + k_{xy} \\frac{\\partial u}{\\partial y} \\right) + \\frac{\\partial}{\\partial y} \\left( k_{xy} \\frac{\\partial u}{\\partial x} + k_{yy} \\frac{\\partial u}{\\partial y} \\right) \\right] = f(x,y)\n$$\nSince the components of $\\boldsymbol{K}$ are constant, the derivatives apply only to the partials of $u$:\n$$\n-\\left[ k_{xx} \\frac{\\partial^2 u}{\\partial x^2} + k_{xy} \\frac{\\partial^2 u}{\\partial x \\partial y} + k_{xy} \\frac{\\partial^2 u}{\\partial y \\partial x} + k_{yy} \\frac{\\partial^2 u}{\\partial y^2} \\right] = f(x,y)\n$$\nBy Clairaut's theorem, for a sufficiently smooth function $u$, the mixed partial derivatives are equal, $\\frac{\\partial^2 u}{\\partial x \\partial y} = \\frac{\\partial^2 u}{\\partial y \\partial x}$. Thus, the source term $f(x,y)$ is given by:\n$$\nf(x,y) = -k_{xx} \\frac{\\partial^2 u}{\\partial x^2} - 2k_{xy} \\frac{\\partial^2 u}{\\partial x \\partial y} - k_{yy} \\frac{\\partial^2 u}{\\partial y^2}\n$$\nWe use the method of manufactured solutions with the given exact solution $u_{\\text{exact}}(x,y) = \\sin(\\pi x) \\sin(2\\pi y)$. We compute its partial derivatives:\n\\begin{align*}\n\\frac{\\partial u_{\\text{exact}}}{\\partial x} &= \\pi \\cos(\\pi x) \\sin(2\\pi y) \\\\\n\\frac{\\partial^2 u_{\\text{exact}}}{\\partial x^2} &= -\\pi^2 \\sin(\\pi x) \\sin(2\\pi y) \\\\\n\\frac{\\partial u_{\\text{exact}}}{\\partial y} &= 2\\pi \\sin(\\pi x) \\cos(2\\pi y) \\\\\n\\frac{\\partial^2 u_{\\text{exact}}}{\\partial y^2} &= -4\\pi^2 \\sin(\\pi x) \\sin(2\\pi y) \\\\\n\\frac{\\partial^2 u_{\\text{exact}}}{\\partial x \\partial y} &= \\frac{\\partial}{\\partial y} (\\pi \\cos(\\pi x) \\sin(2\\pi y)) = 2\\pi^2 \\cos(\\pi x) \\cos(2\\pi y)\n\\end{align*}\nSubstituting these into the expression for $f(x,y)$:\n$$\nf(x,y) = -k_{xx} (-\\pi^2 \\sin(\\pi x) \\sin(2\\pi y)) - 2k_{xy} (2\\pi^2 \\cos(\\pi x) \\cos(2\\pi y)) - k_{yy} (-4\\pi^2 \\sin(\\pi x) \\sin(2\\pi y))\n$$\nSimplifying, we obtain the analytical expression for the source term:\n$$\nf(x,y) = \\pi^2 \\left[ (k_{xx} + 4k_{yy}) \\sin(\\pi x) \\sin(2\\pi y) - 4k_{xy} \\cos(\\pi x) \\cos(2\\pi y) \\right]\n$$\n\n### 2. Finite Difference Discretization\n\nWe discretize the domain $\\Omega = (0,1) \\times (0,1)$ using a uniform grid with $N \\times N$ nodes. The coordinates of a node $(i,j)$ are $(x_i, y_j) = (ih, jh)$ for $i, j \\in \\{0, 1, \\dots, N-1\\}$, where the grid spacing is $h = 1/(N-1)$. The value of the solution at a node is denoted $u_{ij} \\approx u(x_i, y_j)$.\n\nWe approximate the partial derivatives at an interior node $(i,j)$ using second-order accurate central difference formulas:\n\\begin{align*}\n\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{ij} &\\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} \\\\\n\\frac{\\partial^2 u}{\\partial y^2}\\bigg|_{ij} &\\approx \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} \\\\\n\\frac{\\partial^2 u}{\\partial x \\partial y}\\bigg|_{ij} &\\approx \\frac{u_{i+1,j+1} - u_{i-1,j+1} - u_{i+1,j-1} + u_{i-1,j-1}}{4h^2}\n\\end{align*}\nSubstituting these into the PDE, $f_{ij} = -k_{xx} u''_{xx} - 2k_{xy} u''_{xy} - k_{yy} u''_{yy}$, gives the discrete equation at each interior node $(i,j)$, where $i,j \\in \\{1, \\dots, N-2\\}$:\n$$\nf_{ij} \\approx -\\frac{k_{xx}}{h^2}(u_{i+1,j} - 2u_{i,j} + u_{i-1,j}) - \\frac{k_{yy}}{h^2}(u_{i,j+1} - 2u_{i,j} + u_{i,j-1}) - \\frac{2k_{xy}}{4h^2}(u_{i+1,j+1} - u_{i-1,j+1} - u_{i+1,j-1} + u_{i-1,j-1})\n$$\nMultiplying by $h^2$ and rearranging terms to group coefficients of $u_{kl}$:\n\\begin{align*}\nh^2 f_{ij} \\approx &~ [2(k_{xx} + k_{yy})] u_{i,j} \\\\\n& -k_{xx} u_{i+1,j} - k_{xx} u_{i-1,j} \\\\\n& -k_{yy} u_{i,j+1} - k_{yy} u_{i,j-1} \\\\\n& -\\frac{k_{xy}}{2} u_{i+1,j+1} + \\frac{k_{xy}}{2} u_{i-1,j+1} + \\frac{k_{xy}}{2} u_{i+1,j-1} - \\frac{k_{xy}}{2} u_{i-1,j-1}\n\\end{align*}\nThis equation defines a $9$-point stencil. The set of equations for all $(N-2)^2$ interior nodes forms a large, sparse linear system of equations, $A\\mathbf{u}_{\\text{int}} = \\mathbf{b}$, where $\\mathbf{u}_{\\text{int}}$ is a vector of the $(N-2)^2$ unknown interior node values.\n\n### 3. Numerical Algorithm and Error Analysis\n\nThe algorithm proceeds as follows:\n1.  **System Assembly**: For a given grid size $N$, we construct the $(N-2)^2 \\times (N-2)^2$ matrix $A$ and the right-hand-side vector $\\mathbf{b}$. The unknowns $u_{ij}$ are mapped to a $1$D vector. For each interior node $(i,j)$, its corresponding row in the system is constructed. The diagonal entry of $A$ is the coefficient of $u_{ij}$, which is $2(k_{xx} + k_{yy})$. Off-diagonal entries are the coefficients of neighboring nodes. If a neighbor is another interior node, its coefficient is placed in the corresponding column of $A$. If a neighbor is on the boundary (i.e., $i=0, N-1$ or $j=0, N-1$), its value is known from the Dirichlet boundary condition, $u|_{\\partial\\Omega} = u_{\\text{exact}}$. The term involving this known boundary value is moved to the right-hand side of the equation. Thus, the vector $\\mathbf{b}$ for node $(i,j)$ contains the source term $h^2 f(x_i, y_j)$ adjusted by contributions from all neighboring boundary nodes.\n\n2.  **System Solution**: The resulting sparse linear system $A\\mathbf{u}_{\\text{int}} = \\mathbf{b}$ is solved for the vector of interior node values $\\mathbf{u}_{\\text{int}}$ using a direct sparse solver.\n\n3.  **Error Calculation**: The $1$D solution vector $\\mathbf{u}_{\\text{int}}$ is reshaped back into a $2$D $(N-2) \\times (N-2)$ array and embedded into an $N \\times N$ grid, $u^{\\text{num}}$, which is filled with the exact solution values on the boundary. The discrete $L^2$ error, $E_N$, is then computed using the given formula:\n    $$\n    E_{N} = \\left( h^{2} \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} \\left( u_{ij}^{\\text{num}} - u_{\\text{exact}}(x_{i}, y_{j}) \\right)^{2} \\right)^{1/2}\n    $$\n\n4.  **Order of Accuracy (OOA) Estimation**: For each test case, the error $E_{N}$ is computed for a sequence of grid resolutions $\\{N_{\\ell}\\}_{\\ell=1}^{L}$. The theoretical error for a $p$-th order accurate method behaves as $E_N \\approx C h^p = C (N-1)^{-p}$, where $C$ is a constant. Taking the logarithm gives $\\log E_N \\approx p \\log h + \\log C$. This is a linear relationship between $\\log E_N$ and $\\log h$. We perform a linear least-squares fit to the data points $(\\log h_{\\ell}, \\log E_{N_{\\ell}})$. The slope of the resulting line provides an estimate for the Observed Order of Accuracy (OOA), $p$. Since our discretization scheme is second-order accurate, we expect the OOA to be close to $2.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path, isotropic)\n        {'k_xx': 1.0, 'k_xy': 0.0, 'k_yy': 1.0, 'N_levels': [16, 32, 64, 128]},\n        # Test case 2 (strong diagonal anisotropy)\n        {'k_xx': 1.0, 'k_xy': 0.0, 'k_yy': 100.0, 'N_levels': [16, 32, 64, 128]},\n        # Test case 3 (anisotropy with cross coupling)\n        {'k_xx': 2.0, 'k_xy': 0.3, 'k_yy': 3.0, 'N_levels': [16, 32, 64, 128]},\n        # Test case 4 (edge case near-degenerate)\n        {'k_xx': 1.0, 'k_xy': 0.09, 'k_yy': 0.1, 'N_levels': [8, 16, 32, 64]}\n    ]\n\n    ooa_results = []\n    for params in test_cases:\n        ooa = compute_ooa_for_case(params['k_xx'], params['k_xy'], params['k_yy'], params['N_levels'])\n        ooa_results.append(ooa)\n\n    # Format and print the final output\n    formatted_results = [f\"{res:.3f}\" for res in ooa_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef compute_ooa_for_case(k_xx, k_xy, k_yy, N_levels):\n    \"\"\"\n    Computes the Observed Order of Accuracy (OOA) for a single test case.\n    \"\"\"\n    h_values = []\n    error_values = []\n\n    # Define exact solution and source term functions\n    def u_exact_func(x, y):\n        return np.sin(np.pi * x) * np.sin(2 * np.pi * y)\n\n    def f_source_func(x, y, kxx, kxy, kyy):\n        term1 = (kxx + 4 * kyy) * np.sin(np.pi * x) * np.sin(2 * np.pi * y)\n        term2 = -4 * kxy * np.cos(np.pi * x) * np.cos(2 * np.pi * y)\n        return np.pi**2 * (term1 + term2)\n\n    for N in N_levels:\n        h = 1.0 / (N - 1)\n        h_values.append(h)\n\n        # Create grid\n        x = np.linspace(0.0, 1.0, N)\n        y = np.linspace(0.0, 1.0, N)\n        X, Y = np.meshgrid(x, y, indexing='ij')\n\n        # Evaluate exact solution and source term on the grid\n        u_exact_grid = u_exact_func(X, Y)\n        f_grid = f_source_func(X, Y, k_xx, k_xy, k_yy)\n\n        # Assemble the linear system A * u_int = b\n        N_int = N - 2\n        num_unknowns = N_int * N_int\n\n        if num_unknowns == 0:\n            error_values.append(0.0)\n            continue\n            \n        A = lil_matrix((num_unknowns, num_unknowns), dtype=np.float64)\n        b = np.zeros(num_unknowns, dtype=np.float64)\n        \n        # Stencil coefficients\n        c_center = 2.0 * (k_xx + k_yy)\n        c_ew = -k_xx\n        c_ns = -k_yy\n        c_ne_sw = -k_xy / 2.0\n        c_nw_se = k_xy / 2.0\n\n        stencil_coeffs = {\n            (0, 0): c_center,\n            (1, 0): c_ew, (-1, 0): c_ew,\n            (0, 1): c_ns, (0, -1): c_ns,\n            (1, 1): c_ne_sw, (-1, -1): c_ne_sw,\n            (-1, 1): c_nw_se, (1, -1): c_nw_se,\n        }\n\n        for i_int in range(N_int):\n            for j_int in range(N_int):\n                k_glob = i_int * N_int + j_int\n                i_grid, j_grid = i_int + 1, j_int + 1\n                \n                b[k_glob] = h**2 * f_grid[i_grid, j_grid]\n\n                for (di, dj), coeff in stencil_coeffs.items():\n                    ni, nj = i_grid + di, j_grid + dj\n                    \n                    is_interior = (1 <= ni < N - 1) and (1 <= nj < N - 1)\n\n                    if (di, dj) == (0, 0):\n                        A[k_glob, k_glob] = coeff\n                    elif is_interior:\n                        ni_int, nj_int = ni - 1, nj - 1\n                        nk_glob = ni_int * N_int + nj_int\n                        A[k_glob, nk_glob] = coeff\n                    else: # Boundary node\n                        u_boundary = u_exact_grid[ni, nj]\n                        b[k_glob] -= coeff * u_boundary\n\n        # Solve the system\n        A_csr = A.tocsr()\n        u_int = spsolve(A_csr, b)\n        \n        # Reconstruct full solution grid\n        u_num = np.copy(u_exact_grid)\n        if num_unknowns > 0:\n            u_num[1:-1, 1:-1] = u_int.reshape((N_int, N_int))\n\n        # Compute L2 error\n        error = h * np.linalg.norm(u_num - u_exact_grid)\n        error_values.append(error)\n\n    # Perform linear regression on log-log data to find OOA\n    log_h = np.log(np.array(h_values))\n    log_E = np.log(np.array(error_values))\n    \n    # Use polyfit to find the slope (order of accuracy)\n    p, _ = np.polyfit(log_h, log_E, 1)\n\n    return p\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A numerical simulation provides an approximation to the true solution of a physical model, with inherent errors due to discretization. This final practice addresses the critical task of quantifying that uncertainty. You will apply a systematic procedure based on Richardson extrapolation to analyze results from a series of simulations on progressively refined grids, allowing you to estimate the discretization error and report the final result with a confidence interval using the Grid Convergence Index (GCI). ",
            "id": "3907212",
            "problem": "You are studying Direct Numerical Simulation (DNS) of heat conduction in reconstructed battery electrode microstructures to predict the isotropic effective thermal conductivity, denoted by $k_{\\mathrm{eff}}$. Under stationary conditions and small imposed thermal gradients, Fourier’s law at the homogenized (volume-averaged) level reads $\\mathbf{q} = -k_{\\mathrm{eff}} \\nabla T$, where $\\mathbf{q}$ is the heat flux and $T$ is the temperature. In numerical practice, $k_{\\mathrm{eff}}$ is approximated on a computational grid with characteristic spacing $h$; denote the grid-approximated effective conductivity by $k_h$. For sufficiently fine grids and a consistent discretization, the discretization error is well-approximated by the leading-order algebraic model $k_h = k_{\\infty} + C h^{p}$, where $k_{\\infty}$ is the exact (grid-independent) value, $C$ is a constant independent of $h$, and $p$ is the order of accuracy. \n\nGiven three nested grids with spacings $h_1$, $h_2$, and $h_3$ such that $h_2 = r h_1$ and $h_3 = r h_2$ for a common refinement ratio $r > 1$, and their corresponding effective conductivity estimates $k_{h_1}$, $k_{h_2}$, and $k_{h_3}$, your task is to:\n- Derive, from the leading-order discretization error model, a three-solution Richardson extrapolation procedure to estimate the observed order of accuracy $p$ and the extrapolated conductivity $k_{\\infty}$.\n- Using the standard definition of the Grid Convergence Index (GCI) with safety factor $1.25$ applied to the fine–medium grid pair, compute the fine-grid GCI as a decimal fraction (not a percentage).\n- From the GCI, report symmetric uncertainty bounds for the finest-grid estimate $k_{h_1}$ as $[k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$, in watts per meter-kelvin (W m$^{-1}$ K$^{-1}$). Express all reported conductivities and bounds in W m$^{-1}$ K$^{-1}$ and round all outputs to $6$ decimal places.\n\nAssumptions:\n- Convergence is monotonic across the three grids (no odd-even oscillations).\n- The refinement ratio $r$ is the same between successive grid levels.\n- The discretization error follows the leading-order algebraic model for sufficiently fine grids.\n\nInput for each test case consists of the tuple $(r, k_{h_1}, k_{h_2}, k_{h_3})$, with all $k$ values in W m$^{-1}$ K$^{-1}$. Use the following four test cases (the test suite), chosen to probe a typical case, a non-integer refinement ratio case, a case converging from below, and a near-converged edge case:\n- Case $1$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,2\\,,\\,1.52\\,,\\,1.58\\,,\\,1.82\\,)$\n- Case $2$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,1.5\\,,\\,0.85\\,,\\,0.891856\\,,\\,0.96875\\,)$\n- Case $3$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,2\\,,\\,1.92\\,,\\,1.6324\\,,\\,0.5298\\,)$\n- Case $4$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,2\\,,\\,1.001\\,,\\,1.004\\,,\\,1.016\\,)$\n\nYour program must:\n- Implement the derivation to compute, for each test case: the observed order $p$, the extrapolated conductivity $k_{\\infty}$, the fine-grid GCI as a decimal fraction, and the symmetric uncertainty bounds $[k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$ around $k_{h_1}$ in W m$^{-1}$ K$^{-1}$.\n- Round all reported floating-point outputs to $6$ decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list $[p, k_{\\infty}, \\mathrm{GCI}, k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$ with all entries as floats. For example: $[[p_1,k_{\\infty,1},\\mathrm{GCI}_1,k_{\\mathrm{lower},1},k_{\\mathrm{upper},1}],[p_2,\\dots],\\dots]$.\n\nNote: Angles are not involved, so no angle unit specification is required. Percentages must be expressed as decimal fractions (for example, use $0.05$ instead of $5$ percent).",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical analysis (specifically, discretization error analysis), well-posed, and objective. It provides a complete and consistent set of data and assumptions to derive a unique solution using standard, verifiable methods.\n\nThe task is to estimate the grid-independent effective thermal conductivity $k_{\\infty}$ and the observed order of accuracy $p$ from a series of three numerical simulations on nested grids. We are also asked to compute the Grid Convergence Index (GCI) and the corresponding uncertainty interval for the finest-grid solution.\n\nThe starting point is the provided leading-order algebraic error model for the numerically approximated effective thermal conductivity, $k_h$:\n$$k_h = k_{\\infty} + C h^{p}$$\nwhere $k_{\\infty}$ is the exact, grid-independent value, $h$ is the characteristic grid spacing, $p$ is the order of accuracy, and $C$ is a constant.\n\nWe are given three conductivity estimates, $k_{h_1}$, $k_{h_2}$, and $k_{h_3}$, computed on three grids with spacings $h_1$ (fine), $h_2$ (medium), and $h_3$ (coarse). The grids are related by a common refinement ratio $r > 1$ such that $h_2 = r h_1$ and $h_3 = r h_2 = r^2 h_1$.\n\nWe can write the error model for each of the three grids:\n$$k_{h_1} = k_{\\infty} + C h_1^p \\quad (1)$$\n$$k_{h_2} = k_{\\infty} + C h_2^p = k_{\\infty} + C (r h_1)^p \\quad (2)$$\n$$k_{h_3} = k_{\\infty} + C h_3^p = k_{\\infty} + C (r^2 h_1)^p \\quad (3)$$\n\nTo find the observed order of accuracy $p$, we first eliminate the unknown $k_{\\infty}$ by taking differences between successive equations.\nSubtracting $(1)$ from $(2)$:\n$$k_{h_2} - k_{h_1} = C ((r h_1)^p - h_1^p) = C h_1^p (r^p - 1) \\quad (4)$$\nSubtracting $(2)$ from $(3)$:\n$$k_{h_3} - k_{h_2} = C ((r^2 h_1)^p - (r h_1)^p) = C (r h_1)^p (r^p - 1) = C h_1^p r^p (r^p - 1) \\quad (5)$$\n\nNow, to eliminate $C$ and $h_1$, we take the ratio of equation $(5)$ to equation $(4)$:\n$$\\frac{k_{h_3} - k_{h_2}}{k_{h_2} - k_{h_1}} = \\frac{C h_1^p r^p (r^p - 1)}{C h_1^p (r^p - 1)} = r^p$$\nThis allows us to solve for $p$ by taking the natural logarithm of both sides:\n$$\\ln\\left(\\frac{k_{h_3} - k_{h_2}}{k_{h_2} - k_{h_1}}\\right) = \\ln(r^p) = p \\ln(r)$$\nThus, the observed order of accuracy is:\n$$p = \\frac{\\ln\\left(\\frac{k_{h_3} - k_{h_2}}{k_{h_2} - k_{h_1}}\\right)}{\\ln(r)}$$\n\nWith the value of $p$ determined, we can estimate the grid-independent solution $k_{\\infty}$ using Richardson extrapolation. Rearranging equation $(4)$ gives an expression for the error term on the fine grid, $C h_1^p$:\n$$C h_1^p = \\frac{k_{h_2} - k_{h_1}}{r^p - 1}$$\nSubstituting this back into equation $(1)$:\n$$k_{h_1} = k_{\\infty} + \\frac{k_{h_2} - k_{h_1}}{r^p - 1}$$\nSolving for $k_{\\infty}$:\n$$k_{\\infty} = k_{h_1} - \\frac{k_{h_2} - k_{h_1}}{r^p - 1} = \\frac{k_{h_1}(r^p - 1) - (k_{h_2} - k_{h_1})}{r^p - 1} = \\frac{k_{h_1}r^p - k_{h_1} - k_{h_2} + k_{h_1}}{r^p - 1}$$\nThis simplifies to the Richardson extrapolation formula:\n$$k_{\\infty} = \\frac{r^p k_{h_1} - k_{h_2}}{r^p - 1}$$\n\nNext, we compute the Grid Convergence Index (GCI) for the fine-grid solution, based on the fine-medium grid pair ($h_1, h_2$). The GCI provides an estimate of the numerical uncertainty. Using a factor of safety $F_s = 1.25$, the GCI is defined as a decimal fraction:\n$$\\mathrm{GCI} = F_s \\left| \\frac{\\epsilon_a}{r^p - 1} \\right|$$\nwhere $\\epsilon_a$ is the approximate relative error between the fine and medium grid solutions:\n$$\\epsilon_a = \\frac{k_{h_1} - k_{h_2}}{k_{h_1}}$$\nSubstituting $\\epsilon_a$ into the GCI formula:\n$$\\mathrm{GCI} = 1.25 \\left| \\frac{(k_{h_1} - k_{h_2})/k_{h_1}}{r^p - 1} \\right| = \\frac{1.25}{|r^p - 1|} \\left| \\frac{k_{h_1} - k_{h_2}}{k_{h_1}} \\right|$$\n\nFinally, we determine the symmetric uncertainty bounds for the finest-grid estimate $k_{h_1}$. The GCI represents the relative uncertainty in the solution $k_{h_1}$. The absolute uncertainty is $\\Delta k = \\mathrm{GCI} \\times |k_{h_1}|$. Since thermal conductivity is a positive quantity, $|k_{h_1}| = k_{h_1}$. The symmetric uncertainty interval $[k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$ is therefore:\n$$k_{\\mathrm{lower}} = k_{h_1} - \\mathrm{GCI} \\cdot k_{h_1} = k_{h_1} (1 - \\mathrm{GCI})$$\n$$k_{\\mathrm{upper}} = k_{h_1} + \\mathrm{GCI} \\cdot k_{h_1} = k_{h_1} (1 + \\mathrm{GCI})$$\n\nThese derived formulas will be implemented for each test case, with all final floating-point values rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the grid convergence analysis problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (r, k_h1, k_h2, k_h3)\n        (2.0, 1.52, 1.58, 1.82),\n        (1.5, 0.85, 0.891856, 0.96875),\n        (2.0, 1.92, 1.6324, 0.5298),\n        (2.0, 1.001, 1.004, 1.016),\n    ]\n\n    all_results = []\n    \n    # Safety factor for GCI calculation\n    F_s = 1.25\n\n    for case in test_cases:\n        r, k_h1, k_h2, k_h3 = case\n\n        # Step 1: Calculate the observed order of accuracy 'p'\n        # p = ln((k_h3 - k_h2) / (k_h2 - k_h1)) / ln(r)\n        # Check for division by zero, although problem data avoids this.\n        diff_21 = k_h2 - k_h1\n        if diff_21 == 0:\n            # If the two finer solutions are identical, p cannot be determined from this formula.\n            # This indicates convergence or issues. For this problem, we assume diff_21 != 0.\n            # In a real scenario, one might have to handle this (e.g., assume p is the theoretical order).\n            # For this problem, we can continue as the test data is well-behaved.\n            pass\n        \n        ratio_of_diffs = (k_h3 - k_h2) / diff_21\n        # The argument to log must be positive. Monotonic convergence ensures this.\n        p = np.log(ratio_of_diffs) / np.log(r)\n\n        # Step 2: Calculate the extrapolated grid-independent solution 'k_inf'\n        # k_inf = (r^p * k_h1 - k_h2) / (r^p - 1)\n        rp = r**p\n        k_inf = (rp * k_h1 - k_h2) / (rp - 1)\n\n        # Step 3: Calculate the Grid Convergence Index (GCI) for the fine grid\n        # GCI = F_s * | (k_h1 - k_h2) / k_h1 | / (r^p - 1)\n        approx_rel_err = (k_h1 - k_h2) / k_h1\n        gci = F_s * np.abs(approx_rel_err) / (rp - 1)\n        \n        # Step 4: Calculate the symmetric uncertainty bounds for k_h1\n        # k_lower = k_h1 * (1 - GCI)\n        # k_upper = k_h1 * (1 + GCI)\n        k_lower = k_h1 * (1.0 - gci)\n        k_upper = k_h1 * (1.0 + gci)\n\n        # Append the results for the current test case, after rounding\n        all_results.append([\n            round(p, 6),\n            round(k_inf, 6),\n            round(gci, 6),\n            round(k_lower, 6),\n            round(k_upper, 6),\n        ])\n\n    # Format the final output string as a list of lists.\n    # e.g., [[p1, k_inf1, ...], [p2, k_inf2, ...]]\n    inner_list_strings = []\n    for result_list in all_results:\n        # Convert each float in the list to a string. `str()` handles `2.0` vs `2.123` correctly.\n        str_list = [str(val) for val in result_list]\n        inner_list_strings.append(f\"[{','.join(str_list)}]\")\n    \n    final_output = f\"[{','.join(inner_list_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}