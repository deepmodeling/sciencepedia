## Applications and Interdisciplinary Connections

The preceding chapters have established the theoretical foundations of structural and practical [parameter identifiability](@entry_id:197485), primarily through the lens of the Fisher Information Matrix (FIM) and sensitivity analysis. While the principles are mathematical, their true value is realized when applied to tangible problems in science and engineering. This chapter bridges the gap between theory and practice by exploring how [identifiability analysis](@entry_id:182774) is used to guide experimental design, inform [model selection](@entry_id:155601), and ensure the reliability of scientific conclusions across a diverse range of disciplines. We will see that from designing automated experiments for batteries to understanding the mechanisms of [drug metabolism](@entry_id:151432) and environmental systems, [identifiability analysis](@entry_id:182774) is an indispensable tool for the modern scientist and engineer.

### Automated Experiment Design in Engineering Systems

One of the most powerful applications of [identifiability analysis](@entry_id:182774) is in the prospective [design of experiments](@entry_id:1123585). Rather than performing an experiment and retrospectively discovering that its data are insufficient to identify key parameters, we can use these principles to design experiments that are maximally informative from the outset. This is the cornerstone of automated and [optimal experiment design](@entry_id:181055).

#### Optimal Excitation for Parameter Estimation

The goal of [optimal experiment design](@entry_id:181055) is to select controllable inputs—such as a current profile applied to a battery or a temperature cycle for a chemical reactor—to maximize the information gained about a set of unknown parameters $\boldsymbol{\theta}$. The Fisher Information Matrix, $\mathcal{F}$, serves as the natural link between experimental inputs and parameter uncertainty. An optimal experiment is one that shapes the FIM in a desirable way.

A common and powerful approach is **D-optimality**, which seeks to maximize the determinant of the FIM, $\det(\mathcal{F})$. Maximizing $\log\det(\mathcal{F})$ is equivalent and often more convenient numerically. This criterion has several elegant interpretations. Geometrically, it is equivalent to minimizing the volume of the confidence [ellipsoid](@entry_id:165811) for the parameter estimates, as defined by the Cramér-Rao Lower Bound (CRLB). From an information-theoretic perspective, under a local linear-Gaussian approximation, maximizing a related quantity, $\frac{1}{2}\log\det(\boldsymbol{\Sigma}_{\text{prior}}^{-1} + \mathcal{F})$, corresponds to maximizing the [mutual information](@entry_id:138718) between the measurement data and the parameters. This means the experiment is designed to make the data as informative about the parameters as possible. Because the determinant is the product of the FIM's eigenvalues, this criterion naturally penalizes parameter correlations, as strong correlations lead to small eigenvalues that shrink the determinant . When parameters have different physical units (e.g., resistance in ohms and diffusion time in seconds), it is crucial to work with a scaled or normalized FIM to create a dimensionless and therefore physically comparable design criterion .

A practical application of this principle is in designing test protocols for batteries. Consider a model with parameters representing different physical processes: a thermodynamic parameter (Open-Circuit Voltage slope, $k_{\mathrm{OCV}}$), a kinetic parameter ([charge transfer resistance](@entry_id:276126), $R_{\mathrm{ct}}$), and a mass transport parameter (diffusion time constant, $\tau$). Each process has a [characteristic timescale](@entry_id:276738). To identify all three parameters, the experimental input current must contain excitations that probe these different timescales. A simple pulse test may excite some dynamics, but a richer signal, such as a **multisine** current composed of sinusoids at various frequencies, can excite a wider range of dynamics simultaneously. A well-designed experiment sequence might therefore combine a pulse to observe the main transient, a multisine signal to probe frequency-dependent phenomena like [charge transfer](@entry_id:150374) and diffusion, and a rest period to allow the system to relax and reveal thermodynamic properties. By simulating the FIM for different combinations of these experimental blocks, one can select the shortest possible experiment that still meets all identifiability criteria, such as full FIM rank, a minimum eigenvalue threshold (E-optimality), and acceptably low parameter correlations .

#### Diagnosing and Remediating Poor Experiments

Identifiability analysis is also a powerful diagnostic tool. The shape of the [profile likelihood](@entry_id:269700), which shows the likelihood of the data as a function of one parameter while optimizing over all others, can reveal much about the quality of an experiment. For instance, in estimating the [solid-state diffusion coefficient](@entry_id:1131918) $D$ in a battery particle model, a well-designed experiment with rich current excitation will produce a sharp, well-defined peak in the [profile likelihood](@entry_id:269700) for $D$. This corresponds to a tight confidence interval and a precise estimate.

In contrast, if the experiment provides poor excitation—for example, applying only a constant current step—the sensitivities of the voltage to different parameters can become highly correlated. This manifests as a very flat or "sloppy" [profile likelihood](@entry_id:269700), yielding a wide [confidence interval](@entry_id:138194) and signaling that the data are insufficient to distinguish the true parameter value from many others. Similarly, an experiment that is too short to capture the full system dynamics or is overwhelmed by high measurement noise will also lead to a shallow likelihood profile and poor practical identifiability. These diagnostics can guide the scientist to redesign the experiment—for instance, by adding richer dynamic content to the input signal or by increasing the measurement time—to obtain more reliable parameter estimates .

### Model Selection and Regularization

Identifiability is not solely a function of the experimental design; it is also deeply connected to the structure of the mathematical model being used. The choice of model and the methods used to fit it have profound implications for whether parameters can be identified.

#### The Trade-off Between Empirical and Physics-Based Models

Scientists often face a choice between fitting a simple, empirical model or a more complex, physics-based one. For example, in battery modeling, one might choose a simple Equivalent Circuit Model (ECM) with several resistor-capacitor (RC) pairs, or a more complex Single Particle Model (SPM) derived from electrochemical first principles. Identifiability analysis reveals critical trade-offs in this choice.

An ECM with distinct RC time constants is often fully identifiable, meaning each resistor and capacitor value can be uniquely determined from a sufficiently rich dataset. However, these parameters are empirical and may lack a direct, [one-to-one mapping](@entry_id:183792) to underlying physical processes. Conversely, a physics-based model like the SPM has parameters with clear physical meaning, such as the particle radius $R_p$ and the solid diffusion coefficient $D_s$. However, these models often contain structural non-identifiabilities. From terminal voltage and current measurements alone, it is impossible to distinguish the effects of $R_p$ and $D_s$ individually; only the composite diffusion time constant $\tau = R_p^2 / D_s$ is structurally identifiable. This fundamental ambiguity persists regardless of the experimental input. This illustrates a key trade-off: an [empirical model](@entry_id:1124412) may have better identifiability but less physical insight, while a physics-based model offers more insight but may contain inherent parameter ambiguities that must be acknowledged or resolved through other means  . Furthermore, even simple models can exhibit [structural non-identifiability](@entry_id:263509) if other system properties, such as total capacity $Q$, are unknown. An unknown $Q$ can become confounded with the parameters of the [open-circuit voltage](@entry_id:270130) function, as the entire state-of-charge axis effectively becomes rescaled .

#### Regularization Techniques for Ill-Conditioned Problems

When an experiment or model structure leads to poor practical identifiability—a "sloppy" system with a nearly singular FIM—the resulting parameter estimates can have enormous variance and be highly sensitive to noise. **Regularization** is a class of techniques used to mitigate this by introducing additional information into the estimation problem, typically trading a small amount of bias for a large reduction in variance.

A common technique is **Tikhonov regularization**, or [ridge regression](@entry_id:140984). In a Bayesian framework, this is equivalent to placing a zero-mean Gaussian prior on the parameters. This modifies the posterior [precision matrix](@entry_id:264481) (the regularized FIM) to $F_{\lambda} = F + \lambda I$, where $\lambda$ is the regularization strength. This addition of a [diagonal matrix](@entry_id:637782) directly increases all eigenvalues of the [precision matrix](@entry_id:264481), which in turn shrinks the eigenvalues of the [posterior covariance matrix](@entry_id:753631), thereby reducing [estimator variance](@entry_id:263211). It also has the effect of reducing the magnitude of off-diagonal terms in the covariance matrix relative to the diagonal terms, which systematically reduces [parameter correlation](@entry_id:274177). This method provides a robust way to stabilize parameter estimates in the face of [ill-conditioning](@entry_id:138674) .

A more sophisticated approach is to use physically-informed Bayesian priors. When fitting a [battery aging](@entry_id:158781) model, for instance, the regressors for different aging mechanisms might be highly collinear, leading to a nearly singular FIM. Instead of using a generic ridge prior, one can leverage physical knowledge. Parameters like degradation coefficients must be non-negative. Their approximate magnitude might be known from Arrhenius scaling laws or other physical constraints. This information can be encoded in the model using weakly informative, truncated priors (e.g., a truncated [log-normal distribution](@entry_id:139089)). These priors add curvature to the log-posterior surface precisely in the "sloppy" directions where the likelihood is flat, rendering the estimation problem well-posed and yielding a proper, identifiable posterior distribution that is consistent with physical reality .

### Interdisciplinary Case Studies

The principles of identifiability are universal, and their application extends far beyond the realm of battery engineering. The same mathematical structures and challenges appear in fields as disparate as biology, pharmacology, materials science, and environmental science.

#### Systems Biology and Pharmacology

In [computational systems biology](@entry_id:747636), models of signaling pathways often involve networks of reactions with unknown [rate constants](@entry_id:196199). Consider a simplified model of G-protein coupled receptor (GPCR) signaling, where the concentration of a [second messenger](@entry_id:149538), $y(t)$, follows a first-order response to a stimulus: $y(t; \theta_1, \theta_2) = \frac{\theta_1}{\theta_2}(1 - e^{-\theta_2 t})$. Here, the initial slope of the response reveals $\theta_1$ and the final plateau reveals the ratio $\theta_1/\theta_2$. Both parameters are thus **structurally identifiable**. However, a seemingly minor change in the model structure, such as $y(t; \theta_1, \theta_2, \theta_3) = \frac{\theta_1 \theta_2}{\theta_3}(1 - e^{-\theta_3 t})$ where $\theta_3$ is known, renders $\theta_1$ and $\theta_2$ **structurally non-identifiable**, as only their product $\theta_1 \theta_2$ can be determined. This illustrates how subtle differences in model assumptions can have drastic consequences for [identifiability](@entry_id:194150) . Even in structurally identifiable models, high [parameter correlation](@entry_id:274177) can lead to "[sloppiness](@entry_id:195822)" and [practical non-identifiability](@entry_id:270178), a common feature in complex biological models .

In pharmacology, Physiologically Based Pharmacokinetic (PBPK) models describe the absorption, distribution, metabolism, and [excretion](@entry_id:138819) (ADME) of a drug. A simple model for a single tissue compartment might have parameters for blood flow ($Q_t$) and a tissue-to-blood [partition coefficient](@entry_id:177413) ($K_{p,t}$). Crucially, the identifiability of these parameters depends on what is measured. If one can only measure the drug concentration in the blood leaving the tissue ($C_{v,t}$), the parameters $Q_t$ and $K_{p,t}$ become structurally non-identifiable, as their effects are lumped into a single observable time constant. However, if the experimental design is changed to allow for direct measurement of the drug concentration within the tissue itself ($C_t$), the plateau and transient of the response provide separate information, rendering both $Q_t$ and $K_{p,t}$ structurally identifiable. This provides a clear example of how changing the observable can resolve a fundamental identifiability issue .

#### Electrochemistry and Environmental Science

In [microkinetic modeling](@entry_id:175129) of electrocatalytic reactions, the [rate constants](@entry_id:196199) of elementary steps are often described by an Arrhenius-type expression, $k = A \exp(-\Delta G^{\ddagger} / (k_B T))$. Here, the pre-exponential factor $A$ and the activation barrier $\Delta G^{\ddagger}$ are often confounded. If experiments are performed at only a single temperature, the effects of $A$ and $\Delta G^{\ddagger}$ on the reaction rate cannot be separated; only the lumped combination $A \exp(-\Delta G^{\ddagger} / (k_B T))$ can be identified. This is a classic case of structural non-identifiability that can only be resolved by introducing a new experimental dimension, namely varying the temperature, to break the correlation between the parameters . Similarly, in coupled multi-physics models, such as an electro-thermal battery model, parameters from different physical domains (e.g., electrochemical resistance and thermal heat transfer) can become confounded. To decouple them, one must apply independent excitations to each domain—for example, using a dynamic current profile to excite the electrical dynamics and a separate, dynamic ambient temperature profile to excite the thermal dynamics. Analyzing the correlation of parameter sensitivities is an effective way to verify that such an experimental design successfully decouples the parameter effects .

In environmental modeling, simple lumped conceptual models are often used to describe complex systems like river catchments. A common rainfall-runoff model uses two parallel linear reservoirs to represent quickflow and baseflow, with parameters for their respective recession constants ($k_q, k_b$) and an input partitioning fraction ($\phi$). If the two reservoirs happen to have the same time constant ($k_q = k_b$), the system behaves as a single reservoir, and the partitioning fraction $\phi$ becomes structurally non-identifiable. Even if the time constants are different, if data is only collected over a short period, the slow dynamics of the baseflow reservoir may not be observable, rendering its parameter $k_b$ practically non-identifiable. This challenge can be overcome by incorporating different types of data, such as analyzing the pre-storm recession curve to independently estimate $k_b$, thereby breaking the [parameter correlation](@entry_id:274177) during the analysis of the storm event itself .

### Advanced Topics: Identifiability in Evolving and Hierarchical Systems

The concepts of identifiability can be extended to even more complex scenarios where parameters themselves are not static or where one is interested in the statistics of a population of systems.

#### Identifiability of Aging and Degradation Parameters

In many systems, such as batteries, parameters evolve over time due to aging and degradation. For example, a battery's capacity $Q$ is not constant but fades over its lifetime. A model for this might be $\dot{Q}(t) = -\alpha |I(t)|$, where the aging rate $\alpha$ is an unknown parameter to be identified alongside the initial capacity $Q_0$. The identifiability of these aging parameters can be subtle. It depends critically on other features of the system. For instance, if the battery's open-circuit voltage is flat with respect to its state-of-charge, the voltage signal contains no information about the state-of-charge and therefore no information about the capacity or its fade rate; the aging parameters become structurally unidentifiable. This highlights how identifiability of one set of parameters can depend on the properties of another part of the model . Similarly, in temperature-dependent aging models, if an experiment is run at a single constant temperature, the base aging rate and the temperature-sensitivity parameter (e.g., an Arrhenius activation energy) become structurally non-identifiable .

#### Hierarchical Modeling and Transfer Learning for Fleet Analysis

When dealing with a population or fleet of similar systems (e.g., a pack of battery cells, a cohort of patients), we are often interested in not just the parameters of one individual, but the distribution of parameters across the population. Bayesian [hierarchical models](@entry_id:274952) are a powerful framework for this. Instead of estimating parameters for each cell independently, a hierarchical model assumes that each cell's parameters are drawn from a shared, pack-level distribution.

This approach offers a powerful solution to [practical identifiability](@entry_id:190721) problems, a concept closely related to **transfer learning** in machine learning. Suppose we have limited or poor-quality data for a single "target" cell, making its aging parameters practically non-identifiable. By fitting a hierarchical model to data from the entire fleet, we can learn the population-level distribution of parameters. This learned distribution then serves as an informative prior (an "empirical Bayes" prior) for the estimation of the target cell's parameters. This informative prior regularizes the ill-posed estimation problem, dramatically reducing posterior variance and correlation, and allowing for robust [parameter estimation](@entry_id:139349) even from sparse data. This "partial pooling" of information across the fleet is a cornerstone of modern statistical analysis for population data .

### Conclusion

This chapter has journeyed through a wide array of applications, demonstrating that [identifiability analysis](@entry_id:182774) is a foundational and broadly applicable discipline. We have seen its central role in designing optimal experiments, choosing between competing models, and ensuring the credibility of parameter estimates. The principles are universal: whether analyzing the kinetics of a biological pathway, the degradation of an engineering asset, or the response of a watershed, a careful consideration of structural and [practical identifiability](@entry_id:190721) is essential for robust scientific inquiry and reliable engineering design. By moving beyond a simple "best-fit" mentality to a deeper understanding of what a given model and experiment can truly tell us, we elevate the quality and impact of our quantitative analysis.