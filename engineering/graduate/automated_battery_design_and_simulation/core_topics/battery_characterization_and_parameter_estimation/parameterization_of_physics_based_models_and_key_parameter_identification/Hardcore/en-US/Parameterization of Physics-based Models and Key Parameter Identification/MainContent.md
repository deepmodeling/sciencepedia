## Introduction
Physics-based models, which describe the intricate electrochemical processes inside a battery, are indispensable tools for accelerating battery design, optimizing performance, and predicting lifetime. These models, built from fundamental physical laws, hold the promise of unparalleled predictive power. However, their utility is entirely dependent on one critical step: parameterization. A model with inaccurate parameters is merely a mathematical curiosity; to transform it into a predictive engineering tool, we must assign accurate numerical values to its constituent parameters, from diffusion coefficients to [reaction rate constants](@entry_id:187887). This process of [parameter identification](@entry_id:275485) presents a significant challenge, bridging the gap between theoretical models and real-world experimental data.

This article provides a systematic journey through the theory and practice of parameterizing [physics-based battery models](@entry_id:1129654). It addresses the core problem of how to confidently determine parameter values from experimental measurements and leverage them for practical applications. Over the next three chapters, you will gain a deep, structured understanding of this vital discipline.

The journey begins in the **"Principles and Mechanisms"** chapter, where we will deconstruct a physics-based model into its fundamental components—parameters, states, inputs, and outputs. We will explore the physical meaning behind the most critical parameters governing battery performance and degradation. Crucially, this chapter introduces the formal concept of [identifiability](@entry_id:194150), asking whether it is even possible to uniquely determine these parameters and presenting the mathematical tools, like sensitivity analysis and the Fisher Information Matrix, needed to answer this question.

Next, the **"Applications and Interdisciplinary Connections"** chapter bridges theory and practice. We will investigate how specialized experimental techniques, such as GITT and Electrochemical Impedance Spectroscopy (EIS), are designed to probe and extract specific parameters. You will learn how to connect microscopic structural data from tomography to macroscopic model parameters, apply advanced statistical frameworks like Bayesian inference to quantify uncertainty, and see how these concepts culminate in the development of a battery Digital Twin—a living, adaptive model of a physical asset.

Finally, the **"Hands-On Practices"** chapter offers the opportunity to solidify your understanding through practical application. You will work through problems that guide you from deriving models for experimental analysis to using statistical methods for [robust estimation](@entry_id:261282) and designing optimal experiments, equipping you with the skills to tackle real-world parameterization challenges.

## Principles and Mechanisms

Physics-based models, such as the Doyle-Fuller-Newman (DFN) framework, provide a powerful lens for understanding and predicting the behavior of electrochemical systems like lithium-ion batteries. These models are constructed from fundamental conservation laws and [constitutive relations](@entry_id:186508), resulting in a system of coupled partial differential equations (PDEs) and algebraic constraints. To effectively utilize these models for design, control, and diagnostics, we must first develop a rigorous understanding of their mathematical structure and the physical meaning of their constituent parts. This chapter elucidates the core principles of parameterization, defining the essential components of a model and exploring the physical significance of key parameters. It then transitions to the critical question of whether these parameters can be uniquely determined from experimental data—a concept known as [identifiability](@entry_id:194150)—and introduces the mathematical tools required to analyze it.

### The Anatomy of a Physics-Based Model

Any dynamic system described by differential equations can be deconstructed into four fundamental categories of variables: parameters, states, inputs, and outputs. A precise understanding of these distinctions is the first step toward a systematic approach to modeling and parameterization. Let us define these categories based on their mathematical role within the initial [boundary value problem](@entry_id:138753) that constitutes the model. 

A **parameter** is a quantity that characterizes the intrinsic physical, chemical, or geometric properties of the system. In the mathematical formulation, parameters appear as coefficients within the governing differential and algebraic equations. They are typically assumed to be constant for the duration of a single simulation, though they may depend on external conditions like temperature or evolve slowly over long timescales due to degradation. Crucially, a parameter is not solved for as part of the system's dynamic evolution; it is a property of the model itself and does not require an initial condition for time integration.

A **state** (or state variable) is an internal variable whose evolution in time is governed by a differential equation. The collection of all state variables at a given moment—the state vector—provides a complete snapshot of the system's internal condition. To determine the system's trajectory, one must specify the initial value of every state variable at time $t=0$. Given these initial conditions and the system's inputs, the future evolution of all states is uniquely determined by integrating the governing equations. States are, by definition, dynamic.

An **input** (or [forcing function](@entry_id:268893)) is a time-varying quantity imposed on the system by an external agent or environment. It acts as a driver for the system's dynamics and is not solved for by the model's internal equations. In the context of battery models, inputs often appear as boundary conditions (e.g., applied current) or as source terms in the equations.

An **output** (or observable) is a quantity that can be measured from the system. Mathematically, it is defined by a function—typically algebraic—of the system's states, parameters, and possibly inputs at a given instant. An output does not have its own independent dynamics; its value is entirely determined by the other components of the model at that moment.

To make these definitions concrete, consider the DFN model.   The **[solid-phase diffusion](@entry_id:1131915) coefficient ($D_s$)**, **kinetic rate constant ($k$ or $i_0$)**, and **cation [transference number](@entry_id:262367) ($t^+$)** are material properties that appear as coefficients in the transport and reaction equations. They are therefore **parameters**. The **solid-phase lithium concentration ($c_s(r,x,t)$)** and **electrolyte concentration ($c_e(x,t)$)** are governed by mass conservation PDEs and require initial concentration profiles to begin a simulation. They are therefore **states**. The **applied current ($I_{\mathrm{app}}(t)$)** is externally set by the testing equipment and drives the cell's response, making it the **input** in galvanostatic operation. Finally, the **terminal voltage ($V(t)$)** is calculated from the potentials and overpotentials, which are themselves functions of the internal states. The voltage is the measured response of the system, making it the **output**.

### A Deeper Examination of Key Model Parameters

With the foundational concepts established, we now turn to a detailed examination of the most critical parameters in [battery models](@entry_id:1121428). We will explore their physical origins, their mathematical roles, and the phenomena they govern.

#### Thermodynamic Properties: The Open-Circuit Voltage

The thermodynamic driving force of a battery is its **open-circuit voltage (OCV)**, also known as the equilibrium potential. For a full cell, this is the difference between the equilibrium potentials of the positive and negative electrodes: $U_{\text{cell}} = U_p - U_n$. Each electrode's potential, $U_i(x_i)$, is a fundamental thermodynamic property of the active material, determined by the Gibbs free energy of the lithium intercalation reaction. It is a function of the material's **[stoichiometry](@entry_id:140916)**, $x_i$, which is the fraction of available lithium sites that are occupied. The stoichiometry is directly related to the volume-averaged solid-phase concentration, $\bar{c}_{s,i}$, by $x_i = \bar{c}_{s,i} / c_{s,\max}^i$, where $c_{s,\max}^i$ is the maximum possible lithium concentration in the material. The function $U_i(x_i)$ is independent of kinetics and is typically measured experimentally using quasi-equilibrium techniques like the Galvanostatic Intermittent Titration Technique (GITT) or Potentiostatic Intermittent Titration Technique (PITT). 

A crucial task in [model parameterization](@entry_id:752079) is to link the macroscopic cell-level **State of Charge (SOC)** to the microscopic electrode stoichiometries $x_p$ and $x_n$. This mapping is determined by the principle of lithium conservation and the relative capacities of the two electrodes. For a cell where the positive electrode is capacity-limiting, the stoichiometry of the positive electrode, $x_p$, will sweep its full operational range (e.g., from $x_p^{\min}$ to $x_p^{\max}$), while the negative electrode [stoichiometry](@entry_id:140916), $x_n$, will sweep only a fraction of its available range, scaled by the ratio of electrode capacities. This mapping, $s \to (x_p(s), x_n(s))$, is essential for initializing the concentration states of a physics-based model to correspond to a desired cell SOC. At open circuit, concentration gradients within particles are assumed to have relaxed, so the surface concentration equals the average concentration, allowing the average state defined by SOC to serve as a valid initial condition for the entire particle. 

#### Transport Properties in the Solid Phase

Perhaps the most well-known transport parameter is the **[solid-phase diffusion](@entry_id:1131915) coefficient ($D_s$)**. This parameter is the proportionality constant in Fick's first law, $J_r = -D_s \frac{\partial c_s}{\partial r}$, linking the flux of lithium ions within an active material particle to the concentration gradient. It governs the rate at which concentration gradients relax inside the particle, as described by Fick's second law:
$$ \frac{\partial c_s}{\partial t} = \nabla \cdot (D_s \nabla c_s) $$
For a spherical particle of radius $R_p$, this parameter defines a characteristic diffusion time scale, $\tau_d \sim R_p^2 / D_s$. This time scale represents the time required for lithium to diffuse across the particle. A higher $D_s$ implies faster diffusion, allowing the particle to sustain higher currents with smaller internal concentration gradients. Conversely, a low $D_s$ leads to significant [concentration polarization](@entry_id:266906) at the particle surface, which can limit [rate capability](@entry_id:1130583) and lead to premature voltage cutoffs. 

#### Transport Properties in the Electrolyte Phase

Transport in the liquid electrolyte is more complex, as it involves the coupled movement of cations, anions, and solvent molecules. In the [concentrated solution theory](@entry_id:1122829) that underpins the DFN model, this transport is described by three key concentration-dependent parameters. 

The **electrolyte diffusion coefficient ($D_e(c_e)$)** is the [chemical diffusion coefficient](@entry_id:197568) of the salt (e.g., LiPF$_6$). It quantifies the flux of neutral salt pairs driven by a gradient in the salt concentration, $\nabla c_e$. It is distinct from the tracer diffusivity of a single ion.

The **ionic conductivity ($\kappa(c_e)$)** describes the electrolyte's ability to conduct charge. It is the primary coefficient relating the [ionic current](@entry_id:175879) to the gradient in the electrolyte potential, $\nabla \phi_e$ (Ohm's law for ions). However, in concentrated solutions, $\kappa$ also appears in a cross-term showing that a concentration gradient can itself induce an ionic current, an effect known as the diffusion potential.

The **cation [transference number](@entry_id:262367) ($t_+^0(c_e)$)** is defined as the fraction of the total [ionic current](@entry_id:175879) carried by the cations (Li$^+$) in a reference frame moving with the solvent. It is a critical coupling parameter. It determines how much the overall ionic current contributes to the movement of salt (migration), and it also governs the magnitude of the aforementioned diffusion potential.

#### The Influence of Microstructure: Effective Transport Properties

The parameters $D_e$ and $\kappa$ are intrinsic properties of the bulk electrolyte liquid. However, within a porous electrode, ions must travel through a tortuous network of interconnected pores. This [complex geometry](@entry_id:159080) hinders transport. To account for this at the macroscopic level, we use **effective transport properties**. This homogenization is captured by two key microstructural parameters. 

The **porosity ($\varepsilon$)** is the [volume fraction](@entry_id:756566) of the electrode occupied by the electrolyte-filled pore space. It represents the reduced cross-sectional area available for transport.

The **tortuosity ($\tau$)** quantifies the degree to which the transport path is lengthened by the convoluted pore structure. A higher tortuosity implies a longer, more winding path for the ions to travel between two points.

The combined effect of porosity and tortuosity is to reduce the effective transport coefficients. A common empirical model for this reduction is the **Bruggeman relation**:
$$ D_{\text{eff}} = D_e \, \varepsilon^\beta \quad \text{and} \quad \kappa_{\text{eff}} = \kappa \, \varepsilon^\beta $$
Here, $\beta$ is the **Bruggeman exponent**, a parameter that encapsulates the geometric complexity of the pore network. For a random packing of spheres, $\beta$ is typically found to be around $1.5$. This power-law relationship provides a simple yet powerful way to connect the microstructural design of an electrode to its macroscopic transport performance.  

#### Interfacial Kinetics: The Butler-Volmer Equation

The rate at which lithium ions transfer between the electrolyte and the solid active material is governed by [electrochemical kinetics](@entry_id:155032) at the interface. The cornerstone model for this process is the **Butler-Volmer equation**, which is characterized by two fundamental kinetic parameters. 

The **[exchange current density](@entry_id:159311) ($i_0$)** represents the [rate of reaction](@entry_id:185114) at equilibrium. At zero overpotential, the forward (e.g., [intercalation](@entry_id:161533)) and reverse (deintercalation) reactions occur at equal and opposite rates. The magnitude of this rate, expressed as a current density, is $i_0$. It is a measure of the intrinsic catalytic activity of the electrode surface: a higher $i_0$ signifies a more kinetically facile reaction. It depends on temperature and the concentrations of reactants and products at the interface.

The **charge-transfer symmetry factors ($\alpha_a$ and $\alpha_c$)** are dimensionless numbers, typically between 0 and 1, that describe how the applied interfacial overpotential alters the activation energy barriers for the anodic (oxidation) and cathodic (reduction) reactions, respectively. They encode the asymmetry of the energy landscape of the transition state. For a single-step, one-electron transfer, it is common to assume $\alpha_a + \alpha_c = 1$. These factors determine the shape of the current-voltage curve in the nonlinear (high overpotential) regime, governing the Tafel slopes.

#### Degradation Mechanisms: The Solid Electrolyte Interphase (SEI)

On the negative electrode of a lithium-ion battery, a [passivation layer](@entry_id:160985) known as the **Solid Electrolyte Interphase (SEI)** forms from the reduction of electrolyte components. While essential for stability, the SEI is an ionic conductor and electronic insulator, and its presence and continued growth contribute to performance degradation. Modeling the SEI requires its own set of parameters. 

The **SEI resistance ($R_{\text{SEI}}$)** quantifies the impedance to lithium-[ion transport](@entry_id:273654) across this layer. For a film of uniform thickness $L_{\text{SEI}}$ and ionic conductivity $\kappa_{\text{SEI}}$, the resistance is given by $R_{\text{SEI}} = L_{\text{SEI}} / (A \cdot \kappa_{\text{SEI}})$, where $A$ is the interfacial area. This resistance contributes an additional [ohmic overpotential](@entry_id:262967), $\eta_{\text{SEI}} = I \cdot R_{\text{SEI}}$, which increases cell polarization and reduces efficiency.

The **SEI growth rate ($dL_{\text{SEI}}/dt$)** describes how the film thickens over time. Growth is modeled as a parasitic side reaction. According to Faraday's law, the rate of growth is proportional to the [molar flux](@entry_id:156263) of the [side reaction](@entry_id:271170), which is in turn proportional to the **side-reaction current density ($i_{\text{SEI}}$)**. The constant of proportionality involves the [molar volume](@entry_id:145604) of the SEI species ($V_m = M_{\text{SEI}}/\rho_{\text{SEI}}$). The side-reaction current itself is typically modeled using a charge-transfer kinetic expression, such as a Tafel equation, which depends on parameters like an [exchange current density](@entry_id:159311) and the local overpotential. The SEI thickness $L_{\text{SEI}}$ thus becomes a state variable in aging models.

### Parameter Identifiability: Can We Find the Numbers?

Defining the parameters of a model is only the first step. The ultimate goal is to assign them accurate numerical values, a process called parameter estimation or identification. This raises a fundamental question: given a specific experiment, can we uniquely determine the value of a parameter? This is the question of **[identifiability](@entry_id:194150)**.

#### Structural versus Practical Identifiability

It is crucial to distinguish between two levels of identifiability. 

**Structural identifiability** is an intrinsic property of the mathematical model and the chosen experimental setup (i.e., which inputs are controlled and which outputs are measured). A parameter is structurally identifiable if, given ideal, noise-free measurements over an infinitely informative experimental range, its value can be uniquely determined. If two different values of a parameter produce the exact same model output, the parameter is structurally unidentifiable.

**Practical identifiability** concerns the ability to estimate a parameter with acceptable precision from finite, noisy data obtained from a real-world experiment. A parameter that is structurally identifiable may be practically unidentifiable if the experimental data is not sufficiently sensitive to that parameter, or if its effect is highly correlated with the effect of another parameter.

A classic example illustrates this distinction. In a small-signal experiment like Electrochemical Impedance Spectroscopy (EIS), the system's response is linear. The [interfacial kinetics](@entry_id:1126605) are characterized by the charge-transfer resistance, $R_{ct}$, which is related to the [exchange current density](@entry_id:159311) by $R_{ct} = RT/(Fi_0)$ (assuming symmetric kinetics). The experiment can determine the value of $R_{ct}$ very well. Therefore, $R_{ct}$ is structurally identifiable. However, since the model output in this linear regime only depends on the specific combination $RT/(Fi_0)$, it is impossible to determine $i_0$ independently from this experiment alone. Thus, $i_0$ is structurally unidentifiable in a small-signal experiment. To identify $i_0$, one must use a large-signal experiment that probes the nonlinear Butler-Volmer kinetics. 

#### Parameter Confounding and Identifiable Groups

The issue with $i_0$ and $R_{ct}$ is a specific case of a broader phenomenon known as **parameter confounding**, where the model output depends only on a specific combination or group of parameters rather than on the individual parameters themselves. In such cases, only the group is structurally identifiable.

Another prominent example occurs when analyzing diffusion. Consider a GITT experiment on a single spherical particle. During the rest period, the voltage relaxes as the concentration gradient inside the particle, established by the preceding current pulse, dissipates. The time constant of this relaxation is found to be determined not by $D_s$ or $R_p$ alone, but by the ratio $\kappa = D_s/R_p^2$. A particle with a high diffusion coefficient and a large radius can exhibit the same relaxation time as a particle with a low diffusion coefficient and a small radius, as long as the ratio $D_s/R_p^2$ is the same. Therefore, from a single voltage relaxation measurement, only the group $D_s/R_p^2$ is identifiable. To separate $D_s$ and $R_p$, one would need additional information, such as an independent measurement of the particle radius $R_p$ from [microscopy](@entry_id:146696).  Similarly, in a porous electrode, the measured macroscopic kinetic response often depends on the product of the specific surface area $a_s$ and the intrinsic exchange current density $i_0$. Without knowing $a_s$, one can only identify the group $a_s i_0$. 

Observability can also be lost under specific operating conditions. For instance, the influence of the solid diffusion coefficient $D_s$ on the terminal voltage is transmitted through changes in the [surface concentration](@entry_id:265418), which in turn alter the [equilibrium potential](@entry_id:166921) $U(c_s)$. If an experiment is performed at a state of charge where the OCV curve is flat (i.e., $dU/dc_s = 0$), then changes in surface concentration will not produce a first-order change in voltage. In this situation, the effect of diffusion becomes unobservable in the voltage signal, and $D_s$ becomes locally unidentifiable. 

### Quantifying Identifiability: Sensitivity Analysis and Information Theory

To move beyond conceptual discussions, we need mathematical tools to quantify [parameter identifiability](@entry_id:197485). This is the domain of sensitivity analysis and information theory.

#### Local Sensitivity Analysis

The cornerstone of [identifiability analysis](@entry_id:182774) is the **local sensitivity**, which measures how much the model output changes in response to a small perturbation in a parameter. For a model with output vector $y(t)$ and parameter vector $\boldsymbol{\theta}$, the [sensitivity matrix](@entry_id:1131475) is defined as:
$$ \boldsymbol{S}(t) = \frac{\partial y(t)}{\partial \boldsymbol{\theta}} $$
Each element $S_{ij}(t) = \partial y_i(t) / \partial \theta_j$ quantifies the sensitivity of the $i$-th output to the $j$-th parameter at time $t$. A parameter with a consistently low sensitivity value across the experiment will be difficult to identify, as changes in its value have little effect on the measured output. 

Computing these sensitivities for complex DAE systems like the DFN model can be done in several ways. The most straightforward is the **[finite difference method](@entry_id:141078)**, where the model is simulated once with a nominal parameter value $\theta_j$ and again with a perturbed value $\theta_j + \delta$, and the derivative is approximated by the difference in the outputs. While simple to implement, this method is computationally expensive, requiring one or two additional full model simulations for each parameter. A more advanced and efficient technique, especially for models with many parameters, is the **adjoint method**. This method involves solving an additional "adjoint" system of DAEs backward in time. From the solution of this single [adjoint system](@entry_id:168877), the sensitivities of a specific output with respect to *all* model parameters can be calculated simultaneously, offering significant computational savings. 

#### The Fisher Information Matrix and the Cramér-Rao Bound

While sensitivity at a single point in time is informative, we need a way to aggregate this information over the entire duration of an experiment. This is provided by the **Fisher Information Matrix (FIM)**. For a time-series measurement with uncorrelated noise, the FIM, denoted $\mathcal{I}$, can be calculated from the sensitivity matrices and a weighting matrix $\boldsymbol{W}$ (which is typically the inverse of the noise covariance matrix):
$$ \mathcal{I} = \sum_{t=1}^{T} \boldsymbol{S}(t)^T \boldsymbol{W} \boldsymbol{S}(t) $$
The FIM is a square matrix whose size is the number of parameters. Each diagonal element represents the total information the experiment provides about a specific parameter, while the off-diagonal elements represent the correlations in the information about pairs of parameters. 

The profound importance of the FIM is revealed by the **Cramér-Rao Bound (CRB)**. This fundamental theorem of [estimation theory](@entry_id:268624) states that the inverse of the Fisher Information Matrix, $\mathcal{I}^{-1}$, provides a theoretical lower bound on the variance-covariance matrix of *any* [unbiased estimator](@entry_id:166722) for the parameters. Specifically, the variance of an estimate for the $k$-th parameter, $\hat{\theta}_k$, is bounded by the $k$-th diagonal element of the inverse FIM:
$$ \text{Var}(\hat{\theta}_k) \ge (\mathcal{I}^{-1})_{kk} = \left( \left( \sum_{t=1}^{T} \boldsymbol{S}(t)^T \boldsymbol{W} \boldsymbol{S}(t) \right)^{-1} \right)_{kk} $$
This powerful result connects the abstract concept of identifiability to a concrete, quantitative measure. A parameter is practically identifiable if this variance bound is small. If the FIM is singular (non-invertible), it implies that at least one parameter or combination of parameters is structurally unidentifiable. By analyzing the FIM, we can diagnose identifiability issues, understand trade-offs between parameters, and even design experiments that maximize the [information content](@entry_id:272315), thereby minimizing the uncertainty of our final parameter estimates.