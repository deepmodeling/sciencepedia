{
    "hands_on_practices": [
        {
            "introduction": "Before employing a mathematical function as a probability density function (PDF), we must ensure it adheres to the fundamental axioms of probability theory. This initial practice  lays the essential groundwork by guiding you through the process of normalizing the continuous power-law distribution. By deriving the normalization constant $C$ and the crucial condition on the exponent $\\alpha$, you will construct a valid PDF and its corresponding survival function, a tool indispensable for both theoretical analysis and empirical data visualization.",
            "id": "4137150",
            "problem": "In a model of cascading failures in a heterogeneous infrastructure network, suppose the event size $x$ is represented as a continuous random variable with a power-law probability density function (PDF) for $x \\in [x_{\\min}, \\infty)$, where $x_{\\min}  0$ is a known lower detection threshold. The PDF has the parametric form $p(x) = C x^{-\\alpha}$ for some exponent $\\alpha  0$ and normalization constant $C  0$. Using only the axioms of probability and the definition of the survival function (also called the complementary cumulative distribution function (CCDF)), derive the normalization constant $C$ required for $p(x)$ to be a valid PDF on $[x_{\\min}, \\infty)$, and then express the survival function $S(x) = \\mathbb{P}(X \\geq x)$ for all real $x$. Explicitly state any necessary and sufficient conditions on $\\alpha$ for $C$ to exist. Your final expressions must be in terms of $\\alpha$ and $x_{\\min}$ only. The final answer should be given as exact analytic expressions; no numerical approximation or rounding is required.",
            "solution": "The problem requires the derivation of the normalization constant $C$, the necessary and sufficient conditions on the exponent $\\alpha$ for the existence of $C$, and the survival function $S(x)$ for a power-law probability density function (PDF) $p(x) = C x^{-\\alpha}$ defined on the domain $[x_{\\min}, \\infty)$, where $x_{\\min}  0$.\n\nFirst, we address the normalization of the PDF. For $p(x)$ to be a valid PDF, it must satisfy the axiom of total probability, which states that the integral of the PDF over its entire domain of support must be equal to $1$. The domain of support for the random variable $X$ is given as $[x_{\\min}, \\infty)$.\n\nThe normalization condition is:\n$$ \\int_{x_{\\min}}^{\\infty} p(x) dx = 1 $$\nSubstituting the given form of the PDF, $p(x) = C x^{-\\alpha}$, we have:\n$$ \\int_{x_{\\min}}^{\\infty} C x^{-\\alpha} dx = 1 $$\nSince $C$ is a constant, we can factor it out of the integral:\n$$ C \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = 1 $$\nTo find $C$, we must first evaluate the improper integral. The integral is defined as a limit:\n$$ \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = \\lim_{b \\to \\infty} \\int_{x_{\\min}}^{b} x^{-\\alpha} dx $$\nWe evaluate the definite integral. We must consider two cases based on the value of $\\alpha$. The problem states $\\alpha  0$.\n\nCase 1: $\\alpha = 1$.\nThe integral becomes:\n$$ \\lim_{b \\to \\infty} \\int_{x_{\\min}}^{b} x^{-1} dx = \\lim_{b \\to \\infty} [\\ln(x)]_{x_{\\min}}^{b} = \\lim_{b \\to \\infty} (\\ln(b) - \\ln(x_{\\min})) $$\nSince $\\lim_{b \\to \\infty} \\ln(b) = \\infty$, the integral diverges. Therefore, a normalizable PDF does not exist for $\\alpha = 1$.\n\nCase 2: $\\alpha \\neq 1$.\nThe antiderivative of $x^{-\\alpha}$ is $\\frac{x^{-\\alpha+1}}{-\\alpha+1}$. The integral becomes:\n$$ \\lim_{b \\to \\infty} \\left[ \\frac{x^{1-\\alpha}}{1-\\alpha} \\right]_{x_{\\min}}^{b} = \\lim_{b \\to \\infty} \\left( \\frac{b^{1-\\alpha}}{1-\\alpha} - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha} \\right) $$\nThe convergence of this limit depends on the sign of the exponent $1-\\alpha$.\n- If $1-\\alpha  0$ (i.e., $0  \\alpha  1$), then $\\lim_{b \\to \\infty} b^{1-\\alpha} = \\infty$, and the integral diverges.\n- If $1-\\alpha  0$ (i.e., $\\alpha  1$), then $\\lim_{b \\to \\infty} b^{1-\\alpha} = \\lim_{b \\to \\infty} \\frac{1}{b^{\\alpha-1}} = 0$, since $\\alpha-1  0$. In this case, the integral converges.\n\nTherefore, the necessary and sufficient condition for the normalization constant $C$ to exist (i.e., for the integral to be finite and non-zero) is $\\alpha  1$.\n\nUnder the condition $\\alpha  1$, the value of the integral is:\n$$ \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = 0 - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha} = \\frac{-x_{\\min}^{1-\\alpha}}{-(\\alpha-1)} = \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1} $$\nNow we use this result in the normalization equation:\n$$ C \\left( \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1} \\right) = 1 $$\nSolving for the normalization constant $C$, we obtain:\n$$ C = \\frac{\\alpha-1}{x_{\\min}^{1-\\alpha}} = (\\alpha-1)x_{\\min}^{\\alpha-1} $$\n\nNext, we derive the survival function $S(x) = \\mathbb{P}(X \\geq x)$. This is defined for all real $x$ by the integral of the PDF from $x$ to $\\infty$:\n$$ S(x) = \\int_{x}^{\\infty} p(t) dt $$\nWe must remember that the PDF $p(t)$ is defined as $p(t) = C t^{-\\alpha}$ for $t \\geq x_{\\min}$ and $p(t) = 0$ for $t  x_{\\min}$. We consider two regimes for the value of $x$.\n\nCase A: $x \\leq x_{\\min}$.\nThe integral for $S(x)$ is split at $x_{\\min}$:\n$$ S(x) = \\int_{x}^{\\infty} p(t) dt = \\int_{x}^{x_{\\min}} p(t) dt + \\int_{x_{\\min}}^{\\infty} p(t) dt $$\nSince $p(t) = 0$ for $t  x_{\\min}$, the first integral is $0$. The second integral is the total probability, which is $1$.\n$$ S(x) = \\int_{x}^{x_{\\min}} 0 \\cdot dt + 1 = 0 + 1 = 1 $$\nSo, for $x \\leq x_{\\min}$, the survival function is $S(x) = 1$.\n\nCase B: $x  x_{\\min}$.\nIn this regime, the lower limit of integration $x$ is within the support of the PDF.\n$$ S(x) = \\int_{x}^{\\infty} p(t) dt = \\int_{x}^{\\infty} C t^{-\\alpha} dt $$\nThis integral is evaluated similarly to the normalization integral, assuming $\\alpha  1$:\n$$ S(x) = C \\left[ \\frac{t^{1-\\alpha}}{1-\\alpha} \\right]_{x}^{\\infty} = C \\left( \\lim_{b \\to \\infty} \\frac{b^{1-\\alpha}}{1-\\alpha} - \\frac{x^{1-\\alpha}}{1-\\alpha} \\right) $$\nAs before, the limit term is $0$ because $\\alpha  1$.\n$$ S(x) = C \\left( - \\frac{x^{1-\\alpha}}{1-\\alpha} \\right) = C \\frac{x^{1-\\alpha}}{\\alpha-1} $$\nSubstituting the derived expression for $C = (\\alpha-1)x_{\\min}^{\\alpha-1}$:\n$$ S(x) = \\left( (\\alpha-1)x_{\\min}^{\\alpha-1} \\right) \\frac{x^{1-\\alpha}}{\\alpha-1} $$\n$$ S(x) = x_{\\min}^{\\alpha-1} x^{1-\\alpha} = \\frac{x_{\\min}^{\\alpha-1}}{x^{\\alpha-1}} = \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha-1} $$\n\nCombining the results for both cases, the complete survival function $S(x)$ for all real $x$ is given by the piecewise function:\n$$ S(x) = \\begin{cases} 1  \\text{for } x \\leq x_{\\min} \\\\ \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha-1}  \\text{for } x  x_{\\min} \\end{cases} $$\nThis result is valid under the necessary and sufficient condition that $\\alpha  1$.\n\nIn summary, the necessary and sufficient condition for the existence of a valid normalization constant is $\\alpha  1$. Given this condition, the normalization constant is $C = (\\alpha-1)x_{\\min}^{\\alpha-1}$, and the survival function is the piecewise function derived above.",
            "answer": "$$\n\\boxed{\n\\pmatrix{\n(\\alpha - 1) x_{\\min}^{\\alpha - 1}  \n\\begin{cases} 1  \\text{for } x \\leq x_{\\min} \\\\ \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha-1}  \\text{for } x  x_{\\min} \\end{cases}\n}\n}\n$$"
        },
        {
            "introduction": "With a mathematically valid model in hand, the next step is to connect it to real-world observations. This exercise  introduces the principle of Maximum Likelihood Estimation (MLE), a powerful and widely used statistical method for determining the model parameters that best explain a given dataset. You will derive the celebrated closed-form estimator for the scaling exponent $\\alpha$, a fundamental result that forms the backbone of quantitative analysis for countless scale-free systems.",
            "id": "4137125",
            "problem": "In a complex adaptive system such as a communication network or an ecosystem, extreme events (for example, cascade sizes or city sizes) are often observed to follow a heavy-tailed distribution above a lower threshold. Suppose a researcher has collected $n$ independent and identically distributed (i.i.d.) observations $\\{x_i\\}_{i=1}^{n}$ from the tail of a continuous distribution that exhibits scale invariance above a known lower bound $x_{\\min}$. Specifically, it is known that for $x \\ge x_{\\min}$ the probability density function (PDF) is proportional to $x^{-\\alpha}$ for some unknown tail exponent $\\alpha  1$, and zero otherwise. The researcher models the tail as a properly normalized continuous power-law (Pareto-type) distribution on $[x_{\\min}, \\infty)$ and seeks to estimate $\\alpha$ by the principle of maximum likelihood.\n\nStarting from first principles—namely, the definition of a PDF and its normalization over $[x_{\\min}, \\infty)$, the construction of the likelihood for i.i.d. samples, and the maximization of the log-likelihood with respect to the parameter—derive the closed-form expression for the maximum likelihood estimator (MLE) $\\hat{\\alpha}$ in terms of $n$, $x_{\\min}$, and the observed $\\{x_i\\}_{i=1}^{n}$. You must explicitly determine the normalization constant of the tail PDF prior to forming the likelihood. Ensure that your derivation verifies that the solution corresponds to a global maximum under the constraint $\\alpha  1$.\n\nYour final answer must be a single closed-form analytic expression for $\\hat{\\alpha}$ in terms of $n$, $x_{\\min}$, and $\\{x_i\\}_{i=1}^{n}$. No numerical evaluation is required. Do not use any pre-packaged formulas or intermediate results beyond the foundational definitions and properties stated above. No rounding is needed, and no units are required.",
            "solution": "The task is to derive the maximum likelihood estimator (MLE) for the tail exponent $\\alpha$ of a continuous power-law distribution. The probability density function (PDF) is given to be proportional to $x^{-\\alpha}$ for $x \\ge x_{\\min}$ with a lower bound $x_{\\min}  0$, and zero otherwise. The parameter $\\alpha$ is constrained to be greater than $1$.\n\nFirst, we must determine the normalization constant for the PDF. Let the PDF be $p(x | \\alpha, x_{\\min}) = C x^{-\\alpha}$ for $x \\ge x_{\\min}$, where $C$ is the normalization constant. For $p(x)$ to be a valid PDF, its integral over its support must equal $1$.\n$$\n\\int_{x_{\\min}}^{\\infty} p(x | \\alpha, x_{\\min}) dx = 1\n$$\nSubstituting the form of the PDF, we have:\n$$\n\\int_{x_{\\min}}^{\\infty} C x^{-\\alpha} dx = 1\n$$\nWe can pull the constant $C$ out of the integral:\n$$\nC \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = 1\n$$\nThe integral is evaluated as follows:\n$$\n\\int x^{-\\alpha} dx = \\frac{x^{-\\alpha+1}}{-\\alpha+1} + \\text{constant}\n$$\nSince the problem states $\\alpha  1$, the exponent $-\\alpha+1$ is negative. This ensures that the integral converges as $x \\to \\infty$. Evaluating the definite integral:\n$$\n\\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx = \\left[ \\frac{x^{1-\\alpha}}{1-\\alpha} \\right]_{x_{\\min}}^{\\infty} = \\lim_{b \\to \\infty} \\left( \\frac{b^{1-\\alpha}}{1-\\alpha} \\right) - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha}\n$$\nAs $b \\to \\infty$ and $1-\\alpha  0$, the term $b^{1-\\alpha} \\to 0$. The integral becomes:\n$$\n0 - \\frac{x_{\\min}^{1-\\alpha}}{1-\\alpha} = \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1}\n$$\nSubstituting this back into the normalization equation:\n$$\nC \\left( \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1} \\right) = 1\n$$\nSolving for $C$, we find the normalization constant:\n$$\nC = (\\alpha-1) x_{\\min}^{\\alpha-1}\n$$\nThus, the properly normalized PDF for the power-law tail is:\n$$\np(x | \\alpha, x_{\\min}) = (\\alpha-1) x_{\\min}^{\\alpha-1} x^{-\\alpha} \\quad \\text{for } x \\ge x_{\\min}\n$$\n\nNext, we construct the likelihood function for a set of $n$ independent and identically distributed (i.i.d.) observations $\\{x_i\\}_{i=1}^{n}$, where each $x_i \\ge x_{\\min}$. The likelihood function $L(\\alpha | \\{x_i\\})$ is the product of the individual probability densities evaluated at each observation:\n$$\nL(\\alpha | \\{x_i\\}) = \\prod_{i=1}^{n} p(x_i | \\alpha, x_{\\min})\n$$\nSubstituting the expression for the PDF:\n$$\nL(\\alpha) = \\prod_{i=1}^{n} \\left[ (\\alpha-1) x_{\\min}^{\\alpha-1} x_i^{-\\alpha} \\right]\n$$\nWe can separate the terms that depend on $\\alpha$ and the data:\n$$\nL(\\alpha) = \\left[ (\\alpha-1) x_{\\min}^{\\alpha-1} \\right]^n \\left( \\prod_{i=1}^{n} x_i \\right)^{-\\alpha}\n$$\n\nTo simplify the maximization procedure, we work with the log-likelihood function, $\\mathcal{L}(\\alpha) = \\ln(L(\\alpha))$. Taking the natural logarithm of the likelihood function gives:\n$$\n\\mathcal{L}(\\alpha) = \\ln \\left( \\left[ (\\alpha-1) x_{\\min}^{\\alpha-1} \\right]^n \\right) + \\ln \\left( \\left( \\prod_{i=1}^{n} x_i \\right)^{-\\alpha} \\right)\n$$\nUsing the properties of logarithms, $\\ln(a^b) = b\\ln(a)$ and $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$\n\\mathcal{L}(\\alpha) = n \\ln\\left( (\\alpha-1) x_{\\min}^{\\alpha-1} \\right) - \\alpha \\ln\\left( \\prod_{i=1}^{n} x_i \\right)\n$$\n$$\n\\mathcal{L}(\\alpha) = n \\left[ \\ln(\\alpha-1) + \\ln(x_{\\min}^{\\alpha-1}) \\right] - \\alpha \\sum_{i=1}^{n} \\ln(x_i)\n$$\n$$\n\\mathcal{L}(\\alpha) = n \\ln(\\alpha-1) + n(\\alpha-1) \\ln(x_{\\min}) - \\alpha \\sum_{i=1}^{n} \\ln(x_i)\n$$\nTo find the maximum likelihood estimator $\\hat{\\alpha}$, we differentiate $\\mathcal{L}(\\alpha)$ with respect to $\\alpha$ and set the result to zero.\n$$\n\\frac{d\\mathcal{L}}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ n \\ln(\\alpha-1) + n\\alpha \\ln(x_{\\min}) - n \\ln(x_{\\min}) - \\alpha \\sum_{i=1}^{n} \\ln(x_i) \\right]\n$$\n$$\n\\frac{d\\mathcal{L}}{d\\alpha} = \\frac{n}{\\alpha-1} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i)\n$$\nSetting this derivative to zero to find the critical point, which we denote $\\hat{\\alpha}$:\n$$\n\\frac{n}{\\hat{\\alpha}-1} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) = 0\n$$\nNow, we solve for $\\hat{\\alpha}$:\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n} \\ln(x_i) - n \\ln(x_{\\min})\n$$\nThe right-hand side can be simplified using the property $n \\ln(a) = \\ln(a^n) = \\sum_{i=1}^n \\ln(a)$:\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n} \\ln(x_i) - \\sum_{i=1}^{n} \\ln(x_{\\min}) = \\sum_{i=1}^{n} \\left[ \\ln(x_i) - \\ln(x_{\\min}) \\right]\n$$\nUsing the property $\\ln(a) - \\ln(b) = \\ln(a/b)$:\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)\n$$\nInverting both sides:\n$$\n\\frac{\\hat{\\alpha}-1}{n} = \\frac{1}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\n$$\n\\hat{\\alpha}-1 = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\nFinally, the maximum likelihood estimator for $\\alpha$ is:\n$$\n\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\nTo verify that this corresponds to a maximum, we compute the second derivative of the log-likelihood function:\n$$\n\\frac{d^2\\mathcal{L}}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left( \\frac{n}{\\alpha-1} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) \\right) = \\frac{d}{d\\alpha} \\left( n(\\alpha-1)^{-1} \\right) = -n(\\alpha-1)^{-2}\n$$\n$$\n\\frac{d^2\\mathcal{L}}{d\\alpha^2} = -\\frac{n}{(\\alpha-1)^2}\n$$\nSince $n$ (the number of samples) is positive and $(\\alpha-1)^2$ is positive for any $\\alpha \\neq 1$, the second derivative $\\frac{d^2\\mathcal{L}}{d\\alpha^2}$ is always negative for any value of $\\alpha$ in its domain $(\\alpha  1)$. This indicates that the log-likelihood function is strictly concave, and thus the critical point we found is a unique global maximum. Since each $x_i \\ge x_{\\min}$, we have $\\frac{x_i}{x_{\\min}} \\ge 1$ and $\\ln(\\frac{x_i}{x_{\\min}}) \\ge 0$. As long as at least one observation $x_i$ is strictly greater than $x_{\\min}$ (which is expected for a continuous distribution), the sum in the denominator will be positive, ensuring that $\\hat{\\alpha}  1$, which is consistent with the initial parameter constraint.",
            "answer": "$$\n\\boxed{1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}}\n$$"
        },
        {
            "introduction": "Moving from idealized models to the complexities of empirical data requires a comprehensive and rigorous workflow. Real-world analysis demands that we not only estimate model parameters but also determine the domain over which the model is valid and, most importantly, test whether the model is a plausible description of the data. This capstone practice  delves into the Clauset-Shalizi-Newman procedure, a gold-standard methodology that combines the estimation of the lower bound $x_{\\min}$ with a robust goodness-of-fit test, equipping you with the skills to perform defensible and nuanced analysis of power-law phenomena.",
            "id": "4137200",
            "problem": "A population of interactions in a large adaptive network produces positive-valued observations $\\{x_i\\}$ representing event sizes that are hypothesized to follow a scale-free tail above an unknown threshold $x_{\\min}$. In complex adaptive systems, heavy-tailed phenomena often arise due to aggregation, feedback, and heterogeneity, which motivates modeling the upper tail by a continuous power-law (Pareto) distribution with exponent $\\alpha$ for $x \\ge x_{\\min}$. Consider the problem of estimating $x_{\\min}$ and $\\alpha$ and assessing goodness-of-fit using standard statistical definitions, including the likelihood for parametric models, the cumulative distribution function (CDF), and the Kolmogorov–Smirnov (KS) statistic. Which option best describes the Clauset–Shalizi–Newman procedure for this task and correctly justifies the use of the KS distance?\n\nA. For each candidate threshold $x_{\\min}$ drawn from the observed $\\{x_i\\}$, estimate $\\alpha$ by Maximum Likelihood Estimation (MLE) using the subsample $\\{x_i : x_i \\ge x_{\\min}\\}$, form the model CDF $F(x \\mid \\alpha, x_{\\min})$ on $x \\ge x_{\\min}$, compute the Kolmogorov–Smirnov (KS) distance $D = \\sup_{x \\ge x_{\\min}} |S(x) - F(x \\mid \\alpha, x_{\\min})|$ where $S(x)$ is the empirical CDF of the tail, and select $x_{\\min}$ that minimizes $D$. After selecting $x_{\\min}$, re-estimate $\\alpha$ by MLE on the tail and assess goodness-of-fit by a parametric bootstrap: repeatedly simulate synthetic datasets from $F(x \\mid \\hat{\\alpha}, \\hat{x}_{\\min})$, re-estimate $\\alpha$ and $x_{\\min}$ for each, compute their KS statistics, and obtain a $p$-value by the fraction of synthetic KS distances exceeding the empirical $D$. The KS distance is justified because it is a nonparametric, binning-free supremum norm on the CDF that is sensitive to discrepancies across all scales, and, under a continuous null, its reference distribution is distribution-free when parameters are known; when parameters are estimated, parametric bootstrap restores valid testing.\n\nB. Choose $x_{\\min}$ as the smallest observation, estimate $\\alpha$ by linear regression of $\\log$-frequency on $\\log x$ over all $\\{x_i\\}$, and compute a single KS statistic on the full sample $\\{x_i\\}$ without restricting to $x \\ge x_{\\min}$. Use the asymptotic KS critical values directly for a $p$-value. The KS distance is justified because it is computationally simpler than alternative metrics and because linear regression on $\\log$-$\\log$ provides unbiased slope estimates for power laws.\n\nC. Select $x_{\\min}$ by minimizing the Pearson $\\chi^2$ statistic of a log-binned histogram of $\\{x_i\\}$, estimate $\\alpha$ by the method of moments on the tail, and use the KS statistic because it is robust to dependence among observations and emphasizes tail deviations. The $p$-value is computed from the standard KS tables without adjustment for parameter estimation or threshold selection.\n\nD. Jointly choose $(x_{\\min}, \\alpha)$ by maximizing the total likelihood over all $\\{x_i\\}$, including those below $x_{\\min}$, then validate the fit using the Anderson–Darling (AD) statistic but refer to it as KS for convenience. Justify the use of the KS distance on the grounds that it upweights the tails relative to the center and therefore is strictly more powerful for detecting tail discrepancies in heavy-tailed data.\n\nE. Determine $x_{\\min}$ by minimizing the mean squared error between the empirical complementary cumulative distribution function (CCDF) and a fitted power-law CCDF over $x \\ge x_{\\min}$, estimate $\\alpha$ by least squares on $\\log$-CCDF, and compute a single KS statistic on the chosen tail. Justify the KS distance mainly because it is fast to compute, with its reference distribution unaffected by parameter estimation or threshold selection.",
            "solution": "The Clauset–Shalizi–Newman (CSN) procedure is a three-step method for fitting power-law distributions to empirical data and assessing the plausibility of the fit.\n\n**1. Estimation of the Power-Law Exponent $\\alpha$ for a Fixed $x_{\\min}$**\nFor a continuous variable $x$ that follows a power-law distribution for $x \\ge x_{\\min}$, the maximum likelihood estimator (MLE) for the exponent $\\alpha$ is:\n$$ \\hat{\\alpha} = 1 + n \\left( \\sum_{i=1}^{n} \\ln \\frac{x_i}{x_{\\min}} \\right)^{-1} $$\nwhere the sum is over the $n$ data points such that each $x_i \\ge x_{\\min}$. This estimator is known to be consistent and minimally biased for a fixed $x_{\\min}$.\n\n**2. Estimation of the Lower Bound $x_{\\min}$**\nThe CSN method treats $x_{\\min}$ as an unknown parameter to be estimated from the data. The procedure is as follows:\n- For each possible value of $x_{\\min}$ (typically, each unique value in the dataset), the subset of data $\\{x_i : x_i \\ge x_{\\min}\\}$ is considered.\n- For this subset, the MLE $\\hat{\\alpha}$ is calculated. This gives a candidate power-law model defined by the parameters $(x_{\\min}, \\hat{\\alpha})$.\n- The quality of this model is assessed by measuring the distance between the empirical data distribution and the theoretical power-law model using the Kolmogorov–Smirnov (KS) statistic, $D$:\n$$ D = \\sup_{x \\ge x_{\\min}} |S(x) - P(x)| $$\nwhere $S(x)$ is the empirical CDF of the data for $x \\ge x_{\\min}$, and $P(x)$ is the CDF of the fitted power-law model.\n- The optimal value for the lower bound, $\\hat{x}_{\\min}$, is the one that minimizes this distance $D$.\n\n**3. Goodness-of-Fit Testing**\nAfter estimating $\\hat{x}_{\\min}$ and the corresponding $\\hat{\\alpha}$, one must assess whether the power-law model is a plausible description of the data. A small value of the KS statistic $D$ is not, by itself, sufficient evidence, because the parameters were estimated from the data itself, which invalidates the standard statistical tables for the KS statistic.\n- The CSN method resolves this by using a **parametric bootstrap** to generate a $p$-value. The procedure is:\n    a. Calculate the KS statistic, $D_{\\text{empirical}}$, for the original data against its fitted model $(\\hat{x}_{\\min}, \\hat{\\alpha})$.\n    b. Generate a large number of synthetic datasets drawn from the fitted power-law distribution with parameters $(\\hat{x}_{\\min}, \\hat{\\alpha})$.\n    c. For *each* synthetic dataset, repeat the *entire* fitting procedure: estimate its own $(\\hat{x}_{\\min}^{\\text{syn}}, \\hat{\\alpha}^{\\text{syn}})$ by minimizing the KS statistic, just as was done for the real data.\n    d. Calculate the KS statistic for this synthetic dataset against *its own fitted model*, $D_{\\text{syn}}$.\n    e. The $p$-value is the fraction of synthetic datasets for which $D_{\\text{syn}} \\ge D_{\\text{empirical}}$. A high $p$-value (e.g., $p  0.1$) indicates that the model is a plausible fit.\n\n**Evaluation of Options**\n\n*   **A.** This option correctly describes all three steps of the CSN procedure: estimating $\\alpha$ via MLE for each candidate $x_{\\min}$, selecting $x_{\\min}$ by minimizing the KS distance, and using a parametric bootstrap (which involves re-estimating both parameters for each synthetic dataset) to assess goodness-of-fit. The justification for using the KS statistic is also correct: it is non-parametric, binning-free, and its use in a goodness-of-fit test is made valid by the bootstrap procedure.\n*   **B.** This option describes statistically unsound methods (log-log regression for $\\alpha$, using standard KS tables) that are explicitly warned against in the CSN paper.\n*   **C.** This option proposes using a $\\chi^2$ statistic and the method of moments, which are not part of the CSN method. It also makes incorrect claims about the KS test.\n*   **D.** This option incorrectly describes the estimation of $x_{\\min}$ and conflates the KS and Anderson-Darling statistics.\n*   **E.** This option describes using least squares, not MLE, and incorrectly claims the KS distribution is unaffected by parameter estimation, which is a critical statistical error the CSN bootstrap method is designed to correct.\n\nTherefore, option A is the only accurate and complete description of the Clauset–Shalizi–Newman procedure.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}