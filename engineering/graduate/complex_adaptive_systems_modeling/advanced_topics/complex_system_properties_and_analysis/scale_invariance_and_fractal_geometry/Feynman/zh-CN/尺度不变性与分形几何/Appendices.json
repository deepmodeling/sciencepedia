{
    "hands_on_practices": [
        {
            "introduction": "分形维数描述了空间被填充的方式，而空隙度（lacunarity）则量化了模式的纹理或“空隙性”。本实践将指导您实现滑箱算法（gliding-box algorithm）来测量空隙度。通过这种方法，您将能够从计算上区分确定性分形（deterministic fractal）的统一结构、完全均匀场（uniform field）的同质性以及逾渗模型（percolation model）的聚集随机性 。",
            "id": "4141583",
            "problem": "您的任务是设计并实现一个完整的、可运行的程序，该程序使用滑动盒算法计算空隙度，并量化分形纹理代表性的合成二值图像中空隙度如何跨尺度变化。其目的是将计算和解释建立在复杂适应系统（CAS）中标度不变性和分形几何学的原理之上。您必须遵守下文所述的精确定义，并从第一性原理出发实现该算法。\n\n从以下基本基础开始：\n\n- 考虑一个二元占据场 $I \\in \\{0,1\\}^{N \\times N}$，其中 $I_{x,y} = 1$ 表示占据，而 $I_{x,y} = 0$ 表示为空。这种二元场通常用于模拟复杂适应系统（CAS）中的空间模式，其中异构聚类和尺度依赖结构由局部相互作用产生。\n- 对于给定的盒尺寸 $r \\in \\mathbb{N}$，将滑动盒质量 $M_{i,j}(r)$ 定义为左上角位于 $(i,j)$ 的 $r \\times r$ 窗口内的占据总和：\n$$\nM_{i,j}(r) = \\sum_{u=0}^{r-1}\\sum_{v=0}^{r-1} I_{i+u,j+v},\n$$\n对于所有使 $r \\times r$ 窗口完全位于图像内的 $(i,j)$，即 $0 \\le i \\le N - r$ 和 $0 \\le j \\le N - r$。\n- 将这些质量在所有有效 $(i,j)$ 上的尺度相关均值和二阶矩定义为：\n$$\n\\mu_r = \\mathbb{E}[M(r)] = \\frac{1}{(N - r + 1)^2} \\sum_{i=0}^{N-r} \\sum_{j=0}^{N-r} M_{i,j}(r),\n$$\n$$\n\\mathbb{E}[M(r)^2] = \\frac{1}{(N - r + 1)^2} \\sum_{i=0}^{N-r} \\sum_{j=0}^{N-r} M_{i,j}(r)^2.\n$$\n- 尺度 $r$ 上的空隙度定义为：\n$$\n\\Lambda(r) = \\frac{\\mathbb{E}[M(r)^2]}{\\mu_r^2}.\n$$\n该定义等价于 $1 + \\left(\\frac{\\sigma_r}{\\mu_r}\\right)^2$，其中 $\\sigma_r^2$ 是 $M(r)$ 的方差，它量化了尺度 $r$ 上质量分布的平移不变性程度。对于完全均匀的场，所有 $r$ 的 $\\Lambda(r) = 1$。对于异构或聚类的场，$\\Lambda(r) > 1$，对于许多自相似分形，该值通常随 $r$ 的增加而减小。\n- 为总结尺度依赖性，在对数空间中拟合一个线性模型：\n$$\n\\log \\Lambda(r) \\approx \\alpha \\log r + \\beta,\n$$\n并使用普通最小二乘法对一组 $r$ 值的点对 $(\\log r, \\log \\Lambda(r))$ 估计斜率 $\\hat{\\alpha}$ 和决定系数 $\\hat{R}^2$。\n\n您的程序必须：\n\n1. 构建指定的合成二值图像：\n   - 一个 Sierpiński 地毯分形图像，使用一个 $3 \\times 3$ 的生成元，其中中心单元为空，所有其他单元被占据，迭代指定次数以产生一个大小为 $3^k \\times 3^k$（$k$为整数）的自相似二值模式。形式上，如果 $G \\in \\{0,1\\}^{3 \\times 3}$ 由 $G_{1,1}=0$ 且对于 $(i,j) \\neq (1,1)$ 有 $G_{i,j}=1$ 给出，则 $G$ 与自身的 $k$ 次 Kronecker 积产生一个深度为 $k$ 的 Sierpiński 地毯。\n   - 一个所有条目都等于 $1$ 的均匀图像。\n   - 一个方形晶格上的位点逾渗图像，其中每个位点以概率 $p$ 独立占据，使用固定的随机种子生成以保证可复现性。\n2. 实现滑动盒算法，为所有有效的 $(i,j)$ 计算 $M_{i,j}(r)$，并为一组盒尺寸 $r$ 精确计算如上定义的 $\\Lambda(r)$。\n3. 对每张图像，计算：\n   - 所有测试盒尺寸对应的空隙度值列表 $[\\Lambda(r_1), \\Lambda(r_2), \\dots]$。\n   - 从 $\\log \\Lambda(r)$ 对 $\\log r$ 的线性回归中得到的估计斜率 $\\hat{\\alpha}$ 和决定系数 $\\hat{R}^2$。\n   - 一个布尔值，指示空隙度是否跨尺度非递增，即对于所有连续的测试尺度 $r_i$，是否有 $\\Lambda(r_{i+1}) \\le \\Lambda(r_i)$（对计算出的浮点值使用精确比较）。\n\n使用以下参数值的测试套件：\n- 测试用例1（正常路径，自相似分形）：深度 $k=5$ 的 Sierpiński 地毯，生成图像大小 $N=3^5=243$。使用盒尺寸 $r \\in \\{1,3,9,27,81\\}$。\n- 测试用例2（边界条件，均匀场）：大小为 $N=243$ 的均匀图像。使用盒尺寸 $r \\in \\{1,3,9,27,81\\}$。\n- 测试用例 3（边缘情况，临界点附近的无序簇）：位点逾渗，图像大小 $N=256$，占据概率 $p=0.6$，随机种子固定为 $12345$。使用盒尺寸 $r \\in \\{1,4,16,64\\}$。\n\n您的程序必须为每个测试用例输出一个列表，其中包含：\n- 估计的斜率 $\\hat{\\alpha}$，为一个浮点数。\n- 关于跨测试尺度非递增空隙度的单调性布尔值。\n- 决定系数 $\\hat{R}^2$，为一个浮点数。\n- 在指定尺度下的空隙度值列表。\n\n您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果。顶层列表应按上述顺序列出每个测试用例的条目，每个条目本身也是按所述顺序排列的列表。例如，最终输出格式应类似于：\n$$\n[\\,[\\hat{\\alpha}_1,\\mathrm{mono}_1,\\hat{R}^2_1,[\\Lambda_1(r_1),\\dots]],\\,[\\hat{\\alpha}_2,\\mathrm{mono}_2,\\hat{R}^2_2,[\\Lambda_2(r_1),\\dots]],\\,[\\hat{\\alpha}_3,\\mathrm{mono}_3,\\hat{R}^2_3,[\\Lambda_3(r_1),\\dots]]\\,]\n$$\n此计算不涉及物理单位，所有角度都与此任务无关。所有数值输出必须以指定的单行格式返回，且不得包含任何附加文本。",
            "solution": "该问题已根据指定标准进行了验证，并被确定为 **有效**。它在科学上植根于分形几何学的原理，通过所有必要的定义和参数实现了自包含，并且在计算上是可行的。该问题设定良好，可以通过算法确定唯一的解决方案。\n\n任务是为几种合成二值图像在一系列尺度 $r$ 上计算空隙度 $\\Lambda(r)$。空隙度量化了空间模式的“缝隙性”或平移不变性。它随尺度的变化为了解模式的分形性质和结构异质性提供了见解。该分析涉及三个主要阶段：合成图像生成、高效的空隙度计算以及通过线性回归进行的尺度分析。\n\n计算滑动盒质量 $M_{i,j}(r)$ 需要一种高效的算法。一个朴素的实现，即为 $(N-r+1)^2$ 个大小为 $r \\times r$ 的盒子中的每一个重新计算总和，对于每个尺度 $r$ 的复杂度将是 $O((N-r+1)^2 \\cdot r^2)$。对于给定的图像和盒子大小，这在计算上是令人望而却步的。一种标准且计算上更优越的方法是利用和区域表（summed-area table），也称为积分图（integral image）。积分图 $S$ 是从输入图像 $I$ 构建的，使得每个元素 $S_{x,y}$ 存储从原点 $(0,0)$ 到 $(x,y)$ 的矩形内所有像素值的总和：\n$$\nS_{x,y} = \\sum_{u=0}^{x} \\sum_{v=0}^{y} I_{u,v}.\n$$\n该表可以在图像上单次遍历计算完成，总复杂度为 $O(N^2)$。一旦计算出 $S$，任何左上角为 $(i,j)$、右下角为 $(i+r-1, j+r-1)$ 的矩形区域的质量 $M$ 都可以通过对 $S$ 的填充版本进行四次查找，在常数时间 $O(1)$ 内计算出来：\n$$\nM_{i,j}(r) = S_{i+r-1, j+r-1} - S_{i-1, j+r-1} - S_{i+r-1, j-1} + S_{i-1, j-1}.\n$$\n这将为给定尺度 $r$ 查找所有 $(N-r+1)^2$ 个质量值的复杂度降低到 $O((N-r+1)^2)$，这是一个显著的改进。\n\n有了给定尺度 $r$ 的所有质量集合 $\\{M_{i,j}(r)\\}$，均值 $\\mu_r$ 和二阶矩 $\\mathbb{E}[M(r)^2]$ 按定义计算：\n$$\n\\mu_r = \\frac{1}{(N - r + 1)^2} \\sum_{i,j} M_{i,j}(r),\n$$\n$$\n\\mathbb{E}[M(r)^2] = \\frac{1}{(N - r + 1)^2} \\sum_{i,j} M_{i,j}(r)^2.\n$$\n然后，空隙度 $\\Lambda(r)$ 由以下比率给出：\n$$\n\\Lambda(r) = \\frac{\\mathbb{E}[M(r)^2]}{\\mu_r^2}.\n$$\n对每个指定的盒尺寸 $r$ 重复此计算，以获得一组空隙度值。\n\n该问题要求分析三种类型的图像：\n1.  **Sierpiński地毯**：一个确定性分形，由一个生成元矩阵 $G \\in \\{0,1\\}^{3 \\times 3}$ 的迭代Kronecker积生成，其中 $G_{1,1}=0$ 且所有其他条目为 $1$。第 $k$ 次迭代 $S_k$ 是一个 $3^k \\times 3^k$ 的矩阵，由 $S_k = G \\otimes S_{k-1}$ 给出，其中 $S_1 = G$。这会产生一个典型的自相似结构。\n2.  **均匀图像**：一个非分形、完全均匀的图像，其中所有 $(x,y)$ 的 $I_{x,y}=1$。对于此图像，每个大小为 $r$ 的盒子的质量 $M=r^2$。因此，质量的方差为零，理论预测所有 $r$ 的 $\\Lambda(r)=1$。这可以作为算法正确性的重要基准和测试。\n3.  **位点逾渗图像**：一个随机场，其中每个位点以概率 $p$ 被占据。选择的概率 $p=0.6$ 接近方形晶格上位点逾渗的临界阈值（$p_c \\approx 0.5927$），在该阈值处已知会形成标度不变的簇。\n\n最后，通过使用普通最小二乘法（OLS）拟合线性模型 $\\log \\Lambda(r) \\approx \\alpha \\log r + \\beta$，对计算出的点对 $(\\log r, \\log \\Lambda(r))$ 进行尺度分析。这将得到估计的尺度指数 $\\hat{\\alpha}$ 和决定系数 $\\hat{R}^2$，后者量化了幂律拟合的优度。对于均匀图像，其中 $\\Lambda(r)=1$，所有 $r$ 的 $\\log \\Lambda(r)=0$。在这种特殊情况下，数据点完全位于一条水平线上，因此斜率 $\\hat{\\alpha}$ 精确为 $0$，拟合是完美的，得到 $\\hat{R}^2=1$。此外，还通过检查空隙度值序列在测试尺度上是否非递增来确定一个布尔标志，即对于所有 $i$，是否有 $\\Lambda(r_{i+1}) \\le \\Lambda(r_i)$。\n\n实现将首先定义用于生成每种图像类型的函数。一个核心函数将使用和区域表优化来为给定图像和一组盒尺寸计算空隙度值。另一个函数将执行线性回归和单调性检查。主函数将遍历指定的测试用例，协调这些计算，并将结果格式化为所需的单行输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n\n    def generate_sierpinski(k: int) -> np.ndarray:\n        \"\"\"\n        Generates a Sierpiński carpet of depth k.\n        \"\"\"\n        g = np.ones((3, 3), dtype=np.int8)\n        g[1, 1] = 0\n        \n        image = g.copy()\n        for _ in range(k - 1):\n            image = np.kron(image, g)\n        return image\n\n    def generate_uniform(n: int) -> np.ndarray:\n        \"\"\"\n        Generates a uniform image of size n x n.\n        \"\"\"\n        return np.ones((n, n), dtype=np.int8)\n\n    def generate_percolation(n: int, p: float, seed: int) -> np.ndarray:\n        \"\"\"\n        Generates a site percolation image of size n x n.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        return (rng.random((n, n))  p).astype(np.int8)\n\n    def compute_lacunarity(image: np.ndarray, box_sizes: list[int]) -> list[float]:\n        \"\"\"\n        Computes lacunarity for a given image and list of box sizes using a\n        summed-area table (integral image) for efficiency.\n        \"\"\"\n        n = image.shape[0]\n        # Pad with 1 row/col of zeros on top and left for easier calculation\n        integral_image = np.pad(image.cumsum(axis=0).cumsum(axis=1), 1, 'constant')\n        \n        lac_values = []\n        for r in box_sizes:\n            if r > n:\n                continue\n\n            # Efficiently get all r x r box sums using the integral image\n            bottom_right = integral_image[r:n + 1, r:n + 1]\n            bottom_left = integral_image[r:n + 1, 0:n - r + 1]\n            top_right = integral_image[0:n - r + 1, r:n + 1]\n            top_left = integral_image[0:n - r + 1, 0:n - r + 1]\n            \n            masses = bottom_right - bottom_left - top_right + top_left\n            masses = masses.astype(np.float64) # Use float64 for precision\n\n            mu_r = np.mean(masses)\n            \n            if mu_r == 0:\n                # This case happens only for an all-zero image within the gliding boxes\n                # Lacunarity is ill-defined, often set to 1 by convention.\n                lacunarity = 1.0\n            else:\n                e_m2_r = np.mean(np.square(masses))\n                lacunarity = e_m2_r / (mu_r**2)\n            \n            lac_values.append(lacunarity)\n            \n        return lac_values\n\n    def analyze_scaling(scales: list[int], lacunarities: list[float]):\n        \"\"\"\n        Performs linear regression on log-log data and checks for monotonicity.\n        \"\"\"\n        # 1. Scaling analysis\n        log_scales = np.log(scales)\n        log_lacunarities = np.log(lacunarities)\n        \n        # Handle the special case of a uniform field where lacunarity is always 1,\n        # leading to log_lacunarities being all zeros. In this case, R^2 is 1.\n        if np.all(log_lacunarities == 0):\n            alpha = 0.0\n            r_squared = 1.0\n        else:\n            res = linregress(log_scales, log_lacunarities)\n            alpha = res.slope\n            r_squared = res.rvalue**2\n\n        # 2. Monotonicity check (nonincreasing)\n        is_nonincreasing = all(\n            lacunarities[i] = lacunarities[i-1] for i in range(1, len(lacunarities))\n        )\n\n        return alpha, is_nonincreasing, r_squared\n    \n    test_cases = [\n        {\n            'type': 'sierpinski', 'params': {'k': 5}, \n            'box_sizes': [1, 3, 9, 27, 81]\n        },\n        {\n            'type': 'uniform', 'params': {'n': 243},\n            'box_sizes': [1, 3, 9, 27, 81]\n        },\n        {\n            'type': 'percolation', 'params': {'n': 256, 'p': 0.6, 'seed': 12345},\n            'box_sizes': [1, 4, 16, 64]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        image_type = case['type']\n        params = case['params']\n        box_sizes = case['box_sizes']\n        \n        if image_type == 'sierpinski':\n            image = generate_sierpinski(**params)\n        elif image_type == 'uniform':\n            image = generate_uniform(**params)\n        elif image_type == 'percolation':\n            image = generate_percolation(**params)\n\n        lac_vals = compute_lacunarity(image, box_sizes)\n        \n        alpha, mono_bool, r_sq = analyze_scaling(box_sizes, lac_vals)\n        \n        all_results.append([alpha, mono_bool, r_sq, lac_vals])\n\n    # Format the final output string exactly as required\n    formatted_cases = []\n    for res in all_results:\n        alpha, mono, r2, lac_list = res\n        lac_list_str = \"[\" + \",\".join(map(repr, lac_list)) + \"]\"\n        case_str = f\"[{repr(alpha)},{repr(mono)},{repr(r2)},{lac_list_str}]\"\n        formatted_cases.append(case_str)\n\n    final_output_string = f\"[{','.join(formatted_cases)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "尺度不变性不仅限于静态的空间对象，它也是许多复杂时间过程的标志。本练习将介绍去趋势波动分析（Detrended Fluctuation Analysis, DFA），这是一种即使在非平稳数据中也能稳健检测长程相关性的强大方法。您将学习实现DFA，并关键性地运用代理数据测试（surrogate data testing）来可靠地区分真实的长程相关性与由非平稳趋势或短程记忆引起的伪尺度行为 。",
            "id": "4141518",
            "problem": "您接到的指令是，通过构建保留边际分布的代理测试，来区分时间序列中的真实长程相关性和非平稳趋势。任务是设计并实现一个程序，对于一组代表复杂自适应系统建模的合成时间序列，判断每个序列是否表现出真实的标度不变长程相关性，或者其表观的标度性是由非平稳趋势或短程效应引起的。该决策应基于标度不变性和分形几何的第一性原理，并使用保留观测序列边际分布的代理数据分析。\n\n从以下基本概念开始：\n\n- 一个离散时间随机过程 $\\{x_t\\}_{t=1}^N$ 是（弱）平稳的，如果其均值 $\\mathbb{E}[x_t]$ 是常数，且其自协方差 $\\gamma(k) = \\mathbb{E}[(x_t - \\mathbb{E}[x_t])(x_{t+k} - \\mathbb{E}[x_{t+k}])]$ 仅取决于滞后 $k$ 而不取决于 $t$。\n- 真实长程相关性的特征是缓慢衰减的自协方差，使得 $\\sum_{k=1}^{\\infty} \\gamma(k)$ 发散；等价地，其低频谱密度在 $\\lambda \\to 0^+$ 时表现为 $f(\\lambda) \\sim C \\lvert \\lambda \\rvert^{-\\beta}$（其中 $C > 0$ 且 $\\beta \\in (0,1)$），以及标度不变的涨落统计特性。对于平稳长记忆过程（如分数斯高噪声），该机制通常与赫斯特指数 $H \\in (0.5,1)$ 相关联。\n- 非平稳趋势（例如，随机游走或确定性漂移）可以模仿表观的标度不变性，但违反了平稳性。在大的时间窗口下，它们的涨落统计通常比平稳长记忆过程的标度增长更快。\n\n为实现可操作的检测，使用去趋势涨落分析（DFA）。在DFA中，对于 $k = 1,\\dots,N$，构建积分剖面 $y(k) = \\sum_{t=1}^{k} (x_t - \\bar{x})$，将 $y(k)$ 分割成大小为 $s$ 的不重叠窗口，在每个窗口内拟合并移除一个指定阶数的多项式趋势，然后计算去趋势后残差的均方根涨落 $F(s)$。标度不变性表现为幂律关系 $F(s) \\propto s^{\\alpha}$，其中斜率 $\\alpha$ 通过在一定尺度范围内对 $\\log F(s)$ 与 $\\log s$ 进行线性回归来估计。对于平稳长记忆过程，$\\alpha \\in (0.5,1)$；而对于非平稳的积分过程，如随机游走（一个 $H \\approx 0.5$ 的分数布朗运动），在使用线性DFA时，$\\alpha \\in (1,2)$。\n\n必须使用保留边际分布的代理测试，以确保检测到的标度性不是单点分布的假象。将考虑两种代理数据：\n\n1. 完全重排的代理数据：通过随机置换 $\\{x_t\\}$，它能精确保留边际分布，并破坏所有序列相关性。如果表观的标度性仅由边际分布引起，那么重排后的代理数据应表现出接近 $0.5$ 的 $\\alpha$。\n2. 分块重排的代理数据：将 $\\{x_t\\}$ 划分为大小固定的连续块 $B$，然后随机置换这些块。这既保留了边际分布，也保留了块内的短程结构，同时破坏了长程有序性。这有助于探究大尺度上的标度性是由长程相关性还是由大尺度非平稳趋势引起的。\n\n实现一个程序，对测试套件中的每个时间序列执行以下操作：\n\n- 使用线性去趋势和尺度 $s \\in \\{16,32,64,128,256,512\\}$，计算去趋势涨落分析（DFA）指数 $\\alpha$。\n- 生成 $M$ 个保留原始序列边际分布的重排代理数据和 $M$ 个分块重排代理数据（分别为随机置换和随机置换数据块），并计算它们的DFA指数。\n- 计算相对于重排代理数据的单侧 $p$ 值，定义为代理数据指数大于或等于原始指数的比例。如果此 $p$ 值小于或等于给定的显著性水平 $\\eta$，则宣布该标度性超出了仅由边际分布所能解释的范围。\n- 根据第一性原理，通过一个两步规则来区分非平稳趋势与真实长程相关性：\n  - 如果原始DFA指数 $\\alpha \\geq \\alpha_{\\text{ns}}$（非平稳性阈值），则将序列分类为“非平稳趋势”，因为这样的指数与平稳长记忆不符。\n  - 否则，如果重排测试显著，并且原始指数与分块代理数据平均指数之差至少为 $\\Delta_{\\text{LRD}}$，则将序列分类为“真实长程相关性”。如果不满足此条件，则分类为“无长程相关性”（包括短程相关过程）。\n\n使用以下参数值和测试套件：\n\n- 序列长度 $N = 2048$。\n- DFA多项式阶数为 $1$ （线性去趋势）。\n- DFA尺度 $s \\in \\{16,32,64,128,256,512\\}$。\n- 每种代理数据类型的代理数量 $M = 40$。\n- 块大小 $B = 64$。\n- 显著性水平 $\\eta = 0.01$。\n- 非平稳性阈值 $\\alpha_{\\text{ns}} = 1.05$。\n- 长程相关性阈值 $\\Delta_{\\text{LRD}} = 0.15$。\n\n构建四个代表不同机制的合成时间序列：\n\n- 案例A（“平稳长程相关性”）：分数差分整合噪声（FARIMA$(0,d,0)$），分数差分参数 $d = 0.35$，通过将白噪声与分数二项式权重进行卷积生成。这是一个具有真实长程相关性的平稳过程。\n- 案例B（“非平稳趋势”）：随机游走，定义为 $x_t = x_{t-1} + \\varepsilon_t$，其中 $\\varepsilon_t$ 是标准正态分布，且 $x_1 = 0$，产生一个具有大尺度趋势的非平稳过程。\n- 案例C（“短程相关性”）：1阶自回归过程，$x_t = \\phi x_{t-1} + \\varepsilon_t$，其中 $\\phi = 0.9$ 且 $\\varepsilon_t$ 是标准正态分布，这是一个具有短程相关性的平稳过程。\n- 案例D（“重尾独立同分布”）：独立同分布的 Student-$t$ 噪声，自由度 $\\nu = 3$，标准化为单位方差，无序列相关性。\n\n您的程序必须实现上述逻辑，并生成单行输出，其中包含四个测试案例的分类结果，以逗号分隔的列表形式并用方括号括起来。使用整数编码分类：\n- $2$ 代表“真实长程相关性”，\n- $1$ 代表“非平稳趋势”，\n- $0$ 代表“无长程相关性”。\n\n因此，最终输出必须是 $[\\text{r}_A,\\text{r}_B,\\text{r}_C,\\text{r}_D]$ 的形式，其中 $\\text{r}_\\cdot \\in \\{0,1,2\\}$ 是案例 A–D 的分类。本问题不涉及物理单位，也不出现角度或百分比；所有输出均为整数代码。您的程序应自成一体，无需输入，并仅使用指定的运行时环境。它必须遵循确切的输出格式，输出行中不得包含任何额外文本。",
            "solution": "问题陈述已经过严格审查，并被确定为有效。它在科学上基于时间序列分析、标度不变性和分形几何的原理。该问题是适定的、客观的，并提供了一套完整且一致的定义、参数和程序，以构建一个独特且有意义的解决方案。任务是实现一个分类算法，以区分合成时间序列中的真实长程相关性、非平稳趋势以及短程或无相关性。\n\n解决方案将遵循系统性的、基于原理的方法来构建。首先，将生成四种指定类别的时间序列。其次，将实现用于量化标度性的核心工具——去趋势涨落分析（DFA）算法。第三，将准备代理数据的生成方法——重排和分块重排。最后，这些组件将被集成到一个分类函数中，该函数将执行问题陈述中指定的决策逻辑。\n\n### 1. 合成时间序列的生成\n\n问题要求生成四个长度为 $N=2048$ 的合成时间序列，每个序列代表一类独特的时序动态。将使用一个带有固定种子的伪随机数生成器以确保可复现性。\n\n- **案例A：平稳长程相关性 (FARIMA(0,d,0))**：该过程通过将一个独立同分布（i.i.d.）的高斯白噪声序列 $\\varepsilon_t$ 与一个滤波器进行卷积生成，该滤波器的权重 $\\psi_k$ 呈幂律衰减。该过程定义为 $x_t = (1-L)^{-d} \\varepsilon_t$，其中 $L$ 是后移算子，$d$ 是分数差分参数。滤波器权重是 $(1-z)^{-d}$ 的泰勒展开系数，由 $\\psi_k = \\frac{\\Gamma(k+d)}{\\Gamma(k+1)\\Gamma(d)}$ 给出（$k \\ge 0$）。这些权重可以通过递归方式高效计算：$\\psi_0=1$ 且对于 $k \\ge 1$，$\\psi_k = \\psi_{k-1} \\frac{k-1+d}{k}$。在本问题中，$d=0.35$。生成的平稳序列表现出真实的长程相关性，其预期的DFA指数为 $\\alpha \\approx d+0.5 = 0.85$。\n\n- **案例B：非平稳趋势（随机游走）**：该过程通过对 i.i.d. 高斯白噪声进行累加和生成：$x_t = \\sum_{i=1}^{t} \\varepsilon_i$。这等价于一个积分过程，$x_t = x_{t-1} + \\varepsilon_t$ 且 $x_0=0$。随机游走是一个典型的非平稳过程，其积分剖面表现出强烈的趋势。其理论上的DFA指数为 $\\alpha=1.5$。\n\n- **案例C：短程相关性 (AR(1))**：一个1阶自回归过程由递推关系 $x_t = \\phi x_{t-1} + \\varepsilon_t$ 定义，其中 $\\phi=0.9$。由于 $|\\phi|  1$，该过程是平稳的。其自协方差函数呈指数衰减，$\\gamma(k) \\propto \\phi^{|k|}$，代表短程记忆。其DFA指数预计会大于 $0.5$，但显著小于长程相关过程中的指数。\n\n- **案例D：重尾I.I.D.噪声**：该序列由从自由度为 $\\nu=3$ 的 Student-$t$ 分布中抽取的 i.i.d. 随机变量组成。该分布具有重尾（四阶矩无穷大）特性，这一性质有时会产生伪标度假象。通过将每个变量除以 $\\sqrt{\\nu/(\\nu-2)} = \\sqrt{3}$，将序列标准化为单位方差。作为一个 i.i.d. 序列，其理论DFA指数为 $\\alpha = 0.5$。\n\n### 2. 去趋势涨落分析（DFA）\n\nDFA是一种量化时间序列中长程相关性的方法。对于一个给定的序列 $\\{x_t\\}_{t=1}^N$，其步骤如下：\n\n1.  **积分**：首先，从序列中减去均值 $\\bar{x}$，然后计算累积和（或剖面）：\n    $$y(k) = \\sum_{t=1}^{k} (x_t - \\bar{x}) \\quad \\text{其中 } k=1, \\dots, N$$\n2.  **分段**：将剖面 $y(k)$ 分为 $\\lfloor N/s \\rfloor$ 个长度为 $s$ 的不重叠片段，其中 $s$ 是观测的时间尺度。使用的尺度为 $s \\in \\{16, 32, 64, 128, 256, 512\\}$。\n3.  **去趋势**：在每个片段内，对 $y(k)$ 的局部片段进行多项式趋势拟合。问题指定使用1阶多项式（一条直线）。设 $p_{v}(k)$ 为片段 $v$ 的线性拟合。\n4.  **涨落计算**：对于给定的尺度 $s$，通过对所有片段的残差方差进行平均来计算均方根（RMS）涨落：\n    $$F(s) = \\sqrt{\\frac{1}{\\lfloor N/s \\rfloor \\cdot s} \\sum_{v=1}^{\\lfloor N/s \\rfloor} \\sum_{k=(v-1)s+1}^{vs} [y(k) - p_v(k)]^2}$$\n5.  **标度指数估计**：如果序列表现出标度不变性，$F(s)$ 将遵循幂律关系 $F(s) \\propto s^{\\alpha}$。标度指数 $\\alpha$ 通过在指定尺度范围内对点 $(\\log(s), \\log(F(s)))$ 进行线性拟合的斜率来估计。\n\n### 3. 代理数据检验\n\n代理数据用于检验关于观测到的标度性来源的假设。\n\n-   **重排代理数据**：通过随机置换原始时间序列 $\\{x_t\\}$ 来创建一个重排代理数据。此过程精确地保留了边际分布（值的直方图），但完全破坏了时间顺序，从而消除了任何序列相关性。如果原始序列的指数 $\\alpha$ 显著大于来自重排代理数据的指数分布，我们就可以拒绝原假设，即标度性仅仅是非高斯边际分布的假象。显著性通过 $p$ 值来量化：$M=40$ 个代理指数中大于或等于原始指数的比例。低于显著性水平 $\\eta=0.01$ 的 $p$ 值表明存在显著的、超出仅由数值分布所能解释的标度性。\n\n-   **分块重排代理数据**：通过将序列分割成大小固定为 $B=64$ 的不重叠块，然后随机置换这些块，来创建分块重排代理数据。这保留了边际分布和每个块*内部*的相关结构，但破坏了尺度大于 $B$ 的长程相关性。如果一个序列具有真实的长程相关性，其 $\\alpha$ 指数应显著高于其分块重排代理数据的平均指数，因为这些代理数据的长程有序性已被破坏。\n\n### 4. 分类算法\n\n问题的核心是一个决策算法，它结合DFA和代理测试的输出来对每个时间序列进行分类。对于一个给定的时间序列，步骤如下：\n\n1.  计算原始序列的DFA指数 $\\alpha_{\\text{orig}}$。\n2.  生成 $M=40$ 个重排代理数据并计算它们的指数 $\\{\\alpha_{\\text{shuffled},i}\\}$。\n3.  生成 $M=40$ 个分块重排代理数据并计算它们的指数 $\\{\\alpha_{\\text{block},i}\\}$。\n4.  计算相对于重排代理数据的 $p$ 值：$p = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbb{I}(\\alpha_{\\text{shuffled},i} \\ge \\alpha_{\\text{orig}})$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n5.  依次应用以下分类规则：\n    a. 如果 $\\alpha_{\\text{orig}} \\ge \\alpha_{\\text{ns}}$ （其中 $\\alpha_{\\text{ns}}=1.05$），涨落的快速增长是非平稳过程的特征。该序列被分类为**“非平稳趋势”（代码 $1$）**。\n    b. 否则，如果 $p \\le \\eta$ （其中 $\\eta=0.01$）且 $(\\alpha_{\\text{orig}} - \\text{mean}(\\{\\alpha_{\\text{block},i}\\})) \\ge \\Delta_{\\text{LRD}}$ （其中 $\\Delta_{\\text{LRD}}=0.15$），这些条件表明存在统计上显著的标度性，它不是边际分布的假象，并且这种标度性被证明是长程的（即，它被分块重排所破坏）。该序列被分类为**“真实长程相关性”（代码 $2$）**。\n    c. 否则，该序列要么缺乏显著的标度性，要么其标度性可归因于短程相关性或分布假象。它被分类为**“无长程相关性”（代码 $0$）**。\n\n这个结构化的程序允许基于不同的潜在过程的基本标度属性，对其进行稳健、自动化的区分。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to generate, analyze, and classify time series.\n    \"\"\"\n    # Set a random seed for reproducibility of the synthetic time series.\n    np.random.seed(42)\n\n    # --- Problem Parameters ---\n    N = 2048\n    DFA_SCALES = np.array([16, 32, 64, 128, 256, 512], dtype=int)\n    DFA_ORDER = 1\n    M = 40\n    BLOCK_SIZE = 64\n    ETA = 0.01\n    ALPHA_NS = 1.05\n    DELTA_LRD = 0.15\n\n    # --- Time Series Generation Functions ---\n\n    def generate_farima_0d0(d, n_points):\n        \"\"\"Generates a FARIMA(0,d,0) series using fractional binomial weights.\"\"\"\n        if not (0  d  0.5):\n            raise ValueError(\"d must be between 0 and 0.5 for stationary LRD.\")\n        \n        # Calculate filter weights recursively for numerical stability\n        weights = np.zeros(n_points)\n        weights[0] = 1.0\n        for k in range(1, n_points):\n            weights[k] = weights[k - 1] * (k - 1 + d) / k\n        \n        noise = np.random.randn(n_points)\n        series = np.convolve(weights, noise)[:n_points]\n        return series\n\n    def generate_ar1(phi, n_points):\n        \"\"\"Generates an AR(1) series.\"\"\"\n        if not (abs(phi)  1.0):\n            raise ValueError(\"phi must be less than 1 for a stationary AR(1) process.\")\n        \n        noise = np.random.randn(n_points)\n        series = np.zeros(n_points)\n        series[0] = noise[0]\n        for t in range(1, n_points):\n            series[t] = phi * series[t - 1] + noise[t]\n        return series\n\n    # --- Core Analysis Functions ---\n\n    def dfa(x, scales, order=1):\n        \"\"\"\n        Performs Detrended Fluctuation Analysis (DFA).\n        Returns the scaling exponent alpha.\n        \"\"\"\n        y = np.cumsum(x - np.mean(x))\n        fluctuations = []\n\n        for s in scales:\n            num_windows = len(x) // s\n            if num_windows == 0:\n                continue\n\n            # Reshape into non-overlapping windows\n            y_reshaped = y[:num_windows * s].reshape(num_windows, s)\n            \n            # Time axis for each window\n            time_axis = np.arange(s)\n            \n            # Detrend each window\n            residuals_sq = 0\n            for window in y_reshaped:\n                coeffs = np.polyfit(time_axis, window, order)\n                fit = np.polyval(coeffs, time_axis)\n                residuals_sq += np.sum((window - fit) ** 2)\n            \n            rms = np.sqrt(residuals_sq / (num_windows * s))\n            fluctuations.append(rms)\n\n        if not fluctuations:\n            return np.nan\n\n        # Fit log(F(s)) vs log(s) to find alpha\n        valid_scales = scales[:len(fluctuations)]\n        log_scales = np.log(valid_scales)\n        log_fluctuations = np.log(fluctuations)\n        \n        # Filter out NaN/inf values from log_fluctuations\n        finite_mask = np.isfinite(log_fluctuations)\n        if np.sum(finite_mask)  2:\n            return np.nan\n\n        alpha, _ = np.polyfit(log_scales[finite_mask], log_fluctuations[finite_mask], 1)\n        return alpha\n\n    def classify_series(x, scales, m, b, eta, alpha_ns, delta_lrd):\n        \"\"\"\n        Classifies a time series based on DFA and surrogate analysis.\n        \"\"\"\n        alpha_orig = dfa(x, scales, order=DFA_ORDER)\n\n        # 1. Nonstationarity Check\n        if alpha_orig >= alpha_ns:\n            return 1  # nonstationary trend\n\n        # 2. Surrogate Analysis\n        alpha_shuffled = np.zeros(m)\n        alpha_block = np.zeros(m)\n        \n        # Prepare for block shuffling\n        n_blocks = len(x) // b\n        truncated_len = n_blocks * b\n        x_truncated = x[:truncated_len]\n\n        for i in range(m):\n            # Shuffled surrogate\n            shuffled_series = np.random.permutation(x)\n            alpha_shuffled[i] = dfa(shuffled_series, scales, order=DFA_ORDER)\n            \n            # Block-shuffled surrogate\n            block_indices = np.random.permutation(n_blocks)\n            x_reshaped = x_truncated.reshape(n_blocks, b)\n            block_shuffled_series = x_reshaped[block_indices, :].flatten()\n            alpha_block[i] = dfa(block_shuffled_series, scales, order=DFA_ORDER)\n\n        # p-value against fully shuffled surrogates\n        p_value = np.sum(alpha_shuffled >= alpha_orig) / m\n\n        # Mean exponent of block-shuffled surrogates\n        mean_alpha_block = np.nanmean(alpha_block)\n\n        # 3. LRD vs. Short-Range/No Dependence Check\n        if p_value = eta and (alpha_orig - mean_alpha_block) >= delta_lrd:\n            return 2  # true long-range dependence\n        else:\n            return 0  # no long-range dependence\n\n    # --- Generate Test Cases ---\n    test_cases = [\n        # Case A: Stationary long-range dependence (FARIMA)\n        generate_farima_0d0(d=0.35, n_points=N),\n        # Case B: Nonstationary trend (Random Walk)\n        np.cumsum(np.random.randn(N)),\n        # Case C: Short-range dependence (AR1)\n        generate_ar1(phi=0.9, n_points=N),\n        # Case D: Heavy-tailed i.i.d. noise (Student-t)\n        np.random.standard_t(df=3, size=N) / np.sqrt(3.0 / (3.0 - 2.0)),\n    ]\n\n    # --- Perform Classification ---\n    results = []\n    for case_data in test_cases:\n        classification = classify_series(case_data, DFA_SCALES, M, BLOCK_SIZE, ETA, ALPHA_NS, DELTA_LRD)\n        results.append(classification)\n\n    # --- Print Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多复杂系统表现出一种更丰富的尺度伸缩形式，称为多重分形性（multifractality），其中不同大小的波动以不同的指数进行尺度伸缩。这项高级实践将DFA扩展到多重分形领域（MF-DFA）以探究这种复杂性。您将实现一个复杂的测试程序，使用两种类型的代理数据来分离和量化时间相关性与重尾分布对观测到的多重分形信号的贡献 。",
            "id": "4141515",
            "problem": "您的任务是设计并实现一个基于特定原理的计算测试，用于区分时间序列数据中源于时间相关性的多重分形与源于重尾边缘分布的多重分形。该任务植根于复杂自适应系统建模以及对标度不变性和分形几何的研究。\n\n该任务的基本原理如下：\n- 当适当定义的涨落函数随窗口大小服从幂律标度时，我们观察到时间序列中的标度不变性。考虑一个实值离散时间序列 $\\{x_t\\}_{t=1}^N$。定义积分剖面 $Y(i) = \\sum_{t=1}^{i} (x_t - \\bar{x})$，其中 $\\bar{x}$ 是样本均值。对于一个窗口（分段）大小 $s$，将剖面划分为长度为 $s$ 的不重叠分段，通过拟合一个 $m$ 阶多项式（通常对于线性去趋势，$m=1$）对每个分段进行去趋势，并计算每个分段中残差的方差。设 $F^2(\\nu,s)$ 表示在尺度 $s$ 下，索引为 $\\nu$ 的分段中的残差方差。$q$ 阶涨落函数定义为\n$$\nF_q(s) =\n\\begin{cases}\n\\left[ \\frac{1}{M(s)} \\sum\\limits_{\\nu=1}^{M(s)} \\left( F^2(\\nu,s) \\right)^{\\frac{q}{2}} \\right]^{\\frac{1}{q}},   q \\neq 0, \\\\\n\\exp\\left( \\frac{1}{2 M(s)} \\sum\\limits_{\\nu=1}^{M(s)} \\log\\left( F^2(\\nu,s) \\right) \\right),   q = 0,\n\\end{cases}\n$$\n其中 $M(s)$ 是尺度 $s$ 的分段数量。如果对于每个 $q$，在一个尺度范围内 $F_q(s) \\propto s^{h(q)}$，则该时间序列表现出标度不变性，其中 $h(q)$ 是广义赫斯特指数。单分形过程的 $h(q)$ 与 $q$ 无关，而多重分形过程的 $h(q)$ 随 $q$ 变化。\n\n目标是明确量化多重分形性，并将其归因于时间相关性或重尾边缘分布。为实现此目标，且不为目标概念提供捷径，您必须：\n1.  对于一组矩 $q$ 和一个尺度范围 $s$，仅使用上述定义和对数-对数关系 $\\log F_q(s)$ 与 $\\log s$ 上的普通最小二乘法，估计 $h(q)$。\n2.  定义一个多重分形强度度量\n$$\nM(x) = \\max_{q \\in \\mathcal{Q}} h_x(q) - \\min_{q \\in \\mathcal{Q}} h_x(q),\n$$\n其中 $h_x(q)$ 是为时间序列 $x$ 估计的广义赫斯特指数，$\\mathcal{Q}$ 是一个包含正矩和负矩（包括 $q=0$）的对称集合。\n3.  构造 $x$ 的两个代理数据：\n   - 一个置换代理数据 $x^{(\\mathrm{shuf})}$，通过对 $x$ 应用随机排列获得，它保留了边缘分布但破坏了时间相关性。\n   - 一个高斯化代理数据 $x^{(\\mathrm{gauss})}$，通过基于排序的分位数变换 $x^{(\\mathrm{gauss})}_t = \\Phi^{-1} \\left( \\hat{F}_x(x_t) \\right)$ 获得，其中 $\\hat{F}_x$ 是 $x$ 的经验累积分布函数，$\\Phi^{-1}$ 是标准正态分布的反累积分布函数。这保留了时间排序（及其相关性），同时大致将边缘分布替换为标准正态分布。\n4.  通过以下方式将多重分形性归因于时间相关性和重尾边缘分布：\n$$\nC(x) := M\\left( x^{(\\mathrm{gauss})} \\right), \\quad D(x) := M\\left( x^{(\\mathrm{shuf})} \\right),\n$$\n其中 $C(x)$ 估计相关性驱动的多重分形性，$D(x)$ 估计分布驱动的多重分形性。\n5.  对主导来源进行分类：\n   - 如果 $C(x)$ 超过 $D(x)$ 至少一个边际值 $\\delta$，并且 $C(x)$ 超过一个显著性阈值 $\\epsilon$（相关性主导的多重分形性），则返回整数代码 $2$。\n   - 如果 $D(x)$ 超过 $C(x)$ 至少 $\\delta$，并且 $D(x)$ 超过 $\\epsilon$（重尾主导的多重分形性），则返回整数代码 $1$。\n   - 如果 $C(x)$ 和 $D(x)$ 都超过 $\\epsilon$，并且 $|C(x) - D(x)|  \\delta$（两种来源都存在且强度相似），则返回整数代码 $3$。\n   - 否则返回整数代码 $0$（未检测到显著多重分形性）。\n您必须根据估计变异性以及指定长度的单分形和多重分形信号的 $M(x)$ 的典型量级，为 $\\epsilon$ 和 $\\delta$ 选择合理的值并说明理由。\n\n角度单位不适用。不涉及物理单位。所有数值输出都应为无量纲的浮点数或整数。\n\n使用以下测试套件实现程序。每个案例指定了生成器类型和参数，用于生成长度为 $N$ 的时间序列。使用指定的随机种子以确保可复现性：\n- 案例1（正常路径，基线）：独立同分布的高斯噪声，其中 $N = 4096$，种子为 $123$。\n- 案例2（分布驱动的多重分形性）：独立同分布的学生t分布噪声，自由度 $\\nu = 1.5$，其中 $N = 4096$，种子为 $456$。\n- 案例3（边界条件：相关但为单分形）：高斯 $1/f^\\beta$ 噪声，其中 $\\beta = 1.0$，$N = 4096$，种子为 $789$。\n- 案例4（边缘案例：同时具有重尾和相关性）：由 $w_t = \\phi w_{t-1} + \\sigma_w \\xi_t$（其中 $\\xi_t \\sim \\mathcal{N}(0,1)$, $\\phi = 0.9$, $\\sigma_w = 0.5$）定义的对数正态波动率的自回归波动率驱动高斯新息，以及 $x_t = \\exp(w_t) \\epsilon_t$（其中 $\\epsilon_t \\sim \\mathcal{N}(0,1)$），$N = 4096$，种子为 $321$。\n\n您的程序必须：\n- 实现所述的用于估计 $h(q)$ 的多重分形去趋势涨落分析估计器。\n- 使用矩 $\\mathcal{Q} = \\{-4, -2, -1, 0, 1, 2, 4\\}$ 和在区间 $[16, \\lfloor N/4 \\rfloor]$ 内对数均匀分布的整数尺度 $s$。\n- 按照定义计算 $M(x)$、$C(x)$ 和 $D(x)$，然后根据上述规则和您选择的 $\\epsilon$ 和 $\\delta$ 对每个案例进行分类。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含四个测试案例的四个整数分类代码，格式为逗号分隔的列表，并用方括号括起来（例如，\"[c1,c2,c3,c4]\"）。",
            "solution": "该问题要求设计并实现一种计算方法，以识别和量化时间相关性和重尾概率分布对时间序列多重分形特性的贡献。这是一个基于复杂系统分析既定原理、定义明确的问题，具体使用了多重分形去趋势涨落分析 (MF-DFA)。问题陈述在科学上是合理的、自洽的，并且在算法上是明确的。我们将着手解决。\n\n该方法的核心是 MF-DFA，它量化了时间序列的统计涨落如何随观测窗口的大小而变化。复杂、高度相关系统的一个关键特征是，这种标度行为可能因小涨落和大涨落而异，这一性质被称为多重分形性。\n\n首先，我们形式化指定 MF-DFA 过程。给定一个时间序列 $\\{x_t\\}_{t=1}^N$，我们计算其积分剖面，通常称为“游走”或“轨迹”，即 $Y(i) = \\sum_{t=1}^{i} (x_t - \\bar{x})$，其中 $\\bar{x}$ 是序列的均值。这个初始步骤将一个平稳的噪声信号转换为一个非平稳的、类随机游走的过程，这对于检测长程相关性是必要的。\n\n然后，将剖面 $Y(i)$ 划分为 $M(s) = \\lfloor N/s \\rfloor$ 个不重叠的分段，每个分段长度为 $s$。在每个分段 $\\nu$ 内，我们通过拟合一个 $m$ 阶多项式（这里 $m=1$ 表示线性去趋势，适用于去除平稳过程积分剖面中的趋势）来移除局部趋势，并计算残差的方差。这个残差方差，表示为 $F^2(\\nu,s)$，量化了在该特定分段中尺度 $s$ 的涨落幅度。\n\n为了表征不同幅度涨落的标度行为，我们计算 $q$ 阶涨落函数 $F_q(s)$。这是分段方差的广义平均值，由矩参数 $q$ 加权。对于 $q \\neq 0$：\n$$F_q(s) = \\left[ \\frac{1}{M(s)} \\sum_{\\nu=1}^{M(s)} \\left( F^2(\\nu,s) \\right)^{\\frac{q}{2}} \\right]^{\\frac{1}{q}}$$\n对于 $q=0$，定义采用对数形式，对应于几何平均值：\n$$F_q(s) = \\exp\\left( \\frac{1}{2 M(s)} \\sum_{\\nu=1}^{M(s)} \\log\\left( F^2(\\nu,s) \\right) \\right)$$\n正的 $q$ 值会放大具有大方差（大涨落）的分段的贡献，而负的 $q$ 值会放大具有小方差（小涨落）的分段的贡献。\n\n如果时间序列表现出标度不变性，$F_q(s)$ 将遵循幂律：$F_q(s) \\propto s^{h(q)}$。指数 $h(q)$ 是广义赫斯特指数。我们通过对数转换后的变量（$\\log F_q(s)$ 对 $\\log s$）进行普通最小二乘线性回归，为每个 $q$ 估计 $h(q)$。对于单分形序列，$h(q)$ 是常数。对于多重分形序列，$h(q)$ 是 $q$ 的递减函数。\n\n时间序列 $x$ 的多重分形强度由度量 $M(x)$ 量化，定义为在一个对称矩集合 $\\mathcal{Q}$ 上估计的指数范围：\n$$M(x) = \\max_{q \\in \\mathcal{Q}} h_x(q) - \\min_{q \\in \\mathcal{Q}} h_x(q)$$\n\n关键步骤是将观察到的多重分形性 $M(x)$ 归因于其来源。已知有两个主要来源：\n1.  **时间相关性**：序列或其波动率中的长程相关性可以产生多重分形性。\n2.  **重尾分布**：值 $x_t$ 的宽泛、非高斯概率分布也可以产生多重分形性。\n\n为了解开这些来源，我们采用代理数据方法。我们从原始序列 $x$ 生成两个代理序列：\n- **置换代理数据**，$x^{(\\mathrm{shuf})}$，通过随机排列 $x$ 的元素创建。此过程保留了精确的边缘概率分布（以及因此产生的任何重尾效应），同时破坏了时间顺序，从而破坏了任何长程相关性。\n- **高斯化代理数据**，$x^{(\\mathrm{gauss})}$，通过基于排序的逆变换采样创建。每个值 $x_t$ 被替换为 $x^{(\\mathrm{gauss})}_t = \\Phi^{-1} \\left( \\hat{F}_x(x_t) \\right)$，其中 $\\hat{F}_x$ 是 $x$ 的经验累积分布函数（CDF），$\\Phi^{-1}$ 是标准正态分布的反 CDF。此过程保留了原始序列的时间排序次序（及其相关结构），同时强制使边缘分布变为高斯分布，从而消除了任何重尾效应。\n\n通过分析这些代理数据，我们可以分离每个来源的贡献。从高斯化代理数据测量的多重分形强度 $C(x) = M(x^{(\\mathrm{gauss})})$，量化了相关性驱动的多重分形性。从置换代理数据测量的强度 $D(x) = M(x^{(\\mathrm{shuf})})$，量化了分布驱动的多重分形性。\n\n最后一步是基于 $C(x)$ 和 $D(x)$ 的相对大小，对多重分形性的主导来源进行分类。我们必须建立一个显著性阈值 $\\epsilon$ 和一个比较边际值 $\\delta$。\n- 对于单分形序列，对有限长度数据的估计将产生小的、非零的 $M(x)$、$C(x)$ 和 $D(x)$ 值。根据对长度为 $N=4096$ 的序列的经验结果，这些度量的基线“噪声”水平约为 $0.02-0.04$。我们选择显著性阈值 $\\epsilon=0.05$，以确保只有强于此噪声底线的多重分形性才被认为是显著的。\n- 边际值 $\\delta$ 决定一个来源是否决定性地强于另一个。选择 $\\delta = 0.05$ 提供了一个一致的标准：要成为主导来源，其贡献不仅必须自身显著，而且还必须比另一来源的贡献超出显著性阈值以上。问题中将 $|C(x)-D(x)|  \\delta$ 用于类别3，因此我们将 $\\delta$ 设为0.05。\n\n然后按问题陈述中指定的规则应用分类：\n- 代码 2（相关性主导）：$C(x) > D(x) + \\delta$ 且 $C(x) > \\epsilon$。\n- 代码 1（分布主导）：$D(x) > C(x) + \\delta$ 且 $D(x) > \\epsilon$。\n- 代码 3（两种来源）：$C(x) > \\epsilon$，$D(x) > \\epsilon$，且 $|C(x) - D(x)|  \\delta$。\n- 代码 0（无显著多重分形性）：所有其他情况。\n\n这种基于原理的方法允许对多重分形时间序列进行稳健的、定量的分类，这在以下程序中得以实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef generate_series(N, seed, series_type, params=None):\n    \"\"\"\n    Generates a time series based on the specified type and parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    if series_type == 'iid_gauss':\n        return rng.standard_normal(N)\n    \n    elif series_type == 'iid_t':\n        return rng.standard_t(df=params['nu'], size=N)\n    \n    elif series_type == 'fgn':\n        beta = params['beta']\n        # Generate Gaussian white noise\n        white_noise = rng.standard_normal(N)\n        # FFT of the noise\n        fft_white_noise = np.fft.rfft(white_noise)\n        # Frequencies for rfft\n        fft_freq = np.fft.rfftfreq(N)\n        \n        # Create the 1/f^beta filter in frequency domain\n        # The filter scales the amplitude, so it's f^(-beta/2)\n        filter_scaling = fft_freq**(-beta / 2.0)\n        # Set DC component (f=0) to zero to avoid infinity and a mean offset\n        filter_scaling[0] = 0\n        \n        # Apply the filter\n        fft_fgn = fft_white_noise * filter_scaling\n        \n        # Inverse FFT to get the time series\n        fgn = np.fft.irfft(fft_fgn, n=N)\n        \n        # Normalize to have unit variance for consistency\n        fgn = (fgn - np.mean(fgn)) / np.std(fgn)\n        return fgn\n        \n    elif series_type == 'stoch_vol':\n        phi = params['phi']\n        sigma_w = params['sigma_w']\n        \n        xi = rng.standard_normal(N)\n        eps = rng.standard_normal(N)\n        \n        w = np.zeros(N)\n        # Initialize w[0] from the stationary distribution of the AR(1) process\n        w[0] = (sigma_w / np.sqrt(1 - phi**2)) * xi[0]\n        \n        for t in range(1, N):\n            w[t] = phi * w[t-1] + sigma_w * xi[t]\n        \n        # The final series is x_t = exp(w_t) * epsilon_t\n        x = np.exp(w) * eps\n        return x\n        \n    else:\n        raise ValueError(f\"Unknown series type: {series_type}\")\n\ndef create_surrogates(x, seed):\n    \"\"\"\n    Creates shuffled and Gaussianized surrogates of a time series.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Shuffled surrogate\n    x_shuf = rng.permutation(x)\n    \n    # Gaussianized surrogate\n    N = len(x)\n    # Get ranks, convert to probabilities in (0, 1) to avoid -inf/inf from ppf\n    ranks = stats.rankdata(x, method='average')\n    p = ranks / (N + 1)\n    # Apply inverse normal CDF (percent point function)\n    x_gauss = stats.norm.ppf(p)\n    \n    return x_shuf, x_gauss\n\ndef mf_dfa(x, q_values, scales, m=1):\n    \"\"\"\n    Performs Multifractal Detrended Fluctuation Analysis (MF-DFA).\n    \"\"\"\n    N = len(x)\n    # 1. Compute integrated profile\n    Y = np.cumsum(x - np.mean(x))\n    \n    h_q_list = []\n    \n    for q in q_values:\n        F_s_list = []\n        valid_scales = []\n        \n        for s in scales:\n            if s = m + 2:  # Not enough points for polyfit\n                continue\n            \n            # 2. Divide profile into non-overlapping segments\n            num_segments = N // s\n            segments_Y = Y[:num_segments * s].reshape((num_segments, s))\n            \n            seg_indices = np.arange(s)\n            F2_nu_list = []\n            \n            for nu in range(num_segments):\n                # 3. Detrend each segment\n                segment = segments_Y[nu, :]\n                try:\n                    p_coeffs = np.polyfit(seg_indices, segment, m)\n                    fit = np.polyval(p_coeffs, seg_indices)\n                    residuals = segment - fit\n                    # 4. Compute variance of residuals\n                    F2_nu = np.mean(residuals**2)\n                    F2_nu_list.append(F2_nu)\n                except np.linalg.LinAlgError:\n                    continue # Skip segment if fit fails\n            \n            F2_nu_array = np.array(F2_nu_list)\n            # Filter out zero-variance segments which cause issues with log or negative powers\n            F2_nu_array = F2_nu_array[F2_nu_array > 1e-12] # Use a small epsilon\n            \n            if len(F2_nu_array) == 0:\n                continue\n            \n            # 5. Calculate q-order fluctuation function\n            if q == 0:\n                F_q_s = np.exp(0.5 * np.mean(np.log(F2_nu_array)))\n            else:\n                F_q_s = (np.mean(F2_nu_array**(q / 2.0)))**(1.0 / q)\n\n            if F_q_s > 0:\n                F_s_list.append(F_q_s)\n                valid_scales.append(s)\n\n        if len(valid_scales)  2:\n            h_q_list.append(np.nan)\n            continue\n            \n        # 6. Estimate h(q) via log-log regression\n        log_F = np.log(F_s_list)\n        log_s = np.log(valid_scales)\n        \n        try:\n            h_q, _ = np.polyfit(log_s, log_F, 1)\n            h_q_list.append(h_q)\n        except np.linalg.LinAlgError:\n            h_q_list.append(np.nan)\n    \n    return np.array(h_q_list)\n\ndef classify(C, D, epsilon, delta):\n    \"\"\"\n    Classifies the dominant source of multifractality based on the problem rules.\n    \"\"\"\n    is_C_sig = C > epsilon\n    is_D_sig = D > epsilon\n\n    if C > D + delta and is_C_sig:\n        return 2  # Correlation-dominant\n    elif D > C + delta and is_D_sig:\n        return 1  # Distribution-dominant\n    elif is_C_sig and is_D_sig and abs(C - D)  delta:\n        return 3  # Both sources present\n    else:\n        return 0  # No significant multifractality\n\ndef solve():\n    # Define parameters for the analysis\n    N = 4096\n    detrend_order = 1\n    q_values = np.array([-4, -2, -1, 0, 1, 2, 4])\n    \n    # Generate logarithmically spaced integer scales\n    min_scale, max_scale = 16, N // 4\n    num_scales = 20\n    scales = np.logspace(np.log10(min_scale), np.log10(max_scale), num_scales)\n    scales = np.unique(scales.astype(int))\n\n    # Define thresholds\n    epsilon = 0.05\n    delta = 0.05\n\n    test_cases = [\n        {'type': 'iid_gauss', 'seed': 123, 'params': None},\n        {'type': 'iid_t', 'seed': 456, 'params': {'nu': 1.5}},\n        {'type': 'fgn', 'seed': 789, 'params': {'beta': 1.0}},\n        {'type': 'stoch_vol', 'seed': 321, 'params': {'phi': 0.9, 'sigma_w': 0.5}},\n    ]\n    \n    results = []\n    \n    for i, case in enumerate(test_cases):\n        # Generate the original time series\n        x = generate_series(N, case['seed'], case['type'], case['params'])\n        \n        # Create surrogates. Use a fixed seed for surrogate creation for reproducibility\n        surrogate_seed = i + 1000\n        x_shuf, x_gauss = create_surrogates(x, surrogate_seed)\n\n        # Analyze Gaussianized surrogate for correlation effects\n        h_gauss = mf_dfa(x_gauss, q_values, scales, detrend_order)\n        C = np.nanmax(h_gauss) - np.nanmin(h_gauss) if np.all(np.isfinite(h_gauss)) and len(h_gauss)>0 else 0\n        \n        # Analyze shuffled surrogate for distribution effects\n        h_shuf = mf_dfa(x_shuf, q_values, scales, detrend_order)\n        D = np.nanmax(h_shuf) - np.nanmin(h_shuf) if np.all(np.isfinite(h_shuf)) and len(h_shuf)>0 else 0\n        \n        # Classify the result\n        code = classify(C, D, epsilon, delta)\n        results.append(code)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}