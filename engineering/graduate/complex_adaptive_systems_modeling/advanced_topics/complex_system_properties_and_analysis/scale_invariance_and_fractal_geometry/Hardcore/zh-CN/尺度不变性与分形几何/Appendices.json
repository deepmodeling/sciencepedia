{
    "hands_on_practices": [
        {
            "introduction": "此实践练习探讨了将分形几何与可观测统计数据联系起来的最基本原则之一：质量-半径关系。您将通过计算验证一个簇的分形维数 ($D$)、簇半径 ($R$) 的分布以及簇大小 ($s$) 的分布之间的理论联系。这项练习将巩固您对复杂系统中几何结构如何约束统计行为的理解。",
            "id": "4141569",
            "problem": "你的任务是形式化并实现一种从数据中估计尺度不变指数的方法，并评估这些指数在空间嵌入式复杂自适应系统中与分形几何的一致性。请从第一性原理出发，使用生存函数和分形维数的核心定义，并推导任何必要的关系。然后，实现相应的计算以产生可量化的结果。你的程序必须是一个完整的、可运行的程序，并且只能使用指定的环境。\n\n使用的定义：\n- 一个非负随机变量 $X$ 的生存函数 $S(x)$ 为 $S(x) = \\mathbb{P}(X \\ge x)$。一个尺度不变的尾部具有这样的性质：存在 $x_{\\min}  0$ 和一个指数 $\\alpha  0$，使得对于足够大的 $x$，$S(x)$ 的行为满足 $S(x) \\propto x^{-\\alpha}$（当 $x \\ge x_{\\min}$ 时）。\n- 平面中一个有界集合的盒计数分形维数 $D$ 通过 $N(\\varepsilon) \\propto \\varepsilon^{-D}$ 定义，其中 $N(\\varepsilon)$ 是覆盖该集合所需边长为 $\\varepsilon$ 的盒子数量。\n- 簇质量（或大小）$s$ 和簇半径 $R$ 满足自相似的质量-半径标度关系 $s \\propto R^{D}$，其中 $D$ 是支撑这些簇的空间集的分形维数。\n- 大尺寸簇的大小分布是尺度不变的，具有幂律密度 $p_s(s)$，这意味着存在 $s_{\\min}  0$ 和一个指数 $\\tau  1$，使得对于 $s \\ge s_{\\min}$，$p_s(s)$ 的行为满足 $p_s(s) \\propto s^{-\\tau}$。\n- 概率密度函数 (PDF) 是函数 $f$，使得 $f(x)\\,dx$ 给出随机变量落在无穷小区间 $[x, x+dx)$ 内的概率。\n- 最大似然估计 (MLE) 是在指定参数模型下，使观测数据的似然最大化的估计量。\n\n任务：\n1. 从具有幂律行为（当 $r \\ge r_{\\min}$ 时）的生存函数 $S_R(r)$ 的重尾簇半径 $R$ 的合成样本中，基于第一性原理，从经验生存函数估计尾部指数 $\\alpha$。该估计必须从适用于幂律尾部生存函数的似然函数开始推导，并且必须对已知下界 $r_{\\min}$ 的样本进行操作。\n2. 从下面指定的迭代自相似构造生成的合成空间数据中，通过计算一系列尺度 $\\varepsilon$ 下的 $N(\\varepsilon)$ 并对标度律进行适当的回归来估计盒计数分形维数 $D$。\n3. 从具有幂律密度（当 $s \\ge s_{\\min}$ 时）的重尾簇大小 $s$ 的合成样本中，通过基于第一性原理的适当方法估计密度指数 $\\tau$。\n4. 仅使用上述核心定义和质量-半径标度关系 $s \\propto R^{D}$，推导连接半径的生存尾部指数、分形维数和大小的密度指数的隐含关系。使用该关系实现对任务1-3中估计的指数之间的一致性检验。将一致性定义为所推导关系式两边的绝对差不超过给定的容差 $T  0$。\n\n空间数据生成：\n- 在单位正方形 $[0,1]^2$ 内的自相似空间集是通过从单个占据的单元开始，并在 $m$ 个层级上迭代应用一个 $3 \\times 3$ 的掩码 $M$ 来生成的。在每次迭代中，每个被占据的单元被细分为一个 $3 \\times 3$ 的网格，只有与 $M$ 中为1的位置对应的子单元保持被占据。这将产生一个大小为 $3^m \\times 3^m$ 的二元网格。观测到的空间集是被占据子单元的并集。必须通过盒计数法从这个观测集估计分形维数 $D$。\n\n你的程序必须：\n- 使用指定的掩码 $M$ 和迭代深度 $m$ 为每个测试用例生成空间集，通过盒计数法估计 $D$。\n- 使用指定的下限截断值 $r_{\\min}$ 和 $s_{\\min}$ 以及指数，生成簇半径的独立样本 $R_i$ 和簇大小的独立样本 $s_j$。计算半径的基于生存函数的尾部指数估计和大小的密度指数估计。\n- 实现从质量-半径标度关系推导的一致性检验，并为每个测试用例输出一个布尔值，指示估计的指数在给定容差 $T$ 内是否一致。\n\n测试套件和参数：\n对于每个测试用例，使用以下参数。掩码以 $3 \\times 3$ 的矩阵 $M$ 形式给出，其元素在 $\\{0,1\\}$ 中，迭代深度 $m$ 是一个正整数，下限截断值 $r_{\\min}$ 和 $s_{\\min}$ 是正实数，指数 $\\alpha_{\\text{true}}$ 和 $\\tau_{\\text{true}}$ 是正实数，样本大小 $N_R$ 和 $N_S$ 是正整数。容差 $T$ 是正实数。\n\n- 测试用例 A (正常路径，经典 Sierpiński 地毯)：\n  - 掩码 $M = \\begin{pmatrix} 1  1  1 \\\\ 1  0  1 \\\\ 1  1  1 \\end{pmatrix}$。\n  - 迭代深度 $m = 4$。\n  - 半径下限截断值 $r_{\\min} = 1.0$。\n  - 大小下限截断值 $s_{\\min} = 1.0$。\n  - 真实半径尾指数 $\\alpha_{\\text{true}} = 2.271347$。\n  - 真实大小密度指数 $\\tau_{\\text{true}} = 2.2$。\n  - 半径样本大小 $N_R = 5000$。\n  - 大小样本大小 $N_S = 5000$。\n  - 容差 $T = 0.2$。\n\n- 测试用例 B (小样本和较少迭代次数)：\n  - 掩码 $M = \\begin{pmatrix} 1  1  1 \\\\ 1  0  1 \\\\ 1  1  1 \\end{pmatrix}$。\n  - 迭代深度 $m = 3$。\n  - 半径下限截断值 $r_{\\min} = 0.5$。\n  - 大小下限截断值 $s_{\\min} = 0.5$。\n  - 真实半径尾指数 $\\alpha_{\\text{true}} = 1.135673$。\n  - 真实大小密度指数 $\\tau_{\\text{true}} = 1.6$。\n  - 半径样本大小 $N_R = 800$。\n  - 大小样本大小 $N_S = 800$。\n  - 容差 $T = 0.25$。\n\n- 测试用例 C (备用掩码，包含五个被占据的子单元)：\n  - 掩码 $M = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix}$。\n  - 迭代深度 $m = 4$。\n  - 半径下限截断值 $r_{\\min} = 1.0$。\n  - 大小下限截断值 $s_{\\min} = 1.0$。\n  - 真实半径尾指数 $\\alpha_{\\text{true}} = 1.464973$。\n  - 真实大小密度指数 $\\tau_{\\text{true}} = 2.0$。\n  - 半径样本大小 $N_R = 4000$。\n  - 大小样本大小 $N_S = 4000$。\n  - 容差 $T = 0.2$。\n\n注意：\n- 所有角度（如果出现）必须以弧度为单位，但此问题不要求角度量。\n- 不涉及物理单位；所有量均为无量纲。\n- 你的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，\"[$\\text{result}_1$,$\\text{result}_2$,$\\text{result}_3$]\"）。每个测试用例的输出必须是布尔值，指示如上文定义的一致性。",
            "solution": "我们从核心定义开始。生存函数是 $S(x) = \\mathbb{P}(X \\ge x)$。一个尺度不变的生存尾部满足 $S(x) \\propto x^{-\\alpha}$，对于 $x \\ge x_{\\min}$ 和某个 $\\alpha  0$。对于 Pareto I 型模型，生存函数精确地由 $S(x) = \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$ 给出（当 $x \\ge x_{\\min}$ 时），而概率密度函数 (PDF) 是 $f(x) = \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)}$。\n\nPareto 尾部生存尾指数的最大似然估计 (MLE)：假设我们观测到独立样本 $x_1, \\dots, x_n$，受限于 $x_i \\ge x_{\\min}$。在 Pareto I 型模型下，似然函数是\n$$\nL(\\alpha; x_1,\\dots,x_n) = \\prod_{i=1}^n \\alpha x_{\\min}^{\\alpha} x_i^{-(\\alpha+1)}.\n$$\n对数似然函数是\n$$\n\\ell(\\alpha) = \\sum_{i=1}^n \\left[ \\log \\alpha + \\alpha \\log x_{\\min} - (\\alpha+1)\\log x_i \\right] = n \\log \\alpha + \\alpha \\sum_{i=1}^n \\log\\left(\\frac{x_{\\min}}{x_i}\\right) - \\sum_{i=1}^n \\log x_i.\n$$\n对 $\\alpha$ 求导并设为零，得到\n$$\n\\frac{d\\ell}{d\\alpha} = \\frac{n}{\\alpha} + \\sum_{i=1}^n \\log\\left(\\frac{x_{\\min}}{x_i}\\right) = 0 \\quad \\Rightarrow \\quad \\widehat{\\alpha} = \\frac{n}{\\sum_{i=1}^n \\log\\left(\\frac{x_i}{x_{\\min}}\\right)}.\n$$\n这个估计量基于生存尾部模型，并且与在下限截断值 $x_{\\min}$ 处对 $\\log S(x)$ 与 $\\log x$ 进行拟合的 Pareto 情形相吻合，从而确保了一致性。\n\n通过盒计数法计算分形维数：对于平面中的一个有界集，盒计数维数 $D$ 满足 $N(\\varepsilon) \\propto \\varepsilon^{-D}$，其中 $N(\\varepsilon)$ 是覆盖该集合所需边长为 $\\varepsilon$ 的盒子数量。取对数得到\n$$\n\\log N(\\varepsilon) = -D \\log \\varepsilon + \\text{constant} = D \\log \\left(\\frac{1}{\\varepsilon}\\right) + \\text{constant}.\n$$\n给定在一系列尺度 $\\varepsilon_k$ 上的观测计数 $N(\\varepsilon_k)$，拟合 $\\log N(\\varepsilon_k)$ 与 $\\log(1/\\varepsilon_k)$ 的回归线的斜率可以估计 $D$。对于具有 $m$ 个层级的迭代 $3 \\times 3$ 掩码，自然尺度是 $\\varepsilon_k = 3^{-k}$（对于 $k = 1,2,\\dots,m$），因为定义域在每个层级都被划分为 $3^k \\times 3^k$ 个盒子。\n\n簇质量-半径标度关系和指数映射：令 $s$ 表示簇质量（大小），$R$ 表示簇半径。自相似性意味着\n$$\ns = c R^{D},\n$$\n对于某个常数 $c  0$ 和分形维数 $D$。考虑一个重尾半径分布，其生存函数为 $S_R(r) = \\mathbb{P}(R \\ge r) \\propto r^{-\\alpha_R}$（当 $r \\ge r_{\\min}$ 时）。单调变换 $s = c r^D$ 导出了 $s$ 的生存函数：\n$$\nS_s(s) = \\mathbb{P}(s \\ge s) = \\mathbb{P}\\left(R \\ge \\left(\\frac{s}{c}\\right)^{\\frac{1}{D}}\\right) \\propto \\left(\\left(\\frac{s}{c}\\right)^{\\frac{1}{D}}\\right)^{-\\alpha_R} = \\left(\\frac{1}{c}\\right)^{\\frac{\\alpha_R}{D}} s^{-\\frac{\\alpha_R}{D}}.\n$$\n因此，$s$ 的生存尾指数是 $\\alpha_s = \\frac{\\alpha_R}{D}$。在 $s$ 的幂律密度（指数为 $\\tau  1$）下，渐近地有 $S_s(s) \\propto s^{-(\\tau - 1)}$，所以\n$$\n\\alpha_s = \\tau - 1 = \\frac{\\alpha_R}{D}.\n$$\n重新整理得到一致性关系\n$$\n\\alpha_R = D(\\tau - 1) \\quad \\text{或等价地} \\quad \\tau = 1 + \\frac{\\alpha_R}{D}.\n$$\n这个映射完全由生存函数定义、盒计数的 $D$ 的标度关系以及单调变换 $s \\propto R^D$ 推导得出，没有使用任何捷径。\n\n估计方案：\n1. 从半径样本 $\\{R_i\\}_{i=1}^{N_R}$ 中，使用已知截断值 $r_{\\min}$ 的生存尾指数的 MLE 估计 $\\widehat{\\alpha}_R$：\n$$\n\\widehat{\\alpha}_R = \\frac{N_R}{\\sum_{i=1}^{N_R} \\log\\left(\\frac{R_i}{r_{\\min}}\\right)}.\n$$\n2. 在生成的自相似占据网格上通过盒计数法估计 $\\widehat{D}$。对于 $k=1,\\dots,m$，当将网格聚合为 $3^k \\times 3^k$ 的盒子时，计算被占据的盒子数 $N_k$。对 $(x_k, y_k)$ 点对进行线性拟合，其中 $x_k = \\log(3^k)$ 和 $y_k = \\log(N_k)$；斜率即为 $\\widehat{D}$。\n3. 从大小样本 $\\{s_j\\}_{j=1}^{N_S}$ 中估计簇大小密度指数 $\\widehat{\\tau}$，首先通过已知截断值 $s_{\\min}$ 的 MLE 估计生存尾形状参数 $\\widehat{\\alpha}_s$：\n$$\n\\widehat{\\alpha}_s = \\frac{N_S}{\\sum_{j=1}^{N_S} \\log\\left(\\frac{s_j}{s_{\\min}}\\right)},\n$$\n然后设置\n$$\n\\widehat{\\tau} = 1 + \\widehat{\\alpha}_s.\n$$\n4. 计算一致性偏差\n$$\n\\Delta = \\left|\\widehat{\\alpha}_R - \\widehat{D}\\,\\left(\\widehat{\\tau} - 1\\right)\\right| = \\left|\\widehat{\\alpha}_R - \\widehat{D}\\,\\widehat{\\alpha}_s\\right|.\n$$\n如果 $\\Delta \\le T$，则为该测试用例返回 True，否则返回 False。\n\n算法考量：\n- 对于空间数据，迭代的 $3 \\times 3$ 掩码构造产生一个二元网格，其中 1 表示被占据的子单元。在尺度 $\\varepsilon_k = 3^{-k}$ 下的盒计数可以通过将占据网格重塑为大小为 $3^{m-k} \\times 3^{m-k}$ 的块，并使用最大值操作来检测每个盒子中的占据情况，然后计算被占据的盒子数量来实现。\n- 重尾变量的随机抽样可以使用逆变换抽样法进行。对于生存函数为 $S(x) = \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$ 的 Pareto I 型分布，如果 $U \\sim \\text{Uniform}(0,1)$，那么\n$$\nX = x_{\\min} U^{-\\frac{1}{\\alpha}}\n$$\n就具有所需的分布。\n- 通过使用对数进行回归和估计，确保数值鲁棒性，避免下溢或上溢。\n\n测试套件和预期：\n- 测试用例 A 使用经典的 Sierpiński 地毯掩码，迭代深度 $m = 4$。真实的指数 $\\alpha_{\\text{true}}$ 和 $\\tau_{\\text{true}}$ 被选择为与理论维度 $D = \\frac{\\log 8}{\\log 3}$ 一致，但程序必须从生成的空间数据中估计 $\\widehat{D}$。对于 $N_R = 5000$ 和 $N_S = 5000$ 以及容差 $T = 0.2$，一致性检验应返回 True。\n- 测试用例 B 使用相同的掩码，但迭代深度 $m=3$ 且样本量较小，容差 $T=0.25$ 以适应更大的估计方差；在典型的运行中，检验预计将返回 True。\n- 测试用例 C 使用一个有五个被占据子单元的备用掩码，产生较小的分形维数，其中 $m = 4$, $N_R = 4000$, $N_S = 4000$, 以及 $T = 0.2$；检验应返回 True。\n\n最终输出：\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如，\"[$\\text{result}_\\text{A}$,$\\text{result}_\\text{B}$,$\\text{result}_\\text{C}$]\"。每个结果必须是一个布尔值，指示根据推导的质量-半径标度关系，估计的指数在指定容差内是否一致。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_fractal_grid(mask: np.ndarray, iterations: int) - np.ndarray:\n    \"\"\"\n    Generate a binary occupancy grid for a 3x3 mask repeated over 'iterations' levels.\n    The output grid has shape (3**iterations, 3**iterations).\n    \"\"\"\n    grid = np.array([[1]], dtype=np.uint8)\n    for _ in range(iterations):\n        grid = np.kron(grid, mask.astype(np.uint8))\n    return grid\n\ndef box_counting_dimension(grid: np.ndarray) - float:\n    \"\"\"\n    Estimate the box-counting dimension D by counting occupied boxes at scales epsilon_k = 3^{-k}.\n    For k in [1, m], where m = log_3(grid.shape[0]), compute N_k and fit log N_k vs log(3^k).\n    \"\"\"\n    m = int(round(np.log(grid.shape[0]) / np.log(3)))\n    assert grid.shape == (3**m, 3**m), \"Grid size must be a power of 3.\"\n    log_eps_inv = []\n    log_N = []\n    for k in range(1, m + 1):\n        boxes_per_side = 3**k\n        block_size = 3**(m - k)\n        # Reshape into (boxes_per_side, block_size, boxes_per_side, block_size)\n        reshaped = grid.reshape(boxes_per_side, block_size, boxes_per_side, block_size)\n        # Occupancy per coarse box: max over internal cells\n        coarse = reshaped.max(axis=(1, 3))\n        N_k = int((coarse > 0).sum())\n        log_eps_inv.append(k * np.log(3.0))\n        log_N.append(np.log(float(N_k)))\n    # Linear regression: log_N = D * log_eps_inv + c\n    slope, intercept = np.polyfit(log_eps_inv, log_N, 1)\n    return float(slope)\n\ndef pareto_type1_sample(xmin: float, alpha: float, size: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Sample from Pareto Type I with survival S(x) = (xmin/x)^alpha for x >= xmin.\n    Inverse transform: X = xmin * U^(-1/alpha), U ~ Uniform(0,1).\n    \"\"\"\n    # Avoid exact 0 by clipping U to (eps,1)\n    eps = np.finfo(float).tiny\n    U = rng.uniform(low=eps, high=1.0, size=size)\n    return xmin * np.power(U, -1.0 / alpha)\n\ndef pareto_type1_mle_alpha(samples: np.ndarray, xmin: float) - float:\n    \"\"\"\n    MLE for Pareto Type I survival-tail exponent alpha, given lower cutoff xmin.\n    alpha_hat = n / sum(log(x / xmin)) for x >= xmin.\n    \"\"\"\n    x = samples\n    if np.any(x  xmin):\n        # Filter to respect xmin; in our synthetic generation this should not occur.\n        x = x[x >= xmin]\n    n = x.size\n    logs = np.log(x / xmin)\n    return float(n / logs.sum())\n\ndef run_test_case(mask_matrix, iterations, r_min, s_min, alpha_true, tau_true, n_r, n_s, tol, seed):\n    \"\"\"\n    Run a single test case:\n    - Generate fractal grid and estimate D via box counting.\n    - Sample R ~ Pareto(alpha_true, r_min) and estimate alpha_hat.\n    - Sample s ~ Pareto(alpha_s_true=tau_true-1, s_min) and estimate tau_hat.\n    - Check consistency: |alpha_hat - D_hat*(tau_hat - 1)| = tol.\n    \"\"\"\n    mask = np.array(mask_matrix, dtype=np.uint8)\n    rng = np.random.default_rng(seed)\n\n    # Generate spatial fractal grid and estimate D\n    grid = generate_fractal_grid(mask, iterations)\n    D_hat = box_counting_dimension(grid)\n\n    # Radius samples and alpha estimate\n    R_samples = pareto_type1_sample(r_min, alpha_true, n_r, rng)\n    alpha_hat = pareto_type1_mle_alpha(R_samples, r_min)\n\n    # Size samples and tau estimate\n    alpha_s_true = tau_true - 1.0\n    S_samples = pareto_type1_sample(s_min, alpha_s_true, n_s, rng)\n    alpha_s_hat = pareto_type1_mle_alpha(S_samples, s_min)\n    tau_hat = 1.0 + alpha_s_hat\n\n    # Consistency check\n    deviation = abs(alpha_hat - D_hat * (tau_hat - 1.0))\n    return deviation = tol\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\n            \"mask_matrix\": [[1,1,1],[1,0,1],[1,1,1]],\n            \"iterations\": 4,\n            \"r_min\": 1.0,\n            \"s_min\": 1.0,\n            \"alpha_true\": 2.271347,\n            \"tau_true\": 2.2,\n            \"n_r\": 5000,\n            \"n_s\": 5000,\n            \"tol\": 0.2,\n            \"seed\": 12345,\n        },\n        # Test Case B\n        {\n            \"mask_matrix\": [[1,1,1],[1,0,1],[1,1,1]],\n            \"iterations\": 3,\n            \"r_min\": 0.5,\n            \"s_min\": 0.5,\n            \"alpha_true\": 1.135673,\n            \"tau_true\": 1.6,\n            \"n_r\": 800,\n            \"n_s\": 800,\n            \"tol\": 0.25,\n            \"seed\": 54321,\n        },\n        # Test Case C\n        {\n            \"mask_matrix\": [[1,0,1],[0,1,0],[1,0,1]],\n            \"iterations\": 4,\n            \"r_min\": 1.0,\n            \"s_min\": 1.0,\n            \"alpha_true\": 1.464973,\n            \"tau_true\": 2.0,\n            \"n_r\": 4000,\n            \"n_s\": 4000,\n            \"tol\": 0.2,\n            \"seed\": 98765,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            mask_matrix=case[\"mask_matrix\"],\n            iterations=case[\"iterations\"],\n            r_min=case[\"r_min\"],\n            s_min=case[\"s_min\"],\n            alpha_true=case[\"alpha_true\"],\n            tau_true=case[\"tau_true\"],\n            n_r=case[\"n_r\"],\n            n_s=case[\"n_s\"],\n            tol=case[\"tol\"],\n            seed=case[\"seed\"],\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，我们将注意力从空间模式转向时间动态。分析来自复杂系统的时间序列时，一个关键挑战是区分真实的长时间相关性（记忆）与可能模仿标度行为的非平稳趋势。此实践练习介绍了去趋势波动分析 (Detrended Fluctuation Analysis, DFA) 和代理数据测试，这些强大的工具可让您严格地测试和验证时间数据中是否存在真正的标度不变性。",
            "id": "4141518",
            "problem": "您接到的指令是，通过构建能保持边缘分布的代理检验，来区分时间序列中的真实长程相关性与非平稳趋势。任务是设计并实现一个程序，该程序针对一组代表复杂自适应系统建模的合成时间序列，判断每个序列是表现出真实的标度不变长程相关性，还是其表观标度性源于非平稳趋势或短程效应。该决策应基于标度不变性和分形几何学的基本原理，并使用能保持观测序列边缘分布的代理数据分析方法。\n\n从以下基本基础开始：\n\n- 一个离散时间随机过程 $\\{x_t\\}_{t=1}^N$ 是（弱）平稳的，如果其均值 $\\mathbb{E}[x_t]$ 是常数，且其自协方差 $\\gamma(k) = \\mathbb{E}[(x_t - \\mathbb{E}[x_t])(x_{t+k} - \\mathbb{E}[x_{t+k}])]$ 仅取决于延迟 $k$ 而非 $t$。\n- 真实长程相关性的特征是缓慢衰减的自协方差，使得 $\\sum_{k=1}^{\\infty} \\gamma(k)$ 发散；等价地，其低频谱密度在 $\\lambda \\to 0^+$ 时表现为 $f(\\lambda) \\sim C \\lvert \\lambda \\rvert^{-\\beta}$（对于某个 $C  0$ 和 $\\beta \\in (0,1)$）；以及标度不变的波动统计量。该机制通常与平稳长记忆过程（如分数高斯噪声）的 Hurst 指数 $H \\in (0.5,1)$ 相关联。\n- 非平稳趋势（例如，随机游走或确定性漂移）可以模仿表观的标度不变性，但违反了平稳性。在大的窗口下，它们的波动统计量通常比平稳长记忆过程的波动统计量增长得更快。\n\n为了实施检测，使用去趋势波动分析 (DFA)。在 DFA 中，首先构建积分剖面 $y(k) = \\sum_{t=1}^{k} (x_t - \\bar{x})$，其中 $k = 1,\\dots,N$，然后将 $y(k)$ 分割成大小为 $s$ 的不重叠窗口，在每个窗口内拟合并移除一个指定阶数的多项式趋势，最后计算去趋势残差的均方根波动 $F(s)$。标度不变性表现为幂律关系 $F(s) \\propto s^{\\alpha}$，其中斜率 $\\alpha$ 通过对一系列标度上的 $\\log F(s)$ 对 $\\log s$ 进行线性回归来估计。对于平稳长记忆过程，$\\alpha \\in (0.5,1)$；而对于非平稳积分过程，如随机游走（一种 $H \\approx 0.5$ 的分数布朗运动），在使用线性 DFA 时，$\\alpha \\in (1,2)$。\n\n必须使用保持边缘分布的代理检验，以确保检测到的标度性不是单点分布的产物。将考虑两种代理：\n\n1.  完全置乱代理，它随机置换 $\\{x_t\\}$，从而精确地保持边缘分布，并破坏所有序列相关性。如果表观标度性仅仅由边缘分布引起，那么置乱代理应表现出接近 $0.5$ 的 $\\alpha$ 值。\n2.  分块置乱代理，它将 $\\{x_t\\}$ 分割成固定大小为 $B$ 的连续块并随机置换这些块，从而在破坏长程有序性的同时，保持边缘分布和块内的短程结构。这有助于探究大尺度上的标度性是源于长程相关性还是大尺度的非平稳趋势。\n\n实现一个程序，对测试套件中的每个时间序列执行以下操作：\n\n- 使用线性去趋势和标度 $s \\in \\{16,32,64,128,256,512\\}$，计算去趋势波动分析 (DFA) 指数 $\\alpha$。\n- 生成 $M$ 个置乱代理和 $M$ 个分块置乱代理，它们分别通过随机置换和随机置换块来保持原始序列的边缘分布，并计算它们的 DFA 指数。\n- 计算相对于置乱代理的单边 $p$ 值，定义为代理指数大于或等于原始指数的比例。如果该 $p$ 值小于或等于给定的显著性水平 $\\eta$，则声明该标度性超出了仅由边缘分布可以解释的范围。\n- 通过一个基于基本原理的两步规则来区分非平稳趋势和真实长程相关性：\n  - 如果原始 DFA 指数 $\\alpha \\geq \\alpha_{\\text{ns}}$（非平稳性阈值），则将序列分类为“非平稳趋势”，因为这样的指数与平稳长记忆不符。\n  - 否则，如果置乱检验是显著的，并且原始指数与分块代理指数均值之差至少为 $\\Delta_{\\text{LRD}}$，则将序列分类为“真实长程相关性”。如果不满足，则分类为“无长程相关性”（包括短程相关过程）。\n\n使用以下参数值和测试套件：\n\n- 序列长度 $N = 2048$。\n- DFA 多项式阶数等于 $1$（线性去趋势）。\n- DFA 标度 $s \\in \\{16,32,64,128,256,512\\}$。\n- 每种代理类型的代理数量 $M = 40$。\n- 块大小 $B = 64$。\n- 显著性水平 $\\eta = 0.01$。\n- 非平稳性阈值 $\\alpha_{\\text{ns}} = 1.05$。\n- 长程相关性阈值 $\\Delta_{\\text{LRD}} = 0.15$。\n\n构建四个代表不同机制的合成时间序列：\n\n- 情况 A（“平稳长程相关性”）：分数整合噪声 (FARIMA$(0,d,0)$)，分数差分参数 $d = 0.35$，通过将白噪声与分数二项式权重进行卷积生成。这是一个具有真实长程相关性的平稳过程。\n- 情况 B（“非平稳趋势”）：随机游走，定义为 $x_t = x_{t-1} + \\varepsilon_t$，其中 $\\varepsilon_t$ 是标准正态分布，且 $x_1 = 0$，产生一个具有大尺度趋势的非平稳过程。\n- 情况 C（“短程相关性”）：1 阶自回归过程，$x_t = \\phi x_{t-1} + \\varepsilon_t$，其中 $\\phi = 0.9$，$\\varepsilon_t$ 是标准正态分布，这是一个具有短程相关性的平稳过程。\n- 情况 D（“重尾独立同分布”）：独立同分布的学生 t 分布噪声，自由度 $\\nu = 3$，标准化为单位方差，无序列相关性。\n\n您的程序必须实现上述逻辑，并生成一行输出，其中包含四个测试案例的分类结果，形式为方括号内以逗号分隔的列表。使用整数编码分类：\n- $2$ 代表“真实长程相关性”，\n- $1$ 代表“非平稳趋势”，\n- $0$ 代表“无长程相关性”。\n\n因此，最终输出必须是 $[\\text{r}_A,\\text{r}_B,\\text{r}_C,\\text{r}_D]$ 的形式，其中 $\\text{r}_\\cdot \\in \\{0,1,2\\}$ 是案例 A–D 的分类。本问题不涉及物理单位，也不出现角度或百分比；所有输出均为整数代码。您的程序应是自包含的，无需输入，并且仅使用指定的运行时环境。它必须遵循确切的输出格式，输出行中无任何附加文本。",
            "solution": "问题陈述已经过严格审查，并被确定为有效。它在科学上植根于时间序列分析、标度不变性和分形几何学的原理。该问题定义明确、客观，并提供了一套完整且一致的定义、参数和程序，以构建一个唯一且有意义的解决方案。任务是实现一个分类算法，以区分合成时间序列中的真实长程相关性、非平稳趋势以及短程或无相关性。\n\n解决方案将遵循一种系统性的、基于原理的方法来构建。首先，将生成四种指定类别的时间序列。其次，将实现作为量化标度性核心工具的去趋势波动分析 (DFA) 算法。第三，将准备代理数据生成方法——置乱和分块置乱。最后，这些组件将被整合到一个分类函数中，该函数执行问题陈述中指定的决策逻辑。\n\n### 1. 合成时间序列的生成\n\n该问题需要四个长度为 $N=2048$ 的合成时间序列，每个代表一类独特的动力学特征。将使用一个带有固定种子的伪随机数生成器来确保可复现性。\n\n- **情况 A：平稳长程相关性 (FARIMA(0,d,0))**：该过程通过将一串独立同分布 (i.i.d.) 的高斯白噪声 $\\varepsilon_t$ 与一个滤波器进行卷积来生成，该滤波器的权重 $\\psi_k$ 呈幂律衰减。该过程由 $x_t = (1-L)^{-d} \\varepsilon_t$ 定义，其中 $L$ 是滞后算子， $d$ 是分数差分参数。滤波器权重是 $(1-z)^{-d}$ 泰勒展开的系数，由 $\\psi_k = \\frac{\\Gamma(k+d)}{\\Gamma(k+1)\\Gamma(d)}$ 给出（$k \\ge 0$）。这些系数可以通过递归方式高效计算：$\\psi_0=1$ 和 $\\psi_k = \\psi_{k-1} \\frac{k-1+d}{k}$（$k \\ge 1$）。对于本问题，$d=0.35$。生成的平稳序列表现出真实的长程相关性，其期望的 DFA 指数 $\\alpha \\approx d+0.5 = 0.85$。\n\n- **情况 B：非平稳趋势（随机游走）**：该过程通过对 i.i.d. 高斯白噪声进行累积求和生成：$x_t = \\sum_{i=1}^{t} \\varepsilon_i$。这等价于一个积分过程，$x_t = x_{t-1} + \\varepsilon_t$，$x_0=0$。随机游走是其积分剖面表现出强趋势的非平稳过程的典型例子。其理论 DFA 指数为 $\\alpha=1.5$。\n\n- **情况 C：短程相关性 (AR(1))**：1 阶自回归过程由递推关系 $x_t = \\phi x_{t-1} + \\varepsilon_t$ 定义，其中 $\\phi=0.9$。由于 $|\\phi|  1$，该过程是平稳的。其自协方差函数呈指数衰减，$\\gamma(k) \\propto \\phi^{|k|}$，代表短程记忆。其 DFA 指数预计将大于 $0.5$，但显著小于长程相关过程中的值。\n\n- **情况 D：重尾独立同分布噪声**：该序列由从自由度为 $\\nu=3$ 的学生 t 分布中抽取的 i.i.d. 随机变量组成。该分布具有重尾（四阶矩无穷大），这一特性有时会产生伪标度伪影。通过将每个变量除以 $\\sqrt{\\nu/(\\nu-2)} = \\sqrt{3}$，将序列标准化为单位方差。作为一个 i.i.d. 序列，其理论 DFA 指数为 $\\alpha = 0.5$。\n\n### 2. 去趋势波动分析 (DFA)\n\nDFA 是一种量化时间序列中长程相关性的方法。对于一个给定的序列 $\\{x_t\\}_{t=1}^N$，其步骤如下：\n\n1.  **积分**：首先，从序列中减去均值 $\\bar{x}$，然后计算累积和（或剖面）：\n    $$y(k) = \\sum_{t=1}^{k} (x_t - \\bar{x}) \\quad \\text{for } k=1, \\dots, N$$\n2.  **分段**：将剖面 $y(k)$ 分为 $\\lfloor N/s \\rfloor$ 个长度为 $s$ 的不重叠片段，其中 $s$ 是观测的时间标度。使用的标度为 $s \\in \\{16, 32, 64, 128, 256, 512\\}$。\n3.  **去趋势**：在每个片段内，对 $y(k)$ 的局部部分拟合一个多项式趋势。问题指定了 1 阶多项式（一条直线）。设 $p_{v}(k)$ 为片段 $v$ 的线性拟合。\n4.  **波动计算**：通过对所有片段的残差方差进行平均，计算给定标度 $s$ 的均方根 (RMS) 波动：\n    $$F(s) = \\sqrt{\\frac{1}{\\lfloor N/s \\rfloor \\cdot s} \\sum_{v=1}^{\\lfloor N/s \\rfloor} \\sum_{k=(v-1)s+1}^{vs} [y(k) - p_v(k)]^2}$$\n5.  **标度指数估计**：如果序列表现出标度不变性，$F(s)$ 将遵循幂律关系 $F(s) \\propto s^{\\alpha}$。标度指数 $\\alpha$ 被估计为在指定标度范围内，对点 $(\\log(s), \\log(F(s)))$ 进行线性拟合的斜率。\n\n### 3. 代理数据检验\n\n代理数据用于检验关于观测到的标度性来源的假设。\n\n-   **置乱代理**：通过随机置换原始时间序列 $\\{x_t\\}$ 来创建一个置乱代理。此过程保留了精确的边缘分布（值的直方图），但完全破坏了时间顺序，从而破坏了任何序列相关性。如果原始序列的指数 $\\alpha$ 显著大于置乱代理的指数分布，我们可以拒绝原假设，即标度性仅仅是非高斯边缘分布的产物。显著性通过一个 $p$ 值来量化：$M=40$ 个代理指数中大于或等于原始指数的比例。低于显著性水平 $\\eta=0.01$ 的 $p$ 值表明存在显著的、超出仅由值分布所能解释的标度性。\n\n-   **分块置乱代理**：通过将序列分割成固定大小为 $B=64$ 的不重叠块，然后随机置换这些块来创建一个分块置乱代理。这保留了边缘分布和每个块*内部*的相关结构，但破坏了大于 $B$ 的尺度上的长程相关性。如果一个序列具有真实的长程相关性，其 $\\alpha$ 指数应显著高于其分块置乱代理的平均指数，因为这些代理的长程有序性已被破坏。\n\n### 4. 分类算法\n\n问题的核心是一个决策算法，它结合 DFA 和代理检验的输出来对每个时间序列进行分类。对于一个给定的时间序列，步骤如下：\n\n1.  计算原始序列的 DFA 指数 $\\alpha_{\\text{orig}}$。\n2.  生成 $M=40$ 个置乱代理并计算它们的指数 $\\{\\alpha_{\\text{shuffled},i}\\}$。\n3.  生成 $M=40$ 个分块置乱代理并计算它们的指数 $\\{\\alpha_{\\text{block},i}\\}$。\n4.  计算相对于置乱代理的 $p$ 值：$p = \\frac{1}{M} \\sum_{i=1}^{M} \\mathbb{I}(\\alpha_{\\text{shuffled},i} \\ge \\alpha_{\\text{orig}})$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n5.  按顺序应用以下分类规则：\n    a. 如果 $\\alpha_{\\text{orig}} \\ge \\alpha_{\\text{ns}}$（其中 $\\alpha_{\\text{ns}}=1.05$），则波动的快速增长是非平稳过程的特征。序列被分类为**“非平稳趋势”（代码 $1$）**。\n    b. 否则，如果 $p \\le \\eta$（其中 $\\eta=0.01$）且 $(\\alpha_{\\text{orig}} - \\text{mean}(\\{\\alpha_{\\text{block},i}\\})) \\ge \\Delta_{\\text{LRD}}$（其中 $\\Delta_{\\text{LRD}}=0.15$），这些条件表明存在统计上显著的标度性，它不是边缘分布的产物，并且这种标度性被证明是长程的（即，它被分块置乱所破坏）。序列被分类为**“真实长程相关性”（代码 $2$）**。\n    c. 否则，该序列要么缺乏显著的标度性，要么其标度性可归因于短程相关性或分布伪影。它被分类为**“无长程相关性”（代码 $0$）**。\n\n这个结构化的程序允许基于它们的基本标度属性，对不同的底层过程进行稳健的、自动化的区分。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to generate, analyze, and classify time series.\n    \"\"\"\n    # Set a random seed for reproducibility of the synthetic time series.\n    np.random.seed(42)\n\n    # --- Problem Parameters ---\n    N = 2048\n    DFA_SCALES = np.array([16, 32, 64, 128, 256, 512], dtype=int)\n    DFA_ORDER = 1\n    M = 40\n    BLOCK_SIZE = 64\n    ETA = 0.01\n    ALPHA_NS = 1.05\n    DELTA_LRD = 0.15\n\n    # --- Time Series Generation Functions ---\n\n    def generate_farima_0d0(d, n_points):\n        \"\"\"Generates a FARIMA(0,d,0) series using fractional binomial weights.\"\"\"\n        if not (0  d  0.5):\n            raise ValueError(\"d must be between 0 and 0.5 for stationary LRD.\")\n        \n        # Calculate filter weights recursively for numerical stability\n        weights = np.zeros(n_points)\n        weights[0] = 1.0\n        for k in range(1, n_points):\n            weights[k] = weights[k - 1] * (k - 1 + d) / k\n        \n        noise = np.random.randn(n_points)\n        series = np.convolve(weights, noise)[:n_points]\n        return series\n\n    def generate_ar1(phi, n_points):\n        \"\"\"Generates an AR(1) series.\"\"\"\n        if not (abs(phi)  1.0):\n            raise ValueError(\"phi must be less than 1 for a stationary AR(1) process.\")\n        \n        noise = np.random.randn(n_points)\n        series = np.zeros(n_points)\n        series[0] = noise[0]\n        for t in range(1, n_points):\n            series[t] = phi * series[t - 1] + noise[t]\n        return series\n\n    # --- Core Analysis Functions ---\n\n    def dfa(x, scales, order=1):\n        \"\"\"\n        Performs Detrended Fluctuation Analysis (DFA).\n        Returns the scaling exponent alpha.\n        \"\"\"\n        y = np.cumsum(x - np.mean(x))\n        fluctuations = []\n\n        for s in scales:\n            num_windows = len(x) // s\n            if num_windows == 0:\n                continue\n\n            # Reshape into non-overlapping windows\n            y_reshaped = y[:num_windows * s].reshape(num_windows, s)\n            \n            # Time axis for each window\n            time_axis = np.arange(s)\n            \n            # Detrend each window\n            residuals_sq = 0\n            for window in y_reshaped:\n                coeffs = np.polyfit(time_axis, window, order)\n                fit = np.polyval(coeffs, time_axis)\n                residuals_sq += np.sum((window - fit) ** 2)\n            \n            rms = np.sqrt(residuals_sq / (num_windows * s))\n            fluctuations.append(rms)\n\n        if not fluctuations:\n            return np.nan\n\n        # Fit log(F(s)) vs log(s) to find alpha\n        valid_scales = scales[:len(fluctuations)]\n        log_scales = np.log(valid_scales)\n        log_fluctuations = np.log(fluctuations)\n        \n        # Filter out NaN/inf values from log_fluctuations\n        finite_mask = np.isfinite(log_fluctuations)\n        if np.sum(finite_mask)  2:\n            return np.nan\n\n        alpha, _ = np.polyfit(log_scales[finite_mask], log_fluctuations[finite_mask], 1)\n        return alpha\n\n    def classify_series(x, scales, m, b, eta, alpha_ns, delta_lrd):\n        \"\"\"\n        Classifies a time series based on DFA and surrogate analysis.\n        \"\"\"\n        alpha_orig = dfa(x, scales, order=DFA_ORDER)\n\n        # 1. Nonstationarity Check\n        if alpha_orig >= alpha_ns:\n            return 1  # nonstationary trend\n\n        # 2. Surrogate Analysis\n        alpha_shuffled = np.zeros(m)\n        alpha_block = np.zeros(m)\n        \n        # Prepare for block shuffling\n        n_blocks = len(x) // b\n        truncated_len = n_blocks * b\n        x_truncated = x[:truncated_len]\n\n        for i in range(m):\n            # Shuffled surrogate\n            shuffled_series = np.random.permutation(x)\n            alpha_shuffled[i] = dfa(shuffled_series, scales, order=DFA_ORDER)\n            \n            # Block-shuffled surrogate\n            block_indices = np.random.permutation(n_blocks)\n            x_reshaped = x_truncated.reshape(n_blocks, b)\n            block_shuffled_series = x_reshaped[block_indices, :].flatten()\n            alpha_block[i] = dfa(block_shuffled_series, scales, order=DFA_ORDER)\n\n        # p-value against fully shuffled surrogates\n        p_value = np.sum(alpha_shuffled >= alpha_orig) / m\n\n        # Mean exponent of block-shuffled surrogates\n        mean_alpha_block = np.nanmean(alpha_block)\n\n        # 3. LRD vs. Short-Range/No Dependence Check\n        if p_value = eta and (alpha_orig - mean_alpha_block) >= delta_lrd:\n            return 2  # true long-range dependence\n        else:\n            return 0  # no long-range dependence\n\n    # --- Generate Test Cases ---\n    test_cases = [\n        # Case A: Stationary long-range dependence (FARIMA)\n        generate_farima_0d0(d=0.35, n_points=N),\n        # Case B: Nonstationary trend (Random Walk)\n        np.cumsum(np.random.randn(N)),\n        # Case C: Short-range dependence (AR1)\n        generate_ar1(phi=0.9, n_points=N),\n        # Case D: Heavy-tailed i.i.d. noise (Student-t)\n        np.random.standard_t(df=3, size=N) / np.sqrt(3.0 / (3.0 - 2.0)),\n    ]\n\n    # --- Perform Classification ---\n    results = []\n    for case_data in test_cases:\n        classification = classify_series(case_data, DFA_SCALES, M, BLOCK_SIZE, ETA, ALPHA_NS, DELTA_LRD)\n        results.append(classification)\n\n    # --- Print Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在上一练习的基础上，我们深入研究复杂系统一个更细微的特征：多重分形性，即单一标度指数不足以描述其动态。这项高级实践使用多重分形去趋势波动分析 (Multifractal Detrended Fluctuation Analysis, MF-DFA) 和复杂的代理数据方法来剖析多重分形标度的起源。您将学习如何区分观察到的复杂性是源于长时间的时间相关性，还是源于数据的重尾分布。",
            "id": "4141515",
            "problem": "你的任务是设计并实现一个规范的计算检验方法，用于区分时间序列数据中源于时间相关性的多重分形性与源于重尾边缘分布的多重分形性。该任务基于复杂自适应系统建模以及对标度不变性和分形几何的研究。\n\n此任务的基本基础如下：\n- 当适当定义的波动函数服从与窗口大小的幂律标度关系时，可观测到时间序列中的标度不变性。考虑一个实值离散时间序列 $\\{x_t\\}_{t=1}^N$。定义累积剖面 $Y(i) = \\sum_{t=1}^{i} (x_t - \\bar{x})$，其中 $\\bar{x}$ 是样本均值。对于一个窗口（分段）大小 $s$，将该剖面划分为长度为 $s$ 的非重叠分段，通过拟合一个 $m$ 阶多项式（通常对于线性去趋势，$m = 1$）来对每个分段进行去趋势处理，并计算每个分段中残差的方差。设 $F^2(\\nu,s)$ 表示在尺度 $s$ 下、索引为 $\\nu$ 的分段中的残差方差。$q$ 阶波动函数定义为\n$$\nF_q(s) =\n\\begin{cases}\n\\left[ \\frac{1}{M(s)} \\sum\\limits_{\\nu=1}^{M(s)} \\left( F^2(\\nu,s) \\right)^{\\frac{q}{2}} \\right]^{\\frac{1}{q}},  q \\neq 0, \\\\\n\\exp\\left( \\frac{1}{2 M(s)} \\sum\\limits_{\\nu=1}^{M(s)} \\log\\left( F^2(\\nu,s) \\right) \\right),  q = 0,\n\\end{cases}\n$$\n其中 $M(s)$ 是尺度 $s$ 的分段数量。如果对于每个 $q$，$F_q(s) \\propto s^{h(q)}$ 在一定尺度范围内成立，则称时间序列表现出标度不变性，其中 $h(q)$ 是广义Hurst指数。单分形过程的 $h(q)$ 与 $q$ 无关，而多重分形过程的 $h(q)$ 随 $q$ 变化。\n\n目标是明确地量化多重分形性，并将其归因于时间相关性或重尾边缘分布。为实现这一目标，且不提供绕过目标概念的捷径，你必须：\n1. 对于一组矩 $q$ 和一系列尺度 $s$，仅使用上述定义和在对数-对数关系 $\\log F_q(s)$ 与 $\\log s$ 上的普通最小二乘法来估计 $h(q)$。\n2. 定义一个多重分形强度度量\n$$\nM(x) = \\max_{q \\in \\mathcal{Q}} h_x(q) - \\min_{q \\in \\mathcal{Q}} h_x(q),\n$$\n其中 $h_x(q)$ 是为时间序列 $x$ 估计的广义Hurst指数，$\\mathcal{Q}$ 是一个包含正负矩（包括 $q=0$）的对称集合。\n3. 构建 $x$ 的两个代理数据：\n   - 一个置乱代理数据 $x^{(\\mathrm{shuf})}$，通过对 $x$ 应用随机排列获得，它保留了边缘分布但破坏了时间相关性。\n   - 一个高斯化代理数据 $x^{(\\mathrm{gauss})}$，通过基于排序的分位数变换 $x^{(\\mathrm{gauss})}_t = \\Phi^{-1} \\left( \\hat{F}_x(x_t) \\right)$ 获得，其中 $\\hat{F}_x$ 是 $x$ 的经验累积分布函数，$\\Phi^{-1}$ 是标准正态分布的逆累积分布函数。这保留了时间顺序（及其依赖关系），同时将边缘分布近似替换为标准正态分布。\n4. 通过以下方式将多重分形性归因于时间相关性和重尾边缘分布\n$$\nC(x) := M\\left( x^{(\\mathrm{gauss})} \\right), \\quad D(x) := M\\left( x^{(\\mathrm{shuf})} \\right),\n$$\n其中 $C(x)$ 估计相关性驱动的多重分形性，而 $D(x)$ 估计分布驱动的多重分形性。\n5. 对主导来源进行分类：\n   - 如果 $C(x)$ 超过 $D(x)$ 至少一个裕度 $\\delta$，并且 $C(x)$ 超过一个显著性阈值 $\\epsilon$，则返回整数代码 $2$（相关性主导的多重分形性）。\n   - 如果 $D(x)$ 超过 $C(x)$ 至少 $\\delta$，并且 $D(x)$ 超过 $\\epsilon$，则返回整数代码 $1$（重尾主导的多重分形性）。\n   - 如果 $C(x)$ 和 $D(x)$ 都超过 $\\epsilon$，且 $|C(x) - D(x)| \\le \\delta$，则返回整数代码 $3$（两种来源均存在且强度相似）。\n   - 否则返回整数代码 $0$（未检测到显著的多重分形性）。\n你必须根据估计变异性以及给定长度的单分形和多重分形信号的 $M(x)$ 典型量级，为 $\\epsilon$ 和 $\\delta$ 选择合理的值并说明理由。\n\n角度单位不适用。不涉及物理单位。所有数值输出应为无量纲浮点数或整数。\n\n使用以下测试套件实现程序。每个案例指定一个生成器类型和参数，用于生成长度为 $N$ 的时间序列。使用指定的随机种子以保证可复现性：\n- 案例1（理想路径，基准）：独立同分布的高斯噪声，$N = 4096$，种子 $123$。\n- 案例2（分布驱动的多重分形性）：独立同分布的学生t分布噪声，自由度 $\\nu = 1.5$，$N = 4096$，种子 $456$。\n- 案例3（边界条件：相关但为单分形）：高斯 $1/f^\\beta$ 噪声，$\\beta = 1.0$，$N = 4096$，种子 $789$。\n- 案例4（边缘情况：兼具重尾和相关性）：对数正态波动率的自回归波动率驱动高斯新息，定义为 $w_t = \\phi w_{t-1} + \\sigma_w \\xi_t$，其中 $\\xi_t \\sim \\mathcal{N}(0,1)$，$\\phi = 0.9$，$\\sigma_w = 0.5$，以及 $x_t = \\exp(w_t) \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,1)$，$N = 4096$，种子 $321$。\n\n你的程序必须：\n- 实现所述的多重分形去趋势波动分析估计器以计算 $h(q)$。\n- 使用矩 $\\mathcal{Q} = \\{-4, -2, -1, 0, 1, 2, 4\\}$ 和在区间 $[16, \\lfloor N/4 \\rfloor]$ 内对数间隔的整数尺度 $s$。\n- 按定义计算 $M(x)$、$C(x)$ 和 $D(x)$，然后根据上述规则以及选定的 $\\epsilon$ 和 $\\delta$ 对每个案例进行分类。\n\n最终输出格式：\n你的程序应生成单行输出，包含四个测试案例的四个整数分类代码，形式为逗号分隔的列表，并用方括号括起（例如，“[c1,c2,c3,c4]”）。",
            "solution": "该问题要求设计并实现一种计算方法，用于辨别和量化时间相关性与重尾概率分布对时间序列多重分形特性的贡献。这是一个基于复杂系统分析既定原则的适定问题，具体使用了多重分形去趋势波动分析（MF-DFA）。问题陈述在科学上是合理的、内容自洽且算法上是明确的。我们将着手提供一个解决方案。\n\n该方法的核心是MF-DFA，它量化了时间序列的统计波动如何随观测窗口的大小而变化。复杂、高度相关的系统的一个关键特征是，这种标度行为对于小波动和大的波动可能不同，这一特性被称为多重分形性。\n\n首先，我们按规定将MF-DFA过程形式化。给定一个时间序列 $\\{x_t\\}_{t=1}^N$，我们计算其积分剖面，通常称为“游走”或“轨迹”，即 $Y(i) = \\sum_{t=1}^{i} (x_t - \\bar{x})$，其中 $\\bar{x}$ 是序列的均值。这一初始步骤将一个平稳的噪声信号转换为一个非平稳的、类随机游走过程，这对于检测长程相关性是必要的。\n\n然后，将剖面 $Y(i)$ 分割成 $M(s) = \\lfloor N/s \\rfloor$ 个非重叠分段，每个分段长度为 $s$。在每个分段 $\\nu$ 内，我们通过拟合一个 $m$ 阶多项式（这里 $m=1$，用于线性去趋势，适用于去除平稳过程积分剖面中的趋势）来移除局部趋势，并计算残差的方差。这个残差方差，记为 $F^2(\\nu,s)$，量化了在该特定分段中尺度 $s$ 下的波动大小。\n\n为了表征不同大小波动的标度行为，我们计算 $q$ 阶波动函数 $F_q(s)$。这是分段方差的广义平均，由矩参数 $q$ 加权。对于 $q \\neq 0$：\n$$F_q(s) = \\left[ \\frac{1}{M(s)} \\sum_{\\nu=1}^{M(s)} \\left( F^2(\\nu,s) \\right)^{\\frac{q}{2}} \\right]^{\\frac{1}{q}}$$\n对于 $q=0$，定义采用对数形式，这对应于几何平均值：\n$$F_q(s) = \\exp\\left( \\frac{1}{2 M(s)} \\sum_{\\nu=1}^{M(s)} \\log\\left( F^2(\\nu,s) \\right) \\right)$$\n正值 $q$ 会放大具有大方差（大波动）分段的贡献，而负值 $q$ 会放大具有小方差（小波动）分段的贡献。\n\n如果时间序列表现出标度不变性，$F_q(s)$ 将遵循幂律：$F_q(s) \\propto s^{h(q)}$。指数 $h(q)$ 是广义Hurst指数。我们通过对对数变换后的变量 $\\log F_q(s)$ 与 $\\log s$ 进行普通最小二乘线性回归来为每个 $q$ 估计 $h(q)$。对于单分形序列，$h(q)$ 是常数。对于多重分形序列，$h(q)$ 是 $q$ 的递减函数。\n\n时间序列 $x$ 的多重分形强度由度量 $M(x)$ 量化，定义为在一个对称的矩集合 $\\mathcal{Q}$ 上估计的指数范围：\n$$M(x) = \\max_{q \\in \\mathcal{Q}} h_x(q) - \\min_{q \\in \\mathcal{Q}} h_x(q)$$\n\n关键步骤是将观测到的多重分形性 $M(x)$ 归因于其来源。已知有两个主要来源：\n1.  **时间相关性**：序列或其波动率中的长程相关性可以产生多重分形性。\n2.  **重尾分布**：值 $x_t$ 的宽尾、非高斯概率分布也可以产生多重分形性。\n\n为了解开这些来源，我们采用代理数据方法论。我们从原始序列 $x$ 生成两个代理序列：\n- **置乱代理数据** $x^{(\\mathrm{shuf})}$ 是通过随机排列 $x$ 的元素创建的。此过程保留了精确的边缘概率分布（因此也保留了任何由重尾引起的影响），同时破坏了时间顺序，从而消除了任何长程相关性。\n- **高斯化代理数据** $x^{(\\mathrm{gauss})}$ 是通过基于排序的逆变换采样创建的。每个值 $x_t$ 被替换为 $x^{(\\mathrm{gauss})}_t = \\Phi^{-1} \\left( \\hat{F}_x(x_t) \\right)$，其中 $\\hat{F}_x$ 是 $x$ 的经验累积分布函数（CDF），$\\Phi^{-1}$ 是标准正态分布的逆CDF。此过程保留了原始序列的时间排序顺序（及其相关性结构），同时强制边缘分布为高斯分布，从而移除了任何重尾效应。\n\n通过分析这些代理数据，我们可以分离每个来源的贡献。从高斯化代理数据中测得的多重分形强度 $C(x) = M(x^{(\\mathrm{gauss})})$ 量化了相关性驱动的多重分形性。从置乱代理数据中测得的强度 $D(x) = M(x^{(\\mathrm{shuf})})$ 量化了分布驱动的多重分形性。\n\n最后一步是根据 $C(x)$ 和 $D(x)$ 的相对大小对多重分形性的主导来源进行分类。我们必须建立一个显著性阈值 $\\epsilon$ 和一个比较裕度 $\\delta$。\n- 对于单分形序列，对有限长度数据的估计将产生小的、非零的 $M(x)$、$C(x)$ 和 $D(x)$ 值。根据对长度 $N=4096$ 序列的经验结果，这些度量的基线“噪声”水平大约在 $0.02-0.04$ 之间。我们选择一个显著性阈值 $\\epsilon=0.05$，以确保只有强于此噪声基底的多重分形性才被认为是显著的。\n- 裕度 $\\delta$ 决定了一个来源是否决定性地强于另一个。选择 $\\delta = \\epsilon = 0.05$ 提供了一个一致的标准：要成为主导来源，其贡献不仅必须自身是显著的，而且还必须比另一个来源的贡献大出超过显著性阈值的量。\n\n然后，按照问题陈述中指定的规则应用分类：\n- 代码 2（相关性主导）：$C(x)  D(x) + \\delta$ 且 $C(x)  \\epsilon$。\n- 代码 1（分布主导）：$D(x)  C(x) + \\delta$ 且 $D(x)  \\epsilon$。\n- 代码 3（两种来源）：$C(x)  \\epsilon$，$D(x)  \\epsilon$，且 $|C(x) - D(x)| \\le \\delta$。\n- 代码 0（无显著多重分形性）：所有其他情况。\n\n这种有原则的方法可以对多重分形时间序列进行稳健的、定量的分类，并将在以下程序中实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef generate_series(N, seed, series_type, params=None):\n    \"\"\"\n    Generates a time series based on the specified type and parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    if series_type == 'iid_gauss':\n        return rng.standard_normal(N)\n    \n    elif series_type == 'iid_t':\n        return rng.standard_t(df=params['nu'], size=N)\n    \n    elif series_type == 'fgn':\n        beta = params['beta']\n        # Generate Gaussian white noise\n        white_noise = rng.standard_normal(N)\n        # FFT of the noise\n        fft_white_noise = np.fft.rfft(white_noise)\n        # Frequencies for rfft\n        fft_freq = np.fft.rfftfreq(N)\n        \n        # Create the 1/f^beta filter in frequency domain\n        # The filter scales the amplitude, so it's f^(-beta/2)\n        filter_scaling = fft_freq**(-beta / 2.0)\n        # Set DC component (f=0) to zero to avoid infinity and a mean offset\n        filter_scaling[0] = 0\n        \n        # Apply the filter\n        fft_fgn = fft_white_noise * filter_scaling\n        \n        # Inverse FFT to get the time series\n        fgn = np.fft.irfft(fft_fgn, n=N)\n        \n        # Normalize to have unit variance for consistency\n        fgn = (fgn - np.mean(fgn)) / np.std(fgn)\n        return fgn\n        \n    elif series_type == 'stoch_vol':\n        phi = params['phi']\n        sigma_w = params['sigma_w']\n        \n        xi = rng.standard_normal(N)\n        eps = rng.standard_normal(N)\n        \n        w = np.zeros(N)\n        # Initialize w[0] from the stationary distribution of the AR(1) process\n        w[0] = (sigma_w / np.sqrt(1 - phi**2)) * xi[0]\n        \n        for t in range(1, N):\n            w[t] = phi * w[t-1] + sigma_w * xi[t]\n        \n        # The final series is x_t = exp(w_t) * epsilon_t\n        x = np.exp(w) * eps\n        return x\n        \n    else:\n        raise ValueError(f\"Unknown series type: {series_type}\")\n\ndef create_surrogates(x, seed):\n    \"\"\"\n    Creates shuffled and Gaussianized surrogates of a time series.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Shuffled surrogate\n    x_shuf = rng.permutation(x)\n    \n    # Gaussianized surrogate\n    N = len(x)\n    # Get ranks, convert to probabilities in (0, 1) to avoid -inf/inf from ppf\n    ranks = stats.rankdata(x, method='average')\n    p = ranks / (N + 1)\n    # Apply inverse normal CDF (percent point function)\n    x_gauss = stats.norm.ppf(p)\n    \n    return x_shuf, x_gauss\n\ndef mf_dfa(x, q_values, scales, m=1):\n    \"\"\"\n    Performs Multifractal Detrended Fluctuation Analysis (MF-DFA).\n    \"\"\"\n    N = len(x)\n    # 1. Compute integrated profile\n    Y = np.cumsum(x - np.mean(x))\n    \n    h_q_list = []\n    \n    for q in q_values:\n        F_s_list = []\n        valid_scales = []\n        \n        for s in scales:\n            if s  m + 2:  # Not enough points for polyfit\n                continue\n            \n            # 2. Divide profile into non-overlapping segments\n            num_segments = N // s\n            segments_Y = Y[:num_segments * s].reshape((num_segments, s))\n            \n            seg_indices = np.arange(s)\n            F2_nu_list = []\n            \n            for nu in range(num_segments):\n                # 3. Detrend each segment\n                segment = segments_Y[nu, :]\n                try:\n                    p_coeffs = np.polyfit(seg_indices, segment, m)\n                    fit = np.polyval(p_coeffs, seg_indices)\n                    residuals = segment - fit\n                    # 4. Compute variance of residuals\n                    F2_nu = np.mean(residuals**2)\n                    F2_nu_list.append(F2_nu)\n                except np.linalg.LinAlgError:\n                    continue # Skip segment if fit fails\n            \n            F2_nu_array = np.array(F2_nu_list)\n            # Filter out zero-variance segments which cause issues with log or negative powers\n            F2_nu_array = F2_nu_array[F2_nu_array > 1e-12] # Use a small epsilon\n            \n            if len(F2_nu_array) == 0:\n                continue\n            \n            # 5. Calculate q-order fluctuation function\n            if q == 0:\n                F_q_s = np.exp(0.5 * np.mean(np.log(F2_nu_array)))\n            else:\n                F_q_s = (np.mean(F2_nu_array**(q / 2.0)))**(1.0 / q)\n\n            if F_q_s > 0:\n                F_s_list.append(F_q_s)\n                valid_scales.append(s)\n\n        if len(valid_scales)  2:\n            h_q_list.append(np.nan)\n            continue\n            \n        # 6. Estimate h(q) via log-log regression\n        log_F = np.log(F_s_list)\n        log_s = np.log(valid_scales)\n        \n        try:\n            h_q, _ = np.polyfit(log_s, log_F, 1)\n            h_q_list.append(h_q)\n        except np.linalg.LinAlgError:\n            h_q_list.append(np.nan)\n    \n    return np.array(h_q_list)\n\ndef classify(C, D, epsilon, delta):\n    \"\"\"\n    Classifies the dominant source of multifractality.\n    \"\"\"\n    is_C_sig = C > epsilon\n    is_D_sig = D > epsilon\n    \n    if is_C_sig and not is_D_sig and C > D + delta:\n        return 2 # Correlation-dominant\n    if C > D + delta and is_C_sig: # Simplified rule from problem\n        return 2\n\n    if is_D_sig and not is_C_sig and D > C + delta:\n        return 1 # Distribution-dominant\n    if D > C + delta and is_D_sig:\n        return 1\n\n    if is_C_sig and is_D_sig and abs(C - D) = delta:\n        return 3 # Both sources present\n    \n    return 0 # No significant multifractality\n\ndef solve():\n    # Define parameters for the analysis\n    N = 4096\n    detrend_order = 1\n    q_values = np.array([-4, -2, -1, 0, 1, 2, 4])\n    \n    # Generate logarithmically spaced integer scales\n    min_scale, max_scale = 16, N // 4\n    num_scales = 20\n    scales = np.logspace(np.log10(min_scale), np.log10(max_scale), num_scales)\n    scales = np.unique(scales.astype(int))\n\n    # Define thresholds\n    epsilon = 0.05\n    delta = 0.05\n\n    test_cases = [\n        {'type': 'iid_gauss', 'seed': 123, 'params': None},\n        {'type': 'iid_t', 'seed': 456, 'params': {'nu': 1.5}},\n        {'type': 'fgn', 'seed': 789, 'params': {'beta': 1.0}},\n        {'type': 'stoch_vol', 'seed': 321, 'params': {'phi': 0.9, 'sigma_w': 0.5}},\n    ]\n    \n    results = []\n    \n    for i, case in enumerate(test_cases):\n        # Generate the original time series\n        x = generate_series(N, case['seed'], case['type'], case['params'])\n        \n        # Create surrogates. Use a fixed seed for surrogate creation for reproducibility\n        surrogate_seed = i + 1000\n        x_shuf, x_gauss = create_surrogates(x, surrogate_seed)\n\n        # Analyze Gaussianized surrogate for correlation effects\n        h_gauss = mf_dfa(x_gauss, q_values, scales, detrend_order)\n        C = np.nanmax(h_gauss) - np.nanmin(h_gauss) if np.all(np.isfinite(h_gauss)) and len(h_gauss)>0 else 0\n        \n        # Analyze shuffled surrogate for distribution effects\n        h_shuf = mf_dfa(x_shuf, q_values, scales, detrend_order)\n        D = np.nanmax(h_shuf) - np.nanmin(h_shuf) if np.all(np.isfinite(h_shuf)) and len(h_shuf)>0 else 0\n        \n        # Classify the result\n        code = classify(C, D, epsilon, delta)\n        results.append(code)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}