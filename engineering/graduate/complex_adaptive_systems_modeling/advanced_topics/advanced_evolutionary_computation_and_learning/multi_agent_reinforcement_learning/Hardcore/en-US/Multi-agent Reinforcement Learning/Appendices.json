{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in multi-agent reinforcement learning is non-stationarity: as agents simultaneously learn and adapt their policies, the environment becomes a moving target from each individual agent's perspective. This exercise provides a concrete, hands-on calculation of this effect by simulating one step of simultaneous gradient ascent. By computing the change in the learning gradients, you will directly quantify how one agent's update alters the optimization landscape for the other, a fundamental concept to grasp when designing stable MARL algorithms .",
            "id": "4130831",
            "problem": "Two agents engage in Multi-Agent Reinforcement Learning (MARL), modeled as simultaneous myopic gradient ascent on a shared differentiable stage utility within a complex adaptive system. Both agents choose scalar continuous actions, denoted $a_1$ and $a_2$, respectively. Each agent updates its action by moving a small step along the gradient of its own utility with respect to its own action, evaluated at the current joint action. Formally, for agent $i \\in \\{1,2\\}$ with learning rate $\\alpha_i$, the update rule is $a_i(t+1)=a_i(t)+\\alpha_i \\nabla_{a_i} u_i(a_1(t),a_2(t))$. Assume both agents maximize the same stage utility $u_1(a_1,a_2)=u_2(a_1,a_2)=a_1 a_2$. At time $t=0$, the initial actions are $a_1(0)=0.6$ and $a_2(0)=-0.4$, and the learning rates are $\\alpha_1=0.1$ and $\\alpha_2=0.01$.\n\nPerform one simultaneous gradient-ascent update to obtain $a_1(1)$ and $a_2(1)$. Then, to quantify the induced non-stationarity in each agent’s local objective landscape between $t=0$ and $t=1$, define the non-stationarity magnitude as the Euclidean norm of the change in the agents’ own-action gradients:\n$$\nN \\equiv \\sqrt{\\left(\\nabla_{a_1} u_1(a_1(1),a_2(1))-\\nabla_{a_1} u_1(a_1(0),a_2(0))\\right)^{2}+\\left(\\nabla_{a_2} u_2(a_1(1),a_2(1))-\\nabla_{a_2} u_2(a_1(0),a_2(0))\\right)^{2}}.\n$$\nCompute $N$ for the given parameters. Round your final numerical answer to four significant figures. Express your answer as a pure number with no units.",
            "solution": "The user wants to solve a problem in multi-agent reinforcement learning.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n\n*   **Agents and Actions**: Two agents, $i \\in \\{1,2\\}$, with scalar continuous actions $a_1$ and $a_2$.\n*   **Utility Function**: A shared, differentiable utility function for both agents: $u_1(a_1, a_2) = u_2(a_1, a_2) = a_1 a_2$.\n*   **Learning Dynamics**: Simultaneous myopic gradient ascent. The update rule for agent $i$ is $a_i(t+1) = a_i(t) + \\alpha_i \\nabla_{a_i} u_i(a_1(t), a_2(t))$.\n*   **Initial Conditions at $t=0$**: $a_1(0) = 0.6$ and $a_2(0) = -0.4$.\n*   **Learning Rates**: $\\alpha_1 = 0.1$ and $\\alpha_2 = 0.01$.\n*   **Objective**: Compute the non-stationarity magnitude $N$, defined as:\n    $$\n    N \\equiv \\sqrt{\\left(\\nabla_{a_1} u_1(a_1(1),a_2(1))-\\nabla_{a_1} u_1(a_1(0),a_2(0))\\right)^{2}+\\left(\\nabla_{a_2} u_2(a_1(1),a_2(1))-\\nabla_{a_2} u_2(a_1(0),a_2(0))\\right)^{2}}\n    $$\n*   **Output Format**: Final numerical answer rounded to four significant figures.\n\n**1.2. Validate Using Extracted Givens**\n\n*   **Scientific Grounding**: The problem is well-grounded in the field of multi-agent reinforcement learning. The use of simultaneous gradient ascent for independent learners on a shared utility function is a standard model for studying learning dynamics and non-stationarity. The utility function $u(a_1, a_2) = a_1 a_2$ represents a simple, non-trivial interaction landscape (a saddle point at the origin).\n*   **Well-Posedness**: The problem is clearly stated, with all necessary parameters, initial conditions, and definitions provided. The calculations are a direct application of differential calculus and algebra, leading to a unique, well-defined solution.\n*   **Objectivity**: The problem is expressed using precise mathematical language and is free from subjective or ambiguous statements.\n\n**1.3. Verdict and Action**\n\nThe problem is scientifically grounded, well-posed, and objective. It is deemed **valid**. The solution process will now proceed.\n\n### Step 2: Solution Derivation\n\nThe solution requires a sequential calculation: first, determine the agents' actions at time $t=1$, and second, use these new actions to compute the change in gradients and the non-stationarity magnitude $N$.\n\n**2.1. Compute Gradients of the Utility Function**\n\nThe shared utility function is $u(a_1, a_2) = a_1 a_2$. We need to find the gradient of this utility with respect to each agent's own action.\nFor agent $1$, the utility is $u_1(a_1, a_2) = a_1 a_2$. The gradient with respect to its own action $a_1$ is:\n$$\n\\nabla_{a_1} u_1(a_1, a_2) = \\frac{\\partial}{\\partial a_1}(a_1 a_2) = a_2\n$$\nFor agent $2$, the utility is $u_2(a_1, a_2) = a_1 a_2$. The gradient with respect to its own action $a_2$ is:\n$$\n\\nabla_{a_2} u_2(a_1, a_2) = \\frac{\\partial}{\\partial a_2}(a_1 a_2) = a_1\n$$\n\n**2.2. Perform One Simultaneous Update**\n\nThe agents update their actions from $t=0$ to $t=1$. First, we evaluate the gradients at the initial state $(a_1(0), a_2(0)) = (0.6, -0.4)$.\n$$\n\\nabla_{a_1} u_1(a_1(0), a_2(0)) = a_2(0) = -0.4\n$$\n$$\n\\nabla_{a_2} u_2(a_1(0), a_2(0)) = a_1(0) = 0.6\n$$\nNow, we apply the update rule $a_i(t+1) = a_i(t) + \\alpha_i \\nabla_{a_i} u_i(t)$ for each agent.\nFor agent $1$:\n$$\na_1(1) = a_1(0) + \\alpha_1 \\nabla_{a_1} u_1(a_1(0), a_2(0)) = 0.6 + (0.1)(-0.4) = 0.6 - 0.04 = 0.56\n$$\nFor agent $2$:\n$$\na_2(1) = a_2(0) + \\alpha_2 \\nabla_{a_2} u_2(a_1(0), a_2(0)) = -0.4 + (0.01)(0.6) = -0.4 + 0.006 = -0.394\n$$\nSo, the new joint action at $t=1$ is $(a_1(1), a_2(1)) = (0.56, -0.394)$.\n\n**2.3. Compute the Non-stationarity Magnitude $N$**\n\nThe non-stationarity magnitude $N$ is defined as the Euclidean norm of the change in each agent's own-action gradient. We have the gradients at $t=0$. Now we compute the gradients at the new state $(a_1(1), a_2(1))$.\n$$\n\\nabla_{a_1} u_1(a_1(1), a_2(1)) = a_2(1) = -0.394\n$$\n$$\n\\nabla_{a_2} u_2(a_1(1), a_2(1)) = a_1(1) = 0.56\n$$\nNow, we can compute the change in each gradient:\nThe change in agent $1$'s gradient is $\\Delta_1$:\n$$\n\\Delta_1 = \\nabla_{a_1} u_1(a_1(1), a_2(1)) - \\nabla_{a_1} u_1(a_1(0), a_2(0)) = -0.394 - (-0.4) = 0.006\n$$\nThe change in agent $2$'s gradient is $\\Delta_2$:\n$$\n\\Delta_2 = \\nabla_{a_2} u_2(a_1(1), a_2(1)) - \\nabla_{a_2} u_2(a_1(0), a_2(0)) = 0.56 - 0.6 = -0.04\n$$\nFinally, we substitute these changes into the formula for $N$:\n$$\nN = \\sqrt{(\\Delta_1)^2 + (\\Delta_2)^2} = \\sqrt{(0.006)^2 + (-0.04)^2}\n$$\n$$\nN = \\sqrt{0.000036 + 0.0016} = \\sqrt{0.001636}\n$$\nCalculating the numerical value:\n$$\nN \\approx 0.040447496...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first significant figure is the $4$ in the hundredths place. Counting four figures gives $0.04044$. The next digit is $7$, which is $5$ or greater, so we round up the last digit.\n$$\nN \\approx 0.04045\n$$\nThis is the final numerical answer.",
            "answer": "$$\n\\boxed{0.04045}\n$$"
        },
        {
            "introduction": "A powerful paradigm for cooperative MARL is Centralized Training with Decentralized Execution (CTDE), where a central critic informs decentralized actors. This exercise explores how value-decomposition networks guarantee that greedy decentralized action selection is globally optimal, a property known as Individual-Global-Max (IGM) consistency. You will apply this principle to determine the optimal joint action by having each agent simply maximize its own learned utility function, demonstrating the elegance and efficiency of this approach .",
            "id": "4130864",
            "problem": "Consider a cooperative two-agent Markov game under the Centralized Training with Decentralized Execution (CTDE) paradigm. Let the joint action-value function be represented by a monotonic linear mixing network defined as $Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$, where $w_1 \\ge 0$, $w_2 \\ge 0$, $b \\in \\mathbb{R}$, $Q_i(s,a_i)$ is the agent-specific utility for agent $i \\in \\{1,2\\}$ at state $s$ and action $a_i$, and $\\mathbf{a}=(a_1,a_2)$ is the joint action. Assume a fixed state $s$ and discrete action sets $\\mathcal{A}_1=\\{0,1,2\\}$ and $\\mathcal{A}_2=\\{0,1,2\\}$ with the following agent utilities:\n$$Q_1(s,0)=1.437,\\quad Q_1(s,1)=1.513,\\quad Q_1(s,2)=1.499,$$\n$$Q_2(s,0)=2.105,\\quad Q_2(s,1)=2.062,\\quad Q_2(s,2)=2.176,$$\nand mixing parameters $w_1=0.73$, $w_2=1.27$, $b=-0.418$. Using only the foundational definitions of a joint action-value function, greedy selection, and monotone mappings, determine the greedy joint action under decentralized selection, where each agent independently chooses an action that maximizes its own agent-specific utility. The final answer must be the ordered pair of chosen action indices for agents $1$ and $2$ in that order. No rounding is required.",
            "solution": "The problem will first be validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n-   **System**: A cooperative two-agent Markov game.\n-   **Paradigm**: Centralized Training with Decentralized Execution (CTDE).\n-   **Joint Action-Value Function**: $Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$.\n-   **Agent Utilities**: $Q_i(s,a_i)$ for agent $i \\in \\{1,2\\}$ at state $s$ with action $a_i$.\n-   **Joint Action**: $\\mathbf{a}=(a_1,a_2)$.\n-   **Mixing Network Parameters**:\n    -   Weights: $w_1=0.73$, $w_2=1.27$.\n    -   Bias: $b=-0.418$.\n-   **Parameter Constraints**: $w_1 \\ge 0$, $w_2 \\ge 0$.\n-   **State**: A fixed state $s$.\n-   **Action Sets**: $\\mathcal{A}_1=\\{0,1,2\\}$ and $\\mathcal{A}_2=\\{0,1,2\\}$.\n-   **Agent 1 Utilities**:\n    -   $Q_1(s,0)=1.437$\n    -   $Q_1(s,1)=1.513$\n    -   $Q_1(s,2)=1.499$\n-   **Agent 2 Utilities**:\n    -   $Q_2(s,0)=2.105$\n    -   $Q_2(s,1)=2.062$\n    -   $Q_2(s,2)=2.176$\n-   **Selection Rule**: \"decentralized selection, where each agent independently chooses an action that maximizes its own agent-specific utility.\"\n-   **Objective**: Determine the greedy joint action $(a_1, a_2)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n-   **Scientifically Grounded**: The problem describes a simplified linear value-mixing network, which is a foundational concept in multi-agent reinforcement learning (MARL), related to methods like VDN (Value-Decomposition Networks) and QMIX. The CTDE paradigm is a standard approach in MARL. The stated monotonicity condition ($w_i \\ge 0$) is critical for ensuring that decentralized greedy action selection corresponds to the greedy action for the joint action-value function, a principle known as the Individual-Global-Max (IGM) consistency. The problem is scientifically sound.\n-   **Well-Posed**: All necessary data (agent utilities, action sets) and a precise rule for action selection are provided. The problem is self-contained and free from ambiguity, leading to a unique solution.\n-   **Objective**: The problem is stated using precise, formal language common in the field of reinforcement learning. It is free of subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is well-posed, scientifically grounded, and presents a clear task based on established principles in MARL. A solution will now be derived.\n\n### Solution Derivation\nThe objective is to find the greedy joint action under decentralized execution. The problem explicitly states the rule for this selection: \"each agent independently chooses an action that maximizes its own agent-specific utility.\"\n\nLet $a_1^*$ be the action chosen by agent $1$ and $a_2^*$ be the action chosen by agent $2$. According to the specified rule, these actions are determined as follows:\n$$a_1^* = \\arg\\max_{a_1 \\in \\mathcal{A}_1} Q_1(s, a_1)$$\n$$a_2^* = \\arg\\max_{a_2 \\in \\mathcal{A}_2} Q_2(s, a_2)$$\n\n**Agent 1 Action Selection:**\nWe must find the action $a_1 \\in \\{0, 1, 2\\}$ that maximizes $Q_1(s, a_1)$. The given utilities for agent $1$ are:\n-   $Q_1(s,0) = 1.437$\n-   $Q_1(s,1) = 1.513$\n-   $Q_1(s,2) = 1.499$\n\nBy comparing these values, we find that $1.513$ is the maximum.\n$$\\max(1.437, 1.513, 1.499) = 1.513$$\nThis maximum utility corresponds to the action index $1$. Therefore, agent $1$ chooses action $a_1^*=1$.\n\n**Agent 2 Action Selection:**\nWe must find the action $a_2 \\in \\{0, 1, 2\\}$ that maximizes $Q_2(s, a_2)$. The given utilities for agent $2$ are:\n-   $Q_2(s,0) = 2.105$\n-   $Q_2(s,1) = 2.062$\n-   $Q_2(s,2) = 2.176$\n\nBy comparing these values, we find that $2.176$ is the maximum.\n$$\\max(2.105, 2.062, 2.176) = 2.176$$\nThis maximum utility corresponds to the action index $2$. Therefore, agent $2$ chooses action $a_2^*=2$.\n\n**Greedy Joint Action:**\nThe greedy joint action under decentralized selection is the ordered pair of the individually selected actions, $(a_1^*, a_2^*)$.\nBased on the calculations above, the joint action is $(1, 2)$.\n\nIt is important to note the role of the mixing network parameters. The problem provides $Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$ with non-negative weights $w_1=0.73$ and $w_2=1.27$. The non-negativity of weights ensures that $Q_{\\text{tot}}$ is a monotonic function of each agent's utility $Q_i$. This guarantees that maximizing $Q_{\\text{tot}}$ over the joint action space is equivalent to each agent maximizing its local utility $Q_i$.\n$$\\frac{\\partial Q_{\\text{tot}}}{\\partial Q_i} = w_i \\ge 0$$\nThis property, known as IGM consistency, is the theoretical foundation that allows for decentralized execution. The action pair $(1,2)$ not only results from decentralized selection but also maximizes the global action-value function $Q_{\\text{tot}}$. The parameters $w_1$, $w_2$, and $b$ are contextual information confirming the validity of the decentralized approach, but they are not required for the direct computation as per the problem's explicit instruction.\n\nThe final answer is the ordered pair of action indices for agent $1$ and agent $2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In cooperative settings with a shared team reward, assigning credit to individual agents for their contributions is a difficult but critical task. The Counterfactual Multi-Agent (COMA) algorithm addresses this by using a sophisticated baseline that marginalizes out an agent's action to assess its specific contribution. This practice asks you to compute the COMA advantage value, giving you a hands-on understanding of how this counterfactual reasoning isolates an agent's impact on the team's performance .",
            "id": "4130833",
            "problem": "Consider a fully cooperative Markov game with $n$ agents trained under centralized critic and decentralized actors. At a particular time step, fix the global state $s$ and the other agents’ joint action $\\mathbf{a}_{-i}$. Agent $i$ has a discrete action set $\\{a_i^{(0)}, a_i^{(1)}, a_i^{(2)}\\}$, and at this time step its actor policy $\\pi_i$ is uniform, so $\\pi_i(a_i^{(k)} \\mid o_i) = \\frac{1}{3}$ for $k \\in \\{0,1,2\\}$, where $o_i$ denotes agent $i$’s local observation. The centralized critic outputs the following action-values for counterfactual substitutions of agent $i$’s action while holding $\\mathbf{a}_{-i}$ fixed:\n$$Q\\!\\left(s,\\left(a_i^{(0)},\\mathbf{a}_{-i}\\right)\\right) = 4.75,\\quad Q\\!\\left(s,\\left(a_i^{(1)},\\mathbf{a}_{-i}\\right)\\right) = 3.05,\\quad Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) = 5.20.$$\nSuppose the realized joint action is $\\mathbf{a} = \\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)$, so $Q(s,\\mathbf{a}) = 5.20$. Using the principles of variance-reduced multi-agent policy gradients with state-dependent baselines and the counterfactual baseline concept of Counterfactual Multi-Agent (COMA) policy gradients, derive the analytic expression for agent $i$’s advantage at $(s,\\mathbf{a})$ and compute its numerical value under the given uniform $\\pi_i$ and critic outputs. Express the final answer as a pure number. No rounding is required.",
            "solution": "The starting point is the multi-agent actor-critic policy gradient, which for agent $i$ uses the advantage function to reduce variance while keeping the gradient unbiased. The reinforcement learning objective $J(\\theta)$, where $\\theta$ denotes all policy parameters, yields for agent $i$ the gradient\n$$\\nabla_{\\theta_i} J(\\theta) = \\mathbb{E}\\!\\left[\\nabla_{\\theta_i} \\ln \\pi_i(a_i \\mid o_i)\\, A_i(s,\\mathbf{a})\\right],$$\nwhere $A_i(s,\\mathbf{a})$ is any baseline-adjusted signal that depends on $s$ and the joint action $\\mathbf{a}$, and is constructed so that its expectation conditioned on $o_i$ does not bias the gradient:\n$$\\mathbb{E}\\!\\left[\\nabla_{\\theta_i} \\ln \\pi_i(a_i \\mid o_i)\\, b_i(s,\\mathbf{a}_{-i})\\right] = 0,$$\nprovided the baseline $b_i(s,\\mathbf{a}_{-i})$ does not depend on $a_i$. A common choice is to take $A_i(s,\\mathbf{a}) = Q(s,\\mathbf{a}) - b_i(s,\\mathbf{a}_{-i})$, where $Q(s,\\mathbf{a})$ is the centralized action-value of the joint action at state $s$.\n\nThe Counterfactual Multi-Agent (COMA) baseline is designed to address multi-agent credit assignment by holding other agents’ actions fixed and marginalizing over the actions of agent $i$ under its current policy. Specifically, the COMA baseline for agent $i$ is defined as\n$$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) = \\sum_{a_i'} \\pi_i(a_i' \\mid o_i)\\, Q\\!\\left(s,\\left(a_i',\\mathbf{a}_{-i}\\right)\\right),$$\nwhich is the expectation of the critic’s value with respect to agent $i$’s policy, counterfactually substituting its action while keeping $\\mathbf{a}_{-i}$ fixed. This preserves unbiasedness because $b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i})$ is independent of the realized $a_i$ when conditioned on $o_i$.\n\nTherefore, the COMA advantage for agent $i$ at $(s,\\mathbf{a})$ is\n$$A_i^{\\text{COMA}}(s,\\mathbf{a}) = Q(s,\\mathbf{a}) - b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}).$$\n\nWith the given uniform policy $\\pi_i(a_i' \\mid o_i) = \\frac{1}{3}$ for all $a_i' \\in \\{a_i^{(0)}, a_i^{(1)}, a_i^{(2)}\\}$, we compute the COMA baseline as the simple average of the three provided critic values:\n\n$$\n\\begin{aligned}\nb_i^{\\text{COMA}}(s,\\mathbf{a}_{-i})\n&= \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(0)},\\mathbf{a}_{-i}\\right)\\right)\n+ \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(1)},\\mathbf{a}_{-i}\\right)\\right)\n+ \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) \\\\\n&= \\frac{1}{3}\\left(4.75 + 3.05 + 5.20\\right).\n\\end{aligned}\n$$\n\nTo maintain exactness, convert these decimals to rational numbers:\n$$4.75 = \\frac{19}{4}, \\quad 3.05 = \\frac{61}{20}, \\quad 5.20 = \\frac{26}{5}.$$\nSumming,\n\n$$\n\\begin{aligned}\n\\frac{19}{4} + \\frac{61}{20} + \\frac{26}{5}\n&= \\frac{95}{20} + \\frac{61}{20} + \\frac{104}{20} \\\\\n&= \\frac{260}{20} \\\\\n&= 13.\n\\end{aligned}\n$$\n\nThus,\n$$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) = \\frac{1}{3} \\cdot 13 = \\frac{13}{3}.$$\nThe realized joint action is $\\mathbf{a} = \\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)$, so $Q(s,\\mathbf{a}) = Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) = 5.20 = \\frac{26}{5}$.\n\nFinally, compute the COMA advantage:\n\n$$\n\\begin{aligned}\nA_i^{\\text{COMA}}(s,\\mathbf{a})\n&= Q(s,\\mathbf{a}) - b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) \\\\\n&= \\frac{26}{5} - \\frac{13}{3} \\\\\n&= \\frac{78}{15} - \\frac{65}{15} \\\\\n&= \\frac{13}{15}.\n\\end{aligned}\n$$\n\nThis is the exact numerical value of the COMA advantage under the given conditions.",
            "answer": "$$\\boxed{\\frac{13}{15}}$$"
        }
    ]
}