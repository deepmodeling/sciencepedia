{
    "hands_on_practices": [
        {
            "introduction": "多智能体强化学习的核心挑战之一是环境的非平稳性，即每个智能体的最优策略会随着其他智能体策略的改变而改变。本练习通过一个简单的梯度上升场景，让您亲手计算这种非平稳性，直观地理解一个智能体的学习如何改变另一个智能体的优化目标。通过量化梯度变化，我们将非平稳性这个抽象概念变得具体可衡量。",
            "id": "4130831",
            "problem": "两个智能体进行多智能体强化学习（MARL），该过程被建模为在一个复杂自适应系统内，对一个共享的可微阶段效用函数进行同步短视梯度上升。两个智能体都选择标量连续动作，分别表示为 $a_1$ 和 $a_2$。每个智能体通过在当前联合动作处，沿着自身效用函数相对于自身动作的梯度方向移动一小步来更新其动作。形式上，对于学习率为 $\\alpha_i$ 的智能体 $i \\in \\{1,2\\}$，其更新规则为 $a_i(t+1)=a_i(t)+\\alpha_i \\nabla_{a_i} u_i(a_1(t),a_2(t))$。假设两个智能体都最大化相同的阶段效用 $u_1(a_1,a_2)=u_2(a_1,a_2)=a_1 a_2$。在时间 $t=0$ 时，初始动作为 $a_1(0)=0.6$ 和 $a_2(0)=-0.4$，学习率分别为 $\\alpha_1=0.1$ 和 $\\alpha_2=0.01$。\n\n执行一次同步梯度上升更新以获得 $a_1(1)$ 和 $a_2(1)$。然后，为了量化在 $t=0$ 和 $t=1$ 之间每个智能体局部目标景观中诱导的非平稳性，将非平稳性幅度定义为智能体自身动作梯度变化的欧几里得范数：\n$$\nN \\equiv \\sqrt{\\left(\\nabla_{a_1} u_1(a_1(1),a_2(1))-\\nabla_{a_1} u_1(a_1(0),a_2(0))\\right)^{2}+\\left(\\nabla_{a_2} u_2(a_1(1),a_2(1))-\\nabla_{a_2} u_2(a_1(0),a_2(0))\\right)^{2}}.\n$$\n计算给定参数下的 $N$。将您的最终数值答案四舍五入到四位有效数字。以纯数字形式表示您的答案，不带单位。",
            "solution": "用户想要解决一个多智能体强化学习中的问题。\n\n### 第1步：问题验证\n\n**1.1. 提取已知条件**\n\n*   **智能体和动作**：两个智能体 $i \\in \\{1,2\\}$，其标量连续动作为 $a_1$ 和 $a_2$。\n*   **效用函数**：两个智能体共享的可微效用函数：$u_1(a_1, a_2) = u_2(a_1, a_2) = a_1 a_2$。\n*   **学习动态**：同步短视梯度上升。智能体 $i$ 的更新规则为 $a_i(t+1) = a_i(t) + \\alpha_i \\nabla_{a_i} u_i(a_1(t), a_2(t))$。\n*   **$t=0$ 时的初始条件**：$a_1(0) = 0.6$ 和 $a_2(0) = -0.4$。\n*   **学习率**：$\\alpha_1 = 0.1$ 和 $\\alpha_2 = 0.01$。\n*   **目标**：计算非平稳性幅度 $N$，其定义为：\n    $$\n    N \\equiv \\sqrt{\\left(\\nabla_{a_1} u_1(a_1(1),a_2(1))-\\nabla_{a_1} u_1(a_1(0),a_2(0))\\right)^{2}+\\left(\\nabla_{a_2} u_2(a_1(1),a_2(1))-\\nabla_{a_2} u_2(a_1(0),a_2(0))\\right)^{2}}\n    $$\n*   **输出格式**：最终数值答案四舍五入到四位有效数字。\n\n**1.2. 使用提取的已知条件进行验证**\n\n*   **科学依据**：该问题在多智能体强化学习领域有充分的理论基础。独立学习者在共享效用函数上使用同步梯度上升是研究学习动态和非平稳性的标准模型。效用函数 $u(a_1, a_2) = a_1 a_2$ 代表了一个简单的、非平凡的交互景观（原点处的一个鞍点）。\n*   **适定性**：问题陈述清晰，所有必要的参数、初始条件和定义都已提供。计算过程是微积分和代数的直接应用，可以导出一个唯一的、明确定义的解。\n*   **客观性**：问题使用精确的数学语言表达，没有主观或模糊的陈述。\n\n**1.3. 结论与行动**\n\n该问题具有科学依据、适定且客观。它被认为是**有效的**。现在开始解题过程。\n\n### 第2步：求解推导\n\n求解需要一个顺序计算过程：首先，确定智能体在时间 $t=1$ 时的动作；其次，使用这些新动作计算梯度的变化和非平稳性幅度 $N$。\n\n**2.1. 计算效用函数的梯度**\n\n共享效用函数为 $u(a_1, a_2) = a_1 a_2$。我们需要求出该效用函数相对于每个智能体自身动作的梯度。\n对于智能体1，效用函数为 $u_1(a_1, a_2) = a_1 a_2$。相对于其自身动作 $a_1$ 的梯度为：\n$$\n\\nabla_{a_1} u_1(a_1, a_2) = \\frac{\\partial}{\\partial a_1}(a_1 a_2) = a_2\n$$\n对于智能体2，效用函数为 $u_2(a_1, a_2) = a_1 a_2$。相对于其自身动作 $a_2$ 的梯度为：\n$$\n\\nabla_{a_2} u_2(a_1, a_2) = \\frac{\\partial}{\\partial a_2}(a_1 a_2) = a_1\n$$\n\n**2.2. 执行一次同步更新**\n\n智能体将其动作从 $t=0$ 更新到 $t=1$。首先，我们在初始状态 $(a_1(0), a_2(0)) = (0.6, -0.4)$ 处计算梯度。\n$$\n\\nabla_{a_1} u_1(a_1(0), a_2(0)) = a_2(0) = -0.4\n$$\n$$\n\\nabla_{a_2} u_2(a_1(0), a_2(0)) = a_1(0) = 0.6\n$$\n现在，我们对每个智能体应用更新规则 $a_i(t+1) = a_i(t) + \\alpha_i \\nabla_{a_i} u_i(t)$。\n对于智能体1：\n$$\na_1(1) = a_1(0) + \\alpha_1 \\nabla_{a_1} u_1(a_1(0), a_2(0)) = 0.6 + (0.1)(-0.4) = 0.6 - 0.04 = 0.56\n$$\n对于智能体2：\n$$\na_2(1) = a_2(0) + \\alpha_2 \\nabla_{a_2} u_2(a_1(0), a_2(0)) = -0.4 + (0.01)(0.6) = -0.4 + 0.006 = -0.394\n$$\n所以，在 $t=1$ 时新的联合动作为 $(a_1(1), a_2(1)) = (0.56, -0.394)$。\n\n**2.3. 计算非平稳性幅度 $N$**\n\n非平稳性幅度 $N$ 被定义为每个智能体自身动作梯度变化的欧几里得范数。我们已经有了 $t=0$ 时的梯度。现在我们计算新状态 $(a_1(1), a_2(1))$ 处的梯度。\n$$\n\\nabla_{a_1} u_1(a_1(1), a_2(1)) = a_2(1) = -0.394\n$$\n$$\n\\nabla_{a_2} u_2(a_1(1), a_2(1)) = a_1(1) = 0.56\n$$\n现在，我们可以计算每个梯度的变化：\n智能体1梯度的变化为 $\\Delta_1$：\n$$\n\\Delta_1 = \\nabla_{a_1} u_1(a_1(1), a_2(1)) - \\nabla_{a_1} u_1(a_1(0), a_2(0)) = -0.394 - (-0.4) = 0.006\n$$\n智能体2梯度的变化为 $\\Delta_2$：\n$$\n\\Delta_2 = \\nabla_{a_2} u_2(a_1(1), a_2(1)) - \\nabla_{a_2} u_2(a_1(0), a_2(0)) = 0.56 - 0.6 = -0.04\n$$\n最后，我们将这些变化代入 $N$ 的公式中：\n$$\nN = \\sqrt{(\\Delta_1)^2 + (\\Delta_2)^2} = \\sqrt{(0.006)^2 + (-0.04)^2}\n$$\n$$\nN = \\sqrt{0.000036 + 0.0016} = \\sqrt{0.001636}\n$$\n计算数值：\n$$\nN \\approx 0.040447496...\n$$\n题目要求答案四舍五入到四位有效数字。第一位有效数字是百分位上的 $4$。数四位数字得到 $0.04044$。下一位数字是 $7$，大于等于 $5$，所以我们对最后一位进行向上取整。\n$$\nN \\approx 0.04045\n$$\n这是最终的数值答案。",
            "answer": "$$\n\\boxed{0.04045}\n$$"
        },
        {
            "introduction": "“中心化训练，去中心化执行” (CTDE) 是解决多智能体协作问题的有效范式。本练习将带您实践价值分解网络（Value-Decomposition Networks）的核心思想，特别是如何通过保证单调性的混合网络，使得每个智能体仅需最大化自己的局部效用函数，就能确保联合动作是全局最优的。这个练习揭示了实现高效去中心化决策的关键机制。",
            "id": "4130864",
            "problem": "考虑一个在集中式训练、分布式执行 (CTDE) 范式下的协作式双智能体马尔可夫博弈。设联合动作价值函数由一个单调线性混合网络表示，定义为 $Q_{\\text{tot}}(s,\\mathbf{a})=w_1 Q_1(s,a_1)+w_2 Q_2(s,a_2)+b$，其中 $w_1 \\ge 0$, $w_2 \\ge 0$, $b \\in \\mathbb{R}$，$Q_i(s,a_i)$ 是智能体 $i \\in \\{1,2\\}$ 在状态 $s$ 和动作 $a_i$ 下的智能体特定效用，$\\mathbf{a}=(a_1,a_2)$ 是联合动作。假设状态 $s$ 固定，离散动作集为 $\\mathcal{A}_1=\\{0,1,2\\}$ 和 $\\mathcal{A}_2=\\{0,1,2\\}$，且具有以下智能体效用：\n$$Q_1(s,0)=1.437,\\quad Q_1(s,1)=1.513,\\quad Q_1(s,2)=1.499,$$\n$$Q_2(s,0)=2.105,\\quad Q_2(s,1)=2.062,\\quad Q_2(s,2)=2.176,$$\n混合参数为 $w_1=0.73$，$w_2=1.27$，$b=-0.418$。仅使用联合动作价值函数、贪婪选择和单调映射的基本定义，确定在分布式选择下的贪婪联合动作，其中每个智能体独立选择使其自身智能体特定效用最大化的动作。最终答案必须是智能体1和智能体2所选动作索引的有序对，并按此顺序排列。无需进行四舍五入。",
            "solution": "### 求解过程\n目标是在去中心化执行下找到贪婪联合动作。问题明确指出了此选择的规则：“每个智能体独立选择使其自身智能体特定效用最大化的动作。”\n\n设 $a_1^*$ 为智能体1选择的动作，$a_2^*$ 为智能体2选择的动作。根据指定规则，这些动作的确定方式如下：\n$$a_1^* = \\arg\\max_{a_1 \\in \\mathcal{A}_1} Q_1(s, a_1)$$\n$$a_2^* = \\arg\\max_{a_2 \\in \\mathcal{A}_2} Q_2(s, a_2)$$\n\n**智能体1的动作选择：**\n我们必须找到动作 $a_1 \\in \\{0, 1, 2\\}$ 以最大化 $Q_1(s, a_1)$。为智能体1给出的效用如下：\n-   $Q_1(s,0) = 1.437$\n-   $Q_1(s,1) = 1.513$\n-   $Q_1(s,2) = 1.499$\n\n通过比较这些值，我们发现最大值为 $1.513$。\n$$\\max(1.437, 1.513, 1.499) = 1.513$$\n这个最大效用对应的动作索引是 $1$。因此，智能体1选择动作 $a_1^*=1$。\n\n**智能体2的动作选择：**\n我们必须找到动作 $a_2 \\in \\{0, 1, 2\\}$ 以最大化 $Q_2(s, a_2)$。为智能体2给出的效用如下：\n-   $Q_2(s,0) = 2.105$\n-   $Q_2(s,1) = 2.062$\n-   $Q_2(s,2) = 2.176$\n\n通过比较这些值，我们发现最大值为 $2.176$。\n$$\\max(2.105, 2.062, 2.176) = 2.176$$\n这个最大效用对应的动作索引是 $2$。因此，智能体2选择动作 $a_2^*=2$。\n\n**贪婪联合动作：**\n在去中心化选择下的贪婪联合动作是由各个智能体独立选择的动作组成的有序对 $(a_1^*, a_2^*)$。\n根据以上计算，联合动作为 $(1, 2)$。\n\n问题的设定（特别是混合网络权重 $w_1, w_2$ 为非负）确保了每个智能体的局部贪婪选择能够汇合成全局最优的贪婪联合动作。这个属性被称为个体-全局-最大化（IGM）一致性，是价值分解方法（如VDN和QMIX）的核心。因此，我们只需要为每个智能体独立找到最优动作即可。\n\n最终答案是智能体1和智能体2的动作索引的有序对。",
            "answer": "$$\n\\boxed{(1, 2)}\n$$"
        },
        {
            "introduction": "在多智能体策略梯度方法中，如何准确地分配“功劳”是一个关键难题。本练习聚焦于反事实多智能体（COMA）算法的精髓，即通过构建一个反事实基线来计算优势函数（Advantage Function）。您将计算一个智能体在特定联合动作下的优势值，从而理解COMA如何巧妙地衡量单个智能体动作对团队整体收益的边际贡献。",
            "id": "4130833",
            "problem": "考虑一个完全合作的马尔可夫博弈，其中有 $n$ 个智能体，采用集中式评价者和分布式执行者（centralized critic and decentralized actors）的框架进行训练。在某个特定时间步，固定全局状态 $s$ 和其他智能体的联合动作 $\\mathbf{a}_{-i}$。智能体 $i$ 有一个离散动作集 $\\{a_i^{(0)}, a_i^{(1)}, a_i^{(2)}\\}$，在此时间步，其执行者策略 $\\pi_i$ 是均匀的，即对所有 $k \\in \\{0,1,2\\}$，有 $\\pi_i(a_i^{(k)} \\mid o_i) = \\frac{1}{3}$，其中 $o_i$ 表示智能体 $i$ 的局部观测。集中式评价者对智能体 $i$ 的动作进行反事实替换（同时保持 $\\mathbf{a}_{-i}$ 不变）后，输出以下动作值：\n$$Q\\!\\left(s,\\left(a_i^{(0)},\\mathbf{a}_{-i}\\right)\\right) = 4.75,\\quad Q\\!\\left(s,\\left(a_i^{(1)},\\mathbf{a}_{-i}\\right)\\right) = 3.05,\\quad Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) = 5.20.$$\n假设实现的联合动作为 $\\mathbf{a} = \\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)$，因此 $Q(s,\\mathbf{a}) = 5.20$。运用带有状态依赖基线的方差缩减多智能体策略梯度原理，以及反事实多智能体（Counterfactual Multi-Agent, COMA）策略梯度中的反事实基线概念，推导智能体 $i$ 在 $(s,\\mathbf{a})$ 处的优势函数的解析表达式，并根据给定的均匀策略 $\\pi_i$ 和评价者输出计算其数值。将最终答案表示为一个纯数字。无需进行四舍五入。",
            "solution": "起点是多智能体执行者-评价者（actor-critic）策略梯度。对于智能体 $i$，它使用优势函数来减少方差，同时保持梯度无偏。强化学习的目标 $J(\\theta)$（其中 $\\theta$ 表示所有策略参数）对智能体 $i$ 产生的梯度为\n$$\\nabla_{\\theta_i} J(\\theta) = \\mathbb{E}\\!\\left[\\nabla_{\\theta_i} \\ln \\pi_i(a_i \\mid o_i)\\, A_i(s,\\mathbf{a})\\right],$$\n其中 $A_i(s,\\mathbf{a})$ 是任何经过基线调整的信号，它依赖于状态 $s$ 和联合动作 $\\mathbf{a}$，并且其构造方式保证了在以 $o_i$ 为条件时，其期望不会使梯度产生偏差：\n$$\\mathbb{E}\\!\\left[\\nabla_{\\theta_i} \\ln \\pi_i(a_i \\mid o_i)\\, b_i(s,\\mathbf{a}_{-i})\\right] = 0,$$\n前提是基线 $b_i(s,\\mathbf{a}_{-i})$ 不依赖于 $a_i$。一个常见的选择是令 $A_i(s,\\mathbf{a}) = Q(s,\\mathbf{a}) - b_i(s,\\mathbf{a}_{-i})$，其中 $Q(s,\\mathbf{a})$ 是在状态 $s$ 下联合动作的集中式动作值。\n\n反事实多智能体（COMA）基线旨在通过固定其他智能体的动作，并根据智能体 $i$ 的当前策略对其动作进行边缘化，来解决多智能体信用分配问题。具体来说，智能体 $i$ 的 COMA 基线定义为\n$$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) = \\sum_{a_i'} \\pi_i(a_i' \\mid o_i)\\, Q\\!\\left(s,\\left(a_i',\\mathbf{a}_{-i}\\right)\\right),$$\n这是评价者值关于智能体 $i$ 策略的期望，通过反事实地替换其动作同时保持 $\\mathbf{a}_{-i}$ 不变来计算。这种方法保持了无偏性，因为在以 $o_i$ 为条件时，$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i})$ 与实现的动作 $a_i$ 无关。\n\n因此，智能体 $i$ 在 $(s,\\mathbf{a})$ 处的 COMA 优势为\n$$A_i^{\\text{COMA}}(s,\\mathbf{a}) = Q(s,\\mathbf{a}) - b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}).$$\n\n对于给定的均匀策略 $\\pi_i(a_i' \\mid o_i) = \\frac{1}{3}$（对所有 $a_i' \\in \\{a_i^{(0)}, a_i^{(1)}, a_i^{(2)}\\}$ 成立），我们将 COMA 基线计算为所提供的三个评价值的简单平均值：\n\n$$\n\\begin{aligned}\nb_i^{\\text{COMA}}(s,\\mathbf{a}_{-i})\n= \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(0)},\\mathbf{a}_{-i}\\right)\\right)\n+ \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(1)},\\mathbf{a}_{-i}\\right)\\right)\n+ \\frac{1}{3}\\, Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) \\\\\n= \\frac{1}{3}\\left(4.75 + 3.05 + 5.20\\right).\n\\end{aligned}\n$$\n\n为保持精确性，将这些小数转换为有理数：\n$$4.75 = \\frac{19}{4}, \\quad 3.05 = \\frac{61}{20}, \\quad 5.20 = \\frac{26}{5}.$$\n求和，\n\n$$\n\\begin{aligned}\n\\frac{19}{4} + \\frac{61}{20} + \\frac{26}{5}\n= \\frac{95}{20} + \\frac{61}{20} + \\frac{104}{20} \\\\\n= \\frac{260}{20} \\\\\n= 13.\n\\end{aligned}\n$$\n\n因此，\n$$b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) = \\frac{1}{3} \\cdot 13 = \\frac{13}{3}.$$\n实现的联合动作为 $\\mathbf{a} = \\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)$，所以 $Q(s,\\mathbf{a}) = Q\\!\\left(s,\\left(a_i^{(2)},\\mathbf{a}_{-i}\\right)\\right) = 5.20 = \\frac{26}{5}$。\n\n最后，计算 COMA 优势：\n\n$$\n\\begin{aligned}\nA_i^{\\text{COMA}}(s,\\mathbf{a})\n= Q(s,\\mathbf{a}) - b_i^{\\text{COMA}}(s,\\mathbf{a}_{-i}) \\\\\n= \\frac{26}{5} - \\frac{13}{3} \\\\\n= \\frac{78}{15} - \\frac{65}{15} \\\\\n= \\frac{13}{15}.\n\\end{aligned}\n$$\n\n这就是在给定条件下 COMA 优势的精确数值。",
            "answer": "$$\\boxed{\\frac{13}{15}}$$"
        }
    ]
}