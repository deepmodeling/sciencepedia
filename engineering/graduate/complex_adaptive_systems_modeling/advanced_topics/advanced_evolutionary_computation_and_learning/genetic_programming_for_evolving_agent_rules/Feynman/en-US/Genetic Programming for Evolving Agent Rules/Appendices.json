{
    "hands_on_practices": [
        {
            "introduction": "Genetic Programming (GP) explores the vast space of possible programs using operators like mutation. In Strongly Typed GP, these operators must respect type constraints to ensure offspring are syntactically valid, a crucial feature for evolving complex agent rules. This exercise explores the interplay between the random nature of mutation and the rigid constraints of a type system.\n\nBy calculating the exact probability of a valid mutation, you will gain a deeper, quantitative understanding of how the composition of the function and terminal sets directly influences the search process . This is a foundational skill for designing and troubleshooting effective GP systems for agent-based modeling.",
            "id": "4125248",
            "problem": "Consider a strongly typed Genetic Programming (GP) system evolving agent decision rules in a multi-type domain with the type set $\\{\\mathsf{R}, \\mathsf{B}, \\mathsf{U}\\}$, where $\\mathsf{R}$ denotes real-valued quantities, $\\mathsf{B}$ denotes booleans, and $\\mathsf{U}$ denotes actions. The function and terminal library is defined by arity and type signatures as follows.\n\n- Arity $0$ terminals (each terminal has only an output type): there are $5$ terminals of type $\\mathsf{R}$, $2$ of type $\\mathsf{B}$, and $3$ of type $\\mathsf{U}$.\n- Arity $1$ functions (each function has one input and one output type): there are $4$ functions of signature $\\mathsf{R} \\to \\mathsf{R}$, $2$ of $\\mathsf{B} \\to \\mathsf{B}$, $1$ of $\\mathsf{U} \\to \\mathsf{R}$, $1$ of $\\mathsf{U} \\to \\mathsf{B}$, and $1$ of $\\mathsf{R} \\to \\mathsf{B}$.\n- Arity $2$ functions (each function has two ordered inputs and one output type): there are $3$ of $(\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}$, $2$ of $(\\mathsf{R},\\mathsf{R}) \\to \\mathsf{B}$, $3$ of $(\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}$, $1$ of $(\\mathsf{U},\\mathsf{R}) \\to \\mathsf{U}$, and $1$ of $(\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}$.\n- Arity $3$ functions (each function has three ordered inputs and one output type): there is $1$ of $(\\mathsf{B},\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}$, $1$ of $(\\mathsf{B},\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}$, and $1$ of $(\\mathsf{B},\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}$.\n\nA particular well-typed program tree (agent rule) has $12$ nodes, each with the following context, where each node’s context consists of its arity $k$, the ordered tuple of its children’s output types, and its own output type (which matches the parent’s expected input type for that position in the original well-typed tree, or the program’s output type if it is the root):\n\n- Node $1$: $k=3$, children $(\\mathsf{B},\\mathsf{U},\\mathsf{U})$, output $\\mathsf{U}$.\n- Node $2$: $k=1$, child $(\\mathsf{U})$, output $\\mathsf{B}$.\n- Node $3$: $k=2$, children $(\\mathsf{R},\\mathsf{R})$, output $\\mathsf{B}$.\n- Node $4$: $k=2$, children $(\\mathsf{R},\\mathsf{R})$, output $\\mathsf{R}$.\n- Node $5$: $k=1$, child $(\\mathsf{R})$, output $\\mathsf{R}$.\n- Node $6$: $k=0$, output $\\mathsf{U}$.\n- Node $7$: $k=0$, output $\\mathsf{R}$.\n- Node $8$: $k=0$, output $\\mathsf{B}$.\n- Node $9$: $k=3$, children $(\\mathsf{B},\\mathsf{R},\\mathsf{R})$, output $\\mathsf{R}$.\n- Node $10$: $k=2$, children $(\\mathsf{B},\\mathsf{B})$, output $\\mathsf{B}$.\n- Node $11$: $k=1$, child $(\\mathsf{R})$, output $\\mathsf{B}$.\n- Node $12$: $k=0$, output $\\mathsf{R}$.\n\nA point mutation is defined as follows: a node is selected uniformly at random among the $12$ nodes, and its symbol is replaced by a symbol drawn uniformly at random from the global library of symbols of the same arity $k$. The tree shape and the subtrees of the node’s children are unchanged. The mutation operator is not type-aware; in general, the replacement may or may not be type-compatible with the children and the parent’s expected type.\n\nUsing the foundational property of strong typing for trees that global type correctness is preserved if and only if, at the mutated node, the replacement symbol’s input types match exactly the children’s output types in order and its output type matches the type expected by the parent at that position (or the program’s output type if the node is the root), compute the exact probability that a single uniformly random point mutation (as defined) yields a well-typed tree.\n\nExpress your final answer as a single simplified exact fraction. Do not round.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n\n- **Type Set**: $\\{\\mathsf{R}, \\mathsf{B}, \\mathsf{U}\\}$, representing real, boolean, and action types.\n- **Function and Terminal Library**:\n    - **Arity 0 (Terminals)**:\n        - $5$ of type $\\mathsf{R}$.\n        - $2$ of type $\\mathsf{B}$.\n        - $3$ of type $\\mathsf{U}$.\n    - **Arity 1 (Functions)**:\n        - $4$ of signature $\\mathsf{R} \\to \\mathsf{R}$.\n        - $2$ of signature $\\mathsf{B} \\to \\mathsf{B}$.\n        - $1$ of signature $\\mathsf{U} \\to \\mathsf{R}$.\n        - $1$ of signature $\\mathsf{U} \\to \\mathsf{B}$.\n        - $1$ of signature $\\mathsf{R} \\to \\mathsf{B}$.\n    - **Arity 2 (Functions)**:\n        - $3$ of signature $(\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}$.\n        - $2$ of signature $(\\mathsf{R},\\mathsf{R}) \\to \\mathsf{B}$.\n        - $3$ of signature $(\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}$.\n        - $1$ of signature $(\\mathsf{U},\\mathsf{R}) \\to \\mathsf{U}$.\n        - $1$ of signature $(\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}$.\n    - **Arity 3 (Functions)**:\n        - $1$ of signature $(\\mathsf{B},\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}$.\n        - $1$ of signature $(\\mathsf{B},\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}$.\n        - $1$ of signature $(\\mathsf{B},\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}$.\n- **Program Tree Structure**: A tree of $12$ nodes with the following contexts (arity $k$, children's output types, own output type):\n    - Node $1$: $k=3$, children $(\\mathsf{B},\\mathsf{U},\\mathsf{U})$, output $\\mathsf{U}$.\n    - Node $2$: $k=1$, child $(\\mathsf{U})$, output $\\mathsf{B}$.\n    - Node $3$: $k=2$, children $(\\mathsf{R},\\mathsf{R})$, output $\\mathsf{B}$.\n    - Node $4$: $k=2$, children $(\\mathsf{R},\\mathsf{R})$, output $\\mathsf{R}$.\n    - Node $5$: $k=1$, child $(\\mathsf{R})$, output $\\mathsf{R}$.\n    - Node $6$: $k=0$, output $\\mathsf{U}$.\n    - Node $7$: $k=0$, output $\\mathsf{R}$.\n    - Node $8$: $k=0$, output $\\mathsf{B}$.\n    - Node $9$: $k=3$, children $(\\mathsf{B},\\mathsf{R},\\mathsf{R})$, output $\\mathsf{R}$.\n    - Node $10$: $k=2$, children $(\\mathsf{B},\\mathsf{B})$, output $\\mathsf{B}$.\n    - Node $11$: $k=1$, child $(\\mathsf{R})$, output $\\mathsf{B}$.\n    - Node $12$: $k=0$, output $\\mathsf{R}$.\n- **Mutation Operator**:\n    - A node is selected uniformly at random from the $12$ nodes.\n    - Its symbol is replaced by another symbol drawn uniformly at random from the subset of the global library containing all symbols of the same arity $k$.\n- **Condition for Validity**: A mutated tree is well-typed if and only if the replacement symbol's input types match the children's output types and its output type matches the type expected by its parent.\n- **Goal**: Compute the exact probability that a single such mutation yields a well-typed tree.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is set in the context of Genetic Programming (GP), a subfield of evolutionary computation and artificial intelligence. Strongly typed GP and point mutation are standard concepts in this field. The problem is a well-defined exercise in combinatorial probability based on these concepts. It is free of pseudoscience and speculation.\n- **Well-Posed**: The problem provides all necessary information: the complete set of types, a complete inventory of available genetic operators (terminals and functions) with their type signatures, the structure of the program tree, and a precise definition of the mutation operation and the condition for a successful (type-correct) outcome. This allows for the calculation of a unique, exact probability.\n- **Objective**: The problem is stated in precise, formal language common to computer science and mathematics. It is free of subjective claims or ambiguity.\n\nThe problem is a standard, self-contained, and consistent exercise in probability theory applied to a specific algorithm in computer science. It meets all criteria for validity.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A solution will be provided.\n\n## SOLUTION\n\nThe problem asks for the probability that a single point mutation on a specific $12$-node program tree results in a well-typed tree. The mutation process involves two uniformly random choices: first, selecting one of the $N=12$ nodes to mutate, and second, selecting a replacement symbol from the set of all symbols with the same arity as the chosen node.\n\nLet $E$ be the event that the mutation results in a well-typed tree, and let $C_i$ be the event that node $i$ is chosen for mutation, for $i \\in \\{1, 2, \\dots, 12\\}$. Since a node is chosen uniformly at random, the probability of choosing any specific node $i$ is $P(C_i) = \\frac{1}{12}$.\n\nThe total probability of a valid mutation is the sum of probabilities of a valid mutation at each node, weighted by the probability of choosing that node. By the law of total probability:\n$$P(E) = \\sum_{i=1}^{12} P(E | C_i) P(C_i)$$\nSince $P(C_i) = \\frac{1}{12}$ for all $i$, this simplifies to:\n$$P(E) = \\frac{1}{12} \\sum_{i=1}^{12} P(E | C_i)$$\nHere, $P(E | C_i)$ is the probability that a mutation at node $i$ is type-correct. This occurs if the randomly chosen replacement symbol has a type signature that matches the context of node $i$. The replacement is chosen uniformly from all symbols in the library with the same arity as node $i$.\n\nLet's first tabulate the total number of available symbols for each arity $k$. Let $N_k$ be the total number of symbols with arity $k$.\n- Arity $k=0$: $N_0 = 5 (\\text{type }\\mathsf{R}) + 2 (\\text{type }\\mathsf{B}) + 3 (\\text{type }\\mathsf{U}) = 10$.\n- Arity $k=1$: $N_1 = 4 (\\mathsf{R} \\to \\mathsf{R}) + 2 (\\mathsf{B} \\to \\mathsf{B}) + 1 (\\mathsf{U} \\to \\mathsf{R}) + 1 (\\mathsf{U} \\to \\mathsf{B}) + 1 (\\mathsf{R} \\to \\mathsf{B}) = 9$.\n- Arity $k=2$: $N_2 = 3 ((\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}) + 2 ((\\mathsf{R},\\mathsf{R}) \\to \\mathsf{B}) + 3 ((\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}) + 1 ((\\mathsf{U},\\mathsf{R}) \\to \\mathsf{U}) + 1 ((\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}) = 10$.\n- Arity $k=3$: $N_3 = 1 ((\\mathsf{B},\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}) + 1 ((\\mathsf{B},\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}) + 1 ((\\mathsf{B},\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}) = 3$.\n\nNow we calculate $P_i = P(E|C_i)$ for each node $i=1, \\dots, 12$. $P_i$ is the ratio of the number of symbols with the required type signature to the total number of symbols of the same arity.\n\n- **Node 1**: $k=3$, signature $(\\mathsf{B},\\mathsf{U},\\mathsf{U}) \\to \\mathsf{U}$.\nThe library has $1$ function with this signature. Total functions of arity $3$ is $N_3=3$.\n$P_1 = \\frac{1}{3}$.\n\n- **Node 2**: $k=1$, signature $\\mathsf{U} \\to \\mathsf{B}$.\nThe library has $1$ function with this signature. Total functions of arity $1$ is $N_1=9$.\n$P_2 = \\frac{1}{9}$.\n\n- **Node 3**: $k=2$, signature $(\\mathsf{R},\\mathsf{R}) \\to \\mathsf{B}$.\nThe library has $2$ functions with this signature. Total functions of arity $2$ is $N_2=10$.\n$P_3 = \\frac{2}{10} = \\frac{1}{5}$.\n\n- **Node 4**: $k=2$, signature $(\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}$.\nThe library has $3$ functions with this signature. Total functions of arity $2$ is $N_2=10$.\n$P_4 = \\frac{3}{10}$.\n\n- **Node 5**: $k=1$, signature $\\mathsf{R} \\to \\mathsf{R}$.\nThe library has $4$ functions with this signature. Total functions of arity $1$ is $N_1=9$.\n$P_5 = \\frac{4}{9}$.\n\n- **Node 6**: $k=0$, output type $\\mathsf{U}$.\nThe library has $3$ terminals of this type. Total terminals of arity $0$ is $N_0=10$.\n$P_6 = \\frac{3}{10}$.\n\n- **Node 7**: $k=0$, output type $\\mathsf{R}$.\nThe library has $5$ terminals of this type. Total terminals of arity $0$ is $N_0=10$.\n$P_7 = \\frac{5}{10} = \\frac{1}{2}$.\n\n- **Node 8**: $k=0$, output type $\\mathsf{B}$.\nThe library has $2$ terminals of this type. Total terminals of arity $0$ is $N_0=10$.\n$P_8 = \\frac{2}{10} = \\frac{1}{5}$.\n\n- **Node 9**: $k=3$, signature $(\\mathsf{B},\\mathsf{R},\\mathsf{R}) \\to \\mathsf{R}$.\nThe library has $1$ function with this signature. Total functions of arity $3$ is $N_3=3$.\n$P_9 = \\frac{1}{3}$.\n\n- **Node 10**: $k=2$, signature $(\\mathsf{B},\\mathsf{B}) \\to \\mathsf{B}$.\nThe library has $3$ functions with this signature. Total functions of arity $2$ is $N_2=10$.\n$P_{10} = \\frac{3}{10}$.\n\n- **Node 11**: $k=1$, signature $\\mathsf{R} \\to \\mathsf{B}$.\nThe library has $1$ function with this signature. Total functions of arity $1$ is $N_1=9$.\n$P_{11} = \\frac{1}{9}$.\n\n- **Node 12**: $k=0$, output type $\\mathsf{R}$.\nThe library has $5$ terminals of this type. Total terminals of arity $0$ is $N_0=10$.\n$P_{12} = \\frac{5}{10} = \\frac{1}{2}$.\n\nNow, we sum these probabilities:\n$$ \\sum_{i=1}^{12} P_i = P_1 + P_2 + P_3 + P_4 + P_5 + P_6 + P_7 + P_8 + P_9 + P_{10} + P_{11} + P_{12} $$\n$$ \\sum_{i=1}^{12} P_i = \\frac{1}{3} + \\frac{1}{9} + \\frac{1}{5} + \\frac{3}{10} + \\frac{4}{9} + \\frac{3}{10} + \\frac{1}{2} + \\frac{1}{5} + \\frac{1}{3} + \\frac{3}{10} + \\frac{1}{9} + \\frac{1}{2} $$\nGrouping terms with common denominators:\n$$ \\sum_{i=1}^{12} P_i = \\left(\\frac{1}{3}+\\frac{1}{3}\\right) + \\left(\\frac{1}{9}+\\frac{4}{9}+\\frac{1}{9}\\right) + \\left(\\frac{1}{5}+\\frac{1}{5}\\right) + \\left(\\frac{3}{10}+\\frac{3}{10}+\\frac{3}{10}\\right) + \\left(\\frac{1}{2}+\\frac{1}{2}\\right) $$\n$$ \\sum_{i=1}^{12} P_i = \\frac{2}{3} + \\frac{6}{9} + \\frac{2}{5} + \\frac{9}{10} + \\frac{2}{2} $$\n$$ \\sum_{i=1}^{12} P_i = \\frac{2}{3} + \\frac{2}{3} + \\frac{2}{5} + \\frac{9}{10} + 1 $$\n$$ \\sum_{i=1}^{12} P_i = \\frac{4}{3} + \\frac{2}{5} + \\frac{9}{10} + 1 $$\nTo sum these fractions, we find a common denominator, which is $\\text{lcm}(3, 5, 10) = 30$.\n$$ \\sum_{i=1}^{12} P_i = \\frac{4 \\times 10}{3 \\times 10} + \\frac{2 \\times 6}{5 \\times 6} + \\frac{9 \\times 3}{10 \\times 3} + \\frac{30}{30} $$\n$$ \\sum_{i=1}^{12} P_i = \\frac{40}{30} + \\frac{12}{30} + \\frac{27}{30} + \\frac{30}{30} $$\n$$ \\sum_{i=1}^{12} P_i = \\frac{40 + 12 + 27 + 30}{30} = \\frac{109}{30} $$\nFinally, we compute the total probability $P(E)$:\n$$ P(E) = \\frac{1}{12} \\sum_{i=1}^{12} P_i = \\frac{1}{12} \\times \\frac{109}{30} $$\n$$ P(E) = \\frac{109}{12 \\times 30} = \\frac{109}{360} $$\nThe number $109$ is a prime number, so this fraction is in its simplest form.",
            "answer": "$$\\boxed{\\frac{109}{360}}$$"
        },
        {
            "introduction": "Evolving agent rules rarely involves optimizing for a single, simple goal; more often, we must balance competing objectives such as maximizing economic efficiency while minimizing a fairness penalty. A common technique in Genetic Programming is to combine these objectives into a single scalar fitness value, often through a weighted sum, where the fitness to be maximized is $f_i(\\alpha) = \\alpha E_i - (1 - \\alpha) G_i$. This practice delves into the critical implications of this method.\n\nBy analyzing how the ranking of candidate rules changes as the weight parameter $\\alpha$ varies, you will discover that the choice of weights actively shapes the fitness landscape and determines which trade-offs the evolutionary process will favor . This hands-on analysis builds an essential intuition for multi-objective optimization and the importance of exploring the entire frontier of optimal trade-offs.",
            "id": "4125227",
            "problem": "Consider a population of candidate agent rules evaluated within a complex adaptive system. In Genetic Programming (GP), selection operates on a scalar fitness, which here is defined by a weighted-sum scalarization of two objectives: an efficiency metric and a fairness penalty. For each candidate rule $i$ with efficiency $E_i \\in \\mathbb{R}$ and fairness penalty $G_i \\in \\mathbb{R}$, define the scalarized fitness as $f_i(\\alpha) = \\alpha E_i - (1 - \\alpha) G_i$ for a weight parameter $\\alpha \\in [0,1]$. Assume the scalarized fitness $f_i(\\alpha)$ is to be maximized. The goal is to analyze the sensitivity of the ranking of rules induced by $f_i(\\alpha)$ with respect to changes in $\\alpha$.\n\nStarting from fundamental definitions in multi-objective optimization and without using any shortcut formulas, do the following for each test case:\n\n- Given $n$ candidate rules with metrics $\\{E_i\\}_{i=1}^n$ and $\\{G_i\\}_{i=1}^n$, determine all critical values of $\\alpha \\in [0,1]$ where the ranking of rules by $f_i(\\alpha)$ can change. These occur when $f_i(\\alpha) = f_j(\\alpha)$ for some $i \\neq j$. Use a numerical tolerance $\\varepsilon = 10^{-9}$ to treat near-equalities robustly. Include the endpoints $\\alpha = 0$ and $\\alpha = 1$ as boundaries of the analysis.\n- Partition the interval $[0,1]$ by the sorted unique critical values of $\\alpha$ to obtain regions over which the ranking remains constant. Report the number of such regions.\n- Compute the Kendall rank correlation coefficient (Kendall tau) between the rankings at $\\alpha = 0$ and $\\alpha = 1$ to quantify the extremal sensitivity of rankings. Use the standard definition of Kendall tau as implemented in statistical software. When ties occur, break ties deterministically by ascending index $i$ for stability.\n- Report whether the identity of the top-ranked rule (the rule with maximal fitness) changes from $\\alpha = 0$ to $\\alpha = 1$.\n- Provide the list of all critical $\\alpha$ values in $[0,1]$ (including duplicates consolidated by tolerance), sorted in ascending order. Express all $\\alpha$ values as decimals rounded to six places.\n\nYour program must implement this analysis and produce the results for the following test suite. For each test case, the inputs are lists of $E_i$ and $G_i$:\n\n- Test Case 1 (general crossing case): $E = \\{0.80, 0.60, 0.90, 0.50\\}$ and $G = \\{0.30, 0.20, 0.60, 0.10\\}$.\n- Test Case 2 (boundary intersections and ties): $E = \\{1.00, 0.50, 0.50\\}$ and $G = \\{0.40, 0.40, 0.10\\}$.\n- Test Case 3 (degenerate parallel lines): $E = \\{0.70, 0.50, 0.90\\}$ and $G = \\{0.30, 0.50, 0.10\\}$.\n- Test Case 4 (higher-dimensional sensitivity): $E = \\{0.92, 0.81, 0.74, 0.66, 0.59, 0.52\\}$ and $G = \\{0.12, 0.28, 0.21, 0.33, 0.27, 0.38\\}$.\n\nAngle units are not applicable, and no physical units are involved; all quantities are dimensionless real numbers. Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of the form $[N_\\text{regions}, \\tau_{0,1}, \\text{top\\_changed}, [\\text{breakpoints}]]$. Here, $N_\\text{regions}$ is an integer, $\\tau_{0,1}$ is a float (rounded to six decimal places), $\\text{top\\_changed}$ is a boolean, and $[\\text{breakpoints}]$ is a list of floats (each rounded to six decimal places). For example, the final output format should resemble $[[2,0.333333,True,[0.000000,1.000000]],\\dots]$ with no spaces anywhere in the line.",
            "solution": "The user-provided problem has been assessed and is determined to be **valid**. It is a well-posed, scientifically grounded problem in the domain of multi-objective optimization, specifically using the weighted-sum method for scalarization. The problem is self-contained, with all necessary data and definitions provided. We may therefore proceed with the derivation and solution.\n\nThe core of the problem is to analyze the sensitivity of the ranking of a set of $n$ candidate rules, where the ranking is determined by a scalar fitness function $f_i(\\alpha)$. The fitness of rule $i$ is given by:\n$$f_i(\\alpha) = \\alpha E_i - (1 - \\alpha) G_i$$\nwhere $E_i$ is the efficiency, $G_i$ is the fairness penalty, and $\\alpha \\in [0,1]$ is a weight parameter. The optimization objective is to maximize $f_i(\\alpha)$.\n\nThe ranking of the rules is a list of their indices, sorted in descending order of their fitness values $f_i(\\alpha)$. This ranking can change only when the fitness values of two distinct rules become equal. These occurrences define the critical values of $\\alpha$. Let's consider two distinct rules, $i$ and $j$, where $i \\neq j$. A critical value of $\\alpha$ occurs when their fitness values are equal:\n$$f_i(\\alpha) = f_j(\\alpha)$$\nSubstituting the definition of the fitness function:\n$$\\alpha E_i - (1 - \\alpha) G_i = \\alpha E_j - (1 - \\alpha) G_j$$\nWe can rearrange this equation to solve for $\\alpha$. Expanding the terms:\n$$\\alpha E_i - G_i + \\alpha G_i = \\alpha E_j - G_j + \\alpha G_j$$\nGrouping terms with $\\alpha$ on one side and constant terms on the other:\n$$\\alpha E_i + \\alpha G_i - \\alpha E_j - \\alpha G_j = G_i - G_j$$\nFactoring out $\\alpha$:\n$$\\alpha (E_i + G_i - E_j - G_j) = G_i - G_j$$\nThis can be rewritten using $\\Delta E = E_i - E_j$ and $\\Delta G = G_i - G_j$:\n$$\\alpha (\\Delta E + \\Delta G) = \\Delta G$$\nProvided that the denominator is non-zero, the critical value $\\alpha_{ij}$ at which rules $i$ and $j$ have the same fitness is:\n$$\\alpha_{ij} = \\frac{G_i - G_j}{(E_i + G_i) - (E_j + G_j)}$$\nA critical value $\\alpha_{ij}$ is relevant to our analysis only if it falls within the interval $[0,1]$.\n\nA special case arises if the denominator is zero, i.e., $(E_i + G_i) - (E_j + G_j) = 0$. This implies that the functions $f_i(\\alpha)$ and $f_j(\\alpha)$, which are linear in $\\alpha$, have the same slope.\n- If the numerator $G_i - G_j$ is also zero, the lines are identical, meaning $f_i(\\alpha) = f_j(\\alpha)$ for all $\\alpha$. The relative order of rules $i$ and $j$ is then determined solely by the tie-breaking rule (by index) and never changes.\n- If the numerator is non-zero, the lines are parallel and distinct. They never intersect, so the relative ranking between rules $i$ and $j$ remains constant for all $\\alpha$.\nIn both parallel cases, no new critical point is generated.\n\nThe overall algorithm for each test case is as follows:\n\n1.  **Identify Critical Points**: Iterate through all unique pairs of rules $(i, j)$ with $i < j$. For each pair, calculate the denominator $D = (E_i + G_i) - (E_j + G_j)$. If $|D|$ is greater than a small tolerance $\\varepsilon = 10^{-9}$, calculate the critical value $\\alpha_{ij} = (G_i - G_j)/D$. Collect all such $\\alpha_{ij}$ that lie within the interval $[0,1]$.\n\n2.  **Partition the Interval**: Create a set of breakpoints by taking all collected critical points from step $1$ and adding the boundaries $\\alpha=0$ and $\\alpha=1$. Sort these points and remove duplicates using the tolerance $\\varepsilon$ to merge near-identical values. The number of regions of constant ranking, $N_\\text{regions}$, is one less than the number of unique breakpoints.\n\n3.  **Analyze Boundary Rankings**:\n    a. Determine the ranking at $\\alpha=0$. The fitness is $f_i(0) = -G_i$. The ranking is obtained by sorting the rule indices based on descending values of $-G_i$.\n    b. Determine the ranking at $\\alpha=1$. The fitness is $f_i(1) = E_i$. The ranking is obtained by sorting the rule indices based on descending values of $E_i$.\n    c. In both cases, ties in fitness values are broken by choosing the rule with the lower original index $i$.\n\n4.  **Compute Sensitivity Metrics**:\n    a. **Top-Ranked Rule Change**: Compare the index of the top-ranked rule at $\\alpha=0$ with the index of the top-ranked rule at $\\alpha=1$. This yields a boolean value, `top_changed`.\n    b. **Kendall Tau Correlation**: Using the complete rankings from $\\alpha=0$ and $\\alpha=1$, compute the Kendall rank correlation coefficient, $\\tau$. This coefficient measures the ordinal association between the two rankings. A value of $\\tau=1$ implies identical rankings, $\\tau=-1$ implies perfectly inverted rankings, and $\\tau=0$ suggests no correlation. The problem statement permits the use of a standard library implementation for this, such as `scipy.stats.kendalltau`. The formula is $\\tau = (N_c - N_d) / (N_c + N_d)$, where $N_c$ is the number of concordant pairs and $N_d$ is the number of discordant pairs between the two rankings.\n\n5.  **Format and Report**: Combine the results—$N_\\text{regions}$, the computed value of $\\tau$, the `top_changed` boolean, and the sorted list of unique breakpoints—into the specified output format. All floating-point numbers ($\\tau$ and the breakpoints) must be rounded to six decimal places.\n\nThis procedure provides a complete and rigorous analysis of the ranking sensitivity as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kendalltau\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (general crossing case)\n        (\n            [0.80, 0.60, 0.90, 0.50],\n            [0.30, 0.20, 0.60, 0.10]\n        ),\n        # Test Case 2 (boundary intersections and ties)\n        (\n            [1.00, 0.50, 0.50],\n            [0.40, 0.40, 0.10]\n        ),\n        # Test Case 3 (degenerate parallel lines)\n        (\n            [0.70, 0.50, 0.90],\n            [0.30, 0.50, 0.10]\n        ),\n        # Test Case 4 (higher-dimensional sensitivity)\n        (\n            [0.92, 0.81, 0.74, 0.66, 0.59, 0.52],\n            [0.12, 0.28, 0.21, 0.33, 0.27, 0.38]\n        )\n    ]\n\n    all_results_str = []\n    \n    for E_list, G_list in test_cases:\n        E = np.array(E_list, dtype=float)\n        G = np.array(G_list, dtype=float)\n        n = len(E)\n        epsilon = 1e-9\n\n        # --- Step 1: Identify Critical Points ---\n        critical_alphas = []\n        slopes = E + G\n        for i in range(n):\n            for j in range(i + 1, n):\n                denominator = slopes[i] - slopes[j]\n                \n                if abs(denominator) > epsilon:\n                    numerator = G[i] - G[j]\n                    alpha_ij = numerator / denominator\n                    \n                    if -epsilon <= alpha_ij <= 1 + epsilon:\n                        # Clamp to [0, 1] for floating point inaccuracies near boundaries\n                        alpha_ij = max(0.0, min(1.0, alpha_ij))\n                        critical_alphas.append(alpha_ij)\n\n        # --- Step 2: Partition the Interval ---\n        breakpoints = [0.0, 1.0] + critical_alphas\n        breakpoints.sort()\n        \n        unique_breakpoints = []\n        if breakpoints:\n            unique_breakpoints.append(breakpoints[0])\n            for i in range(1, len(breakpoints)):\n                if breakpoints[i] - unique_breakpoints[-1] > epsilon:\n                    unique_breakpoints.append(breakpoints[i])\n        \n        N_regions = len(unique_breakpoints) - 1\n        \n        # Helper for ranking\n        def get_ranking(alpha, E_vec, G_vec):\n            fitness = alpha * E_vec - (1 - alpha) * G_vec\n            # Sort by -fitness (desc), then by index (asc) for tie-breaking\n            indexed_fitness = sorted(enumerate(fitness), key=lambda x: (-x[1], x[0]))\n            return [index for index, fit in indexed_fitness]\n\n        # --- Step 3: Analyze Boundary Rankings ---\n        ranking_at_0 = get_ranking(0.0, E, G)\n        ranking_at_1 = get_ranking(1.0, E, G)\n        \n        # --- Step 4: Compute Sensitivity Metrics ---\n        # Top-ranked rule change\n        top_changed = ranking_at_0[0] != ranking_at_1[0]\n        \n        # Kendall Tau\n        # We need to compute ranks for kendalltau. The rank of item i is its position in the ranking list.\n        # rank_vec[i] = position_of_i\n        rank_vec_0 = np.zeros(n, dtype=int)\n        rank_vec_1 = np.zeros(n, dtype=int)\n        for rank, index in enumerate(ranking_at_0):\n            rank_vec_0[index] = rank\n        for rank, index in enumerate(ranking_at_1):\n            rank_vec_1[index] = rank\n            \n        tau, _ = kendalltau(rank_vec_0, rank_vec_1)\n\n        # --- Step 5: Format and Report ---\n        bp_list_str = f\"[{','.join([f'{bp:.6f}' for bp in unique_breakpoints])}]\"\n        result_str = (\n            f\"[{N_regions},{tau:.6f},{str(top_changed)},{bp_list_str}]\"\n        )\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After an evolutionary run produces a set of high-performing agent rules, the critical final step is to compare them rigorously. In complex adaptive systems, environmental stochasticity means an agent's performance is itself a random variable, making comparisons based on single simulation runs potentially misleading. This practice addresses how to compare agent rules in a statistically sound manner.\n\nThis exercise introduces Common Random Numbers (CRN), a powerful variance reduction technique that is standard practice in simulation science. By deriving the variance reduction achieved by CRN and applying the concept, you will master a crucial tool for obtaining more precise and reliable comparisons between evolved agents, which is a cornerstone of robust scientific findings in agent-based modeling .",
            "id": "4125258",
            "problem": "Consider two agent rules in a complex adaptive system. For each run, the environment generates exogenous stochastic inputs modeled as a random vector $\\boldsymbol{\\xi}$. Each agent rule transforms $\\boldsymbol{\\xi}$ into a scalar performance outcome: rule $A$ yields $Y_A(\\boldsymbol{\\xi})$ and rule $B$ yields $Y_B(\\boldsymbol{\\xi})$. Assume $Y_A(\\boldsymbol{\\xi})$ and $Y_B(\\boldsymbol{\\xi})$ are square-integrable random variables with finite variances. The objective is to estimate the mean performance difference $\\Delta = \\mathbb{E}[Y_A(\\boldsymbol{\\xi})] - \\mathbb{E}[Y_B(\\boldsymbol{\\xi})]$.\n\nYou will compare two experimental designs:\n- Independent runs: separate pseudo-random number streams are used for $A$ and $B$, producing independent samples of $Y_A$ and $Y_B$.\n- Common Random Numbers (CRN): shared pseudo-random number streams are used so that both $A$ and $B$ see the same $\\boldsymbol{\\xi}$ in each paired run.\n\nStarting only from fundamental definitions of expectation, variance, covariance, independence, and the Monte Carlo estimator of a mean, derive the variance of the difference-in-means estimator under the independent design and under the CRN design. Then, express the variance reduction of CRN relative to independent runs as a fraction in terms of the marginal variances and the correlation coefficient under CRN. Your derivation must start from the definitions of $\\mathbb{E}[\\cdot]$, $\\operatorname{Var}[\\cdot]$, and $\\operatorname{Cov}[\\cdot]$, and must assume $n$ independent and identically distributed paired runs under CRN (or $n$ independent runs per rule under the independent design).\n\nFor reproducibility and scientific realism, propose a CRN design for these agent rules that aligns the pseudo-random number generator calls so that for each paired run the micro-level random events in the environment are synchronized between $A$ and $B$. Your proposal must ensure that $Y_A(\\boldsymbol{\\xi})$ and $Y_B(\\boldsymbol{\\xi})$ achieve a well-defined nonnegative correlation when rules react similarly to $\\boldsymbol{\\xi}$.\n\nYour program must compute and report, for each test case, the theoretical variance reduction fraction of CRN relative to independent runs, defined as $R = 1 - \\frac{\\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{CRN}}]}{\\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{ind}}]}$, expressed as a decimal (no percent sign). Use the parameterization $(\\sigma_A^2, \\sigma_B^2, \\rho, n)$, where $\\sigma_A^2 = \\operatorname{Var}[Y_A(\\boldsymbol{\\xi})]$, $\\sigma_B^2 = \\operatorname{Var}[Y_B(\\boldsymbol{\\xi})]$, $\\rho$ is the correlation coefficient $\\rho = \\frac{\\operatorname{Cov}[Y_A(\\boldsymbol{\\xi}), Y_B(\\boldsymbol{\\xi})]}{\\sigma_A \\sigma_B}$ under the CRN design, and $n$ is the number of paired runs. Your program should compute $R$ for each case and output a single line containing a comma-separated list enclosed in square brackets. Round each result to $6$ decimal places.\n\nUse the following test suite of parameter values to cover typical and boundary behaviors:\n- Case $1$ (positive correlation, equal variances, typical): $(\\sigma_A^2 = 4.0, \\sigma_B^2 = 4.0, \\rho = 0.8, n = 100)$.\n- Case $2$ (perfect positive correlation, equal variances, boundary): $(\\sigma_A^2 = 9.0, \\sigma_B^2 = 9.0, \\rho = 1.0, n = 50)$.\n- Case $3$ (negative correlation, asymmetric variances, edge): $(\\sigma_A^2 = 1.0, \\sigma_B^2 = 4.0, \\rho = -0.5, n = 200)$.\n- Case $4$ (zero correlation, asymmetric variances, baseline): $(\\sigma_A^2 = 2.25, \\sigma_B^2 = 1.0, \\rho = 0.0, n = 1000)$.\n- Case $5$ (high positive correlation, highly asymmetric variances): $(\\sigma_A^2 = 16.0, \\sigma_B^2 = 1.0, \\rho = 0.9, n = 300)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the computed and rounded variance reduction fraction for case $i$.",
            "solution": "The problem requires a formal derivation of the variance of the difference-in-means estimator for two experimental designs—independent runs and Common Random Numbers (CRN)—followed by an expression for the variance reduction of CRN. It also requests a proposal for implementing CRN in an agent-based simulation context and a final computation of the variance reduction for several test cases.\n\nLet us begin by establishing the fundamental definitions and the estimator for the quantity of interest.\nThe performance outcomes for agent rules $A$ and $B$ are the random variables $Y_A(\\boldsymbol{\\xi})$ and $Y_B(\\boldsymbol{\\xi})$, respectively. For simplicity, we denote them as $Y_A$ and $Y_B$. The objective is to estimate the difference in their expected performance, $\\Delta = \\mathbb{E}[Y_A] - \\mathbb{E}[Y_B]$. Let $\\mu_A = \\mathbb{E}[Y_A]$ and $\\mu_B = \\mathbb{E}[Y_B]$. The variances are given as $\\operatorname{Var}[Y_A] = \\sigma_A^2$ and $\\operatorname{Var}[Y_B] = \\sigma_B^2$.\n\nThe Monte Carlo method is used for estimation. Based on $n$ simulation runs, we obtain samples for each rule. Let the outcomes for rule $A$ be $\\{Y_{A,1}, Y_{A,2}, \\dots, Y_{A,n}\\}$ and for rule $B$ be $\\{Y_{B,1}, Y_{B,2}, \\dots, Y_{B,n}\\}$. The estimators for the individual means are:\n$$ \\hat{\\mu}_A = \\frac{1}{n} \\sum_{i=1}^{n} Y_{A,i} \\quad \\text{and} \\quad \\hat{\\mu}_B = \\frac{1}{n} \\sum_{i=1}^{n} Y_{B,i} $$\nThe estimator for the difference $\\Delta$ is naturally $\\hat{\\Delta} = \\hat{\\mu}_A - \\hat{\\mu}_B$. We can write this as:\n$$ \\hat{\\Delta} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_{A,i} - Y_{B,i}) $$\nOur goal is to find the variance of this estimator, $\\operatorname{Var}[\\hat{\\Delta}]$, under the two different experimental designs.\n\n**Variance of the Estimator under Independent Runs**\n\nIn the independent runs design, the set of random numbers used to generate $\\{Y_{A,i}\\}$ is statistically independent of the set used to generate $\\{Y_{B,i}\\}$. This implies that $Y_{A,i}$ is independent of $Y_{B,j}$ for all $i,j \\in \\{1, \\dots, n\\}$. Consequently, the estimators $\\hat{\\mu}_A$ and $\\hat{\\mu}_B$ are independent random variables.\n\nThe variance of a sum of two independent random variables is the sum of their variances. Therefore, the variance of the estimator $\\hat{\\Delta}_{\\mathrm{ind}}$ is:\n$$ \\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{ind}}] = \\operatorname{Var}[\\hat{\\mu}_A - \\hat{\\mu}_B] = \\operatorname{Var}[\\hat{\\mu}_A] + \\operatorname{Var}[-\\hat{\\mu}_B] = \\operatorname{Var}[\\hat{\\mu}_A] + (-1)^2 \\operatorname{Var}[\\hat{\\mu}_B] = \\operatorname{Var}[\\hat{\\mu}_A] + \\operatorname{Var}[\\hat{\\mu}_B] $$\nNow, we derive the variance of $\\hat{\\mu}_A$. The samples $Y_{A,1}, \\dots, Y_{A,n}$ are independent and identically distributed (i.i.d.) with variance $\\sigma_A^2$.\n$$ \\operatorname{Var}[\\hat{\\mu}_A] = \\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{A,i}\\right] $$\nUsing the property $\\operatorname{Var}[cX] = c^2\\operatorname{Var}[X]$:\n$$ \\operatorname{Var}[\\hat{\\mu}_A] = \\frac{1}{n^2} \\operatorname{Var}\\left[\\sum_{i=1}^{n} Y_{A,i}\\right] $$\nSince the $Y_{A,i}$ are independent, the variance of their sum is the sum of their variances:\n$$ \\operatorname{Var}[\\hat{\\mu}_A] = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}[Y_{A,i}] = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma_A^2 = \\frac{1}{n^2} (n \\sigma_A^2) = \\frac{\\sigma_A^2}{n} $$\nBy identical reasoning, $\\operatorname{Var}[\\hat{\\mu}_B] = \\frac{\\sigma_B^2}{n}$.\nSubstituting these into the expression for $\\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{ind}}]$ yields:\n$$ \\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{ind}}] = \\frac{\\sigma_A^2}{n} + \\frac{\\sigma_B^2}{n} = \\frac{\\sigma_A^2 + \\sigma_B^2}{n} $$\n\n**Variance of the Estimator under Common Random Numbers (CRN)**\n\nIn the CRN design, for each paired run $i \\in \\{1, \\dots, n\\}$, both rules are simulated with the same sequence of random numbers, i.e., the same exogenous input $\\boldsymbol{\\xi}_i$. This induces a correlation between $Y_{A,i}$ and $Y_{B,i}$. The pairs $(Y_{A,i}, Y_{B,i})$ are i.i.d. across $i=1, \\dots, n$.\nLet $Z_i = Y_{A,i} - Y_{B,i}$. Then the estimator is $\\hat{\\Delta}_{\\mathrm{CRN}} = \\frac{1}{n} \\sum_{i=1}^{n} Z_i$. Since the pairs $(Y_{A,i}, Y_{B,i})$ are i.i.d., the differences $Z_i$ are also i.i.d.\nThe variance of the estimator is:\n$$ \\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{CRN}}] = \\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Z_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}[Z_i] = \\frac{n \\operatorname{Var}[Z_1]}{n^2} = \\frac{\\operatorname{Var}[Z_1]}{n} $$\nWe now need to find $\\operatorname{Var}[Z_1] = \\operatorname{Var}[Y_{A,1} - Y_{B,1}]$. The variance of a difference of two correlated random variables is given by the identity:\n$$ \\operatorname{Var}[X - Y] = \\operatorname{Var}[X] + \\operatorname{Var}[Y] - 2\\operatorname{Cov}[X, Y] $$\nApplying this identity, we get:\n$$ \\operatorname{Var}[Z_1] = \\operatorname{Var}[Y_{A,1}] + \\operatorname{Var}[Y_{B,1}] - 2\\operatorname{Cov}[Y_{A,1}, Y_{B,1}] $$\nThe marginal variances $\\operatorname{Var}[Y_{A,1}]$ and $\\operatorname{Var}[Y_{B,1}]$ are still $\\sigma_A^2$ and $\\sigma_B^2$. The covariance term, $\\operatorname{Cov}[Y_{A,1}, Y_{B,1}]$, which we denote $\\operatorname{Cov}[Y_A, Y_B]$, is generally non-zero under CRN.\nThus, the variance of the estimator under CRN is:\n$$ \\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{CRN}}] = \\frac{\\sigma_A^2 + \\sigma_B^2 - 2\\operatorname{Cov}[Y_A, Y_B]}{n} $$\n\n**Variance Reduction Fraction**\n\nThe variance reduction fraction $R$ is defined as $R = 1 - \\frac{\\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{CRN}}]}{\\operatorname{Var}[\\hat{\\Delta}_{\\mathrm{ind}}]}$. Substituting the derived expressions:\n$$ R = 1 - \\frac{(\\sigma_A^2 + \\sigma_B^2 - 2\\operatorname{Cov}[Y_A, Y_B])/n}{(\\sigma_A^2 + \\sigma_B^2)/n} $$\nThe sample size $n$ cancels out, indicating that the relative variance reduction is independent of the number of runs:\n$$ R = 1 - \\frac{\\sigma_A^2 + \\sigma_B^2 - 2\\operatorname{Cov}[Y_A, Y_B]}{\\sigma_A^2 + \\sigma_B^2} $$\n$$ R = 1 - \\left( \\frac{\\sigma_A^2 + \\sigma_B^2}{\\sigma_A^2 + \\sigma_B^2} - \\frac{2\\operatorname{Cov}[Y_A, Y_B]}{\\sigma_A^2 + \\sigma_B^2} \\right) = 1 - \\left( 1 - \\frac{2\\operatorname{Cov}[Y_A, Y_B]}{\\sigma_A^2 + \\sigma_B^2} \\right) $$\n$$ R = \\frac{2\\operatorname{Cov}[Y_A, Y_B]}{\\sigma_A^2 + \\sigma_B^2} $$\nFinally, we express this in terms of the correlation coefficient $\\rho = \\frac{\\operatorname{Cov}[Y_A, Y_B]}{\\sigma_A \\sigma_B}$, where $\\sigma_A = \\sqrt{\\sigma_A^2}$ and $\\sigma_B = \\sqrt{\\sigma_B^2}$ are the standard deviations. From this definition, $\\operatorname{Cov}[Y_A, Y_B] = \\rho \\sigma_A \\sigma_B$.\nSubstituting this into the expression for $R$:\n$$ R = \\frac{2 \\rho \\sigma_A \\sigma_B}{\\sigma_A^2 + \\sigma_B^2} $$\nThis is the final expression for the theoretical variance reduction fraction. A positive correlation ($\\rho > 0$) leads to a positive variance reduction ($R > 0$), while a negative correlation ($\\rho < 0$) leads to a variance increase ($R < 0$). If $\\rho=0$, there is no change in variance compared to the independent design.\n\n**Proposal for a CRN Design**\n\nTo implement CRN effectively in an agent-based model, the goal is to synchronize the stochastic events experienced by agents under rule $A$ and rule $B$ for each paired run. This ensures that any observed performance difference is more likely due to the rules themselves rather than differences in random luck.\n\nA scientifically robust CRN design can be structured as follows:\n1.  **Replication Seeding**: The simulation is conducted for $n$ paired replications. A master sequence of $n$ distinct seeds $\\{S_1, S_2, \\dots, S_n\\}$ should be generated. For each replication $i$, the pseudo-random number generator (PRNG) for the rule $A$ simulation and the PRNG for the rule $B$ simulation are both initialized with the same seed $S_i$.\n2.  **Synchronization of Random Number Streams**: The core of CRN lies in ensuring that the $k$-th random number requested in the simulation for rule $A$ is used for the same logical purpose as the $k$-th random number in the simulation for rule $B$.\n    - For example, if the first random event is determining the initial location of a resource, both simulations should use the first random number, $U_1$, from their synchronized streams for this purpose. If the second event is to determine if an agent's communication attempt is successful, both should use $U_2$.\n    - A critical challenge arises when the logic of rule $A$ and rule $B$ dictates a different number of random draws. For instance, if rule $A$ leads to an action requiring $3$ random numbers, while rule $B$ leads to an action requiring only $1$, the streams will become desynchronized for all subsequent events.\n3.  **Robust Synchronization via Multiple Streams**: A superior approach to mitigate desynchronization is to use multiple, independent PRNG streams, each dedicated to a specific source of randomness in the model. For instance:\n    - Stream $1$: For initial agent placements.\n    - Stream $2$: For stochastic environmental changes (e.g., resource replenishment).\n    - Stream $3$: For agent action success probabilities (e.g., foraging success).\n    Each stream is part of the common random numbers package; for replication $i$, stream $j$ in the rule $A$ simulation is synchronized with stream $j$ in the rule $B$ simulation. This way, if rule $A$ requires an extra draw for a purpose unique to its logic, it can draw from a dedicated stream not used by rule $B$, leaving the common streams synchronized.\n4.  **Achieving Non-negative Correlation**: By synchronizing the underlying random events, we ensure that both rules face the same \"favorable\" or \"unfavorable\" conditions in a given run. If the agent rules $A$ and $B$ are structurally similar (e.g., variants of a base heuristic), they are likely to respond to these conditions in a similar manner. A favorable set of random events (e.g., abundant resources, low risk) will likely lead to high performance for both ($Y_A$ and $Y_B$ are high), while unfavorable events will cause both to perform poorly ($Y_A$ and $Y_B$ are low). This tendency for $Y_A$ and $Y_B$ to move in the same direction results in a positive covariance, $\\operatorname{Cov}[Y_A, Y_B] > 0$, and thus a positive correlation $\\rho > 0$. This positive correlation is the source of the variance reduction.\n\nThis structured design ensures that the CRN technique is correctly applied, leading to a statistically efficient comparison between agent rules.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the theoretical variance reduction fraction of Common Random Numbers (CRN)\n    relative to independent runs for a set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (sigma_A^2, sigma_B^2, rho, n)\n    # sigma_A^2: Variance of outcome for rule A\n    # sigma_B^2: Variance of outcome for rule B\n    # rho: Correlation coefficient between Y_A and Y_B under CRN\n    # n: Number of paired runs (note: n is not used in the final formula for R)\n    test_cases = [\n        (4.0, 4.0, 0.8, 100),\n        (9.0, 9.0, 1.0, 50),\n        (1.0, 4.0, -0.5, 200),\n        (2.25, 1.0, 0.0, 1000),\n        (16.0, 1.0, 0.9, 300)\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma_A_sq, sigma_B_sq, rho, _ = case\n        \n        # The theoretical variance reduction fraction is given by the formula:\n        # R = (2 * rho * sigma_A * sigma_B) / (sigma_A^2 + sigma_B^2)\n        \n        # Calculate the standard deviations\n        sigma_A = np.sqrt(sigma_A_sq)\n        sigma_B = np.sqrt(sigma_B_sq)\n        \n        # Numerator of the fraction R\n        numerator = 2 * rho * sigma_A * sigma_B\n        \n        # Denominator of the fraction R\n        denominator = sigma_A_sq + sigma_B_sq\n        \n        # The problem statement gives positive variances, so no division by zero is expected.\n        result = numerator / denominator\n        \n        # Round the result to 6 decimal places as required.\n        results.append(round(result, 6))\n\n    # Format the final output as a comma-separated list in square brackets.\n    # The map(str, ...) is necessary to handle potential negative signs and floating point representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver function.\nsolve()\n```"
        }
    ]
}