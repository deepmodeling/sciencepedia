{
    "hands_on_practices": [
        {
            "introduction": "强化学习的核心是策略优化。这个练习将我们带回到一个最简单的单状态马尔可夫决策过程，来探索一个高级概念：熵正则化。通过推导一个带有熵正则化目标函数的最优策略，你将清晰地理解智能体如何在最大化奖励和保持策略随机性（鼓励探索）之间进行权衡，这是现代强化学习中的一个核心思想。",
            "id": "4113170",
            "problem": "考虑一个单状态马尔可夫决策过程 (MDP)，其状态为 $s$，动作集为 $\\mathcal{A} = \\{0,1\\}$，确定性奖励为 $r(s,1) = r_{1}$ 和 $r(s,0) = r_{0}$，其中 $r_{1}$ 和 $r_{0}$ 是实数。无论选择何种动作，环境都确定性地转换回状态 $s$。智能体采用一个平稳随机策略 $\\pi_{\\theta}(a \\mid s)$，该策略由一个标量参数 $\\theta \\in \\mathbb{R}$ 通过对动作 logit 的玻尔兹曼 (softmax) 参数化来定义，其中 logit 定义为 $\\ell(1) = \\theta$ 和 $\\ell(0) = 0$。温度被隐式地包含在参数化中。因此，该策略满足\n$$\n\\pi_{\\theta}(1 \\mid s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + \\exp(0)}, \\quad \\pi_{\\theta}(0 \\mid s) = \\frac{\\exp(0)}{\\exp(\\theta) + \\exp(0)}.\n$$\n智能体的目标是最大化一个折扣熵正则化目标函数，折扣因子为 $\\gamma \\in (0,1)$，熵系数为 $\\lambda > 0$。具体来说，智能体最大化的是每步即时奖励的期望折扣总和，外加一个由 $\\lambda$ 加权的香农熵奖励项，其中策略在状态 $s$ 的每步香农熵由下式给出\n$$\nH(\\pi_{\\theta}(\\cdot \\mid s)) = - \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a \\mid s) \\ln \\big(\\pi_{\\theta}(a \\mid s)\\big).\n$$\n严格从期望折扣回报、平稳策略和香农熵的基本定义出发，推导出最大化该目标函数的唯一最优参数 $\\theta^{\\star}$ 的精确闭式表达式。请将您的最终答案表示为仅包含 $r_{1}$、$r_{0}$ 和 $\\lambda$ 的单个解析表达式。无需进行数值计算，也无需四舍五入。答案必须是单个闭式表达式。",
            "solution": "用户要求我解决一个强化学习问题。\n\n我的计划如下：\n1.  **验证问题：** 我需要检查问题是否定义明确、科学上合理且完整。\n    *   **提取已知条件：** 我将列出所有提供的定义和参数。\n        *   MDP：单状态 $s$。\n        *   动作集：$\\mathcal{A} = \\{0, 1\\}$。\n        *   奖励：$r(s, 1) = r_1$, $r(s, 0) = r_0$。两者都是实数。\n        *   转移：确定性的，总是返回状态 $s$。所以对所有 $a \\in \\mathcal{A}$ 都有 $P(s' = s | s, a) = 1$。\n        *   策略：平稳随机策略 $\\pi_{\\theta}(a|s)$。\n        *   参数化：使用 logit $\\ell(1) = \\theta$ 和 $\\ell(0) = 0$ 的玻尔兹曼 (softmax) 参数化。\n        *   策略概率：\n            *   $\\pi_{\\theta}(1|s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + \\exp(0)} = \\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}$。\n            *   $\\pi_{\\theta}(0|s) = \\frac{\\exp(0)}{\\exp(\\theta) + \\exp(0)} = \\frac{1}{\\exp(\\theta) + 1}$。\n        *   目标：最大化折扣熵正则化回报。\n        *   折扣因子：$\\gamma \\in (0, 1)$。\n        *   熵系数：$\\lambda > 0$。\n        *   每步香农熵：$H(\\pi_{\\theta}(\\cdot|s)) = - \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) \\ln(\\pi_{\\theta}(a|s))$。\n        *   任务：找到最大化目标函数的最优参数 $\\theta^{\\star}$。答案应为包含 $r_1$、$r_0$ 和 $\\lambda$ 的闭式表达式。\n\n    *   **验证：**\n        *   **科学依据：** 是的，这是熵正则化强化学习中的一个标准问题设置。MDP、softmax 策略和香农熵的概念都已广为确立。\n        *   **适定性：** 问题要求对单个参数 $\\theta$ 最大化一个函数。目标函数看起来定义良好。$\\theta$ 的定义域是 $\\mathbb{R}$。我需要检查是否存在唯一的最大值。目标函数涉及对数和指数，它们都是平滑函数。由于策略概率总是正的，熵项中对数的定义域得到了处理。问题看起来是适定的。\n        *   **客观性：** 语言精确且数学化。没有主观性陈述。\n        *   **不完整/矛盾：** 问题设置是自洽的。所有必要组件（状态、动作、奖励、转移、策略、目标）都已定义。\n        *   **不切实际/不可行：** 这是一个理论问题，所以物理现实性不是主要关注点。其数学设置是合理的。\n        *   **病态/结构不良：** 结构清晰。目标是最大化一个目标函数。我需要建立这个函数，然后用微积分找到最大值。\n        *   **伪深刻/琐碎：** 虽然单状态 MDP 是一个简化，但加入熵正则化和特定的策略参数化使其成为一个非凡的微积分问题，用以测试对核心 RL 概念的理解。它不是一个同义反复的平凡问题。\n        *   **超出科学可验证性：** 解可以通过标准的数学推导（微积分）来验证。\n\n    *   **结论：** 问题是**有效的**。\n\n2.  **解决问题：**\n    *   **定义目标函数：** 目标是奖励和熵奖励的期望折扣总和。由于状态始终是 $s$，策略 $\\pi_{\\theta}$ 在状态 $s$ 的价值函数就是我们想要最大化的目标。我们称之为 $J(\\theta)$。\n        *   智能体始终处于状态 $s$。在每个时间步 $t=0, 1, 2, ...$，智能体根据 $\\pi_{\\theta}(\\cdot|s)$ 采取一个动作 $a_t$。\n        *   在时间步 $t$ 的即时奖励是一个随机变量 $R_{t+1}$，它取决于动作 $a_t$。即时奖励的期望是：\n            $E_{\\pi_{\\theta}}[R_{t+1}] = \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) r(s, a) = \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0$。\n        *   由于策略是平稳的并且只有一个状态，每一步的熵奖励也是恒定的：$\\lambda H(\\pi_{\\theta}(\\cdot|s))$。\n        *   每步的总增广奖励是 $E[R_{t+1}] + \\lambda H(\\pi_{\\theta}(\\cdot|s))$。\n        *   目标函数是这些增广奖励的期望折扣总和：\n            $J(\\theta) = E_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t (R_{t+1} + \\lambda H(\\pi_{\\theta}(\\cdot|s))) \\right]$。\n        *   由于每步的期望增广奖励是恒定的，我们可以将其从求和中提出来：\n            $J(\\theta) = \\left( E[R_{t+1}] + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right) \\sum_{t=0}^{\\infty} \\gamma^t$。\n        *   该几何级数求和为 $\\sum_{t=0}^{\\infty} \\gamma^t = \\frac{1}{1-\\gamma}$。\n        *   所以，$J(\\theta) = \\frac{1}{1-\\gamma} \\left( \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0 + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right)$。\n        *   为了最大化 $J(\\theta)$，我们可以忽略常数正因子 $\\frac{1}{1-\\gamma}$，只需最大化括号内的项。我们定义简化的目标函数 $L(\\theta)$：\n            $L(\\theta) = \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0 + \\lambda H(\\pi_{\\theta}(\\cdot|s))$。\n\n    *   **代入策略和熵的表达式：**\n        *   设 $p_1 = \\pi_{\\theta}(1|s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}$ 和 $p_0 = \\pi_{\\theta}(0|s) = \\frac{1}{\\exp(\\theta) + 1}$。\n        *   还要注意 $p_1$ 是 $\\theta$ 的 sigmoid 函数，通常写作 $\\sigma(\\theta)$。所以 $p_0 = 1 - p_1 = 1 - \\sigma(\\theta) = \\sigma(-\\theta)$。\n        *   熵项是：\n            $H(\\pi_{\\theta}(\\cdot|s)) = -(p_1 \\ln(p_1) + p_0 \\ln(p_0))$。\n        *   将这些代入 $L(\\theta)$：\n            $L(\\theta) = p_1 r_1 + p_0 r_0 - \\lambda (p_1 \\ln(p_1) + p_0 \\ln(p_0))$。\n        *   现在用 $\\theta$ 表示 $p_1$ 和 $p_0$ 的表达式并代入：\n            $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$\n            $p_0 = \\frac{1}{\\exp(\\theta)+1}$\n            $\\ln(p_1) = \\ln\\left(\\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}\\right) = \\theta - \\ln(\\exp(\\theta) + 1)$\n            $\\ln(p_0) = \\ln\\left(\\frac{1}{\\exp(\\theta) + 1}\\right) = - \\ln(\\exp(\\theta) + 1)$\n        *   让我们把这些代入熵项：\n            $H = -[ p_1(\\theta - \\ln(\\exp(\\theta)+1)) + p_0(-\\ln(\\exp(\\theta)+1)) ]$\n            $H = -[ p_1 \\theta - (p_1+p_0)\\ln(\\exp(\\theta)+1) ]$\n            由于 $p_1+p_0 = 1$：\n            $H = -[ p_1 \\theta - \\ln(\\exp(\\theta)+1) ]$\n            $H = \\ln(\\exp(\\theta)+1) - p_1 \\theta$\n        *   现在将这个熵表达式代回 $L(\\theta)$：\n            $L(\\theta) = p_1 r_1 + p_0 r_0 + \\lambda (\\ln(\\exp(\\theta)+1) - p_1 \\theta)$\n        *   使用 $p_0 = 1-p_1$：\n            $L(\\theta) = p_1 r_1 + (1-p_1)r_0 + \\lambda (\\ln(\\exp(\\theta)+1) - p_1 \\theta)$\n            $L(\\theta) = p_1(r_1 - r_0) + r_0 + \\lambda \\ln(\\exp(\\theta)+1) - \\lambda p_1 \\theta$\n            $L(\\theta) = p_1 (r_1 - r_0 - \\lambda \\theta) + r_0 + \\lambda \\ln(\\exp(\\theta)+1)$\n        *   代入 $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$：\n            $L(\\theta) = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} (r_1 - r_0 - \\lambda \\theta) + r_0 + \\lambda \\ln(\\exp(\\theta)+1)$\n            这种形式求导有点复杂。让我们试试另一种方法。\n\n    *   **L(theta)的另一种表达式：**\n        *   $L(\\theta) = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} r_1 + \\frac{1}{\\exp(\\theta)+1} r_0 - \\lambda \\left[ \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} \\ln\\left(\\frac{\\exp(\\theta)}{\\exp(\\theta)+1}\\right) + \\frac{1}{\\exp(\\theta)+1} \\ln\\left(\\frac{1}{\\exp(\\theta)+1}\\right) \\right]$\n        *   $L(\\theta) = \\frac{1}{\\exp(\\theta)+1} \\left( r_1\\exp(\\theta) + r_0 \\right) - \\frac{\\lambda}{\\exp(\\theta)+1} \\left[ \\exp(\\theta)(\\theta - \\ln(\\exp(\\theta)+1)) + 1(-\\ln(\\exp(\\theta)+1)) \\right]$\n        *   $L(\\theta) = \\frac{1}{\\exp(\\theta)+1} \\left( r_1\\exp(\\theta) + r_0 \\right) - \\frac{\\lambda}{\\exp(\\theta)+1} \\left[ \\theta\\exp(\\theta) - (\\exp(\\theta)+1)\\ln(\\exp(\\theta)+1) \\right]$\n        *   $L(\\theta) = \\frac{r_1\\exp(\\theta) + r_0 - \\lambda\\theta\\exp(\\theta)}{\\exp(\\theta)+1} + \\lambda\\ln(\\exp(\\theta)+1)$\n        这看起来是正确的。\n\n    *   **求最大值：** 为了找到最优的 $\\theta^{\\star}$，我们需要计算 $L(\\theta)$ 关于 $\\theta$ 的导数，并令其为零：$\\frac{dL}{d\\theta} = 0$。\n        *   我们逐项对 $L(\\theta)$ 求导。\n        *   $L(\\theta) = p_1 r_1 + p_0 r_0 - \\lambda (p_1 \\ln(p_1) + p_0 \\ln(p_0))$。\n        *   我们需要 $p_1$ 和 $p_0$ 关于 $\\theta$ 的导数。\n        *   $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$。\n            $\\frac{dp_1}{d\\theta} = \\frac{\\exp(\\theta)(\\exp(\\theta)+1) - \\exp(\\theta)\\exp(\\theta)}{(\\exp(\\theta)+1)^2} = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2} = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} \\frac{1}{\\exp(\\theta)+1} = p_1 p_0$。\n        *   $p_0 = 1 - p_1$。\n            $\\frac{dp_0}{d\\theta} = - \\frac{dp_1}{d\\theta} = -p_1 p_0$。\n        *   现在对 $L(\\theta)$ 求导：\n            $\\frac{dL}{d\\theta} = r_1 \\frac{dp_1}{d\\theta} + r_0 \\frac{dp_0}{d\\theta} - \\lambda \\frac{d}{d\\theta} (p_1 \\ln(p_1) + p_0 \\ln(p_0))$\n        *   首先关注熵的导数部分：\n            $\\frac{d}{d\\theta}(p_1 \\ln p_1) = \\frac{dp_1}{d\\theta} \\ln p_1 + p_1 \\frac{1}{p_1} \\frac{dp_1}{d\\theta} = \\frac{dp_1}{d\\theta} (\\ln p_1 + 1)$。\n            $\\frac{d}{d\\theta}(p_0 \\ln p_0) = \\frac{dp_0}{d\\theta} \\ln p_0 + p_0 \\frac{1}{p_0} \\frac{dp_0}{d\\theta} = \\frac{dp_0}{d\\theta} (\\ln p_0 + 1)$。\n        *   所以，$\\frac{d}{d\\theta} (p_1 \\ln p_1 + p_0 \\ln p_0) = \\frac{dp_1}{d\\theta} (\\ln p_1 + 1) + \\frac{dp_0}{d\\theta} (\\ln p_0 + 1)$。\n        *   代入 $\\frac{dp_1}{d\\theta} = p_1 p_0$ 和 $\\frac{dp_0}{d\\theta} = -p_1 p_0$：\n            $= p_1 p_0 (\\ln p_1 + 1) - p_1 p_0 (\\ln p_0 + 1)$\n            $= p_1 p_0 (\\ln p_1 + 1 - \\ln p_0 - 1)$\n            $= p_1 p_0 (\\ln p_1 - \\ln p_0) = p_1 p_0 \\ln(p_1/p_0)$。\n        *   现在，我们来计算 $\\ln(p_1/p_0)$：\n            $p_1/p_0 = \\frac{\\exp(\\theta)/(\\exp(\\theta)+1)}{1/(\\exp(\\theta)+1)} = \\exp(\\theta)$。\n            所以, $\\ln(p_1/p_0) = \\ln(\\exp(\\theta)) = \\theta$。\n        *   熵部分的导数是 $p_1 p_0 \\theta$。\n        *   回到 $L(\\theta)$ 的完整导数：\n            $\\frac{dL}{d\\theta} = r_1 (p_1 p_0) + r_0 (-p_1 p_0) - \\lambda (p_1 p_0 \\theta)$\n            $\\frac{dL}{d\\theta} = p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$。\n        *   令导数为零以找到临界点：\n            $p_1 p_0 (r_1 - r_0 - \\lambda \\theta) = 0$。\n        *   $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$ 永不为零。\n        *   $p_0 = \\frac{1}{\\exp(\\theta)+1}$ 永不为零。\n        *   因此，要使乘积为零，第三项必须为零：\n            $r_1 - r_0 - \\lambda \\theta = 0$。\n        *   解出 $\\theta$：\n            $\\lambda \\theta = r_1 - r_0$\n            $\\theta = \\frac{r_1 - r_0}{\\lambda}$。\n\n    *   **检查是否为最大值：**\n        *   我们需要检查二阶导数 $\\frac{d^2L}{d\\theta^2}$ 的符号。\n        *   $\\frac{dL}{d\\theta} = p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$。\n        *   设 $f(\\theta) = p_1 p_0 = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2}$ 且 $g(\\theta) = r_1 - r_0 - \\lambda \\theta$。\n        *   $\\frac{d^2L}{d\\theta^2} = \\frac{df}{d\\theta} g(\\theta) + f(\\theta) \\frac{dg}{d\\theta}$。\n        *   在临界点 $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$，我们有 $g(\\theta^{\\star}) = 0$。\n        *   所以，$\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} = f(\\theta^{\\star}) \\frac{dg}{d\\theta}\\bigg|_{\\theta=\\theta^{\\star}}$。\n        *   $f(\\theta) = p_1 p_0 = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2}$。因为 $\\exp(\\theta) > 0$，所以对于所有 $\\theta \\in \\mathbb{R}$，都有 $f(\\theta) > 0$。\n        *   $\\frac{dg}{d\\theta} = \\frac{d}{d\\theta} (r_1 - r_0 - \\lambda \\theta) = -\\lambda$。\n        *   所以，$\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} = f(\\theta^{\\star}) (-\\lambda) = - \\lambda \\frac{\\exp(\\theta^{\\star})}{(\\exp(\\theta^{\\star})+1)^2}$。\n        *   我们已知 $\\lambda > 0$。并且如前所述，分数项总是正的。\n        *   因此，$\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} < 0$。\n        *   这证实了临界点 $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$ 是一个局部最大值。由于这是 $\\mathbb{R}$ 上唯一的临界点，并且函数是平滑的，因此它必定是唯一的全局最大值。\n\n    *   **最终答案的构建：**\n        *   问题要求唯一最优参数 $\\theta^{\\star}$ 的精确闭式表达式。\n        *   我的推导得出 $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$。\n        *   这是一个用 $r_1$、$r_0$ 和 $\\lambda$ 表示的解析表达式。\n        *   它满足所有要求。\n\n    *   **仔细检查推导和解释。**\n        *   目标是价值函数 $V^{\\pi_\\theta}(s)$。\n        *   策略 $\\pi$ 的熵正则化贝尔曼方程是：\n            $Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{\\pi}(s')$\n            $V^{\\pi}(s) = \\sum_a \\pi(a|s) Q^{\\pi}(s,a) + \\lambda H(\\pi(\\cdot|s))$\n        *   在我们的例子中，$P(s'|s,a) = \\delta_{s,s'}$。所以 $\\sum_{s'} P(s'|s,a) V^{\\pi}(s') = V^{\\pi}(s)$。\n        *   $Q^{\\pi_\\theta}(s,a) = r(s,a) + \\gamma V^{\\pi_\\theta}(s)$\n        *   $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) (r(s,a) + \\gamma V^{\\pi_\\theta}(s)) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\gamma V^{\\pi_\\theta}(s) \\sum_a \\pi_{\\theta}(a|s) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   由于 $\\sum_a \\pi_{\\theta}(a|s) = 1$：\n            $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\gamma V^{\\pi_\\theta}(s) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $(1-\\gamma) V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $V^{\\pi_\\theta}(s) = \\frac{1}{1-\\gamma} \\left( \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right)$。\n        *   这与我最初对目标函数 $J(\\theta)$ 的构建相符。最大化 $V^{\\pi_\\theta}(s)$ 等价于最大化括号内的项，我称之为 $L(\\theta)$。\n        *   $L(\\theta) = \\pi_{\\theta}(1|s)r_1 + \\pi_{\\theta}(0|s)r_0 + \\lambda H(\\pi_{\\theta}(\\cdot | s))$。\n        *   再检查一下熵的计算。\n            $H = - \\sum_{a} \\pi(a|s) \\ln \\pi(a|s)$。\n            对于 $\\pi(1|s) = p_1$ 和 $\\pi(0|s) = p_0$：\n            $H = - p_1 \\ln p_1 - p_0 \\ln p_0$。\n            $\\frac{dH}{d\\theta} = -\\frac{d}{d\\theta}(p_1 \\ln p_1) - \\frac{d}{d\\theta}(p_0 \\ln p_0)$\n            $= - [ \\frac{dp_1}{d\\theta}(\\ln p_1 + 1) + \\frac{dp_0}{d\\theta}(\\ln p_0 + 1) ]$\n            $= - [ (p_1 p_0)(\\ln p_1 + 1) + (-p_1 p_0)(\\ln p_0 + 1) ]$\n            $= - p_1 p_0 (\\ln p_1 - \\ln p_0) = - p_1 p_0 \\ln(p_1/p_0)$。\n            由于 $p_1/p_0 = \\exp(\\theta)$，我们有 $\\ln(p_1/p_0) = \\theta$。\n            所以 $\\frac{dH}{d\\theta} = - p_1 p_0 \\theta$。\n        *   现在 $L(\\theta)$ 的导数：\n            $\\frac{dL}{d\\theta} = \\frac{d}{d\\theta} (p_1 r_1 + p_0 r_0) + \\lambda \\frac{dH}{d\\theta}$\n            $= ( \\frac{dp_1}{d\\theta} r_1 + \\frac{dp_0}{d\\theta} r_0 ) + \\lambda (-p_1 p_0 \\theta)$\n            $= (p_1 p_0 r_1 - p_1 p_0 r_0) - \\lambda p_1 p_0 \\theta$\n            $= p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$。\n        *   这证实了我之前的导数计算。其余的论证也成立。\n        *   结果 $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$ 是熵正则化下双动作情况的一个著名结果。其逻辑是最优 logit 差应与奖励差成正比，并由熵系数（其作用类似于温度）进行缩放。如果 $\\lambda$ 很大（高温），智能体变得更随机，logit 被推向零，因此 $\\theta^{\\star}$ 很小。如果 $\\lambda$ 很小（低温），智能体更不具探索性，最优 logit $\\theta^{\\star}$ 很大，更强烈地反映了奖励差异。如果 $r_1 > r_0$，则 $\\theta^{\\star} > 0$，偏好动作 1。如果 $r_1 < r_0$，则 $\\theta^{\\star} < 0$，偏好动作 0。如果 $r_1 = r_0$，则 $\\theta^{\\star} = 0$，导致均匀策略 $\\pi(1|s) = \\pi(0|s) = 0.5$，这是合理的，因为没有奖励激励偏好任一动作，因此智能体最大化熵。该结果在物理上和直觉上都与熵正则化强化学习的原理一致。\n\n    *   步骤如下：\n        1.  构建目标函数 $J(\\theta)$，即价值函数 $V^{\\pi_\\theta}(s)$。\n        2.  证明最大化 $J(\\theta)$ 等价于最大化一个更简单的项 $L(\\theta)$。\n        3.  用参数 $\\theta$ 表示 $L(\\theta)$。\n        4.  对 $L(\\theta)$ 关于 $\\theta$ 求导。\n        5.  令导数为零，解出 $\\theta$。\n        6.  使用二阶导数检验验证该解对应一个最大值。\n        7.  写下最终表达式。\n\n解题的结构应清晰并遵循这些步骤。所有数学实体都将使用 LaTeX。\n最终答案是单个表达式 $\\frac{r_1 - r_0}{\\lambda}$。我将把它放在 `$$\\boxed{...}$$` 中。\n一切看起来都正确。我准备好编写解题过程了。",
            "answer": "$$\n\\boxed{\\frac{r_{1} - r_{0}}{\\lambda}}\n$$"
        },
        {
            "introduction": "学会了如何优化策略后，一个自然的问题是如何让学习过程变得更快。本练习探讨了基于势函数的奖励塑造（potential-based reward shaping），这是一种为环境奖励增加额外密集学习信号的原则性方法。通过推导奖励塑造对价值函数的影响，你将深刻理解为何该技术在可能加速收敛的同时，能够保持最优策略不变。",
            "id": "4113139",
            "problem": "考虑一个嵌入在复杂自适应系统中的单个学习智能体，该系统被建模为马尔可夫决策过程 (MDP)。一个马尔可夫决策过程 (MDP) 由一个状态空间 $\\mathcal{S}$、一个动作空间 $\\mathcal{A}$、一个转移核 $P(s' \\mid s, a)$ 以及一个有界的单步奖励函数 $r(s,a,s')$ 定义，其中 $|r(s,a,s')| \\leq R_{\\max} < \\infty$。该智能体遵循一个平稳策略 $\\pi(a \\mid s)$，并在一个无限时域内以折扣因子 $\\gamma \\in (0,1)$ 进行交互。在时间 $t$ 实现的单步奖励表示为 $R_t$，从时间 $t$ 开始的折扣回报定义为 $G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k}$。在策略 $\\pi$ 下对应的价值函数为 $V^{\\pi}(s) = \\mathbb{E}[G_t \\mid s_t = s]$，其中期望是基于由 $\\pi$ 和 $P$ 诱导的轨迹分布计算的。\n\n为了在保持决策结构的同时加速学习，智能体的环境奖励通过一个基于势的塑造项进行修改，该项使用一个有界势函数 $\\Phi : \\mathcal{S} \\to \\mathbb{R}$，且满足 $\\sup_{s \\in \\mathcal{S}} |\\Phi(s)| < \\infty$。在时间 $t$ 的塑造后奖励定义为 $R'_t = R_t + \\gamma \\, \\Phi(s_{t+1}) - \\Phi(s_t)$，从而产生一个塑造后的折扣回报 $G'_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k}$ 和一个塑造后的价值函数 $V'^{\\pi}(s) = \\mathbb{E}[G'_t \\mid s_t = s]$。\n\n从折扣回报和期望的基本定义出发，并且不使用任何快捷恒等式，推导在任何能诱导良定义轨迹分布的平稳策略 $\\pi$ 下，对于任意初始状态 $s_0 \\in \\mathcal{S}$，差值 $V'^{\\pi}(s_0) - V^{\\pi}(s_0)$ 的精确闭式表达式。假设所有需要的期望都存在，并且上述有界性条件成立，从而保证无限级数收敛，并且极限与期望的交换可以由测度论概率中的标准定理来证明其合理性。\n\n将您的最终答案表示为仅包含 $\\Phi(s_0)$ 的单个解析表达式。不需要数值近似、四舍五入、单位或百分比格式。",
            "solution": "问题要求对于任意初始状态 $s_0$，推导塑造后的价值函数 $V'^{\\pi}(s_0)$ 与原始价值函数 $V^{\\pi}(s_0)$ 之差的精确闭式表达式。推导将从问题陈述中提供的基本定义开始。\n\n需要推导的量是 $V'^{\\pi}(s_0) - V^{\\pi}(s_0)$。\n\n根据定义，价值函数是期望的折扣回报：\n$$V'^{\\pi}(s_0) = \\mathbb{E}[G'_t \\mid s_t = s_0]$$\n$$V^{\\pi}(s_0) = \\mathbb{E}[G_t \\mid s_t = s_0]$$\n期望 $\\mathbb{E}[\\cdot \\mid s_t = s_0]$ 是对所有从状态 $s_0$ 开始的可能轨迹计算的，其轨迹分布由策略 $\\pi(a \\mid s)$ 和转移核 $P(s' \\mid s, a)$ 诱导。\n\n差值可以写为：\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t \\mid s_t = s_0] - \\mathbb{E}[G_t \\mid s_t = s_0]$$\n根据期望的线性性质，我们可以合并这些项：\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t - G_t \\mid s_t = s_0]$$\n\n接下来，我们代入折扣回报 $G'_t$ 和 $G_t$ 的定义：\n$$G'_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k}$$\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k}$$\n回报之间的差值为：\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k} - \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k} = \\sum_{k=0}^{\\infty} \\gamma^{k} (R'_{t+k} - R_{t+k})$$\n问题陈述中定义了塑造后的奖励 $R'_t$ 为 $R'_t = R_t + \\gamma \\, \\Phi(s_{t+1}) - \\Phi(s_t)$。将此应用于时间步 $t+k$ 的奖励：\n$$R'_{t+k} = R_{t+k} + \\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k})$$\n由此，奖励的差值为：\n$$R'_{t+k} - R_{t+k} = \\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k})$$\n将此表达式代回到 $G'_t - G_t$ 的求和中：\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} (\\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k}))$$\n我们可以将 $\\gamma^k$ 项分配到括号内：\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} (\\gamma^{k+1} \\Phi(s_{t+k+1}) - \\gamma^k \\Phi(s_{t+k}))$$\n这是一个伸缩级数。为了看清这一点，我们定义一个序列 $A_k = \\gamma^k \\Phi(s_{t+k})$。则求和变为：\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} (A_{k+1} - A_k)$$\n我们来分析截至任意整数 $N$ 的部分和：\n$$S_N = \\sum_{k=0}^{N} (A_{k+1} - A_k) = (A_1 - A_0) + (A_2 - A_1) + \\dots + (A_{N+1} - A_N)$$\n中间项相互抵消，剩下：\n$$S_N = A_{N+1} - A_0$$\n代回 $A_k$ 的定义：\n$$S_N = \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\gamma^0 \\Phi(s_{t+0}) = \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\Phi(s_t)$$\n为了求出无限级数的和，我们取 $N \\to \\infty$ 的极限：\n$$G'_t - G_t = \\lim_{N\\to\\infty} S_N = \\lim_{N\\to\\infty} \\left( \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\Phi(s_t) \\right)$$\n问题陈述中指出势函数是有界的，即 $\\sup_{s \\in \\mathcal{S}} |\\Phi(s)| < \\infty$。我们用 $\\Phi_{\\max}$ 表示这个上界。我们还有 $\\gamma \\in (0,1)$。\n项 $\\gamma^{N+1} \\Phi(s_{t+N+1})$ 的大小是有界的：\n$$|\\gamma^{N+1} \\Phi(s_{t+N+1})| \\leq \\gamma^{N+1} \\Phi_{\\max}$$\n因为 $\\lim_{N\\to\\infty} \\gamma^{N+1} = 0$，根据夹逼定理可得：\n$$\\lim_{N\\to\\infty} \\gamma^{N+1} \\Phi(s_{t+N+1}) = 0$$\n项 $-\\Phi(s_t)$ 与 $N$ 无关。因此，部分和的极限是：\n$$G'_t - G_t = 0 - \\Phi(s_t) = -\\Phi(s_t)$$\n现在我们可以回到价值函数差值的表达式：\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t - G_t \\mid s_t = s_0] = \\mathbb{E}[-\\Phi(s_t) \\mid s_t = s_0]$$\n对量 $-\\Phi(s_t)$ 的期望是在时间 $t$ 的状态为 $s_0$ 的事件下计算的。表达式 $-\\Phi(s_t)$ 是状态 $s_t$ 的一个确定性函数。在给定条件 $s_t = s_0$ 的情况下，该表达式的值固定为 $-\\Phi(s_0)$。因此，期望就是这个固定值：\n$$\\mathbb{E}[-\\Phi(s_t) \\mid s_t = s_0] = -\\Phi(s_0)$$\n这导出了最终结果：\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = -\\Phi(s_0)$$\n至此，按要求从第一性原理出发的推导完成。",
            "answer": "$$\\boxed{-\\Phi(s_0)}$$"
        },
        {
            "introduction": "在许多现实世界的应用中，我们无法自由地在线测试新策略，而必须使用从过去实验中收集的数据（离线数据）来评估它们。这个练习介绍了离线策略评估（off-policy evaluation），并要求你推导和应用一个强大的工具——双重鲁棒估计器。它巧妙地将重要性采样与预测模型相结合，以获得准确的策略价值估计，让你体验解决实际强化学习挑战的过程。",
            "id": "4113154",
            "problem": "考虑一个分幕式马尔可夫决策过程 (MDP)，其具有有限状态空间 $\\mathcal{S}=\\{A,B\\}$、有限动作空间 $\\mathcal{A}=\\{L,R\\}$，以及时域 $H=2$（时间索引 $t\\in\\{0,1\\}$）。初始状态确定为 $s_0=A$。奖励是有界的实数值。设折扣因子为 $\\gamma=\\frac{1}{2}$。一个包含 $N=3$ 个独立幕的数据集由行为策略 $b(a\\mid s)$ 生成，而我们的目标是评估一个不同的目标策略 $\\pi(a\\mid s)$，这种情况在强化学习中被称为离策略评估 (OPE)。记录的幕如下：\n- 幕 1： $(s_0=A,a_0=L,r_0=1,s_1=B,a_1=R,r_1=2)$。\n- 幕 2： $(s_0=A,a_0=R,r_0=0,s_1=A,a_1=L,r_1=3)$。\n- 幕 3： $(s_0=A,a_0=L,r_0=2,s_1=B,a_1=L,r_1=0)$。\n\n行为策略和目标策略如下：\n- 对于 $s=A$：$b(L\\mid A)=\\frac{1}{2}$，$b(R\\mid A)=\\frac{1}{2}$；$\\pi(L\\mid A)=\\frac{2}{3}$，$\\pi(R\\mid A)=\\frac{1}{3}$。\n- 对于 $s=B$：$b(L\\mid B)=\\frac{1}{3}$，$b(R\\mid B)=\\frac{2}{3}$；$\\pi(L\\mid B)=\\frac{1}{2}$，$\\pi(R\\mid B)=\\frac{1}{2}$。\n\n假设我们能访问一个已学习的模型，该模型在时间 $t=0,1$ 提供了目标策略下状态-动作价值的估计值 $\\hat{q}_t(s,a)$，并定义 $\\hat{v}_t(s)\\triangleq \\sum_{a\\in\\mathcal{A}}\\pi(a\\mid s)\\hat{q}_t(s,a)$（对于 $t=0,1$），其中终点约定为 $\\hat{v}_2(\\cdot)=0$。模型输出如下：\n- 在 $t=0$ 时：$\\hat{q}_0(A,L)=2$，$\\hat{q}_0(A,R)=1$。\n- 在 $t=1$ 时：$\\hat{q}_1(A,L)=2$，$\\hat{q}_1(A,R)=0$，$\\hat{q}_1(B,L)=1$，$\\hat{q}_1(B,R)=3$。\n\n从策略价值的基本定义 $V^{\\pi}(s_0)=\\mathbb{E}_{\\pi}\\!\\left[\\sum_{t=0}^{1}\\gamma^t r_t \\,\\middle|\\, s_0\\right]$ 以及关联 $\\pi$ 和 $b$ 下期望的似然比恒等式出发，从第一性原理推导出一个步进式估计器。该估计器使用基于模型的基线（由 $\\hat{q}_t$ 和 $\\hat{v}_t$ 构建）和重要性采样校正，以在模型正确时得到 $V^{\\pi}(A)$ 的无偏估计，否则得到一致估计。然后，通过对 $N=3$ 个幕的每条轨迹价值进行平均，在给定的数据集上评估此估计器。使用提供的 $\\gamma$、策略和模型输出。将最终答案表示为单个精确的实数。无需四舍五入。",
            "solution": "该问题是有效的。它在强化学习中离策略评估 (OPE) 的既定框架内提出了一个适定问题。所有必要组成部分——状态和动作空间、策略、折扣因子、数据集以及一个预测模型——都定义清晰且内部一致。\n\n目标是根据行为策略 $b$ 产生的数据，估计目标策略 $\\pi$ 的价值，记为 $V^{\\pi}(s_0)$。该问题要求推导并应用一个步进式估计器，该估计器结合了基于模型的估计与重要性采样校正。这是双重稳健 (Doubly Robust, DR) 估计器的一种形式，我们将从第一性原理出发，针对有限时域 $H$ 进行推导。\n\n目标策略在时间 $t$ 的价值是 $V_t^{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=t}^{H-1} \\gamma^{k-t} r_k \\mid s_t=s\\right]$。我们的目标是估计 $V_0^{\\pi}(s_0)$。我们给定一个模型，包含状态-动作价值估计 $\\hat{q}_t(s,a)$ 和状态价值估计 $\\hat{v}_t(s) = \\sum_{a} \\pi(a|s)\\hat{q}_t(s,a)$。按照惯例，对于所有 $s$，都有 $V_H^{\\pi}(s) = \\hat{v}_H(s) = 0$。\n\n我们为初始模型估计的误差 $V_0^{\\pi}(s_0) - \\hat{v}_0(s_0)$ 构建一个伸缩和：\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\sum_{a_0} \\pi(a_0|s_0)Q_0^{\\pi}(s_0,a_0) - \\sum_{a_0} \\pi(a_0|s_0)\\hat{q}_0(s_0,a_0) = \\mathbb{E}_{a_0 \\sim \\pi(\\cdot|s_0)}[Q_0^{\\pi}(s_0,a_0) - \\hat{q}_0(s_0,a_0)]$$\n使用重要性采样切换到行为策略 $b$ 下的期望：\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\mathbb{E}_{(s_0,a_0)\\sim b}\\left[\\frac{\\pi(a_0|s_0)}{b(a_0|s_0)}(Q_0^{\\pi}(s_0,a_0) - \\hat{q}_0(s_0,a_0))\\right]$$\n令 $\\rho_t = \\frac{\\pi(a_t|s_t)}{b(a_t|s_t)}$。代入贝尔曼方程 $Q_0^{\\pi}(s_0, a_0) = r_0 + \\gamma \\mathbb{E}[V_1^{\\pi}(s_1)] = r_0 + \\gamma V_1^{\\pi}(s_1)$ (因为在数据中 $r_0$ 和 $s_1$ 由 $s_0, a_0$ 决定)：\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\mathbb{E}_{b}[\\rho_0 (r_0 + \\gamma V_1^{\\pi}(s_1) - \\hat{q}_0(s_0,a_0))]$$\n我们将模型的单步贝尔曼残差定义为 $\\delta_t = r_t + \\gamma \\hat{v}_{t+1}(s_{t+1}) - \\hat{q}_t(s_t,a_t)$。那么我们可以重写该表达式：\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\mathbb{E}_{b}[\\rho_0 (r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) + \\gamma V_1^{\\pi}(s_1) - \\gamma \\hat{v}_1(s_1))] = \\mathbb{E}_{b}[\\rho_0 \\delta_0 + \\gamma \\rho_0 (V_1^{\\pi}(s_1) - \\hat{v}_1(s_1))]$$\n这个表达式是递归的。我们可以用同样的方式展开项 $(V_1^{\\pi}(s_1) - \\hat{v}_1(s_1))$：\n$$V_1^{\\pi}(s_1) - \\hat{v}_1(s_1) = \\mathbb{E}_{b}[\\rho_1 (r_1 + \\gamma V_2^{\\pi}(s_2) - \\hat{q}_1(s_1,a_1)) | s_1]$$\n因为 $H=2$，$t \\in \\{0, 1\\}$。因此 $V_2^{\\pi}(s_2) = 0$ 且 $\\hat{v}_2(s_2) = 0$。在 $t=1$ 时的残差是 $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1)$。\n$$V_1^{\\pi}(s_1) - \\hat{v}_1(s_1) = \\mathbb{E}_{b}[\\rho_1 \\delta_1 | s_1]$$\n将此代回 $V_0^{\\pi}(s_0)$ 的方程中，并使用全期望定律 ($\\mathbb{E}[\\mathbb{E}[X|Y]] = \\mathbb{E}[X]$)：\n$$V_0^{\\pi}(s_0) = \\hat{v}_0(s_0) + \\mathbb{E}_{b}[\\rho_0 \\delta_0] + \\mathbb{E}_{b}[\\gamma \\rho_0 \\rho_1 \\delta_1] = \\mathbb{E}_{b}[\\hat{v}_0(s_0) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1]$$\n因此，对于一条轨迹 $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1)$，其单轨迹估计器 $\\hat{V}(\\tau)$ 为：\n$$\\hat{V}(\\tau) = \\hat{v}_0(s_0) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1$$\n最终的估计值是 $N=3$ 条轨迹的样本均值：$\\hat{V}^{\\pi}(A) = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}(\\tau_i)$。\n\n现在我们使用 $\\gamma = \\frac{1}{2}$ 和 $s_0=A$ 计算所需的各个组成部分。\n重要性比率 $\\rho(s,a) = \\frac{\\pi(a|s)}{b(a|s)}$:\n- $\\rho(A,L) = \\frac{2/3}{1/2} = \\frac{4}{3}$\n- $\\rho(A,R) = \\frac{1/3}{1/2} = \\frac{2}{3}$\n- $\\rho(B,L) = \\frac{1/2}{1/3} = \\frac{3}{2}$\n- $\\rho(B,R) = \\frac{1/2}{2/3} = \\frac{3}{4}$\n\n基于模型的状态价值 $\\hat{v}_t(s)$:\n- $\\hat{v}_0(A) = \\pi(L|A)\\hat{q}_0(A,L) + \\pi(R|A)\\hat{q}_0(A,R) = (\\frac{2}{3})(2) + (\\frac{1}{3})(1) = \\frac{4}{3} + \\frac{1}{3} = \\frac{5}{3}$。\n- $\\hat{v}_1(A) = \\pi(L|A)\\hat{q}_1(A,L) + \\pi(R|A)\\hat{q}_1(A,R) = (\\frac{2}{3})(2) + (\\frac{1}{3})(0) = \\frac{4}{3}$。\n- $\\hat{v}_1(B) = \\pi(L|B)\\hat{q}_1(B,L) + \\pi(R|B)\\hat{q}_1(B,R) = (\\frac{1}{2})(1) + (\\frac{1}{2})(3) = \\frac{4}{2} = 2$。\n\n我们对每一幕评估该估计器。\n\n幕 1： $\\tau_1 = (s_0=A, a_0=L, r_0=1, s_1=B, a_1=R, r_1=2)$\n- $\\rho_0 = \\rho(A,L) = \\frac{4}{3}$。\n- $\\rho_1 = \\rho(B,R) = \\frac{3}{4}$。\n- $\\delta_0 = r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) = 1 + \\frac{1}{2}\\hat{v}_1(B) - \\hat{q}_0(A,L) = 1 + \\frac{1}{2}(2) - 2 = 0$。\n- $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1) = 2 - \\hat{q}_1(B,R) = 2 - 3 = -1$。\n- $\\hat{V}(\\tau_1) = \\hat{v}_0(A) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1 = \\frac{5}{3} + (\\frac{4}{3})(0) + (\\frac{1}{2})(\\frac{4}{3})(\\frac{3}{4})(-1) = \\frac{5}{3} - \\frac{1}{2} = \\frac{10-3}{6} = \\frac{7}{6}$。\n\n幕 2： $\\tau_2 = (s_0=A, a_0=R, r_0=0, s_1=A, a_1=L, r_1=3)$\n- $\\rho_0 = \\rho(A,R) = \\frac{2}{3}$。\n- $\\rho_1 = \\rho(A,L) = \\frac{4}{3}$。\n- $\\delta_0 = r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) = 0 + \\frac{1}{2}\\hat{v}_1(A) - \\hat{q}_0(A,R) = \\frac{1}{2}(\\frac{4}{3}) - 1 = \\frac{2}{3} - 1 = -\\frac{1}{3}$。\n- $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1) = 3 - \\hat{q}_1(A,L) = 3 - 2 = 1$。\n- $\\hat{V}(\\tau_2) = \\hat{v}_0(A) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1 = \\frac{5}{3} + (\\frac{2}{3})(-\\frac{1}{3}) + (\\frac{1}{2})(\\frac{2}{3})(\\frac{4}{3})(1) = \\frac{5}{3} - \\frac{2}{9} + \\frac{4}{9} = \\frac{15}{9} + \\frac{2}{9} = \\frac{17}{9}$。\n\n幕 3： $\\tau_3 = (s_0=A, a_0=L, r_0=2, s_1=B, a_1=L, r_1=0)$\n- $\\rho_0 = \\rho(A,L) = \\frac{4}{3}$。\n- $\\rho_1 = \\rho(B,L) = \\frac{3}{2}$。\n- $\\delta_0 = r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) = 2 + \\frac{1}{2}\\hat{v}_1(B) - \\hat{q}_0(A,L) = 2 + \\frac{1}{2}(2) - 2 = 1$。\n- $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1) = 0 - \\hat{q}_1(B,L) = 0 - 1 = -1$。\n- $\\hat{V}(\\tau_3) = \\hat{v}_0(A) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1 = \\frac{5}{3} + (\\frac{4}{3})(1) + (\\frac{1}{2})(\\frac{4}{3})(\\frac{3}{2})(-1) = \\frac{5}{3} + \\frac{4}{3} - 1 = \\frac{9}{3} - 1 = 3 - 1 = 2$。\n\n最后，我们对 $N=3$ 个幕的估计值取平均：\n$$\\hat{V}^{\\pi}(A) = \\frac{1}{3}(\\hat{V}(\\tau_1) + \\hat{V}(\\tau_2) + \\hat{V}(\\tau_3)) = \\frac{1}{3}\\left(\\frac{7}{6} + \\frac{17}{9} + 2\\right)$$\n为了对分数求和，我们找到公分母 $18$：\n$$\\hat{V}^{\\pi}(A) = \\frac{1}{3}\\left(\\frac{7 \\times 3}{18} + \\frac{17 \\times 2}{18} + \\frac{2 \\times 18}{18}\\right) = \\frac{1}{3}\\left(\\frac{21 + 34 + 36}{18}\\right) = \\frac{1}{3}\\left(\\frac{91}{18}\\right) = \\frac{91}{54}$$",
            "answer": "$$\\boxed{\\frac{91}{54}}$$"
        }
    ]
}