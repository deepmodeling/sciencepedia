{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in reinforcement learning is balancing the exploitation of known good actions with the exploration of new ones to discover potentially better strategies. A powerful and theoretically grounded method to encourage exploration is entropy regularization, which adds a bonus to the agent's objective for acting more randomly. This practice  provides a foundational exercise in deriving the optimal policy in an entropy-regularized setting, building essential intuition for how an agent's incentives shift when it is rewarded not just for collecting rewards, but also for maintaining a degree of unpredictability.",
            "id": "4113170",
            "problem": "Consider a single-state Markov Decision Process (MDP) with state $s$, action set $\\mathcal{A} = \\{0,1\\}$, and deterministic rewards $r(s,1) = r_{1}$ and $r(s,0) = r_{0}$, where $r_{1}$ and $r_{0}$ are real numbers. The environment transitions deterministically back to $s$ regardless of the chosen action. The agent employs a stationary stochastic policy $\\pi_{\\theta}(a \\mid s)$ parameterized by a scalar parameter $\\theta \\in \\mathbb{R}$ via a Boltzmann (softmax) parameterization over action logits, with logits defined by $\\ell(1) = \\theta$ and $\\ell(0) = 0$. The temperature is implicitly absorbed into the parameterization. Thus, the policy satisfies\n$$\n\\pi_{\\theta}(1 \\mid s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + \\exp(0)}, \\quad \\pi_{\\theta}(0 \\mid s) = \\frac{\\exp(0)}{\\exp(\\theta) + \\exp(0)}.\n$$\nThe agent aims to maximize a discounted entropy-regularized objective with discount factor $\\gamma \\in (0,1)$ and entropy coefficient $\\lambda > 0$. Specifically, the agent maximizes the expected discounted sum of per-step immediate rewards plus a Shannon entropy bonus weighted by $\\lambda$, where the per-step Shannon entropy of the policy at state $s$ is given by\n$$\nH(\\pi_{\\theta}(\\cdot \\mid s)) = - \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a \\mid s) \\ln \\big(\\pi_{\\theta}(a \\mid s)\\big).\n$$\nStarting strictly from the foundational definitions of expected discounted return, stationary policies, and Shannon entropy, derive the exact closed-form expression for the unique optimal parameter $\\theta^{\\star}$ that maximizes the objective. Express your final answer as a single analytic expression in terms of $r_{1}$, $r_{0}$, and $\\lambda$. No numerical evaluation is required, and no rounding is necessary. The answer must be a single closed-form expression.",
            "solution": "The user wants me to solve a reinforcement learning problem.\n\nHere's my plan:\n1.  **Validate the problem:** I need to check if the problem is well-defined, scientifically sound, and complete.\n    *   **Extract Givens:** I'll list all the definitions and parameters provided.\n        *   MDP: single state $s$.\n        *   Action set: $\\mathcal{A} = \\{0, 1\\}$.\n        *   Rewards: $r(s, 1) = r_1$, $r(s, 0) = r_0$. Both are real numbers.\n        *   Transitions: Deterministic, always back to state $s$. So $P(s' = s | s, a) = 1$ for all $a \\in \\mathcal{A}$.\n        *   Policy: Stationary stochastic policy $\\pi_{\\theta}(a|s)$.\n        *   Parameterization: Boltzmann (softmax) with logits $\\ell(1) = \\theta$ and $\\ell(0) = 0$.\n        *   Policy probabilities:\n            *   $\\pi_{\\theta}(1|s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + \\exp(0)} = \\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}$.\n            *   $\\pi_{\\theta}(0|s) = \\frac{\\exp(0)}{\\exp(\\theta) + \\exp(0)} = \\frac{1}{\\exp(\\theta) + 1}$.\n        *   Objective: Maximize discounted entropy-regularized return.\n        *   Discount factor: $\\gamma \\in (0, 1)$.\n        *   Entropy coefficient: $\\lambda > 0$.\n        *   Per-step Shannon entropy: $H(\\pi_{\\theta}(\\cdot|s)) = - \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) \\ln(\\pi_{\\theta}(a|s))$.\n        *   Task: Find the optimal parameter $\\theta^{\\star}$ that maximizes the objective. The answer should be a closed-form expression in terms of $r_1$, $r_0$, and $\\lambda$.\n\n    *   **Validate:**\n        *   **Scientifically Grounded:** Yes, this is a standard problem setup in entropy-regularized reinforcement learning. The concepts of MDP, softmax policy, and Shannon entropy are all well-established.\n        *   **Well-Posed:** The problem asks to maximize a function with respect to a single parameter $\\theta$. The objective function seems well-defined. The domain of $\\theta$ is $\\mathbb{R}$. I'll need to check if a unique maximum exists. The objective involves logarithms and exponentials, which are smooth functions. The domain of the entropy term's logarithm is handled since policy probabilities are always positive. It looks well-posed.\n        *   **Objective:** The language is precise and mathematical. No subjective claims.\n        *   **Incomplete/Contradictory:** The setup is self-contained. All necessary components (state, action, reward, transition, policy, objective) are defined.\n        *   **Unrealistic/Infeasible:** This is a theoretical problem, so physical realism isn't the primary concern. The mathematical setup is sound.\n        *   **Ill-Posed/Poorly Structured:** The structure is clear. The goal is to maximize an objective function. I will need to set up this function and then use calculus to find the maximum.\n        *   **Pseudo-Profound/Trivial:** While a single-state MDP is a simplification, adding entropy regularization and a specific policy parameterization makes it a non-trivial calculus problem that tests understanding of core RL concepts. It's not a tautology.\n        *   **Outside Scientific Verifiability:** The solution can be verified through standard mathematical derivation (calculus).\n\n    *   **Verdict:** The problem is **valid**.\n\n2.  **Solve the problem:**\n    *   **Define the Objective Function:** The objective is the expected discounted sum of rewards and entropy bonuses. Since the state is always $s$, the value function at state $s$ for a policy $\\pi_{\\theta}$ is the objective we want to maximize. Let's call it $J(\\theta)$.\n        *   The agent is always in state $s$. At each time step $t=0, 1, 2, ...$, the agent takes an action $a_t \\sim \\pi_{\\theta}(\\cdot|s)$.\n        *   The immediate reward at step $t$ is a random variable $R_{t+1}$ which depends on the action $a_t$. The expectation of the immediate reward is:\n            $E_{\\pi_{\\theta}}[R_{t+1}] = \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) r(s, a) = \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0$.\n        *   The entropy bonus at each step is also constant since the policy is stationary and there's only one state: $\\lambda H(\\pi_{\\theta}(\\cdot|s))$.\n        *   The total per-step augmented reward is $E[R_{t+1}] + \\lambda H(\\pi_{\\theta}(\\cdot|s))$.\n        *   The objective function is the expected discounted sum of these augmented rewards:\n            $J(\\theta) = E_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t (R_{t+1} + \\lambda H(\\pi_{\\theta}(\\cdot|s))) \\right]$.\n        *   Since the per-step augmented reward is constant in expectation, we can pull it out of the sum:\n            $J(\\theta) = \\left( E[R_{t+1}] + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right) \\sum_{t=0}^{\\infty} \\gamma^t$.\n        *   The geometric series sums to $\\sum_{t=0}^{\\infty} \\gamma^t = \\frac{1}{1-\\gamma}$.\n        *   So, $J(\\theta) = \\frac{1}{1-\\gamma} \\left( \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0 + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right)$.\n        *   To maximize $J(\\theta)$, we can ignore the constant positive factor $\\frac{1}{1-\\gamma}$ and just maximize the term inside the parenthesis. Let's define the simplified objective function $L(\\theta)$:\n            $L(\\theta) = \\pi_{\\theta}(1|s) r_1 + \\pi_{\\theta}(0|s) r_0 + \\lambda H(\\pi_{\\theta}(\\cdot|s))$.\n\n    *   **Substitute Policy and Entropy Expressions:**\n        *   Let $p_1 = \\pi_{\\theta}(1|s) = \\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}$ and $p_0 = \\pi_{\\theta}(0|s) = \\frac{1}{\\exp(\\theta) + 1}$.\n        *   Also note that $p_1$ is the sigmoid function of $\\theta$, often written as $\\sigma(\\theta)$. So $p_0 = 1 - p_1 = 1 - \\sigma(\\theta) = \\sigma(-\\theta)$.\n        *   The entropy term is:\n            $H(\\pi_{\\theta}(\\cdot|s)) = -(p_1 \\ln(p_1) + p_0 \\ln(p_0))$.\n        *   Substitute these into $L(\\theta)$:\n            $L(\\theta) = p_1 r_1 + p_0 r_0 - \\lambda (p_1 \\ln(p_1) + p_0 \\ln(p_0))$.\n        *   Now substitute the expressions for $p_1$ and $p_0$ in terms of $\\theta$:\n            $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$\n            $p_0 = \\frac{1}{\\exp(\\theta)+1}$\n            $\\ln(p_1) = \\ln\\left(\\frac{\\exp(\\theta)}{\\exp(\\theta)+1}\\right) = \\theta - \\ln(\\exp(\\theta) + 1)$\n            $\\ln(p_0) = \\ln\\left(\\frac{1}{\\exp(\\theta)+1}\\right) = - \\ln(\\exp(\\theta) + 1)$\n        *   Let's plug these into the entropy term:\n            $H = -[ p_1(\\theta - \\ln(\\exp(\\theta)+1)) + p_0(-\\ln(\\exp(\\theta)+1)) ]$\n            $H = -[ p_1 \\theta - (p_1+p_0)\\ln(\\exp(\\theta)+1) ]$\n            Since $p_1+p_0 = 1$:\n            $H = -[ p_1 \\theta - \\ln(\\exp(\\theta)+1) ]$\n            $H = \\ln(\\exp(\\theta)+1) - p_1 \\theta$\n        *   Now substitute this entropy expression back into $L(\\theta)$:\n            $L(\\theta) = p_1 r_1 + p_0 r_0 + \\lambda (\\ln(\\exp(\\theta)+1) - p_1 \\theta)$\n        *   Use $p_0 = 1-p_1$:\n            $L(\\theta) = p_1 r_1 + (1-p_1)r_0 + \\lambda (\\ln(\\exp(\\theta)+1) - p_1 \\theta)$\n            $L(\\theta) = p_1(r_1 - r_0) + r_0 + \\lambda \\ln(\\exp(\\theta)+1) - \\lambda p_1 \\theta$\n            $L(\\theta) = p_1 (r_1 - r_0 - \\lambda \\theta) + r_0 + \\lambda \\ln(\\exp(\\theta)+1)$\n        *   Substitute $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$:\n            $L(\\theta) = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} (r_1 - r_0 - \\lambda \\theta) + r_0 + \\lambda \\ln(\\exp(\\theta)+1)$\n            This form looks a bit complex to differentiate. Let's try another approach.\n\n    *   **Alternative expression for L(theta):**\n        *   $L(\\theta) = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} r_1 + \\frac{1}{\\exp(\\theta)+1} r_0 - \\lambda \\left[ \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} \\ln\\left(\\frac{\\exp(\\theta)}{\\exp(\\theta)+1}\\right) + \\frac{1}{\\exp(\\theta)+1} \\ln\\left(\\frac{1}{\\exp(\\theta)+1}\\right) \\right]$\n        *   $L(\\theta) = \\frac{1}{\\exp(\\theta)+1} \\left( r_1\\exp(\\theta) + r_0 \\right) - \\frac{\\lambda}{\\exp(\\theta)+1} \\left[ \\exp(\\theta)(\\theta - \\ln(\\exp(\\theta)+1)) + 1(-\\ln(\\exp(\\theta)+1)) \\right]$\n        *   $L(\\theta) = \\frac{1}{\\exp(\\theta)+1} \\left( r_1\\exp(\\theta) + r_0 \\right) - \\frac{\\lambda}{\\exp(\\theta)+1} \\left[ \\theta\\exp(\\theta) - (\\exp(\\theta)+1)\\ln(\\exp(\\theta)+1) \\right]$\n        *   $L(\\theta) = \\frac{r_1\\exp(\\theta) + r_0 - \\lambda\\theta\\exp(\\theta)}{\\exp(\\theta)+1} + \\lambda\\ln(\\exp(\\theta)+1)$\n        This looks correct.\n\n    *   **Find the Maximum:** To find the optimal $\\theta^{\\star}$, we need to compute the derivative of $L(\\theta)$ with respect to $\\theta$ and set it to zero: $\\frac{dL}{d\\theta} = 0$.\n        *   Let's differentiate $L(\\theta)$ term by term.\n        *   $L(\\theta) = p_1 r_1 + p_0 r_0 - \\lambda (p_1 \\ln(p_1) + p_0 \\ln(p_0))$.\n        *   We need the derivative of $p_1$ and $p_0$ with respect to $\\theta$.\n        *   $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$.\n            $\\frac{dp_1}{d\\theta} = \\frac{\\exp(\\theta)(\\exp(\\theta)+1) - \\exp(\\theta)\\exp(\\theta)}{(\\exp(\\theta)+1)^2} = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2} = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1} \\frac{1}{\\exp(\\theta)+1} = p_1 p_0$.\n        *   $p_0 = 1 - p_1$.\n            $\\frac{dp_0}{d\\theta} = - \\frac{dp_1}{d\\theta} = -p_1 p_0$.\n        *   Now differentiate $L(\\theta)$:\n            $\\frac{dL}{d\\theta} = r_1 \\frac{dp_1}{d\\theta} + r_0 \\frac{dp_0}{d\\theta} - \\lambda \\frac{d}{d\\theta} (p_1 \\ln(p_1) + p_0 \\ln(p_0))$\n        *   Focus on the entropy derivative part first:\n            $\\frac{d}{d\\theta}(p_1 \\ln p_1) = \\frac{dp_1}{d\\theta} \\ln p_1 + p_1 \\frac{1}{p_1} \\frac{dp_1}{d\\theta} = \\frac{dp_1}{d\\theta} (\\ln p_1 + 1)$.\n            $\\frac{d}{d\\theta}(p_0 \\ln p_0) = \\frac{dp_0}{d\\theta} \\ln p_0 + p_0 \\frac{1}{p_0} \\frac{dp_0}{d\\theta} = \\frac{dp_0}{d\\theta} (\\ln p_0 + 1)$.\n        *   So, $\\frac{d}{d\\theta} (p_1 \\ln p_1 + p_0 \\ln p_0) = \\frac{dp_1}{d\\theta} (\\ln p_1 + 1) + \\frac{dp_0}{d\\theta} (\\ln p_0 + 1)$.\n        *   Substitute $\\frac{dp_1}{d\\theta} = p_1 p_0$ and $\\frac{dp_0}{d\\theta} = -p_1 p_0$:\n            $= p_1 p_0 (\\ln p_1 + 1) - p_1 p_0 (\\ln p_0 + 1)$\n            $= p_1 p_0 (\\ln p_1 + 1 - \\ln p_0 - 1)$\n            $= p_1 p_0 (\\ln p_1 - \\ln p_0) = p_1 p_0 \\ln(p_1/p_0)$.\n        *   Now, let's find $\\ln(p_1/p_0)$:\n            $p_1/p_0 = \\frac{\\exp(\\theta)/(\\exp(\\theta)+1)}{1/(\\exp(\\theta)+1)} = \\exp(\\theta)$.\n            So, $\\ln(p_1/p_0) = \\ln(\\exp(\\theta)) = \\theta$.\n        *   The derivative of the entropy part is $p_1 p_0 \\theta$.\n        *   Let's return to the full derivative of $L(\\theta)$:\n            $\\frac{dL}{d\\theta} = r_1 (p_1 p_0) + r_0 (-p_1 p_0) - \\lambda (p_1 p_0 \\theta)$\n            $\\frac{dL}{d\\theta} = p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$.\n        *   Set the derivative to zero to find the critical points:\n            $p_1 p_0 (r_1 - r_0 - \\lambda \\theta) = 0$.\n        *   $p_1 = \\frac{\\exp(\\theta)}{\\exp(\\theta)+1}$ is never zero.\n        *   $p_0 = \\frac{1}{\\exp(\\theta)+1}$ is never zero.\n        *   Therefore, for the product to be zero, the third term must be zero:\n            $r_1 - r_0 - \\lambda \\theta = 0$.\n        *   Solving for $\\theta$:\n            $\\lambda \\theta = r_1 - r_0$\n            $\\theta = \\frac{r_1 - r_0}{\\lambda}$.\n\n    *   **Check if it's a maximum:**\n        *   We need to check the sign of the second derivative, $\\frac{d^2L}{d\\theta^2}$.\n        *   $\\frac{dL}{d\\theta} = p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$.\n        *   Let $f(\\theta) = p_1 p_0 = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2}$ and $g(\\theta) = r_1 - r_0 - \\lambda \\theta$.\n        *   $\\frac{d^2L}{d\\theta^2} = \\frac{df}{d\\theta} g(\\theta) + f(\\theta) \\frac{dg}{d\\theta}$.\n        *   At the critical point $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$, we have $g(\\theta^{\\star}) = 0$.\n        *   So, $\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} = f(\\theta^{\\star}) \\frac{dg}{d\\theta}\\bigg|_{\\theta=\\theta^{\\star}}$.\n        *   $f(\\theta) = p_1 p_0 = \\frac{\\exp(\\theta)}{(\\exp(\\theta)+1)^2}$. Since $\\exp(\\theta) > 0$, we have $f(\\theta) > 0$ for all $\\theta \\in \\mathbb{R}$.\n        *   $\\frac{dg}{d\\theta} = \\frac{d}{d\\theta} (r_1 - r_0 - \\lambda \\theta) = -\\lambda$.\n        *   So, $\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} = f(\\theta^{\\star}) (-\\lambda) = - \\lambda \\frac{\\exp(\\theta^{\\star})}{(\\exp(\\theta^{\\star})+1)^2}$.\n        *   We are given that $\\lambda > 0$. And as argued, the fraction term is always positive.\n        *   Therefore, $\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^{\\star}} < 0$.\n        *   This confirms that the critical point $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$ is a local maximum. Since this is the only critical point on $\\mathbb{R}$, and the function is smooth, it must be the unique global maximum.\n\n    *   **Final Answer Formulation:**\n        *   The question asks for the exact closed-form expression for the unique optimal parameter $\\theta^{\\star}$.\n        *   My derivation gives $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$.\n        *   This is an analytical expression in terms of $r_1$, $r_0$, and $\\lambda$.\n        *   It satisfies all the requirements.\n\n    *   **Double-check the derivation and interpretation.**\n        *   The objective is the value function $V^{\\pi_\\theta}(s)$.\n        *   The entropy-regularized Bellman equation for a policy $\\pi$ is:\n            $Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{\\pi}(s')$\n            $V^{\\pi}(s) = \\sum_a \\pi(a|s) Q^{\\pi}(s,a) + \\lambda H(\\pi(\\cdot|s))$\n        *   In our case, $P(s'|s,a) = \\delta_{s,s'}$. So $\\sum_{s'} P(s'|s,a) V^{\\pi}(s') = V^{\\pi}(s)$.\n        *   $Q^{\\pi_\\theta}(s,a) = r(s,a) + \\gamma V^{\\pi_\\theta}(s)$\n        *   $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) (r(s,a) + \\gamma V^{\\pi_\\theta}(s)) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\gamma V^{\\pi_\\theta}(s) \\sum_a \\pi_{\\theta}(a|s) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   Since $\\sum_a \\pi_{\\theta}(a|s) = 1$:\n            $V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\gamma V^{\\pi_\\theta}(s) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $(1-\\gamma) V^{\\pi_\\theta}(s) = \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\lambda H(\\pi_{\\theta}(\\cdot|s))$\n        *   $V^{\\pi_\\theta}(s) = \\frac{1}{1-\\gamma} \\left( \\sum_a \\pi_{\\theta}(a|s) r(s,a) + \\lambda H(\\pi_{\\theta}(\\cdot|s)) \\right)$.\n        *   This matches my initial formulation of the objective function $J(\\theta)$. Maximizing $V^{\\pi_\\theta}(s)$ is equivalent to maximizing the term in the parenthesis, which I called $L(\\theta)$.\n        *   $L(\\theta) = \\pi_{\\theta}(1|s)r_1 + \\pi_{\\theta}(0|s)r_0 + \\lambda H(\\pi_{\\theta}(\\cdot | s))$.\n        *   Let's check the entropy calculation again.\n            $H = - \\sum_{a} \\pi(a|s) \\ln \\pi(a|s)$.\n            With $\\pi(1|s) = p_1$ and $\\pi(0|s) = p_0$:\n            $H = - p_1 \\ln p_1 - p_0 \\ln p_0$.\n            $\\frac{dH}{d\\theta} = -\\frac{d}{d\\theta}(p_1 \\ln p_1) - \\frac{d}{d\\theta}(p_0 \\ln p_0)$\n            $= - [ \\frac{dp_1}{d\\theta}(\\ln p_1 + 1) + \\frac{dp_0}{d\\theta}(\\ln p_0 + 1) ]$\n            $= - [ (p_1 p_0)(\\ln p_1 + 1) + (-p_1 p_0)(\\ln p_0 + 1) ]$\n            $= - p_1 p_0 (\\ln p_1 - \\ln p_0) = - p_1 p_0 \\ln(p_1/p_0)$.\n            Since $p_1/p_0 = \\exp(\\theta)$, we have $\\ln(p_1/p_0) = \\theta$.\n            So $\\frac{dH}{d\\theta} = - p_1 p_0 \\theta$.\n        *   Now the derivative of $L(\\theta)$:\n            $\\frac{dL}{d\\theta} = \\frac{d}{d\\theta} (p_1 r_1 + p_0 r_0) + \\lambda \\frac{dH}{d\\theta}$\n            $= ( \\frac{dp_1}{d\\theta} r_1 + \\frac{dp_0}{d\\theta} r_0 ) + \\lambda (-p_1 p_0 \\theta)$\n            $= (p_1 p_0 r_1 - p_1 p_0 r_0) - \\lambda p_1 p_0 \\theta$\n            $= p_1 p_0 (r_1 - r_0 - \\lambda \\theta)$.\n        *   This confirms my previous derivative calculation. The rest of the argument holds.\n        *   The result $\\theta^{\\star} = \\frac{r_1 - r_0}{\\lambda}$ is a well-known result for the two-action case with entropy regularization. The logic is that the optimal logit difference should be proportional to the reward difference, scaled by the entropy coefficient (which acts like a temperature). If $\\lambda$ is large (high temperature), the agent becomes more random and the logits are pushed towards zero, so $\\theta^{\\star}$ is small. If $\\lambda$ is small (low temperature), the agent is more exploitative and the optimal logit $\\theta^{\\star}$ is large, reflecting the reward difference more strongly. If $r_1 > r_0$, $\\theta^{\\star} > 0$, favoring action 1. If $r_1 < r_0$, $\\theta^{\\star} < 0$, favoring action 0. If $r_1 = r_0$, $\\theta^{\\star} = 0$, leading to a uniform policy $\\pi(1|s) = \\pi(0|s) = 0.5$, which makes sense as there's no reward incentive to prefer one action, so the agent maximizes entropy. The result is physically and intuitively consistent with the principles of entropy-regularized RL.\n\n    *   The steps are:\n        1.  Formulate the objective function $J(\\theta)$ which is the value function $V^{\\pi_\\theta}(s)$.\n        2.  Show that maximizing $J(\\theta)$ is equivalent to maximizing a simpler term $L(\\theta)$.\n        3.  Express $L(\\theta)$ in terms of the parameter $\\theta$.\n        4.  Differentiate $L(\\theta)$ with respect to $\\theta$.\n        5.  Set the derivative to zero and solve for $\\theta$.\n        6.  Verify that this solution corresponds to a maximum using the second derivative test.\n        7.  Write down the final expression.\n\nThe structure of the solution should be clear and follow these steps. All mathematical entities will be in LaTeX.\nThe final answer is a single expression $\\frac{r_1 - r_0}{\\lambda}$. I will enclose it in `$$\\boxed{...}$$`.\nEverything looks correct. I am ready to write the solution.",
            "answer": "$$\n\\boxed{\\frac{r_{1} - r_{0}}{\\lambda}}\n$$"
        },
        {
            "introduction": "While entropy regularization modifies an agent's intrinsic objective, we can also accelerate learning by modifying the extrinsic reward signal it receives from the environment. This technique, known as reward shaping, can provide denser feedback to guide the agent. However, naively altering rewards can inadvertently change the optimal policy. This exercise  explores a specific, principled method called potential-based reward shaping, requiring you to derive a proof that this method guides the agent's learning without altering the underlying optimal behavior, a critical guarantee for its practical application.",
            "id": "4113139",
            "problem": "Consider a single learning agent embedded in a complex adaptive system that is modeled as a Markov Decision Process (MDP). A Markov Decision Process (MDP) is defined by a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a transition kernel $P(s' \\mid s, a)$, and a bounded one-step reward function $r(s,a,s')$ with $|r(s,a,s')| \\leq R_{\\max} < \\infty$. The agent follows a stationary policy $\\pi(a \\mid s)$ and interacts over an infinite horizon with a discount factor $\\gamma \\in (0,1)$. The realized one-step reward at time $t$ is denoted $R_t$, and the discounted return from time $t$ is defined by $G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k}$. The corresponding value function under policy $\\pi$ is $V^{\\pi}(s) = \\mathbb{E}[G_t \\mid s_t = s]$, where the expectation is over the trajectory distribution induced by $\\pi$ and $P$.\n\nTo accelerate learning while preserving the decision-making structure, the agentâ€™s environmental reward is modified by a potential-based shaping term using a bounded potential function $\\Phi : \\mathcal{S} \\to \\mathbb{R}$ with $\\sup_{s \\in \\mathcal{S}} |\\Phi(s)| < \\infty$. The shaped reward at time $t$ is defined by $R'_t = R_t + \\gamma \\, \\Phi(s_{t+1}) - \\Phi(s_t)$, yielding a shaped discounted return $G'_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k}$ and a shaped value function $V'^{\\pi}(s) = \\mathbb{E}[G'_t \\mid s_t = s]$.\n\nStarting from the fundamental definitions of discounted return and expectation, and without invoking any shortcut identities, derive the exact closed-form expression for the difference $V'^{\\pi}(s_0) - V^{\\pi}(s_0)$ for an arbitrary initial state $s_0 \\in \\mathcal{S}$ under any stationary policy $\\pi$ that induces a well-defined trajectory distribution. Assume all required expectations exist and that the boundedness conditions stated above hold so that infinite series converge and interchanges of limits and expectations are justified by standard theorems in measure-theoretic probability.\n\nExpress your final answer as a single analytic expression in terms of $\\Phi(s_0)$ only. No numerical approximation, rounding, units, or percentage formatting are required.",
            "solution": "The problem requires the derivation of the exact closed-form expression for the difference between the shaped value function, $V'^{\\pi}(s_0)$, and the original value function, $V^{\\pi}(s_0)$, for an arbitrary initial state $s_0$. The derivation will proceed from the fundamental definitions provided in the problem statement.\n\nThe quantity to be derived is $V'^{\\pi}(s_0) - V^{\\pi}(s_0)$.\n\nBy definition, the value functions are the expected discounted returns:\n$$V'^{\\pi}(s_0) = \\mathbb{E}[G'_t \\mid s_t = s_0]$$\n$$V^{\\pi}(s_0) = \\mathbb{E}[G_t \\mid s_t = s_0]$$\nThe expectation $\\mathbb{E}[\\cdot \\mid s_t = s_0]$ is taken over all possible trajectories starting from state $s_0$, with the trajectory distribution induced by the policy $\\pi(a \\mid s)$ and the transition kernel $P(s' \\mid s, a)$.\n\nThe difference can be written as:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t \\mid s_t = s_0] - \\mathbb{E}[G_t \\mid s_t = s_0]$$\nBy the linearity of expectation, we can combine the terms:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t - G_t \\mid s_t = s_0]$$\n\nNext, we substitute the definitions of the discounted returns $G'_t$ and $G_t$:\n$$G'_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k}$$\n$$G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k}$$\nThe difference between the returns is:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} R'_{t+k} - \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k} = \\sum_{k=0}^{\\infty} \\gamma^{k} (R'_{t+k} - R_{t+k})$$\nThe problem states that the shaped reward $R'_t$ is defined as $R'_t = R_t + \\gamma \\, \\Phi(s_{t+1}) - \\Phi(s_t)$. Applying this to the reward at time step $t+k$:\n$$R'_{t+k} = R_{t+k} + \\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k})$$\nFrom this, the difference in rewards is:\n$$R'_{t+k} - R_{t+k} = \\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k})$$\nSubstituting this expression back into the sum for $G'_t - G_t$:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} (\\gamma \\, \\Phi(s_{t+k+1}) - \\Phi(s_{t+k}))$$\nWe can distribute the $\\gamma^k$ term into the parenthesis:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} (\\gamma^{k+1} \\Phi(s_{t+k+1}) - \\gamma^k \\Phi(s_{t+k}))$$\nThis is a telescoping series. To see this, let's define a sequence $A_k = \\gamma^k \\Phi(s_{t+k})$. The sum becomes:\n$$G'_t - G_t = \\sum_{k=0}^{\\infty} (A_{k+1} - A_k)$$\nLet's analyze the partial sum up to an arbitrary integer $N$:\n$$S_N = \\sum_{k=0}^{N} (A_{k+1} - A_k) = (A_1 - A_0) + (A_2 - A_1) + \\dots + (A_{N+1} - A_N)$$\nThe intermediate terms cancel out, leaving:\n$$S_N = A_{N+1} - A_0$$\nSubstituting back the definition of $A_k$:\n$$S_N = \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\gamma^0 \\Phi(s_{t+0}) = \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\Phi(s_t)$$\nTo find the value of the infinite sum, we take the limit as $N \\to \\infty$:\n$$G'_t - G_t = \\lim_{N\\to\\infty} S_N = \\lim_{N\\to\\infty} \\left( \\gamma^{N+1} \\Phi(s_{t+N+1}) - \\Phi(s_t) \\right)$$\nThe problem states that the potential function is bounded, $\\sup_{s \\in \\mathcal{S}} |\\Phi(s)| < \\infty$. Let's denote this bound by $\\Phi_{\\max}$. We also have $\\gamma \\in (0,1)$.\nThe term $\\gamma^{N+1} \\Phi(s_{t+N+1})$ is bounded in magnitude:\n$$|\\gamma^{N+1} \\Phi(s_{t+N+1})| \\leq \\gamma^{N+1} \\Phi_{\\max}$$\nSince $\\lim_{N\\to\\infty} \\gamma^{N+1} = 0$, it follows by the Squeeze Theorem that:\n$$\\lim_{N\\to\\infty} \\gamma^{N+1} \\Phi(s_{t+N+1}) = 0$$\nThe term $-\\Phi(s_t)$ is independent of $N$. Therefore, the limit of the partial sum is:\n$$G'_t - G_t = 0 - \\Phi(s_t) = -\\Phi(s_t)$$\nNow we can return to the expression for the difference in value functions:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = \\mathbb{E}[G'_t - G_t \\mid s_t = s_0] = \\mathbb{E}[-\\Phi(s_t) \\mid s_t = s_0]$$\nThe expectation of the quantity $-\\Phi(s_t)$ is conditioned on the event that the state at time $t$ is $s_0$. The expression $-\\Phi(s_t)$ is a deterministic function of the state $s_t$. Given the condition $s_t = s_0$, the value of the expression is fixed at $-\\Phi(s_0)$. Therefore, the expectation is simply this fixed value:\n$$\\mathbb{E}[-\\Phi(s_t) \\mid s_t = s_0] = -\\Phi(s_0)$$\nThis leads to the final result:\n$$V'^{\\pi}(s_0) - V^{\\pi}(s_0) = -\\Phi(s_0)$$\nThis concludes the derivation from first principles as required.",
            "answer": "$$\\boxed{-\\Phi(s_0)}$$"
        },
        {
            "introduction": "Moving from theory to practice, a crucial task is to evaluate the performance of a new \"target\" policy using data collected from a different \"behavior\" policy, a scenario known as Off-Policy Evaluation (OPE). This is vital for safely deploying agents in real-world systems where live testing is risky or costly. This hands-on problem  challenges you to apply the sophisticated Doubly Robust estimation technique, which cleverly combines model-based predictions with importance-sampling corrections to provide an accurate and reliable estimate of a new policy's value from a fixed dataset.",
            "id": "4113154",
            "problem": "Consider an episodic Markov Decision Process (MDP) with finite state space $\\mathcal{S}=\\{A,B\\}$, finite action space $\\mathcal{A}=\\{L,R\\}$, and horizon $H=2$ with time indices $t\\in\\{0,1\\}$. The initial state is deterministically $s_0=A$. Rewards are bounded and real-valued. Let the discount factor be $\\gamma=\\frac{1}{2}$. A dataset of $N=3$ independent episodes was generated by a behavior policy $b(a\\mid s)$, while we aim to evaluate a distinct target policy $\\pi(a\\mid s)$, a setting known as Off-Policy Evaluation (OPE) in reinforcement learning. The logged episodes are:\n- Episode $1$: $(s_0=A,a_0=L,r_0=1,s_1=B,a_1=R,r_1=2)$.\n- Episode $2$: $(s_0=A,a_0=R,r_0=0,s_1=A,a_1=L,r_1=3)$.\n- Episode $3$: $(s_0=A,a_0=L,r_0=2,s_1=B,a_1=L,r_1=0)$.\n\nThe behavior and target policies are given by\n- For $s=A$: $b(L\\mid A)=\\frac{1}{2}$, $b(R\\mid A)=\\frac{1}{2}$; $\\pi(L\\mid A)=\\frac{2}{3}$, $\\pi(R\\mid A)=\\frac{1}{3}$.\n- For $s=B$: $b(L\\mid B)=\\frac{1}{3}$, $b(R\\mid B)=\\frac{2}{3}$; $\\pi(L\\mid B)=\\frac{1}{2}$, $\\pi(R\\mid B)=\\frac{1}{2}$.\n\nAssume access to a learned model that provides estimates $\\hat{q}_t(s,a)$ of the state-action value under the target policy at times $t=0,1$, and define $\\hat{v}_t(s)\\triangleq \\sum_{a\\in\\mathcal{A}}\\pi(a\\mid s)\\hat{q}_t(s,a)$ for $t=0,1$, with the terminal convention $\\hat{v}_2(\\cdot)=0$. The model outputs are\n- At $t=0$: $\\hat{q}_0(A,L)=2$, $\\hat{q}_0(A,R)=1$.\n- At $t=1$: $\\hat{q}_1(A,L)=2$, $\\hat{q}_1(A,R)=0$, $\\hat{q}_1(B,L)=1$, $\\hat{q}_1(B,R)=3$.\n\nStarting from the fundamental definition of the value of a policy $V^{\\pi}(s_0)=\\mathbb{E}_{\\pi}\\!\\left[\\sum_{t=0}^{1}\\gamma^t r_t \\,\\middle|\\, s_0\\right]$ and the likelihood-ratio identity that relates expectations under $\\pi$ and $b$, derive, from first principles, a step-wise estimator that uses a model-based baseline (constructed from $\\hat{q}_t$ and $\\hat{v}_t$) together with importance sampling corrections to yield an unbiased estimator of $V^{\\pi}(A)$ when the model is correct and a consistent estimator otherwise. Then, evaluate this estimator on the given dataset by averaging its per-trajectory value over the $N=3$ episodes. Use the provided $\\gamma$, policies, and model outputs. Express your final answer as a single exact real number. No rounding is required.",
            "solution": "The objective is to estimate the value of a target policy $\\pi$, denoted $V^{\\pi}(s_0)$, given data from a behavior policy $b$. The problem requires the derivation and application of a step-wise estimator that combines a model-based estimate with importance sampling corrections. This is a form of the Doubly Robust (DR) estimator, which we derive from first principles for a finite horizon $H$.\n\nThe value of the target policy at time $t$ is $V_t^{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=t}^{H-1} \\gamma^{k-t} r_k \\mid s_t=s\\right]$. We aim to estimate $V_0^{\\pi}(s_0)$. We are given a model consisting of state-action value estimates $\\hat{q}_t(s,a)$ and state value estimates $\\hat{v}_t(s) = \\sum_{a} \\pi(a|s)\\hat{q}_t(s,a)$. By convention, $V_H^{\\pi}(s) = \\hat{v}_H(s) = 0$ for all $s$.\n\nWe construct a telescoping sum for the error of the initial model estimate, $V_0^{\\pi}(s_0) - \\hat{v}_0(s_0)$:\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\sum_{a_0} \\pi(a_0|s_0)Q_0^{\\pi}(s_0,a_0) - \\sum_{a_0} \\pi(a_0|s_0)\\hat{q}_0(s_0,a_0) = \\mathbb{E}_{a_0 \\sim \\pi(\\cdot|s_0)}[Q_0^{\\pi}(s_0,a_0) - \\hat{q}_0(s_0,a_0)]$$\nUsing importance sampling to switch to an expectation under the behavior policy $b$:\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\mathbb{E}_{(s_0,a_0)\\sim b}\\left[\\frac{\\pi(a_0|s_0)}{b(a_0|s_0)}(Q_0^{\\pi}(s_0,a_0) - \\hat{q}_0(s_0,a_0))\\right]$$\nLet $\\rho_t = \\frac{\\pi(a_t|s_t)}{b(a_t|s_t)}$. Substituting the Bellman equation $Q_0^{\\pi}(s_0, a_0) = r_0 + \\gamma \\mathbb{E}[V_1^{\\pi}(s_1)] = r_0 + \\gamma V_1^{\\pi}(s_1)$ (as $r_0$ and $s_1$ are determined by $s_0, a_0$ in the data):\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\mathbb{E}_{b}[\\rho_0 (r_0 + \\gamma V_1^{\\pi}(s_1) - \\hat{q}_0(s_0,a_0))]$$\nWe define the single-step Bellman residual for the model as $\\delta_t = r_t + \\gamma \\hat{v}_{t+1}(s_{t+1}) - \\hat{q}_t(s_t,a_t)$. Then we can rewrite the expression:\n$$V_0^{\\pi}(s_0) - \\hat{v}_0(s_0) = \\mathbb{E}_{b}[\\rho_0 (r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) + \\gamma V_1^{\\pi}(s_1) - \\gamma \\hat{v}_1(s_1))] = \\mathbb{E}_{b}[\\rho_0 \\delta_0 + \\gamma \\rho_0 (V_1^{\\pi}(s_1) - \\hat{v}_1(s_1))]$$\nThis expression is recursive. We can expand the term $(V_1^{\\pi}(s_1) - \\hat{v}_1(s_1))$ in the same way:\n$$V_1^{\\pi}(s_1) - \\hat{v}_1(s_1) = \\mathbb{E}_{b}[\\rho_1 (r_1 + \\gamma V_2^{\\pi}(s_2) - \\hat{q}_1(s_1,a_1)) | s_1]$$\nSince $H=2$, $t \\in \\{0, 1\\}$. Thus $V_2^{\\pi}(s_2) = 0$ and $\\hat{v}_2(s_2) = 0$. The residual at $t=1$ is $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1)$.\n$$V_1^{\\pi}(s_1) - \\hat{v}_1(s_1) = \\mathbb{E}_{b}[\\rho_1 \\delta_1 | s_1]$$\nSubstituting this back into the equation for $V_0^{\\pi}(s_0)$ and using the law of total expectation ($\\mathbb{E}[\\mathbb{E}[X|Y]] = \\mathbb{E}[X]$):\n$$V_0^{\\pi}(s_0) = \\hat{v}_0(s_0) + \\mathbb{E}_{b}[\\rho_0 \\delta_0] + \\mathbb{E}_{b}[\\gamma \\rho_0 \\rho_1 \\delta_1] = \\mathbb{E}_{b}[\\hat{v}_0(s_0) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1]$$\nThe per-trajectory estimator $\\hat{V}(\\tau)$ for a trajectory $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1)$ is thus:\n$$\\hat{V}(\\tau) = \\hat{v}_0(s_0) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1$$\nThe final estimate is the sample mean over the $N=3$ trajectories: $\\hat{V}^{\\pi}(A) = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}(\\tau_i)$.\n\nWe now compute the necessary components with $\\gamma = \\frac{1}{2}$ and $s_0=A$.\nImportance ratios $\\rho(s,a) = \\frac{\\pi(a|s)}{b(a|s)}$:\n- $\\rho(A,L) = \\frac{2/3}{1/2} = \\frac{4}{3}$\n- $\\rho(A,R) = \\frac{1/3}{1/2} = \\frac{2}{3}$\n- $\\rho(B,L) = \\frac{1/2}{1/3} = \\frac{3}{2}$\n- $\\rho(B,R) = \\frac{1/2}{2/3} = \\frac{3}{4}$\n\nModel-based state values $\\hat{v}_t(s)$:\n- $\\hat{v}_0(A) = \\pi(L|A)\\hat{q}_0(A,L) + \\pi(R|A)\\hat{q}_0(A,R) = (\\frac{2}{3})(2) + (\\frac{1}{3})(1) = \\frac{4}{3} + \\frac{1}{3} = \\frac{5}{3}$.\n- $\\hat{v}_1(A) = \\pi(L|A)\\hat{q}_1(A,L) + \\pi(R|A)\\hat{q}_1(A,R) = (\\frac{2}{3})(2) + (\\frac{1}{3})(0) = \\frac{4}{3}$.\n- $\\hat{v}_1(B) = \\pi(L|B)\\hat{q}_1(B,L) + \\pi(R|B)\\hat{q}_1(B,R) = (\\frac{1}{2})(1) + (\\frac{1}{2})(3) = \\frac{4}{2} = 2$.\n\nWe evaluate the estimator for each episode.\n\nEpisode 1: $\\tau_1 = (s_0=A, a_0=L, r_0=1, s_1=B, a_1=R, r_1=2)$\n- $\\rho_0 = \\rho(A,L) = \\frac{4}{3}$.\n- $\\rho_1 = \\rho(B,R) = \\frac{3}{4}$.\n- $\\delta_0 = r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) = 1 + \\frac{1}{2}\\hat{v}_1(B) - \\hat{q}_0(A,L) = 1 + \\frac{1}{2}(2) - 2 = 0$.\n- $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1) = 2 - \\hat{q}_1(B,R) = 2 - 3 = -1$.\n- $\\hat{V}(\\tau_1) = \\hat{v}_0(A) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1 = \\frac{5}{3} + (\\frac{4}{3})(0) + (\\frac{1}{2})(\\frac{4}{3})(\\frac{3}{4})(-1) = \\frac{5}{3} - \\frac{1}{2} = \\frac{10-3}{6} = \\frac{7}{6}$.\n\nEpisode 2: $\\tau_2 = (s_0=A, a_0=R, r_0=0, s_1=A, a_1=L, r_1=3)$\n- $\\rho_0 = \\rho(A,R) = \\frac{2}{3}$.\n- $\\rho_1 = \\rho(A,L) = \\frac{4}{3}$.\n- $\\delta_0 = r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) = 0 + \\frac{1}{2}\\hat{v}_1(A) - \\hat{q}_0(A,R) = \\frac{1}{2}(\\frac{4}{3}) - 1 = \\frac{2}{3} - 1 = -\\frac{1}{3}$.\n- $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1) = 3 - \\hat{q}_1(A,L) = 3 - 2 = 1$.\n- $\\hat{V}(\\tau_2) = \\hat{v}_0(A) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1 = \\frac{5}{3} + (\\frac{2}{3})(-\\frac{1}{3}) + (\\frac{1}{2})(\\frac{2}{3})(\\frac{4}{3})(1) = \\frac{5}{3} - \\frac{2}{9} + \\frac{4}{9} = \\frac{15}{9} + \\frac{2}{9} = \\frac{17}{9}$.\n\nEpisode 3: $\\tau_3 = (s_0=A, a_0=L, r_0=2, s_1=B, a_1=L, r_1=0)$\n- $\\rho_0 = \\rho(A,L) = \\frac{4}{3}$.\n- $\\rho_1 = \\rho(B,L) = \\frac{3}{2}$.\n- $\\delta_0 = r_0 + \\gamma \\hat{v}_1(s_1) - \\hat{q}_0(s_0,a_0) = 2 + \\frac{1}{2}\\hat{v}_1(B) - \\hat{q}_0(A,L) = 2 + \\frac{1}{2}(2) - 2 = 1$.\n- $\\delta_1 = r_1 - \\hat{q}_1(s_1,a_1) = 0 - \\hat{q}_1(B,L) = 0 - 1 = -1$.\n- $\\hat{V}(\\tau_3) = \\hat{v}_0(A) + \\rho_0 \\delta_0 + \\gamma \\rho_0 \\rho_1 \\delta_1 = \\frac{5}{3} + (\\frac{4}{3})(1) + (\\frac{1}{2})(\\frac{4}{3})(\\frac{3}{2})(-1) = \\frac{5}{3} + \\frac{4}{3} - 1 = \\frac{9}{3} - 1 = 3 - 1 = 2$.\n\nFinally, we average the estimates over the $N=3$ episodes:\n$$\\hat{V}^{\\pi}(A) = \\frac{1}{3}(\\hat{V}(\\tau_1) + \\hat{V}(\\tau_2) + \\hat{V}(\\tau_3)) = \\frac{1}{3}\\left(\\frac{7}{6} + \\frac{17}{9} + 2\\right)$$\nTo sum the fractions, we find a common denominator of $18$:\n$$\\hat{V}^{\\pi}(A) = \\frac{1}{3}\\left(\\frac{7 \\times 3}{18} + \\frac{17 \\times 2}{18} + \\frac{2 \\times 18}{18}\\right) = \\frac{1}{3}\\left(\\frac{21 + 34 + 36}{18}\\right) = \\frac{1}{3}\\left(\\frac{91}{18}\\right) = \\frac{91}{54}$$",
            "answer": "$$\\boxed{\\frac{91}{54}}$$"
        }
    ]
}