{
    "hands_on_practices": [
        {
            "introduction": "Gittins 指数是决策理论中的一项基石性成就，为折扣多臂老虎机问题提供了一个优雅的最优解。此练习将引导您从贝叶斯信念更新和最优停止的首要原则出发，为经典的 Beta-伯努利老虎机严格推导该指数 。通过将问题构建为动态规划，您将对探索的价值如何被数学化地构建和计算获得深刻的基础性理解。",
            "id": "4148030",
            "problem": "考虑一个单臂贝叶斯伯努利赌博机，它在离散时间 $t \\in \\{0,1,2,\\dots\\}$ 被激活。在时间 $t$ 被激活时，该臂产生一个随机奖励 $X_t \\in \\{0,1\\}$，其分布为 $\\mathrm{Bernoulli}(\\theta)$。其中，未知的成功概率 $\\theta \\in [0,1]$ 的先验分布为 $\\mathrm{Beta}(\\alpha,\\beta)$，参数为 $\\alpha>0$ 和 $\\beta>0$。在时间 $t$ 观测到一次成功后，$\\theta$ 的后验分布更新为 $\\mathrm{Beta}(\\alpha+1,\\beta)$；观测到一次失败后，更新为 $\\mathrm{Beta}(\\alpha,\\beta+1)$。未来的奖励以已知的因子 $\\gamma \\in (0,1)$ 进行几何折扣。令 $\\mathcal{F}_t$ 表示由截至时间 $t$（含）观测到的奖励所生成的 $\\sigma$-代数，并令 $\\tau$ 表示关于滤子 $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ 的一个停止时间。你可以假设对于几何折扣的多臂赌博机，Gittins 指数定理是有效的。\n\n从第一性原理出发，包括贝塔-伯努利模型的贝叶斯信念更新和最优停止的定义，推导该臂在信念状态 $\\mathrm{Beta}(\\alpha,\\beta)$ 和折扣因子 $\\gamma$ 下的 Gittins 指数 (GI) 的严格表达式。你的推导过程应通过构建一个比较继续采样与停止的最优停止问题，并将其转化为关于信念状态的动态规划。然后，使用“为继续而补贴”的公式，推导在每步补贴 $\\lambda \\in \\mathbb{R}$ 下定义单臂价值函数的贝尔曼递归，并将 Gittins 指数刻画为使该臂在继续与停止之间恰好无差异的阈值补贴。清晰地陈述并证明你为确保该指数存在且唯一所使用的任何单调性或收缩性质。\n\n最后，作为你的答案，提供一个单一的解析表达式，将信念状态 $\\mathrm{Beta}(\\alpha,\\beta)$ 和折扣因子 $\\gamma$ 下的 Gittins 指数定义为两个折扣期望之比在所有停止时间上的上确界。无需进行数值计算，也无需四舍五入。最终答案中不要包含任何单位。",
            "solution": "我们首先将带有贝叶斯学习和几何折扣的单臂问题形式化。状态是对未知成功概率 $\\theta$ 的信念 $(\\alpha,\\beta)$，其先验和后验分布属于共轭族 $\\mathrm{Beta}(\\alpha,\\beta)$。当该臂被激活时，它产生奖励 $X_t \\in \\{0,1\\}$，其条件分布为 $X_t \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$。对于贝塔-伯努利模型，后验更新规则是一个经过充分检验的事实：如果当前信念为 $\\mathrm{Beta}(\\alpha,\\beta)$，观测到一次成功会将信念更新为 $\\mathrm{Beta}(\\alpha+1,\\beta)$，一次失败则更新为 $\\mathrm{Beta}(\\alpha,\\beta+1)$。在信念 $(\\alpha,\\beta)$ 下的后验均值为 $\\mu(\\alpha,\\beta) = \\alpha/(\\alpha+\\beta)$。\n\n对于几何折扣的多臂赌博机，Gittins 指数将最优分配策略分解为一种逐臂指数最大化策略。对于单臂，其在某个信念状态下的 Gittins 指数是一个相关的一维最优停止问题的值，我们现在从第一性原理出发构建该问题。\n\n定义一个停止时间 $\\tau$（关于由观测奖励生成的滤子 $\\{\\mathcal{F}_t\\}_{t \\ge 0}$）作为我们停止对该臂进行采样的时间。在折扣因子为 $\\gamma \\in (0,1)$ 的几何折扣下，从信念 $(\\alpha,\\beta)$ 开始，采样至时间 $\\tau$（不含）所获得的期望折扣奖励为\n$$\nR(\\alpha,\\beta;\\tau) \\equiv \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} X_t \\right],\n$$\n其中 $\\mathbb{E}_{\\alpha,\\beta}[\\cdot]$ 表示关于由先验 $\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)$ 和在停止规则 $\\tau$ 下的条件抽取 $X_t \\mid \\theta$ 所导出的联合律的期望。累积至 $\\tau$ 的折扣时间为\n$$\nT(\\tau) \\equiv \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\right].\n$$\n$R(\\alpha,\\beta;\\tau)$ 和 $T(\\tau)$ 都是有限的，因为 $\\gamma \\in (0,1)$ 意味着 $\\sum_{t=0}^{\\infty} \\gamma^t  \\infty$，并且奖励有界于 $[0,1]$。\n\nGittins 指数背后的最优停止问题表现为在所有容许的停止时间上最大化期望折扣奖励与期望折扣时间之比：\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) \\equiv \\sup_{\\tau \\in \\mathcal{T}} \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)},\n$$\n其中 $\\mathcal{T}$ 是所有关于 $\\{\\mathcal{F}_t\\}$ 且取值于 $\\{1,2,\\dots\\}$ 的停止时间的集合。这种比率解释可以通过考虑拉格朗日或补贴公式得到，我们现在推导它以产生动态规划递归。\n\n为继续采样该臂引入一个每期补贴 $\\lambda \\in \\mathbb{R}$，并将补贴 $\\lambda$ 下的价值函数定义为最大期望折扣净奖励：\n$$\nV^{\\lambda}(\\alpha,\\beta) \\equiv \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\big( X_t - \\lambda \\big) \\right].\n$$\n这是由贝塔-伯努利更新所引出的信念状态马尔可夫链上的一个最优停止问题的值。信念上的过程是一个在可数状态空间 $\\{(\\alpha+i,\\beta+j): i,j \\in \\mathbb{N}_0\\}$ 上的时齐马尔可夫链，且折扣因子 $\\gamma \\in (0,1)$ 的几何折扣确保了 $V^{\\lambda}$ 的贝尔曼方程是一个收缩映射。\n\n根据最优性原理，在信念 $(\\alpha,\\beta)$ 处，贝尔曼递归为\n$$\nV^{\\lambda}(\\alpha,\\beta)\n=\n\\max \\Big\\{ 0,\\, \\underbrace{\\mu(\\alpha,\\beta) - \\lambda}_{\\text{net immediate reward}} + \\gamma \\Big[ \\mu(\\alpha,\\beta)\\, V^{\\lambda}(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, V^{\\lambda}(\\alpha,\\beta+1) \\Big] \\Big\\},\n$$\n其中 $\\mu(\\alpha,\\beta) = \\alpha/(\\alpha+\\beta)$。最大值内的两项分别对应立即停止（产生 0 的继续价值，因为我们将停止后的价值定义为 0）和再玩一次（收集期望即时净奖励 $\\mu(\\alpha,\\beta)-\\lambda$ 加上折扣后的期望继续价值，其中下一个信念以概率 $\\mu(\\alpha,\\beta)$ 变为 $(\\alpha+1,\\beta)$，以概率 $1-\\mu(\\alpha,\\beta)$ 变为 $(\\alpha,\\beta+1)$）。\n\n对于有界奖励、时齐马尔可夫决策过程（Markov Decision Process (MDP)）中的折扣最优停止问题的标准论证表明，对于每个固定的 $\\lambda$，算子\n$$\n\\mathcal{T}^{\\lambda}[f](\\alpha,\\beta)\n=\n\\max \\Big\\{ 0,\\, \\mu(\\alpha,\\beta) - \\lambda + \\gamma \\big[ \\mu(\\alpha,\\beta)\\, f(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, f(\\alpha,\\beta+1) \\big] \\Big\\}\n$$\n是在配备了上确界范数的有界函数空间上的一个 $\\gamma$-收缩。因此，贝尔曼方程有一个唯一的有界不动点 $V^{\\lambda}$，并且价值迭代会收敛到它。\n\n我们现在将 $V^{\\lambda}$ 与比率刻画联系起来。对于任何停止时间 $\\tau$，\n$$\n\\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\big( X_t - \\lambda \\big) \\right]\n=\nR(\\alpha,\\beta;\\tau) - \\lambda\\, T(\\tau).\n$$\n根据 $V^{\\lambda}$ 的定义，对所有 $\\tau$ 我们有 $V^{\\lambda}(\\alpha,\\beta) \\ge R(\\alpha,\\beta;\\tau) - \\lambda\\, T(\\tau)$，因此\n$$\n\\lambda \\ge \\frac{R(\\alpha,\\beta;\\tau) - V^{\\lambda}(\\alpha,\\beta)}{T(\\tau)}.\n$$\n当 $V^{\\lambda}(\\alpha,\\beta) = 0$ 时，这得到\n$$\n\\lambda \\ge \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)} \\quad \\text{for all } \\tau \\in \\mathcal{T}.\n$$\n由此可知，如果 $V^{\\lambda}(\\alpha,\\beta) = 0$，那么 $\\lambda$ 是比率 $R(\\alpha,\\beta;\\tau)/T(\\tau)$ 在所有停止时间上的一个上界，所以 $\\lambda \\ge \\sup_{\\tau \\in \\mathcal{T}} R(\\alpha,\\beta;\\tau)/T(\\tau)$。反之，假设 $\\lambda  \\sup_{\\tau \\in \\mathcal{T}} R(\\alpha,\\beta;\\tau)/T(\\tau)$。那么存在一个停止时间 $\\tau^{\\star}$ 使得 $R(\\alpha,\\beta;\\tau^{\\star}) - \\lambda\\, T(\\tau^{\\star})  0$，因此 $V^{\\lambda}(\\alpha,\\beta) \\ge R(\\alpha,\\beta;\\tau^{\\star}) - \\lambda\\, T(\\tau^{\\star})  0$。因此，集合 $\\{\\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta)  0\\}$ 是开区间 $(-\\infty, \\mathcal{G}(\\alpha,\\beta;\\gamma))$，并且对于所有 $\\lambda \\ge \\mathcal{G}(\\alpha,\\beta;\\gamma)$，有 $V^{\\lambda}(\\alpha,\\beta) = 0$。\n\n$V^{\\lambda}$ 关于 $\\lambda$ 的单调性可从贝尔曼算子得出：如果 $\\lambda_1  \\lambda_2$，那么对于任何 $f$，$\\mathcal{T}^{\\lambda_1}[f](\\alpha,\\beta) \\ge \\mathcal{T}^{\\lambda_2}[f](\\alpha,\\beta)$ 逐点成立，且唯一不动点满足 $V^{\\lambda_1}(\\alpha,\\beta) \\ge V^{\\lambda_2}(\\alpha,\\beta)$。结合有界性和关于 $\\lambda$ 的连续性，这些事实意味着存在唯一的阈值 $\\lambda^{\\star}$，使得当 $\\lambda  \\lambda^{\\star}$ 时 $V^{\\lambda}(\\alpha,\\beta)  0$，当 $\\lambda \\ge \\lambda^{\\star}$ 时 $V^{\\lambda}(\\alpha,\\beta) = 0$。这个阈值正是 Gittins 指数，并且它与比率的上确界一致：\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\sup_{\\tau \\in \\mathcal{T}} \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)}.\n$$\n\n为了通过信念状态上的动态规划进行计算，可以固定一个候选补贴 $\\lambda$ 并通过价值迭代计算 $V^{\\lambda}$：\n$$\nV^{\\lambda}_{k+1}(\\alpha,\\beta) \\leftarrow \\max \\Big\\{ 0,\\, \\mu(\\alpha,\\beta) - \\lambda + \\gamma \\big[ \\mu(\\alpha,\\beta)\\, V^{\\lambda}_{k}(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, V^{\\lambda}_{k}(\\alpha,\\beta+1) \\big] \\Big\\},\n$$\n初始化为对所有 $(\\alpha,\\beta)$ 都有 $V^{\\lambda}_{0}(\\alpha,\\beta) \\equiv 0$，该迭代会一致收敛到 $V^{\\lambda}$。根据关于 $\\lambda$ 的单调性和界限 $0 \\le \\mathcal{G}(\\alpha,\\beta;\\gamma) \\le 1$（因为奖励在 $[0,1]$ 内），对 $\\lambda \\in [0,1]$ 进行简单的二分搜索可以得到满足 $V^{\\lambda^{\\star}}(\\alpha,\\beta) = 0$ 的唯一 $\\lambda^{\\star}$，这即是信念 $(\\alpha,\\beta)$ 下的 Gittins 指数。\n\n总结来说，在信念 $\\mathrm{Beta}(\\alpha,\\beta)$ 和折扣因子 $\\gamma$ 下的 Gittins 指数是，在所有适应于观测奖励的停止时间上，期望折扣奖励与期望折扣时间之比的上确界：\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\sup_{\\tau \\in \\mathcal{T}} \\frac{ \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} X_t \\right] }{ \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\right] }.\n$$\n能够进行计算的动态规划递归由补贴形式的贝尔曼方程给出\n$$\nV^{\\lambda}(\\alpha,\\beta) = \\max \\Big\\{ 0,\\, \\frac{\\alpha}{\\alpha+\\beta} - \\lambda + \\gamma \\Big[ \\frac{\\alpha}{\\alpha+\\beta}\\, V^{\\lambda}(\\alpha+1,\\beta) + \\frac{\\beta}{\\alpha+\\beta}\\, V^{\\lambda}(\\alpha,\\beta+1) \\Big] \\Big\\},\n$$\n且 Gittins 指数是\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\inf \\big\\{ \\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta) = 0 \\big\\} = \\sup \\big\\{ \\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta)  0 \\big\\}.\n$$\n这些等价的刻画中的任何一个都定义了同一个唯一的实数值 $\\mathcal{G}(\\alpha,\\beta;\\gamma)$。",
            "answer": "$$\\boxed{\\displaystyle \\mathcal{G}(\\alpha,\\beta;\\gamma)=\\sup_{\\tau \\in \\mathcal{T}}\\,\\frac{\\mathbb{E}_{\\alpha,\\beta}\\!\\left[\\sum_{t=0}^{\\tau-1}\\gamma^{t} X_t\\right]}{\\mathbb{E}_{\\alpha,\\beta}\\!\\left[\\sum_{t=0}^{\\tau-1}\\gamma^{t}\\right]}}$$"
        },
        {
            "introduction": "在推导出 Gittins 指数的理论形式之后，建立对其真正含义的直观理解至关重要：即信息的价值。此练习  呈现了一个简化但富有洞察力的场景，您可以直接比较最优指数策略与朴素的短视策略。通过量化因未能探索而导致的期望损失，您将领会到指数如何正确定价发现高回报状态的“期权价值”。",
            "id": "4148037",
            "problem": "考虑一个以几何折扣的多臂老虎机（MAB）为框架的双臂决策问题，该问题模拟了复杂自适应系统中的探索与利用之间的权衡。时间是离散的，未来的奖励以一个因子 $\\,\\gamma \\in (0,1)\\,$ 进行折扣。在每个时间步，您可以选择两个臂中的一个：\n- 臂 $\\,B\\,$ 每次操作产生一个已知的恒定奖励 $\\,b\\,$，永久如此。\n- 臂 $\\,A\\,$ 的类型未知。它有 $\\,p\\,$ 的先验概率为高收益类型，每次操作永久产生 $\\,h\\,$ 的奖励；有 $\\,1-p\\,$ 的先验概率为低收益类型，每次操作永久产生 $\\,\\ell\\,$ 的奖励。操作一次臂 $\\,A\\,$ 会为其后所有时间完全揭示其类型（因为实现的奖励可以区分这两种类型），之后如果有利，您可以继续选择臂 $\\,A\\,$，或者停止并切换到臂 $\\,B\\,$。\n\n使用折扣最优停止的基本原则，将一个臂的 Gittins 指数定义为一个恒定备选流的保留价值 $\\,c^{\\ast}\\,$，该价值使得决策者在“从该臂开始”与“立即选择恒定流”之间无差异，同时假定之后可以选择停止并切换。从这一原则和折扣总和的几何级数出发，为所描述的一步揭示模型推导臂 $\\,A\\,$ 的 Gittins 指数的保留价值表达式。\n\n然后，将问题具体化到参数值 $\\,\\gamma = 0.9\\,$, $\\,p = 0.3\\,$, $\\,h = 10\\,$, $\\,\\ell = 0\\,$ 和 $\\,b = 4\\,$。计算：\n1. 臂 $\\,A\\,$ 的 Gittins 指数 $\\,c^{\\ast}\\,$，并将其与短视的期望即时奖励 $\\,p h + (1-p)\\ell\\,$ 进行比较。\n2. 短视的利用策略（立即并永久选择臂 $\\,B\\,$）相对于最优指数策略（探索一次臂 $\\,A\\,$ 然后以最优方式继续）所产生的期望折扣损失。\n\n将最终损失表示为单个实数。无需四舍五入。",
            "solution": "首先验证问题，以确保其具有科学依据、是良定的且客观的。\n\n**步骤 1：提取已知条件**\n- 具有离散时间的双臂老虎机问题。\n- 折扣因子：$\\gamma \\in (0,1)$。\n- 臂 $B$：每次操作产生一个已知的恒定奖励 $b$。\n- 臂 $A$：类型未知。\n  - 高收益类型：每次操作奖励为 $h$，先验概率为 $p$。\n  - 低收益类型：每次操作奖励为 $\\ell$，先验概率为 $1-p$。\n- 操作一次臂 $A$ 会揭示其类型。\n- Gittins 指数被定义为一个恒定备选项的保留价值 $c^{\\ast}$，它使得智能体在“探索该臂一步”和“立即选择备选项”之间无差异。\n- 具体参数值：$\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, $b = 4$。\n- 需要计算：\n  1. 臂 $A$ 的 Gittins 指数 $c^{\\ast}$ 及其与短视期望奖励的比较。\n  2. 短视策略（总是选择臂 $B$）相对于最优指数策略的期望折扣损失。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题是贝叶斯多臂老虎机问题的标准、良定表述，是复杂自适应系统和强化学习研究中的一个核心课题。Gittins 指数是此类折扣问题的标准解法。该问题是自洽的，所有变量和条件都已明确指定。不存在科学或逻辑上的矛盾、模糊性或主观因素。因此，该问题是有效的。\n\n**步骤 3：结论与行动**\n该问题有效。将提供完整解答。\n\n**臂 $A$ 的 Gittins 指数推导**\n\n臂 $A$ 的 Gittins 指数 $c^{\\ast}$ 定义为一个假设的备选臂的恒定奖励值，该值使得智能体在以下两个初始选择之间无差异：\n1. 选择假设的臂，永久获得奖励 $c^{\\ast}$。总折扣价值为 $V_{C} = \\sum_{t=0}^{\\infty} \\gamma^t c^{\\ast} = \\frac{c^{\\ast}}{1-\\gamma}$。\n2. 选择臂 $A$ 一个时间步，观察结果，然后以最优方式继续。最优的后续行动是选择具有更高永久奖励流的臂：即臂 $A$ 被揭示的价值或保留价值 $c^{\\ast}$。\n\n选择臂 $A$ 一步的价值 $V_{A}$，是期望即时奖励与折扣后的期望未来价值之和。\n在时间 $t=0$ 时拉动臂 $A$ 的期望即时奖励为 $E[R_0] = ph + (1-p)\\ell$。\n\n这次操作之后，臂 $A$ 的类型就已知了。\n- 以概率 $p$，臂 $A$ 是高收益类型（奖励为 $h$）。智能体将比较 $h$ 和 $c^{\\ast}$。最优选择将永久产生 $\\max(h, c^{\\ast})$ 的奖励。从时间 $t=1$ 开始，这个未来流的折扣价值是 $\\frac{\\max(h, c^{\\ast})}{1-\\gamma}$。\n- 以概率 $1-p$，臂 $A$ 是低收益类型（奖励为 $\\ell$）。智能体将比较 $\\ell$ 和 $c^{\\ast}$。最优选择将永久产生 $\\max(\\ell, c^{\\ast})$ 的奖励。从时间 $t=1$ 开始，这个未来流的折扣价值是 $\\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma}$。\n\n策略 2 的总期望折扣价值为：\n$$V_{A} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\frac{\\max(h, c^{\\ast})}{1-\\gamma} + (1-p) \\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma} \\right]$$\n在无差异点，$V_{A} = V_{C}$：\n$$\\frac{c^{\\ast}}{1-\\gamma} = (ph + (1-p)\\ell) + \\frac{\\gamma}{1-\\gamma} \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\n乘以 $(1-\\gamma)$ 得到 Gittins 指数的基本方程：\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\n为了解出 $c^{\\ast}$，我们假设一个非平凡情况，即 $\\ell  c^{\\ast}  h$。这是合理的，因为如果 $c^{\\ast}$ 在这个范围之外，学习后切换的决定就会被放弃（例如，如果 $c^{\\ast} \\le \\ell$，人们将永远不会切换到备选项）。在这个假设下，$\\max(h, c^{\\ast}) = h$ 且 $\\max(\\ell, c^{\\ast}) = c^{\\ast}$。\n将这些代入方程中：\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ ph + (1-p)c^{\\ast} \\right]$$\n现在，我们求解 $c^{\\ast}$：\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma ph - \\gamma(1-p)\\ell + \\gamma ph + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma(1-p)\\ell + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} - \\gamma(1-p)c^{\\ast} = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast}(1 - \\gamma(1-p)) = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast} = \\frac{ph + (1-p)(1-\\gamma)\\ell}{1 - \\gamma + p\\gamma}$$\n这就是臂 $A$ 的 Gittins 指数的保留价值表达式。\n\n**针对具体参数值的计算**\n\n给定参数为 $\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, 和 $b = 4$。\n\n1. **Gittins 指数与短视奖励的比较**\n首先，我们计算臂 $A$ 的 Gittins 指数 $c^{\\ast}$：\n$$c^{\\ast} = \\frac{(0.3)(10) + (1-0.3)(1-0.9)(0)}{1 - 0.9 + (0.3)(0.9)} = \\frac{3 + 0}{1 - 0.9 + 0.27} = \\frac{3}{0.1 + 0.27} = \\frac{3}{0.37} = \\frac{300}{37}$$\nGittins 指数的值是 $c^{\\ast} = \\frac{300}{37} \\approx 8.108$。\n短视的期望即时奖励为：\n$$E_{myopic} = ph + (1-p)\\ell = (0.3)(10) + (0.7)(0) = 3$$\n比较两者，$c^{\\ast} = \\frac{300}{37} > 3$。差值 $c^{\\ast} - E_{myopic}$ 代表了“信息价值”或期权价值，该价值内在于当臂 $A$ 被发现是低收益类型时能够切换走的能力。\n\n2. **短视策略的期望折扣损失**\n最优策略由 Gittins 指数规则决定：在任何阶段，选择指数最高的臂。\n臂 $A$ 的指数为 $c^{\\ast} = \\frac{300}{37}$。\n臂 $B$ 的指数是其恒定奖励 $b = 4$。\n由于 $c^{\\ast} = \\frac{300}{37} \\approx 8.108  4 = b$，最优策略是首先探索臂 $A$。\n\n问题将“短视的利用策略”定义为立即并永久选择臂 $B$。我们来计算该策略的期望总折扣价值 $V_{myopic}$。\n$$V_{myopic} = \\sum_{t=0}^{\\infty} \\gamma^t b = \\frac{b}{1-\\gamma} = \\frac{4}{1-0.9} = \\frac{4}{0.1} = 40$$\n\n现在，我们计算最优策略的期望总折扣价值 $V_{optimal}$。该策略从操作臂 $A$ 开始。\n- 即时奖励为 $ph + (1-p)\\ell = 3$。\n- 操作之后，我们重新评估。\n  - 以概率 $p=0.3$，臂 $A$ 是高收益类型（$h=10$）。由于 $h > b$，我们继续选择臂 $A$。从 $t=1$ 开始的未来折扣价值为 $\\frac{h}{1-\\gamma} = \\frac{10}{0.1} = 100$。\n  - 以概率 $1-p=0.7$，臂 $A$ 是低收益类型（$\\ell=0$）。由于 $\\ell  b$，我们切换到臂 $B$。从 $t=1$ 开始的未来折扣价值为 $\\frac{b}{1-\\gamma} = \\frac{4}{0.1} = 40$。\n\n最优策略的总期望价值为：\n$$V_{optimal} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\left( \\frac{h}{1-\\gamma} \\right) + (1-p) \\left( \\frac{b}{1-\\gamma} \\right) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ (0.3)(100) + (0.7)(40) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ 30 + 28 \\right]$$\n$$V_{optimal} = 3 + 0.9(58)$$\n$$V_{optimal} = 3 + 52.2 = 55.2$$\n\n短视策略所产生的期望折扣损失是最优策略与短视策略之间的价值差异。\n$$\\text{损失} = V_{optimal} - V_{myopic} = 55.2 - 40 = 15.2$$\n这个损失代表了因未能探索可能回报非常高的臂 $A$ 而放弃的期望价值。以分数形式表示，该损失为 $15.2 = \\frac{152}{10} = \\frac{76}{5}$。",
            "answer": "$$\n\\boxed{15.2}\n$$"
        },
        {
            "introduction": "虽然 Gittins 指数是最优的，但在复杂的高维问题中其计算通常是难以处理的。此练习  将我们带到现代贝叶斯优化的前沿，用高斯过程对未知的奖励函数进行建模。您将分析一个对比两种强大算法——GP-UCB 和汤普森采样——的场景，并揭示 UCB 的“面对不确定性时的乐观主义”原则有时如何导致次优的过度探索，从而使您能够量化由此产生的性能差异。",
            "id": "4148010",
            "problem": "考虑一个复杂自适应系统，该系统被建模为一个在有限决策集 $\\mathcal{X} = \\{x_{1}, x_{2}, \\dots, x_{N}\\}$ 上的随机优化问题，其中未知的奖励函数 $f: \\mathcal{X} \\to \\mathbb{R}$ 被建模为一个具有独立坐标的高斯过程先验：对于每个 $x_{i}$，先验边缘分布为 $f(x_{i}) \\sim \\mathcal{N}(0, s^{2})$，且各坐标是独立的。观测是无噪声的。将决策序列 $(x_{t})_{t \\geq 1}$ 的累积遗憾定义为 $R_{T} := \\sum_{t=1}^{T} \\big(f(x^{\\star}) - f(x_{t})\\big)$，其中 $x^{\\star} \\in \\mathcal{X}$ 是 $f$ 的一个最大化子。\n\n假设在时间 $t=0$ 的一次初步试点观测揭示了在特定设计点 $x^{\\star}$ 处的奖励，为 $f(x^{\\star}) = \\Delta$（对于某个已知的 $\\Delta  0$），且所有其他未采样点保持其先验分布 $\\mathcal{N}(0, s^{2})$。因此，在时间 $t=1$ 时，$x^{\\star}$ 处的后验均值为 $m_{1}(x^{\\star}) = \\Delta$，方差为 $\\sigma_{1}(x^{\\star}) = 0$，而对于每个 $x \\neq x^{\\star}$，后验均值为 $m_{1}(x) = 0$，方差为 $\\sigma_{1}(x) = s$。\n\n在时间 $t=1$ 时：\n- 高斯过程上置信界（GP-UCB）算法，正式名称为 Gaussian Process Upper Confidence Bound (GP-UCB)，对于给定的探索参数 $\\beta_{1}  0$，选择 $x_{1}^{\\mathrm{UCB}} \\in \\arg\\max_{x \\in \\mathcal{X}} \\big(m_{1}(x) + \\sqrt{\\beta_{1}} \\, \\sigma_{1}(x)\\big)$。\n- 函数上的汤普森采样，正式名称为 Thompson Sampling (TS)，从当前关于 $f$ 的后验中抽取一个样本函数 $g$（因此 $g(x^{\\star}) = \\Delta$ 是确定性的，而对于未采样的 $x$，$g(x) \\sim \\mathcal{N}(0, s^{2})$ 且相互独立），并选择 $x_{1}^{\\mathrm{TS}} \\in \\arg\\max_{x \\in \\mathcal{X}} g(x)$。\n\n假设在时间 $t=1$ 时，系统对于 GP-UCB 处于过度探索状态，即 $\\sqrt{\\beta_{1}} \\, s  \\Delta$，使得 GP-UCB 严格偏好任何未采样的 $x \\neq x^{\\star}$ 而非 $x^{\\star}$。将在时间 $t=1$ 时 GP-UCB 相对于 TS 的瞬时遗憾惩罚定义为它们在时间 $t=1$ 的期望瞬时遗憾之差，即\n$$\\Pi := \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{UCB}})\\big] - \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{TS}})\\big].$$\n\n推导 $\\Pi$ 的一个闭式解析表达式，用 $\\Delta$、$s$、$N$ 和标准正态累积分布函数 $\\Phi(\\cdot)$ 表示。您的最终答案必须是单一的解析表达式。不需要数值近似。",
            "solution": "目标是推导在时间 $t=1$ 时瞬时遗憾惩罚 $\\Pi$ 的闭式表达式。该惩罚定义为高斯过程上置信界（GP-UCB）算法和汤普森采样（TS）之间的期望瞬时遗憾之差。\n\n惩罚 $\\Pi$ 由下式给出\n$$ \\Pi := \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{UCB}})\\big] - \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{TS}})\\big] $$\n期望 $\\mathbb{E}[\\cdot]$ 是对所有随机性来源求取的。在此问题中，随机性来源于未知的真实函数值 $f(x)$（对于 $x \\neq x^{\\star}$）和汤普森采样抽取的函数样本 $g(x)$。\n鉴于 $f(x^{\\star})=\\Delta$ 是一个已知常数，我们可以利用期望的线性性质：\n$$ \\Pi = \\big(\\mathbb{E}[f(x^{\\star})] - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})]\\big) - \\big(\\mathbb{E}[f(x^{\\star})] - \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})]\\big) $$\n$$ \\Pi = (\\Delta - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})]) - (\\Delta - \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})]) $$\n$$ \\Pi = \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] $$\n为了求得 $\\Pi$，我们必须计算在时间 $t=1$ 时每个算法的期望奖励。\n\n首先，我们分析 GP-UCB 的选择 $x_{1}^{\\mathrm{UCB}}$。\n该算法从 $\\mathcal{X}$ 中选择一个动作，以最大化 UCB 分数 $m_{1}(x) + \\sqrt{\\beta_{1}} \\sigma_{1}(x)$。\n对于点 $x^{\\star}$，后验均值为 $m_{1}(x^{\\star}) = \\Delta$，后验标准差为 $\\sigma_{1}(x^{\\star}) = 0$。其 UCB 分数为：\n$$ \\text{UCB}(x^{\\star}) = \\Delta + \\sqrt{\\beta_{1}} \\cdot 0 = \\Delta $$\n对于任何其他点 $x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$，后验均值为 $m_{1}(x) = 0$，后验标准差为 $\\sigma_{1}(x) = s$。其 UCB 分数为：\n$$ \\text{UCB}(x) = 0 + \\sqrt{\\beta_{1}} \\cdot s = \\sqrt{\\beta_{1}} s $$\n问题指出系统处于过度探索状态，由条件 $\\sqrt{\\beta_{1}} s  \\Delta$ 定义。这意味着任何未采样点 $x \\neq x^{\\star}$ 的 UCB 分数都严格大于 $x^{\\star}$ 的分数。\n因此，GP-UCB 将会选择 $N-1$ 个未采样点之一。由于所有这些点具有相同的 UCB 分数，算法必须打破平局。假设采用标准的均匀随机打破平局规则，$x_{1}^{\\mathrm{UCB}}$ 从集合 $\\mathcal{X} \\setminus \\{x^{\\star}\\}$ 中均匀选择。\nGP-UCB 的期望奖励为 $\\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})]$。我们使用全期望定律，以所选动作为条件来计算这个值：\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] = \\sum_{x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}} \\mathbb{E}[f(x)] P(x_{1}^{\\mathrm{UCB}} = x) $$\n选择任何特定 $x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$ 的概率是 $P(x_{1}^{\\mathrm{UCB}} = x) = \\frac{1}{N-1}$。\n对于任何未采样点，真实奖励 $\\mathbb{E}[f(x)]$ 的期望值是其分布的均值。问题指出，对于 $x \\neq x^{\\star}$，$f(x)$ 的后验是 $\\mathcal{N}(0, s^{2})$。因此，$\\mathbb{E}[f(x)] = 0$。\n将这些代入期望奖励的表达式中：\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] = \\sum_{x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}} (0) \\cdot \\left(\\frac{1}{N-1}\\right) = 0 $$\n\n接下来，我们分析汤普森采样的选择 $x_{1}^{\\mathrm{TS}}$。\nTS 首先从关于 $f$ 的后验分布中抽取一个样本函数 $g$。在 $t=1$ 时，这意味着：\n- $g(x^{\\star}) = \\Delta$（一个确定性值，因为 $x^{\\star}$ 处的后验是一个点质量）。\n- 对于每个 $x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$，$g(x)$ 从 $\\mathcal{N}(0, s^{2})$ 中独立抽取。\n令这 $N-1$ 个采样值为 $g_{i} \\sim \\mathcal{N}(0, s^{2})$，独立同分布 (i.i.d.)。\nTS 接着选择使这个样本函数最大化的动作：\n$$ x_{1}^{\\mathrm{TS}} = \\arg\\max_{x \\in \\mathcal{X}} g(x) = \\arg\\max \\left\\{ \\Delta, g_{1}, g_{2}, \\dots, g_{N-1} \\right \\} $$\n期望奖励为 $\\mathbb{E}[f(x_{1}^{\\mathrm{TS}})]$。期望是针对真实函数 $f$ 和样本函数 $g$ 中的随机性计算的。我们使用全期望定律，以 TS 采样过程 $g$ 的结果为条件：\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] = \\mathbb{E}_{g} \\left[ \\mathbb{E}_{f} \\left[ f(x_{1}^{\\mathrm{TS}}(g)) \\mid g \\right] \\right] $$\n选择 $x_{1}^{\\mathrm{TS}}(g)$ 仅依赖于 $g$。让我们分析内部期望 $\\mathbb{E}_{f}[\\cdot \\mid g]$。\n- 如果 $x_{1}^{\\mathrm{TS}}(g) = x^{\\star}$，那么 $f(x_{1}^{\\mathrm{TS}}(g)) = f(x^{\\star}) = \\Delta$。期望为 $\\mathbb{E}_{f}[\\Delta] = \\Delta$。\n- 如果 $x_{1}^{\\mathrm{TS}}(g) = x_{i}$（对于某个 $x_{i} \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$），那么 $f(x_{1}^{\\mathrm{TS}}(g)) = f(x_{i})$。真实函数值 $\\{f(x_i)\\}$是随机变量，独立于样本函数值 $\\{g_i\\}$。期望为 $\\mathbb{E}_{f}[f(x_i)] = 0$，因为 $f(x_i) \\sim \\mathcal{N}(0, s^{2})$。\n所以，内部期望简化为：\n$$ \\mathbb{E}_{f} \\left[ f(x_{1}^{\\mathrm{TS}}(g)) \\mid g \\right] = \\Delta \\cdot \\mathbf{1}_{\\{x_{1}^{\\mathrm{TS}}(g) = x^{\\star}\\}} $$\n其中 $\\mathbf{1}_{\\{\\cdot\\}}$ 是指示函数。\n现在我们对 $g$ 求外部期望：\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] = \\mathbb{E}_{g}\\left[ \\Delta \\cdot \\mathbf{1}_{\\{x_{1}^{\\mathrm{TS}}(g) = x^{\\star}\\}} \\right] = \\Delta \\cdot P_{g}(x_{1}^{\\mathrm{TS}}(g) = x^{\\star}) $$\n事件 $x_{1}^{\\mathrm{TS}}(g) = x^{\\star}$ 发生当且仅当 $g(x^{\\star})$ 是所有采样值中的最大值。这意味着对于所有 $i=1, \\dots, N-1$，有 $\\Delta  g_{i}$。\n$$ P(x_{1}^{\\mathrm{TS}} = x^{\\star}) = P(g_{1}  \\Delta, g_{2}  \\Delta, \\dots, g_{N-1}  \\Delta) $$\n由于 $g_{i}$ 是从 $\\mathcal{N}(0, s^{2})$ 中抽取的独立同分布样本，这个概率是：\n$$ P(x_{1}^{\\mathrm{TS}} = x^{\\star}) = \\left( P(g_{1}  \\Delta) \\right)^{N-1} $$\n为了求得 $P(g_{1}  \\Delta)$，我们将随机变量 $g_{1}$ 标准化。令 $Z \\sim \\mathcal{N}(0, 1)$。则 $g_{1} = sZ$。\n$$ P(g_{1}  \\Delta) = P(sZ  \\Delta) = P\\left(Z  \\frac{\\Delta}{s}\\right) $$\n这个概率由标准正态分布的累积分布函数（CDF）给出，记为 $\\Phi(\\cdot)$。所以，$P(g_{1}  \\Delta) = \\Phi(\\Delta/s)$。\n因此，TS 选择最优动作的概率是：\n$$ P(x_{1}^{\\mathrm{TS}} = x^{\\star}) = \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} $$\nTS 的期望奖励是：\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] = \\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} $$\n最后，我们将期望奖励代回到 $\\Pi$ 的表达式中：\n$$ \\Pi = \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] = \\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} - 0 $$\n$$ \\Pi = \\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} $$\n这个表达式代表了惩罚，它是在指定的过度探索条件下，GP-UCB 的期望遗憾超出 TS 期望遗憾的量。",
            "answer": "$$\\boxed{\\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1}}$$"
        }
    ]
}