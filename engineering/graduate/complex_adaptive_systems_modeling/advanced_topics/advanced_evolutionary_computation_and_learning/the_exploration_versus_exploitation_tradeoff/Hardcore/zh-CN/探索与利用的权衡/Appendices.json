{
    "hands_on_practices": [
        {
            "introduction": "理论虽然为探索与利用的权衡提供了深刻的见解，但具体的计算练习能极大地加深我们对核心概念的直观理解。通过在一个简化的多臂老虎机场景中计算吉廷斯指数（Gittins index），并量化短视策略所导致的预期损失，我们可以亲身体会到“信息价值”这一抽象概念的具体经济意义 。这个练习旨在将吉廷斯指数从一个纯理论构造，转变为一个可计算、可比较的决策工具。",
            "id": "4148037",
            "problem": "考虑一个双臂决策问题，该问题被构建为一个带有几何折扣的多臂赌博机（MAB）模型，用于模拟复杂自适应系统中的探索与利用权衡。时间是离散的，未来奖励以一个因子 $\\,\\gamma \\in (0,1)\\,$ 进行折扣。在每个时间步，您可以选择两个臂中的一个：\n- 臂 $\\,B\\,$ 每次操作产生一个已知的恒定奖励 $\\,b\\,$，永久如此。\n- 臂 $\\,A\\,$ 的类型未知。它有 $\\,p\\,$ 的先验概率为高类型，每次操作永久产生奖励 $\\,h\\,$；有 $\\,1-p\\,$ 的先验概率为低类型，每次操作永久产生奖励 $\\,\\ell\\,$。拉动一次臂 $\\,A\\,$ 将为其后所有时间完美地揭示其类型（因为实现的奖励可以区分这两种类型），之后如果有利可图，您可以继续选择臂 $\\,A\\,$，或者停止并切换到臂 $\\,B\\,$。\n\n使用折扣最优停止的基本原理，将一个臂的 Gittins 指数定义为一个恒定备选流的保留价值 $\\,c^{\\ast}\\,$，该价值使得决策者在“开始时选择该臂”与“立即选择该恒定流”之间无差异，并且之后有停止并切换的选项。从这一原理和用于计算折扣和的几何级数出发，为所描述的单步揭示模型推导臂 $\\,A\\,$ 的 Gittins 指数的保留价值表达式。\n\n然后，将参数具体化为 $\\,\\gamma = 0.9\\,$, $\\,p = 0.3\\,$, $\\,h = 10\\,$, $\\,\\ell = 0\\,$ 和 $\\,b = 4\\,$。计算：\n1. 臂 $\\,A\\,$ 的 Gittins 指数 $\\,c^{\\ast}\\,$，并将其与短视期望即时奖励 $\\,p h + (1-p)\\ell\\,$ 进行比较。\n2. 短视利用策略（立即并永久选择臂 $\\,B\\,$）相对于最优指数策略（探索臂 $\\,A\\,$ 一次然后以最优方式继续）所产生的期望折扣损失。\n\n将最终损失表示为单个实数。无需四舍五入。",
            "solution": "首先验证问题，以确保其具有科学依据、良构且客观。\n\n**第1步：提取已知条件**\n- 具有离散时间的双臂赌博机问题。\n- 折扣因子：$\\gamma \\in (0,1)$。\n- 臂 $B$：每次操作产生已知的恒定奖励 $b$。\n- 臂 $A$：类型未知。\n  - 高类型：每次操作奖励为 $h$，先验概率为 $p$。\n  - 低类型：每次操作奖励为 $\\ell$，先验概率为 $1-p$。\n- 拉动一次臂 $A$ 会揭示其类型。\n- Gittins 指数被定义为一个恒定备选项的保留价值 $c^{\\ast}$，该价值使得智能体在“探索该臂一步”与“立即选择备选项”之间无差异。\n- 具体参数值：$\\gamma = 0.9$，$p = 0.3$，$h = 10$，$\\ell = 0$，$b = 4$。\n- 需要计算：\n  1. 臂 $A$ 的 Gittins 指数 $c^{\\ast}$ 及其与短视期望奖励的比较。\n  2. 短视策略（总是选择臂 $B$）相对于最优指数策略的期望折扣损失。\n\n**第2步：使用提取的已知条件进行验证**\n该问题是贝叶斯多臂赌博机问题的标准、良构的表述，这是复杂自适应系统和强化学习研究中的一个核心主题。Gittins 指数是解决此类折扣问题的经典方法。该问题是自洽的，所有变量和条件都已明确指定。不存在科学或逻辑上的矛盾、模糊性或主观因素。因此，该问题是有效的。\n\n**第3步：结论与行动**\n问题有效。将提供完整解答。\n\n**臂 $A$ 的 Gittins 指数推导**\n\n臂 $A$ 的 Gittins 指数 $c^{\\ast}$ 被定义为一个假设的备选臂的恒定奖励值，该值使得智能体在以下两种初始选择之间无差异：\n1. 选择假设的臂，永久获得奖励 $c^{\\ast}$。总折扣价值为 $V_{C} = \\sum_{t=0}^{\\infty} \\gamma^t c^{\\ast} = \\frac{c^{\\ast}}{1-\\gamma}$。\n2. 选择臂 $A$ 一个时间步，观察结果，然后以最优方式继续。最优的后续行动是选择具有更高永久奖励流的臂：即臂 $A$ 的揭示值或保留价值 $c^{\\ast}$。\n\n选择臂 $A$ 一步的价值 $V_{A}$ 是期望即时奖励与折扣期望未来价值之和。\n在时间 $t=0$ 拉动臂 $A$ 的期望即时奖励是 $E[R_0] = ph + (1-p)\\ell$。\n\n这次拉动之后，臂 $A$ 的类型就已知了。\n- 以概率 $p$，臂 $A$ 是高类型（奖励 $h$）。智能体将比较 $h$ 和 $c^{\\ast}$。最优选择将永久产生 $\\max(h, c^{\\ast})$ 的奖励。从时间 $t=1$ 开始，这个未来流的折扣价值是 $\\frac{\\max(h, c^{\\ast})}{1-\\gamma}$。\n- 以概率 $1-p$，臂 $A$ 是低类型（奖励 $\\ell$）。智能体将比较 $\\ell$ 和 $c^{\\ast}$。最优选择将永久产生 $\\max(\\ell, c^{\\ast})$ 的奖励。从时间 $t=1$ 开始，这个未来流的折扣价值是 $\\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma}$。\n\n策略2的总期望折扣价值是：\n$$V_{A} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\frac{\\max(h, c^{\\ast})}{1-\\gamma} + (1-p) \\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma} \\right]$$\n在无差异点上，$V_{A} = V_{C}$：\n$$\\frac{c^{\\ast}}{1-\\gamma} = (ph + (1-p)\\ell) + \\frac{\\gamma}{1-\\gamma} \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\n两边乘以 $(1-\\gamma)$ 得到 Gittins 指数的基本方程：\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\n为了求解 $c^{\\ast}$，我们假设一个非平凡情况，即 $\\ell  c^{\\ast}  h$。这是合理的，因为如果 $c^{\\ast}$ 在此范围之外，学习后切换的决定就会被放弃（例如，如果 $c^{\\ast} \\le \\ell$，人们将永远不会切换到备选项）。在此假设下，$\\max(h, c^{\\ast}) = h$ 且 $\\max(\\ell, c^{\\ast}) = c^{\\ast}$。\n将这些代入方程：\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ ph + (1-p)c^{\\ast} \\right]$$\n现在，我们求解 $c^{\\ast}$：\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma ph - \\gamma(1-p)\\ell + \\gamma ph + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma(1-p)\\ell + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} - \\gamma(1-p)c^{\\ast} = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast}(1 - \\gamma(1-p)) = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast} = \\frac{ph + (1-p)(1-\\gamma)\\ell}{1 - \\gamma + p\\gamma}$$\n这就是臂 $A$ 的 Gittins 指数的保留价值表达式。\n\n**具体参数值的计算**\n\n给定参数为 $\\gamma = 0.9$，$p = 0.3$，$h = 10$，$\\ell = 0$ 和 $b = 4$。\n\n1. **Gittins 指数与短视奖励的比较**\n首先，我们计算臂 $A$ 的 Gittins 指数 $c^{\\ast}$：\n$$c^{\\ast} = \\frac{(0.3)(10) + (1-0.3)(1-0.9)(0)}{1 - 0.9 + (0.3)(0.9)} = \\frac{3 + 0}{1 - 0.9 + 0.27} = \\frac{3}{0.1 + 0.27} = \\frac{3}{0.37} = \\frac{300}{37}$$\nGittins 指数的值为 $c^{\\ast} = \\frac{300}{37} \\approx 8.108$。\n短视期望即时奖励为：\n$$E_{myopic} = ph + (1-p)\\ell = (0.3)(10) + (0.7)(0) = 3$$\n比较两者，$c^{\\ast} = \\frac{300}{37} > 3$。差值 $c^{\\ast} - E_{myopic}$ 代表了“信息价值”或期权价值，该价值内在于当臂 $A$ 被发现是低类型时能够切换走的能力。\n\n2. **短视策略的期望折扣损失**\n最优策略由 Gittins 指数规则决定：在任何阶段，选择指数最高的臂。\n臂 $A$ 的指数是 $c^{\\ast} = \\frac{300}{37}$。\n臂 $B$ 的指数是其恒定奖励 $b = 4$。\n由于 $c^{\\ast} = \\frac{300}{37} \\approx 8.108 > 4 = b$，最优策略是首先探索臂 $A$。\n\n问题将“短视利用策略”定义为立即并永久选择臂 $B$。让我们计算该策略的期望总折扣价值 $V_{myopic}$。\n$$V_{myopic} = \\sum_{t=0}^{\\infty} \\gamma^t b = \\frac{b}{1-\\gamma} = \\frac{4}{1-0.9} = \\frac{4}{0.1} = 40$$\n\n现在，我们计算最优策略的期望总折扣价值 $V_{optimal}$。该策略从拉动臂 $A$ 开始。\n- 即时奖励是 $ph + (1-p)\\ell = 3$。\n- 拉动后，我们重新评估。\n  - 以概率 $p=0.3$，臂 $A$ 是高类型 ($h=10$)。由于 $h > b$，我们继续选择臂 $A$。从 $t=1$ 开始的未来折扣价值是 $\\frac{h}{1-\\gamma} = \\frac{10}{0.1} = 100$。\n  - 以概率 $1-p=0.7$，臂 $A$ 是低类型 ($\\ell=0$)。由于 $\\ell  b$，我们切换到臂 $B$。从 $t=1$ 开始的未来折扣价值是 $\\frac{b}{1-\\gamma} = \\frac{4}{0.1} = 40$。\n\n最优策略的总期望价值为：\n$$V_{optimal} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\left( \\frac{h}{1-\\gamma} \\right) + (1-p) \\left( \\frac{b}{1-\\gamma} \\right) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ (0.3)(100) + (0.7)(40) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ 30 + 28 \\right]$$\n$$V_{optimal} = 3 + 0.9(58)$$\n$$V_{optimal} = 3 + 52.2 = 55.2$$\n\n短视策略所产生的期望折扣损失是最优策略与短视策略之间的价值差异。\n$$\\text{Loss} = V_{optimal} - V_{myopic} = 55.2 - 40 = 15.2$$\n这个损失代表了因未能探索可能回报非常高的臂 $A$ 而放弃的期望价值。以分数形式表示，损失为 $15.2 = \\frac{152}{10} = \\frac{76}{5}$。",
            "answer": "$$\n\\boxed{15.2}\n$$"
        },
        {
            "introduction": "在许多现实世界的复杂系统中，计算精确的最优策略（如吉廷斯指数）是不可行的，因此我们需要依赖于高效的启发式算法。本练习通过一个精心设计的场景，对比了两种不同的探索哲学：基于“不确定性下的乐观主义”的置信上界（UCB）算法和纯粹追求信息增益的熵导向策略 。通过计算和比较这两种策略在单步决策中的预期遗憾，我们能更深刻地理解，有效的探索并不仅仅是降低不确定性，而是有策略地探索那些最有可能成为最优选项的决策。",
            "id": "4147968",
            "problem": "考虑一个双臂伯努利多臂老虎机问题，它为一个复杂自适应系统的子系统建模。臂 $1$ 和臂 $2$ 产生独立的伯努利奖励，其未知的成功概率分别为 $\\theta_1$ 和 $\\theta_2$。在初始数据收集阶段之后，基于 $n_1 = 100$ 和 $n_2 = 100$ 次观测，经验观察到的样本均值分别为 $\\hat{p}_1 = 0.9$ 和 $\\hat{p}_2 = 0.5$。为了评估下一步决策的期望悔憾，假设环境是平稳的，并且真实的期望奖励等于观察到的样本均值，即 $\\mu_1 = \\theta_1 = 0.9$ 和 $\\mu_2 = \\theta_2 = 0.5$。\n\n在时间 $t = 200$ 时，考虑使用两种决策策略来选择下一个要拉的臂：\n\n1. 一种寻求熵的策略，选择具有最大预测结果熵 $H_i$ 的臂 $i$，其中 $H_i = - \\hat{p}_i \\ln \\hat{p}_i - (1 - \\hat{p}_i) \\ln(1 - \\hat{p}_i)$（使用自然对数，因此熵的单位是纳特）。\n\n2. 一种上置信界（UCB）策略，选择使指数\n$$\n\\mathrm{UCB}_i(t) = \\hat{p}_i + \\sqrt{\\frac{2 \\ln t}{n_i}}.\n$$\n最大化的臂 $i$。\n\n将选择臂 $i$ 的单步期望悔憾定义为 $r_i = \\mu^\\star - \\mu_i$，其中 $\\mu^\\star = \\max\\{\\mu_1, \\mu_2\\}$。设在时间 $t=200$ 时，寻求熵的策略选择的臂为 $i_{\\mathrm{E}}$，UCB策略选择的臂为 $i_{\\mathrm{U}}$。\n\n定义悔憾-信息权衡比为\n$$\n\\mathcal{T} = \\frac{r_{i_{\\mathrm{E}}} - r_{i_{\\mathrm{U}}}}{H_{i_{\\mathrm{E}}} - H_{i_{\\mathrm{U}}}},\n$$\n即寻求熵的策略相对于UCB策略所产生的超额期望单步悔憾，除以其所获得的观测的超额预测结果熵。\n\n计算给定场景下 $\\mathcal{T}$ 的数值。在整个计算中使用自然对数。将最终答案四舍五入到四位有效数字。",
            "solution": "首先验证问题，发现其提法恰当、有科学依据且内部一致。我们可以进行定量求解。\n\n问题要求计算在一个双臂老虎机问题的特定场景下的悔憾-信息权衡比 $\\mathcal{T}$。解决此问题的步骤如下：首先，确定最优臂以及选择每个臂的单步期望悔憾；其次，确定寻求熵的策略和UCB策略所选择的臂；第三，使用这些选择来计算 $\\mathcal{T}$ 的值。\n\n首先，我们确定每个臂的真实期望奖励，给定为 $\\mu_1 = 0.9$ 和 $\\mu_2 = 0.5$。最优臂是具有最高真实期望奖励的臂。因此，最优奖励为 $\\mu^\\star = \\max\\{\\mu_1, \\mu_2\\} = \\max\\{0.9, 0.5\\} = 0.9$。臂 $1$ 是最优臂。\n\n选择臂 $i$ 的单步期望悔憾定义为 $r_i = \\mu^\\star - \\mu_i$。\n对于臂 $1$，其悔憾为 $r_1 = \\mu^\\star - \\mu_1 = 0.9 - 0.9 = 0$。\n对于臂 $2$，其悔憾为 $r_2 = \\mu^\\star - \\mu_2 = 0.9 - 0.5 = 0.4$。\n\n接下来，我们确定寻求熵的策略所选择的臂 $i_{\\mathrm{E}}$。该策略选择使预测结果熵 $H_i = - \\hat{p}_i \\ln \\hat{p}_i - (1 - \\hat{p}_i) \\ln(1 - \\hat{p}_i)$ 最大化的臂 $i$。给定的样本均值为 $\\hat{p}_1 = 0.9$ 和 $\\hat{p}_2 = 0.5$。\n\n对于臂 $1$，其熵为：\n$$ H_1 = -0.9 \\ln(0.9) - (1-0.9) \\ln(1-0.9) = -0.9 \\ln(0.9) - 0.1 \\ln(0.1) $$\n对于臂 $2$，其熵为：\n$$ H_2 = -0.5 \\ln(0.5) - (1-0.5) \\ln(1-0.5) = -0.5 \\ln(0.5) - 0.5 \\ln(0.5) = -\\ln(0.5) = \\ln(2) $$\n函数 $f(p) = -p\\ln p - (1-p)\\ln(1-p)$ 代表伯努利试验的熵。该函数在 $p=0.5$ 时最大化。由于 $\\hat{p}_2 = 0.5$，所以 $H_2$ 具有伯努利随机变量可能的最大值。由于 $\\hat{p}_1 = 0.9 \\neq 0.5$，我们必然有 $H_1  H_2$。\n从数值上看，$H_2 = \\ln(2) \\approx 0.6931$ 纳特，而 $H_1 \\approx -0.9(-0.1054) - 0.1(-2.3026) = 0.0949 + 0.2303 = 0.3252$ 纳特。\n由于 $H_2 > H_1$，寻求熵的策略选择臂 $2$。因此，$i_{\\mathrm{E}} = 2$。\n\n现在，我们确定上置信界（UCB）策略所选择的臂 $i_{\\mathrm{U}}$。该策略选择使指数 $\\mathrm{UCB}_i(t) = \\hat{p}_i + \\sqrt{\\frac{2 \\ln t}{n_i}}$ 最大化的臂。决策在时间 $t=200$ 时做出。每个臂的拉动次数为 $n_1 = 100$ 和 $n_2 = 100$。\n\n对于臂 $1$：\n$$ \\mathrm{UCB}_1(200) = \\hat{p}_1 + \\sqrt{\\frac{2 \\ln(200)}{n_1}} = 0.9 + \\sqrt{\\frac{2 \\ln(200)}{100}} = 0.9 + \\sqrt{\\frac{\\ln(200)}{50}} $$\n对于臂 $2$：\n$$ \\mathrm{UCB}_2(200) = \\hat{p}_2 + \\sqrt{\\frac{2 \\ln(200)}{n_2}} = 0.5 + \\sqrt{\\frac{2 \\ln(200)}{100}} = 0.5 + \\sqrt{\\frac{\\ln(200)}{50}} $$\n由于 $n_1 = n_2$，探索项 $\\sqrt{\\frac{\\ln(200)}{50}}$ 对两个臂来说是相同的。因此，决策由利用项 $\\hat{p}_i$ 决定。由于 $\\hat{p}_1 = 0.9 > \\hat{p}_2 = 0.5$，因此 $\\mathrm{UCB}_1(200) > \\mathrm{UCB}_2(200)$。\nUCB策略选择臂 $1$。因此，$i_{\\mathrm{U}} = 1$。\n\n最后，我们计算悔憾-信息权衡比 $\\mathcal{T}$。其定义为：\n$$ \\mathcal{T} = \\frac{r_{i_{\\mathrm{E}}} - r_{i_{\\mathrm{U}}}}{H_{i_{\\mathrm{E}}} - H_{i_{\\mathrm{U}}}} $$\n我们代入已求得的值：\n$i_{\\mathrm{E}} = 2$，所以 $r_{i_{\\mathrm{E}}} = r_2 = 0.4$ 且 $H_{i_{\\mathrm{E}}} = H_2 = \\ln(2)$。\n$i_{\\mathrm{U}} = 1$，所以 $r_{i_{\\mathrm{U}}} = r_1 = 0$ 且 $H_{i_{\\mathrm{U}}} = H_1 = -0.9 \\ln(0.9) - 0.1 \\ln(0.1)$。\n\n将这些值代入 $\\mathcal{T}$ 的表达式中：\n$$ \\mathcal{T} = \\frac{0.4 - 0}{\\ln(2) - (-0.9 \\ln(0.9) - 0.1 \\ln(0.1))} = \\frac{0.4}{\\ln(2) + 0.9 \\ln(0.9) + 0.1 \\ln(0.1)} $$\n我们现在计算其数值。\n使用自然对数值：\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.9) \\approx -0.105361$\n$\\ln(0.1) \\approx -2.302585$\n\n分母是：\n$H_2 - H_1 \\approx 0.693147 + 0.9(-0.105361) + 0.1(-2.302585)$\n$H_2 - H_1 \\approx 0.693147 - 0.094825 - 0.230259 = 0.368063$\n\n因此，该比率为：\n$$ \\mathcal{T} \\approx \\frac{0.4}{0.368063} \\approx 1.086766 $$\n四舍五入到四位有效数字，我们得到 $1.087$。\n量 $\\mathcal{T}$ 代表信息的“价格”，以每单位熵（纳特）的期望悔憾为单位。在这个具体例子中，寻求熵的（探索）策略相对于UCB策略产生了 $0.4$ 的超额悔憾，以获得约 $0.368$ 纳特信息的超额增益，从而得出了计算出的比率。",
            "answer": "$$\n\\boxed{1.087}\n$$"
        },
        {
            "introduction": "决策常常需要在特定情境下做出，这引导我们将探索-利用问题从简单的多臂老虎机扩展到更具实际意义的情境老虎机（contextual bandits）。本练习将我们带入一个以逻辑回归为模型的贝叶斯决策问题中，通过拉普拉斯近似来量化一次探索性行动所能带来的预期参数方差缩减量 。通过计算一个综合了信息增益和机会成本的“价值评分”，我们学会了如何在复杂的参数模型中对探索行为进行一步向前看的成本效益分析，这是将自适应系统建模应用于实践的关键一步。",
            "id": "4147984",
            "problem": "考虑一个在贝叶斯复杂自适应系统建模框架下的情境二元决策问题，该问题被建模为一个逻辑斯谛赌博机(Logistic bandit)。设未知参数为 $\\theta \\in \\mathbb{R}^{3}$，其服从高斯先验分布 $\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$，其中 $\\mu = \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{pmatrix}$ 且 $\\Sigma = \\operatorname{diag}(0.5, 0.3, 0.2)$。对于一个情境（特征）向量 $x \\in \\mathbb{R}^{3}$，二元奖励 $y \\in \\{0,1\\}$ 根据伯努利模型 $\\Pr(y = 1 \\mid x, \\theta) = \\sigma(\\theta^{\\top} x)$ 生成，其中逻辑斯谛函数为 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$。您正在考虑通过在 $x_{e} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$ 处采取一个探索性动作来获取一个额外样本。\n\n在对 $x_{e}$ 处进行一次观测后的后验更新使用拉普拉斯-高斯近似的情况下，利用伯努利-逻辑斯谛模型和费雪信息的基本定义，推导因采取这一次探索性采样而导致的后验参数方差的期望减少量，该减少量被量化为后验协方差矩阵迹的减少量。然后，设当前的利用基线动作为 $x_{b} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 0 \\end{pmatrix}$。将探索的即时机会成本定义为利用动作和探索动作之间的期望即时奖励之差，即 $\\delta_{\\mathrm{op}} = \\sigma(\\mu^{\\top} x_{b}) - \\sigma(\\mu^{\\top} x_{e})$。最后，将探索的净短视价值分数定义为 $W = c \\cdot \\Delta_{\\mathrm{tr}} - \\delta_{\\mathrm{op}}$，其中 $c = 1$ 是一个已知系数，用于捕捉规划器在目标中对减少方差的重视程度，而 $\\Delta_{\\mathrm{tr}}$ 是探索性样本带来的协方差矩阵迹的期望减少量。\n\n使用上述拉普拉斯-高斯近似，为给定的 $\\mu$、$\\Sigma$、$x_{e}$、$x_{b}$ 和 $c$ 计算 $W$。将最终的价值分数四舍五入到四位有效数字。答案无需物理单位。",
            "solution": "该问题是有效的，因为它在科学上基于贝叶斯统计和强化学习，提供了所有必要信息故而是适定的，并且其表述是客观的。我们将着手求解净短视价值分数 $W$。\n\n分数 $W$ 定义为 $W = c \\cdot \\Delta_{\\mathrm{tr}} - \\delta_{\\mathrm{op}}$，其中 $c=1$。我们必须计算两个组成部分：$\\Delta_{\\mathrm{tr}}$，即后验协方差矩阵迹的期望减少量，以及 $\\delta_{\\mathrm{op}}$，即探索的即时机会成本。\n\n### 1. 期望迹减少量 $\\Delta_{\\mathrm{tr}}$ 的计算\n\n该问题设置在贝叶斯逻辑斯谛回归框架内。在一次观测 $(x, y)$ 后，参数 $\\theta$ 的后验分布由贝叶斯法则给出。在拉普拉斯-高斯近似下，后验协方差 $\\Sigma_{\\text{post}}$ 是对数后验的负海森矩阵在后验众数处求值后的逆矩阵。为了计算在观测到结果*之前*的*期望*信息增益，一种标准的简化方法是在当前参数均值 $\\mu$ 处评估此海森矩阵。这导致后验精度（协方差的逆）近似为先验精度与在 $\\mu$ 处评估的费雪信息矩阵之和。\n\n先验精度是 $\\Sigma^{-1}$。对于在情境 $x_e$ 处的单次观测，在 $\\theta = \\mu$ 处评估的费雪信息矩阵由下式给出：\n$$\nI(\\mu) = \\sigma(\\mu^{\\top} x_e) (1 - \\sigma(\\mu^{\\top} x_e)) x_e x_e^{\\top}\n$$\n因此，后验精度近似为：\n$$\n\\Sigma_{\\text{post}}^{-1} \\approx \\Sigma^{-1} + I(\\mu) = \\Sigma^{-1} + \\sigma(\\mu^{\\top} x_e) (1 - \\sigma(\\mu^{\\top} x_e)) x_e x_e^{\\top}\n$$\n让我们定义标量权重 $w_e = \\sigma(\\mu^{\\top} x_e) (1 - \\sigma(\\mu^{\\top} x_e))$。则表达式变为：\n$$\n\\Sigma_{\\text{post}}^{-1} \\approx \\Sigma^{-1} + w_e x_e x_e^{\\top}\n$$\n为了求得后验协方差 $\\Sigma_{\\text{post}}$，我们使用针对秩-1 更新的 Sherman-Morrison-Woodbury 公式来对此表达式求逆：$(A + uv^{\\top})^{-1} = A^{-1} - \\frac{A^{-1}uv^{\\top}A^{-1}}{1 + v^{\\top}A^{-1}u}$。这里，我们设 $A = \\Sigma^{-1}$，$u = w_e x_e$ 和 $v = x_e$。因此，$A^{-1}=\\Sigma$。\n$$\n\\Sigma_{\\text{post}} \\approx \\left(\\Sigma^{-1} + w_e x_e x_e^{\\top}\\right)^{-1} = \\Sigma - \\frac{\\Sigma (w_e x_e) x_e^{\\top} \\Sigma}{1 + x_e^{\\top} \\Sigma (w_e x_e)} = \\Sigma - \\frac{w_e (\\Sigma x_e) (x_e^{\\top} \\Sigma)}{1 + w_e (x_e^{\\top} \\Sigma x_e)}\n$$\n协方差的减少量为 $\\Sigma - \\Sigma_{\\text{post}}$。我们感兴趣的量 $\\Delta_{\\mathrm{tr}}$ 是这个减少量矩阵的迹：\n$$\n\\Delta_{\\mathrm{tr}} = \\operatorname{tr}(\\Sigma - \\Sigma_{\\text{post}}) = \\operatorname{tr}\\left( \\frac{w_e (\\Sigma x_e) (x_e^{\\top} \\Sigma)}{1 + w_e x_e^{\\top} \\Sigma x_e} \\right)\n$$\n利用迹的循环性质 $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$，我们可以简化分子中外积的迹。令 $v = \\Sigma x_e$。分子项为 $w_e v v^{\\top}$。其迹为 $\\operatorname{tr}(w_e v v^{\\top}) = w_e \\operatorname{tr}(v v^{\\top}) = w_e v^{\\top}v$。\n$$\nv^{\\top}v = (\\Sigma x_e)^{\\top} (\\Sigma x_e) = x_e^{\\top} \\Sigma^{\\top} \\Sigma x_e = x_e^{\\top} \\Sigma^2 x_e\n$$\n（因为 $\\Sigma$ 是对称的，所以 $\\Sigma^{\\top} = \\Sigma$）。因此，$\\Delta_{\\mathrm{tr}}$ 的最终表达式为：\n$$\n\\Delta_{\\mathrm{tr}} = \\frac{w_e (x_e^{\\top} \\Sigma^2 x_e)}{1 + w_e (x_e^{\\top} \\Sigma x_e)}\n$$\n我们现在代入给定值：\n$\\mu = \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{pmatrix}$，$\\Sigma = \\operatorname{diag}(0.5, 0.3, 0.2)$，$x_{e} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$。\n\n首先，计算逻辑斯谛函数的自变量：\n$$\nz_e = \\mu^{\\top} x_e = (0.1)(1) + (-0.2)(-1) + (0.05)(2) = 0.1 + 0.2 + 0.1 = 0.4\n$$\n接下来，计算 $w_e$：\n$$\n\\sigma(0.4) = \\frac{1}{1 + \\exp(-0.4)} \\approx 0.5986877\n$$\n$$\nw_e = \\sigma(0.4)(1 - \\sigma(0.4)) \\approx 0.5986877 \\times (1 - 0.5986877) \\approx 0.2402612\n$$\n现在，计算二次型：\n$$\nx_e^{\\top} \\Sigma x_e = \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 0.5  0  0 \\\\ 0  0.3  0 \\\\ 0  0  0.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} = (1)^2(0.5) + (-1)^2(0.3) + (2)^2(0.2) = 0.5 + 0.3 + 0.8 = 1.6\n$$\n由于 $\\Sigma$ 是对角矩阵，$\\Sigma^2 = \\operatorname{diag}(0.5^2, 0.3^2, 0.2^2) = \\operatorname{diag}(0.25, 0.09, 0.04)$。\n$$\nx_e^{\\top} \\Sigma^2 x_e = (1)^2(0.25) + (-1)^2(0.09) + (2)^2(0.04) = 0.25 + 0.09 + 0.16 = 0.5\n$$\n最后，将这些值代入 $\\Delta_{\\mathrm{tr}}$ 的表达式中：\n$$\n\\Delta_{\\mathrm{tr}} = \\frac{0.2402612 \\times 0.5}{1 + 0.2402612 \\times 1.6} = \\frac{0.1201306}{1 + 0.3844179} = \\frac{0.1201306}{1.3844179} \\approx 0.0867733\n$$\n\n### 2. 机会成本 $\\delta_{\\mathrm{op}}$ 的计算\n\n探索的即时机会成本定义为已知的最佳利用动作 $x_b$ 与探索性动作 $x_e$ 之间的期望即时奖励之差。对于一个动作 $x$，其期望奖励使用当前均值参数 $\\mu$ 估计为 $\\sigma(\\mu^{\\top}x)$。\n$$\n\\delta_{\\mathrm{op}} = \\sigma(\\mu^{\\top} x_{b}) - \\sigma(\\mu^{\\top} x_{e})\n$$\n给定 $x_b = \\begin{pmatrix} 3 \\\\ -2 \\\\ 0 \\end{pmatrix}$。首先，计算 $\\mu^{\\top}x_b$：\n$$\nz_b = \\mu^{\\top} x_b = (0.1)(3) + (-0.2)(-2) + (0.05)(0) = 0.3 + 0.4 = 0.7\n$$\n现在我们计算所需的逻辑斯谛函数值：\n$$\n\\sigma(z_b) = \\sigma(0.7) = \\frac{1}{1 + \\exp(-0.7)} \\approx 0.6681878\n$$\n我们已经计算出 $\\sigma(\\mu^{\\top}x_e) = \\sigma(0.4) \\approx 0.5986877$。\n机会成本为：\n$$\n\\delta_{\\mathrm{op}} = \\sigma(0.7) - \\sigma(0.4) \\approx 0.6681878 - 0.5986877 = 0.0695001\n$$\n\n### 3. 价值分数 $W$ 的计算\n\n最后，我们使用 $c=1$ 计算净短视价值分数 $W$：\n$$\nW = c \\cdot \\Delta_{\\mathrm{tr}} - \\delta_{\\mathrm{op}} = 1 \\times \\Delta_{\\mathrm{tr}} - \\delta_{\\mathrm{op}}\n$$\n$$\nW \\approx 0.0867733 - 0.0695001 = 0.0172732\n$$\n题目要求将答案四舍五入到四位有效数字。第一位有效数字是百分位上的 $1$。第四位有效数字是第二个 $7$。后面的数字是 $3$，小于 $5$，因此我们舍去。\n$$\nW \\approx 0.01727\n$$",
            "answer": "$$\n\\boxed{0.01727}\n$$"
        }
    ]
}