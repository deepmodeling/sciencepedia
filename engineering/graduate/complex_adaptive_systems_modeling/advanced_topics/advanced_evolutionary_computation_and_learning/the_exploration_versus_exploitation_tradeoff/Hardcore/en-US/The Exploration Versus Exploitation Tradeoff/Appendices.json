{
    "hands_on_practices": [
        {
            "introduction": "The Gittins index provides a remarkably elegant and optimal solution to the discounted multi-armed bandit problem. This exercise goes to the heart of this cornerstone result, tasking you with deriving the index from first principles for the classic Beta-Bernoulli model . By formulating the problem as one of optimal stopping and using the Bellman recursion on belief states, you will uncover how the index emerges as a \"break-even\" subsidy, providing deep insight into the mathematical structure of the exploration-exploitation tradeoff.",
            "id": "4148030",
            "problem": "Consider a single-armed Bayesian Bernoulli bandit that is activated at discrete times $t \\in \\{0,1,2,\\dots\\}$. When activated at time $t$, the arm yields a random reward $X_t \\in \\{0,1\\}$ distributed as $\\mathrm{Bernoulli}(\\theta)$, where the unknown success probability $\\theta \\in [0,1]$ has prior distribution $\\mathrm{Beta}(\\alpha,\\beta)$ with parameters $\\alpha0$ and $\\beta0$. The posterior over $\\theta$ after observing a success at time $t$ updates to $\\mathrm{Beta}(\\alpha+1,\\beta)$, and after a failure updates to $\\mathrm{Beta}(\\alpha,\\beta+1)$. Future rewards are geometrically discounted with a known factor $\\gamma \\in (0,1)$. Let $\\mathcal{F}_t$ denote the $\\sigma$-algebra generated by the rewards observed up to and including time $t$, and let $\\tau$ denote a stopping time with respect to the filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$. You may assume the validity of the Gittins index theorem for geometrically discounted multi-armed bandits.\n\nStart from first principles consisting of Bayesian belief updating for the Beta-Bernoulli model and the definition of optimal stopping, and derive a rigorous expression for the Gittins index (GI) of this arm at belief state $\\mathrm{Beta}(\\alpha,\\beta)$ with discount factor $\\gamma$. Your derivation should proceed by formulating the optimal stopping problem that compares continuing to sample the arm versus stopping, transforming it into a dynamic program on belief states. Then, using a subsidy-for-continuation formulation, derive the Bellman recursion that defines the value function of the single arm under a per-step subsidy $\\lambda \\in \\mathbb{R}$, and characterize the Gittins index as the threshold subsidy that makes the arm just indifferent between continuing and stopping. Clearly state and justify any monotonicity or contraction properties you use to ensure existence and uniqueness of the index.\n\nFinally, as your answer, provide a single analytical expression that defines the Gittins index at belief state $\\mathrm{Beta}(\\alpha,\\beta)$ with discount factor $\\gamma$ as a supremum over stopping times of a ratio of two discounted expectations. No numerical evaluation is required, and no rounding is necessary. Do not include any units in your final answer.",
            "solution": "We begin by formalizing the single-arm problem with Bayesian learning and geometric discounting. The state is the belief $(\\alpha,\\beta)$ over the unknown success probability $\\theta$, with prior and posterior distributions in the conjugate family $\\mathrm{Beta}(\\alpha,\\beta)$. The arm, when activated, yields reward $X_t \\in \\{0,1\\}$ with conditional distribution $X_t \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$. The posterior updating rule is a well-tested fact for the Beta-Bernoulli model: if the current belief is $\\mathrm{Beta}(\\alpha,\\beta)$, observing a success updates the belief to $\\mathrm{Beta}(\\alpha+1,\\beta)$ and a failure updates it to $\\mathrm{Beta}(\\alpha,\\beta+1)$. The posterior mean at belief $(\\alpha,\\beta)$ is $\\mu(\\alpha,\\beta) = \\alpha/(\\alpha+\\beta)$.\n\nThe Gittins index provides, for geometrically discounted multi-armed bandits, a decomposition of the optimal allocation policy into a per-arm index maximization policy. For a single arm, the Gittins index at a belief state is the value of an associated one-dimensional optimal stopping problem, which we now construct from first principles.\n\nDefine a stopping time $\\tau$ (with respect to the filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ generated by the observed rewards) as the time at which we stop sampling the arm. Under geometric discounting with factor $\\gamma \\in (0,1)$, and starting from belief $(\\alpha,\\beta)$, the expected discounted reward obtained by sampling the arm up to (but not including) time $\\tau$ is\n$$\nR(\\alpha,\\beta;\\tau) \\equiv \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} X_t \\right],\n$$\nwhere $\\mathbb{E}_{\\alpha,\\beta}[\\cdot]$ denotes expectation with respect to the joint law induced by the prior $\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)$ and the conditional draws $X_t \\mid \\theta$ under the stopping rule $\\tau$. The discounted time accumulated up to $\\tau$ is\n$$\nT(\\tau) \\equiv \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\right].\n$$\nBoth $R(\\alpha,\\beta;\\tau)$ and $T(\\tau)$ are finite because $\\gamma \\in (0,1)$ implies $\\sum_{t=0}^{\\infty} \\gamma^t  \\infty$ and the rewards are bounded in $[0,1]$.\n\nThe optimal stopping problem underlying the Gittins index takes the form of maximizing the ratio of expected discounted reward to expected discounted time over all admissible stopping times:\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) \\equiv \\sup_{\\tau \\in \\mathcal{T}} \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)},\n$$\nwhere $\\mathcal{T}$ is the set of all stopping times with respect to $\\{\\mathcal{F}_t\\}$ taking values in $\\{1,2,\\dots\\}$. This ratio interpretation can be obtained by considering the Lagrangian or subsidy formulation, which we now derive to produce the dynamic programming recursion.\n\nIntroduce a per-period subsidy $\\lambda \\in \\mathbb{R}$ for continuing to sample the arm, and define the value function under subsidy $\\lambda$ as the maximal expected discounted net reward:\n$$\nV^{\\lambda}(\\alpha,\\beta) \\equiv \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\big( X_t - \\lambda \\big) \\right].\n$$\nThis is the value of an optimal stopping problem on the belief-state Markov chain induced by Beta-Bernoulli updating. The process on beliefs is a time-homogeneous Markov chain on the countable state space $\\{(\\alpha+i,\\beta+j): i,j \\in \\mathbb{N}_0\\}$, and geometric discounting with $\\gamma \\in (0,1)$ ensures that the Bellman equation for $V^{\\lambda}$ is a contraction mapping.\n\nBy the principle of optimality, at belief $(\\alpha,\\beta)$, the Bellman recursion is\n$$\nV^{\\lambda}(\\alpha,\\beta)\n=\n\\max \\Big\\{ 0,\\, \\underbrace{\\mu(\\alpha,\\beta) - \\lambda}_{\\text{net immediate reward}} + \\gamma \\Big[ \\mu(\\alpha,\\beta)\\, V^{\\lambda}(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, V^{\\lambda}(\\alpha,\\beta+1) \\Big] \\Big\\},\n$$\nwith $\\mu(\\alpha,\\beta) = \\alpha/(\\alpha+\\beta)$. The two terms inside the maximum correspond to stopping immediately (yielding $0$ continuation value because we define the post-stop value to be $0$) and continuing for one more play (collecting the expected immediate net reward $\\mu(\\alpha,\\beta)-\\lambda$ plus the discounted expected continuation value, where the next belief is $(\\alpha+1,\\beta)$ with probability $\\mu(\\alpha,\\beta)$ and $(\\alpha,\\beta+1)$ with probability $1-\\mu(\\alpha,\\beta)$).\n\nStandard arguments for discounted optimal stopping in a bounded-reward, time-homogeneous Markov decision process (MDP) imply that for each fixed $\\lambda$, the operator\n$$\n\\mathcal{T}^{\\lambda}[f](\\alpha,\\beta)\n=\n\\max \\Big\\{ 0,\\, \\mu(\\alpha,\\beta) - \\lambda + \\gamma \\big[ \\mu(\\alpha,\\beta)\\, f(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, f(\\alpha,\\beta+1) \\big] \\Big\\}\n$$\nis a $\\gamma$-contraction on the space of bounded functions equipped with the sup norm. Hence, the Bellman equation has a unique bounded fixed point $V^{\\lambda}$, and value iteration converges to it.\n\nWe now connect $V^{\\lambda}$ to the ratio characterization. For any stopping time $\\tau$,\n$$\n\\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\big( X_t - \\lambda \\big) \\right]\n=\nR(\\alpha,\\beta;\\tau) - \\lambda\\, T(\\tau).\n$$\nBy definition of $V^{\\lambda}$, we have $V^{\\lambda}(\\alpha,\\beta) \\ge R(\\alpha,\\beta;\\tau) - \\lambda\\, T(\\tau)$ for all $\\tau$, hence\n$$\n\\lambda \\ge \\frac{R(\\alpha,\\beta;\\tau) - V^{\\lambda}(\\alpha,\\beta)}{T(\\tau)}.\n$$\nWhen $V^{\\lambda}(\\alpha,\\beta) = 0$, this yields\n$$\n\\lambda \\ge \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)} \\quad \\text{for all } \\tau \\in \\mathcal{T}.\n$$\nIt follows that if $V^{\\lambda}(\\alpha,\\beta) = 0$, then $\\lambda$ is an upper bound on the ratio $R(\\alpha,\\beta;\\tau)/T(\\tau)$ over all stopping times, so $\\lambda \\ge \\sup_{\\tau \\in \\mathcal{T}} R(\\alpha,\\beta;\\tau)/T(\\tau)$. Conversely, suppose $\\lambda  \\sup_{\\tau \\in \\mathcal{T}} R(\\alpha,\\beta;\\tau)/T(\\tau)$. Then there exists a stopping time $\\tau^{\\star}$ such that $R(\\alpha,\\beta;\\tau^{\\star}) - \\lambda\\, T(\\tau^{\\star})  0$, and hence $V^{\\lambda}(\\alpha,\\beta) \\ge R(\\alpha,\\beta;\\tau^{\\star}) - \\lambda\\, T(\\tau^{\\star})  0$. Therefore, the set $\\{\\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta)  0\\}$ is the open interval $(-\\infty, \\mathcal{G}(\\alpha,\\beta;\\gamma))$, and $V^{\\lambda}(\\alpha,\\beta) = 0$ for all $\\lambda \\ge \\mathcal{G}(\\alpha,\\beta;\\gamma)$.\n\nMonotonicity of $V^{\\lambda}$ in $\\lambda$ follows from the Bellman operator: if $\\lambda_1  \\lambda_2$, then $\\mathcal{T}^{\\lambda_1}[f](\\alpha,\\beta) \\ge \\mathcal{T}^{\\lambda_2}[f](\\alpha,\\beta)$ pointwise for any $f$, and the unique fixed points satisfy $V^{\\lambda_1}(\\alpha,\\beta) \\ge V^{\\lambda_2}(\\alpha,\\beta)$. Together with boundedness and continuity in $\\lambda$, these facts imply the existence and uniqueness of a threshold $\\lambda^{\\star}$ such that $V^{\\lambda}(\\alpha,\\beta)  0$ for $\\lambda  \\lambda^{\\star}$ and $V^{\\lambda}(\\alpha,\\beta) = 0$ for $\\lambda \\ge \\lambda^{\\star}$. This threshold is precisely the Gittins index, and it coincides with the ratio supremum:\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\sup_{\\tau \\in \\mathcal{T}} \\frac{R(\\alpha,\\beta;\\tau)}{T(\\tau)}.\n$$\n\nFor computation via dynamic programming on belief states, one can fix a candidate subsidy $\\lambda$ and compute $V^{\\lambda}$ by value iteration:\n$$\nV^{\\lambda}_{k+1}(\\alpha,\\beta) \\leftarrow \\max \\Big\\{ 0,\\, \\mu(\\alpha,\\beta) - \\lambda + \\gamma \\big[ \\mu(\\alpha,\\beta)\\, V^{\\lambda}_{k}(\\alpha+1,\\beta) + \\big(1-\\mu(\\alpha,\\beta)\\big)\\, V^{\\lambda}_{k}(\\alpha,\\beta+1) \\big] \\Big\\},\n$$\ninitialized with $V^{\\lambda}_{0}(\\alpha,\\beta) \\equiv 0$ for all $(\\alpha,\\beta)$, which converges uniformly to $V^{\\lambda}$. By monotonicity in $\\lambda$ and the bounds $0 \\le \\mathcal{G}(\\alpha,\\beta;\\gamma) \\le 1$ (since rewards lie in $[0,1]$), a simple bisection search on $\\lambda \\in [0,1]$ yields the unique $\\lambda^{\\star}$ satisfying $V^{\\lambda^{\\star}}(\\alpha,\\beta) = 0$, which is the Gittins index at belief $(\\alpha,\\beta)$.\n\nSummarizing, the Gittins index at belief $\\mathrm{Beta}(\\alpha,\\beta)$ with discount factor $\\gamma$ is the supremum, over all stopping times adapted to the observed rewards, of the ratio of expected discounted reward to expected discounted time:\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\sup_{\\tau \\in \\mathcal{T}} \\frac{ \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} X_t \\right] }{ \\mathbb{E}_{\\alpha,\\beta}\\!\\left[ \\sum_{t=0}^{\\tau-1} \\gamma^{t} \\right] }.\n$$\nThe dynamic programming recursion that enables computation is given by the subsidy-form Bellman equation\n$$\nV^{\\lambda}(\\alpha,\\beta) = \\max \\Big\\{ 0,\\, \\frac{\\alpha}{\\alpha+\\beta} - \\lambda + \\gamma \\Big[ \\frac{\\alpha}{\\alpha+\\beta}\\, V^{\\lambda}(\\alpha+1,\\beta) + \\frac{\\beta}{\\alpha+\\beta}\\, V^{\\lambda}(\\alpha,\\beta+1) \\Big] \\Big\\},\n$$\nand the Gittins index is\n$$\n\\mathcal{G}(\\alpha,\\beta;\\gamma) = \\inf \\big\\{ \\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta) = 0 \\big\\} = \\sup \\big\\{ \\lambda \\in \\mathbb{R} : V^{\\lambda}(\\alpha,\\beta)  0 \\big\\}.\n$$\nAny of these equivalent characterizations defines the same unique real value $\\mathcal{G}(\\alpha,\\beta;\\gamma)$.",
            "answer": "$$\\boxed{\\displaystyle \\mathcal{G}(\\alpha,\\beta;\\gamma)=\\sup_{\\tau \\in \\mathcal{T}}\\,\\frac{\\mathbb{E}_{\\alpha,\\beta}\\!\\left[\\sum_{t=0}^{\\tau-1}\\gamma^{t} X_t\\right]}{\\mathbb{E}_{\\alpha,\\beta}\\!\\left[\\sum_{t=0}^{\\tau-1}\\gamma^{t}\\right]}}$$"
        },
        {
            "introduction": "While the Gittins index offers a theoretical optimum, its true power is illuminated through practical application. This problem provides a concrete scenario to build intuition about the \"option value\" inherent in exploration . You will calculate the Gittins index for an arm with uncertain quality and compare it to its simple myopic reward, directly observing the value of the information an exploratory action might reveal. By quantifying the expected long-term loss of a purely exploitative policy, this exercise demonstrates in tangible terms the cost of ignoring the need to learn.",
            "id": "4148037",
            "problem": "Consider a two-armed decision problem framed as a Multi-Armed Bandit (MAB) with geometric discounting, modeling the exploration versus exploitation tradeoff in complex adaptive systems. Time is discrete, and future rewards are discounted by a factor $\\,\\gamma \\in (0,1)\\,$. At each time step, you may select one of two arms:\n- Arm $\\,B\\,$ yields a known constant reward $\\,b\\,$ per play, forever.\n- Arm $\\,A\\,$ has an unknown type. With prior probability $\\,p\\,$ it is High-type and yields $\\,h\\,$ per play forever; with prior probability $\\,1-p\\,$ it is Low-type and yields $\\,\\ell\\,$ per play forever. Pulling arm $\\,A\\,$ once perfectly reveals its type for all subsequent times (because the realized reward distinguishes the types), after which you may continue with arm $\\,A\\,$ if beneficial or stop and switch to arm $\\,B\\,$.\n\nUse the foundational principle of discounted optimal stopping to define the Gittins index of an arm as the reservation value $\\,c^{\\ast}\\,$ of a constant alternative stream that makes the decision-maker indifferent between beginning with the arm and choosing the constant stream immediately, given the option to stop and switch thereafter. Starting from this principle and the geometric series for discounted sums, derive the reservation-value expression for the Gittins index of arm $\\,A\\,$ for the one-step revealing model described.\n\nThen, specialize to the parameter values $\\,\\gamma = 0.9\\,$, $\\,p = 0.3\\,$, $\\,h = 10\\,$, $\\,\\ell = 0\\,$, and $\\,b = 4\\,$. Compute:\n1. The Gittins index $\\,c^{\\ast}\\,$ for arm $\\,A\\,$ and compare it to the myopic expected immediate reward $\\,p h + (1-p)\\ell\\,$.\n2. The expected discounted loss incurred by the myopic exploitation policy that chooses arm $\\,B\\,$ immediately and forever, rather than the optimal index policy that explores arm $\\,A\\,$ once and then continues optimally.\n\nExpress the final loss as a single real number. No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Two-armed bandit problem with discrete time.\n- Discount factor: $\\gamma \\in (0,1)$.\n- Arm $B$: yields a known constant reward $b$ per play.\n- Arm $A$: unknown type.\n  - High-type: reward $h$ per play, prior probability $p$.\n  - Low-type: reward $\\ell$ per play, prior probability $1-p$.\n- Pulling arm $A$ once reveals its type.\n- The Gittins index is defined as the reservation value $c^{\\ast}$ of a constant alternative that makes the agent indifferent between exploring the arm for one step and taking the alternative immediately.\n- Specific parameter values: $\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, $b = 4$.\n- Required computations:\n  1. The Gittins index $c^{\\ast}$ for arm $A$ and its comparison to the myopic expected reward.\n  2. The expected discounted loss of a myopic policy (always choosing arm $B$) versus the optimal index policy.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard, well-defined formulation of a Bayesian multi-armed bandit problem, a core topic in the study of complex adaptive systems and reinforcement learning. The Gittins index is the canonical solution for such discounted problems. The problem is self-contained, with all variables and conditions clearly specified. There are no scientific or logical contradictions, vagueness, or subjective elements. The problem is therefore valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Derivation of the Gittins Index for Arm $A$**\n\nThe Gittins index, $c^{\\ast}$, for arm $A$ is defined as the constant reward value of a hypothetical alternative arm that makes the agent indifferent between two initial choices:\n1. Choose the hypothetical arm, receiving reward $c^{\\ast}$ forever. The total discounted value is $V_{C} = \\sum_{t=0}^{\\infty} \\gamma^t c^{\\ast} = \\frac{c^{\\ast}}{1-\\gamma}$.\n2. Choose arm $A$ for one time step, observe the outcome, and then proceed optimally. The optimal subsequent action is to choose the arm with the higher perpetual reward stream: the revealed value of arm $A$ or the reservation value $c^{\\ast}$.\n\nThe value of choosing arm $A$ for one step, $V_{A}$, is the sum of the expected immediate reward and the discounted expected future value.\nThe expected immediate reward from pulling arm $A$ at time $t=0$ is $E[R_0] = ph + (1-p)\\ell$.\n\nAfter this pull, the type of arm $A$ is known.\n- With probability $p$, arm $A$ is High-type (reward $h$). The agent will then compare $h$ and $c^{\\ast}$. The optimal choice yields a reward of $\\max(h, c^{\\ast})$ forever. The discounted value of this future stream, as of time $t=1$, is $\\frac{\\max(h, c^{\\ast})}{1-\\gamma}$.\n- With probability $1-p$, arm $A$ is Low-type (reward $\\ell$). The agent will then compare $\\ell$ and $c^{\\ast}$. The optimal choice yields a reward of $\\max(\\ell, c^{\\ast})$ forever. The discounted value of this future stream, as of time $t=1$, is $\\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma}$.\n\nThe total expected discounted value of strategy 2 is:\n$$V_{A} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\frac{\\max(h, c^{\\ast})}{1-\\gamma} + (1-p) \\frac{\\max(\\ell, c^{\\ast})}{1-\\gamma} \\right]$$\nAt the point of indifference, $V_{A} = V_{C}$:\n$$\\frac{c^{\\ast}}{1-\\gamma} = (ph + (1-p)\\ell) + \\frac{\\gamma}{1-\\gamma} \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\nMultiplying by $(1-\\gamma)$ gives the fundamental equation for the Gittins index:\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ p \\max(h, c^{\\ast}) + (1-p) \\max(\\ell, c^{\\ast}) \\right]$$\nTo solve for $c^{\\ast}$, we assume a non-trivial case where $\\ell  c^{\\ast}  h$. This is reasonable because if $c^{\\ast}$ were outside this range, the decision to switch after learning would be foregone (e.g., if $c^{\\ast} \\le \\ell$, one would never switch to the alternative). With this assumption, $\\max(h, c^{\\ast}) = h$ and $\\max(\\ell, c^{\\ast}) = c^{\\ast}$.\nSubstituting these into the equation:\n$$c^{\\ast} = (1-\\gamma)(ph + (1-p)\\ell) + \\gamma \\left[ ph + (1-p)c^{\\ast} \\right]$$\nNow, we solve for $c^{\\ast}$:\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma ph - \\gamma(1-p)\\ell + \\gamma ph + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} = ph + (1-p)\\ell - \\gamma(1-p)\\ell + \\gamma(1-p)c^{\\ast}$$\n$$c^{\\ast} - \\gamma(1-p)c^{\\ast} = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast}(1 - \\gamma(1-p)) = ph + (1-p)(1-\\gamma)\\ell$$\n$$c^{\\ast} = \\frac{ph + (1-p)(1-\\gamma)\\ell}{1 - \\gamma + p\\gamma}$$\nThis is the reservation-value expression for the Gittins index of arm $A$.\n\n**Computations for Specific Parameter Values**\n\nThe given parameters are $\\gamma = 0.9$, $p = 0.3$, $h = 10$, $\\ell = 0$, and $b = 4$.\n\n1. **Gittins Index and Myopic Reward Comparison**\nFirst, we compute the Gittins index $c^{\\ast}$ for arm $A$:\n$$c^{\\ast} = \\frac{(0.3)(10) + (1-0.3)(1-0.9)(0)}{1 - 0.9 + (0.3)(0.9)} = \\frac{3 + 0}{1 - 0.9 + 0.27} = \\frac{3}{0.1 + 0.27} = \\frac{3}{0.37} = \\frac{300}{37}$$\nThe value of the Gittins index is $c^{\\ast} = \\frac{300}{37} \\approx 8.108$.\nThe myopic expected immediate reward is:\n$$E_{myopic} = ph + (1-p)\\ell = (0.3)(10) + (0.7)(0) = 3$$\nComparing the two, $c^{\\ast} = \\frac{300}{37}  3$. The difference, $c^{\\ast} - E_{myopic}$, represents the \"value of information\" or the option value inherent in being able to switch away from arm $A$ if it turns out to be a Low-type.\n\n2. **Expected Discounted Loss of Myopic Policy**\nThe optimal policy is dictated by the Gittins index rule: at any stage, play the arm with the highest index.\nThe index for arm $A$ is $c^{\\ast} = \\frac{300}{37}$.\nThe index for arm $B$ is its constant reward, $b = 4$.\nSince $c^{\\ast} = \\frac{300}{37} \\approx 8.108  4 = b$, the optimal policy is to explore arm $A$ first.\n\nThe problem defines the \"myopic exploitation policy\" as choosing arm $B$ immediately and forever. Let's calculate the expected total discounted value of this policy, $V_{myopic}$.\n$$V_{myopic} = \\sum_{t=0}^{\\infty} \\gamma^t b = \\frac{b}{1-\\gamma} = \\frac{4}{1-0.9} = \\frac{4}{0.1} = 40$$\n\nNow, we calculate the expected total discounted value of the optimal policy, $V_{optimal}$. This policy starts with pulling arm $A$.\n- The immediate reward is $ph + (1-p)\\ell = 3$.\n- After the pull, we re-evaluate.\n  - With probability $p=0.3$, arm $A$ is High-type ($h=10$). Since $h  b$, we continue with arm $A$. The future discounted value from $t=1$ is $\\frac{h}{1-\\gamma} = \\frac{10}{0.1} = 100$.\n  - With probability $1-p=0.7$, arm $A$ is Low-type ($\\ell=0$). Since $\\ell  b$, we switch to arm $B$. The future discounted value from $t=1$ is $\\frac{b}{1-\\gamma} = \\frac{4}{0.1} = 40$.\n\nThe total expected value for the optimal policy is:\n$$V_{optimal} = (ph + (1-p)\\ell) + \\gamma \\left[ p \\left( \\frac{h}{1-\\gamma} \\right) + (1-p) \\left( \\frac{b}{1-\\gamma} \\right) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ (0.3)(100) + (0.7)(40) \\right]$$\n$$V_{optimal} = 3 + 0.9 \\left[ 30 + 28 \\right]$$\n$$V_{optimal} = 3 + 0.9(58)$$\n$$V_{optimal} = 3 + 52.2 = 55.2$$\n\nThe expected discounted loss incurred by the myopic policy is the difference in value between the optimal policy and the myopic policy.\n$$\\text{Loss} = V_{optimal} - V_{myopic} = 55.2 - 40 = 15.2$$\nThis loss represents the expected value forgone by failing to explore the potentially very rewarding arm $A$. In fractional form, the loss is $15.2 = \\frac{152}{10} = \\frac{76}{5}$.",
            "answer": "$$\n\\boxed{15.2}\n$$"
        },
        {
            "introduction": "As decision problems become more complex, such as in Bayesian optimization over structured spaces, the choice of algorithm becomes critical. This practice explores the subtle yet profound differences between two leading strategies: the optimism-based Gaussian Process Upper Confidence Bound (GP-UCB) and the posterior-sampling approach of Thompson Sampling (TS) . By constructing a specific scenario where GP-UCB is prone to \"over-exploration,\" you will analytically derive the resulting regret penalty compared to TS. This highlights how an algorithm's core philosophy dictates its behavior and provides a framework for reasoning about which strategy might be better suited for different problems.",
            "id": "4148010",
            "problem": "Consider a complex adaptive system modeled as a stochastic optimization problem over a finite decision set $\\mathcal{X} = \\{x_{1}, x_{2}, \\dots, x_{N}\\}$, where the unknown reward function $f: \\mathcal{X} \\to \\mathbb{R}$ is modeled by a Gaussian process prior with independent coordinates: for each $x_{i}$, the prior marginal is $f(x_{i}) \\sim \\mathcal{N}(0, s^{2})$, and the coordinates are independent. Observations are noiseless. Define the cumulative regret over a sequence of decisions $(x_{t})_{t \\geq 1}$ as $R_{T} := \\sum_{t=1}^{T} \\big(f(x^{\\star}) - f(x_{t})\\big)$, where $x^{\\star} \\in \\mathcal{X}$ is a maximizer of $f$.\n\nSuppose a preliminary pilot observation at time $t=0$ has revealed the reward at a particular design point $x^{\\star}$, with $f(x^{\\star}) = \\Delta$ for some known $\\Delta  0$, and all other unsampled points retain their prior distributions $\\mathcal{N}(0, s^{2})$. Thus, at time $t=1$, the posterior at $x^{\\star}$ has mean $m_{1}(x^{\\star}) = \\Delta$ and variance $\\sigma_{1}(x^{\\star}) = 0$, while for each $x \\neq x^{\\star}$, the posterior has mean $m_{1}(x) = 0$ and variance $\\sigma_{1}(x) = s$.\n\nAt time $t=1$:\n- The Gaussian Process Upper Confidence Bound (GP-UCB) algorithm selects $x_{1}^{\\mathrm{UCB}} \\in \\arg\\max_{x \\in \\mathcal{X}} \\big(m_{1}(x) + \\sqrt{\\beta_{1}} \\, \\sigma_{1}(x)\\big)$ for a given exploration parameter $\\beta_{1}  0$.\n- Thompson Sampling (TS) draws a single sample function $g$ from the current posterior over $f$ (so $g(x^{\\star}) = \\Delta$ deterministically and $g(x) \\sim \\mathcal{N}(0, s^{2})$ independently for unsampled $x$), and selects $x_{1}^{\\mathrm{TS}} \\in \\arg\\max_{x \\in \\mathcal{X}} g(x)$.\n\nAssume the system is in an over-exploration regime for GP-UCB at $t=1$, namely that $\\sqrt{\\beta_{1}} \\, s  \\Delta$, so that GP-UCB strictly prefers any unsampled $x \\neq x^{\\star}$ over $x^{\\star}$. Define the instantaneous regret penalty at $t=1$ of GP-UCB relative to TS as the difference in their expected instantaneous regrets at $t=1$, that is,\n$$\\Pi := \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{UCB}})\\big] - \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{TS}})\\big].$$\n\nDerive a closed-form analytic expression for $\\Pi$ in terms of $\\Delta$, $s$, $N$, and the standard normal cumulative distribution function $\\Phi(\\cdot)$. Your final answer must be a single analytic expression. No numerical approximation is required.",
            "solution": "The objective is to derive a closed-form expression for the instantaneous regret penalty $\\Pi$ at time $t=1$. The penalty is defined as the difference in expected instantaneous regrets between the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm and Thompson Sampling (TS).\n\nThe penalty $\\Pi$ is given by\n$$ \\Pi := \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{UCB}})\\big] - \\mathbb{E}\\big[f(x^{\\star}) - f(x_{1}^{\\mathrm{TS}})\\big] $$\nThe expectation $\\mathbb{E}[\\cdot]$ is taken over all sources of randomness. In this problem, the randomness originates from the unknown true function values $f(x)$ for $x \\neq x^{\\star}$, and the function sample $g(x)$ drawn by Thompson Sampling.\nGiven that $f(x^{\\star})=\\Delta$ is a known constant, we can use the linearity of expectation:\n$$ \\Pi = \\big(\\mathbb{E}[f(x^{\\star})] - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})]\\big) - \\big(\\mathbb{E}[f(x^{\\star})] - \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})]\\big) $$\n$$ \\Pi = (\\Delta - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})]) - (\\Delta - \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})]) $$\n$$ \\Pi = \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] $$\nTo find $\\Pi$, we must compute the expected reward for each algorithm at time $t=1$.\n\nFirst, we analyze the GP-UCB selection, $x_{1}^{\\mathrm{UCB}}$.\nThe algorithm selects an action from $\\mathcal{X}$ that maximizes the UCB score, $m_{1}(x) + \\sqrt{\\beta_{1}} \\sigma_{1}(x)$.\nFor the point $x^{\\star}$, the posterior mean is $m_{1}(x^{\\star}) = \\Delta$ and the posterior standard deviation is $\\sigma_{1}(x^{\\star}) = 0$. Its UCB score is:\n$$ \\text{UCB}(x^{\\star}) = \\Delta + \\sqrt{\\beta_{1}} \\cdot 0 = \\Delta $$\nFor any other point $x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$, the posterior mean is $m_{1}(x) = 0$ and the posterior standard deviation is $\\sigma_{1}(x) = s$. Its UCB score is:\n$$ \\text{UCB}(x) = 0 + \\sqrt{\\beta_{1}} \\cdot s = \\sqrt{\\beta_{1}} s $$\nThe problem states that the system is in an over-exploration regime, defined by the condition $\\sqrt{\\beta_{1}} s  \\Delta$. This implies that the UCB score for any unsampled point $x \\neq x^{\\star}$ is strictly greater than the score for $x^{\\star}$.\nTherefore, GP-UCB will select one of the $N-1$ unsampled points. Assuming a standard uniform random tie-breaking rule, $x_{1}^{\\mathrm{UCB}}$ is chosen uniformly from the set $\\mathcal{X} \\setminus \\{x^{\\star}\\}$.\nThe expected reward for GP-UCB is $\\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})]$. We compute this using the law of total expectation, conditioning on the chosen action:\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] = \\sum_{x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}} \\mathbb{E}[f(x)] P(x_{1}^{\\mathrm{UCB}} = x) $$\nThe probability of selecting any specific $x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$ is $P(x_{1}^{\\mathrm{UCB}} = x) = \\frac{1}{N-1}$.\nThe expected value of the true reward $\\mathbb{E}[f(x)]$ for any unsampled point is the mean of its distribution. The problem states that for $x \\neq x^{\\star}$, the posterior for $f(x)$ is $\\mathcal{N}(0, s^{2})$. Thus, $\\mathbb{E}[f(x)] = 0$.\nSubstituting these into the expression for the expected reward:\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] = \\sum_{x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}} (0) \\cdot \\left(\\frac{1}{N-1}\\right) = 0 $$\n\nNext, we analyze the Thompson Sampling selection, $x_{1}^{\\mathrm{TS}}$.\nTS first draws a sample function $g$ from the posterior distribution over $f$. At $t=1$, this means:\n- $g(x^{\\star}) = \\Delta$ (a deterministic value, since the posterior at $x^{\\star}$ is a point mass).\n- For each $x \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$, $g(x)$ is drawn independently from $\\mathcal{N}(0, s^{2})$.\nLet these $N-1$ sampled values be $g_{i} \\sim \\mathcal{N}(0, s^{2})$, i.i.d.\nTS then selects the action that maximizes this sampled function:\n$$ x_{1}^{\\mathrm{TS}} = \\arg\\max_{x \\in \\mathcal{X}} g(x) = \\arg\\max \\left\\{ \\Delta, g_{1}, g_{2}, \\dots, g_{N-1} \\right \\} $$\nThe expected reward is $\\mathbb{E}[f(x_{1}^{\\mathrm{TS}})]$. The expectation is over the randomness in the true function $f$ and the sampled function $g$. We use the law of total expectation, conditioning on the outcome of the TS sampling procedure $g$:\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] = \\mathbb{E}_{g} \\left[ \\mathbb{E}_{f} \\left[ f(x_{1}^{\\mathrm{TS}}(g)) \\mid g \\right] \\right] $$\nThe choice $x_{1}^{\\mathrm{TS}}(g)$ depends only on $g$. Let's analyze the inner expectation $\\mathbb{E}_{f}[\\cdot \\mid g]$.\n- If $x_{1}^{\\mathrm{TS}}(g) = x^{\\star}$, then $f(x_{1}^{\\mathrm{TS}}(g)) = f(x^{\\star}) = \\Delta$. The expectation is $\\mathbb{E}_{f}[\\Delta] = \\Delta$.\n- If $x_{1}^{\\mathrm{TS}}(g) = x_{i}$ for some $x_{i} \\in \\mathcal{X} \\setminus \\{x^{\\star}\\}$, then $f(x_{1}^{\\mathrm{TS}}(g)) = f(x_{i})$. The true function values $\\{f(x_i)\\}$ are random variables independent of the sampled function values $\\{g_i\\}$. The expectation is $\\mathbb{E}_{f}[f(x_i)] = 0$, since $f(x_i) \\sim \\mathcal{N}(0, s^{2})$.\nSo, the inner expectation simplifies to:\n$$ \\mathbb{E}_{f} \\left[ f(x_{1}^{\\mathrm{TS}}(g)) \\mid g \\right] = \\Delta \\cdot \\mathbf{1}_{\\{x_{1}^{\\mathrm{TS}}(g) = x^{\\star}\\}} $$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function.\nNow we take the outer expectation over $g$:\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] = \\mathbb{E}_{g}\\left[ \\Delta \\cdot \\mathbf{1}_{\\{x_{1}^{\\mathrm{TS}}(g) = x^{\\star}\\}} \\right] = \\Delta \\cdot P_{g}(x_{1}^{\\mathrm{TS}}(g) = x^{\\star}) $$\nThe event $x_{1}^{\\mathrm{TS}}(g) = x^{\\star}$ occurs if and only if $g(x^{\\star})$ is the maximum value among all sampled values. This means $\\Delta  g_{i}$ for all $i=1, \\dots, N-1$.\n$$ P(x_{1}^{\\mathrm{TS}} = x^{\\star}) = P(g_{1}  \\Delta, g_{2}  \\Delta, \\dots, g_{N-1}  \\Delta) $$\nSince the $g_{i}$ are i.i.d. draws from $\\mathcal{N}(0, s^{2})$, this probability is:\n$$ P(x_{1}^{\\mathrm{TS}} = x^{\\star}) = \\left( P(g_{1}  \\Delta) \\right)^{N-1} $$\nTo find $P(g_{1}  \\Delta)$, we standardize the random variable $g_{1}$. Let $Z \\sim \\mathcal{N}(0, 1)$. Then $g_{1} = sZ$.\n$$ P(g_{1}  \\Delta) = P(sZ  \\Delta) = P\\left(Z  \\frac{\\Delta}{s}\\right) $$\nThis probability is given by the cumulative distribution function (CDF) of the standard normal distribution, denoted $\\Phi(\\cdot)$. So, $P(g_{1}  \\Delta) = \\Phi(\\Delta/s)$.\nTherefore, the probability of TS selecting the optimal action is:\n$$ P(x_{1}^{\\mathrm{TS}} = x^{\\star}) = \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} $$\nAnd the expected reward for TS is:\n$$ \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] = \\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} $$\nFinally, we substitute the expected rewards back into the expression for $\\Pi$:\n$$ \\Pi = \\mathbb{E}[f(x_{1}^{\\mathrm{TS}})] - \\mathbb{E}[f(x_{1}^{\\mathrm{UCB}})] = \\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} - 0 $$\n$$ \\Pi = \\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1} $$\nThis expression represents the penalty, which is the amount by which the expected regret of GP-UCB exceeds that of TS under the specified over-exploration conditions.",
            "answer": "$$\\boxed{\\Delta \\left(\\Phi\\left(\\frac{\\Delta}{s}\\right)\\right)^{N-1}}$$"
        }
    ]
}