{
    "hands_on_practices": [
        {
            "introduction": "Genetic Algorithms are not one-size-fits-all; their components must be adapted to the problem's structure. For combinatorial optimization problems, where solutions are often represented as permutations, standard crossover operators would create invalid offspring. This exercise challenges you to implement the Partially Mapped Crossover (PMX), a classic operator designed specifically to preserve the crucial permutation property, ensuring that recombination always produces valid solutions .",
            "id": "4124850",
            "problem": "Consider a genetic algorithm (GA) operating on the permutation representation for combinatorial optimization, where each chromosome is a permutation of a fixed allele set. The fundamental base for this problem is the definition of a permutation as a bijection from an index set to an allele set, along with the GA constraint that crossover must preserve the permutation structure. A permutation of length $n$ is a sequence $(x_0,x_1,\\dots,x_{n-1})$ such that each allele from a fixed set $A$ appears exactly once. Partially Mapped Crossover (PMX) is a crossover operator designed for permutation-coded genetic algorithms that must preserve the bijection constraint inherent to permutations. The PMX operator uses two crossover points to define a mapping between subsequences of two parent permutations and resolves conflicts by repeatedly applying this mapping.\n\nYour task is to implement PMX for two parent permutations with two specified crossover points using the following principled constraints:\n\n- Representation: Each parent is a permutation on the same allele set $A$ of size $n$, meaning both parents consist of the same $n$ distinct alleles in possibly different order.\n- Crossover: Given two crossover points $c_1$ and $c_2$ with $0 \\le c_1  c_2 \\le n-1$, PMX proceeds by:\n  1. Copying the segment from index $c_1$ through index $c_2$ (inclusive) from the first parent into the first offspring, and similarly copying the same segment from the second parent into the second offspring.\n  2. Constructing a mapping between alleles in the copied segments: for each index $i \\in \\{c_1,\\dots,c_2\\}$, record the pair $(p^{(1)}_i \\mapsto p^{(2)}_i)$ for the first offspring and the inverse mapping $(p^{(2)}_i \\mapsto p^{(1)}_i)$ for the second offspring, where $p^{(1)}_i$ and $p^{(2)}_i$ denote the alleles at index $i$ in the first and second parent respectively.\n  3. For positions outside the segment, attempt to fill the first offspring at position $j \\notin [c_1,c_2]$ with $p^{(2)}_j$. If this candidate allele is already present in the copied segment of the first offspring, repeatedly apply the mapping defined in step $2$, replacing the candidate allele $x$ by the mapped value $f(x)$ until it is not in the copied segment, and then place it. Perform the symmetric operation for the second offspring using the inverse mapping and $p^{(1)}_j$ as the initial candidate.\n- Validation: Verify that each resulting offspring is a valid permutation of the allele set $A$, that is, each offspring is of length $n$, contains each allele of $A$ exactly once, and thus forms a bijection.\n\nStarting from these core definitions and constraints, implement PMX in a complete, runnable program, and compute the offspring for the following test suite. For each test case, output a list consisting of the first offspring, a boolean validation of the first offspring, the second offspring, and a boolean validation of the second offspring. The final program output must be a single line that is a JSON array containing the results for all test cases, with no spaces.\n\nUse zero-based indexing for positions, and treat all sequences as permutations over their respective allele sets.\n\nTest Suite:\n- Case $1$ (general case with nontrivial mapping):\n  - $n = 10$\n  - $P_1 = [9,1,8,4,3,7,6,5,2,10]$\n  - $P_2 = [1,10,3,6,2,8,5,4,9,7]$\n  - $c_1 = 3$, $c_2 = 7$\n- Case $2$ (boundary case where the crossover segment spans the entire chromosome):\n  - $n = 10$\n  - $P_1 = [4,3,2,1,5,6,7,8,9,10]$\n  - $P_2 = [10,9,8,7,6,5,4,3,2,1]$\n  - $c_1 = 0$, $c_2 = 9$\n- Case $3$ (mapping chain requires multiple resolution steps):\n  - $n = 8$\n  - $P_1 = [8,3,1,4,5,6,2,7]$\n  - $P_2 = [3,8,5,1,2,7,6,4]$\n  - $c_1 = 2$, $c_2 = 5$\n- Case $4$ (minimal nontrivial length):\n  - $n = 2$\n  - $P_1 = [2,1]$\n  - $P_2 = [1,2]$\n  - $c_1 = 0$, $c_2 = 1$\n\nRequired Final Output Format:\n- Your program should produce a single line of output containing the results as a JSON array without spaces, where each element corresponds to a test case result of the form $[\\text{offspring}_1,\\ \\text{validity}_1,\\ \\text{offspring}_2,\\ \\text{validity}_2]$. Each offspring is a list of integers and each validity is a boolean. For example, an output with two test cases should look like $[[[1,2,3],true,[3,2,1],true],[[4,5],true,[5,4],true]]$.",
            "solution": "The problem presents a task to implement the Partially Mapped Crossover (PMX) operator for permutation-based genetic algorithms. The solution must adhere to the specific procedural definition of PMX and validate the resulting offspring. The problem is scientifically grounded, well-posed, and objective, providing a clear algorithmic specification and a verifiable test suite. Therefore, the problem is valid, and a rigorous, step-by-step solution can be constructed.\n\nThe foundation of this problem lies in combinatorial optimization, where genetic algorithms are employed to find solutions in a vast search space. When a problem's solution can be encoded as a permutation of a set of items (e.g., the traveling-salesperson problem), the chromosomes in the genetic algorithm are permutations. A critical constraint is that genetic operators, such as crossover and mutation, must preserve the permutation property; that is, an offspring produced from two valid permutation parents must also be a valid permutation. Standard crossover operators like one-point or two-point crossover would produce invalid offspring containing duplicate alleles. PMX is a specialized operator designed to overcome this and maintain the bijection from an index set to an allele set that defines a permutation.\n\nLet the two parent permutations be $p^{(1)}$ and $p^{(2)}$, both of length $n$ over an allele set $A$. Let the two crossover points be $c_1$ and $c_2$, with the constraint $0 \\le c_1  c_2 \\le n-1$. The PMX algorithm proceeds as follows to generate two offspring, $o^{(1)}$ and $o^{(2)}$.\n\n**Step 1: Segment Copying**\n\nThe core of PMX involves swapping a segment between the two parents. An offspring template is created for each child, initially empty. The chromosomal segment from $p^{(1)}$ between indices $c_1$ and $c_2$ (inclusive) is copied directly into the corresponding positions in the first offspring, $o^{(1)}$. Symmetrically, the segment from $p^{(2)}$ between the same indices is copied into the second offspring, $o^{(2)}$.\n\nFor $i \\in \\{c_1, \\dots, c_2\\}$:\n$$o^{(1)}_i = p^{(1)}_i$$\n$$o^{(2)}_i = p^{(2)}_i$$\n\n**Step 2: Mapping Construction**\n\nThe copied segments define a mapping between the alleles they contain. For each index $i$ within the crossover segment, we establish a bidirectional correspondence between the allele from $p^{(1)}$ and the allele from $p^{(2)}$. This mapping is the key to resolving conflicts that arise when filling the rest of the chromosomes.\n\nFor the generation of offspring $o^{(1)}$, a mapping $M_{1 \\to 2}$ is created:\n$$M_{1 \\to 2} = \\{ (p^{(1)}_i, p^{(2)}_i) \\mid i \\in \\{c_1, \\dots, c_2\\} \\}$$\nThis can be read as \"$p^{(1)}_i$ maps to $p^{(2)}_i$\".\n\nFor the generation of offspring $o^{(2)}$, the inverse mapping $M_{2 \\to 1}$ is created:\n$$M_{2 \\to 1} = \\{ (p^{(2)}_i, p^{(1)}_i) \\mid i \\in \\{c_1, \\dots, c_2\\} \\}$$\n\n**Step 3: Allele Filling with Conflict Resolution**\n\nThe remaining positions in each offspring (those outside the range $[c_1, c_2]$) are filled. The basic idea is to take the allele from the *other* parent. However, this may introduce a duplicate if that allele was already part of the segment copied in Step 1. The mapping from Step 2 is used to resolve such conflicts.\n\nTo fill positions in $o^{(1)}$ for indices $j \\notin [c_1, c_2]$:\n1.  The initial candidate allele is taken from the second parent, $x = p^{(2)}_j$.\n2.  A check is performed to see if this candidate $x$ is already present in the segment copied into $o^{(1)}$ from $p^{(1)}$. This is equivalent to checking if $x$ is a key in the mapping $M_{1 \\to 2}$.\n3.  If $x$ is not present in the copied segment, it is a valid allele for this position. We set $o^{(1)}_j = x$.\n4.  If $x$ is present, a conflict exists. The allele $x$ is already in $o^{(1)}$ at some position $k \\in [c_1, c_2]$. To resolve this, we use the mapping $M_{1 \\to 2}$ to find a new candidate. We set $x' = M_{1 \\to 2}(x)$ and repeat the check from step 2 with this new candidate $x'$. This process continues, forming a \"mapping chain,\" until a candidate is found that does not create a conflict. This final non-conflicting allele is placed at position $j$, i.e., $o^{(1)}_j = x_{\\text{final}}$.\n\nThe procedure to fill $o^{(2)}$ is symmetric. For indices $j \\notin [c_1, c_2]$:\n1.  The initial candidate allele is $y = p^{(1)}_j$.\n2.  Check if $y$ is present in the segment copied into $o^{(2)}$ from $p^{(2)}$, which is equivalent to checking if $y$ is a key in the mapping $M_{2 \\to 1}$.\n3.  If no conflict, set $o^{(2)}_j = y$.\n4.  If conflict, find the new candidate $y' = M_{2 \\to 1}(y)$ and repeat the check until a non-conflicting allele is found and placed.\n\nThis conflict resolution mechanism guarantees that no allele is duplicated, thus preserving the permutation property.\n\n**Step 4: Validation**\n\nThe final required step is to validate that each resulting offspring, $o^{(1)}$ and $o^{(2)}$, is a valid permutation of the original allele set $A$. A valid permutation must satisfy two conditions:\n1.  Its length must be $n$, the same as the parents.\n2.  The set of its constituent alleles must be identical to the original allele set $A$. That is, $\\text{set}(o) = A$. This single check confirms that all original alleles are present and that there are no duplicates.\n\nBy rigorously following these steps, a correct implementation of the PMX operator can be achieved, producing valid permutation offspring from valid permutation parents.",
            "answer": "```python\nimport numpy as np\n\ndef is_valid_permutation(perm, original_set):\n    \"\"\"\n    Validates if a list is a valid permutation of a given set of alleles.\n    A valid permutation must have the same number of elements and the exact same\n    set of unique elements as the original.\n    \"\"\"\n    return len(perm) == len(original_set) and set(perm) == original_set\n\ndef pmx_crossover(p1, p2, c1, c2):\n    \"\"\"\n    Performs Partially Mapped Crossover (PMX) on two parent permutations.\n\n    Args:\n        p1 (list): The first parent permutation.\n        p2 (list): The second parent permutation.\n        c1 (int): The first crossover point (inclusive).\n        c2 (int): The second crossover point (inclusive).\n\n    Returns:\n        tuple: A tuple containing the two offspring permutations.\n    \"\"\"\n    n = len(p1)\n    # Initialize offspring with None placeholders\n    o1 = [None] * n\n    o2 = [None] * n\n\n    # Step 1: Copy the crossover segments directly\n    o1[c1:c2 + 1] = p1[c1:c2 + 1]\n    o2[c1:c2 + 1] = p2[c1:c2 + 1]\n\n    # Step 2: Create the mappings based on the segments\n    map1_to_2 = {p1[i]: p2[i] for i in range(c1, c2 + 1)}\n    map2_to_1 = {p2[i]: p1[i] for i in range(c1, c2 + 1)}\n\n    # Step 3: Fill the remaining positions for Offspring 1\n    for i in list(range(c1)) + list(range(c2 + 1, n)):\n        candidate = p2[i]\n        # Resolve conflicts by following the mapping chain\n        while candidate in map1_to_2:\n            candidate = map1_to_2[candidate]\n        o1[i] = candidate\n\n    # Step 3 (symmetric): Fill the remaining positions for Offspring 2\n    for i in list(range(c1)) + list(range(c2 + 1, n)):\n        candidate = p1[i]\n        # Resolve conflicts by following the mapping chain\n        while candidate in map2_to_1:\n            candidate = map2_to_1[candidate]\n        o2[i] = candidate\n        \n    return o1, o2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general)\n        ([9, 1, 8, 4, 3, 7, 6, 5, 2, 10], [1, 10, 3, 6, 2, 8, 5, 4, 9, 7], 3, 7),\n        # Case 2 (boundary)\n        ([4, 3, 2, 1, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1], 0, 9),\n        # Case 3 (mapping chain)\n        ([8, 3, 1, 4, 5, 6, 2, 7], [3, 8, 5, 1, 2, 7, 6, 4], 2, 5),\n        # Case 4 (minimal nontrivial)\n        ([2, 1], [1, 2], 0, 1),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        p1, p2, c1, c2 = case\n        original_allele_set = set(p1)\n\n        o1, o2 = pmx_crossover(p1, p2, c1, c2)\n\n        valid1 = is_valid_permutation(o1, original_allele_set)\n        valid2 = is_valid_permutation(o2, original_allele_set)\n\n        # Manually construct JSON-like string to avoid spaces and format booleans correctly\n        o1_str = f\"[{','.join(map(str, o1))}]\"\n        v1_str = str(valid1).lower()\n        o2_str = f\"[{','.join(map(str, o2))}]\"\n        v2_str = str(valid2).lower()\n        \n        result_str = f\"[{o1_str},{v1_str},{o2_str},{v2_str}]\"\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format (no spaces)\n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The choice of representation is a cornerstone of effective Genetic Algorithm (GA) design, as it defines the landscape the algorithm will search. A desirable property is locality, where small genotypic changes correspond to small phenotypic changes. This practice guides you through an analytical comparison of two common representations—real-valued and binary fixed-point—by calculating the expected fitness change caused by a small mutation on a convex quadratic landscape . This exercise sharpens your analytical skills and reveals how representation choices influence the very nature of local search.",
            "id": "4124812",
            "problem": "A research group is benchmarking mutation locality in Genetic Algorithms (GA) for Complex Adaptive Systems models by comparing two representations on the same convex quadratic objective. The objective is given by $f(\\mathbf{x}) = \\tfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{A} (\\mathbf{x} - \\boldsymbol{\\mu})$, where $\\mathbf{x} \\in \\mathbb{R}^d$, $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$, and $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite. The representation and mutation operators are:\n\nRepresentation R (real-valued): each genome encodes $\\mathbf{x}$ directly. Mutation is additive Gaussian, $\\mathbf{x}^\\prime = \\mathbf{x} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$, where $\\sigma  0$ and $\\mathbf{I}_d$ is the $d \\times d$ identity matrix.\n\nRepresentation B (binary fixed-point): each coordinate is encoded with $m$ bits at resolution $\\Delta  0$, so that a unit change in the least significant bit (LSB) corresponds to a change of magnitude $\\Delta$ in that coordinate. A small mutation is defined as: select a coordinate index $i \\in \\{1, \\dots, d\\}$ uniformly at random, then flip its LSB, which changes the phenotype by $\\delta \\mathbf{e}_i$ with $\\delta \\in \\{+\\Delta, -\\Delta\\}$ with equal probability and $\\mathbf{e}_i$ the $i$-th standard basis vector. Assume the current $\\mathbf{x}$ is strictly interior to the representable domain so that no boundary clipping occurs.\n\nUsing only general definitions and properties of expectations and convex quadratic forms, determine the exact conditional expectation of the fitness change $f(\\mathbf{x}^\\prime) - f(\\mathbf{x})$ given $\\mathbf{x}$ under each representation, and identify the correct comparison of mutation locality between the two encodings. Here, mutation locality is understood as the expected increase in $f$ under a small random perturbation, with smaller expected increases indicating better locality near $\\mathbf{x}$.\n\nWhich option is correct?\n\nA. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{\\Delta^2}{2 d} \\operatorname{tr}(\\mathbf{A})$. Therefore, if $\\Delta^2 / d = \\sigma^2$, both encodings have identical expected fitness change and thus identical mutation locality in expectation on the convex quadratic.\n\nB. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = 0$ by symmetry of the Gaussian. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = 0$ by symmetry of the LSB flip. Therefore, both encodings exhibit perfect neutrality for any $\\sigma$ and $\\Delta$.\n\nC. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\sigma^2 \\lambda_{\\max}(\\mathbf{A})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{\\Delta^2}{2 d} \\lambda_{\\min}(\\mathbf{A})$. Therefore, the real-valued encoding is less local whenever $\\sigma^2  \\Delta^2 / (2 d)$.\n\nD. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\Delta^2 \\operatorname{tr}(\\mathbf{A})$. Therefore, for any fixed $\\sigma$ and $\\Delta$, the binary encoding is less local by a factor of $d$.\n\nE. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A}) + \\tfrac{1}{2} \\sigma^2 (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{A} (\\mathbf{x} - \\boldsymbol{\\mu})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{\\Delta^2}{2 d} \\operatorname{tr}(\\mathbf{A})$. Therefore, the real-valued encoding becomes less local as $\\mathbf{x}$ moves away from $\\boldsymbol{\\mu}$, even for fixed $\\sigma$.",
            "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Objective function: $f(\\mathbf{x}) = \\tfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{A} (\\mathbf{x} - \\boldsymbol{\\mu})$\n- Variables and parameters: $\\mathbf{x}, \\boldsymbol{\\mu} \\in \\mathbb{R}^d$, $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive definite matrix.\n- Representation R (real-valued):\n  - Encoding: $\\mathbf{x}$ directly.\n  - Mutation: $\\mathbf{x}^\\prime = \\mathbf{x} + \\boldsymbol{\\varepsilon}$, where the perturbation $\\boldsymbol{\\varepsilon}$ is a random variable drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$, with $\\sigma  0$ and $\\mathbf{I}_d$ being the $d \\times d$ identity matrix.\n- Representation B (binary fixed-point):\n  - Encoding: Each coordinate of $\\mathbf{x}$ is encoded with $m$ bits at a resolution $\\Delta  0$.\n  - Mutation: A coordinate index $i \\in \\{1, \\dots, d\\}$ is selected uniformly at random. Its least significant bit (LSB) is flipped. The resulting change in the phenotype vector $\\mathbf{x}$ is $\\delta \\mathbf{e}_i$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector and $\\delta$ is a random variable with $P(\\delta=+\\Delta) = P(\\delta=-\\Delta) = 1/2$. The new phenotype is $\\mathbf{x}^\\prime = \\mathbf{x} + \\delta \\mathbf{e}_i$.\n  - Assumption: No boundary clipping occurs.\n- Quantity to be determined: The conditional expectation of the fitness change, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}]$, for each representation.\n- Definition of mutation locality: A smaller expected increase in $f$ indicates better locality.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It utilizes standard mathematical concepts from linear algebra, probability theory, and evolutionary computation. The objective function is a convex quadratic, a standard benchmark. The mutation operators are clearly defined. The definition of mutation for Representation B, which models the effect of an LSB flip as a symmetric probabilistic change of $\\pm\\Delta$, is a valid modeling choice that ensures the problem is self-contained and unambiguously solvable without needing to specify the exact binary encoding scheme (e.g., two's complement). All necessary information is provided.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Expected Fitness Change\n\nLet the change in phenotype be the vector $\\boldsymbol{\\delta}_{\\text{mut}}$, so that $\\mathbf{x}^\\prime = \\mathbf{x} + \\boldsymbol{\\delta}_{\\text{mut}}$. The change in fitness is:\n$$ f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) = \\tfrac{1}{2} (\\mathbf{x} + \\boldsymbol{\\delta}_{\\text{mut}} - \\boldsymbol{\\mu})^\\top \\mathbf{A} (\\mathbf{x} + \\boldsymbol{\\delta}_{\\text{mut}} - \\boldsymbol{\\mu}) - \\tfrac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{A} (\\mathbf{x} - \\boldsymbol{\\mu}) $$\nLet $\\mathbf{y} = \\mathbf{x} - \\boldsymbol{\\mu}$. The expression becomes:\n$$ f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) = \\tfrac{1}{2} (\\mathbf{y} + \\boldsymbol{\\delta}_{\\text{mut}})^\\top \\mathbf{A} (\\mathbf{y} + \\boldsymbol{\\delta}_{\\text{mut}}) - \\tfrac{1}{2} \\mathbf{y}^\\top \\mathbf{A} \\mathbf{y} $$\nExpanding the first term:\n$$ \\tfrac{1}{2} (\\mathbf{y}^\\top\\mathbf{A}\\mathbf{y} + \\mathbf{y}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}} + \\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\mathbf{y} + \\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}}) $$\nSince $\\mathbf{A}$ is symmetric, $\\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\mathbf{y} = (\\mathbf{y}^\\top\\mathbf{A}^\\top\\boldsymbol{\\delta}_{\\text{mut}})^\\top = (\\mathbf{y}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}})^\\top$. As this is a scalar, it equals its transpose. Thus, the two cross-terms are equal.\n$$ f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{y}^\\top\\mathbf{A}\\mathbf{y} + 2\\mathbf{y}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}} + \\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}}) - \\tfrac{1}{2}\\mathbf{y}^\\top\\mathbf{A}\\mathbf{y} $$\n$$ f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) = \\mathbf{y}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}} + \\tfrac{1}{2}\\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}} $$\nTaking the expectation conditional on $\\mathbf{x}$ (and thus on $\\mathbf{y}$):\n$$ \\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\mathbb{E}[\\mathbf{y}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}}] + \\mathbb{E}[\\tfrac{1}{2}\\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}}] $$\n$$ \\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\mathbf{y}^\\top\\mathbf{A}\\mathbb{E}[\\boldsymbol{\\delta}_{\\text{mut}}] + \\tfrac{1}{2}\\mathbb{E}[\\boldsymbol{\\delta}_{\\text{mut}}^\\top\\mathbf{A}\\boldsymbol{\\delta}_{\\text{mut}}] $$\n\n#### Representation R (real-valued)\nFor this representation, the perturbation is $\\boldsymbol{\\delta}_{\\text{mut}} = \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_d)$.\nThe expectation of the perturbation is $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$. The first term vanishes: $\\mathbf{y}^\\top\\mathbf{A}\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$.\nFor the second term, we evaluate the expectation of the quadratic form $\\boldsymbol{\\varepsilon}^\\top\\mathbf{A}\\boldsymbol{\\varepsilon}$. Using the property $\\mathbb{E}[\\mathbf{z}^\\top \\mathbf{M} \\mathbf{z}] = \\operatorname{tr}(\\mathbf{M} \\boldsymbol{\\Sigma}_\\mathbf{z}) + \\boldsymbol{\\mu}_\\mathbf{z}^\\top \\mathbf{M} \\boldsymbol{\\mu}_\\mathbf{z}$, with $\\mathbf{z}=\\boldsymbol{\\varepsilon}$, $\\mathbf{M}=\\mathbf{A}$, $\\boldsymbol{\\mu}_\\mathbf{z}=\\mathbf{0}$, and $\\boldsymbol{\\Sigma}_\\mathbf{z} = \\sigma^2 \\mathbf{I}_d$.\n$$ \\mathbb{E}[\\boldsymbol{\\varepsilon}^\\top\\mathbf{A}\\boldsymbol{\\varepsilon}] = \\operatorname{tr}(\\mathbf{A} (\\sigma^2 \\mathbf{I}_d)) + \\mathbf{0}^\\top\\mathbf{A}\\mathbf{0} = \\operatorname{tr}(\\sigma^2\\mathbf{A}) = \\sigma^2 \\operatorname{tr}(\\mathbf{A}) $$\nThus, the expected fitness change for Representation R is:\n$$ \\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = 0 + \\tfrac{1}{2}(\\sigma^2 \\operatorname{tr}(\\mathbf{A})) = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A}) $$\n\n#### Representation B (binary fixed-point)\nFor this representation, the perturbation is $\\boldsymbol{\\delta}_{\\text{mut}} = \\boldsymbol{\\eta} = \\delta \\mathbf{e}_i$. The randomness is over the index $i \\in \\{1, \\dots, d\\}$ (uniform) and the sign variable $\\delta \\in \\{+\\Delta, -\\Delta\\}$ (uniform).\nFirst, we compute the expectation of the perturbation $\\boldsymbol{\\eta}$:\n$$ \\mathbb{E}[\\boldsymbol{\\eta}] = \\mathbb{E}_i[\\mathbb{E}_\\delta[\\delta \\mathbf{e}_i \\mid i]] = \\mathbb{E}_i[\\mathbf{e}_i \\mathbb{E}_\\delta[\\delta]] $$\nThe expectation of $\\delta$ is $\\mathbb{E}[\\delta] = (+\\Delta)(1/2) + (-\\Delta)(1/2) = 0$.\nTherefore, $\\mathbb{E}[\\boldsymbol{\\eta}] = \\mathbf{0}$, and the first term $\\mathbf{y}^\\top\\mathbf{A}\\mathbb{E}[\\boldsymbol{\\eta}]$ is zero.\nFor the second term, we evaluate $\\mathbb{E}[\\boldsymbol{\\eta}^\\top\\mathbf{A}\\boldsymbol{\\eta}]$.\n$$ \\boldsymbol{\\eta}^\\top\\mathbf{A}\\boldsymbol{\\eta} = (\\delta \\mathbf{e}_i)^\\top\\mathbf{A}(\\delta \\mathbf{e}_i) = \\delta^2 (\\mathbf{e}_i^\\top\\mathbf{A}\\mathbf{e}_i) = \\delta^2 A_{ii} $$\nwhere $A_{ii}$ is the $i$-th diagonal element of $\\mathbf{A}$. Now we take the expectation over $i$ and $\\delta$. As they are independent:\n$$ \\mathbb{E}[\\boldsymbol{\\eta}^\\top\\mathbf{A}\\boldsymbol{\\eta}] = \\mathbb{E}[\\delta^2 A_{ii}] = \\mathbb{E}[\\delta^2] \\mathbb{E}[A_{ii}] $$\nThe expectation of $\\delta^2$ is $\\mathbb{E}[\\delta^2] = (+\\Delta)^2(1/2) + (-\\Delta)^2(1/2) = \\Delta^2$.\nThe expectation of $A_{ii}$ over the uniform choice of $i$ is:\n$$ \\mathbb{E}[A_{ii}] = \\sum_{k=1}^d A_{kk} P(i=k) = \\sum_{k=1}^d A_{kk} \\frac{1}{d} = \\frac{1}{d} \\operatorname{tr}(\\mathbf{A}) $$\nCombining these, $\\mathbb{E}[\\boldsymbol{\\eta}^\\top\\mathbf{A}\\boldsymbol{\\eta}] = \\Delta^2 \\frac{1}{d}\\operatorname{tr}(\\mathbf{A})$.\nThus, the expected fitness change for Representation B is:\n$$ \\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = 0 + \\tfrac{1}{2}\\left(\\frac{\\Delta^2}{d}\\operatorname{tr}(\\mathbf{A})\\right) = \\tfrac{\\Delta^2}{2d}\\operatorname{tr}(\\mathbf{A}) $$\n\n### Option-by-Option Analysis\n\nA. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{\\Delta^2}{2 d} \\operatorname{tr}(\\mathbf{A})$. Therefore, if $\\Delta^2 / d = \\sigma^2$, both encodings have identical expected fitness change and thus identical mutation locality in expectation on the convex quadratic.\n- The derived expression for Rep R is $\\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A})$, which matches.\n- The derived expression for Rep B is $\\tfrac{\\Delta^2}{2 d} \\operatorname{tr}(\\mathbf{A})$, which matches.\n- The comparison is as follows: setting the two expressions equal, $\\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A}) = \\tfrac{\\Delta^2}{2d}\\operatorname{tr}(\\mathbf{A})$. Since $\\mathbf{A}$ is positive definite, its trace $\\operatorname{tr}(\\mathbf{A})  0$, so we can divide by it and by $\\tfrac{1}{2}$ to obtain $\\sigma^2 = \\Delta^2/d$. This condition correctly equates the two expected fitness changes. The conclusion about identical locality under this condition is correct.\n- Verdict: **Correct**.\n\nB. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = 0$ by symmetry of the Gaussian. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = 0$ by symmetry of the LSB flip. Therefore, both encodings exhibit perfect neutrality for any $\\sigma$ and $\\Delta$.\n- The expected fitness change for Rep R is $\\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A})$, which is strictly positive, not $0$. The symmetry of the mutation distribution makes the first-order term in the Taylor expansion average to zero, but not the second-order term.\n- Similarly, the expected fitness change for Rep B is $\\tfrac{\\Delta^2}{2d}\\operatorname{tr}(\\mathbf{A})$, which is also strictly positive, not $0$.\n- The premises are false.\n- Verdict: **Incorrect**.\n\nC. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\sigma^2 \\lambda_{\\max}(\\mathbf{A})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{\\Delta^2}{2 d} \\lambda_{\\min}(\\mathbf{A})$. Therefore, the real-valued encoding is less local whenever $\\sigma^2  \\Delta^2 / (2 d)$.\n- The derived expressions involve $\\operatorname{tr}(\\mathbf{A})$, which is the sum of eigenvalues, not the maximum eigenvalue $\\lambda_{\\max}(\\mathbf{A})$ or the minimum eigenvalue $\\lambda_{\\min}(\\mathbf{A})$. Both formulas are incorrect.\n- Verdict: **Incorrect**.\n\nD. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\Delta^2 \\operatorname{tr}(\\mathbf{A})$. Therefore, for any fixed $\\sigma$ and $\\Delta$, the binary encoding is less local by a factor of $d$.\n- The expression for Rep R is correct.\n- The expression for Rep B is incorrect; it is missing the factor of $1/d$. The correct expression is $\\tfrac{\\Delta^2}{2d}\\operatorname{tr}(\\mathbf{A})$.\n- The conclusion is based on faulty premises.\n- Verdict: **Incorrect**.\n\nE. Under Representation R, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{1}{2} \\sigma^2 \\operatorname{tr}(\\mathbf{A}) + \\tfrac{1}{2} \\sigma^2 (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{A} (\\mathbf{x} - \\boldsymbol{\\mu})$. Under Representation B, $\\mathbb{E}[f(\\mathbf{x}^\\prime) - f(\\mathbf{x}) \\mid \\mathbf{x}] = \\tfrac{\\Delta^2}{2 d} \\operatorname{tr}(\\mathbf{A})$. Therefore, the real-valued encoding becomes less local as $\\mathbf{x}$ moves away from $\\boldsymbol{\\mu}$, even for fixed $\\sigma$.\n- The expression for Rep R is incorrect. Our derivation shows that the expected fitness change is independent of $\\mathbf{x}$. The term depending on $\\mathbf{x}$ in the formula is spurious.\n- The expression for Rep B is correct.\n- The conclusion is based on a faulty premise about Rep R.\n- Verdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Why do Genetic Algorithms sometimes fail? One critical reason is 'deception,' where the fitness landscape misleads the search toward a suboptimal solution. This practice explores this phenomenon by modeling the GA's population dynamics on a specially constructed deceptive function. By applying the replicator equation and numerically simulating the system, you will determine the basins of attraction for both the deceptive local optimum and the true global optimum . This advanced exercise demonstrates how tools from dynamical systems can provide deep insights into the collective behavior and potential pitfalls of evolutionary search.",
            "id": "4124845",
            "problem": "You are modeling a Genetic Algorithm (GA) within the broader framework of Complex Adaptive Systems (CAS). Consider binary-string representation with blockwise unitation-based fitness, under infinite population and proportional selection dynamics. Assume complete mixing recombination (linkage equilibrium) so that the genotype distribution factorizes into independent per-locus Bernoulli trials with common allele-$1$ frequency. Your task is to construct a deceptive fitness function on a single block and, under proportional selection, compute the basin of attraction volumes for the deceptive local optimum versus the global optimum by analyzing the allele-frequency replicator mapping.\n\nFundamental base. Use the following foundational facts:\n- Proportional selection (the replicator equation) updates the probability mass function $p(x)$ over genotypes $x$ according to $p'(x) = \\dfrac{p(x) f(x)}{\\mathbb{E}[f(X)]}$, where $f(x)$ is fitness and $\\mathbb{E}[f(X)]$ is the expected fitness under $p(x)$.\n- Under complete mixing recombination and binary loci, if a block is $k$ bits long and each locus is an independent Bernoulli with allele-$1$ frequency $p \\in [0,1]$, then the distribution of unitation (number of ones) $U \\in \\{0,1,\\dots,k\\}$ is binomial with probability mass\n$$\n\\Pr(U=u) = \\binom{k}{u} p^u (1-p)^{k-u}.\n$$\n- The allele-$1$ frequency update under proportional selection on block fitness is\n$$\np' = \\dfrac{\\mathbb{E}\\left[\\dfrac{U}{k} f(U)\\right]}{\\mathbb{E}[f(U)]},\n$$\nwhere expectations are taken under the binomial law for $U$.\n\nDeceptive block fitness. Define a single-block deceptive trap function on unitation $u$ for block length $k$ as follows. Let the global optimum be at $u=k$ with fitness $H$, and for all $u \\in \\{0,1,\\dots,k-1\\}$ define a strictly decreasing affine fitness with respect to $u$:\n$$\nf(u) =\n\\begin{cases}\nH,  \\text{if } u = k, \\\\\nb - a \\dfrac{u}{k},  \\text{if } u \\in \\{0,1,\\dots,k-1\\},\n\\end{cases}\n$$\nwith parameters $H  0$, $b  0$, $a  0$, and $b  H$ so that the local deceptive optimum at $u=0$ has fitness $f(0) = b$ that is lower than the global optimum $H$ yet exceeds all other $f(u)$ for $u \\in \\{1,\\dots,k-1\\}$. This construction ensures that selection acting on partial progress toward the global optimum ($u$ increasing but $uk$) appears deleterious, attractively pulling the population toward $u=0$ from a range of initial allele frequencies.\n\nReplicator mapping. Under the binomial distribution of $U$ with parameter $p$, the proportional selection update induces the one-dimensional allele-frequency map\n$$\np' = \\Phi(p;k,H,b,a) = \\frac{\\sum_{u=0}^{k} \\left(\\frac{u}{k}\\right) \\binom{k}{u} p^u (1-p)^{k-u} f(u)}{\\sum_{u=0}^{k} \\binom{k}{u} p^u (1-p)^{k-u} f(u)}.\n$$\nThe fixed points at $p=0$ and $p=1$ correspond to the deceptive local optimum and the true global optimum, respectively. For parameter ranges inducing deception, there exists an unstable interior separatrix $p^\\star \\in (0,1)$ satisfying $p^\\star = \\Phi(p^\\star;k,H,b,a)$ that partitions the interval $[0,1]$ into two basins of attraction under iteration of $\\Phi$: initial conditions $p \\in [0,p^\\star)$ flow to $p=0$ and those $p \\in (p^\\star,1]$ flow to $p=1$.\n\nTask. For each parameter set in the test suite below, numerically compute the basin of attraction volumes for the deceptive local optimum and the global optimum, defined as the Lebesgue measure of the subsets of $p \\in (0,1)$ whose forward iterates under the map $p_{t+1} = \\Phi(p_t;k,H,b,a)$ converge to $0$ and $1$, respectively. Use a uniform grid over the open interval $(0,1)$ to approximate these volumes by fractions of grid points converging to each attractor. Iterate until convergence with a small tolerance in $p$, stopping when the change $|p_{t+1} - p_t|$ falls below a specified threshold or a maximum number of iterations is reached. Report the two basin volumes as decimal numbers for each test case.\n\nTest suite. Use these parameter values:\n- Case $1$: $k=4$, $H=1.0$, $b=0.90$, $a=0.90$.\n- Case $2$: $k=5$, $H=1.0$, $b=0.85$, $a=0.70$.\n- Case $3$: $k=6$, $H=1.0$, $b=0.99$, $a=0.79$.\n- Case $4$ (non-deceptive baseline): $k=5$, $H=1.0$, $b=0.10$, $a=0.00$.\n- Case $5$ (boundary block size): $k=1$, $H=1.0$, $b=0.90$, $a=0.90$.\n\nComputation details. Use a uniform grid of $N=2001$ points over $(0,1)$, excluding endpoints, namely $p_i = \\dfrac{i}{N+1}$ for $i \\in \\{1,2,\\dots,N\\}$. Use maximum iteration count $T_{\\max} = 1000$ and convergence tolerance $\\epsilon = 10^{-12}$. Classify a trajectory as converging to the deceptive optimum if its final $p$ after iteration is less than $10^{-6}$, to the global optimum if its final $p$ exceeds $1 - 10^{-6}$, and in any ambiguous remaining case by the sign of the last update step $p_{t+1} - p_t$ (negative implies flow to $0$, positive implies flow to $1$).\n\nRequired output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[V_{\\text{deceptive}},V_{\\text{global}}]$ of decimal numbers rounded to six digits after the decimal point, in the order of the test suite given above. For example, the output format must be of the form $$[[v_{1d},v_{1g}],[v_{2d},v_{2g}],\\dots,[v_{5d},v_{5g}]].$$",
            "solution": "The user has provided a problem in the domain of genetic algorithms and complex adaptive systems, which requires the analysis of a one-dimensional discrete dynamical system. The validation process is as follows.\n\n### Step 1: Extract Givens\n- **Model**: Infinite population genetic algorithm with proportional selection and complete mixing.\n- **Genome**: A block of $k$ binary loci. State is described by allele-$1$ frequency $p \\in [0,1]$.\n- **Unitation Distribution**: The number of ones, $U$, in a block follows a binomial distribution, $U \\sim \\text{Binomial}(k, p)$.\n- **Fitness Function**: A deceptive trap function defined on unitation $u \\in \\{0, 1, \\dots, k\\}$:\n$$\nf(u) =\n\\begin{cases}\nH,  \\text{if } u = k, \\\\\nb - a \\dfrac{u}{k},  \\text{if } u \\in \\{0,1,\\dots,k-1\\},\n\\end{cases}\n$$\nwith parameters $H  0$, $b  0$, $a  0$, and $b  H$.\n- **Replicator Map**: The allele-$1$ frequency $p$ is updated according to the discrete map $p' = \\Phi(p; k, H, b, a)$, given by:\n$$\np' = \\frac{\\mathbb{E}\\left[\\frac{U}{k} f(U)\\right]}{\\mathbb{E}[f(U)]} = \\frac{\\sum_{u=0}^{k} \\left(\\frac{u}{k}\\right) \\binom{k}{u} p^u (1-p)^{k-u} f(u)}{\\sum_{u=0}^{k} \\binom{k}{u} p^u (1-p)^{k-u} f(u)}.\n$$\n- **Task**: Compute the basin of attraction volumes for the fixed points $p=0$ (deceptive optimum) and $p=1$ (global optimum).\n- **Numerical Method**:\n    - Discretize the interval $(0,1)$ with a uniform grid of $N=2001$ points.\n    - For each grid point, iterate the map $p_{t+1}=\\Phi(p_t)$ up to $T_{\\max}=1000$ times or until $|p_{t+1}-p_t|  \\epsilon = 10^{-12}$.\n    - Classify convergence: to $p=0$ if final $p  10^{-6}$, to $p=1$ if final $p  1-10^{-6}$. Ambiguous cases are decided by the sign of the last iteration step.\n- **Test Cases**:\n    - 1: $(k, H, b, a) = (4, 1.0, 0.90, 0.90)$\n    - 2: $(k, H, b, a) = (5, 1.0, 0.85, 0.70)$\n    - 3: $(k, H, b, a) = (6, 1.0, 0.99, 0.79)$\n    - 4: $(k, H, b, a) = (5, 1.0, 0.10, 0.00)$\n    - 5: $(k, H, b, a) = (1, 1.0, 0.90, 0.90)$\n- **Output**: A list of pairs $[V_{\\text{deceptive}}, V_{\\text{global}}]$ for each test case, with values rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the established theory of population genetics and evolutionary computation. The replicator equation for allele frequency under proportional selection is a standard model. The deceptive fitness function is a classic construction used to study the limitations of simple hill-climbing dynamics in GAs. The problem is well-posed, providing a clear objective and a complete, numerically sound procedure for achieving it. All parameters and constants are defined, and there are no contradictions. The computational task is feasible. The language is objective and precise. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be developed.\n\n### Solution\n\nThe core of the problem is to analyze the dynamics of the one-dimensional replicator map $p' = \\Phi(p)$. A direct numerical computation of the sums in the definition of $\\Phi(p)$ for each iteration would be inefficient. A more elegant and computationally superior approach is to derive a closed-form analytical expression for $\\Phi(p)$.\n\nLet $P(u) = \\binom{k}{u} p^u (1-p)^{k-u}$ be the binomial probability mass function. The denominator of $\\Phi(p)$ is the expected fitness $\\mathbb{E}[f(U)]$. We can separate the term for the global optimum $u=k$:\n$$\n\\mathbb{E}[f(U)] = \\sum_{u=0}^{k-1} P(u) \\left(b - a\\frac{u}{k}\\right) + P(k)H\n$$\nUsing the linearity of expectation and properties of the binomial distribution, where $\\mathbb{E}[U] = kp$, we can simplify the sum:\n$$\n\\sum_{u=0}^{k-1} P(u) \\left(b - a\\frac{u}{k}\\right) = b \\sum_{u=0}^{k-1} P(u) - \\frac{a}{k} \\sum_{u=0}^{k-1} u P(u)\n$$\nThe sums are over a partial range. We use $\\sum_{u=0}^k P(u)=1$ and $\\sum_{u=0}^k uP(u) = \\mathbb{E}[U] = kp$.\n$$\n\\sum_{u=0}^{k-1} P(u) = 1 - P(k) = 1 - p^k\n$$\n$$\n\\sum_{u=0}^{k-1} u P(u) = \\mathbb{E}[U] - k P(k) = kp - k p^k\n$$\nSubstituting these back gives the denominator:\n$$\n\\mathbb{E}[f(U)] = b(1-p^k) - \\frac{a}{k}(kp - kp^k) + p^k H = b - ap + (H - b + a)p^k\n$$\nSimilarly, the numerator of $\\Phi(p)$ is $\\mathbb{E}[\\frac{U}{k}f(U)]$. Following a similar procedure and using the second moment $\\mathbb{E}[U^2] = \\text{Var}(U) + (\\mathbb{E}[U])^2 = kp(1-p) + (kp)^2$:\n$$\n\\mathbb{E}\\left[\\frac{U}{k}f(U)\\right] = \\sum_{u=0}^{k-1} \\frac{u}{k} P(u) \\left(b - a\\frac{u}{k}\\right) + \\frac{k}{k} P(k)H\n$$\n$$\n= \\frac{b}{k}\\sum_{u=0}^{k-1} u P(u) - \\frac{a}{k^2}\\sum_{u=0}^{k-1} u^2 P(u) + p^k H\n$$\n$$\n= \\frac{b}{k}(kp - kp^k) - \\frac{a}{k^2}(\\mathbb{E}[U^2] - k^2 P(k)) + p^k H\n$$\n$$\n= b(p - p^k) - \\frac{a}{k^2}(kp(1-p) + (kp)^2 - k^2 p^k) + p^k H\n$$\nAfter simplification, the numerator becomes:\n$$\n\\mathbb{E}\\left[\\frac{U}{k}f(U)\\right] = p\\left(b - \\frac{a}{k}\\right) + p^2\\left(\\frac{a}{k} - a\\right) + p^k(H - b + a)\n$$\nThus, the map $\\Phi(p)$ is a rational function of $p$ and $p^k$:\n$$\n\\Phi(p) = \\frac{p(b - a/k) + p^2(a/k - a) + p^k(H - b + a)}{b - ap + (H - b + a)p^k}\n$$\nThis analytical form is numerically stable and efficient to compute.\n\nThe overall algorithm proceeds as follows:\n1.  For each test case, defined by parameters $(k, H, b, a)$, initialize counters for the deceptive and global basins to zero.\n2.  Generate a uniform grid of $N=2001$ initial points $p_0$ in the interval $(0,1)$.\n3.  For each initial point $p_0$:\n    a. Iterate the map $p_{t+1} = \\Phi(p_t)$, starting with $p_0$.\n    b. The iteration stops if the change $|p_{t+1} - p_t|$ is less than the tolerance $\\epsilon=10^{-12}$ (convergence) or the maximum number of iterations $T_{\\max}=1000$ is reached.\n    c. The final value of the trajectory, $p_{\\text{final}}$, is classified. If $p_{\\text{final}}  10^{-6}$, the trajectory is counted towards the deceptive basin. If $p_{\\text{final}}  1 - 10^{-6}$, it is counted towards the global basin.\n    d. If the final point is in the ambiguous region between these thresholds, its basin is determined by the direction of the final iteration step: a negative step implies convergence to $p=0$, and a non-negative step implies convergence to $p=1$.\n4. After all grid points have been classified, the volume of each basin of attraction is approximated by the fraction of points that converged to the corresponding attractor. Specifically, $V_{\\text{deceptive}} = \\text{count}_{\\text{deceptive}}/N$ and $V_{\\text{global}} = \\text{count}_{\\text{global}}/N$.\n5. The results for each test case are formatted as required and collected for the final output. The Python program below implements this procedure.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the basin of attraction volumes for a genetic algorithm's \n    replicator dynamics on a deceptive fitness function.\n    \"\"\"\n\n    # Define parameters from the problem statement\n    test_cases = [\n        # k, H, b, a\n        (4, 1.0, 0.90, 0.90),\n        (5, 1.0, 0.85, 0.70),\n        (6, 1.0, 0.99, 0.79),\n        (5, 1.0, 0.10, 0.00),  # non-deceptive baseline\n        (1, 1.0, 0.90, 0.90),  # boundary block size k=1\n    ]\n    N = 2001\n    T_max = 1000\n    epsilon = 1e-12\n    conv_thresh_low = 1e-6\n    conv_thresh_high = 1.0 - conv_thresh_low\n\n    # List to store formatted results for each case\n    all_results = []\n    \n    def phi_map(p, k, H, b, a):\n        \"\"\"\n        Computes the next allele frequency p' = Phi(p) using the derived \n        analytical expression for the replicator map.\n        \"\"\"\n        if p == 0.0:\n            return 0.0\n        if p == 1.0:\n            return 1.0\n        \n        # Use high-precision floating point numbers for intermediate calculations\n        p, k, H, b, a = map(np.float64, [p, k, H, b, a])\n        \n        pk = np.power(p, k)\n        \n        # Denominator: E[f(U)] = b - a*p + (H - b + a)*p^k\n        # Numerator: E[(U/k)*f(U)] = p*(b-a/k) + p^2*(a/k - a) + p^k*(H-b+a)\n        \n        C3 = H - b + a\n        \n        denominator = b - a * p + C3 * pk\n        \n        # Guard against k=0 which is not in test cases but good practice\n        if k  0:\n            C1 = b - a / k\n            C2 = a / k - a\n            numerator = p * C1 + np.power(p, 2) * C2 + pk * C3\n        else:\n             numerator = 0.0\n\n        # This condition should not be met with the problem's constraints (H0, b0)\n        # but is included as a safeguard.\n        if denominator == 0:\n            return p \n\n        return numerator / denominator\n\n    # Iterate over each test case\n    for k, H, b, a in test_cases:\n        deceptive_count = 0\n        global_count = 0\n        \n        # Create a uniform grid of N points over the open interval (0, 1)\n        grid_points = np.linspace(0.0, 1.0, N + 2, dtype=np.float64)[1:-1]\n\n        # For each initial point, iterate the map to find its attractor\n        for p_initial in grid_points:\n            p_current = p_initial\n            last_step_delta = 0.0\n\n            for _ in range(T_max):\n                p_next = phi_map(p_current, k, H, b, a)\n                \n                # Clamp values to [0,1] to handle potential minor floating point inaccuracies\n                if p_next  0.0: p_next = 0.0\n                if p_next  1.0: p_next = 1.0\n\n                last_step_delta = p_next - p_current\n                \n                if np.abs(last_step_delta)  epsilon:\n                    p_current = p_next\n                    break\n                \n                p_current = p_next\n            \n            p_final = p_current\n            \n            # Classify trajectory based on the final position\n            if p_final  conv_thresh_low:\n                deceptive_count += 1\n            elif p_final  conv_thresh_high:\n                global_count += 1\n            else:\n                # Classify ambiguous cases by the sign of the last update step\n                if last_step_delta  0:\n                    deceptive_count += 1\n                else:  # Includes last_step_delta = 0\n                    global_count += 1\n                    \n        # Calculate basin volumes as fractions of the grid\n        v_d = deceptive_count / N\n        v_g = global_count / N\n        \n        result_str = f\"[{v_d:.6f},{v_g:.6f}]\"\n        all_results.append(result_str)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}