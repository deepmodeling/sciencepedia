{
    "hands_on_practices": [
        {
            "introduction": "Before applying powerful causality detection methods like Granger causality or Transfer Entropy, we must first ensure our underlying model of the system is valid. For Vector Autoregressive (VAR) models, this means verifying covariance-stationarity, which guarantees that the system's statistical properties do not change over time. This essential diagnostic practice guides you through assessing model stability by analyzing the eigenvalues of the companion matrix, a fundamental skill for any rigorous time series analysis .",
            "id": "4116835",
            "problem": "You are given a set of vector autoregression models and must decide their stability by constructing and analyzing the eigenvalues of the block companion matrix, using a principled derivation from linear systems concepts. The context is complex adaptive systems modeling where causality detection methods such as Granger causality (GC) and transfer entropy (TE) rely on the existence of a stationary distribution for the underlying multivariate time series. A Vector Autoregression (VAR) of order $p$ for a $k$-dimensional process is defined by the fundamental relation $X_t = \\sum_{i=1}^{p} A_i X_{t-i} + \\varepsilon_t$, where $X_t \\in \\mathbb{R}^k$ is the state, $A_i \\in \\mathbb{R}^{k \\times k}$ are coefficient matrices, and $\\varepsilon_t$ is a zero-mean white noise. In a stable Linear Time-Invariant (LTI) realization obtained by state augmentation, the state transition matrix has all eigenvalues strictly inside the unit circle in the complex plane. Your task is to, for each provided model, derive and construct the appropriate block companion matrix from first principles and then compute its eigenvalues to assess stability. Classify each model as stable, boundary, or unstable according to the following rule based on the spectral radius: if the maximum magnitude of the eigenvalues is strictly less than $1$, classify as stable; if it equals $1$ within a tolerance $\\tau$, classify as boundary; if greater than $1$, classify as unstable. Use a numerical comparison tolerance of $\\tau = 10^{-10}$ when comparing to $1$. The final output for each test case must be a two-item list containing an integer classification code and a float spectral radius, where stable is encoded as $1$, boundary as $0$, and unstable as $-1$. The spectral radius must be rounded to $6$ decimal places.\n\nImplement the algorithm to:\n- construct the block companion matrix $C \\in \\mathbb{R}^{kp \\times kp}$ implied by the VAR$(p)$ definition,\n- compute all eigenvalues of $C$,\n- compute the spectral radius as the maximum of the magnitudes of these eigenvalues,\n- classify stability using the threshold rule relative to the unit circle as specified above.\n\nTest suite:\n- Case $1$ (happy path, VAR$(1)$, clearly stable): $k = 2$, $p = 1$, $A_1 = \\begin{bmatrix} 0.5 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix}$.\n- Case $2$ (boundary, unit root exactly at the unit circle): $k = 2$, $p = 2$, $A_1 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix}$, $A_2 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$.\n- Case $3$ (unstable, outside the unit circle): $k = 2$, $p = 2$, $A_1 = \\begin{bmatrix} 1.1 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$, $A_2 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$.\n- Case $4$ (edge case with cross-lag interactions and complex eigenvalues, intended stable): $k = 3$, $p = 2$, $A_1 = \\begin{bmatrix} 0.5 & 0.1 & 0.0 \\\\ 0.0 & 0.4 & 0.2 \\\\ 0.0 & 0.0 & 0.3 \\end{bmatrix}$, $A_2 = \\begin{bmatrix} -0.1 & 0.0 & 0.0 \\\\ 0.0 & -0.2 & 0.0 \\\\ 0.0 & 0.0 & -0.1 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-item list $[c, r]$ with $c \\in \\{-1, 0, 1\\}$ the classification code and $r$ the spectral radius rounded to $6$ decimal places. For example, the overall output should look structurally like $[[c_1, r_1],[c_2, r_2],[c_3, r_3],[c_4, r_4]]$.\n\nNo physical units apply in this problem. Angles are not involved.",
            "solution": "The problem requires an assessment of the stability of several Vector Autoregressive (VAR) models. A VAR process is stable if it is covariance-stationary. This property is crucial for the valid application of causality detection methods like Granger Causality and Transfer Entropy, as these methods rely on the existence of a stationary distribution for the underlying time series. The stability of a VAR($p$) process is determined by the eigenvalues of its associated block companion matrix. The process is stable if and only if all eigenvalues of this matrix lie strictly inside the unit circle in the complex plane.\n\nOur task is to derive the structure of this companion matrix from first principles, construct it for each given model, compute its eigenvalues, determine the spectral radius (the maximum eigenvalue magnitude), and classify the model's stability based on whether this radius is less than, equal to, or greater than $1$.\n\n**1. Derivation of the Block Companion Matrix**\n\nA $k$-dimensional VAR process of order $p$, denoted VAR($p$), is defined by the linear recurrence relation:\n$$\nX_t = \\sum_{i=1}^{p} A_i X_{t-i} + \\varepsilon_t\n$$\nwhere $X_t \\in \\mathbb{R}^k$ is the state vector at time $t$, $A_i \\in \\mathbb{R}^{k \\times k}$ are the coefficient matrices, and $\\varepsilon_t \\in \\mathbb{R}^k$ is a vector of zero-mean white noise terms.\n\nThis $p$-th order system can be transformed into an equivalent first-order system (a VAR($1$) process) by augmenting the state vector to include past values. Let us define a new, augmented state vector $Y_t \\in \\mathbb{R}^{kp}$ that stacks the current state $X_t$ and its $p-1$ most recent lags:\n$$\nY_t = \\begin{bmatrix}\nX_t \\\\\nX_{t-1} \\\\\n\\vdots \\\\\nX_{t-p+1}\n\\end{bmatrix}\n$$\nOur goal is to find a state transition matrix $C$ such that $Y_t$ evolves according to a first-order recurrence relation, $Y_t = C Y_{t-1} + E_t$, where $E_t$ is a corresponding noise vector. The vector $Y_{t-1}$ is:\n$$\nY_{t-1} = \\begin{bmatrix}\nX_{t-1} \\\\\nX_{t-2} \\\\\n\\vdots \\\\\nX_{t-p}\n\\end{bmatrix}\n$$\nWe can now construct the relationship between $Y_t$ and $Y_{t-1}$:\n\n- The first block-row of $Y_t$ is $X_t$. From the VAR($p$) definition, we have:\n  $X_t = A_1 X_{t-1} + A_2 X_{t-2} + \\dots + A_p X_{t-p} + \\varepsilon_t$. This equation expresses the first $k$ components of $Y_t$ as a linear combination of the components of $Y_{t-1}$.\n\n- The second block-row of $Y_t$ is $X_{t-1}$. This is, by definition, the first block-row of $Y_{t-1}$. Thus, $X_{t-1} = I_k X_{t-1}$, where $I_k$ is the $k \\times k$ identity matrix.\n\n- The third block-row of $Y_t$ is $X_{t-2}$, which is the second block-row of $Y_{t-1}$.\n\n- This pattern continues until the last block-row of $Y_t$, which is $X_{t-p+1}$, corresponding to the $(p-1)$-th block-row of $Y_{t-1}$.\n\nCombining these relationships into a single matrix equation, we get:\n$$\n\\begin{bmatrix}\nX_t \\\\\nX_{t-1} \\\\\nX_{t-2} \\\\\n\\vdots \\\\\nX_{t-p+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nA_1 & A_2 & \\dots & A_{p-1} & A_p \\\\\nI_k & 0_k & \\dots & 0_k & 0_k \\\\\n0_k & I_k & \\dots & 0_k & 0_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0_k & 0_k & \\dots & I_k & 0_k\n\\end{bmatrix}\n\\begin{bmatrix}\nX_{t-1} \\\\\nX_{t-2} \\\\\nX_{t-3} \\\\\n\\vdots \\\\\nX_{t-p}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\varepsilon_t \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n$$\nThis equation is of the form $Y_t = C Y_{t-1} + E_t$, where $C$ is the $kp \\times kp$ block companion matrix:\n$$\nC = \\begin{bmatrix}\nA_1 & A_2 & \\dots & A_{p-1} & A_p \\\\\nI_k & 0_k & \\dots & 0_k & 0_k \\\\\n0_k & I_k & \\dots & 0_k & 0_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0_k & 0_k & \\dots & I_k & 0_k\n\\end{bmatrix}\n$$\nand $0_k$ is the $k \\times k$ zero matrix.\n\n**2. Stability Criterion and Algorithm**\n\nThe stability of the original VAR($p$) process is equivalent to the stability of this augmented VAR($1$) system. A discrete-time linear time-invariant system is stable if and only if all eigenvalues of its state transition matrix have a magnitude (or modulus) strictly less than $1$. The eigenvalues of the companion matrix $C$ are the roots of the characteristic polynomial $\\det(\\lambda^p I_k - \\sum_{i=1}^p \\lambda^{p-i} A_i) = 0$.\n\nThe spectral radius, $\\rho(C)$, is defined as the maximum magnitude among all eigenvalues of $C$. The stability condition can then be stated succinctly:\n- If $\\rho(C) < 1$, the system is **stable**.\n- If $\\rho(C) = 1$, the system is on the **boundary** of stability (possessing a unit root).\n- If $\\rho(C) > 1$, the system is **unstable**.\n\nThe algorithm to solve the problem for each test case is as follows:\n1.  From the given parameters $k$, $p$, and the list of coefficient matrices $\\{A_1, A_2, \\dots, A_p\\}$, construct the $kp \\times kp$ block companion matrix $C$.\n    - The first $k$ rows of $C$ are formed by horizontally concatenating the matrices $A_1, \\dots, A_p$.\n    - For each block-row $j$ from $1$ to $p-1$, place a $k \\times k$ identity matrix $I_k$ in the block at row $j$ and column $j-1$ (using $0$-based block indexing).\n2.  Compute the $kp$ eigenvalues of the matrix $C$ using a standard numerical linear algebra routine.\n3.  Calculate the magnitude (absolute value) of each eigenvalue.\n4.  Determine the spectral radius, $\\rho(C)$, by finding the maximum of these magnitudes.\n5.  Classify the model's stability based on $\\rho(C)$ and a tolerance $\\tau = 10^{-10}$:\n    - If $|\\rho(C) - 1.0| \\le \\tau$, the classification is **boundary** (code $0$).\n    - If $\\rho(C) < 1.0$, the classification is **stable** (code $1$).\n    - If $\\rho(C) > 1.0$, the classification is **unstable** (code $-1$).\n6.  Format the output as a list containing the integer classification code and the spectral radius rounded to $6$ decimal places.\n\nThis procedure will be applied to each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the VAR model stability problem for a given suite of test cases.\n    \"\"\"\n\n    def check_var_stability(k, p, a_matrices):\n        \"\"\"\n        Constructs the companion matrix for a VAR(p) model and checks its stability.\n\n        Args:\n            k (int): The dimensionality of the time series.\n            p (int): The order of the VAR model.\n            a_matrices (list of np.ndarray): A list of coefficient matrices [A_1, ..., A_p].\n\n        Returns:\n            list: A list containing [classification_code, spectral_radius].\n                  - classification_code: 1 for stable, 0 for boundary, -1 for unstable.\n                  - spectral_radius: The spectral radius rounded to 6 decimal places.\n        \"\"\"\n        # A VAR(1) model's companion matrix is simply the coefficient matrix itself.\n        if p == 1:\n            companion_matrix = a_matrices[0]\n        else:\n            # General construction for VAR(p) models where p > 1\n            kp_dim = k * p\n            companion_matrix = np.zeros((kp_dim, kp_dim))\n            \n            # The first block row consists of the concatenated A_i matrices.\n            companion_matrix[:k, :] = np.hstack(a_matrices)\n            \n            # The sub-diagonal blocks are identity matrices.\n            # This populates the blocks that propagate the lagged variables.\n            # C[(i+1)k:(i+2)k, ik:(i+1)k] = I_k for i in 0..p-2\n            identity_block = np.eye(k)\n            for i in range(p - 1):\n                row_start = (i + 1) * k\n                row_end = row_start + k\n                col_start = i * k\n                col_end = col_start + k\n                companion_matrix[row_start:row_end, col_start:col_end] = identity_block\n\n        # Compute eigenvalues of the companion matrix\n        eigenvalues = np.linalg.eigvals(companion_matrix)\n\n        # Compute the spectral radius (maximum of the magnitudes of the eigenvalues)\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        # Classify stability based on the spectral radius\n        # Use a tolerance tau for comparison with 1.0\n        tolerance = 1e-10\n        if np.abs(spectral_radius - 1.0) <= tolerance:\n            # Boundary case\n            classification_code = 0\n        elif spectral_radius < 1.0:\n            # Stable case\n            classification_code = 1\n        else: # spectral_radius > 1.0\n            # Unstable case\n            classification_code = -1\n            \n        return [classification_code, round(spectral_radius, 6)]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: k=2, p=1, A_1\n        (2, 1, [np.array([[0.5, 0.0], [0.0, 0.3]])]),\n        \n        # Case 2: k=2, p=2, A_1, A_2\n        (2, 2, [np.array([[1.0, 0.0], [0.0, 0.2]]), \n                np.array([[0.0, 0.0], [0.0, 0.0]])]),\n        \n        # Case 3: k=2, p=2, A_1, A_2\n        (2, 2, [np.array([[1.1, 0.0], [0.0, 0.9]]), \n                np.array([[0.0, 0.0], [0.0, 0.0]])]),\n\n        # Case 4: k=3, p=2, A_1, A_2\n        (3, 2, [np.array([[0.5, 0.1, 0.0], [0.0, 0.4, 0.2], [0.0, 0.0, 0.3]]), \n                np.array([[-0.1, 0.0, 0.0], [0.0, -0.2, 0.0], [0.0, 0.0, -0.1]])])\n    ]\n\n    results = []\n    for k, p, a_mats in test_cases:\n        result = check_var_stability(k, p, a_mats)\n        results.append(result)\n\n    # Format the final output list as a string.\n    # e.g., [[1, 0.5], [0, 1.0], ...] -> \"[[1, 0.500000],[0, 1.000000],...]\"\n    # The float formatting ensures 6 decimal places.\n    result_strings = [f\"[{res[0]}, {res[1]:.6f}]\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "One of the most significant challenges in causal inference is distinguishing true causal relationships from spurious correlations arising from unobserved confounders. A common example in real-world data is a shared seasonal driver that influences two otherwise independent processes, creating the illusion of a direct causal link. This hands-on simulation will demonstrate how this artifact arises and, more importantly, how to correct for it by properly including seasonal terms in your model—a crucial technique for robust causality detection .",
            "id": "4116757",
            "problem": "You must write a complete, runnable program that constructs a simulated scenario where omitted seasonality creates spurious Granger causality and then shows how including seasonal terms removes this artifact. The task must be solved from first principles in complex adaptive systems modeling, using causality detection grounded in linear autoregression and nested model comparison.\n\nAssume two scalar stochastic time series $\\{X_t\\}$ and $\\{Y_t\\}$, sampled at integer times $t \\in \\{0,1,\\dots,T-1\\}$. There is an unobserved seasonal driver $\\{Z_t\\}$ which is deterministic and periodic with period $S$, given by $Z_t = \\sin\\!\\left(2\\pi t / S\\right)$. The data generating process is\n$$\nX_t = a_x\\, Z_t + \\varepsilon_{x,t},\\qquad Y_t = a_y\\, Z_{t-\\delta} + \\varepsilon_{y,t},\n$$\nwhere $\\varepsilon_{x,t} \\sim \\mathcal{N}(0,\\sigma_x^2)$ and $\\varepsilon_{y,t} \\sim \\mathcal{N}(0,\\sigma_y^2)$ are independent across $t$ and series, and there is no direct causal influence from $\\{X_t\\}$ to $\\{Y_t\\}$. Thus any predictive power of $X$ for $Y$ when $Z$ is omitted is necessarily spurious due to the common seasonal driver.\n\nGranger causality from $\\{X_t\\}$ to $\\{Y_t\\}$ at lag order $p$ is defined as follows: $\\{X_t\\}$ Granger-causes $\\{Y_t\\}$ if past values of $\\{X_t\\}$ provide statistically significant incremental predictive power for $\\{Y_t\\}$ beyond the past values of $\\{Y_t\\}$. Operationalize this by comparing two nested linear models for $Y_t$ over $t \\in \\{p,\\dots,T-1\\}$:\n- A restricted model using only endogenous lags of $Y_t$ up to order $p$ (and an intercept), optionally augmented with exogenous seasonal regressors if included.\n- An unrestricted model that adds the lags of $X_t$ up to order $p$ to the restricted model.\n\nUse ordinary least squares to fit both models and a classical nested-model test to form a test statistic based on the reduction in residual sum of squares due to adding the $p$ lagged predictors of $X_t$. Use a significance level $\\alpha$ to convert the test statistic into a boolean decision: $X \\to Y$ detected or not detected. You must implement the test using linear algebra and a well-known continuous reference distribution for the test statistic under the null hypothesis, without relying on any external time series libraries.\n\nYou must run the following test suite, which contains three parameter sets designed to test a typical case, a boundary condition on the lag order, and a no-seasonality edge case. For each case, simulate $\\{X_t\\}$ and $\\{Y_t\\}$ with the given parameters, fix a global random seed so the results are deterministic, and perform two Granger causality tests from $X$ to $Y$:\n- One test that omits seasonal terms from both restricted and unrestricted models.\n- A second test that includes seasonal terms in both restricted and unrestricted models, using the fundamental harmonics $\\sin(2\\pi t/S)$ and $\\cos(2\\pi t/S)$ as exogenous regressors.\n\nFor each case, produce two boolean outputs:\n- $b_1$: whether Granger causality from $X$ to $Y$ is detected at level $\\alpha$ when seasonality is omitted.\n- $b_2$: whether Granger causality from $X$ to $Y$ is not detected at level $\\alpha$ when seasonal terms are included, which indicates the artifact is removed.\n\nUse the following test suite with all mathematical constants specified:\n- Case $1$: $T=1000$, $S=24$, $a_x=5.0$, $a_y=5.0$, $\\delta=1$, $\\sigma_x=0.3$, $\\sigma_y=0.3$, $p=1$, $\\alpha=0.01$.\n- Case $2$: $T=1000$, $S=24$, $a_x=5.0$, $a_y=5.0$, $\\delta=3$, $\\sigma_x=0.5$, $\\sigma_y=0.5$, $p=1$, $\\alpha=0.01$.\n- Case $3$: $T=1000$, $S=24$, $a_x=0.0$, $a_y=0.0$, $\\delta=1$, $\\sigma_x=1.0$, $\\sigma_y=1.0$, $p=1$, $\\alpha=0.01$.\n\nYour program must:\n- Implement data generation for the specified parameters.\n- Implement ordinary least squares fitting and a nested-model hypothesis test for Granger causality at lag order $p$, with and without the seasonal regressors $\\sin(2\\pi t/S)$ and $\\cos(2\\pi t/S)$.\n- Use a fixed random seed so the outputs are deterministic.\n- For each case, return the pair $[b_1,b_2]$ as defined above.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain one two-element list per case, in order of the cases above. For example, the printed line must have the form \"[[b1_case1,b2_case1],[b1_case2,b2_case2],[b1_case3,b2_case3]]\" with boolean literals.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of time series analysis and econometrics, specifically addressing the well-documented issue of spurious causality arising from omitted variable bias. The problem is well-posed, with a clearly defined data generating process, a precise statistical methodology (Granger causality via nested linear models and an F-test), and a complete set of parameters for simulation. It is objective and formalizable, requiring the implementation of standard, verifiable algorithms. The task directly pertains to the specified topic of causality detection in complex adaptive systems modeling, as it demonstrates how indirect influences mediated by a common environmental driver can be mistaken for direct causal links if the system is not modeled completely.\n\n### Introduction\nThe problem requires an empirical demonstration of a critical artifact in causality detection: the generation of spurious Granger causality due to an unobserved common driver, specifically a seasonal component. We will simulate two time series, $\\{X_t\\}$ and $\\{Y_t\\}$, that are not causally linked but are both influenced by a shared sinusoidal signal. We will then show that a standard Granger causality test erroneously detects a causal link from $X$ to $Y$ when this signal is omitted from the model. Subsequently, we will demonstrate that by properly including deterministic seasonal terms in the regression models, this spurious causal inference is correctly eliminated.\n\n### Data Generating Process\nThe two scalar stochastic time series $\\{X_t\\}$ and $\\{Y_t\\}$ for $t \\in \\{0, 1, \\dots, T-1\\}$ are generated according to the following process:\n$$\nZ_t = \\sin\\left(\\frac{2\\pi t}{S}\\right)\n$$\n$$\nX_t = a_x Z_t + \\varepsilon_{x,t}\n$$\n$$\nY_t = a_y Z_{t-\\delta} + \\varepsilon_{y,t}\n$$\nwhere $\\{Z_t\\}$ is a deterministic seasonal driver with period $S$. The terms $\\varepsilon_{x,t}$ and $\\varepsilon_{y,t}$ are independent and identically distributed Gaussian white noise processes with mean $0$ and variances $\\sigma_x^2$ and $\\sigma_y^2$, respectively. Crucially, the structure dictates no direct causal influence from $\\{X_t\\}$ to $\\{Y_t\\}$; their statistical association arises solely from the common influence of $\\{Z_t\\}$.\n\n### Granger Causality Test Formulation\nGranger causality is a statistical hypothesis test for determining whether one time series is useful in forecasting another. We assess if past values of $\\{X_t\\}$ contain information that helps predict $\\{Y_t\\}$ beyond the information already contained in past values of $\\{Y_t\\}$. This is formalized by comparing two nested linear autoregressive models using an F-test.\n\nFor a lag order $p$, the models are defined for the time range $t \\in \\{p, \\dots, T-1\\}$.\n\n1.  **Restricted Model ($M_R$):** This model regresses $Y_t$ only on its own past values (and an intercept).\n    $$\n    Y_t = c + \\sum_{i=1}^{p} \\beta_i Y_{t-i} + \\epsilon_{R,t}\n    $$\n\n2.  **Unrestricted Model ($M_U$):** This model adds the past values of $X_t$ as additional predictors.\n    $$\n    Y_t = c + \\sum_{i=1}^{p} \\beta_i Y_{t-i} + \\sum_{i=1}^{p} \\gamma_i X_{t-i} + \\epsilon_{U,t}\n    $$\n\nThe null hypothesis ($H_0$) for the test is that $\\{X_t\\}$ does not Granger-cause $\\{Y_t\\}$, which corresponds to all coefficients for the lagged $X_t$ variables being zero:\n$$\nH_0: \\gamma_1 = \\gamma_2 = \\dots = \\gamma_p = 0\n$$\n\n### Parameter Estimation and Hypothesis Testing\nThe coefficients for both models are estimated using Ordinary Least Squares (OLS). For a given model $y = \\mathbf{A}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is the vector of observations of the dependent variable, $\\mathbf{A}$ is the design matrix of predictors, and $\\boldsymbol{\\theta}$ is the vector of coefficients, the OLS estimate is $\\hat{\\boldsymbol{\\theta}} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}$.\n\nThe goodness of fit for each model is measured by its Residual Sum of Squares (RSS), defined as $RSS = \\sum_{t=p}^{T-1} (Y_t - \\hat{Y}_t)^2$, where $\\hat{Y}_t$ are the predicted values from the fitted model. Let $RSS_R$ and $RSS_U$ be the RSS for the restricted and unrestricted models, respectively.\n\nThe F-statistic is constructed to test the significance of the reduction in RSS when moving from $M_R$ to $M_U$:\n$$\nF = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (n - k)}\n$$\nwhere:\n- $n = T-p$ is the number of observations used in the regression.\n- $q = p$ is the number of additional predictors in the unrestricted model (the number of restrictions under $H_0$).\n- $k$ is the total number of predictors in the unrestricted model (including the intercept).\n\nUnder the null hypothesis, this statistic follows an F-distribution with $q$ and $n-k$ degrees of freedom, $F(q, n-k)$. We calculate the $p$-value associated with the observed $F$-statistic. If this $p$-value is less than the chosen significance level $\\alpha$, we reject $H_0$ and conclude that $\\{X_t\\}$ Granger-causes $\\{Y_t\\}$.\n\n### Spurious Causality and Its Correction\n**1. Omitted Seasonality:**\nWhen the seasonal driver is omitted, the past values of $X$, i.e., $X_{t-i} = a_x Z_{t-i} + \\varepsilon_{x, t-i}$, serve as a proxy for the unobserved seasonal term $Z_{t-i}$. Since $Y_t$ is driven by $Z_{t-\\delta}$, and sinusoids at different lags are correlated, $X_{t-i}$ will be correlated with $Y_t$. This correlation may persist even after accounting for the past of $Y$, $Y_{t-i}$. Consequently, adding lagged $X$ values to the model significantly reduces the RSS ($RSS_U \\ll RSS_R$), leading to a large F-statistic and a small $p$-value. This results in a false detection of causality ($b_1 = \\text{True}$).\n\n**2. Included Seasonality:**\nTo correct this, we augment both the restricted and unrestricted models with exogenous regressors that explicitly capture the seasonal pattern. A sinusoidal signal with period $S$ can be fully described by a linear combination of $\\sin(2\\pi t/S)$ and $\\cos(2\\pi t/S)$.\nThe augmented restricted model becomes:\n$$\nY_t = c + \\sum_{i=1}^{p} \\beta_i Y_{t-i} + s_1 \\sin\\left(\\frac{2\\pi t}{S}\\right) + s_2 \\cos\\left(\\frac{2\\pi t}{S}\\right) + \\epsilon'_{R,t}\n$$\nThis model can now directly account for the seasonal influence from $Z_{t-\\delta}$, since $Z_{t-\\delta} = \\sin(2\\pi(t-\\delta)/S)$ is a linear combination of the $\\sin$ and $\\cos$ regressors. Once this effect is properly modeled in the restricted model, the lagged values of $X_t$ (which are noisy versions of the same seasonal signal) provide no significant additional predictive power. Therefore, $RSS_R \\approx RSS_U$, the F-statistic is small, the $p$-value is large, and we correctly fail to reject the null hypothesis ($b_2 = \\text{True}$).\n\n### Algorithmic Implementation\nThe solution is implemented as follows:\n1.  **Data Generation**: For each set of parameters, generate the time series $\\{X_t\\}$ and $\\{Y_t\\}$ of length $T=1000$ based on the specified DGP, using a fixed random seed for reproducibility.\n2.  **Model Specification**: Define two test configurations: one without seasonal terms and one with $\\sin(2\\pi t/S)$ and $\\cos(2\\pi t/S)$ as exogenous regressors.\n3.  **Granger Causality Test**: For each configuration:\n    a.  Construct the response vector $\\mathbf{y} = [Y_p, \\dots, Y_{T-1}]^T$.\n    b.  Construct the design matrices $\\mathbf{A}_R$ and $\\mathbf{A}_U$ for the restricted and unrestricted models based on the lagged series and any seasonal regressors.\n    c.  Use `numpy.linalg.lstsq` to find the OLS coefficients for each model.\n    d.  Manually calculate the RSS for each model to ensure robustness.\n    e.  Compute the F-statistic and its corresponding $p$-value using the survival function (`sf`) from `scipy.stats.f`.\n    f.  Compare the $p$-value to $\\alpha$ to determine if causality is detected.\n4.  **Output Compilation**: For each test case, compile the two boolean results ($b_1$: causality detected without seasonal terms; $b_2$: causality not detected with seasonal terms) into a list. The final output is a list of these lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and Granger causality tests\n    for the specified test cases.\n    \"\"\"\n    # Fix the random seed for reproducibility.\n    RANDOM_SEED = 42\n    np.random.seed(RANDOM_SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Typical case with seasonality\n        {'T': 1000, 'S': 24, 'a_x': 5.0, 'a_y': 5.0, 'delta': 1, 'sigma_x': 0.3, 'sigma_y': 0.3, 'p': 1, 'alpha': 0.01},\n        # Case 2: Boundary condition with larger lag in seasonal driver\n        {'T': 1000, 'S': 24, 'a_x': 5.0, 'a_y': 5.0, 'delta': 3, 'sigma_x': 0.5, 'sigma_y': 0.5, 'p': 1, 'alpha': 0.01},\n        # Case 3: Edge case with no seasonality (pure noise)\n        {'T': 1000, 'S': 24, 'a_x': 0.0, 'a_y': 0.0, 'delta': 1, 'sigma_x': 1.0, 'sigma_y': 1.0, 'p': 1, 'alpha': 0.01},\n    ]\n\n    final_results = []\n    for params in test_cases:\n        case_results = perform_single_case(**params)\n        final_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\ndef generate_data(T, S, a_x, a_y, delta, sigma_x, sigma_y):\n    \"\"\"\n    Generates two time series X and Y based on the specified DGP.\n    \"\"\"\n    t = np.arange(T)\n    # Seasonal driver Z_t for X and Z_{t-delta} for Y\n    Z_x = a_x * np.sin(2 * np.pi * t / S)\n    Z_y = a_y * np.sin(2 * np.pi * (t - delta) / S)\n    \n    # Gaussian white noise\n    eps_x = np.random.normal(0, sigma_x, size=T)\n    eps_y = np.random.normal(0, sigma_y, size=T)\n    \n    # Final time series\n    X = Z_x + eps_x\n    Y = Z_y + eps_y\n    \n    return X, Y\n\ndef ols_fit_and_rss(y, A):\n    \"\"\"\n    Performs OLS regression and returns the Residual Sum of Squares (RSS).\n    \"\"\"\n    # Use np.linalg.lstsq for robust OLS solution\n    coeffs, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n    \n    # Calculate residuals and RSS manually for clarity and robustness\n    predictions = A @ coeffs\n    residuals = y - predictions\n    rss = np.sum(residuals**2)\n    \n    return rss\n\ndef run_granger_test(X, Y, p, S, alpha, include_seasonality):\n    \"\"\"\n    Performs a Granger causality test from X to Y.\n\n    Returns:\n        bool: True if Granger causality is detected at significance alpha.\n    \"\"\"\n    T = len(X)\n    n = T - p  # Number of observations for regression\n\n    # Target variable for regression: Y_t for t in {p, ..., T-1}\n    y_target = Y[p:]\n\n    # --- Construct predictor matrices ---\n\n    # Intercept\n    intercept = np.ones((n, 1))\n\n    # Lags of Y\n    lags_Y_list = [Y[p-i:T-i] for i in range(1, p + 1)]\n    lags_Y = np.vstack(lags_Y_list).T\n\n    # Lags of X\n    lags_X_list = [X[p-i:T-i] for i in range(1, p + 1)]\n    lags_X = np.vstack(lags_X_list).T\n\n    # --- Restricted Model (Y's lags only) ---\n    A_R_components = [intercept, lags_Y]\n    if include_seasonality:\n        t_reg = np.arange(p, T)\n        sin_term = np.sin(2 * np.pi * t_reg / S).reshape(-1, 1)\n        cos_term = np.cos(2 * np.pi * t_reg / S).reshape(-1, 1)\n        A_R_components.extend([sin_term, cos_term])\n    \n    A_R = np.hstack(A_R_components)\n    \n    # --- Unrestricted Model (Y's lags + X's lags) ---\n    A_U = np.hstack([A_R, lags_X])\n\n    # --- Calculate RSS for both models ---\n    RSS_R = ols_fit_and_rss(y_target, A_R)\n    RSS_U = ols_fit_and_rss(y_target, A_U)\n\n    # --- Perform F-test ---\n    k = A_U.shape[1] # Total number of parameters in the unrestricted model\n    q = p            # Number of restrictions (coefficients of lagged X)\n    \n    df1 = q\n    df2 = n - k\n\n    # Ensure df2 is positive\n    if df2 <= 0:\n        # This case is unlikely with the given parameters but is good practice\n        return False\n\n    F_statistic = ((RSS_R - RSS_U) / df1) / (RSS_U / df2)\n    \n    p_value = f.sf(F_statistic, df1, df2)\n\n    return p_value < alpha\n\ndef perform_single_case(T, S, a_x, a_y, delta, sigma_x, sigma_y, p, alpha):\n    \"\"\"\n    Runs both Granger causality tests for a single set of parameters.\n    \"\"\"\n    # 1. Generate data\n    X, Y = generate_data(T, S, a_x, a_y, delta, sigma_x, sigma_y)\n\n    # 2. Test without seasonal terms\n    # b1: True if Granger causality IS detected\n    gc_detected_no_season = run_granger_test(X, Y, p, S, alpha, include_seasonality=False)\n    b1 = gc_detected_no_season\n\n    # 3. Test with seasonal terms\n    # b2: True if Granger causality IS NOT detected\n    gc_detected_with_season = run_granger_test(X, Y, p, S, alpha, include_seasonality=True)\n    b2 = not gc_detected_with_season\n\n    return [b1, b2]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Complex systems are often characterized by intricate networks of interactions where influence is indirect. This exercise explores a classic mediated causal chain, $X \\to Z \\to Y$, where the influence of a source $X$ on a target $Y$ is entirely transmitted through an intermediary process $Z$. By working through this scenario, you will see how bivariate analysis can be misleading and how conditional measures like Granger causality and Transfer Entropy are essential for dissecting these pathways, sharpening your ability to interpret causal relationships in a multivariate context .",
            "id": "4116778",
            "problem": "Consider three discrete-time stochastic processes in a complex adaptive system with a mediator, defined for integer time index $t$ by\n$$\nX_t = \\varepsilon^X_t,\\quad Z_t = c\\,X_{t-1} + \\varepsilon^Z_t,\\quad Y_t = d\\,Z_{t-1} + \\varepsilon^Y_t,\n$$\nwhere $\\varepsilon^X_t$, $\\varepsilon^Z_t$, and $\\varepsilon^Y_t$ are mutually independent, zero-mean, Gaussian white-noise innovations with variances $\\sigma_X^{2}$, $\\sigma_Z^{2}$, and $\\sigma_Y^{2}$, respectively. Assume there is no direct influence from $X$ to $Y$ except through the mediator $Z$. The setting is consistent with a first-order Vector Autoregression (VAR) for the trivariate system $(X_t,Z_t,Y_t)$ and induces an apparent lag-$2$ relation between $X$ and $Y$ in the bivariate $(X_t,Y_t)$ system.\n\nUsing only fundamental definitions, namely that Granger causality is defined by whether including the past of a driver reduces the mean-squared prediction error of a target beyond the target’s own past, and that Transfer Entropy (TE) is defined as the conditional mutual information from the driver’s past to the target’s present given the target’s past, answer the following:\n\n1. Explain, at the level of model structure, why bivariate Granger causality from $X$ to $Y$ at lag $2$ can be detected in the reduced $(X_t,Y_t)$ system even though there is no direct $X\\to Y$ edge, and why conditioning on the mediator $Z$ in the full $(X_t,Z_t,Y_t)$ system at lag $1$ removes the apparent $X\\to Y$ Granger causality.\n\n2. For the parameter choice $c=1$, $d=1$, $\\sigma_X^{2}=1$, $\\sigma_Z^{2}=1$, and $\\sigma_Y^{2}=1$, compute the bivariate Transfer Entropy from $X$ to $Y$ at lag $2$ in the reduced $(X_t,Y_t)$ system, expressed in nats. Provide the final result as an exact analytical expression. Do not round.",
            "solution": "The problem posed is scientifically grounded, well-posed, and objective. It describes a standard first-order Vector Autoregressive (VAR) model and asks for an analysis using the established concepts of Granger causality and Transfer Entropy. All necessary parameters and definitions are provided for a unique and meaningful solution. We may therefore proceed with the solution.\n\nThe system of stochastic processes is defined as:\n$$\n\\begin{cases}\nX_t = \\varepsilon^X_t \\\\\nZ_t = c\\,X_{t-1} + \\varepsilon^Z_t \\\\\nY_t = d\\,Z_{t-1} + \\varepsilon^Y_t\n\\end{cases}\n$$\nwhere $\\varepsilon^X_t$, $\\varepsilon^Z_t$, and $\\varepsilon^Y_t$ are mutually independent, zero-mean Gaussian white-noise processes with variances $\\sigma_X^{2}$, $\\sigma_Z^{2}$, and $\\sigma_Y^{2}$, respectively.\n\n**Part 1: Granger Causality Analysis**\n\nGranger causality from a process $X$ to a process $Y$ exists if the past of $X$ contains information that helps predict the present of $Y$ over and above the information already contained in the past of $Y$. This is quantified by a reduction in the mean-squared prediction error.\n\nFirst, let us establish the relationship between $Y_t$ and the past of $X$. By substituting the expression for $Z_{t-1}$ into the equation for $Y_t$, we get:\n$$\nY_t = d\\,(c\\,X_{t-2} + \\varepsilon^Z_{t-1}) + \\varepsilon^Y_t = dc\\,X_{t-2} + d\\,\\varepsilon^Z_{t-1} + \\varepsilon^Y_t\n$$\nThis equation reveals that $Y_t$ is directly influenced by $X_{t-2}$, establishing a causal link with a lag of $2$ time steps, mediated by $Z$.\n\n_Bivariate Granger Causality in the $(X_t, Y_t)$ system:_\n\nTo assess bivariate Granger causality, we compare the prediction of $Y_t$ based on its own past, $\\mathbf{Y}_{past} = \\{Y_{t-1}, Y_{t-2}, \\dots\\}$, with the prediction based on both its own past and the past of $X$, $\\mathbf{X}_{past} = \\{X_{t-1}, X_{t-2}, \\dots\\}$.\nThe optimal linear predictor for $Y_t$ using only its own past will have some residual error. Now, consider adding the past of $X$ to the set of predictors. From the equation $Y_t = dc\\,X_{t-2} + d\\,\\varepsilon^Z_{t-1} + \\varepsilon^Y_t$, we see that $X_{t-2}$ is a component of $Y_t$. To determine if $X_{t-2}$ provides new information not already present in $\\mathbf{Y}_{past}$, we examine the composition of past $Y$ values. For example,\n$$\nY_{t-1} = dc\\,X_{t-3} + d\\,\\varepsilon^Z_{t-2} + \\varepsilon^Y_{t-1}\n$$\nThe past of $Y$ contains information about the past of $X$ at lags of $3$ and greater (i.e., $X_{t-3}, X_{t-4}, \\dots$), but it does not contain information about $X_{t-2}$. The term $X_{t-2}$ is an independent random variable with respect to the entire past history of $Y$, $\\mathbf{Y}_{past}$. Therefore, including $X_{t-2}$ (which is part of $\\mathbf{X}_{past}$) in the prediction model for $Y_t$ will certainly reduce the prediction error. The residual based on $\\mathbf{Y}_{past}$ alone contains the term $dc\\,X_{t-2}$, whose variance can be eliminated from the error term by including $X_{t-2}$ as a predictor. Thus, in the bivariate system, $X$ Granger-causes $Y$. The dependence $Y_t \\propto X_{t-2}$ demonstrates why this causality is detected at lag $2$.\n\n_Conditional Granger Causality in the $(X_t, Z_t, Y_t)$ system:_\n\nNow, we condition on the mediator process $Z$. We assess if $\\mathbf{X}_{past}$ helps predict $Y_t$ beyond the information provided by both $\\mathbf{Y}_{past}$ and $\\mathbf{Z}_{past} = \\{Z_{t-1}, Z_{t-2}, \\dots\\}$. The governing equation for $Y_t$ is $Y_t = d\\,Z_{t-1} + \\varepsilon^Y_t$. This equation shows that given $Z_{t-1}$, the value of $Y_t$ is determined up to the innovation noise $\\varepsilon^Y_t$. Since $\\varepsilon^Y_t$ is independent of all past values of $X$, $Y$, and $Z$, the best possible linear prediction of $Y_t$ given the full past history of all three variables is $\\hat{Y}_t = d\\,Z_{t-1}$. The corresponding prediction error is simply $\\varepsilon^Y_t$, with mean-squared error $\\sigma_Y^2$. Critically, the process $Y_t$ is conditionally independent of the past of $X$ (and its own past) given $Z_{t-1}$. That is, $p(Y_t | \\mathbf{Y}_{past}, \\mathbf{Z}_{past}, \\mathbf{X}_{past}) = p(Y_t | Z_{t-1})$.\nBecause including $\\mathbf{X}_{past}$ does not provide any additional information for predicting $Y_t$ once $\\mathbf{Z}_{past}$ (specifically $Z_{t-1}$) is known, the prediction error is not reduced. Therefore, the conditional Granger causality from $X$ to $Y$ given $Z$ is zero. The mediator $Z$ \"screens off\" the influence of $X$ on $Y$, rendering the direct causal link from $X$ to $Y$ (in the Granger sense) null when $Z$ is observed.\n\n**Part 2: Transfer Entropy Calculation**\n\nWe are given the parameter values $c=1$, $d=1$, $\\sigma_X^{2}=1$, $\\sigma_Z^{2}=1$, and $\\sigma_Y^{2}=1$.\nThe system equations become:\n$$\n\\begin{cases}\nX_t = \\varepsilon^X_t \\\\\nZ_t = X_{t-1} + \\varepsilon^Z_t \\\\\nY_t = Z_{t-1} + \\varepsilon^Y_t\n\\end{cases}\n$$\nAnd the composite relation is $Y_t = X_{t-2} + \\varepsilon^Z_{t-1} + \\varepsilon^Y_t$.\n\nThe Transfer Entropy (TE) from $X$ to $Y$ is the conditional mutual information $TE_{X \\to Y} = I(Y_t; \\mathbf{X}_{past} | \\mathbf{Y}_{past})$. Given the model structure, the only part of $\\mathbf{X}_{past}$ that carries information about $Y_t$ is $X_{t-2}$. Thus, the expression simplifies to $TE_{X \\to Y} = I(Y_t; X_{t-2} | \\mathbf{Y}_{past})$.\n\nFor Gaussian processes, TE can be written in terms of conditional variances:\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln \\frac{\\text{Var}(Y_t | \\mathbf{Y}_{past})}{\\text{Var}(Y_t | \\mathbf{Y}_{past}, X_{t-2})}\n$$\nLet us analyze the statistical properties of the process $Y_t$. The innovations $\\{\\varepsilon^X_t, \\varepsilon^Z_t, \\varepsilon^Y_t\\}$ are mutually independent for all time lags. The process $Y_t = X_{t-2} + \\varepsilon^Z_{t-1} + \\varepsilon^Y_t$ is a sum of independent white-noise processes, each at a unique time lag. Let's compute the autocovariance of $Y_t$ for a lag $k \\ge 1$:\n$$\n\\text{Cov}(Y_t, Y_{t-k}) = E[Y_t Y_{t-k}] = E[(X_{t-2} + \\varepsilon^Z_{t-1} + \\varepsilon^Y_t) (X_{t-2-k} + \\varepsilon^Z_{t-1-k} + \\varepsilon^Y_{t-k})]\n$$\nSince all innovation processes are white noise and mutually independent, every cross-term in this expectation is zero. For example, $E[X_{t-2} X_{t-2-k}]=0$ because $k \\ge 1$. Similarly, $E[X_{t-2} \\varepsilon^Z_{t-1-k}]=0$ as all innovations are independent. Therefore, $\\text{Cov}(Y_t, Y_{t-k}) = 0$ for all $k \\ge 1$. This means that $Y_t$ is a white-noise process itself.\n\nAs a consequence, the past of $Y$, $\\mathbf{Y}_{past}$, is independent of its present, $Y_t$. Furthermore, the set of innovations that constitute $\\mathbf{Y}_{past}$ is disjoint from the set of innovations that constitute the pair $(Y_t, X_{t-2})$. Thus, $\\mathbf{Y}_{past}$ is independent of $(Y_t, X_{t-2})$. This makes the conditioning on $\\mathbf{Y}_{past}$ in the TE expression vacuous:\n$$\nTE_{X \\to Y} = I(Y_t; X_{t-2} | \\mathbf{Y}_{past}) = I(Y_t; X_{t-2})\n$$\nThe problem reduces to calculating the standard mutual information between $Y_t$ and $X_{t-2}$. For Gaussian variables this is:\n$$\nI(Y_t; X_{t-2}) = \\frac{1}{2} \\ln \\frac{\\text{Var}(Y_t)}{\\text{Var}(Y_t | X_{t-2})}\n$$\nWe compute the required variances:\n1.  The unconditional variance of $Y_t$:\n    $$\n    \\text{Var}(Y_t) = \\text{Var}(X_{t-2} + \\varepsilon^Z_{t-1} + \\varepsilon^Y_t)\n    $$\n    Due to the independence of the components, this is the sum of their variances:\n    $$\n    \\text{Var}(Y_t) = \\text{Var}(X_{t-2}) + \\text{Var}(\\varepsilon^Z_{t-1}) + \\text{Var}(\\varepsilon^Y_t) = \\sigma_X^2 + \\sigma_Z^2 + \\sigma_Y^2 = 1 + 1 + 1 = 3\n    $$\n2.  The conditional variance of $Y_t$ given $X_{t-2}$. The best linear predictor for $Y_t$ given $X_{t-2}$ is $E[Y_t|X_{t-2}]$.\n    $$\n    E[Y_t | X_{t-2}] = E[X_{t-2} + \\varepsilon^Z_{t-1} + \\varepsilon^Y_t | X_{t-2}] = E[X_{t-2}|X_{t-2}] + E[\\varepsilon^Z_{t-1}|X_{t-2}] + E[\\varepsilon^Y_t|X_{t-2}]\n    $$\n    Since the innovations are zero-mean and independent of $X_{t-2}$, the last two terms are zero. Thus, $E[Y_t | X_{t-2}] = X_{t-2}$. The prediction residual is:\n    $$\n    Y_t - E[Y_t | X_{t-2}] = (X_{t-2} + \\varepsilon^Z_{t-1} + \\varepsilon^Y_t) - X_{t-2} = \\varepsilon^Z_{t-1} + \\varepsilon^Y_t\n    $$\n    The conditional variance is the variance of this residual:\n    $$\n    \\text{Var}(Y_t | X_{t-2}) = \\text{Var}(\\varepsilon^Z_{t-1} + \\varepsilon^Y_t) = \\text{Var}(\\varepsilon^Z_{t-1}) + \\text{Var}(\\varepsilon^Y_t) = \\sigma_Z^2 + \\sigma_Y^2 = 1+1=2\n    $$\nFinally, we substitute these variances back into the formula for Transfer Entropy:\n$$\nTE_{X \\to Y} = \\frac{1}{2} \\ln \\left( \\frac{\\text{Var}(Y_t)}{\\text{Var}(Y_t | X_{t-2})} \\right) = \\frac{1}{2} \\ln \\left( \\frac{3}{2} \\right)\n$$\nThe result is in nats, as the natural logarithm was used.",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(\\frac{3}{2}\\right)}\n$$"
        }
    ]
}