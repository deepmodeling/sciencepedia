{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation in Bayesian calibration, we begin with an idealized scenario where the likelihood function is analytically tractable. This exercise demonstrates the core mechanic of Bayesian inference—updating a prior distribution into a posterior distribution using data—within the convenient framework of conjugate models. By deriving the posterior for a simple rate parameter, you will gain intuition for how prior knowledge and observed evidence are mathematically combined, and importantly, you will consider why this straightforward approach is often insufficient for the intricate nature of complex adaptive systems .",
            "id": "4115316",
            "problem": "Consider calibrating a simple count-based submodel within a complex adaptive system, where the number of interaction events accumulated under known exposures is used to infer an underlying event rate. Let $y_{1},\\dots,y_{n}$ denote nonnegative integer counts observed under strictly positive exposures $e_{1},\\dots,e_{n}$. Assume conditional independence of the observations given the rate parameter $\\lambda>0$, and model the data using a Poisson sampling model with exposure:\n$$\np\\left(y_{i}\\mid \\lambda\\right)=\\frac{\\left(\\lambda e_{i}\\right)^{y_{i}}\\exp\\!\\left(-\\lambda e_{i}\\right)}{y_{i}!},\\quad i=1,\\dots,n.\n$$\nIn order to obtain an analytically tractable Bayesian calibration, adopt a Gamma prior on $\\lambda$ in shape–rate parameterization:\n$$\np\\left(\\lambda\\right)=\\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma\\!\\left(\\alpha_{0}\\right)}\\,\\lambda^{\\alpha_{0}-1}\\exp\\!\\left(-\\beta_{0}\\lambda\\right),\\quad \\alpha_{0}>0,\\;\\beta_{0}>0.\n$$\nStarting from Bayes’ theorem and the above definitions of the likelihood and prior, derive the closed-form posterior distribution for $\\lambda$ and identify its parameters in terms of $\\alpha_{0}$, $\\beta_{0}$, $\\{y_{i}\\}_{i=1}^{n}$, and $\\{e_{i}\\}_{i=1}^{n}$. Then, provide the exact closed-form analytic expression for the posterior mean $\\mathbb{E}\\!\\left[\\lambda\\mid y_{1},\\dots,y_{n}\\right]$ in terms of these symbols. Your answer should be a single analytic expression; no rounding is required. The calibrated rate parameter $\\lambda$ has units of events per exposure unit; do not include units in your expression.\n\nBriefly (no calculation required), articulate why this conjugate construction may be limiting for calibration in complex adaptive systems, and when Approximate Bayesian Computation (ABC) would be preferred because of likelihood intractability. The final answer must be the exact expression for the posterior mean.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   Observed data: Nonnegative integer counts $y_{1}, \\dots, y_{n}$.\n-   Exposures: Strictly positive values $e_{1}, \\dots, e_{n}$.\n-   Parameter of interest: Rate parameter $\\lambda > 0$.\n-   Sampling model (likelihood): The observations $y_i$ are conditionally independent given $\\lambda$ and follow a Poisson distribution with mean $\\lambda e_i$. The probability mass function for a single observation is:\n    $$\n    p\\left(y_{i}\\mid \\lambda\\right)=\\frac{\\left(\\lambda e_{i}\\right)^{y_{i}}\\exp\\!\\left(-\\lambda e_{i}\\right)}{y_{i}!}\n    $$\n-   Prior distribution: The prior on $\\lambda$ is a Gamma distribution with shape parameter $\\alpha_0 > 0$ and rate parameter $\\beta_0 > 0$. The probability density function is:\n    $$\n    p\\left(\\lambda\\right)=\\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma\\!\\left(\\alpha_{0}\\right)}\\,\\lambda^{\\alpha_{0}-1}\\exp\\!\\left(-\\beta_{0}\\lambda\\right)\n    $$\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is grounded in standard Bayesian statistical theory. The use of a Poisson likelihood for count data and a Gamma prior for a rate parameter is a canonical example of a conjugate prior setup, a fundamental topic in Bayesian statistics.\n-   **Well-Posed**: The problem is well-posed. It asks for the derivation of a posterior distribution and its mean, given a fully specified likelihood and prior. The structure ensures a unique and meaningful solution exists.\n-   **Objective**: The problem is stated using precise, formal mathematical language, free from any subjective or ambiguous terminology.\n-   **Completeness**: All necessary information (the likelihood, the prior, the relationship between data and parameters) is provided.\n-   **Consistency**: The givens are internally consistent. The domains of the variables and parameters ($\\lambda>0$, $y_i \\in \\{0, 1, 2, \\dots\\}$, $e_i>0$, $\\alpha_0>0$, $\\beta_0>0$) are all compatible with the definitions of the Poisson and Gamma distributions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe derivation proceeds by applying Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution. Let $Y = (y_1, \\dots, y_n)$ denote the set of all observations.\n\nThe posterior distribution for $\\lambda$ is given by:\n$$\np(\\lambda \\mid Y) \\propto p(Y \\mid \\lambda) \\, p(\\lambda)\n$$\nFirst, we construct the likelihood function $p(Y \\mid \\lambda)$. Due to the conditional independence of the observations $y_i$, the joint likelihood is the product of the individual likelihoods:\n$$\np(Y \\mid \\lambda) = \\prod_{i=1}^{n} p(y_i \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{(\\lambda e_i)^{y_i} \\exp(-\\lambda e_i)}{y_i!}\n$$\nWe can separate the terms that depend on $\\lambda$ from those that do not.\n$$\np(Y \\mid \\lambda) = \\left( \\prod_{i=1}^{n} \\frac{e_i^{y_i}}{y_i!} \\right) \\left( \\prod_{i=1}^{n} \\lambda^{y_i} \\exp(-\\lambda e_i) \\right)\n$$\nCombining the terms within the second parenthesis:\n$$\n\\prod_{i=1}^{n} \\lambda^{y_i} \\exp(-\\lambda e_i) = \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} e_i\\right)\n$$\nAs we are interested in the shape of the posterior distribution for $\\lambda$, we can treat any term not involving $\\lambda$ as part of the normalization constant. Thus, the likelihood is proportional to:\n$$\np(Y \\mid \\lambda) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} e_i\\right)\n$$\nNext, we consider the prior distribution, which is a Gamma distribution $\\text{Gamma}(\\alpha_0, \\beta_0)$:\n$$\np(\\lambda) = \\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma(\\alpha_{0})} \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0}\\lambda)\n$$\nThe kernel of the prior distribution (the part depending on $\\lambda$) is:\n$$\np(\\lambda) \\propto \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0}\\lambda)\n$$\nNow, we multiply the likelihood kernel by the prior kernel to find the posterior kernel:\n$$\np(\\lambda \\mid Y) \\propto \\left( \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} e_i\\right) \\right) \\cdot \\left( \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0}\\lambda) \\right)\n$$\nCombining terms by adding exponents:\n$$\np(\\lambda \\mid Y) \\propto \\lambda^{\\left(\\alpha_{0} - 1\\right) + \\sum_{i=1}^{n} y_i} \\exp\\left(-\\beta_{0}\\lambda - \\lambda \\sum_{i=1}^{n} e_i\\right)\n$$\n$$\np(\\lambda \\mid Y) \\propto \\lambda^{\\left(\\alpha_{0} + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-\\left(\\beta_{0} + \\sum_{i=1}^{n} e_i\\right)\\lambda\\right)\n$$\nThis resulting functional form is the kernel of a Gamma distribution. A random variable $X$ that follows a Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$, denoted $X \\sim \\text{Gamma}(\\alpha, \\beta)$, has a probability density function proportional to $x^{\\alpha-1}\\exp(-\\beta x)$.\n\nBy comparing our posterior kernel with the standard Gamma kernel, we can identify the parameters of the posterior distribution. Let the posterior distribution be a Gamma distribution with shape parameter $\\alpha_n$ and rate parameter $\\beta_n$. Then:\n$$\n\\alpha_n = \\alpha_0 + \\sum_{i=1}^{n} y_i\n$$\n$$\n\\beta_n = \\beta_0 + \\sum_{i=1}^{n} e_i\n$$\nThus, the posterior distribution for $\\lambda$ is a Gamma distribution with these updated parameters:\n$$\n\\lambda \\mid Y \\sim \\text{Gamma}\\left(\\alpha_0 + \\sum_{i=1}^{n} y_i, \\beta_0 + \\sum_{i=1}^{n} e_i\\right)\n$$\nThe mean of a Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$ is given by the ratio $\\frac{\\alpha}{\\beta}$. Therefore, the posterior mean of $\\lambda$, denoted $\\mathbb{E}[\\lambda \\mid Y]$, is:\n$$\n\\mathbb{E}[\\lambda \\mid y_1, \\dots, y_n] = \\frac{\\alpha_n}{\\beta_n} = \\frac{\\alpha_0 + \\sum_{i=1}^{n} y_i}{\\beta_0 + \\sum_{i=1}^{n} e_i}\n$$\n\nRegarding the limitations of this conjugate construction for complex adaptive systems (CAS): this approach is predicated on the ability to write down and evaluate an analytical likelihood function, $p(y \\mid \\lambda)$. In many CAS models, such as agent-based models or large-scale network simulations, the relationship between model parameters ($\\theta$) and observable outputs ($y$) is defined by a complex, stochastic simulation process. The likelihood function $p(y \\mid \\theta)$ is consequently intractable, meaning it cannot be expressed or evaluated in a closed analytical form. The model acts as a black-box simulator.\n\nApproximate Bayesian Computation (ABC) is preferred in such scenarios precisely because it circumvents the need to evaluate the likelihood. ABC performs \"likelihood-free\" inference by simulating synthetic datasets from the model at various parameter settings and accepting parameter values that generate synthetic data \"close\" to the observed data. This makes it possible to perform Bayesian calibration for models where the likelihood is intractable, which is a common characteristic of complex adaptive systems.",
            "answer": "$$\n\\boxed{\\frac{\\alpha_{0} + \\sum_{i=1}^{n} y_{i}}{\\beta_{0} + \\sum_{i=1}^{n} e_{i}}}\n$$"
        },
        {
            "introduction": "Obtaining a posterior distribution for a model's parameters is only the first step; we must also critically assess whether the model provides a good description of the data. This practice introduces the posterior predictive check, a powerful method for model criticism that involves simulating new data from the calibrated model to see if it resembles the data we actually observed. By calculating the posterior predictive mean and variance of a summary statistic, you will learn to quantify the consistency between your model and reality, a crucial skill for diagnosing model misspecification .",
            "id": "4115312",
            "problem": "Consider an agent-based contagion model in a complex adaptive system where daily new cases are aggregated as a count time series. Let $y = (y_{1}, \\dots, y_{n})$ denote $n$ conditionally independent daily counts given a latent rate parameter $\\lambda$, with the generative model $y_{i} \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$ for $i = 1, \\dots, n$. Suppose calibration was performed using Approximate Bayesian Computation (ABC), yielding an approximate Bayesian posterior for $\\lambda$ given the observed data $y^{\\text{obs}}$ of the form $\\lambda \\mid y^{\\text{obs}} \\sim \\text{Gamma}(\\alpha, \\beta)$ in the rate parameterization, with density $p(\\lambda \\mid y^{\\text{obs}}) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$, $\\alpha > 0$, and $\\beta > 0$. Define the summary statistic $s(y) = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, which is the mean daily count.\n\nUsing only the foundational definitions of the posterior predictive distribution $p(y_{\\text{new}} \\mid y^{\\text{obs}}) = \\int p(y_{\\text{new}} \\mid \\theta) p(\\theta \\mid y^{\\text{obs}}) \\, d\\theta$ and the law of total expectation and variance, derive closed-form analytic expressions for the posterior predictive mean $\\mu_{s} = \\mathbb{E}[s(Y_{\\text{new}}) \\mid y^{\\text{obs}}]$ and variance $\\sigma_{s}^{2} = \\operatorname{Var}(s(Y_{\\text{new}}) \\mid y^{\\text{obs}})$ in terms of $\\alpha$, $\\beta$, and $n$. Then, interpret what it would mean if the observed summary $s(y^{\\text{obs}})$ satisfies $|s(y^{\\text{obs}}) - \\mu_{s}| / \\sigma_{s} \\approx 2$ under a posterior predictive check.\n\nExpress the final analytic expressions exactly; no rounding is required. The final answer must be given as a two-entry row matrix containing $\\mu_{s}$ and $\\sigma_{s}^{2}$, in that order.",
            "solution": "### Step 1: Extract Givens\n- **Generative Process**: $n$ conditionally independent daily counts $y = (y_1, \\dots, y_n)$, where $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$ for $i=1, \\dots, n$.\n- **Observed Data**: $y^{\\text{obs}}$.\n- **Approximate Posterior Distribution**: The posterior distribution of the rate parameter $\\lambda$ given the observed data $y^{\\text{obs}}$ is approximated as $\\lambda \\mid y^{\\text{obs}} \\sim \\text{Gamma}(\\alpha, \\beta)$ (rate parameterization).\n- **Posterior Density**: $p(\\lambda \\mid y^{\\text{obs}}) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$, $\\alpha > 0$, $\\beta > 0$.\n- **Summary Statistic**: $s(y) = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$.\n- **Posterior Predictive Distribution Definition**: $p(y_{\\text{new}} \\mid y^{\\text{obs}}) = \\int p(y_{\\text{new}} \\mid \\theta) p(\\theta \\mid y^{\\text{obs}}) \\, d\\theta$.\n- **Required Tools**: Law of total expectation and law of total variance.\n- **Objectives**:\n    1.  Derive closed-form analytic expressions for the posterior predictive mean $\\mu_{s} = \\mathbb{E}[s(Y_{\\text{new}}) \\mid y^{\\text{obs}}]$ and variance $\\sigma_{s}^{2} = \\operatorname{Var}(s(Y_{\\text{new}}) \\mid y^{\\text{obs}})$.\n    2.  Interpret the meaning of $|s(y^{\\text{obs}}) - \\mu_{s}| / \\sigma_{s} \\approx 2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the principles of Bayesian statistics and probability theory. The use of a Poisson model for count data, a Gamma distribution as a posterior for the rate parameter (it is the conjugate prior, making it a plausible posterior form), and the concept of posterior predictive checks are all standard elements of modern statistical modeling.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information (the generative model, the form of the posterior, the summary statistic) to uniquely determine the requested quantities ($\\mu_s$ and $\\sigma_s^2$).\n- **Objective**: The problem is stated in precise, objective mathematical language, free of subjective claims or ambiguity.\n- **Completeness and Consistency**: The setup is complete and internally consistent. No information is missing or contradictory.\n- **Topic Relevance**: The problem is directly and relevantly situated within the specified topic of *Bayesian calibration and approximate Bayesian computation (ABC)* for *complex adaptive systems modeling*.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now proceed.\n\n### Derivation of Posterior Predictive Mean and Variance\n\nLet $Y_{\\text{new}} = (Y_{\\text{new},1}, \\dots, Y_{\\text{new},n})$ be a new, replicated dataset generated from the model, where $Y_{\\text{new},i} \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$. The summary statistic for this new dataset is $s(Y_{\\text{new}}) = \\frac{1}{n} \\sum_{i=1}^{n} Y_{\\text{new},i}$. We need to find the mean and variance of this statistic, averaged over the posterior distribution of $\\lambda$. The conditioning on $y^{\\text{obs}}$ is implicit in the use of the posterior $p(\\lambda \\mid y^{\\text{obs}})$.\n\n**1. Posterior Predictive Mean $\\mu_s$**\n\nWe use the law of total expectation: $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Z]]$. Let $X = s(Y_{\\text{new}})$ and $Z = \\lambda$.\n$$\n\\mu_s = \\mathbb{E}[s(Y_{\\text{new}}) \\mid y^{\\text{obs}}] = \\mathbb{E}_{\\lambda \\mid y^{\\text{obs}}} \\left[ \\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda, y^{\\text{obs}}] \\right]\n$$\nGiven $\\lambda$, the generation of new data $Y_{\\text{new}}$ is independent of the observed data $y^{\\text{obs}}$. Thus, $\\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda, y^{\\text{obs}}] = \\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda]$.\n\nFirst, we find the inner expectation conditional on $\\lambda$:\n$$\n\\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{\\text{new},i} \\mid \\lambda \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_{\\text{new},i} \\mid \\lambda]\n$$\nFor a Poisson random variable $Y_{\\text{new},i} \\sim \\text{Poisson}(\\lambda)$, the expectation is $\\mathbb{E}[Y_{\\text{new},i} \\mid \\lambda] = \\lambda$. Therefore:\n$$\n\\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda] = \\frac{1}{n} \\sum_{i=1}^{n} \\lambda = \\frac{n\\lambda}{n} = \\lambda\n$$\nNow, we take the expectation of this result with respect to the posterior distribution of $\\lambda$, which is $\\lambda \\mid y^{\\text{obs}} \\sim \\text{Gamma}(\\alpha, \\beta)$.\n$$\n\\mu_s = \\mathbb{E}_{\\lambda \\mid y^{\\text{obs}}}[\\lambda]\n$$\nThe mean of a Gamma distribution with shape $\\alpha$ and rate $\\beta$ is $\\frac{\\alpha}{\\beta}$.\n$$\n\\mu_s = \\frac{\\alpha}{\\beta}\n$$\n\n**2. Posterior Predictive Variance $\\sigma_s^2$**\n\nWe use the law of total variance: $\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid Z)] + \\operatorname{Var}(\\mathbb{E}[X \\mid Z])$. Again, let $X = s(Y_{\\text{new}})$ and $Z = \\lambda$.\n$$\n\\sigma_s^2 = \\operatorname{Var}(s(Y_{\\text{new}}) \\mid y^{\\text{obs}}) = \\mathbb{E}_{\\lambda \\mid y^{\\text{obs}}}[\\operatorname{Var}(s(Y_{\\text{new}}) \\mid \\lambda)] + \\operatorname{Var}_{\\lambda \\mid y^{\\text{obs}}}(\\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda])\n$$\nWe need to compute the two terms on the right-hand side.\n\nFor the first term, we start with the inner variance conditional on $\\lambda$:\n$$\n\\operatorname{Var}(s(Y_{\\text{new}}) \\mid \\lambda) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{\\text{new},i} \\mid \\lambda\\right)\n$$\nSince the $Y_{\\text{new},i}$ are conditionally independent given $\\lambda$:\n$$\n\\operatorname{Var}(s(Y_{\\text{new}}) \\mid \\lambda) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(Y_{\\text{new},i} \\mid \\lambda)\n$$\nThe variance of a Poisson random variable $Y_{\\text{new},i} \\sim \\text{Poisson}(\\lambda)$ is $\\operatorname{Var}(Y_{\\text{new},i} \\mid \\lambda) = \\lambda$.\n$$\n\\operatorname{Var}(s(Y_{\\text{new}}) \\mid \\lambda) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\lambda = \\frac{n\\lambda}{n^2} = \\frac{\\lambda}{n}\n$$\nNow we take the expectation of this quantity with respect to the posterior of $\\lambda$:\n$$\n\\mathbb{E}_{\\lambda \\mid y^{\\text{obs}}}\\left[\\frac{\\lambda}{n}\\right] = \\frac{1}{n} \\mathbb{E}_{\\lambda \\mid y^{\\text{obs}}}[\\lambda] = \\frac{1}{n} \\frac{\\alpha}{\\beta}\n$$\nThis is the first term.\n\nFor the second term, we need the variance of the conditional expectation we found earlier:\n$$\n\\operatorname{Var}_{\\lambda \\mid y^{\\text{obs}}}(\\mathbb{E}[s(Y_{\\text{new}}) \\mid \\lambda]) = \\operatorname{Var}_{\\lambda \\mid y^{\\text{obs}}}(\\lambda)\n$$\nThis is simply the variance of the posterior distribution for $\\lambda$, which is $\\text{Gamma}(\\alpha, \\beta)$. The variance of a Gamma distribution with shape $\\alpha$ and rate $\\beta$ is $\\frac{\\alpha}{\\beta^2}$.\n$$\n\\operatorname{Var}_{\\lambda \\mid y^{\\text{obs}}}(\\lambda) = \\frac{\\alpha}{\\beta^2}\n$$\nThis is the second term.\n\nFinally, we sum the two terms to find the total posterior predictive variance:\n$$\n\\sigma_s^2 = \\frac{\\alpha}{n\\beta} + \\frac{\\alpha}{\\beta^2}\n$$\nCombining the terms with a common denominator:\n$$\n\\sigma_s^2 = \\frac{\\alpha\\beta}{n\\beta^2} + \\frac{\\alpha n}{n\\beta^2} = \\frac{\\alpha\\beta + \\alpha n}{n\\beta^2} = \\frac{\\alpha(n + \\beta)}{n\\beta^2}\n$$\n\n### Interpretation of the Posterior Predictive Check\n\nThe procedure of comparing an observed quantity to its posterior predictive distribution is known as a posterior predictive check (PPC). It is a method for assessing the goodness-of-fit of a Bayesian model. The quantity $|s(y^{\\text{obs}}) - \\mu_{s}| / \\sigma_{s}$ is a standardized measure of discrepancy. It quantifies how many standard deviations the observed summary statistic, $s(y^{\\text{obs}})$, lies from the mean of the distribution of replicated summary statistics, $\\mu_s$.\n\nIf $|s(y^{\\text{obs}}) - \\mu_{s}| / \\sigma_{s} \\approx 2$, it means the observed mean daily count is approximately two standard deviations away from the center of the posterior predictive distribution for that statistic. In frequentist hypothesis testing, a z-score of $2$ is often considered a threshold for statistical significance (e.g., at the $\\alpha \\approx 0.05$ level for a two-tailed test, assuming a normal-like distribution).\n\nIn a Bayesian context, this result would be interpreted as a significant inconsistency between the model and the data, with respect to the feature captured by the summary statistic $s(y)$. The model-generated data, as characterized by the posterior predictive distribution, do not typically produce mean values as extreme as the one observed in the actual data. The observed mean is in the tails of the predictive distribution, indicating it is an unlikely outcome under the fitted model.\n\nThis discrepancy suggests a model misspecification. The model is failing to replicate a key feature of the observed data. Possible reasons for this failure include:\n1.  **Incorrect Generative Model**: The Poisson assumption might be wrong. For instance, real-world contagion data often exhibit overdispersion (variance greater than the mean), which the Poisson model ($Var(y) = E[y]$) does not account for. A Negative Binomial model might be more appropriate.\n2.  **Poor ABC Approximation**: The approximate posterior $\\text{Gamma}(\\alpha, \\beta)$ produced by the ABC algorithm may not be a faithful representation of the true posterior distribution.\n3.  **Inadequate Prior**: The prior distribution used in the Bayesian analysis (which is implicitly part of the calculation leading to the posterior) may have been inappropriate, placing too little mass on the true parameter values.\n\nIn conclusion, a value of $\\approx 2$ for the standardized discrepancy is a strong signal that the model should be critically reviewed, revised, and potentially replaced with a more suitable one.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\alpha}{\\beta} & \\frac{\\alpha(n+\\beta)}{n\\beta^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "We now move to a comprehensive application that mirrors the real-world challenges of calibrating a complex adaptive system. This exercise requires you to implement an Approximate Bayesian Computation (ABC) workflow for an agent-based model where the likelihood is intractable. You will confront the critical choice of summary statistics and employ an advanced diagnostic technique: comparing the resulting posterior distributions to detect subtle forms of model misspecification. This practice integrates simulation, inference, and model criticism, representing the complete hands-on-process of modern Bayesian calibration for complex systems .",
            "id": "4115335",
            "problem": "Consider an agent-based susceptible-infected-susceptible model in a complex adaptive system with $N$ agents on a ring lattice of degree $d=4$ (each agent connected to its two nearest and next-nearest neighbors). Each agent at time $t$ is either susceptible or infected. A susceptible agent with $m$ infected neighbors becomes infected at the next time step with probability $$1 - (1 - p)^{m},$$ and an infected agent recovers with probability $r$ independently of others. Let $\\theta = (p, r)$ denote the model parameters. Let the initial fraction of infected agents be $f_0$, and the system evolve for $T$ discrete time steps. The observed data are the time series of infected counts over time, denoted by $y_{\\text{obs}} = (I_1, I_2, \\dots, I_T)$, where $I_t$ is the number of infected agents at time $t$.\n\nBayesian calibration seeks the posterior distribution $p(\\theta \\mid y_{\\text{obs}})$ via Bayes’ rule $$p(\\theta \\mid y_{\\text{obs}}) \\propto p(y_{\\text{obs}} \\mid \\theta)\\,p(\\theta),$$ but the likelihood $p(y_{\\text{obs}} \\mid \\theta)$ is typically intractable for complex adaptive systems. Approximate Bayesian Computation (ABC) approximates the posterior by comparing summary statistics of simulated data to those of observed data and accepting parameters that produce sufficiently close summaries. Specifically, let $S(y)$ denote a vector of summary statistics of data $y$, and define a distance $d(S(y_{\\text{obs}}), S(y_{\\text{sim}}))$ between the observed summary vector and the summary vector from data $y_{\\text{sim}}$ simulated under $\\theta$. With a tolerance $\\epsilon$, the ABC rejection posterior is the distribution of $\\theta$ under the prior $p(\\theta)$ conditioned on $d(S(y_{\\text{obs}}), S(y_{\\text{sim}})) \\le \\epsilon$.\n\nTo diagnose model misspecification, one can compare ABC posteriors obtained under alternative summary sets. Systematic shifts in the ABC posterior across summary sets can indicate that the model fails to jointly explain different aspects of the observed data, suggesting misspecification.\n\nYour task is to implement an ABC misspecification diagnostic by computing ABC posteriors under three alternative summary sets and quantifying systematic shifts among their posterior means relative to posterior variability.\n\nFundamental base and definitions to use:\n- Bayes’ rule: $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$.\n- Approximate Bayesian Computation (ABC): accept $\\theta$ if $d(S(y_{\\text{obs}}), S(y_{\\text{sim}})) \\le \\epsilon$ for a chosen or data-adaptive tolerance $\\epsilon$.\n- Rejection sampling under a uniform prior over a specified parameter domain.\n- Agent-based susceptible-infected-susceptible dynamics as specified above.\n\nImplement the following:\n1. Prior and parameter domain. Use a uniform prior for $\\theta = (p, r)$ with $p \\sim \\text{Uniform}[0.05, 0.50]$ and $r \\sim \\text{Uniform}[0.05, 0.40]$.\n2. ABC sampling. Use rejection ABC with $M = 1200$ independent parameter draws from the prior. For each draw, simulate the model for $T$ steps and compute the distance between the simulated and observed summary vectors. Define the tolerance $\\epsilon$ as the empirical $q$-quantile of the distances with $q = 0.03$, so that approximately the best $3\\%$ of parameter draws are accepted.\n3. Simulation model for ABC posterior computation. The ABC posterior must be computed using the specified susceptible-infected-susceptible dynamics with infection probability $1 - (1 - p)^m$ and recovery probability $r$.\n4. Summary sets. Compute ABC posteriors with each of the following summary sets. For all summaries, normalize by $N$ and $T$ as specified to make components dimensionless and comparable:\n   - Summary set $\\mathcal{S}_A(y) = \\big[ \\overline{I}/N,\\ \\mathrm{Var}(I)/N^2,\\ \\rho_1 \\big]$, where $\\overline{I}$ is the mean of $I_t$ over $t=1,\\dots,T$, $\\mathrm{Var}(I)$ is the sample variance of $I_t$ over time, and $\\rho_1$ is the lag-$1$ autocorrelation of $I_t$ over time (define $\\rho_1 = 0$ if the variance is zero).\n   - Summary set $\\mathcal{S}_B(y) = \\big[ \\max_t I_t/N,\\ t^\\ast/T,\\ (I_{t_0} - I_1)/N \\big]$, where $t^\\ast$ is the time index of the peak infected count, and $t_0 = \\min\\{5, T\\}$ is a small early-time index used to approximate an initial slope.\n   - Summary set $\\mathcal{S}_C(y) = \\big[ I_T/N,\\ \\sum_{t=1}^T I_t/(N T) \\big]$, the final infected fraction and the average infected fraction over time.\n5. Distance function. For any summary vector $S = (s_1, \\dots, s_k)$, use the Euclidean distance $$d(S(y_{\\text{obs}}), S(y_{\\text{sim}})) = \\left\\| S(y_{\\text{obs}}) - S(y_{\\text{sim}}) \\right\\|_2.$$\n6. Posterior mean and variability. For each summary set, compute the posterior mean $\\mu^{(A)}$, $\\mu^{(B)}$, $\\mu^{(C)}$ of $\\theta$ and the posterior standard deviations $\\sigma^{(A)}$, $\\sigma^{(B)}$, $\\sigma^{(C)}$, where standard deviations are componentwise for $p$ and $r$. Let $\\|\\sigma^{(S)}\\|_2$ denote the Euclidean norm of the posterior standard deviation vector for summary set $S$.\n7. Misspecification diagnostic. Define the maximum pairwise posterior mean distance $$\\Delta_{\\max} = \\max\\left\\{ \\left\\| \\mu^{(A)} - \\mu^{(B)} \\right\\|_2,\\ \\left\\| \\mu^{(A)} - \\mu^{(C)} \\right\\|_2,\\ \\left\\| \\mu^{(B)} - \\mu^{(C)} \\right\\|_2 \\right\\}.$$ Define the average posterior variability $$\\bar{\\sigma} = \\frac{1}{3}\\left( \\|\\sigma^{(A)}\\|_2 + \\|\\sigma^{(B)}\\|_2 + \\|\\sigma^{(C)}\\|_2 \\right).$$ Compute the normalized shift $$S^\\ast = \\frac{\\Delta_{\\max}}{\\bar{\\sigma}}.$$ Declare the model misspecified if $S^\\ast \\ge \\tau$ with threshold $\\tau = 1.25$.\n8. Randomness control. Use a fixed random seed so that results are reproducible.\n\nObserved data generation for the test suite:\n- In well-specified scenarios, generate $y_{\\text{obs}}$ using the same simulation model described in item $3$ with specified true parameters and no additional noise.\n- In misspecified scenarios, generate $y_{\\text{obs}}$ using a mechanism that violates the ABC simulation model assumptions to induce systematic summary-dependent posterior shifts, as specified below.\n- In the weak-data scenario, include observation noise to reduce informativeness.\n\nTest suite. For each test case, use initial infected fraction $f_0 = 0.10$ and degree $d=4$. The detailed test cases are:\n1. Case $1$ (well-specified baseline): $N = 30$, $T = 20$, true parameters $\\theta^\\dagger = (0.25, 0.15)$. Observed data generated with the specified susceptible-infected-susceptible dynamics and no observation noise.\n2. Case $2$ (threshold misspecification): $N = 30$, $T = 20$, true parameters $\\theta^\\dagger = (0.25, 0.15)$. Observed data generated with a threshold contagion effect not present in the ABC simulation model: for a susceptible agent, if the fraction of infected neighbors $m/d$ is at least $\\alpha = 0.30$, then use infection parameter $p_{\\text{eff}} = p$; otherwise use $p_{\\text{eff}} = 0.20\\,p$. The infection probability is $1 - (1 - p_{\\text{eff}})^{m}$ and recovery remains $r$.\n3. Case $3$ (weakly informative data): $N = 20$, $T = 12$, true parameters $\\theta^\\dagger = (0.25, 0.15)$. Observed data generated with the specified susceptible-infected-susceptible dynamics and additive mean-zero Gaussian observation noise with standard deviation $\\sigma = 2$ applied to each $I_t$, rounded to the nearest integer and clipped to the interval $[0, N]$.\n4. Case $4$ (exogenous shock misspecification): $N = 30$, $T = 20$, true parameters $\\theta^\\dagger = (0.25, 0.15)$. Observed data generated with the specified susceptible-infected-susceptible dynamics and exogenous shocks at times $t = 8$ and $t = 15$ that instantaneously infect $k = 4$ randomly chosen susceptible agents at those times; this mechanism is absent in the ABC simulation model.\n\nAlgorithmic requirements:\n- Implement agent-based simulation with vectorized updates over agents for computational efficiency.\n- Use the same ABC sampling procedure for all summary sets and test cases, with the prior, $M$, $q$, and tolerance rule specified above.\n- Use a fixed random seed for reproducibility.\n\nOutput specification:\n- For each of the $4$ test cases, compute the misspecification diagnostic as a boolean value: $\\text{True}$ if $S^\\ast \\ge \\tau$ and $\\text{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases $1$ to $4$. For example, output in the format $\\big[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\big]$, where each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$.",
            "solution": "The problem requires the implementation of a model misspecification diagnostic for an agent-based model using Approximate Bayesian Computation (ABC). The diagnostic relies on quantifying the systematic shifts in posterior parameter estimates when different sets of summary statistics are used for the approximation. A large, statistically significant shift suggests that the model is misspecified, as it cannot consistently reproduce different observable features of the data.\n\nThe validation of the problem statement confirms that it is scientifically grounded, well-posed, and complete. It describes a standard agent-based susceptible-infected-susceptible (SIS) model and uses established methodologies from computational statistics (ABC) and model checking. All parameters, algorithms, and test cases are explicitly and unambiguously defined, allowing for a direct and verifiable implementation.\n\nThe solution proceeds by first constructing the necessary computational components as specified, and then applying them to the four test cases. The core components are:\n1.  An agent-based simulation of the SIS model.\n2.  Functions to compute the three distinct sets of summary statistics.\n3.  An ABC rejection sampling algorithm to approximate the posterior distribution $p(\\theta \\mid y_{\\text{obs}})$.\n4.  A procedure to calculate the misspecification diagnostic $S^\\ast$.\n\n**1. The Agent-Based SIS Simulation Model**\n\nThe system consists of $N$ agents arranged on a ring lattice where each agent is connected to its $d=4$ neighbors: two on each side. The state of each agent $i$ at a discrete time $t$, denoted $s_i(t)$, can be either susceptible ($s_i(t)=0$) or infected ($s_i(t)=1$). The system dynamics are governed by two stochastic processes: infection and recovery.\n\n-   **Infection:** A susceptible agent $i$ (with $s_i(t)=0$) has $m_i(t)$ infected neighbors. It becomes infected at time $t+1$ with a probability given by $P_{\\text{inf}} = 1 - (1 - p)^{m_i(t)}$, where $p$ is the per-contact transmission probability. This formulation assumes that infection can occur from any infected neighbor independently.\n-   **Recovery:** An infected agent $i$ (with $s_i(t)=1$) recovers and becomes susceptible again at time $t+1$ with a constant probability $r$.\n\nThe model parameters are therefore the vector $\\theta = (p, r)$. A simulation starts with an initial fraction $f_0$ of randomly chosen infected agents and evolves for $T$ time steps. The simulation is implemented using vectorized operations for efficiency. The neighborhood structure is pre-computed as an adjacency matrix $\\mathbf{A}$, where $\\mathbf{A}_{ij} = 1$ if agents $i$ and $j$ are neighbors, and $0$ otherwise. At each time step $t$, the vector of infected neighbor counts for all agents is computed as $\\mathbf{m}(t) = \\mathbf{A}\\mathbf{s}(t)$, where $\\mathbf{s}(t)$ is the state vector of all agents. Synchronous updates are performed based on the state at time $t$ to determine the state at $t+1$.\n\nFor generating the observed data $y_{\\text{obs}}$ in the misspecified and noisy scenarios, this core simulation is modified as per the test case specifications:\n-   **Case 2 (Threshold Misspecification):** The infection parameter $p$ is replaced by an effective parameter $p_{\\text{eff}}$ which depends on the local density of infection. If an agent's fraction of infected neighbors $m/d \\ge \\alpha=0.30$, then $p_{\\text{eff}}=p$; otherwise, $p_{\\text{eff}} = 0.20p$. The ABC inference algorithm, however, remains unaware of this mechanism and uses the simpler, standard SIS model.\n-   **Case 3 (Weakly Informative Data):** The true infected counts $I_t$ are corrupted by additive Gaussian noise with mean $0$ and standard deviation $\\sigma=2$. The resulting values are rounded to the nearest integer and clipped to the valid range $[0, N]$.\n-   **Case 4 (Exogenous Shock Misspecification):** At specified times $t \\in \\{8, 15\\}$, a fixed number of $k=4$ susceptible agents are instantaneously and randomly chosen to become infected. This external influence is not part of the standard SIS model used for ABC.\n\n**2. Summary Statistics and Distance Metric**\n\nThe likelihood function $p(y_{\\text{obs}} \\mid \\theta)$ is intractable, necessitating the ABC approach. This involves replacing the full data vector $y = (I_1, \\dots, I_T)$ with a lower-dimensional summary statistic vector $S(y)$. Three different summary statistic sets are used to probe different aspects of the system's dynamics:\n\n-   $\\mathcal{S}_A(y) = \\big[ \\overline{I}/N,\\ \\mathrm{Var}(I)/N^2,\\ \\rho_1 \\big]$: Captures the central tendency, variability, and temporal correlation of the infected count. $\\overline{I}$ is the time-mean, $\\mathrm{Var}(I)$ is the sample variance, and $\\rho_1$ is the lag-$1$ autocorrelation.\n-   $\\mathcal{S}_B(y) = \\big[ \\max_t I_t/N,\\ t^\\ast/T,\\ (I_{t_0} - I_1)/N \\big]$: Focuses on the epidemic peak characteristics (magnitude and timing) and initial growth dynamics. $t^\\ast$ is the time of the peak, and $t_0 = \\min\\{5, T\\}$.\n-   $\\mathcal{S}_C(y) = \\big[ I_T/N,\\ \\sum_{t=1}^T I_t/(N T) \\big]$: Captures the final state and overall prevalence.\n\nThe distance between the observed summary vector $S(y_{\\text{obs}})$ and a simulated summary vector $S(y_{\\text{sim}})$ is the Euclidean distance: $d(S_{\\text{obs}}, S_{\\text{sim}}) = \\| S_{\\text{obs}} - S_{\\text{sim}} \\|_2$.\n\n**3. ABC Rejection Sampling**\n\nTo approximate the posterior distribution for each summary set, a rejection sampling algorithm is employed. The steps are:\n1.  Draw a total of $M=1200$ parameter vectors $\\theta_i = (p_i, r_i)$ from the uniform prior distributions: $p_i \\sim \\text{Uniform}[0.05, 0.50]$ and $r_i \\sim \\text{Uniform}[0.05, 0.40]$.\n2.  For each $\\theta_i$, simulate the standard SIS model to generate a synthetic dataset $y_{\\text{sim}}^{(i)}$.\n3.  Compute the summary statistics $S(y_{\\text{sim}}^{(i)})$ for the simulated data.\n4.  Calculate the distance $d_i = \\| S(y_{\\text{obs}}) - S(y_{\\text{sim}}^{(i)}) \\|_2$.\n5.  After all $M$ simulations, determine the acceptance tolerance $\\epsilon$ as the empirical $q=0.03$ quantile of the computed distances $\\{d_1, \\dots, d_M\\}$.\n6.  The set of accepted parameters, forming the approximate posterior sample, is $\\{\\theta_i \\mid d_i \\le \\epsilon\\}$.\n\nThis process is repeated independently for each of the three summary sets $\\mathcal{S}_A$, $\\mathcal{S}_B$, and $\\mathcal{S}_C$.\n\n**4. The Misspecification Diagnostic**\n\nWith the three approximate posterior samples, we compute the posterior mean vector $\\mu^{(S)}$ and the vector of posterior component-wise standard deviations $\\sigma^{(S)}$ for each summary set $S \\in \\{A, B, C\\}$. A significant shift between the posterior means, relative to the posterior uncertainty, indicates model misspecification. This is quantified by the diagnostic $S^\\ast$.\n\n-   First, we find the maximum pairwise Euclidean distance between the posterior means:\n    $$ \\Delta_{\\max} = \\max\\left\\{ \\left\\| \\mu^{(A)} - \\mu^{(B)} \\right\\|_2,\\ \\left\\| \\mu^{(A)} - \\mu^{(C)} \\right\\|_2,\\ \\left\\| \\mu^{(B)} - \\mu^{(C)} \\right\\|_2 \\right\\} $$\n-   Next, we compute a measure of the average posterior variability by averaging the Euclidean norms of the standard deviation vectors:\n    $$ \\bar{\\sigma} = \\frac{1}{3}\\left( \\|\\sigma^{(A)}\\|_2 + \\|\\sigma^{(B)}\\|_2 + \\|\\sigma^{(C)}\\|_2 \\right) $$\n-   The normalized shift is the ratio of these two quantities:\n    $$ S^\\ast = \\frac{\\Delta_{\\max}}{\\bar{\\sigma}} $$\n-   The model is declared misspecified if $S^\\ast$ exceeds a predefined threshold $\\tau=1.25$. This threshold implies that the maximum shift between posterior locations is at least $1.25$ times the average scale of posterior uncertainty.\n\nThis procedure is applied to each of the four test cases. A fixed random seed ensures that the generation of observed data and the stochasticity within the ABC algorithm are reproducible, yielding a deterministic final result for each case. The expected outcome is that the well-specified case (Case 1) and the weak-data case (Case 3) will not trigger the diagnostic, while the two misspecified cases (Cases 2 and 4) will.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ABC misspecification diagnostic for all test cases.\n    \"\"\"\n    RANDOM_SEED = 42\n    np.random.seed(RANDOM_SEED)\n\n    def build_lattice_adjacency(N, d):\n        \"\"\"Builds the adjacency matrix for a ring lattice.\"\"\"\n        adj_matrix = np.zeros((N, N), dtype=int)\n        for i in range(N):\n            for j in range(1, d // 2 + 1):\n                adj_matrix[i, (i - j) % N] = 1\n                adj_matrix[i, (i + j) % N] = 1\n        return adj_matrix\n\n    def run_sis_model(N, T, p, r, f0, adj_matrix, d=4, scenario_params=None):\n        \"\"\"\n        Simulates the SIS agent-based model.\n        Flexibly handles different data-generating scenarios.\n        \"\"\"\n        if scenario_params is None:\n            scenario_params = {}\n        \n        # Initial states\n        states = np.zeros(N, dtype=int)\n        num_initial_infected = round(N * f0)\n        initial_infected_indices = np.random.choice(N, num_initial_infected, replace=False)\n        states[initial_infected_indices] = 1\n        \n        infected_counts = np.zeros(T, dtype=int)\n        \n        for t in range(T):\n            # Exogenous shocks (for y_obs generation in Case 4)\n            if scenario_params.get('type') == 'shock' and t in scenario_params['times']:\n                susceptible_indices = np.where(states == 0)[0]\n                if len(susceptible_indices) > 0:\n                    k = scenario_params['k']\n                    num_to_infect = min(k, len(susceptible_indices))\n                    shocked_indices = np.random.choice(susceptible_indices, num_to_infect, replace=False)\n                    states[shocked_indices] = 1\n\n            old_states = states.copy()\n            infected_neighbors = adj_matrix @ old_states\n            \n            # Determine infection probability\n            p_eff = p\n            if scenario_params.get('type') == 'threshold':\n                alpha = scenario_params['alpha']\n                m_frac = infected_neighbors / d\n                p_eff_vec = np.full(N, 0.2 * p)\n                p_eff_vec[m_frac >= alpha] = p\n            else:\n                prob_infection = 1 - (1 - p_eff)**infected_neighbors\n\n            # Stochastic updates\n            rand_inf = np.random.rand(N)\n            rand_rec = np.random.rand(N)\n\n            newly_infected = (old_states == 0) & (rand_inf < prob_infection)\n            newly_recovered = (old_states == 1) & (rand_rec < r)\n\n            states[newly_infected] = 1\n            states[newly_recovered] = 0\n            \n            infected_counts[t] = np.sum(states)\n\n        # Observation noise (for y_obs generation in Case 3)\n        if scenario_params.get('type') == 'noise':\n            noise_std = scenario_params['sigma']\n            noise = np.random.normal(0, noise_std, T)\n            infected_counts = np.round(infected_counts + noise).astype(int)\n            infected_counts = np.clip(infected_counts, 0, N)\n            \n        return infected_counts\n\n    def compute_summaries(y, N, T):\n        \"\"\"Computes the three sets of summary statistics for a given time series y.\"\"\"\n        len_y = len(y)\n        if len_y == 0:\n          y = np.zeros(1)\n        \n        # S_A: Mean, Variance, Autocorrelation\n        mean_I = np.mean(y)\n        var_I = np.var(y, ddof=1) if len_y > 1 else 0.0\n        rho1 = 0.0\n        if var_I > 0 and len_y > 1:\n            rho1 = np.corrcoef(y[:-1], y[1:])[0, 1]\n            if np.isnan(rho1): rho1 = 0.0\n        \n        S_A = np.array([mean_I / N, var_I / (N**2), rho1])\n\n        # S_B: Peak, Time-to-peak, Initial slope\n        max_I = np.max(y) if len_y > 0 else 0.0\n        t_star = (np.argmax(y) + 1) if len_y > 0 else 0.0\n        t0 = min(5, T)\n        initial_slope = (y[t0 - 1] - y[0]) / N if len_y >= t0 else 0.0\n        \n        S_B = np.array([max_I / N, t_star / T, initial_slope])\n\n        # S_C: Final state, Total prevalence\n        final_I = y[-1] if len_y > 0 else 0.0\n        total_I = np.sum(y)\n\n        S_C = np.array([final_I / N, total_I / (N * T)])\n\n        return S_A, S_B, S_C\n\n    test_cases = [\n        {'id': 1, 'N': 30, 'T': 20, 'theta_true': (0.25, 0.15), 'scenario_params': {'type': 'well_spec'}},\n        {'id': 2, 'N': 30, 'T': 20, 'theta_true': (0.25, 0.15), 'scenario_params': {'type': 'threshold', 'alpha': 0.30}},\n        {'id': 3, 'N': 20, 'T': 12, 'theta_true': (0.25, 0.15), 'scenario_params': {'type': 'noise', 'sigma': 2.0}},\n        {'id': 4, 'N': 30, 'T': 20, 'theta_true': (0.25, 0.15), 'scenario_params': {'type': 'shock', 'times': [8, 15], 'k': 4}}\n    ]\n    \n    f0 = 0.10\n    d = 4\n    M = 1200\n    q = 0.03\n    tau = 1.25\n    prior_p_range = [0.05, 0.50]\n    prior_r_range = [0.05, 0.40]\n\n    results = []\n\n    for case in test_cases:\n        N, T = case['N'], case['T']\n        p_true, r_true = case['theta_true']\n        adj_matrix = build_lattice_adjacency(N, d)\n        \n        # 1. Generate observed data y_obs\n        y_obs = run_sis_model(N, T, p_true, r_true, f0, adj_matrix, d, case['scenario_params'])\n        S_obs_A, S_obs_B, S_obs_C = compute_summaries(y_obs, N, T)\n        \n        # 2. Run ABC sampling\n        prior_draws = np.random.rand(M, 2)\n        prior_draws[:, 0] = prior_draws[:, 0] * (prior_p_range[1] - prior_p_range[0]) + prior_p_range[0]\n        prior_draws[:, 1] = prior_draws[:, 1] * (prior_r_range[1] - prior_r_range[0]) + prior_r_range[0]\n        \n        distances_A, distances_B, distances_C = [], [], []\n\n        for i in range(M):\n            p_sim, r_sim = prior_draws[i]\n            y_sim = run_sis_model(N, T, p_sim, r_sim, f0, adj_matrix, d)\n            S_sim_A, S_sim_B, S_sim_C = compute_summaries(y_sim, N, T)\n            \n            distances_A.append(np.linalg.norm(S_obs_A - S_sim_A, 2))\n            distances_B.append(np.linalg.norm(S_obs_B - S_sim_B, 2))\n            distances_C.append(np.linalg.norm(S_obs_C - S_sim_C, 2))\n\n        distances_A = np.array(distances_A)\n        distances_B = np.array(distances_B)\n        distances_C = np.array(distances_C)\n            \n        # 3. Compute posterior statistics for each summary set\n        posterior_stats = {}\n        for S_name, distances in [('A', distances_A), ('B', distances_B), ('C', distances_C)]:\n            epsilon = np.quantile(distances, q)\n            # Handle cases where epsilon is 0 and no samples are accepted\n            if epsilon == 0:\n                accepted_mask = distances == 0\n                if not np.any(accepted_mask): # if still nothing, take the first M*q samples\n                    accepted_mask = np.argsort(distances)[:int(M*q)]\n            else:\n                accepted_mask = distances <= epsilon\n\n            accepted_thetas = prior_draws[accepted_mask]\n            \n            if len(accepted_thetas) > 1:\n                mu = np.mean(accepted_thetas, axis=0)\n                sigma = np.std(accepted_thetas, axis=0)\n            elif len(accepted_thetas) == 1:\n                mu = accepted_thetas[0]\n                sigma = np.zeros(2)\n            else: # No samples accepted, shouldn't happen with quantile method but as a fallback\n                mu = np.zeros(2)\n                sigma = np.zeros(2)\n            \n            posterior_stats[S_name] = {'mu': mu, 'sigma': sigma}\n\n        # 4. Compute misspecification diagnostic S*\n        mu_A, mu_B, mu_C = posterior_stats['A']['mu'], posterior_stats['B']['mu'], posterior_stats['C']['mu']\n        sigma_A, sigma_B, sigma_C = posterior_stats['A']['sigma'], posterior_stats['B']['sigma'], posterior_stats['C']['sigma']\n        \n        dist_AB = np.linalg.norm(mu_A - mu_B, 2)\n        dist_AC = np.linalg.norm(mu_A - mu_C, 2)\n        dist_BC = np.linalg.norm(mu_B - mu_C, 2)\n        \n        delta_max = max(dist_AB, dist_AC, dist_BC)\n        \n        norm_sigma_A = np.linalg.norm(sigma_A, 2)\n        norm_sigma_B = np.linalg.norm(sigma_B, 2)\n        norm_sigma_C = np.linalg.norm(sigma_C, 2)\n        \n        sigma_bar = (norm_sigma_A + norm_sigma_B + norm_sigma_C) / 3.0\n        \n        S_star = delta_max / sigma_bar if sigma_bar > 0 else 0.0\n        \n        results.append(S_star >= tau)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}