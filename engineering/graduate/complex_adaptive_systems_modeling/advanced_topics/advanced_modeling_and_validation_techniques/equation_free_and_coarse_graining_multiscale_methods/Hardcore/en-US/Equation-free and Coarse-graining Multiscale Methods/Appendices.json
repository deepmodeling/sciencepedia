{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any coarse-graining method lies in the operators that translate information between microscopic and macroscopic scales. This first practice guides you through implementing the core `restriction` and `lifting` operators for a one-dimensional system. By constructing a lifting operator that generates the smoothest possible fine-scale field consistent with coarse averages , you will gain hands-on experience with the variational principles that often underpin the robust design of these inter-scale bridges.",
            "id": "4121742",
            "problem": "Consider a one-dimensional periodic lattice with $N$ sites indexed by $j \\in \\{0,1,\\ldots,N-1\\}$ and lattice spacing set to a unitless value. Let a microscopic field be represented by a vector $u \\in \\mathbb{R}^N$, where $u_j$ denotes the field at site $j$. A coarse-graining partition divides the lattice into $K$ contiguous, non-overlapping blocks of uniform size $m$, so that $N = K m$ and the $i$-th block is $B_i = \\{i m, i m + 1, \\ldots, (i+1)m - 1\\}$ with periodic indexing understood modulo $N$.\n\nRestriction is defined by block averaging: the coarse variable vector $\\bar{u} \\in \\mathbb{R}^K$ has components\n$$\n\\bar{u}_i = \\frac{1}{m} \\sum_{j \\in B_i} u_j, \\quad i = 0,1,\\ldots,K-1.\n$$\nLifting is defined as the solution of a constrained interpolation problem: given target block averages $\\bar{u} \\in \\mathbb{R}^K$, reconstruct a fine field $u \\in \\mathbb{R}^N$ that minimizes a smoothness functional subject to the coarse constraints. Let the discrete second difference operator (the discrete Laplacian on the ring) act as\n$$\n(\\Delta u)_j = u_{j+1} - 2 u_j + u_{j-1}, \\quad j \\in \\{0,1,\\ldots,N-1\\},\n$$\nwith periodic boundary conditions $u_{-1} = u_{N-1}$ and $u_N = u_0$. The lifting $u^\\star$ is the minimizer of the curvature energy\n$$\nE(u) = \\sum_{j=0}^{N-1} \\left((\\Delta u)_j\\right)^2\n$$\nsubject to the block-average constraints\n$$\n\\sum_{j \\in B_i} u_j = m \\, \\bar{u}_i, \\quad i = 0,1,\\ldots,K-1.\n$$\nAngles appearing inside trigonometric functions must be interpreted in radians.\n\nYour tasks are:\n- Implement the restriction operator by block averaging as defined above.\n- Implement the lifting operator by solving the constrained interpolation problem: among all $u \\in \\mathbb{R}^N$ that satisfy the given block-average constraints, select the one that minimizes $E(u)$.\n- Verify consistency of the lifting by computing the coarse averages of the lifted $u^\\star$ and comparing them to the prescribed $\\bar{u}$. Use the maximum absolute discrepancy over blocks as the consistency metric.\n- Report, for each test case, a boolean indicating whether the consistency metric does not exceed a prescribed tolerance, the maximum absolute discrepancy itself, and the curvature energy $E(u^\\star)$.\n\nFundamental base to use:\n- Definitions of restriction by block averages and lifting by constrained interpolation as stated.\n- The periodic discrete second difference operator as stated.\n- The principle of constrained minimization under linear equality constraints, and that the minimum-energy solution exists and is unique up to null-space modes removed by constraints.\n\nNumerical and algorithmic requirements:\n- Treat indices modulo $N$ to enforce periodicity.\n- Ensure $N$ and $m$ satisfy $N = K m$ with integer $K$.\n- Use a fixed tolerance $\\varepsilon = 10^{-10}$ for the consistency boolean.\n- Angles inside trigonometric test fields are in radians.\n- No physical units are involved; all quantities are dimensionless.\n\nTest suite:\n- Case $A$: $N = 128$, $m = 8$, $u_j = \\sin\\!\\left(2\\pi \\cdot 3 \\cdot \\frac{j}{N}\\right) + 0.25 \\cos\\!\\left(2\\pi \\cdot 7 \\cdot \\frac{j}{N}\\right) + 0.05 \\frac{j}{N}$.\n- Case $B$: $N = 64$, $m = 1$, $u_j = \\sin\\!\\left(2\\pi \\cdot 5 \\cdot \\frac{j}{N}\\right)$.\n- Case $C$: $N = 96$, $m = 24$, $u_j = \\sin\\!\\left(2\\pi \\cdot 10 \\cdot \\frac{j}{N}\\right) + 0.1 \\cos\\!\\left(2\\pi \\cdot 12 \\cdot \\frac{j}{N}\\right)$.\n- Case $D$: $N = 72$, $m = 9$, $u_j$ is piecewise constant: $u_j = 1$ for $0 \\le j  N/3$, $u_j = -0.5$ for $N/3 \\le j  2N/3$, and $u_j = 0$ for $2N/3 \\le j  N$.\n\nFor each case, compute $\\bar{u}$ by restriction applied to the specified $u$, perform lifting to obtain $u^\\star$ consistent with $\\bar{u}$, and then compute:\n- The boolean $b$ indicating whether $\\max_i \\left| \\bar{u}^\\star_i - \\bar{u}_i \\right| \\le \\varepsilon$, where $\\bar{u}^\\star$ denotes the restriction of $u^\\star$, and $\\varepsilon = 10^{-10}$.\n- The maximum absolute discrepancy $d = \\max_i \\left| \\bar{u}^\\star_i - \\bar{u}_i \\right|$.\n- The curvature energy $E(u^\\star)$ as defined.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the form $[b, d, E]$. Thus, the final line must look like $[\\,[b_A, d_A, E_A], [b_B, d_B, E_B], [b_C, d_C, E_C], [b_D, d_D, E_D]\\,]$ with the specified values substituted.",
            "solution": "The user has posed a valid problem from the domain of multiscale modeling, specifically concerning equation-free and coarse-graining methods. The task is to implement and test a pair of operators: a **restriction** operator that coarsens a fine-scale field and a **lifting** operator that reconstructs a fine-scale field from coarse data.\n\nThe problem is a well-posed constrained optimization problem. The core task is to find a fine-grained field $u^\\star \\in \\mathbb{R}^N$ that minimizes a \"curvature energy\" functional $E(u)$ while satisfying a set of linear constraints imposed by prescribed block averages. This methodology is standard in a variety of scientific computing contexts for coupling models at different scales.\n\nHerein, I will detail the mathematical formulation and the subsequent algorithmic approach to solving this problem.\n\n### 1. Mathematical Formulation\n\nLet the microscopic field be a vector $u \\in \\mathbb{R}^N$. The problem asks for the solution to the following constrained minimization problem:\n$$\n\\text{minimize} \\quad E(u) = \\sum_{j=0}^{N-1} \\left((\\Delta u)_j\\right)^2 \\quad \\text{subject to} \\quad \\sum_{j \\in B_i} u_j = m \\bar{u}_i, \\quad \\text{for } i=0, \\ldots, K-1.\n$$\nHere, $\\Delta$ is the discrete periodic Laplacian operator, and $B_i$ are the coarse-graining blocks.\n\nThis problem can be elegantly expressed in matrix-vector notation.\nThe objective function is a quadratic form:\n$$\nE(u) = \\|L u\\|_2^2 = u^T L^T L u\n$$\nwhere $L$ is the $N \\times N$ matrix representing the discrete periodic Laplacian operator $\\Delta$. For a $1D$ periodic lattice, this is a symmetric circulant matrix with $-2$ on the main diagonal and $1$ on the two adjacent diagonals (and at the corners due to periodicity). Since $L$ is symmetric, $L^T=L$, and the objective function becomes $u^T L^2 u$. The matrix $L^2$ represents the discrete biharmonic operator $\\Delta^2$.\n\nThe constraints are a set of $K$ linear equations. We can define a $K \\times N$ \"summation\" matrix $C$, where $C_{ij} = 1$ if site $j$ is in block $B_i$ and $C_{ij}=0$ otherwise. The constraints are then given by the system:\n$$\nC u = m \\bar{u}\n$$\nwhere $\\bar{u} \\in \\mathbb{R}^K$ is the vector of given coarse averages.\n\n### 2. Method of Lagrange Multipliers\n\nThis constrained quadratic programming problem can be solved using the method of Lagrange multipliers. We introduce a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^K$ and form the Lagrangian:\n$$\n\\mathcal{L}(u, \\lambda) = u^T L^2 u - \\lambda^T (C u - m \\bar{u})\n$$\nTo find the minimum, we seek the stationary point of $\\mathcal{L}$ by setting its gradients with respect to $u$ and $\\lambda$ to zero.\n\nThe gradient with respect to $u$ is:\n$$\n\\nabla_u \\mathcal{L} = 2 L^2 u - C^T \\lambda = 0\n$$\nThe gradient with respect to $\\lambda$ recovers the constraints:\n$$\n\\nabla_\\lambda \\mathcal{L} = -(C u - m \\bar{u}) = 0 \\implies C u = m \\bar{u}\n$$\n\n### 3. The Karush-Kuhn-Tucker (KKT) System\n\nThese two equations form a single, larger linear system known as the Karush-Kuhn-Tucker (KKT) system. In block matrix form, it is:\n$$\n\\begin{pmatrix}\n2 L^2  -C^T \\\\\nC  0_{K \\times K}\n\\end{pmatrix}\n\\begin{pmatrix}\nu^\\star \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0_{N \\times 1} \\\\\nm\\bar{u}\n\\end{pmatrix}\n$$\nwhere $u^\\star$ is the optimal lifted field we seek. This is a sparse, symmetric indefinite linear system of size $(N+K) \\times (N+K)$. The KKT matrix is invertible as long as the constraints properly remove the null space of the objective function's Hessian ($L^2$). The null space of $L^2$ is the constant field, and the constraints $C u = m \\bar{u}$ are generally inconsistent with a constant $u$ unless all $\\bar{u}_i$ are equal, thus ensuring a unique solution.\n\n### 4. Algorithmic Implementation\n\nThe solution proceeds as follows for each test case:\n1.  **Initialization**: Given parameters $N$ and $m$, calculate $K = N/m$. The initial fine-scale field $u \\in \\mathbb{R}^N$ is constructed according to the provided formula.\n\n2.  **Restriction**: The coarse-scale field $\\bar{u} \\in \\mathbb{R}^K$ is computed by block averaging. A numerically efficient way to implement this is to reshape the $N$-element vector $u$ into a $K \\times m$ matrix and then compute the mean of each row.\n\n3.  **Lifting Construction**: The matrices $L$ and $C$ are constructed. The matrix $L$ is a circulant matrix of size $N \\times N$. The matrix $C$ is a sparse matrix of size $K \\times N$. The KKT system is assembled from these building blocks ($L^2$, $C$, $C^T$) and the right-hand side vector is formed using the computed $\\bar{u}$.\n\n4.  **Lifting Solution**: The $(N+K) \\times (N+K)$ KKT linear system is solved for the composite vector containing $u^\\star$ and $\\lambda$. The first $N$ components of the solution vector constitute the desired lifted field $u^\\star \\in \\mathbb{R}^N$.\n\n5.  **Verification and Analysis**:\n    *   To verify consistency, the restriction operator is applied to the computed lifted field $u^\\star$, yielding a new coarse field $\\bar{u}^\\star$. The maximum absolute discrepancy $d = \\max_i |\\bar{u}^\\star_i - \\bar{u}_i|$ is calculated.\n    *   The consistency boolean $b$ is true if $d \\le \\varepsilon = 10^{-10}$.\n    *   The curvature energy of the lifted field, $E(u^\\star) = \\| L u^\\star \\|_2^2$, is computed.\n\nThis procedure, based on fundamental principles of constrained optimization, is implemented to determine the required outputs for each specified test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests restriction and lifting operators for a 1D periodic lattice.\n    The lifting operator is found by solving a constrained minimization problem\n    to find the smoothest field consistent with given coarse-grain averages.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 128, 'm': 8, 'u_func': 'trig_linear', 'params': (3, 7, 0.25, 0.05)},\n        {'N': 64,  'm': 1, 'u_func': 'trig', 'params': (5,)},\n        {'N': 96,  'm': 24, 'u_func': 'trig_cosine', 'params': (10, 12, 0.1)},\n        {'N': 72,  'm': 9, 'u_func': 'piecewise', 'params': None}\n    ]\n\n    epsilon = 1e-10\n    results = []\n\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        N = case['N']\n        m = case['m']\n        K = N // m\n        \n        # Generate the initial fine-grained field u\n        j = np.arange(N)\n        u_func = case['u_func']\n        params = case['params']\n        \n        if u_func == 'trig_linear':\n            k1, k2, c2, c3 = params\n            u = np.sin(2 * np.pi * k1 * j / N) + c2 * np.cos(2 * np.pi * k2 * j / N) + c3 * j / N\n        elif u_func == 'trig':\n            k1, = params\n            u = np.sin(2 * np.pi * k1 * j / N)\n        elif u_func == 'trig_cosine':\n            k1, k2, c2 = params\n            u = np.sin(2 * np.pi * k1 * j / N) + c2 * np.cos(2 * np.pi * k2 * j / N)\n        elif u_func == 'piecewise':\n            u = np.zeros(N)\n            n_third = N // 3\n            u[0:n_third] = 1.0\n            u[n_third : 2 * n_third] = -0.5\n            u[2 * n_third : N] = 0.0\n\n        # 1. Restriction: Compute coarse field ubar by block averaging\n        ubar = u.reshape((K, m)).mean(axis=1)\n\n        # 2. Lifting: Solve the constrained minimization problem via KKT system\n        \n        # Construct L, the N x N matrix for the periodic discrete Laplacian (Delta)\n        L = -2 * np.eye(N) + np.roll(np.eye(N), 1, axis=1) + np.roll(np.eye(N), -1, axis=1)\n        \n        # Construct L2, the N x N matrix for the biharmonic operator (Delta^2)\n        L2 = L @ L\n        \n        # Construct C, the K x N constraint matrix for block summation\n        C = np.zeros((K, N))\n        for i in range(K):\n            C[i, i*m:(i+1)*m] = 1.0\n        \n        # Assemble the (N+K) x (N+K) KKT matrix A and right-hand side vector b_vec\n        A_top = np.hstack([2 * L2, -C.T])\n        A_bot = np.hstack([C, np.zeros((K, K))])\n        A = np.vstack([A_top, A_bot])\n        \n        b_vec = np.concatenate([np.zeros(N), m * ubar])\n        \n        # Solve the linear system A * x = b_vec for x = [u_star, lambda]^T\n        x = np.linalg.solve(A, b_vec)\n        u_star = x[:N]\n\n        # 3. Verify consistency\n        # Restrict the lifted field u_star to get ubar_star\n        ubar_star = u_star.reshape((K, m)).mean(axis=1)\n        \n        # Compute the maximum absolute discrepancy d\n        d = np.max(np.abs(ubar_star - ubar))\n        \n        # Check if the discrepancy is within the tolerance epsilon\n        b_consistency = d = epsilon\n        \n        # 4. Calculate curvature energy E(u_star)\n        delta_u_star = L @ u_star\n        E = np.sum(delta_u_star**2)\n\n        results.append([bool(b_consistency), d, E])\n\n    # Final print statement in the exact required format.\n    formatted_results = ','.join(map(str, results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we can move between scales, we can simulate the coarse dynamics without ever writing down a coarse equation. This exercise implements the quintessential Equation-Free `Lifting - Simulation - Restriction` loop to create a coarse time-stepper for a simple slow-fast system. You will investigate the crucial role of the `healing time` , a parameter designed to mitigate the errors introduced by an imperfect, or `inconsistent`, lifting of the coarse state.",
            "id": "4121758",
            "problem": "You are tasked with implementing a computational experiment in equation-free coarse-graining to isolate the effect of the healing time parameter $ \\tau_h $ on coarse predictions. Consider a dimensionless deterministic microscopic model with two time scales, defined by the Ordinary Differential Equation (ODE) system\n$$\n\\frac{dx}{dt} = -x + 0.5\\,y,\\quad\n\\frac{dy}{dt} = \\frac{1}{\\varepsilon}\\left(-y + x^2\\right),\n$$\nwhere $ x $ is a slow variable, $ y $ is a fast variable, and $ \\varepsilon $ is a small positive parameter controlling time-scale separation. All quantities are dimensionless. The purpose of Equation-Free (EF) coarse-graining is to approximate a coarse time-stepper $ M_{\\Delta t} $ for the coarse variable $ U = x $ without deriving closed-form coarse equations. The EF coarse time-stepper with healing time is defined by\n$$\nM_{\\Delta t}^{(\\tau_h)}(U) \\equiv \\left(R \\circ S_{\\tau_h+\\Delta t} \\circ L\\right)(U),\n$$\nwhere $ L $ (lifting) maps $ U $ to a microscopic state by choosing $ x = U $ and $ y = y_{\\text{lift}} $, with $ y_{\\text{lift}} $ specified below; $ S_t $ is the microscopic simulator that advances the ODEs for time $ t $; and $ R(x,y) = x $ is the restriction. The healing time $ \\tau_h $ is the interval during which fast variables relax toward the slow manifold, mitigating lifting inconsistency before the coarse reporting horizon $ \\Delta t $.\n\nImplement the EF coarse time-stepper using the following specifications:\n- Use $ y_{\\text{lift}} = 0 $ for all liftings, intentionally introducing an initial inconsistency with the slow manifold $ y \\approx x^2 $ to expose the role of $ \\tau_h $.\n- Integrate the microscopic ODEs deterministically with a fixed-step explicit Runge–Kutta method of order $ 4 $ (classical RK4). Use a time step $ h = \\min\\{0.001,\\; 0.05\\,\\varepsilon\\} $.\n- Define the reference healing time $ \\tau_{\\text{ref}} = 5.0 $ and measure the sensitivity of coarse predictions by the absolute difference\n$$\nD(U_0,\\Delta t,\\varepsilon,\\tau_h) = \\left|M_{\\Delta t}^{(\\tau_h)}(U_0) - M_{\\Delta t}^{(\\tau_{\\text{ref}})}(U_0)\\right|.\n$$\n\nYour program must compute $ D(U_0,\\Delta t,\\varepsilon,\\tau_h) $ for each $ \\tau_h $ in a prescribed list for each controlled test case in the following test suite. Each test case specifies $ (U_0,\\Delta t,\\varepsilon) $ and a list of $ \\tau_h $ values:\n- Test Case $ 1 $ (happy path, strong separation): $ U_0 = 0.7 $, $ \\Delta t = 1.0 $, $ \\varepsilon = 0.05 $, $ \\tau_h \\in \\{0.0, 0.1, 0.5, 1.0\\} $.\n- Test Case $ 2 $ (boundary: healing equals reporting): $ U_0 = 0.2 $, $ \\Delta t = 0.2 $, $ \\varepsilon = 0.05 $, $ \\tau_h \\in \\{0.0, 0.2\\} $.\n- Test Case $ 3 $ (weaker separation): $ U_0 = -0.5 $, $ \\Delta t = 1.0 $, $ \\varepsilon = 0.3 $, $ \\tau_h \\in \\{0.0, 0.5, 1.0\\} $.\n- Test Case $ 4 $ (small reporting horizon): $ U_0 = 1.0 $, $ \\Delta t = 0.01 $, $ \\varepsilon = 0.05 $, $ \\tau_h \\in \\{0.0, 0.01, 0.1\\} $.\n- Test Case $ 5 $ (long reporting horizon): $ U_0 = 0.0 $, $ \\Delta t = 2.0 $, $ \\varepsilon = 0.05 $, $ \\tau_h \\in \\{0.0, 0.5, 1.0, 2.0\\} $.\n\nAll quantities are dimensionless. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of floats (absolute differences) in the same order as the listed $ \\tau_h $ values. For example, the output format must be of the form $ [[d_{1,1},d_{1,2},\\dots],[d_{2,1},\\dots],\\dots] $ with no additional text. Each $ d_{i,j} $ must be a float.",
            "solution": "The problem is valid as it is scientifically sound, well-posed, and provides a complete, unambiguous set of specifications for a standard computational exercise in the field of multiscale modeling.\n\nThe objective is to implement an Equation-Free (EF) coarse-graining simulation to analyze the effect of the healing time parameter, $\\tau_h$, on the accuracy of coarse-grained predictions. This is achieved by measuring the deviation of predictions made with a given $\\tau_h$ from a reference prediction made with a long, presumably sufficient, healing time $\\tau_{\\text{ref}}$.\n\nThe core of the problem is a microscopic model described by a system of two coupled Ordinary Differential Equations (ODEs):\n$$\n\\frac{dx}{dt} = -x + 0.5\\,y \\\\\n\\frac{dy}{dt} = \\frac{1}{\\varepsilon}\\left(-y + x^2\\right)\n$$\nHere, $x(t)$ is the slow variable and $y(t)$ is the fast variable. The parameter $\\varepsilon \\ll 1$ governs the separation of time scales. The term $1/\\varepsilon$ causes $y$ to evolve much more rapidly than $x$. For a small $\\varepsilon$, the fast variable $y$ quickly relaxes towards a quasi-steady state dictated by the slow variable $x$. By setting $dy/dt \\approx 0$, we find that the system's dynamics are attracted to a slow manifold, which is an invariant or near-invariant lower-dimensional surface in the state space, approximately described by the relation $y \\approx x^2$.\n\nThe Equation-Free (EF) method provides a way to simulate the evolution of coarse variables (here, $U=x$) without deriving an explicit closed-form equation for their dynamics. The evolution is captured by a coarse time-stepper, $M_{\\Delta t}$, which advances the coarse state by a time step $\\Delta t$. The problem defines a specific form of this operator that includes a healing time $\\tau_h$:\n$$\nM_{\\Delta t}^{(\\tau_h)}(U) \\equiv \\left(R \\circ S_{\\tau_h+\\Delta t} \\circ L\\right)(U)\n$$\nThis composite operator works in three stages:\n\n1.  **Lifting ($L$):** The coarse state $U_0$ is mapped to a full microscopic state $(x_0, y_0)$. The problem specifies the lifting rule as $L(U_0) = (x_0, y_0) = (U_0, 0)$. This choice is intentionally \"inconsistent\" because, for a generic $U_0 \\neq 0$, the lifted point $(U_0, 0)$ does not lie on the slow manifold $y \\approx x^2$. This inconsistency introduces an initial transient error.\n\n2.  **Simulation ($S_t$):** The microscopic ODE system is numerically integrated forward in time for a total duration of $t = \\tau_h + \\Delta t$, starting from the lifted initial condition $(x_0, y_0)$. The initial phase of this simulation, of duration $\\tau_h$, is the \"healing\" period. During this time, the fast variable $y$ rapidly converges from its inconsistent initial value ($y_0=0$) towards the slow manifold. After this healing, the subsequent evolution over the \"reporting horizon\" $\\Delta t$ is assumed to be representative of the true coarse dynamics.\n\n3.  **Restriction ($R$):** After the total simulation time has elapsed, the resulting microscopic state $(x_f, y_f)$ is mapped back to a coarse state. The problem specifies the restriction operator as $R(x, y) = x$. Thus, the new coarse state is simply the value of the slow variable at the end of the simulation, $U_f = x_f$.\n\nThe numerical integration of the microscopic ODEs must be performed using the classical fourth-order Runge-Kutta (RK4) method with a fixed time step $h = \\min\\{0.001, 0.05\\,\\varepsilon\\}$. For a vector an ODE system $\\frac{d\\vec{z}}{dt} = \\vec{f}(t, \\vec{z})$ where $\\vec{z} = [x, y]^T$, the RK4 update from time step $n$ to $n+1$ is:\n$$\n\\vec{z}_{n+1} = \\vec{z}_n + \\frac{h}{6}(\\vec{k}_1 + 2\\vec{k}_2 + 2\\vec{k}_3 + \\vec{k}_4)\n$$\nwhere the intermediate slopes are calculated as:\n$$\n\\vec{k}_1 = \\vec{f}(t_n, \\vec{z}_n) \\\\\n\\vec{k}_2 = \\vec{f}(t_n + h/2, \\vec{z}_n + h/2 \\cdot \\vec{k}_1) \\\\\n\\vec{k}_3 = \\vec{f}(t_n + h/2, \\vec{z}_n + h/2 \\cdot \\vec{k}_2) \\\\\n\\vec{k}_4 = \\vec{f}(t_n + h, \\vec{z}_n + h \\cdot \\vec{k}_3)\n$$\nThe total number of integration steps, $N$, for a total simulation duration $T = \\tau_h + \\Delta t$, must be sufficient to cover this period, i.e., $N = \\lceil T/h \\rceil$.\n\nThe sensitivity of the coarse prediction to the choice of $\\tau_h$ is quantified by the absolute difference $D$:\n$$\nD(U_0,\\Delta t,\\varepsilon,\\tau_h) = \\left|M_{\\Delta t}^{(\\tau_h)}(U_0) - M_{\\Delta t}^{(\\tau_{\\text{ref}})}(U_0)\\right|\n$$\nwhere $\\tau_{\\text{ref}} = 5.0$ is a reference healing time assumed to be long enough for the initial transient to fully dissipate. For each test case defined by $(U_0, \\Delta t, \\varepsilon)$, we first compute the reference outcome $U_{\\text{ref}} = M_{\\Delta t}^{(\\tau_{\\text{ref}})}(U_0)$. Then, for each $\\tau_h$ in the specified list, we compute the test outcome $U_{\\text{test}} = M_{\\Delta t}^{(\\tau_h)}(U_0)$ and calculate the difference $|U_{\\text{test}} - U_{\\text{ref}}|$. The collections of these differences for each test case constitute the final result.",
            "answer": "```python\nimport numpy as np\n# scipy is not used as the problem requires a custom RK4 implementation.\n\ndef get_derivatives(state, epsilon):\n    \"\"\"\n    Computes the derivatives for the microscopic ODE system.\n\n    Args:\n        state (np.ndarray): A 1D array [x, y] representing the current state.\n        epsilon (float): The time-scale separation parameter.\n\n    Returns:\n        np.ndarray: A 1D array [dx/dt, dy/dt].\n    \"\"\"\n    x, y = state\n    dx_dt = -x + 0.5 * y\n    dy_dt = (1.0 / epsilon) * (-y + x**2)\n    return np.array([dx_dt, dy_dt])\n\ndef rk4_step(state, h, epsilon):\n    \"\"\"\n    Performs a single step of the classical Runge-Kutta 4th order method.\n\n    Args:\n        state (np.ndarray): The current state vector [x, y].\n        h (float): The time step size.\n        epsilon (float): The time-scale separation parameter.\n\n    Returns:\n        np.ndarray: The state vector [x, y] at the next time step.\n    \"\"\"\n    k1 = get_derivatives(state, epsilon)\n    k2 = get_derivatives(state + 0.5 * h * k1, epsilon)\n    k3 = get_derivatives(state + 0.5 * h * k2, epsilon)\n    k4 = get_derivatives(state + h * k3, epsilon)\n    return state + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\ndef coarse_stepper(U0, delta_t, tau_h, epsilon):\n    \"\"\"\n    Implements the Equation-Free coarse time-stepper M.\n\n    Args:\n        U0 (float): The initial coarse state.\n        delta_t (float): The coarse reporting horizon.\n        tau_h (float): The healing time.\n        epsilon (float): The time-scale separation parameter.\n\n    Returns:\n        float: The coarse state after one coarse time step.\n    \"\"\"\n    # Lifting: Map coarse state U0 to a microscopic state (x, y)\n    state = np.array([U0, 0.0])\n\n    # Simulation: Integrate the microscopic ODEs\n    # Determine the microscopic time step\n    h = min(0.001, 0.05 * epsilon)\n    \n    # Determine total simulation time and number of steps\n    total_time = tau_h + delta_t\n    if total_time = 0:\n        return U0\n\n    num_steps = int(np.ceil(total_time / h))\n\n    # Run the microscopic simulation\n    for _ in range(num_steps):\n        state = rk4_step(state, h, epsilon)\n\n    # Restriction: Return the slow variable x\n    return state[0]\n\ndef solve():\n    \"\"\"\n    Runs the full computational experiment for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (U0, delta_t, epsilon, list_of_tau_h)\n        (0.7, 1.0, 0.05, [0.0, 0.1, 0.5, 1.0]),\n        (0.2, 0.2, 0.05, [0.0, 0.2]),\n        (-0.5, 1.0, 0.3, [0.0, 0.5, 1.0]),\n        (1.0, 0.01, 0.05, [0.0, 0.01, 0.1]),\n        (0.0, 2.0, 0.05, [0.0, 0.5, 1.0, 2.0]),\n    ]\n    \n    tau_ref = 5.0\n    all_results = []\n\n    for U0, delta_t, epsilon, tau_h_list in test_cases:\n        # Calculate the reference coarse prediction using tau_ref\n        U_ref = coarse_stepper(U0, delta_t, tau_ref, epsilon)\n        \n        case_results = []\n        for tau_h in tau_h_list:\n            # Calculate the test coarse prediction\n            U_test = coarse_stepper(U0, delta_t, tau_h, epsilon)\n            \n            # Compute the absolute difference\n            diff = abs(U_test - U_ref)\n            case_results.append(diff)\n            \n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists of floats.\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Why go through the trouble of equation-free simulation instead of deriving a simple coarse model? This final practice delves into the profound reason: memory. By analyzing a system where the exact coarse dynamics are non-Markovian, you will quantify how neglecting memory effects leads to incorrect predictions about system stability and bifurcation points . This exercise highlights the fundamental limitations of naive coarse-graining and powerfully justifies the need for the `equation-free` paradigm.",
            "id": "4121738",
            "problem": "Consider a coarse variable $x(t)$ that is the restriction of a higher-dimensional microscopic description in the equation-free (EF) approach. In the presence of unresolved degrees of freedom, the exact coarse dynamics is non-Markovian and can be represented by a linear Generalized Langevin Equation (GLE) with a memory kernel. Assume the following linear Volterra integro-differential equation governs the coarse dynamics:\n$$\n\\frac{dx}{dt} = \\mu\\, x(t) + \\int_{0}^{\\infty} K(s)\\, x(t - s)\\, ds\n$$\nwhere the control parameter is $ \\mu \\in \\mathbb{R} $ and the memory kernel is exponential,\n$$\nK(s) = \\kappa\\, e^{-s/\\tau}\n$$\nwith $ \\kappa \\in \\mathbb{R} $ and $ \\tau  0 $. This memory kernel can be embedded into a finite-dimensional Markovian system by introducing an auxiliary variable\n$$\ny(t) = \\int_{0}^{\\infty} e^{-s/\\tau}\\, x(t - s)\\, ds\n$$\nwhich satisfies\n$$\n\\frac{dy}{dt} = \\frac{1}{\\tau}\\left(x(t) - y(t)\\right)\n$$\nand yields the two-dimensional linear system\n$$\n\\frac{d}{dt}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \n\\begin{bmatrix}\n\\mu  \\kappa \\\\\n\\frac{1}{\\tau}  -\\frac{1}{\\tau}\n\\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}.\n$$\nLet the matrix be denoted by $A(\\mu,\\kappa,\\tau)$.\n\nIn the EF coarse time-stepper, a lifting operator constructs a consistent microscopic state from the coarse variable value. To model neglect of memory, suppose the lifting sets $y(0)=0$ whenever initializing from $x(0)$. For a short-burst simulation of duration $\\Delta t0$, define the coarse map $x(0) \\mapsto x(\\Delta t)$ computed by the two-dimensional linear system, and estimate a Markovian coarse growth rate by the finite-difference estimator\n$$\n\\hat{\\lambda}(\\Delta t) = \\frac{x(\\Delta t)-x(0)}{\\Delta t\\, x(0)}\n$$\nwith the fixed lifting $x(0)=1$ and $y(0)=0$.\n\nStarting from these foundations, address the following analysis and computation tasks:\n\n1. Compute the dominant (largest real part) eigenvalue $\\lambda_{\\text{true}}$ of $A(\\mu,\\kappa,\\tau)$, which governs coarse stability in the presence of memory.\n2. Define the naive Markovian prediction that neglects memory as $\\lambda_{\\text{naive}}=\\mu$ and the naive critical threshold as $\\mu_{c,\\text{naive}}=0$ (the value at which the Markovian model $\\frac{dx}{dt}=\\mu x$ changes stability).\n3. Derive the true critical threshold $\\mu_{c,\\text{true}}$ at which an eigenvalue of $A(\\mu,\\kappa,\\tau)$ crosses zero, and compute it numerically for given parameters.\n4. Quantify the following biases:\n   - Eigenvalue bias due to neglecting memory: $\\Delta\\lambda = \\lambda_{\\text{true}} - \\lambda_{\\text{naive}}$.\n   - Critical threshold bias due to neglecting memory: $\\Delta\\mu_c = \\mu_{c,\\text{true}} - \\mu_{c,\\text{naive}}$.\n   - EF short-burst estimator bias relative to the true dominant eigenvalue: $\\Delta\\lambda_{\\text{EF}} = \\hat{\\lambda}(\\Delta t) - \\lambda_{\\text{true}}$.\nAll quantities must be computed as real numbers.\n\nYour program must implement these computations exactly for the test suite below, with all calculations performed using the linear two-dimensional embedding dynamics. For each test case, use $x(0)=1$ and $y(0)=0$ for the EF short-burst estimator. No physical units are required. All angles, if any appear, should be in radians; however, this problem has no angles.\n\nTest suite (each test case is a tuple $(\\mu,\\kappa,\\tau,\\Delta t)$):\n- Case 1 (happy path with destabilizing memory relative to naive stability): $(-0.05,\\,0.10,\\,2.0,\\,0.5)$.\n- Case 2 (happy path with stabilizing memory relative to naive instability): $(0.05,\\,-0.20,\\,1.0,\\,0.5)$.\n- Case 3 (boundary case with no memory): $(-0.10,\\,0.0,\\,1.0,\\,0.5)$.\n- Case 4 (long memory, weak coupling): $(-0.10,\\,0.09,\\,10.0,\\,1.0)$.\n- Case 5 (small $\\Delta t$ edge case): $(0.15,\\,0.10,\\,0.2,\\,0.05)$.\n- Case 6 (large $\\Delta t$ edge case): $(-0.01,\\,0.02,\\,5.0,\\,2.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of three floats $[\\Delta\\lambda,\\,\\Delta\\mu_c,\\,\\Delta\\lambda_{\\text{EF}}]$, so the final output is a list of lists ordered according to the test cases, for example:\n$$\n\\big[[\\Delta\\lambda_1,\\,\\Delta\\mu_{c,1},\\,\\Delta\\lambda_{\\text{EF},1}],\\dots,[\\Delta\\lambda_6,\\,\\Delta\\mu_{c,6},\\,\\Delta\\lambda_{\\text{EF},6}]\\big].\n$$",
            "solution": "The starting point is the recognition that coarse dynamics derived from unresolved microscopic variables is non-Markovian. The Mori–Zwanzig (MZ) projection operator formalism and the Generalized Langevin Equation (GLE) provide a systematic basis for such memory effects. With an exponential memory kernel $K(s)=\\kappa e^{-s/\\tau}$, the nonlocal term can be represented by an auxiliary variable, yielding a finite-dimensional Markovian embedding. Specifically, define\n$$\ny(t)=\\int_0^\\infty e^{-s/\\tau}\\, x(t-s)\\, ds,\n$$\nwhich satisfies\n$$\n\\frac{dy}{dt} = \\frac{1}{\\tau}\\left(x(t)-y(t)\\right).\n$$\nSubstituting this into the dynamics results in the linear system\n$$\n\\frac{d}{dt}\\begin{bmatrix}x\\\\y\\end{bmatrix} = \n\\begin{bmatrix}\n\\mu  \\kappa \\\\\n\\frac{1}{\\tau}  -\\frac{1}{\\tau}\n\\end{bmatrix}\n\\begin{bmatrix}x\\\\y\\end{bmatrix} \\equiv A(\\mu,\\kappa,\\tau)\\begin{bmatrix}x\\\\y\\end{bmatrix}.\n$$\nThis system is a mathematically consistent representation of the original GLE with exponential memory kernel.\n\nCoarse stability is determined by the eigenvalues of $A(\\mu,\\kappa,\\tau)$. The dominant (largest real part) eigenvalue, denoted $\\lambda_{\\text{true}}$, governs the long-time behavior of $x(t)$ when $y(t)$ is part of the embedded dynamics. In contrast, neglecting memory corresponds to the naive Markovian model\n$$\n\\frac{dx}{dt} = \\mu\\, x(t),\n$$\nwhose eigenvalue is simply $\\lambda_{\\text{naive}}=\\mu$ and whose stability threshold is $\\mu_{c,\\text{naive}}=0$.\n\nTo derive the true critical threshold $\\mu_{c,\\text{true}}$, consider the characteristic polynomial of $A$:\n$$\np(\\lambda) = \\det\\left(\\lambda I - A\\right) = \\lambda^2 - \\left(\\mu - \\frac{1}{\\tau}\\right)\\lambda - \\frac{\\mu+\\kappa}{\\tau}.\n$$\nA loss of stability via a real eigenvalue crossing occurs when $\\lambda=0$ is an eigenvalue, which requires\n$$\np(0) = -\\frac{\\mu+\\kappa}{\\tau} = 0 \\quad \\Rightarrow \\quad \\mu + \\kappa = 0.\n$$\nThus, the true critical threshold is\n$$\n\\mu_{c,\\text{true}} = -\\kappa,\n$$\nindependent of $\\tau$ in this embedding. This immediately quantifies the threshold bias due to neglecting memory as\n$$\n\\Delta\\mu_c = \\mu_{c,\\text{true}} - \\mu_{c,\\text{naive}} = -\\kappa - 0 = -\\kappa.\n$$\n\nThe eigenvalue bias $\\Delta\\lambda$ is defined by\n$$\n\\Delta\\lambda = \\lambda_{\\text{true}} - \\lambda_{\\text{naive}} = \\lambda_{\\text{true}} - \\mu,\n$$\nwhere $\\lambda_{\\text{true}}$ is the dominant eigenvalue of $A(\\mu,\\kappa,\\tau)$ computed numerically.\n\nTo connect to the equation-free (EF) coarse time-stepper, specify lifting that neglects memory by setting the auxiliary variable initially to zero:\n$$\nx(0)=1,\\quad y(0)=0.\n$$\nEvolving the embedded dynamics for a short burst of duration $\\Delta t0$ yields\n$$\n\\begin{bmatrix}x(\\Delta t)\\\\y(\\Delta t)\\end{bmatrix} = e^{A\\Delta t}\\begin{bmatrix}1\\\\0\\end{bmatrix},\n$$\nwhere $e^{A\\Delta t}$ is the matrix exponential. The EF short-burst estimator for a Markovian coarse growth rate is then\n$$\n\\hat{\\lambda}(\\Delta t) = \\frac{x(\\Delta t)-x(0)}{\\Delta t\\, x(0)} = \\frac{x(\\Delta t)-1}{\\Delta t}.\n$$\nThis estimator mixes the immediate drift $\\mu$ with the transient build-up of memory $y(t)$ and therefore is biased relative to the true dominant eigenvalue. The EF estimator bias is\n$$\n\\Delta\\lambda_{\\text{EF}} = \\hat{\\lambda}(\\Delta t) - \\lambda_{\\text{true}}.\n$$\n\nAlgorithmic steps for each test case $(\\mu,\\kappa,\\tau,\\Delta t)$:\n1. Form the matrix\n$$\nA = \\begin{bmatrix}\n\\mu  \\kappa \\\\\n\\frac{1}{\\tau}  -\\frac{1}{\\tau}\n\\end{bmatrix}.\n$$\n2. Compute the eigenvalues of $A$ and set $\\lambda_{\\text{true}}$ to the eigenvalue with the largest real part.\n3. Set $\\lambda_{\\text{naive}}=\\mu$ and $\\mu_{c,\\text{naive}}=0$.\n4. Compute the true threshold $\\mu_{c,\\text{true}}=-\\kappa$, yielding $\\Delta\\mu_c=-\\kappa$.\n5. Compute $e^{A\\Delta t}$, then $x(\\Delta t)$ from $[x(\\Delta t),y(\\Delta t)]^\\top = e^{A\\Delta t}[1,0]^\\top$. Compute $\\hat{\\lambda}(\\Delta t) = (x(\\Delta t)-1)/\\Delta t$.\n6. Compute biases: $\\Delta\\lambda = \\lambda_{\\text{true}} - \\mu$ and $\\Delta\\lambda_{\\text{EF}} = \\hat{\\lambda}(\\Delta t) - \\lambda_{\\text{true}}$.\n7. Aggregate the three floats $[\\Delta\\lambda,\\,\\Delta\\mu_c,\\,\\Delta\\lambda_{\\text{EF}}]$ for output.\n\nInterpretation across the test suite:\n- Case 1 has $\\mu0$ but $\\kappa0$, so $\\mu+\\kappa0$ and the true dominant eigenvalue is expected to be positive (instability), demonstrating that neglecting memory can reverse the stability conclusion.\n- Case 2 has $\\mu0$ but $\\kappa0$, making $\\mu+\\kappa0$; the true dominant eigenvalue can be negative (stability), again reversing naive conclusions.\n- Case 3 sets $\\kappa=0$ (no memory), making all biases zero.\n- Case 4 explores long memory ($\\tau$ large) with weak coupling, stressing the EF estimator’s sensitivity to finite $\\Delta t$ when memory builds slowly.\n- Case 5 uses small $\\Delta t$ where the EF estimator approaches the instantaneous drift and thus may reduce $\\Delta\\lambda_{\\text{EF}}$.\n- Case 6 uses large $\\Delta t$, highlighting that the EF estimator can be more biased as it averages over transient memory effects.\n\nThe program will implement these computations and print a single line of nested lists for the given test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef dominant_real_eigenvalue(A: np.ndarray) - float:\n    \"\"\"Return the eigenvalue of A with the largest real part.\"\"\"\n    eigvals = np.linalg.eigvals(A)\n    # Select by largest real part\n    real_parts = np.real(eigvals)\n    idx = np.argmax(real_parts)\n    return real_parts[idx]\n\ndef ef_short_burst_estimator(A: np.ndarray, dt: float) - float:\n    \"\"\"\n    Compute the EF short-burst estimator hat{lambda}(dt)\n    with lifting x(0)=1, y(0)=0 for the 2D linear system dx/dt = A @ [x;y].\n    \"\"\"\n    v0 = np.array([1.0, 0.0], dtype=float)\n    M = expm(A * dt)\n    x_dt = (M @ v0)[0]\n    hat_lambda = (x_dt - 1.0) / dt\n    return hat_lambda\n\ndef compute_biases(mu: float, kappa: float, tau: float, dt: float):\n    \"\"\"\n    Compute [Delta_lambda, Delta_mu_c, Delta_lambda_EF]\n    for parameters (mu, kappa, tau, dt).\n    \"\"\"\n    # Build the 2x2 system matrix A\n    A = np.array([[mu, kappa], [1.0 / tau, -1.0 / tau]], dtype=float)\n    # True dominant eigenvalue\n    lam_true = dominant_real_eigenvalue(A)\n    # Naive eigenvalue neglecting memory\n    lam_naive = mu\n    # True and naive thresholds\n    mu_c_true = -kappa\n    mu_c_naive = 0.0\n    # Biases\n    delta_lambda = lam_true - lam_naive\n    delta_mu_c = mu_c_true - mu_c_naive\n    # EF short-burst estimator bias relative to true dominant eigenvalue\n    hat_lambda = ef_short_burst_estimator(A, dt)\n    delta_lambda_EF = hat_lambda - lam_true\n    return [float(delta_lambda), float(delta_mu_c), float(delta_lambda_EF)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (-0.05,  0.10, 2.0, 0.5),  # Case 1\n        ( 0.05, -0.20, 1.0, 0.5),  # Case 2\n        (-0.10,  0.00, 1.0, 0.5),  # Case 3\n        (-0.10,  0.09,10.0, 1.0),  # Case 4\n        ( 0.15,  0.10, 0.2, 0.05), # Case 5\n        (-0.01,  0.02, 5.0, 2.0),  # Case 6\n    ]\n\n    results = []\n    for mu, kappa, tau, dt in test_cases:\n        result = compute_biases(mu, kappa, tau, dt)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Print as a single line list of lists\n    def format_list_of_lists(lst):\n        inner = []\n        for sub in lst:\n            inner.append(\"[\" + \",\".join(f\"{x:.12g}\" for x in sub) + \"]\")\n        return \"[\" + \",\".join(inner) + \"]\"\n\n    print(format_list_of_lists(results))\n\nsolve()\n```"
        }
    ]
}