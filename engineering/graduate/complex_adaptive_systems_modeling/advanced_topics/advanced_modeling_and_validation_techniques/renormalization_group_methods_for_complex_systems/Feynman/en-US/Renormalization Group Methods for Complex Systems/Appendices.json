{
    "hands_on_practices": [
        {
            "introduction": "This first practice is a cornerstone of the Renormalization Group method, applying it to the celebrated 2D Ising model. You will use the Migdal-Kadanoff approximation, a computationally tractable scheme, to derive the recursion relation for the coupling constant under a real-space coarse-graining transformation. This exercise  bridges theory and computation, allowing you to analyze the RG flow and locate the non-trivial fixed point that governs the model's critical phase transition.",
            "id": "4139880",
            "problem": "You are tasked with implementing a coarse-graining step of the two-dimensional ferromagnetic Ising model using Kadanoff block-spin averaging and deriving the corresponding renormalized nearest-neighbor coupling. The two-dimensional Ising model is defined on a square lattice with spin variables $s_i \\in \\{-1,+1\\}$ and Hamiltonian\n$$\nH(s) = - J \\sum_{\\langle i,j \\rangle} s_i s_j,\n$$\nwhere $J \\ge 0$ is the dimensionless ferromagnetic coupling, and the sum runs over nearest-neighbor pairs $\\langle i,j \\rangle$. The equilibrium probability of a configuration $s$ is given by the Boltzmann distribution $P(s) \\propto \\exp(-H(s))$.\n\nA renormalization group (RG) transformation coarse-grains the microscopic spins into block spins, here by grouping $2 \\times 2$ spins into one block and assigning the block spin $S_B = \\mathrm{sign}\\left(\\sum_{i \\in B} s_i\\right)$ using the majority rule. To approximate the effective theory on the coarse lattice as another nearest-neighbor Ising model with Hamiltonian\n$$\nH'(S) = - J' \\sum_{\\langle B,B' \\rangle} S_B S_{B'},\n$$\none must integrate out microscopic spins while preserving the lattice coordination structure. A standard approximation that enforces this while maintaining tractability is the Migdal-Kadanoff (MK) bond-moving construction (Migdal-Kadanoff (MK)), in which bonds are redistributed and combined along paths connecting neighboring blocks, followed by tracing out intermediate spins. Under these assumptions, ignoring the generation of higher-order multi-spin couplings, the coarse-graining produces a functional relation $J' = f(J)$.\n\nYour tasks are:\n\n1) Starting from the Ising Hamiltonian and the Boltzmann weight, use exact tracing over a single two-bond chain connecting two coarse-grained block spins to obtain the effective two-spin interaction along that path. Then apply the MK bond-moving factor appropriate for spatial dimension $d=2$ and block size $b=2$ to derive the RG map $J' = f(J)$ in closed form. The derivation must begin from the definitions above, proceed by equating Boltzmann weight ratios for aligned versus anti-aligned boundary spins, and apply the MK rescaling factor $b^{d-1}$ logically to preserve connectivity under coarse-graining.\n\n2) Implement a program that:\n- Computes $f(J)$ for given input $J$.\n- Computes the derivative $f'(J)$.\n- Finds the nontrivial fixed point $J^*$ satisfying $f(J^*) = J^*$ for $J \\in (0, +\\infty)$ to within an absolute tolerance of $10^{-12}$.\nAll quantities are dimensionless and must be expressed as real numbers. No physical units are required.\n\n3) Use the following test suite to exercise your implementation:\n- Case A (boundary): $J = 0.0$. Return $f(0.0)$.\n- Case B (small-coupling expansion): $J = 0.05$. Return $f(0.05)$, and the absolute deviation from the leading small-$J$ expansion, namely $\\left|f(0.05) - 2 \\cdot (0.05)^2\\right|$.\n- Case C (typical value near the fixed point): $J = 0.62$. Return $f(0.62)$.\n- Case D (fixed point): Return $J^*$ and $f'(J^*)$.\n- Case E (monotonicity check): Return the boolean value of the predicate $f(0.4)  f(0.6)$.\n- Case F (two-step RG on a moderate coupling): Return $f(f(0.3))$.\n- Case G (strong coupling behavior): $J = 5.0$. Return $f(5.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following exact order:\n$$\n\\left[f(0.0),\\, f(0.05),\\, \\left|f(0.05) - 2 \\cdot (0.05)^2\\right|,\\, f(0.62),\\, J^*,\\, f'(J^*),\\, \\text{monotonicity boolean},\\, f(f(0.3)),\\, f(5.0)\\right].\n$$\nEach numeric entry must be a floating-point number. The monotonicity entry must be a boolean. No angles or physical units are involved in this problem; all quantities are dimensionless real numbers.",
            "solution": "We begin with the two-dimensional Ising model having Hamiltonian\n$$\nH(s) = -J \\sum_{\\langle i,j \\rangle} s_i s_j,\n$$\nwith $J \\ge 0$ and $s_i \\in \\{-1,+1\\}$, and Boltzmann weight $P(s) \\propto \\exp\\left(J \\sum_{\\langle i,j \\rangle} s_i s_j\\right)$.\n\nUnder Kadanoff block-spin averaging with $2 \\times 2$ blocks, the coarse-grained variables $S_B$ are defined by the majority rule. To derive an effective nearest-neighbor coupling $J'$ between neighboring block spins, we consider the Migdal-Kadanoff (MK) bond-moving approximation with scale factor $b=2$ in spatial dimension $d=2$. In MK, bonds are moved to maintain the lattice’s coordination structure after coarse-graining, and interactions along paths between block spins are combined via tracing out intermediate microscopic spins. This procedure preserves a nearest-neighbor Ising form at the expense of approximating away multi-spin interactions.\n\nConsider two neighboring blocks connected by a path consisting of two identical bonds of strength $J$ in series: spins $s_0$ and $s_2$ at the ends (representing boundary spins aligned with the block spins) and a single intermediate spin $s_1$. The partial Boltzmann weight after tracing out $s_1$ is\n$$\nW(s_0, s_2) = \\sum_{s_1 = \\pm 1} \\exp\\left[J s_0 s_1 + J s_1 s_2\\right] = \\sum_{s_1 = \\pm 1} \\exp\\left[J s_1 (s_0 + s_2)\\right] = 2 \\cosh\\left(J(s_0 + s_2)\\right).\n$$\nWhen $s_0 = s_2$ (aligned), we have $s_0 + s_2 = \\pm 2$, and $W_{\\mathrm{aligned}} = 2 \\cosh(2J)$. When $s_0 = -s_2$ (anti-aligned), we have $s_0 + s_2 = 0$, and $W_{\\mathrm{anti}} = 2 \\cosh(0) = 2$. If we seek an effective two-spin interaction of the Ising form $\\exp\\left[J_{\\mathrm{series}} s_0 s_2\\right]$, then the ratio of aligned to anti-aligned weights must match the ratio of Boltzmann factors:\n$$\n\\frac{W_{\\mathrm{aligned}}}{W_{\\mathrm{anti}}} = \\frac{\\exp(J_{\\mathrm{series}} \\cdot (+1))}{\\exp(J_{\\mathrm{series}} \\cdot (-1))} = \\exp(2 J_{\\mathrm{series}}).\n$$\nHence,\n$$\n\\exp(2 J_{\\mathrm{series}}) = \\frac{2 \\cosh(2J)}{2} = \\cosh(2J),\n$$\nwhich gives\n$$\nJ_{\\mathrm{series}} = \\frac{1}{2} \\ln\\left(\\cosh(2J)\\right).\n$$\n\nThe Migdal-Kadanoff bond-moving step in $d=2$ with block size $b=2$ introduces a multiplicative factor $b^{d-1} = 2$ to the effective coupling to preserve connectivity under coarse-graining. Consequently, the renormalized nearest-neighbor coupling after one RG step is\n$$\nJ' = f(J) = 2 J_{\\mathrm{series}} = \\ln\\left(\\cosh(2J)\\right).\n$$\nEquivalently, using hyperbolic identities,\n$$\nf(J) = 2 \\,\\mathrm{artanh}\\!\\left(\\tanh^2 J\\right),\n$$\nsince\n$$\n\\mathrm{artanh}(x) = \\frac{1}{2} \\ln\\left(\\frac{1+x}{1-x}\\right), \\quad \\frac{1+\\tanh^2 J}{1-\\tanh^2 J} = \\cosh(2J).\n$$\nBoth forms are identical; the logarithmic form is numerically convenient:\n$$\nf(J) = \\ln\\left(\\cosh(2J)\\right).\n$$\n\nThe derivative of $f(J)$ follows directly:\n$$\nf'(J) = \\frac{d}{dJ} \\ln\\left(\\cosh(2J)\\right) = \\frac{2 \\sinh(2J)}{\\cosh(2J)} = 2 \\tanh(2J).\n$$\n\nSmall-coupling expansion provides a consistency check. Using $\\cosh x = 1 + \\frac{x^2}{2} + \\mathcal{O}(x^4)$, for small $J$ we have\n$$\n\\cosh(2J) = 1 + \\frac{(2J)^2}{2} + \\mathcal{O}(J^4) = 1 + 2J^2 + \\mathcal{O}(J^4),\n$$\nso\n$$\nf(J) = \\ln\\left(1 + 2J^2 + \\mathcal{O}(J^4)\\right) = 2J^2 + \\mathcal{O}(J^4),\n$$\nconfirming the leading behavior $f(J) \\approx 2J^2$ as $J \\to 0$.\n\nA fixed point $J^*$ satisfies\n$$\nJ^* = f(J^*) = \\ln\\left(\\cosh(2J^*)\\right).\n$$\nNumerically, this can be solved using a robust one-dimensional root finder for the function $g(J) = f(J) - J$ over an interval such as $J \\in [0, 2]$. The derivative at the fixed point is\n$$\nf'(J^*) = 2 \\tanh(2J^*),\n$$\nwhich, being greater than one, reflects the unstable nature of the nontrivial fixed point under this RG map. The monotonicity of $f(J)$ for $J \\ge 0$ follows from $f'(J) = 2 \\tanh(2J) \\ge 0$ and strictly increasing for $J  0$.\n\nAlgorithmic plan for the program:\n- Implement $f(J) = \\ln(\\cosh(2J))$ using standard library functions.\n- Implement $f'(J) = 2 \\tanh(2J)$.\n- Use a root solver to find $J^*$ solving $f(J) - J = 0$ to tolerance $10^{-12}$.\n- Compute the test outputs in the specified order:\n  1. $f(0.0)$,\n  2. $f(0.05)$,\n  3. $\\left|f(0.05) - 2 \\cdot (0.05)^2\\right|$,\n  4. $f(0.62)$,\n  5. $J^*$,\n  6. $f'(J^*)$,\n  7. the boolean $f(0.4)  f(0.6)$,\n  8. $f(f(0.3))$,\n  9. $f(5.0)$.\nThese are all dimensionless. The program prints a single line containing a comma-separated list enclosed in square brackets in the exact order above.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef f(J: float) - float:\n    \"\"\"\n    Renormalization group map under Migdal-Kadanoff (MK) with b=2 in d=2:\n    J' = ln(cosh(2J)).\n    \"\"\"\n    return float(np.log(np.cosh(2.0 * J)))\n\ndef f_prime(J: float) - float:\n    \"\"\"\n    Derivative of the RG map: f'(J) = 2 * tanh(2J).\n    \"\"\"\n    return float(2.0 * np.tanh(2.0 * J))\n\ndef find_fixed_point(tol: float = 1e-12) - float:\n    \"\"\"\n    Find J* satisfying f(J*) = J* using a robust bracketing method.\n    \"\"\"\n    # Define function g(J) = f(J) - J\n    def g(J):\n        return f(J) - J\n\n    # Bracket the root in [0, 2]; g(0) = 0, but we seek the nontrivial fixed point J*0.\n    # g(0+)  0 (since small J: f(J) ~ 2J^2), and for sufficiently large J, f(J) - J becomes large positive.\n    # However, we need to ensure a crossing where g(J) switches sign to use brentq.\n    # Inspect numerically to find a bracket where g changes sign.\n    # We'll search for a sign change between 0.3 and 1.2.\n    left, right = 0.3, 1.2\n    g_left, g_right = g(left), g(right)\n\n    # If not bracketing, expand search.\n    if g_left * g_right  0:\n        # Try alternate brackets systematically.\n        candidates = [(0.0 + 1e-12, 2.0), (0.2, 0.9), (0.5, 1.5)]\n        bracket_found = False\n        for a, b in candidates:\n            ga, gb = g(a), g(b)\n            if ga * gb = 0:\n                left, right = a, b\n                bracket_found = True\n                break\n        if not bracket_found:\n            # Fallback: scan for sign change\n            xs = np.linspace(0.0 + 1e-9, 2.0, 1001)\n            gs = np.array([g(x) for x in xs])\n            signs = np.sign(gs)\n            # Find any adjacent entries with opposite signs\n            idx = None\n            for i in range(len(xs) - 1):\n                if signs[i] == 0:\n                    left, right = xs[i], xs[i+1]\n                    idx = i\n                    break\n                if signs[i] * signs[i+1]  0:\n                    left, right = xs[i], xs[i+1]\n                    idx = i\n                    break\n            if idx is None:\n                # As a last resort, raise error (should not happen for this function)\n                raise RuntimeError(\"Failed to bracket the fixed point.\")\n    # Use Brent's method to find the root\n    sol = root_scalar(lambda x: f(x) - x, bracket=[left, right], xtol=tol, rtol=tol, method='brentq')\n    if not sol.converged:\n        raise RuntimeError(\"Root finding did not converge.\")\n    return float(sol.root)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # We will compute the outputs in the exact required order:\n    # [f(0.0), f(0.05), |f(0.05) - 2*(0.05)^2|, f(0.62), J_star, f'(J_star), monotonicity_boolean, f(f(0.3)), f(5.0)]\n    J_values = {\n        \"A\": 0.0,\n        \"B\": 0.05,\n        \"C\": 0.62,\n        \"F_inner\": 0.3,\n        \"G\": 5.0\n    }\n\n    # Compute required values\n    f_A = f(J_values[\"A\"])\n    f_B = f(J_values[\"B\"])\n    small_J_approx = 2.0 * (J_values[\"B\"] ** 2)\n    small_J_error = abs(f_B - small_J_approx)\n    f_C = f(J_values[\"C\"])\n    J_star = find_fixed_point(tol=1e-12)\n    fprime_at_Jstar = f_prime(J_star)\n    monotonicity_boolean = f(0.4)  f(0.6)\n    fF_inner = f(J_values[\"F_inner\"])\n    fF = f(fF_inner)\n    f_G = f(J_values[\"G\"])\n\n    results = [\n        f_A,\n        f_B,\n        small_J_error,\n        f_C,\n        J_star,\n        fprime_at_Jstar,\n        monotonicity_boolean,\n        fF,\n        f_G\n    ]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the mechanics of RG transformations, this exercise explores one of their most profound implications: universality. Using a simple and exactly solvable 1D spin chain, you will demonstrate how distinct microscopic parameter sets can flow to the exact same effective model at a larger scale. This hands-on calculation  makes the abstract concept of many-to-one RG flows concrete and highlights the fundamental information loss that gives rise to universal behavior in complex systems.",
            "id": "4139877",
            "problem": "Consider a binary-state interaction model frequently used in complex adaptive systems: three spins $s_1, s_2, s_3 \\in \\{-1, +1\\}$ coupled linearly in a chain. The microscopic Hamiltonian is $H(s_1, s_2, s_3 \\mid K_1, K_2) = - K_1 s_1 s_2 - K_2 s_2 s_3$, where $K_1$ and $K_2$ are microscopic coupling parameters. The microscopic distribution is the Boltzmann distribution $P(s_1, s_2, s_3 \\mid K_1, K_2) \\propto \\exp(-H)$.\n\nCoarse-graining by a single-step Renormalization Group (RG) transformation proceeds by marginalizing over the middle spin $s_2$ to obtain a two-spin effective model for $(s_1, s_3)$ of the form $P_{\\text{eff}}(s_1, s_3 \\mid K'(K_1, K_2)) \\propto \\exp(K'(K_1, K_2)\\, s_1 s_3)$, where $K'(K_1, K_2)$ is the effective coupling after decimation and the overall factor that does not depend on $(s_1, s_3)$ is absorbed into the normalization constant. This RG step is a standard coarse-graining operation in statistical models of complex adaptive systems, grounded in the principle that macro-level statistics arise from integrating out microscopic degrees of freedom.\n\nUsing only the foundational definitions above, carry out the following:\n\n1. Derive an explicit expression for the effective coupling $K'(K_1, K_2)$ by performing the marginalization over $s_2$.\n2. Show that the mapping $(K_1, K_2) \\mapsto K'(K_1, K_2)$ is many-to-one by constructing two distinct microscopic parameter pairs $(K_1, K_2)$ and $(\\tilde{K}_1, \\tilde{K}_2)$ that produce the same effective coupling. Concretely, consider the target value $K' = \\tfrac{1}{2} \\ln 2$. Provide two pairs:\n   - Pair $\\mathcal{A}$: $K_1 = K_2 = \\tfrac{1}{2} \\arccosh(2)$.\n   - Pair $\\mathcal{B}$: $K_1 = \\tfrac{1}{2}\\left(\\arccosh(3) + \\arccosh\\!\\left(\\tfrac{3}{2}\\right)\\right)$ and $K_2 = \\tfrac{1}{2}\\left(\\arccosh(3) - \\arccosh\\!\\left(\\tfrac{3}{2}\\right)\\right)$.\n   Verify that both pairs yield the same $K'$.\n3. Analyze the implications for inference by considering the Fisher Information Matrix (FIM) on microscopic parameters induced by observations at the coarse-grained level. For the effective two-spin model $P_{\\text{eff}}(s_1, s_3 \\mid K') \\propto \\exp(K' s_1 s_3)$, the per-sample Fisher information for $K'$ is defined by $I(K') = \\mathbb{E}\\left[\\left(\\partial_{K'} \\ln P_{\\text{eff}}(s_1, s_3 \\mid K')\\right)^{2}\\right]$. The pullback FIM on $(K_1, K_2)$ is $F(K_1, K_2) = I(K') \\nabla K'(K_1, K_2)\\, \\nabla K'(K_1, K_2)^{\\top}$, and its single nonzero eigenvalue is $\\lambda(K_1, K_2) = I(K') \\|\\nabla K'(K_1, K_2)\\|^{2}$.\n\nCompute the ratio $R$ of the nonzero eigenvalues at the two microscopic pairs $\\mathcal{A}$ and $\\mathcal{B}$ that map to the same effective coupling $K' = \\tfrac{1}{2} \\ln 2$, that is,\n$$\nR \\equiv \\frac{\\lambda(K_1, K_2)\\big|_{\\mathcal{B}}}{\\lambda(K_1, K_2)\\big|_{\\mathcal{A}}}.\n$$\n\nExpress your final answer as a simplified fraction. No rounding is required. There are no physical units in this problem.",
            "solution": "The problem asks for a three-part analysis of a coarse-graining procedure in a simple one-dimensional Ising model with three spins. We will address each part in sequence after validating the problem statement. The problem is scientifically sound, well-posed, and self-contained. The model is a standard construct in statistical physics, and the renormalization group (RG) step described is a canonical example of decimation. The use of the Fisher Information Matrix (FIM) to analyze parameter sensitivity is a standard technique in information geometry. Therefore, the problem is valid.\n\n### Part 1: Derivation of the Effective Coupling $K'$\n\nThe microscopic distribution for the three spins $s_1, s_2, s_3 \\in \\{-1, +1\\}$ is given by the Boltzmann distribution:\n$$P(s_1, s_2, s_3 \\mid K_1, K_2) \\propto \\exp(-H(s_1, s_2, s_3 \\mid K_1, K_2)) = \\exp(K_1 s_1 s_2 + K_2 s_2 s_3)$$\nThe RG transformation consists of marginalizing over the middle spin, $s_2$, to obtain an effective distribution for the remaining spins, $s_1$ and $s_3$.\n$$P_{\\text{eff}}(s_1, s_3) = \\sum_{s_2 \\in \\{-1, +1\\}} P(s_1, s_2, s_3)$$\nSubstituting the expression for the probability and factoring out terms not dependent on $s_2$, we get:\n$$P_{\\text{eff}}(s_1, s_3) \\propto \\sum_{s_2 \\in \\{-1, +1\\}} \\exp(s_2(K_1 s_1 + K_2 s_3))$$\nThe sum over $s_2$ can be written out explicitly:\n$$P_{\\text{eff}}(s_1, s_3) \\propto \\exp(K_1 s_1 + K_2 s_3) + \\exp(-(K_1 s_1 + K_2 s_3))$$\nThis is the definition of the hyperbolic cosine function, scaled by a factor of $2$:\n$$P_{\\text{eff}}(s_1, s_3) \\propto 2 \\cosh(K_1 s_1 + K_2 s_3)$$\nSince we are interested in proportionality, the factor of $2$ can be absorbed into the normalization constant.\n$$P_{\\text{eff}}(s_1, s_3) \\propto \\cosh(K_1 s_1 + K_2 s_3)$$\nWe use the hyperbolic identity $\\cosh(a+b) = \\cosh(a)\\cosh(b) + \\sinh(a)\\sinh(b)$. As $s_1$ and $s_3$ are binary variables $\\{-1, +1\\}$, we have $s_1^2 = 1$ and $s_3^2=1$, which implies $\\cosh(K_1 s_1) = \\cosh(K_1)$ and $\\sinh(K_1 s_1) = s_1 \\sinh(K_1)$ (and similarly for $s_3$). Therefore, we can expand the expression as:\n$$\n\\begin{align*}\n\\cosh(K_1 s_1 + K_2 s_3) = \\cosh(K_1 s_1)\\cosh(K_2 s_3) + \\sinh(K_1 s_1)\\sinh(K_2 s_3) \\\\\n= \\cosh(K_1)\\cosh(K_2) + s_1 s_3 \\sinh(K_1)\\sinh(K_2)\n\\end{align*}\n$$\nThe problem states that the effective distribution has the form $P_{\\text{eff}}(s_1, s_3) \\propto \\exp(K' s_1 s_3)$. We can also expand this exponential form using the fact that $(s_1 s_3)^2 = 1$:\n$$\\exp(K' s_1 s_3) = \\cosh(K') + s_1 s_3 \\sinh(K')$$\nFor the two forms of the effective distribution to be proportional, their functional dependence on $s_1 s_3$ must match. Comparing the two expressions:\n$$A + B s_1 s_3 \\propto C + D s_1 s_3$$\nwhere $A = \\cosh(K_1)\\cosh(K_2)$, $B = \\sinh(K_1)\\sinh(K_2)$, $C=\\cosh(K')$, and $D=\\sinh(K')$. This requires the ratio of coefficients to be equal:\n$$\\frac{B}{A} = \\frac{D}{C}$$\n$$\\frac{\\sinh(K_1)\\sinh(K_2)}{\\cosh(K_1)\\cosh(K_2)} = \\frac{\\sinh(K')}{\\cosh(K')}$$\nThis simplifies to:\n$$\\tanh(K_1)\\tanh(K_2) = \\tanh(K')$$\nSolving for $K'$, we obtain the RG mapping:\n$$K'(K_1, K_2) = \\arctanh(\\tanh(K_1)\\tanh(K_2))$$\n\n### Part 2: Verification of Parameter Pairs\n\nWe are asked to show that two distinct microscopic parameter pairs, $\\mathcal{A}$ and $\\mathcal{B}$, map to the same effective coupling $K' = \\frac{1}{2} \\ln 2$.\nFirst, let's find the value of $\\tanh(K')$ corresponding to the target $K'$:\n$$\\tanh(K') = \\tanh\\left(\\frac{1}{2} \\ln 2\\right)$$\nUsing the definition of tanh in terms of exponentials:\n$$\\tanh\\left(\\frac{1}{2} \\ln 2\\right) = \\frac{\\exp(\\frac{1}{2} \\ln 2) - \\exp(-\\frac{1}{2} \\ln 2)}{\\exp(\\frac{1}{2} \\ln 2) + \\exp(-\\frac{1}{2} \\ln 2)} = \\frac{\\sqrt{2} - 1/\\sqrt{2}}{\\sqrt{2} + 1/\\sqrt{2}} = \\frac{2-1}{2+1} = \\frac{1}{3}$$\nSo, we must verify that for both pairs, $\\tanh(K_1)\\tanh(K_2) = 1/3$.\n\n**Pair $\\mathcal{A}$:** $K_1 = K_2 = \\frac{1}{2} \\arccosh(2)$.\nLet $K_{\\mathcal{A}} = K_1 = K_2$. Then we need to show $\\tanh^2(K_{\\mathcal{A}}) = 1/3$.\nFrom the given $K_{\\mathcal{A}}$, we have $2K_{\\mathcal{A}} = \\arccosh(2)$, which means $\\cosh(2K_{\\mathcal{A}}) = 2$.\nUsing the identity $\\cosh(2x) = \\frac{1+\\tanh^2(x)}{1-\\tanh^2(x)}$, we have:\n$$2 = \\frac{1+\\tanh^2(K_{\\mathcal{A}})}{1-\\tanh^2(K_{\\mathcal{A}})}$$\nLet $t^2 = \\tanh^2(K_{\\mathcal{A}})$. Then $2(1-t^2) = 1+t^2 \\implies 2-2t^2=1+t^2 \\implies 1=3t^2 \\implies t^2=1/3$.\nThus, for Pair $\\mathcal{A}$, $\\tanh(K_1)\\tanh(K_2) = 1/3$.\n\n**Pair $\\mathcal{B}$:** $K_1 = \\frac{1}{2}(\\arccosh(3) + \\arccosh(\\frac{3}{2}))$ and $K_2 = \\frac{1}{2}(\\arccosh(3) - \\arccosh(\\frac{3}{2}))$.\nLet $u = \\arccosh(3)$ and $v = \\arccosh(\\frac{3}{2})$. Then $K_1 = (u+v)/2$ and $K_2 = (u-v)/2$.\nWe need to compute $\\tanh(K_1)\\tanh(K_2)$. A useful identity is $\\tanh(x)\\tanh(y) = \\frac{\\cosh(x+y)-\\cosh(x-y)}{\\cosh(x+y)+\\cosh(x-y)}$.\nLet $x=K_1$ and $y=K_2$. Then $x+y = K_1+K_2 = u = \\arccosh(3)$ and $x-y = K_1-K_2 = v = \\arccosh(\\frac{3}{2})$.\nBy definition, $\\cosh(u)=3$ and $\\cosh(v)=3/2$. Substituting this into the identity:\n$$\\tanh(K_1)\\tanh(K_2) = \\frac{\\cosh(u) - \\cosh(v)}{\\cosh(u) + \\cosh(v)} = \\frac{3 - 3/2}{3 + 3/2} = \\frac{3/2}{9/2} = \\frac{3}{9} = \\frac{1}{3}$$\nThus, for Pair $\\mathcal{B}$ as well, $\\tanh(K_1)\\tanh(K_2) = 1/3$.\nBoth pairs map to $K' = \\arctanh(1/3) = \\frac{1}{2} \\ln 2$.\n\n### Part 3: Computation of the Ratio of Eigenvalues\n\nThe ratio $R$ to be computed is:\n$$R = \\frac{\\lambda(K_1, K_2)\\big|_{\\mathcal{B}}}{\\lambda(K_1, K_2)\\big|_{\\mathcal{A}}}$$\nwhere $\\lambda(K_1, K_2) = I(K') \\|\\nabla K'(K_1, K_2)\\|^{2}$.\nSince both pairs $\\mathcal{A}$ and $\\mathcal{B}$ map to the same effective coupling $K'$, the factor $I(K')$ is identical for the numerator and denominator and thus cancels out.\n$$R = \\frac{\\|\\nabla K'(K_1, K_2)\\|^{2} \\big|_{\\mathcal{B}}}{\\|\\nabla K'(K_1, K_2)\\|^{2} \\big|_{\\mathcal{A}}}$$\nWe need to compute the gradient of $K'(K_1, K_2) = \\arctanh(\\tanh(K_1)\\tanh(K_2))$.\nLet $t_1 = \\tanh(K_1)$ and $t_2 = \\tanh(K_2)$. Then $K'=\\arctanh(t_1 t_2)$.\nThe partial derivatives are:\n$$\\frac{\\partial K'}{\\partial K_1} = \\frac{1}{1-(t_1 t_2)^2} \\cdot \\frac{\\partial(t_1 t_2)}{\\partial K_1} = \\frac{(1-t_1^2) t_2}{1-t_1^2 t_2^2}$$\n$$\\frac{\\partial K'}{\\partial K_2} = \\frac{1}{1-(t_1 t_2)^2} \\cdot \\frac{\\partial(t_1 t_2)}{\\partial K_2} = \\frac{t_1 (1-t_2^2)}{1-t_1^2 t_2^2}$$\nThe squared norm of the gradient is $\\|\\nabla K'\\|^2 = (\\frac{\\partial K'}{\\partial K_1})^2 + (\\frac{\\partial K'}{\\partial K_2})^2$:\n$$\\|\\nabla K'\\|^2 = \\frac{(1-t_1^2)^2 t_2^2 + t_1^2 (1-t_2^2)^2}{(1-t_1^2 t_2^2)^2} = \\frac{t_2^2-2t_1^2t_2^2+t_1^4t_2^2 + t_1^2-2t_1^2t_2^2+t_1^2t_2^4}{(1-t_1^2 t_2^2)^2}$$\n$$ = \\frac{t_1^2+t_2^2 - 4(t_1t_2)^2 + (t_1t_2)^2(t_1^2+t_2^2)}{(1-(t_1t_2)^2)^2}$$\nLet $S = t_1^2+t_2^2$ and $P=t_1 t_2$. The expression becomes:\n$$\\|\\nabla K'\\|^2 = \\frac{S - 4P^2 + P^2 S}{(1-P^2)^2} = \\frac{S(1+P^2) - 4P^2}{(1-P^2)^2}$$\nFor both pairs, we have $P = t_1 t_2 = 1/3$ and $P^2=1/9$.\nThe expression for $\\|\\nabla K'\\|^2$ simplifies to a function of $S$:\n$$\\|\\nabla K'\\|^2 = \\frac{S(1+1/9) - 4/9}{(1-1/9)^2} = \\frac{S(10/9) - 4/9}{(8/9)^2} = \\frac{(10S-4)/9}{64/81} = \\frac{9(10S-4)}{64} = \\frac{9(5S-2)}{32}$$\nThe ratio $R$ is therefore:\n$$R = \\frac{\\frac{9(5S_{\\mathcal{B}}-2)}{32}}{\\frac{9(5S_{\\mathcal{A}}-2)}{32}} = \\frac{5S_{\\mathcal{B}}-2}{5S_{\\mathcal{A}}-2}$$\nwhere $S_{\\mathcal{A}}$ and $S_{\\mathcal{B}}$ are the values of $S=t_1^2+t_2^2$ for pairs $\\mathcal{A}$ and $\\mathcal{B}$.\n\n**For Pair $\\mathcal{A}$:** $K_1=K_2$. We found $t_1^2=\\tanh^2(K_1)=1/3$ and $t_2^2=\\tanh^2(K_2)=1/3$.\n$$S_{\\mathcal{A}} = t_1^2+t_2^2 = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$$\n**For Pair $\\mathcal{B}$:** Let $u = \\arccosh(3)$ and $v = \\arccosh(3/2)$.\n$\\cosh(u)=3 \\implies \\sinh(u)=\\sqrt{\\cosh^2(u)-1}=\\sqrt{8}=2\\sqrt{2}$. So $\\tanh(u) = 2\\sqrt{2}/3$.\n$\\cosh(v)=3/2 \\implies \\sinh(v)=\\sqrt{(3/2)^2-1}=\\sqrt{5/4}=\\sqrt{5}/2$. So $\\tanh(v) = \\sqrt{5}/3$.\nWe have $K_1+K_2 = u$ and $K_1-K_2 = v$.\n$t_1+t_2 = \\tanh(K_1)+\\tanh(K_2)$ and $t_1-t_2 = \\tanh(K_1)-\\tanh(K_2)$.\nUsing sum identities for tanh:\n$\\tanh(u) = \\tanh(K_1+K_2) = \\frac{t_1+t_2}{1+t_1t_2} \\implies \\frac{2\\sqrt{2}}{3} = \\frac{t_1+t_2}{1+1/3} \\implies t_1+t_2 = \\frac{4}{3} \\frac{2\\sqrt{2}}{3} = \\frac{8\\sqrt{2}}{9}$.\n$\\tanh(v) = \\tanh(K_1-K_2) = \\frac{t_1-t_2}{1-t_1t_2} \\implies \\frac{\\sqrt{5}}{3} = \\frac{t_1-t_2}{1-1/3} \\implies t_1-t_2 = \\frac{2}{3} \\frac{\\sqrt{5}}{3} = \\frac{2\\sqrt{5}}{9}$.\nWe need $S_{\\mathcal{B}} = t_1^2+t_2^2$. We use the identity $2(t_1^2+t_2^2)=(t_1+t_2)^2+(t_1-t_2)^2$.\n$$2S_{\\mathcal{B}} = \\left(\\frac{8\\sqrt{2}}{9}\\right)^2 + \\left(\\frac{2\\sqrt{5}}{9}\\right)^2 = \\frac{64 \\cdot 2}{81} + \\frac{4 \\cdot 5}{81} = \\frac{128+20}{81} = \\frac{148}{81}$$\n$$S_{\\mathcal{B}} = \\frac{148}{2 \\cdot 81} = \\frac{74}{81}$$\nNow, we compute the ratio $R$:\n$$R = \\frac{5S_{\\mathcal{B}}-2}{5S_{\\mathcal{A}}-2} = \\frac{5\\left(\\frac{74}{81}\\right)-2}{5\\left(\\frac{2}{3}\\right)-2} = \\frac{\\frac{370}{81}-\\frac{162}{81}}{\\frac{10}{3}-\\frac{6}{3}} = \\frac{208/81}{4/3}$$\n$$R = \\frac{208}{81} \\cdot \\frac{3}{4} = \\frac{52}{27}$$\nThis is a simplified fraction as $52 = 4 \\times 13$ and $27=3^3$.",
            "answer": "$$\\boxed{\\frac{52}{27}}$$"
        },
        {
            "introduction": "We now pivot to a powerful and elegant RG technique designed for systems where quenched disorder, rather than thermal fluctuations, dictates the physics. This practice introduces the Strong-Disorder Renormalization Group (SDRG), an iterative, non-perturbative approach ideal for disordered networks and spin glasses. By deriving the decimation rule from first principles and implementing the algorithm , you will gain experience with a method that sequentially simplifies a complex system to reveal its essential low-energy properties.",
            "id": "4139836",
            "problem": "Consider a symmetric weighted network of $N$ nodes with nonnegative pairwise couplings represented by a real symmetric matrix $J \\in \\mathbb{R}^{N \\times N}$, where $J_{ij} \\ge 0$ for all $i \\ne j$ and $J_{ii} = 0$ for all $i$. Assume the network models a disordered ferromagnetic interaction system whose energy is defined by the Ising Hamiltonian $H = -\\sum_{ij} J_{ij} s_i s_j$, where $s_i \\in \\{-1, +1\\}$ denotes the spin of node $i$. The strong-disorder regime is characterized by a hierarchy in the magnitudes of the couplings, where the largest coupling $J_{ij}$ is significantly greater than its neighboring couplings.\n\nYour task is to formalize and implement a Strong-Disorder Renormalization Group (SDRG) procedure for this network that operates by iteratively eliminating the strongest bond and computing effective couplings for the remaining nodes. Begin from fundamental principles as follows:\n\n- Use the definition of the Ising Hamiltonian $H$ and the assumption of a strong ferromagnetic coupling $J_{ij}$ to argue that the dominant interaction constrains the low-energy subspace to configurations where $s_i = s_j$. Using a systematic projection onto this low-energy subspace, derive a principled rule for how couplings between the merged pair and any other node $k$ must be transformed to produce an effective coarse-grained description that preserves the leading-order energetic contributions.\n\n- Ensure your derivation and algorithm respect these constraints:\n  1. The matrix $J$ must remain symmetric with nonnegative off-diagonal entries and zero diagonal throughout the procedure.\n  2. At each iteration, select the pair $(i,j)$ with the largest $J_{ij}$, breaking ties by choosing the lexicographically smallest pair among those with equal value (the pair with the smallest $i$, and among those the smallest $j$).\n  3. Merge nodes $i$ and $j$ into a single effective node, reduce the matrix size by one, and update all couplings to the remaining nodes in a manner consistent with the low-energy projection you derived.\n  4. Stop the iterative elimination process when either the maximum coupling among all remaining pairs is less than or equal to a specified threshold $\\lambda_{\\min}$ or only a single node remains.\n\nYou must then implement this SDRG procedure as a complete, runnable program that takes no input and instead executes on the following fixed test suite. For each test case, start from the given $(N, J, \\lambda_{\\min})$, apply the SDRG until the stopping condition is met, and return the final effective coupling matrix:\n\nTest suite:\n- Case $\\,(1)$: $N = 4$, \n$$\nJ^{(1)} = \n\\begin{pmatrix}\n0  3.0  0.5  0.0 \\\\\n3.0  0  1.0  0.2 \\\\\n0.5  1.0  0  2.0 \\\\\n0.0  0.2  2.0  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(1)} = 1.5.\n$$\n- Case $\\,(2)$: $N = 3$,\n$$\nJ^{(2)} = \n\\begin{pmatrix}\n0  2.0  2.0 \\\\\n2.0  0  1.0 \\\\\n2.0  1.0  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(2)} = 1.5.\n$$\n- Case $\\,(3)$: $N = 4$,\n$$\nJ^{(3)} = \n\\begin{pmatrix}\n0  0.4  0.3  0.2 \\\\\n0.4  0  0.1  0.2 \\\\\n0.3  0.1  0  0.3 \\\\\n0.2  0.2  0.3  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(3)} = 0.5.\n$$\n- Case $\\,(4)$: $N = 5$,\n$$\nJ^{(4)} = \n\\begin{pmatrix}\n0  1.0  1.0  1.0  1.0 \\\\\n1.0  0  0.0  0.0  0.0 \\\\\n1.0  0.0  0  0.0  0.0 \\\\\n1.0  0.0  0.0  0  0.0 \\\\\n1.0  0.0  0.0  0.0  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(4)} = 1.0.\n$$\n\nAll matrix entries are in dimensionless units. Your program must:\n- Implement the SDRG computation consistent with the derived projection rule and constraints above.\n- For each test case, output the final effective coupling matrix as a nested list of lists of floats, preserving symmetry and zero diagonal.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case’s final matrix represented as a nested list of lists of floats. For example: $[M^{(1)},M^{(2)},M^{(3)},M^{(4)}]$ where $M^{(c)}$ is the final matrix for case $c$.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical physics, specifically the study of disordered systems using the renormalization group. The problem is well-posed, with a clear initial state, a deterministic iterative procedure, well-defined rules for tie-breaking, and an unambiguous stopping condition, ensuring a unique solution exists. The language is objective and precise. No flaws related to scientific soundness, completeness, consistency, or verifiability were found.\n\nThe task is to formalize and implement a Strong-Disorder Renormalization Group (SDRG) procedure for a disordered ferromagnetic Ising model. The derivation of the renormalization rule and the subsequent algorithm are detailed below.\n\nThe energy of the system is described by the Ising Hamiltonian:\n$$ H = -\\sum_{ij} J_{ij} s_i s_j $$\nwhere $s_i \\in \\{-1, +1\\}$ is the spin at node $i$, and $J_{ij} \\ge 0$ is the symmetric matrix of non-negative ferromagnetic couplings, with $J_{ii}=0$.\n\nThe SDRG method relies on the strong-disorder assumption: at any stage, the strongest coupling in the network is significantly larger than all other couplings involving the two nodes connected by this bond. Let the strongest coupling be $\\Omega = J_{uv}$ for a pair of nodes $(u, v)$. The Hamiltonian can be partitioned to isolate the dominant term:\n$$ H = -J_{uv} s_u s_v - \\sum_{k \\neq u,v} \\left(J_{uk} s_u s_k + J_{vk} s_v s_k\\right) - \\sum_{k,l \\neq u,v; kl} J_{kl} s_k s_l $$\nAccording to the strong-disorder hypothesis, $\\Omega \\gg J_{uk}, J_{vk}$ for all $k \\neq u,v$. The low-energy states of the system will be those that minimize the dominant energy term, $-J_{uv} s_u s_v$. Since $J_{uv}  0$, this term is minimized when $s_u s_v = +1$, which implies the spins are aligned: $s_u = s_v$. Configurations where $s_u = -s_v$ are separated by a large energy gap of $2 J_{uv}$ and are thus energetically suppressed at low temperatures.\n\nThe core of the SDRG procedure is to project the system onto the low-energy subspace defined by the constraint $s_u = s_v$. We introduce a new composite spin variable, $S_U$, representing the merged node pair, such that $S_U = s_u = s_v$. We substitute this constraint into the Hamiltonian to derive an effective Hamiltonian, $H_{\\text{eff}}$, for the coarse-grained system.\n\nThe terms in the Hamiltonian are transformed as follows:\n$1$. The dominant interaction $-J_{uv} s_u s_v$ becomes $-J_{uv} S_U^2 = -J_{uv}$. This is a constant energy contribution to the ground state and does not influence the relative spin configurations of the remaining system. Thus, it can be absorbed into the total energy offset and disregarded for the purpose of finding the effective couplings.\n\n$2$. The interactions between the pair $(u, v)$ and any other node $k$ are contained in the term $H_{U,k} = -(J_{uk} s_u s_k + J_{vk} s_v s_k)$. Under the constraint $s_u = s_v = S_U$, this term becomes:\n$$ H'_{U,k} = -(J_{uk} S_U s_k + J_{vk} S_U s_k) = -(J_{uk} + J_{vk}) S_U s_k $$\nThis expression has the form of a new effective Ising interaction, $-J'_{Uk} S_U s_k$. By direct comparison, we derive the renormalization rule for the effective coupling between the new composite node $U$ and any other node $k$:\n$$ J'_{Uk} = J_{uk} + J_{vk} $$\nThis rule is key to the SDRG procedure. It dictates that the new coupling is the sum of the couplings of the original nodes to the target node.\n\n$3$. Interactions between nodes $k$ and $l$, where neither is $u$ or $v$, remain unaffected by this coarse-graining step. Their couplings $J_{kl}$ are carried over to the new effective Hamiltonian without change.\n\nThis derived rule preserves the required properties of the coupling matrix. Since all $J_{ij} \\ge 0$, the new couplings $J'_{Uk}$ will also be non-negative. If the original matrix $J$ is symmetric ($J_{ij}=J_{ji}$), the new matrix $J'$ also remains symmetric, as $J'_{Uk} = J_{uk} + J_{vk}$ and $J'_{kU} = J_{ku} + J_{kv} = J_{uk} + J_{vk} = J'_{Uk}$. The diagonal elements remain $0$ as we are defining couplings between distinct nodes.\n\nThe overall SDRG algorithm is as follows:\nStart with the initial coupling matrix $J$ and threshold $\\lambda_{\\min}$.\nThe procedure is iterated as long as the number of nodes is greater than $1$ and the maximum coupling strength is greater than $\\lambda_{\\min}$.\n\nIn each iteration:\n$1$. **Identify Strongest Bond**: Find the maximum off-diagonal coupling, $J_{\\max} = \\max_{ij} J_{ij}$.\n$2$. **Check Stopping Condition**: If $J_{\\max} \\le \\lambda_{\\min}$, the procedure terminates, and the current matrix $J$ is the result.\n$3$. **Select Pair**: Identify the pair of indices $(u, v)$ corresponding to $J_{\\max}$. If there are multiple such pairs, the lexicographically smallest pair $(u, v)$ with $u  v$ is chosen.\n$4$. **Renormalize Couplings**: The nodes $u$ and $v$ are merged. We can designate $u$ as the index for the new composite node. For every other node $k$ (where $k \\neq u, v$), the couplings to node $u$ are updated according to the derived rule: $J_{uk} \\leftarrow J_{uk} + J_{vk}$ and $J_{ku} \\leftarrow J_{ku} + J_{kv}$.\n$5$. **Reduce Matrix**: The coupling matrix is reduced in size by removing the row and column corresponding to node $v$. The resulting matrix is one dimension smaller, symmetric, and has non-negative off-diagonal entries and a zero diagonal.\n\nThis iterative process generates a sequence of smaller, effective networks until the stopping condition is met, yielding the final coarse-grained coupling matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sdrg_procedure(J_initial, lambda_min):\n    \"\"\"\n    Performs the Strong-Disorder Renormalization Group (SDRG) procedure.\n    \n    Args:\n        J_initial (np.ndarray): The initial symmetric coupling matrix.\n        lambda_min (float): The minimum coupling threshold for stopping.\n        \n    Returns:\n        np.ndarray: The final effective coupling matrix.\n    \"\"\"\n    # Use a copy to avoid modifying the original input matrix\n    J = np.copy(J_initial)\n    \n    while J.shape[0]  1:\n        # Consider only the upper triangle to find the max coupling and its unique index\n        J_upper = np.triu(J, k=1)\n        max_val = np.max(J_upper)\n        \n        # Stopping condition: max coupling is not greater than the threshold\n        if max_val = lambda_min:\n            break\n            \n        # Find the location of the max coupling.\n        # np.argwhere finds all occurrences; it scans row-by-row, so the first\n        # result corresponds to the lexicographically smallest (i, j) pair.\n        indices = np.argwhere(J_upper == max_val)\n        u, v = indices[0] # u  v is guaranteed by using np.triu\n\n        # Renormalize couplings: merge node v into node u\n        # The new coupling from the merged node u to any other node k\n        # is the sum of the old couplings J_uk + J_vk.\n        # This can be efficiently done by adding row/column v to row/column u.\n        \n        # Update row u and column u\n        J[u, :] += J[v, :]\n        J[:, u] = J[u, :]\n        \n        # The diagonal element J[u, u] will become non-zero after the sum.\n        # It must be reset to 0, consistent with J_ii = 0.\n        J[u, u] = 0.0\n        \n        # Reduce the matrix by deleting the row and column for the merged node v.\n        J = np.delete(J, v, axis=0) # Delete row v\n        J = np.delete(J, v, axis=1) # Delete column v\n        \n    return J\n\ndef solve():\n    \"\"\"\n    Defines and solves the test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (4, np.array([\n            [0.0, 3.0, 0.5, 0.0],\n            [3.0, 0.0, 1.0, 0.2],\n            [0.5, 1.0, 0.0, 2.0],\n            [0.0, 0.2, 2.0, 0.0]\n        ]), 1.5),\n        # Case 2\n        (3, np.array([\n            [0.0, 2.0, 2.0],\n            [2.0, 0.0, 1.0],\n            [2.0, 1.0, 0.0]\n        ]), 1.5),\n        # Case 3\n        (4, np.array([\n            [0.0, 0.4, 0.3, 0.2],\n            [0.4, 0.0, 0.1, 0.2],\n            [0.3, 0.1, 0.0, 0.3],\n            [0.2, 0.2, 0.3, 0.0]\n        ]), 0.5),\n        # Case 4\n        (5, np.array([\n            [0.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0]\n        ]), 1.0)\n    ]\n\n    results = []\n    for N, J, lambda_min in test_cases:\n        final_J = sdrg_procedure(J, lambda_min)\n        # Convert the final numpy array to a nested list of floats\n        results.append(final_J.tolist())\n\n    # The string representation of a list of lists of floats adds spaces.\n    # To create a compact string, we process it.\n    results_str = [str(res).replace(\" \", \"\") for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}