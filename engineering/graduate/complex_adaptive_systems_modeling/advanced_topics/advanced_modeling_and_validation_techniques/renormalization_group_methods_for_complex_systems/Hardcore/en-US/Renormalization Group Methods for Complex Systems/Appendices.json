{
    "hands_on_practices": [
        {
            "introduction": "This first exercise guides you through a foundational application of the renormalization group: the real-space coarse-graining of the 2D Ising model. You will employ the Migdal-Kadanoff approximation, a classic technique that makes the RG flow analytically tractable, to derive the recursion relation for the coupling constant. This practice is essential for building intuition about how scale invariance and critical fixed points emerge from the iterative removal of microscopic degrees of freedom .",
            "id": "4139880",
            "problem": "You are tasked with implementing a coarse-graining step of the two-dimensional ferromagnetic Ising model using Kadanoff block-spin averaging and deriving the corresponding renormalized nearest-neighbor coupling. The two-dimensional Ising model is defined on a square lattice with spin variables $s_i \\in \\{-1,+1\\}$ and Hamiltonian\n$$\nH(s) = - J \\sum_{\\langle i,j \\rangle} s_i s_j,\n$$\nwhere $J \\ge 0$ is the dimensionless ferromagnetic coupling, and the sum runs over nearest-neighbor pairs $\\langle i,j \\rangle$. The equilibrium probability of a configuration $s$ is given by the Boltzmann distribution $P(s) \\propto \\exp(-H(s))$.\n\nA renormalization group (RG) transformation coarse-grains the microscopic spins into block spins, here by grouping $2 \\times 2$ spins into one block and assigning the block spin $S_B = \\mathrm{sign}\\left(\\sum_{i \\in B} s_i\\right)$ using the majority rule. To approximate the effective theory on the coarse lattice as another nearest-neighbor Ising model with Hamiltonian\n$$\nH'(S) = - J' \\sum_{\\langle B,B' \\rangle} S_B S_{B'},\n$$\none must integrate out microscopic spins while preserving the lattice coordination structure. A standard approximation that enforces this while maintaining tractability is the Migdal-Kadanoff (MK) bond-moving construction (Migdal-Kadanoff (MK)), in which bonds are redistributed and combined along paths connecting neighboring blocks, followed by tracing out intermediate spins. Under these assumptions, ignoring the generation of higher-order multi-spin couplings, the coarse-graining produces a functional relation $J' = f(J)$.\n\nYour tasks are:\n\n1) Starting from the Ising Hamiltonian and the Boltzmann weight, use exact tracing over a single two-bond chain connecting two coarse-grained block spins to obtain the effective two-spin interaction along that path. Then apply the MK bond-moving factor appropriate for spatial dimension $d=2$ and block size $b=2$ to derive the RG map $J' = f(J)$ in closed form. The derivation must begin from the definitions above, proceed by equating Boltzmann weight ratios for aligned versus anti-aligned boundary spins, and apply the MK rescaling factor $b^{d-1}$ logically to preserve connectivity under coarse-graining.\n\n2) Implement a program that:\n- Computes $f(J)$ for given input $J$.\n- Computes the derivative $f'(J)$.\n- Finds the nontrivial fixed point $J^*$ satisfying $f(J^*) = J^*$ for $J \\in (0, +\\infty)$ to within an absolute tolerance of $10^{-12}$.\nAll quantities are dimensionless and must be expressed as real numbers. No physical units are required.\n\n3) Use the following test suite to exercise your implementation:\n- Case A (boundary): $J = 0.0$. Return $f(0.0)$.\n- Case B (small-coupling expansion): $J = 0.05$. Return $f(0.05)$, and the absolute deviation from the leading small-$J$ expansion, namely $\\left|f(0.05) - 2 \\cdot (0.05)^2\\right|$.\n- Case C (typical value near the fixed point): $J = 0.62$. Return $f(0.62)$.\n- Case D (fixed point): Return $J^*$ and $f'(J^*)$.\n- Case E (monotonicity check): Return the boolean value of the predicate $f(0.4)  f(0.6)$.\n- Case F (two-step RG on a moderate coupling): Return $f(f(0.3))$.\n- Case G (strong coupling behavior): $J = 5.0$. Return $f(5.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following exact order:\n$$\n\\left[f(0.0),\\, f(0.05),\\, \\left|f(0.05) - 2 \\cdot (0.05)^2\\right|,\\, f(0.62),\\, J^*,\\, f'(J^*),\\, \\text{monotonicity boolean},\\, f(f(0.3)),\\, f(5.0)\\right].\n$$\nEach numeric entry must be a floating-point number. The monotonicity entry must be a boolean. No angles or physical units are involved in this problem; all quantities are dimensionless real numbers.",
            "solution": "We begin with the two-dimensional Ising model having Hamiltonian\n$$\nH(s) = -J \\sum_{\\langle i,j \\rangle} s_i s_j,\n$$\nwith $J \\ge 0$ and $s_i \\in \\{-1,+1\\}$, and Boltzmann weight $P(s) \\propto \\exp\\left(J \\sum_{\\langle i,j \\rangle} s_i s_j\\right)$.\n\nUnder Kadanoff block-spin averaging with $2 \\times 2$ blocks, the coarse-grained variables $S_B$ are defined by the majority rule. To derive an effective nearest-neighbor coupling $J'$ between neighboring block spins, we consider the Migdal-Kadanoff (MK) bond-moving approximation with scale factor $b=2$ in spatial dimension $d=2$. In MK, bonds are moved to maintain the lattice’s coordination structure after coarse-graining, and interactions along paths between block spins are combined via tracing out intermediate microscopic spins. This procedure preserves a nearest-neighbor Ising form at the expense of approximating away multi-spin interactions.\n\nConsider two neighboring blocks connected by a path consisting of two identical bonds of strength $J$ in series: spins $s_0$ and $s_2$ at the ends (representing boundary spins aligned with the block spins) and a single intermediate spin $s_1$. The partial Boltzmann weight after tracing out $s_1$ is\n$$\nW(s_0, s_2) = \\sum_{s_1 = \\pm 1} \\exp\\left[J s_0 s_1 + J s_1 s_2\\right] = \\sum_{s_1 = \\pm 1} \\exp\\left[J s_1 (s_0 + s_2)\\right] = 2 \\cosh\\left(J(s_0 + s_2)\\right).\n$$\nWhen $s_0 = s_2$ (aligned), we have $s_0 + s_2 = \\pm 2$, and $W_{\\mathrm{aligned}} = 2 \\cosh(2J)$. When $s_0 = -s_2$ (anti-aligned), we have $s_0 + s_2 = 0$, and $W_{\\mathrm{anti}} = 2 \\cosh(0) = 2$. If we seek an effective two-spin interaction of the Ising form $\\exp\\left[J_{\\mathrm{series}} s_0 s_2\\right]$, then the ratio of aligned to anti-aligned weights must match the ratio of Boltzmann factors:\n$$\n\\frac{W_{\\mathrm{aligned}}}{W_{\\mathrm{anti}}} = \\frac{\\exp(J_{\\mathrm{series}} \\cdot (+1))}{\\exp(J_{\\mathrm{series}} \\cdot (-1))} = \\exp(2 J_{\\mathrm{series}}).\n$$\nHence,\n$$\n\\exp(2 J_{\\mathrm{series}}) = \\frac{2 \\cosh(2J)}{2} = \\cosh(2J),\n$$\nwhich gives\n$$\nJ_{\\mathrm{series}} = \\frac{1}{2} \\ln\\left(\\cosh(2J)\\right).\n$$\n\nThe Migdal-Kadanoff bond-moving step in $d=2$ with block size $b=2$ introduces a multiplicative factor $b^{d-1} = 2$ to the effective coupling to preserve connectivity under coarse-graining. Consequently, the renormalized nearest-neighbor coupling after one RG step is\n$$\nJ' = f(J) = 2 J_{\\mathrm{series}} = \\ln\\left(\\cosh(2J)\\right).\n$$\nEquivalently, using hyperbolic identities,\n$$\nf(J) = 2 \\,\\mathrm{artanh}\\!\\left(\\tanh^2 J\\right),\n$$\nsince\n$$\n\\mathrm{artanh}(x) = \\frac{1}{2} \\ln\\left(\\frac{1+x}{1-x}\\right), \\quad \\frac{1+\\tanh^2 J}{1-\\tanh^2 J} = \\cosh(2J).\n$$\nBoth forms are identical; the logarithmic form is numerically convenient:\n$$\nf(J) = \\ln\\left(\\cosh(2J)\\right).\n$$\n\nThe derivative of $f(J)$ follows directly:\n$$\nf'(J) = \\frac{d}{dJ} \\ln\\left(\\cosh(2J)\\right) = \\frac{2 \\sinh(2J)}{\\cosh(2J)} = 2 \\tanh(2J).\n$$\n\nSmall-coupling expansion provides a consistency check. Using $\\cosh x = 1 + \\frac{x^2}{2} + \\mathcal{O}(x^4)$, for small $J$ we have\n$$\n\\cosh(2J) = 1 + \\frac{(2J)^2}{2} + \\mathcal{O}(J^4) = 1 + 2J^2 + \\mathcal{O}(J^4),\n$$\nso\n$$\nf(J) = \\ln\\left(1 + 2J^2 + \\mathcal{O}(J^4)\\right) = 2J^2 + \\mathcal{O}(J^4),\n$$\nconfirming the leading behavior $f(J) \\approx 2J^2$ as $J \\to 0$.\n\nA fixed point $J^*$ satisfies\n$$\nJ^* = f(J^*) = \\ln\\left(\\cosh(2J^*)\\right).\n$$\nNumerically, this can be solved using a robust one-dimensional root finder for the function $g(J) = f(J) - J$ over an interval such as $J \\in [0, 2]$. The derivative at the fixed point is\n$$\nf'(J^*) = 2 \\tanh(2J^*),\n$$\nwhich, being greater than one, reflects the unstable nature of the nontrivial fixed point under this RG map. The monotonicity of $f(J)$ for $J \\ge 0$ follows from $f'(J) = 2 \\tanh(2J) \\ge 0$ and strictly increasing for $J  0$.\n\nAlgorithmic plan for the program:\n- Implement $f(J) = \\ln(\\cosh(2J))$ using standard library functions.\n- Implement $f'(J) = 2 \\tanh(2J)$.\n- Use a root solver to find $J^*$ solving $f(J) - J = 0$ to tolerance $10^{-12}$.\n- Compute the test outputs in the specified order:\n  1. $f(0.0)$,\n  2. $f(0.05)$,\n  3. $\\left|f(0.05) - 2 \\cdot (0.05)^2\\right|$,\n  4. $f(0.62)$,\n  5. $J^*$,\n  6. $f'(J^*)$,\n  7. the boolean $f(0.4)  f(0.6)$,\n  8. $f(f(0.3))$,\n  9. $f(5.0)$.\nThese are all dimensionless. The program prints a single line containing a comma-separated list enclosed in square brackets in the exact order above.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef f(J: float) - float:\n    \"\"\"\n    Renormalization group map under Migdal-Kadanoff (MK) with b=2 in d=2:\n    J' = ln(cosh(2J)).\n    \"\"\"\n    return float(np.log(np.cosh(2.0 * J)))\n\ndef f_prime(J: float) - float:\n    \"\"\"\n    Derivative of the RG map: f'(J) = 2 * tanh(2J).\n    \"\"\"\n    return float(2.0 * np.tanh(2.0 * J))\n\ndef find_fixed_point(tol: float = 1e-12) - float:\n    \"\"\"\n    Find J* satisfying f(J*) = J* using a robust bracketing method.\n    \"\"\"\n    # Define function g(J) = f(J) - J\n    def g(J):\n        return f(J) - g\n\n    # Bracket the root in [0, 2]; g(0) = 0, but we seek the nontrivial fixed point J*0.\n    # g(0+)  0 (since small J: f(J) ~ 2J^2), and for sufficiently large J, f(J) - J becomes large positive.\n    # However, we need to ensure a crossing where g(J) switches sign to use brentq.\n    # Inspect numerically to find a bracket where g changes sign.\n    # We'll search for a sign change between 0.3 and 1.2.\n    left, right = 0.3, 1.2\n    g_left, g_right = g(left), g(right)\n\n    # If not bracketing, expand search.\n    if g_left * g_right  0:\n        # Try alternate brackets systematically.\n        candidates = [(0.0 + 1e-12, 2.0), (0.2, 0.9), (0.5, 1.5)]\n        bracket_found = False\n        for a, b in candidates:\n            ga, gb = g(a), g(b)\n            if ga * gb = 0:\n                left, right = a, b\n                bracket_found = True\n                break\n        if not bracket_found:\n            # Fallback: scan for sign change\n            xs = np.linspace(0.0 + 1e-9, 2.0, 1001)\n            gs = np.array([g(x) for x in xs])\n            signs = np.sign(gs)\n            # Find any adjacent entries with opposite signs\n            idx = None\n            for i in range(len(xs) - 1):\n                if signs[i] == 0:\n                    left, right = xs[i], xs[i+1]\n                    idx = i\n                    break\n                if signs[i] * signs[i+1]  0:\n                    left, right = xs[i], xs[i+1]\n                    idx = i\n                    break\n            if idx is None:\n                # As a last resort, raise error (should not happen for this function)\n                raise RuntimeError(\"Failed to bracket the fixed point.\")\n    # Use Brent's method to find the root\n    sol = root_scalar(lambda x: f(x) - x, bracket=[left, right], xtol=tol, rtol=tol, method='brentq')\n    if not sol.converged:\n        raise RuntimeError(\"Root finding did not converge.\")\n    return float(sol.root)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # We will compute the outputs in the exact required order:\n    # [f(0.0), f(0.05), |f(0.05) - 2*(0.05)^2|, f(0.62), J_star, f'(J_star), monotonicity_boolean, f(f(0.3)), f(5.0)]\n    J_values = {\n        \"A\": 0.0,\n        \"B\": 0.05,\n        \"C\": 0.62,\n        \"F_inner\": 0.3,\n        \"G\": 5.0\n    }\n\n    # Compute required values\n    f_A = f(J_values[\"A\"])\n    f_B = f(J_values[\"B\"])\n    small_J_approx = 2.0 * (J_values[\"B\"] ** 2)\n    small_J_error = abs(f_B - small_J_approx)\n    f_C = f(J_values[\"C\"])\n    J_star = find_fixed_point(tol=1e-12)\n    fprime_at_Jstar = f_prime(J_star)\n    monotonicity_boolean = f(0.4)  f(0.6)\n    fF_inner = f(J_values[\"F_inner\"])\n    fF = f(fF_inner)\n    f_G = f(J_values[\"G\"])\n\n    results = [\n        f_A,\n        f_B,\n        small_J_error,\n        f_C,\n        J_star,\n        fprime_at_Jstar,\n        monotonicity_boolean,\n        fF,\n        f_G\n    ]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from systems governed by thermal fluctuations, we now explore the physics of strong disorder. The Strong-Disorder Renormalization Group (SDRG) provides a powerful, non-perturbative framework for such systems by iteratively eliminating the strongest interactions in a network. This hands-on problem requires you to derive and implement the SDRG update rule, offering insight into the unique fixed points and hierarchical structures that characterize disorder-dominated complex systems .",
            "id": "4139836",
            "problem": "Consider a symmetric weighted network of $N$ nodes with nonnegative pairwise couplings represented by a real symmetric matrix $J \\in \\mathbb{R}^{N \\times N}$, where $J_{ij} \\ge 0$ for all $i \\ne j$ and $J_{ii} = 0$ for all $i$. Assume the network models a disordered ferromagnetic interaction system whose energy is defined by the Ising Hamiltonian $H = -\\sum_{ij} J_{ij} s_i s_j$, where $s_i \\in \\{-1, +1\\}$ denotes the spin of node $i$. The strong-disorder regime is characterized by a hierarchy in the magnitudes of the couplings, where the largest coupling $J_{ij}$ is significantly greater than its neighboring couplings.\n\nYour task is to formalize and implement a Strong-Disorder Renormalization Group (SDRG) procedure for this network that operates by iteratively eliminating the strongest bond and computing effective couplings for the remaining nodes. Begin from fundamental principles as follows:\n\n- Use the definition of the Ising Hamiltonian $H$ and the assumption of a strong ferromagnetic coupling $J_{ij}$ to argue that the dominant interaction constrains the low-energy subspace to configurations where $s_i = s_j$. Using a systematic projection onto this low-energy subspace, derive a principled rule for how couplings between the merged pair and any other node $k$ must be transformed to produce an effective coarse-grained description that preserves the leading-order energetic contributions.\n\n- Ensure your derivation and algorithm respect these constraints:\n  1. The matrix $J$ must remain symmetric with nonnegative off-diagonal entries and zero diagonal throughout the procedure.\n  2. At each iteration, select the pair $(i,j)$ with the largest $J_{ij}$, breaking ties by choosing the lexicographically smallest pair among those with equal value (the pair with the smallest $i$, and among those the smallest $j$).\n  3. Merge nodes $i$ and $j$ into a single effective node, reduce the matrix size by one, and update all couplings to the remaining nodes in a manner consistent with the low-energy projection you derived.\n  4. Stop the iterative elimination process when either the maximum coupling among all remaining pairs is less than or equal to a specified threshold $\\lambda_{\\min}$ or only a single node remains.\n\nYou must then implement this SDRG procedure as a complete, runnable program that takes no input and instead executes on the following fixed test suite. For each test case, start from the given $(N, J, \\lambda_{\\min})$, apply the SDRG until the stopping condition is met, and return the final effective coupling matrix:\n\nTest suite:\n- Case (1): $N = 4$, \n$$\nJ^{(1)} = \n\\begin{pmatrix}\n0  3.0  0.5  0.0 \\\\\n3.0  0  1.0  0.2 \\\\\n0.5  1.0  0  2.0 \\\\\n0.0  0.2  2.0  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(1)} = 1.5.\n$$\n- Case (2): $N = 3$,\n$$\nJ^{(2)} = \n\\begin{pmatrix}\n0  2.0  2.0 \\\\\n2.0  0  1.0 \\\\\n2.0  1.0  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(2)} = 1.5.\n$$\n- Case (3): $N = 4$,\n$$\nJ^{(3)} = \n\\begin{pmatrix}\n0  0.4  0.3  0.2 \\\\\n0.4  0  0.1  0.2 \\\\\n0.3  0.1  0  0.3 \\\\\n0.2  0.2  0.3  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(3)} = 0.5.\n$$\n- Case (4): $N = 5$,\n$$\nJ^{(4)} = \n\\begin{pmatrix}\n0  1.0  1.0  1.0  1.0 \\\\\n1.0  0  0.0  0.0  0.0 \\\\\n1.0  0.0  0  0.0  0.0 \\\\\n1.0  0.0  0.0  0  0.0 \\\\\n1.0  0.0  0.0  0.0  0\n\\end{pmatrix}\n,\\quad \\lambda_{\\min}^{(4)} = 1.0.\n$$\n\nAll matrix entries are in dimensionless units. Your program must:\n- Implement the SDRG computation consistent with the derived projection rule and constraints above.\n- For each test case, output the final effective coupling matrix as a nested list of lists of floats, preserving symmetry and zero diagonal.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case’s final matrix represented as a nested list of lists of floats. For example: $[M^{(1)},M^{(2)},M^{(3)},M^{(4)}]$ where $M^{(c)}$ is the final matrix for case $c$.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical physics, specifically the study of disordered systems using the renormalization group. The problem is well-posed, with a clear initial state, a deterministic iterative procedure, well-defined rules for tie-breaking, and an unambiguous stopping condition, ensuring a unique solution exists. The language is objective and precise. No flaws related to scientific soundness, completeness, consistency, or verifiability were found.\n\nThe task is to formalize and implement a Strong-Disorder Renormalization Group (SDRG) procedure for a disordered ferromagnetic Ising model. The derivation of the renormalization rule and the subsequent algorithm are detailed below.\n\nThe energy of the system is described by the Ising Hamiltonian:\n$$ H = -\\sum_{ij} J_{ij} s_i s_j $$\nwhere $s_i \\in \\{-1, +1\\}$ is the spin at node $i$, and $J_{ij} \\ge 0$ is the symmetric matrix of non-negative ferromagnetic couplings, with $J_{ii}=0$.\n\nThe SDRG method relies on the strong-disorder assumption: at any stage, the strongest coupling in the network is significantly larger than all other couplings involving the two nodes connected by this bond. Let the strongest coupling be $\\Omega = J_{uv}$ for a pair of nodes $(u, v)$. The Hamiltonian can be partitioned to isolate the dominant term:\n$$ H = -J_{uv} s_u s_v - \\sum_{k \\neq u,v} \\left(J_{uk} s_u s_k + J_{vk} s_v s_k\\right) - \\sum_{k,l \\neq u,v; kl} J_{kl} s_k s_l $$\nAccording to the strong-disorder hypothesis, $\\Omega \\gg J_{uk}, J_{vk}$ for all $k \\neq u,v$. The low-energy states of the system will be those that minimize the dominant energy term, $-J_{uv} s_u s_v$. Since $J_{uv}  0$, this term is minimized when $s_u s_v = +1$, which implies the spins are aligned: $s_u = s_v$. Configurations where $s_u = -s_v$ are separated by a large energy gap of $2 J_{uv}$ and are thus energetically suppressed at low temperatures.\n\nThe core of the SDRG procedure is to project the system onto the low-energy subspace defined by the constraint $s_u = s_v$. We introduce a new composite spin variable, $S_U$, representing the merged node pair, such that $S_U = s_u = s_v$. We substitute this constraint into the Hamiltonian to derive an effective Hamiltonian, $H_{\\text{eff}}$, for the coarse-grained system.\n\nThe terms in the Hamiltonian are transformed as follows:\n$1$. The dominant interaction $-J_{uv} s_u s_v$ becomes $-J_{uv} S_U^2 = -J_{uv}$. This is a constant energy contribution to the ground state and does not influence the relative spin configurations of the remaining system. Thus, it can be absorbed into the total energy offset and disregarded for the purpose of finding the effective couplings.\n\n$2$. The interactions between the pair $(u, v)$ and any other node $k$ are contained in the term $H_{U,k} = -(J_{uk} s_u s_k + J_{vk} s_v s_k)$. Under the constraint $s_u = s_v = S_U$, this term becomes:\n$$ H'_{U,k} = -(J_{uk} S_U s_k + J_{vk} S_U s_k) = -(J_{uk} + J_{vk}) S_U s_k $$\nThis expression has the form of a new effective Ising interaction, $-J'_{Uk} S_U s_k$. By direct comparison, we derive the renormalization rule for the effective coupling between the new composite node $U$ and any other node $k$:\n$$ J'_{Uk} = J_{uk} + J_{vk} $$\nThis rule is key to the SDRG procedure. It dictates that the new coupling is the sum of the couplings of the original nodes to the target node.\n\n$3$. Interactions between nodes $k$ and $l$, where neither is $u$ or $v$, remain unaffected by this coarse-graining step. Their couplings $J_{kl}$ are carried over to the new effective Hamiltonian without change.\n\nThis derived rule preserves the required properties of the coupling matrix. Since all $J_{ij} \\ge 0$, the new couplings $J'_{Uk}$ will also be non-negative. If the original matrix $J$ is symmetric ($J_{ij}=J_{ji}$), the new matrix $J'$ also remains symmetric, as $J'_{Uk} = J_{uk} + J_{vk}$ and $J'_{kU} = J_{ku} + J_{kv} = J_{uk} + J_{vk} = J'_{Uk}$. The diagonal elements remain $0$ as we are defining couplings between distinct nodes.\n\nThe overall SDRG algorithm is as follows:\nStart with the initial coupling matrix $J$ and threshold $\\lambda_{\\min}$.\nThe procedure is iterated as long as the number of nodes is greater than $1$ and the maximum coupling strength is greater than $\\lambda_{\\min}$.\n\nIn each iteration:\n$1$. **Identify Strongest Bond**: Find the maximum off-diagonal coupling, $J_{\\max} = \\max_{ij} J_{ij}$.\n$2$. **Check Stopping Condition**: If $J_{\\max} \\le \\lambda_{\\min}$, the procedure terminates, and the current matrix $J$ is the result.\n$3$. **Select Pair**: Identify the pair of indices $(u, v)$ corresponding to $J_{\\max}$. If there are multiple such pairs, the lexicographically smallest pair $(u, v)$ with $u  v$ is chosen.\n$4$. **Renormalize Couplings**: The nodes $u$ and $v$ are merged. We can designate $u$ as the index for the new composite node. For every other node $k$ (where $k \\neq u, v$), the couplings to node $u$ are updated according to the derived rule: $J_{uk} \\leftarrow J_{uk} + J_{vk}$ and $J_{ku} \\leftarrow J_{ku} + J_{kv}$.\n$5$. **Reduce Matrix**: The coupling matrix is reduced in size by removing the row and column corresponding to node $v$. The resulting matrix is one dimension smaller, symmetric, and has non-negative off-diagonal entries and a zero diagonal.\n\nThis iterative process generates a sequence of smaller, effective networks until the stopping condition is met, yielding the final coarse-grained coupling matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sdrg_procedure(J_initial, lambda_min):\n    \"\"\"\n    Performs the Strong-Disorder Renormalization Group (SDRG) procedure.\n    \n    Args:\n        J_initial (np.ndarray): The initial symmetric coupling matrix.\n        lambda_min (float): The minimum coupling threshold for stopping.\n        \n    Returns:\n        np.ndarray: The final effective coupling matrix.\n    \"\"\"\n    # Use a copy to avoid modifying the original input matrix\n    J = np.copy(J_initial)\n    \n    while J.shape[0]  1:\n        # Consider only the upper triangle to find the max coupling and its unique index\n        J_upper = np.triu(J, k=1)\n        max_val = np.max(J_upper)\n        \n        # Stopping condition: max coupling is not greater than the threshold\n        if max_val = lambda_min:\n            break\n            \n        # Find the location of the max coupling.\n        # np.argwhere finds all occurrences; it scans row-by-row, so the first\n        # result corresponds to the lexicographically smallest (i, j) pair.\n        indices = np.argwhere(J_upper == max_val)\n        u, v = indices[0] # u  v is guaranteed by using np.triu\n\n        # Renormalize couplings: merge node v into node u\n        # The new coupling from the merged node u to any other node k\n        # is the sum of the old couplings J_uk + J_vk.\n        # This can be efficiently done by adding row/column v to row/column u.\n        \n        # Update row u and column u\n        J[u, :] += J[v, :]\n        J[:, u] = J[u, :]\n        \n        # The diagonal element J[u, u] will become non-zero after the sum.\n        # It must be reset to 0, consistent with J_ii = 0.\n        J[u, u] = 0.0\n        \n        # Reduce the matrix by deleting the row and column for the merged node v.\n        J = np.delete(J, v, axis=0) # Delete row v\n        J = np.delete(J, v, axis=1) # Delete column v\n        \n    return J\n\ndef solve():\n    \"\"\"\n    Defines and solves the test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (4, np.array([\n            [0.0, 3.0, 0.5, 0.0],\n            [3.0, 0.0, 1.0, 0.2],\n            [0.5, 1.0, 0.0, 2.0],\n            [0.0, 0.2, 2.0, 0.0]\n        ]), 1.5),\n        # Case 2\n        (3, np.array([\n            [0.0, 2.0, 2.0],\n            [2.0, 0.0, 1.0],\n            [2.0, 1.0, 0.0]\n        ]), 1.5),\n        # Case 3\n        (4, np.array([\n            [0.0, 0.4, 0.3, 0.2],\n            [0.4, 0.0, 0.1, 0.2],\n            [0.3, 0.1, 0.0, 0.3],\n            [0.2, 0.2, 0.3, 0.0]\n        ]), 0.5),\n        # Case 4\n        (5, np.array([\n            [0.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0, 0.0]\n        ]), 1.0)\n    ]\n\n    results = []\n    for N, J, lambda_min in test_cases:\n        final_J = sdrg_procedure(J, lambda_min)\n        # Convert the final numpy array to a nested list of floats\n        results.append(final_J.tolist())\n\n    # The string representation of a list of lists of floats adds spaces.\n    # To create a compact string, we process it.\n    results_str = [str(res).replace(\" \", \"\") for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate power of the renormalization group lies in its ability to make quantitative predictions about universal behavior. This final practice demonstrates how to connect the abstract machinery of RG transformations to measurable physical quantities, such as the critical exponents of avalanche distributions in Self-Organized Criticality (SOC). By analyzing the fixed-point condition, you will derive a fundamental scaling relation that links an observable exponent to the eigenvalues of the RG flow, bridging the gap between theory and observation .",
            "id": "4139846",
            "problem": "Consider a complex adaptive system exhibiting avalanche dynamics at a critical point of Self-Organized Criticality (SOC). Let the avalanche size be denoted by $s \\in [s_{0}, \\infty)$, with $s_{0}  0$, and let the probability density function of avalanche sizes at criticality be denoted by $P(s)$, normalized so that $\\int_{s_{0}}^{\\infty} P(s) \\, ds = 1$. A Renormalization Group (RG) coarse-graining step aggregates microscopic degrees of freedom into blocks of linear size $b$, introducing a rescaled avalanche size variable $s' = s / b^{\\sigma}$, where $\\sigma  0$ is the fractal scaling exponent of avalanche size under the coarse-graining. The change of variables implies $ds' = ds / b^{\\sigma}$, and conservation of probability measure at fixed control parameter implies that the unrenormalized coarse-grained density $P_{r}(s')$ satisfies $P_{r}(s') \\, ds' = P(s) \\, ds$. Near criticality, there exists a single relevant operator controlling the flow of the activity density, with RG eigenvalue $\\lambda  1$ associated with the coarse-graining factor $b$, meaning that the amplitude of the coarse-grained distribution is multiplied by $\\lambda$ per RG step to compensate for the loss of active degrees of freedom into absorbing states. The renormalized coarse-grained distribution $P'(s')$ is thus given by $P'(s') = \\lambda \\, P_{r}(s')$.\n\nAssume that at the RG fixed point the distribution is asymptotically scale invariant and can be represented by a pure power-law $P^{*}(s) \\propto s^{-\\tau}$ for large $s$, with an exponent $\\tau  1$. Using only the definitions above, and the requirement that the fixed-point shape is invariant under the RG step, derive an expression for $\\tau$ in terms of $\\lambda$, $b$, and $\\sigma$, starting from the measure transformation and the amplitude rescaling. Then, for a two-dimensional system where the avalanche size fractal scaling exponent is $\\sigma = 2$, suppose a coarse-graining factor $b = 4$ and a measured RG eigenvalue $\\lambda = 2$. Compute the theoretical value of the avalanche size exponent and compare it with an empirical value $\\tau_{\\mathrm{emp}} = 1.27$ obtained from observations on large systems. Report the absolute discrepancy $|\\tau - \\tau_{\\mathrm{emp}}|$. Round your final discrepancy to four significant figures. Express the final answer as a single real number with no units.",
            "solution": "The problem statement is first subjected to a validation procedure.\n\n**Step 1: Extract Givens**\n-   System: A complex adaptive system at a critical point of Self-Organized Criticality (SOC).\n-   Avalanche size: $s \\in [s_{0}, \\infty)$, with $s_{0}  0$.\n-   Probability density function (PDF): $P(s)$, with normalization $\\int_{s_{0}}^{\\infty} P(s) \\, ds = 1$.\n-   Coarse-graining scale factor: $b$.\n-   Rescaled avalanche size: $s' = s / b^{\\sigma}$.\n-   Fractal scaling exponent: $\\sigma  0$.\n-   Differential relation: $ds' = ds / b^{\\sigma}$.\n-   Unrenormalized coarse-grained PDF: $P_{r}(s')$.\n-   Conservation of probability measure: $P_{r}(s') \\, ds' = P(s) \\, ds$.\n-   RG eigenvalue: $\\lambda  1$ for the activity density operator.\n-   Renormalized coarse-grained PDF: $P'(s') = \\lambda \\, P_{r}(s')$.\n-   Fixed-point PDF form: $P^{*}(s) \\propto s^{-\\tau}$ for large $s$, with $\\tau  1$.\n-   Fixed-point condition: The functional form of the PDF is invariant under the RG transformation, i.e., $P'(s')$ has the same shape as $P^{*}(s)$.\n-   Given numerical values for calculation:\n    -   System dimensionality: $2$D.\n    -   Avalanche size fractal scaling exponent: $\\sigma = 2$.\n    -   Coarse-graining factor: $b = 4$.\n    -   RG eigenvalue: $\\lambda = 2$.\n    -   Empirical avalanche size exponent: $\\tau_{\\mathrm{emp}} = 1.27$.\n-   Required output: The absolute discrepancy $|\\tau - \\tau_{\\mathrm{emp}}|$, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness**: The problem is well-grounded in the established principles of statistical physics, specifically the Renormalization Group (RG) framework as applied to critical phenomena and Self-Organized Criticality (SOC). The relationships provided for the transformation of the probability distribution under coarse-graining and rescaling are standard in this field.\n-   **Well-Posedness**: The problem is well-posed. It provides a complete set of definitions and relationships necessary to derive the scaling relation for the exponent $\\tau$. The request for a numerical calculation and comparison is unambiguous.\n-   **Objectivity**: The problem is stated in precise, objective, and quantitative language, devoid of any subjective or non-scientific claims.\n-   **Conclusion**: There are no identified flaws. The problem is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, ill-posed, or trivial. It is a standard, solvable problem in theoretical physics.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be provided.\n\nThe core of the problem is to use the fixed-point condition of the Renormalization Group transformation to derive a relationship between the system's parameters. A distribution $P^{*}(s)$ is a fixed point of the RG transformation if its functional form is preserved after one RG step.\n\nLet the fixed-point distribution have the form $P^{*}(s) = C s^{-\\tau}$ for large $s$, where $C$ is a normalization constant and $\\tau$ is the critical exponent we seek to determine.\n\nAn RG step consists of two parts:\n1.  Coarse-graining: The system is rescaled. The new size variable is $s'$, and the corresponding unrenormalized probability density is $P_r(s')$. The conservation of probability states that $P_{r}(s') \\, ds' = P^{*}(s) \\, ds$.\n2.  Amplitude Rescaling: The density is rescaled by the eigenvalue $\\lambda$ to account for the change in the total activity. The new, renormalized distribution is $P'(s') = \\lambda \\, P_{r}(s')$.\n\nWe can combine these steps to find the form of $P'(s')$. From the definition of $s'$, we have $s = s' b^{\\sigma}$, which implies $ds = b^{\\sigma} ds'$.\nSubstituting this into the probability conservation equation:\n$P_{r}(s') \\, ds' = P^{*}(s) \\, (b^{\\sigma} ds')$\n$P_{r}(s') = b^{\\sigma} P^{*}(s)$\n\nNow, we use the amplitude rescaling relation:\n$P'(s') = \\lambda \\, P_{r}(s') = \\lambda b^{\\sigma} P^{*}(s)$\n\nTo express $P'(s')$ as a function of $s'$, we substitute $s = s' b^{\\sigma}$ and the assumed power-law form of $P^{*}(s)$ into the equation:\n$P'(s') = \\lambda b^{\\sigma} \\left[ C (s' b^{\\sigma})^{-\\tau} \\right]$\n$P'(s') = \\lambda b^{\\sigma} C (s')^{-\\tau} (b^{\\sigma})^{-\\tau}$\n$P'(s') = C \\left( \\lambda b^{\\sigma} b^{-\\sigma\\tau} \\right) (s')^{-\\tau}$\n$P'(s') = C \\left( \\lambda b^{\\sigma(1-\\tau)} \\right) (s')^{-\\tau}$\n\nThe fixed-point condition requires that the transformed distribution $P'(s')$ has the same functional form as the original distribution $P^{*}(s')$. That is, $P'(s') = C (s')^{-\\tau}$.\nComparing this required form with our derived expression for $P'(s')$:\n$C \\left( \\lambda b^{\\sigma(1-\\tau)} \\right) (s')^{-\\tau} = C (s')^{-\\tau}$\n\nFor this equality to hold for all $s'$, the prefactors must be identical. Assuming $C \\neq 0$:\n$\\lambda b^{\\sigma(1-\\tau)} = 1$\n\nThis is the fundamental scaling relation that connects the critical exponent $\\tau$ to the RG parameters $\\lambda$, $b$, and $\\sigma$. We now solve this equation for $\\tau$. Taking the natural logarithm of both sides:\n$\\ln(\\lambda b^{\\sigma(1-\\tau)}) = \\ln(1)$\n$\\ln(\\lambda) + \\ln(b^{\\sigma(1-\\tau)}) = 0$\n$\\ln(\\lambda) + \\sigma(1-\\tau) \\ln(b) = 0$\n$\\sigma(1-\\tau) \\ln(b) = -\\ln(\\lambda)$\n$1-\\tau = -\\frac{\\ln(\\lambda)}{\\sigma \\ln(b)}$\n$\\tau = 1 + \\frac{\\ln(\\lambda)}{\\sigma \\ln(b)}$\n\nThis is the derived expression for $\\tau$.\n\nNext, we compute the theoretical value of $\\tau$ using the provided numerical values: $\\sigma = 2$, $b = 4$, and $\\lambda = 2$.\n$\\tau = 1 + \\frac{\\ln(2)}{2 \\ln(4)}$\n\nWe can simplify the term $\\ln(4)$ as $\\ln(2^2) = 2 \\ln(2)$.\n$\\tau = 1 + \\frac{\\ln(2)}{2 (2 \\ln(2))}$\n$\\tau = 1 + \\frac{\\ln(2)}{4 \\ln(2)}$\n\nSince $\\ln(2) \\neq 0$, we can cancel this term from the numerator and denominator:\n$\\tau = 1 + \\frac{1}{4}$\n$\\tau = 1.25$\n\nFinally, we are asked to compute the absolute discrepancy between this theoretical value and the empirical value $\\tau_{\\mathrm{emp}} = 1.27$.\nThe absolute discrepancy is $|\\tau - \\tau_{\\mathrm{emp}}|$.\n$|\\tau - \\tau_{\\mathrm{emp}}| = |1.25 - 1.27| = |-0.02| = 0.02$\n\nThe problem requires this value to be rounded to four significant figures. The exact value is $0.02$. To express this with four significant figures, we write it as $0.02000$.",
            "answer": "$$\\boxed{0.02000}$$"
        }
    ]
}