{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of network reconstruction for continuous data is the Gaussian Graphical Model (GGM), where the absence of an edge between two nodes signifies their conditional independence given all other nodes. This exercise bridges theory and practice by demonstrating how this conditional independence structure is directly encoded in the precision matrix $\\widehat{\\Theta}$, the inverse of the covariance matrix. By working from first principles to relate the entries of $\\widehat{\\Theta}$ to partial correlations, you will perform a core inference task: reconstructing a network graph by thresholding these statistical dependency measures .",
            "id": "4133196",
            "problem": "Consider a zero-mean multivariate normal random vector $X \\in \\mathbb{R}^{5}$ forming a Gaussian Graphical Model (GGM). Suppose a regularized maximum likelihood estimator yields a symmetric positive definite precision matrix $\\widehat{\\Theta} \\in \\mathbb{R}^{5 \\times 5}$ for the index set $V=\\{1,2,3,4,5\\}$:\n$$\n\\widehat{\\Theta} \\;=\\;\n\\begin{pmatrix}\n1.5 & -0.3 & 0 & 0 & -0.2 \\\\\n-0.3 & 1.2 & -0.25 & 0 & 0 \\\\\n0 & -0.25 & 1.0 & -0.4 & 0 \\\\\n0 & 0 & -0.4 & 1.3 & -0.35 \\\\\n-0.2 & 0 & 0 & -0.35 & 1.1\n\\end{pmatrix}.\n$$\nWork from foundational properties of the multivariate normal distribution and the definition of partial correlation to express the pairwise partial correlation $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ in terms of entries of the precision matrix, without assuming any specialized formula. Then, using your derived relation, compute all pairwise partial correlations implied by $\\widehat{\\Theta}$.\n\nAdopt the following thresholding rule to infer an undirected edge set: include an undirected edge between nodes $i$ and $j$ if and only if $\\left|\\rho_{ij \\cdot V \\setminus \\{i,j\\}}\\right| \\ge \\tau$, where $\\tau = 0.2$. Count each undirected edge once (i.e., consider only pairs with $i<j$).\n\nWhat is the total number of inferred undirected edges $m$ under this criterion? Provide $m$ as a single integer. No rounding is necessary and no units are required.",
            "solution": "The user wants me to solve the following problem:\nGiven a zero-mean multivariate normal random vector $X \\in \\mathbb{R}^{5}$ forming a Gaussian Graphical Model (GGM). A regularized maximum likelihood estimator yields a symmetric positive definite precision matrix $\\widehat{\\Theta} \\in \\mathbb{R}^{5 \\times 5}$ for the index set $V=\\{1,2,3,4,5\\}$:\n$$\n\\widehat{\\Theta} \\;=\\;\n\\begin{pmatrix}\n1.5 & -0.3 & 0 & 0 & -0.2 \\\\\n-0.3 & 1.2 & -0.25 & 0 & 0 \\\\\n0 & -0.25 & 1.0 & -0.4 & 0 \\\\\n0 & 0 & -0.4 & 1.3 & -0.35 \\\\\n-0.2 & 0 & 0 & -0.35 & 1.1\n\\end{pmatrix}.\n$$\nThe task involves two parts. First, to derive the expression for the pairwise partial correlation $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ in terms of the entries of the precision matrix, starting from foundational principles. Second, to use this derived relation to compute all pairwise partial correlations from $\\widehat{\\Theta}$, apply a thresholding rule with threshold $\\tau = 0.2$ to infer an undirected edge set, and count the total number of inferred edges $m$.\n\n### Step 1: Derivation of the Partial Correlation Formula\n\nLet $X = (X_1, X_2, \\dots, X_p)^T$ be a $p$-dimensional random vector following a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$, where $\\Sigma$ is the covariance matrix. The precision matrix is defined as the inverse of the covariance matrix, $\\Theta = \\Sigma^{-1}$. In this problem, $p=5$ and the mean is zero, $\\mu=0$.\n\nThe partial correlation between two variables $X_i$ and $X_j$ given a set of other variables $X_S$ is denoted by $\\rho_{ij \\cdot S}$. By definition, this is the Pearson correlation between $X_i$ and $X_j$ conditional on $X_S$. For a multivariate normal distribution, this is equivalent to the correlation of the residuals obtained after linearly regressing $X_i$ and $X_j$ on $X_S$.\n$$ \\rho_{ij \\cdot S} = \\mathrm{Corr}(X_i, X_j | X_S) = \\frac{\\mathrm{Cov}(X_i, X_j | X_S)}{\\sqrt{\\mathrm{Var}(X_i | X_S) \\mathrm{Var}(X_j | X_S)}} $$\nWe are interested in the case where $S = V \\setminus \\{i,j\\}$, i.e., all other variables in the model.\n\nA key property of the multivariate normal distribution is that the conditional distribution of a subset of variables, given another subset, is also normal. Let us partition the random vector $X$ into two sub-vectors, $X_A = (X_i, X_j)^T$ and $X_B = X_{V \\setminus \\{i,j\\}}$. The covariance matrix $\\Sigma$ and precision matrix $\\Theta$ can be partitioned conformably:\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{AA} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{BB} \\end{pmatrix}, \\quad \\Theta = \\begin{pmatrix} \\Theta_{AA} & \\Theta_{AB} \\\\ \\Theta_{BA} & \\Theta_{BB} \\end{pmatrix} $$\nHere, $\\Sigma_{AA}$ is the $2 \\times 2$ block of $\\Sigma$ corresponding to variables $i$ and $j$, and $\\Theta_{AA}$ is the $2 \\times 2$ block of $\\Theta$ corresponding to the same variables.\n\nThe conditional distribution of $X_A$ given $X_B = x_B$ is a normal distribution with covariance matrix $\\Sigma_{A|B} = \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}$. This matrix $\\Sigma_{A|B}$ is the conditional covariance matrix of $(X_i, X_j)^T$ given all other variables, which contains the terms we need for the partial correlation formula:\n$$ \\Sigma_{A|B} = \\mathrm{Cov}(X_A | X_B) = \\begin{pmatrix} \\mathrm{Var}(X_i | X_B) & \\mathrm{Cov}(X_i, X_j | X_B) \\\\ \\mathrm{Cov}(X_j, X_i | X_B) & \\mathrm{Var}(X_j | X_B) \\end{pmatrix} $$\nA fundamental result from the block matrix inversion formula relates the sub-blocks of $\\Theta = \\Sigma^{-1}$ to the sub-blocks of $\\Sigma$. Specifically, the top-left block of the precision matrix, $\\Theta_{AA}$, is the inverse of the Schur complement of $\\Sigma_{BB}$ in $\\Sigma$, which is exactly the conditional covariance matrix $\\Sigma_{A|B}$. That is,\n$$ \\Theta_{AA} = (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1} = (\\Sigma_{A|B})^{-1} $$\nTherefore, the conditional covariance matrix we seek is the inverse of the corresponding submatrix of the precision matrix: $\\Sigma_{A|B} = (\\Theta_{AA})^{-1}$.\n\nThe submatrix $\\Theta_{AA}$ is given by:\n$$ \\Theta_{AA} = \\begin{pmatrix} \\Theta_{ii} & \\Theta_{ij} \\\\ \\Theta_{ji} & \\Theta_{jj} \\end{pmatrix} $$\nSince $\\Theta$ is symmetric, $\\Theta_{ij} = \\Theta_{ji}$. We can now compute the inverse of this $2 \\times 2$ matrix:\n$$ \\Sigma_{A|B} = (\\Theta_{AA})^{-1} = \\frac{1}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}\\Theta_{ji}} \\begin{pmatrix} \\Theta_{jj} & -\\Theta_{ij} \\\\ -\\Theta_{ji} & \\Theta_{ii} \\end{pmatrix} = \\frac{1}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} \\begin{pmatrix} \\Theta_{jj} & -\\Theta_{ij} \\\\ -\\Theta_{ij} & \\Theta_{ii} \\end{pmatrix} $$\nFrom this matrix, we can identify the conditional variances and covariance:\n$$ \\mathrm{Var}(X_i | X_B) = \\frac{\\Theta_{jj}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n$$ \\mathrm{Var}(X_j | X_B) = \\frac{\\Theta_{ii}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\n$$ \\mathrm{Cov}(X_i, X_j | X_B) = \\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2} $$\nSubstituting these expressions into the formula for partial correlation:\n$$ \\rho_{ij \\cdot B} = \\frac{\\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}}{\\sqrt{\\left(\\frac{\\Theta_{jj}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}\\right) \\left(\\frac{\\Theta_{ii}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}\\right)}} = \\frac{\\frac{-\\Theta_{ij}}{\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2}}{\\frac{\\sqrt{\\Theta_{ii}\\Theta_{jj}}}{|\\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2|}} $$\nSince $\\Theta$ is positive definite, all its principal submatrices have positive determinants. Thus, $\\det(\\Theta_{AA}) = \\Theta_{ii}\\Theta_{jj} - \\Theta_{ij}^2 > 0$. Also, the diagonal entries $\\Theta_{ii}$ and $\\Theta_{jj}$ must be positive. This allows us to simplify the expression:\n$$ \\rho_{ij \\cdot V \\setminus \\{i,j\\}} = \\frac{-\\Theta_{ij}}{\\sqrt{\\Theta_{ii}\\Theta_{jj}}} $$\nThis is the desired relationship, derived from foundational principles.\n\n### Step 2: Computation of Partial Correlations and Edge Inference\n\nWe now apply this formula to the given estimated precision matrix $\\widehat{\\Theta}$. We need to compute $\\rho_{ij \\cdot V \\setminus \\{i,j\\}}$ for all pairs $(i, j)$ with $1 \\le i < j \\le 5$. There are $\\binom{5}{2} = 10$ such pairs.\n\nThe diagonal elements of $\\widehat{\\Theta}$ are:\n$\\widehat{\\Theta}_{11} = 1.5$, $\\widehat{\\Theta}_{22} = 1.2$, $\\widehat{\\Theta}_{33} = 1.0$, $\\widehat{\\Theta}_{44} = 1.3$, $\\widehat{\\Theta}_{55} = 1.1$.\n\nThe relevant off-diagonal elements $\\widehat{\\Theta}_{ij}$ with $i < j$ are:\n$\\widehat{\\Theta}_{12} = -0.3$, $\\widehat{\\Theta}_{13} = 0$, $\\widehat{\\Theta}_{14} = 0$, $\\widehat{\\Theta}_{15} = -0.2$, $\\widehat{\\Theta}_{23} = -0.25$, $\\widehat{\\Theta}_{24} = 0$, $\\widehat{\\Theta}_{25} = 0$, $\\widehat{\\Theta}_{34} = -0.4$, $\\widehat{\\Theta}_{35} = 0$, $\\widehat{\\Theta}_{45} = -0.35$.\n\nIf $\\widehat{\\Theta}_{ij} = 0$, then $\\rho_{ij \\cdot V \\setminus \\{i,j\\}} = 0$. This applies to the pairs $(1,3)$, $(1,4)$, $(2,4)$, $(2,5)$, and $(3,5)$. For these pairs, the absolute partial correlation is $0$, which is less than the threshold $\\tau = 0.2$, so no edges are inferred.\n\nNow we compute the partial correlations for the pairs with non-zero $\\widehat{\\Theta}_{ij}$:\n\n1.  Pair $(1,2)$:\n    $\\rho_{12 \\cdot \\{3,4,5\\}} = \\frac{-(-0.3)}{\\sqrt{\\widehat{\\Theta}_{11}\\widehat{\\Theta}_{22}}} = \\frac{0.3}{\\sqrt{1.5 \\times 1.2}} = \\frac{0.3}{\\sqrt{1.8}}$.\n    To check against the threshold $\\tau=0.2$: $|\\rho_{12 \\cdot \\{3,4,5\\}}| \\ge 0.2 \\iff \\frac{0.3}{\\sqrt{1.8}} \\ge 0.2 \\iff 1.5 \\ge \\sqrt{1.8} \\iff 1.5^2 \\ge 1.8 \\iff 2.25 \\ge 1.8$. This is true. An edge $(1,2)$ is inferred.\n\n2.  Pair $(1,5)$:\n    $\\rho_{15 \\cdot \\{2,3,4\\}} = \\frac{-(-0.2)}{\\sqrt{\\widehat{\\Theta}_{11}\\widehat{\\Theta}_{55}}} = \\frac{0.2}{\\sqrt{1.5 \\times 1.1}} = \\frac{0.2}{\\sqrt{1.65}}$.\n    To check: $|\\rho_{15 \\cdot \\{2,3,4\\}}| \\ge 0.2 \\iff \\frac{0.2}{\\sqrt{1.65}} \\ge 0.2 \\iff 1 \\ge \\sqrt{1.65} \\iff 1 \\ge 1.65$. This is false. No edge $(1,5)$ is inferred.\n\n3.  Pair $(2,3)$:\n    $\\rho_{23 \\cdot \\{1,4,5\\}} = \\frac{-(-0.25)}{\\sqrt{\\widehat{\\Theta}_{22}\\widehat{\\Theta}_{33}}} = \\frac{0.25}{\\sqrt{1.2 \\times 1.0}} = \\frac{0.25}{\\sqrt{1.2}}$.\n    To check: $|\\rho_{23 \\cdot \\{1,4,5\\}}| \\ge 0.2 \\iff \\frac{0.25}{\\sqrt{1.2}} \\ge 0.2 \\iff 1.25 \\ge \\sqrt{1.2} \\iff 1.25^2 \\ge 1.2 \\iff 1.5625 \\ge 1.2$. This is true. An edge $(2,3)$ is inferred.\n\n4.  Pair $(3,4)$:\n    $\\rho_{34 \\cdot \\{1,2,5\\}} = \\frac{-(-0.4)}{\\sqrt{\\widehat{\\Theta}_{33}\\widehat{\\Theta}_{44}}} = \\frac{0.4}{\\sqrt{1.0 \\times 1.3}} = \\frac{0.4}{\\sqrt{1.3}}$.\n    To check: $|\\rho_{34 \\cdot \\{1,2,5\\}}| \\ge 0.2 \\iff \\frac{0.4}{\\sqrt{1.3}} \\ge 0.2 \\iff 2 \\ge \\sqrt{1.3} \\iff 4 \\ge 1.3$. This is true. An edge $(3,4)$ is inferred.\n\n5.  Pair $(4,5)$:\n    $\\rho_{45 \\cdot \\{1,2,3\\}} = \\frac{-(-0.35)}{\\sqrt{\\widehat{\\Theta}_{44}\\widehat{\\Theta}_{55}}} = \\frac{0.35}{\\sqrt{1.3 \\times 1.1}} = \\frac{0.35}{\\sqrt{1.43}}$.\n    To check: $|\\rho_{45 \\cdot \\{1,2,3\\}}| \\ge 0.2 \\iff \\frac{0.35}{\\sqrt{1.43}} \\ge 0.2 \\iff 1.75 \\ge \\sqrt{1.43} \\iff 1.75^2 \\ge 1.43 \\iff 3.0625 \\ge 1.43$. This is true. An edge $(4,5)$ is inferred.\n\n### Step 3: Count the Total Number of Edges\n\nThe inferred undirected edges are:\n- $(1,2)$\n- $(2,3)$\n- $(3,4)$\n- $(4,5)$\n\nThe total number of inferred edges, $m$, is the count of these edges. Counting them, we find there are $4$ edges.\nThus, $m = 4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Inferring a network from data rarely yields a single, unambiguous answer; more often, we are faced with several candidate models of varying complexity. A denser network might capture the training data more closely, but at the risk of overfitting to noise. This practice introduces a principled method for navigating this trade-off using information criteria like the Akaike Information Criterion ($AIC$) and Bayesian Information Criterion ($BIC$), which formalize the principle of parsimony by penalizing models for each additional parameter. Through this exercise, you will apply these essential tools to select the most plausible network structure from a set of alternatives based on their likelihood and complexity .",
            "id": "4133224",
            "problem": "Consider an undirected binary network model (the Ising model) for a complex adaptive system with $p=10$ nodes. The generative mechanism assigns one local field parameter $h_i$ to each node $i \\in \\{1,\\dots,10\\}$ and one symmetric pairwise interaction parameter $J_{ij}$ to each undirected edge $\\{i,j\\}$, and produces $n=500$ independent and identically distributed node-state observations. Three candidate network structures $\\mathcal{E}_1$, $\\mathcal{E}_2$, and $\\mathcal{E}_3$ are fitted by maximum likelihood under the same data set, yielding maximized log-likelihood values $\\ell_1=-134.2$, $\\ell_2=-128.9$, and $\\ell_3=-120.0$, respectively. The corresponding edge counts are $|\\mathcal{E}_1|=12$, $|\\mathcal{E}_2|=16$, and $|\\mathcal{E}_3|=25$. Assume that the total number of free parameters in a candidate is the sum of node-local fields and edge couplings, that is $k_m=p+|\\mathcal{E}_m|$ for model $m\\in\\{1,2,3\\}$.\n\nStarting from the foundational principles that the Akaike Information Criterion arises as an asymptotically unbiased estimator of the expected Kullbackâ€“Leibler divergence risk between the candidate model and the data-generating process, and that the Bayesian Information Criterion emerges from a Laplace approximation to the model evidence under a regular prior with fixed dimension, compute both criteria for each candidate structure and identify the preferred structure according to the Bayesian Information Criterion. Round all information criterion values to four significant figures. Express your final answer as the index $m \\in \\{1,2,3\\}$ of the selected candidate structure.",
            "solution": "The problem requires a comparison of three candidate network structures using two standard information criteria: the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). The goal is to identify the model preferred by the BIC.\n\nThe general definition of the Akaike Information Criterion is given by:\n$$\n\\text{AIC} = -2\\ell_{\\text{max}} + 2k\n$$\nwhere $\\ell_{\\text{max}}$ is the maximized value of the log-likelihood function for the model, and $k$ is the number of free parameters in the model. AIC estimates the prediction error and thereby the relative quality of statistical models for a given set of data. A lower AIC value indicates a better fit, penalizing models with more parameters to avoid overfitting.\n\nThe general definition of the Bayesian Information Criterion is given by:\n$$\n\\text{BIC} = -2\\ell_{\\text{max}} + k \\ln(n)\n$$\nwhere $n$ is the number of data points or observations. The BIC is derived from a Bayesian framework and penalizes model complexity more heavily than AIC for $n > \\exp(2) \\approx 7.4$. A lower BIC value is preferred, indicating a model that is more likely to be the true model.\n\nThe problem provides the following data:\n- Number of nodes: $p=10$.\n- Number of observations: $n=500$.\n- For candidate structure $m=1$: maximized log-likelihood $\\ell_1 = -134.2$ and edge count $|\\mathcal{E}_1|=12$.\n- For candidate structure $m=2$: maximized log-likelihood $\\ell_2 = -128.9$ and edge count $|\\mathcal{E}_2|=16$.\n- For candidate structure $m=3$: maximized log-likelihood $\\ell_3 = -120.0$ and edge count $|\\mathcal{E}_3|=25$.\n\nThe number of free parameters, $k_m$, for each model $m$ is defined as the sum of node-local fields (one per node) and edge couplings (one per edge):\n$$\nk_m = p + |\\mathcal{E}_m|\n$$\nWe first compute the number of parameters for each of the three candidate models.\nFor model $m=1$:\n$$\nk_1 = p + |\\mathcal{E}_1| = 10 + 12 = 22\n$$\nFor model $m=2$:\n$$\nk_2 = p + |\\mathcal{E}_2| = 10 + 16 = 26\n$$\nFor model $m=3$:\n$$\nk_3 = p + |\\mathcal{E}_3| = 10 + 25 = 35\n$$\nNext, we compute the AIC for each model.\nFor model $m=1$:\n$$\n\\text{AIC}_1 = -2\\ell_1 + 2k_1 = -2(-134.2) + 2(22) = 268.4 + 44 = 312.4\n$$\nFor model $m=2$:\n$$\n\\text{AIC}_2 = -2\\ell_2 + 2k_2 = -2(-128.9) + 2(26) = 257.8 + 52 = 309.8\n$$\nFor model $m=3$:\n$$\n\\text{AIC}_3 = -2\\ell_3 + 2k_3 = -2(-120.0) + 2(35) = 240.0 + 70 = 310.0\n$$\nThe values rounded to four significant figures are $\\text{AIC}_1 = 312.4$, $\\text{AIC}_2 = 309.8$, and $\\text{AIC}_3 = 310.0$. According to AIC, model $2$ would be the preferred model as it has the lowest score.\n\nNow, we compute the BIC for each model. The number of observations is $n=500$. The natural logarithm term is $\\ln(n) = \\ln(500)$.\nFor model $m=1$:\n$$\n\\text{BIC}_1 = -2\\ell_1 + k_1 \\ln(n) = -2(-134.2) + 22 \\ln(500) = 268.4 + 22 \\times 6.214608... \\approx 268.4 + 136.721378 = 405.121378...\n$$\nFor model $m=2$:\n$$\n\\text{BIC}_2 = -2\\ell_2 + k_2 \\ln(n) = -2(-128.9) + 26 \\ln(500) = 257.8 + 26 \\times 6.214608... \\approx 257.8 + 161.579810 = 419.379810...\n$$\nFor model $m=3$:\n$$\n\\text{BIC}_3 = -2\\ell_3 + k_3 \\ln(n) = -2(-120.0) + 35 \\ln(500) = 240.0 + 35 \\times 6.214608... \\approx 240.0 + 217.511283 = 457.511283...\n$$\nRounding these values to four significant figures as requested:\n$$\n\\text{BIC}_1 \\approx 405.1\n$$\n$$\n\\text{BIC}_2 \\approx 419.4\n$$\n$$\n\\text{BIC}_3 \\approx 457.5\n$$\nThe selection criterion is to choose the model with the minimum BIC value. Comparing the computed values:\n$$\n405.1 < 419.4 < 457.5\n$$\nThe minimum value is $\\text{BIC}_1$. Therefore, the Bayesian Information Criterion selects the first candidate structure, $\\mathcal{E}_1$. The index of this preferred structure is $m=1$. Note that the BIC, with its stronger penalty for complexity ($k \\ln(500)$ vs. $2k$), has disfavored the more complex models that AIC ranked higher.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Once a network reconstruction algorithm has produced a set of candidate edges, often ranked by confidence scores, the crucial final step is to evaluate its performance. This task moves beyond simple accuracy to assess the algorithm's ability to correctly rank true connections above false ones. This hands-on exercise guides you through the computation and interpretation of two indispensable evaluation metrics: the Area Under the Receiver Operating Characteristic curve ($AUC$) and the Average Precision ($AP$). By implementing these metrics, you will gain a deep understanding of how to quantify and compare the discriminative power of different inference methods, a fundamental skill for validating any network model .",
            "id": "4133178",
            "problem": "Consider a binary link existence task for a complex adaptive network, where a reconstruction algorithm produces for each potential edge an epistemic posterior score $s_e \\in [0,1]$ indicating the degree of belief that the edge exists given the data. Let the ground truth binary labels be $y_e \\in \\{0,1\\}$, where $y_e = 1$ denotes an existing edge and $y_e = 0$ denotes a non-edge. This setup creates a ranking problem over candidate edges based on the scores $s_e$.\n\nStarting from elementary decision-theoretic and probability foundations, define for any threshold $\\tau \\in [0,1]$ the predicted positive set $\\{e : s_e \\ge \\tau\\}$ and the counts of true positives $TP(\\tau)$, false positives $FP(\\tau)$, false negatives $FN(\\tau)$, and true negatives $TN(\\tau)$ relative to the ground truth labels. By definition, the True Positive Rate (TPR) is $TPR(\\tau) = TP(\\tau) / P$ with $P = \\sum_e y_e$, and the False Positive Rate (FPR) is $FPR(\\tau) = FP(\\tau) / N$ with $N = \\sum_e (1 - y_e)$. The Receiver Operating Characteristics (ROC) curve is the parametric curve $(FPR(\\tau), TPR(\\tau))$ as $\\tau$ varies, and the Area Under the Curve (AUC) is the integral of $TPR$ with respect to $FPR$ across the interval $[0,1]$. Similarly, the Precision-Recall (PR) curve is the parametric curve $(Recall(\\tau), Precision(\\tau))$ where $Recall(\\tau) = TP(\\tau)/P$ and $Precision(\\tau) = TP(\\tau)/(TP(\\tau)+FP(\\tau))$, and its summary as Average Precision (AP) is the area under the PR curve constructed by the natural stepwise interpolation induced by score-based ranking. In degenerate cases where $P = 0$ or $N = 0$, assume the ROC AUC equals $0.5$ by convention, and if $P = 0$, set the Average Precision to $0.0$.\n\nYour task is to implement a complete, runnable program that:\n- Accepts no input and uses the fixed test suite specified below.\n- For each test case, computes two floats: the ROC AUC and the Average Precision (AP) based only on $s_e$ and $y_e$, treating ties in $s_e$ conservatively via stable ordering and tie-averaged contributions.\n- Rounds each reported float to six decimal places.\n\nScientific realism requirements:\n- Interpret $s_e$ as epistemic posterior scores derived from a reconstruction model; no model details are needed beyond the assumption that $s_e$ are produced from a valid inference process.\n- The algorithm must be derived from fundamental definitions of $TP(\\tau)$, $FP(\\tau)$, $FN(\\tau)$, $TN(\\tau)$, $TPR(\\tau)$, $FPR(\\tau)$, $Precision(\\tau)$, and $Recall(\\tau)$ as functions of $\\tau$, without invoking black-box shortcuts beyond what can be justified from those definitions.\n\nDiscuss how the ROC AUC and PR AP reflect epistemic uncertainty in the reconstruction: when $s_e$ are diffuse or tied (scores close across edges), discrimination degrades, affecting the curves and areas; when $s_e$ are sharply separated, discrimination improves.\n\nTest suite:\n- Case $1$ (happy path, moderate uncertainty, balanced positives and negatives): $y = [\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0\\,]$, $s = [\\,0.9,\\,0.8,\\,0.7,\\,0.6,\\,0.4,\\,0.5,\\,0.3,\\,0.2,\\,0.1,\\,0.05\\,]$.\n- Case $2$ (class imbalance, few positives among many negatives): $y = [\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,]$, $s = [\\,0.9,\\,0.95,\\,0.55,\\,0.52,\\,0.5,\\,0.48,\\,0.46,\\,0.44,\\,0.42,\\,0.4,\\,0.38,\\,0.36,\\,0.34,\\,0.32,\\,0.3,\\,0.28,\\,0.26,\\,0.24,\\,0.8,\\,0.85\\,]$.\n- Case $3$ (ties in scores reflecting high epistemic uncertainty): $y = [\\,1,\\,0,\\,1,\\,0,\\,1\\,]$, $s = [\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5\\,]$.\n- Case $4$ (near-perfect separation): $y = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0\\,]$, $s = [\\,0.99,\\,0.95,\\,0.9,\\,0.1,\\,0.05,\\,0.01\\,]$.\n- Case $5$ (worst case ordering): $y = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$, $s = [\\,0.99,\\,0.95,\\,0.9,\\,0.1,\\,0.05,\\,0.01\\,]$.\n\nAnswer specification:\n- For each case, return the pair $[\\,\\text{ROC AUC},\\, \\text{AP}\\,]$ as two floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair for a case in the same order as listed above. For example: $[\\,[\\,a_1,\\,b_1\\,],\\,[\\,a_2,\\,b_2\\,],\\ldots\\,]$.\n- No physical units are involved. Report all values as decimals (fractions or floating-point numbers), not percentages.",
            "solution": "The task is to compute the Area Under the Receiver Operating Characteristics curve (ROC AUC) and the Average Precision (AP). The solution requires deriving the algorithms from fundamental definitions, discussing their relationship to epistemic uncertainty, and providing an implementation.\n\n### Derivation and Algorithmic Approach\nLet there be a set of $M$ items (potential edges), with scores $s_e$ and true labels $y_e \\in \\{0, 1\\}$. Let $P = \\sum y_e$ be the total number of positive items and $N = M - P$ be the total number of negative items.\n\n#### ROC AUC Calculation\nThe ROC AUC can be computed via its probabilistic interpretation: it is the probability that a randomly selected positive item receives a higher score than a randomly selected negative item. This corresponds to the Wilcoxon-Mann-Whitney U statistic and naturally handles ties as requested. Let $S_P = \\{s_e | y_e = 1\\}$ and $S_N = \\{s_e | y_e = 0\\}$. The AUC is calculated as:\n$$ \\text{AUC} = \\frac{1}{P \\times N} \\sum_{s_p \\in S_P} \\sum_{s_n \\in S_N} I(s_p, s_n) $$\nwhere the indicator function $I(s_p, s_n)$ is:\n$$ I(s_p, s_n) = \\begin{cases} 1 & \\text{if } s_p > s_n \\\\ 0.5 & \\text{if } s_p = s_n \\\\ 0 & \\text{if } s_p < s_n \\end{cases} $$\nIf $P=0$ or $N=0$, the AUC is conventionally set to $0.5$.\n\n#### Average Precision (AP) Calculation\nAverage Precision (AP) summarizes the Precision-Recall (PR) curve. It is computed as a weighted average of precisions. First, all items are sorted in descending order of their score $s_e$, using a stable sort for ties. AP is then the sum of precision values at each rank $k$ that corresponds to a positive item, divided by the total number of positive items.\n$$ \\text{AP} = \\sum_{k} \\text{Precision}(k) \\times \\Delta \\text{Recall}(k) = \\frac{1}{P} \\sum_{k=1}^{M} \\text{Precision}(k) \\times y_k $$\nwhere $y_k=1$ if the item at rank $k$ is positive, and 0 otherwise. If $P=0$, AP is $0.0$.\n\n### Discussion on Epistemic Uncertainty\nEpistemic uncertainty in a reconstruction model refers to the model's own uncertainty about its predictions. This manifests in the posterior scores $s_e$.\n- **High Epistemic Uncertainty**: When the model is uncertain, it produces \"diffuse\" or \"tied\" scores where positive and negative examples are not well-separated. This leads to poor ranking performance. The **ROC AUC** will be close to $0.5$ (random chance), and the **Average Precision (AP)** will be low, approaching the baseline prevalence of positives ($P/M$).\n- **Low Epistemic Uncertainty**: When the model is confident, it produces sharply separated scores. The rank-ordered list will place positives before negatives. The **ROC AUC** and **AP** will both approach $1.0$ (perfect discrimination).\nIn summary, ROC AUC and AP are quantitative measures of a model's discriminative power, which is inversely related to its epistemic uncertainty.\n\n### Implementation\nThe runnable program below implements these calculations for the specified test suite.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It computes ROC AUC and Average Precision for each case and prints the results.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: happy path, moderate uncertainty, balanced\n        {'y': np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0]), \n         's': np.array([0.9, 0.8, 0.7, 0.6, 0.4, 0.5, 0.3, 0.2, 0.1, 0.05])},\n        # Case 2: class imbalance, few positives\n        {'y': np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]), \n         's': np.array([0.9, 0.95, 0.55, 0.52, 0.5, 0.48, 0.46, 0.44, 0.42, 0.4, 0.38, 0.36, 0.34, 0.32, 0.3, 0.28, 0.26, 0.24, 0.8, 0.85])},\n        # Case 3: ties in scores (high uncertainty)\n        {'y': np.array([1, 0, 1, 0, 1]), \n         's': np.array([0.5, 0.5, 0.5, 0.5, 0.5])},\n        # Case 4: near-perfect separation\n        {'y': np.array([1, 1, 1, 0, 0, 0]), \n         's': np.array([0.99, 0.95, 0.9, 0.1, 0.05, 0.01])},\n        # Case 5: worst case ordering\n        {'y': np.array([0, 0, 0, 1, 1, 1]), \n         's': np.array([0.99, 0.95, 0.9, 0.1, 0.05, 0.01])},\n    ]\n\n    results = []\n    for case in test_cases:\n        y_true = case['y']\n        y_scores = case['s']\n        \n        P = np.sum(y_true)\n        N = len(y_true) - P\n\n        # --- ROC AUC Calculation ---\n        if P == 0 or N == 0:\n            roc_auc = 0.5\n        else:\n            pos_scores = y_scores[y_true == 1]\n            neg_scores = y_scores[y_true == 0]\n            \n            u_statistic = 0.0\n            for ps in pos_scores:\n                for ns in neg_scores:\n                    if ps > ns:\n                        u_statistic += 1.0\n                    elif ps == ns:\n                        u_statistic += 0.5\n            roc_auc = u_statistic / (P * N)\n\n        # --- Average Precision (AP) Calculation ---\n        if P == 0:\n            ap = 0.0\n        else:\n            indices = np.arange(len(y_scores))\n            # Sort by score (descending). Python's default sort is stable.\n            sorted_indices = sorted(indices, key=lambda i: y_scores[i], reverse=True)\n            \n            ap_sum = 0.0\n            tp_count = 0\n            for k, idx in enumerate(sorted_indices, 1):\n                if y_true[idx] == 1:\n                    tp_count += 1\n                    precision_at_k = tp_count / k\n                    ap_sum += precision_at_k\n            \n            ap = ap_sum / P\n\n        results.append([round(roc_auc, 6), round(ap, 6)])\n\n    # Format the final output string as specified\n    inner_strings = [f\"[{r[0]:.6f},{r[1]:.6f}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```",
            "answer": "[[0.560000,0.658730],[0.941176,0.638889],[0.500000,0.755556],[1.000000,1.000000],[0.000000,0.383333]]"
        }
    ]
}