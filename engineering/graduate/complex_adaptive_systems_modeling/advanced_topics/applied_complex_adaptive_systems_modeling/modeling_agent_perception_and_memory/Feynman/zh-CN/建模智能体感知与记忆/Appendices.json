{
    "hands_on_practices": [
        {
            "introduction": "这第一个实践为将感知建模为贝叶斯推理过程奠定了基础。在这里，我们将探讨一个场景，其中智能体的世界内部模型存在缺陷，从而导致感知偏差。通过计算“真实”的后验信念和智能体的有偏信念，您将获得贝叶斯更新的实践经验，并学会使用 Kullback-Leibler 散度来量化由此产生的感知错误。",
            "id": "4128046",
            "problem": "考虑一个复杂自适应系统中的单智能体贝叶斯估计器，它必须从单个线索 $y$ 推断一个潜在刺激 $s \\in \\mathbb{R}$。该智能体维持一个关于 $s$ 的基于记忆的先验，该先验由一个均值为 $m_{0}$、方差为 $v_{0}$ 的高斯分布给出。物理环境根据一个真实的似然函数生成线索，该似然函数是一个均值为 $s$、方差为 $\\sigma_{\\text{true}}^{2}$ 的高斯分布。然而，由于感知偏差和不完美的内部建模，该智能体使用了一个错误设定的似然函数，它是一个均值为 $s + b$（一个固定的移位偏差）、方差为 $\\sigma_{\\text{mis}}^{2}$ 的高斯分布。给定以下参数值：$m_{0} = 0$，$v_{0} = 1$，$\\sigma_{\\text{true}}^{2} = 1$，$\\sigma_{\\text{mis}}^{2} = 4$，$b = 0.5$，以及一个观测到的线索 $y = 1$。\n\n仅从贝叶斯法则和高斯概率密度函数的定义出发，首先通过配方法推导真实后验 $p_{\\text{T}}(s \\mid y)$ 和智能体的错误设定的后验 $p_{\\text{A}}(s \\mid y)$ 作为高斯密度，并明确指出它们的均值和方差。然后，使用Kullback-Leibler散度 (KLD) 的定义，计算从真实后验到智能体后验的散度 $D_{\\mathrm{KL}}(p_{\\text{T}}(\\cdot \\mid y) \\,\\|\\, p_{\\text{A}}(\\cdot \\mid y))$，结果为单个标量值。\n\n将最终数值答案四舍五入到 $4$ 位有效数字。将最终结果表示为无单位的纯数。",
            "solution": "该问题要求推导一个真实后验分布和一个错误设定的后验分布，然后计算它们之间的Kullback-Leibler (KL) 散度。分析从贝叶斯法则开始，该法则指出后验概率正比于似然和先验的乘积：$p(s \\mid y) \\propto p(y \\mid s) p(s)$。由于所有涉及的分布都是高斯分布，它们的乘积也将是高斯分布。一个变量 $x$ 的均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯概率密度函数 (PDF) 由 $\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$ 给出。\n\n我们可以通过考察后验的对数来找到后验高斯分布的参数，该对数将是潜在变量 $s$ 的一个二次函数。\n$\\ln p(s \\mid y) = \\ln p(y \\mid s) + \\ln p(s) + \\text{常数}$。\n对数后验中与 $s$ 相关的项具有以下形式：\n$$ \\ln p(s \\mid y) = -\\frac{(s-\\mu_{\\text{post}})^2}{2v_{\\text{post}}} + C = -\\frac{1}{2v_{\\text{post}}}s^2 + \\frac{\\mu_{\\text{post}}}{v_{\\text{post}}}s - \\frac{\\mu_{\\text{post}}^2}{2v_{\\text{post}}} + C $$\n其中 $\\mu_{\\text{post}}$ 和 $v_{\\text{post}}$ 分别是后验均值和方差。通过组合对数似然和对数先验，然后匹配 $s^2$ 和 $s$ 的系数，我们可以推导出后验参数。\n\n问题给出的先验为 $p(s) = \\mathcal{N}(s \\mid m_0, v_0)$，其中 $m_0=0$ 且 $v_0=1$。\n对数先验为 $\\ln p(s) = -\\frac{(s-m_0)^2}{2v_0} + C_1$。\n\n首先，我们推导真实后验 $p_{\\text{T}}(s \\mid y)$。\n真实似然为 $p_{\\text{T}}(y \\mid s) = \\mathcal{N}(y \\mid s, \\sigma_{\\text{true}}^2)$，其中 $\\sigma_{\\text{true}}^2=1$。\n对数似然为 $\\ln p_{\\text{T}}(y \\mid s) = -\\frac{(y-s)^2}{2\\sigma_{\\text{true}}^2} + C_2$。\n对数后验为：\n$$ \\ln p_{\\text{T}}(s \\mid y) \\propto -\\frac{(s-m_0)^2}{2v_0} - \\frac{(y-s)^2}{2\\sigma_{\\text{true}}^2} $$\n$$ = -\\frac{1}{2}\\left( \\frac{s^2 - 2sm_0 + m_0^2}{v_0} + \\frac{s^2 - 2sy + y^2}{\\sigma_{\\text{true}}^2} \\right) $$\n按 $s$ 的幂次合并项：\n$$ = -\\frac{1}{2}\\left[ s^2\\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2}\\right) - 2s\\left(\\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2}\\right) \\right] + \\text{常数} $$\n将此与对数高斯函数的一般形式进行比较，我们确定真实后验方差 $v_{\\text{T}}$ 的倒数和均值 $m_{\\text{T}}$：\n$$ \\frac{1}{v_{\\text{T}}} = \\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2} \\implies v_{\\text{T}} = \\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{true}}^2}\\right)^{-1} $$\n$$ \\frac{m_{\\text{T}}}{v_{\\text{T}}} = \\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2} \\implies m_{\\text{T}} = v_{\\text{T}}\\left(\\frac{m_0}{v_0} + \\frac{y}{\\sigma_{\\text{true}}^2}\\right) $$\n代入给定值 $m_0=0$, $v_0=1$, $\\sigma_{\\text{true}}^2=1$ 和 $y=1$：\n$$ v_{\\text{T}} = \\left(\\frac{1}{1} + \\frac{1}{1}\\right)^{-1} = (2)^{-1} = 0.5 $$\n$$ m_{\\text{T}} = 0.5 \\left(\\frac{0}{1} + \\frac{1}{1}\\right) = 0.5(1) = 0.5 $$\n因此，真实后验为 $p_{\\text{T}}(s \\mid y) = \\mathcal{N}(s \\mid m_{\\text{T}}, v_{\\text{T}}) = \\mathcal{N}(s \\mid 0.5, 0.5)$。\n\n接下来，我们推导智能体的错误设定的后验 $p_{\\text{A}}(s \\mid y)$。\n智能体使用相同的先验 $p(s) = \\mathcal{N}(s \\mid m_0, v_0)$，但使用一个错误设定的似然 $p_{\\text{A}}(y \\mid s) = \\mathcal{N}(y \\mid s+b, \\sigma_{\\text{mis}}^2)$，其中 $b=0.5$ 且 $\\sigma_{\\text{mis}}^2=4$。\n智能体使用的对数似然为 $\\ln p_{\\text{A}}(y \\mid s) = -\\frac{(y-(s+b))^2}{2\\sigma_{\\text{mis}}^2} + C_3 = -\\frac{((y-b)-s)^2}{2\\sigma_{\\text{mis}}^2} + C_3$。\n这与真实似然具有相同的函数形式，但观测值 $y$ 被替换为“有效观测值” $y' = y-b$，方差 $\\sigma_{\\text{true}}^2$ 被替换为 $\\sigma_{\\text{mis}}^2$。\n推导结构是相同的。智能体的后验方差 $v_{\\text{A}}$ 和均值 $m_{\\text{A}}$ 为：\n$$ v_{\\text{A}} = \\left(\\frac{1}{v_0} + \\frac{1}{\\sigma_{\\text{mis}}^2}\\right)^{-1} $$\n$$ m_{\\text{A}} = v_{\\text{A}}\\left(\\frac{m_0}{v_0} + \\frac{y-b}{\\sigma_{\\text{mis}}^2}\\right) $$\n代入给定值 $m_0=0$, $v_0=1$, $\\sigma_{\\text{mis}}^2=4$, $b=0.5$ 和 $y=1$：\n$$ v_{\\text{A}} = \\left(\\frac{1}{1} + \\frac{1}{4}\\right)^{-1} = \\left(\\frac{5}{4}\\right)^{-1} = \\frac{4}{5} = 0.8 $$\n$$ m_{\\text{A}} = 0.8 \\left(\\frac{0}{1} + \\frac{1-0.5}{4}\\right) = 0.8 \\left(\\frac{0.5}{4}\\right) = 0.8(0.125) = 0.1 $$\n因此，智能体的后验为 $p_{\\text{A}}(s \\mid y) = \\mathcal{N}(s \\mid m_{\\text{A}}, v_{\\text{A}}) = \\mathcal{N}(s \\mid 0.1, 0.8)$。\n\n最后，我们计算从真实后验到智能体后验的KL散度，$D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}})$。\n对于两个一维高斯分布 $p_1 = \\mathcal{N}(\\mu_1, v_1)$ 和 $p_2 = \\mathcal{N}(\\mu_2, v_2)$，KL散度由以下公式给出：\n$$ D_{\\mathrm{KL}}(p_1 \\| p_2) = \\frac{1}{2} \\left( \\ln\\left(\\frac{v_2}{v_1}\\right) - 1 + \\frac{v_1}{v_2} + \\frac{(\\mu_1-\\mu_2)^2}{v_2} \\right) $$\n在我们的例子中，$p_1$ 是真实后验 $p_{\\text{T}}$，$p_2$ 是智能体的后验 $p_{\\text{A}}$。参数为：\n$\\mu_1 = m_{\\text{T}} = 0.5$, $v_1 = v_{\\text{T}} = 0.5$\n$\\mu_2 = m_{\\text{A}} = 0.1$, $v_2 = v_{\\text{A}} = 0.8$\n\n将这些值代入KL散度公式：\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln\\left(\\frac{0.8}{0.5}\\right) - 1 + \\frac{0.5}{0.8} + \\frac{(0.5-0.1)^2}{0.8} \\right) $$\n我们计算括号内的每一项：\n$$ \\frac{v_{\\text{A}}}{v_{\\text{T}}} = \\frac{0.8}{0.5} = 1.6 $$\n$$ \\frac{v_{\\text{T}}}{v_{\\text{A}}} = \\frac{0.5}{0.8} = 0.625 $$\n$$ \\frac{(m_{\\text{T}}-m_{\\text{A}})^2}{v_{\\text{A}}} = \\frac{(0.4)^2}{0.8} = \\frac{0.16}{0.8} = 0.2 $$\n现在将这些值代回散度的表达式中：\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln(1.6) - 1 + 0.625 + 0.2 \\right) $$\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) = \\frac{1}{2} \\left( \\ln(1.6) - 0.175 \\right) $$\n使用计算器计算自然对数：\n$$ \\ln(1.6) \\approx 0.470003629 $$\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) \\approx \\frac{1}{2} (0.470003629 - 0.175) = \\frac{1}{2} (0.295003629) \\approx 0.1475018145 $$\n问题要求答案四舍五入到4位有效数字。\n$$ D_{\\mathrm{KL}}(p_{\\text{T}} \\| p_{\\text{A}}) \\approx 0.1475 $$\n这个值表示当用智能体的错误设定的分布来近似真实后验分布时，以奈特（nats）为单位衡量的信息损失。",
            "answer": "$$\\boxed{0.1475}$$"
        },
        {
            "introduction": "在掌握了基本原理之后，这个实践将探讨如何使感知模型更加稳健，以应对充满噪声或异常值的现实世界数据。通过在贝叶斯模型中使用“重尾”先验，我们可以为罕见事件分配更高的合理性，从而避免模型被极端数据点过度影响。 这个练习将引导您比较最大后验 (MAP) 估计和后验均值估计在柯西先验下的表现，从而深入分析不同的建模选择如何影响智能体对意外观测的恢复能力。",
            "id": "4128032",
            "problem": "考虑一个复杂自适应系统中的智能体，其对潜在标量刺激 $s$ 的知觉推断是根据贝叶斯原理建模的。该智能体对环境变异性的内部记忆由一个重尾先验表示，具体来说是一个位置为 $0$、尺度为 $\\gamma$ 的柯西分布，即 $p(s) \\propto \\left[1 + \\left(s/\\gamma\\right)^{2}\\right]^{-1}$。观测模型是方差已知的 $\\sigma^{2}$ 的高斯分布，即 $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma^{2})$。该智能体使用最大后验 (MAP) 估计 $\\hat{s}_{\\mathrm{MAP}}(o)$ 或后验均值估计 $\\hat{s}_{\\mathrm{PM}}(o) = \\mathbb{E}[s \\mid o]$ 来形成其知觉。\n\n仅从贝叶斯法则以及 MAP 和后验均值的定义出发，通过将对数后验关于 $s$ 的导数设为零，推导表征 $\\hat{s}_{\\mathrm{MAP}}(o)$ 的隐式方程。然后，将关于输入扰动的影响函数 (IF) 定义为 $\\mathrm{IF}(o) = \\frac{\\partial \\hat{s}(o)}{\\partial o}$，并使用对 $\\hat{s}_{\\mathrm{MAP}}(o)$ 的隐式微分，结合对 $\\hat{s}_{\\mathrm{PM}}(o)$ 的适当微积分和尾部行为论证，分析每个估计量对 $o$ 中离群值的稳健性。特别地，确定每个估计量的 $\\mathrm{IF}(o)$ 是有界的还是无界的，并描述其在 $\\lvert o \\rvert \\to \\infty$ 时的渐近行为。\n\n下列哪个陈述是正确的？\n\nA. 当 $\\gamma^{2} > \\sigma^{2}/4$ 时，最大后验估计量的影响函数 $\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{MAP}}(o)}{\\partial o}$ 在 $o$ 上是一致有界的，其上确界为 $\\sup_{o} \\mathrm{IF}_{\\mathrm{MAP}}(o) = \\left(1 - \\sigma^{2}/(4\\gamma^{2})\\right)^{-1}$；如果 $\\gamma^{2} \\le \\sigma^{2}/4$，$\\mathrm{IF}_{\\mathrm{MAP}}(o)$ 可能变得无界。\n\nB. 后验均值估计量的影响函数 $\\mathrm{IF}_{\\mathrm{PM}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{PM}}(o)}{\\partial o}$ 对所有 $o$ 都等于 $1$，且与重尾先验无关。\n\nC. 当 $\\lvert o \\rvert \\to \\infty$ 时，$\\mathrm{IF}_{\\mathrm{MAP}}(o)$ 和 $\\mathrm{IF}_{\\mathrm{PM}}(o)$ 都趋近于 $1$，并且两个估计都满足 $\\hat{s}(o) = o + \\mathcal{O}(1/o)$，这意味着极端离群值以单位增益渐近传递。\n\nD. 在柯西先验下，后验均值 $\\hat{s}_{\\mathrm{PM}}(o)$ 不存在，因为柯西分布没有有限的一阶矩。\n\nE. 在柯西先验和高斯似然下，对于所有的 $o$，最大后验估计量等于后验均值。\n\n选择所有适用项。",
            "solution": "这是一个有效的问题陈述。\n\n该问题要求在给定观测值 $o$ 的情况下，对潜变量 $s$ 的最大后验 (MAP) 和后验均值 (PM) 估计量进行分析。该模型将 $s$ 的重尾柯西先验与给定 $s$ 时 $o$ 的高斯似然相结合。\n\n**1. 已知条件与模型设定**\n\n贝叶斯模型的组成部分是：\n- 潜变量: $s \\in \\mathbb{R}$\n- 观测值: $o \\in \\mathbb{R}$\n- 先验分布: $p(s) = \\frac{1}{\\pi\\gamma} \\frac{1}{1 + (s/\\gamma)^2} \\propto \\left[1 + \\left(\\frac{s}{\\gamma}\\right)^2\\right]^{-1}$。这是一个位置为 $0$、尺度为 $\\gamma$ 的柯西分布。\n- 似然（观测模型）: $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(o-s)^2}{2\\sigma^2}\\right)$。这是一个均值为 $s$、方差为 $\\sigma^2$ 的高斯分布。\n\n**2. 后验分布与估计量**\n\n根据贝叶斯法则，$s$ 在给定 $o$ 时的后验分布与似然和先验的乘积成正比：\n$$p(s \\mid o) \\propto p(o \\mid s) p(s)$$\n$$p(s \\mid o) \\propto \\exp\\left(-\\frac{(o-s)^2}{2\\sigma^2}\\right) \\left[1 + \\left(\\frac{s}{\\gamma}\\right)^2\\right]^{-1}$$\n\n待分析的两个估计量是：\n- 最大后验 (MAP): $\\hat{s}_{\\mathrm{MAP}}(o) = \\arg\\max_{s} p(s \\mid o) = \\arg\\max_{s} \\log p(s \\mid o)$。\n- 后验均值 (PM): $\\hat{s}_{\\mathrm{PM}}(o) = \\mathbb{E}[s \\mid o] = \\int_{-\\infty}^{\\infty} s \\, p(s \\mid o) \\, ds$。\n\n**3. MAP 估计量（$\\hat{s}_{\\mathrm{MAP}}$）的分析**\n\n为了找到 MAP 估计，我们最大化对数后验。在相差一个加性常数的情况下，对数后验是：\n$$L(s) = \\log p(s \\mid o) = C - \\frac{(o-s)^2}{2\\sigma^2} - \\log\\left(1 + \\frac{s^2}{\\gamma^2}\\right)$$\n我们通过将关于 $s$ 的导数设为零来找到最大值：\n$$\\frac{dL(s)}{ds} = \\frac{o-s}{\\sigma^2} - \\frac{1}{1+s^2/\\gamma^2} \\cdot \\frac{2s}{\\gamma^2} = \\frac{o-s}{\\sigma^2} - \\frac{2s}{\\gamma^2+s^2}$$\n将 $s = \\hat{s}_{\\mathrm{MAP}}$ 处的导数设为零，得到隐式方程：\n$$\\frac{o-\\hat{s}_{\\mathrm{MAP}}}{\\sigma^2} = \\frac{2\\hat{s}_{\\mathrm{MAP}}}{\\gamma^2+\\hat{s}_{\\mathrm{MAP}}^2}$$\n这可以被重新整理为 $o = \\hat{s}_{\\mathrm{MAP}} + \\frac{2\\sigma^2 \\hat{s}_{\\mathrm{MAP}}}{\\gamma^2+\\hat{s}_{\\mathrm{MAP}}^2}$。这就是所要求的隐式方程。\n\n现在我们来分析 MAP 估计量的影响函数 (IF)，$\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{MAP}}(o)}{\\partial o}$。我们对整理后的方程应用隐式微分，令 $\\hat{s} = \\hat{s}_{\\mathrm{MAP}}(o)$：\n$$1 = \\frac{d\\hat{s}}{do} + \\frac{d}{do}\\left(\\frac{2\\sigma^2 \\hat{s}}{\\gamma^2 + \\hat{s}^2}\\right) = \\frac{d\\hat{s}}{do} + \\frac{d}{d\\hat{s}}\\left(\\frac{2\\sigma^2 \\hat{s}}{\\gamma^2 + \\hat{s}^2}\\right) \\frac{d\\hat{s}}{do}$$\n$$1 = \\frac{d\\hat{s}}{do} \\left[ 1 + \\frac{(\\gamma^2+\\hat{s}^2)(2\\sigma^2) - (2\\sigma^2\\hat{s})(2\\hat{s})}{(\\gamma^2+\\hat{s}^2)^2} \\right]$$\n$$1 = \\frac{d\\hat{s}}{do} \\left[ 1 + \\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2} \\right]$$\n因此，影响函数是：\n$$\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{d\\hat{s}}{do} = \\left[ 1 + \\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2} \\right]^{-1}$$\n如果分母可以为零，IF 就会变得无界。设 $u = \\hat{s}^2 \\ge 0$。分母为零的条件是 $1 + \\frac{2\\sigma^2(\\gamma^2-u)}{(\\gamma^2+u)^2} = 0$，这等价于关于 $u$ 的二次方程：\n$$u^2 + (2\\gamma^2 - 2\\sigma^2)u + (\\gamma^4 + 2\\sigma^2\\gamma^2) = 0$$\n该方程有实数根 $u$ 的条件是其判别式 $\\Delta \\ge 0$：\n$$\\Delta = (2\\gamma^2-2\\sigma^2)^2 - 4(\\gamma^4+2\\sigma^2\\gamma^2) = 4\\sigma^2(\\sigma^2-4\\gamma^2)$$\n对于实数根，我们需要 $\\sigma^2-4\\gamma^2 \\ge 0$，即 $\\gamma^2 \\le \\sigma^2/4$。如果这个条件成立，则存在使 IF 无界的 $\\hat{s}$ 值。\n如果 $\\gamma^2 > \\sigma^2/4$，则 $\\Delta < 0$，分母永不为零，IF 是有界的。为了找到其上确界，我们求分母 $D(\\hat{s}) = 1 + \\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2}$ 的最小值。最小值在 $\\hat{s}^2=3\\gamma^2$ 时取得，最小值为 $D_{\\min} = 1 - \\sigma^2/(4\\gamma^2)$。因此 IF 的上确界是 $\\sup_o \\mathrm{IF}_{\\mathrm{MAP}}(o) = (D_{\\min})^{-1} = \\left(1 - \\frac{\\sigma^2}{4\\gamma^2}\\right)^{-1}$。\n\n$\\hat{s}_{\\mathrm{MAP}}$ 的渐近行为：当 $|o| \\to \\infty$ 时，我们必有 $|\\hat{s}| \\to \\infty$。从隐式方程 $o = \\hat{s} + \\frac{2\\sigma^2 \\hat{s}}{\\gamma^2+\\hat{s}^2}$ 来看，第二项的行为类似于 $2\\sigma^2\\hat{s}/\\hat{s}^2 = \\mathcal{O}(1/\\hat{s})$。因此，$\\hat{s} \\approx o$。将其代回得到 $\\hat{s}(o) \\approx o - \\frac{2\\sigma^2}{o}$，其形式为 $\\hat{s}(o) = o + \\mathcal{O}(1/o)$。\n$\\mathrm{IF}_{\\mathrm{MAP}}$ 的渐近行为：当 $|o| \\to \\infty$ 时， $|\\hat{s}| \\to \\infty$。项 $\\frac{2\\sigma^2(\\gamma^2-\\hat{s}^2)}{(\\gamma^2+\\hat{s}^2)^2} \\to 0$。因此，$\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{MAP}}(o) = 1$。\n\n**4. 后验均值估计量（$\\hat{s}_{\\mathrm{PM}}$）的分析**\n\n后验均值为 $\\hat{s}_{\\mathrm{PM}}(o) = \\mathbb{E}[s \\mid o]$。其存在性是有保证的，因为后验密度 $p(s|o)$ 的尾部由于高斯似然的存在而以 $\\exp(-s^2/(2\\sigma^2))$ 的速度衰减，这确保了 $\\int |s| p(s|o) ds < \\infty$。\n\n对于高斯似然，PM 估计量的影响函数有一个已知的形式（Tweedie 公式）：\n$$\\mathrm{IF}_{\\mathrm{PM}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{PM}}(o)}{\\partial o} = \\frac{1}{\\sigma^2} \\mathrm{Var}(s \\mid o)$$\n其中 $\\mathrm{Var}(s \\mid o) = \\mathbb{E}[s^2 \\mid o] - (\\mathbb{E}[s \\mid o])^2$ 是 $s$ 的后验方差。\n\n$\\hat{s}_{\\mathrm{PM}}$ 的渐近行为：当 $|o| \\to \\infty$ 时，似然项 $p(o|s)$ 在 $s=o$ 附近变得非常尖锐。先验 $p(s)$ 在这个区域非常平坦。这使得后验分布 $p(s|o)$ 近似为高斯分布，$p(s \\mid o) \\approx \\mathcal{N}(s; o, \\sigma^2)$。因此，$\\mathbb{E}[s \\mid o] \\approx o$ 且 $\\mathrm{Var}(s \\mid o) \\approx \\sigma^2$。一个更详细的展开（如对 MAP 情况所做，通过在似然峰值周围展开先验项）表明 $\\hat{s}_{\\mathrm{PM}}(o) \\approx o - \\frac{2\\sigma^2}{o}$，这意味着 $\\hat{s}_{\\mathrm{PM}}(o) = o + \\mathcal{O}(1/o)$。\n$\\mathrm{IF}_{\\mathrm{PM}}$ 的渐近行为：使用渐近后验方差，我们发现：\n$$\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{PM}}(o) = \\lim_{|o|\\to\\infty} \\frac{\\mathrm{Var}(s \\mid o)}{\\sigma^2} = \\frac{\\sigma^2}{\\sigma^2} = 1$$\n由于对于有限的 $\\sigma^2$，$\\mathrm{Var}(s \\mid o)$ 总是有限且为正，因此 $\\mathrm{IF}_{\\mathrm{PM}}(o)$ 总是有界的。\n\n**5. 选项评估**\n\n**A. 当 $\\gamma^{2} > \\sigma^{2}/4$ 时，最大后验估计量的影响函数 $\\mathrm{IF}_{\\mathrm{MAP}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{MAP}}(o)}{\\partial o}$ 在 $o$ 上是一致有界的，其上确界为 $\\sup_{o} \\mathrm{IF}_{\\mathrm{MAP}}(o) = \\left(1 - \\sigma^{2}/(4\\gamma^{2})\\right)^{-1}$；如果 $\\gamma^{2} \\le \\sigma^{2}/4$，$\\mathrm{IF}_{\\mathrm{MAP}}(o)$ 可能变得无界。**\n我们在第3节的推导证实了这一陈述的每个部分。有界性条件是正确的，上确界的公式是正确的，无界性条件也是正确的。\n**结论：正确。**\n\n**B. 后验均值估计量的影响函数 $\\mathrm{IF}_{\\mathrm{PM}}(o) = \\frac{\\partial \\hat{s}_{\\mathrm{PM}}(o)}{\\partial o}$ 对所有 $o$ 都等于 $1$，且与重尾先验无关。**\n我们的推导表明 $\\mathrm{IF}_{\\mathrm{PM}}(o) = \\mathrm{Var}(s|o)/\\sigma^2$。后验方差 $\\mathrm{Var}(s|o)$ 取决于先验和似然之间的相互作用，通常不等于 $\\sigma^2$（除非在平坦先验或 $|o|\\to\\infty$ 的极限情况下）。例如，当 $o=0$ 时，先验将后验拉向原点，与单独的似然相比减小了其方差，因此 $\\mathrm{Var}(s|0) < \\sigma^2$ 且 $\\mathrm{IF}_{\\mathrm{PM}}(0) < 1$。因此，IF 不是常数，也不总是等于 1。\n**结论：不正确。**\n\n**C. 当 $\\lvert o \\rvert \\to \\infty$ 时，$\\mathrm{IF}_{\\mathrm{MAP}}(o)$ 和 $\\mathrm{IF}_{\\mathrm{PM}}(o)$ 都趋近于 $1$，并且两个估计都满足 $\\hat{s}(o) = o + \\mathcal{O}(1/o)$，这意味着极端离群值以单位增益渐近传递。**\n我们在第3节和第4节的分析表明 $\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{MAP}}(o) = 1$ 和 $\\lim_{|o|\\to\\infty} \\mathrm{IF}_{\\mathrm{PM}}(o) = 1$。我们还推导出，对于这两个估计量，$\\hat{s}(o) \\approx o - 2\\sigma^2/o$，其形式为 $o + \\mathcal{O}(1/o)$。影响函数趋近于 1 和估计量 $\\hat{s}(o) \\approx o$ 意味着极端观测值以接近单位的增益通过。\n**结论：正确。**\n\n**D. 在柯西先验下，后验均值 $\\hat{s}_{\\mathrm{PM}}(o)$ 不存在，因为柯西分布没有有限的一阶矩。**\n这是一个常见的误解。虽然柯西分布的*先验*均值 $\\mathbb{E}[s]$ 不存在，但*后验*均值 $\\mathbb{E}[s \\mid o]$ 是存在的。由于高斯似然项的存在，后验密度 $p(s|o)$ 对于大的 $|s|$ 呈指数衰减，这足以确保积分 $\\int_{-\\infty}^{\\infty} s p(s \\mid o) ds$ 的收敛。因此，后验均值是良定义的。\n**结论：不正确。**\n\n**E. 在柯西先验和高斯似然下，对于所有的 $o$，最大后验估计量等于后验均值。**\n一个分布的均值和众数相等，当且仅当该分布是对称的。后验分布 $p(s|o) \\propto \\exp(-\\frac{(o-s)^2}{2\\sigma^2}) [1 + (s/\\gamma)^2]^{-1}$ 是一个以 $o$ 为中心的高斯分布和一个以 $0$ 为中心的柯西分布的乘积。对于任何 $o \\neq 0$，得到的分布是非对称的（偏态的）。因此，其均值和众数不会重合。它们仅在 $o=0$ 的特殊情况下（此时均为 $0$）以及在 $|o|\\to\\infty$ 的极限下相等。陈述“对于所有 $o$”是错误的。\n**结论：不正确。**",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "本章的最后一个实践将从抽象的统计模型转向现代的计算记忆架构。认知系统通常被认为使用了多种记忆系统：一个用于积累一般知识的慢速、参数化系统，以及一个用于快速编码特定情节的快速、非参数化系统。这个练习对比了一个简单的参数化模型（逻辑回归）和一个使用注意力机制的非参数化外部记忆模块。 这个编码实践让您能够在一个“小样本”学习场景中实现并比较这两种截然不同的记忆架构，您将亲眼观察到基于注意力的外部记忆如何使智能体快速适应新信息，而这正是具有错位先验的纯参数化模型通常难以完成的任务。",
            "id": "4128110",
            "problem": "考虑一个复杂自适应系统中的智能体，该智能体必须使用内部参数模型或外部可微内存，在感知到的刺激和二元标签之间快速绑定新的关联。智能体将每个刺激感知为一个 $d$ 维实数空间中的键向量。设每个刺激由向量 $x \\in \\mathbb{R}^d$ 表示，感知到的键为 $k = f(x)$，其中 $f$ 定义为恒等映射 $f(x) = x$。存在两个类别，其标签为 $y \\in \\{0,1\\}$。对于一个给定的任务，类别原型是固定的未知向量 $\\mu_0, \\mu_1 \\in \\mathbb{R}^d$。类别 $c \\in \\{0,1\\}$ 的支持样本生成方式为 $k = \\mu_c + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d)$，$I_d$ 是 $d \\times d$ 的单位矩阵，$\\sigma > 0$ 是感知噪声尺度。查询样本以同样的方式生成，并用于评估分类准确率。\n\n无外部内存的内部参数模型：智能体维护一个线性参数向量 $w \\in \\mathbb{R}^d$，并使用伯努利逻辑回归似然。给定键 $k$，标签 $y=1$ 的概率为 $p(y=1 \\mid k, w) = \\sigma(w^\\top k)$，其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 是 logistic 函数。智能体从一个源自早前任务的先验参数 $w_0 \\in \\mathbb{R}^d$ 开始，并使用 $n_s$ 个支持样本 $\\{(k_i, y_i)\\}_{i=1}^{n_s^{\\mathrm{tot}}}$（其中 $n_s^{\\mathrm{tot}}$ 是两个类别的支持样本总数）对伯努利对数似然执行单步梯度上升。伯努利对数似然相对于 $w$ 的梯度为 $\\nabla_w \\sum_i \\left[ y_i \\log \\sigma(w^\\top k_i) + (1-y_i)\\log(1-\\sigma(w^\\top k_i)) \\right] = \\sum_i (y_i - \\sigma(w^\\top k_i)) k_i$。在学习率为 $\\eta > 0$ 的一步更新后，更新后的参数为 $w_1 = w_0 + \\eta \\sum_{i=1}^{n_s^{\\mathrm{tot}}} \\left(y_i - \\sigma(w_0^\\top k_i)\\right) k_i$。智能体将键为 $k_q$ 的查询分类为 $\\hat{y} = 1$（如果 $\\sigma(w_1^\\top k_q) \\ge 0.5$）或 $\\hat{y} = 0$（否则）。\n\n外部可微内存模块：智能体拥有一个外部内存 $M$，最多可存储 $C$ 个键值对 $(k_i, y_i)$。在观察到支持样本后，智能体将每个键值对顺序写入内存。如果存储的键值对数量超过 $C$，则只保留最新的 $C$ 对，早期的键值对将被丢弃。在查询时，智能体使用基于相似度的 softmax 注意力机制从内存中读取。给定一个查询键 $k_q$，支持样本 $i$ 的注意力权重为 $a_i = \\frac{\\exp(\\beta k_q^\\top k_i)}{\\sum_{j} \\exp(\\beta k_q^\\top k_j)}$，其中 $\\beta > 0$ 是一个控制锐度的逆温度。预测结果是加权平均值 $\\hat{y}_{\\mathrm{mem}} = \\sum_i a_i y_i$，如果 $\\hat{y}_{\\mathrm{mem}} \\ge 0.5$，则分类标签为 $\\hat{y} = 1$，否则为 $\\hat{y} = 0$。如果内存为空，则定义 $\\hat{y}_{\\mathrm{mem}} = 0.5$。\n\n任务生成与先验：对于每个测试用例，生成类别原型 $\\mu_0, \\mu_1$ 作为 $\\mathbb{R}^d$ 中的随机单位向量。先验参数 $w_0$ 由早前不相关的原型 $v_0, v_1$ 生成，即 $w_0 = \\alpha (v_1 - v_0)$，其中 $v_0, v_1$ 是 $\\mathbb{R}^d$ 中的随机单位向量，$\\alpha \\ge 0$ 控制了来自先前任务的偏置强度。这个先验封装了一个可能与当前任务不一致的参数化期望。\n\n目标：展示外部内存如何通过实现新关联的快速绑定来改善少样本感知，并在一系列任务上计算有内存和无内存情况下的分类准确率。准确率定义为在一组查询中被正确分类的样本所占的比例，表示为 $[0,1]$ 范围内的一个小数。不涉及任何物理单位。\n\n程序要求：\n- 实现一个程序，对于每个测试用例：\n  - 使用指定的维度和种子生成原型 $\\mu_0, \\mu_1$ 和先验 $w_0$，以确保可复现性。\n  - 每个类别抽取 $n_s$ 个支持样本，并按以下顺序写入内存：对于从 $1$ 到 $n_s$ 的 shot 索引 $t$，先写入 $(k^{(t)}_0, 0)$，然后写入 $(k^{(t)}_1, 1)$。\n  - 使用所有支持样本执行一次梯度上升步骤以获得 $w_1$。\n  - 每个类别抽取 $T$ 个查询样本，并在所有 $2T$ 个查询上评估两种方法的准确率。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按每个测试用例的配对的扁平顺序排列：$[\\mathrm{acc}_{\\mathrm{no\\_mem,1}}, \\mathrm{acc}_{\\mathrm{mem,1}}, \\mathrm{acc}_{\\mathrm{no\\_mem,2}}, \\mathrm{acc}_{\\mathrm{mem,2}}, \\dots]$，其中每个 $\\mathrm{acc}$ 是一个十进制数。\n\n测试套件：\n- 案例 1 (理想路径，少样本，内存启用):\n  - 维度 $d = 64$\n  - 先验强度 $\\alpha = 2.0$\n  - 学习率 $\\eta = 0.2$\n  - 每类样本数 (Shots) $n_s = 1$\n  - 容量 $C = 2$\n  - 逆温度 $\\beta = 12.0$\n  - 噪声尺度 $\\sigma = 0.1$\n  - 每类查询数 $T = 2000$\n  - 随机种子 $s = 12345$\n- 案例 2 (边界情况，零样本，空内存):\n  - 维度 $d = 32$\n  - 先验强度 $\\alpha = 1.0$\n  - 学习率 $\\eta = 0.2$\n  - 每类样本数 (Shots) $n_s = 0$\n  - 容量 $C = 0$\n  - 逆温度 $\\beta = 12.0$\n  - 噪声尺度 $\\sigma = 0.1$\n  - 每类查询数 $T = 1000$\n  - 随机种子 $s = 23456$\n- 案例 3 (内存干扰，容量为 1):\n  - 维度 $d = 32$\n  - 先验强度 $\\alpha = 1.0$\n  - 学习率 $\\eta = 0.2$\n  - 每类样本数 (Shots) $n_s = 1$\n  - 容量 $C = 1$\n  - 逆温度 $\\beta = 12.0$\n  - 噪声尺度 $\\sigma = 0.1$\n  - 每类查询数 $T = 2000$\n  - 随机种子 $s = 34567$\n- 案例 4 (高噪声下的性能下降):\n  - 维度 $d = 32$\n  - 先验强度 $\\alpha = 1.0$\n  - 学习率 $\\eta = 0.2$\n  - 每类样本数 (Shots) $n_s = 5$\n  - 容量 $C = 10$\n  - 逆温度 $\\beta = 12.0$\n  - 噪声尺度 $\\sigma = 1.0$\n  - 每类查询数 $T = 4000$\n  - 随机种子 $s = 45678$\n\n您的任务是实现该程序，以计算这些案例的准确率，并以指定的单行格式打印它们。不涉及任何角度；所有报告的量都是无单位的小数。程序的正确性将通过其是否遵循指定的模型并为测试套件生成所要求的单行输出格式来评估。",
            "solution": "问题已经过分析并被认为是有效的。这是一个在复杂自适应系统建模领域内提法得当且有科学依据的问题，特别关注智能体的感知和内存。问题陈述提供了一套完整且一致的定义、参数和约束，足以产生一个唯一且可验证的计算解。\n\n解决方案涉及实现一个模拟，以比较两种用于少样本分类任务的认知模型：一个使用逻辑回归的参数模型和一个基于外部可微内存的非参数模型。下面详细描述了每个测试用例的处理过程，该过程遵循了问题的规范。\n\n**1. 初始化与任务生成**\n对于每个测试用例，使用提供的种子 $s$ 初始化一个专用的伪随机数生成器，以确保可复现性。任务的核心由两个类别原型 $\\mu_0, \\mu_1 \\in \\mathbb{R}^d$ 定义，它们代表了 $d$ 维特征空间中每个类别的均值。这些原型通过从各向同性多维正态分布 $\\mathcal{N}(0, I_d)$ 中抽取向量并将其归一化为单位长度来生成，即 $z \\sim \\mathcal{N}(0, I_d)$，$\\mu = z / \\|z\\|_2$。\n\n**2. 先验知识构建**\n参数模型从一个先验参数向量 $w_0$ 开始，该向量封装了来自先前不相关任务经验的偏置。这个先验的构建方式为 $w_0 = \\alpha(v_1 - v_0)$，其中 $v_0$ 和 $v_1$ 是另一对以与原型相同方式生成的独立随机单位向量，$\\alpha \\ge 0$ 是一个控制此先验偏置强度的标量。\n\n**3. 支持数据生成**\n为适应当前任务，智能体体接收到来自两个类别各 $n_s$ 个支持样本。对于每个类别 $c \\in \\{0, 1\\}$，支持键 $k$ 从以相应原型为中心的高斯分布中采样：$k = \\mu_c + \\epsilon$，其中噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_d)$。问题指定了一个顺序观察顺序：对于从 $1$ 到 $n_s$ 的每个 shot 索引 $t$，智能体首先观察来自类别 0 的样本 $(k_0^{(t)}, 0)$，然后观察来自类别 1 的样本 $(k_1^{(t)}, 1)$。这将生成总共 $n_s^{\\mathrm{tot}} = 2n_s$ 个支持样本。\n\n**4. 参数模型更新（无外部内存）**\n内部模型是一个逻辑回归分类器。一个键 $k$ 属于类别 1 的概率由 $p(y=1 \\mid k, w) = \\sigma(w^\\top k)$ 给出，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是 logistic S型函数。智能体通过对观察到的支持集的对数似然执行单步梯度上升来更新其先验参数 $w_0$。更新后的参数向量 $w_1$ 计算如下：\n$$w_1 = w_0 + \\eta \\sum_{i=1}^{n_s^{\\mathrm{tot}}} \\left(y_i - \\sigma(w_0^\\top k_i)\\right) k_i$$\n此处，$\\eta > 0$ 是学习率，求和遍及所有 $n_s^{\\mathrm{tot}}$ 个支持样本 $\\{(k_i, y_i)\\}$。如果 $n_s=0$，则不执行更新，且 $w_1 = w_0$。在评估时，如果 $\\sigma(w_1^\\top k_q) \\ge 0.5$，则查询键 $k_q$ 被分类为标签 $\\hat{y}=1$，否则为 $\\hat{y}=0$。\n\n**5. 外部内存模型**\n外部内存模型是一种直接利用近期经验的非参数方法。它维护一个容量为 $C$ 的内存缓冲区。智能体在观察到支持对 $(k_i, y_i)$ 时将其存储。如果内存超过容量，最旧的条目将被丢弃，只保留最新的 $C$ 对。在查询时，给定一个键 $k_q$，模型使用基于内容的注意力机制检索信息。每个内存项 $j$（键为 $k_j$）与查询 $k_q$ 的相关性通过注意力权重来衡量，该权重通过对缩放点积相似度应用 softmax 函数计算得出：\n$$ a_j = \\frac{\\exp(\\beta k_q^\\top k_j)}{\\sum_{l \\in M} \\exp(\\beta k_q^\\top k_l)} $$\n其中 $M$ 表示内存中的项集，$\\beta > 0$ 是一个控制注意力分布锐度的逆温度参数。最终预测 $\\hat{y}_{\\mathrm{mem}}$ 是内存中标签 $y_j$ 的加权平均值：$\\hat{y}_{\\mathrm{mem}} = \\sum_{j \\in M} a_j y_j$。如果 $\\hat{y}_{\\mathrm{mem}} \\ge 0.5$，则分类为 $\\hat{y}=1$，否则为 $\\hat{y}=0$。\n\n如果内存为空（即 $n_s=0$ 或 $C=0$），则适用一个特殊情况：预测值固定为 $\\hat{y}_{\\mathrm{mem}} = 0.5$。根据分类规则，这将导致所有查询都被分类为标签 1。\n\n**6. 评估**\n对于每个模型，性能通过分类准确率来衡量。生成一个包含 $2T$ 个查询样本的测试集（每个类别 $T$ 个，使用与支持集相同的生成过程）。准确率是这两个模型各自正确分类的查询样本在 $2T$ 个总查询样本中所占的比例。最终输出聚合了所有指定测试用例的这些准确率。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n\n    def sigmoid(z):\n        \"\"\"Computes the logistic sigmoid function.\"\"\"\n        # Clip z to avoid overflow in exp\n        z_clipped = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z_clipped))\n\n    def run_simulation(d, alpha, eta, n_s, C, beta, sigma_noise, T, seed):\n        \"\"\"\n        Runs the simulation for a single test case.\n\n        Returns:\n            (float, float): A tuple containing the accuracy of the no-memory model\n                            and the accuracy of the memory model.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        def generate_unit_vector(dim):\n            \"\"\"Generates a random unit vector in d dimensions.\"\"\"\n            vec = rng.normal(size=dim)\n            norm = np.linalg.norm(vec)\n            if norm == 0:\n                # Fallback for the extremely unlikely case of a zero vector\n                unit_vec = np.zeros(dim)\n                unit_vec[0] = 1.0\n                return unit_vec\n            return vec / norm\n\n        # Generate task prototypes mu_0, mu_1\n        mu0 = generate_unit_vector(d)\n        mu1 = generate_unit_vector(d)\n\n        # Generate prior w_0 from unrelated task prototypes v_0, v_1\n        v0 = generate_unit_vector(d)\n        v1 = generate_unit_vector(d)\n        w0 = alpha * (v1 - v0)\n\n        # Generate support set based on n_s (shots per class)\n        support_keys_list = []\n        support_labels_list = []\n        if n_s > 0:\n            for _ in range(n_s):\n                # Class 0 example\n                k0 = mu0 + rng.normal(0, sigma_noise, size=d)\n                support_keys_list.append(k0)\n                support_labels_list.append(0)\n                # Class 1 example\n                k1 = mu1 + rng.normal(0, sigma_noise, size=d)\n                support_keys_list.append(k1)\n                support_labels_list.append(1)\n        \n        support_keys = np.array(support_keys_list)\n        support_labels = np.array(support_labels_list)\n        n_s_tot = 2 * n_s\n\n        # ---- 1. Parametric model (no memory) update ----\n        if n_s_tot > 0:\n            # Predictions on support set with prior w_0\n            preds_w0 = sigmoid(support_keys @ w0)\n            # Gradient of log-likelihood\n            errors = support_labels - preds_w0\n            grad = support_keys.T @ errors\n            # Single gradient ascent step\n            w1 = w0 + eta * grad\n        else:\n            # Zero-shot case: no update\n            w1 = w0\n        \n        # ---- 2. External memory model setup ----\n        if C > 0 and n_s_tot > 0:\n            memory_keys = support_keys[-C:]\n            memory_labels = support_labels[-C:]\n        else:\n            # Memory is empty if capacity is 0 or no supports are given\n            memory_keys = np.array([])\n            memory_labels = np.array([])\n\n        # Generate query set\n        query_keys_0 = mu0 + rng.normal(0, sigma_noise, size=(T, d))\n        query_keys_1 = mu1 + rng.normal(0, sigma_noise, size=(T, d))\n        query_keys = np.vstack((query_keys_0, query_keys_1))\n        # True labels for the query set\n        query_labels = np.array([0] * T + [1] * T)\n        \n        # ---- Evaluate no-memory model ----\n        preds_no_mem_prob = sigmoid(query_keys @ w1)\n        preds_no_mem_label = (preds_no_mem_prob >= 0.5).astype(int)\n        acc_no_mem = np.mean(preds_no_mem_label == query_labels)\n        \n        # ---- Evaluate memory model ----\n        if memory_keys.shape[0] == 0:\n            # As per spec, empty memory predicts 0.5, which classifies as 1\n            preds_mem_label = np.ones_like(query_labels)\n        else:\n            # Similarities: (2T, d) @ (d, C_eff) -> (2T, C_eff)\n            sims = query_keys @ memory_keys.T\n            scaled_sims = beta * sims\n            \n            # Stable softmax (row-wise) to get attention weights\n            max_per_row = np.max(scaled_sims, axis=1, keepdims=True)\n            exp_sims = np.exp(scaled_sims - max_per_row)\n            sum_exp_sims = np.sum(exp_sims, axis=1, keepdims=True)\n            attention_weights = exp_sims / sum_exp_sims\n            \n            # Weighted average prediction: (2T, C_eff) @ (C_eff,) -> (2T,)\n            preds_mem_prob = attention_weights @ memory_labels\n            preds_mem_label = (preds_mem_prob >= 0.5).astype(int)\n            \n        acc_mem = np.mean(preds_mem_label == query_labels)\n        \n        return acc_no_mem, acc_mem\n\n    test_cases = [\n        {'d': 64, 'alpha': 2.0, 'eta': 0.2, 'n_s': 1, 'C': 2, 'beta': 12.0, 'sigma': 0.1, 'T': 2000, 's': 12345},\n        {'d': 32, 'alpha': 1.0, 'eta': 0.2, 'n_s': 0, 'C': 0, 'beta': 12.0, 'sigma': 0.1, 'T': 1000, 's': 23456},\n        {'d': 32, 'alpha': 1.0, 'eta': 0.2, 'n_s': 1, 'C': 1, 'beta': 12.0, 'sigma': 0.1, 'T': 2000, 's': 34567},\n        {'d': 32, 'alpha': 1.0, 'eta': 0.2, 'n_s': 5, 'C': 10, 'beta': 12.0, 'sigma': 1.0, 'T': 4000, 's': 45678},\n    ]\n\n    results = []\n    for case in test_cases:\n        acc_no_mem, acc_mem = run_simulation(\n            d=case['d'],\n            alpha=case['alpha'],\n            eta=case['eta'],\n            n_s=case['n_s'],\n            C=case['C'],\n            beta=case['beta'],\n            sigma_noise=case['sigma'],\n            T=case['T'],\n            seed=case['s']\n        )\n        results.append(acc_no_mem)\n        results.append(acc_mem)\n\n    # Print results in the specified single-line format\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}