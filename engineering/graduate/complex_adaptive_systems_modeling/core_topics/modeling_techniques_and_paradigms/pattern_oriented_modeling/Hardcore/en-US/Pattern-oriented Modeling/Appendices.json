{
    "hands_on_practices": [
        {
            "introduction": "A key signature of many complex adaptive systems is the emergence of non-random spatial structure. To use these structures as constraining patterns in a modeling context, we must first quantify them. This exercise  introduces the calculation of Moran's $I$, a fundamental statistic for measuring spatial autocorrelation, which tells us whether similar values tend to cluster together in space. Mastering this calculation is a first step toward validating the spatial mechanisms in your agent-based models.",
            "id": "4136524",
            "problem": "A research team is calibrating a minimal cellular automaton (CA) within the framework of Pattern-Oriented Modeling (POM) to reproduce a target spatial pattern in a $3\\times 3$ raster representing local agent density after $t$ time steps. To test whether the CA’s local interaction rule induces spatial autocorrelation in the emergent pattern, compute the Moran’s $I$ statistic for the following raster of observed values (indexed row-major by $i=1,\\dots,9$):\n$$\n\\begin{pmatrix}\n2 & 3 & 3 \\\\\n3 & 4 & 4 \\\\\n2 & 3 & 3\n\\end{pmatrix}\n$$\nAssume the following weight structure: the spatial weights $w_{ij}$ are binary Rook-adjacency weights on the lattice, with $w_{ij}=1$ if cells $i$ and $j$ are horizontally or vertically adjacent and $i \\neq j$, and $w_{ij}=0$ otherwise; weights are symmetric and $w_{ii}=0$. Let the sample size be $n=9$.\n\nCompute Moran’s $I$ for this configuration using only the specified weights and cell values. Round your answer to four significant figures. Express the final answer without units.",
            "solution": "The problem requires the computation of Moran's $I$ statistic for a given $3 \\times 3$ raster of values. The formula for Moran's $I$ is given by:\n$$\nI = \\frac{n}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}} \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (x_i - \\bar{x}) (x_j - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n$$\nwhere $n$ is the number of spatial units, $x_i$ is the value of the variable in unit $i$, $\\bar{x}$ is the mean of the variable, and $w_{ij}$ are the elements of the spatial weights matrix.\n\nFirst, we identify the given parameters and data. The sample size is $n=9$. The raster of observed values is:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n2 & 3 & 3 \\\\\n3 & 4 & 4 \\\\\n2 & 3 & 3\n\\end{pmatrix}\n$$\nFollowing the specified row-major indexing ($i=1, \\dots, 9$), the values $x_i$ are:\n$x_1=2$, $x_2=3$, $x_3=3$, $x_4=3$, $x_5=4$, $x_6=4$, $x_7=2$, $x_8=3$, $x_9=3$.\n\nThe next step is to calculate the sample mean, $\\bar{x}$:\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i = \\frac{1}{9} (2+3+3+3+4+4+2+3+3) = \\frac{27}{9} = 3\n$$\n\nNow, we compute the deviations from the mean, $(x_i - \\bar{x})$, for each cell:\n$x_1 - \\bar{x} = 2 - 3 = -1$\n$x_2 - \\bar{x} = 3 - 3 = 0$\n$x_3 - \\bar{x} = 3 - 3 = 0$\n$x_4 - \\bar{x} = 3 - 3 = 0$\n$x_5 - \\bar{x} = 4 - 3 = 1$\n$x_6 - \\bar{x} = 4 - 3 = 1$\n$x_7 - \\bar{x} = 2 - 3 = -1$\n$x_8 - \\bar{x} = 3 - 3 = 0$\n$x_9 - \\bar{x} = 3 - 3 = 0$\nLet $z_i = x_i - \\bar{x}$. The vector of deviations is $\\mathbf{z} = (-1, 0, 0, 0, 1, 1, -1, 0, 0)$.\n\nThe denominator of the second term in the Moran's $I$ formula is the sum of squared deviations:\n$$\n\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^9 z_i^2 = (-1)^2 + 0^2 + 0^2 + 0^2 + 1^2 + 1^2 + (-1)^2 + 0^2 + 0^2 = 1+0+0+0+1+1+1+0+0 = 4\n$$\n\nNext, we establish the spatial weights matrix $\\mathbf{W} = [w_{ij}]$ based on Rook adjacency. The weight $w_{ij}=1$ if cells $i$ and $j$ share a common edge, and $w_{ij}=0$ otherwise. Also, $w_{ii}=0$. We need to find the total sum of weights, $S_0 = \\sum_{i=1}^n \\sum_{j=1}^n w_{ij}$. This is the sum of all elements in the weights matrix. It can be calculated by summing the number of neighbors for each cell.\nThe grid has $4$ corners (each with $2$ neighbors), $4$ edge cells (each with $3$ neighbors), and $1$ center cell (with $4$ neighbors).\n$$\nS_0 = (4 \\times 2) + (4 \\times 3) + (1 \\times 4) = 8 + 12 + 4 = 24\n$$\n\nThe final component needed is the numerator of the second term, $\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (x_i - \\bar{x}) (x_j - \\bar{x}) = \\sum_{i=1}^n \\sum_{j=1}^n w_{ij} z_i z_j$. This sum is non-zero only for pairs of adjacent cells $(i, j)$ where both $z_i$ and $z_j$ are non-zero.\nThe deviations can be represented on the grid:\n$$\n\\begin{pmatrix}\nz_1 & z_2 & z_3 \\\\\nz_4 & z_5 & z_6 \\\\\nz_7 & z_8 & z_9\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1 & 0 & 0 \\\\\n0 & 1 & 1 \\\\\n-1 & 0 & 0\n\\end{pmatrix}\n$$\nWe need to identify all adjacent pairs $(i,j)$ for which $w_{ij}=1$ and calculate the sum of the products $z_i z_j$. The only adjacent pair where both deviations are non-zero is $(5,6)$.\nThe sum is over all $i$ and $j$, so we consider each directed edge.\n- Horizontal adjacency:\n  - Pair $(1,2)$: $w_{12}=1$, $z_1 z_2 = (-1)(0) = 0$.\n  - Pair $(2,3)$: $w_{23}=1$, $z_2 z_3 = (0)(0) = 0$.\n  - Pair $(4,5)$: $w_{45}=1$, $z_4 z_5 = (0)(1) = 0$.\n  - Pair $(5,6)$: $w_{56}=1$, $z_5 z_6 = (1)(1) = 1$.\n  - Pair $(7,8)$: $w_{78}=1$, $z_7 z_8 = (-1)(0) = 0$.\n  - Pair $(8,9)$: $w_{89}=1$, $z_8 z_9 = (0)(0) = 0$.\n- Vertical adjacency:\n  - Pair $(1,4)$: $w_{14}=1$, $z_1 z_4 = (-1)(0) = 0$.\n  - Pair $(2,5)$: $w_{25}=1$, $z_2 z_5 = (0)(1) = 0$.\n  - Pair $(3,6)$: $w_{36}=1$, $z_3 z_6 = (0)(1) = 0$.\n  - Pair $(4,7)$: $w_{47}=1$, $z_4 z_7 = (0)(-1) = 0$.\n  - Pair $(5,8)$: $w_{58}=1$, $z_5 z_8 = (1)(0) = 0$.\n  - Pair $(6,9)$: $w_{69}=1$, $z_6 z_9 = (1)(0) = 0$.\n\nThe only adjacent pair with non-zero products is $(5,6)$. The sum must account for both $w_{56} z_5 z_6$ and $w_{65} z_6 z_5$. Since the weights are symmetric ($w_{56}=w_{65}=1$), the total sum is:\n$$\n\\sum_{i=1}^9 \\sum_{j=1}^9 w_{ij} z_i z_j = w_{56}z_5 z_6 + w_{65}z_6 z_5 = (1)(1)(1) + (1)(1)(1) = 2\n$$\n\nNow we substitute all calculated components into the Moran's $I$ formula:\n$n=9$\n$S_0 = 24$\n$\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (x_i - \\bar{x}) (x_j - \\bar{x}) = 2$\n$\\sum_{i=1}^n (x_i - \\bar{x})^2 = 4$\n$$\nI = \\frac{9}{24} \\frac{2}{4} = \\frac{3}{8} \\times \\frac{1}{2} = \\frac{3}{16}\n$$\nAs a decimal, this is:\n$$\nI = 0.1875\n$$\nThe problem requires the answer to be rounded to four significant figures. The value $0.1875$ already has exactly four significant figures ($1$, $8$, $7$, $5$), so no further rounding is necessary.",
            "answer": "$$\\boxed{0.1875}$$"
        },
        {
            "introduction": "In addition to spatial organization, many complex systems generate characteristic statistical distributions, such as the power-law or \"heavy-tailed\" distributions of city sizes, earthquake magnitudes, or wealth. Being able to correctly identify and characterize these distributional patterns is a critical modeling skill. This problem  challenges you to first conceptually distinguish a power-law tail from a common alternative and then apply the powerful method of maximum likelihood estimation to fit a power-law model to data, a core task in analyzing model output.",
            "id": "4136534",
            "problem": "In Pattern-Oriented Modeling (POM), model structure and parameters are constrained by multiple observed patterns that are diagnostic of underlying mechanisms in complex adaptive systems. One such pattern for systems exhibiting large heterogeneity is the tail behavior of size distributions of emergent aggregates (for example, cluster sizes or event magnitudes). Consider two mechanistic hypotheses that imply different tail patterns for the variable $X$: a power-law tail and a lognormal tail. Assume you have filtered tail observations at a threshold $x_{\\min}$ in a way consistent with standard tail analysis.\n\n(a) Starting from the formal definitions of the complementary cumulative distribution function $\\bar{F}(x) = \\mathbb{P}(X \\ge x)$, and the standard forms of the power-law and lognormal distributions, explain how tail behavior can be used as a constraining pattern to discriminate between these two hypotheses. Your explanation should be grounded in asymptotic tail behavior expressed in terms of $\\bar{F}(x)$ for large $x$, including the functional dependence of $\\ln \\bar{F}(x)$ on $\\ln x$, and should explicitly identify the qualitative diagnostic pattern that differs between the two hypotheses.\n\n(b) You are given $n$ tail observations collected from simulations of a candidate agent-based model, thresholded at $x_{\\min} = 10$ and treated as independent and identically distributed. The observed tail values are\n$$\nx_i \\in \\{\\, 10,\\ 12,\\ 15,\\ 20,\\ 25,\\ 50,\\ 100,\\ 11,\\ 13,\\ 18,\\ 40,\\ 80 \\,\\}.\n$$\nAssuming that for $x \\ge x_{\\min}$ the tail follows a continuous power-law probability density function with exponent $\\alpha$ and support $[x_{\\min}, \\infty)$, derive from first principles the maximum likelihood estimator $\\hat{\\alpha}$ for $\\alpha$, and then compute its numerical value for the data given above. Report only the final numerical value of $\\hat{\\alpha}$, rounded to four significant figures. Do not include any units in your final reported value.",
            "solution": "(a) This part of the problem asks for an explanation of how to use tail behavior, specifically the complementary cumulative distribution function (CCDF) $\\bar{F}(x) = \\mathbb{P}(X \\ge x)$, to discriminate between a power-law hypothesis and a lognormal hypothesis for the tail of a distribution for $x \\ge x_{\\min}$. The key lies in the distinct asymptotic behavior of $\\bar{F}(x)$ for large $x$ under each hypothesis, which manifests as a qualitatively different functional form on a log-log plot.\n\nFirst, consider a continuous random variable $X$ whose tail follows a power-law distribution for $x \\ge x_{\\min}$. The probability density function (PDF) is given by $p(x) = C x^{-\\alpha}$ for some normalization constant $C$ and tail exponent $\\alpha > 1$. The CCDF is found by integrating the PDF from $x$ to $\\infty$:\n$$\n\\bar{F}(x) = \\mathbb{P}(X \\ge x) = \\int_x^{\\infty} p(y) dy = \\int_x^{\\infty} C y^{-\\alpha} dy = C \\left[ \\frac{y^{-\\alpha+1}}{1-\\alpha} \\right]_x^\\infty\n$$\nSince $\\alpha > 1$, the term $-\\alpha+1$ is negative, so $y^{-\\alpha+1} \\to 0$ as $y \\to \\infty$. The result is:\n$$\n\\bar{F}(x) = -C \\frac{x^{1-\\alpha}}{1-\\alpha} = \\frac{C}{\\alpha-1} x^{-(\\alpha-1)}\n$$\nFor a properly normalized PDF on $[x_{\\min}, \\infty)$, we have $\\bar{F}(x_{\\min}) = 1$, which implies that $\\frac{C}{\\alpha-1} = x_{\\min}^{\\alpha-1}$. Thus, the CCDF for a power-law tail is:\n$$\n\\bar{F}(x) = \\left(\\frac{x}{x_{\\min}}\\right)^{-(\\alpha-1)}\n$$\nTo analyze its behavior on a log-log scale, we take the natural logarithm of both sides:\n$$\n\\ln \\bar{F}(x) = -(\\alpha-1) \\ln\\left(\\frac{x}{x_{\\min}}\\right) = -(\\alpha-1) (\\ln x - \\ln x_{\\min})\n$$\nThis equation has the form $Y = m Z + c$, where $Y = \\ln \\bar{F}(x)$, $Z = \\ln x$, the slope is $m = -(\\alpha-1)$, and the intercept is $c = (\\alpha-1)\\ln x_{\\min}$. Therefore, for a true power-law distribution, a plot of $\\ln \\bar{F}(x)$ versus $\\ln x$ (a log-log plot of the CCDF) will yield a straight line for $x \\ge x_{\\min}$ with a negative slope.\n\nNext, consider the lognormal distribution. A random variable $X$ is lognormally distributed if $Y = \\ln X$ is normally distributed with mean $\\mu$ and standard deviation $\\sigma$. The PDF of the lognormal distribution is $p(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\right)$. The CCDF is $\\bar{F}(x) = \\mathbb{P}(X \\ge x) = \\mathbb{P}(\\ln X \\ge \\ln x) = \\mathbb{P}(Y \\ge \\ln x)$. This integral does not have a simple closed-form expression in terms of elementary functions. However, its asymptotic behavior for large $x$ (and thus large $\\ln x$) can be analyzed. For large $z$, the tail probability of a normal distribution $\\mathbb{P}(Y \\ge z)$ can be approximated as $\\mathbb{P}(Y \\ge z) \\sim \\frac{1}{z} \\phi(z)$, where $\\phi(z)$ is the normal PDF. Applying this to our case with $z = \\ln x$:\n$$\n\\bar{F}(x) \\sim \\frac{1}{\\ln x} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\right)\n$$\nTaking the natural logarithm, for large $x$:\n$$\n\\ln \\bar{F}(x) \\approx -\\ln(\\ln x) - \\frac{(\\ln x - \\mu)^2}{2\\sigma^2} + \\text{constant}\n$$\nThe dominant term in this expression is the quadratic term $-(\\ln x)^2/(2\\sigma^2)$. This means that $\\ln \\bar{F}(x)$ is a quadratic function of $\\ln x$.\n\nThe diagnostic pattern to discriminate between the two hypotheses is the functional form of the relationship between $\\ln \\bar{F}(x)$ and $\\ln x$.\n1.  **Power-Law Hypothesis:** A plot of $\\ln \\bar{F}(x)$ versus $\\ln x$ is a straight line.\n2.  **Lognormal Hypothesis:** A plot of $\\ln \\bar{F}(x)$ versus $\\ln x$ is not a straight line but is a downward-curving concave function, asymptotically behaving like a parabola.\nThis qualitative difference provides a powerful visual diagnostic. If the empirical CCDF plotted on log-log axes appears linear, it supports the power-law hypothesis. If it appears consistently curved downwards, it supports the lognormal hypothesis (or another \"faster-than-power-law\" decaying distribution).\n\n(b) This part requires deriving the maximum likelihood estimator (MLE) for the exponent $\\alpha$ of a continuous power-law distribution and computing its value for a given dataset.\nThe data are $n=12$ observations $\\{x_i\\}$, assumed to be i.i.d. draws from a distribution with PDF $p(x)$ for $x \\ge x_{\\min}$, where $x_{\\min}=10$.\nThe PDF for a continuous power-law on the support $[x_{\\min}, \\infty)$ is $p(x; \\alpha, x_{\\min}) = C x^{-\\alpha}$. We first find the normalization constant $C$ by ensuring $\\int_{x_{\\min}}^{\\infty} p(x) dx = 1$:\n$$\n\\int_{x_{\\min}}^{\\infty} C x^{-\\alpha} dx = C \\frac{x_{\\min}^{1-\\alpha}}{\\alpha-1} = 1 \\implies C = (\\alpha-1)x_{\\min}^{\\alpha-1}\n$$\nThis is valid for $\\alpha > 1$. The normalized PDF is:\n$$\np(x; \\alpha, x_{\\min}) = (\\alpha-1)x_{\\min}^{\\alpha-1} x^{-\\alpha}\n$$\nThe likelihood function $L(\\alpha)$ for $n$ i.i.d. observations $\\{x_i\\}$ is the product of the individual probabilities:\n$$\nL(\\alpha | \\{x_i\\}) = \\prod_{i=1}^n p(x_i; \\alpha, x_{\\min}) = \\prod_{i=1}^n \\left[ (\\alpha-1)x_{\\min}^{\\alpha-1} x_i^{-\\alpha} \\right]\n$$\n$$\nL(\\alpha) = (\\alpha-1)^n x_{\\min}^{n(\\alpha-1)} \\left( \\prod_{i=1}^n x_i \\right)^{-\\alpha}\n$$\nIt is easier to work with the log-likelihood function $\\mathcal{L}(\\alpha) = \\ln L(\\alpha)$:\n$$\n\\mathcal{L}(\\alpha) = n \\ln(\\alpha-1) + n(\\alpha-1)\\ln x_{\\min} - \\alpha \\sum_{i=1}^n \\ln x_i\n$$\nTo find the MLE $\\hat{\\alpha}$, we differentiate $\\mathcal{L}(\\alpha)$ with respect to $\\alpha$ and set the result to zero:\n$$\n\\frac{d\\mathcal{L}}{d\\alpha} = \\frac{n}{\\alpha-1} + n \\ln x_{\\min} - \\sum_{i=1}^n \\ln x_i = 0\n$$\nSolving for $\\alpha$ (which we denote $\\hat{\\alpha}$):\n$$\n\\frac{n}{\\hat{\\alpha}-1} = \\sum_{i=1}^n \\ln x_i - n \\ln x_{\\min} = \\sum_{i=1}^n (\\ln x_i - \\ln x_{\\min}) = \\sum_{i=1}^n \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)\n$$\n$$\n\\hat{\\alpha}-1 = \\frac{n}{\\sum_{i=1}^n \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\n$$\n\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^n \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}\n$$\nThis is the derived MLE for $\\alpha$.\n\nNow, we compute its numerical value for the given data. We have $n=12$, $x_{\\min}=10$, and the data set is $x_i \\in \\{10, 12, 15, 20, 25, 50, 100, 11, 13, 18, 40, 80\\}$.\nFirst, we compute the sum in the denominator:\n$$\nS = \\sum_{i=1}^{12} \\ln\\left(\\frac{x_i}{10}\\right) = \\ln\\left(\\frac{10}{10}\\right) + \\ln\\left(\\frac{12}{10}\\right) + \\ln\\left(\\frac{15}{10}\\right) + \\ln\\left(\\frac{20}{10}\\right) + \\ln\\left(\\frac{25}{10}\\right) + \\ln\\left(\\frac{50}{10}\\right) + \\ln\\left(\\frac{100}{10}\\right) + \\ln\\left(\\frac{11}{10}\\right) + \\ln\\left(\\frac{13}{10}\\right) + \\ln\\left(\\frac{18}{10}\\right) + \\ln\\left(\\frac{40}{10}\\right) + \\ln\\left(\\frac{80}{10}\\right)\n$$\n$$\nS = \\ln(1) + \\ln(1.2) + \\ln(1.5) + \\ln(2) + \\ln(2.5) + \\ln(5) + \\ln(10) + \\ln(1.1) + \\ln(1.3) + \\ln(1.8) + \\ln(4) + \\ln(8)\n$$\nCalculating the sum of these natural logarithms:\n$S \\approx 0 + 0.18232 + 0.40547 + 0.69315 + 0.91629 + 1.60944 + 2.30259 + 0.09531 + 0.26236 + 0.58779 + 1.38629 + 2.07944$\n$S \\approx 10.520445$\nNow, substitute this sum into the estimator for $\\hat{\\alpha}$:\n$$\n\\hat{\\alpha} = 1 + \\frac{12}{10.520445} \\approx 1 + 1.140635 = 2.140635\n$$\nThe problem requires the answer to be rounded to four significant figures. Rounding $2.140635$ yields $2.141$.",
            "answer": "$$\\boxed{2.141}$$"
        },
        {
            "introduction": "The core philosophy of Pattern-Oriented Modeling is to constrain models using multiple, diverse patterns simultaneously. This raises a crucial question: how does one combine the model's deviation from several patterns, especially when they have different units, variances, and correlations? This practice  demonstrates the use of the Mahalanobis distance, a statistically robust metric that normalizes for variance and accounts for covariance, to compute a single, unified discrepancy score between an empirical pattern vector and a model's output.",
            "id": "4136568",
            "problem": "In Pattern-Oriented Modeling (POM), multiple observed summary patterns are compared to model-generated patterns using discrepancy measures that reflect the joint variability and correlation structure of the patterns across stochastic replicates. Consider an agent-based model calibrated to an empirical pattern vector $\\mathbf{s}_{e}=(2.0,0.5,3.1)$, where each component represents a distinct, scale-heterogeneous summary statistic. From $n$ independent model replicates at a candidate parameter setting, the estimated mean pattern is $\\boldsymbol{\\mu}=(1.8,0.4,3.3)$ and the estimated covariance matrix is $\\Sigma=\\mathrm{diag}(0.04,0.01,0.09)$, reflecting an assumption that the three summary patterns are uncorrelated with heterogeneous variances across replicates.\n\nUnder the standard assumption in complex adaptive systems calibration that the sampling distribution of summary patterns around the model’s mean pattern is multivariate Gaussian with covariance $\\Sigma$, compute the Mahalanobis distance between $\\mathbf{s}_{e}$ and $\\boldsymbol{\\mu}$ using $\\Sigma$. Report the value rounded to four significant figures. No units are required.",
            "solution": "The objective is to compute the Mahalanobis distance, $D_M$, between an empirical pattern vector, $\\mathbf{s}_{e}$, and a model's mean pattern vector, $\\boldsymbol{\\mu}$, given the model's estimated covariance matrix, $\\Sigma$. The Mahalanobis distance is a measure of the distance between two points in a multivariate space, which accounts for the correlations and variances of the variables. The formula for the Mahalanobis distance between two vectors $\\mathbf{x}$ and $\\mathbf{y}$ with a covariance matrix $\\Sigma$ is:\n$$\nD_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{y})}\n$$\nwhere $T$ denotes the transpose of the vector.\n\nThe given data are:\nThe empirical pattern vector:\n$$\n\\mathbf{s}_{e} = \\begin{pmatrix} 2.0 \\\\ 0.5 \\\\ 3.1 \\end{pmatrix}\n$$\nThe estimated mean pattern vector from model replicates:\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} 1.8 \\\\ 0.4 \\\\ 3.3 \\end{pmatrix}\n$$\nThe estimated covariance matrix:\n$$\n\\Sigma = \\mathrm{diag}(0.04, 0.01, 0.09) = \\begin{pmatrix} 0.04 & 0 & 0 \\\\ 0 & 0.01 & 0 \\\\ 0 & 0 & 0.09 \\end{pmatrix}\n$$\n\nFirst, we compute the difference vector, $\\mathbf{d} = \\mathbf{s}_{e} - \\boldsymbol{\\mu}$:\n$$\n\\mathbf{d} = \\begin{pmatrix} 2.0 - 1.8 \\\\ 0.5 - 0.4 \\\\ 3.1 - 3.3 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ 0.1 \\\\ -0.2 \\end{pmatrix}\n$$\n\nNext, we need the inverse of the covariance matrix, $\\Sigma^{-1}$. Since $\\Sigma$ is a diagonal matrix, its inverse is found by taking the reciprocal of each element on the main diagonal:\n$$\n\\Sigma^{-1} = \\begin{pmatrix} \\frac{1}{0.04} & 0 & 0 \\\\ 0 & \\frac{1}{0.01} & 0 \\\\ 0 & 0 & \\frac{1}{0.09} \\end{pmatrix} = \\begin{pmatrix} 25 & 0 & 0 \\\\ 0 & 100 & 0 \\\\ 0 & 0 & \\frac{100}{9} \\end{pmatrix}\n$$\n\nNow we can compute the squared Mahalanobis distance, $D_M^2 = \\mathbf{d}^T \\Sigma^{-1} \\mathbf{d}$:\n$$\nD_M^2 = \\begin{pmatrix} 0.2 & 0.1 & -0.2 \\end{pmatrix} \\begin{pmatrix} 25 & 0 & 0 \\\\ 0 & 100 & 0 \\\\ 0 & 0 & \\frac{100}{9} \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ 0.1 \\\\ -0.2 \\end{pmatrix}\n$$\nBecause the covariance matrix is diagonal, this simplifies the calculation. The squared distance is the sum of the squared differences, with each term weighted by the inverse of the corresponding variance:\n$$\nD_M^2 = \\frac{(s_{e,1} - \\mu_1)^2}{\\sigma_1^2} + \\frac{(s_{e,2} - \\mu_2)^2}{\\sigma_2^2} + \\frac{(s_{e,3} - \\mu_3)^2}{\\sigma_3^2}\n$$\nwhere $\\sigma_i^2$ are the diagonal elements of $\\Sigma$.\nSubstituting the values:\n$$\nD_M^2 = \\frac{(0.2)^2}{0.04} + \\frac{(0.1)^2}{0.01} + \\frac{(-0.2)^2}{0.09}\n$$\n$$\nD_M^2 = \\frac{0.04}{0.04} + \\frac{0.01}{0.01} + \\frac{0.04}{0.09}\n$$\n$$\nD_M^2 = 1 + 1 + \\frac{4}{9} = 2 + \\frac{4}{9} = \\frac{18}{9} + \\frac{4}{9} = \\frac{22}{9}\n$$\nThe value of the squared Mahalanobis distance is approximately $D_M^2 \\approx 2.444...$\n\nFinally, the Mahalanobis distance $D_M$ is the square root of this value:\n$$\nD_M = \\sqrt{\\frac{22}{9}} = \\frac{\\sqrt{22}}{3}\n$$\nTo provide the numerical answer, we calculate the value:\n$$\nD_M \\approx \\frac{4.69041575...}{3} \\approx 1.56346858...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $1$, $5$, $6$, and $3$. The fifth significant digit is $4$, which is less than $5$, so we round down.\nThe final result is $1.563$.",
            "answer": "$$\\boxed{1.563}$$"
        }
    ]
}