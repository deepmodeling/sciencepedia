{
    "hands_on_practices": [
        {
            "introduction": "This first exercise focuses on the fundamental control parameter of the Minority Game, $\\alpha$. This dimensionless ratio connects the size of the strategy space to the number of agents, and its value determines whether the system is in an efficient phase or a maladaptive one. By calculating $\\alpha$ for a specific system, you will practice applying its definition and learn to place a given system within the game's phase diagram .",
            "id": "4115092",
            "problem": "Consider the standard formulation of the Minority Game, where $N$ agents each select between two actions, and their strategies are conditioned on the last $M$ binary outcomes, forming a binary history space. The canonical control parameter is defined as the ratio of the size of the binary history space to the population size. In the widely studied two-strategy-per-agent case, the system exhibits a well-characterized crossover (often described as a phase transition in the thermodynamic limit) at a canonical critical point $\\alpha_{c} = 0.3374$. Assume this canonical value for the critical point.\n\nFor $N = 301$ and $M = 7$, compute the control parameter $\\alpha$ from first principles, and determine whether the system is above or below the canonical critical point. Report your final answer as a row matrix $\\begin{pmatrix} \\alpha  \\chi \\end{pmatrix}$, where $\\chi$ is $1$ if $\\alpha$ is above the critical point and $0$ if $\\alpha$ is below it. Round $\\alpha$ to four significant figures.",
            "solution": "The problem requires the calculation of the control parameter $\\alpha$ for a Minority Game with a specified number of agents $N$ and memory length $M$. We must then compare this parameter to a given critical value $\\alpha_c$ to determine the state of the system.\n\nThe control parameter $\\alpha$ is defined as the ratio of the size of the strategy space to the number of agents. In the standard formulation of the Minority Game, the \"strategy space\" refers to the set of all possible histories that agents can observe. The history is a binary string of the last $M$ outcomes. Therefore, the number of distinct possible histories, which we can denote as $P$, is the number of binary sequences of length $M$. This is given by:\n$$P = 2^M$$\nThe control parameter $\\alpha$ is defined as the ratio of this quantity to the number of agents, $N$:\n$$\\alpha = \\frac{P}{N} = \\frac{2^M}{N}$$\nThis parameter quantifies the effective number of available strategies per agent.\n\nThe problem provides the following values:\n-   Number of agents, $N = 301$.\n-   Memory length, $M = 7$.\n-   Canonical critical point, $\\alpha_c = 0.3374$.\n\nFirst, we calculate the size of the history space, $P$, using the given value of $M$:\n$$P = 2^7 = 128$$\nNext, we substitute the values of $P$ and $N$ into the formula for $\\alpha$:\n$$\\alpha = \\frac{128}{301}$$\nPerforming the division gives the numerical value of $\\alpha$:\n$$\\alpha \\approx 0.4252491694...$$\nThe problem requires this value to be rounded to four significant figures. The first four significant figures are $4$, $2$, $5$, and $2$. The fifth digit is $4$, which is less than $5$, so we round down.\n$$\\alpha \\approx 0.4252$$\nNow, we must determine if the system is above or below the canonical critical point $\\alpha_c = 0.3374$. We compare our calculated value of $\\alpha$ to $\\alpha_c$:\n$$0.4252 > 0.3374$$\nSince $\\alpha > \\alpha_c$, the system is in the phase above the critical point. This is the efficient, information-rich phase where agent strategies are diverse. This diversity prevents large-scale herding, so fluctuations in attendance are close to what would be expected from random chance.\n\nThe problem defines a variable $\\chi$ such that $\\chi = 1$ if $\\alpha$ is above the critical point and $\\chi = 0$ if it is below. Based on our comparison, we have:\n$$\\chi = 1$$\nThe final answer is required in the form of a row matrix $\\begin{pmatrix} \\alpha  \\chi \\end{pmatrix}$. Substituting the values we have determined:\n$$\\begin{pmatrix} 0.4252  1 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.4252  1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "To appreciate the emergence of collective intelligence, we must first understand the absence of it. This practice asks you to derive the expected outcomes when agents act not strategically, but completely at random. Establishing this \"null model\" provides a crucial baseline for volatility, allowing us to later quantify how much more or less efficient a system becomes when agents use adaptive strategies .",
            "id": "4115109",
            "problem": "Consider an artificial market stylized as the Minority Game (MG), a canonical model in Complex Adaptive Systems (CAS) where $N \\in \\mathbb{N}$ agents repeatedly make binary decisions $a_{i}(t) \\in \\{-1,+1\\}$ at each discrete time $t \\in \\mathbb{Z}_{\\ge 0}$. Under the null benchmark of random independent actions, assume $a_{i}(t)$ are independent across agents and time with $P(a_{i}(t)=+1)=P(a_{i}(t)=-1)=\\frac{1}{2}$. Define the aggregate excess demand as $A(t)=\\sum_{i=1}^{N} a_{i}(t)$. To capture marginal strategic impact as commonly analyzed in artificial market efficiency baselines, define the instantaneous marginal payoff (excluding self-impact) for agent $i$ as $u_{i}(t)=-a_{i}(t) A_{-i}(t)$, where $A_{-i}(t)=\\sum_{j \\neq i} a_{j}(t)$. Define the per-step crowding loss as $L(t)=A(t)^{2}$, which measures allocative inefficiency due to crowding.\n\nStarting from the fundamental properties of independent, identically distributed binary random variables and standard results on expectations and variances, derive the exact expressions for the expected marginal payoff $\\mathbb{E}[u_{i}(t)]$ and the expected crowding loss $\\mathbb{E}[A(t)^{2}]$ under the stated random independent benchmark. Express your final answer as a two-entry row matrix containing $\\mathbb{E}[u_{i}(t)]$ and $\\mathbb{E}[A(t)^{2}]$. No rounding is required. No units are involved.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It presents a standard calculation for a benchmark case in the well-established Minority Game model. We may proceed with the derivation.\n\nThe task is to compute the expected values of the marginal payoff for an agent $i$, $\\mathbb{E}[u_{i}(t)]$, and the crowding loss, $\\mathbb{E}[A(t)^{2}]$, under the specified random benchmark. The benchmark assumes that each of the $N$ agents makes a binary decision $a_{i}(t) \\in \\{-1,+1\\}$ independently and with equal probability.\n\nFirst, let us establish the fundamental statistical properties of a single agent's action, $a_{i}(t)$. The probability distribution is given by $P(a_{i}(t)=+1) = \\frac{1}{2}$ and $P(a_{i}(t)=-1) = \\frac{1}{2}$. The actions $a_{i}(t)$ are independent and identically distributed (i.i.d.) for all agents $i$ and time steps $t$.\n\nThe expected value of a single agent's action is:\n$$\n\\mathbb{E}[a_{i}(t)] = (+1) \\cdot P(a_{i}(t)=+1) + (-1) \\cdot P(a_{i}(t)=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0\n$$\nNext, let's find the variance of a single agent's action, $\\text{Var}(a_{i}(t))$. We first need the expectation of the square of the action, $\\mathbb{E}[a_{i}(t)^{2}]$. Since $a_{i}(t)$ only takes values $-1$ and $+1$, $a_{i}(t)^{2}$ is always $1$. Therefore:\n$$\n\\mathbb{E}[a_{i}(t)^{2}] = (+1)^{2} \\cdot P(a_{i}(t)=+1) + (-1)^{2} \\cdot P(a_{i}(t)=-1) = 1 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = 1\n$$\nThe variance is defined as $\\text{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$. Thus,\n$$\n\\text{Var}(a_{i}(t)) = \\mathbb{E}[a_{i}(t)^{2}] - (\\mathbb{E}[a_{i}(t)])^{2} = 1 - 0^{2} = 1\n$$\n\nWith these properties established, we can now compute the two required quantities.\n\n1.  **Expected Marginal Payoff $\\mathbb{E}[u_{i}(t)]$**\n\nThe marginal payoff for agent $i$ is defined as $u_{i}(t) = -a_{i}(t) A_{-i}(t)$, where $A_{-i}(t) = \\sum_{j \\neq i} a_{j}(t)$. Using the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}[u_{i}(t)] = \\mathbb{E}[-a_{i}(t) A_{-i}(t)] = -\\mathbb{E}[a_{i}(t) A_{-i}(t)]\n$$\nThe action of agent $i$, $a_{i}(t)$, is independent of the actions of all other agents. Consequently, $a_{i}(t)$ is independent of the sum of the other agents' actions, $A_{-i}(t)$. For two independent random variables $X$ and $Y$, the expectation of their product is the product of their expectations, $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$. Applying this property:\n$$\n\\mathbb{E}[a_{i}(t) A_{-i}(t)] = \\mathbb{E}[a_{i}(t)] \\mathbb{E}[A_{-i}(t)]\n$$\nWe have already shown that $\\mathbb{E}[a_{i}(t)] = 0$. Although not strictly necessary at this point, we can also compute $\\mathbb{E}[A_{-i}(t)]$:\n$$\n\\mathbb{E}[A_{-i}(t)] = \\mathbb{E}\\left[\\sum_{j \\neq i} a_{j}(t)\\right] = \\sum_{j \\neq i} \\mathbb{E}[a_{j}(t)] = \\sum_{j \\neq i} 0 = 0\n$$\nThe sum consists of $N-1$ terms. Substituting back, we find:\n$$\n\\mathbb{E}[u_{i}(t)] = -\\mathbb{E}[a_{i}(t)] \\mathbb{E}[A_{-i}(t)] = -(0) \\cdot (0) = 0\n$$\nThus, the expected marginal payoff for any agent under the random benchmark is $0$.\n\n2.  **Expected Crowding Loss $\\mathbb{E}[A(t)^{2}]$**\n\nThe crowding loss is $L(t) = A(t)^{2}$, where the aggregate excess demand is $A(t) = \\sum_{i=1}^{N} a_{i}(t)$. We need to find $\\mathbb{E}[A(t)^{2}]$.\nWe recognize this quantity in the definition of the variance of $A(t)$:\n$$\n\\text{Var}(A(t)) = \\mathbb{E}[A(t)^{2}] - (\\mathbb{E}[A(t)])^{2}\n$$\nLet's first compute the expected value of $A(t)$:\n$$\n\\mathbb{E}[A(t)] = \\mathbb{E}\\left[\\sum_{i=1}^{N} a_{i}(t)\\right] = \\sum_{i=1}^{N} \\mathbb{E}[a_{i}(t)] = \\sum_{i=1}^{N} 0 = 0\n$$\nSubstituting this into the variance formula, we get:\n$$\n\\text{Var}(A(t)) = \\mathbb{E}[A(t)^{2}] - 0^{2} \\implies \\mathbb{E}[A(t)^{2}] = \\text{Var}(A(t))\n$$\nSo, the problem reduces to finding the variance of the aggregate demand, $A(t)$. The variable $A(t)$ is a sum of $N$ i.i.d. random variables $a_i(t)$. For a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(A(t)) = \\text{Var}\\left(\\sum_{i=1}^{N} a_{i}(t)\\right) = \\sum_{i=1}^{N} \\text{Var}(a_{i}(t))\n$$\nWe have already calculated that $\\text{Var}(a_{i}(t)) = 1$ for any agent $i$. Therefore:\n$$\n\\text{Var}(A(t)) = \\sum_{i=1}^{N} 1 = N\n$$\nSince $\\mathbb{E}[A(t)^{2}] = \\text{Var}(A(t))$, we conclude that:\n$$\n\\mathbb{E}[A(t)^{2}] = N\n$$\nThis result signifies that the expected squared deviation from a perfect balance (i.e., the expected crowding) scales linearly with the number of agents in the system when they act randomly.\n\nThe two derived quantities are $\\mathbb{E}[u_{i}(t)] = 0$ and $\\mathbb{E}[A(t)^{2}] = N$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  N \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The sign of an interaction can change everything. This exercise deepens your understanding of the Minority Game's anti-coordination incentive by contrasting it with its conceptual opposite, the Majority Game, where herding is rewarded. By analyzing the different Nash equilibria and emergent dynamics, you will see precisely how the microscopic payoff rule creates macroscopic patterns of either efficiency or extreme predictability and crowding .",
            "id": "4115076",
            "problem": "Consider a population of $N$ agents indexed by $i\\in\\{1,\\dots,N\\}$ who, at each discrete time $t$, choose a binary action $a_i(t)\\in\\{-1,+1\\}$. The aggregate action is $A(t)=\\sum_{i=1}^N a_i(t)$. Each agent’s one-period payoff in the Majority Game is $u_i^{\\mathrm{Maj}}(t)=a_i(t)A(t)$, while in the Minority Game (MG) it is $u_i^{\\mathrm{Min}}(t)=-a_i(t)A(t)$. Agents may condition their actions on publicly observed information states $\\mu(t)$, but actions are chosen simultaneously. Assume $N$ is odd and that, when needed, randomization is independent across agents.\n\nStart from the definitions of $A(t)$ and the individual payoffs given above, and from the standard definitions of Nash equilibrium and variance, and reason from first principles. In particular, use only the following fundamental bases: the definition of aggregate action as a sum of individual actions, the definition of a best response in Nash equilibrium, and the decomposition of variance of a sum into variances and covariances. Do not assume any particular learning rule beyond best-response intuition unless explicitly stated.\n\nWhich of the following statements are correct about the Majority Game and how its coordination incentives reverse outcomes relative to the Minority Game?\n\nA. In the one-shot Majority Game with $a_i(t)\\in\\{-1,+1\\}$ and $u_i^{\\mathrm{Maj}}(t)=a_i(t)A(t)$, the action profiles with $a_1=\\cdots=a_N=+1$ and $a_1=\\cdots=a_N=-1$ are Nash equilibria, because any unilateral deviation strictly decreases the deviator’s payoff.\n\nB. If repeated play induces strong positive correlation among actions, then $\\operatorname{Var}[A(t)]$ scales as $\\mathcal{O}(N^2)$ in the Majority Game, whereas in the Minority Game it is at most of order $\\mathcal{O}(N)$ under comparable information and memory, reflecting crowding versus anti-crowding.\n\nC. Just as in the Minority Game, adaptive behavior in the Majority Game generically drives the system toward an information-efficient state with vanishing predictability, in the sense that the conditional mean $\\mathbb{E}[A(t)\\mid \\mu(t)]$ tends to $0$ for all information states $\\mu(t)$.\n\nD. The sign flip between $u_i^{\\mathrm{Maj}}(t)=a_i(t)A(t)$ and $u_i^{\\mathrm{Min}}(t)=-a_i(t)A(t)$ implies that the sum of payoffs is $+\\!A(t)^2$ in the Majority Game and $-\\!A(t)^2$ in the Minority Game; therefore, coordination is rewarded in the former and penalized in the latter, reversing crowding versus anti-crowding incentives.\n\nE. The symmetric mixed action profile where each agent independently chooses $+1$ with probability $1/2$ is the unique equilibrium of the one-shot Majority Game for odd $N$.\n\nSelect all that apply.",
            "solution": "We are tasked with evaluating five statements regarding the properties of the Majority Game and its contrast with the Minority Game. The analysis will be conducted from first principles as stipulated. The setup involves $N$ agents ($N$ is odd), each choosing an action $a_i(t) \\in \\{-1, +1\\}$. The aggregate action is $A(t) = \\sum_{i=1}^N a_i(t)$. The payoffs are $u_i^{\\mathrm{Maj}}(t) = a_i(t)A(t)$ in the Majority Game and $u_i^{\\mathrm{Min}}(t) = -a_i(t)A(t)$ in the Minority Game.\n\n**Analysis of Option A**\nThis statement proposes that the fully coordinated action profiles are Nash equilibria (NE) in the one-shot Majority Game. A profile is a NE if no agent has an incentive for a unilateral deviation.\n\nCase 1: All agents choose $a_i = +1$.\nThe aggregate action is $A = \\sum_{i=1}^N (+1) = N$. The payoff for any agent $i$ is $u_i = a_i A = (+1)(N) = N$.\nConsider a unilateral deviation by agent $k$, who switches to $a_k' = -1$. All other $N-1$ agents maintain $a_j = +1$. The new aggregate action becomes $A' = (-1) + (N-1)(+1) = N-2$. The payoff for the deviator $k$ is now $u_k' = a_k' A' = (-1)(N-2) = 2-N$.\nA deviation is not profitable if $u_k \\ge u_k'$, which means $N \\ge 2-N$, or $2N \\ge 2$, which is true for $N \\ge 1$. The statement claims a strict decrease in payoff, which requires $N > 2-N$, or $N > 1$. Since $N$ is odd, the smallest non-trivial population is $N=3$, so this condition holds. The payoff strictly decreases from $N$ to $2-N$. Thus, this profile is a strict NE.\n\nCase 2: All agents choose $a_i = -1$.\nThe aggregate action is $A = \\sum_{i=1}^N (-1) = -N$. The payoff for any agent $i$ is $u_i = a_i A = (-1)(-N) = N$.\nConsider a unilateral deviation by agent $k$, who switches to $a_k' = +1$. The new aggregate action is $A' = (+1) + (N-1)(-1) = 1 - (N - 1) = 2-N$. The deviator's payoff is $u_k' = a_k' A' = (+1)(2-N) = 2-N$.\nAs before, for $N > 1$, we have $N > 2-N$, so the deviation strictly reduces the agent's payoff. This profile is also a strict NE.\n\nThe statement and its reasoning are correct. Both fully coordinated states are strict Nash equilibria.\nVerdict: **Correct**.\n\n**Analysis of Option B**\nThis statement concerns the scaling of the variance of the aggregate action, $\\operatorname{Var}[A(t)]$. From first principles, the variance of a sum of variables is:\n$$ \\operatorname{Var}[A(t)] = \\operatorname{Var}\\left[\\sum_{i=1}^N a_i(t)\\right] = \\sum_{i=1}^N \\operatorname{Var}[a_i(t)] + 2\\sum_{1 \\le i  j \\le N} \\operatorname{Cov}[a_i(t), a_j(t)] $$\nThe term $\\operatorname{Var}[a_i(t)]$ is of order $\\mathcal{O}(1)$. The sum of variances is thus of order $\\mathcal{O}(N)$. The dominant term for large $N$ is the sum of covariances, as there are $\\binom{N}{2} \\sim \\mathcal{O}(N^2)$ such terms.\n\nIn the Majority Game, the payoff $u_i^{\\mathrm{Maj}} = a_i A$ rewards agents who align their action with the aggregate. This \"crowding\" incentive fosters positive feedback. In repeated play, best-response dynamics will lead agents to correlate their actions positively, aiming to be in the majority. If strong positive correlation is achieved, the average covariance $\\operatorname{Cov}[a_i, a_j]$ will be a positive constant for $i \\neq j$. The sum of covariances will then scale as $\\mathcal{O}(N^2)$, making $\\operatorname{Var}[A(t)]$ scale as $\\mathcal{O}(N^2)$.\n\nIn the Minority Game, the payoff $u_i^{\\mathrm{Min}} = -a_i A$ rewards agents who misalign their action with the aggregate. This \"anti-crowding\" incentive creates negative feedback. Agents are punished for being in the majority, which actively suppresses positive correlations. If their actions were uncorrelated, $\\operatorname{Cov}[a_i, a_j]=0$, and $\\operatorname{Var}[A(t)]$ would scale as $\\mathcal{O}(N)$. If they achieve anti-correlation ($\\operatorname{Cov}[a_i, a_j]0$), the variance could be even smaller. Thus, the incentive structure ensures that the variance will not scale faster than $\\mathcal{O}(N)$. The statement that the variance is \"at most of order $\\mathcal{O}(N)$\" is a correct characterization of the consequences of the anti-crowding incentive, in sharp contrast to the Majority Game.\nVerdict: **Correct**.\n\n**Analysis of Option C**\nThis statement posits that adaptive behavior in the Majority Game drives the system to an \"information-efficient\" state where $\\mathbb{E}[A(t)\\mid \\mu(t)] \\to 0$, similar to the Minority Game.\n\nIn the Minority Game, such a state emerges due to negative feedback. If any information state $\\mu(t)$ allows for a predictable non-zero expectation of $A(t)$, for instance $\\mathbb{E}[A(t)\\mid \\mu(t)] > 0$, agents can exploit this by choosing the expected minority action (here, $-1$). This collective response would reduce $A(t)$, thereby closing the prediction-profit loop and pushing $\\mathbb{E}[A(t)\\mid \\mu(t)]$ towards $0$.\n\nIn the Majority Game, the feedback is positive. If agents predict $\\mathbb{E}[A(t)\\mid \\mu(t)] > 0$, their best response is to choose $a_i=+1$ to maximize the chance of a positive payoff $u_i=a_i A$. This collective response reinforces the initial prediction, making $A(t)$ even more positive. This dynamic amplifies any nascent predictability, leading to convention formation or herding, where $|\\mathbb{E}[A(t)\\mid \\mu(t)]|$ becomes large. The system becomes highly predictable, which is the opposite of the outcome described. The statement incorrectly generalizes a property of the Minority Game to the Majority Game, where the reversed incentives lead to opposite emergent behavior.\nVerdict: **Incorrect**.\n\n**Analysis of Option D**\nThis statement calculates the sum of payoffs for each game and interprets the result.\n\nFor the Majority Game, the sum of all agents' payoffs is:\n$$ \\sum_{i=1}^N u_i^{\\mathrm{Maj}}(t) = \\sum_{i=1}^N a_i(t)A(t) $$\nSince $A(t)$ is common to all agents at time $t$, we can factor it out:\n$$ A(t) \\sum_{i=1}^N a_i(t) = A(t) \\cdot A(t) = A(t)^2 $$\nFor the Minority Game, the calculation is analogous:\n$$ \\sum_{i=1}^N u_i^{\\mathrm{Min}}(t) = \\sum_{i=1}^N -a_i(t)A(t) = -A(t) \\sum_{i=1}^N a_i(t) = -A(t) \\cdot A(t) = -A(t)^2 $$\nThese calculations are correct. The interpretation is that social welfare (the sum of payoffs) is maximized in the Majority Game by maximizing coordination ($|A(t)| \\to N$), and in the Minority Game by minimizing coordination ($|A(t)| \\to 1$). This correctly captures how the sign flip in the payoff function reverses the system-level incentives from rewarding coordination (\"crowding\") to penalizing it (\"anti-crowding\").\nVerdict: **Correct**.\n\n**Analysis of Option E**\nThis statement asserts that the symmetric mixed strategy profile (each agent chooses $+1$ or $-1$ with probability $1/2$) is the *unique* equilibrium of the one-shot Majority Game.\n\nFirst, let's verify if it is an equilibrium. An agent must be indifferent between their pure strategies given that all others play the mixed strategy. The expected payoff for agent $i$ choosing $a_i=+1$ is $\\mathbb{E}[u_i | a_i=+1] = \\mathbb{E}[1 \\cdot (1 + \\sum_{j \\neq i} a_j)] = 1 + \\sum_{j \\neq i} \\mathbb{E}[a_j]$. Since $\\mathbb{E}[a_j] = (1/2)(+1) + (1/2)(-1) = 0$ for $j \\neq i$, the expected payoff is $1+0=1$. The expected payoff for choosing $a_i=-1$ is $\\mathbb{E}[u_i | a_i=-1] = \\mathbb{E}[-1 \\cdot (-1 + \\sum_{j \\neq i} a_j)] = 1 - \\sum_{j \\neq i} \\mathbb{E}[a_j] = 1-0=1$. As the expected payoffs are equal, agents are indifferent, and the profile is a Nash equilibrium.\n\nHowever, the claim is that this equilibrium is *unique*. Our analysis for Option A demonstrated the existence of two pure-strategy Nash equilibria: all agents playing $+1$ and all agents playing $-1$. Since there are at least three distinct Nash equilibria, the claim of uniqueness is false.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}