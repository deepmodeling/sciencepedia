{
    "hands_on_practices": [
        {
            "introduction": "这项首个练习旨在探索“群体智慧”的数学基础。我们将从第一性原理出发，推导简单平均如何有效降低集体估计的误差，并揭示一个关键因素：个体之间的相关性如何限制这一优势，从而凸显了观点多样性的重要性。通过这个计算 ，你将亲手量化群体规模和相关性对集体精确度的影响，这是理解聚合信息价值的核心。",
            "id": "4128693",
            "problem": "一个由 $n$ 个代理人组成的小组协同估计一个固定的标量 $\\theta$，每个代理人报告其个人估计值 $Y_{i} = \\theta + \\varepsilon_{i}$，其中 $i \\in \\{1,\\dots,n\\}$。误差 $\\varepsilon_{i}$ 是无偏的，即 $ \\mathbb{E}[\\varepsilon_{i}] = 0$，并且是可交换的，具有共同方差 $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^{2}$ 和对于所有 $i \\neq j$ 的恒定成对相关性 $\\operatorname{Corr}(\\varepsilon_{i},\\varepsilon_{j}) = \\rho$。该小组将简单平均值 $ \\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i} $ 作为其集体估计。\n\n从方差和协方差的核心定义出发，不使用任何快捷公式，推导 $\\operatorname{Var}(\\bar{Y})$ 的一个闭式表达式，该表达式以 $n$、$\\sigma^{2}$ 和 $\\rho$ 表示。然后，当 $n=25$，$\\sigma^{2}=1$ 且 $\\rho=0.2$ 时，计算该表达式的值。最后，计算组平均方差与单个代理人方差之比，即 $\\operatorname{Var}(\\bar{Y}) / \\sigma^{2}$。\n\n请以精确值的形式给出数对 $\\big(\\operatorname{Var}(\\bar{Y}),\\ \\operatorname{Var}(\\bar{Y}) / \\sigma^{2}\\big)$ 的最终答案。不要四舍五入。您的最终答案必须使用 $\\text{pmatrix}$ 环境以行矩阵的形式给出。",
            "solution": "该问题经评估为有效。它在科学上基于标准概率论，提法明确，包含所有必要信息，并且陈述客观。提供的数值（$n=25$, $\\sigma^2=1$, $\\rho=0.2$）是一致的；具体来说，相关性 $\\rho$ 位于一个 $n$ 维可交换相关矩阵的有效范围内，即 $\\rho \\in [-\\frac{1}{n-1}, 1]$。对于 $n=25$，这个范围是 $[-\\frac{1}{24}, 1]$，而 $\\rho=0.2$ 完全在此范围内。\n\n任务是推导组平均方差 $\\operatorname{Var}(\\bar{Y})$ 的表达式，然后对其进行求值。组平均定义为 $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}$。\n\n首先，我们用真实量 $\\theta$ 和误差项 $\\varepsilon_i$ 来表示 $\\bar{Y}$。\n$$\n\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} (\\theta + \\varepsilon_{i}) = \\frac{1}{n} \\left( \\sum_{i=1}^{n} \\theta + \\sum_{i=1}^{n} \\varepsilon_{i} \\right) = \\frac{1}{n} \\left( n\\theta + \\sum_{i=1}^{n} \\varepsilon_{i} \\right) = \\theta + \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\n$$\n随机变量加上一个常数后的方差等于该随机变量本身的方差。因此，\n$$\n\\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\theta + \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\\right) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\\right)\n$$\n使用性质 $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$，其中 $a = \\frac{1}{n}$ 是一个常数，我们得到：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)\n$$\n问题要求从核心定义出发推导和的方差。令 $S = \\sum_{i=1}^{n} \\varepsilon_{i}$。$S$ 的方差定义为 $\\operatorname{Var}(S) = \\mathbb{E}[(S - \\mathbb{E}[S])^2]$。\n首先，我们求 $S$ 的期望：\n$$\n\\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{i=1}^{n} \\varepsilon_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[\\varepsilon_{i}]\n$$\n鉴于对于所有 $i$ 都有 $\\mathbb{E}[\\varepsilon_{i}] = 0$，我们得到 $\\mathbb{E}[S] = 0$。\n$S$ 的方差简化为：\n$$\n\\operatorname{Var}(S) = \\mathbb{E}[(S - 0)^2] = \\mathbb{E}[S^2]\n$$\n我们现在展开 $S^2$：\n$$\nS^2 = \\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)^2 = \\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)\\left(\\sum_{j=1}^{n} \\varepsilon_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\varepsilon_{i}\\varepsilon_{j}\n$$\n我们可以将这个双重求和分解为索引相等 ($i=j$) 的项和索引不相等 ($i \\neq j$) 的项：\n$$\nS^2 = \\sum_{i=1}^{n} \\varepsilon_{i}^2 + \\sum_{i \\neq j} \\varepsilon_{i}\\varepsilon_{j}\n$$\n现在，我们对 $S^2$ 取期望：\n$$\n\\mathbb{E}[S^2] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} \\varepsilon_{i}^2 + \\sum_{i \\neq j} \\varepsilon_{i}\\varepsilon_{j} \\right] = \\sum_{i=1}^{n} \\mathbb{E}[\\varepsilon_{i}^2] + \\sum_{i \\neq j} \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}]\n$$\n我们将这些期望项与给定的方差和相关性联系起来。\n对于任何随机变量 $X$，$\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。由于 $\\mathbb{E}[\\varepsilon_{i}] = 0$ 且 $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^2$，我们得到：\n$$\n\\sigma^2 = \\mathbb{E}[\\varepsilon_{i}^2] - 0^2 \\implies \\mathbb{E}[\\varepsilon_{i}^2] = \\sigma^2\n$$\n对于任意两个随机变量 $X$ 和 $Y$，$\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$。对于 $i \\neq j$ 的情况，由于 $\\mathbb{E}[\\varepsilon_{i}] = \\mathbb{E}[\\varepsilon_{j}] = 0$，我们得到：\n$$\n\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j}) = \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}] - 0 \\cdot 0 \\implies \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}] = \\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j})\n$$\n相关性定义为 $\\operatorname{Corr}(\\varepsilon_{i}, \\varepsilon_{j}) = \\frac{\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j})}{\\sqrt{\\operatorname{Var}(\\varepsilon_{i})\\operatorname{Var}(\\varepsilon_{j})}}$。根据给定的值 $\\operatorname{Corr}(\\varepsilon_{i}, \\varepsilon_{j}) = \\rho$ 和 $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^2$，我们求出 $i \\neq j$ 时的协方差：\n$$\n\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j}) = \\rho \\sqrt{\\sigma^2 \\cdot \\sigma^2} = \\rho \\sigma^2\n$$\n现在我们将这些结果代回到 $\\mathbb{E}[S^2]$ 的表达式中：\n$$\n\n\\operatorname{Var}(S) = \\mathbb{E}[S^2] = \\sum_{i=1}^{n} \\sigma^2 + \\sum_{i \\neq j} \\rho\\sigma^2\n$$\n第一个和式有 $n$ 个相同的项，所以其值为 $n\\sigma^2$。第二个和式是对所有不同的索引对 $(i,j)$ 进行求和。这样的索引对有 $n(n-1)$ 个。因此，第二个和式的值为 $n(n-1)\\rho\\sigma^2$。\n$$\n\\operatorname{Var}(S) = n\\sigma^2 + n(n-1)\\rho\\sigma^2\n$$\n最后，我们将此结果代回到我们关于 $\\operatorname{Var}(\\bar{Y})$ 的表达式中：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n^2} \\operatorname{Var}(S) = \\frac{1}{n^2} [n\\sigma^2 + n(n-1)\\rho\\sigma^2]\n$$\n从方括号项中提取 $n\\sigma^2$ 得到闭式表达式：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{n\\sigma^2}{n^2} [1 + (n-1)\\rho] = \\frac{\\sigma^2}{n}[1 + (n-1)\\rho]\n$$\n推导完成。\n\n接下来，我们对 $n=25$、$\\sigma^2=1$ 和 $\\rho=0.2$ 的情况计算该表达式的值。\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{25}[1 + (25-1)(0.2)] = \\frac{1}{25}[1 + (24)(0.2)] = \\frac{1}{25}[1 + 4.8] = \\frac{5.8}{25}\n$$\n为了提供一个精确值，我们将小数转换为分数：\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{5.8}{25} = \\frac{58}{250} = \\frac{29}{125}\n$$\n最后，我们计算组平均方差与单个代理人方差之比，$\\operatorname{Var}(\\bar{Y}) / \\sigma^2$。\n使用推导出的闭式表达式：\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{1}{\\sigma^2} \\left( \\frac{\\sigma^2}{n}[1 + (n-1)\\rho] \\right) = \\frac{1}{n}[1 + (n-1)\\rho]\n$$\n使用给定的值：\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{1}{25}[1 + (24)(0.2)] = \\frac{5.8}{25} = \\frac{29}{125}\n$$\n或者，使用先前计算出的 $\\operatorname{Var}(\\bar{Y})$ 值和给定的 $\\sigma^2=1$：\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{29/125}{1} = \\frac{29}{125}\n$$\n这对答案是 $\\left(\\operatorname{Var}(\\bar{Y}), \\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2}\\right) = \\left(\\frac{29}{125}, \\frac{29}{125}\\right)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{29}{125}  \\frac{29}{125} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "简单平均虽然直观，但它隐含地假设所有信息来源的质量相同。当我们可以获取关于个体误差结构（即协方差）的更多信息时，我们能够设计出更优的聚合策略。本练习  将指导你运用广义最小二乘法 (Generalized Least Squares, GLS) 的原理，为一组相关的估计推导出方差最小的最优权重，从而超越简单平均，实现更精确的集体判断。",
            "id": "4128739",
            "problem": "在一个集体智能情境中，一个协调者希望聚合2个代理关于一个标量潜状态 $\\theta$ 的估计值。令观测向量为 $\\mathbf{y} = \\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{2}$ 是全1向量，$\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$，且 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\Sigma$。协调者使用一个线性无偏聚合器 $\\hat{\\theta} = \\mathbf{w}^{\\top} \\mathbf{y}$，并带有无偏性约束 $\\mathbf{1}^{\\top}\\mathbf{w} = 1$。仅使用无偏性、线性形式的协方差和一阶最优性条件的定义，从第一性原理推导使估计量方差 $\\operatorname{Var}(\\hat{\\theta})$ 最小化的广义最小二乘法 (GLS) 权重（广义最小二乘法 (GLS) 是在已知误差协方差下具有最小方差的线性无偏估计量），然后计算所得方差。\n\n给定协方差矩阵\n$$\n\\Sigma = \\begin{pmatrix} 1  0.5 \\\\ 0.5  1 \\end{pmatrix}.\n$$\n完成以下任务：\n- 在无偏性约束下推导最优的 $\\mathbf{w}$。\n- 计算最优估计量的方差。\n- 计算朴素等权重估计量 $\\mathbf{w}_{\\mathrm{eq}} = \\begin{pmatrix}1/2\\\\1/2\\end{pmatrix}$ 的方差。\n提供精确值（不要四舍五入）。将你的最终答案表示为有序四元组 $(w_1, w_2, V_{\\mathrm{GLS}}, V_{\\mathrm{eq}})$。",
            "solution": "该问题要求从向量观测 $\\mathbf{y}$ 中推导标量参数 $\\theta$ 的线性无偏估计量的最优权重，并计算其相关方差，以及一个简单等权重估计量的方差。\n\n模型由 $\\mathbf{y} = \\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\mathbf{y} \\in \\mathbb{R}^{2}$，$\\mathbf{1}$ 是全1向量，$\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$，且 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\Sigma$。估计量是一个线性聚合器 $\\hat{\\theta} = \\mathbf{w}^{\\top} \\mathbf{y}$。\n\n首先，我们建立无偏性条件。估计量的期望为：\n$$\n\\mathbb{E}[\\hat{\\theta}] = \\mathbb{E}[\\mathbf{w}^{\\top} \\mathbf{y}] = \\mathbf{w}^{\\top} \\mathbb{E}[\\mathbf{y}]\n$$\n观测向量 $\\mathbf{y}$ 的期望为：\n$$\n\\mathbb{E}[\\mathbf{y}] = \\mathbb{E}[\\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}] = \\theta \\mathbf{1} + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\theta \\mathbf{1}\n$$\n将此代入 $\\mathbb{E}[\\hat{\\theta}]$ 的表达式中：\n$$\n\\mathbb{E}[\\hat{\\theta}] = \\mathbf{w}^{\\top} (\\theta \\mathbf{1}) = \\theta (\\mathbf{w}^{\\top} \\mathbf{1})\n$$\n为使估计量无偏，对于任意 $\\theta$ 值，我们必须有 $\\mathbb{E}[\\hat{\\theta}] = \\theta$。这要求乘以 $\\theta$ 的项为1：\n$$\n\\mathbf{w}^{\\top} \\mathbf{1} = 1\n$$\n这就是无偏性约束，对于 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ 可以写成 $w_1 + w_2 = 1$。\n\n接下来，我们求估计量的方差，这是需要最小化的量。\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}(\\mathbf{w}^{\\top} \\mathbf{y}) = \\operatorname{Var}(\\mathbf{w}^{\\top} (\\theta \\mathbf{1} + \\boldsymbol{\\varepsilon}))\n$$\n由于 $\\theta$ 是一个非随机（潜）状态，它对方法差没有贡献。因此：\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}(\\mathbf{w}^{\\top} \\boldsymbol{\\varepsilon}) = \\mathbf{w}^{\\top} \\operatorname{Cov}(\\boldsymbol{\\varepsilon}) \\mathbf{w} = \\mathbf{w}^{\\top} \\Sigma \\mathbf{w}\n$$\n问题是在无偏性约束下最小化该方差。这是一个约束优化问题：最小化 $f(\\mathbf{w}) = \\mathbf{w}^{\\top} \\Sigma \\mathbf{w}$，约束条件为 $g(\\mathbf{w}) = \\mathbf{w}^{\\top} \\mathbf{1} - 1 = 0$。\n\n我们使用拉格朗日乘子法。拉格朗日函数 $\\mathcal{L}$ 为：\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top} \\Sigma \\mathbf{w} - \\lambda (\\mathbf{w}^{\\top} \\mathbf{1} - 1)\n$$\n为了找到最优权重 $\\mathbf{w}$，我们求 $\\mathcal{L}$ 关于 $\\mathbf{w}$ 的梯度并将其设为零（一阶最优性条件）。由于 $\\Sigma$ 是对称矩阵，二次型 $\\mathbf{w}^{\\top} \\Sigma \\mathbf{w}$ 关于 $\\mathbf{w}$ 的梯度是 $2 \\Sigma \\mathbf{w}$。\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L} = 2 \\Sigma \\mathbf{w} - \\lambda \\mathbf{1} = \\mathbf{0}\n$$\n这得到 $2 \\Sigma \\mathbf{w} = \\lambda \\mathbf{1}$。假设 $\\Sigma$ 是可逆的，我们可以解出 $\\mathbf{w}$：\n$$\n\\mathbf{w} = \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\n$$\n为了找到拉格朗日乘子 $\\lambda$，我们将这个 $\\mathbf{w}$ 的表达式代回到约束 $\\mathbf{w}^{\\top} \\mathbf{1} = 1$ 中：\n$$\n\\left(\\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\\right)^{\\top} \\mathbf{1} = 1\n$$\n$$\n\\frac{\\lambda}{2} \\mathbf{1}^{\\top} (\\Sigma^{-1})^{\\top} \\mathbf{1} = 1\n$$\n由于 $\\Sigma$ 是协方差矩阵，它是对称的，所以它的逆矩阵也是对称的，即 $(\\Sigma^{-1})^{\\top} = \\Sigma^{-1}$。\n$$\n\\frac{\\lambda}{2} \\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1} = 1 \\implies \\lambda = \\frac{2}{\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1}}\n$$\n将这个 $\\lambda$ 代回到 $\\mathbf{w}$ 的表达式中，得到最优权重向量，记为 $\\mathbf{w}_{\\mathrm{GLS}}$：\n$$\n\\mathbf{w}_{\\mathrm{GLS}} = \\frac{1}{2} \\left( \\frac{2}{\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1}} \\right) \\Sigma^{-1} \\mathbf{1} = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1}}\n$$\n这就完成了从第一性原理推导最优权重的过程。\n\n现在我们计算具体的值。给定的协方差矩阵是：\n$$\n\\Sigma = \\begin{pmatrix} 1  0.5 \\\\ 0.5  1 \\end{pmatrix} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}\n$$\n首先，我们求它的逆矩阵 $\\Sigma^{-1}$。行列式为 $\\det(\\Sigma) = (1)(1) - (1/2)(1/2) = 1 - 1/4 = 3/4$。\n$$\n\\Sigma^{-1} = \\frac{1}{3/4} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix}\n$$\n接下来，我们计算 $\\mathbf{w}_{\\mathrm{GLS}}$ 表达式中的各项。其中 $\\mathbf{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$：\n$$\n\\Sigma^{-1} \\mathbf{1} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4/3 - 2/3 \\\\ -2/3 + 4/3 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}\n$$\n分母是：\n$$\n\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = 2/3 + 2/3 = 4/3\n$$\n最优权重是：\n$$\n\\mathbf{w}_{\\mathrm{GLS}} = \\frac{\\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}}{4/3} = \\begin{pmatrix} (2/3)/(4/3) \\\\ (2/3)/(4/3) \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\n所以，最优权重是 $w_1 = 1/2$ 和 $w_2 = 1/2$。\n\n最优估计量的方差 $V_{\\mathrm{GLS}}$ 是 $\\mathbf{w}_{\\mathrm{GLS}}^{\\top} \\Sigma \\mathbf{w}_{\\mathrm{GLS}}$。\n$$\nV_{\\mathrm{GLS}} = \\begin{pmatrix} 1/2  1/2 \\end{pmatrix} \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/2 \\end{pmatrix} \\begin{pmatrix} 1(1/2) + (1/2)(1/2) \\\\ (1/2)(1/2) + 1(1/2) \\end{pmatrix} = \\begin{pmatrix} 1/2  1/2 \\end{pmatrix} \\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix}\n$$\n$$\nV_{\\mathrm{GLS}} = (1/2)(3/4) + (1/2)(3/4) = 3/8 + 3/8 = 6/8 = 3/4\n$$\n另外，一般可以证明最小方差为 $V_{\\mathrm{GLS}} = (\\mathbf{1}^{\\top} \\Sigma^{-1} \\mathbf{1})^{-1}$，这里是 $(4/3)^{-1} = 3/4$。\n\n最后，我们计算朴素等权重估计量 $\\mathbf{w}_{\\mathrm{eq}} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$ 的方差。这个权重向量恰好与我们刚刚推导出的最优权重向量相同。因此，其方差 $V_{\\mathrm{eq}}$ 必须与 $V_{\\mathrm{GLS}}$ 相同。\n$$\nV_{\\mathrm{eq}} = \\mathbf{w}_{\\mathrm{eq}}^{\\top} \\Sigma \\mathbf{w}_{\\mathrm{eq}} = \\begin{pmatrix} 1/2  1/2 \\end{pmatrix} \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = 3/4\n$$\n有序四元组 $(w_1, w_2, V_{\\mathrm{GLS}}, V_{\\mathrm{eq}})$ 是 $(1/2, 1/2, 3/4, 3/4)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  \\frac{3}{4}  \\frac{3}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "集体智慧的形成并非总是静态的一次性聚合，而常常是一个动态的社会互动过程。本练习  采用经典的 DeGroot 模型来模拟观点在社交网络中的演化。通过引入持有固定信念的“固执个体”，我们得以探索外部信息源或坚定不移的少数派如何锚定并最终塑造整个群体的稳态共识，这为了解现实世界中舆论的形成提供了深刻的数学洞见。",
            "id": "4128756",
            "problem": "考虑一个由线性加权平均动态（常称为 DeGroot 模型）建模的集体意见形成过程：在离散时间 $t \\in \\{0,1,2,\\dots\\}$，意见向量 $x(t) \\in \\mathbb{R}^{n}$ 根据 $x(t+1) = W x(t)$ 演化，其中 $W \\in \\mathbb{R}^{n \\times n}$ 是一个行随机影响矩阵，其元素非负且每行之和为 $1$。在此设定中，一个固执代理人由 $W$ 中等于标准基向量的一行来编码，这施加了一个为 $1$ 的自权重和对所有其他代理人为零的权重，从而使其意见随时间固定不变。这样一个固执代理人子集代表了群体中的外生信号，其余代理人则是适应性的。\n\n构建如下一个包含 $n=5$ 个代理人的网络，其中有两个固执代理人和三个适应性代理人。代理人 $1$ 和 $4$ 是固执的。影响矩阵 $W$ 由下式给出\n$$\nW \\;=\\;\n\\begin{pmatrix}\n1  0  0  0  0\\\\\n0.3  0.4  0.2  0.1  0\\\\\n0  0.3  0.5  0.2  0\\\\\n0  0  0  1  0\\\\\n0.1  0  0.5  0.4  0\n\\end{pmatrix}.\n$$\n设固执意见为 $x_{1}(0) = s_{1} = 0.3$ 和 $x_{4}(0) = s_{4} = 0.9$，由于 $W$ 中对固执性的编码，这些意见在所有时间内保持不变。适应性代理人 $2$、 $3$ 和 $5$ 的初始意见是任意有限实数。\n\n从线性加权平均动态、固执代理人引起的吸收行为以及稳态条件 $x^{\\ast} = W x^{\\ast}$ 的核心定义出发，推导适应性代理人的稳态意见，并确定代理人 $5$ 的稳态意见。将代理人 $5$ 的最终值表示为一个精确分数。不需要单位。",
            "solution": "该问题陈述经评估是有效的。它在科学上基于已建立的线性共识模型（DeGroot模型）理论，是适定的且有唯一解，并以客观、正式的语言表述。不存在矛盾、数据缺失或其它可能使其无效的缺陷。因此，我们可以继续进行推导。\n\n集体意见形成过程的动态由线性系统 $x(t+1) = W x(t)$ 描述，其中 $x(t) \\in \\mathbb{R}^{5}$ 是在时间 $t$ 的意见向量， $W \\in \\mathbb{R}^{5 \\times 5}$ 是影响矩阵。稳态意见向量，记为 $x^{\\ast}$，必须满足条件 $x^{\\ast} = W x^{\\ast}$。这代表了该动力系统的一个不动点，在该点上意见不再随时间变化。\n\n影响矩阵 $W$ 如下所示：\n$$\nW \\;=\\;\n\\begin{pmatrix}\n1  0  0  0  0\\\\\n0.3  0.4  0.2  0.1  0\\\\\n0  0.3  0.5  0.2  0\\\\\n0  0  0  1  0\\\\\n0.1  0  0.5  0.4  0\n\\end{pmatrix}\n$$\n代理人 $1$ 和 $4$ 是固执的。这反映在 $W$ 的第一行和第四行，它们分别是标准基向量 $e_1^T = (1, 0, 0, 0, 0)$ 和 $e_4^T = (0, 0, 0, 1, 0)$。这种结构确保了他们的意见是固定的，即对所有 $t$ 都有 $x_1(t+1) = x_1(t)$ 和 $x_4(t+1) = x_4(t)$。他们的意见作为外生信号给出：对所有 $t \\geq 0$，有 $x_1(t) = s_1 = 0.3$ 和 $x_4(t) = s_4 = 0.9$。因此，他们的稳态意见是 $x_1^{\\ast} = 0.3$ 和 $x_4^{\\ast} = 0.9$。\n\n代理人 $2$、 $3$ 和 $5$ 是适应性的，他们的稳态意见由整个网络的影响决定。我们可以将稳态条件 $x^{\\ast} = W x^{\\ast}$ 写成一个线性方程组：\n$x_1^{\\ast} = 1 \\cdot x_1^{\\ast}$\n$x_2^{\\ast} = 0.3 x_1^{\\ast} + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.1 x_4^{\\ast} + 0 x_5^{\\ast}$\n$x_3^{\\ast} = 0 x_1^{\\ast} + 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.2 x_4^{\\ast} + 0 x_5^{\\ast}$\n$x_4^{\\ast} = 1 \\cdot x_4^{\\ast}$\n$x_5^{\\ast} = 0.1 x_1^{\\ast} + 0 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.4 x_4^{\\ast} + 0 x_5^{\\ast}$\n\n第一个和第四个方程是恒等式，与代理人 $1$ 和 $4$ 的固执性质一致。我们将 $x_1^{\\ast} = 0.3$ 和 $x_4^{\\ast} = 0.9$ 的已知值代入适应性代理人的方程中：\n对于代理人 $2$：\n$x_2^{\\ast} = 0.3(0.3) + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.1(0.9)$\n$x_2^{\\ast} = 0.09 + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.09$\n重新整理各项以求解 $x_2^{\\ast}$：\n$(1 - 0.4) x_2^{\\ast} - 0.2 x_3^{\\ast} = 0.18$\n$0.6 x_2^{\\ast} - 0.2 x_3^{\\ast} = 0.18$\n为了清晰起见，乘以 $10$：\n$6 x_2^{\\ast} - 2 x_3^{\\ast} = 1.8 \\implies 3 x_2^{\\ast} - x_3^{\\ast} = 0.9 \\quad (1)$\n\n对于代理人 $3$：\n$x_3^{\\ast} = 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.2(0.9)$\n$x_3^{\\ast} = 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.18$\n重新整理各项：\n$(1 - 0.5) x_3^{\\ast} - 0.3 x_2^{\\ast} = 0.18$\n$0.5 x_3^{\\ast} - 0.3 x_2^{\\ast} = 0.18$\n乘以 $10$：\n$5 x_3^{\\ast} - 3 x_2^{\\ast} = 1.8 \\quad (2)$\n\n现在我们得到了一个关于 $x_2^{\\ast}$ 和 $x_3^{\\ast}$ 的二元线性方程组。从方程 $(1)$ 中，我们可以用 $x_2^{\\ast}$ 来表示 $x_3^{\\ast}$：\n$x_3^{\\ast} = 3 x_2^{\\ast} - 0.9$\n\n将 $x_3^{\\ast}$ 的这个表达式代入方程 $(2)$：\n$5 (3 x_2^{\\ast} - 0.9) - 3 x_2^{\\ast} = 1.8$\n$15 x_2^{\\ast} - 4.5 - 3 x_2^{\\ast} = 1.8$\n$12 x_2^{\\ast} = 1.8 + 4.5$\n$12 x_2^{\\ast} = 6.3$\n$x_2^{\\ast} = \\frac{6.3}{12} = \\frac{63}{120}$\n为了简化这个分数，我们将分子和分母同除以它们的最大公约数 $3$：\n$x_2^{\\ast} = \\frac{63 \\div 3}{120 \\div 3} = \\frac{21}{40}$\n\n现在我们可以求出 $x_3^{\\ast}$：\n$x_3^{\\ast} = 3 x_2^{\\ast} - 0.9 = 3 \\left(\\frac{21}{40}\\right) - \\frac{9}{10} = \\frac{63}{40} - \\frac{36}{40} = \\frac{27}{40}$\n\n最后，我们使用代理人 $5$ 对应的方程来确定其稳态意见：\n$x_5^{\\ast} = 0.1 x_1^{\\ast} + 0.5 x_3^{\\ast} + 0.4 x_4^{\\ast}$\n代入 $x_1^{\\ast}$、$x_3^{\\ast}$ 和 $x_4^{\\ast}$ 的已知值：\n$x_5^{\\ast} = 0.1(0.3) + 0.5\\left(\\frac{27}{40}\\right) + 0.4(0.9)$\n$x_5^{\\ast} = 0.03 + \\frac{1}{2}\\left(\\frac{27}{40}\\right) + 0.36$\n$x_5^{\\ast} = \\frac{3}{100} + \\frac{27}{80} + \\frac{36}{100}$\n合并分母为 $100$ 的项：\n$x_5^{\\ast} = \\frac{39}{100} + \\frac{27}{80}$\n为了将这些分数相加，我们找到一个公分母。$100 = 2^2 \\cdot 5^2$ 和 $80 = 2^4 \\cdot 5$ 的最小公倍数是 $2^4 \\cdot 5^2 = 16 \\cdot 25 = 400$。\n$x_5^{\\ast} = \\frac{39 \\cdot 4}{100 \\cdot 4} + \\frac{27 \\cdot 5}{80 \\cdot 5}$\n$x_5^{\\ast} = \\frac{156}{400} + \\frac{135}{400}$\n$x_5^{\\ast} = \\frac{156 + 135}{400} = \\frac{291}{400}$\n\n分数 $\\frac{291}{400}$ 是最简形式，因为分子的质因数分解是 $291 = 3 \\cdot 97$，分母的质因数分解是 $400 = 2^4 \\cdot 5^2$。它们没有共同的质因数。\n因此，代理人 $5$ 的稳态意见是 $\\frac{291}{400}$。",
            "answer": "$$\\boxed{\\frac{291}{400}}$$"
        }
    ]
}