{
    "hands_on_practices": [
        {
            "introduction": "We begin with the most fundamental model of collective intelligence: simple averaging. This exercise asks you to derive the variance of a group's average estimate from first principles, revealing the mathematical underpinnings of the 'wisdom of crowds' phenomenon. By working through the derivation, you will gain a precise understanding of how group size $n$ and inter-agent correlation $\\rho$ impact collective accuracy.",
            "id": "4128693",
            "problem": "A group of $n$ agents collaboratively estimates a fixed scalar quantity $\\theta$ by reporting individual estimates $Y_{i} = \\theta + \\varepsilon_{i}$ for $i \\in \\{1,\\dots,n\\}$. The errors $\\varepsilon_{i}$ are unbiased, $ \\mathbb{E}[\\varepsilon_{i}] = 0$, and exchangeable with common variance $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^{2}$ and constant pairwise correlation $\\operatorname{Corr}(\\varepsilon_{i},\\varepsilon_{j}) = \\rho$ for all $i \\neq j$. The group forms the simple average $ \\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i} $ as its collective estimate.\n\nStarting from the core definitions of variance and covariance, and without invoking any shortcut formulas, derive a closed-form expression for $\\operatorname{Var}(\\bar{Y})$ in terms of $n$, $\\sigma^{2}$, and $\\rho$. Then evaluate this expression for $n=25$, $\\sigma^{2}=1$, and $\\rho=0.2$. Finally, compute the ratio of the group-average variance to the single-agent variance, that is, $\\operatorname{Var}(\\bar{Y}) / \\sigma^{2}$.\n\nProvide your final answers for the pair $\\big(\\operatorname{Var}(\\bar{Y}),\\ \\operatorname{Var}(\\bar{Y}) / \\sigma^{2}\\big)$ as exact values. Do not round. Your final answer must be given as a row matrix using the $\\text{pmatrix}$ environment.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in standard probability theory, well-posed with all necessary information, and stated objectively. The provided numerical values ($n=25$, $\\sigma^2=1$, $\\rho=0.2$) are consistent; specifically, the correlation $\\rho$ lies in the valid range for an $n$-dimensional exchangeable correlation matrix, which is $\\rho \\in [-\\frac{1}{n-1}, 1]$. For $n=25$, this range is $[-\\frac{1}{24}, 1]$, and $\\rho=0.2$ is well within these bounds.\n\nThe task is to derive an expression for the variance of the group average, $\\operatorname{Var}(\\bar{Y})$, and then evaluate it. The group average is defined as $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}$.\n\nFirst, we express $\\bar{Y}$ in terms of the true quantity $\\theta$ and the error terms $\\varepsilon_i$.\n$$\n\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} (\\theta + \\varepsilon_{i}) = \\frac{1}{n} \\left( \\sum_{i=1}^{n} \\theta + \\sum_{i=1}^{n} \\varepsilon_{i} \\right) = \\frac{1}{n} \\left( n\\theta + \\sum_{i=1}^{n} \\varepsilon_{i} \\right) = \\theta + \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\n$$\nThe variance of a random variable shifted by a constant is the variance of the random variable itself. Therefore,\n$$\n\\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\theta + \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\\right) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_{i}\\right)\n$$\nUsing the property $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$, where $a = \\frac{1}{n}$ is a constant, we have:\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)\n$$\nThe problem requires deriving the variance of the sum from core definitions. Let $S = \\sum_{i=1}^{n} \\varepsilon_{i}$. The variance of $S$ is defined as $\\operatorname{Var}(S) = \\mathbb{E}[(S - \\mathbb{E}[S])^2]$.\nFirst, we find the expectation of $S$:\n$$\n\\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{i=1}^{n} \\varepsilon_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[\\varepsilon_{i}]\n$$\nGiven that $\\mathbb{E}[\\varepsilon_{i}] = 0$ for all $i$, we have $\\mathbb{E}[S] = 0$.\nThe variance of $S$ simplifies to:\n$$\n\\operatorname{Var}(S) = \\mathbb{E}[(S - 0)^2] = \\mathbb{E}[S^2]\n$$\nWe now expand $S^2$:\n$$\nS^2 = \\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)^2 = \\left(\\sum_{i=1}^{n} \\varepsilon_{i}\\right)\\left(\\sum_{j=1}^{n} \\varepsilon_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\varepsilon_{i}\\varepsilon_{j}\n$$\nWe can split this double summation into terms where the indices are equal ($i=j$) and terms where they are not ($i \\neq j$):\n$$\nS^2 = \\sum_{i=1}^{n} \\varepsilon_{i}^2 + \\sum_{i \\neq j} \\varepsilon_{i}\\varepsilon_{j}\n$$\nNow, we take the expectation of $S^2$:\n$$\n\\mathbb{E}[S^2] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} \\varepsilon_{i}^2 + \\sum_{i \\neq j} \\varepsilon_{i}\\varepsilon_{j} \\right] = \\sum_{i=1}^{n} \\mathbb{E}[\\varepsilon_{i}^2] + \\sum_{i \\neq j} \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}]\n$$\nWe relate these expectation terms to the given variance and correlation.\nFor any random variable $X$, $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Since $\\mathbb{E}[\\varepsilon_{i}] = 0$ and $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^2$, we have:\n$$\n\\sigma^2 = \\mathbb{E}[\\varepsilon_{i}^2] - 0^2 \\implies \\mathbb{E}[\\varepsilon_{i}^2] = \\sigma^2\n$$\nFor any two random variables $X$ and $Y$, $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. For $i \\neq j$, since $\\mathbb{E}[\\varepsilon_{i}] = \\mathbb{E}[\\varepsilon_{j}] = 0$, we have:\n$$\n\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j}) = \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}] - 0 \\cdot 0 \\implies \\mathbb{E}[\\varepsilon_{i}\\varepsilon_{j}] = \\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j})\n$$\nThe correlation is defined as $\\operatorname{Corr}(\\varepsilon_{i}, \\varepsilon_{j}) = \\frac{\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j})}{\\sqrt{\\operatorname{Var}(\\varepsilon_{i})\\operatorname{Var}(\\varepsilon_{j})}}$. With the given values $\\operatorname{Corr}(\\varepsilon_{i}, \\varepsilon_{j}) = \\rho$ and $\\operatorname{Var}(\\varepsilon_{i}) = \\sigma^2$, we find the covariance for $i \\neq j$:\n$$\n\\operatorname{Cov}(\\varepsilon_{i}, \\varepsilon_{j}) = \\rho \\sqrt{\\sigma^2 \\cdot \\sigma^2} = \\rho \\sigma^2\n$$\nNow we substitute these results back into the expression for $\\mathbb{E}[S^2]$:\n$$\n\n\\operatorname{Var}(S) = \\mathbb{E}[S^2] = \\sum_{i=1}^{n} \\sigma^2 + \\sum_{i \\neq j} \\rho\\sigma^2\n$$\nThe first sum has $n$ identical terms, so its value is $n\\sigma^2$. The second sum is over all pairs of distinct indices $(i,j)$. There are $n(n-1)$ such pairs. Thus, the value of the second sum is $n(n-1)\\rho\\sigma^2$.\n$$\n\\operatorname{Var}(S) = n\\sigma^2 + n(n-1)\\rho\\sigma^2\n$$\nFinally, we substitute this back into our expression for $\\operatorname{Var}(\\bar{Y})$:\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n^2} \\operatorname{Var}(S) = \\frac{1}{n^2} [n\\sigma^2 + n(n-1)\\rho\\sigma^2]\n$$\nFactoring out $n\\sigma^2$ from the bracketed term gives the closed-form expression:\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{n\\sigma^2}{n^2} [1 + (n-1)\\rho] = \\frac{\\sigma^2}{n}[1 + (n-1)\\rho]\n$$\nThis completes the derivation.\n\nNext, we evaluate this expression for $n=25$, $\\sigma^2=1$, and $\\rho=0.2$.\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{1}{25}[1 + (25-1)(0.2)] = \\frac{1}{25}[1 + (24)(0.2)] = \\frac{1}{25}[1 + 4.8] = \\frac{5.8}{25}\n$$\nTo provide an exact value, we convert the decimal to a fraction:\n$$\n\\operatorname{Var}(\\bar{Y}) = \\frac{5.8}{25} = \\frac{58}{250} = \\frac{29}{125}\n$$\nFinally, we compute the ratio of the group-average variance to the single-agent variance, $\\operatorname{Var}(\\bar{Y}) / \\sigma^2$.\nUsing the derived closed-form expression:\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{1}{\\sigma^2} \\left( \\frac{\\sigma^2}{n}[1 + (n-1)\\rho] \\right) = \\frac{1}{n}[1 + (n-1)\\rho]\n$$\nUsing the provided values:\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{1}{25}[1 + (24)(0.2)] = \\frac{5.8}{25} = \\frac{29}{125}\n$$\nAlternatively, using the previously calculated value for $\\operatorname{Var}(\\bar{Y})$ and the given $\\sigma^2=1$:\n$$\n\\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2} = \\frac{29/125}{1} = \\frac{29}{125}\n$$\nThe pair of answers is $\\left(\\operatorname{Var}(\\bar{Y}), \\frac{\\operatorname{Var}(\\bar{Y})}{\\sigma^2}\\right) = \\left(\\frac{29}{125}, \\frac{29}{125}\\right)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{29}{125} & \\frac{29}{125} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Collective intelligence is often not a one-shot aggregation but an evolving process of social influence. This practice introduces a model of opinion dynamics, where agents repeatedly average their neighbors' opinions, described by the update rule $x(t+1) = W x(t)$. You will analyze a network that includes 'stubborn agents'—individuals who hold fixed beliefs—to determine how they anchor the final steady-state opinions of the entire adaptive group.",
            "id": "4128756",
            "problem": "Consider a collective opinion formation process modeled by the linear weighted averaging dynamics (often called the DeGroot model): at discrete time $t \\in \\{0,1,2,\\dots\\}$, the opinion vector $x(t) \\in \\mathbb{R}^{n}$ evolves according to $x(t+1) = W x(t)$, where $W \\in \\mathbb{R}^{n \\times n}$ is a row-stochastic influence matrix with nonnegative entries and each row summing to $1$. In this setting, a stubborn agent is encoded by a row of $W$ equal to a standard basis vector, which imposes a self-weight $1$ and zero weight on all others, thereby fixing its opinion over time. Such a subset of stubborn agents represents exogenous signals in the population, and the rest of the agents are adaptive.\n\nConstruct the following network of $n=5$ agents with two stubborn agents and three adaptive agents. Agents $1$ and $4$ are stubborn. The influence matrix $W$ is given by\n$$\nW \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0.3 & 0.4 & 0.2 & 0.1 & 0\\\\\n0 & 0.3 & 0.5 & 0.2 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0.1 & 0 & 0.5 & 0.4 & 0\n\\end{pmatrix}.\n$$\nLet the stubborn opinions be $x_{1}(0) = s_{1} = 0.3$ and $x_{4}(0) = s_{4} = 0.9$, which remain fixed for all time due to the encoding of stubbornness in $W$. The initial opinions of the adaptive agents $2$, $3$, and $5$ are arbitrary finite real numbers.\n\nStarting from the core definitions of linear weighted averaging dynamics, absorbing behavior induced by stubborn agents, and the steady-state condition $x^{\\ast} = W x^{\\ast}$, derive the steady-state opinions of the adaptive agents and determine the steady-state opinion of agent $5$. Express the final value for agent $5$ as an exact fraction. No units are required.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established theory of linear consensus models (the DeGroot model), is well-posed with a unique solution, and is expressed in objective, formal language. There are no contradictions, missing data, or other flaws that would invalidate it. We may therefore proceed with the derivation.\n\nThe dynamics of the collective opinion formation process are described by the linear system $x(t+1) = W x(t)$, where $x(t) \\in \\mathbb{R}^{5}$ is the vector of opinions at time $t$ and $W \\in \\mathbb{R}^{5 \\times 5}$ is the influence matrix. The steady-state opinion vector, denoted by $x^{\\ast}$, must satisfy the condition $x^{\\ast} = W x^{\\ast}$. This represents a fixed point of the dynamical system, where the opinions no longer change over time.\n\nThe influence matrix $W$ is given as:\n$$\nW \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0.3 & 0.4 & 0.2 & 0.1 & 0\\\\\n0 & 0.3 & 0.5 & 0.2 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0.1 & 0 & 0.5 & 0.4 & 0\n\\end{pmatrix}\n$$\nAgents $1$ and $4$ are stubborn. This is reflected in the first and fourth rows of $W$, which are the standard basis vectors $e_1^T = (1, 0, 0, 0, 0)$ and $e_4^T = (0, 0, 0, 1, 0)$, respectively. This structure ensures that their opinions are fixed, i.e., $x_1(t+1) = x_1(t)$ and $x_4(t+1) = x_4(t)$ for all $t$. Their opinions are given as exogenous signals: $x_1(t) = s_1 = 0.3$ and $x_4(t) = s_4 = 0.9$ for all $t \\geq 0$. Consequently, their steady-state opinions are $x_1^{\\ast} = 0.3$ and $x_4^{\\ast} = 0.9$.\n\nAgents $2$, $3$, and $5$ are adaptive, and their steady-state opinions are determined by the influence of the entire network. We can write the steady-state condition $x^{\\ast} = W x^{\\ast}$ as a system of linear equations:\n$x_1^{\\ast} = 1 \\cdot x_1^{\\ast}$\n$x_2^{\\ast} = 0.3 x_1^{\\ast} + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.1 x_4^{\\ast} + 0 x_5^{\\ast}$\n$x_3^{\\ast} = 0 x_1^{\\ast} + 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.2 x_4^{\\ast} + 0 x_5^{\\ast}$\n$x_4^{\\ast} = 1 \\cdot x_4^{\\ast}$\n$x_5^{\\ast} = 0.1 x_1^{\\ast} + 0 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.4 x_4^{\\ast} + 0 x_5^{\\ast}$\n\nThe first and fourth equations are identities, consistent with the stubborn nature of agents $1$ and $4$. We substitute the known values of $x_1^{\\ast} = 0.3$ and $x_4^{\\ast} = 0.9$ into the equations for the adaptive agents:\nFor agent $2$:\n$x_2^{\\ast} = 0.3(0.3) + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.1(0.9)$\n$x_2^{\\ast} = 0.09 + 0.4 x_2^{\\ast} + 0.2 x_3^{\\ast} + 0.09$\nRearranging the terms to solve for $x_2^{\\ast}$:\n$(1 - 0.4) x_2^{\\ast} - 0.2 x_3^{\\ast} = 0.18$\n$0.6 x_2^{\\ast} - 0.2 x_3^{\\ast} = 0.18$\nMultiplying by $10$ for clarity:\n$6 x_2^{\\ast} - 2 x_3^{\\ast} = 1.8 \\implies 3 x_2^{\\ast} - x_3^{\\ast} = 0.9 \\quad (1)$\n\nFor agent $3$:\n$x_3^{\\ast} = 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.2(0.9)$\n$x_3^{\\ast} = 0.3 x_2^{\\ast} + 0.5 x_3^{\\ast} + 0.18$\nRearranging the terms:\n$(1 - 0.5) x_3^{\\ast} - 0.3 x_2^{\\ast} = 0.18$\n$0.5 x_3^{\\ast} - 0.3 x_2^{\\ast} = 0.18$\nMultiplying by $10$:\n$5 x_3^{\\ast} - 3 x_2^{\\ast} = 1.8 \\quad (2)$\n\nWe now have a system of two linear equations for $x_2^{\\ast}$ and $x_3^{\\ast}$. From equation $(1)$, we can express $x_3^{\\ast}$ in terms of $x_2^{\\ast}$:\n$x_3^{\\ast} = 3 x_2^{\\ast} - 0.9$\n\nSubstitute this expression for $x_3^{\\ast}$ into equation $(2)$:\n$5 (3 x_2^{\\ast} - 0.9) - 3 x_2^{\\ast} = 1.8$\n$15 x_2^{\\ast} - 4.5 - 3 x_2^{\\ast} = 1.8$\n$12 x_2^{\\ast} = 1.8 + 4.5$\n$12 x_2^{\\ast} = 6.3$\n$x_2^{\\ast} = \\frac{6.3}{12} = \\frac{63}{120}$\nTo simplify this fraction, we divide the numerator and denominator by their greatest common divisor, which is $3$:\n$x_2^{\\ast} = \\frac{63 \\div 3}{120 \\div 3} = \\frac{21}{40}$\n\nNow we can find $x_3^{\\ast}$:\n$x_3^{\\ast} = 3 x_2^{\\ast} - 0.9 = 3 \\left(\\frac{21}{40}\\right) - \\frac{9}{10} = \\frac{63}{40} - \\frac{36}{40} = \\frac{27}{40}$\n\nFinally, we determine the steady-state opinion of agent $5$ using its corresponding equation:\n$x_5^{\\ast} = 0.1 x_1^{\\ast} + 0.5 x_3^{\\ast} + 0.4 x_4^{\\ast}$\nSubstitute the known values of $x_1^{\\ast}$, $x_3^{\\ast}$, and $x_4^{\\ast}$:\n$x_5^{\\ast} = 0.1(0.3) + 0.5\\left(\\frac{27}{40}\\right) + 0.4(0.9)$\n$x_5^{\\ast} = 0.03 + \\frac{1}{2}\\left(\\frac{27}{40}\\right) + 0.36$\n$x_5^{\\ast} = \\frac{3}{100} + \\frac{27}{80} + \\frac{36}{100}$\nCombine the terms with denominator $100$:\n$x_5^{\\ast} = \\frac{39}{100} + \\frac{27}{80}$\nTo add these fractions, we find a common denominator. The least common multiple of $100 = 2^2 \\cdot 5^2$ and $80 = 2^4 \\cdot 5$ is $2^4 \\cdot 5^2 = 16 \\cdot 25 = 400$.\n$x_5^{\\ast} = \\frac{39 \\cdot 4}{100 \\cdot 4} + \\frac{27 \\cdot 5}{80 \\cdot 5}$\n$x_5^{\\ast} = \\frac{156}{400} + \\frac{135}{400}$\n$x_5^{\\ast} = \\frac{156 + 135}{400} = \\frac{291}{400}$\n\nThe fraction $\\frac{291}{400}$ is in simplest form because the prime factorization of the numerator is $291 = 3 \\cdot 97$, and the prime factorization of the denominator is $400 = 2^4 \\cdot 5^2$. They share no common prime factors.\nThus, the steady-state opinion of agent $5$ is $\\frac{291}{400}$.",
            "answer": "$$\\boxed{\\frac{291}{400}}$$"
        },
        {
            "introduction": "In many real-world scenarios, the reliability of different information sources is unknown and may change over time. This advanced practice challenges you to derive and implement an online learning algorithm that adaptively adjusts the weights given to different agents based on their past performance. This exercise bridges theory and computation, demonstrating how a collective system can learn to perform nearly as well as the best expert in hindsight, even without knowing who that expert is in advance.",
            "id": "4128694",
            "problem": "You are modeling a collective intelligence system that aggregates the advice of $m=5$ agents on a sequence of $T=1000$ tasks. On each task indexed by $t \\in \\{1,\\dots,T\\}$, you must choose a weight vector $w_t \\in \\Delta_m$, where $\\Delta_m = \\{ w \\in \\mathbb{R}^m : \\forall i,\\; w_i \\ge 0,\\; \\sum_{i=1}^m w_i = 1 \\}$, to combine agents’ advice. After choosing $w_t$, a loss vector $\\ell_t \\in [0,1]^m$ is revealed, and you incur the scalar loss $L_t = w_t^\\top \\ell_t$. The cumulative regret after $T$ tasks is defined as\n$$\n\\mathrm{Reg}_T \\;=\\; \\sum_{t=1}^T w_t^\\top \\ell_t \\;-\\; \\min_{w \\in \\Delta_m} \\sum_{t=1}^T w^\\top \\ell_t.\n$$\nYour goal is to derive, implement, and evaluate an online weighting algorithm based on first principles that updates $w_t$ using only past information.\n\nStart from the following fundamental base in online convex optimization and information geometry:\n- At each round $t$, the instantaneous loss $f_t(w) = w^\\top \\ell_t$ is a convex function over the probability simplex $\\Delta_m$.\n- Online Mirror Descent (OMD) with a strictly convex regularizer $R$ updates by minimizing a first-order approximation of the cumulative loss plus a Bregman divergence term. For a differentiable, strictly convex function $R$, the Bregman divergence between $w$ and $u$ is $D_R(w \\,\\|\\, u) = R(w) - R(u) - \\nabla R(u)^\\top (w-u)$.\n- The negative entropy regularizer $R(w) = \\sum_{i=1}^m w_i \\log w_i$ on $\\Delta_m$ induces the Kullback–Leibler (KL) divergence as its Bregman divergence.\n\nTasks:\n1. From these principles, derive the OMD update on the probability simplex with the negative entropy regularizer and a constant learning rate $\\eta > 0$. The derivation must begin from the definition of OMD as minimizing a linearized loss plus a Bregman divergence and arrive at a closed-form update for $w_{t+1}$ in terms of $w_t$, $\\ell_t$, and $\\eta$, including the necessary normalization to ensure $w_{t+1} \\in \\Delta_m$.\n2. Prove that for the regret comparator $\\min_{w \\in \\Delta_m} \\sum_{t=1}^T w^\\top \\ell_t$, the optimal $w^\\star$ is attained at an extreme point of $\\Delta_m$ and therefore corresponds to placing all mass on the single agent with the smallest cumulative loss $\\sum_{t=1}^T \\ell_{t,i}$. Justify this from the linearity of the objective over a polytope.\n3. Implement the derived algorithm with $m=5$ and $T=1000$, initializing $w_1$ to the uniform distribution over $m$ agents. Use the following stochastic loss generators, each specified by a fixed random seed for reproducibility. All losses are unitless and constrained to the interval $[0,1]$.\n\nLoss generators:\n- Stationary correlated Gaussian generator: For each $t$, draw $\\ell_t \\sim \\mathcal{N}(\\mu, \\Sigma)$ with $\\mu = (0.30, 0.40, 0.20, 0.50, 0.60)$, marginal standard deviations $\\sigma_i = 0.05$ for all $i$, and constant pairwise correlation $\\rho = 0.30$. Construct $\\Sigma$ with entries $\\Sigma_{ii} = \\sigma_i^2$ and $\\Sigma_{ij} = \\rho \\sigma_i \\sigma_j$ for $i \\ne j$. Then clip each component to $[0,1]$. Use the specified seed for each test.\n- Nonstationary piecewise Gaussian generator: For $t \\le 500$, use the stationary generator with mean $\\mu^{(1)} = (0.30, 0.40, 0.20, 0.50, 0.60)$; for $t \\ge 501$, use the same covariance but mean $\\mu^{(2)} = (0.60, 0.50, 0.40, 0.30, 0.20)$. Clip to $[0,1]$ and use the specified seed.\n\nTest suite:\n- Test $1$ (happy path): stationary correlated Gaussian, seed $42$, learning rate $\\eta = 0.50$.\n- Test $2$ (boundary: very small learning rate): stationary correlated Gaussian, seed $43$, learning rate $\\eta = 0.01$.\n- Test $3$ (boundary: aggressive learning rate): stationary correlated Gaussian, seed $44$, learning rate $\\eta = 2.00$.\n- Test $4$ (edge: nonstationary drift): nonstationary piecewise Gaussian, seed $45$, learning rate $\\eta = 0.50$.\n\nImplementation requirements and output:\n- For each test, run the algorithm for $T=1000$ rounds and compute the cumulative regret $\\mathrm{Reg}_T$ as defined above, where the comparator is the best fixed weight vector in hindsight over the entire horizon $t \\in \\{1,\\dots,1000\\}$.\n- Your program should produce a single line of output containing the regrets for Tests $1$ through $4$ as a comma-separated list of floating-point numbers rounded to exactly six digits after the decimal point, enclosed in square brackets. For example: $[r_1,r_2,r_3,r_4]$, where each $r_k$ is a float with six digits after the decimal point.\n- No user input is required, and all randomness must be controlled exactly by the stated seeds.",
            "solution": "The problem statement is parsed and validated. All givens are extracted and checked for scientific soundness, self-consistency, and completeness.\n\n**Givens:**\n- Number of agents: $m=5$.\n- Number of tasks (time horizon): $T=1000$.\n- Task index: $t \\in \\{1,\\dots,T\\}$.\n- Weight vector space: The probability simplex $\\Delta_m = \\{ w \\in \\mathbb{R}^m : \\forall i,\\; w_i \\ge 0,\\; \\sum_{i=1}^m w_i = 1 \\}$.\n- Weight vector at time $t$: $w_t \\in \\Delta_m$.\n- Loss vector at time $t$: $\\ell_t \\in [0,1]^m$.\n- Incurred loss at time $t$: $L_t = w_t^\\top \\ell_t$.\n- Cumulative regret definition: $\\mathrm{Reg}_T = \\sum_{t=1}^T w_t^\\top \\ell_t - \\min_{w \\in \\Delta_m} \\sum_{t=1}^T w^\\top \\ell_t$.\n- Algorithmic framework: Online Mirror Descent (OMD) with a strictly convex regularizer $R$.\n- Specified regularizer: Negative entropy, $R(w) = \\sum_{i=1}^m w_i \\log w_i$.\n- Initial condition: $w_1$ is the uniform distribution, i.e., $w_{1,i} = 1/m$ for all $i \\in \\{1, \\dots, m\\}$.\n- Loss generators and test parameters are specified in detail.\n\n**Validation Verdict:**\nThe problem is **valid**. It is a well-posed and standard problem in the field of online machine learning, specifically the \"prediction with expert advice\" setting. The theoretical tasks (derivation and proof) are fundamental exercises in online convex optimization. The implementation is fully specified with reproducible loss-generation processes, ensuring a unique and verifiable solution. The problem is scientifically grounded, objective, and complete.\n\nWe proceed with the solution, addressing each task in order.\n\n**Task 1: Derivation of the OMD Update Rule**\n\nThe Online Mirror Descent (OMD) algorithm generates the next weight vector $w_{t+1}$ by minimizing a trade-off between the linearized instantaneous loss and the proximity to the current weight vector $w_t$. The proximity is measured by the Bregman divergence $D_R(w \\,\\|\\, w_t)$ induced by a regularizer $R(w)$. The update rule is given by:\n$$\nw_{t+1} = \\arg\\min_{w \\in \\Delta_m} \\left\\{ \\eta \\langle \\nabla f_t(w_t), w \\rangle + D_R(w \\,\\|\\, w_t) \\right\\}\n$$\nwhere $f_t(w) = w^\\top \\ell_t$ is the loss function at time $t$, and $\\eta > 0$ is the learning rate.\n\n1.  **Identify Components:**\n    -   The loss function is $f_t(w) = w^\\top \\ell_t = \\sum_{i=1}^m w_i \\ell_{t,i}$. Its gradient with respect to $w$ is constant: $\\nabla f_t(w) = \\ell_t$.\n    -   The regularizer is the negative entropy function: $R(w) = \\sum_{i=1}^m w_i \\log w_i$. The domain of $R(w)$ is the interior of the simplex.\n    -   The gradient of the regularizer is $\\nabla R(w)_i = \\frac{\\partial}{\\partial w_i} (\\sum_{j=1}^m w_j \\log w_j) = \\log w_i + 1$.\n    -   The Bregman divergence $D_R(w \\,\\|\\, u)$ is defined as $R(w) - R(u) - \\langle \\nabla R(u), w-u \\rangle$. Substituting our regularizer $R(w)$:\n        $$\n        \\begin{aligned}\n        D_R(w \\,\\|\\, w_t) &= \\sum_{i=1}^m w_i \\log w_i - \\sum_{i=1}^m w_{t,i} \\log w_{t,i} - \\sum_{i=1}^m (\\log w_{t,i} + 1)(w_i - w_{t,i}) \\\\\n        &= \\sum_i w_i \\log w_i - \\sum_i w_{t,i} \\log w_{t,i} - \\sum_i w_i \\log w_{t,i} - \\sum_i w_i + \\sum_i w_{t,i} \\log w_{t,i} + \\sum_i w_{t,i} \\\\\n        &= \\sum_i w_i (\\log w_i - \\log w_{t,i}) - \\left(\\sum_i w_i - \\sum_i w_{t,i}\\right)\n        \\end{aligned}\n        $$\n        Since both $w$ and $w_t$ are in $\\Delta_m$, their components sum to $1$. Thus, $\\sum_i w_i = \\sum_i w_{t,i} = 1$. The last term vanishes.\n        $$\n        D_R(w \\,\\|\\, w_t) = \\sum_{i=1}^m w_i \\log\\left(\\frac{w_i}{w_{t,i}}\\right) = D_{KL}(w \\,\\|\\, w_t)\n        $$\n        This is the Kullback–Leibler (KL) divergence.\n\n2.  **Formulate the Optimization Problem:**\n    Substituting the gradient $\\ell_t$ and the KL divergence into the OMD update, we need to solve:\n    $$\n    w_{t+1} = \\arg\\min_{w \\in \\Delta_m} \\left\\{ \\eta \\sum_{i=1}^m w_i \\ell_{t,i} + \\sum_{i=1}^m w_i \\log\\left(\\frac{w_i}{w_{t,i}}\\right) \\right\\}\n    $$\n    This is a constrained optimization problem. We introduce a Lagrange multiplier $\\lambda$ for the constraint $\\sum_i w_i - 1 = 0$. The KKT conditions for non-negativity $w_i \\ge 0$ will be satisfied naturally. The Lagrangian is:\n    $$\n    \\mathcal{L}(w, \\lambda) = \\eta \\sum_{i=1}^m w_i \\ell_{t,i} + \\sum_{i=1}^m w_i \\log w_i - \\sum_{i=1}^m w_i \\log w_{t,i} + \\lambda \\left(\\sum_{i=1}^m w_i - 1\\right)\n    $$\n\n3.  **Solve for the Optimal $w$:**\n    We take the partial derivative of $\\mathcal{L}$ with respect to each $w_i$ and set it to $0$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\eta \\ell_{t,i} + (\\log w_i + 1) - \\log w_{t,i} + \\lambda = 0\n    $$\n    Solving for $\\log w_i$:\n    $$\n    \\log w_i = \\log w_{t,i} - \\eta \\ell_{t,i} - 1 - \\lambda\n    $$\n    Exponentiating both sides gives $w_i$:\n    $$\n    w_i = \\exp(\\log w_{t,i} - \\eta \\ell_{t,i} - 1 - \\lambda) = w_{t,i} \\exp(-\\eta \\ell_{t,i}) \\exp(-1-\\lambda)\n    $$\n\n4.  **Enforce Normalization:**\n    The term $\\exp(-1-\\lambda)$ is a normalization constant. Let's call it $1/Z$. Then $w_i = \\frac{1}{Z} w_{t,i} \\exp(-\\eta \\ell_{t,i})$. We use the constraint $\\sum_i w_i = 1$ to find $Z$:\n    $$\n    \\sum_{i=1}^m w_i = \\frac{1}{Z} \\sum_{i=1}^m w_{t,i} \\exp(-\\eta \\ell_{t,i}) = 1 \\implies Z = \\sum_{j=1}^m w_{t,j} \\exp(-\\eta \\ell_{t,j})\n    $$\n    Substituting $Z$ back, we obtain the closed-form update rule for each component $i$ of $w_{t+1}$:\n    $$\n    w_{t+1,i} = \\frac{w_{t,i} \\exp(-\\eta \\ell_{t,i})}{\\sum_{j=1}^m w_{t,j} \\exp(-\\eta \\ell_{t,j})}\n    $$\n    This is the celebrated Multiplicative Weights Update Algorithm, also known as the Hedge algorithm. The non-negativity constraint $w_{t+1,i} \\ge 0$ is inherently satisfied as $w_{t,i} \\ge 0$ and the exponential function is always positive. The sum-to-one constraint is satisfied by construction.\n\n**Task 2: Proof for the Optimal Hindsight Comparator**\n\nThe regret is measured against the best-fixed weight vector in hindsight. The loss term of this comparator is $\\min_{w \\in \\Delta_m} \\sum_{t=1}^T w^\\top \\ell_t$. We must prove that the optimal weight vector $w^\\star$ for this problem is an extreme point of $\\Delta_m$.\n\n1.  **Define the Optimization Problem:**\n    The problem is to find $w^\\star = \\arg\\min_{w \\in \\Delta_m} \\sum_{t=1}^T w^\\top \\ell_t$.\n    By linearity of the dot product and summation, we can rewrite the objective function:\n    $$\n    \\sum_{t=1}^T w^\\top \\ell_t = w^\\top \\left(\\sum_{t=1}^T \\ell_t\\right)\n    $$\n    Let $C = \\sum_{t=1}^T \\ell_t$ be the vector of cumulative losses for each of the $m$ agents. $C$ is a constant vector computed after all $T$ rounds. The optimization problem is:\n    $$\n    \\min_{w} w^\\top C \\quad \\text{subject to} \\quad w \\in \\Delta_m = \\left\\{ w \\in \\mathbb{R}^m : \\sum_{i=1}^m w_i = 1, w_i \\ge 0 \\text{ for all } i \\right\\}\n    $$\n\n2.  **Apply Principles of Linear Programming:**\n    This is a Linear Program (LP). The objective function, $f(w) = w^\\top C$, is linear in the decision variable $w$. The feasible region, the probability simplex $\\Delta_m$, is a compact convex set, specifically a polytope.\n    A fundamental theorem of linear programming states that the optimal value of a linear objective function over a compact convex polytope is always achieved at least at one of the polytope's vertices (extreme points).\n\n3.  **Identify Vertices of the Simplex:**\n    The vertices of the standard $m$-simplex $\\Delta_m$ are the standard basis vectors in $\\mathbb{R}^m$:\n    $$\n    e_1 = (1, 0, \\dots, 0)^\\top, \\quad e_2 = (0, 1, \\dots, 0)^\\top, \\quad \\dots, \\quad e_m = (0, 0, \\dots, 1)^\\top\n    $$\n    Each vertex $e_j$ corresponds to a deterministic strategy of always choosing agent $j$.\n\n4.  **Conclusion:**\n    According to the theorem, the optimal weight vector $w^\\star$ must be one of these vertices, $w^\\star = e_j$ for some $j \\in \\{1, \\dots, m\\}$. The value of the objective function at such a vertex is:\n    $$\n    (e_j)^\\top C = C_j = \\sum_{t=1}^T \\ell_{t,j}\n    $$\n    This is the total cumulative loss of agent $j$. The minimum value is therefore $\\min_{j \\in \\{1, \\dots, m\\}} C_j$. The optimal strategy in hindsight is to have placed all weight on the single agent who performed best over the entire history.\n\n**Algorithmic Implementation**\n\nThe implementation follows the derived update rule and regret calculation. For each of the four tests, a simulation is run for $T=1000$ rounds with $m=5$ agents.\n\n1.  **Initialization:**\n    - The weight vector is initialized to the uniform distribution: $w_1 = (1/5, 1/5, 1/5, 1/5, 1/5)^\\top$.\n    - A running total of the algorithm's loss, $\\sum_{t=1}^T w_t^\\top \\ell_t$, is initialized to $0$.\n    - A list is used to store all revealed loss vectors $\\ell_1, \\dots, \\ell_T$ to compute the hindsight comparator at the end.\n\n2.  **Loss Generation:**\n    - At each round $t$, a loss vector $\\ell_t$ is drawn from a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ and then clipped to the range $[0,1]^m$.\n    - The covariance matrix $\\Sigma$ is constructed with diagonal entries $\\Sigma_{ii} = \\sigma_i^2$ and off-diagonal entries $\\Sigma_{ij} = \\rho \\sigma_i \\sigma_j$, using $\\sigma_i = 0.05$ and $\\rho=0.30$.\n    - For the stationary case, the mean vector $\\mu = (0.30, 0.40, 0.20, 0.50, 0.60)$ is constant.\n    - For the nonstationary case, the mean vector switches from $\\mu^{(1)}=(0.30, 0.40, 0.20, 0.50, 0.60)$ to $\\mu^{(2)}=(0.60, 0.50, 0.40, 0.30, 0.20)$ at $t=501$.\n    - A separate random number generator is seeded for each test case to ensure reproducibility.\n\n3.  **Simulation Loop (for $t=1, \\dots, T$):**\n    a. The current weights $w_t$ are known.\n    b. A loss vector $\\ell_t$ is generated.\n    c. The incurred loss $L_t = w_t^\\top \\ell_t$ is calculated and added to the algorithm's total loss.\n    d. The next weight vector, $w_{t+1}$, is computed using the derived multiplicative update rule:\n       $$\n       w_{t+1,i} = \\frac{w_{t,i} \\exp(-\\eta \\ell_{t,i})}{\\sum_{j=1}^m w_{t,j} \\exp(-\\eta \\ell_{t,j})}\n       $$\n       This new vector becomes the current weight vector for the next iteration.\n\n4.  **Regret Calculation:**\n    - After $T$ rounds, the total loss of the algorithm, $L_{\\text{algo}} = \\sum_{t=1}^T w_t^\\top \\ell_t$, is finalized.\n    - The cumulative loss for each individual agent is computed: $C_i = \\sum_{t=1}^T \\ell_{t,i}$.\n    - The loss of the best agent in hindsight is found: $L_{\\text{best}} = \\min_{i \\in \\{1,\\dots,m\\}} C_i$.\n    - The final regret is computed as $\\mathrm{Reg}_T = L_{\\text{algo}} - L_{\\text{best}}$.\n\nThis procedure is repeated for each of the four test cases, varying the learning rate $\\eta$ and the loss generation process as specified.",
            "answer": "```python\nimport numpy as np\n\ndef get_loss_generator(m, T, mu1, mu2, sigma, rho, seed, is_stationary):\n    \"\"\"\n    Creates a generator that yields loss vectors for T rounds.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    cov = np.full((m, m), rho * sigma**2)\n    np.fill_diagonal(cov, sigma**2)\n\n    for t_idx in range(T):\n        if is_stationary or t_idx < 500:\n            mean = mu1\n        else:\n            mean = mu2\n        \n        loss_vec = rng.multivariate_normal(mean, cov)\n        yield np.clip(loss_vec, 0.0, 1.0)\n\ndef run_simulation(m, T, eta, loss_config):\n    \"\"\"\n    Runs one full OMD simulation and returns the cumulative regret.\n    \"\"\"\n    # Initialize weights to uniform distribution\n    w = np.full(m, 1.0 / m)\n    \n    # Initialize loss tracking\n    total_algorithm_loss = 0.0\n    all_losses = []\n\n    # Get the appropriate loss generator\n    is_stationary = loss_config[\"type\"] == \"stationary\"\n    loss_gen = get_loss_generator(\n        m=m,\n        T=T,\n        mu1=loss_config[\"mu1\"],\n        mu2=loss_config[\"mu2\"],\n        sigma=loss_config[\"sigma\"],\n        rho=loss_config[\"rho\"],\n        seed=loss_config[\"seed\"],\n        is_stationary=is_stationary\n    )\n\n    # Main simulation loop for T rounds\n    for _ in range(T):\n        # 1. A new loss vector is revealed\n        loss_vec = next(loss_gen)\n        all_losses.append(loss_vec)\n\n        # 2. Incur loss based on current weights w_t\n        total_algorithm_loss += np.dot(w, loss_vec)\n\n        # 3. Update weights to w_{t+1} using the multiplicative update rule\n        numerator = w * np.exp(-eta * loss_vec)\n        denominator = np.sum(numerator)\n        \n        # Avoid division by zero if all weights become infinitesimal\n        if denominator > 0:\n            w = numerator / denominator\n        else:\n            # Re-initialize to uniform if weights collapse, a robust-ness measure\n            w = np.full(m, 1.0 / m)\n\n    # Calculate the cumulative regret\n    # The comparator is the best single agent in hindsight\n    all_losses_matrix = np.array(all_losses)\n    cumulative_agent_losses = np.sum(all_losses_matrix, axis=0)\n    best_agent_loss = np.min(cumulative_agent_losses)\n\n    regret = total_algorithm_loss - best_agent_loss\n    return regret\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs simulations, and prints the final results.\n    \"\"\"\n    m = 5\n    T = 1000\n    mu1 = np.array([0.30, 0.40, 0.20, 0.50, 0.60])\n    mu2 = np.array([0.60, 0.50, 0.40, 0.30, 0.20])\n    sigma = 0.05\n    rho = 0.30\n\n    test_cases = [\n        # Test 1: stationary, seed 42, eta = 0.50\n        {\"type\": \"stationary\", \"seed\": 42, \"eta\": 0.50},\n        # Test 2: stationary, seed 43, eta = 0.01\n        {\"type\": \"stationary\", \"seed\": 43, \"eta\": 0.01},\n        # Test 3: stationary, seed 44, eta = 2.00\n        {\"type\": \"stationary\", \"seed\": 44, \"eta\": 2.00},\n        # Test 4: nonstationary, seed 45, eta = 0.50\n        {\"type\": \"nonstationary\", \"seed\": 45, \"eta\": 0.50},\n    ]\n\n    results = []\n    for case in test_cases:\n        loss_config = {\n            \"type\": case[\"type\"],\n            \"mu1\": mu1,\n            \"mu2\": mu2,\n            \"sigma\": sigma,\n            \"rho\": rho,\n            \"seed\": case[\"seed\"],\n        }\n        regret = run_simulation(m=m, T=T, eta=case[\"eta\"], loss_config=loss_config)\n        results.append(regret)\n\n    # Format the output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}