{
    "hands_on_practices": [
        {
            "introduction": "The most celebrated result of the Barabási-Albert model is that it generates a network with a power-law degree distribution. This exercise  guides you through the classic mean-field derivation to obtain the stationary distribution $P(k)$. By then verifying the distribution's normalization and calculating its mean, you will connect the model's microscopic growth rules directly to its macroscopic, scale-free structure.",
            "id": "4141924",
            "problem": "Consider the Barabási-Albert (BA) model of network growth defined as follows: start from an initial connected seed of $m_{0}$ nodes. At each discrete time step $t \\mapsto t+1$, introduce one new node and attach it to $m$ existing nodes by $m$ new edges, where each of the $m$ targets is chosen independently with probability proportional to its current degree. Denote by $N_{k}(t)$ the expected number of nodes of degree $k$ at time $t$, by $E(t)$ the number of edges at time $t$, and by $P(k)$ the large-$t$ stationary degree distribution, defined by $P(k) = \\lim_{t \\to \\infty} N_{k}(t)/N(t)$, where $N(t)$ is the number of nodes at time $t$.\n\nStarting only from:\n- the preferential attachment rule, namely that the probability that a given existing node $i$ of degree $k_{i}(t)$ receives one of the $m$ new edges at time $t$ is $k_{i}(t)/\\sum_{j} k_{j}(t)$,\n- the conservation law $\\sum_{j} k_{j}(t) = 2 E(t)$, and the fact that $E(t)$ grows as $E(t+1) = E(t) + m$,\nderive from first principles a closed-form expression for the stationary degree distribution $P(k)$ for $k \\ge m$. Then, using your derived $P(k)$, compute the two quantities\n$$S_{1} = \\sum_{k=m}^{\\infty} P(k) \\quad \\text{and} \\quad S_{2} = \\sum_{k=m}^{\\infty} k\\,P(k),$$\nand interpret $S_{2}$ in terms of the expected average degree $\\langle k \\rangle$ of the network in the large-$t$ limit.\n\nYour final answer must be a single analytic expression containing $S_{1}$ and $S_{2}$ arranged as a row matrix. No numerical rounding is required.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-contained. It is a canonical problem in the study of complex networks. We may proceed with the solution.\n\nThe derivation of the stationary degree distribution $P(k)$ for the Barabási-Albert (BA) model is based on a rate equation approach, often termed the continuum or mean-field approximation. We analyze the change in the expected number of nodes of degree $k$, denoted by $N_k(t)$, over time.\n\nFirst, let's establish the total number of nodes $N(t)$ and edges $E(t)$ at a large time $t$. The network starts with $m_0$ nodes at $t=0$. At each time step, one new node is added. Thus, the total number of nodes at time $t$ is $N(t) = m_0 + t$. For large $t$, we can approximate $N(t) \\approx t$.\nAt each step, $m$ new edges are added. So, the number of edges grows as $E(t+1) = E(t) + m$. This implies that for large $t$, the total number of edges is $E(t) \\approx m t$.\nThe sum of degrees over all nodes is given by the conservation law $\\sum_{j} k_j(t) = 2 E(t)$. Using the large-$t$ approximation for $E(t)$, we have $\\sum_{j} k_j(t) \\approx 2mt$.\n\nThe probability $\\Pi_i$ that a new edge attaches to a specific existing node $i$ with degree $k_i(t)$ is given by the preferential attachment rule:\n$$ \\Pi_i = \\frac{k_i(t)}{\\sum_{j} k_j(t)} \\approx \\frac{k_i(t)}{2mt} $$\nSince we are interested in the expected number of nodes of a certain degree, we consider the overall probability that a new edge attaches to *any* node of degree $k$. There are $N_k(t)$ such nodes. Assuming that all nodes of the same degree are statistically equivalent, the probability that a particular new edge attaches to any node of degree $k$ is:\n$$ \\Pi(k) = \\sum_{i \\text{ with } k_i=k} \\Pi_i = \\sum_{i \\text{ with } k_i=k} \\frac{k}{2mt} = \\frac{k N_k(t)}{2mt} $$\nAt each time step, $m$ new edges are added. The expected number of new edges connecting to nodes of degree $k$ is $m \\times \\Pi(k)$:\n$$ \\Delta N_{\\text{attach}}(k) = m \\frac{k N_k(t)}{2mt} = \\frac{k N_k(t)}{2t} $$\n\nNow, we can write the rate equation for $N_k(t)$. The number of nodes with degree $k$ changes due to two processes:\n1.  A node of degree $k-1$ gains a link and becomes a node of degree $k$. This increases $N_k(t)$. The rate of this process is the rate at which nodes of degree $k-1$ gain links, which is $\\frac{(k-1) N_{k-1}(t)}{2t}$.\n2.  A node of degree $k$ gains a link and becomes a node of degree $k+1$. This decreases $N_k(t)$. The rate of this process is $\\frac{k N_k(t)}{2t}$.\n\nFor $k  m$, the rate equation is:\n$$ N_k(t+1) - N_k(t) = \\frac{(k-1) N_{k-1}(t)}{2t} - \\frac{k N_k(t)}{2t} $$\nFor $k=m$, the situation is different. At each time step, one new node is introduced with degree $m$. This provides a source term of $1$. Nodes of degree $m$ can also gain links and become degree $m+1$. So, for $k=m$:\n$$ N_m(t+1) - N_m(t) = 1 - \\frac{m N_m(t)}{2t} $$\nWe now employ the continuum approximation, where we treat $t$ as a continuous variable and replace the difference $N_k(t+1) - N_k(t)$ with the derivative $\\frac{d N_k(t)}{dt}$.\nThe stationary degree distribution is defined as $P(k) = \\lim_{t \\to \\infty} \\frac{N_k(t)}{N(t)}$. In the large-$t$ limit, $N(t) \\approx t$, so we can assume a time-invariant solution of the form $N_k(t) = P(k) N(t) \\approx P(k)t$. Differentiating with respect to $t$ gives $\\frac{d N_k(t)}{dt} = P(k)$.\n\nSubstituting these into the rate equations:\nFor $k  m$:\n$$ P(k) = \\frac{(k-1) (t P(k-1))}{2t} - \\frac{k (t P(k))}{2t} $$\n$$ P(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k) $$\n$$ P(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1) $$\n$$ P(k) \\left(\\frac{k+2}{2}\\right) = \\frac{k-1}{2} P(k-1) $$\nThis yields the recurrence relation:\n$$ P(k) = \\frac{k-1}{k+2} P(k-1) \\quad \\text{for } k  m $$\n\nFor $k=m$:\n$$ P(m) = 1 - \\frac{m (t P(m))}{2t} $$\n$$ P(m) = 1 - \\frac{m}{2} P(m) $$\n$$ P(m) \\left(1 + \\frac{m}{2}\\right) = 1 $$\n$$ P(m) = \\frac{1}{1 + m/2} = \\frac{2}{m+2} $$\nThis gives the value of $P(m)$, which we can use to solve the recurrence relation for $P(k)$ where $k \\ge m$.\n$$ P(k) = P(k-1) \\frac{k-1}{k+2} = P(k-2) \\frac{k-2}{k+1} \\frac{k-1}{k+2} = \\dots = P(m) \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} $$\nLet's evaluate the product term:\n$$ \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m)(m+1)\\dots(k-1)}{(m+3)(m+4)\\dots(k+2)} $$\nThis can be written using factorials:\n$$ \\frac{(k-1)!/(m-1)!}{(k+2)!/(m+2)!} = \\frac{(k-1)!}{(m-1)!} \\frac{(m+2)!}{(k+2)!} = \\frac{(m+2)(m+1)m}{k(k+1)(k+2)} $$\nNow, we substitute the expressions for $P(m)$ and the product:\n$$ P(k) = \\left(\\frac{2}{m+2}\\right) \\left(\\frac{m(m+1)(m+2)}{k(k+1)(k+2)}\\right) $$\n$$ P(k) = \\frac{2m(m+1)}{k(k+1)(k+2)} $$\nThis is the closed-form expression for the stationary degree distribution for $k \\ge m$.\n\nNow we compute $S_1$ and $S_2$.\n$$ S_1 = \\sum_{k=m}^{\\infty} P(k) = \\sum_{k=m}^{\\infty} \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)} $$\nWe use partial fraction decomposition for the term in the sum:\n$$ \\frac{1}{k(k+1)(k+2)} = \\frac{A}{k} + \\frac{B}{k+1} + \\frac{C}{k+2} $$\nSolving for the coefficients gives $A=1/2$, $B=-1$, $C=1/2$. So:\n$$ \\frac{1}{k(k+1)(k+2)} = \\frac{1}{2k} - \\frac{1}{k+1} + \\frac{1}{2(k+2)} = \\frac{1}{2} \\left[ \\left(\\frac{1}{k} - \\frac{1}{k+1}\\right) - \\left(\\frac{1}{k+1} - \\frac{1}{k+2}\\right) \\right] $$\nLet $h(k) = \\frac{1}{k} - \\frac{1}{k+1}$. The term is $\\frac{1}{2}[h(k) - h(k+1)]$. The sum becomes a telescoping series:\n$$ \\sum_{k=m}^{\\infty} \\frac{1}{2}[h(k) - h(k+1)] = \\frac{1}{2} \\left[ (h(m)-h(m+1)) + (h(m+1)-h(m+2)) + \\dots \\right] $$\nThe sum converges to $\\frac{1}{2} h(m)$.\n$$ \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)} = \\frac{1}{2}h(m) = \\frac{1}{2}\\left(\\frac{1}{m} - \\frac{1}{m+1}\\right) = \\frac{1}{2} \\frac{m+1-m}{m(m+1)} = \\frac{1}{2m(m+1)} $$\nSubstituting this back into the expression for $S_1$:\n$$ S_1 = 2m(m+1) \\left( \\frac{1}{2m(m+1)} \\right) = 1 $$\nThis result is expected, as $P(k)$ for $k \\ge m$ must be a properly normalized probability distribution (for large networks where the number of nodes with degree less than $m$ is negligible).\n\nNext, we compute $S_2$:\n$$ S_2 = \\sum_{k=m}^{\\infty} k P(k) = \\sum_{k=m}^{\\infty} k \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{(k+1)(k+2)} $$\nAgain, we use partial fractions: $\\frac{1}{(k+1)(k+2)} = \\frac{1}{k+1} - \\frac{1}{k+2}$. This leads to another telescoping series:\n$$ \\sum_{k=m}^{\\infty} \\left( \\frac{1}{k+1} - \\frac{1}{k+2} \\right) = \\left(\\frac{1}{m+1} - \\frac{1}{m+2}\\right) + \\left(\\frac{1}{m+2} - \\frac{1}{m+3}\\right) + \\dots $$\nThis sum converges to the first term, $\\frac{1}{m+1}$.\nSubstituting this back into the expression for $S_2$:\n$$ S_2 = 2m(m+1) \\left( \\frac{1}{m+1} \\right) = 2m $$\n\nFinally, we interpret $S_2$. The expected average degree of the network is defined as $\\langle k \\rangle = \\sum_{k} k P(k)$. In the large-$t$ limit, the fraction of nodes with degree smaller than $m$ (which can only be nodes from the initial seed) becomes negligible. Therefore, the sum can be taken from $k=m$ to $\\infty$.\n$$ \\langle k \\rangle_{\\text{large-}t} = \\sum_{k=m}^{\\infty} k P(k) = S_2 $$\nSo, $S_2$ represents the expected average degree of the network in the large-time limit. This can be verified from first principles. The average degree is $\\langle k \\rangle(t) = \\frac{\\sum_j k_j(t)}{N(t)} = \\frac{2E(t)}{N(t)}$. In the large-$t$ limit, $E(t) \\approx mt$ and $N(t) \\approx t$.\n$$ \\lim_{t \\to \\infty} \\langle k \\rangle(t) = \\lim_{t \\to \\infty} \\frac{2mt}{t} = 2m $$\nThis confirms our calculation that $S_2 = 2m$ and its interpretation as the average degree of the network.\n\nThe required answer is a row matrix containing $S_1$ and $S_2$.\n$S_1 = 1$\n$S_2 = 2m$\nThe final answer is $(1, 2m)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  2m\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While theoretical derivations provide deep insights, simulating a model is essential for exploring its properties and comparing it to real-world systems. This practice  challenges you to implement an efficient algorithm for generating a BA network, focusing on the core task of sampling nodes with probabilities proportional to their degrees. This computational skill is fundamental for any practitioner in the field of network science.",
            "id": "4141898",
            "problem": "You are to design and implement an efficient simulation algorithm to generate a Barabási–Albert (BA) network with preferential attachment. The BA network grows by adding nodes one by one, where each new node connects to existing nodes with probability proportional to their current degree. Your task is to produce a program that constructs such a network for specified parameters and returns verifiable summary statistics for a small test suite.\n\nThe fundamental base consists of the following standard definitions and facts.\n\n- A simple undirected network on $N$ nodes has a degree sequence $\\{k_i\\}_{i=0}^{N-1}$, where $k_i$ is the number of edges incident to node $i$. The Handshake Lemma states $\\sum_{i=0}^{N-1} k_i = 2E$, where $E$ is the number of edges.\n- The Barabási–Albert preferential attachment mechanism grows an undirected network starting from an initial connected seed of $m_0$ nodes. At each discrete time step $t \\in \\{m_0, m_0+1, \\dots, N-1\\}$, a new node enters and forms $m$ edges to $m$ distinct existing nodes. The probability that the new node attaches to an existing node $i$ is proportional to the current degree $k_i$, i.e., $P(i) = k_i / \\sum_j k_j$ over the existing nodes $j$.\n- To avoid degenerate zero-probability denominators at initialization and to ensure a connected starting seed with strictly positive degrees, take $m_0 = \\max(m, 2)$ and initialize the seed as a clique on $m_0$ nodes. The initial number of edges is $E_{\\mathrm{init}} = \\frac{m_0(m_0-1)}{2}$, and the initial degree of each seed node is $m_0 - 1$.\n\nAlgorithmic requirements.\n\n- The implementation must maintain dynamic sampling of existing nodes in time $O(\\log N)$ per sample, proportional to current degrees. You must implement a Binary Indexed Tree (BIT; also known as a Fenwick tree) or an equivalent data structure that supports:\n  1. Point updates to node weights (degrees) in $O(\\log N)$.\n  2. Prefix-sum queries in $O(\\log N)$.\n  3. Inverse-transform sampling by cumulative weight: given a target $u \\in [0, \\sum_j k_j)$, find the smallest index $i$ such that the cumulative sum up to $i$ strictly exceeds $u$, in $O(\\log N)$.\n- Within a single time step when attaching $m$ edges for the new node, you must sample without replacement among existing nodes. Use temporary zeroing of selected targets’ weights in the Binary Indexed Tree to prevent multiple edges to the same target within that step, then restore and increment their weights after the $m$ targets have been chosen. Self-loops are disallowed.\n\nSimulation inputs and outputs.\n\n- For each parameter pair $(N, m)$ with $N \\in \\mathbb{Z}_{\\ge 1}$ and $m \\in \\mathbb{Z}_{\\ge 1}$, construct the network using the above process with $m_0 = \\max(m, 2)$. Do not store edges explicitly; it suffices to maintain the degree sequence $\\{k_i\\}$. Use deterministic randomness by seeding the pseudorandom number generator explicitly as specified in the test suite.\n- For each simulation, compute:\n  1. The total number of edges $E$.\n  2. The maximum degree $k_{\\max} = \\max_i k_i$.\n  3. The scale-normalized maximum degree ratio $r = \\dfrac{k_{\\max}}{m \\sqrt{N}}$.\n  4. A boolean flag $H$ indicating whether the Handshake Lemma holds exactly for the computed degree sequence, i.e., whether $\\sum_i k_i = 2E$.\n\nTest suite.\n\nRun your program for the following three test cases, each with a specified pseudorandom seed to ensure reproducibility. In each case, initialize $m_0 = \\max(m, 2)$ and the $m_0$-node seed as a clique.\n\n- Case A: $(N, m, \\text{seed}) = (1000, 1, 42)$.\n- Case B: $(N, m, \\text{seed}) = (5000, 3, 7)$.\n- Case C: $(N, m, \\text{seed}) = (3, 3, 123)$.\n\nFinal output format.\n\n- Your program should produce a single line of output containing the results as a comma-separated list of three lists, one per test case, in the order A, B, C. Each inner list must be of the form $[E, k_{\\max}, r, H]$, where $E$ and $k_{\\max}$ are integers, $r$ is a real number rounded to six digits after the decimal point, and $H$ is a boolean. For example, the output line should look like:\n  [ [EA,kmaxA,rA,H_A], [EB,kmaxB,rB,H_B], [EC,kmaxC,rC,H_C] ]\nUse no extra whitespace beyond what is required for valid Python list formatting. Angles do not apply, and there are no physical units in this problem. All results are to be returned as pure numbers or booleans as specified.",
            "solution": "The user wants me to act as a meticulous STEM professor to validate and solve a problem on simulating a Barabási-Albert (BA) network.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n- **Network Model**: Simple undirected graph growing from $m_0$ to $N$ nodes.\n- **Growth Process**: At each step $t \\in \\{m_0, \\dots, N-1\\}$, a new node is added, forming $m$ edges to $m$ distinct existing nodes.\n- **Attachment Rule**: The probability of a new node attaching to an existing node $i$ is $P(i) = k_i / \\sum_j k_j$, where $k_i$ is the degree of node $i$.\n- **Initial Conditions**: The network starts with a seed of $m_0 = \\max(m, 2)$ nodes arranged as a clique. The initial number of edges is $E_{\\mathrm{init}} = \\frac{m_0(m_0-1)}{2}$, and the initial degree of each seed node is $k_i = m_0-1$.\n- **Algorithmic Constraint**: Implementation must use a data structure, such as a Binary Indexed Tree (BIT), that supports point updates, prefix-sum queries, and inverse-transform sampling in $O(\\log N)$ time.\n- **Sampling Constraint**: When adding $m$ edges for a new node, sampling of target nodes must be without replacement. The suggested method is to temporarily zero out the weights of selected targets in the BIT.\n- **Outputs**: For each test case, compute:\n    1. Total edges $E$.\n    2. Maximum degree $k_{\\max}$.\n    3. Scale-normalized maximum degree ratio $r = \\frac{k_{\\max}}{m \\sqrt{N}}$.\n    4. Handshake Lemma validity flag $H$, where $H$ is true if $\\sum_i k_i = 2E$.\n- **Test Suite**:\n    - Case A: $(N, m, \\text{seed}) = (1000, 1, 42)$.\n    - Case B: $(N, m, \\text{seed}) = (5000, 3, 7)$.\n    - Case C: $(N, m, \\text{seed}) = (3, 3, 123)$.\n- **Reproducibility**: The pseudorandom number generator (PRNG) must be seeded for each case.\n- **Output Format**: A single-line string representing a list of lists: `[[EA,kmaxA,rA,H_A],[EB,kmaxB,rB,H_B],[EC,kmaxC,rC,H_C]]`, with $r$ rounded to 6 decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem describes the standard Barabási-Albert model, a fundamental concept in network science. The preferential attachment mechanism is a well-established principle. The use of a BIT for efficient dynamic sampling is a standard and sound algorithmic technique for simulating such processes. The problem is firmly rooted in computational science and complex systems modeling.\n- **Well-Posed**: The problem is well-defined. All parameters ($N$, $m$), initial conditions (clique of size $m_0 = \\max(m,2)$), and dynamic rules (preferential attachment, sampling without replacement) are specified without ambiguity. The use of a fixed PRNG seed for each test case ensures that the simulation is deterministic, leading to a unique, verifiable solution.\n- **Objective**: The problem is stated in precise, objective, and technical language.\n- **Incomplete or Contradictory Setup**: The problem specification is complete and internally consistent. The definition of $m_0 = \\max(m, 2)$ correctly handles cases like $m=1$, ensuring the initial seed network is connected and has non-zero degrees, which is essential for the preferential attachment probability to be well-defined.\n- **Unrealistic or Infeasible**: The computational task is feasible. The largest network has $N=5000$ nodes. An efficient implementation using a BIT, as required, would have a time complexity of approximately $O(N \\cdot m \\cdot (\\log N)^2)$ for the growth phase (where one $\\log N$ comes from the BIT operations and the other from the binary search for the target node), which is well within the limits of typical modern hardware.\n- **Ill-Posed or Poorly Structured**: The problem is well-structured, with a clear objective and a clearly defined path to a solution. The expected outputs are unambiguous.\n- **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a correct implementation of both the BA growth model and an efficient, non-trivial data structure (the BIT), including the logic for inverse transform sampling and sampling without replacement. Case C, while simple, serves as a valid check of the initialization logic.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, objective, and computationally feasible. I will proceed with a full solution.\n\n### Solution\n\nThe problem requires the design and implementation of an algorithm to simulate the growth of a Barabási-Albert (BA) network. The solution must adhere to specific algorithmic and performance requirements, centered on the principle of preferential attachment.\n\n**Principle-Based Design**\n\nThe BA model is a dynamic process where the network grows one node at a time. The defining characteristic is preferential attachment: a new node is more likely to connect to existing nodes that already have a high degree. This \"rich-get-richer\" phenomenon gives rise to scale-free degree distributions, a hallmark of many real-world networks.\n\n**1. Algorithmic Core: Efficient Weighted Sampling**\n\nThe core computational challenge lies in efficiently sampling target nodes according to the attachment probability $P(i) = k_i / \\sum_j k_j$. The degree distribution $\\{k_i\\}$ changes at every time step, so the sampling mechanism must be dynamic. A naive approach of creating a weighted list of all nodes and sampling from it is highly inefficient, as the list would have to be rebuilt at each step.\n\nA more sophisticated approach is inverse transform sampling. We can conceptualize the degrees of all existing nodes $\\{k_0, k_1, \\dots, k_{t-1}\\}$ as forming a discrete probability distribution. The total weight is the sum of all degrees, $\\sum k_j = S$. We can sample a value $u$ uniformly from the interval $[0, S)$. The chosen target node $i$ is the one whose degree \"covers\" the value $u$; formally, it is the smallest index $i$ such that $\\sum_{j=0}^{i} k_j  u$.\n\nTo implement this efficiently, we require a data structure that can:\n1.  Rapidly calculate prefix sums of degrees ($\\sum_{j=0}^{i} k_j$).\n2.  Be updated quickly when a node's degree changes.\n3.  Efficiently find the index $i$ corresponding to a given cumulative value $u$.\n\nThe **Binary Indexed Tree (BIT)**, or Fenwick Tree, is an ideal data structure for this task. It supports both point updates (changing a single $k_i$) and prefix sum queries in $O(\\log N)$ time, where $N$ is the maximum number of nodes.\n\n**2. Binary Indexed Tree Implementation**\n\nA BIT will be implemented to store the degrees of the nodes. Its key operations will be:\n- `update(index, delta)`: Adds `delta` to the degree of the node at `index`, propagating this change through the tree structure in $O(\\log N)$ time.\n- `query(index)`: Computes the cumulative degree sum from node $0$ to `index` in $O(\\log N)$ time.\n- `find_target(u)`: This function performs the inverse transform sampling. It uses the `query` function within a binary search over the node indices $\\{0, \\dots, N-1\\}$ to find the smallest index $i$ for which the cumulative degree sum `query(i)` is strictly greater than the random value $u$. This search takes $O(\\log N)$ iterations, and each involves an $O(\\log N)$ query, leading to an overall complexity of $O((\\log N)^2)$ for finding a single target.\n\n**3. Simulation Algorithm**\n\nThe simulation proceeds as follows for a given set of parameters $(N, m)$:\n\n**a. Initialization:**\n- The network begins with a seed of $m_0 = \\max(m, 2)$ nodes.\n- We initialize a degree array `degrees` of size $N$ and a BIT of the same size.\n- To form the initial $m_0$-clique, we establish an edge between every pair of the first $m_0$ nodes.\n- The initial degree of each of these $m_0$ nodes is $k_i = m_0 - 1$.\n- The `degrees` array and the BIT are updated accordingly: for each $i \\in \\{0, \\dots, m_0-1\\}$, we set `degrees[i] = m_0 - 1` and call `bit.update(i, m_0 - 1)`.\n- The initial number of edges is $E = \\frac{m_0(m_0-1)}{2}$.\n\n**b. Network Growth:**\n- The main simulation loop iterates for each new node to be added, indexed from $t = m_0$ to $N-1$.\n- At each step $t$, we add the new node `t` and form $m$ edges to $m$ distinct existing nodes.\n- **Sampling Without Replacement**: To select $m$ distinct targets, we follow the procedure mandated by the problem:\n    1.  A list, `selected_targets`, is initialized to store the chosen nodes for the current step.\n    2.  A list, `temp_modifications`, is used to track the temporary changes made to the BIT.\n    3.  We loop $m$ times. In each iteration:\n        i.   Calculate the current total degree sum, $S$, by querying the BIT: $S = \\text{bit.query}(t-1)$.\n        ii.  Generate a uniform random number $u \\in [0, S)$.\n        iii. Use the `find_target(u)` function to find the target node index, `target`.\n        iv.  Add `target` to `selected_targets`.\n        v.   To prevent this node from being selected again in this step, its weight in the BIT is temporarily set to zero. We find its current weight in the BIT (which might have been affected by previous selections in the same step) and subtract it. This operation, `(target, weight_removed)`, is stored in `temp_modifications`.\n    4.  After selecting all $m$ targets, we restore the BIT to its state at the beginning of step $t$ by reversing the temporary modifications. For each `(target, weight_removed)` in `temp_modifications`, we call `bit.update(target, weight_removed)`.\n    5.  Finally, we apply the permanent updates for step $t$:\n        - For each distinct node `v` in `selected_targets`, its degree increases by one. We update `degrees[v] += 1` and `bit.update(v, 1)`.\n        - The new node `t` has degree $m$. We set `degrees[t] = m` and `bit.update(t, m)`.\n        - The total number of edges is incremented by $m$: $E \\leftarrow E + m$.\n\n**c. Final Calculations:**\n- After the growth loop completes, the `degrees` array contains the final degree sequence of the network of $N$ nodes.\n- The total number of edges, $E$, has been tracked throughout the simulation.\n- The maximum degree is found by $k_{\\max} = \\max(\\text{degrees})$.\n- The normalized ratio is calculated as $r = \\frac{k_{\\max}}{m \\sqrt{N}}$.\n- The Handshake Lemma is verified by checking if the sum of all computed degrees, $\\sum_{i=0}^{N-1} k_i$, is exactly equal to $2E$. The result is stored in the boolean flag $H$.\n\nThis design faithfully implements the Barabási-Albert model while satisfying all algorithmic and performance requirements. The use of a fixed PRNG seed ensures determinism and reproducibility for the given test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass BIT:\n    \"\"\"Binary Indexed Tree (Fenwick Tree) for 0-indexed arrays.\"\"\"\n    \n    def __init__(self, size):\n        \"\"\"Initializes a BIT of a given size.\"\"\"\n        self.size = size\n        self.tree = np.zeros(size + 1, dtype=np.int64)\n\n    def update(self, index, delta):\n        \"\"\"Adds delta to the element at index.\"\"\"\n        i = index + 1\n        while i = self.size:\n            self.tree[i] += delta\n            i += i  -i\n\n    def query(self, index):\n        \"\"\"Computes the prefix sum up to index (inclusive).\"\"\"\n        if index  0:\n            return 0\n        s = 0\n        i = index + 1\n        while i  0:\n            s += self.tree[i]\n            i -= i  -i\n        return s\n\n    def get_value(self, index):\n        \"\"\"Gets the exact value at a given index.\"\"\"\n        return self.query(index) - self.query(index - 1)\n\n    def find_target(self, cumulative_freq):\n        \"\"\"\n        Finds the smallest index i such that the cumulative frequency (prefix sum)\n        up to i is strictly greater than cumulative_freq.\n        This is implemented using a binary search over the indices.\n        Complexity: O((log N)^2)\n        \"\"\"\n        low, high = 0, self.size - 1\n        target_index = -1\n        \n        while low = high:\n            mid = (low + high) // 2\n            if self.query(mid)  cumulative_freq:\n                target_index = mid\n                high = mid - 1\n            else:\n                low = mid + 1\n        \n        return target_index\n\ndef run_simulation(N, m, seed):\n    \"\"\"\n    Runs a single Barabási-Albert network simulation.\n    \n    Args:\n        N (int): Total number of nodes in the final network.\n        m (int): Number of edges each new node adds.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        list: A list containing [E, k_max, r, H].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    m0 = max(m, 2)\n\n    # Handle the trivial case where N = m0\n    if N = m0:\n        degrees = np.zeros(N, dtype=int)\n        for i in range(N):\n            degrees[i] = N - 1\n        \n        total_edges = N * (N - 1) // 2\n        max_degree = N - 1 if N  0 else 0\n        if N  0 and m  0:\n             ratio = max_degree / (m * np.sqrt(N))\n        else:\n             ratio = 0.0\n        \n        handshake_holds = (np.sum(degrees) == 2 * total_edges)\n        return [total_edges, int(max_degree), ratio, handshake_holds]\n\n    # ------ Initialization ------\n    degrees = np.zeros(N, dtype=int)\n    bit = BIT(N)\n    \n    # Initialize the m0-clique\n    initial_degree = m0 - 1\n    total_edges = m0 * (m0 - 1) // 2\n    \n    for i in range(m0):\n        degrees[i] = initial_degree\n        bit.update(i, initial_degree)\n        \n    # ------ Network Growth ------\n    for t in range(m0, N): # t is the index of the new node\n        \n        selected_targets = []\n        temp_modifications = []\n        num_existing_nodes = t\n\n        for _ in range(m):\n            total_degree_sum = bit.query(num_existing_nodes - 1)\n            if total_degree_sum == 0:\n                # This should not happen in a valid BA model with m = 1 and connected seed\n                break\n            \n            rand_val = rng.uniform(0, total_degree_sum)\n            target_node = bit.find_target(rand_val)\n            \n            selected_targets.append(target_node)\n            \n            # Temporarily zero out the weight of the selected node\n            weight_in_bit = bit.get_value(target_node)\n            bit.update(target_node, -weight_in_bit)\n            temp_modifications.append((target_node, weight_in_bit))\n\n        # Restore BIT to pre-selection state\n        for node_idx, weight in temp_modifications:\n            bit.update(node_idx, weight)\n        \n        # Apply permanent updates for the current step\n        for target_node in selected_targets:\n            degrees[target_node] += 1\n            bit.update(target_node, 1)\n            \n        degrees[t] = m\n        bit.update(t, m)\n        total_edges += m\n\n    # ------ Final Calculations ------\n    max_degree = np.max(degrees) if N  0 else 0\n    ratio = max_degree / (m * np.sqrt(N))\n    handshake_holds = (np.sum(degrees) == 2 * total_edges)\n\n    return [int(total_edges), int(max_degree), ratio, handshake_holds]\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs simulations, and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        # (N, m, seed)\n        (1000, 1, 42),\n        (5000, 3, 7),\n        (3, 3, 123),\n    ]\n\n    all_results = []\n    for N, m, seed in test_cases:\n        result = run_simulation(N, m, seed)\n        \n        # Format the result list elements\n        E, k_max, r, H = result\n        r_str = f\"{r:.6f}\"\n        H_str = str(H)\n        \n        inner_list_str = f\"[{E},{k_max},{r_str},{H_str}]\"\n        all_results.append(inner_list_str)\n\n    # Final print statement in the exact required format.\n    final_output_str = f\"[{','.join(all_results)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In empirical studies, identifying a true power-law distribution can be challenging, as other heavy-tailed distributions can appear similar. This exercise  addresses the critical task of model selection, asking you to devise a statistical method to distinguish a power-law tail from a lognormal tail. Developing this rigor is crucial for making valid claims about the mechanisms, like preferential attachment, that might underlie an observed network.",
            "id": "4141881",
            "problem": "A network has large size $n$ and degree variable $K$. Two candidate models are posited for the high-degree tail of the degree distribution: a power law with exponent $\\gamma$ and a lognormal with parameters $(\\mu,\\sigma)$. Let $\\overline{F}(k) = \\mathbb{P}(K \\ge k)$ denote the complementary cumulative distribution function (CCDF). The power law model assumes $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$ for $k \\ge k_{\\min}$, which implies a regularly varying tail for $\\overline{F}(k)$ with index $\\alpha = \\gamma - 1$. The lognormal model assumes $\\ln K \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where $\\mathcal{N}$ denotes the normal distribution. From these core definitions, and standard properties of the normal distribution, reason about the asymptotic behavior of $\\overline{F}(k)$ at high $k$ under each model and how this behavior transforms under a fixed multiplicative rescaling of $k$. Based on that, identify a statistically principled procedure that, using only data from the high-degree tail $k \\ge k_{\\min}$ in a complex adaptive system setting, can distinguish between these two models. Which option correctly characterizes the comparative tail behavior at high $k$ and proposes an appropriate test?\n\nA. For a power law, $\\overline{F}(k)$ is regularly varying, so for any fixed $c>1$, the ratio $R(k;c) = \\overline{F}(c k)/\\overline{F}(k)$ converges to a constant $c^{-(\\gamma-1)}$ as $k \\to \\infty$. For a lognormal, $R(k;c)$ decays to $0$ as $k \\to \\infty$ for any fixed $c>1$. A practical test is to estimate $R(k;c)$ over a range of large $k$, regress $\\ln R(k;c)$ on $\\ln k$, and test $H_0$: slope $=0$ (consistent with a power law) against $H_1$: slope $0$ (consistent with a lognormal), using tail-only data.\n\nB. At high $k$, a lognormal tail decays more slowly than any power law, so $R(k;c)$ is asymptotically constant for both models. A practical test is to compute $R(k;c)$ at a single large $k$ and accept the model with the larger constant ratio.\n\nC. Both tails are asymptotically indistinguishable because they are heavy-tailed. A practical test is to fit both models to the entire degree distribution (not just the tail) and select using the Kolmogorov-Smirnov (KS) statistic evaluated over all $k$.\n\nD. A moment-based test suffices: compute the sample variance of degrees above $k_{\\min}$ and accept the lognormal if it is finite and the power law otherwise, since power laws have infinite variance for any $\\gamma$.",
            "solution": "The problem asks for a statistically principled method to distinguish between a power-law and a lognormal distribution, focusing on their respective tail behaviors for large values of the random variable $K$. The method must use only data from the high-degree tail, defined as $k \\ge k_{\\min}$.\n\nI shall first validate the problem statement.\n\n### Step 1: Extract Givens\n-   Network size $n$ is large.\n-   Degree is a random variable $K$.\n-   Model 1 (Power Law):\n    -   Probability mass function (PMF): $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$ for $k \\ge k_{\\min}$.\n    -   The complementary cumulative distribution function (CCDF), $\\overline{F}(k) = \\mathbb{P}(K \\ge k)$, has a regularly varying tail with index $\\alpha = \\gamma - 1$.\n-   Model 2 (Lognormal):\n    -   The natural logarithm of the variable is normally distributed: $\\ln K \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n-   The task is to analyze the asymptotic behavior of $\\overline{F}(k)$ as $k \\to \\infty$ for both models, particularly under a multiplicative rescaling $k \\to ck$ for some constant $c  1$, and to identify a distinguishing statistical test based on this analysis.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: The distinction between power-law and lognormal distributions is a fundamental and well-studied problem in statistical physics, computer science, and complex systems modeling. The definitions provided for both distributions are standard. The concept of regular variation is the mathematically formal basis for \"scale-free\" behavior.\n-   **Well-Posedness**: The problem defines two distinct, well-understood models and asks for a comparative analysis of their asymptotic properties, which is a tractable mathematical exercise. The question is precise and seeks a specific type of procedure.\n-   **Objectivity**: The problem is articulated in formal mathematical language, free from subjective or ambiguous terms.\n-   **Flaws**: There are no identifiable flaws. The premises are factually sound. For a discrete power-law PMF $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$, the CCDF for large $k$ is well approximated by $\\overline{F}(k) \\approx \\int_k^\\infty x^{-\\gamma} dx \\propto k^{-(\\gamma-1)}$, which is a regularly varying function with index $-(\\gamma-1)$. The problem's statement that the index is $\\alpha = \\gamma-1$ is consistent with this, where $\\alpha$ is the magnitude of the exponent.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the derivation and analysis.\n\n### Derivation of Asymptotic Tail Behavior\n\nThe core of the problem lies in the asymptotic behavior of the ratio $R(k;c) = \\frac{\\overline{F}(ck)}{\\overline{F}(k)}$ for a fixed constant $c  1$ as $k \\to \\infty$.\n\n**1. Power-Law Model**\nThe problem states that for a power law, $\\overline{F}(k)$ is a regularly varying function with index $-\\alpha = -(\\gamma-1)$. By the definition of a regularly varying function, for any constant $c  0$, the following limit holds:\n$$ \\lim_{k \\to \\infty} \\frac{\\overline{F}(ck)}{\\overline{F}(k)} = c^{-\\alpha} $$\nSubstituting $\\alpha = \\gamma - 1$, we have:\n$$ \\lim_{k \\to \\infty} R(k;c) = \\lim_{k \\to \\infty} \\frac{\\overline{F}(ck)}{\\overline{F}(k)} = c^{-(\\gamma - 1)} $$\nThis limit is a positive constant that depends on $c$ and $\\gamma$ but is independent of $k$. This property is the definition of a scale-free tail. Taking the natural logarithm:\n$$ \\lim_{k \\to \\infty} \\ln(R(k;c)) = \\ln(c^{-(\\gamma-1)}) = -(\\gamma-1)\\ln(c) $$\nThe logarithm of the ratio converges to a constant. This implies that a plot of $\\ln(R(k;c))$ versus $\\ln(k)$ would be asymptotically a horizontal line.\n\n**2. Lognormal Model**\nFor the lognormal model, $\\ln K$ follows a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$. The CCDF is:\n$$ \\overline{F}(k) = \\mathbb{P}(K \\ge k) = \\mathbb{P}(\\ln K \\ge \\ln k) $$\nLet $X = \\ln K$. Then $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The standardized variable is $Z = (X - \\mu)/\\sigma \\sim \\mathcal{N}(0,1)$.\n$$ \\overline{F}(k) = \\mathbb{P}\\left(Z \\ge \\frac{\\ln k - \\mu}{\\sigma}\\right) = \\overline{\\Phi}\\left(\\frac{\\ln k - \\mu}{\\sigma}\\right) $$\nwhere $\\overline{\\Phi}(z)$ is the CCDF of the standard normal distribution. For large $z$, a standard asymptotic approximation (an application of L'Hôpital's rule, often called Mill's ratio) is:\n$$ \\overline{\\Phi}(z) \\approx \\frac{1}{z\\sqrt{2\\pi}} e^{-z^2/2} $$\nAs $k \\to \\infty$, $z(k) = (\\ln k - \\mu)/\\sigma$ also goes to infinity. Substituting $z(k)$ into the approximation, the dominant term in the exponent determines the decay rate. The logarithm of the CCDF behaves as:\n$$ \\ln \\overline{F}(k) \\approx -\\frac{z(k)^2}{2} = -\\frac{1}{2\\sigma^2}(\\ln k - \\mu)^2 $$\nFor large $k$, this is dominated by the $(\\ln k)^2$ term:\n$$ \\ln \\overline{F}(k) \\approx -\\frac{(\\ln k)^2}{2\\sigma^2} $$\nNow we analyze the logarithm of the ratio $R(k;c)$:\n$$ \\ln R(k;c) = \\ln \\overline{F}(ck) - \\ln \\overline{F}(k) $$\n$$ \\ln R(k;c) \\approx \\left( -\\frac{(\\ln(ck))^2}{2\\sigma^2} \\right) - \\left( -\\frac{(\\ln k)^2}{2sigma^2} \\right) $$\n$$ \\ln R(k;c) \\approx -\\frac{1}{2\\sigma^2} \\left[ (\\ln c + \\ln k)^2 - (\\ln k)^2 \\right] $$\n$$ \\ln R(k;c) \\approx -\\frac{1}{2\\sigma^2} \\left[ (\\ln c)^2 + 2(\\ln c)(\\ln k) + (\\ln k)^2 - (\\ln k)^2 \\right] $$\n$$ \\ln R(k;c) \\approx -\\frac{1}{2\\sigma^2} \\left[ 2(\\ln c)(\\ln k) + (\\ln c)^2 \\right] $$\nAs $k \\to \\infty$, the term linear in $\\ln k$ dominates:\n$$ \\ln R(k;c) \\approx -\\frac{\\ln c}{\\sigma^2} \\ln k $$\nSince $c  1$ and $\\sigma^2  0$, the coefficient $-\\frac{\\ln c}{\\sigma^2}$ is a negative constant. This shows that $\\ln R(k;c)$ is asymptotically a linear function of $\\ln k$ with a negative slope. Consequently, as $k \\to \\infty$, $\\ln R(k;c) \\to -\\infty$, which means $R(k;c) \\to 0$.\n\nThis key difference—$R(k;c)$ converging to a constant for a power law versus $R(k;c)$ converging to $0$ for a lognormal—provides a basis for a statistical test. A regression of an empirical estimate of $\\ln R(k;c)$ on $\\ln k$ for large $k$ values should have a slope of $0$ for a power law and a negative slope for a lognormal.\n\n### Option-by-Option Analysis\n\n**A. For a power law, $\\overline{F}(k)$ is regularly varying, so for any fixed $c>1$, the ratio $R(k;c) = \\overline{F}(c k)/\\overline{F}(k)$ converges to a constant $c^{-(\\gamma-1)}$ as $k \\to \\infty$. For a lognormal, $R(k;c)$ decays to $0$ as $k \\to \\infty$ for any fixed $c>1$. A practical test is to estimate $R(k;c)$ over a range of large $k$, regress $\\ln R(k;c)$ on $\\ln k$, and test $H_0$: slope $=0$ (consistent with a power law) against $H_1$: slope $0$ (consistent with a lognormal), using tail-only data.**\nThis option perfectly matches the derived asymptotic behaviors. The first sentence correctly states the property of regular variation for power laws. The second sentence correctly states the faster-than-power-law decay of the lognormal tail, leading to $R_k(c) \\to 0$. The proposed test is a direct and sound statistical formalization of this difference. The regression of $\\ln R(k;c)$ on $\\ln k$ directly tests the asymptotic relationship we derived. The use of tail-only data is consistent with the problem's constraints.\n**Verdict: Correct.**\n\n**B. At high $k$, a lognormal tail decays more slowly than any power law, so $R(k;c)$ is asymptotically constant for both models. A practical test is to compute $R(k;c)$ at a single large $k$ and accept the model with the larger constant ratio.**\nThis option begins with a statement that is factually incorrect. A lognormal tail decays *faster* than any power law. To see this, consider the ratio of the lognormal CCDF to a power-law function $k^{-\\alpha}$: $\\overline{F}_{LN}(k) / k^{-\\alpha}$. The logarithm of this ratio is $\\ln \\overline{F}_{LN}(k) - \\ln(k^{-\\alpha}) \\approx -(\\ln k)^2/(2\\sigma^2) + \\alpha \\ln k = \\ln k (\\alpha - \\frac{\\ln k}{2\\sigma^2})$. As $k \\to \\infty$, this expression goes to $-\\infty$, meaning the ratio goes to $0$. The lognormal tail is \"sub-power-law\". The rest of the option is based on this false premise.\n**Verdict: Incorrect.**\n\n**C. Both tails are asymptotically indistinguishable because they are heavy-tailed. A practical test is to fit both models to the entire degree distribution (not just the tail) and select using the Kolmogorov-Smirnov (KS) statistic evaluated over all $k$.**\nThis option is incorrect on two counts. First, the premise that the tails are asymptotically indistinguishable is false, as demonstrated by the analysis of the ratio $R(k;c)$. While both are often colloquially termed \"heavy-tailed\", their mathematical asymptotic properties are distinctly different. Second, the proposed test violates a key constraint of the problem, which is to use \"only data from the high-degree tail $k \\ge k_{\\min}$\". This option proposes using the \"entire degree distribution\". Furthermore, a goodness-of-fit test like the KS test is generally a poor tool for model selection between two specific alternatives, especially when the main difference lies in the extreme tail, where data is sparse.\n**Verdict: Incorrect.**\n\n**D. A moment-based test suffices: compute the sample variance of degrees above $k_{\\min}$ and accept the lognormal if it is finite and the power law otherwise, since power laws have infinite variance for any $\\gamma$.**\nThis option contains a critical factual error. The variance of a power-law distribution $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$ is finite if and only if the exponent $\\gamma  3$. For many empirically observed networks, the exponent is estimated to be in the range $2  \\gamma \\le 3$, for which the variance is indeed infinite. However, the statement that this holds \"for any $\\gamma$\" is false. If $\\gamma  3$, both the power-law and lognormal (which has all moments finite) distributions would have finite variance, rendering the test useless. Additionally, inferring an infinite population moment from a finite sample moment is a statistically delicate and unreliable procedure. The sample variance is always finite.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}