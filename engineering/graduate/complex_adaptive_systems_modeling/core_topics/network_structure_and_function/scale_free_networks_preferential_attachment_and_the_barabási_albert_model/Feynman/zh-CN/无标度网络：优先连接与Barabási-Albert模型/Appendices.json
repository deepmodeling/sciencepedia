{
    "hands_on_practices": [
        {
            "introduction": "Barabási-Albert 模型的一个决定性特征是，在网络规模较大时，它会形成一个具有稳定、静态度分布的网络。本练习将指导你完成这个著名的幂律分布 $P(k)$ 的经典平均场推导。通过计算该分布的总和与预期平均度，你不仅将推导出一个基石性的结果，还将验证该理论框架的内在一致性 。",
            "id": "4141924",
            "problem": "考虑如下定义的 Barabási-Albert (BA) 网络增长模型：从一个包含 $m_0$ 个节点的初始连接种子开始。在每个离散时间步 $t \\mapsto t+1$，引入一个新节点，并通过 $m$ 条新边将其连接到 $m$ 个现有节点上，其中 $m$ 个目标节点中的每一个都是独立选择的，其被选择的概率与其当前度成正比。记 $N_k(t)$ 为时间 $t$ 时度为 $k$ 的节点的期望数量，$E(t)$ 为时间 $t$ 时的边数，$P(k)$ 为大 $t$ 极限下的稳态度分布，定义为 $P(k) = \\lim_{t \\to \\infty} N_k(t)/N(t)$，其中 $N(t)$ 是时间 $t$ 时的节点数。\n\n仅从以下条件出发：\n- 优先连接规则，即在时间 $t$，一个度为 $k_i(t)$ 的给定现有节点 $i$ 接收到 $m$ 条新边之一的概率为 $k_i(t)/\\sum_{j} k_j(t)$，\n- 守恒定律 $\\sum_{j} k_j(t) = 2 E(t)$，以及 $E(t)$ 按 $E(t+1) = E(t) + m$ 增长的事实，\n从第一性原理推导当 $k \\ge m$ 时稳态度分布 $P(k)$ 的闭式表达式。然后，使用你推导出的 $P(k)$，计算以下两个量\n$$S_1 = \\sum_{k=m}^{\\infty} P(k) \\quad \\text{和} \\quad S_2 = \\sum_{k=m}^{\\infty} k\\,P(k),$$\n并根据网络在大 $t$ 极限下的期望平均度 $\\langle k \\rangle$ 来解释 $S_2$。\n\n你的最终答案必须是包含 $S_1$ 和 $S_2$ 的单一解析表达式，并排列成一个行矩阵。无需进行数值舍入。",
            "solution": "该问题经验证具有科学依据、适定、客观且自洽。它是复杂网络研究中的一个典型问题。我们可以开始求解。\n\nBarabási-Albert (BA) 模型的稳态度分布 $P(k)$ 的推导基于速率方程方法，通常称为连续近似或平均场近似。我们分析度为 $k$ 的节点的期望数量 $N_k(t)$ 随时间的变化。\n\n首先，让我们确定在大时间 $t$ 时节点的总数 $N(t)$ 和边的总数 $E(t)$。网络在 $t=0$ 时以 $m_0$ 个节点开始。在每个时间步，会增加一个新节点。因此，在时间 $t$ 的节点总数为 $N(t) = m_0 + t$。对于大 $t$，我们可以近似为 $N(t) \\approx t$。\n在每一步，会增加 $m$ 条新边。所以，边的数量按 $E(t+1) = E(t) + m$ 增长。这意味着对于大 $t$，边的总数为 $E(t) \\approx m t$。\n所有节点的度之和由守恒定律 $\\sum_{j} k_j(t) = 2 E(t)$ 给出。使用 $E(t)$ 的大 $t$ 近似，我们有 $\\sum_{j} k_j(t) \\approx 2mt$。\n\n根据优先连接规则，一条新边连接到度为 $k_i(t)$ 的特定现有节点 $i$ 的概率 $\\Pi_i$ 为：\n$$ \\Pi_i = \\frac{k_i(t)}{\\sum_{j} k_j(t)} \\approx \\frac{k_i(t)}{2mt} $$\n由于我们关心的是特定度的节点的期望数量，我们考虑一条新边连接到*任何*度为 $k$ 的节点的总概率。存在 $N_k(t)$ 个这样的节点。假设所有相同度的节点在统计上是等价的，那么一条特定的新边连接到任何度为 $k$ 的节点的概率是：\n$$ \\Pi(k) = \\sum_{i \\text{ with } k_i=k} \\Pi_i = \\sum_{i \\text{ with } k_i=k} \\frac{k}{2mt} = \\frac{k N_k(t)}{2mt} $$\n在每个时间步，会增加 $m$ 条新边。连接到度为 $k$ 的节点的新边的期望数量是 $m \\times \\Pi(k)$：\n$$ \\Delta N_{\\text{attach}}(k) = m \\frac{k N_k(t)}{2mt} = \\frac{k N_k(t)}{2t} $$\n\n现在，我们可以写出 $N_k(t)$ 的速率方程。度为 $k$ 的节点数量因两个过程而改变：\n1.  一个度为 $k-1$ 的节点获得一条连接，变为度为 $k$ 的节点。这会增加 $N_k(t)$。该过程的速率是度为 $k-1$ 的节点获得连接的速率，即 $\\frac{(k-1) N_{k-1}(t)}{2t}$。\n2.  一个度为 $k$ 的节点获得一条连接，变为度为 $k+1$ 的节点。这会减少 $N_k(t)$。该过程的速率是 $\\frac{k N_k(t)}{2t}$。\n\n对于 $k > m$，速率方程为：\n$$ N_k(t+1) - N_k(t) = \\frac{(k-1) N_{k-1}(t)}{2t} - \\frac{k N_k(t)}{2t} $$\n对于 $k=m$，情况有所不同。在每个时间步，会引入一个度为 $m$ 的新节点。这提供了一个大小为 1 的源项。度为 $m$ 的节点也可以获得连接，变为度为 $m+1$ 的节点。所以，对于 $k=m$：\n$$ N_m(t+1) - N_m(t) = 1 - \\frac{m N_m(t)}{2t} $$\n我们现在采用连续近似，将 $t$ 视为连续变量，并用导数 $\\frac{d N_k(t)}{dt}$ 替换差分 $N_k(t+1) - N_k(t)$。\n稳态度分布定义为 $P(k) = \\lim_{t \\to \\infty} \\frac{N_k(t)}{N(t)}$。在大 $t$ 极限下，$N(t) \\approx t$，所以我们可以假设一个形式为 $N_k(t) = P(k) N(t) \\approx P(k)t$ 的时不变解。对 $t$ 求导得到 $\\frac{d N_k(t)}{dt} = P(k)$。\n\n将这些代入速率方程：\n对于 $k > m$：\n$$ P(k) = \\frac{(k-1) (t P(k-1))}{2t} - \\frac{k (t P(k))}{2t} $$\n$$ P(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k) $$\n$$ P(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1) $$\n$$ P(k) \\left(\\frac{k+2}{2}\\right) = \\frac{k-1}{2} P(k-1) $$\n这就得到了递推关系：\n$$ P(k) = \\frac{k-1}{k+2} P(k-1) \\quad \\text{对于 } k > m $$\n\n对于 $k=m$：\n$$ P(m) = 1 - \\frac{m (t P(m))}{2t} $$\n$$ P(m) = 1 - \\frac{m}{2} P(m) $$\n$$ P(m) \\left(1 + \\frac{m}{2}\\right) = 1 $$\n$$ P(m) = \\frac{1}{1 + m/2} = \\frac{2}{m+2} $$\n这给出了 $P(m)$ 的值，我们可以用它来求解当 $k \\ge m$ 时 $P(k)$ 的递推关系。\n$$ P(k) = P(k-1) \\frac{k-1}{k+2} = P(k-2) \\frac{k-2}{k+1} \\frac{k-1}{k+2} = \\dots = P(m) \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} $$\n让我们计算这个乘积项：\n$$ \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m)(m+1)\\dots(k-1)}{(m+3)(m+4)\\dots(k+2)} $$\n这可以用阶乘来表示：\n$$ \\frac{(k-1)!/(m-1)!}{(k+2)!/(m+2)!} = \\frac{(k-1)!}{(m-1)!} \\frac{(m+2)!}{(k+2)!} = \\frac{(m+2)(m+1)m}{k(k+1)(k+2)} $$\n现在，我们代入 $P(m)$ 和乘积的表达式：\n$$ P(k) = \\left(\\frac{2}{m+2}\\right) \\left(\\frac{m(m+1)(m+2)}{k(k+1)(k+2)}\\right) $$\n$$ P(k) = \\frac{2m(m+1)}{k(k+1)(k+2)} $$\n这就是当 $k \\ge m$ 时稳态度分布的闭式表达式。\n\n现在我们计算 $S_1$ 和 $S_2$。\n$$ S_1 = \\sum_{k=m}^{\\infty} P(k) = \\sum_{k=m}^{\\infty} \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)} $$\n我们对和中的项使用部分分式分解：\n$$ \\frac{1}{k(k+1)(k+2)} = \\frac{A}{k} + \\frac{B}{k+1} + \\frac{C}{k+2} $$\n解出系数可得 $A=1/2$，$B=-1$，$C=1/2$。所以：\n$$ \\frac{1}{k(k+1)(k+2)} = \\frac{1}{2k} - \\frac{1}{k+1} + \\frac{1}{2(k+2)} = \\frac{1}{2} \\left[ \\left(\\frac{1}{k} - \\frac{1}{k+1}\\right) - \\left(\\frac{1}{k+1} - \\frac{1}{k+2}\\right) \\right] $$\n令 $h(k) = \\frac{1}{k} - \\frac{1}{k+1}$。该项为 $\\frac{1}{2}[h(k) - h(k+1)]$。求和变成一个伸缩级数：\n$$ \\sum_{k=m}^{\\infty} \\frac{1}{2}[h(k) - h(k+1)] = \\frac{1}{2} \\left[ (h(m)-h(m+1)) + (h(m+1)-h(m+2)) + \\dots \\right] $$\n该和收敛于 $\\frac{1}{2} h(m)$。\n$$ \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)} = \\frac{1}{2}h(m) = \\frac{1}{2}\\left(\\frac{1}{m} - \\frac{1}{m+1}\\right) = \\frac{1}{2} \\frac{m+1-m}{m(m+1)} = \\frac{1}{2m(m+1)} $$\n将此结果代回 $S_1$ 的表达式中：\n$$ S_1 = 2m(m+1) \\left( \\frac{1}{2m(m+1)} \\right) = 1 $$\n这个结果是符合预期的，因为对于 $k \\ge m$ 的 $P(k)$ 必须是一个归一化的概率分布（对于大型网络，度小于 $m$ 的节点数量可以忽略不计）。\n\n接下来，我们计算 $S_2$：\n$$ S_2 = \\sum_{k=m}^{\\infty} k P(k) = \\sum_{k=m}^{\\infty} k \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{(k+1)(k+2)} $$\n同样，我们使用部分分式：$\\frac{1}{(k+1)(k+2)} = \\frac{1}{k+1} - \\frac{1}{k+2}$。这会得到另一个伸缩级数：\n$$ \\sum_{k=m}^{\\infty} \\left( \\frac{1}{k+1} - \\frac{1}{k+2} \\right) = \\left(\\frac{1}{m+1} - \\frac{1}{m+2}\\right) + \\left(\\frac{1}{m+2} - \\frac{1}{m+3}\\right) + \\dots $$\n这个和收敛于第一项，即 $\\frac{1}{m+1}$。\n将此结果代回 $S_2$ 的表达式中：\n$$ S_2 = 2m(m+1) \\left( \\frac{1}{m+1} \\right) = 2m $$\n\n最后，我们解释 $S_2$。网络的期望平均度定义为 $\\langle k \\rangle = \\sum_{k} k P(k)$。在大 $t$ 极限下，度小于 $m$ 的节点（只能是来自初始种子的节点）的比例变得可以忽略不计。因此，求和可以从 $k=m$ 到 $\\infty$。\n$$ \\langle k \\rangle_{\\text{large-}t} = \\sum_{k=m}^{\\infty} k P(k) = S_2 $$\n所以，$S_2$ 表示网络在大时间极限下的期望平均度。这可以从第一性原理得到验证。平均度为 $\\langle k \\rangle(t) = \\frac{\\sum_j k_j(t)}{N(t)} = \\frac{2E(t)}{N(t)}$。在大 $t$ 极限下，$E(t) \\approx mt$ 且 $N(t) \\approx t$。\n$$ \\lim_{t \\to \\infty} \\langle k \\rangle(t) = \\lim_{t \\to \\infty} \\frac{2mt}{t} = 2m $$\n这证实了我们计算出的 $S_2 = 2m$ 及其作为网络平均度的解释。\n\n所要求的答案是一个包含 $S_1$ 和 $S_2$ 的行矩阵。\n$S_1 = 1$\n$S_2 = 2m$\n最终答案是 $(1, 2m)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  2m\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理论分析揭示了 BA 模型的长期特性，但逐步模拟其增长过程对于探索其动态行为和生成网络以供进一步研究至关重要。本实践将挑战你从理论走向实践，设计一个高效的算法来构建一个 BA 网络。你将解决优先连接中的核心计算难题，学习如何根据节点不断变化的度来进行动态抽样，这是计算复杂系统建模中的一项关键技能 。",
            "id": "4141898",
            "problem": "您的任务是设计并实现一个高效的模拟算法，用以生成一个具有优先连接特性的Barabási–Albert (BA)网络。BA网络通过逐一添加节点来增长，每个新节点以与其当前度成正比的概率连接到现有节点。您的任务是编写一个程序，根据指定的参数构建这样一个网络，并为一个小规模的测试套件返回可验证的摘要统计信息。\n\n基本依据包括以下标准定义和事实。\n\n- 一个包含 $N$ 个节点的简单无向网络有一个度序列 $\\{k_i\\}_{i=0}^{N-1}$，其中 $k_i$ 是与节点 $i$ 相关联的边的数量。握手引理指出 $\\sum_{i=0}^{N-1} k_i = 2E$，其中 $E$ 是边的总数。\n- Barabási–Albert优先连接机制从一个包含 $m_0$ 个节点的初始连通种子开始，生长一个无向网络。在每个离散时间步 $t \\in \\{m_0, m_0+1, \\dots, N-1\\}$，一个新节点加入网络，并与 $m$ 个不同的现有节点形成 $m$ 条边。新节点连接到现有节点 $i$ 的概率与其当前度 $k_i$ 成正比，即 $P(i) = k_i / \\sum_j k_j$，其中 $j$ 遍历所有现有节点。\n- 为避免在初始化时出现退化的零概率分母，并确保初始种子是连通的且所有节点度均为正，取 $m_0 = \\max(m, 2)$ 并将种子初始化为一个包含 $m_0$ 个节点的团。初始边数为 $E_{\\mathrm{init}} = \\frac{m_0(m_0-1)}{2}$，每个种子节点的初始度为 $m_0 - 1$。\n\n算法要求。\n\n- 实现必须能够以与当前度成正比的方式，在每次采样 $O(\\log N)$ 的时间内对现有节点进行动态采样。您必须实现一个二进制索引树（BIT；也称为Fenwick树）或等效的数据结构，该结构支持：\n  1. 在 $O(\\log N)$ 时间内对节点权重（度）进行点更新。\n  2. 在 $O(\\log N)$ 时间内进行前缀和查询。\n  3. 按累积权重进行逆变换采样：给定一个目标值 $u \\in [0, \\sum_j k_j)$，在 $O(\\log N)$ 时间内找到最小的索引 $i$，使得到 $i$ 的累积和严格大于 $u$。\n- 在为新节点连接 $m$ 条边的单个时间步内，您必须对现有节点进行无放回抽样。通过在二进制索引树中将被选中目标的权重临时置零，以防止在该步骤内与同一目标形成多条边，然后在选定 $m$ 个目标后恢复并增加它们的权重。不允许自环。\n\n模拟输入和输出。\n\n- 对于每对参数 $(N, m)$，其中 $N \\in \\mathbb{Z}_{\\ge 1}$ 且 $m \\in \\mathbb{Z}_{\\ge 1}$，使用上述过程构建网络，并设置 $m_0 = \\max(m, 2)$。无需显式存储边；仅维护度序列 $\\{k_i\\}$ 即可。通过按测试套件中的规定显式地为伪随机数生成器设置种子，以使用确定性随机。\n- 对每次模拟，计算：\n  1. 边的总数 $E$。\n  2. 最大度 $k_{\\max} = \\max_i k_i$。\n  3. 尺度归一化的最大度比率 $r = \\frac{k_{\\max}}{m \\sqrt{N}}$，表示为实数。\n  4. 一个布尔标志 $H$，指示握手引理对于计算出的度序列是否精确成立，即 $\\sum_i k_i = 2E$ 是否成立。\n\n测试套件。\n\n为以下三个测试用例运行您的程序，每个用例都指定了伪随机种子以确保可复现性。在每个用例中，初始化 $m_0 = \\max(m, 2)$ 并将包含 $m_0$ 个节点的种子初始化为团。\n\n- 用例 A: $(N, m, \\text{seed}) = (1000, 1, 42)$。\n- 用例 B: $(N, m, \\text{seed}) = (5000, 3, 7)$。\n- 用例 C: $(N, m, \\text{seed}) = (3, 3, 123)$。\n\n最终输出格式。\n\n- 您的程序应生成单行输出，其中包含一个由三个列表组成的逗号分隔列表，每个列表对应一个测试用例，顺序为A、B、C。每个内部列表的形式必须为 $[E, k_{\\max}, r, H]$，其中 $E$ 和 $k_{\\max}$ 是整数，$r$ 是一个四舍五入到小数点后六位的实数，$H$ 是一个布尔值。例如，输出行应如下所示：\n  [ [EA,kmaxA,rA,H_A], [EB,kmaxB,rB,H_B], [EC,kmaxC,rC,H_C] ]\n除了有效的Python列表格式化所需的空格外，不要使用任何额外的空格。角度不适用，此问题中没有物理单位。所有结果都必须按规定以纯数字或布尔值形式返回。",
            "solution": "该问题要求设计并实现一个算法来模拟Barabási-Albert (BA)网络的生长。该解决方案必须遵循特定的算法和性能要求，并以优先连接原理为中心。\n\n**基于原理的设计**\n\nBA模型是一个动态过程，网络一次增长一个节点。其决定性特征是优先连接：新节点更有可能连接到已经具有高度的现有节点。这种“富者愈富”现象导致了无标度度分布，这是许多真实世界网络的一个标志。\n\n**1. 算法核心：高效的加权采样**\n\n核心计算挑战在于根据连接概率 $P(i) = k_i / \\sum_j k_j$ 高效地采样目标节点。度分布 $\\{k_i\\}$ 在每个时间步都会改变，因此采样机制必须是动态的。创建一个包含所有节点的加权列表并从中进行采样的朴素方法效率极低，因为该列表在每一步都必须重建。\n\n一种更复杂的方法是逆变换采样。我们可以将所有现有节点 $\\{k_0, k_1, \\dots, k_{t-1}\\}$ 的度概念化为一个离散概率分布。总权重是所有度的总和，$\\sum k_j = S$。我们可以从区间 $[0, S)$ 中均匀采样一个值 $u$。被选中的目标节点 $i$ 是其度“覆盖”了值 $u$ 的那个节点；形式上，它是满足 $\\sum_{j=0}^{i} k_j > u$ 的最小索引 $i$。\n\n为了高效地实现这一点，我们需要一个能够实现以下功能的数据结构：\n1. 快速计算度的前缀和 ($\\sum_{j=0}^{i} k_j$)。\n2. 在节点度发生变化时能被快速更新。\n3. 高效地找到与给定累积值 $u$ 对应的索引 $i$。\n\n**二进制索引树 (BIT)**，或称Fenwick树，是完成此任务的理想数据结构。它支持在 $O(\\log N)$ 时间内进行点更新（更改单个 $k_i$）和前缀和查询，其中 $N$ 是最大节点数。\n\n**2. 二进制索引树实现**\n\n将实现一个BIT来存储节点的度。其关键操作将是：\n- `update(index, delta)`：将 `delta` 加到 `index` 处节点的度上，在 $O(\\log N)$ 时间内通过树结构传播此更改。\n- `query(index)`：在 $O(\\log N)$ 时间内计算从节点 $0$ 到 `index` 的累积度总和。\n- `find_target(u)`：此函数执行逆变换采样。它在节点索引 $\\{0, \\dots, N-1\\}$ 上使用二分搜索，并在其中调用 `query` 函数，以找到使累积度总和 `query(i)` 严格大于随机值 $u$ 的最小索引 $i$。此搜索需要 $O(\\log N)$ 次迭代，每次迭代都涉及一次 $O(\\log N)$ 的查询，因此找到单个目标的总复杂度为 $O((\\log N)^2)$。\n\n**3. 模拟算法**\n\n对于给定的参数集 $(N, m)$，模拟按以下步骤进行：\n\n**a. 初始化：**\n- 网络从一个包含 $m_0 = \\max(m, 2)$ 个节点的种子开始。\n- 我们初始化一个大小为 $N$ 的度数组 `degrees` 和一个同样大小的BIT。\n- 为了形成初始的 $m_0$-团，我们在前 $m_0$ 个节点的每一对之间建立一条边。\n- 这 $m_0$ 个节点中每个节点的初始度为 $k_i = m_0 - 1$。\n- `degrees` 数组和BIT会相应更新：对于每个 $i \\in \\{0, \\dots, m_0-1\\}$，我们设置 `degrees[i] = m_0 - 1` 并调用 `bit.update(i, m_0 - 1)`。\n- 初始边数为 $E = \\frac{m_0(m_0-1)}{2}$。\n\n**b. 网络增长：**\n- 主模拟循环对每个要添加的新节点进行迭代，索引从 $t = m_0$ 到 $N-1$。\n- 在每个步骤 $t$，我们添加新节点 `t` 并与 $m$ 个不同的现有节点形成 $m$ 条边。\n- **无放回抽样**：为选择 $m$ 个不同的目标，我们遵循问题规定的程序：\n    1. 初始化一个列表 `selected_targets`，用于存储当前步骤中选择的节点。\n    2. 使用一个列表 `temp_modifications` 来跟踪对BIT所做的临时更改。\n    3. 我们循环 $m$ 次。在每次迭代中：\n        i. 通过查询BIT计算当前的总度数和 $S$：$S = \\text{bit.query}(t-1)$。\n        ii. 生成一个在 $[0, S)$ 区间内的均匀随机数 $u$。\n        iii. 使用 `find_target(u)` 函数找到目标节点的索引 `target`。\n        iv. 将 `target` 添加到 `selected_targets`。\n        v. 为防止此节点在本步骤中再次被选中，其在BIT中的权重被临时设置为零。我们找到它在BIT中的当前权重（可能已受同一步骤中先前选择的影响）并减去它。此操作 `(target, weight_removed)` 存储在 `temp_modifications` 中。\n    4. 选完所有 $m$ 个目标后，我们通过逆转临时修改，将BIT恢复到步骤 $t$ 开始时的状态。对于 `temp_modifications` 中的每个 `(target, weight_removed)`，我们调用 `bit.update(target, weight_removed)`。\n    5. 最后，我们应用步骤 $t$ 的永久性更新：\n        - 对于 `selected_targets` 中的每个不同节点 `v`，其度增加一。我们更新 `degrees[v] += 1` 并调用 `bit.update(v, 1)`。\n        - 新节点 `t` 的度为 $m$。我们设置 `degrees[t] = m` 并调用 `bit.update(t, m)`。\n        - 边的总数增加 $m$：$E \\leftarrow E + m$。\n\n**c. 最终计算：**\n- 增长循环完成后，`degrees` 数组包含 $N$ 个节点的网络的最终度序列。\n- 在整个模拟过程中，边的总数 $E$ 一直被跟踪。\n- 最大度通过 $k_{\\max} = \\max(\\text{degrees})$ 找到。\n- 归一化比率计算为 $r = \\frac{k_{\\max}}{m \\sqrt{N}}$。\n- 通过检查所有计算出的度的总和 $\\sum_{i=0}^{N-1} k_i$ 是否精确等于 $2E$ 来验证握手引理。结果存储在布尔标志 $H$ 中。\n\n此设计忠实地实现了Barabási-Albert模型，同时满足了所有的算法和性能要求。使用固定的伪随机数生成器种子确保了给定测试套件的确定性和可复现性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass BIT:\n    \"\"\"Binary Indexed Tree (Fenwick Tree) for 0-indexed arrays.\"\"\"\n    \n    def __init__(self, size):\n        \"\"\"Initializes a BIT of a given size.\"\"\"\n        self.size = size\n        self.tree = np.zeros(size + 1, dtype=np.int64)\n\n    def update(self, index, delta):\n        \"\"\"Adds delta to the element at index.\"\"\"\n        i = index + 1\n        while i = self.size:\n            self.tree[i] += delta\n            i += i  -i\n\n    def query(self, index):\n        \"\"\"Computes the prefix sum up to index (inclusive).\"\"\"\n        if index  0:\n            return 0\n        s = 0\n        i = index + 1\n        while i > 0:\n            s += self.tree[i]\n            i -= i  -i\n        return s\n\n    def get_value(self, index):\n        \"\"\"Gets the exact value at a given index.\"\"\"\n        return self.query(index) - self.query(index - 1)\n\n    def find_target(self, cumulative_freq):\n        \"\"\"\n        Finds the smallest index i such that the cumulative frequency (prefix sum)\n        up to i is strictly greater than cumulative_freq.\n        This is implemented using a binary search over the indices.\n        Complexity: O((log N)^2)\n        \"\"\"\n        low, high = 0, self.size - 1\n        target_index = -1\n        \n        while low = high:\n            mid = (low + high) // 2\n            if self.query(mid) > cumulative_freq:\n                target_index = mid\n                high = mid - 1\n            else:\n                low = mid + 1\n        \n        return target_index\n\ndef run_simulation(N, m, seed):\n    \"\"\"\n    Runs a single Barabási-Albert network simulation.\n    \n    Args:\n        N (int): Total number of nodes in the final network.\n        m (int): Number of edges each new node adds.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        list: A list containing [E, k_max, r, H].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    m0 = max(m, 2)\n\n    # Handle the trivial case where N = m0\n    if N = m0:\n        degrees = np.zeros(N, dtype=int)\n        if N > 0:\n            for i in range(N):\n                degrees[i] = N - 1\n        \n        total_edges = N * (N - 1) // 2 if N > 1 else 0\n        max_degree = N - 1 if N > 0 else 0\n        if N > 0 and m > 0:\n             ratio = max_degree / (m * np.sqrt(N))\n        else:\n             ratio = 0.0\n        \n        handshake_holds = (np.sum(degrees) == 2 * total_edges)\n        return [int(total_edges), int(max_degree), ratio, handshake_holds]\n\n    # ------ Initialization ------\n    degrees = np.zeros(N, dtype=int)\n    bit = BIT(N)\n    \n    # Initialize the m0-clique\n    initial_degree = m0 - 1\n    total_edges = m0 * (m0 - 1) // 2\n    \n    for i in range(m0):\n        degrees[i] = initial_degree\n        bit.update(i, initial_degree)\n        \n    # ------ Network Growth ------\n    for t in range(m0, N): # t is the index of the new node\n        \n        selected_targets = []\n        temp_modifications = []\n        num_existing_nodes = t\n\n        for _ in range(m):\n            total_degree_sum = bit.query(num_existing_nodes - 1)\n            if total_degree_sum == 0:\n                # This should not happen in a valid BA model with m >= 1 and connected seed\n                break\n            \n            rand_val = rng.uniform(0, total_degree_sum)\n            target_node = bit.find_target(rand_val)\n            \n            selected_targets.append(target_node)\n            \n            # Temporarily zero out the weight of the selected node\n            weight_in_bit = bit.get_value(target_node)\n            bit.update(target_node, -weight_in_bit)\n            temp_modifications.append((target_node, weight_in_bit))\n\n        # Restore BIT to pre-selection state\n        for node_idx, weight in temp_modifications:\n            bit.update(node_idx, weight)\n        \n        # Apply permanent updates for the current step\n        unique_targets = set(selected_targets)\n        for target_node in unique_targets:\n            degrees[target_node] += 1\n            bit.update(target_node, 1)\n            \n        degrees[t] = m\n        bit.update(t, m)\n        total_edges += m\n\n    # ------ Final Calculations ------\n    max_degree = np.max(degrees) if N > 0 else 0\n    ratio = max_degree / (m * np.sqrt(N))\n    handshake_holds = (np.sum(degrees) == 2 * total_edges)\n\n    return [int(total_edges), int(max_degree), ratio, handshake_holds]\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs simulations, and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        # (N, m, seed)\n        (1000, 1, 42),\n        (5000, 3, 7),\n        (3, 3, 123),\n    ]\n\n    all_results = []\n    for N, m, seed in test_cases:\n        result = run_simulation(N, m, seed)\n        \n        # Format the result list elements\n        E, k_max, r, H = result\n        # Ensure r is not NaN or Inf before formatting\n        if not np.isfinite(r):\n            r = 0.0\n        r_str = f\"{r:.6f}\"\n        H_str = str(H)\n        \n        inner_list_str = f\"[{E},{k_max},{r_str},{H_str}]\"\n        all_results.append(inner_list_str)\n\n    # Final print statement in the exact required format.\n    final_output_str = f\"[{','.join(all_results)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "BA 模型预测了幂律度分布，但我们如何才能在真实或模拟数据中确信地识别出这种模式，特别是当其他“重尾”分布看起来很相似时？最后一个实践采用统计学家的视角，专注于模型验证的关键任务。你将研究幂律分布和对数正态分布之间微妙但根本的渐近差异，并设计一个有原则的统计检验来区分它们，从而磨练你进行严谨数据分析的技能 。",
            "id": "4141881",
            "problem": "一个网络具有大尺寸 $n$ 和度变量 $K$。针对度分布的高阶尾部，提出了两种候选模型：一个是指数为 $\\gamma$ 的幂律分布，另一个是参数为 $(\\mu, \\sigma)$ 的对数正态分布。令 $\\overline{F}(k) = \\mathbb{P}(K \\ge k)$ 表示互补累积分布函数 (CCDF)。幂律模型假设对于 $k \\ge k_{\\min}$，有 $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$，这意味着 $\\overline{F}(k)$ 的尾部是正则变化的，其指数为 $\\alpha = \\gamma - 1$。对数正态模型假设 $\\ln K \\sim \\mathcal{N}(\\mu,\\sigma^2)$，其中 $\\mathcal{N}$ 表示正态分布。基于这些核心定义以及正态分布的标准性质，推断在每个模型下，当 $k$ 很大时 $\\overline{F}(k)$ 的渐近行为，以及这种行为在对 $k$ 进行固定的乘法重标度下的变换。在此基础上，确定一个有统计学原理的程序，该程序在复杂自适应系统的背景下，仅使用来自高阶尾部 $k \\ge k_{\\min}$ 的数据，就能区分这两种模型。哪个选项正确地描述了高 $k$ 值下两种尾部行为的比较，并提出了一个合适的检验方法？\n\nA. 对于幂律分布，$\\overline{F}(k)$ 是正则变化的，因此对于任何固定的 $c>1$，比率 $R(k;c) = \\overline{F}(ck)/\\overline{F}(k)$ 在 $k \\to \\infty$ 时收敛于一个常数 $c^{-(\\gamma-1)}$。对于对数正态分布，对于任何固定的 $c>1$，$R(k;c)$ 在 $k \\to \\infty$ 时衰减到 $0$。一个可行的检验方法是，在一系列大的 $k$ 值上估计 $R(k;c)$，将 $\\ln R(k;c)$ 对 $\\ln k$ 进行回归，并检验零假设 $H_0$：斜率 $=0$（与幂律分布一致）与备择假设 $H_1$：斜率 $0$（与对数正态分布一致），仅使用尾部数据。\n\nB. 在高 $k$ 值时，对数正态分布的尾部比任何幂律分布衰减得更慢，因此对于两种模型，$R(k;c)$ 都是渐近恒定的。一个可行的检验方法是，在单个大的 $k$ 值处计算 $R(k;c)$，并接受具有较大常数比率的模型。\n\nC. 两种尾部在渐近上是无法区分的，因为它们都是重尾的。一个可行的检验方法是，将两种模型拟合到整个度分布（不仅仅是尾部），并使用在所有 $k$ 上评估的柯尔莫哥洛夫-斯米尔诺夫 (KS) 统计量进行选择。\n\nD. 一个基于矩的检验就足够了：计算高于 $k_{\\min}$ 的度的样本方差，如果它是有限的，则接受对数正态分布，否则接受幂律分布，因为对于任何 $\\gamma$，幂律分布都具有无限方差。",
            "solution": "问题要求提出一种有统计学原理的方法，用于区分幂律分布和对数正态分布，重点关注随机变量 $K$ 取较大值时它们各自的尾部行为。该方法必须仅使用来自高阶尾部的数据，定义为 $k \\ge k_{\\min}$。\n\n我将首先验证问题陈述。\n\n### 步骤1：提取已知条件\n-   网络大小 $n$ 很大。\n-   度是一个随机变量 $K$。\n-   模型1（幂律分布）：\n    -   概率质量函数 (PMF)：对于 $k \\ge k_{\\min}$，$\\mathbb{P}(K=k) \\propto k^{-\\gamma}$。\n    -   互补累积分布函数 (CCDF) $\\overline{F}(k) = \\mathbb{P}(K \\ge k)$ 具有一个正则变化的尾部，其指数为 $\\alpha = \\gamma - 1$。\n-   模型2（对数正态分布）：\n    -   变量的自然对数服从正态分布：$\\ln K \\sim \\mathcal{N}(\\mu, \\sigma^2)$。\n-   任务是分析当 $k \\to \\infty$ 时两种模型下 $\\overline{F}(k)$ 的渐近行为，特别是在某个常数 $c > 1$ 的乘法重标度 $k \\to ck$ 下的行为，并基于此分析确定一个区分性的统计检验。\n\n### 步骤2：使用提取的已知条件进行验证\n问题陈述具有科学依据、提法恰当且客观。\n-   **科学依据**：区分幂律分布和对数正态分布是统计物理学、计算机科学和复杂系统建模中的一个基本且经过充分研究的问题。为这两种分布提供的定义是标准的。“正则变化”的概念是“无标度”行为的数学形式化基础。\n-   **提法恰当性**：该问题定义了两个不同且易于理解的模型，并要求对其渐近性质进行比较分析，这是一个可行的数学练习。问题是精确的，并且寻求一种特定类型的程序。\n-   **客观性**：问题以形式化的数学语言阐述，没有主观或含糊的术语。\n-   **缺陷**：没有可识别的缺陷。前提在事实上是合理的。对于离散的幂律PMF $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$，对于大的 $k$，CCDF可以很好地近似为 $\\overline{F}(k) \\approx \\int_k^\\infty x^{-\\gamma} dx \\propto k^{-(\\gamma-1)}$，这是一个指数为 $-(\\gamma-1)$ 的正则变化函数。问题中关于指数为 $\\alpha = \\gamma-1$ 的陈述与此一致，其中 $\\alpha$ 是指数的量值。\n\n### 步骤3：结论与行动\n问题陈述是有效的。我将继续进行推导和分析。\n\n### 渐近尾部行为的推导\n\n问题的核心在于当 $k \\to \\infty$ 时，对于一个固定的常数 $c > 1$，比率 $R(k;c) = \\frac{\\overline{F}(ck)}{\\overline{F}(k)}$ 的渐近行为。\n\n**1. 幂律模型**\n问题指出，对于幂律分布，$\\overline{F}(k)$ 是一个指数为 $-\\alpha = -(\\gamma-1)$ 的正则变化函数。根据正则变化函数的定义，对于任何常数 $c > 0$，以下极限成立：\n$$ \\lim_{k \\to \\infty} \\frac{\\overline{F}(ck)}{\\overline{F}(k)} = c^{-\\alpha} $$\n代入 $\\alpha = \\gamma - 1$，我们得到：\n$$ \\lim_{k \\to \\infty} R(k;c) = \\lim_{k \\to \\infty} \\frac{\\overline{F}(ck)}{\\overline{F}(k)} = c^{-(\\gamma - 1)} $$\n这个极限是一个正的常数，它依赖于 $c$ 和 $\\gamma$，但与 $k$ 无关。这个性质是无标度尾部的定义。取自然对数：\n$$ \\lim_{k \\to \\infty} \\ln(R(k;c)) = \\ln(c^{-(\\gamma-1)}) = -(\\gamma-1)\\ln(c) $$\n该比率的对数收敛于一个常数。这意味着 $\\ln(R(k;c))$ 对 $\\ln(k)$ 的图像在渐近上是一条水平线。\n\n**2. 对数正态模型**\n对于对数正态模型，$\\ln K$ 服从正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$。CCDF 为：\n$$ \\overline{F}(k) = \\mathbb{P}(K \\ge k) = \\mathbb{P}(\\ln K \\ge \\ln k) $$\n令 $X = \\ln K$。则 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$。标准化变量为 $Z = (X - \\mu)/\\sigma \\sim \\mathcal{N}(0,1)$。\n$$ \\overline{F}(k) = \\mathbb{P}\\left(Z \\ge \\frac{\\ln k - \\mu}{\\sigma}\\right) = \\overline{\\Phi}\\left(\\frac{\\ln k - \\mu}{\\sigma}\\right) $$\n其中 $\\overline{\\Phi}(z)$ 是标准正态分布的 CCDF。对于大的 $z$，一个标准的渐近近似（洛必达法则的一个应用，通常称为米尔斯比率）是：\n$$ \\overline{\\Phi}(z) \\approx \\frac{1}{z\\sqrt{2\\pi}} e^{-z^2/2} $$\n当 $k \\to \\infty$ 时，$z(k) = (\\ln k - \\mu)/\\sigma$ 也趋于无穷大。将 $z(k)$ 代入近似式，指数中的主导项决定了衰减率。CCDF 的对数行为如下：\n$$ \\ln \\overline{F}(k) \\approx -\\frac{z(k)^2}{2} = -\\frac{1}{2\\sigma^2}(\\ln k - \\mu)^2 $$\n对于大的 $k$，这由 $(\\ln k)^2$ 项主导：\n$$ \\ln \\overline{F}(k) \\approx -\\frac{(\\ln k)^2}{2\\sigma^2} $$\n现在我们分析比率 $R(k;c)$ 的对数：\n$$ \\ln R(k;c) = \\ln \\overline{F}(ck) - \\ln \\overline{F}(k) $$\n$$ \\ln R(k;c) \\approx \\left( -\\frac{(\\ln(ck))^2}{2\\sigma^2} \\right) - \\left( -\\frac{(\\ln k)^2}{2\\sigma^2} \\right) $$\n$$ \\ln R(k;c) \\approx -\\frac{1}{2\\sigma^2} \\left[ (\\ln c + \\ln k)^2 - (\\ln k)^2 \\right] $$\n$$ \\ln R(k;c) \\approx -\\frac{1}{2\\sigma^2} \\left[ (\\ln c)^2 + 2(\\ln c)(\\ln k) + (\\ln k)^2 - (\\ln k)^2 \\right] $$\n$$ \\ln R(k;c) \\approx -\\frac{1}{2\\sigma^2} \\left[ 2(\\ln c)(\\ln k) + (\\ln c)^2 \\right] $$\n当 $k \\to \\infty$ 时，$\\ln k$ 的线性项占主导地位：\n$$ \\ln R(k;c) \\approx -\\frac{\\ln c}{\\sigma^2} \\ln k $$\n由于 $c > 1$ 且 $\\sigma^2 > 0$，系数 $-\\frac{\\ln c}{\\sigma^2}$ 是一个负常数。这表明 $\\ln R(k;c)$ 渐近地是 $\\ln k$ 的一个具有负斜率的线性函数。因此，当 $k \\to \\infty$ 时，$\\ln R(k;c) \\to -\\infty$，这意味着 $R(k;c) \\to 0$。\n\n这个关键区别——对于幂律分布，$R(k;c)$ 收敛到一个常数，而对于对数正态分布，$R(k;c)$ 收敛到 $0$——为统计检验提供了基础。对于大的 $k$ 值，将 $\\ln R(k;c)$ 的经验估计值对 $\\ln k$ 进行回归，对于幂律分布，其斜率应为 $0$，而对于对数正态分布，其斜率应为负。\n\n### 逐项分析选项\n\n**A. 对于幂律分布，$\\overline{F}(k)$ 是正则变化的，因此对于任何固定的 $c>1$，比率 $R(k;c) = \\overline{F}(c k)/\\overline{F}(k)$ 在 $k \\to \\infty$ 时收敛于一个常数 $c^{-(\\gamma-1)}$。对于对数正态分布，对于任何固定的 $c>1$，$R(k;c)$ 在 $k \\to \\infty$ 时衰减到 $0$。一个可行的检验方法是，在一系列大的 $k$ 值上估计 $R(k;c)$，将 $\\ln R(k;c)$ 对 $\\ln k$ 进行回归，并检验零假设 $H_0$：斜率 $=0$（与幂律分布一致）与备择假设 $H_1$：斜率 $0$（与对数正态分布一致），仅使用尾部数据。**\n该选项完美匹配推导出的渐近行为。第一句正确陈述了幂律分布的正则变化性质。第二句正确陈述了对数正态尾部比幂律更快的衰减，导致 $R_k(c) \\to 0$。所提出的检验是对此差异的直接且合理的统计形式化。将 $\\ln R(k;c)$ 对 $\\ln k$ 进行回归，直接检验了我们推导出的渐近关系。仅使用尾部数据也符合问题的约束条件。\n**结论：正确。**\n\n**B. 在高 $k$ 值时，对数正态分布的尾部比任何幂律分布衰减得更慢，因此对于两种模型，$R(k;c)$ 都是渐近恒定的。一个可行的检验方法是，在单个大的 $k$ 值处计算 $R(k;c)$，并接受具有较大常数比率的模型。**\n此选项以一个事实不正确的陈述开始。对数正态分布的尾部比任何幂律分布衰减得*更快*。要看到这一点，考虑对数正态CCDF与幂律函数 $k^{-\\alpha}$ 的比率：$\\overline{F}_{LN}(k) / k^{-\\alpha}$。该比率的对数为 $\\ln \\overline{F}_{LN}(k) - \\ln(k^{-\\alpha}) \\approx -(\\ln k)^2/(2\\sigma^2) + \\alpha \\ln k = \\ln k (\\alpha - \\frac{\\ln k}{2\\sigma^2})$。当 $k \\to \\infty$ 时，该表达式趋于 $-\\infty$，意味着比率趋于 $0$。对数正态尾部是“亚幂律”的。该选项的其余部分都基于这个错误的前提。\n**结论：不正确。**\n\n**C. 两种尾部在渐近上是无法区分的，因为它们都是重尾的。一个可行的检验方法是，将两种模型拟合到整个度分布（不仅仅是尾部），并使用在所有 $k$ 上评估的柯尔莫哥洛夫-斯米尔诺夫 (KS) 统计量进行选择。**\n此选项在两点上是不正确的。首先，关于尾部在渐近上无法区分的前提是错误的，正如对比率 $R(k;c)$ 的分析所证明的那样。虽然两者通常都被口语化地称为“重尾”，但它们的数学渐近性质有显著不同。其次，所提出的检验违反了问题的一个关键约束，即“仅使用来自高阶尾部 $k \\ge k_{\\min}$ 的数据”。此选项建议使用“整个度分布”。此外，像KS检验这样的拟合优度检验通常不是在两个特定备选模型之间进行选择的好工具，特别是当主要差异在于数据稀疏的极端尾部时。\n**结论：不正确。**\n\n**D. 一个基于矩的检验就足够了：计算高于 $k_{\\min}$ 的度的样本方差，如果它是有限的，则接受对数正态分布，否则接受幂律分布，因为对于任何 $\\gamma$，幂律分布都具有无限方差。**\n此选项包含一个关键的事实错误。幂律分布 $\\mathbb{P}(K=k) \\propto k^{-\\gamma}$ 的方差是有限的，当且仅当指数 $\\gamma > 3$。对于许多经验观察到的网络，指数估计在 $2  \\gamma \\le 3$ 的范围内，此时方差确实是无限的。然而，“对于任何 $\\gamma$”都成立的说法是错误的。如果 $\\gamma > 3$，那么幂律分布和对数正态分布（其所有矩都是有限的）都将具有有限方差，从而使该检验无效。此外，从有限的样本矩推断无限的总体矩是一个在统计上很微妙且不可靠的过程。样本方差总是有限的。\n**结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}