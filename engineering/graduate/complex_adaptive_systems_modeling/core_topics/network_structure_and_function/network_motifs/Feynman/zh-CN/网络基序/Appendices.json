{
    "hands_on_practices": [
        {
            "introduction": "在网络科学的实际应用中，我们用来描绘复杂系统的图谱往往是不完整的。本练习旨在提供一个数据感知分析的基础训练 。我们将运用概率论的基本原理，推导出一个校正因子，以修正因网络中边丢失（即假阴性）而导致的模体计数系统性偏差。",
            "id": "4134008",
            "problem": "在一个表示复杂自适应系统的有向简单网络中，网络基序被定义为在少量节点上的一个固定有向模式。考虑一个由 $m$ 条有向边（$m \\in \\mathbb{N}$）组成的特定基序模式 $H$。设真实底层网络为一个有向简单图 $G_{\\mathrm{true}}$，它包含 $n$ 个带标签的节点，且无多重边和自环。定义真实计数 $C_{\\mathrm{true}}(H, G_{\\mathrm{true}})$ 为从 $H$ 的节点到 $G_{\\mathrm{true}}$ 的节点的单射映射的数量，使得 $H$ 的每一条有向边都在 $G_{\\mathrm{true}}$ 中对应的像节点对之间存在。在像节点之间存在但 $H$ 未要求的额外边不影响该计数；也就是说，该计数是针对非导出子图的出现次数。\n\n假设观测网络 $G_{\\mathrm{obs}}$ 是通过一个仅有假阴性的边检测过程从 $G_{\\mathrm{true}}$ 生成的：$G_{\\mathrm{true}}$ 中存在的每条有向边以概率 $p \\in (0,1]$ 被独立地检测并保留在 $G_{\\mathrm{obs}}$ 中，而 $G_{\\mathrm{true}}$ 中不存在的有向边不会出现在 $G_{\\mathrm{obs}}$ 中（即无假阳性）。设 $C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}})$ 表示在 $G_{\\mathrm{obs}}$ 中基序 $H$ 的观测计数，其定义与 $C_{\\mathrm{true}}$ 类似。\n\n从概率论的第一性原理（指示随机变量、独立性和期望的线性性）和上述定义出发，推导一个乘法修正因子 $g(p,m)$，使得估计量 $\\widehat{C}_{\\mathrm{true}} = g(p,m)\\, C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}})$ 在以 $G_{\\mathrm{true}}$ 为条件下，是 $C_{\\mathrm{true}}(H, G_{\\mathrm{true}})$ 的无偏估计。你的最终答案应为一个关于 $p$ 和 $m$ 的单一闭式表达式 $g(p,m)$。不需要进行数值计算，也无需四舍五入。请将最终答案表示为一个无单位的符号表达式。",
            "solution": "目标是找到一个乘法修正因子 $g(p,m)$，使得估计量 $\\widehat{C}_{\\mathrm{true}} = g(p,m)\\, C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}})$ 是真实基序计数 $C_{\\mathrm{true}}(H, G_{\\mathrm{true}})$ 的一个无偏估计量。在以真实底层网络 $G_{\\mathrm{true}}$ 为条件下，无偏估计量的条件由下式给出：\n$$\n\\mathbb{E}\\left[\\widehat{C}_{\\mathrm{true}} | G_{\\mathrm{true}}\\right] = C_{\\mathrm{true}}(H, G_{\\mathrm{true}})\n$$\n将估计量的定义代入此方程，我们得到：\n$$\n\\mathbb{E}\\left[g(p,m)\\, C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right] = C_{\\mathrm{true}}(H, G_{\\mathrm{true}})\n$$\n由于 $g(p,m)$ 是常数 $p$ 和 $m$ 的确定性函数，并且我们以 $G_{\\mathrm{true}}$ 为条件，我们可以利用期望的线性性将 $g(p,m)$ 移到期望算子之外：\n$$\ng(p,m)\\, \\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right] = C_{\\mathrm{true}}(H, G_{\\mathrm{true}})\n$$\n由此，我们可以将修正因子 $g(p,m)$ 表示为：\n$$\ng(p,m) = \\frac{C_{\\mathrm{true}}(H, G_{\\mathrm{true}})}{\\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right]}\n$$\n问题的核心是计算观测基序计数的期望值 $\\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right]$。我们将遵循题意，使用指示随机变量的方法。\n\n设 $V(H)$ 为基序模式 $H$ 中的节点集，$V(G_{\\mathrm{true}})$ 为真实图中 $n$ 个节点的集合。设 $\\mathcal{S}$ 是从 $V(H)$ 到 $V(G_{\\mathrm{true}})$ 的所有可能单射映射的集合。对于每个特定的映射 $\\phi \\in \\mathcal{S}$，我们可以定义两个指示变量：\n\n1.  设 $X_{\\phi}$ 为一个指示变量，表示在真实图 $G_{\\mathrm{true}}$ 中是否存在与 $\\phi$ 对应的基序实例。\n    $$\n    X_{\\phi} =\n    \\begin{cases}\n    1  \\text{如果对于 } E(H) \\text{ 中的每一条边 } (u, v)\\text{，边 } (\\phi(u), \\phi(v)) \\in E(G_{\\mathrm{true}}) \\\\\n    0  \\text{否则}\n    \\end{cases}\n    $$\n    根据定义，基序 $H$ 的真实计数是这些指示变量在所有可能映射上的总和：\n    $$\n    C_{\\mathrm{true}}(H, G_{\\mathrm{true}}) = \\sum_{\\phi \\in \\mathcal{S}} X_{\\phi}\n    $$\n\n2.  设 $Y_{\\phi}$ 为一个指示随机变量，表示在观测图 $G_{\\mathrm{obs}}$ 中是否存在与 $\\phi$ 对应的基序实例。\n    $$\n    Y_{\\phi} =\n    \\begin{cases}\n    1  \\text{如果对于 } E(H) \\text{ 中的每一条边 } (u,v)\\text{，边 } (\\phi(u), \\phi(v)) \\in E(G_{\\mathrm{obs}}) \\\\\n    0  \\text{否则}\n    \\end{cases}\n    $$\n    基序 $H$ 的观测计数是这些随机指示变量的总和：\n    $$\n    C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) = \\sum_{\\phi \\in \\mathcal{S}} Y_{\\phi}\n    $$\n\n现在，我们计算 $C_{\\mathrm{obs}}$ 的条件期望。利用期望的线性性：\n$$\n\\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right] = \\mathbb{E}\\left[\\sum_{\\phi \\in \\mathcal{S}} Y_{\\phi} \\Big| G_{\\mathrm{true}}\\right] = \\sum_{\\phi \\in \\mathcal{S}} \\mathbb{E}\\left[Y_{\\phi} | G_{\\mathrm{true}}\\right]\n$$\n一个指示变量的期望是它所指示事件的概率。因此，我们需要求 $\\mathbb{E}\\left[Y_{\\phi} | G_{\\mathrm{true}}\\right] = P(Y_{\\phi}=1 | G_{\\mathrm{true}})$。\n\n让我们考虑一个特定的映射 $\\phi$。要使 $Y_{\\phi}$ 为 $1$，由基序模式 $H$ 和映射 $\\phi$ 指定的 $m$ 条有向边的集合，我们称之为 $E_{\\phi, H} = \\{(\\phi(u), \\phi(v)) | (u,v) \\in E(H)\\}$，必须全部存在于 $G_{\\mathrm{obs}}$ 中。\n\n在以 $G_{\\mathrm{true}}$ 为条件下，有两种情况：\n\n情况A：映射 $\\phi$ 不对应于 $H$ 在 $G_{\\mathrm{true}}$ 中的一次出现。\n在这种情况下，$X_{\\phi} = 0$。这意味着集合 $E_{\\phi, H}$ 中至少有一条边不存在于 $E(G_{\\mathrm{true}})$ 中。由于观测过程没有假阳性，一条不在 $G_{\\mathrm{true}}$ 中的边不可能出现在 $G_{\\mathrm{obs}}$ 中。因此，可以确定 $E_{\\phi, H}$ 中的并非所有边都会在 $E(G_{\\mathrm{obs}})$ 中。这意味着 $Y_{\\phi}$ 必须为 $0$。所以，如果 $X_{\\phi} = 0$，则 $P(Y_{\\phi}=1 | G_{\\mathrm{true}}) = 0$。\n\n情况B：映射 $\\phi$ 对应于 $H$ 在 $G_{\\mathrm{true}}$ 中的一次出现。\n在这种情况下，$X_{\\phi} = 1$。这意味着集合 $E_{\\phi, H}$ 中的所有 $m$ 条边都存在于 $E(G_{\\mathrm{true}})$ 中。要使 $Y_{\\phi}$ 为 $1$，所有这 $m$ 条边都必须被独立地检测并保留在 $G_{\\mathrm{obs}}$ 中。$G_{\\mathrm{true}}$ 中的任何单条边被检测到的概率是 $p$。由于检测是独立事件，所有 $m$ 条边都被检测到的概率是它们各自概率的乘积：\n$$\nP(\\text{E}_{\\phi,H} \\text{中的所有 } m \\text{ 条边都被检测到}) = \\underbrace{p \\times p \\times \\cdots \\times p}_{m \\text{ 次}} = p^m\n$$\n所以，如果 $X_{\\phi} = 1$，则 $P(Y_{\\phi}=1 | G_{\\mathrm{true}}) = p^m$。\n\n我们可以巧妙地将这两种情况结合起来。对于任何给定的映射 $\\phi$，其条件概率为：\n$$\nP(Y_{\\phi}=1 | G_{\\mathrm{true}}) = X_{\\phi} \\cdot p^m\n$$\n这是因为如果 $X_{\\phi}=0$，概率为 $0 \\cdot p^m = 0$。如果 $X_{\\phi}=1$，概率为 $1 \\cdot p^m = p^m$。\n因此，指示变量 $Y_{\\phi}$ 的条件期望是：\n$$\n\\mathbb{E}\\left[Y_{\\phi} | G_{\\mathrm{true}}\\right] = X_{\\phi} p^m\n$$\n现在我们可以将此结果代回到期望观测计数的求和式中：\n$$\n\\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right] = \\sum_{\\phi \\in \\mathcal{S}} (X_{\\phi} p^m)\n$$\n由于 $p^m$ 是一个关于求和变量 $\\phi$ 的常数，我们可以将其从求和符号中提出：\n$$\n\\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right] = p^m \\sum_{\\phi \\in \\mathcal{S}} X_{\\phi}\n$$\n认识到 $\\sum_{\\phi \\in \\mathcal{S}} X_{\\phi}$ 就是 $C_{\\mathrm{true}}(H, G_{\\mathrm{true}})$ 的定义，我们得到以下关系：\n$$\n\\mathbb{E}\\left[C_{\\mathrm{obs}}(H, G_{\\mathrm{obs}}) | G_{\\mathrm{true}}\\right] = p^m C_{\\mathrm{true}}(H, G_{\\mathrm{true}})\n$$\n最后，我们将此结果代回到修正因子 $g(p,m)$ 的表达式中：\n$$\ng(p,m) = \\frac{C_{\\mathrm{true}}(H, G_{\\mathrm{true}})}{p^m C_{\\mathrm{true}}(H, G_{\\mathrm{true}})}\n$$\n假设 $C_{\\mathrm{true}}(H, G_{\\mathrm{true}}) > 0$（非平凡情况），我们可以从分子和分母中消去这一项：\n$$\ng(p,m) = \\frac{1}{p^m} = p^{-m}\n$$\n这个因子修正了由于边的概率性丢失而导致的对基序的系统性计数不足。估计量 $\\widehat{C}_{\\mathrm{true}} = p^{-m} C_{\\mathrm{obs}}$ 补偿了这样一个事实：基序 $H$ 的任何给定实例仅以概率 $p^m$ 在观测过程中得以保留。",
            "answer": "$$\n\\boxed{p^{-m}}\n$$"
        },
        {
            "introduction": "即使我们获得了准确的模体计数，解读这些数字也并非易事。如果一个网络的基本属性（如其度分布）本身就倾向于产生大量某种模式，那么原始的高计数值可能并不具备统计显著性 。本练习将引导你应对这一核心挑战，通过与一个随机零模型进行比较来对模体计数进行归一化，从而识别出那些真正“出人意料”的结构模式。",
            "id": "4134009",
            "problem": "考虑一个称为前馈环的有向网络模体，它定义在不同的节点 $i,j,k$ 上，由有向边 $i \\rightarrow j$、$j \\rightarrow k$ 和 $i \\rightarrow k$ 的存在来定义。在稀疏网络的有向配置模型 (Chung–Lu) 中，假设从节点 $u$ 到节点 $v$ 的边的基准期望概率由 $p_{uv} = k_{u}^{\\mathrm{out}} k_{v}^{\\mathrm{in}} / m$ 给出，其中 $k_{u}^{\\mathrm{out}}$ 是节点 $u$ 的出度，$k_{v}^{\\mathrm{in}}$ 是节点 $v$ 的入度，$m$ 是边的总数。在边出现事件独立的近似下进行计算。\n\n给定一个包含节点 $a,b,c,d,e$ 的有向网络，其边如下：\n$a \\rightarrow b$, $a \\rightarrow c$, $a \\rightarrow d$, $a \\rightarrow e$, $b \\rightarrow c$, $b \\rightarrow d$, $b \\rightarrow e$, $c \\rightarrow d$, $c \\rightarrow e$, $d \\rightarrow e$。\n\n仅使用基本定义和所述基准，首先从第一性原理出发，论证为何在此基准下，具有较大 $k^{\\mathrm{out}}$ 或较大 $k^{\\mathrm{in}}$ 的节点会倾向于夸大原始前馈环的计数。然后，推导一种归一化方法，其中每个观测到的 $(i,j,k)$ 上的前馈环被赋予一个权重，该权重等于其三条边的基准边概率乘積的倒数。将归一化模体得分 $S$ 定义为给定网络中所有观测到的前馈环的这些权重之和。\n\n计算该网络中 $S$ 的精确值。请用精确值表示您的最终答案，无需四舍五入。最终答案中不包含任何单位。",
            "solution": "此问题已经过验证。\n\n### 步骤 1：提取已知条件\n- **模体定义**：前馈环 (FFL) 定义在不同的节点 $i,j,k$ 上，由有向边 $i \\rightarrow j$、$j \\rightarrow k$ 和 $i \\rightarrow k$ 的存在来定义。\n- **网络模型**：稀疏网络的有向配置模型 (Chung–Lu)。\n- **基准边概率**：从节点 $u$ 到节点 $v$ 的边的概率为 $p_{uv} = k_{u}^{\\mathrm{out}} k_{v}^{\\mathrm{in}} / m$。\n- **模型参数**：$k_{u}^{\\mathrm{out}}$ 是节点 $u$ 的出度，$k_{v}^{\\mathrm{in}}$ 是节点 $v$ 的入度，$m$ 是边的总数。\n- **近似**：边的出现被视为独立事件。\n- **网络规格**：节点为 $\\{a,b,c,d,e\\}$。边为 $a \\rightarrow b$、$a \\rightarrow c$、$a \\rightarrow d$、$a \\rightarrow e$、$b \\rightarrow c$、$b \\rightarrow d$、$b \\rightarrow e$、$c \\rightarrow d$、$c \\rightarrow e$、$d \\rightarrow e$。\n- **任务 1**：从第一性原理出发，论证为何在此基准下，具有较大 $k^{\\mathrm{out}}$ 或 $k^{\\mathrm{in}}$ 的节点会夸大原始 FFL 计数。\n- **任务 2**：定义一个归一化模体得分 $S$，其中每个观测到的 $(i,j,k)$ 上的 FFL 的权重是其三条构成边的基准概率乘积的倒数。$S$ 是这些权重之和。\n- **任务 3**：计算给定网络中 $S$ 的精确值。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题在网络科学和复杂系统领域有坚实的理论基础。前馈环是一个典型的网络模体。有向配置模型（或 Chung-Lu 模型）是用于模体显著性分析的标准零模型。通过零模型概率的倒数进行归一化的方法是一种有效且成熟的技术。\n- **适定性**：该问题是适定的。网络结构已明确给出，零模型已定义，待计算量 ($S$) 由清晰的数学步骤指定。所有计算唯一解所需的信息都已提供。\n- **客观性**：问题陈述是客观的，使用了精确、正式的定义，避免了任何模糊或主观的语言。\n\n### 步骤 3：结论与行动\n该问题是有效的，因为它科学上合理、适定且客观。将提供完整的解答。\n\n### 解题推导\n\n解题过程分三个阶段：首先，基于原理对模体计数被夸大的原因进行论证；其次，详细分析给定网络以确定其属性并识别所有 FFL；第三，计算归一化得分 $S$。\n\n**第 1 部分：原始计数夸大的论证**\n\n在不同节点 $(i,j,k)$ 上的前馈环 (FFL) 由边 $i \\rightarrow j$、$j \\rightarrow k$ 和 $i \\rightarrow k$ 组成。在指定的 Chung-Lu 基准模型中，每个潜在边的出现是一个独立的概率事件。从节点 $u$ 到节点 $v$ 的边的概率由 $p_{uv} = k_{u}^{\\mathrm{out}} k_{v}^{\\mathrm{in}} / m$ 给出。\n\n由该模型生成的随机网络中 FFL 的期望数 $N_{\\mathrm{FFL}}^{\\mathrm{exp}}$，是对于所有不同节点三元组 $(i,j,k)$，该三元组形成 FFL 的概率之和。根据独立性假设，这个概率是各条边概率的乘积：\n$$P(\\text{FFL on } i,j,k) = p_{ij} p_{jk} p_{ik}$$\n代入给定的基准概率公式：\n$$P(\\text{FFL on } i,j,k) = \\left(\\frac{k_{i}^{\\mathrm{out}} k_{j}^{\\mathrm{in}}}{m}\\right) \\left(\\frac{k_{j}^{\\mathrm{out}} k_{k}^{\\mathrm{in}}}{m}\\right) \\left(\\frac{k_{i}^{\\mathrm{out}} k_{k}^{\\mathrm{in}}}{m}\\right) = \\frac{(k_{i}^{\\mathrm{out}})^{2} k_{j}^{\\mathrm{in}} k_{j}^{\\mathrm{out}} (k_{k}^{\\mathrm{in}})^{2}}{m^{3}}$$\nFFL 的总期望数是所有可能三元组的这些概率之和：\n$$N_{\\mathrm{FFL}}^{\\mathrm{exp}} = \\sum_{i \\neq j \\neq k \\neq i} \\frac{(k_{i}^{\\mathrm{out}})^{2} k_{j}^{\\mathrm{in}} k_{j}^{\\mathrm{out}} (k_{k}^{\\mathrm{in}})^{2}}{m^{3}}$$\n这个表达式从第一性原理证明了，FFL 的期望数在所有网络结构中并非均匀分布，而是高度依赖于度序列。具体来说，任何涉及节点 $i, j, k$ 的潜在 FFL 对期望计数的贡献，与源节点 $i$ 的出度的平方、汇节点 $k$ 的入度的平方以及中间节点 $j$ 的入度和出度的乘积成比例。因此，具有大度数的节点（中心节点）会不成比例地增加 FFL 的期望数。因此，在观测到的网络中，FFL 的原始计数因为高度数节点的存在而被“夸大”，因为无论是否存在任何更高阶的结构组织，这个基准期望值都很高。需要进行归一化，以将度序列的贡献与真实的结构过表达分离开来。\n\n**第 2 部分：网络分析与 FFL 识别**\n\n给定网络有 5 个节点 $\\{a,b,c,d,e\\}$ 和 10 条边。边的总数 $m=10$。我们首先计算每个节点的入度 ($k^{\\mathrm{in}}$) 和出度 ($k^{\\mathrm{out}}$)。\n- 节点 $a$：$k_{a}^{\\mathrm{in}} = 0$, $k_{a}^{\\mathrm{out}} = 4$\n- 节点 $b$：$k_{b}^{\\mathrm{in}} = 1$, $k_{b}^{\\mathrm{out}} = 3$\n- 节点 $c$：$k_{c}^{\\mathrm{in}} = 2$, $k_{c}^{\\mathrm{out}} = 2$\n- 节点 $d$：$k_{d}^{\\mathrm{in}} = 3$, $k_{d}^{\\mathrm{out}} = 1$\n- 节点 $e$：$k_{e}^{\\mathrm{in}} = 4$, $k_{e}^{\\mathrm{out}} = 0$\n度的总和为 $\\sum k^{\\mathrm{in}} = 10$ 和 $\\sum k^{\\mathrm{out}} = 10$，与 $m=10$ 一致。\n\n接下来，我们系统地识别网络中所有的 FFL $(i,j,k)$。一个 FFL 需要一条长度为 2 的路径 ($i \\rightarrow j \\rightarrow k$) 和一条直接的“捷径”边 ($i \\rightarrow k$)。\n1. FFL $(a,b,c)$：路径 $a \\rightarrow b \\rightarrow c$ 存在。捷径 $a \\rightarrow c$ 存在。\n2. FFL $(a,b,d)$：路径 $a \\rightarrow b \\rightarrow d$ 存在。捷径 $a \\rightarrow d$ 存在。\n3. FFL $(a,b,e)$：路径 $a \\rightarrow b \\rightarrow e$ 存在。捷径 $a \\rightarrow e$ 存在。\n4. FFL $(a,c,d)$：路径 $a \\rightarrow c \\rightarrow d$ 存在。捷径 $a \\rightarrow d$ 存在。\n5. FFL $(a,c,e)$：路径 $a \\rightarrow c \\rightarrow e$ 存在。捷径 $a \\rightarrow e$ 存在。\n6. FFL $(a,d,e)$：路径 $a \\rightarrow d \\rightarrow e$ 存在。捷径 $a \\rightarrow e$ 存在。\n7. FFL $(b,c,d)$：路径 $b \\rightarrow c \\rightarrow d$ 存在。捷径 $b \\rightarrow d$ 存在。\n8. FFL $(b,c,e)$：路径 $b \\rightarrow c \\rightarrow e$ 存在。捷径 $b \\rightarrow e$ 存在。\n9. FFL $(b,d,e)$：路径 $b \\rightarrow d \\rightarrow e$ 存在。捷径 $b \\rightarrow e$ 存在。\n10. FFL $(c,d,e)$：路径 $c \\rightarrow d \\rightarrow e$ 存在。捷径 $c \\rightarrow e$ 存在。\n网络中总共有 10 个 FFL。\n\n**第 3 部分：归一化得分 S 的计算**\n\n归一化得分 $S$ 是所有观测到的 FFL 的权重之和。节点 $(i,j,k)$ 上的 FFL 的权重 $w_{ijk}$ 是其构成边的基准概率乘积的倒数：\n$$w_{ijk} = \\frac{1}{p_{ij} p_{jk} p_{ik}} = \\frac{1}{\\frac{(k_{i}^{\\mathrm{out}})^{2} k_{j}^{\\mathrm{in}} k_{j}^{\\mathrm{out}} (k_{k}^{\\mathrm{in}})^{2}}{m^{3}}} = \\frac{m^{3}}{(k_{i}^{\\mathrm{out}})^{2} k_{j}^{\\mathrm{in}} k_{j}^{\\mathrm{out}} (k_{k}^{\\mathrm{in}})^{2}}$$\n我们使用 $m=10$（因此 $m^3=1000$）和之前计算出的度值，为 10 个已识别的 FFL 中的每一个计算这个权重。\n\n- $w_{abc} = \\frac{1000}{(k_{a}^{\\mathrm{out}})^{2} k_{b}^{\\mathrm{in}} k_{b}^{\\mathrm{out}} (k_{c}^{\\mathrm{in}})^{2}} = \\frac{1000}{(4)^{2}(1)(3)(2)^{2}} = \\frac{1000}{192}$\n- $w_{abd} = \\frac{1000}{(k_{a}^{\\mathrm{out}})^{2} k_{b}^{\\mathrm{in}} k_{b}^{\\mathrm{out}} (k_{d}^{\\mathrm{in}})^{2}} = \\frac{1000}{(4)^{2}(1)(3)(3)^{2}} = \\frac{1000}{432}$\n- $w_{abe} = \\frac{1000}{(k_{a}^{\\mathrm{out}})^{2} k_{b}^{\\mathrm{in}} k_{b}^{\\mathrm{out}} (k_{e}^{\\mathrm{in}})^{2}} = \\frac{1000}{(4)^{2}(1)(3)(4)^{2}} = \\frac{1000}{768}$\n- $w_{acd} = \\frac{1000}{(k_{a}^{\\mathrm{out}})^{2} k_{c}^{\\mathrm{in}} k_{c}^{\\mathrm{out}} (k_{d}^{\\mathrm{in}})^{2}} = \\frac{1000}{(4)^{2}(2)(2)(3)^{2}} = \\frac{1000}{576}$\n- $w_{ace} = \\frac{1000}{(k_{a}^{\\mathrm{out}})^{2} k_{c}^{\\mathrm{in}} k_{c}^{\\mathrm{out}} (k_{e}^{\\mathrm{in}})^{2}} = \\frac{1000}{(4)^{2}(2)(2)(4)^{2}} = \\frac{1000}{1024}$\n- $w_{ade} = \\frac{1000}{(k_{a}^{\\mathrm{out}})^{2} k_{d}^{\\mathrm{in}} k_{d}^{\\mathrm{out}} (k_{e}^{\\mathrm{in}})^{2}} = \\frac{1000}{(4)^{2}(3)(1)(4)^{2}} = \\frac{1000}{768}$\n- $w_{bcd} = \\frac{1000}{(k_{b}^{\\mathrm{out}})^{2} k_{c}^{\\mathrm{in}} k_{c}^{\\mathrm{out}} (k_{d}^{\\mathrm{in}})^{2}} = \\frac{1000}{(3)^{2}(2)(2)(3)^{2}} = \\frac{1000}{324}$\n- $w_{bce} = \\frac{1000}{(k_{b}^{\\mathrm{out}})^{2} k_{c}^{\\mathrm{in}} k_{c}^{\\mathrm{out}} (k_{e}^{\\mathrm{in}})^{2}} = \\frac{1000}{(3)^{2}(2)(2)(4)^{2}} = \\frac{1000}{576}$\n- $w_{bde} = \\frac{1000}{(k_{b}^{\\mathrm{out}})^{2} k_{d}^{\\mathrm{in}} k_{d}^{\\mathrm{out}} (k_{e}^{\\mathrm{in}})^{2}} = \\frac{1000}{(3)^{2}(3)(1)(4)^{2}} = \\frac{1000}{432}$\n- $w_{cde} = \\frac{1000}{(k_{c}^{\\mathrm{out}})^{2} k_{d}^{\\mathrm{in}} k_{d}^{\\mathrm{out}} (k_{e}^{\\mathrm{in}})^{2}} = \\frac{1000}{(2)^{2}(3)(1)(4)^{2}} = \\frac{1000}{192}$\n\n总得分 $S$ 是这 10 个权重之和：\n$$S = \\frac{1000}{192} + \\frac{1000}{432} + \\frac{1000}{768} + \\frac{1000}{576} + \\frac{1000}{1024} + \\frac{1000}{768} + \\frac{1000}{324} + \\frac{1000}{576} + \\frac{1000}{432} + \\frac{1000}{192}$$\n将各项相加：\n$$S = 1000 \\left( \\frac{2}{192} + \\frac{2}{432} + \\frac{2}{768} + \\frac{2}{576} + \\frac{1}{1024} + \\frac{1}{324} \\right)$$\n$$S = 1000 \\left( \\frac{1}{96} + \\frac{1}{216} + \\frac{1}{384} + \\frac{1}{288} + \\frac{1}{1024} + \\frac{1}{324} \\right)$$\n为了对这些分数求和，我们找到分母的最小公倍数：\n$96 = 2^5 \\times 3^1$，$216 = 2^3 \\times 3^3$，$384 = 2^7 \\times 3^1$，$288 = 2^5 \\times 3^2$，$1024 = 2^{10}$，$324 = 2^2 \\times 3^4$。\n最小公倍数是 $2^{10} \\times 3^4 = 1024 \\times 81 = 82944$。\n用这个公分母表示每个分数：\n$$S = 1000 \\left( \\frac{864}{82944} + \\frac{384}{82944} + \\frac{216}{82944} + \\frac{288}{82944} + \\frac{81}{82944} + \\frac{256}{82944} \\right)$$\n$$S = 1000 \\left( \\frac{864+384+216+288+81+256}{82944} \\right) = 1000 \\left( \\frac{2089}{82944} \\right)$$\n$$S = \\frac{2089000}{82944}$$\n这个分数可以通过约去公因数来简化。$1000 = 8 \\times 125 = 2^3 \\times 5^3$ 且 $82944 = 8 \\times 10368$。\n$$S = \\frac{125 \\times 2089}{10368}$$\n分子：$125 \\times 2089 = 261125$。\n分母：$10368$。\n分子 $2089$ 不能被 2 或 3 整除（各位数字之和为 19）。分母 $10368 = 2^7 \\times 3^4$ 没有因子 5。因此，该分数已为最简形式。\n$$S = \\frac{261125}{10368}$$\n这就是归一化模体得分的精确值。",
            "answer": "$$\\boxed{\\frac{261125}{10368}}$$"
        },
        {
            "introduction": "网络模体不仅是全局性的结构特征，其在网络中的分布还能揭示单个节点的功能角色。这个综合性实践将指导你完成一个完整的计算流程：在节点层面进行模体计数，并利用统计显著性图谱，将节点分类为促进局部聚集的“派系导向型”或连接不同社区的“中介导向型” 。通过这个练习，你将把模体分析的理论知识转化为对网络组件进行功能推断的实用技能。",
            "id": "4291115",
            "problem": "给定无向简单图，以邻接矩阵表示。您的任务是计算每个节点的节点级模体参与概况，并使用这些概况将节点分类为不同的功能角色。所考虑的模体是 $3$-节点三角形和 $3$-节点楔形（以某个节点为中心的开放三元组）。分类必须从第一性原理计算得出，并且是确定性的。\n\n基本定义与约束：\n- 图由一个对称邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示，其中对于所有 $i$ 都有 $A_{ii} = 0$，$n$ 是节点数。\n- 对于节点 $i$，令 $N(i) = \\{j \\in \\{1,\\dots,n\\} \\mid A_{ij} = 1\\}$ 表示其邻居集合，而 $d_i = |N(i)|$ 表示其度。\n- 节点 $i$ 参与的三角形定义为 $N(i)$ 中任意满足 $A_{jk}=1$ 的无序对 $\\{j,k\\}$。节点级的三角形计数为\n$$\nT_i = \\sum_{\\{j,k\\} \\subset N(i),\\, j < k} A_{jk}\n$$\n- 节点 $i$ 参与的楔形定义为 $N(i)$ 中任意满足 $A_{jk}=0$ 的无序对 $\\{j,k\\}$。节点级的楔形计数为\n$$\nW_i = \\sum_{\\{j,k\\} \\subset N(i),\\, j < k} (1 - A_{jk})\n$$\n- 注意，对于给定节点 $i$，总有 $T_i + W_i = \\binom{d_i}{2}$。\n\n功能角色分类规则：\n1.  **外围 (Peripheral)** (角色0)：度 $d_i < 2$ 的节点。这些节点没有足够的连接来形成三角形或楔形。\n2.  **派系导向型 (Clique-oriented)** (角色1)：度 $d_i \\ge 2$ 的节点，其三角形计数相对于度保持零模型而言，比楔形计数更具统计显著性。形式上，$z_T \\ge z_W$。\n3.  **中介导向型 (Broker-oriented)** (角色2)：度 $d_i \\ge 2$ 的节点，其楔形计数相对于度保持零模型而言，比三角形计数更具统计显著性。形式上，$z_T < z_W$。\n$z_T$ 和 $z_W$ 分别是 $T_i$ 和 $W_i$ 的z分数，计算方式为 $z = (x_{\\text{obs}} - \\mu_{\\text{null}})/\\sigma_{\\text{null}}$。零模型均值 $\\mu$ 和标准差 $\\sigma$ 必须通过生成一个保留度序列的随机图系综来经验估计。\n\n您将获得一个函数 `classify_roles(A, R, S, seed)`，其中：\n- `A`: 一个numpy数组，表示无向图的邻接矩阵。\n- `R`: 要生成的随机图数量，用于构建零模型统计数据。\n- `S`: 用于随机化每个图的度保持边交换尝试次数。\n- `seed`: 用于可重复性的随机数生成器种子。\n\n您的任务是实现这个函数，以返回一个代表每个节点角色的整数列表。您需要编写辅助函数来实现：\n1.  对每个节点计算观测到的 $T_i$ 和 $W_i$。\n2.  实现一个度保持的边交换算法，以从观测图中生成随机图。\n3.  为每个节点计算 $\\mu_T, \\sigma_T, \\mu_W, \\sigma_W$。\n4.  计算z分数并应用分类规则。如果 $\\sigma=0$，则z分数应为 $0$。\n\n您的最终答案应该是一个Python代码块，包含 `classify_roles` 的完整实现及其所有必要的辅助函数。对以下三个测试案例运行您的实现，并以一个不带空格的单行字符串形式打印结果列表的列表，例如 `[[0,1,2],[2,1,0],[0,0,1]]`。\n- Test 1: $n=6$, edges=`[(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(0,4),(4,5)]`, $R=200$, $S=400$, seed=`202311`\n- Test 2: $n=6$, edges=`[(0,1),(1,2),(2,3),(3,4),(4,5),(5,0)]`, $R=200$, $S=400$, seed=`202312`\n- Test 3: $n=6$, edges=`[(0,1),(0,2),(0,3),(0,4),(0,5)]`, $R=200$, $S=400$, seed=`202313`",
            "solution": "我们从简单无向图的基本表示开始，即一个对角线为零的对称邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$。对于每个节点 $i$，其邻居集合 $N(i)$ 和度 $d_i$ 定义为 $N(i) = \\{j \\mid A_{ij} = 1\\}$ 和 $d_i = |N(i)|$。\n\n所考虑的节点级模体是三角形和楔形（以该节点为中心的开放三元组）。三角形计数 $T_i$ 计算的是 $N(i)$ 中相互连接的无序邻居对 $\\{j,k\\}$ 的数量，而楔形计数 $W_i$ 计算的是不相互连接的无序邻居对的数量。形式上，\n$$\nT_i = \\sum_{\\{j,k\\} \\subset N(i),\\, j < k} A_{jk} \\quad \\text{and} \\quad W_i = \\sum_{\\{j,k\\} \\subset N(i),\\, j < k} (1 - A_{jk})\n$$\n对于 $d_i  2$ 的节点，这些计数为零。\n\n功能分类规则取决于将观测到的计数 $T_i$ 和 $W_i$ 与从保留度序列的随机图系综（即配置模型）中得出的期望值进行比较。这种比较通过z分数进行量化：\n$$\nz_T(i) = \\frac{T_i - \\mu_T(i)}{\\sigma_T(i)}, \\quad z_W(i) = \\frac{W_i - \\mu_W(i)}{\\sigma_W(i)}\n$$\n其中 $\\mu$ 和 $\\sigma$ 分别是零模型系综中的均值和标准差。如果 $\\sigma=0$（例如，对于给定节点，所有随机图中的模体计数都相同），则z分数定义为0。\n\n分类逻辑如下：\n- 如果 $d_i  2$，节点 $i$ 是 **外围** (角色0)。\n- 如果 $d_i \\ge 2$，我们比较z分数：\n    - 如果 $z_T \\ge z_W$，节点 $i$ 是 **派系导向型** (角色1)。\n    - 如果 $z_T  z_W$，节点 $i$ 是 **中介导向型** (角色2)。\n\n这个框架要求一个完整的计算流程：\n1.  **观测计数**：对于给定图 $A$ 中的每个节点 $i$，计算 $T_i$ 和 $W_i$。\n2.  **零模型生成**：通过对原始图 $A$ 进行度保持边交换，生成 $R$ 个随机图。这通常是通过重复选择两条不共享节点的边 $(u,v)$ 和 $(x,y)$，并将它们重新连接为 $(u,x)$ 和 $(v,y)$（或 $(u,y)$ 和 $(v,x)$），前提是新边不存在。\n3.  **零模型统计**：在 $R$ 个随机图的每一个中，为每个节点计算 $T_i$ 和 $W_i$。然后，对于每个节点 $i$，计算这 $R$ 个计数的均值和样本标准差，得到 $\\mu_T(i), \\sigma_T(i), \\mu_W(i), \\sigma_W(i)$。\n4.  **分类**：使用观测到的计数和零模型统计数据计算z分数，并应用上述规则来分配角色。\n\n实现这些步骤需要系统化的代码，包括用于图操作、模体计数、图随机化和统计计算的函数。",
            "answer": "```python\nimport numpy as np\n\ndef build_adjacency(n, edges):\n    \"\"\"Build symmetric adjacency matrix for an undirected simple graph.\"\"\"\n    A = np.zeros((n, n), dtype=int)\n    for u, v in edges:\n        if u == v:\n            continue\n        A[u, v] = 1\n        A[v, u] = 1\n    np.fill_diagonal(A, 0)\n    return A\n\ndef neighbors_of(A, i):\n    \"\"\"Return neighbors of node i.\"\"\"\n    return np.where(A[i] == 1)[0]\n\ndef count_triangles_wedges_per_node(A):\n    \"\"\"Count triangles and wedges per node.\"\"\"\n    n = A.shape[0]\n    triangles = np.zeros(n, dtype=int)\n    wedges = np.zeros(n, dtype=int)\n    for i in range(n):\n        nbrs = neighbors_of(A, i)\n        k = len(nbrs)\n        if k  2:\n            continue\n        # count unordered pairs among neighbors\n        for a in range(k):\n            for b in range(a + 1, k):\n                u = nbrs[a]\n                v = nbrs[b]\n                if A[u, v] == 1:\n                    triangles[i] += 1\n                else:\n                    wedges[i] += 1\n    return triangles, wedges\n\ndef get_edge_list(A):\n    \"\"\"Return list of undirected edges (u,v) with u  v.\"\"\"\n    n = A.shape[0]\n    edges = []\n    for u in range(n):\n        for v in range(u + 1, n):\n            if A[u, v] == 1:\n                edges.append((u, v))\n    return edges\n\ndef attempt_swap(A, rng):\n    \"\"\"Attempt a single degree-preserving edge swap; return True if performed.\"\"\"\n    edges = get_edge_list(A)\n    m = len(edges)\n    if m  2:\n        return False\n    # choose two distinct edges uniformly at random\n    idx = rng.integers(0, m, size=2)\n    if idx[0] == idx[1]:\n        return False\n    (u, v) = edges[idx[0]]\n    (x, y) = edges[idx[1]]\n    # ensure all endpoints distinct\n    endpoints = {u, v, x, y}\n    if len(endpoints)  4:\n        return False\n    # choose one of two rewiring options at random\n    if rng.integers(0, 2) == 0:\n        a, b = u, y\n        c, d = x, v\n    else:\n        a, b = u, x\n        c, d = v, y\n    # avoid self-loops\n    if a == b or c == d:\n        return False\n    # avoid existing edges and parallel edges\n    if A[a, b] == 1 or A[c, d] == 1:\n        return False\n    # perform swap: remove original edges, add new edges\n    A[u, v] = 0\n    A[v, u] = 0\n    A[x, y] = 0\n    A[y, x] = 0\n    A[a, b] = 1\n    A[b, a] = 1\n    A[c, d] = 1\n    A[d, c] = 1\n    return True\n\ndef randomize_graph(A, swaps, rng):\n    \"\"\"Return a randomized graph via degree-preserving swaps starting from A.\"\"\"\n    Ar = A.copy()\n    attempts = 0\n    # perform 'swaps' attempts\n    while attempts  swaps:\n        attempts += 1\n        attempt_swap(Ar, rng)\n    return Ar\n\ndef null_stats(A, R, S, rng):\n    \"\"\"Compute null model mean and std for triangles and wedges per node.\"\"\"\n    n = A.shape[0]\n    tri_samples = np.zeros((R, n), dtype=float)\n    wed_samples = np.zeros((R, n), dtype=float)\n    for r in range(R):\n        Ar = randomize_graph(A, S, rng)\n        T, W = count_triangles_wedges_per_node(Ar)\n        tri_samples[r] = T\n        wed_samples[r] = W\n    mu_T = tri_samples.mean(axis=0)\n    mu_W = wed_samples.mean(axis=0)\n    # sample standard deviation (ddof=1); handle R==1 separately if needed\n    if R  1:\n        sigma_T = tri_samples.std(axis=0, ddof=1)\n        sigma_W = wed_samples.std(axis=0, ddof=1)\n    else:\n        sigma_T = np.zeros(n, dtype=float)\n        sigma_W = np.zeros(n, dtype=float)\n    return mu_T, sigma_T, mu_W, sigma_W\n\ndef classify_roles(A, R, S, seed):\n    \"\"\"Classify nodes into roles 0,1,2 based on node-level motif z-scores.\"\"\"\n    rng = np.random.default_rng(seed)\n    degrees = A.sum(axis=1)\n    T_obs, W_obs = count_triangles_wedges_per_node(A)\n    mu_T, sigma_T, mu_W, sigma_W = null_stats(A, R, S, rng)\n    # compute z-scores with zero fallback\n    z_T = np.zeros_like(mu_T)\n    z_W = np.zeros_like(mu_W)\n    for i in range(A.shape[0]):\n        if sigma_T[i]  0:\n            z_T[i] = (T_obs[i] - mu_T[i]) / sigma_T[i]\n        else:\n            z_T[i] = 0.0\n        if sigma_W[i]  0:\n            z_W[i] = (W_obs[i] - mu_W[i]) / sigma_W[i]\n        else:\n            z_W[i] = 0.0\n    roles = []\n    for i in range(A.shape[0]):\n        if degrees[i]  2:\n            roles.append(0)  # peripheral\n        else:\n            if z_T[i] = z_W[i]:\n                roles.append(1)  # clique-oriented\n            else:\n                roles.append(2)  # broker-oriented\n    return roles\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, edges, R, S, seed)\n        (6, [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(0,4),(4,5)], 200, 400, 202311),\n        (6, [(0,1),(1,2),(2,3),(3,4),(4,5),(5,0)], 200, 400, 202312),\n        (6, [(0,1),(0,2),(0,3),(0,4),(0,5)], 200, 400, 202313),\n    ]\n\n    results = []\n    for n, edges, R, S, seed in test_cases:\n        A = build_adjacency(n, edges)\n        roles = classify_roles(A, R, S, seed)\n        results.append(roles)\n\n    # Format the output as a single line with no spaces: [[...],[...],[...]]\n    formatted = \"[\" + \",\".join(\"[\" + \",\".join(map(str, r)) + \"]\" for r in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}