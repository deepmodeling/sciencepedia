{
    "hands_on_practices": [
        {
            "introduction": "Before modeling the effects of selection or bias, we must first establish a null model: what happens to cultural variants due to chance alone in finite populations? This exercise  guides you through a foundational derivation using the Wright-Fisher model, the bedrock for understanding cultural drift. By proving that a neutral variant's fixation probability equals its initial frequency, you will master a key baseline expectation for any evolutionary process and gain practice with the powerful tools of stochastic process theory.",
            "id": "2699390",
            "problem": "Consider a finite population of constant size $N$ evolving under the Wright–Fisher copying model for cultural transmission with no innovation. Time is discrete, indexed by $t \\in \\{0,1,2,\\dots\\}$, and each generation $t+1$ is formed by sampling $N$ cultural parents with replacement from generation $t$, each draw choosing a parent uniformly from the $N$ individuals in generation $t$. There are two cultural variants, denoted $A$ and $B$. Let $X_t \\in \\{0,1,\\dots,N\\}$ be the number of $A$-bearers in generation $t$, and let $x_t = X_t/N$ be the corresponding frequency. Assume neutrality and no innovation: in every draw the parent is chosen independently and uniformly, so that conditional on $X_t=i$, the distribution of $X_{t+1}$ is $\\mathrm{Binomial}(N, i/N)$.\n\nDefine the fixation probability $\\phi(i;N)$ to be the probability that variant $A$ eventually reaches frequency $1$ (i.e., $X_t$ hits the absorbing state $N$) starting from $X_0=i$. Equivalently, define $\\phi(x_0;N)$ with $x_0=i/N$.\n\nUsing only the fundamental definitions above, derive a closed-form expression for the fixation probability $\\phi(x_0;N)$ as a function of the initial frequency $x_0 \\in [0,1]$ and the population size $N$. Express your final answer as a single analytic expression. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the fixation probability of a neutral cultural variant in a finite population of size $N$ evolving under the Wright–Fisher model. Let $X_t$ be the number of individuals carrying variant $A$ at generation $t$, and let $x_t = X_t/N$ be the frequency of this variant. The process starts with an initial count $X_0=i$, corresponding to an initial frequency $x_0 = i/N$. We are tasked with finding the probability that the variant $A$ eventually becomes fixed, meaning its frequency reaches $1$. This probability is denoted by $\\phi(x_0;N)$.\n\nThe state space of the process is $\\{0, 1, \\dots, N\\}$. The states $0$ and $N$ are absorbing states, corresponding to the loss and fixation of variant $A$, respectively. The process evolves according to the transition rule that, conditional on $X_t=k$, the number of $A$-bearers in the next generation, $X_{t+1}$, follows a binomial distribution:\n$$ X_{t+1} | (X_t = k) \\sim \\mathrm{Binomial}(N, k/N) $$\n\nA direct and rigorous method to solve this problem is to analyze the properties of the frequency process $\\{x_t\\}_{t \\geq 0}$. We will demonstrate that under the specified neutral model, the frequency of the variant is a martingale. A process $\\{Y_t\\}$ is a martingale if $\\mathbb{E}[Y_{t+1} | Y_t, Y_{t-1}, \\dots, Y_0] = Y_t$.\n\nLet us compute the conditional expectation of $x_{t+1}$ given the state at time $t$. Suppose at time $t$, the state is $X_t = i$. The frequency is $x_t = i/N$. The expected number of $A$-bearers at time $t+1$ is the expectation of a $\\mathrm{Binomial}(N, p=i/N)$ random variable. The expectation of a binomial distribution $\\mathrm{Binomial}(n,p)$ is $np$. Therefore,\n$$ \\mathbb{E}[X_{t+1} | X_t = i] = N \\cdot \\left(\\frac{i}{N}\\right) = i $$\nNow we can find the expected frequency at time $t+1$:\n$$ \\mathbb{E}[x_{t+1} | X_t = i] = \\mathbb{E}\\left[\\frac{X_{t+1}}{N} \\Big| X_t = i\\right] = \\frac{1}{N} \\mathbb{E}[X_{t+1} | X_t = i] = \\frac{i}{N} $$\nSince we have defined $x_t = i/N$, this shows that $\\mathbb{E}[x_{t+1} | x_t] = x_t$. This equality confirms that the frequency process $\\{x_t\\}$ is a martingale.\n\nThe process stops when it reaches one of the absorbing boundaries, either $x_t=0$ (loss) or $x_t=1$ (fixation). Let $T$ be the stopping time of this process, defined as:\n$$ T = \\inf\\{t \\geq 0 : x_t = 0 \\text{ or } x_t = 1\\} $$\nFor a finite population $N$, eventual absorption into one of these states is guaranteed, meaning the stopping time $T$ is almost surely finite. The martingale $\\{x_t\\}$ is bounded, as its values are always within the interval $[0,1]$.\n\nThese conditions—a bounded martingale and an almost surely finite stopping time—allow us to apply the Optional Stopping Theorem. The theorem states that the expected value of the martingale at the stopping time is equal to its initial value:\n$$ \\mathbb{E}[x_T] = x_0 $$\nThe value of the process at the stopping time, $x_T$, is a random variable that can only take one of two values: $1$ if the variant $A$ fixes, or $0$ if it is lost. The probability of fixation is precisely what we aim to find, $\\phi(x_0;N)$. Thus,\n$$ P(x_T = 1) = \\phi(x_0;N) $$\nAnd the probability of loss is:\n$$ P(x_T = 0) = 1 - \\phi(x_0;N) $$\nThe expectation $\\mathbb{E}[x_T]$ can be calculated from its definition:\n$$ \\mathbb{E}[x_T] = (1) \\cdot P(x_T = 1) + (0) \\cdot P(x_T = 0) = 1 \\cdot \\phi(x_0;N) + 0 \\cdot (1 - \\phi(x_0;N)) = \\phi(x_0;N) $$\nBy equating the two expressions for $\\mathbb{E}[x_T]$, we obtain the final result:\n$$ \\phi(x_0;N) = x_0 $$\nThis result demonstrates a fundamental principle of population genetics: in the absence of selection, mutation, or migration, the probability that a neutral allele or cultural variant will eventually become fixed in the population is equal to its initial frequency. The result is independent of the population size $N$. Given the initial frequency $x_0 = i/N$, the fixation probability is simply $i/N$.",
            "answer": "$$\\boxed{x_{0}}$$"
        },
        {
            "introduction": "While drift is a crucial baseline, much of cultural evolution is driven by the fact that some behaviors are more beneficial or effective than others. Replicator dynamics provide a powerful framework for modeling this selection process, where successful traits spread through imitation. In this practice , you will analyze a classic coordination game to see how payoff differences between traits create distinct evolutionary outcomes, including multiple stable equilibria and critical social tipping points.",
            "id": "4119283",
            "problem": "Consider a large, well-mixed population in which individuals carry one of two cultural traits, labeled $T_{1}$ and $T_{2}$. When individuals interact, they receive payoffs determined by a symmetric coordination interaction with payoff matrix $A$, where $A_{11}=a$, $A_{22}=a$, and $A_{12}=A_{21}=b$, with $b<a$. Cultural transmission proceeds by proportional imitation: the expected change in the frequency $x$ of trait $T_{1}$ over time is governed by the standard replicator dynamics, defined by $ \\dot{x} = x \\left( \\pi_{1}(x) - \\bar{\\pi}(x) \\right)$, where $\\pi_{1}(x)$ is the expected payoff to $T_{1}$ at frequency $x$, and $\\bar{\\pi}(x)$ is the population average payoff.\n\nStarting from the above definitions and the specification of the payoff matrix $A$, do the following:\n- Express $\\pi_{1}(x)$, $\\pi_{2}(x)$, and $\\bar{\\pi}(x)$ in terms of $x$, $a$, and $b$.\n- Derive the one-dimensional replicator dynamics $ \\dot{x} = g(x; a,b)$ explicitly as a polynomial in $x$ and the parameters $a$ and $b$.\n- Compute all fixed points of the dynamics by solving $g(x; a,b)=0$ on the interval $[0,1]$.\n- Assess the local stability of each fixed point by linearizing the dynamics and evaluating the sign of the derivative $g'(x; a,b)$ at each fixed point, under the condition $b<a$. Explain the basin structure implied by your stability results.\n\nYour reasoning must begin from the core definitions stated above and proceed step by step. As the final deliverable, report only the internal fixed point frequency $x^{\\ast}$ as a simplified analytic expression in terms of $a$ and $b$. Do not include any units. No numerical rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a complete analytical solution. It is a standard problem in the field of evolutionary game theory, specifically modeling cultural evolution with replicator dynamics.\n\nThe task is to analyze the dynamics of two cultural traits, $T_1$ and $T_2$, in a large, well-mixed population. The frequency of trait $T_1$ is denoted by $x$, and thus the frequency of trait $T_2$ is $1-x$. The problem specifies a symmetric coordination game with the payoff matrix $A$ given by $A_{11}=a$, $A_{22}=a$, and $A_{12}=A_{21}=b$, with the condition $b<a$.\n\nFirst, we express the expected payoffs for individuals carrying each trait. The expected payoff to a $T_1$ individual, $\\pi_{1}(x)$, is calculated by considering the probabilities of interacting with a $T_1$ or a $T_2$ individual.\n$$\n\\pi_{1}(x) = x \\cdot A_{11} + (1-x) \\cdot A_{12} = ax + b(1-x)\n$$\nSimplifying this expression gives:\n$$\n\\pi_{1}(x) = (a-b)x + b\n$$\nSimilarly, the expected payoff to a $T_2$ individual, $\\pi_{2}(x)$, is:\n$$\n\\pi_{2}(x) = x \\cdot A_{21} + (1-x) \\cdot A_{22} = bx + a(1-x)\n$$\nSimplifying this expression gives:\n$$\n\\pi_{2}(x) = -(a-b)x + a\n$$\nThe population average payoff, $\\bar{\\pi}(x)$, is the frequency-weighted average of the individual payoffs:\n$$\n\\bar{\\pi}(x) = x \\pi_{1}(x) + (1-x) \\pi_{2}(x)\n$$\nSubstituting the expressions for $\\pi_1(x)$ and $\\pi_2(x)$:\n$$\n\\bar{\\pi}(x) = x(ax + b(1-x)) + (1-x)(bx + a(1-x))\n$$\n$$\n\\bar{\\pi}(x) = ax^2 + bx - bx^2 + bx - bx^2 + a(1-2x+x^2)\n$$\n$$\n\\bar{\\pi}(x) = ax^2 + 2bx - 2bx^2 + a - 2ax + ax^2\n$$\n$$\n\\bar{\\pi}(x) = (2a - 2b)x^2 + (2b - 2a)x + a = 2(a-b)x^2 - 2(a-b)x + a\n$$\nNext, we derive the one-dimensional replicator dynamics, $\\dot{x} = g(x; a,b)$. The governing equation is given as $\\dot{x} = x(\\pi_1(x) - \\bar{\\pi}(x))$. It is a standard result that for a two-strategy game, this can be rewritten as:\n$$\n\\dot{x} = x(1-x)(\\pi_1(x) - \\pi_2(x))\n$$\nWe calculate the difference in payoffs:\n$$\n\\pi_1(x) - \\pi_2(x) = ((a-b)x + b) - (-(a-b)x + a) = 2(a-b)x - (a-b) = (a-b)(2x-1)\n$$\nSubstituting this result into the dynamics equation yields the function $g(x; a,b)$:\n$$\n\\dot{x} = g(x; a,b) = x(1-x)(a-b)(2x-1)\n$$\nTo express this as a polynomial in $x$, we expand the expression:\n$$\ng(x; a,b) = (x-x^2)(a-b)(2x-1) = (a-b)(2x^2 - x - 2x^3 + x^2)\n$$\n$$\ng(x; a,b) = (a-b)(-2x^3 + 3x^2 - x)\n$$\nThis is the required polynomial form of the replicator dynamics.\n\nNow, we compute the fixed points of the dynamics by solving $g(x; a,b)=0$ for $x \\in [0,1]$.\n$$\nx(1-x)(a-b)(2x-1) = 0\n$$\nGiven the condition $b<a$, the term $a-b$ is non-zero. Therefore, the equation is satisfied if and only if one of the other factors is zero:\n$x = 0$\n$1-x = 0 \\implies x = 1$\n$2x-1 = 0 \\implies x = \\frac{1}{2}$\nThe fixed points are $x^*_1=0$, $x^*_2=1$, and $x^*_3=\\frac{1}{2}$. All lie within the interval $[0,1]$. The point $x^*_3=\\frac{1}{2}$ is the unique internal fixed point.\n\nTo assess the local stability of each fixed point, we analyze the sign of the derivative $g'(x; a,b)$ at each point. A fixed point $x^*$ is locally stable if $g'(x^*) < 0$ and unstable if $g'(x^*) > 0$.\nWe first find the derivative of $g(x; a,b) = (a-b)(-2x^3 + 3x^2 - x)$:\n$$\ng'(x; a,b) = \\frac{d}{dx} \\left[ (a-b)(-2x^3 + 3x^2 - x) \\right] = (a-b)(-6x^2 + 6x - 1)\n$$\nWe use the fact that $a-b > 0$.\n\nFor the fixed point $x^*_1=0$:\n$$\ng'(0) = (a-b)(-6(0)^2 + 6(0) - 1) = -(a-b)\n$$\nSince $a-b>0$, $g'(0)<0$. Therefore, $x^*_1=0$ is a locally stable fixed point.\n\nFor the fixed point $x^*_2=1$:\n$$\ng'(1) = (a-b)(-6(1)^2 + 6(1) - 1) = (a-b)(-6 + 6 - 1) = -(a-b)\n$$\nSince $a-b>0$, $g'(1)<0$. Therefore, $x^*_2=1$ is also a locally stable fixed point.\n\nFor the internal fixed point $x^*_3=\\frac{1}{2}$:\n$$\ng'\\left(\\frac{1}{2}\\right) = (a-b)\\left(-6\\left(\\frac{1}{2}\\right)^2 + 6\\left(\\frac{1}{2}\\right) - 1\\right) = (a-b)\\left(-6\\left(\\frac{1}{4}\\right) + 3 - 1\\right)\n$$\n$$\ng'\\left(\\frac{1}{2}\\right) = (a-b)\\left(-\\frac{3}{2} + 2\\right) = (a-b)\\left(\\frac{1}{2}\\right) = \\frac{a-b}{2}\n$$\nSince $a-b>0$, $g'(\\frac{1}{2})>0$. Therefore, $x^*_3=\\frac{1}{2}$ is an unstable fixed point.\n\nThe basin structure is determined by these stability results. The state space $[0,1]$ is partitioned by the unstable internal fixed point $x^*=\\frac{1}{2}$. This point acts as a separatrix.\nThe basin of attraction for the stable fixed point $x^*=0$ is the interval $[0, \\frac{1}{2})$. Any initial frequency $x(0)$ in this interval will lead to the extinction of trait $T_1$ and fixation of trait $T_2$.\nThe basin of attraction for the stable fixed point $x^*=1$ is the interval $(\\frac{1}{2}, 1]$. Any initial frequency $x(0)$ in this interval will lead to the fixation of trait $T_1$ and the extinction of trait $T_2$.\nThis dynamic reflects the nature of a coordination game: whichever trait is initially more common (i.e., has a frequency greater than $\\frac{1}{2}$) will be reinforced by selection until it becomes the sole trait in the population.\n\nThe problem asks for the internal fixed point frequency $x^*$ as a simplified analytic expression. Based on our analysis, this is $x^*_3$.\n$$\nx^* = \\frac{1}{2}\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Theoretical models provide essential insights, but cultural evolution is ultimately an empirical science that must connect with data. This exercise  introduces you to the Price equation, a powerful accounting framework for partitioning observed change into distinct causal components. By applying this method to raw transmission data, you will learn to empirically distinguish the effects of cultural selection (some individuals are copied more than others) from the effects of transmission bias (traits are systematically altered during learning).",
            "id": "2699362",
            "problem": "You are given individual-level cultural data for a single generational step in a population with a set of cultural models (parents) and a set of learners. Each learner chooses one model to learn from and acquires a possibly modified trait value. Your task is to compute two components of population-level cultural change that partition the change in the mean trait value into a component attributable to cultural selection (differential influence of models) and a component attributable to transmission bias (systematic within-line changes during learning).\n\nFundamental base and definitions:\n- Let there be $N$ models indexed by $i \\in \\{1,\\dots,N\\}$, each with trait value $z_i \\in \\mathbb{R}$. Let the mean model trait be $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$.\n- Let there be $M$ learners indexed by $j \\in \\{1,\\dots,M\\}$. Each learner $j$ chooses exactly one model $m(j) \\in \\{1,\\dots,N\\}$ and acquires trait $z'_j \\in \\mathbb{R}$ after learning.\n- The cultural influence (number of learners) of model $i$ is $w_i = \\left| \\{ j : m(j) = i \\} \\right|$. The mean influence is $\\bar{w} = \\frac{1}{N} \\sum_{i=1}^N w_i$. Note that $\\sum_{i=1}^N w_i = M$ implies $\\bar{w} = M / N$.\n- Define the per-model learner mean $\\mu_i$ as follows: if $w_i > 0$, then $\\mu_i = \\frac{1}{w_i} \\sum_{j : m(j) = i} z'_j$; if $w_i = 0$, adopt the convention $\\mu_i = z_i$ so that $\\mu_i - z_i = 0$ contributes nothing to the transmission bias.\n\nYour program must compute, for each provided dataset:\n1. The cultural selection covariance term $S$, defined as the covariance between relative influence and model trait,\n$$\nS \\equiv \\operatorname{Cov}\\!\\left(\\frac{w_i}{\\bar{w}}, z_i\\right) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}} - 1\\right)\\left(z_i - \\bar{z}\\right).\n$$\n2. The transmission bias term $T$, defined as the expected within-line change weighted by relative influence,\n$$\nT \\equiv \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right)\\left(\\mu_i - z_i\\right).\n$$\n3. The total change in the mean trait across the generational step,\n$$\n\\Delta \\bar{z} \\equiv \\left(\\frac{1}{M}\\sum_{j=1}^M z'_j\\right) - \\bar{z}.\n$$\n4. An interpretation code indicating which component has the larger magnitude. Let $\\varepsilon = 10^{-9}$. Define\n$$\nd \\equiv \\left|S\\right| - \\left|T\\right|.\n$$\nReturn an integer code per dataset as follows: if $d > \\varepsilon$, return $1$ (selection magnitude exceeds transmission); if $d < -\\varepsilon$, return $-1$ (transmission magnitude exceeds selection); otherwise return $0$ (magnitudes are comparable within tolerance).\n\nInput specification for each dataset:\n- $N$ and the model trait vector $\\mathbf{z} = [z_1,\\dots,z_N]$.\n- The learner-to-model index vector $\\mathbf{m} = [m(1),\\dots,m(M)]$ with zero-based indices in $\\{0,\\dots,N-1\\}$.\n- The learner trait vector after learning $\\mathbf{z}' = [z'_1,\\dots,z'_M]$.\nAll quantities are dimensionless real numbers.\n\nTest suite:\nProvide results for the following four datasets. Indices in $\\mathbf{m}$ are zero-based.\n\n- Dataset A:\n  - $N = 4$.\n  - $\\mathbf{z} = [0.0, 1.0, 2.0, 3.0]$.\n  - $\\mathbf{m} = [0, 1, 1, 2, 2, 3, 3, 3, 3, 3]$.\n  - $\\mathbf{z}' = [0.2, 1.0, 1.0, 1.9, 1.9, 2.8, 2.8, 2.8, 2.9, 2.9]$.\n\n- Dataset B:\n  - $N = 3$.\n  - $\\mathbf{z} = [1.0, 2.0, 3.0]$.\n  - $\\mathbf{m} = [0, 0, 1, 1, 2, 2]$.\n  - $\\mathbf{z}' = [1.3, 1.3, 2.0, 2.0, 2.8, 2.8]$.\n\n- Dataset C:\n  - $N = 3$.\n  - $\\mathbf{z} = [0.0, 1.0, 3.0]$.\n  - $\\mathbf{m} = [0, 1, 1, 2, 2, 2]$.\n  - $\\mathbf{z}' = [0.0, 1.0, 1.0, 3.0, 3.0, 3.0]$.\n\n- Dataset D:\n  - $N = 4$.\n  - $\\mathbf{z} = [1.0, 2.0, 3.0, 4.0]$.\n  - $\\mathbf{m} = [1, 2, 3, 3]$.\n  - $\\mathbf{z}' = [1.9, 3.2, 3.7, 3.9]$.\n\nOutput specification:\n- For each dataset, output a list of the form $[S, T, \\Delta\\bar{z}, \\text{code}]$ where $S$, $T$, and $\\Delta\\bar{z}$ are floats rounded to six decimal places, and $\\text{code}$ is an integer as defined above.\n- Your program should produce a single line of output containing the results for all datasets as a comma-separated list of these lists enclosed in square brackets. For example, the output format must be\n$[[S_A,T_A,\\Delta\\bar{z}_A,\\text{code}_A],[S_B,T_B,\\Delta\\bar{z}_B,\\text{code}_B],[S_C,T_C,\\Delta\\bar{z}_C,\\text{code}_C],[S_D,T_D,\\Delta\\bar{z}_D,\\text{code}_D]]$\nwith the required rounding applied.",
            "solution": "The definitions and computations requested are a direct application of the Price equation framework, a cornerstone of evolutionary analysis, adapted for the study of cultural evolution. All terms are defined with mathematical precision, the data provided are self-contained, and the objectives are unambiguous. We shall proceed with the derivation of the solution.\n\nThe core task is to partition the total change in the mean population trait value, $\\Delta\\bar{z}$, into two components: a selection component, $S$, and a transmission bias component, $T$. The problem provides definitions for these three quantities. A fundamental check of consistency is to demonstrate that the total change is, in fact, the sum of the two components, i.e., $\\Delta\\bar{z} = S + T$. This identity forms the basis of our analysis.\n\nLet us begin with the definitions provided. The mean trait of the $N$ models is $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$. The mean trait of the $M$ learners in the next generation is $\\bar{z}' = \\frac{1}{M}\\sum_{j=1}^M z'_j$. The total change is $\\Delta\\bar{z} = \\bar{z}' - \\bar{z}$.\n\nWe can express $\\bar{z}'$ by grouping learners according to their chosen model $m(j)$. The number of learners who chose model $i$ is its influence, $w_i$. The sum of the trait values of learners who chose model $i$ is $\\sum_{j : m(j) = i} z'_j$. By definition, the per-model learner mean is $\\mu_i = \\frac{1}{w_i} \\sum_{j : m(j) = i} z'_j$ for $w_i > 0$. Therefore, $\\sum_{j : m(j) = i} z'_j = w_i \\mu_i$. This relation holds even for $w_i=0$, as both sides are zero. The total sum of learner traits is $\\sum_{j=1}^M z'_j = \\sum_{i=1}^N \\sum_{j : m(j) = i} z'_j = \\sum_{i=1}^N w_i \\mu_i$.\n\nSubstituting this into the expression for $\\bar{z}'$:\n$$\n\\bar{z}' = \\frac{1}{M} \\sum_{i=1}^N w_i \\mu_i\n$$\nThe mean influence is $\\bar{w} = \\sum_{i=1}^N w_i / N = M/N$. Thus, $M = N\\bar{w}$. Substituting this for $M$:\n$$\n\\bar{z}' = \\frac{1}{N\\bar{w}} \\sum_{i=1}^N w_i \\mu_i = \\frac{1}{N} \\sum_{i=1}^N \\frac{w_i}{\\bar{w}} \\mu_i\n$$\nNow, we can write the total change $\\Delta\\bar{z}$ as:\n$$\n\\Delta\\bar{z} = \\bar{z}' - \\bar{z} = \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) \\mu_i - \\bar{z}\n$$\nTo partition this change, we introduce the term $\\frac{1}{N} \\sum_{i=1}^N (\\frac{w_i}{\\bar{w}}) z_i$ by adding and subtracting it:\n$$\n\\Delta\\bar{z} = \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) \\mu_i - \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) z_i \\right) + \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) z_i - \\bar{z} \\right)\n$$\nLet us analyze the two parenthesized terms. The first term can be written as:\n$$\n\\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) (\\mu_i - z_i)\n$$\nThis is precisely the definition of the transmission bias term, $T$. It represents the average change in trait value from model to learner, weighted by the relative influence of each model. The convention $\\mu_i = z_i$ when $w_i = 0$ ensures that models with no learners contribute nothing to this term.\n\nThe second term can be recognized as the covariance between relative influence $\\frac{w_i}{\\bar{w}}$ and model trait $z_i$. The expectation of a variable $X_i$ over the population of models is $E[X] = \\frac{1}{N}\\sum_{i=1}^N X_i$. The covariance is $\\operatorname{Cov}(X, Y) = E[(X-E[X])(Y-E[Y])]$.\nLet $X_i = \\frac{w_i}{\\bar{w}}$ and $Y_i = z_i$. Then $E[Y] = \\bar{z}$. The expectation of $X_i$ is $E[X] = \\frac{1}{N}\\sum_{i=1}^N \\frac{w_i}{\\bar{w}} = \\frac{1}{N\\bar{w}}\\sum_{i=1}^N w_i = \\frac{M}{N(M/N)} = 1$.\nThe second term is $E[XY] - \\bar{z} = E[XY] - E[X]E[Y]$, which is $\\operatorname{Cov}(X,Y)$. Expanding this gives:\n$$\n\\operatorname{Cov}\\left(\\frac{w_i}{\\bar{w}}, z_i\\right) = \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}} - 1\\right) (z_i - \\bar{z})\n$$\nThis is precisely the definition of the cultural selection term, $S$. It measures the statistical association between a model's trait value and its cultural influence.\n\nThus, we have demonstrated the identity $\\Delta\\bar{z} = S + T$. The total change is perfectly partitioned. The computational procedure is as follows:\n\nFor each dataset ($N$, $\\mathbf{z}$, $\\mathbf{m}$, $\\mathbf{z}'$):\n1.  Compute the mean model trait $\\bar{z} = \\frac{1}{N}\\sum z_i$.\n2.  Determine the number of learners $M$ from the length of $\\mathbf{m}$.\n3.  Calculate the influence vector $\\mathbf{w}$, where $w_i$ is the count of model index $i$ in $\\mathbf{m}$. The indices in $\\mathbf{m}$ are given as zero-based, $\\{0, \\dots, N-1\\}$.\n4.  Compute the mean influence $\\bar{w} = M/N$.\n5.  Determine the per-model learner mean vector $\\boldsymbol{\\mu}$. For each model $i$, if $w_i > 0$, $\\mu_i$ is the mean of $z'_j$ for all learners $j$ who chose model $i$. If $w_i = 0$, $\\mu_i$ is set to $z_i$.\n6.  Calculate the selection term $S = \\frac{1}{N}\\sum_{i=0}^{N-1} (\\frac{w_i}{\\bar{w}} - 1)(z_i - \\bar{z})$.\n7.  Calculate the transmission bias term $T = \\frac{1}{N}\\sum_{i=0}^{N-1} (\\frac{w_i}{\\bar{w}})(\\mu_i - z_i)$.\n8.  Calculate the total change $\\Delta\\bar{z} = (\\frac{1}{M}\\sum z'_j) - \\bar{z}$. As a verification, one must confirm that $S + T$ is approximately equal to $\\Delta\\bar{z}$.\n9.  Compute the magnitude difference $d = |S| - |T|$ and determine the interpretation code based on the given tolerance $\\varepsilon = 10^{-9}$. If $d > \\varepsilon$, code is $1$; if $d < -\\varepsilon$, code is $-1$; otherwise, code is $0$.\n10. Format the results as a list $[S, T, \\Delta\\bar{z}, \\text{code}]$, with floating-point numbers rounded to six decimal places.\n\nThis algorithm will be implemented to process the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cultural evolution problem for all given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"N\": 4,\n            \"z_models\": [0.0, 1.0, 2.0, 3.0],\n            \"m_learners\": [0, 1, 1, 2, 2, 3, 3, 3, 3, 3],\n            \"z_prime_learners\": [0.2, 1.0, 1.0, 1.9, 1.9, 2.8, 2.8, 2.8, 2.9, 2.9],\n        },\n        # Dataset B\n        {\n            \"N\": 3,\n            \"z_models\": [1.0, 2.0, 3.0],\n            \"m_learners\": [0, 0, 1, 1, 2, 2],\n            \"z_prime_learners\": [1.3, 1.3, 2.0, 2.0, 2.8, 2.8],\n        },\n        # Dataset C\n        {\n            \"N\": 3,\n            \"z_models\": [0.0, 1.0, 3.0],\n            \"m_learners\": [0, 1, 1, 2, 2, 2],\n            \"z_prime_learners\": [0.0, 1.0, 1.0, 3.0, 3.0, 3.0],\n        },\n        # Dataset D\n        {\n            \"N\": 4,\n            \"z_models\": [1.0, 2.0, 3.0, 4.0],\n            \"m_learners\": [1, 2, 3, 3],\n            \"z_prime_learners\": [1.9, 3.2, 3.7, 3.9],\n        },\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        s, t, delta_z, code = calculate_components(\n            case[\"N\"],\n            case[\"z_models\"],\n            case[\"m_learners\"],\n            case[\"z_prime_learners\"]\n        )\n        \n        # Format the output for the current case\n        s_str = f\"{s:.6f}\"\n        t_str = f\"{t:.6f}\"\n        delta_z_str = f\"{delta_z:.6f}\"\n        \n        result_str = f\"[{s_str},{t_str},{delta_z_str},{code}]\"\n        results_str_list.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\ndef calculate_components(N_models, z_models_list, m_learners_list, z_prime_learners_list):\n    \"\"\"\n    Computes S, T, delta_z, and the interpretation code for a single dataset.\n    \"\"\"\n    \n    # Convert lists to numpy arrays for vectorized operations\n    z_models = np.array(z_models_list, dtype=float)\n    m_learners = np.array(m_learners_list, dtype=int)\n    z_prime_learners = np.array(z_prime_learners_list, dtype=float)\n    \n    # Number of learners\n    M_learners = len(m_learners)\n\n    # 1. Compute model-level quantities\n    z_bar = np.mean(z_models)\n\n    # 2. Compute learner-level and linking quantities\n    # Influence of each model (number of learners choosing each model)\n    w = np.bincount(m_learners, minlength=N_models)\n    \n    # Mean influence\n    if N_models > 0:\n        w_bar = M_learners / N_models\n    else:\n        w_bar = 0\n\n    # 3. Calculate per-model learner mean mu\n    mu = np.zeros(N_models, dtype=float)\n    for i in range(N_models):\n        if w[i] > 0:\n            learners_of_model_i = z_prime_learners[m_learners == i]\n            mu[i] = np.mean(learners_of_model_i)\n        else:\n            # Convention: if w_i = 0, mu_i = z_i\n            mu[i] = z_models[i]\n\n    # Handle case where all models have zero influence\n    if w_bar == 0:\n        rel_w = np.zeros(N_models, dtype=float)\n    else:\n        rel_w = w / w_bar\n\n    # 4. Calculate S (Selection)\n    # S = (1/N) * sum((w_i/w_bar - 1) * (z_i - z_bar))\n    s_term_per_model = (rel_w - 1) * (z_models - z_bar)\n    S = np.mean(s_term_per_model)\n\n    # 5. Calculate T (Transmission)\n    # T = (1/N) * sum((w_i/w_bar) * (mu_i - z_i))\n    t_term_per_model = rel_w * (mu - z_models)\n    T = np.mean(t_term_per_model)\n\n    # 6. Calculate Delta z_bar (Total Change)\n    if M_learners > 0:\n        z_prime_bar = np.mean(z_prime_learners)\n    else:\n        z_prime_bar = z_bar # No learners, no change\n    delta_z_bar = z_prime_bar - z_bar\n\n    # 7. Calculate the interpretation code\n    epsilon = 1e-9\n    d = abs(S) - abs(T)\n    if d > epsilon:\n        code = 1\n    elif d  -epsilon:\n        code = -1\n    else:\n        code = 0\n        \n    return S, T, delta_z_bar, code\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}