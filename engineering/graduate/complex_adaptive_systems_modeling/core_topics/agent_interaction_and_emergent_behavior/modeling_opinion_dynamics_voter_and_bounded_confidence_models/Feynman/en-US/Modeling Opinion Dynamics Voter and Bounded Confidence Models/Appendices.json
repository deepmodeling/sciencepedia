{
    "hands_on_practices": [
        {
            "introduction": "The standard Voter Model offers a minimalist framework for consensus formation, where agents simply adopt the opinions of their neighbors. A key to its analysis lies in understanding what properties, if any, are conserved as opinions evolve across a network. This exercise guides you through the use of martingale theory, a powerful tool from stochastic processes, to derive one of the model's most fundamental and elegant results concerning the ultimate probability of reaching a uniform consensus.",
            "id": "4129388",
            "problem": "Consider a finite, connected, undirected, simple graph $G$ on $N$ vertices that is $k$-regular for some integer $k \\geq 1$. Each vertex $i \\in \\{1,\\dots,N\\}$ holds an opinion $X_i(t) \\in \\{0,1\\}$ at discrete time $t \\in \\mathbb{N}$. The system evolves according to the standard voter model: at each time step $t \\mapsto t+1$, a single vertex $I_t$ is chosen uniformly at random from $\\{1,\\dots,N\\}$, and then a neighbor $J_t \\in \\mathcal{N}(I_t)$ is chosen uniformly at random among the $k$ neighbors of $I_t$. The vertex $I_t$ updates its opinion to match its chosen neighbor, so that $X_{I_t}(t+1) = X_{J_t}(t)$, while all other vertices keep their opinions, i.e., $X_i(t+1) = X_i(t)$ for all $i \\neq I_t$. Let the initial configuration be arbitrary and define the initial fraction of opinion $1$ as $p_0 := \\frac{1}{N}\\sum_{i=1}^{N} X_i(0)$.\n\nDefine the empirical fraction of opinion $1$ at time $t$ by $M(t) := \\frac{1}{N}\\sum_{i=1}^{N} X_i(t)$, and let $T$ be the first time the system reaches consensus, that is, $T := \\inf\\{t \\geq 0 : X_i(t) = X_j(t) \\text{ for all } i,j\\}$. The absorbing states are the all-$0$ configuration and the all-$1$ configuration.\n\nStarting from the fundamental definition of the voter model dynamics and using only first principles about conditional expectations and martingales, derive an expression for the probability that the system eventually reaches the all-$1$ consensus, in terms of $p_0$. Your derivation must be based on showing that $M(t)$ is a martingale with respect to the natural filtration and applying the Optional Stopping Theorem (OST) for martingales. Express your final answer as a single closed-form analytic expression. No numerical approximation or rounding is required, and no physical units apply.",
            "solution": "We begin by formalizing the dynamics and the quantity of interest. The voter model specified is a time-homogeneous Markov chain on the finite state space $\\{0,1\\}^{N}$. Let $\\mathcal{F}_t$ denote the natural filtration generated by the process up to time $t$, i.e., $\\mathcal{F}_t = \\sigma\\big((X_i(s))_{i=1}^{N}, 0 \\leq s \\leq t\\big)$. The quantity of interest is the probability that the system reaches the all-$1$ absorbing state. Let $A$ denote the event that consensus at time $T$ is all-$1$, i.e., $A := \\{X_i(T) = 1 \\text{ for all } i\\}$.\n\nWe define the empirical fraction of opinion $1$ at time $t$ as\n$$\nM(t) := \\frac{1}{N}\\sum_{i=1}^{N} X_i(t).\n$$\nWe will show that $\\{M(t)\\}_{t \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_t\\}_{t \\geq 0}$, and then apply the Optional Stopping Theorem (OST) for martingales to the stopping time $T$.\n\nStep $1$: Computation of the one-step conditional expectation of $M(t)$.\n\nBy construction, between time $t$ and $t+1$, only one coordinate may change, namely $X_{I_t}$. The update rule is $X_{I_t}(t+1) = X_{J_t}(t)$, with $I_t$ uniform over $\\{1,\\dots,N\\}$ and $J_t$ uniform over $\\mathcal{N}(I_t)$, independent of past given $\\mathcal{F}_t$. Therefore,\n$$\nM(t+1) - M(t) = \\frac{1}{N}\\big(X_{I_t}(t+1) - X_{I_t}(t)\\big) = \\frac{1}{N}\\big(X_{J_t}(t) - X_{I_t}(t)\\big).\n$$\nTaking conditional expectation given $\\mathcal{F}_t$ and using the independence and uniform choices,\n$$\n\\mathbb{E}\\big[M(t+1) - M(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\left(\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big] - \\mathbb{E}\\big[X_{I_t}(t) \\mid \\mathcal{F}_t\\big]\\right).\n$$\nWe compute the two expectations separately. Since $I_t$ is uniform on $\\{1,\\dots,N\\}$,\n$$\n\\mathbb{E}\\big[X_{I_t}(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\sum_{i=1}^{N} X_i(t) = M(t).\n$$\nFor $\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big]$, note that conditional on $I_t=i$, $J_t$ is uniform over $\\mathcal{N}(i)$, so\n$$\n\\mathbb{E}\\big[X_{J_t}(t) \\mid I_t=i, \\mathcal{F}_t\\big] = \\frac{1}{k}\\sum_{j \\in \\mathcal{N}(i)} X_j(t).\n$$\nAveraging over $I_t$,\n$$\n\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{k}\\sum_{j \\in \\mathcal{N}(i)} X_j(t).\n$$\nBecause the graph is $k$-regular and undirected, the sum $\\sum_{i=1}^{N}\\sum_{j \\in \\mathcal{N}(i)} X_j(t)$ counts each vertex $j$ exactly $\\deg(j)=k$ times across directed incidences $(i,j)$, hence\n$$\n\\sum_{i=1}^{N} \\sum_{j \\in \\mathcal{N}(i)} X_j(t) = \\sum_{j=1}^{N} X_j(t)\\deg(j) = k \\sum_{j=1}^{N} X_j(t).\n$$\nTherefore,\n$$\n\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\cdot \\frac{1}{k}\\cdot k \\sum_{j=1}^{N} X_j(t) = \\frac{1}{N}\\sum_{j=1}^{N} X_j(t) = M(t).\n$$\nCombining the two,\n$$\n\\mathbb{E}\\big[M(t+1) - M(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\big(M(t) - M(t)\\big) = 0,\n$$\nwhich implies\n$$\n\\mathbb{E}\\big[M(t+1) \\mid \\mathcal{F}_t\\big] = M(t).\n$$\nThus $\\{M(t)\\}_{t \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_t\\}_{t \\geq 0}$.\n\nStep $2$: Boundedness and applicability of the Optional Stopping Theorem (OST).\n\nBy definition, $M(t) \\in [0,1]$ for all $t$, so the martingale is bounded. On a finite, connected graph, the voter model almost surely reaches consensus in finite time due to its duality with coalescing random walks, which coalesce to a single particle almost surely; hence $T < \\infty$ almost surely. A bounded martingale is uniformly integrable, so the Optional Stopping Theorem (OST) applies to the stopping time $T$ and yields\n$$\n\\mathbb{E}\\big[M(T)\\big] = \\mathbb{E}\\big[M(0)\\big] = M(0) = p_0.\n$$\n\nStep $3$: Identification of $\\mathbb{E}\\big[M(T)\\big]$ with the probability of all-$1$ consensus.\n\nAt the stopping time $T$, the configuration is either all-$0$ or all-$1$. Therefore,\n$$\nM(T) = \\begin{cases}\n0 & \\text{if consensus is all-$0$},\\\\\n1 & \\text{if consensus is all-$1$}.\n\\end{cases}\n$$\nHence $M(T)$ is exactly the indicator of event $A$, i.e., $M(T) = \\mathbf{1}_{A}$. It follows that\n$$\n\\mathbb{E}\\big[M(T)\\big] = \\mathbb{E}\\big[\\mathbf{1}_{A}\\big] = \\mathbb{P}(A).\n$$\nCombining with the result from OST,\n$$\n\\mathbb{P}(\\text{consensus is all-$1$}) = \\mathbb{P}(A) = p_0.\n$$\n\nTherefore, the probability that the voter model on a finite, connected, undirected, $k$-regular graph reaches the absorbing state with all opinions equal to $1$ is equal to the initial fraction of opinion $1$, $p_0$.",
            "answer": "$$\\boxed{p_0}$$"
        },
        {
            "introduction": "While the pure Voter Model inevitably reaches consensus, real-world systems are subject to noise and random perturbations. This practice explores the symmetric noisy voter model, where spontaneous opinion flips compete with social influence. You will use a Fokker-Planck diffusion approximation to derive the critical condition under which the system's collective state transitions from a single central peak (unimodal) to two polarized peaks (bimodal), and then implement this criterion in a short program.",
            "id": "4129384",
            "problem": "Consider the symmetric noisy voter model on a complete graph with $N$ agents, where each agent holds a binary opinion in $\\{0,1\\}$. At continuous time, each agent updates as follows: with rate $c$, an agent copies the opinion of a randomly selected neighbor; with rate $a$, an agent spontaneously flips its opinion due to exogenous noise. Define the dimensionless noise parameter $\\eta = a/c$. Let $n$ denote the number of agents holding opinion $1$, and $x = n/N$ the fraction of opinion $1$.\n\nStarting from the birth-death process for $n$ on a complete graph, use the principle that for a one-dimensional birth-death chain, the macroscopic evolution for large $N$ can be approximated by a diffusion process in $x$ whose drift $f(x)$ and diffusion coefficient $g(x)$ can be derived from the microscopic transition rates. From this diffusion approximation and the stationary solution of the corresponding Fokker–Planck equation with zero probability current and reflecting boundaries at $x=0$ and $x=1$, derive a criterion for whether the stationary distribution of $x$ is unimodal (peaked at $x=1/2$) or bimodal (peaked near the boundaries $x=0$ and $x=1$) as the parameter $\\eta$ varies. Express the bifurcation condition entirely in terms of $N$ and $\\eta$ for fixed $c$.\n\nYour program must implement this criterion to classify the stationary distribution shape for a given $(N,\\eta)$ pair. Map the classification to an integer as follows: return $1$ if unimodal, return $-1$ if bimodal, and return $0$ if exactly at the bifurcation (critical case). The classification must be determined by a mathematically derived condition in terms of $N$ and $\\eta$.\n\nUse the following test suite of parameter values to cover normal behavior, below-threshold, above-threshold, and boundary cases:\n- $N=100$, $\\eta=0.020$\n- $N=100$, $\\eta=0.005$\n- $N=100$, $\\eta=0.010$\n- $N=2$, $\\eta=0.500$\n- $N=50$, $\\eta=0.000$\n- $N=50$, $\\eta=1.000$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5,r_6]$, where each $r_i$ is the integer classification for the corresponding test case in the order listed. There are no physical units involved. Angles do not appear. Percentages do not appear. All numerical outputs are integers.",
            "solution": "The problem statement is a valid, well-posed scientific question grounded in the theory of stochastic processes and its application to modeling opinion dynamics. All necessary parameters and definitions are provided, and there are no contradictions or ambiguities. We shall proceed with the derivation and solution.\n\nThe system consists of $N$ agents on a complete graph, each holding a binary opinion in $\\{0,1\\}$. The state of the system can be described by the number of agents with opinion $1$, denoted by $n$, where $n \\in \\{0, 1, \\dots, N\\}$. The fraction of agents with opinion $1$ is $x=n/N$.\n\nWe model the system's evolution as a one-dimensional birth-death process for the variable $n$. A \"birth\" corresponds to a transition from $n$ to $n+1$, and a \"death\" corresponds to a transition from $n$ to $n-1$. We first determine the transition rates, $T^+(n)$ for $n \\to n+1$ and $T^-(n)$ for $n \\to n-1$.\n\nA transition $n \\to n+1$ occurs if an agent with opinion $0$ changes its opinion to $1$. There are $N-n$ such agents. This change can happen in two ways:\n1. Spontaneous flip: An agent with opinion $0$ flips to $1$ due to exogenous noise. Each of the $N-n$ agents does so at a rate $a$. The total rate for this process is $a(N-n)$.\n2. Copying: An agent with opinion $0$ copies the opinion of a randomly selected neighbor. Since the graph is complete, any other agent is a neighbor. The fraction of agents with opinion $1$ is $n/N$. An agent copies a neighbor at rate $c$. Therefore, the total rate at which the $N-n$ agents with opinion $0$ flip to $1$ via copying is $c (N-n) \\frac{n}{N}$.\n\nThe total birth rate is the sum of these two rates:\n$$T^+(n) = c(N-n)\\frac{n}{N} + a(N-n) = (N-n)\\left(c\\frac{n}{N} + a\\right)$$\n\nSimilarly, a transition $n \\to n-1$ occurs if an agent with opinion $1$ (of which there are $n$) changes its opinion to $0$.\n1. Spontaneous flip: An agent with opinion $1$ flips to $0$ at rate $a$. The total rate is $an$.\n2. Copying: An agent with opinion $1$ copies an agent with opinion $0$. The fraction of agents with opinion $0$ is $(N-n)/N$. The total rate is $c n \\frac{N-n}{N}$.\n\nThe total death rate is:\n$$T^-(n) = cn\\frac{N-n}{N} + an = n\\left(c\\frac{N-n}{N} + a\\right)$$\n\nFor large $N$, this birth-death process can be approximated by a continuous diffusion process for the fraction $x=n/N$. The evolution of the probability density $\\mathcal{P}(x,t)$ is described by the Fokker-Planck equation:\n$$\\frac{\\partial \\mathcal{P}(x, t)}{\\partial t} = -\\frac{\\partial}{\\partial x} [f(x) \\mathcal{P}(x,t)] + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} [g(x) \\mathcal{P}(x,t)]$$\nThe drift coefficient $f(x)$ and diffusion coefficient $g(x)$ are derived from the moments of the change $\\Delta x = \\pm 1/N$:\n$$f(x) = \\lim_{\\Delta t \\to 0} \\frac{\\langle \\Delta x \\rangle}{\\Delta t} = \\frac{1}{N} [T^+(Nx) - T^-(Nx)]$$\n$$g(x) = \\lim_{\\Delta t \\to 0} \\frac{\\langle (\\Delta x)^2 \\rangle}{\\Delta t} = \\frac{1}{N^2} [T^+(Nx) + T^-(Nx)]$$\n\nSubstituting the rates with $n=Nx$:\n$$f(x) = \\frac{1}{N} \\left[ (N-Nx)\\left(cx+a\\right) - Nx\\left(c(1-x)+a\\right) \\right]$$\n$$f(x) = (1-x)(cx+a) - x(c(1-x)+a) = cx(1-x)+a(1-x) - cx(1-x)-ax = a(1-2x)$$\n\n$$g(x) = \\frac{1}{N^2} \\left[ (N-Nx)\\left(cx+a\\right) + Nx\\left(c(1-x)+a\\right) \\right]$$\n$$g(x) = \\frac{1}{N} \\left[ (1-x)(cx+a) + x(c(1-x)+a) \\right] = \\frac{1}{N} [2cx(1-x) + a(1-x+x)] = \\frac{1}{N} [2cx(1-x)+a]$$\n\nThe stationary distribution $\\mathcal{P}_{st}(x)$ is found by setting the time derivative to zero, which implies a constant probability current. With reflecting boundaries at $x=0$ and $x=1$, this current is zero. The stationary solution satisfies:\n$$f(x) \\mathcal{P}_{st}(x) - \\frac{1}{2} \\frac{d}{dx} [g(x) \\mathcal{P}_{st}(x)] = 0$$\nThis can be solved to show that $\\mathcal{P}_{st}(x)$ is proportional to $\\frac{1}{g(x)} \\exp\\left(\\int \\frac{2f(x)}{g(x)} dx\\right)$.\n\nThe shape of the stationary distribution (unimodal vs. bimodal) is determined by its extrema, which occur where $\\frac{d\\mathcal{P}_{st}(x)}{dx} = 0$. It is more convenient to analyze the extrema of the potential $V(x) = -\\ln \\mathcal{P}_{st}(x)$. The maxima of $\\mathcal{P}_{st}(x)$ correspond to the minima of $V(x)$.\nThe condition for an extremum is $V'(x)=0$. From the zero-current equation, we have $2f(x)/g(x) = (d/dx)\\ln(g(x)\\mathcal{P}_{st}(x))$. Also, $V'(x) = -\\frac{\\mathcal{P}_{st}'(x)}{\\mathcal{P}_{st}(x)}$. The zero-current equation can be written as $f(x) = \\frac{1}{2}g'(x) + \\frac{1}{2}g(x)\\frac{\\mathcal{P}_{st}'(x)}{\\mathcal{P}_{st}(x)}$. Rearranging gives $-\\frac{\\mathcal{P}_{st}'(x)}{\\mathcal{P}_{st}(x)} = \\frac{g'(x)-2f(x)}{g(x)}$. Thus, the extrema are at the roots of $V'(x)=0$, which implies $g'(x) - 2f(x) = 0$.\n\nLet's compute the terms:\n$f(x) = a(1-2x)$\n$g'(x) = \\frac{d}{dx}\\left[\\frac{1}{N}(2cx-2cx^2+a)\\right] = \\frac{1}{N}(2c-4cx) = \\frac{2c}{N}(1-2x)$\n\nThe extremum condition becomes:\n$$\\frac{2c}{N}(1-2x) - 2a(1-2x) = 0$$\n$$2\\left(\\frac{c}{N} - a\\right)(1-2x) = 0$$\nThis equation always has a solution at $x=1/2$, which is the center of the opinion space. The distribution is symmetric, and this is its central extremum. Whether it is a maximum (unimodal distribution) or a minimum (bimodal distribution) depends on the sign of the second derivative, $V''(1/2)$.\n\nWe compute $V''(x) = \\frac{d}{dx}\\left(\\frac{g'(x)-2f(x)}{g(x)}\\right)$. At $x=1/2$, we have $f(1/2)=0$ and $g'(1/2)=0$.\nUsing the quotient rule, the numerator of $V''(1/2)$ is $(g''(1/2)-2f'(1/2))g(1/2) - (g'(1/2)-2f(1/2))g'(1/2)$. The second term is zero.\n$f'(x) = -2a$\n$g''(x) = -\\frac{4c}{N}$\n$g(1/2) = \\frac{1}{N}(2c(\\frac{1}{2})(1-\\frac{1}{2})+a) = \\frac{1}{N}(\\frac{c}{2}+a)$. As $c>0, a\\ge0, N>0$, we have $g(1/2) > 0$.\nSo, $V''(1/2) = \\frac{g''(1/2)-2f'(1/2)}{g(1/2)} = \\frac{-4c/N - 2(-2a)}{g(1/2)} = \\frac{4(a-c/N)}{g(1/2)}$.\n\nThe sign of $V''(1/2)$ is determined by the sign of $a-c/N$.\n1. Unimodal distribution: The distribution has a single peak at $x=1/2$. This corresponds to a minimum of the potential $V(x)$ at $x=1/2$, so $V''(1/2) > 0$. This requires $a-c/N > 0$, or $a > c/N$.\n2. Bimodal distribution: The distribution has two peaks near the boundaries and a minimum at $x=1/2$. This corresponds to a maximum of $V(x)$ at $x=1/2$, so $V''(1/2) < 0$. This requires $a-c/N < 0$, or $a < c/N$.\n3. Critical case (bifurcation): This occurs when the curvature at $x=1/2$ is zero, $V''(1/2) = 0$. This requires $a-c/N = 0$, or $a = c/N$.\n\nWe express this criterion in terms of the dimensionless noise parameter $\\eta = a/c$. Since $c$ is a rate, $c>0$.\n- Unimodal: $a/c > 1/N \\implies \\eta > 1/N$. Mapped to integer $1$.\n- Bimodal: $a/c < 1/N \\implies \\eta < 1/N$. Mapped to integer $-1$.\n- Critical: $a/c = 1/N \\implies \\eta = 1/N$. Mapped to integer $0$.\n\nThis provides the required mathematical criterion to classify the stationary distribution for any given pair $(N, \\eta)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies the derived criterion to classify the stationary distribution shape\n    for the noisy voter model based on N and eta.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, eta)\n        (100, 0.020),\n        (100, 0.005),\n        (100, 0.010),\n        (2, 0.500),\n        (50, 0.000),\n        (50, 1.000),\n    ]\n\n    results = []\n    for N, eta in test_cases:\n        # The bifurcation condition separating unimodal from bimodal\n        # stationary distributions for the symmetric noisy voter model\n        # on a complete graph is derived from a Fokker-Planck approximation.\n        # The criterion compares the dimensionless noise parameter eta = a/c\n        # with the inverse of the system size N.\n        #\n        # - Unimodal (peaked at x=1/2): eta > 1/N\n        # - Bimodal (peaked near x=0 and x=1): eta < 1/N\n        # - Critical (at the bifurcation): eta = 1/N\n\n        critical_eta = 1.0 / N\n\n        # Use np.isclose for robust floating-point comparison.\n        if np.isclose(eta, critical_eta):\n            # Critical case\n            classification = 0\n        elif eta > critical_eta:\n            # Unimodal case\n            classification = 1\n        else: # eta < critical_eta\n            # Bimodal case\n            classification = -1\n        \n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having studied the Voter and Bounded Confidence models, a critical question for any practitioner is: which model better explains an observed phenomenon? This capstone computational practice places you in the role of a researcher tasked with model selection. You will implement a complete validation framework to compare the out-of-sample predictive power of both models against synthetic data, a crucial skill for applying abstract models to real-world evidence.",
            "id": "4129409",
            "problem": "You are given a set of canonical definitions for two opinion dynamics models on a fixed undirected network and a method to generate synthetic observations. Your task is to implement a validation framework that integrates structural network data and observed opinion time series to discriminate between the voter model and the bounded confidence model by comparing their out-of-sample predictive performance. The output must be a single line containing the predicted model indices for a provided test suite.\n\nFundamental base. Consider a static, simple, undirected graph $G = (V, E)$ with $|V| = N$ nodes and adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$, where $A_{ij} = 1$ if $(i,j) \\in E$ and $A_{ij} = 0$ otherwise. Let $x(t) = (x_1(t), \\dots, x_N(t))$ denote the vector of opinions at discrete time $t$, with $t \\in \\{0, 1, 2, \\dots\\}$. One time step is defined as a sweep of $N$ asynchronous pairwise updates, where in each update an edge $(i,j) \\in E$ is sampled uniformly at random. Opinions evolve according to one of the following models:\n\n- Voter model. Opinions are binary with $x_i(t) \\in \\{0,1\\}$. At each asynchronous update, select a random edge $(i,j)$ uniformly and a random orientation; then the selected node copies the opinion of its neighbor:\n$$\nx_i(t+1) \\leftarrow x_j(t) \\quad \\text{or} \\quad x_j(t+1) \\leftarrow x_i(t).\n$$\nA sweep applies $N$ such updates.\n\n- Bounded confidence model (Deffuant-Weisbuch). Opinions are continuous with $x_i(t) \\in [0,1]$. At each asynchronous update, select a random edge $(i,j)$ uniformly. If the opinion difference is within a confidence bound $\\epsilon > 0$, both endpoints move toward each other by a convergence parameter $\\mu \\in (0, \\tfrac{1}{2}]$, using the symmetric update:\n$$\n\\text{if } |x_i - x_j| \\le \\epsilon: \\quad \\begin{cases}\nx_i \\leftarrow x_i + \\mu (x_j - x_i), \\\\\nx_j \\leftarrow x_j + \\mu (x_i - x_j),\n\\end{cases}\n\\quad \\text{else: no change.}\n$$\nA sweep applies $N$ such updates.\n\nObservation and prediction protocol. Let $T_{\\mathrm{obs}} \\in \\mathbb{N}$ denote the number of observed time steps and $T_{\\mathrm{pred}} \\in \\mathbb{N}$ the length of the held-out horizon. You are given both the network $A$ and the full time series $x(0), x(1), \\dots, x(T_{\\mathrm{obs}} + T_{\\mathrm{pred}})$, but you must fit models using only the first $T_{\\mathrm{obs}}$ steps and evaluate predictive performance on the held-out horizon of length $T_{\\mathrm{pred}}$. Use Monte Carlo ensemble forecasting with $K \\in \\mathbb{N}$ independent runs per forecast, starting from the last observed state $x(T_{\\mathrm{obs}})$ and simulating forward. For a forecast horizon $h \\in \\{1,\\dots,T_{\\mathrm{pred}}\\}$, the ensemble mean prediction $\\hat{x}(T_{\\mathrm{obs}}+h)$ is the average state across the $K$ simulated trajectories at step $h$.\n\nModel selection rule. Compute the out-of-sample mean squared error (Mean Squared Error (MSE)) for each candidate model by averaging the squared deviations between ensemble mean predictions and the held-out states across all nodes and forecast steps:\n$$\n\\mathrm{MSE}(M) = \\frac{1}{N \\, T_{\\mathrm{pred}}} \\sum_{h=1}^{T_{\\mathrm{pred}}} \\sum_{i=1}^{N} \\left( \\hat{x}_i(T_{\\mathrm{obs}}+h; M) - x_i(T_{\\mathrm{obs}}+h) \\right)^2.\n$$\nFor the bounded confidence model, select $(\\epsilon, \\mu)$ via a grid search that minimizes a training criterion computed only on the observed segment, specifically the average one-step-ahead MSE over $t \\in \\{0,\\dots,T_{\\mathrm{obs}}-1\\}$ using ensemble mean predictions from $x(t)$ to $x(t+1)$. For the voter model, there are no tunable parameters under the stated assumptions. After fitting, choose the model with the smaller out-of-sample $\\mathrm{MSE}$. In case of exact equality within numerical tolerance (take tolerance $\\tau = 10^{-9}$), select the voter model.\n\nTest suite and required outputs. Implement the above framework and apply it to the following four synthetic test cases. For reproducibility, fix all random seeds as specified. For each case, generate the network $A$, the initial opinions $x(0)$, and the full time series $x(t)$ for $t=1,\\dots,T_{\\mathrm{obs}}+T_{\\mathrm{pred}}$ under the stated true model and parameters. Use a time step definition of one sweep of $N$ asynchronous pairwise updates.\n\nCommon settings for all cases:\n- Number of nodes: $N = 30$.\n- Observed steps: $T_{\\mathrm{obs}} = 20$.\n- Held-out steps: $T_{\\mathrm{pred}} = 5$.\n- Monte Carlo ensemble size: $K = 50$.\n- Bounded confidence parameter grid for fitting: $\\epsilon \\in \\{0.15, 0.25, 0.35\\}$, $\\mu \\in \\{0.3, 0.5\\}$.\n- Tie-breaking tolerance for equal out-of-sample $\\mathrm{MSE}$: $\\tau = 10^{-9}$.\n\nCase definitions:\n1. Case $1$ (bounded confidence, ring graph):\n   - Network: a ring (cycle) of $N$ nodes, edges $(i,(i+1) \\bmod N)$.\n   - True model: bounded confidence with $\\epsilon_{\\mathrm{true}} = 0.25$, $\\mu_{\\mathrm{true}} = 0.5$.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Uniform}(0,1)$, use random seed $\\mathrm{seed} = 7$.\n\n2. Case $2$ (voter, Erdős–Rényi graph):\n   - Network: Erdős–Rényi $G(N,p)$ with $p = 0.2$, use graph seed $\\mathrm{seed} = 123$.\n   - True model: voter.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Bernoulli}(0.5)$, use random seed $\\mathrm{seed} = 5$.\n\n3. Case $3$ (bounded confidence, disconnected graph with two communities):\n   - Network: two disjoint rings of sizes $15$ and $15$ (block-diagonal adjacency with ring edges in each block).\n   - True model: bounded confidence with $\\epsilon_{\\mathrm{true}} = 0.20$, $\\mu_{\\mathrm{true}} = 0.5$.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Uniform}(0,1)$, use random seed $\\mathrm{seed} = 33$.\n\n4. Case $4$ (voter, star graph):\n   - Network: star graph on $N$ nodes with center node $0$ connected to all others.\n   - True model: voter.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Bernoulli}(0.5)$, use random seed $\\mathrm{seed} = 21$.\n\nProgram requirements:\n- Implement functions to simulate one time step (one sweep) for each model as defined above.\n- Implement ensemble forecasting and the training grid search for the bounded confidence parameters $(\\epsilon,\\mu)$ using only $x(0),\\dots,x(T_{\\mathrm{obs}})$.\n- Compute out-of-sample $\\mathrm{MSE}$ on $x(T_{\\mathrm{obs}}+1),\\dots,x(T_{\\mathrm{obs}}+T_{\\mathrm{pred}})$ for both models (using the fitted $(\\epsilon,\\mu)$ for bounded confidence) and select the model with the lower $\\mathrm{MSE}$, or the voter model if equal within tolerance $\\tau$.\n- For each test case, output the predicted model index as an integer: output $0$ for the voter model and $1$ for the bounded confidence model.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0,1,1,0]$). No additional text should be printed.\n\nNo physical units, angle units, or percentages are involved in this problem. All quantities are dimensionless real numbers or integers.",
            "solution": "The user-provided problem is a well-defined computational task in the field of complex adaptive systems modeling, specifically concerning model selection for opinion dynamics on networks. The problem is scientifically grounded, internally consistent, and possesses all necessary information for a unique, verifiable solution. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe core task is to discriminate between the Voter Model and the Bounded Confidence (BC) Model given a network structure and a time series of opinions. The methodology follows a standard empirical validation framework:\n1.  **Data Generation**: For each test case, generate a synthetic dataset consisting of a network and a full opinion time series, `x(0), ..., x(T_{obs} + T_{pred})`, according to a specified \"true\" model.\n2.  **Model Fitting**: The available data is partitioned into a training set, `x(0), ..., x(T_{obs})`, and a test set, `x(T_{obs}+1), ..., x(T_{obs}+T_{pred})`. The Voter model has no free parameters. The BC model's parameters, $(\\epsilon, \\mu)$, are fitted by minimizing a one-step-ahead prediction error on the training set via a grid search.\n3.  **Out-of-Sample Prediction**: Both the Voter model and the fitted BC model are used to generate forecasts for the test set horizon, `h = 1, ..., T_{pred}`. These forecasts start from the last observed state, `x(T_{obs})`, and are produced by averaging an ensemble of $K$ independent stochastic simulations.\n4.  **Model Selection**: The predictive accuracy of each model is quantified by the Mean Squared Error (MSE) on the test set. The model with the lower out-of-sample MSE is selected. A specific tie-breaking rule favors the Voter model.\n\nWe will systematically implement each component of this framework.\n\n### 1. Network and Data Generation\n\nFor each test case, we first construct the specified network topology as an adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$. From $A$, we derive an edge list $E$, which is more efficient for the simulation step requiring uniform random sampling of edges.\n\nThe initial opinion vector $x(0)$ is generated by drawing from either a Uniform distribution for the BC model or a Bernoulli distribution for the Voter model, using the specified random seeds for reproducibility.\n\nThe complete time series $x(1), \\dots, x(T_{\\mathrm{obs}}+T_{\\mathrm{pred}})$ is then generated by iteratively applying the update rule of the true model for $T_{\\mathrm{obs}}+T_{\\mathrm{pred}}$ time steps, starting from $x(0)$. A single time step involves a sweep of $N$ asynchronous updates as defined in the problem.\n\n### 2. Simulation of Opinion Dynamics\n\nThe evolution of opinions is governed by stochastic, pairwise interactions on the network. A single time step for both models consists of $N$ micro-updates. In each micro-update, an edge $(i,j)$ is chosen uniformly at random from the set of all edges $E$. The opinion state vector is updated sequentially within the sweep.\n\n**Voter Model Simulation Step**:\nGiven a state $x(t)$, the state $x(t+1)$ is obtained by performing $N$ updates on a copy of $x(t)$. For each update:\n1.  Sample an edge $(i,j) \\in E$ uniformly at random.\n2.  Sample a direction, i.e., choose one node to be the copier and the other the source. This is equivalent to choosing $u, v \\in \\{i,j\\}$ with $u \\neq v$.\n3.  Update the opinion: $x_u \\leftarrow x_v$.\n\n**Bounded Confidence Model Simulation Step**:\nGiven a state $x(t)$, parameters $(\\epsilon, \\mu)$, the state $x(t+1)$ is obtained by performing $N$ updates. For each update:\n1.  Sample an edge $(i,j) \\in E$ uniformly at random.\n2.  Check the confidence condition: if $|x_i - x_j| \\le \\epsilon$.\n3.  If the condition is met, perform the symmetric update. To avoid race conditions, the change is calculated based on the state before the update:\n    $$ \\Delta x = \\mu (x_j - x_i) $$\n    The new opinions are then:\n    $$ x_i \\leftarrow x_i + \\Delta x $$\n    $$ x_j \\leftarrow x_j - \\Delta x $$\n    If the condition is not met, no change occurs.\n\n### 3. Forecasting and Parameter Fitting\n\n**Ensemble Forecasting**:\nTo predict the system's evolution, we use Monte Carlo simulation. Starting from a given state $x_{\\mathrm{start}}$, we run $K$ independent simulations for a specified number of time steps, $T_{\\mathrm{forecast}}$. Each of the $K$ simulations produces a different trajectory due to the stochastic nature of the update rules. The ensemble mean prediction at a future step $h$ is the average of the opinion vectors across all $K$ trajectories at that step:\n$$ \\hat{x}(x_{\\mathrm{start}}, h) = \\frac{1}{K} \\sum_{k=1}^{K} x^{(k)}(h) $$\nwhere $x^{(k)}(h)$ is the state of the $k$-th simulation run at step $h$.\n\n**Fitting the Bounded Confidence Model**:\nThe BC model has two parameters, the confidence bound $\\epsilon$ and the convergence parameter $\\mu$. These are selected from a predefined grid, $\\{\\epsilon_1, \\dots, \\epsilon_m\\} \\times \\{\\mu_1, \\dots, \\mu_n\\}$. The optimal pair $(\\epsilon^*, \\mu^*)$ is the one that minimizes the in-sample, one-step-ahead prediction error. This training criterion is calculated over the observed time series $x(0), \\dots, x(T_{\\mathrm{obs}})$:\n$$ (\\epsilon^*, \\mu^*) = \\underset{(\\epsilon, \\mu)}{\\arg\\min} \\frac{1}{N T_{\\mathrm{obs}}} \\sum_{t=0}^{T_{\\mathrm{obs}}-1} \\sum_{i=1}^{N} \\left( \\hat{x}_i(x(t), 1; \\epsilon, \\mu) - x_i(t+1) \\right)^2 $$\nHere, $\\hat{x}(x(t), 1; \\epsilon, \\mu)$ is the one-step-ahead ensemble mean prediction starting from the true observed state $x(t)$, using parameters $(\\epsilon, \\mu)$.\n\n### 4. Model Selection\n\nAfter determining the optimal parameters $(\\epsilon^*, \\mu^*)$ for the BC model, we evaluate both candidate models on the held-out test data, which comprises the time steps $T_{\\mathrm{obs}}+1, \\dots, T_{\\mathrm{obs}}+T_{\\mathrm{pred}}$.\n\nFor each model $M \\in \\{\\text{Voter}, \\text{BC}(\\epsilon^*, \\mu^*)\\}$, we first generate the out-of-sample ensemble mean predictions, $\\hat{x}(T_{\\mathrm{obs}}+h; M)$, for the entire forecast horizon $h=1, \\dots, T_{\\mathrm{pred}}$. All forecasts start from the last observed state, $x(T_{\\mathrm{obs}})$.\n\nThe out-of-sample Mean Squared Error (MSE) is then computed for each model:\n$$ \\mathrm{MSE}(M) = \\frac{1}{N T_{\\mathrm{pred}}} \\sum_{h=1}^{T_{\\mathrm{pred}}} \\sum_{i=1}^{N} \\left( \\hat{x}_i(T_{\\mathrm{obs}}+h; M) - x_i(T_{\\mathrm{obs}}+h) \\right)^2 $$\nwhere $x_i(T_{\\mathrm{obs}}+h)$ are the \"ground truth\" values from the pre-generated time series.\n\nThe final decision rule is:\n1.  Calculate $\\mathrm{MSE}_{\\mathrm{Voter}}$ and $\\mathrm{MSE}_{\\mathrm{BC}}$.\n2.  If $\\mathrm{MSE}_{\\mathrm{BC}} < \\mathrm{MSE}_{\\mathrm{Voter}} - \\tau$, where $\\tau = 10^{-9}$ is the numerical tolerance, select the Bounded Confidence model (index $1$).\n3.  Otherwise, select the Voter model (index $0$). This includes the cases where $\\mathrm{MSE}_{\\mathrm{Voter}} \\le \\mathrm{MSE}_{\\mathrm{BC}}$ or when they are virtually equal.\n\nThis procedure is applied independently to each of the four test cases defined in the problem statement. The implementation will use `numpy` for efficient numerical computation and will carefully manage the random number generators with the specified seeds to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef make_ring_graph(n):\n    \"\"\"Generates an adjacency matrix and edge list for a ring graph.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        adj[i, (i + 1) % n] = 1\n        adj[(i + 1) % n, i] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef make_er_graph(n, p, rng):\n    \"\"\"Generates an adjacency matrix and edge list for an Erdős–Rényi graph.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        for j in range(i + 1, n):\n            if rng.random() < p:\n                adj[i, j] = 1\n                adj[j, i] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef make_two_rings_graph(n1, n2):\n    \"\"\"Generates two disjoint rings.\"\"\"\n    n = n1 + n2\n    adj = np.zeros((n, n), dtype=int)\n    # First ring\n    for i in range(n1):\n        adj[i, (i + 1) % n1] = 1\n        adj[(i + 1) % n1, i] = 1\n    # Second ring\n    for i in range(n1, n):\n        idx_in_block = i - n1\n        neighbor_in_block = (idx_in_block + 1) % n2\n        adj[i, n1 + neighbor_in_block] = 1\n        adj[n1 + neighbor_in_block, i] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef make_star_graph(n):\n    \"\"\"Generates a star graph with center node 0.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(1, n):\n        adj[0, i] = 1\n        adj[i, 0] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef simulate_voter_sweep(x, n_nodes, edges, rng):\n    \"\"\"Simulates one sweep of the Voter model.\"\"\"\n    x_new = x.copy()\n    if edges.shape[0] == 0:\n        return x_new\n    \n    edge_indices = rng.integers(0, len(edges), size=n_nodes)\n    for edge_idx in edge_indices:\n        i, j = edges[edge_idx]\n        # Randomly choose direction\n        if rng.random() < 0.5:\n            x_new[i] = x_new[j]\n        else:\n            x_new[j] = x_new[i]\n    return x_new\n\ndef simulate_bc_sweep(x, n_nodes, edges, epsilon, mu, rng):\n    \"\"\"Simulates one sweep of the Bounded Confidence model.\"\"\"\n    x_new = x.copy()\n    if edges.shape[0] == 0:\n        return x_new\n        \n    edge_indices = rng.integers(0, len(edges), size=n_nodes)\n    for edge_idx in edge_indices:\n        i, j = edges[edge_idx]\n        if abs(x_new[i] - x_new[j]) <= epsilon:\n            dx = mu * (x_new[j] - x_new[i])\n            x_new[i] += dx\n            x_new[j] -= dx\n    return x_new\n\ndef generate_full_timeseries(x0, n_steps, true_model, params):\n    \"\"\"Generates the ground truth time series.\"\"\"\n    n_nodes = len(x0)\n    timeseries = np.zeros((n_steps + 1, n_nodes))\n    timeseries[0] = x0\n    x_current = x0.copy()\n    \n    for t in range(1, n_steps + 1):\n        if true_model == 'voter':\n            x_current = simulate_voter_sweep(x_current, n_nodes, params['edges'], params['rng'])\n        else: # bc\n            x_current = simulate_bc_sweep(x_current, n_nodes, params['edges'], params['epsilon'], params['mu'], params['rng'])\n        timeseries[t] = x_current\n    return timeseries\n\ndef ensemble_forecast(x_start, model_type, t_forecast, k_ensemble, n_nodes, edges, rng, **model_params):\n    \"\"\"Generates an ensemble forecast.\"\"\"\n    trajectories = np.zeros((k_ensemble, t_forecast, n_nodes))\n    \n    for k in range(k_ensemble):\n        x_current = x_start.copy()\n        for h in range(t_forecast):\n            if model_type == 'voter':\n                x_current = simulate_voter_sweep(x_current, n_nodes, edges, rng)\n            else: # bc\n                x_current = simulate_bc_sweep(x_current, n_nodes, edges, model_params['epsilon'], model_params['mu'], rng)\n            trajectories[k, h, :] = x_current\n            \n    return np.mean(trajectories, axis=0)\n\ndef solve():\n    common_settings = {\n        'N': 30,\n        'T_obs': 20,\n        'T_pred': 5,\n        'K': 50,\n        'bc_param_grid': {'epsilon': [0.15, 0.25, 0.35], 'mu': [0.3, 0.5]},\n        'tau': 1e-9\n    }\n\n    test_cases = [\n        {\n            'name': 'Case 1', 'graph_type': 'ring', 'true_model': 'bc',\n            'model_params': {'epsilon_true': 0.25, 'mu_true': 0.5},\n            'opinion_dist': 'uniform', 'op_seed': 7, 'graph_seed': None\n        },\n        {\n            'name': 'Case 2', 'graph_type': 'er', 'graph_params': {'p': 0.2}, 'true_model': 'voter',\n            'model_params': {}, 'opinion_dist': 'bernoulli', 'op_seed': 5, 'graph_seed': 123\n        },\n        {\n            'name': 'Case 3', 'graph_type': 'two_rings', 'graph_params': {'n1': 15, 'n2': 15}, 'true_model': 'bc',\n            'model_params': {'epsilon_true': 0.20, 'mu_true': 0.5},\n            'opinion_dist': 'uniform', 'op_seed': 33, 'graph_seed': None\n        },\n        {\n            'name': 'Case 4', 'graph_type': 'star', 'true_model': 'voter',\n            'model_params': {}, 'opinion_dist': 'bernoulli', 'op_seed': 21, 'graph_seed': None\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = common_settings['N']\n        T_obs = common_settings['T_obs']\n        T_pred = common_settings['T_pred']\n        K = common_settings['K']\n        \n        # Setup RNGs for reproducibility\n        master_seed = case['op_seed'] if case['graph_seed'] is None else case['graph_seed']\n        rng = np.random.default_rng(master_seed)\n        \n        # --- 1. Data Generation ---\n        if case['graph_type'] == 'ring':\n            adj, edges = make_ring_graph(N)\n        elif case['graph_type'] == 'er':\n            graph_rng = np.random.default_rng(case['graph_seed'])\n            adj, edges = make_er_graph(N, case['graph_params']['p'], graph_rng)\n        elif case['graph_type'] == 'two_rings':\n            adj, edges = make_two_rings_graph(case['graph_params']['n1'], case['graph_params']['n2'])\n        elif case['graph_type'] == 'star':\n            adj, edges = make_star_graph(N)\n\n        op_rng = np.random.default_rng(case['op_seed'])\n        if case['opinion_dist'] == 'uniform':\n            x0 = op_rng.uniform(0, 1, size=N)\n        else:  # bernoulli\n            x0 = op_rng.integers(0, 2, size=N).astype(float)\n        \n        sim_params = {'edges': edges, 'rng': rng}\n        if case['true_model'] == 'bc':\n            sim_params['epsilon'] = case['model_params']['epsilon_true']\n            sim_params['mu'] = case['model_params']['mu_true']\n            \n        full_ts = generate_full_timeseries(x0, T_obs + T_pred, case['true_model'], sim_params)\n        \n        x_obs = full_ts[:T_obs + 1]\n        x_start_pred = x_obs[-1]\n        x_heldout = full_ts[T_obs + 1:]\n\n        # --- 2. Fit Bounded Confidence Model ---\n        best_bc_params = {}\n        min_train_mse = float('inf')\n        bc_grid = common_settings['bc_param_grid']\n        \n        for eps_val in bc_grid['epsilon']:\n            for mu_val in bc_grid['mu']:\n                total_se = 0\n                for t in range(T_obs):\n                    x_t = x_obs[t]\n                    x_t_plus_1_true = x_obs[t + 1]\n                    x_t_plus_1_pred = ensemble_forecast(x_t, 'bc', 1, K, N, edges, rng, epsilon=eps_val, mu=mu_val)[0]\n                    total_se += np.sum((x_t_plus_1_pred - x_t_plus_1_true)**2)\n                \n                train_mse = total_se / (N * T_obs)\n                if train_mse < min_train_mse:\n                    min_train_mse = train_mse\n                    best_bc_params = {'epsilon': eps_val, 'mu': mu_val}\n        \n        # --- 3. Out-of-Sample Evaluation ---\n        # Voter Model\n        voter_preds = ensemble_forecast(x_start_pred, 'voter', T_pred, K, N, edges, rng)\n        mse_voter = np.mean((voter_preds - x_heldout)**2)\n        \n        # Bounded Confidence Model\n        bc_preds = ensemble_forecast(x_start_pred, 'bc', T_pred, K, N, edges, rng, **best_bc_params)\n        mse_bc = np.mean((bc_preds - x_heldout)**2)\n        \n        # --- 4. Model Selection ---\n        if mse_bc < mse_voter - common_settings['tau']:\n            results.append(1) # Bounded Confidence\n        else:\n            results.append(0) # Voter\n            \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}