{
    "hands_on_practices": [
        {
            "introduction": "The voter model is a fundamental building block for understanding consensus formation. While its tendency to lead to a uniform opinion state is intuitive, proving its properties rigorously requires powerful mathematical tools. This exercise guides you through a classic derivation using martingales, a cornerstone of stochastic process theory, to determine the exact probability that the system reaches a specific consensus state, reinforcing your analytical skills for stochastic modeling .",
            "id": "4129388",
            "problem": "Consider a finite, connected, undirected, simple graph $G$ on $N$ vertices that is $k$-regular for some integer $k \\geq 1$. Each vertex $i \\in \\{1,\\dots,N\\}$ holds an opinion $X_i(t) \\in \\{0,1\\}$ at discrete time $t \\in \\mathbb{N}$. The system evolves according to the standard voter model: at each time step $t \\mapsto t+1$, a single vertex $I_t$ is chosen uniformly at random from $\\{1,\\dots,N\\}$, and then a neighbor $J_t \\in \\mathcal{N}(I_t)$ is chosen uniformly at random among the $k$ neighbors of $I_t$. The vertex $I_t$ updates its opinion to match its chosen neighbor, so that $X_{I_t}(t+1) = X_{J_t}(t)$, while all other vertices keep their opinions, i.e., $X_i(t+1) = X_i(t)$ for all $i \\neq I_t$. Let the initial configuration be arbitrary and define the initial fraction of opinion $1$ as $p_0 := \\frac{1}{N}\\sum_{i=1}^{N} X_i(0)$.\n\nDefine the empirical fraction of opinion $1$ at time $t$ by $M(t) := \\frac{1}{N}\\sum_{i=1}^{N} X_i(t)$, and let $T$ be the first time the system reaches consensus, that is, $T := \\inf\\{t \\geq 0 : X_i(t) = X_j(t) \\text{ for all } i,j\\}$. The absorbing states are the all-$0$ configuration and the all-$1$ configuration.\n\nStarting from the fundamental definition of the voter model dynamics and using only first principles about conditional expectations and martingales, derive an expression for the probability that the system eventually reaches the all-$1$ consensus, in terms of $p_0$. Your derivation must be based on showing that $M(t)$ is a martingale with respect to the natural filtration and applying the Optional Stopping Theorem (OST) for martingales. Express your final answer as a single closed-form analytic expression. No numerical approximation or rounding is required, and no physical units apply.",
            "solution": "We begin by formalizing the dynamics and the quantity of interest. The voter model specified is a time-homogeneous Markov chain on the finite state space $\\{0,1\\}^{N}$. Let $\\mathcal{F}_t$ denote the natural filtration generated by the process up to time $t$, i.e., $\\mathcal{F}_t = \\sigma\\big((X_i(s))_{i=1}^{N}, 0 \\leq s \\leq t\\big)$. The quantity of interest is the probability that the system reaches the all-$1$ absorbing state. Let $A$ denote the event that consensus at time $T$ is all-$1$, i.e., $A := \\{X_i(T) = 1 \\text{ for all } i\\}$.\n\nWe define the empirical fraction of opinion $1$ at time $t$ as\n$$\nM(t) := \\frac{1}{N}\\sum_{i=1}^{N} X_i(t).\n$$\nWe will show that $\\{M(t)\\}_{t \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_t\\}_{t \\geq 0}$, and then apply the Optional Stopping Theorem (OST) for martingales to the stopping time $T$.\n\nStep $1$: Computation of the one-step conditional expectation of $M(t)$.\n\nBy construction, between time $t$ and $t+1$, only one coordinate may change, namely $X_{I_t}$. The update rule is $X_{I_t}(t+1) = X_{J_t}(t)$, with $I_t$ uniform over $\\{1,\\dots,N\\}$ and $J_t$ uniform over $\\mathcal{N}(I_t)$, independent of past given $\\mathcal{F}_t$. Therefore,\n$$\nM(t+1) - M(t) = \\frac{1}{N}\\big(X_{I_t}(t+1) - X_{I_t}(t)\\big) = \\frac{1}{N}\\big(X_{J_t}(t) - X_{I_t}(t)\\big).\n$$\nTaking conditional expectation given $\\mathcal{F}_t$ and using the independence and uniform choices,\n$$\n\\mathbb{E}\\big[M(t+1) - M(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\left(\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big] - \\mathbb{E}\\big[X_{I_t}(t) \\mid \\mathcal{F}_t\\big]\\right).\n$$\nWe compute the two expectations separately. Since $I_t$ is uniform on $\\{1,\\dots,N\\}$,\n$$\n\\mathbb{E}\\big[X_{I_t}(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\sum_{i=1}^{N} X_i(t) = M(t).\n$$\nFor $\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big]$, note that conditional on $I_t=i$, $J_t$ is uniform over $\\mathcal{N}(i)$, so\n$$\n\\mathbb{E}\\big[X_{J_t}(t) \\mid I_t=i, \\mathcal{F}_t\\big] = \\frac{1}{k}\\sum_{j \\in \\mathcal{N}(i)} X_j(t).\n$$\nAveraging over $I_t$,\n$$\n\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{k}\\sum_{j \\in \\mathcal{N}(i)} X_j(t).\n$$\nBecause the graph is $k$-regular and undirected, the sum $\\sum_{i=1}^{N}\\sum_{j \\in \\mathcal{N}(i)} X_j(t)$ counts each vertex $j$ exactly $\\deg(j)=k$ times across directed incidences $(i,j)$, hence\n$$\n\\sum_{i=1}^{N} \\sum_{j \\in \\mathcal{N}(i)} X_j(t) = \\sum_{j=1}^{N} X_j(t)\\deg(j) = k \\sum_{j=1}^{N} X_j(t).\n$$\nTherefore,\n$$\n\\mathbb{E}\\big[X_{J_t}(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\cdot \\frac{1}{k}\\cdot k \\sum_{j=1}^{N} X_j(t) = \\frac{1}{N}\\sum_{j=1}^{N} X_j(t) = M(t).\n$$\nCombining the two,\n$$\n\\mathbb{E}\\big[M(t+1) - M(t) \\mid \\mathcal{F}_t\\big] = \\frac{1}{N}\\big(M(t) - M(t)\\big) = 0,\n$$\nwhich implies\n$$\n\\mathbb{E}\\big[M(t+1) \\mid \\mathcal{F}_t\\big] = M(t).\n$$\nThus $\\{M(t)\\}_{t \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_t\\}_{t \\geq 0}$.\n\nStep $2$: Boundedness and applicability of the Optional Stopping Theorem (OST).\n\nBy definition, $M(t) \\in [0,1]$ for all $t$, so the martingale is bounded. On a finite, connected graph, the voter model almost surely reaches consensus in finite time due to its duality with coalescing random walks, which coalesce to a single particle almost surely; hence $T  \\infty$ almost surely. A bounded martingale is uniformly integrable, so the Optional Stopping Theorem (OST) applies to the stopping time $T$ and yields\n$$\n\\mathbb{E}\\big[M(T)\\big] = \\mathbb{E}\\big[M(0)\\big] = M(0) = p_0.\n$$\n\nStep $3$: Identification of $\\mathbb{E}\\big[M(T)\\big]$ with the probability of all-$1$ consensus.\n\nAt the stopping time $T$, the configuration is either all-$0$ or all-$1$. Therefore,\n$$\nM(T) = \\begin{cases}\n0  \\text{if consensus is all-$0$},\\\\\n1  \\text{if consensus is all-$1$}.\n\\end{cases}\n$$\nHence $M(T)$ is exactly the indicator of event $A$, i.e., $M(T) = \\mathbf{1}_{A}$. It follows that\n$$\n\\mathbb{E}\\big[M(T)\\big] = \\mathbb{E}\\big[\\mathbf{1}_{A}\\big] = \\mathbb{P}(A).\n$$\nCombining with the result from OST,\n$$\n\\mathbb{P}(\\text{consensus is all-$1$}) = \\mathbb{P}(A) = p_0.\n$$\n\nTherefore, the probability that the voter model on a finite, connected, undirected, $k$-regular graph reaches the absorbing state with all opinions equal to $1$ is equal to the initial fraction of opinion $1$, $p_0$.",
            "answer": "$$\\boxed{p_0}$$"
        },
        {
            "introduction": "Bounded confidence models capture the realistic notion that influence is limited to those with similar opinions, but how should \"similarity\" be defined, especially in a multidimensional opinion space? This practice explores the profound geometric consequences of this modeling choice by having you calculate how the boundary between opinion clusters is shaped by different distance metrics. It provides a tangible demonstration of how an abstract mathematical assumption—the choice of a norm—directly impacts the large-scale patterns of social segregation that the model can produce .",
            "id": "4129383",
            "problem": "Consider a two-dimensional continuous opinion space modeled by a bounded confidence interaction rule: an agent at position $\\mathbf{x} \\in \\mathbb{R}^{2}$ averages its opinion with agents inside a confidence set $\\mathcal{B}_{\\varepsilon}(\\mathbf{x}) = \\{ \\mathbf{y} \\in \\mathbb{R}^{2} : \\| \\mathbf{y} - \\mathbf{x} \\| \\leq \\varepsilon \\}$, where $\\| \\cdot \\|$ is a norm and $\\varepsilon  0$ is a confidence threshold. Suppose the population has self-organized into two dense clusters centered at $\\mathbf{c}_{1} = (0,0)$ and $\\mathbf{c}_{2} = (a,b)$, with $a  0$ and $b  0$, and equal total mass. Assume the interaction kernel is isotropic and strictly decreasing in the norm-induced distance, so that the local drift of $\\mathbf{x}$ toward cluster $\\mathbf{c}_{i}$ is proportional to a strictly decreasing function $g(\\| \\mathbf{x} - \\mathbf{c}_{i} \\|)$ with $g'(\\cdot)  0$, equal for both clusters. In the mean-field limit where only the two clusters dominate the local drift, the instantaneous basin boundary is the set of points where the influence from both clusters is equal.\n\nDefine the basin boundary for a given norm as the zero level set of the function $f(\\mathbf{x}) = \\| \\mathbf{x} - \\mathbf{c}_{1} \\| - \\| \\mathbf{x} - \\mathbf{c}_{2} \\|$. The local orientation of this boundary at a point is given by the normal vector proportional to $\\nabla f(\\mathbf{x})$ where $\\nabla f$ exists. Consider the midpoint $\\mathbf{x}^{\\star} = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)$, and compare the boundary normals when $\\| \\cdot \\|$ is the Euclidean $\\ell_{2}$ norm versus the Manhattan $\\ell_{1}$ norm.\n\nStarting only from the core definitions of the $\\ell_{p}$ norms, the bounded confidence mechanism, and the equal-influence boundary condition, derive the angle $\\Delta \\theta$ in radians between the $\\ell_{1}$ and $\\ell_{2}$ boundary normals at $\\mathbf{x}^{\\star}$ as a closed-form analytic expression in terms of $a$ and $b$. Express your final answer as a single analytic expression. State your answer in radians. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the angle $\\Delta \\theta$ between the normal vectors to the basin boundary at a specific point $\\mathbf{x}^{\\star}$ for two different norms, the Euclidean $\\ell_2$ norm and the Manhattan $\\ell_1$ norm. The solution proceeds by first validating the problem statement and then deriving the normal vectors in each case to compute the angle between them.\n\nThe problem statement is deemed valid. It is scientifically grounded in the theory of opinion dynamics, specifically bounded confidence models. It is mathematically well-posed, providing clear and consistent definitions for the boundary, the point of interest, and the norms to be used. The constraints $a  0$ and $b  0$ ensure the problem is non-degenerate. The task is a direct application of vector calculus to find gradients of norm functions.\n\nLet the position of an agent be $\\mathbf{x} = (x, y) \\in \\mathbb{R}^{2}$. The two cluster centers are given as $\\mathbf{c}_{1} = (0, 0)$ and $\\mathbf{c}_{2} = (a, b)$. The point of interest is the midpoint $\\mathbf{x}^{\\star} = \\left( \\frac{a}{2}, \\frac{b}{2} \\right)$.\n\nThe basin boundary is defined as the zero level set of the function $f(\\mathbf{x}) = \\| \\mathbf{x} - \\mathbf{c}_{1} \\| - \\| \\mathbf{x} - \\mathbf{c}_{2} \\|$. The normal vector to this boundary at a point $\\mathbf{x}$ is proportional to the gradient of $f(\\mathbf{x})$, provided the gradient exists.\nThe gradient is given by:\n$$ \\nabla f(\\mathbf{x}) = \\nabla \\left( \\| \\mathbf{x} - \\mathbf{c}_{1} \\| - \\| \\mathbf{x} - \\mathbf{c}_{2} \\| \\right) = \\nabla \\| \\mathbf{x} - \\mathbf{c}_{1} \\| - \\nabla \\| \\mathbf{x} - \\mathbf{c}_{2} \\| $$\nWe need to evaluate this gradient at $\\mathbf{x} = \\mathbf{x}^{\\star}$. Let's define the vectors from the cluster centers to $\\mathbf{x}^{\\star}$:\n$$ \\mathbf{v}_{1} = \\mathbf{x}^{\\star} - \\mathbf{c}_{1} = \\left( \\frac{a}{2}, \\frac{b}{2} \\right) - (0, 0) = \\left( \\frac{a}{2}, \\frac{b}{2} \\right) $$\n$$ \\mathbf{v}_{2} = \\mathbf{x}^{\\star} - \\mathbf{c}_{2} = \\left( \\frac{a}{2}, \\frac{b}{2} \\right) - (a, b) = \\left( -\\frac{a}{2}, -\\frac{b}{2} \\right) $$\n\nLet's compute the normal vectors for the $\\ell_2$ and $\\ell_1$ norms separately.\n\n**Case 1: Euclidean $\\ell_2$ norm**\nThe $\\ell_2$ norm of a vector $\\mathbf{u} = (u_x, u_y)$ is $\\| \\mathbf{u} \\|_2 = \\sqrt{u_x^2 + u_y^2}$. The gradient of the $\\ell_2$ norm with respect to $\\mathbf{u}$ is:\n$$ \\nabla_{\\mathbf{u}} \\| \\mathbf{u} \\|_2 = \\frac{\\mathbf{u}}{\\| \\mathbf{u} \\|_2} $$ for $\\mathbf{u} \\neq \\mathbf{0}$.\nApplying the chain rule, $\\nabla_{\\mathbf{x}} \\| \\mathbf{x} - \\mathbf{c} \\|_2 = \\frac{\\mathbf{x} - \\mathbf{c}}{\\| \\mathbf{x} - \\mathbf{c} \\|_2}$.\n\nThe normal vector to the $\\ell_2$-boundary, which we denote $\\mathbf{n}_2$, is proportional to $\\nabla f_2(\\mathbf{x}^{\\star})$:\n$$ \\mathbf{n}_2 \\propto \\frac{\\mathbf{x}^{\\star} - \\mathbf{c}_{1}}{\\| \\mathbf{x}^{\\star} - \\mathbf{c}_{1} \\|_2} - \\frac{\\mathbf{x}^{\\star} - \\mathbf{c}_{2}}{\\| \\mathbf{x}^{\\star} - \\mathbf{c}_{2} \\|_2} $$\nSubstituting $\\mathbf{v}_1$ and $\\mathbf{v}_2$:\n$$ \\| \\mathbf{v}_{1} \\|_2 = \\sqrt{\\left(\\frac{a}{2}\\right)^2 + \\left(\\frac{b}{2}\\right)^2} = \\frac{1}{2}\\sqrt{a^2 + b^2} $$\n$$ \\| \\mathbf{v}_{2} \\|_2 = \\sqrt{\\left(-\\frac{a}{2}\\right)^2 + \\left(-\\frac{b}{2}\\right)^2} = \\frac{1}{2}\\sqrt{a^2 + b^2} $$\nAs expected, the distances are equal, so $\\mathbf{x}^{\\star}$ is on the boundary.\n$$ \\mathbf{n}_2 \\propto \\frac{\\left( \\frac{a}{2}, \\frac{b}{2} \\right)}{\\frac{1}{2}\\sqrt{a^2 + b^2}} - \\frac{\\left( -\\frac{a}{2}, -\\frac{b}{2} \\right)}{\\frac{1}{2}\\sqrt{a^2 + b^2}} = \\frac{(a, b) - (-a, -b)}{\\sqrt{a^2 + b^2}} = \\frac{2(a, b)}{\\sqrt{a^2 + b^2}} $$\nThe normal vector $\\mathbf{n}_2$ is therefore proportional to the vector $(a, b)$, which connects the two cluster centers. We can choose $\\mathbf{n}_2 = (a, b)$ for simplicity.\n\n**Case 2: Manhattan $\\ell_1$ norm**\nThe $\\ell_1$ norm of a vector $\\mathbf{u} = (u_x, u_y)$ is $\\| \\mathbf{u} \\|_1 = |u_x| + |u_y|$. The gradient of the $\\ell_1$ norm with respect to $\\mathbf{u}$ is:\n$$ \\nabla_{\\mathbf{u}} \\| \\mathbf{u} \\|_1 = (\\text{sgn}(u_x), \\text{sgn}(u_y)) $$\nwhere $\\text{sgn}(\\cdot)$ is the sign function. This gradient is defined as long as neither component of $\\mathbf{u}$ is zero.\nGiven $a  0$ and $b  0$, the components of $\\mathbf{v}_1 = (\\frac{a}{2}, \\frac{b}{2})$ are both positive, and the components of $\\mathbf{v}_2 = (-\\frac{a}{2}, -\\frac{b}{2})$ are both negative. Thus, the gradient is well-defined at our point of evaluation.\n\nThe normal vector to the $\\ell_1$-boundary, $\\mathbf{n}_1$, is proportional to $\\nabla f_1(\\mathbf{x}^{\\star})$:\n$$ \\mathbf{n}_1 \\propto \\left. \\nabla_{\\mathbf{x}} \\| \\mathbf{x} - \\mathbf{c}_{1} \\|_1 \\right|_{\\mathbf{x}=\\mathbf{x}^{\\star}} - \\left. \\nabla_{\\mathbf{x}} \\| \\mathbf{x} - \\mathbf{c}_{2} \\|_1 \\right|_{\\mathbf{x}=\\mathbf{x}^{\\star}} $$\n$$ \\mathbf{n}_1 \\propto (\\text{sgn}(a/2), \\text{sgn}(b/2)) - (\\text{sgn}(-a/2), \\text{sgn}(-b/2)) $$\n$$ \\mathbf{n}_1 \\propto (1, 1) - (-1, -1) = (2, 2) $$\nThe normal vector $\\mathbf{n}_1$ is proportional to $(1, 1)$. We can choose $\\mathbf{n}_1 = (1, 1)$.\n\n**Angle Calculation**\nWe now find the angle $\\Delta \\theta$ between the two normal vectors $\\mathbf{n}_1 = (1, 1)$ and $\\mathbf{n}_2 = (a, b)$. The angle between two vectors is found using the dot product formula, which implicitly uses the Euclidean $\\ell_2$ norm.\n$$ \\cos(\\Delta \\theta) = \\frac{\\mathbf{n}_1 \\cdot \\mathbf{n}_2}{\\| \\mathbf{n}_1 \\|_2 \\| \\mathbf{n}_2 \\|_2} $$\nThe dot product is:\n$$ \\mathbf{n}_1 \\cdot \\mathbf{n}_2 = (1)(a) + (1)(b) = a + b $$\nThe magnitudes (Euclidean norms) of the normal vectors are:\n$$ \\| \\mathbf{n}_1 \\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2} $$\n$$ \\| \\mathbf{n}_2 \\|_2 = \\sqrt{a^2 + b^2} $$\nSubstituting these into the cosine formula:\n$$ \\cos(\\Delta \\theta) = \\frac{a+b}{\\sqrt{2}\\sqrt{a^2+b^2}} = \\frac{a+b}{\\sqrt{2(a^2+b^2)}} $$\nThe angle $\\Delta \\theta$ is the arccosine of this expression. The problem asks for the angle in radians, which is the standard output of the $\\arccos$ function.\n$$ \\Delta \\theta = \\arccos\\left( \\frac{a+b}{\\sqrt{2(a^2+b^2)}} \\right) $$\nThis is the final closed-form analytic expression for the angle in terms of $a$ and $b$.",
            "answer": "$$ \\boxed{\\arccos\\left( \\frac{a+b}{\\sqrt{2(a^2+b^2)}} \\right)} $$"
        },
        {
            "introduction": "A key task for any modeler is not just to analyze a model, but to determine which model best explains observed data. This computational exercise provides a complete framework for model validation, challenging you to discriminate between the Voter and Bounded Confidence models based on their predictive power. You will implement the models, perform parameter fitting via grid search, and use out-of-sample forecasting to make a principled, data-driven decision—a core skill set for applying complex systems modeling to real-world problems .",
            "id": "4129409",
            "problem": "You are given a set of canonical definitions for two opinion dynamics models on a fixed undirected network and a method to generate synthetic observations. Your task is to implement a validation framework that integrates structural network data and observed opinion time series to discriminate between the voter model and the bounded confidence model by comparing their out-of-sample predictive performance. The output must be a single line containing the predicted model indices for a provided test suite.\n\nFundamental base. Consider a static, simple, undirected graph $G = (V, E)$ with $|V| = N$ nodes and adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$, where $A_{ij} = 1$ if $(i,j) \\in E$ and $A_{ij} = 0$ otherwise. Let $x(t) = (x_1(t), \\dots, x_N(t))$ denote the vector of opinions at discrete time $t$, with $t \\in \\{0, 1, 2, \\dots\\}$. One time step is defined as a sweep of $N$ asynchronous pairwise updates, where in each update an edge $(i,j) \\in E$ is sampled uniformly at random. Opinions evolve according to one of the following models:\n\n- Voter model. Opinions are binary with $x_i(t) \\in \\{0,1\\}$. At each asynchronous update, select a random edge $(i,j)$ uniformly and a random orientation; then the selected node copies the opinion of its neighbor:\n$$\nx_i(t+1) \\leftarrow x_j(t) \\quad \\text{or} \\quad x_j(t+1) \\leftarrow x_i(t).\n$$\nA sweep applies $N$ such updates.\n\n- Bounded confidence model (Deffuant-Weisbuch). Opinions are continuous with $x_i(t) \\in [0,1]$. At each asynchronous update, select a random edge $(i,j)$ uniformly. If the opinion difference is within a confidence bound $\\epsilon  0$, both endpoints move toward each other by a convergence parameter $\\mu \\in (0, \\tfrac{1}{2}]$, using the symmetric update:\n$$\n\\text{if } |x_i - x_j| \\le \\epsilon: \\quad \\begin{cases}\nx_i \\leftarrow x_i + \\mu (x_j - x_i), \\\\\nx_j \\leftarrow x_j + \\mu (x_i - x_j),\n\\end{cases}\n\\quad \\text{else: no change.}\n$$\nA sweep applies $N$ such updates.\n\nObservation and prediction protocol. Let $T_{\\mathrm{obs}} \\in \\mathbb{N}$ denote the number of observed time steps and $T_{\\mathrm{pred}} \\in \\mathbb{N}$ the length of the held-out horizon. You are given both the network $A$ and the full time series $x(0), x(1), \\dots, x(T_{\\mathrm{obs}} + T_{\\mathrm{pred}})$, but you must fit models using only the first $T_{\\mathrm{obs}}$ steps and evaluate predictive performance on the held-out horizon of length $T_{\\mathrm{pred}}$. Use Monte Carlo ensemble forecasting with $K \\in \\mathbb{N}$ independent runs per forecast, starting from the last observed state $x(T_{\\mathrm{obs}})$ and simulating forward. For a forecast horizon $h \\in \\{1,\\dots,T_{\\mathrm{pred}}\\}$, the ensemble mean prediction $\\hat{x}(T_{\\mathrm{obs}}+h)$ is the average state across the $K$ simulated trajectories at step $h$.\n\nModel selection rule. Compute the out-of-sample mean squared error (Mean Squared Error (MSE)) for each candidate model by averaging the squared deviations between ensemble mean predictions and the held-out states across all nodes and forecast steps:\n$$\n\\mathrm{MSE}(M) = \\frac{1}{N \\, T_{\\mathrm{pred}}} \\sum_{h=1}^{T_{\\mathrm{pred}}} \\sum_{i=1}^{N} \\left( \\hat{x}_i(T_{\\mathrm{obs}}+h; M) - x_i(T_{\\mathrm{obs}}+h) \\right)^2.\n$$\nFor the bounded confidence model, select $(\\epsilon, \\mu)$ via a grid search that minimizes a training criterion computed only on the observed segment, specifically the average one-step-ahead MSE over $t \\in \\{0,\\dots,T_{\\mathrm{obs}}-1\\}$ using ensemble mean predictions from $x(t)$ to $x(t+1)$. For the voter model, there are no tunable parameters under the stated assumptions. After fitting, choose the model with the smaller out-of-sample $\\mathrm{MSE}$. In case of exact equality within numerical tolerance (take tolerance $\\tau = 10^{-9}$), select the voter model.\n\nTest suite and required outputs. Implement the above framework and apply it to the following four synthetic test cases. For reproducibility, fix all random seeds as specified. For each case, generate the network $A$, the initial opinions $x(0)$, and the full time series $x(t)$ for $t=1,\\dots,T_{\\mathrm{obs}}+T_{\\mathrm{pred}}$ under the stated true model and parameters. Use a time step definition of one sweep of $N$ asynchronous pairwise updates.\n\nCommon settings for all cases:\n- Number of nodes: $N = 30$.\n- Observed steps: $T_{\\mathrm{obs}} = 20$.\n- Held-out steps: $T_{\\mathrm{pred}} = 5$.\n- Monte Carlo ensemble size: $K = 50$.\n- Bounded confidence parameter grid for fitting: $\\epsilon \\in \\{0.15, 0.25, 0.35\\}$, $\\mu \\in \\{0.3, 0.5\\}$.\n- Tie-breaking tolerance for equal out-of-sample $\\mathrm{MSE}$: $\\tau = 10^{-9}$.\n\nCase definitions:\n1. Case $1$ (bounded confidence, ring graph):\n   - Network: a ring (cycle) of $N$ nodes, edges $(i,(i+1) \\bmod N)$.\n   - True model: bounded confidence with $\\epsilon_{\\mathrm{true}} = 0.25$, $\\mu_{\\mathrm{true}} = 0.5$.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Uniform}(0,1)$, use random seed $\\mathrm{seed} = 7$.\n\n2. Case $2$ (voter, Erdős–Rényi graph):\n   - Network: Erdős–Rényi $G(N,p)$ with $p = 0.2$, use graph seed $\\mathrm{seed} = 123$.\n   - True model: voter.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Bernoulli}(0.5)$, use random seed $\\mathrm{seed} = 5$.\n\n3. Case $3$ (bounded confidence, disconnected graph with two communities):\n   - Network: two disjoint rings of sizes $15$ and $15$ (block-diagonal adjacency with ring edges in each block).\n   - True model: bounded confidence with $\\epsilon_{\\mathrm{true}} = 0.20$, $\\mu_{\\mathrm{true}} = 0.5$.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Uniform}(0,1)$, use random seed $\\mathrm{seed} = 33$.\n\n4. Case $4$ (voter, star graph):\n   - Network: star graph on $N$ nodes with center node $0$ connected to all others.\n   - True model: voter.\n   - Initial opinions: $x_i(0) \\sim \\mathrm{Bernoulli}(0.5)$, use random seed $\\mathrm{seed} = 21$.\n\nProgram requirements:\n- Implement functions to simulate one time step (one sweep) for each model as defined above.\n- Implement ensemble forecasting and the training grid search for the bounded confidence parameters $(\\epsilon,\\mu)$ using only $x(0),\\dots,x(T_{\\mathrm{obs}})$.\n- Compute out-of-sample $\\mathrm{MSE}$ on $x(T_{\\mathrm{obs}}+1),\\dots,x(T_{\\mathrm{obs}}+T_{\\mathrm{pred}})$ for both models (using the fitted $(\\epsilon,\\mu)$ for bounded confidence) and select the model with the lower $\\mathrm{MSE}$, or the voter model if equal within tolerance $\\tau$.\n- For each test case, output the predicted model index as an integer: output $0$ for the voter model and $1$ for the bounded confidence model.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0,1,1,0]$). No additional text should be printed.\n\nNo physical units, angle units, or percentages are involved in this problem. All quantities are dimensionless real numbers or integers.",
            "solution": "The user-provided problem is a well-defined computational task in the field of complex adaptive systems modeling, specifically concerning model selection for opinion dynamics on networks. The problem is scientifically grounded, internally consistent, and possesses all necessary information for a unique, verifiable solution. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe core task is to discriminate between the Voter Model and the Bounded Confidence (BC) Model given a network structure and a time series of opinions. The methodology follows a standard empirical validation framework:\n1.  **Data Generation**: For each test case, generate a synthetic dataset consisting of a network and a full opinion time series, `x(0), ..., x(T_{obs} + T_{pred})`, according to a specified \"true\" model.\n2.  **Model Fitting**: The available data is partitioned into a training set, `x(0), ..., x(T_{obs})`, and a test set, `x(T_{obs}+1), ..., x(T_{obs}+T_{pred})`. The Voter model has no free parameters. The BC model's parameters, $(\\epsilon, \\mu)$, are fitted by minimizing a one-step-ahead prediction error on the training set via a grid search.\n3.  **Out-of-Sample Prediction**: Both the Voter model and the fitted BC model are used to generate forecasts for the test set horizon, `h = 1, ..., T_{pred}`. These forecasts start from the last observed state, `x(T_{obs})`, and are produced by averaging an ensemble of $K$ independent stochastic simulations.\n4.  **Model Selection**: The predictive accuracy of each model is quantified by the Mean Squared Error (MSE) on the test set. The model with the lower out-of-sample MSE is selected. A specific tie-breaking rule favors the Voter model.\n\nWe will systematically implement each component of this framework.\n\n### 1. Network and Data Generation\n\nFor each test case, we first construct the specified network topology as an adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$. From $A$, we derive an edge list $E$, which is more efficient for the simulation step requiring uniform random sampling of edges.\n\nThe initial opinion vector $x(0)$ is generated by drawing from either a Uniform distribution for the BC model or a Bernoulli distribution for the Voter model, using the specified random seeds for reproducibility.\n\nThe complete time series $x(1), \\dots, x(T_{\\mathrm{obs}}+T_{\\mathrm{pred}})$ is then generated by iteratively applying the update rule of the true model for $T_{\\mathrm{obs}}+T_{\\mathrm{pred}}$ time steps, starting from $x(0)$. A single time step involves a sweep of $N$ asynchronous updates as defined in the problem.\n\n### 2. Simulation of Opinion Dynamics\n\nThe evolution of opinions is governed by stochastic, pairwise interactions on the network. A single time step for both models consists of $N$ micro-updates. In each micro-update, an edge $(i,j)$ is chosen uniformly at random from the set of all edges $E$. The opinion state vector is updated sequentially within the sweep.\n\n**Voter Model Simulation Step**:\nGiven a state $x(t)$, the state $x(t+1)$ is obtained by performing $N$ updates on a copy of $x(t)$. For each update:\n1.  Sample an edge $(i,j) \\in E$ uniformly at random.\n2.  Sample a direction, i.e., choose one node to be the copier and the other the source. This is equivalent to choosing $u, v \\in \\{i,j\\}$ with $u \\neq v$.\n3.  Update the opinion: $x_u \\leftarrow x_v$.\n\n**Bounded Confidence Model Simulation Step**:\nGiven a state $x(t)$, parameters $(\\epsilon, \\mu)$, the state $x(t+1)$ is obtained by performing $N$ updates. For each update:\n1.  Sample an edge $(i,j) \\in E$ uniformly at random.\n2.  Check the confidence condition: if $|x_i - x_j| \\le \\epsilon$.\n3.  If the condition is met, perform the symmetric update. To avoid race conditions, the change is calculated based on the state before the update:\n    $$ \\Delta x = \\mu (x_j - x_i) $$\n    The new opinions are then:\n    $$ x_i \\leftarrow x_i + \\Delta x $$\n    $$ x_j \\leftarrow x_j - \\Delta x $$\n    If the condition is not met, no change occurs.\n\n### 3. Forecasting and Parameter Fitting\n\n**Ensemble Forecasting**:\nTo predict the system's evolution, we use Monte Carlo simulation. Starting from a given state $x_{\\mathrm{start}}$, we run $K$ independent simulations for a specified number of time steps, $T_{\\mathrm{forecast}}$. Each of the $K$ simulations produces a different trajectory due to the stochastic nature of the update rules. The ensemble mean prediction at a future step $h$ is the average of the opinion vectors across all $K$ trajectories at that step:\n$$ \\hat{x}(x_{\\mathrm{start}}, h) = \\frac{1}{K} \\sum_{k=1}^{K} x^{(k)}(h) $$\nwhere $x^{(k)}(h)$ is the state of the $k$-th simulation run at step $h$.\n\n**Fitting the Bounded Confidence Model**:\nThe BC model has two parameters, the confidence bound $\\epsilon$ and the convergence parameter $\\mu$. These are selected from a predefined grid, $\\{\\epsilon_1, \\dots, \\epsilon_m\\} \\times \\{\\mu_1, \\dots, \\mu_n\\}$. The optimal pair $(\\epsilon^*, \\mu^*)$ is the one that minimizes the in-sample, one-step-ahead prediction error. This training criterion is calculated over the observed time series $x(0), \\dots, x(T_{\\mathrm{obs}})$:\n$$ (\\epsilon^*, \\mu^*) = \\underset{(\\epsilon, \\mu)}{\\arg\\min} \\frac{1}{N T_{\\mathrm{obs}}} \\sum_{t=0}^{T_{\\mathrm{obs}}-1} \\sum_{i=1}^{N} \\left( \\hat{x}_i(x(t), 1; \\epsilon, \\mu) - x_i(t+1) \\right)^2 $$\nHere, $\\hat{x}(x(t), 1; \\epsilon, \\mu)$ is the one-step-ahead ensemble mean prediction starting from the true observed state $x(t)$, using parameters $(\\epsilon, \\mu)$.\n\n### 4. Model Selection\n\nAfter determining the optimal parameters $(\\epsilon^*, \\mu^*)$ for the BC model, we evaluate both candidate models on the held-out test data, which comprises the time steps $T_{\\mathrm{obs}}+1, \\dots, T_{\\mathrm{obs}}+T_{\\mathrm{pred}}$.\n\nFor each model $M \\in \\{\\text{Voter}, \\text{BC}(\\epsilon^*, \\mu^*)\\}$, we first generate the out-of-sample ensemble mean predictions, $\\hat{x}(T_{\\mathrm{obs}}+h; M)$, for the entire forecast horizon $h=1, \\dots, T_{\\mathrm{pred}}$. All forecasts start from the last observed state, $x(T_{\\mathrm{obs}})$.\n\nThe out-of-sample Mean Squared Error (MSE) is then computed for each model:\n$$ \\mathrm{MSE}(M) = \\frac{1}{N T_{\\mathrm{pred}}} \\sum_{h=1}^{T_{\\mathrm{pred}}} \\sum_{i=1}^{N} \\left( \\hat{x}_i(T_{\\mathrm{obs}}+h; M) - x_i(T_{\\mathrm{obs}}+h) \\right)^2 $$\nwhere $x_i(T_{\\mathrm{obs}}+h)$ are the \"ground truth\" values from the pre-generated time series.\n\nThe final decision rule is:\n1.  Calculate $\\mathrm{MSE}_{\\mathrm{Voter}}$ and $\\mathrm{MSE}_{\\mathrm{BC}}$.\n2.  If $\\mathrm{MSE}_{\\mathrm{BC}}  \\mathrm{MSE}_{\\mathrm{Voter}} - \\tau$, where $\\tau = 10^{-9}$ is the numerical tolerance, select the Bounded Confidence model (index $1$).\n3.  Otherwise, select the Voter model (index $0$). This includes the cases where $\\mathrm{MSE}_{\\mathrm{Voter}} \\le \\mathrm{MSE}_{\\mathrm{BC}}$ or when they are virtually equal.\n\nThis procedure is applied independently to each of the four test cases defined in the problem statement. The implementation will use `numpy` for efficient numerical computation and will carefully manage the random number generators with the specified seeds to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef make_ring_graph(n):\n    \"\"\"Generates an adjacency matrix and edge list for a ring graph.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        adj[i, (i + 1) % n] = 1\n        adj[(i + 1) % n, i] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef make_er_graph(n, p, rng):\n    \"\"\"Generates an adjacency matrix and edge list for an Erdős–Rényi graph.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        for j in range(i + 1, n):\n            if rng.random()  p:\n                adj[i, j] = 1\n                adj[j, i] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef make_two_rings_graph(n1, n2):\n    \"\"\"Generates two disjoint rings.\"\"\"\n    n = n1 + n2\n    adj = np.zeros((n, n), dtype=int)\n    # First ring\n    for i in range(n1):\n        adj[i, (i + 1) % n1] = 1\n        adj[(i + 1) % n1, i] = 1\n    # Second ring\n    for i in range(n1, n):\n        idx_in_block = i - n1\n        neighbor_in_block = (idx_in_block + 1) % n2\n        adj[i, n1 + neighbor_in_block] = 1\n        adj[n1 + neighbor_in_block, i] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef make_star_graph(n):\n    \"\"\"Generates a star graph with center node 0.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(1, n):\n        adj[0, i] = 1\n        adj[i, 0] = 1\n    edges = np.argwhere(np.triu(adj, k=1))\n    return adj, edges\n\ndef simulate_voter_sweep(x, n_nodes, edges, rng):\n    \"\"\"Simulates one sweep of the Voter model.\"\"\"\n    x_new = x.copy()\n    if edges.shape[0] == 0:\n        return x_new\n    \n    edge_indices = rng.integers(0, len(edges), size=n_nodes)\n    for edge_idx in edge_indices:\n        i, j = edges[edge_idx]\n        # Randomly choose direction\n        if rng.random()  0.5:\n            x_new[i] = x_new[j]\n        else:\n            x_new[j] = x_new[i]\n    return x_new\n\ndef simulate_bc_sweep(x, n_nodes, edges, epsilon, mu, rng):\n    \"\"\"Simulates one sweep of the Bounded Confidence model.\"\"\"\n    x_new = x.copy()\n    if edges.shape[0] == 0:\n        return x_new\n        \n    edge_indices = rng.integers(0, len(edges), size=n_nodes)\n    for edge_idx in edge_indices:\n        i, j = edges[edge_idx]\n        if abs(x_new[i] - x_new[j]) = epsilon:\n            dx = mu * (x_new[j] - x_new[i])\n            x_new[i] += dx\n            x_new[j] -= dx\n    return x_new\n\ndef generate_full_timeseries(x0, n_steps, true_model, params):\n    \"\"\"Generates the ground truth time series.\"\"\"\n    n_nodes = len(x0)\n    timeseries = np.zeros((n_steps + 1, n_nodes))\n    timeseries[0] = x0\n    x_current = x0.copy()\n    \n    for t in range(1, n_steps + 1):\n        if true_model == 'voter':\n            x_current = simulate_voter_sweep(x_current, n_nodes, params['edges'], params['rng'])\n        else: # bc\n            x_current = simulate_bc_sweep(x_current, n_nodes, params['edges'], params['epsilon'], params['mu'], params['rng'])\n        timeseries[t] = x_current\n    return timeseries\n\ndef ensemble_forecast(x_start, model_type, t_forecast, k_ensemble, n_nodes, edges, rng, **model_params):\n    \"\"\"Generates an ensemble forecast.\"\"\"\n    trajectories = np.zeros((k_ensemble, t_forecast, n_nodes))\n    \n    for k in range(k_ensemble):\n        x_current = x_start.copy()\n        for h in range(t_forecast):\n            if model_type == 'voter':\n                x_current = simulate_voter_sweep(x_current, n_nodes, edges, rng)\n            else: # bc\n                x_current = simulate_bc_sweep(x_current, n_nodes, edges, model_params['epsilon'], model_params['mu'], rng)\n            trajectories[k, h, :] = x_current\n            \n    return np.mean(trajectories, axis=0)\n\ndef solve():\n    common_settings = {\n        'N': 30,\n        'T_obs': 20,\n        'T_pred': 5,\n        'K': 50,\n        'bc_param_grid': {'epsilon': [0.15, 0.25, 0.35], 'mu': [0.3, 0.5]},\n        'tau': 1e-9\n    }\n\n    test_cases = [\n        {\n            'name': 'Case 1', 'graph_type': 'ring', 'true_model': 'bc',\n            'model_params': {'epsilon_true': 0.25, 'mu_true': 0.5},\n            'opinion_dist': 'uniform', 'op_seed': 7, 'graph_seed': None\n        },\n        {\n            'name': 'Case 2', 'graph_type': 'er', 'graph_params': {'p': 0.2}, 'true_model': 'voter',\n            'model_params': {}, 'opinion_dist': 'bernoulli', 'op_seed': 5, 'graph_seed': 123\n        },\n        {\n            'name': 'Case 3', 'graph_type': 'two_rings', 'graph_params': {'n1': 15, 'n2': 15}, 'true_model': 'bc',\n            'model_params': {'epsilon_true': 0.20, 'mu_true': 0.5},\n            'opinion_dist': 'uniform', 'op_seed': 33, 'graph_seed': None\n        },\n        {\n            'name': 'Case 4', 'graph_type': 'star', 'true_model': 'voter',\n            'model_params': {}, 'opinion_dist': 'bernoulli', 'op_seed': 21, 'graph_seed': None\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = common_settings['N']\n        T_obs = common_settings['T_obs']\n        T_pred = common_settings['T_pred']\n        K = common_settings['K']\n        \n        # Setup RNGs for reproducibility\n        master_seed = case['op_seed'] if case['graph_seed'] is None else case['graph_seed']\n        rng = np.random.default_rng(master_seed)\n        \n        # --- 1. Data Generation ---\n        if case['graph_type'] == 'ring':\n            adj, edges = make_ring_graph(N)\n        elif case['graph_type'] == 'er':\n            graph_rng = np.random.default_rng(case['graph_seed'])\n            adj, edges = make_er_graph(N, case['graph_params']['p'], graph_rng)\n        elif case['graph_type'] == 'two_rings':\n            adj, edges = make_two_rings_graph(case['graph_params']['n1'], case['graph_params']['n2'])\n        elif case['graph_type'] == 'star':\n            adj, edges = make_star_graph(N)\n\n        op_rng = np.random.default_rng(case['op_seed'])\n        if case['opinion_dist'] == 'uniform':\n            x0 = op_rng.uniform(0, 1, size=N)\n        else:  # bernoulli\n            x0 = op_rng.integers(0, 2, size=N).astype(float)\n        \n        sim_params = {'edges': edges, 'rng': rng}\n        if case['true_model'] == 'bc':\n            sim_params['epsilon'] = case['model_params']['epsilon_true']\n            sim_params['mu'] = case['model_params']['mu_true']\n            \n        full_ts = generate_full_timeseries(x0, T_obs + T_pred, case['true_model'], sim_params)\n        \n        x_obs = full_ts[:T_obs + 1]\n        x_start_pred = x_obs[-1]\n        x_heldout = full_ts[T_obs + 1:]\n\n        # --- 2. Fit Bounded Confidence Model ---\n        best_bc_params = {}\n        min_train_mse = float('inf')\n        bc_grid = common_settings['bc_param_grid']\n        \n        for eps_val in bc_grid['epsilon']:\n            for mu_val in bc_grid['mu']:\n                total_se = 0\n                for t in range(T_obs):\n                    x_t = x_obs[t]\n                    x_t_plus_1_true = x_obs[t + 1]\n                    x_t_plus_1_pred = ensemble_forecast(x_t, 'bc', 1, K, N, edges, rng, epsilon=eps_val, mu=mu_val)[0]\n                    total_se += np.sum((x_t_plus_1_pred - x_t_plus_1_true)**2)\n                \n                train_mse = total_se / (N * T_obs)\n                if train_mse  min_train_mse:\n                    min_train_mse = train_mse\n                    best_bc_params = {'epsilon': eps_val, 'mu': mu_val}\n        \n        # --- 3. Out-of-Sample Evaluation ---\n        # Voter Model\n        voter_preds = ensemble_forecast(x_start_pred, 'voter', T_pred, K, N, edges, rng)\n        mse_voter = np.mean((voter_preds - x_heldout)**2)\n        \n        # Bounded Confidence Model\n        bc_preds = ensemble_forecast(x_start_pred, 'bc', T_pred, K, N, edges, rng, **best_bc_params)\n        mse_bc = np.mean((bc_preds - x_heldout)**2)\n        \n        # --- 4. Model Selection ---\n        if mse_bc  mse_voter - common_settings['tau']:\n            results.append(1) # Bounded Confidence\n        else:\n            results.append(0) # Voter\n            \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}