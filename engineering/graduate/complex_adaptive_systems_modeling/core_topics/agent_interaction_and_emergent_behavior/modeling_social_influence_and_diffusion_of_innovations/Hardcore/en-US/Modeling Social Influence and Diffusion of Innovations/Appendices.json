{
    "hands_on_practices": [
        {
            "introduction": "Understanding how innovations spread often requires looking beyond simple connectivity to the specific architecture of social ties. This exercise uses a complex contagion model to illustrate a fundamental principle: the importance of local network structure. By comparing two networks with identical degree sequences but different levels of clustering, you will discover firsthand how social reinforcement through dense local connections can be the critical factor that determines whether a cascade takes off or fizzles out .",
            "id": "4129695",
            "problem": "Consider two simple, undirected graphs that both have the same degree sequence but differ in local clustering. The population consists of $12$ agents, each represented by a node, and each agent has exactly $4$ neighbors. Adoption of an innovation proceeds via a deterministic threshold rule: at discrete time $t=0,1,2,\\dots$, a node adopts permanently at time $t+1$ if and only if at least a fraction $\\phi$ of its neighbors had adopted by time $t$. Assume $1/4<\\phi\\le 1/2$, so that at least $2$ adopted neighbors are required for adoption when the degree is $4$. The initial seed consists of exactly $2$ adjacent nodes chosen uniformly at random from the set of edges.\n\nDefine the two networks explicitly:\n\n- High-clustering network $\\mathcal{H}$: Nodes are labeled $0,1,\\dots,11$ and edges are between $i$ and $(i\\pm 1)\\bmod 12$, and between $i$ and $(i\\pm 2)\\bmod 12$. Each node has degree $4$. Because neighbors-of-neighbors are often themselves neighbors, this graph contains many triangles.\n\n- Low-clustering network $\\mathcal{L}$: Partition the $12$ nodes into two sets $L=\\{L_0,L_1,\\dots,L_5\\}$ and $R=\\{R_0,R_1,\\dots,R_5\\}$. For each $i\\in\\{0,1,\\dots,5\\}$, include edges from $L_i$ to $R_i$, $R_{(i+1)\\bmod 6}$, $R_{(i+2)\\bmod 6}$, and $R_{(i+3)\\bmod 6}$. Each node has degree $4$, and the graph is bipartite, so there are no triangles.\n\nUsing only fundamental definitions of the threshold adoption rule and basic consequences of graph structure (e.g., the absence of triangles in a bipartite graph), reason about the qualitative difference in cascade probabilities under the specified regime $1/4<\\phi\\le 1/2$ and adjacent-pair seeding. Which of the following statements are correct?\n\nA. For $1/4<\\phi\\le 1/2$, an adjacent seed pair in $\\mathcal{H}$ triggers a global cascade with probability $1$, whereas in $\\mathcal{L}$ the process stalls immediately and the probability of a global cascade is $0$.\n\nB. Because both networks have identical degree sequences, their cascade probabilities must be identical for any $\\phi$ and any seeding.\n\nC. For $1/4<\\phi\\le 1/2$, an adjacent seed pair in $\\mathcal{L}$ yields a higher cascade probability than in $\\mathcal{H}$ because lower clustering increases exposure diversity.\n\nD. For $1/4<\\phi\\le 1/2$, an adjacent seed pair cannot trigger a cascade on either network because no nonseed node has at least $2$ active neighbors at time $t=0$.",
            "solution": "The user wants me to solve a problem. I need to output my answer in a specific format.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- **Graphs**: Two simple, undirected graphs, $\\mathcal{H}$ and $\\mathcal{L}$, both with the same degree sequence.\n- **Population**: $12$ agents (nodes).\n- **Degree**: Each node has degree $k=4$.\n- **Adoption Rule**: Deterministic threshold rule. A node adopts permanently at time $t+1$ if and only if at least a fraction $\\phi$ of its neighbors had adopted by time $t$. Time $t=0,1,2,\\dots$.\n- **Threshold**: $1/4 < \\phi \\le 1/2$. For degree $k=4$, the number of required adopted neighbors is $\\lceil k\\phi \\rceil = \\lceil 4\\phi \\rceil$. Since $1 < 4\\phi \\le 2$, the required number of adopted neighbors is $2$.\n- **Initial Seed**: Exactly $2$ adjacent nodes chosen uniformly at random from the set of edges.\n- **Network $\\mathcal{H}$ (High-clustering)**:\n    - Nodes: $\\{0, 1, \\dots, 11\\}$.\n    - Edges for node $i$: $(i, (i \\pm 1) \\pmod{12})$ and $(i, (i \\pm 2) \\pmod{12})$.\n    - Property: Contains many triangles.\n- **Network $\\mathcal{L}$ (Low-clustering)**:\n    - Nodes: Partitioned into $L=\\{L_0, \\dots, L_5\\}$ and $R=\\{R_0, \\dots, R_5\\}$.\n    - Edges for node $L_i$: $(L_i, R_i)$, $(L_i, R_{(i+1)\\pmod 6})$, $(L_i, R_{(i+2)\\pmod 6})$, and $(L_i, R_{(i+3)\\pmod 6})$ for $i \\in \\{0, 1, \\dots, 5\\}$.\n    - Property: Bipartite, no triangles.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem employs the well-established threshold model of diffusion on networks, a standard topic in complex systems and network science. The graphs $\\mathcal{H}$ and $\\mathcal{L}$ are well-defined mathematical objects ($4$-regular graphs on $12$ vertices). The premise is scientifically and mathematically sound.\n- **Well-Posedness**: The adoption rule is deterministic, and the seeding is probabilistic over a well-defined set. The question asks for a qualitative comparison of cascade probabilities, which can be derived from the given parameters and network structures. A unique, meaningful solution exists.\n- **Objectivity**: The problem is stated in precise, formal language. The descriptions \"high-clustering\" and \"low-clustering\" are justified by the explicit graph constructions, which respectively guarantee the presence and absence of triangles. The problem is objective.\n- **Flaw Checklist**: The problem does not violate any of the specified criteria for invalidity. It is self-contained, consistent, and scientifically formalizable.\n\n**Step 3: Verdict and Action**\n- The problem is **valid**. The solution process will now proceed.\n\n**Derivation**\n\nThe core of this problem lies in understanding how the local network structure interacts with the adoption threshold. The adoption threshold is $\\phi$, where $1/4 < \\phi \\le 1/2$. For a node with degree $k=4$, the number of adopted neighbors required to trigger its own adoption is $\\lceil k\\phi \\rceil = \\lceil 4\\phi \\rceil$. Given the range of $\\phi$, we have $1 < 4\\phi \\le 2$, which means an inactive node must have at least $2$ adopted neighbors to adopt.\n\nThe process starts at time $t=0$ with a seed of two adjacent nodes, let's call them $u$ and $v$. At time $t=1$, we examine all other currently inactive nodes. For a cascade to begin, at least one inactive node, say $w$, must adopt. According to the rule, node $w$ must have at least $2$ neighbors that were adopted by time $t=0$. Since the only adopted nodes at $t=0$ are $u$ and $v$, node $w$ must be a neighbor of both $u$ and $v$.\n\nThe condition for the cascade to propagate beyond the initial seed is the existence of a common neighbor to the seed nodes $u$ and $v$. If such a node $w$ exists, then the edges $(u,v)$, $(v,w)$, and $(w,u)$ must all be present in the graph. This set of three nodes and three edges forms a triangle, or a $3$-cycle.\n\nLet's analyze the two networks in this context.\n\n**Analysis of Network $\\mathcal{L}$ (Low-clustering):**\nThe network $\\mathcal{L}$ is explicitly constructed to be bipartite, with node partitions $L=\\{L_0, L_1, \\dots, L_5\\}$ and $R=\\{R_0, R_1, \\dots, R_5\\}$. All edges in $\\mathcal{L}$ connect a node in $L$ to a node in $R$. By definition, a bipartite graph is a graph whose vertices can be divided into two disjoint sets such that every edge connects a vertex in the first set to one in the second set. A fundamental property of bipartite graphs is that they do not contain any odd-length cycles. As a triangle is a cycle of length $3$, network $\\mathcal{L}$ contains no triangles.\n\nTherefore, for any adjacent seed pair $(u,v)$ in $\\mathcal{L}$, there cannot be a common neighbor $w$. If such a $w$ existed, $\\{u,v,w\\}$ would form a triangle, which is impossible in $\\mathcal{L}$. This means that at $t=0$, no inactive node has more than one neighbor in the adopted set $\\{u,v\\}$. Since the threshold for adoption is $2$, no new node will adopt at $t=1$. The process stalls immediately, with the set of adopted nodes permanently fixed at $\\{u,v\\}$. A \"global cascade\" (defined as adoption beyond the initial seed, and typically meaning a significant fraction of the network) does not occur. The probability of a global cascade is therefore $0$.\n\n**Analysis of Network $\\mathcal{H}$ (High-clustering):**\nThe network $\\mathcal{H}$ is constructed on nodes $\\{0, 1, \\dots, 11\\}$ with edges connecting node $i$ to $(i\\pm 1)\\bmod 12$ and $(i\\pm 2)\\bmod 12$. The problem states the graph has many triangles. We must verify that for any adjacent seed pair, at least one common neighbor exists. Edges in this graph can be classified by the distance between their endpoints: distance-$1$ edges like $(i, i+1)$ and distance-$2$ edges like $(i, i+2)$.\n\n1.  **Seed is a distance-$1$ edge**: Let the seed be the adjacent pair $(0,1)$. We identify their common neighbors.\n    - Neighbors of $0$: $N(0) = \\{11, 1, 2, 10\\}$.\n    - Neighbors of $1$: $N(1) = \\{0, 2, 3, 11\\}$.\n    - The set of common neighbors is $N(0) \\cap N(1) = \\{2, 11\\}$.\n    Since both nodes $2$ and $11$ are connected to the initial adopters $0$ and $1$, they both meet the threshold of $2$ adopted neighbors. At $t=1$, nodes $2$ and $11$ will adopt. The set of adopters becomes $\\{0, 1, 2, 11\\}$. This initiates a cascade that, as shown by further step-by-step analysis, will eventually lead to the adoption of all $12$ nodes.\n\n2.  **Seed is a distance-$2$ edge**: Let the seed be the adjacent pair $(0,2)$. We identify their common neighbors.\n    - Neighbors of $0$: $N(0) = \\{11, 1, 2, 10\\}$.\n    - Neighbors of $2$: $N(2) = \\{0, 1, 3, 4\\}$.\n    - The set of common neighbors is $N(0) \\cap N(2) = \\{1\\}$.\n    Node $1$ is connected to the initial adopters $0$ and $2$. It meets the threshold and will adopt at $t=1$. The set of adopters becomes $\\{0,1,2\\}$. This larger active set is sufficient to trigger further adoptions (e.g., nodes $3$ and $11$ will adopt at $t=2$), leading to a global cascade.\n\nSince the graph $\\mathcal{H}$ is edge-transitive on these two types of edges, and both types of edges initiate a global cascade, any randomly chosen edge as a seed will trigger a global cascade. The probability of a global cascade in $\\mathcal{H}$ is $1$.\n\n**Option-by-Option Analysis**\n\n**A. For $1/4<\\phi\\le 1/2$, an adjacent seed pair in $\\mathcal{H}$ triggers a global cascade with probability $1$, whereas in $\\mathcal{L}$ the process stalls immediately and the probability of a global cascade is $0$.**\nThis statement accurately summarizes the results of our derivation. In $\\mathcal{H}$, the high number of triangles ensures that any seed edge provides the necessary redundant connections to start a cascade, which then propagates globally. The probability is $1$. In $\\mathcal{L}$, the absence of triangles prevents the cascade from ever starting. The probability is $0$.\nVerdict: **Correct**.\n\n**B. Because both networks have identical degree sequences, their cascade probabilities must be identical for any $\\phi$ and any seeding.**\nThis statement is a common fallacy in network analysis. The degree sequence is a first-order property and is not sufficient to determine dynamic processes like cascades. As our analysis shows, higher-order structures like clustering (triangles) are crucial and lead to drastically different outcomes ($1$ versus $0$ cascade probability) even with identical degree sequences.\nVerdict: **Incorrect**.\n\n**C. For $1/4<\\phi\\le 1/2$, an adjacent seed pair in $\\mathcal{L}$ yields a higher cascade probability than in $\\mathcal{H}$ because lower clustering increases exposure diversity.**\nOur analysis shows the cascade probability is $0$ in $\\mathcal{L}$ and $1$ in $\\mathcal{H}$. The statement makes the opposite claim. The justification, \"lower clustering increases exposure diversity,\" is a concept (related to Granovetter's \"strength of weak ties\") that applies to simple contagion or information spreading, where a single exposure is sufficient. For complex contagions with a threshold greater than $1$, social reinforcement from high-clustering (redundant ties) is necessary. The logic is misapplied to this context.\nVerdict: **Incorrect**.\n\n**D. For $1/4<\\phi\\le 1/2$, an adjacent seed pair cannot trigger a cascade on either network because no nonseed node has at least $2$ active neighbors at time $t=0$.**\nThis statement correctly describes the situation in network $\\mathcal{L}$ but fails for network $\\mathcal{H}$. In $\\mathcal{H}$, for any adjacent seed pair, there is at least one non-seed node that has both seed nodes as neighbors (i.e., has $2$ active neighbors at $t=0$). Therefore, a cascade is triggered in $\\mathcal{H}$. Since the statement claims this for \"either network,\" it is false.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond the network's structure, the nature of the information being shared is paramount. This practice delves into a subtle but crucial aspect of social learning: the problem of correlated information sources. You will contrast naive DeGroot agents with a fully rational Bayesian observer to quantify the 'exaggeration factor' of confidence that arises when agents fail to account for the fact that they are hearing the same underlying news from multiple peers, a key mechanism behind echo chambers .",
            "id": "4129757",
            "problem": "Consider a population of three agents engaged in social learning about the unknown quality $\\,\\theta\\,$ of a new innovation. The agents interact on a fully connected network and repeatedly average their neighbors’ current beliefs using the linear opinion pool described by the DeGroot model of social influence. The row-stochastic interaction matrix is\n$$\nW \\;=\\; \\begin{pmatrix}\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\n\\end{pmatrix},\n$$\nand the initial beliefs are simply the agents’ private signals. Two agents (agents $\\,1\\,$ and $\\,2\\,$) read the same expert report and therefore share a common information component, while the third agent (agent $\\,3\\,$) uses a source independent of the expert report. Formally, conditional on $\\,\\theta\\,$, the private signals are generated by\n$$\nx_1 \\;=\\; \\theta \\;+\\; u \\;+\\; \\epsilon_1, \\qquad\nx_2 \\;=\\; \\theta \\;+\\; u \\;+\\; \\epsilon_2, \\qquad\nx_3 \\;=\\; \\theta \\;+\\; \\epsilon_3,\n$$\nwhere $\\,u \\sim \\mathcal{N}(0,\\tau^2)\\,$ is the common source component affecting agents $\\,1\\,$ and $\\,2\\,$, and $\\,\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\,$ are mutually independent idiosyncratic noises, independent of $\\,u\\,$. All random variables are jointly Gaussian, and $\\,\\tau^2 > 0\\,$ and $\\,\\sigma^2 > 0\\,$ are known.\n\nSuppose that the agents are naive in the sense that, after the DeGroot averaging converges, they assess their own posterior about $\\,\\theta\\,$ as if the three signals were conditionally independent given $\\,\\theta\\,$ with identical variance $\\,\\sigma^2\\,$. A fully rational Bayesian aggregator, by contrast, knows the true joint distribution of $\\,x=(x_1,x_2,x_3)^\\top\\,$ and computes the minimum-variance unbiased linear estimate of $\\,\\theta\\,$ based on $\\,x\\,$ together with its associated posterior precision.\n\nStarting from the fundamental definitions of linear unbiased estimation under joint Gaussian noise and the definition of consensus under repeated averaging on a row-stochastic network, derive from first principles the posterior precision exaggeration factor defined by\n$$\nf \\;\\equiv\\; \\frac{\\text{naive posterior precision}}{\\text{fully Bayesian posterior precision}} \\;-\\; 1,\n$$\nas a closed-form analytic expression in terms of $\\,\\tau^2\\,$ and $\\,\\sigma^2\\,$. Your final answer must be a single analytic expression. Do not use any rounding.",
            "solution": "The problem asks for the posterior precision exaggeration factor, $f$, which quantifies the overconfidence of naive agents who ignore correlations in their information signals. This factor is defined as:\n$$\nf \\equiv \\frac{\\text{naive posterior precision}}{\\text{fully Bayesian posterior precision}} - 1\n$$\nTo derive $f$, we must compute two quantities: the naive posterior precision and the fully Bayesian posterior precision.\n\n### 1. Naive Posterior Precision\n\nThe naive agents assess their posterior belief about the quality $\\theta$ under the incorrect assumption that their private signals, $x_1$, $x_2$, and $x_3$, are conditionally independent given $\\theta$. Specifically, they model the signals as independent draws from a Gaussian distribution with mean $\\theta$ and a common variance $\\sigma^2$:\n$$\nx_i | \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2) \\quad \\text{for } i \\in \\{1, 2, 3\\}, \\text{ assumed independent}\n$$\nUnder this (incorrect) model and assuming a non-informative flat prior on $\\theta$, the posterior distribution for $\\theta$ is derived from the likelihood function. The likelihood of observing the signals $x = (x_1, x_2, x_3)^\\top$ is:\n$$\np(x | \\theta) = \\prod_{i=1}^{3} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\theta)^2}{2\\sigma^2}\\right) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{3} (x_i - \\theta)^2\\right)\n$$\nThe posterior probability density $p(\\theta | x)$ is proportional to the likelihood, $p(\\theta | x) \\propto p(x | \\theta)$. The log-posterior is therefore:\n$$\n\\ln p(\\theta | x) = C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{3} (x_i - \\theta)^2\n$$\nwhere $C$ is a constant. We can complete the square for $\\theta$ in the exponent. Let $\\bar{x} = \\frac{1}{3}\\sum_{i=1}^{3} x_i$ be the sample mean of the signals.\n$$\n\\sum_{i=1}^{3} (x_i - \\theta)^2 = \\sum_{i=1}^{3} (x_i - \\bar{x} + \\bar{x} - \\theta)^2 = \\sum_{i=1}^{3} (x_i - \\bar{x})^2 + \\sum_{i=1}^{3} (\\bar{x} - \\theta)^2 = \\sum_{i=1}^{3} (x_i - \\bar{x})^2 + 3(\\bar{x} - \\theta)^2\n$$\nThe first term does not depend on $\\theta$, so it can be absorbed into the constant. The log-posterior becomes:\n$$\n\\ln p(\\theta | x) = C' - \\frac{3(\\theta - \\bar{x})^2}{2\\sigma^2} = C' - \\frac{(\\theta - \\bar{x})^2}{2(\\sigma^2/3)}\n$$\nThis is the logarithm of a Gaussian probability density function for $\\theta$ with mean $\\bar{x}$ and variance $\\sigma^2/3$. The posterior distribution under the naive assumption is $\\theta | x \\sim \\mathcal{N}(\\bar{x}, \\sigma^2/3)$.\n\nThe problem states that agents use the DeGroot model, converging to a consensus belief $b_{\\infty} = \\frac{1}{3}x_1 + \\frac{1}{3}x_2 + \\frac{1}{3}x_3 = \\bar{x}$. This is consistent with their posterior mean.\n\nThe naive posterior precision is the reciprocal of the naive posterior variance:\n$$\n\\text{naive posterior precision} = \\frac{1}{\\text{Var}_{\\text{naive}}(\\theta|x)} = \\frac{1}{\\sigma^2/3} = \\frac{3}{\\sigma^2}\n$$\n\n### 2. Fully Bayesian Posterior Precision\n\nA fully rational Bayesian aggregator uses the true joint distribution of the signals. The signals are given by:\n$$\nx_1 = \\theta + u + \\epsilon_1\n$$\n$$\nx_2 = \\theta + u + \\epsilon_2\n$$\n$$\nx_3 = \\theta + \\epsilon_3\n$$\nwhere $u \\sim \\mathcal{N}(0, \\tau^2)$ and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent noise terms.\nWe can express this in the form of a linear model $x = \\mathbf{1}\\theta + v$, where $x=(x_1, x_2, x_3)^\\top$, $\\mathbf{1}=(1, 1, 1)^\\top$, and $v$ is a vector of zero-mean noise terms $v = (x_1-\\theta, x_2-\\theta, x_3-\\theta)^\\top$.\n$$\nv_1 = u + \\epsilon_1, \\quad v_2 = u + \\epsilon_2, \\quad v_3 = \\epsilon_3\n$$\nThe covariance matrix of the noise vector $v$, denoted $\\Sigma_x$, is found by computing the covariances $E[v_i v_j]$:\n$$\n\\text{Var}(v_1) = E[(u+\\epsilon_1)^2] = E[u^2] + E[\\epsilon_1^2] = \\tau^2 + \\sigma^2\n$$\n$$\n\\text{Var}(v_2) = E[(u+\\epsilon_2)^2] = E[u^2] + E[\\epsilon_2^2] = \\tau^2 + \\sigma^2\n$$\n$$\n\\text{Var}(v_3) = E[\\epsilon_3^2] = \\sigma^2\n$$\n$$\n\\text{Cov}(v_1, v_2) = E[(u+\\epsilon_1)(u+\\epsilon_2)] = E[u^2] = \\tau^2\n$$\n$$\n\\text{Cov}(v_1, v_3) = E[(u+\\epsilon_1)\\epsilon_3] = 0\n$$\n$$\n\\text{Cov}(v_2, v_3) = E[(u+\\epsilon_2)\\epsilon_3] = 0\n$$\nThe covariance matrix is therefore:\n$$\n\\Sigma_x = \\begin{pmatrix} \\tau^2+\\sigma^2 & \\tau^2 & 0 \\\\ \\tau^2 & \\tau^2+\\sigma^2 & 0 \\\\ 0 & 0 & \\sigma^2 \\end{pmatrix}\n$$\nFor a linear Gaussian model $x \\sim \\mathcal{N}(\\mathbf{1}\\theta, \\Sigma_x)$, the posterior precision of $\\theta$ (with a flat prior) is given by the quadratic term in the log-likelihood, which is equivalent to the precision of the minimum-variance unbiased linear estimator. This precision is given by $\\mathbf{1}^\\top \\Sigma_x^{-1} \\mathbf{1}$.\n\nWe must first compute the inverse of $\\Sigma_x$. Since $\\Sigma_x$ is block-diagonal, we can invert the blocks separately. The top-left $2 \\times 2$ block is $M = \\begin{pmatrix} \\tau^2+\\sigma^2 & \\tau^2 \\\\ \\tau^2 & \\tau^2+\\sigma^2 \\end{pmatrix}$.\nThe determinant is $\\det(M) = (\\tau^2+\\sigma^2)^2 - (\\tau^2)^2 = \\tau^4 + 2\\tau^2\\sigma^2 + \\sigma^4 - \\tau^4 = \\sigma^2(2\\tau^2+\\sigma^2)$.\nThe inverse is:\n$$\nM^{-1} = \\frac{1}{\\sigma^2(2\\tau^2+\\sigma^2)} \\begin{pmatrix} \\tau^2+\\sigma^2 & -\\tau^2 \\\\ -\\tau^2 & \\tau^2+\\sigma^2 \\end{pmatrix}\n$$\nThe inverse of the full covariance matrix is:\n$$\n\\Sigma_x^{-1} = \\begin{pmatrix} M^{-1} & \\mathbf{0} \\\\ \\mathbf{0}^\\top & 1/\\sigma^2 \\end{pmatrix} = \\frac{1}{\\sigma^2(2\\tau^2+\\sigma^2)} \\begin{pmatrix} \\tau^2+\\sigma^2 & -\\tau^2 & 0 \\\\ -\\tau^2 & \\tau^2+\\sigma^2 & 0 \\\\ 0 & 0 & 2\\tau^2+\\sigma^2 \\end{pmatrix}\n$$\nThe fully Bayesian posterior precision is the sum of all elements of $\\Sigma_x^{-1}$, since $\\mathbf{1}^\\top \\Sigma_x^{-1} \\mathbf{1} = \\sum_{i,j} (\\Sigma_x^{-1})_{ij}$:\n$$\n\\text{fully Bayesian posterior precision} = \\frac{2}{2\\tau^2+\\sigma^2} + \\frac{1}{\\sigma^2} = \\frac{2\\sigma^2 + (2\\tau^2+\\sigma^2)}{\\sigma^2(2\\tau^2+\\sigma^2)} = \\frac{2\\tau^2+3\\sigma^2}{\\sigma^2(2\\tau^2+\\sigma^2)}\n$$\n\n### 3. Posterior Precision Exaggeration Factor\n\nNow we compute the factor $f$:\n$$\nf = \\frac{\\text{naive posterior precision}}{\\text{fully Bayesian posterior precision}} - 1\n$$\n$$\nf = \\frac{3/\\sigma^2}{\\frac{2\\tau^2+3\\sigma^2}{\\sigma^2(2\\tau^2+\\sigma^2)}} - 1\n$$\n$$\nf = \\frac{3}{\\sigma^2} \\cdot \\frac{\\sigma^2(2\\tau^2+\\sigma^2)}{2\\tau^2+3\\sigma^2} - 1\n$$\n$$\nf = \\frac{3(2\\tau^2+\\sigma^2)}{2\\tau^2+3\\sigma^2} - 1\n$$\nPutting everything over a common denominator:\n$$\nf = \\frac{3(2\\tau^2+\\sigma^2) - (2\\tau^2+3\\sigma^2)}{2\\tau^2+3\\sigma^2}\n$$\n$$\nf = \\frac{6\\tau^2+3\\sigma^2 - 2\\tau^2-3\\sigma^2}{2\\tau^2+3\\sigma^2}\n$$\n$$\nf = \\frac{4\\tau^2}{2\\tau^2+3\\sigma^2}\n$$\nThis is the final closed-form expression for the posterior precision exaggeration factor.",
            "answer": "$$\\boxed{\\frac{4\\tau^2}{2\\tau^2+3\\sigma^2}}$$"
        },
        {
            "introduction": "Real-world social interactions are not static; they are events that occur at specific points in time. This final practice moves from static graphs to the more realistic domain of temporal networks, where causality and the sequence of events are key. You will develop an algorithm to trace the flow of influence through a series of time-stamped contacts, providing a practical tool for calculating the reach of a diffusion process within a specific time window .",
            "id": "4129747",
            "problem": "Consider a temporal network representing the flow of social influence for diffusion of an innovation. The network is specified as a finite list of timestamped directed contacts, each contact being a triple $(i,j,t)$ that denotes that agent $i$ influences agent $j$ at absolute time $t$. Time is measured in seconds, and contacts occur at discrete times. Assume instantaneous transmission along a contact, negligible processing delays, and that influence cannot propagate backward in time. The diffusion process is initiated by a single seed agent $s$ at time $0$ that is already influenced at time $0$.\n\nDefine a time-respecting path from $s$ to a node $v$ as a finite sequence of contacts $((v_0,v_1,t_1), (v_1,v_2,t_2), \\ldots, (v_{k-1},v_k,t_k))$ with $v_0 = s$, $v_k = v$, $t_1 \\le t_2 \\le \\cdots \\le t_k$, and $t_1 \\ge 0$. A node $v$ is temporally reachable from $s$ within the window $[0,T]$ if there exists at least one time-respecting path from $s$ to $v$ whose last timestamp $t_k$ satisfies $t_k \\le T$. The temporal reachability of the seed $s$ is the set $R(T)$ of all nodes $v \\neq s$ that are temporally reachable within $[0,T]$.\n\nStarting from the fundamental principle of causality and time-ordering in temporal networks, and using only the assumption that influence can traverse a contact $(i,j,t)$ if and only if agent $i$ is already influenced by time $t$, derive an algorithm that computes $R(T)$ from a given contact list and seed $s$ for any finite $T \\ge 0$. The algorithm should produce the cardinality $|R(T)|$, an integer count of temporally reachable nodes excluding the seed. The derivation must not rely on shortcut formulas and must justify correctness from first principles.\n\nYour program must implement this algorithm and apply it to the following test suite (times are in seconds):\n\n- Test case $1$: contacts $[(0,1,1),(1,2,2),(2,3,3)]$, seed $s=0$, window $T=3$. Expected behavior: a straightforward chain with strictly increasing times.\n- Test case $2$: contacts $[(0,1,2),(1,2,2)]$, seed $s=0$, window $T=2$. Expected behavior: equality at the boundary $t=T$ is allowed.\n- Test case $3$: contacts $[(0,1,1),(1,2,1),(2,3,1)]$, seed $s=0$, window $T=1$. Expected behavior: simultaneous timestamps allow instantaneous chaining with non-decreasing times.\n- Test case $4$: contacts $[(1,0,1)]$, seed $s=0$, window $T=10$. Expected behavior: reversed direction prevents outward diffusion from the seed.\n- Test case $5$: contacts $[(0,1,5),(1,0,6),(1,2,7),(0,2,10)]$, seed $s=0$, window $T=8$. Expected behavior: cycles and multiple paths; only those concluding by $T$ contribute.\n\nEach test case is defined solely by the list of contacts, the seed $s$, and the window $T$. The required output is a single line containing a comma-separated list of the integer cardinalities $|R(T)|$ for the test cases in the order listed above, enclosed in square brackets. For example, the program must output a string of the form $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$.\n\nTime values are in seconds; the output counts are unitless integers. No angles or percentages are involved. The program must be self-contained and require no input. The final output line must contain only the specified list format without any additional text.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It describes a classic temporal reachability problem on a network defined by a sequence of timed events. This is a standard problem in the study of dynamic networks and complex systems, and all terms and conditions are formally and unambiguously defined. Therefore, I will proceed with the derivation and solution.\n\nThe problem asks for an algorithm to compute the size of the set of temporally reachable nodes, $|R(T)|$, from a seed node $s$ within a time window $[0, T]$. The derivation must be based on the fundamental principles of causality and time-ordering.\n\nThe core principle is causality: an agent $j$ can be influenced by an agent $i$ via a contact $(i,j,t)$ only if agent $i$ is itself already influenced at or before time $t$. The goal is to find all nodes that can be reached from the seed $s$ through sequences of such causally-ordered events. This is equivalent to finding the earliest possible time that each node can be influenced.\n\nLet $\\tau(v)$ be the earliest time at which agent $v$ can become influenced. The problem specifies that the seed agent $s$ is influenced at time $0$, so we have the initial condition:\n$$ \\tau(s) = 0 $$\nFor any other agent $v \\neq s$, it is initially not influenced. We can represent this state by setting its earliest influence time to infinity:\n$$ \\tau(v) = \\infty \\quad \\forall v \\neq s $$\n\nThe diffusion process unfolds as a series of discrete events, which are the contacts. A time-respecting path is defined by a sequence of contacts $((v_0,v_1,t_1), (v_1,v_2,t_2), \\ldots, (v_{k-1},v_k,t_k))$ where $v_0 = s$ and the timestamps are non-decreasing, $t_1 \\le t_2 \\le \\cdots \\le t_k$. This non-decreasing time constraint is the mathematical formalization of causality and time-ordering. An algorithm that respects this structure will naturally be correct.\n\nThe most direct way to model the flow of time is to process events in the order they occur. Therefore, the first step of our algorithm must be to sort the list of all contacts, $C$, by their timestamp $t$ in non-decreasing order. Let this sorted list be $C_{\\text{sorted}}$.\n\nWe can now iterate through $C_{\\text{sorted}}$ and update the earliest influence times, $\\tau(v)$, for all nodes. For each contact $(i, j, t) \\in C_{\\text{sorted}}$, we apply the causal condition: if the source agent $i$ has been influenced at or before time $t$, i.e., if $\\tau(i) \\le t$, then influence can propagate to agent $j$. Since transmission is instantaneous, agent $j$ can become influenced at time $t$. This provides a new potential influence time for $j$. Because we seek the *earliest* such time, we must update $\\tau(j)$ only if this new time $t$ is smaller than the current known value of $\\tau(j)$. This leads to the update rule:\n$$ \\text{If } \\tau(i) \\le t, \\text{ then } \\tau(j) := \\min(\\tau(j), t) $$\n\nThis single pass through the time-sorted contacts is sufficient to find the earliest influence time for all nodes. To justify this, consider any arbitrary time-respecting path from $s$ to some node $v$. The path consists of a sequence of contacts with non-decreasing timestamps. Because our algorithm processes contacts in this same non-decreasing temporal order, a contact $(v_{m}, v_{m+1}, t_{m+1})$ in the path will only be processed after or at the same time as the preceding contact $(v_{m-1}, v_m, t_m)$ (since $t_m \\le t_{m+1}$). By induction, when we process contact $(v_m, v_{m+1}, t_{m+1})$, the value of $\\tau(v_m)$ will have already been correctly established to be no later than $t_m$ by the propagation along the prefix of the path. Since $t_m \\le t_{m+1}$, the condition $\\tau(v_m) \\le t_{m+1}$ will be met, and the influence will correctly propagate to $v_{m+1}$. This holds for all possible time-respecting paths.\n\nAfter iterating through all contacts in $C_{\\text{sorted}}$, the array or map of $\\tau$ values will hold the earliest possible influence time for every node reachable from $s$.\n\nThe final step is to compute the desired quantity, $|R(T)|$. The set $R(T)$ consists of all nodes $v \\neq s$ that are temporally reachable within the window $[0, T]$. A node $v$ is reachable within this window if and only if its earliest influence time is less than or equal to $T$. Therefore, we count the number of nodes $v$ that satisfy the two conditions: $v \\neq s$ and $\\tau(v) \\le T$.\n\nThe complete algorithm is as follows:\n\n1.  Gather the set of all unique agent identifiers, $V$, from the contact list.\n2.  Initialize a data structure, `influence_times`, to store $\\tau(v)$ for each $v \\in V$. Set $\\tau(s) = 0$ and $\\tau(v) = \\infty$ for all $v \\in V, v \\neq s$.\n3.  Sort the contact list $C$ by timestamp in non-decreasing order to get $C_{\\text{sorted}}$.\n4.  For each contact $(i, j, t)$ in $C_{\\text{sorted}}$:\n    a. Check if the source agent $i$ could have been influenced by time $t$: $\\tau(i) \\le t$.\n    b. If so, update the target agent's earliest influence time: $\\tau(j) = \\min(\\tau(j), t)$.\n5.  After the loop, initialize a counter `reachable_count` to $0$.\n6.  Iterate through all agents $v \\in V$:\n    a. If $v \\neq s$ and $\\tau(v) \\le T$, increment `reachable_count`.\n7.  The final result is `reachable_count`.\n\nThis algorithm correctly derives the solution from the specified first principles and will be implemented.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the temporal reachability problem for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"contacts\": [(0, 1, 1), (1, 2, 2), (2, 3, 3)],\n            \"s\": 0,\n            \"T\": 3\n        },\n        {\n            \"contacts\": [(0, 1, 2), (1, 2, 2)],\n            \"s\": 0,\n            \"T\": 2\n        },\n        {\n            \"contacts\": [(0, 1, 1), (1, 2, 1), (2, 3, 1)],\n            \"s\": 0,\n            \"T\": 1\n        },\n        {\n            \"contacts\": [(1, 0, 1)],\n            \"s\": 0,\n            \"T\": 10\n        },\n        {\n            \"contacts\": [(0, 1, 5), (1, 0, 6), (1, 2, 7), (0, 2, 10)],\n            \"s\": 0,\n            \"T\": 8\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        contacts = case[\"contacts\"]\n        s = case[\"s\"]\n        T = case[\"T\"]\n\n        # 1. Gather all unique nodes from the contact list.\n        nodes = set()\n        for i, j, t in contacts:\n            nodes.add(i)\n            nodes.add(j)\n        if not nodes:\n            results.append(0)\n            continue\n        nodes.add(s) # Ensure the seed node is included.\n\n        # 2. Initialize influence times.\n        # tau[v] stores the earliest time agent v can be influenced.\n        # Initialize seed at time 0, others at infinity.\n        influence_times = {node: np.inf for node in nodes}\n        influence_times[s] = 0\n\n        # 3. Sort contacts by timestamp in non-decreasing order.\n        # The key to this algorithm, processing events as they occur in time.\n        sorted_contacts = sorted(contacts, key=lambda x: x[2])\n\n        # 4. Iterate through sorted contacts and propagate influence.\n        for i, j, t in sorted_contacts:\n            # Causal condition: If agent 'i' was influenced at or before time 't',\n            # influence can propagate.\n            # We use .get() to be safe, though 'i' should always be in the dict.\n            if influence_times.get(i, np.inf) <= t:\n                # Update agent 'j's influence time if this path is faster.\n                current_time_j = influence_times.get(j, np.inf)\n                influence_times[j] = min(current_time_j, t)\n        \n        # 5. Count reachable nodes within the time window T.\n        # According to the problem, the set R(T) excludes the seed s.\n        reachable_count = 0\n        for node, time in influence_times.items():\n            if node != s and time <= T:\n                reachable_count += 1\n        \n        results.append(reachable_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}