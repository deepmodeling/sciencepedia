{
    "hands_on_practices": [
        {
            "introduction": "The Pólya urn is a canonical model for understanding processes with reinforcement, where past events increase the probability of similar future events. This exercise  asks you to formalize this path dependence by identifying the minimal information from the past—the number of balls of each color—that is sufficient to predict the future. By constructing this augmented state space, you will practice a fundamental technique for transforming a seemingly non-Markovian process into a tractable Markov chain.",
            "id": "4136085",
            "problem": "Consider a classical Pólya urn with two colors, red and blue. At time $t=0$, the urn contains $r_{0} \\in \\mathbb{N}$ red balls and $b_{0} \\in \\mathbb{N}$ blue balls. At each discrete time step $t \\mapsto t+1$, one ball is drawn uniformly at random from the urn, its color is observed, the ball is returned to the urn, and one additional ball of the same color is added. Let $X_{t} \\in \\{\\text{red}, \\text{blue}\\}$ denote the color drawn at time $t$, and let $R_{t}$ and $B_{t}$ denote the number of red and blue balls in the urn immediately before the draw at time $t$, respectively.\n\nYou are to formalize the path dependence and historical contingency of this process using the Markov property. Starting from the standard definition of the Markov property and the sampling rule of the urn, determine a minimal sufficient augmented state process $S_{t} = \\varphi\\!\\left(X_{0}, X_{1}, \\dots, X_{t}\\right)$ such that $\\{S_{t}\\}_{t \\ge 0}$ is a time-homogeneous Markov chain whose one-step transition probabilities do not depend explicitly on $t$. Justify minimality in terms of the smallest information from the history that renders the dynamics time-homogeneous and Markov.\n\nThen, derive the exact one-step transition probabilities on this augmented state: for a generic state $(r,b)$ with $r \\in \\mathbb{N}$ and $b \\in \\mathbb{N}$, write the transition probabilities to the two possible next states, namely $(r+1,b)$ and $(r,b+1)$, as exact algebraic expressions in $r$ and $b$.\n\nYour final answer must be a single row matrix with two entries corresponding to these two transition probabilities, in this order. Do not approximate any quantities.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It describes the classical Pólya urn model, a fundamental stochastic process used to illustrate concepts of reinforcement and path dependence. All necessary information is provided to formalize the process and derive its transition probabilities. Therefore, the problem is valid.\n\nThe core of the problem is to formalize the path-dependent nature of the Pólya urn process within the framework of Markov chains. Path dependence implies that the history of events influences future outcomes. A Markov process, by contrast, is memoryless: its future evolution depends only on its current state, not the path taken to reach it. The resolution lies in defining a state variable that completely encapsulates all relevant information from the past.\n\nLet the state of the system at time $t \\geq 0$ be defined by the pair $S_t = (R_t, B_t)$, where $R_t$ and $B_t$ are the number of red and blue balls, respectively, in the urn immediately before the draw at time $t$. The initial state is $S_0 = (r_0, b_0)$.\n\nThe state $S_t$ is a function of the history of draws, $\\{X_0, X_1, \\dots, X_{t-1}\\}$. Specifically, for $t \\ge 1$, the number of red balls $R_t$ is the initial count $r_0$ plus the number of times a red ball was drawn in the preceding $t$ steps. A similar logic applies to the blue balls. Formally:\n$$R_t = r_0 + \\sum_{i=0}^{t-1} \\mathbb{I}(X_i = \\text{red})$$\n$$B_t = b_0 + \\sum_{i=0}^{t-1} \\mathbb{I}(X_i = \\text{blue})$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The state $S_t=(R_t, B_t)$ is the \"augmented state\" that accumulates historical information. The fact that different sequences of draws (paths) can lead to different states $(R_t, B_t)$ is the essence of historical contingency in this model. The total number of balls at time $t$ is $N_t = R_t + B_t = r_0 + b_0 + t$.\n\nWe must now show that the process $\\{S_t\\}_{t \\geq 0}$ is a time-homogeneous Markov chain and that $S_t$ is a minimal sufficient state.\n\n**1. Markov Property and Time-Homogeneity**\n\nTo demonstrate the Markov property, we must show that the conditional probability of the next state $S_{t+1}$ depends only on the current state $S_t$, not on any past states $S_{t-1}, \\dots, S_0$.\nThe state transition from $S_t$ to $S_{t+1}$ occurs via the draw at time $t$. Let the current state be $S_t = (R_t, B_t) = (r, b)$. The total number of balls is $r+b$.\nThe probability of drawing a red ball at time $t$ is:\n$$P(X_t = \\text{red} | S_t=(r,b), S_{t-1}, \\dots, S_0) = \\frac{r}{r+b}$$\nThis probability depends only on the components of the current state $S_t=(r,b)$. If a red ball is drawn, the urn's composition for the next step becomes $(r+1, b)$. Therefore, the next state is $S_{t+1}=(r+1, b)$.\nThe probability of drawing a blue ball is:\n$$P(X_t = \\text{blue} | S_t=(r,b), S_{t-1}, \\dots, S_0) = \\frac{b}{r+b}$$\nwhich also depends only on $S_t$. If a blue ball is drawn, the next state is $S_{t+1}=(r, b+1)$.\n\nThe one-step transition probabilities from a state $(r,b)$ are:\n$$P(S_{t+1}=(r+1, b) | S_t=(r,b)) = \\frac{r}{r+b}$$\n$$P(S_{t+1}=(r, b+1) | S_t=(r,b)) = \\frac{b}{r+b}$$\nSince these probabilities depend only on the current state $(r,b)$ and not on the time index $t$, the process $\\{S_t\\}_{t \\geq 0}$ is a time-homogeneous Markov chain.\n\n**2. Minimality**\n\nTo justify that $S_t=(R_t, B_t)$ is a minimal sufficient state, we must show that no smaller set of information can define a time-homogeneous Markov process. The transition probabilities fundamentally depend on the ratio of red to total balls, $R_t/(R_t+B_t)$, and blue to total balls, $B_t/(R_t+B_t)$. To compute these ratios, one needs to know both $R_t$ and $B_t$.\n- If the state were defined only by $R_t$, we could not compute the denominator $R_t+B_t$ without knowing $t$ or $B_t$.\n- If the state were defined only by the total number of balls $N_t = R_t+B_t = r_0+b_0+t$, which is equivalent to knowing time $t$, we could not compute the numerators $R_t$ or $B_t$.\n- If the state were the proportion of red balls, $P_t=R_t/(R_t+B_t)$, a transition to the next proportion would depend on the total number of balls $N_t$, making the process time-inhomogeneous since $N_t$ changes deterministically with time.\nThus, the pair $(R_t, B_t)$ is the minimal representation of the history necessary to specify the transition probabilities without explicit reference to time $t$.\n\n**3. Derivation of Transition Probabilities**\n\nNow, we derive the requested transition probabilities for a generic state $(r,b)$, where $r \\in \\mathbb{N}$ and $b \\in \\mathbb{N}$. The urn contains $r$ red balls and $b$ blue balls, for a total of $r+b$ balls.\n\nThe first possible next state is $(r+1, b)$. This state is reached if and only if a red ball is drawn. The probability of this event is the number of red balls divided by the total number of balls.\n$$P((r,b) \\to (r+1,b)) = \\frac{r}{r+b}$$\n\nThe second possible next state is $(r, b+1)$. This state is reached if and only if a blue ball is drawn. The probability of this event is the number of blue balls divided by the total number of balls.\n$$P((r,b) \\to (r,b+1)) = \\frac{b}{r+b}$$\n\nThese are the only two possible one-step transitions from state $(r,b)$. Their sum is $\\frac{r}{r+b} + \\frac{b}{r+b} = \\frac{r+b}{r+b} = 1$, as required for a complete probability distribution over successor states.\n\nThe final answer requires these two probabilities in a row matrix.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{r}{r+b}  \\frac{b}{r+b} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Path dependence is often characterized by \"lock-in,\" where a system settles into one of several possible stable states. This practice  explores this phenomenon using the classic double-well potential, a model for everything from ecological regimes to market standards. You will use Kramers' rate theory to calculate the expected time for a noise-driven escape from one basin of attraction to another, providing a quantitative link between microscopic fluctuations and macroscopic historical contingency.",
            "id": "4136056",
            "problem": "Consider an overdamped agent-based aggregate represented at the mesoscopic scale by a single state variable $x_t \\in \\mathbb{R}$ evolving under a double-well potential. The coarse-grained dynamics are modeled by the overdamped Langevin Stochastic Differential Equation (SDE), defined here as\n$$\ndx_t \\;=\\; -V'(x_t)\\,dt \\;+\\; \\sqrt{2\\,\\varepsilon}\\,dW_t,\n$$\nwhere $W_t$ is a standard Wiener process, $\\varepsilon > 0$ is a small, dimensionless noise intensity, and $V(x)$ is the potential energy landscape. The potential is\n$$\nV(x) \\;=\\; \\frac{\\alpha}{4}\\,x^{4} \\;-\\; \\frac{\\beta}{2}\\,x^{2},\n$$\nwith parameters $\\alpha = 1$ and $\\beta = 3$. Time $t$ is measured in seconds. Assume the system starts in the left well near the local minimum at $x = -\\sqrt{\\beta/\\alpha}$ and that $\\varepsilon = 0.2$. In the small-noise regime, escapes from one well to the other are rare events whose expected time scales are governed by energy barriers and noise-driven activation.\n\nStarting from fundamental definitions of gradient flow, the potential barrier between metastable states, and the quasi-stationary distribution for the overdamped Fokker–Planck dynamics, derive the energy barrier $\\Delta V$ from first principles and then use the classic Kramers’ rate in the overdamped limit to estimate the expected escape time from the left well over the central saddle. Connect the computation to the concept of path dependence and historical contingency by explaining why rare early escapes can lock the system into a different basin for long horizons.\n\nCompute the numerical value of the expected escape time and express your final answer in seconds. Round your final numerical value to four significant figures.",
            "solution": "The problem is subjected to validation.\n\n**Step 1: Extract Givens**\n- **SDE:** $dx_t = -V'(x_t)\\,dt + \\sqrt{2\\,\\varepsilon}\\,dW_t$\n- **State variable:** $x_t \\in \\mathbb{R}$\n- **Noise term:** $W_t$ is a standard Wiener process\n- **Noise intensity:** $\\varepsilon = 0.2$ (dimensionless)\n- **Potential function:** $V(x) = \\frac{\\alpha}{4}\\,x^{4} - \\frac{\\beta}{2}\\,x^{2}$\n- **Potential parameters:** $\\alpha = 1$, $\\beta = 3$\n- **Time unit:** seconds\n- **Initial condition:** The system starts in the left well near the local minimum $x = -\\sqrt{\\beta/\\alpha}$.\n- **Objective:** Derive the energy barrier $\\Delta V$, use Kramers' rate to estimate the expected escape time $\\tau$, connect the result to path dependence and historical contingency, and compute the numerical value of $\\tau$ rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the established theory of stochastic processes, specifically the application of the Langevin equation and Kramers' escape rate theory to a bistable system. The double-well potential is a canonical model in physics, chemistry, and complex systems for studying noise-induced transitions. All concepts are standard and scientifically sound.\n- **Well-Posed:** The problem is well-posed. All necessary parameters ($\\alpha$, $\\beta$, $\\varepsilon$) and the functional form of the potential $V(x)$ are provided. The objective is clearly stated, and a unique, meaningful solution for the expected escape time can be derived from the given information.\n- **Objective:** The problem is stated in precise, objective mathematical and physical language, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It satisfies all criteria for being scientifically grounded, well-posed, and objective. There are no identifiable flaws. A full solution will be provided.\n\n**Derivation and Solution**\n\nThe dynamics of the system are governed by the gradient flow on the potential energy landscape $V(x)$, perturbed by a stochastic term. The potential is given by:\n$$\nV(x) = \\frac{\\alpha}{4}x^{4} - \\frac{\\beta}{2}x^{2}\n$$\nThe deterministic part of the dynamics is driven by the force $F(x) = -V'(x)$. The equilibrium points of the system correspond to the locations where this force is zero, i.e., where the potential gradient is zero:\n$$\nV'(x) = \\frac{d}{dx}\\left(\\frac{\\alpha}{4}x^{4} - \\frac{\\beta}{2}x^{2}\\right) = \\alpha x^{3} - \\beta x = 0\n$$\nFactoring the expression gives $x(\\alpha x^2 - \\beta) = 0$, which yields three equilibrium points:\n$x_1 = 0$\n$x_2 = \\sqrt{\\frac{\\beta}{\\alpha}}$\n$x_3 = -\\sqrt{\\frac{\\beta}{\\alpha}}$\n\nTo determine the stability of these points, we examine the second derivative of the potential, $V''(x)$:\n$$\nV''(x) = \\frac{d}{dx}(\\alpha x^{3} - \\beta x) = 3\\alpha x^{2} - \\beta\n$$\n- At $x_1 = 0$: $V''(0) = -\\beta$. Since $\\beta = 3 > 0$, $V''(0)  0$. This indicates that $x=0$ is a local maximum, corresponding to an unstable equilibrium (a saddle point or potential barrier peak).\n- At $x_{2,3} = \\pm\\sqrt{\\frac{\\beta}{\\alpha}}$: $V''\\left(\\pm\\sqrt{\\frac{\\beta}{\\alpha}}\\right) = 3\\alpha\\left(\\frac{\\beta}{\\alpha}\\right) - \\beta = 3\\beta - \\beta = 2\\beta$. Since $\\beta=3>0$, $V''(\\pm\\sqrt{\\beta/\\alpha}) > 0$. This indicates that these two points are local minima, corresponding to stable equilibrium states (the bottoms of the potential wells).\n\nThe problem states the system starts in the left well, which is the basin of attraction for the stable minimum at $x_{\\text{min}} = -\\sqrt{\\beta/\\alpha}$. For the system to escape to the right well (around $x=+\\sqrt{\\beta/\\alpha}$), it must pass over the potential barrier located at the saddle point $x_{\\text{saddle}} = 0$.\n\nThe energy barrier, $\\Delta V$, is the difference in potential energy between the saddle point and the initial well's minimum:\n$$\n\\Delta V = V(x_{\\text{saddle}}) - V(x_{\\text{min}}) = V(0) - V\\left(-\\sqrt{\\frac{\\beta}{\\alpha}}\\right)\n$$\nWe calculate the potential at these points:\n$V(0) = \\frac{\\alpha}{4}(0)^{4} - \\frac{\\beta}{2}(0)^{2} = 0$\n$V\\left(-\\sqrt{\\frac{\\beta}{\\alpha}}\\right) = \\frac{\\alpha}{4}\\left(-\\sqrt{\\frac{\\beta}{\\alpha}}\\right)^4 - \\frac{\\beta}{2}\\left(-\\sqrt{\\frac{\\beta}{\\alpha}}\\right)^2 = \\frac{\\alpha}{4}\\left(\\frac{\\beta^2}{\\alpha^2}\\right) - \\frac{\\beta}{2}\\left(\\frac{\\beta}{\\alpha}\\right) = \\frac{\\beta^2}{4\\alpha} - \\frac{\\beta^2}{2\\alpha} = -\\frac{\\beta^2}{4\\alpha}$\nTherefore, the energy barrier is:\n$$\n\\Delta V = 0 - \\left(-\\frac{\\beta^2}{4\\alpha}\\right) = \\frac{\\beta^2}{4\\alpha}\n$$\nThe expected escape time $\\tau$ can be estimated using Kramers' rate formula for the overdamped limit. The escape rate $k$ (the probability per unit time of an escape event) is given by:\n$$\nk = \\frac{\\sqrt{V''(x_{\\text{min}}) |V''(x_{\\text{saddle}})|}}{2\\pi} \\exp\\left(-\\frac{\\Delta V}{\\varepsilon}\\right)\n$$\nHere, we have identified the noise term $\\varepsilon$ in the given SDE with the thermal energy scale $k_B T$ in the standard physics formulation, and the friction coefficient is implicitly $\\gamma=1$.\nWe have:\n$V''(x_{\\text{min}}) = V''(-\\sqrt{\\beta/\\alpha}) = 2\\beta$\n$V''(x_{\\text{saddle}}) = V''(0) = -\\beta$, so $|V''(x_{\\text{saddle}})| = \\beta$.\n\nSubstituting these into the rate formula:\n$$\nk = \\frac{\\sqrt{(2\\beta)(\\beta)}}{2\\pi} \\exp\\left(-\\frac{\\beta^2}{4\\alpha\\varepsilon}\\right) = \\frac{\\sqrt{2\\beta^2}}{2\\pi} \\exp\\left(-\\frac{\\beta^2}{4\\alpha\\varepsilon}\\right) = \\frac{\\beta\\sqrt{2}}{2\\pi} \\exp\\left(-\\frac{\\beta^2}{4\\alpha\\varepsilon}\\right)\n$$\nThe expected escape time $\\tau$ is the reciprocal of the rate $k$:\n$$\n\\tau = \\frac{1}{k} = \\frac{2\\pi}{\\beta\\sqrt{2}} \\exp\\left(\\frac{\\beta^2}{4\\alpha\\varepsilon}\\right) = \\frac{\\pi\\sqrt{2}}{\\beta} \\exp\\left(\\frac{\\beta^2}{4\\alpha\\varepsilon}\\right)\n$$\nThis expression provides the mean first passage time from the left well to the right well.\n\n**Connection to Path Dependence and Historical Contingency**\nThis model provides a clear illustration of path dependence and historical contingency.\n1.  **Path Dependence:** The long-term state of the system is dependent on its history. The initial condition places the system in the left well. Due to the energy barrier $\\Delta V$ and the small noise intensity $\\varepsilon$, the system will remain trapped in this well for a long time, on average for a duration of $\\tau$. The system's state at any time $t \\ll \\tau$ is overwhelmingly likely to be in the left well. Thus, the future state exhibits a strong dependence on its past trajectory.\n2.  **Historical Contingency:** The transition from the left well to the right well is a rare, stochastic event. The exact moment of escape is unpredictable; it is a \"contingent\" event. If, by chance, a sufficiently large series of noise fluctuations compounds to push the system over the barrier at an early time (a \"historical accident\"), the system's fate is dramatically altered. It becomes \"locked-in\" to the basin of attraction of the right well, where it will again remain for a long time (on average, another period of $\\tau$). The long-term history of the system is therefore not pre-ordained but is contingent upon the random occurrence of this rare escape event. An observer encountering the system long after such a switch might be unable to infer its original state, as the system now appears to naturally belong to the right well.\n\n**Numerical Calculation**\nWe substitute the given values $\\alpha = 1$, $\\beta = 3$, and $\\varepsilon = 0.2$ into the expression for $\\tau$.\nFirst, we compute the argument of the exponential:\n$$\n\\frac{\\Delta V}{\\varepsilon} = \\frac{\\beta^2}{4\\alpha\\varepsilon} = \\frac{3^2}{4(1)(0.2)} = \\frac{9}{0.8} = 11.25\n$$\nNow, we compute the pre-factor:\n$$\n\\frac{\\pi\\sqrt{2}}{\\beta} = \\frac{\\pi\\sqrt{2}}{3}\n$$\nPutting it all together:\n$$\n\\tau = \\frac{\\pi\\sqrt{2}}{3} \\exp(11.25)\n$$\nWe now compute the numerical value:\n$$\n\\tau \\approx \\frac{3.14159 \\times 1.41421}{3} \\times \\exp(11.25) \\approx \\frac{4.44288}{3} \\times 76878.56 \\approx 1.48096 \\times 76878.56\n$$\n$$\n\\tau \\approx 113851.34\n$$\nThe problem requires rounding the final answer to four significant figures.\n$$\n\\tau \\approx 113900 \\text{ s}\n$$\nThis can be expressed in scientific notation as $1.139 \\times 10^5$ seconds.",
            "answer": "$$\\boxed{1.139 \\times 10^{5}}$$"
        },
        {
            "introduction": "In many complex systems, not only does history matter, but the specific sequence and timing of events are crucial. This computational exercise  tasks you with building a model of complex contagion where an agent's memory and nonlinear reinforcement create sensitivity to the temporal pattern of exposures. By implementing different memory kernels and quantifying the effect of event ordering, you will gain hands-on experience in modeling and measuring historical contingency in diffusion processes.",
            "id": "4136121",
            "problem": "Consider a discrete-time diffusion model for Complex Adaptive Systems (CAS) in which a single focal agent accumulates exposures from its neighbors over time. Let time be indexed by integer steps $t \\in \\{1,2,\\dots,T\\}$. At each step $t$, the focal agent receives an integer number of exposures $E(t) \\geq 0$ from interactions with already adopted neighbors. The agent has not adopted initially and adoption is modeled as a first-arrival event driven by an inhomogeneous hazard process.\n\nFundamental base definitions:\n- The instantaneous discrete-time hazard is a nonnegative function $\\lambda(t) \\geq 0$, interpreted as the rate at which the agent adopts at time $t$ conditional on not yet having adopted.\n- Under the standard survival analysis assumption that events arrive according to a conditionally Poisson process, the survival probability up to time $T$ is\n$$S(T) = \\exp\\left(-\\sum_{t=1}^{T} \\lambda(t)\\right).$$\n- The adoption probability by time $T$ is $P(T) = 1 - S(T)$.\n- Complex contagion with reinforcement is represented by a nonlinear mapping of the memory-weighted exposure intensity. Let the memory kernel be a nonnegative function $K(\\tau)$ over integer lags $\\tau \\in \\{1,2,\\dots\\}$ that weights past exposures by their age $\\tau$. Define the memory-weighted exposure at time $t$ as\n$$S_{\\text{mem}}(t) = \\sum_{\\tau=1}^{t} K(\\tau)\\, E(t - \\tau).$$\n- The reinforcement function is $f(s) = s^{\\alpha}$ with $\\alpha \\geq 1$, producing the hazard\n$$\\lambda(t) = \\beta\\, \\big(S_{\\text{mem}}(t)\\big)^{\\alpha},$$\nwhere $\\beta > 0$ is a coupling parameter.\n\nPath dependence and historical contingency in this setting emerge because the kernel $K(\\tau)$ and the nonlinear reinforcement exponent $\\alpha$ can cause the adoption probability $P(T)$ to depend on the temporal arrangement (order) of exposures, even if the total number of exposures is the same. To quantify this, define the Path Dependence Index (PDI) as the difference in adoption probabilities under two exposure sequences with the same total exposure count but opposite temporal order:\n$$\\text{PDI} = P_{\\text{front}}(T) - P_{\\text{back}}(T),$$\nwhere $P_{\\text{front}}(T)$ uses a sequence with exposures concentrated earlier (front-loaded) and $P_{\\text{back}}(T)$ uses the same number of exposures concentrated later (back-loaded).\n\nImplement a program that:\n1. Computes $P(T)$ deterministically from $E(t)$, $K(\\tau)$, $\\alpha$, and $\\beta$ using the above base definitions.\n2. Computes $\\text{PDI}$ for a specified test suite of parameter sets.\n\nMemory kernels to be investigated:\n- Exponential kernel: $K(\\tau) = \\exp(-\\kappa \\tau)$ with $\\kappa > 0$.\n- Power-law kernel: $K(\\tau) = (\\tau + 1)^{-\\gamma}$ with $\\gamma > 0$.\n- Uniform finite-memory kernel: $K(\\tau) = 1$ for $\\tau \\leq M$, and $K(\\tau) = 0$ for $\\tau > M$, with integer memory length $M \\geq 1$.\n\nTest suite specification:\nFor each case, use the discrete horizon $t = 1,2,\\dots,T$ with integer exposures $E(t)$ specified by indices where $E(t) = 1$ and $E(t) = 0$ otherwise. All answers should be returned as real numbers without units. The required final output format is a single line containing the $\\text{PDI}$ for each case as a comma-separated list enclosed in square brackets (e.g., \"[x1,x2,x3,x4,x5]\").\n\n- Case 1 (happy path; exponential memory, complex reinforcement):\n  - Parameters: $T = 20$, $\\beta = 0.2$, $\\alpha = 2$, exponential kernel with $\\kappa = 0.3$.\n  - Front-loaded exposures: $E(t) = 1$ for $t \\in \\{2,3,4,5\\}$.\n  - Back-loaded exposures: $E(t) = 1$ for $t \\in \\{16,17,18,19\\}$.\n\n- Case 2 (heavy-tailed memory, complex reinforcement):\n  - Parameters: $T = 20$, $\\beta = 0.2$, $\\alpha = 2$, power-law kernel with $\\gamma = 0.5$.\n  - Front-loaded exposures: $E(t) = 1$ for $t \\in \\{2,3,4,5\\}$.\n  - Back-loaded exposures: $E(t) = 1$ for $t \\in \\{16,17,18,19\\}$.\n\n- Case 3 (no reinforcement; uniform finite memory; designed to be path independent):\n  - Parameters: $T = 30$, $\\beta = 0.2$, $\\alpha = 1$, uniform kernel with $M = 5$.\n  - Front-loaded exposures (interior to avoid boundary effects): $E(t) = 1$ for $t \\in \\{10,11,12,13,14\\}$.\n  - Back-loaded exposures (interior): $E(t) = 1$ for $t \\in \\{15,16,17,18,19\\}$.\n\n- Case 4 (boundary case; zero exposure):\n  - Parameters: $T = 20$, $\\beta = 0.2$, $\\alpha = 2$, exponential kernel with $\\kappa = 0.3$.\n  - Front-loaded exposures: none (no $t$ with $E(t) = 1$).\n  - Back-loaded exposures: none (no $t$ with $E(t) = 1$).\n\n- Case 5 (finite memory, stronger reinforcement, pronounced path dependence):\n  - Parameters: $T = 20$, $\\beta = 0.5$, $\\alpha = 3$, uniform kernel with $M = 3$.\n  - Front-loaded exposures: $E(t) = 1$ for $t \\in \\{2,3,4\\}$.\n  - Back-loaded exposures: $E(t) = 1$ for $t \\in \\{17,18,19\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above, i.e., \"[PDI1,PDI2,PDI3,PDI4,PDI5]\". Each element must be a real-valued float.",
            "solution": "The problem requires the computation of a Path Dependence Index (PDI) for a discrete-time diffusion model. The PDI quantifies how the temporal ordering of exposures affects the final adoption probability of an agent. The solution involves implementing the provided mathematical model and applying it to a series of specified test cases.\n\nThe model is defined by a set of core equations. An agent's adoption is treated as a first-arrival event governed by an inhomogeneous discrete-time hazard process. The probability of an agent surviving (i.e., not adopting) up to time $T$ is given by:\n$$S(T) = \\exp\\left(-\\sum_{t=1}^{T} \\lambda(t)\\right)$$\nwhere $\\lambda(t)$ is the instantaneous hazard at time $t$. The probability of adoption by time $T$ is then $P(T) = 1 - S(T)$.\n\nThe hazard $\\lambda(t)$ is a function of the accumulated history of exposures, $E(t')$, for $t'  t$. This history is captured by the memory-weighted exposure, $S_{\\text{mem}}(t)$:\n$$S_{\\text{mem}}(t) = \\sum_{\\tau=1}^{t} K(\\tau)\\, E(t - \\tau)$$\nHere, $K(\\tau)$ is a memory kernel that discounts past exposures based on their age $\\tau$. The problem assumes that exposures $E(t)$ are zero for any time $t \\leq 0$. The relationship between memory-weighted exposure and the hazard incorporates nonlinear reinforcement, modeled as:\n$$\\lambda(t) = \\beta\\, \\big(S_{\\text{mem}}(t)\\big)^{\\alpha}$$\nwhere $\\beta > 0$ is a coupling constant and $\\alpha \\geq 1$ is the reinforcement exponent. When $\\alpha > 1$, the process exhibits complex contagion, where multiple exposures have a synergistic effect greater than the sum of their individual effects.\n\nThe Path Dependence Index is defined as the difference in adoption probabilities between a \"front-loaded\" exposure sequence and a \"back-loaded\" one, holding the total number of exposures constant:\n$$\\text{PDI} = P_{\\text{front}}(T) - P_{\\text{back}}(T)$$\nA non-zero PDI indicates that the timing of exposures, not just their total count, influences the outcome, which is a hallmark of historical contingency.\n\nThe computational procedure to solve the problem is as follows:\n\n1.  **Define Kernel Functions**: Three memory kernels are specified:\n    -   Exponential: $K(\\tau) = \\exp(-\\kappa \\tau)$ for $\\kappa > 0$.\n    -   Power-law: $K(\\tau) = (\\tau + 1)^{-\\gamma}$ for $\\gamma > 0$.\n    -   Uniform finite-memory: $K(\\tau) = 1$ for $\\tau \\leq M$ and $K(\\tau) = 0$ for $\\tau > M$, for an integer memory length $M \\geq 1$.\n    These are implemented as functions that take the lag $\\tau$ as input.\n\n2.  **Implement the Adoption Probability Calculation**: A core function, say `calculate_p`, is created to compute $P(T)$ given the parameters $T$, $\\beta$, $\\alpha$, a chosen kernel function, and an exposure sequence represented as an array $E$. The exposure array is indexed from $0$ to $T$, with $E[0]$ implicitly set to $0$.\n\n3.  **Iterate Through Time**: The function iterates through discrete time steps $t$ from $1$ to $T$.\n\n4.  **Compute Memory-Weighted Exposure**: At each time $t$, $S_{\\text{mem}}(t)$ is calculated by summing the product of kernel weights $K(\\tau)$ and past exposures $E(t-\\tau)$ for lags $\\tau$ from $1$ to $t$.\n\n5.  **Compute Hazard**: The hazard $\\lambda(t)$ is then computed using the reinforcement function: $\\lambda(t) = \\beta (S_{\\text{mem}}(t))^{\\alpha}$.\n\n6.  **Accumulate Total Hazard**: The instantaneous hazards $\\lambda(t)$ are summed over all time steps from $t=1$ to $T$ to find the cumulative hazard, $\\Lambda(T) = \\sum_{t=1}^{T} \\lambda(t)$.\n\n7.  **Calculate Final Probability**: The adoption probability is computed as $P(T) = 1 - \\exp(-\\Lambda(T))$.\n\n8.  **Process Test Cases**: For each of the five test cases provided:\n    a. The parameters ($T, \\beta, \\alpha$, kernel type, and kernel parameters) are set.\n    b. Two exposure arrays, $E_{\\text{front}}$ and $E_{\\text{back}}$, are constructed. These are arrays of size $T+1$ initialized to zeros, with elements corresponding to the specified exposure times set to $1$.\n    c. The `calculate_p` function is called twice: once with $E_{\\text{front}}$ to get $P_{\\text{front}}(T)$ and once with $E_{\\text{back}}$ to get $P_{\\text{back}}(T)$.\n    d. The PDI is calculated as their difference and stored.\n\n9.  **Format Output**: The resulting PDI values for all five cases are collected into a list and formatted into the required output string: `[PDI1,PDI2,PDI3,PDI4,PDI5]`.\n\nThis systematic procedure deterministically computes the required quantities based on the provided theoretical framework. The implementation uses standard numerical libraries for mathematical operations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the path dependence problem for all specified test cases.\n    \"\"\"\n\n    # Kernel function factories\n    def make_exponential_kernel(kappa):\n        \"\"\"Returns the exponential kernel function.\"\"\"\n        def kernel(tau):\n            return np.exp(-kappa * tau)\n        return kernel\n\n    def make_power_law_kernel(gamma):\n        \"\"\"Returns the power-law kernel function.\"\"\"\n        def kernel(tau):\n            return (tau + 1.0) ** (-gamma)\n        return kernel\n\n    def make_uniform_kernel(M):\n        \"\"\"Returns the uniform finite-memory kernel function.\"\"\"\n        def kernel(tau):\n            return 1.0 if tau = M else 0.0\n        return kernel\n\n    def calculate_p(T, beta, alpha, kernel_func, E):\n        \"\"\"\n        Computes the adoption probability P(T).\n\n        Args:\n            T (int): The time horizon.\n            beta (float): The coupling parameter.\n            alpha (float): The reinforcement exponent.\n            kernel_func (callable): The memory kernel function K(tau).\n            E (np.ndarray): The exposure sequence array of size T+1, indexed 0..T.\n        \"\"\"\n        total_hazard = 0.0\n        \n        # Pre-compute kernel values for efficiency\n        kernel_values = np.array([kernel_func(tau) for tau in range(1, T + 1)])\n\n        for t in range(1, T + 1):\n            # Calculate S_mem(t) = sum_{tau=1 to t} K(tau) * E(t - tau)\n            s_mem_t = 0.0\n            # A direct dot product is equivalent and can be faster\n            # The indices are: K(1..t) and E(t-1..0)\n            # kernel_values is 0-indexed for tau=1..T, so K(tau) is kernel_values[tau-1]\n            # E is 0-indexed for t=0..T\n            \n            # Equivalent loop:\n            # for tau in range(1, t + 1):\n            #      s_mem_t += kernel_values[tau - 1] * E[t - tau]\n            \n            # Vectorized calculation of S_mem(t):\n            # E indices from t-1 down to 0\n            relevant_E = E[0:t] \n            # kernel_values indices from 0 to t-1\n            relevant_K = kernel_values[0:t]\n            s_mem_t = np.dot(relevant_E[::-1], relevant_K)\n\n            hazard_t = beta * (s_mem_t ** alpha)\n            total_hazard += hazard_t\n        \n        survival_prob = np.exp(-total_hazard)\n        adoption_prob = 1.0 - survival_prob\n        return adoption_prob\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'T': 20, 'beta': 0.2, 'alpha': 2, 'kernel': 'exp', 'k_params': {'kappa': 0.3},\n            'front_t': [2, 3, 4, 5], 'back_t': [16, 17, 18, 19]\n        },\n        # Case 2\n        {\n            'T': 20, 'beta': 0.2, 'alpha': 2, 'kernel': 'pow', 'k_params': {'gamma': 0.5},\n            'front_t': [2, 3, 4, 5], 'back_t': [16, 17, 18, 19]\n        },\n        # Case 3\n        {\n            'T': 30, 'beta': 0.2, 'alpha': 1, 'kernel': 'uni', 'k_params': {'M': 5},\n            'front_t': [10, 11, 12, 13, 14], 'back_t': [15, 16, 17, 18, 19]\n        },\n        # Case 4\n        {\n            'T': 20, 'beta': 0.2, 'alpha': 2, 'kernel': 'exp', 'k_params': {'kappa': 0.3},\n            'front_t': [], 'back_t': []\n        },\n        # Case 5\n        {\n            'T': 20, 'beta': 0.5, 'alpha': 3, 'kernel': 'uni', 'k_params': {'M': 3},\n            'front_t': [2, 3, 4], 'back_t': [17, 18, 19]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        T = case['T']\n        beta = case['beta']\n        alpha = case['alpha']\n\n        # E vectors are of size T+1 for indices 0...T, with E[0]=0.\n        E_front = np.zeros(T + 1)\n        E_back = np.zeros(T + 1)\n\n        for t_val in case['front_t']:\n            E_front[t_val] = 1.0\n        for t_val in case['back_t']:\n            E_back[t_val] = 1.0\n\n        if case['kernel'] == 'exp':\n            kernel_func = make_exponential_kernel(**case['k_params'])\n        elif case['kernel'] == 'pow':\n            kernel_func = make_power_law_kernel(**case['k_params'])\n        elif case['kernel'] == 'uni':\n            kernel_func = make_uniform_kernel(**case['k_params'])\n        \n        p_front = calculate_p(T, beta, alpha, kernel_func, E_front)\n        p_back = calculate_p(T, beta, alpha, kernel_func, E_back)\n\n        pdi = p_front - p_back\n        results.append(pdi)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}