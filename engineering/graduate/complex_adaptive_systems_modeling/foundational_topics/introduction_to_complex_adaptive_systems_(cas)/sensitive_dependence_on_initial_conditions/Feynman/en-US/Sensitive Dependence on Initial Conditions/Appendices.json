{
    "hands_on_practices": [
        {
            "introduction": "Understanding a complex idea often begins with a simple, canonical model. This practice provides a first-principles derivation of the Largest Lyapunov Exponent (LLE) for the doubling map, one of the simplest systems exhibiting chaotic behavior. By working through this foundational example, you will make the abstract definition of the LLE concrete and tractable, building a clear understanding of how the exponential divergence characteristic of sensitive dependence is formally measured.",
            "id": "4143062",
            "problem": "Consider a stylized microdynamic for an agent’s internal phase on the unit circle, a canonical component in complex adaptive systems modeling. The state space is the unit circle, represented as the quotient of the interval $[0,1)$ with endpoints identified. Let the discrete-time update be the doubling map $f:[0,1)\\to[0,1)$ defined by $f(x)=2x \\bmod 1$. Distances on the circle are measured using the intrinsic metric $d(x,y)=\\min\\{|x-y|,\\,1-|x-y|\\}$. Starting from the core definition of the Largest Lyapunov Exponent (LLE), which for a one-dimensional discrete-time map $f$ along a trajectory $\\{x_{n}\\}_{n\\geq 0}$ is given by\n$$\n\\lambda(x_{0})=\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k=0}^{n-1}\\ln\\big|f'(x_{k})\\big|\\quad\\text{with }x_{k}=f^{k}(x_{0}),\n$$\nderive the LLE for the doubling map. Then, using first-order (linearized) separation dynamics implied by the derivative, deduce Sensitive Dependence on Initial Conditions (SDIC), defined here as: for any sufficiently small initial separation $d_{0}0$ and any tolerance $\\varepsilon\\in(0,\\tfrac{1}{2})$, there exists a finite time $n$ such that $d(x_{n},y_{n})\\geq\\varepsilon$ for two initial conditions $x_{0}$ and $y_{0}$ with $d(x_{0},y_{0})=d_{0}$. Your final answer must be the symbolic value of the LLE. No rounding is required. Express logarithms using the natural logarithm.",
            "solution": "The problem requires the derivation of the Largest Lyapunov Exponent (LLE) for the doubling map and a deduction of Sensitive Dependence on Initial Conditions (SDIC) from this result.\n\nFirst, we calculate the LLE. The system's evolution is governed by the discrete-time map $f:[0,1) \\to [0,1)$ defined as the doubling map, $f(x) = 2x \\pmod 1$. This function can be expressed in a piecewise linear form:\n$$\nf(x) =\n\\begin{cases}\n2x  \\text{if } 0 \\le x  \\frac{1}{2} \\\\\n2x-1  \\text{if } \\frac{1}{2} \\le x  1\n\\end{cases}\n$$\nThe LLE, $\\lambda(x_0)$, for a trajectory $\\{x_k\\}_{k\\geq0}$ originating from an initial condition $x_0$ is defined by the formula:\n$$\n\\lambda(x_0) = \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{k=0}^{n-1} \\ln\\big|f'(x_k)\\big| \\quad \\text{where } x_k = f^k(x_0)\n$$\nTo evaluate this expression, we first need the derivative of the map, $f'(x)$. By differentiating the piecewise components of $f(x)$, we obtain:\n$$\nf'(x) = 2, \\quad \\text{for } x \\in [0,1) \\setminus \\{\\tfrac{1}{2}\\}\n$$\nThe derivative is undefined at the point $x=\\frac{1}{2}$, where the function $f(x)$ has a jump discontinuity in its representation on the interval $[0,1)$. For any point $x_k$ in a trajectory where the derivative is defined, its absolute value is $|f'(x_k)|=2$.\n\nThe LLE formula involves an average over the entire trajectory. For this average to be well-defined, the trajectory must avoid the point of non-differentiability, $x=\\frac{1}{2}$. The set of initial conditions $x_0$ for which the orbit $\\{x_k\\}$ eventually includes the point $\\frac{1}{2}$ is precisely the set of dyadic rationals (numbers of the form $\\frac{p}{2^q}$ for integers $p, q$). This set is countable and has a Lebesgue measure of zero. For almost all initial conditions $x_0 \\in [0,1)$, including all irrational numbers, the trajectory $x_k = f^k(x_0)$ will never land on the point $x=\\frac{1}{2}$.\n\nConsequently, for a typical trajectory, the term $|f'(x_k)|$ in the sum is equal to $2$ for all $k \\ge 0$. We can now substitute this into the definition of the LLE:\n$$\n\\lambda(x_0) = \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{k=0}^{n-1} \\ln(2)\n$$\nThe summation term is a sum of $n$ identical constants:\n$$\n\\sum_{k=0}^{n-1} \\ln(2) = n \\ln(2)\n$$\nPlacing this result back into the limit expression gives the value of the LLE:\n$$\n\\lambda(x_0) = \\lim_{n\\to\\infty} \\frac{1}{n} (n \\ln(2)) = \\ln(2)\n$$\nThus, the LLE for the doubling map is $\\lambda = \\ln(2)$ for almost all initial conditions $x_0$.\n\nNext, we deduce SDIC from the calculated LLE. SDIC is the property that initially close trajectories diverge over time. The LLE quantifies the average exponential rate of this divergence. Consider two nearby initial conditions, $x_0$ and $y_0 = x_0 + \\delta_0$, where $\\delta_0$ represents a very small initial separation. The separation after one iteration is $\\delta_1 = f(y_0) - f(x_0)$. Using a first-order Taylor expansion (linearization) for small $\\delta_0$, we have $f(x_0 + \\delta_0) \\approx f(x_0) + f'(x_0)\\delta_0$. This leads to the linearized separation dynamics $\\delta_1 \\approx f'(x_0)\\delta_0$.\n\nAfter $n$ iterations, the separation $\\delta_n = y_n - x_n$ is approximately:\n$$\n\\delta_n \\approx \\left(\\prod_{k=0}^{n-1} f'(x_k)\\right) \\delta_0\n$$\nTaking the absolute value, we get the magnitude of the separation:\n$$\n|\\delta_n| \\approx |\\delta_0| \\prod_{k=0}^{n-1} |f'(x_k)|\n$$\nBy taking the natural logarithm, dividing by $n$, and considering the limit as $n \\to \\infty$, we recover the definition of the LLE:\n$$\n\\lim_{n\\to\\infty} \\frac{1}{n} \\ln\\left(\\frac{|\\delta_n|}{|\\delta_0|}\\right) = \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{k=0}^{n-1} \\ln|f'(x_k)| = \\lambda\n$$\nThis limiting relationship implies that for large $n$, the separation magnitude evolves according to $|\\delta_n| \\approx |\\delta_0| \\exp(\\lambda n)$. Since we found $\\lambda = \\ln(2) > 0$, the separation grows exponentially:\n$$\n|\\delta_n| \\approx |\\delta_0| \\exp(n \\ln(2)) = |\\delta_0| 2^n\n$$\nA positive LLE indicates exponential divergence of nearby trajectories, which is the defining characteristic of SDIC. The problem defines SDIC as: for any sufficiently small initial separation $d_0 = d(x_0, y_0) > 0$ and any tolerance $\\varepsilon \\in (0, \\frac{1}{2})$, there exists a finite time $n$ such that $d(x_n, y_n) \\ge \\varepsilon$.\n\nThe exponential growth $|\\delta_n| \\approx d_0 2^n$ (where $d_0$ is the initial distance) ensures that any microscopic separation $d_0$ is amplified to a macroscopic scale $\\varepsilon$ in a finite time. We can estimate this time $n$ by solving for when the linearized separation reaches $\\varepsilon$: $d_0 2^n = \\varepsilon$, which yields $n = \\log_2(\\varepsilon/d_0)$. As $d_0$ is small, $\\varepsilon/d_0 > 1$, and $n$ is a finite positive number. While the linear approximation is only valid for small separations, the exponential tendency ensures that the separation will grow until it is no longer small and becomes a significant fraction of the state space diameter. Therefore, the positive LLE of $\\lambda = \\ln(2)$ directly implies SDIC.",
            "answer": "$$\n\\boxed{\\ln(2)}\n$$"
        },
        {
            "introduction": "While analytical models are instructive, measuring chaos in most complex systems requires robust numerical methods. This exercise bridges the gap between the theoretical definition of the Lyapunov exponent and its practical computation for systems described by differential equations. You will explore the derivation of the standard algorithm, which uses periodic renormalization to overcome the numerical instabilities inherent in simulating chaotic dynamics, developing a crucial understanding of how abstract concepts are translated into stable and accurate computational tools.",
            "id": "4143157",
            "problem": "Consider a complex adaptive system modeled as a smooth autonomous ordinary differential equation (ODE) $\\dot{x} = f(x)$ on $\\mathbb{R}^n$, where $f$ is continuously differentiable and $x(t)$ denotes the system trajectory from a typical initial condition $x(0)$. Let $\\delta x(t)$ denote an infinitesimal perturbation that evolves under the linearization along the trajectory, obeying the variational equation $\\dot{\\delta x} = Df(x(t))\\,\\delta x$, where $Df(x(t))$ is the Jacobian matrix of $f$ evaluated on the trajectory. Sensitive dependence on initial conditions is quantified by the maximal Lyapunov exponent $\\lambda_{\\max}$, which characterizes the asymptotic exponential growth rate of $\\|\\delta x(t)\\|$ under the tangent dynamics.\n\nFrom the fundamental definition of the maximal Lyapunov exponent in terms of the tangent flow and norms,\n$$\n\\lambda_{\\max} = \\lim_{t \\to \\infty} \\frac{1}{t}\\,\\ln\\left(\\frac{\\|\\delta x(t)\\|}{\\|\\delta x(0)\\|}\\right),\n$$\nderive a practical estimator based on evolving the variational equation and explain how periodic renormalization of $\\delta x$ contributes to obtaining a numerically stable and asymptotically consistent estimate. Select the option that correctly specifies a valid procedure to estimate $\\lambda_{\\max}$ from $\\dot{\\delta x} = Df(x(t))\\,\\delta x$, including the correct role and effect of renormalization.\n\nA. Initialize $\\delta x(0)$ with $\\|\\delta x(0)\\| = 1$. For a fixed $\\Delta t > 0$, for $i = 1,\\dots,k$, integrate $\\dot{\\delta x} = Df(x(t))\\,\\delta x$ concurrently with $\\dot{x} = f(x)$ over the interval $[(i-1)\\Delta t, i\\Delta t]$ to obtain the updated $\\delta x$. Let $s_i = \\|\\delta x(i\\Delta t)\\|$ be the stretch factor over the $i$-th interval. Immediately renormalize by setting $\\delta x(i\\Delta t) \\leftarrow \\delta x(i\\Delta t)/s_i$, accumulate $S \\leftarrow S + \\ln(s_i)$, and continue. After total time $T = k\\,\\Delta t$, estimate $\\lambda_{\\max} \\approx S/T$. The renormalization prevents numerical overflow/underflow while preserving the asymptotic rate, and repeated application aligns $\\delta x$ with the most expanding Oseledec subspace, so that the accumulated $\\ln(s_i)$ sums local growth rates along the dominant direction without altering their average per unit time.\n\nB. Initialize any nonzero $\\delta x(0)$. Evolve $\\delta x$ using $\\dot{\\delta x} = Df(x(t))\\,\\delta x$ and, at each step of length $\\Delta t$, renormalize $\\delta x$ to unit norm but estimate $\\lambda_{\\max}$ using the arithmetic mean of instantaneous growth rates $\\frac{\\|\\delta x(i\\Delta t)\\| - \\|\\delta x((i-1)\\Delta t)\\|}{\\Delta t}$ over $i = 1,\\dots,k$. The renormalization primarily reduces measurement noise and has no effect on the asymptotic estimate.\n\nC. Evolve $\\delta x$ using $\\dot{\\delta x} = Df(x(t))\\,\\delta x$ with no renormalization and estimate $\\lambda_{\\max} \\approx \\frac{1}{T}\\,\\ln\\left(\\frac{\\|\\delta x(T)\\|}{\\|\\delta x(0)\\|}\\right)$ for large $T$. Renormalization is unnecessary because the trace of $Df(x(t))$ determines the asymptotic behavior of $\\|\\delta x(t)\\|$.\n\nD. Estimate $\\lambda_{\\max}$ by averaging the largest instantaneous eigenvalue of $Df(x(t))$ along the trajectory, i.e., set $\\lambda_{\\max} \\approx \\frac{1}{T}\\int_0^T \\alpha(Df(x(\\tau)))\\,d\\tau$, where $\\alpha(M)$ denotes the spectral abscissa (the largest real part of the eigenvalues of $M$). Renormalization is not needed because the dominant instantaneous eigenvalue determines the asymptotic growth rate of $\\|\\delta x(t)\\|$.",
            "solution": "The problem statement will first be validated for scientific and mathematical soundness.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- System model: An autonomous ordinary differential equation (ODE) $\\dot{x} = f(x)$ on $\\mathbb{R}^n$.\n- Smoothness conditions: $f$ is continuously differentiable.\n- Trajectory: $x(t)$ is the system trajectory from an initial condition $x(0)$.\n- Perturbation: $\\delta x(t)$ is an infinitesimal perturbation.\n- Variational equation: The evolution of the perturbation is governed by the linearization $\\dot{\\delta x} = Df(x(t))\\,\\delta x$, where $Df(x(t))$ is the Jacobian matrix of $f$ evaluated at $x(t)$.\n- Definition of Maximal Lyapunov Exponent: $\\lambda_{\\max} = \\lim_{t \\to \\infty} \\frac{1}{t}\\,\\ln\\left(\\frac{\\|\\delta x(t)\\|}{\\|\\delta x(0)\\|}\\right)$.\n- Task: Derive a practical estimator for $\\lambda_{\\max}$ from the variational equation, explain the role of periodic renormalization, and select the correct option describing this procedure.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded. The setup is the standard framework for defining and analyzing Lyapunov exponents in the theory of dynamical systems and chaos. The ODE model $\\dot{x}=f(x)$, the use of the variational equation $\\dot{\\delta x} = Df(x(t))\\,\\delta x$ to describe the evolution of tangent vectors, and the limit definition of the maximal Lyapunov exponent are all fundamental and correct concepts from this field.\n\nThe problem is well-posed and objective. It asks for the derivation of a standard numerical algorithm (the Benettin et al. algorithm or equivalent) based on the provided fundamental definition. The language is precise and mathematical, using standard terminology without ambiguity. There are no contradictions, missing pieces of essential information, or scientifically unsound premises. The problem does not violate any of the criteria for validity.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. A solution will be derived.\n\n**Derivation and Analysis**\n\nThe problem asks for a practical method to estimate the maximal Lyapunov exponent, defined as:\n$$\n\\lambda_{\\max} = \\lim_{t \\to \\infty} \\frac{1}{t}\\,\\ln\\left(\\frac{\\|\\delta x(t)\\|}{\\|\\delta x(0)\\|}\\right)\n$$\nA direct numerical implementation of this formula is fraught with peril. If the system is chaotic, then $\\lambda_{\\max} > 0$, implying that the norm of the perturbation vector, $\\|\\delta x(t)\\|$, grows exponentially. For large $t$, this would rapidly cause a numerical overflow in any finite-precision computer arithmetic. Conversely, for a stable system with $\\lambda_{\\max}  0$, $\\|\\delta x(t)\\|$ would decay exponentially, leading to underflow and loss of precision.\n\nTo circumvent this, we can exploit the properties of the logarithm. Let us divide the total integration time $T$ into $k$ small, equal intervals of duration $\\Delta t$, such that $T = k\\Delta t$. The ratio of the norms can be expressed as a product of growth factors over each small interval:\n$$\n\\frac{\\|\\delta x(T)\\|}{\\|\\delta x(0)\\|} = \\frac{\\|\\delta x(k\\Delta t)\\|}{\\|\\delta x(0)\\|} = \\left(\\frac{\\|\\delta x(k\\Delta t)\\|}{\\|\\delta x((k-1)\\Delta t)\\|}\\right) \\left(\\frac{\\|\\delta x((k-1)\\Delta t)\\|}{\\|\\delta x((k-2)\\Delta t)\\|}\\right) \\dots \\left(\\frac{\\|\\delta x(\\Delta t)\\|}{\\|\\delta x(0)\\|}\\right)\n$$\nLet us define $t_i = i\\Delta t$. If we denote the perturbation vector at time $t_{i-1}$ as $\\delta x(t_{i-1})$ and the vector it evolves into at time $t_i$ as $\\delta x(t_i)$, then we could write this as a product of local growth factors. However, the magnitude of the vector would still grow or shrink uncontrollably.\n\nThe standard and correct procedure introduces a **renormalization** step. Let $\\delta\\hat{x}_{i-1}$ be a unit vector representing the direction of the perturbation at the beginning of the $i$-th interval (at time $t_{i-1}$).\nThe algorithm proceeds as follows:\n$1$. Initialize the main system with $x(0)$ and the tangent vector with a random unit vector $\\delta\\hat{x}_0$ (i.e., $\\|\\delta\\hat{x}_0\\| = 1$). Initialize an accumulator for the logarithmic growth, $S=0$.\n$2$. For each step $i = 1, 2, \\dots, k$:\n    a. Simultaneously integrate the system equation $\\dot{x} = f(x)$ and the variational equation $\\dot{\\delta x} = Df(x(t))\\,\\delta x$ over the time interval $[t_{i-1}, t_i]$. The initial condition for the variational equation is $\\delta x(t_{i-1}) = \\delta\\hat{x}_{i-1}$. Let the solution at the end of the interval be $\\delta x'$.\n    b. Calculate the norm of this evolved vector, $s_i = \\|\\delta x'\\|$. This value $s_i$ represents the factor by which the length of the perturbation vector was stretched during the interval $\\Delta t$.\n    c. Add the logarithm of this stretch factor to the accumulator: $S \\leftarrow S + \\ln(s_i)$.\n    d. Renormalize the vector to have unit length for the next iteration: $\\delta\\hat{x}_i = \\delta x' / s_i$. This step is crucial. It resets the magnitude to $1$ while preserving the vector's direction, thus preventing overflow/underflow.\n$3$. After $k$ steps, the total time elapsed is $T = k\\Delta t$. The maximal Lyapunov exponent is estimated by the average logarithmic growth rate:\n$$\n\\lambda_{\\max} \\approx \\frac{S}{T} = \\frac{1}{k\\Delta t} \\sum_{i=1}^k \\ln(s_i)\n$$\nThis procedure is numerically stable. Furthermore, according to Oseledec's Multiplicative Ergodic Theorem, for a typical starting vector $\\delta\\hat{x}_0$, repeated application of the linearized flow and renormalization will cause the direction of the vector $\\delta\\hat{x}_i$ to align with the most unstable direction in the tangent space, known as the first Oseledec subspace. Therefore, the measured growth factors $s_i$ correspond to stretching along this dominant direction, and their geometric mean (which corresponds to the arithmetic mean of their logarithms) correctly yields the maximal Lyapunov exponent.\n\n**Evaluation of Options**\n\n**A.** This option describes the procedure derived above.\n- It correctly specifies initializing a unit vector, integrating over intervals $\\Delta t$, and calculating the stretch factor $s_i = \\|\\delta x(i\\Delta t)\\|$.\n- It correctly describes the renormalization step $\\delta x(i\\Delta t) \\leftarrow \\delta x(i\\Delta t)/s_i$.\n- It correctly specifies accumulating the sum of logarithms, $S \\leftarrow S + \\ln(s_i)$.\n- It correctly gives the final estimator $\\lambda_{\\max} \\approx S/T$.\n- The explanation is also flawless: renormalization prevents overflow/underflow, repeated application aligns the vector with the most expanding Oseledec subspace, and the procedure correctly sums the local logarithmic growth rates along this dominant direction to find the correct asymptotic average.\n**Verdict: Correct.**\n\n**B.** This option proposes estimating $\\lambda_{\\max}$ using the arithmetic mean of rates like $\\frac{\\|\\delta x(i\\Delta t)\\| - \\|\\delta x((i-1)\\Delta t)\\|}{\\Delta t}$. This is incorrect. The growth of a perturbation in a chaotic system is exponential, so its rate of growth must be characterized logarithmically. The expression given is an approximation of a linear rate of change, not an exponential growth rate. The correct quantity to average is the logarithmic growth rate, $\\frac{1}{\\Delta t}\\ln(\\|\\delta x(i\\Delta t)\\|/\\|\\delta x((i-1)\\Delta t)\\|)$. The explanation that renormalization \"reduces measurement noise\" is also vague and misses the primary point of preventing catastrophic numerical failure.\n**Verdict: Incorrect.**\n\n**C.** This option suggests using the definitional formula directly, without renormalization. As explained in the derivation, this approach is numerically infeasible for any system with a non-zero Lyapunov exponent due to overflow or underflow. The justification provided is also fundamentally incorrect. The trace of the Jacobian, $\\text{tr}(Df(x(t)))$, relates to the rate of change of phase-space volume elements. By Liouville's theorem extended to nonlinear flows, the sum of all Lyapunov exponents is equal to the time-average of the trace of the Jacobian: $\\sum_{j=1}^n \\lambda_j = \\lim_{T\\to\\infty} \\frac{1}{T}\\int_0^T \\text{tr}(Df(x(\\tau))) d\\tau$. This governs volume evolution, not the stretching of a single vector, which is determined by $\\lambda_{\\max}$.\n**Verdict: Incorrect.**\n\n**D.** This option suggests that $\\lambda_{\\max}$ can be estimated by averaging the largest instantaneous eigenvalue (or more precisely, the largest real part of any eigenvalue, the spectral abscissa) of the Jacobian matrix $Df(x(t))$ along the trajectory. This is a common misconception. The maximal Lyapunov exponent is a property of the non-autonomous linear system $\\dot{\\delta x} = A(t)\\delta x$ where $A(t) = Df(x(t))$. The asymptotic growth rate of solutions to such a system is not, in general, the time-average of the largest eigenvalue of $A(t)$. The reason is that the eigenvectors of $A(t)$ rotate as the system evolves along the trajectory. The perturbation vector $\\delta x$ does not have sufficient time to align with the instantaneous most expanding direction before that direction itself changes. The long-term growth is a result of the cumulative, multiplicative action of the flow, which is correctly captured by the procedure in option A, not by averaging local eigenvalues.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "This practice serves as a capstone experience, applying the numerical algorithm for calculating Lyapunov exponents to a classic model of atmospheric chaos—the Lorenz system. By implementing and experimenting with different numerical integrators and step sizes, you will enter a virtual laboratory to explore the interplay between a system's intrinsic dynamics and the tools used to simulate it. This hands-on coding problem provides deep insights into the practical challenges of modeling complex systems and the critical importance of evaluating the accuracy and limitations of numerical methods, such as the divergence of a computed trajectory from a true one, a concept known as shadowing.",
            "id": "4143119",
            "problem": "You are tasked with designing and implementing a numerical experiment to quantify the impact of fixed-step size and integrator order on the accuracy of computed maximal Lyapunov exponents in a chaotic flow, while also evaluating shadowing behavior. The problem must be addressed purely in mathematical and algorithmic terms, using well-tested definitions and formulas as the base.\n\nConsider the three-dimensional Lorenz system with parameters chosen in the classical chaotic regime, defined by the ordinary differential equation\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma (y - x) \\\\\nx (\\rho - z) - y \\\\\nx y - \\beta z\n\\end{bmatrix},\n$$\nwhere $(\\sigma, \\rho, \\beta) = (10, 28, 8/3)$, and initial condition $x(0) = 1$, $y(0) = 1$, $z(0) = 1$. The fundamental concept of sensitivity to initial conditions is quantified by the maximal Lyapunov exponent, defined for a continuous-time dynamical system by\n$$\n\\lambda_{\\max} = \\lim_{t \\to \\infty} \\frac{1}{t} \\ln \\frac{\\lVert \\delta x(t) \\rVert}{\\lVert \\delta x(0) \\rVert},\n$$\nwhere $\\delta x(t)$ evolves according to the variational (tangent) equation\n$$\n\\frac{d}{dt}\\delta x = J(x(t))\\,\\delta x,\n$$\nwith $J(x)$ the Jacobian of the vector field evaluated along the trajectory $x(t)$.\n\nYour program must:\n- Implement fixed-step explicit Runge–Kutta integrators to evolve both the state $x(t)$ and the tangent vector $\\delta x(t)$ in a stage-consistent manner. Specifically, implement the classical order-$2$ midpoint method (denoted $\\mathrm{RK2}$) and the classical order-$4$ method (denoted $\\mathrm{RK4}$).\n- Estimate the finite-time maximal Lyapunov exponent over a fixed horizon $T = 20$ by repeatedly renormalizing the tangent vector at each step. The estimator at step size $h$ with $N = T/h$ steps is\n$$\n\\hat{\\lambda}(h) = \\frac{1}{T} \\sum_{k=1}^{N} \\ln \\left( \\frac{\\lVert \\delta x_{k} \\rVert}{\\lVert \\delta x_{k-1} \\rVert} \\right),\n$$\nwith the renormalization $\\delta x_{k} \\leftarrow \\delta x_{k} / \\lVert \\delta x_{k} \\rVert$ performed after each step. Initialize the tangent vector with $\\delta x(0)$ of unit norm.\n- Construct a high-fidelity reference by integrating the same system and variational equations using the $\\mathrm{RK4}$ method with a small fixed step size $h_{\\mathrm{ref}} = 2 \\times 10^{-4}$ over the same horizon $T = 20$. Use this to compute a reference exponent $\\lambda_{\\mathrm{ref}}$ and a dense reference trajectory $x_{\\mathrm{ref}}(t)$.\n- For each numerical experiment, evaluate shadowing with respect to the reference trajectory, defined as follows. For a tolerance $\\varepsilon = 0.1$, define the shadowing time $\\tau(\\varepsilon)$ as the supremum of times $t \\in [0, T]$ such that for all $s \\in [0, t]$,\n$$\n\\lVert x_{\\mathrm{num}}(s) - x_{\\mathrm{ref}}(s) \\rVert \\le \\varepsilon.\n$$\nIn practice, approximate this by checking the inequality at the discrete times of the numerical solution and interpolating $x_{\\mathrm{ref}}(t)$ linearly to those times; define $\\tau(\\varepsilon)$ as the first time the inequality is violated (or $T$ if it is never violated).\n- Quantify accuracy by the absolute error $|\\hat{\\lambda}(h) - \\lambda_{\\mathrm{ref}}|$.\n\nYour program must run all test cases below, each specified by an integrator method and step size $h$, using the horizon $T = 20$:\n1. $(\\mathrm{RK4}, h = 0.01)$,\n2. $(\\mathrm{RK4}, h = 0.005)$,\n3. $(\\mathrm{RK2}, h = 0.01)$,\n4. $(\\mathrm{RK2}, h = 0.005)$,\n5. $(\\mathrm{RK4}, h = 0.05)$.\n\nFor each test case, compute and return a triple consisting of:\n- the estimated maximal Lyapunov exponent $\\hat{\\lambda}(h)$ in inverse time units,\n- the absolute error $|\\hat{\\lambda}(h) - \\lambda_{\\mathrm{ref}}|$ in inverse time units,\n- the shadowing time $\\tau(\\varepsilon)$ in the same time units as $T$.\n\nAll quantities here are dimensionless; do not attach physical units such as meters or seconds. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list in the order given above, and each sub-list is formatted as $[\\hat{\\lambda},\\mathrm{error},\\tau]$. For example, the final output format must be\n$$\n[\\,[\\hat{\\lambda}_1,\\mathrm{error}_1,\\tau_1],\\ldots,[\\hat{\\lambda}_5,\\mathrm{error}_5,\\tau_5]\\,].\n$$\n\nThe experiment must be scientifically realistic and self-consistent, and you must build all derivations starting from the above definitions, without using shortcut formulas that bypass the core reasoning about sensitive dependence on initial conditions in chaotic systems. The test suite covers normal operation, finer step sizes, lower-order integration effects, and a coarse step size edge case that may degrade accuracy and shadowing behavior. All outputs must be floats.",
            "solution": "The user requires a numerical experiment to evaluate the performance of different numerical integrators and step sizes in computing the maximal Lyapunov exponent and shadowing time for the Lorenz system. The solution is formulated by first establishing the mathematical and algorithmic framework, then implementing it to process the specified test cases.\n\n### Mathematical and Algorithmic Formulation\n\nThe core of the problem is the Lorenz system, a set of three coupled, nonlinear ordinary differential equations (ODEs) describing a simplified model of atmospheric convection. The state of the system at time $t$ is given by the vector $\\mathbf{x}(t) = [x(t), y(t), z(t)]^T$. The equations of motion are:\n$$\n\\frac{d\\mathbf{x}}{dt} = F(\\mathbf{x}) =\n\\begin{bmatrix}\n\\sigma (y - x) \\\\\nx (\\rho - z) - y \\\\\nx y - \\beta z\n\\end{bmatrix}\n$$\nThe parameters are set to their classical chaotic values: $(\\sigma, \\rho, \\beta) = (10, 28, 8/3)$. The initial condition is $\\mathbf{x}(0) = [1, 1, 1]^T$.\n\nTo compute the maximal Lyapunov exponent, we must track the evolution of an infinitesimal separation vector in the tangent space, denoted $\\mathbf{v}(t) = \\delta \\mathbf{x}(t)$. Its dynamics are governed by the linear variational equation:\n$$\n\\frac{d\\mathbf{v}}{dt} = J(\\mathbf{x}(t)) \\mathbf{v}(t)\n$$\nwhere $J(\\mathbf{x})$ is the Jacobian matrix of the vector field $F(\\mathbf{x})$, evaluated along the trajectory $\\mathbf{x}(t)$. The Jacobian is:\n$$\nJ(\\mathbf{x}) = \\frac{\\partial F}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n-\\sigma  \\sigma  0 \\\\\n\\rho-z  -1  -x \\\\\ny  x  -\\beta\n\\end{bmatrix}\n$$\nFor numerical integration, we combine the state and tangent vectors into a single $6$-dimensional state vector $Y(t) = [\\mathbf{x}(t)^T, \\mathbf{v}(t)^T]^T$. The full system of ODEs is:\n$$\n\\frac{dY}{dt} =\n\\begin{bmatrix}\nF(\\mathbf{x}(t)) \\\\\nJ(\\mathbf{x}(t))\\mathbf{v}(t)\n\\end{bmatrix}\n$$\nThis augmented system is integrated numerically using two explicit Runge-Kutta methods: the order-$2$ midpoint method ($\\mathrm{RK2}$) and the classical order-$4$ method ($\\mathrm{RK4}$). Applying the integrator to the $6D$ system ensures that the evolution of the state $\\mathbf{x}$ and the tangent vector $\\mathbf{v}$ are computed in a stage-consistent manner, meaning the Jacobian $J$ is evaluated at the appropriate intermediate stages required by the Runge-Kutta formulation.\n\n### Maximal Lyapunov Exponent Estimation\n\nThe maximal Lyapunov exponent, $\\lambda_{\\max}$, quantifies the average exponential rate of divergence of nearby trajectories. It is estimated over a finite time horizon $T$. A naive integration of the tangent vector $\\mathbf{v}(t)$ would lead to numerical overflow due to its exponential growth. To prevent this, the tangent vector is periodically renormalized. The algorithm proceeds as follows:\n1. Initialize the tangent vector $\\mathbf{v}_0$ to a unit vector, e.g., $\\mathbf{v}_0 = [1, 0, 0]^T$.\n2. For each integration step $k=1, \\dots, N$, where $N=T/h$:\n   a. Advance the augmented system $Y_{k-1} = [\\mathbf{x}_{k-1}^T, \\mathbf{v}_{k-1}^T]^T$ by a single step of size $h$ to obtain a temporary next state $Y'_k = [(\\mathbf{x}'_k)^T, (\\mathbf{v}'_k)^T]^T$.\n   b. The state vector is updated: $\\mathbf{x}_k = \\mathbf{x}'_k$.\n   c. Calculate the norm of the unnormalized tangent vector, $d_k = \\lVert \\mathbf{v}'_k \\rVert$. This represents the local stretching factor over the interval $[t_{k-1}, t_k]$.\n   d. Add the logarithm of this factor to a running sum.\n   e. Renormalize the tangent vector for the next step: $\\mathbf{v}_k = \\mathbf{v}'_k / d_k$.\n3. The finite-time maximal Lyapunov exponent estimate $\\hat{\\lambda}(h)$ is the time-averaged sum of the logarithmic growth factors:\n$$\n\\hat{\\lambda}(h) = \\frac{1}{T} \\sum_{k=1}^{N} \\ln(d_k)\n$$\n\n### Reference Solution and Error Analysis\n\nTo assess the accuracy of the numerical results, a high-fidelity reference solution is generated. This involves computing a reference exponent $\\lambda_{\\mathrm{ref}}$ and a reference trajectory $\\mathbf{x}_{\\mathrm{ref}}(t)$ by integrating the augmented system using the $\\mathrm{RK4}$ method with a very small step size, $h_{\\mathrm{ref}} = 2 \\times 10^{-4}$, over the horizon $T=20$.\n\nThe accuracy of a computed exponent $\\hat{\\lambda}(h)$ is quantified by the absolute error: $|\\hat{\\lambda}(h) - \\lambda_{\\mathrm{ref}}|$.\n\nThe concept of shadowing addresses whether a numerical trajectory, despite diverging from the true trajectory with the same initial condition, remains close to *some* true trajectory. We approximate this by measuring how long the numerical trajectory $\\mathbf{x}_{\\mathrm{num}}(t)$ stays within a small tube of radius $\\varepsilon$ around the reference trajectory $\\mathbf{x}_{\\mathrm{ref}}(t)$. The shadowing time $\\tau(\\varepsilon)$ is defined as the first time $t_k = k \\cdot h$ at which the Euclidean distance exceeds a given tolerance $\\varepsilon = 0.1$:\n$$\n\\tau(\\varepsilon) = \\min \\{ t_k \\mid \\lVert \\mathbf{x}_{\\mathrm{num}}(t_k) - \\mathbf{x}_{\\mathrm{ref}}(t_k) \\rVert  \\varepsilon \\}\n$$\nIf the distance never exceeds $\\varepsilon$ over the interval $[0, T]$, then $\\tau(\\varepsilon) = T$. To perform this comparison, the dense reference trajectory $\\mathbf{x}_{\\mathrm{ref}}(t)$ is linearly interpolated to the discrete time points $t_k$ of the numerical trajectory $\\mathbf{x}_{\\mathrm{num}}$.\n\n### Implementation Details\n\nThe solution is provided as a Python program structured as follows:\n1.  A function `lorenz_augmented_deriv` computes the derivative of the $6D$ augmented state vector $Y$.\n2.  Functions `rk2_step` and `rk4_step` implement the respective Runge-Kutta integration steps.\n3.  A primary function, `run_simulation`, orchestrates the integration over the full time horizon $T$, performing the step-wise renormalization of the tangent vector and accumulating the data needed for the Lyapunov exponent calculation. It returns the estimated exponent and the computed trajectory.\n4.  A helper function `calculate_shadowing_time` computes $\\tau(\\varepsilon)$ by interpolating the reference trajectory and finding the first point of violation.\n5.  The main `solve` block first generates the reference solution. It then iterates through the five specified test cases, calling `run_simulation` for each, calculating the error and shadowing time, and collecting the results. Finally, it prints the collected data in the specified format. All calculations use standard double-precision floating-point arithmetic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Problem-defined constants and parameters\nSIGMA = 10.0\nRHO = 28.0\nBETA = 8.0 / 3.0\nT_HORIZON = 20.0\nEPSILON = 0.1\nH_REF = 2e-4\nX0 = np.array([1.0, 1.0, 1.0])\nV0 = np.array([1.0, 0.0, 0.0])  # Initial tangent vector, must be unit norm\n\ndef lorenz_augmented_deriv(t, y):\n    \"\"\"\n    Computes the derivative of the augmented 6D Lorenz system.\n    y = [x, y, z, vx, vy, vz]\n    \"\"\"\n    state = y[:3]\n    tangent = y[3:]\n    x, y_coord, z = state[0], state[1], state[2]\n\n    # State dynamics (F(x))\n    d_state = np.array([\n        SIGMA * (y_coord - x),\n        x * (RHO - z) - y_coord,\n        x * y_coord - BETA * z\n    ])\n\n    # Variational dynamics (J(x) * v)\n    jacobian = np.array([\n        [-SIGMA, SIGMA, 0.0],\n        [RHO - z, -1.0, -x],\n        [y_coord, x, -BETA]\n    ])\n    d_tangent = jacobian @ tangent\n\n    return np.concatenate((d_state, d_tangent))\n\ndef rk2_step(f, t, y, h):\n    \"\"\"Classical Runge-Kutta 2nd order (midpoint) step.\"\"\"\n    k1 = h * f(t, y)\n    k2 = h * f(t + h / 2.0, y + k1 / 2.0)\n    return y + k2\n\ndef rk4_step(f, t, y, h):\n    \"\"\"Classical Runge-Kutta 4th order step.\"\"\"\n    k1 = h * f(t, y)\n    k2 = h * f(t + h / 2.0, y + k1 / 2.0)\n    k3 = h * f(t + h / 2.0, y + k2 / 2.0)\n    k4 = h * f(t + h, y + k3)\n    return y + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0\n\ndef run_simulation(method_str, h):\n    \"\"\"\n    Integrates the Lorenz system and computes the maximal Lyapunov exponent.\n    \"\"\"\n    if method_str == 'RK2':\n        step_func = rk2_step\n    elif method_str == 'RK4':\n        step_func = rk4_step\n    else:\n        raise ValueError(f\"Unknown integration method: {method_str}\")\n\n    n_steps = int(round(T_HORIZON / h))\n    # Using a fixed number of steps can slightly alter h, ensuring endpoint is T\n    h_actual = T_HORIZON / n_steps\n    \n    times = np.linspace(0.0, T_HORIZON, n_steps + 1)\n    \n    y_current = np.concatenate((X0, V0))\n    trajectory = [X0]\n    log_sum = 0.0\n    \n    for i in range(n_steps):\n        t_current = i * h_actual\n        y_next_unnorm = step_func(lorenz_augmented_deriv, t_current, y_current, h_actual)\n        \n        x_next = y_next_unnorm[:3]\n        v_next_unnorm = y_next_unnorm[3:]\n        \n        norm_v = np.linalg.norm(v_next_unnorm)\n        \n        # Add log of the stretching factor. Handle norm_v=0 case.\n        if norm_v > 0:\n            log_sum += np.log(norm_v)\n        \n        # Renormalize the tangent vector for the next step.\n        v_next_norm = v_next_unnorm / norm_v if norm_v > 0 else v_next_unnorm\n        \n        y_current = np.concatenate((x_next, v_next_norm))\n        trajectory.append(x_next)\n        \n    lambda_est = log_sum / T_HORIZON\n    \n    return lambda_est, times, np.array(trajectory)\n\ndef calculate_shadowing_time(num_times, num_traj, ref_times, ref_traj):\n    \"\"\"\n    Calculates the shadowing time of a numerical trajectory against a reference.\n    \"\"\"\n    # Linearly interpolate the reference trajectory to the numerical time points.\n    ref_x_interp = np.interp(num_times, ref_times, ref_traj[:, 0])\n    ref_y_interp = np.interp(num_times, ref_times, ref_traj[:, 1])\n    ref_z_interp = np.interp(num_times, ref_times, ref_traj[:, 2])\n    \n    ref_traj_interp = np.vstack((ref_x_interp, ref_y_interp, ref_z_interp)).T\n    \n    # Calculate the Euclidean distance at each time point.\n    distances = np.linalg.norm(num_traj - ref_traj_interp, axis=1)\n    \n    # Find the first index where distance > EPSILON.\n    # We search from index 1, since distance at index 0 (t=0) is 0.\n    violation_indices = np.where(distances[1:] > EPSILON)[0]\n    \n    if len(violation_indices) > 0:\n        # Get the time corresponding to the first violation.\n        # Add 1 to index to account for slicing from index 1.\n        first_violation_index = violation_indices[0] + 1\n        shadow_time = num_times[first_violation_index]\n    else:\n        # If the tolerance is never exceeded, shadowing time is the full horizon.\n        shadow_time = T_HORIZON\n        \n    return shadow_time\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('RK4', 0.01),\n        ('RK4', 0.005),\n        ('RK2', 0.01),\n        ('RK2', 0.005),\n        ('RK4', 0.05)\n    ]\n\n    # 1. Generate high-fidelity reference solution\n    lambda_ref, t_ref, traj_ref = run_simulation('RK4', H_REF)\n\n    # 2. Run all test cases and compute metrics\n    results = []\n    for method, h in test_cases:\n        # Run simulation for the current test case\n        lambda_hat, t_num, traj_num = run_simulation(method, h)\n        \n        # Calculate absolute error in Lyapunov exponent\n        error = np.abs(lambda_hat - lambda_ref)\n        \n        # Calculate shadowing time\n        shadow_time = calculate_shadowing_time(t_num, traj_num, t_ref, traj_ref)\n        \n        # Store the triplet of results\n        results.append([lambda_hat, error, shadow_time])\n\n    # 3. Final print statement in the exact required format.\n    # The str() representation of a Python list includes spaces, and joining\n    # them with a comma and enclosing in brackets matches the example format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}