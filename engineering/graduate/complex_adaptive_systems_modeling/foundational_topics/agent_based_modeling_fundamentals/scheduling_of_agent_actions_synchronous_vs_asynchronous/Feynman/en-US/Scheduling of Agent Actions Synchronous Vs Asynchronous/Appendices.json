{
    "hands_on_practices": [
        {
            "introduction": "The choice between synchronous and asynchronous scheduling is not a mere technicality; it can fundamentally alter a model's behavior. This first exercise provides a stark and minimalist demonstration of *path dependence*, a critical feature of many asynchronous systems . By tracing the evolution of a small, three-agent system with a cyclic dependency, you will see firsthand how simply changing the order of updates leads to dramatically different outcomes, revealing the non-commutative nature of agent interactions.",
            "id": "4142152",
            "problem": "Consider an Agent-Based Model (ABM) with $3$ agents labeled $A$, $B$, and $C$ arranged in a directed cycle $A \\to B \\to C \\to A$. Each agent $i \\in \\{A,B,C\\}$ has a binary state $x_i \\in \\{0,1\\}$ and updates its state according to a local threshold rule that depends only on its single in-neighbor. Let $H:\\mathbb{R}\\to\\{0,1\\}$ denote the Heaviside step function defined by $H(u)=1$ if $u>0$ and $H(u)=0$ if $u \\le 0$. The local update rules are\n$$\nx_A \\leftarrow H\\!\\big(x_C - \\theta\\big), \\quad x_B \\leftarrow H\\!\\big(x_A - \\theta\\big), \\quad x_C \\leftarrow H\\!\\big(x_B - \\theta\\big),\n$$\nwith threshold $\\theta = 0.5$. The initial state at the start of a short time window is\n$$\n\\boldsymbol{x}^{(0)} = \\begin{pmatrix} x_A^{(0)} \\\\ x_B^{(0)} \\\\ x_C^{(0)} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nWithin a short time window, a synchronous schedule updates all three agents simultaneously using the pre-window states, while an asynchronous schedule updates agents one at a time in-place using the most recently updated values. Consider two asynchronous schedules represented by permutations $\\pi_1=(A,B,C)$ and $\\pi_2=(B,C,A)$, each applying exactly one local update to each agent during the window.\n\nDefine the path-dependence magnitude as the $1$-norm of the difference between the two post-window states,\n$$\nD \\equiv \\left\\| \\boldsymbol{x}^{(\\pi_1)} - \\boldsymbol{x}^{(\\pi_2)} \\right\\|_1,\n$$\nwhere $\\boldsymbol{x}^{(\\pi)}$ denotes the state vector resulting from executing permutation $\\pi$ within the window.\n\nStarting from $\\boldsymbol{x}^{(0)}$ and using the update rule above, compute $D$. Express your final result as an exact integer. No rounding is required. No physical units are involved.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\nThe givens are:\n- A set of $3$ agents, $\\{A, B, C\\}$, with states $x_A, x_B, x_C \\in \\{0, 1\\}$.\n- A directed network topology: $A \\to B \\to C \\to A$.\n- A Heaviside step function $H(u)$ defined as $1$ for $u > 0$ and $0$ for $u \\le 0$.\n- Local update rules: $x_A \\leftarrow H(x_C - \\theta)$, $x_B \\leftarrow H(x_A - \\theta)$, $x_C \\leftarrow H(x_B - \\theta)$.\n- A fixed threshold $\\theta = 0.5$.\n- An initial state vector at time $t=0$: $\\boldsymbol{x}^{(0)} = \\begin{pmatrix} x_A^{(0)} \\\\ x_B^{(0)} \\\\ x_C^{(0)} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n- Two asynchronous update schedules (permutations): $\\pi_1=(A,B,C)$ and $\\pi_2=(B,C,A)$.\n- The definition of path-dependence magnitude: $D \\equiv \\left\\| \\boldsymbol{x}^{(\\pi_1)} - \\boldsymbol{x}^{(\\pi_2)} \\right\\|_1$.\n\nThe problem is a well-defined exercise in discrete dynamical systems, specifically Boolean networks, a standard topic in complex systems modeling. All terms are formally defined, and the initial conditions and rules are sufficient to compute a unique solution. The problem is therefore valid.\n\nThe solution proceeds by computing the final state vector for each asynchronous schedule, $\\boldsymbol{x}^{(\\pi_1)}$ and $\\boldsymbol{x}^{(\\pi_2)}$, and then calculating the $1$-norm of their difference. Asynchronous updates are \"in-place,\" meaning the state of an agent is updated immediately and this new value is used by any subsequent agent in the update sequence.\n\nLet the state vector be $\\boldsymbol{x} = \\begin{pmatrix} x_A \\\\ x_B \\\\ x_C \\end{pmatrix}$.\nThe initial state is $\\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nThe threshold is $\\theta = 0.5$.\n\nFirst, we compute the final state $\\boldsymbol{x}^{(\\pi_1)}$ for the schedule $\\pi_1 = (A,B,C)$.\nThe updates are performed sequentially: $A$, then $B$, then $C$.\n\n1.  Update agent $A$: The rule is $x_A \\leftarrow H(x_C - \\theta)$. The current state is $(x_A, x_B, x_C) = (0, 0, 1)$. The input to $A$ is $x_C=1$.\n    $$x_A' = H(1 - 0.5) = H(0.5) = 1$$\n    The state vector becomes $\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n2.  Update agent $B$: The rule is $x_B \\leftarrow H(x_A - \\theta)$. The current state is $(x_A, x_B, x_C) = (1, 0, 1)$. The input to $B$ is the newly updated state $x_A=1$.\n    $$x_B' = H(1 - 0.5) = H(0.5) = 1$$\n    The state vector becomes $\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n\n3.  Update agent $C$: The rule is $x_C \\leftarrow H(x_B - \\theta)$. The current state is $(x_A, x_B, x_C) = (1, 1, 1)$. The input to $C$ is the newly updated state $x_B=1$.\n    $$x_C' = H(1 - 0.5) = H(0.5) = 1$$\n    The state vector becomes $\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n\nThe final state after executing schedule $\\pi_1$ is $\\boldsymbol{x}^{(\\pi_1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n\nNext, we compute the final state $\\boldsymbol{x}^{(\\pi_2)}$ for the schedule $\\pi_2 = (B,C,A)$.\nThe updates are performed sequentially: $B$, then $C$, then $A$. The system starts again from the initial state $\\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n1.  Update agent $B$: The rule is $x_B \\leftarrow H(x_A - \\theta)$. The current state is $(x_A, x_B, x_C) = (0, 0, 1)$. The input to $B$ is the initial state $x_A=0$.\n    $$x_B' = H(0 - 0.5) = H(-0.5) = 0$$\n    The state vector remains $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n2.  Update agent $C$: The rule is $x_C \\leftarrow H(x_B - \\theta)$. The current state is $(x_A, x_B, x_C) = (0, 0, 1)$. The input to $C$ is the newly updated state $x_B=0$.\n    $$x_C' = H(0 - 0.5) = H(-0.5) = 0$$\n    The state vector becomes $\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n3.  Update agent $A$: The rule is $x_A \\leftarrow H(x_C - \\theta)$. The current state is $(x_A, x_B, x_C) = (0, 0, 0)$. The input to $A$ is the newly updated state $x_C=0$.\n    $$x_A' = H(0 - 0.5) = H(-0.5) = 0$$\n    The state vector remains $\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe final state after executing schedule $\\pi_2$ is $\\boldsymbol{x}^{(\\pi_2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nFinally, we compute the path-dependence magnitude $D$.\n$$D = \\left\\| \\boldsymbol{x}^{(\\pi_1)} - \\boldsymbol{x}^{(\\pi_2)} \\right\\|_1$$\nThe difference vector is:\n$$\\boldsymbol{x}^{(\\pi_1)} - \\boldsymbol{x}^{(\\pi_2)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nThe $1$-norm is the sum of the absolute values of the components of the vector:\n$$D = |1| + |1| + |1| = 1 + 1 + 1 = 3$$\nThe path-dependence magnitude is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While synchronous scheduling elegantly sidesteps the path-dependence issues of asynchronous models, it introduces its own set of characteristic artifacts. This practice focuses on one of the most common challenges: *update collisions*, where multiple agents simultaneously attempt to take a mutually exclusive action, such as moving to the same location . This exercise challenges you to move from a qualitative understanding to a quantitative one by deriving the expected number of collisions in a spatial model, directly linking microscopic agent rules to a macroscopic system property.",
            "id": "4142125",
            "problem": "Consider a two-dimensional toroidal lattice (periodic boundary conditions) of $M$ cells, each cell having exactly $z$ neighboring cells according to a fixed symmetric neighborhood (for example, $z=4$ for the von Neumann neighborhood or $z=8$ for the Moore neighborhood). At most one agent can occupy a cell, and initial occupancies are independent and identically distributed (IID) with occupation probability (agent density) $\\rho \\in [0,1]$, so the expected number of agents is $N=\\rho M$. Under synchronous scheduling, each agent simultaneously selects one neighboring cell uniformly at random from its $z$ neighbors and issues a single movement claim to that chosen cell. A collision is defined as any cell that receives at least two movement claims within the same synchronous step. Using only the foundational definitions of Bernoulli trials and the binomial distribution, derive a closed-form analytic expression for the expected number of collision cells as a function of the agent density $\\rho$, the neighborhood size $z$, and the number of cells $M$. Express your final answer as a single closed-form expression and do not perform any rounding.",
            "solution": "The problem asks for the expected number of collision cells in a synchronous agent-based model on a toroidal lattice. Let $C$ be the random variable representing the total number of collision cells. The objective is to compute the expectation of $C$, denoted as $E[C]$.\n\nWe can approach this problem by utilizing the linearity of expectation. Let the cells of the lattice be indexed from $j=1$ to $M$. We define a set of indicator random variables $\\{I_j\\}_{j=1}^{M}$, where $I_j=1$ if cell $j$ is a collision cell, and $I_j=0$ otherwise. The total number of collision cells is the sum of these indicators:\n$$C = \\sum_{j=1}^{M} I_j$$\nBy the linearity of expectation, the expected total number of collision cells is the sum of the expectations of the individual indicator variables:\n$$E[C] = E\\left[\\sum_{j=1}^{M} I_j\\right] = \\sum_{j=1}^{M} E[I_j]$$\nThe expectation of an indicator variable is equal to the probability of the event it indicates. Thus, $E[I_j] = P(I_j=1)$, which is the probability that cell $j$ becomes a collision cell.\n\nThe problem states that the lattice has periodic boundary conditions (it is a torus), the neighborhood structure is symmetric and identical for every cell, and the initial agent distribution is IID. Due to this systemic homogeneity, the probability of any given cell $j$ being a collision cell is the same for all $j$. Let us denote this constant probability by $P_{coll}$.\n$$E[I_j] = P(I_j=1) = P_{coll} \\quad \\forall j \\in \\{1, 2, \\dots, M\\}$$\nTherefore, the expected number of collision cells simplifies to:\n$$E[C] = \\sum_{j=1}^{M} P_{coll} = M \\cdot P_{coll}$$\nThe problem is now reduced to calculating $P_{coll}$ for an arbitrary cell. Let's focus on a single, arbitrary cell, which we can label as cell $j$.\n\nA collision is defined as a cell receiving at least two movement claims in a single synchronous step. Let $K_j$ be the random variable for the number of movement claims received by cell $j$. A collision occurs at cell $j$ if the event $\\{K_j \\ge 2\\}$ occurs. Thus,\n$$P_{coll} = P(K_j \\ge 2)$$\nIt is often easier to compute this probability using the complement rule:\n$$P_{coll} = 1 - P(K_j < 2) = 1 - P(K_j = 0) - P(K_j = 1)$$\nA movement claim can be issued to cell $j$ only by an agent residing in one of its neighboring cells. The problem specifies that each cell has exactly $z$ neighbors. Let $\\mathcal{N}(j)$ be the set of $z$ neighbors of cell $j$.\n\nWe now model the number of claims $K_j$ by considering each neighbor in $\\mathcal{N}(j)$ as a source of a potential claim. For each neighbor $k \\in \\mathcal{N}(j)$, a claim is sent to cell $j$ if two conditions are met:\n1.  Cell $k$ is occupied by an agent. The probability of this event is given as the agent density, $\\rho$.\n2.  The agent at cell $k$, if present, chooses to move to cell $j$. Since each agent selects one of its $z$ neighbors uniformly at random, and the neighborhood is symmetric (if $k \\in \\mathcal{N}(j)$, then $j \\in \\mathcal{N}(k)$), the probability of this choice is $\\frac{1}{z}$.\n\nThe event of an agent being at cell $k$ and its movement choice are independent. Thus, the probability that a specific neighbor $k \\in \\mathcal{N}(j)$ sends a claim to cell $j$ is the product of these probabilities. Let's call this probability $p$:\n$$p = P(\\text{claim from } k) = P(\\text{agent at } k) \\times P(\\text{agent at } k \\text{ moves to } j) = \\rho \\cdot \\frac{1}{z} = \\frac{\\rho}{z}$$\nThe problem states that initial agent occupancies are IID. The movement decisions of different agents are also independent events. Therefore, whether cell $j$ receives a claim from one neighbor is an event independent of whether it receives a claim from any other neighbor. This establishes a sequence of $z$ independent Bernoulli trials, where each trial corresponds to a neighbor, and \"success\" corresponds to that neighbor sending a claim to cell $j$. The probability of success for each trial is $p = \\frac{\\rho}{z}$.\n\nThe total number of claims, $K_j$, is the total number of successes in these $z$ trials. This follows a binomial distribution with parameters $n=z$ and $p=\\frac{\\rho}{z}$. We write this as $K_j \\sim \\text{Binomial}(n=z, p=\\frac{\\rho}{z})$.\n\nThe probability mass function for the binomial distribution is given by $P(K_j=k) = \\binom{n}{k} p^k (1-p)^{n-k}$. We can now calculate the probabilities for $K_j=0$ and $K_j=1$:\nFor $K_j=0$:\n$$P(K_j=0) = \\binom{z}{0} \\left(\\frac{\\rho}{z}\\right)^0 \\left(1 - \\frac{\\rho}{z}\\right)^{z-0} = 1 \\cdot 1 \\cdot \\left(1 - \\frac{\\rho}{z}\\right)^z = \\left(1 - \\frac{\\rho}{z}\\right)^z$$\nFor $K_j=1$:\n$$P(K_j=1) = \\binom{z}{1} \\left(\\frac{\\rho}{z}\\right)^1 \\left(1 - \\frac{\\rho}{z}\\right)^{z-1} = z \\cdot \\frac{\\rho}{z} \\cdot \\left(1 - \\frac{\\rho}{z}\\right)^{z-1} = \\rho \\left(1 - \\frac{\\rho}{z}\\right)^{z-1}$$\nNow, we can substitute these results into the expression for $P_{coll}$:\n$$P_{coll} = 1 - P(K_j=0) - P(K_j=1) = 1 - \\left(1 - \\frac{\\rho}{z}\\right)^z - \\rho \\left(1 - \\frac{\\rho}{z}\\right)^{z-1}$$\nFinally, we substitute this expression for $P_{coll}$ back into our equation for the total expected number of collision cells, $E[C] = M \\cdot P_{coll}$:\n$$E[C] = M \\left[ 1 - \\left(1 - \\frac{\\rho}{z}\\right)^z - \\rho \\left(1 - \\frac{\\rho}{z}\\right)^{z-1} \\right]$$\nThis is the closed-form analytic expression for the expected number of collision cells as a function of the agent density $\\rho$, the neighborhood size $z$, and the number of cells $M$.",
            "answer": "$$\\boxed{M \\left[ 1 - \\left(1 - \\frac{\\rho}{z}\\right)^z - \\rho \\left(1 - \\frac{\\rho}{z}\\right)^{z-1} \\right]}$$"
        },
        {
            "introduction": "The distinction between synchronous and asynchronous scheduling is not always a rigid dichotomy. This final exercise bridges the two paradigms by exploring a sophisticated technique to achieve the deterministic results of a synchronous schedule through parallel, asynchronous-style updates . By analyzing the model's underlying interaction graph, you will derive the conditions for a conflict-free parallel schedule and apply the concept of graph coloring to find the most efficient way to implement it, illustrating a profound link between a system's structure and its optimal simulation strategy.",
            "id": "4142171",
            "problem": "Consider a Complex Adaptive System (CAS) consisting of $n$ agents indexed by the vertex set $V$ of an undirected simple graph $G=(V,E)$. Each agent $i \\in V$ has a state $x_i(t) \\in \\mathbb{R}$ at discrete time $t \\in \\mathbb{Z}_{\\ge 0}$. The local update rule for agent $i$ is a deterministic function $F_i$ that depends only on the current state of $i$ and the current states of its neighbors, that is, \n$$x_i(t+1)=F_i\\!\\Big(x_i(t),\\{x_j(t):j\\in N(i)\\}\\Big),$$ \nwhere $N(i)=\\{j\\in V:(i,j)\\in E\\}$ is the open neighborhood of $i$. The synchronous schedule applies all local update rules simultaneously at time $t$ to produce $x(t+1)$ from $x(t)$ in one step. An asynchronous schedule performs updates on subsets of agents in parallel, and may sequence multiple such parallel subsets to complete one macro-step from $t$ to $t+1$.\n\nYou are asked to reason from first principles of dependency graphs and local update semantics to derive conditions under which an asynchronous schedule restricted to independent sets of $G$ (sets of vertices with no internal edges) can exactly mimic the synchronous schedule for one macro-step, while avoiding write-write and read-write conflicts. Specifically:\n\n- Derive necessary and sufficient conditions on $G$, on the family of local update functions $\\{F_i\\}_{i\\in V}$, and on the asynchronous scheduling protocol such that performing parallel updates on color classes $C_1,\\dots,C_m$ that form a proper coloring of $G$ yields the same $x(t+1)$ as the synchronous schedule, for all initial states $x(t)$.\n\n- Your derivation must start from core definitions: neighborhood locality, independence in graphs, and the semantics of reading from a time-$t$ snapshot and writing to a time-$(t+1)$ buffer. Do not assume or quote any shortcut formula beyond these foundational notions.\n\nAfter establishing these conditions, compute the minimal number of colors required to form such conflict-free parallel update sets for the following specific interaction graph $G$:\n- The vertex set is $V=\\{1,2,3,4,5,6,7,8,9,10\\}$.\n- The edge set $E$ consists of the pairs \n$$\\{(1,2),(1,3),(1,4),(2,3),(2,4),(3,4),(1,5),(2,5),(2,6),(3,6),(3,7),(4,7),(4,8),(1,8),(1,9),(3,9),(2,10),(4,10)\\}.$$\n\nInterpret the edges as undirected adjacencies. The final answer must be the minimal number of colors needed for a proper coloring of $G$ that enables parallel independent-set updates to mimic synchronous updates as per your derived conditions. Express the final answer as a single integer. No rounding is required and no physical units are involved.",
            "solution": "The problem asks for two main components: first, to derive the necessary and sufficient conditions under which a specific asynchronous updating schedule can exactly mimic a synchronous one for a general Complex Adaptive System (CAS) on a graph; second, to apply these findings to compute a specific property of a given graph $G$.\n\n### Part 1: Derivation of Conditions\n\nLet the system be defined by the graph $G=(V,E)$, with agent states $x(t) = (x_i(t))_{i \\in V}$ at discrete time $t \\in \\mathbb{Z}_{\\ge 0}$.\n\n**1. Semantics of the Synchronous Schedule**\n\nIn a synchronous update, all agents' next states are computed based on the complete state of the system at the current time $t$. For every agent $i \\in V$, the new state $x_i(t+1)_{\\text{sync}}$ is given by:\n$$x_i(t+1)_{\\text{sync}} = F_i\\Big(x_i(t), \\{x_j(t) : j \\in N(i)\\}\\Big)$$\nThe crucial principle is that every function $F_i$ across the entire system reads from the same \"time-$t$ snapshot\" of the state vector $x(t)$. The collection of all results $\\{x_i(t+1)_{\\text{sync}}\\}_{i \\in V}$ forms the new state vector $x(t+1)$. This can be conceptualized as a two-buffer process:\n- A `read` buffer contains $x(t)$ and is not modified during the update step.\n- A `write` buffer, initially empty, is populated with the new states $x_i(t+1)$.\nAfter all computations are complete, the `write` buffer contains the full state vector $x(t+1)$.\n\n**2. Semantics of the Asynchronous Schedule**\n\nThe proposed asynchronous schedule partitions the vertex set $V$ into disjoint independent sets $C_1, C_2, \\dots, C_m$, which are the color classes of a proper vertex coloring. The schedule proceeds sequentially through these classes, performing parallel updates on all agents within a given class. Let's analyze the data dependencies.\n\nTo exactly mimic the synchronous schedule, the asynchronous protocol must yield a final state $x(t+1)_{\\text{async}}$ such that $x_i(t+1)_{\\text{async}} = x_i(t+1)_{\\text{sync}}$ for all $i \\in V$ and for any arbitrary initial state $x(t)$ and any set of valid local update functions $\\{F_i\\}$.\n\nThis implies that for every agent an $i$, the arguments supplied to its update function $F_i$ must be identical to those in the synchronous case. That is, the update calculation for agent $i$ must use the states of agent $i$ and its neighbors $N(i)$ from the time-$t$ snapshot.\n\nLet's consider two possible implementations of the asynchronous protocol.\n\n**Case A: In-place Updates (Single Buffer)**\nIf the updates are performed \"in-place,\" the state of an agent is overwritten as soon as it is computed. Let $x^{(k)}$ be the state vector after the agents in $C_k$ have been updated, with $x^{(0)} = x(t)$. When updating an agent $i \\in C_p$ (where $p>1$), the computation would be:\n$$x_i' = F_i\\Big(x_i^{(p-1)}, \\{x_j^{(p-1)} : j \\in N(i)\\}\\Big)$$\nFor this to match the synchronous update, we require $x_k^{(p-1)} = x_k(t)$ for all $k$ in the input set of $F_i$, i.e., for $k \\in \\{i\\} \\cup N(i)$.\n- Agent $i$ is in $C_p$, so it has not been updated in steps $1, \\dots, p-1$. Thus, $x_i^{(p-1)} = x_i(t)$ is automatically satisfied.\n- However, for a neighbor $j \\in N(i)$, if $j$ belongs to a color class $C_k$ with $k < p$, its state would have already been updated, meaning $x_j^{(p-1)} \\neq x_j(t)$ in general. This is a read-after-write hazard relative to the synchronous semantics.\n- To prevent this, one would have to require that for any $i \\in C_p$, none of its neighbors $j \\in N(i)$ belong to any class $C_k$ with $k<p$. But since the relationship is symmetric, for the update of $j \\in C_k$ to be correct, none of its neighbors (including $i$) could belong to any class processed before $C_k$. If $k \\neq p$, this creates a contradiction. If $k=p$, it is disallowed because $C_k$ is an independent set. Thus, an in-place update scheme cannot fulfill the requirement.\n\n**Case B: Double-Buffer Updates**\nThe contradiction in the in-place model forces the adoption of a protocol that separates reads and writes, as strongly suggested by the problem's phrasing \"reading from a time-$t$ snapshot and writing to a time-$(t+1)$ buffer\".\n\nUnder this protocol:\n1. All calculations for the macro-step $t \\to t+1$ read from a non-modifiable buffer holding $x(t)$.\n2. The results of the calculations are written to a separate buffer for $x(t+1)$.\n\nWhen updating agents in a class $C_k$ in parallel, the calculation for each $i \\in C_k$ is:\n$$x_i(\\text{new}) = F_i\\Big(x_i(t), \\{x_j(t) : j \\in N(i)\\}\\Big)$$\nThis is identical to the calculation in the synchronous schedule by definition. Since this holds true for all agents in all color classes $C_1, \\dots, C_m$, the final state vector in the `write` buffer will be identical to the one produced by a fully synchronous update.\n\n**3. Conditions and Role of Independent Sets**\n\nWith the double-buffer protocol, the correctness of the final result is guaranteed regardless of the scheduling order. The remaining condition relates to the feasibility of *parallel* updates and the avoidance of conflicts.\n- **Read-Write Conflicts:** Are eliminated by the double-buffer scheme. Reads from one buffer, writes to another.\n- **Write-Write Conflicts:** Agent $i$ writes to state component $x_i$ and agent $j$ to $x_j$. Since these are distinct for $i \\neq j$, there is no conflict at the logical level.\n- **Dependency Cycles:** The requirement to use independent sets ($C_k$) for parallel updates addresses the issue of computational dependencies. If two agents $i$ and $j$ are updated in parallel, we must ensure their computations are independent. The computation for $i$ depends on inputs from its neighborhood $N(i)$ and itself, while the computation for $j$ depends on inputs from its neighborhood $N(j)$ and itself. If $i$ and $j$ were adjacent, then the computation for $i$ would require the state of $j$, and the computation for $j$ would require the state of $i$. While the double-buffer scheme makes the values independent (both read from time $t$), this direct dependency loop is often what parallelization schemes seek to avoid for clean decomposition. By stipulating that any parallel set $C_k$ is an independent set, we ensure that for any $i, j \\in C_k$ with $i \\neq j$, we have $j \\notin N(i)$ and $i \\notin N(j)$. This means no two simultaneously executing update functions read each other's state, preventing such direct dependency cycles.\n\n**Summary of Conditions:**\n- **On the scheduling protocol:** The protocol must implement a double-buffer mechanism, where all state reads for the macro-step derive from the initial state $x(t)$, and all writes are directed to a separate buffer for the final state $x(t+1)$. This is necessary and sufficient to ensure the computed values match the synchronous update.\n- **On the partitioning $\\{C_k\\}$ and graph $G$:** The sets $C_k$ on which parallel updates are performed must be independent sets to prevent direct dependency cycles between concurrent computations. The collection of sets $\\{C_k\\}_{k=1}^m$ must form a partition of the vertex set $V$. This is precisely the definition of a **proper vertex coloring** of $G$. The number of sequential steps, $m$, is the number of colors used.\n- **On the functions $\\{F_i\\}$:** No conditions beyond the problem statement are required. The derivation holds for any deterministic function with the specified local dependencies.\n\nThe minimal number of sequential parallel-update steps required to mimic one synchronous macro-step is therefore the minimum $m$ for which such a partition exists. By definition, this is the **chromatic number** of the graph $G$, denoted $\\chi(G)$.\n\n### Part 2: Computation for the Specific Graph\n\nThe task reduces to finding the chromatic number $\\chi(G)$ for the given graph.\nThe graph is $G=(V,E)$ with $V=\\{1,2,3,4,5,6,7,8,9,10\\}$ and $E=\\{(1,2),(1,3),(1,4),(2,3),(2,4),(3,4),(1,5),(2,5),(2,6),(3,6),(3,7),(4,7),(4,8),(1,8),(1,9),(3,9),(2,10),(4,10)\\}$.\n\n**1. Lower Bound on $\\chi(G)$**\nThe chromatic number of a graph is at least the size of its largest clique (a subset of vertices where every two distinct vertices are adjacent).\nThe vertex subset $\\{1,2,3,4\\}$ has all six possible edges between its vertices: $(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)$. This forms a $4$-clique, or $K_4$.\nA $K_4$ requires $4$ distinct colors. Therefore, $\\chi(G) \\ge 4$.\n\n**2. Upper Bound on $\\chi(G)$**\nWe now attempt to construct a valid $4$-coloring of $G$. If successful, this will prove that $\\chi(G) \\le 4$. Combining with the lower bound, this will establish $\\chi(G)=4$.\nLet the four colors be $c_1, c_2, c_3, c_4$.\n\n- First, color the vertices of the $K_4$ clique, which requires $4$ colors:\n  - color$(1) = c_1$\n  - color$(2) = c_2$\n  - color$(3) = c_3$\n  - color$(4) = c_4$\n\n- Next, color the remaining vertices $\\{5,6,7,8,9,10\\}$ ensuring no adjacent vertices share a color. We use a greedy approach.\n  - **Vertex 5:** $N(5)=\\{1,2\\}$. Colors of neighbors are $\\{c_1, c_2\\}$. We can assign color$(5) = c_3$.\n  - **Vertex 6:** $N(6)=\\{2,3\\}$. Colors of neighbors are $\\{c_2, c_3\\}$. We can assign color$(6) = c_1$.\n  - **Vertex 7:** $N(7)=\\{3,4\\}$. Colors of neighbors are $\\{c_3, c_4\\}$. We can assign color$(7) = c_1$.\n  - **Vertex 8:** $N(8)=\\{1,4\\}$. Colors of neighbors are $\\{c_1, c_4\\}$. We can assign color$(8) = c_2$.\n  - **Vertex 9:** $N(9)=\\{1,3\\}$. Colors of neighbors are $\\{c_1, c_3\\}$. We can assign color$(9) = c_2$.\n  - **Vertex 10:** $N(10)=\\{2,4\\}$. Colors of neighbors are $\\{c_2, c_4\\}$. We can assign color$(10) = c_1$.\n\nThis procedure assigns a color to every vertex. We must verify that this is a proper coloring. The coloring of $\\{1,2,3,4\\}$ is valid by construction. The coloring of each outer vertex $\\{5, \\dots, 10\\}$ is valid with respect to its neighbors in the core clique. The only remaining check is for edges between the outer vertices themselves. The given edge set $E$ contains no edges connecting any two vertices from the set $\\{5,6,7,8,9,10\\}$. This set is an independent set, so no conflicts can arise among these vertices.\n\nThe resulting color classes are:\n- $C_1 = \\{1, 6, 7, 10\\}$\n- $C_2 = \\{2, 8, 9\\}$\n- $C_3 = \\{3, 5\\}$\n- $C_4 = \\{4\\}$\n\nWe have successfully constructed a valid $4$-coloring of $G$. Thus, $\\chi(G) \\le 4$.\n\n**3. Conclusion**\nSince we have shown that $4 \\le \\chi(G)$ and $\\chi(G) \\le 4$, we can conclude that the chromatic number of $G$ is exactly $4$. The minimal number of colors required is $4$.",
            "answer": "$$\\boxed{4}$$"
        }
    ]
}