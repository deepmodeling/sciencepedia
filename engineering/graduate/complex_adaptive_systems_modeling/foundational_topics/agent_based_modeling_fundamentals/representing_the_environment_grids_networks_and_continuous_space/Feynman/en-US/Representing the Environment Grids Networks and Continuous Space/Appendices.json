{
    "hands_on_practices": [
        {
            "introduction": "This problem tackles a fundamental aspect of agent-based modeling on grids: the profound effect of boundary conditions on global dynamics. By deriving and comparing the Mean First Passage Time ($MFPT$) for a simple random walker under reflecting versus absorbing boundary conditions, you will gain a quantitative understanding of how local rules at the edge of a simulated world can alter expected transit times throughout the entire system. This exercise  hones skills in setting up and solving recurrence relations, which are ubiquitous in the analysis of discrete stochastic processes.",
            "id": "4140773",
            "problem": "Consider a discrete-time, unbiased random walk on a one-dimensional grid with sites $0, 1, 2, \\dots, L$, where $L \\in \\mathbb{N}$ and $L \\geq 2$. At each interior site $i \\in \\{1, 2, \\dots, L-1\\}$, the walker moves to $i-1$ with probability $1/2$ and to $i+1$ with probability $1/2$. The dynamics at the boundaries differ by scenario, and the target set is the single site $0$.\n\nScenario R (reflecting boundary at $L$): When at $L$, the walker deterministically moves to $L-1$ in one time step. Let $T^{(R)}_i$ denote the Mean First Passage Time (MFPT) to the target set $\\{0\\}$, starting from site $i \\in \\{1, \\dots, L\\}$, under Scenario R.\n\nScenario A (absorbing boundary at $L$): When at $L$, the walker is absorbed and the process terminates. Let $T^{(A|0)}_i$ denote the MFPT to the target set $\\{0\\}$, starting from site $i \\in \\{1, \\dots, L-1\\}$, conditioned on the event that site $0$ is reached before absorption at $L$.\n\nStarting from the foundational definition of MFPT via backward equations for Markov chains and without invoking any pre-derived shortcut formulas, derive expressions for $T^{(R)}_i$ and $T^{(A|0)}_i$ as functions of $L$ and $i$. Then, compute the ratio\n$$\n\\frac{T^{(A|0)}_i}{T^{(R)}_i}\n$$\nand provide it in simplest exact form. No rounding is required. The final answer must be a single real number written as a simplified fraction.",
            "solution": "The Mean First Passage Time (MFPT) to a target set, starting from a site $i$, can be found by solving the backward Kolmogorov equation. Let $E_i$ be the MFPT starting from site $i$. By conditioning on the first step, we have the general relation:\n$$\nE_i = 1 + \\sum_{j} P_{ij} E_j\n$$\nwhere $P_{ij}$ is the probability of transitioning from site $i$ to site $j$ in one step. The time for any site within the target set is $0$.\n\n**Part 1: Scenario R (Reflecting boundary at $L$)**\n\nLet $E_i = T^{(R)}_i$. The target site is $0$, so we have the boundary condition $E_0 = 0$.\nFor the interior sites $i \\in \\{1, 2, \\dots, L-1\\}$, the walker moves to $i-1$ or $i+1$ with equal probability $1/2$. The equation is:\n$$\nE_i = 1 + \\frac{1}{2} E_{i-1} + \\frac{1}{2} E_{i+1}\n$$\nAt the reflecting boundary $i=L$, the walker deterministically moves to $L-1$:\n$$\nE_L = 1 + E_{L-1}\n$$\nRearranging the equation for the interior sites gives a second-order linear difference equation:\n$$\nE_{i+1} - 2E_i + E_{i-1} = -2\n$$\nLet us define the difference $\\Delta_i = E_i - E_{i-1}$. The equation becomes:\n$$\n(E_{i+1} - E_i) - (E_i - E_{i-1}) = -2 \\implies \\Delta_{i+1} - \\Delta_i = -2\n$$\nThis shows that the differences $\\Delta_i$ form an arithmetic progression with a common difference of $-2$. Thus, $\\Delta_i$ can be expressed in terms of $\\Delta_1$:\n$$\n\\Delta_i = \\Delta_1 - 2(i-1)\n$$\nSince $E_0=0$, we have $\\Delta_1 = E_1 - E_0 = E_1$. So, $\\Delta_i = E_1 - 2(i-1)$.\nWe can find $E_i$ by summing the differences:\n$$\nE_i = E_i - E_0 = \\sum_{k=1}^{i} (E_k - E_{k-1}) = \\sum_{k=1}^{i} \\Delta_k\n$$\n$$\nE_i = \\sum_{k=1}^{i} (E_1 - 2(k-1)) = iE_1 - 2 \\sum_{k=1}^{i} (k-1) = iE_1 - 2 \\frac{(i-1)i}{2} = iE_1 - i(i-1)\n$$\nTo find the unknown $E_1$, we use the boundary condition at $L$: $E_L = 1 + E_{L-1}$. Using our expression for $E_i$:\n$$\nL E_1 - L(L-1) = 1 + \\left[ (L-1)E_1 - (L-1)(L-2) \\right]\n$$\n$$\nL E_1 - L^2 + L = 1 + (L-1)E_1 - (L^2 - 3L + 2)\n$$\n$$\nL E_1 - L^2 + L = (L-1)E_1 - L^2 + 3L - 1\n$$\n$$\n(L - (L-1)) E_1 = (3L - 1) - L\n$$\n$$\nE_1 = 2L - 1\n$$\nSubstituting this value of $E_1$ back into the expression for $E_i$:\n$$\nT^{(R)}_i = E_i = i(2L-1) - i(i-1) = 2Li - i - i^2 + i = 2Li - i^2\n$$\nSo, the MFPT under Scenario R is:\n$$\nT^{(R)}_i = i(2L - i)\n$$\n\n**Part 2: Scenario A (Conditional MFPT with absorbing boundary at $L$)**\n\nLet $F_i = T^{(A|0)}_i$. This is the MFPT to site $0$ conditioned on reaching $0$ before reaching site $L$.\nFirst, we must find the probability $h_i$ of reaching site $0$ before site $L$, starting from site $i$. The quantity $h_i$ satisfies the backward equation for a martingale:\n$$\nh_i = \\frac{1}{2}h_{i-1} + \\frac{1}{2}h_{i+1}\n$$\nwith boundary conditions $h_0=1$ (success at target) and $h_L=0$ (failure at absorbing boundary). The recurrence implies $h_i$ is a linear function of $i$, $h_i = ai+b$.\nUsing the boundary conditions:\n$h_0 = 1 \\implies a(0)+b = 1 \\implies b=1$.\n$h_L = 0 \\implies aL+b = 0 \\implies aL+1=0 \\implies a = -1/L$.\nThus, the success probability is $h_i = 1 - \\frac{i}{L} = \\frac{L-i}{L}$.\n\nThe backward equation for the conditional MFPT, $F_i$, is given by weighting the future expectations by the conditional probabilities of following that path to success:\n$$\nF_i = 1 + \\frac{P(i \\to i-1)h_{i-1}}{h_i} F_{i-1} + \\frac{P(i \\to i+1)h_{i+1}}{h_i} F_{i+1}\n$$\nSubstituting the probabilities and the expression for $h_i$:\n$$\nF_i = 1 + \\frac{(1/2) \\frac{L-(i-1)}{L}}{\\frac{L-i}{L}} F_{i-1} + \\frac{(1/2) \\frac{L-(i+1)}{L}}{\\frac{L-i}{L}} F_{i+1}\n$$\n$$\nF_i = 1 + \\frac{L-i+1}{2(L-i)} F_{i-1} + \\frac{L-i-1}{2(L-i)} F_{i+1} \\quad \\text{for } i \\in \\{1, \\dots, L-1\\}\n$$\nThe boundary condition is $F_0=0$. In terms of differences $\\delta_i = F_i - F_{i-1}$, the recurrence can be shown to lead to the solution $\\delta_i = \\frac{2(L-i)+1}{3}$.\nWe find $F_i$ by summing the differences, using $F_0=0$:\n$$\nF_i = \\sum_{k=1}^{i} \\delta_k = \\sum_{k=1}^{i} \\frac{2(L-k)+1}{3} = \\frac{1}{3} \\sum_{k=1}^{i} (2L+1-2k)\n$$\n$$\nF_i = \\frac{1}{3} \\left( i(2L+1) - 2\\sum_{k=1}^{i} k \\right) = \\frac{1}{3} \\left( i(2L+1) - 2\\frac{i(i+1)}{2} \\right)\n$$\n$$\nF_i = \\frac{1}{3} \\left( i(2L+1) - i(i+1) \\right) = \\frac{i}{3} ( (2L+1) - (i+1) ) = \\frac{i}{3} (2L-i)\n$$\nSo, the conditional MFPT under Scenario A is:\n$$\nT^{(A|0)}_i = \\frac{i(2L-i)}{3}\n$$\n\n**Part 3: Compute the Ratio**\n\nWe are asked to compute the ratio $\\frac{T^{(A|0)}_i}{T^{(R)}_i}$ for $i \\in \\{1. \\dots, L-1\\}$.\nUsing the expressions derived above:\n$$\nT^{(R)}_i = i(2L - i)\n$$\n$$\nT^{(A|0)}_i = \\frac{i(2L - i)}{3}\n$$\nThe ratio is:\n$$\n\\frac{T^{(A|0)}_i}{T^{(R)}_i} = \\frac{\\frac{i(2L - i)}{3}}{i(2L - i)}\n$$\nFor $i \\in \\{1, \\dots, L-1\\}$, the term $i(2L-i)$ is non-zero, so we can cancel it.\n$$\n\\frac{T^{(A|0)}_i}{T^{(R)}_i} = \\frac{1}{3}\n$$\nThe ratio is a constant, independent of the starting position $i$ and the system size $L$.",
            "answer": "$$\n\\boxed{\\frac{1}{3}}\n$$"
        },
        {
            "introduction": "Networks are central to modeling complex systems, but their global structure can change dramatically with small variations in their connectivity. This exercise explores one of the most celebrated results in random graph theory: the emergence of a giant component. By using a branching process approximation, you will derive the critical threshold at which a sparse, fragmented network coalesces into one with a single, massive connected component . This practice provides deep insight into phase transitions, a concept crucial for understanding phenomena like disease percolation, information diffusion, and system resilience.",
            "id": "4140762",
            "problem": "A population of interacting agents is embedded in a stochastic network used to represent the environment, modeled as an Erdős–Rényi (ER) random graph $G(n,p)$: there are $n$ labeled vertices and each of the $\\frac{n(n-1)}{2}$ possible undirected edges is present independently with probability $p$. The degree distribution of a uniformly chosen vertex is $\\mathrm{Binomial}(n-1,p)$, and for large $n$ the graph is locally tree-like at finite distances with high probability. Let $p = p(n)$ and define $c_{n} \\equiv n p(n)$. Working in the asymptotic regime $n \\to \\infty$, use only these foundational properties to determine the exact threshold value $c^{\\star}$ such that if $c_{n} \\to c < c^{\\star}$ then the largest connected component has size $o(n)$ with high probability, whereas if $c_{n} \\to c > c^{\\star}$ then with high probability the graph contains a connected component of size $\\Theta(n)$. Provide the value of $c^{\\star}$ as a single exact real number. Express your final answer as an exact value with no rounding.",
            "solution": "The problem asks for the critical threshold for the emergence of a \"giant\" connected component in an Erdős–Rényi random graph $G(n,p)$. The transition from a state where all components are small (of size $o(n)$) to a state with a single large component (of size $\\Theta(n)$) is a classic phase transition phenomenon. We can determine the threshold $c^{\\star}$ by modeling the exploration of a connected component as a branching process, an approach justified by the given property that the graph is locally tree-like for large $n$.\n\nLet us consider the process of exploring the connected component containing a randomly selected vertex. This can be viewed as a breadth-first search. We start with a single vertex (generation $0$). Its neighbors form generation $1$. The neighbors of generation $1$ vertices (not already discovered) form generation $2$, and so on. In the limit $n \\to \\infty$, for a finite number of steps, the probability of encountering a vertex that has already been visited is negligible. This allows us to approximate the component exploration as a Galton-Watson branching process, where vertices are individuals and edges to new vertices represent offspring.\n\nThe fate of a branching process (extinction versus indefinite survival) is determined by the mean number of offspring per individual, which we denote as $\\mu$. A fundamental theorem of branching processes states that if $\\mu > 1$, there is a non-zero probability of survival, whereas if $\\mu \\le 1$, the process becomes extinct with probability $1$. In our graph context, \"survival\" corresponds to a giant component of size $\\Theta(n)$, and \"extinction\" corresponds to a small component of size $o(n)$. Thus, the phase transition occurs at $\\mu=1$.\n\nOur task reduces to calculating the mean number of offspring $\\mu$ as a function of the parameter $c = \\lim_{n \\to \\infty} c_n = \\lim_{n \\to \\infty} n p(n)$. The \"offspring\" of a vertex $v$ in the exploration process are its neighbors, excluding the vertex from which $v$ was discovered. The number of such offspring is therefore $d(v) - 1$, where $d(v)$ is the degree of $v$.\n\nFirst, we establish the degree distribution of a vertex chosen uniformly at random. The problem states this is a binomial distribution, $D \\sim \\mathrm{Binomial}(n-1, p)$. In the asymptotic regime where $n \\to \\infty$ and $p = c_n/n$ with $c_n \\to c$, this binomial distribution converges to a Poisson distribution with mean $c$. The probability mass function for a vertex to have degree $k$ becomes:\n$$ \\mathbb{P}(D=k) \\to \\frac{c^k e^{-c}}{k!} $$\nThe mean degree is $\\langle D \\rangle = \\sum_{k=0}^{\\infty} k \\, \\mathbb{P}(D=k) = c$.\n\nNext, we must find the distribution for the number of offspring. For any vertex in the branching process other than the root, we arrive at it by traversing an edge. The degree of a vertex reached by following a random edge does not follow the same $\\mathrm{Poisson}(c)$ distribution. High-degree vertices are incident to more edges, so they are more likely to be selected. The probability of an edge leading to a vertex of degree $k$ is proportional to $k \\mathbb{P}(D=k)$. The resulting distribution for the degree of a vertex at the end of a random edge, $D_{\\text{edge}}$, is:\n$$ \\mathbb{P}(D_{\\text{edge}}=k) = \\frac{k \\mathbb{P}(D=k)}{\\sum_j j \\mathbb{P}(D=j)} = \\frac{k \\mathbb{P}(D=k)}{\\langle D \\rangle} $$\nThe number of offspring for such a vertex is its degree minus one, $D_{\\text{edge}} - 1$. The mean number of offspring, $\\mu$, is the expectation of this quantity:\n$$ \\mu = \\mathbb{E}[D_{\\text{edge}} - 1] = \\mathbb{E}[D_{\\text{edge}}] - 1 $$\nThe expectation $\\mathbb{E}[D_{\\text{edge}}]$ is:\n$$ \\mathbb{E}[D_{\\text{edge}}] = \\sum_{k=1}^{\\infty} k \\, \\mathbb{P}(D_{\\text{edge}}=k) = \\sum_{k=1}^{\\infty} k \\frac{k \\mathbb{P}(D=k)}{\\langle D \\rangle} = \\frac{1}{\\langle D \\rangle} \\sum_{k=0}^{\\infty} k^2 \\mathbb{P}(D=k) = \\frac{\\langle D^2 \\rangle}{\\langle D \\rangle} $$\nFor a Poisson distribution with parameter $c$, the mean is $\\langle D \\rangle = c$ and the variance is $\\mathrm{Var}(D) = c$. The second moment is given by $\\langle D^2 \\rangle = \\mathrm{Var}(D) + \\langle D \\rangle^2 = c + c^2$.\nSubstituting these values, we find the mean number of offspring:\n$$ \\mu = \\frac{\\langle D^2 \\rangle}{\\langle D \\rangle} - 1 = \\frac{c + c^2}{c} - 1 = (1+c) - 1 = c $$\nThe mean number of offspring in the branching process approximation is equal to $c$, the asymptotic average degree of the graph.\n\nThe critical threshold for the phase transition corresponds to the point where the mean number of offspring is exactly $1$.\n$$ \\mu = 1 $$\n$$ c = 1 $$\nTherefore, the threshold value is $c^{\\star} = 1$. If $c_n \\to c < 1$, the branching process dies out, implying the largest component is of size $o(n)$. If $c_n \\to c > 1$, the branching process survives with non-zero probability, implying the existence of a giant component of size $\\Theta(n)$. This aligns perfectly with the conditions given in the problem statement. The exact threshold value is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Many models in complex systems require placing agents or events in a continuous space according to a specified, non-uniform density. This practice  provides both the theoretical foundation and the practical implementation of the thinning algorithm, a cornerstone method for simulating inhomogeneous Poisson point processes. By starting with a simple homogeneous process and selectively \"thinning\" out points, this technique allows for the generation of complex spatial patterns. You will not only implement the algorithm but also formally prove its correctness, connecting abstract probability theory to a tangible and powerful simulation tool.",
            "id": "4140718",
            "problem": "You are tasked with designing and implementing a thinning algorithm to simulate inhomogeneous Poisson point processes on continuous domains, together with a principle-based justification of correctness grounded in first principles. An inhomogeneous Poisson point process on a measurable domain $\\Omega \\subset \\mathbb{R}^{d}$ with a nonnegative, integrable intensity function $\\lambda(x)$ is defined by the following two properties: for every bounded measurable subset $A \\subset \\Omega$, the point count $N(A)$ is Poisson distributed with mean $\\int_{A} \\lambda(x)\\,dx$, and for pairwise disjoint measurable subsets $A_{1}, \\dots, A_{k}$ the random variables $N(A_{1}), \\dots, N(A_{k})$ are independent.\n\nYou must derive and implement a thinning-based simulator starting from a homogeneous Poisson point process on the same domain with constant rate $\\Lambda_{\\max}$, where $\\Lambda_{\\max} \\ge \\sup_{x \\in \\Omega} \\lambda(x)$. The algorithm must be expressed purely in mathematical terms and implemented in code. Angles must be interpreted in radians.\n\nThe fundamental base you may assume includes the definition of a Poisson point process, properties of the Poisson distribution, and standard independence and conditioning rules. The target thinning algorithm must not be assumed; it must be derived from the base. You must also justify correctness by showing that the output of your simulator satisfies the defining properties of the inhomogeneous Poisson point process.\n\nAlgorithmic task: implement the following thinning simulator for two environments that represent continuous space.\n\n1. For one-dimensional time on the interval $[0, T]$ with intensity $\\lambda(t)$ and bound $\\Lambda_{\\max}$:\n   - Generate a homogeneous Poisson process with rate $\\Lambda_{\\max}$ on $[0, T]$ by drawing a Poisson number $N \\sim \\mathrm{Poisson}(\\Lambda_{\\max} T)$ of candidate times and placing them independently and uniformly in $[0, T]$.\n   - For each candidate time $t$, retain it independently with probability $p(t) = \\lambda(t)/\\Lambda_{\\max}$, and discard it otherwise.\n   - Sort the retained times to obtain the simulated inhomogeneous process.\n\n2. For two-dimensional space on the rectangle $[a_{x}, b_{x}] \\times [a_{y}, b_{y}]$ with intensity $\\lambda(x,y)$ and bound $\\Lambda_{\\max}$:\n   - Generate a homogeneous Poisson point process with rate $\\Lambda_{\\max}$ on the rectangle by drawing $N \\sim \\mathrm{Poisson}(\\Lambda_{\\max} |[a_{x}, b_{x}] \\times [a_{y}, b_{y}]|)$ candidate points independently and uniformly in the rectangle.\n   - For each candidate point $(x, y)$, retain it independently with probability $p(x,y) = \\lambda(x,y)/\\Lambda_{\\max}$, and discard it otherwise.\n\nFor the one-dimensional case, define the integrated intensity $\\Lambda(t) = \\int_{0}^{t} \\lambda(s)\\,ds$. For a realization with retained times $0 < t_{1} < \\cdots < t_{n} \\le T$, define the transformed increments $e_{i} = \\Lambda(t_{i}) - \\Lambda(t_{i-1})$, with the convention $t_{0} = 0$. The time-rescaling theorem states that if $(t_{i})$ is an inhomogeneous Poisson process with intensity $\\lambda(t)$, then $(e_{i})$ are independent and identically distributed as $\\mathrm{Exponential}(1)$.\n\nYour implementation must use the above algorithm with the following fixed test suite to produce deterministic outputs. You must use the specified seeds for the pseudo-random number generator for reproducibility. All angles are in radians. No physical units are involved.\n\nTest suite:\n\n- Test $1$ (one-dimensional sinusoidal intensity): simulate on $[0, T]$ with $T = 10$, $\\lambda(t) = 2 + \\sin(t)$, and $\\Lambda_{\\max} = 3$. Use pseudo-random seed $314159$. Let $n_{1}$ be the number of retained events and let $D_{1}$ be the Kolmogorov–Smirnov statistic comparing the empirical distribution of $(e_{i})$ to the $\\mathrm{Exponential}(1)$ distribution via the one-sample Kolmogorov–Smirnov test.\n\n- Test $2$ (two-dimensional linear intensity in $x$): simulate on $[a_{x}, b_{x}] \\times [a_{y}, b_{y}] = [0, 1] \\times [0, 1]$ with $\\lambda(x,y) = 10 (1 + x)$ and $\\Lambda_{\\max} = 20$. Use pseudo-random seed $271828$. Let $n_{2}$ be the number of retained points, let $\\bar{x}_{2}$ be the sample mean of the $x$-coordinates of retained points, and let $\\delta_{2} = \\bar{x}_{2} - \\frac{5}{9}$, where $\\frac{5}{9}$ is the exact expected $x$-mean under the normalized density proportional to $1+x$ on $[0,1]$.\n\n- Test $3$ (one-dimensional piecewise intensity with a zero region): simulate on $[0, T]$ with $T = 5$, $\\lambda(t) = 0$ for $t \\in [0, 2.5]$ and $\\lambda(t) = 4$ for $t \\in (2.5, 5]$, and $\\Lambda_{\\max} = 4$. Use pseudo-random seed $161803$. Let $n_{3}^{(L)}$ be the number of retained events in $[0, 2.5]$ and $n_{3}^{(R)}$ be the number of retained events in $(2.5, 5]$.\n\n- Test $4$ (one-dimensional very low homogeneous intensity): simulate on $[0, T]$ with $T = 10$, $\\lambda(t) = 10^{-3}$ and $\\Lambda_{\\max} = 10^{-3}$. Use pseudo-random seed $141421$. Let $n_{4}$ be the total number of retained events.\n\nAnswer specification:\n\n- Your program must output a single line containing the results of the test suite in the following order and types: $[n_{1}, D_{1}, n_{2}, \\bar{x}_{2}, \\delta_{2}, n_{3}^{(L)}, n_{3}^{(R)}, n_{4}]$.\n- Floats $D_{1}$, $\\bar{x}_{2}$, and $\\delta_{2}$ must be rounded to exactly six digits after the decimal point. Integers must be printed without rounding. The output must be a single line with a comma-separated list enclosed in square brackets and contain no spaces, for example $[42,0.123456,17,0.555556,0.000000,0,9,0]$.",
            "solution": "The task is to derive and justify the correctness of the thinning algorithm for simulating an inhomogeneous Poisson point process (IPPP), and then to implement this algorithm for a defined test suite. The derivation must be from first principles.\n\nAn IPPP on a domain $\\Omega \\subset \\mathbb{R}^{d}$ with a non-negative, integrable intensity function $\\lambda(x)$ is characterized by two defining properties:\n$1$. For any bounded measurable subset $A \\subset \\Omega$, the point count $N(A)$ follows a Poisson distribution with mean $\\mu(A) = \\int_{A} \\lambda(x)\\,dx$.\n$2$. For any finite collection of pairwise disjoint measurable subsets $A_{1}, \\dots, A_{k} \\subset \\Omega$, the random variables $N(A_{1}), \\dots, N(A_{k})$ are independent.\n\nThe thinning algorithm simulates such a process. It begins with a homogeneous Poisson point process (HPPP), denoted $\\mathcal{P}_{\\text{hom}}$, on the same domain $\\Omega$ with a constant rate $\\Lambda_{\\max}$, where $\\Lambda_{\\max}$ is an upper bound for the intensity function, i.e., $\\Lambda_{\\max} \\ge \\sup_{x \\in \\Omega} \\lambda(x)$. A new point process, $\\mathcal{P}_{\\text{thin}}$, is then generated by independently retaining each point $x$ from $\\mathcal{P}_{\\text{hom}}$ with a location-dependent probability $p(x) = \\lambda(x) / \\Lambda_{\\max}$. This procedure is known as thinning. We must now prove that the resulting process $\\mathcal{P}_{\\text{thin}}$ is indeed an IPPP with intensity $\\lambda(x)$.\n\n**Proof of Correctness**\n\nWe must verify that $\\mathcal{P}_{\\text{thin}}$ satisfies the two defining properties of an IPPP.\n\n**1. Point Count Distribution**\nLet $A$ be a bounded measurable subset of $\\Omega$. We need to show that the number of points from the thinned process located in $A$, denoted $N_{\\text{thin}}(A)$, follows a Poisson distribution with mean $\\int_{A} \\lambda(x)\\,dx$.\n\nThe number of points from the HPPP in the set $A$, denoted $N_{\\text{hom}}(A)$, is a random variable following a Poisson distribution with mean $\\Lambda_{\\max} |A|$, where $|A|$ is the Lebesgue measure of $A$. Given that $N_{\\text{hom}}(A) = k$, the $k$ points of the HPPP are independently and uniformly distributed within the set $A$. The decision to retain each point is an independent Bernoulli trial with a location-dependent success probability. A key property of Poisson processes (known as Poisson splitting or marking) states that if a process with mean $\\mu$ is thinned, where each event at location $y$ is kept with probability $p(y)$, the resulting process is also Poisson with a new mean.\n\nFormally, we can use the law of total probability. The probability of having $n$ points in the thinned process in region $A$ is:\n$$ P(N_{\\text{thin}}(A)=n) = \\sum_{k=n}^{\\infty} P(N_{\\text{thin}}(A)=n | N_{\\text{hom}}(A)=k) P(N_{\\text{hom}}(A)=k) $$\nThe conditional probability is that of a Binomial distribution $B(k, \\bar{p}_A)$, where $\\bar{p}_A = \\frac{1}{|A|} \\int_A \\frac{\\lambda(x)}{\\Lambda_{\\max}} dx$. The sum evaluates to the probability mass function of a Poisson distribution with mean $\\mu_{\\text{thin}} = \\Lambda_{\\max} |A| \\bar{p}_A$.\n$$ \\mu_{\\text{thin}} = \\Lambda_{\\max} |A| \\left( \\frac{1}{\\Lambda_{\\max} |A|} \\int_A \\lambda(y)\\,dy \\right) = \\int_A \\lambda(y)\\,dy $$\nThus, $N_{\\text{thin}}(A) \\sim \\text{Poisson}(\\int_A \\lambda(x)\\,dx)$, satisfying the first property.\n\n**2. Independence of Counts**\nLet $A_1, \\dots, A_k$ be pairwise disjoint measurable subsets of $\\Omega$. We must show that the counts $N_{\\text{thin}}(A_1), \\dots, N_{\\text{thin}}(A_k)$ are independent random variables.\n\nA fundamental property of the HPPP $\\mathcal{P}_{\\text{hom}}$ is that the point processes restricted to disjoint sets are independent. This means the random sets of points $\\mathcal{P}_{\\text{hom}} \\cap A_1, \\dots, \\mathcal{P}_{\\text{hom}} \\cap A_k$ are mutually independent.\n\nThe thinning operation is performed on each point of $\\mathcal{P}_{\\text{hom}}$ independently of all other points. The decision to retain a point $x \\in A_i$ depends only on its location $x$ and an independent random draw. This decision is not affected by the presence or retention of any point in another set $A_j$ for $j \\neq i$.\n\nSince the initial point configurations in the disjoint sets $A_i$ are independent, and the thinning operations within each set are independent of operations in other sets, the resulting thinned point configurations, $\\mathcal{P}_{\\text{thin}} \\cap A_1, \\dots, \\mathcal{P}_{\\text{thin}} \\cap A_k$, are also mutually independent. Consequently, their respective counts, $N_{\\text{thin}}(A_1), \\dots, N_{\\text{thin}}(A_k)$, are independent random variables. This establishes the second property.\n\nBoth defining properties of an IPPP are satisfied. Therefore, the thinning algorithm correctly generates a realization of an inhomogeneous Poisson point process with the desired intensity function $\\lambda(x)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the thinning algorithm simulator.\n    \"\"\"\n    \n    def test_1(seed):\n        \"\"\"\n        Test 1: 1D sinusoidal intensity.\n        Simulates on [0, T] with T = 10, lambda(t) = 2 + sin(t), and Lambda_max = 3.\n        Calculates number of events and KS statistic vs. Exponential(1).\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = 10.0\n        Lambda_max = 3.0\n        \n        lambda_t = lambda t: 2 + np.sin(t)\n        \n        # 1. Generate homogeneous Poisson process candidates\n        num_candidates = rng.poisson(Lambda_max * T)\n        candidate_times = rng.uniform(0, T, num_candidates)\n        \n        # 2. Thin the candidates\n        retention_probs = lambda_t(candidate_times) / Lambda_max\n        retained_mask = rng.uniform(0, 1, num_candidates)  retention_probs\n        retained_times = candidate_times[retained_mask]\n        \n        n1 = len(retained_times)\n        \n        # 3. Sort times and perform time-rescaling\n        retained_times.sort()\n        \n        # Integrated intensity: Lambda(t) = integral_0^t (2+sin(s))ds = 2t - cos(t) + 1\n        Lambda_of_t = lambda t: 2 * t - np.cos(t) + 1\n        \n        if n1  2:\n            # KS test requires at least one data point, and increments require at least 2.\n            D1 = 0.0\n        else:\n            Lambda_vals = Lambda_of_t(retained_times)\n            # Transformed increments e_i = Lambda(t_i) - Lambda(t_{i-1})\n            e_values = np.diff(Lambda_vals, prepend=Lambda_of_t(0))\n            # 4. Compare with Exponential(1) using KS test\n            ks_result = stats.kstest(e_values, 'expon')\n            D1 = ks_result.statistic\n        \n        return n1, D1\n\n    def test_2(seed):\n        \"\"\"\n        Test 2: 2D linear intensity in x.\n        Simulates on [0, 1]x[0, 1] with lambda(x,y) = 10(1+x), Lambda_max = 20.\n        Calculates number of points, sample mean of x, and its deviation from theoretical mean.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        ax, bx = 0.0, 1.0\n        ay, by = 0.0, 1.0\n        Lambda_max = 20.0\n        \n        area = (bx - ax) * (by - ay)\n        lambda_xy = lambda x, y: 10 * (1 + x)\n        \n        # 1. Generate homogeneous Poisson process candidates\n        num_candidates = rng.poisson(Lambda_max * area)\n        candidate_points_x = rng.uniform(ax, bx, num_candidates)\n        candidate_points_y = rng.uniform(ay, by, num_candidates)\n        \n        # 2. Thin the candidates\n        retention_probs = lambda_xy(candidate_points_x, candidate_points_y) / Lambda_max\n        retained_mask = rng.uniform(0, 1, num_candidates)  retention_probs\n        retained_points_x = candidate_points_x[retained_mask]\n        \n        n2 = len(retained_points_x)\n        \n        if n2 == 0:\n            x_bar_2 = 0.0\n        else:\n            x_bar_2 = np.mean(retained_points_x)\n            \n        # Theoretical mean E[X] for density proportional to 1+x on [0,1] is 5/9\n        delta_2 = x_bar_2 - 5/9\n        \n        return n2, x_bar_2, delta_2\n\n    def test_3(seed):\n        \"\"\"\n        Test 3: 1D piecewise intensity.\n        Simulates on [0, 5] with lambda(t) = 0 for t in [0, 2.5] and 4 for t in (2.5, 5].\n        Lambda_max = 4. Counts events in each part of the interval.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = 5.0\n        Lambda_max = 4.0\n        \n        def lambda_t(t):\n            return np.piecewise(t, [t = 2.5], [0, 4])\n\n        # 1. Generate homogeneous Poisson process candidates\n        num_candidates = rng.poisson(Lambda_max * T)\n        candidate_times = rng.uniform(0, T, num_candidates)\n        \n        # 2. Thin the candidates\n        retention_probs = lambda_t(candidate_times) / Lambda_max\n        retained_mask = rng.uniform(0, 1, num_candidates)  retention_probs\n        retained_times = candidate_times[retained_mask]\n        \n        # 3. Count events in specified intervals\n        n3_L = np.sum(retained_times = 2.5)\n        n3_R = np.sum(retained_times > 2.5)\n        \n        return n3_L, n3_R\n\n    def test_4(seed):\n        \"\"\"\n        Test 4: 1D very low homogeneous intensity.\n        Simulates on [0, 10] with lambda(t) = 1e-3, Lambda_max = 1e-3.\n        Calculates total number of events.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = 10.0\n        lambda_val = 1e-3\n        Lambda_max = 1e-3 # This implies retention probability is 1\n        \n        # Since lambda/Lambda_max = 1, all candidates are retained.\n        # The number of events is just a draw from Poisson(lambda * T).\n        n4 = rng.poisson(lambda_val * T)\n        \n        return n4\n\n    # Run all tests with specified seeds\n    n1, D1 = test_1(seed=314159)\n    n2, x_bar_2, delta_2 = test_2(seed=271828)\n    n3_L, n3_R = test_3(seed=161803)\n    n4 = test_4(seed=141421)\n\n    # Assemble results and format them as specified\n    output_list = [\n        str(n1),\n        f'{D1:.6f}',\n        str(n2),\n        f'{x_bar_2:.6f}',\n        f'{delta_2:.6f}',\n        str(n3_L),\n        str(n3_R),\n        str(n4)\n    ]\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(output_list)}]\")\n\nsolve()\n```"
        }
    ]
}