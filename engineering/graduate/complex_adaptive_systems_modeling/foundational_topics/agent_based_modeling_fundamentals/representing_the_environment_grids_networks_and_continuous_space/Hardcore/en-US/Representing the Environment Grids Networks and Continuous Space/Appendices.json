{
    "hands_on_practices": [
        {
            "introduction": "Modeling the placement of agents or events in continuous space is a fundamental task, especially when their density varies. This exercise introduces the thinning algorithm, a powerful and elegant method for simulating inhomogeneous Poisson point processes from a simpler, homogeneous base. By deriving the algorithm from first principles and proving its correctness, you will gain a deep understanding of how to generate complex spatial patterns in a computationally tractable way. ",
            "id": "4140718",
            "problem": "You are tasked with designing and implementing a thinning algorithm to simulate inhomogeneous Poisson point processes on continuous domains, together with a principle-based justification of correctness grounded in first principles. An inhomogeneous Poisson point process on a measurable domain $\\Omega \\subset \\mathbb{R}^{d}$ with a nonnegative, integrable intensity function $\\lambda(x)$ is defined by the following two properties: for every bounded measurable subset $A \\subset \\Omega$, the point count $N(A)$ is Poisson distributed with mean $\\int_{A} \\lambda(x)\\,dx$, and for pairwise disjoint measurable subsets $A_{1}, \\dots, A_{k}$ the random variables $N(A_{1}), \\dots, N(A_{k})$ are independent.\n\nYou must derive and implement a thinning-based simulator starting from a homogeneous Poisson point process on the same domain with constant rate $\\Lambda_{\\max}$, where $\\Lambda_{\\max} \\ge \\sup_{x \\in \\Omega} \\lambda(x)$. The algorithm must be expressed purely in mathematical terms and implemented in code. Angles must be interpreted in radians.\n\nThe fundamental base you may assume includes the definition of a Poisson point process, properties of the Poisson distribution, and standard independence and conditioning rules. The target thinning algorithm must not be assumed; it must be derived from the base. You must also justify correctness by showing that the output of your simulator satisfies the defining properties of the inhomogeneous Poisson point process.\n\nAlgorithmic task: implement the following thinning simulator for two environments that represent continuous space.\n\n1. For one-dimensional time on the interval $[0, T]$ with intensity $\\lambda(t)$ and bound $\\Lambda_{\\max}$:\n   - Generate a homogeneous Poisson process with rate $\\Lambda_{\\max}$ on $[0, T]$ by drawing a Poisson number $N \\sim \\mathrm{Poisson}(\\Lambda_{\\max} T)$ of candidate times and placing them independently and uniformly in $[0, T]$.\n   - For each candidate time $t$, retain it independently with probability $p(t) = \\lambda(t)/\\Lambda_{\\max}$, and discard it otherwise.\n   - Sort the retained times to obtain the simulated inhomogeneous process.\n\n2. For two-dimensional space on the rectangle $[a_{x}, b_{x}] \\times [a_{y}, b_{y}]$ with intensity $\\lambda(x,y)$ and bound $\\Lambda_{\\max}$:\n   - Generate a homogeneous Poisson point process with rate $\\Lambda_{\\max}$ on the rectangle by drawing $N \\sim \\mathrm{Poisson}(\\Lambda_{\\max} |[a_{x}, b_{x}] \\times [a_{y}, b_{y}]|)$ candidate points independently and uniformly in the rectangle.\n   - For each candidate point $(x, y)$, retain it independently with probability $p(x,y) = \\lambda(x,y)/\\Lambda_{\\max}$, and discard it otherwise.\n\nFor the one-dimensional case, define the integrated intensity $\\Lambda(t) = \\int_{0}^{t} \\lambda(s)\\,ds$. For a realization with retained times $0  t_{1}  \\cdots  t_{n} \\le T$, define the transformed increments $e_{i} = \\Lambda(t_{i}) - \\Lambda(t_{i-1})$, with the convention $t_{0} = 0$. The time-rescaling theorem states that if $(t_{i})$ is an inhomogeneous Poisson process with intensity $\\lambda(t)$, then $(e_{i})$ are independent and identically distributed as $\\mathrm{Exponential}(1)$.\n\nYour implementation must use the above algorithm with the following fixed test suite to produce deterministic outputs. You must use the specified seeds for the pseudo-random number generator for reproducibility. All angles are in radians. No physical units are involved.\n\nTest suite:\n\n- Test $1$ (one-dimensional sinusoidal intensity): simulate on $[0, T]$ with $T = 10$, $\\lambda(t) = 2 + \\sin(t)$, and $\\Lambda_{\\max} = 3$. Use pseudo-random seed $314159$. Let $n_{1}$ be the number of retained events and let $D_{1}$ be the Kolmogorov–Smirnov statistic comparing the empirical distribution of $(e_{i})$ to the $\\mathrm{Exponential}(1)$ distribution via the one-sample Kolmogorov–Smirnov test.\n\n- Test $2$ (two-dimensional linear intensity in $x$): simulate on $[a_{x}, b_{x}] \\times [a_{y}, b_{y}] = [0, 1] \\times [0, 1]$ with $\\lambda(x,y) = 10 (1 + x)$ and $\\Lambda_{\\max} = 20$. Use pseudo-random seed $271828$. Let $n_{2}$ be the number of retained points, let $\\bar{x}_{2}$ be the sample mean of the $x$-coordinates of retained points, and let $\\delta_{2} = \\bar{x}_{2} - \\frac{5}{9}$, where $\\frac{5}{9}$ is the exact expected $x$-mean under the normalized density proportional to $1+x$ on $[0,1]$.\n\n- Test $3$ (one-dimensional piecewise intensity with a zero region): simulate on $[0, T]$ with $T = 5$, $\\lambda(t) = 0$ for $t \\in [0, 2.5]$ and $\\lambda(t) = 4$ for $t \\in (2.5, 5]$, and $\\Lambda_{\\max} = 4$. Use pseudo-random seed $161803$. Let $n_{3}^{(L)}$ be the number of retained events in $[0, 2.5]$ and $n_{3}^{(R)}$ be the number of retained events in $(2.5, 5]$.\n\n- Test $4$ (one-dimensional very low homogeneous intensity): simulate on $[0, T]$ with $T = 10$, $\\lambda(t) = 10^{-3}$ and $\\Lambda_{\\max} = 10^{-3}$. Use pseudo-random seed $141421$. Let $n_{4}$ be the total number of retained events.\n\nAnswer specification:\n\n- Your program must output a single line containing the results of the test suite in the following order and types: $[n_{1}, D_{1}, n_{2}, \\bar{x}_{2}, \\delta_{2}, n_{3}^{(L)}, n_{3}^{(R)}, n_{4}]$.\n- Floats $D_{1}$, $\\bar{x}_{2}$, and $\\delta_{2}$ must be rounded to exactly six digits after the decimal point. Integers must be printed without rounding. The output must be a single line with a comma-separated list enclosed in square brackets and contain no spaces, for example $[42,0.123456,17,0.555556,0.000000,0,9,0]$.",
            "solution": "The task is to derive and justify the correctness of the thinning algorithm for simulating an inhomogeneous Poisson point process (IPPP), and then to implement this algorithm for a defined test suite. The derivation must be from first principles.\n\nAn IPPP on a domain $\\Omega \\subset \\mathbb{R}^{d}$ with a non-negative, integrable intensity function $\\lambda(x)$ is characterized by two defining properties:\n$1$. For any bounded measurable subset $A \\subset \\Omega$, the point count $N(A)$ follows a Poisson distribution with mean $\\mu(A) = \\int_{A} \\lambda(x)\\,dx$.\n$2$. For any finite collection of pairwise disjoint measurable subsets $A_{1}, \\dots, A_{k} \\subset \\Omega$, the random variables $N(A_{1}), \\dots, N(A_{k})$ are independent.\n\nThe thinning algorithm simulates such a process. It begins with a homogeneous Poisson point process (HPPP), denoted $\\mathcal{P}_{\\text{hom}}$, on the same domain $\\Omega$ with a constant rate $\\Lambda_{\\max}$, where $\\Lambda_{\\max}$ is an upper bound for the intensity function, i.e., $\\Lambda_{\\max} \\ge \\sup_{x \\in \\Omega} \\lambda(x)$. A new point process, $\\mathcal{P}_{\\text{thin}}$, is then generated by independently retaining each point $x$ from $\\mathcal{P}_{\\text{hom}}$ with a location-dependent probability $p(x) = \\lambda(x) / \\Lambda_{\\max}$. This procedure is known as thinning. We must now prove that the resulting process $\\mathcal{P}_{\\text{thin}}$ is indeed an IPPP with intensity $\\lambda(x)$.\n\n**Proof of Correctness**\n\nWe must verify that $\\mathcal{P}_{\\text{thin}}$ satisfies the two defining properties of an IPPP.\n\n**1. Point Count Distribution**\nLet $A$ be a bounded measurable subset of $\\Omega$. We need to show that the number of points from the thinned process located in $A$, denoted $N_{\\text{thin}}(A)$, follows a Poisson distribution with mean $\\int_{A} \\lambda(x)\\,dx$.\n\nThe number of points from the HPPP in the set $A$, denoted $N_{\\text{hom}}(A)$, is a random variable following a Poisson distribution with mean $\\Lambda_{\\max} |A|$, where $|A|$ is the Lebesgue measure of $A$. The probability mass function is $P(N_{\\text{hom}}(A)=k) = e^{-\\Lambda_{\\max} |A|} \\frac{(\\Lambda_{\\max} |A|)^k}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nGiven that $N_{\\text{hom}}(A) = k$, the $k$ points of the HPPP are independently and uniformly distributed within the set $A$. Let these points be $\\{Y_1, \\dots, Y_k\\}$. Each point $Y_j$ is retained with probability $p(Y_j) = \\lambda(Y_j) / \\Lambda_{\\max}$. Let $I_j$ be the indicator variable that equals $1$ if point $Y_j$ is retained and $0$ otherwise. The total number of retained points in $A$ is $N_{\\text{thin}}(A) = \\sum_{j=1}^k I_j$.\n\nThe probability of retaining a single, arbitrary point that is uniformly distributed in $A$ is the average of $p(y)$ over $A$:\n$$ \\bar{p}_A = E[p(Y_j)] = \\int_A p(y) f_Y(y)\\,dy = \\int_A \\frac{\\lambda(y)}{\\Lambda_{\\max}} \\frac{1}{|A|}\\,dy = \\frac{1}{\\Lambda_{\\max} |A|} \\int_A \\lambda(y)\\,dy $$\nwhere $f_Y(y) = 1/|A|$ for $y \\in A$ is the probability density function for a point chosen uniformly from $A$.\n\nConditioned on $N_{\\text{hom}}(A) = k$, the number of retained points $N_{\\text{thin}}(A)$ is the sum of $k$ independent Bernoulli trials. The number of successes in $k$ trials, where each trial has a different success probability $p(Y_j)$, does not follow a simple binomial distribution. However, a well-known property of Poisson processes (often called Poisson splitting or marking) states that if a Poisson process with mean $\\mu$ is thinned, where each event is kept with probability $p$, the resulting process is also Poisson with mean $\\mu p$. This can be generalized to location-dependent thinning.\n\nA more formal derivation is as follows. We condition on $N_{\\text{hom}}(A)=k$ and then average over the Poisson distribution of $k$. Given $k$ points uniformly distributed in $A$, the number of retained points $N_{\\text{thin}}(A)$ follows a binomial distribution $B(k, \\bar{p}_A)$. Thus, $P(N_{\\text{thin}}(A)=n | N_{\\text{hom}}(A)=k) = \\binom{k}{n} (\\bar{p}_A)^n (1-\\bar{p}_A)^{k-n}$.\n\nUsing the law of total probability, the unconditional probability mass function of $N_{\\text{thin}}(A)$ is:\n$$ P(N_{\\text{thin}}(A)=n) = \\sum_{k=n}^{\\infty} P(N_{\\text{thin}}(A)=n | N_{\\text{hom}}(A)=k) P(N_{\\text{hom}}(A)=k) $$\n$$ P(N_{\\text{thin}}(A)=n) = \\sum_{k=n}^{\\infty} \\left[ \\binom{k}{n} (\\bar{p}_A)^n (1-\\bar{p}_A)^{k-n} \\right] \\left[ e^{-\\Lambda_{\\max} |A|} \\frac{(\\Lambda_{\\max} |A|)^k}{k!} \\right] $$\nBy rearranging terms:\n$$ P(N_{\\text{thin}}(A)=n) = \\frac{(\\bar{p}_A \\Lambda_{\\max} |A|)^n}{n!} e^{-\\Lambda_{\\max} |A|} \\sum_{k=n}^{\\infty} \\frac{1}{(k-n)!} \\left( (1-\\bar{p}_A) \\Lambda_{\\max} |A| \\right)^{k-n} $$\nLet $j=k-n$. The summation becomes $\\sum_{j=0}^{\\infty} \\frac{1}{j!} ((1-\\bar{p}_A)\\Lambda_{\\max} |A|)^j$, which is the Taylor series for $e^{(1-\\bar{p}_A)\\Lambda_{\\max} |A|}$.\nSubstituting this back, we get:\n$$ P(N_{\\text{thin}}(A)=n) = \\frac{(\\bar{p}_A \\Lambda_{\\max} |A|)^n}{n!} e^{-\\Lambda_{\\max} |A|} e^{(1-\\bar{p}_A)\\Lambda_{\\max} |A|} $$\n$$ P(N_{\\text{thin}}(A)=n) = \\frac{(\\bar{p}_A \\Lambda_{\\max} |A|)^n}{n!} e^{-\\bar{p}_A \\Lambda_{\\max} |A|} $$\nThis is the probability mass function of a Poisson distribution with mean $\\mu_{\\text{thin}} = \\bar{p}_A \\Lambda_{\\max} |A|$. Substituting the expression for $\\bar{p}_A$:\n$$ \\mu_{\\text{thin}} = \\left( \\frac{1}{\\Lambda_{\\max} |A|} \\int_A \\lambda(y)\\,dy \\right) \\Lambda_{\\max} |A| = \\int_A \\lambda(y)\\,dy $$\nThus, $N_{\\text{thin}}(A) \\sim \\text{Poisson}(\\int_A \\lambda(x)\\,dx)$, satisfying the first property.\n\n**2. Independence of Counts**\nLet $A_1, \\dots, A_k$ be pairwise disjoint measurable subsets of $\\Omega$. We must show that the counts $N_{\\text{thin}}(A_1), \\dots, N_{\\text{thin}}(A_k)$ are independent random variables.\n\nA fundamental property of the HPPP $\\mathcal{P}_{\\text{hom}}$ is that the point processes restricted to disjoint sets are independent. This means the random sets of points $\\mathcal{P}_{\\text{hom}} \\cap A_1, \\dots, \\mathcal{P}_{\\text{hom}} \\cap A_k$ are mutually independent.\n\nThe thinning operation is performed on each point of $\\mathcal{P}_{\\text{hom}}$ independently of all other points. The decision to retain a point $x \\in A_i$ depends only on its location $x$ and an independent random draw (e.g., comparing $p(x) = \\lambda(x)/\\Lambda_{\\max}$ to a draw from $U(0,1)$). This decision is not affected by the presence or retention of any point in another set $A_j$ for $j \\neq i$.\n\nSince the initial point configurations in the disjoint sets $A_i$ are independent, and the thinning operations within each set are independent of operations in other sets, the resulting thinned point configurations, $\\mathcal{P}_{\\text{thin}} \\cap A_1, \\dots, \\mathcal{P}_{\\text{thin}} \\cap A_k$, are also mutually independent. Consequently, their respective counts, $N_{\\text{thin}}(A_1), \\dots, N_{\\text{thin}}(A_k)$, are independent random variables. This establishes the second property.\n\nBoth defining properties of an IPPP are satisfied. Therefore, the thinning algorithm correctly generates a realization of an inhomogeneous Poisson point process with the desired intensity function $\\lambda(x)$. The condition $\\Lambda_{\\max} \\ge \\sup \\lambda(x)$ is crucial as it guarantees the retention probability $p(x) = \\lambda(x)/\\Lambda_{\\max}$ is well-defined and lies in the interval $[0, 1]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the thinning algorithm simulator.\n    \"\"\"\n    \n    def test_1(seed):\n        \"\"\"\n        Test 1: 1D sinusoidal intensity.\n        Simulates on [0, T] with T = 10, lambda(t) = 2 + sin(t), and Lambda_max = 3.\n        Calculates number of events and KS statistic vs. Exponential(1).\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = 10.0\n        Lambda_max = 3.0\n        \n        lambda_t = lambda t: 2 + np.sin(t)\n        \n        # 1. Generate homogeneous Poisson process candidates\n        num_candidates = rng.poisson(Lambda_max * T)\n        candidate_times = rng.uniform(0, T, num_candidates)\n        \n        # 2. Thin the candidates\n        retention_probs = lambda_t(candidate_times) / Lambda_max\n        retained_mask = rng.uniform(0, 1, num_candidates)  retention_probs\n        retained_times = candidate_times[retained_mask]\n        \n        n1 = len(retained_times)\n        \n        # 3. Sort times and perform time-rescaling\n        retained_times.sort()\n        \n        # Integrated intensity: Lambda(t) = integral_0^t (2+sin(s))ds = 2t - cos(t) + 1\n        Lambda_of_t = lambda t: 2 * t - np.cos(t) + 1\n        \n        if n1  2:\n            # KS test requires at least one data point, and increments require at least 2.\n            D1 = 0.0\n        else:\n            Lambda_vals = Lambda_of_t(retained_times)\n            # Transformed increments e_i = Lambda(t_i) - Lambda(t_{i-1})\n            e_values = np.diff(Lambda_vals, prepend=Lambda_of_t(0))\n            # 4. Compare with Exponential(1) using KS test\n            ks_result = stats.kstest(e_values, 'expon')\n            D1 = ks_result.statistic\n        \n        return n1, D1\n\n    def test_2(seed):\n        \"\"\"\n        Test 2: 2D linear intensity in x.\n        Simulates on [0, 1]x[0, 1] with lambda(x,y) = 10(1+x), Lambda_max = 20.\n        Calculates number of points, sample mean of x, and its deviation from theoretical mean.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        ax, bx = 0.0, 1.0\n        ay, by = 0.0, 1.0\n        Lambda_max = 20.0\n        \n        area = (bx - ax) * (by - ay)\n        lambda_xy = lambda x, y: 10 * (1 + x)\n        \n        # 1. Generate homogeneous Poisson process candidates\n        num_candidates = rng.poisson(Lambda_max * area)\n        candidate_points_x = rng.uniform(ax, bx, num_candidates)\n        candidate_points_y = rng.uniform(ay, by, num_candidates)\n        \n        # 2. Thin the candidates\n        retention_probs = lambda_xy(candidate_points_x, candidate_points_y) / Lambda_max\n        retained_mask = rng.uniform(0, 1, num_candidates)  retention_probs\n        retained_points_x = candidate_points_x[retained_mask]\n        \n        n2 = len(retained_points_x)\n        \n        if n2 == 0:\n            x_bar_2 = 0.0\n        else:\n            x_bar_2 = np.mean(retained_points_x)\n            \n        # Theoretical mean E[X] for density proportional to 1+x on [0,1] is 5/9\n        delta_2 = x_bar_2 - 5/9\n        \n        return n2, x_bar_2, delta_2\n\n    def test_3(seed):\n        \"\"\"\n        Test 3: 1D piecewise intensity.\n        Simulates on [0, 5] with lambda(t) = 0 for t in [0, 2.5] and 4 for t in (2.5, 5].\n        Lambda_max = 4. Counts events in each part of the interval.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = 5.0\n        Lambda_max = 4.0\n        \n        def lambda_t(t):\n            return np.piecewise(t, [t = 2.5], [0, 4])\n\n        # 1. Generate homogeneous Poisson process candidates\n        num_candidates = rng.poisson(Lambda_max * T)\n        candidate_times = rng.uniform(0, T, num_candidates)\n        \n        # 2. Thin the candidates\n        retention_probs = lambda_t(candidate_times) / Lambda_max\n        retained_mask = rng.uniform(0, 1, num_candidates)  retention_probs\n        retained_times = candidate_times[retained_mask]\n        \n        # 3. Count events in specified intervals\n        n3_L = np.sum(retained_times = 2.5)\n        n3_R = np.sum(retained_times  2.5)\n        \n        return n3_L, n3_R\n\n    def test_4(seed):\n        \"\"\"\n        Test 4: 1D very low homogeneous intensity.\n        Simulates on [0, 10] with lambda(t) = 1e-3, Lambda_max = 1e-3.\n        Calculates total number of events.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = 10.0\n        lambda_val = 1e-3\n        Lambda_max = 1e-3 # This implies retention probability is 1\n        \n        # Since lambda/Lambda_max = 1, all candidates are retained.\n        # The number of events is just a draw from Poisson(lambda * T).\n        n4 = rng.poisson(lambda_val * T)\n        \n        return n4\n\n    # Run all tests with specified seeds\n    n1, D1 = test_1(seed=314159)\n    n2, x_bar_2, delta_2 = test_2(seed=271828)\n    n3_L, n3_R = test_3(seed=161803)\n    n4 = test_4(seed=141421)\n\n    # Assemble results and format them as specified\n    output_list = [\n        str(n1),\n        f'{D1:.6f}',\n        str(n2),\n        f'{x_bar_2:.6f}',\n        f'{delta_2:.6f}',\n        str(n3_L),\n        str(n3_R),\n        str(n4)\n    ]\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(output_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many environmental processes, like the transport of a substance in a fluid, are described by continuous partial differential equations. This practice explores how to translate such a continuous model—the linear advection equation—onto a discrete grid, a core task in computational science. By deriving and analyzing two common numerical schemes, you will uncover the critical trade-offs between accuracy, stability, and the introduction of numerical artifacts like artificial diffusion and dispersion. ",
            "id": "4140822",
            "problem": "Consider representing a one-dimensional continuous environment with a uniform spatial grid for the linear advection Partial Differential Equation (PDE) given by $\\partial_{t} u(x,t) + c\\,\\partial_{x} u(x,t) = 0$, where $c0$ is a constant signal speed. Let the environment be periodic on an interval of length $L$ and discretize space as $x_{j} = j\\,\\Delta x$ for integer $j$ and time as $t^{n} = n\\,\\Delta t$ for integer $n$. Define the dimensionless Courant–Friedrichs–Lewy (CFL) number as $\\nu = \\dfrac{c\\,\\Delta t}{\\Delta x}$. Assume that the grid-based model appropriately approximates the continuous advection dynamics.\n\nStarting from core definitions and calculus, derive:\n1. A first-order explicit upwind finite-difference scheme consistent with the advection direction $c0$.\n2. A second-order explicit Lax–Wendroff finite-difference scheme by a second-order Taylor expansion in time, using the PDE to re-express time derivatives with spatial derivatives, and approximating spatial derivatives with centered finite differences consistent with the grid representation.\n\nThen, analyze both discrete schemes using Fourier-mode von Neumann analysis on the periodic grid. For each scheme, assume a discrete Fourier mode of the form $u_{j}^{n} = \\hat{u}^{n}\\,\\exp(i\\,j\\,\\theta)$, where $\\theta = k\\,\\Delta x$ is the dimensionless wavenumber associated with the continuous wavenumber $k$. Derive the exact amplification factor $G(\\theta,\\nu)$ such that $\\hat{u}^{n+1} = G(\\theta,\\nu)\\,\\hat{u}^{n}$.\n\nUsing the amplification factors, compare the dispersion and dissipation characteristics of the two schemes:\n- Express the magnitude $|G(\\theta,\\nu)|$ and the argument $\\arg(G(\\theta,\\nu))$ in closed form.\n- For small $\\theta$, obtain leading-order expansions of $|G(\\theta,\\nu)|$ and of the ratio of numerical to physical phase speed $c_{\\mathrm{num}}/c$, where $c_{\\mathrm{num}}$ is defined by the numerical phase shift per time step via $G(\\theta,\\nu) = |G(\\theta,\\nu)|\\,\\exp\\!\\big(-i\\,\\omega_{\\mathrm{num}}\\,\\Delta t\\big)$ and $c_{\\mathrm{num}} = \\omega_{\\mathrm{num}}/k$.\n- State the stability condition on $\\nu$ under which each scheme is stable, using the von Neumann criterion on the amplification factor.\n\nAnswer specification:\nProvide your final answer as the pair of exact amplification factors $G_{\\mathrm{up}}(\\theta,\\nu)$ and $G_{\\mathrm{LW}}(\\theta,\\nu)$ for the upwind and Lax–Wendroff schemes, respectively. Express them as a single row matrix in terms of $\\theta$ and $\\nu$. No rounding is required. No units are required in the final answer.",
            "solution": "The linear advection equation is given by:\n$$\n\\partial_{t} u(x,t) + c\\,\\partial_{x} u(x,t) = 0\n$$\nwhere $c0$ is the constant advection speed. We discretize space with grid spacing $\\Delta x$ so that $x_j = j\\,\\Delta x$, and time with step size $\\Delta t$ so that $t^n = n\\,\\Delta t$. The numerical approximation of $u(x_j, t^n)$ is denoted by $u_j^n$. The Courant–Friedrichs–Lewy (CFL) number is $\\nu = \\frac{c\\,\\Delta t}{\\Delta x}$.\n\n**1. Derivation of the First-Order Upwind Scheme**\n\nWe approximate the time and space derivatives using finite differences. For the time derivative, we use a first-order forward difference:\n$$\n\\partial_{t} u(x_j, t^n) \\approx \\frac{u_j^{n+1} - u_j^n}{\\Delta t}\n$$\nFor the spatial derivative, since the wave speed $c$ is positive ($c0$), information propagates from left to right (from smaller $x$ to larger $x$). Therefore, a \"physically-motivated\" or \"upwind\" scheme should use information from the upwind direction, which corresponds to grid points with index less than $j$. We use a first-order backward difference for the spatial derivative:\n$$\n\\partial_{x} u(x_j, t^n) \\approx \\frac{u_j^n - u_{j-1}^n}{\\Delta x}\n$$\nSubstituting these approximations into the PDE, $\\partial_t u = -c\\,\\partial_x u$, we get:\n$$\n\\frac{u_j^{n+1} - u_j^n}{\\Delta t} = -c \\left(\\frac{u_j^n - u_{j-1}^n}{\\Delta x}\\right)\n$$\nSolving for $u_j^{n+1}$:\n$$\nu_j^{n+1} = u_j^n - \\frac{c\\,\\Delta t}{\\Delta x} (u_j^n - u_{j-1}^n)\n$$\nUsing the definition of the CFL number $\\nu$:\n$$\nu_j^{n+1} = u_j^n - \\nu (u_j^n - u_{j-1}^n) = (1-\\nu)u_j^n + \\nu u_{j-1}^n\n$$\nThis is the first-order explicit upwind finite-difference scheme.\n\n**2. Derivation of the Second-Order Lax–Wendroff Scheme**\n\nWe begin with a second-order Taylor series expansion of $u(x, t+\\Delta t)$ in time:\n$$\nu(x, t+\\Delta t) = u(x,t) + \\Delta t\\,\\partial_t u(x,t) + \\frac{(\\Delta t)^2}{2}\\,\\partial_{tt} u(x,t) + O((\\Delta t)^3)\n$$\nFrom the PDE, we can express the time derivatives in terms of spatial derivatives:\n$$\n\\partial_t u = -c\\,\\partial_x u\n$$\n$$\n\\partial_{tt} u = \\partial_t(\\partial_t u) = \\partial_t(-c\\,\\partial_x u) = -c\\,\\partial_{xt} u = -c\\,\\partial_x(\\partial_t u) = -c\\,\\partial_x(-c\\,\\partial_x u) = c^2\\,\\partial_{xx} u\n$$\nSubstituting these into the Taylor expansion:\n$$\nu(x, t+\\Delta t) \\approx u(x,t) - c\\,\\Delta t\\,\\partial_x u(x,t) + \\frac{c^2\\,(\\Delta t)^2}{2}\\,\\partial_{xx} u(x,t)\n$$\nNow, we discretize this expression at $(x_j, t^n)$. We approximate the spatial derivatives using second-order centered finite differences:\n$$\n(\\partial_x u)_j^n \\approx \\frac{u_{j+1}^n - u_{j-1}^n}{2\\,\\Delta x}\n$$\n$$\n(\\partial_{xx} u)_j^n \\approx \\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{(\\Delta x)^2}\n$$\nSubstituting these into the discretized time-expanded equation gives:\n$$\nu_j^{n+1} = u_j^n - c\\,\\Delta t \\left(\\frac{u_{j+1}^n - u_{j-1}^n}{2\\,\\Delta x}\\right) + \\frac{c^2\\,(\\Delta t)^2}{2} \\left(\\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{(\\Delta x)^2}\\right)\n$$\nIntroducing the CFL number $\\nu = \\frac{c\\,\\Delta t}{\\Delta x}$:\n$$\nu_j^{n+1} = u_j^n - \\frac{\\nu}{2}(u_{j+1}^n - u_{j-1}^n) + \\frac{\\nu^2}{2}(u_{j+1}^n - 2u_j^n + u_{j-1}^n)\n$$\nThis is the explicit Lax–Wendroff scheme. We can group terms by spatial index to see the stencil coefficients:\n$$\nu_j^{n+1} = \\left(\\frac{\\nu^2}{2} + \\frac{\\nu}{2}\\right) u_{j-1}^n + (1 - \\nu^2) u_j^n + \\left(\\frac{\\nu^2}{2} - \\frac{\\nu}{2}\\right) u_{j+1}^n\n$$\n\n**3. Von Neumann Stability Analysis**\n\nWe analyze the stability and propagation characteristics of these schemes by examining their effect on a single Fourier mode, $u_j^n = \\hat{u}^n\\,\\exp(i\\,j\\,\\theta)$, where $\\theta = k\\,\\Delta x$ is the dimensionless wavenumber. The amplification factor $G(\\theta,\\nu)$ is defined by the relation $\\hat{u}^{n+1} = G(\\theta,\\nu)\\,\\hat{u}^n$.\n\n**Analysis of the Upwind Scheme:**\nSubstitute the Fourier mode into the upwind scheme $u_j^{n+1} = (1-\\nu)u_j^n + \\nu u_{j-1}^n$:\n$$\n\\hat{u}^{n+1}\\,\\exp(i\\,j\\,\\theta) = (1-\\nu)\\hat{u}^n\\,\\exp(i\\,j\\,\\theta) + \\nu\\hat{u}^n\\,\\exp(i\\,(j-1)\\,\\theta)\n$$\nDividing by $\\hat{u}^n\\,\\exp(i\\,j\\,\\theta)$:\n$$\nG_{\\mathrm{up}}(\\theta, \\nu) = \\frac{\\hat{u}^{n+1}}{\\hat{u}^n} = (1-\\nu) + \\nu\\,\\exp(-i\\,\\theta)\n$$\nThis is the exact amplification factor for the upwind scheme.\n$G_{\\mathrm{up}}(\\theta, \\nu) = 1-\\nu + \\nu(\\cos\\theta - i\\sin\\theta) = (1-\\nu+\\nu\\cos\\theta) - i\\nu\\sin\\theta$.\nThe squared magnitude is:\n$$\n|G_{\\mathrm{up}}|^2 = (1-\\nu+\\nu\\cos\\theta)^2 + (-\\nu\\sin\\theta)^2 = 1 - 2\\nu(1-\\nu)(1-\\cos\\theta) = 1 - 4\\nu(1-\\nu)\\sin^2(\\theta/2)\n$$\nFor stability, we require $|G_{\\mathrm{up}}| \\le 1$ for all $\\theta$. This implies $|G_{\\mathrm{up}}|^2 \\le 1$, so $-4\\nu(1-\\nu)\\sin^2(\\theta/2) \\le 0$. Since $\\sin^2(\\theta/2) \\ge 0$ and $\\nu  0$, we need $1-\\nu \\ge 0$. The stability condition is $0 \\le \\nu \\le 1$.\n\n**Analysis of the Lax-Wendroff Scheme:**\nSubstitute the Fourier mode into the Lax-Wendroff update rule $u_j^{n+1} = u_j^n - \\frac{\\nu}{2}(u_{j+1}^n - u_{j-1}^n) + \\frac{\\nu^2}{2}(u_{j+1}^n - 2u_j^n + u_{j-1}^n)$:\n$$\n\\hat{u}^{n+1}\\,\\exp(i\\,j\\,\\theta) = \\hat{u}^n\\,\\exp(i\\,j\\,\\theta) - \\frac{\\nu}{2}\\hat{u}^n(\\exp(i(j+1)\\theta) - \\exp(i(j-1)\\theta)) + \\frac{\\nu^2}{2}\\hat{u}^n(\\exp(i(j+1)\\theta) - 2\\exp(ij\\theta) + \\exp(i(j-1)\\theta))\n$$\nDividing by $\\hat{u}^n\\,\\exp(i\\,j\\,\\theta)$:\n$$\nG_{\\mathrm{LW}}(\\theta, \\nu) = 1 - \\frac{\\nu}{2}(\\exp(i\\theta) - \\exp(-i\\theta)) + \\frac{\\nu^2}{2}(\\exp(i\\theta) - 2 + \\exp(-i\\theta))\n$$\nUsing Euler's identities, $\\sin\\theta = \\frac{\\exp(i\\theta)-\\exp(-i\\theta)}{2i}$ and $\\cos\\theta = \\frac{\\exp(i\\theta)+\\exp(-i\\theta)}{2}$:\n$$\nG_{\\mathrm{LW}}(\\theta, \\nu) = 1 - i\\nu\\sin\\theta + \\frac{\\nu^2}{2}(2\\cos\\theta - 2) = 1 - i\\nu\\sin\\theta + \\nu^2(\\cos\\theta - 1)\n$$\nThis is the exact amplification factor for the Lax-Wendroff scheme.\nThe squared magnitude is:\n$$\n|G_{\\mathrm{LW}}|^2 = (1+\\nu^2(\\cos\\theta-1))^2 + (-\\nu\\sin\\theta)^2 = (1-\\nu^2(1-\\cos\\theta))^2 + \\nu^2\\sin^2\\theta\n$$\n$$\n|G_{\\mathrm{LW}}|^2 = 1 - 2\\nu^2(1-\\cos\\theta) + \\nu^4(1-\\cos\\theta)^2 + \\nu^2(1-\\cos^2\\theta)\n$$\nSince $1-\\cos^2\\theta = (1-\\cos\\theta)(1+\\cos\\theta) = (1-\\cos\\theta)(2-(1-\\cos\\theta))$,\n$$\n|G_{\\mathrm{LW}}|^2 = 1 + (1-\\cos\\theta) [-2\\nu^2 + \\nu^2(2-(1-\\cos\\theta))] + \\nu^4(1-\\cos\\theta)^2\n$$\n$$\n|G_{\\mathrm{LW}}|^2 = 1 - \\nu^2(1-\\cos\\theta)^2 + \\nu^4(1-\\cos\\theta)^2 = 1 - \\nu^2(1-\\nu^2)(1-\\cos\\theta)^2\n$$\n$$\n|G_{\\mathrm{LW}}|^2 = 1 - 4\\nu^2(1-\\nu^2)\\sin^4(\\theta/2)\n$$\nFor stability, $|G_{\\mathrm{LW}}|^2 \\le 1$, which requires $\\nu^2(1-\\nu^2) \\ge 0$. As $\\nu  0$, this implies $1-\\nu^2 \\ge 0$, or $\\nu^2 \\le 1$. The stability condition is $0  \\nu \\le 1$.\n\n**Comparison of Dispersion and Dissipation for small $\\theta$**\n- **Dissipation (Amplitude Error):**\nFor upwind, $|G_{\\mathrm{up}}|^2 \\approx 1 - 4\\nu(1-\\nu)(\\theta/2)^2 = 1 - \\nu(1-\\nu)\\theta^2$. Thus, $|G_{\\mathrm{up}}| \\approx 1 - \\frac{1}{2}\\nu(1-\\nu)\\theta^2$. The amplitude reduction is of order $O(\\theta^2)$, indicating numerical diffusion.\nFor Lax-Wendroff, $|G_{\\mathrm{LW}}|^2 \\approx 1 - 4\\nu^2(1-\\nu^2)(\\theta/2)^4 = 1 - \\frac{1}{4}\\nu^2(1-\\nu^2)\\theta^4$. Thus, $|G_{\\mathrm{LW}}| \\approx 1 - \\frac{1}{8}\\nu^2(1-\\nu^2)\\theta^4$. The amplitude reduction is of order $O(\\theta^4)$, which is much smaller for small $\\theta$, indicating a far less dissipative scheme.\n- **Dispersion (Phase Error):**\nThe numerical phase speed $c_{\\mathrm{num}}$ is related to the physical speed $c$ by $c_{\\mathrm{num}}/c = -\\arg(G)/(\\nu\\theta)$.\nFor upwind, $\\arg(G_{\\mathrm{up}}) \\approx -\\nu\\theta + \\frac{\\nu}{6}(2\\nu-1)(\\nu-1)\\theta^3$. This gives $c_{\\mathrm{num}}/c \\approx 1 - \\frac{1}{6}(2\\nu-1)(\\nu-1)\\theta^2$.\nFor Lax-Wendroff, $\\arg(G_{\\mathrm{LW}}) \\approx -\\nu\\theta + \\frac{\\nu(1-\\nu^2)}{6}\\theta^3$. This gives $c_{\\mathrm{num}}/c \\approx 1 - \\frac{1-\\nu^2}{6}\\theta^2$.\nBoth schemes have a phase error of order $O(\\theta^2)$, consistent with them being second-order accurate in space (using centered differences implicitly or explicitly). The Lax-Wendroff scheme is always dispersive with phase lag ($c_{\\mathrm{num}}c$) for $\\nu1$, while the upwind scheme's phase error changes character at $\\nu=1/2$.\n\nThe final answer requires the pair of exact amplification factors.\nFor the upwind scheme: $G_{\\mathrm{up}}(\\theta,\\nu) = 1 - \\nu + \\nu\\,\\exp(-i\\,\\theta)$.\nFor the Lax-Wendroff scheme: $G_{\\mathrm{LW}}(\\theta,\\nu) = 1 - i\\,\\nu\\,\\sin(\\theta) + \\nu^2(\\cos(\\theta)-1)$.\nThese are the expressions to be provided.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 - \\nu + \\nu\\,\\exp(-i\\,\\theta)  1 - i\\,\\nu\\,\\sin(\\theta) + \\nu^2(\\cos(\\theta)-1) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond grids and continuous space, environments can be represented as networks of abstract connections, capturing social, ecological, or infrastructural relationships. This exercise investigates a fundamental property of the classic Erdős–Rényi random network model: the sudden emergence of a 'giant' connected component as connectivity increases. By deriving the critical threshold for this phase transition, you will develop insight into how large-scale structure can abruptly appear in complex systems. ",
            "id": "4140762",
            "problem": "A population of interacting agents is embedded in a stochastic network used to represent the environment, modeled as an Erdős–Rényi (ER) random graph $G(n,p)$: there are $n$ labeled vertices and each of the $\\frac{n(n-1)}{2}$ possible undirected edges is present independently with probability $p$. The degree distribution of a uniformly chosen vertex is $\\mathrm{Binomial}(n-1,p)$, and for large $n$ the graph is locally tree-like at finite distances with high probability. Let $p = p(n)$ and define $c_{n} \\equiv n p(n)$. Working in the asymptotic regime $n \\to \\infty$, use only these foundational properties to determine the exact threshold value $c^{\\star}$ such that if $c_{n} \\to c  c^{\\star}$ then the largest connected component has size $o(n)$ with high probability, whereas if $c_{n} \\to c  c^{\\star}$ then with high probability the graph contains a connected component of size $\\Theta(n)$. Provide the value of $c^{\\star}$ as a single exact real number. Express your final answer as an exact value with no rounding.",
            "solution": "The problem asks for the critical threshold for the emergence of a \"giant\" connected component in an Erdős–Rényi random graph $G(n,p)$. The transition from a state where all components are small (of size $o(n)$) to a state with a single large component (of size $\\Theta(n)$) is a classic phase transition phenomenon. We can determine the threshold $c^{\\star}$ by modeling the exploration of a connected component as a branching process, an approach justified by the given property that the graph is locally tree-like for large $n$.\n\nLet us consider the process of exploring the connected component containing a randomly selected vertex. This can be viewed as a breadth-first search. We start with a single vertex (generation $0$). Its neighbors form generation $1$. The neighbors of generation $1$ vertices (not already discovered) form generation $2$, and so on. In the limit $n \\to \\infty$, for a finite number of steps, the probability of encountering a vertex that has already been visited is negligible. This allows us to approximate the component exploration as a Galton-Watson branching process, where vertices are individuals and edges to new vertices represent offspring.\n\nThe fate of a branching process (extinction versus indefinite survival) is determined by the mean number of offspring per individual, which we denote as $\\mu$. A fundamental theorem of branching processes states that if $\\mu  1$, there is a non-zero probability of survival, whereas if $\\mu \\le 1$, the process becomes extinct with probability $1$. In our graph context, \"survival\" corresponds to a giant component of size $\\Theta(n)$, and \"extinction\" corresponds to a small component of size $o(n)$. Thus, the phase transition occurs at $\\mu=1$.\n\nOur task reduces to calculating the mean number of offspring $\\mu$ as a function of the parameter $c = \\lim_{n \\to \\infty} c_n = \\lim_{n \\to \\infty} n p(n)$. The \"offspring\" of a vertex $v$ in the exploration process are its neighbors, excluding the vertex from which $v$ was discovered. The number of such offspring is therefore $d(v) - 1$, where $d(v)$ is the degree of $v$.\n\nFirst, we establish the degree distribution of a vertex chosen uniformly at random. The problem states this is a binomial distribution, $D \\sim \\mathrm{Binomial}(n-1, p)$. In the asymptotic regime where $n \\to \\infty$ and $p = c_n/n$ with $c_n \\to c$, this binomial distribution converges to a Poisson distribution with mean $c$. The probability mass function for a vertex to have degree $k$ becomes:\n$$ \\mathbb{P}(D=k) \\to \\frac{c^k e^{-c}}{k!} $$\nThe mean degree is $\\langle D \\rangle = \\sum_{k=0}^{\\infty} k \\, \\mathbb{P}(D=k) = c$.\n\nNext, we must find the distribution for the number of offspring. For any vertex in the branching process other than the root, we arrive at it by traversing an edge. The degree of a vertex reached by following a random edge does not follow the same $\\mathrm{Poisson}(c)$ distribution. High-degree vertices are incident to more edges, so they are more likely to be selected. The probability of an edge leading to a vertex of degree $k$ is proportional to $k \\mathbb{P}(D=k)$. The resulting distribution for the degree of a vertex at the end of a random edge, $D_{\\text{edge}}$, is:\n$$ \\mathbb{P}(D_{\\text{edge}}=k) = \\frac{k \\mathbb{P}(D=k)}{\\sum_j j \\mathbb{P}(D=j)} = \\frac{k \\mathbb{P}(D=k)}{\\langle D \\rangle} $$\nThe number of offspring for such a vertex is its degree minus one, $D_{\\text{edge}} - 1$. The mean number of offspring, $\\mu$, is the expectation of this quantity:\n$$ \\mu = \\mathbb{E}[D_{\\text{edge}} - 1] = \\mathbb{E}[D_{\\text{edge}}] - 1 $$\nThe expectation $\\mathbb{E}[D_{\\text{edge}}]$ is:\n$$ \\mathbb{E}[D_{\\text{edge}}] = \\sum_{k=1}^{\\infty} k \\, \\mathbb{P}(D_{\\text{edge}}=k) = \\sum_{k=1}^{\\infty} k \\frac{k \\mathbb{P}(D=k)}{\\langle D \\rangle} = \\frac{1}{\\langle D \\rangle} \\sum_{k=0}^{\\infty} k^2 \\mathbb{P}(D=k) = \\frac{\\langle D^2 \\rangle}{\\langle D \\rangle} $$\nFor a Poisson distribution with parameter $c$, the mean is $\\langle D \\rangle = c$ and the variance is $\\mathrm{Var}(D) = c$. The second moment is given by $\\langle D^2 \\rangle = \\mathrm{Var}(D) + \\langle D \\rangle^2 = c + c^2$.\nSubstituting these values, we find the mean number of offspring:\n$$ \\mu = \\frac{\\langle D^2 \\rangle}{\\langle D \\rangle} - 1 = \\frac{c + c^2}{c} - 1 = (1+c) - 1 = c $$\nThe mean number of offspring in the branching process approximation is equal to $c$, the asymptotic average degree of the graph.\n\nThe critical threshold for the phase transition corresponds to the point where the mean number of offspring is exactly $1$.\n$$ \\mu = 1 $$\n$$ c = 1 $$\nTherefore, the threshold value is $c^{\\star} = 1$. If $c_n \\to c  1$, the branching process dies out, implying the largest component is of size $o(n)$. If $c_n \\to c  1$, the branching process survives with non-zero probability, implying the existence of a giant component of size $\\Theta(n)$. This aligns perfectly with the conditions given in the problem statement. The exact threshold value is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}