{
    "hands_on_practices": [
        {
            "introduction": "智能体行为规则的核心是其决策的概率性质。本练习将引导你从第一性原理出发，推导在一个简单环境中，一个具有随机策略的智能体的最终行动概率。通过这个实践，你将掌握如何利用全概率定律，将智能体的内部状态依赖策略与外部环境的状态分布联系起来，这是理解和建模随机行为的基础。",
            "id": "4119665",
            "problem": "考虑一个嵌入在复杂自适应系统（CAS）中的智能体，该智能体被建模为马尔可夫决策过程（MDP）中的一个有限状态随机决策者。设状态空间为 $\\mathcal{S}=\\{1,2\\}$，动作空间为 $\\mathcal{A}=\\{0,1\\}$。在时间 $t$，环境根据 $\\mathcal{S}$ 上的概率分布 $\\mu_t$ 产生一个随机状态 $S_t \\in \\mathcal{S}$，其中 $\\mu_t(1)+\\mu_t(2)=1$ 且对于每个 $s\\in\\mathcal{S}$ 都有 $\\mu_t(s)\\in[0,1]$。智能体的行为规则由一个平稳策略 $\\pi$ 给出，该策略是一个条件概率核，满足对于每个 $s\\in\\mathcal{S}$，$\\pi(a|s)\\in[0,1]$ 且 $\\sum_{a\\in\\mathcal{A}}\\pi(a|s)=1$。具体来说，假设策略满足 $\\pi(a=1\\,|\\,s=1)=p$ 和 $\\pi(a=1\\,|\\,s=2)=q$，其中 $p,q\\in[0,1]$，因此 $\\pi(a=0\\,|\\,s=1)=1-p$ 和 $\\pi(a=0\\,|\\,s=2)=1-q$。\n\n定义动作随机变量 $A_t\\in\\mathcal{A}$ 是根据行为规则 $\\pi(\\cdot\\,|\\,S_t)$ 抽取的，即在给定 $S_t=s$ 的条件下，$A_t$ 的条件分布是 $\\pi(\\cdot\\,|\\,s)$。仅使用条件概率的核心定义、全概率定律以及作为条件概率核的行为规则规范，推导边际概率 $\\mathbb{P}(A_t=1)$ 作为 $p$、$q$、$\\mu_t(1)$ 和 $\\mu_t(2)$ 的函数的闭式表达式。请将您的最终结果表示为单个解析表达式。无需数值四舍五入，也无物理单位适用。",
            "solution": "该设定为状态和动作变量提供了一个有限概率空间。设 $S_t$ 是时间 $t$ 的状态，$A_t$ 是时间 $t$ 的动作。状态分布由 $\\mu_t$ 给出，因此对于 $s\\in\\{1,2\\}$，有 $\\mathbb{P}(S_t=s)=\\mu_t(s)$。智能体的行为规则由一个条件概率核 $\\pi$ 表示，它规定了 $\\mathbb{P}(A_t=a\\,|\\,S_t=s)=\\pi(a\\,|\\,s)$。\n\n我们要求解边际概率 $\\mathbb{P}(A_t=1)$。根据由状态导出的划分上的全概率定律，并使用条件概率的定义，我们有\n$$\n\\mathbb{P}(A_t=1)\n=\\sum_{s\\in\\mathcal{S}}\\mathbb{P}(A_t=1\\,|\\,S_t=s)\\,\\mathbb{P}(S_t=s)\n=\\sum_{s\\in\\{1,2\\}}\\pi(1\\,|\\,s)\\,\\mu_t(s).\n$$\n题目给出了 $\\pi(1\\,|\\,1)=p$ 和 $\\pi(1\\,|\\,2)=q$。将这些代入求和式中得到\n$$\n\\mathbb{P}(A_t=1)=p\\,\\mu_t(1)+q\\,\\mu_t(2).\n$$\n该表达式是策略参数 $p$ 和 $q$ 以及状态分布分量 $\\mu_t(1)$ 和 $\\mu_t(2)$ 的闭式解析函数，直接从核心定义和全概率定律推导得出，没有任何额外假设。",
            "answer": "$$\\boxed{p\\,\\mu_t(1)+q\\,\\mu_t(2)}$$"
        },
        {
            "introduction": "并非所有随机策略都具有相同程度的“随机性”。为了量化智能体行为的不确定性，我们引入了信息论中的熵概念。此练习要求你推导一个二元行动策略的熵，并分析其在确定性与随机性规则下的表现，最终找出使不确定性最大化的条件。这将帮助你深入理解策略的探索性，并为更高级的主题（如探索-利用权衡）奠定基础。",
            "id": "4119714",
            "problem": "在一个建模为马尔可夫决策过程的单状态决策环境中，一个智能体在状态 $s$ 通过一个在二元动作集 $\\{a_{0}, a_{1}\\}$ 上的平稳随机策略 $\\pi(\\cdot \\mid s)$ 来定义其行为规则。该智能体以概率 $p \\in [0,1]$ 选择 $a_{1}$，以概率 $1-p$ 选择 $a_{0}$，这参数化了一族行为规则，范围从确定性规则 ($p \\in \\{0,1\\}$) 到随机性规则 ($p \\in (0,1)$)。使用离散分布的香农熵核心定义，以自然对数（熵的单位为奈特）为基础起点。以此为基础，推导熵 $H(\\pi(\\cdot \\mid s))$ 作为 $p$ 的函数表达式，并通过第一性原理进行推理，描述确定性规则和随机性规则之间的熵如何比较。找出使熵最大化的 $p$ 值，并根据推导出的表达式进行证明，不要预先假设其最大值的任何结论。最后，使用自然对数计算当 $p=\\frac{1}{2}$ 时 $H(\\pi(\\cdot \\mid s))$ 的精确闭式解。以奈特为单位表示最终的熵值，并提供精确表达式；不要进行数值近似或四舍五入。",
            "solution": "对于离散概率分布 $P = \\{p_1, p_2, \\dots, p_n\\}$，香non熵的基本定义由下式给出：\n$$H(P) = -\\sum_{i=1}^{n} p_i \\log_b(p_i)$$\n问题指定使用自然对数，因此底数 $b$ 是欧拉数 $e$，熵以奈特为单位。策略 $\\pi(\\cdot \\mid s)$ 在动作集 $\\{a_0, a_1\\}$ 上定义了一个离散概率分布。两个动作的概率为：\n$$ \\pi(a_1 \\mid s) = p $$\n$$ \\pi(a_0 \\mid s) = 1-p $$\n将香non熵的定义应用于此策略，我们得到熵 $H(\\pi(\\cdot \\mid s))$ 作为 $p$ 的函数，我们将其记为 $H(p)$：\n$$ H(p) = - \\left( \\pi(a_0 \\mid s) \\ln(\\pi(a_0 \\mid s)) + \\pi(a_1 \\mid s) \\ln(\\pi(a_1 \\mid s)) \\right) $$\n代入给定的概率，得到熵的表达式：\n$$ H(p) = - \\left( (1-p)\\ln(1-p) + p\\ln(p) \\right) $$\n这个表达式对 $p \\in (0,1)$ 有效。对于端点 $p=0$ 和 $p=1$，我们必须评估当 $x \\to 0^+$ 时 $x\\ln(x)$ 的极限。使用洛必达法则：\n$$ \\lim_{x \\to 0^+} x\\ln(x) = \\lim_{x \\to 0^+} \\frac{\\ln(x)}{1/x} = \\lim_{x \\to 0^+} \\frac{1/x}{-1/x^2} = \\lim_{x \\to 0^+} (-x) = 0 $$\n根据这个标准约定，我们定义 $0\\ln(0) = 0$。\n\n接下来，我们比较确定性规则和随机性规则的熵。\n- **确定性规则：** $p \\in \\{0, 1\\}$。\n  - 如果 $p=0$，策略是总是选择 $a_0$。熵为 $H(0) = -((1-0)\\ln(1-0) + 0\\ln(0)) = -(1\\ln(1) + 0) = 0$。\n  - 如果 $p=1$，策略是总是选择 $a_1$。熵为 $H(1) = -((1-1)\\ln(1-1) + 1\\ln(1)) = -(0\\ln(0) + 0) = 0$。\n  因此，对于确定性规则，熵为 $0$ 奈特。这反映了要采取的动作是完全确定的。\n- **随机性规则：** $p \\in (0, 1)$。\n  - 对于此开区间中的任何 $p$，$p$ 和 $1-p$ 都是严格介于 $0$ 和 $1$ 之间的数。\n  - 介于 $0$ 和 $1$ 之间的数的自然对数是负数。因此，$\\ln(p)  0$ 且 $\\ln(1-p)  0$。\n  - 项 $p\\ln(p)$ 和 $(1-p)\\ln(1-p)$ 都是负数。\n  - 它们的和是严格为负的。\n  - 因此，$H(p) = -(\\text{负值}) > 0$。\n任何随机性规则的熵都严格大于任何确定性规则的熵。熵量化了不确定性，而随机性规则固有地比确定性规则拥有更多的不确定性。\n\n为了找到使熵最大化的 $p$ 值，我们在区间 $[0,1]$ 上分析函数 $H(p) = -p\\ln(p) - (1-p)\\ln(1-p)$。我们通过求关于 $p$ 的一阶导数并将其设为零来找到临界点。\n$$ \\frac{dH}{dp} = \\frac{d}{dp} \\left( -p\\ln(p) - (1-p)\\ln(1-p) \\right) $$\n使用乘积法则进行微分：\n$$ \\frac{dH}{dp} = -\\left[\\left(1\\cdot\\ln(p) + p\\cdot\\frac{1}{p}\\right) + \\left(-1\\cdot\\ln(1-p) + (1-p)\\cdot\\frac{-1}{1-p}\\right)\\right] $$\n$$ \\frac{dH}{dp} = -\\left[ (\\ln(p) + 1) + (-\\ln(1-p) - 1) \\right] $$\n$$ \\frac{dH}{dp} = -[\\ln(p) - \\ln(1-p)] = \\ln(1-p) - \\ln(p) = \\ln\\left(\\frac{1-p}{p}\\right) $$\n将导数设为零以找到极值：\n$$ \\ln\\left(\\frac{1-p}{p}\\right) = 0 $$\n$$ \\frac{1-p}{p} = \\exp(0) = 1 $$\n$$ 1-p = p \\implies 2p = 1 \\implies p = \\frac{1}{2} $$\n为确认这是一个最大值，我们考察二阶导数：\n$$ \\frac{d^2H}{dp^2} = \\frac{d}{dp}\\left( \\ln(1-p) - \\ln(p) \\right) = \\frac{-1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right) $$\n对于任何 $p \\in (0, 1)$，$p$ 和 $1-p$ 都是正数。因此，项 $\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right)$ 严格为正。这使得二阶导数 $\\frac{d^2H}{dp^2}$ 对所有 $p \\in (0,1)$ 都严格为负。负的二阶导数表明函数 $H(p)$ 是严格凹的，因此临界点 $p=\\frac{1}{2}$ 是一个唯一的全局最大值点。当结果的不确定性最大时，熵达到最大化，这发生在两个动作等概率时。\n\n最后，我们计算当 $p = \\frac{1}{2}$ 时的最大熵的精确值。\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\left(1-\\frac{1}{2}\\right)\\ln\\left(1-\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( 2 \\cdot \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) = -\\ln\\left(\\frac{1}{2}\\right) $$\n使用对数性质 $-\\ln(x) = \\ln(x^{-1})$：\n$$ H\\left(\\frac{1}{2}\\right) = \\ln\\left(\\left(\\frac{1}{2}\\right)^{-1}\\right) = \\ln(2) $$\n最大熵的精确闭式解为 $\\ln(2)$ 奈特。",
            "answer": "$$\\boxed{\\ln(2)}$$"
        },
        {
            "introduction": "理论模型必须能与现实数据相结合，才能发挥其预测和解释能力。本练习将带你实践如何为一个参数化的行为规则（逻辑策略）构建似然函数，并从观测数据中推导出其参数的最大似然估计。掌握这一过程对于从经验数据中校准和验证智能体模型至关重要，是连接理论与应用的关键一步。",
            "id": "4119704",
            "problem": "考虑一个复杂自适应系统中的智能体群体。在由 $t \\in \\{1,2,\\dots,T\\}$ 索引的离散时间点，每个智能体观察一个二元状态 $s_t \\in \\{0,1\\}$ 并选择一个二元行动 $a_t \\in \\{0,1\\}$。假设智能体的行为规则是参数化的，由一个逻辑斯谛策略给出\n$$\n\\pi_{\\theta}(a=1 \\mid s) \\;=\\; \\sigma(\\theta s),\n$$\n其中 $\\theta \\in \\mathbb{R}$ 是一个未知参数，$\\sigma(x)$ 表示逻辑斯谛S型函数，定义为 $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$。假设在给定状态的情况下，行动在时间上是条件独立的，因此在给定状态 $(s_1,\\dots,s_T)$ 和参数 $\\theta$ 的条件下，观测到的行动 $(a_1,\\dots,a_T)$ 的联合概率可以分解为 $\\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t)$。\n\n从伯努利分布和逻辑斯谛S型函数的基本定义出发，执行以下操作：\n- 为观测数据构建似然函数 $\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t)$。\n- 从第一性原理推导 $\\theta$ 的最大似然估计量 (MLE)，即最大化似然（或等价地，对数似然）的值 $\\hat{\\theta}$，并用观测数据的封闭形式表达式表示。\n\n令 $n_1 = \\sum_{t=1}^{T} \\mathbf{1}\\{s_t=1\\}$ 且 $k_1 = \\sum_{t=1}^{T} a_t \\mathbf{1}\\{s_t=1\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。假设 $n_1 \\ge 1$ 且在 $s_t=1$ 的时间点中的条件样本均值满足 $0  \\frac{k_1}{n_1}  1$，以确保 MLE 存在且为有限值。将您的最终答案表示为关于 $n_1$ 和 $k_1$ 的单个封闭形式解析表达式。无需四舍五入。不要包含任何单位。",
            "solution": "该问题要求推导逻辑斯谛策略模型参数 $\\theta$ 的最大似然估计量 (MLE)。推导将从第一性原理出发，首先构建观测数据的似然函数。\n\n智能体的行为规则是一个由下式给出的逻辑斯谛策略：\n$$\n\\pi_{\\theta}(a=1 \\mid s) = \\sigma(\\theta s) = \\frac{1}{1+\\exp(-\\theta s)}\n$$\n其中 $a_t \\in \\{0,1\\}$ 是智能体在时间 $t$ 的行动，$s_t \\in \\{0,1\\}$ 是观测到的状态。对于给定的状态 $s_t$，行动 $a_t$ 可以被建模为一个伯努利随机变量，$a_t \\sim \\text{Bernoulli}(p_t)$，其成功概率为 $p_t = \\pi_{\\theta}(a_t=1 \\mid s_t)$。\n\n单个观测 $(a_t, s_t)$ 的概率质量函数可以写成紧凑形式：\n$$\n\\pi_{\\theta}(a_t \\mid s_t) = \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t}\n$$\n当 $a_t=1$ 时，此表达式正确地给出 $\\sigma(\\theta s_t)$；当 $a_t=0$ 时，此表达式正确地给出 $1-\\sigma(\\theta s_t)$。\n\n问题陈述，在给定状态的情况下，行动在时间上是条件独立的。因此，整个观测序列 $\\{(a_t, s_t)\\}_{t=1}^{T}$ 的总似然函数 $\\mathcal{L}(\\theta)$ 是各个概率的乘积：\n$$\n\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t) = \\prod_{t=1}^{T} \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t}\n$$\n这就是观测数据的似然函数。\n\n为了找到 MLE $\\hat{\\theta}$，最大化对数似然函数 $\\ell(\\theta) = \\ln(\\mathcal{L}(\\theta))$ 更为方便，因为对数是严格递增函数，它将产生相同的最大化参数。\n$$\n\\ell(\\theta) = \\ln\\left( \\prod_{t=1}^{T} \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t} \\right)\n$$\n利用对数的性质，上式变为：\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\ln(\\sigma(\\theta s_t)) + (1-a_t) \\ln(1 - \\sigma(\\theta s_t)) \\right]\n$$\n我们可以简化涉及S型函数的项。一个有用的恒等式是 $1-\\sigma(x) = \\sigma(-x)$。另一个方法是将这些项表示为与 $\\ln(1+\\exp(x))$ 相关的形式。\n使用 $\\sigma(x) = \\frac{\\exp(x)}{1+\\exp(x)}$ 和 $1-\\sigma(x) = \\frac{1}{1+\\exp(x)}$，我们有：\n$\\ln(\\sigma(x)) = \\ln(\\exp(x)) - \\ln(1+\\exp(x)) = x - \\ln(1+\\exp(x))$。\n$\\ln(1-\\sigma(x)) = -\\ln(1+\\exp(x))$。\n代入 $x = \\theta s_t$：\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t (\\theta s_t - \\ln(1+\\exp(\\theta s_t))) + (1-a_t)(-\\ln(1+\\exp(\\theta s_t))) \\right]\n$$\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\theta s_t - a_t \\ln(1+\\exp(\\theta s_t)) - \\ln(1+\\exp(\\theta s_t)) + a_t \\ln(1+\\exp(\\theta s_t)) \\right]\n$$\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\theta s_t - \\ln(1+\\exp(\\theta s_t)) \\right]\n$$\n为了继续推导，我们可以根据状态 $s_t \\in \\{0, 1\\}$ 的值来拆分求和。\n令 $T_1 = \\{t \\mid s_t=1\\}$ 且 $T_0 = \\{t \\mid s_t=0\\}$。\n$$\n\\ell(\\theta) = \\sum_{t \\in T_0} \\left[ a_t \\theta (0) - \\ln(1+\\exp(\\theta \\cdot 0)) \\right] + \\sum_{t \\in T_1} \\left[ a_t \\theta (1) - \\ln(1+\\exp(\\theta \\cdot 1)) \\right]\n$$\n对于 $t \\in T_0$，项为 $0 - \\ln(1+\\exp(0)) = -\\ln(2)$。\n对于 $t \\in T_1$，项为 $a_t \\theta - \\ln(1+\\exp(\\theta))$。\n令 $n_1 = |T_1| = \\sum_{t=1}^{T} \\mathbf{1}\\{s_t=1\\}$ 且 $n_0 = |T_0| = T-n_1$。对数似然函数变为：\n$$\n\\ell(\\theta) = \\sum_{t \\in T_0} (-\\ln(2)) + \\sum_{t \\in T_1} (a_t \\theta - \\ln(1+\\exp(\\theta)))\n$$\n$$\n\\ell(\\theta) = -n_0 \\ln(2) + \\theta \\sum_{t \\in T_1} a_t - \\sum_{t \\in T_1} \\ln(1+\\exp(\\theta))\n$$\n求和 $\\sum_{t \\in T_1} a_t$ 是当状态为 $1$ 时行动为 $1$ 的次数。这正是给定的量 $k_1 = \\sum_{t=1}^{T} a_t \\mathbf{1}\\{s_t=1\\}$。第二个求和是对一个常数项进行的，所以它等于 $n_1 \\ln(1+\\exp(\\theta))$。\n$$\n\\ell(\\theta) = -n_0 \\ln(2) + k_1 \\theta - n_1 \\ln(1+\\exp(\\theta))\n$$\n为了找到最大化 $\\ell(\\theta)$ 的值 $\\hat{\\theta}$，我们对 $\\theta$ 求导，并令结果为零。\n$$\n\\frac{d\\ell}{d\\theta} = \\frac{d}{d\\theta} [-n_0 \\ln(2) + k_1 \\theta - n_1 \\ln(1+\\exp(\\theta))]\n$$\n$$\n\\frac{d\\ell}{d\\theta} = 0 + k_1 - n_1 \\frac{1}{1+\\exp(\\theta)} \\cdot \\exp(\\theta) = k_1 - n_1 \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}\n$$\n将导数设为零以找到临界点 $\\hat{\\theta}$：\n$$\nk_1 - n_1 \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})} = 0\n$$\n$$\nk_1 = n_1 \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}\n$$\n根据假设 $n_1 \\ge 1$，我们可以除以 $n_1$：\n$$\n\\frac{k_1}{n_1} = \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}\n$$\n令 $p = \\frac{k_1}{n_1}$。方程为 $p = \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}$。我们求解 $\\hat{\\theta}$：\n$$\np(1+\\exp(\\hat{\\theta})) = \\exp(\\hat{\\theta})\n$$\n$$\np + p \\exp(\\hat{\\theta}) = \\exp(\\hat{\\theta})\n$$\n$$\np = \\exp(\\hat{\\theta}) - p \\exp(\\hat{\\theta}) = \\exp(\\hat{\\theta})(1-p)\n$$\n$$\n\\frac{p}{1-p} = \\exp(\\hat{\\theta})\n$$\n对两边取自然对数，得到 MLE $\\hat{\\theta}$：\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{p}{1-p}\\right)\n$$\n将 $p = k_1/n_1$ 代回表达式中：\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{k_1/n_1}{1 - k_1/n_1}\\right) = \\ln\\left(\\frac{k_1/n_1}{(n_1-k_1)/n_1}\\right)\n$$\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{k_1}{n_1-k_1}\\right)\n$$\n假设 $0  k_1/n_1  1$ 确保了 $k_1 > 0$ 和 $k_1  n_1$，这意味着 $n_1-k_1 > 0$。因此，对数的参数是正且有限的，这保证了 $\\hat{\\theta}$ 是一个定义良好且有限的实数。对数似然的二阶导数是 $\\frac{d^2\\ell}{d\\theta^2} = -n_1 \\sigma(\\theta)(1-\\sigma(\\theta))$，对于任何有限的 $\\theta$（因为 $n_1 \\ge 1$ 且 $\\sigma(\\theta) \\in (0,1)$），它都是严格为负的，这证实了该临界点是唯一的全局最大值点。",
            "answer": "$$\n\\boxed{\\ln\\left(\\frac{k_1}{n_1-k_1}\\right)}\n$$"
        }
    ]
}