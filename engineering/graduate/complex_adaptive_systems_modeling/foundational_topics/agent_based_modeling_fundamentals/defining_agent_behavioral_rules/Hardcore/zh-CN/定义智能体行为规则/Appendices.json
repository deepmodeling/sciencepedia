{
    "hands_on_practices": [
        {
            "introduction": "定义智能体行为规则的第一步是精确描述其决策过程的随机性。本练习将引导你运用全概率定律，这是一个基础但极其强大的工具，用于连接智能体的内部策略（在特定状态下选择某个动作的倾向）和外部环境状态的概率分布。通过这个实践，你将掌握如何从智能体的策略 $\\pi(a|s)$ 和状态分布 $\\mu_t(s)$ 出发，计算其在任意时刻 $t$ 采取某一特定行动的边际概率 。",
            "id": "4119665",
            "problem": "考虑一个嵌入在复杂自适应系统 (CAS) 中的智能体，该智能体被建模为马尔可夫决策过程 (MDP) 内的有限状态随机决策者。设状态空间为 $\\mathcal{S}=\\{1,2\\}$，动作空间为 $\\mathcal{A}=\\{0,1\\}$。在时间 $t$，环境根据 $\\mathcal{S}$ 上的一个概率分布 $\\mu_t$ 产生一个随机状态 $S_t \\in \\mathcal{S}$，其中 $\\mu_t(1)+\\mu_t(2)=1$ 且对于每个 $s\\in\\mathcal{S}$ 都有 $\\mu_t(s)\\in[0,1]$。该智能体的行为规则由一个稳态策略 $\\pi$ 给出，它是一个条件概率核，满足对于每个 $s\\in\\mathcal{S}$，都有 $\\pi(a|s)\\in[0,1]$ 和 $\\sum_{a\\in\\mathcal{A}}\\pi(a|s)=1$。具体来说，假设该策略满足 $\\pi(a=1\\,|\\,s=1)=p$ 和 $\\pi(a=1\\,|\\,s=2)=q$，其中 $p,q\\in[0,1]$，因此有 $\\pi(a=0\\,|\\,s=1)=1-p$ 和 $\\pi(a=0\\,|\\,s=2)=1-q$。\n\n定义动作随机变量 $A_t\\in\\mathcal{A}$ 是根据行为规则 $\\pi(\\cdot\\,|\\,S_t)$ 抽取的，即在给定 $S_t=s$ 的条件下，$A_t$ 的条件分布是 $\\pi(\\cdot\\,|\\,s)$。仅使用条件概率的核心定义、全概率定律以及作为条件概率核的行为规则的规范，推导边缘概率 $\\mathbb{P}(A_t=1)$ 作为 $p$、$q$、$\\mu_t(1)$ 和 $\\mu_t(2)$ 的函数的封闭形式表达式。请以单一解析表达式的形式给出你的最终结果。不需要进行数值取整，也不适用任何物理单位。",
            "solution": "该设定为状态和动作变量提供了一个有限概率空间。设 $S_t$ 是时间 $t$ 的状态，$A_t$ 是时间 $t$ 的动作。状态分布由 $\\mu_t$ 给出，因此对于 $s\\in\\{1,2\\}$ 有 $\\mathbb{P}(S_t=s)=\\mu_t(s)$。智能体的行为规则由条件概率核 $\\pi$ 表示，它指定了 $\\mathbb{P}(A_t=a\\,|\\,S_t=s)=\\pi(a\\,|\\,s)$。\n\n我们要求解边缘概率 $\\mathbb{P}(A_t=1)$。根据由状态导出的划分上的全概率定律，并使用条件概率的定义，我们有\n$$\n\\mathbb{P}(A_t=1)\n=\\sum_{s\\in\\mathcal{S}}\\mathbb{P}(A_t=1\\,|\\,S_t=s)\\,\\mathbb{P}(S_t=s)\n=\\sum_{s\\in\\{1,2\\}}\\pi(1\\,|\\,s)\\,\\mu_t(s).\n$$\n问题给出了 $\\pi(1\\,|\\,1)=p$ 和 $\\pi(1\\,|\\,2)=q$。将这些代入求和式中可得\n$$\n\\mathbb{P}(A_t=1)=p\\,\\mu_t(1)+q\\,\\mu_t(2).\n$$\n这个表达式是策略参数 $p$ 和 $q$ 以及状态分布分量 $\\mu_t(1)$ 和 $\\mu_t(2)$ 的一个封闭形式解析函数，它是直接从核心定义和全概率定律推导出来的，没有任何额外的假设。",
            "answer": "$$\\boxed{p\\,\\mu_t(1)+q\\,\\mu_t(2)}$$"
        },
        {
            "introduction": "在建立了概率性行为规则的数学基础后，一个自然的问题是：如何量化规则的“不确定性”或“随机性”？本练习引入了信息论中的核心概念——香农熵，来度量一个策略的内在不确定性。你将从第一性原理出发，推导一个简单二元动作策略的熵表达式，并分析熵与策略确定性之间的关系，最终找到并计算最大熵所对应的条件 。",
            "id": "4119714",
            "problem": "在一个建模为马尔可夫决策过程的单状态决策环境中，一个智能体通过在二元动作集 $\\{a_{0}, a_{1}\\}$ 上的一个平稳随机策略 $\\pi(\\cdot \\mid s)$ 来定义其在状态 $s$ 的行为规则。该智能体以概率 $p \\in [0,1]$ 选择动作 $a_{1}$，并以概率 $1-p$ 选择动作 $a_{0}$，这参数化了一系列从确定性（$p \\in \\{0,1\\}$）到随机性（$p \\in (0,1)$）的行为规则。请使用以自然对数（熵的单位为奈特）计算的离散分布香农熵的核心定义作为基础出发点。以此为基础，推导熵 $H(\\pi(\\cdot \\mid s))$ 作为 $p$ 的函数的表达式，并通过第一性原理推理，描述确定性规则与随机性规则之间熵的比较。找出使熵最大化的 $p$ 值，并根据推导出的表达式进行证明，不假设任何关于其最大值的已知结论。最后，使用自然对数计算当 $p=\\frac{1}{2}$ 时 $H(\\pi(\\cdot \\mid s))$ 的精确闭式解。最终的熵值需以奈特为单位表示，并提供精确表达式；不要进行数值近似或四舍五入。",
            "solution": "离散概率分布 $P = \\{p_1, p_2, \\dots, p_n\\}$ 的香农熵基本定义为 $H(P) = -\\sum_{i=1}^{n} p_i \\log_b(p_i)$。根据题意，我们使用自然对数（底数为 $e$），熵的单位为奈特。\n\n智能体的策略 $\\pi(\\cdot \\mid s)$ 在动作集 $\\{a_0, a_1\\}$ 上定义了一个概率分布，其中 $\\pi(a_1 \\mid s) = p$ 且 $\\pi(a_0 \\mid s) = 1-p$。将这些概率代入熵的定义，我们得到熵 $H(\\pi(\\cdot \\mid s))$ 作为 $p$ 的函数，记为 $H(p)$：\n$$ H(p) = - \\left( p\\ln(p) + (1-p)\\ln(1-p) \\right) $$\n该表达式对 $p \\in (0,1)$ 有效。在计算中，我们使用标准约定 $0\\ln(0) = 0$。\n\n**熵的比较**：\n-   对于**确定性规则**（$p=0$ 或 $p=1$）：\n    -   若 $p=0$, $H(0) = -(0\\ln(0) + 1\\ln(1)) = 0$。\n    -   若 $p=1$, $H(1) = -(1\\ln(1) + 0\\ln(0)) = 0$。\n    确定性规则的熵为0，表示结果完全没有不确定性。\n-   对于**随机性规则**（$p \\in (0,1)$）：\n    由于 $p$ 和 $1-p$ 均在 $(0,1)$ 区间内，它们的自然对数都是负数。因此，$p\\ln(p)  0$ 且 $(1-p)\\ln(1-p)  0$。所以，$H(p) = -(\\text{负数}) > 0$。\n    这表明任何随机性规则的熵都严格大于确定性规则的熵。\n\n**最大化熵**：\n为了找到使熵最大化的 $p$ 值，我们对 $H(p)$ 求一阶导数并将其设为零：\n$$ \\frac{dH}{dp} = -\\left[ (1 \\cdot \\ln(p) + p \\cdot \\frac{1}{p}) + (-1 \\cdot \\ln(1-p) + (1-p) \\cdot \\frac{-1}{1-p}) \\right] $$\n$$ \\frac{dH}{dp} = -[ (\\ln(p) + 1) - (\\ln(1-p) + 1) ] = \\ln(1-p) - \\ln(p) = \\ln\\left(\\frac{1-p}{p}\\right) $$\n令导数为零：\n$$ \\ln\\left(\\frac{1-p}{p}\\right) = 0 \\implies \\frac{1-p}{p} = 1 \\implies 1-p = p \\implies p = \\frac{1}{2} $$\n为了确认这是一个最大值，我们检查二阶导数：\n$$ \\frac{d^2H}{dp^2} = \\frac{d}{dp}\\left( \\ln(1-p) - \\ln(p) \\right) = -\\frac{1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right) $$\n对于 $p \\in (0,1)$，该二阶导数恒为负，表明 $H(p)$ 是一个严格凹函数，因此 $p=\\frac{1}{2}$ 是唯一的全局最大值点。这符合直觉，即当两个动作等可能时，不确定性最大。\n\n**计算最大熵值**：\n将 $p=\\frac{1}{2}$ 代入熵的表达式：\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) = -\\ln\\left(\\frac{1}{2}\\right) $$\n使用对数性质 $-\\ln(x) = \\ln(1/x)$，我们得到：\n$$ H\\left(\\frac{1}{2}\\right) = \\ln(2) $$\n因此，最大熵的精确值为 $\\ln(2)$ 奈特。",
            "answer": "$$\\boxed{\\ln(2)}$$"
        },
        {
            "introduction": "理论模型与现实应用的桥梁在于我们能否从观测数据中学习智能体的行为规则。本练习将带你进入参数估计的核心领域，具体任务是为一个采用逻辑斯谛策略（一种在机器学习和统计学中广泛使用的模型）的智能体推导其参数的最大似然估计（MLE）。通过构建似然函数并进行优化，你将亲手实践如何从智能体的历史决策数据 $(s_t, a_t)$ 中“反向工程”出其内在的行为模式 。",
            "id": "4119704",
            "problem": "考虑一个复杂适应系统中的主体群体。在由 $t \\in \\{1,2,\\dots,T\\}$ 索引的离散时间点，每个主体观察一个二元状态 $s_t \\in \\{0,1\\}$ 并选择一个二元行动 $a_t \\in \\{0,1\\}$。假设主体的行为规则是参数化的，并由逻辑策略给出\n$$\n\\pi_{\\theta}(a=1 \\mid s) \\;=\\; \\sigma(\\theta s),\n$$\n其中 $\\theta \\in \\mathbb{R}$ 是一个未知参数，$\\sigma(x)$ 表示逻辑S型函数，定义为 $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$。假设给定状态时，行动在时间上是条件独立的，因此给定状态 $(s_1,\\dots,s_T)$ 和参数 $\\theta$ 时，观测到的行动 $(a_1,\\dots,a_T)$ 的联合概率可以分解为 $\\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t)$。\n\n从伯努利分布和逻辑S型函数的基本定义出发，执行以下操作：\n- 为观测数据构建似然函数 $\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t)$。\n- 从第一性原理推导 $\\theta$ 的最大似然估计量 (MLE)，即最大化似然函数（或等价地，对数似然函数）的值 $\\hat{\\theta}$，并以观测数据的封闭形式表示。\n\n令 $n_1 = \\sum_{t=1}^{T} \\mathbf{1}\\{s_t=1\\}$ 和 $k_1 = \\sum_{t=1}^{T} a_t \\mathbf{1}\\{s_t=1\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。假设 $n_1 \\ge 1$ 并且在 $s_t=1$ 的时间点上的条件样本均值满足 $0  \\frac{k_1}{n_1}  1$，以便 MLE 存在且为有限值。将你的最终答案表示为关于 $n_1$ 和 $k_1$ 的单个封闭形式解析表达式。不需要四舍五入。不要包含任何单位。",
            "solution": "该问题要求推导逻辑策略模型参数 $\\theta$ 的最大似然估计量 (MLE)。推导将从第一性原理出发，首先构建观测数据的似然函数。\n\n主体的行为规则是由以下逻辑策略给出的：\n$$\n\\pi_{\\theta}(a=1 \\mid s) = \\sigma(\\theta s) = \\frac{1}{1+\\exp(-\\theta s)}\n$$\n其中 $a_t \\in \\{0,1\\}$ 是主体在时间 $t$ 的行动，$s_t \\in \\{0,1\\}$ 是观测到的状态。对于给定的状态 $s_t$，行动 $a_t$ 可以被建模为一个伯努利随机变量，$a_t \\sim \\text{Bernoulli}(p_t)$，其成功概率为 $p_t = \\pi_{\\theta}(a_t=1 \\mid s_t)$。\n\n单个观测 $(a_t, s_t)$ 的概率质量函数可以紧凑地写成：\n$$\n\\pi_{\\theta}(a_t \\mid s_t) = \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t}\n$$\n这个表达式在 $a_t=1$ 时正确地给出 $\\sigma(\\theta s_t)$，在 $a_t=0$ 时给出 $1-\\sigma(\\theta s_t)$。\n\n问题陈述，给定状态，行动在时间上是条件独立的。因此，整个观测序列 $\\{(a_t, s_t)\\}_{t=1}^{T}$ 的总似然函数 $\\mathcal{L}(\\theta)$ 是各个概率的乘积：\n$$\n\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\pi_{\\theta}(a_t \\mid s_t) = \\prod_{t=1}^{T} \\left[ \\sigma(\\theta s_t) \\right]^{a_t} \\left[ 1 - \\sigma(\\theta s_t) \\right]^{1-a_t}\n$$\n这就是观测数据的似然函数。\n\n为了找到 MLE $\\hat{\\theta}$，更方便的是最大化对数似然函数 $\\ell(\\theta) = \\ln(\\mathcal{L}(\\theta))$，因为对数是一个严格递增函数，它将产生相同的最大化参数。\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\ln(\\sigma(\\theta s_t)) + (1-a_t) \\ln(1 - \\sigma(\\theta s_t)) \\right]\n$$\n使用恒等式 $\\ln(\\sigma(x)) = x - \\ln(1+\\exp(x))$ 和 $\\ln(1-\\sigma(x)) = -\\ln(1+\\exp(x))$，并代入 $x=\\theta s_t$，对数似然函数可以简化为：\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T} \\left[ a_t \\theta s_t - \\ln(1+\\exp(\\theta s_t)) \\right]\n$$\n我们可以根据状态 $s_t \\in \\{0, 1\\}$ 的值来拆分求和。\n令 $T_1 = \\{t \\mid s_t=1\\}$ 且 $n_1 = |T_1|$，$T_0 = \\{t \\mid s_t=0\\}$ 且 $n_0 = |T_0|$。\n$$\n\\ell(\\theta) = \\sum_{t \\in T_0} \\left[ 0 - \\ln(1+\\exp(0)) \\right] + \\sum_{t \\in T_1} \\left[ a_t \\theta - \\ln(1+\\exp(\\theta)) \\right]\n$$\n$$\n\\ell(\\theta) = -n_0 \\ln(2) + \\theta \\sum_{t \\in T_1} a_t - n_1 \\ln(1+\\exp(\\theta))\n$$\n根据定义，$k_1 = \\sum_{t \\in T_1} a_t = \\sum_{t=1}^{T} a_t \\mathbf{1}\\{s_t=1\\}$。因此，\n$$\n\\ell(\\theta) = -n_0 \\ln(2) + k_1 \\theta - n_1 \\ln(1+\\exp(\\theta))\n$$\n为了找到最大化 $\\ell(\\theta)$ 的值 $\\hat{\\theta}$，我们对 $\\theta$ 求导，并令结果为零。\n$$\n\\frac{d\\ell}{d\\theta} = k_1 - n_1 \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}\n$$\n将导数设为零以找到临界点 $\\hat{\\theta}$：\n$$\nk_1 - n_1 \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})} = 0\n$$\n根据假设 $n_1 \\ge 1$，我们可以除以 $n_1$：\n$$\n\\frac{k_1}{n_1} = \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}\n$$\n令 $p = \\frac{k_1}{n_1}$，我们求解 $p = \\frac{\\exp(\\hat{\\theta})}{1+\\exp(\\hat{\\theta})}$ 以得到 $\\hat{\\theta}$：\n$$\np(1+\\exp(\\hat{\\theta})) = \\exp(\\hat{\\theta}) \\implies p = \\exp(\\hat{\\theta})(1-p) \\implies \\frac{p}{1-p} = \\exp(\\hat{\\theta})\n$$\n对两边取自然对数，得到 MLE $\\hat{\\theta}$：\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{p}{1-p}\\right)\n$$\n将 $p = k_1/n_1$ 代回表达式中：\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{k_1/n_1}{1 - k_1/n_1}\\right) = \\ln\\left(\\frac{k_1/n_1}{(n_1-k_1)/n_1}\\right) = \\ln\\left(\\frac{k_1}{n_1-k_1}\\right)\n$$\n假设 $0  k_1/n_1  1$ 确保了 $k_1 > 0$ 和 $n_1-k_1 > 0$，因此对数的参数是正且有限的，保证了 $\\hat{\\theta}$ 是一个定义良好、有限的实数。对数似然函数的二阶导数是 $\\frac{d^2\\ell}{d\\theta^2} = -n_1 \\sigma(\\theta)(1-\\sigma(\\theta))$，对于任何有限的 $\\theta$，它都是严格为负的，这证实了该临界点是一个唯一的全局最大值点。",
            "answer": "$$\n\\boxed{\\ln\\left(\\frac{k_1}{n_1-k_1}\\right)}\n$$"
        }
    ]
}