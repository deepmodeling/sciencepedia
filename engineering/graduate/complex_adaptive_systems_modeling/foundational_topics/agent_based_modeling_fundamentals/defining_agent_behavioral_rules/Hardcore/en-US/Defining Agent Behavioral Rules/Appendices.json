{
    "hands_on_practices": [
        {
            "introduction": "To model agent behavior, we must first master the language of probability. This practice grounds us in the fundamentals by asking us to calculate the overall probability of an agent taking a specific action, given its internal stochastic policy and the distribution of states it might encounter. By applying the law of total probability, we bridge the gap between the conditional rule defined by the policy and the marginal behavior we observe in the system .",
            "id": "4119665",
            "problem": "Consider an agent embedded in a Complex Adaptive System (CAS), modeled as a finite-state, stochastic decision-maker within a Markov Decision Process (MDP). Let the state space be $\\mathcal{S}=\\{1,2\\}$ and the action space be $\\mathcal{A}=\\{0,1\\}$. At time $t$, the environment produces a random state $S_t \\in \\mathcal{S}$ according to a probability distribution $\\mu_t$ on $\\mathcal{S}$, with $\\mu_t(1)+\\mu_t(2)=1$ and $\\mu_t(s)\\in[0,1]$ for each $s\\in\\mathcal{S}$. The agent’s behavioral rule is given by a stationary policy $\\pi$, which is a conditional probability kernel satisfying, for each $s\\in\\mathcal{S}$, $\\pi(a|s)\\in[0,1]$ and $\\sum_{a\\in\\mathcal{A}}\\pi(a|s)=1$. Specifically, suppose the policy satisfies $\\pi(a=1\\,|\\,s=1)=p$ and $\\pi(a=1\\,|\\,s=2)=q$, with $p,q\\in[0,1]$, and hence $\\pi(a=0\\,|\\,s=1)=1-p$ and $\\pi(a=0\\,|\\,s=2)=1-q$.\n\nDefine the action random variable $A_t\\in\\mathcal{A}$ as drawn according to the behavioral rule $\\pi(\\cdot\\,|\\,S_t)$, that is, the conditional distribution of $A_t$ given $S_t=s$ is $\\pi(\\cdot\\,|\\,s)$. Using only the core definitions of conditional probability, the law of total probability, and the specification of the behavioral rule as a conditional probability kernel, derive the closed-form expression for the marginal probability $\\mathbb{P}(A_t=1)$ as a function of $p$, $q$, $\\mu_t(1)$, and $\\mu_t(2)$. Provide your final result as a single analytic expression. No numerical rounding is required and no physical units apply.",
            "solution": "The setting provides a finite probability space for the state and action variables. Let $S_t$ be the state at time $t$ and $A_t$ be the action at time $t$. The state distribution is given by $\\mu_t$, so $\\mathbb{P}(S_t=s)=\\mu_t(s)$ for $s\\in\\{1,2\\}$. The agent’s behavioral rule is represented by a conditional probability kernel $\\pi$, which specifies $\\mathbb{P}(A_t=a\\,|\\,S_t=s)=\\pi(a\\,|\\,s)$.\n\nWe seek the marginal probability $\\mathbb{P}(A_t=1)$. By the law of total probability over the partition induced by the states, and using the definition of conditional probability, we have\n$$\n\\mathbb{P}(A_t=1)\n=\\sum_{s\\in\\mathcal{S}}\\mathbb{P}(A_t=1\\,|\\,S_t=s)\\,\\mathbb{P}(S_t=s)\n=\\sum_{s\\in\\{1,2\\}}\\pi(1\\,|\\,s)\\,\\mu_t(s).\n$$\nThe problem provides $\\pi(1\\,|\\,1)=p$ and $\\pi(1\\,|\\,2)=q$. Substituting these into the sum yields\n$$\n\\mathbb{P}(A_t=1)=p\\,\\mu_t(1)+q\\,\\mu_t(2).\n$$\nThis expression is a closed-form analytic function of the policy parameters $p$ and $q$ and the state distribution components $\\mu_t(1)$ and $\\mu_t(2)$, derived directly from the core definitions and the law of total probability without any additional assumptions.",
            "answer": "$$\\boxed{p\\,\\mu_t(1)+q\\,\\mu_t(2)}$$"
        },
        {
            "introduction": "Behavioral rules can range from perfectly predictable (deterministic) to completely random. This exercise introduces Shannon entropy as a powerful tool to quantify the level of uncertainty or \"randomness\" inherent in an agent's policy. By analyzing how entropy changes with the policy's parameters, you will develop a deeper intuition for the nature of stochastic behavior and its role in concepts like exploration and information processing .",
            "id": "4119714",
            "problem": "In a single-state decision environment modeled as a Markov decision process, an agent defines a behavioral rule at state $s$ by a stationary stochastic policy $\\,\\pi(\\cdot \\mid s)\\,$ over a binary action set $\\{a_{0}, a_{1}\\}$. The agent selects $a_{1}$ with probability $p \\in [0,1]$ and $a_{0}$ with probability $1-p$, which parameterizes a family of behavioral rules ranging from deterministic ($p \\in \\{0,1\\}$) to stochastic ($p \\in (0,1)$). Use the core definition of Shannon entropy for a discrete distribution with natural logarithm (entropy in nats) as the foundational starting point. From this base, derive the expression for the entropy $H(\\pi(\\cdot \\mid s))$ as a function of $p$, and, by reasoning from first principles, characterize how the entropy compares between deterministic and stochastic rules. Identify the value of $p$ that maximizes the entropy and justify it from the derived expression without assuming any result about its maximum. Finally, compute the exact closed-form value of $H(\\pi(\\cdot \\mid s))$ at $p=\\frac{1}{2}$ using natural logarithms. Express the final entropy value in nats and provide the exact expression; do not numerically approximate or round.",
            "solution": "The problem statement is first validated for correctness and completeness.\n\n### Step 1: Extract Givens\n- **Environment:** A single-state decision environment modeled as a Markov decision process.\n- **State:** $s$.\n- **Policy:** A stationary stochastic policy $\\pi(\\cdot \\mid s)$ over a binary action set.\n- **Action Set:** $\\{a_{0}, a_{1}\\}$.\n- **Policy Parameterization:** The agent selects action $a_1$ with probability $p$ and action $a_0$ with probability $1-p$, where $p \\in [0,1]$.\n- **Rule Types:**\n    - Deterministic rules correspond to $p \\in \\{0,1\\}$.\n    - Stochastic rules correspond to $p \\in (0,1)$.\n- **Entropy Definition:** Shannon entropy for a discrete distribution, using the natural logarithm (base $e$), with entropy measured in nats.\n- **Objectives:**\n    1.  Derive the expression for the entropy $H(\\pi(\\cdot \\mid s))$ as a function of $p$.\n    2.  Compare the entropy of deterministic versus stochastic rules.\n    3.  Find the value of $p$ that maximizes the entropy, justifying from first principles.\n    4.  Compute the exact closed-form value of $H(\\pi(\\cdot \\mid s))$ at $p=\\frac{1}{2}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in information theory and reinforcement learning. The concepts of Shannon entropy and stochastic policies are standard and well-defined. The problem is well-posed, providing all necessary information to derive the requested expressions and values. The language is objective and formal. The problem contains no scientific fallacies, contradictions, or ambiguities. It is a standard, formalizable exercise in applying the definition of entropy.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived.\n\n### Derivation and Analysis\n\nThe foundational definition of Shannon entropy for a discrete probability distribution $P = \\{p_1, p_2, \\dots, p_n\\}$ is given by:\n$$H(P) = -\\sum_{i=1}^{n} p_i \\log_b(p_i)$$\nThe problem specifies the use of the natural logarithm, so the base $b$ is Euler's number, $e$, and the entropy is measured in nats. The policy $\\pi(\\cdot \\mid s)$ defines a discrete probability distribution over the action set $\\{a_0, a_1\\}$. The probabilities of the two actions are:\n$$ \\pi(a_1 \\mid s) = p $$\n$$ \\pi(a_0 \\mid s) = 1-p $$\nApplying the definition of Shannon entropy to this policy, we obtain the entropy $H(\\pi(\\cdot \\mid s))$ as a function of $p$, which we denote as $H(p)$:\n$$ H(p) = - \\left( \\pi(a_0 \\mid s) \\ln(\\pi(a_0 \\mid s)) + \\pi(a_1 \\mid s) \\ln(\\pi(a_1 \\mid s)) \\right) $$\nSubstituting the given probabilities yields the expression for the entropy:\n$$ H(p) = - \\left( (1-p)\\ln(1-p) + p\\ln(p) \\right) $$\nThis expression is valid for $p \\in (0,1)$. For the endpoints $p=0$ and $p=1$, we must evaluate the limit of $x\\ln(x)$ as $x \\to 0^+$. Using L'Hôpital's rule:\n$$ \\lim_{x \\to 0^+} x\\ln(x) = \\lim_{x \\to 0^+} \\frac{\\ln(x)}{1/x} = \\lim_{x \\to 0^+} \\frac{1/x}{-1/x^2} = \\lim_{x \\to 0^+} (-x) = 0 $$\nBy this standard convention, we define $0\\ln(0) = 0$.\n\nNext, we compare the entropy for deterministic and stochastic rules.\n- **Deterministic rules:** $p \\in \\{0, 1\\}$.\n  - If $p=0$, the policy is to always choose $a_0$. The entropy is $H(0) = -((1-0)\\ln(1-0) + 0\\ln(0)) = -(1\\ln(1) + 0) = 0$.\n  - If $p=1$, the policy is to always choose $a_1$. The entropy is $H(1) = -((1-1)\\ln(1-1) + 1\\ln(1)) = -(0\\ln(0) + 0) = 0$.\n  Thus, for deterministic rules, the entropy is $0$ nats. This reflects the complete certainty of the action to be taken.\n- **Stochastic rules:** $p \\in (0, 1)$.\n  - For any $p$ in this open interval, both $p$ and $1-p$ are numbers strictly between $0$ and $1$.\n  - The natural logarithm of a number between $0$ and $1$ is negative. Therefore, $\\ln(p) < 0$ and $\\ln(1-p) < 0$.\n  - The terms $p\\ln(p)$ and $(1-p)\\ln(1-p)$ are both negative.\n  - Their sum is strictly negative.\n  - Consequently, $H(p) = -(\\text{negative value}) > 0$.\nThe entropy of any stochastic rule is strictly greater than the entropy of any deterministic rule. Entropy quantifies uncertainty, and stochastic rules inherently possess more uncertainty than deterministic ones.\n\nTo find the value of $p$ that maximizes the entropy, we analyze the function $H(p) = -p\\ln(p) - (1-p)\\ln(1-p)$ on the interval $[0,1]$. We find the critical points by taking the first derivative with respect to $p$ and setting it to zero.\n$$ \\frac{dH}{dp} = \\frac{d}{dp} \\left( -p\\ln(p) - (1-p)\\ln(1-p) \\right) $$\nUsing the product rule for differentiation:\n$$ \\frac{dH}{dp} = -\\left[\\left(1\\cdot\\ln(p) + p\\cdot\\frac{1}{p}\\right) + \\left(-1\\cdot\\ln(1-p) + (1-p)\\cdot\\frac{-1}{1-p}\\right)\\right] $$\n$$ \\frac{dH}{dp} = -\\left[ (\\ln(p) + 1) + (-\\ln(1-p) - 1) \\right] $$\n$$ \\frac{dH}{dp} = -[\\ln(p) - \\ln(1-p)] = \\ln(1-p) - \\ln(p) = \\ln\\left(\\frac{1-p}{p}\\right) $$\nSetting the derivative to zero to find the extremum:\n$$ \\ln\\left(\\frac{1-p}{p}\\right) = 0 $$\n$$ \\frac{1-p}{p} = \\exp(0) = 1 $$\n$$ 1-p = p \\implies 2p = 1 \\implies p = \\frac{1}{2} $$\nTo confirm this is a maximum, we examine the second derivative:\n$$ \\frac{d^2H}{dp^2} = \\frac{d}{dp}\\left( \\ln(1-p) - \\ln(p) \\right) = \\frac{-1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right) $$\nFor any $p \\in (0, 1)$, both $p$ and $1-p$ are positive. Therefore, the term $\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right)$ is strictly positive. This makes the second derivative $\\frac{d^2H}{dp^2}$ strictly negative for all $p \\in (0,1)$. A negative second derivative indicates that the function $H(p)$ is strictly concave, and thus the critical point $p=\\frac{1}{2}$ is a unique global maximum. The entropy is maximized when there is maximum uncertainty about the outcome, which occurs when both actions are equally probable.\n\nFinally, we compute the exact value of the maximum entropy at $p = \\frac{1}{2}$.\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\left(1-\\frac{1}{2}\\right)\\ln\\left(1-\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = - \\left( 2 \\cdot \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\right) = -\\ln\\left(\\frac{1}{2}\\right) $$\nUsing the logarithm property $-\\ln(x) = \\ln(x^{-1})$:\n$$ H\\left(\\frac{1}{2}\\right) = \\ln\\left(\\left(\\frac{1}{2}\\right)^{-1}\\right) = \\ln(2) $$\nThe exact closed-form value of the maximum entropy is $\\ln(2)$ nats.",
            "answer": "$$\\boxed{\\ln(2)}$$"
        },
        {
            "introduction": "Agent behavioral rules are often not arbitrary but are derived from underlying principles of optimization, where an agent seeks to maximize its utility subject to environmental or internal constraints. This practice introduces the Karush-Kuhn-Tucker (KKT) conditions, a fundamental framework for solving such constrained optimization problems. By deriving and applying these conditions, you will learn how to formally model and solve for the optimal behavior of a \"rational\" agent facing limitations .",
            "id": "4119693",
            "problem": "Consider a single representative agent in a complex adaptive system who, at state $s \\in \\mathcal{S}$, selects a scalar action $a \\in \\mathbb{R}$ according to a behavioral rule that maximizes a differentiable utility $u(s,a)$ subject to $m$ differentiable inequality constraints $g_i(s,a) \\le 0$ for $i \\in \\{1,\\dots,m\\}$. Assume the feasible set $\\{a \\in \\mathbb{R} \\mid g_i(s,a) \\le 0, \\, i=1,\\dots,m\\}$ is nonempty and convex, and that a constraint qualification such as Slater's condition holds (there exists an $a$ with $g_i(s,a) < 0$ for all $i$). Starting from the foundational definition of constrained optimality and Lagrangian duality, derive the Karush–Kuhn–Tucker (KKT) conditions that characterize an optimal action $a^{\\star}(s)$ for the agent under these inequality constraints.\n\nThen instantiate the setup with a single inequality constraint and a concave quadratic utility. Let\n$$\nu(s,a) \\equiv -\\frac{1}{2}\\,\\alpha \\,\\big(a - \\mu\\big)^{2} + \\beta,\n$$\nwhere $\\alpha > 0$, and let the constraint be\n$$\ng(s,a) \\equiv \\rho\\, a - \\gamma \\le 0,\n$$\nwith $\\rho > 0$. Here $\\mu, \\beta, \\gamma \\in \\mathbb{R}$ may depend on the state $s$, but treat them as given parameters when solving for $a^{\\star}(s)$. Using the KKT conditions you derived, compute the agent’s optimal action $a^{\\star}(s)$ in exact closed form as a function of $(\\mu, \\alpha, \\rho, \\gamma, \\beta)$, simplifying the expression as far as possible. Express the final answer as a single analytic expression; no rounding is required.",
            "solution": "The problem is divided into two parts. First, we must derive the general Karush–Kuhn–Tucker (KKT) conditions for a constrained optimization problem. Second, we apply these conditions to a specific instance of utility maximization to find the agent's optimal action.\n\n### Part 1: Derivation of the KKT Conditions\n\nThe agent's problem is to choose an action $a \\in \\mathbb{R}$ to solve the following optimization problem for a given state $s \\in \\mathcal{S}$:\n$$\n\\max_{a \\in \\mathbb{R}} u(s,a)\n$$\nsubject to $m$ inequality constraints:\n$$\ng_i(s,a) \\le 0, \\quad \\text{for } i \\in \\{1, \\dots, m\\}\n$$\nWe are given that the functions $u(s,a)$ and $g_i(s,a)$ are differentiable with respect to $a$. The feasible set is nonempty and convex, and a constraint qualification (such as Slater's condition) holds. For notational simplicity, we will suppress the explicit dependence on the state $s$, treating it as fixed.\n\nTo find the conditions for an optimal action $a^{\\star}$, we use the method of Lagrange multipliers. We formulate the Lagrangian function, $\\mathcal{L}$, which incorporates the objective function and the constraints. For a maximization problem with \"less than or equal to\" constraints, the Lagrangian is defined as:\n$$\n\\mathcal{L}(a, \\boldsymbol{\\lambda}) = u(a) - \\sum_{i=1}^{m} \\lambda_i g_i(a)\n$$\nwhere $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, \\dots, \\lambda_m)$ is the vector of Lagrange multipliers, also known as dual variables.\n\nThe KKT conditions are the first-order necessary conditions for a point $a^{\\star}$ to be a local maximum. Given that the objective function is concave and the feasible set is convex (as specified for the second part of the problem, and a general condition for KKT to be sufficient), these necessary conditions are also sufficient for a global maximum. The conditions are derived from the principles of stationarity with respect to the choice variable $a$ and the complementary nature of the constraints.\n\nLet $a^{\\star}$ be an optimal solution and $\\boldsymbol{\\lambda}^{\\star}$ be the corresponding vector of optimal Lagrange multipliers. The KKT conditions are:\n\n1.  **Stationarity:** At an optimal point $a^{\\star}$, the gradient of the Lagrangian with respect to $a$ must be zero. Since $a$ is a scalar, this simplifies to the derivative being zero.\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial a} \\bigg|_{a=a^{\\star}, \\boldsymbol{\\lambda}=\\boldsymbol{\\lambda}^{\\star}} = \\frac{\\partial u}{\\partial a}(a^{\\star}) - \\sum_{i=1}^{m} \\lambda_i^{\\star} \\frac{\\partial g_i}{\\partial a}(a^{\\star}) = 0\n    $$\n\n2.  **Primal Feasibility:** The optimal action $a^{\\star}$ must satisfy all the original constraints.\n    $$\n    g_i(a^{\\star}) \\le 0, \\quad \\text{for all } i \\in \\{1, \\dots, m\\}\n    $$\n\n3.  **Dual Feasibility:** The Lagrange multipliers associated with the inequality constraints must be non-negative.\n    $$\n    \\lambda_i^{\\star} \\ge 0, \\quad \\text{for all } i \\in \\{1, \\dots, m\\}\n    $$\n\n4.  **Complementary Slackness:** For each constraint, the product of the multiplier and the constraint function value must be zero. This means that if a constraint is not active (i.e., $g_i(a^{\\star}) < 0$), its corresponding multiplier must be zero ($\\lambda_i^{\\star} = 0$). Conversely, if a multiplier is positive ($\\lambda_i^{\\star} > 0$), its corresponding constraint must be active (i.e., $g_i(a^{\\star}) = 0$).\n    $$\n    \\lambda_i^{\\star} g_i(a^{\\star}) = 0, \\quad \\text{for all } i \\in \\{1, \\dots, m\\}\n    $$\n\nThese four sets of conditions together constitute the KKT conditions for the given constrained optimization problem.\n\n### Part 2: Application to a Specific Case\n\nWe are given the specific utility and constraint functions:\nUtility: $u(s,a) = -\\frac{1}{2}\\alpha(a - \\mu)^{2} + \\beta$, with $\\alpha > 0$.\nConstraint: $g(s,a) = \\rho a - \\gamma \\le 0$, with $\\rho > 0$.\nThe parameters $\\mu, \\beta, \\gamma, \\alpha, \\rho$ are treated as given constants for the optimization at a fixed state $s$. Note that the utility function $u(a)$ is strictly concave in $a$ since its second derivative, $\\frac{d^2u}{da^2} = -\\alpha$, is negative. The constraint function $g(a)$ is linear, hence it is a convex function. This fulfills the conditions for the KKT conditions to be sufficient for a unique global maximum.\n\nWe now apply the KKT conditions. There is a single constraint, so we have a single Lagrange multiplier $\\lambda \\ge 0$.\n\nThe Lagrangian is:\n$$\n\\mathcal{L}(a, \\lambda) = \\left(-\\frac{1}{2}\\alpha(a - \\mu)^{2} + \\beta\\right) - \\lambda(\\rho a - \\gamma)\n$$\n\nThe KKT conditions for the optimal action $a^{\\star}$ and multiplier $\\lambda^{\\star}$ are:\n\n1.  **Stationarity:**\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial a} = -\\alpha(a^{\\star} - \\mu) - \\lambda^{\\star}\\rho = 0 \\implies \\alpha(a^{\\star} - \\mu) + \\lambda^{\\star}\\rho = 0\n    $$\n\n2.  **Primal Feasibility:**\n    $$\n    \\rho a^{\\star} - \\gamma \\le 0\n    $$\n\n3.  **Dual Feasibility:**\n    $$\n    \\lambda^{\\star} \\ge 0\n    $$\n\n4.  **Complementary Slackness:**\n    $$\n    \\lambda^{\\star}(\\rho a^{\\star} - \\gamma) = 0\n    $$\n\nWe solve this system of conditions by considering two cases based on the complementary slackness condition.\n\n**Case 1: The constraint is not binding (slack).**\nIn this case, $\\rho a^{\\star} - \\gamma < 0$. Complementary slackness implies that $\\lambda^{\\star} = 0$.\nSubstituting $\\lambda^{\\star} = 0$ into the stationarity condition gives:\n$$\n\\alpha(a^{\\star} - \\mu) = 0\n$$\nSince $\\alpha > 0$, we must have $a^{\\star} - \\mu = 0$, which implies $a^{\\star} = \\mu$.\nThis solution is valid only if it satisfies the primal feasibility condition under which this case is defined: $\\rho a^{\\star} - \\gamma \\le 0$. Substituting $a^{\\star} = \\mu$, we get:\n$$\n\\rho \\mu - \\gamma \\le 0 \\quad \\text{or} \\quad \\mu \\le \\frac{\\gamma}{\\rho}\n$$\nSo, if $\\mu \\le \\frac{\\gamma}{\\rho}$, the optimal action is $a^{\\star} = \\mu$. This corresponds to the unconstrained maximum of the utility function being feasible. Note that the parameter $\\beta$ does not influence the location of the optimum.\n\n**Case 2: The constraint is binding (active).**\nIn this case, $\\rho a^{\\star} - \\gamma = 0$. This directly gives the value for the optimal action:\n$$\na^{\\star} = \\frac{\\gamma}{\\rho}\n$$\nFor this to be the solution, the corresponding Lagrange multiplier $\\lambda^{\\star}$ must be non-negative ($\\lambda^{\\star} \\ge 0$). From the stationarity condition:\n$$\n\\lambda^{\\star}\\rho = -\\alpha(a^{\\star} - \\mu)\n$$\nSubstituting $a^{\\star} = \\gamma/\\rho$:\n$$\n\\lambda^{\\star}\\rho = -\\alpha\\left(\\frac{\\gamma}{\\rho} - \\mu\\right) = \\alpha\\left(\\mu - \\frac{\\gamma}{\\rho}\\right)\n$$\nSince $\\rho > 0$, we can solve for $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{\\alpha}{\\rho}\\left(\\mu - \\frac{\\gamma}{\\rho}\\right)\n$$\nThe dual feasibility condition $\\lambda^{\\star} \\ge 0$ implies that $\\frac{\\alpha}{\\rho}\\left(\\mu - \\frac{\\gamma}{\\rho}\\right) \\ge 0$. Since $\\alpha > 0$ and $\\rho > 0$, this is equivalent to:\n$$\n\\mu - \\frac{\\gamma}{\\rho} \\ge 0 \\quad \\text{or} \\quad \\mu \\ge \\frac{\\gamma}{\\rho}\n$$\nSo, if $\\mu \\ge \\frac{\\gamma}{\\rho}$, the optimal action is $a^{\\star} = \\frac{\\gamma}{\\rho}$. This corresponds to the unconstrained maximum being outside the feasible set, so the optimum lies on the boundary.\n\n**Combining the results:**\nWe have found that:\n- If $\\mu \\le \\frac{\\gamma}{\\rho}$, then $a^{\\star} = \\mu$.\n- If $\\mu > \\frac{\\gamma}{\\rho}$, then $a^{\\star} = \\frac{\\gamma}{\\rho}$.\n\nThis logic can be expressed concisely using the minimum function. The agent's problem is to maximize a quadratic centered at $\\mu$, subject to the constraint $a \\le \\gamma/\\rho$. This is equivalent to finding the point in the feasible interval $(-\\infty, \\gamma/\\rho]$ that is closest to $\\mu$. This point is simply the minimum of $\\mu$ and the upper bound $\\gamma/\\rho$.\n\nTherefore, the optimal action $a^{\\star}$ can be written as a single analytic expression:\n$$\na^{\\star}(s) = \\min\\left(\\mu, \\frac{\\gamma}{\\rho}\\right)\n$$\nThe parameters $\\alpha$ and $\\beta$ do not appear in the final expression for the optimal action $a^{\\star}$. The parameter $\\alpha$ scales the utility function but does not change the location of its peak, and $\\beta$ is an additive constant that shifts the utility value but also does not affect the optimal action.",
            "answer": "$$\\boxed{\\min\\left(\\mu, \\frac{\\gamma}{\\rho}\\right)}$$"
        }
    ]
}