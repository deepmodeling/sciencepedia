{
    "hands_on_practices": [
        {
            "introduction": "Understanding complex networks begins with mastering the simplest generative models. The Erdős–Rényi random graph, $G(N,p)$, serves as a fundamental benchmark in network science. This exercise  challenges you to derive the first and second moments of its degree distribution directly from first principles, using indicator variables and the linearity of expectation. Mastering this foundational calculation is essential for building the intuition and mathematical toolkit needed to analyze more intricate network structures.",
            "id": "4132834",
            "problem": "Consider the undirected simple random graph generated by the Erdős–Rényi model $G(N,p)$: there are $N$ labeled nodes, and each of the $\\binom{N}{2}$ possible edges is included independently with probability $p$, where $0p1$. For a fixed node $i$, let its degree be the random variable $k_i$. Using only the foundational definitions of $G(N,p)$, indicator random variables for edges, linearity of expectation, and independence of distinct edges, derive closed-form expressions for the first and second moments of the degree distribution, namely $\\langle k_i \\rangle$ and $\\langle k_i^2 \\rangle$, in terms of $N$ and $p$. Then obtain the variance $\\operatorname{Var}(k_i)$ and, from first principles, demonstrate its scaling with $N$ and $p$ in the regimes where $p$ is a constant independent of $N$ (dense regime) and where $p=c/N$ for a fixed constant $c0$ (sparse regime). Express the final results for $\\langle k_i \\rangle$, $\\langle k_i^2 \\rangle$, and $\\operatorname{Var}(k_i)$ as exact symbolic expressions in terms of $N$ and $p$. Do not round or approximate your final expressions, and do not include any units. Your final answer must be a single line containing the three expressions in a single row matrix.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the theory of random graphs and all necessary parameters are defined. We may proceed with the solution.\n\nThe problem asks for the first moment $\\langle k_i \\rangle$, the second moment $\\langle k_i^2 \\rangle$, and the variance $\\operatorname{Var}(k_i)$ of the degree $k_i$ of a fixed node $i$ in an Erdős–Rényi graph $G(N, p)$. The graph has $N$ nodes, and any pair of distinct nodes is connected by an edge with an independent probability $p$.\n\nLet the set of nodes be $V = \\{1, 2, \\dots, N\\}$. For a fixed node $i \\in V$, its degree $k_i$ is the number of edges incident to it. There are $N-1$ other nodes to which node $i$ can be connected. Let these other nodes be indexed by $j \\in V \\setminus \\{i\\}$.\n\nTo formalize the problem, we use indicator random variables. For each $j \\in V$ with $j \\neq i$, let $X_{ij}$ be the indicator random variable for the event that an edge exists between node $i$ and node $j$. By the definition of the $G(N,p)$ model:\n$$\nX_{ij} =\n\\begin{cases}\n1  \\text{if an edge exists between } i \\text{ and } j \\text{ (with probability } p) \\\\\n0  \\text{if no edge exists between } i \\text{ and } j \\text{ (with probability } 1-p)\n\\end{cases}\n$$\nThe degree $k_i$ of node $i$ is the sum of these indicator variables over all possible neighbors:\n$$\nk_i = \\sum_{j \\in V, j \\neq i} X_{ij}\n$$\nThe sum consists of $N-1$ terms.\n\nFirst, we calculate the expectation of a single indicator variable $X_{ij}$:\n$$\nE[X_{ij}] = 1 \\cdot P(X_{ij}=1) + 0 \\cdot P(X_{ij}=0) = 1 \\cdot p + 0 \\cdot (1-p) = p\n$$\n\nNow, we can derive the first moment (the mean) of the degree distribution, $\\langle k_i \\rangle = E[k_i]$. Using the linearity of expectation:\n$$\n\\langle k_i \\rangle = E[k_i] = E\\left[\\sum_{j \\neq i} X_{ij}\\right] = \\sum_{j \\neq i} E[X_{ij}]\n$$\nSince there are $N-1$ terms in the sum and $E[X_{ij}] = p$ for all $j \\neq i$, we have:\n$$\n\\langle k_i \\rangle = (N-1)p\n$$\n\nNext, we derive the second moment, $\\langle k_i^2 \\rangle = E[k_i^2]$. We start by squaring the expression for $k_i$:\n$$\nk_i^2 = \\left(\\sum_{j \\neq i} X_{ij}\\right)^2 = \\left(\\sum_{j \\neq i} X_{ij}\\right)\\left(\\sum_{l \\neq i} X_{il}\\right) = \\sum_{j \\neq i} \\sum_{l \\neq i} X_{ij}X_{il}\n$$\nWe can split the double summation into two cases: when the indices are the same ($j=l$) and when they are different ($j \\neq l$).\n$$\nk_i^2 = \\sum_{j \\neq i} X_{ij}^2 + \\sum_{j \\neq i} \\sum_{l \\neq i, l \\neq j} X_{ij}X_{il}\n$$\nNow, we take the expectation of this expression. Using the linearity of expectation:\n$$\n\\langle k_i^2 \\rangle = E[k_i^2] = E\\left[\\sum_{j \\neq i} X_{ij}^2\\right] + E\\left[\\sum_{j \\neq i} \\sum_{l \\neq i, l \\neq j} X_{ij}X_{il}\\right] = \\sum_{j \\neq i} E[X_{ij}^2] + \\sum_{j \\neq i} \\sum_{l \\neq i, l \\neq j} E[X_{ij}X_{il}]\n$$\nWe need to calculate the expectations $E[X_{ij}^2]$ and $E[X_{ij}X_{il}]$ for $j \\neq l$.\nFor the first term, since $X_{ij}$ is an indicator variable, it can only take values $0$ or $1$. Therefore, $X_{ij}^2 = X_{ij}$.\n$$\nE[X_{ij}^2] = E[X_{ij}] = p\n$$\nFor the second term, involving $X_{ij}$ and $X_{il}$ where $j \\neq l$, the edges $(i,j)$ and $(i,l)$ are distinct. According to the problem definition, the existence of each edge is an independent event. Thus, the random variables $X_{ij}$ and $X_{il}$ are independent. The expectation of their product is the product of their expectations:\n$$\nE[X_{ij}X_{il}] = E[X_{ij}] E[X_{il}] = p \\cdot p = p^2\n$$\nNow we can evaluate the sums. The first sum has $N-1$ terms, each equal to $p$:\n$$\n\\sum_{j \\neq i} E[X_{ij}^2] = (N-1)p\n$$\nThe second double sum corresponds to ordered pairs of distinct nodes $(j,l)$ from the set $V \\setminus \\{i\\}$. The number of such pairs is $(N-1)(N-2)$. Each term in this sum is $p^2$:\n$$\n\\sum_{j \\neq i} \\sum_{l \\neq i, l \\neq j} E[X_{ij}X_{il}] = (N-1)(N-2)p^2\n$$\nCombining these results, we get the expression for the second moment:\n$$\n\\langle k_i^2 \\rangle = (N-1)p + (N-1)(N-2)p^2\n$$\n\nFinally, we compute the variance of the degree, $\\operatorname{Var}(k_i)$, using the standard formula $\\operatorname{Var}(k_i) = \\langle k_i^2 \\rangle - \\langle k_i \\rangle^2$:\n$$\n\\operatorname{Var}(k_i) = \\left[(N-1)p + (N-1)(N-2)p^2\\right] - \\left[(N-1)p\\right]^2\n$$\n$$\n\\operatorname{Var}(k_i) = (N-1)p + (N-1)(N-2)p^2 - (N-1)^2p^2\n$$\nWe can factor out $(N-1)p$:\n$$\n\\operatorname{Var}(k_i) = (N-1)p \\left[1 + (N-2)p - (N-1)p\\right]\n$$\n$$\n\\operatorname{Var}(k_i) = (N-1)p \\left[1 + Np - 2p - Np + p\\right]\n$$\n$$\n\\operatorname{Var}(k_i) = (N-1)p \\left[1 - p\\right]\n$$\nThis result is consistent with the fact that $k_i$ is the sum of $N-1$ independent and identically distributed Bernoulli trials, which means $k_i$ follows a binomial distribution $B(n, p)$ with $n=N-1$. The variance of such a distribution is indeed $np(1-p)$.\n\nNow we analyze the scaling of the variance $\\operatorname{Var}(k_i) = (N-1)p(1-p)$ in the specified regimes for large $N$.\n\n1.  **Dense regime**: $p$ is a constant independent of $N$.\n    In this case, the term $p(1-p)$ is a non-zero constant. The variance expression is $\\operatorname{Var}(k_i) = (N-1) \\times (\\text{constant})$. Therefore, the variance scales linearly with $N$:\n    $$\n    \\operatorname{Var}(k_i) \\sim O(N)\n    $$\n\n2.  **Sparse regime**: $p = c/N$ for a fixed constant $c > 0$.\n    We substitute $p=c/N$ into the variance formula:\n    $$\n    \\operatorname{Var}(k_i) = (N-1) \\left(\\frac{c}{N}\\right) \\left(1 - \\frac{c}{N}\\right)\n    $$\n    $$\n    \\operatorname{Var}(k_i) = \\left(\\frac{N-1}{N}\\right) c \\left(1 - \\frac{c}{N}\\right) = \\left(1 - \\frac{1}{N}\\right) c \\left(1 - \\frac{c}{N}\\right)\n    $$\n    As $N \\to \\infty$, the term $(1 - 1/N) \\to 1$ and $(1 - c/N) \\to 1$. Thus, the variance approaches a constant value:\n    $$\n    \\lim_{N \\to \\infty} \\operatorname{Var}(k_i) = c\n    $$\n    In this regime, for large $N$, the variance is approximately constant and does not scale with $N$. The mean degree is $\\langle k_i \\rangle = (N-1)p = (N-1)c/N \\to c$. The distribution of $k_i$ approaches a Poisson distribution with parameter $\\lambda = c$, for which both the mean and variance are equal to $\\lambda=c$. Our result is consistent with this well-known limit.\n\nThe problem requires the final exact symbolic expressions for $\\langle k_i \\rangle$, $\\langle k_i^2 \\rangle$, and $\\operatorname{Var}(k_i)$.\nThe derived expressions are:\n$\\langle k_i \\rangle = (N-1)p$\n$\\langle k_i^2 \\rangle = (N-1)p + (N-1)(N-2)p^2$\n$\\operatorname{Var}(k_i) = (N-1)p(1-p)$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} (N-1)p  (N-1)p + (N-1)(N-2)p^2  (N-1)p(1-p) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A degree distribution $P(k)$ describes the prevalence of nodes with a certain degree, but it does not reveal *how* these nodes are connected to one another. This practice  delves into this crucial structural feature by having you compute the assortativity coefficient, which quantifies degree-degree correlations. By working through this problem, you will learn to distinguish between node-centric and edge-centric distributions and apply statistical methods to determine if a network exhibits assortative (like-likes-like) or disassortative mixing.",
            "id": "4132898",
            "problem": "Consider a large, undirected, simple network drawn from a stationary ensemble. Let the degree distribution be $P(k)$ over $k \\in \\{1,2,3,4\\}$ given by $P(1)=0.15$, $P(2)=0.35$, $P(3)=0.30$, and $P(4)=0.20$. Let $P(k,k')$ denote the probability that the two ends of a uniformly random edge have degrees $k$ and $k'$ (the joint endpoint-degree distribution), which is symmetric in $k$ and $k'$ and satisfies $\\sum_{k,k'} P(k,k')=1$. Suppose $P(k,k')$ is specified by the values\n$P(1,1)=\\frac{1}{102}$, $P(1,2)=\\frac{4}{102}$, $P(1,3)=\\frac{1}{102}$, $P(1,4)=0$, $P(2,2)=\\frac{6}{102}$, $P(2,3)=\\frac{10}{102}$, $P(2,4)=\\frac{8}{102}$, $P(3,3)=\\frac{12}{102}$, $P(3,4)=\\frac{13}{102}$, and $P(4,4)=\\frac{11}{102}$, with $P(k',k)=P(k,k')$ for all off-diagonal entries.\n\nUsing only the following foundational definitions:\n- The distribution $q_k$ of the degree seen at the end of a uniformly random edge is $q_k = \\frac{k P(k)}{\\langle k \\rangle}$, where $\\langle k \\rangle = \\sum_{k} k P(k)$.\n- The Pearson correlation coefficient of the degrees at the two ends of a uniformly random edge, denoted $r$, is defined as the correlation of the random variables representing the endpoint degrees under the joint distribution $P(k,k')$.\n\nCompute the assortativity coefficient $r$ of degrees for this network from first principles. Express your answer as an exact fraction (no rounding). Then, interpret the sign of $r$ in terms of degree correlations (assortative or disassortative mixing). The interpretation does not affect the form of your final numeric answer.",
            "solution": "The problem asks for the computation of the assortativity coefficient, denoted $r$, for a network with a given degree distribution $P(k)$ and joint endpoint-degree distribution $P(k,k')$. The assortativity coefficient is defined as the Pearson correlation coefficient of the degrees at the two ends of a uniformly random edge.\n\nFirst, we must validate the problem statement.\nThe given degree distribution is $P(k)$ for $k \\in \\{1,2,3,4\\}$, with $P(1)=0.15$, $P(2)=0.35$, $P(3)=0.30$, and $P(4)=0.20$. The sum is $\\sum_k P(k) = 0.15 + 0.35 + 0.30 + 0.20 = 1.00$, so it is a valid probability distribution.\nThe given joint endpoint-degree distribution $P(k,k')$ is specified for $k \\le k'$. The total probability is $\\sum_{k,k'} P(k,k') = \\sum_k P(k,k) + 2 \\sum_{kk'} P(k,k')$.\nThe sum of diagonal terms is $P(1,1)+P(2,2)+P(3,3)+P(4,4) = \\frac{1}{102}+\\frac{6}{102}+\\frac{12}{102}+\\frac{11}{102} = \\frac{30}{102}$.\nThe sum of off-diagonal terms for $kk'$ is $P(1,2)+P(1,3)+P(1,4)+P(2,3)+P(2,4)+P(3,4) = \\frac{4}{102}+\\frac{1}{102}+0+\\frac{10}{102}+\\frac{8}{102}+\\frac{13}{102} = \\frac{36}{102}$.\nThe total sum is $\\frac{30}{102} + 2 \\times \\frac{36}{102} = \\frac{30+72}{102} = \\frac{102}{102} = 1$, so $P(k,k')$ is a valid joint probability distribution.\n\nA crucial consistency check is required between $P(k)$ and $P(k,k')$. The marginal distribution of $P(k,k')$, which represents the probability $q_k$ that a random edge is connected to a node of degree $k$, must be consistent with the definition $q_k = \\frac{k P(k)}{\\langle k \\rangle}$.\nFirst, we compute the average degree $\\langle k \\rangle$ from the node-centric distribution $P(k)$:\n$$ \\langle k \\rangle = \\sum_{k} k P(k) = (1)(0.15) + (2)(0.35) + (3)(0.30) + (4)(0.20) = 0.15 + 0.70 + 0.90 + 0.80 = 2.55 $$\nNow, we compute the edge-endpoint degree distribution $q_k$ using its definition:\n$$ q_1 = \\frac{1 \\times P(1)}{\\langle k \\rangle} = \\frac{0.15}{2.55} = \\frac{15}{255} = \\frac{1}{17} $$\n$$ q_2 = \\frac{2 \\times P(2)}{\\langle k \\rangle} = \\frac{0.70}{2.55} = \\frac{70}{255} = \\frac{14}{51} $$\n$$ q_3 = \\frac{3 \\times P(3)}{\\langle k \\rangle} = \\frac{0.90}{2.55} = \\frac{90}{255} = \\frac{18}{51} = \\frac{6}{17} $$\n$$ q_4 = \\frac{4 \\times P(4)}{\\langle k \\rangle} = \\frac{0.80}{2.55} = \\frac{80}{255} = \\frac{16}{51} $$\nNext, we compute the marginal distribution from $P(k,k')$, let's call it $q'_k = \\sum_{k'} P(k,k')$. We use the symmetry $P(k',k) = P(k,k')$.\n$$ q'_1 = P(1,1)+P(1,2)+P(1,3)+P(1,4) = \\frac{1}{102}+\\frac{4}{102}+\\frac{1}{102}+0 = \\frac{6}{102} = \\frac{1}{17} $$\n$$ q'_2 = P(2,1)+P(2,2)+P(2,3)+P(2,4) = \\frac{4}{102}+\\frac{6}{102}+\\frac{10}{102}+\\frac{8}{102} = \\frac{28}{102} = \\frac{14}{51} $$\n$$ q'_3 = P(3,1)+P(3,2)+P(3,3)+P(3,4) = \\frac{1}{102}+\\frac{10}{102}+\\frac{12}{102}+\\frac{13}{102} = \\frac{36}{102} = \\frac{6}{17} $$\n$$ q'_4 = P(4,1)+P(4,2)+P(4,3)+P(4,4) = 0+\\frac{8}{102}+\\frac{13}{102}+\\frac{11}{102} = \\frac{32}{102} = \\frac{16}{51} $$\nSince $q'_k=q_k$ for all $k$, the provided distributions are consistent. The problem is valid.\n\nLet $K$ and $K'$ be the random variables for the degrees at the ends of a uniformly chosen edge. Their joint distribution is $P(k,k')$. Since $P(k,k')$ is symmetric, $K$ and $K'$ are identically distributed with the marginal distribution $q_k$. The assortativity coefficient $r$ is the Pearson correlation coefficient:\n$$ r = \\frac{\\text{Cov}(K, K')}{\\sigma_K \\sigma_{K'}} = \\frac{\\mathbb{E}[KK'] - \\mathbb{E}[K]\\mathbb{E}[K']}{\\sigma_q^2} = \\frac{\\mathbb{E}[KK'] - (\\mu_q)^2}{\\mathbb{E}[K^2] - (\\mu_q)^2} $$\nwhere $\\mu_q = \\mathbb{E}[K]$ and $\\sigma_q^2 = \\text{Var}(K)$ are the mean and variance of the distribution $q_k$.\n\nWe calculate the necessary moments.\nThe mean of the distribution $q_k$:\n$$ \\mu_q = \\mathbb{E}[K] = \\sum_k k q_k = (1)\\frac{1}{17} + (2)\\frac{14}{51} + (3)\\frac{6}{17} + (4)\\frac{16}{51} = \\frac{3}{51} + \\frac{28}{51} + \\frac{54}{51} + \\frac{64}{51} = \\frac{3+28+54+64}{51} = \\frac{149}{51} $$\nThe second moment of the distribution $q_k$:\n$$ \\mathbb{E}[K^2] = \\sum_k k^2 q_k = (1^2)\\frac{1}{17} + (2^2)\\frac{14}{51} + (3^2)\\frac{6}{17} + (4^2)\\frac{16}{51} = \\frac{3}{51} + \\frac{56}{51} + \\frac{162}{51} + \\frac{256}{51} = \\frac{3+56+162+256}{51} = \\frac{477}{51} $$\nThe variance of the distribution $q_k$ is the denominator of $r$:\n$$ \\sigma_q^2 = \\mathbb{E}[K^2] - (\\mu_q)^2 = \\frac{477}{51} - \\left(\\frac{149}{51}\\right)^2 = \\frac{477 \\times 51 - 149^2}{51^2} = \\frac{24327 - 22201}{2601} = \\frac{2126}{2601} $$\nNext, we compute the mixed moment $\\mathbb{E}[KK']$ from the joint distribution $P(k,k')$:\n$$ \\mathbb{E}[KK'] = \\sum_{k,k'} k k' P(k,k') = \\sum_k k^2 P(k,k) + 2\\sum_{kk'} k k' P(k,k') $$\nThe diagonal part:\n$$ (1^2)P(1,1) + (2^2)P(2,2) + (3^2)P(3,3) + (4^2)P(4,4) = \\frac{1}{102} + \\frac{4 \\cdot 6}{102} + \\frac{9 \\cdot 12}{102} + \\frac{16 \\cdot 11}{102} = \\frac{1+24+108+176}{102} = \\frac{309}{102} $$\nThe off-diagonal part:\n$$ 2 \\left[ (1\\cdot2)P(1,2) + (1\\cdot3)P(1,3) + (2\\cdot3)P(2,3) + (2\\cdot4)P(2,4) + (3\\cdot4)P(3,4) \\right] $$\n$$ = 2 \\left[ (2)\\frac{4}{102} + (3)\\frac{1}{102} + (6)\\frac{10}{102} + (8)\\frac{8}{102} + (12)\\frac{13}{102} \\right] = \\frac{2}{102} [8+3+60+64+156] = \\frac{2 \\times 291}{102} = \\frac{582}{102} $$\n$$ \\mathbb{E}[KK'] = \\frac{309}{102} + \\frac{582}{102} = \\frac{891}{102} $$\nThe numerator of $r$ is $\\mathbb{E}[KK'] - (\\mu_q)^2$:\n$$ \\mathbb{E}[KK'] - (\\mu_q)^2 = \\frac{891}{102} - \\left(\\frac{149}{51}\\right)^2 = \\frac{891}{2 \\times 51} - \\frac{149^2}{51^2} = \\frac{891 \\times 51 - 2 \\times 149^2}{2 \\times 51^2} $$\n$$ = \\frac{45441 - 2 \\times 22201}{2 \\times 2601} = \\frac{45441 - 44402}{5202} = \\frac{1039}{5202} $$\nFinally, we compute $r$ by dividing the numerator by the variance $\\sigma_q^2$:\n$$ r = \\frac{\\frac{1039}{5202}}{\\frac{2126}{2601}} = \\frac{1039}{5202} \\times \\frac{2601}{2126} $$\nSince $5202 = 2 \\times 2601$, this simplifies to:\n$$ r = \\frac{1039}{2 \\times 2126} = \\frac{1039}{4252} $$\nThe number $1039$ is a prime number. The number $4252$ is not divisible by $1039$. Therefore, this fraction is in its simplest form.\n\nThe assortativity coefficient is $r = \\frac{1039}{4252}$, which is a positive value ($r \\approx 0.244$). A positive assortativity coefficient signifies that nodes in the network tend to connect to other nodes of similar degree. This phenomenon is known as assortative mixing. High-degree nodes have a preference to connect to other high-degree nodes, and low-degree nodes have a preference to connect to other low-degree nodes.",
            "answer": "$$\\boxed{\\frac{1039}{4252}}$$"
        },
        {
            "introduction": "The abstract properties of a degree distribution often have dramatic, system-wide consequences, especially for scale-free networks with power-law distributions. In this capstone exercise , you will analyze the vulnerability of such a network to a targeted attack on its high-degree hubs. By applying the Molloy-Reed criterion, you will determine the critical fraction of nodes that must be removed to dismantle the network's giant component, providing a powerful demonstration of how theoretical analysis yields crucial insights into the resilience of complex adaptive systems.",
            "id": "4132896",
            "problem": "Consider a large, sparse network generated by the Configuration Model (CM), representing a complex adaptive system with independent node degrees drawn from a continuous power-law degree distribution. The degree distribution is given by $P(k) = C k^{-\\gamma}$ for $k \\geq k_{\\min}$, where $\\gamma = 4$ and $k_{\\min} = \\frac{3}{2}$, and $C$ is the normalization constant. A targeted attack removes all nodes with degree strictly greater than a chosen cutoff $k_c$, leaving the remaining nodes and their connections intact. Assume the network is sufficiently large and locally tree-like so that branching-process arguments apply.\n\nUsing foundational definitions of the CM and the general condition for emergence of a giant connected component in random graphs with independent degrees, determine the minimal fraction $p_c$ of nodes that must be removed by the targeted attack (i.e., by choosing $k_c$) so that the remaining network has no giant component. Your derivation should explicitly:\n- Normalize $P(k)$,\n- Express the removed fraction $p$ in terms of the cutoff $k_c$ by integrating the tail of $P(k)$,\n- Renormalize the truncated degree distribution on $[k_{\\min}, k_c]$,\n- Compute the first and second moments of the renormalized degree distribution,\n- Impose the criticality condition for disappearance of the giant component, and solve for the cutoff $k_c$ and thus $p_c$.\n\nExpress the final answer for $p_c$ as an exact number. No rounding is necessary. Do not include units in your answer.",
            "solution": "We begin by normalizing the power-law degree distribution $P(k) = C k^{-\\gamma}$ on $[k_{\\min}, \\infty)$ for $\\gamma = 4$ and $k_{\\min} = \\frac{3}{2}$. The normalization constant $C$ satisfies\n$$\n\\int_{k_{\\min}}^{\\infty} C k^{-\\gamma} \\, dk = 1.\n$$\nWith $\\gamma = 4$, we compute\n$$\n\\int_{k_{\\min}}^{\\infty} k^{-4} \\, dk = \\left[ -\\frac{1}{3} k^{-3} \\right]_{k_{\\min}}^{\\infty} = \\frac{1}{3} k_{\\min}^{-3}.\n$$\nThus $C \\cdot \\frac{1}{3} k_{\\min}^{-3} = 1$, implying\n$$\nC = 3 k_{\\min}^{3}.\n$$\n\nA targeted attack removes all nodes with degree $k  k_c$. The fraction removed is the tail probability\n$$\np = \\int_{k_c}^{\\infty} P(k) \\, dk = \\int_{k_c}^{\\infty} C k^{-4} \\, dk = C \\left[ -\\frac{1}{3} k^{-3} \\right]_{k_c}^{\\infty} = \\frac{C}{3} k_c^{-3}.\n$$\nSubstituting $C = 3 k_{\\min}^{3}$ yields\n$$\np = \\frac{3 k_{\\min}^{3}}{3} k_c^{-3} = \\left( \\frac{k_{\\min}}{k_c} \\right)^{3}.\n$$\nTherefore, the remaining fraction is $1 - p$, and the truncated, renormalized degree distribution on $[k_{\\min}, k_c]$ is\n$$\nP_r(k) = \\frac{P(k)}{1-p} = \\frac{C k^{-4}}{1-p}, \\quad k \\in [k_{\\min}, k_c].\n$$\n\nWe compute the first and second moments of the renormalized distribution. The first moment is\n$$\n\\langle k \\rangle_r = \\int_{k_{\\min}}^{k_c} k \\, P_r(k) \\, dk = \\frac{C}{1-p} \\int_{k_{\\min}}^{k_c} k^{-3} \\, dk = \\frac{C}{1-p} \\left[ -\\frac{1}{2} k^{-2} \\right]_{k_{\\min}}^{k_c} = \\frac{C}{1-p} \\cdot \\frac{1}{2} \\left( k_{\\min}^{-2} - k_c^{-2} \\right).\n$$\nThe second moment is\n$$\n\\langle k^{2} \\rangle_r = \\int_{k_{\\min}}^{k_c} k^{2} \\, P_r(k) \\, dk = \\frac{C}{1-p} \\int_{k_{\\min}}^{k_c} k^{-2} \\, dk = \\frac{C}{1-p} \\left[ -k^{-1} \\right]_{k_{\\min}}^{k_c} = \\frac{C}{1-p} \\left( k_{\\min}^{-1} - k_c^{-1} \\right).\n$$\n\nFor a CM network that is locally tree-like, the emergence of a giant component is governed by the Molloy-Reed criterion (MRC), which can be derived from branching-process arguments: a giant component exists if and only if the mean excess degree exceeds one, equivalently\n$$\n\\frac{\\langle k(k-1) \\rangle}{\\langle k \\rangle}  1.\n$$\nSince $\\langle k(k-1) \\rangle = \\langle k^{2} \\rangle - \\langle k \\rangle$, this inequality is equivalent to\n$$\n\\frac{\\langle k^{2} \\rangle}{\\langle k \\rangle}  2,\n$$\nand the disappearance of the giant component occurs exactly at criticality when\n$$\n\\frac{\\langle k^{2} \\rangle_r}{\\langle k \\rangle_r} = 2.\n$$\n\nWe set the ratio computed for the truncated distribution equal to $2$:\n$$\n\\frac{\\langle k^{2} \\rangle_r}{\\langle k \\rangle_r} = \\frac{ \\frac{C}{1-p} \\left( k_{\\min}^{-1} - k_c^{-1} \\right) }{ \\frac{C}{1-p} \\cdot \\frac{1}{2} \\left( k_{\\min}^{-2} - k_c^{-2} \\right) } = 2 \\cdot \\frac{ k_{\\min}^{-1} - k_c^{-1} }{ k_{\\min}^{-2} - k_c^{-2} }.\n$$\nSet this equal to $2$:\n$$\n2 \\cdot \\frac{ k_{\\min}^{-1} - k_c^{-1} }{ k_{\\min}^{-2} - k_c^{-2} } = 2.\n$$\nCancel the factor $2$ from both sides to obtain\n$$\n\\frac{ k_{\\min}^{-1} - k_c^{-1} }{ k_{\\min}^{-2} - k_c^{-2} } = 1.\n$$\nWe simplify the left-hand side. Note that\n$$\nk_{\\min}^{-1} - k_c^{-1} = \\frac{k_c - k_{\\min}}{k_{\\min} k_c}, \\quad k_{\\min}^{-2} - k_c^{-2} = \\frac{k_c^{2} - k_{\\min}^{2}}{k_{\\min}^{2} k_c^{2}} = \\frac{(k_c - k_{\\min})(k_c + k_{\\min})}{k_{\\min}^{2} k_c^{2}}.\n$$\nThus\n$$\n\\frac{ k_{\\min}^{-1} - k_c^{-1} }{ k_{\\min}^{-2} - k_c^{-2} } = \\frac{ \\frac{k_c - k_{\\min}}{k_{\\min} k_c} }{ \\frac{(k_c - k_{\\min})(k_c + k_{\\min})}{k_{\\min}^{2} k_c^{2}} } = \\frac{ k_{\\min} k_c }{ k_c + k_{\\min} }.\n$$\nThe criticality equation therefore reduces to\n$$\n\\frac{ k_{\\min} k_c }{ k_c + k_{\\min} } = 1.\n$$\nSolving for $k_c$ gives\n$$\nk_{\\min} k_c = k_c + k_{\\min} \\quad \\Longrightarrow \\quad k_c (k_{\\min} - 1) = k_{\\min} \\quad \\Longrightarrow \\quad k_c = \\frac{k_{\\min}}{k_{\\min} - 1}.\n$$\n\nWith $k_{\\min} = \\frac{3}{2}$, we find\n$$\nk_c = \\frac{ \\frac{3}{2} }{ \\frac{3}{2} - 1 } = \\frac{ \\frac{3}{2} }{ \\frac{1}{2} } = 3.\n$$\nThe corresponding removed fraction at criticality is\n$$\np_c = \\left( \\frac{k_{\\min}}{k_c} \\right)^{3} = \\left( \\frac{ \\frac{3}{2} }{ 3 } \\right)^{3} = \\left( \\frac{1}{2} \\right)^{3} = \\frac{1}{8}.\n$$\nTherefore, the minimal fraction of nodes that must be removed by targeted attack to eliminate the giant component is exactly $\\frac{1}{8}$.",
            "answer": "$$\\boxed{\\frac{1}{8}}$$"
        }
    ]
}