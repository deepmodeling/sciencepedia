{
    "hands_on_practices": [
        {
            "introduction": "本练习深入探讨了基于主体的建模中的一个核心机制：记忆。它将引导您从第一性原理出发，推导出指数遗忘机制，这是主体总结过去观测数据的一个基本工具。通过从一个优化问题开始 ()，您不仅将理解这个更新规则是如何工作的，而且将更深刻地理解为什么它是在历史信息和新数据之间取得平衡的一种最优方式。",
            "id": "4120395",
            "problem": "考虑一个复杂自适应系统中的智能体，它接收到一个标量观测流 $\\{x_t\\}_{t \\in \\mathbb{Z}_{\\ge 0}}$。该智能体维持一个标量内部属性 $m_t$，该属性总结了过去的观测值，并每个时间步更新一次。假设在时间 $t+1$，智能体选择 $m_{t+1}$ 以最小化在半无限历史上的一个折扣二次损失，\n$$\nJ_{t+1}(m) \\equiv \\sum_{k=0}^{\\infty} \\delta^{k} \\left(m - x_{t-k}\\right)^{2},\n$$\n其中 $\\delta \\in (0,1)$ 是一个固定的折扣因子，反映了指数新近度加权。\n\n任务：\n1. 从凸优化第一性原理和几何级数的性质出发，推导 $J_{t+1}(m)$ 的闭式最小化解，并证明它是一个过去观测值的几何加权平均。然后，将此表示代数转换为一个关联 $m_{t+1}$ 与 $m_t$ 和 $x_t$ 的单步递归关系；将 $m_t$ 上的系数解释为遗忘因子 $\\alpha \\in (0,1)$，并用 $\\delta$ 表示 $\\alpha$。陈述当 $x_t \\equiv 0$ 时得到的齐次递归的稳定性条件。\n2. 为了形式化属性 $m_t$ 的有效记忆时域的概念，将在时间 $t+1$ 分配给观测值 $x_{t-k}$ 的归一化权重定义为 $p_k$，因此 $p_k$ 与应用于延迟 $k$ 的折扣成正比，并满足 $\\sum_{k=0}^{\\infty} p_k = 1$。将有效记忆时域 $H(\\alpha)$ 定义为该分布下的期望延迟，\n$$\nH(\\alpha) \\equiv \\sum_{k=0}^{\\infty} k \\, p_k.\n$$\n以 $\\alpha$ 的函数形式计算 $H(\\alpha)$ 的闭式解。你最终报告的答案必须仅为 $H(\\alpha)$ 作为 $\\alpha$ 函数的表达式（无单位）。无需四舍五入。",
            "solution": "该问题要求对一个智能体的内部属性进行两部分的推导，该属性通过最小化一个折扣二次损失函数来更新。我们将首先验证问题，然后进行求解。\n\n### 问题验证\n问题陈述提供了以下给定条件：\n- 一个标量观测流 $\\{x_t\\}_{t \\in \\mathbb{Z}_{\\ge 0}}$。\n- 一个标量内部属性 $m_t$。\n- 在时间 $t+1$ 为确定 $m_{t+1}$ 而需最小化的损失函数：$J_{t+1}(m) \\equiv \\sum_{k=0}^{\\infty} \\delta^{k} \\left(m - x_{t-k}\\right)^{2}$。\n- 一个固定的折扣因子 $\\delta \\in (0,1)$。\n- 有效记忆时域的定义：$H(\\alpha) \\equiv \\sum_{k=0}^{\\infty} k \\, p_k$，其中 $p_k$ 是归一化权重。\n\n该问题具有科学依据，因为它描述了如何从优化第一性原理推导指数加权移动平均（EWMA），这是时间序列分析和信号处理中的一个基本工具。该问题是适定 (well-posed) 的；损失函数是关于 $m$ 的严格凸二次函数，这保证了唯一最小值的存在。使用比率为 $\\delta \\in (0,1)$ 的几何级数确保了所有和的收敛性。问题陈述是客观、完整且数学上自洽的。没有违反科学原理、隐藏的歧义或事实上的不健全之处。因此，该问题被认为是有效的。\n\n### 第1部分：递归更新规则的推导\n\n在时间 $t+1$，智能体的属性（记为 $m_{t+1}$）被选择以最小化损失函数 $J_{t+1}(m)$：\n$$\nJ_{t+1}(m) = \\sum_{k=0}^{\\infty} \\delta^{k} (m - x_{t-k})^{2}\n$$\n该函数是关于 $m$ 的平方项之和，可以展开为：\n$$\nJ_{t+1}(m) = \\sum_{k=0}^{\\infty} \\delta^{k} (m^2 - 2mx_{t-k} + x_{t-k}^2) = m^2 \\sum_{k=0}^{\\infty} \\delta^k - 2m \\sum_{k=0}^{\\infty} \\delta^k x_{t-k} + \\sum_{k=0}^{\\infty} \\delta^k x_{t-k}^2\n$$\n这是 $m$ 的一个二次函数。由于 $\\delta \\in (0,1)$，几何级数 $\\sum_{k=0}^{\\infty} \\delta^k$ 收敛于 $\\frac{1}{1-\\delta} > 0$。$m^2$ 项的系数为正，因此抛物线开口向上，且 $J_{t+1}(m)$ 是严格凸的。其唯一最小值在它关于 $m$ 的导数等于零处找到。\n\n我们计算一阶导数：\n$$\n\\frac{dJ_{t+1}}{dm} = \\frac{d}{dm} \\sum_{k=0}^{\\infty} \\delta^{k} (m - x_{t-k})^{2} = \\sum_{k=0}^{\\infty} \\delta^{k} \\cdot 2(m - x_{t-k})\n$$\n将导数设为零以找到最小化解，我们称之为 $m_{t+1}$：\n$$\n\\sum_{k=0}^{\\infty} \\delta^{k} \\cdot 2(m_{t+1} - x_{t-k}) = 0\n$$\n$$\nm_{t+1} \\sum_{k=0}^{\\infty} \\delta^{k} - \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-k} = 0\n$$\n使用几何级数求和公式 $\\sum_{k=0}^{\\infty} \\delta^k = \\frac{1}{1-\\delta}$，我们求解 $m_{t+1}$：\n$$\nm_{t+1} \\left( \\frac{1}{1-\\delta} \\right) = \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-k}\n$$\n$$\nm_{t+1} = (1-\\delta) \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-k}\n$$\n该表达式表明 $m_{t+1}$ 是所有过去和当前观测值 $\\{x_t, x_{t-1}, \\dots\\}$ 的几何加权平均。观测值 $x_{t-k}$ 的权重是 $(1-\\delta)\\delta^k$。所有权重的总和为 $\\sum_{k=0}^{\\infty} (1-\\delta)\\delta^k = (1-\\delta) \\sum_{k=0}^{\\infty} \\delta^k = (1-\\delta)\\frac{1}{1-\\delta} = 1$。\n\n现在，我们将其转换为一个单步递归。让我们展开 $m_{t+1}$ 的求和式：\n$$\nm_{t+1} = (1-\\delta) \\left( \\delta^0 x_t + \\delta^1 x_{t-1} + \\delta^2 x_{t-2} + \\dots \\right)\n$$\n$$\nm_{t+1} = (1-\\delta)x_t + (1-\\delta) \\sum_{k=1}^{\\infty} \\delta^{k} x_{t-k}\n$$\n从求和项中提出因子 $\\delta$：\n$$\nm_{t+1} = (1-\\delta)x_t + \\delta \\left( (1-\\delta) \\sum_{k=1}^{\\infty} \\delta^{k-1} x_{t-k} \\right)\n$$\n我们定义一个新索引 $j = k-1$。求和变为 $\\sum_{j=0}^{\\infty} \\delta^{j} x_{t-1-j}$。\n$$\nm_{t+1} = (1-\\delta)x_t + \\delta \\left( (1-\\delta) \\sum_{j=0}^{\\infty} \\delta^{j} x_{t-1-j} \\right)\n$$\n根据定义，前一个时间步的最小化解 $m_t$ 由下式给出：\n$$\nm_t = (1-\\delta) \\sum_{k=0}^{\\infty} \\delta^{k} x_{t-1-k}\n$$\n将此代入 $m_{t+1}$ 的表达式中：\n$$\nm_{t+1} = (1-\\delta)x_t + \\delta m_t\n$$\n这就是所求的单步递归表示。问题要求将 $m_t$ 上的系数解释为遗忘因子 $\\alpha$。在我们推导的方程中，$m_t$ 的系数是 $\\delta$。因此，我们确定 $\\alpha = \\delta$。由于 $\\delta \\in (0,1)$，我们得到 $\\alpha \\in (0,1)$，符合要求。该递归通常写为 $m_{t+1} = \\alpha m_t + (1-\\alpha) x_t$。\n\n对于所有 $t$ 都有 $x_t \\equiv 0$ 的齐次情况，递归变为 $m_{t+1} = \\delta m_t$。这是一个一阶线性齐次差分方程。其解为 $m_t = m_0 \\delta^t$。稳定性的条件是对于任何有限的初始条件 $m_0$，当 $t \\to \\infty$ 时解收敛到 $0$。这要求 $|\\delta|  1$。由于问题陈述 $\\delta \\in (0,1)$，此条件得到满足。\n\n### 第2部分：有效记忆时域的计算\n\n有效记忆时域 $H(\\alpha)$ 定义为期望延迟，即 $H(\\alpha) = \\sum_{k=0}^{\\infty} k \\, p_k$。从第1部分可知，分配给观测值 $x_{t-k}$ 的归一化权重 $p_k$ 是 $p_k = (1-\\delta)\\delta^k$。这在非负整数 $k=0, 1, 2, \\dots$ 上定义了一个几何概率分布。\n\n我们需要计算这个和：\n$$\nH(\\alpha) = \\sum_{k=0}^{\\infty} k \\, p_k = \\sum_{k=0}^{\\infty} k (1-\\delta)\\delta^k = (1-\\delta) \\sum_{k=0}^{\\infty} k \\delta^k\n$$\n为了计算和 $S = \\sum_{k=0}^{\\infty} k \\delta^k$，我们使用一个涉及几何级数公式的标准技巧。对于 $|\\delta|  1$：\n$$\n\\sum_{k=0}^{\\infty} \\delta^k = \\frac{1}{1-\\delta}\n$$\n对两边关于 $\\delta$ 求导：\n$$\n\\frac{d}{d\\delta} \\sum_{k=0}^{\\infty} \\delta^k = \\sum_{k=0}^{\\infty} \\frac{d}{d\\delta} (\\delta^k) = \\sum_{k=1}^{\\infty} k \\delta^{k-1}\n$$\n以及：\n$$\n\\frac{d}{d\\delta} \\left( \\frac{1}{1-\\delta} \\right) = - (1-\\delta)^{-2} (-1) = \\frac{1}{(1-\\delta)^2}\n$$\n因此，我们有：\n$$\n\\sum_{k=1}^{\\infty} k \\delta^{k-1} = \\frac{1}{(1-\\delta)^2}\n$$\n为了求出我们的和 $S = \\sum_{k=0}^{\\infty} k \\delta^k = \\sum_{k=1}^{\\infty} k \\delta^k$，我们将上述结果乘以 $\\delta$：\n$$\n\\delta \\sum_{k=1}^{\\infty} k \\delta^{k-1} = \\sum_{k=1}^{\\infty} k \\delta^k = \\frac{\\delta}{(1-\\delta)^2}\n$$\n所以，$S = \\frac{\\delta}{(1-\\delta)^2}$。\n\n现在我们将此代回 $H(\\alpha)$ 的表达式中：\n$$\nH(\\alpha) = (1-\\delta) S = (1-\\delta) \\frac{\\delta}{(1-\\delta)^2} = \\frac{\\delta}{1-\\delta}\n$$\n最后，我们将此结果表示为 $\\alpha$ 的函数。从第1部分，我们发现 $\\alpha = \\delta$。代入这个关系，得到有效记忆时域的最终表达式：\n$$\nH(\\alpha) = \\frac{\\alpha}{1-\\alpha}\n$$\n这个量代表了对智能体当前内部状态 $m_t$ 有贡献的观测值的平均“年龄”。较大的 $\\alpha$（接近1）意味着更长的记忆时域，因为过去的值被“遗忘”得更慢。较小的 $\\alpha$（接近0）意味着更短的记忆，因为更多的权重被放在了最新的观测值上。",
            "answer": "$$\\boxed{\\frac{\\alpha}{1-\\alpha}}$$"
        },
        {
            "introduction": "从单个主体的动态演化扩展到整个种群，本练习旨在解决模型初始化的关键任务。您将学习如何为一个主体种群设计初始化策略 ()，确保其属性不仅满足约束条件，还在统计上与经验数据相匹配。通过使用贝塔分布和狄利克雷分布来复现实证矩特征，本练习将理论与实际应用联系起来。",
            "id": "4120431",
            "problem": "考虑一个复杂自适应系统中的一个智能体群体。每个智能体有两个属性：一个表示有界倾向性的标量状态 $x \\in [0,1]$，以及一个 $3$ 维分配向量 $r \\in \\Delta^{3}$，其中 $\\Delta^{3} = \\{ r \\in \\mathbb{R}^{3}_{\\ge 0} : r_{1} + r_{2} + r_{3} = 1 \\}$。您需要为这些智能体状态设计一个初始化策略，该策略需满足约束条件 $x \\in [0,1]$ 和 $r \\in \\Delta^{3}$，同时匹配指定的经验矩。\n\n从一个大型外部数据集获得的经验目标如下：\n- 对于标量属性 $x$，经验均值为 $m = 0.62$，经验方差为 $v = 0.045$。\n- 对于分配向量 $r$，经验均值向量为 $\\mu = (0.2, 0.5, 0.3)$，且各分量的方差满足 $ \\mathrm{Var}(r_{j}) = c \\, \\mu_{j} (1 - \\mu_{j})$，其中常数 $c = 0.02$ 对所有 $j \\in \\{1,2,3\\}$ 均适用。\n\n任务1（初始化设计）：为 $x$ 和 $r$ 提出一个参数化初始化模型，以确保 $x \\in [0,1]$ 和 $r \\in \\Delta^{3}$，并推导出能精确匹配上述指定经验矩的参数值。请以闭式形式提供参数；不要进行数值舍入。\n\n任务2（矩估计的样本量）：假设您随后从您的初始化模型中抽取 $N$ 个独立同分布的 $x$ 样本，以计算两个样本矩估计量：样本均值 $\\overline{x}$ 和样本二阶矩 $\\overline{x^{2}}$。确定最小的整数 $N$，使得偏差 $|\\overline{x} - \\mathbb{E}[x]|$ 和 $|\\overline{x^{2}} - \\mathbb{E}[x^{2}]|$ 分别同时小于 $\\varepsilon_{1} = 0.01$ 和 $\\varepsilon_{2} = 0.015$ 的概率至少为 $0.97$。您必须使用一种对 $[0,1]$ 上的有界变量有效的非渐近集中方法，并且必须同时考虑两个矩约束。报告满足这些要求的最小整数 $N$。任务1中不需要对实数值进行舍入；对于任务2，报告精确的最小整数样本量 $N$（由于 $N$ 是整数，无需有效数字说明）。",
            "solution": "该问题经验证是自洽、一致且科学合理的。解答根据任务要求分两部分进行。\n\n**任务1：初始化设计**\n\n本任务旨在为智能体属性 $x$ 和 $r$ 指定参数分布，以满足定义域约束并匹配给定的经验矩。\n\n**标量属性 $x$ 的初始化模型**\n\n对于标量属性 $x \\in [0,1]$，一个自然且广泛使用的参数模型是贝塔分布，记为 $x \\sim \\mathrm{Beta}(\\alpha, \\beta)$，其中 $\\alpha  0$ 和 $\\beta  0$ 是形状参数。其概率密度函数的支撑区间为 $[0,1]$。\n\n服从贝塔分布的随机变量的均值和方差由以下公式给出：\n$$\n\\mathbb{E}[x] = \\frac{\\alpha}{\\alpha+\\beta}\n$$\n$$\n\\mathrm{Var}(x) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)}\n$$\n给定经验目标为均值 $m = 0.62$ 和方差 $v = 0.045$。我们将它们与理论矩进行匹配：\n$$\nm = \\frac{\\alpha}{\\alpha+\\beta}\n$$\n$$\nv = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)}\n$$\n我们可以通过代入 $\\frac{\\alpha}{\\alpha+\\beta} = m$ 和 $\\frac{\\beta}{\\alpha+\\beta} = 1 - \\frac{\\alpha}{\\alpha+\\beta} = 1-m$ 来重写方差表达式：\n$$\nv = \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right) \\left(\\frac{\\beta}{\\alpha+\\beta}\\right) \\frac{1}{\\alpha+\\beta+1} = m(1-m)\\frac{1}{\\alpha+\\beta+1}\n$$\n从这个方程中，我们可以解出参数之和 $\\alpha+\\beta$：\n$$\n\\alpha+\\beta+1 = \\frac{m(1-m)}{v} \\implies \\alpha+\\beta = \\frac{m(1-m)}{v} - 1\n$$\n我们定义一个复合参数 $S = \\alpha+\\beta = \\frac{m(1-m)}{v} - 1$。对于一个有效的贝塔分布，我们需要 $\\alpha  0$ 和 $\\beta  0$，这要求 $S  0$，即 $\\frac{m(1-m)}{v}  1$。\n使用给定值 $m=0.62$ 和 $v=0.045$：\n$$\nS = \\frac{0.62 \\times (1-0.62)}{0.045} - 1 = \\frac{0.62 \\times 0.38}{0.045} - 1 = \\frac{0.2356}{0.045} - 1 = \\frac{2356}{450} - 1 = \\frac{1178}{225} - \\frac{225}{225} = \\frac{953}{225}\n$$\n由于 $S = \\frac{953}{225}  0$，可以确定有效的参数。现在我们求出单个参数 $\\alpha$ 和 $\\beta$：\n$$\n\\alpha = m(\\alpha+\\beta) = mS = 0.62 \\times \\frac{953}{225} = \\frac{62}{100} \\times \\frac{953}{225} = \\frac{31}{50} \\times \\frac{953}{225} = \\frac{29543}{11250}\n$$\n$$\n\\beta = (1-m)(\\alpha+\\beta) = (1-m)S = 0.38 \\times \\frac{953}{225} = \\frac{38}{100} \\times \\frac{953}{225} = \\frac{19}{50} \\times \\frac{953}{225} = \\frac{18107}{11250}\n$$\n贝塔分布的参数（以闭式形式表示）为 $\\alpha = \\frac{29543}{11250}$ 和 $\\beta = \\frac{18107}{11250}$。\n\n**分配向量 $r$ 的初始化模型**\n\n对于分配向量 $r \\in \\Delta^{3}$，其中 $\\Delta^{3} = \\{ r \\in \\mathbb{R}^{3}_{\\ge 0} : r_{1} + r_{2} + r_{3} = 1 \\}$，其标准参数模型是狄利克雷分布，记为 $r \\sim \\mathrm{Dirichlet}(\\vec{\\alpha})$，参数向量为 $\\vec{\\alpha} = (\\alpha_1, \\alpha_2, \\alpha_3)$，其中每个 $\\alpha_j  0$。\n\n令 $\\alpha_0 = \\sum_{j=1}^{3} \\alpha_j$。每个分量 $r_j$ 的均值及其方差由以下公式给出：\n$$\n\\mathbb{E}[r_j] = \\frac{\\alpha_j}{\\alpha_0}\n$$\n$$\n\\mathrm{Var}(r_j) = \\frac{\\alpha_j(\\alpha_0 - \\alpha_j)}{\\alpha_0^2 (\\alpha_0 + 1)}\n$$\n给定经验均值向量 $\\mu = (0.2, 0.5, 0.3)$ 和方差结构 $\\mathrm{Var}(r_j) = c \\, \\mu_j (1 - \\mu_j)$，其中 $c = 0.02$。\n我们将理论均值 $\\mathbb{E}[r_j]$ 与经验均值 $\\mu_j$ 进行匹配，因此有 $\\mu_j = \\frac{\\alpha_j}{\\alpha_0}$。将此代入方差公式：\n$$\n\\mathrm{Var}(r_j) = \\frac{(\\alpha_j/\\alpha_0)(\\alpha_0 - \\alpha_j)}{\\alpha_0(\\alpha_0+1)} = \\frac{\\mu_j(\\alpha_0 - \\mu_j\\alpha_0)}{\\alpha_0(\\alpha_0+1)} = \\frac{\\mu_j \\alpha_0 (1-\\mu_j)}{\\alpha_0(\\alpha_0+1)} = \\frac{\\mu_j(1-\\mu_j)}{\\alpha_0+1}\n$$\n将此与给定的经验结构 $\\mathrm{Var}(r_j) = c \\, \\mu_j (1 - \\mu_j)$ 进行比较，我们可以确定常数 $c$：\n$$\nc = \\frac{1}{\\alpha_0+1}\n$$\n我们可以解出集中参数 $\\alpha_0$：\n$$\n\\alpha_0 = \\frac{1}{c} - 1\n$$\n使用给定值 $c = 0.02 = \\frac{1}{50}$：\n$$\n\\alpha_0 = \\frac{1}{0.02} - 1 = 50 - 1 = 49\n$$\n现在我们可以使用关系式 $\\alpha_j = \\mu_j \\alpha_0$ 来找到各个参数 $\\alpha_j$：\n$$\n\\alpha_1 = \\mu_1 \\alpha_0 = 0.2 \\times 49 = 9.8 = \\frac{49}{5}\n$$\n$$\n\\alpha_2 = \\mu_2 \\alpha_0 = 0.5 \\times 49 = 24.5 = \\frac{49}{2}\n$$\n$$\n\\alpha_3 = \\mu_3 \\alpha_0 = 0.3 \\times 49 = 14.7 = \\frac{147}{10}\n$$\n狄利克雷分布的参数（以闭式形式表示）为 $\\alpha_1 = \\frac{49}{5}$，$\\alpha_2 = \\frac{49}{2}$ 和 $\\alpha_3 = \\frac{147}{10}$。\n\n**任务2：矩估计的样本量**\n\n我们需要找到最小的整数样本量 $N$，使得关于 $x$ 的样本矩的两个条件能以高概率同时满足。具体来说，对于来自 $x \\sim \\mathrm{Beta}(\\alpha, \\beta)$ 的 $N$ 个独立同分布样本 $x_i$，我们希望找到最小的 $N$，使得：\n$$\nP\\left(|\\overline{x} - \\mathbb{E}[x]|  \\varepsilon_1 \\text{ and } |\\overline{x^2} - \\mathbb{E}[x^2]|  \\varepsilon_2\\right) \\ge 0.97\n$$\n其中 $\\overline{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$ 且 $\\overline{x^2} = \\frac{1}{N}\\sum_{i=1}^N x_i^2$。给定的容差水平为 $\\varepsilon_1 = 0.01$ 和 $\\varepsilon_2 = 0.015$。要求的概率是 $1-\\delta = 0.97$，这意味着 $\\delta = 0.03$。\n\n令 $A$ 为事件 $|\\overline{x} - \\mathbb{E}[x]|  \\varepsilon_1$，$B$ 为事件 $|\\overline{x^2} - \\mathbb{E}[x^2]|  \\varepsilon_2$。我们希望找到 $N$ 使得 $P(A \\cap B) \\ge 1-\\delta$。使用联合界（布尔不等式），我们有 $P(A \\cap B) = 1 - P(A^c \\cup B^c) \\ge 1 - (P(A^c) + P(B^c))$。为了满足该条件，我们可以要求 $P(A^c) + P(B^c) \\le \\delta$。我们可以通过将总误差概率 $\\delta$ 分配给两个事件来实现这一点，例如，要求 $P(A^c) \\le \\delta/2$ 和 $P(B^c) \\le \\delta/2$。\n\n随机变量 $x$ 从贝塔分布中抽取，因此其值有界于 $[0,1]$。因此，随机变量 $x^2$ 也同样有界于 $[0,1]$。对于有界独立同分布随机变量的均值，我们可以使用霍夫丁不等式 (Hoeffding's inequality)，它提供了一个非渐近集中界。对于一个取值在 $[a,b]$ 内的随机变量 $Z$，霍夫丁不等式表述为：\n$$\nP(|\\overline{Z} - \\mathbb{E}[Z]| \\ge \\varepsilon) \\le 2\\exp\\left(\\frac{-2N\\varepsilon^2}{(b-a)^2}\\right)\n$$\n对于关于 $\\overline{x}$ 的第一个约束，变量是 $x \\in [0,1]$，所以 $(b-a)^2 = (1-0)^2 = 1$。条件 $P(A^c) = P(|\\overline{x} - \\mathbb{E}[x]| \\ge \\varepsilon_1) \\le \\delta/2$ 变为：\n$$\n2\\exp(-2N\\varepsilon_1^2) \\le \\frac{\\delta}{2} \\implies \\exp(-2N\\varepsilon_1^2) \\le \\frac{\\delta}{4} \\implies -2N\\varepsilon_1^2 \\le \\ln\\left(\\frac{\\delta}{4}\\right)\n$$\n解出 $N$ 可得：\n$$\nN \\ge \\frac{-\\ln(\\delta/4)}{2\\varepsilon_1^2} = \\frac{\\ln(4/\\delta)}{2\\varepsilon_1^2}\n$$\n对于关于 $\\overline{x^2}$ 的第二个约束，变量是 $x^2 \\in [0,1]$，所以 $(b-a)^2=1$。条件 $P(B^c) = P(|\\overline{x^2} - \\mathbb{E}[x^2]| \\ge \\varepsilon_2) \\le \\delta/2$ 变为：\n$$\n2\\exp(-2N\\varepsilon_2^2) \\le \\frac{\\delta}{2} \\implies N \\ge \\frac{\\ln(4/\\delta)}{2\\varepsilon_2^2}\n$$\n为同时满足两个约束，$N$ 必须大于或等于两个推导出的下界中的最大值：\n$$\nN \\ge \\max\\left( \\frac{\\ln(4/\\delta)}{2\\varepsilon_1^2}, \\frac{\\ln(4/\\delta)}{2\\varepsilon_2^2} \\right) = \\frac{\\ln(4/\\delta)}{2 \\min(\\varepsilon_1^2, \\varepsilon_2^2)}\n$$\n我们代入给定值：$\\delta=0.03$，$\\varepsilon_1=0.01$ 和 $\\varepsilon_2=0.015$。首先，比较 $\\varepsilon_1^2$ 和 $\\varepsilon_2^2$：\n$$\n\\varepsilon_1^2 = (0.01)^2 = 0.0001\n$$\n$$\n\\varepsilon_2^2 = (0.015)^2 = 0.000225\n$$\n因此，$\\min(\\varepsilon_1^2, \\varepsilon_2^2) = \\varepsilon_1^2 = 0.0001$。更严格的条件来自于均值 $\\mathbb{E}[x]$ 的估计。所需的样本量为：\n$$\nN \\ge \\frac{\\ln(4/0.03)}{2 \\times 0.0001} = \\frac{\\ln(400/3)}{0.0002} = 5000 \\ln\\left(\\frac{400}{3}\\right)\n$$\n现在，我们计算其数值：\n$$\n5000 \\ln\\left(\\frac{400}{3}\\right) \\approx 5000 \\times 4.892823 \\approx 24464.115\n$$\n由于样本量 $N$ 必须是整数，我们对这个值进行上取整：\n$$\nN = \\lceil 24464.115 \\rceil = 24465\n$$\n所需的最小整数样本量为 $24465$。\n\n结果如下：\n$x \\sim \\mathrm{Beta}(\\alpha, \\beta)$ 的参数：$\\alpha = \\frac{29543}{11250}$，$\\beta = \\frac{18107}{11250}$。\n$r \\sim \\mathrm{Dirichlet}(\\alpha_1, \\alpha_2, \\alpha_3)$ 的参数：$\\alpha_1 = \\frac{49}{5}$，$\\alpha_2 = \\frac{49}{2}$，$\\alpha_3 = \\frac{147}{10}$。\n矩估计的最小样本量：$N = 24465$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{29543}{11250}  \\frac{18107}{11250}  \\frac{49}{5}  \\frac{49}{2}  \\frac{147}{10}  24465\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "最后一个练习将带您直面复杂主体建模中的一个根本性挑战：“维度灾难”。您将通过第一性原理的推导，探索当使用离散网格来近似连续状态空间时，其近似误差是如何随着属性维度的增加而爆炸性增长的 ()。这个练习将使“维度灾难”这一抽象概念变得具体可感，并强调在高维系统中采用更高级状态表示技术的必要性。",
            "id": "4120425",
            "problem": "考虑在复杂自适应系统（CAS）建模中设计智能体属性和状态的任务，其中每个智能体由一个连续属性向量 $\\mathbf{x} \\in [0,1]^{n}$ 和一个标量状态评估函数 $f:[0,1]^{n} \\to \\mathbb{R}$ 描述。假设 $f$ 在 $[0,1]^{n}$ 上关于欧几里得范数是 Lipschitz 连续的，其 Lipschitz 常数为 $L  0$，即对于所有的 $\\mathbf{x},\\mathbf{y} \\in [0,1]^{n}$，不等式 $|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_{2}$ 成立。您使用一个每个维度有 $m$ 个点的均匀轴对齐网格来离散化连续状态空间，并通过最近网格点上的值来近似 $f(\\mathbf{x})$（最近邻近似）。\n\n从定义出发，并且不借助任何预先推导的离散化误差公式，推导最坏情况下的近似误差如何随维度 $n$ 和网格分辨率 $m$ 变化，并利用此结果从第一性原理展示维度灾难是如何出现的。然后，计算所需的最小总网格大小 $N$（即 $[0,1]^{n}$ 中的总网格点数），以保证对于所有 $\\mathbf{x} \\in [0,1]^{n}$，最坏情况下的近似误差的上界为一个给定的容差 $\\epsilon  0$。\n\n将您的最终答案表示为关于 $n$、$L$ 和 $\\epsilon$ 的单个符号表达式。不需要进行数值舍入，也不涉及物理单位。",
            "solution": "该问题要求推导在均匀网格上对 Lipschitz 连续函数进行近似时的最坏情况误差，展示维度灾难，并计算达到给定误差容差 $\\epsilon$ 所需的最小总网格点数 $N$。\n\n设连续属性空间为 $n$ 维超立方体 $\\mathcal{C} = [0,1]^n$。智能体的状态由一个函数 $f: \\mathcal{C} \\to \\mathbb{R}$ 评估，该函数关于欧几里得范数 $\\|\\cdot\\|_2$ 是 Lipschitz 连续的，常数为 $L0$。这意味着对于任意两点 $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}$，我们有：\n$$|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_2$$\n连续空间 $\\mathcal{C}$ 被一个均匀网格 $\\mathcal{G}$ 离散化。函数 $f(\\mathbf{x})$ 被近似为 $\\hat{f}(\\mathbf{x}) = f(\\mathbf{g}(\\mathbf{x}))$，其中 $\\mathbf{g}(\\mathbf{x})$ 是网格 $\\mathcal{G}$ 中在欧几里得范数下离 $\\mathbf{x}$ 最近的点。\n在点 $\\mathbf{x}$ 处的近似误差为 $|f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})|$。利用 Lipschitz 性质，我们可以界定这个误差：\n$$|f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})| = |f(\\mathbf{x}) - f(\\mathbf{g}(\\mathbf{x}))| \\leq L \\|\\mathbf{x} - \\mathbf{g}(\\mathbf{x})\\|_2$$\n最坏情况下的近似误差 $E_{max}$ 是该误差在所有 $\\mathbf{x} \\in \\mathcal{C}$ 上的上确界。\n$$E_{max} = \\sup_{\\mathbf{x} \\in \\mathcal{C}} |f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})| \\leq L \\left( \\sup_{\\mathbf{x} \\in \\mathcal{C}} \\|\\mathbf{x} - \\mathbf{g}(\\mathbf{x})\\|_2 \\right)$$\n对于一个适当选择的函数 $f$（例如，对于某个 $\\mathbf{x}_0$，有 $f(\\mathbf{x}) = L \\|\\mathbf{x} - \\mathbf{x}_0\\|_2$），该不等式可以取等号，因此最坏情况误差恰好是 $L$ 乘以超立方体中任意点到其最近网格点的最大可能距离。\n$$E_{max} = L \\cdot \\max_{\\mathbf{x} \\in \\mathcal{C}} \\left( \\min_{\\mathbf{y} \\in \\mathcal{G}} \\|\\mathbf{x} - \\mathbf{y}\\|_2 \\right)$$\n我们的第一个任务是确定这个最大距离。这取决于网格点的布置。问题指定了一个“每个维度有 $m$ 个点的均匀轴对齐网格”。为了最小化到网格点的最大距离，必须使用一种最优的布置策略。对于一维区间 $[0,1]$，要使到最近点的最大距离最小化，放置 $m$ 个点意味着将它们定位在 $m$ 个相等子区间的中心。也就是说，对于每个维度 $i \\in \\{1, \\dots, n\\}$，网格坐标由集合 $\\{ \\frac{2k-1}{2m} \\mid k = 1, 2, \\dots, m \\}$ 给出。\n通过这种布置，区间 $[0,1]$ 被划分为 $m$ 个形式为 $[\\frac{k-1}{m}, \\frac{k}{m}]$ 的单元（边界处稍作修改）。网格点 $\\frac{2k-1}{2m}$ 是第 $k$ 个单元的中心。在这个一维单元中，任意点到其中心的最大距离是单元宽度的一半，即 $\\frac{1}{2m}$。\n\n这个结构可以扩展到 $n$ 维。网格 $\\mathcal{G}$ 由所有点 $\\mathbf{p} = (p_1, \\dots, p_n)$ 组成，其中每个分量 $p_i$ 都从上面定义的 1D 网格坐标集中选取。这个网格将超立方体 $[0,1]^n$ 划分为 $m^n$ 个边长为 $\\frac{1}{m}$ 的较小超立方体（单元），每个单元的中心有一个网格点。\n在 $[0,1]^n$ 中离其最近网格点最远的点 $\\mathbf{x}$ 将是这些单元的顶点之一。让我们考虑单元 $[0, \\frac{1}{m}]^n$。它的中心（即最近的网格点）是 $(\\frac{1}{2m}, \\frac{1}{2m}, \\dots, \\frac{1}{2m})$。该单元的一个顶点是原点 $\\mathbf{0}=(0, 0, \\dots, 0)$。这个顶点与单元中心之间的欧几里得距离是：\n$$d_{max} = \\left\\| \\left(\\frac{1}{2m}, \\dots, \\frac{1}{2m}\\right) - (0, \\dots, 0) \\right\\|_2 = \\sqrt{\\sum_{i=1}^n \\left(\\frac{1}{2m}\\right)^2} = \\sqrt{n \\cdot \\frac{1}{4m^2}} = \\frac{\\sqrt{n}}{2m}$$\n这是从 $[0,1]^n$ 中任意点到最近网格点的最大距离。\n\n现在，我们可以用维度 $n$ 和网格分辨率 $m$ 来表示最坏情况下的近似误差 $E_{max}$：\n$$E_{max} = L \\cdot d_{max} = \\frac{L\\sqrt{n}}{2m}$$\n这个方程揭示了最坏情况误差如何变化。对于每个维度固定的网格点数 $m$，误差随着维度的平方根 $\\sqrt{n}$ 增长。\n\n这直接导致了维度灾难。网格中的总点数是 $N = m^n$。我们可以用 $N$ 和 $n$ 来表示 $m$，即 $m = N^{1/n}$。将此代入误差公式：\n$$E_{max} = \\frac{L\\sqrt{n}}{2N^{1/n}}$$\n为了在维度 $n$ 增加时保持恒定的误差水平 $E_{max} = C$，总网格点数 $N$ 必须急剧增长。对 $N$ 进行重排，我们得到 $N^{1/n} = \\frac{L\\sqrt{n}}{2C}$，这意味着 $N = \\left(\\frac{L\\sqrt{n}}{2C}\\right)^n$。$N$ 对 $n$ 的指数依赖性表明，要将空间离散化到固定精度所需的点数随维度呈爆炸性增长，这使得基于均匀网格的方法在高维空间中计算上变得不可行。这是维度灾难的典型表现。\n\n最后，我们计算所需的最小总网格大小 $N$，以确保最坏情况误差被容差 $\\epsilon  0$ 所限制。我们设定条件：\n$$E_{max} \\leq \\epsilon$$\n$$\\frac{L\\sqrt{n}}{2m} \\leq \\epsilon$$\n我们必须找到满足这个不等式的最小整数 $m$。求解 $m$：\n$$2m\\epsilon \\ge L\\sqrt{n}$$\n$$m \\ge \\frac{L\\sqrt{n}}{2\\epsilon}$$\n由于 $m$ 必须是一个整数（它是每个维度的点数），所需的最小值是大于或等于这个下界的最小整数。这由向上取整函数（ceiling function）给出：\n$$m_{min} = \\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil$$\n总网格点数是 $N = m^n$。因此，保证误差容差所需的最小总网格大小 $N_{min}$ 是：\n$$N_{min} = (m_{min})^n = \\left(\\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil\\right)^n$$\n这个表达式给出了最小网格点数，它是维度 $n$、Lipschitz 常数 $L$ 和所需误差容差 $\\epsilon$ 的函数。",
            "answer": "$$\\boxed{\\left(\\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil\\right)^{n}}$$"
        }
    ]
}