{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration with the fundamental building block of a dynamic agent state: the update rule. A common approach in agent-based modeling is to represent internal attributes like habits or beliefs as states that adapt over time in response to external stimuli. This first practice  uses a simple linear model of habit formation to explore how the choice of an adaptation rate parameter governs the system's dynamic behavior, specifically its stability and convergence properties. Mastering this relationship is the first step toward designing agents that learn and adapt in a predictable and controlled manner.",
            "id": "4120455",
            "problem": "Consider an agent in a Complex Adaptive Systems (CAS) setting with a scalar habit attribute state $h_t \\in \\mathbb{R}$ that updates in discrete time according to the rule $h_{t+1} = h_t + \\gamma \\left( x_t - h_t \\right)$, where $x_t \\in \\mathbb{R}$ is an externally supplied observable stimulus and $\\gamma \\ge 0$ is a tunable adaptation rate. Assume that $x_t$ is bounded and piecewise constant, with step changes at times $t_0 \\in \\mathbb{Z}_{\\ge 0}$ so that $x_t = x^{-}$ for $t < t_0$ and $x_t = x^{+}$ for $t \\ge t_0$, where $x^{-}, x^{+} \\in \\mathbb{R}$. Define the tracking error after a step at $t_0$ as $e_t := h_t - x_t$ for $t \\ge t_0$.\n\nUsing only foundational principles of discrete-time linear dynamical systems, determine the largest value $\\gamma^{\\star}$ such that for every step change of $x_t$ and every initial condition $h_{t_0} \\in \\mathbb{R}$, the trajectory $h_t$ converges to the new level $x^{+}$ without oscillation, where “without oscillation” is understood as the property that the error $e_t$ does not change sign for $t \\ge t_0$ (equivalently, the response is monotone toward $x^{+}$). Report the single real number $\\gamma^{\\star}$ as your final answer. No rounding is required and no physical units are involved.",
            "solution": "The agent’s state update is $h_{t+1} = h_t + \\gamma \\left( x_t - h_t \\right)$, which can be rewritten as\n$$\nh_{t+1} = (1 - \\gamma) h_t + \\gamma x_t.\n$$\nWe analyze the response to a step change at time $t_0$, where $x_t = x^{+}$ for all $t \\ge t_0$. For $t \\ge t_0$, the system becomes a linear time-invariant affine recursion with a constant input $x^{+}$:\n$$\nh_{t+1} = (1 - \\gamma) h_t + \\gamma x^{+}.\n$$\nDefine the post-step error $e_t := h_t - x^{+}$ for $t \\ge t_0$. Substituting $h_t = e_t + x^{+}$ into the update gives\n\\begin{align*}\ne_{t+1} + x^{+} &= (1 - \\gamma)(e_t + x^{+}) + \\gamma x^{+}, \\\\\ne_{t+1} &= (1 - \\gamma) e_t + (1 - \\gamma) x^{+} + \\gamma x^{+} - x^{+}, \\\\\ne_{t+1} &= (1 - \\gamma) e_t.\n\\end{align*}\nThus the error evolves according to a homogeneous linear recursion with constant multiplier $(1 - \\gamma)$:\n$$\ne_t = (1 - \\gamma)^{t - t_0} e_{t_0}, \\quad t \\ge t_0.\n$$\nThe basic requirements for tracking are:\n\n- Stability (convergence of $h_t$ to $x^{+}$): from $e_t \\to 0$, which occurs if and only if $|1 - \\gamma| < 1$, i.e.,\n$$\n-1 < 1 - \\gamma < 1 \\quad \\Longleftrightarrow \\quad 0 < \\gamma < 2.\n$$\n\n- Non-oscillation (monotone approach to $x^{+}$): the error $e_t$ must not change sign for $t \\ge t_0$. Because $e_{t+1} = (1 - \\gamma) e_t$, if $(1 - \\gamma) < 0$ then the sign of $e_t$ flips at every step, producing oscillation. Therefore non-oscillation requires\n$$\n1 - \\gamma \\ge 0 \\quad \\Longleftrightarrow \\quad \\gamma \\le 1.\n$$\nCombining the two conditions gives the set of $\\gamma$ that both converge and do so without oscillation:\n$$\n0 < \\gamma \\le 1.\n$$\nAmong these, the largest admissible $\\gamma$ that preserves non-oscillatory tracking is\n$$\n\\gamma^{\\star} = 1.\n$$\nTo verify that $\\gamma = 1$ is non-oscillatory and stable, observe that when $\\gamma = 1$ the update reduces to $h_{t+1} = x_t$, so after the step change $h_{t_1} = x^{+}$ for $t_1 = t_0 + 1$, and for all $t \\ge t_1$ we have $h_t = x^{+}$ exactly, implying $e_t = 0$ thereafter and no sign changes. Hence $\\gamma^{\\star} = 1$ is the supremum value satisfying the requirement.\n\nTherefore, the supremum adaptation rate ensuring convergence without oscillation for any step input and any initial condition is $\\gamma^{\\star} = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "After examining the dynamics of a single attribute, a critical question arises: what happens when we describe an agent with many attributes? Simply expanding the agent's state vector into a high-dimensional space introduces a profound challenge known as the \"curse of dimensionality.\" This exercise  guides you through a first-principles derivation of this concept, demonstrating how the computational resources needed to accurately represent a continuous state space grow exponentially with the number of dimensions. Understanding this scaling law is essential for designing models that remain computationally feasible as their complexity increases.",
            "id": "4120425",
            "problem": "Consider the task of designing agent attributes and states in Complex Adaptive Systems (CAS) modeling, where each agent is described by a continuous attribute vector $\\mathbf{x} \\in [0,1]^{n}$ and a scalar state-evaluation function $f:[0,1]^{n} \\to \\mathbb{R}$. Suppose that $f$ is Lipschitz continuous on $[0,1]^{n}$ with respect to the Euclidean norm, with Lipschitz constant $L > 0$, meaning that for all $\\mathbf{x},\\mathbf{y} \\in [0,1]^{n}$, the inequality $|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_{2}$ holds. You discretize the continuous state space with a uniform axis-aligned grid with $m$ points per dimension, and approximate $f(\\mathbf{x})$ by the value at the nearest grid point (nearest-neighbor approximation).\n\nStarting from the definitions and without appealing to any pre-derived discretization error formulas, derive how the worst-case approximation error scales with the dimension $n$ and the grid resolution $m$, and use this to demonstrate how the curse of dimensionality emerges from first principles. Then, compute the minimal total grid size $N$ (the total number of grid points in $[0,1]^{n}$) required to guarantee that the worst-case approximation error is bounded above by a given tolerance $\\epsilon > 0$ for all $\\mathbf{x} \\in [0,1]^{n}$.\n\nExpress your final answer as a single symbolic expression in terms of $n$, $L$, and $\\epsilon$. No numerical rounding is required and no physical units are involved.",
            "solution": "The problem asks for a derivation of the worst-case approximation error for a Lipschitz continuous function on a uniform grid, a demonstration of the curse of dimensionality, and the minimal total grid size $N$ required to achieve a given error tolerance $\\epsilon$.\n\nLet the continuous attribute space be the $n$-dimensional hypercube $\\mathcal{C} = [0,1]^n$. An agent's state is evaluated by a function $f: \\mathcal{C} \\to \\mathbb{R}$, which is Lipschitz continuous with constant $L>0$ with respect to the Euclidean norm $\\|\\cdot\\|_2$. This means for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}$, we have:\n$$|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_2$$\nThe continuous space $\\mathcal{C}$ is discretized by a uniform grid $\\mathcal{G}$. The function $f(\\mathbf{x})$ is approximated by $\\hat{f}(\\mathbf{x}) = f(\\mathbf{g}(\\mathbf{x}))$, where $\\mathbf{g}(\\mathbf{x})$ is the grid point in $\\mathcal{G}$ nearest to $\\mathbf{x}$ in the Euclidean norm.\nThe approximation error at a point $\\mathbf{x}$ is $|f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})|$. Using the Lipschitz property, we can bound this error:\n$$|f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})| = |f(\\mathbf{x}) - f(\\mathbf{g}(\\mathbf{x}))| \\leq L \\|\\mathbf{x} - \\mathbf{g}(\\mathbf{x})\\|_2$$\nThe worst-case approximation error, $E_{max}$, is the supremum of this error over all $\\mathbf{x} \\in \\mathcal{C}$.\n$$E_{max} = \\sup_{\\mathbf{x} \\in \\mathcal{C}} |f(\\mathbf{x}) - \\hat{f}(\\mathbf{x})| \\leq L \\left( \\sup_{\\mathbf{x} \\in \\mathcal{C}} \\|\\mathbf{x} - \\mathbf{g}(\\mathbf{x})\\|_2 \\right)$$\nThe inequality becomes an equality for a suitably chosen function $f$ (e.g., $f(\\mathbf{x}) = L \\|\\mathbf{x} - \\mathbf{x}_0\\|_2$ for some $\\mathbf{x}_0$), so the worst-case error is precisely $L$ times the maximum possible distance from any point in the hypercube to its nearest grid point.\n$$E_{max} = L \\cdot \\max_{\\mathbf{x} \\in \\mathcal{C}} \\left( \\min_{\\mathbf{y} \\in \\mathcal{G}} \\|\\mathbf{x} - \\mathbf{y}\\|_2 \\right)$$\nOur first task is to determine this maximum distance. This depends on the placement of the grid points. The problem specifies a \"uniform axis-aligned grid with $m$ points per dimension.\" To minimize the maximum distance to a grid point, an optimal placement strategy must be used. For a one-dimensional interval $[0,1]$, placing $m$ points to minimize the maximum distance to the nearest point means positioning them at the centers of $m$ equal sub-intervals. That is, for each dimension $i \\in \\{1, \\dots, n\\}$, the grid coordinates are given by the set $\\{ \\frac{2k-1}{2m} \\mid k = 1, 2, \\dots, m \\}$.\nWith this placement, the interval $[0,1]$ is partitioned into $m$ cells of the form $[\\frac{k-1}{m}, \\frac{k}{m}]$ (with slight modification for the boundaries). The grid point $\\frac{2k-1}{2m}$ is the center of the $k$-th cell. The maximum distance from any point in this 1D cell to its center is half the cell width, which is $\\frac{1}{2m}$.\n\nThis structure extends to $n$ dimensions. The grid $\\mathcal{G}$ consists of all points $\\mathbf{p} = (p_1, \\dots, p_n)$ where each component $p_i$ is chosen from the 1D set of grid coordinates defined above. This grid partitions the hypercube $[0,1]^n$ into $m^n$ smaller hypercubes (cells) of side length $\\frac{1}{m}$, with a grid point at the center of each cell.\nThe point $\\mathbf{x} \\in [0,1]^n$ that is maximally distant from its nearest grid point will be one of the vertices of these cells. Let's consider the cell $[0, \\frac{1}{m}]^n$. Its center (the nearest grid point) is $(\\frac{1}{2m}, \\frac{1}{2m}, \\dots, \\frac{1}{2m})$. A vertex of this cell is the origin $\\mathbf{0}=(0, 0, \\dots, 0)$. The Euclidean distance between this vertex and the cell center is:\n$$d_{max} = \\left\\| \\left(\\frac{1}{2m}, \\dots, \\frac{1}{2m}\\right) - (0, \\dots, 0) \\right\\|_2 = \\sqrt{\\sum_{i=1}^n \\left(\\frac{1}{2m}\\right)^2} = \\sqrt{n \\cdot \\frac{1}{4m^2}} = \\frac{\\sqrt{n}}{2m}$$\nThis is the maximum distance from any point in $[0,1]^n$ to the nearest grid point.\n\nNow, we can express the worst-case approximation error, $E_{max}$, in terms of the dimension $n$ and the grid resolution $m$:\n$$E_{max} = L \\cdot d_{max} = \\frac{L\\sqrt{n}}{2m}$$\nThis equation reveals how the worst-case error scales. For a fixed number of grid points per dimension, $m$, the error grows with the square root of the dimension, $\\sqrt{n}$.\n\nThis leads directly to the curse of dimensionality. The total number of points in the grid is $N = m^n$. We can express $m$ in terms of $N$ and $n$ as $m = N^{1/n}$. Substituting this into the error formula:\n$$E_{max} = \\frac{L\\sqrt{n}}{2N^{1/n}}$$\nTo maintain a constant error level $E_{max} = C$ as the dimension $n$ increases, the total number of grid points $N$ must grow dramatically. Rearranging for $N$, we get $N^{1/n} = \\frac{L\\sqrt{n}}{2C}$, which implies $N = \\left(\\frac{L\\sqrt{n}}{2C}\\right)^n$. The exponential dependence of $N$ on $n$ demonstrates that the number of points required to discretize the space to a fixed accuracy grows explosively with the dimension, making uniform grid-based methods computationally intractable in high-dimensional spaces. This is a classic manifestation of the curse of dimensionality.\n\nFinally, we compute the minimal total grid size $N$ required to ensure the worst-case error is bounded by a tolerance $\\epsilon > 0$. We set the condition:\n$$E_{max} \\leq \\epsilon$$\n$$\\frac{L\\sqrt{n}}{2m} \\leq \\epsilon$$\nWe must find the minimal integer $m$ that satisfies this inequality. Solving for $m$:\n$$2m\\epsilon \\ge L\\sqrt{n}$$\n$$m \\ge \\frac{L\\sqrt{n}}{2\\epsilon}$$\nSince $m$ must be an integer (it is the number of points per dimension), the minimum required value is the smallest integer greater than or equal to this lower bound. This is given by the ceiling function:\n$$m_{min} = \\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil$$\nThe total number of grid points is $N = m^n$. Therefore, the minimal total grid size $N_{min}$ required to guarantee the error tolerance is:\n$$N_{min} = (m_{min})^n = \\left(\\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil\\right)^n$$\nThis expression gives the minimal number of grid points as a function of the dimension $n$, the Lipschitz constant $L$, and the desired error tolerance $\\epsilon$.",
            "answer": "$$\\boxed{\\left(\\left\\lceil \\frac{L\\sqrt{n}}{2\\epsilon} \\right\\rceil\\right)^{n}}$$"
        },
        {
            "introduction": "Finally, we address a crucial and practical aspect of state design: imposing constraints. Real-world agent attributes, such as resource levels or probabilities, are often bounded. This practice  investigates the subtle but important consequences of implementing such constraints, comparing the effects of \"hard\" clipping versus \"soft\" saturation on an agent's attribute distribution. By working through this analysis, you will learn how design choices in handling boundaries can introduce systemic statistical bias, a sophisticated consideration for building robust and accurate agent-based models.",
            "id": "4120443",
            "problem": "Consider a population of agents in a complex adaptive system whose scalar attribute update input $X$ is modeled as a Gaussian random variable with mean $\\mu$ and variance $\\sigma^{2}$, i.e., $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. The agent designer enforces an upper saturation constraint on the realized attribute through a transformation $S(\\cdot)$ prior to state storage. Two design choices are considered:\n\n- A hard clipping transformation $S_{\\mathrm{clip}}(x) = \\min(x, c)$ with saturation level $c$.\n- A smooth soft-saturation transformation $S_{\\alpha}(x) = x - \\alpha (x - c)^{3} H(x - c)$, where $\\alpha > 0$ is a smoothing parameter and $H(\\cdot)$ is the Heaviside step function defined by $H(y) = 0$ for $y < 0$ and $H(y) = 1$ for $y \\ge 0$. This $S_{\\alpha}$ is continuously differentiable at $x=c$ and reduces the post-threshold growth while keeping $S_{\\alpha}(x)$ strictly increasing.\n\nFrom the standpoint of designing agent attributes and states, the fundamental performance criterion is the bias induced by the saturation relative to the identity mapping $S(x) = x$. Define the bias for a transformation $S$ as $b(S) = \\mathbb{E}[S(X)] - \\mathbb{E}[X]$. The hard-clip bias is $b_{\\mathrm{clip}} = \\mathbb{E}[S_{\\mathrm{clip}}(X)] - \\mu$, and the smooth bias is $b_{\\mathrm{smooth}} = \\mathbb{E}[S_{\\alpha}(X)] - \\mu$. Also define the clipping occurrence probability $p_{\\mathrm{clip}} = \\mathbb{P}(X > c)$ as the condition under which the state reaches the saturating regime.\n\nUsing only the core definitions of the Gaussian probability density function (PDF) and cumulative distribution function (CDF), the law of the unconscious statistician for expectations, and standard properties of truncated Gaussian moments, derive the following in closed form as functions of $\\mu$, $\\sigma$, $c$, and $\\alpha$:\n\n1. The clipping occurrence probability $p_{\\mathrm{clip}}$.\n2. The bias difference $\\Delta b = b_{\\mathrm{clip}} - b_{\\mathrm{smooth}}$, which quantifies the bias introduced by hard clipping compared to the smooth soft-saturation.\n\nProvide your final expressions in closed form using the standard normal PDF $\\phi(\\cdot)$ and CDF $\\Phi(\\cdot)$, and write them explicitly in terms of $\\mu$, $\\sigma$, $c$, and $\\alpha$. No numerical approximation is required and no rounding is permitted. State any intermediate substitutions used in your derivation, but do not introduce untested or ad hoc approximations. The final answer must be provided as a pair of expressions in a single row, in the order $(p_{\\mathrm{clip}}, \\Delta b)$.",
            "solution": "#### 1. Clipping Occurrence Probability, $p_{\\mathrm{clip}}$\nThe clipping occurrence probability is defined as $p_{\\mathrm{clip}} = \\mathbb{P}(X > c)$.\nThe random variable $X$ follows a Gaussian distribution $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. To express this probability in terms of the standard normal CDF, $\\Phi(\\cdot)$, we standardize the variable $X$. Let $Z = \\frac{X-\\mu}{\\sigma}$, such that $Z \\sim \\mathcal{N}(0, 1)$.\n\nThe inequality $X > c$ can be rewritten for the standardized variable $Z$:\n$$\n\\frac{X - \\mu}{\\sigma} > \\frac{c - \\mu}{\\sigma}\n$$\n$$\nZ > \\frac{c - \\mu}{\\sigma}\n$$\nThe probability is then:\n$$\np_{\\mathrm{clip}} = \\mathbb{P}\\left(Z > \\frac{c - \\mu}{\\sigma}\\right)\n$$\nUsing the definition of the standard normal CDF, $\\Phi(z) = \\mathbb{P}(Z \\le z)$, and the property $\\mathbb{P}(Z > z) = 1 - \\mathbb{P}(Z \\le z)$, we obtain:\n$$\np_{\\mathrm{clip}} = 1 - \\Phi\\left(\\frac{c - \\mu}{\\sigma}\\right)\n$$\n\n#### 2. Bias Difference, $\\Delta b$\nThe bias difference is defined as $\\Delta b = b_{\\mathrm{clip}} - b_{\\mathrm{smooth}}$.\nSubstituting the definitions of the biases:\n$$\n\\Delta b = (\\mathbb{E}[S_{\\mathrm{clip}}(X)] - \\mu) - (\\mathbb{E}[S_{\\alpha}(X)] - \\mu)\n$$\n$$\n\\Delta b = \\mathbb{E}[S_{\\mathrm{clip}}(X)] - \\mathbb{E}[S_{\\alpha}(X)] = \\mathbb{E}[S_{\\mathrm{clip}}(X) - S_{\\alpha}(X)]\n$$\nWe analyze the difference function $D(x) = S_{\\mathrm{clip}}(x) - S_{\\alpha}(x)$.\n-   For $x \\le c$:\n    $S_{\\mathrm{clip}}(x) = \\min(x, c) = x$.\n    $S_{\\alpha}(x) = x - \\alpha(x - c)^3 H(x - c) = x - 0 = x$, since $x-c \\le 0$ implies $H(x-c)=0$.\n    Thus, $D(x) = x - x = 0$ for $x \\le c$.\n-   For $x > c$:\n    $S_{\\mathrm{clip}}(x) = \\min(x, c) = c$.\n    $S_{\\alpha}(x) = x - \\alpha(x - c)^3 H(x - c) = x - \\alpha(x - c)^3$, since $x-c > 0$ implies $H(x-c)=1$.\n    Thus, $D(x) = c - (x - \\alpha(x-c)^3) = -(x-c) + \\alpha(x-c)^3$.\n\nThe expectation is calculated using the law of the unconscious statistician. Let $f_X(x)$ be the PDF of $X$. Since $D(x)$ is non-zero only for $x > c$, the integral is non-zero only over this domain.\n$$\n\\Delta b = \\int_{-\\infty}^{\\infty} D(x) f_X(x) dx = \\int_{c}^{\\infty} [-(x-c) + \\alpha(x-c)^3] f_X(x) dx\n$$\n$$\n\\Delta b = \\alpha \\int_{c}^{\\infty} (x-c)^3 f_X(x) dx - \\int_{c}^{\\infty} (x-c) f_X(x) dx\n$$\nLet's denote the two integrals as $I_3$ and $I_1$ respectively, so $\\Delta b = \\alpha I_3 - I_1$.\nWe evaluate these integrals by standardizing the variable of integration. Let $z = \\frac{x-\\mu}{\\sigma}$, so $x = \\sigma z + \\mu$ and $dx = \\sigma dz$. The PDF of $X$ becomes $f_X(x) dx = \\phi(z) dz$. The integration limit $x=c$ corresponds to $z = \\frac{c-\\mu}{\\sigma}$. We define the constant $\\beta = \\frac{c-\\mu}{\\sigma}$. TheTerm of integration $(x-c)$ becomes $\\sigma z + \\mu - c = \\sigma z - (c-\\mu) = \\sigma(z-\\beta)$.\n\nThe integral $I_1$ is:\n$$\nI_1 = \\int_{\\beta}^{\\infty} \\sigma(z-\\beta) \\phi(z) dz = \\sigma \\left( \\int_{\\beta}^{\\infty} z\\phi(z)dz - \\beta \\int_{\\beta}^{\\infty} \\phi(z)dz \\right)\n$$\nWe use the following standard integral identities:\n$\\int_{\\beta}^{\\infty} \\phi(z) dz = 1 - \\Phi(\\beta)$.\n$\\int_{\\beta}^{\\infty} z\\phi(z) dz = \\int_{\\beta}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz = \\frac{1}{\\sqrt{2\\pi}} [-\\exp(-z^2/2)]_{\\beta}^{\\infty} = \\phi(\\beta)$.\nSubstituting these gives:\n$$\nI_1 = \\sigma (\\phi(\\beta) - \\beta(1 - \\Phi(\\beta)))\n$$\nThe integral $I_3$ is:\n$$\nI_3 = \\int_{\\beta}^{\\infty} [\\sigma(z-\\beta)]^3 \\phi(z) dz = \\sigma^3 \\int_{\\beta}^{\\infty} (z-\\beta)^3 \\phi(z) dz\n$$\nExpanding the cubic term: $(z-\\beta)^3 = z^3 - 3\\beta z^2 + 3\\beta^2 z - \\beta^3$.\n$$\n\\int_{\\beta}^{\\infty} (z-\\beta)^3 \\phi(z)dz = \\int_{\\beta}^{\\infty} z^3\\phi(z)dz - 3\\beta\\int_{\\beta}^{\\infty} z^2\\phi(z)dz + 3\\beta^2\\int_{\\beta}^{\\infty} z\\phi(z)dz - \\beta^3\\int_{\\beta}^{\\infty} \\phi(z)dz\n$$\nWe need the second and third truncated moments of the standard normal distribution. These can be found using integration by parts, based on the recurrence relation $\\int z^n\\phi(z)dz = -z^{n-1}\\phi(z) + (n-1)\\int z^{n-2}\\phi(z)dz$.\n- For $n=2$: $\\int_{\\beta}^{\\infty} z^2\\phi(z)dz = [-z\\phi(z)]_{\\beta}^{\\infty} + \\int_{\\beta}^{\\infty} \\phi(z)dz = \\beta\\phi(\\beta) + 1 - \\Phi(\\beta)$.\n- For $n=3$: $\\int_{\\beta}^{\\infty} z^3\\phi(z)dz = [-z^2\\phi(z)]_{\\beta}^{\\infty} + 2\\int_{\\beta}^{\\infty} z\\phi(z)dz = \\beta^2\\phi(\\beta) + 2\\phi(\\beta) = (\\beta^2+2)\\phi(\\beta)$.\nSubstituting these results into the expanded integral for $I_3$:\n\\begin{align*}\n\\frac{I_3}{\\sigma^3} &= (\\beta^2+2)\\phi(\\beta) - 3\\beta(\\beta\\phi(\\beta) + 1 - \\Phi(\\beta)) + 3\\beta^2(\\phi(\\beta)) - \\beta^3(1 - \\Phi(\\beta)) \\\\\n&= (\\beta^2+2 - 3\\beta^2 + 3\\beta^2)\\phi(\\beta) + (-3\\beta - \\beta^3)(1-\\Phi(\\beta)) \\\\\n&= (\\beta^2+2)\\phi(\\beta) - (3\\beta+\\beta^3)(1-\\Phi(\\beta))\n\\end{align*}\nSo, $I_3 = \\sigma^3 [(\\beta^2+2)\\phi(\\beta) - (3\\beta+\\beta^3)(1-\\Phi(\\beta))]$.\n\nNow we compute $\\Delta b = \\alpha I_3 - I_1$:\n$$\n\\Delta b = \\alpha \\sigma^3 [(\\beta^2+2)\\phi(\\beta) - (3\\beta+\\beta^3)(1-\\Phi(\\beta))] - \\sigma [\\phi(\\beta) - \\beta(1 - \\Phi(\\beta))]\n$$\nGrouping terms by $\\phi(\\beta)$ and $(1 - \\Phi(\\beta))$:\n$$\n\\Delta b = [\\alpha\\sigma^3(\\beta^2+2) - \\sigma]\\phi(\\beta) + [-\\alpha\\sigma^3(3\\beta+\\beta^3) + \\sigma\\beta](1-\\Phi(\\beta))\n$$\nFinally, substitute $\\beta = \\frac{c-\\mu}{\\sigma}$ and simplify the coefficients.\nCoefficient of $\\phi(\\beta)$:\n$$\n\\alpha\\sigma^3\\left(\\left(\\frac{c-\\mu}{\\sigma}\\right)^2+2\\right) - \\sigma = \\alpha\\sigma^3\\left(\\frac{(c-\\mu)^2+2\\sigma^2}{\\sigma^2}\\right) - \\sigma = \\alpha\\sigma((c-\\mu)^2+2\\sigma^2) - \\sigma = \\sigma[\\alpha((c-\\mu)^2+2\\sigma^2)-1]\n$$\nCoefficient of $(1-\\Phi(\\beta))$:\n\\begin{align*}\n\\sigma\\beta - \\alpha\\sigma^3(3\\beta+\\beta^3) &= \\sigma\\left(\\frac{c-\\mu}{\\sigma}\\right) - \\alpha\\sigma^3\\left(3\\frac{c-\\mu}{\\sigma} + \\frac{(c-\\mu)^3}{\\sigma^3}\\right) \\\\\n&= (c-\\mu) - \\alpha(3\\sigma^2(c-\\mu) + (c-\\mu)^3) \\\\\n&= (c-\\mu)[1 - \\alpha(3\\sigma^2 + (c-\\mu)^2)]\n\\end{align*}\nCombining these simplified coefficients, we arrive at the final expression for the bias difference:\n$$\n\\Delta b = \\sigma[\\alpha((c-\\mu)^2+2\\sigma^2)-1]\\phi\\left(\\frac{c-\\mu}{\\sigma}\\right) + (c-\\mu)[1 - \\alpha(3\\sigma^2 + (c-\\mu)^2)]\\left(1-\\Phi\\left(\\frac{c-\\mu}{\\sigma}\\right)\\right)\n$$\nThe two required expressions have been derived in closed form as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\Phi\\left(\\frac{c-\\mu}{\\sigma}\\right) & \\sigma[\\alpha((c-\\mu)^2+2\\sigma^2)-1]\\phi\\left(\\frac{c-\\mu}{\\sigma}\\right) + (c-\\mu)[1 - \\alpha(3\\sigma^2 + (c-\\mu)^2)]\\left(1-\\Phi\\left(\\frac{c-\\mu}{\\sigma}\\right)\\right)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}