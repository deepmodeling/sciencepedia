{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of bounded rationality is the concept of \"satisficing,\" introduced by Herbert Simon. Rather than undertaking an exhaustive search for the single best option, which may be computationally intractable, agents often set an aspiration level and accept the first alternative that meets this \"good enough\" criterion. This practice provides a hands-on opportunity to formalize this intuitive idea, challenging you to calculate the optimal aspiration level for an agent in a sequential search task. By doing so, you will explore the trade-off between the potential for finding a better option and the accumulating costs of continued search .",
            "id": "4114236",
            "problem": "Consider a sequential search process in a complex adaptive environment populated by heterogeneous options. An agent with bounded rationality adopts a satisficing rule: she sets an aspiration threshold $\\alpha$ and accepts the first encountered option whose value $X$ exceeds $\\alpha$. Each option is independently and identically distributed (i.i.d.) with distribution $X \\sim \\text{Exponential}(\\lambda)$, where the rate parameter $\\lambda0$ is known to the agent. Every evaluation of an option, including the accepted one, incurs a fixed search cost $c0$. The agent aims to maximize her expected net payoff, defined as the expected value of the accepted option minus the expected cumulative search cost. Assume the agent evaluates options until one meets or exceeds the aspiration level $\\alpha$, and then stops.\n\nWorking from first principles of probability and expected values for i.i.d. sampling and exponential distributions, derive the expression for the expected net payoff as a function of $\\alpha$, and compute the optimal aspiration level $\\alpha^{\\ast}$ that maximizes the expected net payoff. Assume $0\\lambda c1$, ensuring the existence of an interior optimum. Express your final answer as a closed-form analytic expression. No rounding is required.",
            "solution": "The problem asks for the derivation of the expected net payoff for a satisficing agent and the optimal aspiration level $\\alpha^{\\ast}$ that maximizes this payoff. The problem is well-posed and scientifically grounded in the theory of optimal stopping and search.\n\nFirst, we define the components of the agent's expected net payoff, which we denote by $J(\\alpha)$. The payoff is the value of the accepted option minus the cumulative search cost.\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)]$$\nwhere $V(\\alpha)$ is the value of the accepted option when the aspiration is $\\alpha$, and $C(\\alpha)$ is the total search cost.\n\nThe agent inspects options with values $X$ drawn independently from an exponential distribution with rate parameter $\\lambda > 0$. The probability density function (PDF) is $f(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$, and the cumulative distribution function (CDF) is $F(x) = 1 - \\exp(-\\lambda x)$ for $x \\ge 0$.\n\nThe agent stops and accepts an option if its value $X$ meets or exceeds the aspiration level $\\alpha$. The problem text contains a minor ambiguity between \"exceeds $\\alpha$\" and \"meets or exceeds $\\alpha$\". However, for a continuous distribution like the exponential distribution, the probability of obtaining a value exactly equal to $\\alpha$ is zero, i.e., $P(X = \\alpha) = 0$. Therefore, $P(X > \\alpha) = P(X \\ge \\alpha)$, and the ambiguity is inconsequential. We proceed by using the condition $X \\ge \\alpha$.\n\nLet $p$ be the probability of a single search being successful, i.e., the probability that an option's value meets or exceeds $\\alpha$.\n$$p = P(X \\ge \\alpha) = 1 - P(X  \\alpha) = 1 - F(\\alpha) = 1 - (1 - \\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\nAlternatively, we can compute this via integration:\n$$p = \\int_{\\alpha}^{\\infty} \\lambda \\exp(-\\lambda x) dx = \\left[ -\\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} = 0 - (-\\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\n\nThe number of searches, let's call it $N$, follows a geometric distribution with success probability $p$. The agent searches until the first success. The expected number of searches is:\n$$E[N] = \\frac{1}{p} = \\frac{1}{\\exp(-\\lambda \\alpha)} = \\exp(\\lambda \\alpha)$$\n\nEach search incurs a cost $c > 0$. The expected total search cost, $E[C(\\alpha)]$, is the cost per search multiplied by the expected number of searches:\n$$E[C(\\alpha)] = c \\cdot E[N] = c \\exp(\\lambda \\alpha)$$\n\nNext, we determine the expected value of the accepted option, $E[V(\\alpha)]$. This is the conditional expectation of $X$, given that $X \\ge \\alpha$.\n$$E[V(\\alpha)] = E[X | X \\ge \\alpha] = \\frac{\\int_{\\alpha}^{\\infty} x f(x) dx}{P(X \\ge \\alpha)}$$\nWe have already calculated the denominator $P(X \\ge \\alpha) = \\exp(-\\lambda \\alpha)$. We now compute the numerator using integration by parts: $\\int u dv = uv - \\int v du$. Let $u = x$ and $dv = \\lambda \\exp(-\\lambda x) dx$. Then $du = dx$ and $v = -\\exp(-\\lambda x)$.\n\\begin{align*} \\int_{\\alpha}^{\\infty} x \\lambda \\exp(-\\lambda x) dx = \\left[ -x \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} - \\int_{\\alpha}^{\\infty} (-\\exp(-\\lambda x)) dx \\\\ = \\left( \\lim_{x \\to \\infty} -x \\exp(-\\lambda x) - (-\\alpha \\exp(-\\lambda \\alpha)) \\right) + \\int_{\\alpha}^{\\infty} \\exp(-\\lambda x) dx \\\\ = (0 + \\alpha \\exp(-\\lambda \\alpha)) + \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} \\\\ = \\alpha \\exp(-\\lambda \\alpha) + \\left( 0 - (-\\frac{1}{\\lambda} \\exp(-\\lambda \\alpha)) \\right) \\\\ = \\alpha \\exp(-\\lambda \\alpha) + \\frac{1}{\\lambda} \\exp(-\\lambda \\alpha) \\\\ = \\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha) \\end{align*}\nNow, we can find the conditional expectation:\n$$E[V(\\alpha)] = \\frac{\\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha)}{\\exp(-\\lambda \\alpha)} = \\alpha + \\frac{1}{\\lambda}$$\nThis result elegantly reflects the memoryless property of the exponential distribution.\n\nCombining the components, the expected net payoff function $J(\\alpha)$ is:\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)] = \\left(\\alpha + \\frac{1}{\\lambda}\\right) - c \\exp(\\lambda \\alpha)$$\nTo find the optimal aspiration level $\\alpha^{\\ast}$ that maximizes $J(\\alpha)$, we compute the first derivative with respect to $\\alpha$ and set it to zero.\n$$\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\left(\\alpha + \\frac{1}{\\lambda} - c \\exp(\\lambda \\alpha)\\right) = 1 - c \\lambda \\exp(\\lambda \\alpha)$$\nSetting the derivative to zero:\n$$1 - c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 0$$\n$$c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 1$$\n$$\\exp(\\lambda \\alpha^{\\ast}) = \\frac{1}{c \\lambda}$$\nTaking the natural logarithm of both sides:\n$$\\lambda \\alpha^{\\ast} = \\ln\\left(\\frac{1}{c \\lambda}\\right) = -\\ln(c \\lambda)$$\nSolving for $\\alpha^{\\ast}$:\n$$\\alpha^{\\ast} = -\\frac{1}{\\lambda}\\ln(c \\lambda)$$\nThe problem provides the constraint $0  \\lambda c  1$. This ensures that $\\ln(\\lambda c)$ is negative, which in turn guarantees that the optimal aspiration level $\\alpha^{\\ast}$ is positive.\n\nTo confirm that this critical point corresponds to a maximum, we examine the second derivative of $J(\\alpha)$:\n$$\\frac{d^2J}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left(1 - c \\lambda \\exp(\\lambda \\alpha)\\right) = -c \\lambda^2 \\exp(\\lambda \\alpha)$$\nGiven that $c > 0$ and $\\lambda > 0$, the exponential term $\\exp(\\lambda \\alpha)$ is always positive. Therefore, $\\frac{d^2J}{d\\alpha^2}$ is always negative. This confirms that the payoff function $J(\\alpha)$ is strictly concave, and our solution $\\alpha^{\\ast}$ is indeed the unique global maximum.",
            "answer": "$$ \\boxed{-\\frac{1}{\\lambda} \\ln(\\lambda c)} $$"
        },
        {
            "introduction": "Moving from individual decision-making to strategic environments, we examine how bounded rationality impacts interactions between agents. While classical game theory often assumes perfect rationality, real-world agents are prone to errors and noisy decision-making. This exercise introduces the Quantal Response Equilibrium (QRE), a key model in behavioral game theory that replaces the deterministic best response with a probabilistic choice rule. You will derive the equilibrium for a simple game where agents' choices are stochastically related to their expected payoffs, providing insight into how predictions change when we relax the assumption of perfect optimization .",
            "id": "4114252",
            "problem": "Consider two agents, Player $1$ and Player $2$, interacting in a $2 \\times 2$ normal-form game with Player $1$'s actions $\\{X,Y\\}$ and Player $2$'s actions $\\{L,R\\}$. The payoff matrices are defined by $u_{1}(X,L)=5$, $u_{1}(X,R)=5$, $u_{1}(Y,L)=2$, $u_{1}(Y,R)=2$ for Player $1$, and $u_{2}(L,X)=1$, $u_{2}(R,X)=3$, $u_{2}(L,Y)=1$, $u_{2}(R,Y)=3$ for Player $2$. Assume each agent exhibits bounded rationality according to the logit Quantal Response Equilibrium (QRE), where each agent $i$ chooses action $a$ with probability proportional to $\\exp(\\lambda \\,\\mathbb{E}[u_{i}(a)])$, with $\\lambda0$ denoting the rationality parameter and $\\mathbb{E}[u_{i}(a)]$ the expected utility of action $a$ given beliefs about the other agent's mixed strategy. A Quantal Response Equilibrium (QRE) is defined as a fixed point in which each agent's mixed strategy coincides with the logit response to the expected utilities induced by the other agent's mixed strategy.\n\nStarting from the core definitions of expected utility and the logit choice model under bounded rationality, derive the fixed-point conditions for the mixed strategies in this $2 \\times 2$ game and solve them. Express the equilibrium probability that Player $1$ chooses action $X$ as a closed-form function of $\\lambda$. Provide your final answer as a single simplified analytic expression. No rounding is required.",
            "solution": "The problem statement will first be validated for scientific soundness, completeness, and objectivity.\n\n### Step 1: Extract Givens\n- **Game Structure**: A $2 \\times 2$ normal-form game.\n- **Players and Actions**: Player $1$ with actions $\\{X,Y\\}$ and Player $2$ with actions $\\{L,R\\}$.\n- **Payoff Matrix for Player 1 ($u_1$)**:\n  - $u_{1}(X,L)=5$\n  - $u_{1}(X,R)=5$\n  - $u_{1}(Y,L)=2$\n  - $u_{1}(Y,R)=2$\n- **Payoff Matrix for Player 2 ($u_2$)**:\n  - $u_{2}(L,X)=1$\n  - $u_{2}(R,X)=3$\n  - $u_{2}(L,Y)=1$\n  - $u_{2}(R,Y)=3$\n- **Behavioral Model**: Logit Quantal Response Equilibrium (QRE).\n- **Choice Probability Rule**: For agent $i$, the probability of choosing action $a$ is proportional to $\\exp(\\lambda \\,\\mathbb{E}[u_{i}(a)])$, where $\\lambda > 0$ is the rationality parameter and $\\mathbb{E}[u_{i}(a)]$ is the expected utility of action $a$.\n- **Equilibrium Condition**: A QRE is a fixed point where each agent's mixed strategy coincides with the logit response to the expected utilities induced by the other agent's mixed strategy.\n- **Objective**: Derive the equilibrium probability that Player $1$ chooses action $X$ as a closed-form function of $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the concept of Quantal Response Equilibrium, a well-established and widely used model in behavioral game theory and economics for analyzing strategic interactions under bounded rationality. The logit specification is the most common form of QRE. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information (payoffs, behavioral model) to define a system of equations for the equilibrium strategies. It asks for a specific quantity (an equilibrium probability) as a function of a given parameter. The existence of a QRE is guaranteed by Brouwer's fixed-point theorem, so a solution exists and the problem is well-posed.\n- **Objective**: The problem is stated using formal mathematical and game-theoretic language, free of subjective or ambiguous terminology.\n- **Completeness and Consistency**: The payoff structure is fully defined for both players and all action profiles. The rationality model is explicitly given. There are no contradictions in the provided information.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, objective, and complete. A solution will now be derived.\n\nLet $p$ be the probability that Player $1$ chooses action $X$. Then the probability of choosing action $Y$ is $1-p$. The mixed strategy for Player $1$ is $\\sigma_1 = (p, 1-p)$.\nLet $q$ be the probability that Player $2$ chooses action $L$. Then the probability of choosing action $R$ is $1-q$. The mixed strategy for Player $2$ is $\\sigma_2 = (q, 1-q)$.\n\nFirst, we calculate the expected utility for each action for Player $1$, given Player $2$'s mixed strategy $\\sigma_2$.\nThe expected utility for Player $1$ of choosing action $X$ is:\n$$\n\\mathbb{E}[u_{1}(X)] = q \\cdot u_{1}(X,L) + (1-q) \\cdot u_{1}(X,R) = q \\cdot 5 + (1-q) \\cdot 5 = 5\n$$\nThe expected utility for Player $1$ of choosing action $Y$ is:\n$$\n\\mathbb{E}[u_{1}(Y)] = q \\cdot u_{1}(Y,L) + (1-q) \\cdot u_{1}(Y,R) = q \\cdot 2 + (1-q) \\cdot 2 = 2\n$$\nNotice that for Player $1$, the expected utility of each action is independent of Player $2$'s strategy $q$. This is a specific feature of the given payoff matrix, where $u_1(X,L) = u_1(X,R)$ and $u_1(Y,L) = u_1(Y,R)$.\n\nAccording to the logit QRE model, the probability that Player $1$ chooses action $X$ is given by the logistic choice formula:\n$$\nP_1(X) = \\frac{\\exp(\\lambda \\, \\mathbb{E}[u_{1}(X)])}{\\exp(\\lambda \\, \\mathbb{E}[u_{1}(X)]) + \\exp(\\lambda \\, \\mathbb{E}[u_{1}(Y)])}\n$$\nIn a QRE, this probability must be equal to the agent's own mixed strategy probability, so $p = P_1(X)$.\nSubstituting the calculated expected utilities:\n$$\np = \\frac{\\exp(\\lambda \\cdot 5)}{\\exp(\\lambda \\cdot 5) + \\exp(\\lambda \\cdot 2)}\n$$\nThis equation directly gives the equilibrium probability $p$ for Player $1$ as a function of $\\lambda$. It does not depend on Player $2$'s strategy, so we do not need to solve a system of coupled equations. We only need to simplify this expression.\n\nTo simplify, we can factor out $\\exp(2\\lambda)$ from the numerator and denominator, or equivalently, multiply the numerator and denominator by $\\exp(-2\\lambda)$:\n$$\np = \\frac{\\exp(5\\lambda) \\cdot \\exp(-2\\lambda)}{\\exp(5\\lambda) \\cdot \\exp(-2\\lambda) + \\exp(2\\lambda) \\cdot \\exp(-2\\lambda)}\n$$\n$$\np = \\frac{\\exp(5\\lambda - 2\\lambda)}{\\exp(5\\lambda - 2\\lambda) + \\exp(2\\lambda - 2\\lambda)}\n$$\n$$\np = \\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + \\exp(0)}\n$$\n$$\np = \\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + 1}\n$$\nThis is the closed-form expression for the equilibrium probability that Player $1$ chooses action $X$.\n\nFor completeness, although not required by the problem, we can find the equilibrium probability $q$ for Player $2$ in the same manner.\nThe expected utility for Player $2$ of choosing action $L$ is:\n$$\n\\mathbb{E}[u_{2}(L)] = p \\cdot u_{2}(L,X) + (1-p) \\cdot u_{2}(L,Y) = p \\cdot 1 + (1-p) \\cdot 1 = 1\n$$\nThe expected utility for Player $2$ of choosing action $R$ is:\n$$\n\\mathbb{E}[u_{2}(R)] = p \\cdot u_{2}(R,X) + (1-p) \\cdot u_{2}(R,Y) = p \\cdot 3 + (1-p) \\cdot 3 = 3\n$$\nThe probability that Player $2$ chooses action $L$ is:\n$$\nq = \\frac{\\exp(\\lambda \\, \\mathbb{E}[u_{2}(L)])}{\\exp(\\lambda \\, \\mathbb{E}[u_{2}(L)]) + \\exp(\\lambda \\, \\mathbb{E}[u_{2}(R)])} = \\frac{\\exp(\\lambda \\cdot 1)}{\\exp(\\lambda \\cdot 1) + \\exp(\\lambda \\cdot 3)} = \\frac{\\exp(\\lambda)}{\\exp(\\lambda) + \\exp(3\\lambda)}\n$$\nSimplifying gives $q = \\frac{1}{1 + \\exp(2\\lambda)}$. The equilibrium is the pair $(p, q)$. The question, however, only asks for the expression for $p$.\nThe final expression for the probability that Player $1$ chooses action $X$ is a function solely of the rationality parameter $\\lambda$.",
            "answer": "$$\\boxed{\\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + 1}}$$"
        },
        {
            "introduction": "This final practice delves into the nature of agency itself, moving beyond simple extrinsic rewards to consider intrinsic motivations. We will explore \"empowerment,\" an information-theoretic concept that quantifies an agent's potential to influence its environment, as an intrinsic drive. The exercise places you in a scenario where the pursuit of high immediate reward conflicts directly with the goal of maintaining future control. By modeling an agent that weighs these competing objectives, you will analyze how intrinsic motivation can shape behavior in ways that purely reward-driven models cannot capture, a crucial concept in designing autonomous and adaptive agents .",
            "id": "4114153",
            "problem": "Construct a deterministic, finite-state, discrete-time decision environment to analyze the interaction between empowerment as an intrinsic drive and extrinsic reward under bounded rationality. Begin from the following foundational bases: (i) the definition of Mutual Information (MI) and the notion of empowerment as the channel capacity from action sequences to future states, and (ii) the Boltzmann decision rule for bounded rationality that yields stochastic choices proportional to exponentiated evaluations.\n\nEnvironment specification: Let there be a start state $S_0$ with two actions $L$ and $R$. Action $L$ transitions deterministically to a terminal state $T_L$; action $R$ transitions deterministically to an intermediate state $R_1$. At $R_1$ there are exactly two distinct actions, $A$ and $B$, that lead deterministically to two distinct terminal states $R_{2a}$ and $R_{2b}$. At each terminal state, including $T_L$, a single \"wait\" action deterministically returns to itself, representing an absorbing state with no further control. The transition graph is:\n- $S_0 \\xrightarrow{L} T_L$, $S_0 \\xrightarrow{R} R_1$,\n- $T_L \\xrightarrow{W} T_L$,\n- $R_1 \\xrightarrow{A} R_{2a}$, $R_1 \\xrightarrow{B} R_{2b}$,\n- $R_{2a} \\xrightarrow{W} R_{2a}$, $R_{2b} \\xrightarrow{W} R_{2b}$.\nExtrinsic reward is immediate and only assigned at $S_0$: $r(S_0,L)=10$ and $r(S_0,R)=1$, with all other immediate rewards equal to $0$. All logarithms must be natural.\n\nEmpowerment definition: For a state $s$, and a one-step action horizon, define empowerment $E_1(s)$ as the channel capacity of the deterministic mapping from the current action to the next state. Using the standard definition of Mutual Information, $I(X;Y)=H(Y)-H(Y|X)$, where $H(\\cdot)$ is the Shannon entropy, and given deterministic transitions so that $H(Y|X)=0$, the empowerment reduces to the maximum entropy of the next-state distribution induced by actions. For a finite set of available actions whose deterministic outcomes are $k$ distinct next states, the maximum is achieved by the uniform distribution over actions mapped to distinct outcomes, yielding $E_1(s)=\\log k$. In absorbing terminal states that have a single available action returning to themselves, set $k=1$ so that $E_1(s)=\\log 1 = 0$.\n\nBounded rationality and evaluation: The agent at $S_0$ evaluates each action by combining extrinsic reward and empowerment at the subsequent state with a trade-off weight $\\alpha \\in [0,1]$ representing the agent's intrinsic motivation weight. Bounded rationality is modeled by a Boltzmann choice rule with temperature $\\tau0$: actions with higher evaluations are chosen with higher probability but not deterministically unless $\\tau \\to 0$. Your algorithm must, starting from the above bases, derive a scalar evaluation for each action at $S_0$ that respects the principle of combining immediate extrinsic reward at $S_0$ with the empowerment $E_1(\\cdot)$ at the next state reached by that action, and then convert these evaluations into a choice probability via the Boltzmann rule.\n\nTask: Implement a complete program that:\n1. Encodes the environment and computes $E_1(T_L)$ and $E_1(R_1)$ as per the above definition.\n2. For each test case $(\\alpha,\\tau)$, computes the Boltzmann choice probability at $S_0$ of selecting action $R$ over $L$ based on the derived evaluations and temperature $\\tau$.\n3. Aggregates the results into a single line of output as specified.\n\nTest suite:\n- Case 1 (happy path): $\\alpha=0.3$, $\\tau=0.5$.\n- Case 2 (extrinsic-only boundary): $\\alpha=0$, $\\tau=0.1$.\n- Case 3 (intrinsic-only boundary): $\\alpha=1$, $\\tau=0.1$.\n- Case 4 (decision indifference boundary): $\\alpha^\\star = \\frac{9}{9+\\log 2}$, $\\tau=0.2$.\n- Case 5 (high-noise edge): $\\alpha=0.5$, $\\tau=10$.\n\nOutput specification: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the probability (a float) of choosing action $R$ at $S_0$ for the corresponding test case in the order listed above (for example, $\\texttt{[p_1,p_2,p_3,p_4,p_5]}$). No additional text should be printed. There are no physical units, and angles are not involved. Percentages must be expressed as decimals, which is already implied by the probability outputs.",
            "solution": "The problem is determined to be valid as it is scientifically grounded in established principles of information theory and decision theory, mathematically well-posed, objective, and self-contained. All definitions, constraints, and data required for a unique solution are provided. We proceed with a step-by-step derivation.\n\n**Step 1: Environment Analysis and Empowerment Calculation**\n\nThe problem defines a deterministic finite-state environment. We first calculate the one-step empowerment $E_1(s)$ for the relevant non-initial states, $T_L$ and $R_1$. Empowerment is defined as the channel capacity from actions to next states, which for a deterministic system with $k$ distinct outcomes simplifies to $E_1(s) = \\log k$, where $\\log$ denotes the natural logarithm.\n\n1.  **State $T_L$**: From state $T_L$, a single action $W$ is available, which deterministically transitions back to $T_L$. There is only one possible outcome state. Therefore, the number of distinct outcomes is $k=1$. The empowerment at $T_L$ is:\n    $$ E_1(T_L) = \\log(1) = 0 $$\n\n2.  **State $R_1$**: From state $R_1$, two distinct actions, $A$ and $B$, are available. Action $A$ leads to terminal state $R_{2a}$, and action $B$ leads to terminal state $R_{2b}$. Since $R_{2a}$ and $R_{2b}$ are distinct, there are two distinct outcome states. Therefore, the number of distinct outcomes is $k=2$. The empowerment at $R_1$ is:\n    $$ E_1(R_1) = \\log(2) $$\n\n**Step 2: Derivation of Action-Value Functions**\n\nThe agent at the start state $S_0$ evaluates actions $L$ and $R$ based on a weighted combination of immediate extrinsic reward $r(S_0, a)$ and the intrinsic value derived from the empowerment $E_1(s')$ of the resulting state $s'$. The evaluation function (or action-value function) $Q(S_0, a)$ is given by the convex combination:\n\n$$ Q(S_0, a; \\alpha) = (1-\\alpha) \\cdot r(S_0, a) + \\alpha \\cdot E_1(s'_{S_0,a}) $$\n\nHere, $\\alpha \\in [0, 1]$ is the weight on intrinsic motivation, and $s'_{S_0,a}$ is the state reached after taking action $a$ in state $S_0$.\n\n1.  **Evaluation of Action $L$**: Taking action $L$ from $S_0$ leads to state $T_L$ and provides an immediate reward of $r(S_0, L) = 10$. The evaluation is:\n    $$ Q(S_0, L; \\alpha) = (1-\\alpha) \\cdot r(S_0, L) + \\alpha \\cdot E_1(T_L) $$\n    $$ Q(S_0, L; \\alpha) = (1-\\alpha) \\cdot 10 + \\alpha \\cdot 0 $$\n    $$ Q(S_0, L; \\alpha) = 10(1-\\alpha) $$\n\n2.  **Evaluation of Action $R$**: Taking action $R$ from $S_0$ leads to state $R_1$ and provides an immediate reward of $r(S_0, R) = 1$. The evaluation is:\n    $$ Q(S_0, R; \\alpha) = (1-\\alpha) \\cdot r(S_0, R) + \\alpha \\cdot E_1(R_1) $$\n    $$ Q(S_0, R; \\alpha) = (1-\\alpha) \\cdot 1 + \\alpha \\cdot \\log(2) $$\n\n**Step 3: Derivation of the Boltzmann Choice Probability**\n\nBounded rationality is modeled using the Boltzmann decision rule, where the probability of choosing an action is proportional to its exponentiated evaluation, scaled by a temperature parameter $\\tau  0$. The probability of choosing action $R$ at $S_0$ is:\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{\\exp(Q(S_0, R; \\alpha) / \\tau)}{\\exp(Q(S_0, L; \\alpha) / \\tau) + \\exp(Q(S_0, R; \\alpha) / \\tau)} $$\n\nThis can be expressed more compactly using the logistic sigmoid function by dividing the numerator and denominator by $\\exp(Q(S_0, R; \\alpha) / \\tau)$:\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{1}{\\frac{\\exp(Q(S_0, L; \\alpha) / \\tau)}{\\exp(Q(S_0, R; \\alpha) / \\tau)} + 1} = \\frac{1}{1 + \\exp\\left(\\frac{Q(S_0, L; \\alpha) - Q(S_0, R; \\alpha)}{\\tau}\\right)} $$\n\nWe compute the difference in evaluations, $\\Delta Q(\\alpha) = Q(S_0, L; \\alpha) - Q(S_0, R; \\alpha)$:\n\n$$ \\Delta Q(\\alpha) = 10(1-\\alpha) - \\left[ (1-\\alpha) + \\alpha \\log(2) \\right] $$\n$$ \\Delta Q(\\alpha) = 10 - 10\\alpha - 1 + \\alpha - \\alpha \\log(2) $$\n$$ \\Delta Q(\\alpha) = 9 - 9\\alpha - \\alpha \\log(2) $$\n$$ \\Delta Q(\\alpha) = 9 - \\alpha(9 + \\log(2)) $$\n\nSubstituting this back into the probability formula gives the final expression for the probability of choosing action $R$:\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{1}{1 + \\exp\\left(\\frac{9 - \\alpha(9 + \\log(2))}{\\tau}\\right)} $$\n\nThis formula will be implemented to calculate the results for the given test cases. As a check, we note that decision indifference ($P(R)=0.5$) occurs when the exponent is zero, which means $\\Delta Q(\\alpha) = 0$. Solving $9 - \\alpha(9 + \\log(2)) = 0$ yields $\\alpha = \\frac{9}{9 + \\log(2)}$, which matches the value $\\alpha^\\star$ given in a test case, confirming the correctness of our derived model.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Boltzmann choice probability for action 'R' at state S0\n    for a set of test cases involving a trade-off between extrinsic reward\n    and intrinsic empowerment.\n    \"\"\"\n    # Pre-calculate log(2) as it's used repeatedly.\n    # The problem specifies natural logarithms.\n    log_2 = np.log(2)\n\n    # The special value of alpha for the indifference case, derived from\n    # setting the evaluations Q(L) and Q(R) to be equal.\n    # alpha_star = 9 / (9 + log(2))\n    alpha_star = 9.0 / (9.0 + log_2)\n\n    # Define the test cases from the problem statement as (alpha, tau).\n    test_cases = [\n        (0.3, 0.5),                # Case 1 (happy path)\n        (0.0, 0.1),                # Case 2 (extrinsic-only boundary)\n        (1.0, 0.1),                # Case 3 (intrinsic-only boundary)\n        (alpha_star, 0.2),         # Case 4 (decision indifference boundary)\n        (0.5, 10.0),               # Case 5 (high-noise edge)\n    ]\n\n    results = []\n    # Loop through each test case to compute the probability.\n    for alpha, tau in test_cases:\n        # Calculate the difference in evaluations, Delta Q = Q(L) - Q(R).\n        # Based on the derived formula:\n        # Delta Q = 9 - alpha * (9 + log(2))\n        delta_q = 9.0 - alpha * (9.0 + log_2)\n\n        # Calculate the probability of choosing action R using the logistic function form\n        # of the Boltzmann (softmax) rule for two actions.\n        # P(R) = 1 / (1 + exp((Q(L) - Q(R)) / tau))\n        # This form is numerically stable, especially for large exponents.\n        prob_r = 1.0 / (1.0 + np.exp(delta_q / tau))\n        \n        results.append(prob_r)\n\n    # Format the output as a comma-separated list of floats inside square brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Print the final result in the exact required format.\n    print(output_str)\n\n# Execute the main function.\nsolve()\n```"
        }
    ]
}