{
    "hands_on_practices": [
        {
            "introduction": "这第一个练习将我们带入有限理性的一个基本概念：满意化（satisficing）。满意化智能体不会详尽搜索以寻找绝对最优选项，而是设定一个期望水平，并接受第一个满足该水平的选项。这个练习要求你对这种行为进行数学形式化，并推导出最优的期望水平本身，揭示了在约束下决定“如何决策”的一种元理性（meta-rationality）。",
            "id": "4114236",
            "problem": "考虑一个在由异构选项构成的复杂自适应环境中的序贯搜索过程。一个具有有限理性的主体采用满意原则：她设定一个期望阈值 $\\alpha$，并接受第一个遇到的价值 $X$ 超过 $\\alpha$ 的选项。每个选项都是独立同分布的（i.i.d.；independent and identically distributed），其分布为 $X \\sim \\text{Exponential}(\\lambda)$，其中率参数 $\\lambda>0$ 对主体是已知的。对每个选项的评估（包括被接受的那个）都会产生固定的搜索成本 $c>0$。该主体的目标是最大化其期望净收益，定义为被接受选项的期望值减去预期的累积搜索成本。假设该主体持续评估选项，直到有一个满足或超过期望水平 $\\alpha$，然后停止。\n\n从概率和期望值的基本原理出发，针对独立同分布抽样和指数分布，推导期望净收益作为 $\\alpha$ 的函数的表达式，并计算使期望净收益最大化的最优期望水平 $\\alpha^{\\ast}$。假设 $0  \\lambda c  1$，以确保存在内部最优解。请用封闭形式的解析表达式表示你的最终答案。无需四舍五入。",
            "solution": "该问题要求推导一个满意主体的期望净收益，以及最大化此收益的最优期望水平 $\\alpha^{\\ast}$。该问题是适定的，并且在最优停止和搜索理论中有其科学依据。\n\n首先，我们定义主体期望净收益的组成部分，我们将其表示为 $J(\\alpha)$。收益是被接受选项的价值减去累积搜索成本。\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)]$$\n其中 $V(\\alpha)$ 是当期望为 $\\alpha$ 时被接受选项的价值，而 $C(\\alpha)$ 是总搜索成本。\n\n主体检查的选项价值 $X$ 是从率参数为 $\\lambda > 0$ 的指数分布中独立抽取的。其概率密度函数（PDF）为 $f(x) = \\lambda \\exp(-\\lambda x)$（当 $x \\ge 0$ 时），累积分布函数（CDF）为 $F(x) = 1 - \\exp(-\\lambda x)$（当 $x \\ge 0$ 时）。\n\n如果一个选项的价值 $X$ 达到或超过期望水平 $\\alpha$，主体就会停止并接受该选项。问题文本在“超过 $\\alpha$”和“达到或超过 $\\alpha$”之间存在轻微的歧义。然而，对于像指数分布这样的连续分布，获得一个恰好等于 $\\alpha$ 的值的概率为零，即 $P(X = \\alpha) = 0$。因此，$P(X > \\alpha) = P(X \\ge \\alpha)$，这个歧义无关紧要。我们接下来使用条件 $X \\ge \\alpha$。\n\n令 $p$ 为单次搜索成功的概率，即一个选项的价值达到或超过 $\\alpha$ 的概率。\n$$p = P(X \\ge \\alpha) = 1 - P(X  \\alpha) = 1 - F(\\alpha) = 1 - (1 - \\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\n或者，我们可以通过积分来计算：\n$$p = \\int_{\\alpha}^{\\infty} \\lambda \\exp(-\\lambda x) dx = \\left[ -\\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} = 0 - (-\\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\n\n搜索次数，我们称之为 $N$，服从成功概率为 $p$ 的几何分布。主体会一直搜索直到第一次成功。期望的搜索次数是：\n$$E[N] = \\frac{1}{p} = \\frac{1}{\\exp(-\\lambda \\alpha)} = \\exp(\\lambda \\alpha)$$\n\n每次搜索都会产生成本 $c > 0$。预期的总搜索成本 $E[C(\\alpha)]$ 是每次搜索的成本乘以期望的搜索次数：\n$$E[C(\\alpha)] = c \\cdot E[N] = c \\exp(\\lambda \\alpha)$$\n\n接下来，我们确定被接受选项的期望值 $E[V(\\alpha)]$。这是在 $X \\ge \\alpha$ 条件下 $X$ 的条件期望。\n$$E[V(\\alpha)] = E[X | X \\ge \\alpha] = \\frac{\\int_{\\alpha}^{\\infty} x f(x) dx}{P(X \\ge \\alpha)}$$\n我们已经计算了分母 $P(X \\ge \\alpha) = \\exp(-\\lambda \\alpha)$。现在我们使用分部积分法计算分子：$\\int u dv = uv - \\int v du$。令 $u = x$ 和 $dv = \\lambda \\exp(-\\lambda x) dx$。则 $du = dx$ 且 $v = -\\exp(-\\lambda x)$。\n\\begin{align*} \\int_{\\alpha}^{\\infty} x \\lambda \\exp(-\\lambda x) dx = \\left[ -x \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} - \\int_{\\alpha}^{\\infty} (-\\exp(-\\lambda x)) dx \\\\ = \\left( \\lim_{x \\to \\infty} -x \\exp(-\\lambda x) - (-\\alpha \\exp(-\\lambda \\alpha)) \\right) + \\int_{\\alpha}^{\\infty} \\exp(-\\lambda x) dx \\\\ = (0 + \\alpha \\exp(-\\lambda \\alpha)) + \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} \\\\ = \\alpha \\exp(-\\lambda \\alpha) + \\left( 0 - (-\\frac{1}{\\lambda} \\exp(-\\lambda \\alpha)) \\right) \\\\ = \\alpha \\exp(-\\lambda \\alpha) + \\frac{1}{\\lambda} \\exp(-\\lambda \\alpha) \\\\ = \\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha) \\end{align*}\n现在，我们可以求出条件期望：\n$$E[V(\\alpha)] = \\frac{\\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha)}{\\exp(-\\lambda \\alpha)} = \\alpha + \\frac{1}{\\lambda}$$\n这个结果优雅地反映了指数分布的无记忆性。\n\n结合这些组成部分，期望净收益函数 $J(\\alpha)$ 为：\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)] = \\left(\\alpha + \\frac{1}{\\lambda}\\right) - c \\exp(\\lambda \\alpha)$$\n为了找到最大化 $J(\\alpha)$ 的最优期望水平 $\\alpha^{\\ast}$，我们计算关于 $\\alpha$ 的一阶导数并将其设为零。\n$$\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\left(\\alpha + \\frac{1}{\\lambda} - c \\exp(\\lambda \\alpha)\\right) = 1 - c \\lambda \\exp(\\lambda \\alpha)$$\n将导数设为零：\n$$1 - c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 0$$\n$$c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 1$$\n$$\\exp(\\lambda \\alpha^{\\ast}) = \\frac{1}{c \\lambda}$$\n对两边取自然对数：\n$$\\lambda \\alpha^{\\ast} = \\ln\\left(\\frac{1}{c \\lambda}\\right) = -\\ln(c \\lambda)$$\n解出 $\\alpha^{\\ast}$：\n$$\\alpha^{\\ast} = -\\frac{1}{\\lambda}\\ln(c \\lambda)$$\n问题给出了约束条件 $0  \\lambda c  1$。这确保了 $\\ln(\\lambda c)$ 为负，从而保证了最优期望水平 $\\alpha^{\\ast}$ 为正。\n\n为确认此临界点对应于一个最大值，我们考察 $J(\\alpha)$ 的二阶导数：\n$$\\frac{d^2J}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left(1 - c \\lambda \\exp(\\lambda \\alpha)\\right) = -c \\lambda^2 \\exp(\\lambda \\alpha)$$\n鉴于 $c > 0$ 和 $\\lambda > 0$，指数项 $\\exp(\\lambda \\alpha)$ 总是正的。因此，$\\frac{d^2J}{d\\alpha^2}$ 总是负的。这证实了收益函数 $J(\\alpha)$ 是严格凹的，我们的解 $\\alpha^{\\ast}$ 确实是唯一的全局最大值。",
            "answer": "$$ \\boxed{-\\frac{1}{\\lambda} \\ln(\\lambda c)} $$"
        },
        {
            "introduction": "在个体决策的基础上，我们现在考虑有限理性在策略互动中的表现，在这些互动中，结果取决于他人的行动。本练习介绍了量化反应均衡（Quantal Response Equilibrium, QRE），这是一个模型，其中智能体更频繁地做出更好的选择，但并非完全最优。你将为一个简单博弈推导均衡，从而获得模拟智能体不完美理性如何塑造集体结果的实践经验 。",
            "id": "4114252",
            "problem": "考虑两个玩家，玩家1和玩家2，在一个$2 \\times 2$的范式博弈中互动。玩家1的动作为$\\{X,Y\\}$，玩家2的动作为$\\{L,R\\}$。支付矩阵定义如下：玩家1的支付为 $u_1(X,L)=5$, $u_1(X,R)=5$, $u_1(Y,L)=2$, $u_1(Y,R)=2$；玩家2的支付为 $u_2(L,X)=1$, $u_2(R,X)=3$, $u_2(L,Y)=1$, $u_2(R,Y)=3$。假设每个玩家都表现出遵循logit量化反应均衡（QRE）的有限理性，其中每个玩家$i$选择动作$a$的概率与$\\exp(\\lambda \\,\\mathbb{E}[u_i(a)])$成正比，$\\lambda0$表示理性参数，$\\mathbb{E}[u_i(a)]$是在给定对另一玩家混合策略的信念下，动作$a$的期望效用。量化反应均衡（QRE）被定义为一个不动点，其中每个玩家的混合策略与由另一玩家混合策略所引致的期望效用的logit反应相一致。\n\n从期望效用和有限理性下logit选择模型的核心定义出发，推导这个$2 \\times 2$博弈中混合策略的不动点条件并求解。将玩家1选择动作$X$的均衡概率表示为$\\lambda$的闭式函数。请以单一简化的解析表达式形式给出最终答案。无需四舍五入。",
            "solution": "我们旨在推导在量化反应均衡（QRE）下，玩家1选择动作$X$的概率。\n\n设$p$为玩家1选择动作$X$的概率，则其选择动作$Y$的概率为$1-p$。玩家1的混合策略为$\\sigma_1 = (p, 1-p)$。\n设$q$为玩家2选择动作$L$的概率，则其选择动作$R$的概率为$1-q$。玩家2的混合策略为$\\sigma_2 = (q, 1-q)$。\n\n首先，我们计算在给定玩家2的混合策略$\\sigma_2$的情况下，玩家1每个动作的期望效用。\n玩家1选择动作$X$的期望效用为：\n$$\n\\mathbb{E}[u_1(X)] = q \\cdot u_1(X,L) + (1-q) \\cdot u_1(X,R) = q \\cdot 5 + (1-q) \\cdot 5 = 5\n$$\n玩家1选择动作$Y$的期望效用为：\n$$\n\\mathbb{E}[u_1(Y)] = q \\cdot u_1(Y,L) + (1-q) \\cdot u_1(Y,R) = q \\cdot 2 + (1-q) \\cdot 2 = 2\n$$\n值得注意的是，由于支付矩阵的特殊结构（$u_1(X,L) = u_1(X,R)$且$u_1(Y,L) = u_1(Y,R)$），玩家1的期望效用并不依赖于玩家2的策略$q$。\n\n根据logit QRE模型，玩家1选择动作$X$的概率$p$由以下公式给出：\n$$\np = \\frac{\\exp(\\lambda \\cdot \\mathbb{E}[u_1(X)])}{\\exp(\\lambda \\cdot \\mathbb{E}[u_1(X)]) + \\exp(\\lambda \\cdot \\mathbb{E}[u_1(Y)])}\n$$\n代入我们计算出的期望效用值：\n$$\np = \\frac{\\exp(5\\lambda)}{\\exp(5\\lambda) + \\exp(2\\lambda)}\n$$\n这个方程直接给出了玩家1的均衡概率$p$作为理性参数$\\lambda$的函数。为了简化表达式，我们将分子和分母同乘以$\\exp(-2\\lambda)$：\n$$\np = \\frac{\\exp(5\\lambda - 2\\lambda)}{\\exp(5\\lambda - 2\\lambda) + \\exp(2\\lambda - 2\\lambda)} = \\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + 1}\n$$\n这就是玩家1选择动作$X$的均衡概率的最终闭式表达式。",
            "answer": "$$\\boxed{\\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + 1}}$$"
        },
        {
            "introduction": "我们最后的练习探讨了“能动性”（agency）的一个更微妙的方面：外部目标与内在驱动力之间的相互作用。我们引入“赋能”（empowerment），一个信息论概念，它量化了智能体对其环境的控制感，作为一种内在动机。这个练习将你置于一个最大化外在奖励与最大化赋能相冲突的场景中，让你能够模拟一个有限理性的智能体如何平衡这些相互竞争的目标 。",
            "id": "4114153",
            "problem": "构建一个确定性的、有限状态的、离散时间的决策环境，用以分析在有限理性条件下，作为内在驱动力的赋能与外在奖励之间的相互作用。从以下基本概念出发：(i) 互信息（MI）的定义以及将赋能视为从动作序列到未来状态的信道容量的概念，以及 (ii) 用于有限理性的玻尔兹曼决策规则，该规则产生与指数化评估值成比例的随机选择。\n\n环境规范：设存在一个起始状态 $S_0$，有两个动作 $L$ 和 $R$。动作 $L$ 确定性地转移到终止状态 $T_L$；动作 $R$ 确定性地转移到中间状态 $R_1$。在 $R_1$ 处，有且仅有两个不同的动作 $A$ 和 $B$，它们确定性地导向两个不同的终止状态 $R_{2a}$ 和 $R_{2b}$。在每个终止状态（包括 $T_L$），单个“等待”动作确定性地返回其自身，表示一个无法再进行控制的吸收态。转移图如下：\n- $S_0 \\xrightarrow{L} T_L$, $S_0 \\xrightarrow{R} R_1$,\n- $T_L \\xrightarrow{W} T_L$,\n- $R_1 \\xrightarrow{A} R_{2a}$, $R_1 \\xrightarrow{B} R_{2b}$,\n- $R_{2a} \\xrightarrow{W} R_{2a}$, $R_{2b} \\xrightarrow{W} R_{2b}$。\n外在奖励是即时的，并且仅在 $S_0$ 处分配：$r(S_0,L)=10$ 和 $r(S_0,R)=1$，所有其他即时奖励均为 $0$。所有对数运算必须是自然对数。\n\n赋能定义：对于一个状态 $s$ 和单步动作时域，将赋能 $E_1(s)$ 定义为从当前动作到下一状态的确定性映射的信道容量。使用互信息的标准定义 $I(X;Y)=H(Y)-H(Y|X)$，其中 $H(\\cdot)$ 是 Shannon 熵，并且在确定性转移的条件下 $H(Y|X)=0$，赋能简化为由动作引起的下一状态分布的最大熵。对于一个可用动作的有限集合，其确定性结果是 $k$ 个不同的下一状态，最大值通过映射到不同结果的动作上的均匀分布达到，从而得到 $E_1(s)=\\log k$。在只有一个可用动作返回自身的吸收终止状态中，设 $k=1$，因此 $E_1(s)=\\log 1 = 0$。\n\n有限理性与评估：在 $S_0$ 处的智能体通过将外在奖励与后续状态的赋能相结合来评估每个动作，其中权衡权重 $\\alpha \\in [0,1]$ 代表智能体的内在动机权重。有限理性通过带有温度 $\\tau0$ 的玻尔兹曼选择规则建模：评估值较高的动作被选择的概率也较高，但除非 $\\tau \\to 0$，否则选择不是确定性的。您的算法必须从上述基础出发，为 $S_0$ 处的每个动作推导出一个标量评估值，该评估值遵循将在 $S_0$ 处的即时外在奖励与通过该动作达到的下一状态的赋能 $E_1(\\cdot)$ 相结合的原则，然后通过玻尔兹曼规则将这些评估值转换为选择概率。\n\n任务：实现一个完整的程序，该程序：\n1. 编码环境，并根据上述定义计算 $E_1(T_L)$ 和 $E_1(R_1)$。\n2. 对于每个测试用例 $(\\alpha,\\tau)$，基于推导出的评估值和温度 $\\tau$，计算在 $S_0$ 处选择动作 $R$ 相对于 $L$ 的玻尔兹曼选择概率。\n3. 按照规定将结果汇总到单行输出中。\n\n测试集：\n- 案例 1 (理想路径)：$\\alpha=0.3$, $\\tau=0.5$。\n- 案例 2 (仅外在奖励边界)：$\\alpha=0$, $\\tau=0.1$。\n- 案例 3 (仅内在奖励边界)：$\\alpha=1$, $\\tau=0.1$。\n- 案例 4 (决策无差异边界)：$\\alpha^{\\star} = \\dfrac{9}{9+\\log 2}$, $\\tau=0.2$。\n- 案例 5 (高噪声边缘)：$\\alpha=0.5$, $\\tau=10$。\n\n输出规范：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应测试用例在 $S_0$ 处选择动作 $R$ 的概率（一个浮点数），顺序如上所列（例如，$\\texttt{[p_1,p_2,p_3,p_4,p_5]}$）。不应打印任何额外文本。不涉及物理单位和角度。百分比必须以小数形式表示，这一点已由概率输出所隐含。",
            "solution": "该问题被确定为有效，因为它在科学上基于信息论和决策论的既定原则，在数学上是适定的、客观的且自洽的。获得唯一解所需的所有定义、约束和数据均已提供。我们逐步进行推导。\n\n**步骤 1：环境分析与赋能计算**\n\n问题定义了一个确定性有限状态环境。我们首先计算相关非初始状态 $T_L$ 和 $R_1$ 的单步赋能 $E_1(s)$。赋能被定义为从动作到下一状态的信道容量，对于一个具有 $k$ 个不同结果的确定性系统，这简化为 $E_1(s) = \\log k$，其中 $\\log$ 表示自然对数。\n\n1.  **状态 $T_L$**：从状态 $T_L$ 出发，只有一个可用动作 $W$，它确定性地转移回 $T_L$。只有一个可能的结果状态。因此，不同结果的数量为 $k=1$。$T_L$ 处的赋能为：\n    $$ E_1(T_L) = \\log(1) = 0 $$\n\n2.  **状态 $R_1$**：从状态 $R_1$ 出发，有两个不同的可用动作 $A$ 和 $B$。动作 $A$ 导向终止状态 $R_{2a}$，动作 $B$ 导向终止状态 $R_{2b}$。由于 $R_{2a}$ 和 $R_{2b}$ 是不同的，因此有两个不同的结果状态。因此，不同结果的数量为 $k=2$。$R_1$ 处的赋能为：\n    $$ E_1(R_1) = \\log(2) $$\n\n**步骤 2：动作价值函数的推导**\n\n处于起始状态 $S_0$ 的智能体，基于即时外在奖励 $r(S_0, a)$ 和由结果状态 $s'$ 的赋能 $E_1(s')$ 派生出的内在价值的加权组合来评估动作 $L$ 和 $R$。评估函数（或动作价值函数）$Q(S_0, a)$ 由以下凸组合给出：\n\n$$ Q(S_0, a; \\alpha) = (1-\\alpha) \\cdot r(S_0, a) + \\alpha \\cdot E_1(s'_{S_0,a}) $$\n\n此处，$\\alpha \\in [0, 1]$ 是内在动机的权重，而 $s'_{S_0,a}$ 是在状态 $S_0$ 中采取动作 $a$ 后达到的状态。\n\n1.  **动作 $L$ 的评估**：从 $S_0$ 采取动作 $L$ 会导向状态 $T_L$，并提供即时奖励 $r(S_0, L) = 10$。评估值为：\n    $$ Q(S_0, L; \\alpha) = (1-\\alpha) \\cdot r(S_0, L) + \\alpha \\cdot E_1(T_L) $$\n    $$ Q(S_0, L; \\alpha) = (1-\\alpha) \\cdot 10 + \\alpha \\cdot 0 $$\n    $$ Q(S_0, L; \\alpha) = 10(1-\\alpha) $$\n\n2.  **动作 $R$ 的评估**：从 $S_0$ 采取动作 $R$ 会导向状态 $R_1$，并提供即时奖励 $r(S_0, R) = 1$。评估值为：\n    $$ Q(S_0, R; \\alpha) = (1-\\alpha) \\cdot r(S_0, R) + \\alpha \\cdot E_1(R_1) $$\n    $$ Q(S_0, R; \\alpha) = (1-\\alpha) \\cdot 1 + \\alpha \\cdot \\log(2) $$\n\n**步骤 3：玻尔兹曼选择概率的推导**\n\n有限理性通过玻尔兹曼决策规则建模，其中选择一个动作的概率与其指数化的评估值成正比，并由一个温度参数 $\\tau  0$ 进行缩放。在 $S_0$ 处选择动作 $R$ 的概率为：\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{\\exp(Q(S_0, R; \\alpha) / \\tau)}{\\exp(Q(S_0, L; \\alpha) / \\tau) + \\exp(Q(S_0, R; \\alpha) / \\tau)} $$\n\n通过将分子和分母同除以 $\\exp(Q(S_0, R; \\alpha) / \\tau)$，这可以更紧凑地使用 logistic sigmoid 函数表示：\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{1}{1 + \\exp\\left(\\frac{Q(S_0, L; \\alpha) - Q(S_0, R; \\alpha)}{\\tau}\\right)} $$\n\n我们计算评估值之差，$\\Delta Q(\\alpha) = Q(S_0, L; \\alpha) - Q(S_0, R; \\alpha)$：\n\n$$ \\Delta Q(\\alpha) = 10(1-\\alpha) - \\left[ (1-\\alpha) + \\alpha \\log(2) \\right] $$\n$$ \\Delta Q(\\alpha) = 10 - 10\\alpha - 1 + \\alpha - \\alpha \\log(2) $$\n$$ \\Delta Q(\\alpha) = 9 - 9\\alpha - \\alpha \\log(2) $$\n$$ \\Delta Q(\\alpha) = 9 - \\alpha(9 + \\log(2)) $$\n\n将此代入概率公式，得到选择动作 $R$ 的概率的最终表达式：\n\n$$ P(R|S_0; \\alpha, \\tau) = \\frac{1}{1 + \\exp\\left(\\frac{9 - \\alpha(9 + \\log(2))}{\\tau}\\right)} $$\n\n该公式将被用于实现，以计算给定测试用例的结果。作为检验，我们注意到决策无差异（$P(R)=0.5$）发生在指数为零时，这意味着 $\\Delta Q(\\alpha) = 0$。解方程 $9 - \\alpha(9 + \\log(2)) = 0$ 可得 $\\alpha = \\frac{9}{9 + \\log(2)}$，这与测试用例中给出的值 $\\alpha^{\\star}$ 相匹配，从而证实了我们推导模型的正确性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Boltzmann choice probability for action 'R' at state S0\n    for a set of test cases involving a trade-off between extrinsic reward\n    and intrinsic empowerment.\n    \"\"\"\n    # Pre-calculate log(2) as it's used repeatedly.\n    # The problem specifies natural logarithms.\n    log_2 = np.log(2)\n\n    # The special value of alpha for the indifference case, derived from\n    # setting the evaluations Q(L) and Q(R) to be equal.\n    # alpha_star = 9 / (9 + log(2))\n    alpha_star = 9.0 / (9.0 + log_2)\n\n    # Define the test cases from the problem statement as (alpha, tau).\n    test_cases = [\n        (0.3, 0.5),                # Case 1 (happy path)\n        (0.0, 0.1),                # Case 2 (extrinsic-only boundary)\n        (1.0, 0.1),                # Case 3 (intrinsic-only boundary)\n        (alpha_star, 0.2),         # Case 4 (decision indifference boundary)\n        (0.5, 10.0),               # Case 5 (high-noise edge)\n    ]\n\n    results = []\n    # Loop through each test case to compute the probability.\n    for alpha, tau in test_cases:\n        # Calculate the difference in evaluations, Delta Q = Q(L) - Q(R).\n        # Based on the derived formula:\n        # Delta Q = 9 - alpha * (9 + log(2))\n        delta_q = 9.0 - alpha * (9.0 + log_2)\n\n        # Calculate the probability of choosing action R using the logistic function form\n        # of the Boltzmann (softmax) rule for two actions.\n        # P(R) = 1 / (1 + exp((Q(L) - Q(R)) / tau))\n        # This form is numerically stable, especially for large exponents.\n        prob_r = 1.0 / (1.0 + np.exp(delta_q / tau))\n        \n        results.append(prob_r)\n\n    # Format the output as a comma-separated list of floats inside square brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Print the final result in the exact required format.\n    print(output_str)\n\n# Execute the main function.\nsolve()\n```"
        }
    ]
}