{
    "hands_on_practices": [
        {
            "introduction": "This practice explores Herbert Simon's foundational concept of \"satisficing,\" a cornerstone of bounded rationality. Rather than assuming agents perform exhaustive optimization, this model posits that they set an aspiration level and accept the first option that meets it. This exercise () challenges you to translate this behavioral heuristic into a formal mathematical problem, deriving the optimal aspiration level that maximizes expected payoff in a sequential search task.",
            "id": "4114236",
            "problem": "Consider a sequential search process in a complex adaptive environment populated by heterogeneous options. An agent with bounded rationality adopts a satisficing rule: she sets an aspiration threshold $\\alpha$ and accepts the first encountered option whose value $X$ exceeds $\\alpha$. Each option is independently and identically distributed (i.i.d.) with distribution $X \\sim \\text{Exponential}(\\lambda)$, where the rate parameter $\\lambda>0$ is known to the agent. Every evaluation of an option, including the accepted one, incurs a fixed search cost $c>0$. The agent aims to maximize her expected net payoff, defined as the expected value of the accepted option minus the expected cumulative search cost. Assume the agent evaluates options until one meets or exceeds the aspiration level $\\alpha$, and then stops.\n\nWorking from first principles of probability and expected values for i.i.d. sampling and exponential distributions, derive the expression for the expected net payoff as a function of $\\alpha$, and compute the optimal aspiration level $\\alpha^{\\ast}$ that maximizes the expected net payoff. Assume $0<\\lambda c<1$, ensuring the existence of an interior optimum. Express your final answer as a closed-form analytic expression. No rounding is required.",
            "solution": "The problem asks for the derivation of the expected net payoff for a satisficing agent and the optimal aspiration level $\\alpha^{\\ast}$ that maximizes this payoff. The problem is well-posed and scientifically grounded in the theory of optimal stopping and search.\n\nFirst, we define the components of the agent's expected net payoff, which we denote by $J(\\alpha)$. The payoff is the value of the accepted option minus the cumulative search cost.\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)]$$\nwhere $V(\\alpha)$ is the value of the accepted option when the aspiration is $\\alpha$, and $C(\\alpha)$ is the total search cost.\n\nThe agent inspects options with values $X$ drawn independently from an exponential distribution with rate parameter $\\lambda > 0$. The probability density function (PDF) is $f(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$, and the cumulative distribution function (CDF) is $F(x) = 1 - \\exp(-\\lambda x)$ for $x \\ge 0$.\n\nThe agent stops and accepts an option if its value $X$ meets or exceeds the aspiration level $\\alpha$. The problem text contains a minor ambiguity between \"exceeds $\\alpha$\" and \"meets or exceeds $\\alpha$\". However, for a continuous distribution like the exponential distribution, the probability of obtaining a value exactly equal to $\\alpha$ is zero, i.e., $P(X = \\alpha) = 0$. Therefore, $P(X > \\alpha) = P(X \\ge \\alpha)$, and the ambiguity is inconsequential. We proceed by using the condition $X \\ge \\alpha$.\n\nLet $p$ be the probability of a single search being successful, i.e., the probability that an option's value meets or exceeds $\\alpha$.\n$$p = P(X \\ge \\alpha) = 1 - P(X < \\alpha) = 1 - F(\\alpha) = 1 - (1 - \\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\nAlternatively, we can compute this via integration:\n$$p = \\int_{\\alpha}^{\\infty} \\lambda \\exp(-\\lambda x) dx = \\left[ -\\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} = 0 - (-\\exp(-\\lambda \\alpha)) = \\exp(-\\lambda \\alpha)$$\n\nThe number of searches, let's call it $N$, follows a geometric distribution with success probability $p$. The agent searches until the first success. The expected number of searches is:\n$$E[N] = \\frac{1}{p} = \\frac{1}{\\exp(-\\lambda \\alpha)} = \\exp(\\lambda \\alpha)$$\n\nEach search incurs a cost $c > 0$. The expected total search cost, $E[C(\\alpha)]$, is the cost per search multiplied by the expected number of searches:\n$$E[C(\\alpha)] = c \\cdot E[N] = c \\exp(\\lambda \\alpha)$$\n\nNext, we determine the expected value of the accepted option, $E[V(\\alpha)]$. This is the conditional expectation of $X$, given that $X \\ge \\alpha$.\n$$E[V(\\alpha)] = E[X | X \\ge \\alpha] = \\frac{\\int_{\\alpha}^{\\infty} x f(x) dx}{P(X \\ge \\alpha)}$$\nWe have already calculated the denominator $P(X \\ge \\alpha) = \\exp(-\\lambda \\alpha)$. We now compute the numerator using integration by parts: $\\int u dv = uv - \\int v du$. Let $u = x$ and $dv = \\lambda \\exp(-\\lambda x) dx$. Then $du = dx$ and $v = -\\exp(-\\lambda x)$.\n\\begin{align*} \\int_{\\alpha}^{\\infty} x \\lambda \\exp(-\\lambda x) dx &= \\left[ -x \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} - \\int_{\\alpha}^{\\infty} (-\\exp(-\\lambda x)) dx \\\\ &= \\left( \\lim_{x \\to \\infty} -x \\exp(-\\lambda x) - (-\\alpha \\exp(-\\lambda \\alpha)) \\right) + \\int_{\\alpha}^{\\infty} \\exp(-\\lambda x) dx \\\\ &= (0 + \\alpha \\exp(-\\lambda \\alpha)) + \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda x) \\right]_{\\alpha}^{\\infty} \\\\ &= \\alpha \\exp(-\\lambda \\alpha) + \\left( 0 - (-\\frac{1}{\\lambda} \\exp(-\\lambda \\alpha)) \\right) \\\\ &= \\alpha \\exp(-\\lambda \\alpha) + \\frac{1}{\\lambda} \\exp(-\\lambda \\alpha) \\\\ &= \\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha) \\end{align*}\nNow, we can find the conditional expectation:\n$$E[V(\\alpha)] = \\frac{\\left(\\alpha + \\frac{1}{\\lambda}\\right) \\exp(-\\lambda \\alpha)}{\\exp(-\\lambda \\alpha)} = \\alpha + \\frac{1}{\\lambda}$$\nThis result elegantly reflects the memoryless property of the exponential distribution.\n\nCombining the components, the expected net payoff function $J(\\alpha)$ is:\n$$J(\\alpha) = E[V(\\alpha)] - E[C(\\alpha)] = \\left(\\alpha + \\frac{1}{\\lambda}\\right) - c \\exp(\\lambda \\alpha)$$\nTo find the optimal aspiration level $\\alpha^{\\ast}$ that maximizes $J(\\alpha)$, we compute the first derivative with respect to $\\alpha$ and set it to zero.\n$$\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\left(\\alpha + \\frac{1}{\\lambda} - c \\exp(\\lambda \\alpha)\\right) = 1 - c \\lambda \\exp(\\lambda \\alpha)$$\nSetting the derivative to zero:\n$$1 - c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 0$$\n$$c \\lambda \\exp(\\lambda \\alpha^{\\ast}) = 1$$\n$$\\exp(\\lambda \\alpha^{\\ast}) = \\frac{1}{c \\lambda}$$\nTaking the natural logarithm of both sides:\n$$\\lambda \\alpha^{\\ast} = \\ln\\left(\\frac{1}{c \\lambda}\\right) = -\\ln(c \\lambda)$$\nSolving for $\\alpha^{\\ast}$:\n$$\\alpha^{\\ast} = -\\frac{1}{\\lambda}\\ln(c \\lambda)$$\nThe problem provides the constraint $0 < \\lambda c < 1$. This ensures that $\\ln(\\lambda c)$ is negative, which in turn guarantees that the optimal aspiration level $\\alpha^{\\ast}$ is positive.\n\nTo confirm that this critical point corresponds to a maximum, we examine the second derivative of $J(\\alpha)$:\n$$\\frac{d^2J}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left(1 - c \\lambda \\exp(\\lambda \\alpha)\\right) = -c \\lambda^2 \\exp(\\lambda \\alpha)$$\nGiven that $c > 0$ and $\\lambda > 0$, the exponential term $\\exp(\\lambda \\alpha)$ is always positive. Therefore, $\\frac{d^2J}{d\\alpha^2}$ is always negative. This confirms that the payoff function $J(\\alpha)$ is strictly concave, and our solution $\\alpha^{\\ast}$ is indeed the unique global maximum.",
            "answer": "$$ \\boxed{-\\frac{1}{\\lambda} \\ln(\\lambda c)} $$"
        },
        {
            "introduction": "Moving from individual decisions to strategic interactions, we now consider how bounded rationality affects outcomes in games. While classical game theory assumes perfect rationality, the Quantal Response Equilibrium (QRE) model provides a more behaviorally realistic alternative by allowing for probabilistic errors in decision-making. This exercise () guides you through the derivation of a logit QRE, illustrating how a \"rationality parameter\" $\\lambda$ governs the transition from random choice to perfect rationality in a multi-agent system.",
            "id": "4114252",
            "problem": "Consider two agents, Player $1$ and Player $2$, interacting in a $2 \\times 2$ normal-form game with Player $1$'s actions $\\{X,Y\\}$ and Player $2$'s actions $\\{L,R\\}$. The payoff matrices are defined by $u_{1}(X,L)=5$, $u_{1}(X,R)=5$, $u_{1}(Y,L)=2$, $u_{1}(Y,R)=2$ for Player $1$, and $u_{2}(L,X)=1$, $u_{2}(R,X)=3$, $u_{2}(L,Y)=1$, $u_{2}(R,Y)=3$ for Player $2$. Assume each agent exhibits bounded rationality according to the logit Quantal Response Equilibrium (QRE), where each agent $i$ chooses action $a$ with probability proportional to $\\exp(\\lambda \\,\\mathbb{E}[u_{i}(a)])$, with $\\lambda>0$ denoting the rationality parameter and $\\mathbb{E}[u_{i}(a)]$ the expected utility of action $a$ given beliefs about the other agent's mixed strategy. A Quantal Response Equilibrium (QRE) is defined as a fixed point in which each agent's mixed strategy coincides with the logit response to the expected utilities induced by the other agent's mixed strategy.\n\nStarting from the core definitions of expected utility and the logit choice model under bounded rationality, derive the fixed-point conditions for the mixed strategies in this $2 \\times 2$ game and solve them. Express the equilibrium probability that Player $1$ chooses action $X$ as a closed-form function of $\\lambda$. Provide your final answer as a single simplified analytic expression. No rounding is required.",
            "solution": "The problem statement will first be validated for scientific soundness, completeness, and objectivity.\n\n### Step 1: Extract Givens\n- **Game Structure**: A $2 \\times 2$ normal-form game.\n- **Players and Actions**: Player $1$ with actions $\\{X,Y\\}$ and Player $2$ with actions $\\{L,R\\}$.\n- **Payoff Matrix for Player 1 ($u_1$)**:\n  - $u_{1}(X,L)=5$\n  - $u_{1}(X,R)=5$\n  - $u_{1}(Y,L)=2$\n  - $u_{1}(Y,R)=2$\n- **Payoff Matrix for Player 2 ($u_2$)**:\n  - $u_{2}(L,X)=1$\n  - $u_{2}(R,X)=3$\n  - $u_{2}(L,Y)=1$\n  - $u_{2}(R,Y)=3$\n- **Behavioral Model**: Logit Quantal Response Equilibrium (QRE).\n- **Choice Probability Rule**: For agent $i$, the probability of choosing action $a$ is proportional to $\\exp(\\lambda \\,\\mathbb{E}[u_{i}(a)])$, where $\\lambda > 0$ is the rationality parameter and $\\mathbb{E}[u_{i}(a)]$ is the expected utility of action $a$.\n- **Equilibrium Condition**: A QRE is a fixed point where each agent's mixed strategy coincides with the logit response to the expected utilities induced by the other agent's mixed strategy.\n- **Objective**: Derive the equilibrium probability that Player $1$ chooses action $X$ as a closed-form function of $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the concept of Quantal Response Equilibrium, a well-established and widely used model in behavioral game theory and economics for analyzing strategic interactions under bounded rationality. The logit specification is the most common form of QRE. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information (payoffs, behavioral model) to define a system of equations for the equilibrium strategies. It asks for a specific quantity (an equilibrium probability) as a function of a given parameter. The existence of a QRE is guaranteed by Brouwer's fixed-point theorem, so a solution exists and the problem is well-posed.\n- **Objective**: The problem is stated using formal mathematical and game-theoretic language, free of subjective or ambiguous terminology.\n- **Completeness and Consistency**: The payoff structure is fully defined for both players and all action profiles. The rationality model is explicitly given. There are no contradictions in the provided information.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, objective, and complete. A solution will now be derived.\n\nLet $p$ be the probability that Player $1$ chooses action $X$. Then the probability of choosing action $Y$ is $1-p$. The mixed strategy for Player $1$ is $\\sigma_1 = (p, 1-p)$.\nLet $q$ be the probability that Player $2$ chooses action $L$. Then the probability of choosing action $R$ is $1-q$. The mixed strategy for Player $2$ is $\\sigma_2 = (q, 1-q)$.\n\nFirst, we calculate the expected utility for each action for Player $1$, given Player $2$'s mixed strategy $\\sigma_2$.\nThe expected utility for Player $1$ of choosing action $X$ is:\n$$\n\\mathbb{E}[u_{1}(X)] = q \\cdot u_{1}(X,L) + (1-q) \\cdot u_{1}(X,R) = q \\cdot 5 + (1-q) \\cdot 5 = 5\n$$\nThe expected utility for Player $1$ of choosing action $Y$ is:\n$$\n\\mathbb{E}[u_{1}(Y)] = q \\cdot u_{1}(Y,L) + (1-q) \\cdot u_{1}(Y,R) = q \\cdot 2 + (1-q) \\cdot 2 = 2\n$$\nNotice that for Player $1$, the expected utility of each action is independent of Player $2$'s strategy $q$. This is a specific feature of the given payoff matrix, where $u_1(X,L) = u_1(X,R)$ and $u_1(Y,L) = u_1(Y,R)$.\n\nAccording to the logit QRE model, the probability that Player $1$ chooses action $X$ is given by the logistic choice formula:\n$$\nP_1(X) = \\frac{\\exp(\\lambda \\, \\mathbb{E}[u_{1}(X)])}{\\exp(\\lambda \\, \\mathbb{E}[u_{1}(X)]) + \\exp(\\lambda \\, \\mathbb{E}[u_{1}(Y)])}\n$$\nIn a QRE, this probability must be equal to the agent's own mixed strategy probability, so $p = P_1(X)$.\nSubstituting the calculated expected utilities:\n$$\np = \\frac{\\exp(\\lambda \\cdot 5)}{\\exp(\\lambda \\cdot 5) + \\exp(\\lambda \\cdot 2)}\n$$\nThis equation directly gives the equilibrium probability $p$ for Player $1$ as a function of $\\lambda$. It does not depend on Player $2$'s strategy, so we do not need to solve a system of coupled equations. We only need to simplify this expression.\n\nTo simplify, we can factor out $\\exp(2\\lambda)$ from the numerator and denominator, or equivalently, multiply the numerator and denominator by $\\exp(-2\\lambda)$:\n$$\np = \\frac{\\exp(5\\lambda) \\cdot \\exp(-2\\lambda)}{\\exp(5\\lambda) \\cdot \\exp(-2\\lambda) + \\exp(2\\lambda) \\cdot \\exp(-2\\lambda)}\n$$\n$$\np = \\frac{\\exp(5\\lambda - 2\\lambda)}{\\exp(5\\lambda - 2\\lambda) + \\exp(2\\lambda - 2\\lambda)}\n$$\n$$\np = \\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + \\exp(0)}\n$$\n$$\np = \\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + 1}\n$$\nThis is the closed-form expression for the equilibrium probability that Player $1$ chooses action $X$.\n\nFor completeness, although not required by the problem, we can find the equilibrium probability $q$ for Player $2$ in the same manner.\nThe expected utility for Player $2$ of choosing action $L$ is:\n$$\n\\mathbb{E}[u_{2}(L)] = p \\cdot u_{2}(L,X) + (1-p) \\cdot u_{2}(L,Y) = p \\cdot 1 + (1-p) \\cdot 1 = 1\n$$\nThe expected utility for Player $2$ of choosing action $R$ is:\n$$\n\\mathbb{E}[u_{2}(R)] = p \\cdot u_{2}(R,X) + (1-p) \\cdot u_{2}(R,Y) = p \\cdot 3 + (1-p) \\cdot 3 = 3\n$$\nThe probability that Player $2$ chooses action $L$ is:\n$$\nq = \\frac{\\exp(\\lambda \\, \\mathbb{E}[u_{2}(L)])}{\\exp(\\lambda \\, \\mathbb{E}[u_{2}(L)]) + \\exp(\\lambda \\, \\mathbb{E}[u_{2}(R)])} = \\frac{\\exp(\\lambda \\cdot 1)}{\\exp(\\lambda \\cdot 1) + \\exp(\\lambda \\cdot 3)} = \\frac{\\exp(\\lambda)}{\\exp(\\lambda) + \\exp(3\\lambda)}\n$$\nSimplifying gives $q = \\frac{1}{1 + \\exp(2\\lambda)}$. The equilibrium is the pair $(p, q)$. The question, however, only asks for the expression for $p$.\nThe final expression for the probability that Player $1$ chooses action $X$ is a function solely of the rationality parameter $\\lambda$.",
            "answer": "$$\\boxed{\\frac{\\exp(3\\lambda)}{\\exp(3\\lambda) + 1}}$$"
        },
        {
            "introduction": "An essential feature of agency in complex adaptive systems is the ability to learn and adapt from experience. However, a significant challenge arises because an agent's actions influence the data it observes, creating selection bias that can corrupt learning. This advanced practice () uses a formal causal inference framework to quantify this bias and demonstrates how a powerful statistical technique, the Doubly Robust estimator, can provide an unbiased estimate of an action's value.",
            "id": "4114212",
            "problem": "Consider a population of agents embedded in a complex adaptive system. Each agent must evaluate the expected value of taking a fixed action $a$ under a post-change policy. Due to bounded rationality, the agent combines realized outcomes with model-based counterfactual predictions for contexts where action $a$ was not realized. The agent’s learning rule assigns weight $\\,\\alpha \\in [0,1]\\,$ to realized outcomes and weight $\\,1-\\alpha\\,$ to model-based counterfactuals.\n\nFundamental base:\n- Use the Neyman–Rubin potential outcomes framework and the law of total expectation.\n- Let the context be a discrete random variable $\\,X \\in \\mathcal{X} = \\{x_1, x_2, x_3\\}\\,$ with distribution $\\,p(x_i) = p_i\\,$ for $\\,i \\in \\{1,2,3\\}\\,$ and $\\,\\sum_{i=1}^3 p_i = 1\\,$.\n- Let $\\,Y(a)\\,$ denote the potential outcome under action $\\,a\\,$ and define the conditional mean $\\,\\mu_a(x) = \\mathbb{E}[Y(a) \\mid X=x]$.\n- After a policy change, the behavior policy is $\\,\\pi_1(a \\mid x)\\,$, the probability of taking action $\\,a\\,$ in context $\\,x\\,$. Assume $\\,0 < \\pi_1(a \\mid x) \\le 1\\,$ for all $\\,x \\in \\mathcal{X}\\,$.\n- The agent uses a predictive model $\\,\\hat{\\mu}_a(x)\\,$ for counterfactuals with model error $\\,\\epsilon_a(x) = \\hat{\\mu}_a(x) - \\mu_a(x)\\,$.\n\nTarget quantity:\n- The true expected value of action $\\,a\\,$ is\n$$\nV(a) = \\mathbb{E}_X[\\mu_a(X)] = \\sum_{i=1}^3 p_i \\,\\mu_a(x_i).\n$$\n\nBounded-rational agent’s estimator:\n- The agent forms, per context, the mixture\n$$\ng_a(X) \\;=\\; \\alpha \\cdot \\mathbb{I}\\{A=a\\} \\cdot Y \\;+\\; (1-\\alpha) \\cdot \\hat{\\mu}_a(X),\n$$\nwhere $\\,A\\,$ is the action drawn from $\\,\\pi_1(a \\mid X)\\,$ after the policy change, $\\,Y = Y(A)\\,$ is the realized outcome, and $\\,\\mathbb{I}\\{\\cdot\\}\\,$ denotes the indicator function.\n- The agent’s estimate of $\\,V(a)\\,$ is $\\,\\hat{V}_{\\text{mix}}(a) = \\mathbb{E}[g_a(X)]\\,$. Define the bias as\n$$\n\\text{Bias}_{\\text{mix}}(a) \\;=\\; \\mathbb{E}[g_a(X)] - V(a).\n$$\n\nCorrection method (to be derived and implemented):\n- Use Inverse Probability Weighting (IPW) and Doubly Robust (DR) estimation. The DR estimator is\n$$\n\\hat{V}_{\\text{DR}}(a) \\;=\\; \\mathbb{E}\\!\\left[\\hat{\\mu}_a(X) \\;+\\; \\frac{\\mathbb{I}\\{A=a\\}}{\\pi_1(a \\mid X)} \\left( Y - \\hat{\\mu}_a(X) \\right)\\right].\n$$\n- Define the corrected bias as\n$$\n\\text{Bias}_{\\text{DR}}(a) \\;=\\; \\hat{V}_{\\text{DR}}(a) - V(a).\n$$\n\nYour task:\n1. Starting from the fundamental base above and the law of total expectation, derive a closed-form expression for $\\,\\text{Bias}_{\\text{mix}}(a)\\,$ in terms of $\\,p_i\\,$, $\\,\\mu_a(x_i)\\,$, $\\,\\pi_1(a \\mid x_i)\\,$, $\\,\\epsilon_a(x_i)\\,$, and $\\,\\alpha\\,$.\n2. Show that the DR estimator eliminates the bias under the stated assumptions, and provide a computable expression for $\\,\\text{Bias}_{\\text{DR}}(a)\\,$.\n3. Implement a program that, for the parameter sets specified in the test suite, computes $\\,\\text{Bias}_{\\text{mix}}(a)\\,$ and $\\,\\text{Bias}_{\\text{DR}}(a)\\,$. Each result must be returned as a floating point number.\n\nTest suite:\n- Use the following fixed context probabilities and potential outcomes for action $\\,a\\,$ across all test cases:\n$$\np = (0.2,\\,0.5,\\,0.3), \\quad \\mu_a(x) \\text{ represented by } \\mu = (1.0,\\,0.5,\\,-0.2).\n$$\n- For each test case $\\,T_k\\,$, you are given $\\,\\alpha\\,$, $\\,\\pi_1(a \\mid x_i)\\,$, and $\\,\\epsilon_a(x_i)\\,$:\n    - $T_1$: $\\,\\alpha = 0.6\\,$, $\\,\\pi_1 = (0.7,\\,0.2,\\,0.5)\\,$, $\\,\\epsilon = (0.1,\\,-0.05,\\,0.0)$.\n    - $T_2$: $\\,\\alpha = 0.6\\,$, $\\,\\pi_1 = (0.7,\\,0.2,\\,0.5)\\,$, $\\,\\epsilon = (0.0,\\,0.0,\\,0.0)$.\n    - $T_3$: $\\,\\alpha = 0.8\\,$, $\\,\\pi_1 = (10^{-4},\\,10^{-2},\\,0.2)\\,$, $\\,\\epsilon = (0.2,\\,-0.2,\\,0.1)$.\n    - $T_4$: $\\,\\alpha = 0.0\\,$, $\\,\\pi_1 = (0.3,\\,0.3,\\,0.3)\\,$, $\\,\\epsilon = (-0.3,\\,0.1,\\,0.05)$.\n    - $T_5$: $\\,\\alpha = 1.0\\,$, $\\,\\pi_1 = (0.6,\\,0.4,\\,0.9)\\,$, $\\,\\epsilon = (0.2,\\,-0.1,\\,0.0)$.\n\nOutput specification:\n- Your program should produce a single line of output containing the $\\,10\\,$ computed results as a comma-separated list enclosed in square brackets, in the following order: first $\\,\\text{Bias}_{\\text{mix}}(a)\\,$ for $\\,T_1$ through $\\,T_5$, then $\\,\\text{Bias}_{\\text{DR}}(a)\\,$ for $\\,T_1$ through $\\,T_5$.\n- Express each number as a float rounded to six decimal places.\n- Example format (with placeholder numbers): $[b_1,b_2,b_3,b_4,b_5,c_1,c_2,c_3,c_4,c_5]$.",
            "solution": "We begin by formalizing the learning process under bounded rationality and selection induced by policy change. The law of total expectation and the Neyman–Rubin framework provide the foundational tools.\n\nDefinitions and setup:\n- Let $\\,X \\in \\{x_1,x_2,x_3\\}\\,$ with $\\,\\mathbb{P}(X=x_i) = p_i\\,$ and $\\,\\sum_i p_i = 1\\,$.\n- Let $\\,Y(a)\\,$ be the potential outcome under action $\\,a\\,$ and $\\,\\mu_a(x) = \\mathbb{E}[Y(a) \\mid X=x]\\,$.\n- After the policy change, the action $\\,A\\,$ is drawn according to $\\,\\pi_1(a \\mid x) = \\mathbb{P}(A=a \\mid X=x)\\,$. Outcomes $\\,Y\\,$ are observed only for the realized action, i.e., $\\,Y = Y(A)\\,$.\n- The agent’s predictive model produces $\\,\\hat{\\mu}_a(x)\\,$ with error $\\,\\epsilon_a(x) = \\hat{\\mu}_a(x) - \\mu_a(x)\\,$.\n- The agent’s mixture rule is\n$$\ng_a(X) \\;=\\; \\alpha \\cdot \\mathbb{I}\\{A=a\\} \\cdot Y \\;+\\; (1-\\alpha) \\cdot \\hat{\\mu}_a(X),\n$$\nwith $\\,\\alpha \\in [0,1]\\,$ capturing bounded rationality: $\\,\\alpha\\,$ is the behavioral weight placed on realized outcomes, while $\\,1-\\alpha\\,$ is the weight on model-based counterfactuals.\n\nTrue value:\n$$\nV(a) \\;=\\; \\mathbb{E}_X[\\mu_a(X)] \\;=\\; \\sum_{i=1}^3 p_i \\,\\mu_a(x_i).\n$$\n\nDerivation of the bias of the mixture estimator:\nCompute $\\,\\hat{V}_{\\text{mix}}(a) = \\mathbb{E}[g_a(X)]\\,$. Conditioning on $\\,X\\,$ and using the law of total expectation,\n\n$$\n\\mathbb{E}[g_a(X)] \\;=\\; \\mathbb{E}_X\\!\\left[ \\alpha \\cdot \\mathbb{E}\\!\\left[\\mathbb{I}\\{A=a\\} \\cdot Y \\mid X\\right] \\;+\\; (1-\\alpha) \\cdot \\mathbb{E}\\!\\left[\\hat{\\mu}_a(X) \\mid X\\right] \\right].\n$$\n\nSince $\\,\\mathbb{E}[\\mathbb{I}\\{A=a\\} \\cdot Y \\mid X=x] = \\pi_1(a \\mid x) \\cdot \\mathbb{E}[Y(a) \\mid X=x] = \\pi_1(a \\mid x) \\cdot \\mu_a(x)\\,$ and $\\,\\mathbb{E}[\\hat{\\mu}_a(X) \\mid X=x] = \\hat{\\mu}_a(x) = \\mu_a(x) + \\epsilon_a(x)\\,$, we obtain\n\n$$\n\\mathbb{E}[g_a(X)] \\;=\\; \\mathbb{E}_X\\!\\left[ \\alpha \\cdot \\pi_1(a \\mid X) \\cdot \\mu_a(X) \\;+\\; (1-\\alpha) \\cdot \\left(\\mu_a(X) + \\epsilon_a(X)\\right) \\right].\n$$\n\nSubtracting $\\,V(a) = \\mathbb{E}_X[\\mu_a(X)]\\,$, the bias is\n\n$$\n\\text{Bias}_{\\text{mix}}(a) \\;=\\; \\mathbb{E}_X\\!\\left[ \\mu_a(X) \\left( \\alpha \\cdot \\pi_1(a \\mid X) + (1-\\alpha) - 1 \\right) \\;+\\; (1-\\alpha) \\cdot \\epsilon_a(X) \\right].\n$$\n\nSimplifying,\n\n$$\n\\text{Bias}_{\\text{mix}}(a) \\;=\\; \\mathbb{E}_X\\!\\left[ -\\alpha \\cdot (1 - \\pi_1(a \\mid X)) \\cdot \\mu_a(X) \\;+\\; (1-\\alpha) \\cdot \\epsilon_a(X) \\right].\n$$\n\nIn discrete form over $\\,\\{x_1,x_2,x_3\\}\\,$,\n\n$$\n\\text{Bias}_{\\text{mix}}(a) \\;=\\; \\sum_{i=1}^3 p_i \\left( -\\alpha \\cdot \\big(1 - \\pi_1(a \\mid x_i)\\big) \\cdot \\mu_a(x_i) \\;+\\; (1-\\alpha) \\cdot \\epsilon_a(x_i) \\right).\n$$\n\nThis expression decomposes the bias into two components: a selection-induced attenuation term $\\, -\\alpha \\cdot (1 - \\pi_1(a \\mid X)) \\cdot \\mu_a(X)\\,$ due to unrealized actions, and a model-induced error term $\\, (1-\\alpha) \\cdot \\epsilon_a(X)\\,$ due to counterfactual prediction bias.\n\nCorrection via Doubly Robust estimation:\nConsider the Doubly Robust (DR) estimator\n\n$$\n\\hat{V}_{\\text{DR}}(a) \\;=\\; \\mathbb{E}\\!\\left[\\hat{\\mu}_a(X) \\;+\\; \\frac{\\mathbb{I}\\{A=a\\}}{\\pi_1(a \\mid X)} \\left( Y - \\hat{\\mu}_a(X) \\right)\\right].\n$$\n\nCondition on $\\,X\\,$ and take the expectation over $\\,A\\,$ and $\\,Y\\,$:\n\n$$\n\\mathbb{E}\\!\\left[\\hat{\\mu}_a(X) \\;+\\; \\frac{\\mathbb{I}\\{A=a\\}}{\\pi_1(a \\mid X)} \\left( Y - \\hat{\\mu}_a(X) \\right) \\,\\Big|\\, X \\right]\n= \\hat{\\mu}_a(X) \\;+\\; \\mathbb{E}\\!\\left[\\frac{\\mathbb{I}\\{A=a\\}}{\\pi_1(a \\mid X)} \\,\\Big|\\, X \\right] \\cdot \\left( \\mathbb{E}[Y(a) \\mid X] - \\hat{\\mu}_a(X) \\right).\n$$\n\nSince $\\,\\mathbb{E}[\\mathbb{I}\\{A=a\\} \\mid X] = \\pi_1(a \\mid X)\\,$, it follows that $\\,\\mathbb{E}\\!\\left[ \\frac{\\mathbb{I}\\{A=a\\}}{\\pi_1(a \\mid X)} \\,\\Big|\\, X \\right] = 1\\,$. Therefore,\n\n$$\n\\hat{V}_{\\text{DR}}(a) \\;=\\; \\mathbb{E}_X\\!\\left[ \\hat{\\mu}_a(X) + \\big( \\mu_a(X) - \\hat{\\mu}_a(X) \\big) \\right] \\;=\\; \\mathbb{E}_X[\\mu_a(X)] \\;=\\; V(a).\n$$\n\nThus,\n\n$$\n\\text{Bias}_{\\text{DR}}(a) \\;=\\; \\hat{V}_{\\text{DR}}(a) - V(a) \\;=\\; 0,\n$$\n\ndemonstrating that the DR estimator eliminates the bias introduced by learning from model-based counterfactuals that were unrealized due to policy changes, under the stated positivity assumption $\\,0 < \\pi_1(a \\mid x) \\le 1\\,$.\n\nAlgorithmic design for the program:\n- For each test case, compute the mixture bias using\n\n$$\n\\text{Bias}_{\\text{mix}}(a) \\;=\\; \\sum_{i=1}^3 p_i \\left( -\\alpha \\cdot \\big(1 - \\pi_{1,i}\\big) \\cdot \\mu_i \\;+\\; (1-\\alpha) \\cdot \\epsilon_i \\right),\n$$\n\nwhere $\\,\\pi_{1,i} = \\pi_1(a \\mid x_i)\\,$, $\\,\\mu_i = \\mu_a(x_i)\\,$, and $\\,\\epsilon_i = \\epsilon_a(x_i)\\,$.\n- Compute the corrected bias as $\\,\\text{Bias}_{\\text{DR}}(a) = 0\\,$ for each test case, consistent with the derivation:\n\n$$\n\\hat{V}_{\\text{DR}}(a) \\;=\\; \\sum_{i=1}^3 p_i \\,\\mu_i \\quad \\Rightarrow \\quad \\text{Bias}_{\\text{DR}}(a) = \\hat{V}_{\\text{DR}}(a) - V(a) = 0.\n$$\n\n- Aggregate outputs into a single list: first five $\\,\\text{Bias}_{\\text{mix}}(a)\\,$ values for $\\,T_1,\\dots,T_5\\,$, then five $\\,\\text{Bias}_{\\text{DR}}(a)\\,$ values for $\\,T_1,\\dots,T_5\\,$. Each value is a float rounded to six decimal places, printed in the specified format.\n\nScientific realism and edge cases:\n- The positivity condition $\\,0 < \\pi_1(a \\mid x) \\le 1\\,$ avoids division-by-zero in the DR estimator. The test suite includes small but positive propensities (e.g., $\\,10^{-4}\\,$) to reflect rare action realization scenarios.\n- Boundary conditions include $\\,\\alpha = 0\\,$ (agent relies entirely on counterfactual predictions, yielding $\\,\\text{Bias}_{\\text{mix}}(a) = \\mathbb{E}_X[\\epsilon_a(X)]\\,$), and $\\,\\alpha = 1\\,$ (agent relies entirely on realized outcomes, yielding selection-induced bias $\\, -\\mathbb{E}_X[(1-\\pi_1(a \\mid X))\\mu_a(X)]\\,$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_mixture_bias(p, mu, pi1, eps, alpha):\n    \"\"\"\n    Compute Bias_mix(a) = sum_i p_i [ -(alpha)*(1 - pi1_i)*mu_i + (1 - alpha)*eps_i ].\n    Parameters are numpy arrays of equal length for p, mu, pi1, eps and a float alpha.\n    \"\"\"\n    p = np.asarray(p, dtype=float)\n    mu = np.asarray(mu, dtype=float)\n    pi1 = np.asarray(pi1, dtype=float)\n    eps = np.asarray(eps, dtype=float)\n    term_selection = -alpha * (1.0 - pi1) * mu\n    term_model = (1.0 - alpha) * eps\n    bias = np.sum(p * (term_selection + term_model))\n    return float(bias)\n\ndef compute_dr_bias():\n    \"\"\"\n    From the derivation, Bias_DR(a) = 0 under positivity.\n    We return 0.0 explicitly.\n    \"\"\"\n    return 0.0\n\ndef solve():\n    # Fixed context probabilities and potential outcomes for action a.\n    p = np.array([0.2, 0.5, 0.3], dtype=float)\n    mu = np.array([1.0, 0.5, -0.2], dtype=float)\n\n    # Test cases: each is a dict with alpha, pi1, eps\n    test_cases = [\n        # T1\n        {\n            \"alpha\": 0.6,\n            \"pi1\": np.array([0.7, 0.2, 0.5], dtype=float),\n            \"eps\": np.array([0.1, -0.05, 0.0], dtype=float),\n        },\n        # T2\n        {\n            \"alpha\": 0.6,\n            \"pi1\": np.array([0.7, 0.2, 0.5], dtype=float),\n            \"eps\": np.array([0.0, 0.0, 0.0], dtype=float),\n        },\n        # T3\n        {\n            \"alpha\": 0.8,\n            \"pi1\": np.array([1e-4, 1e-2, 0.2], dtype=float),\n            \"eps\": np.array([0.2, -0.2, 0.1], dtype=float),\n        },\n        # T4\n        {\n            \"alpha\": 0.0,\n            \"pi1\": np.array([0.3, 0.3, 0.3], dtype=float),\n            \"eps\": np.array([-0.3, 0.1, 0.05], dtype=float),\n        },\n        # T5\n        {\n            \"alpha\": 1.0,\n            \"pi1\": np.array([0.6, 0.4, 0.9], dtype=float),\n            \"eps\": np.array([0.2, -0.1, 0.0], dtype=float),\n        },\n    ]\n\n    results = []\n    # First, append Bias_mix for each test case\n    for case in test_cases:\n        bmix = compute_mixture_bias(p, mu, case[\"pi1\"], case[\"eps\"], case[\"alpha\"])\n        results.append(round(bmix, 6))\n\n    # Then, append Bias_DR (which is zero) for each test case\n    for _ in test_cases:\n        bdr = compute_dr_bias()\n        results.append(round(bdr, 6))\n\n    # Final print statement in the exact required format: single line, comma-separated list enclosed in brackets.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}