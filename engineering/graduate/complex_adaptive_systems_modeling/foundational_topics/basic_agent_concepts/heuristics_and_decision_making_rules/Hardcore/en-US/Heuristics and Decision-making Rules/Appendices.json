{
    "hands_on_practices": [
        {
            "introduction": "Agents in complex systems often face choices with multiple, conflicting objectives, and the decision rule they use is a critical component of their behavior. This practice contrasts two fundamental types of heuristics: compensatory rules like weighted aggregation, where a deficit in one criterion can be traded for a surplus in another, and non-compensatory rules like lexicographic ordering, which enforce strict priorities. By working through this problem , you will gain a deeper appreciation for the normative implications—such as continuity and the handling of trade-offs—that are embedded in the choice of a decision rule.",
            "id": "4125860",
            "problem": "Consider a population of adaptive agents in a complex environment who must choose between two strategies, denoted by $A$ and $B$. Their decision-making is guided by three criteria normalized to the unit interval $[0,1]$: safety $x_{s}$, timeliness $x_{t}$, and energy efficiency $x_{e}$, with larger values indicating better performance on each criterion. Two decision rules are employed in the population: a lexicographic rule with strict priority $x_{s} \\succ x_{t} \\succ x_{e}$, and an additive weighted aggregation rule with fixed, positive weights that sum to $1$. The criteria values for the two strategies are:\n- For $A$: $x_{s}(A) = 0.95$, $x_{t}(A) = 0.60$, $x_{e}(A) = 0.50$.\n- For $B$: $x_{s}(B) = 0.92$, $x_{t}(B) = 0.90$, $x_{e}(B) = 0.70$.\nThe additive aggregation weights are $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$.\n\nFrom first principles and core definitions of lexicographic ordering and additive aggregation, determine which strategy each rule selects and analyze the normative implications in terms of continuity, compensation, Pareto monotonicity, and scale dependence. Then, select all statements below that are correct.\n\nA. Under the lexicographic rule with priority $x_{s} \\succ x_{t} \\succ x_{e}$, $A$ is selected; under additive weighted aggregation with $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$, $B$ is selected. The disagreement is due to the compensatory nature of the weighted rule, which permits trade-offs on lower-priority criteria.\n\nB. The additive weighted aggregation outcome is invariant under any strictly increasing transformation applied separately to each criterion (for example, replacing $x_{t}$ by $f(x_{t})$ where $f$ is strictly increasing), provided the weights are unchanged.\n\nC. Lexicographic ordering violates the Archimedean continuity axiom; no finite improvement in $x_{t}$ or $x_{e}$ can compensate for a shortfall in $x_{s}$. This non-compensatory structure can be normatively justified in safety-critical domains where the highest-priority criterion encodes a hard constraint.\n\nD. Both rules respect Pareto dominance: if $A$ weakly dominates $B$ in all criteria and strictly dominates $B$ in at least one criterion, then both rules will select $A$.\n\nE. In a population where agents learn via Reinforcement Learning (RL) with identical, time-invariant information, the aggregate behavior necessarily converges to the same long-run distribution over $A$ and $B$ under both lexicographic and additive weighted aggregation rules, because both rules are monotonic in each criterion.\n\nSelect all correct options.",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   **Strategies**: Two strategies, denoted by $A$ and $B$.\n-   **Criteria**: Three criteria, normalized to the interval $[0,1]$: safety ($x_s$), timeliness ($x_t$), and energy efficiency ($x_e$).\n-   **Criteria Values for Strategy A**: $x_{s}(A) = 0.95$, $x_{t}(A) = 0.60$, $x_{e}(A) = 0.50$.\n-   **Criteria Values for Strategy B**: $x_{s}(B) = 0.92$, $x_{t}(B) = 0.90$, $x_{e}(B) = 0.70$.\n-   **Decision Rule 1**: Lexicographic rule with strict priority $x_{s} \\succ x_{t} \\succ x_{e}$.\n-   **Decision Rule 2**: Additive weighted aggregation rule with fixed, positive weights $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$. The weights are stated to sum to $1$, which is confirmed: $0.50 + 0.40 + 0.10 = 1.00$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. Lexicographic ordering and additive weighted aggregation are standard, well-defined methods in multi-criteria decision analysis (MCDA). The problem provides all necessary numerical data and definitions to apply these rules and analyze their properties. The criteria and their values are presented in an objective, clear manner. The problem does not violate any fundamental principles, is not ambiguous, and leads to a unique, derivable solution. It is a well-posed problem statement relevant to the topic of heuristics and decision-making in complex systems.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of Choices for Each Rule\n\n**1. Lexicographic Rule**\n\nThe lexicographic rule with priority $x_{s} \\succ x_{t} \\succ x_{e}$ compares strategies based on the most important criterion first. If there is a tie, it proceeds to the next criterion in the priority order.\n\n-   **Compare on $x_s$**:\n    -   $x_s(A) = 0.95$\n    -   $x_s(B) = 0.92$\n-   Since $x_s(A) > x_s(B)$ (i.e., $0.95 > 0.92$), strategy $A$ is strictly preferred. The rule terminates without considering the lower-priority criteria, $x_t$ and $x_e$.\n-   **Conclusion**: The lexicographic rule selects strategy $A$.\n\n**2. Additive Weighted Aggregation Rule**\n\nThis rule calculates a utility score, $U$, for each strategy by summing the product of each criterion's value and its corresponding weight. The strategy with the highest utility score is selected. The utility function is $U(S) = w_s x_s(S) + w_t x_t(S) + w_e x_e(S)$.\n\n-   **Calculate Utility for Strategy A**:\n    -   $U(A) = (0.50)(0.95) + (0.40)(0.60) + (0.10)(0.50)$\n    -   $U(A) = 0.475 + 0.240 + 0.050 = 0.765$\n\n-   **Calculate Utility for Strategy B**:\n    -   $U(B) = (0.50)(0.92) + (0.40)(0.90) + (0.10)(0.70)$\n    -   $U(B) = 0.460 + 0.360 + 0.070 = 0.890$\n\n-   **Compare Utilities**:\n    -   Since $U(B) > U(A)$ (i.e., $0.890 > 0.765$), strategy $B$ is preferred.\n-   **Conclusion**: The additive weighted aggregation rule selects strategy $B$.\n\n### Option-by-Option Analysis\n\n**A. Under the lexicographic rule with priority $x_{s} \\succ x_{t} \\succ x_{e}$, $A$ is selected; under additive weighted aggregation with $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$, $B$ is selected. The disagreement is due to the compensatory nature of the weighted rule, which permits trade-offs on lower-priority criteria.**\n\n-   The first part of the statement, asserting that the lexicographic rule selects $A$ and the additive rule selects $B$, is confirmed by the calculations above.\n-   The second part provides the correct reason for this disagreement. The lexicographic rule is non-compensatory: the superiority of $A$ on the highest-priority criterion $x_s$ cannot be offset by any advantage of $B$ on lower-priority criteria ($x_t$, $x_e$). In contrast, the additive weighted rule is compensatory: the significant advantages of $B$ in timeliness ($x_t(B) = 0.90$ vs. $x_t(A) = 0.60$) and energy efficiency ($x_e(B) = 0.70$ vs. $x_e(A) = 0.50$) are able to compensate for its small disadvantage in safety ($x_s(B) = 0.92$ vs. $x_s(A) = 0.95$), leading to a higher overall utility score. The statement is entirely correct.\n\nVerdict: **Correct**.\n\n**B. The additive weighted aggregation outcome is invariant under any strictly increasing transformation applied separately to each criterion (for example, replacing $x_{t}$ by $f(x_{t})$ where $f$ is strictly increasing), provided the weights are unchanged.**\n\n-   This statement is a claim about the scale-dependence of the additive aggregation rule. The rule $U = \\sum_i w_i x_i$ presupposes that the criteria values $x_i$ are measured on at least an interval scale, where differences are meaningful. Applying an arbitrary non-linear, strictly increasing transformation (e.g., $f(x) = x^2$ or $f(x) = \\sqrt{x}$) to a criterion scale destroys the interval property.\n-   Let's construct a counterexample. Start with two alternatives $C = (x_1=0.8, x_2=0.2)$ and $D = (x_1=0.6, x_2=0.7)$. Let the weights be $(w_1=0.5, w_2=0.5)$.\n-   Original scores: $U(C) = 0.5(0.8) + 0.5(0.2) = 0.4 + 0.1 = 0.5$. $U(D) = 0.5(0.6) + 0.5(0.7) = 0.3 + 0.35 = 0.65$. So, $D$ is preferred to $C$.\n-   Now, apply the strictly increasing transformation $f(x_1) = x_1^3$ to the first criterion only.\n-   New scores: $U'(C) = 0.5(0.8^3) + 0.5(0.2) = 0.5(0.512) + 0.1 = 0.256 + 0.1 = 0.356$. $U'(D) = 0.5(0.6^3) + 0.5(0.7) = 0.5(0.216) + 0.35 = 0.108 + 0.35 = 0.458$. The preference $D \\succ C$ is maintained.\n-   Let's reverse the initial preference. Let $C=(0.8, 0.2)$ and $D=(0.9, 0.05)$ and $w=(0.5, 0.5)$.\n-   $U(C)=0.5(0.8)+0.5(0.2)=0.5$. $U(D)=0.5(0.9)+0.5(0.05)=0.45+0.025=0.475$. So $C \\succ D$.\n-   Apply $f(x_1) = \\sqrt{x_1}$. $U'(C)=0.5\\sqrt{0.8}+0.5(0.2) \\approx 0.5(0.894)+0.1 = 0.447+0.1 = 0.547$. $U'(D)=0.5\\sqrt{0.9}+0.5(0.05) \\approx 0.5(0.949)+0.025 = 0.4745+0.025=0.4995$. The preference $C \\succ D$ is maintained.\n-   The statement is false. The property being described, invariance to monotonic transformations of criteria, is characteristic of ordinal aggregation methods, not cardinal ones like the weighted sum. The weighted sum is invariant only under positive affine transformations of the form $f(x_i) = a_i x_i + b_i$ ($a_i > 0$) if the weights and comparison threshold are adjusted accordingly, but certainly not for *any* strictly increasing function. A simple counterexample shows a preference reversal: let $A=(0.8, 0.1)$, $B=(0.2, 0.9)$ with weights $w=(0.6, 0.4)$. $U(A)=0.6(0.8)+0.4(0.1)=0.52$. $U(B)=0.6(0.2)+0.4(0.9)=0.48$. So $A \\succ B$. Now transform $x_1$ with $f(x_1)=x_1^3$. $U'(A) = 0.6(0.8^3)+0.4(0.1)=0.3472$. $U'(B) = 0.6(0.2^3)+0.4(0.9)=0.3648$. Now $U'(B)>U'(A)$. The preference reverses.\n\nVerdict: **Incorrect**.\n\n**C. Lexicographic ordering violates the Archimedean continuity axiom; no finite improvement in $x_{t}$ or $x_{e}$ can compensate for a shortfall in $x_{s}$. This non-compensatory structure can be normatively justified in safety-critical domains where the highest-priority criterion encodes a hard constraint.**\n\n-   The Archimedean property, applied to preferences, essentially states that no good is infinitely more desirable than another. Lexicographic preferences violate this by definition. For any two alternatives $Y = (x_s(Y), ...)$ and $Z = (x_s(Z), ...)$ where $x_s(Y) > x_s(Z)$, alternative $Y$ is preferred to $Z$ regardless of the values of the other criteria. An arbitrarily small advantage in $x_s$ outweighs an arbitrarily large advantage in $x_t$ or $x_e$. This is a violation of continuity.\n-   The normative justification is also sound. In many engineering, medical, or financial systems, certain criteria act as non-negotiable constraints. For example, a drug must be proven safe before its efficacy is even considered. A bridge must support a minimum load before its cost or aesthetic is evaluated. In such cases, modeling the preference structure as lexicographic is a rational and justifiable approach.\n\nVerdict: **Correct**.\n\n**D. Both rules respect Pareto dominance: if $A$ weakly dominates $B$ in all criteria and strictly dominates $B$ in at least one criterion, then both rules will select $A$.**\n\n-   Pareto Dominance Definition: Strategy $A$ Pareto dominates strategy $B$ if $x_i(A) \\ge x_i(B)$ for all criteria $i$, and $x_j(A) > x_j(B)$ for at least one criterion $j$.\n-   **Lexicographic Rule**: Assume $A$ Pareto dominates $B$. The rule checks criteria in order of priority $x_s, x_t, x_e$. Let the first criterion in this order where the values for $A$ and $B$ differ be $x_k$. Because $A$ Pareto dominates $B$, it must be that $x_k(A) > x_k(B)$ (it cannot be less). For all criteria $x_i$ with higher priority than $x_k$, we must have had $x_i(A) = x_i(B)$. Therefore, the lexicographic rule will select $A$ at step $k$. This holds for any priority ordering.\n-   **Additive Weighted Aggregation Rule**: Assume $A$ Pareto dominates $B$, and all weights $w_i$ are positive as given. The difference in utility is $U(A) - U(B) = \\sum_i w_i (x_i(A) - x_i(B))$. Since $x_i(A) \\ge x_i(B)$ for all $i$, each term $(x_i(A) - x_i(B))$ is non-negative. Since $w_i > 0$, each term $w_i(x_i(A) - x_i(B))$ is also non-negative. For the at least one criterion $j$ where $x_j(A) > x_j(B)$, the term $w_j(x_j(A) - x_j(B))$ is strictly positive. Therefore, the sum is strictly positive: $U(A) - U(B) > 0$, which implies $U(A) > U(B)$. The rule will select $A$.\n-   Both rules indeed respect Pareto dominance.\n\nVerdict: **Correct**.\n\n**E. In a population where agents learn via Reinforcement Learning (RL) with identical, time-invariant information, the aggregate behavior necessarily converges to the same long-run distribution over $A$ and $B$ under both lexicographic and additive weighted aggregation rules, because both rules are monotonic in each criterion.**\n\n-   This statement connects the static decision rules to dynamic learning behavior. A population of RL agents learning to maximize a scalar reward will, under standard assumptions, converge to selecting the action that provides the highest expected reward.\n-   The \"reward\" in this context would be derived from the outcome of the choice, evaluated by the decision rule.\n-   If the reward is the utility from the additive rule, agents will learn to choose $B$ because $U(B) = 0.890 > U(A) = 0.765$.\n-   If the \"reward\" is determined by the lexicographic preference, agents will learn to choose $A$, as it is the preferred option under that rule.\n-   Since the two rules select different strategies ($A$ vs. $B$), populations of RL agents optimizing for each respective rule will converge to different behaviors. One population will favor $A$, the other will favor $B$. The long-run distributions over choices will be different, not the same.\n-   The reason provided (\"because both rules are monotonic\") is insufficient. While both rules are monotonic (a property related to respecting Pareto dominance, as shown in D), they are fundamentally different aggregation functions and can, as shown, produce different preference orderings. Monotonicity alone does not guarantee that two different functions will produce the same output.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Many agents in adaptive systems do not use fixed decision rules but instead learn them through trial and error. This exercise provides a concrete, step-by-step walkthrough of Q-learning, a cornerstone algorithm for modeling adaptive agents that learn in environments with delayed rewards. Tracing the updates manually  demystifies the process of temporal-difference learning, showing precisely how an agent's internal valuation of actions is refined through a sequence of experiences.",
            "id": "4125800",
            "problem": "Consider a finite Markov decision process (MDP) with state space $\\mathcal{S}=\\{s_1, s_2\\}$ and action space $\\mathcal{A}=\\{a,b\\}$. The agent employs an off-policy temporal-difference control heuristic (Q-learning) as its decision-making rule within a complex adaptive system, while action selection is $\\varepsilon$-greedy with a fixed $\\varepsilon \\in (0,1)$. The immediate reward function is deterministic and defined by $r(s,a,s')=1$ if $s=s_1$ and $a=a$, and $r(s,a,s')=0$ otherwise. The agent updates its state-action values starting from $Q_0(s,a)=0$ for all $(s,a)\\in \\mathcal{S}\\times\\mathcal{A}$ with a constant learning rate $\\alpha=0.5$ and discount factor $\\gamma=0.9$. The following sequence of experienced transitions is observed:\n- Step 1: $(s_1,a) \\rightarrow s_2$,\n- Step 2: $(s_2,b) \\rightarrow s_1$,\n- Step 3: $(s_1,b) \\rightarrow s_1$,\n- Step 4: $(s_1,a) \\rightarrow s_1$,\n- Step 5: $(s_2,a) \\rightarrow s_2$,\n- Step 6: $(s_1,a) \\rightarrow s_2$.\n\nAssume that at each step the target for the update uses the optimal Bellman backup at the next state as is standard in Q-learning. Starting from the core definitions of the Bellman optimality equation for state-action values and the principle of stochastic approximation underlying temporal-difference learning, derive the update applied at each step and compute the exact value of $Q_6(s_1,a)$ after the six updates. Express your final answer as a reduced rational number. No rounding is required.",
            "solution": "We begin from the Bellman optimality equation for state-action values in a Markov decision process (MDP):\n$$\nQ^{\\star}(s,a)=\\mathbb{E}\\!\\left[r(s,a,s')+\\gamma \\max_{a'\\in\\mathcal{A}} Q^{\\star}(s',a') \\,\\middle|\\, s,a\\right].\n$$\nOff-policy temporal-difference control (Q-learning) is a stochastic approximation scheme that iteratively moves $Q_{t}(s,a)$ toward the fixed point of the Bellman optimality operator by applying the update\n$$\nQ_{t+1}(s_{t},a_{t})=Q_{t}(s_{t},a_{t})+\\alpha\\Big(r_{t}+\\gamma \\max_{a'\\in\\mathcal{A}} Q_{t}(s_{t+1},a')-Q_{t}(s_{t},a_{t})\\Big),\n$$\nwhile leaving $Q_{t+1}(s,a)=Q_{t}(s,a)$ for all $(s,a)\\neq (s_{t},a_{t})$. Here $r_{t}=r(s_{t},a_{t},s_{t+1})$. With $\\alpha=0.5$ and $\\gamma=0.9$, we initialize $Q_{0}(s,a)=0$ for all $(s,a)$. For compactness, note that the reward structure implies $r_{t}=1$ if $(s_{t},a_{t})=(s_{1},a)$ and $r_{t}=0$ otherwise.\n\nWe now compute step-by-step, keeping exact rational values. Let $\\alpha=\\frac{1}{2}$ and $\\gamma=\\frac{9}{10}$.\n\nStep 1: $(s_1, a) \\rightarrow s_2$. Reward $r_0=1$. Since $Q_0(s,a)=0$ for all $(s,a)$, we have\n$$\n\\max_{a'} Q_{0}(s_2,a')=0.\n$$\nThus\n$$\nQ_{1}(s_1,a)=Q_{0}(s_1,a)+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{10}\\cdot 0 - Q_{0}(s_1,a)\\Big)=0+\\tfrac{1}{2}\\cdot 1=\\tfrac{1}{2}.\n$$\nAll other $Q$-values remain $0$.\n\nStep 2: $(s_2, b) \\rightarrow s_1$. Reward $r_1=0$. We compute\n$$\n\\max_{a'} Q_{1}(s_1,a')=\\max\\!\\left\\{Q_{1}(s_1,a),Q_{1}(s_1,b)\\right\\}=\\max\\!\\left\\{\\tfrac{1}{2},0\\right\\}=\\tfrac{1}{2}.\n$$\nThus\n$$\nQ_{2}(s_2,b)=Q_{1}(s_2,b)+\\tfrac{1}{2}\\Big(0+\\tfrac{9}{10}\\cdot \\tfrac{1}{2}-Q_{1}(s_2,b)\\Big)=0+\\tfrac{1}{2}\\cdot \\tfrac{9}{20}=\\tfrac{9}{40}.\n$$\nAll other $Q$-values unchanged, so $Q_{2}(s_1,a)=\\tfrac{1}{2}$.\n\nStep 3: $(s_1, b) \\rightarrow s_1$. Reward $r_2=0$. Compute\n$$\n\\max_{a'} Q_{2}(s_1,a')=\\max\\!\\left\\{\\tfrac{1}{2},0\\right\\}=\\tfrac{1}{2}.\n$$\nHence\n$$\nQ_{3}(s_1,b)=Q_{2}(s_1,b)+\\tfrac{1}{2}\\Big(0+\\tfrac{9}{10}\\cdot \\tfrac{1}{2}-Q_{2}(s_1,b)\\Big)=0+\\tfrac{1}{2}\\cdot \\tfrac{9}{20}=\\tfrac{9}{40}.\n$$\nOther values unchanged, so $Q_{3}(s_1,a)=\\tfrac{1}{2}$ and $Q_{3}(s_2,b)=\\tfrac{9}{40}$.\n\nStep 4: $(s_1, a) \\rightarrow s_1$. Reward $r_3=1$. Compute\n$$\n\\max_{a'} Q_{3}(s_1,a')=\\max\\!\\left\\{Q_{3}(s_1,a),Q_{3}(s_1,b)\\right\\}=\\max\\!\\left\\{\\tfrac{1}{2},\\tfrac{9}{40}\\right\\}=\\tfrac{1}{2}.\n$$\nThus\n$$\n\\begin{aligned}\nQ_{4}(s_1,a)&=Q_{3}(s_1,a)+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{10}\\cdot \\tfrac{1}{2}-Q_{3}(s_1,a)\\Big) \\\\\n&=\\tfrac{1}{2}+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{20}-\\tfrac{1}{2}\\Big) \\\\\n&=\\tfrac{1}{2}+\\tfrac{1}{2}\\Big(\\tfrac{20}{20}+\\tfrac{9}{20}-\\tfrac{10}{20}\\Big) \\\\\n&=\\tfrac{1}{2}+\\tfrac{1}{2}\\cdot \\tfrac{19}{20} \\\\\n&=\\tfrac{1}{2}+\\tfrac{19}{40} \\\\\n&=\\tfrac{20}{40}+\\tfrac{19}{40} \\\\\n&=\\tfrac{39}{40}.\n\\end{aligned}\n$$\nAll other values unchanged.\n\nStep 5: $(s_2, a) \\rightarrow s_2$. Reward $r_4=0$. Compute\n$$\n\\max_{a'} Q_{4}(s_2,a')=\\max\\!\\left\\{Q_{4}(s_2,a),Q_{4}(s_2,b)\\right\\}=\\max\\!\\left\\{0,\\tfrac{9}{40}\\right\\}=\\tfrac{9}{40}.\n$$\nThus\n$$\nQ_{5}(s_2,a)=Q_{4}(s_2,a)+\\tfrac{1}{2}\\Big(0+\\tfrac{9}{10}\\cdot \\tfrac{9}{40}-Q_{4}(s_2,a)\\Big)=0+\\tfrac{1}{2}\\cdot \\tfrac{81}{400}=\\tfrac{81}{800}.\n$$\n\nStep 6: $(s_1, a) \\rightarrow s_2$. Reward $r_5=1$. Compute\n$$\n\\max_{a'} Q_{5}(s_2,a')=\\max\\!\\left\\{Q_{5}(s_2,a),Q_{5}(s_2,b)\\right\\}=\\max\\!\\left\\{\\tfrac{81}{800},\\tfrac{9}{40}\\right\\}=\\tfrac{9}{40},\n$$\nsince $\\tfrac{9}{40}=\\tfrac{180}{800}>\\tfrac{81}{800}$. Therefore\n$$\n\\begin{aligned}\nQ_{6}(s_1,a)&=Q_{5}(s_1,a)+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{10}\\cdot \\tfrac{9}{40}-Q_{5}(s_1,a)\\Big) \\\\\n&=\\tfrac{39}{40}+\\tfrac{1}{2}\\Big(1+\\tfrac{81}{400}-\\tfrac{39}{40}\\Big) \\\\\n&=\\tfrac{39}{40}+\\tfrac{1}{2}\\Big(\\tfrac{400}{400}+\\tfrac{81}{400}-\\tfrac{390}{400}\\Big) \\\\\n&=\\tfrac{39}{40}+\\tfrac{1}{2}\\cdot \\tfrac{91}{400} \\\\\n&=\\tfrac{39}{40}+\\tfrac{91}{800} \\\\\n&=\\tfrac{780}{800}+\\tfrac{91}{800} \\\\\n&=\\tfrac{871}{800}.\n\\end{aligned}\n$$\n\nHence, after six updates, the exact value is $Q_{6}(s_{1},a)=\\tfrac{871}{800}$.",
            "answer": "$$\\boxed{\\frac{871}{800}}$$"
        },
        {
            "introduction": "The essence of complex systems is the emergence of macroscopic patterns from microscopic interactions. This hands-on coding challenge bridges the gap between agent-level heuristics and system-level dynamics by asking you to simulate a classic threshold model of influence on a network. Implementing this simulation  is a capstone exercise, providing essential practice in using computation as a laboratory for exploring how simple local rules can give rise to complex collective behaviors like informational cascades.",
            "id": "4125788",
            "problem": "You are asked to write a complete, runnable program that estimates the expected cascade fraction under a deterministic threshold adoption rule on a random network. The network model is the Erdős–Rényi (ER) random graph $G(n,p)$ defined as follows: there are $n$ nodes labeled $\\{0,1,\\dots,n-1\\}$, and each unordered pair of distinct nodes is connected by an edge independently with probability $p$. The decision-making rule is a deterministic threshold rule: each node adopts if and only if it has at least $\\theta$ neighbors that have adopted. A seed set of nodes is initially forced to adopt and remains adopted thereafter. The adoption process proceeds in synchronous rounds, and adoption is irreversible.\n\nFundamental bases for this task are the standard definition of the Erdős–Rényi random graph $G(n,p)$ and the monotone deterministic threshold update rule. The algorithm must proceed from these bases only, without introducing any shortcut formulas.\n\nYour program must estimate the expectation of the final cascade fraction by Monte Carlo simulation. Specifically, for each test case:\n- Draw a fresh network $G(n,p)$ per trial.\n- Select a seed set of size $k$ uniformly at random from the $n$ nodes without replacement.\n- Run the synchronous threshold adoption dynamics with threshold $\\theta$ until convergence (no new adoptions), starting from the seed set as initially active.\n- Record the final cascade fraction, defined as the number of adopted nodes divided by $n$, expressed as a decimal.\n- Repeat for the specified number of trials $T$ and output the average of these fractions across trials for that test case.\n\nUse a fixed pseudorandom number generator seed of $42$ for reproducibility. There are no physical units and no angles in this problem. All fractions must be expressed as decimals. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each result rounded to $6$ decimal places (for example, $[0.123456,0.500000]$).\n\nTest suite (each test case is a tuple of $(n,p,\\theta,k,T)$):\n- Base case (happy path): $(n,p,\\theta,k,T)=(500,0.01,3,5,200)$.\n- Empty network boundary: $(n,p,\\theta,k,T)=(300,0.0,1,10,100)$.\n- Zero threshold edge case (immediate full adoption): $(n,p,\\theta,k,T)=(300,0.01,0,5,50)$.\n- Threshold above any possible degree (adoption restricted to seeds): $(n,p,\\theta,k,T)=(400,0.05,401,2,50)$.\n- High connectivity with moderate threshold: $(n,p,\\theta,k,T)=(200,0.2,5,5,100)$.\n\nYour program must implement the following without using any external data:\n- Construction of $G(n,p)$ by independently deciding each edge between unordered pairs of distinct nodes with probability $p$.\n- Synchronous threshold adoption dynamics with irreversible adoption and monotone updates, starting from $k$ random seeds.\n- Monte Carlo averaging across $T$ independent trials per test case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the specified order of test cases, with each result rounded to $6$ decimal places.",
            "solution": "The problem requires the estimation of the expected final cascade fraction for a deterministic threshold adoption process on an Erdős–Rényi random graph. The estimation is to be performed via Monte Carlo simulation.\n\nProblem validation confirms that the problem is well-posed, scientifically grounded, and programmatically specified. It is based on standard models from network science—the Erdős–Rényi graph $G(n,p)$ and a deterministic threshold rule—and provides all necessary parameters and procedures for a computational solution. The test cases include standard scenarios as well as well-defined edge cases that test the robustness of the implementation. Therefore, the problem is valid.\n\nThe solution is developed by algorithmically implementing the specified model from first principles. This involves three primary stages for each simulation trial: network generation, seed set selection, and the execution of the adoption dynamics.\n\n**1. Network Generation: Erdős–Rényi Graph $G(n,p)$**\n\nThe network is an Erdős–Rényi random graph, denoted $G(n,p)$, with $n$ nodes and an edge probability $p$. The nodes are labeled $V = \\{0, 1, \\dots, n-1\\}$. For any distinct pair of nodes $(i, j)$ with $i, j \\in V$ and $i \\ne j$, an undirected edge is created between them with probability $p$. This process is independent for all such pairs.\n\nComputationally, this graph can be represented by an $n \\times n$ adjacency matrix $A$, where $A_{ij} = 1$ if an edge exists between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. Since the graph is undirected, the matrix $A$ is symmetric, i.e., $A_{ij} = A_{ji}$. By convention, there are no self-loops, so the diagonal elements are zero, $A_{ii} = 0$.\n\nTo construct this matrix for a given $n$ and $p$, we consider the $\\binom{n}{2} = \\frac{n(n-1)}{2}$ unique pairs of nodes. For each pair, we draw a random number $r$ from a uniform distribution on $[0,1)$. If $r < p$, we add an edge. In the implementation, this is achieved by generating random values for the upper triangle of the adjacency matrix and then reflecting them onto the lower triangle to ensure symmetry.\n\n**2. Synchronous Threshold Adoption Dynamics**\n\nThe state of each node $i$ at a discrete time step $t$ is represented by $S_i(t) \\in \\{0, 1\\}$, where $S_i(t) = 1$ signifies adoption and $S_i(t) = 0$ signifies non-adoption.\n\n- **Initialization ($t=0$):** A seed set $S_{\\text{seed}} \\subset V$ of size $k$ is selected uniformly at random without replacement. For all nodes $i \\in S_{\\text{seed}}$, their state is set to $S_i(0) = 1$. For all other nodes $j \\notin S_{\\text{seed}}$, $S_j(0) = 0$. These seed nodes are irreversibly adopted for all $t > 0$.\n\n- **Update Rule ($t \\to t+1$):** The system evolves in synchronous rounds. In each round, every non-adopted node $i$ evaluates its neighborhood. A node $i$ adopts if the number of its neighbors that have already adopted meets or exceeds a fixed threshold $\\theta$. Mathematically, for a node $i$ that is not yet adopted ($S_i(t) = 0$), its state at the next time step is updated as follows:\n$$\nS_i(t+1) = 1 \\quad \\text{if} \\quad \\sum_{j=0}^{n-1} A_{ij} S_j(t) \\ge \\theta\n$$\nAdoption is irreversible; once a node adopts, it remains adopted for all subsequent time steps. This can be expressed as $S_i(t+1) = 1$ if $S_i(t)=1$. The synchronous nature implies that all nodes are evaluated based on the state of the network at time $t$, and their states are updated simultaneously to form the network state at time $t+1$.\n\n- **Convergence:** The process is guaranteed to converge. The set of adopted nodes is monotonically non-decreasing with each time step. Since the total number of nodes $n$ is finite, the process must eventually reach a fixed point where no new nodes adopt in a round. At this point, $S(t+1) = S(t)$, and the simulation for that trial terminates.\n\n**3. Monte Carlo Estimation**\n\nThe goal is to estimate the expectation of the final cascade fraction, $\\mathbb{E}[f_{\\text{final}}]$. The final fraction for a single trial is defined as $f_{\\text{final}} = N_{\\text{adopted}} / n$, where $N_{\\text{adopted}}$ is the total number of adopted nodes at convergence.\n\nThe expectation is taken over the distribution of random graphs and the random selection of seed sets. The Monte Carlo method approximates this expectation by averaging the results of a large number of independent trials, $T$. For each test case defined by the tuple $(n,p,\\theta,k,T)$, the procedure is:\n1. Initialize an accumulator for the total fraction to $0$.\n2. For $i=1, \\dots, T$:\n    a. Generate a new random graph $G_i \\sim G(n,p)$.\n    b. Select a new random seed set $S_{\\text{seed},i}$ of size $k$.\n    c. Run the adoption dynamics simulation to convergence.\n    d. Calculate the final fraction $f_{\\text{final}}^{(i)}$.\n    e. Add this fraction to the accumulator.\n3. The estimated expectation is the average:\n$$\n\\hat{\\mathbb{E}}[f_{\\text{final}}] = \\frac{1}{T} \\sum_{i=1}^{T} f_{\\text{final}}^{(i)}\n$$\nA fixed pseudorandom number generator seed of $42$ ensures the reproducibility of the entire sequence of random choices across all trials and test cases. The implementation utilizes `numpy` for efficient array and matrix operations, particularly the matrix-vector product `A @ S(t)` to concurrently compute the number of active neighbors for all nodes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_trial(n, p, theta, k, rng):\n    \"\"\"\n    Runs a single trial of the cascade simulation.\n\n    Args:\n        n (int): Number of nodes.\n        p (float): Edge probability for the Erdős–Rényi graph.\n        theta (int): Adoption threshold.\n        k (int): Size of the initial seed set.\n        rng (numpy.random.Generator): The random number generator.\n\n    Returns:\n        float: The final fraction of adopted nodes.\n    \"\"\"\n    # 1. Generate the adjacency matrix for G(n, p)\n    # This prevents creating a huge intermediate matrix for large n\n    adj_matrix = np.zeros((n, n), dtype=np.int8)\n    \n    # Get indices for the upper triangle of the matrix (excluding the diagonal)\n    # to represent unique unordered pairs of nodes.\n    upper_tri_indices = np.triu_indices(n, k=1)\n    num_possible_edges = len(upper_tri_indices[0])\n    \n    # Decide which edges exist based on probability p\n    edge_exists = rng.random(num_possible_edges) < p\n    \n    # Populate the adjacency matrix\n    adj_matrix[upper_tri_indices] = edge_exists\n    adj_matrix = adj_matrix + adj_matrix.T\n\n    # 2. Select a seed set of size k uniformly at random\n    if k > n:\n        raise ValueError(\"Seed set size k cannot be larger than number of nodes n.\")\n    seeds = rng.choice(n, size=k, replace=False)\n\n    # 3. Run the synchronous threshold adoption dynamics\n    adopted = np.zeros(n, dtype=bool)\n    adopted[seeds] = True\n\n    # Handle the theta=0 edge case efficiently: all nodes will adopt\n    if theta == 0:\n        return 1.0\n\n    while True:\n        num_adopted_before = np.sum(adopted)\n        \n        # Calculate the number of adopted neighbors for each node\n        # Using matrix multiplication is highly efficient.\n        # The result is a vector where the i-th element is the sum of adopted neighbors of node i.\n        num_adopted_neighbors = adj_matrix @ adopted\n\n        # Identify nodes that are not yet adopted but meet the threshold\n        potential_adopters_mask = ~adopted\n        \n        # A node will adopt if its number of adopted neighbors is >= theta\n        will_adopt_mask = (num_adopted_neighbors >= theta)\n        \n        # The set of newly adopting nodes are those that can and will adopt\n        newly_adopted_mask = potential_adopters_mask & will_adopt_mask\n\n        # If there are any new adoptions, update the state\n        if np.any(newly_adopted_mask):\n            adopted[newly_adopted_mask] = True\n        else:\n            # If no new adoptions, the cascade has converged\n            break\n            \n    # 4. Record the final cascade fraction\n    final_fraction = np.sum(adopted) / n\n    return final_fraction\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Use a fixed pseudorandom number generator for reproducibility.\n    # np.random.default_rng is the modern, preferred way.\n    rng = np.random.default_rng(42)\n\n    # Test suite (each test case is a tuple of (n, p, theta, k, T))\n    test_cases = [\n        # Base case (happy path)\n        (500, 0.01, 3, 5, 200),\n        # Empty network boundary\n        (300, 0.0, 1, 10, 100),\n        # Zero threshold edge case (immediate full adoption)\n        (300, 0.01, 0, 5, 50),\n        # Threshold above any possible degree (adoption restricted to seeds)\n        (400, 0.05, 401, 2, 50),\n        # High connectivity with moderate threshold\n        (200, 0.2, 5, 5, 100),\n    ]\n\n    results = []\n    for n, p, theta, k, T in test_cases:\n        trial_fractions = []\n        for _ in range(T):\n            fraction = run_single_trial(n, p, theta, k, rng)\n            trial_fractions.append(fraction)\n        \n        # Calculate the average fraction across all trials for this test case\n        avg_fraction = np.mean(trial_fractions)\n        results.append(avg_fraction)\n\n    # Format the output as a comma-separated list of 6-decimal-place floats\n    # enclosed in square brackets.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}