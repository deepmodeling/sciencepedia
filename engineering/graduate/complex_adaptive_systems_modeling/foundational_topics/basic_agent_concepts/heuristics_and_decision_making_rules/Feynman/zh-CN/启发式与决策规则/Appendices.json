{
    "hands_on_practices": [
        {
            "introduction": "任何决策启发式的核心在于其评估和比较多标准下不同选项的方法。本练习将对比两种基本方法：补偿性的加权求和规则与非补偿性的字典序规则。通过将这两种规则应用于一个具体情景，您将看到它们如何导致不同的选择，并更深刻地理解它们关于权衡和优先级的内在假设，这是构建真实智能体模型的关键一步 。",
            "id": "4125860",
            "problem": "考虑一个复杂环境中的自适应代理群体，他们必须在两种策略（表示为 $A$ 和 $B$）之间进行选择。他们的决策由三个归一化到单位区间 $[0,1]$ 的标准引导：安全性 $x_{s}$、及时性 $x_{t}$ 和能源效率 $x_{e}$，数值越大表示在该标准上的表现越好。该群体采用两种决策规则：一种是具有严格优先级 $x_{s} \\succ x_{t} \\succ x_{e}$ 的字典序规则，另一种是具有固定正权重且权重之和为 $1$ 的加权加法聚合规则。两种策略的标准值如下：\n- 对于 $A$：$x_{s}(A) = 0.95$, $x_{t}(A) = 0.60$, $x_{e}(A) = 0.50$。\n- 对于 $B$：$x_{s}(B) = 0.92$, $x_{t}(B) = 0.90$, $x_{e}(B) = 0.70$。\n加法聚合权重为 $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$。\n\n根据字典序排序和加权加法聚合的基本原理和核心定义，确定每种规则选择哪种策略，并从连续性、补偿性、帕累托单调性和尺度依赖性方面分析其规范性含义。然后，选择下面所有正确的陈述。\n\nA. 在具有优先级 $x_{s} \\succ x_{t} \\succ x_{e}$ 的字典序规则下，策略 $A$ 被选中；在加权加法聚合规则（权重为 $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$）下，策略 $B$ 被选中。这种分歧是由于加权规则的补偿性质造成的，它允许在较低优先级的标准上进行权衡。\n\nB. 只要权重不变，加权加法聚合的结果在对每个标准分别应用任何严格递增的变换（例如，用 $f(x_{t})$ 替换 $x_{t}$，其中 $f$ 是严格递增的）时保持不变。\n\nC. 字典序排序违反了阿基米德连续性公理；$x_{t}$ 或 $x_{e}$ 的任何有限改进都无法补偿 $x_{s}$ 上的不足。这种非补偿性结构在安全关键领域中可以得到规范性的辩护，在这些领域中，最高优先级的标准代表了一个硬性约束。\n\nD. 两种规则都遵循帕累托占优原则：如果策略 $A$ 在所有标准上弱占优于策略 $B$，并且在至少一个标准上严格占优于策略 $B$，那么两种规则都会选择 $A$。\n\nE. 在一个代理通过强化学习（RL）在相同且时不变的信息下进行学习的群体中，无论采用字典序规则还是加权加法聚合规则，其聚合行为必然会收敛到关于 $A$ 和 $B$ 的相同长期分布，因为这两种规则在每个标准上都是单调的。\n\n选择所有正确的选项。",
            "solution": "在进行求解之前，首先评估问题陈述的有效性。\n\n### 第 1 步：提取已知条件\n- **策略**：两种策略，表示为 $A$ 和 $B$。\n- **标准**：三个标准，归一化到区间 $[0,1]$：安全性 ($x_s$)、及时性 ($x_t$) 和能源效率 ($x_e$)。\n- **策略 A 的标准值**：$x_{s}(A) = 0.95$, $x_{t}(A) = 0.60$, $x_{e}(A) = 0.50$。\n- **策略 B 的标准值**：$x_{s}(B) = 0.92$, $x_{t}(B) = 0.90$, $x_{e}(B) = 0.70$。\n- **决策规则 1**：具有严格优先级 $x_{s} \\succ x_{t} \\succ x_{e}$ 的字典序规则。\n- **决策规则 2**：具有固定正权重 $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$ 的加权加法聚合规则。权重之和声明为 $1$，经确认：$0.50 + 0.40 + 0.10 = 1.00$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题在科学上和数学上都是合理的。字典序排序和加权加法聚合是多标准决策分析（MCDA）中的标准、定义明確的方法。该问题提供了应用这些规则并分析其属性所需的所有数值数据和定义。标准及其值以客观、清晰的方式呈现。该问题不违反任何基本原则，没有歧义，并能导出一个唯一的、可推导的解。这是一个与复杂系统中的启发式和决策制定主题相关的适定问题陈述。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将推导解答。\n\n### 推导每种规则的选择\n\n**1. 字典序规则**\n\n具有优先级 $x_{s} \\succ x_{t} \\succ x_{e}$ 的字典序规则首先根据最重要的标准比较策略。如果出现平局，则按优先级顺序进入下一个标准。\n\n- **在 $x_s$ 上进行比较**：\n    - $x_s(A) = 0.95$\n    - $x_s(B) = 0.92$\n- 由于 $x_s(A) > x_s(B)$（即 $0.95 > 0.92$），策略 $A$ 是严格更优的。规则终止，不再考虑较低优先级的标准 $x_t$ 和 $x_e$。\n- **结论**：字典序规则选择策略 $A$。\n\n**2. 加权加法聚合规则**\n\n该规则通过将每个标准的值与其对应的权重相乘后求和，来为每个策略计算一个效用分数 $U$。选择效用分数最高的策略。效用函数为 $U(S) = w_s x_s(S) + w_t x_t(S) + w_e x_e(S)$。\n\n- **计算策略 A 的效用**：\n    - $U(A) = (0.50)(0.95) + (0.40)(0.60) + (0.10)(0.50)$\n    - $U(A) = 0.475 + 0.240 + 0.050 = 0.765$\n\n- **计算策略 B 的效用**：\n    - $U(B) = (0.50)(0.92) + (0.40)(0.90) + (0.10)(0.70)$\n    - $U(B) = 0.460 + 0.360 + 0.070 = 0.890$\n\n- **比较效用**：\n    - 由于 $U(B) > U(A)$（即 $0.890 > 0.765$），策略 $B$ 是更优的。\n- **结论**：加权加法聚合规则选择策略 $B$。\n\n### 逐项分析\n\n**A. 在具有优先级 $x_{s} \\succ x_{t} \\succ x_{e}$ 的字典序规则下，策略 $A$ 被选中；在加权加法聚合规则（权重为 $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$）下，策略 $B$ 被选中。这种分歧是由于加权规则的补偿性质造成的，它允许在较低优先级的标准上进行权衡。**\n\n- 陈述的第一部分，即断言字典序规则选择 A 而加法规则选择 B，已由上述计算证实。\n- 第二部分给出了这种分歧的正确原因。字典序规则是非补偿性的：A 在最高优先级标准 $x_s$ 上的优越性不能被 B 在较低优先级标准（$x_t$, $x_e$）上的任何优势所抵消。相比之下，加权加法规则是补偿性的：B 在及时性（$x_t(B) = 0.90$ vs. $x_t(A) = 0.60$）和能源效率（$x_e(B) = 0.70$ vs. $x_e(A) = 0.50$）方面的显著优势能够补偿其在安全性上的微小劣势（$x_s(B) = 0.92$ vs. $x_s(A) = 0.95$），从而获得了更高的总效用分数。该陈述完全正确。\n\n结论：**正确**。\n\n**B. 只要权重不变，加权加法聚合的结果在对每个标准分别应用任何严格递增的变换（例如，用 $f(x_{t})$ 替换 $x_{t}$，其中 $f$ 是严格递增的）时保持不变。**\n\n- 该陈述是关于加法聚合规则的尺度依赖性的主张。该规则 $U = \\sum_i w_i x_i$ 预设了标准值 $x_i$ 至少是在区间标度上测量的，其中差异是有意义的。对一个标准尺度应用任意的非线性、严格递增的变换（例如，$f(x) = x^2$ 或 $f(x) = \\sqrt{x}$）会破坏其区间属性。\n- 我们构造一个反例。从两个备选项 $C = (x_1=0.8, x_2=0.2)$ 和 $D = (x_1=0.6, x_2=0.7)$ 开始。设权重为 $(w_1=0.5, w_2=0.5)$。\n- 原始分数为：$U(C) = 0.5(0.8) + 0.5(0.2) = 0.4 + 0.1 = 0.5$。$U(D) = 0.5(0.6) + 0.5(0.7) = 0.3 + 0.35 = 0.65$。所以，$D$ 优于 $C$。\n- 现在，仅对第一个标准应用严格递增变换 $f(x_1) = x_1^3$。\n- 新分数为：$U'(C) = 0.5(0.8^3) + 0.5(0.2) = 0.5(0.512) + 0.1 = 0.256 + 0.1 = 0.356$。$U'(D) = 0.5(0.6^3) + 0.5(0.7) = 0.5(0.216) + 0.35 = 0.108 + 0.35 = 0.458$。$D \\succ C$ 的偏好关系得以保持。\n- 我们尝试一个不同的变换，它会放大尺度高端的差异。设 $f(x_2) = (x_2)^ {1/3}$。\n- 用原始的 $C, D$ 和这个新的对 $x_2$ 的变换重新计算：$U''(C) = 0.5(0.8) + 0.5((0.2)^{1/3}) \\approx 0.4 + 0.5(0.585) = 0.4 + 0.2925 = 0.6925$。$U''(D) = 0.5(0.6) + 0.5((0.7)^{1/3}) \\approx 0.3 + 0.5(0.888) = 0.3 + 0.444 = 0.744$。偏好仍然是 $D \\succ C$。\n- 让我们反转初始偏好。设 $C=(0.8, 0.2)$ 和 $D=(0.9, 0.05)$ 且 $w=(0.5, 0.5)$。\n- $U(C)=0.5(0.8)+0.5(0.2)=0.5$。$U(D)=0.5(0.9)+0.5(0.05)=0.45+0.025=0.475$。所以 $C \\succ D$。\n- 应用 $f(x_1) = \\sqrt{x_1}$。$U'(C)=0.5\\sqrt{0.8}+0.5(0.2) \\approx 0.5(0.894)+0.1 = 0.447+0.1 = 0.547$。$U'(D)=0.5\\sqrt{0.9}+0.5(0.05) \\approx 0.5(0.949)+0.025 = 0.4745+0.025=0.4995$。偏好 $C \\succ D$ 得以保持。\n- 该陈述是错误的。所描述的性质，即对标准的单调变换的不变性，是序数聚合方法的特征，而不是像加权和这样的基数方法。加权和仅在正仿射变换（形式为 $f(x_i) = a_i x_i + b_i$，$a_i > 0$）下保持不变，并且需要相应地调整权重和比较阈值，但绝不是对*任何*严格递增函数都成立。思维过程中的前一个反例是正确的：$A=(0.8, 0.1)$，$B=(0.2, 0.9)$，$w=(0.6, 0.4)$。$U(A)=0.52 > U(B)=0.48$。用 $f(x_1)=x_1^3$ 变换 $x_1$。$U'(A) = 0.6(0.8^3)+0.4(0.1)=0.3472$。$U'(B) = 0.6(0.2^3)+0.4(0.9)=0.3648$。现在 $U'(B)>U'(A)$。偏好发生逆转。\n\n结论：**错误**。\n\n**C. 字典序排序违反了阿基米德连续性公理；$x_{t}$ 或 $x_{e}$ 的任何有限改进都无法补偿 $x_{s}$ 上的不足。这种非补偿性结构在安全关键领域中可以得到规范性的辩护，在这些领域中，最高优先级的标准代表了一个硬性约束。**\n\n- 阿基米德性质应用于偏好时，本质上是说没有一种物品比另一种物品具有无限大的 desirability。字典序偏好根据其定义就违反了这一点。对于任何两个备选项 $Y = (x_s(Y), ...)$ 和 $Z = (x_s(Z), ...)$，其中 $x_s(Y) > x_s(Z)$，无论其他标准的值如何，备选项 $Y$都优于 $Z$。$x_s$ 上一个任意小的优势会压倒 $x_t$ 或 $x_e$ 上一个任意大的优势。这违反了连续性。\n- 规范性辩护也是合理的。在许多工程、医疗或金融系统中，某些标准充当了不可协商的约束。例如，一种药物在考虑其疗效之前必须被证明是安全的。一座桥梁在评估其成本或美学之前必须能支撑最小载荷。在这种情况下，将偏好结构建模为字典序是一种理性的、可辩护的方法。\n\n结论：**正确**。\n\n**D. 两种规则都遵循帕累托占优原则：如果策略 $A$ 在所有标准上弱占优于策略 $B$，并且在至少一个标准上严格占优于策略 $B$，那么两种规则都会选择 $A$。**\n\n- 帕累托占优定义：如果对于所有标准 $i$ 都有 $x_i(A) \\ge x_i(B)$，并且至少对于一个标准 $j$ 有 $x_j(A) > x_j(B)$，则策略 $A$ 帕累托占优于策略 $B$。\n- **字典序规则**：假设 $A$ 帕累托占优于 $B$。规则按优先级 $x_s, x_t, x_e$ 的顺序检查标准。设此顺序中第一个 $A$ 和 $B$ 值不同的标准为 $x_k$。因为 $A$ 帕累托占优于 $B$，所以必定有 $x_k(A) > x_k(B)$（不可能是小于）。对于所有优先级高于 $x_k$ 的标准 $x_i$，我们必定有 $x_i(A) = x_i(B)$。因此，字典序规则将在第 $k$ 步选择 $A$。这对任何优先级排序都成立。\n- **加权加法聚合规则**：假设 $A$ 帕累托占优于 $B$，且所有权重 $w_i$ 均为正。效用差为 $U(A) - U(B) = \\sum_i w_i (x_i(A) - x_i(B))$。由于对所有 $i$ 都有 $x_i(A) \\ge x_i(B)$，所以每一项 $(x_i(A) - x_i(B))$ 都是非负的。由于 $w_i > 0$，每一项 $w_i(x_i(A) - x_i(B))$ 也都是非负的。对于至少一个标准 $j$ 使得 $x_j(A) > x_j(B)$，该项 $w_j(x_j(A) - x_j(B))$ 是严格为正的。因此，总和严格为正：$U(A) - U(B) > 0$，这意味着 $U(A) > U(B)$。该规则将选择 $A$。\n- 两种规则确实都遵循帕累托占优原则。\n\n结论：**正确**。\n\n**E. 在一个代理通过强化学习（RL）在相同且时不变的信息下进行学习的群体中，无论采用字典序规则还是加权加法聚合规则，其聚合行为必然会收敛到关于 $A$ 和 $B$ 的相同长期分布，因为这两种规则在每个标准上都是单调的。**\n\n- 该陈述将静态决策规则与动态学习行为联系起来。一个学习最大化标量奖励的强化学习代理群体，在标准假设下，将收敛到选择提供最高期望奖励的行动。\n- 在这种情况下，“奖励”将源于选择的结果，并由决策规则进行评估。\n- 如果奖励来自加法规则的效用，代理将学会选择 $B$，因为 $U(B) = 0.890 > U(A) = 0.765$。\n- 如果“奖励”由字典序偏好决定，代理将学会选择 $A$，因为在该规则下它是更优的选项。\n- 由于两种规则选择了不同的策略（$A$ vs. $B$），为各自规则进行优化的强化学习代理群体将收敛到不同的行为。一个群体将偏好 $A$，另一个将偏好 $B$。对选择的长期分布将是不同的，而不是相同的。\n- 所给出的理由（“因为这两种规则在每个标准上都是单调的”）是不充分的。虽然两种规则都是单调的（一个与遵循帕累托占优有关的性质，如在D中所示），但它们是根本不同的聚合函数，并且可以（如此处所示）产生不同的偏好排序。单独的单调性并不能保证两个不同的函数会产生相同的输出。\n\n结论：**错误**。",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "复杂系统中的智能体通常缺乏对其所处世界的完美模型，必须通过经验进行学习。本练习将带您动手演练 Q-learning，这是一种强大的算法，能让智能体通过试错学习最优行动。通过手动追踪状态-行动价值（$Q$ 值）的逐步更新过程，您将对智能体如何根据环境反馈学习和调整其行为建立一个具体、机械化的理解 。",
            "id": "4125800",
            "problem": "考虑一个有限马尔可夫决策过程 (MDP)，其状态空间为 $\\mathcal{S}=\\{s_{1}, s_{2}\\}$，动作空间为 $\\mathcal{A}=\\{a,b\\}$。在一个复杂自适应系统中，智能体采用离策略时序差分控制启发式算法 (Q学习) 作为其决策规则，而动作选择是 $\\varepsilon$-贪心策略，其中 $\\varepsilon \\in (0,1)$ 是一个固定值。即时奖励函数是确定性的，定义为：当 $s=s_{1}$ 且 $a=a$ 时，$r(s,a,s')=1$，否则 $r(s,a,s')=0$。智能体从 $Q_{0}(s,a)=0$（对于所有 $(s,a)\\in \\mathcal{S}\\times\\mathcal{A}$）开始更新其状态-动作值，使用恒定的学习率 $\\alpha=0.5$ 和折扣因子 $\\gamma=0.9$。观测到以下经历的转移序列：\n- 步骤 1：$(s_{1},a)\\rightarrow s_{2}$，\n- 步骤 2：$(s_{2},b)\\rightarrow s_{1}$，\n- 步骤 3：$(s_{1},b)\\rightarrow s_{1}$，\n- 步骤 4：$(s_{1},a)\\rightarrow s_{1}$，\n- 步骤 5：$(s_{2},a)\\rightarrow s_{2}$，\n- 步骤 6：$(s_{1},a)\\rightarrow s_{2}$。\n\n假设在每一步中，更新的目标都使用下一个状态的最优贝尔曼备份，这是Q学习的标准做法。从状态-动作值的贝尔曼最优方程的核心定义以及作为时序差分学习基础的随机近似原理出发，推导每一步应用的更新规则，并计算经过六次更新后 $Q_{6}(s_{1},a)$ 的精确值。将您的最终答案表示为一个最简分数。无需四舍五入。",
            "solution": "我们从马尔可夫决策过程 (MDP) 中状态-动作值的贝尔曼最优方程开始：\n$$\nQ^{\\star}(s,a)=\\mathbb{E}\\!\\left[r(s,a,s')+\\gamma \\max_{a'\\in\\mathcal{A}} Q^{\\star}(s',a') \\,\\middle|\\, s,a\\right].\n$$\n离策略时序差分控制 (Q学习) 是一种随机近似方案，它通过应用以下更新，将 $Q_{t}(s,a)$ 迭代地移向贝尔曼最优算子的不动点：\n$$\nQ_{t+1}(s_{t},a_{t})=Q_{t}(s_{t},a_{t})+\\alpha\\Big(r_{t}+\\gamma \\max_{a'\\in\\mathcal{A}} Q_{t}(s_{t+1},a')-Q_{t}(s_{t},a_{t})\\Big),\n$$\n同时对于所有 $(s,a)\\neq (s_{t},a_{t})$，保持 $Q_{t+1}(s,a)=Q_{t}(s,a)$ 不变。此处 $r_{t}=r(s_{t},a_{t},s_{t+1})$。给定 $\\alpha=0.5$ 和 $\\gamma=0.9$，我们初始化 $Q_{0}(s,a)=0$（对于所有 $(s,a)$）。为简洁起见，请注意奖励结构意味着：若 $(s_{t},a_{t})=(s_{1},a)$，则 $r_{t}=1$，否则 $r_{t}=0$。\n\n我们现在逐步计算，并保留精确的有理数值。令 $\\alpha=\\frac{1}{2}$ 和 $\\gamma=\\frac{9}{10}$。\n\n步骤 1：$(s_{1},a)\\rightarrow s_{2}$。奖励 $r_{0}=1$。由于对所有 $(s,a)$ 都有 $Q_{0}(s,a)=0$，我们得到\n$$\n\\max_{a'} Q_{0}(s_{2},a')=0.\n$$\n因此\n$$\nQ_{1}(s_{1},a)=Q_{0}(s_{1},a)+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{10}\\cdot 0 - Q_{0}(s_{1},a)\\Big)=0+\\tfrac{1}{2}\\cdot 1=\\tfrac{1}{2}.\n$$\n所有其他Q值保持为0。\n\n步骤 2：$(s_{2},b)\\rightarrow s_{1}$。奖励 $r_{1}=0$。我们计算\n$$\n\\max_{a'} Q_{1}(s_{1},a')=\\max\\!\\left\\{Q_{1}(s_{1},a),Q_{1}(s_{1},b)\\right\\}=\\max\\!\\left\\{\\tfrac{1}{2},0\\right\\}=\\tfrac{1}{2}.\n$$\n因此\n$$\nQ_{2}(s_{2},b)=Q_{1}(s_{2},b)+\\tfrac{1}{2}\\Big(0+\\tfrac{9}{10}\\cdot \\tfrac{1}{2}-Q_{1}(s_{2},b)\\Big)=0+\\tfrac{1}{2}\\cdot \\tfrac{9}{20}=\\tfrac{9}{40}.\n$$\n所有其他Q值不变，所以 $Q_{2}(s_{1},a)=\\tfrac{1}{2}$。\n\n步骤 3：$(s_{1},b)\\rightarrow s_{1}$。奖励 $r_{2}=0$。计算\n$$\n\\max_{a'} Q_{2}(s_{1},a')=\\max\\!\\left\\{\\tfrac{1}{2},0\\right\\}=\\tfrac{1}{2}.\n$$\n因此\n$$\nQ_{3}(s_{1},b)=Q_{2}(s_{1},b)+\\tfrac{1}{2}\\Big(0+\\tfrac{9}{10}\\cdot \\tfrac{1}{2}-Q_{2}(s_{1},b)\\Big)=0+\\tfrac{1}{2}\\cdot \\tfrac{9}{20}=\\tfrac{9}{40}.\n$$\n其他值不变，所以 $Q_{3}(s_{1},a)=\\tfrac{1}{2}$ 且 $Q_{3}(s_{2},b)=\\tfrac{9}{40}$。\n\n步骤 4：$(s_{1},a)\\rightarrow s_{1}$。奖励 $r_{3}=1$。计算\n$$\n\\max_{a'} Q_{3}(s_{1},a')=\\max\\!\\left\\{Q_{3}(s_{1},a),Q_{3}(s_{1},b)\\right\\}=\\max\\!\\left\\{\\tfrac{1}{2},\\tfrac{9}{40}\\right\\}=\\tfrac{1}{2}.\n$$\n因此\n$$\n\\begin{aligned}\nQ_{4}(s_{1},a) &= Q_{3}(s_{1},a)+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{10}\\cdot \\tfrac{1}{2}-Q_{3}(s_{1},a)\\Big) \\\\\n&= \\tfrac{1}{2}+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{20}-\\tfrac{1}{2}\\Big) \\\\\n&= \\tfrac{1}{2}+\\tfrac{1}{2}\\Big(\\tfrac{20}{20}+\\tfrac{9}{20}-\\tfrac{10}{20}\\Big) \\\\\n&= \\tfrac{1}{2}+\\tfrac{1}{2}\\cdot \\tfrac{19}{20} \\\\\n&= \\tfrac{1}{2}+\\tfrac{19}{40} \\\\\n&= \\tfrac{20}{40}+\\tfrac{19}{40} \\\\\n&= \\tfrac{39}{40}.\n\\end{aligned}\n$$\n所有其他值不变。\n\n步骤 5：$(s_{2},a)\\rightarrow s_{2}$。奖励 $r_{4}=0$。计算\n$$\n\\max_{a'} Q_{4}(s_{2},a')=\\max\\!\\left\\{Q_{4}(s_{2},a),Q_{4}(s_{2},b)\\right\\}=\\max\\!\\left\\{0,\\tfrac{9}{40}\\right\\}=\\tfrac{9}{40}.\n$$\n因此\n$$\nQ_{5}(s_{2},a)=Q_{4}(s_{2},a)+\\tfrac{1}{2}\\Big(0+\\tfrac{9}{10}\\cdot \\tfrac{9}{40}-Q_{4}(s_{2},a)\\Big)=0+\\tfrac{1}{2}\\cdot \\tfrac{81}{400}=\\tfrac{81}{800}.\n$$\n\n步骤 6：$(s_{1},a)\\rightarrow s_{2}$。奖励 $r_{5}=1$。计算\n$$\n\\max_{a'} Q_{5}(s_{2},a')=\\max\\!\\left\\{Q_{5}(s_{2},a),Q_{5}(s_{2},b)\\right\\}=\\max\\!\\left\\{\\tfrac{81}{800},\\tfrac{9}{40}\\right\\}=\\tfrac{9}{40},\n$$\n因为 $\\tfrac{9}{40}=\\tfrac{180}{800}>\\tfrac{81}{800}$。所以\n$$\n\\begin{aligned}\nQ_{6}(s_{1},a)=Q_{5}(s_{1},a)+\\tfrac{1}{2}\\Big(1+\\tfrac{9}{10}\\cdot \\tfrac{9}{40}-Q_{5}(s_{1},a)\\Big) \\\\\n=\\tfrac{39}{40}+\\tfrac{1}{2}\\Big(1+\\tfrac{81}{400}-\\tfrac{39}{40}\\Big) \\\\\n=\\tfrac{39}{40}+\\tfrac{1}{2}\\Big(\\tfrac{400}{400}+\\tfrac{81}{400}-\\tfrac{390}{400}\\Big) \\\\\n=\\tfrac{39}{40}+\\tfrac{1}{2}\\cdot \\tfrac{91}{400} \\\\\n=\\tfrac{39}{40}+\\tfrac{91}{800} \\\\\n=\\tfrac{780}{800}+\\tfrac{91}{800} \\\\\n=\\tfrac{871}{800}.\n\\end{aligned}\n$$\n\n因此，经过六次更新后，精确值为 $Q_{6}(s_{1},a)=\\tfrac{871}{800}$。",
            "answer": "$$\\boxed{\\frac{871}{800}}$$"
        },
        {
            "introduction": "复杂自适应系统的一个关键特征是从智能体之间简单的局部互动中涌现出宏观模式。最后一个练习将从分析性习题转向计算建模，这是研究复杂自适应系统的一项核心技能。您将通过编写一个仿真程序，探索一个简单的局部阈值启发式如何在网络上引发系统范围的采纳级联，从而为理解社会传染、创新扩散或金融恐慌等现象提供洞见 。",
            "id": "4125788",
            "problem": "你需要编写一个完整、可运行的程序，在随机网络上根据确定性阈值采纳规则来估计预期的级联分数。网络模型为 Erdős–Rényi (ER) 随机图 $G(n,p)$，其定义如下：图中共有 $n$ 个标记为 $\\{0,1,\\dots,n-1\\}$ 的节点，每个无序的不同节点对之间都以概率 $p$ 独立地由一条边连接。决策规则是确定性阈值规则：每个节点当且仅当其至少有 $\\theta$ 个邻居已采纳时，才会采纳。一组种子节点集最初被强制采纳，此后保持采纳状态。采纳过程以同步轮次进行，且采纳是不可逆的。\n\n此任务的基本依据是 Erdős–Rényi 随机图 $G(n,p)$ 的标准定义和单调确定性阈值更新规则。算法必须仅基于这些基础进行，不得引入任何捷径公式。\n\n你的程序必须通过蒙特卡洛模拟来估计最终级联分数的期望值。具体而言，对于每个测试用例：\n- 每次试验重新生成一个网络 $G(n,p)$。\n- 从 $n$ 个节点中无放回地均匀随机选择一个大小为 $k$ 的种子集。\n- 以种子集作为初始活跃节点，运行阈值为 $\\theta$ 的同步阈值采纳动态过程，直至收敛（无新节点采纳）。\n- 记录最终的级联分数，其定义为已采纳节点数除以 $n$，表示为小数。\n- 对指定的试验次数 $T$ 重复此过程，并输出该测试用例在所有试验中这些分数的平均值。\n\n为保证可复现性，使用固定的伪随机数生成器种子 $42$。此问题中没有物理单位，也没有角度。所有分数必须表示为小数。你的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表，列表中的每个结果都四舍五入到 $6$ 位小数（例如，$[0.123456,0.500000]$）。\n\n测试套件（每个测试用例是一个 $(n,p,\\theta,k,T)$ 元组）：\n- 基本情况（正常路径）：$(n,p,\\theta,k,T)=(500,0.01,3,5,200)$。\n- 空网络边界情况：$(n,p,\\theta,k,T)=(300,0.0,1,10,100)$。\n- 零阈值边界情况（立即完全采纳）：$(n,p,\\theta,k,T)=(300,0.01,0,5,50)$。\n- 阈值高于任何可能度数（采纳仅限于种子节点）：$(n,p,\\theta,k,T)=(400,0.05,401,2,50)$。\n- 高连通性与中等阈值：$(n,p,\\theta,k,T)=(200,0.2,5,5,100)$。\n\n你的程序必须在不使用任何外部数据的情况下实现以下功能：\n- 通过以概率 $p$ 独立地决定无序的不同节点对之间的每条边来构建 $G(n,p)$。\n- 从 $k$ 个随机种子开始，进行具有不可逆采纳和单调更新的同步阈值采纳动态过程。\n- 对每个测试用例，在 $T$ 次独立试验中进行蒙特卡洛平均。\n\n你的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表，结果按指定的测试用例顺序排列，每个结果都四舍五入到 $6$ 位小数。",
            "solution": "该问题要求在一个 Erdős–Rényi 随机图上，对一个确定性阈值采纳过程，估计其预期的最终级联分数。该估计将通过蒙特卡洛模拟进行。\n\n问题验证确认了该问题定义良好、有科学依据且编程上已明确规定。它基于网络科学中的标准模型——Erdős–Rényi 图 $G(n,p)$ 和确定性阈值规则——并为计算解决方案提供了所有必要的参数和步骤。测试用例包括标准场景以及用于测试实现鲁棒性的明确定义的边界情况。因此，该问题是有效的。\n\n解决方案是通过从第一性原理出发，以算法方式实现指定的模型来开发的。对于每次模拟试验，这涉及三个主要阶段：网络生成、种子集选择以及采纳动态过程的执行。\n\n**1. 网络生成：Erdős–Rényi 图 $G(n,p)$**\n\n网络是一个 Erdős–Rényi 随机图，记作 $G(n,p)$，有 $n$ 个节点和边概率 $p$。节点标记为 $V = \\{0, 1, \\dots, n-1\\}$。对于任何不同的节点对 $(i, j)$，其中 $i, j \\in V$ 且 $i \\ne j$，它们之间以概率 $p$ 创建一条无向边。对所有这样的节点对，此过程都是独立的。\n\n在计算上，该图可以用一个 $n \\times n$ 的邻接矩阵 $A$ 来表示，其中如果节点 $i$ 和 $j$ 之间存在边，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。由于图是无向的，矩阵 $A$ 是对称的，即 $A_{ij} = A_{ji}$。按照惯例，没有自环，所以对角线元素为零，$A_{ii} = 0$。\n\n要为给定的 $n$ 和 $p$ 构建此矩阵，我们考虑 $\\binom{n}{2} = \\frac{n(n-1)}{2}$ 个唯一的节点对。对于每一对，我们从 $[0,1)$ 的均匀分布中抽取一个随机数 $r$。如果 $r \\lt p$，我们便添加一条边。在实现中，这是通过为邻接矩阵的上三角部分生成随机值，然后将其反映到下三角部分以确保对称性来实现的。\n\n**2. 同步阈值采纳动态过程**\n\n在离散时间步 $t$，每个节点 $i$ 的状态由 $S_i(t) \\in \\{0, 1\\}$ 表示，其中 $S_i(t) = 1$ 表示采纳，$S_i(t) = 0$ 表示未采纳。\n\n- **初始化 ($t=0$):** 从节点集 $V$ 中无放回地均匀随机选择一个大小为 $k$ 的种子集 $S_{\\text{seed}} \\subset V$。对于所有节点 $i \\in S_{\\text{seed}}$，其状态被设置为 $S_i(0) = 1$。对于所有其他节点 $j \\notin S_{\\text{seed}}$，其状态为 $S_j(0) = 0$。这些种子节点在所有 $t > 0$ 的时间步中都保持不可逆的采纳状态。\n\n- **更新规则 ($t \\to t+1$):** 系统以同步轮次演化。在每一轮中，每个未采纳的节点 $i$ 都会评估其邻域。如果一个尚未采纳的节点 $i$ ($S_i(t) = 0$) 的已采纳邻居数量达到或超过一个固定的阈值 $\\theta$，该节点就会采纳。数学上，其在下一个时间步的状态更新如下：\n$$\nS_i(t+1) = 1 \\quad \\text{if} \\quad \\sum_{j=0}^{n-1} A_{ij} S_j(t) \\ge \\theta\n$$\n采纳是不可逆的；一旦一个节点采纳，它在所有后续时间步中都保持采纳状态。这可以表示为，如果 $S_i(t)=1$，则 $S_i(t+1) = 1$。同步的性质意味着所有节点都基于时间 $t$ 的网络状态进行评估，并同时更新它们的状态以形成时间 $t+1$ 的网络状态。\n\n- **收敛:** 该过程保证收敛。随着每个时间步的推移，已采纳节点的集合是单调不减的。由于节点总数 $n$ 是有限的，该过程最终必然会达到一个不动点，即在一轮中没有新的节点采纳。此时，$S(t+1) = S(t)$，该次试验的模拟终止。\n\n**3. 蒙特卡洛估计**\n\n目标是估计最终级联分数的期望值 $\\mathbb{E}[f_{\\text{final}}]$。单次试验的最终分数定义为 $f_{\\text{final}} = N_{\\text{adopted}} / n$，其中 $N_{\\text{adopted}}$ 是收敛时已采纳节点的总数。\n\n期望是在随机图的分布和种子集的随机选择上计算的。蒙特卡洛方法通过对大量独立试验 $T$ 的结果进行平均来近似这个期望。对于由元组 $(n,p,\\theta,k,T)$ 定义的每个测试用例，过程如下：\n1. 初始化总分数的累加器为 $0$。\n2. 对于 $i=1, \\dots, T$：\n    a. 生成一个新的随机图 $G_i \\sim G(n,p)$。\n    b. 选择一个新的大小为 $k$ 的随机种子集 $S_{\\text{seed},i}$。\n    c. 运行采纳动态模拟直至收敛。\n    d. 计算最终分数 $f_{\\text{final}}^{(i)}$。\n    e. 将此分数加到累加器中。\n3. 估计的期望是平均值：\n$$\n\\hat{\\mathbb{E}}[f_{\\text{final}}] = \\frac{1}{T} \\sum_{i=1}^{T} f_{\\text{final}}^{(i)}\n$$\n使用固定的伪随机数生成器种子 $42$ 可以确保在所有试验和测试用例中，整个随机选择序列的可复现性。该实现利用 `numpy` 进行高效的数组和矩阵运算，特别是使用矩阵-向量乘积 `A @ S(t)` 来并行计算所有节点的活跃邻居数量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_trial(n, p, theta, k, rng):\n    \"\"\"\n    Runs a single trial of the cascade simulation.\n\n    Args:\n        n (int): Number of nodes.\n        p (float): Edge probability for the Erdős–Rényi graph.\n        theta (int): Adoption threshold.\n        k (int): Size of the initial seed set.\n        rng (numpy.random.Generator): The random number generator.\n\n    Returns:\n        float: The final fraction of adopted nodes.\n    \"\"\"\n    # 1. Generate the adjacency matrix for G(n, p)\n    # This prevents creating a huge intermediate matrix for large n\n    adj_matrix = np.zeros((n, n), dtype=np.int8)\n    \n    # Get indices for the upper triangle of the matrix (excluding the diagonal)\n    # to represent unique unordered pairs of nodes.\n    upper_tri_indices = np.triu_indices(n, k=1)\n    num_possible_edges = len(upper_tri_indices[0])\n    \n    # Decide which edges exist based on probability p\n    edge_exists = rng.random(num_possible_edges) < p\n    \n    # Populate the adjacency matrix\n    adj_matrix[upper_tri_indices] = edge_exists\n    adj_matrix = adj_matrix + adj_matrix.T\n\n    # 2. Select a seed set of size k uniformly at random\n    if k > n:\n        raise ValueError(\"Seed set size k cannot be larger than number of nodes n.\")\n    seeds = rng.choice(n, size=k, replace=False)\n\n    # 3. Run the synchronous threshold adoption dynamics\n    adopted = np.zeros(n, dtype=bool)\n    adopted[seeds] = True\n\n    # Handle the theta=0 edge case efficiently: all nodes will adopt\n    if theta == 0:\n        return 1.0\n\n    while True:\n        num_adopted_before = np.sum(adopted)\n        \n        # Calculate the number of adopted neighbors for each node\n        # Using matrix multiplication is highly efficient.\n        # The result is a vector where the i-th element is the sum of adopted neighbors of node i.\n        num_adopted_neighbors = adj_matrix @ adopted\n\n        # Identify nodes that are not yet adopted but meet the threshold\n        potential_adopters_mask = ~adopted\n        \n        # A node will adopt if its number of adopted neighbors is >= theta\n        will_adopt_mask = (num_adopted_neighbors >= theta)\n        \n        # The set of newly adopting nodes are those that can and will adopt\n        newly_adopted_mask = potential_adopters_mask & will_adopt_mask\n\n        # If there are any new adoptions, update the state\n        if np.any(newly_adopted_mask):\n            adopted[newly_adopted_mask] = True\n        else:\n            # If no new adoptions, the cascade has converged\n            break\n            \n    # 4. Record the final cascade fraction\n    final_fraction = np.sum(adopted) / n\n    return final_fraction\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Use a fixed pseudorandom number generator for reproducibility.\n    # np.random.default_rng is the modern, preferred way.\n    rng = np.random.default_rng(42)\n\n    # Test suite (each test case is a tuple of (n, p, theta, k, T))\n    test_cases = [\n        # Base case (happy path)\n        (500, 0.01, 3, 5, 200),\n        # Empty network boundary\n        (300, 0.0, 1, 10, 100),\n        # Zero threshold edge case (immediate full adoption)\n        (300, 0.01, 0, 5, 50),\n        # Threshold above any possible degree (adoption restricted to seeds)\n        (400, 0.05, 401, 2, 50),\n        # High connectivity with moderate threshold\n        (200, 0.2, 5, 5, 100),\n    ]\n\n    results = []\n    for n, p, theta, k, T in test_cases:\n        trial_fractions = []\n        for _ in range(T):\n            fraction = run_single_trial(n, p, theta, k, rng)\n            trial_fractions.append(fraction)\n        \n        # Calculate the average fraction across all trials for this test case\n        avg_fraction = np.mean(trial_fractions)\n        results.append(avg_fraction)\n\n    # Format the output as a comma-separated list of 6-decimal-place floats\n    # enclosed in square brackets.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}