{
    "hands_on_practices": [
        {
            "introduction": "An agent's decision-making process is often a trade-off among multiple, sometimes conflicting, criteria. The very architecture of the decision rule an agent uses can have profound normative and behavioral implications. This exercise  explores the fundamental differences between compensatory rules, which allow trade-offs between criteria, and non-compensatory rules that enforce strict priorities. By analyzing a concrete scenario where these rules diverge, you will gain a deeper appreciation for core concepts like continuity and Pareto efficiency in the design of agent-based models.",
            "id": "4125860",
            "problem": "Consider a population of adaptive agents in a complex environment who must choose between two strategies, denoted by $A$ and $B$. Their decision-making is guided by three criteria normalized to the unit interval $[0,1]$: safety $x_{s}$, timeliness $x_{t}$, and energy efficiency $x_{e}$, with larger values indicating better performance on each criterion. Two decision rules are employed in the population: a lexicographic rule with strict priority $x_{s} \\succ x_{t} \\succ x_{e}$, and an additive weighted aggregation rule with fixed, positive weights that sum to $1$. The criteria values for the two strategies are:\n- For $A$: $x_{s}(A) = 0.95$, $x_{t}(A) = 0.60$, $x_{e}(A) = 0.50$.\n- For $B$: $x_{s}(B) = 0.92$, $x_{t}(B) = 0.90$, $x_{e}(B) = 0.70$.\nThe additive aggregation weights are $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$.\n\nFrom first principles and core definitions of lexicographic ordering and additive aggregation, determine which strategy each rule selects and analyze the normative implications in terms of continuity, compensation, Pareto monotonicity, and scale dependence. Then, select all statements below that are correct.\n\nA. Under the lexicographic rule with priority $x_{s} \\succ x_{t} \\succ x_{e}$, $A$ is selected; under additive weighted aggregation with $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$, $B$ is selected. The disagreement is due to the compensatory nature of the weighted rule, which permits trade-offs on lower-priority criteria.\n\nB. The additive weighted aggregation outcome is invariant under any strictly increasing transformation applied separately to each criterion (for example, replacing $x_{t}$ by $f(x_{t})$ where $f$ is strictly increasing), provided the weights are unchanged.\n\nC. Lexicographic ordering violates the Archimedean continuity axiom; no finite improvement in $x_{t}$ or $x_{e}$ can compensate for a shortfall in $x_{s}$. This non-compensatory structure can be normatively justified in safety-critical domains where the highest-priority criterion encodes a hard constraint.\n\nD. Both rules respect Pareto dominance: if $A$ weakly dominates $B$ in all criteria and strictly dominates $B$ in at least one criterion, then both rules will select $A$.\n\nE. In a population where agents learn via Reinforcement Learning (RL) with identical, time-invariant information, the aggregate behavior necessarily converges to the same long-run distribution over $A$ and $B$ under both lexicographic and additive weighted aggregation rules, because both rules are monotonic in each criterion.\n\nSelect all correct options.",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   **Strategies**: Two strategies, denoted by $A$ and $B$.\n-   **Criteria**: Three criteria, normalized to the interval $[0,1]$: safety ($x_s$), timeliness ($x_t$), and energy efficiency ($x_e$).\n-   **Criteria Values for Strategy A**: $x_{s}(A) = 0.95$, $x_{t}(A) = 0.60$, $x_{e}(A) = 0.50$.\n-   **Criteria Values for Strategy B**: $x_{s}(B) = 0.92$, $x_{t}(B) = 0.90$, $x_{e}(B) = 0.70$.\n-   **Decision Rule 1**: Lexicographic rule with strict priority $x_{s} \\succ x_{t} \\succ x_{e}$.\n-   **Decision Rule 2**: Additive weighted aggregation rule with fixed, positive weights $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$. The weights are stated to sum to $1$, which is confirmed: $0.50 + 0.40 + 0.10 = 1.00$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. Lexicographic ordering and additive weighted aggregation are standard, well-defined methods in multi-criteria decision analysis (MCDA). The problem provides all necessary numerical data and definitions to apply these rules and analyze their properties. The criteria and their values are presented in an objective, clear manner. The problem does not violate any fundamental principles, is not ambiguous, and leads to a unique, derivable solution. It is a well-posed problem statement relevant to the topic of heuristics and decision-making in complex systems.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of Choices for Each Rule\n\n**1. Lexicographic Rule**\n\nThe lexicographic rule with priority $x_{s} \\succ x_{t} \\succ x_{e}$ compares strategies based on the most important criterion first. If there is a tie, it proceeds to the next criterion in the priority order.\n\n-   **Compare on $x_s$**:\n    -   $x_s(A) = 0.95$\n    -   $x_s(B) = 0.92$\n-   Since $x_s(A) > x_s(B)$ (i.e., $0.95 > 0.92$), strategy $A$ is strictly preferred. The rule terminates without considering the lower-priority criteria, $x_t$ and $x_e$.\n-   **Conclusion**: The lexicographic rule selects strategy $A$.\n\n**2. Additive Weighted Aggregation Rule**\n\nThis rule calculates a utility score, $U$, for each strategy by summing the product of each criterion's value and its corresponding weight. The strategy with the highest utility score is selected. The utility function is $U(S) = w_s x_s(S) + w_t x_t(S) + w_e x_e(S)$.\n\n-   **Calculate Utility for Strategy A**:\n    -   $U(A) = (0.50)(0.95) + (0.40)(0.60) + (0.10)(0.50)$\n    -   $U(A) = 0.475 + 0.240 + 0.050 = 0.765$\n\n-   **Calculate Utility for Strategy B**:\n    -   $U(B) = (0.50)(0.92) + (0.40)(0.90) + (0.10)(0.70)$\n    -   $U(B) = 0.460 + 0.360 + 0.070 = 0.890$\n\n-   **Compare Utilities**:\n    -   Since $U(B) > U(A)$ (i.e., $0.890 > 0.765$), strategy $B$ is preferred.\n-   **Conclusion**: The additive weighted aggregation rule selects strategy $B$.\n\n### Option-by-Option Analysis\n\n**A. Under the lexicographic rule with priority $x_{s} \\succ x_{t} \\succ x_{e}$, $A$ is selected; under additive weighted aggregation with $(w_{s}, w_{t}, w_{e}) = (0.50, 0.40, 0.10)$, $B$ is selected. The disagreement is due to the compensatory nature of the weighted rule, which permits trade-offs on lower-priority criteria.**\n\n-   The first part of the statement, asserting that the lexicographic rule selects $A$ and the additive rule selects $B$, is confirmed by the calculations above.\n-   The second part provides the correct reason for this disagreement. The lexicographic rule is non-compensatory: the superiority of $A$ on the highest-priority criterion $x_s$ cannot be offset by any advantage of $B$ on lower-priority criteria ($x_t$, $x_e$). In contrast, the additive weighted rule is compensatory: the significant advantages of $B$ in timeliness ($x_t(B) = 0.90$ vs. $x_t(A) = 0.60$) and energy efficiency ($x_e(B) = 0.70$ vs. $x_e(A) = 0.50$) are able to compensate for its small disadvantage in safety ($x_s(B) = 0.92$ vs. $x_s(A) = 0.95$), leading to a higher overall utility score. The statement is entirely correct.\n\nVerdict: **Correct**.\n\n**B. The additive weighted aggregation outcome is invariant under any strictly increasing transformation applied separately to each criterion (for example, replacing $x_{t}$ by $f(x_{t})$ where $f$ is strictly increasing), provided the weights are unchanged.**\n\n-   This statement is a claim about the scale-dependence of the additive aggregation rule. The rule $U = \\sum_i w_i x_i$ presupposes that the criteria values $x_i$ are measured on at least an interval scale, where differences are meaningful. Applying an arbitrary non-linear, strictly increasing transformation (e.g., $f(x) = x^2$ or $f(x) = \\sqrt{x}$) to a criterion scale destroys the interval property.\n-   Let's construct a counterexample. Start with two alternatives $C = (x_1=0.8, x_2=0.2)$ and $D = (x_1=0.6, x_2=0.7)$. Let the weights be $(w_1=0.5, w_2=0.5)$.\n-   Original scores: $U(C) = 0.5(0.8) + 0.5(0.2) = 0.4 + 0.1 = 0.5$. $U(D) = 0.5(0.6) + 0.5(0.7) = 0.3 + 0.35 = 0.65$. So, $D$ is preferred to $C$.\n-   Now, apply the strictly increasing transformation $f(x_1) = x_1^3$ to the first criterion only.\n-   New scores: $U'(C) = 0.5(0.8^3) + 0.5(0.2) = 0.5(0.512) + 0.1 = 0.256 + 0.1 = 0.356$. $U'(D) = 0.5(0.6^3) + 0.5(0.7) = 0.5(0.216) + 0.35 = 0.108 + 0.35 = 0.458$. The preference $D \\succ C$ is maintained.\n-   Let's try a different transformation that magnifies differences at the high end of the scale. Let $f(x_2) = (x_2)^ {1/3}$.\n-   Recalculating with original $C, D$ and this new transformation on $x_2$: $U''(C) = 0.5(0.8) + 0.5((0.2)^{1/3}) \\approx 0.4 + 0.5(0.585) = 0.4 + 0.2925 = 0.6925$. $U''(D) = 0.5(0.6) + 0.5((0.7)^{1/3}) \\approx 0.3 + 0.5(0.888) = 0.3 + 0.444 = 0.744$. Preference is still $D \\succ C$.\n-   Let's reverse the initial preference. Let $C=(0.8, 0.2)$ and $D=(0.9, 0.05)$ and $w=(0.5, 0.5)$.\n-   $U(C)=0.5(0.8)+0.5(0.2)=0.5$. $U(D)=0.5(0.9)+0.5(0.05)=0.45+0.025=0.475$. So $C \\succ D$.\n-   Apply $f(x_1) = \\sqrt{x_1}$. $U'(C)=0.5\\sqrt{0.8}+0.5(0.2) \\approx 0.5(0.894)+0.1 = 0.447+0.1 = 0.547$. $U'(D)=0.5\\sqrt{0.9}+0.5(0.05) \\approx 0.5(0.949)+0.025 = 0.4745+0.025=0.4995$. The preference $C \\succ D$ is maintained.\n-   The statement is false. The property being described, invariance to monotonic transformations of criteria, is characteristic of ordinal aggregation methods, not cardinal ones like the weighted sum. The weighted sum is invariant only under positive affine transformations of the form $f(x_i) = a_i x_i + b_i$ ($a_i > 0$) if the weights and comparison threshold are adjusted accordingly, but certainly not for *any* strictly increasing function. The previous counterexample in the thought process was correct: $A=(0.8, 0.1)$, $B=(0.2, 0.9)$, $w=(0.6, 0.4)$. $U(A)=0.52 > U(B)=0.48$. Transform $x_1$ with $f(x_1)=x_1^3$. $U'(A) = 0.6(0.8^3)+0.4(0.1)=0.3472$. $U'(B) = 0.6(0.2^3)+0.4(0.9)=0.3648$. Now $U'(B)>U'(A)$. The preference reverses.\n\nVerdict: **Incorrect**.\n\n**C. Lexicographic ordering violates the Archimedean continuity axiom; no finite improvement in $x_{t}$ or $x_{e}$ can compensate for a shortfall in $x_{s}$. This non-compensatory structure can be normatively justified in safety-critical domains where the highest-priority criterion encodes a hard constraint.**\n\n-   The Archimedean property, applied to preferences, essentially states that no good is infinitely more desirable than another. Lexicographic preferences violate this by definition. For any two alternatives $Y = (x_s(Y), ...)$ and $Z = (x_s(Z), ...)$ where $x_s(Y) > x_s(Z)$, alternative $Y$ is preferred to $Z$ regardless of the values of the other criteria. An arbitrarily small advantage in $x_s$ outweighs an arbitrarily large advantage in $x_t$ or $x_e$. This is a violation of continuity.\n-   The normative justification is also sound. In many engineering, medical, or financial systems, certain criteria act as non-negotiable constraints. For example, a drug must be proven safe before its efficacy is even considered. A bridge must support a minimum load before its cost or aesthetic is evaluated. In such cases, modeling the preference structure as lexicographic is a rational and justifiable approach.\n\nVerdict: **Correct**.\n\n**D. Both rules respect Pareto dominance: if $A$ weakly dominates $B$ in all criteria and strictly dominates $B$ in at least one criterion, then both rules will select $A$.**\n\n-   Pareto Dominance Definition: Strategy $A$ Pareto dominates strategy $B$ if $x_i(A) \\ge x_i(B)$ for all criteria $i$, and $x_j(A) > x_j(B)$ for at least one criterion $j$.\n-   **Lexicographic Rule**: Assume $A$ Pareto dominates $B$. The rule checks criteria in order of priority $x_s, x_t, x_e$. Let the first criterion in this order where the values for $A$ and $B$ differ be $x_k$. Because $A$ Pareto dominates $B$, it must be that $x_k(A) > x_k(B)$ (it cannot be less). For all criteria $x_i$ with higher priority than $x_k$, we must have had $x_i(A) = x_i(B)$. Therefore, the lexicographic rule will select $A$ at step $k$. This holds for any priority ordering.\n-   **Additive Weighted Aggregation Rule**: Assume $A$ Pareto dominates $B$, and all weights $w_i$ are positive as given. The difference in utility is $U(A) - U(B) = \\sum_i w_i (x_i(A) - x_i(B))$. Since $x_i(A) \\ge x_i(B)$ for all $i$, each term $(x_i(A) - x_i(B))$ is non-negative. Since $w_i > 0$, each term $w_i(x_i(A) - x_i(B))$ is also non-negative. For the at least one criterion $j$ where $x_j(A) > x_j(B)$, the term $w_j(x_j(A) - x_j(B))$ is strictly positive. Therefore, the sum is strictly positive: $U(A) - U(B) > 0$, which implies $U(A) > U(B)$. The rule will select $A$.\n-   Both rules indeed respect Pareto dominance.\n\nVerdict: **Correct**.\n\n**E. In a population where agents learn via Reinforcement Learning (RL) with identical, time-invariant information, the aggregate behavior necessarily converges to the same long-run distribution over $A$ and $B$ under both lexicographic and additive weighted aggregation rules, because both rules are monotonic in each criterion.**\n\n-   This statement connects the static decision rules to dynamic learning behavior. A population of RL agents learning to maximize a scalar reward will, under standard assumptions, converge to selecting the action that provides the highest expected reward.\n-   The \"reward\" in this context would be derived from the outcome of the choice, evaluated by the decision rule.\n-   If the reward is the utility from the additive rule, agents will learn to choose $B$ because $U(B) = 0.890 > U(A) = 0.765$.\n-   If the \"reward\" is determined by the lexicographic preference, agents will learn to choose $A$, as it is the preferred option under that rule.\n-   Since the two rules select different strategies ($A$ vs. $B$), populations of RL agents optimizing for each respective rule will converge to different behaviors. One population will favor $A$, the other will favor $B$. The long-run distributions over choices will be different, not the same.\n-   The reason provided (\"because both rules are monotonic\") is insufficient. While both rules are monotonic (a property related to respecting Pareto dominance, as shown in D), they are fundamentally different aggregation functions and can, as shown, produce different preference orderings. Monotonicity alone does not guarantee that two different functions will produce the same output.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "In complex adaptive systems, decisions are rarely one-shot; agents learn and adapt their strategies based on feedback over time. This introduces the quintessential exploration-exploitation trade-off, elegantly captured by the multi-armed bandit framework. This practice problem  challenges you to calculate the expected performance of an agent using the simple but effective $\\epsilon$-greedy heuristic. This exercise will solidify your understanding of how balancing the search for new information with the use of current knowledge directly shapes an agent's long-term success.",
            "id": "4125797",
            "problem": "Consider a Multi-Armed Bandit (MAB) decision heuristic embedded in a complex adaptive system with $K=3$ arms whose stationary reward distributions are Bernoulli with means $(0.8, 0.7, 0.6)$. The decision-maker follows a constant $\\epsilon$-greedy rule over a fixed horizon $T=1000$: at each decision epoch $t \\in \\{1,2,\\dots,T\\}$, with probability $\\epsilon=0.05$, the decision-maker explores by selecting one of the $K$ arms uniformly at random, and with probability $1-\\epsilon$, the decision-maker exploits by selecting the arm believed to be best. Assume the reward at each pull is the realized Bernoulli outcome and rewards are additive over time. To isolate the effect of the heuristicâ€™s exploration-exploitation mixing in expectation, assume that exploitation selects the arm with the highest true mean. Under these assumptions, and using only fundamental probability definitions (linearity of expectation, total expectation for mixtures, and properties of Bernoulli random variables), compute two quantities: the expected number of exploratory pulls over the horizon and the expected cumulative reward over the horizon. Provide exact values; do not round. The final answer must be a single two-entry row in the form $\\begin{pmatrix}\\text{expected exploratory pulls} & \\text{expected cumulative reward}\\end{pmatrix}$, with no units, and expressed as real numbers.",
            "solution": "The problem statement has been critically validated and is deemed sound. It is scientifically grounded in the theory of reinforcement learning, specifically Multi-Armed Bandits (MAB). It is well-posed, providing all necessary parameters and a simplifying assumption that, while artificial, renders the problem uniquely solvable. The problem is objective and free of ambiguity. We may, therefore, proceed with a formal solution.\n\nThe problem asks for two quantities: the expected number of exploratory pulls and the expected cumulative reward over a time horizon $T=1000$. The decision-maker employs an $\\epsilon$-greedy strategy with exploration probability $\\epsilon = 0.05$. There are $K=3$ arms with Bernoulli reward distributions, whose means are given as $\\mu_1 = 0.8$, $\\mu_2 = 0.7$, and $\\mu_3 = 0.6$.\n\nFirst, we compute the expected number of exploratory pulls.\nLet $E_t$ be a Bernoulli indicator random variable such that $E_t=1$ if an exploratory action is taken at time step $t \\in \\{1, 2, \\dots, T\\}$, and $E_t=0$ if an exploitative action is taken. By the definition of the $\\epsilon$-greedy algorithm, the probability of exploration at any time step is constant and equal to $\\epsilon$.\n$$P(E_t=1) = \\epsilon$$\nThe expected value of this indicator variable is therefore:\n$$\\mathbb{E}[E_t] = 1 \\cdot P(E_t=1) + 0 \\cdot P(E_t=0) = \\epsilon$$\nThe total number of exploratory pulls over the horizon $T$, denoted $N_{exp}$, is the sum of these indicator variables:\n$$N_{exp} = \\sum_{t=1}^{T} E_t$$\nBy the linearity of expectation, the expected number of exploratory pulls is the sum of the individual expectations:\n$$\\mathbb{E}[N_{exp}] = \\mathbb{E}\\left[\\sum_{t=1}^{T} E_t\\right] = \\sum_{t=1}^{T} \\mathbb{E}[E_t]$$\nSince $\\mathbb{E}[E_t] = \\epsilon$ for all $t$, this simplifies to:\n$$\\mathbb{E}[N_{exp}] = \\sum_{t=1}^{T} \\epsilon = T\\epsilon$$\nSubstituting the given values $T=1000$ and $\\epsilon=0.05$:\n$$\\mathbb{E}[N_{exp}] = 1000 \\times 0.05 = 50$$\n\nNext, we compute the expected cumulative reward over the horizon $T$.\nLet $R_t$ be the random variable representing the reward obtained at time step $t$. The total cumulative reward, $R_{total}$, is the sum of rewards over all time steps:\n$$R_{total} = \\sum_{t=1}^{T} R_t$$\nBy the linearity of expectation, the expected total reward is:\n$$\\mathbb{E}[R_{total}] = \\mathbb{E}\\left[\\sum_{t=1}^{T} R_t\\right] = \\sum_{t=1}^{T} \\mathbb{E}[R_t]$$\nSince the process is stationary (the rule and distributions do not change over time), the expected reward at any step $t$, $\\mathbb{E}[R_t]$, is constant. Let's denote this as $\\mathbb{E}[R]$.\nTo find $\\mathbb{E}[R]$, we use the law of total expectation, conditioning on whether the action at step $t$ was exploratory or exploitative.\n$$\\mathbb{E}[R] = \\mathbb{E}[R | E_t=1] P(E_t=1) + \\mathbb{E}[R | E_t=0] P(E_t=0)$$\n$$\\mathbb{E}[R] = \\mathbb{E}[R | \\text{exploration}] \\cdot \\epsilon + \\mathbb{E}[R | \\text{exploitation}] \\cdot (1-\\epsilon)$$\n\nWe calculate each conditional expectation separately.\nIf the action is exploratory (with probability $\\epsilon$), one of the $K=3$ arms is chosen uniformly at random. The probability of selecting any specific arm is $\\frac{1}{K}$. The expected reward is the average of the means of all arms.\n$$\\mathbb{E}[R | \\text{exploration}] = \\frac{1}{K} \\sum_{k=1}^{K} \\mu_k = \\frac{1}{3}(\\mu_1 + \\mu_2 + \\mu_3)$$\n$$\\mathbb{E}[R | \\text{exploration}] = \\frac{1}{3}(0.8 + 0.7 + 0.6) = \\frac{2.1}{3} = 0.7$$\n\nIf the action is exploitative (with probability $1-\\epsilon$), the problem states to assume that the arm with the highest true mean is selected. The arm with the highest mean is arm $1$, with $\\mu_1 = 0.8$. Let this be $\\mu_{best}$.\n$$\\mu_{best} = \\max\\{\\mu_1, \\mu_2, \\mu_3\\} = \\max\\{0.8, 0.7, 0.6\\} = 0.8$$\nThe expected reward from an exploitative action is therefore:\n$$\\mathbb{E}[R | \\text{exploitation}] = \\mu_{best} = 0.8$$\n\nNow we substitute these conditional expectations back into the expression for $\\mathbb{E}[R]$:\n$$\\mathbb{E}[R] = (0.7) \\cdot \\epsilon + (0.8) \\cdot (1-\\epsilon)$$\nSubstituting $\\epsilon=0.05$:\n$$\\mathbb{E}[R] = (0.7) \\cdot (0.05) + (0.8) \\cdot (1-0.05) = (0.7) \\cdot (0.05) + (0.8) \\cdot (0.95)$$\n$$\\mathbb{E}[R] = 0.035 + 0.76 = 0.795$$\nThe expected reward at any single step is $0.795$.\n\nFinally, the total expected cumulative reward over the horizon $T=1000$ is:\n$$\\mathbb{E}[R_{total}] = \\sum_{t=1}^{1000} \\mathbb{E}[R_t] = T \\cdot \\mathbb{E}[R] = 1000 \\times 0.795 = 795$$\n\nThe two requested quantities are an expected number of exploratory pulls of $50$ and an expected cumulative reward of $795$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n50 & 795\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from a single agent interacting with a static environment to a system of interacting agents requires a shift to game-theoretic thinking. The Quantal Response Equilibrium (QRE) model is a powerful tool for this, relaxing the strong assumption of perfect rationality found in classical game theory. It posits that agents are more likely to choose better options, but are not flawless optimizers. This practice  provides a foundational exercise in deriving a QRE, guiding you from the principles of noisy utility maximization to a solvable fixed-point system for a strategic game.",
            "id": "4125832",
            "problem": "Consider a two-player binary-action game known as matching pennies. The Row player (denoted by $R$) and the Column player (denoted by $C$) each choose one of two actions, Heads ($H$) or Tails ($T$). The Row player's payoff matrix is\n$$\n\\begin{array}{c|cc}\n & C:H & C:T \\\\\n\\hline\nR:H & 1 & -1 \\\\\nR:T & -1 & 1 \\\\\n\\end{array}\n$$\nand the Column player's payoff is the negative of the Row player's payoff, making this a zero-sum interaction. Assume both players use a logit heuristic consistent with the Random Utility Model (RUM) under independent and identically distributed Gumbel noise, characterized by a rationality parameter $\\lambda>0$. Let $p\\in[0,1]$ be the probability that the Row player selects $H$, and let $q\\in[0,1]$ be the probability that the Column player selects $H$. Under the Quantal Response Equilibrium (QRE) concept, each player's mixed strategy must be a fixed point of their logit response to the opponent's mixed strategy.\n\nStarting from the foundational definitions of expected utility and the well-tested result that Gumbel-distributed utility shocks in RUM imply logit choice probabilities proportional to $\\exp(\\lambda \\cdot \\text{utility})$, derive the fixed-point conditions for $(p,q)$ implied by QRE in this game and compute the QRE at precision parameter $\\lambda=2$. Report the mixed strategy probabilities $(p^{\\ast},q^{\\ast})$ for the Row and Column players respectively. Express your final pair of probabilities in decimal form, rounded to four significant figures.",
            "solution": "The matching pennies game is zero-sum with the Row player's payoff matrix given. Let $p\\in[0,1]$ denote the probability that the Row player chooses $H$, and $q\\in[0,1]$ denote the probability that the Column player chooses $H$.\n\nBy definition, the expected utility of an action is the expectation of the payoff with respect to the opponent's mixed strategy. For the Row player:\n- If the Row player chooses $H$, the expected utility is\n$$\nU_{R}(H \\mid q) = 1\\cdot q + (-1)\\cdot (1-q) = 2q - 1.\n$$\n- If the Row player chooses $T$, the expected utility is\n$$\nU_{R}(T \\mid q) = (-1)\\cdot q + 1\\cdot (1-q) = 1 - 2q = -\\left(2q - 1\\right).\n$$\n\nFor the Column player, whose payoff is the negative of the Row player's payoff:\n- If the Column player chooses $H$, the expected utility is\n$$\nU_{C}(H \\mid p) = (-1)\\cdot p + 1\\cdot (1-p) = 1 - 2p.\n$$\n- If the Column player chooses $T$, the expected utility is\n$$\nU_{C}(T \\mid p) = 1\\cdot p + (-1)\\cdot (1-p) = 2p - 1 = -\\left(1 - 2p\\right).\n$$\n\nUnder the Random Utility Model with Gumbel-distributed shocks, the logit choice rule is a well-tested consequence: the probability of choosing an action with deterministic utility $u$ among two actions with utilities $u$ and $v$ is\n$$\n\\frac{\\exp(\\lambda u)}{\\exp(\\lambda u) + \\exp(\\lambda v)}.\n$$\nApplying this to each player with two actions whose utilities are negatives of each other yields symmetric logistic forms. For the Row player,\n$$\np = \\frac{\\exp\\left(\\lambda U_{R}(H \\mid q)\\right)}{\\exp\\left(\\lambda U_{R}(H \\mid q)\\right) + \\exp\\left(\\lambda U_{R}(T \\mid q)\\right)} = \\frac{\\exp\\left(\\lambda (2q-1)\\right)}{\\exp\\left(\\lambda (2q-1)\\right) + \\exp\\left(-\\lambda (2q-1)\\right)}.\n$$\nThis simplifies to the logistic function\n$$\np = \\frac{1}{1 + \\exp\\left(-2\\lambda (2q - 1)\\right)}.\n$$\nSimilarly, for the Column player,\n$$\nq = \\frac{\\exp\\left(\\lambda U_{C}(H \\mid p)\\right)}{\\exp\\left(\\lambda U_{C}(H \\mid p)\\right) + \\exp\\left(\\lambda U_{C}(T \\mid p)\\right)} = \\frac{\\exp\\left(\\lambda (1-2p)\\right)}{\\exp\\left(\\lambda (1-2p)\\right) + \\exp\\left(-\\lambda (1-2p)\\right)} = \\frac{1}{1 + \\exp\\left(-2\\lambda (1 - 2p)\\right)}.\n$$\n\nThese are the fixed-point conditions for Quantal Response Equilibrium (QRE):\n$$\np = \\frac{1}{1 + \\exp\\left(-2\\lambda (2q - 1)\\right)}, \\qquad q = \\frac{1}{1 + \\exp\\left(-2\\lambda (1 - 2p)\\right)}.\n$$\n\nWe now set $\\lambda = 2$ and solve:\n$$\np = \\frac{1}{1 + \\exp\\left(-4 (2q - 1)\\right)}, \\qquad q = \\frac{1}{1 + \\exp\\left(-4 (1 - 2p)\\right)}.\n$$\n\nObserve that the game is symmetric and zero-sum, and the utilities for the two actions for each player are negatives of each other. Consider the candidate symmetric solution $p = \\frac{1}{2}$ and $q = \\frac{1}{2}$. Substituting $q = \\frac{1}{2}$ into the Row player's fixed-point equation, we have $2q - 1 = 0$, so\n$$\np = \\frac{1}{1 + \\exp\\left(0\\right)} = \\frac{1}{2}.\n$$\nSubstituting $p = \\frac{1}{2}$ into the Column player's fixed-point equation, we have $1 - 2p = 0$, so\n$$\nq = \\frac{1}{1 + \\exp\\left(0\\right)} = \\frac{1}{2}.\n$$\nThus $(p,q) = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$ satisfies the fixed-point conditions.\n\nTo argue uniqueness at $\\lambda = 2$, note that if $q > \\frac{1}{2}$, then $U_{R}(H \\mid q) = 2q - 1 > 0$, implying $p > \\frac{1}{2}$ by the strictly increasing logistic map. But then $U_{C}(H \\mid p) = 1 - 2p  0$, implying $q  \\frac{1}{2}$, which contradicts $q > \\frac{1}{2}$. A symmetric contradiction holds if $q  \\frac{1}{2}$. Therefore, the only consistent fixed point is $p = \\frac{1}{2}$ and $q = \\frac{1}{2}$.\n\nFinally, with $\\lambda = 2$, the QRE mixed strategy probabilities are $p^{\\ast} = \\frac{1}{2}$ and $q^{\\ast} = \\frac{1}{2}$. Expressed in decimal form and rounded to four significant figures, these are $0.5000$ and $0.5000$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.5000  0.5000\\end{pmatrix}}$$"
        }
    ]
}