## Introduction
The laws of physics, from the deformation of a solid under load to the flow of heat through a material, are most often expressed in the language of continuous mathematics: partial differential equations (PDEs). While these equations provide a complete description, they are rarely solvable by hand for problems of realistic complexity. The bridge from a continuous physical model to a predictive numerical simulation is built through **discretization**—the process of transforming infinite-dimensional PDEs into finite, solvable systems of algebraic equations. This transformation is the foundational task of computational science and engineering, enabling the simulation of everything from nanoscale material behavior to astrophysical phenomena.

However, this process is far from a simple "plug-and-play" operation. The choice of discretization method has profound consequences for the accuracy, stability, and efficiency of a simulation. A poorly chosen method can lead to unphysical oscillations, artificial energy loss, or results that simply fail to converge to the correct solution. This article addresses the critical knowledge gap between knowing the governing equations and being able to generate a reliable numerical solution. It serves as a comprehensive guide to the principles, mechanics, and applications of the most important [discretization methods](@entry_id:272547) used today.

Across the following chapters, you will build a robust understanding of this crucial field. The journey begins with the theoretical core in **Principles and Mechanisms**, where we will deconstruct how continuous fields are represented by discrete data and explore the essential criteria—consistency, stability, and convergence—that guarantee a reliable approximation. We will delve into the powerful weak formulation and see how it forms the basis for the ubiquitous Finite Element Method. Next, in **Applications and Interdisciplinary Connections**, we will see these methods in action, exploring how FEM, FVM, and BEM are tailored to solve complex problems in solid mechanics, fluid dynamics, and materials science. Finally, a series of **Hands-On Practices** will provide the opportunity to apply these concepts, solidifying your understanding of how to tackle numerical stability, [coupled physics](@entry_id:176278), and boundary conditions.

## Principles and Mechanisms

The transition from a continuous mathematical description of a physical system, embodied by a partial differential equation (PDE), to a finite, computable algebraic model is the central task of discretization. This chapter elucidates the fundamental principles and mechanisms that underpin modern [discretization methods](@entry_id:272547) used in [multiscale materials simulation](@entry_id:1128334) and other fields of computational science and engineering. We will explore how continuous field equations are transformed into [discrete systems](@entry_id:167412), what properties these [discrete systems](@entry_id:167412) must possess to be reliable, and how different methods achieve these goals.

### From Continuous Fields to Discrete Systems: The Core Task

A field equation, whether it describes heat transfer, mechanical deformation, or species concentration, represents an infinite-dimensional problem; the unknown field has a value at every point in the domain. Discretization replaces this infinite-dimensional problem with a finite one by seeking an approximate solution defined by a finite number of parameters, such as the field's values at a discrete set of points or nodes. This process invariably leads to a system of algebraic equations, typically linear or nonlinear, which can then be solved using numerical algorithms.

The success of a discretization method rests on three crucial properties: **consistency**, **stability**, and **convergence**.

*   **Consistency** measures how well the discrete equations approximate the original continuous PDE. A scheme is consistent if its **[local truncation error](@entry_id:147703)**—the residual that results from substituting the exact continuous solution into the discrete equations—vanishes as the grid spacing approaches zero.
*   **Stability** concerns the [boundedness](@entry_id:746948) of the numerical solution. A stable scheme ensures that small perturbations in the input data (such as initial conditions or rounding errors) do not lead to unbounded, catastrophic growth in the computed solution.
*   **Convergence** is the ultimate goal: the numerical solution should approach the true solution of the PDE as the discretization is refined (i.e., as the grid spacing and/or time step approach zero).

For a broad class of linear [initial value problems](@entry_id:144620), these three concepts are elegantly linked by the **Lax Equivalence Theorem**, which states that for a consistent discretization of a [well-posed problem](@entry_id:268832), stability is the necessary and [sufficient condition](@entry_id:276242) for convergence . This theorem elevates stability from a desirable feature to a cornerstone of reliable numerical simulation.

To illustrate the concept of consistency and order of accuracy, consider the Poisson equation $\nabla^{2} u = s$ on a two-dimensional domain. A common discretization on a uniform Cartesian grid with spacing $h$ is the five-point **Finite Difference Method (FDM)**, which approximates the Laplacian operator as:
$$
L_{h} u_{i,j} = \frac{u_{i+1,j} - 2 u_{i,j} + u_{i-1,j}}{h^{2}} + \frac{u_{i,j+1} - 2 u_{i,j} + u_{i,j-1}}{h^{2}}
$$
The [local truncation error](@entry_id:147703) $\tau_{i,j}$ is found by applying this discrete operator to the exact, smooth solution $u(x,y)$ and comparing it to the exact [differential operator](@entry_id:202628). Using Taylor series expansions around the point $(x_i, y_j)$, one can show that:
$$
L_{h} u(x_i, y_j) = (\nabla^{2} u)(x_i, y_j) + \frac{h^{2}}{12} \left( \frac{\partial^{4} u}{\partial x^{4}} + \frac{\partial^{4} u}{\partial y^{4}} \right) + O(h^{4})
$$
The truncation error is the difference between the discrete and continuous operators acting on the exact solution, so its leading-order term is $\tau_{i,j} \approx \frac{h^{2}}{12} (\frac{\partial^{4} u}{\partial x^{4}} + \frac{\partial^{4} u}{\partial y^{4}})$ . Since $\tau_{i,j} \to 0$ as $h \to 0$, the scheme is consistent. The rate at which it vanishes is governed by the power of $h$ in the leading term. Here, the error is proportional to $h^2$, so we say the scheme has a second-order **spatial [order of accuracy](@entry_id:145189)**.

### The Variational Approach: Weak Formulations

While FDM operates directly on the strong form of the PDE, many powerful methods, most notably the Finite Element Method (FEM), begin from a different mathematical standpoint: the **[weak formulation](@entry_id:142897)**. The primary motivation for this is to relax the [differentiability](@entry_id:140863) (regularity) requirements on the solution. A second-order PDE like the Poisson equation seems to require a solution that is twice differentiable. However, the weak form only requires first derivatives to exist in a generalized sense.

The weak formulation is derived by multiplying the PDE by a "[test function](@entry_id:178872)" $v$ and integrating over the domain $\Omega$. The key step is the application of **integration by parts** (or its multidimensional counterpart, the [divergence theorem](@entry_id:145271)), which transfers one order of differentiation from the unknown solution field $u$ to the [test function](@entry_id:178872) $v$.

Let's consider a general steady-state, second-order elliptic field equation, which models phenomena like diffusion or elasticity in a heterogeneous medium:
$$
-\nabla \cdot (A(\mathbf{x}) \nabla u(\mathbf{x})) = f(\mathbf{x}) \quad \text{in } \Omega
$$
Here, $u$ is the unknown scalar field, $f$ is a source term, and $A(\mathbf{x})$ is a material property tensor (e.g., conductivity or stiffness) that may vary with position $\mathbf{x}$ . Multiplying by a test function $v$ and integrating over $\Omega$ gives:
$$
-\int_{\Omega} v \, (\nabla \cdot (A \nabla u)) \, \mathrm{d}\mathbf{x} = \int_{\Omega} v f \, \mathrm{d}\mathbf{x}
$$
Using the [product rule](@entry_id:144424) and the divergence theorem, the left-hand side can be transformed:
$$
\int_{\Omega} \nabla v \cdot (A \nabla u) \, \mathrm{d}\mathbf{x} - \int_{\partial \Omega} v (A \nabla u) \cdot \mathbf{n} \, \mathrm{d}s = \int_{\Omega} v f \, \mathrm{d}\mathbf{x}
$$
where $\mathbf{n}$ is the outward unit normal on the boundary $\partial \Omega$. This is the weak form. Notice that both $u$ and $v$ now appear under only a single gradient operator. This allows for solutions that are continuous but whose derivatives may be discontinuous, a class of functions belonging to the **Sobolev space** $H^1(\Omega)$.

This formulation also provides an elegant and systematic way to handle boundary conditions. Boundary conditions are classified into two types:
1.  **Essential Boundary Conditions**: These are conditions imposed on the solution field $u$ itself, such as the **Dirichlet condition** $u=g$ on a part of the boundary $\Gamma_D$. In the standard Galerkin framework, these conditions are enforced directly on the space of admissible solution functions (the "[trial space](@entry_id:756166)"). The [test functions](@entry_id:166589) are chosen from a corresponding space where the homogeneous version of the condition holds (i.e., $v=0$ on $\Gamma_D$). As a result, the boundary integral $\int_{\Gamma_D} v (A \nabla u) \cdot \mathbf{n} \, \mathrm{d}s$ vanishes automatically.

2.  **Natural Boundary Conditions**: These are conditions on the derivatives of the solution, which naturally emerge from the weak formulation's boundary integral term. Examples include the **Neumann condition**, which prescribes the flux $(A \nabla u) \cdot \mathbf{n} = h$ on $\Gamma_N$, and the **Robin condition**, $(A \nabla u) \cdot \mathbf{n} + \beta u = r$ on $\Gamma_R$. These conditions are incorporated by substituting the prescribed values directly into the boundary integral.

Combining these leads to the final variational problem: Find $u$ satisfying the [essential boundary conditions](@entry_id:173524) such that for all valid [test functions](@entry_id:166589) $v$:
$$
\underbrace{\int_{\Omega} \nabla v \cdot (A \nabla u) \, \mathrm{d}\mathbf{x} + \int_{\Gamma_R} \beta u v \, \mathrm{d}s}_{\text{Bilinear form } a(u,v)} = \underbrace{\int_{\Omega} f v \, \mathrm{d}\mathbf{x} + \int_{\Gamma_N} h v \, \mathrm{d}s + \int_{\Gamma_R} r v \, \mathrm{d}s}_{\text{Linear functional } L(v)}
$$
This abstract equation, $a(u,v) = L(v)$, forms the foundation for the Finite Element Method .

### The Finite Element Method (FEM)

#### The Galerkin Principle and Conforming Spaces

The Finite Element Method discretizes the weak formulation. The core idea is the **Galerkin principle**: we seek an approximate solution $u_h$ from a finite-dimensional [trial space](@entry_id:756166) $V_h$ and require the [variational equation](@entry_id:635018) $a(u_h, v_h) = L(v_h)$ to hold for all [test functions](@entry_id:166589) $v_h$ in a finite-dimensional [test space](@entry_id:755876) $W_h$.

A fundamental distinction arises from the choice of [test space](@entry_id:755876) :
-   In the **Bubnov-Galerkin method**, the [test space](@entry_id:755876) is identical to the [trial space](@entry_id:756166) ($W_h = V_h$). This is the most common approach, especially for problems where the [bilinear form](@entry_id:140194) $a(\cdot, \cdot)$ is symmetric.
-   In the **Petrov-Galerkin method**, the [test space](@entry_id:755876) is different from the [trial space](@entry_id:756166) ($W_h \neq V_h$). This provides additional flexibility that can be exploited to enhance the stability of the numerical scheme, particularly for non-symmetric problems like those involving fluid advection.

A crucial concept in FEM is that of a **[conforming method](@entry_id:165982)**. A method is conforming if the discrete [trial space](@entry_id:756166) $V_h$ is a genuine subspace of the continuous solution space $V$. For second-order [elliptic problems](@entry_id:146817), the continuous [solution space](@entry_id:200470) is typically a Sobolev space like $H^1(\Omega)$. A key question is what properties a function in $V_h$, typically constructed from [piecewise polynomials](@entry_id:634113) on a mesh, must have to belong to $H^1(\Omega)$. A function is in $H^1(\Omega)$ if both the function itself and its weak first derivatives are square-integrable over $\Omega$. For a [piecewise polynomial](@entry_id:144637) function, its [weak gradient](@entry_id:756667) will contain singular (non-$L^2$) terms on element interfaces if the function has jumps across those interfaces. To avoid this, the function must be globally continuous. Therefore, for a standard FEM discretization of a second-order problem, the space $V_h$ must consist of globally continuous ($C^0$) functions. This $C^0$ continuity across element boundaries is the necessary and [sufficient condition](@entry_id:276242) for conformity in $H^1$ .

#### Building the Discrete System: Elements and Assembly

The practical power of FEM comes from its "divide and conquer" strategy. The domain $\Omega$ is partitioned (meshed) into simple geometric shapes called elements (e.g., triangles or quadrilaterals). The complex calculations are performed on a simple, standardized **reference element** $\hat{T}$ and then mapped to each "physical" element $T$ in the mesh.

Let's trace this process for a two-dimensional diffusion problem on a [triangular mesh](@entry_id:756169) using linear ($P_1$) elements .
1.  **Basis Functions on the Reference Element**: On a reference triangle $\hat{T}$, we define simple polynomial **basis functions** (or [shape functions](@entry_id:141015)) $\hat{N}_i$. For $P_1$ elements, these are linear polynomials defined such that $\hat{N}_i$ is 1 at node $i$ and 0 at all other nodes. For example, on a reference triangle with vertices at $(0,0)$, $(1,0)$, and $(0,1)$, the basis functions are $\hat{N}_1 = 1-\xi-\eta$, $\hat{N}_2 = \xi$, and $\hat{N}_3 = \eta$.

2.  **Isoparametric Mapping**: An **[affine mapping](@entry_id:746332)** $\mathbf{x} = F(\hat{\mathbf{x}})$ transforms the reference coordinates $\hat{\mathbf{x}}=(\xi, \eta)$ to the physical coordinates $\mathbf{x}=(x,y)$ of an actual element in the mesh. This mapping is defined by the basis functions themselves: $\mathbf{x} = \sum_i \mathbf{x}_i \hat{N}_i(\hat{\mathbf{x}})$, where $\mathbf{x}_i$ are the coordinates of the physical element's nodes. The derivative of this mapping is the **Jacobian matrix**, $\mathbf{B} = \frac{\partial \mathbf{x}}{\partial \hat{\mathbf{x}}}$.

3.  **Element Stiffness Matrix**: The discrete system of equations is $K\mathbf{U}=\mathbf{F}$, where $\mathbf{U}$ is the vector of unknown nodal values. The global **stiffness matrix** $K$ is assembled from individual **element stiffness matrices** $K^{(e)}$. The entries of an [element stiffness matrix](@entry_id:139369) are given by $K_{ij}^{(e)} = a(N_j, N_i)|_e$. For our diffusion example, this is:
    $$
    K_{ij}^{(e)} = \int_{T} (\nabla_{\mathbf{x}} N_i)^{\top} \mathbf{D} (\nabla_{\mathbf{x}} N_j) \, \mathrm{d}\mathbf{x}
    $$
    The gradients of the basis functions in physical coordinates, $\nabla_{\mathbf{x}} N_i$, are computed from their simpler counterparts in reference coordinates using the [chain rule](@entry_id:147422) and the Jacobian: $\nabla_{\mathbf{x}} N_i = (\mathbf{B}^{-1})^{\top} \nabla_{\hat{\mathbf{x}}} \hat{N}_i$. Since the gradients of $P_1$ basis functions and the determinant of the affine Jacobian are constant over the element, the integral simplifies to the value of the integrand multiplied by the element's area, $|T| = |\det(\mathbf{B})| |\hat{T}|$. For example, the element stiffness for a 1D [bar element](@entry_id:746680) of length $L_e$ and modulus $E_e$ is famously $K^{(e)} = \frac{E_e A}{L_e} \begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}$ .

4.  **Assembly**: The [global stiffness matrix](@entry_id:138630) $K$ is formed by summing the contributions of all element matrices. The **element connectivity**—a map from local element node numbers to global node numbers—dictates where each entry of $K^{(e)}$ is added into $K$. For instance, for a 1D bar composed of elements $(1,2)$ and $(2,3)$, the global stiffness $K_{22}$ at node 2 receives contributions from both element 1 (its second node) and element 2 (its first node): $K_{22} = K_{22}^{(1)} + K_{11}^{(2)}$ . This assembly process results in a global matrix that is highly **sparse** (most entries are zero), as connections only exist between neighboring nodes. This sparsity is a key feature of FEM and is exploited by specialized storage formats (like Compressed Sparse Row, CSR) and solvers.

#### Advanced Topics and Guarantees

While the Galerkin method provides a systematic way to discretize PDEs, this does not automatically guarantee a good solution. The stability of the resulting discrete system is paramount. For symmetric, coercive problems like pure diffusion, the Bubnov-Galerkin method is guaranteed to be stable and even provides the "best" possible approximation in the [energy norm](@entry_id:274966). However, for non-[symmetric operators](@entry_id:272489), such as in the [advection-diffusion equation](@entry_id:144002) $-\varepsilon \Delta u + \mathbf{b} \cdot \nabla u = f$, the standard Bubnov-Galerkin method can become unstable, especially when advection dominates diffusion (i.e., the element Péclet number $\mathrm{Pe}_K = \frac{\|\mathbf{b}\| h_K}{2\varepsilon}$ is large). This instability manifests as spurious, non-physical oscillations in the solution.

The general condition for stability of Galerkin methods is the **[inf-sup condition](@entry_id:174538)** (also known as the Ladyzhenskaya–Babuška–Brezzi or LBB condition), which requires that for any [trial function](@entry_id:173682) $v_h$, there exists a test function $w_h$ that can "see" it, in a sense quantified by the [bilinear form](@entry_id:140194):
$$
\inf_{0 \neq v_h \in V_h} \sup_{0 \neq w_h \in W_h} \frac{a(v_h,w_h)}{\|v_h\|_V \, \|w_h\|_W} \geq \beta > 0
$$
where $\beta$ is a constant independent of the mesh size $h$ . For [advection-dominated problems](@entry_id:746320), the standard Bubnov-Galerkin choice ($W_h = V_h$) fails this condition. Petrov-Galerkin methods are designed to fix this. The **Streamline Upwind Petrov-Galerkin (SUPG)** method modifies the [test space](@entry_id:755876) by adding a perturbation in the direction of the advective field $\mathbf{b}$. This effectively adds artificial diffusion only along [streamlines](@entry_id:266815), damping the oscillations without excessively smearing the solution. An important feature of this approach is that the added terms are proportional to the PDE's residual, meaning they vanish for the exact solution, and thus the method remains **consistent** . Even for symmetric problems, Petrov-Galerkin methods can be considered, but they generally result in non-symmetric stiffness matrices, sacrificing computational efficiency .

Beyond stability, it is often desirable for the numerical solution to inherit qualitative properties of the continuous solution. For a pure diffusion problem without sources, the maximum and minimum values of the field must occur on the boundary. A numerical scheme satisfying a **Discrete Maximum Principle (DMP)** will not produce spurious interior [extrema](@entry_id:271659). For a $\mathbb{P}_1$ [finite element discretization](@entry_id:193156), this property is intimately linked to the properties of the [stiffness matrix](@entry_id:178659). Specifically, the DMP holds if the [stiffness matrix](@entry_id:178659) is an **M-matrix**, which, among other things, requires all of its off-diagonal entries to be non-positive. For the stiffness matrix entry $A_{ij}$ corresponding to an edge shared by two triangles with opposite angles $\alpha$ and $\beta$, the value is $A_{ij} = -\frac{1}{2}(\kappa_1 \cot\alpha + \kappa_2 \cot\beta)$. For this to be non-positive, the angles $\alpha$ and $\beta$ must be non-obtuse. This leads to a strict geometric constraint on the mesh: to guarantee a DMP, the [triangulation](@entry_id:272253) must be non-obtuse (a Delaunay triangulation is a common choice) . This reveals a deep connection between the mathematical properties of the discrete operator and the quality of the underlying mesh.

### Beyond Conforming FEM: The Discontinuous Galerkin Method

Conforming FEM, with its requirement of global $C^0$ continuity, can be restrictive. For problems with highly [heterogeneous materials](@entry_id:196262), complex geometries, or solutions featuring sharp fronts or discontinuities, alternative approaches are attractive. The **Discontinuous Galerkin (DG)** method is a powerful framework that relinquishes the demand for continuity.

In DG methods, the solution is approximated by functions that are polynomials within each element but are allowed to be discontinuous across element faces. This added flexibility simplifies [mesh generation](@entry_id:149105) ([hanging nodes](@entry_id:750145) are allowed) and makes it easy to use different polynomial degrees in different elements ([p-adaptivity](@entry_id:138508)). The price to pay is that the coupling between elements, which was enforced by continuity in FEM, must now be built into the [variational formulation](@entry_id:166033) through terms on the element faces.

A popular and robust variant is the **Symmetric Interior Penalty Galerkin (SIPG)** method . The formulation begins, as usual, with element-wise integration by parts. This generates boundary terms on the faces of every element. These face integrals are then defined in terms of the **jumps** $[u] = u^+ - u^-$ and **averages** $\{ \mathbf{q} \} = \frac{1}{2}(\mathbf{q}^+ + \mathbf{q}^-)$ of the solution and its flux across a face. The SIPG [bilinear form](@entry_id:140194) for the diffusion equation consists of three parts:
1.  The standard sum of element integrals: $\sum_K \int_K \kappa \nabla u_h \cdot \nabla v_h \, \mathrm{d}\mathbf{x}$.
2.  Symmetric **consistency terms** on the faces, which weakly enforce the continuity of the solution and the flux: $-\sum_F \int_F (\{\kappa \nabla u_h \cdot \mathbf{n}\} [v_h] + \{\kappa \nabla v_h \cdot \mathbf{n}\} [u_h]) \, \mathrm{d}s$. These terms are designed to be consistent with the continuous problem.
3.  A **penalty term**: $\sum_F \int_F \sigma [u_h] [v_h] \, \mathrm{d}s$. This term penalizes the jump in the solution across faces, thereby weakly enforcing continuity. The **[penalty parameter](@entry_id:753318)** $\sigma$ must be chosen sufficiently large to ensure the coercivity (and thus stability) of the entire formulation. Its value is critical and must be scaled appropriately with the mesh size $h_F$, the polynomial degree $p$, and the local material properties. A robust choice for [heterogeneous media](@entry_id:750241) is $\sigma \sim \frac{p^2 \max(\kappa^+, \kappa^-)}{h_F}$ .

### Analysis of Time-Dependent Problems: Stability and Dispersion

For time-dependent problems, such as wave propagation or transient diffusion, stability analysis must account for the evolution in time. For linear problems with constant coefficients on [periodic domains](@entry_id:753347), **von Neumann stability analysis** provides a powerful tool. The method examines the behavior of a single Fourier mode, $u_j^n = G^n e^{i k x_j}$, where $n$ is the time level and $j$ is the spatial index. Substituting this ansatz into the discrete scheme yields the **amplification factor** $G(k)$, which describes how the amplitude of the mode with wavenumber $k$ is modified over a single time step. For an [explicit time-stepping](@entry_id:168157) scheme to be stable, the magnitude of the amplification factor must be less than or equal to one for all possible wavenumbers: $|G(k)| \le 1$ . For example, for an explicit upwind advection and centered diffusion scheme, this analysis leads to a coupled stability condition on the advective Courant number $\nu = c\Delta t/\Delta x$ and the diffusive number $r = D\Delta t/\Delta x^2$, typically of the form $\nu + 2r \le 1$ .

However, a stable scheme is not necessarily an accurate one, especially for long-time simulations of wave phenomena. Numerical schemes can introduce non-physical effects that distort the solution. These errors are classified as:
-   **Numerical Dissipation (or Damping)**: The amplitude of waves is artificially reduced. This is related to the imaginary part of the numerical frequency, $\mathrm{Im}(\omega)$. Schemes like the first-order upwind method are highly dissipative .
-   **Numerical Dispersion**: Different wave components travel at different speeds, causing a [wave packet](@entry_id:144436) to spread out and distort. This is related to the real part of the numerical frequency, $\mathrm{Re}(\omega)$.

The dispersive properties of a scheme are quantified by the **numerical [phase velocity](@entry_id:154045)** $c_p(k) = \mathrm{Re}(\omega(k))/k$, the speed at which a single sinusoidal wave of wavenumber $k$ travels, and the **numerical [group velocity](@entry_id:147686)** $c_g(k) = d\mathrm{Re}(\omega(k))/dk$, the speed at which the envelope of a wave packet travels. For the exact advection equation $u_t + a u_x = 0$, both velocities are equal to the constant physical speed $a$.

Different [discretization methods](@entry_id:272547) have vastly different dispersive and dissipative characteristics :
-   **Fourier [spectral methods](@entry_id:141737)**, which compute derivatives exactly in Fourier space, have no [spatial discretization](@entry_id:172158) error for resolved modes. Their numerical phase and group velocities are exactly $a$, making them ideal for high-fidelity wave propagation.
-   **Continuous Galerkin FEM** (with linear elements) is non-dissipative but is dispersive. All wave components travel slower than the physical speed ($c_p(k)  a$), an effect known as phase lag, which causes distortion in [wave packets](@entry_id:154698).
-   **Low-order FDM or DG schemes**, such as the first-order upwind scheme, are both highly dissipative and highly dispersive. Wave amplitudes are damped, and [wave packets](@entry_id:154698) are distorted by components traveling at incorrect speeds.

Understanding these properties is critical for selecting an appropriate discretization method. For problems requiring long-time integration of wave phenomena, minimizing both [numerical dispersion and dissipation](@entry_id:752783) is paramount to achieving physically meaningful results.