{
    "hands_on_practices": [
        {
            "introduction": "When approximating a complex target distribution $P$ with a simpler model $Q_{\\theta}$, the Kullback-Leibler (KL) divergence provides a robust measure of dissimilarity. However, the KL divergence is asymmetric, and the choice between minimizing $D_{\\mathrm{KL}}(P\\|Q_{\\theta})$ versus $D_{\\mathrm{KL}}(Q_{\\theta}\\|P)$ is not arbitrary, carrying profound implications for the resulting model. This exercise  uses a classic example to illuminate the distinct behaviors of these two variational principles, revealing why minimizing $D_{\\mathrm{KL}}(P\\|Q_{\\theta})$ leads to moment-matching properties and is foundational to the standard Relative Entropy Minimization framework.",
            "id": "3802799",
            "problem": "Consider a coarse-graining setting in equilibrium statistical mechanics: a microscopic configuration is a random variable $X$ with probability density $P_X(x) \\propto \\exp(-\\beta U(x))$, where $\\beta$ is the inverse temperature and $U(x)$ is the potential energy. A coarse observable $Y$ is defined by a measurable mapping $Y = \\xi(X)$, with induced coarse density $P_Y(y)$. We wish to approximate $P_Y(y)$ by a parametric model family $Q_{\\theta}(y)$ within the exponential family $Q_{\\theta}(y) = h(y)\\exp\\{\\theta^{\\top} T(y) - A(\\theta)\\}$, where $h(y)$ is a base measure, $T(y)$ is a vector of sufficient statistics, $\\theta$ is the natural parameter, and $A(\\theta)$ is the cumulant generating function (log-partition function). Two distinct variational principles are used in coarse-graining by relative entropy minimization: the information projection (I-projection), which minimizes $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$, and the moment projection (M-projection), which minimizes $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$, where $D_{\\mathrm{KL}}(R\\|S) = \\int r(y)\\log\\frac{r(y)}{s(y)}\\,dy$ for densities $r$ and $s$.\n\nTo concretely illustrate the difference between the I-projection and the M-projection, consider the following simple exponential family approximation problem. Let the target coarse distribution be a symmetric bimodal Gaussian mixture\n$$\nP_Y(y) = \\tfrac{1}{2}\\,\\mathcal{N}(-a, s^2)(y) \\;+\\; \\tfrac{1}{2}\\,\\mathcal{N}(a, s^2)(y),\n$$\nwith parameters $a>0$ and $s>0$, and let the model family be the univariate Gaussian family\n$$\nQ_{\\theta}(y) = \\mathcal{N}(m, v)(y),\n$$\nparametrized by the mean $m \\in \\mathbb{R}$ and variance $v > 0$. Assume the regime $a^2 \\gg s^2$ so that the mixture components are well separated.\n\nWhich of the following statements are correct in this coarse-graining context?\n\nA. For the given $P_Y$ and Gaussian model family $Q_{\\theta}$, the information projection minimizing $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$ is the Gaussian with $m = 0$ and $v = a^2 + s^2$.\n\nB. For the same setup, the moment projection minimizing $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$ necessarily matches both the mean and the variance of $P_Y$, i.e., it also yields $m = 0$ and $v = a^2 + s^2$.\n\nC. Under the separation condition $a^2 \\gg s^2$, the moment projection onto the Gaussian family tends to select a mean $m$ close to either $a$ or $-a$ and a variance $v$ close to $s^2$.\n\nD. In a general exponential family $Q_{\\theta}(y) = h(y)\\exp\\{\\theta^{\\top} T(y) - A(\\theta)\\}$, the information projection $Q_{\\theta^\\star}$ of any target $P_Y$ onto $Q$ is characterized by the moment-matching condition $\\mathbb{E}_{P_Y}[T(Y)] = \\mathbb{E}_{Q_{\\theta^\\star}}[T(Y)]$.\n\nE. Compared to the information projection, the moment projection is “mass-covering,” tending to spread $Q_{\\theta}$ mass across all modes of $P_Y$, whereas the information projection is “mode-seeking,” concentrating mass near a single mode.\n\nF. In coarse-graining, minimizing $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$ has a “zero-avoiding” bias: it penalizes placing $Q_{\\theta}$ mass where $P_Y$ is (near) zero; minimizing $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$ has a “zero-forcing” bias: it penalizes assigning zero by $Q_{\\theta}$ where $P_Y$ has mass.\n\nSelect all that apply.",
            "solution": "The problem statement is first validated according to the specified protocol.\n\n### Step 1: Extract Givens\n- Microscopic configuration: random variable $X$, probability density $P_X(x) \\propto \\exp(-\\beta U(x))$.\n- Coarse observable: $Y = \\xi(X)$ with induced coarse density $P_Y(y)$.\n- Approximating model family: $Q_{\\theta}(y)$, belonging to the exponential family $Q_{\\theta}(y) = h(y)\\exp\\{\\theta^{\\top} T(y) - A(\\theta)\\}$.\n- Information projection (I-projection): Minimizes the Kullback-Leibler (KL) divergence $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$.\n- Moment projection (M-projection): Minimizes the KL divergence $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$.\n- KL divergence definition: $D_{\\mathrm{KL}}(R\\|S) = \\int r(y)\\log\\frac{r(y)}{s(y)}\\,dy$.\n- Target coarse distribution: A symmetric bimodal Gaussian mixture $P_Y(y) = \\frac{1}{2}\\,\\mathcal{N}(-a, s^2)(y) + \\frac{1}{2}\\,\\mathcal{N}(a, s^2)(y)$, with $a>0$ and $s>0$.\n- Model family: The univariate Gaussian family $Q_{\\theta}(y) = \\mathcal{N}(m, v)(y)$, with mean $m \\in \\mathbb{R}$ and variance $v > 0$.\n- Assumption: The mixture components are well separated, $a^2 \\gg s^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n- **Scientifically Grounded:** The problem is firmly based on standard, well-established concepts in statistical mechanics, information theory, and machine learning. Coarse-graining, exponential families, and KL divergence minimization are canonical topics. The specific example of approximating a Gaussian mixture with a single Gaussian is a classic illustration of the properties of I- and M-projections.\n- **Well-Posed:** The problem provides a clear objective: to assess the correctness of several statements regarding two well-defined optimization problems. The target distribution and model family are explicitly defined.\n- **Objective:** The problem is posed using precise, unambiguous mathematical language. There are no subjective or opinion-based assertions in the setup.\n- **Flaw Checklist:**\n    1.  *Scientific/Factual Unsoundness:* None. The premises are standard theoretical constructs.\n    2.  *Non-Formalizable/Irrelevant:* None. The problem is a formal mathematical exercise.\n    3.  *Incomplete/Contradictory Setup:* None. All necessary definitions and conditions are provided.\n    4.  *Unrealistic/Infeasible:* None. This is a theoretical problem; physical feasibility is not a constraint.\n    5.  *Ill-Posed/Poorly Structured:* None. The optimization problems for the projections are well-defined.\n    6.  *Pseudo-Profound/Trivial:* None. The differences between the two projections are conceptually significant and not trivial.\n    7.  *Outside Scientific Verifiability:* None. All statements can be verified through mathematical derivation.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Derivations and Option Analysis\n\nFirst, we determine the moments of the target distribution $P_Y(y)$.\nThe mean of $P_Y(y)$ is:\n$$\n\\mathbb{E}_{P_Y}[Y] = \\int_{-\\infty}^{\\infty} y P_Y(y) \\,dy = \\frac{1}{2} \\int_{-\\infty}^{\\infty} y \\mathcal{N}(-a, s^2)(y) \\,dy + \\frac{1}{2} \\int_{-\\infty}^{\\infty} y \\mathcal{N}(a, s^2)(y) \\,dy\n$$\nBy definition of the mean of a Gaussian distribution, this becomes:\n$$\n\\mathbb{E}_{P_Y}[Y] = \\frac{1}{2}(-a) + \\frac{1}{2}(a) = 0\n$$\nThe second moment of $P_Y(y)$ is:\n$$\n\\mathbb{E}_{P_Y}[Y^2] = \\int_{-\\infty}^{\\infty} y^2 P_Y(y) \\,dy = \\frac{1}{2} \\mathbb{E}_{\\mathcal{N}(-a,s^2)}[Y^2] + \\frac{1}{2} \\mathbb{E}_{\\mathcal{N}(a,s^2)}[Y^2]\n$$\nFor a general Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$, the second moment is $\\mathbb{E}[Y^2] = \\text{Var}[Y] + (\\mathbb{E}[Y])^2 = \\sigma^2 + \\mu^2$. Applying this:\n$$\n\\mathbb{E}_{P_Y}[Y^2] = \\frac{1}{2}(s^2 + (-a)^2) + \\frac{1}{2}(s^2 + a^2) = \\frac{1}{2}(s^2 + a^2 + s^2 + a^2) = a^2 + s^2\n$$\nThe variance of $P_Y(y)$ is:\n$$\n\\text{Var}_{P_Y}[Y] = \\mathbb{E}_{P_Y}[Y^2] - (\\mathbb{E}_{P_Y}[Y])^2 = (a^2+s^2) - 0^2 = a^2+s^2\n$$\n\nNow, we evaluate each option.\n\n**A. For the given $P_Y$ and Gaussian model family $Q_{\\theta}$, the information projection minimizing $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$ is the Gaussian with $m = 0$ and $v = a^2 + s^2$.**\n\nThe information projection minimizes $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta}) = \\int P_Y(y) \\log(P_Y(y)) dy - \\int P_Y(y) \\log(Q_{\\theta}(y)) dy$. This is equivalent to maximizing the cross-entropy term $\\mathbb{E}_{P_Y}[\\log(Q_{\\theta}(Y))]$. The model is $Q_{\\theta}(y) = \\mathcal{N}(m,v)(y)$, so $\\log(Q_{\\theta}(y)) = -\\frac{1}{2}\\log(2\\pi v) - \\frac{(y-m)^2}{2v}$.\nThe objective is to maximize:\n$$\nJ(m,v) = \\mathbb{E}_{P_Y}\\left[-\\frac{1}{2}\\log(2\\pi v) - \\frac{(Y-m)^2}{2v}\\right] = -\\frac{1}{2}\\log(2\\pi v) - \\frac{1}{2v}\\mathbb{E}_{P_Y}[(Y-m)^2]\n$$\nExpanding the expectation:\n$$\n\\mathbb{E}_{P_Y}[(Y-m)^2] = \\mathbb{E}_{P_Y}[Y^2] - 2m\\mathbb{E}_{P_Y}[Y] + m^2 = (a^2+s^2) - 2m(0) + m^2 = a^2+s^2+m^2\n$$\nSo we maximize $J(m,v) = -\\frac{1}{2}\\log(2\\pi v) - \\frac{a^2+s^2+m^2}{2v}$. Setting the partial derivatives to zero:\n$$\n\\frac{\\partial J}{\\partial m} = -\\frac{2m}{2v} = -\\frac{m}{v} = 0 \\quad \\implies \\quad m=0\n$$\n$$\n\\frac{\\partial J}{\\partial v} = -\\frac{1}{2v} + \\frac{a^2+s^2+m^2}{2v^2} = 0 \\quad \\implies \\quad v = a^2+s^2+m^2\n$$\nSubstituting $m=0$ into the second equation gives $v=a^2+s^2$. Thus, the optimal parameters are $m=0$ and $v=a^2+s^2$. This corresponds to a Gaussian that matches the mean and variance of the target distribution $P_Y$.\n**Verdict: Correct.**\n\n**B. For the same setup, the moment projection minimizing $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$ necessarily matches both the mean and the variance of $P_Y$, i.e., it also yields $m = 0$ and $v = a^2 + s^2$.**\n\nThe moment projection minimizes $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$. This optimization problem does not, in general, lead to matching the moments of $P_Y$. The value $m=0, v=a^2+s^2$ corresponds to the I-projection. The M-projection exhibits different behavior, known as mode-seeking. It tries to place the mass of $Q_{\\theta}$ in regions where $P_Y$ is large. Since $P_Y$ is bimodal with modes at $\\pm a$ and a probability minimum at $y=0$ (for $a^2 \\gg s^2$), a Gaussian centered at $m=0$ is a poor fit from the M-projection's perspective, as its peak corresponds to a valley in $P_Y$. Therefore, the M-projection will not yield these parameters.\n**Verdict: Incorrect.**\n\n**C. Under the separation condition $a^2 \\gg s^2$, the moment projection onto the Gaussian family tends to select a mean $m$ close to either $a$ or $-a$ and a variance $v$ close to $s^2$.**\n\nThe M-projection minimizes $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y) = \\int Q_{\\theta}(y) \\log\\frac{Q_{\\theta}(y)}{P_Y(y)} dy$. This divergence becomes large if $Q_{\\theta}(y)$ is significant where $P_Y(y)$ is small. This is a \"zero-avoiding\" property. To minimize the divergence, $Q_{\\theta}$ must place its mass where $P_Y$ has mass. For the bimodal target $P_Y$, this means the unimodal Gaussian $Q_{\\theta}$ will achieve a lower divergence by aligning with one of the modes, rather than averaging them. A Gaussian with mean $m \\approx a$ and variance $v \\approx s^2$ would approximate the mode $\\mathcal{N}(a,s^2)$ well. By symmetry, $m \\approx -a, v \\approx s^2$ is an equally good solution. The optimization problem has two local minima corresponding to these mode-seeking solutions.\n**Verdict: Correct.**\n\n**D. In a general exponential family $Q_{\\theta}(y) = h(y)\\exp\\{\\theta^{\\top} T(y) - A(\\theta)\\}$, the information projection $Q_{\\theta^\\star}$ of any target $P_Y$ onto $Q$ is characterized by the moment-matching condition $\\mathbb{E}_{P_Y}[T(Y)] = \\mathbb{E}_{Q_{\\theta^\\star}}[T(Y)]$.**\n\nThis is a fundamental property of I-projections onto exponential families. To find the optimal parameters $\\theta^\\star$ that minimize $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$, we maximize $\\mathbb{E}_{P_Y}[\\log Q_{\\theta}(Y)]$ with respect to $\\theta$.\n$$\n\\mathbb{E}_{P_Y}[\\log Q_{\\theta}(Y)] = \\mathbb{E}_{P_Y}[\\log h(Y) + \\theta^{\\top} T(Y) - A(\\theta)] = \\mathbb{E}_{P_Y}[\\log h(Y)] + \\theta^{\\top}\\mathbb{E}_{P_Y}[T(Y)] - A(\\theta)\n$$\nTaking the gradient with respect to $\\theta$ and setting it to zero gives:\n$$\n\\nabla_{\\theta} \\left( \\theta^{\\top}\\mathbb{E}_{P_Y}[T(Y)] - A(\\theta) \\right) = \\mathbb{E}_{P_Y}[T(Y)] - \\nabla_{\\theta} A(\\theta) = 0\n$$\nA standard result for exponential families is that the gradient of the log-partition function $A(\\theta)$ yields the expectation of the sufficient statistics under the model distribution $Q_{\\theta}$, i.e., $\\nabla_{\\theta} A(\\theta) = \\mathbb{E}_{Q_{\\theta}}[T(Y)]$.\nTherefore, the optimality condition for the I-projection parameter $\\theta^{\\star}$ is:\n$$\n\\mathbb{E}_{P_Y}[T(Y)] = \\mathbb{E}_{Q_{\\theta^{\\star}}}[T(Y)]\n$$\nThis is the moment-matching condition for the sufficient statistics $T(Y)$. For the specific Gaussian case in option A, the sufficient statistics can be taken as $T(Y)=(Y, Y^2)$, and this general principle leads to matching the mean and second moment (and thus, variance).\n**Verdict: Correct.**\n\n**E. Compared to the information projection, the moment projection is “mass-covering,” tending to spread $Q_{\\theta}$ mass across all modes of $P_Y$, whereas the information projection is “mode-seeking,” concentrating mass near a single mode.**\n\nThis statement incorrectly reverses the qualitative behaviors of the two projections.\n- **Information projection ($D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$):** This projection matches the moments of the target distribution (as per option D). For a multi-modal target, this results in a broad, averaged approximation that covers all modes. In our example, it yields $\\mathcal{N}(0, a^2+s^2)$, which is a single broad Gaussian covering both modes of $P_Y$. This behavior is called **mass-covering**.\n- **Moment projection ($D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$):** This projection penalizes placing mass where the target has none. It finds a local minimum by fitting one of the target's modes, as explained for option C. This behavior is called **mode-seeking**.\nThe statement assigns \"mass-covering\" to the M-projection and \"mode-seeking\" to the I-projection, which is the opposite of the truth.\n**Verdict: Incorrect.**\n\n**F. In coarse-graining, minimizing $D_{\\mathrm{KL}}(Q_{\\theta}\\|P_Y)$ has a “zero-avoiding” bias: it penalizes placing $Q_{\\theta}$ mass where $P_Y$ is (near) zero; minimizing $D_{\\mathrm{KL}}(P_Y\\|Q_{\\theta})$ has a “zero-forcing” bias: it penalizes assigning zero by $Q_{\\theta}$ where $P_Y$ has mass.**\n\nLet's analyze the divergences.\n- **Minimizing $D_{\\mathrm{KL}}(Q\\|P) = \\int Q(y) \\log(Q(y)/P(y)) dy$ (M-projection):** If $Q(y)>0$ in a region where $P(y) \\to 0$, the ratio $Q(y)/P(y) \\to \\infty$, and the integral diverges. The optimization will thus favor solutions where $Q(y)$ is small (it avoids placing mass) wherever $P(y)$ is small. This is correctly termed a \"zero-avoiding\" bias.\n- **Minimizing $D_{\\mathrm{KL}}(P\\|Q) = \\int P(y) \\log(P(y)/Q(y)) dy$ (I-projection):** If $P(y)>0$ in a region where $Q(y) \\to 0$, the ratio $P(y)/Q(y) \\to \\infty$, and the integral diverges. The optimization is thus forced to place mass with $Q$ wherever $P$ has mass to avoid an infinite penalty. This is correctly described as a \"zero-forcing\" bias.\nThe statement accurately describes the characteristic biases of the two projections.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACDF}$$"
        },
        {
            "introduction": "After establishing the objective function $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$, the practical challenge becomes finding the optimal parameters $\\boldsymbol{\\theta}$ that minimize it. For any non-trivial model, this requires numerical optimization, with gradient-based methods being the most powerful tools. This practice  focuses on deriving the essential machinery for these optimizers—the gradient and the Hessian—for a general discrete coarse-grained model, connecting the optimization process to the fundamental statistical properties of the system.",
            "id": "3838736",
            "problem": "Consider a coarse-grained model for materials in which the microscopic configurations have been mapped to a finite set of discrete coarse states indexed by $i \\in \\{1,\\dots,N\\}$. Each coarse state $i$ is described by a feature vector $\\boldsymbol{\\phi}_i \\in \\mathbb{R}^d$. A parametric coarse-grained potential is defined as $U_{\\boldsymbol{\\theta}}(i) = \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i$, where $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ are the parameters to be optimized. At inverse temperature $\\beta = 1/(k_{\\mathrm{B}} T)$, the model distribution over the coarse states is the Boltzmann distribution $p_{\\boldsymbol{\\theta}}(i) = \\exp(-\\beta U_{\\boldsymbol{\\theta}}(i)) / Z(\\boldsymbol{\\theta})$ with partition function $Z(\\boldsymbol{\\theta}) = \\sum_{j=1}^N \\exp(-\\beta U_{\\boldsymbol{\\theta}}(j))$. The target distribution $q(i)$ over these states is obtained by a statistically robust mapping from atomistic simulations and is treated here as given. The optimization objective is Relative Entropy Minimization (REM), which minimizes the Kullback-Leibler divergence $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$.\n\nStarting from the fundamental definitions of the Boltzmann distribution, the partition function, and the Kullback-Leibler divergence,\n- $p_{\\boldsymbol{\\theta}}(i) = \\exp(-\\beta U_{\\boldsymbol{\\theta}}(i)) / Z(\\boldsymbol{\\theta})$,\n- $Z(\\boldsymbol{\\theta}) = \\sum_{j=1}^N \\exp(-\\beta U_{\\boldsymbol{\\theta}}(j))$,\n- $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}}) = \\sum_{i=1}^N q(i) \\log\\left( \\frac{q(i)}{p_{\\boldsymbol{\\theta}}(i)} \\right)$,\nderive, from first principles, the expressions for the gradient $\\nabla_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ and the Hessian $\\nabla^2_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ in terms of expectations with respect to $q$ and $p_{\\boldsymbol{\\theta}}$. The derivation must rely only on these definitions and standard properties of differentiation and logarithms; do not assume or invoke any shortcut formulas.\n\nThen, implement a program that computes, for each test case below, the gradient and Hessian of $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ using the derived formulas. Use numerically stable computation of $p_{\\boldsymbol{\\theta}}$ for large $\\beta$ by appropriate normalization of exponentials. No physical units need to be reported in the output; all quantities are dimensionless.\n\nTest Suite:\n- Test Case 1 (general case):\n  - $N = 5$, $d = 3$,\n  - $\\Phi = \\begin{bmatrix}\n  0.1 & 0.5 & -0.3 \\\\\n  0.0 & -0.2 & 0.8 \\\\\n  1.2 & 0.7 & 0.4 \\\\\n  -0.6 & 0.9 & -1.1 \\\\\n  0.3 & -0.4 & 0.2\n  \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix}0.05 \\\\ 0.15 \\\\ 0.40 \\\\ 0.30 \\\\ 0.10 \\end{bmatrix}$,\n  - $\\beta = 2.0$,\n  - $\\boldsymbol{\\theta} = \\begin{bmatrix}0.7 \\\\ -0.3 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test Case 2 (boundary case with collinear features leading to rank-deficient covariance):\n  - $N = 4$, $d = 2$,\n  - $\\Phi = \\begin{bmatrix}\n  1.0 & 2.0 \\\\\n  2.0 & 4.0 \\\\\n  3.0 & 6.0 \\\\\n  4.0 & 8.0\n  \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix}0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25 \\end{bmatrix}$,\n  - $\\beta = 1.0$,\n  - $\\boldsymbol{\\theta} = \\begin{bmatrix}0.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- Test Case 3 (edge case with large $\\beta$ causing concentration of $p_{\\boldsymbol{\\theta}}$):\n  - $N = 6$, $d = 2$,\n  - $\\Phi = \\begin{bmatrix}\n  -1.0 & 0.5 \\\\\n  0.2 & -0.1 \\\\\n  0.0 & 0.0 \\\\\n  0.8 & 1.2 \\\\\n  -0.5 & 2.0 \\\\\n  1.5 & -1.0\n  \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix}0.10 \\\\ 0.20 \\\\ 0.10 \\\\ 0.10 \\\\ 0.30 \\\\ 0.20 \\end{bmatrix}$,\n  - $\\beta = 50.0$,\n  - $\\boldsymbol{\\theta} = \\begin{bmatrix}0.05 \\\\ -0.02 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the top-level list corresponds to one test case and must be a list of floating-point numbers. For a given test case with feature dimension $d$, the list must contain $d + d^2$ numbers in the following order:\n- the $d$ components of the gradient $\\nabla_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$,\n- followed by the $d^2$ components of the Hessian $\\nabla^2_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ flattened in row-major order.\n\nFor example, if a test case has $d = 2$, the per-case list must have $2 + 4 = 6$ numbers: $[\\text{grad}_1, \\text{grad}_2, \\text{H}_{11}, \\text{H}_{12}, \\text{H}_{21}, \\text{H}_{22}]$. The printed output must be a list of such per-case lists for the three test cases in the order given above.",
            "solution": "We begin from the defining elements of the coarse-grained statistical model. The coarse-grained potential is $U_{\\boldsymbol{\\theta}}(i) = \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i$, where $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ and $\\boldsymbol{\\phi}_i \\in \\mathbb{R}^d$. The Boltzmann distribution at inverse temperature $\\beta$ over the discrete set $\\{1,\\dots,N\\}$ is\n$$\np_{\\boldsymbol{\\theta}}(i) = \\frac{\\exp\\left(-\\beta U_{\\boldsymbol{\\theta}}(i)\\right)}{Z(\\boldsymbol{\\theta})}\n= \\frac{\\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i\\right)}{\\sum_{j=1}^N \\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right)},\n$$\nwith partition function\n$$\nZ(\\boldsymbol{\\theta}) = \\sum_{j=1}^N \\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right).\n$$\nThe Kullback-Leibler divergence (Relative Entropy) from the target distribution $q$ to the model is\n$$\nD_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}}) = \\sum_{i=1}^N q(i) \\log\\left(\\frac{q(i)}{p_{\\boldsymbol{\\theta}}(i)}\\right).\n$$\n\nWe expand $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ using the definition of $p_{\\boldsymbol{\\theta}}$. First, note that\n$$\n\\log p_{\\boldsymbol{\\theta}}(i) = -\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i - \\log Z(\\boldsymbol{\\theta}).\n$$\nTherefore,\n\\begin{align*}\nD_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})\n&= \\sum_{i=1}^N q(i) \\log q(i) - \\sum_{i=1}^N q(i) \\log p_{\\boldsymbol{\\theta}}(i) \\\\\n&= \\sum_{i=1}^N q(i) \\log q(i) - \\sum_{i=1}^N q(i)\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i - \\log Z(\\boldsymbol{\\theta})\\right) \\\\\n&= \\underbrace{\\sum_{i=1}^N q(i) \\log q(i)}_{\\text{constant in } \\boldsymbol{\\theta}} + \\beta \\sum_{i=1}^N q(i)\\,\\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i + \\bigg(\\sum_{i=1}^N q(i)\\bigg)\\log Z(\\boldsymbol{\\theta}).\n\\end{align*}\nBecause $\\sum_{i=1}^N q(i) = 1$, this simplifies to\n$$\nD_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}}) = \\text{const} + \\beta\\, \\boldsymbol{\\theta} \\cdot \\mathbb{E}_{q}[\\boldsymbol{\\phi}] + \\log Z(\\boldsymbol{\\theta}),\n$$\nwhere $\\mathbb{E}_{q}[\\boldsymbol{\\phi}] = \\sum_{i=1}^N q(i)\\,\\boldsymbol{\\phi}_i$.\n\nWe now compute the gradient with respect to $\\boldsymbol{\\theta}$. The derivative of the constant is zero, and the derivative of the linear term is simply\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\left( \\beta\\, \\boldsymbol{\\theta} \\cdot \\mathbb{E}_{q}[\\boldsymbol{\\phi}] \\right)\n= \\beta\\, \\mathbb{E}_{q}[\\boldsymbol{\\phi}].\n$$\nFor the partition function term, we use the standard identity for the gradient of $\\log Z(\\boldsymbol{\\theta})$:\n\\begin{align*}\n\\nabla_{\\boldsymbol{\\theta}} \\log Z(\\boldsymbol{\\theta})\n&= \\frac{1}{Z(\\boldsymbol{\\theta})} \\nabla_{\\boldsymbol{\\theta}} Z(\\boldsymbol{\\theta})\n= \\frac{1}{Z(\\boldsymbol{\\theta})} \\sum_{j=1}^N \\nabla_{\\boldsymbol{\\theta}} \\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right) \\\\\n&= \\frac{1}{Z(\\boldsymbol{\\theta})} \\sum_{j=1}^N \\left(-\\beta \\boldsymbol{\\phi}_j\\right)\\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right)\n= -\\beta \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\,\\boldsymbol{\\phi}_j \\\\\n&= -\\beta\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}].\n\\end{align*}\nCombining these results yields the gradient\n$$\n\\nabla_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})\n= \\beta\\, \\mathbb{E}_{q}[\\boldsymbol{\\phi}] - \\beta\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]\n= \\beta\\left(\\mathbb{E}_{q}[\\boldsymbol{\\phi}] - \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]\\right).\n$$\n\nFor the Hessian, we differentiate the gradient. The term $\\beta\\, \\mathbb{E}_{q}[\\boldsymbol{\\phi}]$ is constant in $\\boldsymbol{\\theta}$ because $q$ is fixed, so its derivative is zero. We must differentiate $- \\beta\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]$ with respect to $\\boldsymbol{\\theta}$. Let us compute the matrix of second derivatives componentwise. Denote by $\\phi_{i,k}$ the $k$-th component of $\\boldsymbol{\\phi}_i$. The $k$-th component of $\\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]$ is $\\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k}$. Its derivative with respect to $\\theta_\\ell$ is\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_\\ell} \\left( \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\right)\n&= \\sum_{i=1}^N \\phi_{i,k} \\frac{\\partial p_{\\boldsymbol{\\theta}}(i)}{\\partial \\theta_\\ell}.\n\\end{align*}\nFrom the definition,\n$$\np_{\\boldsymbol{\\theta}}(i) = \\frac{\\exp(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i)}{Z(\\boldsymbol{\\theta})},\n$$\nwe have\n\\begin{align*}\n\\frac{\\partial p_{\\boldsymbol{\\theta}}(i)}{\\partial \\theta_\\ell}\n&= \\frac{-\\beta \\phi_{i,\\ell}\\exp(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i)}{Z(\\boldsymbol{\\theta})}\n- \\frac{\\exp(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i)}{Z(\\boldsymbol{\\theta})^2}\n\\frac{\\partial Z(\\boldsymbol{\\theta})}{\\partial \\theta_\\ell} \\\\\n&= -\\beta \\phi_{i,\\ell} p_{\\boldsymbol{\\theta}}(i)\n- p_{\\boldsymbol{\\theta}}(i)\\left( -\\beta \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right) \\\\\n&= -\\beta p_{\\boldsymbol{\\theta}}(i)\\left( \\phi_{i,\\ell} - \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right).\n\\end{align*}\nTherefore,\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_\\ell} \\left( \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\right)\n&= \\sum_{i=1}^N \\phi_{i,k} \\left[ -\\beta p_{\\boldsymbol{\\theta}}(i)\\left( \\phi_{i,\\ell} - \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right) \\right] \\\\\n&= -\\beta \\left[ \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\phi_{i,\\ell} - \\left( \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\right)\\left( \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right) \\right].\n\\end{align*}\nThis is the negative of $\\beta$ times the $(k,\\ell)$ component of the covariance of $\\boldsymbol{\\phi}$ under $p_{\\boldsymbol{\\theta}}$. Thus, the Hessian matrix is\n$$\n\\nabla^2_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})\n= \\beta^2\\, \\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi}),\n$$\nwith\n$$\n\\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi}) = \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^\\top] - \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]^\\top.\n$$\n\nAlgorithmic design for computation:\n- Compute the model probabilities $p_{\\boldsymbol{\\theta}}(i)$ using a numerically stable softmax-like transformation. Specifically, let $s_i = -\\beta\\, \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i$. Subtract $\\max_i s_i$ from all $s_i$ to prevent overflow, exponentiate to get unnormalized weights, and divide by their sum to normalize.\n- Compute $\\mathbb{E}_{q}[\\boldsymbol{\\phi}] = \\sum_{i=1}^N q(i)\\,\\boldsymbol{\\phi}_i$ and $\\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}] = \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i)\\,\\boldsymbol{\\phi}_i$.\n- Form the gradient $\\boldsymbol{g} = \\beta (\\mathbb{E}_{q}[\\boldsymbol{\\phi}] - \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}])$.\n- Compute the second moment $\\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^\\top] = \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i)\\,\\boldsymbol{\\phi}_i \\boldsymbol{\\phi}_i^\\top$ and the covariance $\\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi})$, then the Hessian $\\boldsymbol{H} = \\beta^2 \\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi})$.\n- Flatten the Hessian in row-major order and concatenate it after the gradient to form the per-case output list of length $d + d^2$.\n\nNumerical stability considerations:\n- For large $\\beta$, the unnormalized weights $\\exp(s_i)$ can be extremely small or large. Shifting $s_i$ by their maximum ensures that the largest exponent becomes $\\exp(0) = 1$, and others are $\\le 1$, preventing overflow and preserving ratios.\n- In the boundary case where features are perfectly collinear, the covariance (and hence the Hessian) is rank-deficient; this is expected and represents directions with zero curvature in the objective.\n\nThe implementation directly follows these derived expressions and produces, for each test case, the gradient and Hessian components in the required format. No physical units are needed because all quantities are inherently dimensionless in this discrete statistical setting.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef stable_boltzmann_probs(beta: float, theta: np.ndarray, Phi: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute p_theta(i) proportional to exp(-beta * theta dot phi_i) in a numerically stable way.\n    Phi: shape (N, d)\n    theta: shape (d,)\n    Returns p: shape (N,)\n    \"\"\"\n    # s_i = -beta * (theta · phi_i)\n    s = -beta * (Phi @ theta)\n    # Shift by max for numerical stability\n    s_shift = s - np.max(s)\n    w = np.exp(s_shift)\n    Z = np.sum(w)\n    # Normalize to get probabilities\n    p = w / Z\n    return p\n\ndef grad_hess_rem(beta: float, theta: np.ndarray, Phi: np.ndarray, q: np.ndarray):\n    \"\"\"\n    Compute gradient and Hessian of D_KL(q || p_theta) for discrete states with features Phi.\n    beta: scalar\n    theta: (d,)\n    Phi: (N, d)\n    q: (N,)\n    Returns:\n      grad: (d,)\n      hess: (d, d)\n    \"\"\"\n    # Model probabilities\n    p = stable_boltzmann_probs(beta, theta, Phi)\n    # Expectations under q and p\n    # E_q[phi] and E_p[phi]\n    E_q_phi = Phi.T @ q  # shape (d,)\n    E_p_phi = Phi.T @ p  # shape (d,)\n    # Gradient\n    grad = beta * (E_q_phi - E_p_phi)\n    # Second moment under p: sum_i p_i * phi_i phi_i^T\n    # Compute weighted outer products efficiently\n    # Shape (d, d)\n    # Use broadcasting: (N, d) * (N, 1) -> (N, d), then Phi.T @ (weighted Phi) to obtain sum_i p_i phi_i phi_i^T\n    weighted_Phi = Phi * p[:, None]\n    E_p_phi_phiT = Phi.T @ weighted_Phi\n    # Covariance under p\n    cov_p = E_p_phi_phiT - np.outer(E_p_phi, E_p_phi)\n    # Hessian\n    hess = (beta ** 2) * cov_p\n    return grad, hess\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (beta, theta, Phi, q)\n    test_cases = []\n\n    # Test Case 1\n    beta1 = 2.0\n    theta1 = np.array([0.7, -0.3, 0.5], dtype=float)\n    Phi1 = np.array([\n        [0.1, 0.5, -0.3],\n        [0.0, -0.2, 0.8],\n        [1.2, 0.7, 0.4],\n        [-0.6, 0.9, -1.1],\n        [0.3, -0.4, 0.2]\n    ], dtype=float)\n    q1 = np.array([0.05, 0.15, 0.40, 0.30, 0.10], dtype=float)\n\n    test_cases.append((beta1, theta1, Phi1, q1))\n\n    # Test Case 2 (collinear features, rank-deficient covariance)\n    beta2 = 1.0\n    theta2 = np.array([0.0, 0.0], dtype=float)\n    Phi2 = np.array([\n        [1.0, 2.0],\n        [2.0, 4.0],\n        [3.0, 6.0],\n        [4.0, 8.0]\n    ], dtype=float)\n    q2 = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)\n\n    test_cases.append((beta2, theta2, Phi2, q2))\n\n    # Test Case 3 (large beta)\n    beta3 = 50.0\n    theta3 = np.array([0.05, -0.02], dtype=float)\n    Phi3 = np.array([\n        [-1.0,  0.5],\n        [ 0.2, -0.1],\n        [ 0.0,  0.0],\n        [ 0.8,  1.2],\n        [-0.5,  2.0],\n        [ 1.5, -1.0]\n    ], dtype=float)\n    q3 = np.array([0.10, 0.20, 0.10, 0.10, 0.30, 0.20], dtype=float)\n\n    test_cases.append((beta3, theta3, Phi3, q3))\n\n    results = []\n    for beta, theta, Phi, q in test_cases:\n        grad, hess = grad_hess_rem(beta, theta, Phi, q)\n        # Flatten result: gradient components followed by Hessian flattened row-major.\n        per_case = grad.tolist() + hess.flatten(order='C').tolist()\n        results.append(per_case)\n\n    # Final print statement in the exact required format.\n    # Print a single top-level list of per-case lists.\n    print(f\"[{','.join('[' + ','.join(map(str, case)) + ']' for case in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice  synthesizes the principles of relative entropy minimization into a complete computational workflow, bridging the gap from physical theory to practical implementation. Starting with a continuous fine-grained potential, you will generate a discrete target distribution and then implement a full numerical optimization to parameterize a coarse-grained model. This capstone exercise demonstrates how the theoretical concepts are translated into a working algorithm, incorporating practical considerations like numerical stability and regularization.",
            "id": "3802808",
            "problem": "Consider a one-dimensional particle with position $x \\in \\mathbb{R}$ evolving in a potential energy landscape given by a double-well potential. The coarse-graining (CG) map bins continuous positions $x$ into a finite number of discrete bins. The aim is to construct a discrete CG model and determine its optimal log-probabilities by minimizing a regularized Kullback-Leibler divergence (KLD). The program you write must implement the following steps, starting from first principles and well-tested definitions.\n\n1. Fundamental base and target distribution. Let the double-well potential be $U(x) = a\\,(x^2 - b^2)^2$, where $a > 0$ and $b > 0$ are parameters. The thermal equilibrium distribution at inverse temperature $\\beta > 0$ is the Boltzmann distribution\n$$\np(x) = \\frac{1}{Z}\\,\\exp\\left(-\\beta\\,U(x)\\right),\n$$\nwith partition function\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp\\left(-\\beta\\,U(x)\\right)\\,\\mathrm{d}x,\n$$\nwhere $[x_{\\min}, x_{\\max}]$ is a finite domain known to contain essentially all the probability mass of $p(x)$ for the parameters under consideration.\n\n2. Coarse-graining map to discrete bins. Partition the interval $[x_{\\min}, x_{\\max}]$ into $K$ equal-width bins with edges $x_0 = x_{\\min} < x_1 < \\cdots < x_K = x_{\\max}$. The CG variable $i \\in \\{1,\\dots,K\\}$ denotes the bin index. The fine-to-coarse marginal probability for bin $i$ is\n$$\nP_i = \\int_{x_{i-1}}^{x_{i}} p(x)\\,\\mathrm{d}x,\n$$\nwhich defines a discrete probability mass function $\\mathbf{P} = (P_1,\\dots,P_K)$ satisfying $\\sum_{i=1}^{K} P_i = 1$ and $P_i \\ge 0$.\n\n3. Discrete model parameterization. Parameterize the CG model distribution $\\mathbf{Q} = (Q_1,\\dots,Q_K)$ via unconstrained log-probabilities $\\mathbf{l} = (l_1,\\dots,l_K)$ according to the softmax relation\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}.\n$$\nThis ensures $Q_i \\ge 0$ and $\\sum_{i=1}^K Q_i = 1$.\n\n4. Objective for relative entropy minimization with regularization. Define the Kullback-Leibler divergence (KLD) from the fine-grained CG marginal $\\mathbf{P}$ to the model $\\mathbf{Q}(\\mathbf{l})$ as\n$$\nD_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right).\n$$\nTo prevent overfitting and break the gauge invariance of $\\mathbf{l}$, introduce a quadratic regularization relative to a reference log-probability vector $\\mathbf{l}^{\\mathrm{ref}}$, with strength $\\lambda > 0$. The full objective is\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} \\left(l_i - l_i^{\\mathrm{ref}}\\right)^2.\n$$\nYou must minimize $\\mathcal{F}(\\mathbf{l})$ over $\\mathbf{l} \\in \\mathbb{R}^K$ to compute the optimal log-probabilities $\\mathbf{l}^\\star$.\n\n5. Numerical computation requirements. \n- Compute the bin probabilities $\\mathbf{P}$ by numerical integration of $\\exp(-\\beta U(x))$ over each bin and by computing the partition function $Z$ over $[x_{\\min}, x_{\\max}]$ to ensure proper normalization. Use sufficiently accurate numerical quadrature.\n- Minimize $\\mathcal{F}(\\mathbf{l})$ using a robust, gradient-based convex optimization procedure. You must explicitly construct both the objective $\\mathcal{F}(\\mathbf{l})$ and its gradient with respect to $\\mathbf{l}$ and use them in the optimizer.\n\n6. Test suite. Implement your program to compute $\\mathbf{l}^\\star$ for the following test cases, with equal-width bins over the specified domains and reference log-probabilities as given. For each case, output the optimal log-probability vector $\\mathbf{l}^\\star$ as a list of decimal floats. No physical units are required for the outputs.\n- Case A (happy path): $a = 1.0$, $b = 1.0$, $\\beta = 5.0$, $x_{\\min} = -2.5$, $x_{\\max} = 2.5$, $K = 5$, $\\lambda = 0.1$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0)$.\n- Case B (near-uniform marginal, very weak regularization): $a = 1.0$, $b = 1.0$, $\\beta = 0.1$, $x_{\\min} = -3.0$, $x_{\\max} = 3.0$, $K = 4$, $\\lambda = 10^{-6}$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0)$.\n- Case C (strong regularization with a skewed reference): $a = 1.0$, $b = 1.0$, $\\beta = 2.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 6$, $\\lambda = 10.0$, $\\mathbf{l}^{\\mathrm{ref}} = (-1,0,1,2,1,0)$.\n- Case D (low temperature, sharply bimodal): $a = 1.0$, $b = 1.0$, $\\beta = 50.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 8$, $\\lambda = 0.01$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0,0,0,0)$.\n\n7. Final output format. Your program should produce a single line of output containing the list of results for the above four cases, in the following format: a comma-separated list of lists of floats, enclosed in a single pair of square brackets, with each inner list corresponding to $\\mathbf{l}^\\star$ for one case, and with no additional text. For example:\n$$\n\\text{print } [[l^\\star_{A,1},\\dots,l^\\star_{A,K_A}],[l^\\star_{B,1},\\dots,l^\\star_{B,K_B}],\\dots].\n$$\nAll angles, if any, must be in radians; however, no angles appear in this problem. The outputs must be decimal floats. No percentage signs are permitted anywhere in the output. The numerical integrals and optimizations must be performed with sufficient precision to produce stable results.",
            "solution": "The problem requires the determination of optimal log-probabilities for a coarse-grained (CG) model by minimizing a regularized relative entropy (Kullback-Leibler divergence). The procedure involves several steps, from defining the underlying continuous system to performing a numerical optimization for the discrete model parameters.\n\n### Step 1: Target Probability Density and Coarse-Graining\n\nThe system is a one-dimensional particle in a double-well potential $U(x)$, given by:\n$$\nU(x) = a(x^2 - b^2)^2\n$$\nwhere $a > 0$ and $b > 0$. At thermal equilibrium with inverse temperature $\\beta > 0$, the particle's position $x$ follows the Boltzmann distribution over a specified domain $[x_{\\min}, x_{\\max}]$:\n$$\np(x) = \\frac{1}{Z} \\exp(-\\beta U(x))\n$$\nThe normalization constant, known as the partition function, is the integral of the unnormalized Boltzmann factor over this domain:\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\nThe coarse-graining procedure maps the continuous domain $[x_{\\min}, x_{\\max}]$ into a set of $K$ discrete bins. The interval is partitioned by $K+1$ equally spaced points $x_0, x_1, \\dots, x_K$ where $x_0 = x_{\\min}$ and $x_K = x_{\\max}$. The width of each bin is $\\Delta x = (x_{\\max} - x_{\\min}) / K$, and the $i$-th bin corresponds to the interval $[x_{i-1}, x_i]$.\n\nThe target probability mass function for the CG model, denoted by $\\mathbf{P} = (P_1, \\dots, P_K)$, is derived by integrating the fine-grained probability density $p(x)$ over each bin:\n$$\nP_i = \\int_{x_{i-1}}^{x_i} p(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\int_{x_{i-1}}^{x_i} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\nThese integrals must be computed numerically. A robust adaptive quadrature method, such as the one provided by `scipy.integrate.quad`, is appropriate for this task. The calculation proceeds by first computing $Z$ and then computing the integral for each bin $i$ and dividing by $Z$.\n\n### Step 2: The Coarse-Grained Model and Objective Function\n\nThe CG model distribution, $\\mathbf{Q} = (Q_1, \\dots, Q_K)$, is parameterized by a vector of unconstrained log-probabilities $\\mathbf{l} = (l_1, \\dots, l_K)$. The softmax function ensures that $\\mathbf{Q}$ is a valid probability distribution (i.e., $Q_i \\ge 0$ and $\\sum_i Q_i = 1$):\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}\n$$\nThe term $\\sum_{j=1}^{K} \\exp(l_j)$ can be numerically unstable if the values of $l_j$ are large or small. Its logarithm is the log-sum-exp function, $\\text{logsumexp}(\\mathbf{l}) = \\log(\\sum_j \\exp(l_j))$, which can be computed in a stable manner. Using this, we can write $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$.\n\nThe goal is to find the optimal parameters $\\mathbf{l}^\\star$ that make the model distribution $\\mathbf{Q}(\\mathbf{l})$ as close as possible to the target distribution $\\mathbf{P}$. This \"closeness\" is measured by the Kullback-Leibler divergence (KLD):\n$$\nD_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right) = \\sum_{i=1}^{K} P_i (\\log P_i - \\log Q_i(\\mathbf{l}))\n$$\nThe problem specifies adding a quadratic regularization term to the objective function, which penalizes deviations of $\\mathbf{l}$ from a reference vector $\\mathbf{l}^{\\mathrm{ref}}$. The complete objective function to be minimized is:\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2\n$$\nwhere $\\lambda > 0$ is the regularization strength. This objective function $\\mathcal{F}(\\mathbf{l})$ is strictly convex for $\\lambda > 0$, guaranteeing a unique minimizer $\\mathbf{l}^\\star$.\n\n### Step 3: Gradient-Based Optimization\n\nTo minimize $\\mathcal{F}(\\mathbf{l})$ efficiently, we use a gradient-based optimization algorithm, such as L-BFGS-B. This requires the gradient of $\\mathcal{F}(\\mathbf{l})$ with respect to each component $l_k$ of $\\mathbf{l}$.\n\nLet's derive the gradient $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$. We differentiate $\\mathcal{F}(\\mathbf{l})$ with respect to a component $l_k$:\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = \\frac{\\partial}{\\partial l_k} \\left( \\sum_{i=1}^{K} P_i \\log P_i - \\sum_{i=1}^{K} P_i \\log Q_i(\\mathbf{l}) \\right) + \\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right)\n$$\nThe term $\\sum P_i \\log P_i$ is constant with respect to $\\mathbf{l}$. The derivative of the regularization term is straightforward:\n$$\n\\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right) = 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\nNow we focus on the KLD term involving $\\mathbf{Q}(\\mathbf{l})$. Recalling $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$, its contribution to the objective is:\n$$\n-\\sum_{i=1}^{K} P_i (l_i - \\text{logsumexp}(\\mathbf{l})) = -\\sum_{i=1}^{K} P_i l_i + \\left(\\sum_{i=1}^{K} P_i\\right) \\text{logsumexp}(\\mathbf{l}) = -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l})\n$$\nwhere we have used the fact that $\\sum_i P_i = 1$. The derivative of this expression with respect to $l_k$ is:\n$$\n\\frac{\\partial}{\\partial l_k} \\left( -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l}) \\right) = -P_k + \\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right)\n$$\nThe derivative of the log-sum-exp function with respect to $l_k$ is known to be $Q_k(\\mathbf{l})$:\n$$\n\\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right) = \\frac{1}{\\sum_{j} \\exp(l_j)} \\cdot \\exp(l_k) = Q_k(\\mathbf{l})\n$$\nThus, the derivative of the KLD term is $Q_k(\\mathbf{l}) - P_k$. Combining all parts, the $k$-th component of the gradient is:\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = Q_k(\\mathbf{l}) - P_k + 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\nIn vector notation, the full gradient is:\n$$\n\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l}) = \\mathbf{Q}(\\mathbf{l}) - \\mathbf{P} + 2\\lambda (\\mathbf{l} - \\mathbf{l}^{\\mathrm{ref}})\n$$\n\n### Step 4: Algorithmic Implementation\n\nThe overall algorithm for each test case is as follows:\n1.  Define the potential $U(x)$, integrand $f(x) = \\exp(-\\beta U(x))$, and domain $[x_{\\min}, x_{\\max}]$.\n2.  Compute the partition function $Z = \\int_{x_{\\min}}^{x_{\\max}} f(x) \\, \\mathrm{d}x$ using numerical quadrature.\n3.  Establish the $K$ bin edges $x_0, \\dots, x_K$ over $[x_{\\min}, x_{\\max}]$.\n4.  Compute the target probability vector $\\mathbf{P}$ by numerically integrating $f(x)$ over each bin $[x_{i-1}, x_i]$ and dividing by $Z$.\n5.  Implement a function that takes $\\mathbf{l}$ as an argument and returns both the objective value $\\mathcal{F}(\\mathbf{l})$ and its gradient $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$, using the expressions derived above and numerically stable functions like `logsumexp`.\n6.  Use a numerical optimizer, such as `scipy.optimize.minimize` with the `L-BFGS-B` method, to find the vector $\\mathbf{l}^\\star$ that minimizes $\\mathcal{F}(\\mathbf{l})$. Provide the function from step $5$ along with an initial guess (e.g., $\\mathbf{l}_0 = \\mathbf{l}^{\\mathrm{ref}}$) to the optimizer.\n7.  The result of the optimization, $\\mathbf{l}^\\star$, is the solution for the given test case. This procedure is repeated for all provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the optimal log-probabilities for a coarse-grained model by minimizing\n    a regularized Kullback-Leibler divergence.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'a': 1.0, 'b': 1.0, 'beta': 5.0, 'xmin': -2.5, 'xmax': 2.5,\n         'K': 5, 'lam': 0.1, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case B\n        {'a': 1.0, 'b': 1.0, 'beta': 0.1, 'xmin': -3.0, 'xmax': 3.0,\n         'K': 4, 'lam': 1e-6, 'l_ref': [0.0, 0.0, 0.0, 0.0]},\n        # Case C\n        {'a': 1.0, 'b': 1.0, 'beta': 2.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 6, 'lam': 10.0, 'l_ref': [-1.0, 0.0, 1.0, 2.0, 1.0, 0.0]},\n        # Case D\n        {'a': 1.0, 'b': 1.0, 'beta': 50.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 8, 'lam': 0.01, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case['a']\n        b = case['b']\n        beta = case['beta']\n        xmin = case['xmin']\n        xmax = case['xmax']\n        K = case['K']\n        lam = case['lam']\n        l_ref = np.array(case['l_ref'])\n\n        # 1. Define potential and integrand\n        def U(x):\n            return a * (x**2 - b**2)**2\n\n        def integrand(x):\n            return np.exp(-beta * U(x))\n\n        # 2. Compute partition function Z\n        Z, _ = quad(integrand, xmin, xmax, epsabs=1e-12, epsrel=1e-12)\n        if Z == 0:\n            # This case should not happen with the given parameters\n            raise ValueError(\"Partition function Z is zero.\")\n            \n        # 3. Compute target marginal probabilities P\n        bin_edges = np.linspace(xmin, xmax, K + 1)\n        P = np.zeros(K)\n        for i in range(K):\n            bin_integral, _ = quad(integrand, bin_edges[i], bin_edges[i+1], epsabs=1e-12, epsrel=1e-12)\n            P[i] = bin_integral / Z\n        \n        # Ensure P is a valid probability distribution\n        P[P < 0] = 0\n        P /= np.sum(P)\n\n        # 4. Define objective function and its gradient\n        # Handle the P_i * log(P_i) term, where the result is 0 if P_i = 0\n        P_log_P = np.zeros_like(P)\n        non_zero_mask = P > 0\n        P_log_P[non_zero_mask] = P[non_zero_mask] * np.log(P[non_zero_mask])\n        dkl_const_term = np.sum(P_log_P)\n\n        def objective_and_grad(l, P_vec, lam_val, l_ref_vec, dkl_const):\n            \"\"\"\n            Computes the objective function and its gradient.\n            F(l) = D_KL(P||Q(l)) + lambda * ||l - l_ref||^2\n            \"\"\"\n            # Compute Q(l) using logsumexp for stability\n            lse = logsumexp(l)\n            log_Q = l - lse\n            Q = np.exp(log_Q)\n\n            # Objective Function F(l)\n            # D_KL = sum(P * (logP - logQ))\n            dkl = dkl_const - np.sum(P_vec * log_Q)\n            reg = lam_val * np.sum((l - l_ref_vec)**2)\n            obj_val = dkl + reg\n\n            # Gradient of F(l)\n            # grad(F) = Q - P + 2*lambda*(l - l_ref)\n            grad_vec = Q - P_vec + 2 * lam_val * (l - l_ref_vec)\n            \n            return obj_val, grad_vec\n\n        # 5. Perform the optimization\n        l_initial = np.copy(l_ref)\n        \n        result = minimize(\n            fun=objective_and_grad,\n            x0=l_initial,\n            args=(P, lam, l_ref, dkl_const_term),\n            method='L-BFGS-B',\n            jac=True,  # function returns both objective and gradient\n            options={'gtol': 1e-9, 'disp': False}\n        )\n\n        l_star = result.x\n        all_results.append(l_star.tolist())\n\n    # 6. Format the final output string\n    # E.g., [[-4.7,-0.7,0.3,-0.7,-4.7],[-0.1,0.0,0.0,-0.1]]\n    output_parts = []\n    for res_list in all_results:\n        # Format each list as '[val1,val2,...]'\n        formatted_list = f'[{\",\".join(map(str, res_list))}]'\n        output_parts.append(formatted_list)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}