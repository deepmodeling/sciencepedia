## 引言
在[原子尺度模拟](@entry_id:187783)领域，连接第一性原理计算的高精度与经典力场的高效率，是推动科学发现的关键挑战。[机器学习势函数](@entry_id:138428)（MLIPs）作为一种强大的代理模型，正是在这一精度与效率的鸿沟上架起了一座桥梁。然而，构建一个既准确又具有广泛适用性的MLIP，往往需要海量的、通过昂贵量子力学计算得到的训练数据，这构成了知识和实践上的一个主要瓶颈。本文旨在系统性地解决这一问题，深入探讨如何通过主动学习和物理知情训练范式，数据高效地构建稳健可靠的[机器学习势函数](@entry_id:138428)。

在接下来的内容中，读者将踏上一条从理论到实践的学习路径。“原理与机制”部分将奠定理论基础，深入剖析MLIPs必须遵守的物理对称性、局域性假设、主流架构（如图神经网络）的实现，并重点介绍物理知情训练和[主动学习](@entry_id:157812)框架的核心要素。“应用与跨学科交叉”部分将通过材料属性预测、复杂化学过程揭示和[多尺度建模](@entry_id:154964)等一系列真实案例，展示这些原理在解决材料科学、化学及其他交叉学科前沿问题中的强大威力。最后，在“动手实践”部分，我们将通过精心设计的编程练习，引导读者亲手实现[主动学习](@entry_id:157812)中的关键算法，将理论知识转化为解决问题的实践能力。

## 原理与机制

本章旨在深入阐述[机器学习势函数](@entry_id:138428)（MLIPs）的核心原理与工作机制。在引言章节的基础上，我们将不再赘述背景，而是直接深入探讨构建和训练这些势函数所必需的理论基础。我们将从定义一个满足基本物理对称性的[势函数](@entry_id:176105)出发，探讨其不同的架构实现，然后转向利用物理定律指导训练过程的先进方法。随后，我们将分析纯局部模型的局限性，并介绍处理[长程相互作用](@entry_id:140725)的混合方案。最后，我们将详细阐述[主动学习](@entry_id:157812)的完整框架，这是一种数据高效的模型构建策略，涵盖了从不确定性量化到设计自适应学习循环和形式化[停止准则](@entry_id:136282)的全过程。

### [机器学习势函数](@entry_id:138428)的基本原理

从根本上说，一个原子间势函数旨在近似描述在玻恩-奥本海默近似下，由原子核构型决定的[势能面](@entry_id:143655)（Potential Energy Surface, PES）。PES 是一个高维[标量场](@entry_id:151443) $E(\mathbf{R}, \mathbf{Z})$，它将 $N$ 个原子的核位置 $\mathbf{R} \in \mathbb{R}^{3N}$ 和原子种类（核电荷数） $\mathbf{Z} \in \mathbb{N}^{N}$ 映射到一个唯一的标量能量值。[机器学习势函数](@entry_id:138428)（MLIP）的目标就是创建一个能够精确、高效地逼近这个映射的函数。

#### 对称性与[不变性](@entry_id:140168)要求

任何物理上合理的势函数都必须遵守由空间的基本性质和[粒子不可区分性](@entry_id:152187)所施加的对称性约束。这些约束对于 MLIP 的泛化能力和物理真实性至关重要。

首先，[孤立系统](@entry_id:159201)的能量必须在刚性平移和旋转下保持不变，这源于[空间的均匀性](@entry_id:172987)和各向同性。数学上，这意味着对于任意平移向量 $\mathbf{t} \in \mathbb{R}^3$ 和任意正交[旋转矩阵](@entry_id:140302) $\mathbf{Q} \in \mathbb{R}^{3 \times 3}$（满足 $\mathbf{Q}^\top \mathbf{Q} = \mathbf{I}$），势函数必须满足**欧几里得[不变性](@entry_id:140168)**（Euclidean invariance）：

$E((\mathbf{I}_N \otimes \mathbf{Q})\mathbf{R} + \mathbf{1}\otimes \mathbf{t}, \mathbf{Z}) = E(\mathbf{R}, \mathbf{Z})$

其中 $\mathbf{I}_N$ 是 $N \times N$ [单位矩阵](@entry_id:156724)，$\mathbf{1}$ 是一个全为1的 $N \times 1$ 向量。

其次，同种类的原子是不可区分的。这意味着任意重排原子的索引标签，只要其位置和种类也相应地重排，系统的总能量就不应改变。这就是**排列不变性**（permutation invariance）。对于任意一个 $N \times N$ 的排列矩阵 $\mathbf{P}$，我们有：

$E((\mathbf{P} \otimes \mathbf{I}_3)\mathbf{R}, \mathbf{P}\mathbf{Z}) = E(\mathbf{R}, \mathbf{Z})$

一个有效的 MLIP 必须通过其架构设计或[数据增强](@entry_id:266029)来严格或近似地满足这些不变性。此外，为了在[分子动力学](@entry_id:147283)（MD）模拟中使用，[势函数](@entry_id:176105)必须是可微的，以便计算[保守力](@entry_id:170586)：$\mathbf{F}_i = -\nabla_{\mathbf{R}_i} E(\mathbf{R}, \mathbf{Z})$ 。

#### 局域性假设及其推论

为了实现对大规模系统的模拟，现代 MLIPs 普遍采用**局域性假设**（locality ansatz）。该假设认为，系统的总能量可以分解为各个原子能量贡献之和，而每个原子的能量仅由其局部原[子环](@entry_id:154194)境（即在一个有限截断半径 $r_c$ 内的邻近原子）决定：

$E(\mathbf{R}, \mathbf{Z}) \approx \sum_{i=1}^N \varepsilon_i(\mathcal{N}_i)$

其中 $\varepsilon_i$ 是原子 $i$ 的能量贡献，$\mathcal{N}_i$ 是其局部环境。这个分解是实现计算成本与[原子数](@entry_id:746561) $N$ 呈[线性标度](@entry_id:197235)（$O(N)$）的关键，使得模拟数百万甚至更多原子的系统成为可能。

在计算成本和表达能力的光谱中，MLIPs 占据了一个独特的生态位。与[表达能力](@entry_id:149863)有限、但计算速度极快（$O(N)$ 且前置因子小）的**经典[经验力场](@entry_id:1124410)**（例如，由成对项和键角项构成的[力场](@entry_id:147325)）相比，MLIPs 具有高得多的灵活性，能够从数据中学习复杂的高阶多体相互作用。与从第一性原理出发、极为精确但计算成本高昂（对于[Kohn-Sham密度泛函理论](@entry_id:144173)，通常为 $O(N^3)$ 或更高）的**从头计算**方法相比，MLIPs 在保持接近从头计算精度的同时，计算成本降低了数个数量级，尽管其单原子计算成本通常仍高于简单的[经验力场](@entry_id:1124410) 。

### MLIPs 的架构实现

为了满足上述物理原理，研究人员已经开发出多种 MLIP 架构。这些架构的核心区别在于它们如何表征局部原子环境并确保不变性。

#### 基于描述符的[势函数](@entry_id:176105)

一类重要的 MLIPs 遵循“先描述，后学习”的范式。其核心思想是为每个原子构建一个固定的、高维的[特征向量](@entry_id:151813)，即**描述符**（descriptor），该描述符能够唯一地（在旋转和排列[不变性](@entry_id:140168)下）表征其局部原子环境。然后，一个标准的[机器学习模型](@entry_id:262335)（如神经网络）将这个描述符映射到该原子的能量贡献 $\varepsilon_i$。

描述符的设计至关重要。例如，**[原子中心对称函数](@entry_id:174796)**（Atom-Centered Symmetry Functions, ACSF）通过构造原子间距离和三原子角度的标量函数（如高斯函数）来实现[不变性](@entry_id:140168)，因为距离和角度本身就是旋转不变的。其对角[度相关性](@entry_id:1123507)的敏感度取决于所选[对称函数](@entry_id:177113)的类型和参数 。

另一种更系统化的方法是**原子位置光滑重叠**（Smooth Overlap of Atomic Positions, SOAP）。SOAP 首先为中心原子构建一个邻近原子密度场 $\rho_i(\mathbf{r})$。然后，将该密度场展开在一组[径向基函数](@entry_id:754004)和[球谐函数](@entry_id:178380)的乘积上。展开系数 $c_{n\ell m}$ 在旋转下会以特定的方式变换（通过维格纳 D 矩阵）。通过构造这些系数的[功率谱](@entry_id:159996) $p_{n n' \ell} = \sum_m c_{n\ell m} c^*_{n'\ell m}$，可以得到一个严格旋转不变的描述符。SOAP 对角[度相关性](@entry_id:1123507)的分辨率可以通过控制球谐展开的截断阶数 $\ell_{\max}$ 来系统地提高 。

#### [图神经网络势](@entry_id:750016)函数

近年来，**[图神经网络](@entry_id:136853)**（Graph Neural Networks, GNNs）已成为构建 MLIPs 的一种功能强大的端到端方法。在这种范式中，原子系统被自然地表示为一个图，其中原子是节点，截断半径内的邻近关系是边。

GNN 势函数的核心是**[消息传递](@entry_id:751915)**（message passing）机制。在每一层消息传递中，每个原子（节点）会从其邻居那里收集“消息”（通常是关于邻居状态和相对位置的函数），并通过一个置换不变的聚合函数（如求和）来更新自身的状态（一个高维的隐藏向量 $\mathbf{h}_i$）。

$ \mathbf{h}_i^{(t+1)} = \text{Update}^{(t)} \left( \mathbf{h}_i^{(t)}, \text{Aggregate}^{(t)} \left( \left\{ \text{Message}^{(t)}(\mathbf{h}_i^{(t)}, \mathbf{h}_j^{(t)}, \mathbf{r}_{ij}) \mid j \in \mathcal{N}(i) \right\} \right) \right) $

通过堆叠 $T$ 个消息传递层，一个原子的最终状态 $\mathbf{h}_i^{(T)}$ 将能够编码其 $T$ 跳（$T$-hop）邻域内的信息。这意味着，GNN 能够通过组合成对的相对位置信息，自动学习和编码高阶的几何关联。例如，一层消息传递足以让模型感知到以中心原子为顶点的键角（[三体相互作用](@entry_id:1133110)），而两层[消息传递](@entry_id:751915)则允许信息从一个原子的邻居的邻居那里传来，从而能够感知到[二面角](@entry_id:185221)（四体相互作用）。因此，增加[消息传递](@entry_id:751915)层数 $T$ 可以系统地扩大模型能够表示的多体相互作用的阶数和范围 。最终，总能量由所有原子最终状态的贡献加和得到 $E = \sum_i \varepsilon(\mathbf{h}_i^{(T)})$，这自然地保证了排列不变性。

### 物理知情的训练范式

仅仅依靠数据进行黑箱回归，而不融入已知的物理定律，会使得[模型效率](@entry_id:636877)低下且容易产生不符合物理规律的预测。**物理知情**（physics-informed）的训练是开发可靠 MLIPs 的关键。

#### 施加守恒定律

除了通过架构设计来内生地满足对称性之外，我们还可以通过在损失函数中添加**惩罚项**（penalty term）来显式地强制模型遵守某些物理定律。一个典型的例子是动量守恒，它要求一个孤立系统的总受力为零。

考虑一个简单的思想实验：一个 ML 模型直接预测一维空间中每个粒子的力，$\hat{F}_i = a r_i + b$，其中 $\theta=(a,b)$ 是模型参数。这个模型本身不保证总力为零。为了强制执行该约束，我们可以构建一个二次惩罚损失项：

$\mathcal{L}_{\text{penalty}} = \frac{1}{2} \lambda \left( \sum_{i=1}^{N} \hat{F}_i \right)^2 = \frac{1}{2} \lambda \left( a \sum_i r_i + N b \right)^2$

其中 $\lambda$ 是一个正权重。这个损失项是光滑且凸的，并且当且仅当总力为零时取最小值零。在训练过程中最小化这个惩罚项，会驱使模型参数 $(a,b)$ 学会满足 $\sum \hat{F}_i \approx 0$。例如，对于一个包含3个粒子（位置为 $r_1=1, r_2=-2, r_3=3$）的系统，并设 $\lambda=2$，该惩罚项对参数的梯度可以被解析地计算出来，从而用于[梯度下降优化](@entry_id:634206) 。

#### 多任务[损失函数](@entry_id:634569)的权重设定

对于通过对能量求导来获得力的保守势模型，训练过程通常涉及一个包含能量和力的**多任务损失函数**：

$L = \lambda_E \left\| E - E^{\mathrm{ref}} \right\|^2 + \lambda_F \sum_{i=1}^N \left\| \mathbf{F}_i - \mathbf{F}_i^{\mathrm{ref}} \right\|^2$

如何选择权重 $(\lambda_E, \lambda_F)$ 是一个关键问题。一个有原则的方法源于**最大似然估计**（Maximum Likelihood Estimation, MLE）。假设来自参考计算（如DFT）的能量和力标签存在独立的高斯噪声，其方差分别为 $\sigma_E^2$ 和 $\sigma_F^2$。那么，最大化数据似然等价于最小化[负对数似然](@entry_id:637801)，这表明最优的权重应该与噪声方差的倒数成正比：

$\lambda_E \propto \frac{1}{\sigma_E^2}, \quad \lambda_F \propto \frac{1}{\sigma_F^2}$

这意味着我们应该更信任噪声较小的标签。在实践中，这些噪声方差（被称为**[偶然不确定性](@entry_id:634772)**）可以从重复计算或[DFT计算](@entry_id:1123635)的收敛残差等元数据中估计出来。

然而，纯粹的统计考虑可能不够。在MD模拟中，力的精度对于积分[运动方程](@entry_id:264286)和维持轨迹稳定性至关重要，而能量的微小误差则相对可容忍。因此，通常需要引入一个**物理优先级因子** $\beta > 1$，人为地增大力项的权重（即 $\lambda_F' = \beta \cdot \lambda_F$），以确保MD模拟的稳定性 。

一个更先进的方法是将噪声方差本身作为可学习的参数（称为同方差不确定性）。通过优化一个联合的负对数似然函数，模型可以在训练过程中自动学习到每个任务的相对权重，同时我们仍然可以通过施加正则项来体现[对力](@entry_id:159909)精度的物理偏好 。

### 突破局域性限制

局域性假设是MLIPs效率的基石，但它也构成了其根本的局限性，尤其是在处理[长程相互作用](@entry_id:140725)时。相互作用衰减的速度决定了局域性假设的有效性。

#### 局域性假设的失效

*   **金属**：在金属中，移动的导电电子会有效地**屏蔽**（screen）离子实的[库仑相互作用](@entry_id:747947)，使得有效相互作用势呈指数衰减，形式类似于[汤川势](@entry_id:139645) $V(r) \propto \exp(-r/\lambda)/r$，其中 $\lambda$ 是屏蔽长度。只要[截断半径](@entry_id:136708) $r_c$ 远大于 $\lambda$（$r_c \gg \lambda$），被截断的相互作用对能量和力的贡献就呈指数级减小。因此，对于金属，局域性假设是一个非常好的近似 。

*   **范德华系统**：在由中性分子或原子构成的系统中，主要的远程吸[引力](@entry_id:189550)是[伦敦色散力](@entry_id:138610)，其势能渐进行为是 $V(r) = -C_6/r^6$。虽然这种相互作用比库仑相互作用衰减得快，但它仍然是长程的。简单地在 $r_c$ 处截断会引入一个系统性的能量偏差。通过对 $r > r_c$ 的相互作用进行积分，可以估算出这个被忽略的能量贡献，其大小与 $1/r_c^3$ 成正比。这会导致模型系统性地**欠束缚**（underbind）。为了修正这个问题，需要添加一个解析的“[尾部校正](@entry_id:755799)”项或在模型中包含一个专门的非局部色散层 。

*   **离子/极性系统**：在含有显著局部电荷或[宏观极化](@entry_id:141855)的离子和极性材料中，库仑相互作用 $V(r) \propto 1/r$ 起主导作用。这种缓慢的代数衰减使得局域性假设完全失效。在周期性边界条件下，库仑相互作用的[晶格](@entry_id:148274)求和是**[条件收敛](@entry_id:147507)**的，其结果取决于求和的顺序和系统的宏观形状（例如，产生退[极化场](@entry_id:197617)）。任何有限的截断半径 $r_c$ 都无法捕获这种固有的非局域和集体效应  。

#### 处理长程静电的混合模型

为了解决纯局部模型在处理长程静电时的根本缺陷，最先进的方法是采用**混合方案**（hybrid scheme）。其核心思想是将总[能量分解](@entry_id:193582)为一个短程部分和一个长程部分：

$E_{\text{total}} = E_{\text{SR}}^{\text{ML}} + E_{\text{LR}}^{\text{phys}}$

其中，短程部分 $E_{\text{SR}}^{\text{ML}}$ 包含了所有复杂的、依赖于量子力学的相互作用（如[泡利排斥](@entry_id:143554)、[共价键](@entry_id:146178)等），由一个灵活的、局部的 MLIP 来学习。长程部分 $E_{\text{LR}}^{\text{phys}}$ 则由一个具有正确物理形式的解析模型来描述。对于[静电相互作用](@entry_id:166363)，这个解析模型就是**[埃瓦尔德求和](@entry_id:142359)**（Ewald summation）。

埃瓦尔德方法将[点电荷](@entry_id:263616)的[晶格](@entry_id:148274)求和巧妙地分解为两部分：一个在[实空间](@entry_id:754128)中快速收敛的短程项，和一个在[倒易空间](@entry_id:754151)中快速收敛的长程项。完整的能量表达式还包括一个[自能](@entry_id:145608)校正项。为了避免**双重计算**（double counting），短程的 MLIP $E_{\text{SR}}^{\text{ML}}$ 必须被训练来学习“扣除”了埃瓦尔德[实空间](@entry_id:754128)部分所描述的短程库仑相互作用之后的剩余部分。

这种混合方案的一个关键挑战是确定[原子电荷](@entry_id:204820) $q_i$。[原子电荷](@entry_id:204820)并非固定不变，而是会随着局部化学环境的变化而动态调整（极化效应）。因此，需要一个能够从原子构型 $\mathbf{R}$ 预测电荷 $\mathbf{q}(\mathbf{R})$ 的模型。为了保证能量是可微的，这个电荷模型本身也必须是可微的。**电荷均衡**（Charge Equilibration, QEq）模型就是一种物理上合理的选择。它通过最小化一个依赖于环境的[静电能](@entry_id:267406)泛函来确定一组满足电荷中性约束 $\sum_i q_i = Q_{\text{total}}$ 的电荷。现代方法使用机器学习来预测QEq模型中的参数（如依赖于环境的[电负性](@entry_id:147633)和硬度），从而将电荷预测与整体的 MLIP 框架无缝集成 。

### 主动学习：数据高效的模型构建引擎

尽管 MLIPs 计算高效，但训练它们需要大量高质量的标记数据，而这些数据通常来自昂贵的从头计算。**[主动学习](@entry_id:157812)**（Active learning）是一种智能的数据采集策略，旨在通过迭代地选择“最有用”的未标记数据点进行标记，从而以最少的数据量构建出最精确的模型。

#### [不确定性量化](@entry_id:138597)：主动学习的基石

主动学习的核心是**不确定性量化**（Uncertainty Quantification, UQ）。模型应该被要求查询它最“不确定”的构型。在机器学习中，预测不确定性可以分为两类：

*   **[偶然不确定性](@entry_id:634772)**（Aleatoric Uncertainty）：也称为数据不确定性，源于数据本身固有的噪声或随机性。即使拥有无限的数据，这种不确定性也无法消除。

*   **认知不确定性**（Epistemic Uncertainty）：也称为模型不确定性，源于模型本身因训练数据有限而产生的“无知”。在远离训练数据的构型空间区域，模型的认知不确定性通常很高。这种不确定性是可以通过增加数据来减小的。

对于 MLIPs，其训练数据通常来自高度精确且确定性的 DFT 计算（在固定的计算参数下）。这意味着数据本身的噪声（[偶然不确定性](@entry_id:634772)）可以忽略不计。因此，总的预测不确定性主要由**认知不确定性主导**。[主动学习](@entry_id:157812)的目标正是通过在认知不确定性高的区域采集新数据来有效降低这种模型“无知” 。

在实践中，量化认知不确定性的一个强大而通用的方法是使用**模型集成**（ensemble）。通过训练多个（例如，5-10个）从不同随机初始化开始的相同架构的模型，在对一个[新构型](@entry_id:199611)进行预测时，这些模型预测值的分歧（例如，标准差或方差）就可以作为认知不确定性的一个可靠度量 。

#### 內插与外推：定义模型的[适用域](@entry_id:172549)

MLIP 的预测可靠性高度依赖于查询构型与其训练数据分布的关系。这种关系最好在**描述符空间**中定义，而不是在原子的三维坐标空间中。

*   **內插**（Interpolation）：当一个查询构型的描述符落在训练数据描述符集合 $\mathcal{S}$ 的高密度区域内时，我们认为模型在进行内插。

*   **外推**（Extrapolation）：当查询构型的描述符移动到 $\mathcal{S}$ 的低密度区域或其支撑范围之外时，模型就在进行外推，其预测结果的可靠性会急剧下降。

我们可以使用**[马氏距离](@entry_id:269828)**（Mahalanobis distance）来度量一个新描述符点到训练数据分布中心的距离，该距离考虑了描述符特征的方差和协方差，是判断外推的有效指标  。

主动学习的一个高级应用是主动地探测模型在外推方向上的弱点。这可以通过以下步骤实现：首先，通过对训练描述符集 $\mathcal{S}$ 进行主成分分析（PCA），找到方差最小的主成分方向，这些就是数据覆盖最稀疏的“未采样方向”。然后，利用描述符对原子坐标的**[雅可比矩阵](@entry_id:178326)** $J(\mathbf{R}) = \partial \phi / \partial \mathbf{R}$ 的[伪逆](@entry_id:140762)，可以构造出一种原子坐标的微小扰动，该扰动能使描述符精确地沿着这个未采样方向移动。沿着这个构造出的路径进行一系列高精度的参考计算，就可以量化模型误差在特定外推方向上的增长率，并将这些最能暴露模型缺陷的构型加入训练集 。

#### 设计[主动学习](@entry_id:157812)循环

一个完整的“在线”（on-the-fly）主动学习循环通常与MD模拟集成在一起，其步骤如下：

1.  **初始化**：使用一小部分初始数据训练一个 MLIP [集成模型](@entry_id:912825)。
2.  **模拟**：使用当前集成模型的平均预测力来运行一小段MD模拟。
3.  **不确定性监测**：在MD的每一步（或每几步），计算当前构型的不确定性（例如，集成模型预测力的方差）和其与训练集的描述符空间距离。
4.  **查询决策**：如果不确定性或距离超过预设的阈值，则认为该构型是“有[信息量](@entry_id:272315)的”，将其标记为需要进行[高精度计算](@entry_id:200567)的候选。
5.  **预言机计算**：对选出的一批候选构型运行昂贵的从头计算（如DFT），获得其能量和力的“真实”标签。
6.  **模型更新**：将新获得的标记数据加入训练集，并重新训练或更新 MLIP 集成模型。
7.  **循环**：返回第2步，开始新一轮的MD模拟和学习。

这个循环中的**查询函数**（acquisition function）需要巧妙地平衡**探索**（exploration，在模型不确定的新区域采样以减少认知不确定性）和**利用**（exploitation，在已知重要但误差仍较大的区域采样以提高精度）。一个有效的策略是设计一个自适应的调度方案。例如，探索项的权重 $\gamma_t$ 可以与模型的平均认知不确定性 $\overline{\sigma_t^2}$ 和数据噪声方差 $\sigma_n^2$ 关联：

$\gamma_t \propto \frac{\overline{\sigma_t^2}}{\overline{\sigma_t^2} + \sigma_n^2}$

这个形式在物理上很直观：当模型不确定性远大于数据噪声时（$\overline{\sigma_t^2} \gg \sigma_n^2$），$\gamma_t$ 接近其最大值，优先进行探索；当模型不确定性远低于数据噪声时（$\overline{\sigma_t^2} \ll \sigma_n^2$），继续探索的收益递减，$\gamma_t$ 趋于零，学习的重点转向利用 。此外，查询决策中还可以加入物理知情的残差项，例如，优先选择那些不满足已知物理定律（如电荷守恒）的构型 。

#### 形式化的[停止准则](@entry_id:136282)

[主动学习](@entry_id:157812)循环是昂贵的，必须有一个明确的**[停止准则](@entry_id:136282)**（stopping criteria）。一个鲁棒的停止策略通常是以下三个条件的组合：

1.  **不确定性平台期**：模型的学习能力趋于饱和。具体表现为，在最近几轮主动学习中，MD模拟中遇到的平均不确定性不再显著下降，达到了一个平台。

2.  **目标性质收敛**：模拟的最终科学目标已经达成。这通常意味着某个我们关心的[宏观可观测量](@entry_id:751601)（如扩散系数、自由能等）的系综平均值在最近几轮模拟中已经稳定，并且其[统计误差](@entry_id:755391)在可接受的范围内。

3.  **置信区域覆盖率**：最终的MLIP已经足够“了解”在目标[热力学](@entry_id:172368)条件下所有重要的构型。这可以通过检查在最后一次长时MD模拟中，绝大多数（例如 > 99.9%）被采样的构型，其描述符都落在[训练集](@entry_id:636396)定义的置信区域内（例如，[马氏距离](@entry_id:269828)小于某个阈值），来量化地进行验证。

当这三个条件同时满足时，我们可以有信心地终止[主动学习](@entry_id:157812)过程，并使用最终得到的MLIP进行生产级别的科学模拟 。