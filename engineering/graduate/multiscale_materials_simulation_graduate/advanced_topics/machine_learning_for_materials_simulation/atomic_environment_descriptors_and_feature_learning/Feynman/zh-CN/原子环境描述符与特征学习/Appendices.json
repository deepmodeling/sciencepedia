{
    "hands_on_practices": [
        {
            "introduction": "Behler-Parrinello原子对称函数（BP-ASFs）是构建现代机器学习势函数的基本构件。为了深入理解这些描述符的数学构造和物理意义，此练习  提供了一个计算径向（$G_2$）和角向（$G_4$）对称函数在理想化原子环境（四面体和八面体）中的实践机会。通过这个计算过程，您将亲身体验如何通过调整超参数（如 $\\lambda$ 和 $\\zeta$）来增强描述符对不同配位模式的区分能力。",
            "id": "3791376",
            "problem": "您正在使用 Behler–Parrinello 原子对称函数 (BP-ASF) 为多尺度材料模拟中的特征学习设计以原子为中心的描述符。考虑一个中心原子 $i$ 处于两种理想化的局域环境中：\n\n- 一个完美的四面体配位：$4$ 个邻居原子均与中心原子 $i$ 相距 $r_{0}$，且中心原子处的所有键-键角均为四面体角，其 $\\cos\\theta=-\\frac{1}{3}$。\n- 一个完美的八面体配位：$6$ 个邻居原子均与中心原子 $i$ 相距 $r_{0}$，其中中心原子 $i$ 处的键-键角包含 $12$ 个 $\\cos\\theta=0$ 的直角和 $3$ 个 $\\cos\\theta=-1$ 的平角。\n\n使用以下带有余弦截断的 BP-ASF 定义：\n- 径向函数\n$$\nG_{2}^{(i)}(\\eta_{2},R_{s},R_{c})=\\sum_{j\\neq i}\\exp\\!\\big(-\\eta_{2}\\big(r_{ij}-R_{s}\\big)^{2}\\big)\\,f_{c}(r_{ij}),\n$$\n- 角函数\n$$\nG_{4}^{(i)}(\\eta_{4},\\lambda,\\zeta,R_{c})=2^{\\,1-\\zeta}\\sum_{\\substack{j\\neq i\\\\k\\neq i\\\\j<k}}\\big(1+\\lambda\\cos\\theta_{ijk}\\big)^{\\zeta}\\exp\\!\\big(-\\eta_{4}\\big(r_{ij}^{2}+r_{ik}^{2}\\big)\\big)\\,f_{c}(r_{ij})\\,f_{c}(r_{ik}),\n$$\n其中 $r_{ij} = \\lVert \\mathbf{r}_j - \\mathbf{r}_i \\rVert$，$f_{c}(r) = \\frac{1}{2}(\\cos(\\frac{\\pi r}{R_c}) + 1)$ for $r \\le R_c$ 且为 $0$ otherwise。\n\n使用以下自洽的参数：\n- $r_{0}=2.5$, $R_{s}=r_{0}$, $R_{c}=7.5$, $\\eta_{2}=1.0$, $\\eta_{4}=\\frac{\\ln 2}{2r_{0}^{2}}$。\n\n您的任务是：\n1. 计算四面体环境下的 $G_2$ 函数值。\n2. 对于 $G_4$ 函数，考虑一个超参数网格 $\\lambda \\in \\{-1,1\\}$ 和 $\\zeta \\in \\{1,2\\}$。对于每个 $(\\lambda, \\zeta)$ 组合，计算比率 $R(\\lambda,\\zeta) = \\frac{G_{4}^{\\text{tetra}}}{G_{4}^{\\text{octa}}}$。找出使该比率最大化的 $(\\lambda, \\zeta)$ 组合，并报告此最优组合下的 $G_2$（来自任务1）和 $G_{4}^{\\text{tetra}}$ 的值。\n\n请将您的答案格式化为一个包含四个数字的行向量：$[G_{2}^{\\text{tetra}}, G_{4}^{\\text{tetra}}, \\lambda_{\\text{optimal}}, \\zeta_{\\text{optimal}}]$，所有值均精确到小数点后三位。",
            "solution": "为 $G_{2}$ 和 $G_{4}$ 定义的 Behler–Parrinello 原子对称函数 (BP-ASF) 用于编码局域原子环境。我们从它们的定义和指定的理想几何构型开始计算。\n\n首先，我们计算在邻居原子距离 $r_{0}$ 处的截断因子。当 $r_{0}=2.5$ 且 $R_{c}=7.5$ 时，我们有 $\\frac{r_{0}}{R_{c}}=\\frac{1}{3}$，因此\n$$\nf_{c}(r_{0})=\\frac{1}{2}\\Big(\\cos\\big(\\pi/3\\big)+1\\Big)=\\frac{1}{2}\\Big(\\frac{1}{2}+1\\Big)=\\frac{3}{4}.\n$$\n因此 $f_{c}(r_{0})=\\frac{3}{4}$ 且 $f_{c}(r_{0})^{2}=\\Big(\\frac{3}{4}\\Big)^{2}=\\frac{9}{16}$。\n\n任务 1：计算四面体环境下的 $G_{2}$。\n\n在四面体环境中，所有四个邻居原子的距离均为 $r_{ij}=r_{0}$，并且由于 $R_{s}=r_{0}$，\n$$\n\\exp\\!\\big(-\\eta_{2}\\big(r_{ij}-R_{s}\\big)^{2}\\big)=\\exp(0)=1.\n$$\n因此，\n$$\nG_{2}^{\\text{tetra}}=\\sum_{j=1}^{4}1\\cdot f_{c}(r_{0})=4\\,f_{c}(r_{0})=4\\cdot\\frac{3}{4}=3.\n$$\n\n任务 2：在参数网格上计算两种环境下的 $G_{4}$，并找出最优的 $(\\lambda,\\zeta)$。\n\n我们首先简化 $G_{4}$ 中的径向因子。对于任意满足 $r_{ij}=r_{ik}=r_{0}$ 的原子对 $(j,k)$，以及给定的 $\\eta_{4}=\\frac{\\ln 2}{2r_{0}^{2}}$，指数因子变为\n$$\n\\exp\\!\\big(-\\eta_{4}(r_{ij}^{2}+r_{ik}^{2})\\big)=\\exp\\!\\Big(-\\frac{\\ln 2}{2r_{0}^{2}}(r_{0}^{2}+r_{0}^{2})\\Big)=\\exp(-\\ln 2)=\\frac{1}{2}.\n$$\n因此，对于任何有贡献的原子对，径向-截断因子为\n$$\n\\exp\\!\\big(-\\eta_{4}(r_{ij}^{2}+r_{ik}^{2})\\big)\\,f_{c}(r_{ij})\\,f_{c}(r_{ik})=\\frac{1}{2}\\cdot f_{c}(r_{0})^{2}=\\frac{1}{2}\\cdot\\frac{9}{16}=\\frac{9}{32}.\n$$\n\n接下来，我们计算角度的多重性。\n\n- 四面体环境：存在 $\\binom{4}{2}=6$ 个无序邻居原子对 $(j,k)$，它们与中心原子形成单一的键-键角，且均有 $\\cos\\theta=-\\frac{1}{3}$。\n\n- 八面体环境：存在 $\\binom{6}{2}=15$ 个无序邻居原子对，其中 $12$ 对的 $\\cos\\theta=0$（轴相互垂直），$3$ 对的 $\\cos\\theta=-1$（轴方向相反）。\n\n使用 $G_{4}$ 的定义，\n$$\nG_{4}=2^{\\,1-\\zeta}\\sum_{j<k}\\big(1+\\lambda\\cos\\theta_{ijk}\\big)^{\\zeta}\\times(\\text{径向因子})\n$$\n对于四面体：\n$$\nG_{4}^{\\text{tetra}}=2^{\\,1-\\zeta}\\cdot 6\\cdot\\Big(1+\\lambda\\big(-\\tfrac{1}{3}\\big)\\Big)^{\\zeta}\\cdot\\frac{9}{32}=2^{\\,1-\\zeta}\\,\\frac{27}{16}\\,\\Big(1-\\frac{\\lambda}{3}\\Big)^{\\zeta}.\n$$\n对于八面体：\n$$\nG_{4}^{\\text{octa}}=2^{\\,1-\\zeta}\\cdot\\frac{9}{32}\\cdot\\Big[12\\cdot\\big(1+\\lambda(0)\\big)^{\\zeta}+3\\cdot\\big(1+\\lambda(-1)\\big)^{\\zeta}\\Big]=2^{\\,1-\\zeta}\\,\\frac{9}{32}\\Big[12+3(1-\\lambda)^{\\zeta}\\Big].\n$$\n比率 $R(\\lambda,\\zeta) = G_{4}^{\\text{tetra}}/G_{4}^{\\text{octa}}$ 为：\n$$\nR(\\lambda,\\zeta)=\\frac{\\frac{27}{16}(1-\\lambda/3)^{\\zeta}}{\\frac{9}{32}(12+3(1-\\lambda)^{\\zeta})}=\\frac{6(1-\\lambda/3)^{\\zeta}}{12+3(1-\\lambda)^{\\zeta}}.\n$$\n现在我们在网格上计算 $R$：\n-   $(\\lambda=1, \\zeta=1): R = \\frac{6(2/3)}{12+3(0)} = \\frac{4}{12} = 1/3 \\approx 0.333$\n-   $(\\lambda=1, \\zeta=2): R = \\frac{6(2/3)^2}{12+3(0)} = \\frac{6(4/9)}{12} = \\frac{8/3}{12} = 2/9 \\approx 0.222$\n-   $(\\lambda=-1, \\zeta=1): R = \\frac{6(4/3)}{12+3(2)} = \\frac{8}{18} = 4/9 \\approx 0.444$\n-   $(\\lambda=-1, \\zeta=2): R = \\frac{6(4/3)^2}{12+3(2^2)} = \\frac{6(16/9)}{12+12} = \\frac{32/3}{24} = 4/9 \\approx 0.444$\n\n最大比率为 $4/9$，在 $(\\lambda=-1, \\zeta=1)$ 和 $(\\lambda=-1, \\zeta=2)$ 时达到。我们选择第一个组合作为最优解。\n最优参数为 $\\lambda_{\\text{optimal}}=-1, \\zeta_{\\text{optimal}}=1$。\n使用这些参数，我们计算 $G_{4}^{\\text{tetra}}$：\n$$\nG_{4}^{\\text{tetra}}(\\lambda=-1,\\zeta=1)=2^{\\,1-1}\\,\\frac{27}{16}\\,\\Big(1-\\frac{-1}{3}\\Big)^{1}=\\frac{27}{16}\\cdot\\frac{4}{3}=\\frac{9}{4}=2.25.\n$$\n最终结果向量为 $[G_2, G_4^{\\text{tetra}}, \\lambda, \\zeta]$。\n\n将所有值组合成一个行向量，得到 $[3.000, 2.250, -1, 1]$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 3.000 & 2.250 & -1 & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "虽然原子中心对称函数（ACSFs）提供了对原子环境的有效且不变的表示，但我们还可以利用神经网络进一步学习更紧凑、信息更密集的潜在特征。此练习  将指导您构建一个自动编码器，它以一个不变的描述符作为输入，学习其低维潜在表示。这个过程强调了一个核心设计原则：物理对称性（平移、旋转和置换不变性）应通过输入数据的构造来保证，而非依赖于网络架构本身去学习。",
            "id": "3791327",
            "problem": "您的任务是构建一个程序，使用自编码器学习原子环境的紧凑潜在表示，同时强制实现适用于多尺度材料模拟中以原子为中心的描述符的平移、旋转和置换不变性。原子环境为一个中心原子及其在截断半径内的一组有限邻近原子所定义，中心原子位置为 $\\mathbf{r}_i \\in \\mathbb{R}^3$，邻近原子位置为 $\\{\\mathbf{r}_j\\}_{j=1}^{N}$。所学习的潜在表示必须对以下操作保持不变：所有原子位置平移一个恒定向量、通过行列式为 $+1$ 的正交矩阵进行旋转，以及邻居索引的置换。\n\n从以下基本基础开始：中心原子周围的原子环境由 $\\mathbb{R}^3$ 中的位置 $\\{\\mathbf{r}_i, \\mathbf{r}_j\\}$ 描述，欧几里得距离定义为 $d_{ij} = \\lVert \\mathbf{r}_j - \\mathbf{r}_i \\rVert_2$，邻居位移向量 $\\mathbf{v}_j = \\mathbf{r}_j - \\mathbf{r}_i$ 和 $\\mathbf{v}_k = \\mathbf{r}_k - \\mathbf{r}_i$ 之间的夹角通过夹角的余弦定义，$\\cos\\theta_{jk} = \\dfrac{\\mathbf{v}_j \\cdot \\mathbf{v}_k}{\\lVert \\mathbf{v}_j \\rVert_2 \\lVert \\mathbf{v}_k \\rVert_2}$，该值在任何全局旋转下都是不变的。此外，诸如对邻居求和之类的聚合操作会产生置换不变的量。\n\n您必须设计并实现一个自编码器，它将原子环境的不变描述符映射到紧凑的潜在空间，并在输出端重构相同的描述符。不变性必须通过输入描述符的构造或网络架构来强制实现，并且您的实现应基于给定的基本定义来证明这些不变性。对于数值实现，请仅使用邻居距离和成对余弦角的函数，通过求和进行聚合来构建描述符，以确保置换不变性。在计算邻居位移向量之前，必须减去中心原子的位置，以确保平移不变性。旋转不变性必须源于仅使用距离和余弦角。该自编码器可以是一个前馈神经网络，使用标准操作实现，并通过基于梯度的优化进行训练，以最小化均方重构误差。\n\n在您的程序中使用以下科学上真实且自洽的参数化：\n- 截断半径 $R_c = 3.0$ 埃，因此只考虑 $d_{ij} \\le R_c$ 的邻居。\n- 使用 $M = 8$ 个在 $[0, R_c]$ 中线性间隔的中心进行径向基展开，共享宽度为 $\\sigma_r = 0.4$，应用于每个邻居距离并对所有邻居求和。\n- 使用 $P = 6$ 个在 $[-1, 1]$ 中线性间隔的中心进行角向基展开，共享宽度为 $\\sigma_a = 0.2$，应用于每个成对余弦角并对所有无序邻居对求和。\n- 最终描述符是 $M$ 维径向和与 $P$ 维角向和的串联，产生 $M + P$ 的输入维度。\n- 自编码器架构，编码器中有一个隐藏层，解码器中有一个隐藏层：编码器将输入维度 $M+P$ 映射到隐藏维度 $H = 32$（使用双曲正切激活函数），然后到潜在维度 $L = 8$（使用线性激活函数）；解码器将 $L$ 映射到 $H$（使用双曲正切激活函数），然后线性映射到原始输入维度 $M+P$。\n- 使用批梯度下降法优化均方重构误差，学习率为 $\\eta = 0.01$，训练 $T = 800$ 个周期。\n- 所有角度都通过其余弦值使用；因此，除了余弦的定义（因为它是无量纲的）之外，不需要指定角度单位。距离单位必须是埃。\n\n您的程序必须生成自己的合成训练数据集，方法是为每个原子环境从 $\\{1, 2, 3, 4, 5\\}$ 中均匀采样邻居数 $N$，并在以 $\\mathbf{r}_i = \\mathbf{0}$ 为中心、半径为 $R_c$ 的球体内均匀采样邻居位置（您应使用确定性随机种子以确保可复现性）。程序必须使用上述描述符在此数据集上训练自编码器。训练后，程序必须评估以下测试套件，产生五个布尔结果：\n\n- 测试用例 $1$（置换不变性）：对于一个固定的环境，其中有 $N = 4$ 个邻居，中心原子在 $\\mathbf{r}_i = \\mathbf{0}$，验证置换邻居索引会产生相同的潜在编码。输出必须是一个布尔值，指示潜在编码在欧几里得范数下 $10^{-8}$ 的数值公差内是否相等。\n- 测试用例 $2$（旋转不变性）：对于相同的固定环境，将一个由行列式 $\\det(\\mathbf{Q}) = +1$ 的正交矩阵 $\\mathbf{Q} \\in \\mathbb{R}^{3 \\times 3}$ 代表的旋转应用于所有原子位置，并验证潜在编码在欧几里得范数下 $10^{-8}$ 的数值公差内保持不变。\n- 测试用例 $3$（平移不变性）：对于相同的固定环境，将所有原子位置平移一个恒定向量 $\\mathbf{t} \\in \\mathbb{R}^3$，并验证当通过从每个邻居位置减去 $\\mathbf{r}_i$ 来构造描述符时，潜在编码在欧几里得范数下 $10^{-8}$ 的数值公差内保持不变。\n- 测试用例 $4$（空环境边界情况）：对于一个有 $N = 0$ 个邻居的环境，计算描述符，对其进行编码和解码，并验证重构均方误差小于或等于 $10^{-8}$。\n- 测试用例 $5$（训练集上的重构质量）：计算训练数据集上的平均均方重构误差，并验证其小于 $0.05$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 `\"[r_1,r_2,r_3,r_4,r_5]\"`，其中每个 $r_k$ 是与第 $k$ 个测试用例相关联的布尔值。不应打印任何其他文本。",
            "solution": "该问题要求构建并训练一个自编码器，以学习原子环境的紧凑表示，同时确保该表示对原子的平移、旋转和置换保持不变。这是为材料模拟开发机器学习势的一项基础任务，在这些模拟中，这些不变性是物理上的必然要求。解决方案首先设计一个内在地拥有这些对称性的输入描述符，然后训练一个神经网络自编码器来压缩和重构这个描述符。\n\n### 1. 不变描述符设计\n\n核心原则是，只有当输入到模型的信息本身就是不变的时，学习到的表示才可能是不变的。我们基于中心原子 $i$ 的邻居 $j$ 的相对位置，为其局部环境构建一个描述符向量。不变性通过以下方式实现：\n\n- **平移不变性**：整个系统任意平移一个向量 $\\mathbf{t}$，会将位置 $\\mathbf{r}$ 变换为 $\\mathbf{r}' = \\mathbf{r} + \\mathbf{t}$。描述符由位移向量 $\\mathbf{v}_j = \\mathbf{r}_j - \\mathbf{r}_i$ 构建。在平移下，这些向量变为 $\\mathbf{v}'_j = \\mathbf{r}'_j - \\mathbf{r}'_i = (\\mathbf{r}_j + \\mathbf{t}) - (\\mathbf{r}_i + \\mathbf{t}) = \\mathbf{v}_j$。由于描述符仅是这些位移向量的函数，它自动对全局平移保持不变。\n\n- **旋转不变性**：全局旋转由一个行列式 $\\det(\\mathbf{Q})=+1$ 的正交矩阵 $\\mathbf{Q}$ 表示。位移向量 $\\mathbf{v}_j$ 变换为 $\\mathbf{v}'_j = \\mathbf{Qv}_j$。为确保不变性，描述符由在旋转下不变的标量构建。我们使用两种此类量：\n    1.  距离：$d_{ij} = \\lVert \\mathbf{v}_j \\rVert_2$。旋转后，新距离为 $\\lVert \\mathbf{v}'_j \\rVert_2 = \\lVert \\mathbf{Qv}_j \\rVert_2 = \\lVert \\mathbf{v}_j \\rVert_2 = d_{ij}$，因为正交矩阵保持欧几里得范数不变。\n    2.  向量间夹角的余弦：$\\cos\\theta_{jk} = \\frac{\\mathbf{v}_j \\cdot \\mathbf{v}_k}{\\lVert \\mathbf{v}_j \\rVert_2 \\lVert \\mathbf{v}_k \\rVert_2}$。旋转后，该值变为 $\\frac{(\\mathbf{Qv}_j) \\cdot (\\mathbf{Qv}_k)}{\\lVert \\mathbf{Qv}_j \\rVert_2 \\lVert \\mathbf{Qv}_k \\rVert_2} = \\frac{\\mathbf{v}_j \\cdot \\mathbf{v}_k}{\\lVert \\mathbf{v}_j \\rVert_2 \\lVert \\mathbf{v}_k \\rVert_2} = \\cos\\theta_{jk}$，因为正交矩阵也保持点积不变。\n    \n    通过仅使用距离和余弦角，旋转不变性通过构造得到保证。\n\n- **置换不变性**：邻居原子的索引是任意的。描述符必须与此标记无关。这通过对所有邻居使用聚合操作（特别是求和）来实现。对于应用于每个邻居的函数 $g(\\cdot)$，其和 $\\sum_j g(j)$ 是可交换的，因此对邻居索引的置换是不变的。\n\n具体描述符是通过将距离和余弦角投影到一组基上，然后将结果求和来构建的。\n- **径向部分**：每个邻居距离 $d_{ij}$（对于截断半径 $R_c = 3.0$ 内的邻居）使用 $M=8$ 个高斯基函数 $G_k^r(d) = \\exp(-(d - \\mu_k^r)^2/\\sigma_r^2)$ 进行展开，其中中心 $\\mu_k^r$ 在 $[0, R_c]$ 内线性间隔，宽度为 $\\sigma_r = 0.4$。结果对所有邻居求和，得到一个 $M$ 维向量：$D_k^r = \\sum_{j} G_k^r(d_{ij})$。\n- **角向部分**：对于每对唯一的邻居 $(j, k)$，其夹角余弦 $\\cos\\theta_{jk}$ 使用 $P=6$ 个高斯基函数 $G_l^a(\\cos\\theta) = \\exp(-(\\cos\\theta - \\mu_l^a)^2/\\sigma_a^2)$ 进行展开，其中中心 $\\mu_l^a$ 在 $[-1, 1]$ 内线性间隔，宽度为 $\\sigma_a = 0.2$。结果对所有唯一的对（$\\sum_{j<k}$）求和，得到一个 $P$ 维向量：$D_l^a = \\sum_{j<k} G_l^a(\\cos\\theta_{jk})$。\n\n最终的描述符是这两个向量的串联，产生一个 $M+P$ 维的输入，该输入通过构造保证了所有必需的不变性。\n\n### 2. 自编码器架构与训练\n\n自编码器是一个神经网络，旨在学习输入的紧凑潜在空间表示（编码），然后从该表示中重构原始输入（解码）。\n- **架构**：编码器包含一个将输入从 $M+P$ 维映射到 $H=32$ 维的隐藏层（使用 tanh 激活），然后是一个将隐藏层映射到 $L=8$ 维潜在空间的线性层。解码器反映了这一结构，将潜在空间映射回 $H=32$ 维（tanh 激活），然后线性映射回原始的 $M+P$ 维。\n- **训练**：网络通过最小化均方重构误差（MSE）来训练，使用批梯度下降法，学习率为 $\\eta=0.01$，训练 $T=800$ 个周期。训练数据集是通过在以原点为中心的球体内随机采样原子位置来合成生成的。\n\n### 3. 测试与验证\n\n训练后，对自编码器的性能和不变性进行评估：\n- **不变性测试**：为单个原子环境生成一个描述符，然后对环境进行置换、旋转和平移操作。由于描述符本身是不变的，因此其潜在编码也必须在数值公差内保持不变。\n- **边界情况**：测试一个没有邻居的空环境，确保其描述符（一个零向量）能够被完美地重构。\n- **重构质量**：在整个训练集上计算平均 MSE，以验证自编码器已成功学会了数据分布的有效表示。\n\n此方法将物理对称性的强制实施与神经网络的学习能力解耦：对称性由特征工程（描述符设计）处理，而表示学习则由网络处理。这种方法比试图让网络自行学习对称性更为稳健和数据高效。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\ndef solve():\n    \"\"\"\n    Main function to construct, train, and test the invariant autoencoder.\n    \"\"\"\n    # -------------------\n    # 1. PARAMETERS\n    # -------------------\n    # Scientific and model parameters from the problem statement\n    RC = 3.0  # Cutoff radius in angstroms\n    M = 8  # Number of radial basis centers\n    SIGMA_R = 0.4  # Width for radial basis\n    P = 6  # Number of angular basis centers\n    SIGMA_A = 0.2  # Width for angular basis\n    H = 32  # Hidden layer dimension\n    L = 8  # Latent space dimension\n    ETA = 0.01  # Learning rate\n    T = 800  # Number of training epochs\n    INPUT_DIM = M + P\n    SEED = 42 # For reproducibility\n    \n    # Dataset and test parameters\n    NUM_TRAIN_SAMPLES = 200\n    MAX_NEIGHBORS = 5\n    TOLERANCE = 1e-8\n    MSE_THRESHOLD = 0.05\n    rng = np.random.default_rng(SEED)\n\n    # --------------------------------\n    # 2. HELPER  DESCRIPTOR FUNCTIONS\n    # --------------------------------\n    \n    def _tanh(x):\n        return np.tanh(x)\n        \n    def _mse(y_true, y_pred):\n        return np.mean((y_true - y_pred)**2)\n\n    RADIAL_CENTERS = np.linspace(0, RC, M)\n    ANGULAR_CENTERS = np.linspace(-1, 1, P)\n\n    def compute_descriptor(rel_positions):\n        \"\"\"\n        Computes the invariant descriptor for a given atomic environment.\n        \"\"\"\n        neighbors_pos = np.atleast_2d(rel_positions)\n        if neighbors_pos.shape[1] != 3 or neighbors_pos.shape[0] == 0:\n            # Handle empty environment or incorrect input\n            if neighbors_pos.size == 0:\n                return np.zeros(M + P)\n\n        dists = np.linalg.norm(neighbors_pos, axis=1)\n        valid_indices = np.where(dists = RC)[0]\n        \n        if len(valid_indices) == 0:\n            return np.zeros(M + P)\n\n        valid_neighbors = neighbors_pos[valid_indices, :]\n        valid_dists = dists[valid_indices]\n        \n        # Radial part\n        radial_descriptor = np.zeros(M)\n        for d in valid_dists:\n            gaussians = np.exp(-(d - RADIAL_CENTERS)**2 / SIGMA_R**2)\n            radial_descriptor += gaussians\n            \n        # Angular part\n        angular_descriptor = np.zeros(P)\n        num_valid_neighbors = len(valid_dists)\n        if num_valid_neighbors >= 2:\n            for i in range(num_valid_neighbors):\n                for j in range(i + 1, num_valid_neighbors):\n                    v_i = valid_neighbors[i]\n                    v_j = valid_neighbors[j]\n                    d_i = valid_dists[i]\n                    d_j = valid_dists[j]\n                    cos_theta = np.dot(v_i, v_j) / (d_i * d_j)\n                    # Clamp due to potential floating point inaccuracies\n                    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n                    gaussians = np.exp(-(cos_theta - ANGULAR_CENTERS)**2 / SIGMA_A**2)\n                    angular_descriptor += gaussians\n                    \n        return np.concatenate((radial_descriptor, angular_descriptor))\n\n    def generate_environments(num_samples, min_n, max_n, radius, rng):\n        \"\"\"\n        Generates synthetic atomic environments.\n        \"\"\"\n        environments = []\n        for _ in range(num_samples):\n            N = rng.integers(min_n, max_n + 1)\n            # Sample points uniformly in a ball\n            # 1. Sample direction uniformly on a sphere\n            vecs = rng.normal(size=(N, 3))\n            norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n            # Avoid division by zero for an unlikely zero vector\n            directions = np.divide(vecs, norms, out=np.zeros_like(vecs), where=norms!=0)\n            \n            # 2. Sample radius with r^2 distribution for uniform volume\n            radii = radius * (rng.uniform(size=(N, 1))**(1/3.0))\n            \n            positions = directions * radii\n            environments.append(positions)\n        return environments\n\n    # ---------------------\n    # 3. AUTOENCODER CLASS\n    # ---------------------\n    class Autoencoder:\n        def __init__(self, rng):\n            # Glorot initialization\n            lim1 = np.sqrt(6.0 / (INPUT_DIM + H))\n            self.W1 = rng.uniform(-lim1, lim1, (H, INPUT_DIM))\n            self.b1 = np.zeros((H, 1))\n            \n            lim2 = np.sqrt(6.0 / (H + L))\n            self.W2 = rng.uniform(-lim2, lim2, (L, H))\n            self.b2 = np.zeros((L, 1))\n\n            lim3 = np.sqrt(6.0 / (L + H))\n            self.W3 = rng.uniform(-lim3, lim3, (H, L))\n            self.b3 = np.zeros((H, 1))\n\n            lim4 = np.sqrt(6.0 / (H + INPUT_DIM))\n            self.W4 = rng.uniform(-lim4, lim4, (INPUT_DIM, H))\n            self.b4 = np.zeros((INPUT_DIM, 1))\n\n        def encode(self, x):\n            x_vec = x.reshape(-1, 1)\n            z1 = self.W1 @ x_vec + self.b1\n            a1 = _tanh(z1)\n            z2 = self.W2 @ a1 + self.b2\n            return z2.flatten()\n\n        def forward(self, x):\n            x_vec = x.reshape(-1, 1)\n            # Encoder\n            z1 = self.W1 @ x_vec + self.b1\n            a1 = _tanh(z1)\n            latent_code = self.W2 @ a1 + self.b2\n            \n            # Decoder\n            z3 = self.W3 @ latent_code + self.b3\n            a3 = _tanh(z3)\n            reconstruction = self.W4 @ a3 + self.b4\n            \n            return reconstruction.flatten(), latent_code.flatten()\n\n        def train_batch(self, X_train):\n            # Vectorized implementation for batch gradient descent\n            num_samples = X_train.shape[0]\n            X = X_train.T # (input_dim, num_samples)\n\n            # --- Forward pass ---\n            Z1 = self.W1 @ X + self.b1\n            A1 = _tanh(Z1)\n            Z2 = self.W2 @ A1 + self.b2\n            A2 = Z2  # Latent codes\n            Z3 = self.W3 @ A2 + self.b3\n            A3 = _tanh(Z3)\n            Z4 = self.W4 @ A3 + self.b4\n            X_hat = Z4 # Reconstructions\n\n            # --- Backward pass ---\n            # Using MSE, derivative is 2/N * (y_pred - y_true). Here input dim is used in problem.\n            # Assuming mean over features, not batch. Problem description implies MSE over features.\n            # So dL/dX_hat = 2 * (X_hat - X) / INPUT_DIM\n            dZ4 = (2.0 / INPUT_DIM) * (X_hat - X)\n            dW4 = (1.0 / num_samples) * (dZ4 @ A3.T)\n            db4 = (1.0 / num_samples) * np.sum(dZ4, axis=1, keepdims=True)\n            \n            dA3 = self.W4.T @ dZ4\n            dZ3 = dA3 * (1 - A3**2)\n            dW3 = (1.0 / num_samples) * (dZ3 @ A2.T)\n            db3 = (1.0 / num_samples) * np.sum(dZ3, axis=1, keepdims=True)\n\n            dA2 = self.W3.T @ dZ3\n            dZ2 = dA2 # Linear activation derivative is 1\n            dW2 = (1.0 / num_samples) * (dZ2 @ A1.T)\n            db2 = (1.0 / num_samples) * np.sum(dZ2, axis=1, keepdims=True)\n\n            dA1 = self.W2.T @ dZ2\n            dZ1 = dA1 * (1 - A1**2)\n            dW1 = (1.0 / num_samples) * (dZ1 @ X.T)\n            db1 = (1.0 / num_samples) * np.sum(dZ1, axis=1, keepdims=True)\n\n            # --- Parameter update ---\n            self.W1 -= ETA * dW1\n            self.b1 -= ETA * db1\n            self.W2 -= ETA * dW2\n            self.b2 -= ETA * db2\n            self.W3 -= ETA * dW3\n            self.b3 -= ETA * db3\n            self.W4 -= ETA * dW4\n            self.b4 -= ETA * db4\n\n    # ----------------------------\n    # 4. TRAINING  DATA GENERATION\n    # ----------------------------\n    # Generate training data\n    train_envs = generate_environments(NUM_TRAIN_SAMPLES, 1, MAX_NEIGHBORS, RC, rng)\n    X_train = np.array([compute_descriptor(env) for env in train_envs])\n    \n    # Initialize and train the autoencoder\n    autoencoder = Autoencoder(rng)\n    for _ in range(T):\n        autoencoder.train_batch(X_train)\n\n    # -----------------\n    # 5. TEST SUITE\n    # -----------------\n    results = []\n\n    # Generate a fixed test environment\n    test_env_coords = generate_environments(1, 4, 4, RC, rng)[0]\n    \n    # --- Test 1: Permutation Invariance ---\n    desc1_orig = compute_descriptor(test_env_coords)\n    latent1_orig = autoencoder.encode(desc1_orig)\n    \n    permuted_coords = rng.permutation(test_env_coords)\n    desc1_perm = compute_descriptor(permuted_coords)\n    latent1_perm = autoencoder.encode(desc1_perm)\n    \n    # The descriptors themselves should be identical\n    assert np.allclose(desc1_orig, desc1_perm)\n    results.append(np.linalg.norm(latent1_orig - latent1_perm) = TOLERANCE)\n\n    # --- Test 2: Rotation Invariance ---\n    # Scipy rotation requires a specific random state object from numpy  1.17\n    # For compatibility with newer numpy, we create a temporary legacy object.\n    scipy_rng = np.random.RandomState(rng.integers(2**32 - 1))\n    \n    rot_matrix = Rotation.random(random_state=scipy_rng).as_matrix()\n    rotated_coords = (rot_matrix @ test_env_coords.T).T\n    desc2_rot = compute_descriptor(rotated_coords)\n    latent2_rot = autoencoder.encode(desc2_rot)\n\n    # The descriptors themselves should be identical\n    assert np.allclose(desc1_orig, desc2_rot)\n    results.append(np.linalg.norm(latent1_orig - latent2_rot) = TOLERANCE)\n\n    # --- Test 3: Translation Invariance ---\n    central_pos = np.zeros(3)\n    neighbor_pos = test_env_coords\n    t = rng.random(3) * 10.0 # Some arbitrary translation vector\n    \n    # Apply translation\n    new_central_pos = central_pos + t\n    new_neighbor_pos = neighbor_pos + t\n    \n    # Compute relative pos, which cancels the translation\n    new_rel_coords = new_neighbor_pos - new_central_pos\n    \n    desc3_trans = compute_descriptor(new_rel_coords)\n    latent3_trans = autoencoder.encode(desc3_trans)\n\n    # The descriptors themselves should be identical\n    assert np.allclose(desc1_orig, desc3_trans)\n    results.append(np.linalg.norm(latent1_orig - latent3_trans) = TOLERANCE)\n\n    # --- Test 4: Empty Environment Edge Case ---\n    desc4_empty = compute_descriptor(np.array([]))\n    recon4, _ = autoencoder.forward(desc4_empty)\n    mse4 = _mse(desc4_empty, recon4)\n    results.append(mse4 = TOLERANCE)\n\n    # --- Test 5: Reconstruction Quality ---\n    total_mse = 0.0\n    for x_sample in X_train:\n        recon5, _ = autoencoder.forward(x_sample)\n        total_mse += _mse(x_sample, recon5)\n    avg_mse = total_mse / NUM_TRAIN_SAMPLES\n    results.append(avg_mse  MSE_THRESHOLD)\n\n    # -----------------\n    # 6. FINAL OUTPUT\n    # -----------------\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "机器学习模型在实际应用中的一个关键挑战是确保其预测的可靠性和安全性，因为模型只对其训练数据范围内的构型有效。这项高级实践  介绍了一种解决此问题的实用方法。您将学习如何在描述符空间中建立一个概率密度模型，并用它来执行分布外（out-of-distribution, OOD）检测，从而识别那些模型未曾见过、其预测可能不可靠的新原子环境。",
            "id": "3791381",
            "problem": "您的任务是设计并实现一个在原子环境描述符空间中的分布外检测测试，使用密度估计来标记对新原子构型的不安全预测。这项工作在多尺度材料模拟的背景下进行，其中局部原子环境由旋转和置换不变的径向描述符表示。您编写的程序必须是一个完整、可运行的程序，该程序执行以下步骤并输出所需的结果。\n\n从以下基本基础开始：局部原子环境是中心原子周围的相邻原子的集合。假设中心原子位于原点，相邻原子集合表示为 $\\mathcal{N} = \\{ \\mathbf{r}_i \\}_{i=1}^{n}$，其距离为 $r_i = \\|\\mathbf{r}_i\\|$。原子环境描述符通过使用带有截断的径向基展开，构建为从 $\\mathcal{N}$ 到固定长度向量的不变映射。设描述符维度为 $d$，径向中心为 $\\{\\mu_k\\}_{k=1}^{d}$，高斯宽度为 $\\sigma  0$。定义一个截断半径 $r_c  0$。描述符分量由下式给出：\n$$\nD_k(\\mathcal{N}) = \\begin{cases}\n\\frac{1}{n_c} \\sum\\limits_{i=1}^{n_c} \\exp\\!\\left(-\\dfrac{(r_i - \\mu_k)^2}{2\\sigma^2}\\right)  \\text{if } n_c  0,\\\\\n0  \\text{if } n_c = 0,\n\\end{cases}\n$$\n其中 $n_c$ 是满足 $r_i \\le r_c$ 的相邻原子数量。这种构造产生了一个描述符 $\\mathbf{D}(\\mathcal{N}) \\in \\mathbb{R}^d$，它对 $\\mathcal{N}$ 的旋转和置换是不变的。距离必须以埃（Angstrom）为单位进行解释，并且所有使用的距离都必须以埃为单位表示。\n\n在描述符空间上使用概率密度估计来检测分布外环境。通过从训练集中估计均值向量 $\\hat{\\boldsymbol{\\mu}} \\in \\mathbb{R}^d$ 和协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}} \\in \\mathbb{R}^{d \\times d}$，将多元正态密度拟合到训练描述符上。为了数值稳定性，使用协方差收缩：\n$$\n\\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}} = (1 - \\alpha)\\,\\hat{\\boldsymbol{\\Sigma}} + \\alpha\\, \\operatorname{diag}(\\hat{\\boldsymbol{\\Sigma}}) + \\varepsilon \\mathbf{I}_d,\n$$\n其中 $\\varepsilon  0$ 是一个小数，收缩权重 $\\alpha \\in [0,1)$。给定一个描述符 $\\mathbf{x} \\in \\mathbb{R}^d$，计算在拟合密度下的对数似然：\n$$\n\\log \\hat{p}(\\mathbf{x}) = -\\tfrac{1}{2}\\Big[ (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}})^\\top \\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}}^{-1} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}) + d \\log(2\\pi) + \\log \\det(\\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}}) \\Big].\n$$\n将决策阈值 $\\tau$ 校准为训练集对数似然的 $q$-分位数（选择 $q = 0.05$）。如果新构型的对数似然低于 $\\tau$，即 $\\log \\hat{p}(\\mathbf{x})  \\tau$，则将其标记为不安全。\n\n训练数据生成：构建 $N_{\\mathrm{train}}$ 个训练环境，每个环境包含两个具有微小热振动状抖动的配位层。对于每个训练环境，在半径 $r_1$ 处采样 $n_1$ 个相邻原子，在半径 $r_2$ 处采样 $n_2$ 个相邻原子，每个半径都添加标准差为 $s$ 的独立高斯抖动，并在形成描述符时丢弃任何距离超过 $r_c$ 的相邻原子。在整个问题中使用以下固定参数：\n- 描述符中心：$\\mu_k$ 在 $1.5$ 到 $4.0$ 埃（含）之间线性分布，共 $d = 8$ 个分量。\n- 高斯宽度：$\\sigma = 0.15$ 埃。\n- 截断半径：$r_c = 4.5$ 埃。\n- 训练壳层参数：在 $r_1 = 2.5$ 埃处有 $n_1 = 12$ 个，在 $r_2 = 3.5$ 埃处有 $n_2 = 6$ 个，抖动 $s = 0.05$ 埃。\n- 训练环境数量：$N_{\\mathrm{train}} = 200$。\n- 协方差收缩参数：$\\alpha = 0.1$, $\\varepsilon = 10^{-6}$。\n- 阈值分位数：$q = 0.05$。\n\n为确保在现代编程语言中的普遍适用性，仅通过相邻原子距离列表来表示环境（不需要角度）。所有距离必须以埃为单位。描述符计算应仅使用这些距离和指定的参数。学习和检测步骤应完全在描述符空间中进行。\n\n测试套件：实现以下六个测试用例，每个用例由一组相邻原子距离（以埃为单位）定义。对于随机生成，每个测试用例使用固定的随机种子以确保确定性。在每种情况下，距离都是从以指定半径为中心、具有指定数量和抖动的正态分布中独立采样的，然后截断为非负值；任何大于 $r_c$ 的距离都必须被描述符规则排除。\n\n- 用例 1（分布内，小抖动）：在 $r_1 = 2.52$ 处有 $n_1 = 12$ 个，抖动 $s_1 = 0.02$；在 $r_2 = 3.48$ 处有 $n_2 = 6$ 个，抖动 $s_2 = 0.02$。\n- 用例 2（空环境）：没有相邻原子。\n- 用例 3（边界相邻原子）：$n_b = 2$ 个相邻原子正好在 $r_c = 4.5$ 处，抖动 $s_b = 0.0$。\n- 用例 4（短程异常）：在 $r = 1.2$ 处有 $n = 6$ 个相邻原子，抖动 $s = 0.01$。\n- 用例 5（高配位密度）：在 $r_1 = 2.5$ 处有 $n_1 = 30$ 个，抖动 $s_1 = 0.05$；在 $r_2 = 3.5$ 处有 $n_2 = 15$ 个，抖动 $s_2 = 0.05$。\n- 用例 6（偏移的壳层）：在 $r_1 = 2.8$ 处有 $n_1 = 12$ 个，抖动 $s_1 = 0.05$；在 $r_2 = 3.0$ 处有 $n_2 = 6$ 个，抖动 $s_2 = 0.05$。\n\n对于每个用例，计算其描述符 $\\mathbf{D}$，并在已训练的密度模型下计算其高斯对数似然，然后根据阈值规则确定其是否被标记为不安全。最终输出必须按顺序汇总这六个用例的布尔值不安全标志。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[result1,result2,result3,result4,result5,result6]”）。每个结果必须是一个布尔值，True 表示“不安全”，False 表示“安全”。",
            "solution": "该问题要求为原子环境设计并实现一种分布外检测机制，这是确保材料模拟中机器学习模型可靠性的关键任务。解决方案主要分三个阶段进行：首先，为原子环境定义一种合适的表示方法；其次，根据训练数据集构建这些表示的分布概率模型；第三，使用该模型将新环境分类为分布内（安全）或分布外（不安全）。\n\n**1. 原子环境描述符**\n\n一个原子环境，由中心原子周围的相邻原子集合 $\\mathcal{N} = \\{ \\mathbf{r}_i \\}_{i=1}^{n}$ 组成，必须被转换成一个固定长度的数值向量，称为描述符，该描述符对相同原子的旋转、平移和置换是不变的。问题指定了一种径向描述符，它仅取决于相邻原子与中心原子之间的距离 $r_i = \\|\\mathbf{r}_i\\|$。\n\n描述符 $\\mathbf{D}(\\mathcal{N})$ 是一个 $d$ 维向量，其中每个分量 $D_k$ 是通过将相邻原子距离集合投影到径向基函数上计算得出的。该基由 $d$ 个高斯函数组成，这些函数以预定义的半径 $\\{\\mu_k\\}_{k=1}^{d}$ 为中心。定义了一个截断半径 $r_c  0$，只有在此半径内的相邻原子才对描述符有贡献。设 $n_c$ 是满足 $r_i \\le r_c$ 的相邻原子数量。描述符的第 $k$ 个分量由下式给出：\n$$\nD_k(\\mathcal{N}) = \\begin{cases}\n\\frac{1}{n_c} \\sum\\limits_{i=1}^{n_c} \\exp\\!\\left(-\\dfrac{(r_i - \\mu_k)^2}{2\\sigma^2}\\right)  \\text{if } n_c  0, \\\\\n0  \\text{if } n_c = 0.\n\\end{cases}\n$$\n参数 $\\sigma$ 控制高斯基函数的宽度。通过 $1/n_c$ 进行归一化，使得描述符成为一种平均表示，与截断球内的配位数无关。\n\n该描述符的固定参数为：\n-   维度：$d = 8$。\n-   径向中心 $\\{\\mu_k\\}_{k=1}^{8}$：在 $1.5$ 到 $4.0$ 埃之间线性分布。\n-   高斯宽度：$\\sigma = 0.15$ 埃。\n-   截断半径：$r_c = 4.5$ 埃。\n\n**2. 概率密度估计与分布外检测**\n\n检测方法的核心在于对已知、有效的原子环境所对应的描述符的概率密度进行建模。我们假设这些“分布内”构型的描述符可以由一个多元正态分布来建模。\n\n首先，生成一个包含 $N_{\\mathrm{train}} = 200$ 个原子环境的训练集。每个环境都是通过从正态分布中采样相邻原子距离来构建的，以模拟理想配位层周围的热振动：在 $r_1 = 2.5$ Å 附近有 $n_1 = 12$ 个相邻原子，在 $r_2 = 3.5$ Å 附近有 $n_2 = 6$ 个相邻原子，位置抖动的标准差为 $s = 0.05$ Å。对这 $N_{\\mathrm{train}}$ 个环境中的每一个，都计算一个描述符向量，从而构成训练集 $\\{\\mathbf{x}_i\\}_{i=1}^{N_{\\mathrm{train}}}$。\n\n从这个训练集中，我们估计多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ 的参数：\n-   均值向量 $\\hat{\\boldsymbol{\\mu}}$ 是训练描述符的样本均值。\n-   协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}}$ 是样本协方差。\n\n为了确保协方差矩阵是良态且可逆的，尤其是在样本数量有限的情况下，应用了收缩正则化：\n$$\n\\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}} = (1 - \\alpha)\\,\\hat{\\boldsymbol{\\Sigma}} + \\alpha\\, \\operatorname{diag}(\\hat{\\boldsymbol{\\Sigma}}) + \\varepsilon \\mathbf{I}_d\n$$\n其中 $\\alpha = 0.1$ 是收缩权重，$\\varepsilon = 10^{-6}$ 是一个保证正定性的小正则化项，$\\mathbf{I}_d$ 是 $d \\times d$ 的单位矩阵。\n\n利用拟合好的模型 $\\hat{p}(\\mathbf{x}) \\sim \\mathcal{N}(\\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}})$，我们可以评估任何新描述符 $\\mathbf{x}$ 的对数似然：\n$$\n\\log \\hat{p}(\\mathbf{x}) = -\\tfrac{1}{2}\\left[ (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}})^\\top \\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}}^{-1} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}) + d \\log(2\\pi) + \\log \\det(\\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}}) \\right]\n$$\n项 $(\\mathbf{x} - \\hat{\\boldsymbol{\\mu}})^\\top \\hat{\\boldsymbol{\\Sigma}}_{\\text{shrunk}}^{-1} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}})$ 是马氏距离（Mahalanobis distance）的平方，它衡量了 $\\mathbf{x}$ 到分布中心的距离，同时考虑了方差和相关性。\n\n建立一个决策阈值 $\\tau$ 来区分分布内和分布外的样本。该阈值通过计算所有训练样本的对数似然并找到 $q=0.05$ 的分位数来校准。这意味着 $5\\%$ 的训练数据其对数似然将低于 $\\tau$。\n如果一个描述符为 $\\mathbf{x}_{\\text{new}}$ 的新环境的对数似然低于此阈值，即 $\\log \\hat{p}(\\mathbf{x}_{\\text{new}})  \\tau$，则将其分类为“不安全”（分布外）。\n\n**3. 实现与评估**\n\n该算法首先通过执行训练阶段来实现：生成 $N_{\\mathrm{train}}$ 个环境，计算它们的描述符，拟合收缩后的多元高斯模型，并计算决策阈值 $\\tau$。通过为训练数据生成使用固定的随机种子来确保确定性。\n\n接下来，测试阶段评估六个不同的原子环境：\n-   **用例 1**：一个类似分布内的环境，但具有略微不同的平均半径和抖动。\n-   **用例 2**：一个没有相邻原子的空环境。\n-   **用例 3**：一个有两个相邻原子正好在截断半径处的环境。\n-   **用例 4**：一个在异常短距离处有相邻原子的异常环境。\n-   **用例 5**：一个具有非典型高配位数的环境。\n-   **用例 6**：一个配位数有效但壳层半径发生偏移的环境。\n\n对于每个测试用例，生成相邻原子距离（对每个随机用例使用唯一的固定种子），计算相应的描述符，并使用训练好的模型计算其对数似然。然后将此对数似然与阈值 $\\tau$ 进行比较，以确定该环境是否不安全。最终输出是这些布尔安全标志的有序集合。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Implements the out-of-distribution detection test for atomic environments.\n    \"\"\"\n    # -------------------\n    # 1. Define Parameters\n    # -------------------\n    \n    # Descriptor parameters\n    D_DIM = 8\n    MU_K = np.linspace(1.5, 4.0, D_DIM)\n    SIGMA = 0.15\n    R_CUTOFF = 4.5\n\n    # Training parameters\n    N_TRAIN = 200\n    TRAIN_N1 = 12\n    TRAIN_R1 = 2.5\n    TRAIN_N2 = 6\n    TRAIN_R2 = 3.5\n    TRAIN_S = 0.05\n\n    # Density model parameters\n    ALPHA = 0.1\n    EPSILON = 1e-6\n    QUANTILE_Q = 0.05\n    \n    # Random seed for training data generation\n    TRAIN_SEED = 42\n\n    # -------------------\n    # 2. Helper Functions\n    # -------------------\n\n    def compute_descriptor(distances, mus, sigma, r_c):\n        \"\"\"\n        Computes the radial descriptor for a given set of neighbor distances.\n        \"\"\"\n        distances_arr = np.array(distances, dtype=float)\n        neighbors_in_cutoff = distances_arr[distances_arr = r_c]\n        n_c = len(neighbors_in_cutoff)\n\n        if n_c == 0:\n            return np.zeros_like(mus)\n\n        # Use broadcasting for efficient computation\n        # neighbors_in_cutoff shape: (n_c,) -> (n_c, 1)\n        # mus shape: (d,) -> (1, d)\n        term = -(neighbors_in_cutoff[:, np.newaxis] - mus[np.newaxis, :])**2 / (2 * sigma**2)\n        gaussians = np.exp(term)\n        \n        # Sum over neighbors and normalize by n_c\n        descriptor = np.sum(gaussians, axis=0) / n_c\n        return descriptor\n\n    def log_likelihood_gaussian(x, mu, cov_inv, log_det_cov, d):\n        \"\"\"\n        Computes the log-likelihood of x under a multivariate Gaussian distribution.\n        \"\"\"\n        diff = x - mu\n        mahalanobis_sq = diff.T @ cov_inv @ diff\n        log_p = -0.5 * (mahalanobis_sq + d * np.log(2 * np.pi) + log_det_cov)\n        return log_p\n\n    # -------------------\n    # 3. Training Phase\n    # -------------------\n\n    # Generate training environments\n    np.random.seed(TRAIN_SEED)\n    training_environments = []\n    for _ in range(N_TRAIN):\n        # Sample distances for two shells with jitter\n        dists1 = np.random.normal(loc=TRAIN_R1, scale=TRAIN_S, size=TRAIN_N1)\n        dists2 = np.random.normal(loc=TRAIN_R2, scale=TRAIN_S, size=TRAIN_N2)\n        # Concatenate and truncate to be non-negative\n        all_dists = np.concatenate([dists1, dists2])\n        all_dists[all_dists  0] = 0.0\n        training_environments.append(all_dists.tolist())\n\n    # Compute training descriptors\n    training_descriptors = np.array([\n        compute_descriptor(dists, MU_K, SIGMA, R_CUTOFF) for dists in training_environments\n    ])\n\n    # Fit Gaussian model\n    mu_hat = np.mean(training_descriptors, axis=0)\n    cov_hat = np.cov(training_descriptors, rowvar=False)\n\n    # Apply covariance shrinkage\n    cov_shrunk = (\n        (1 - ALPHA) * cov_hat +\n        ALPHA * np.diag(np.diag(cov_hat)) +\n        EPSILON * np.identity(D_DIM)\n    )\n\n    # Pre-compute inverse and log-determinant for efficiency\n    cov_shrunk_inv = linalg.inv(cov_shrunk)\n    sign, log_det_cov_shrunk = np.linalg.slogdet(cov_shrunk)\n    if sign != 1:\n        # This should not happen with the given regularization\n        raise ValueError(\"Covariance matrix is not positive definite.\")\n\n    # Calibrate detection threshold\n    log_likelihoods_train = np.array([\n        log_likelihood_gaussian(x, mu_hat, cov_shrunk_inv, log_det_cov_shrunk, D_DIM)\n        for x in training_descriptors\n    ])\n    threshold = np.quantile(log_likelihoods_train, QUANTILE_Q)\n\n    # -------------------\n    # 4. Testing Phase\n    # -------------------\n\n    test_cases_params = [\n        # Case 1: In-distribution, small jitter\n        {'distributions': [(2.52, 0.02, 12), (3.48, 0.02, 6)], 'seed': 1},\n        # Case 2: Empty environment\n        {'distributions': [], 'seed': 2},\n        # Case 3: Boundary neighbors\n        {'distances': [4.5, 4.5], 'seed': 3},\n        # Case 4: Short-range anomaly\n        {'distributions': [(1.2, 0.01, 6)], 'seed': 4},\n        # Case 5: High coordination density\n        {'distributions': [(2.5, 0.05, 30), (3.5, 0.05, 15)], 'seed': 5},\n        # Case 6: Shifted shells\n        {'distributions': [(2.8, 0.05, 12), (3.0, 0.05, 6)], 'seed': 6},\n    ]\n\n    results = []\n    for params in test_cases_params:\n        np.random.seed(params['seed'])\n        \n        if 'distances' in params: # Deterministic case\n            test_distances = params['distances']\n        else: # Stochastic case\n            all_dists = []\n            for r_mean, r_jitter, n_atoms in params['distributions']:\n                dists = np.random.normal(loc=r_mean, scale=r_jitter, size=n_atoms)\n                all_dists.append(dists)\n            if all_dists:\n                test_distances = np.concatenate(all_dists)\n                test_distances[test_distances  0] = 0.0\n            else:\n                test_distances = []\n\n        # Compute descriptor and log-likelihood for the test case\n        test_descriptor = compute_descriptor(test_distances, MU_K, SIGMA, R_CUTOFF)\n        test_log_likelihood = log_likelihood_gaussian(\n            test_descriptor, mu_hat, cov_shrunk_inv, log_det_cov_shrunk, D_DIM\n        )\n\n        # Flag as unsafe if below threshold\n        is_unsafe = test_log_likelihood  threshold\n        results.append(is_unsafe)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n\n```"
        }
    ]
}