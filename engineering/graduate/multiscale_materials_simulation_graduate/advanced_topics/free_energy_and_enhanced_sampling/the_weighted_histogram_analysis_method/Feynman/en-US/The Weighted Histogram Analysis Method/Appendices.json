{
    "hands_on_practices": [
        {
            "introduction": "The Weighted Histogram Analysis Method (WHAM) is fundamentally an iterative algorithm that solves a set of self-consistent equations. To truly grasp its mechanics, there is no substitute for implementing this iterative core yourself. This first practice focuses on coding the WHAM fixed-point iteration for a small, well-defined toy system, allowing you to observe the convergence of the unbiased probability distribution and window free energies under different data conditions, from well-behaved to challenging .",
            "id": "3832602",
            "problem": "Consider the Weighted Histogram Analysis Method (WHAM) for combining biased histograms from multiple windows to estimate the underlying unbiased probability mass function over a discrete reaction coordinate. Weighted Histogram Analysis Method (WHAM) is a maximum-likelihood estimator for the unbiased distribution that corrects for biasing potentials applied in separate simulation windows and solves a self-consistent fixed-point problem coupling the unbiased distribution and window normalization free energies. In this problem, there are two windows and three discrete bins per window. Energies are reported in units of Boltzmann’s constant times temperature $k_{\\mathrm{B}} T$ so that inverse temperatures are dimensionless. Statistical inefficiencies are ignored (set to $1$) and bin widths are taken to be constant and absorbed into normalization.\n\nStarting from a uniform initial distribution $p^{(0)}(x_j)$ over $j \\in \\{1,2,3\\}$, implement the fixed-point iteration implied by the WHAM equations to update the unbiased distribution $p^{(t)}(x_j)$ and the window free energies $f_k^{(t)}$ for $k \\in \\{1,2\\}$, with normalization of $p^{(t)}(x_j)$ at each step to ensure $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$. Stop when the maximum absolute change $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|$ is less than the tolerance $10^{-6}$. Quantify the number of iterations required to reach the tolerance for each of the test cases below. Your program should begin with $p^{(0)}(x_j) = 1/3$ for all $j$, use the provided histogram counts and bias potentials, and treat the inverse temperature values as given. All computations should be performed in pure mathematical terms with no physical units reported.\n\nTest Suite:\n- Case A (balanced counts, symmetric biases, equal temperature):\n  - Window $1$ counts $H^{(1)} = [60,120,60]$, window $2$ counts $H^{(2)} = [40,80,40]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.0,0.5,1.0]$, window $2$ bias potentials $U^{(2)} = [1.0,0.5,0.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$.\n- Case B (scarce counts, asymmetric coverage, equal temperature):\n  - Window $1$ counts $H^{(1)} = [1,0,0]$, window $2$ counts $H^{(2)} = [0,1,1]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.0,1.0,2.0]$, window $2$ bias potentials $U^{(2)} = [2.0,1.0,0.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$.\n- Case C (unequal temperatures, mildly varying biases):\n  - Window $1$ counts $H^{(1)} = [100,50,25]$, window $2$ counts $H^{(2)} = [10,20,40]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.2,0.2,0.2]$, window $2$ bias potentials $U^{(2)} = [0.0,0.5,1.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 0.5$.\n- Case D (extreme biases with strongly polarized counts, equal temperature):\n  - Window $1$ counts $H^{(1)} = [200,10,1]$, window $2$ counts $H^{(2)} = [1,10,200]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.0,5.0,10.0]$, window $2$ bias potentials $U^{(2)} = [10.0,5.0,0.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$.\n\nAlgorithmic Requirements:\n- Initialize $p^{(0)}(x_j) = 1/3$ for $j \\in \\{1,2,3\\}$.\n- At each iteration $t$, update the window free energies $f_k^{(t)}$ using the normalization condition for window $k$, and then update the unbiased distribution $p^{(t)}(x_j)$ by combining the histograms from both windows with appropriate reweighting by the window biases and $f_k^{(t)}$, followed by normalization so that $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$.\n- Use the stopping criterion $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < 10^{-6}$ and report the total number of iterations executed to reach the tolerance.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets (e.g., $[7,42,13,5]$), where the integers correspond to the iteration counts for Cases A, B, C, and D, in that order.",
            "solution": "The problem has been validated and is determined to be a valid, well-posed scientific problem. All necessary data and conditions are provided, and the task aligns with established principles in statistical mechanics and computational science.\n\nThe problem requires the implementation of the Weighted Histogram Analysis Method (WHAM) to find the unbiased probability mass function, $p(x_j)$, for a system with a discrete reaction coordinate defined by $M=3$ bins. The data is derived from simulations in $K=2$ windows, each biased by a potential $U^{(k)}(x_j)$. The goal is to solve the WHAM equations iteratively for four different test cases and report the number of iterations required for convergence.\n\nThe WHAM equations form a set of self-consistent relations for the unbiased probabilities $p(x_j)$ and the dimensionless free energies $f_k$ associated with each simulation window $k$. For a system with $K$ windows and $M$ bins, these equations are:\n$$\np(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k e^{\\beta^{(k)} f_k} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n\\quad \\quad (1)\n$$\n$$\ne^{-\\beta^{(k)} f_k} = \\sum_{j=1}^{M} p(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)}\n\\quad \\quad (2)\n$$\nHere, $H^{(k)}(x_j)$ is the number of counts observed in bin $j$ from window $k$, $N_k = \\sum_{j=1}^{M} H^{(k)}(x_j)$ is the total number of samples from window $k$, $U^{(k)}(x_j)$ is the bias potential applied to bin $j$ in window $k$, and $\\beta^{(k)}$ is the corresponding inverse temperature (given as a dimensionless quantity). The probability distribution must satisfy the normalization condition $\\sum_{j=1}^{M} p(x_j) = 1$. Note that equation $(1)$ must be interpreted such that the resulting $p(x_j)$ is normalized.\n\nThese coupled, non-linear equations can be solved using a fixed-point iterative scheme. The algorithm proceeds as follows:\n\n1.  **Initialization**: Begin with an initial guess for the probability distribution, $p^{(0)}(x_j)$. The problem specifies a uniform distribution:\n    $$\n    p^{(0)}(x_j) = \\frac{1}{M}\n    $$\n    for $j \\in \\{1, 2, ..., M\\}$. Set the iteration counter $t=0$ and a tolerance $\\epsilon=10^{-6}$.\n\n2.  **Iteration**: For $t = 1, 2, 3, \\dots$, perform the following steps:\n    a.  **Update Free Energies**: Calculate the free energy $f_k^{(t)}$ for each window $k$ using the probability distribution from the previous iteration, $p^{(t-1)}(x_j)$. Rearranging equation $(2)$ gives:\n        $$\n        f_k^{(t)} = -\\frac{1}{\\beta^{(k)}} \\ln\\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)\n        $$\n        For numerical stability, it is often better to compute the quantities $C_k^{(t)} = e^{\\beta^{(k)} f_k^{(t)}}$:\n        $$\n        C_k^{(t)} = \\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)^{-1}\n        $$\n\n    b.  **Update Probabilities**: Use the updated free energy terms $C_k^{(t)}$ to compute a new, unnormalized probability distribution $p_{\\text{un}}^{(t)}(x_j)$ based on equation $(1)$:\n        $$\n        p_{\\text{un}}^{(t)}(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k C_k^{(t)} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n        $$\n        The numerator is the total number of counts in bin $j$ across all windows. The denominator is a reweighting factor that combines information from all windows.\n\n    c.  **Normalization**: The updated probability distribution $p^{(t)}(x_j)$ is obtained by normalizing $p_{\\text{un}}^{(t)}(x_j)$:\n        $$\n        p^{(t)}(x_j) = \\frac{p_{\\text{un}}^{(t)}(x_j)}{\\sum_{j'=1}^{M} p_{\\text{un}}^{(t)}(x_{j'})}\n        $$\n\n    d.  **Convergence Check**: The iteration is terminated when the maximum absolute difference between the current and previous probability distributions is less than the specified tolerance $\\epsilon$:\n        $$\n        \\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < \\epsilon\n        $$\n        If the condition is met, the process stops. Otherwise, increment $t$ and return to step 2a.\n\nThe final output is the number of iterations required to satisfy the convergence criterion for each of the four test cases provided. The implementation will utilize `numpy` for efficient vectorized calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_wham_iteration(H, U, beta, tol=1e-6):\n    \"\"\"\n    Performs WHAM fixed-point iteration to find the unbiased probability distribution.\n\n    Args:\n        H (np.ndarray): A (num_windows, num_bins) array of histogram counts.\n        U (np.ndarray): A (num_windows, num_bins) array of bias potentials.\n        beta (np.ndarray): A (num_windows,) array of inverse temperatures.\n        tol (float): The convergence tolerance.\n\n    Returns:\n        int: The number of iterations required to reach convergence.\n    \"\"\"\n    num_windows, num_bins = H.shape\n    \n    # Total number of samples in each window\n    N = np.sum(H, axis=1)\n    \n    # Total histogram counts across all windows for each bin\n    total_H = np.sum(H, axis=0)\n    \n    # Initial guess for the probability distribution (uniform)\n    p = np.full(num_bins, 1.0 / num_bins, dtype=np.float64)\n    \n    iterations = 0\n    while True:\n        iterations += 1\n        p_old = p.copy()\n        \n        # Pre-compute exponential terms for efficiency\n        # exp(-beta_k * U_kj) for all k, j\n        exp_neg_beta_U = np.exp(-beta[:, np.newaxis] * U)\n        \n        # Step 2a: Update free energies. We compute C_k = exp(beta_k * f_k).\n        # C_k = 1 / sum_j(p_j * exp(-beta_k * U_kj))\n        sum_val = np.sum(p_old[np.newaxis, :] * exp_neg_beta_U, axis=1)\n        # Avoid division by zero, although p_old and exp are always positive\n        # for this problem's setup.\n        C = 1.0 / sum_val\n        \n        # Step 2b: Update unnormalized probabilities p_un(x_j)\n        # Denominator: sum_k(N_k * C_k * exp(-beta_k * U_kj))\n        denominator = np.sum(N[:, np.newaxis] * C[:, np.newaxis] * exp_neg_beta_U, axis=0)\n        \n        p_unnormalized = np.zeros(num_bins, dtype=np.float64)\n        # Avoid division by zero if a bin has no counts at all\n        non_zero_H_mask = total_H > 0\n        non_zero_denom_mask = denominator > 0\n        valid_mask = non_zero_H_mask  non_zero_denom_mask\n        \n        p_unnormalized[valid_mask] = total_H[valid_mask] / denominator[valid_mask]\n\n        # Step 2c: Normalize probabilities\n        p_sum = np.sum(p_unnormalized)\n        if p_sum > 0:\n            p = p_unnormalized / p_sum\n        else:\n            # This would happen if total_H is all zeros, not the case for these problems.\n            # We can stop if no meaningful update is possible.\n            break\n\n        # Step 2d: Check for convergence\n        error = np.max(np.abs(p - p_old))\n        if error  tol:\n            break\n            \n    return iterations\n\ndef solve():\n    \"\"\"\n    Solves the WHAM iteration problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: balanced counts, symmetric biases, equal temperature\n        {\n            \"H\": np.array([[60, 120, 60], [40, 80, 40]], dtype=np.float64),\n            \"U\": np.array([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case B: scarce counts, asymmetric coverage, equal temperature\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 1]], dtype=np.float64),\n            \"U\": np.array([[0.0, 1.0, 2.0], [2.0, 1.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case C: unequal temperatures, mildly varying biases\n        {\n            \"H\": np.array([[100, 50, 25], [10, 20, 40]], dtype=np.float64),\n            \"U\": np.array([[0.2, 0.2, 0.2], [0.0, 0.5, 1.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 0.5], dtype=np.float64),\n        },\n        # Case D: extreme biases with strongly polarized counts, equal temperature\n        {\n            \"H\": np.array([[200, 10, 1], [1, 10, 200]], dtype=np.float64),\n            \"U\": np.array([[0.0, 5.0, 10.0], [10.0, 5.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        iterations = run_wham_iteration(case[\"H\"], case[\"U\"], case[\"beta\"])\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond a correct implementation, the quality of a Potential of Mean Force (PMF) reconstructed via WHAM depends critically on methodological choices, particularly the histogram bin width, $\\Delta x$. This parameter introduces a fundamental trade-off: smaller bins reduce systematic averaging bias but increase statistical variance, while larger bins do the opposite. This exercise provides a hands-on numerical study to quantify this bias-variance trade-off, a crucial step in developing the intuition needed to produce statistically robust scientific results .",
            "id": "2465743",
            "problem": "You will implement a numerical study of the Weighted Histogram Analysis Method (WHAM) to quantify, from first principles, the trade-off between systematic bias and statistical variance in the reconstructed potential of mean force (PMF) as a function of the histogram bin width $\\,\\Delta x\\,$. Your implementation must adhere to the following mathematical model and tasks.\n\nContext and core definitions:\n- In one dimension, the unbiased equilibrium probability density along a reaction coordinate $\\,x\\,$ is $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$, where $\\,U(x)\\,$ is the potential energy and $\\,\\beta = 1/(k_\\mathrm{B} T)\\,$. In this problem, work in reduced units where $\\,\\beta = 1\\,$ so that energies are in units of $\\,k_\\mathrm{B}T\\,$.\n- The potential of mean force (PMF) is defined (up to an additive constant) as $\\,F(x) = -\\ln p(x)\\,$. With $\\,\\beta = 1\\,$ and $\\,p(x) \\propto \\exp(-U(x))\\,$, the true PMF equals $\\,U(x)\\,$ plus a constant.\n- Umbrella sampling measures data under $\\,K\\,$ biased potentials $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$, where $\\,w_k(x)\\,$ is a known bias potential, here harmonic: $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$ with spring constant $\\,\\kappa > 0\\,$ and window center $\\,x_k\\,$.\n- A histogram estimator partitions the domain into bins of width $\\,\\Delta x\\,$ and uses bin counts. The Weighted Histogram Analysis Method (WHAM) combines histograms from multiple biased windows to estimate the unbiased density, and thus the PMF, by solving a self-consistent system for the density and the window free-energy offsets.\n\nYour tasks:\n1) Synthetic data generation from first principles. Let the true potential be the symmetric double-well\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\nwith $\\,\\alpha > 0\\,$ and $\\,c > 0\\,$, and consider the domain $\\,x \\in [-L, L]\\,$. For each umbrella window $\\,k \\in \\{1,\\dots,K\\}\\,$ with harmonic bias $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$ and a specified total sample size $\\,N_k\\,$, generate synthetic histogram counts by:\n- Computing the exact biased bin probabilities from the continuous densities $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ via numerical quadrature on a sufficiently fine grid over $\\,[-L,L]\\,$.\n- Drawing counts per bin for window $\\,k\\,$ from a multinomial distribution with total $\\,N_k\\,$ and the computed bin probabilities. This ensures scientific realism by directly reflecting the Boltzmann distribution under the bias, without relying on shortcuts or closed-form sampling.\n\n2) WHAM reconstruction. For a given $\\,\\Delta x\\,$, implement WHAM to estimate the unbiased density at the bin centers. Your implementation must use a fixed-point iteration that enforces proper normalization of the density and determines the free-energy offsets for each window. The PMF estimate is $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$ up to an additive constant; for numerical comparison, choose the constant such that $\\,\\min_b \\widehat{F}(x_b) = 0\\,$, and likewise set the true PMF $\\,F_\\text{true}(x)=U(x)\\,$ to have $\\,\\min_b F_\\text{true}(x_b) = 0\\,$ when evaluated on the same bin centers.\n\n3) Bias–variance analysis as a function of $\\,\\Delta x\\,$. For each candidate $\\,\\Delta x\\,$, repeat the synthetic dataset generation and WHAM reconstruction for $\\,R\\,$ independent replicates to empirically estimate:\n- The pointwise mean PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ and variance $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$ across replicates.\n- The integrated squared bias\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\nand the integrated variance\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- The mean integrated squared error (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$, which reflects the fundamental trade-off: increasing $\\,\\Delta x\\,$ increases bin-averaging bias while reducing statistical variance; decreasing $\\,\\Delta x\\,$ reduces bias while increasing variance due to fewer counts per bin.\n\nTest suite:\nAdopt the following fixed physical and numerical parameters in reduced units:\n- Double-well potential: $\\,\\alpha = 2\\,$, $\\,c = 1\\,$, domain $\\,[-L,L] = [-2,2]\\,$.\n- Number of umbrellas: $\\,K = 7\\,$ with centers $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$ and spring constant $\\,\\kappa = 20\\,$.\n- Number of independent replicates for bias–variance estimation: $\\,R = 40\\,$.\n- For all cases, assume equal counts per window $\\,N_k = N\\,$.\n\nDefine three test cases that explore variance-dominated, balanced, and bias-dominated regimes by varying $\\,N\\,$ and candidate bin widths:\n- Case $\\,1\\,$ (moderate sampling): $\\,N = 2000\\,$ and candidate $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$.\n- Case $\\,2\\,$ (low sampling): $\\,N = 500\\,$ and candidate $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$.\n- Case $\\,3\\,$ (high sampling): $\\,N = 10000\\,$ and candidate $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$.\n\nWhat your program must do:\n- For each test case, and for each candidate $\\,\\Delta x\\,$, perform the replicate-based bias–variance analysis described above and compute $\\,\\mathrm{MISE}(\\Delta x)\\,$.\n- For each test case, select the $\\,\\Delta x\\,$ that minimizes $\\,\\mathrm{MISE}(\\Delta x)\\,$. If there is a tie, choose the smallest $\\,\\Delta x\\,$ among the minimizers.\n\nFinal output format:\n- Your program should produce a single line of output containing the selected optimal bin widths for the three test cases as a comma-separated list enclosed in square brackets, for example $\\,\\texttt{[0.05,0.10,0.05]}\\,$. No additional text should be printed.\n\nImportant notes:\n- All energies are in units of $\\,k_\\mathrm{B}T\\,$ and $\\,x\\,$ is dimensionless.\n- Angles are not involved.\n- Randomness must be handled internally and reproducibly; fix a random seed.\n- The implementation must be self-contained and must not read or write any files or require user input.",
            "solution": "The problem requires a numerical investigation of the bias-variance trade-off for the Weighted Histogram Analysis Method (WHAM) as a function of histogram bin width, $\\Delta x$. The problem is scientifically valid, well-posed, and all necessary parameters are provided. It represents a standard task in computational chemistry and statistical mechanics, based on established principles. I will proceed with a full solution.\n\n### 1. Synthetic Data Generation\n\nTo ensure a scientifically realistic analysis, synthetic data must be generated directly from the underlying Boltzmann distribution. The true, unbiased potential is a symmetric double-well potential given by $U(x) = \\alpha(x^2 - c^2)^2$, with $\\alpha=2$ and $c=1$. The domain is $x \\in [-2, 2]$. All calculations are performed in reduced units where $\\beta = (k_B T)^{-1} = 1$.\n\nIn umbrella sampling, the system is simulated under $K$ different biasing potentials, $w_k(x)$, to enhance sampling of high-energy regions. Here, harmonic biases $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$ are used, with $\\kappa=20$ and $K=7$ windows centered at $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$.\n\nFor each window $k$, the total potential is $U_k^\\text{tot}(x) = U(x) + w_k(x)$, and the corresponding equilibrium probability density is $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$. To generate histogram counts for a given bin width $\\Delta x$, we first partition the domain $[-L, L]$ into discrete bins. The exact probability of observing a sample in bin $b$ (from $x_b^{\\text{start}}$ to $x_b^{\\text{end}}$) for window $k$ is given by the integral of the normalized density:\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\nThese integrals are computed via numerical quadrature over a fine grid. A high-resolution grid (with spacing significantly smaller than any $\\Delta x$) is used to approximate the integrals by a discrete sum (trapezoidal rule).\n\nWith the vector of bin probabilities $\\{P_{k,b}\\}$ for each window $k$, synthetic histogram counts $\\{N_{k,b}\\}$ are drawn from a multinomial distribution with $N_k$ total trials (samples). This process accurately models the statistical nature of sampling in a molecular simulation.\n\n### 2. WHAM Reconstruction\n\nWHAM provides a way to combine data from multiple biased simulations to compute the optimal estimate of the unbiased probability distribution, $p(x)$, and thus the potential of mean force (PMF), $F(x) = -\\ln p(x)$. The method solves a set of self-consistent equations for the unbiased probabilities $p_b$ at each bin center $x_b$ and the dimensionless free energies $f_k$ of each simulation window. With $\\beta=1$, the WHAM equations are:\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\nHere, $p_b$ are unnormalized probabilities proportional to the true density. These equations are solved using a fixed-point iteration:\n1. Initialize all free energies $f_k = 0$.\n2. Repeatedly compute the probabilities $p_b$ using the current $f_k$.\n3. Use the new $p_b$ to compute updated free energies $f_k^{\\text{new}}$.\n4. To prevent drift, a constraint is applied, such as fixing one free energy (e.g., $f_1=0$).\n5. The iteration continues until the free energies converge to a specified tolerance.\n\nAfter convergence, the final probabilities $p_b$ are used to compute the PMF estimate for the given dataset: $\\widehat{F}(x_b) = -\\ln p_b$. For comparison, the estimated PMF is shifted by an additive constant such that its minimum value is zero. The true PMF, $F_{\\text{true}}(x_b) = U(x_b)$, is also normalized to have a minimum of zero over the same set of bin centers.\n\nA critical issue arises when a bin $b$ has zero counts across all windows, i.e., $\\sum_k N_{k,b} = 0$. In this case, $p_b=0$ and the estimated PMF, $\\widehat{F}(x_b)$, is infinite. This represents a catastrophic failure of the estimator for that bin.\n\n### 3. Bias-Variance Analysis\n\nThe core of the problem is to quantify the trade-off between systematic error (bias) and statistical error (variance) as a function of bin width $\\Delta x$. For each candidate $\\Delta x$, we perform $R=40$ independent replicates of the data generation and WHAM reconstruction process.\n\nAcross the $R$ replicates, we compute:\n- The mean estimated PMF: $\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$.\n- The sample variance of the PMF: $\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$.\n\nThese are then integrated over the domain to yield the integrated squared bias $B^2(\\Delta x)$ and integrated variance $V(\\Delta x)$:\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\nThe Mean Integrated Squared Error (MISE) is the sum: $\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$.\n\nThe choice of $\\Delta x$ governs the trade-off:\n- **Small $\\Delta x$**: Low bias, as bin-averaging error is minimal. However, with fewer samples per bin, the statistical variance is high. This increases the probability of empty bins, which results in an infinite PMF estimate. If any replicate yields an infinite PMF for any bin, the mean PMF for that bin will also be infinite, leading to an infinite MISE.\n- **Large $\\Delta x$**: Low variance, as more samples are pooled into each bin, reducing the chance of empty bins. However, bias increases due to averaging the potential over a wider region.\n\nOur implementation handles the infinite PMF values by assigning an infinite MISE to any $\\Delta x$ that results in one or more empty bins in any of the $R$ replicates. The optimal $\\Delta x$ is then the one that minimizes the MISE among the choices that produce a finite MISE. This correctly penalizes bin widths that are too small for the given sample size. The final algorithm iterates through the candidate bin widths for each test case, calculates the MISE, and selects the optimal $\\Delta x$ according to the specified criteria.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b])  (fine_x  bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "In practical molecular simulations, computational cost is a major constraint, making efficient data collection paramount. This final practice moves from analyzing existing data to proactively planning new simulations. You will devise a principled, greedy algorithm to determine the optimal placement of new umbrella sampling windows, with the goal of minimizing the global uncertainty in the PMF as efficiently as possible, a key skill in the adaptive design of advanced simulation campaigns .",
            "id": "2465751",
            "problem": "You are given a one-dimensional reaction coordinate $x \\in [x_{\\min}, x_{\\max}]$ (expressed in nanometers) and a rough initial estimate of a Potential of Mean Force (PMF), denoted $\\widehat{F}(x)$ (expressed in kilojoules per mole). The system has been previously sampled using umbrella sampling with harmonic bias potentials, and you are asked to devise and implement a principled algorithm that determines where to place new umbrella windows to most rapidly reduce the uncertainty of the PMF when analyzed by the Weighted Histogram Analysis Method (WHAM).\n\nYour algorithm must be derived from first principles, starting from the Boltzmann distribution and the definition of importance sampling and its effective sample size. You must not assume any specialized formula for WHAM; instead, construct the algorithm using only these foundational elements:\n- The Boltzmann distribution: the unbiased probability density along $x$ satisfies $p(x) \\propto \\exp(-\\beta F(x))$ with $\\beta = 1/(k_{\\mathrm{B}} T)$. Here $k_{\\mathrm{B}}$ is the Boltzmann constant (per mole) and $T$ is the absolute temperature.\n- Under a harmonic umbrella centered at $c$ with spring constant $k$ (energy units per square nanometer), the bias potential is $U(x) = \\tfrac{1}{2} k (x - c)^2$, and the biased sampling density is proportional to $p(x)\\exp(-\\beta U(x))$.\n- For importance sampling with weights $w$, the effective sample size of a weighted average can be estimated using the standard importance sampling identity $N_{\\mathrm{eff}} = \\dfrac{(\\sum w)^2}{\\sum w^2}$, which is a well-tested statistical result.\n\nProblem objective and constraints:\n- The uncertainty of the PMF is largest where the unbiased density $p(x)$ is least well determined. Construct an uncertainty proxy along the coordinate using only the effective sample size per discretized bin, and define a scalar objective that aggregates this uncertainty along the coordinate. Your objective must be a sum over bins of a monotone decreasing function of the effective sample size (for example, an expression proportional to $1/\\sqrt{N_{\\mathrm{eff}}}$ is acceptable) and must include a small positive regularizer $\\varepsilon$ to avoid division by zero.\n- Given the current set of umbrella windows and their sample counts, use the rough PMF $\\widehat{F}(x)$ to predict the distribution of samples across bins for each window. Then, estimate the per-bin effective sample size contributed by all current windows combined. You may approximate bin-averaged quantities by evaluating them at bin centers.\n- To select new umbrella windows, use a greedy strategy: place one new window at a time by evaluating a discrete set of candidate centers and choosing the one that most reduces the aggregate uncertainty objective when its expected contribution is added. Repeat until the required number of new windows has been placed. Enforce a minimum separation constraint $\\Delta_{\\min}$ between any pair of window centers (existing or newly selected).\n- All integrals must be approximated by Riemann sums over a uniform grid. All probabilities must be properly normalized.\n\nNumerical and physical settings:\n- Units:\n  - $x$ in nanometers.\n  - $F$ and $U$ in kilojoules per mole.\n  - $k_{\\mathrm{B}} = 8.31446261815324 \\times 10^{-3}$ kilojoules per mole per kelvin.\n  - $T$ in kelvin.\n- Angles are not used in this problem.\n- There are no dimensional conversions beyond those implied by the choices above.\n\nDiscretization and candidates:\n- Discretize $[x_{\\min}, x_{\\max}]$ into $M$ uniformly spaced points $x_j$ with constant spacing $\\Delta x$; use bin centers as candidates for new window centers.\n- For each candidate center $z$, predict its impact on the effective sample sizes by modeling a new harmonic umbrella with specified spring constant and sample count, and add its contribution to the existing contributions from current windows.\n\nYour program must implement the algorithm described above and apply it to the following test suite. In each case, construct $\\widehat{F}(x)$ explicitly from the provided formula, use the specified existing windows, and select the required number of new windows subject to the minimum separation constraint.\n\nTest suite:\n- Common settings for all cases:\n  - $x_{\\min} = 0$ nanometers, $x_{\\max} = 1$ nanometer.\n  - Number of grid points: $M = 201$.\n  - Temperature: $T = 300$ kelvin.\n  - Regularizer for the effective sample size: $\\varepsilon = 10^{-12}$ (dimensionless, added inside any square root or denominator solely to prevent division by zero).\n  - Minimum center separation: $\\Delta_{\\min} = 0.05$ nanometers.\n  - Evaluate all bin-wise quantities at bin centers.\n\n- Case 1 (single-well PMF, typical interior uncertainty):\n  - PMF: $\\widehat{F}(x) = \\tfrac{1}{2} a (x - 0.5)^2$ with $a = 100$ kilojoules per mole per square nanometer.\n  - Existing windows: two windows with centers $c = [0.2, 0.8]$ nanometers, spring constants $k = [1000, 1000]$ kilojoules per mole per square nanometer, and sample counts $N = [1000, 1000]$.\n  - New windows to place: one at a time, select a total of $m_{\\text{new}} = 2$ windows, each with $k_{\\text{new}} = 1000$ kilojoules per mole per square nanometer and $N_{\\text{new}} = 1000$ samples.\n\n- Case 2 (double-well PMF with central barrier, poor overlap near barrier):\n  - PMF: $\\widehat{F}(x) = \\min\\{50(x-0.3)^2,\\,50(x-0.7)^2\\} + 10 \\exp\\!\\big(-\\tfrac{1}{2}\\big(\\tfrac{x-0.5}{0.05}\\big)^2\\big)$, with all energies in kilojoules per mole and $x$ in nanometers.\n  - Existing windows: two windows with centers $c = [0.25, 0.35]$ nanometers, spring constants $k = [1500, 1500]$ kilojoules per mole per square nanometer, and sample counts $N = [1500, 1500]$.\n  - New windows to place: $m_{\\text{new}} = 1$ window with $k_{\\text{new}} = 1500$ kilojoules per mole per square nanometer and $N_{\\text{new}} = 1500$ samples.\n\n- Case 3 (no existing data, uniform PMF, edge case of initial design):\n  - PMF: $\\widehat{F}(x) = 0$ for all $x$.\n  - Existing windows: none.\n  - New windows to place: $m_{\\text{new}} = 1$ window with $k_{\\text{new}} = 500$ kilojoules per mole per square nanometer and $N_{\\text{new}} = 2000$ samples.\n\nOutput requirements:\n- For each case, output a list of the selected new window centers in nanometers, rounded to three decimal places.\n- The final program output must be a single line containing a comma-separated list of the three case results, enclosed in a single pair of square brackets. Each case result must itself be a list of floats. For example: [[c1_1,c1_2],[c2_1],[c3_1]] with no spaces.",
            "solution": "The problem requires the formulation and implementation of a greedy algorithm to determine optimal placements for new umbrella sampling windows. The objective is to minimize the global uncertainty of the Potential of Mean Force (PMF), estimated using the Weighted Histogram Analysis Method (WHAM). The derivation must be based on first principles of statistical mechanics and importance sampling.\n\nFirst, we define the system and probabilities. The one-dimensional reaction coordinate $x$ is discretized into $M$ bins, represented by their centers $x_j$, for $j=1, \\dots, M$. The unbiased probability of the system being in bin $j$ is given by the Boltzmann distribution, $p(x_j) \\propto \\exp(-\\beta F(x_j))$, where $\\beta = 1/(k_{\\mathrm{B}}T)$ is the inverse temperature and $F(x)$ is the PMF. For our predictive algorithm, we use the provided estimate $\\widehat{F}(x)$. The unnormalized probability in bin $j$ is thus taken as $p^{\\text{un}}_j = \\exp(-\\beta \\widehat{F}(x_j))$.\n\nAn umbrella sampling simulation $i$ is characterized by a center $c_i$, a harmonic spring constant $k_i$, and a number of collected samples $N_i$. This introduces a bias potential $U_i(x) = \\frac{1}{2} k_i (x - c_i)^2$. The probability of observing the system in bin $j$ during simulation $i$ is altered by this bias:\n$$\nq_{ij} \\propto p(x_j) \\exp(-\\beta U_i(x_j))\n$$\nUpon normalization over all bins, the probability becomes:\n$$\nq_{ij} = \\frac{p^{\\text{un}}_j \\exp(-\\beta U_{ij})}{\\sum_{k=1}^M p^{\\text{un}}_k \\exp(-\\beta U_{ik})} = \\frac{p^{\\text{un}}_j \\exp(-\\beta U_{ij})}{Z_i}\n$$\nwhere $Z_i$ is the partition function for the biased simulation $i$. The expected number of samples from simulation $i$ that fall into bin $j$ is $n_{ij} = N_i q_{ij}$.\n\nTo combine data from all simulations, we use importance sampling. A sample from simulation $i$ at position $x_j$ must be reweighted to contribute to the unbiased ensemble. The corresponding importance weight $w_{ij}$ is the ratio of the target probability to the sampling probability:\n$$\nw_{ij} = \\frac{p(x_j)}{q_{ij}(x_j)} \\propto \\frac{\\exp(-\\beta \\widehat{F}(x_j))}{\\exp(-\\beta (\\widehat{F}(x_j) + U_{ij}))} = \\exp(\\beta U_{ij})\n$$\nThe proportionality constant is irrelevant for the effective sample size calculation, as it cancels out.\n\nThe problem states that the uncertainty in the PMF is highest where the sample statistics are poorest. We proxy this with the effective sample size, $N_{\\mathrm{eff}}$, for each bin $j$. Given a collection of samples in bin $j$ from all $L$ simulations, where there are $n_{ij}$ samples with weight $w_{ij}$ from each simulation $i$, the total effective sample size in bin $j$ is given by the standard formula:\n$$\nN_{\\mathrm{eff}, j} = \\frac{\\left( \\sum_{i=1}^L n_{ij} w_{ij} \\right)^2}{\\sum_{i=1}^L n_{ij} w_{ij}^2}\n$$\nThe uncertainty in bin $j$ is taken to be inversely related to $\\sqrt{N_{\\mathrm{eff}, j}}$. Our global objective function $J$, which we aim to minimize, is the sum of these uncertainties over all bins, with a small regularizer $\\varepsilon$ to ensure numerical stability:\n$$\nJ = \\sum_{j=1}^M \\frac{1}{\\sqrt{N_{\\mathrm{eff}, j} + \\varepsilon}}\n$$\nTo ensure numerical stability when computing $N_{\\mathrm{eff}, j}$, we reformulate the sums. The term in the numerator sum is $n_{ij} w_{ij} = (N_i p^{\\text{un}}_j/Z_i)$. The term in the denominator sum is $n_{ij} w_{ij}^2 = (N_i p^{\\text{un}}_j/Z_i) \\exp(\\beta U_{ij})$. This leads to:\n$$\nN_{\\mathrm{eff}, j} = \\frac{\\left( p^{\\text{un}}_j \\sum_{i=1}^L \\frac{N_i}{Z_i} \\right)^2}{p^{\\text{un}}_j \\sum_{i=1}^L \\frac{N_i}{Z_i} \\exp(\\beta U_{ij})} = p^{\\text{un}}_j \\frac{\\left(\\sum_i N_i/Z_i\\right)^2}{\\sum_i (N_i/Z_i) \\exp(\\beta U_{ij})}\n$$\nThe term $\\exp(\\beta U_{ij})$ can be very large, leading to overflow. To compute the sum in the denominator robustly, we use a log-sum-exp approach. For each bin $j$, we compute the logarithm of each term in the sum, $\\log(N_i) - \\log(Z_i) + \\beta U_{ij}$, and then use a numerically stable `logaddexp` function to find the logarithm of the sum, from which the sum itself is recovered by exponentiation.\n\nThe algorithm to select $m_{\\text{new}}$ new window centers is greedy. We begin with the set of existing windows and their parameters.\n1. Initialize the set of current window centers with any existing ones.\n2. For each of the $m_{\\text{new}}$ windows to be placed:\n    a. Generate a list of all valid candidate centers. A candidate (a grid point $x_j$) is valid if its distance to every center in the current set is at least $\\Delta_{\\min}$.\n    b. For each valid candidate center $z$, hypothetically add a new window with center $z$ and specified parameters ($k_{\\text{new}}, N_{\\text{new}}$) to the current set.\n    c. Calculate the objective function $J$ for this hypothetical complete set of windows.\n    d. Select the candidate center $z^*$ that results in the minimum value of $J$.\n    e. Permanently add this new window (centered at $z^*$) to the set of current windows for the next iteration.\n3. The final list of selected centers $\\{z^*_1, \\dots, z^*_{m_{\\text{new}}}\\}$ is the result.\n\nThis procedure ensures that each new window is placed where it is predicted to provide the greatest marginal reduction in the overall uncertainty of the PMF.",
            "answer": "```python\nimport numpy as np\n\n# Physical constants and settings\nKB_J_MOL_K = 8.31446261815324\nKB = KB_J_MOL_K / 1000.0  # In kJ/mol/K\n\ndef calculate_objective(centers, ks, Ns, F_hat, x_grid, beta, epsilon):\n    \"\"\"\n    Calculates the aggregate uncertainty objective J based on a stable N_eff formulation.\n    \"\"\"\n    num_bins = len(x_grid)\n    num_windows = len(centers)\n\n    if num_windows == 0:\n        # If there are no windows, N_eff is 0 for all bins.\n        return num_bins / np.sqrt(epsilon)\n\n    # Calculate unnormalized unbiased probabilities stably\n    log_p_un = -beta * F_hat\n\n    # For each window i, calculate U_i(j) and its partition function Z_i\n    U_matrix = np.zeros((num_windows, num_bins), dtype=np.float64)\n    log_Z_values = np.zeros(num_windows, dtype=np.float64)\n    for i in range(num_windows):\n        U_matrix[i, :] = 0.5 * ks[i] * (x_grid - centers[i])**2\n        log_integrand_i = log_p_un - beta * U_matrix[i, :]\n        log_Z_values[i] = np.logaddexp.reduce(log_integrand_i)\n        \n    log_N_values = np.log(np.array(Ns, dtype=np.float64))\n    log_Ni_minus_log_Zi = log_N_values - log_Z_values\n\n    # Calculate S1_j = p_un_j * sum_i(N_i/Z_i)\n    # The sum part is constant over j.\n    C = np.sum(np.exp(log_Ni_minus_log_Zi))\n    S1_j = np.exp(log_p_un) * C\n\n    # Calculate S2_j = p_un_j * sum_i( (N_i/Z_i) * exp(beta * U_ij) )\n    # The term in the sum is exp(log(N_i) - log(Z_i) + beta * U_ij)\n    log_terms_for_S2 = log_Ni_minus_log_Zi[:, np.newaxis] + beta * U_matrix\n    \n    # logsumexp over windows i for each bin j\n    log_sum_S2_terms = np.logaddexp.reduce(log_terms_for_S2, axis=0)\n    S2_j = np.exp(log_p_un + log_sum_S2_terms)\n\n    # Calculate N_eff_j = S1_j^2 / S2_j\n    n_eff = np.zeros(num_bins, dtype=np.float64)\n    non_zero_S2 = S2_j > 1e-300 # Avoid division by zero\n    n_eff[non_zero_S2] = (S1_j[non_zero_S2]**2) / S2_j[non_zero_S2]\n    \n    # Calculate final objective J\n    objective = np.sum(1.0 / np.sqrt(n_eff + epsilon))\n    return objective\n\ndef solve_case(params):\n    \"\"\"\n    Solves a single test case using the greedy window placement algorithm.\n    \"\"\"\n    x_grid = np.linspace(params['x_min'], params['x_max'], params['M'])\n    beta = 1.0 / (KB * params['T'])\n\n    # Define and calculate the PMF estimate on the grid\n    if params['case_id'] == 1:\n        a = 100\n        F_hat = 0.5 * a * (x_grid - 0.5)**2\n    elif params['case_id'] == 2:\n        well1 = 50 * (x_grid - 0.3)**2\n        well2 = 50 * (x_grid - 0.7)**2\n        barrier = 10 * np.exp(-0.5 * ((x_grid - 0.5) / 0.05)**2)\n        F_hat = np.minimum(well1, well2) + barrier\n    else: # Case 3\n        F_hat = np.zeros_like(x_grid)\n\n    current_centers = list(params['existing_c'])\n    current_ks = list(params['existing_k'])\n    current_Ns = list(params['existing_N'])\n    \n    newly_selected_centers = []\n    \n    for _ in range(params['m_new']):\n        best_objective = np.inf\n        best_center = None\n        \n        # Filter candidate centers based on minimum separation\n        valid_candidates = []\n        for cand_c in x_grid:\n            if all(np.abs(cand_c - c) >= params['delta_min'] for c in current_centers):\n                valid_candidates.append(cand_c)\n\n        if not valid_candidates:\n            break\n\n        for cand_center in valid_candidates:\n            temp_centers = current_centers + [cand_center]\n            temp_ks = current_ks + [params['k_new']]\n            temp_Ns = current_Ns + [params['N_new']]\n            \n            objective = calculate_objective(\n                temp_centers, temp_ks, temp_Ns,\n                F_hat, x_grid, beta, params['epsilon']\n            )\n            \n            if objective  best_objective:\n                best_objective = objective\n                best_center = cand_center\n        \n        if best_center is not None:\n            current_centers.append(best_center)\n            current_ks.append(params['k_new'])\n            current_Ns.append(params['N_new'])\n            newly_selected_centers.append(best_center)\n            \n    return [round(c, 3) for c in newly_selected_centers]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and orchestrates their solution.\n    \"\"\"\n    common_settings = {\n        'x_min': 0.0,\n        'x_max': 1.0,\n        'M': 201,\n        'T': 300.0,\n        'epsilon': 1e-12,\n        'delta_min': 0.05,\n    }\n\n    test_cases = [\n        {\n            'case_id': 1,\n            **common_settings,\n            'existing_c': [0.2, 0.8],\n            'existing_k': [1000.0, 1000.0],\n            'existing_N': [1000, 1000],\n            'm_new': 2,\n            'k_new': 1000.0,\n            'N_new': 1000,\n        },\n        {\n            'case_id': 2,\n            **common_settings,\n            'existing_c': [0.25, 0.35],\n            'existing_k': [1500.0, 1500.0],\n            'existing_N': [1500, 1500],\n            'm_new': 1,\n            'k_new': 1500.0,\n            'N_new': 1500,\n        },\n        {\n            'case_id': 3,\n            **common_settings,\n            'existing_c': [],\n            'existing_k': [],\n            'existing_N': [],\n            'm_new': 1,\n            'k_new': 500.0,\n            'N_new': 2000,\n        },\n    ]\n\n    results = [solve_case(case) for case in test_cases]\n    \n    # Format the output string exactly as specified.\n    # repr() adds spaces, so we remove them.\n    formatted_results = [repr(res).replace(' ', '') for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}