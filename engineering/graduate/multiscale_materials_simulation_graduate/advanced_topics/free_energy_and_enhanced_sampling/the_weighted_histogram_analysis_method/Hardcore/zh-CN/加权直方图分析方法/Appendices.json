{
    "hands_on_practices": [
        {
            "introduction": "理解加权直方图分析方法 (WHAM) 的最佳途径之一是亲手实现其核心的迭代方程。本练习  提供了一个简化的“玩具系统”，旨在让您专注于 WHAM 方程的自洽场特性，即无偏概率分布和窗口自由能之间的耦合关系。通过解决这个问题，您将具体地理解 WHAM 是如何通过数值迭代收敛到最终解的，并探索不同数据条件对收敛速度的影响。",
            "id": "3832602",
            "problem": "考虑使用加权直方图分析方法 (WHAM) 结合来自多个窗口的偏置直方图，以估计离散反应坐标上的潜在无偏置概率质量函数。加权直方图分析方法 (WHAM) 是一种用于无偏置分布的最大似然估计器，它校正了在不同模拟窗口中施加的偏置势，并求解一个耦合了无偏置分布和窗口归一化自由能的自洽不动点问题。在本问题中，有两个窗口，每个窗口有三个离散的箱 (bin)。能量以玻尔兹曼常数乘以温度 $k_{\\mathrm{B}} T$ 为单位报告，因此逆温度是无量纲的。统计效率被忽略（设为$1$），并且箱宽被视为常数并被吸收到归一化中。\n\n从 $j \\in \\{1,2,3\\}$ 上的均匀初始分布 $p^{(0)}(x_j)$ 开始，实现 WHAM 方程所隐含的不动点迭代，以更新无偏置分布 $p^{(t)}(x_j)$ 和窗口自由能 $f_k^{(t)}$（其中 $k \\in \\{1,2\\}$），并在每一步对 $p^{(t)}(x_j)$ 进行归一化以确保 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。当最大绝对变化 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|$ 小于容差 $10^{-6}$ 时停止。对于下面的每个测试用例，量化达到容差所需的迭代次数。您的程序应以对所有 $j$ 均有 $p^{(0)}(x_j) = 1/3$ 开始，使用提供的直方图计数和偏置势，并将给定的逆温度值视为已知。所有计算都应以纯数学形式进行，不报告物理单位。\n\n测试套件：\n- 测试用例 A (平衡计数，对称偏置，等温):\n  - 窗口 $1$ 计数 $H^{(1)} = [60,120,60]$，窗口 $2$ 计数 $H^{(2)} = [40,80,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,0.5,1.0]$，窗口 $2$ 偏置势 $U^{(2)} = [1.0,0.5,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 1.0$。\n- 测试用例 B (稀疏计数，不对称覆盖，等温):\n  - 窗口 $1$ 计数 $H^{(1)} = [1,0,0]$，窗口 $2$ 计数 $H^{(2)} = [0,1,1]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,1.0,2.0]$，窗口 $2$ 偏置势 $U^{(2)} = [2.0,1.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 1.0$。\n- 测试用例 C (不等温，轻度变化的偏置):\n  - 窗口 $1$ 计数 $H^{(1)} = [100,50,25]$，窗口 $2$ 计数 $H^{(2)} = [10,20,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.2,0.2,0.2]$，窗口 $2$ 偏置势 $U^{(2)} = [0.0,0.5,1.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 0.5$。\n- 测试用例 D (极端偏置，计数高度极化，等温):\n  - 窗口 $1$ 计数 $H^{(1)} = [200,10,1]$，窗口 $2$ 计数 $H^{(2)} = [1,10,200]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,5.0,10.0]$，窗口 $2$ 偏置势 $U^{(2)} = [10.0,5.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$，$\\beta^{(2)} = 1.0$。\n\n算法要求：\n- 初始化 $p^{(0)}(x_j) = 1/3$，其中 $j \\in \\{1,2,3\\}$。\n- 在每次迭代 $t$ 中，使用窗口 $k$ 的归一化条件更新窗口自由能 $f_k^{(t)}$，然后通过结合来自两个窗口的直方图，并使用窗口偏置和 $f_k^{(t)}$ 进行适当的重加权，来更新无偏置分布 $p^{(t)}(x_j)$，之后进行归一化以使 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。\n- 使用停止准则 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < 10^{-6}$，并报告达到容差所执行的总迭代次数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的整数列表（例如，$[7,42,13,5]$），其中的整数按顺序对应于测试用例 A、B、C 和 D 的迭代次数。",
            "solution": "该问题已经过验证，被确定为一个有效且适定的科学问题。所有必要的数据和条件均已提供，并且该任务符合统计力学和计算科学中的既定原则。\n\n该问题要求实现加权直方图分析方法 (WHAM)，以找到一个具有由 $M=3$ 个箱定义的离散反应坐标的系统的无偏置概率质量函数 $p(x_j)$。数据来源于在 $K=2$ 个窗口中的模拟，每个窗口都由一个势 $U^{(k)}(x_j)$ 进行偏置。目标是针对四个不同的测试用例迭代求解 WHAM 方程，并报告收敛所需的迭代次数。\n\nWHAM 方程为无偏置概率 $p(x_j)$ 和与每个模拟窗口 $k$ 相关的无量纲自由能 $f_k$ 构成了一组自洽关系。对于一个有 $K$ 个窗口和 $M$ 个箱的系统，这些方程是：\n$$\np(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k e^{\\beta^{(k)} f_k} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n\\quad \\quad (1)\n$$\n$$\ne^{-\\beta^{(k)} f_k} = \\sum_{j=1}^{M} p(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)}\n\\quad \\quad (2)\n$$\n这里，$H^{(k)}(x_j)$ 是在窗口 $k$ 的箱 $j$ 中观测到的计数，$N_k = \\sum_{j=1}^{M} H^{(k)}(x_j)$ 是来自窗口 $k$ 的总样本数，$U^{(k)}(x_j)$ 是施加在窗口 $k$ 的箱 $j$ 上的偏置势，而 $\\beta^{(k)}$ 是相应的逆温度（以无量纲量给出）。概率分布必须满足归一化条件 $\\sum_{j=1}^{M} p(x_j) = 1$。请注意，方程 $(1)$ 必须被解释为使得最终得到的 $p(x_j)$ 是归一化的。\n\n这些耦合的非线性方程可以使用不动点迭代方案求解。算法流程如下：\n\n1.  **初始化**：从概率分布的初始猜测 $p^{(0)}(x_j)$ 开始。问题指定了均匀分布：\n    $$\n    p^{(0)}(x_j) = \\frac{1}{M}\n    $$\n    对于 $j \\in \\{1, 2, ..., M\\}$。设置迭代计数器 $t=0$ 和容差 $\\epsilon=10^{-6}$。\n\n2.  **迭代**：对于 $t = 1, 2, 3, \\dots$，执行以下步骤：\n    a.  **更新自由能**：使用前一次迭代的概率分布 $p^{(t-1)}(x_j)$ 计算每个窗口 $k$ 的自由能 $f_k^{(t)}$。重新整理方程 $(2)$ 可得：\n        $$\n        f_k^{(t)} = -\\frac{1}{\\beta^{(k)}} \\ln\\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)\n        $$\n        为了数值稳定性，通常最好计算量 $C_k^{(t)} = e^{\\beta^{(k)} f_k^{(t)}}$：\n        $$\n        C_k^{(t)} = \\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)^{-1}\n        $$\n\n    b.  **更新概率**：使用更新后的自由能项 $C_k^{(t)}$，根据方程 $(1)$ 计算一个新的、未归一化的概率分布 $p_{\\text{un}}^{(t)}(x_j)$：\n        $$\n        p_{\\text{un}}^{(t)}(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k C_k^{(t)} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n        $$\n        分子是所有窗口中箱 $j$ 的总计数。分母是一个重加权因子，它结合了所有窗口的信息。\n\n    c.  **归一化**：通过对 $p_{\\text{un}}^{(t)}(x_j)$ 进行归一化，得到更新后的概率分布 $p^{(t)}(x_j)$：\n        $$\n        p^{(t)}(x_j) = \\frac{p_{\\text{un}}^{(t)}(x_j)}{\\sum_{j'=1}^{M} p_{\\text{un}}^{(t)}(x_{j'})}\n        $$\n\n    d.  **收敛性检查**：当当前和前一次概率分布之间的最大绝对差小于指定的容差 $\\epsilon$ 时，迭代终止：\n        $$\n        \\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < \\epsilon\n        $$\n        如果满足条件，则过程停止。否则，增加 $t$ 并返回到步骤 2a。\n\n最终输出是为满足所提供的四个测试用例中每个用例的收敛准则所需的迭代次数。实现将利用 `numpy` 进行高效的向量化计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_wham_iteration(H, U, beta, tol=1e-6):\n    \"\"\"\n    Performs WHAM fixed-point iteration to find the unbiased probability distribution.\n\n    Args:\n        H (np.ndarray): A (num_windows, num_bins) array of histogram counts.\n        U (np.ndarray): A (num_windows, num_bins) array of bias potentials.\n        beta (np.ndarray): A (num_windows,) array of inverse temperatures.\n        tol (float): The convergence tolerance.\n\n    Returns:\n        int: The number of iterations required to reach convergence.\n    \"\"\"\n    num_windows, num_bins = H.shape\n    \n    # Total number of samples in each window\n    N = np.sum(H, axis=1)\n    \n    # Total histogram counts across all windows for each bin\n    total_H = np.sum(H, axis=0)\n    \n    # Initial guess for the probability distribution (uniform)\n    p = np.full(num_bins, 1.0 / num_bins, dtype=np.float64)\n    \n    iterations = 0\n    while True:\n        iterations += 1\n        p_old = p.copy()\n        \n        # Pre-compute exponential terms for efficiency\n        # exp(-beta_k * U_kj) for all k, j\n        exp_neg_beta_U = np.exp(-beta[:, np.newaxis] * U)\n        \n        # Step 2a: Update free energies. We compute C_k = exp(beta_k * f_k).\n        # C_k = 1 / sum_j(p_j * exp(-beta_k * U_kj))\n        sum_val = np.sum(p_old[np.newaxis, :] * exp_neg_beta_U, axis=1)\n        # Avoid division by zero, although p_old and exp are always positive\n        # for this problem's setup.\n        C = 1.0 / sum_val\n        \n        # Step 2b: Update unnormalized probabilities p_un(x_j)\n        # Denominator: sum_k(N_k * C_k * exp(-beta_k * U_kj))\n        denominator = np.sum(N[:, np.newaxis] * C[:, np.newaxis] * exp_neg_beta_U, axis=0)\n        \n        p_unnormalized = np.zeros(num_bins, dtype=np.float64)\n        # Avoid division by zero if a bin has no counts at all\n        non_zero_H_mask = total_H > 0\n        non_zero_denom_mask = denominator > 0\n        valid_mask = non_zero_H_mask  non_zero_denom_mask\n        \n        p_unnormalized[valid_mask] = total_H[valid_mask] / denominator[valid_mask]\n\n        # Step 2c: Normalize probabilities\n        p_sum = np.sum(p_unnormalized)\n        if p_sum > 0:\n            p = p_unnormalized / p_sum\n        else:\n            # This would happen if total_H is all zeros, not the case for these problems.\n            # We can stop if no meaningful update is possible.\n            break\n\n        # Step 2d: Check for convergence\n        error = np.max(np.abs(p - p_old))\n        if error  tol:\n            break\n            \n    return iterations\n\ndef solve():\n    \"\"\"\n    Solves the WHAM iteration problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: balanced counts, symmetric biases, equal temperature\n        {\n            \"H\": np.array([[60, 120, 60], [40, 80, 40]], dtype=np.float64),\n            \"U\": np.array([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case B: scarce counts, asymmetric coverage, equal temperature\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 1]], dtype=np.float64),\n            \"U\": np.array([[0.0, 1.0, 2.0], [2.0, 1.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case C: unequal temperatures, mildly varying biases\n        {\n            \"H\": np.array([[100, 50, 25], [10, 20, 40]], dtype=np.float64),\n            \"U\": np.array([[0.2, 0.2, 0.2], [0.0, 0.5, 1.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 0.5], dtype=np.float64),\n        },\n        # Case D: extreme biases with strongly polarized counts, equal temperature\n        {\n            \"H\": np.array([[200, 10, 1], [1, 10, 200]], dtype=np.float64),\n            \"U\": np.array([[0.0, 5.0, 10.0], [10.0, 5.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        iterations = run_wham_iteration(case[\"H\"], case[\"U\"], case[\"beta\"])\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "从理想化的实现过渡到实际挑战，这个练习  提出了一个在科学研究中常见且棘手的问题：模拟数据的丢失。这种情况考验了我们对 WHAM 方法一个关键先决条件——窗口间统计重叠——的理解。您的任务是评估处理不完整数据的不同策略，并理解为何直接使用可用数据会导致最终的平均力势（PMF）出现特定且可预测的缺陷，从而强调数据质量评估的重要性。",
            "id": "2465737",
            "problem": "您在一个温度为 $T = 300\\,\\mathrm{K}$ 的分子体系中，沿着一维反应坐标 $x$ 进行伞形采样。采样共设 $30$ 个窗口，中心位置为 $x_i = (i-1)\\times 0.10\\,\\mathrm{nm}$（$i \\in \\{1,2,\\dots,30\\}$），每个窗口都施加了谐波伞形偏置 $w_i(x) = \\tfrac{1}{2}k (x - x_i)^2$，力常数为 $k = 1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}$。需要使用加权直方图分析方法（Weighted Histogram Analysis Method, WHAM）估算沿 $x$ 坐标的无偏平均力势（potential of mean force, PMF）。窗口 $\\#8$ 和 $\\#21$ 的时间序列数据已丢失，并且在当前的时间限制下无法恢复或重新模拟。您可以假设在每个窗口内，潜在的无偏势能 $U_0(x)$ 在 $x_i$ 附近的局部变化与伞形曲率相比是温和的，因此 $x$ 的有偏采样主要由伞形偏置决定，可以近似为方差为 $\\sigma^2 \\approx k_B T / k$ 的局部高斯分布，其中 $k_B$ 是玻尔兹曼常数。\n\n在这些限制条件下，以下哪种策略能产生最佳的 PMF？您预期结果中会出现什么缺陷？\n\nA. 通过对相邻直方图沿 $x$ 轴进行线性插值来填补缺失的直方图，然后对所有 $30$ 个窗口运行 WHAM 以强制连续性。得到的 PMF 将是连续的，且不确定性与邻近区域相当。\n\nB. WHAM 需要完整、连续的覆盖；没有了缺失的窗口，WHAM 根本无法使用。唯一可行的选择是丢弃整个数据集，直到窗口 $\\#8$ 和 $\\#21$ 被重新模拟。\n\nC. 对可用的 $28$ 个窗口应用 WHAM（或多态贝内特接受率方法，Multistate Bennett Acceptance Ratio, MBAR），首先检查沿 $x$ 轴的统计重叠。将数据视为由窗口 $\\#8$ 和 $\\#21$ 处的间隙分隔开的最多三个连通分量。在每个连通分量上提取分段 PMF，并报告 (i) 不相连片段之间的相对自由能偏移量是未确定的，以及 (ii) 在邻近缺失窗口的边缘处，不确定性会增大，潜在偏差也会增加。可以选择使用更粗的 $x$ 轴分箱来稳定噪声，但不要在没有新数据的情况下尝试弥合间隙。\n\nD. 通过后处理重加权轨迹，以等效地增强所有窗口的伞形偏置（即增大 $k$ 值），从而扩大有效覆盖范围并恢复跨越间隙的重叠。然后，WHAM 将重构出一个单一连续的 PMF，其误差最多为一个统一的加性常数。\n\nE. 从 WHAM 切换到仅使用可用窗口对平均力 $\\langle \\partial w_i/\\partial x \\rangle$ 沿 $x$ 进行伞形积分。因为力积分可得到 PMF，所以 $x$ 坐标上的间隙不影响积分，得到的 PMF 将是正确的，最多只相差一个全局加性常数。",
            "solution": "首先对问题陈述进行严格验证。\n\n步骤 1：提取已知条件\n- 方法：伞形采样和加权直方图分析方法（WHAM）。\n- 反应坐标：一维坐标 $x$。\n- 温度：$T = 300\\,\\mathrm{K}$。\n- 窗口数量：$30$ 个。\n- 窗口中心：$x_i = (i-1)\\times 0.10\\,\\mathrm{nm}$，其中 $i \\in \\{1,2,\\dots,30\\}$。\n- 伞形偏置势：$w_i(x) = \\tfrac{1}{2}k (x - x_i)^2$。\n- 力常数：$k = 1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}$。\n- 目标：估算无偏平均力势（PMF），$U_0(x)$。\n- 数据丢失：窗口 $\\#8$ 和 $\\#21$ 的时间序列数据丢失。\n- 假设：潜在的无偏势能 $U_0(x)$ 在 $x_i$ 附近的局部变化与伞形曲率相比是温和的。\n- 近似：在窗口 $i$ 中采样的 $x$ 的分布是局部高斯分布，方差为 $\\sigma^2 \\approx k_B T / k$。\n- 常数：玻尔兹曼常数 $k_B$。\n\n步骤 2：使用提取的已知条件进行验证\n该问题描述了计算化学中一个常见情景，涉及伞形采样模拟和使用 WHAM 重构 PMF。所有概念，包括伞形偏置势的形式、WHAM、PMF 和高斯近似，都是标准的和科学上合理的。提供的温度、力常数和窗口间距的数值对于此类模拟是物理上现实的。问题的核心是确定处理特定且现实的数据丢失类型的最佳实践方法。该问题提法得当，因为它要求的是科学上最可信的策略和预期的缺陷，而不是一个无法获得的精确解。语言客观而精确。该问题没有违反任何有效性标准。\n\n步骤 3：结论与行动\n问题陈述被宣布为有效。将基于统计力学和 WHAM 形式体系的基本原理推导解决方案。\n\nWHAM 的主要目的是结合来自多个有偏模拟（窗口）的数据，以获得沿坐标 $x$ 的无偏概率分布 $P_0(x)$ 的全局最优估计。然后，PMF 由 $U_0(x) = -k_B T \\ln P_0(x) + C$ 给出，其中 $C$ 是一个任意常数。WHAM 方程是一组自洽方程，用于求解 $P_0(x)$ 和每个窗口的相对偏置自由能 $F_i$：\n$$ P_0(x) \\propto \\frac{\\sum_{i=1}^{M} N_i(x)}{\\sum_{j=1}^{M} n_j \\exp[-(w_j(x) - F_j)/(k_B T)]} $$\n$$ \\exp(-F_i / (k_B T)) = \\int P_0(x') \\exp(-w_i(x') / (k_B T)) dx' $$\n这里，$M$ 是窗口数，$N_i(x)$ 是来自窗口 $i$ 的坐标 $x$ 的直方图，$n_i = \\int N_i(x')dx'$ 是该窗口中的总样本数。\n\nWHAM 方程要能得出一个覆盖整个 $x$ 范围的、单一连续的 PMF，一个关键要求是相邻窗口之间必须有统计重叠。这种重叠使得窗口之间的自由能差异（体现在 $F_i$ 项中）能够被确定，从而将整个系列的模拟连接起来。\n\n让我们根据提供的信息评估重叠情况。问题陈述中指出，每个窗口中的有偏概率分布近似为一个方差为 $\\sigma^2 \\approx k_B T / k$ 的高斯分布。\n使用 $k_B \\approx 8.314 \\times 10^{-3} \\,\\mathrm{kJ\\,mol^{-1}\\,K^{-1}}$，$T = 300\\,\\mathrm{K}$，以及 $k = 1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}$：\n$$ \\sigma^2 \\approx \\frac{(8.314 \\times 10^{-3} \\,\\mathrm{kJ\\,mol^{-1}\\,K^{-1}}) \\times (300\\,\\mathrm{K})}{1000\\,\\mathrm{kJ\\,mol^{-1}\\,nm^{-2}}} \\approx 2.494 \\times 10^{-3}\\,\\mathrm{nm^2} $$\n因此，标准差为：\n$$ \\sigma \\approx \\sqrt{2.494 \\times 10^{-3}\\,\\mathrm{nm^2}} \\approx 0.0499\\,\\mathrm{nm} \\approx 0.05\\,\\mathrm{nm} $$\n相邻窗口中心之间的间距为 $\\Delta x = 0.10\\,\\mathrm{nm}$。这对应于 $0.10\\,\\mathrm{nm} / 0.05\\,\\mathrm{nm} = 2\\sigma$ 的分离。这种分离是标准的，确保了相邻窗口之间有良好的统计重叠。\n\n然而，由于窗口 $\\#8$ 和 $\\#21$ 的缺失，数据中存在两个间隙。\n1. 在窗口 $\\#7$（中心 $x_7 = 0.70\\,\\mathrm{nm}$）和窗口 $\\#9$（中心 $x_9 = 0.80\\,\\mathrm{nm}$）之间。分离距离为 $0.20\\,\\mathrm{nm}$，约为 $4\\sigma$。\n2. 在窗口 $\\#20$（中心 $x_{20} = 1.90\\,\\mathrm{nm}$）和窗口 $\\#22$（中心 $x_{22} = 2.10\\,\\mathrm{nm}$）之间。分离距离同样为 $0.20\\,\\mathrm{nm}$，约为 $4\\sigma$。\n\n两个相隔 $4\\sigma$ 的高斯分布之间的重叠极小。一个高斯分布在另一个均值处的概率密度与 $\\exp(- (4\\sigma)^2 / (2\\sigma^2)) = \\exp(-8)$ 成正比，这是一个非常小的数字。这种近乎为零的重叠意味着没有统计上可靠的方法来连接这些间隙两端的自由能。WHAM 程序将有效地把数据视为三个不相连的模拟集合（或“分量”）：\n- 分量 1：窗口 $\\#1$ 到 $\\#7$。\n- 分量 2：窗口 $\\#9$ 到 $\\#20$。\n- 分量 3：窗口 $\\#22$ 到 $\\#30$。\n\nWHAM（或其更通用的变体 MBAR）可以单独应用于每个分量，以产生一个连续的 PMF 片段。然而，这三个片段之间的相对自由能偏移量将是未确定的。我们可以得到在 $[x_1, x_7]$、$[x_9, x_{20}]$ 和 $[x_{22}, x_{30}]$ 范围内的 PMF 形状，但我们无法知道如何垂直移动这三条曲线以将它们对齐成一个单一的全局 PMF。此外，在这些分量的边缘附近（例如，从窗口 $\\#7$、$\\#9$、$\\#20$ 和 $\\#22$ 导出的 PMF），估计的统计质量会下降。$P_0(x)$ 的 WHAM 估计器受益于所有采样了区域 $x$ 的窗口的贡献。在窗口 $\\#8$ 附近的间隙处，数据缺少一个关键贡献者，导致在 $x_7$ 和 $x_9$ 周围的 PMF 估计具有更高的统计不确定性和潜在的偏差。\n\n基于以上理解，我们评估每个选项。\n\nA. 通过对相邻直方图沿 $x$ 轴进行线性插值来填补缺失的直方图，然后对所有 $30$ 个窗口运行 WHAM 以强制连续性。得到的 PMF 将是连续的，且不确定性与邻近区域相当。\n这个策略存在根本性缺陷。直方图是随机过程的结果，而不是一个平滑的确定性函数。对直方图进行插值相当于捏造数据。这种方法在统计力学中没有依据，并将引入无法控制和量化的系统误差。间隙内真实 PMF 的任何特征（例如能垒或势阱）都将被人工平滑掉。声称不确定性将“与邻近区域相当”是错误的，因为伪造数据的误差属性是未知的。结论：**不正确**。\n\nB. WHAM 需要完整、连续的覆盖；没有了缺失的窗口，WHAM 根本无法使用。唯一可行的选择是丢弃整个数据集，直到窗口 $\\#8$ 和 $\\#21$ 被重新模拟。\n这种说法过于绝对且不切实际。WHAM 是一种稳健的方法，可以应用于任何状态（窗口）集合，无论它们是否连通。如果状态不连通，结果是一组相对偏移量未知的 PMF，这仍然是有价值的信息。丢弃 $28$ 个有效的模拟是计算资源和数据的巨大浪费。提取部分信息远胜于不提取任何信息。结论：**不正确**。\n\nC. 对可用的 $28$ 个窗口应用 WHAM（或多态贝内特接受率方法，MBAR），首先检查沿 $x$ 轴的统计重叠。将数据视为由窗口 $\\#8$ 和 $\\#21$ 处的间隙分隔开的最多三个连通分量。在每个连通分量上提取分段 PMF，并报告 (i) 不相连片段之间的相对自由能偏移量是未确定的，以及 (ii) 在邻近缺失窗口的边缘处，不确定性会增大，潜在偏差也会增加。可以选择使用更粗的 $x$ 轴分箱来稳定噪声，但不要在没有新数据的情况下尝试弥合间隙。\n这个策略是唯一在统计上严谨且在科学上诚实的。它正确地指出了重叠性差的后果：数据集被划分为不连通的分量。它正确地诊断了主要缺陷：这些分量之间的相对自由能是未确定的。它还正确地预测了次要缺陷：由于信息缺失，在间隙边界处不确定性增加和潜在偏差增大。提及 MBAR 是恰当的，因为它是一个相关的、更通用的方法，但会面临同样的限制。这种方法在最大化利用可用数据的同时，透明地报告了所得 PMF 的局限性。结论：**正确**。\n\nD. 通过后处理重加权轨迹，以等效地增强所有窗口的伞形偏置（即增大 $k$ 值），从而扩大有效覆盖范围并恢复跨越间隙的重叠。\n这个提议是基于对重加权和力常数 $k$ 效应的误解。增加 $k$ 会使谐波束缚*更强*或*更硬*，这会导致采样分布*更窄*，而不是更宽。这会使重叠恶化，而不是恢复它。要扩大采样范围，需要重加权到一个更小的 $k$ 值。然而，重加权仅在目标分布和采样分布有显著重叠时才有效。试图通过重加权窗口 $\\#7$ 的数据来预测窗口 $\\#8$ 区域（距离窗口 $\\#7$ 中心 $2\\sigma$ 的间隙）的行为已经很勉强了，而通过重加权来弥合窗口 $\\#7$ 和 $\\#9$ 均值之间整整 $4\\sigma$ 的间隙在统计上是毫无希望的。这将导致巨大的统计误差，因为权重将被少数不具代表性的数据点所主导。结论：**不正确**。\n\nE. 从 WHAM 切换到仅使用可用窗口对平均力 $\\langle \\partial w_i/\\partial x \\rangle$ 沿 $x$ 进行伞形积分。因为力积分可得到 PMF，所以 $x$ 坐标上的间隙不影响积分，得到的 PMF 将是正确的，最多只相差一个全局加性常数。\n这个选项是错误的。伞形积分通过对无偏平均力进行积分来重构 PMF，即 $U_0(x) = -\\int \\langle F_{unbiased}(x') \\rangle dx'$。无偏平均力 $\\langle F_{unbiased}(x') \\rangle$ 本身必须在整个积分范围内的所有 $x'$ 值处被估算。这通常是通过结合所有窗口的数据来完成的。由于窗口 $\\#8$ 和 $\\#21$ 缺失，在某些 $x$ 的区域我们没有数据来估算 $\\langle F_{unbiased}(x') \\rangle$。声称“$x$ 坐标上的间隙不影响积分”是错误的。积分要求被积函数在积分域上是已知的。没有间隙中的数据，积分就无法跨越它们进行，PMF 也无法连接起来。该方法与 WHAM 存在同样根本的问题。结论：**不正确**。",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "现在，我们进入一个更高级的主题：如何优化分析方法本身以获得最准确的结果。与许多统计方法一样，WHAM 的应用涉及一些关键参数的选择，这些选择会直接影响结果的准确性。本练习  聚焦于直方图的“箱子宽度” $\\Delta x$，并探索一个基本性的权衡关系：系统偏差（源于在宽箱子内的平均效应）与统计方差（源于在窄箱子内样本量过少）之间的权衡。通过数值模拟这种权衡，您将学会如何选择最优参数以最小化总误差（平均积分平方误差，MISE）。",
            "id": "2465743",
            "problem": "您将实现一个关于加权直方图分析方法 (WHAM) 的数值研究，以便从第一性原理量化重建的平均力势 (PMF) 中系统偏差和统计方差之间随直方图箱宽 $\\,\\Delta x\\,$ 变化的权衡关系。您的实现必须遵循以下数学模型和任务。\n\n上下文与核心定义：\n- 在一维空间中，沿反应坐标 $\\,x\\,$ 的无偏倚平衡概率密度为 $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$，其中 $\\,U(x)\\,$ 是势能，$\\,\\beta = 1/(k_\\mathrm{B} T)\\,$。在本问题中，请在约化单位下工作，其中 $\\,\\beta = 1\\,$，因此能量单位为 $\\,k_\\mathrm{B}T\\,$。\n- 平均力势 (PMF) 定义为（不计一个附加常数）$\\,F(x) = -\\ln p(x)\\,$。当 $\\,\\beta = 1\\,$ 且 $\\,p(x) \\propto \\exp(-U(x))\\,$ 时，真实的 PMF 等于 $\\,U(x)\\,$ 加上一个常数。\n- 伞形采样在 $\\,K\\,$ 个偏置势 $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$ 下测量数据，其中 $\\,w_k(x)\\,$ 是一个已知的偏置势，此处为谐波势：$\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$，$\\,\\kappa  0\\,$ 为弹簧常数，$\\,x_k\\,$ 为窗口中心。\n- 直方图估计器将域划分为宽度为 $\\,\\Delta x\\,$ 的箱，并使用箱内计数。加权直方图分析方法 (WHAM) 结合来自多个偏置窗口的直方图，通过求解一个关于密度和窗口自由能偏移量的自洽系统，来估计无偏倚的密度，并由此得到 PMF。\n\n您的任务：\n1) 从第一性原理生成合成数据。设真实势为对称双阱势\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\n其中 $\\,\\alpha  0\\,$ 且 $\\,c  0\\,$，并考虑域 $\\,x \\in [-L, L]\\,$。对于每个具有谐波偏置 $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$ 和指定总样本量 $\\,N_k\\,$ 的伞形窗口 $\\,k \\in \\{1,\\dots,K\\}\\,$，通过以下方式生成合成的直方图计数：\n- 通过在 $\\,[-L,L]\\,$ 上足够精细的网格上进行数值积分，从连续密度 $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ 计算出精确的偏置箱概率。\n- 对于窗口 $\\,k\\,$，根据总数为 $\\,N_k\\,$ 和已计算出的箱概率，从一个多项分布中抽取每个箱的计数。这通过直接反映偏置下的玻尔兹曼分布来确保科学真实性，而不依赖于捷径或封闭形式的采样。\n\n2) WHAM 重建。对于给定的 $\\,\\Delta x\\,$，实现 WHAM 以估计箱中心的无偏倚密度。您的实现必须使用一个不动点迭代法，该方法强制执行密度的适当归一化，并确定每个窗口的自由能偏移量。PMF 估计值为 $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$（不计一个附加常数）；为进行数值比较，选择常数使得 $\\,\\min_b \\widehat{F}(x_b) = 0\\,$，同样地，将真实 PMF $\\,F_\\text{true}(x)=U(x)\\,$ 设置为在相同箱中心上求值时 $\\,\\min_b F_\\text{true}(x_b) = 0\\,$。\n\n3) 作为 $\\,\\Delta x\\,$ 函数的偏差-方差分析。对于每个候选的 $\\,\\Delta x\\,$，重复合成数据集生成和 WHAM 重建过程 $\\,R\\,$ 次独立重复实验，以经验性地估计：\n- 逐点平均 PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ 和跨重复实验的方差 $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$。\n- 积分平方偏差\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\n和积分方差\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- 平均积分平方误差 (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$，它反映了基本的权衡关系：增加 $\\,\\Delta x\\,$ 会增加分箱平均偏差，但减少统计方差；减少 $\\,\\Delta x\\,$ 会减少偏差，但由于每个箱的计数更少，会增加方差。\n\n测试套件：\n在约化单位中采用以下固定的物理和数值参数：\n- 双阱势：$\\,\\alpha = 2\\,$，$\\,c = 1\\,$，域 $\\,[-L,L] = [-2,2]\\,$。\n- 伞形窗口数量：$\\,K = 7\\,$，中心位于 $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$，弹簧常数 $\\,\\kappa = 20\\,$。\n- 用于偏差-方差估计的独立重复实验次数：$\\,R = 40\\,$。\n- 在所有情况下，假设每个窗口的计数相等 $\\,N_k = N\\,$。\n\n通过改变 $\\,N\\,$ 和候选的箱宽，定义三个测试用例，以探索方差主导、平衡和偏差主导的情况：\n- 用例 $\\,1\\,$ (中等采样量)：$\\,N = 2000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,2\\,$ (低采样量)：$\\,N = 500\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,3\\,$ (高采样量)：$\\,N = 10000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n\n您的程序必须做什么：\n- 对于每个测试用例和每个候选的 $\\,\\Delta x\\,$，执行上述基于重复实验的偏差-方差分析，并计算 $\\,\\mathrm{MISE}(\\Delta x)\\,$。\n- 对于每个测试用例，选择使 $\\,\\mathrm{MISE}(\\Delta x)\\,$ 最小化的 $\\,\\Delta x\\,$。如果出现平局，选择最小化项中最小的 $\\,\\Delta x\\,$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例选出的最优箱宽，形式为一个用方括号括起来的、以逗号分隔的列表，例如 $\\,\\texttt{[0.05,0.10,0.05]}\\,$。不应打印任何其他文本。\n\n重要说明：\n- 所有能量单位均为 $\\,k_\\mathrm{B}T\\,$，$\\,x\\,$ 是无量纲的。\n- 不涉及角度。\n- 随机性必须在内部处理且可复现；请固定一个随机种子。\n- 实现必须是自包含的，不得读取或写入任何文件，也不需要用户输入。",
            "solution": "该问题要求对加权直方图分析方法 (WHAM) 的偏差-方差权衡进行数值研究，该权衡是直方图箱宽 $\\Delta x$ 的函数。该问题在科学上有效、定义明确，并且所有必要的参数都已提供。它代表了计算化学和统计力学中的一项标准任务，基于已建立的原理。我将着手提供一个完整的解决方案。\n\n解决方案分三个主要阶段实现：合成数据生成、使用 WHAM 进行 PMF 重建，以及在多个重复实验中进行偏差-方差分析。\n\n### 1. 合成数据生成\n\n为确保科学上真实的分析，合成数据必须直接从底层的玻尔兹曼分布生成。真实的、无偏倚的势是一个对称双阱势，由 $U(x) = \\alpha(x^2 - c^2)^2$ 给出，其中 $\\alpha=2$ 且 $c=1$。域为 $x \\in [-2, 2]$。所有计算均在 $\\beta = (k_B T)^{-1} = 1$ 的约化单位下进行。\n\n在伞形采样中，系统在 $K$ 个不同的偏置势 $w_k(x)$ 下进行模拟，以增强对高能区域的采样。此处使用谐波偏置 $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$，其中 $\\kappa=20$，$K=7$ 个窗口的中心位于 $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$。\n\n对于每个窗口 $k$，总势为 $U_k^\\text{tot}(x) = U(x) + w_k(x)$，相应的平衡概率密度为 $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$。为了为给定的箱宽 $\\Delta x$ 生成直方图计数，我们首先将域 $[-L, L]$ 划分成离散的箱。对于窗口 $k$，观察到样本在箱 $b$（从 $x_b^{\\text{start}}$ 到 $x_b^{\\text{end}}$）中的确切概率由归一化密度的积分给出：\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\n这些积分通过在精细网格上进行数值积分来计算。使用高分辨率网格（其间距远小于任何 $\\Delta x$）通过离散求和（梯形法则）来近似积分。\n\n有了每个窗口 $k$ 的箱概率向量 $\\{P_{k,b}\\}$，就可以从具有 $N_k$ 次总试验（样本）的多项分布中抽取合成的直方图计数 $\\{N_{k,b}\\}$。这个过程精确地模拟了分子模拟中采样的统计性质。\n\n### 2. WHAM 重建\n\nWHAM 提供了一种方法，可以结合来自多个偏置模拟的数据，以计算无偏倚概率分布 $p(x)$ 的最优估计，从而得到平均力势 (PMF) $F(x) = -\\ln p(x)$。该方法求解一组关于每个箱中心 $x_b$ 处的无偏倚概率 $p_b$ 和每个模拟窗口的无量纲自由能 $f_k$ 的自洽方程。当 $\\beta=1$ 时，WHAM 方程为：\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\n这里，$p_b$ 是与真实密度成正比的未归一化概率。这些方程通过不动点迭代求解：\n1.  初始化所有自由能 $f_k = 0$。\n2.  使用当前的 $f_k$ 重复计算概率 $p_b$。\n3.  使用新的 $p_b$ 计算更新后的自由能 $f_k^{\\text{new}}$。\n4.  为防止漂移，施加一个约束，例如固定一个自由能（如 $f_1=0$）。\n5.  迭代继续，直到自由能收敛到指定的容差。\n\n收敛后，最终的概率 $p_b$ 用于计算给定数据集的 PMF 估计值：$\\widehat{F}(x_b) = -\\ln p_b$。为了便于比较，估计的 PMF 通过一个附加常数进行平移，使其最小值为零。真实的 PMF，$F_{\\text{true}}(x_b) = U(x_b)$，也被归一化，使其在同一组箱中心上的最小值为零。\n\n当一个箱 $b$ 在所有窗口中的计数均为零时，即 $\\sum_k N_{k,b} = 0$，会出现一个关键问题。在这种情况下，$p_b=0$，估计的 PMF $\\widehat{F}(x_b)$ 为无穷大。这代表该箱的估计器发生了灾难性失效。\n\n### 3. 偏差-方差分析\n\n问题的核心是量化系统误差（偏差）和统计误差（方差）之间随箱宽 $\\Delta x$ 变化的权衡。对于每个候选的 $\\Delta x$，我们执行 $R=40$ 次独立的数据生成和 WHAM 重建过程的重复实验。\n\n在这 $R$ 次重复实验中，我们计算：\n-   平均估计 PMF：$\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$。\n-   PMF 的样本方差：$\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$。\n\n然后将这些量在域上积分，得到积分平方偏差 $B^2(\\Delta x)$ 和积分方差 $V(\\Delta x)$：\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\n平均积分平方误差 (MISE) 是它们的和：$\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$。\n\n$\\Delta x$ 的选择决定了这种权衡：\n-   **小 $\\Delta x$**：偏差低，因为分箱平均误差最小。然而，由于每个箱的样本较少，统计方差会很高。这增加了空箱的概率，从而导致无限大的 PMF 估计值。如果任何一次重复实验对任何一个箱产生无限大的 PMF，那么该箱的平均 PMF 也将是无限的，导致无限大的 MISE。\n-   **大 $\\Delta x$**：方差低，因为更多的样本被汇集到每个箱中，减少了空箱的几率。然而，由于在更宽的区域上对势进行平均，偏差会增加。\n\n我们的实现通过将无限大的 MISE 分配给任何在 $R$ 次重复实验中导致一个或多个空箱的 $\\Delta x$ 来处理无限大的 PMF 值。最优的 $\\Delta x$ 则是那些产生有限 MISE 的选择中使 MISE 最小的那个。这会正确地惩罚那些对于给定样本量而言过小的箱宽。最终算法会遍历每个测试用例的候选箱宽，计算 MISE，并根据指定标准选择最优的 $\\Delta x$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b])  (fine_x  bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        }
    ]
}