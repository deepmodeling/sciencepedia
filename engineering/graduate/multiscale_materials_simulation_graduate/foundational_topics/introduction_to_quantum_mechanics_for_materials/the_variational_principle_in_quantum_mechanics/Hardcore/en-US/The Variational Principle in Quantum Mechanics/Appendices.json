{
    "hands_on_practices": [
        {
            "introduction": "In moving from the abstract formulation of quantum mechanics to concrete numerical implementations, the choice of basis is paramount. While orthonormal bases simplify the mathematics, many physically-motivated basis sets, such as atomic orbitals, are non-orthogonal. This practice explores the direct consequences of this choice, which transforms the standard eigenvalue problem into a generalized one and introduces the overlap matrix $S$. By analyzing a minimal two-state system, you will gain hands-on experience in diagnosing the numerical instabilities that can arise from near-linear dependence in a basis set .",
            "id": "3852559",
            "problem": "In atomistic-to-continuum multiscale materials simulation, nonorthogonal localized basis sets are widely used to represent quantum states, for example in Kohn–Sham Density Functional Theory (DFT) and tight-binding models. Consider a minimal model of two spatially localized, normalized basis functions $\\{\\phi_{1},\\phi_{2}\\}$ with nonzero mutual overlap, where the overlap integral $s \\equiv \\langle \\phi_{1} \\mid \\phi_{2} \\rangle$ satisfies $0  s  1$. The overlap matrix is the Gram matrix\n$$\nS \\equiv \\begin{pmatrix}\n1  s \\\\\ns  1\n\\end{pmatrix},\n$$\nwhich is real symmetric and positive definite. The system Hamiltonian in this basis is represented by a Hermitian matrix $H \\in \\mathbb{C}^{2 \\times 2}$, and stationary states solve the generalized Hermitian eigenproblem $H c = E S c$.\n\nStarting from core definitions only (Gram matrices, eigenvalues of symmetric positive definite matrices, and induced matrix norms), do the following:\n\n- Derive the $2$-norm spectral condition number of the overlap matrix $S$ as a function of $s$.\n\n- Quantify, to first order in small Hermitian perturbations $\\Delta H$ and $\\Delta S$ with $\\|\\Delta H\\|_{2} \\ll \\|H\\|_{2}$ and $\\|\\Delta S\\|_{2} \\ll \\|S\\|_{2}$, how nonorthogonality (measured by $s$) impacts the stability of a simple eigenvalue $E$ of the generalized problem $H c = E S c$. Use only the linear algebraic structure of generalized eigenpairs and the fact that normalized generalized eigenvectors can be chosen to satisfy $c^{*} S c = 1$. Your analysis should make explicit the dependence on $s$ through the overlap matrix and should conclude how the sensitivity behaves as $s \\to 1^{-}$.\n\nReport only the closed-form analytic expression for the $2$-norm condition number $\\kappa_{2}(S)$ as a function of $s$ as your final answer. No numerical evaluation or rounding is required, and no physical units are needed.",
            "solution": "The problem presents two tasks: first, to derive the spectral condition number of a specific $2 \\times 2$ overlap matrix $S$, and second, to analyze the stability of the generalized eigenvalue problem $H c = E S c$ with respect to small perturbations in $H$ and $S$.\n\nThe problem is validated as sound. It is scientifically grounded in the principles of quantum mechanics and linear algebra as applied to computational materials science. The setup is well-posed, objective, and contains all necessary information to derive a unique solution for both parts.\n\nFirst, I will derive the $2$-norm spectral condition number, $\\kappa_{2}(S)$, of the overlap matrix $S$. The matrix is given by:\n$$\nS = \\begin{pmatrix}\n1  s \\\\\ns  1\n\\end{pmatrix}\n$$\nwhere $0  s  1$. By definition, the $2$-norm spectral condition number of a matrix $A$ is $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. For a real symmetric positive-definite matrix like $S$, the $2$-norm is its largest eigenvalue, $\\|S\\|_{2} = \\lambda_{\\max}(S)$, and the $2$-norm of its inverse is the reciprocal of its smallest eigenvalue, $\\|S^{-1}\\|_{2} = 1/\\lambda_{\\min}(S)$. Therefore, the condition number is the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa_{2}(S) = \\frac{\\lambda_{\\max}(S)}{\\lambda_{\\min}(S)}\n$$\nTo find the eigenvalues of $S$, we solve the characteristic equation $\\det(S - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det \\begin{pmatrix}\n1-\\lambda  s \\\\\ns  1-\\lambda\n\\end{pmatrix} = 0\n$$\nThis gives $(1-\\lambda)^{2} - s^{2} = 0$. Taking the square root of both sides, we get $1-\\lambda = \\pm s$, which yields the two eigenvalues:\n$$\n\\lambda = 1 \\mp s\n$$\nThe two eigenvalues are $\\lambda_{1} = 1+s$ and $\\lambda_{2} = 1-s$. Since the problem specifies that $0  s  1$, both eigenvalues are positive, confirming that $S$ is positive-definite. The largest and smallest eigenvalues are:\n$$\n\\lambda_{\\max}(S) = 1+s\n$$\n$$\n\\lambda_{\\min}(S) = 1-s\n$$\nSubstituting these into the formula for the condition number gives:\n$$\n\\kappa_{2}(S) = \\frac{1+s}{1-s}\n$$\n\nSecond, I will analyze how non-orthogonality impacts the stability of a simple eigenvalue $E$. We start with the unperturbed generalized eigenvalue problem $H c = E S c$ and the perturbed problem $(H + \\Delta H)(c + \\Delta c) = (E + \\Delta E)(S + \\Delta S)(c + \\Delta c)$. We expand the perturbed equation and retain only terms of first order in the small perturbations $\\Delta H$, $\\Delta S$, $\\Delta c$, and $\\Delta E$:\n$$\nHc + H\\Delta c + \\Delta H c + \\mathcal{O}(\\delta^{2}) = (ESc + ES\\Delta c + E\\Delta S c + \\Delta E S c) + \\mathcal{O}(\\delta^{2})\n$$\nUsing the unperturbed equation $Hc = ESc$, we can cancel terms from both sides:\n$$\nH\\Delta c + \\Delta H c \\approx ES\\Delta c + E\\Delta S c + \\Delta E S c\n$$\nTo find the first-order change in the eigenvalue, $\\Delta E$, we left-multiply by the adjoint of the unperturbed eigenvector, $c^{*}$:\n$$\nc^{*}H\\Delta c + c^{*}\\Delta H c \\approx c^{*}ES\\Delta c + c^{*}E\\Delta S c + c^{*}\\Delta E S c\n$$\nSince $H$ is Hermitian, $H=H^{*}$, and the eigenvalue $E$ of a Hermitian generalized eigenproblem is real ($E=E^{*}$). Thus, we can take the adjoint of the unperturbed equation: $(Hc)^{*} = (ESc)^{*}$, which gives $c^{*}H^{*} = c^{*}E^{*}S^{*}$, and so $c^{*}H = E c^{*}S$. Using this relationship, the term $c^{*}H\\Delta c$ on the left-hand side is equal to $E c^{*}S\\Delta c$, which is equivalent to the first term on the right-hand side, $c^{*}ES\\Delta c$. These terms cancel, yielding:\n$$\nc^{*}\\Delta H c \\approx c^{*}E\\Delta S c + c^{*}\\Delta E S c\n$$\nRearranging to solve for $\\Delta E$ gives $\\Delta E \\, c^{*} S c \\approx c^{*}\\Delta H c - E \\, c^{*}\\Delta S c$. The problem states that the generalized eigenvectors can be chosen to satisfy the normalization condition $c^{*} S c = 1$. Applying this normalization simplifies the expression for the first-order change in the eigenvalue to:\n$$\n\\Delta E \\approx c^{*}\\Delta H c - E c^{*}\\Delta S c = c^{*}(\\Delta H - E \\Delta S)c\n$$\nTo quantify the sensitivity, we take the magnitude of both sides and apply the Cauchy-Schwarz inequality and properties of induced matrix norms:\n$$\n|\\Delta E| \\approx |c^{*}(\\Delta H - E \\Delta S)c| \\le \\|c\\|_{2}^{2} \\|\\Delta H - E \\Delta S\\|_{2} \\le \\|c\\|_{2}^{2} (\\|\\Delta H\\|_{2} + |E| \\|\\Delta S\\|_{2})\n$$\nThe sensitivity of the eigenvalue $E$ is thus amplified by the factor $\\|c\\|_{2}^{2}$. We can relate this factor to the non-orthogonality parameter $s$ via the properties of the overlap matrix $S$. From the normalization condition $c^{*} S c = 1$ and the Rayleigh-Ritz theorem, we have:\n$$\n\\lambda_{\\min}(S) \\|c\\|_{2}^{2} \\le c^{*} S c \\le \\lambda_{\\max}(S) \\|c\\|_{2}^{2}\n$$\nSubstituting $c^{*} S c = 1$ and $\\lambda_{\\min}(S) = 1-s$ gives:\n$$\n(1-s) \\|c\\|_{2}^{2} \\le 1 \\implies \\|c\\|_{2}^{2} \\le \\frac{1}{1-s}\n$$\nThis inequality shows that the amplification factor $\\|c\\|_{2}^{2}$ is bounded above by $(1-s)^{-1}$. The stability of the eigenvalue $E$ against perturbations is therefore directly affected by the non-orthogonality. As $s \\to 1^{-}$, the basis functions become nearly linearly dependent, $\\lambda_{\\min}(S) \\to 0$, and the matrix $S$ becomes singular. Consequently, the condition number $\\kappa_{2}(S) = \\frac{1+s}{1-s}$ diverges. This ill-conditioning is reflected in the eigenvalue sensitivity, as the upper bound on the amplification factor $\\|c\\|_{2}^{2}$ diverges as $(1-s)^{-1}$. Therefore, increasing non-orthogonality (i.e., $s \\to 1^{-}$) can lead to severe numerical instability in the calculated eigenvalues.",
            "answer": "$$\n\\boxed{\\frac{1+s}{1-s}}\n$$"
        },
        {
            "introduction": "The ill-conditioning identified in the previous exercise is not merely a theoretical curiosity; it presents a major practical barrier to efficient variational minimization, causing standard gradient-based algorithms to converge very slowly. This problem investigates preconditioning, the standard numerical technique used to overcome this \"stiffness\" and accelerate electronic structure calculations. By dissecting the geometric and optimization-based justifications for this method, you will understand how algorithms can be intelligently designed to navigate complex energy landscapes and efficiently find the variational minimum .",
            "id": "3852597",
            "problem": "In multiscale materials simulation of electronic structure, one often minimizes the Rayleigh quotient energy functional of a Hermitian Hamiltonian operator $H$ over a trial wavefunction $\\lvert \\psi \\rangle$ to approximate the ground state energy. The variational principle states that the ground-state energy $E_0$ satisfies $E_0 \\leq \\frac{\\langle \\psi \\lvert H \\rvert \\psi \\rangle}{\\langle \\psi \\lvert \\psi \\rangle}$ for any admissible $\\lvert \\psi \\rangle$. When representing $\\lvert \\psi \\rangle$ in a nonorthonormal basis $\\{\\lvert \\chi_i \\rangle\\}_{i=1}^N$ with coefficients $c_i$ so that $\\lvert \\psi \\rangle = \\sum_{i=1}^N c_i \\lvert \\chi_i \\rangle$, the overlap matrix $S$ is defined by $S_{ij} = \\langle \\chi_i \\lvert \\chi_j \\rangle$ and is positive definite, and the discrete energy functional becomes a generalized Rayleigh quotient of $H$ and $S$. In practice, gradient-based minimization methods are frequently accelerated by preconditioning, for example using a matrix approximating the inverse of the overlap $S$ or the inverse of the local Hessian of the energy functional. Considering the variational structure and the multiscale character of basis sets that mix strongly localized and extended functions, which of the following statements correctly justify the use of such preconditioners to accelerate convergence while maintaining scientific correctness?\n\nA. In a nonorthonormal basis, the physically meaningful notion of distance in coefficient space is induced by the overlap matrix $S$, and choosing search directions that are steepest with respect to the $S$-inner product effectively applies the inverse of $S$ to the Euclidean gradient, which justifies preconditioning by a matrix approximating $S^{-1}$.\n\nB. Near a minimizer where the energy landscape is locally quadratic in the coefficients, applying a preconditioner that approximates the inverse of the Hessian rescales the coordinates to reduce the condition number of the problem toward $1$, balancing relaxation rates along different eigendirections and thereby accelerating convergence without changing the location of the true minimum.\n\nC. Preconditioning changes the location of the variational minimum and can violate the upper-bound property of the variational principle; its apparent convergence benefits arise solely from taking larger steps, so it should be avoided in variational minimization.\n\nD. In multiscale materials models, the presence of widely separated length scales produces a large spread in the eigenvalues of the overlap or Hessian, making the problem stiff; applying an inverse-overlap or inverse-Hessian preconditioner compresses the spectrum and mitigates stiffness, reducing the number of iterations needed for convergence.\n\nE. Preconditioning is equivalent to adding a constant shift to the Hamiltonian $H \\mapsto H + \\alpha I$ with some scalar $\\alpha$, which lowers all eigenvalues uniformly and thereby makes the ground state more dominant, explaining faster convergence.",
            "solution": "The user wants me to validate the problem statement and, if it is valid, provide a detailed solution evaluating each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Topic:** Multiscale materials simulation of electronic structure.\n- **Method:** Minimization of the Rayleigh quotient energy functional for a Hermitian Hamiltonian operator $H$.\n- **Trial Wavefunction:** $\\lvert \\psi \\rangle$.\n- **Variational Principle:** The ground-state energy $E_0$ satisfies $E_0 \\leq \\frac{\\langle \\psi \\lvert H \\rvert \\psi \\rangle}{\\langle \\psi \\lvert \\psi \\rangle}$ for any admissible $\\lvert \\psi \\rangle$.\n- **Basis Set:** A nonorthonormal basis $\\{\\lvert \\chi_i \\rangle\\}_{i=1}^N$.\n- **Wavefunction Representation:** $\\lvert \\psi \\rangle = \\sum_{i=1}^N c_i \\lvert \\chi_i \\rangle$, with coefficients $c_i$.\n- **Overlap Matrix:** $S$ with elements $S_{ij} = \\langle \\chi_i \\lvert \\chi_j \\rangle$. It is stated that $S$ is positive definite.\n- **Discrete Energy Functional:** A generalized Rayleigh quotient of $H$ and $S$.\n- **Numerical Method:** Gradient-based minimization.\n- **Acceleration Technique:** Preconditioning.\n- **Example Preconditioners:** A matrix approximating the inverse of the overlap matrix, $S^{-1}$, or a matrix approximating the inverse of the local Hessian of the energy functional.\n- **Physical Context:** Multiscale character of basis sets, which mix strongly localized and extended functions.\n- **Question:** Which of the given statements correctly justify using these preconditioners to accelerate convergence while maintaining scientific correctness?\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly located at the intersection of quantum mechanics, numerical linear algebra, and computational materials science. The use of the variational principle, nonorthonormal basis sets (common in quantum chemistry), the generalized eigenvalue problem, and preconditioned gradient methods are all standard and well-established concepts and techniques in this field. The description is factually and scientifically sound.\n- **Well-Posed:** The problem provides a clear and self-contained context. It asks for the correct justifications for a specific numerical technique (preconditioning) within this context. The question is unambiguous and has a definite set of correct conceptual answers among the options.\n- **Objective:** The language is formal, technical, and free of subjective or biased statements. It describes a standard computational physics scenario.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. All concepts are standard in the field.\n2.  **Non-Formalizable or Irrelevant:** The problem is highly relevant and formalizable within the specified domain.\n3.  **Incomplete or Contradictory Setup:** The setup is complete and internally consistent.\n4.  **Unrealistic or Infeasible:** The scenario described is a realistic representation of challenges in modern electronic structure calculations.\n5.  **Ill-Posed or Poorly Structured:** The question is well-defined.\n6.  **Pseudo-Profound, Trivial, or Tautological:** The question addresses a fundamental and non-trivial aspect of numerical methods in quantum mechanics, requiring an understanding of geometry, optimization, and physics.\n7.  **Outside Scientific Verifiability:** The correctness of the statements can be verified against standard textbooks on numerical analysis and computational physics.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with the derivation of the solution and the analysis of each option.\n\n### Derivation and Solution\n\nThe energy functional to be minimized is the Rayleigh quotient:\n$$E[\\psi] = \\frac{\\langle \\psi \\lvert H \\rvert \\psi \\rangle}{\\langle \\psi \\lvert \\psi \\rangle}$$\nIn the nonorthonormal basis $\\{\\lvert \\chi_i \\rangle\\}$, with $\\lvert \\psi \\rangle = \\sum_i c_i \\lvert \\chi_i \\rangle$, this becomes a function of the coefficient vector $\\mathbf{c}$:\n$$E(\\mathbf{c}) = \\frac{\\sum_{i,j} c_i^* \\langle \\chi_i \\lvert H \\rvert \\chi_j \\rangle c_j}{\\sum_{i,j} c_i^* \\langle \\chi_i \\lvert \\chi_j \\rangle c_j} = \\frac{\\mathbf{c}^\\dagger \\mathbf{H} \\mathbf{c}}{\\mathbf{c}^\\dagger \\mathbf{S} \\mathbf{c}}$$\nwhere $\\mathbf{H}$ is the Hamiltonian matrix with elements $H_{ij} = \\langle \\chi_i \\lvert H \\rvert \\chi_j \\rangle$ and $\\mathbf{S}$ is the overlap matrix with elements $S_{ij} = \\langle \\chi_i \\lvert \\chi_j \\rangle$. Minimizing $E(\\mathbf{c})$ is equivalent to finding the lowest-energy solution to the generalized eigenvalue problem $\\mathbf{H}\\mathbf{c} = E \\mathbf{S}\\mathbf{c}$.\n\nGradient-based methods iteratively update the coefficient vector $\\mathbf{c}$ using a search direction derived from the gradient of the energy. The (complex) gradient of $E(\\mathbf{c})$ with respect to $\\mathbf{c}^*$ is:\n$$\\mathbf{g} = \\frac{\\partial E}{\\partial \\mathbf{c}^*} = \\frac{1}{\\mathbf{c}^\\dagger \\mathbf{S} \\mathbf{c}} (\\mathbf{H}\\mathbf{c} - E(\\mathbf{c})\\mathbf{S}\\mathbf{c})$$\nThe vector $\\mathbf{r} = \\mathbf{H}\\mathbf{c} - E(\\mathbf{c})\\mathbf{S}\\mathbf{c}$ is the residual. A simple (steepest) gradient descent update would be $\\mathbf{c}_{k+1} = \\mathbf{c}_k - \\alpha_k \\mathbf{g}_k$, where $\\alpha_k$ is a step size. This update rule uses the Euclidean metric in the space of coefficients $\\mathbf{c}$.\n\n**Preconditioning** modifies this update to $\\mathbf{c}_{k+1} = \\mathbf{c}_k - \\alpha_k \\mathbf{P} \\mathbf{g}_k$, where $\\mathbf{P}$ is the preconditioner matrix. The goal is to choose a $\\mathbf{P}$ that accelerates convergence. Two primary justifications for preconditioning are relevant here: one based on geometry and the other on optimization theory.\n\n### Option-by-Option Analysis\n\n**A. In a nonorthonormal basis, the physically meaningful notion of distance in coefficient space is induced by the overlap matrix $S$, and choosing search directions that are steepest with respect to the $S$-inner product effectively applies the inverse of $S$ to the Euclidean gradient, which justifies preconditioning by a matrix approximating $S^{-1}$.**\n- **Analysis:** This statement addresses the geometric interpretation. The distance between two wavefunctions $\\lvert \\psi_1 \\rangle$ and $\\lvert \\psi_2 \\rangle$ is given by the norm of their difference, $||\\psi_1 - \\psi_2||^2 = \\langle \\psi_1 - \\psi_2 \\lvert \\psi_1 - \\psi_2 \\rangle$. For an infinitesimal change in the wavefunction, $\\lvert \\delta \\psi \\rangle = \\sum_i \\delta c_i \\lvert \\chi_i \\rangle$, the squared norm is $||\\delta \\psi||^2 = \\langle \\delta \\psi \\lvert \\delta \\psi \\rangle = \\delta \\mathbf{c}^\\dagger \\mathbf{S} \\delta \\mathbf{c}$. This shows that the physically correct metric (or inner product) on the space of coefficient vectors is the $\\mathbf{S}$-inner product, $(\\mathbf{u}, \\mathbf{v})_\\mathbf{S} = \\mathbf{u}^\\dagger \\mathbf{S} \\mathbf{v}$. The direction of \"steepest descent\" should be defined with respect to this metric. The gradient in the $\\mathbf{S}$-metric, $\\mathbf{g}_\\mathbf{S}$, is related to the Euclidean gradient $\\mathbf{g}$ by $\\mathbf{g} = \\mathbf{S} \\mathbf{g}_\\mathbf{S}$. Therefore, the steepest descent direction in the natural geometry is $-\\mathbf{g}_\\mathbf{S} = -\\mathbf{S}^{-1}\\mathbf{g}$. This is exactly a preconditioned gradient step with preconditioner $\\mathbf{P} = \\mathbf{S}^{-1}$. The statement is a precise and correct justification.\n- **Verdict:** Correct.\n\n**B. Near a minimizer where the energy landscape is locally quadratic in the coefficients, applying a preconditioner that approximates the inverse of the Hessian rescales the coordinates to reduce the condition number of the problem toward $1$, balancing relaxation rates along different eigendirections and thereby accelerating convergence without changing the location of the true minimum.**\n- **Analysis:** This statement is based on standard optimization theory. Near a local minimum, any sufficiently smooth function can be approximated by a quadratic form, $E(\\mathbf{c}) \\approx E(\\mathbf{c}_{min}) + \\frac{1}{2}(\\mathbf{c}-\\mathbf{c}_{min})^\\dagger \\mathcal{H} (\\mathbf{c}-\\mathbf{c}_{min})$, where $\\mathcal{H}$ is the Hessian matrix of second derivatives. The speed of convergence of gradient-based methods is limited by the condition number of $\\mathcal{H}$ (the ratio of its largest to smallest eigenvalue). A large condition number means the energy contours are highly elliptical, and gradient descent makes slow, zigzagging progress. The ideal preconditioner is $\\mathbf{P} \\approx \\mathcal{H}^{-1}$. This transforms the effective Hessian of the preconditioned update to $\\mathbf{P}^{1/2}\\mathcal{H}\\mathbf{P}^{1/2} \\approx \\mathbf{I}$, making the condition number close to $1$. This effectively makes the energy contours spherical, allowing gradient-based methods to converge rapidly. Importantly, preconditioning only alters the path to the minimum; it does not change the functional being minimized or the location of its minimum. The statement is entirely correct.\n- **Verdict:** Correct.\n\n**C. Preconditioning changes the location of the variational minimum and can violate the upper-bound property of the variational principle; its apparent convergence benefits arise solely from taking larger steps, so it should be avoided in variational minimization.**\n- **Analysis:** This statement is fundamentally flawed. Preconditioning is a technique to determine a better *search direction* for an iterative optimization algorithm. The function being minimized, $E(\\mathbf{c})$, remains unchanged. Therefore, the minimum of the function is also unchanged. At every step $k$, the energy $E(\\mathbf{c}_k)$ is calculated for a valid trial wavefunction $\\lvert \\psi_k \\rangle$, so the variational principle $E(\\mathbf{c}_k) \\ge E_0$ is never violated. The benefit is not merely taking larger steps, but taking steps in a direction that more effectively reduces the energy and approaches the minimum. The statement mischaracterizes the entire purpose and mechanism of preconditioning.\n- **Verdict:** Incorrect.\n\n**D. In multiscale materials models, the presence of widely separated length scales produces a large spread in the eigenvalues of the overlap or Hessian, making the problem stiff; applying an inverse-overlap or inverse-Hessian preconditioner compresses the spectrum and mitigates stiffness, reducing the number of iterations needed for convergence.**\n- **Analysis:** This statement correctly diagnoses the physical origin of the numerical difficulty. A basis set containing both very localized functions (short length scale) and very extended functions (long length scale) is \"multiscale\". This leads to the overlap matrix $\\mathbf{S}$ and the Hessian $\\mathcal{H}$ being ill-conditioned, meaning their eigenvalues are spread over many orders of magnitude. This large spectral width (high condition number) is a direct cause of numerical \"stiffness\", where different error components decay at vastly different rates, slowing down overall convergence. A good preconditioner $\\mathbf{P}$ (like an approximation to $\\mathbf{S}^{-1}$ or $\\mathcal{H}^{-1}$) is designed such that the preconditioned matrix (e.g., $\\mathbf{P}\\mathbf{S}$ or $\\mathbf{P}\\mathcal{H}$) has its eigenvalues clustered together, i.e., a \"compressed spectrum\" and a condition number closer to $1$. This directly remedies the stiffness and accelerates convergence. The statement provides a correct and physically insightful justification.\n- **Verdict:** Correct.\n\n**E. Preconditioning is equivalent to adding a constant shift to the Hamiltonian $H \\mapsto H + \\alpha I$ with some scalar $\\alpha$, which lowers all eigenvalues uniformly and thereby makes the ground state more dominant, explaining faster convergence.**\n- **Analysis:** This statement incorrectly conflates two distinct numerical techniques. Preconditioning, as explained above, modifies the gradient to produce a better search direction ($\\mathbf{d}_k = -\\mathbf{P}\\mathbf{g}_k$). In contrast, a spectral shift operation changes the operator itself, for instance by replacing $\\mathbf{H}$ with $\\mathbf{H} - \\sigma \\mathbf{S}$ in the generalized eigenvalue problem. This is a key component of shift-and-invert methods, which target eigenvalues near the shift $\\sigma$. Adding a term $\\alpha I$ to $H$ (or more accurately, $\\alpha S$ to the matrix $H_{ij}$) shifts all eigenvalues by $\\alpha$, but it does not change the spacing between them and thus does not, by itself, improve the condition number or convergence rate of a simple gradient descent. The two concepts are not equivalent.\n- **Verdict:** Incorrect.",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "Applying the variational principle to crystalline solids requires a strategy for handling the continuous electronic states within the Brillouin zone. In practice, this involves replacing the continuous integral over crystal momenta $\\mathbf{k}$ with a discrete sum over a finite grid of \"k-points.\" This hands-on coding exercise guides you through the implementation of the Monkhorst-Pack sampling scheme, a cornerstone of modern solid-state calculations, to determine the ground-state energy of a model crystal. By studying the convergence of the total energy with k-point density, you will perform a fundamental task in computational materials science and develop an intuition for the trade-off between accuracy and computational cost .",
            "id": "3852574",
            "problem": "You are to implement, analyze, and test a variational Brillouin Zone (BZ) sampling scheme using Monkhorst-Pack (MP) grids for a spin-degenerate, non-interacting, single-band tight-binding model on a two-dimensional square lattice, and to quantify the convergence of the ground-state total energy per electron with respect to the k-point density. All trigonometric functions must use radians. All energies must be expressed in electronvolts (eV). The lattice constant must be expressed in angstroms (Å).\n\nStart from the following fundamental base:\n- The Rayleigh-Ritz variational principle for quantum mechanics: for any trial state in a subspace of the full Hilbert space, the expectation value of the Hamiltonian provides an upper bound to the exact ground-state energy.\n- The tight-binding Hamiltonian on a two-dimensional square Bravais lattice with one orbital per site and nearest-neighbor hopping amplitude $t$ leads to a single-particle dispersion relation\n$$\n\\varepsilon(\\mathbf{k}) \\;=\\; -2t\\left(\\cos(k_x a) + \\cos(k_y a)\\right),\n$$\nwhere $a$ is the lattice constant, $\\mathbf{k}=(k_x,k_y)$ is the crystal momentum in the first Brillouin Zone (BZ), and the cosine arguments are angles measured in radians.\n- The first Brillouin Zone (BZ) of the square lattice of lattice constant $a$ is the square $\\left[-\\frac{\\pi}{a},\\frac{\\pi}{a}\\right]\\times\\left[-\\frac{\\pi}{a},\\frac{\\pi}{a}\\right]$ in reciprocal space, spanned by reciprocal lattice vectors $\\mathbf{b}_1=\\frac{2\\pi}{a}\\hat{\\mathbf{x}}$ and $\\mathbf{b}_2=\\frac{2\\pi}{a}\\hat{\\mathbf{y}}$.\n- For non-interacting, spin-degenerate ($g_s=2$) electrons at zero temperature, the many-electron ground state is obtained by occupying single-particle states in order of increasing energy, subject to the Pauli exclusion principle. In the continuum, with a normalized measure over the BZ, the total energy per unit cell is the integral of $\\varepsilon(\\mathbf{k})$ with an occupation function $f(\\mathbf{k})\\in[0,1]$ per spin, under the constraint that the average occupation equals the electron count per spin.\n\nFormulate a discrete variational problem by replacing the BZ integral with a Riemann sum over a Monkhorst-Pack (MP) grid. Define an $N\\times N$ MP grid in fractional coordinates by the set of points\n$$\nu_i \\;=\\; \\frac{2i - N - 1}{2N},\\quad v_j \\;=\\; \\frac{2j - N - 1}{2N},\\quad i,j\\in\\{1,2,\\dots,N\\},\n$$\nand map to reciprocal space via\n$$\n\\mathbf{k}_{ij} \\;=\\; u_i \\,\\mathbf{b}_1 + v_j \\,\\mathbf{b}_2.\n$$\nAssign each point an equal weight $w=\\frac{1}{N^2}$ so that $\\sum_{i,j} w = 1$. For a given number of electrons per unit cell $n_e\\in[0,2]$ (with $2$ being full filling of the single band due to spin degeneracy $g_s=2$), define discrete occupations $f_{ij}\\in[0,1]$ per spin, and minimize the discrete total energy per electron\n$$\nE_N \\;=\\; \\frac{g_s \\sum_{i,j} w \\, f_{ij} \\, \\varepsilon(\\mathbf{k}_{ij})}{n_e}\n$$\nsubject to the constraint $\\sum_{i,j} w \\, f_{ij} \\,=\\, \\frac{n_e}{g_s}$ and the box constraints $0\\le f_{ij}\\le 1$.\n\nYour program must:\n- Construct the $N\\times N$ MP grid for each test case.\n- Solve the stated discrete variational problem to obtain the discrete ground-state total energy per electron $E_N$ in electronvolts (eV). You must determine the occupations $f_{ij}$ that minimize the energy given the constraints.\n- For a prescribed large reference grid size $N_{\\mathrm{ref}}$ (common to all test cases), compute the corresponding reference variational energy $E_{N_{\\mathrm{ref}}}$ for each test case using the same procedure.\n- Quantify the convergence by computing the absolute error $\\Delta E = \\lvert E_N - E_{N_{\\mathrm{ref}}}\\rvert$ in electronvolts (eV).\n\nUse the following test suite, where each test case is a quadruple $(t, a, n_e, N)$:\n- Test 1: ($t=1.0$ eV, $a=1.0$ Å, $n_e=1.0$, $N=5$).\n- Test 2: ($t=1.0$ eV, $a=1.0$ Å, $n_e=1.0$, $N=9$).\n- Test 3: ($t=1.0$ eV, $a=1.0$ Å, $n_e=0.5$, $N=7$).\n- Test 4: ($t=1.0$ eV, $a=1.0$ Å, $n_e=2.0$, $N=7$).\n- Test 5: ($t=2.5$ eV, $a=3.0$ Å, $n_e=1.3$, $N=11$).\n\nUse a single common reference grid size $N_{\\mathrm{ref}}=181$ for all cases. All energies must be reported in electronvolts (eV). Angles in all trigonometric functions are in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the absolute errors $\\Delta E$ for the five test cases, in electronvolts (eV), as a comma-separated list enclosed in square brackets, in the same order as the test suite. For example, a valid output format is\n$[x_1,x_2,x_3,x_4,x_5]$,\nwhere each $x_i$ is a floating-point number in electronvolts (eV).",
            "solution": "We begin from the Rayleigh-Ritz variational principle in quantum mechanics: for any normalized trial state $\\lvert\\psi\\rangle$, the expectation value $\\langle\\psi\\lvert \\hat{H} \\rvert\\psi\\rangle$ is an upper bound to the exact ground-state energy $E_0$. In a non-interacting electron system, the many-body ground state is a Slater determinant of the lowest-energy single-particle eigenstates. At zero temperature with spin degeneracy $g_s=2$, the ground-state occupancy is to fill single-particle states in order of increasing single-particle energy, subject to the Pauli exclusion principle.\n\nIn a periodic crystal, the single-particle eigenstates are Bloch waves characterized by crystal momentum $\\mathbf{k}$ in the first Brillouin Zone (BZ). For the square-lattice tight-binding model with nearest-neighbor hopping amplitude $t$ and a single orbital per site, the single-particle energies are given by\n$$\n\\varepsilon(\\mathbf{k}) \\;=\\; -2t\\left(\\cos(k_x a) + \\cos(k_y a)\\right),\n$$\nwhere $a$ is the lattice constant and the arguments of the cosine are angles measured in radians. The first Brillouin Zone (BZ) is the square $\\left[-\\frac{\\pi}{a},\\frac{\\pi}{a}\\right]\\times\\left[-\\frac{\\pi}{a},\\frac{\\pi}{a}\\right]$ spanned by reciprocal lattice vectors $\\mathbf{b}_1=\\frac{2\\pi}{a}\\hat{\\mathbf{x}}$ and $\\mathbf{b}_2=\\frac{2\\pi}{a}\\hat{\\mathbf{y}}$.\n\nThe total energy per unit cell for $n_e$ electrons per unit cell (counting both spins) at zero temperature is, in the continuum, the integral\n$$\nE \\;=\\; \\frac{g_s \\int_{\\mathrm{BZ}} \\varepsilon(\\mathbf{k}) \\, f(\\mathbf{k}) \\,\\mathrm{d}\\mu(\\mathbf{k})}{n_e},\n$$\nwhere $g_s=2$ is the spin degeneracy, $f(\\mathbf{k})\\in[0,1]$ is the occupation function per spin, and $\\mathrm{d}\\mu(\\mathbf{k})$ is a normalized measure over the BZ such that $\\int_{\\mathrm{BZ}}\\mathrm{d}\\mu(\\mathbf{k})=1$. The electron number constraint is\n$$\n\\int_{\\mathrm{BZ}} f(\\mathbf{k}) \\,\\mathrm{d}\\mu(\\mathbf{k}) \\;=\\; \\frac{n_e}{g_s}.\n$$\nThe variational principle dictates that the minimizing $f(\\mathbf{k})$ is a projector onto the set of lowest-energy states up to a Fermi contour, i.e., $f(\\mathbf{k})=\\Theta(\\varepsilon_F-\\varepsilon(\\mathbf{k}))$ with an appropriate Fermi energy $\\varepsilon_F$; the minimizer fills lower-energy states first, which follows from rearrangement inequalities for linear functionals under box constraints.\n\nTo compute numerically, we approximate the BZ integral by a Riemann sum over a Monkhorst-Pack (MP) grid. The two-dimensional $N\\times N$ MP grid in fractional coordinates is given by\n$$\nu_i \\;=\\; \\frac{2i - N - 1}{2N},\\quad v_j \\;=\\; \\frac{2j - N - 1}{2N},\\quad i,j\\in\\{1,2,\\dots,N\\},\n$$\nand the associated crystal momenta are\n$$\n\\mathbf{k}_{ij} \\;=\\; u_i\\,\\mathbf{b}_1 + v_j\\,\\mathbf{b}_2,\n$$\nwith equal weights $w=\\frac{1}{N^2}$ so that $\\sum_{i,j} w=1$. Note that the angles entering the cosine in $\\varepsilon(\\mathbf{k})$ are $k_x a = 2\\pi u_i$ and $k_y a = 2\\pi v_j$, which are in radians. The discrete variational problem is then\n$$\n\\min_{\\{f_{ij}\\}} \\; \\sum_{i,j} w \\, f_{ij} \\, \\varepsilon(\\mathbf{k}_{ij}) \\;\\;\\text{subject to}\\;\\; \\sum_{i,j} w \\, f_{ij} \\;=\\; \\frac{n_e}{g_s},\\;\\; 0\\le f_{ij}\\le 1,\n$$\nand the discrete total energy per electron is\n$$\nE_N \\;=\\; \\frac{g_s \\sum_{i,j} w \\, f_{ij} \\, \\varepsilon(\\mathbf{k}_{ij})}{n_e}.\n$$\n\nSolution structure and justification:\n- The objective is linear in the variables $\\{f_{ij}\\}$, with box constraints and a linear equality constraint on their average. This is a continuous knapsack problem. By the rearrangement inequality and the fact that the feasible set is a polymatroid, the minimizer is obtained by sorting $\\varepsilon(\\mathbf{k}_{ij})$ in ascending order and setting $f_{ij}=1$ for the lowest-energy points until the constraint is nearly saturated, with at most one partially occupied point to satisfy the constraint exactly. Specifically, let $\\{\\varepsilon_\\ell\\}_{\\ell=1}^{N^2}$ be the sorted list in ascending order, with common weight $w=\\frac{1}{N^2}$, and let $x=\\frac{n_e}{g_s}\\in[0,1]$ be the required average occupation per spin. Define $m=\\left\\lfloor \\frac{x}{w}\\right\\rfloor$ and the residual $r=x-mw\\in[0,w)$. The minimizing occupations are $f_\\ell=1$ for $\\ell\\in\\{1,\\dots,m\\}$, $f_{m+1}=\\frac{r}{w}$ if $r0$, and $f_\\ell=0$ for all larger $\\ell$. The discrete energy per electron is then\n$$\nE_N \\;=\\; \\frac{g_s}{n_e} \\left( w \\sum_{\\ell=1}^{m} \\varepsilon_\\ell \\;+\\; r \\,\\varepsilon_{m+1}\\,\\mathbf{1}_{\\{r0\\}} \\right).\n$$\nThis is the direct discrete analogue of zero-temperature Fermi filling and is itself a Rayleigh-Ritz variational minimization within the subspace of states representable on the chosen MP grid.\n- The convergence with respect to the k-point density arises because the MP Riemann sums converge to the integral as $N\\to\\infty$, and the discrete variational minimizer converges to the continuum minimizer. Therefore, the absolute error $\\Delta E=\\lvert E_N-E_{N_{\\mathrm{ref}}}\\rvert$ with a large reference $N_{\\mathrm{ref}}$ quantifies the sampling error.\n\nAlgorithmic plan:\n- For each grid size $N$, build the fractional MP coordinates $u_i$ and $v_j$, compute $\\varepsilon(\\mathbf{k}_{ij})=-2t\\left(\\cos(2\\pi u_i)+\\cos(2\\pi v_j)\\right)$ in electronvolts, flatten and sort ascending, and set $w=\\frac{1}{N^2}$.\n- For a given electron count per unit cell $n_e\\in[0,2]$, set $x=\\frac{n_e}{g_s}$ with $g_s=2$. Compute $m=\\min\\left(\\left\\lfloor \\frac{x}{w}\\right\\rfloor,N^2\\right)$ and residual $r=\\max\\left(0,\\,x-mw\\right)$. Compute\n$$\nE_N \\;=\\; \\frac{g_s}{n_e}\\left(w\\sum_{\\ell=1}^{m}\\varepsilon_\\ell + r\\,\\varepsilon_{m+1}\\,\\mathbf{1}_{\\{mN^2 \\wedge r0\\}}\\right).\n$$\n- For each test case $(t,a,n_e,N)$ in the suite, compute $E_N$ and $E_{N_{\\mathrm{ref}}}$ with a common large $N_{\\mathrm{ref}}=181$, then compute $\\Delta E=\\lvert E_N-E_{N_{\\mathrm{ref}}}\\rvert$ in electronvolts.\n- Report the list $[\\Delta E_1,\\Delta E_2,\\Delta E_3,\\Delta E_4,\\Delta E_5]$ in a single line.\n\nEdge cases and coverage rationale:\n- Test 1 and 2 compare two different k-point densities at half-filling $n_e=1.0$, showing reduction of $\\Delta E$ with increased $N$.\n- Test 3 uses quarter-filling $n_e=0.5$, exercising fractional last-occupation behavior in the discrete scheme.\n- Test 4 uses full filling $n_e=2.0$, where the entire band is occupied. For this symmetric dispersion the exact BZ average is zero, so $E_N$ equals the MP average and $\\Delta E$ should be near zero, probing symmetry and averaging behavior.\n- Test 5 changes both the hopping $t$ and the electron count $n_e$, verifying correct energy scaling with $t$ and generality across fillings.\n\nAll energies are in electronvolts (eV) and all angles are in radians. The final program must produce a single line containing the five absolute errors as a comma-separated list enclosed in square brackets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mp_energies_sorted(N: int, t_ev: float, a_angstrom: float) - tuple[np.ndarray, float]:\n    \"\"\"\n    Generate an N x N Monkhorst-Pack grid for a square lattice and return\n    the sorted single-particle energies in eV along with the uniform weight w = 1/N^2.\n\n    Parameters:\n        N (int): divisions along each reciprocal lattice direction.\n        t_ev (float): nearest-neighbor hopping amplitude t in eV.\n        a_angstrom (float): lattice constant a in angstroms (Å).\n\n    Returns:\n        energies_sorted (np.ndarray): sorted energies (ascending), shape (N*N,)\n        w (float): uniform weight = 1/(N^2)\n    \"\"\"\n    # Fractional MP coordinates u_i, v_j in [-0.5, 0.5), centered.\n    i = np.arange(1, N + 1, dtype=float)\n    u = (2.0 * i - N - 1.0) / (2.0 * N)  # shape (N,)\n    # Create meshgrid of fractional coordinates\n    U, V = np.meshgrid(u, u, indexing='ij')  # shape (N,N)\n\n    # For square lattice: kx * a = 2*pi*U, ky * a = 2*pi*V. Angles are in radians.\n    kxa = 2.0 * np.pi * U\n    kya = 2.0 * np.pi * V\n\n    # Tight-binding dispersion: epsilon(k) = -2 t [cos(kx a) + cos(ky a)]\n    # Energies are in eV if t_ev is in eV.\n    eps = -2.0 * t_ev * (np.cos(kxa) + np.cos(kya))  # shape (N,N)\n\n    energies_sorted = np.sort(eps.ravel())\n    w = 1.0 / (N * N)\n    return energies_sorted, w\n\ndef variational_energy_per_electron_from_sorted(energies_sorted: np.ndarray,\n                                                w: float,\n                                                n_e: float,\n                                                g_s: float = 2.0) - float:\n    \"\"\"\n    Compute the discrete variational ground-state total energy per electron (in eV)\n    for a given sorted list of single-particle energies and uniform weight.\n\n    Parameters:\n        energies_sorted (np.ndarray): sorted ascending energies, length K = N^2\n        w (float): uniform weight = 1/K\n        n_e (float): number of electrons per unit cell (including both spins), 0 = n_e = 2\n        g_s (float): spin degeneracy (default 2)\n\n    Returns:\n        E_per_e (float): total energy per electron in eV\n    \"\"\"\n    if n_e = 0.0:\n        # No electrons - energy per electron undefined; not used by the given test suite.\n        return 0.0\n\n    K = energies_sorted.size\n    x = n_e / g_s  # required average occupation per spin in [0,1]\n    # Compute number of fully occupied k-points and residual fractional occupation.\n    # Add a small epsilon to guard against floating-point representation issues.\n    m = int(np.floor(x / w + 1e-14))\n    if m  K:\n        m = K\n    # Residual fraction toward the next state\n    residual = x - m * w\n    if residual  0.0:\n        residual = 0.0\n\n    # Sum energies of the lowest m states\n    energy_sum = 0.0\n    if m  0:\n        energy_sum += w * float(np.sum(energies_sorted[:m]))\n    # Add fractional occupation of the (m+1)-th state if needed and available\n    if (m  K) and (residual  1e-18):\n        energy_sum += residual * float(energies_sorted[m])\n\n    # Total energy per electron\n    E_per_cell = g_s * energy_sum  # energy per unit cell (both spins)\n    E_per_e = E_per_cell / n_e\n    return E_per_e\n\ndef compute_error_for_case(t_ev: float, a_angstrom: float, n_e: float,\n                           N_small: int, N_ref: int) - float:\n    \"\"\"\n    Compute the absolute error |E_N - E_Nref| in eV for a given test case.\n\n    Parameters:\n        t_ev (float): hopping parameter t in eV\n        a_angstrom (float): lattice constant a in angstroms (Å)\n        n_e (float): electrons per unit cell (0 = n_e = 2)\n        N_small (int): small MP grid size\n        N_ref (int): reference MP grid size (large)\n\n    Returns:\n        abs_error (float): absolute error in eV\n    \"\"\"\n    # Small grid energy\n    e_small_sorted, w_small = mp_energies_sorted(N_small, t_ev, a_angstrom)\n    E_small = variational_energy_per_electron_from_sorted(e_small_sorted, w_small, n_e, g_s=2.0)\n\n    # Reference grid energy\n    e_ref_sorted, w_ref = mp_energies_sorted(N_ref, t_ev, a_angstrom)\n    E_ref = variational_energy_per_electron_from_sorted(e_ref_sorted, w_ref, n_e, g_s=2.0)\n\n    return abs(E_small - E_ref)\n\ndef solve():\n    # Define the test cases from the problem statement: (t [eV], a [Å], n_e, N_small)\n    test_cases = [\n        (1.0, 1.0, 1.0, 5),\n        (1.0, 1.0, 1.0, 9),\n        (1.0, 1.0, 0.5, 7),\n        (1.0, 1.0, 2.0, 7),\n        (2.5, 3.0, 1.3, 11),\n    ]\n    # Common reference grid size\n    N_ref = 181\n\n    results = []\n    for t_ev, a_angstrom, n_e, N_small in test_cases:\n        err = compute_error_for_case(t_ev, a_angstrom, n_e, N_small, N_ref)\n        # Format with a reasonable precision to ensure a clean single-line output\n        results.append(f\"{err:.12f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}