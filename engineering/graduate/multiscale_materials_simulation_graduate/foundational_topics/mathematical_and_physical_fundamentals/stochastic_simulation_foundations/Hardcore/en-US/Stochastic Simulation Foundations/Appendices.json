{
    "hands_on_practices": [
        {
            "introduction": "Many crucial processes in materials science, such as atomic diffusion, chemical reactions, or dislocation motion, occur as a series of discrete, stochastic events. Instead of integrating continuous equations of motion, we can model these systems as continuous-time Markov jump processes. This practice guides you through a foundational derivation of the Gillespie algorithm, a cornerstone of kinetic Monte Carlo methods, revealing how to rigorously determine both *when* the next event will happen and *which* event it will be based on the properties of competing Poisson processes .",
            "id": "3841418",
            "problem": "Consider a mesoscopic model of a multiscale materials system in which rare events on a lattice (such as adsorption, desorption, diffusion hops, or bond rearrangements) are represented as reaction channels indexed by $i \\in \\{1, \\dots, M\\}$. Assume that, at a given state of the system, each channel $i$ is an independent Poisson process with constant rate (also called propensity) $r_i  0$. The evolution is modeled as a continuous-time Markov jump process. You are tasked with deriving a sampling scheme for the next-event time increment and identifying which channel fires next under the foundations of stochastic simulation.\n\nStarting only from the following fundamental bases:\n- For a homogeneous Poisson process with rate $r0$, the waiting time to the next event is exponentially distributed with probability density function $f(t) = r \\exp(-r t)$ for $t \\geq 0$, and the process is memoryless, meaning $P(T  s + t \\,|\\, T  s) = P(T  t)$ for all $s, t \\geq 0$.\n- Independent Poisson processes superpose to give a Poisson process with rate equal to the sum of the rates.\n- The inverse transformation method: if $U$ is uniformly distributed on $(0,1)$ and $F$ is a continuous cumulative distribution function, then $F^{-1}(U)$ is a sample from the distribution with cumulative distribution function $F$.\n\nUsing only these bases and fundamental probability rules, derive:\n1. The closed-form expression for the waiting time increment $\\Delta t$ to the next event in terms of a single sample $u$ drawn from the uniform distribution on $(0,1)$ and the rates $\\{r_i\\}_{i=1}^{M}$.\n2. The closed-form expression for the probability that the next event is channel $k \\in \\{1, \\dots, M\\}$, expressed in terms of the rates $\\{r_i\\}_{i=1}^{M}$.\n\nYour derivation must invoke the memoryless property of exponential waiting times and the independence of channels. Express your final results in closed form. No numerical evaluation is required, and no intermediate shortcut formulas are permitted in the problem statement. Provide the final expressions without units.",
            "solution": "The problem statement is evaluated as scientifically sound, well-posed, objective, and self-contained, thus meeting all criteria for a valid problem. It requests the derivation of the core components of the Gillespie Stochastic Simulation Algorithm (SSA) from first principles, a fundamental task in the study of stochastic processes in materials science and related fields.\n\nLet the set of $M$ reaction channels be indexed by $i \\in \\{1, \\dots, M\\}$. Each channel $i$ is modeled as an independent homogeneous Poisson process with a constant rate (propensity) $r_i  0$. Let $T_i$ be the random variable representing the waiting time until the next event occurs in channel $i$.\n\nAccording to the first provided basis, the process is memoryless, and the waiting time for a single channel $i$ follows an exponential distribution. The probability density function (PDF) for $T_i$ is $p_i(t) = r_i \\exp(-r_i t)$ for $t \\ge 0$. The memoryless property is crucial as it justifies modeling the time to the next event with this distribution starting from any arbitrary state of the system, without regard to its history. The cumulative distribution function (CDF) for $T_i$ is $F_i(t) = P(T_i \\le t) = \\int_{0}^{t} r_i \\exp(-r_i \\tau) d\\tau = 1 - \\exp(-r_i t)$. Consequently, the survival function is $S_i(t) = P(T_i  t) = 1 - F_i(t) = \\exp(-r_i t)$.\n\n**1. Derivation of the waiting time increment $\\Delta t$**\n\nThe next event in the overall system is the a priori unknown event that occurs first among all possible channels. Therefore, the waiting time until the next system event, denoted by $\\Delta t$, is the minimum of the individual waiting times of all channels:\n$$\n\\Delta t = \\min(T_1, T_2, \\dots, T_M)\n$$\nTo find the distribution of $\\Delta t$, we first derive its survival function, $S_{\\Delta t}(t) = P(\\Delta t  t)$. The condition $\\Delta t  t$ is met if and only if all individual waiting times $T_i$ are greater than $t$.\n$$\nP(\\Delta t  t) = P(\\min(T_1, \\dots, T_M)  t) = P(T_1  t, T_2  t, \\dots, T_M  t)\n$$\nSince the channels are stated to be independent processes, the joint probability is the product of the individual probabilities:\n$$\nP(\\Delta t  t) = \\prod_{i=1}^{M} P(T_i  t)\n$$\nSubstituting the survival function for each exponential distribution, $P(T_i  t) = \\exp(-r_i t)$:\n$$\nP(\\Delta t  t) = \\prod_{i=1}^{M} \\exp(-r_i t) = \\exp\\left(-\\sum_{i=1}^{M} r_i t\\right)\n$$\nLet the total rate be $R = \\sum_{i=1}^{M} r_i$. The survival function for $\\Delta t$ is $S_{\\Delta t}(t) = \\exp(-R t)$. This result demonstrates the second provided basis: the superposition of independent Poisson processes yields a new Poisson process with a rate equal to the sum of the individual rates. The random variable $\\Delta t$ is thus exponentially distributed with rate $R$.\n\nThe CDF of $\\Delta t$ is $F_{\\Delta t}(t) = 1 - S_{\\Delta t}(t) = 1 - \\exp(-R t)$.\n\nTo generate a sample of $\\Delta t$, we use the inverse transformation method, as specified in the third basis. Let $u$ be a random number drawn from the uniform distribution on $(0,1)$, denoted $U \\sim (0,1)$. We set $F_{\\Delta t}(\\Delta t) = u$:\n$$\nu = 1 - \\exp(-R \\Delta t)\n$$\nWe now solve for $\\Delta t$:\n$$\n\\exp(-R \\Delta t) = 1 - u\n$$\n$$\n-R \\Delta t = \\ln(1 - u)\n$$\n$$\n\\Delta t = -\\frac{1}{R} \\ln(1 - u)\n$$\nSince $u$ is a random variable uniformly distributed on $(0,1)$, the variable $u' = 1-u$ is also uniformly distributed on $(0,1)$. Therefore, we can simplify the expression by replacing $1-u$ with a new uniform random variate, which we can also call $u$ without loss of generality.\n$$\n\\Delta t = -\\frac{1}{R} \\ln(u) = -\\frac{\\ln(u)}{\\sum_{i=1}^{M} r_i}\n$$\nThis is the closed-form expression for the waiting time increment.\n\n**2. Derivation of the probability of channel $k$ firing next**\n\nWe wish to find the probability, $P_k$, that a specific channel $k \\in \\{1, \\dots, M\\}$ is the one that fires next. This is the probability that its waiting time, $T_k$, is the minimum among all waiting times:\n$$\nP_k = P(T_k = \\min(T_1, \\dots, T_M)) = P(T_k \\le T_i \\text{ for all } i \\in \\{1, \\dots, M\\})\n$$\nTo calculate this, we can condition on the value of $T_k$ and integrate over all possible values of its waiting time, $t$. The overall probability is the integral of the joint probability density function over the region where $t_k \\le t_i$ for all $i$. A more direct way is to use the law of total probability:\n$$\nP_k = \\int_{0}^{\\infty} P(T_k \\le T_i \\text{ for all } i \\ne k \\mid T_k = t) \\cdot p_k(t) dt\n$$\nIf $T_k = t$, the condition becomes $t \\le T_i$ for all $i \\ne k$. Due to the independence of the channels, the conditional probability is simply the probability of the event itself:\n$$\nP(T_i \\ge t \\text{ for all } i \\ne k \\mid T_k = t) = P(T_i \\ge t \\text{ for all } i \\ne k) = \\prod_{i \\ne k} P(T_i \\ge t)\n$$\nUsing the survival function $P(T_i \\ge t) = \\exp(-r_i t)$, this product becomes:\n$$\n\\prod_{i \\ne k} \\exp(-r_i t) = \\exp\\left(-t \\sum_{i \\ne k} r_i\\right)\n$$\nNow, we substitute this back into the integral, along with the PDF for $T_k$, $p_k(t) = r_k \\exp(-r_k t)$:\n$$\nP_k = \\int_{0}^{\\infty} \\left[ \\exp\\left(-t \\sum_{i \\ne k} r_i\\right) \\right] \\cdot [r_k \\exp(-r_k t)] dt\n$$\nCombining the exponential terms:\n$$\nP_k = \\int_{0}^{\\infty} r_k \\exp\\left(-t \\left(\\sum_{i \\ne k} r_i + r_k\\right)\\right) dt\n$$\nThe sum in the exponent is the total rate $R = \\sum_{i=1}^{M} r_i$.\n$$\nP_k = r_k \\int_{0}^{\\infty} \\exp(-R t) dt\n$$\nEvaluating the definite integral:\n$$\nP_k = r_k \\left[ -\\frac{1}{R} \\exp(-R t) \\right]_{t=0}^{t=\\infty} = r_k \\left( -\\frac{1}{R} \\lim_{t \\to \\infty}\\exp(-R t) - \\left(-\\frac{1}{R} \\exp(0)\\right) \\right)\n$$\nSince $R = \\sum r_i  0$, the limit is $0$.\n$$\nP_k = r_k \\left( 0 - \\left(-\\frac{1}{R}\\right) \\right) = \\frac{r_k}{R}\n$$\nThus, the probability that channel $k$ is the next to fire is its rate divided by the sum of all rates:\n$$\nP_k = \\frac{r_k}{\\sum_{i=1}^{M} r_i}\n$$\nThis is the second required closed-form expression.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{\\ln(u)}{\\sum_{i=1}^{M} r_i}  \\frac{r_k}{\\sum_{i=1}^{M} r_i} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A fundamental assumption underpinning the use of simulation to compute material properties is ergodicityâ€”the idea that a single, long simulation trajectory adequately samples the entire relevant phase space, making its time-averaged properties equivalent to ensemble averages. However, systems can become trapped in metastable states, violating this assumption. This exercise provides a hands-on method for testing ergodicity by comparing the variability of an observable across multiple independent simulation runs to the theoretical variability expected within a single ergodic run, allowing you to build a practical diagnostic for this critical simulation failure mode .",
            "id": "3841358",
            "problem": "A materials scientist studies a scalar observable $X(t)$, for example, the local shear stress in a representative volume element, recorded from a micro-scale dynamics simulator. Assume $X(t)$ is a strictly stationary, ergodic, and mixing process with mean $\\mu$, variance $\\sigma^{2}$, and time-autocovariance $C_{X}(t) = \\mathrm{Cov}(X(0), X(t)) = \\sigma^{2} \\exp(-|t|/\\tau)$, where $\\tau$ is the characteristic correlation time. The scientist runs $M$ independent simulations, each of total duration $T_{\\mathrm{run}}$, and partitions each run into non-overlapping blocks of duration $T_{b}$ with $T_{b} \\gg \\tau$. For block $b$ in run $r$, the block mean is defined by $\\bar{X}_{r,b} = \\frac{1}{T_{b}} \\int_{0}^{T_{b}} X_{r}(t) \\,\\mathrm{d}t$.\n\nYour tasks are:\n\n1. Starting only from the definitions of stationarity, ergodicity, and the autocovariance $C_{X}(t)$, derive the asymptotic distribution of a single block mean $\\bar{X}$ under ergodic sampling in the limit $T_{b} \\gg \\tau$. Express the distribution in terms of $\\mu$, $\\sigma^{2}$, $\\tau$, and $T_{b}$.\n\n2. Formulate a scalar diagnostic that detects nonergodicity by comparing the empirical across-run variability of block means to the theoretical ergodic prediction. The diagnostic must be constructed so that it equals $1$ under ergodic sampling and deviates systematically above $1$ when independent runs sample distinct long-lived states. Your diagnostic should be defined using observable quantities derived from the block means across runs, and the theoretical variance that you derive.\n\nFor the purposes of evaluation, provide the analytic expression for the probability density function $p(\\bar{x})$ of the block mean under ergodic sampling in the limit $T_{b} \\gg \\tau$. No numerical evaluation is required, and no physical units need be reported. The final answer must be a single closed-form expression.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a standard problem in the statistical analysis of time-series data from simulations, a core topic in stochastic simulation foundations. All necessary information is provided, and the tasks are well-defined.\n\nThe solution proceeds in two parts as requested. First, we derive the asymptotic distribution of the block mean $\\bar{X}$ under ergodic sampling. Second, we formulate a scalar diagnostic for detecting nonergodicity.\n\n### Part 1: Asymptotic Distribution of the Block Mean\n\nWe are given a strictly stationary, ergodic, and mixing scalar process $X(t)$ with mean $E[X(t)] = \\mu$, variance $\\mathrm{Var}(X(t)) = \\sigma^2$, and time-autocovariance function $C_X(t) = \\mathrm{Cov}(X(0), X(t)) = \\sigma^2 \\exp(-|t|/\\tau)$.\n\nA block mean is defined as $\\bar{X} = \\frac{1}{T_b} \\int_{0}^{T_b} X(t) \\, \\mathrm{d}t$. We seek its distribution in the limit where the block time $T_b$ is much larger than the correlation time $\\tau$, i.e., $T_b \\gg \\tau$.\n\nThe distribution of $\\bar{X}$ is characterized by its mean and variance. The Central Limit Theorem for ergodic processes states that in the limit of large averaging time ($T_b$), the distribution of the time average $\\bar{X}$ will be a normal (Gaussian) distribution.\n\n**1. Mean of the Block Mean**\n\nThe expectation of the block mean $E[\\bar{X}]$ is found by applying the linearity of the expectation operator.\n$$E[\\bar{X}] = E\\left[ \\frac{1}{T_b} \\int_{0}^{T_b} X(t) \\, \\mathrm{d}t \\right]$$\nBy Fubini's theorem, we can interchange the expectation and integration:\n$$E[\\bar{X}] = \\frac{1}{T_b} \\int_{0}^{T_b} E[X(t)] \\, \\mathrm{d}t$$\nSince the process $X(t)$ is stationary, its mean is constant for all time $t$, so $E[X(t)] = \\mu$.\n$$E[\\bar{X}] = \\frac{1}{T_b} \\int_{0}^{T_b} \\mu \\, \\mathrm{d}t = \\frac{1}{T_b} [\\mu t]_{0}^{T_b} = \\frac{\\mu T_b}{T_b} = \\mu$$\nThus, the block mean is an unbiased estimator of the process mean.\n\n**2. Variance of the Block Mean**\n\nThe variance of the block mean, $\\mathrm{Var}(\\bar{X})$, is given by:\n$$\\mathrm{Var}(\\bar{X}) = E\\left[ (\\bar{X} - \\mu)^2 \\right] = E\\left[ \\left( \\frac{1}{T_b} \\int_{0}^{T_b} (X(t) - \\mu) \\, \\mathrm{d}t \\right)^2 \\right]$$\nLet $Y(t) = X(t) - \\mu$. The process $Y(t)$ has zero mean, $E[Y(t)] = 0$, and its autocovariance is the same as for $X(t)$, i.e., $\\mathrm{Cov}(Y(t_1), Y(t_2)) = C_X(t_2-t_1)$.\n$$\\mathrm{Var}(\\bar{X}) = \\frac{1}{T_b^2} E\\left[ \\left( \\int_{0}^{T_b} Y(t_1) \\, \\mathrm{d}t_1 \\right) \\left( \\int_{0}^{T_b} Y(t_2) \\, \\mathrm{d}t_2 \\right) \\right]$$\n$$\\mathrm{Var}(\\bar{X}) = \\frac{1}{T_b^2} E\\left[ \\int_{0}^{T_b} \\int_{0}^{T_b} Y(t_1) Y(t_2) \\, \\mathrm{d}t_1 \\, \\mathrm{d}t_2 \\right]$$\nAgain, interchanging expectation and integration:\n$$\\mathrm{Var}(\\bar{X}) = \\frac{1}{T_b^2} \\int_{0}^{T_b} \\int_{0}^{T_b} E[Y(t_1) Y(t_2)] \\, \\mathrm{d}t_1 \\, \\mathrm{d}t_2$$\nSince $E[Y(t)]=0$, $E[Y(t_1) Y(t_2)] = \\mathrm{Cov}(Y(t_1), Y(t_2)) = C_X(t_2-t_1)$.\n$$\\mathrm{Var}(\\bar{X}) = \\frac{1}{T_b^2} \\int_{0}^{T_b} \\int_{0}^{T_b} C_X(t_2-t_1) \\, \\mathrm{d}t_1 \\, \\mathrm{d}t_2$$\nSubstituting the given form $C_X(t) = \\sigma^2 \\exp(-|t|/\\tau)$:\n$$\\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{T_b^2} \\int_{0}^{T_b} \\int_{0}^{T_b} \\exp\\left(-\\frac{|t_2 - t_1|}{\\tau}\\right) \\, \\mathrm{d}t_1 \\, \\mathrm{d}t_2$$\nThis integral can be evaluated as:\n$$I = \\int_{0}^{T_b} \\int_{0}^{T_b} \\exp\\left(-\\frac{|t_2 - t_1|}{\\tau}\\right) \\, \\mathrm{d}t_1 \\, \\mathrm{d}t_2 = 2 \\int_{0}^{T_b} \\int_{0}^{t_2} \\exp\\left(-\\frac{t_2 - t_1}{\\tau}\\right) \\, \\mathrm{d}t_1 \\, \\mathrm{d}t_2$$\n$$I = 2 \\int_{0}^{T_b} \\exp\\left(-\\frac{t_2}{\\tau}\\right) \\left[ \\tau \\exp\\left(\\frac{t_1}{\\tau}\\right) \\right]_{0}^{t_2} \\, \\mathrm{d}t_2 = 2\\tau \\int_{0}^{T_b} \\exp\\left(-\\frac{t_2}{\\tau}\\right) \\left( \\exp\\left(\\frac{t_2}{\\tau}\\right) - 1 \\right) \\, \\mathrm{d}t_2$$\n$$I = 2\\tau \\int_{0}^{T_b} \\left( 1 - \\exp\\left(-\\frac{t_2}{\\tau}\\right) \\right) \\, \\mathrm{d}t_2 = 2\\tau \\left[ t_2 + \\tau \\exp\\left(-\\frac{t_2}{\\tau}\\right) \\right]_{0}^{T_b}$$\n$$I = 2\\tau \\left[ \\left( T_b + \\tau \\exp\\left(-\\frac{T_b}{\\tau}\\right) \\right) - (0 + \\tau) \\right] = 2\\tau \\left( T_b - \\tau + \\tau \\exp\\left(-\\frac{T_b}{\\tau}\\right) \\right)$$\n$$I = 2\\tau T_b - 2\\tau^2 \\left( 1 - \\exp\\left(-\\frac{T_b}{\\tau}\\right) \\right)$$\nTherefore, the exact variance is:\n$$\\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{T_b^2} \\left[ 2\\tau T_b - 2\\tau^2 \\left( 1 - \\exp\\left(-\\frac{T_b}{\\tau}\\right) \\right) \\right] = \\frac{2\\sigma^2\\tau}{T_b} \\left[ 1 - \\frac{\\tau}{T_b}\\left(1 - \\exp\\left(-\\frac{T_b}{\\tau}\\right)\\right) \\right]$$\n\n**3. Asymptotic Limit and Distribution**\n\nWe are interested in the limit $T_b \\gg \\tau$. In this limit, $T_b/\\tau \\to \\infty$, which implies $\\exp(-T_b/\\tau) \\to 0$ and $\\tau/T_b \\to 0$. The expression for the variance simplifies significantly:\n$$\\mathrm{Var}(\\bar{X}) \\approx \\frac{2\\sigma^2\\tau}{T_b}$$\nThis is a standard result, often written as $\\mathrm{Var}(\\bar{X}) \\approx \\frac{2\\tau_{\\text{int}}\\sigma^2}{T_b}$, where the integrated correlation time $\\tau_{\\text{int}} = \\frac{1}{\\sigma^2}\\int_0^\\infty C_X(t)\\,\\mathrm{d}t$. For the given exponential decay, $\\tau_{\\text{int}} = \\tau$.\n\nBased on the Central Limit Theorem for correlated processes, the distribution of $\\bar{X}$ for large $T_b$ converges to a normal distribution with mean $\\mu$ and variance $\\mathrm{Var}(\\bar{X})$.\n$$\\bar{X} \\sim N\\left(\\mu, \\frac{2\\sigma^2\\tau}{T_b}\\right)$$\nThe probability density function (PDF), $p(\\bar{x})$, for a variable $\\bar{x}$ drawn from this distribution is:\n$$p(\\bar{x}) = \\frac{1}{\\sqrt{2\\pi \\mathrm{Var}(\\bar{X})}} \\exp\\left( -\\frac{(\\bar{x} - \\mu)^2}{2\\mathrm{Var}(\\bar{X})} \\right)$$\nSubstituting the asymptotic variance, we obtain:\n$$p(\\bar{x}) = \\frac{1}{\\sqrt{2\\pi \\left(\\frac{2\\sigma^2\\tau}{T_b}\\right)}} \\exp\\left( -\\frac{(\\bar{x} - \\mu)^2}{2\\left(\\frac{2\\sigma^2\\tau}{T_b}\\right)} \\right) = \\frac{1}{\\sqrt{\\frac{4\\pi\\sigma^2\\tau}{T_b}}} \\exp\\left( -\\frac{(\\bar{x} - \\mu)^2 T_b}{4\\sigma^2\\tau} \\right)$$\n\n### Part 2: Diagnostic for Nonergodicity\n\nA diagnostic for nonergodicity can be constructed by comparing the empirically observed variability of block means across independent runs to the theoretical variability predicted by the ergodic hypothesis.\n\nLet $\\{\\bar{X}_{r,b}\\}$ be the set of block means, where $r = 1, \\dots, M$ denotes the run index and $b$ denotes the block index within a run. For simplicity, let's consider a single block mean from each of the $M$ runs, denoted $\\{\\bar{X}_r\\}_{r=1}^M$.\n\n**1. Variability under Ergodic Hypothesis**\n\nUnder the ergodic hypothesis, all runs sample the same underlying statistical ensemble. The block means $\\bar{X}_r$ are independent and identically distributed (i.i.d.) random variables drawn from the distribution derived in Part 1, $N(\\mu, \\sigma^2_{\\bar{X},\\text{th}})$, where the theoretical variance is $\\sigma^2_{\\bar{X},\\text{th}} = \\mathrm{Var}_{\\text{ergodic}}(\\bar{X}) \\approx \\frac{2\\sigma^2\\tau}{T_b}$.\n\n**2. Variability under Nonergodic Hypothesis**\n\nNonergodicity, as described, implies that different runs get trapped in distinct long-lived states, each with its own mean. We can model this by letting the mean of run $r$ be a random variable $\\mu_r$, drawn from a distribution with overall mean $E[\\mu_r] = \\mu$ and variance $\\mathrm{Var}(\\mu_r) = \\sigma_{\\mu}^2  0$. The value $\\sigma_{\\mu}^2$ quantifies the extent of ergodicity breaking.\nWithin each run $r$, the block mean $\\bar{X}_r$ fluctuates around $\\mu_r$ with variance $\\sigma^2_{\\bar{X},\\text{th}}$.\nThe total variance of $\\bar{X}_r$ across the ensemble of runs can be found using the law of total variance:\n$$\\mathrm{Var}_{\\text{non-ergodic}}(\\bar{X}_r) = E[\\mathrm{Var}(\\bar{X}_r|\\mu_r)] + \\mathrm{Var}(E[\\bar{X}_r|\\mu_r])$$\nHere, $E[\\bar{X}_r|\\mu_r] = \\mu_r$ and $\\mathrm{Var}(\\bar{X}_r|\\mu_r) = \\sigma^2_{\\bar{X},\\text{th}}$.\n$$\\mathrm{Var}_{\\text{non-ergodic}}(\\bar{X}_r) = E[\\sigma^2_{\\bar{X},\\text{th}}] + \\mathrm{Var}(\\mu_r) = \\sigma^2_{\\bar{X},\\text{th}} + \\sigma_{\\mu}^2$$\nThe total variance is thus increased by an amount $\\sigma_{\\mu}^2$ due to the variability of the means across runs.\n\n**3. The Diagnostic**\n\nWe define the diagnostic, $D$, as the ratio of the empirical across-run variance of block means to the theoretical variance predicted by the ergodic hypothesis.\nThe empirical variance of the block means $\\{\\bar{X}_r\\}_{r=1}^M$ is given by the sample variance:\n$$S^2_{\\bar{X}} = \\frac{1}{M-1} \\sum_{r=1}^M (\\bar{X}_r - \\langle\\bar{X}\\rangle)^2 \\quad \\text{where} \\quad \\langle\\bar{X}\\rangle = \\frac{1}{M}\\sum_{r=1}^M \\bar{X}_r$$\nThe diagnostic is then:\n$$D = \\frac{S^2_{\\bar{X}}}{\\sigma^2_{\\bar{X},\\text{th}}} = \\frac{S^2_{\\bar{X}}}{\\left( \\frac{2\\sigma^2\\tau}{T_b} \\right)}$$\nThe quantities $\\sigma^2$ and $\\tau$ are intrinsic properties of the process, which can be estimated from the simulation data (e.g., from the variance and autocorrelation function of $X(t)$ within each run).\n\n**Behavior of the Diagnostic:**\n- **Ergodic case:** $S^2_{\\bar{X}}$ is an unbiased estimator of $\\sigma^2_{\\bar{X},\\text{th}}$. For large $M$, $S^2_{\\bar{X}} \\to \\sigma^2_{\\bar{X},\\text{th}}$, so $D \\to 1$.\n- **Nonergodic case:** $S^2_{\\bar{X}}$ is an unbiased estimator of $\\mathrm{Var}_{\\text{non-ergodic}}(\\bar{X}) = \\sigma^2_{\\bar{X},\\text{th}} + \\sigma_{\\mu}^2$. For large $M$, $S^2_{\\bar{X}} \\to \\sigma^2_{\\bar{X},\\text{th}} + \\sigma_{\\mu}^2$. The diagnostic becomes:\n$$D \\to \\frac{\\sigma^2_{\\bar{X},\\text{th}} + \\sigma_{\\mu}^2}{\\sigma^2_{\\bar{X},\\text{th}}} = 1 + \\frac{\\sigma_{\\mu}^2}{\\sigma^2_{\\bar{X},\\text{th}}} = 1 + \\frac{\\sigma_{\\mu}^2 T_b}{2\\sigma^2\\tau}$$\nSince $\\sigma_{\\mu}^2  0$, the diagnostic $D$ will systematically be greater than $1$, indicating a breaking of ergodicity. This meets all requirements for the diagnostic.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{\\frac{4\\pi\\sigma^2\\tau}{T_b}}} \\exp\\left( -\\frac{(\\bar{x} - \\mu)^2}{ \\frac{4\\sigma^2\\tau}{T_b} } \\right)}$$"
        },
        {
            "introduction": "Once a valid simulation trajectory is obtained, the final step is to compute average properties and, just as importantly, their statistical uncertainty. A common mistake is to treat every data point from a simulation as an independent measurement, which severely underestimates the true statistical error because successive states in a simulation are inherently correlated. This practice confronts this issue directly by introducing the concept of the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, to quantify the 'memory' of the process, leading you to derive and compute the effective sample size, $n_{\\mathrm{eff}}$, which is the true number of independent samples your simulation has produced .",
            "id": "3841383",
            "problem": "A molecular-scale observable $A$ (for example, the shear stress) is recorded from a long Molecular Dynamics (MD) trajectory of a nanocrystalline metal at equally spaced times $t_{k}=k\\,\\Delta t$ with $\\Delta t=1\\,\\mathrm{fs}$, yielding $n=2\\times 10^{5}$ samples. Assume the process $\\{A_{k}\\}$ is strictly stationary with finite variance $\\sigma^{2}=\\gamma(0)$ and normalized autocorrelation function $\\rho(k)=\\gamma(k)/\\gamma(0)$, where $\\gamma(k)$ is the autocovariance function. The autocorrelation is modeled as a physically motivated two-timescale mixture,\n$$\n\\rho(k)=a\\,\\exp\\!\\left(-\\frac{k\\,\\Delta t}{\\tau_{1}}\\right)+\\left(1-a\\right)\\,\\exp\\!\\left(-\\frac{k\\,\\Delta t}{\\tau_{2}}\\right),\n$$\nwith parameters $a=0.4$, $\\tau_{1}=3\\,\\mathrm{fs}$, and $\\tau_{2}=20\\,\\mathrm{fs}$.\n\nStarting from first principles, namely the definition of the variance of the sample mean in terms of the autocovariance,\n$$\n\\operatorname{Var}\\!\\left(\\bar{A}\\right)=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{k=1}^{n}A_{k}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\gamma(|i-j|),\n$$\nderive a large-$n$ expression for the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ (dimensionless, in units of sampling steps) in terms of $\\rho(k)$, and from it derive a formula for the effective sample size $n_{\\mathrm{eff}}$ defined by equating $\\operatorname{Var}(\\bar{A})$ to that of $n_{\\mathrm{eff}}$ independent samples. Then, using the given $\\rho(k)$, compute the numerical value of $n_{\\mathrm{eff}}$ for the specified $n$ and parameters.\n\nFinally, briefly explain how the computed $n_{\\mathrm{eff}}$ modifies the half-width of a normal-approximation confidence interval for the mean of $A$ relative to the case of $n$ independent samples, making explicit the scaling with $n_{\\mathrm{eff}}$.\n\nRound your final numerical answer for $n_{\\mathrm{eff}}$ to four significant figures. No units are required for $n_{\\mathrm{eff}}$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the statistical analysis of time series from molecular simulations, is well-posed with all necessary information provided, and is expressed in objective, formal language. We may therefore proceed with a solution.\n\nThe starting point is the definition of the variance of the sample mean $\\bar{A}$ of a stationary time series $\\{A_k\\}$:\n$$\n\\operatorname{Var}(\\bar{A}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{k=1}^{n}A_{k}\\right) = \\frac{1}{n^2}\\operatorname{Cov}\\left(\\sum_{i=1}^{n}A_{i}, \\sum_{j=1}^{n}A_{j}\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\operatorname{Cov}(A_i, A_j).\n$$\nFor a stationary process, the autocovariance function $\\gamma(k) = \\operatorname{Cov}(A_i, A_{i+k})$ depends only on the time lag $k$. Thus, $\\operatorname{Cov}(A_i, A_j) = \\gamma(|i-j|)$. The given expression is recovered:\n$$\n\\operatorname{Var}(\\bar{A}) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\gamma(|i-j|).\n$$\nTo simplify the double summation, we group terms with the same lag $k = |i-j|$. For a given lag $k \\in \\{1, 2, \\dots, n-1\\}$, there are $2(n-k)$ pairs $(i, j)$ in the sum such that $|i-j|=k$. For $k=0$, there are $n$ pairs $(i,i)$. The sum can be rewritten as:\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\gamma(|i-j|) = n\\gamma(0) + \\sum_{k=1}^{n-1} 2(n-k)\\gamma(k).\n$$\nSubstituting this back into the expression for $\\operatorname{Var}(\\bar{A})$ and using the definitions $\\sigma^2 = \\gamma(0)$ and $\\rho(k) = \\gamma(k)/\\gamma(0)$:\n$$\n\\operatorname{Var}(\\bar{A}) = \\frac{1}{n^2} \\left[ n\\sigma^2 + \\sum_{k=1}^{n-1} 2(n-k)\\sigma^2\\rho(k) \\right] = \\frac{\\sigma^2}{n} \\left[ 1 + \\frac{2}{n}\\sum_{k=1}^{n-1}(n-k)\\rho(k) \\right].\n$$\n$$\n\\operatorname{Var}(\\bar{A}) = \\frac{\\sigma^2}{n} \\left[ 1 + 2\\sum_{k=1}^{n-1}\\left(1-\\frac{k}{n}\\right)\\rho(k) \\right].\n$$\nFor a large number of samples $n$, and an autocorrelation function $\\rho(k)$ that decays sufficiently fast, the term $k/n$ is negligible for the values of $k$ where $\\rho(k)$ is significantly different from zero. Also, the upper limit of the sum can be extended from $n-1$ to $\\infty$ with negligible error. This yields the large-$n$ approximation:\n$$\n\\operatorname{Var}(\\bar{A}) \\approx \\frac{\\sigma^2}{n} \\left[ 1 + 2\\sum_{k=1}^{\\infty}\\rho(k) \\right].\n$$\nThe problem asks us to derive an expression for the dimensionless integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, in units of sampling steps. A standard definition in this context is:\n$$\n\\tau_{\\mathrm{int}} \\equiv \\sum_{k=1}^{\\infty}\\rho(k).\n$$\nWith this definition, the variance becomes:\n$$\n\\operatorname{Var}(\\bar{A}) \\approx \\frac{\\sigma^2}{n} (1 + 2\\tau_{\\mathrm{int}}).\n$$\nThe effective sample size, $n_{\\mathrm{eff}}$, is defined by equating this variance to the variance of the mean of $n_{\\mathrm{eff}}$ independent and identically distributed samples, which is $\\sigma^2/n_{\\mathrm{eff}}$.\n$$\n\\frac{\\sigma^2}{n_{\\mathrm{eff}}} = \\frac{\\sigma^2}{n} (1 + 2\\tau_{\\mathrm{int}}).\n$$\nSolving for $n_{\\mathrm{eff}}$ gives the desired formula:\n$$\nn_{\\mathrm{eff}} = \\frac{n}{1 + 2\\tau_{\\mathrm{int}}}.\n$$\nNow, we compute the numerical value of $n_{\\mathrm{eff}}$. First, we must calculate $\\tau_{\\mathrm{int}}$ using the given model for $\\rho(k)$:\n$$\n\\rho(k) = a\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_1}\\right) + (1-a)\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_2}\\right).\n$$\nWe compute the sum $\\tau_{\\mathrm{int}} = \\sum_{k=1}^{\\infty}\\rho(k)$:\n$$\n\\tau_{\\mathrm{int}} = \\sum_{k=1}^{\\infty} \\left[ a\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_1}\\right) + (1-a)\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_2}\\right) \\right].\n$$\nThis is the sum of two geometric series. Let $r_1 = \\exp(-\\Delta t/\\tau_1)$ and $r_2 = \\exp(-\\Delta t/\\tau_2)$. The sum of a geometric series $\\sum_{k=1}^{\\infty} q^k$ is $\\frac{q}{1-q}$ for $|q|1$.\n$$\n\\tau_{\\mathrm{int}} = a \\left(\\sum_{k=1}^{\\infty} r_1^k\\right) + (1-a) \\left(\\sum_{k=1}^{\\infty} r_2^k\\right) = a \\frac{r_1}{1-r_1} + (1-a) \\frac{r_2}{1-r_2}.\n$$\nThis can also be written as:\n$$\n\\tau_{\\mathrm{int}} = a \\frac{\\exp(-\\Delta t/\\tau_1)}{1-\\exp(-\\Delta t/\\tau_1)} + (1-a) \\frac{\\exp(-\\Delta t/\\tau_2)}{1-\\exp(-\\Delta t/\\tau_2)} = a \\frac{1}{\\exp(\\Delta t/\\tau_1)-1} + (1-a) \\frac{1}{\\exp(\\Delta t/\\tau_2)-1}.\n$$\nWe substitute the given parameter values: $a=0.4$, $\\tau_1=3\\,\\mathrm{fs}$, $\\tau_2=20\\,\\mathrm{fs}$, and $\\Delta t=1\\,\\mathrm{fs}$.\n$$\n\\frac{\\Delta t}{\\tau_1} = \\frac{1}{3}, \\quad \\frac{\\Delta t}{\\tau_2} = \\frac{1}{20} = 0.05.\n$$\n$$\n\\tau_{\\mathrm{int}} = 0.4 \\left( \\frac{1}{\\exp(1/3)-1} \\right) + 0.6 \\left( \\frac{1}{\\exp(0.05)-1} \\right).\n$$\nNow we compute the values:\n$$\n\\exp(1/3) \\approx 1.3956124 \\\\\n\\exp(0.05) \\approx 1.0512711 \\\\\n$$\n$$\n\\tau_{\\mathrm{int}} \\approx 0.4 \\left( \\frac{1}{1.3956124 - 1} \\right) + 0.6 \\left( \\frac{1}{1.0512711 - 1} \\right) \\\\\n\\tau_{\\mathrm{int}} \\approx 0.4 \\left( \\frac{1}{0.3956124} \\right) + 0.6 \\left( \\frac{1}{0.0512711} \\right) \\\\\n\\tau_{\\mathrm{int}} \\approx 0.4 (2.527723) + 0.6 (19.504221) \\\\\n\\tau_{\\mathrm{int}} \\approx 1.011089 + 11.702533 \\approx 12.713622.\n$$\nNow we compute $n_{\\mathrm{eff}}$ using $n = 2 \\times 10^5$:\n$$\nn_{\\mathrm{eff}} = \\frac{n}{1 + 2\\tau_{\\mathrm{int}}} \\approx \\frac{2 \\times 10^5}{1 + 2(12.713622)} = \\frac{2 \\times 10^5}{1 + 25.427244} = \\frac{2 \\times 10^5}{26.427244} \\approx 7567.9228.\n$$\nRounding to four significant figures, we get $n_{\\mathrm{eff}} = 7568$.\n\nFinally, we explain the impact on the confidence interval for the mean of $A$. Under the Central Limit Theorem for correlated processes, the sampling distribution of the mean $\\bar{A}$ is approximately normal for large $n$, with mean $\\mu=E[A]$ and variance $\\operatorname{Var}(\\bar{A})$. The standard error of the mean is $SE(\\bar{A}) = \\sqrt{\\operatorname{Var}(\\bar{A})}$.\nFor correlated samples, we have $SE(\\bar{A}) = \\sqrt{\\sigma^2/n_{\\mathrm{eff}}} = \\sigma/\\sqrt{n_{\\mathrm{eff}}}$.\nIf one were to incorrectly assume the $n$ samples are independent, the standard error would be calculated as $SE_{\\mathrm{naive}}(\\bar{A}) = \\sigma/\\sqrt{n}$.\nThe half-width $W$ of a normal-approximation confidence interval is proportional to the standard error, e.g., $W = z_{\\alpha/2} SE(\\bar{A})$.\nThe presence of positive correlation ($\\tau_{\\mathrm{int}}  0$) leads to $n_{\\mathrm{eff}}  n$. This means the true standard error is larger than the naive estimate. The scaling of the half-width is with $n_{\\mathrm{eff}}^{-1/2}$.\nThe ratio of the correct half-width to the naive half-width is:\n$$\n\\frac{W_{\\mathrm{correct}}}{W_{\\mathrm{naive}}} = \\frac{z_{\\alpha/2} \\sigma / \\sqrt{n_{\\mathrm{eff}}}}{z_{\\alpha/2} \\sigma / \\sqrt{n}} = \\sqrt{\\frac{n}{n_{\\mathrm{eff}}}} = \\sqrt{1 + 2\\tau_{\\mathrm{int}}}.\n$$\nIn this problem, this factor is $\\sqrt{26.427...} \\approx 5.14$. This means that ignoring the temporal correlations would lead to underestimating the statistical error and the width of the confidence interval by a factor of more than $5$. The half-width scales as $n_{\\mathrm{eff}}^{-1/2}$ rather than the $n^{-1/2}$ scaling expected for independent data.",
            "answer": "$$\\boxed{7568}$$"
        }
    ]
}