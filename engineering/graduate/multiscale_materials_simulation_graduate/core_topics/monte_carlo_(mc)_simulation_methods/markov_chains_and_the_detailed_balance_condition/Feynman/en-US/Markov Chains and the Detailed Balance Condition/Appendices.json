{
    "hands_on_practices": [
        {
            "introduction": "In multiscale modeling, we often construct coarse-grained models from extensive simulation data. A critical question is whether these empirical models are consistent with the physical principle of equilibrium. This exercise provides a direct way to test for violations of detailed balance by calculating the net probability fluxes from simulated transition counts, giving you a quantitative tool to assess if your system is truly at equilibrium. ",
            "id": "3823225",
            "problem": "A coarse-grained Kinetic Monte Carlo (KMC) simulation of defect migration in a crystalline alloy produces a three-state embedded jump Markov chain representing mesoscopic configurations $x \\in \\{1,2,3\\}$ at a fixed temperature. Over a long run, the simulation recorded the following counts of departures and directed transitions:\n- Total departures from each state: $N_{1} = 5000$, $N_{2} = 3000$, $N_{3} = 2000$.\n- Directed transition counts: $N_{12} = 2500$, $N_{13} = 2500$; $N_{21} = 1500$, $N_{23} = 1500$; $N_{31} = 1600$, $N_{32} = 400$. All other directed counts $N_{xy}$ with $x=y$ or not listed can be taken as $0$, and for each $x$ the counts satisfy $\\sum_{y \\neq x} N_{xy} = N_{x}$.\n\nStarting from the definitions of a discrete-time Markov chain, the stationary (long-time) distribution, and the equilibrium detailed balance requirement for probability fluxes between states, proceed as follows:\n1. Using the recorded counts, form the empirical stationary distribution $\\hat{\\pi}(x)$ and the empirical transition matrix $\\hat{P}(x,y)$ by $\\,\\hat{\\pi}(x) = N_{x} / \\sum_{z} N_{z}$ and $\\,\\hat{P}(x,y) = N_{xy} / N_{x}$ for $x \\neq y$ (with $\\,\\hat{P}(x,x) = 0$ here).\n2. From the principle that equilibrium detailed balance requires equal opposing probability fluxes between every pair of states, define a net probability flux $J_{xy}$ for each unordered pair $\\{x,y\\}$ and compute its value using your $\\hat{\\pi}$ and $\\hat{P}$.\n3. Quantify deviations from detailed balance by the scalar metric $D = \\sum_{x<y} |J_{xy}|$, summing over the unordered pairs $\\{1,2\\}$, $\\{1,3\\}$, and $\\{2,3\\}$.\n\nReport the final value of $D$ as a unitless number, rounded to four significant figures. Do not report intermediate values; only the single scalar $D$ is required in your final answer.",
            "solution": "The problem statement has been validated and is deemed sound. It provides a self-contained, consistent, and well-posed set of instructions for analyzing empirical data from a Markov chain simulation, grounded in the standard principles of statistical mechanics and stochastic processes. All necessary data and definitions are provided, and the task is objective and computationally verifiable.\n\nThe solution proceeds by following the three steps specified in the problem.\n\n1.  **Empirical Stationary Distribution and Transition Matrix**\n\nFirst, we calculate the total number of observed departure events, $N_{total}$, by summing the departures from each state $x \\in \\{1,2,3\\}$.\n$$N_{total} = \\sum_{z=1}^{3} N_{z} = N_{1} + N_{2} + N_{3} = 5000 + 3000 + 2000 = 10000$$\n\nUsing the provided definition, the empirical stationary distribution $\\hat{\\pi}(x)$ is computed for each state:\n$$\\hat{\\pi}(1) = \\frac{N_{1}}{N_{total}} = \\frac{5000}{10000} = 0.5$$\n$$\\hat{\\pi}(2) = \\frac{N_{2}}{N_{total}} = \\frac{3000}{10000} = 0.3$$\n$$\\hat{\\pi}(3) = \\frac{N_{3}}{N_{total}} = \\frac{2000}{10000} = 0.2$$\nThe distribution is properly normalized: $\\hat{\\pi}(1) + \\hat{\\pi}(2) + \\hat{\\pi}(3) = 0.5 + 0.3 + 0.2 = 1$.\n\nNext, we compute the empirical transition probabilities $\\hat{P}(x,y)$ for $x \\neq y$. The problem specifies that $\\hat{P}(x,x) = 0$.\nFor state $1$:\n$$\\hat{P}(1,2) = \\frac{N_{12}}{N_{1}} = \\frac{2500}{5000} = 0.5$$\n$$\\hat{P}(1,3) = \\frac{N_{13}}{N_{1}} = \\frac{2500}{5000} = 0.5$$\nFor state $2$:\n$$\\hat{P}(2,1) = \\frac{N_{21}}{N_{2}} = \\frac{1500}{3000} = 0.5$$\n$$\\hat{P}(2,3) = \\frac{N_{23}}{N_{2}} = \\frac{1500}{3000} = 0.5$$\nFor state $3$:\n$$\\hat{P}(3,1) = \\frac{N_{31}}{N_{3}} = \\frac{1600}{2000} = 0.8$$\n$$\\hat{P}(3,2) = \\frac{N_{32}}{N_{3}} = \\frac{400}{2000} = 0.2$$\n\nThe full empirical transition matrix $\\hat{P}$ is:\n$$\n\\hat{P} = \\begin{pmatrix}\n0 & 0.5 & 0.5 \\\\\n0.5 & 0 & 0.5 \\\\\n0.8 & 0.2 & 0\n\\end{pmatrix}\n$$\n\n2.  **Net Probability Flux Calculation**\n\nThe principle of detailed balance for a Markov chain in equilibrium with stationary distribution $\\pi$ states that for any pair of states $\\{x,y\\}$, the probability flux from $x$ to $y$ equals the flux from $y$ to $x$. The flux from $x$ to $y$ is the joint probability of being in state $x$ and transitioning to state $y$, which is given by $\\pi(x)P(x,y)$. The detailed balance condition is thus:\n$$\\pi(x)P(x,y) = \\pi(y)P(y,x)$$\n\nThe net probability flux, $J_{xy}$, for an unordered pair $\\{x,y\\}$ is defined as the difference between these opposing fluxes. We define it as:\n$$J_{xy} = \\pi(x)P(x,y) - \\pi(y)P(y,x)$$\nA non-zero value for $J_{xy}$ indicates a violation of detailed balance, signifying a non-equilibrium stationary state. We compute the empirical net fluxes using our calculated $\\hat{\\pi}$ and $\\hat{P}$.\n\nFor the pair $\\{1,2\\}$:\n$$J_{12} = \\hat{\\pi}(1)\\hat{P}(1,2) - \\hat{\\pi}(2)\\hat{P}(2,1) = (0.5)(0.5) - (0.3)(0.5) = 0.25 - 0.15 = 0.10$$\n\nFor the pair $\\{1,3\\}$:\n$$J_{13} = \\hat{\\pi}(1)\\hat{P}(1,3) - \\hat{\\pi}(3)\\hat{P}(3,1) = (0.5)(0.5) - (0.2)(0.8) = 0.25 - 0.16 = 0.09$$\n\nFor the pair $\\{2,3\\}$:\n$$J_{23} = \\hat{\\pi}(2)\\hat{P}(2,3) - \\hat{\\pi}(3)\\hat{P}(3,2) = (0.3)(0.5) - (0.2)(0.2) = 0.15 - 0.04 = 0.11$$\n\n3.  **Scalar Metric for Deviation from Detailed Balance**\n\nThe total deviation from detailed balance, $D$, is quantified by summing the absolute values of the net fluxes over all unique (unordered) pairs of states. The unordered pairs are $\\{1,2\\}$, $\\{1,3\\}$, and $\\{2,3\\}$, corresponding to the summation index $x<y$.\n$$D = \\sum_{x<y} |J_{xy}| = |J_{12}| + |J_{13}| + |J_{23}|$$\nSubstituting the calculated values:\n$$D = |0.10| + |0.09| + |0.11| = 0.10 + 0.09 + 0.11 = 0.30$$\nThe problem requires the result to be rounded to four significant figures. The exact value is $0.3$, which is written as $0.3000$ to meet this requirement.",
            "answer": "$$\\boxed{0.3000}$$"
        },
        {
            "introduction": "Now that we can diagnose violations of detailed balance, let's explore why reversible systems are so analytically powerful. This idealized two-state model allows you to derive a direct relationship between the microscopic transition probabilities, the spectral gap of the transition matrix, and the system's relaxation dynamics. Mastering this connection is key to understanding how quickly a simulation converges and how efficiently it samples the state space. ",
            "id": "3823252",
            "problem": "Consider a coarse-grained two-basin model of a crystalline defect in multiscale materials simulation, where the system is reduced to a discrete-time Markov chain on two metastable basins $\\{A,B\\}$ with energies $E_{A}$ and $E_{B}$, respectively. Transitions are proposed at each time step with probabilities $P(A \\to B) = \\alpha$ and $P(B \\to A) = \\beta$, with $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$. The transition matrix is\n$$\nP \\;=\\; \\begin{pmatrix}\n1-\\alpha & \\alpha \\\\\n\\beta & 1-\\beta\n\\end{pmatrix}.\n$$\nAssume that the chain is physically consistent with thermal equilibrium at inverse temperature $\\beta_{\\mathrm{th}}$ by satisfying the detailed balance condition with respect to the Boltzmann stationary distribution, i.e., $\\pi_{i} \\propto \\exp(-\\beta_{\\mathrm{th}} E_{i})$ and $\\pi_{i} P_{ij} = \\pi_{j} P_{ji}$ for $i,j \\in \\{A,B\\}$. Let the observable of interest be the energy $H(X_{t}) = E_{X_{t}}$, and assume $E_{A} \\neq E_{B}$ so that the variance of $H$ under stationarity is nonzero. Define the centered observable $h(X_{t}) = H(X_{t}) - \\mathbb{E}_{\\pi}[H]$ and the normalized autocorrelation function $\\rho(t) = \\mathbb{E}_{\\pi}[h(X_{0}) h(X_{t})]/\\mathbb{E}_{\\pi}[h(X_{0})^{2}]$ for $t \\in \\mathbb{N}$, where $\\mathbb{E}_{\\pi}[\\cdot]$ denotes expectation with respect to the stationary distribution.\n\nStarting only from the core definitions of a discrete-time Markov chain, stationary distribution, detailed balance (reversibility), eigen-decomposition of a reversible transition operator, and the definition of the integrated autocorrelation time\n$$\n\\tau_{\\mathrm{int}} \\;=\\; 1 + 2 \\sum_{t=1}^{\\infty} \\rho(t),\n$$\nperform the following steps:\n\n1. Derive the stationary distribution $\\pi$ in terms of $\\alpha$ and $\\beta$ and check that detailed balance holds with the Boltzmann form.\n\n2. Compute the eigenvalues of $P$ and define the spectral gap $g$ as $g = 1 - \\sup\\{|\\lambda| : \\lambda \\in \\mathrm{spec}(P), \\lambda \\neq 1\\}$.\n\n3. Using the reversibility of $P$ and the fact that the space orthogonal to constants is one-dimensional for a two-state chain, express $\\rho(t)$ for the centered energy $h$ and hence obtain a closed-form expression for $\\tau_{\\mathrm{int}}$.\n\nProvide your final answer as a single closed-form analytic expression for the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ in terms of $\\alpha$ and $\\beta$. Express $\\tau_{\\mathrm{int}}$ in units of steps. No rounding is required.",
            "solution": "The problem asks for the derivation of the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for the energy observable in a two-state discrete-time Markov chain. The derivation will be performed in steps as requested by the problem statement.\n\nFirst, we address the three tasks outlined in the problem: derivation of the stationary distribution and verification of detailed balance, computation of the eigenvalues and spectral gap, and finally the derivation of the autocorrelation function and the integrated autocorrelation time.\n\n**1. Stationary Distribution and Detailed Balance**\nLet the stationary distribution be $\\pi = \\begin{pmatrix} \\pi_A & \\pi_B \\end{pmatrix}$. By definition, it must satisfy the stationarity condition $\\pi P = \\pi$, where $P$ is the transition matrix.\n$$\n\\begin{pmatrix} \\pi_A & \\pi_B \\end{pmatrix} \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{pmatrix} = \\begin{pmatrix} \\pi_A & \\pi_B \\end{pmatrix}\n$$\nThis matrix equation yields two linear equations:\n$1$) $\\pi_A (1-\\alpha) + \\pi_B \\beta = \\pi_A$\n$2$) $\\pi_A \\alpha + \\pi_B (1-\\beta) = \\pi_B$\n\nFrom equation $1$), we find $-\\pi_A \\alpha + \\pi_B \\beta = 0$, which simplifies to $\\pi_A \\alpha = \\pi_B \\beta$. This demonstrates that the two equations are linearly dependent, as equation $2$) gives the same relationship. Along with the normalization condition $\\pi_A + \\pi_B = 1$, we can solve for $\\pi_A$ and $\\pi_B$.\nSubstituting $\\pi_B = \\pi_A (\\frac{\\alpha}{\\beta})$ into the normalization equation gives:\n$$\n\\pi_A + \\pi_A \\frac{\\alpha}{\\beta} = 1 \\implies \\pi_A \\left(1 + \\frac{\\alpha}{\\beta}\\right) = 1 \\implies \\pi_A \\left(\\frac{\\beta + \\alpha}{\\beta}\\right) = 1\n$$\nThis yields $\\pi_A = \\frac{\\beta}{\\alpha+\\beta}$.\nConsequently, $\\pi_B = 1 - \\pi_A = 1 - \\frac{\\beta}{\\alpha+\\beta} = \\frac{\\alpha}{\\alpha+\\beta}$.\nThe stationary distribution is therefore $\\pi = \\left( \\frac{\\beta}{\\alpha+\\beta}, \\frac{\\alpha}{\\alpha+\\beta} \\right)$.\n\nNext, we check the detailed balance condition, $\\pi_i P_{ij} = \\pi_j P_{ji}$. The non-trivial case to check is for $i=A$ and $j=B$. We need to verify if $\\pi_A P_{AB} = \\pi_B P_{BA}$.\nThe transition probabilities are given as $P_{AB} = \\alpha$ and $P_{BA} = \\beta$.\nThe left-hand side is $\\pi_A P_{AB} = \\left(\\frac{\\beta}{\\alpha+\\beta}\\right) \\alpha = \\frac{\\alpha\\beta}{\\alpha+\\beta}$.\nThe right-hand side is $\\pi_B P_{BA} = \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right) \\beta = \\frac{\\alpha\\beta}{\\alpha+\\beta}$.\nSince the two sides are equal, the detailed balance condition is satisfied. This implies the Markov chain is reversible.\nThe consistency with the Boltzmann form $\\pi_i \\propto \\exp(-\\beta_{\\mathrm{th}} E_i)$ requires the ratio of stationary probabilities to match the ratio of Boltzmann factors:\n$$\n\\frac{\\pi_A}{\\pi_B} = \\frac{\\beta/(\\alpha+\\beta)}{\\alpha/(\\alpha+\\beta)} = \\frac{\\beta}{\\alpha}\n$$\nThis must equal $\\frac{\\exp(-\\beta_{\\mathrm{th}} E_A)}{\\exp(-\\beta_{\\mathrm{th}} E_B)} = \\exp(\\beta_{\\mathrm{th}}(E_B - E_A))$. Thus, the model is physically consistent if the transition rates are related to the energy difference via $\\frac{\\beta}{\\alpha} = \\exp(\\beta_{\\mathrm{th}}(E_B - E_A))$.\n\n**2. Eigenvalues and Spectral Gap**\nThe eigenvalues $\\lambda$ of the transition matrix $P$ are the roots of the characteristic equation $\\det(P - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 1-\\alpha-\\lambda & \\alpha \\\\ \\beta & 1-\\beta-\\lambda \\end{pmatrix} = (1-\\alpha-\\lambda)(1-\\beta-\\lambda) - \\alpha\\beta = 0\n$$\nExpanding this gives the quadratic equation:\n$$\n\\lambda^2 - (2-\\alpha-\\beta)\\lambda + (1-\\alpha)(1-\\beta) - \\alpha\\beta = 0\n$$\n$$\n\\lambda^2 - (2-\\alpha-\\beta)\\lambda + (1-\\alpha-\\beta+\\alpha\\beta) - \\alpha\\beta = 0\n$$\n$$\n\\lambda^2 - (2-\\alpha-\\beta)\\lambda + (1-\\alpha-\\beta) = 0\n$$\nFor any stochastic matrix, one eigenvalue is always $\\lambda_1 = 1$. We can verify this: $1^2 - (2-\\alpha-\\beta)(1) + (1-\\alpha-\\beta) = 1-2+\\alpha+\\beta+1-\\alpha-\\beta=0$.\nThe sum of the eigenvalues equals the trace of the matrix, $\\mathrm{Tr}(P) = (1-\\alpha) + (1-\\beta) = 2-\\alpha-\\beta$.\nTherefore, $\\lambda_1 + \\lambda_2 = 1 + \\lambda_2 = 2-\\alpha-\\beta$, which gives the second eigenvalue $\\lambda_2 = 1-\\alpha-\\beta$.\nSince $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$, we have $0 < \\alpha+\\beta < 2$, which implies $-1 < 1-\\alpha-\\beta < 1$. Thus, $|\\lambda_2| < 1$.\n\nThe spectral gap $g$ is defined as $g = 1 - \\sup\\{|\\lambda| : \\lambda \\in \\mathrm{spec}(P), \\lambda \\neq 1\\}$.\nHere, the only eigenvalue other than $1$ is $\\lambda_2 = 1-\\alpha-\\beta$. So, $\\sup\\{|\\lambda| : \\lambda \\neq 1\\} = |\\lambda_2| = |1-\\alpha-\\beta|$.\nThe spectral gap is $g = 1 - |1-\\alpha-\\beta|$.\n\n**3. Autocorrelation Function and Integrated Autocorrelation Time**\nThe normalized autocorrelation function is defined as $\\rho(t) = \\mathbb{E}_{\\pi}[h(X_0) h(X_t)] / \\mathbb{E}_{\\pi}[h(X_0)^2]$. The term $\\mathbb{E}_{\\pi}[h(X_0) h(X_t)]$ can be expressed using the transition operator. Let $h$ be a column vector representing the function $h$ on the state space, $h = \\begin{pmatrix} h(A) \\\\ h(B) \\end{pmatrix}$. The expected value of $h$ at time $t$ given an initial state $i$ is $(P^t h)_i$. The stationary expectation is then:\n$\\mathbb{E}_{\\pi}[h(X_0) h(X_t)] = \\sum_{i \\in \\{A,B\\}} \\pi_i h(i) (P^t h)_i$. This is the inner product $\\langle h, P^t h \\rangle_{\\pi}$.\n\nThe operator $P$ is reversible with respect to $\\pi$. The space of functions on $\\{A,B\\}$ is two-dimensional. It is spanned by the right eigenvectors of $P$. The eigenvector for $\\lambda_1=1$ is a constant vector, $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The centered observable $h$ is defined such that its stationary expectation is zero: $\\mathbb{E}_{\\pi}[h] = \\sum_i \\pi_i h(i) = \\langle \\mathbf{1}, h \\rangle_{\\pi} = 0$. This means $h$ is orthogonal to the constant eigenvector $v_1$ in the inner product weighted by $\\pi$.\n\nFor a two-state system, the space of functions orthogonal to the constants is one-dimensional. This space is spanned by the second eigenvector, $v_2$, corresponding to $\\lambda_2$. Therefore, the function $h$ must be an eigenvector of $P$ with eigenvalue $\\lambda_2$.\nThis implies that applying the transition operator $P$ to $h$ scales $h$ by $\\lambda_2$, i.e., $Ph = \\lambda_2 h$. By extension, $P^t h = \\lambda_2^t h$.\n\nNow we can compute the numerator of $\\rho(t)$:\n$$\n\\mathbb{E}_{\\pi}[h(X_0) h(X_t)] = \\langle h, P^t h \\rangle_{\\pi} = \\langle h, \\lambda_2^t h \\rangle_{\\pi} = \\lambda_2^t \\langle h, h \\rangle_{\\pi} = \\lambda_2^t \\mathbb{E}_{\\pi}[h^2]\n$$\nSubstituting this into the definition of $\\rho(t)$:\n$$\n\\rho(t) = \\frac{\\lambda_2^t \\mathbb{E}_{\\pi}[h^2]}{\\mathbb{E}_{\\pi}[h^2]} = \\lambda_2^t = (1-\\alpha-\\beta)^t\n$$\nThe variance $\\mathbb{E}_{\\pi}[h^2]$ is non-zero because $E_A \\neq E_B$, so this cancellation is valid.\n\nFinally, we compute the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ using the provided formula:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{t=1}^{\\infty} \\rho(t) = 1 + 2 \\sum_{t=1}^{\\infty} (1-\\alpha-\\beta)^t\n$$\nThe summation is a geometric series with ratio $r = 1-\\alpha-\\beta$. Since $|\\lambda_2| = |r| < 1$, the series converges. The sum of a geometric series is $\\sum_{k=1}^{\\infty} r^k = \\frac{r}{1-r}$.\n$$\n\\sum_{t=1}^{\\infty} (1-\\alpha-\\beta)^t = \\frac{1-\\alpha-\\beta}{1 - (1-\\alpha-\\beta)} = \\frac{1-\\alpha-\\beta}{\\alpha+\\beta}\n$$\nSubstituting this sum back into the expression for $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\left( \\frac{1-\\alpha-\\beta}{\\alpha+\\beta} \\right) = \\frac{\\alpha+\\beta}{\\alpha+\\beta} + \\frac{2 - 2\\alpha - 2\\beta}{\\alpha+\\beta}\n$$\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\alpha+\\beta + 2 - 2\\alpha - 2\\beta}{\\alpha+\\beta} = \\frac{2 - \\alpha - \\beta}{\\alpha+\\beta}\n$$\nThis gives the closed-form expression for the integrated autocorrelation time in terms of $\\alpha$ and $\\beta$.\nThis expression can also be written in terms of the second eigenvalue $\\lambda_2 = 1-\\alpha-\\beta$ as $\\tau_{\\mathrm{int}} = \\frac{1+\\lambda_2}{1-\\lambda_2}$.",
            "answer": "$$\n\\boxed{\\frac{2 - \\alpha - \\beta}{\\alpha + \\beta}}\n$$"
        },
        {
            "introduction": "Real-world simulation data is finite, meaning our empirical models often exhibit small violations of detailed balance. This final practice addresses this challenge head-on by asking you to derive and apply a projection method that finds the 'closest' reversible model to a given empirical one. You will then use this physically consistent model to compute the asymptotic variance of a material property, linking the mathematical fix directly to the statistical quality of your simulation results. ",
            "id": "3823192",
            "problem": "In a multiscale materials simulation, coarse-grained mesoscale states are used to approximate transitions between local microstructural motifs during high-temperature annealing, where the equilibrium occupancy of the motifs is approximately uniform. Consider a discrete-time Markov chain on the state space $\\{1,2,3\\}$ with an empirically obtained transition kernel $P$ that is not reversible. The target stationary distribution is the uniform distribution $\\pi(1)=\\pi(2)=\\pi(3)=\\frac{1}{3}$, consistent with the high-temperature limit. The given kernel is\n$$\nP \\;=\\;\n\\begin{pmatrix}\n0.2 & 0.5 & 0.3 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.5 & 0.2 & 0.3\n\\end{pmatrix}.\n$$\nTo enable unbiased equilibrium sampling of material properties with respect to $\\pi$ using Markov Chain Monte Carlo (MCMC), the kernel must satisfy detailed balance. Starting from the fundamental definitions of a reversible Markov chain and the detailed balance condition, and using a least-squares variational principle on the space of transition kernels that preserve stochasticity and $\\pi$ as stationary, formulate a projection that minimally adjusts $P$ to a $\\pi$-reversible kernel $\\widetilde{P}$ in the weighted Frobenius sense. Derive the analytic form of this projection and compute the explicit $\\widetilde{P}$ for the data above.\n\nNext, consider a dimensionless normalized material property $f$ defined on the three states by $f(1)=3$, $f(2)=1$, and $f(3)=-2$. Define the centered observable $g$ by $g(i)=f(i)-\\sum_{j}\\pi(j)f(j)$. Using only core facts about reversible Markov operators on $L^{2}(\\pi)$ and their spectral representation, derive an expression for the large-sample asymptotic variance constant $v(f)$, defined as the limit of $N \\,\\mathrm{Var}\\!\\left(\\frac{1}{N}\\sum_{t=1}^{N} f(X_{t})\\right)$ for the Markov chain with transition kernel $\\widetilde{P}$ started in stationarity. Evaluate $v(f)$ exactly for the $\\widetilde{P}$ found above and the provided $f$, and express your final answer as a single exact rational number. No rounding is required, and the final answer is dimensionless.",
            "solution": "The problem is divided into two parts. The first part requires the derivation of a projection of a given transition kernel $P$ onto the space of reversible kernels with a specific stationary distribution $\\pi$. The second part asks for the calculation of the asymptotic variance of a material property using the projected kernel.\n\nFirst, we validate the problem. All given information is scientifically sound and mathematically consistent. The state space is $\\{1, 2, 3\\}$. The given transition kernel is\n$$\nP = \\begin{pmatrix} 0.2 & 0.5 & 0.3 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.5 & 0.2 & 0.3 \\end{pmatrix}\n$$\nThe rows of $P$ sum to $1$, so it is a valid stochastic matrix. The target stationary distribution is the uniform distribution $\\pi(i) = 1/3$ for $i \\in \\{1, 2, 3\\}$. The kernel $P$ is not reversible with respect to $\\pi$, as this would require $P$ to be symmetric (since $\\pi$ is uniform), which it is not (e.g., $P_{12}=0.5 \\neq P_{21}=0.3$). The problem is well-posed and internally consistent.\n\n**Part 1: Projection of the Transition Kernel**\n\nWe are tasked with finding a transition kernel $\\widetilde{P}$ that is \"minimally\" different from $P$ in the weighted Frobenius sense, subject to certain constraints. The objective is to minimize the function:\n$$\nF(\\widetilde{P}) = \\| \\widetilde{P} - P \\|^2_{F,\\pi} = \\sum_{i=1}^3 \\sum_{j=1}^3 \\pi(i) (\\widetilde{P}_{ij} - P_{ij})^2\n$$\nThe kernel $\\widetilde{P}$ must satisfy the following constraints for all $i,j \\in \\{1,2,3\\}$:\n1.  Non-negativity: $\\widetilde{P}_{ij} \\ge 0$.\n2.  Stochasticity: $\\sum_j \\widetilde{P}_{ij} = 1$.\n3.  Detailed balance (reversibility): $\\pi(i) \\widetilde{P}_{ij} = \\pi(j) \\widetilde{P}_{ji}$.\n\nThe detailed balance and stochasticity conditions together ensure that $\\pi$ is the stationary distribution for $\\widetilde{P}$:\n$$\n(\\pi \\widetilde{P})_j = \\sum_i \\pi(i) \\widetilde{P}_{ij} = \\sum_i \\pi(j) \\widetilde{P}_{ji} = \\pi(j) \\sum_i \\widetilde{P}_{ji} = \\pi(j) \\cdot 1 = \\pi(j)\n$$\nSo we do not need to enforce the stationary distribution condition separately.\n\nThe objective function can be separated for off-diagonal ($i \\neq j$) and diagonal ($i=j$) terms. The overall minimization can be achieved by first minimizing with respect to the off-diagonal elements under the detailed balance constraint, and then setting the diagonal elements to satisfy the stochasticity constraint.\n\nConsider the terms in the sum for a single pair $(i, j)$ with $i \\neq j$:\n$$\n\\pi(i)(\\widetilde{P}_{ij} - P_{ij})^2 + \\pi(j)(\\widetilde{P}_{ji} - P_{ji})^2\n$$\nThe detailed balance constraint for this pair is $\\pi(i)\\widetilde{P}_{ij} = \\pi(j)\\widetilde{P}_{ji}$. Let us define this common value as $s_{ij}$. Then $\\widetilde{P}_{ij} = s_{ij}/\\pi(i)$ and $\\widetilde{P}_{ji} = s_{ij}/\\pi(j)$. Substituting this into the expression, we need to minimize the following function with respect to $s_{ij}$:\n$$\nL(s_{ij}) = \\pi(i)\\left(\\frac{s_{ij}}{\\pi(i)} - P_{ij}\\right)^2 + \\pi(j)\\left(\\frac{s_{ij}}{\\pi(j)} - P_{ji}\\right)^2 = \\frac{1}{\\pi(i)}(s_{ij} - \\pi(i)P_{ij})^2 + \\frac{1}{\\pi(j)}(s_{ij} - \\pi(j)P_{ji})^2\n$$\nTo find the minimum, we take the derivative with respect to $s_{ij}$ and set it to zero:\n$$\n\\frac{dL}{ds_{ij}} = \\frac{2}{\\pi(i)}(s_{ij} - \\pi(i)P_{ij}) + \\frac{2}{\\pi(j)}(s_{ij} - \\pi(j)P_{ji}) = 0\n$$\n$$\ns_{ij} \\left( \\frac{1}{\\pi(i)} + \\frac{1}{\\pi(j)} \\right) = P_{ij} + P_{ji}\n$$\n$$\ns_{ij} \\left( \\frac{\\pi(i)+\\pi(j)}{\\pi(i)\\pi(j)} \\right) = P_{ij} + P_{ji}\n$$\nSolving for $s_{ij}$ gives:\n$$\ns_{ij} = \\frac{\\pi(i)\\pi(j)}{\\pi(i)+\\pi(j)}(P_{ij} + P_{ji})\n$$\nFrom this, we find the off-diagonal entries of $\\widetilde{P}$:\n$$\n\\widetilde{P}_{ij} = \\frac{s_{ij}}{\\pi(i)} = \\frac{\\pi(j)}{\\pi(i)+\\pi(j)}(P_{ij} + P_{ji}) \\quad \\text{for } i \\neq j\n$$\nThe diagonal entries $\\widetilde{P}_{ii}$ are then determined by the stochasticity constraint:\n$$\n\\widetilde{P}_{ii} = 1 - \\sum_{j \\neq i} \\widetilde{P}_{ij}\n$$\nThis procedure defines the unique projection $\\widetilde{P}$. Since all $P_{ij}$ are non-negative, the off-diagonal $\\widetilde{P}_{ij}$ will also be non-negative. We must check if $\\widetilde{P}_{ii} \\ge 0$ after calculation.\n\nFor the given problem, $\\pi(i) = 1/3$ for all $i$. The formula for the off-diagonal elements simplifies significantly:\n$$\n\\widetilde{P}_{ij} = \\frac{1/3}{1/3+1/3}(P_{ij} + P_{ji}) = \\frac{1}{2}(P_{ij} + P_{ji}) \\quad \\text{for } i \\neq j\n$$\nSo, the off-diagonal entries of $\\widetilde{P}$ are the average of the corresponding entries of $P$ and $P^T$.\nUsing the given matrix $P$:\n$$\n\\widetilde{P}_{12} = \\widetilde{P}_{21} = \\frac{1}{2}(P_{12} + P_{21}) = \\frac{1}{2}(0.5 + 0.3) = 0.4\n$$\n$$\n\\widetilde{P}_{13} = \\widetilde{P}_{31} = \\frac{1}{2}(P_{13} + P_{31}) = \\frac{1}{2}(0.3 + 0.5) = 0.4\n$$\n$$\n\\widetilde{P}_{23} = \\widetilde{P}_{32} = \\frac{1}{2}(P_{23} + P_{32}) = \\frac{1}{2}(0.3 + 0.2) = 0.25\n$$\nNow we compute the diagonal elements:\n$$\n\\widetilde{P}_{11} = 1 - (\\widetilde{P}_{12} + \\widetilde{P}_{13}) = 1 - (0.4 + 0.4) = 0.2\n$$\n$$\n\\widetilde{P}_{22} = 1 - (\\widetilde{P}_{21} + \\widetilde{P}_{23}) = 1 - (0.4 + 0.25) = 0.35\n$$\n$$\n\\widetilde{P}_{33} = 1 - (\\widetilde{P}_{31} + \\widetilde{P}_{32}) = 1 - (0.4 + 0.25) = 0.35\n$$\nAll entries are non-negative, so the kernel is valid. The resulting reversible kernel is:\n$$\n\\widetilde{P} = \\begin{pmatrix} 0.2 & 0.4 & 0.4 \\\\ 0.4 & 0.35 & 0.25 \\\\ 0.4 & 0.25 & 0.35 \\end{pmatrix}\n$$\n\n**Part 2: Asymptotic Variance Calculation**\n\nThe large-sample asymptotic variance constant $v(f)$ for an observable $f$ is given by the Green-Kubo formula for a stationary, reversible Markov chain:\n$$\nv(f) = \\sum_{k=-\\infty}^{\\infty} \\mathrm{Cov}_{\\pi}(f(X_0), f(X_k)) = \\mathrm{Var}_{\\pi}(f) + 2 \\sum_{k=1}^{\\infty} \\mathrm{Cov}_{\\pi}(f(X_0), f(X_k))\n$$\nIt is more convenient to work with the centered observable $g(i) = f(i) - E_{\\pi}[f]$.\nGiven $f(1)=3, f(2)=1, f(3)=-2$ and $\\pi(i)=1/3$:\n$$\nE_{\\pi}[f] = \\sum_j \\pi(j)f(j) = \\frac{1}{3}(3+1-2) = \\frac{2}{3}\n$$\nThe centered observable is $g = (f(1)-\\frac{2}{3}, f(2)-\\frac{2}{3}, f(3)-\\frac{2}{3}) = (\\frac{7}{3}, \\frac{1}{3}, -\\frac{8}{3})$.\nThe variance formula becomes $v(f) = \\langle g, g \\rangle_{\\pi} + 2 \\sum_{k=1}^{\\infty} \\langle g, \\widetilde{P}^k g \\rangle_{\\pi}$, where the inner product is $\\langle u, v \\rangle_{\\pi} = \\sum_i u(i)v(i)\\pi(i)$.\n\nLet $\\widetilde{P}$ be considered an operator on the space $L^2(\\pi)$ of functions on $\\{1,2,3\\}$. Since $\\widetilde{P}$ is reversible with respect to $\\pi$, it is self-adjoint on $L^2(\\pi)$. It has a spectral decomposition with real eigenvalues $1=\\lambda_1 \\ge \\lambda_2 \\ge \\dots$ and a corresponding orthonormal basis of eigenvectors $\\{u_i\\}$. Since $g$ is centered, $E_{\\pi}[g]=0$, which means $g$ is orthogonal to the eigenvector $u_1$ corresponding to $\\lambda_1=1$ (the constant function).\nThus, we can write $g = \\sum_{i \\ge 2} c_i u_i$ where $c_i = \\langle g, u_i \\rangle_{\\pi}$.\nThen $\\widetilde{P}^k g = \\sum_{i \\ge 2} \\lambda_i^k c_i u_i$.\nSubstituting this into the formula for $v(f)$:\n$$\nv(f) = \\langle \\sum_i c_i u_i, \\sum_j c_j u_j \\rangle_{\\pi} + 2 \\sum_{k=1}^{\\infty} \\langle \\sum_i c_i u_i, \\sum_j \\lambda_j^k c_j u_j \\rangle_{\\pi}\n$$\n$$\nv(f) = \\sum_i c_i^2 + 2 \\sum_{k=1}^{\\infty} \\sum_i \\lambda_i^k c_i^2 = \\sum_i c_i^2 \\left(1 + 2\\sum_{k=1}^{\\infty} \\lambda_i^k \\right)\n$$\nAssuming $|\\lambda_i|<1$ for $i \\ge 2$, the geometric series converges: $\\sum_{k=1}^{\\infty}\\lambda_i^k = \\frac{\\lambda_i}{1-\\lambda_i}$.\n$$\nv(f) = \\sum_{i \\ge 2} c_i^2 \\left(1 + \\frac{2\\lambda_i}{1-\\lambda_i}\\right) = \\sum_{i \\ge 2} c_i^2 \\left(\\frac{1-\\lambda_i+2\\lambda_i}{1-\\lambda_i}\\right) = \\sum_{i \\ge 2} |\\langle g, u_i \\rangle_{\\pi}|^2 \\frac{1+\\lambda_i}{1-\\lambda_i}\n$$\nWe need to find the eigenvalues and eigenvectors of $\\widetilde{P}$. Since $\\pi$ is uniform, $\\widetilde{P}$ is a symmetric matrix, and its eigenvectors are orthogonal with respect to the standard dot product. The $L^2(\\pi)$ inner product is just $1/3$ times the standard dot product.\nThe eigenvalues (other than $\\lambda_1=1$) and eigenvectors of $\\widetilde{P}$ are:\n$\\lambda_2=-0.2$: eigenvector $u_2 = (-2, 1, 1)^T$.\n$\\lambda_3=0.1$: eigenvector $u_3 = (0, 1, -1)^T$.\nThese eigenvectors are orthogonal as required. Let's find the squared norms in $L^2(\\pi)$:\n$$\n\\|u_2\\|^2_{\\pi} = \\frac{1}{3}((-2)^2+1^2+1^2) = \\frac{6}{3}=2\n$$\n$$\n\\|u_3\\|^2_{\\pi} = \\frac{1}{3}(0^2+1^2+(-1)^2) = \\frac{2}{3}\n$$\nThe orthonormal eigenvectors are $\\hat{u}_2=u_2/\\sqrt{2}$ and $\\hat{u}_3=u_3/\\sqrt{2/3}$.\nThe coefficients are $c_i^2=|\\langle g, \\hat{u}_i \\rangle_{\\pi}|^2$.\n$$\n\\langle g, u_2 \\rangle_{\\pi} = \\frac{1}{3} \\left( \\frac{7}{3}(-2)+\\frac{1}{3}(1)-\\frac{8}{3}(1) \\right) = \\frac{1}{9}(-14+1-8) = \\frac{-21}{9}=-\\frac{7}{3}\n$$\n$$\nc_2^2 = |\\langle g, \\hat{u}_2 \\rangle_{\\pi}|^2 = \\frac{|\\langle g, u_2 \\rangle_{\\pi}|^2}{\\|u_2\\|^2_{\\pi}} = \\frac{(-7/3)^2}{2} = \\frac{49/9}{2} = \\frac{49}{18}\n$$\n$$\n\\langle g, u_3 \\rangle_{\\pi} = \\frac{1}{3} \\left( \\frac{7}{3}(0)+\\frac{1}{3}(1)-\\frac{8}{3}(-1) \\right) = \\frac{1}{9}(1+8) = \\frac{9}{9}=1\n$$\n$$\nc_3^2 = |\\langle g, \\hat{u}_3 \\rangle_{\\pi}|^2 = \\frac{|\\langle g, u_3 \\rangle_{\\pi}|^2}{\\|u_3\\|^2_{\\pi}} = \\frac{1^2}{2/3} = \\frac{3}{2}\n$$\nNow we compute $v(f)$ using the spectral expression:\n$$\nv(f) = c_2^2 \\frac{1+\\lambda_2}{1-\\lambda_2} + c_3^2 \\frac{1+\\lambda_3}{1-\\lambda_3}\n$$\nWith $\\lambda_2 = -0.2 = -1/5$ and $\\lambda_3 = 0.1 = 1/10$:\n$$\n\\frac{1+\\lambda_2}{1-\\lambda_2} = \\frac{1-1/5}{1+1/5} = \\frac{4/5}{6/5} = \\frac{2}{3}\n$$\n$$\n\\frac{1+\\lambda_3}{1-\\lambda_3} = \\frac{1+1/10}{1-1/10} = \\frac{11/10}{9/10} = \\frac{11}{9}\n$$\nSubstituting all the values:\n$$\nv(f) = \\frac{49}{18} \\cdot \\frac{2}{3} + \\frac{3}{2} \\cdot \\frac{11}{9} = \\frac{49}{27} + \\frac{11}{6}\n$$\nTo sum these fractions, we find a common denominator, which is $54$:\n$$\nv(f) = \\frac{49 \\cdot 2}{27 \\cdot 2} + \\frac{11 \\cdot 9}{6 \\cdot 9} = \\frac{98}{54} + \\frac{99}{54} = \\frac{98+99}{54} = \\frac{197}{54}\n$$\nThe final result is an exact rational number as requested.",
            "answer": "$$\\boxed{\\frac{197}{54}}$$"
        }
    ]
}