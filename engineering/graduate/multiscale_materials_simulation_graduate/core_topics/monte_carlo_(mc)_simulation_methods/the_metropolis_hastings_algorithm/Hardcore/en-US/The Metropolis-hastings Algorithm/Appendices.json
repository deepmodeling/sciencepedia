{
    "hands_on_practices": [
        {
            "introduction": "The Metropolis-Hastings algorithm's power lies in its acceptance rule, which ensures the resulting Markov chain correctly samples from the desired target distribution, $\\pi$. This first exercise provides a foundational check on your understanding of this core mechanism . By calculating the acceptance probability in a simple, discrete system, you will see how the algorithm balances the intrinsic probability of states under $\\pi$ against the probabilities of proposing moves between them.",
            "id": "1962671",
            "problem": "A computational simulation uses the Metropolis-Hastings algorithm to explore the conformational states of a protein. The model simplifies the protein's structure into three possible discrete states: State 1, State 2, and State 3. The goal of the simulation is to generate samples from a target stationary distribution, $\\pi$, which represents the equilibrium probability of finding the protein in each state. This target distribution is given by:\n$\\pi(\\text{State 1}) = 0.1$\n$\\pi(\\text{State 2}) = 0.3$\n$\\pi(\\text{State 3}) = 0.6$\n\nThe algorithm proceeds by proposing moves from a current state $i$ to a new state $j$ according to a proposal distribution $q(j|i)$. The proposal mechanism is defined as follows:\n- From State 1, a move to State 2 is proposed with probability $0.2$, and a move to State 3 is proposed with probability $0.8$.\n- From State 2, a move to State 1 is proposed with probability $0.5$, and a move to State 3 is proposed with probability $0.5$.\n- From State 3, a move to State 1 is proposed with probability $0.1$, and a move to State 2 is proposed with probability $0.9$.\n\nSuppose the simulation is currently in State 1 and a move to State 3 is proposed. Calculate the acceptance probability for this specific transition. Express your final answer as a simplified fraction.",
            "solution": "In the Metropolis-Hastings algorithm, the acceptance probability for a proposed move from a current state $i$ to a new state $j$ is\n$$\n\\alpha_{i \\to j} = \\min\\left(1, \\frac{\\pi(j)\\,q(i \\mid j)}{\\pi(i)\\,q(j \\mid i)}\\right).\n$$\nFor the proposed transition from State 1 to State 3, use the given target probabilities and proposal probabilities. Converting the given decimals to exact fractions,\n$$\n\\pi(\\text{State 3}) = \\frac{3}{5}, \\quad \\pi(\\text{State 1}) = \\frac{1}{10}, \\quad q(3 \\mid 1) = \\frac{4}{5}, \\quad q(1 \\mid 3) = \\frac{1}{10}.\n$$\nSubstitute into the acceptance ratio:\n$$\n\\frac{\\pi(3)\\,q(1 \\mid 3)}{\\pi(1)\\,q(3 \\mid 1)} = \\frac{\\left(\\frac{3}{5}\\right)\\left(\\frac{1}{10}\\right)}{\\left(\\frac{1}{10}\\right)\\left(\\frac{4}{5}\\right)} = \\frac{\\frac{3}{50}}{\\frac{4}{50}} = \\frac{3}{4}.\n$$\nTherefore,\n$$\n\\alpha_{1 \\to 3} = \\min\\left(1, \\frac{3}{4}\\right) = \\frac{3}{4}.\n$$",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "A correctly implemented algorithm is not necessarily an efficient one, as its performance hinges on the choice of the proposal distribution. This process, often called 'tuning,' is a crucial practical skill. This exercise  presents a common diagnostic scenario where the sampler's acceptance rate is extremely low, prompting you to deduce the underlying issue. Understanding this scenario, along with its counterpart where the acceptance rate is nearly 100% , is key to mastering the trade-offs involved in setting the proposal step size for efficient exploration.",
            "id": "1962620",
            "problem": "A statistician, Jordan, is using the Metropolis-Hastings algorithm to generate samples from a one-dimensional posterior probability distribution, $p(\\theta | D)$, where $\\theta$ is a model parameter and $D$ represents observed data. The target distribution $p(\\theta | D)$ is known to be unimodal (has a single peak) and is approximately symmetric.\n\nJordan implements a Random-Walk Metropolis sampler. At each step $i$ in the Markov chain, given the current state $\\theta_i$, a candidate state $\\theta'$ is proposed from a normal distribution centered at the current state: $\\theta' \\sim \\mathcal{N}(\\theta_i, \\sigma_p^2)$. The parameter $\\sigma_p^2$ is the variance of this proposal distribution, which Jordan must set.\n\nAfter running the sampler for 100,000 iterations, Jordan observes that the chain has barely moved from its starting position, and the overall acceptance rate is calculated to be less than 0.1%. This indicates that nearly all proposed moves are being rejected.\n\nWhich of the following statements best explains the likely issue with Jordan's choice for the proposal distribution?\n\nA. The proposal variance $\\sigma_p^2$ is too large.\n\nB. The proposal variance $\\sigma_p^2$ is too small.\n\nC. The proposal distribution is not symmetric, i.e., $q(\\theta'|\\theta_i) \\neq q(\\theta_i|\\theta')$.\n\nD. The mean of the proposal distribution is fixed and not centered at the current state $\\theta_i$.\n\nE. The target distribution $p(\\theta|D)$ is not normalized, meaning its integral over all $\\theta$ is not equal to 1.",
            "solution": "We consider a Random-Walk Metropolis algorithm with proposal $q(\\theta'|\\theta_{i}) = \\mathcal{N}(\\theta_{i}, \\sigma_{p}^{2})$. The Metropolis-Hastings acceptance probability is\n$$\n\\alpha(\\theta_{i}, \\theta') = \\min\\left(1, \\frac{p(\\theta'|D)}{p(\\theta_{i}|D)} \\cdot \\frac{q(\\theta_{i}|\\theta')}{q(\\theta'|\\theta_{i})}\\right).\n$$\nFor a Gaussian random-walk centered at the current state, the proposal is symmetric:\n$$\nq(\\theta'|\\theta_{i}) = q(\\theta_{i}|\\theta') \\quad \\text{because} \\quad q(\\theta'|\\theta_{i}) \\propto \\exp\\left(-\\frac{(\\theta'-\\theta_{i})^{2}}{2\\sigma_{p}^{2}}\\right),\n$$\nwhich depends only on $(\\theta'-\\theta_{i})^{2}$. Hence\n$$\n\\alpha(\\theta_{i}, \\theta') = \\min\\left(1, \\frac{p(\\theta'|D)}{p(\\theta_{i}|D)}\\right).\n$$\nNow assume $p(\\theta|D)$ is unimodal and approximately symmetric, and suppose the chain is near the high-density region (after some initial iterations or even from the start). If the proposal variance $\\sigma_{p}^{2}$ is too large, the proposed increment $\\delta = \\theta' - \\theta_{i}$ has typical magnitude of order $\\sigma_{p}$, so most proposals fall far into regions where $p(\\theta'|D) \\ll p(\\theta_{i}|D)$. For an illustrative approximation, if the target is roughly Gaussian with variance $\\sigma_{t}^{2}$ around its mode, then\n$$\n\\frac{p(\\theta'|D)}{p(\\theta_{i}|D)} \\approx \\exp\\left(-\\frac{\\delta^{2}}{2\\sigma_{t}^{2}}\\right).\n$$\nWhen $\\sigma_{p}^{2} \\gg \\sigma_{t}^{2}$, typical $\\delta^{2}$ is of order $\\sigma_{p}^{2}$, making the exponent large and negative, so $\\alpha(\\theta_{i}, \\theta')$ is near zero for most proposals. This yields an extremely low acceptance rate and a chain that rarely moves, exactly as observed.\n\nIn contrast, if $\\sigma_{p}^{2}$ were too small, then $\\delta$ would be very small, $p(\\theta'|D) \\approx p(\\theta_{i}|D)$, and the acceptance probability would be close to $1$, not near zero. Therefore an acceptance rate below $0.001$ in decimal is not consistent with too small a variance.\n\nOption C is incorrect because the Gaussian random-walk proposal is symmetric. Option D is incorrect because the mean is, by construction, centered at the current state $\\theta_{i}$. Option E is not an issue for Metropolis-Hastings because the normalization constant cancels in the ratio:\n$$\n\\frac{p(\\theta'|D)}{p(\\theta_{i}|D)} = \\frac{\\tilde{p}(\\theta')/Z}{\\tilde{p}(\\theta_{i})/Z} = \\frac{\\tilde{p}(\\theta')}{\\tilde{p}(\\theta_{i})}.\n$$\nThus, the likely issue is that the proposal variance is too large, causing proposals to be made far from the typical set and to be rejected with very high probability.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "In theory, an irreducible and aperiodic Markov chain will eventually explore its entire state space. In practice, 'eventually' can be longer than the age of the universe, especially for target distributions with multiple, well-separated modes. This exercise  explores this critical distinction between theoretical ergodicity and practical mixing by examining a sampler on a disconnected state space, a scenario that serves as a potent analogy for simulating physical systems with high energy barriers between metastable states.",
            "id": "3252214",
            "problem": "Consider a one-dimensional target distribution with unnormalized density given by $\\tilde{\\pi}(x) = \\exp(-x^2)\\,\\mathbf{1}_{\\{|x|\\ge 1\\}}$ on $\\mathbb{R}$, where $\\mathbf{1}_{\\{|x|\\ge 1\\}}$ is the indicator function that is $1$ if $|x|\\ge 1$ and $0$ otherwise. Let $\\pi(x) = \\tilde{\\pi}(x)/Z$ denote the normalized target distribution, where $Z$ is the appropriate normalizing constant. The support of $\\pi$ is the disconnected set $S = (-\\infty,-1]\\cup[1,\\infty)$. We run a Markov chain Monte Carlo (MCMC) method using the Metropolis-Hastings (MH) algorithm with the random-walk Gaussian proposal $q(y\\mid x)$ equal to a normal distribution centered at the current state, specifically $q(y\\mid x) = \\mathcal{N}(x,\\sigma^2)$ with $\\sigma0$. The algorithm uses the usual MH accept-reject mechanism to ensure $\\pi$ is invariant.\n\nAnalyze the behavior of this sampler in the presence of the disconnected support and select all statements that are correct:\n\nA. For any $\\sigma0$, the resulting Markov chain on $S$ is irreducible and aperiodic, and hence converges in distribution to $\\pi$.\n\nB. When $\\sigma$ is sufficiently small, the probability of proposing a jump from the left component $\\{x\\le -1\\}$ to the right component $\\{x\\ge 1\\}$ is exponentially small in $1/\\sigma^2$, which implies exponentially large expected transition times between components and severe finite-time bias for estimators that depend on both components.\n\nC. If one instead uses an independence MH sampler with proposal $q(y)=\\mathcal{N}(0,\\tau^2)$, the chain is uniformly ergodic for all $\\tau0$ because $q$ has positive density everywhere on $\\mathbb{R}$.\n\nD. Any proposal landing in the gap region $\\{-1x1\\}$ is accepted with probability $1$ because the MH acceptance ratio uses unnormalized densities.\n\nE. The random-walk MH chain is reducible because the target support $S$ is disconnected.\n\nSelect all correct options.",
            "solution": "The problem statement is a valid exercise in analyzing the properties of the Metropolis-Hastings (MH) algorithm. It is scientifically grounded, well-posed, objective, and contains all necessary information. We may proceed with the solution.\n\nThe unnormalized target density is given by $\\tilde{\\pi}(x) = \\exp(-x^2)\\,\\mathbf{1}_{\\{|x|\\ge 1\\}}$. The support of this distribution is the set $S = (-\\infty,-1]\\cup[1,\\infty)$. The proposal distribution is a random-walk Gaussian, $q(y\\mid x) = \\mathcal{N}(x,\\sigma^2)$, which is a normal distribution with mean $x$ and variance $\\sigma^2$. The density function for the proposal is $q(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right)$. Since this function is symmetric in $x$ and $y$, i.e., $q(y|x) = q(x|y)$, the Metropolis-Hastings acceptance probability simplifies to the Metropolis form:\n$$ \\alpha(y|x) = \\min\\left(1, \\frac{\\tilde{\\pi}(y)q(x|y)}{\\tilde{\\pi}(x)q(y|x)}\\right) = \\min\\left(1, \\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(x)}\\right) $$\nFor a proposed move from a current state $x \\in S$ to a new state $y$, the acceptance probability is:\n$$ \\alpha(y|x) = \\begin{cases} \\min\\left(1, \\frac{\\exp(-y^2)}{\\exp(-x^2)}\\right)  \\text{if } |y| \\ge 1 \\\\ 0  \\text{if } |y|  1 \\end{cases} $$\nThe chain moves from $X_t=x$ to $X_{t+1}=y$ with probability $\\alpha(y|x)$ if a state $y$ is proposed. Otherwise, the chain remains at $x$, i.e., $X_{t+1}=x$.\n\nWe now evaluate each statement.\n\n**A. For any $\\sigma0$, the resulting Markov chain on $S$ is irreducible and aperiodic, and hence converges in distribution to $\\pi$.**\n\n*   **Irreducibility**: A Markov chain on a state space $S$ is irreducible if it is possible to go from any state (or non-empty set of states) to any other non-empty set of states. For a continuous state space, this means the transition kernel allows movement between any two regions of positive measure. Let's consider any two states $x \\in S$ and $y \\in S$. The proposal density $q(y|x)$ is a Gaussian density, which is strictly positive for all real numbers $x$ and $y$. Thus, for any $x, y \\in S$, the probability of proposing a move from $x$ to a neighborhood of $y$ is positive. The acceptance probability is $\\alpha(y|x) = \\min(1, \\exp(x^2-y^2))$. Since $x, y \\in S$, $\\tilde{\\pi}(x)  0$ and $\\tilde{\\pi}(y)  0$, which implies $\\alpha(y|x)  0$. Since both the proposal density and the acceptance probability are positive for any pair of states in the support $S$, the chain can transition between any region in $S$ to any other region in $S$. Therefore, the chain is irreducible. The disconnected nature of the support $S$ does not imply reducibility because the proposal distribution can \"jump\" over the gap.\n*   **Aperiodicity**: A chain is aperiodic if it does not get trapped in cycles. A sufficient condition for aperiodicity is that for any state $x$, the probability of staying in that state, $P(X_{t+1}=x|X_t=x)$, is positive. In the MH algorithm, a move can be rejected, causing the chain to stay in the same state. The probability of rejection is $1-\\int_S q(y|x)\\alpha(y|x)dy$. Since the total acceptance probability is less than $1$, the rejection probability is greater than $0$. Thus, $P(X_{t+1}=x|X_t=x)  0$, and the chain is aperiodic.\n*   **Convergence**: An irreducible, aperiodic Markov chain that has $\\pi$ as its invariant distribution is guaranteed to converge to $\\pi$ in distribution. The MH algorithm is constructed specifically to ensure that $\\pi$ is the invariant distribution. Given that the chain is irreducible and aperiodic, it will converge to the target distribution $\\pi(x)$.\n\nTherefore, the statement is **Correct**.\n\n**B. When $\\sigma$ is sufficiently small, the probability of proposing a jump from the left component $\\{x\\le -1\\}$ to the right component $\\{x\\ge 1\\}$ is exponentially small in $1/\\sigma^2$, which implies exponentially large expected transition times between components and severe finite-time bias for estimators that depend on both components.**\n\n*   Let the chain be in the left component, at a state $x \\le -1$. A jump to the right component, $[1, \\infty)$, requires proposing a state $y \\ge 1$. The proposal is drawn from $Y \\sim \\mathcal{N}(x, \\sigma^2)$. The probability of proposing such a jump is $P(Y \\ge 1)$. This probability is maximized for $x$ closest to the gap, i.e., $x = -1$.\n*   For $x=-1$, the probability of proposing a state $y \\ge 1$ is:\n    $$ P(Y \\ge 1) = P\\left(\\frac{Y - (-1)}{\\sigma} \\ge \\frac{1 - (-1)}{\\sigma}\\right) = P\\left(Z \\ge \\frac{2}{\\sigma}\\right) $$\n    where $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable.\n*   For small $\\sigma  0$, the value $z = 2/\\sigma$ is large. The tail probability of a standard normal distribution, $P(Z \\ge z)$, can be approximated for large $z$ using Mill's ratio or other bounds, which show it behaves like $\\frac{1}{z\\sqrt{2\\pi}}\\exp(-z^2/2)$. Substituting $z=2/\\sigma$:\n    $$ P\\left(Z \\ge \\frac{2}{\\sigma}\\right) \\propto \\frac{\\sigma}{2} \\exp\\left(-\\frac{(2/\\sigma)^2}{2}\\right) = \\frac{\\sigma}{2} \\exp\\left(-\\frac{2}{\\sigma^2}\\right) $$\n*   This probability is exponentially small in $1/\\sigma^2$. Since the acceptance probability is at most $1$, the total probability of transitioning between components is also exponentially small.\n*   The expected number of steps to make such a transition is inversely proportional to this probability, and is therefore exponentially large in $1/\\sigma^2$. This means the sampler will mix very slowly between the two components.\n*   If a simulation is run for a finite number of steps that is much smaller than this expected transition time, the chain will likely remain trapped in its starting component. Any statistical estimator that relies on sampling from the entire support (e.g., the mean, which should be $0$ by symmetry) will be severely biased towards the values in the explored component.\n\nTherefore, the statement is **Correct**.\n\n**C. If one instead uses an independence MH sampler with proposal $q(y)=\\mathcal{N}(0,\\tau^2)$, the chain is uniformly ergodic for all $\\tau0$ because $q$ has positive density everywhere on $\\mathbb{R}$.**\n\n*   An independence sampler proposes moves from a distribution $q(y)$ that is independent of the current state $x$. A sufficient condition for uniform ergodicity of an independence sampler is that the ratio of the target density to the proposal density, $w(x) = \\tilde{\\pi}(x)/q(x)$, must be bounded over the support $S$.\n*   Let's compute this ratio for $|x| \\ge 1$:\n    $$ w(x) = \\frac{\\tilde{\\pi}(x)}{q(x)} = \\frac{\\exp(-x^2)}{\\frac{1}{\\sqrt{2\\pi\\tau^2}}\\exp(-x^2/(2\\tau^2))} = \\sqrt{2\\pi\\tau^2} \\exp\\left(-x^2 + \\frac{x^2}{2\\tau^2}\\right) = \\sqrt{2\\pi\\tau^2} \\exp\\left(x^2\\left(\\frac{1}{2\\tau^2} - 1\\right)\\right) $$\n*   For $w(x)$ to be bounded for all $x \\in S$, the term in the exponent cannot be positive. If the coefficient of $x^2$ is positive, $w(x)$ will grow without bound as $|x| \\to \\infty$.\n*   The coefficient is $\\frac{1}{2\\tau^2} - 1$. This is positive if $\\frac{1}{2\\tau^2}  1$, which simplifies to $\\tau^2  1/2$.\n*   When $\\tau^2  1/2$, the ratio $w(x)$ is unbounded on $S$. Therefore, the chain is not uniformly ergodic in this case.\n*   The statement claims uniform ergodicity for all $\\tau  0$. Since we have found a range of $\\tau$ for which this fails, the statement is false. The justification given (\"because $q$ has positive density everywhere on $\\mathbb{R}$\") is a condition for irreducibility, not uniform ergodicity.\n\nTherefore, the statement is **Incorrect**.\n\n**D. Any proposal landing in the gap region $\\{-1x1\\}$ is accepted with probability $1$ because the MH acceptance ratio uses unnormalized densities.**\n\n*   Let the current state be $x \\in S$. A new state $y$ is proposed such that $-1  y  1$.\n*   According to the definition of the unnormalized target density, $\\tilde{\\pi}(y) = \\exp(-y^2) \\cdot \\mathbf{1}_{\\{|y|\\ge 1\\}}$. Since $|y|  1$, the indicator function $\\mathbf{1}_{\\{|y|\\ge 1\\}}$ is $0$. Thus, $\\tilde{\\pi}(y) = 0$.\n*   The current state $x$ is in the support $S$, so $\\tilde{\\pi}(x) = \\exp(-x^2)  0$.\n*   The acceptance probability is $\\alpha(y|x) = \\min\\left(1, \\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(x)}\\right) = \\min\\left(1, \\frac{0}{\\exp(-x^2)}\\right) = \\min(1, 0) = 0$.\n*   A proposal landing in the gap region is always rejected (i.e., accepted with probability $0$), not accepted with probability $1$. The use of unnormalized densities is standard and does not change this outcome.\n\nTherefore, the statement is **Incorrect**.\n\n**E. The random-walk MH chain is reducible because the target support $S$ is disconnected.**\n\n*   This statement claims the chain is reducible. As established in the analysis of statement A, the chain is irreducible.\n*   Reducibility would mean there exists a partition of the state space $S$ into sets from which the chain cannot escape. For example, if the chain could not move from the component $(-\\infty, -1]$ to $[1, \\infty)$.\n*   However, the Gaussian proposal distribution $q(y|x) = \\mathcal{N}(x,\\sigma^2)$ has a density that is positive over all of $\\mathbb{R}$. This means that for any starting point $x \\in (-\\infty, -1]$, there is a non-zero probability density of proposing any state $y \\in [1, \\infty)$.\n*   As shown in the analysis for A, the acceptance probability for such a move, $\\alpha(y|x)$, is also positive.\n*   Since transitions between the two disconnected components of the support are possible (with positive probability), the chain is not reducible. The statement makes a common but incorrect inference that a disconnected support implies a reducible chain.\n\nTherefore, the statement is **Incorrect**.",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}