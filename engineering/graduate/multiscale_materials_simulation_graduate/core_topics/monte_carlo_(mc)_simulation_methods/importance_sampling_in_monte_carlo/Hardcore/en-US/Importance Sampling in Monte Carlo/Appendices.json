{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of importance sampling, we begin with a foundational exercise that explores its core principle: variance reduction. This problem challenges you to go beyond simply applying the method and instead to analytically derive the variance of an importance sampling estimator for a simple integral. By finding the proposal distribution parameter that minimizes this variance, you will uncover the theoretical ideal for importance sampling and gain a deep intuition for why matching the proposal distribution to the integrand is the key to its power .",
            "id": "3253345",
            "problem": "Consider the task of numerically evaluating the improper integral $I=\\int_{0}^{\\infty} \\exp(-x)\\,dx$ using Monte Carlo (MC) integration with Importance Sampling (IS). Let the proposal density be the exponential family $q(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\ge 0$ and $\\lambda>0$. You will construct the single-sample IS estimator and analyze its variance as a function of the parameter $\\lambda$. Starting only from the definitions of MC integration and IS (namely, that IS replaces integration with expectation under a proposal density and uses the weight $w(x)=\\frac{f(x)}{q(x;\\lambda)}$), perform the following:\n\n1. Derive the single-sample IS estimator $\\widehat{I}$ for $I$ based on a draw $X\\sim q(\\cdot;\\lambda)$ and prove that it is unbiased for any $\\lambda>0$.\n2. Derive the variance $\\operatorname{Var}_{q(\\cdot;\\lambda)}[\\widehat{I}]$ as a function of $\\lambda$ and determine precisely for which $\\lambda$ this variance is finite.\n3. Find the value of $\\lambda$ that minimizes $\\operatorname{Var}_{q(\\cdot;\\lambda)}[\\widehat{I}]$ over all $\\lambda>0$.\n\nExpress your final answer as the single optimal value of $\\lambda$ that minimizes the variance. No rounding is required.",
            "solution": "The problem as stated is well-posed, self-contained, and scientifically sound. It is a standard problem in the field of numerical methods, specifically Monte Carlo integration theory. We may proceed with the solution.\n\nThe integral to be estimated is $I = \\int_{0}^{\\infty} \\exp(-x)\\,dx$. The integrand is $f(x) = \\exp(-x)$. The true value of this integral is $I = [-\\exp(-x)]_{0}^{\\infty} = 0 - (-1) = 1$.\nThe proposal probability density function is given by the exponential distribution $q(x;\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$ and parameter $\\lambda > 0$.\n\nAccording to the principle of Importance Sampling (IS), the integral $I$ can be rewritten as an expectation with respect to the proposal distribution $q(x;\\lambda)$:\n$$I = \\int_{0}^{\\infty} f(x) \\,dx = \\int_{0}^{\\infty} \\frac{f(x)}{q(x;\\lambda)} q(x;\\lambda) \\,dx = \\mathbb{E}_{X \\sim q(\\cdot;\\lambda)}[W(X)]$$\nwhere $W(x) = \\frac{f(x)}{q(x;\\lambda)}$ is the importance weight.\n\n1. Derivation of the single-sample IS estimator and proof of unbiasedness.\n\nThe single-sample Importance Sampling estimator, $\\widehat{I}$, is constructed from one sample $X$ drawn from the proposal distribution $q(x;\\lambda)$. The estimator is simply the importance weight evaluated at that sample:\n$$\\widehat{I} = W(X) = \\frac{f(X)}{q(X;\\lambda)}$$\nSubstituting the given functions for $f(x)$ and $q(x;\\lambda)$:\n$$W(x) = \\frac{\\exp(-x)}{\\lambda \\exp(-\\lambda x)} = \\frac{1}{\\lambda} \\exp(-x + \\lambda x) = \\frac{1}{\\lambda} \\exp(x(\\lambda-1))$$\nThus, the single-sample estimator is $\\widehat{I} = \\frac{1}{\\lambda} \\exp(X(\\lambda-1))$, where $X \\sim q(\\cdot;\\lambda)$.\n\nTo prove that this estimator is unbiased for any $\\lambda > 0$, we must show that its expected value is equal to the true value of the integral, $I$. The expectation is taken with respect to the proposal distribution $q(x;\\lambda)$.\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\widehat{I}(x) q(x;\\lambda) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\left( \\frac{1}{\\lambda} \\exp(x(\\lambda-1)) \\right) (\\lambda \\exp(-\\lambda x)) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\exp(x(\\lambda-1) - \\lambda x) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\exp(\\lambda x - x - \\lambda x) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\exp(-x) \\,dx = I$$\nSince we have already calculated $I=1$, we have $\\mathbb{E}_{q}[\\widehat{I}] = 1$. This holds for any value of $\\lambda > 0$ for which the derivation is valid (which is all $\\lambda > 0$). Thus, the estimator $\\widehat{I}$ is unbiased.\n\n2. Derivation of the variance and its domain of finiteness.\n\nThe variance of the estimator $\\widehat{I}$ is given by $\\operatorname{Var}_{q}[\\widehat{I}] = \\mathbb{E}_{q}[\\widehat{I}^2] - (\\mathbb{E}_{q}[\\widehat{I}])^2$.\nSince the estimator is unbiased with $\\mathbb{E}_{q}[\\widehat{I}]=1$, the variance simplifies to:\n$$\\operatorname{Var}_{q}[\\widehat{I}] = \\mathbb{E}_{q}[\\widehat{I}^2] - 1^2 = \\mathbb{E}_{q}[\\widehat{I}^2] - 1$$\nWe first compute the second moment, $\\mathbb{E}_{q}[\\widehat{I}^2]$:\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\int_{0}^{\\infty} \\widehat{I}(x)^2 q(x;\\lambda) \\,dx = \\int_{0}^{\\infty} \\left(\\frac{f(x)}{q(x;\\lambda)}\\right)^2 q(x;\\lambda) \\,dx = \\int_{0}^{\\infty} \\frac{f(x)^2}{q(x;\\lambda)} \\,dx$$\nSubstituting the expressions for $f(x)$ and $q(x;\\lambda)$:\n$$f(x)^2 = (\\exp(-x))^2 = \\exp(-2x)$$\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\int_{0}^{\\infty} \\frac{\\exp(-2x)}{\\lambda \\exp(-\\lambda x)} \\,dx = \\frac{1}{\\lambda} \\int_{0}^{\\infty} \\exp(-2x+\\lambda x) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\frac{1}{\\lambda} \\int_{0}^{\\infty} \\exp(-x(2-\\lambda)) \\,dx$$\nThis integral is of the form $\\int_0^\\infty \\exp(-ax) \\,dx$, which converges if and only if the coefficient $a$ is strictly positive. In our case, $a = 2-\\lambda$. Therefore, for the integral to converge and the variance to be finite, we must have $2-\\lambda > 0$, which implies $\\lambda < 2$.\nGiven the problem's constraint that $\\lambda > 0$, the variance is finite precisely for $\\lambda \\in (0, 2)$.\n\nFor $\\lambda \\in (0, 2)$, we can evaluate the integral:\n$$\\int_{0}^{\\infty} \\exp(-x(2-\\lambda)) \\,dx = \\left[ \\frac{\\exp(-x(2-\\lambda))}{-(2-\\lambda)} \\right]_0^\\infty = 0 - \\frac{1}{-(2-\\lambda)} = \\frac{1}{2-\\lambda}$$\nThus, the second moment is:\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\frac{1}{\\lambda} \\cdot \\frac{1}{2-\\lambda} = \\frac{1}{\\lambda(2-\\lambda)}$$\nThe variance as a function of $\\lambda$ is:\n$$\\operatorname{Var}_{q(\\cdot;\\lambda)}[\\widehat{I}] = V(\\lambda) = \\frac{1}{\\lambda(2-\\lambda)} - 1 \\quad \\text{for } \\lambda \\in (0, 2)$$\n\n3. Finding the optimal value of $\\lambda$.\n\nWe seek to find the value of $\\lambda \\in (0, 2)$ that minimizes the variance $V(\\lambda)$.\n$$V(\\lambda) = \\frac{1}{2\\lambda - \\lambda^2} - 1$$\nMinimizing $V(\\lambda)$ is equivalent to minimizing the term $\\frac{1}{2\\lambda - \\lambda^2}$, since the $-1$ is a constant offset. Minimizing this fraction is, in turn, equivalent to maximizing its denominator, $g(\\lambda) = 2\\lambda - \\lambda^2$, over the interval $\\lambda \\in (0, 2)$.\n\nTo find the maximum of $g(\\lambda)$, we calculate its derivative with respect to $\\lambda$ and set it to zero:\n$$\\frac{dg}{d\\lambda} = \\frac{d}{d\\lambda}(2\\lambda - \\lambda^2) = 2 - 2\\lambda$$\nSetting the derivative to zero:\n$$2 - 2\\lambda = 0 \\implies 2\\lambda = 2 \\implies \\lambda = 1$$\nThis critical point $\\lambda=1$ lies within the domain $(0, 2)$. To confirm it is a maximum, we examine the second derivative:\n$$\\frac{d^2g}{d\\lambda^2} = -2$$\nSince the second derivative is negative, the function $g(\\lambda)$ has a local maximum at $\\lambda=1$. As $g(\\lambda)$ is a downward-opening parabola, this is its global maximum.\nTherefore, the variance $V(\\lambda)$ is minimized at $\\lambda=1$.\n\nThis result is conceptually sound. The ideal proposal distribution for IS is one that is proportional to the integrand, $f(x)$. The normalized version of our integrand is $p(x) = \\frac{f(x)}{I} = \\frac{\\exp(-x)}{1} = \\exp(-x)$, which is an exponential distribution with parameter $1$. By choosing $\\lambda=1$, our proposal distribution $q(x;1) = 1 \\cdot \\exp(-1 \\cdot x)$ becomes identical to this ideal choice. In this case, the importance weight becomes $W(x) = \\frac{\\exp(-x)}{\\exp(-x)} = 1$ for all $x$, and the estimator $\\widehat{I}$ is always $1$. The variance of a constant is zero, which is the minimum possible variance.\nThe value of $\\lambda$ that minimizes the variance is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "After seeing the power of an optimal proposal distribution, it is equally important to understand the potential pitfalls of a poor one. This exercise serves as a critical lesson in the failure modes of importance sampling, demonstrating that a badly chosen proposal can yield results that are worse than simple uniform sampling. By analyzing the variance of different estimators, you will learn to identify the mathematical conditions that lead to increased or even infinite variance, a crucial skill for designing robust Monte Carlo simulations .",
            "id": "2402925",
            "problem": "You are asked to estimate the integral of the function $f(x)=x^2$ over the interval $[0,1]$ using importance sampling. Let $g(x)$ denote a normalized proposal probability density function on $[0,1]$. For the purpose of this question, define “worse than uniform sampling” to mean that the importance sampling estimator using $g(x)$ has a variance that is strictly larger than (or not finite compared to) the variance of the standard Monte Carlo estimator that samples $x$ uniformly on $[0,1]$. Which of the following choices for $g(x)$ makes importance sampling provably worse than uniform sampling for estimating $\\int_{0}^{1} x^2\\,dx$?\n\nA. $g(x)=1$ for $x\\in[0,1]$.\n\nB. $g(x)=\\dfrac{3}{2}\\,(1-x)^{1/2}$ for $x\\in[0,1]$.\n\nC. $g(x)=3x^2$ for $x\\in[0,1]$.\n\nD. $g(x)=2(1-x)$ for $x\\in[0,1]$.\n\nSelect all that apply. Your justification should rely on a variance comparison grounded in first principles of importance sampling, not on heuristic intuition.",
            "solution": "The problem statement must first be validated for scientific and logical integrity before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The integral to be estimated is $I = \\int_{0}^{1} f(x)\\,dx$, where the integrand is $f(x) = x^2$.\n- The integration interval is $[0,1]$.\n- A normalized proposal probability density function (PDF) on $[0,1]$ is denoted by $g(x)$.\n- Uniform sampling uses the PDF $p(x) = 1$ for $x \\in [0,1]$.\n- The definition of “worse than uniform sampling” is that the variance of the importance sampling estimator using $g(x)$ is strictly larger than, or not finite compared to, the variance of the standard Monte Carlo estimator (uniform sampling).\n- The task is to identify which of the provided functions $g(x)$ result in an importance sampling estimator that is worse than the uniform sampling estimator.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a standard application of the theory of Monte Carlo integration and variance reduction techniques, specifically importance sampling. The concepts are fundamental to computational physics and statistics. The problem is scientifically sound.\n- **Well-Posedness:** The problem is clearly formulated. The quantities to be compared—variances of estimators—are well-defined. The provided functions $g(x)$ are all valid, non-negative, and normalized probability density functions on the interval $[0,1]$.\n    - A: $\\int_0^1 1 \\,dx = 1$.\n    - B: $\\int_0^1 \\frac{3}{2}(1-x)^{1/2} \\,dx = \\frac{3}{2} [-\\frac{2}{3}(1-x)^{3/2}]_0^1 = 1$.\n    - C: $\\int_0^1 3x^2 \\,dx = [x^3]_0^1 = 1$.\n    - D: $\\int_0^1 2(1-x) \\,dx = 2[x - \\frac{x^2}{2}]_0^1 = 1$.\n- **Objectivity:** The criterion for \"worse\" is given as a strict inequality of variances, which is an objective mathematical condition. There is no ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, self-contained, and scientifically grounded problem. A solution will now be derived.\n\n### Derivation of Solution\n\nThe integral in question is $I = \\int_0^1 x^2 \\,dx = \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{3}$.\n\n**1. Variance of the Standard (Uniform) Monte Carlo Estimator**\n\nFor uniform sampling, samples $x_i$ are drawn from the PDF $p(x)=1$ on $[0,1]$. The estimator for the integral is $\\hat{I}_{unif} = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$. The single-sample variance is $\\sigma^2_{unif} = \\text{Var}_{p}(f(x))$.\nThis variance is given by:\n$$\n\\sigma^2_{unif} = E_p[f(x)^2] - (E_p[f(x)])^2\n$$\nThe expectation of $f(x)$ is the integral itself:\n$$\nE_p[f(x)] = \\int_0^1 f(x) p(x) \\,dx = \\int_0^1 x^2 \\cdot 1 \\,dx = I = \\frac{1}{3}\n$$\nThe expectation of $f(x)^2$ is:\n$$\nE_p[f(x)^2] = \\int_0^1 (f(x))^2 p(x) \\,dx = \\int_0^1 (x^2)^2 \\cdot 1 \\,dx = \\int_0^1 x^4 \\,dx = \\left[\\frac{x^5}{5}\\right]_0^1 = \\frac{1}{5}\n$$\nThus, the variance for uniform sampling is:\n$$\n\\sigma^2_{unif} = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{5} - \\frac{1}{9} = \\frac{9 - 5}{45} = \\frac{4}{45}\n$$\n\n**2. Variance of the Importance Sampling Estimator**\n\nFor importance sampling, samples $x_i$ are drawn from a proposal PDF $g(x)$. The integral is expressed as $I = \\int_0^1 \\frac{f(x)}{g(x)} g(x) \\,dx = E_g\\left[\\frac{f(x)}{g(x)}\\right]$. The single-sample variance is $\\sigma^2_g = \\text{Var}_g\\left(\\frac{f(x)}{g(x)}\\right)$.\nThis variance is given by:\n$$\n\\sigma^2_g = E_g\\left[\\left(\\frac{f(x)}{g(x)}\\right)^2\\right] - \\left(E_g\\left[\\frac{f(x)}{g(x)}\\right]\\right)^2\n$$\nThe second term is $(I)^2 = (1/3)^2 = 1/9$, since the estimator must be unbiased. The first term is:\n$$\nE_g\\left[\\left(\\frac{f(x)}{g(x)}\\right)^2\\right] = \\int_0^1 \\left(\\frac{f(x)}{g(x)}\\right)^2 g(x) \\,dx = \\int_0^1 \\frac{f(x)^2}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{g(x)} \\,dx\n$$\nSo, the variance for the importance sampling estimator is:\n$$\n\\sigma^2_g = \\int_0^1 \\frac{x^4}{g(x)} \\,dx - \\frac{1}{9}\n$$\n\n**3. Condition for \"Worse than Uniform Sampling\"**\n\nAccording to the problem definition, the importance sampling is worse if $\\sigma^2_g > \\sigma^2_{unif}$ or if $\\sigma^2_g$ is infinite.\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx - \\frac{1}{9} > \\frac{4}{45}\n$$\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx > \\frac{4}{45} + \\frac{1}{9} = \\frac{4+5}{45} = \\frac{9}{45} = \\frac{1}{5}\n$$\nSo, a choice of $g(x)$ is worse if $\\int_0^1 \\frac{x^4}{g(x)} \\,dx > \\frac{1}{5}$ or if this integral diverges.\n\n### Option-by-Option Analysis\n\n**A. $g(x)=1$ for $x\\in[0,1]$.**\nThis is uniform sampling. We check the condition:\n$$\n\\int_0^1 \\frac{x^4}{1} \\,dx = \\int_0^1 x^4 \\,dx = \\frac{1}{5}\n$$\nThe condition is $\\frac{1}{5} > \\frac{1}{5}$, which is false. The variance is equal to that of uniform sampling, not strictly larger.\n**Verdict: Incorrect.**\n\n**B. $g(x)=\\dfrac{3}{2}\\,(1-x)^{1/2}$ for $x\\in[0,1]$.**\nWe evaluate the integral:\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{\\frac{3}{2}(1-x)^{1/2}} \\,dx = \\frac{2}{3} \\int_0^1 x^4 (1-x)^{-1/2} \\,dx\n$$\nThis is related to the Beta function, $B(z_1, z_2) = \\int_0^1 t^{z_1-1} (1-t)^{z_2-1} \\,dt$. Here, $z_1-1=4$ and $z_2-1=-1/2$, so $z_1=5$ and $z_2=1/2$.\n$$\n\\int_0^1 x^4 (1-x)^{-1/2} \\,dx = B(5, 1/2) = \\frac{\\Gamma(5)\\Gamma(1/2)}{\\Gamma(5+1/2)} = \\frac{4! \\sqrt{\\pi}}{\\frac{9}{2} \\cdot \\frac{7}{2} \\cdot \\frac{5}{2} \\cdot \\frac{3}{2} \\cdot \\frac{1}{2}\\sqrt{\\pi}} = \\frac{24}{\\frac{945}{32}} = \\frac{24 \\cdot 32}{945} = \\frac{768}{945} = \\frac{256}{315}\n$$\nSo, the full term is:\n$$\n\\frac{2}{3} \\cdot \\frac{256}{315} = \\frac{512}{945}\n$$\nNow we check the condition $\\frac{512}{945} > \\frac{1}{5}$.\n$$\n\\frac{1}{5} = \\frac{189}{945}\n$$\nSince $512 > 189$, the inequality $\\frac{512}{945} > \\frac{189}{945}$ is true. The variance is strictly larger.\n**Verdict: Correct.**\n\n**C. $g(x)=3x^2$ for $x\\in[0,1]$.**\nThis proposal PDF is proportional to the integrand $f(x)=x^2$. This is the ideal choice for importance sampling, which should yield minimum, not maximum, variance.\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{3x^2} \\,dx = \\frac{1}{3} \\int_0^1 x^2 \\,dx = \\frac{1}{3} \\left[\\frac{x^3}{3}\\right]_0^1 = \\frac{1}{9}\n$$\nWe check the condition $\\frac{1}{9} > \\frac{1}{5}$. This is false.\nIndeed, the variance is $\\sigma^2_g = \\frac{1}{9} - \\frac{1}{9} = 0$, the minimum possible variance. This is far better, not worse, than uniform sampling.\n**Verdict: Incorrect.**\n\n**D. $g(x)=2(1-x)$ for $x\\in[0,1]$.**\nWe evaluate the integral for the variance calculation:\n$$\n\\int_0^1 \\frac{x^4}{g(x)} \\,dx = \\int_0^1 \\frac{x^4}{2(1-x)} \\,dx = \\frac{1}{2} \\int_0^1 \\frac{x^4}{1-x} \\,dx\n$$\nWe must analyze the convergence of this integral. As $x \\to 1^-$, let $u = 1-x$. The integrand behaves like $\\frac{(1-u)^4}{u} \\approx \\frac{1}{u}$. The integral $\\int \\frac{1}{u} \\,du$ diverges logarithmically at $u=0$.\nTherefore, the integral $\\int_0^1 \\frac{x^4}{1-x} \\,dx$ is divergent. This means the variance $\\sigma^2_g$ is infinite. An infinite variance is, by the problem's definition, \"worse than uniform sampling\". This catastrophic failure occurs because the proposal distribution $g(x)$ goes to zero at $x=1$, where the function $f(x)^2=x^4$ is non-zero.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "Moving from theory to practical application, this problem addresses a common challenge in advanced simulations: diagnosing the quality of an importance sampling run. In complex systems like those in materials science, it is rare to have a perfect proposal, often leading to a situation where a few samples have overwhelmingly large weights. This exercise introduces the Effective Sample Size ($N_{\\mathrm{eff}}$) as a vital diagnostic tool to quantify this \"weight collapse\" and provides a principled criterion for deciding when to apply corrective measures like resampling .",
            "id": "3816828",
            "problem": "You are estimating a fine-scale observable in a multiscale materials simulation where atomistic states $\\mathbf{x}$ are distributed according to a Boltzmann target density $p(\\mathbf{x}) \\propto \\exp\\!\\left(-U_{\\mathrm{f}}(\\mathbf{x})/k_{B}T\\right)$ associated with a high-fidelity potential $U_{\\mathrm{f}}(\\mathbf{x})$. To reduce variance, you sample from a coarse-grained proposal density $q(\\mathbf{x}) \\propto \\exp\\!\\left(-U_{\\mathrm{c}}(\\mathbf{x})/k_{B}T\\right)$, and construct an Importance Sampling (IS) estimator using weights $w(\\mathbf{x}) = p(\\mathbf{x})/q(\\mathbf{x}) = \\exp\\!\\left(-\\big(U_{\\mathrm{f}}(\\mathbf{x})-U_{\\mathrm{c}}(\\mathbf{x})\\big)/k_{B}T\\right)$. In this setting, materials configurations exhibiting localized defects or high-strain microstructures can produce a heavy-tailed distribution of $w(\\mathbf{x})$ due to large negative values of $\\big(U_{\\mathrm{f}}(\\mathbf{x})-U_{\\mathrm{c}}(\\mathbf{x})\\big)$.\n\nSuppose you collect $N$ independent samples $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ from $q(\\mathbf{x})$ with corresponding synthetic weights\n$$\nw_{1}=0.012,\\quad\nw_{2}=0.019,\\quad\nw_{3}=0.011,\\quad\nw_{4}=0.024,\\quad\nw_{5}=0.031,\\quad\nw_{6}=0.022,\\quad\nw_{7}=0.018,\\quad\nw_{8}=0.015,\\quad\nw_{9}=0.028,\\quad\nw_{10}=2.5,\\quad\nw_{11}=6.0,\\quad\nw_{12}=120.0,\n$$\nintended to mimic heavy-tailed behavior from rare-but-dominant fine-scale contributions. Treat the estimator as self-normalized with normalized weights $\\tilde{w}_{i} = w_{i}/\\sum_{j=1}^{N} w_{j}$.\n\nStarting from foundational definitions of Monte Carlo (MC) and Importance Sampling (IS), and using a principled notion of weight concentration that equates the second moment of the normalized weights to that of a uniform weighting over an “effective” number of samples, derive from first principles a mathematically consistent expression for the Effective Sample Size (ESS), denoted $N_{\\mathrm{eff}}$, in terms of $\\{w_{i}\\}_{i=1}^{N}$. Then, compute $N_{\\mathrm{eff}}$ for the given weights and, using the Sequential Monte Carlo (SMC) resampling criterion “resample when $N_{\\mathrm{eff}} < \\tau$ with $\\tau = 0.5N$,” diagnose whether resampling is needed to stabilize the IS estimates.\n\nRound your $N_{\\mathrm{eff}}$ to four significant figures. Provide your final numerical answer for $N_{\\mathrm{eff}}$ only. No physical units are required.",
            "solution": "The problem requires the derivation of an expression for the Effective Sample Size ($N_{\\mathrm{eff}}$) in the context of self-normalized Importance Sampling (IS), followed by its calculation for a given set of synthetic weights, and a diagnosis of the need for resampling.\n\nFirst, we establish the foundational principles of Importance Sampling. The goal is to estimate the expectation of an observable $A(\\mathbf{x})$ with respect to a target probability density $p(\\mathbf{x})$, defined as $\\langle A \\rangle_{p} = \\int A(\\mathbf{x}) p(\\mathbf{x}) d\\mathbf{x}$. Direct sampling from $p(\\mathbf{x})$ may be difficult. Instead, we draw $N$ independent samples $\\{\\mathbf{x}_i\\}_{i=1}^N$ from a proposal density $q(\\mathbf{x})$. The expectation can be rewritten as:\n$$\n\\langle A \\rangle_{p} = \\int A(\\mathbf{x}) \\frac{p(\\mathbf{x})}{q(\\mathbf{x})} q(\\mathbf{x}) d\\mathbf{x} = \\int A(\\mathbf{x}) w(\\mathbf{x}) q(\\mathbf{x}) d\\mathbf{x} = \\mathbb{E}_{q}[A(\\mathbf{x})w(\\mathbf{x})]\n$$\nwhere $w(\\mathbf{x}) = p(\\mathbf{x})/q(\\mathbf{x})$ is the importance weight. The standard Monte Carlo estimator for this expectation is $\\frac{1}{N} \\sum_{i=1}^N w(\\mathbf{x}_i) A(\\mathbf{x}_i)$.\n\nIn many practical applications, such as the one described involving Boltzmann distributions, the normalization constants of $p(\\mathbf{x})$ and $q(\\mathbf{x})$ are unknown. We only have access to unnormalized densities $p(\\mathbf{x}) \\propto \\bar{p}(\\mathbf{x})$ and $q(\\mathbf{x}) \\propto \\bar{q}(\\mathbf{x})$. The unnormalized weights are $w_i = \\bar{p}(\\mathbf{x}_i) / \\bar{q}(\\mathbf{x}_i)$. In this case, the self-normalized IS estimator is used:\n$$\n\\langle A \\rangle_{p} \\approx \\frac{\\sum_{i=1}^N w_i A(\\mathbf{x}_i)}{\\sum_{j=1}^N w_j} = \\sum_{i=1}^N \\tilde{w}_i A(\\mathbf{x}_i)\n$$\nwhere $\\tilde{w}_i = w_i / \\sum_{j=1}^N w_j$ are the normalized weights, which satisfy $\\sum_{i=1}^N \\tilde{w}_i = 1$.\n\nThe variance of this estimator is highly dependent on the variance of the weights. If a few weights are much larger than the others, the sum is dominated by a few terms, effectively reducing the number of samples contributing to the estimate. The Effective Sample Size ($N_{\\mathrm{eff}}$) is a heuristic measure to quantify this degradation.\n\nWe are tasked to derive an expression for $N_{\\mathrm{eff}}$ based on the principle of equating the second moment of the normalized weights to that of a uniform weighting over an \"effective\" number of samples.\n\nLet the set of actual normalized weights be $\\{\\tilde{w}_i\\}_{i=1}^N$. The second moment of this distribution of weights is given by:\n$$\nM_2 = \\sum_{i=1}^N \\tilde{w}_i^2\n$$\nNow, consider an ideal sample of size $N_{\\mathrm{eff}}$ where all weights are uniform. To satisfy the normalization condition, each of these ideal weights must be $\\tilde{w}_{\\mathrm{ideal}} = 1/N_{\\mathrm{eff}}$. The second moment of this ideal, uniform weight distribution is:\n$$\nM_{2, \\mathrm{ideal}} = \\sum_{j=1}^{N_{\\mathrm{eff}}} \\left(\\frac{1}{N_{\\mathrm{eff}}}\\right)^2 = N_{\\mathrm{eff}} \\cdot \\frac{1}{N_{\\mathrm{eff}}^2} = \\frac{1}{N_{\\mathrm{eff}}}\n$$\nEquating the two second moments as per the problem's instruction, $M_2 = M_{2, \\mathrm{ideal}}$, we get:\n$$\n\\sum_{i=1}^N \\tilde{w}_i^2 = \\frac{1}{N_{\\mathrm{eff}}}\n$$\nSolving for $N_{\\mathrm{eff}}$ yields the desired expression from first principles:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\tilde{w}_i^2}\n$$\nThis expression can be rewritten in terms of the unnormalized weights $\\{w_i\\}_{i=1}^N$, which is often more convenient for computation. Substituting $\\tilde{w}_i = w_i / \\sum_{j=1}^N w_j$:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\left(\\frac{w_i}{\\sum_{j=1}^N w_j}\\right)^2} = \\frac{1}{\\frac{\\sum_{i=1}^N w_i^2}{\\left(\\sum_{j=1}^N w_j\\right)^2}} = \\frac{\\left(\\sum_{i=1}^N w_i\\right)^2}{\\sum_{i=1}^N w_i^2}\n$$\nThis is the mathematically consistent expression for the Effective Sample Size derived from the specified principle.\n\nNext, we compute $N_{\\mathrm{eff}}$ for the $N=12$ given synthetic weights:\n$w_1=0.012$, $w_2=0.019$, $w_3=0.011$, $w_4=0.024$, $w_5=0.031$, $w_6=0.022$, $w_7=0.018$, $w_8=0.015$, $w_9=0.028$, $w_{10}=2.5$, $w_{11}=6.0$, $w_{12}=120.0$.\n\nWe require two quantities: the sum of the weights, $S_1 = \\sum_{i=1}^{12} w_i$, and the sum of the squares of the weights, $S_2 = \\sum_{i=1}^{12} w_i^2$.\n\nCalculation of $S_1$:\n$$\nS_1 = 0.012+0.019+0.011+0.024+0.031+0.022+0.018+0.015+0.028+2.5+6.0+120.0 = 128.68\n$$\nCalculation of $S_2$:\n\\begin{align*}\nS_2 &= (0.012)^2 + (0.019)^2 + (0.011)^2 + (0.024)^2 + (0.031)^2 + (0.022)^2 + (0.018)^2 + (0.015)^2 + (0.028)^2 \\\\\n   &\\quad + (2.5)^2 + (6.0)^2 + (120.0)^2 \\\\\n   &= 0.000144 + 0.000361 + 0.000121 + 0.000576 + 0.000961 + 0.000484 + 0.000324 + 0.000225 + 0.000784 \\\\\n   &\\quad + 6.25 + 36.0 + 14400.0 \\\\\n   &= 0.00398 + 6.25 + 36.0 + 14400.0 = 14442.25398\n\\end{align*}\nNow, we use the derived formula for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{S_1^2}{S_2} = \\frac{(128.68)^2}{14442.25398} = \\frac{16558.6224}{14442.25398} \\approx 1.1465319...\n$$\nRounding to four significant figures as requested, we obtain:\n$$\nN_{\\mathrm{eff}} \\approx 1.147\n$$\nFinally, we diagnose the need for resampling using the criterion \"resample when $N_{\\mathrm{eff}} < \\tau$ with $\\tau = 0.5N$\".\nThe total number of samples is $N = 12$. The resampling threshold is:\n$$\n\\tau = 0.5 \\times N = 0.5 \\times 12 = 6\n$$\nWe compare our calculated $N_{\\mathrm{eff}}$ to this threshold:\n$$\n1.147 < 6\n$$\nThe condition $N_{\\mathrm{eff}} < \\tau$ is satisfied. Therefore, a resampling step is necessary to mitigate the high variance and stabilize the importance sampling estimates. The low value of $N_{\\mathrm{eff}}$ correctly reflects the extreme concentration of weight in a single sample ($w_{12}$), which renders the remaining $11$ samples virtually irrelevant.",
            "answer": "$$\n\\boxed{1.147}\n$$"
        }
    ]
}