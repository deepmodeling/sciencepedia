{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of Configurational-Bias Monte Carlo stems from its intelligent selection of trial configurations based on their energetic favorability, which is governed by a Boltzmann weight calculated from a \"local energy.\" This exercise  will help you solidify your understanding of which specific energy contributions—both bonded and non-bonded—must be included in this local energy calculation during a polymer regrowth step. Mastering this concept is crucial for correctly implementing the biasing mechanism of the CBMC algorithm and understanding its interplay with common simulation practices like potential truncation.",
            "id": "3797247",
            "problem": "A linear polymer is simulated at fixed temperature $T$ using Configurational-Bias Monte Carlo (CBMC). The intramolecular energy is composed of bonded terms $u_{\\text{bond}}(r)$, $u_{\\text{angle}}(\\theta)$, and $u_{\\text{dihedral}}(\\phi)$ that depend on bond lengths $r$, bond angles $\\theta$, and dihedral angles $\\phi$, respectively, plus nonbonded interactions $u_{\\text{nb}}(r)$ that act between pairs of sites not otherwise bonded. The nonbonded potential is pairwise additive, truncated and shifted at a cutoff $r_c$ so that $u_{\\text{nb}}(r\\ge r_c)=0$, and neighbor lists are maintained so that only pairs within $r_c$ are considered.\n\nIn a regrowth step, a single new site $i$ is attached to site $i-1$, and $k$ trial conformations $\\{\\mathbf{q}_\\ell\\}_{\\ell=1}^k$ are generated for the local degrees of freedom that determine $r_{i,i-1}$, $\\theta_{i,i-1,i-2}$, $\\phi_{i,i-1,i-2,i-3}$, and the distances $r_{ij}$ between the new site $i$ and surrounding sites $j$. CBMC chooses one of the $k$ trials with a probability proportional to a Boltzmann factor of a local energy $U_{\\text{loc}}(\\mathbf{q}_\\ell)$, and uses the corresponding Rosenbluth weight to ensure detailed balance in the canonical ensemble.\n\nStarting from the canonical distribution $p(\\mathbf{x}) \\propto \\exp(-\\beta U(\\mathbf{x}))$ with $\\beta = 1/(k_{\\text{B}} T)$, and the definition of detailed balance for trial selection, identify which expression correctly specifies the local energy $U_{\\text{loc}}$ to be used in the trial weights for this regrowth step, and which justification correctly explains why distant interactions can be neglected under neighbor lists in this context. Assume a standard force-field convention in which nonbonded interactions excluded by bonding topology (such as $1$-$2$ and $1$-$3$ pairs) are not included in $u_{\\text{nb}}$, and that any $1$-$4$ scaling is already built into $u_{\\text{dihedral}}$ or $u_{\\text{nb}}$ as appropriate.\n\nChoose the single best option.\n\nA. $U_{\\text{loc}}$ equals the sum of the bonded contributions for the newly formed local geometry, $u_{\\text{bond}}(r_{i,i-1}) + u_{\\text{angle}}(\\theta_{i,i-1,i-2}) + u_{\\text{dihedral}}(\\phi_{i,i-1,i-2,i-3})$, plus the sum of nonbonded interactions between the new site $i$ and all sites $j$ in its neighbor list within the cutoff, $\\sum_{j \\in \\mathcal{N}(i)} u_{\\text{nb}}(r_{ij})$. Distant interactions can be neglected because the truncated-and-shifted $u_{\\text{nb}}$ is exactly zero for $r \\ge r_c$, so contributions from pairs beyond $r_c$ are independent of the trial and cancel in the relative trial probabilities.\n\nB. $U_{\\text{loc}}$ includes only bonded terms $u_{\\text{bond}}(r_{i,i-1}) + u_{\\text{angle}}(\\theta_{i,i-1,i-2}) + u_{\\text{dihedral}}(\\phi_{i,i-1,i-2,i-3})$, while nonbonded terms are excluded because neighbor lists already reduce computational cost and nonbonded interactions are independent of the chosen trial conformation.\n\nC. $U_{\\text{loc}}$ is the full-system energy difference $U_{\\text{after}} - U_{\\text{before}}$, summing all bonded and nonbonded interactions in the polymer, because detailed balance requires accounting for long-range electrostatics even when neighbor lists are used; distant interactions must be included.\n\nD. $U_{\\text{loc}}$ is the sum of bonded and nonbonded interactions of the new site $i$ with all atoms in the system, $\\sum_{j} u_{\\text{nb}}(r_{ij})$ without any cutoff, because truncation of pair potentials would violate detailed balance; therefore, interactions beyond $r_c$ cannot be neglected.\n\nE. $U_{\\text{loc}}$ equals the bonded terms plus nonbonded interactions with neighbor-list partners, but a mean-field background energy density must be subtracted to compensate for neglected distant atoms; this mean-field correction is necessary to preserve unbiased trial selection.",
            "solution": "The problem statement is submitted to validation.\n\n**Step 1: Extract Givens**\n- **Simulation Method:** Configurational-Bias Monte Carlo (CBMC) at fixed temperature $T$.\n- **System:** A linear polymer.\n- **Ensemble:** Canonical, with probability distribution $p(\\mathbf{x}) \\propto \\exp(-\\beta U(\\mathbf{x}))$, where $\\beta = 1/(k_{\\text{B}} T)$.\n- **Intramolecular Potential:** Composed of bonded terms: $u_{\\text{bond}}(r)$, $u_{\\text{angle}}(\\theta)$, and $u_{\\text{dihedral}}(\\phi)$.\n- **Nonbonded Potential:** $u_{\\text{nb}}(r)$, pairwise additive, truncated and shifted such that $u_{\\text{nb}}(r \\ge r_c) = 0$.\n- **Computational Details:** Neighbor lists are used to consider pairs within the cutoff $r_c$.\n- **CBMC Move:** A regrowth step where a new site $i$ is attached to site $i-1$.\n- **Trial Generation:** $k$ trial conformations $\\{\\mathbf{q}_\\ell\\}_{\\ell=1}^k$ are generated for the local degrees of freedom ($r_{i,i-1}$, $\\theta_{i,i-1,i-2}$, $\\phi_{i,i-1,i-2,i-3}$).\n- **Trial Selection:** One trial $\\ell$ is chosen with probability proportional to a Boltzmann-like weight, $\\exp(-\\beta U_{\\text{loc}}(\\mathbf{q}_\\ell))$.\n- **Detailed Balance:** The overall procedure uses a Rosenbluth weight to satisfy detailed balance.\n- **Force-Field Convention:** Nonbonded interactions are excluded for $1$-$2$ and $1$-$3$ pairs.\n- **Question:** Identify the correct expression for the local energy $U_{\\text{loc}}$ used in the trial weights and the correct justification for neglecting distant interactions.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes the standard CBMC algorithm applied to polymer chains, a cornerstone technique in molecular simulation. The use of bonded and nonbonded potentials, potential truncation, neighbor lists, and the concepts of Rosenbluth weights and detailed balance are all fundamental and correctly represented.\n- **Well-Posed:** The question asks for the definition of a specific term, $U_{\\text{loc}}$, within the CBMC algorithm under the specified conditions. This is a clearly defined question with a unique and meaningful answer derivable from the principles of statistical mechanics.\n- **Objective:** The problem is stated in precise, technical language common to the field of computational physics and chemistry. There are no subjective or ambiguous terms.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, and objective. It is a valid problem. The solution process may proceed.\n\n**Derivation of the Solution**\n\nThe objective of a Monte Carlo simulation in the canonical ensemble is to generate a sequence of configurations $\\mathbf{x}$ such that their probability of occurrence follows the Boltzmann distribution, $p(\\mathbf{x}) \\propto \\exp(-\\beta U(\\mathbf{x}))$. This is achieved by constructing moves that satisfy the detailed balance condition.\n\nThe Configurational-Bias Monte Carlo (CBMC) method is an advanced technique that enhances sampling efficiency, particularly for chain molecules. In a regrowth step, rather than generating a single random trial position for a new site and accepting or rejecting it, CBMC generates a set of $k$ trial positions and intelligently selects one.\n\nLet the state of the system before adding site $i$ be $\\mathbf{x}_{\\text{old}}$, with total energy $U(\\mathbf{x}_{\\text{old}})$. A new site $i$ is to be grown from site $i-1$. For each of the $k$ trial conformations, $\\mathbf{q}_\\ell$, which define the position $\\mathbf{r}_i^{(\\ell)}$ of the new site, a corresponding \"new\" system configuration $\\mathbf{x}_{\\text{new}}^{(\\ell)}$ is defined. The total energy of this new configuration would be $U(\\mathbf{x}_{\\text{new}}^{(\\ell)})$.\n\nThe energy change upon adding site $i$ in trial conformation $\\ell$ is:\n$$\n\\Delta U^{(\\ell)} = U(\\mathbf{x}_{\\text{new}}^{(\\ell)}) - U(\\mathbf{x}_{\\text{old}})\n$$\nThis energy change can be attributed entirely to the interactions involving the newly added site $i$. It comprises the new intramolecular (bonded) interactions and the new nonbonded interactions of site $i$ with all other sites $j$ in the system.\n$$\n\\Delta U^{(\\ell)} = \\left[ u_{\\text{bond}}(r_{i,i-1}^{(\\ell)}) + u_{\\text{angle}}(\\theta_{i,i-1,i-2}^{(\\ell)}) + u_{\\text{dihedral}}(\\phi_{i,i-1,i-2,i-3}^{(\\ell)}) \\right] + \\sum_{j} u_{\\text{nb}}(r_{ij}^{(\\ell)})\n$$\nThe sum over $j$ includes all other sites in the system with which site $i$ has a nonbonded interaction, according to the force-field rules (e.g., excluding sites $i-1$, $i-2$, $i-3$).\n\nIn CBMC, we bias the selection of the trial conformation. Instead of choosing one of the $k$ trials uniformly, we select trial $\\ell$ with a probability $P_{\\text{select}}(\\ell)$ that is proportional to a weight $w_\\ell$. To improve sampling, this weight is chosen to favor low-energy conformations. The standard choice is a Boltzmann factor of some \"local energy\" $U_{\\text{loc}}(\\mathbf{q}_\\ell)$:\n$$\nw_\\ell = \\exp(-\\beta U_{\\text{loc}}(\\mathbf{q}_\\ell))\n$$\nThe probability of selecting trial $\\ell$ is then:\n$$\nP_{\\text{select}}(\\ell) = \\frac{w_\\ell}{\\sum_{m=1}^k w_m} = \\frac{\\exp(-\\beta U_{\\text{loc}}(\\mathbf{q}_\\ell))}{\\sum_{m=1}^k \\exp(-\\beta U_{\\text{loc}}(\\mathbf{q}_m))}\n$$\nThe term $W = \\sum_{m=1}^k w_m$ is the Rosenbluth factor, which is used in the final acceptance step to correct for the bias introduced in this selection step, thereby ensuring detailed balance is globally satisfied.\n\nThe crucial question is the definition of $U_{\\text{loc}}$. To make the biasing effective, $U_{\\text{loc}}$ should include all the energy terms that depend on the specific trial position of site $i$. This is precisely the total energy change $\\Delta U^{(\\ell)}$ calculated above. Any energy term not involving site $i$ is constant with respect to the choice of trial $\\ell$ and would cancel out from the relative probabilities $w_\\ell/w_m$. Therefore, the ideal choice for the local energy is the interaction energy of the new particle with the rest of the system.\n$$\nU_{\\text{loc}}(\\mathbf{q}_\\ell) = \\left[ u_{\\text{bond}}(r_{i,i-1}^{(\\ell)}) + u_{\\text{angle}}(\\theta_{i,i-1,i-2}^{(\\ell)}) + u_{\\text{dihedral}}(\\phi_{i,i-1,i-2,i-3}^{(\\ell)}) \\right] + \\sum_{j} u_{\\text{nb}}(r_{ij}^{(\\ell)})\n$$\nNow, we must consider the nature of the nonbonded potential $u_{\\text{nb}}(r)$. The problem states it is truncated and shifted, so $u_{\\text{nb}}(r \\ge r_c) = 0$. This has a critical consequence for the summation $\\sum_{j} u_{\\text{nb}}(r_{ij}^{(\\ell)})$. Any term for which the distance $r_{ij}^{(\\ell)}$ is greater than or equal to the cutoff $r_c$ contributes exactly zero to the sum. Thus, the sum can be restricted to sites $j$ for which $r_{ij}^{(\\ell)} < r_c$ without any loss of accuracy.\n$$\n\\sum_{j} u_{\\text{nb}}(r_{ij}^{(\\ell)}) = \\sum_{j \\text{ where } r_{ij}^{(\\ell)} < r_c} u_{\\text{nb}}(r_{ij}^{(\\ell)})\n$$\nThis is precisely where neighbor lists are useful; they provide a pre-compiled list of particles that are potentially within the cutoff radius of a given region, drastically reducing the computational cost of finding these interacting pairs.\n\nTherefore, the appropriate local energy $U_{\\text{loc}}$ consists of the new bonded energy terms plus the sum of nonbonded interactions between the new site $i$ and all other sites $j$ within the cutoff distance $r_c$. The neglect of interactions with distant atoms (beyond $r_c$) is not an approximation for this potential model; it is an exact result of the potential being defined as zero for $r \\ge r_c$.\n\n**Evaluation of Options**\n\n**A. $U_{\\text{loc}}$ equals the sum of the bonded contributions for the newly formed local geometry, $u_{\\text{bond}}(r_{i,i-1}) + u_{\\text{angle}}(\\theta_{i,i-1,i-2}) + u_{\\text{dihedral}}(\\phi_{i,i-1,i-2,i-3})$, plus the sum of nonbonded interactions between the new site $i$ and all sites $j$ in its neighbor list within the cutoff, $\\sum_{j \\in \\mathcal{N}(i)} u_{\\text{nb}}(r_{ij})$. Distant interactions can be neglected because the truncated-and-shifted $u_{\\text{nb}}$ is exactly zero for $r \\ge r_c$, so contributions from pairs beyond $r_c$ are independent of the trial and cancel in the relative trial probabilities.**\n- The expression for $U_{\\text{loc}}$ is correct. It includes all the new bonded and nonbonded interactions that depend on the trial position of site $i$, considering the cutoff.\n- The justification is correct. For a truncated potential, interactions beyond the cutoff are strictly zero, not merely neglected or approximated. This means their contribution to the energy is zero, and they do not affect the relative probabilities of the trials.\n- **Verdict: Correct.**\n\n**B. $U_{\\text{loc}}$ includes only bonded terms $u_{\\text{bond}}(r_{i,i-1}) + u_{\\text{angle}}(\\theta_{i,i-1,i-2}) + u_{\\text{dihedral}}(\\phi_{i,i-1,i-2,i-3})$, while nonbonded terms are excluded because neighbor lists already reduce computational cost and nonbonded interactions are independent of the chosen trial conformation.**\n- The expression for $U_{\\text{loc}}$ is incorrect. A key purpose of CBMC is to avoid generating trial positions with severe steric clashes, which arise from nonbonded interactions. Excluding these from the biasing energy $U_{\\text{loc}}$ would defeat this purpose and make the algorithm highly inefficient.\n- The justification is incorrect on two counts. First, while neighbor lists reduce cost, that is not a reason to omit nonbonded energies from the bias. Second, the nonbonded interaction energy $u_{\\text{nb}}(r_{ij})$ is strongly dependent on the trial conformation, as the distance $r_{ij}$ changes with the choice of trial.\n- **Verdict: Incorrect.**\n\n**C. $U_{\\text{loc}}$ is the full-system energy difference $U_{\\text{after}} - U_{\\text{before}}$, summing all bonded and nonbonded interactions in the polymer, because detailed balance requires accounting for long-range electrostatics even when neighbor lists are used; distant interactions must be included.**\n- The expression for $U_{\\text{loc}}$ is subtly confused. While $U_{\\text{loc}}$ should be equal to the change in system energy for the chosen trial, $\\Delta U^{(\\ell)}$, the phrasing \"full-system energy difference\" is ambiguous. More importantly, the justification is incorrect. The problem explicitly states a truncated-and-shifted potential is used, which has no long-range component by definition. Mentioning \"long-range electrostatics\" contradicts the given potential form.\n- **Verdict: Incorrect.**\n\n**D. $U_{\\text{loc}}$ is the sum of bonded and nonbonded interactions of the new site $i$ with all atoms in the system, $\\sum_{j} u_{\\text{nb}}(r_{ij})$ without any cutoff, because truncation of pair potentials would violate detailed balance; therefore, interactions beyond $r_c$ cannot be neglected.**\n- The justification is fundamentally incorrect. The use of a truncated potential does not violate detailed balance. A Monte Carlo simulation can perfectly satisfy detailed balance for *any* well-defined potential energy function $U(\\mathbf{x})$. Truncating the potential simply defines a new energy function and thus a new physical model. The simulation correctly samples the ensemble for this new model.\n- Because the justification is false, the conclusion to not use a cutoff is also based on a false premise (within the context of simulating the truncated model).\n- **Verdict: Incorrect.**\n\n**E. $U_{\\text{loc}}$ equals the bonded terms plus nonbonded interactions with neighbor-list partners, but a mean-field background energy density must be subtracted to compensate for neglected distant atoms; this mean-field correction is necessary to preserve unbiased trial selection.**\n- The inclusion of a mean-field correction in the microscopic energy calculation for an MC move is incorrect. Such corrections (tail corrections) are sometimes applied to ensemble averages of observables (like total energy or pressure) to estimate the properties of a system with a non-truncated potential, based on a simulation using a truncated one. They are not part of the potential energy $U(\\mathbf{x})$ used to generate and accept configurations. Furthermore, since the model is defined by the truncated potential, no compensation is needed.\n- The justification is also incorrect. The trial selection is inherently *biased*; the Rosenbluth factor in the acceptance step is what ensures the overall move is correct (unbiased in the long run). The mean-field term is not related to this process.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The Rosenbluth factor, which is central to the CBMC acceptance rule, is calculated by summing the Boltzmann weights of all trial configurations. However, a naive summation of exponential terms is notoriously unstable and can lead to floating-point overflow or underflow errors, corrupting the simulation results. This practical exercise  focuses on this critical implementation detail, challenging you to implement the \"log-sum-exp\" technique to ensure your calculation of the Rosenbluth factor is numerically robust and reliable across diverse physical conditions.",
            "id": "3745270",
            "problem": "Consider a polymer growth step in Configurational Bias Monte Carlo (CBMC), where a monomer is to be placed by sampling from $k$ trial positions. For trial $j$ at segment $i$, let the incremental energy change be $\\Delta U_i^{(j)}$, measured in units of thermal energy so that it is dimensionless multiples of Boltzmann’s constant times temperature $k_{\\mathrm{B}} T$ and therefore the inverse thermal energy is $\\beta = 1$. The Rosenbluth sum (also called Rosenbluth factor) for segment $i$ is defined as $w_i = \\sum_{j=1}^{k} \\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$, and the corresponding normalized sampling probability for trial $j$ is $p_i^{(j)} = \\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right) / w_i$. In multiscale modeling and analysis, CBMC relies on correct evaluation of $w_i$ for acceptance decisions and bias corrections. From statistical mechanics, use the Boltzmann weighting principle that the relative weight of a configuration is proportional to $\\exp\\!\\left(-\\beta U\\right)$ to justify the form of $w_i$ and $p_i^{(j)}$ from first principles. Then analyze the floating-point pitfalls that occur for large-magnitude $\\Delta U_i^{(j)}$, focusing on underflow when $\\Delta U_i^{(j)}$ is large and positive (making $\\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$ extremely small) and overflow when $\\Delta U_i^{(j)}$ is large and negative (making $\\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$ extremely large). Derive a numerically stable expression for $\\log w_i$ using the log-sum-exp identity. Starting from $x_j = -\\beta \\,\\Delta U_i^{(j)}$, show how to compute\n$$\n\\log w_i = m + \\log\\!\\left(\\sum_{j=1}^{k} \\exp(x_j - m)\\right), \\quad \\text{where } m = \\max_{1 \\le j \\le k} x_j,\n$$\nand explain why computing $\\log w_i$ this way avoids overflow and underflow in the intermediate exponentials. Explain how to recover $w_i$ from $\\log w_i$ and how to detect when $w_i$ itself cannot be represented in double-precision floating-point because $\\log w_i$ exceeds the threshold at which $\\exp(\\log w_i)$ overflows. Use these derivations to design an algorithm that, given a list of trial energy differences $\\Delta U_i^{(j)}$ (dimensionless, measured in units of $k_{\\mathrm{B}} T$ so that $\\beta = 1$), produces for each set:\n- The stabilized Rosenbluth sum $w_i$ computed via $\\log w_i$; if $\\log w_i$ is larger than the natural logarithm of the maximum representable floating-point number, return $+\\infty$ for $w_i$.\n- The stabilized logarithm $\\log w_i$.\n- A boolean indicating whether direct naive summation of $\\sum_j \\exp\\!\\left(-\\beta \\,\\Delta U_i^{(j)}\\right)$ matches the stabilized $w_i$ within a relative tolerance of $10^{-12}$, or both are $+\\infty$, or both are exactly $0.0$.\n\nYour program must implement this algorithm and apply it to the following test suite of trial energy difference lists, each list expressed as values of $\\Delta U$ in units of $k_{\\mathrm{B}} T$ (dimensionless):\n- Case $1$ (moderate values, happy path): $\\left[ -0.1, 0.0, 0.2, 0.5 \\right]$.\n- Case $2$ (underflow of individual terms but finite sum): $\\left[ 100.0, 800.0, 1000.0 \\right]$.\n- Case $3$ (overflow in naive exponentials): $\\left[ -800.0, -1000.0, -1200.0 \\right]$.\n- Case $4$ (mixed extremes): $\\left[ -1000.0, 0.0, 1000.0 \\right]$.\n- Case $5$ (single trial boundary): $\\left[ 0.0 \\right]$.\n- Case $6$ (large set spanning negative to positive): a list of $100$ values linearly spaced from $-50.0$ to $50.0$ inclusive.\n- Case $7$ (extreme negative values): $\\left[ -1500.0, -1400.0 \\right]$.\n\nThe final output format must be a single line consisting of a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list of the form $\\left[ w_i, \\log w_i, \\text{eq} \\right]$, where $w_i$ and $\\log w_i$ are floating-point numbers (with $+\\infty$ represented as Python’s string $\\texttt{inf}$ if overflow is detected), and $\\text{eq}$ is a boolean. For example, the overall output should look like $\\left[ [\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],\\ldots \\right]$ with no additional whitespace outside of commas. Angles are not involved in this problem; no angle units are required. Because all energies are given in units of $k_{\\mathrm{B}} T$, the outputs are dimensionless and require no physical units.\n\nYour task is to produce a complete, runnable program that carries out these computations exactly for the given test suite and prints only the single-line output in the format described.",
            "solution": "The foundation for Configurational Bias Monte Carlo (CBMC) is the Boltzmann distribution from statistical mechanics, which states that the relative probability of a microstate with energy $U$ at inverse temperature $\\beta$ is proportional to $\\exp\\!\\left(-\\beta U\\right)$. In CBMC polymer growth, at segment $i$ we propose $k$ trial positions with incremental energy changes $\\Delta U_i^{(j)}$ relative to some reference. The unnormalized weight for trial $j$ follows directly as $\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)$ by the Boltzmann principle. Normalizing across the $k$ options gives the sampling probability\n$$\np_i^{(j)} = \\frac{\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)}{\\sum_{\\ell=1}^{k} \\exp\\!\\left(-\\beta \\Delta U_i^{(\\ell)}\\right)},\n$$\nwhere the denominator is the Rosenbluth sum\n$$\nw_i = \\sum_{j=1}^{k} \\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right).\n$$\nThis definition is a direct consequence of the well-tested Boltzmann weighting principle and ensures detailed balance when combined with appropriate acceptance criteria for regrowth moves in multiscale modeling.\n\nWhen implementing $w_i$, naive evaluation can fail numerically:\n- If $\\Delta U_i^{(j)}$ is large and positive, then $-\\beta \\Delta U_i^{(j)}$ is large and negative, so $\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)$ may underflow to $0.0$ in double precision.\n- If $\\Delta U_i^{(j)}$ is large and negative, then $-\\beta \\Delta U_i^{(j)}$ is large and positive, so $\\exp\\!\\left(-\\beta \\Delta U_i^{(j)}\\right)$ may overflow to $+\\infty$.\n\nUnderflow or overflow of individual terms can invalidate $w_i$ if the sum is computed directly as a sum of exponentials. To stabilize the computation, define $x_j = -\\beta \\,\\Delta U_i^{(j)}$ and let $m = \\max_j x_j$. Then factor out $\\exp(m)$ from the sum:\n$$\nw_i = \\sum_{j=1}^{k} \\exp(x_j) = \\exp(m) \\sum_{j=1}^{k} \\exp(x_j - m).\n$$\nTaking the natural logarithm on both sides yields the log-sum-exp identity:\n$$\n\\log w_i = m + \\log\\!\\left(\\sum_{j=1}^{k} \\exp(x_j - m)\\right).\n$$\nThis expression is numerically stable for two reasons. First, all exponents $(x_j - m)$ are $\\le 0$, so $\\exp(x_j - m) \\in (0,1]$, eliminating overflow in the inner exponentials. Second, terms that would underflow in the naive computation become tiny but are accumulated safely inside the sum before taking the logarithm. The only remaining risk is when $\\log w_i$ itself is so large that $\\exp(\\log w_i)$ cannot be represented in double precision when converting back to $w_i$.\n\nTo recover $w_i$ from $\\log w_i$, compute $w_i = \\exp(\\log w_i)$ if $\\log w_i$ is below the overflow threshold. For IEEE $754$ double precision, the largest finite floating-point number is approximately $1.7976931348623157 \\times 10^{308}$, and the threshold for the exponential function is around $\\log(1.7976931348623157 \\times 10^{308}) \\approx 709.782712893384$. Therefore, if $\\log w_i > \\log(\\text{max float})$, we should return $+\\infty$ to indicate overflow when representing $w_i$ even though $\\log w_i$ is finite.\n\nAlgorithm design:\n1. Inputs are lists of $\\Delta U_i^{(j)}$, all dimensionless in units of $k_{\\mathrm{B}} T$, implying $\\beta = 1$.\n2. For each list, compute $x_j = -\\beta \\,\\Delta U_i^{(j)}$ so that $x_j = -\\Delta U_i^{(j)}$.\n3. Compute $m = \\max_j x_j$, then evaluate $s = \\sum_j \\exp(x_j - m)$ in double precision. Compute $\\log w_i = m + \\log(s)$.\n4. If $\\log w_i$ exceeds the threshold $\\log(\\text{max float})$, set $w_i = +\\infty$; otherwise set $w_i = \\exp(\\log w_i)$.\n5. For comparison, compute the naive sum $w_i^{\\text{naive}} = \\sum_j \\exp(x_j)$, which may be $0.0$, finite, or $+\\infty$ due to underflow or overflow in individual terms.\n6. Define an equivalence check:\n   - If both $w_i$ and $w_i^{\\text{naive}}$ are $+\\infty$, return true.\n   - Else if both are exactly $0.0$, return true.\n   - Else if both are finite, check relative agreement $\\left|w_i - w_i^{\\text{naive}}\\right| / \\max(1.0, |w_i|, |w_i^{\\text{naive}}|) \\le 10^{-12}$ and return true if satisfied.\n   - Otherwise return false.\n7. Output, for each test case, the triplet $\\left[w_i, \\log w_i, \\text{eq}\\right]$.\n8. Aggregate all triplets into a single list printed as one line in the required format with commas and no extraneous whitespace.\n\nTest suite analysis:\n- Case $1$: Moderate values produce a well-conditioned sum; naive and stabilized agree.\n- Case $2$: Some exponentials underflow to zero individually (e.g., $\\exp(-800)$ and $\\exp(-1000)$), but the largest term $\\exp(-100)$ is finite; stabilized and naive agree, demonstrating that underflow of small contributions does not break the sum if the dominant term is representable.\n- Case $3$: Extremely negative $\\Delta U$ yield exponents like $\\exp(800)$ which overflow; naive sum returns $+\\infty$. The stabilized $\\log w_i$ is finite (near $800$), but $w_i$ must be reported as $+\\infty$ because $\\exp(\\log w_i)$ cannot be represented; equivalence is true since both are $+\\infty$.\n- Case $4$: Mixed extremes include both overflow and underflow in naive exponentials; stabilized treatment yields finite $\\log w_i$ and $w_i = +\\infty$; equivalence holds.\n- Case $5$: Single element gives $w_i = \\exp(0) = 1.0$ exactly.\n- Case $6$: The range from $-50.0$ to $50.0$ produces a large but finite sum; stabilized and naive agree.\n- Case $7$: More extreme negatives ensure overflow in naive exponentials and $w_i = +\\infty$; stabilized $\\log w_i$ is finite and large; equivalence holds.\n\nThis procedure adheres to first principles via Boltzmann weighting, uses a well-tested numerical stabilization strategy, and produces deterministic outputs for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef log_sum_exp(x: np.ndarray) -> float:\n    \"\"\"\n    Compute log(sum(exp(x))) in a numerically stable manner.\n    Returns a Python float.\n    \"\"\"\n    # Handle empty input defensively (not expected in this problem)\n    if x.size == 0:\n        return -np.inf\n    # Use max trick\n    m = np.max(x)\n    # If m is -inf (all entries -inf), then sum exp(x - m) is 0 -> log is -inf\n    if not np.isfinite(m):\n        # If any finite exists, np.max would be finite; here m is -inf => all -inf\n        return -np.inf\n    # Compute the sum of exponentials of shifted values\n    s = np.sum(np.exp(x - m))\n    # s should be >= 1.0 if at least one finite term exists\n    return float(m + np.log(s))\n\ndef stable_rosenbluth(dU: np.ndarray, beta: float = 1.0):\n    \"\"\"\n    Given an array of Delta U values (dimensionless, in units of k_B T),\n    compute the stabilized Rosenbluth sum w and its logarithm log_w.\n    Also compute the naive sum for comparison.\n    \"\"\"\n    # Convert to exponent arguments x = -beta * dU\n    x = -beta * dU\n    # Stabilized log-sum-exp\n    log_w = log_sum_exp(x)\n    # Determine overflow threshold for exp\n    log_max = np.log(np.finfo(np.float64).max)\n    if log_w > log_max:\n        w = float('inf')\n    else:\n        w = float(np.exp(log_w))\n    # Naive sum (may underflow/overflow)\n    exp_x = np.exp(x)\n    naive_w = float(np.sum(exp_x))\n    return w, log_w, naive_w\n\ndef compare_w(w: float, naive_w: float, rtol: float = 1e-12) -> bool:\n    \"\"\"\n    Compare stabilized w with naive w using relative tolerance,\n    accounting for infinities and exact zeros.\n    \"\"\"\n    if np.isinf(w) and np.isinf(naive_w):\n        return True\n    if w == 0.0 and naive_w == 0.0:\n        return True\n    if np.isfinite(w) and np.isfinite(naive_w):\n        denom = max(1.0, abs(w), abs(naive_w))\n        rel_err = abs(w - naive_w) / denom\n        return rel_err <= rtol\n    return False\n\ndef format_value(val):\n    \"\"\"\n    Format a value (float, bool, list/tuple) without spaces, as required.\n    \"\"\"\n    if isinstance(val, (list, tuple)):\n        return \"[\" + \",\".join(format_value(v) for v in val) + \"]\"\n    if isinstance(val, (np.floating, float)):\n        if np.isinf(val):\n            return \"inf\"\n        # Use repr for a compact precise representation\n        return repr(float(val))\n    if isinstance(val, (np.bool_, bool)):\n        return \"True\" if bool(val) else \"False\"\n    # Fallback for integers if any appear\n    if isinstance(val, (np.integer, int)):\n        return str(int(val))\n    # Fallback: convert to string\n    return str(val)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # All energies are dimensionless in units of k_B T (beta = 1).\n    test_cases = [\n        [-0.1, 0.0, 0.2, 0.5],               # Case 1\n        [100.0, 800.0, 1000.0],              # Case 2\n        [-800.0, -1000.0, -1200.0],          # Case 3\n        [-1000.0, 0.0, 1000.0],              # Case 4\n        [0.0],                               # Case 5\n        list(np.linspace(-50.0, 50.0, 100)), # Case 6\n        [-1500.0, -1400.0],                  # Case 7\n    ]\n\n    beta = 1.0  # energies are in k_B T units\n    results = []\n    for case in test_cases:\n        dU = np.array(case, dtype=np.float64)\n        w, log_w, naive_w = stable_rosenbluth(dU, beta=beta)\n        eq = compare_w(w, naive_w, rtol=1e-12)\n        results.append([w, log_w, eq])\n\n    # Final print statement in the exact required format: single line, no extra spaces.\n    print(format_value(results))\n\nsolve()\n```"
        },
        {
            "introduction": "A correctly implemented Monte Carlo algorithm must satisfy the detailed balance condition to guarantee that it samples from the desired target probability distribution. While the CBMC algorithm is theoretically sound, implementation bugs can inadvertently violate this crucial property, leading to incorrect physical predictions. This advanced practice  demonstrates how to move from theoretical proof to practical validation, guiding you to set up a numerical test on a toy system to confirm that your implementation indeed satisfies detailed balance within statistical certainty.",
            "id": "3745237",
            "problem": "Consider a two-step polymer chain on a square lattice with steps restricted to the set $\\{\\text{U},\\text{R}\\}$. A chain configuration is a sequence of two steps, yielding four possible states: $\\text{UU}$, $\\text{UR}$, $\\text{RU}$, and $\\text{RR}$. The chain energy $U(x)$ of a state $x$ is defined as the sum of per-step directional energies and a bending penalty applied on the second step. Specifically, let the first-step directional energies be $\\varepsilon_{\\text{U}} = 0$ and $\\varepsilon_{\\text{R}} = 0.5$. For the second step, add a bending penalty of $0$ if the second step equals the first, and $1$ if it differs. The total energy is then\n$$\nU(x) = \\varepsilon_{d_1} + \\varepsilon_{d_2} + \\text{bend}(d_1,d_2),\n$$\nwhere $x = (d_1,d_2)$ with $d_1,d_2 \\in \\{\\text{U},\\text{R}\\}$ and $\\text{bend}(d_1,d_2) = 0$ if $d_2 = d_1$ and $1$ otherwise.\n\nLet the target stationary distribution be the canonical Boltzmann distribution at inverse temperature $\\beta$, defined (up to a normalizing constant $Z$) by\n$$\n\\pi(x) = \\frac{1}{Z}\\exp\\big(-\\beta U(x)\\big).\n$$\nA Markov Chain Monte Carlo (MCMC) algorithm is used to sample from $\\pi(x)$ via a regrowth move inspired by Configurational Bias Monte Carlo (CBMC). The CBMC proposal grows the chain in two stages using Rosenbluth weighting:\n- At the first step, draw $d_1 \\in \\{\\text{U},\\text{R}\\}$ with probability proportional to $\\exp\\big(-\\beta \\varepsilon_{d_1}\\big)$, with normalization\n$$\nW_1 = \\exp\\big(-\\beta \\varepsilon_{\\text{U}}\\big) + \\exp\\big(-\\beta \\varepsilon_{\\text{R}}\\big).\n$$\n- At the second step, given $d_1$, draw $d_2 \\in \\{\\text{U},\\text{R}\\}$ with probability proportional to $\\exp\\big(-\\beta(\\varepsilon_{d_2} + \\text{bend}(d_1,d_2))\\big)$, with normalization\n$$\nW_2(d_1) = \\exp\\big(-\\beta(\\varepsilon_{\\text{U}} + \\text{bend}(d_1,\\text{U}))\\big) + \\exp\\big(-\\beta(\\varepsilon_{\\text{R}} + \\text{bend}(d_1,\\text{R}))\\big).\n$$\nThis yields a proposal distribution $q(x)$ that is proportional to $\\exp\\big(-\\beta U(x)\\big)/(W_1 W_2(d_1))$.\n\nUsing the Metropolis-Hastings principle, derive an acceptance probability that ensures detailed balance with respect to $\\pi(x)$, given the CBMC proposal structure above. Then implement a numerical validation of detailed balance by estimating the ratio\n$$\nR(x,x') = \\frac{\\pi(x) P(x \\to x')}{\\pi(x') P(x' \\to x)},\n$$\nwhere $P(x \\to x')$ is the one-step transition probability under the constructed Markov chain. The estimate must be obtained from sampled transitions in a long simulation. You must:\n- Implement the CBMC proposal mechanism precisely as described.\n- Use the derived acceptance probability to perform regrowth moves in a discrete-time Markov chain on the four states $\\{\\text{UU},\\text{UR},\\text{RU},\\text{RR}\\}$.\n- From the sampled transitions, estimate $P(x \\to x')$ as the fraction of accepted transitions from state $x$ to state $x'$ over the number of proposals made while in state $x$. To ensure robustness, use the Jeffreys smoothed estimator\n$$\n\\widehat{P}(x \\to x') = \\frac{K(x \\to x') + 0.5}{N(x) + 1},\n$$\nwhere $N(x)$ is the number of proposals attempted while the chain is in state $x$, and $K(x \\to x')$ is the number of accepted transitions from $x$ to $x'$.\n- Form the empirical ratio\n$$\n\\widehat{R}(x,x') = \\frac{\\pi(x) \\widehat{P}(x \\to x')}{\\pi(x') \\widehat{P}(x' \\to x)} = \\exp\\big(-\\beta(U(x) - U(x'))\\big)\\cdot \\frac{\\widehat{P}(x \\to x')}{\\widehat{P}(x' \\to x)}.\n$$\n- Construct a $95\\%$ confidence interval for $\\log \\widehat{R}(x,x')$ using a delta-method approximation for the binomial proportions, treating $\\log \\widehat{P}$ as approximately normal:\n$$\n\\operatorname{Var}\\big(\\log \\widehat{P}(x \\to x')\\big) \\approx \\frac{1 - \\widehat{P}(x \\to x')}{\\widehat{P}(x \\to x')\\cdot (N(x) + 1)}.\n$$\nAssume independence between the forward and reverse estimates and sum the variances to approximate $\\operatorname{Var}\\big(\\log \\widehat{R}(x,x')\\big)$. Then transform the confidence interval back to the original scale to test inclusion of $1$.\n\nYour program must run three test cases that differ in inverse temperature $\\beta$, sample length, and state pairs $(x,x')$:\n- Test case $1$: $\\beta = 1.0$, number of proposals $N_{\\text{sim}} = 200000$, pair $(x,x') = (\\text{UU},\\text{UR})$.\n- Test case $2$: $\\beta = 0.1$, number of proposals $N_{\\text{sim}} = 100000$, pair $(x,x') = (\\text{RR},\\text{RU})$.\n- Test case $3$: $\\beta = 3.0$, number of proposals $N_{\\text{sim}} = 150000$, pair $(x,x') = (\\text{UU},\\text{RR})$.\n\nFor each test case, return a boolean indicating whether the $95\\%$ confidence interval for $\\widehat{R}(x,x')$ includes $1$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true]\" but using Python boolean capitalization), in the exact order of the test cases listed above. No physical units are involved; all quantities are dimensionless. Angles are not used. Percentages must not be used; all fractional quantities must be expressed as decimals.",
            "solution": "The problem requires the derivation of a Metropolis-Hastings acceptance probability for a configurational bias Monte Carlo (CBMC) regrowth move and a subsequent numerical validation of the detailed balance condition.\n\nFirst, we establish the theoretical foundation. The goal of the Markov Chain Monte Carlo (MCMC) simulation is to generate a sequence of states $x$ drawn from the canonical Boltzmann distribution, $\\pi(x) \\propto \\exp(-\\beta U(x))$, where $U(x)$ is the energy of state $x$ and $\\beta$ is the inverse temperature. A sufficient condition for the chain's stationary distribution to be $\\pi(x)$ is the principle of detailed balance, which states that for any two states $x$ and $x'$, the steady-state transition rates must be equal:\n$$\n\\pi(x) P(x \\to x') = \\pi(x') P(x' \\to x)\n$$\nHere, $P(x \\to x')$ is the one-step transition probability from state $x$ to $x'$. In the Metropolis-Hastings algorithm, this transition probability is a product of a proposal probability $q(x \\to x')$ and an acceptance probability $A(x \\to x')$: $P(x \\to x') = q(x \\to x') A(x \\to x')$. The acceptance probability is designed to enforce detailed balance and is given by:\n$$\nA(x \\to x') = \\min\\left(1, \\frac{\\pi(x') q(x' \\to x)}{\\pi(x) q(x \\to x')}\\right)\n$$\n\nThe system consists of a $2$-step polymer chain with step directions $d_1, d_2 \\in \\{\\text{U}, \\text{R}\\}$. The four possible states are $x_{\\text{UU}}=(\\text{U},\\text{U})$, $x_{\\text{UR}}=(\\text{U},\\text{R})$, $x_{\\text{RU}}=(\\text{R},\\text{U})$, and $x_{\\text{RR}}=(\\text{R},\\text{R})$. The energy function is $U(x) = \\varepsilon_{d_1} + \\varepsilon_{d_2} + \\text{bend}(d_1,d_2)$, with $\\varepsilon_{\\text{U}}=0$, $\\varepsilon_{\\text{R}}=0.5$, and the bending penalty $\\text{bend}(d_1, d_2)$ being $1$ if $d_1 \\neq d_2$ and $0$ otherwise. The energies for the four states are:\n- $U(x_{\\text{UU}}) = \\varepsilon_{\\text{U}} + \\varepsilon_{\\text{U}} + \\text{bend}(\\text{U},\\text{U}) = 0 + 0 + 0 = 0$\n- $U(x_{\\text{UR}}) = \\varepsilon_{\\text{U}} + \\varepsilon_{\\text{R}} + \\text{bend}(\\text{U},\\text{R}) = 0 + 0.5 + 1 = 1.5$\n- $U(x_{\\text{RU}}) = \\varepsilon_{\\text{R}} + \\varepsilon_{\\text{U}} + \\text{bend}(\\text{R},\\text{U}) = 0.5 + 0 + 1 = 1.5$\n- $U(x_{\\text{RR}}) = \\varepsilon_{\\text{R}} + \\varepsilon_{\\text{R}} + \\text{bend}(\\text{R},\\text{R}) = 0.5 + 0.5 + 0 = 1.0$\n\nThe proposal mechanism is a full regrowth of the chain. This means the proposed state $x'$ is generated from scratch, independent of the current state $x$. The proposal distribution $q(x \\to x')$ is therefore a function only of the proposed state $x'$, let's call it $g(x')$. The probability of generating a specific new chain $x'=(d'_1, d'_2)$ is the product of the probabilities of choosing each step, as defined by the CBMC procedure:\n$$\ng(x') = \\left( \\frac{\\exp(-\\beta \\varepsilon_{d'_1})}{W_1} \\right) \\left( \\frac{\\exp(-\\beta(\\varepsilon_{d'_2} + \\text{bend}(d'_1, d'_2)))}{W_2(d'_1)} \\right)\n$$\nwhere $W_1$ and $W_2(d'_1)$ are the Rosenbluth weights for the first and second growth stages, respectively. Combining terms, we see that $g(x') = \\frac{\\exp(-\\beta U(x'))}{W_1 W_2(d'_1)}$. Let's define the total Rosenbluth weight for generating a configuration $x'=(d'_1, d'_2)$ as $W(x') = W_1 W_2(d'_1)$. Thus, the proposal probability is $g(x') = \\exp(-\\beta U(x')) / W(x')$.\n\nFor this independent sampler, $q(x \\to x') = g(x')$ and the reverse proposal probability is $q(x' \\to x) = g(x)$. Substituting these and $\\pi(x) \\propto \\exp(-\\beta U(x))$ into the Metropolis-Hastings acceptance rule:\n$$\nA(x \\to x') = \\min\\left(1, \\frac{\\exp(-\\beta U(x')) g(x)}{\\exp(-\\beta U(x)) g(x')}\\right) = \\min\\left(1, \\frac{\\exp(-\\beta U(x')) [\\exp(-\\beta U(x))/W(x)]}{\\exp(-\\beta U(x)) [\\exp(-\\beta U(x'))/W(x')]}\\right)\n$$\n$$\nA(x \\to x') = \\min\\left(1, \\frac{W(x')}{W(x)}\\right)\n$$\nThis is the standard acceptance probability for a CBMC full regrowth move. The Rosenbluth weight for a configuration $x=(d_1, d_2)$ is $W(x) = W_1 W_2(d_1)$.\n$W_1 = \\exp(-\\beta \\varepsilon_{\\text{U}}) + \\exp(-\\beta \\varepsilon_{\\text{R}}) = 1 + \\exp(-0.5\\beta)$.\nThe second-stage weight $W_2(d_1)$ depends on the first step $d_1$:\n- For $d_1 = \\text{U}$: $W_2(\\text{U}) = \\exp(-\\beta(\\varepsilon_{\\text{U}} + 0)) + \\exp(-\\beta(\\varepsilon_{\\text{R}} + 1)) = 1 + \\exp(-\\beta(0.5+1)) = 1 + \\exp(-1.5\\beta)$.\n- For $d_1 = \\text{R}$: $W_2(\\text{R}) = \\exp(-\\beta(\\varepsilon_{\\text{U}} + 1)) + \\exp(-\\beta(\\varepsilon_{\\text{R}} + 0)) = \\exp(-\\beta) + \\exp(-0.5\\beta)$.\nSince $W(x')/W(x) = (W_1 W_2(d'_1))/(W_1 W_2(d_1)) = W_2(d'_1)/W_2(d_1)$, the acceptance probability simplifies to:\n$$\nA(x \\to x') = \\min\\left(1, \\frac{W_2(d'_1)}{W_2(d_1)}\\right)\n$$\nwhere $d_1$ is the first step of the old state $x$ and $d'_1$ is the first step of the proposed state $x'$.\n\nTo validate detailed balance numerically, we simulate the Markov chain for a large number of steps, $N_{\\text{sim}}$. We record the number of times a proposal is made from each state $x$, denoted $N(x)$, and the number of accepted transitions from $x$ to $x'$, denoted $K(x \\to x')$. The transition probability $P(x \\to x')$ is estimated using the Jeffreys smoothed estimator:\n$$\n\\widehat{P}(x \\to x') = \\frac{K(x \\to x') + 0.5}{N(x) + 1}\n$$\nDetailed balance requires $\\pi(x) P(x \\to x') = \\pi(x') P(x' \\to x)$, or equivalently, the ratio $R(x,x') = \\frac{\\pi(x) P(x \\to x')}{\\pi(x') P(x' \\to x)}$ must be equal to $1$. We test this by forming the empirical ratio:\n$$\n\\widehat{R}(x,x') = \\frac{\\pi(x) \\widehat{P}(x \\to x')}{\\pi(x') \\widehat{P}(x' \\to x)} = \\frac{\\exp(-\\beta U(x)) \\widehat{P}(x \\to x')}{\\exp(-\\beta U(x')) \\widehat{P}(x' \\to x)} = \\exp\\big(-\\beta(U(x) - U(x'))\\big)\\cdot \\frac{\\widehat{P}(x \\to x')}{\\widehat{P}(x' \\to x)}\n$$\nWe analyze this ratio in logarithmic space, as this transforms the product into a sum, simplifying variance calculations. The log-ratio is $\\log\\widehat{R}(x,x') = -\\beta(U(x) - U(x')) + \\log\\widehat{P}(x \\to x') - \\log\\widehat{P}(x' \\to x)$. We construct a confidence interval for $\\log\\widehat{R}(x,x')$ at the $0.95$ confidence level. The variance of the logarithm of the probability estimator is approximated using the delta method, as specified:\n$$\n\\operatorname{Var}\\big(\\log \\widehat{P}(x \\to x')\\big) \\approx \\frac{1 - \\widehat{P}(x \\to x')}{\\widehat{P}(x \\to x')\\cdot (N(x) + 1)}\n$$\nAssuming the forward and reverse transition estimates are independent, their variances add: $\\operatorname{Var}(\\log \\widehat{R}) \\approx \\operatorname{Var}(\\log \\widehat{P}(x \\to x')) + \\operatorname{Var}(\\log \\widehat{P}(x' \\to x))$. The confidence interval for $\\log\\widehat{R}$ is $[\\log\\widehat{R} - z_{0.975} \\sigma_{\\log\\widehat{R}}, \\log\\widehat{R} + z_{0.975} \\sigma_{\\log\\widehat{R}}]$, where $\\sigma_{\\log\\widehat{R}}$ is the standard deviation and $z_{0.975} \\approx 1.96$ is the critical value for the standard normal distribution. Detailed balance is considered validated if this interval contains $0$, which is equivalent to the confidence interval for $\\widehat{R}$ containing $1$.\n\nThe implementation will proceed by first setting up the constants for the $4$ states. Then, for each test case, it will run the MCMC simulation. At each step, a new state is proposed via the 2-stage CBMC growth process, the acceptance probability is computed using the ratio of $W_2$ weights, and the move is accepted or rejected. Counters for $N(x)$ and $K(x \\to x')$ are updated accordingly. After the simulation, the statistical analysis is performed for the specified pair $(x, x')$ to determine if the confidence interval for the log-ratio contains $0$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for three test cases\n    and checking the detailed balance condition.\n    \"\"\"\n    test_cases = [\n        (1.0, 200000, ('UU', 'UR')),\n        (0.1, 100000, ('RR', 'RU')),\n        (3.0, 150000, ('UU', 'RR')),\n    ]\n\n    results = []\n    for beta, n_sim, pair in test_cases:\n        result = run_and_analyze_case(beta, n_sim, pair)\n        results.append(result)\n\n    # Format the final output as a string with lowercase booleans\n    # The problem description is slightly ambiguous here.\n    # \"[true,false,true]\" but using Python boolean capitalization.\n    # str(True) -> 'True'. This seems the most direct interpretation.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_and_analyze_case(beta, n_sim, pair):\n    \"\"\"\n    Runs a single test case simulation and performs the statistical analysis.\n    \"\"\"\n    # State mapping and properties\n    states_map = {'UU': 0, 'UR': 1, 'RU': 2, 'RR': 3}\n    energies = np.array([0.0, 1.5, 1.5, 1.0])\n    first_steps = ['U', 'U', 'R', 'R']  # d_1 for each state index\n\n    # Pre-calculate values dependent on beta\n    exp_m05b = np.exp(-0.5 * beta)\n    exp_m10b = np.exp(-1.0 * beta)\n    exp_m15b = np.exp(-1.5 * beta)\n    \n    # Rosenbluth weights for the second step\n    w2_vals = {\n        'U': 1.0 + exp_m15b,\n        'R': exp_m10b + exp_m05b\n    }\n\n    # Proposal probabilities for CBMC regrowth\n    p_d1_U = 1.0 / (1.0 + exp_m05b)\n    p_d2_U_given_d1_U = 1.0 / w2_vals['U']\n    p_d2_U_given_d1_R = exp_m10b / w2_vals['R']\n\n    def propose_new_state():\n        # Step 1: Propose d1\n        d1_prime = 'U' if np.random.rand() < p_d1_U else 'R'\n        \n        # Step 2: Propose d2\n        if d1_prime == 'U':\n            d2_prime = 'U' if np.random.rand() < p_d2_U_given_d1_U else 'R'\n        else:  # d1_prime == 'R'\n            d2_prime = 'U' if np.random.rand() < p_d2_U_given_d1_R else 'R'\n            \n        return states_map[d1_prime + d2_prime]\n\n    # MCMC Simulation\n    # Start at an arbitrary state (e.g., UU)\n    current_state_idx = states_map['UU']\n    # N(x): number of proposals from state x\n    N_counts = np.zeros(4, dtype=np.int64)\n    # K(x -> x'): number of accepted transitions\n    K_counts = np.zeros((4, 4), dtype=np.int64)\n\n    for _ in range(n_sim):\n        old_state_idx = current_state_idx\n        N_counts[old_state_idx] += 1\n\n        proposed_state_idx = propose_new_state()\n        \n        # Acceptance probability: min(1, W2(d'_1) / W2(d_1))\n        d1_old = first_steps[old_state_idx]\n        d1_new = first_steps[proposed_state_idx]\n        \n        w2_old = w2_vals[d1_old]\n        w2_new = w2_vals[d1_new]\n\n        acceptance_prob = min(1.0, w2_new / w2_old)\n\n        if np.random.rand() < acceptance_prob:\n            # Accept the move\n            current_state_idx = proposed_state_idx\n            K_counts[old_state_idx, proposed_state_idx] += 1\n        else:\n            # Reject the move, stay in the same state\n            K_counts[old_state_idx, old_state_idx] += 1\n            # current_state_idx remains old_state_idx\n\n    # Post-simulation analysis for the specified pair (x, x')\n    x_name, xp_name = pair\n    x_idx, xp_idx = states_map[x_name], states_map[xp_name]\n\n    # Jeffreys smoothed estimators for transition probabilities\n    N_x = N_counts[x_idx]\n    N_xp = N_counts[xp_idx]\n    \n    K_x_to_xp = K_counts[x_idx, xp_idx]\n    K_xp_to_x = K_counts[xp_idx, x_idx]\n    \n    # +1 ensures no division by zero if a state is never visited (unlikely for large n_sim)\n    p_hat_fwd = (K_x_to_xp + 0.5) / (N_x + 1)\n    p_hat_rev = (K_xp_to_x + 0.5) / (N_xp + 1)\n    \n    # Log of the empirical ratio R_hat\n    U_x = energies[x_idx]\n    U_xp = energies[xp_idx]\n    log_R_hat = -beta * (U_x - U_xp) + np.log(p_hat_fwd) - np.log(p_hat_rev)\n\n    # Variance of log_R_hat using the provided formula\n    var_log_p_fwd = (1.0 - p_hat_fwd) / (p_hat_fwd * (N_x + 1.0))\n    var_log_p_rev = (1.0 - p_hat_rev) / (p_hat_rev * (N_xp + 1.0))\n    \n    var_log_R_hat = var_log_p_fwd + var_log_p_rev\n    std_log_R_hat = np.sqrt(var_log_R_hat)\n    \n    # Construct 95% CI for log(R_hat)\n    z_score = norm.ppf(0.975)  # for 95% confidence\n    ci_lower = log_R_hat - z_score * std_log_R_hat\n    ci_upper = log_R_hat + z_score * std_log_R_hat\n    \n    # Check if the CI for log(R) contains 0 (which means CI for R contains 1)\n    return (ci_lower <= 0) and (ci_upper >= 0)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}