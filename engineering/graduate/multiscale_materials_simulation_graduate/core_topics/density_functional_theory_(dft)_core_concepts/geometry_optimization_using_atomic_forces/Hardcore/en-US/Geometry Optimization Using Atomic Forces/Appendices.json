{
    "hands_on_practices": [
        {
            "introduction": "This foundational exercise establishes the core principle of geometry optimization using the classic Lennard-Jones potential for a simple dimer. By analytically determining the equilibrium distance from the condition that the force is zero ($dE/dr=0$), and confirming stability through the potential's curvature ($d^2E/dr^2 \\gt 0$), you will build a concrete understanding of how the potential energy surface dictates stable molecular structures. This practice bridges the abstract concept of a potential energy minimum with tangible physical properties like bond length and vibrational frequency. ",
            "id": "3811712",
            "problem": "In multiscale materials simulation, coarse-grained pair interactions such as the Lennard–Jones potential are used to model atomic forces and perform geometry optimization. Consider a two-atom system (a dimer) with center-of-mass motion removed, interacting via the Lennard–Jones potential\n$$\nE(r) = 4 \\varepsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right],\n$$\nwhere $r$ is the interatomic separation, $\\varepsilon$ sets the energy scale, and $\\sigma$ sets the length scale. The interatomic force is $F(r) = -\\frac{dE}{dr}$, and a mechanically stable equilibrium geometry corresponds to a stationary point of $E(r)$ with positive curvature.\n\nStarting only from Newton’s second law and the definition of the potential energy function $E(r)$, do the following:\n- Determine the equilibrium separation $r_{\\mathrm{eq}}$ by solving $\\frac{dE}{dr}=0$ and verify that the curvature $\\frac{d^{2}E}{dr^{2}}$ at $r_{\\mathrm{eq}}$ is positive, indicating a local minimum suitable for geometry optimization using atomic forces.\n- Linearize the relative motion about $r_{\\mathrm{eq}}$ using the harmonic approximation obtained from a Taylor expansion of $E(r)$ and derive the small-amplitude vibrational angular frequency $\\omega$ of the dimer in terms of the reduced mass $\\mu$ of the two-atom system and the curvature at $r_{\\mathrm{eq}}$.\n\nReport your final result as the dimensionless row vector\n$$\n\\left( \\frac{r_{\\mathrm{eq}}}{\\sigma} \\;\\; , \\;\\; \\frac{\\omega}{\\sqrt{\\varepsilon/(\\mu \\sigma^{2})}} \\right),\n$$\nso that no unit conversion is required. You must provide a single, closed-form analytic expression. Do not round or approximate your result.",
            "solution": "The user has provided a valid, well-posed problem in classical mechanics and materials modeling. The problem is scientifically grounded, self-contained, and calls for a standard derivation.\n\nThe first task is to determine the equilibrium separation $r_{\\mathrm{eq}}$ and verify the stability of this equilibrium. The potential energy of the two-atom system is given by the Lennard–Jones potential:\n$$\nE(r) = 4 \\varepsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right]\n$$\nThis can be rewritten for easier differentiation as:\n$$\nE(r) = 4 \\varepsilon \\left[ \\sigma^{12}r^{-12} - \\sigma^{6}r^{-6} \\right]\n$$\nAn equilibrium position occurs where the net force is zero. The force $F(r)$ is the negative gradient of the potential energy, $F(r) = -\\frac{dE}{dr}$. Thus, the equilibrium condition is $\\frac{dE}{dr} = 0$.\n\nWe compute the first derivative of $E(r)$ with respect to $r$:\n$$\n\\frac{dE}{dr} = 4 \\varepsilon \\left[ \\sigma^{12}(-12)r^{-13} - \\sigma^{6}(-6)r^{-7} \\right] = 4 \\varepsilon \\left[ -12\\sigma^{12}r^{-13} + 6\\sigma^{6}r^{-7} \\right]\n$$\nSetting this derivative to zero to find the equilibrium separation $r_{\\mathrm{eq}}$:\n$$\n4 \\varepsilon \\left[ -12\\sigma^{12}r_{\\mathrm{eq}}^{-13} + 6\\sigma^{6}r_{\\mathrm{eq}}^{-7} \\right] = 0\n$$\nSince $4\\varepsilon \\neq 0$, the term in the brackets must be zero:\n$$\n12\\sigma^{12}r_{\\mathrm{eq}}^{-13} = 6\\sigma^{6}r_{\\mathrm{eq}}^{-7}\n$$\nDividing both sides by $6\\sigma^{6}r_{\\mathrm{eq}}^{-13}$ (assuming $r_{\\mathrm{eq}} \\neq 0$, $\\sigma \\neq 0$):\n$$\n2\\sigma^{6} = r_{\\mathrm{eq}}^{6}\n$$\nSolving for $r_{\\mathrm{eq}}$ gives:\n$$\nr_{\\mathrm{eq}} = (2\\sigma^{6})^{1/6} = 2^{1/6}\\sigma\n$$\nThe first component of the required dimensionless vector is $\\frac{r_{\\mathrm{eq}}}{\\sigma}$, which is:\n$$\n\\frac{r_{\\mathrm{eq}}}{\\sigma} = 2^{1/6}\n$$\nNext, we must verify that this equilibrium is stable by checking if the curvature of the potential, $\\frac{d^{2}E}{dr^{2}}$, is positive at $r = r_{\\mathrm{eq}}$. This indicates a local minimum. We differentiate $\\frac{dE}{dr}$ with respect to $r$:\n$$\n\\frac{d^{2}E}{dr^{2}} = 4 \\varepsilon \\left[ -12\\sigma^{12}(-13)r^{-14} + 6\\sigma^{6}(-7)r^{-8} \\right] = 4 \\varepsilon \\left[ 156\\sigma^{12}r^{-14} - 42\\sigma^{6}r^{-8} \\right]\n$$\nNow, we evaluate this expression at $r = r_{\\mathrm{eq}}$. We use the relation $r_{\\mathrm{eq}}^{6} = 2\\sigma^{6}$:\n$$\n\\frac{d^{2}E}{dr^{2}}\\bigg|_{r_{\\mathrm{eq}}} = 4 \\varepsilon \\left[ 156\\sigma^{12}(r_{\\mathrm{eq}}^{14})^{-1} - 42\\sigma^{6}(r_{\\mathrm{eq}}^{8})^{-1} \\right] = 4 \\varepsilon \\left[ 156\\sigma^{12}(r_{\\mathrm{eq}}^{6})^{-14/6} - 42\\sigma^{6}(r_{\\mathrm{eq}}^{6})^{-8/6} \\right]\n$$\nSubstituting $r_{eq}^6 = 2\\sigma^6$:\n$$\n= 4 \\varepsilon \\left[ 156\\sigma^{12}(2\\sigma^6)^{-7/3} - 42\\sigma^6(2\\sigma^6)^{-4/3} \\right] = 4 \\varepsilon \\left[ 156\\sigma^{12}2^{-7/3}\\sigma^{-14} - 42\\sigma^6 2^{-4/3}\\sigma^{-8} \\right]\n$$\n$$\n= \\frac{4\\varepsilon}{\\sigma^2} \\left[ 156 \\cdot 2^{-7/3} - 42 \\cdot 2^{-4/3} \\right] = \\frac{4\\varepsilon}{\\sigma^2} 2^{-4/3} \\left[ 156 \\cdot 2^{-1} - 42 \\right] = \\frac{4\\varepsilon}{\\sigma^2} 2^{-4/3} [78 - 42] = \\frac{4\\varepsilon}{\\sigma^2} \\frac{36}{2^{4/3}}\n$$\nSince $\\varepsilon > 0$, the curvature is positive, confirming that $r_{\\mathrm{eq}}$ corresponds to a stable equilibrium (a local energy minimum).\n\nThe second task is to find the small-amplitude vibrational angular frequency $\\omega$. The motion of the dimer about its center of mass can be described by a single particle of reduced mass $\\mu$ moving in the potential $E(r)$. Newton's second law for the relative coordinate $r$ is:\n$$\n\\mu \\frac{d^{2}r}{dt^{2}} = F(r) = -\\frac{dE}{dr}\n$$\nFor small oscillations around $r_{\\mathrm{eq}}$, we expand the potential energy $E(r)$ in a Taylor series about $r = r_{\\mathrm{eq}}$. Let $x = r - r_{\\mathrm{eq}}$ be the small displacement from equilibrium:\n$$\nE(r) = E(r_{\\mathrm{eq}}) + \\frac{dE}{dr}\\bigg|_{r_{\\mathrm{eq}}} x + \\frac{1}{2}\\frac{d^{2}E}{dr^{2}}\\bigg|_{r_{\\mathrm{eq}}} x^{2} + \\dots\n$$\nSince $\\frac{dE}{dr}\\big|_{r_{\\mathrm{eq}}}=0$, and ignoring higher-order terms for small displacements (the harmonic approximation), the potential energy is approximately:\n$$\nE(x) \\approx E(r_{\\mathrm{eq}}) + \\frac{1}{2} k x^{2}\n$$\nwhere $k = \\frac{d^{2}E}{dr^{2}}\\big|_{r_{\\mathrm{eq}}}$ is the effective spring constant. The constant term $E(r_{\\mathrm{eq}})$ does not affect the dynamics. The equation of motion becomes:\n$$\n\\mu \\frac{d^{2}x}{dt^{2}} = -\\frac{d}{dx}\\left(\\frac{1}{2}kx^{2}\\right) = -kx\n$$\nThis is the equation for a simple harmonic oscillator, $\\frac{d^{2}x}{dt^{2}} + \\frac{k}{\\mu}x = 0$, for which the angular frequency $\\omega$ is given by:\n$$\n\\omega = \\sqrt{\\frac{k}{\\mu}}\n$$\nWe have already calculated the spring constant $k$:\n$$\nk = \\frac{d^{2}E}{dr^{2}}\\bigg|_{r_{\\mathrm{eq}}} = \\frac{144 \\varepsilon}{\\sigma^{2} \\cdot 2^{4/3}}\n$$\nLet's simplify this expression. Since $144 = 36 \\times 4 = 36 \\times 2^{2}$:\n$$\nk = \\frac{36 \\cdot 2^{2} \\varepsilon}{\\sigma^{2} \\cdot 2^{4/3}} = 36 \\cdot 2^{2-4/3} \\frac{\\varepsilon}{\\sigma^{2}} = 36 \\cdot 2^{2/3} \\frac{\\varepsilon}{\\sigma^{2}}\n$$\nNow, we can find $\\omega$:\n$$\n\\omega^{2} = \\frac{k}{\\mu} = \\frac{36 \\cdot 2^{2/3} \\varepsilon}{\\mu\\sigma^{2}}\n$$\n$$\n\\omega = \\sqrt{\\frac{36 \\cdot 2^{2/3} \\varepsilon}{\\mu\\sigma^{2}}} = 6 \\cdot (2^{2/3})^{1/2} \\sqrt{\\frac{\\varepsilon}{\\mu\\sigma^{2}}} = 6 \\cdot 2^{1/3} \\sqrt{\\frac{\\varepsilon}{\\mu\\sigma^{2}}}\n$$\nThe second component of the required dimensionless vector is $\\frac{\\omega}{\\sqrt{\\varepsilon/(\\mu \\sigma^{2})}}$:\n$$\n\\frac{\\omega}{\\sqrt{\\varepsilon/(\\mu \\sigma^{2})}} = \\frac{6 \\cdot 2^{1/3} \\sqrt{\\varepsilon/(\\mu\\sigma^{2})}}{\\sqrt{\\varepsilon/(\\mu\\sigma^{2})}} = 6 \\cdot 2^{1/3}\n$$\nCombining the two dimensionless results into the specified row vector, we get:\n$$\n\\left( \\frac{r_{\\mathrm{eq}}}{\\sigma} \\;\\; , \\;\\; \\frac{\\omega}{\\sqrt{\\varepsilon/(\\mu \\sigma^{2})}} \\right) = \\left( 2^{1/6} \\;\\; , \\;\\; 6 \\cdot 2^{1/3} \\right)\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2^{1/6} & 6 \\cdot 2^{1/3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real potential energy surfaces are complex and feature not just minima but also saddle points, which correspond to transition states. This practice delves into the crucial algorithmic challenge of distinguishing between these stationary points to ensure convergence to a stable structure. By analyzing the behavior of various optimization algorithms on a toy model of a saddle point, you will gain insight into why a simple steepest-descent approach can fail and how more sophisticated methods like Newton's method and L-BFGS are designed to successfully navigate complex energy landscapes. ",
            "id": "3811750",
            "problem": "Consider the toy two-dimensional potential energy surface (PES) for a coupled pair of atomic degrees of freedom, modeled as a local quadratic approximation near a transition-state region in multiscale materials simulation, given by $$E(x,y)=x^{2}-y^{2}+\\alpha\\, x\\, y,$$ where $\\alpha\\in\\mathbb{R}$ controls the coupling of the modes. Geometry optimization based on atomic forces uses $\\mathbf{F}=-\\nabla E$ to step coordinates toward lower energy. Starting only from fundamental definitions, analyze the behavior of common optimization schemes in a neighborhood of the stationary point at the origin and propose strategies to avoid saddle trapping during minimization. Select all statements that are correct.\n\nA. For the continuous steepest-descent dynamics defined by $$\\dot{\\mathbf{r}}=-\\nabla E(\\mathbf{r}),\\quad \\mathbf{r}=(x,y),$$ the origin is a saddle-type equilibrium: in the direction associated with negative curvature of the Hessian, trajectories generically depart from the origin. Thus, pure steepest-descent dynamics does not get trapped at the origin.\n\nB. For discrete gradient descent with fixed step size $\\eta$ satisfying $$0<\\eta<\\frac{2}{\\sqrt{\\alpha^{2}+4}},$$ the origin is an attracting fixed point in all directions; therefore gradient descent is trapped by the saddle under these step-size conditions.\n\nC. Exact Newton’s method for minimization, with step $$\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\mathbf{H}^{-1}\\nabla E(\\mathbf{x}_{k}),$$ where $\\mathbf{H}$ is the Hessian at $\\mathbf{x}_{k}$, sends any starting point to the origin in a single iteration on this quadratic PES, thereby converging to the saddle and being unsafe for minimization unless modified.\n\nD. A trust-region modified Newton strategy that solves the local quadratic model $$\\min_{p:\\ \\|p\\|\\le \\Delta}\\ m(p)=g^{\\top}p+\\tfrac{1}{2}p^{\\top}\\mathbf{H}p,$$ and replaces $\\mathbf{H}$ by $\\mathbf{H}+\\mu \\mathbf{I}$ when $\\mathbf{H}$ is indefinite (choosing $\\mu>0$ large enough to make $\\mathbf{H}+\\mu \\mathbf{I}$ positive definite), will produce a descent step and avoid attraction to the saddle.\n\nE. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) with a line search enforcing the strong Wolfe conditions maintains a positive-definite inverse Hessian approximation and therefore generates descent directions; on this PES it will not converge to the saddle for generic initializations.\n\nF. Mass-weighted preconditioning based on a positive-definite matrix $\\mathbf{M}$ that rescales forces to $\\mathbf{M}^{-1}\\nabla E$ can always be chosen to render the effective Hessian $\\mathbf{M}^{-1}\\mathbf{H}$ positive definite, thereby eliminating saddle behavior.\n\nSelect all that apply.",
            "solution": "To analyze the behavior of different optimization schemes, we first characterize the stationary point at the origin for the potential energy surface $E(x,y) = x^2 - y^2 + \\alpha xy$. The gradient $\\nabla E$ and Hessian matrix $\\mathbf{H}$ are:\n$$\n\\nabla E = \\begin{pmatrix} 2x + \\alpha y \\\\ \\alpha x - 2y \\end{pmatrix}, \\quad \\mathbf{H} = \\nabla^2 E = \\begin{pmatrix} 2 & \\alpha \\\\ \\alpha & -2 \\end{pmatrix}\n$$\nThe gradient is zero only at the origin $(0,0)$, which is the unique stationary point. To classify this point, we find the eigenvalues of the Hessian by solving the characteristic equation $\\det(\\mathbf{H} - \\lambda\\mathbf{I}) = 0$:\n$$\n(2-\\lambda)(-2-\\lambda) - \\alpha^2 = \\lambda^2 - 4 - \\alpha^2 = 0 \\implies \\lambda = \\pm\\sqrt{4+\\alpha^2}\n$$\nSince $\\alpha \\in \\mathbb{R}$, we have one positive and one negative eigenvalue. This indicates that the origin is a saddle point.\n\nNow, we evaluate each statement:\n\n**A. Correct.** The continuous steepest-descent dynamics are described by the ODE system $\\dot{\\mathbf{r}} = -\\nabla E(\\mathbf{r}) = -\\mathbf{H}\\mathbf{r}$. The eigenvalues of the system matrix $-\\mathbf{H}$ are the negatives of the eigenvalues of $\\mathbf{H}$, which are $\\mp\\sqrt{4+\\alpha^2}$. Since there is one positive and one negative eigenvalue, the origin is a saddle-type equilibrium for the dynamics. Trajectories with any component along the unstable eigenvector (corresponding to the positive eigenvalue) will move away from the origin. For a generic starting point, this component will be non-zero, so the trajectory will depart from the origin. Thus, the dynamics do not get trapped.\n\n**B. Incorrect.** The discrete gradient descent update is $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\eta \\nabla E(\\mathbf{x}_k) = (\\mathbf{I} - \\eta \\mathbf{H})\\mathbf{x}_k$. The origin is an attracting fixed point only if all eigenvalues of the iteration matrix $\\mathbf{I} - \\eta \\mathbf{H}$ have magnitudes less than 1. The eigenvalues of this matrix are $1 - \\eta\\lambda_i$. For the negative eigenvalue of $\\mathbf{H}$, $\\lambda_2 = -\\sqrt{4+\\alpha^2}$, the corresponding eigenvalue of the iteration matrix is $1 - \\eta(-\\sqrt{4+\\alpha^2}) = 1 + \\eta\\sqrt{4+\\alpha^2}$. Since $\\eta > 0$, this value is always greater than 1. Therefore, the origin is an unstable fixed point, not an attracting one, regardless of the step size $\\eta$.\n\n**C. Correct.** For a quadratic potential, the gradient is a linear function of the coordinates, $\\nabla E(\\mathbf{x}) = \\mathbf{H}\\mathbf{x}$. Newton's method is $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\mathbf{H}^{-1}\\nabla E(\\mathbf{x}_k)$. Substituting the gradient gives $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\mathbf{H}^{-1}(\\mathbf{H}\\mathbf{x}_k) = \\mathbf{x}_k - \\mathbf{x}_k = \\mathbf{0}$. Thus, the method converges to the stationary point (the saddle) in a single step from any starting position. This is unsafe for minimization, as the goal is to find a minimum, not a saddle point.\n\n**D. Correct.** The strategy described is a Levenberg-Marquardt type modification commonly used in trust-region methods. When the Hessian $\\mathbf{H}$ is indefinite (as it is here), it is replaced by a positive-definite matrix $\\mathbf{H}' = \\mathbf{H} + \\mu\\mathbf{I}$ (by choosing $\\mu$ larger than the magnitude of the most negative eigenvalue of $\\mathbf{H}$). The resulting step direction $p = -(\\mathbf{H}')^{-1}g$ is a guaranteed descent direction because $(\\mathbf{H}')^{-1}$ is positive definite. An optimizer that always takes descent steps will move to lower energy regions, and thus will move away from a saddle point along the direction(s) of negative curvature. This avoids attraction to the saddle.\n\n**E. Correct.** The BFGS update procedure, when combined with a line search satisfying the strong Wolfe conditions, is designed to maintain a positive-definite approximation to the Hessian (or its inverse). The resulting search direction is therefore always a descent direction. As with the modified Newton method in (D), by consistently moving \"downhill\" in energy, the algorithm will naturally avoid converging to a saddle point for any generic starting point not on the stable manifold of the saddle.\n\n**F. Incorrect.** Preconditioning with a positive-definite matrix $\\mathbf{M}$ is equivalent to performing optimization in a transformed coordinate system. The effective Hessian becomes $\\mathbf{M}^{-1}\\mathbf{H}$. For a matrix to be positive definite, its determinant must be positive. The determinant of the effective Hessian is $\\det(\\mathbf{M}^{-1}\\mathbf{H}) = \\det(\\mathbf{M}^{-1})\\det(\\mathbf{H})$. Since $\\mathbf{M}$ is positive definite, $\\det(\\mathbf{M}^{-1}) > 0$. We calculated $\\det(\\mathbf{H}) = -4 - \\alpha^2  0$. Therefore, $\\det(\\mathbf{M}^{-1}\\mathbf{H})  0$. A matrix with a negative determinant cannot be positive definite. Thus, no such preconditioning matrix $\\mathbf{M}$ exists.",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "Moving from theory to practice, this hands-on coding exercise challenges you to implement and test the Fast Inertial Relaxation Engine (FIRE), a powerful and widely used geometry optimizer. This algorithm cleverly incorporates concepts from molecular dynamics, using adaptive time-stepping and velocity mixing to efficiently guide a system towards a potential energy minimum. By simulating the optimization process on a noisy potential, you will gain practical experience with the dynamics of structural relaxation and the robustness of modern algorithms in realistic computational environments. ",
            "id": "3811703",
            "problem": "Consider geometry optimization using atomic forces in a multiscale materials simulation setting, where the update rule is grounded in Newton's second law and a momentum alignment heuristic. Let the energy of a point-like atom in a two-dimensional Euclidean space be given by the quartic potential $$V(\\mathbf{r})=\\frac{k}{4}\\lVert \\mathbf{r}\\rVert^4,$$ where $\\mathbf{r}\\in\\mathbb{R}^2$ and $k0$ is a stiffness parameter. The exact atomic force is defined by the fundamental relationship $$\\mathbf{F}_{\\text{true}}(\\mathbf{r})=-\\nabla V(\\mathbf{r}).$$ In practical simulations, gradients are contaminated by measurement or discretization noise; model this by an observed gradient $$\\nabla V(\\mathbf{r})+\\boldsymbol{\\xi},$$ where $\\boldsymbol{\\xi}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I})$ is a Gaussian random vector with independent components of variance $\\sigma^2$, sampled independently at each iteration. The observed force used in the dynamics is $$\\mathbf{F}(\\mathbf{r})=-(\\nabla V(\\mathbf{r})+\\boldsymbol{\\xi}) = \\mathbf{F}_{\\text{true}}(\\mathbf{r})-\\boldsymbol{\\xi}.$$\n\nStarting from Newton's second law $$m\\frac{d\\mathbf{v}}{dt}=\\mathbf{F}(\\mathbf{r}),\\quad \\frac{d\\mathbf{r}}{dt}=\\mathbf{v},$$ consider a discrete-time scheme with time step $dt$ and mass $m0$. The update rule per iteration is based on explicit time stepping combined with the Fast Inertial Relaxation Engine (FIRE) mixing. At iteration $t$, let $\\mathbf{r}_t$ and $\\mathbf{v}_t$ be the position and velocity. Compute $\\mathbf{F}_t=\\mathbf{F}(\\mathbf{r}_t)$, then perform the explicit updates $$\\mathbf{v}_{t+\\frac{1}{2}}=\\mathbf{v}_t+\\frac{dt}{m}\\mathbf{F}_t,\\quad \\mathbf{r}_{t+1}=\\mathbf{r}_t+dt\\,\\mathbf{v}_{t+\\frac{1}{2}}.$$ Define the power $$P_t=\\mathbf{v}_{t+\\frac{1}{2}}\\cdot \\mathbf{F}_t.$$ Apply the FIRE alignment step to $\\mathbf{v}_{t+\\frac{1}{2}}$ as follows. Maintain a mixing parameter $\\alpha$ and an integer counter $n_{\\text{pos}}$ initialized to $\\alpha=\\alpha_0$ and $n_{\\text{pos}}=0$. If $P_t0$, update $$\\mathbf{v}_{t+1}=(1-\\alpha)\\,\\mathbf{v}_{t+\\frac{1}{2}}+\\alpha\\,\\frac{\\lVert \\mathbf{v}_{t+\\frac{1}{2}}\\rVert}{\\lVert \\mathbf{F}_t\\rVert}\\,\\mathbf{F}_t,$$ increase the time step by $$dt\\leftarrow \\min(f_{\\mathrm{inc}}\\cdot dt,\\,dt_{\\max}),$$ increment the counter $n_{\\text{pos}}\\leftarrow n_{\\text{pos}}+1$, and, whenever $n_{\\text{pos}}N_{\\mathrm{delay}}$, reduce $\\alpha$ by $$\\alpha\\leftarrow \\alpha\\cdot \\alpha_{\\mathrm{decay}}.$$ If $P_t\\le 0$, set $$\\mathbf{v}_{t+1}=\\mathbf{0},$$ decrease the time step by $$dt\\leftarrow f_{\\mathrm{dec}}\\cdot dt,$$ reset $$\\alpha\\leftarrow \\alpha_0,\\quad n_{\\text{pos}}\\leftarrow 0.$$\n\nUse the above scheme to simulate $N$ iterations from an initial state $\\mathbf{r}_0$ and $\\mathbf{v}_0=\\mathbf{0}$. For each simulation, define the contraction of the gradient norm after $N$ iterations as the ratio $$C=\\frac{\\mathbb{E}[\\lVert \\nabla V(\\mathbf{r}_N)\\rVert]}{\\lVert \\nabla V(\\mathbf{r}_0)\\rVert},$$ where the expectation is over the Gaussian noises $\\boldsymbol{\\xi}$ across all iterations. Estimate $\\mathbb{E}[\\lVert \\nabla V(\\mathbf{r}_N)\\rVert]$ by averaging over $M$ independent replicates with a fixed pseudorandom generator initialized to seed $42$ to ensure reproducibility. Note that $C$ is dimensionless; the problem is posed in dimensionless units.\n\nImplement the algorithm and compute $C$ for the following test suite. In all cases, take a two-dimensional system ($\\mathbf{r}\\in\\mathbb{R}^2$), set the initial velocity $\\mathbf{v}_0=\\mathbf{0}$, and perform $M$ independent replicates. Report each $C$ as a float rounded to six decimal places.\n\nTest suite:\n- Case $1$ (baseline without noise; happy path): $k=1$, $m=1$, $\\sigma=0$, $\\mathbf{r}_0=[2,0]$, $dt_0=0.1$, $dt_{\\max}=0.5$, $f_{\\mathrm{inc}}=1.1$, $f_{\\mathrm{dec}}=0.5$, $\\alpha_0=0.1$, $\\alpha_{\\mathrm{decay}}=0.99$, $N_{\\mathrm{delay}}=5$, $N=50$, $M=300$.\n- Case $2$ (moderate noise): $k=1$, $m=1$, $\\sigma=0.1$, $\\mathbf{r}_0=[2,0]$, $dt_0=0.1$, $dt_{\\max}=0.5$, $f_{\\mathrm{inc}}=1.1$, $f_{\\mathrm{dec}}=0.5$, $\\alpha_0=0.1$, $\\alpha_{\\mathrm{decay}}=0.99$, $N_{\\mathrm{delay}}=5$, $N=50$, $M=300$.\n- Case $3$ (high noise; robustness edge): $k=1$, $m=1$, $\\sigma=0.5$, $\\mathbf{r}_0=[2,0]$, $dt_0=0.1$, $dt_{\\max}=0.5$, $f_{\\mathrm{inc}}=1.1$, $f_{\\mathrm{dec}}=0.5$, $\\alpha_0=0.1$, $\\alpha_{\\mathrm{decay}}=0.99$, $N_{\\mathrm{delay}}=5$, $N=50$, $M=300$.\n- Case $4$ (near-stationary start; boundary of small gradients): $k=1$, $m=1$, $\\sigma=0.1$, $\\mathbf{r}_0=[0.2,-0.2]$, $dt_0=0.05$, $dt_{\\max}=0.5$, $f_{\\mathrm{inc}}=1.1$, $f_{\\mathrm{dec}}=0.5$, $\\alpha_0=0.1$, $\\alpha_{\\mathrm{decay}}=0.99$, $N_{\\mathrm{delay}}=5$, $N=50$, $M=300$.\n\nYour program should produce a single line of output containing the contraction results for the four cases as a comma-separated list enclosed in square brackets, ordered as listed above, for example $$[c_1,c_2,c_3,c_4],$$ where each $c_i$ is the rounded float. Angles are not used in this problem. Do not include any additional text in the output. All results are dimensionless numbers.",
            "solution": "The problem requires the implementation of a geometry optimization algorithm known as the Fast Inertial Relaxation Engine (FIRE) to find the minimum of a two-dimensional quartic potential, $V(\\mathbf{r})=\\frac{k}{4}\\lVert \\mathbf{r}\\rVert^4$. The simulation incorporates stochastic noise to model measurement or discretization errors. The objective is to compute the contraction of the gradient norm, $C$, after a fixed number of iterations for several test cases.\n\nFirst, we establish the analytical form of the gradient of the potential, which is essential for calculating the force. The potential energy is given by $V(\\mathbf{r})=\\frac{k}{4}\\lVert \\mathbf{r}\\rVert^4$, where $\\mathbf{r}=[x, y]^T \\in \\mathbb{R}^2$. The squared norm is $\\lVert \\mathbf{r}\\rVert^2 = x^2+y^2$. The potential can be written as $V(x,y) = \\frac{k}{4}(x^2+y^2)^2$. The gradient, $\\nabla V$, is the vector of partial derivatives:\n$$\n\\nabla V(\\mathbf{r}) = \\begin{pmatrix} \\frac{\\partial V}{\\partial x} \\\\ \\frac{\\partial V}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\frac{k}{4} \\cdot 2(x^2+y^2) \\cdot 2x \\\\ \\frac{k}{4} \\cdot 2(x^2+y^2) \\cdot 2y \\end{pmatrix} = \\begin{pmatrix} kx(x^2+y^2) \\\\ ky(x^2+y^2) \\end{pmatrix}\n$$\nThis can be expressed more compactly in vector notation as:\n$$\n\\nabla V(\\mathbf{r}) = k \\lVert \\mathbf{r} \\rVert^2 \\mathbf{r}\n$$\nThe true physical force, $\\mathbf{F}_{\\text{true}}$, is the negative of the potential gradient, a fundamental relationship in classical mechanics:\n$$\n\\mathbf{F}_{\\text{true}}(\\mathbf{r}) = -\\nabla V(\\mathbf{r}) = -k \\lVert \\mathbf{r} \\rVert^2 \\mathbf{r}\n$$\nThe problem specifies that the force used in the simulation is a noisy observation of the true force. The observed force, $\\mathbf{F}(\\mathbf{r})$, is modeled as:\n$$\n\\mathbf{F}(\\mathbf{r}) = \\mathbf{F}_{\\text{true}}(\\mathbf{r}) - \\boldsymbol{\\xi}\n$$\nwhere $\\boldsymbol{\\xi}$ is a random noise vector drawn from a two-dimensional Gaussian distribution with mean $\\mathbf{0}$ and covariance matrix $\\sigma^2\\mathbf{I}$, denoted as $\\boldsymbol{\\xi} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})$. This noise is sampled independently at each iteration.\n\nThe core of the simulation is the FIRE algorithm, which is a discrete-time integration scheme based on Newton's second law, $m\\ddot{\\mathbf{r}} = \\mathbf{F}$. The algorithm proceeds in iterations. For each iteration $t$, starting with position $\\mathbf{r}_t$ and velocity $\\mathbf{v}_t$, the update follows two main stages:\n\n1.  **Modified Velocity-Verlet Integration**: First, an explicit time-stepping scheme, similar to the velocity Verlet algorithm, is used to update the position and a half-step velocity. Given a time step $dt$ and mass $m$, the updates are:\n    $$\n    \\mathbf{v}_{t+\\frac{1}{2}}=\\mathbf{v}_t+\\frac{dt}{m}\\mathbf{F}_t\n    $$\n    $$\n    \\mathbf{r}_{t+1}=\\mathbf{r}_t+dt\\,\\mathbf{v}_{t+\\frac{1}{2}}\n    $$\n    where $\\mathbf{F}_t = \\mathbf{F}(\\mathbf{r}_t)$.\n\n2.  **FIRE Heuristic for Velocity and Parameter Adaptation**: The second stage adjusts the velocity for the next full step, $\\mathbf{v}_{t+1}$, and adapts the simulation parameters $dt$ and $\\alpha$. This stage is controlled by the sign of the power, $P_t = \\mathbf{v}_{t+\\frac{1}{2}} \\cdot \\mathbf{F}_t$. The power indicates whether the kinetic energy is increasing or decreasing, which in this context signals whether the particle is moving \"downhill\" toward the potential minimum or not.\n\n    -   If $P_t  0$: This indicates motion along the force direction. The algorithm accelerates.\n        -   The velocity is updated by mixing $\\mathbf{v}_{t+\\frac{1}{2}}$ with a vector of the same magnitude but aligned with the force $\\mathbf{F}_t$:\n            $$\n            \\mathbf{v}_{t+1}=(1-\\alpha)\\,\\mathbf{v}_{t+\\frac{1}{2}}+\\alpha\\,\\frac{\\lVert \\mathbf{v}_{t+\\frac{1}{2}}\\rVert}{\\lVert \\mathbf{F}_t\\rVert}\\,\\mathbf{F}_t\n            $$\n            This steering mechanism is the \"fast inertial relaxation\" component.\n        -   The time step is increased to accelerate progress, capped at $dt_{\\max}$: $dt \\leftarrow \\min(f_{\\mathrm{inc}} \\cdot dt, dt_{\\max})$.\n        -   A counter $n_{\\text{pos}}$ for consecutive positive-power steps is incremented. If this counter exceeds a delay threshold $N_{\\mathrm{delay}}$, the mixing parameter $\\alpha$ is decreased ($\\alpha \\leftarrow \\alpha \\cdot \\alpha_{\\mathrm{decay}}$) to make the steering less aggressive as the system presumably stabilizes.\n\n    -   If $P_t \\le 0$: This suggests the particle has overshot the minimum or is moving \"uphill\". The algorithm applies a strong correction.\n        -   The motion is stopped entirely by resetting the velocity: $\\mathbf{v}_{t+1} = \\mathbf{0}$.\n        -   The time step is drastically reduced: $dt \\leftarrow f_{\\mathrm{dec}} \\cdot dt$.\n        -   The state is reset for a fresh start: the counter is cleared ($n_{\\text{pos}} \\leftarrow 0$) and the mixing parameter is reset to its initial value ($\\alpha \\leftarrow \\alpha_0$).\n\nThe simulation runs for $N$ iterations starting from an initial position $\\mathbf{r}_0$ and zero initial velocity $\\mathbf{v}_0 = \\mathbf{0}$. The performance metric is the contraction ratio $C$:\n$$\nC=\\frac{\\mathbb{E}[\\lVert \\nabla V(\\mathbf{r}_N)\\rVert]}{\\lVert \\nabla V(\\mathbf{r}_0)\\rVert}\n$$\nThe expectation $\\mathbb{E}[\\cdot]$ is estimated via a Monte Carlo method, by averaging the final gradient norm, $\\lVert \\nabla V(\\mathbf{r}_N)\\rVert$, over $M$ independent simulation replicates. To ensure reproducibility, all replicates are run using a single pseudorandom number generator initialized with a fixed seed.\n\nThe implementation consists of a main function that iterates through the test cases. For each case, it calculates the constant denominator $\\lVert \\nabla V(\\mathbf{r}_0)\\rVert$. It then runs $M$ replicates of the $N$-step FIRE simulation. For each replicate, it re-initializes the state variables ($\\mathbf{r}$, $\\mathbf{v}$, $dt$, $\\alpha$, $n_{\\text{pos}}$) and runs the iterative algorithm. The final gradient norm is computed and stored. After all replicates are complete, the average of these norms is calculated to estimate $\\mathbb{E}[\\lVert \\nabla V(\\mathbf{r}_N)\\rVert]$. Finally, $C$ is computed and formatted.\nNumerical stability is considered. The term $\\frac{1}{\\lVert \\mathbf{F}_t\\rVert}$ could be problematic if $\\mathbf{F}_t$ is near zero. However, if $\\lVert\\mathbf{F}_t\\rVert$ is very small, then $P_t = \\mathbf{v}_{t+\\frac{1}{2}} \\cdot \\mathbf{F}_t$ will also be very small and likely $\\le 0$, thus avoiding the division in the $P_t0$ branch. A small tolerance is added in the implementation as a safeguard.",
            "answer": "```python\nimport numpy as np\n\ndef grad_V(r: np.ndarray, k: float) - np.ndarray:\n    \"\"\"Computes the gradient of the potential V(r) = k/4 * ||r||^4.\"\"\"\n    r_norm_sq = np.dot(r, r)\n    return k * r_norm_sq * r\n\ndef run_single_replicate(params: dict, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Runs a single simulation replicate for N iterations.\n    \"\"\"\n    # Unpack parameters\n    k = params['k']\n    m = params['m']\n    sigma = params['sigma']\n    r0 = np.array(params['r0'], dtype=float)\n    dt0 = params['dt0']\n    dt_max = params['dt_max']\n    f_inc = params['f_inc']\n    f_dec = params['f_dec']\n    alpha0 = params['alpha0']\n    alpha_decay = params['alpha_decay']\n    N_delay = params['N_delay']\n    N = params['N']\n\n    # Initialize state for the replicate\n    r = r0.copy()\n    v = np.zeros_like(r0)\n    dt = dt0\n    alpha = alpha0\n    n_pos = 0\n\n    for _ in range(N):\n        # Calculate force with noise\n        grad = grad_V(r, k)\n        noise = rng.normal(scale=sigma, size=r.shape)\n        F = -(grad + noise)\n\n        # Modified Velocity-Verlet step\n        v_half = v + (dt / m) * F\n        r = r + dt * v_half\n\n        # FIRE heuristic\n        P = np.dot(v_half, F)\n\n        if P  0:\n            F_norm = np.linalg.norm(F)\n            v_half_norm = np.linalg.norm(v_half)\n            \n            # Velocity mixing/steering\n            if F_norm  1e-15:  # Numerical stability\n                v_steered = (v_half_norm / F_norm) * F\n                v = (1 - alpha) * v_half + alpha * v_steered\n            else:\n                # This case is unlikely if P  0, but as a safeguard\n                v = (1 - alpha) * v_half\n            \n            # Adapt dt and alpha\n            dt = min(f_inc * dt, dt_max)\n            n_pos += 1\n            if n_pos  N_delay:\n                alpha *= alpha_decay\n        else: # P = 0\n            # Halt and reset\n            v = np.zeros_like(r)\n            dt = f_dec * dt\n            alpha = alpha0\n            n_pos = 0\n            \n    return r\n\ndef compute_contraction(params: dict) - float:\n    \"\"\"\n    Computes the contraction ratio C by averaging over M replicates.\n    \"\"\"\n    k = params['k']\n    r0 = np.array(params['r0'], dtype=float)\n    M = params['M']\n\n    # Calculate initial gradient norm (denominator of C)\n    grad_V_r0 = grad_V(r0, k)\n    norm_grad_V_r0 = np.linalg.norm(grad_V_r0)\n\n    # If initial gradient is already zero, the problem is trivial or ill-defined.\n    # The test cases ensure this is not the case.\n    if norm_grad_V_r0  1e-15:\n        return 0.0\n\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    final_grad_norms = []\n    for _ in range(M):\n        # Run one full simulation from r0\n        r_final = run_single_replicate(params, rng)\n        \n        # Calculate and store the norm of the true gradient at the final position\n        grad_V_r_final = grad_V(r_final, k)\n        norm_grad_V_r_final = np.linalg.norm(grad_V_r_final)\n        final_grad_norms.append(norm_grad_V_r_final)\n    \n    # Compute the expectation by averaging over replicates\n    avg_final_grad_norm = np.mean(final_grad_norms)\n\n    # Compute and return the contraction ratio\n    C = avg_final_grad_norm / norm_grad_V_r0\n    return C\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'k': 1, 'm': 1, 'sigma': 0, 'r0': [2.0, 0.0], 'dt0': 0.1, 'dt_max': 0.5, 'f_inc': 1.1, 'f_dec': 0.5, 'alpha0': 0.1, 'alpha_decay': 0.99, 'N_delay': 5, 'N': 50, 'M': 300},\n        {'k': 1, 'm': 1, 'sigma': 0.1, 'r0': [2.0, 0.0], 'dt0': 0.1, 'dt_max': 0.5, 'f_inc': 1.1, 'f_dec': 0.5, 'alpha0': 0.1, 'alpha_decay': 0.99, 'N_delay': 5, 'N': 50, 'M': 300},\n        {'k': 1, 'm': 1, 'sigma': 0.5, 'r0': [2.0, 0.0], 'dt0': 0.1, 'dt_max': 0.5, 'f_inc': 1.1, 'f_dec': 0.5, 'alpha0': 0.1, 'alpha_decay': 0.99, 'N_delay': 5, 'N': 50, 'M': 300},\n        {'k': 1, 'm': 1, 'sigma': 0.1, 'r0': [0.2, -0.2], 'dt0': 0.05, 'dt_max': 0.5, 'f_inc': 1.1, 'f_dec': 0.5, 'alpha0': 0.1, 'alpha_decay': 0.99, 'N_delay': 5, 'N': 50, 'M': 300},\n    ]\n\n    results = []\n    for params in test_cases:\n        contraction_ratio = compute_contraction(params)\n        results.append(f\"{contraction_ratio:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}