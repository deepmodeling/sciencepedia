{
    "hands_on_practices": [
        {
            "introduction": "This first exercise goes to the heart of the Andersen thermostat's stochastic nature. By modeling the collisions as a memoryless Poisson process, we can build a clear statistical picture of the thermostat's action. This practice challenges you to derive the probability distribution for the total number of collisions in the system, connecting the microscopic, per-particle rate parameter $\\nu$ to a macroscopic, system-wide observable .",
            "id": "4110000",
            "problem": "Consider a Molecular Dynamics (MD) simulation of a complex fluid in a single cubic cell containing $N$ indistinguishable particles coupled to a heat bath via the Andersen thermostat. In this thermostat, each particle undergoes stochastic velocity reassignment events (“collisions”), modeled as occurring at random times such that the waiting-time distribution for each particle is memoryless and stationary with a constant rate parameter $\\nu$ (events per unit time). Assume collisions for different particles and across time are independent.\n\nTo connect the continuous-time stochastic description to a standard discrete-time MD integrator, suppose the dynamics are advanced in fixed steps of duration $\\Delta t$, and in each step each particle either collides once or not at all, with a per-step collision probability $p(\\Delta t)$ chosen to be consistent with the stated memoryless property. Under these assumptions, and in the limit $\\Delta t \\to 0$ holding the observation time fixed, derive from first principles the probability mass function of the total number of collision events $K$ observed in the entire cell over a unit time interval of length $1$, expressed in terms of the integer count $k$, the rate parameter $\\nu$, and the particle count $N$.\n\nYour final answer must be a single closed-form analytic expression for the probability $\\mathbb{P}(K = k)$ as a function of $k$, $\\nu$, and $N$. No numerical rounding is required. Do not include any units in your final expression.",
            "solution": "The user wants to derive the probability mass function for the total number of collision events in an Andersen-thermostatted system.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- System: Molecular Dynamics (MD) simulation of a complex fluid.\n- Geometry: Single cubic cell.\n- Particles: $N$ indistinguishable particles.\n- Thermostat: Andersen thermostat.\n- Collision Dynamics: Stochastic velocity reassignment events (\"collisions\").\n- Waiting-Time Distribution: Memoryless and stationary for each particle.\n- Rate Parameter: Constant rate $\\nu$ (events per unit time) for each particle.\n- Independence: Collisions for different particles and across time are independent.\n- Discretization: Time is advanced in fixed steps of duration $\\Delta t$.\n- Per-Step Model: In each step, a particle collides at most once, with probability $p(\\Delta t)$.\n- Consistency: The per-step probability $p(\\Delta t)$ is consistent with the memoryless property.\n- Limit: The derivation must be performed in the limit $\\Delta t \\to 0$, holding the observation time fixed.\n- Objective: Derive the probability mass function $\\mathbb{P}(K = k)$ for the total number of collision events $K$ in the entire cell over a unit time interval (length $1$). The expression should be in terms of the integer count $k$, the rate $\\nu$, and the particle count $N$.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientific Grounding (Critical):** The problem is scientifically sound. The Andersen thermostat is a standard algorithm in computational physics. The modeling of stochastic collisions via a memoryless process is the canonical definition of a Poisson process, which is fundamental to statistical mechanics and probability theory.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information and constraints (the nature of the stochastic process, particle independence, the connection between continuous and discrete models) to derive a unique probability distribution.\n- **Objective (Critical):** The problem is stated in precise, objective, and formal language, free of any subjectivity or ambiguity.\n\n**1.3. Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. It passes all validation checks. We may proceed to the solution.\n\n### Step 2: Derivation of the Probability Mass Function\n\nThe problem requires us to derive the probability mass function (PMF) for the total number of collisions, $K$, in a system of $N$ particles over a time interval of length $1$. The derivation must start from a discrete-time model and take the limit as the time step $\\Delta t \\to 0$.\n\nFirst, let's formalize the stochastic process for a single particle. A waiting-time distribution that is memoryless and stationary with a constant rate parameter $\\nu$ defines a Poisson process. For such a process, the probability of observing zero events in a time interval of duration $\\tau$ is given by $\\exp(-\\nu \\tau)$.\n\nThe problem introduces a discrete-time model with time steps of duration $\\Delta t$. In each step, a particle collides with a probability $p(\\Delta t)$. The problem states that this discrete model must be \"consistent with the stated memoryless property\". This consistency requirement allows us to determine the functional form of $p(\\Delta t)$. For the discrete model to match the continuous-time Poisson process, the probability of a particle *not* colliding in a time step $\\Delta t$ must be equal to the probability of zero events in that duration for the Poisson process.\nIn the discrete model, the event \"no collision\" has probability $1 - p(\\Delta t)$.\nIn the continuous model, the probability of zero events in time $\\Delta t$ is $\\exp(-\\nu \\Delta t)$.\nEquating these gives the consistency condition:\n$$1 - p(\\Delta t) = \\exp(-\\nu \\Delta t)$$\nTherefore, the per-step, per-particle collision probability is:\n$$p(\\Delta t) = 1 - \\exp(-\\nu \\Delta t)$$\n\nNow, we consider the entire system over a unit time interval, i.e., the interval $[0, 1]$. We divide this interval into $M$ small, non-overlapping time steps, each of duration $\\Delta t = \\frac{1}{M}$. As we take the limit $\\Delta t \\to 0$, we have $M \\to \\infty$.\n\nIn each time step of length $\\Delta t$, there are $N$ particles, and each particle has an independent chance to collide. The problem specifies that collisions are independent across particles and across time. We can therefore model the total number of collisions $K$ as the sum of successes in a series of independent Bernoulli trials.\nThe total number of trials, $n_{\\text{trials}}$, is the number of particles multiplied by the number of time steps:\n$$n_{\\text{trials}} = N \\times M$$\nThe probability of success for any single trial (i.e., one specific particle colliding in one specific time step) is:\n$$p_{\\text{success}} = p(\\Delta t) = p\\left(\\frac{1}{M}\\right) = 1 - \\exp\\left(-\\frac{\\nu}{M}\\right)$$\nThe total number of collisions $K$ thus follows a Binomial distribution, $K \\sim \\text{Binomial}(n_{\\text{trials}}, p_{\\text{success}})$. The PMF is given by:\n$$\\mathbb{P}(K = k) = \\binom{n_{\\text{trials}}}{k} (p_{\\text{success}})^k (1 - p_{\\text{success}})^{n_{\\text{trials}} - k}$$\nSubstituting the expressions for $n_{\\text{trials}}$ and $p_{\\text{success}}$:\n$$\\mathbb{P}(K = k) = \\binom{NM}{k} \\left(1 - \\exp\\left(-\\frac{\\nu}{M}\\right)\\right)^k \\left(\\exp\\left(-\\frac{\\nu}{M}\\right)\\right)^{NM - k}$$\n\nWe are asked to find the distribution in the limit $\\Delta t \\to 0$, which corresponds to $M \\to \\infty$. This is a classic application of the Poisson limit theorem, which states that a Binomial distribution $\\text{Binomial}(n, p)$ converges to a Poisson distribution with parameter $\\Lambda = np$ if $n \\to \\infty$, $p \\to 0$, and the product $np$ converges to a finite value $\\Lambda$.\n\nLet's check these conditions for our case:\n1. $n_{\\text{trials}} = NM \\to \\infty$ as $M \\to \\infty$ (since $N$ is a fixed positive integer).\n2. $p_{\\text{success}} = 1 - \\exp(-\\frac{\\nu}{M}) \\to 1 - \\exp(0) = 1 - 1 = 0$ as $M \\to \\infty$.\n3. We must evaluate the limit of the product, which will be the parameter $\\Lambda$ of the resulting Poisson distribution.\n$$\\Lambda = \\lim_{M \\to \\infty} n_{\\text{trials}} \\cdot p_{\\text{success}} = \\lim_{M \\to \\infty} (NM) \\left(1 - \\exp\\left(-\\frac{\\nu}{M}\\right)\\right)$$\nTo evaluate this limit, we can use the well-known result from calculus, $\\lim_{x \\to 0} \\frac{\\exp(ax) - 1}{x} = a$. Let $x = 1/M$. As $M \\to \\infty$, $x \\to 0$.\n$$\\Lambda = N \\nu \\lim_{M \\to \\infty} \\frac{1 - \\exp\\left(-\\frac{\\nu}{M}\\right)}{\\frac{\\nu}{M}} = N \\nu \\lim_{x \\to 0} \\frac{1 - \\exp(-\\nu x)}{\\nu x} = N \\nu \\lim_{x \\to 0} \\frac{- (-\\nu \\exp(-\\nu x))}{\\nu} = N\\nu$$\nThe limit of the mean number of collisions is $\\Lambda = N\\nu$.\n\nSince all conditions for the Poisson limit are met, the distribution of the total number of collisions $K$ converges to a Poisson distribution with mean $\\Lambda = N\\nu$. The probability mass function for a Poisson distribution with mean $\\Lambda$ is:\n$$\\mathbb{P}(K = k) = \\frac{\\Lambda^k \\exp(-\\Lambda)}{k!}$$\nSubstituting $\\Lambda = N\\nu$, we obtain the final expression for the probability of observing exactly $k$ collisions in the entire cell over a unit time interval:\n$$\\mathbb{P}(K = k) = \\frac{(N\\nu)^k \\exp(-N\\nu)}{k!}$$\nThis result is physically intuitive: the total collision process for the entire cell is a superposition of $N$ independent Poisson processes, each with rate $\\nu$. The superposition of independent Poisson processes is itself a Poisson process whose rate is the sum of the individual rates, which is $N\\nu$.",
            "answer": "$$\n\\boxed{\\frac{(N\\nu)^k \\exp(-N\\nu)}{k!}}\n$$"
        },
        {
            "introduction": "Having established the statistical foundation, we now explore its consequences for a key conserved quantity: the total momentum. This practice examines the equilibrium distribution of the center-of-mass momentum and provides the theoretical justification for a crucial technique in molecular dynamics simulations—the removal of center-of-mass drift . Understanding why this correction does not bias the system's structural properties is essential for conducting robust and accurate simulations.",
            "id": "4110003",
            "problem": "Consider a three-dimensional periodic cubic cell containing $N$ classical point particles with positions $\\{\\boldsymbol{r}_{i}\\}_{i=1}^{N}$ and momenta $\\{\\boldsymbol{p}_{i}\\}_{i=1}^{N}$, with individual masses $\\{m_{i}\\}_{i=1}^{N}$ and total mass $M=\\sum_{i=1}^{N} m_{i}$. The system is subject to only internal, pairwise forces derived from a potential energy $U(\\{\\boldsymbol{r}_{i}\\})$ that depends on relative separations and is invariant under a global Galilean transformation. There are no external fields. The system is coupled to a heat bath at absolute temperature $T$ via either the Andersen thermostat or the Nosé–Hoover thermostat so that, in equilibrium, the stationary distribution for phase space is canonical.\n\nStarting only from fundamental principles for classical equilibrium statistical mechanics and Newtonian dynamics, derive the marginal equilibrium probability density $f_{\\boldsymbol{P}}(\\boldsymbol{P})$ of the center-of-mass momentum $\\boldsymbol{P}=\\sum_{i=1}^{N}\\boldsymbol{p}_{i}$ in the canonical ensemble at temperature $T$, expressed as an explicit normalized analytic function of $\\boldsymbol{P}$ and the parameters $M$ and $T$ (and universal constants as needed).\n\nThen, justify from first principles why, in a periodic system with no external fields, subtracting the instantaneous center-of-mass velocity $\\boldsymbol{V}_{\\mathrm{cm}}=\\boldsymbol{P}/M$ from each particle velocity at each time step (or at regular intervals) removes drift without biasing the configurational sampling of the canonical ensemble. In your justification, address how this operation interacts with the momentum properties of the Andersen thermostat and the Nosé–Hoover thermostat.\n\nYour final reported result must be the explicit normalized analytic expression for $f_{\\boldsymbol{P}}(\\boldsymbol{P})$. No numerical evaluation is required. Do not include units in the final reported expression.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- System: $N$ classical point particles in a three-dimensional periodic cubic cell.\n- Particle properties: positions $\\{\\boldsymbol{r}_{i}\\}_{i=1}^{N}$, momenta $\\{\\boldsymbol{p}_{i}\\}_{i=1}^{N}$, individual masses $\\{m_{i}\\}_{i=1}^{N}$.\n- Total mass: $M=\\sum_{i=1}^{N} m_{i}$.\n- Center-of-mass momentum: $\\boldsymbol{P}=\\sum_{i=1}^{N}\\boldsymbol{p}_{i}$.\n- Center-of-mass velocity: $\\boldsymbol{V}_{\\mathrm{cm}}=\\boldsymbol{P}/M$.\n- Forces: Internal, pairwise forces derived from a potential energy $U(\\{\\boldsymbol{r}_{i}\\})$ that depends on relative separations.\n- Symmetries: Potential energy is invariant under a global Galilean transformation.\n- External conditions: No external fields.\n- Ensemble: Canonical ensemble at absolute temperature $T$, maintained by an Andersen or Nosé–Hoover thermostat.\n- Equilibrium distribution: The stationary phase space distribution is canonical.\n- Task 1: Derive the marginal equilibrium probability density $f_{\\boldsymbol{P}}(\\boldsymbol{P})$ of the center-of-mass momentum $\\boldsymbol{P}$, as a normalized analytic function of $\\boldsymbol{P}$, $M$, and $T$.\n- Task 2: Justify from first principles why subtracting $\\boldsymbol{V}_{\\mathrm{cm}}$ from each particle velocity at each time step removes drift without biasing the configurational sampling of the canonical ensemble, considering the properties of Andersen and Nosé–Hoover thermostats.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientifically Grounded:** The problem is a standard exercise in classical statistical mechanics and molecular dynamics simulation methodology. It deals with fundamental concepts like the canonical ensemble, Hamiltonian separability, and the properties of common thermostats. All concepts are based on established physical and mathematical principles.\n- **Well-Posed:** The problem provides all necessary information (system definition, ensemble, forces) to derive the requested probability distribution and to formulate the justification. The questions are precise, and a unique, meaningful solution exists for both parts.\n- **Objective:** The language is formal and unbiased. The terms used (canonical ensemble, Andersen thermostat, Galilean transformation) have precise, universally accepted meanings in physics.\n\nThe problem does not exhibit any of the invalidity flaws. It is a legitimate and well-defined problem in computational statistical physics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n## Solution\n\nThe solution is presented in two parts as requested by the problem statement. First, we derive the probability density for the center-of-mass momentum. Second, we justify the procedure of removing center-of-mass drift in simulations.\n\n### Part 1: Derivation of the Center-of-Mass Momentum Distribution\n\nIn classical equilibrium statistical mechanics, the probability density for a system in the canonical ensemble at temperature $T$ to be in a specific microstate, defined by particle positions $\\{\\boldsymbol{r}_{i}\\}$ and momenta $\\{\\boldsymbol{p}_{i}\\}$, is given by the Boltzmann distribution:\n$$\n\\rho(\\{\\boldsymbol{r}_{i}\\}, \\{\\boldsymbol{p}_{i}\\}) = \\frac{1}{Z} \\exp\\left(-\\frac{H(\\{\\boldsymbol{r}_{i}\\}, \\{\\boldsymbol{p}_{i}\\})}{k_{\\mathrm{B}} T}\\right)\n$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant, $H$ is the system's Hamiltonian, and $Z$ is the canonical partition function which normalizes the distribution. For the specified system, the Hamiltonian is the sum of the kinetic energy $K$ and the potential energy $U$:\n$$\nH(\\{\\boldsymbol{r}_{i}\\}, \\{\\boldsymbol{p}_{i}\\}) = K(\\{\\boldsymbol{p}_{i}\\}) + U(\\{\\boldsymbol{r}_{i}\\}) = \\sum_{i=1}^{N} \\frac{\\boldsymbol{p}_{i}^2}{2m_{i}} + U(\\{\\boldsymbol{r}_{i}\\})\n$$\nThe problem states that the potential $U$ depends only on relative separations. This, along with the absence of external fields, makes the Hamiltonian separable into a term for the center-of-mass (CoM) motion and a term for the internal motion (relative to the CoM).\n\nLet's define the center-of-mass position $\\boldsymbol{R} = \\frac{1}{M}\\sum_{i=1}^N m_i \\boldsymbol{r}_i$ and momentum $\\boldsymbol{P} = \\sum_{i=1}^N \\boldsymbol{p}_i$. The velocity of particle $i$ relative to the CoM is $\\boldsymbol{v}'_i = \\boldsymbol{v}_i - \\boldsymbol{V}_{\\mathrm{cm}}$, where $\\boldsymbol{v}_i = \\boldsymbol{p}_i/m_i$ and $\\boldsymbol{V}_{\\mathrm{cm}} = \\boldsymbol{P}/M$. The momentum of particle $i$ relative to the CoM frame is $\\boldsymbol{p}'_i = m_i \\boldsymbol{v}'_i$. By construction, $\\sum_{i=1}^N \\boldsymbol{p}'_i = \\boldsymbol{0}$.\n\nThe total kinetic energy $K$ can be written as:\n$$\nK = \\sum_{i=1}^{N} \\frac{1}{2}m_{i}\\boldsymbol{v}_{i}^2 = \\sum_{i=1}^{N} \\frac{1}{2}m_{i}(\\boldsymbol{V}_{\\mathrm{cm}} + \\boldsymbol{v}'_{i})^2\n$$\n$$\nK = \\sum_{i=1}^{N} \\frac{1}{2}m_{i}(\\boldsymbol{V}_{\\mathrm{cm}}^2 + 2\\boldsymbol{V}_{\\mathrm{cm}}\\cdot\\boldsymbol{v}'_{i} + (\\boldsymbol{v}'_i)^2)\n$$\n$$\nK = \\frac{1}{2}\\boldsymbol{V}_{\\mathrm{cm}}^2 \\sum_{i=1}^{N}m_{i} + \\boldsymbol{V}_{\\mathrm{cm}}\\cdot\\sum_{i=1}^{N}m_{i}\\boldsymbol{v}'_{i} + \\sum_{i=1}^{N}\\frac{1}{2}m_{i}(\\boldsymbol{v}'_i)^2\n$$\nThe first term is $\\frac{1}{2}MV^2_{\\mathrm{cm}} = \\frac{\\boldsymbol{P}^2}{2M}$. The second term's sum is $\\sum_{i=1}^N m_i \\boldsymbol{v}'_i = \\sum_{i=1}^N \\boldsymbol{p}'_i = \\boldsymbol{0}$, so the cross-term vanishes. The third term is the internal kinetic energy, $K_{\\mathrm{internal}} = \\sum_{i=1}^N \\frac{(\\boldsymbol{p}'_i)^2}{2m_i}$. The potential energy $U$ depends only on relative positions, so it is a function of the internal coordinates only.\nThus, the Hamiltonian separates completely:\n$$\nH(\\{\\boldsymbol{r}_{i}\\}, \\{\\boldsymbol{p}_{i}\\}) = \\frac{\\boldsymbol{P}^2}{2M} + \\left( K_{\\mathrm{internal}}(\\{\\boldsymbol{p}'_i\\}) + U(\\{\\boldsymbol{r}'_i\\}) \\right) = H_{\\mathrm{CoM}}(\\boldsymbol{P}) + H_{\\mathrm{internal}}(\\{\\boldsymbol{r}'_i\\}, \\{\\boldsymbol{p}'_i\\})\n$$\nDue to this separability, the phase space probability density factorizes:\n$$\n\\rho \\propto \\exp\\left(-\\frac{H_{\\mathrm{CoM}}}{k_{\\mathrm{B}} T}\\right) \\exp\\left(-\\frac{H_{\\mathrm{internal}}}{k_{\\mathrm{B}} T}\\right)\n$$\nThe marginal probability density for the center-of-mass momentum, $f_{\\boldsymbol{P}}(\\boldsymbol{P})$, is obtained by integrating the full probability density over all other degrees of freedom (all internal positions and momenta).\n$$\nf_{\\boldsymbol{P}}(\\boldsymbol{P}) = \\int d\\Gamma_{\\mathrm{internal}} \\rho(\\boldsymbol{P}, \\Gamma_{\\mathrm{internal}})\n$$\nwhere $d\\Gamma_{\\mathrm{internal}}$ represents the volume element of the internal phase space.\n$$\nf_{\\boldsymbol{P}}(\\boldsymbol{P}) \\propto \\int \\exp\\left(-\\frac{\\boldsymbol{P}^2}{2M k_{\\mathrm{B}} T}\\right) \\exp\\left(-\\frac{H_{\\mathrm{internal}}}{k_{\\mathrm{B}} T}\\right) d\\Gamma_{\\mathrm{internal}}\n$$\nSince the CoM term $\\exp(-\\frac{\\boldsymbol{P}^2}{2M k_{\\mathrm{B}} T})$ is independent of the internal variables, it can be factored out of the integral:\n$$\nf_{\\boldsymbol{P}}(\\boldsymbol{P}) \\propto \\exp\\left(-\\frac{\\boldsymbol{P}^2}{2M k_{\\mathrm{B}} T}\\right) \\left( \\int \\exp\\left(-\\frac{H_{\\mathrm{internal}}}{k_{\\mathrm{B}} T}\\right) d\\Gamma_{\\mathrm{internal}} \\right)\n$$\nThe integral over the internal phase space is a constant with respect to $\\boldsymbol{P}$. Therefore, the functional form of the distribution is:\n$$\nf_{\\boldsymbol{P}}(\\boldsymbol{P}) = C \\exp\\left(-\\frac{\\boldsymbol{P}^2}{2M k_{\\mathrm{B}} T}\\right)\n$$\nwhere $C$ is a normalization constant. The constant is determined by the requirement that the total probability is unity:\n$$\n\\int f_{\\boldsymbol{P}}(\\boldsymbol{P}) d^3\\boldsymbol{P} = 1\n$$\n$$\nC \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{P_x^2+P_y^2+P_z^2}{2M k_{\\mathrm{B}} T}\\right) dP_x dP_y dP_z = 1\n$$\nThis integral is a product of three identical Gaussian integrals of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$, with $a = 1/(2M k_{\\mathrm{B}} T)$.\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{P_x^2}{2M k_{\\mathrm{B}} T}\\right) dP_x = \\sqrt{2\\pi M k_{\\mathrm{B}} T}\n$$\nThe three-dimensional integral is $(\\sqrt{2\\pi M k_{\\mathrm{B}} T})^3 = (2\\pi M k_{\\mathrm{B}} T)^{3/2}$.\nThe normalization condition becomes $C \\cdot (2\\pi M k_{\\mathrm{B}} T)^{3/2} = 1$, which gives $C = (2\\pi M k_{\\mathrm{B}} T)^{-3/2}$.\nThe final normalized probability density is:\n$$\nf_{\\boldsymbol{P}}(\\boldsymbol{P}) = (2\\pi M k_{\\mathrm{B}} T)^{-3/2} \\exp\\left(-\\frac{\\boldsymbol{P}^2}{2M k_{\\mathrm{B}} T}\\right)\n$$\nThis is a Maxwell-Boltzmann distribution for a single particle of mass $M$.\n\n### Part 2: Justification for Removing Center-of-Mass Drift\n\nIn molecular dynamics simulations of systems with periodic boundary conditions and no external fields, the total momentum $\\boldsymbol{P}$ is a conserved quantity of the physical dynamics derived from the Hamiltonian. However, numerical integration errors can lead to a slow accumulation of non-zero total momentum, an artifact known as center-of-mass drift. The procedure of periodically subtracting the CoM velocity, $\\boldsymbol{v}_i \\to \\boldsymbol{v}_i - \\boldsymbol{V}_{\\mathrm{cm}}$, is equivalent to resetting the total momentum to zero: $\\boldsymbol{P} \\to \\boldsymbol{0}$. We must justify why this does not bias the sampling of the configurational phase space.\n\n1.  **Hamiltonian Separability and Statistical Independence:** As established in Part 1, for a system with only internal forces that are invariant under Galilean transformations (i.e., depend only on relative positions), the Hamiltonian separates into a CoM term and an internal term: $H = H_{\\mathrm{CoM}} + H_{\\mathrm{internal}}$. This separability implies that in the canonical ensemble, the CoM degrees of freedom are statistically independent of the internal degrees of freedom. The equilibrium probability density factorizes: $\\rho(\\{\\boldsymbol{r}_{i}\\}, \\{\\boldsymbol{p}_{i}\\}) \\propto \\exp(-H_{\\mathrm{CoM}}/k_{\\mathrm{B}} T) \\exp(-H_{\\mathrm{internal}}/k_{\\mathrm{B}} T)$.\n\n2.  **Configurational Averages:** Physical properties related to the structure and thermodynamics of the fluid, such as the radial distribution function or pressure, are determined by configurational averages. These averages are integrals over the configurational part of the phase space, weighted by the configurational probability density, $P(\\{\\boldsymbol{r}_i\\})$. This density is obtained by integrating the full phase space density over all momenta:\n    $$\n    P(\\{\\boldsymbol{r}_i\\}) \\propto \\int \\exp\\left(-\\frac{K(\\{\\boldsymbol{p}_i\\})}{k_{\\mathrm{B}} T}\\right) d^{3N}\\boldsymbol{p} \\cdot \\exp\\left(-\\frac{U(\\{\\boldsymbol{r}_i\\})}{k_{\\mathrm{B}} T}\\right)\n    $$\n    The momentum integral is a constant factor. Thus, $P(\\{\\boldsymbol{r}_i\\}) \\propto \\exp(-U(\\{\\boldsymbol{r}_i\\})/k_{\\mathrm{B}} T)$. Since the potential energy $U$ depends only on internal coordinates, the distribution of configurations is entirely determined by $H_{\\mathrm{internal}}$.\n\n3.  **The Nature of the Drift Removal Operation:** The operation of subtracting the CoM velocity, $\\boldsymbol{v}_i(t) \\to \\boldsymbol{v}_i(t) - \\boldsymbol{V}_{\\mathrm{cm}}(t)$, is a transformation on the momenta only. It does not alter the particle positions $\\{\\boldsymbol{r}_i\\}$ at the instant it is applied. This operation merely projects the system from its current state in the full NVT ensemble onto the sub-ensemble where $\\boldsymbol{P}=\\boldsymbol{0}$. Because the configurational distribution is independent of $\\boldsymbol{P}$, sampling within the NVT, $\\boldsymbol{P}=\\boldsymbol{0}$ ensemble yields the exact same averages for any configurational property as sampling in the full NVT ensemble. Therefore, removing the CoM drift does not bias configurational sampling.\n\n4.  **Interaction with Thermostats:**\n    - **Andersen Thermostat:** This thermostat models interaction with a heat bath via stochastic collisions that re-sample particle velocities from a Maxwell-Boltzmann distribution at temperature $T$. These collisions do not conserve total momentum. The total momentum $\\boldsymbol{P}$ undergoes a random walk, and its equilibrium distribution is precisely the $f_{\\boldsymbol{P}}(\\boldsymbol{P})$ derived in Part 1. Removing the CoM drift is a practical measure to counteract this diffusion and prevent the unphysical \"flying ice cube\" effect. The justification remains valid: the thermostat aims to produce a canonical distribution of kinetic energy, and the separability of the Hamiltonian ensures that pinning $\\boldsymbol{P}$ to zero does not affect the independent configurational distribution.\n    - **Nosé–Hoover Thermostat:** This is a deterministic thermostat which extends the phase space. In its common formulation for molecular systems where total momentum should be conserved, the thermostat is designed to act only on the momenta relative to the CoM, $\\{\\boldsymbol{p}'_i\\}$. This ensures that it thermostats the internal degrees of freedom while explicitly conserving $\\boldsymbol{P}$ (if $\\boldsymbol{P}$ is initially zero, it remains zero). In this case, any non-zero $\\boldsymbol{P}$ is purely a numerical artifact. Periodically resetting $\\boldsymbol{P}$ to zero is simply a correction that restores the system to the intended manifold of phase space where the conserved quantity has its proper value. If a less sophisticated Nosé-Hoover thermostat is used that acts on the full momenta $\\{\\boldsymbol{p}_i\\}$, it couples the CoM motion to the thermostat. Removing the CoM motion is then not just desirable for practical reasons but is necessary to ensure one is sampling the properties of the internal state of the system at temperature $T$, rather than a mix of internal and CoM motion. In all implementations, the fundamental argument based on Hamiltonian separability guarantees that the configurational sampling remains unbiased.\n\nIn summary, removing CoM drift is a valid and often necessary procedure in simulations of periodic systems because the dynamics of interest (structural and thermodynamic properties) are governed by the internal Hamiltonian, which is statistically independent of the center-of-mass motion in the canonical ensemble.",
            "answer": "$$\n\\boxed{(2\\pi M k_{\\mathrm{B}} T)^{-3/2} \\exp\\left(-\\frac{\\boldsymbol{P}^2}{2M k_{\\mathrm{B}} T}\\right)}\n$$"
        },
        {
            "introduction": "The final practice bridges theory and application by addressing the most critical user-defined parameter in the Andersen thermostat: the collision frequency $\\nu$. A low $\\nu$ preserves the natural system dynamics but may be slow to thermalize the system, while a high $\\nu$ enforces the target temperature rapidly at the cost of disrupting dynamical correlations. This exercise provides a quantitative framework for navigating this trade-off, asking you to find an optimal range for $\\nu$ by balancing sampling efficiency against the perturbation of the system's velocity autocorrelation function .",
            "id": "4110018",
            "problem": "Consider a simple fluid whose true equilibrium velocity autocorrelation function (VACF) is well-approximated at intermediate times by a single exponential decay. Let the true VACF be modeled as $C_{\\mathrm{true}}(t) = \\frac{k_{\\mathrm{B}} T}{m} \\exp(-\\gamma t)$, where $k_{\\mathrm{B}}$ is Boltzmann's constant, $T$ is the temperature, $m$ is the particle mass, and $\\gamma$ (in $\\mathrm{s}^{-1}$) is an effective decay rate associated with intermolecular interactions. Under the Andersen thermostat, each particle's velocity is instantaneously reassigned according to the Maxwell-Boltzmann distribution at random times governed by a Poisson process with rate $\\nu$ (in $\\mathrm{s}^{-1}$). Between collisions, the dynamics are otherwise governed by Newton's laws under the same interactions that produce the true VACF. In this setting, the Andersen thermostat modifies the persistence of velocity memory by superimposing an independent memoryless collision process. The combined effect yields an Andersen-thermostatted VACF of the form $C_{\\mathrm{A}}(t;\\nu) = \\frac{k_{\\mathrm{B}} T}{m} \\exp\\big(-( \\gamma + \\nu ) t\\big)$.\n\nA practitioner aims to choose the collision frequency $\\nu$ to balance two competing aims:\n\n- Rapid decorrelation: Shorter correlation time improves sampling efficiency in canonical ensemble simulations of complex fluids.\n- Dynamical fidelity: The Andersen thermostat should not excessively distort the true VACF.\n\nTo formalize this balance, define the normalized integrated squared error between the thermostatted and true VACFs as\n$$E_{\\mathrm{norm}}(\\nu) = \\gamma \\int_{0}^{\\infty} \\left( \\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{true}}(0)} - \\frac{C_{\\mathrm{true}}(t)}{C_{\\mathrm{true}}(0)} \\right)^2 \\, dt,$$\nwhich is dimensionless. Also define the thermostatted correlation time as\n$$\\tau_{\\mathrm{A}}(\\nu) = \\int_{0}^{\\infty} \\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{A}}(0)} \\, dt = \\frac{1}{\\gamma + \\nu},$$\nin $\\mathrm{s}$, and the true correlation time $\\tau_{\\mathrm{true}} = 1/\\gamma$.\n\nGiven the tolerances $\\tau_{\\max}$ (in $\\mathrm{s}$) for decorrelation time and $\\varepsilon$ (dimensionless) for error (with the requirement $0 \\le \\varepsilon \\le \\frac{1}{2}$ to ensure a meaningful bound), define the feasible range of $\\nu$ as those values satisfying both constraints:\n- $\\tau_{\\mathrm{A}}(\\nu) \\le \\tau_{\\max}$,\n- $E_{\\mathrm{norm}}(\\nu) \\le \\varepsilon$.\n\nStarting only from Newton's laws, the exponential VACF form for the true fluid, and the Poisson collision model of the Andersen thermostat, derive the closed-form expression of $E_{\\mathrm{norm}}(\\nu)$ and show how to solve the inequality $E_{\\mathrm{norm}}(\\nu) \\le \\varepsilon$ for $\\nu$. Then, implement an algorithm that:\n\n- Computes the lower bound $\\nu_{\\min}$ imposed by decorrelation, and the upper bound $\\nu_{\\max}$ imposed by error tolerance, using analytic expressions. If $\\varepsilon \\ge \\frac{1}{2}$, the error constraint imposes no finite upper bound, which should be treated as an unbounded upper limit.\n- Returns the intersection range $[\\nu_{\\min}, \\nu_{\\max}]$ if feasible, otherwise returns the empty list if no $\\nu$ satisfies both constraints.\n\nYour program should evaluate the following test suite. All rates $\\gamma$ and $\\nu$ are in $\\mathrm{s}^{-1}$, all times $\\tau_{\\max}$ are in $\\mathrm{s}$, and all tolerances $\\varepsilon$ are dimensionless:\n\n- Test case $1$: $\\gamma = 1$, $\\tau_{\\max} = 1$, $\\varepsilon = 0.1$.\n- Test case $2$: $\\gamma = 1$, $\\tau_{\\max} = 0.2$, $\\varepsilon = 0.1$.\n- Test case $3$: $\\gamma = 0.01$, $\\tau_{\\max} = 5$, $\\varepsilon = 0.49$.\n- Test case $4$: $\\gamma = 2$, $\\tau_{\\max} = 0.05$, $\\varepsilon = 0.6$.\n- Test case $5$: $\\gamma = 5$, $\\tau_{\\max} = 0.1$, $\\varepsilon = 0.3$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry either a two-element list $[\\nu_{\\min}, \\nu_{\\max}]$ in $\\mathrm{s}^{-1}$ rounded to six decimal places, or an empty list if infeasible. If the upper bound is unbounded, represent it using the special floating-point value $+\\infty$ and print it as $\\mathrm{inf}$. For example, the output should look like $[[\\nu_{\\min,1},\\nu_{\\max,1}],[\\nu_{\\min,2},\\nu_{\\max,2}],\\ldots]$, with each numeric bound in $\\mathrm{s}^{-1}$ rounded to six decimal places.",
            "solution": "The goal is to find the feasible range for the collision frequency $\\nu$ by solving two inequalities based on the decorrelation time and dynamic error.\n\n### Derivation of the Error Term $E_{\\mathrm{norm}}(\\nu)$\nFirst, we derive a closed-form expression for the normalized integrated squared error, $E_{\\mathrm{norm}}(\\nu)$. The definition is:\n$$E_{\\mathrm{norm}}(\\nu) = \\gamma \\int_{0}^{\\infty} \\left( \\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{true}}(0)} - \\frac{C_{\\mathrm{true}}(t)}{C_{\\mathrm{true}}(0)} \\right)^2 \\, dt$$\nThe normalized VACFs are $\\frac{C_{\\mathrm{true}}(t)}{C_{\\mathrm{true}}(0)} = \\exp(-\\gamma t)$ and $\\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{true}}(0)} = \\exp(-(\\gamma + \\nu) t)$. Substituting these gives:\n$$E_{\\mathrm{norm}}(\\nu) = \\gamma \\int_{0}^{\\infty} \\left( \\exp(-(\\gamma + \\nu) t) - \\exp(-\\gamma t) \\right)^2 \\, dt$$\nExpanding the squared term in the integrand:\n$$ \\left( \\exp(-(\\gamma + \\nu) t) - \\exp(-\\gamma t) \\right)^2 = \\exp(-2(\\gamma + \\nu) t) - 2\\exp(-(2\\gamma + \\nu) t) + \\exp(-2\\gamma t) $$\nWe integrate term-by-term from $t=0$ to $t=\\infty$, using the standard integral $\\int_{0}^{\\infty} e^{-ax} dx = 1/a$:\n$$ \\frac{E_{\\mathrm{norm}}(\\nu)}{\\gamma} = \\int_{0}^{\\infty} \\left( \\dots \\right)^2 dt = \\frac{1}{2(\\gamma + \\nu)} - \\frac{2}{2\\gamma + \\nu} + \\frac{1}{2\\gamma} $$\nTo simplify, we find a common denominator:\n$$ E_{\\mathrm{norm}}(\\nu) = \\gamma \\left[ \\left(\\frac{1}{2(\\gamma + \\nu)} + \\frac{1}{2\\gamma}\\right) - \\frac{2}{2\\gamma + \\nu} \\right] $$\n$$ = \\gamma \\left[ \\frac{\\gamma + (\\gamma + \\nu)}{2\\gamma(\\gamma + \\nu)} - \\frac{2}{2\\gamma + \\nu} \\right] = \\gamma \\left[ \\frac{2\\gamma + \\nu}{2\\gamma(\\gamma + \\nu)} - \\frac{2}{2\\gamma + \\nu} \\right] $$\n$$ = \\frac{2\\gamma + \\nu}{2(\\gamma + \\nu)} - \\frac{2\\gamma}{2\\gamma + \\nu} $$\nCreating a common denominator $2(\\gamma+\\nu)(2\\gamma+\\nu)$:\n$$ E_{\\mathrm{norm}}(\\nu) = \\frac{(2\\gamma + \\nu)^2 - 2\\gamma \\cdot 2(\\gamma + \\nu)}{2(\\gamma+\\nu)(2\\gamma+\\nu)} = \\frac{(4\\gamma^2 + 4\\gamma\\nu + \\nu^2) - (4\\gamma^2 + 4\\gamma\\nu)}{2(\\gamma+\\nu)(2\\gamma+\\nu)} $$\n$$ E_{\\mathrm{norm}}(\\nu) = \\frac{\\nu^2}{2(\\gamma+\\nu)(2\\gamma+\\nu)} $$\nThis is the final closed-form expression for the error.\n\n### Solving the Constraints\nNow we solve the two inequalities for $\\nu$.\n1.  **Decorrelation Time Constraint:**\n    The requirement is $\\tau_{\\mathrm{A}}(\\nu) \\le \\tau_{\\max}$.\n    $$ \\frac{1}{\\gamma + \\nu} \\le \\tau_{\\max} $$\n    Since $\\gamma > 0$ and the collision frequency $\\nu$ must be non-negative, $\\gamma+\\nu > 0$. We can multiply without changing the inequality direction:\n    $$ 1 \\le \\tau_{\\max}(\\gamma + \\nu) \\implies \\frac{1}{\\tau_{\\max}} \\le \\gamma + \\nu \\implies \\nu \\ge \\frac{1}{\\tau_{\\max}} - \\gamma $$\n    Since $\\nu$ must be non-negative, the lower bound for $\\nu$ is:\n    $$ \\nu_{\\min} = \\max\\left(0, \\frac{1}{\\tau_{\\max}} - \\gamma\\right) $$\n\n2.  **Error Tolerance Constraint:**\n    The requirement is $E_{\\mathrm{norm}}(\\nu) \\le \\varepsilon$.\n    $$ \\frac{\\nu^2}{2(\\gamma+\\nu)(2\\gamma+\\nu)} \\le \\varepsilon $$\n    Since the denominator is positive for $\\nu \\ge 0$, we can multiply:\n    $$ \\nu^2 \\le 2\\varepsilon(\\gamma+\\nu)(2\\gamma+\\nu) \\implies \\nu^2 \\le 2\\varepsilon(2\\gamma^2 + 3\\gamma\\nu + \\nu^2) $$\n    Rearranging gives a quadratic inequality in $\\nu$:\n    $$ (2\\varepsilon - 1)\\nu^2 + 6\\varepsilon\\gamma\\nu + 4\\varepsilon\\gamma^2 \\ge 0 $$\n    Let $f(\\nu) = A\\nu^2 + B\\nu + C$ with $A = 2\\varepsilon - 1$, $B = 6\\varepsilon\\gamma$, and $C = 4\\varepsilon\\gamma^2$.\n\n    **Case A:** $\\varepsilon \\ge 1/2$.\n    If $\\varepsilon \\ge 1/2$, then $A \\ge 0$. The quadratic $f(\\nu)$ is a parabola opening upwards (or a line for $\\varepsilon=1/2$). Its roots are negative (or undefined if $A=B=0$), so for all $\\nu \\ge 0$, the inequality $f(\\nu) \\ge 0$ holds. Thus, there is no finite upper bound on $\\nu$.\n    $$ \\nu_{\\max} = \\infty \\quad (\\text{for } \\varepsilon \\ge 1/2) $$\n\n    **Case B:** $0 \\le \\varepsilon  1/2$.\n    Here, $A = 2\\varepsilon - 1  0$. The quadratic $f(\\nu)$ is a parabola opening downwards. The inequality $f(\\nu) \\ge 0$ holds for values of $\\nu$ between the two roots. One root is negative and the other is positive. The valid range for non-negative $\\nu$ is $[0, \\nu_{\\text{upper}}]$, where $\\nu_{\\text{upper}}$ is the positive root of $f(\\nu)=0$.\n    $$ \\nu_{\\text{upper}} = \\frac{-6\\varepsilon\\gamma - \\sqrt{(6\\varepsilon\\gamma)^2 - 4(2\\varepsilon-1)(4\\varepsilon\\gamma^2)}}{2(2\\varepsilon-1)} = \\gamma \\frac{3\\varepsilon + \\sqrt{9\\varepsilon^2 - 4\\varepsilon(2\\varepsilon-1)}}{1 - 2\\varepsilon} = \\gamma \\frac{3\\varepsilon + \\sqrt{\\varepsilon^2+4\\varepsilon}}{1 - 2\\varepsilon} $$\n    Therefore, the upper bound on $\\nu$ from the error constraint is:\n    $$ \\nu_{\\max} = \\gamma \\frac{3\\varepsilon + \\sqrt{\\varepsilon(\\varepsilon + 4)}}{1 - 2\\varepsilon} \\quad (\\text{for } 0 \\le \\varepsilon  1/2) $$\n\n### Summary of the Algorithm\n1.  Calculate $\\nu_{\\min} = \\max\\left(0, \\frac{1}{\\tau_{\\max}} - \\gamma\\right)$.\n2.  If $\\varepsilon \\ge 0.5$, set $\\nu_{\\max} = \\infty$. Otherwise, calculate $\\nu_{\\max} = \\gamma \\frac{3\\varepsilon + \\sqrt{\\varepsilon(\\varepsilon + 4)}}{1 - 2\\varepsilon}$.\n3.  The feasible range is $[\\nu_{\\min}, \\nu_{\\max}]$. If $\\nu_{\\min} > \\nu_{\\max}$, the range is empty.\nThis procedure provides the basis for the implementation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the feasible range of the Andersen thermostat collision frequency\n    based on decorrelation time and dynamical error constraints.\n    \"\"\"\n    # Test cases: (gamma, tau_max, epsilon)\n    # gamma in s^-1, tau_max in s, epsilon is dimensionless.\n    test_cases = [\n        (1.0, 1.0, 0.1),\n        (1.0, 0.2, 0.1),\n        (0.01, 5.0, 0.49),\n        (2.0, 0.05, 0.6),\n        (5.0, 0.1, 0.3),\n    ]\n\n    results = []\n    \n    for gamma, tau_max, epsilon in test_cases:\n        # Step 1: Compute the lower bound nu_min from the decorrelation time constraint.\n        # tau_A(nu) = 1/(gamma + nu) = tau_max  ==  nu = 1/tau_max - gamma.\n        # Since nu must be non-negative, nu_min = max(0, 1/tau_max - gamma).\n        nu_min = max(0.0, 1.0 / tau_max - gamma)\n\n        # Step 2: Compute the upper bound nu_max from the error tolerance constraint.\n        # E_norm(nu) = nu^2 / (2 * (gamma + nu) * (2*gamma + nu)) = epsilon.\n        # This leads to a quadratic inequality: (2*epsilon - 1)*nu^2 + 6*epsilon*gamma*nu + 4*epsilon*gamma^2 = 0.\n        \n        if epsilon = 0.5:\n            # For epsilon = 0.5, the error constraint is satisfied for all nu = 0.\n            # The parabola opens upwards (or is a line for epsilon=0.5) and is non-negative for nu = 0.\n            # Thus, the upper bound is effectively infinite.\n            nu_max = np.inf\n        else:\n            # For 0 = epsilon  0.5, the parabola opens downwards.\n            # The inequality holds between the roots. The operative range for nu = 0\n            # is [0, nu_upper_root].\n            # nu_max = gamma * (3*eps + sqrt(eps*(eps+4))) / (1 - 2*eps)\n            numerator = 3.0 * epsilon + np.sqrt(epsilon * (epsilon + 4.0))\n            denominator = 1.0 - 2.0 * epsilon\n            nu_max = gamma * numerator / denominator\n\n        # Step 3: Check for feasibility. The feasible range is the intersection\n        # [nu_min, Infinity) and [0, nu_max], which is [nu_min, nu_max].\n        # The range is non-empty if and only if nu_min = nu_max.\n        if nu_min > nu_max:\n            results.append(\"[]\")\n        else:\n            # Format the valid range.\n            formatted_min = f\"{nu_min:.6f}\"\n            if np.isinf(nu_max):\n                formatted_max = \"inf\"\n            else:\n                formatted_max = f\"{nu_max:.6f}\"\n            results.append(f\"[{formatted_min},{formatted_max}]\")\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}