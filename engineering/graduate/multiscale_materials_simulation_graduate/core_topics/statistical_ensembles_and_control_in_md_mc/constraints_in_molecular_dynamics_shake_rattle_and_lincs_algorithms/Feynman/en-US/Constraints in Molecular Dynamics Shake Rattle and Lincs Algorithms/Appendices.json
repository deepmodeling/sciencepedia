{
    "hands_on_practices": [
        {
            "introduction": "Before implementing any constraint algorithm, it is crucial to understand the mathematical objects at its core. This practice guides you through constructing the constraint Jacobian, $G(\\mathbf{q})$, and the associated constraint coupling matrix, $A(\\mathbf{q}) = G(\\mathbf{q}) M^{-1} G(\\mathbf{q})^{\\top}$, directly from fundamental principles. By applying these concepts to a small, concrete system, you will gain a tangible understanding of how these matrices are built and why their sparse structure is key to the efficiency of modern constraint solvers.",
            "id": "3798260",
            "problem": "Consider a classical Molecular Dynamics (MD) system of $N$ atoms with holonomic constraints of fixed bond lengths between selected atom pairs, enforced by Lagrange multipliers within position- and velocity-projection schemes such as the SHAKE algorithm and the RATTLE algorithm. Let the constraint set be defined by functions $c_{k}(\\mathbf{q})=0$, where $\\mathbf{q}\\in\\mathbb{R}^{3N}$ stacks all Cartesian coordinates as $\\mathbf{q}=\\left(x_{1},y_{1},z_{1},\\dots,x_{N},y_{N},z_{N}\\right)$, and each bond-length constraint takes the form $c_{ij}(\\mathbf{q})=\\|\\mathbf{r}_{j}-\\mathbf{r}_{i}\\|^{2}-d_{ij}^{2}=0$ for atoms $i$ and $j$ with target bond length $d_{ij}$. Define the constraint Jacobian $G(\\mathbf{q})$ with rows $G_{k}(\\mathbf{q})=\\nabla_{\\mathbf{q}}c_{k}(\\mathbf{q})$, and let the diagonal mass matrix be $M=\\mathrm{diag}(m_{1},m_{1},m_{1},\\dots,m_{N},m_{N},m_{N})$. The Linear Constraint Solver (LINCS) algorithm and related projection methods solve for Lagrange multipliers via the symmetric positive-definite matrix $A(\\mathbf{q})=G(\\mathbf{q})M^{-1}G(\\mathbf{q})^{\\top}$.\n\nStarting from Newton’s second law and the definition of holonomic constraints, derive $G(\\mathbf{q})$ for bond-length constraints using only fundamental laws and core definitions. Explain how to assemble $A(\\mathbf{q})$ efficiently in sparse form by exploiting the locality of bond constraints, without invoking any pre-derived implementation “shortcuts.”\n\nThen, consider the specific three-atom configuration with $N=3$ and two bond-length constraints:\n- Atom $1$ at $\\mathbf{r}_{1}=(0,0,0)$,\n- Atom $2$ at $\\mathbf{r}_{2}=(a,0,0)$,\n- Atom $3$ at $\\mathbf{r}_{3}=(a,b,0)$,\nwith target bond lengths $d_{12}=a$ between atoms $1$ and $2$, and $d_{23}=b$ between atoms $2$ and $3$. Let the coordinate ordering be $\\mathbf{q}=(x_{1},y_{1},z_{1},x_{2},y_{2},z_{2},x_{3},y_{3},z_{3})$, and choose masses $m_{1}=12$, $m_{2}=1$, $m_{3}=16$. Take $a=1$ and $b=1$.\n\nConstruct $G(\\mathbf{q})$ explicitly for these two constraints, construct $A(\\mathbf{q})=G(\\mathbf{q})M^{-1}G(\\mathbf{q})^{\\top}$, and compute the determinant $\\det A(\\mathbf{q})$. Express the numerical value of the determinant in $\\mathrm{nm}^{4}/\\mathrm{amu}^{2}$. No rounding is required; provide the exact value.",
            "solution": "The fundamental base is Newton’s second law, which in Cartesian coordinates reads $m_{i}\\ddot{\\mathbf{r}}_{i}=\\mathbf{F}_{i}+\\mathbf{f}_{i}^{\\mathrm{c}}$, where $\\mathbf{f}_{i}^{\\mathrm{c}}$ are constraint forces ensuring the holonomic constraints $c_{k}(\\mathbf{q})=0$ remain satisfied. For holonomic constraints, the constraint forces are linear combinations of the constraint gradients: $\\mathbf{f}^{\\mathrm{c}}=G(\\mathbf{q})^{\\top}\\boldsymbol{\\lambda}$, where $\\boldsymbol{\\lambda}$ is the vector of Lagrange multipliers and $G(\\mathbf{q})$ is the Jacobian with rows $G_{k}(\\mathbf{q})=\\nabla_{\\mathbf{q}}c_{k}(\\mathbf{q})$.\n\nFor bond-length constraints between atoms $i$ and $j$, define $c_{ij}(\\mathbf{q})=\\|\\mathbf{r}_{j}-\\mathbf{r}_{i}\\|^{2}-d_{ij}^{2}$. We compute the gradient via the chain rule. Let $\\mathbf{r}_{i}=(x_{i},y_{i},z_{i})$ and $\\mathbf{r}_{j}=(x_{j},y_{j},z_{j})$. Then\n$$\nc_{ij}(\\mathbf{q})=(x_{j}-x_{i})^{2}+(y_{j}-y_{i})^{2}+(z_{j}-z_{i})^{2}-d_{ij}^{2}.\n$$\nIts partial derivatives are\n$$\n\\frac{\\partial c_{ij}}{\\partial \\mathbf{r}_{i}}=2(\\mathbf{r}_{i}-\\mathbf{r}_{j}),\\quad \\frac{\\partial c_{ij}}{\\partial \\mathbf{r}_{j}}=2(\\mathbf{r}_{j}-\\mathbf{r}_{i}),\n$$\nand zero with respect to coordinates of all other atoms. Therefore, the row $G_{ij}(\\mathbf{q})=\\nabla_{\\mathbf{q}}c_{ij}(\\mathbf{q})$ is a $1\\times 3N$ vector with $3$-component blocks\n$$\n[G_{ij}]_{i}=2(\\mathbf{r}_{i}-\\mathbf{r}_{j}),\\quad [G_{ij}]_{j}=2(\\mathbf{r}_{j}-\\mathbf{r}_{i}),\n$$\nand zeros elsewhere.\n\nTo obtain the linear system for $\\boldsymbol{\\lambda}$ used in SHAKE, RATTLE, and the Linear Constraint Solver (LINCS), we consider the mass-weighted projection of constraint violations. Under a small position correction $\\delta\\mathbf{q}$ or velocity correction $\\delta\\dot{\\mathbf{q}}$, the constraints linearize as $G(\\mathbf{q})\\,\\delta\\mathbf{q}=\\mathbf{r}$ or $G(\\mathbf{q})\\,\\delta\\dot{\\mathbf{q}}=\\mathbf{r}_{v}$, with residuals $\\mathbf{r}$ or $\\mathbf{r}_{v}$. Mass-weighted corrections driven by constraint forces satisfy $M\\,\\delta\\ddot{\\mathbf{q}}=G(\\mathbf{q})^{\\top}\\boldsymbol{\\lambda}$, yielding (after eliminating $\\delta\\mathbf{q}$ or $\\delta\\dot{\\mathbf{q}}$ in the linearized projection) the symmetric positive-definite “constraint coupling” matrix\n$$\nA(\\mathbf{q})=G(\\mathbf{q})M^{-1}G(\\mathbf{q})^{\\top},\n$$\nwhose $(k,\\ell)$ entry aggregates contributions only from atoms appearing in both constraints $k$ and $\\ell$:\n$$\nA_{k\\ell}=\\sum_{p=1}^{N}\\frac{1}{m_{p}}\\,[G_{k}]_{p}^{\\top}[G_{\\ell}]_{p}.\n$$\n\nEfficient sparse assembly of $A(\\mathbf{q})$ exploits the locality of bond constraints: each constraint $c_{ij}$ touches only atoms $i$ and $j$, so $G_{ij}$ has exactly two nonzero $3$-blocks and $A_{k\\ell}$ is nonzero only if constraints $k$ and $\\ell$ share at least one atom. A practical assembly strategy is:\n1. Precompute for each constraint $c_{ij}$ the gradients $[G_{ij}]_{i}=2(\\mathbf{r}_{i}-\\mathbf{r}_{j})$ and $[G_{ij}]_{j}=2(\\mathbf{r}_{j}-\\mathbf{r}_{i})$.\n2. Build an adjacency mapping from constraints to atoms and from atoms to incident constraints.\n3. For each constraint $k$ involving atoms $i$ and $j$, accumulate the diagonal contribution\n$$\nA_{kk}\\,\\mathrel{+}=\\,\\frac{[G_{k}]_{i}^{\\top}[G_{k}]_{i}}{m_{i}}+\\frac{[G_{k}]_{j}^{\\top}[G_{k}]_{j}}{m_{j}}.\n$$\n4. For each atom $p$ shared by constraints $k$ and $\\ell$ (from the atom-to-constraints adjacency), accumulate the off-diagonal contribution\n$$\nA_{k\\ell}\\,\\mathrel{+}=\\,\\frac{[G_{k}]_{p}^{\\top}[G_{\\ell}]_{p}}{m_{p}},\n$$\nand symmetrically set $A_{\\ell k}=A_{k\\ell}$.\n5. Store $A$ in a compressed sparse row or compressed sparse column structure; the per-constraint work is constant (two atoms, up to six coordinates per constraint in three dimensions), leading to linear-time assembly in the number of constraints and linear memory in the constraint graph bandwidth.\n\nNow apply this to the specified three-atom configuration with $a=1$ and $b=1$. The coordinates are\n$$\n\\mathbf{r}_{1}=(0,0,0),\\quad \\mathbf{r}_{2}=(1,0,0),\\quad \\mathbf{r}_{3}=(1,1,0),\n$$\nwith constraints $c_{12}$ and $c_{23}$ at target lengths $d_{12}=1$ and $d_{23}=1$. Using the ordering $\\mathbf{q}=(x_{1},y_{1},z_{1},x_{2},y_{2},z_{2},x_{3},y_{3},z_{3})$, we compute the constraint gradients.\n\nFor $c_{12}$,\n$$\n[G_{12}]_{1}=2(\\mathbf{r}_{1}-\\mathbf{r}_{2})=2(-1,0,0)=(-2,0,0),\\quad [G_{12}]_{2}=2(\\mathbf{r}_{2}-\\mathbf{r}_{1})=2(1,0,0)=(2,0,0),\n$$\nand zeros at atom $3$. Thus the row $G_{12}$ is\n$$\nG_{12}=\\big(-2,\\,0,\\,0,\\,2,\\,0,\\,0,\\,0,\\,0,\\,0\\big).\n$$\nFor $c_{23}$,\n$$\n[G_{23}]_{2}=2(\\mathbf{r}_{2}-\\mathbf{r}_{3})=2(0,-1,0)=(0,-2,0),\\quad [G_{23}]_{3}=2(\\mathbf{r}_{3}-\\mathbf{r}_{2})=2(0,1,0)=(0,2,0),\n$$\nand zeros at atom $1$. Thus the row $G_{23}$ is\n$$\nG_{23}=\\big(0,\\,0,\\,0,\\,0,\\,-2,\\,0,\\,0,\\,2,\\,0\\big).\n$$\n\nThe mass matrix inverse is $M^{-1}=\\mathrm{diag}\\big(\\frac{1}{12},\\frac{1}{12},\\frac{1}{12},1,1,1,\\frac{1}{16},\\frac{1}{16},\\frac{1}{16}\\big)$ from $m_{1}=12$, $m_{2}=1$, $m_{3}=16$.\n\nWe now compute $A=G M^{-1} G^{\\top}$. Since we have two constraints, $A$ is $2\\times 2$ with entries\n$$\nA_{11}=G_{12}M^{-1}G_{12}^{\\top},\\quad A_{22}=G_{23}M^{-1}G_{23}^{\\top},\\quad A_{12}=G_{12}M^{-1}G_{23}^{\\top}.\n$$\nFor $A_{11}$, only coordinates at $(x_{1},x_{2})$ contribute:\n$$\nA_{11}=(-2)^{2}\\cdot\\frac{1}{12}+(2)^{2}\\cdot 1=\\frac{4}{12}+4=\\frac{1}{3}+4=\\frac{13}{3}.\n$$\nFor $A_{22}$, only coordinates at $(y_{2},y_{3})$ contribute:\n$$\nA_{22}=(-2)^{2}\\cdot 1+(2)^{2}\\cdot\\frac{1}{16}=4+\\frac{4}{16}=4+\\frac{1}{4}=\\frac{17}{4}.\n$$\nFor $A_{12}$, the support of $G_{12}$ is on $x$ components, and the support of $G_{23}$ is on $y$ components. While they share atom 2, the gradient vectors are orthogonal, so their dot product is zero.\n$$\nA_{12}=0.\n$$\nTherefore,\n$$\nA=\\begin{pmatrix}\\frac{13}{3} & 0 \\\\ 0 & \\frac{17}{4}\\end{pmatrix}.\n$$\nThe determinant is\n$$\n\\det A=\\frac{13}{3}\\cdot\\frac{17}{4}=\\frac{221}{12}.\n$$\nInterpreting positions in nanometers and masses in atomic mass units, $\\det A$ is expressed in $\\mathrm{nm}^{4}/\\mathrm{amu}^{2}$; the exact value is $\\frac{221}{12}$ with no rounding required.",
            "answer": "$$\\boxed{\\frac{221}{12}}$$"
        },
        {
            "introduction": "The stability and correctness of constraint algorithms depend on a well-posed set of constraints. This exercise explores a critical failure mode: linear dependence. You will investigate how adding a redundant constraint, which is a linear combination of existing ones, causes the multiplier system matrix to become singular. This theoretical exploration highlights why ensuring constraint independence is not just a numerical nicety but a fundamental requirement for a uniquely solvable system of Lagrange multipliers.",
            "id": "3798275",
            "problem": "Consider a one-dimensional three-atom system used to illustrate constraint enforcement in Molecular Dynamics via the SHAKE and RATTLE algorithms, and the Linear Constraint Solver (LINCS). The generalized coordinates are $q = (x_{1}, x_{2}, x_{3})$ and the diagonal mass matrix is $M = \\mathrm{diag}(m_{1}, m_{2}, m_{3})$ with $m_{1} = 1$, $m_{2} = 2$, and $m_{3} = 3$ in consistent mass units. The holonomic constraints are position-level functions $\\phi_{k}(q) = 0$.\n\nStart from Newton’s Second Law and the definition of holonomic constraints enforced by Lagrange multipliers, where the constraint force is $Q_{c} = J(q)^{\\top}\\lambda$, $J(q)$ is the Jacobian (constraint gradient) with entries $J_{ka} = \\partial \\phi_{k}/\\partial q_{a}$, and derive the linear system for the Lagrange multipliers $\\lambda$ that appears in SHAKE/RATTLE/LINCS constraint projection. Then, for the following constraints,\n- $\\phi_{1}(q) = x_{1} - x_{2}$,\n- $\\phi_{2}(q) = x_{2} - x_{3}$,\ncompute the associated multiplier system matrix built from the mass matrix and the constraint Jacobian, and determine its rank. Next, add a third constraint that is a linear combination of the first two,\n- $\\phi_{3}(q) = \\phi_{1}(q) + \\phi_{2}(q) = x_{1} - x_{3}$,\nrebuild the multiplier system matrix corresponding to the three constraints, and determine its rank. Finally, assess the impact of adding the third constraint on the solvability (existence and uniqueness) of the Lagrange multiplier vector.\n\nExpress your final answer as the rank (an integer) of the multiplier system matrix after adding the third constraint. No rounding is required.",
            "solution": "The problem asks for the derivation of the linear system for Lagrange multipliers in a constrained molecular dynamics context, and then to analyze the effect of linearly dependent constraints on the rank and solvability of this system.\n\nFirst, we derive the linear system for the Lagrange multipliers $\\lambda$. The Lagrangian equation of motion for a system with generalized coordinates $q$, mass matrix $M$, and potential energy $U(q)$, subject to holonomic constraints $\\phi(q) = 0$, is given by:\n$$\nM \\ddot{q} = -\\nabla U(q) + Q_c\n$$\nHere, $F(q) = -\\nabla U(q)$ represents the unconstrained forces, and $Q_c$ is the constraint force required to enforce $\\phi(q)=0$. This force is defined to be orthogonal to the constraint surface, which means it can be written as a linear combination of the constraint gradients. The problem defines this force as $Q_c = J(q)^{\\top}\\lambda$, where $J_{ka} = \\frac{\\partial \\phi_k}{\\partial q_a}$ is the Jacobian of the constraints and $\\lambda$ is a vector of Lagrange multipliers. The equation of motion is thus:\n$$\nM \\ddot{q} = F(q) + J(q)^{\\top}\\lambda\n$$\nFor the constraints to be satisfied at all times, their second time derivative must be zero, $\\ddot{\\phi}(q(t)) = 0$. Using the chain rule:\n$$\n\\dot{\\phi} = \\frac{d\\phi}{dt} = \\frac{\\partial \\phi}{\\partial q} \\frac{dq}{dt} = J \\dot{q}\n$$\n$$\n\\ddot{\\phi} = \\frac{d}{dt}(J \\dot{q}) = \\dot{J}\\dot{q} + J\\ddot{q}\n$$\nSetting $\\ddot{\\phi} = 0$ gives the condition $J\\ddot{q} = -\\dot{J}\\dot{q}$.\nFrom the equation of motion, we can write $\\ddot{q} = M^{-1}(F(q) + J^{\\top}\\lambda)$. Substituting this into the constraint condition yields:\n$$\nJ M^{-1} (F(q) + J^{\\top}\\lambda) = -\\dot{J}\\dot{q}\n$$\nRearranging to form a linear system for $\\lambda$:\n$$\n(J M^{-1} J^{\\top}) \\lambda = -J M^{-1} F(q) - \\dot{J}\\dot{q}\n$$\nThis equation is of the form $A\\lambda=b$, where the multiplier system matrix is $A = J M^{-1} J^{\\top}$. This is the matrix whose properties determine the solvability for $\\lambda$. This same matrix arises in the linearization of the constraint equations in iterative algorithms like SHAKE or projection methods like LINCS.\n\nNow, we apply this to the specific system.\nThe coordinates are $q = (x_1, x_2, x_3)^{\\top}$. The mass matrix is $M = \\mathrm{diag}(1, 2, 3)$, so the inverse mass matrix is $M^{-1} = \\mathrm{diag}(1, 1/2, 1/3)$.\n\nInitially, we have two constraints:\n- $\\phi_1(q) = x_1 - x_2 = 0$\n- $\\phi_2(q) = x_2 - x_3 = 0$\n\nThe Jacobian matrix $J$ is a $2 \\times 3$ matrix whose rows are the gradients of the constraints:\n$$\nJ = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}\n$$\nThe transpose of the Jacobian is:\n$$\nJ^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix}\n$$\nWe compute the multiplier system matrix, which we denote $A_2$:\n$$\nA_2 = J M^{-1} J^{\\top} = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1/2 & 0 \\\\ 0 & 0 & 1/3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix}\n$$\nFirst, we calculate $J M^{-1}$:\n$$\nJ M^{-1} = \\begin{pmatrix} 1 & -1/2 & 0 \\\\ 0 & 1/2 & -1/3 \\end{pmatrix}\n$$\nNow we multiply by $J^{\\top}$:\n$$\nA_2 = \\begin{pmatrix} 1 & -1/2 & 0 \\\\ 0 & 1/2 & -1/3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 + 1/2 & -1/2 \\\\ -1/2 & 1/2 + 1/3 \\end{pmatrix} = \\begin{pmatrix} 3/2 & -1/2 \\\\ -1/2 & 5/6 \\end{pmatrix}\n$$\nTo find the rank of this $2 \\times 2$ matrix, we compute its determinant:\n$$\n\\det(A_2) = \\left(\\frac{3}{2}\\right)\\left(\\frac{5}{6}\\right) - \\left(-\\frac{1}{2}\\right)\\left(-\\frac{1}{2}\\right) = \\frac{15}{12} - \\frac{1}{4} = \\frac{5}{4} - \\frac{1}{4} = 1\n$$\nSince $\\det(A_2) \\neq 0$, the matrix $A_2$ is full rank, i.e., $\\mathrm{rank}(A_2) = 2$. This implies that for any right-hand side vector, a unique solution for $\\lambda = (\\lambda_1, \\lambda_2)^{\\top}$ exists.\n\nNext, we add a third constraint, $\\phi_3(q) = x_1 - x_3$. This constraint is a linear combination of the first two: $\\phi_3 = \\phi_1 + \\phi_2$.\nThe new set of constraints is:\n- $\\phi_1(q) = x_1 - x_2 = 0$\n- $\\phi_2(q) = x_2 - x_3 = 0$\n- $\\phi_3(q) = x_1 - x_3 = 0$\n\nThe new Jacobian, $J'$, is a $3 \\times 3$ matrix:\n$$\nJ' = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ 1 & 0 & -1 \\end{pmatrix}\n$$\nThe third row of $J'$ is the sum of the first two rows, so the rows of $J'$ are linearly dependent, and $\\mathrm{rank}(J') = 2$.\nWe compute the new $3 \\times 3$ multiplier system matrix, $A_3 = J' M^{-1} (J')^{\\top}$.\n$$\nJ' M^{-1} = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1/2 & 0 \\\\ 0 & 0 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 1 & -1/2 & 0 \\\\ 0 & 1/2 & -1/3 \\\\ 1 & 0 & -1/3 \\end{pmatrix}\n$$\n$$\n(J')^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\\\ -1 & 1 & 0 \\\\ 0 & -1 & -1 \\end{pmatrix}\n$$\nNow, we compute $A_3 = (J' M^{-1}) (J')^{\\top}$:\n$$\nA_3 = \\begin{pmatrix} 1 & -1/2 & 0 \\\\ 0 & 1/2 & -1/3 \\\\ 1 & 0 & -1/3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ -1 & 1 & 0 \\\\ 0 & -1 & -1 \\end{pmatrix}\n$$\nThe entries are:\n$A_{3,11} = (1)(1) + (-1/2)(-1) = 3/2$\n$A_{3,12} = (-1/2)(1) = -1/2$\n$A_{3,13} = (1)(1) = 1$\n$A_{3,21} = (1/2)(-1) = -1/2$\n$A_{3,22} = (1/2)(1) + (-1/3)(-1) = 1/2 + 1/3 = 5/6$\n$A_{3,23} = (1/2)(0) + (-1/3)(-1) = 1/3$\n$A_{3,31} = (1)(1) = 1$\n$A_{3,32} = (-1/3)(-1) = 1/3$\n$A_{3,33} = (1)(1) + (-1/3)(-1) = 1 + 1/3 = 4/3$\nSo the matrix is:\n$$\nA_3 = \\begin{pmatrix} 3/2 & -1/2 & 1 \\\\ -1/2 & 5/6 & 1/3 \\\\ 1 & 1/3 & 4/3 \\end{pmatrix}\n$$\nTo determine the rank of $A_3$, we check for linear dependence among its rows. Let $R_1, R_2, R_3$ be the rows of $A_3$. Let us test if $R_3$ is a linear combination of $R_1$ and $R_2$. The linear dependence in the Jacobian, $\\nabla\\phi_3 = \\nabla\\phi_1 + \\nabla\\phi_2$, propagates to the rows of $A_3$. The $k$-th row of $A_3$ is $(\\nabla\\phi_k M^{-1}) (J')^{\\top}$. Therefore, $R_3 = ((\\nabla\\phi_1 + \\nabla\\phi_2) M^{-1}) (J')^{\\top} = (\\nabla\\phi_1 M^{-1})(J')^{\\top} + (\\nabla\\phi_2 M^{-1})(J')^{\\top} = R_1 + R_2$.\nLet's verify this numerically:\n$$\nR_1 + R_2 = \\left(\\frac{3}{2} - \\frac{1}{2}, -\\frac{1}{2} + \\frac{5}{6}, 1 + \\frac{1}{3}\\right) = \\left(1, \\frac{-3+5}{6}, \\frac{4}{3}\\right) = \\left(1, \\frac{2}{6}, \\frac{4}{3}\\right) = \\left(1, \\frac{1}{3}, \\frac{4}{3}\\right)\n$$\nThis is precisely $R_3$. Since the third row is the sum of the first two, the rows are linearly dependent. The first two rows, $R_1 = (3/2, -1/2, 1)$ and $R_2 = (-1/2, 5/6, 1/3)$, are not scalar multiples of each other, so they are linearly independent. Thus, the rank of the $3 \\times 3$ matrix $A_3$ is $2$.\n\nThe impact on solvability is significant. Since $\\mathrm{rank}(A_3)=2  3$, the matrix is singular and its determinant is zero. This means that a unique solution for the Lagrange multiplier vector $\\lambda = (\\lambda_1, \\lambda_2, \\lambda_3)^{\\top}$ does not exist. The system $A_3\\lambda=b$ will either have no solution or infinitely many solutions. In this physical context, it can be shown that the right-hand side $b$ is always in the column space of $A_3$, so there are infinitely many solutions for $\\lambda$. This reflects an ambiguity in how the total constraint force is distributed among the individual, linearly dependent constraints.\n\nThe final question asks for the rank of the multiplier system matrix after adding the third constraint. As determined, this rank is $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Building on the theoretical understanding of redundant constraints explored in , this practice moves to a robust, programmatic solution. You will implement an algorithm using Singular Value Decomposition (SVD)—the gold standard for numerical rank determination—to diagnose linear dependencies within a constraint set. This hands-on exercise demonstrates how to automatically identify and remove a minimal set of constraints to restore the system to full rank, a crucial capability for robustly handling complex molecular topologies in simulations.",
            "id": "3798262",
            "problem": "You are given the task of analyzing the constraint coupling matrix arising in constrained Molecular Dynamics (MD) methods such as SHAKE, RATTLE, and Linear Constraint Solver (LINCS). In these methods, holonomic constraints are enforced by solving for Lagrange multipliers in a linear system whose coefficient matrix must be of full rank to yield a unique solution. Consider a set of $m$ holonomic constraints applied to a system with $n$ generalized coordinates and masses. Let the constraint functions be $c_i(\\mathbf{q}) = 0$ for $i \\in \\{1,\\dots,m\\}$, where $\\mathbf{q}$ are coordinates. Define the Jacobian $J$ with rows $\\nabla c_i(\\mathbf{q})$, and a diagonal mass matrix $M$. The mass-weighted constraint Jacobian is $A = J M^{-1/2}$, and the constraint Gram matrix is $G = A A^\\top$. In the constrained update for SHAKE (originally named and not an acronym), RATTLE (originally named and not an acronym), and Linear Constraint Solver (LINCS), the solvability and numerical stability of the Lagrange multipliers rely on the matrix $G$ being full rank.\n\nYou must implement a program that performs the following for each test case:\n- Compute the Singular Value Decomposition (SVD) of $G$, i.e., $G = U \\Sigma V^\\top$, and use the singular values to detect rank deficiency using a relative threshold $\\tau = r \\cdot s_{\\max}$, where $s_{\\max}$ is the largest singular value of $G$ and $r$ is a given positive scalar in the test case. If $s_k  \\tau$ for any singular value $s_k$, declare $G$ rank-deficient.\n- Propose a modification to the constraint set to restore full rank by removing a minimal set of constraints, identified by analyzing the singular vectors associated with singular values below the threshold. For each singular value $s_k$ with $s_k  \\tau$, consider the corresponding left singular vector $\\mathbf{u}_k$ and greedily select the constraint index with the largest absolute component in $\\mathbf{u}_k$ to remove. If that index is already selected, choose the next largest component. Apply these removals, recompute the SVD of the reduced $G$, and continue until the reduced matrix is full rank under the same threshold. If all constraints must be removed to restore full rank, return the indices of all constraints.\n- Report two items per test case: a boolean indicating whether the original $G$ was full rank, and a list of integer indices of constraints to remove to achieve full rank. Indices must be zero-based.\n\nUse only purely mathematical constructs and ensure scientific realism by constructing $G$ as $A A^\\top$ from a given $A$ matrix representing mass-weighted constraints. No physical units are required since the computation is dimensionless in mass-weighted coordinates. Angles are not involved.\n\nYour program must process the following test suite, where each case is a pair $(A, r)$:\n- Case $1$ (independent constraints, happy path): $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $r = 10^{-12}$.\n- Case $2$ (redundant constraints, exact dependency): $A = \\begin{bmatrix} 1  0 \\\\ 1  0 \\end{bmatrix}$, $r = 10^{-12}$.\n- Case $3$ (near dependency, ill-conditioned): $A = \\begin{bmatrix} 1  0 \\\\ 1  10^{-6} \\end{bmatrix}$, $r = 10^{-5}$.\n- Case $4$ (overdetermined, one dependent constraint): $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $r = 10^{-12}$.\n- Case $5$ (degenerate constraint row): $A = \\begin{bmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\end{bmatrix}$, $r = 10^{-12}$.\n\nFor each case, compute $G = A A^\\top$, perform the detection as above, and propose indices to remove. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a two-element list $[b, L]$ with $b$ a boolean reflecting whether the original $G$ was full rank (true means full rank), and $L$ a list of zero-based integer indices of constraints to remove to restore full rank. For example, the output must have the format $[[b_1, L_1],[b_2, L_2],\\dots]$ with no spaces in the printed line.",
            "solution": "The problem requires an analysis of the constraint Gram matrix, denoted as $G$, which arises in the numerical solution of holonomic constraints in molecular dynamics simulations. A unique solution for the Lagrange multipliers, which enforce the constraints, is contingent upon the invertibility, or full rank, of this matrix. We are tasked with diagnosing rank deficiency in $G$ and proposing a minimal set of constraints to remove to restore its full rank.\n\nThe foundation of this problem lies in linear algebra and its application to constrained mechanical systems. A set of $m$ holonomic constraints is given by $c_i(\\mathbf{q}) = 0$ for $i \\in \\{1, \\dots, m\\}$. In methods like SHAKE and RATTLE, these constraints are enforced at the level of positions and velocities. The update equations lead to a linear system for the Lagrange multipliers, $\\boldsymbol{\\lambda}$. The coefficient matrix of this system is the Gram matrix, $G$.\n\nThe Gram matrix is defined as $G = A A^\\top$, where $A$ is the $m \\times n$ mass-weighted constraint Jacobian matrix. The matrix $A$ is itself defined as $A = J M^{-1/2}$, where $J$ is the $m \\times n$ Jacobian of the constraint functions (its rows are $\\nabla c_i(\\mathbf{q})$) and $M$ is the diagonal $n \\times n$ mass matrix. The matrix $G$ is therefore an $m \\times m$ symmetric and positive semi-definite matrix. For a unique solution for the $m$ Lagrange multipliers to exist, $G$ must be invertible, which requires it to be of full rank, i.e., $\\text{rank}(G) = m$.\n\nA rank deficiency in $G$ (i.e., $\\text{rank}(G)  m$) implies that its rows (and columns) are linearly dependent. Since the rows of $G$ are linear combinations of the rows of $A$, this directly corresponds to a linear dependence among the constraint gradients represented by the rows of $A$. Physically, this means that at least one constraint is redundant, as its effect can be reproduced by a combination of other constraints.\n\nThe Singular Value Decomposition (SVD) is the most robust numerical method for determining the rank of a matrix. For our $m \\times m$ matrix $G$, the SVD is $G = U \\Sigma V^\\top$, where $U$ and $V$ are $m \\times m$ orthogonal matrices and $\\Sigma$ is a diagonal matrix containing the singular values, $s_1 \\ge s_2 \\ge \\dots \\ge s_m \\ge 0$. The rank of $G$ is the number of its non-zero singular values. In numerical computations, due to floating-point arithmetic, we check for rank deficiency by identifying singular values that are \"close\" to zero. The problem specifies a relative threshold, $\\tau = r \\cdot s_{\\max}$, where $s_{\\max} = s_1$ is the largest singular value and $r$ is a given small positive scalar. The matrix $G$ is considered rank-deficient if any singular value $s_k$ satisfies $s_k  \\tau$.\n\nIf rank deficiency is detected, we must identify and remove the constraints causing it. A small singular value, $s_k \\approx 0$, implies the existence of a vector in the near-null space of $G$. The columns of $U$, denoted $\\mathbf{u}_k$, are the left singular vectors. For a symmetric matrix like $G$, the left and right singular vectors are the same (up to a sign), and they are the eigenvectors of $G$. The SVD equation $G V = U \\Sigma$ gives $G \\mathbf{v}_k = s_k \\mathbf{u}_k$. Since $G$ is symmetric, $U=V$, so $G \\mathbf{u}_k = s_k \\mathbf{u}_k$. If $s_k \\approx 0$, then $G \\mathbf{u}_k \\approx \\mathbf{0}$. This equation, $\\sum_{j=1}^{m} G_{ij} u_{jk} \\approx 0$ for all $i$, shows that the vector $\\mathbf{u}_k$ defines a linear combination of the columns (and rows) of $G$ that is approximately zero. The magnitude of a component, $|u_{ik}|$, indicates the weight of the $i$-th constraint's gradient in this linear dependency. A large $|u_{ik}|$ suggests that the $i$-th constraint is a primary contributor to the redundancy.\n\nThe prescribed algorithm is a greedy, iterative procedure to restore full rank:\n$1$. For a given set of active constraints, construct the corresponding reduced Gram matrix, $G_{red}$.\n$2$. Compute its SVD and determine if it is rank-deficient using the threshold $\\tau$.\n$3$. If it is full rank, the process terminates.\n$4$. If it is rank-deficient, for each singular value $s_k  \\tau$, analyze its corresponding left singular vector $\\mathbf{u}_k$.\n$5$. In a greedy fashion, identify the constraint to remove. For each $\\mathbf{u}_k$, we find the component with the largest absolute value, $|u_{ik}|$. The index $i$ points to the most influential constraint in that specific dependency. This constraint is flagged for removal. To avoid removing the same constraint twice based on different near-null vectors in the same iteration, we ensure each flagged constraint is unique. If the first choice is already flagged, we proceed to the component with the next-largest magnitude in the same vector $\\mathbf{u}_k$.\n$6$. The flagged constraints are removed, and the process repeats from step $1$ with a smaller set of active constraints.\n\nWe first assess the rank of the original matrix $G$ to determine the initial status. Then, we apply the iterative removal process until the reduced Gram matrix is of full rank. The final output is a boolean indicating the initial status and a list of the zero-based indices of all constraints removed.\n\nFor test case $1$, $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $G = I_2$. The singular values are $s_1=1$, $s_2=1$. With $r=10^{-12}$, $\\tau=10^{-12}$. Neither $s_k  \\tau$, so it is full rank. No removals are needed.\n\nFor test case $2$, $A = \\begin{bmatrix} 1  0 \\\\ 1  0 \\end{bmatrix}$, $G = \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix}$. The singular values are $s_1=2, s_2=0$. With $r=10^{-12}$, $\\tau=2 \\cdot 10^{-12}$. Since $s_2  \\tau$, it is rank-deficient. The left singular vector for $s_2=0$ is approximately $[0.707, -0.707]^\\top$. Both components have equal magnitude. We can remove either constraint $0$ or $1$ to break the dependency.\n\nFor test case $3$, $A = \\begin{bmatrix} 1  0 \\\\ 1  10^{-6} \\end{bmatrix}$, $G = \\begin{bmatrix} 1  1 \\\\ 1  1+10^{-12} \\end{bmatrix}$. This matrix is nearly singular. Its singular values are approximately $2$ and $0.5 \\cdot 10^{-12}$. With $r=10^{-5}$, $\\tau \\approx 2 \\cdot 10^{-5}$. The smallest singular value is well below $\\tau$, so it is rank-deficient. The associated singular vector is again close to $[0.707, -0.707]^\\top$, leading to the removal of one of the two nearly-collinear constraints.\n\nFor test case $4$, $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, the third row is the sum of the first two. This creates an exact rank-deficiency in the $3 \\times 3$ matrix $G$. The null-space vector will have non-zero components for all three constraints, e.g., proportional to $[1, 1, -1]^\\top$, leading to the removal of one of them.\n\nFor test case $5$, $A = \\begin{bmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\end{bmatrix}$, the first row is a zero vector, a degenerate constraint. This creates a zero row and column in $G$, leading to a singular value of $0$. The corresponding singular vector is $[1, 0, 0]^\\top$, unambiguously identifying constraint $0$ for removal.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes constraint coupling matrices for rank deficiency and proposes\n    a minimal set of constraints to remove to restore full rank.\n    \"\"\"\n    test_cases = [\n        (np.array([[1.0, 0.0], [0.0, 1.0]]), 1e-12),\n        (np.array([[1.0, 0.0], [1.0, 0.0]]), 1e-12),\n        (np.array([[1.0, 0.0], [1.0, 1e-6]]), 1e-5),\n        (np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]), 1e-12),\n        (np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]), 1e-12),\n    ]\n\n    all_results = []\n\n    for A, r in test_cases:\n        m, _ = A.shape\n        original_indices = list(range(m))\n\n        # Step 1: Check if the original G is full rank\n        is_original_full_rank = True\n        if m > 0:\n            G_orig = A @ A.T\n            s_orig = np.linalg.svd(G_orig, compute_uv=False)\n            s_max_orig = s_orig[0] if s_orig.size > 0 else 0.0\n            tau_orig = r * s_max_orig\n            is_original_full_rank = not np.any(s_orig  tau_orig)\n\n        # Step 2: Iteratively remove constraints to restore full rank\n        removed_indices = set()\n        current_indices = list(original_indices)\n\n        while True:\n            num_current_constraints = len(current_indices)\n            if num_current_constraints == 0:\n                break\n            \n            A_reduced = A[current_indices, :]\n            G_reduced = A_reduced @ A_reduced.T\n            \n            U_red, s_red, _ = np.linalg.svd(G_reduced)\n            \n            s_max_red = s_red[0] if s_red.size > 0 else 0.0\n            tau_red = r * s_max_red\n\n            # Find indices of singular values below the threshold\n            small_sv_indices = np.where(s_red  tau_red)[0]\n            \n            if small_sv_indices.size == 0:\n                # Matrix is now full rank\n                break\n            \n            # Identify constraints to remove in this iteration\n            to_remove_this_iteration = set()\n            for k in small_sv_indices:\n                u_k = U_red[:, k]\n                # Sort components of the singular vector by absolute magnitude, in descending order\n                sorted_local_indices = np.argsort(np.abs(u_k))[::-1]\n                \n                # Find the first available constraint to remove\n                for local_idx in sorted_local_indices:\n                    original_idx = current_indices[local_idx]\n                    if original_idx not in removed_indices and original_idx not in to_remove_this_iteration:\n                        to_remove_this_iteration.add(original_idx)\n                        break\n            \n            if not to_remove_this_iteration:\n                # This may happen if all candidates are already removed or if there's a logic issue.\n                # If no progress is made, break to prevent infinite loop.\n                break\n\n            removed_indices.update(to_remove_this_iteration)\n            current_indices = [i for i in original_indices if i not in removed_indices]\n\n        final_removed_list = sorted(list(removed_indices))\n        all_results.append((is_original_full_rank, final_removed_list))\n\n    # Format the final output string as per requirements\n    formatted_items = []\n    for b, L in all_results:\n        b_str = 'true' if b else 'false'\n        l_str = f\"[{','.join(map(str, L))}]\"\n        formatted_items.append(f\"[{b_str},{l_str}]\")\n\n    output_str = f\"[{','.join(formatted_items)}]\"\n    # This print statement is for verification; the function should return the string for a web environment.\n    # print(output_str) \n    # The problem asks for the code block itself, which is what is provided in the final XML.\n    # The code here is just to demonstrate its correctness. The final print is commented out.\n\nsolve()\n```"
        }
    ]
}