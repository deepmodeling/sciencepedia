{
    "hands_on_practices": [
        {
            "introduction": "The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is a cornerstone for modeling volatility in financial and energy time series. Before we can confidently detect a regime *change*, it is essential to understand the conditions that define a *stable* regime. This foundational exercise  has you derive the condition for covariance stationarity in a GARCH(1,1) process, a crucial diagnostic for determining if a model's parameters describe a predictable, mean-reverting system.",
            "id": "4068740",
            "problem": "An energy systems analyst studies deseasonalized and de-meaned hourly log-returns of day-ahead electricity prices, denoted by the series $\\{\\epsilon_t\\}$. The conditional heteroskedasticity of volatility is modeled using the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) framework, specifically a $\\text{GARCH}(1,1)$ process. The data-generating scheme is assumed to be\n$$\n\\epsilon_t = \\sqrt{h_t}\\, z_t,\\quad \\text{with}\\quad \\mathbb{E}[z_t]=0,\\ \\mathbb{E}[z_t^2]=1,\\ \\text{$\\{z_t\\}$ independent of $\\mathcal{F}_{t-1}$},\n$$\nand the conditional variance recursion\n$$\nh_t \\equiv \\mathbb{V}\\mathrm{ar}(\\epsilon_t \\mid \\mathcal{F}_{t-1}) = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta h_{t-1}.\n$$\nHere, $\\omega0$, $\\alpha \\ge 0$, and $\\beta \\ge 0$ are unknown parameters, and $\\mathcal{F}_{t-1}$ denotes the information set up to time $t-1$. In the context of regime change detection in energy markets, a key diagnostic is whether the volatility dynamics are covariance-stationary, because a loss of covariance stationarity can signal a transition to a new regime with structurally different risk.\n\nStarting from the fundamental definitions of covariance stationarity (existence of a finite, time-invariant unconditional variance) and the law of total expectation, derive the parameter restrictions that ensure the existence of a finite, time-invariant unconditional variance for $\\{\\epsilon_t\\}$ under the $\\text{GARCH}(1,1)$ recursion above, and obtain the expression for the unconditional variance in terms of $\\omega$, $\\alpha$, and $\\beta$.\n\nThen, using the parameter estimates $\\omega = 3.6 \\times 10^{-6}$, $\\alpha = 0.07$, and $\\beta = 0.90$ obtained from a high-frequency electricity price return series in a period with no detected regime change, compute the numerical value of the unconditional variance. Express your final answer as a pure number with no units and round your answer to four significant figures.",
            "solution": "The problem asks for two main derivations followed by a numerical calculation. First, we must derive the parameter restriction for the covariance stationarity of a $\\text{GARCH}(1,1)$ process. Second, we must find the expression for the unconditional variance under this restriction. Finally, we will compute the numerical value of this variance using the provided parameter estimates.\n\nThe $\\text{GARCH}(1,1)$ process for the log-returns $\\{\\epsilon_t\\}$ is defined by:\n$$\n\\epsilon_t = \\sqrt{h_t}\\, z_t\n$$\n$$\nh_t = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta h_{t-1}\n$$\nwhere $\\{z_t\\}$ is a sequence of independent and identically distributed random variables with $\\mathbb{E}[z_t] = 0$ and $\\mathbb{E}[z_t^2] = 1$. The parameters are constrained such that $\\omega  0$, $\\alpha \\ge 0$, and $\\beta \\ge 0$. The variable $h_t$ is the conditional variance of $\\epsilon_t$ given the information set $\\mathcal{F}_{t-1}$ up to time $t-1$, i.e., $h_t = \\mathbb{V}\\mathrm{ar}(\\epsilon_t \\mid \\mathcal{F}_{t-1})$.\n\nFor the process $\\{\\epsilon_t\\}$ to be covariance-stationary (or weakly stationary), its first two unconditional moments must exist and be time-invariant. Specifically, the unconditional mean $\\mathbb{E}[\\epsilon_t]$ must be a constant, and the unconditional variance $\\mathbb{V}\\mathrm{ar}(\\epsilon_t)$ must be a finite constant.\n\nFirst, let's compute the unconditional mean of $\\epsilon_t$. We use the law of total expectation:\n$$\n\\mathbb{E}[\\epsilon_t] = \\mathbb{E}[\\mathbb{E}[\\epsilon_t \\mid \\mathcal{F}_{t-1}]]\n$$\nThe inner expectation is:\n$$\n\\mathbb{E}[\\epsilon_t \\mid \\mathcal{F}_{t-1}] = \\mathbb{E}[\\sqrt{h_t}\\, z_t \\mid \\mathcal{F}_{t-1}]\n$$\nSince $h_t$ is a function of variables known at time $t-1$ (specifically $\\epsilon_{t-1}^2$ and $h_{t-1}$), it is $\\mathcal{F}_{t-1}$-measurable. Thus, we can treat $\\sqrt{h_t}$ as a constant with respect to the conditional expectation:\n$$\n\\mathbb{E}[\\epsilon_t \\mid \\mathcal{F}_{t-1}] = \\sqrt{h_t}\\, \\mathbb{E}[z_t \\mid \\mathcal{F}_{t-1}]\n$$\nThe problem states that $\\{z_t\\}$ is independent of $\\mathcal{F}_{t-1}$, which implies $\\mathbb{E}[z_t \\mid \\mathcal{F}_{t-1}] = \\mathbb{E}[z_t]$. We are given $\\mathbb{E}[z_t] = 0$. Therefore:\n$$\n\\mathbb{E}[\\epsilon_t \\mid \\mathcal{F}_{t-1}] = \\sqrt{h_t} \\cdot 0 = 0\n$$\nSubstituting this back into the law of total expectation:\n$$\n\\mathbb{E}[\\epsilon_t] = \\mathbb{E}[0] = 0\n$$\nThe unconditional mean of the process is $0$ for all $t$.\n\nNext, let's compute the unconditional variance, which we denote as $\\sigma^2$. Since the mean is $0$, the variance is equal to the second moment:\n$$\n\\sigma^2 = \\mathbb{V}\\mathrm{ar}(\\epsilon_t) = \\mathbb{E}[\\epsilon_t^2] - (\\mathbb{E}[\\epsilon_t])^2 = \\mathbb{E}[\\epsilon_t^2]\n$$\nAgain, we apply the law of total expectation:\n$$\n\\sigma^2 = \\mathbb{E}[\\epsilon_t^2] = \\mathbb{E}[\\mathbb{E}[\\epsilon_t^2 \\mid \\mathcal{F}_{t-1}]]\n$$\nThe inner expectation is the conditional variance of $\\epsilon_t$, since the conditional mean is $0$:\n$$\n\\mathbb{E}[\\epsilon_t^2 \\mid \\mathcal{F}_{t-1}] = \\mathbb{V}\\mathrm{ar}(\\epsilon_t \\mid \\mathcal{F}_{t-1}) + (\\mathbb{E}[\\epsilon_t \\mid \\mathcal{F}_{t-1}])^2 = h_t + 0^2 = h_t\n$$\nThis gives a crucial relationship: the unconditional variance of $\\epsilon_t$ is the unconditional expectation of the conditional variance $h_t$:\n$$\n\\sigma^2 = \\mathbb{E}[h_t]\n$$\nNow, we take the unconditional expectation of the $h_t$ recursion equation:\n$$\n\\mathbb{E}[h_t] = \\mathbb{E}[\\omega + \\alpha \\epsilon_{t-1}^2 + \\beta h_{t-1}]\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}[h_t] = \\omega + \\alpha \\mathbb{E}[\\epsilon_{t-1}^2] + \\beta \\mathbb{E}[h_{t-1}]\n$$\nFor covariance stationarity, the unconditional variance must be constant over time. Let $\\sigma^2 = \\mathbb{V}\\mathrm{ar}(\\epsilon_t) = \\mathbb{V}\\mathrm{ar}(\\epsilon_{t-1})$. This implies $\\mathbb{E}[\\epsilon_t^2] = \\mathbb{E}[\\epsilon_{t-1}^2] = \\sigma^2$. Consequently, the unconditional expectation of the conditional variance must also be constant, i.e., $\\mathbb{E}[h_t] = \\mathbb{E}[h_{t-1}] = \\sigma^2$.\n\nSubstituting these into the expected recursion:\n$$\n\\sigma^2 = \\omega + \\alpha \\sigma^2 + \\beta \\sigma^2\n$$\nWe can now solve for $\\sigma^2$:\n$$\n\\sigma^2 = \\omega + (\\alpha + \\beta) \\sigma^2\n$$\n$$\n\\sigma^2 - (\\alpha + \\beta) \\sigma^2 = \\omega\n$$\n$$\n\\sigma^2 (1 - (\\alpha + \\beta)) = \\omega\n$$\n$$\n\\sigma^2 = \\frac{\\omega}{1 - (\\alpha + \\beta)}\n$$\nThis is the expression for the unconditional variance of the $\\text{GARCH}(1,1)$ process.\n\nFor this unconditional variance $\\sigma^2$ to be well-defined, finite, and positive, two conditions must be met. Since $\\omega  0$ by definition, the denominator must be strictly positive:\n$$\n1 - (\\alpha + \\beta)  0\n$$\nThis yields the parameter restriction for covariance stationarity:\n$$\n\\alpha + \\beta  1\n$$\nCombined with the non-negativity constraints, the full set of conditions is $\\omega  0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta  1$.\n\nNow, we perform the numerical calculation using the provided parameter estimates:\n$$\n\\omega = 3.6 \\times 10^{-6}\n$$\n$$\n\\alpha = 0.07\n$$\n$$\n\\beta = 0.90\n$$\nFirst, we check the stationarity condition:\n$$\n\\alpha + \\beta = 0.07 + 0.90 = 0.97\n$$\nSince $0.97  1$, the condition for covariance stationarity is satisfied, and a finite, positive unconditional variance exists.\n\nWe compute the unconditional variance $\\sigma^2$ using the derived formula:\n$$\n\\sigma^2 = \\frac{\\omega}{1 - (\\alpha + \\beta)} = \\frac{3.6 \\times 10^{-6}}{1 - 0.97}\n$$\n$$\n\\sigma^2 = \\frac{3.6 \\times 10^{-6}}{0.03}\n$$\n$$\n\\sigma^2 = \\frac{3.6}{3 \\times 10^{-2}} \\times 10^{-6} = 1.2 \\times 10^2 \\times 10^{-6} = 1.2 \\times 10^{-4}\n$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $1.2 \\times 10^{-4}$, which in decimal form is $0.00012$. To express this with four significant figures, we write it as $0.0001200$ or, in scientific notation, $1.200 \\times 10^{-4}$.",
            "answer": "$$\\boxed{1.200 \\times 10^{-4}}$$"
        },
        {
            "introduction": "Moving from single-regime models, we now explore systems with explicit, state-dependent dynamics. The Threshold Autoregressive (TAR) model offers an intuitive framework for processes that switch behavior based on an observable variable, such as energy demand's response to temperature. This hands-on problem  requires you to compute a regime-specific impulse response function (IRF), providing a clear, practical demonstration of how a system's reaction to a shock can differ dramatically across regimes.",
            "id": "4068748",
            "problem": "Consider an energy demand deviation process modeled as a Threshold Autoregressive (TAR) system, where the short-run dynamics depend on the ambient temperature regime. Let the deviation of net load from its baseline at discrete time index $t$ be denoted by $y_t$ (in megawatts). The process follows a first-order TAR recursion with a single threshold on temperature:\n$$\ny_t =\n\\begin{cases}\n\\phi_{\\text{cold}} \\, y_{t-1} + \\varepsilon_t,  \\text{if } T_{t-1}  \\tau, \\\\\n\\phi_{\\text{warm}} \\, y_{t-1} + \\varepsilon_t,  \\text{if } T_{t-1} \\ge \\tau,\n\\end{cases}\n$$\nwhere $T_{t}$ is the ambient temperature (in degrees Celsius), $\\tau$ is a fixed threshold (in degrees Celsius), $\\phi_{\\text{cold}}$ and $\\phi_{\\text{warm}}$ are regime-specific autoregressive coefficients, and $\\varepsilon_t$ is an additive shock.\n\nDefine the impulse response function (IRF) in this nonlinear setting as the deterministic path of $y_t$ when a single shock of magnitude $s$ is applied at time $t=0$, with $y_{-1}=0$, $\\varepsilon_0 = s$, and $\\varepsilon_t = 0$ for all $t \\ge 1$. Under this definition, the IRF sequence $\\{y_t\\}_{t=0}^H$ over a finite horizon $H$ obeys\n$$\ny_0 = s, \\quad\ny_t =\n\\begin{cases}\n\\phi_{\\text{cold}} \\, y_{t-1},  \\text{if } T_{t-1}  \\tau, \\\\\n\\phi_{\\text{warm}} \\, y_{t-1},  \\text{if } T_{t-1} \\ge \\tau,\n\\end{cases}\n\\quad \\text{for } t=1,2,\\dots,H,\n$$\nwith the regime at step $t$ determined by the temperature at the previous time $T_{t-1}$. The temperature path $\\{T_{0}, T_{1}, \\dots, T_{H-1}\\}$ is treated as exogenous and known.\n\nYour task is to implement a program that, given regime coefficients, a threshold, a shock magnitude, a horizon, and a temperature path, computes the regime-specific IRF sequence $\\{y_t\\}_{t=0}^H$ according to the above recursion. All IRF values must be returned in megawatts, rounded to six decimal places.\n\nStarting from accepted foundations in time series analysis and energy systems modeling, the derivation must rely only on the following base:\n- The autoregressive definition that a first-order autoregression satisfies $y_t = \\phi \\, y_{t-1} + \\varepsilon_t$.\n- The definition of an impulse response function as the propagation of a one-time shock with all subsequent shocks set to zero.\n- The regime selection rule based on an exogenous threshold variable where the coefficients switch according to whether $T_{t-1}$ is below or above the threshold $\\tau$.\n\nNo additional shortcut formulas are to be provided in the problem statement.\n\nImplement your program to compute the IRF for the following test suite, which explores different facets of regime-specific shock propagation:\n\n- Test Case 1 (Persistent cold): $\\phi_{\\text{cold}} = 0.8$, $\\phi_{\\text{warm}} = 0.3$, $\\tau = 0.0$ degrees Celsius, $s = 100.0$ megawatts, $H = 10$, and a constant temperature path $T_t = -5.0$ degrees Celsius for all $t=0,\\dots,9$.\n\n- Test Case 2 (Persistent warm with higher threshold): $\\phi_{\\text{cold}} = 0.7$, $\\phi_{\\text{warm}} = 0.4$, $\\tau = 15.0$ degrees Celsius, $s = 120.0$ megawatts, $H = 8$, and a constant temperature path $T_t = 20.0$ degrees Celsius for all $t=0,\\dots,7$.\n\n- Test Case 3 (Regime switch from cold to warm): $\\phi_{\\text{cold}} = 0.98$, $\\phi_{\\text{warm}} = 0.4$, $\\tau = 0.0$ degrees Celsius, $s = 50.0$ megawatts, $H = 10$, and a temperature path $T_t$ given by the list $[-3.0,-2.0,-1.0,0.5,5.0,5.0,5.0,5.0,5.0,5.0]$.\n\n- Test Case 4 (Threshold equality boundary with sign change): $\\phi_{\\text{cold}} = 0.6$, $\\phi_{\\text{warm}} = -0.1$, $\\tau = 10.0$ degrees Celsius, $s = 80.0$ megawatts, $H = 9$, and a temperature path $T_t$ given by the list $[10.0,10.0,9.5,10.0,10.5,10.0,10.0,10.0,10.0]$. Note that the warm regime is used when $T_{t-1} \\ge \\tau$ by definition.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each \"result\" must itself be a bracketed comma-separated list of floats (rounded to six decimals) representing the IRF sequence $[y_0,y_1,\\dots,y_H]$ in megawatts for the corresponding test case.",
            "solution": "The problem is valid. It is scientifically grounded in the established theory of nonlinear time series analysis, specifically Threshold Autoregressive (TAR) models, which are frequently used in modeling physical and economic systems with regime-switching behavior, such as energy demand. The problem is well-posed, providing a complete and unambiguous set of definitions, parameters, and a clear recursive formula, ensuring a unique and computable solution. All terms are defined objectively and precisely, and the provided test cases are consistent and computationally feasible.\n\nThe problem asks for the computation of an impulse response function (IRF) for a first-order TAR model. The model describes the evolution of the net load deviation, $y_t$, as a function of its past value, $y_{t-1}$, and an exogenous threshold variable, the ambient temperature $T_{t-1}$. The system dynamics switch between two regimes, \"cold\" and \"warm,\" depending on whether the temperature is below or above a given threshold $\\tau$.\n\nThe model is defined by the recursion:\n$$\ny_t =\n\\begin{cases}\n\\phi_{\\text{cold}} \\, y_{t-1} + \\varepsilon_t,  \\text{if } T_{t-1}  \\tau \\\\\n\\phi_{\\text{warm}} \\, y_{t-1} + \\varepsilon_t,  \\text{if } T_{t-1} \\ge \\tau\n\\end{cases}\n$$\nHere, $y_t$ is the deviation in megawatts, $T_{t-1}$ is the temperature in degrees Celsius at time $t-1$, $\\tau$ is the temperature threshold, $\\phi_{\\text{cold}}$ and $\\phi_{\\text{warm}}$ are the autoregressive coefficients for the cold and warm regimes, respectively, and $\\varepsilon_t$ is a random shock term.\n\nThe IRF is defined as the deterministic path of the system's response to a single, one-time shock. Specifically, we are given an initial shock $\\varepsilon_0 = s$ at time $t=0$, with the system starting from rest ($y_{-1}=0$) and no subsequent shocks ($\\varepsilon_t = 0$ for $t \\ge 1$). Substituting these conditions into the general model yields the recursive formula for the IRF sequence $\\{y_t\\}_{t=0}^H$ over a finite horizon $H$:\n\nThe initial response at time $t=0$ is simply the shock itself, as $y_0 = \\phi \\cdot y_{-1} + \\varepsilon_0 = \\phi \\cdot 0 + s$.\n$$\ny_0 = s\n$$\nFor all subsequent times $t=1, 2, \\dots, H$, the shock term is zero, and the evolution is governed by the deterministic part of the model:\n$$\ny_t =\n\\begin{cases}\n\\phi_{\\text{cold}} \\, y_{t-1},  \\text{if } T_{t-1}  \\tau \\\\\n\\phi_{\\text{warm}} \\, y_{t-1},  \\text{if } T_{t-1} \\ge \\tau\n\\end{cases}\n$$\nThis is a first-order difference equation where the coefficient is not constant but is selected at each time step based on the exogenously given temperature path $\\{T_0, T_1, \\dots, T_{H-1}\\}$.\n\nThe algorithm to compute the IRF sequence $\\{y_0, y_1, \\dots, y_H\\}$ is a direct, iterative application of this recursive definition.\n\n1.  Initialize an empty sequence to store the IRF values. Let this be denoted by $\\mathcal{Y}$.\n2.  Set the initial value of the response, $y_0$, to $s$ and add it to the sequence $\\mathcal{Y}$.\n3.  Iterate for each time step $t$ from $1$ to $H$. In each step:\n    a. Retrieve the previous value of the response, $y_{t-1}$, which is the last element computed and stored in $\\mathcal{Y}$.\n    b. Retrieve the temperature at the previous time step, $T_{t-1}$, from the given temperature path.\n    c. Compare $T_{t-1}$ with the threshold $\\tau$ to determine the active regime and select the corresponding autoregressive coefficient, $\\phi$.\n    $$\n    \\phi =\n    \\begin{cases}\n    \\phi_{\\text{cold}},  \\text{if } T_{t-1}  \\tau \\\\\n    \\phi_{\\text{warm}},  \\text{if } T_{t-1} \\ge \\tau\n    \\end{cases}\n    $$\n    d. Compute the next value of the response, $y_t = \\phi \\cdot y_{t-1}$.\n    e. Append $y_t$ to the sequence $\\mathcal{Y}$.\n4.  After iterating up to $t=H$, the sequence $\\mathcal{Y}$ will contain the $H+1$ elements of the IRF, $\\{y_0, y_1, \\dots, y_H\\}$.\n5.  As per the problem's requirement, each value in the final sequence must be rounded to six decimal places.\n\nLet us trace this procedure for Test Case 3: $\\phi_{\\text{cold}} = 0.98$, $\\phi_{\\text{warm}} = 0.4$, $\\tau = 0.0$, $s = 50.0$, $H = 10$, and $T_t = [-3.0, -2.0, -1.0, 0.5, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]$.\n\n-   $t=0$: $y_0 = s = 50.0$.\n-   $t=1$: The relevant temperature is $T_0 = -3.0$. Since $-3.0  0.0$, we use $\\phi_{\\text{cold}} = 0.98$.\n    $y_1 = \\phi_{\\text{cold}} \\cdot y_0 = 0.98 \\times 50.0 = 49.0$.\n-   $t=2$: The relevant temperature is $T_1 = -2.0$. Since $-2.0  0.0$, we use $\\phi_{\\text{cold}} = 0.98$.\n    $y_2 = \\phi_{\\text{cold}} \\cdot y_1 = 0.98 \\times 49.0 = 48.02$.\n-   $t=3$: The relevant temperature is $T_2 = -1.0$. Since $-1.0  0.0$, we use $\\phi_{\\text{cold}} = 0.98$.\n    $y_3 = \\phi_{\\text{cold}} \\cdot y_2 = 0.98 \\times 48.02 = 47.0596$.\n-   $t=4$: The relevant temperature is $T_3 = 0.5$. Since $0.5 \\ge 0.0$, the regime switches. We use $\\phi_{\\text{warm}} = 0.4$.\n    $y_4 = \\phi_{\\text{warm}} \\cdot y_3 = 0.4 \\times 47.0596 = 18.82384$.\n-   $t=5$: The relevant temperature is $T_4 = 5.0$. Since $5.0 \\ge 0.0$, we continue in the warm regime.\n    $y_5 = \\phi_{\\text{warm}} \\cdot y_4 = 0.4 \\times 18.82384 = 7.529536$.\n\nThis process continues iteratively for $t=6, \\dots, 10$. The implementation will generalize this logic for any given set of parameters. Special attention is given to the boundary condition $T_{t-1} = \\tau$, which, by definition, falls into the warm regime.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed for this problem.\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the impulse response function (IRF)\n    for a Threshold Autoregressive (TAR) process for several test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1: (phi_cold, phi_warm, tau, s, H, T_path)\n        (0.8, 0.3, 0.0, 100.0, 10, [-5.0] * 10),\n        # Test Case 2\n        (0.7, 0.4, 15.0, 120.0, 8, [20.0] * 8),\n        # Test Case 3\n        (0.98, 0.4, 0.0, 50.0, 10, [-3.0, -2.0, -1.0, 0.5, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]),\n        # Test Case 4\n        (0.6, -0.1, 10.0, 80.0, 9, [10.0, 10.0, 9.5, 10.0, 10.5, 10.0, 10.0, 10.0, 10.0]),\n    ]\n\n    def compute_irf(phi_cold, phi_warm, tau, s, H, T_path):\n        \"\"\"\n        Computes the IRF for a TAR(1) model.\n\n        Args:\n            phi_cold (float): AR coefficient for the cold regime.\n            phi_warm (float): AR coefficient for the warm regime.\n            tau (float): Temperature threshold.\n            s (float): Magnitude of the initial shock.\n            H (int): Horizon for the IRF.\n            T_path (list of float): Exogenous temperature path of length H.\n\n        Returns:\n            list of float: The IRF sequence [y_0, y_1, ..., y_H],\n                           with each value rounded to 6 decimal places.\n        \"\"\"\n        irf_sequence = []\n\n        # Initial value of the response at t=0\n        y_current = s\n        irf_sequence.append(y_current)\n\n        # Iterate from t=1 to H to compute the rest of the sequence\n        for t in range(1, H + 1):\n            # The regime is determined by the temperature at the previous time_step\n            # For y_t, we need T_{t-1}\n            temp_prev = T_path[t - 1]\n            y_prev = irf_sequence[t - 1]\n\n            if temp_prev  tau:\n                # Cold regime\n                phi = phi_cold\n            else:\n                # Warm regime (handles T_{t-1} = tau)\n                phi = phi_warm\n            \n            y_current = phi * y_prev\n            irf_sequence.append(y_current)\n        \n        # Round all values in the sequence to 6 decimal places\n        rounded_irf = [round(val, 6) for val in irf_sequence]\n        \n        return rounded_irf\n\n    results = []\n    for case in test_cases:\n        phi_cold, phi_warm, tau, s, H, T_path = case\n        result_sequence = compute_irf(phi_cold, phi_warm, tau, s, H, T_path)\n        \n        # Format the sequence as a string \"[v1,v2,...]\"\n        result_str = \"[\" + \",\".join(map(str, result_sequence)) + \"]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many real-world scenarios, the timing of a structural break is unknown. This is where flexible, non-parametric methods like Gaussian Processes (GPs) provide a powerful solution. By constructing a specialized \"changepoint kernel,\" a GP can use the data to infer the most likely location of a regime shift. This capstone exercise  guides you through the process of building a complete GP model to not only detect a changepoint in a synthetic solar generation series but also to quantify forecasting uncertainty around that break.",
            "id": "4068711",
            "problem": "You are tasked with building a complete, runnable program that applies a Gaussian Process (GP) with a changepoint kernel to forecast solar photovoltaic generation and compute predictive means and predictive variances across a detected regime change. The context is energy systems modeling with advanced time series modeling and regime change detection. All angles must be treated in radians. All predictive means must be expressed in kilowatts and all predictive variances must be expressed in the square of kilowatts, but you must output raw numerical values as floating-point numbers without unit strings.\n\nAssume a Gaussian Process (GP) prior for a scalar time series $y(t)$ with zero mean and covariance $k(t,t')$. The training inputs are times $t$ measured in days and the outputs $y(t)$ are deterministic solar generation measurements derived from a physically plausible diurnal pattern. For solar generation, use the physically realistic half-wave sinusoid\n$$\ng(t) = \\max\\{0, \\sin(2\\pi t)\\},\n$$\nwhich models daylight production over a period of $p=1$ day. Consider a regime change at an unknown break location $t_0$ in days, using a smooth logistic transition weight\n$$\nw(t; a, t_0) = \\frac{1}{1 + \\exp\\left(-a\\,(t - t_0)\\right)},\n$$\nwhere $a$ is a positive steepness parameter. Define a base covariance that captures both daily periodicity and slow temporal correlation, as the sum of a periodic kernel and a squared-exponential kernel:\n$$\nk_{\\mathrm{per}}(t,t'; p, \\ell_p) = \\exp\\!\\left(-\\frac{2\\sin^2\\!\\left(\\pi\\,\\frac{|t-t'|}{p}\\right)}{\\ell_p^2}\\right),\n$$\n$$\nk_{\\mathrm{se}}(t,t'; \\ell_{\\mathrm{se}}) = \\exp\\!\\left(-\\frac{(t-t')^2}{2\\ell_{\\mathrm{se}}^2}\\right),\n$$\nand\n$$\nk_{\\mathrm{base}}(t,t') = k_{\\mathrm{per}}(t,t'; p, \\ell_p) + k_{\\mathrm{se}}(t,t'; \\ell_{\\mathrm{se}}).\n$$\nLet pre- and post-regime amplitudes be $ \\alpha_1 $ and $ \\alpha_2 $, respectively. The changepoint covariance is specified as\n$$\nk_{\\mathrm{cp}}(t,t') = \\bigl(1-w(t; a, t_0)\\bigr)\\bigl(1-w(t'; a, t_0)\\bigr)\\,\\alpha_1^2\\, k_{\\mathrm{base}}(t,t') \\;+\\; w(t; a, t_0)\\, w(t'; a, t_0)\\,\\alpha_2^2\\, k_{\\mathrm{base}}(t,t').\n$$\nAssume independent and identically distributed Gaussian observation noise with variance $\\sigma_n^2$ so that the training covariance is $K = k_{\\mathrm{cp}}(X,X) + \\sigma_n^2 I$, where $X$ is the vector of training times.\n\nYour program must:\n- Generate training data deterministically for each test case by sampling times $t$ from $0$ to $T_{\\mathrm{end}}$ in steps of $\\Delta t = 1/6$ day. The true solar generation is\n$$\ny(t) = \\begin{cases}\n\\alpha_1\\, g(t),  t  t_{\\mathrm{break}}\\\\\n\\alpha_2\\, g(t),  t \\ge t_{\\mathrm{break}}\n\\end{cases}\n$$\nwith no random noise added to $y(t)$. The quantity $t_{\\mathrm{break}}$ is the true break used to construct the data; it is unknown to the GP and is not to be used by the algorithm except for constructing the training set. The GP model uses the changepoint kernel with fixed $(\\alpha_1,\\alpha_2)$ taken from the data-generating amplitudes and fixed $(p,\\ell_p,\\ell_{\\mathrm{se}},a,\\sigma_n)$ specified below.\n- Detect the break location $t_0$ by maximizing the log marginal likelihood over a grid of candidate break locations $t_0 \\in [0.5, T_{\\mathrm{end}}-0.5]$ in increments of $0.25$ days, holding all other hyperparameters fixed. Use the standard Gaussian Process log marginal likelihood definition and a Cholesky factorization for numerical stability, with a diagonal jitter added to the covariance for robustness.\n- With the detected break $\\hat{t}_0$, compute the GP predictive mean and predictive variance for the latent function at three query times: $t_q^{-} = \\hat{t}_0 - 0.25$, $t_q^{0} = \\hat{t}_0$, and $t_q^{+} = \\hat{t}_0 + 0.25$. Use the posterior predictive mean and variance for the latent function (exclude observation noise from the variance at the query), derived from the Gaussian prior and the Gaussian likelihood. Express the means in kilowatts and the variances in the square of kilowatts. Angles in all trigonometric functions must use radians.\n\nFixed hyperparameters for all test cases:\n- Period $p = 1$.\n- Periodic length scale $\\ell_p = 0.3$.\n- Squared-exponential length scale $\\ell_{\\mathrm{se}} = 1.5$.\n- Logistic steepness $a = 15$.\n- Observation noise standard deviation $\\sigma_n = 0.05$.\n\nTest Suite (each tuple supplies $(T_{\\mathrm{end}}, t_{\\mathrm{break}}, \\alpha_1, \\alpha_2)$):\n- Case $1$: $(30, 12, 5.0, 3.5)$.\n- Case $2$ (boundary break and amplitude increase): $(10, 1.5, 4.0, 6.0)$.\n- Case $3$: $(20, 15, 6.0, 4.2)$.\n- Case $4$ (no-break scenario in data, but proceed with detection): $(15, 0.0, 5.0, 5.0)$.\n\nAlgorithmic requirements:\n- Use the above definitions and fundamental properties of Gaussian random vectors to derive and implement the posterior and log marginal likelihood needed for detection and forecasting. Do not assume any shortcut formulas not derivable from the Gaussian Process prior and Gaussian likelihood.\n- Use a diagonal jitter of $10^{-8}$ added to the covariance matrix to ensure numerically stable Cholesky factorization.\n\nFinal output format:\n- For each test case, produce a list of $6$ floating-point numbers in the order $[\\mu^{-}, v^{-}, \\mu^{0}, v^{0}, \\mu^{+}, v^{+}]$, where $\\mu^{\\cdot}$ are predictive means (in kilowatts) and $v^{\\cdot}$ are predictive variances (in the square of kilowatts) at $t_q^{-}$, $t_q^{0}$, and $t_q^{+}$, respectively.\n- Aggregate the results for all test cases into a single Python list, flattened and concatenated in order of the test suite, and print exactly one line containing this list in the format $[r_1,r_2,\\dots,r_{24}]$, where $24$ is the total number of values across the four test cases.",
            "solution": "The problem is evaluated against the validation criteria.\n\n**Step 1: Extract Givens**\n- **Data-generating model:** $y(t) = \\begin{cases} \\alpha_1\\, g(t),  t  t_{\\mathrm{break}}\\\\ \\alpha_2\\, g(t),  t \\ge t_{\\mathrm{break}} \\end{cases}$, with $g(t) = \\max\\{0, \\sin(2\\pi t)\\}$.\n- **GP Prior:** Zero mean. Covariance $k(t,t')$ is a changepoint kernel $k_{\\mathrm{cp}}(t,t')$.\n- **Changepoint Kernel:** $k_{\\mathrm{cp}}(t,t') = \\bigl(1-w(t; a, t_0)\\bigr)\\bigl(1-w(t'; a, t_0)\\bigr)\\,\\alpha_1^2\\, k_{\\mathrm{base}}(t,t') \\;+\\; w(t; a, t_0)\\, w(t'; a, t_0)\\,\\alpha_2^2\\, k_{\\mathrm{base}}(t,t')$.\n- **Transition Weight:** $w(t; a, t_0) = \\frac{1}{1 + \\exp\\left(-a\\,(t - t_0)\\right)}$.\n- **Base Kernel:** $k_{\\mathrm{base}}(t,t') = k_{\\mathrm{per}}(t,t'; p, \\ell_p) + k_{\\mathrm{se}}(t,t'; \\ell_{\\mathrm{se}})$.\n- **Constituent Kernels:** $k_{\\mathrm{per}}(t,t'; p, \\ell_p) = \\exp\\!\\left(-\\frac{2\\sin^2\\!\\left(\\pi\\,\\frac{|t-t'|}{p}\\right)}{\\ell_p^2}\\right)$ and $k_{\\mathrm{se}}(t,t'; \\ell_{\\mathrm{se}}) = \\exp\\!\\left(-\\frac{(t-t')^2}{2\\ell_{\\mathrm{se}}^2}\\right)$.\n- **Observation Noise:** i.i.d. Gaussian with variance $\\sigma_n^2$.\n- **Training Covariance:** $K_y = k_{\\mathrm{cp}}(X,X) + \\sigma_n^2 I$.\n- **Training Data Sampling:** Times $t$ from $0$ to $T_{\\mathrm{end}}$ in steps of $\\Delta t = 1/6$ day.\n- **Changepoint Detection:** Maximize log marginal likelihood over $t_0 \\in [0.5, T_{\\mathrm{end}}-0.5]$ with step $0.25$ days.\n- **Prediction Query Times:** $t_q^{-} = \\hat{t}_0 - 0.25$, $t_q^{0} = \\hat{t}_0$, and $t_q^{+} = \\hat{t}_0 + 0.25$, where $\\hat{t}_0$ is the detected break.\n- **Fixed Hyperparameters:** $p = 1$, $\\ell_p = 0.3$, $\\ell_{\\mathrm{se}} = 1.5$, $a = 15$, $\\sigma_n = 0.05$.\n- **Numerical Stability:** Jitter of $10^{-8}$ is added to the covariance diagonal.\n- **Test Cases** $(T_{\\mathrm{end}}, t_{\\mathrm{break}}, \\alpha_1, \\alpha_2)$: Case $1$: $(30, 12, 5.0, 3.5)$; Case $2$: $(10, 1.5, 4.0, 6.0)$; Case $3$: $(20, 15, 6.0, 4.2)$; Case $4$: $(15, 0.0, 5.0, 5.0)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound, based on the established framework of Gaussian Processes. All models, parameters, and procedures are explicitly defined, rendering the problem well-posed, objective, and complete. The computational task is feasible. No inconsistencies or violations of scientific principles are found.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A principled solution follows.\n\n**Theoretical Formulation**\nA Gaussian Process (GP) defines a distribution over functions. We model the time series $f(t)$ as being drawn from a GP with a zero mean function and a covariance function (or kernel) $k(t, t')$. The observations $y$ are assumed to be noisy versions of the true function $f$, such that $y(t) = f(t) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nLet $X = \\{t_1, \\dots, t_N\\}$ be the set of $N$ training time points, and $y$ be the corresponding vector of observed solar generation values. Let $X_*$ be the set of test (query) time points where we wish to make predictions. The joint distribution of the training outputs $y$ and the latent function values $f_*$ at the test points $X_*$ is given by a multivariate Gaussian:\n$$\n\\begin{pmatrix} y \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N}\\left( \\mathbf{0}, \\begin{pmatrix} K(X,X)+\\sigma_n^2 I  K(X,X_*) \\\\ K(X_*,X)  K(X_*,X_*) \\end{pmatrix} \\right)\n$$\nwhere $K(A, B)$ is the matrix of covariances $k(t_a, t_b)$ for all $t_a \\in A, t_b \\in B$, and $I$ is the identity matrix. For convenience, let $K_y = K(X,X)+\\sigma_n^2 I$, $K_* = K(X,X_*)$, and $K_{**} = K(X_*,X_*)$.\n\n**Changepoint Detection via Log Marginal Likelihood (LML)**\nThe model parameter $t_0$ is determined by maximizing the log marginal likelihood of the observations $y$ given the inputs $X$ and the hyperparameters $\\theta = \\{t_0, \\alpha_1, \\alpha_2, p, \\ell_p, \\ell_{\\mathrm{se}}, a, \\sigma_n\\}$. The LML is the log of the probability density of the evidence, $p(y | X, \\theta)$, integrated over all possible functions $f$:\n$$\n\\log p(y | X, \\theta) = -\\frac{1}{2} y^T K_y^{-1} y - \\frac{1}{2} \\log |K_y| - \\frac{N}{2} \\log(2\\pi)\n$$\nDirect computation of the inverse $K_y^{-1}$ and determinant $|K_y|$ is numerically unstable and computationally expensive ($O(N^3)$). Instead, we use the Cholesky decomposition $K_y = L L^T$, where $L$ is a lower-triangular matrix. The terms of the LML are then computed efficiently and stably:\n1. The log-determinant term is $\\log|K_y| = \\log|L L^T| = 2 \\log|L| = 2 \\sum_{i=1}^N \\log(L_{ii})$.\n2. The quadratic term $y^T K_y^{-1} y$ is found by first solving $L \\mathbf{v} = y$ for $\\mathbf{v}$ (forward substitution), and then $L^T \\boldsymbol{\\alpha} = \\mathbf{v}$ for $\\boldsymbol{\\alpha}$ (backward substitution), where $\\boldsymbol{\\alpha} = K_y^{-1} y$. The term is then $y^T \\boldsymbol{\\alpha}$.\n\nThe LML is maximized over a discrete grid of candidate $t_0$ values to find the optimal break location, $\\hat{t}_0$.\n\n**Posterior Prediction**\nUsing the rules for conditioning on a multivariate Gaussian distribution, we can find the distribution of $f_*$ given the training data, $p(f_* | X, y, X_*)$. This posterior distribution is also a Gaussian, $\\mathcal{N}(\\mu_*, \\Sigma_*)$, with mean and covariance given by:\n$$\n\\mu_* = K(X_*, X) [K(X,X)+\\sigma_n^2 I]^{-1} y = K_*^T K_y^{-1} y\n$$\n$$\n\\Sigma_* = K(X_*, X_*) - K(X_*, X) [K(X,X)+\\sigma_n^2 I]^{-1} K(X, X_*) = K_{**} - K_*^T K_y^{-1} K_*\n$$\nThe problem asks for the predictive variances of the latent function, which are the diagonal elements of the posterior covariance matrix $\\Sigma_*$. These are also computed efficiently using the Cholesky factor $L$ of $K_y$:\n- Predictive Mean: $\\mu_* = K_*^T \\boldsymbol{\\alpha}$, using the same $\\boldsymbol{\\alpha} = K_y^{-1} y$ calculated for the LML.\n- Predictive Covariance: Let $V$ be the matrix obtained by solving the system $L V = K_*$ for $V$ using forward substitution. Then $K_*^T K_y^{-1} K_* = (L^{-1}K_*)^T(L^{-1}K_*) = V^T V$. Thus, $\\Sigma_* = K_{**} - V^T V$.\n\n**Algorithmic Implementation**\nThe solution is implemented by first defining functions for the solar generation model $g(t)$ and all kernel components ($w$, $k_{\\mathrm{per}}$, $k_{\\mathrm{se}}$, $k_{\\mathrm{base}}$, $k_{\\mathrm{cp}}$). These functions use vectorized `numpy` operations for efficiency.\n\nFor each test case:\n1. Training data $(X, y)$ are generated according to the specified rules using $T_{\\mathrm{end}}$, $t_{\\mathrm{break}}$, and the given amplitudes $\\alpha_1, \\alpha_2$.\n2. A grid search is performed for the changepoint parameter $t_0$. For each candidate $t_0$ on the grid, the covariance matrix $K_y$ is constructed using the full changepoint kernel $k_{\\mathrm{cp}}$, a jitter of $10^{-8}$ is added to its diagonal for numerical robustness, and the log marginal likelihood is computed via a Cholesky decomposition. The $t_0$ that yields the highest LML is selected as $\\hat{t}_0$.\n3. Using the determined $\\hat{t}_0$, the query points $X_* = \\{\\hat{t}_0 - 0.25, \\hat{t}_0, \\hat{t}_0 + 0.25\\}$ are defined.\n4. The necessary covariance matrices $K_y$, $K_*$, and $K_{**}$ are constructed using $\\hat{t}_0$.\n5. The posterior predictive mean $\\mu_*$ and variance (the diagonal of $\\Sigma_*$) are calculated using the Cholesky-based formulas described above.\n6. The resulting six values (three means and three variances) are collected and appended to a final list for output.\n\nThis procedure is repeated for all four test cases, and the results are aggregated into a single flat list for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the GP changepoint detection and forecasting problem.\n    \"\"\"\n    # Fixed hyperparameters for all test cases as specified in the problem.\n    P = 1.0\n    L_P = 0.3\n    L_SE = 1.5\n    A = 15.0\n    SIGMA_N = 0.05\n    JITTER = 1e-8\n\n    def g(t):\n        \"\"\"Physically realistic half-wave sinusoid for solar generation.\"\"\"\n        return np.maximum(0, np.sin(2 * np.pi * t))\n\n    def w(t, a, t0):\n        \"\"\"Smooth logistic transition weight.\"\"\"\n        return 1.0 / (1.0 + np.exp(-a * (t - t0)))\n\n    def k_per(t1, t2, p, lp):\n        \"\"\"Periodic kernel.\"\"\"\n        dist = np.abs(np.subtract.outer(t1, t2))\n        return np.exp(-2.0 * (np.sin(np.pi * dist / p)**2) / lp**2)\n\n    def k_se(t1, t2, l_se):\n        \"\"\"Squared-exponential kernel.\"\"\"\n        sq_dist = np.subtract.outer(t1, t2)**2\n        return np.exp(-sq_dist / (2.0 * l_se**2))\n\n    def k_base(t1, t2, p, lp, l_se):\n        \"\"\"Base kernel, sum of periodic and squared-exponential.\"\"\"\n        return k_per(t1, t2, p, lp) + k_se(t1, t2, l_se)\n\n    def k_cp(t1, t2, a, t0, alpha1, alpha2, p, lp, l_se):\n        \"\"\"Changepoint covariance function.\"\"\"\n        t1_flat = t1.flatten()\n        t2_flat = t2.flatten()\n        \n        w_t1 = w(t1_flat, a, t0)[:, np.newaxis]\n        w_t2 = w(t2_flat, a, t0)[np.newaxis, :]\n        \n        k_b = k_base(t1_flat, t2_flat, p, lp, l_se)\n        \n        term1 = ((1.0 - w_t1) @ (1.0 - w_t2)) * alpha1**2\n        term2 = (w_t1 @ w_t2) * alpha2**2\n        \n        return (term1 + term2) * k_b\n        \n    def solve_case(T_end, t_break, alpha1, alpha2):\n        \"\"\"\n        Processes a single test case: generates data, finds the changepoint,\n        and computes predictions.\n        \"\"\"\n        # 1. Generate training data\n        X_train = np.arange(0, T_end + 1e-9, 1/6)\n        N = len(X_train)\n        y_train = np.where(X_train  t_break, alpha1 * g(X_train), alpha2 * g(X_train))\n        y_train = y_train[:, np.newaxis] # Ensure y is a column vector\n\n        # 2. Find best changepoint location t0_hat by maximizing log marginal likelihood\n        t0_grid = np.arange(0.5, T_end - 0.5 + 1e-9, 0.25)\n        max_lml = -np.inf\n        t0_hat = -1\n\n        for t0_candidate in t0_grid:\n            K = k_cp(X_train, X_train, A, t0_candidate, alpha1, alpha2, P, L_P, L_SE)\n            Ky = K + np.eye(N) * SIGMA_N**2 + np.eye(N) * JITTER\n            \n            try:\n                L = cholesky(Ky, lower=True)\n                alpha_vec = solve_triangular(L.T, solve_triangular(L, y_train, lower=True))\n                # Constant term -(N/2)log(2pi) is omitted as it doesn't affect maximization\n                lml = -0.5 * y_train.T @ alpha_vec - np.sum(np.log(np.diag(L)))\n                \n                if lml  max_lml:\n                    max_lml = lml\n                    t0_hat = t0_candidate\n            except np.linalg.LinAlgError:\n                # In case of numerical issues, skip this candidate\n                continue\n        \n        # 3. Perform prediction with the detected changepoint t0_hat\n        X_star = np.array([t0_hat - 0.25, t0_hat, t0_hat + 0.25])\n        \n        K = k_cp(X_train, X_train, A, t0_hat, alpha1, alpha2, P, L_P, L_SE)\n        Ky = K + np.eye(N) * SIGMA_N**2 + np.eye(N) * JITTER\n        L = cholesky(Ky, lower=True)\n        alpha_vec = solve_triangular(L.T, solve_triangular(L, y_train, lower=True))\n        \n        K_star = k_cp(X_train, X_star, A, t0_hat, alpha1, alpha2, P, L_P, L_SE)\n        K_star_star = k_cp(X_star, X_star, A, t0_hat, alpha1, alpha2, P, L_P, L_SE)\n\n        # Predictive mean: mu_star = K_star^T * alpha_vec\n        mu_star = K_star.T @ alpha_vec\n\n        # Predictive variance: Sigma_star = K_star_star - K_star^T * Ky^-1 * K_star\n        v = solve_triangular(L, K_star, lower=True)\n        Sigma_star = K_star_star - v.T @ v\n        var_star = np.diag(Sigma_star)\n        \n        mu_star_flat = mu_star.flatten()\n        return [\n            mu_star_flat[0], var_star[0],\n            mu_star_flat[1], var_star[1],\n            mu_star_flat[2], var_star[2]\n        ]\n\n    test_cases = [\n        (30, 12, 5.0, 3.5),\n        (10, 1.5, 4.0, 6.0),\n        (20, 15, 6.0, 4.2),\n        (15, 0.0, 5.0, 5.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = solve_case(*case)\n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.8f}' for x in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}