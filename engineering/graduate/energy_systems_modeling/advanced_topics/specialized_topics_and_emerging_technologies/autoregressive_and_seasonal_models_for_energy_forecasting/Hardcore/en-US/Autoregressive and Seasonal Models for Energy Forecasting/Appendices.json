{
    "hands_on_practices": [
        {
            "introduction": "To build robust forecasting skills, we must begin with the fundamentals. This first practice focuses on deriving a one-step-ahead forecast for a simple Autoregressive (AR) model from first principles. By working through this exercise , you will solidify your understanding of the forecast as a conditional expectation, a concept that forms the bedrock of all time series prediction.",
            "id": "4070510",
            "problem": "An Independent System Operator (ISO) models the deseasonalized hourly net-load anomaly as an Autoregressive (AR) model of order $2$. Specifically, the residual process $\\{y_t\\}$ after removing the deterministic daily seasonal component and long-run mean is assumed to satisfy\n$$\ny_t \\;=\\; \\phi_1 y_{t-1} \\;+\\; \\phi_2 y_{t-2} \\;+\\; \\epsilon_t,\n$$\nwhere $\\{\\epsilon_t\\}$ is a zero-mean, serially uncorrelated innovation process with constant variance $\\sigma_{\\epsilon}^{2}$ and independent of the information set generated by past observations. You are given the coefficients $\\phi_1 = 0.84$, $\\phi_2 = -0.27$, the two most recent residual observations $y_t = 17.9$ and $y_{t-1} = -11.8$ (in megawatts), and the innovation variance $\\sigma_{\\epsilon}^{2} = 16$.\n\nUsing only the definition of an Autoregressive (AR) process and the properties of the innovation process, derive from first principles the one-step-ahead minimum mean-square error forecast $\\hat{y}_{t+1|t}$ of $y_{t+1}$ given information up to time $t$. Write the forecast formula explicitly in terms of $\\phi_1$, $\\phi_2$, $y_t$, and $y_{t-1}$, then evaluate it numerically for the provided values.\n\nReport the final forecast value as a single number in megawatts (MW), rounded to four significant figures.",
            "solution": "The problem statement is first subjected to a validation process.\n\n### Step 1: Extract Givens\n-   **Model Type**: Autoregressive (AR) model of order $2$.\n-   **Process Equation**: $y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t$.\n-   **Process $\\{y_t\\}$**: Deseasonalized hourly net-load anomaly residual process.\n-   **Innovation Process $\\{\\epsilon_t\\}$**: Zero-mean, serially uncorrelated, constant variance $\\sigma_{\\epsilon}^{2}$, and independent of the information set generated by past observations.\n-   **Coefficient $\\phi_1$**: $0.84$.\n-   **Coefficient $\\phi_2$**: $-0.27$.\n-   **Recent Observation $y_t$**: $17.9$ (in megawatts).\n-   **Past Observation $y_{t-1}$**: $-11.8$ (in megawatts).\n-   **Innovation Variance $\\sigma_{\\epsilon}^{2}$**: $16$.\n-   **Objective**: Derive the one-step-ahead minimum mean-square error (MMSE) forecast $\\hat{y}_{t+1|t}$ of $y_{t+1}$ given information up to time $t$. Provide the explicit formula and evaluate it numerically, rounding to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the Autoregressive model, a standard and fundamental concept in time series analysis, which is widely applied in fields like econometrics, signal processing, and energy systems modeling. The setup is scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. It asks for the MMSE forecast, which is uniquely defined as the conditional expectation. All necessary data ($\\phi_1$, $\\phi_2$, $y_t$, $y_{t-1}$) to compute this forecast are provided. The stationarity conditions for an AR($2$) process, $|\\phi_2|  1$, $\\phi_1 + \\phi_2  1$, and $\\phi_2 - \\phi_1  1$, are satisfied by the given coefficients ($|-0.27|  1$; $0.84 - 0.27 = 0.57  1$; $-0.27 - 0.84 = -1.11  1$), ensuring the model describes a stable physical process.\n3.  **Objective**: The problem is stated in precise, quantitative terms, free from ambiguity or subjectivity.\n4.  **Completeness and Consistency**: The problem is self-contained and consistent. It provides all necessary parameters for the point forecast. The inclusion of the innovation variance $\\sigma_{\\epsilon}^{2}=16$ is not required for the point forecast itself but would be essential for calculating forecast error variance or confidence intervals, and its presence does not create a contradiction.\n5.  **Realism**: The chosen coefficients and observed values are of a realistic magnitude for modeling anomalies in an energy system.\n\n### Step 3: Verdict and Action\nThe problem is valid. We may proceed with the solution.\n\n### Derivation and Solution\nThe objective is to find the one-step-ahead minimum mean-square error (MMSE) forecast of the process $y_t$ at time $t+1$, given the information available up to time $t$. Let the information set at time $t$ be denoted by $\\mathcal{I}_t = \\{y_t, y_{t-1}, \\dots\\}$. The MMSE forecast, $\\hat{y}_{t+1|t}$, is defined as the conditional expectation of $y_{t+1}$ given $\\mathcal{I}_t$.\n$$\n\\hat{y}_{t+1|t} = E[y_{t+1} | \\mathcal{I}_t]\n$$\nThe autoregressive model of order $2$, AR($2$), is defined by the equation:\n$$\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t\n$$\nTo find the forecast for time $t+1$, we advance the time index in the model equation by one step:\n$$\ny_{t+1} = \\phi_1 y_t + \\phi_2 y_{t-1} + \\epsilon_{t+1}\n$$\nNow, we take the conditional expectation of this expression with respect to the information set $\\mathcal{I}_t$:\n$$\nE[y_{t+1} | \\mathcal{I}_t] = E[\\phi_1 y_t + \\phi_2 y_{t-1} + \\epsilon_{t+1} | \\mathcal{I}_t]\n$$\nBy the linearity of the expectation operator, we can separate the terms on the right-hand side:\n$$\nE[y_{t+1} | \\mathcal{I}_t] = E[\\phi_1 y_t | \\mathcal{I}_t] + E[\\phi_2 y_{t-1} | \\mathcal{I}_t] + E[\\epsilon_{t+1} | \\mathcal{I}_t]\n$$\nWe evaluate each term individually:\n1.  The coefficients $\\phi_1$ and $\\phi_2$ are known constants. The values of the process $y_t$ and $y_{t-1}$ are known at time $t$, meaning they are contained within the information set $\\mathcal{I}_t$. The expectation of a known quantity, conditional on it being known, is the quantity itself.\n    $$\n    E[\\phi_1 y_t | \\mathcal{I}_t] = \\phi_1 y_t\n    $$\n    $$\n    E[\\phi_2 y_{t-1} | \\mathcal{I}_t] = \\phi_2 y_{t-1}\n    $$\n2.  The term $\\epsilon_{t+1}$ is the innovation at time $t+1$. The problem states that $\\{\\epsilon_t\\}$ is an innovation process independent of the information set generated by past observations. This means that information up to time $t$ provides no information about the realization of $\\epsilon_{t+1}$. Therefore, the conditional expectation of $\\epsilon_{t+1}$ is equal to its unconditional expectation.\n    $$\n    E[\\epsilon_{t+1} | \\mathcal{I}_t] = E[\\epsilon_{t+1}]\n    $$\n    Furthermore, the problem specifies that $\\{\\epsilon_t\\}$ is a zero-mean process. Thus, its unconditional expectation is zero.\n    $$\n    E[\\epsilon_{t+1}] = 0\n    $$\nSubstituting these results back into the forecast equation, we obtain the explicit formula for the one-step-ahead forecast:\n$$\n\\hat{y}_{t+1|t} = \\phi_1 y_t + \\phi_2 y_{t-1} + 0\n$$\n$$\n\\hat{y}_{t+1|t} = \\phi_1 y_t + \\phi_2 y_{t-1}\n$$\nThis is the required general formula for the forecast. We now substitute the given numerical values: $\\phi_1 = 0.84$, $\\phi_2 = -0.27$, $y_t = 17.9$, and $y_{t-1} = -11.8$.\n$$\n\\hat{y}_{t+1|t} = (0.84)(17.9) + (-0.27)(-11.8)\n$$\nPerforming the multiplication for each term:\n$$\n(0.84)(17.9) = 15.036\n$$\n$$\n(-0.27)(-11.8) = 3.186\n$$\nAdding these values together gives the forecast:\n$$\n\\hat{y}_{t+1|t} = 15.036 + 3.186 = 18.222\n$$\nThe problem requires the final answer to be rounded to four significant figures. The calculated value is $18.222$. The four significant figures are $1$, $8$, $2$, and $2$. The fifth digit is $2$, so we do not round up the last significant digit.\nThe final forecast value is $18.22$ MW.",
            "answer": "$$\\boxed{18.22}$$"
        },
        {
            "introduction": "Real-world energy data often exhibit complex seasonal patterns that simple AR models cannot capture. This exercise  challenges you to work with a more powerful Seasonal ARIMA (SARIMA) model, which explicitly accounts for daily cycles. You will learn to compute multi-step-ahead forecasts by expanding the model's lag-polynomial form and applying a recursive approach, a crucial technique when predicting more than one period into the future.",
            "id": "4070541",
            "problem": "Consider hourly electricity demand $\\{y_t\\}$ for a small island grid, modeled by a Seasonal Autoregressive Integrated Moving Average (SARIMA) process with daily seasonality. The fitted model is SARIMA$(1,0,1)\\times(0,1,1)_{24}$, with seasonal period $s=24$, and satisfies the lag-polynomial equation\n$$(1-\\phi_1 B)\\,(1-B^{24})\\,y_t \\;=\\; (1+\\theta_1 B)\\,(1+\\Theta_1 B^{24})\\,\\epsilon_t,$$\nwhere $B$ is the backshift operator, $\\epsilon_t$ is a zero-mean, serially uncorrelated innovation, and $\\phi_1$, $\\theta_1$, $\\Theta_1$ are fixed parameters estimated from data. Let the fitted parameters be $\\phi_1=0.85$, $\\theta_1=-0.25$, and $\\Theta_1=0.40$. You are given the following most recent observations and residuals (all in megawatts, MW): $y_T=1520$, $y_{T-24}=1480$, $y_{T-23}=1490$, $y_{T-22}=1510$, $\\epsilon_T=-5$, $\\epsilon_{T-24}=3$, $\\epsilon_{T-23}=1$, and $\\epsilon_{T-22}=-2$. Assume future innovations satisfy $\\mathbb{E}[\\epsilon_{T+h} \\mid \\mathcal{F}_T]=0$ for all $h\\geq 1$, where $\\mathcal{F}_T$ is the information set up to time $T$.\n\nStarting from core definitions of seasonal differencing and linear time-series models, derive recursive relations for the conditional mean using the given lag polynomials and past residuals, and compute the two-step-ahead forecast $\\widehat{y}_{T+2\\mid T}$. Round your final answer to four significant figures. Express the forecast in megawatts (MW).",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. All necessary parameters and data are provided to compute the required forecast. The SARIMA model is a standard framework in time-series analysis for forecasting seasonal data.\n\nThe problem states that the hourly electricity demand $\\{y_t\\}$ is described by a SARIMA$(1,0,1)\\times(0,1,1)_{24}$ model with a seasonal period $s=24$. The governing lag-polynomial equation is:\n$$ (1-\\phi_1 B)\\,(1-B^{24})\\,y_t \\;=\\; (1+\\theta_1 B)\\,(1+\\Theta_1 B^{24})\\,\\epsilon_t $$\nwhere $B$ is the backshift operator such that $B^k y_t = y_{t-k}$, and $\\epsilon_t$ is a zero-mean, serially uncorrelated innovation process.\n\nWe begin by expanding the lag polynomials to express $y_t$ in terms of its past values and the innovations.\nThe left-hand side (LHS) is:\n$$ (1-\\phi_1 B - B^{24} + \\phi_1 B^{25})y_t = y_t - \\phi_1 y_{t-1} - y_{t-24} + \\phi_1 y_{t-25} $$\nThe right-hand side (RHS) is:\n$$ (1+\\theta_1 B + \\Theta_1 B^{24} + \\theta_1 \\Theta_1 B^{25})\\epsilon_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\Theta_1 \\epsilon_{t-24} + \\theta_1 \\Theta_1 \\epsilon_{t-25} $$\nEquating the LHS and RHS and solving for $y_t$ gives the explicit expression for the process:\n$$ y_t = \\phi_1 y_{t-1} + y_{t-24} - \\phi_1 y_{t-25} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\Theta_1 \\epsilon_{t-24} + \\theta_1 \\Theta_1 \\epsilon_{t-25} $$\nThe $h$-step-ahead forecast at time $T$, denoted $\\widehat{y}_{T+h\\mid T}$, is the conditional expectation of $y_{T+h}$ given the information set $\\mathcal{F}_T$ up to time $T$:\n$$ \\widehat{y}_{T+h\\mid T} = \\mathbb{E}[y_{T+h} \\mid \\mathcal{F}_T] $$\nWe are given that future innovations have a conditional expectation of zero: $\\mathbb{E}[\\epsilon_{T+k} \\mid \\mathcal{F}_T] = 0$ for $k  0$. For $k \\le 0$, $\\epsilon_{T+k}$ is part of the information set $\\mathcal{F}_T$, so $\\mathbb{E}[\\epsilon_{T+k} \\mid \\mathcal{F}_T] = \\epsilon_{T+k}$. Similarly, $\\mathbb{E}[y_{T+k} \\mid \\mathcal{F}_T] = y_{T+k}$ for $k \\le 0$ and $\\mathbb{E}[y_{T+k} \\mid \\mathcal{F}_T] = \\widehat{y}_{T+k\\mid T}$ for $k  0$.\n\nOur goal is to compute $\\widehat{y}_{T+2\\mid T}$. We must first compute the one-step-ahead forecast, $\\widehat{y}_{T+1\\mid T}$.\nFor $t=T+1$, the equation for $y_{T+1}$ is:\n$$ y_{T+1} = \\phi_1 y_{T} + y_{T-23} - \\phi_1 y_{T-24} + \\epsilon_{T+1} + \\theta_1 \\epsilon_{T} + \\Theta_1 \\epsilon_{T-23} + \\theta_1 \\Theta_1 \\epsilon_{T-24} $$\nTaking the conditional expectation given $\\mathcal{F}_T$:\n$$ \\widehat{y}_{T+1\\mid T} = \\mathbb{E}[y_{T+1} \\mid \\mathcal{F}_T] = \\phi_1 y_{T} + y_{T-23} - \\phi_1 y_{T-24} + \\mathbb{E}[\\epsilon_{T+1} \\mid \\mathcal{F}_T] + \\theta_1 \\epsilon_{T} + \\Theta_1 \\epsilon_{T-23} + \\theta_1 \\Theta_1 \\epsilon_{T-24} $$\nUsing $\\mathbb{E}[\\epsilon_{T+1} \\mid \\mathcal{F}_T]=0$, we get:\n$$ \\widehat{y}_{T+1\\mid T} = \\phi_1 y_{T} + y_{T-23} - \\phi_1 y_{T-24} + \\theta_1 \\epsilon_{T} + \\Theta_1 \\epsilon_{T-23} + \\theta_1 \\Theta_1 \\epsilon_{T-24} $$\nThe given parameter values are $\\phi_1=0.85$, $\\theta_1=-0.25$, and $\\Theta_1=0.40$. The product of MA parameters is $\\theta_1 \\Theta_1 = (-0.25)(0.40) = -0.10$.\nThe given data are: $y_T=1520$, $y_{T-23}=1490$, $y_{T-24}=1480$, $\\epsilon_T=-5$, $\\epsilon_{T-23}=1$, and $\\epsilon_{T-24}=3$.\nSubstituting these values:\n$$ \\widehat{y}_{T+1\\mid T} = (0.85)(1520) + 1490 - (0.85)(1480) + (-0.25)(-5) + (0.40)(1) + (-0.10)(3) $$\n$$ \\widehat{y}_{T+1\\mid T} = 1292 + 1490 - 1258 + 1.25 + 0.40 - 0.30 $$\n$$ \\widehat{y}_{T+1\\mid T} = 1524 + 1.35 = 1525.35 $$\nNow we proceed to compute the two-step-ahead forecast, $\\widehat{y}_{T+2\\mid T}$. For $t=T+2$, the equation for $y_{T+2}$ is:\n$$ y_{T+2} = \\phi_1 y_{T+1} + y_{T-22} - \\phi_1 y_{T-23} + \\epsilon_{T+2} + \\theta_1 \\epsilon_{T+1} + \\Theta_1 \\epsilon_{T-22} + \\theta_1 \\Theta_1 \\epsilon_{T-23} $$\nTaking the conditional expectation given $\\mathcal{F}_T$:\n$$ \\widehat{y}_{T+2\\mid T} = \\mathbb{E}[y_{T+2} \\mid \\mathcal{F}_T] = \\phi_1 \\mathbb{E}[y_{T+1} \\mid \\mathcal{F}_T] + y_{T-22} - \\phi_1 y_{T-23} + \\mathbb{E}[\\epsilon_{T+2} \\mid \\mathcal F_T] + \\theta_1 \\mathbb{E}[\\epsilon_{T+1} \\mid \\mathcal F_T] + \\Theta_1 \\epsilon_{T-22} + \\theta_1 \\Theta_1 \\epsilon_{T-23} $$\nUsing $\\mathbb{E}[\\epsilon_{T+2} \\mid \\mathcal F_T] = 0$, $\\mathbb{E}[\\epsilon_{T+1} \\mid \\mathcal F_T] = 0$, and $\\mathbb{E}[y_{T+1} \\mid \\mathcal{F}_T] = \\widehat{y}_{T+1\\mid T}$, we obtain the recursive forecasting equation:\n$$ \\widehat{y}_{T+2\\mid T} = \\phi_1 \\widehat{y}_{T+1\\mid T} + y_{T-22} - \\phi_1 y_{T-23} + \\Theta_1 \\epsilon_{T-22} + \\theta_1 \\Theta_1 \\epsilon_{T-23} $$\nWe use the previously calculated value $\\widehat{y}_{T+1\\mid T}=1525.35$ and the additional given data: $y_{T-22}=1510$, $y_{T-23}=1490$, $\\epsilon_{T-22}=-2$, and $\\epsilon_{T-23}=1$. Substituting these values into the equation for $\\widehat{y}_{T+2\\mid T}$:\n$$ \\widehat{y}_{T+2\\mid T} = (0.85)(1525.35) + 1510 - (0.85)(1490) + (0.40)(-2) + (-0.10)(1) $$\n$$ \\widehat{y}_{T+2\\mid T} = 1296.5475 + 1510 - 1266.5 - 0.80 - 0.10 $$\n$$ \\widehat{y}_{T+2\\mid T} = 2806.5475 - 1266.5 - 0.90 $$\n$$ \\widehat{y}_{T+2\\mid T} = 1540.0475 - 0.90 = 1539.1475 $$\nThe problem requires the final answer to be rounded to four significant figures. The value $1539.1475$ rounded to four significant figures is $1539$.\nThe forecast for the electricity demand two hours ahead of time $T$ is $1539$ MW.",
            "answer": "$$ \\boxed{1539} $$"
        },
        {
            "introduction": "Modern forecasting systems often rely on state-space representations for their flexibility and computational efficiency. This advanced practice  guides you through implementing a Kalman filter, the optimal algorithm for estimating the latent state in a linear Gaussian system. By translating a SARIMA model into state-space form and writing the code to perform the filtering and prediction steps, you will gain hands-on experience with the engine that powers many real-time energy forecasting applications.",
            "id": "4070543",
            "problem": "You are given a linear Gaussian state-space model that represents a Seasonal Autoregressive Integrated Moving Average (SARIMA) process for energy demand forecasting after appropriate differencing. The latent state is the differenced load, and the seasonal effect is captured by a lag-$s$ autoregression in the state transition. Denote the differenced latent process by $w_t$. The model is defined as follows. The state vector $x_t \\in \\mathbb{R}^s$ stacks recent values of $w_t$ as\n$$\nx_t = \\begin{bmatrix} w_t \\\\ w_{t-1} \\\\ \\vdots \\\\ w_{t-s+1} \\end{bmatrix},\n$$\nwith the state transition given by\n$$\nx_{t+1} = F x_t + G \\xi_{t+1}, \\quad \\xi_t \\sim \\mathcal{N}(0,q),\n$$\nand the observation equation\n$$\ny_t = H x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0,r).\n$$\nHere, $F \\in \\mathbb{R}^{s \\times s}$ is the companion-like matrix\n$$\nF = \\begin{bmatrix}\n\\phi  0  \\cdots  0  \\Phi \\\\\n1  0  \\cdots  0  0 \\\\\n0  1  \\ddots  0  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  \\vdots \\\\\n0  \\cdots  0  1  0\n\\end{bmatrix},\n$$\n$G \\in \\mathbb{R}^{s \\times 1}$ equals $G = [1, 0, \\ldots, 0]^\\top$, $H \\in \\mathbb{R}^{1 \\times s}$ equals $H = [1, 0, \\ldots, 0]$, $\\phi \\in \\mathbb{R}$ is the nonseasonal autoregressive coefficient, and $\\Phi \\in \\mathbb{R}$ is the seasonal autoregressive coefficient at lag $s$. The process noise variance is $q > 0$ and the observation noise variance is $r > 0$. This structure is a standard state-space embedding of Seasonal Autoregressive Integrated Moving Average (SARIMA) for differenced load, where the latent dynamic is a seasonal autoregression and measurements are contaminated by Gaussian observation noise.\n\nInitialize the filter with $a_{0|0} = \\mathbb{E}[x_0] = 0 \\in \\mathbb{R}^s$ and $P_{0|0} = \\sigma_0^2 I_s$, where $I_s$ is the $s \\times s$ identity matrix and $\\sigma_0^2 > 0$ is a given diffuse variance. For each time $t$, define the one-step prediction $a_{t|t-1} = \\mathbb{E}[x_t \\mid y_{1:t-1}]$, its covariance $P_{t|t-1} = \\operatorname{Cov}(x_t \\mid y_{1:t-1})$, the one-step prediction of the observation $y_{t|t-1} = H a_{t|t-1}$, the one-step prediction error $e_t = y_t - y_{t|t-1}$, and the prediction error variance $S_t = H P_{t|t-1} H^\\top + r$. Use the optimal linear filter for linear Gaussian systems to compute the filtered state $a_{t|t} = \\mathbb{E}[x_t \\mid y_{1:t}]$ and its covariance $P_{t|t}$, and the incremental Gaussian log-likelihood\n$$\n\\ell_t = -\\frac{1}{2}\\left( \\log(2\\pi) + \\log S_t + \\frac{e_t^2}{S_t} \\right).\n$$\n\nTask: Implement a program that, for specified values of $s$, $\\phi$, $\\Phi$, $q$, $r$, $\\sigma_0^2$, and a given observation sequence $\\{y_t\\}_{t=1}^T$, performs the following:\n- Computes the filtered state estimates $\\{a_{t|t}\\}_{t=1}^T$, the one-step prediction errors $\\{e_t\\}_{t=1}^T$, and the incremental log-likelihoods $\\{\\ell_t\\}_{t=1}^T$.\n- Outputs, for each test case, the total log-likelihood $\\sum_{t=1}^T \\ell_t$, the last one-step prediction error $e_T$, and the first component of the final filtered state estimate $a_{T|T}^{(1)}$.\n\nUse the following test suite (all sequences are dimensionless; there are no physical units involved):\n- Test Case $1$ (daily seasonality, hourly differenced series):\n    - $s = 24$, $\\phi = 0.5$, $\\Phi = 0.3$, $q = 0.2$, $r = 0.1$, $\\sigma_0^2 = 10$,\n    - $T = 36$,\n    - $y_{1:36} = [0.2, 0.5, 0.1, -0.1, 0.3, 0.6, 0.4, 0.9, 1.2, 0.8, 0.2, -0.2, 0.0, 0.3, 0.7, 0.6, 0.2, 0.4, 0.1, 0.0, 0.5, 0.8, 1.0, 0.9, 0.3, 0.2, 0.1, 0.0, 0.4, 0.5, 0.7, 1.1, 1.0, 0.6, 0.3, 0.2]$.\n- Test Case $2$ (short seasonality, near unit root nonseasonal autoregression):\n    - $s = 4$, $\\phi = 0.95$, $\\Phi = 0.0$, $q = 10^{-6}$, $r = 0.05$, $\\sigma_0^2 = 10$,\n    - $T = 12$,\n    - $y_{1:12} = [0.0, 0.1, 0.2, 0.4, 0.6, 0.9, 1.3, 1.6, 1.8, 2.0, 2.2, 2.3]$.\n- Test Case $3$ (monthly seasonality, strong seasonal autoregression with larger measurement noise):\n    - $s = 12$, $\\phi = 0.2$, $\\Phi = 0.8$, $q = 0.01$, $r = 0.5$, $\\sigma_0^2 = 10$,\n    - $T = 24$,\n    - $y_{1:24} = [1.0, 0.8, 0.7, 0.6, 0.5, 0.6, 0.7, 0.9, 1.2, 1.5, 1.8, 2.0, 1.1, 0.9, 0.8, 0.7, 0.6, 0.7, 0.8, 1.0, 1.3, 1.6, 1.9, 2.1]$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of three lists, each corresponding to one test case, in the form\n$$\n\\text{[ [LL\\_1, e\\_T^{(1)}, a\\_{T|T}^{(1,1)}], [LL\\_2, e\\_T^{(2)}, a\\_{T|T}^{(2,1)}], [LL\\_3, e\\_T^{(3)}, a\\_{T|T}^{(3,1)}] ]},\n$$\nwhere $LL_i$ is the total log-likelihood for test case $i$, $e_T^{(i)}$ is the last one-step prediction error, and $a_{T|T}^{(i,1)}$ is the first component of the final filtered state vector for test case $i$. Print all values as decimal floats on a single line, with six digits after the decimal point, enclosed in square brackets exactly as shown, and no additional text.",
            "solution": "The user-provided problem statement has been validated and found to be valid. It is scientifically grounded, well-posed, objective, and self-contained, presenting a standard application of the Kalman filter to a linear Gaussian state-space model representing a SARIMA process. All necessary parameters, equations, and data are provided.\n\nThe task is to implement the Kalman filter to estimate the latent state of a time series model. The model is a linear Gaussian state-space model, for which the Kalman filter provides the optimal (in the mean square error sense) estimate of the state. The model is defined by a state transition equation and an observation equation.\n\nThe state vector at time $t$ is $x_t \\in \\mathbb{R}^s$, which is a stack of the $s$ most recent values of a differenced process $w_t$:\n$$\nx_t = \\begin{bmatrix} w_t  w_{t-1}  \\cdots  w_{t-s+1} \\end{bmatrix}^\\top\n$$\n\nThe state transition equation describes the evolution of the state over time:\n$$\nx_{t+1} = F x_t + G \\xi_{t+1}, \\quad \\text{where } \\xi_{t+1} \\sim \\mathcal{N}(0, q)\n$$\nThis equation implies that the process noise covariance matrix is $Q = \\mathbb{E}[(G\\xi_{t+1})(G\\xi_{t+1})^\\top] = G \\mathbb{E}[\\xi_{t+1}^2] G^\\top = q G G^\\top$. Since $G = [1, 0, \\ldots, 0]^\\top$, $Q$ is an $s \\times s$ matrix with $q$ as its top-left element and zeros elsewhere.\n\nThe observation equation relates the latent state to the measurement $y_t$:\n$$\ny_t = H x_t + v_t, \\quad \\text{where } v_t \\sim \\mathcal{N}(0, r)\n$$\nHere, $H = [1, 0, \\ldots, 0]$, which means $y_t$ is a noisy measurement of the most recent value $w_t$.\n\nThe Kalman filter recursively updates the estimate of the state mean $a_{t|t} = \\mathbb{E}[x_t | y_{1:t}]$ and covariance $P_{t|t} = \\operatorname{Cov}(x_t | y_{1:t})$ through a two-step process: prediction and update.\n\n**Initialization (at $t=0$):**\nThe filter is initialized with the prior mean and covariance of the state at $t=0$:\n- Initial state mean: $a_{0|0} = \\mathbb{E}[x_0] = \\mathbf{0} \\in \\mathbb{R}^s$\n- Initial state covariance: $P_{0|0} = \\operatorname{Cov}(x_0) = \\sigma_0^2 I_s$, where $I_s$ is the $s \\times s$ identity matrix.\n\n**Recursive Loop (for $t=1, \\ldots, T$):**\n\n**1. Prediction Step (Time Update):**\nGiven the filtered state estimate $a_{t-1|t-1}$ and its covariance $P_{t-1|t-1}$ from the previous step, we predict the state and its covariance at the current time $t$ before observing $y_t$. The prediction propagates the state through the system dynamics.\n- Predicted state mean: $a_{t|t-1} = \\mathbb{E}[x_t | y_{1:t-1}] = F a_{t-1|t-1}$\n- Predicted state covariance: $P_{t|t-1} = \\operatorname{Cov}(x_t | y_{1:t-1}) = F P_{t-1|t-1} F^\\top + Q$\n\n**2. Update Step (Measurement Update):**\nOnce the measurement $y_t$ becomes available, the predicted state is corrected.\n- One-step prediction error (innovation): $e_t = y_t - y_{t|t-1} = y_t - H a_{t|t-1}$\n- Innovation covariance: $S_t = \\operatorname{Var}(e_t) = H P_{t|t-1} H^\\top + r$\n- Kalman gain: $K_t = P_{t|t-1} H^\\top S_t^{-1}$. Since $y_t$ is a scalar, $S_t$ is also a scalar, and its inverse is $1/S_t$.\n- Updated (filtered) state mean: $a_{t|t} = a_{t|t-1} + K_t e_t$\n- Updated (filtered) state covariance: $P_{t|t} = (I_s - K_t H) P_{t|t-1}$\n\n**3. Log-Likelihood Calculation:**\nFor a Gaussian system, the incremental log-likelihood of observation $y_t$ is given by the log of the Gaussian probability density function of the innovation $e_t$, which has mean $0$ and variance $S_t$:\n$$\n\\ell_t = \\log p(y_t | y_{1:t-1}) = -\\frac{1}{2}\\left( \\log(2\\pi) + \\log S_t + \\frac{e_t^2}{S_t} \\right)\n$$\nThe total log-likelihood for the entire sequence $y_{1:T}$ is the sum of these incremental likelihoods: $\\mathcal{L} = \\sum_{t=1}^T \\ell_t$.\n\nThis algorithm will be implemented and applied to each of the three test cases provided. For each case, we will compute the total log-likelihood $\\mathcal{L}$, the final one-step prediction error $e_T$, and the first element of the final filtered state, $a_{T|T}^{(1)}$. The matrices $F$, $G$, and $H$ are constructed based on the parameters $s$, $\\phi$, and $\\Phi$ for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Kalman filtering problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"s\": 24, \"phi\": 0.5, \"Phi\": 0.3, \"q\": 0.2, \"r\": 0.1, \"sigma0_sq\": 10,\n            \"T\": 36,\n            \"y\": np.array([0.2, 0.5, 0.1, -0.1, 0.3, 0.6, 0.4, 0.9, 1.2, 0.8, 0.2, -0.2, 0.0, 0.3, 0.7, 0.6, 0.2, 0.4, 0.1, 0.0, 0.5, 0.8, 1.0, 0.9, 0.3, 0.2, 0.1, 0.0, 0.4, 0.5, 0.7, 1.1, 1.0, 0.6, 0.3, 0.2])\n        },\n        {\n            \"s\": 4, \"phi\": 0.95, \"Phi\": 0.0, \"q\": 1e-6, \"r\": 0.05, \"sigma0_sq\": 10,\n            \"T\": 12,\n            \"y\": np.array([0.0, 0.1, 0.2, 0.4, 0.6, 0.9, 1.3, 1.6, 1.8, 2.0, 2.2, 2.3])\n        },\n        {\n            \"s\": 12, \"phi\": 0.2, \"Phi\": 0.8, \"q\": 0.01, \"r\": 0.5, \"sigma0_sq\": 10,\n            \"T\": 24,\n            \"y\": np.array([1.0, 0.8, 0.7, 0.6, 0.5, 0.6, 0.7, 0.9, 1.2, 1.5, 1.8, 2.0, 1.1, 0.9, 0.8, 0.7, 0.6, 0.7, 0.8, 1.0, 1.3, 1.6, 1.9, 2.1])\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        s = case[\"s\"]\n        phi = case[\"phi\"]\n        Phi = case[\"Phi\"]\n        q = case[\"q\"]\n        r = case[\"r\"]\n        sigma0_sq = case[\"sigma0_sq\"]\n        T = case[\"T\"]\n        y_obs = case[\"y\"]\n\n        # 1. Construct state-space matrices\n        F = np.zeros((s, s))\n        F[0, 0] = phi\n        if s > 1:\n            F[0, s-1] = Phi\n        for i in range(1, s):\n            F[i, i-1] = 1.0\n\n        G = np.zeros((s, 1))\n        G[0, 0] = 1.0\n\n        H = np.zeros((1, s))\n        H[0, 0] = 1.0\n\n        # Process noise covariance matrix Q\n        Q = G @ G.T * q\n        \n        # Identity matrix\n        I_s = np.eye(s)\n\n        # 2. Initialize filter\n        # a_{t|t} is stored in a_filt, initialized to a_{0|0}\n        a_filt = np.zeros((s, 1))\n        # P_{t|t} is stored in P_filt, initialized to P_{0|0}\n        P_filt = sigma0_sq * I_s\n\n        total_log_likelihood = 0.0\n        e_final = 0.0\n        \n        # 3. Kalman filter recursion\n        for t in range(T):\n            # Previous filtered state and covariance\n            a_prev_filt = a_filt\n            P_prev_filt = P_filt\n            \n            # --- Prediction Step ---\n            a_pred = F @ a_prev_filt\n            P_pred = F @ P_prev_filt @ F.T + Q\n            \n            # --- Update Step ---\n            y_t = y_obs[t]\n            \n            # Innovation (prediction error)\n            # H @ a_pred gives a 1x1 matrix\n            y_pred = (H @ a_pred)[0, 0]\n            e_t = y_t - y_pred\n            \n            # Innovation covariance\n            # H @ P_pred @ H.T gives a 1x1 matrix\n            S_t = (H @ P_pred @ H.T)[0, 0] + r\n            \n            # Kalman gain\n            # P_pred @ H.T gives an s x 1 vector\n            K_t = (P_pred @ H.T) / S_t\n\n            # Updated (filtered) state mean and covariance\n            a_filt = a_pred + K_t * e_t\n            P_filt = (I_s - K_t @ H) @ P_pred\n            \n            # Incremental log-likelihood\n            ll_t = -0.5 * (np.log(2 * np.pi) + np.log(S_t) + e_t**2 / S_t)\n            total_log_likelihood += ll_t\n            \n            if t == T - 1:\n                e_final = e_t\n\n        a_T_T_1 = a_filt[0, 0]\n        \n        all_results.append([total_log_likelihood, e_final, a_T_T_1])\n\n    # 4. Format and print the final output\n    case_results_str = []\n    for res in all_results:\n        case_str = f\"[{res[0]:.6f}, {res[1]:.6f}, {res[2]:.6f}]\"\n        case_results_str.append(case_str)\n    \n    final_output = \"[\" + \", \".join(case_results_str) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}