{
    "hands_on_practices": [
        {
            "introduction": "In high-resolution geospatial modeling, data precision is paramount. Seemingly minor discrepancies, such as a mismatch between geodetic datums like WGS84 and NAD83, can introduce positional errors that have significant consequences when aggregating point data into coarse grid cells. This exercise provides a hands-on method to quantify the impact of such shifts on aggregated resource statistics, a critical skill for ensuring data integrity and understanding model sensitivity. ",
            "id": "4093404",
            "problem": "You are tasked with formalizing and quantifying the effect of geodetic datum shifts on aggregated energy resource mapping. Distinguish the World Geodetic System 1984 (WGS84) from the North American Datum 1983 (NAD83) in terms of their geodetic definitions and realizations, and derive how a horizontal translation of magnitude $1\\,\\mathrm{m}$ in a projected planar coordinate system can alter capacity factor aggregation when points are grouped into polygons of area $100\\,\\mathrm{km}^2$. For algorithmic tractability and scientific realism over a local region, assume a locally planar map projection consistent with the Universal Transverse Mercator (UTM) family where small horizontal shifts are preserved as planar translations in meters.\n\nFundamental bases you must use include the definition of a geodetic datum as a specification of an Earth-centered Earth-fixed coordinate reference frame and associated ellipsoid, the notion that datum differences can be represented by a seven-parameter Helmert transform, and the definition of capacity factor as a dimensionless ratio of actual energy production to maximum possible production over a period.\n\nDefine a partition of the plane into axis-aligned square polygons with area $100\\,\\mathrm{km}^2$, each having side length $L = 10{,}000\\,\\mathrm{m}$, and adopt the following membership rule to avoid ambiguity on boundaries: a point with coordinates $(x,y)$ belongs to the polygon $[x_0,x_1) \\times [y_0,y_1)$ if and only if $x_0 \\le x < x_1$ and $y_0 \\le y < y_1$. Capacity factor aggregation for a polygon is the arithmetic mean of the capacity factors of all points belonging to that polygon under the membership rule.\n\nYou must implement a program that, for each test case provided below, computes the maximum absolute change in polygon-level aggregated capacity factor caused by translating all points by the specified horizontal shift vector. Specifically, for each test case, let $\\mu_p$ denote the pre-shift mean capacity factor for polygon $p$, and let $\\mu'_p$ denote the post-shift mean after translating every point by the given vector. Compute the impact metric $I = \\max_{p} \\left| \\mu'_p - \\mu_p \\right|$ for that test case. Express all coordinates in meters, and treat all capacity factors as dimensionless decimals. The final outputs are the values of $I$ per test case, expressed as decimals rounded to six places.\n\nUse the following test suite. Each test case consists of a set of polygons, a set of resource points with associated capacity factors, and a horizontal shift vector $\\Delta \\mathbf{x} = (\\Delta x, \\Delta y)$ in meters. Polygons are axis-aligned squares of side length $10{,}000\\,\\mathrm{m}$, defined by half-open intervals $[x_0,x_1) \\times [y_0,y_1)$ as specified.\n\nTest Case $1$ (happy path, no boundary crossings):\n- Polygons: a single polygon $P_0$ with $[0,10{,}000) \\times [0,10{,}000)$.\n- Points and capacity factors: $(x,y,c)$ triplets\n  - $(5{,}000,5{,}000,0.4)$\n  - $(2{,}000,2{,}000,0.5)$\n- Shift vector: $\\Delta \\mathbf{x} = (1,0)\\,\\mathrm{m}$.\n\nTest Case $2$ (east boundary crossing between adjacent polygons):\n- Polygons: $P_0$ with $[0,10{,}000) \\times [0,10{,}000)$ and $P_1$ with $[10{,}000,20{,}000) \\times [0,10{,}000)$.\n- Points and capacity factors:\n  - $(9{,}999.6,3{,}000,0.6)$\n  - $(7{,}000,7{,}000,0.2)$\n  - $(15{,}000,5{,}000,0.8)$\n- Shift vector: $\\Delta \\mathbf{x} = (1,0)\\,\\mathrm{m}$.\n\nTest Case $3$ (north boundary crossing between adjacent polygons):\n- Polygons: $P_0$ with $[0,10{,}000) \\times [0,10{,}000)$ and $P_2$ with $[0,10{,}000) \\times [10{,}000,20{,}000)$.\n- Points and capacity factors:\n  - $(5{,}000,9{,}999.5,0.9)$\n  - $(4{,}000,2{,}000,0.1)$\n  - $(5{,}000,15{,}000,0.3)$\n- Shift vector: $\\Delta \\mathbf{x} = (0,1)\\,\\mathrm{m}$.\n\nTest Case $4$ (edge case with point exactly on a boundary and westward shift):\n- Polygons: $P_0$ with $[0,10{,}000) \\times [0,10{,}000)$ and $P_1$ with $[10{,}000,20{,}000) \\times [0,10{,}000)$.\n- Points and capacity factors:\n  - $(10{,}000.0,5{,}000,0.7)$\n  - $(8{,}000,6{,}000,0.5)$\n  - $(15{,}000,4{,}000,0.6)$\n- Shift vector: $\\Delta \\mathbf{x} = (-1,0)\\,\\mathrm{m}$.\n\nYour program should produce a single line of output containing the four impact metric values as a comma-separated list enclosed in square brackets, rounded to six decimal places, for example, $[0.000000,0.123456,0.500000,0.010000]$. No other text should be printed. The computation must be deterministic and use only the provided test suite data.",
            "solution": "The problem presented is a valid and well-posed formalization of a practical issue in geospatial data analysis, specifically within energy resource modeling. It concerns the sensitivity of aggregated spatial data to small perturbations in underlying coordinate systems, such as those arising from differences between geodetic datums like the World Geodetic System 1984 (WGS84) and the North American Datum 1983 (NAD83). While these datums are defined by complex geodetic parameters (reference ellipsoid, origin, orientation), for local-scale analysis, the difference between their realizations can often be well-approximated by a simple horizontal translation vector, as posited in the problem. The magnitude of this shift is on the order of $1\\,\\mathrm{m}$, which is physically realistic. The problem asks us to quantify the maximum impact of such a shift on the mean capacity factor aggregated over a regular grid of square polygons.\n\nThe core scientific principle being investigated is the effect of boundary conditions in discrete spatial aggregation. When continuous point data are aggregated into discrete zones (polygons), points lying near a boundary are susceptible to being reassigned to an adjacent zone if their coordinates are slightly perturbed. This reassignment alters the composition of the point sets within the affected polygons, thereby changing their aggregated statistics. The problem provides a rigorous framework to analyze this effect.\n\nOur algorithm will systematically compute the change in aggregated values by following a precise sequence of steps based on the provided definitions.\n\n**Algorithmic Formulation**\n\n1.  **System Definition**: The system consists of a set of resource data points $S = \\{(x_i, y_i, c_i)\\}_{i=1}^N$, where $(x_i, y_i)$ are planar coordinates in meters and $c_i$ is the dimensionless capacity factor. The plane is partitioned into a grid of axis-aligned square polygons, each of side length $L = 10{,}000\\,\\mathrm{m}$ and area $100\\,\\mathrm{km}^2$.\n\n2.  **Polygon Membership Rule**: A point with coordinates $(x,y)$ is assigned to the unique polygon $P_{j,k}$ defined by the half-open interval $[jL, (j+1)L) \\times [kL, (k+1)L)$, where the integer indices $(j,k)$ are determined by the floor function:\n    $$\n    j = \\lfloor x/L \\rfloor, \\quad k = \\lfloor y/L \\rfloor\n    $$\n    This rule is unambiguous and ensures every point in the plane belongs to exactly one polygon.\n\n3.  **Aggregation Function**: The aggregated capacity factor for a polygon $p$ is the arithmetic mean of the capacity factors of all points contained within it. Let $S_p = \\{(x_i, y_i, c_i) \\in S \\mid (x_i, y_i) \\in p\\}$. The pre-shift mean capacity factor, $\\mu_p$, is:\n    $$\n    \\mu_p = \\begin{cases} \\frac{1}{|S_p|} \\sum_{(x,y,c) \\in S_p} c & \\text{if } |S_p| > 0 \\\\ 0 & \\text{if } |S_p| = 0 \\end{cases}\n    $$\n\n4.  **Coordinate Transformation**: A datum shift is modeled as a uniform horizontal translation by a vector $\\Delta \\mathbf{x} = (\\Delta x, \\Delta y)$. Each point $(x_i, y_i)$ is transformed to a new coordinate $(x'_i, y'_i)$:\n    $$\n    (x'_i, y'_i) = (x_i + \\Delta x, y_i + \\Delta y)\n    $$\n    This creates a new set of shifted points $S' = \\{(x'_i, y'_i, c_i)\\}_{i=1}^N$. Note that the capacity factor $c_i$ is an intrinsic property of the resource point and remains invariant under coordinate transformation.\n\n5.  **Post-Shift Aggregation**: The aggregation procedure is repeated for the shifted points. Let $S'_p = \\{(x'_i, y'_i, c_i) \\in S' \\mid (x'_i, y'_i) \\in p\\}$. The post-shift mean capacity factor, $\\mu'_p$, is:\n    $$\n    \\mu'_p = \\begin{cases} \\frac{1}{|S'_p|} \\sum_{(x',y',c) \\in S'_p} c & \\text{if } |S'_p| > 0 \\\\ 0 & \\text{if } |S'_p| = 0 \\end{cases}\n    $$\n\n6.  **Impact Metric**: The impact of the shift is quantified by the maximum absolute change in the mean capacity factor across all affected polygons. Let $\\mathcal{P}_{affected}$ be the set of all polygons $p$ for which either $S_p$ or $S'_p$ is non-empty. The impact metric $I$ is:\n    $$\n    I = \\max_{p \\in \\mathcal{P}_{affected}} |\\mu'_p - \\mu_p|\n    $$\n\n**Implementation Strategy**\n\nThe algorithm is implemented by processing each test case as follows:\n- Define the side length $L=10000.0$.\n- For each test case, we first compute the pre-shift aggregations. We use a dictionary to map polygon indices $(j,k)$ to a list of capacity factors.\n- We iterate through the given points, determine the polygon index for each point using the floor-division rule, and populate the dictionary.\n- From this dictionary, we compute a second dictionary mapping polygon indices to their pre-shift mean capacity factor, $\\mu_p$.\n- Next, we apply the shift vector $(\\Delta x, \\Delta y)$ to all point coordinates.\n- We repeat the aggregation process with the shifted coordinates to compute the post-shift mean capacity factors, $\\mu'_p$.\n- We then identify the set of all unique polygon indices that appeared in either the pre-shift or post-shift calculations.\n- For each of these polygons, we calculate the absolute difference $|\\mu'_p - \\mu_p|$, retrieving values from the mean dictionaries (using a default of $0$ if a polygon is empty in one of the states).\n- The maximum of these differences is the impact metric $I$ for the test case.\n\nLet us trace Test Case 2 as an example:\n- **Polygons**: $P_{0,0} = [0, 10000) \\times [0, 10000)$ and $P_{1,0} = [10000, 20000) \\times [0, 10000)$.\n- **Points**: $(9999.6, 3000, 0.6)$, $(7000, 7000, 0.2)$, $(15000, 5000, 0.8)$.\n- **Shift**: $\\Delta \\mathbf{x} = (1, 0)$.\n\n**Pre-shift analysis**:\n- Point $(9999.6, 3000)$: $\\lfloor 9999.6/10000 \\rfloor = 0$, $\\lfloor 3000/10000 \\rfloor = 0$. Belongs to $P_{0,0}$.\n- Point $(7000, 7000)$: $\\lfloor 7000/10000 \\rfloor = 0$, $\\lfloor 7000/10000 \\rfloor = 0$. Belongs to $P_{0,0}$.\n- Point $(15000, 5000)$: $\\lfloor 15000/10000 \\rfloor = 1$, $\\lfloor 5000/10000 \\rfloor = 0$. Belongs to $P_{1,0}$.\n- Pre-shift means:\n  - $\\mu_{P_{0,0}} = (0.6 + 0.2) / 2 = 0.4$.\n  - $\\mu_{P_{1,0}} = 0.8 / 1 = 0.8$.\n\n**Post-shift analysis**:\n- Shifted points: $(10000.6, 3000)$, $(7001, 7000)$, $(15001, 5000)$.\n- Point $(10000.6, 3000)$: $\\lfloor 10000.6/10000 \\rfloor = 1$. Belongs to $P_{1,0}$. (This point crossed the boundary).\n- Point $(7001, 7000)$: $\\lfloor 7001/10000 \\rfloor = 0$. Belongs to $P_{0,0}$.\n- Point $(15001, 5000)$: $\\lfloor 15001/10000 \\rfloor = 1$. Belongs to $P_{1,0}$.\n- Post-shift means:\n  - $\\mu'_{P_{0,0}} = 0.2 / 1 = 0.2$.\n  - $\\mu'_{P_{1,0}} = (0.6 + 0.8) / 2 = 0.7$.\n\n**Impact calculation**:\n- Change for $P_{0,0}$: $|\\mu'_{P_{0,0}} - \\mu_{P_{0,0}}| = |0.2 - 0.4| = 0.2$.\n- Change for $P_{1,0}$: $|\\mu'_{P_{1,0}} - \\mu_{P_{1,0}}| = |0.7 - 0.8| = 0.1$.\n- $I = \\max(0.2, 0.1) = 0.2$.\n\nThis procedure is applied to all test cases to derive the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"points\": [(5000.0, 5000.0, 0.4), (2000.0, 2000.0, 0.5)],\n            \"shift\": (1.0, 0.0)\n        },\n        {\n            \"points\": [(9999.6, 3000.0, 0.6), (7000.0, 7000.0, 0.2), (15000.0, 5000.0, 0.8)],\n            \"shift\": (1.0, 0.0)\n        },\n        {\n            \"points\": [(5000.0, 9999.5, 0.9), (4000.0, 2000.0, 0.1), (5000.0, 15000.0, 0.3)],\n            \"shift\": (0.0, 1.0)\n        },\n        {\n            \"points\": [(10000.0, 5000.0, 0.7), (8000.0, 6000.0, 0.5), (15000.0, 4000.0, 0.6)],\n            \"shift\": (-1.0, 0.0)\n        }\n    ]\n\n    L = 10000.0  # Side length of the square polygons in meters.\n\n    def get_polygon_aggregates(points, L_val):\n        \"\"\"\n        Aggregates capacity factors by polygon.\n\n        Args:\n            points (list): A list of (x, y, capacity_factor) tuples.\n            L_val (float): The side length of the polygon grid.\n\n        Returns:\n            dict: A dictionary mapping polygon indices (ix, iy) to a list of capacity factors.\n        \"\"\"\n        aggregates = {}\n        for x, y, c in points:\n            # Determine the polygon index using the floor function.\n            # This corresponds to the rule x_0 <= x < x_1.\n            ix = int(np.floor(x / L_val))\n            iy = int(np.floor(y / L_val))\n            key = (ix, iy)\n            if key not in aggregates:\n                aggregates[key] = []\n            aggregates[key].append(c)\n        return aggregates\n\n    def calculate_means(aggregates):\n        \"\"\"\n        Calculates the mean capacity factor for each polygon.\n\n        Args:\n            aggregates (dict): A dictionary mapping polygon indices to lists of factors.\n\n        Returns:\n            dict: A dictionary mapping polygon indices to mean capacity factors.\n        \"\"\"\n        means = {}\n        for key, values in aggregates.items():\n            if values:\n                means[key] = sum(values) / len(values)\n            else:\n                means[key] = 0.0\n        return means\n\n    def compute_impact_for_case(points, shift, L_val):\n        \"\"\"\n        Computes the impact metric I for a single test case.\n\n        Args:\n            points (list): A list of (x, y, capacity_factor) tuples.\n            shift (tuple): A (dx, dy) shift vector.\n            L_val (float): The side length of the polygon grid.\n\n        Returns:\n            float: The computed impact metric I.\n        \"\"\"\n        # 1. Pre-shift calculations\n        pre_shift_aggs = get_polygon_aggregates(points, L_val)\n        pre_shift_means = calculate_means(pre_shift_aggs)\n\n        # 2. Post-shift calculations\n        dx, dy = shift\n        shifted_points = [(x + dx, y + dy, c) for x, y, c in points]\n        post_shift_aggs = get_polygon_aggregates(shifted_points, L_val)\n        post_shift_means = calculate_means(post_shift_aggs)\n\n        # 3. Calculate impact metric I\n        # Get the union of all polygon keys that have points in either state.\n        all_polygon_keys = set(pre_shift_means.keys()) | set(post_shift_means.keys())\n        \n        max_abs_change = 0.0\n        for key in all_polygon_keys:\n            # Use .get(key, 0.0) to handle polygons that become empty or are newly populated.\n            mu_p = pre_shift_means.get(key, 0.0)\n            mu_p_prime = post_shift_means.get(key, 0.0)\n            abs_change = abs(mu_p_prime - mu_p)\n            if abs_change > max_abs_change:\n                max_abs_change = abs_change\n        \n        return max_abs_change\n\n    results = []\n    for case in test_cases:\n        impact_metric = compute_impact_for_case(case[\"points\"], case[\"shift\"], L)\n        results.append(impact_metric)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each value rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A fundamental task in energy systems modeling is the conversion of raw meteorological time series into metrics of power generation potential. This practice guides you through the process of deriving and implementing physics-based performance models for both wind and photovoltaic (PV) technologies from first principles. By transforming hourly wind speed and solar irradiance data into site-specific capacity factors, you will build an essential tool for any comprehensive resource assessment. ",
            "id": "4093383",
            "problem": "A geospatial and high-resolution spatiotemporal resource modeling workflow often requires converting hourly resource series into capacity factor fields for wind turbines and photovoltaic systems. Starting from fundamental physical relations, derive algorithmic methods that map an hourly wind speed series $v(t)$ to turbine power output via a physically motivated power curve, and an hourly plane-of-array irradiance series $POA(t)$ (denoted as $G(t)$) and ambient temperature series $T_{\\mathrm{amb}}(t)$ to photovoltaic power output via a temperature-adjusted performance model. Use these to compute the capacity factor for each site as the ratio of generated energy to the energy that would have been produced at rated power over the same period. The capacity factor is dimensionless and must be expressed as a decimal.\n\nYour derivations must begin from a valid fundamental base. For wind, begin from the kinetic energy flux through an area and the notion that the turbine power curve enforces cut-in, rated, and cut-out behavior. For photovoltaics, begin from plane-of-array irradiance conversion to direct current power via module efficiency at Standard Test Conditions and a linear temperature coefficient, with cell temperature estimated from the Nominal Operating Cell Temperature. No shortcut formulas should be assumed; derive each mapping logically from these bases. Ensure scientific realism in all algorithmic choices.\n\nUnits must be used as follows:\n- Wind speed in $\\mathrm{m/s}$.\n- Irradiance in $\\mathrm{W/m^2}$.\n- Ambient and cell temperatures in $^\\circ\\mathrm{C}$.\n- Power in $\\mathrm{kW}$.\n- Capacity factor as a dimensionless decimal.\n\nDefine the capacity factor for a time series of $N$ hourly samples as $CF = \\dfrac{\\sum_{t=1}^{N} P(t)}{P_{\\mathrm{rated}} \\cdot N}$, where $P(t)$ is the hour-$t$ power and $P_{\\mathrm{rated}}$ is the rated power. This definition is universally applicable; when the provided series spans a full year, it yields the annual capacity factor.\n\nImplement the derived methods and compute capacity factors for the following test suite, each case representing a distinct site or grid cell:\n\n- Wind Case A (happy path):\n  - Rated power $P_{\\mathrm{rated}} = 3000\\,\\mathrm{kW}$.\n  - Cut-in speed $v_{\\mathrm{ci}} = 3\\,\\mathrm{m/s}$.\n  - Rated speed $v_{\\mathrm{r}} = 12\\,\\mathrm{m/s}$.\n  - Cut-out speed $v_{\\mathrm{co}} = 25\\,\\mathrm{m/s}$.\n  - Hourly wind speeds $v(t)$ for $N=10$ hours: $[0, 2.5, 3, 6, 10, 12, 15, 24, 26, 8]$ in $\\mathrm{m/s}$.\n\n- Wind Case B (edge case, always below cut-in):\n  - Rated power $P_{\\mathrm{rated}} = 3000\\,\\mathrm{kW}$.\n  - Cut-in speed $v_{\\mathrm{ci}} = 3\\,\\mathrm{m/s}$.\n  - Rated speed $v_{\\mathrm{r}} = 12\\,\\mathrm{m/s}$.\n  - Cut-out speed $v_{\\mathrm{co}} = 25\\,\\mathrm{m/s}$.\n  - Hourly wind speeds $v(t)$ for $N=10$ hours: $[0, 1, 2, 2.9, 0.5, 2.8, 2.999, 2, 1.5, 0]$ in $\\mathrm{m/s}$.\n\n- Photovoltaic Case C (day-night variation, no clipping):\n  - Rated power $P_{\\mathrm{rated}} = 500\\,\\mathrm{kW}$.\n  - Reference module efficiency at Standard Test Conditions $\\eta_{\\mathrm{ref}} = 0.18$.\n  - Power temperature coefficient $\\beta = -0.004\\,^\\circ\\mathrm{C}^{-1}$.\n  - Nominal Operating Cell Temperature $NOCT = 45\\,^\\circ\\mathrm{C}$.\n  - Hourly plane-of-array irradiance $G(t)$ for $N=10$ hours: $[0, 0, 200, 400, 600, 800, 1000, 800, 400, 0]$ in $\\mathrm{W/m^2}$.\n  - Hourly ambient temperature $T_{\\mathrm{amb}}(t)$ for $N=10$ hours: $[15, 15, 16, 17, 18, 20, 22, 24, 22, 18]$ in $^\\circ\\mathrm{C}$.\n\n- Photovoltaic Case D (high irradiance with inverter clipping):\n  - Rated power $P_{\\mathrm{rated}} = 1000\\,\\mathrm{kW}$.\n  - Reference module efficiency at Standard Test Conditions $\\eta_{\\mathrm{ref}} = 0.18$.\n  - Power temperature coefficient $\\beta = -0.004\\,^\\circ\\mathrm{C}^{-1}$.\n  - Nominal Operating Cell Temperature $NOCT = 45\\,^\\circ\\mathrm{C}$.\n  - Hourly plane-of-array irradiance $G(t)$ for $N=10$ hours: $[0, 300, 600, 900, 1100, 1200, 1100, 900, 600, 300]$ in $\\mathrm{W/m^2}$.\n  - Hourly ambient temperature $T_{\\mathrm{amb}}(t)$ for $N=10$ hours: $[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$ in $^\\circ\\mathrm{C}$.\n\nYour program should produce a single line of output containing the four capacity factor values corresponding to the cases above, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example $[x_1, x_2, x_3, x_4]$.",
            "solution": "The problem requires the derivation and implementation of algorithmic methods for calculating the capacity factor of wind and photovoltaic (PV) power generation systems from hourly time series data. The derivation must originate from fundamental physical principles as specified.\n\nFirst, we address the wind turbine model. The power available in the wind derives from the kinetic energy of moving air. The kinetic energy $E_k$ of an air mass $m$ moving at speed $v$ is given by $E_k = \\frac{1}{2}mv^2$. The power is the rate of energy flow. Considering a stream of air passing through a turbine's rotor area $A$ in time $\\Delta t$, the volume of air is $V = A \\cdot v \\cdot \\Delta t$. With air density $\\rho$, the mass is $m = \\rho V = \\rho A v \\Delta t$. The kinetic energy flux, or power in the wind, is therefore $P_{\\text{wind}} = \\frac{E_k}{\\Delta t} = \\frac{1}{2}\\rho A v^3$. This establishes that the available power is proportional to the cube of the wind speed, i.e., $P_{\\text{wind}} \\propto v^3$.\n\nA real wind turbine's electrical power output, $P(t)$, is described by a power curve which is a function of the instantaneous wind speed $v(t)$. This curve is defined by four operational regions based on the cut-in speed $v_{\\mathrm{ci}}$, rated speed $v_{\\mathrm{r}}$, and cut-out speed $v_{\\mathrm{co}}$.\n1.  For $v(t) < v_{\\mathrm{ci}}$, the wind speed is insufficient to overcome the turbine's mechanical and electrical losses, so the power output is zero: $P(t) = 0$.\n2.  For $v_{\\mathrm{ci}} \\le v(t) < v_{\\mathrm{r}}$, the turbine generates power. Following the physical motivation that extracted power should relate to the available power in the wind, the output in this region is modeled as a function of $v(t)^3$. A common and physically consistent model that satisfies the boundary conditions $P(v_{\\mathrm{ci}}) = 0$ and $P(v_{\\mathrm{r}}) = P_{\\mathrm{rated}}$ is a scaled cubic function:\n    $$P(t) = P_{\\mathrm{rated}} \\cdot \\left( \\frac{v(t)^3 - v_{\\mathrm{ci}}^3}{v_{\\mathrm{r}}^3 - v_{\\mathrm{ci}}^3} \\right)$$\n    This ensures a smooth, physically-based power ramp-up.\n3.  For $v_{\\mathrm{r}} \\le v(t) < v_{\\mathrm{co}}$, the turbine's control system limits the output to its rated power $P_{\\mathrm{rated}}$ to protect the generator and power electronics. Thus, $P(t) = P_{\\mathrm{rated}}$.\n4.  For $v(t) \\ge v_{\\mathrm{co}}$, the turbine shuts down to prevent structural damage from extreme wind loads. The power output drops to zero: $P(t) = 0$.\n\nCombining these regions, the complete algorithmic mapping from $v(t)$ to $P(t)$ for a wind turbine is:\n$$\nP(t) = \n\\begin{cases} \n0 & \\text{if } v(t) < v_{\\mathrm{ci}} \\text{ or } v(t) \\ge v_{\\mathrm{co}} \\\\\nP_{\\mathrm{rated}} \\cdot \\left(\\frac{v(t)^3 - v_{\\mathrm{ci}}^3}{v_{\\mathrm{r}}^3 - v_{\\mathrm{ci}}^3}\\right) & \\text{if } v_{\\mathrm{ci}} \\le v(t) < v_{\\mathrm{r}} \\\\\nP_{\\mathrm{rated}} & \\text{if } v_{\\mathrm{r}} \\le v(t) < v_{\\mathrm{co}}\n\\end{cases}\n$$\n\nNext, we address the photovoltaic (PV) system model. The power generated by a PV system depends on the incoming solar irradiance, the system's area, and its conversion efficiency. The rated power, $P_{\\mathrm{rated}}$, is specified at Standard Test Conditions (STC), which are a reference irradiance $G_{\\mathrm{ref}} = 1000\\,\\mathrm{W/m^2}$ and a reference cell temperature $T_{\\mathrm{ref}} = 25\\,^\\circ\\mathrm{C}$. At these conditions, the power is $P_{\\mathrm{rated}} = G_{\\mathrm{ref}} \\cdot A_{\\mathrm{sys}} \\cdot \\eta_{\\mathrm{ref}}$, where $A_{\\mathrm{sys}}$ is the total system area and $\\eta_{\\mathrm{ref}}$ is the reference module efficiency. We must handle units carefully; if $P_{\\mathrm{rated}}$ is in $\\mathrm{kW}$, we convert it to $\\mathrm{W}$ by multiplying by $1000$. The effective area of the system can be derived as $A_{\\mathrm{sys}} = \\frac{1000 \\cdot P_{\\mathrm{rated}}}{G_{\\mathrm{ref}} \\cdot \\eta_{\\mathrm{ref}}}$.\nThe power generated under an arbitrary plane-of-array irradiance $G(t)$, without considering temperature effects, would be $P_{\\text{potential}}(t) = G(t) \\cdot A_{\\mathrm{sys}} \\cdot \\eta_{\\mathrm{ref}}$. Substituting the expression for $A_{\\mathrm{sys}}$ and using $G_{\\mathrm{ref}} = 1000\\,\\mathrm{W/m^2}$:\n$$P_{\\text{potential}}(t) = G(t) \\cdot \\left(\\frac{1000 \\cdot P_{\\mathrm{rated}}}{1000 \\cdot \\eta_{\\mathrm{ref}}}\\right) \\cdot \\eta_{\\mathrm{ref}} = P_{\\mathrm{rated}} \\cdot \\frac{G(t)}{1000}$$\nThis provides a direct scaling of the rated power by the irradiance ratio, with $P(t)$ having the same units as $P_{\\mathrm{rated}}$ (kW).\n\nThe module's efficiency, and thus its power output, is sensitive to its operating temperature, $T_{\\mathrm{cell}}(t)$. First, we must estimate $T_{\\mathrm{cell}}(t)$. The problem specifies using the Nominal Operating Cell Temperature ($NOCT$) model. $NOCT$ is the cell temperature under conditions of $G_{\\mathrm{NOCT}} = 800\\,\\mathrm{W/m^2}$ and $T_{\\mathrm{amb,NOCT}} = 20\\,^\\circ\\mathrm{C}$. The cell temperature rises above ambient temperature in proportion to the incident irradiance. The model is:\n$$T_{\\mathrm{cell}}(t) = T_{\\mathrm{amb}}(t) + (NOCT - T_{\\mathrm{amb,NOCT}}) \\cdot \\frac{G(t)}{G_{\\mathrm{NOCT}}} = T_{\\mathrm{amb}}(t) + (NOCT - 20) \\cdot \\frac{G(t)}{800}$$\nwhere all temperatures are in $^\\circ\\mathrm{C}$.\n\nNext, we account for the temperature's effect on power output using the given linear power temperature coefficient, $\\beta$. This coefficient modifies the power output relative to the reference temperature $T_{\\mathrm{ref}} = 25\\,^\\circ\\mathrm{C}$. The total DC power generated, $P_{\\mathrm{DC}}(t)$, is then:\n$$P_{\\mathrm{DC}}(t) = P_{\\text{potential}}(t) \\cdot [1 + \\beta(T_{\\mathrm{cell}}(t) - T_{\\mathrm{ref}})] = P_{\\mathrm{rated}} \\cdot \\frac{G(t)}{1000} \\cdot [1 + \\beta(T_{\\mathrm{cell}}(t) - 25)]$$\nSince $\\beta$ is typically negative for crystalline silicon PV modules, higher cell temperatures lead to lower power output.\n\nFinally, the AC power delivered to the grid, $P(t)$, is limited by the inverter's maximum power output, which is assumed to be equal to the system's rated power $P_{\\mathrm{rated}}$. This phenomenon is known as inverter clipping. Therefore, the final power output is the lesser of the calculated DC power and the rated power. We also ensure power is non-negative.\n$$P(t) = \\max(0, \\min(P_{\\mathrm{DC}}(t), P_{\\mathrm{rated}}))$$\n\nFor both wind and PV systems, the capacity factor ($CF$) for a period of $N$ hours is defined as the ratio of the total energy actually generated to the maximum possible energy that could have been generated if the system operated at its rated power $P_{\\mathrm{rated}}$ for the entire period.\n$$CF = \\frac{\\sum_{t=1}^{N} P(t) \\cdot (1\\,\\text{hr})}{P_{\\mathrm{rated}} \\cdot N \\cdot (1\\,\\text{hr})} = \\frac{\\sum_{t=1}^{N} P(t)}{P_{\\mathrm{rated}} \\cdot N}$$\nUsing the derived functions for $P(t)$, we can compute the capacity factor for each of the provided test cases. All calculations will maintain power in units of $\\mathrm{kW}$ as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are needed for this problem.\n\ndef solve():\n    \"\"\"\n    Derives and implements models for wind and PV power generation to calculate\n    capacity factors for a given set of test cases.\n    \"\"\"\n\n    # --- Model Implementation ---\n\n    def calculate_wind_power_series(v_series, p_rated, v_ci, v_r, v_co):\n        \"\"\"\n        Calculates hourly power output for a wind turbine from a wind speed series.\n        \n        Args:\n            v_series (list or np.array): Hourly wind speeds in m/s.\n            p_rated (float): Rated power in kW.\n            v_ci (float): Cut-in speed in m/s.\n            v_r (float): Rated speed in m/s.\n            v_co (float): Cut-out speed in m/s.\n\n        Returns:\n            np.array: Hourly power output in kW.\n        \"\"\"\n        v = np.array(v_series, dtype=float)\n        power = np.zeros_like(v)\n\n        # Region 1: Normal operation between cut-in and rated speed\n        # P(v) = P_rated * (v^3 - v_ci^3) / (v_r^3 - v_ci^3)\n        op_mask = (v >= v_ci) & (v < v_r)\n        v_op = v[op_mask]\n        \n        # Avoid division by zero if v_r <= v_ci, though not expected for valid problems\n        v_r3_minus_v_ci3 = v_r**3 - v_ci**3\n        if v_r3_minus_v_ci3 > 0:\n            power[op_mask] = p_rated * (v_op**3 - v_ci**3) / v_r3_minus_v_ci3\n\n        # Region 2: Rated power output\n        rated_mask = (v >= v_r) & (v < v_co)\n        power[rated_mask] = p_rated\n\n        # Regions 3 & 4 (below cut-in and above cut-out) are already 0.\n        \n        return power\n\n    def calculate_pv_power_series(g_series, t_amb_series, p_rated, beta, noct):\n        \"\"\"\n        Calculates hourly power output for a PV system.\n        \n        Args:\n            g_series (list or np.array): Hourly plane-of-array irradiance in W/m^2.\n            t_amb_series (list or np.array): Hourly ambient temperature in °C.\n            p_rated (float): Rated power in kW.\n            beta (float): Power temperature coefficient in 1/°C.\n            noct (float): Nominal Operating Cell Temperature in °C.\n\n        Returns:\n            np.array: Hourly power output in kW.\n        \"\"\"\n        g = np.array(g_series, dtype=float)\n        t_amb = np.array(t_amb_series, dtype=float)\n        power = np.zeros_like(g)\n\n        # Constants from PV modeling standards\n        G_REF = 1000.0  # W/m^2 (STC irradiance)\n        T_REF = 25.0    # °C (STC cell temperature)\n        G_NOCT = 800.0  # W/m^2 (NOCT irradiance)\n        T_AMB_NOCT = 20.0 # °C (NOCT ambient temperature)\n\n        # Calculate only for hours with sunlight\n        sunlight_mask = g > 0\n        g_sun = g[sunlight_mask]\n        t_amb_sun = t_amb[sunlight_mask]\n\n        # 1. Calculate cell temperature using the NOCT model\n        t_cell = t_amb_sun + (noct - T_AMB_NOCT) * g_sun / G_NOCT\n\n        # 2. Calculate DC power with temperature adjustment\n        # P_potential = P_rated * G / G_ref\n        # P_dc = P_potential * (1 + beta * (T_cell - T_ref))\n        p_dc = (p_rated * g_sun / G_REF) * (1 + beta * (t_cell - T_REF))\n        \n        # 3. Apply inverter clipping and ensure non-negative power\n        p_final = np.maximum(0, np.minimum(p_dc, p_rated))\n\n        power[sunlight_mask] = p_final\n        return power\n\n    # --- Test Case Definitions ---\n    \n    test_cases = [\n        {\n            \"type\": \"wind\",\n            \"params\": {\n                \"v_series\": [0, 2.5, 3, 6, 10, 12, 15, 24, 26, 8],\n                \"p_rated\": 3000.0, \"v_ci\": 3.0, \"v_r\": 12.0, \"v_co\": 25.0\n            }\n        },\n        {\n            \"type\": \"wind\",\n            \"params\": {\n                \"v_series\": [0, 1, 2, 2.9, 0.5, 2.8, 2.999, 2, 1.5, 0],\n                \"p_rated\": 3000.0, \"v_ci\": 3.0, \"v_r\": 12.0, \"v_co\": 25.0\n            }\n        },\n        {\n            \"type\": \"pv\",\n            \"params\": {\n                \"g_series\": [0, 0, 200, 400, 600, 800, 1000, 800, 400, 0],\n                \"t_amb_series\": [15, 15, 16, 17, 18, 20, 22, 24, 22, 18],\n                \"p_rated\": 500.0, \"beta\": -0.004, \"noct\": 45.0\n            }\n        },\n        {\n            \"type\": \"pv\",\n            \"params\": {\n                \"g_series\": [0, 300, 600, 900, 1100, 1200, 1100, 900, 600, 300],\n                \"t_amb_series\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                \"p_rated\": 1000.0, \"beta\": -0.004, \"noct\": 45.0\n            }\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        p_rated = params[\"p_rated\"]\n        \n        if case[\"type\"] == \"wind\":\n            power_series = calculate_wind_power_series(**params)\n            N = len(params[\"v_series\"])\n        elif case[\"type\"] == \"pv\":\n            power_series = calculate_pv_power_series(**params)\n            N = len(params[\"g_series\"])\n        \n        # Calculate Capacity Factor\n        # CF = sum(P_t) / (P_rated * N)\n        total_energy_generated = np.sum(power_series)\n        max_possible_energy = p_rated * N\n        \n        if max_possible_energy == 0:\n            capacity_factor = 0.0\n        else:\n            capacity_factor = total_energy_generated / max_possible_energy\n            \n        results.append(capacity_factor)\n\n    # Format results to six decimal places for the final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond assessing resources, energy systems modeling involves planning the infrastructure needed to harness them. This exercise simulates the real-world challenge of siting a transmission corridor by applying least-cost path analysis on a raster-based cost surface. You will learn to construct a cost function that incorporates physical obstacles like steep terrain and policy constraints like protected areas, and then use graph search algorithms to identify the optimal route. ",
            "id": "4093384",
            "problem": "You are tasked with implementing a Least-Cost Path (LCP) procedure for transmission corridor design on a discrete raster. The goal is to formalize and compute the minimum total route cost and the corresponding route length, given a cost surface that penalizes slope and protected areas. The formulation must be grounded in physically meaningful definitions. The program must solve the cases specified in the provided test suite and output the final results in the required format.\n\nDefinitions and modeling base:\n- The corridor design problem assumes a regular grid of cells with uniform spacing. Let the cell size be $c$ in meters. Let the elevation field be a Digital Elevation Model (DEM) $Z(i,j)$ in meters. Let the protected area mask be $P(i,j)$, where $P(i,j)=1$ indicates a protected cell and $P(i,j)=0$ otherwise. All movement is constrained to the $8$-connected neighborhood (cardinal and diagonal moves).\n- The per-unit-length traversal cost at a location is modeled as\n$$\nC(i,j) = \\gamma + \\alpha \\cdot \\theta(i,j) + \\beta \\cdot P(i,j),\n$$\nwhere $\\gamma$ is the base cost in dollars per meter, $\\alpha$ is the slope penalty coefficient in dollars per meter per radian, and $\\beta$ is the protected area penalty in dollars per meter, and $\\theta(i,j)$ is the local slope angle in radians.\n- The local slope angle $\\theta(i,j)$ is derived from the surface gradient magnitude. Approximate the partial derivatives by finite differences:\n$$\ng_x(i,j)=\n\\begin{cases}\n\\dfrac{Z(i+1,j)-Z(i,j)}{c} & \\text{if } i=0, \\\\\n\\dfrac{Z(i,j)-Z(i-1,j)}{c} & \\text{if } i=n_x-1, \\\\\n\\dfrac{Z(i+1,j)-Z(i-1,j)}{2c} & \\text{otherwise},\n\\end{cases}\n\\qquad\ng_y(i,j)=\n\\begin{cases}\n\\dfrac{Z(i,j+1)-Z(i,j)}{c} & \\text{if } j=0, \\\\\n\\dfrac{Z(i,j)-Z(i,j-1)}{c} & \\text{if } j=n_y-1, \\\\\n\\dfrac{Z(i,j+1)-Z(i,j-1)}{2c} & \\text{otherwise}.\n\\end{cases}\n$$\nThen the slope magnitude is\n$$\nm(i,j)=\\sqrt{g_x(i,j)^2+g_y(i,j)^2},\n$$\nand the slope angle is\n$$\n\\theta(i,j)=\\arctan\\!\\big(m(i,j)\\big),\n$$\nexpressed in radians.\n- For a movement from cell $i$ to an adjacent cell $j$ in the $8$-connected neighborhood, the edge length $d_{ij}$ is\n$$\nd_{ij}=\n\\begin{cases}\nc & \\text{for cardinal moves},\\\\\nc\\sqrt{2} & \\text{for diagonal moves}.\n\\end{cases}\n$$\nTo discretize the line integral of cost along the path, approximate the edge traversal cost by the trapezoidal rule,\n$$\nw_{ij}=d_{ij}\\cdot \\dfrac{C(i)+C(j)}{2}.\n$$\nThe total route cost is the sum of $w_{ij}$ along the chosen path, and the route length is the sum of $d_{ij}$ along the path.\n- Compute the least-cost path between a specified start cell $(s_x,s_y)$ and target cell $(t_x,t_y)$ on the grid, treating each cell as a node in a graph with edge weights $w_{ij}$ as defined above. Implement a path-search method consistent with nonnegative edge weights.\n\nAngle unit requirement:\n- All angles must be computed and used in radians.\n\nUnits and output requirements:\n- Express the final route length in meters and the final route cost in dollars. Round both numbers to $3$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $[ \\text{total\\_cost}, \\text{route\\_length} ]$. For example, the output should look like $[[x_1,y_1],[x_2,y_2],\\dots]$ with no spaces.\n\nTest suite:\nImplement the algorithm for the following parameter sets. In each case, $Z$ is provided in meters, $P$ is dimensionless with entries $0$ or $1$, $c$ is in meters, and the costs $\\gamma$, $\\alpha$, and $\\beta$ are in dollars per meter (with $\\alpha$ per radian).\n\nCase $1$ (general path with central protected area):\n- Grid size: $n_x=5$, $n_y=5$.\n- Cell size: $c=100$.\n- Elevation $Z$:\n$$\n\\begin{bmatrix}\n100 & 120 & 140 & 160 & 180 \\\\\n100 & 120 & 140 & 160 & 200 \\\\\n100 & 120 & 160 & 200 & 240 \\\\\n100 & 120 & 160 & 200 & 240 \\\\\n90 & 110 & 130 & 150 & 170\n\\end{bmatrix}\n$$\n- Protected mask $P$:\n$$\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Parameters: $\\gamma=500$, $\\alpha=1000$, $\\beta=4000$.\n- Start and target: $(s_x,s_y)=(0,0)$, $(t_x,t_y)=(4,4)$.\n\nCase $2$ (start equals target, flat terrain):\n- Grid size: $n_x=3$, $n_y=3$.\n- Cell size: $c=50$.\n- Elevation $Z$:\n$$\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n- Protected mask $P$:\n$$\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n- Parameters: $\\gamma=400$, $\\alpha=800$, $\\beta=2000$.\n- Start and target: $(s_x,s_y)=(1,1)$, $(t_x,t_y)=(1,1)$.\n\nCase $3$ (ridge-induced slope penalties with a small protected cell):\n- Grid size: $n_x=4$, $n_y=4$.\n- Cell size: $c=100$.\n- Elevation $Z$:\n$$\n\\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 50 & 200 & 250 \\\\\n0 & 50 & 200 & 250 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Protected mask $P$:\n$$\n\\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Parameters: $\\gamma=600$, $\\alpha=1200$, $\\beta=3000$.\n- Start and target: $(s_x,s_y)=(0,3)$, $(t_x,t_y)=(3,0)$.\n\nCase $4$ (corridor avoidance around protected barrier):\n- Grid size: $n_x=6$, $n_y=6$.\n- Cell size: $c=200$.\n- Elevation $Z$:\n$$\n\\begin{bmatrix}\n0 & 10 & 20 & 30 & 40 & 50 \\\\\n10 & 20 & 30 & 40 & 50 & 60 \\\\\n20 & 30 & 40 & 50 & 60 & 70 \\\\\n30 & 40 & 50 & 60 & 70 & 80 \\\\\n40 & 50 & 60 & 70 & 80 & 90 \\\\\n50 & 60 & 70 & 80 & 90 & 100\n\\end{bmatrix}\n$$\n- Protected mask $P$:\n$$\n\\begin{bmatrix}\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Parameters: $\\gamma=700$, $\\alpha=900$, $\\beta=5000$.\n- Start and target: $(s_x,s_y)=(0,0)$, $(t_x,t_y)=(5,5)$.\n\nImplementation and output:\n- Implement the slope computation as specified, compute $C(i,j)$, set up the $8$-connected graph with edge weights $w_{ij}$, and find the least-cost path from the start to the target. Compute both the total route cost and the total route length. Use radians for all angle computations.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result formatted as $[\\text{total\\_cost},\\text{route\\_length}]$, rounded to $3$ decimal places, and with no spaces. For example: $[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4]]$.",
            "solution": "The problem of finding the least-cost path on a raster grid is solved by modeling the grid as a graph and applying a shortest path algorithm. Each grid cell $(i,j)$ becomes a node, and edges connect it to its 8 neighbors. The solution process involves three primary stages: computing a cost surface, defining graph edge weights, and executing Dijkstra's algorithm.\n\nFirst, a cost surface is computed. The per-unit-length traversal cost $C(i,j)$ at each cell depends on the local slope angle $\\theta(i,j)$ and protected area status $P(i,j)$. To find the slope, the partial derivatives of the elevation grid, $g_x$ and $g_y$, are calculated using the specified finite difference formulas. This is efficiently implemented using `numpy.gradient`. The slope magnitude $m(i,j) = \\sqrt{g_x(i,j)^2 + g_y(i,j)^2}$ and angle $\\theta(i,j) = \\arctan(m(i,j))$ are then computed for each cell. Finally, the complete cost surface $C$ is assembled using the linear model: $C(i,j) = \\gamma + \\alpha \\cdot \\theta(i,j) + \\beta \\cdot P(i,j)$.\n\nSecond, the edge weights for the graph are defined. The cost to traverse an edge between two adjacent cells, $i$ and $j$, is calculated using the trapezoidal rule: $w_{ij} = d_{ij} \\cdot \\frac{C(i)+C(j)}{2}$. This averages the per-unit-length costs of the two cells and multiplies by the physical distance of the move, $d_{ij}$. The distance is $c$ for cardinal moves and $c\\sqrt{2}$ for diagonal moves.\n\nThird, Dijkstra's algorithm is employed to find the least-cost path. Since all cost parameters and the slope angle are non-negative, all edge weights $w_{ij}$ are also non-negative, making Dijkstra's algorithm a suitable and optimal choice. The algorithm is implemented with a priority queue (min-heap) for efficiency. Two grids, `dist` and `length`, are maintained to track the minimum accumulated cost and the corresponding physical path length to each cell from the start. The algorithm iteratively explores the grid, always expanding from the lowest-cost unvisited node, until the shortest path to the target cell is found. The final cost and length are then retrieved from the `dist` and `length` grids at the target cell's coordinates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport heapq\n\ndef _calculate_path(Z, P, c, gamma, alpha, beta, start, target):\n    \"\"\"\n    Calculates the least-cost path cost and length on a raster grid.\n    \"\"\"\n    start_node = tuple(start)\n    target_node = tuple(target)\n\n    if start_node == target_node:\n        return [0.0, 0.0]\n\n    Z = np.array(Z, dtype=float)\n    P = np.array(P, dtype=float)\n    n_rows, n_cols = Z.shape\n\n    # Step 1: Compute slope and cost grids\n    # The finite difference formulas match numpy's gradient function when\n    # a single spacing 'c' is provided.\n    # np.gradient returns [d/d_axis0, d/d_axis1], which corresponds to [g_x, g_y].\n    g_x, g_y = np.gradient(Z, c)\n    m = np.sqrt(g_x**2 + g_y**2)\n    theta = np.arctan(m)\n    \n    C = gamma + alpha * theta + beta * P\n\n    # Step 2: Dijkstra's algorithm\n    dist = np.full((n_rows, n_cols), np.inf)\n    length = np.full((n_rows, n_cols), np.inf)\n    \n    dist[start_node] = 0.0\n    length[start_node] = 0.0\n    \n    # Priority queue stores (cost, (row, col))\n    pq = [(0.0, start_node)]\n    visited = set()\n    \n    sqrt2c = c * np.sqrt(2)\n\n    while pq:\n        cost, current_node = heapq.heappop(pq)\n        \n        if current_node in visited:\n            continue\n        \n        visited.add(current_node)\n        \n        if current_node == target_node:\n            break\n            \n        r, c_node = current_node\n        \n        # Explore 8-connected neighbors\n        for dr in [-1, 0, 1]:\n            for dc in [-1, 0, 1]:\n                if dr == 0 and dc == 0:\n                    continue\n                \n                nr, nc = r + dr, c_node + dc\n                neighbor_node = (nr, nc)\n                \n                if not (0 = nr  n_rows and 0 = nc  n_cols):\n                    continue\n\n                # Calculate edge length and cost\n                is_cardinal = (dr == 0 or dc == 0)\n                d_ij = c if is_cardinal else sqrt2c\n                \n                # Trapezoidal rule for edge weight\n                avg_cost = (C[current_node] + C[neighbor_node]) / 2.0\n                w_ij = d_ij * avg_cost\n                \n                new_cost = dist[current_node] + w_ij\n                \n                if new_cost  dist[neighbor_node]:\n                    dist[neighbor_node] = new_cost\n                    length[neighbor_node] = length[current_node] + d_ij\n                    heapq.heappush(pq, (new_cost, neighbor_node))\n\n    final_cost = dist[target_node]\n    final_length = length[target_node]\n    \n    return [round(final_cost, 3), round(final_length, 3)]\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": [[100, 120, 140, 160, 180],\n                  [100, 120, 140, 160, 200],\n                  [100, 120, 160, 200, 240],\n                  [100, 120, 160, 200, 240],\n                  [90, 110, 130, 150, 170]],\n            \"P\": [[0, 0, 0, 0, 0],\n                  [0, 0, 1, 1, 0],\n                  [0, 1, 1, 1, 0],\n                  [0, 0, 1, 0, 0],\n                  [0, 0, 0, 0, 0]],\n            \"c\": 100, \"gamma\": 500, \"alpha\": 1000, \"beta\": 4000,\n            \"start\": (0, 0), \"target\": (4, 4)\n        },\n        {\n            \"Z\": [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n            \"P\": [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n            \"c\": 50, \"gamma\": 400, \"alpha\": 800, \"beta\": 2000,\n            \"start\": (1, 1), \"target\": (1, 1)\n        },\n        {\n            \"Z\": [[0, 0, 0, 0],\n                  [0, 50, 200, 250],\n                  [0, 50, 200, 250],\n                  [0, 0, 0, 0]],\n            \"P\": [[0, 0, 0, 0],\n                  [0, 0, 1, 0],\n                  [0, 0, 0, 0],\n                  [0, 0, 0, 0]],\n            \"c\": 100, \"gamma\": 600, \"alpha\": 1200, \"beta\": 3000,\n            \"start\": (0, 3), \"target\": (3, 0)\n        },\n        {\n            \"Z\": [[0, 10, 20, 30, 40, 50],\n                  [10, 20, 30, 40, 50, 60],\n                  [20, 30, 40, 50, 60, 70],\n                  [30, 40, 50, 60, 70, 80],\n                  [40, 50, 60, 70, 80, 90],\n                  [50, 60, 70, 80, 90, 100]],\n            \"P\": [[0, 0, 1, 0, 0, 0],\n                  [0, 0, 1, 0, 0, 0],\n                  [0, 0, 1, 1, 1, 0],\n                  [0, 0, 1, 0, 0, 0],\n                  [0, 0, 1, 0, 0, 0],\n                  [0, 0, 1, 0, 0, 0]],\n            \"c\": 200, \"gamma\": 700, \"alpha\": 900, \"beta\": 5000,\n            \"start\": (0, 0), \"target\": (5, 5)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _calculate_path(\n            case[\"Z\"], case[\"P\"], case[\"c\"],\n            case[\"gamma\"], case[\"alpha\"], case[\"beta\"],\n            case[\"start\"], case[\"target\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{cost},{length}]\" for cost, length in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```"
        }
    ]
}