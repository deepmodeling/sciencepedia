{
    "hands_on_practices": [
        {
            "introduction": "Real-time monitoring is crucial for maintaining grid stability, requiring methods that can detect significant changes in load or generation as they happen. This exercise guides you through the first-principles derivation of the Sequential Probability Ratio Test (SPRT), an optimal procedure for online change detection. By deriving the test statistic and its expected stopping time, you will gain a fundamental understanding of how to balance detection speed with statistical certainty in streaming data analysis .",
            "id": "4077365",
            "problem": "A transmission operator monitors minute-level aggregate electrical load from a large balancing area. Let the streaming load measurements be $\\{X_t\\}_{t \\ge 1}$, recorded once per minute in megawatts, modeled as independent and identically distributed Gaussian random variables with known variance $\\sigma^2$ and an unknown mean that may shift due to a persistent demand change. Under nominal conditions, the mean is $\\mu_0$, and if a step change occurs, it changes to $\\mu_1 > \\mu_0$ and remains there. For online detection, consider the simple versus simple hypotheses $H_0: X_t \\sim \\mathcal{N}(\\mu_0,\\sigma^2)$ versus $H_1: X_t \\sim \\mathcal{N}(\\mu_1,\\sigma^2)$ with known $\\mu_0$, $\\mu_1$, and $\\sigma^2$.\n\nStarting only from the definition of the Gaussian probability density function and the definition of a likelihood ratio for simple hypotheses, derive the Sequential Probability Ratio Test (SPRT) for detecting the upward mean shift. Your derivation must establish the following components from first principles:\n- The per-sample log-likelihood increment $Z_t$ and the cumulative log-likelihood $S_n = \\sum_{t=1}^{n} Z_t$.\n- A stopping rule based on two fixed thresholds $a$ and $b$ applied to $S_n$ that achieves Type I error probability $\\alpha$ and Type II error probability $\\beta$.\n- The expected per-sample drift of $S_n$ under $H_0$ and under $H_1$.\n\nThen, using Waldâ€™s identity and a negligible-overshoot approximation at the stopping boundaries, derive analytic expressions for the expected sample size under $H_0$, denoted $\\mathbb{E}_0[N]$, and under $H_1$, denoted $\\mathbb{E}_1[N]$, in terms of the drifts and the thresholds $a$ and $b$. Finally, evaluate these expressions numerically for the following parameters:\n- $\\mu_0 = 500\\,\\text{MW}$,\n- $\\mu_1 = 520\\,\\text{MW}$,\n- $\\sigma = 20\\,\\text{MW}$,\n- $\\alpha = 0.01$,\n- $\\beta = 0.05$.\n\nExpress the final expected sample sizes as the expected number of one-minute samples. Round your numerical results to four significant figures. Provide your final answer as a row vector $\\big[\\mathbb{E}_0[N],\\,\\mathbb{E}_1[N]\\big]$ with no units in the vector.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The data are streaming load measurements $\\{X_t\\}_{t \\ge 1}$, recorded once per minute.\n- The measurements are modeled as independent and identically distributed (i.i.d.) Gaussian random variables.\n- The variance is known and denoted by $\\sigma^2$.\n- The two simple hypotheses are:\n    - $H_0: X_t \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ (nominal conditions).\n    - $H_1: X_t \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ (post-change conditions).\n- The means $\\mu_0$ and $\\mu_1$, and the standard deviation $\\sigma$ are known constants.\n- It is specified that $\\mu_1 > \\mu_0$.\n- The desired Type I error probability is $\\alpha$.\n- The desired Type II error probability is $\\beta$.\n- Numerical parameters for evaluation:\n    - $\\mu_0 = 500\\,\\text{MW}$\n    - $\\mu_1 = 520\\,\\text{MW}$\n    - $\\sigma = 20\\,\\text{MW}$\n    - $\\alpha = 0.01$\n    - $\\beta = 0.05$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded**: The problem is a standard application of Sequential Probability Ratio Test (SPRT), a fundamental and well-established method in sequential analysis and statistical process control. The modeling of aggregate electrical load as a Gaussian process is a common and reasonable simplification in many energy system analyses.\n- **Well-Posed**: All necessary parameters ($\\mu_0, \\mu_1, \\sigma, \\alpha, \\beta$) for the derivation and subsequent numerical calculation are provided. The objectives are clearly stated, leading to a unique analytical and numerical solution.\n- **Objective**: The problem is stated in precise, formal mathematical and statistical language, free of any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the invalidity flaws. It is a well-defined, standard problem in applied statistics.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Derivation of the Sequential Probability Ratio Test\n\n#### Per-Sample Log-Likelihood Increment\nThe foundation of the SPRT is the likelihood ratio. For a single observation $X_t$, this ratio compares the probability of observing $X_t$ under hypothesis $H_1$ to the probability under $H_0$. The probability density function (PDF) for a Gaussian random variable $X$ with mean $\\mu$ and variance $\\sigma^2$ is:\n$$f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\nThe likelihood ratio for the observation $X_t$ is:\n$$L_t = \\frac{f(X_t; \\mu_1, \\sigma^2)}{f(X_t; \\mu_0, \\sigma^2)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_t - \\mu_1)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_t - \\mu_0)^2}{2\\sigma^2}\\right)}$$\nThe pre-exponential factors cancel, yielding:\n$$L_t = \\exp\\left( \\frac{(X_t - \\mu_0)^2 - (X_t - \\mu_1)^2}{2\\sigma^2} \\right)$$\nFor computational convenience and theoretical analysis, we work with the natural logarithm of the likelihood ratio, known as the log-likelihood increment, $Z_t$:\n$$Z_t = \\ln(L_t) = \\frac{1}{2\\sigma^2} \\left[ (X_t - \\mu_0)^2 - (X_t - \\mu_1)^2 \\right]$$\nExpanding the squared terms:\n$$Z_t = \\frac{1}{2\\sigma^2} \\left[ (X_t^2 - 2X_t\\mu_0 + \\mu_0^2) - (X_t^2 - 2X_t\\mu_1 + \\mu_1^2) \\right]$$\nSimplifying the expression by canceling the $X_t^2$ terms:\n$$Z_t = \\frac{1}{2\\sigma^2} \\left[ 2X_t(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 \\right]$$\nFactoring $(\\mu_0^2 - \\mu_1^2) = -(\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0)$:\n$$Z_t = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} X_t - \\frac{(\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0)}{2\\sigma^2}$$\nThis can be written more compactly as:\n$$Z_t = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( X_t - \\frac{\\mu_0 + \\mu_1}{2} \\right)$$\nThe test statistic is the cumulative sum of these log-likelihood increments over $n$ samples:\n$$S_n = \\sum_{t=1}^{n} Z_t$$\n\n#### Stopping Rule and Thresholds\nThe SPRT proceeds by updating $S_n$ with each new observation $X_n$. At each step $n$, $S_n$ is compared to two fixed thresholds, $a$ and $b$. The decision rule is:\n1. If $S_n \\le a$, stop sampling and accept $H_0$.\n2. If $S_n \\ge b$, stop sampling and accept $H_1$.\n3. If $a  S_n  b$, continue sampling by taking the next observation $X_{n+1}$.\n\nThe thresholds $a$ and $b$ are chosen to satisfy the desired error probabilities, $\\alpha$ (Type I error, a false alarm) and $\\beta$ (Type II error, a missed detection). Wald's approximations, which are accurate for small $\\alpha$ and $\\beta$, relate the thresholds to the error probabilities as follows:\n$$a \\approx \\ln\\left(\\frac{\\beta}{1-\\alpha}\\right)$$\n$$b \\approx \\ln\\left(\\frac{1-\\beta}{\\alpha}\\right)$$\nGiven that $\\alpha  1-\\beta$ and $\\beta  1-\\alpha$ for any meaningful test, we have $\\beta/(1-\\alpha)  1$ and $(1-\\beta)/\\alpha  1$, which ensures $a  0  b$.\n\n#### Expected Per-Sample Drift\nThe behavior of the cumulative sum $S_n$ is characterized by its expected per-sample increment, or drift. We calculate this drift under both hypotheses.\nUnder $H_0$, the true mean is $\\mu_0$, so $\\mathbb{E}_0[X_t] = \\mu_0$. The expected value of $Z_t$ is:\n$$\\mathbb{E}_0[Z_t] = \\mathbb{E}_0\\left[ \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( X_t - \\frac{\\mu_0 + \\mu_1}{2} \\right) \\right]$$\n$$\\mathbb{E}_0[Z_t] = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( \\mathbb{E}_0[X_t] - \\frac{\\mu_0 + \\mu_1}{2} \\right) = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( \\mu_0 - \\frac{\\mu_0 + \\mu_1}{2} \\right)$$\n$$\\mathbb{E}_0[Z_t] = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( \\frac{2\\mu_0 - \\mu_0 - \\mu_1}{2} \\right) = -\\frac{(\\mu_1 - \\mu_0)^2}{2\\sigma^2}$$\nUnder $H_1$, the true mean is $\\mu_1$, so $\\mathbb{E}_1[X_t] = \\mu_1$. The expected value of $Z_t$ is:\n$$\\mathbb{E}_1[Z_t] = \\mathbb{E}_1\\left[ \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( X_t - \\frac{\\mu_0 + \\mu_1}{2} \\right) \\right]$$\n$$\\mathbb{E}_1[Z_t] = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( \\mathbb{E}_1[X_t] - \\frac{\\mu_0 + \\mu_1}{2} \\right) = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( \\mu_1 - \\frac{\\mu_0 + \\mu_1}{2} \\right)$$\n$$\\mathbb{E}_1[Z_t] = \\frac{\\mu_1 - \\mu_0}{\\sigma^2} \\left( \\frac{2\\mu_1 - \\mu_0 - \\mu_1}{2} \\right) = \\frac{(\\mu_1 - \\mu_0)^2}{2\\sigma^2}$$\nNote that $\\mathbb{E}_0[Z_t] = -\\mathbb{E}_1[Z_t]$. Under $H_0$, the statistic $S_n$ drifts downwards towards threshold $a$, and under $H_1$, it drifts upwards towards threshold $b$.\n\n#### Expected Sample Size\nLet $N$ be the stopping time of the test. Wald's identity states that for a sum of i.i.d. random variables, $\\mathbb{E}[S_N] = \\mathbb{E}[N] \\mathbb{E}[Z_t]$. We can use this to find the average sample number (ASN), or expected sample size, $\\mathbb{E}[N]$.\nThe approximation of negligible overshoot assumes that when the process stops, $S_N$ is approximately equal to the boundary it crosses, i.e., $S_N \\approx a$ or $S_N \\approx b$.\n\nUnder $H_0$, the test stops at boundary $a$ with probability $1-\\alpha$ (correct decision) and at boundary $b$ with probability $\\alpha$ (Type I error). The expected value of $S_N$ is approximately:\n$$\\mathbb{E}_0[S_N] \\approx (1-\\alpha) \\cdot a + \\alpha \\cdot b$$\nApplying Wald's identity:\n$$\\mathbb{E}_0[N] \\mathbb{E}_0[Z_t] \\approx (1-\\alpha)a + \\alpha b$$\nSolving for the expected sample size under $H_0$:\n$$\\mathbb{E}_0[N] \\approx \\frac{(1-\\alpha)a + \\alpha b}{\\mathbb{E}_0[Z_t]}$$\nUnder $H_1$, the test stops at boundary $b$ with probability $1-\\beta$ (correct decision) and at boundary $a$ with probability $\\beta$ (Type II error). The expected value of $S_N$ is approximately:\n$$\\mathbb{E}_1[S_N] \\approx \\beta \\cdot a + (1-\\beta) \\cdot b$$\nApplying Wald's identity:\n$$\\mathbb{E}_1[N] \\mathbb{E}_1[Z_t] \\approx \\beta a + (1-\\beta) b$$\nSolving for the expected sample size under $H_1$:\n$$\\mathbb{E}_1[N] \\approx \\frac{\\beta a + (1-\\beta) b}{\\mathbb{E}_1[Z_t]}$$\n\n### Numerical Evaluation\nWe now substitute the given numerical values into the derived expressions.\n- Parameters: $\\mu_0 = 500$, $\\mu_1 = 520$, $\\sigma = 20$, $\\alpha = 0.01$, $\\beta = 0.05$.\n\nFirst, calculate the thresholds $a$ and $b$:\n$$a = \\ln\\left(\\frac{\\beta}{1-\\alpha}\\right) = \\ln\\left(\\frac{0.05}{1-0.01}\\right) = \\ln\\left(\\frac{0.05}{0.99}\\right) \\approx -2.985676$$\n$$b = \\ln\\left(\\frac{1-\\beta}{\\alpha}\\right) = \\ln\\left(\\frac{1-0.05}{0.01}\\right) = \\ln\\left(\\frac{0.95}{0.01}\\right) = \\ln(95) \\approx 4.553877$$\n\nNext, calculate the expected drifts:\n$$\\mu_1 - \\mu_0 = 520 - 500 = 20$$\n$$\\sigma^2 = 20^2 = 400$$\n$$\\mathbb{E}_1[Z_t] = \\frac{(\\mu_1 - \\mu_0)^2}{2\\sigma^2} = \\frac{20^2}{2 \\cdot 400} = \\frac{400}{800} = 0.5$$\n$$\\mathbb{E}_0[Z_t] = -0.5$$\n\nFinally, calculate the expected sample sizes $\\mathbb{E}_0[N]$ and $\\mathbb{E}_1[N]$:\n$$\\mathbb{E}_0[N] \\approx \\frac{(1-0.01) \\cdot (-2.985676) + 0.01 \\cdot (4.553877)}{-0.5}$$\n$$\\mathbb{E}_0[N] \\approx \\frac{0.99 \\cdot (-2.985676) + 0.01 \\cdot (4.553877)}{-0.5} = \\frac{-2.955819 + 0.045539}{-0.5} = \\frac{-2.91028}{-0.5} \\approx 5.82056$$\nRounding to four significant figures, $\\mathbb{E}_0[N] \\approx 5.821$.\n\n$$\\mathbb{E}_1[N] \\approx \\frac{0.05 \\cdot (-2.985676) + (1-0.05) \\cdot (4.553877)}{0.5}$$\n$$\\mathbb{E}_1[N] \\approx \\frac{0.05 \\cdot (-2.985676) + 0.95 \\cdot (4.553877)}{0.5} = \\frac{-0.149284 + 4.326183}{0.5} = \\frac{4.176899}{0.5} \\approx 8.35380$$\nRounding to four significant figures, $\\mathbb{E}_1[N] \\approx 8.354$.\n\nThese values represent the expected number of one-minute samples required to reach a decision.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 5.821  8.354 \\end{pmatrix} } $$"
        },
        {
            "introduction": "While online methods are vital for immediate response, offline analysis of historical data is essential for model building, forecasting, and understanding long-term system behavior. This practice moves from detecting a single change to segmenting a complete time series into multiple distinct regimes using dynamic programming. You will implement an algorithm to find the optimal segmentation that balances data fidelity against model complexity, providing a hands-on feel for the trade-offs involved in avoiding both under- and oversegmentation .",
            "id": "4077437",
            "problem": "Consider a univariate solar irradiance time series with intermittent cloud passages measured at a fixed sampling interval of $1$ minute, where irradiance is recorded in watts per square meter (W/m$^2$). Let the time series be denoted by $\\{x_t\\}_{t=1}^N$, where $x_t$ is modeled as a piecewise constant mean corrupted by independent Gaussian noise: $x_t = \\mu_{s(t)} + \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ and $s(t)$ the segment index for time $t$. A change point is any index $t$ at which the segment mean changes. In energy systems modeling, change point detection in energy time series is often posed as a penalized likelihood problem with a minimum segment length constraint to reduce oversegmentation and undersegmentation.\n\nStarting from the definitions of independent Gaussian noise and the principle of model selection via minimum description length and information criteria, derive and implement a program that, given a penalty magnitude $\\lambda$ applied per segment and a minimum segment length $L_{\\min}$, returns the integer classification of oversegmentation versus undersegmentation for a fixed, synthetic solar irradiance time series with known ground truth change points. Oversegmentation is defined as detecting a strictly greater number of segments than the ground truth ($+1$), undersegmentation is defined as detecting a strictly smaller number of segments than the ground truth ($-1$), and matched segmentation is defined as detecting exactly the same number of segments as the ground truth ($0$).\n\nUse the following scientifically realistic synthetic scenario to define $\\{x_t\\}_{t=1}^N$: Let $N = 480$ minutes. The ground truth consists of $9$ constant irradiance segments with means (in W/m$^2$) and durations (in minutes) given by the ordered lists\nmeans $= [800, 300, 820, 500, 850, 450, 830, 350, 820]$,\ndurations $= [90, 30, 80, 40, 60, 10, 70, 50, 50]$,\nso that the ground-truth change points occur at cumulative indices $[90, 120, 200, 240, 300, 310, 380, 430]$. Add independent Gaussian measurement noise with standard deviation $\\sigma = 60$ W/m$^2$. Fix the random seed deterministically so that results are reproducible. The program must solve the penalized segmentation problem that minimizes the sum of within-segment squared errors plus $\\lambda$ times the number of segments, subject to the constraint that each segment has length at least $L_{\\min}$.\n\nImplement an algorithm that computes the optimal segmentation under these constraints and returns the integer classification $+1$, $0$, or $-1$ for each of the test cases below. Do not assume any external libraries beyond the specified runtime environment. Angles are not used in this problem. Time is measured in minutes and irradiance in W/m$^2$, but the final outputs are unitless integers as defined above.\n\nTest Suite:\n- Case $1$: $\\lambda = 0$, $L_{\\min} = 1$.\n- Case $2$: $\\lambda = 10^9$, $L_{\\min} = 1$.\n- Case $3$: $\\lambda = 10^6$, $L_{\\min} = 1$.\n- Case $4$: $\\lambda = 10^3$, $L_{\\min} = 20$.\n- Case $5$: $\\lambda = 5 \\times 10^6$, $L_{\\min} = 50$.\n- Case $6$: $\\lambda = 10^6$, $L_{\\min} = 50$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$result_1,result_2,\\dots,result_6$]\"), where each $result_i$ is the integer classification for the corresponding test case in the order listed above. No other output should be produced. The final outputs are unitless integers.",
            "solution": "The user wants me to solve a change point detection problem.\n\n### Step 1: Extract Givens\n-   **Time Series Model**: A univariate time series $\\{x_t\\}_{t=1}^N$ is modeled as $x_t = \\mu_{s(t)} + \\varepsilon_t$.\n-   **Noise**: The noise term $\\varepsilon_t$ is independent and identically distributed as a Gaussian distribution, $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n-   **Signal**: The mean $\\mu_{s(t)}$ is piecewise constant, where $s(t)$ is the segment index for time $t$.\n-   **Objective Function**: Find a segmentation that minimizes the sum of within-segment squared errors plus a penalty term, $\\lambda \\times (\\text{number of segments})$.\n-   **Constraint**: Each segment must have a length of at least $L_{\\min}$.\n-   **Output Classification**:\n    -   Oversegmentation ($+1$): Number of detected segments  number of ground-truth segments.\n    -   Undersegmentation ($-1$): Number of detected segments  number of ground-truth segments.\n    -   Matched segmentation ($0$): Number of detected segments = number of ground-truth segments.\n-   **Synthetic Data Parameters**:\n    -   Total length: $N = 480$ minutes.\n    -   Sampling interval: $1$ minute.\n    -   Ground-truth means (W/m$^2$): `means` $= [800, 300, 820, 500, 850, 450, 830, 350, 820]$.\n    -   Ground-truth durations (minutes): `durations` $= [90, 30, 80, 40, 60, 10, 70, 50, 50]$.\n    -   Number of ground-truth segments: $K_{true} = 9$.\n    -   Noise standard deviation: $\\sigma = 60$ W/m$^2$.\n    -   A deterministic random seed must be used.\n-   **Test Suite**:\n    -   Case 1: $\\lambda = 0$, $L_{\\min} = 1$.\n    -   Case 2: $\\lambda = 10^9$, $L_{\\min} = 1$.\n    -   Case 3: $\\lambda = 10^6$, $L_{\\min} = 1$.\n    -   Case 4: $\\lambda = 10^3$, $L_{\\min} = 20$.\n    -   Case 5: $\\lambda = 5 \\times 10^6$, $L_{\\min} = 50$.\n    -   Case 6: $\\lambda = 10^6$, $L_{\\min} = 50$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically well-defined.\n-   **Scientifically Grounded**: The model of a piecewise constant signal corrupted by Gaussian noise is a canonical model in signal processing and statistics. The problem of change point detection is fundamental in time series analysis. Using a penalized likelihood is a standard technique for model selection, directly related to information criteria like AIC and BIC. The physical context of solar irradiance and the numerical values provided are realistic. The problem is valid.\n-   **Well-Posed**: The task is to solve a clearly defined optimization problem. An optimal solution is guaranteed to exist. The use of dynamic programming provides a concrete method to find this unique optimal solution. The problem is valid.\n-   **Objective**: The problem is specified with precise mathematical definitions, numerical data, and unambiguous terminology. The objective function and constraints are clearly stated. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution Derivation\n\nThe problem is to find an optimal partition of the time series $\\{x_t\\}_{t=0}^{N-1}$ into segments that minimizes a penalized cost function. This is a classic change point detection problem that can be solved exactly using dynamic programming.\n\n**1. Mathematical Formulation**\n\nLet the time series be $x_0, x_1, \\dots, x_{N-1}$. A segmentation is defined by a set of change points $0 = \\tau_0  \\tau_1  \\dots  \\tau_K = N$, which partition the data into $K$ segments. The $k$-th segment consists of data points $\\{x_t\\}_{t=\\tau_{k-1}}^{\\tau_k-1}$.\n\nThe problem is to minimize the total cost, which is the sum of two terms: the total within-segment sum of squared errors and a penalty term proportional to the number of segments.\nThe cost function to minimize is:\n$$ \\mathcal{J}(\\tau_1, \\dots, \\tau_K) = \\sum_{k=1}^{K} \\left( \\sum_{t=\\tau_{k-1}}^{\\tau_k-1} (x_t - \\hat{\\mu}_k)^2 \\right) + \\lambda K $$\nwhere $\\hat{\\mu}_k$ is the estimated mean of the $k$-th segment and $K$ is the total number of segments.\n\nThis formulation is derived from the principle of maximum likelihood estimation under a Gaussian noise model, combined with a penalty for model complexity, akin to information criteria such as AIC or BIC. For a segment of data assumed to be drawn from $\\mathcal{N}(\\mu, \\sigma^2)$, the maximum likelihood estimate for the mean $\\mu$ is the sample mean. Minimizing the sum of squared errors $\\sum(x_t - \\hat{\\mu})^2$ is equivalent to maximizing the likelihood of the data. The term $\\lambda K$ is a penalty for model complexity, preventing overfitting (oversegmentation) by penalizing each additional segment.\n\nLet $c(i, j)$ denote the minimum sum of squared errors for a single segment of data from index $i$ to $j-1$, i.e., $\\{x_t\\}_{t=i}^{j-1}$. This cost is achieved when the mean is the sample mean $\\hat{\\mu}_{i..j-1} = \\frac{1}{j-i} \\sum_{t=i}^{j-1} x_t$. The cost is then:\n$$ c(i, j) = \\sum_{t=i}^{j-1} (x_t - \\hat{\\mu}_{i..j-1})^2 = \\left(\\sum_{t=i}^{j-1} x_t^2\\right) - \\frac{1}{j-i} \\left(\\sum_{t=i}^{j-1} x_t\\right)^2 $$\nThe total cost can be expressed as:\n$$ \\mathcal{J} = \\sum_{k=1}^{K} \\left( c(\\tau_{k-1}, \\tau_k) + \\lambda \\right) $$\n\n**2. Dynamic Programming Solution**\n\nThis optimization problem exhibits optimal substructure, making it ideal for dynamic programming. Let $F[i]$ be the minimum cost for an optimal segmentation of the first $i$ data points, $x_0, \\dots, x_{i-1}$. Our goal is to compute $F[N]$.\n\nThe base case is for an empty prefix, which has a cost of $0$. Thus, $F[0] = 0$.\n\nFor $i  0$, the value of $F[i]$ can be computed by considering all possible end points for the previous segment. If the last segment is $x_j, \\dots, x_{i-1}$, then the total cost is the optimal cost for segmenting up to $x_{j-1}$ (which is $F[j]$) plus the cost of this new final segment (which is $c(j, i) + \\lambda$). We must minimize this over all possible start points $j$.\n\nThe recurrence relation is:\n$$ F[i] = \\min_{0 \\le j  i} \\{ F[j] + c(j, i) + \\lambda \\} $$\nWe are also given the constraint that each segment must have a length of at least $L_{\\min}$. The length of the segment from $j$ to $i-1$ is $i-j$. Therefore, we must have $i-j \\ge L_{\\min}$, which implies $j \\le i - L_{\\min}$.\n\nThe constrained recurrence relation becomes:\n$$ F[i] = \\min_{0 \\le j \\le i - L_{\\min}} \\{ F[j] + c(j, i) + \\lambda \\} $$\nfor $i = 1, \\dots, N$. To reconstruct the segmentation, we also store the optimal index $j$ that gives the minimum for each $i$. Let this be $P[i]$.\n\n**3. Efficient Implementation**\n\nA naive implementation of this DP would be slow due to the repeated calculation of $c(j, i)$. The calculation of $c(j,i)$ takes $O(i-j)$ time, making the total complexity $O(N^3)$. We can reduce this to $O(N^2)$ by pre-computing cumulative sums.\nLet $S_1[i] = \\sum_{t=0}^{i-1} x_t$ and $S_2[i] = \\sum_{t=0}^{i-1} x_t^2$.\nThen $\\sum_{t=j}^{i-1} x_t = S_1[i] - S_1[j]$ and $\\sum_{t=j}^{i-1} x_t^2 = S_2[i] - S_2[j]$.\nThe cost $c(j, i)$ can now be computed in $O(1)$ time:\n$$ c(j, i) = (S_2[i] - S_2[j]) - \\frac{(S_1[i] - S_1[j])^2}{i-j} $$\nWith this optimization, the DP algorithm runs in $O(N^2)$ time, which is efficient enough for $N=480$.\n\n**4. Reconstructing the Solution and Classification**\n\nAfter populating the DP table $F$ and the pointer table $P$ up to $N$, the optimal cost is $F[N]$. To find the number of segments, we backtrack using the pointers from $P[N]$.\nLet the current index be `idx`$=N$.\nThe start of the last segment is $P[N]$.\nThe start of the second to last segment is $P[P[N]]$, and so on, until we reach $0$.\nThe number of segments is the number of backtracking steps required to go from $N$ to $0$.\n\nLet $K_{found}$ be the number of segments found by the algorithm. The ground truth number of segments is $K_{true} = 9$. The final classification is determined by the sign of $K_{found} - K_{true}$:\n-   If $K_{found}  K_{true}$, the result is $+1$ (oversegmentation).\n-   If $K_{found}  K_{true}$, the result is $-1$ (undersegmentation).\n-   If $K_{found} = K_{true}$, the result is $0$ (matched segmentation).\n\nThis procedure is applied to each test case with its specific $\\lambda$ and $L_{\\min}$ values.",
            "answer": "[1,-1,0,-1,-1,-1]"
        },
        {
            "introduction": "Many standard statistical methods, including those based on least-squares, assume Gaussian noise, but real-world energy data often feature heavy-tailed fluctuations from sources like intermittent renewables. This exercise explores the critical concept of statistical robustness by comparing a standard mean-based detector with a median-based one under Laplace noise. Through the derivation of the Pitman Asymptotic Relative Efficiency (ARE), you will quantify the performance gain from using robust statistics and understand why it is essential for analyzing volatile energy time series .",
            "id": "4077349",
            "problem": "A utility-scale photovoltaic field produces a time series of power deviations $y_t$ from its short-horizon forecast. Cloud transients and inverter control produce heavy-tailed fluctuation residuals that are well approximated by a Laplace distribution. Consider a canonical single change point model for location (level) shifts in the residual time series: for times $t  \\tau$, $y_t = \\varepsilon_t$ and for times $t \\ge \\tau$, $y_t = \\delta + \\varepsilon_t$, where $\\varepsilon_t$ are independent and identically distributed with the Laplace distribution of location $0$ and scale $b$, having probability density function $f(x) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|x|}{b}\\right)$. The pre-change location equals $0$ and the post-change location equals $\\delta$, with $|\\delta|$ small relative to $b$.\n\nYou are asked to compare two Cumulative Sum (CUSUM, Cumulative Sum) detectors that aggregate instantaneous increments $T(y_t)$:\n\n- A mean-based CUSUM using $T_{\\mathrm{mean}}(y_t) = y_t$ to target shifts in the mean.\n- A median-based CUSUM using $T_{\\mathrm{med}}(y_t) = \\operatorname{sign}(y_t)$, motivated by the influence function of the sample median in a symmetric distribution, to target shifts in the median.\n\nDefine the Pitman asymptotic relative efficiency (ARE) of $T_{\\mathrm{med}}$ relative to $T_{\\mathrm{mean}}$ for detecting small location shifts under the Laplace noise model as the ratio\n$$\n\\mathrm{ARE} \\equiv \\frac{\\left(\\Delta_{\\mathrm{med}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)}{\\left(\\Delta_{\\mathrm{mean}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)},\n$$\nwhere $Y$ denotes a pre-change random variable with distribution Laplace$(0,b)$, $\\operatorname{Var}_{0}$ denotes variance under the pre-change distribution, and $\\Delta_{\\mathrm{stat}}$ is the leading-order change in the expectation of $T_{\\mathrm{stat}}(Y + \\delta)$ as $\\delta \\to 0$.\n\nStarting from the stated distributional properties and general definitions of expectation and variance, derive a closed-form expression for $\\mathrm{ARE}$ under the Laplace noise model. Express the final answer as a dimensionless number. No rounding is required.",
            "solution": "The user has asked for the derivation of the Pitman Asymptotic Relative Efficiency (ARE) for two change point detectors under a Laplace noise model.\n\n### Step 1: Extract Givens\n- The time series model for power deviations $y_t$ has a single change point at time $\\tau$.\n- For $t  \\tau$, $y_t = \\varepsilon_t$.\n- For $t \\ge \\tau$, $y_t = \\delta + \\varepsilon_t$.\n- The noise terms $\\varepsilon_t$ are independent and identically distributed (i.i.d.) random variables from a Laplace distribution with location $0$ and scale $b$.\n- The probability density function (PDF) of the noise is $f(x) = \\frac{1}{2 b} \\exp\\left(-\\frac{|x|}{b}\\right)$.\n- The first detector is a mean-based CUSUM with increment function $T_{\\mathrm{mean}}(y_t) = y_t$.\n- The second detector is a median-based CUSUM with increment function $T_{\\mathrm{med}}(y_t) = \\operatorname{sign}(y_t)$.\n- The ARE is defined as:\n$$\n\\mathrm{ARE} \\equiv \\frac{\\left(\\Delta_{\\mathrm{med}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)}{\\left(\\Delta_{\\mathrm{mean}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)}\n$$\n- $Y$ is a pre-change random variable, so $Y \\sim \\text{Laplace}(0,b)$.\n- $\\operatorname{Var}_{0}$ denotes variance under the pre-change distribution (i.e., for $Y$).\n- $\\Delta_{\\mathrm{stat}}$ is the leading-order change in the expectation of $T_{\\mathrm{stat}}(Y + \\delta)$ as $\\delta \\to 0$. This is equivalent to the derivative $\\left. \\frac{d}{d\\delta} \\mathbb{E}[T_{\\mathrm{stat}}(Y+\\delta)] \\right|_{\\delta=0}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the fields of statistical signal processing and time series analysis. The Laplace distribution, CUSUM algorithm, and Pitman ARE are all standard, well-established concepts. The problem is well-posed, providing all necessary definitions, distributions, and a clear objective. The language is precise and objective. The setup is mathematically and statistically consistent, non-contradictory, and the required calculations are feasible. Therefore, the problem is deemed **valid**.\n\n### Step 3: Derivation of the Solution\nTo compute the ARE, we must calculate four quantities: $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)$, $\\Delta_{\\mathrm{mean}}$, $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)$, and $\\Delta_{\\mathrm{med}}$.\n\nLet $Y$ be a random variable with the pre-change Laplace distribution, $Y \\sim \\text{Laplace}(0, b)$. Its PDF is $f_Y(y) = \\frac{1}{2b} \\exp(-\\frac{|y|}{b})$. The standard properties of this distribution are that its mean is $\\mathbb{E}[Y]=0$ and its variance is $\\operatorname{Var}(Y) = 2b^2$.\n\n**Analysis of the Mean-Based Detector ($T_{\\mathrm{mean}}(Y) = Y$)**\n\n1.  **Variance under the null hypothesis ($H_0: \\delta=0$):**\n    The increment function is $T_{\\mathrm{mean}}(Y) = Y$. The variance under the pre-change distribution is:\n    $$\n    \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right) = \\operatorname{Var}(Y) = 2b^2\n    $$\n\n2.  **Efficacy term ($\\Delta_{\\mathrm{mean}}$):**\n    We need to compute the derivative of the expectation of the post-change statistic, $T_{\\mathrm{mean}}(Y+\\delta) = Y+\\delta$, with respect to $\\delta$ at $\\delta=0$.\n    The expectation of the post-change statistic is:\n    $$\n    \\mathbb{E}[T_{\\mathrm{mean}}(Y+\\delta)] = \\mathbb{E}[Y+\\delta] = \\mathbb{E}[Y] + \\delta\n    $$\n    Since $\\mathbb{E}[Y]=0$, we have:\n    $$\n    \\mathbb{E}[T_{\\mathrm{mean}}(Y+\\delta)] = \\delta\n    $$\n    Now, we compute $\\Delta_{\\mathrm{mean}}$:\n    $$\n    \\Delta_{\\mathrm{mean}} = \\left. \\frac{d}{d\\delta} \\mathbb{E}[T_{\\mathrm{mean}}(Y+\\delta)] \\right|_{\\delta=0} = \\left. \\frac{d}{d\\delta}(\\delta) \\right|_{\\delta=0} = 1\n    $$\n\n**Analysis of the Median-Based Detector ($T_{\\mathrm{med}}(Y) = \\operatorname{sign}(Y)$)**\n\n1.  **Variance under the null hypothesis ($H_0: \\delta=0$):**\n    The increment function is $T_{\\mathrm{med}}(Y) = \\operatorname{sign}(Y)$. First, we compute its expectation under the pre-change distribution.\n    $$\n    \\mathbb{E}_{0}[T_{\\mathrm{med}}(Y)] = \\mathbb{E}[\\operatorname{sign}(Y)] = \\int_{-\\infty}^{\\infty} \\operatorname{sign}(y) f_Y(y) dy\n    $$\n    Since $\\operatorname{sign}(y)$ is an odd function and $f_Y(y)$ is an even function (symmetric about $0$), their product is an odd function, and the integral over a symmetric interval $(-\\infty, \\infty)$ is $0$. Thus, $\\mathbb{E}_{0}[T_{\\mathrm{med}}(Y)] = 0$.\n    The variance is then:\n    $$\n    \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right) = \\mathbb{E}_{0}[(T_{\\mathrm{med}}(Y))^2] - (\\mathbb{E}_{0}[T_{\\mathrm{med}}(Y)])^2 = \\mathbb{E}[(\\operatorname{sign}(Y))^2] - 0^2\n    $$\n    For any $y \\neq 0$, $(\\operatorname{sign}(y))^2=1$. Since $Y$ is a continuous random variable, $P(Y=0)=0$.\n    $$\n    \\mathbb{E}[(\\operatorname{sign}(Y))^2] = \\int_{-\\infty}^{\\infty} (\\operatorname{sign}(y))^2 f_Y(y) dy = \\int_{-\\infty}^{\\infty} 1 \\cdot f_Y(y) dy = 1\n    $$\n    Therefore, the variance is:\n    $$\n    \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right) = 1\n    $$\n\n2.  **Efficacy term ($\\Delta_{\\mathrm{med}}$):**\n    We need to compute $\\Delta_{\\mathrm{med}} = \\left. \\frac{d}{d\\delta} \\mathbb{E}[T_{\\mathrm{med}}(Y+\\delta)] \\right|_{\\delta=0}$. The expectation is:\n    $$\n    \\mathbb{E}[T_{\\mathrm{med}}(Y+\\delta)] = \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = \\int_{-\\infty}^{\\infty} \\operatorname{sign}(y+\\delta) f_Y(y) dy\n    $$\n    This is equivalent to $P(Y+\\delta  0) - P(Y+\\delta  0) = P(Y  -\\delta) - P(Y  -\\delta)$.\n    Let $F_Y(y)$ be the cumulative distribution function (CDF) of $Y \\sim \\text{Laplace}(0,b)$. Then $P(Y  -\\delta) = F_Y(-\\delta)$ and $P(Y  -\\delta) = 1 - F_Y(-\\delta)$.\n    $$\n    \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = (1 - F_Y(-\\delta)) - F_Y(-\\delta) = 1 - 2F_Y(-\\delta)\n    $$\n    The CDF of a Laplace($0,b$) distribution is:\n    $$\n    F_Y(y) = \\begin{cases} \\frac{1}{2} \\exp\\left(\\frac{y}{b}\\right)  y \\le 0 \\\\ 1 - \\frac{1}{2} \\exp\\left(-\\frac{y}{b}\\right)  y  0 \\end{cases}\n    $$\n    We analyze the expectation for small positive and negative $\\delta$.\n    - For $\\delta  0$, $-\\delta  0$, so we use the first case of the CDF: $F_Y(-\\delta) = \\frac{1}{2}\\exp(-\\frac{\\delta}{b})$.\n      $$\n      \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = 1 - 2 \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\delta}{b}\\right)\\right) = 1 - \\exp\\left(-\\frac{\\delta}{b}\\right)\n      $$\n    - For $\\delta  0$, $-\\delta  0$, so we use the second case of the CDF: $F_Y(-\\delta) = 1 - \\frac{1}{2}\\exp(-(-\\frac{\\delta}{b})) = 1 - \\frac{1}{2}\\exp(\\frac{\\delta}{b})$.\n      $$\n      \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = 1 - 2 \\left(1 - \\frac{1}{2} \\exp\\left(\\frac{\\delta}{b}\\right)\\right) = e^{\\frac{\\delta}{b}} - 1\n      $$\n    To find $\\Delta_{\\mathrm{med}}$, we differentiate with respect to $\\delta$ and evaluate at $\\delta=0$. We can use the one-sided limits.\n    For $\\delta \\to 0^+$:\n    $$\n    \\lim_{\\delta \\to 0^+} \\frac{1 - \\exp\\left(-\\frac{\\delta}{b}\\right)}{\\delta} = \\lim_{\\delta \\to 0^+} \\frac{-\\left(-\\frac{1}{b}\\right)\\exp\\left(-\\frac{\\delta}{b}\\right)}{1} = \\frac{1}{b}\n    $$\n    For $\\delta \\to 0^-$:\n    $$\n    \\lim_{\\delta \\to 0^-} \\frac{\\exp\\left(\\frac{\\delta}{b}\\right) - 1}{\\delta} = \\lim_{\\delta \\to 0^-} \\frac{\\frac{1}{b}\\exp\\left(\\frac{\\delta}{b}\\right)}{1} = \\frac{1}{b}\n    $$\n    Since both limits are equal, the derivative exists at $\\delta=0$ and its value is $\\frac{1}{b}$.\n    Alternatively, the leading-order term in the Taylor expansion of $\\mathbb{E}[\\operatorname{sign}(Y+\\delta)]$ around $\\delta = 0$ is $\\frac{\\delta}{b}$. The coefficient is $\\frac{1}{b}$.\n    Thus, $\\Delta_{\\mathrm{med}} = \\frac{1}{b}$.\n\n**Calculation of the ARE**\n\nWe now substitute the four calculated quantities into the ARE formula:\n- $\\left(\\Delta_{\\mathrm{mean}}\\right)^{2} = 1^2 = 1$\n- $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right) = 2b^2$\n- $\\left(\\Delta_{\\mathrm{med}}\\right)^{2} = (\\frac{1}{b})^2 = \\frac{1}{b^2}$\n- $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right) = 1$\n\nThe efficacy of the mean-based detector is $\\mathcal{E}_{\\mathrm{mean}} = \\frac{\\left(\\Delta_{\\mathrm{mean}}\\right)^{2}}{\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)} = \\frac{1}{2b^2}$.\nThe efficacy of the median-based detector is $\\mathcal{E}_{\\mathrm{med}} = \\frac{\\left(\\Delta_{\\mathrm{med}}\\right)^{2}}{\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)} = \\frac{1/b^2}{1} = \\frac{1}{b^2}$.\n\nThe ARE is the ratio of these efficacies:\n$$\n\\mathrm{ARE} = \\frac{\\mathcal{E}_{\\mathrm{med}}}{\\mathcal{E}_{\\mathrm{mean}}} = \\frac{1/b^2}{1/(2b^2)} = \\frac{1}{b^2} \\cdot \\frac{2b^2}{1} = 2\n$$\nThe Asymptotic Relative Efficiency of the median-based detector relative to the mean-based detector for a location shift in Laplace noise is $2$. This value being greater than $1$ indicates that the median-based detector is more efficient, which is expected given the heavy tails of the Laplace distribution.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}